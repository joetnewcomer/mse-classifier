question_id,title,body,tags
2927763,Differential operators on a smooth mainifold,"Let $M$ be a smooth manifold. If I'm not wrong, the set of differential operators on $M$ is defined as $\mathcal{D}_M  $ can be defined by using vector fields. I.e. for each $D \in \mathcal{D}$ we have $D = X_1 \circ \dots \circ X_k$ for some smooth vector fields $X_1, \dots X_k$ . Is it correct to think about differential operators in this way? Moreover I was told that jets are the homomrphism from $\mathcal{D}$ to $C^\infty(M)$ . Can you give me an example of jet? My idea: take $f \in C^{\infty}(M)$ . Let's define $J_f$ as follows: for every $D \in \mathcal{D}$ $$
J_f(D) := D(f).
$$ This should be an homomorphism from $\mathcal{D}$ to $C^\infty(M)$ , right? Are all jets defined in this way?","['jet-bundles', 'differential-topology', 'modules', 'differential-geometry']"
2927799,"Moment Generating Functions of Normal/Gaussian Random Variable: 3rd, 4th,..,kth Moment","Derivation of Normal Random MGF: I'm having trouble deriving the answers for $E\,[X^3]$ and $E\,[X^4]$ given the information from the image I've posted. In the image I understand how they setup the equation for normal random distribution $X$ : given as $G(\theta)$ . What I am not understanding is how you can go from simply completing the square to finding the 4th moment. Am I simply misinterpreting the equation given in the image or do I need to complete more steps that are not stated in the image?","['stochastic-processes', 'statistics', 'probability']"
2927808,Is it possible that probability of occuring intersection of two events will be greater than probability of occuring each of them?,"If $A$ and $B$ are correlated events, Is it possible that we have : $$P\left(A \cap B \right) \geq P\left( A\right)$$ and $$P\left(A \cap B \right) \geq P\left( B\right)$$ Is it possible?","['statistical-inference', 'statistics', 'probability-theory', 'probability']"
2927809,On the real roots of the chromatic polynomial,"If $\chi_G(k)$ is the chromatic polynomial of graph $G$ on $n$ vertices, we want to prove the following statements: The only real root $x<1$ for $\chi_G(k)$ except for $x=0$ . There is no real root $x>n$ for $\chi_G(k)$ . Things that I have tried: I can show that the terms of the chromatic polynomial alternate in sign. This means that there is no negative root. But still cannot handle the interval $(0,1)$ . By definition of the chromatic polynomial, it is obvious that there is no integer root. But hard to say something about the real roots. Any help is appreciated.","['coloring', 'graph-theory', 'algebraic-graph-theory', 'polynomials', 'discrete-mathematics']"
2927823,Set difference between three sets,"I am trying to prove whether the following claim is true or false. Given sets $X, Y, Z$ , is the following equality of sets true or false. $$(X \setminus Y) \setminus Z = (X \setminus Z) \setminus Y $$ I believe the claim is true. My thinking is that the set on the LHS is simply all those elements of $X$ that are not in $Y$ and not in $Z$ . The set on the RHS is also all those elements of $X$ that are not in $Z$ and not in $Y$ . For example, if $X = \{1,2,3,4,5\}$ , $Y = \{1,2,3\}$ and $Z = \{3,4\}$ then $(X \setminus Y) \setminus Z = \{5\}$ and $(X \setminus Z) \setminus Y = \{5\}$ as well. I am having trouble proving the claim formally though (perhaps I am overthinking the question or missing something obvious). If anyone could help me out, that would be great! Thanks!",['elementary-set-theory']
2927828,Prove $\lim_{x\rightarrow -3} 1-4x=13$,"Problem Prove $$\lim_{x\rightarrow -3} 1-4x=13$$ Using $\delta, \epsilon$ definiton of limits. Attempt to solve I can use $\delta ,\epsilon$ definition of limit. If i can show $$ |x-a| < \delta \implies |f(x)-L| < \epsilon$$ It implies limit exists according to $\delta, \epsilon$ definition of limits. $$ |x-(-3)|< \delta \implies |1-4x-13|< \epsilon $$ $$ |1-4x-13|< \epsilon \iff |-4x-12| < \epsilon \iff |4x+12|< \epsilon $$ $$ |x-(-3)|< \delta \iff |x+3| < \delta $$ $$ \text{let }  \delta = \epsilon/4$$ $$ |x+3| < \delta \implies |x+3|<\epsilon/4 \implies$$ $$ 4|x+3|<\epsilon \implies $$ $$ |4(x+3)|< \epsilon \implies $$ $$ |4x+12| < \epsilon $$ $$\tag*{$\square$}$$ I would like to have some feedback if my solution looks correct or not.","['limits', 'proof-verification', 'epsilon-delta', 'real-analysis']"
2927883,"Is there any function f which is only differentiable on $(0,\infty)$ and such that $f^{-1} = f'$?","Is there any function f which is only differentiable on $(0,\infty)$ and such that $f^{-1} = f'$ ? I thinks  there exists no such function. I thought about  constant, exponential, trigonometric function, etc., but I didn't find any function which is only    differentiable on $(0,\infty)$ and such that $f^{-1} = f'$ . Is it true?","['inverse-function', 'derivatives', 'real-analysis']"
2927894,"How many $10$-letter words can be formed using the $26$ letters of the alphabet if repetition is allowed, but letters are listed alphabetically","How many $10$ -letter words can be formed using the $26$ letters of the alphabet if: a) Repetition is allowed b) Repetition is not allowed c) Repetition is allowed, but letters are listed alphabetically I know that: a) is $26^{10}$ b) is $P(26, 10)$ But I'm not sure where to start with c. Initially, I thought I could subtract $10$ from $26$ since you couldn't possibly form a word alphabetically if you didn't have enough letters but since repetition is allowed you could technically have the word 'ZZZZZZZZZZ'. So that rules that thought out. Any ideas as to how I should attack this problem?","['permutations', 'combinatorics']"
2928002,What does the Riesz representation theorem say?,"$\newcommand{\R}{\mathbf R}$ $\newcommand{\C}{\mathbf C}$ I am getting confused by the various different statements of the Riesz representation theorem found on the internet.
So I want to clear up the confusion once and for all. Let $X$ be a compact metric space. $\bullet$ $M_s(X)$ denotes the set of all the finite signed Borel measures on $X$ . $\bullet$ $M_c(X)$ denotes the set of all the complex Borel measures on $X$ . $\bullet$ $M(X)$ denotes the set of all the finite (positive) Borel measures on $X$ . $\bullet$ $C(X, \R)$ denotes the space of all the real valued continuous maps on $X$ in the sup-norm topology. This is naturally a real linear space. $\bullet$ $C(X, \C)$ denotes the space of all the complex values continuous maps in the sup-norm topology. This is naturally a complex linear space. $\bullet$ $C(X, \R)^*$ denotes the space of all the real valued bounded real-linear maps with domain $C(X, \R)$ . $\bullet$ $C(X, \C)^*$ denotes the set of all the complex valued bounded complex-linear maps with $C(X, \C)$ as the domain. A member $F\in C(X, \R)^*$ or $C(X, \C)^*$ is said to be positive if $F(f)\geq 0$ whenever $f\geq 0$ . RRT1. The map $M_s(X)\to C(X, \R)^*$ defined by $\mu\mapsto (f\mapsto\int_Xf\ d\mu)$ is bijective. RRT2. The map $M(X)\to C(X, \R)^*$ defined by $\mu\mapsto (f\mapsto\int_Xf\ d\mu)$ is injective and has its image as the set of all the positive members of $C(X, \R)^*$ . RRT3 The map $M_c(X)\to C(X, \C)^*$ defined as $\mu\mapsto (f\mapsto \int_X f\ d\mu)$ is bijective. RRT4 The map $M(X)\to C(X, \C)^*$ defined by $\mu\mapsto (f\mapsto\int_X f\ d\mu)$ is injective and has its image as the set of all the positive members of $C(X, \C)^*$ . Which of the above is/are true?","['measure-theory', 'functional-analysis']"
2928023,"When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$?","The question is: When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? The most obvious solution is a linear function of the form $f(x)=ax+b$. Is this the only solution? Edit I should add that $f:\mathbb R\to\mathbb R$ to the question.","['delay-differential-equations', 'ordinary-differential-equations', 'real-analysis']"
2928075,Extensions of amenable groups,"Let $1 \to N \to G \to Q \to 1$ be an extension of (discrete) groups, where $N$ and $Q$ are amenable. Using the fixed-point theorem, I know how to show that $G$ is amenable. However, I was wondering if there is a proof which only uses the Følner property, namely $G$ is amenable if and only if: for every finite $S \subset G$ and for every $\epsilon > 0$ there exists some finite $A \subset G$ such that $|SA \Delta A| < \epsilon |A|$ . I am asking this since in some texts this is used as the actual definition of an amenable group.","['group-theory', 'abstract-algebra', 'functional-analysis', 'geometric-group-theory', 'amenability']"
2928111,Continuity of a harmonic integral,"Consider the open unit disk $\mathbb{D}$ in $\mathbb{R}^2$ . Given a continuous function $f : \partial\mathbb{D} \to \mathbb{R}$ we define $$
u(x) \overset{\texttt{def}}{=} \int_{\partial \mathbb{D}} f(y)\ln\left\vert x-y\right\vert\,\mathrm{d}S(y)
$$ for any point $x \in \overline{\mathbb{D}}$ . I have proven that this integral exists for all $x$ belonging to the closure of $\mathbb{D}$ . Using the dominated convergence theorem, I have also shown that $u$ is harmonic in $\mathbb{D}$ . In particular, $u \in C(\mathbb{D})$ . However, I have been unable to show that $u$ is continuous up to the boundary of $\mathbb{D}$ . Dominated convergence does not seem to apply here since the logarithm blows up as $x$ approaches a point on the boundary.  I suspect that I will have to use the fact that $u$ is harmonic in the open disk to prove this. Any help would be greatly appreciated.","['harmonic-functions', 'analysis', 'real-analysis']"
2928117,Theorema Egregium and coeficcients of the second fundamental form,"The Theorema Egregium says that Gaussian curvature $K$ of a regular surface $S$ is invariant under local isometries. We have a local description of the Gaussian curvature as follows $$K = \dfrac{eg-f^2}{EG-F^2}$$ where $E$ , $F$ , and $G$ are the coefficients of the first fundamental form of $S$ and $e$ , $f$ and $g$ the coefficients of the second fundamental form of $S$ . This implies that $eg-f^2$ is invariant under local isometries as well. I am following the proof of Manfredo's Differential Geometry of Curves and Surfaces for the theorema Egregium, but the end of the proof seems less natural to me that the proof of the fact that Christoffel's symbols are invariant under local isometries. My question: Is there another way of proving the quantity $eg-f^2$ depends only on Christoffel's symbols and the coefficients of the fundamental form, thus it is also invariant under local isometries? Thank you in advance. EDIT : Maybe I should add the following, it is not hard to see that, if $\chi: U \to S$ is a local parametrization compatible with some orientation $N$ on $S$ then $$eg-f^2 = \langle \chi_{u,u},\chi_{v,v} \rangle - \langle \chi_{u,v},\chi_{u,v} \rangle  + \mathrm{Christoffel's ~symbols}$$ Therefore my question reduces to showing that $\langle \chi_{u,u},\chi_{v,v} \rangle - \langle \chi_{u,v},\chi_{u,v} \rangle$ can be expressed in terms of Christoffel's symbols and the coefficients of the first fundamental form.","['riemannian-geometry', 'surfaces', 'ordinary-differential-equations']"
2928134,Is a function defined by an algorithme that must end rigourously defined?,"I wonder if it is commonly agreed by the mathematics community that a function that is describe by an algortihme that is shown to be always solvable in a finite amount of step is correctly defined. As for an example : If say I want to prove that a finite list of real numbers must have a minimum; may I state that I can sort the list by some algorithme that I would show to be correctly solvable in finite amount of step and then must that first element of the be list the minimum (and since there is a minimum then it must exist). Or rather : $\exists a / a= Min(L)$ because $Min(L) = x_0 / (\ (x_0, \dots,x_n) = Sort(L)\ )$ ? I'm sorry if my question is dumb, I'm far from a professional mathematician. Anyway thank you for your time.","['discrete-mathematics', 'algorithms']"
2928149,Rewrite each of the following sentences using logical connectives,"Rewrite each of the following sentences using logical connectives. Assume that each symbol $f, x_0, n, x, S, B$ represents some fixed object. (a) If $f$ has a relative minimum at $x_0$ and if $f$ is differentiable at $x_0$ , then $f’(x_0)=0$ a) (( $f$ has a relative minimum at $x_0$ ) $\land$ ( $f$ is differentiable at $x_0$ )) $ \implies$ $f’(x_0)=0$ (b) If $n$ is prime, then $n = 2$ or $n $ is odd. b) $n$ is prime $\implies$ (( $n = 2$ ) $\lor$ ( $n$ is odd)) (c) A number $x$ is real and not rational whenever $x$ is irrational. c) ((A number $x$ is real) $\land$ $\lnot$ (rational)) $\iff$ $x$ is irrational. (d) If $x=1$ or $x=−1$ ,then $|x| =1$ . d) (( $x=1$ ) $\lor$ ( $x=−1$ )) $\implies$ $|x| =1$ (e) $f$ has a critical point at $x_0$ iff $f’(x_0) = 0$ or $f ′(x_0)$ does not exist. e) ( $f$ has a critical point at $x_0$ ) $\iff$ (( $f’(x_0) = 0$ ) $\lor$ ( $f ′(x_0)$ does not exist)) (f) $S$ is compact iff $S$ is closed and bounded. f) $S$ is compact $\iff$ (( $S$ is closed) $\land$ (bounded)). (g) $B$ is invertible is a necessary and sufficient condition for $\det B$ not equal to $0$ . g) ( $B$ is invertible) $\iff$ $\lnot (\det B= 0)$ . (h) $6\geq n−3$ only if $n>4$ or $n>10$ . h) (( $n>4$ ) $\lor$ ( $n>10$ )) $\implies$ (( $6> n−3$ ) $\lor$ ( $6=n-3$ )) (i) $x$ is Cauchy implies $x$ is convergent. i) $x$ is Cauchy $\implies$ $x$ is convergent. (j) $f$ is continuous at $x_0$ whenever $\lim_{x\to x_0} f(x)=f(x_0)$ j) $f$ is continuous at $x_0$ $\iff$ \$lim_{x\to x_0} f(x)=f(x_0)$ (k) If $f$ is differentiable at $x_0$ and $f$ is strictly increasing at $x_0$ , then $f′(x_0) > 0$ . k) (( $f$ is differentiable at $x_0$ ) $\land$ ( $f$ is strictly increasing at $x_0$ )) $\implies$ $f′(x_0) > 0$ .","['propositional-calculus', 'logic', 'discrete-mathematics', 'logic-translation']"
2928177,A functional equation of a matrix,"How would one prove the following theorem? $p(A)$ is a nonzero polynomial of the entries of $A$ and satisfies $p(AB)=p(A)p(B)$ , for all square matrices $A$ and $B$ of complex numbers. Prove $p(A)=(\det\,A)^k$ for some nonnegative integer $k$ . My attempt: A matrix $A$ can be Shur-decomposed  into $A=QTQ^\dagger$ where $T$ is a triangular matrix and $Q$ a unitary matrix. Now $p(Q)p(Q^\dagger)=p(QQ^\dagger)=p(I)=1$ where the last equation comes from the second paragraph below. Now $p(A)=p(Q)p(T)p(Q^\dagger)=p(T)$ . If we can show $p(T)=(\Pi_i\ T_{i,i})^k$ for some integer $k$ , then we are done. Now if $T$ is diagonal with its diagonal entries all equal to $1$ except one being a complex variable $x$ , we can show $p(T)=x^k$ for some nonnegative integer $k$ . Let $p(T)=\sum_{i=0}^k a_ix^i$ for some nonnegative integer $k$ and $a_k\ne0$ . $\sum_{i=0}^k a_ix^{2i}=p(T^2)=p(T)^2=\big(\sum_{i=0}^k a_ix^i\big)^2$ . Expand the last expression and collecting coefficients of $\{x^i\}_{i=0}^k$ . By the linear independence of $\{x^i\}_{i=0}^k$ , the coefficient of the left hand side and that of the right hand side of the same order terms have to match. Considering the coefficents of the terms of order no less than $k$ , we draw the following conclusion. The coefficients of the odd order terms have to be zero. $a_k=1$ . Recursively we conclude $a_i=0,\,\forall i<k$ . As $p(T_1T_2)=p(T_1)p(T_2)$ , $p(T)=\Pi_i\ T_{i,i}$ for any diagonal matrix $T$ . But I am unable to proceed further to the triangular matrix.","['functional-equations', 'determinant', 'matrices', 'linear-algebra', 'polynomials']"
2928203,"How many integers from 1 through 1000 are divisible by 3 and by at least one of 2,5,7, and 11?","So I got 294 for this question, though I'm not a hundred percent sure but let me explain my process: I did to find all the multiples:
(floor of each) 1000/6 = 166 
1000/15 = 66
1000/21 = 47
1000/33 = 30
166+66+47+30 = 309 to eliminate the repeats:
(floor of each) 66/6 = 11 
47/6 = 7
30/6 = 5
47/15 = 3
30/15 = 2
30/21 = 1 11+7+5+3+2+1 = 29 to add in the repeats that were eliminated twice over:
(floor of each) 5/2 = 2
7/2 = 3 
11/2 = 5
7/5 = 1
11/5 = 2
11/7 = 1 2+3+5+1+2+1 = 14 309 - 29 + 14 = 294","['statistics', 'combinatorics', 'probability']"
2928249,Gagliardo-Nirenberg-Sobolev Inequality w.r.t. general probability measure,The Gagliardo-Nirenberg-Sobolev inequality states that the $L^{p*}$ norm of a function with compact support is bounded by the $L^p$ norm of its derivative. Typically $L^p$ is in the sense of the Lebesgue integral on $\mathbb{\mathbb{R}^d}$ . Can we get similar results if we consider a general probability measure $\mu$ on $\mathbb{R}^n$ in which the integration is with respect to $\mu$ instead of the Lebesgue measure?,"['partial-differential-equations', 'sobolev-spaces', 'probability-theory', 'reference-request']"
2928263,Matrix Regression for linear ODE system,"Background I have the following homogeneous ODE system as an Initial Value Problem: $$
y'=A\cdot y\quad\wedge\quad y(0)=y_0
$$ where $y\in\mathbb{R}^{N\times 1}$ is the unknown vector and $A\in\mathbb{R}^{N\times N}$ is a known, constant coefficient nonsingular, diagonalizable matrix with only $N$ independent entries. For instance, if $N=3$ it could be: $$
A=\begin{pmatrix}
-x_1 & 0 & 0\\
x_1 & -x_2 & 0\\
0 & x_2 & -x_3
\end{pmatrix}
$$ with $x_1,x_2,x_3$ known. It is possible to express a closed form solution of this system as: $$
y(t)=\sum_{i=1}^NK_i e^{\lambda_it}\cdot u_i
$$ where $\lambda_i$ is the i-th eigenvalue of $A$ and $u_i$ is the i-th eigenvector of $A$ . The $K_i$ are constants of integration such that the Initial Condition is followed. Question Suppose to have the above ODE system, with the matrix $A$ with a known structure but the entries' values $x_1,\ldots,x_N$ are unknown. Suppose also to have $t_k$ and $y(t_k)=y_k$ values for $k$ ""experiments"" with $k\gg N$ , including the initial condition state, $k=0\to t_{k=0}=0$ . Is there a way to calculate such entries values by means of a regression? If so, what kind of regression? Also we assume that $y_k \sim \mathcal{N}(0,\sigma^2)$ . Above all, I think the answer to the first question being ""yes"" since the number of unknown parameters $N$ is way less than the number of experimental data $k$ . Edit 1 - The Brute Force Approach Since the eigenbasis of $A$ is invariant under uniform scaling, meaning that for any constant $K\neq 0$ if $u_i$ is an eigenvector of $A$ (with eigenvalue $\lambda_i$ ) then $w_i=Ku_i$ is also an eigenvector (with eigenvalue $\lambda_i$ ), since: $$
Au_i=\lambda_iu_i\quad\to\quad AKu_i=\lambda_iKu_i\quad\to\quad Aw_i=\lambda_iw_i
$$ the general solution can be rewritten as a model: $$
\hat{y}(t,\lambda,W)=\sum_{i=1}^N e^{\lambda_it}\cdot w_i
$$ or, by component: $$
y_j(t) = \sum_{i=1}^N e^{\lambda_it}\cdot w_{ji}
$$ This yields $N$ equations with a total of $N+N^2$ parameters ( $N$ eigenvalues and $N^2$ eigenvector components of the matrix $W=\{w\}_{ji}$ ) and thus $k\gg N(N+1)$ is required at least. The regression is expressed as a minimization problem: $$
\min_{\lambda, W}\sum_{j=1}^{k} \left\lvert y_{j} - \hat{y} (t_j,\lambda,W)\right\rvert_2
$$ and once $\lambda$ and $W$ are known, then: $$
A=W^{-1}\mathrm{diag}(\lambda)W
$$ This process is however tedious, boring, and does not exploit the structure and properties of A. What are the improvements?","['regression', 'parameter-estimation', 'ordinary-differential-equations']"
2928272,Why does dividing a polynomial by $x-a$ give the same quotient as evaluating it at $x=a$ using synthetic division?,"I know synthetic division is a table representation of the calculations that occur when evaluating a polynomial in Horner's form at some $x$ value ( $x=a$ ). For example, if there's some polynomial in standard form say $x^3 + 4x^2 -5x + 5$ it can be transformed into Horner's form through successive groupings and factoring out $x$ . $x^3 + 4x^2 -5x + 5$ $=(x^2+4x-5)x+5$ $=((x+4)x-5)x+5$ If this polynomial is evaluated at $x=3$ the result is $p(3) =(((3)+4)(3)-5)(3)+5$ $=((7)(3)-5)(3)+5$ $=(21-5)(3)+5$ $=(16)(3)+5$ $=48+5$ $=53$ Which is also what we get using synthetic division Interestingly, synthetic division also gives the same quotient as dividing $x^3 + 4x^2 -5x + 5$ by $x-3$ Which of course is $x^2 + 7x + 16 +\frac{53}{x-3}$ My question are: 1) Why would dividing a polynomial by $x-a$ (using long division) give the same quotient as evaluating it at $x = a$ when using synthetic division? 2) Furthermore why are only the coefficients and $a$ term used in synthetic division? What happened to the variables and their respective powers? They seem to ""disappear"" from the calculation when using synthetic division yet the same quotient is derived just as using long division.","['algebra-precalculus', 'polynomials']"
2928286,Explain carefully why the equation $3x=2$ has no solution in $\mathbb{Z}$.,"My proof is below, but I am not sure if this is ""carefully"" enough. I am sure there are many better proofs out there, but this is the one that came to my mind first. Does this work? Let us assume, for the sake of contradiction, that $3x=2$ has a solution in $\mathbb{Z}$ . This would then imply that $x$ is an element in $Z$ , the set of integers. We can calculate $x$ as follows: $3x=2\Rightarrow \frac{3x}{3}=\frac{2}{3}\Rightarrow x=\frac{2}{3}$ . Any integer can be written as the fraction $z=\frac{a}{b}$ where $a$ and $b$ are any real numbers and, importantly, $a=n\cdot b$ where $n\in\mathbb{Z}$ . This means the numerator must be a multiple of the denominator. There are infinitely many different possibilities for $a,b$ for any $z\in\mathbb{Z}$ . However, it is impossible for this to be true for $\frac{2}{3}$ as the numerator must be strictly less than the denominator. Therefore we have reached a contradiction and $3x=2$ can not have a solution in $\mathbb{Z}$ .","['analysis', 'real-analysis']"
2928319,Seating arrangements: Question about the book solution and summation indices,"$20$ people are to be seated at seven tables, three of which have 4 seats and four of which have 2 seats. If the people are randomly seated, find the expected value of the number of married couples that are seated at the same table. My question 1: In the book solution (see screenshot below) I don't understand the last equality, specifically the indexes that go from $i=1$ to $22$ and $19$ respectively. It seems they should both go from $i=1$ to $10$ . My question 2: Where am I going wrong in my approach? Let $X$ be the number of married couples sitting at the same table. Let $X_i = 1$ if couple $i$ is sitting at the same table for $i=1,...,10$ . Then $$E[X] = E\left[\sum_{i=1}^{10} X_i \right] = \sum_{i=1}^{10} E\left[X_i \right] = 10 \cdot P(X_1 = 1)$$ where the last equality comes from LOE and symmetry. To find $P(X_1 = 1)$ I will condition on the event $A =$ the husband is at a table with $4$ seats and the event $B =$ the husband is at a table with $2$ seats. $$P(X_1 = 1) = P(X_1 = 1 \mid A)P(A) + P(X_1 = 1 \mid B)P(B)$$ $$=\frac{3}{19}\frac{12}{20}+\frac{1}{19}\frac{8}{20} \approx .1157$$ and so $E[X] \approx 1.157$ which does not match the book solution of $2.48$ . Where am I going off the rails? Thanks for your help and patience. Book solution","['expected-value', 'combinatorics', 'conditional-expectation', 'probability']"
2928327,Change of basis of a linear map defined by non-square matrix,"The given: Let a linear map $L : V → U$ be given in the basis $(e_1, e_2, e_3)$ of $V$ and in the basis $(f_1, f_2)$ of $U$ by $\begin{pmatrix} 0 & 1 & 2 \\ 
3 & 4 & 5 \end{pmatrix}$ .  Find the matrix of $L$ with respect to the bases $(e_1, e_1 + e_2, e_1 + e_2 + e_3)$ and $(f_1, f_1 + f_2)$ . Now I know I am being stupid in some way, but I can't make this work. I want to say: we have new bases $(e_1,e_1+e_2,e_1+e_2+e_3)$ and $(f_1,f_1+f_2)$ which correspond to $C = \begin{pmatrix}
  1 & 1 & 1  \\
  0 & 1 & 1  \\
  0 & 0 & 1
\end{pmatrix}$ and $D = \begin{pmatrix}
  1 & 1  \\
  0 & 1   
\end{pmatrix}$ respectively. And Using our change of bases formula, one version of $L_1' = C^TAC$ and the other is $L_2' = D^TAD$ . But here I run into a problem of dimension, the matrix multiplication does not work. I had thought perhaps to try $L'=DAC$ , since this expression has workable dimension, but that's my only reason for trying it. The basis change examples I've seen before involve expressions with one matrix and either its transpose or inverse. Apologies again for my stupidity here, and thanks very much in advance for any assistance. Edit : Initial problem had typo on $D$ , one-zero were swapped incorrectly.","['matrices', 'change-of-basis', 'linear-algebra', 'linear-transformations']"
2928350,Definition of Disjoint Union Spaces,"The following is the definition of disjoint union spaces in John Lee's ""Introduction to Topological Manifolds"": Let $(X_{\alpha})_{\alpha\in A}$ be an indexed family of nonempty topological spaces. We define the disjoint union topology on $\amalg_{\alpha\in A}X_{\alpha}$ by declaring a subset of the disjoint union to be open if and only if its intersection with each set $X_{\alpha}$ (considered as a subset of the disjoint union) is open in $X_{\alpha}$ . With this topology, $\amalg_{\alpha\in A}X_{\alpha}$ is called a disjoint union space . In this definition, what does it mean for a set to be open in $X_{\alpha}$ ? Here the disjoint union $\amalg_{\alpha\in A}X_{\alpha}$ is defined by $$\amalg_{\alpha\in A}X_{\alpha}=\{(x,\alpha):\alpha\in A, x\in X_{\alpha}\},$$ and by ""considered as a subset of the disjoint union"", he means that he is identifying the set $X_{\alpha}$ with the set $$\{(x,\alpha): x\in X_{\alpha}\}.$$",['general-topology']
2928364,Recursion in probability,"A biased coin shows head with a probability of $$\frac{3}{4}$$ and tails with a probability of $$\frac {1}{4}$$ Let $p_{n}$ denote the probability that no three or more heads appear consecutively in $n$ tosses of the coin. If $$p_{n}=\alpha\, p_{n-1}+\beta\,p_{n-2}+\gamma\,p_{n-3}$$ . Find values of $\alpha,\beta$ and $\gamma$ . My Attempt: I am able to understand the case when $n$ th toss is a tail in which situation three or more consecutive heads should not have occurred in previous $(n-1)$ tosses with probability $p_{n-1}$ . So, $\alpha$ equals $\frac{1}{4}$ . I am not able to relate with the other two cases.","['probability-theory', 'probability']"
2928366,"How do you construct the Frey curve for (2,3,p)?","In Darmon's paper on p.14 he lists a table of signatures $(p,q,r)$ and constructed Frey curves.  How do you construct the Frey curve he gives for $(2,3,p)$ ? The curve he gives for this signature is: $y^2=x^3+3bx+2a$ The Frey curve that Poonen,Schaefer,and Stoll give on p.9 of their paper for the signature $(2,3,7)$ is: $y^2=x^3+3bx-2a$ My construction question extends to their curve also.  How are these curves constructed?","['number-theory', 'modular-arithmetic', 'elliptic-curves', 'diophantine-equations']"
2928367,Why isn't this approach in solving $x^2+x+1=0$ valid?,"There is this question in which the real roots of the quadratic equation have to be found: $x^2 + x + 1 = 0$ To approach this problem, one can see that $x \neq 0$ because: $(0)^2 + (0) + 1 = 0$ $1 \neq 0$ Therefore, it is legal to divide each term by $x$ : $x + 1 + \frac{1}{x} = 0$ $x = -1 - \frac{1}{x}$ Now, substitute $x$ into the original equation and solve: $x^2 + (-1-\frac{1}{x}) + 1 = 0$ $x^2-\frac{1}{x} = 0$ $x^3 = 1$ $x = 1$ to get $x = 1$ . Clearly this isn't the right answer. But why? Thanks.","['proof-explanation', 'algebra-precalculus', 'proof-verification', 'proof-writing']"
2928369,Difficulty Finding $A^k$,"Let $A=
\begin{bmatrix}
1& -1 & 1\\ 
0 & 1 & 1 \\ 
0 & 0 &  1\\
\end{bmatrix}$ . Compute $A^k$ . My attempt I'm trying to compute $A^k$ using this approach as follows: $$ 
A=I+N=
\begin{bmatrix}
1& 0 & 0\\ 
0 & 1 & 0 \\ 
0 & 0 &  1
\end{bmatrix}+
\begin{bmatrix}
0& -1 & 1\\ 
0 & 0 & 1 \\ 
0 & 0 &  0\\
\end{bmatrix}
$$ with $$
N^2=
\begin{bmatrix}
0& 0 & -1\\ 
0 & 0 & 0 \\ 
0 & 0 &  0\\
\end{bmatrix}, \, \text{and} \, \,
N^3=
\begin{bmatrix}
0& 0 & 0\\ 
0 & 0 & 0 \\ 
0 & 0 &  0\\
\end{bmatrix}
$$ Then, $$
A^2=(I+N)^2=I+2N+N^2, \\
A^3=(I+N)^3=I+3N+3N^2, \\
A^4=(I+N)^4=I+4N+6N^2, \\
A^5=(I+N)^5=I+5N+10N^2, \\
A^6=(I+N)^5=I+6N+15N^2,
$$ By induction, we can see $A^k=(I+N)^k=I+kN+f[k]N^2$ . But, I couldn't figure out what $f[k]$ is. Any help?","['matrices', 'linear-algebra']"
2928373,Proof verification of $\lim_{x\to-2^+}\frac{x^2+x-2}{\sqrt{x+2}}=0$ using $\epsilon$-$\delta$,"I am trying to prove that $$\lim_{x\to-2^+}\frac{x^2+x-2}{\sqrt{x+2}}=0$$ and this is my approach. Let $\epsilon>0,\delta>0\,$ so that $$\left|\frac{x^2+x-2}{\sqrt{x+2}}\right|<\epsilon\quad\text{whenever}\quad-2<x<-2+\delta$$ We have that $$\left|\frac{x^2+x-2}{\sqrt{x+2}}\right|=|x-1|\sqrt{x+2}$$ Let $\delta<1$ $$-2<x<-2+\delta<-1\implies-3<x-1<-2\implies2<|x-1|<3$$ so $$|x-1|\sqrt{x+2}\leq3\sqrt{x+2}<\epsilon$$ if we pick $\,\delta\leq\min\{1,\epsilon^2/9\}.$ Would this be correct?","['limits', 'calculus', 'proof-verification', 'epsilon-delta']"
2928380,"Prove $\frac{\sqrt{x}}{x-1}$ is continuous on (0,1)","Use the epsilon-delta definition to prove that $$
\dfrac{\sqrt{x}}{x-1} 
$$ is continuous on the interval $(0,1)$ . I've tried a lot but I just can't seem to see the first step. I don't want the answer just a hint to get me rolling. I know it should be continuous because I looked at the graph, but after $$
|\dfrac{\sqrt{x}}{x-1} - \dfrac{\sqrt{x_0}}{x_0-1}| < \varepsilon
$$ I get stuck. I've attempted bringing everything to a common denominator. Subtracting the $x_0$ term to the other side, but to no avail. Any help appreciated","['epsilon-delta', 'analysis', 'real-analysis', 'continuity', 'limits']"
2928412,Evaluating $\sum_{n=1}^{\infty} \frac{sin(nz)}{2^n}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I managed to show that: $$\sum_{z=1}^{\infty} \frac{sin(nz)}{2^n}$$ is analytic on $\{z \in \mathbb{C}| -log(2)<Im(Z)<log(2)\}$ I am assuming that is correct? I am now stuck as to how to evaluate the series on that set. How would I get started? I have so far re-written: $$\sum_{z=1}^{\infty} \frac{sin(nz)}{2^n} = \frac{1}{2i}[\sum \frac{e^{inz}}{2^n} - \sum \frac{e^{-inz}}{2^n}]$$ Thank you!","['complex-analysis', 'power-series']"
2928432,"Solve the integral using beta functions : $\int_0^1 \frac{(1-x^4)^{3/4}}{(1+x^4)^2}\,dx$","I was solving questions on beta and gamma functions and then I came across this question. $$ \int_0^1 \frac{(1-x^4)^{3/4}}{(1+x^4)^2}\,dx $$ Generally in questions of beta functions integrals of the type $\int x^n(1−x^m) \, dx$ can be solved by substituting $x^m = t$ , but in this case substitution is unconventional because if I put $1+x^4=t$ , then I will not get the required form of beta function I tried substituting $x^2 = \tan t$ and also tried substituting $x^2 = \cos t$ but on simplifying each of them further I could not reduce them to standard form of beta function.","['integration', 'beta-function', 'calculus', 'definite-integrals']"
2928498,How to graph $f(x)=(1-\cos(2x-2))/(x-1)^2$ analytically?,"I am asked to graph the function $$f(x)=\frac{1-\cos(2x-2)}{(x-1)^2}$$ I only have an issue when it comes to determining the hole at $x=1$ analytically. Graphing the function on a graphing utility reveals a hole at $x=1$ rather than a vertical asymptote. I know that by simply plugging in the value $1$ for $x$ , one will see that both the numerator and denominator equate to zero. This shows that there is at least one factor of $(x-1)$ that can be extracted from the numerator. I would like to know how to simplify this function analytically in such a way that two factors of $(x-1)$ can be obtained from the numerator. I believe one must make use of an inverse function down the line in order to extract the obvious $(x-1)$ hiding in the $\cos(2x-2)=\cos(2(x-1))$ term. That is as far as I got in the problem. Thank you for your help.","['trigonometry', 'graphing-functions', 'rational-functions']"
2928520,Find the Inverse of an Infinite Square Matrix,For my mathematics assignment I am using polynomial interpolation to solve certain problems and I end up with the following scenario: $\begin{bmatrix}... & 0 &0 & 0 & 1\\... &1^3 & 1^2 & 1 & 1\\ ... &2^3 & 2^2 & 2 & 1\\... &3^3 & 3^2 & 3 & 1\\ \unicode{x22F0} & \vdots & \vdots & \vdots & \vdots\end{bmatrix}^{-1} \unicode{x22c5} \begin{bmatrix}a\\b\\c\\d\\ \vdots \end{bmatrix}$ Where the value in the rows of the produced single-column matrix correspond to the coefficients of an infinite-polynomial with decreasing powers. I was wondering is there was a way to either invert this infinite matrix or find what happens to the polynomial as I increase the size of the matrices used (e.g. see if it approaches another function's taylor or power series),"['matrices', 'infinite-matrices', 'polynomials']"
2928580,"State whether the functions are injective, surjective, or bijective. [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question In the book titled: Discrete Mathematical Structures, by Tremblay; it is stated as Q.#1 in Ex. 2-4.3: Let $f:\mathbb{R}\to \mathbb{R}$ and $g:\mathbb{R}\to \mathbb{R}$ , where $\mathbb{R}$ is the set of real numbers. Find $f\circ g$ and $g\circ f$ , where $f(x)= x^2 -2$ and $g(x) = x+4$ . State whether these functions are injective, surjective, or bijective. $f\circ g = x^2 +8x +14$ , $g\circ f=x^2 +2$ By my earlier post , have gathered that as an even order function will not be having a unique x-value for each y-value so none is injective. But, am  still not sure about disproving surjectivity. My approach is not based on calculus (that am still unable to understand, as stated on mse here ); but is based on disproof by taking cases where failure occurs. The minimum value taken up by $f\circ g = x^2 +8x +14$ is $14$ . Similarly, the minimum value taken up by $g\circ f = x^2 +2$ is $2$ . Hence, the entire range is not mapped. Have a further request to make me understand by the calculus approach. Update In face of comment by @Arthur, the correct minimum value for $f\circ g = x^2 +8x +14$ is given for $x=-4$ , i.e. $-2$ . This is found by simply differentiating wrt $x$ & equating to $0$ to find the minima point.","['functions', 'polynomials']"
2928605,Action of group and structure of group,"In group theory, the action of a group is helpful to determine structure of group. As an example, If $G$ is of order $p^aq^b$ then $G$ is solvable. [Action considered on vector space.] If $G$ is a $p$ -group then its center is non-trivial. [Action is on non-identity elements by conjugation.] If $|G|=2m$ , $m$ odd, then $G$ contains normal subgroup of order $m$ [Left action of $G$ on itself.] Beside this, what are interesting theorems which tell structure of group from action of it on some objects (other than vector space and above mentioned type)? In fact, action of group on vector space has lot of applications to find structure of group. This is in representation theory of finite groups. In above question, I want to see examples from different (category of) objects for action of group, to determine its structure. I will be happy to see if something is derived about group by its action on some topological space, some graph, or on complex domain etc.","['group-theory', 'finite-groups']"
2928624,Possible discrepancy between two forms of the derivative of $|x|^{3/2}$?,"I've come across what seems to be a discrepancy between two different ways of representing the derivative of the function $f:\mathbb{R}\to\mathbb{R}$ defined by $f(x)=|x|^{3/2}$ . I started by using the definition of the absolute value function: $$
f(x) = |x|^{3/2} =
\begin{cases}
x^{3/2}, & x\geq 0 \\
(-x)^{3/2}, & x<0 \\
\end{cases}.
$$ Then using the chain rule, we have $$
f'(x) = 
\begin{cases}
\frac{3}{2}x^{1/2}, & x>0 \\
-\frac{3}{2}(-x)^{1/2}, & x<0 \\
\end{cases}.
$$ Based on the graph of the function, it seemed reasonable to check if the derivative existed at zero. Thus I made the following computations: \begin{equation*}
\begin{split}
f_+'(0)&=\lim_{h\to 0^+}\frac{f(h)-f(0)}{h} = \lim_{h\to 0^+}\frac{h^{3/2}}{h}=\lim_{h\to 0^+}h^{1/2}=0 \\
f_-'(0)&=\lim_{h\to 0^-}\frac{f(h)-f(0)}{h} = \lim_{h\to 0^-}\frac{(-h)^{3/2}}{h}=-\lim_{h\to 0^-}\frac{(-h)^{3/2}}{(-h)}=-\lim_{h\to 0^-}(-h)^{1/2}=0.
\end{split}
\end{equation*} From this, it seems that the derivative exists at $x=0$ and that $f'(0)=0$ . However, if we find the derivative using the chain rule and the fact that we can write the derivative of the absolute value function as $$
\frac{d}{dx}\left[\,|x|\,\right ]=\frac{x}{|x|},
$$ we obtain $$
f'(x)=\frac{3}{2}|x|^{1/2}\cdot \frac{x}{|x|} = \frac{3x}{2\sqrt{|x|}},
$$ which should be undefined at $x=0$ . So the question is, should the derivative be defined at $0$ or should it not? If so, is this just an issue with the notation we use for the derivative of the absolute value function or a sign error or something? Thanks for any input.","['calculus', 'derivatives', 'algebra-precalculus']"
2928660,"Solution to $u_x + y u_y = -u$ with $u(0, y) = \cos y$ [duplicate]","This question already has answers here : Transport equation $u_t + xu_x + u = 0$ with $u(x_0, 0) = \cos(x_0)$ (3 answers) Closed 5 years ago . I am studying for my PDEs test and I want to make sure I can solve this type of equations. I used the method of characteristics. $$\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{b}{a} = y$$ Integrating I get $$C = ye^{-x}$$ where $C$ is a constant. I can make the substitutions $$\xi = x$$ $$\eta = ye^{-x}$$ $$u(x,y) = w(\xi,\eta)$$ Substituting in the PDE, in terms of $w, \eta, \xi$ it becomes $$\frac{\mathrm{d}w}{\mathrm{d}\xi} + w = 0$$ I used the Integrating factor method to solve this ODE, which gives $$w = f(\eta) e ^{-\xi}$$ If we substitute back to $u, x, y$ this becomes $$w(\xi,\eta)=u(x,y)=f(ye^{-x})e^{-x}$$ The initial conditions give $$u(0,y)=\cos y =f(y)$$ Thus, the final solution is $$u(x,y) = \cos (ye^{-x})e^{-x}$$ Is this correct? Thanks in advance.","['ordinary-differential-equations', 'partial-differential-equations']"
2928712,Number of elements in polynomial of degree n and m variables,"How many unique terms does a polynomial of degree $n$ and $m$ variables have? For instance, a polynomial of degree 1 and $m$ variables has $m$ terms: $$ f(x_1,...,x_m) = \sum_{i=1}^m a_ix_i $$ Similarly, a polynomial of degree $n$ and 1 variable has $n$ terms: $$ f(x) = \sum_{i=1}^n a_ix^i $$ A polynomial of degree 2 and 2 (3) variables has 5 (9) terms. A polynomial of degree 3 and 2 variables has 9 terms. I've tried to find a pattern so that the above can be formalised using combinatorics-related functions, but so far have utterly failed. Any hints? PS: naturally, we have to add 1 to all the above, to consider the trivial case of a constant term.",['combinatorics']
2928743,"What's the probability of 2 credit card numbers matching, given incomplete data and known expiry date?","Trying to settle an office argument on a challenging probability scenario, thought I'd see if anyone would like to take a stab: We're trying to determine the probability of 2 credit cards matching amongst a global population given that only the first six and last four digits are known. We also know the expiry date. A couple of rules for each field: First six:
We're working strictly with a single card issuer who's range of first six digits is between 222100-272099 or 510000-559999. For simplicity this allows for 100,000 possible combinations. Last four:
These for simplicity can be assumed to be completely random. (ignoring Luhn checks) Expiry date:
Only valid future dates within a four year time frame. What's the probability of 2 cards in the entire population being the same? So far our best theory by breaking this down into parts is: A = There are 100,000 combinations of the first 6 digits. B = There are 10,000 combinations of the last 4 digits C = There are 48 combinations of the date field. We are looking for the number of clashes in the population. For simplicity we've assumed a population of 1 billion. So we are assuming ultimately that there will be 1 billion events and are looking for the probability of getting 2 identical outcomes from those events: A * B * C = 1,000,000,000x
1/100,000 * 1/10,000 * 1/48 = 1,000,000,000x
x = 1/48 So the theory is that there is a 1/48 chance of there being a single clash in 1bn cards.",['probability']
2928820,How to calculate the bounding square of an ellipse?,I'm trying to find the formula of a bounding square of an ellipse. So in other words: I want a formula that gives the size (length of one side of a the square) that bounds a given ellipse with width $a$ and length $b$ . I know that the formula of an ellipse is given by: $$\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$$ but I just cant figure out what the length of one side of a bounding square is.,"['calculus', 'conic-sections', 'geometry']"
2928866,Limits and While Loops,"Question : Consider the following program. Does $f(1)=\infty$ ? \begin{align*}
f(i):=&|\text{while } \frac{1}{i}>0\\
&||i\leftarrow i+1\\
&|i
\end{align*} I would say that $f(1)=\infty$ is a true statement. The program does not terminate, but one could consider the sequence of points $\{x_i\}_{i\in \mathbb{N}}$ given by $x_i:=\frac{1}{i}$ , so that $\{x_i\}_{i\in \mathbb{N}}$ converges to the limit $0$ which makes $\underbrace{\frac{1}{i}}_{=0}>0$ false which means $f(1)=\infty$ . However, I could be wrong.","['limits', 'programming']"
2928901,Need to find matrix formulation,"I have a $B$ matrix: $B = B_{ij}$ I need to find closed matrix formulation of: $$\sum_i \sum_j \sum_m \sum_n  B_{ij} B_{jm} B_{mn} B_{ni}$$ But I am so confused! Edit by Henrik: Originally, it was asked to express $$\sum_i \sum_j \sum_m \sum_n  B_{mi} B_{mj} B_{ni} B_{nj}$$ in terms of matrices. actually, there is also a condition: i is not equal to j.",['matrices']
2928930,"Geometric meaning of the quantity $|a|^2 |b|^2 |c|^2 - (a \cdot b)(b \cdot c)(c \cdot a)$ for non-coplanar vectors $a$, $b$, $c$","For two non-collinear vectors, $a$ and $b$ , the quantity $$|a|^2 |b|^2 - (a \cdot b)(b \cdot a) = |a \times b|^2$$ is the square of the area of the parallelogram spanned by these two vectors. For three non-coplanar vectors, $a$ , $b$ and $c$ , we can form a similar expression $$B = |a|^2 |b|^2 |c|^2 - (a \cdot b) (b \cdot c) (c \cdot a)$$ which is not equal to the square of the volume of the parallelepiped spanned by these three vectors. Does $B$ have any geometrical (or other) meaning?","['inner-products', 'vectors', 'linear-algebra', 'geometry']"
2928965,Complex Square root bound,"Under what conditions on $a, b$ and the branch cut of the square root does the following hold: $$
|\sqrt{a + b} - \sqrt{a} | \leq C \frac{|b|}{\sqrt{|a| + |b|}}
$$ for a universal constant $C > 0$ .  This seems to be true in the real case, but I'm not familiar enough with the subtleties in the complex plane to generalize this.","['complex-analysis', 'inequality', 'analysis']"
2928971,Tail probability of sum of order statistics of distance from point to a set,"Let $P$ be a distribution on a metric space $(\mathcal X, d)$ . For a point $x \in \mathcal X$ and a Borel $B \subseteq \mathcal X$ , let $d(x,B) := \inf_{y \in B}d(x,y)$ be the distance of $x$ from $B$ . Finally let $x_1,\ldots,x_N$ be an iid sample from $P$ and let $d(x_{(k)}, B)$ be the $k$ th order $k$ smallest element of set $\{d(x_1,B),d(x_2,B),\ldots,d(x_N,B)\}$ , and for $M \le N$ , define $$S_M := \dfrac{1}{N}\sum_{i=1}^M d(x_{(i)},B).
$$ By this post , it follows that if $t \mapsto F(t) := P(d(x_1,B) \le t)$ is the common distribution function of the $x_i$ 's, then the distribution function of $d(x_{(k)},B)$ is given byt $$F_k(t) = P(d(x_{(k)}, B) \le t/N) = \sum_{i=0}^{k-1} {N \choose k} F(t)^k  (1-F(t))^{N-i}
$$ and expected value of $S_M$ is $$
\mathbb E_P [S_M] = \frac{1}{N}\sum_{k=1}^M\int_0^\infty F_k(t)dt = \dfrac{1}{N}\int_{0}^\infty \sum_{k=1}^{M} (M-k){N \choose k}F(t)^k (1-F(t))^{N-k} dt
$$ Question What can be said about the tail probabilities $P(S_M > t)$ ? N.B.: I'm fine with assuming that $P$ satisfies a transportation-cost inequality for the Wasserstein distances $W_1$ or $W_2$ .","['distribution-tails', 'central-limit-theorem', 'probability-theory', 'concentration-of-measure']"
2929005,Stuck in solving the Hermite DE?,"As a part of solving the Schrödinger equation 1 for quantum Harmonic oscillator: \begin{equation}\tag{1}
	\left(-\frac{d^2}{d\tilde{x}^2} + \tilde{x}^2\right) \tilde{\psi}(\tilde{x}) = \tilde{E} \tilde{\psi}(\tilde{x}) \iff \frac{d^2 \tilde{\psi}(\tilde{x}) }{d\tilde{x}^2} = (\tilde{x}^2 - \tilde{E}) \tilde{\psi}(\tilde{x}) 
\end{equation} I'm looking for a solution of the form: \begin{equation}
	\tilde{\psi}(\tilde{x}) = \tilde{x}^{k} e^{-\frac{\tilde{x}^2}{2}} = h(\tilde{x})e^{-\frac{\tilde{x}^2}{2}}
\end{equation} which, when substituted in (1) leads to the Hermite differential equation: \begin{equation}
	\frac{d^{2}h}{d\tilde{x}^2} - 2\tilde{x}\frac{dh}{d\tilde{x}} + (\tilde{E} - 1)h = 0
\end{equation} I'm trying to construct a power series solution $h = \sum_{k = 0}^{\infty} a_{k} \tilde{x}^{k}$ . Substituting back in the equation: \begin{equation}
	\sum_{k = 2}^{\infty} k (k - 1) a_{k} \tilde{x}^{k-2} - 2\tilde{x}\sum_{k = 1}^{\infty} k a_{k} \tilde{x}^{k-1} +  (\tilde{E} - 1)\sum_{k = 0}^{\infty} a_{k} \tilde{x}^{k} = 0
\end{equation} After a shift on the first sum and peeling of the initial term of the first and the last sum: \begin{equation}
	2a_{0} + \sum_{k = 1}^{\infty} (k + 2)(k + 1) a_{k+2} \tilde{x}^{k} - 2\sum_{k = 1}^{\infty} k a_{k} \tilde{x}^{k} + a_{0}(\tilde{E} - 1) + (\tilde{E} - 1)\sum_{k = 1}^{\infty} a_{k} \tilde{x}^{k} = 0
\end{equation} Powers and indexes match, so: \begin{equation}
	2a_{0} + a_{0}(\tilde{E} - 1) + \sum_{k = 1}^{\infty}[(k + 2)(k + 1) a_{k+2} - (2k - \tilde{E} + 1) a_{k} ]\tilde{x}^{k} = 0
\end{equation} Finally, all the coefficients must be equal to zero, so: \begin{equation}
a_{k+2} = \frac{(2k - \tilde{E} + 1)}{(k + 2)(k + 1)} a_{k}
\end{equation} First (smaller) problem. From the above, $\tilde{E} = 2k + 1$ . But from the peeled of terms $a_{0}(\tilde{E} + 1) = 0$ . What should I do with the second relation? Second (main) problem. I expect that by solving the recurrence relation I should somehow deduce that the solution, $h(\tilde{x})$ , are the Hermite polynomials, however, I have no idea how to do that. Could you please suggest how to do that? 1.The unit-free time-independent 1D.","['power-series', 'recurrence-relations', 'ordinary-differential-equations']"
2929019,show $nCk= \frac{n}{n-k}\cdot(n-1)C(k) $,It's a formula getting from a recursion programming but i cannot show that $$nCk= \frac{n}{n-k}\cdot(n-1)C(k) $$,"['combinations', 'combinatorics', 'discrete-mathematics']"
2929024,Is $\cot^{-1}(-\cot x)$ equal to $-x$?,I was just wondering: We know that $\cot ^{-1}(\cot x) = x$ . But what is the algebraic value of $\cot^{-1}(-\cot x)$ ?  Is it $-x$ ? I am just getting a little muddled up with my trigonometric identities. I would appreciate some clarification.,['trigonometry']
2929035,Root-$n$ consistency of an estimator implies its variance is $O(n^{-1})$,"(This is from a claim on page 20 of Hall's 1992 The Bootstrap and the Edgeworth Expansion.) Suppose we observe i.i.d. data $X_1,\ldots,X_n$ from some unknown CDF $F$ . Let $\theta_0=\theta(F)$ be a parameter that can be computed from the CDF and $\hat\theta$ an estimator. Hall then mentions: ""should $\hat\theta$ be $\sqrt{n}$ -consistent for $\theta_0$ , so that $E(\hat\theta-\theta_0)^2=O(n^{-1})$ ..."" Why is this true? Could you please in particular provide a rigorous argument (you can make further assumptions if necessary)? My interest in this study is to see how such an argument can be constructed. My progress so far is that I understand the claim intuitively, as follows: the $\sqrt{n}$ -consistency of $\hat\theta$ means there is some fixed positive number $V$ such that $$
\sqrt{n}(\hat\theta-\theta_0)\approx N(0,V)\implies E[(\hat\theta-\theta_0)^2]\approx\frac{1}{n}V=O(n^{-1}).
$$","['statistics', 'proof-writing', 'probability-theory', 'asymptotics']"
2929094,Derivative of a squared definite integral,"Differentiation of $\int_{a(x)}^{b(x)} f(x,t)\,\text{d}t$ is done by Leibniz's integral rule: $$\frac{\text{d}}{\text{d}x} \left (\int_{a(x)}^{b(x)} f(x,t)\,\text{d}t \right )= f\big(x,b(x)\big)\cdot \frac{\text{d}}{\text{d}x} b(x) - f\big(x,a(x)\big)\cdot \frac{\text{d}}{\text{d}x} a(x) + \int_{a(x)}^{b(x)}\frac{\partial}{\partial x} f(x,t) \,\text{d}t,$$ if $-\infty<a(x),b(x)<\infty$ . Can we say anything in general about the derivative of the powers of the $\int_{a(x)}^{b(x)} f(x,t)\,\text{d}t$ in a similar fashion? So for example $$\frac{\text{d}}{\text{d}x} \left(\left(\int_{a(x)}^{b(x)} f(x,t)\,\text{d}t\right)^2\right)=~?$$","['integration', 'multivariable-calculus', 'definite-integrals', 'derivatives']"
2929109,Question on group action of $\mathbb{Z}/6\mathbb{Z}$ on a regular hexagon,"I was working with $\mathbb{Z}/6\mathbb{Z}$ acting on a regular hexagon where $\bar{n}$ acts as rotation by $2n\pi/3,$ clockwise. By labeling the vertices (I started with $1$ at the top left corner going clockwise) of the hexagon $1,2,3,4,5,6$ we have the homomorphism $\alpha: \mathbb{Z}/6\mathbb{Z} \rightarrow S_6$ such that $$\bar{0},\bar{3}\mapsto (1)$$ $$\bar{1},\bar{4}\mapsto (1\:2\:3\:4\:5\:6)$$ $$\bar{2},\bar{5}\mapsto (1\:3\:5)(\:2\:4\:6)$$ Now, let $O$ be the set of opposite vertices i.e. $O=\{\{1,4\},\{2,5\},\{3,6\}\}.$ Then we have the homomorphism $\beta: \mathbb{Z}/6\mathbb{Z} \rightarrow S_O$ such that $$\bar{0},\bar{3}\mapsto\{\{1,4\},\{2,5\},\{3,6\}\}$$ $$\bar{1},\bar{4}\mapsto\{\{2,5\},\{3,6\},\{1,4\}\}$$ $$\bar{2},\bar{5}\mapsto\{\{3,6\},\{1,4\},\{2,5\}\}$$ Is this correct? To clear any ambiguity the following is how I labeled my hexagon:","['group-actions', 'group-theory', 'abstract-algebra', 'proof-verification']"
2929127,Combinatorics - arrangements around circular table,""" $4$ boys and $3$ girls sit round a table. What is the probability that exactly $2$ girls will sit next to each other?"" I said the following: there are ${7\choose3} = 35$ ways to pick places for the girls, and in $5$ of those ways, the girls will be completely apart (i.e. no two girls sit together), giving a probability of $\frac{5}{35} = \frac{1}{7}$ . Thus the probability that at least two girls sit together is $1-\frac{1}{7} = \frac{6}{7}$ . The probability that all three girls sit together is $\frac{7}{35} = \frac{1}{5}$ , since there are $7$ ways to pick the three places such that all three girls sit together. Hence, the probability that exactly two girls sit together is $\frac{6}{7} - \frac{1}{5} = \frac{23}{35}$ . However, I'm told that the correct answer should be $\frac{3}{5}$ , so my question is: what is wrong with my working above?","['combinatorics', 'probability']"
2929302,Why are these two summations in the calculation of the variance equal?,"Background: The screenshot below is the book solution to a 1st year probability question (Sheldon Ross self test 7.12). I understand everything except the last equality. My Question: Is the red box equal to the blue box below? Can you show me step by step how to do it? I'm guessing there's some identity that will make it easier... My Attempt: Unfortunately in lieu of being able to solve the question using math I put it in python and got different results for the red and blue box... but my script could be wrong. Thanks for your help. n = 5

redBox = 0
for i in range(1,n):
    for j in range(i+1,n+1):
        redBox += (i-1)*(j-n)
redBox = 2*redBox / ((n-2)**2 * (n-1))

blueBox = 0
for i in range(1,n):
    blueBox += (i-1)*(n-i)*(n-i-1)
blueBox = -blueBox / ((n-2)*(n-1)**2)

print(redBox,blueBox) and for $n=5$ I get $\text{red box} = -0.277$ vs $\text{blue box} = -0.20833$ . Thanks. Book solution","['algebra-precalculus', 'probability']"
2929307,how to prove $f'(x) \leq \frac{f(x) - f(y)}{x - y} \leq f'(y) $,"$f(x) = x - \arctan(\ln(x)) $ on the interval $[0,+\infty[$ .
How to use  the Mean value theorem to show that $f'(x) \leq \frac{f(x) - f(y)}{x - y} \leq f'(y)  $ , I know that according to theorem , $\exists c \in  ]x,y[, f'(c) = \frac{f(y) - f(x)}{y-x} $ $ \frac{1}{1+c^2}= \frac{f(y) - f(x)}{y -x} $ All help is appreciated Thanks in advance :)","['calculus', 'trigonometry', 'inequality']"
2929365,Function transformation: shrink horizontally,"Write the formula for $f(x)$ , if the graph of $f$ can be obtained from the graph of $y = g(x)$ by shrink horizontally by a factor of $5$ then shift left $3$ units
The equation should be $f(x) = g(5(x+3))$ or $g\left(\frac{1}{5}(x+3)\right)$ ?
I prefer the second answer but my teacher said the correct is the first one? Can anyone explain for me why it is $5$ instead of $\frac{1}{5}$ while we are dealing with horizontal shrinking? 
Thanks a lot",['algebra-precalculus']
2929413,Show that $\text{SO}(2)$ is compact.,"Let $\text{Mat}_2(\mathbb{R})$ be the set of $2\times 2$ real matrices with the topology obtained by regarding $\text{Mat}_2(\mathbb{R})$ as $\mathbb{R}^4$ . Let $$\text{SO}(2)=\{A\in\text{Mat}_2(\mathbb{R}); A^TA=I_2, \det A=1\}$$ where $A^T$ denotes the transpose of $A$ , and $I_2$ is the $2\times 2$ identity matrix. The subspace topology of $\text{SO}(2)$ is obtained from $\mathbb{R}^4$ , where we identify a $2\times 2$ matrix with a point in $\mathbb{R}^4$ by using the matrix entries as coordinates.
Viewing $\text{SO}(2)$ as a subset of $\mathbb{R}^4$ , it is enough to show that $\text{SO}(2)$ is bounded and closed in $\mathbb{R}^4$ . I was able to show that $\text{SO}(2)$ is bounded. For any matrix $A\in\text{SO}(2)$ , we have that $|A|=\sqrt{2}$ , using the Euclidean metric of $\mathbb{R}^4$ . I want to show that $\text{SO}(2)$ is closed by showing that $\mathbb{R}^4\setminus\text{SO}(2)$ is open. I am not sure how to start this. In addition we have: \begin{align*}
A^T A\\
\\
=\begin{pmatrix} a_1 & a_3 \\ a_2 & a_4\end{pmatrix}\begin{pmatrix} a_1 & a_2 \\ a_3 & a_4\end{pmatrix}\\
\\
=\begin{pmatrix} (a_1)^2+ (a_3)^2 & a_1a_2+a_3a_4 \\ a_1a_2+a_3a_4 & (a_2)^2+(a_4)^2\end{pmatrix}\\
\\
=\begin{pmatrix} 1 & 0 \\ 0 & 1\end{pmatrix}
\end{align*} and \begin{align*}
\det{\begin{pmatrix} a_1 & a_2 \\ a_3 & a_4\end{pmatrix}}\\
\\
=a_1a_4-a_2a_3=1.
\end{align*}","['general-topology', 'linear-groups', 'compactness']"
2929430,Intuition behind the notion of proper morphisms in algebraic geometry,"I'm in understanding the intuition behind the notion of proper morphisms and also how people think about it. https://en.wikipedia.org/wiki/Proper_morphism A morphism of schemes $f:X \rightarrow Y$ is proper if it is separated, finite type, and universally closed. I also know it generalizes the notion of a proper map in topological spaces. https://en.wikipedia.org/wiki/Proper_map But what is not clear to me why separated and universally closed are the right ways to generalize this. Also, how do people generally think about proper morphisms?","['algebraic-geometry', 'schemes', 'intuition']"
2929445,"Check computation of limit distribution of $\max\{X_1, \ldots , X_n\}/\sqrt n$ for i.i.d. $(X_i)$ with PDF $2/x^3$ on $x>1$","$X_1$ , $X_2$ , . . . are iid random variables having pdf $$f_X(x)
 =\frac{2}{x^3}I_{(1,\infty)}(x)$$ Let $$T_n= \frac{\text{max}{\{X_1, . . . , X_n}\}}{\sqrt{n}}$$ (a) Consider the sequence $T_1$ , $T_2$ , . . . and give the pmf or pdf of the limiting distribution. I first note that the cdf of $X$ is given by $$ F_{X}(x)=  
\begin{cases} 
1-\frac{1}{x^2} & x \gt 1 \\
0 & x\leq 1 \\
\end{cases} $$ We have $$\begin{align*}
F_{T_n}(t)
&=\mathsf P(T_n\leq t)\\\\
&=\mathsf P\left(\frac{\text{max}{\{X_1,...,X_n}\}}{\sqrt{n}}\leq t\right)\\\\
&=\mathsf P\left(\text{max}{\{X_1,...,X_n}\}\leq \sqrt{n}\cdot t\right)\\\\
&=\mathsf P\left(X_1\leq \sqrt{n}\cdot t,...,X_n\leq\sqrt{n}\cdot t\right)\\\\
&=\prod_{i=1}^n \mathsf P\left(X_i \leq \sqrt{n}\cdot t\right)\\\\
&=\left(1-\frac{1}{(\sqrt{n}\cdot t)^2}\right)^n
\end{align*}$$ Altogether, we have $$ F_{T_n}(t)=  
\begin{cases} 
\left(1-\frac{1}{(\sqrt{n}\cdot t)^2}\right)^n & t \gt \frac{1}{\sqrt{n}} \\
0 & x\leq \frac{1}{\sqrt{n}} \\
\end{cases} $$ Using the property that $$\lim_{n\rightarrow\infty}\left(1+\frac{x}{n}\right)^n\rightarrow e^x$$ we get $$ \lim_{n\rightarrow\infty}F_{T_n}(t)=  
\begin{cases} 
e^{-\frac{1}{t^2}} & t \gt 0 \\
0 & t \lt 0 \\
\end{cases} $$ Then $$\frac{d}{dt}\left(e^{-\frac{1}{t^2}}\right)=\frac{2e^{-\frac{1}{t^2}}}{t^3}$$ Finally, we get $$f_T(t)=\frac{2e^{-\frac{1}{t^2}}}{t^3}I_{(0,\infty)}(t)$$ Is this a valid solution?","['probability-distributions', 'probability-theory']"
2929450,Find $\lim_{x\rightarrow 0} \frac{5^x-3^x}{3^x-2^x}$,"I want to find the limit $$\lim_{x\rightarrow 0} \frac{5^x-3^x}{3^x-2^x}$$ My efforts: $$\lim_{x\rightarrow 0} \frac{5^x-3^x}{3^x-2^x}=\lim_{x\rightarrow 0}\frac{5^x((3/5)^x-1)}{3^x((2/3)^x-1)}$$ Multiplying and dividing numerator and denominator by $x$ we get, $$\lim_{x\rightarrow 0}\frac{5^x\frac{((3/5)^x-1)}{x-0}}{3^x\frac{((2/3)^x-1)}{x-0}}\tag{1}$$ Let, $$f(x)=(3/5)^x-1, g(x)=(2/3)^x-1$$ Now $$\lim_{x\rightarrow 0}\frac{f(x)-f(0)}{x-0}=f'(x)$$ and similarly for $g$ , we can rewrite $(1)$ as $$\lim_{x\rightarrow 0} \frac{5^x}{3^x}\times \frac{f'(0)}{g'(0)}\tag{2}$$ We know if $h(x)=a^x,$ then $h'(x)=a^x \log(a)$ Computing and putting everything in piece, we write $(2)$ as $$\lim_{x\rightarrow 0}\frac{\log(3/5)}{\log(2/3)}$$ So we get limit equal to $\frac{\log(3/5)}{\log(2/3)}$ . Is my computation correct?","['limits', 'proof-verification', 'sequences-and-series', 'real-analysis']"
2929512,A possible typo in textbook Introduction to Set Theory by Karel Hrbacek and Thomas Jech,"I think that statement (c) possibly contains a typo. (c) $P$ is dense in $C$ , i.e., for any $p,q \in P$ such that $p < q$ , there is $c \in C$ with $p \prec c \prec q$ . From another textbook Set Theory by Thomas Jech, I have a definition: A set $D \subset P$ is a dense subset if for all $a < b$ in P there exists a $d \in D$ such that $a < d < b$ . Thus I think that statement (c) should be: (c) $P$ is dense in $C$ , i.e., for any $p,q \in C$ such that $p \prec q$ , there is $c \in P$ with $p \prec c \prec q$ . Here is a screenshot taken from textbook Introduction to Set Theory by Karel Hrbacek and Thomas Jech: And Here is a screenshot taken from textbook Set Theory by Thomas Jech: Please verify my observation!",['elementary-set-theory']
2929519,How to form a mental image of $\mathbf {ijk}=-1$ in quaternions?,"$\mathbf {ijk}=-1$ is part of the famously stone-carved formula by Sir William Rowan Hamilton, allowing the multiplication of triplets. There is already an intuition question on the topic of quaternions, and a beautifully illustrated post on 3D stereographic projections of quaternion rotations in 4D in the 3Blue1Brown youtube channel . The question is very specific about how to picture in a geometric or intuitive way the equality $\mathbf {ijk}=-1$ part in the formula $$\mathbf {i}^2 = \mathbf {j}^2 = \mathbf {k}^2 =\mathbf {ijk}=-1 $$ Attempt: $\mathbf i$ , $\mathbf j$ and $\mathbf k$ act on the left of a quaternion as a pure rotation; hence, $$\mathbf {ijk}\overset{?}=\mathbf {(ij)k}$$ and $\mathbf {i}$ acts on $\mathbf {j},$ rotating $\mathbf {j}$ into $\mathbf {k}$ in keeping with 3Brown1Blue's Grant Sanderson's ""right-hand rule"" , illustrated with the 3D diagram below, corresponding to the stereographic projection of a 4D hypersphere of quaternions with norm $1$ and $0$ real component, where the $-\infty,$ $-\mathbf i,$ $1,$ $\mathbf i,$ $+\infty$ yellow line is the projection of a circle in 4D running through $-1,$ and centered at $0,$ passing through $-\mathbf i$ and $+\mathbf i;$ while the red-blue $\mathbf j,$ $\mathbf k,$ $-\mathbf j,$ $-\mathbf k$ circle corresponds to the 3D projection of a sphere in the 4D hypersphere passing through $-1,$ and centered at $0,$ and reaching both $\pm \mathbf k$ and $\pm \mathbf j.$ Rotating in the direction of $\mathbf i$ (i.e. multiplying on the left by $\mathbf i$ ) would amount to sliding the yellow line in the direction of the thumb, while rotating the red-blue circle following the clenching of the rest of the fingers: and resulting in $$\begin{align}\mathbf {ijk}&\overset{?}=\mathbf {(ij)k}\\
&=\mathbf {kk}\\
&=\mathbf k^2 =-1
\end{align}$$","['quaternions', 'linear-algebra', 'complex-numbers']"
2929554,Binomial Distribution equals 1 Proof,"In order to prove $b(x,n,p) =$ ${n \choose x}p^x(1-p)^{n-x}$$for$ $x=0,1,2,...,n$ (and $0$ otherwise) we must show $1 = \sum_{x=0}^n{n \choose x}p^x(1-p)^{n-x}$ However, all the answers I've seen are just stating the binomial theorem with no work shown. Would anyone be willing to show me the steps for $1 = 1^n = (p+1-p)^n =  \sum_{x=0}^n{n \choose x}p^x(1-p)^{n-x}$ so I can grasp the idea better?","['probability-distributions', 'probability-theory', 'probability']"
2929596,Question about Cauchy Sequence proof for $\sum_{i=1}^\infty \frac{1}{i^2}$,"I was looking through a proof that the sum of $$\sum_{i=1}^\infty \frac{1}{i^2} =  1+\frac{1}{2^2}+\frac{1}{3^2}+\frac{1}{4^2}...$$ is convergent. I know there are probably other ways to prove this, but because I'm learning about functional analysis I want to prove it by first proving that this sequence is Cauchy. The proof goes as follows. Consider any $\epsilon>0$ , I can find you an $N$ in the sequence that is sufficiently large enough such that $\frac{1}{n}>\sqrt\frac{\epsilon}{2 }$ and $\frac{1}{m}>\sqrt\frac{\epsilon}{2 }$ , where $n$ and $m$ are defined as greater than $N$ . Thus: $$|\frac{1}{n^2}-\frac{1}{m^2}|<\frac{1}{n^2}+\frac{1}{m^2}<\epsilon$$ Then the sequence is Cauchy. But then I thought about the sequence: $$\sum_{i=1}^\infty \frac{1}{i} = \frac{1}{1}+\frac{1}{2}+\frac{1}{3} + \cdots.$$ If I just modify the proof a bit such that I let $\frac{1}{n}>\frac{\epsilon}{2}$ and likewise for $m$ , then won't it prove that this sequence is Cauchy as well? But this sequence is obviously not Cauchy. What's wrong with applying the above proof to this series?","['sequences-and-series', 'cauchy-sequences', 'real-analysis']"
2929598,Resolvent in random matrix theory,"The Stieltjes transform method is a technique to understand the spectrum of a random matrix.  On the way to proving a local semicircle law with this technique, one encounters the stieltjes transform of the semicircle law, which is defined as $$
m(z) := \int_\mathbb{R} \frac{\rho}{x - z} dx
$$ where $\rho = \frac{1}{2 \pi} \sqrt{(4 - x^2)_+}$ .  One can show that $m(z)$ satisfies the equation $$
m(z)  + 1/ m(z)  + z = 0
$$ with Im(z) > 0.  One can solve for $m(z)$ to find that $$
m(z)  = \frac{-z + \sqrt{z^2 - 4}}{2}. 
$$ Now it is claimed that for $x \in [-20, 20]$ and $y \in (0, 20]$ that for $z = x + i y$ , 1) There is a constant $c > 0$ such that $$
 c \leq |m(z)| \leq 1 - cy
$$ 2) There exist constants $C', c' >0$ such that $$
c' \sqrt{\kappa + y} \leq |1 - m^2(z)| \leq C' \sqrt{ \kappa + y}
$$ where $\kappa := ||x| - 2|$ . 3) Also for $|x| \leq 2$ , there exist constants $C'', c''>0$ such that $$
c'' \sqrt{\kappa + y} \leq Im(m(z)) \leq C'' \sqrt{\kappa + y}.
$$ 4) For $|x| \geq 2$ , $$
c'' \frac{y}{\sqrt{\kappa + y}} \leq Im(m(z)) \leq C'' \frac{y}{\sqrt{\kappa + y}}.  
$$ I can show the explicit form of $m(z)$ , but
I'm having difficulty demonstrating any of the 4 properties.  Any suggestions or explicit calculations would be helpful.","['complex-analysis', 'random-matrices', 'probability-theory']"
2929603,Is it best to approach solving this question using Binomial Distribution and Conditional Probability in the following way?,"Problem summary: 99 fair coins 1 unfair coin with a probability of getting heads $P(H)=0.9$ A coin is selected and flipped 10 times. If the coin lands on head 10 times out of 10 flips, what is the probability that the unfair coin was selected? My approach: Event $A$ : unfair coin was picked Event $B$ : Getting 10 heads from 10 flips Event $A^{c}$ : did not pick unfair coin Problem: find $P(A|B)$ $$P(A|B)=\displaystyle \frac{P(A \cap B)}{P(B)}$$ $=\displaystyle \frac{P(A \cap B) }{P(A \cap B) \cup P(A^{c} \cap B)} \label{a}\tag{1}$ $$P(A \cap B)={10 \choose 10} 0.9^{10}0.1^{0}=0.3487 \label{b}\tag{2}$$ $$P(A^{c} \cap B)={10 \choose 10}0.5^{10}0.5^{0}=0.00097656$$ $$P(B)=P(A \cap B)+P(A^{c} \cap B)$$ $$=0.3487+0.00097656=0.34967656 \label{c}\tag{3}$$ Subbing (2) and (3) into (1) gives $$P(A|B)=\frac{0.3487}{0.34967656}=0.997207248$$ Does this seem correct?","['conditional-probability', 'statistics', 'binomial-distribution']"
2929610,How to define Chirality and Polarity on a graph?,"Is there a concept of Chirality or Polarity defined on graphs? For example, consider a system with spin- $1/2$ (up/down polarity) particles which have also chirality (right/left). I thought to take a simple graph representation of Ising model and somehow extend it. I would say that a graph/percolation model for liquid-crystal is what I am looking for at the end. I guess a 4-coloring approach should work here. I am not sure how to state it for doing physics purposes.","['graph-theory', 'physics', 'group-theory', 'chemistry']"
2929658,Solve Inverse Laplace Transform Using Input Integral Theorem,"Problem Using the input integral principle below $$ \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s} \mathscr{L} \left[ f(t) \right] (s), \ s > c $$ Find $ \mathscr{L}^{-1} \left[ \frac{1}{s(s^2+1)} \right](t) \ $ without using partial fractions. Attempt Letting $ \ f(t) = \mathscr{L}^{-1} \left[ \frac{1}{s(s^2+1)} \right](t) $ , $$ \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s} \mathscr{L} \left[ \mathscr{L}^{-1} \left[ \frac{1}{s(s^2+1)} \right](t) \right] (s), \ s > c $$ $$ \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s}  \left[ \frac{1}{s(s^2+1)} \right], \ s > c $$ $$ \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s^2(s^2+1)} , \ s > c $$ Notes Perhaps I've approached this problem incorrectly, but I'm confused as how to proceed with it. All I'm looking for is a hint or correct first step in solving this problem, with a little bit of explanation as to what the correct method toward solving this problem entails. Also, I searched up what the input integral principle is and I'm not finding anything on it. Did my professor invent this name or is it an alias for something else? That being said, any help is appreciated. Thanks!","['integration', 'definite-integrals', 'laplace-transform', 'ordinary-differential-equations']"
2929725,"Sum of Uniform(5,10) random variables to get more than 30","Let $X_i$ be i.i.d. $Uniform(5,10)$ , and let $Y_t = \sum_{i=1}^t X_i$ . Let $T = \mbox{inf}\{t:Y_t \geq 30\}$ , what is $\mathbb{E}[T]$ ? At first I thought this was similar to Choose a random number between $0$ and $1$ and record its value. Keep doing it until the sum of the numbers exceeds $1$ . How many tries do we need? , but after trying some methods mentioned under that question, I still couldn't figure out a solution. I know $T$ can only be $4, 5, 6$ , but is it possible to compute each individual probability?","['random', 'uniform-distribution', 'probability', 'random-variables']"
2929775,Find all functions $f:\mathbb{N}^+\to\mathbb{N}^+$ such that $f\big(f(n)\big)+f(n)=2n$ for every $n\in\mathbb{N}^+$.,"Find all functions $f:\mathbb{N}^+\to\mathbb{N}^+$ such that $$f\big(f(n)\big)+f(n)=2n$$ for every $n\in\mathbb{N}^+$ . I think the answer is $f(n)=n$ ,We prove this by induction. ( at last step I can't induction it ） It is true for $n=1$ because
Let $f(1)=x\ge 1,$ and let $n=1$ ,we have $$f(x)+x=2\Longrightarrow x=1$$ (2) Suppose it is true for $f(n)=n(n\le k)$ , then for $n=k+1$ ,let $f(n+1)=y$ ,so we have $$f(y)+y=2(k+1)\Longrightarrow f(y)=2(k+1)-y$$ it is easy to have $k+1\le y\le 2k+1$ ,and $$f(f(y))+f(y)=2y\Longrightarrow f(2k+2-y)=2y-f(y)=3y-2(k+1)$$ since $2k+2-y\in [1,k+1]$ ,then I can't it ,can you help? Thanks
(if $2k+2-y\in [1,k]$ ,I have done it! bacuase $f(2k+2-y)=2k+2-y$ ,so $y=k+1$ )","['contest-math', 'functional-equations', 'recurrence-relations', 'functions', 'induction']"
2929776,What is different between $G/N$ and $GN/N$?,"What is different between $G/N$ and $GN/N$ ? ( $G$ denotes a group, and N does a normal subgroup of G) If you look at the element of each group above, I think $G/N$ contains element of form $gN$ , where $g$ is an element of G but not in $N$ . and $GN/N$ contains element of form $gnN$ which is equivalent to $gN$ , because $n$ is an element of $N$ . Then it looks like they indicate same group. Is there anything wrong with my reasoning? If so, could you point out and fix it?","['group-theory', 'finite-groups']"
2929825,"Given coordinates of vertices a convex quadrilateral, find which pairs of vertices determine the diagonals","I'm writing a program that will take in four $(x, y, z)$ points that form a quadrilateral, and produce two sets of $(x, y, z)$ points that form the two largest triangles that comprise the quadrilateral. I'm having some difficulty in coming up with a way to determine which points make up the diagonals of the quad. Obviously if I can figure out two points that are guaranteed to make up a diagonal, the two triangles will be comprised of those two points plus one of the other remaining points. So, given four points: $A = (x_A, y_A, z_A)$ $B = (x_B, y_B, z_B)$ $C = (x_C, y_C, z_C)$ $D = (x_D, y_D, z_D)$ How can I determine which points form a diagonal of the quadrilateral?","['geometry', '3d']"
2929832,Main period of $f(x)=\cos 5x+\cos 10x$,"Here is what I have done so far: \begin{align*}
f(x)&=\cos5x+\cos10x\\
f(x)&=\cos5x+2\cos^2(5x)-1\\
f(x)&=2\cos^25x+\cos5x-1\\
\end{align*} I have tried to further simplify the function to a complete square or a function like $\cos^2(\text{something})$ but I was not able to. Then I factorised: $$f(x)=2\left(\cos5x+\frac12\right)(\cos5x-1),$$ where each multiplier has a period of $\dfrac{2π}{5}$ , which makes me think that the main period of the function is $\dfrac{2π}{5}$ . Please excuse me for the bad terminology. English is not my native language.","['periodic-functions', 'trigonometry']"
2929882,Venn representation of $(P \cup Q) \cap R$. What did I do wrong?,Let's say you have a question of sets saying: $$(P \cup Q) \cap R$$ I answer this way: (The black part is the shaded) But the correct answer looks like this: (The black part is the shaded) So why is it like this? Aren't you supposed to shade everything if the question wants union?,['elementary-set-theory']
2929919,The order of $Hg\in G/H$ divides the order of $g\in G$ when $H$ is normal in $G$ and $G$ is finite.,"Proposition: The order of $Hg\in G/H$ divides the order of $g\in G$ when $H$ is normal in $G$ and $G$ is finite. Proof Attempt(by contradiction): Let $H \lhd G$ and $gH\in G/H$ where the order of $gH$ is $k$ in $G$ . Thus, $(gH)^k=g^kH=H \Leftrightarrow g^k\in H$ and $\forall j \lt k, (gH)^j=g^jH \neq H$ which implies $g^j \notin H$ . Suppose $\vert g\vert=n$ such that $g\in G$ so that $g^n=e \in H$ and $\forall m \lt n, g^n\neq e$ . By the division algorithm $\exists q,r\in \Bbb{Z}$ such that $n=kq+r$ where $0\leq r \lt q,k,n$ . Hence, if $r\gt 0$ , $g^r\neq e$ and $g^n=g^{kq+r}=g^{kq}g^r=e\Rightarrow g^r=g^{-kq}$ . Since $g^k \in H$ , then $g^{-kq}=g^r \in H$ . But, $r<k$ so $g^r \notin H$ unless $r=0$ . So $r$ must be $0$ and that means $n=kq$ and $k$ divides $n$ , as desired.","['group-theory', 'proof-verification', 'finite-groups']"
2929924,Example of family of integral curves with constant gonality and increasing genus,"Are there examples of a family of integral curves over some field $k$ which have constant gonality but increasing genus? A related question: If I give you two non-negative integers $n$ and $g$ , can you provide an example of a curve with gonality at most $n$ and genus at least $g$ ? Thanks for any input or references!","['algebraic-curves', 'algebraic-geometry', 'projective-schemes', 'examples-counterexamples']"
2929960,Prove $\frac{d}{dx} x^n=nx^{n-1} : \forall n\in \mathbb{Z}_{+}$ by induction [duplicate],"This question already has an answer here : Proof by induction (power rule of the derivative) (1 answer) Closed 5 years ago . Problem Prove $$\frac{d}{dx} x^n=nx^{n-1} : \forall n\in \mathbb{Z}_{+}$$ by mathematical induction. Attempt to solve Base case when $n=1$ $$ \frac{d}{dx} x^1 = 1 \cdot x ^{0}=1 $$ which is true Induction step $$\frac{d}{dx}x^{n+1}=(n+1)x^{n+1-1}= (n+1)x^{n}$$ At this point not quite sure how to prove this with induction without proving operator $\frac{d}{dx}$ with $$ \frac{d}{dx}f(x)=\lim_{h \rightarrow 0}\frac{f(a+h)-f(a)}{h} $$ and then proving existence of such limit with: $$ 0<|x-a|< \delta \implies|\frac{f(a+h)-f(a)}{h}-\frac{d}{dx}f(a)| < \epsilon
$$ and then we can arrive at implication that $$\frac{d}{dx}x^n=nx^{n-1} \implies \frac{d}{dx}x^{n+1}=(n+1)x^n$$ Most likely there is easier by induction which is capable of showing that $$ \frac{d}{dx}x^n=nx^{n-1} $$ is applicable $\forall n \in \mathbb{Z}_+$ ?","['limits', 'induction', 'derivatives']"
2929976,"How do I calculate $(A\times B)^2$ for $A=\{1,2\}$ and $B=\{a,b,c\}$?","Let $A = \{1,2\}$ and $B = \{a,b,c\}$ , find $(A \times B)^2$ . I found $(A \times B) = \{(1,a),(1,b),(1,c),(2,a),(2,b),(2,c)\}$ But how do I find $$\{(1,a),(1,b),(1,c),(2,a),(2,b),(2,c)\} \times \{(1,a),(1,b),(1,c),(2,a),(2,b),(2,c)\} $$ ?
Is it $\{(1,a,1,a),(1,a,1,b),(1,a,1,c), \ldots\}$ ? If I am wrong please show me the correct method.","['elementary-set-theory', 'products']"
2929983,What does it really mean for a Lagrangian to be independent of the base coordinates?,"In this question I will be considering what is called in physics a ""classical Lagrangian field theory"" from a geometric point of view. One is given an $n$ dimensional (smooth, real) ""spacetime manifold"" $M$ , with some ""field"" $\psi$ on it. From a purely local point of view, the field is assumed to have components $\psi^i(x)$ . A (first-order) Lagrangian is then an $n$ -form $$ \hat L=\mathcal L(\{x^\mu\},\{\psi^i(x)\},\{\partial_\mu\psi^i(x)\})dx^1\wedge...\wedge dx^n $$ that depends on the field, the field derivatives and maybe on the spacetime coordinates explicitly. In physics, it is considered pathological for a Lagrangian to depend explicitly on the spacetime coordinates. Usually it signals a ""nonclosed"" system, where there are externally imposed fields, whose dynamics are not controlled by a Lagrangian. From a more precise point of view, let $E\rightarrow M$ be a rank $k$ vector bundle over $M$ (it can be a general fiber bundle, and in fact, it is often not a vector bundle, but even in those cases, one can often consider it a vector bundle - for example the bundle of metrics, which is nonlinear may be enlarged to the bundle of symmetric (0,2) tensors, which is). The field $\psi$ is considered a section of $E$ . In a local trivialization $E$ has coordinates $(\{x^\mu\},\{y^i\})$ . We can consider the first jet bundle $J^1E$ of (sections of) $E$ , whose local coordinates are $(\{x^\mu\},\{y^i\},\{y^i_\mu\})$ . If $\psi$ is a section of $E$ , it ""looks like"" in local coordinates as $$\psi: x\mapsto (\{x^\mu\},\{\psi^i(x)\}), $$ and its first jet prolongation $j^1\psi$ looks like as $$ j^1\psi:x\mapsto (\{x^\mu\},\{\psi^i(x)\},\{\partial_\mu\psi^i(x)\}).$$ The (first-order) Lagrangian is then a horizontal $n$ -form on $J^1E$ , which in a local trivialization looks like as $$ \hat L=\mathcal L(\{x^\mu\},\{y^i\},\{y^i_\mu\})dx^1\wedge...\wedge dx^n $$ where the $dx^\mu$ are considered 1-forms on $J^1E$ . The fields are not involved here, however, the action functional is defined as $$ S[\psi]=\int (j^1\psi)^\ast\hat L, $$ where $$(j^1\psi)^*\hat L=\mathcal L(\{x^\mu\},\{\psi^i(x)\},\{\partial_\mu\psi^i(x)\})dx^1\wedge...\wedge dx^n.$$ The problem: It is a well known fact that if $E\rightarrow M$ is some fiber bundle (like the $E$ previously defined, or $J^1E$ ), then if $f:E\rightarrow\mathbb R$ is some smooth function (the target space of this map is not really relevant), and in some local trivialization this function looks like as $$ f(\{y^i\}), $$ eg. $f$ doesn't depend on the base coordinates $x^\mu$ , then this is not a trivialization-independent statement. In some other trivialization, it will look like as $$ f^\prime(\{x^{\prime\mu}\},\{y^{\prime i}\}). $$ On the other hand, as I have stated in the beginning of the question, in physics it is considered pathological to have a Lagrangian that depends explicitly on $x^\mu$ . However by the previous argument, it should not be possible to construct a Lagrangian on $J^1E$ , that looks like $$ \hat L=\mathcal L(\{y^i\},\{y^i_\mu\}) $$ in all trivializations. Now, I know that in physics, there is often a lot of background structure assumed. These background structures can break ""general covariance"". I can illustrate this with the following example: For the massless Klein-Gordon field in Minkowski-spacetime, the Lagrangian (in physics language - so this is already pulled back via the field's prolongation) is $$ \mathcal L=\frac{1}{2}\eta^{\mu\nu}\partial_\mu\psi\partial_\nu\psi\ dx^1\wedge...dx^n, $$ where $\eta^{\mu\nu}$ is the canonical ""flat"" form of the dual Lorentz-metric. It is not explicitly $x$ -dependent, but this is not a trivialization-independent statement. If we perform a non-Poincaré coordinate transformation, the Lagrangian will look like $$ \mathcal L=\frac{1}{2}g^{\mu\nu}(x)\partial_\mu\psi\partial_\nu\psi\sqrt{|\det g(x)|}dx^1\wedge...dx^n. $$ Here the general form of the metric $g_{\mu\nu}(x)$ is $x$ -dependent, but is an externally imposed field, so the Lagrangian is $x$ -dependent. However, in physics, if we account for everything, then we can get rid of all background structures. For example, we can consider the gravitational field (the metric) $g$ , a matter field $\psi$ (a Dirac field, or a scalar multiplet), and a gauge connection $A$ , and mesh everything into a glorious Lagrangian $$ \mathcal L(\psi,\partial\psi,A,\partial A,g,\partial g,\partial\partial g) $$ (this Lagrangian will be second order in $g$ , but that doesn't change the problem). Now, this Lagrangian will never acquire explicit $x$ -dependence, through either diffeomorphisms of $M$ , or changes of trivialization in $E$ (here $\psi$ is a section of $E$ , and $E$ is considered a $G$ -bundle for some Lie group $G$ , and $A$ is a $G$ -connection) or for that matter, in the ""connection bundle"" in which $A$ lives. So from this, it seems to me that if we use fiber products to create one massive fiber bundle $\mathcal E$ from all these bundles involved, then the unified Lagrangian is still a horizontal element in $\Omega^n(J^1\mathcal E)$ , yet it will never acquire explicit $x$ -dependence. Question: Considering all the lengthy exposition I have given above, how do you invariantly and geometrically characterize Lagrangians which do not depend explicitly on the base manifold's coordinates? Since this should be a trivialization-dependent statement, yet it also seems to be that it is possible to construct Lagrangians that absolutely do not depend on any background structures, and are as such $x$ -independent.","['calculus-of-variations', 'fiber-bundles', 'euler-lagrange-equation', 'jet-bundles', 'differential-geometry']"
2929993,When mathematicians study specific classes of groups how do they chose which realization of them to use?,"By cayleys theorem every group $G$ is isomorphic to a permutation group over the elements of $G$ yet it seems that in many instances trying to express certain groups in this way only makes it more complicated to study them. But at the same time after reading articles on specific terminology for permutation groups e.g. https://en.wikipedia.org/wiki/Block_(permutation_group_theory) it seems people have developed specific techniques for dealing with permutation groups in particular, which leads me to believe there are some instances in which a permutation group is the most useful way to represent a group. How does one know when this is?","['permutations', 'group-theory', 'group-actions']"
2930015,Spheres cause contradictions in dimensions $10$ and more?,"According to this Numberphile video , if you tightly pack hyper-spheres into a hyper-box and then find the radius of the largest hyper-sphere that could possibly fit in the remaining space, the resulting hyper-sphere would somehow exceed the confines of the box that contained all of the hyper-spheres (where the number of dimensions are greater or equal to 10). Isn't a logical contradiction generally considered a disproof of something? Wouldn't this disprove the generalised formula being used to find the radius of the resulting sphere on n dimensions? Is it possible that mathematicians simply do not understand extra dimensional geometry and its inherent rules?","['euclidean-geometry', 'spheres', 'paradoxes', 'geometry']"
2930035,"For positve $a$, $b$, $c$, $d$ with $a+b+c+d\leq 1$, prove that $\sqrt[4]{(1-a^4)(1-b^4)(1-c^4)(1-d^4)}\geq255\cdot a b c d .$","Let $a,b,c,d\in\mathbb R_+$ such that $a+b+c+d\leqslant1$ . Prove that $$
\sqrt[4]{\smash[b]{(1-a^4)(1-b^4)(1-c^4)(1-d^4)}}\geqslant255·abcd.
$$ My observations: I can see that all of $a,b,c,d$ are positive fractions which makes all the bracketed factors in LHS positive. Now if we raise both sides to the power $4$ , and replace the $4$ th powers of $a,b,c,d$ with $A,B,C,D$ , our inequality gets reduced to $$(1-A)(1-B)(1-C)(1-D)\ge 255^4 \cdot ABCD$$ Now applying AM $\ge$ GM we get two results, $$A+B+C+D\ge 4\cdot\sqrt[4]{ABCD}$$ and $$4-(A+B+C+D)\ge 4\cdot\sqrt[4]{\smash[b]{(1-A)(1-B)(1-C)(1-D)}}$$ Can I somehow use both these results to prove the inequality? Please help","['inequality', 'radicals', 'a.m.-g.m.-inequality', 'products', 'algebra-precalculus']"
2930109,Quotient space of $\mathbb{R}^2$,"This is an example of from the book Topology of Janich. In the picture $X = \mathbb{R}^2$ with standard topology, and the lines represent the equivalence classes, which are closed $1$ dimensional manifolds. The example is aimed to show: Even if the equivalence classes are closed subsets of the Hausdorff space $\mathbb{R}^2$ , the quotient space $X/\sim$ could still be non-Hausdorff. It says that one quotient space in the picture is Hausdorff while the other is not. However I am having hard time finding out which is. Could somebody help me please.",['general-topology']
2930125,Unit of the form $n\alpha + m \beta \sqrt{d}$ in real quadratic fields,"Let $d$ be a positive square-free integer equal to $2$ or $3\mod 4$ . The unit group of $\mathbb{Z}[\sqrt{d}]$ , denoted $\mathbb{Z}[\sqrt{d}]^\times$ , is generated by $\pm 1$ and $p+q\sqrt{d}$ , where $p/q$ is the first convergent in the recurring fraction expansion of $\sqrt{d}$ . Let's take $\alpha, \beta \in \mathbb{Z}$ such that $\gcd(\alpha, \beta)=\gcd(\alpha,d)=1$ . My question is, for every coprime pair $(\alpha,\beta)$ does there exist $n,m \in \mathbb{Z}$ (also necessarily coprime and $n$ coprime to $d$ ) such that $$
(n\alpha) + \sqrt{d} (m\beta) \in \mathbb{Z}[\sqrt{d}]^\times?
$$","['number-theory', 'algebraic-number-theory']"
2930173,Show that the matrix $A = \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix}$ is not diagonalizable.,"I have this question: Show that the matrix $$A = \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix}$$ is not diagonalizable. So is the general strategy is To Find the eigenvectors and then Show that the matrix of eigenvectors is not invertible? If they are invertible, then it has a unique solution to ( $\lambda \bf {I - A)x = 0}$ which would imply that they are linearly independent. If it's linearly independent, then it would be diagonalizable? I'm following this theorem Condition for Diagonalization A $n \times n$ matrix is diagonalizable iff it has $n$ linearly independent eigenvectors. So I have to find the eigenvalue first, which is $2$ because the $2$ is on the diagonal of this matrix in a triangular matrix, using this theorem. Eigenvalues of Triangular Matrices If $A$ is a $n \times n$ triangular matrix, then its eigenvalues are the entries on its main diagonal. Solving for $\lambda {\bf I - A}$ : $$\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} - \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} $$ Since this matrix is not invertible, it is not diagonalizable. Is this right? This is the proof that I'm relying on:","['triangularization', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'diagonalization']"
2930183,"Calculate the integral $ \int_{0}^{1}\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} (x^2+x^2y^3)\, dy dx .$","Calculate the integral $$ \int_{0}^{1}\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} (x^2+x^2y^3) dy dx .$$ My attempt: Notice that $ x^2y^3 $ is an odd function with respect to $ y $ , so \begin{align*} \int_{0}^{1}\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} (x^2+x^2y^3)\, dy dx &= \int_{0}^{1}\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}( x^2 )\, dy dx \\
&=\int_{0}^{1} 2x^2\sqrt{1-x^2}\, dx\\
&= \int_{0}^{\frac{\pi}{2}}2\sin^2\theta\cos^2\theta \,d\theta\\
&= \frac{1}{2}\int_{0}^{\frac{\pi}{2}}\sin^22\theta \,d\theta\\
&= \frac{1}{4}\int_{0}^{\frac{\pi}{2}}(1-\cos 4\theta) \,d\theta\\
&= \frac{\pi}{8}.\end{align*} Am I right? I think it's a little bit too complicated than what it should be?","['integration', 'multivariable-calculus', 'calculus', 'multiple-integral']"
2930195,Why a set that is subset/equal to infinite set isn't infinite? (by definition),"I searched but couldn't really find an answer for that, so sorry if its a duplicate or anything else. My question is why a set, for example A, that has a subset/equal set which is infinite(N for example), isn't infinite by definition?
The formal definition for an infinite set is ""A set is infinite if and only if for every natural number the set has a subset whose cardinality is that natural number."" It makes sense of course, and its easy to prove that A is also infinite, but then why wouldn't N ⊆ A also mean that A is infinite by definition ? Is there an opposite example when it doesn't happen? I'm not very familiar with math definitions in english, so sorry if I wrote something wrong. Thanks. Edit - @Asaf Karagila answered it perfectly. Thanks for the replies.",['elementary-set-theory']
2930215,Divergence of $1/r$ in cylindrical coordinates,"In classical textbooks, like ""Introduction to Electrodynamics"" by J.D. Griffiths, it is given that $$\nabla\cdot\left(\frac{\widehat{r}}{r^2}\right)=4\pi\delta^3(R).$$ To prove this equality, Griffiths first evaluates the divergence and finds it $0$ for $r \neq 0$ and undefined for $r=0$ . Then evaluates the integral of the given divergence and finds it $4\pi$ . Then, he concludes that the above equality should hold since the integral has a constant value containing origin. Now, I am asked about the following equality : $$\nabla \cdot\left(\frac{\widehat{\rho}}{\rho}\right)=2\pi\delta^2(\rho)$$ (note: the equation is claimed to be valid in cylindrical coordinates). I was unable to work out the integral properly. Any help would be appreciated.","['cylindrical-coordinates', 'divergence-operator', 'multivariable-calculus']"
2930266,Proving $\sum_{i=1}^{n} {{n}\choose{i}} (-1)^i = -1$ by induction,"I would like to prove this $$\sum_{i=1}^{n} {{n}\choose{i}} (-1)^i = -1$$ for all $n \in \mathbb{N} $ . I started by replacing consecutively $n$ by $1, 2, 3$ , one at a time, and verified that all of these indeed equal $-1$ . However, I would like to prove it for all $i$ , therefore I tried to do it by Mathematical Induction but failed somewhere. Below is my rationale. $ n=1 \Rightarrow \sum_{i=1}^{n} {{n}\choose{i}} (-1)^i = \sum_{i=1}^{1} {{1}\choose{i}} (-1)^i = {{1}\choose{1}} (-1)^1  = 1 \times (-1) = -1$ $$\sum_{i=1}^{n+1} {{n+1}\choose{i}} (-1)^i \\= 
\sum_{i=1}^{n} {{n+1}\choose{i}} (-1)^i + \sum_{i=n+1}^{n+1} {{n+1}\choose{i}} (-1)^i \\
= \sum_{i=1}^{n} {{n+1}\choose{i}} (-1)^i + {{n+1}\choose{n+1}} (-1)^{n+1}\\
 = \sum_{i=1}^{n} \space [{{n}\choose{i-1}} + {{n}\choose{i}}] (-1)^i + {{n+1}\choose{n+1}} (-1)^{n+1} \\
= \sum_{i=1}^{n} \space {{n}\choose{i-1}} (-1)^i + \sum_{i=1}^{n}{{n}\choose{i}} (-1)^i + {{n+1}\choose{n+1}} (-1)^{n+1} = $$ Given the induction hypothesis, $\sum_{i=1}^{n} {{n}\choose{i}} (-1)^i = -1 $ , hence we have $ \sum_{i=1}^{n} \space {{n}\choose{i-1}} (-1)^i - 1 + (-1)^{n+1} = $ Now the way I see this is that $\sum_{i=1}^{n} \space {{n}\choose{i-1}} (-1)^i$ is similar to $\sum_{i=1}^{n}{{n}\choose{i}} (-1)^i $ , but instead of having all the Pascal triangle terms except the term in ${n}\choose{0}$ , we end up having all elements except the term in ${n}\choose{n}$ . This is why I wrote the previous equation as equal to $ \sum_{i=1}^{n} \space {{n}\choose{i}} (-1)^i + {{n}\choose{0}} (-1)^0 - {{n}\choose{n}} (-1)^n - 1 + (-1)^{n+1} = $ $ -1 + 1 - {{n}\choose{n}} (-1)^n - 1 + (-1)^{n+1} = - (-1)^n - 1 + (-1)^{n+1} = -1 + (-1)^{n+1} + (-1)^{n+1} = $ $ = -1 + 2 \times (-1)^{n+1} $ You see there is a problem in this resolution, because, depending on whether n is odd or even, the final result of this sum will differ. If $n$ is odd, $n+1$ is even, hence we have $-1 + 2 \times 1 = 1$ . If $n$ is even, then $n+1$ is odd, hence we have $-1 + 2 \times (-1) = -3$ ! I did this resolution over and over, but I did not find the hole in my logic. Please help. Thank you.","['summation', 'proof-verification', 'binomial-coefficients', 'combinatorics', 'induction']"
2930279,Combinatorics dance class,"I have a problem that ím trying to solve and I am not completely sure if my answer is correct, I tried looking for it on the web but i cant find a problem quite like it. I am translating the question into English, and im doing my best to translate it correct, but im sorry if there are some stupid grammar mistakes. „During a dance class 4 pairs (a pair consists of one man and one woman) are chosen from 4 men and 7 women. Romeo and Juliet are students in this class, what is the probability that the two will form a pair. My answer would be $$  \frac{\binom{6}{3}3!}{ \binom{7}{4}4!} $$ My thought process is as follows, There is only one possibility two choose Rome and Juliet, and 1 Possibility to choose the 3 men, so im not writing them down.
But there are 6C3 possible ways to choose the 3 women left, and 3!  possibilities to assign them to the 3 men. And there are 7C4 ways to choose the 4 women in total and then 4! Ways to form different pairs. Is this correct? And if not, where did i make a mistake. Many thanks for reading this and helping me out Ps I tried writing it down in mathjax but wasn’t very successful with that - sorry","['self-learning', 'combinatorics', 'probability']"
2930284,"General formula for nth element of the sequence 0, 1, 0, 1, ...","The sequence is $f = 0, 1, 0, 1, \ldots$ I want to find a general formula for the $n$ th element. The sequence starts at $n = 0$ (the $0$ here is not the first element $0$ but rather denotes the $0$ th position). One easy and obvious solution is: $n$ th $f = n \bmod 2$ . This works because even positions have $0$ and odd positions have $1$ . However, this question is part of a homework and modulus has not been discussed (or part of the syllabus or even a prerequisite). And so I am hesitant to use it. Is there another way to solve this problem using only basic arithmetic operations (one that a beginning high schooler knows of)?","['arithmetic', 'pattern-recognition', 'sequences-and-series']"
2930341,How to find the number of solutions to the given equation?,"How do I find the number of integer solutions to the following equation: $$\frac{1}{\sqrt{x}} - \frac{1}{\sqrt{y}} = \frac{1}{2016}$$ I think $x, y$ should be perfect squares but I am not sure... Can anybody help me with this proof and go ahead with the question?",['algebra-precalculus']
2930365,How to compare logarithms $\log_4 5$ and $\log_5 6$? [duplicate],"This question already has answers here : How to know if $\log_78 > \log_89$ without using a calculator? (6 answers) Closed last year . I need to compare $\log_4 5$ and $\log_5 6$ . 
I can estimate both numbers like $1.16$ and $1.11$ . Then I took smallest fraction $\frac{8}{7}$ which is greater than $1.11$ and smaller than $1.16$ and proove two inequalities: $$\log_4 5 > \frac{8}{7}$$ $$\frac{7}{8}\log_4 5 > 1$$ $$\log_{4^8} 5^7 > 1$$ $$\log_{65536} 78125 > 1$$ and $$\log_5 6 < \frac{8}{7}$$ $$\frac{7}{8}\log_5 6 < 1$$ $$\log_{5^8} 6^7 < 1$$ $$\log_{390625} 279936 < 1$$ thats why I have $\log_5 6 < \frac{8}{7} < \log_4 5$ . But for proving I need estimation both logarithms (without this estimation I cannot find the fraction for comparing).
Can you help me to find more clear solution (without graphs)","['inequality', 'logarithms', 'a.m.-g.m.-inequality', 'calculus', 'algebra-precalculus']"
2930370,"Find a closed form of $x_{n+2} = x_{n+1}+2x_n +2$ where $x_1 = x_2 = 1$, $n \in \mathbb N$","Find a closed form $x_n$ for the following recurrence relation: $$
x_{n+2} = x_{n+1} + 2x_n + 2 \\
x_1 = x_2 = 1,\;\;n\in \mathbb N
$$ I'm trying to understand why I get different results for different guesses of a solution for particular part of the recurrence relation. I've started by splitting the solution into two parts: homogenous and particular ones. It's known that $$x_n = x_n^{(h)} + x_n^{(p)}$$ Having the above in mind lets solve for homogenous. I've done this with the help of characteristic polynomial: $$
\lambda^2 - \lambda - 2 = (\lambda + 1)(\lambda - 2) = 0
$$ By this we obtain the form of the homogenous solution: $$
x_n^{(h)} = C_1\cdot(-1)^n + C_2 \cdot2^n
$$ Here is where things get vague for me. Let's try to guess the form of the non-homogenous solution. I've started with $B\cdot n$ , then $$
B\cdot(n+2) = B\cdot(n+1) + 2Bn + 2 \\
B = \frac{2}{1-2n}
$$ Therefore: $$
x_n = C_1 \cdot (-1)^n + C_2\cdot2^n + \frac{2}{1-2n}
$$ Using the initial conditions one may find $C_1 = -{13 \over 9}$ and $C_2 = {7\over 9}$ . Obtain the final form: $$
x_n = -{13 \over 9} \cdot (-1)^n + {7 \over 9} \cdot 2^n + \frac{2}{1-2n}
$$ And that result doesn't match the answer in the book. However if I assume that the solution for non-homogenous part is in the form $B$ , then $C_1 = -{2\over 3}$ and $C_2 = {2 \over 3}$ , which by the steps above results in: $$
x_n = {1\over 3} \cdot (2^{n+1} - 2\cdot(-1)^n) - 1
$$ being a match with the answer from the book. I'm trying to understand where it got wrong and why the answers are different. I could have assumed non-homogenous solution to be in various forms which I believe would still lead to some form of solution. Have I made a mistake in my first assumption?","['algebra-precalculus', 'recurrence-relations']"
2930375,Proving $\sum_{k=1}^n\vert\cos(k)\vert\ge \frac{n}{4}$ for all $n>0$,"Show that for all $n\in\mathbb{N}^* \quad \sum_{k=1}^n\vert\cos(k)\vert\ge \frac{n}{4}.$ I can do it using the fact that $\vert\cos(x)\vert\ge \cos(x)^2$ \begin{equation}
\sum_{k=1}^n\vert\cos(k)\vert\ge \sum_{k=1}^n\cos(k)^2=\frac{n}{2}+\frac1{2}\Re\big(e^{2i}\frac{1-e^{2in}}{1-e^{2i}}\big)\\
=\frac{n}{2}+\frac{1}{2}\Re\big(e^{i(n+1)}\frac{\sin(n)}{\sin{1}}\big)=\frac{n}{2}+\frac{\cos(n+1)\sin(n)}{2\sin(1)}\ge\frac{n}{2}-\frac{1}{2\sin(1)}
\end{equation} I can finish by using ""calculator"" or by hand but I would rather like a ""direct"" proof not using the value of $\sin(1).$ Other methods can be very interesting too.","['trigonometry', 'inequality', 'real-analysis']"
2930420,Why did the authors use $\prec$ instead of $\preccurlyeq$ in the definitions of $S_c$ and $S_{c^\ast}$?,"Recently, I have read the proof of theorem 5.3 from textbook Introduction to Set Theory by Karel Hrbacek and Thomas Jech. I would like to know why the authors didn't define $S_c=\{p\in P\mid p \prec c\}$ or $S_{c^\ast}=\{p\in P\mid p \prec^\ast c^\ast\}$ . I think that $S_c$ would not necessarily equal to $S_{c^\ast}$ as mentioned in the last line if the authors defined as I mentioned. 5.3 Theorem Let $(P, <)$ be a dense linearly ordered set without endpoints. Then there exists a complete linearly ordered set $(C,\prec)$ such that (a) $P\subseteq C$ . (b) If $p,q\in P$ then $p < q$ if and only if $p \prec q$ ( $<$ coincides with $\prec$ on $P$ ). (c) $P$ is dense in $C$ , i.e., for any $a,b\in C$ such that $a\prec b$ , there is $p\in P$ with $a\prec p \prec b$ . (d) $C$ does not have endpoints. Moreover, this complete linearly ordered set $(C,\prec)$ is unique up to isomorphism over $P$ . In other words, if $(C^\ast,\prec^\ast)$ is a complete linearly ordered set which satisfies (a)-(d), then there is an isomorphism $h$ between $(C,\prec)$ and $(C^\ast,\prec^\ast)$ such that $h(x) = x$ for each $x \in P$ . The linearly ordered set $(C,\prec)$ is called the completion of $(P, <)$ . As is usual in theorems of this type, the uniqueness part of the theorem is
  easier to prove. For that reason, we do it first. Proof of Uniqueness of Completion. Let $(C,\prec)$ and $(C^\ast,\prec^\ast)$ be two complete linearly ordered sets satisfying (a)-(d). We show that there is an isomorphism $h$ of $C$ onto $C^\ast$ such that $h(x)=x$ for each $x\in P$ . If $c\in C$ , let $S_c=\{p\in P\mid p \preccurlyeq  c\}$ . Similarly, let $S_{c^\ast}=\{p\in P\mid p\preccurlyeq^\ast c^\ast\}$ for $c^\ast\in C^\ast$ . If $S$ is a nonempty subset of $P$ bounded from above, let $\sup S$ be the supremum of $S$ in $(C,\prec)$ and $\sup^\ast S$ be the supremum of $S$ in $(C^\ast,\prec^\ast)$ . Notice that $\sup S_c=c$ , $\sup^\ast S_{c^\ast}=c^\ast$ . We define the mapping $h$ as follows: $h(c)=\sup^\ast S_c$ . Clearl, $h$ is a mapping of $C$ into $C^\ast$ ; we have to show that the mapping is onto and that (a) If $c\prec d$ , then $h(c)\prec^\ast h(d)$ . (b) $h(x)=x$ for each $x\in P$ . To show that $h$ is onto, let $c^\ast\in C^\ast$ be arbitrary. Then $c^\ast=\sup^\ast S_{c^\ast}$ , and if we let $c=\sup S_{c^\ast}$ , then $S_c=S_{c^\ast}$ and $c^\ast=h(c)$ . Below is the reasoning for my conclusion. Could you please have a check on my it? I would like to verify that $\sup S_c=c$ and $\sup^\ast S_{c^\ast}=c^\ast$ . Since $S_c=\{p\in P\mid p \preccurlyeq c\}$ , $\sup S_c\preccurlyeq c$ . If $\sup S_c\prec c$ , then there exists $p\in P$ such that $\sup S_c\prec p \prec c$ . Thus $p\in S_c$ and $\sup S_c\prec p$ . This is clearly a contradiction. Hence $\sup S_c= c$ . Similarly, we can prove that $\sup^\ast S_{c^\ast}=c^\ast$ . If we change the definition of $S_c$ to $S_c=\{p\in P\mid p \prec c\}$ and that of $S_{c^\ast}$ to $S_{c^\ast}=\{p\in P\mid p \prec^\ast c^\ast\}$ , we still have $\sup S_c=c$ and $\sup^\ast S_{c^\ast}=c^\ast$ . Since $S_c=\{p\in P\mid p \prec c\}$ , $\sup S_c\preccurlyeq c$ . If $\sup S_c\prec c$ , then there exists $p\in P$ such that $\sup S_c\prec p \prec c$ . Thus $p\in S_c$ and $\sup S_c\prec p$ . This is clearly a contradiction. Hence $\sup S_c= c$ . Similarly, we can prove that $\sup^\ast S_{c^\ast}=c^\ast$ . I would like to verify that $S_c=S_{c^\ast}$ as mentioned in the last line. For $c^\ast \in C^\ast$ , $S_{c^\ast}=\{p\in P\mid p \preccurlyeq^\ast c^\ast\}$ . Let $c=\sup S_{c^\ast}$ , then $c\in C$ . We have $S_c=\{p\in P\mid p \preccurlyeq c\}$ . Notice that $S_{c^\ast} \subseteq P$ and $S_c \subseteq P$ . For any $p\in S_c$ , $p \preccurlyeq c$ and thus $p \preccurlyeq \sup S_{c^\ast}$ . Then $p \preccurlyeq p'$ for some $p' \in S_{c^\ast}$ . If not, $p' \prec p$ for all $p' \in S_{c^\ast}$ and thus $p' \prec p \preccurlyeq \sup S_{c^\ast}$ for all $p' \in S_{c^\ast}$ . Then $p=\sup S_{c^\ast}$ and thus $S_c$ has only one element. This clearly contradicts the fact that $P$ does not have a least element. Hence $p \preccurlyeq p'\preccurlyeq^\ast c^\ast$ for some $p' \in S_{c^\ast}$ and consequently $p \preccurlyeq^\ast c^\ast$ . Thus $p\in S_{c^\ast}$ . To sum up, $p\in S_c \implies p\in S_{c^\ast}$ and thus $S_c\subseteq S_{c^\ast}$ . For any $p\in S_{c^\ast}$ , $p \preccurlyeq \sup S_{c^\ast}$ and thus $p \preccurlyeq c$ . Then $p\in S_c$ . Hence $S_{c^\ast} \subseteq S_c$ . As a result, $S_{c^\ast}=S_{c^\ast}$ . I'm quite curious about why the authors didn't define $S_c=\{p\in P\mid p \prec c\}$ or $S_{c^\ast}$ to $S_{c^\ast}=\{p\in P\mid p \prec^\ast c^\ast\}$ . I think that $S_c$ would not necessarily equal to $S_{c^\ast}$ as mentioned in the last line if the authors defined as such. For example, we instead define $S_c=\{p\in P\mid p \prec c\}$ and keep $S_{c^\ast}=\{p\in P\mid p \preccurlyeq^\ast c^\ast\}$ . I found that the first reasoning in 3. is still applicable here. Thus $S_c\subseteq S_{c^\ast}$ . But the second reasoning falls apart because: $p \preccurlyeq \sup S_{c^\ast}=c$ , but $p$ is not necessarily strictly less than $c$ . Hence $p$ does not necessarily belong to $S_c$ and consequently $S_{c^\ast}$ is not necessarily a subset of $S_c$ . To sum up, $S_{c^\ast}$ is not necessarily equal to $S_c$ . Similarly, we instead define $S_c=\{p\in P\mid p \prec c\}$ and $S_{c^\ast}=\{p\in P\mid p \prec^\ast c^\ast\}$ . Then $S_{c^\ast}$ is not necessarily equal to $S_c$ .","['elementary-set-theory', 'proof-explanation']"
2930438,"Find integral of $\operatorname{sinc}(\pi x)$ over range $x=[0,1]$","I want to find the area under a normalised $\operatorname{sinc}$ curve raised to the power of $2 k$ over the range $x=[0,1]$ where $k$ is a non-negative integer. In other words: $$\int_0^1 \operatorname{sinc}^{2k}(\pi x) dx$$ It is well-established that $\int_0^\infty \operatorname{sinc}(\pi x) dx=\frac{1}{2}$ . But I can't figure out how to take a definite integral within this more limited range, and with $\operatorname{sinc}$ raised to power $2k$ that I can then plug any chosen value of $k$ into. (I can use Mathematica to find integrals for a few specific values of $k$ , but I can't find a general solution.) Note that I don't really understand Fourier analysis, sorry. And things are likely to get hairy if I try to plug in a complex exponent because the definition that distinguishes $\operatorname{sinc}(\pi x)$ from $\frac{\sin (\pi x)}{\pi x}$ would be lost. Is it possible to create this general definite integral? How? EDIT: Also, what happens in the limit as $k \rightarrow\infty$ ? Presumably the value of the integral tends to $0$ ... Perhaps that's why there's no general expression for it?","['integration', 'trigonometry', 'definite-integrals']"
2930446,Countably additive finite signed measures form a Banach Space.,"I'm currently studying some topics in measure theory and I am not sure how to prove the following: Let $X$ a set, $\mathcal A$ a $\sigma$ -algebra on X. Consider the set: $$ca(\mathcal A) = \{\mu:\mathcal A \to \mathbb R|\; \mu \; \text{is a
 finite signed measure} \}$$ Note that $ca(\mathcal A)$ is a subspace of $l_\infty(\mathcal A)$ . I
  want to prove the following: $\|\mu\|\stackrel{def}{=} |\mu|(X)$ defines a norm on $ca(\mathcal A)$ ; $(ca(\mathcal A), \| \cdot \|)$ is a Banach Space The following inequality holds: $$\|\mu\|_\infty \leq \|\mu\| \leq 2\|\mu\|_\infty,$$ where $\|\mu\|_\infty\stackrel{def}{=}
 \sup\limits_{A\in \mathcal A} |\mu(A)|$ I was able to prove (1) using Hahn-Jordan decomposition and the inequality on (3) is straightforward. Although I could not prove (2). What I tried: If $(\mu_n)_{n\in \mathbb N}$ is Cauchy sequence on $ca(\mathcal A)$ , it follows from (3) that $(\mu_n)$ is point-wise convergent to $\nu\stackrel{def}{=} \lim \mu_n$ : Given $\varepsilon >0$ , there is $n_0 \geq 1$ such that: $$m>n \geq n_0 \implies \|(\mu_m - \mu_n)\|_\infty\leq \|\mu_m - \mu_n\| <\varepsilon.$$ Hence, for all $A\in \mathcal A$ , $(\mu_n(A))$ is convergent and $\nu$ is well defined. I am not sure how to prove that $\nu$ is a signed measure in $ca(\mathcal A)$ : $\nu(\emptyset)=\lim\mu_n(\emptyset) = 0$ ; Since $(\mu_n(X))$ is convergent sequence on $\mathbb R$ , it is bounded. So $\nu(X) = \lim \mu_n(X)$ is finite. Why, given a family $(A_i)_{i \in \mathbb N}\subset \mathcal A$ of disjoint sets, we have: $$\nu (\bigcup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty}\nu(A_i)$$ EDIT: I had an idea: Given a family $(A_i)_{i \in \mathbb N}\subset \mathcal A$ of disjoint sets, define $B_i = \bigcup\limits_{j=1}^i A_j$ . The family of $(B_i)$ is increasing, $\cup B_i = \cup A_i$ , and: \begin{align*} 
\nu(\bigcup_{i=1}^\infty A_i) &=  \nu(\bigcup_{i=1}^\infty B_i) \\ 
 &=  \lim_{i} \nu(B_i)\\
&= \lim_{i} \lim_{n} \mu_n (\bigcup_{j=1}^i A_j)\\
&= \lim_{i} \lim_{n} \sum_{j=1}^i \mu_n(A_j)\\
&= \lim_{i} \sum_{j=1}^i \lim_{n} \mu_n(A_j) \\
&= \sum_{i=1}^\infty \nu(A_i).
\end{align*} Can anyone check if this is correct? EDIT2: I forgot to check the convergence $\mu_n \stackrel{\|\cdot\|}{\to}\nu$ and I struggling with it.","['measure-theory', 'signed-measures', 'functional-analysis']"
2930470,Find all extreme points of 3 variable polyhedral set,"Find all the extreme points of the polyhedral set, $X=\{(x_1,x_2,x_3):x_1-x_2+x_3\leq 1, x_1-2x_2\leq 4, x_1,x_2,x_3\geq 0\}$ I usually start out by drawing the feasible region but I couldn't do it for this one because it has another variable $x_3.$ How should I go about it? Is it possible to find the extreme points without having to depend on a drawing?","['polyhedra', 'geometry', 'linear-programming']"
2930483,Dunford-Pettis-like theorem for $L^p$?,"The Dunford-Pettis theorem states that a family $\mathcal F\subset L^1(\Omega)$ is relatively weakly compact if and only if $\sup_{\mathcal f\in F}||f||_1 <\infty$ and $\mathcal F$ is equi-integrable, i.e. for $\varepsilon>0$ there exists $\delta>0$ such that $$
\int_A|f(x)|\,dx < \varepsilon
$$ for all $f\in \mathcal F$ whenever $|A|<\delta$ . Is there a way to characterize $p$ -equi-integrability of $\mathcal F\subset L^p(\Omega)$ in a similar manner? $p$ -equi-integrability means that for $\varepsilon>0$ there exists $\delta>0$ such that $$
\int_A|f(x)|^p\,dx < \varepsilon
$$ for all $f\in \mathcal F$ whenever $|A|<\delta$ . Edit: I am sorry if the question was worded poorly. What I want to characterize is $p$ -equi-integrability, not weak compactness in $L^p$ .","['measure-theory', 'real-analysis', 'functional-analysis', 'partial-differential-equations', 'probability']"
