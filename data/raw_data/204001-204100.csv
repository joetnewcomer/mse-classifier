question_id,title,body,tags
4035579,Composite function gradient,"Suppose I have smooth maps $g:R^m \rightarrow R^n$ and $f:R^n \rightarrow R$ . Then I think $$\nabla (f \circ g) = (\nabla g)^T (\nabla f) \circ g,$$ where $\nabla g$ is the $n \times m$ Jacobian matrix. This is the only plausible answer that makes the dimension work out, and if I use row vectors instead of columns, it nicely resembles the univariate chain rule: $$\nabla (f \circ g) = ((\nabla f) \circ g) (\nabla g).$$ But can anyone prove it? And is there some way this can work with column vectors that doesn't involve transposing the Jacobian? Thanks!","['multivariable-calculus', 'vector-analysis']"
4035583,"Show $A_{1}, \cdots, A_{n}$ is independent if and only if $P(A_{i_{1}} \cdots A_{i_{k}})=\prod_{j=1}^{k} P (A_{i_{j}})$","This is the second part of exercise question from book Probability for statistician . b) Show that $A_{1}, \cdots, A_{n}$ are independent if and only if $$
P\left(A_{i_{1}} \cdots A_{i_{k}}\right)=\prod_{j=1}^{k} P\left(A_{i_{j}}\right)
$$ whenever $1 \leq i_{1}<\cdots<i_{k} \leq n$ with $1 \leq k \leq n$ . In part a) we are asked to prove: Show that $P(A B)=P(A) P(B)$ if and only if $\left\{\emptyset, A, A^{c}, \Omega\right\}$ and $\left\{\emptyset, B, B^{c}, \Omega\right\}$ are independent $\sigma$ -fields. Before the exercise, the book states the definition of independent $\sigma$ -field and independent event as follow: Consider various sub $\sigma$ -fields of $\mathcal{A}$ . Call such $\sigma$ -fields $\mathcal{A}_{1}, \cdots, \mathcal{A}_{n}$ independent $\sigma$ -fields if they satisfy $$
P\left(A_{1} \cap \cdots \cap A_{n}\right)=\prod_{1}^{n} P\left(A_{i}\right)
$$ whenever $A_{i} \in \mathcal{A}_{i}$ for $1 \leq i \leq n$ . Events $A_{1}, \cdots, A_{n}$ are independent events if $\sigma\left[A_{1}\right], \cdots$ , $\sigma\left[A_{n}\right]$ are independent $\sigma$ -fields; here we let $$
\sigma\left[A_{i}\right] \equiv\left\{\emptyset, A_{i}, A_{i}^{c}, \Omega\right\}
$$ My idea is that $P\left(A_{i_{1}} \cdots A_{i_{k}}\right)=\prod_{j=1}^{k} P\left(A_{i_{j}}\right)$ implies the $\sigma$ -field generated by ay $A_{i_{1}} \cdots A_{i_{k}}$ are independent from a) part of the question, hence $A_{1}, \cdots, A_{n}$ are independent but I am not sure how to write the proof rigorously. i.e. $P\left(A_{i_{1}} \cdots A_{i_{k}}\right)=\prod_{j=1}^{k} P\left(A_{i_{j}}\right)$ whenever $1 \leq i_{1}<\cdots<i_{k} \leq n$ with $1 \leq k \leq n $ implies $\sigma[A_{i_j}]$ are independent for any $1\leq i_j\leq n$ $\Rightarrow$ $\sigma[A_i]$ are independent for each $n\geq 2$ . It seems the $\Rightarrow$ has lots of gaps within it. Could someone please address this question for me or at least gives me some clues to solve the problem? Also, the first part only involves two events while in second part there are $n$ events. Usually statement that satifies in $n=2$ case can be extended to $n=N$ where $N$ is a finite number but I don't know how the extension can be proved.","['measure-theory', 'independence', 'probability-theory']"
4035585,$L^p$ convergence implies $\int_{\mathbb{R}^d \setminus E} |f_n|^p < \epsilon$ for all $n$.,"Question. Suppose $f_n, f \in L^p(\mathbb{R}^d)$ such that $f_n \to f$ in $L^p$ -norm. Is it necessarily true that for all $n \in \mathbb{N}$ , and any $\epsilon > 0$ , there exists a $E \subset \mathbb{R}^d$ with finite measure $\mu(E) < \infty$ such that $\int_{\mathbb{R}^d \setminus E} |f_n|^p < \epsilon$ . I believe this is true. However, I'm a little unsure of how to rigorously prove. What I have: Since $E$ is measurable, then $F = \mathbb{R}^d \setminus E$ is also measurable. Then $$\int_F |f_n|^p = \int_F |f_n - f + f|^p \leq \int_F |f_n - f|^p + \int_F |f|^p.$$ Since $f_n \to f$ in $L^p$ -norm, for each $\epsilon > 0$ there exists an $N \in \mathbb{N}$ such that $$\int_F |f_n - f|^p < \frac{\epsilon}{2},$$ whenever $n \geq N$ . Further, $f$ is integrable. Meaning there exists some $\epsilon > 0$ such that $$\int_F |f|^p < \frac{\epsilon}{2}.$$ Thus, for $n \geq N$ , $$\int_F |f_n|^p < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon.$$ But how does one proceed when $n < N$ ?","['measure-theory', 'lp-spaces', 'lebesgue-integral']"
4035605,The average number of handshakes between people around a circle.,"$5$ people are around a circle. At a moment and at the same time each
person extend their hand to one of their adjacent person. people who
extended their hand to each other, shaking their hands. In average
between different possibilities how many hand shakings are done? $1)\frac32\quad\quad\quad\quad\quad2)1\quad\quad\quad\quad\quad3)\frac23\quad\quad\quad\quad\quad4)\frac54\quad\quad\quad\quad\quad5)\frac12$ First I noticed that for $5$ people we have $2^5=32$ different ways of extending hands (from this we can see third choice $\frac23$ can't be the answer). also maximum number of handshakes is $2$ and it happens in five different cases (at each case there is a person who is not shaking a hand). also minimum number of hand shaking is zero which happens in two cases (all persons extend their hand in a clockwise or counterclockwise directions). so I conclude there are $32-5-2=25$ cases with only one handshake hence the average is $\cfrac{2\times5+25\times1}{32}=\cfrac{35}{32}$ . but I don't have this in the choices.",['combinatorics']
4035633,"Analog of $C^{\infty}$ multiplication for discrete ""vector fields""","To make the problem simple, consider an undirected graph $G$ with vertices $V$ and edges $E$ . A discrete ""scalar function"" (0-cochain) can be defined on the vertices, taking values $f(v)$ on each vertex $v$ . A discrete ""1-form"" (1-cochain) can be defined as an anti-symmetric function on pairs of vertices $u(v_1,v_2) = -u(v_2,v_1)$ whose value is 0 if $v_1$ and $v_2$ are not connected by an edge. We can naturally think of the non-zero values of $u$ as being encoded on the edges of the graph supplemented with an orientation of each bond, i.e. $u$ is essentially a weighted, directed version of $G$ . For example, by choosing an orientation of each bond one could define $u$ to be the finite difference gradient of $f$ . For concreteness, I am considering $f$ and $u$ to be real or complex valued. A lot of intuitive ideas from differential geometry can be brought over (and generalized further if we consider for example simplicial complexes instead of graphs), like boundary/coboundary etc. My question is, is there a natural analog of $C^\infty$ multiplication in the discrete case? I don't see an obvious way to achieve this because the scalar functions are defined on vertices while the discrete 1-cochains are defined on the edges, so we can't just multiply them pointwise like we can on a manifold.","['differential-geometry', 'graph-theory', 'discrete-mathematics', 'algebraic-topology', 'simplicial-complex']"
4035639,"How to show that $\int \lambda((A-x)\cap B) \, d\lambda = \lambda(A)\lambda(B)$?","Given measurable sets $A, B\subset\mathbb R$ , how can we show that $$\int_\mathbb R \lambda((A-x)\cap B) \, d\lambda = \lambda(A)\lambda(B)$$ holds? I don't see how the integrand might be simplified. The Lebesgue-measure is invariant under translation, so it would be sufficient to show that $\lambda((A-x)\cap B)=\lambda(A-x) \chi_B$ somehow, but I don't know how that might be argued. Is there an argument that allows that transformation? Or is there a better way entirely?","['integration', 'measure-theory', 'lebesgue-measure', 'lebesgue-integral']"
4035664,What is the covering density of a very thin annulus? Is it $\frac{\pi\sqrt{51\sqrt{17}-107}}{16}$?,"Take some very small $\epsilon>0$ , and consider the annulus/ring given by the set $\{(r,\theta)\ |\ 1-\epsilon\le r\le1\}\subset \mathbb{R}^2$ . We wish to place translated copies of this annulus down so that they cover the plane; obviously, this will cover some points multiple times, since the rings do not tile the plane without overlap. How can we do this to minimize the overall density, i.e., the number of times an average point is covered? I can obtain a density of $\pi$ with the following construction (overlapping rings shown in darker shades): However, it turns out this is suboptimal; we can do better by only placing $2/\epsilon$ of these rings in a line, and covering the plane with the resulting shapes: This uses $2/\epsilon$ rings of area $2\pi\epsilon$ each, for a total area of $4\pi$ , per $2\times 3$ rectangle in the tiling, so its density is $\frac{2\pi}3 \approx 2.094$ . We can improve this further by overlapping the above shapes vertically (as before, each of these is formed from $2/\epsilon$ rings): A bit of calculus tells us this construction is optimized when the vertical overlap between two of the red regions is $2-\sqrt{\frac{3\sqrt{17}-5}2}$ , for a total density of $$\frac{\pi\sqrt{51\sqrt{17}-107}}{16}\approx 1.99954$$ Is this optimal? I'm curious about both improvements to this construction, and lower bounds that can be imposed on the density; so far I have not been able to establish lower bounds greater than $1$ . Pointers to literature on this or related questions would also be welcome. (It's fairly easy to show that a random point on a given annulus will be covered an average of at least $1+1/\pi$ times, but this oversamples multiply-covered points, so it doesn't tell us anything directly about the covering density.)","['geometry', 'asymptotics', 'reference-request']"
4035675,Maximal element of a poset,"I am currently studying Zorn's lemma and its my understanding that the definition of a maximal element is a primitive to the lemma. From my understanding a maximal element $M$ of a partially ordered set $(S,<)$ is an element $M \in S$ where, if $M \leq x $ for some $ x \in S$ , then $x=m$ . This easilty illustrated when the set is {∅, {1}, {2}, {3}, {1, 2}}. Then the maximal elements are {3} and {1,2}. But what if the partially ordered set i am considering is $(S,R)$ .
So for example let S=P({a,b,c}) which is the power set of {a,b,c} and the relation $\subseteq$ .
Now whats is the maximal element in this set? Thank you in advance.","['elementary-set-theory', 'order-theory']"
4035676,Solve this limit $\lim\limits_{x\to 1}\left(\dfrac{2017}{1-x^{2017}} -\dfrac{2018}{1-x^{2018}}\right)$,"\begin{align}
\lim\limits_{x\to 1}\left(\dfrac{2017}{1-x^{2017}} -\dfrac{2018}{1-x^{2018}}\right)
&= \lim\limits_{x\to 1}\dfrac{2017(x^{2017} + \dots + 1) - 2018(x^{2016} + \dots + 1)}{(1-x)(x^{2016} + \dots +1)(x^{2017} + \dots +1)}\\\\
&= \lim\limits_{x\to 1}\dfrac{2017x^{2017} - x^{2016} - \dots - 1}{(1-x)(x^{2016} + \dots +1)(x^{2017} + \dots +1)}
\end{align} Did I do the right way with those step above? I think next step is to separate: $2017x^{2017} = \underbrace{x^{2017} + \dots + x^{2017}}_{\text{2017 addends}}$ Then combine with the rest and factorize $(1-x)$ ,
but it has a little confused after factorizing. Please help me!!?","['limits', 'calculus']"
4035720,Changing the time in Brownian motion with a function?,"Let $(B_t)$ be a Brownian motion on some probability space. Let $f:\mathbb R \rightarrow \mathbb R^+$ be a continuous function different from the identity. What is "" $B_{f(t)}$ "" ? Clearly it is not a Brownian motion anymore. How can we rigorously define it and show that it exists (or not)? What is there to know about "" $B_{f(t)}$ "" ? Does it even make sense if $f$ is not monotone ? Can we determine its distribution ? Can we write it as $B_{f(t)}=g(B_t)$ for some function $g$ ?  How can we simulate one starting from $B_t$ ?","['stochastic-analysis', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4035728,Exercise 1.2.1 in Shafarevich's Basic Algebraic Geometry I,"I'm working on the following question from Shafarevich's Basic Algebraic Geometry I and wanted to check that I understand what's going on. The set $X\subset{\mathbb A}^2$ is defined by the equation $f:x^2+y^2=1$ and $g:x=1$ . Find the ideal $\mathfrak{U}_X$ . Is it true that $\mathfrak{U}_X = (f,g)$ ? First, is $X$ defined as the common zeros of $f$ and $g$ ? That is what I would assume but in this case, $X$ would only be a single point $(1,0)$ and that doesn't sound right for this problem. If $X$ just is a single point, I'm not exactly sure what $\mathfrak{U}_X$ is. I would be surprised if it was $(f,g)$ but I am not sure how to prove that there are more polynomials in $\mathfrak{U}_X$ than there are in $(f,g)$ . Could I say for example that $y$ is not in $(f,g)$ because elements of $(f,g)$ have even degrees as polynomials with coefficients in $k[x]$ ?","['algebraic-curves', 'algebraic-geometry', 'affine-varieties']"
4035782,Is there any specific meaning of the formula 2*(max-min)/(max+min)?,"I'm a student who studies machine learning. When I studied this paper( https://dl.acm.org/doi/pdf/10.1145/335191.335388 ), I saw the following formula and explanation. $$(direct_{max} - direct_{min})/direct_{mean} = (indirect_{max} - indirect_{min})/indirect_{mean}$$ The $direct_{mean}$ is the mean value of $direct_{max}$ and $direct_{min}$ . And $indirect_{mean}$ is also the mean value of $indirect_{max}$ and $indirect_{min}$ . In this case, that paper said the fluctuate amount is the same between direct and indirect. But I don't understand one thing. Is there any specific meaning of the below formula? $$2* (value_{max} - value_{min})/(value_{max} + value_{min})$$ I don't know why it has the meaning as the amount of fluctuating. Thank you for reading my question.",['statistics']
4035842,Convergence of $\sum \frac{ \cos\sqrt n} {\sqrt n}$,"As you undestand from the topic title, I am wondering how to determine whether the series $$
\sum_{n=1}^{\infty} \frac{ \cos\sqrt n} {\sqrt n}
$$ converges or not(it diverges actually). You know, the problem is, if it would be something like $$
\sum_{n=1}^{\infty} \frac{ \cos n} {\sqrt n}
$$ Then the task can be easily solved using Dirichlet's test: we just need to show that $|\sum_{n=1}^{N} \cos n| \le K$ for all $N$ having fixed $K$ . This was described here Also, if it was like $$
\sum_{n=1}^{\infty} \frac{ \cos\sqrt n} {n}
$$ then we are in much more complicated situation. Every reasonable solution I found on the internet involves approximating the series with an integral. This type of task implies very cute mathematical background(which I don't have:)) In my school we didn't study improper integrals(even just integrals). And actually our problem distincts from two others described above: $$
\sum_{n=1}^{\infty} \frac{ \cos\sqrt n} {\sqrt n}
$$ This is why I believe there must be a solution which is simpler than using improper integrals. We studied lots of different convergence tests at school, but they seems useless here. Well, actually I think we should apply them, but after some mathematical magic.
Also I tried to expand terms using Taylor's formula, but it didn't help. Any ideas?","['convergence-divergence', 'calculus', 'sequences-and-series', 'real-analysis']"
4035845,What to think about this limit trick $\lim_{x\to 1}f(x)=\lim_{x\to 1}f(\frac 1x)$?,"While searching for duplicates to a question asked recently I stumbled onto this specific answer: Show that $\lim\limits_{x\to 1}\left(\frac p{1-x^p}-\frac q{1-x^q}\right)=\frac {p-q}2$ I reproduce the sketch here: $$L=\lim\limits_{x\to 1}\left(\frac p{1-x^p}-\frac q{1-x^q}\right)=\lim\limits_{x\to 1}\left(\frac p{1-(\frac 1x)^p}-\frac q{1-(\frac 1x)^q}\right)=\lim\limits_{x\to 1}\left(\frac {-px^p}{1-x^p}-\frac {-qx^q}{1-x^q}\right)$$ So by adding first and third term we get $2L=p-q$ . What annoys me is that it looks like an answer we could put in the ""bad math that gets away with it"" thread... It is true that $\lim\limits_{x\to 1}f(x)=\lim\limits_{x\to 1}f(\frac 1x)$ when the limit exists, however here, I think it is part of the problem to show that this limit actually exists. Looking at it more closely we have that: $f_p(x)=\dfrac{p}{1-x^p}=\frac 1{1-x}+\frac{n-1}2+O(1-x)$ $f_p(\frac 1x)=\dfrac{p}{1-(\frac 1x)^p}=-\frac 1{1-x}+\frac{n+1}2+O(1-x)$ So if we were to apply the same process to $\ell_p=\lim\limits_{x\to 1}\dfrac{p}{1-x^p}$ we would get $2\ell_p=p\iff \ell_p=\frac p2$ which is wrong since $1$ is a pole here (limit does not exist). But the solution proposed in the cited answer let us think that we got the result from $\ell_p-\ell _q=\frac p2-\frac q2=\frac{p-q}2$ While in reality we get it from cancellation of the divergent parts in: $\require{cancel}f_p(x)-f_q(x)=\left(\cancel{\frac 1{1-x}}+\frac 12(p-1)+O(1-x)\right)-\left(\cancel{\frac 1{1-x}}+\frac 12(q-1)+O(1-x)\right)=\frac{(p-\cancel{1})-(q-\cancel{1})}2+O(1-x)\to \frac{p-q}2$ So what is your opinion on this answer, do you agree with my point of view ? How would you convince a student that does this, that his method is flawed, or at least that he should prove first that the limit exists before carrying the $\lim$ operator from equality to equality like this (which BTW seems to be a very common habit among posters...).","['limits', 'proof-writing', 'soft-question']"
4035850,"The nonexistence of any ""good"" norm on $\ell^\infty(\mathbb R)$","I'd like to prove by counterexample that there is no norm on the space of all real-valued bounded sequences such that $\lim_{n \to \infty} \|f_n - f\| = 0$ is equivalent to $\lim_{n \to \infty} f_n = f$ . In other words, I want to find a sequence $(f_n)_{n\in \mathbb N} \subset \ell^\infty(\mathbb R)$ which converges to $f$ (pointwise) but for which $$
\lim_{n \to \infty} \|f_n - f\| \neq 0.
$$ This question is inspired by the paper A Note on Pointwise Convergence , which proves that pointwise convergence is not ""metrizable"" on $\mathcal{C}[0,1].$ I haven't been able to find any proofs that this result holds in the space of bounded sequences, but since the space is not even complete under any norm by Baire, I'm quite sure a counterexample exists. I'd really appreciate any ideas y'all have.","['normed-spaces', 'functional-analysis', 'examples-counterexamples']"
4035893,Proof for $\alpha \times \beta \cong \alpha \otimes \beta$,"I need to prove the following theorem: Let $\alpha$ and $\beta$ be ordinals. Then $\alpha \times \beta \cong \alpha \otimes \beta$ , where $\alpha \times \beta$ is in anti-lexicographic order. I have come up with one proof, and I would like to get it validated. Proof. The case $\alpha = \boldsymbol{0}$ is trivial, as $\alpha \times \beta = \alpha \otimes \beta = \boldsymbol{0}$ . We shall assume $\alpha \neq \boldsymbol{0}$ in our following argument. Suppose that $\delta$ is an ordinal. Fix an ordinal $\alpha \neq \boldsymbol{0}$ and define the following set: \begin{equation*}
A = \left\{\beta \in \delta: \alpha \times \beta \cong \alpha \otimes \beta\right\}.
\end{equation*} First consider the case $\beta = \boldsymbol{0}$ . As \begin{equation*}
\alpha \times \boldsymbol{0} = \alpha \otimes \boldsymbol{0} = \boldsymbol{0},
\end{equation*} it is obvious that $\boldsymbol{0} \in A$ . Next assume that $\gamma \subseteq A$ and $\gamma \neq \emptyset$ . That is, for any $\theta \in \gamma$ , $\alpha \times \theta \cong \alpha \otimes \theta$ . Suppose $\gamma = {\theta_{0}}^{+}$ for some $\theta_{0} \in \gamma$ . Then \begin{equation*}
\alpha \times \gamma = \alpha \times {\theta_{0}}^{+} = \alpha \times \left(\theta_{0} \cup \left\{\theta_{0}\right\}\right) = \left(\alpha \times \theta_{0}\right) \cup \left(\alpha \times \left\{\theta_{0}\right\}\right),
\end{equation*} and \begin{equation*}
\alpha \otimes \gamma = \alpha \otimes {\theta_{0}}^{+} = \left(\alpha \otimes \theta_{0}\right) \oplus \alpha.
\end{equation*} Using our induction hypothesis, we have $\alpha \times \theta_{0} \cong \alpha \otimes \theta_{0}$ . Thus, there is an order-isomorphism $f_{1}: \alpha \otimes \theta_{0} \to \alpha \times \theta_{0}$ . Next, assume that $\lambda \in \alpha$ . We immediately have $\left(\alpha \otimes \theta_{0}\right) \oplus \lambda \not\in \alpha \otimes \theta_{0}$ . There is an order-isomorphism $f_{2}: \left\{\left(\alpha \otimes \theta_{0}\right) \oplus \lambda: \lambda \in \alpha\right\} \to \alpha \times \left\{\theta_{0}\right\}$ such that \begin{equation*}
\forall \lambda \in \alpha, f_{2}\left(\alpha \otimes \theta_{0} \oplus \lambda\right) = \left(\lambda,\theta_{0}\right).
\end{equation*} Define a function $f_{3}: \left(\alpha \otimes \theta_{0}\right) \oplus \alpha \to \left(\alpha \times \theta_{0}\right) \cup \left(\alpha \times \left\{\theta_{0}\right\}\right)$ such that \begin{equation*}
f_{3}\left(x\right) = 
\begin{cases}
f_{1}\left(x\right),\ x \in \alpha \otimes \theta_{0}\\
f_{2}\left(x\right),\ x \in \left(\alpha \otimes \theta_{0}\right) \oplus \alpha \textrm{ and } x \not\in \alpha \otimes \theta_{0}
\end{cases}.
\end{equation*} In this way, we have defined an order-isomorphism $f_{3}: \alpha \otimes \gamma \to \alpha \times \gamma$ , and we have $\alpha \otimes \gamma \cong \alpha \times \gamma$ . Thus, we may conclude that $\gamma \subseteq A$ leads to $\gamma \in A$ when $\gamma$ is a successor ordinal. Now suppose that $\gamma$ is a limit ordinal. Then according to definition, we have \begin{equation*}
\alpha \otimes \gamma = \bigcup\left\{\alpha \otimes \lambda: \lambda \in \gamma\right\},
\end{equation*} and further, \begin{equation*}
\alpha \times \gamma = \alpha \times \bigcup{\gamma} = \bigcup\left\{\alpha \times \lambda: \lambda \in \gamma\right\}.
\end{equation*} According to our induction hypothesis, for any $\lambda \in \gamma$ , we have $\alpha \otimes \lambda \cong \alpha \times \lambda$ . Thus, there is a order-isomorphism $f_{\lambda}: \alpha \otimes \lambda \to \alpha \times \lambda$ for every $\lambda \in \gamma$ . We may define the functions so that $f_{\lambda_{1}} = f_{\lambda_{2}}\vert_{\lambda_{1}}$ , if $\lambda_{1} \in \lambda_{2} \in \gamma$ (so that $\lambda_{1} \subseteq \lambda_{2}$ ). Define a set \begin{equation*}
g = \left\{f_{\lambda} \in \mathscr{P}\left(\left(\alpha \otimes \gamma\right) \times \left(\alpha \times \lambda\right)\right): \lambda \in \gamma\right\},
\end{equation*} and further another set \begin{equation*}
f = \bigcup g.
\end{equation*} We first argue that $f$ is a function. If it is not, then there exist at least two 2-tuples $\left(x_{0},y_{1}\right), \left(x_{0},y_{2}\right) \in f$ with $y_{1} \neq y_{2}$ . Then there are $\lambda_{1}, \lambda_{2} \in \gamma$ such that $\lambda_{1} \neq \lambda_{2}$ and $\left(x_{0},y_{1}\right) \in f_{\lambda_{1}}$ and $\left(x_{0},y_{2}\right) \in f_{\lambda_{2}}$ . Without loss of generality, we assume $\lambda_{1} \in \lambda_{2}$ . As $f_{\lambda_{1}} = f_{\lambda_{2}}\vert_{\lambda_{1}}$ , we have $y_{1} = y_{2}$ . This contradicts our assumption for $y_{1}$ and $y_{2}$ . Thus, $f$ is a function. We then argue that $\operatorname{Domain}\left(f\right) = \alpha \otimes \gamma$ and $\operatorname{Range}\left(f\right) = \alpha \times \gamma$ . This step is straightforward, as \begin{equation*}
\operatorname{Domain}\left(f\right) = \bigcup\left\{\alpha \otimes \lambda: \lambda \in \gamma\right\} = \alpha \otimes \gamma,
\end{equation*} and \begin{equation*}
\operatorname{Range}\left(f\right) = \bigcup\left\{\alpha \times \lambda: \lambda \in \gamma\right\} = \alpha \times \bigcup \gamma = \alpha \times \gamma.
\end{equation*} We shall finally argue that $f$ is an order-isomorphism. Assume $\theta_{1} \in \theta_{2} \in \alpha \otimes \gamma$ . Then there is $\lambda_{0} \in \gamma$ such that $\theta_{1}, \theta_{2} \in \alpha \otimes \lambda_{0}$ . As $f_{\lambda_{0}}\left(\theta_{1}\right) < f_{\lambda_{0}}\left(\theta_{2}\right)$ , we have $f_{\lambda}\left(\theta_{1}\right) < f_{\lambda}\left(\theta_{2}\right)$ . We conclude that $\alpha \otimes \gamma \cong \alpha \times \gamma$ , and that $\gamma \subseteq A$ leads to $\gamma \in A$ when $\gamma$ is a limit ordinal. Finally, we have $A = \delta$ , according to transfinite induction for ordinals.","['elementary-set-theory', 'solution-verification']"
4035938,Closed form of $\sum_{n=-\infty}^\infty \frac{(-1)^n}{\sinh (z+n)}$?,"I considered the following function: $$f:\, \mathbb{C}\mapsto\mathbb{C}_{\infty},\, z\mapsto \left(\sum_{n=-\infty}^\infty \frac{(-1)^n}{\sinh (z+n)}\right)^{-1}.$$ It can be seen that $f(z)=0$ at every integer (division of a non-zero complex number by zero is assumed to be complex infinity, so its reciprocal is $0$ ). It also seems that the function is periodic on the real axis with period $2$ , i.e. $$\forall x\in\mathbb{R}:\, f(x+2)=f(x),$$ though I was not able to prove that rigorously. I tried to use the $\sinh$ addition formula, but that's probably not useful. On the whole real axis, $f$ is very close to $$g:\, \mathbb{C}\mapsto \mathbb{C},\, z\mapsto 0.31837572\sin \pi z,$$ but on the imaginary axis, there's a big difference between $f$ and $g$ , for instance: $$f(2i)\approx 5.3796i,\,\text{but}\, g(2i)\approx 85.2499i.$$ Why is that? Can a closed form of $f$ in terms of Weierstrass/Jacobi elliptic functions be found? Note: Curiously, any $$h:\,\mathbb{R}\mapsto\mathbb{R},\, x\mapsto \left(\sum_{n=-\infty}^\infty \frac{(-1)^n}{a^{x+n}-a^{-x-n}}\right)^{-1}$$ where $a\gt 0$ and $a\ne 1$ seems to be $2$ -periodic... Edit: The function $$f:\, \mathbb{C}\mapsto\mathbb{C}_{\infty},\, z\mapsto \left(\sum_{n=-\infty}^\infty \frac{(-1)^n}{\sinh (z+n)}\right)^{-1}$$ is probably a doubly-periodic elliptic function with periods $2$ and $2\pi i$ , so I added an appropriate tag.","['complex-analysis', 'elliptic-functions', 'closed-form', 'sequences-and-series']"
4035968,Is there a notation for the inverse image function itself?,"Let $f$ be a function. If $B \subset im(f)$ then the inverse image of $B$ under $f$ in the current standard notation is $f^{-1}(B) = \{x \in dom(f) : f(x) \in B\}$ . But $f^{-1}$ does not denote the inverse image function ! Using a kind of the typed lambda notation the inverse image function
-- the name $preim$ (from preimage) may be used - can be defined as follows: $preim := F[B \in P(im(f)), \{x \in dom(f): f(x) \in B \}]$ . But in the common case (if $f$ is not necessary bijective): $f^{-1} = F[y \in im(f)), \{x \in dom(f): f(x) = y \}]$ .","['elementary-set-theory', 'notation']"
4035977,Evaluate an absolute monster integral $\int\limits_{0}^{1} \frac{\log(1-x+x^2)}{\sqrt{x}(1+x)}\mathrm{d}x.$,"I want to figure out a way to evaluate $$\int\limits_{0}^{1} \frac{\log(1-x+x^2)}{\sqrt{x}(1+x)}\mathrm{d}x.$$ I tried to substitute $x = u^2$ and cancel the square root in the denominator, getting $$2\int\limits_{0}^{1} \frac{\log(1-u^2+u^4)}{1+u^2}\mathrm{d}u.$$ But now I am stuck again.","['integration', 'improper-integrals']"
4035998,Confusion about lift in the context of tangent bundle,"A lift is defined here: https://mathworld.wolfram.com/Lift.html as a tangent vector field $X$ on a manifold, the same way that a section of the tangent bundle gives us $X$ in the context $\dot g = X(g)$ . This answer to a different question: https://mathoverflow.net/a/111198/172470 suggests that a path $g$ can be lifted into the tangent bundle space as $\tilde g \in T_x M$ , and is a smooth curve there. I am confused about this, because of two reasons: Isn't $T_x M$ the set of vectors tangent to a specific point $x$ (the ""tangent space"" at $x$ )? How can we have a curve there, especially a closed loop, like the cited answer suggests? If we can, how is this lift a vector field? Thanks for any help you can give, including any clarification of definitions I may be wrong about. Pictures are nice too since english isn't my second language, and I've already gone down the wrong road with some ideas in differential geometry because I had formed the wrong picture. Thank you!","['differential-topology', 'differential-geometry']"
4036064,Bounded operator such that image of ball is $(1-\varepsilon)$-dense is surjective.,"This is not a homework problem, I found it in some notes and I am curious on how to prove it. The statement is as follows: Let $T:X\to Y$ be a bounded linear operator between Banach spaces. If there is $\varepsilon>0$ and $R<\infty$ such that $T(B_X(R))$ is $(1-\varepsilon)$ -dense in $B_Y(1)$ then $T$ is surjective. Here $(1-\varepsilon)$ -dense means that for every point in $B_Y(1)$ the ball of radius $(1-\varepsilon)$ intersects $T(B_X(R))$ . What I've thought so far: It is enough to show that $T(B_X(R))$ contains some ball around $0\in Y$ . Assume that this is not the case so that for every $n$ the ball $B_Y(\frac{1}{n})$ is not contained in $T(B_X(R))$ . By the $(1-\varepsilon)$ -density we have that for any $y\in B_Y({\frac{1}{n}})\setminus T(B_X(R))$ there is some $z\in T(B_X(R))$ such that $d(y,z)<(1-\varepsilon)$ . Furthermore, we have $d(0,z)\le d(0,y)+d(y,z)<\frac{1}{n}+(1-\varepsilon)$ . If we let $n$ be large enough so that $(1-\varepsilon)+\frac{1}{n}<1$ then $z\in B_Y(1)$ . I don't know if this works or makes any sense and I don't know how to continue from here. Any help, hints or solutions would be appreciated.","['functional-analysis', 'analysis']"
4036081,"Why is $\int_0^t g(s)dB_s \sim \mathcal{N}\left(0, \int_0^t g^2(s) ds\right)$","I've found in different references that $\int_0^t g(s)dB_s$ is a Wiener integral, where $g(s)$ is a bounded, continuous function and $dB_s$ is the standard Brownian Motion. Also, that $\int_0^t g(s)dB_s \sim \mathcal{N}(0, \int_0^t g^2(s) ds)$ . Why is this? My attempt so far, is to approximate it as a series of normal random variables. Since the increments $B_{t_{i+1}}-B_{t_i}$ have a distribution $\mathcal{N}(0, t_{i+1}-t_i)$ and they are pairwise independent. Then, thinking it as a Riemann integral $\sum_{i=0}^{\infty} g_i(s) (B_{t_{i+1}}-B_{t_i}) \sim \mathcal{N}(0, \sum_{i=0}^{\infty} g_i^2(s)(t_{i+1}-t_i))$ for some $s\in [t_i, t_{i+1}]$ . My question is, is it posible to take the limit $\lim_{t_{i+1} \to t_i} \sum_{i=0}^{\infty} g_i^2(s) (t_{i+1}-t_i) = \int_0^t g^2(s)ds$ in this case? Why?","['brownian-motion', 'normal-distribution', 'probability', 'stochastic-calculus']"
4036083,Writing Back-Propogation out in terms of Matrix Multiplication,"I am trying to write my own backpropogation algorithm for neural networks for a class. For any specific weight of my network I could easily take the derivative, but for computational speed I want to write the derivatives out in terms of matrix expressions that could be simply coded and executed. Let us use the following notation: let $x\in\mathbb{R}^p$ be our set of $p$ inputs to our network. Let us have $m$ hidden layers of nodes each consisting of $n$ nodes. We will write the values at the $i^{th}$ hidden layer as $z_i$ , which are retieved from multipling the $i^{th}$ weight matrix $W_i$ by the previous layer a chosen function $f$ (sigmoid, RELU, etc). Finally, we consider only $1$ output node (since we could loop through the output nodes and do this for all of them, although it'd be nice to generalize). As such we can say that our network is given as $$
z_1 = f(W_1 x)
$$ $$
z_2 = f(W_2 z_1)
$$ $$
\vdots
$$ $$
z_n = f(W_n z_{n-1})
$$ $$
\hat{y} = f(W_{n+1} z_n)
$$ Using Mean Squared Error we get that our objective function for a given point is $$
MSE = (\hat{y} - y)^2
$$ The first update matrix (which is actually a vector) was fairly easy to find the update for. I found that $$
\frac{\partial(MSE)}{\partial W_{n+1}} = 2(\hat{y}-y)f'(W_{n+1}z_n) z_n = k_n z_n
$$ where $k_n=2(\hat{y}-y)f'(W_{n+1}z_n)$ . From here the next layer is also special I found that if we let $$
\frac{\partial(MSE)}{\partial W_n} = k_n (W_{n+1}\circ f'(W_n z_{n-1}))\otimes z_{n-1} = k_{n-1}\otimes z_{n-1}
$$ where $\circ$ represents the element wise multiplication and $\otimes$ represents the outer product. The final formula I get is the one that should (in theory) work for every layer following, however I am getting a code error on the final step that the matrix dimensions I have do not properly line up, which means I must have a problem with my formula. I find that if $$
k_i = (k_{i+1}^T W_i)\circ f'(W_i z_{i-1}) 
$$ then we can say that $$
\frac{\partial (MSE)}{\partial W_i} = k_i \otimes z_{i-1}
$$ If anyone can help me find an error that'd be much appreicated!","['matrices', 'machine-learning', 'multivariable-calculus', 'calculus', 'matrix-calculus']"
4036125,Show $\lim\limits_{x\to0}x\ln(x)=0$ by using Taylor series,"Is it possible to show the limit $\lim\limits_{x\to0}x\ln(x)=0$ by expanding the corresponding Taylor series? My approach: I know that the Taylor series of $\ln(x)$ converges uniformly on a closed interval $(1-\delta,1+\delta)$ where $\delta>0$ is sufficiently small. So let be $$S_n(x):=\sum\limits_{k=1}^n(-1)^{k+1}(x-1)^k\frac{1}{k}\\
\lim\limits_{n\to\infty}S_n(x)=\sum\limits_{k=1}^{\infty}(-1)^{k+1}(x-1)^k\frac{1}{k}=\ln(x)\\
T_n(x):=(x-1)+\sum\limits_{k=2}^n(-1)^{k}(x-1)^k\frac{1}{(k-1)k}\\
\lim\limits_{n\to\infty}T_n(x)=(x-1)+\sum\limits_{k=2}^{\infty}(-1)^{k}(x-1)^k\frac{1}{(k-1)k}\overset{?}{=}x\ln(x),
$$ where $\lim\limits_{n\to\infty}S_n(x)$ denotes the Taylor series of $\ln(x)$ at $x=1$ and $\lim\limits_{n\to\infty}T_n(x)$ the Taylor series of $x\ln(x)$ at $x=1$ . What I can observe so far is that $T_n(x)$ converges uniformly on $(0,1+\delta)$ (the point $x=0$ doesn't cause problems anymore) and this allows us to swap limit taking: $$
\lim\limits_{x\to0}\lim\limits_{n\to\infty}T_n(x)=\lim\limits_{n\to\infty}\lim\limits_{x\to0}T_n(x)=0.
$$ However, I don't know what the limit function is or in other words how to show that $\lim\limits_{n\to\infty}T_n(x)=x\ln(x)$ . Any suggestions how to complete the proof?","['limits', 'sequences-and-series', 'taylor-expansion', 'real-analysis']"
4036146,Prove if the following sets are topologies in $\mathbb{Z}$,"I've been solving the following problem from the start of my general topology course, and I'd like to check if my answers are correct. The problem is: Study if the following sets of subsets of $\mathbb{Z}$ are topologies
or not: $\mathcal{A}_1=\{\emptyset,\{1,2\},\{1,2,3\},\{2,3,-4\},\{1,2,3,-4\},\mathbb{Z}\}$ $\mathcal{A}_2=\{\emptyset\}\cup\{n\mathbb{Z}:n\in\mathbb{N}\}$ $\mathcal{A}_3=\{A\subset\mathbb{Z}: 0 \in A\}$ $\mathcal{A}_4=\{A\subset Z: A \text{ is infinite }\}\cup\{\emptyset\}$ My solutions (I concluded no one is a topology in $\mathbb{Z}$ , all using the definition of topology): NOT, because $\{1,2\}\cap\{2,3,-4\}=\{2\}\notin \mathcal{A}_1.$ NOT, because $\nexists\phantom{,} n\in\mathbb{N}: n\mathbb{Z}=3\mathbb{Z}\cup 7\mathbb{Z}$ (this is because $\text{gcd}(3,7)=1$ , but $1\mathbb{Z}=\mathbb{Z}\neq3\mathbb{Z}\cup 7\mathbb{Z}$ ). NOT, because $\emptyset\notin\mathcal{A}_3.$ NOT, because if we consider the sets $$A=\{z\in\mathbb{Z} : z \text{ is even}\}$$ $$B=\{z\in\mathbb{Z} : z \text{ is odd}\}\cup \{2\},$$ it's clear that both $A,B\in\mathcal{A}_4$ because both are infinite sets, but $A\cap B=\{2\}$ , hence we conclude that $A\cap B\notin\mathcal{A}_4$ (because $\{2\}$ is a finite set), so we conclude $\mathcal{A}_4$ is NOT a topology in $\mathbb{Z}$ . Are my solutions correct? Thanks in advance.","['elementary-set-theory', 'general-topology', 'integers']"
4036164,Integrating $\log(-ix)\exp(-ix)/x^2$,"I would like to compute a few integrals like $$\int_{-\infty}^\infty\frac{\log(-ix)\exp(-ix)}{x^2}\,dx$$ To be clear, here the path of integration is really $z = \epsilon i + x$ , so that it avoids the singularity at $x=0$ , and the branch cut of $\log$ . This expression is ""well-behaved"" in several ways: it falls off faster than $1/x^{3/2}$ on the real line, so it converges decently quickly; in fact, falls off quickly everywhere in the lower half-plane of $z$ . Exponentially quickly, thanks to the $\exp$ term. The function is holomorphic everywhere except the branch cut at $x\le 0$ , so I can deform it pretty easily. The $\log \exp / x^k$ form seems impossible to find an antiderivative for. So this seems like something that should be doable using Cauchy residue theorem and related tricks. The function grows quickly on the upper half plane, so I can't just deform it ""up and away"" and show that it's zero. One can easily reduce it to a keyhole contour around the branch cut, but neither the ""along the cut"" nor the ""around the pole"" term are zero, they both depend on the radius of the keyhole, and neither one seems solvable analytically. Anyone have tricks to tackle this?","['integration', 'definite-integrals', 'improper-integrals', 'complex-analysis', 'contour-integration']"
4036201,Prove that the given set is a finite set.,"$A = \{(a,b) \in N^2 | (2+a)(2-b) \ge 2(a-b)\}$ . Prove that A is a finite set. I'm having problem with proving A is a finite set. So far, Let $x, y \in A$ . Then, \begin{equation*}
\begin{aligned}
    (2+x)(2-y) &\ge 2(x-y)\\
    4-2y+2x-xy &\ge 2x-2y\\
    4-xy &\ge 0\\
    xy &\le 4
\end{aligned}
\end{equation*} This is what I have, but I'm not sure that showing $xy \le 4$ is enough to show that we have finite set. The reason why I thought this was enough because x and y are natural numbers. I searched other proofs and saw someone used contradiction to prove this is a finite set by assuming A is a infinite set. Do you guys think I need contradiction? or is it enough to prove A is a finite set?","['elementary-set-theory', 'solution-verification']"
4036236,Giving a 1-hour talk to highschool math club: any topic suggestion?,"I've been invited (by my kid) to give a one hour talk to her highschool math club. Last year (right before the pandemic hit) I did two such talks on probability, and they loved it. I'm looking for topic suggestions. Logistics: There will be about $10-15$ kids total, from grades $9-12$ .  These kids are mostly ""A"" students, and some of them have skipped a math grade, but they're nowhere near Olympiad standard.  The talk will be on March $1$ st, $2021$ , and will be on zoom.  I will be making slides this weekend. I don't want this to become an ""educational"" lecture, so I'm looking for fun examples to motivate certain math areas.  E.g. last year my talks on probability culminated with Polya Urn and Buffon's Noodle .  Also, I'd like everything (well, $95\%$ ) to be fully explainable within the one hour, i.e. I don't want to just spew facts and then tell them ""just trust me"". My own math skill tends toward discrete: comp-sci, combinatorics, etc.  However I'm open to any idea - esp. if it has worked for you before! - as long as I can learn it during one weekend.  :) Some ideas: modulo arithmetic esp. finite fields: Perfect shuffle is fun.  Would cryptography (e.g. RSA) be too difficult?  What about codes e.g. Hamming?  Any other fun examples of modulo arithmetic? non-Euclidean geometry: especially how a POINT in elliptic geometry is actually $2$ ""ordinary points"".  Can the alternates to the parallel postulate (and some consequence e.g. angle sum in a triangle) be explained sufficiently at highschool level in an hour? finite geometry: Personally I find Finite projective planes very beautiful.  Do they (or finite affine planes) have any application?  (Besides the kiddie game Spot It ?) algorithm: This is actually my work area, but I don't know what I can cover in an hour.  Maybe the $O(N \log N)$ lower bound for sorting?  Some of these kids don't even have coding experience... :( graph theory: Edge coloring of a complete graph is one of my favorite pictorial proofs.  Eulerian tour.  Euler characteristic (of a planar graph) is related.  What else?  Shortest path would require discussion of algorithm.  Hall's marriage theorem is surprising and neat but I don't think I can prove it from scratch in an hour. combinatorics: Start with ${N \choose m}$ and then stars-and-bars for sure.  What are your favorite (elementary) examples?  Can Burnside's lemma be proved from scratch (at highschool level, without group theory ) in an hour? Comments on above and/or any other suggestion most welcome! (Apologies if I'm tagging too widely...)","['graph-theory', 'finite-fields', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics']"
4036260,$\mathbb{Q}$ is not a Baire space?,"The condition of 'X is a Baire space': (cite from Topology by Munkres) Given any countable collection $\{ A_n \}$ of closed sets of X each of which has empty interior in X, their union $\bigcup A_n$ also has empty interior in X. So for rational numbers $\mathbb{Q}$ with the usual topology of $\mathbb{R}$ : (1) countable collection $\{ Q_i \}$ : one can index all rational numbers with  positive integers, so let $Q_i = \{q_i\}$ be
one-point set. (2) $Q_i$ is closed, since $\mathbb{R}- Q_i= (-\infty, q_i) \cup (q_i, \infty)$ is open in $\mathbb{R}$ . (3) $Q_i$ has empty interior, since $Q_i$ contains no open set of $\mathbb{R}$ . (4) $\mathbb{Q}=\bigcup Q_i$ has empty interior, since $\mathbb{Q}$ contains no open set of $\mathbb{R}$ . Thus, $Q_i$ is a Baire space. What's wrong here?",['general-topology']
4036294,"$f(z)$ is analytic on $\{ |z| \leq 1\}$, and $f(z)$ is real on $\{|z| = 1\}$, show $f(z)$ is a constant.","Let $f(z)$ be analytic on $\{|z| < 1\}$ that extends to $\{|z| \leq 1\}$ . Assume $f(z)$ is real on $|z| = 1$ . Prove that $f(z)$ is a constant. Here is my attempt,
define $g(z) = e^{if(z)}$ , for $|z| = 1$ , we have $|g(z)| = 1$ . Then $|\frac{1}{g(z)}| = 1$ as well. So that $|g(z)| = 1$ for $|z| = 1$ . (1) Is there any theorem I can use to show that this leads to $g(z)$ is constant on $|z| \leq 1$ ? (2) If $g(z)$ is constant, can I show that $f(z)$ is constant?",['complex-analysis']
4036313,Which operators commute with curl?,"Let $X$ be the space of infinitely differentiable maps from $\mathbb{R}^3$ to $\mathbb{R}^3$ . Let $C:X\rightarrow X$ denote the curl map. What are all the linear maps from $X$ to $X$ that commute with $C$ ? For example, rotations and translations commute with $C$ . Also of course $C$ commutes with itself. Other than products of these three types of linear operators (translation, rotation, and curl) what other operators commute with $C$ ?","['curl', 'examples-counterexamples', 'multivariable-calculus', 'vector-analysis', 'differential-geometry']"
4036314,"How many functions $f\colon A\to B$ with the following property: $f (x + y) = f (x) + f (y)$ for every $x, y\in A$","Given $A = \{1, 2, 3,\dots , 111\}$ and $B = \{1, 2, 3,\dots , 2021\}$ . How many functions $f\colon A\to B$ with the following property $$f (x + y) = f (x) + f (y)$$ for every $x, y \in A$ ? All I can think of is substituting $x = y$ $$f(2x)=2f(x)$$ After that I was confused what should I do.","['functional-equations', 'functions']"
4036316,Discrete dynamic system to continuous dynamic system,"I have a discrete time dynamical system $$x(t+1)=f(x(t))$$ I would like to write a continuous time system that approximates it well; that is, a continuous system of the form $$\frac{dx}{dt} = g(x)$$ , solve the continuous system, and use the solution to approximate the values of the original discrete time system. Are there any methods for doing this? I did some search and found conversions from a continuous system to a discrete one - what about the other way around? Also, how to bound the error of this approximation?","['recurrence-relations', 'ordinary-differential-equations', 'dynamical-systems']"
4036496,Continuous water-filling,"Disclaimer : I x-posted this question on OR-stackexchange as suggested in the comment below. I reposted it there since I did not receive any satisfactory answer on math-stackexchange up to now. Let $x\in\mathbb{R}^n$ be an optimization variable and $\alpha\in\mathbb{R}^n$ be a given $n$ -dimensional vector. The standard water-filling problem is formulated as \begin{equation}
\begin{array}{ll}
\underset{x}{\operatorname{minimize}} & -\displaystyle\sum_{i=1}^{n} \log \left(\alpha_{i}+x_{i}\right) \\
\text { subject to } & x \succeq 0, \quad \mathbf{1}^{T} x=1,
\end{array}
\end{equation} and it has a well-known solution (see Boyd & Vandenberghe page 245). I was thinking about the case in which we have continuous communication channel slots. Intuitively this may be thought of as the case when the sizes of the communication channels approach zero. How is it possible to generalize this optimization problem to this case? I believe that $\alpha$ and $x$ , which from now on we will denote by $\alpha(x)$ and $f(x)$ respectively, would in this case be continuous real function and the problem would be: \begin{equation}
\begin{array}{ll}
\underset{f \in \mathcal{F}}{\operatorname{minimize}} & -\displaystyle\int_{x} \log \left(\alpha(x)+f(x)\right)dx \\
\text { subject to } & f(x) \geq 0\; \forall x, \quad \int_x f(x)dx=1
\end{array}
\end{equation} where $\mathcal{F}$ is a given class of functions (e.g. Hölder continuous, Lipschitz, etc). We can also assume that the domain of the functions $f(x)$ and $\alpha(x)$ is compact, i.e. $x\in\mathcal{X}\subseteq \mathbb{R}$ , with $\mathcal{X}$ a compact subspace of $\mathbb{R}$ . Is this the right path to follow? Do you have any other on how to solve these types of problems? It looks it is related to the calculus of variations , but I have never seen these types of problems and I have no idea how to solve them. Thanks!","['convex-optimization', 'optimization', 'calculus', 'calculus-of-variations']"
4036510,"Words of length $10$ in alphabet $\{a,b,c\}$ such that the letter $a$ is always doubled","Compute the number of words of length $10$ in alphabet $\{a,b,c\}$ such that letter $a$ is always doubled (for example "" $aabcbcbcaa$ "" is allowed but "" $abcbcaabcc$ "" is forbidden). I am looking for a quick/efficient way to resolve this problem. I thought of fixing "" $aa$ "" in the beginning then draw a tree of the next possibilities but this tree will end up to be a whole forest. Can you help me ?","['combinatorics-on-words', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4036553,Evaluating $\int\limits_{1}^{\infty} \frac{1}{\lfloor{x}\rfloor!}dx$,"I thought the improper integral $\int\limits_1^{\infty} \frac{1}{\lfloor{x}\rfloor!}dx$ converge, while the textbook says it's not. Of course, $\lfloor{x}\rfloor$ is the greatest integer function. Here is my solution: For any $n\in \mathbb N$ , $$\lfloor{x}\rfloor=n\;\; \Leftrightarrow \;\; n\leq x<n+1 $$ Thus if $\; N\leq t<N+1$ $$\begin{align}&\int_{1}^{N}\frac{1}{\lfloor{x}\rfloor!}dx\leq\int_{1}^{t}\frac{1}{\lfloor{x}\rfloor!}dx\leq\int_{1}^{N+1}\frac{1}{\lfloor{x}\rfloor!}dx\\
&\Rightarrow \;\sum_{n=1}^{N-1}\frac{1}{n!}\leq\int_{1}^{t}\frac{1}{\lfloor{x}\rfloor!}dx \leq\sum_{n=1}^{N}\frac{1}{n!}\end{align}$$ Taking $N\rightarrow\infty$ both sides also gives $t\rightarrow\infty$ , and we have $$e-1\leq \int_1^{\infty} \frac{1}{\lfloor{x}\rfloor!}dx \leq e-1$$ Therefore, $\int_1^{\infty} \frac{1}{\lfloor{x}\rfloor!}dx=e-1$ . □ I've tried a sort of times to find some mistakes in what I wrote. But I didn't get anything till now. Can somebody point out what I missed? Thank you.","['integration', 'ceiling-and-floor-functions', 'improper-integrals', 'analysis', 'limits']"
4036597,Writing the Real Part of Complex Integrand,"I'm having a hard time to understand how's Eq. $(6.73)$ become Eq. $(6.75)$ . It's taken from Numerical Methods for Laplace Transform Inversion by Cohen.
Here's the problem: [...]. The basis of their formulation is, like Talbot, that the inverse transform is given by $$f(t)=\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}e^{st}\bar f(s)ds.\tag{6.71}$$ The contour is deformed by means of the path $$s(\theta)=r\theta(\cot\theta+i),\quad-\pi<\theta<\pi,\tag{6.72}$$ where $r$ is a parameter. This path only involves one parameter whereas Talbot's consisted of two. Integration over the deformed path yields $$f(t)=\frac{1}{2\pi i}\int_{-\pi}^\pi e^{ts(\theta)}\bar f(s(\theta))s'(\theta)d\theta.\tag{6.73}$$ Differentiating (6.72) we have $s'(\theta)=ir(1+i\sigma(\theta))$ , where $$\sigma(\theta)=\theta+(\theta\cot\theta-1)\cot\theta.\tag{6.74}$$ We find $$f(t)=\frac{r}{\pi}\int_0^\pi\Re\Bigl[e^{ts(\theta)}\bar f(s(\theta))(1+i\sigma(\theta))\Bigr]d\theta.\tag{6.75}$$ As you can see there, i'm confused about where the $\Re$ got from. And i have 2 speculations about this. Speculation 1 (The idea is originated from my friend, so let me rewrite what he said) Regarding the integral: $s(\theta)$ on $(-\pi,\,\pi)$ is symmetric about $\theta=0$ (i don't understand this part). So we can integrate on $\theta\in (0,\pi)$ and multiply by $2$ in the denominator of the constant (outside the integral). The factor $ir$ in $s'(\theta)$ cancels the $i$ in the constant and gives it $r$ in the numerator. Assuming $f(t)$ is a real-valued function, $f(t)=\Re f(t)$ . For Riemann integrable functions $g(z)$ , we have $\Re \int g(z)\Bbb dz = \int \Re g(z) \Bbb dz$ . Hence, since $f(t)$ is equal to an integral, we can take $\Re$ on both sides and move the real part symbol inside the integral. Speculation 2 (This is my own idea) By letting the whole integrand with $\Lambda$ for simplification purpose (i'm not performing change of variable here, just for a simplification). Then we can consider: $$\begin{align}
\int_{-\pi}^{\pi} \Lambda\,\Bbb d\Lambda &= \int_{-\pi}^{0} \Lambda\,\Bbb d\Lambda + \int_{0}^{\pi} \Lambda\,\Bbb d\Lambda\\
&= -\int_{0}^{-\pi} \Lambda\,\Bbb d\Lambda + \int_{0}^{\pi} \Lambda\,\Bbb d\Lambda\\
&= \int_{0}^{\pi} \overline{\Lambda}\,\Bbb d\overline{\Lambda} + \int_{0}^{\pi} \Lambda\,\Bbb d\Lambda\\
&= 2\int_{0}^{\pi}\Re \Lambda\,\Bbb d \Lambda
\end{align}$$ The last expression is considering this property : $z + \overline{z} = 2\Re z$ , where $\overline{z}$ is the conjugate of $z$ . But i'm not really sure if i can consider the third line is valid about the conjugate? And by the way. The first equation on the picture is the formula for finding the Inverse Laplace Transform of $f(t)$ (real-valued) and the Laplace transform that what i'm talking is one-sided Laplace Transform. Are both speculations correct? Is my speculation is correct? If not, please provide the correct interpretation behind the story of Eq. $(6.73)$ become Eq. $(6.75)$ . Please kindly to help me, i really need your help. Thanks in advance!","['inverse-laplace', 'laplace-transform', 'complex-analysis', 'contour-integration', 'riemann-integration']"
4036612,Intuition for the Area Formula (by Federer),"Question I am trying to get an intuition for the area formula as a ""generalized"" change of variables formula. If $f:\mathbb{R}^m\to\mathbb{R}^n$ is Lipschitz and $m\leq n$ then $$
\int_A g(f(x)) J_m f(x) \lambda^m(dx) = \int_{\mathbb{R^n}} g(y) N(f \mid A, y) \mathcal{H}^m(dy)
$$ whenever $A$ is $\lambda^m$ -measurable, $g:\mathbb{R}^n\to \mathbb{R}$ is Borel and $N(f \mid A, y)=\#\left\{x\in A\, :\, f(x) = y\right\}$ My Attempt at an Intuition The standard change of variables formula for integration by substitution is Let $U\subset\mathbb{R}^n$ be an open set and $\phi: U\to\mathbb{R}^n$ an injective differentiable function with continuous partial derivatives, with non-zero Jacobian. Then for every real-valued, compactly-supported continuous function $f$ with support contained in $\phi(U)$ we have $$
\int_U f(\phi(\boldsymbol{u})) |\text{det}(D\phi)(\boldsymbol{u})|d \boldsymbol{u} = \int_{\phi(U)} f(\boldsymbol{v})d \boldsymbol{v} 
$$ One way in which I understand this formula is that if one is presented with the integral on the RHS and knows that $\boldsymbol{v} = \phi(\boldsymbol{u})$ then it is possible to write the integral in terms of $\boldsymbol{u}$ by: plugging in $\boldsymbol{v} = \phi(\boldsymbol{u})$ mapping the domain of integration from $\phi(U)\subset V$ to $\phi^{-1}(\phi(U)) = U$ , and correcting the integrand for the change in volume brought in by the transformation $|\text{det}(D\phi)(\boldsymbol{u})|$ Or similarly one can see it the other way around: if we find the integral on the LHS and we can recognise the second term to be the Jacobian of a transformation, then we can simplify the integral by substitution and get to the RHS. What is a similar intuition for the Area formula above? Clearly, $f$ seems to be playing the role of $\phi$ , our transformation, and the original function that we want to integrate is now called $g$ . I can see that a major difference between these formulas is the fact that we have the Lebesgue measure on the LHS and the Hausdorff measure on the other. I am confused by the ""cardinality"" term $N(f \mid A, y)$ as this doesn't really appear in the usual change of variable formula.","['measure-theory', 'geometry', 'calculus', 'transformation', 'differential-geometry']"
4036661,"If $(X_n)_{n\in \mathbb{N}}$ is a martingale s.t. $\sup_n E[|X_n|]\leq M < \infty$, then $\sum_{n\geq 2}(X_n-X_{n-1})^2<\infty$ almost surely.",I need to prove the above statement. The hint provided was to consider the stopping time $T_l=\inf\{n\in \mathbb{N}||X_n(w)|\geq l\}$ where $l\in \mathbb{N}$ and then show that $E[\sum_{n=2}^K(X_n-X_{n-1})^21_{T_l>K}]$ is uniformly bounded in $K$ . I have 2 issues here: $E[\sum_{n=2}^K(X_n-X_{n-1})^21_{T_l>K}]$ is uniformly bounded: I know that if $T_l>K$ this implies that $|X_n-X_{n-1}|^21_{T_l>K}\leq (2l)^2 $ . But this just shows that $E[\sum_{n=2}^K(X_n-X_{n-1})^21_{T_l>K}]\leq K(2l)^2$ which is not uniform in K. I feel we have to use the fact that $(X_n)$ converges to a finite limit since it is $L_1$ -bounded. How does the hint then prove the original statement? I am stuck here. I think we have have to use Lebesgue Dominated Convergence Theorem but I don't see how. Any ideas? I have been stuck for some time now. I would also appreciate another alternative method if anyone has one.,"['conditional-probability', 'martingales', 'measure-theory', 'probability-theory']"
4036683,Probability that the number of heads is coprime with the number of throws,"Let's throw a coin $n$ times. Say that we have obtained heads $k$ times and tails $n-k$ times. I'm interested in the probability that the number of heads is coprime with the total number of throws. Obviously, this probability is described by the formula $$
P=\sum_{\substack{0<k<n,\\gcd(k, n)=1}}\binom{n}{k}p^k(1-p)^{n-k},
$$ where $p\in(0, 1)$ is a probability of throwing heads. The question. Is it true that this sum is lower bounded by some positive constant for all integers $n > 1$ ? My thoughts. I think that it should be true. Take $p=1/2$ for simplicity. Some computer calculations suggest that $P\ge 0.1875$ , and this bound is achieved for $n=6$ . But I don't know how to prove it. It's also easy to show that $P\ge c n^{-1/2}$ for some constant $c$ because we can always find such number $k$ , which is coprime with $n$ and close to $n/2$ . But this bound is too weak. Update. @donaastor made some interesting comments below and convinced me that for all constant $p$ the lower limit of the probability equals 0. Also, further computer calculations for $p=1/2$ showed that the probability for $n=30030$ is lower than for n=6 and equals $\approx 0.1828$ .","['number-theory', 'probability']"
4036830,"Why the group $\langle x,y\mid x^3, y^3, yxyxy\rangle$ is not trivial?","This comes from Artin Second Edition, page 219. Artin defined $G=\langle x,y\mid x^3, y^3, yxyxy\rangle$ , and uses the Todd-Coxeter Algorithm to show that the subgroup $h=\langle y\rangle$ has index 1, and therefore he gets $G=H$ is the cyclic group of order 3. My question is: why $\langle y\rangle$ is not trivial? The Todd-Coxeter Algorithm gives no extra information about $y$ except it acts trivially on $\langle y\rangle$ as any other elements in $G$ do. So how do we know that $y\neq 1$ ? Many thanks!","['group-isomorphism', 'combinatorial-group-theory', 'abstract-algebra', 'free-groups', 'group-theory']"
4036839,Is $X_t=X_{t-1}^{\alpha} + \varepsilon_t$ stationary for $\alpha<1$?,"Let { $\varepsilon_t$ } be iid. Then, we have time series defined by $$X_t=cX_{t-1}^{\alpha} + \varepsilon_t,$$ with $0<\alpha<1$ and $c\in\mathbb{R}$ and let $\varepsilon_t$ be non-negative. Is it strictly stationary? If we have $\alpha=1$ we obtain classic AR(1) process, where we need $c<1$ for stationarity. For lower $\alpha$ it seems that $X_t$ is ""smaller"" and should be also stationary, but I have a hard time proving that. Also, do we need then some restriction for $c$ in such case?","['time-series', 'probability-distributions', 'stationary-processes', 'probability']"
4036890,An interesting ODE,"I've been thinking about approaches to solve $$\frac{\textrm{d}^{3}y}{\textrm{d}x^{3}}=\textrm{e}^{-y(x)}.$$ My initial thought was to set $y(x)=\ln(z(x))$ in order to obtain an ODE relating $z$ to $x$ . However, this in fact makes things much worse (or at least much messier). Any insights much appreciated. Thanks","['integration', 'calculus', 'ordinary-differential-equations']"
4036893,Clubs whose intersections are multiples of six (Oddtown variant),"This is a question about generalizing the famous ""Clubs in Oddtown"" problem. The original setup is that a town has $n$ people, and $m$ clubs each consisting of a subset of the population. Each club has an odd number of members, while any two distinct clubs have an even number of members in common. Given this, you the goal is to prove $m\le n$ . You can prove this using linear algebra over $\mathbb F_2$ . One might ask if the results still hold if we replace $2$ with another number $d$ . That is, if we have a town with $n$ people and $m$ clubs such that the number of people in any club is not a multiple of $d$ , while the number of people in common to any two clubs is a multiple of $d$ , what it is the greatest possible value of $m$ in terms of $n$ ? When $d=p$ for any prime $p$ , you can prove $m\le n$ using the same argument but with $\mathbb F_p$ -linear algebra. You can also prove that $m\le n$ when $d=p^k$ is any prime power, see Generalize Oddtown . This leads me to ask, can we also prove that $m\le n$ for all other $d$ ? For concreteness, I am asking about $d=6$ (I like to call this variant ""Clubs in Sioux City""). That is, Question: There is a town with $n$ people and $m$ clubs consisting of subsets of the population. Any club has a number of members which is not a multiple of $6$ , while the number of members in common to any two clubs is a multiple of $6$ . Can we conclude that $m\le n$ ? I know that you can prove $m\le 2n$ , by combining the $d=2$ and $d=3$ results (proving this result is what was asked at that question I linked before ). But from playing with small cases, it does not seem like it is possible to create a setup with $m>n$ , though I also cannot prove this. Does anyone know how to prove that $m\le n$ , or how to construct a set of $n+1$ clubs satisfying the requirements?","['finite-fields', 'extremal-combinatorics', 'linear-algebra', 'combinatorics', 'algebraic-combinatorics']"
4036931,How to find the slope of certain lines matching random points,"I have a large set of random points, $(x_i,y_i)$ , which, for unknown reasons, seem to align along certain lines: I want to calculate the slope of those lines (also their other parameters, but I'm asking about the slope here) Separating the lines algorithmically, seems to be too complex. I guess that I would need a classification algorithm, and those never give stable results. That's why I am only talking about the slope. What I did: I made a synthetic sample generating random lines: The average point $\color{blue}{(c_x,c_y)}$ is drawn in blue, and on this chart, the lines have $-45$ ° angle. I have the intuition that the sum of all points distances to a line passing by the blue point, should be maximal/minimal when the line matches the slope I'm looking for (or the perpendicular slope). I calculated the distance summed of all points to a line passing for the center, with angle $\alpha$ , for different angles (from $0$ to $360$ ), and I got this chart: I drawn the point lines at $-45$ °, and the distance seems to be maximal just around the normal angle $+45$ , so it suggest that I could use a maximization solver to find the slope normal to the lines, by maximizing the distance of all points to a line passing by the center (blue point). It seems to work for different angles I tried, but I'm not sure if that's a correct procedure. Also, the distance is not minimal when the slope $\tan(\alpha)$ is parallel to the lines. EDIT: I just noticed that if the points are clustered in two distant groups, independent of the orientation of the lines, the maximum distance would be the one separating the groups.
Maybe Fourier transforms would be able to detect the orientation of the lines?","['regression', 'geometry', 'calculus', 'algorithms', 'parametrization']"
4036954,Number of ways to split balls of different colours,"Given an even number of balls of different colours, find the number of ways of splitting these balls evenly between two people. The total number of balls is known, it is always even (and therefore could always be split evenly between two people), but may be large. Each ball has a colour. The colours and the quantities of each colour are known in advance, but there could be many colours. The order of the picking doesn't matter - all balls of the same colour are the same. Here are a few examples: Example 1: Given 2 red balls, how many ways there are to split them? Answer: Well, just 1 - each person gets one red ball. Example 2: Given 20 red balls, how many ways there are to split them? Answer: Still just 1 - each person gets 10 red balls. Example 3: Given 19 red balls and 1 blue ball, how many ways there are to split them? Answer: Two ways (in fact, it doesn't matter how many red balls there are in this example, it will always be two ways) - Person one gets 9 red a 1 blue ball and person two gets 10 red balls. Person two gets 9 red a 1 blue ball and person one gets 10 red balls. Example 4: Given 4 red balls (R) and 1 blue ball (B) and 1 yellow ball (Y), how many ways there are to split them? Answer: Four ways - RRR / RBY RBY / RRR RRB / RRY RRY / RRB You get the idea. Now, in the general case, given W blue balls, X green balls, Y red balls, Z brown balls, ... $k$ balls of colour C , how many ways are there to split them evenly between two people? Is there a generic formula that can be applied? I have been at it for days, but short of counting all possible permutations by hand, I cannot really see a pattern that can be generalised.",['combinatorics']
4036963,What is the record for Collatz Conjecture Steps,Does anyone know the record for the number of steps a Collatz Conjecture run has taken to get to 1? I have written a small program in Python to do the Collatz Conjecture on the Mersenne prime where n=5000 and produced a text file of ~88MB with a total step of 67378.,['functions']
4036998,What do weak-star limits of normal states on von Neumann algebras look like?,"Let $N$ be a von Neumann algebra and let $(\phi_\lambda)$ be a net of normal states on $N$ so that $\phi_\lambda\to\phi$ in the weak-* topology, i.e. $\phi_\lambda(x)\to\phi(x)$ for all $x\in N$ . What can we infer about $\phi$ in this case? Of course, $\phi$ is going to be a state on $N$ , since $\phi(x^*x)=\lim_\lambda\phi_\lambda(x^*x)\geq0$ and $\phi(1_N)=\lim_\lambda\phi_\lambda(1_N)=1$ . But can we say anything more about $\phi$ ? If not, can we prove that the normal states on $N$ are dense in the weak-* topology in the state space $S(N)$ ? I was thinking that, if this density is true, it suffices to prove it for the case of $\mathcal{B(H)}$ . Indeed, if this is true for those von Neumann algebras, then we take an arbitrary von Neumann algebra $N$ and concretely represent it on some Hilbert space, i.e. we have an inclusion of von Neumann algebras $N\subset\mathcal{B(H)}$ . Now normal states of von Neumann subalgebras extend to normal states and, in general, states extend to states (in the $C^*$ -algebraic setting). So we take a state $\phi$ on $N$ , we extend it on a state $\Phi$ on $\mathcal{B(H)}$ and then we approximate $\Phi$ in the weak-* sense by normal states on $\mathcal{B(H)}$ . We restrict those to $N$ and these are normal states on $N$ approximating $\phi$ weak-* on $N$ . I have no idea on how to prove this though, any help is appreciated.","['von-neumann-algebras', 'functional-analysis', 'operator-algebras']"
4037057,"What's a ""rank-two update"" to a matrix?","I'm reading a book, and on its numerical optimization chapter, more precisely in the BFGS algorithm section, the authors state that it's a ""rank-two update to the matrix"". I've searched for the definition, but I only found the definition for the rank-one update here . So my question is what is a rank-2 update, and why is the BFGS an example of such an update. Thanks in advance.","['matrices', 'numerical-methods', 'numerical-optimization', 'linear-algebra']"
4037174,Show that the following is only continuous at x=0,"What is this question asking?
How does rational and irrational affect it? What are the absolute values used for in this solution?
Why is the series irrational when $c\ne0$ for any nonzero rational number but then the series is made to be all irrational values? What is this question?","['limits', 'calculus', 'continuity']"
4037207,What is the expected number of ingredients used to make $4$ pizzas if each pizza must contain $4$ of $100$ possible ingredients?,"I'm trying to solve an optimization problem of the Google Hashcode contest and by analyzing the dataset I reduced a part of it down to a statistics problem, which I'm not able to solve. Any help would be appreciated. Assume we have $100$ types of ingredients (from each type infinitely many) and we want to make $4$ pizzas. Each pizza must contain exactly $4$ distinct ingredients. All of the $100$ ingredients are equally likely for a candidate position of an ingredient for a pizza. What is the expected value of the total distinct ingredients used in all $4$ pizzas? Here is my initial thought which is unlikely to be true: $$E[N] = \sum_{i = 4}^{16}{i \times \frac{\binom{100}{i} {\binom{i}{4}}^4 - \binom{100}{i - 1} {\binom{i - 1}{4}}^4}{\binom{100}{16}{\binom{16}{4}}^4}} =15.9. $$ Numerator is essentially difference of total number of states with $i$ ingredients at most and with $i-1$ ingredients at most which gives the total number of states with exactly $i$ ingredients, while denominator is the total number of states with $16$ ingredients. I think the answer for the above method ( $15.9$ ) is not very intuitive. It seems more logical for the answer to be much more lower. ps. I previously tried to put ${\binom{100}{4}}^4$ in the denominator but after doing a simple summation I ended up with a very unreasonable quantity. ps2. Order of pizzas are not important.","['statistics', 'probability']"
4037283,"In $\mathbb{R}^{2}$, why does the convex hull have lower perimeter?","One thing I've kind of seen of all over the shop is that in $\mathbb{R}^{2}$ taking the convex hull of a bounded open set whose closure is connected reduces perimeter and increases area. I can see the argument in the cases of polygons and for sets that are closure of the interior of some simple closed differentiable curve from an intuitive argument. However, it is not clear to me how one could show this in generality. I feel like I may just be missing some startlingly obvious point. Understandably I should mention that I'm using the 1-dimensional Hausdorff measure of the topological boundary for perimeter, but of course if someone has an elegant way using De Giorgi perimeter then that would also hold. Many thanks, Cryptokyo","['convex-geometry', 'general-topology', 'geometry', 'geometric-measure-theory']"
4037298,"Are there infinitely many pairs of primes $(p,q)$ such that $p^2+1=2q$?","For example, $(3,5), (5,13)$ or $(11,61)$ satisfies this condition, but I have no idea how to prove or disprove that such pairs exist infinitely many. Is there any prior researches about it?","['number-theory', 'prime-numbers']"
4037303,When is a locally Borel set Borel?,"Let $X$ be a LCH space. Call $E\subseteq X$ locally Borel if $E\cap K$ is Borel for all compact $K\subseteq X$ . Evidently, locally Borel sets form a $\sigma$ -algebra and every Borel set is locally Borel. My question is the following. Suppose $X$ is a group, or more generally there is a Radon measure $\mu$ on $X$ (using, say, the definition in Folland: finite on compact sets, outer regular on all Borel sets, and inner regular on all open sets) such that $\mathrm{supp}\,\mu=X$ . Does it follow that every locally Borel set is Borel? Note some assumption on $X$ like this is necessary: for example, the first uncountable ordinal has subsets which are locally Borel but not Borel. On the other hand, if $X$ is $\sigma$ -compact, a set is Borel if and only if it is locally Borel.","['borel-sets', 'measure-theory', 'locally-compact-groups']"
4037364,Is the packing density of an ellipse the same as that of a circle?,"It is well-known that the densest packing of circles in the plane is the close hexagonal packing, with a density of $\frac{\pi\sqrt{3}}6\approx0.9069$ : By applying an affine transformation, we obtain a packing of ellipses with the same density: However, not every ellipse packing arises from such a transformation, as we can rotate the ellipses at different angles. So it does not follow that the ellipse must have the same packing density. Are there any ellipses which have a higher packing density than the circle? I realize this question may be very difficult to answer in the affirmative, given the subtlety and difficulty of most packing problems, but I am curious to hear of bounds on the problem (possibly as a function of the aspect ratio) and expert opinions on the likely answer, if nothing else. It is worth noting that in the three-dimensional case, the packing density of an ellipsoid can be higher than that of the sphere, with some ellipsoids having packing densities of at least $0.7585\ldots$ compared to the sphere's $\frac{\pi}{\sqrt{18}}\approx0.7405$ (see Wolfram MathWorld , J. Wills ).","['conic-sections', 'geometry', 'packing-problem']"
4037391,Is there an elegant exact formula for the zeta zero counting function?,"I've noticed the zeta zero counting function defined in formula (1) below isn't exactly correct as was pointed out in this related question and answer on MathOverflow. The function $\vartheta(T)$ is the  Riemann–Siegel theta function. (1) $\quad N_0(T)=1+\frac{\vartheta(T)}{\pi}+\frac{\arg\left(\zeta\left(\frac12+i\,T\right)\right)}{\pi}$ $\qquad\qquad\quad\ =1+\frac{\vartheta(T)}{\pi}+\frac{\Im\left(\log\zeta\left(\frac12+i\,T\right)\right)}{\pi}$ Some of the comments on the question and answer linked above seem to argue that formula (1) above is exactly correct so I'll attempt to clarify with a few illustrations. Figure (1) below illustrates $\frac{\vartheta(T)}{\pi}$ in orange, $\frac{\Im(\log\zeta(1/2+i\,T))}{\pi}$ in green, and $N_0(T)$ in blue. The red discrete dots represent the evaluation points $(\Im(\rho_k),k-1/2)$ . Note $N_0(T)$ seems to evaluate exactly as desired in the range of $T$ values illustrated in Figure (1) below. Figure (1) : Illustration of $\frac{\vartheta(T)}{\pi}$ (orange), $\frac{\Im(\log\zeta(1/2+i\,T))}{\pi}$ (green), and $N_0(T)$ (blue) Now consider Figures (2) and (3) below which illustrate the evaluations of $\frac{\Im(\log\zeta(1/2+i\,T))}{\pi}$ and $N_0(T)$ in the neighborhood of Gram point $g_{126}$ and zeta zero $\rho_{127}$ . The green dots in Figures (2) and (3) below represent the evaluations of $\frac{\Im(\log\zeta(1/2+i\,T))}{\pi}$ and $N_0(T)$ at $T=g_{126}+\epsilon$ , and the red dots in Figures (2) and (3) below represent the evaluations of of $\frac{\Im(\log\zeta(1/2+i\,T))}{\pi}$ and $N_0(T)$ at $T=\Im\left(\rho_{127}\right)+\epsilon$ . Note $g_{126}$ is the first Gram point where Gram's law is violated (see Wikipedia: Gram points ), and the evaluation of formula (1) for $N_0(T)$ illustrated in Figure (3) below corrects itself at $T=\Im\left(\rho_{127}\right)$ . Figure (2) : Illustration of $\frac{\Im(\log\zeta(1/2+i\,T))}{\pi}$ Figure (3) : Illustration of $N_0(T)$ Question : Is there an elegant exact formula for the zeta zero counting function that is practical to evaluate? By ""elegant"" and ""practical to evaluate"" I mean a formula that has been completely derived and preferably a closed form formula that can be evaluated similar to formula (1) above. For the purposes of this question I'm not interested in complex formulas with complex integrals that have been left unevaluated such as the formula for $N^*(T)$ illustrated in this Math StackExchange question . In an attempt to further clarify the source of the problem Figures (4) and (5) below illustrate the real and imaginary parts of $\zeta\left(\frac12+i\,T\right)$ in the neighborhood of Gram point $g_{126}$ and zeta zero $\rho_{127}$ . The green dots in the two figures below represent the evaluation of $\zeta\left(\frac12+i\,T\right)$ at $T=g_{126}$ and the red dots in the two figures below represent the evaluation of $\zeta\left(\frac12+i\,T\right)$ at $T=\Im\left(\rho_{127}\right)$ . The source of the problem is the brief positive transition of $\Im\left(\zeta\left(\frac12+i\,T\right)\right)$ at $T=g_{126}$ illustrated in Figure (5) below. This transition violates Grams's law and causes undesirable branches in $\arg\left(\zeta\left(\frac12+i\,T\right)\right)$ and $\Im(\log\zeta(1/2+i\,T))$ . I'll note I believe that the built-in Mathematica functions all support analytic continuation. Figure (4) : Illustration of $\Re\left(\zeta\left(\frac12+i\,T\right)\right)$ Figure (5) : Illustration of $\Im\left(\zeta\left(\frac12+i\,T\right)\right)$ The following four figures contain 3D plots intended to clarify the Mathematica analytic continuation of $\log\zeta(s)$ in the range $\Re(s)\in (-1,3)$ and $\Im(s)\in (-50,50)$ . Figure (5) : 3D Plot of $|\log\zeta(s)|$ Figure (6) : 3D Plot of $\Re(\log\zeta(s))$ Figure (7) : 3D Plot of $\Im(\log\zeta(s))$ Figure (8) : 3D Plot of $\arg(\log\zeta(s))$","['riemann-zeta', 'number-theory']"
4037398,"Can you use ""limits"" in geometry proofs?","Let's say that I wanted to prove power of a point for a tangent and secant. The diagram would look something like this. Let's say that I am allowed to use the power of a point for a a secant and a secant. Would this proof be valid? Now, I take the ""limit"" as $B$ approaches $A$ . It would look something like this. So, in the final diagram, points $A$ , $B$ , are in the same place, let's call this place $P$ . Since the power of point for secant-secant remained through through this whole transformation until this very exact point, we still have that $(EB)(EA) = (ED)(EC)$ because even a very small nudge away makes this true, so this is still basically true even though the points are in the same place. However, in this case, $EB$ & $EA$ have become the single segment $PE$ . Thus our original equation becomes $(PE)^2 = (ED)(EC)$ , thus proving power of a point for a secant and a tangent.","['euclidean-geometry', 'proof-writing', 'solution-verification', 'geometry']"
4037499,Faster way to calculate $\frac{\sqrt8+\sqrt{27}}{5-\sqrt6}-2(\sqrt[4]9-1)^{-1}$,What is the value of $\cfrac{\sqrt8+\sqrt{27}}{5-\sqrt6}-2(\sqrt[4]9-1)^{-1}$ ? $$1)1+\sqrt3\quad\quad\quad\quad\quad\quad2)-1+\sqrt2\quad\quad\quad\quad\quad\quad3)1-\sqrt2\quad\quad\quad\quad\quad\quad4)\sqrt2-2\sqrt3$$ It was one of the questions from timed exam (for example supposed to solve any question in average one minute).  but I have difficulty to calculate this expression fast. my approach is: $$\left(\frac{2\sqrt2+3\sqrt3}{5-\sqrt6}\times\frac{5+\sqrt6}{5+\sqrt6}\right)-\frac2{\sqrt3-1}$$ By rationalizing the second fraction it is not hard to see it is $\sqrt3+1$ . but I had difficulty to accurately calculate the first one in short time. Is it possible to evaluate it quicker?,"['fractions', 'algebra-precalculus']"
4037522,Why is the sum of two functions expressed as simple functions is the sum of the weighted indicator variable of the intersection?,"If $$g=\sum_{i=1}^n a_i \mathbb I_{A_i}$$ and $$h=\sum_{i=1}^m b_j \mathbb I_{B_j}$$ why is $$g + h = \sum_{i=1}^n \sum_{j=1}^m (a_i + b_j) \mathbb I_{A_i\cap B_j}$$ I would have guessed that the sum would use the indicator variable $\mathbb I_{A_i \cup B_j},$ which is to say the union of both measurable sets.","['elementary-set-theory', 'probability-theory']"
4037589,unexplained inequalities from Niven's 1971 proof $\sum 1/p$ diverges,"Ivan Niven published a much shorter and easier proof that the prime harmonic series $\sum 1/p$ diverges. The proof is now available as open access here in this link . In the short proof he makes use of two inequalities: $$ \left( \sum_{k<n}'1/k \right) \left( \sum_{j<n}1/j^2 \right) \geq \sum_{m<n} 1/m$$ $k$ are the square-free integers, $j$ and $m$ are ordinary integers, and $$\prod_{p<n}(1+1/p) \geq \sum_{k<n}'1/k$$ Question: Is there a simple logical rationale for why these inequalities hold? I can only verify these inequalities by hand, multiplying out series for small $n$ . Obviously this isn't proof, but doing this can sometimes reveal the more general logic. In this case I can't see it. I would appreciate solutions and comments that avoid technical terminology as I, and my students, are not university trained mathematicians. Update: Since Niven's article is now open access, here is a screenshot and a transcription: $$\text{A PROOF OF THE DIVERGENCE OF $\Sigma 1 / p$}$$ $$\text{Ivan Niven, University of Oregon}$$ First we prove that $\Sigma^{\prime} 1 / k$ diverges, where $\Sigma^{\prime}$ denotes the sum over the squarefree positive integers. Each positive integer is uniquely expressible as a product of a squarefree positive integer and a square, so for any positive integer $n$ , $$
\left({\sum_{k<n}}^{\prime} 1 / k\right)\left(\sum_{j<n} 1 / j^{2}\right) \color{red}\geqq \sum_{m<n} 1 / m
$$ Here the second sum is bounded but the third sum is unbounded as $n$ increases, so the first sum must be unbounded. Next suppose that $\Sigma 1 / p$ converges to $\beta$ , the sum taken over all primes $p$ . By dropping all terms beyond $x$ in the series expansion of $e^{x}$ or $\exp (x),$ we see that $\exp (x)>1+x$ for $x>0 .$ Hence for each positive integer $n$ $$
\exp (\beta)>\exp \left(\sum_{p<n} 1 / p\right)=\prod_{p<n} \exp (1 / p)>\prod_{p<n}(1+1 / p) \color{red}\geqq {\sum_{k<n}}^{\prime} 1 / k
$$ But this contradicts the unboundedness of the last sum, so $\Sigma 1 / p$ diverges.","['number-theory', 'proof-explanation', 'prime-numbers']"
4037633,Why does an affine transformation $A$ when constrained by $A^TA=\lambda^2I$ result in a similarity transformation?,Why does an affine transformation $A$ when constrained by $A^TA=\lambda^2I$ result in a similarity transformation? I came across this when studying linear transformations in these notes which says: The similarity group is obtained from the affine group by requiring that $A$ be orthogonal: $A^TA=\lambda^2I$ Can't seem to wrap my head around this one.,"['matrices', 'linear-algebra', 'linear-transformations']"
4037638,"The Question about Euler's Totient Function : How phi(i*p) = p*phi(i)? Here, p is prime and p divides i, phi-> Euler totient function","I have a question about the Euler totient function. I am new to the number theory and i don't know where to start to prove this. If $p$ is prime and $p$ divides $i$ , $$ Φ (i\cdot p) = p \cdot Φ (i),$$ where $Φ (n)$ is Euler Totient Function. I know as $p$ divides $i$ , they( $i$ and $p$ ) will not be co-prime so I can't apply multiplicative property here but I am unable to understand how this result came.","['multiplicative-function', 'elementary-number-theory', 'functions', 'totient-function']"
4037670,The minimum size of an edge set that contains an edge of every cycle,"The question is like this: There is a connected graph $G=(V,E)$ with $n$ vertices and $m$ edges such that $m \ge n$ .
Let's say that the graph has a spanning tree, meaning there is a tree $T$ such that $T = (V, E_t)$ and $E_t$ contained in $E$ .
Find the minimum size $k$ such that a set $F$ exists such that $F \subseteq E$ and these two conditions hold: $|F| = k$ Every cycle in $G$ contains an edge in $F$ . My solution is like: I know that $G$ has a spanning tree $T$ .
A tree has $n-1$ edges so for going from $G$ to $T$ i need to erase a set of edges (call it $F$ ) that their size is $m - (n - 1)$ so after i erase them i have a subgraph of $G$ that has $n-1$ edges as needed for it to be a tree. Let's say that there is a cycle in $G$ that doesn't contain an edge in $F$ so after I erase $F$ from $G$ this cycle will remain as it was and that is a contradiction to the assumption that $T$ is a tree. So, as a result of the contradiction it means that $F$ has to be a set of edges that $|F| = k$ and also every cycle in $G$ contain a set in $F$ , now we need to prove that $k$ is the minimum size of a set like this. (Comment: $k = m - (n-1)$ ) Let's say $m - (n-1)$ is not the minimum size, so we have $k < m - (n - 1)$ such that $k$ is the size of a set $F$ that every cycle in $G$ contains a set in $F$ . If we erase $F$ from $G$ we will get a subgraph $G_t$ which has $m - k$ edges so we can say that the number of edges in $G_t$ is bigger than $n-1$ and we know that the number of vertices is $n$ . Because $G$ is a connected graph and every edge that we erase is contained in a cycle in $G$ , we can infer that $G_t$ is a connected graph. Because $G_t$ has more than $n-1$ edges and only $n$ vertices we can infer that $G_t$ is a connected graph that is not a tree and therefore $G_t$ has at least one cycle.
If $G_t$ has a cycle, its obvious that there is a cycle in $G$ that does not contain an edge in $F$ . Therefore every group of edges that has more than $m - (n-1)$ edges does not satisfy the conditions. The question is about the last step, if I am erasing an edge of a cycle, couldn't it stay a cycle without the edge? (Maybe if its stays a cycle it means that there is a subcycle in $G$ that doesn't contain an edge in $F$ and a subcycle is a cycle and there is the answer). Just want to make sure, and I already wrote all of this so I will post it maybe it will help other people who faced this question.","['graph-theory', 'trees', 'discrete-mathematics']"
4037672,Independence of functions of independent random variables.,"Suppose $X_1,..,X_n$ are independent random variables and $X_i'$ is an independent copy of $X_i$ , then how does one show that $$E[f(X_1,..,X_n)|X_1,..,X_{i-1},X_{i+1},..,X_n]$$ and $$E[f(X_1,..,X_i',..,X_n)|X_1,..,X_{i-1},X_{i+1},..,X_n]$$ are equal almost everywhere and why is $f(X_1,..,X_n)$ and $f(X_1,..,X_i',..,X_n)$ conditionally independent over $G:=\sigma(X_1,..,X_{i-1},X_{i+1},..,X_n)$ ? I am trying to argue using measure theoretic arguments but I can't seem to be able to show this. I am stuck trying to understand the $\sigma$ -algebras generated by both functions which are measurable on $\sigma(X_1,..,X_{i-1},X_{i+1},..,X_n)$ . Any ideas? i.e. Possible proof of conditional independence: Note: $\sigma(f(X_1,..,X_n))\subset \sigma(X_1,..,X_n)$ and $\sigma(f(X_1,..,X_i',..,X_n))\subset \sigma(X_1,..,X_i',..,X_n)$ . Let $A\in  \sigma(X_1)\cup...\cup\sigma(X_n) $ and $B\in \sigma(X_1)\cup..\cup\sigma(X_i')..\cup\sigma(X_n)$ then $$E[1_A1_B|G]=E[1_A|G]E[1_B|G]$$ This implies that it holds for all $A\in \sigma(X_1,..,X_n)$ and $B\in \sigma(X_1,..,X_i',..,X_n)$ .","['conditional-probability', 'measure-theory', 'probability-theory']"
4037694,"If $g:X\to Y$ be a one-to-one correspondence and $X$ is finite, then $Y$ is finite too.","Corollary. Let $g:X\longrightarrow Y$ be a one-to-one correspondence. If the set $X$ is finite, then $Y$ is finite. Proof. Exercise. Book :""Set Theory With Applications"" by You-Feng Lin and Shwu Yeng T.Lin. My try: We know we have a one-to-one correspondence as: $$g:X→Y$$ And also we can define a function as: $$f:X→X$$ So : $$g∘f∘g^{-1}:Y→Y$$ We have: $$g∘f∘g^{-1}(Y)=g∘f(X)$$ $$=g(f(X))$$ And because X is not Dedekind infinite so $f(X)=X$ and: $$g(f(X))=g(X)=Y$$ And by the way : $$g∘f∘g^{-1}(Y)=Y$$ So Y is finite. Is that correct?","['elementary-set-theory', 'set-theory']"
4037751,How do I find the limit of $\lim_{x \to 0} \frac{x-\ln(x+1)}{x(\sin(2x))}$?,"How do I find the limit of $$\lim_{x \to 0} \frac{x-\ln(x+1)}{x(\sin(2x))}$$ I know it is equal to $\frac{1}{4}$ , but how did we get that? Without using L'Hopital's rule. I tried canceling $x$ with $\sin(2x)$ and $\ln(x+1)$ , but still got the wrong  result...","['limits', 'calculus', 'limits-without-lhopital', 'logarithms']"
4037783,Can we turn $\mathfrak{X}(M)$ into a Hilbert Space/ Inner product space,"Given a Compact, Riemannian Manifold $(M,g)$ , I'm wondering if we can induce an inner product space on $\mathfrak{X}(M)$ , the set of smooth vector fields on $M$ that might have some interesting properties. My tentative definition will be: Given $X,Y\in\mathfrak{X}(M)$ we'll take $$X\cdot Y=\int_M g(X,Y)dvol_{M}$$ Since $X,Y$ are smooth and $M$ is compact, we can see that this product will be defined for any vector-fields on $M$ , and that this product will be positive definite, since $g_p$ is positive definite on $T_pM$ for all $p\in M$ . I believe, that in order to make it complete we would have to do quite a bit of extra work and define some kind of completion on this space since vector spaces of differentiable functions don't end up being complete due to the restrictiveness of differentiability. What kind of work would have to be done to turn the underlying structure into something resembling a well-behaved vectorspace? Would the properties of such a space tell us anything about the underlying structure of $M$ ? Are there some notions that I missed, ending up making this space ultimately uninteresting?","['hilbert-spaces', 'inner-products', 'riemannian-geometry', 'differential-geometry']"
4037836,"Continuum between arithmetic mean, AGM, and geometric mean","NOTE: I am aware of this possible duplicate , but my question is slightly different as it also involves arithmetic-geometric mean. The arithmetic mean  of two numbers is defined as: $$\text{am}(a,b) = \frac{a}{2}+\frac{b}{2}$$ For example, $\text{am}(3,12) = 7.5$ . Similarly, geometric mean of two numbers is defined as: $$\text{gm}(a,b) = \sqrt{ab}$$ So $\text{gm}(3,12)=6$ . The arithmetic-geometric mean is a mean ""between"" these two, calculated as follows: $$
a_{0}=a,g_{0}=b\\
a_{n+1}=\text{am}(a_{n},g_{n})\\
g_{n+1}=\text{gm}(a_{n},g_{n})\\
\text{agm}(a,b)=\text{lim}_{n\rightarrow\infty}\;\;(a_n)
$$ For instance, $\text{agm}(3,12) \approx 8.7407$ .
Note that $\text{gm}(a,b)\leq \text{agm}(a,b)\leq \text{am}(a,b)$ . My question: I need to define a general mean $M(a,b,c)$ for $0\leq c \leq 1$ that satifies these properties: $$
M(a,b,0) = \text{am}(a,b)\\
M(a,b,0.5) = \text{agm}(a,b)\\
M(a,b,1) = \text{gm}(a,b)
$$ Power mean is not what I'm looking for, as $PM_{0.5}\;(a,b) \neq \text{agm}(a,b)$ (e.g. $PM_{0.5}\;(3,12) = 8.7464... \neq 8.7407...$ ).","['means', 'functions', 'functional-analysis']"
4037914,How can I evaluate $\lim_{n\to\infty}\Big[ \cot(\pi\sqrt{100n^2+n+1}\Big]$,"While scrolling through questions in Brilliant I saw the following question on limit $$\lim_{n\to\infty}\Big[ \cot(\pi\sqrt{100n^2+n+1}\Big]$$ If we take $n^2$ outside square root we are left with ${100+\frac {1} {n} +\frac {1}{n^2}}$ inside square root and we can't use expansion of $\sqrt{(1+\frac{1}{n})}$ because of three terms inside square root. If we use $$\pi\cot(\pi x)=\frac{1}{x}+\sum_{k=1}^{\infty}\Big(\frac{1}{x-k}+\frac{1}{x+k}\Big)$$ with $x=\sqrt{100n^2+n+1}$ ,then our limit becomes $$\lim_{n\to\infty}\frac{1}{\pi}\Big[\frac{1}{ \sqrt{100n^2+n+1} }+\sum_{k=1}^{\infty}\Big(\frac{1}{\sqrt{100n^2+n+1} -k}+\frac{1}{\sqrt{100n^2+n+1} +k}\Big)\Big]$$ Now I got stuck on summation with this method. How can we show that this limit equals those messy square roots?","['summation', 'calculus', 'sequences-and-series', 'limits', 'trigonometry']"
4037932,Divergence theorem given $\overset{\rightharpoonup} F = x \hat{i} + y \hat{j} +z \hat{k}$ and the volume,"I am trying to calculate $\iint\limits_S \overset{\rightharpoonup} F \cdot \overset{\rightharpoonup} n\ dS$ using the divergence theorem. It is given that $\overset{\rightharpoonup} F = x \hat{i} + y \hat{j} +z \hat{k}$ and $V$ is a 3-dimensional object having volume $4$ bounded by the surface $S$ . Here are the steps I have taken: Divergence theorem $$\iint\limits_S \overset{\rightharpoonup} F \cdot \overset{\rightharpoonup} n\ dS = \iiint\limits_V \text{div} \overset{\rightharpoonup} F dx\ dy\ dz$$ Find the divergence of $\overset{\rightharpoonup} F$ $$\text{div} \overset{\rightharpoonup} F = \frac{\partial}{\partial x}x + \frac{\partial}{\partial y}y + \frac{\partial}{\partial z}z = 3$$ Substitute into the divergence theorem $$\iiint\limits_V (3)\ dx\ dy\ dz$$ $$3 \cdot \iiint\limits_V\ dx\ dy\ dz$$ Given the volume is 4, $$\iint\limits_S \overset{\rightharpoonup} F \cdot \overset{\rightharpoonup} n\ dS = 3 \cdot 4 = 12$$ Is this correct? My main doubt is whether it is permissible to replace $\iiint\limits_V\ dx\ dy\ dz$ with $4$ in the last step.","['divergence-theorem', 'multivariable-calculus', 'multiple-integral']"
4037944,Why is $f_1(n)$ not computable but $f_2(n)$ is?,"I have the following two functions, where the first one is not computable and the second one is. $$f_1(n)= \begin{cases} 
      1 & ,\text{if in the decimal representation of n appears in the decimal fraction expansion of} \ \pi \\
      0 & ,\text{else} 
   \end{cases}
$$ For example, if we have $\pi = 3.1415926 ...$ . Then $f_1(14195) = 1$ but we don't know if $f_1(333)$ has a solution. $$f_2(n)= \begin{cases} 
      1 & ,\text{if in the decimal representation of pi, there are n many consecutive 7's } \ \\
      0 & ,\text{else} 
   \end{cases}
$$ I am having a nightmare trying to understand why one is computable and the other one is not. If we take that $n =3$ , then for $f_2$ , we are trying to determine if $777$ appears in the decimal representation of $\pi$ . But in $f_1$ we already saw that determining if $f(333)$ has a solution is not computable. How are these two any different? PS - the $n$ used in $f_1(n)$ is not the same as the one used in $f_2(n)$ .","['computability', 'turing-machines', 'real-analysis', 'functions', 'computer-science']"
4037946,Difference of the Stirling cycle numbers and the Stirling set numbers,Denote by $\left\langle\!\! \left\langle k\atop  j\right\rangle\!\! \right\rangle$ the second-order Eulerian numbers A340556 . Define $$ \left| n\atop  k\right| = \sum_{j=0}^k \left( \binom{n + j - 1}{2k} - \binom{n + k - j}{2k} \right) \left\langle\!\! \left\langle k\atop  j\right\rangle\!\! \right\rangle .$$ Observe that the difference between the Stirling cycle numbers and the Stirling set numbers can be expressed as: $$ \left[ n\atop k\right] -  \left\{ n\atop  k\right\} = \left| n\atop n- k\right| \quad (0 \le k \le n)$$ It is surprising that the difference between the two Stirling numbers had no entry in the OEIS -- until today! Now they are A341102 . But what we still need is an independent combinatorial interpretation of these numbers. Who knows one? Who is willing to delight the combinatorists among us with a proof of this identity?,"['eulerian-numbers', 'combinatorics', 'stirling-numbers']"
4037983,What is wrong in this choice of $\delta$?,"Suppose I have a wrongly evaluated limit $-$ $$\lim_{x \to2} 3x+3=6$$ Then we get $-$ $$3x+3-6<\varepsilon 
$$ $$3x-3<\varepsilon
$$ $$|x-1|<\frac\varepsilon {3}$$ $$|x-2|<\frac\varepsilon 3 -1$$ $$|x-2|<\frac{\varepsilon-3} 3=\delta $$ What is wrong with this choice of $\delta$ ? (Also, please try to provide a diagram, which will help beginners like me!) P.S I had posted another question, based on this question (which was closed recently). I wrote this post making the question explicit, so would expect a decent answer.Thanks!","['epsilon-delta', 'calculus', 'functions', 'solution-verification', 'limits']"
4038051,Why is it important that every (infinite) dimensional vector space has a (hamel) basis?,"An argument often used in favor of the axiom of choice is that it is equivalent to every  infinite dimensional vector space having a hamel basis. However the article on wikipedia says that those basis are usually not very useful when they're uncountable, and that other concepts such as ""orthogonal basis"" are more important in these cases. So why does it matter whether infinite dimensional vector spaces have hamel basis?","['hamel-basis', 'soft-question', 'functional-analysis']"
4038120,"Help with the proof of Zorn's Lemma: if $A$ and $B$ are conforming subsets, why must it be that $A\subseteq B$ or $B\subseteq A$?","Here we can find a simple proof of Zorn's Lemma, but I find something that I can't really understand. There is an statement that say: If A and B are conforming subsets of $X$ and $A\neq B$ , then one of these two sets is an initial segment of the other. If I take, by example, the set $X=\{0,1,2,3,4,5,6\}$ , I can create two conforming subsets which satisfy the definition but aren't initial segment of each other, like $B=\{0,2,4,5\}$ and $C=\{0,2,3,4\}$ . This proof is similar to another used by Gorodentsev in his book Algebra I , so there is a thing that I am missing here. Definition We shall say that a subset $A$ of $X$ is conforming if the following two conditions hold: The order $\le$ is a well order of the set A. For every element $x \in A$ , we have $x = f(P(A, x))$ , where $f$ maps the initial segment to its strict upper bound x. Definition If $C$ is a chain in $X$ and $x\in C$ , then we define $P(C, x) = \{ y \in C: y < x\}$ . A subset of a chain $C$ that has the form $P(C, x)$ is called an initial segment in $C$ .","['elementary-set-theory', 'axiom-of-choice']"
4038222,Set theory difficult question on proving that set $\mathbf {H}$ always exists,"I have been puzzling over this question for literally hours: Let H be a 1011-element subset of the set {0, 1, 2, ..., 2021}. Prove that it has two (not necessarily distinct) elements a and b such that a+b is a power of 2. After watching a heap of Youtube videos about set theory, I have finally come up with this: $$\mathbf {H} \subset \mathbb {N} | \mathbf {H} _ z = m, m \in \mathbb {N}, 0 \leq m \leq 2021, |\mathbf {H}| = 1011 \\
\mathbf {J} = \{(a, b) | a + b = 2 ^ {n} | n \in \mathbb {N}\} \\
(\mathbf {H} _ x, \mathbf {H} _ y) \in \mathbf {J} | x \in \mathbb {N}, y \in \mathbb {N}, x \neq y$$ However, since I am very inexperienced, I can't solve this... I would really appreciate a solution and an explanation on how you have got to that answer :)",['elementary-set-theory']
4038231,Uniform Distributions Ratio [duplicate],"This question already has answers here : pdf of a quotient of uniform random variables (2 answers) Closed 3 years ago . Let $\xi$ is $U(0, 1)$ , $\nu$ is $U(0, 1)$ . What type of distribution is $g = \frac{\xi}{\nu}$ ? I have build logarithm of $g$ , here is the plot but it seems like it is not normal distribution: Distribution Hist","['statistics', 'probability-distributions', 'uniform-distribution', 'probability']"
4038281,Show that the image of a cube is almost a cube,"Let $C_r = \left \{ x \in \mathbb{R}^n : |x^i| < r \forall 1 \leq i \leq n \right \}$ and $ g \in C^1(U, \mathbb{R}^n)$ for some open $\left \{ 0 \right \} \subset U$ s.t $dg(\vec{0}) = I, g(\vec{0})=\vec{0}$ . and let us choose some $0 < \varepsilon < 1$ . Show that there exists $\delta > 0$ s.t $\forall r < \delta, C_{(1-\varepsilon)r} \subset g(C_{r}) \subset C_{(1+\varepsilon)r}$ I couldn't think of a better title, edits are welcome. This is what I tried: $g(0+\triangle x) - g(0) = dg(0)(\triangle x) +o(\triangle x) \implies g(x) = x+o(x) \implies \frac{||g(x) - x||}{||x||} \xrightarrow[x \to 0]{} 0$ So we can choose some $\delta > 0$ s.t if $||x|| < \delta$ then $||g(x)-x|| < ||x||_{\infty} \varepsilon$ Now $x \in C_r \implies |x^i|<r \implies |g(x)^i| < r+ ||x||_{\infty}\varepsilon \leq r+r \varepsilon \implies g(C_{r}) \subset C_{(1+\varepsilon)r}$ But i'm not sure how to show $C_{(1-\varepsilon)r} \subset g(C_{r})$ . Hints appreciated. Also, does what I did so far seem correct?",['multivariable-calculus']
4038411,Proof that two sets are equal,"$X$ is finite, if and only if, the affirmation is true, $$Y\subseteq X \text{ and } f:Y\to X\text{ surjective } \Rightarrow Y=X$$ It is easy to see that this result is true, but I am not able to properly the solution, mainly the direction $(\Leftarrow)$ . What I managed to write for the direction $(\Rightarrow)$ is the following, We have that: $X$ is finite and $Y \subseteq X \text{ and } f:Y \to X \text{ surjective}$ . Let's prove that $X=Y$ . Note that: if $X \neq Y$ , then there is some element $x \in X$ such that $ x \notin Y$ . By hypothesis, $Y$ is a subset of $X$ , that is finite, so we have that $Y$ is finite and yet $|Y| \leq |X|$ . Thus, there are bijections $\phi_{n}:I_n \to X$ and $\psi_{m}:I_m \to Y $ , where $ m \leq n $ . But, since $f:Y \to X$ is surjective, that is, $\forall x \in X, \exists \, y \in Y$ such that $ f(y)=x$ , and also $|Y|\leq |X|$ , there cannot be $x\in X $ such that $x\notin Y$ . Thus, we conclude that $X =Y$ . Is this solution correct? Could someone help me in the other direction? Edit: A set $A$ is said to be finite, if it is empty or if there is a bijection $\varphi: I_n \to A$ , where $A=\{1,\cdots,n\}$ .","['elementary-set-theory', 'functions', 'real-analysis']"
4038430,Existence and uniqueness of maximal solution for first-order non-linear ODE,"Let $\alpha \in \mathbb{R}$ and $f$ be a real function defined by $f(u)=-ue^{\alpha u}\ln(\lvert u \rvert)$ if $u \neq 0$ and $f(0)=0$ . Let $u_0 \in \mathbb{R}^+$ for the following problem: \begin{equation}
\begin{cases}
u'(t)=f(u(t)), t\in \mathbb{R}\\ 
u(0)=0
\end{cases}
\end{equation} Has the Cauchy problem a unique maximal solution? I tried: Let $g$ be a function such that: \begin{equation}
\begin{cases}
g:\mathbb{R}\times\mathbb{R} \rightarrow \mathbb{R}\\ 
g:(t,u) \rightarrow f(u(t))
\end{cases}
\end{equation} I try to verify the Cauchy-Lipschitz theorem. $f$ is continuous so $g$ is also continuous.
Now I need to prove that $g$ is locally Lipschitz with respect to its second variable. I can't manage to prove it. I tried to use the mean value theorem, but without success...
Maybe it does not verify Cauchy-Lipschitz conditions.","['lipschitz-functions', 'ordinary-differential-equations']"
4038436,Dense subalgebra intersection with essential ideal is dense?,"Consider $A$ is an unital C* algebra, $D$ is a dense *-subalgebra, $B$ is a essential,maximal ideal of $A$ . Is $D\cap B$ dense in $B$ ? I have seen this question , and this question , but my assumption is little bit different, so I think I am asking a different question here.","['c-star-algebras', 'functional-analysis', 'operator-algebras']"
4038439,Analytical solution for a neat semidefinite program (SDP),"Let $A \in S^{n}_{+}$ be a positive semi-definite matrix with all entries being non-negative. I wonder if there is an analytical solution to the following SDP in correlation matrix $X \in S^{n}_{+}$ $$\begin{array}{ll} \underset{X \in S^{n}_{+}}{\text{minimize}} & \mbox{Tr} (A X)\\ \text{subject to} & X_{ii} = 1, \quad \forall i \in [n]\end{array}$$ Does this optimization problem have an analytical solution? To share some idea on the objective, consider the spectral decomposition of matrix $A$ $$A = \sum_{k} \lambda_k y_k y^{T}_k$$ with $\lambda_k > 0$ being the positive eigenvalues of matrix $A$ and $y_k \in \mathbb{R}^{n}$ the corresponding eigenvectors. The objective is to minimize the weighted sum of variances, i.e., $$\mbox{Tr} (A X) = \sum_{k} \lambda_k y^{T}_k X y_k$$ The dual problem is $$\begin{array}{ll} \underset{D \text{ is diagonal}}{\text{maximize}} & \mbox{Tr}(D)\\ \text{subject to} & A \succeq D\end{array}$$ The problem is so neat, so I wonder if there is any hope to have an analytical solution.","['positive-semidefinite', 'convex-optimization', 'semidefinite-programming', 'matrices', 'optimization']"
4038474,An infinite product function,"Before I start, must be said that I am not a math wizard, nor a math student. I just love nudging around with math, and I came across a random function I thought in my head (I do not take credit for nothing, what I mean by 'I though'=what came to my mind) So sorry if the maths you see here are corrupt, and sorry if I post about something that was discovered long time ago. The function is $$ P(k) = \prod_{i=2}^{\infty} \left( 1- \frac{1}{i^k} \right )$$ For any $k \in (0, \infty)$ . I tried playing around with different values of $k$ and go these results: $$\begin{array}{|c|c|}\hline 
k& P(k)  \\ \hline 
2 & 0.5  \\ \hline 
3 & 0.80939 \\ \hline 
4 & 0.91901 \\ \hline 
5 & 0.96325\\ \hline 
\end{array}$$ Now, of-course this is not 'that' interesting for $k \in \mathbb{Z}$ because it tends to $1$ pretty quickly, even from $k \le 9$ we are close to $1$ by a per-mille - $P(9) = 0.99799$ . And so I wanted to see what happens for values $k = [0, 10]$ using the numpy library to use linspace as so: space = np.linspace(0, 10, 100000) # This is our range Which splits the intervals $[0,10]$ into 10K values (each are equally far away from each-other) - then calculated $P(k) ~~ \forall k \in \text{space}$ and got this beautiful output: It look roughly like what I expected it to look like, with an asymptote $y=1$ as we tend to $1$ as $k$ grows. Meaning our function (not specifically the one in the picture) is defined as so: $$ P : [0, \infty) \to [0, 1) \\ P(k) = \prod_{i=2}^{\infty} \left( 1- \frac{1}{i^k} \right ) $$ We can calculate $P(2)$ very easily as so: $$ p(2) := \prod_{i=2}^{\infty} \left( 1- \frac{1}{i^2} \right) =\prod_{i=2}^{\infty} \left( \frac{i^2 - 1}{i^2} \right) = \prod_{i=2}^{\infty} \left( \frac{(i+1)(i-1)}{i^2} \right)  ~~ \text{\\} \lg$$ $$\begin{aligned} \lg(p(2)) &= \sum_{i=2}^{\infty} \lg \left( \frac{(i+1)(i-1)}{i^2} \right) 
 = \lim_{N \to \infty} \sum_{i=2}^{N} \lg \left( (i+1)(i-1) \right) - 2\lg (i) \\&= \lim_{N \to \infty} \lg \left ( \frac{(N+1)!}{2} \cdot (N-1)!  \cdot \frac{1}{(N!)^2} \right) = \lg(0.5) = \lg(P(2)) \implies P(2) = \frac{1}{2} \end{aligned}$$ As for any other $k$ , it might be impossible to solve it as we did for $P(2)$ because $\prod \left (i^3 - 1 \right)$ does not factor nicely. The Python Code (Uses numpy and matplotlib ): import numpy as np
import matplotlib.pyplot as plt

space = np.linspace(0, 10, 10000)
Y = np.array([0])
X = space
for x in X[1:]:
    a = 1
    for i in range(2, 1000):
        a *= (1 - 1/(i ** x))
    Y = np.append(Y, [a])
plt.scatter(X, Y)
plt.show() The for-loop with the range (2, 1000) tries to approximate $\prod_{i=2}^{\infty}$ (And of-course I don't have enough 'time' 😉 to wait until $\infty$ .. so I chose a big enough value that would output a good approximation). My questions are a bit ""dry"" when it comes to the mathematics behind this function. Is this kind of function known somewhere? Used in any way? The function (for me) looks like it has a logarithmic style, is it related in any way? any approximations can be done to this function? (As I said, I am not a math-wiz unfortunately..) Thank you!","['products', 'functional-analysis', 'real-analysis']"
4038495,"If $\tan x\tan y=\frac{b}{a},\,a,\,b\ne 0$, prove that $\frac{\sec^2x}{a\tan^2x+b}+\frac{\sec^2y}{a\tan^2y+b}=\frac{a+b}{ab}$","If $\tan x\tan y=\frac{b}{a},\,a,\,b\ne 0$ , prove that $\frac{\sec^2x}{a\tan^2x+b}+\frac{\sec^2y}{a\tan^2y+b}=\frac{a+b}{ab}$ I solved it in the following way: $\frac{1+\tan^2x}{a\tan^2x+b}+\frac{1+\tan^2y}{a\tan^2y+b}$ $=\frac{a\tan^2y+b+a\tan^2x\tan^2y+b\tan^2x+a\tan^2x+a\tan^2x\tan^2y+b+b\tan^2y}{2b^2+ab(\tan^2x+\tan^2y)}$ $=\frac{a(\tan^2y+\tan^2x)+b(\tan^2x+\tan^2y)+tb+2a\tan^2x\tan^2y}{2b^2+ab(\tan^2x+\tan^2y)}$ $=\frac{(\tan^2x+\tan^2y)(a+b)+2a\tan^2x\tan^2y+2b}{2b^2+ab(\tan^2x+\tan^2y)}$ $2b^2=2ab\tan x\tan y$ $\therefore \frac{1+\tan^2x}{a\tan^2x+b}+\frac{1+\tan^2y}{a\tan^2y+b}=\frac{(\tan^2x+\tan^2y)(a+b)+2a\tan^2x+\tan^2y+2b}{ab(\tan^2x+\tan^2y+2\tan x\tan y)}$ It remains to prove that: $2a\tan^2x\tan^2y+2b=2(a+b)\tan x\tan y$ $2a\tan^2x\tan^2y+2b=ta\tan x\tan y+2b\tan x\tan y$ $2a\tan^2x\tan^2y+2b=2b+2b\tan x\tan y$ $2a\tan^2x\tan^2y=2b\tan x\tan y$ $\tan^2x\tan^2y=\frac{b}{a}\tan x\tan y$ Which is true. $\therefore \frac{1+\tan^2x}{a\tan^2x+b}+\frac{1+\tan^2y}{a\tan^2y+b}=\frac{(a+b)(\tan x+\tan y)^2}{ab(\tan x+\tan y)^2}=\frac{a+b}{ab}$ Which completes the proof. This proof is overly complex and took me over an hour to think of and write. Could you please explain to me a simpler, less messy and more intuitive proof?","['alternative-proof', 'trigonometry', 'linear-algebra', 'problem-solving']"
4038498,Best books for self-studying differential geometry,Next semester (fall 2021) I am planning on taking a grad-student level differential topology course but I have never studied differential geometry which is a pre-requisite for the course. My plan is to self-study in the summer and this semester so that I do not have to waste a semester taking a differential geometry course which will ruin my schedule. I am looking for a book that covers all of the following topics (ideally) but at least most of them: any suggestions would be welcome.,"['differential-topology', 'book-recommendation', 'differential-geometry']"
4038542,How to Evaluate $\int_{0}^{1} \frac{x^2 \ln(1-x^4)}{1+x^4}dx$?,"How to evaluate $$ \int_{0}^{1} \frac{x^2 \ln(1-x^4)}{1+x^4} \,dx \approx -0.162858 \tag{1}$$ The integral arises in the computation of $$\left( \sum_{n=1}^{\infty} \frac{(-1)^n}{n}\right)\left(\sum_{n=1}^{\infty} \frac{(-1)^n}{4n-1}\right)$$ as $$ \scriptsize{\frac{\pi \ln(2)}{4\sqrt{2}} + \frac{\ln(2) \ln(3-2\sqrt{2})}{4\sqrt{2}} -3\ln(2) + \frac{\pi}{2}= \int_{0}^{1} \frac{x^2 \ln(1-x^4)}{1+x^4} \,dx + \int_{0}^{1} \frac{x^{1/4}}{2(1+x)}\left(\tan^{-1}(x^{1/4}) - \tanh^{-1}(x^{1/4}) \right)}dx $$ A similar Integral $$ \int_{0}^{1}\left( \frac{x^2 \ln(2)}{x^4-1} - \frac{x^2 \ln(1+x^4)}{x^4-1}\right)dx = C-\frac{\pi^2}{16}+\frac{\ln^2(\sqrt{2}-1)}{4}+\frac{\pi \ln (\sqrt{2}-1)}{4} \tag{2} $$ Where $ C $ = Catalan Constant Unfortunately the same techniques I used to evaluate $(2)$ have not worked for $(1)$ .
I know only for integration - By parts, U-Sub, and  using Taylor Series as well as Mathematica. Q = Is there a closed form for Integral $(1)$ ? EDIT $$ (1) = \frac{-\pi^2}{8\sqrt{2}} + \frac{\pi \ln(8)}{4\sqrt{2}} +\frac{\ln(8) \ln(3-2\sqrt{2})}{4\sqrt{2}} -\frac{\pi \ln(3-2\sqrt{2})}{8\sqrt{2}} + 4\sum_{k=1}^{\infty} \frac{(-1)^k}{4k-1}\sum_{n=1}^{k} \frac{1}{4n-1} $$ $$\sum_{k=1}^{\infty} \frac{(-1)^k}{4k-1}\sum_{n=1}^{k} \frac{1}{4n-1} = \frac{1}{64}\left(\psi^{(1)}\left(\frac{7}{8}\right)-\psi^{(1)}\left(\frac{3}{8}\right)\right) + W $$ Where $W$ is some value.","['integration', 'definite-integrals', 'sequences-and-series']"
4038557,n-Queens problem possible solutions by logical equivalences,"I'm studying Discrete maths recently, mainly through MIT 6042J and Rosen's Discrete Math and its applications . In the later, I found the following problem I can't figure out how to proceed: The context is the well-known n-Queens problem and on the textbook, the following compound preposition is given: Let $p(i,j)$ be a proposition that is $True$ iff there's a queen in the $i$ th row and $j$ th column, where $i = 1...n$ and $j = 1...n$ . to check all row contains at least one queen: $Q_1 = \land_{i=1}^n\lor_{j=1}^np(i,j)$ to check at most one queen per row: $Q_2 = \land_{i=1}^n\land_{j=1}^{n-1}\land_{k=j+1}^n(\lnot p(i,j)\lor\lnot p(k,j))$ Here comes my first question. I believe it's wrong and should be $Q_2 = \land_{i=1}^n\land_{j=1}^{n-1}\land_{k=j+1}^n(\lnot p(i,j)\lor\lnot p(\textbf{i,k}))$ but I couldn't find any public errata. Does it make sense? to check at most one queen per column: $Q_3 = \land_{j=1}^n\land_{i=1}^{n-1}\land_{k=i+1}^n(\lnot p(i,j)\lor\lnot p(k,j))$ to assert at most one queen on the diagonals: $Q_4 = \land_{i=2}^n\land_{j=1}^{n-1}\land_{k=1}^{min(i-1,n-j)}(\lnot p(i,j)\lor\lnot p(i-k,k+j))$ $Q_5 = \land_{i=1}^{n-1}\land_{j=1}^{n-1}\land_{k=1}^{min(n-i,n-j)}(\lnot p(i,j)\lor\lnot p(i+k,j+k))$ So, to find valid results we need: $Q = Q_1 \land Q_2 \land Q_3 \land Q_4 \land Q_5$ I understand all of the proposed compound propositions and how they work. I could even easily convert them into an algorithm. But the proposed exercises ask us to use them to find all the possible solutions for $n=4$ . There are 65536 combinations for that, so, my second and final question is: is it possible to reduce these compounds with logic equivalences and, I'm not seeing it, or is it more probable that the book expects a computational solution for that? Thanks in advance.","['applications', 'propositional-calculus', 'satisfiability', 'discrete-mathematics']"
4038578,Normalizing constants preserve metric entropy,"Suppose $\mathcal{F}=\left\{f\in L^2([a,b]): 0<\underline{c}\leq f\leq\overline{c} \right\}$ . Consider the following transformation $$\tilde{\mathcal{F}} := \left\{\frac{f}{\int f d\mu}: f\in \mathcal{F}\right\}$$ Want to show the claim that $\mathcal{F}$ and $\tilde{\mathcal{F}}$ have the $\epsilon$ -metric entropy (log of covering/packing number under $L_2$ norm) of same order. To put this in a more concrete context, suppose that $\mathcal{F}$ is a Sobolev ellipsoid, i.e $$\mathcal{F} = \mathcal{E}_k(A) = \left\{f\in L^2([a,b]): f = \sum_{j=0}^\infty \theta_j\phi_j(X), \sum_{j=0}^\infty \theta_j^2j^{2k}< A\right\}$$ Suppose $k>1$ and $\phi_j$ 's are uniformly bounded so that $\mathcal{E}_k(A)$ is uniformly bounded, say by a constant $\rho$ . We can see that a transformation of $\mathcal{E}_k(A)$ , say $$ \tilde{\mathcal{E}}_k(A) := \left\{\frac{f + \rho + 1}{\int f d\mu + \rho + 1}: f\in \mathcal{E}_k(A) \right\}$$ is a subset of $\mathcal{E}_k(A')$ for some $A'$ . In some well-published papers (eg. Yang and Barron 1999, p. 1591-), the authors claim that for $A$ large enough, $\tilde{\mathcal{E}}_k(A)$ and and $\mathcal{E}_k(A)$ have the same order of $L^2$ metric entropy. They stated in the paper ""it is easy to see"". While I can ""see"" it intuitively, I have yet to be able to come up with a rigorous proof. In Yang and Barron's paper, they also give some other function classes that they claim such property holds. Note 1: for the Sobolev ellipsoid, I think the convex property may play a role. Note 2: One direction is easy. Consider any $f,g \in\mathcal{F}$ , and let $\tilde{f} = f/\int f$ and $\tilde{g} = g/\int g$ . We can show that for some constant $C$ , $$ \|\tilde{f}-\tilde{g}\|_2 \leq C \|f - g\|_2 $$ The question is however if there's a reverse inequality of the form $\|f - g\|_2\leq C'\|\tilde{f}-\tilde{g}\|_2 $ for some constant $C'$ .","['entropy', 'real-analysis', 'functional-analysis', 'probability-theory', 'probability']"
4038581,Prove that $\lim_{n\to\infty}\sin(nx)/ \pi x$ is a delta function,"I need to prove that $\delta_n(x)=\sin (nx)/\pi x$ is a delta function. That is, to prove that: $$\underset{n\rightarrow \infty}{\lim}\int^\infty_{-\infty}f(x)\frac{\sin( nx)}{\pi x}dx=f(0).$$ For that I made $y=x/n$ and took the limit under the integral since $f(x)$ is supposed to be analytic in $\mathbb{R}$ . I have a fundamental limit for the $\sin$ leaving me with $$\int^\infty_{-\infty}f(0)\cdot1 \;dy.$$ But this is equal to $f(0)$ $\iff$ $f(x)$ have contribuition only at $x=0$ , but here the only assumption is that $f(x)=0$ when $x\rightarrow \pm \infty$ . What is wrong here?","['limits', 'dirac-delta', 'distribution-theory']"
4038605,The reason for the superiority of Lebesgue measure / Lebesgue integration,"I'm learning about Lebesgue measure and Lebesgue integration. I know that Lebesgue integration is superior to Riemann integration in the sense that we can integrate a much larger class of function with Lebesgue integral. I would like to ask which is the main reason that creates the superiority of Lebesgue integral? EDIT As for me, basically, Lebesgue integration is another way to calculate the integration (or geometrically, the area under the curve), so why just changing the way we compute can make such a huge difference between those 2 kind of integration ?","['measure-theory', 'lebesgue-measure', 'lebesgue-integral']"
4038617,"Hodge star operator and ""Serre duality""","I am familiar with the Hodge star operator or Hodge duality in the theory of finite-dimensional differentiable manifolds, which gives an isomorphism $\star:\Omega^{i}(M)\longrightarrow\Omega^{n-i}(M)$ if $M$ is an $n$ -dimensional manifold. The existence of this star operator arises essentially from the perfect product $\Omega^{i}(M)\wedge\Omega^{n-i}(M)\longrightarrow\Omega^{n}(M)$ . Deligne and Illusie ( https://eudml.org/doc/143480 , p. 255) make the claim that this can be generalized to the case where $X\longrightarrow S$ is a proper smooth morphism of $\mathbb{F}_{p}$ schemes with $\dim(X/S)=n$ and claim that in this case $\Omega^{i}_{X/S}$ and $\Omega^{n-i}_{X/S}$ are dual over $\Omega^{n}_{X/S}$ . This is apparently called Serre duality, but I have difficulties to find an exact proof of this claim. (I am probably too stupid to do a decent literature research). Does anybody have a link or a book reference?","['algebraic-geometry', 'duality-theorems', 'sheaf-cohomology', 'coherent-sheaves']"
4038683,How to divide a Sine wave into n equal power/area sections?,"I'm working on an electronics project which involves using a small micro-controller to dim an AC Mains light bulb. I want to keep it simple and instead of having a continuous dimming adjustment I'll limit it to 'n' settings. So Off, 1/10th brightness, 2/10th brightness... on up to 9/10ths and 10/10 full brightness, always on. I can't do that by simply dividing the sine wave in 10 equal durations, as in 1/10th the period, 2/10ths the period etc. Brightness is related to power which is related to the area under the curve. I only have to consider one positive sine half period section. The same time values would be used for the negative half of the wave's period. Area under the curve involves integrating the sine wave, if I remember my school calculus correctly, which is a cosine function, but the dividing it into equal areas has me stumped. I think I have 0%, 50% and 100% brightness figured out, but that's about it ;) This question has probably been answered already, but I'm probably asking it wrong. There's probably a maths way of expressing the question in a way a search engine could find.","['calculus', 'trigonometry']"
4038685,$u(x)$ satisfies the integral equation $u(x) = \int_0^x \sin(u(t))u(t)^p dt$ on $0 \leq x \leq 1$. show that $u(x) = 0$ on this interval,"Suppose that $u(x)$ is continuous and satisfies the integral equation \begin{equation}\label{1.4.1}
        u(x) = \int_0^x \sin(u(t))u(t)^p dt
    \end{equation} on the interval $0 \leq x \leq 1$ . show that $u(x) = 0$ on this interval if $p \geq 0$ . This is what I have: Since $\sin(u(t))u(t)^p$ is continuous, it follows from the integral definition that $u(x)$ is differentiable. Let us differentiate both sides of equation above with respect to $x$ . This yields: \begin{equation}
    u'(x) = \sin(u(x))u(x)^p
    \end{equation} This ODE is separabale and becomes: \begin{equation}
        \frac{1}{\sin(u(x))u(x)^p}du(x) = dx
    \end{equation} However, this doesn't seem easily solvable so I'm not sure how to show that $u(x) = 0$ from this.",['ordinary-differential-equations']
4038692,Probability of at least $2$ balls in random bin when distributing $K$ identical balls to $N$ distinct bins?,"If we distribute $k$ identical balls to $n$ distinct bins, what is the probability a randomly selected bin will contain at least 2 balls, assuming all balls have a uniform probability of being placed in any bin and $k < n$ . Since it's probability, not ""number of ways to arrange,"" and all distributions have equal probability, first I found the total number of distributions by assuming $k$ distinct balls, $n^k$ , which means there are $n^{k+1}$ bins across all distributions. I can figure out that if there are $k < n$ balls, the number of distributions where every bin has at most $1$ ball is $k \choose n$ (we have $k$ balls and each chooses 1 distinct bin), so guaranteed ${k \choose n} n$ bins across all distributions have fewer than $2$ balls. But I'm not sure how to figure out how many of the $n$ bins contain fewer than $2$ balls in the cases where some bins have $2$ or more balls. I could easily be going about this from the wrong angle completely though.","['balls-in-bins', 'probability']"
4038716,Using definition of limits,"Let $c∈\mathbb{R}$ and let $f:\mathbb{R}\setminus{c}\rightarrow\mathbb{R}$ be a function such that $f(x)>0$ for all $x∈\mathbb{R}$ . Use the definition of limits to prove that $$
\lim_{x\to c}f(x)=\infty  \space\space\space\space\space\space\space\space\space\space\space   \text{iff}  \space\space\space\space\space\space\space\space\space\space\space    \lim_{x\to c}\frac{1}{f(x)}=0.
$$ Proving the "" $\Rightarrow$ "": Here is the definition: $\lim_{x\to c}f(x)=\infty$ if $\forall M∈\mathbb{R},\exists\delta>0$ such that $\forall x∈\mathbb{R}, 0<|x-c|<\delta\Rightarrow f(x)>M$ . Here is my proof: Let $\epsilon >0$ and set $M=\frac{1}{\epsilon}$ . Since $\lim_{x\to c}f(x)=\infty$ , we can find a $\epsilon >0$ such that $f(x)>M$ whenever $0<|x-c|<\delta$ . Thus $0<\frac{1}{f(x)}<\frac{1}{\epsilon}$ whenever $0<|x-c|<\delta$ . This implies that it is possible to find a $\delta>0$ such that $|\frac{1}{f(x)}|<\epsilon$ whenever $0<|x-c|<\delta$ . Since $\epsilon$ is arbitrary, we have proved that $\lim_{x\to c}\frac{1}{f(x)}=0$ . Proving the "" $\Leftarrow$ "" : This proof I am unsure of. I know that by the definition of a limit, $\lim_{x\to c} f(x) = 0$ if $\forall\epsilon>0, \exists\delta>0$ such that $\forall x∈\mathbb{R}\setminus{c}, 0<|x-c|<\delta \Rightarrow |f(x)-0|<\epsilon$ . I am unsure of how to define $\lim_{x\to c}\frac{1}{f(x)}=0$ in a similar way. Any advice would be greatly appreciated.","['limits', 'analysis', 'real-analysis']"
4038743,Various proofs: pairs of points in a circle = Möbius strip,"It is well-known that the space of ""pairs of points in a circle"" (also called the symmetric square of $S^1$ ) can be identified with a Möbius strip. I don't know where the idea originates, but it is a neat elementary result in topology, which has a beautiful application in Vaughan's proof of the rectangular peg problem ( 3blue1brown gives a really nice account of this ). Inspired by a talk I saw on related results, I've been trying to compile a list of proofs that the symmetric square of $S^1$ is a Möbius strip. To be specific, the space $Sym^2(S^1)$ is constructed from $S^1\times S^1$ by identifying $(x,y)\sim (y,x)$ for every $x,y\in S^1$ and taking the quotient topology. Hence, I am asking this question as community wiki, to try and learn of other peoples' favorite ways to prove this result. A quick ground rule: if someone beats you to the chase on your favorite proof, maybe comment to show your preference for it, rather than posting the same proof again? But if you have a proof that seems like a close variant of one already posted, feel free to post it! Since these are all about the same result, there are bound to be variations on the same idea. Also, if you know where your proof comes from, please give proper credit to its originator. If the proof is your own, be explicit about that fact, so that I can make sure to give you proper credit if I ever cite it elsewhere. Here are summaries of some proofs that I know of, to get the ball rolling: View the torus $S^1\times S^1$ as square with opposite sides identified. The identification $(x,y)\sim (y,x)$ corresponds to folding the square along its diagonal, so we get a triangle with gluing between two adjacent sides. A brief cut-and-paste argument turns this into the usual presentation of the Möbius strip. (I don't know where this argument originates.) Charles Livingston: To any pair of points $x,y\in S^1$ in the unit circle of $\mathbb R^2$ , we associate their perpendicular bisector, which is a line through the origin and thus an element of $\mathbb R\mathbb P^1$ (if $x=y$ , we take the line through this point and the origin). This defines a map $Sym^2(S^1)\rightarrow \mathbb R\mathbb P^1$ , which is a fiber bundle with the interval as its fiber. By checking that this bundle is not orientable (i.e. going around $\mathbb R\mathbb P^1$ once flips the interval), we can see that it is a Möbius strip. (This argument—and a nice picture of the previous one—are included as §2.2 of this paper by Tuffley.) Hugh Morton ( summarized on nLab ): This is a case of Morton's more general description of $Sym^n(S^1)$ . Consider $S^1\subset \mathbb C$ as the unit circle and define a map $\text{Sym}^2(S^1)\rightarrow S^1$ by sending a pair $(x,y)$ to their product $xy.$ As in the previous proof, we can check that this is a fiber bundle with the interval as its fiber, and that this bundle is not orientable (i.e. going around $S^1$ once flips the interval), so it is a Möbius strip. Étienne Ghys: We can view $\mathbb R\mathbb P^2$ as the plane $\mathbb R^2$ with a copy of $\mathbb R\mathbb P^1$ at infinity. Then the Möbius strip $M$ is formed by cutting the open unit disk out of our projective plane. For any two points $p,q$ on the unit circle $S^1\subset \mathbb R^2$ , we can take the tangents $\ell_p$ and $\ell_q$ to the circle at these points. The map which sends $(p,q)\mapsto \ell_p\cap \ell_q$ is a homeomorphism $Sym^2(S^1)\rightarrow M$ , where the $\mathbb R\mathbb P^1$ at infinity corresponds to intersections of parallel lines. (I believe this is from “Prolongements des diffeomorphisms de la sphère,” but I am not certain.) Karanbir Sarkaria: We can view $\mathbb R\mathbb P^n$ as the space of non-zero, homogeneous polynomials of degree $n$ in $\mathbb R[x,y]$ , up to multiplication by a non-zero scalar. Then we have a map $\mathbb R\mathbb P^1\times \mathbb R\mathbb P^1\rightarrow \mathbb R\mathbb P^2$ , given by $$\big((rx+sy),(ux+vy)\big)\mapsto (rx+sy)(ux+vy).$$ This descends to a map $\text{Sym}^2(\mathbb R\mathbb P^1)\rightarrow \mathbb R\mathbb P^2$ , which is a homeomorphism onto its image. But $ax^2+bxy+cy^2$ lies in the image of this map if and only $b^2-4ac\geq 0$ . Hence, we must cut out from $\mathbb R\mathbb P^2$ the set of quadratics with complex roots, which is homeomorphic to the open disk. But removing a disk from $\mathbb R\mathbb P^2$ yields the Möbius strip, as desired. (See Sarkaria's note for a nice illustration of this fact.) Map a pair of points $\{p,q\}$ on the unit circle to their midpoint $(p+q)/2$ . This is a bijection between the punctured disk $D^2-{0}$ and pairs $\{p,q\}$ of non-antipodal points, but every pair of antipodal points gets mapped to $0$ . In other words, we can form $Sym^2(S^1)$ from $D^2$ by replacing the origin with a copy of $\mathbb R\mathbb P^1$ . This blowup of the disk is precisely a Möbius strip. (Again, I don't know where this argument originates.) The fact in question also relates to a fascinating result of Bott and Shchepin. Let $\exp_n(S^1)$ consist of all subsets of $S^1$ containing between 1 and $n$ points. This is topologized via the Hausdorff metric on compact subspaces of $S^1$ . Bott showed that $\exp_3(S^1)=S^3$ ("" On the third symmetric potency of $S^1$ "") and Schepin showed that the natural embedding $S^1=\exp_1(S^1)\hookrightarrow \exp_3(S^1)=S^3$ is a trefoil knot (unpublished). Identifying pairs of equal elements with singleton sets, we get $Sym^2(S^1)=\exp_2(S^1)$ , so the Möbius band $\exp_2(S^1)$ fits into this picture as a (non-orientable) Seifert surface of the trefoil. This theorem is refined in the aforementioned article by Tuffley, and has also been proven by: Nakandakari and Tsukuda via elementary pasting techniques (they animate their construction here ), Mostovoy via lattices in $\mathbb C$ , and Rose via isometries of the hyperbolic plane. I am not describing these in detail because they are lengthier and I haven't taken the time to fully understand them yet, but if you know of another description of $Sym^2(S^1)$ inside $\exp_3(S^1)=S^3$ , that would also be a great thing to contribute.","['general-topology', 'configuration-space', 'fiber-bundles']"
4038781,Showing a property holds in arbitrary measure space,"A measure space $\left(X,\mathcal{M},\mu\right)$ is said to have Property A if for every $A\in\mathcal{M}$ with $\mu(A)>0$ there is a set $B\subset A$ , $B\in\mathcal{M}$ such that $0<\mu(B)<\mu(A)$ . Let $\left(X,\mathcal{M},\mu\right)$ be a measure space with Property A . Show that for every $A\in\mathcal{M}$ with $0<\mu(A)<\infty$ and for every $\epsilon>0$ there is a set $B\subset A$ , $B\in\mathcal{M}$ such that $0<\mu(B)<\varepsilon$ . I am not really sure how to go about doing this. I'd expect some argument relying on countable additivity to be made, e.g. $$\mu(B)<\sum_n \frac{\varepsilon}{2^n} = \varepsilon.$$ But I am just not sure overall how to do such problem.","['measure-theory', 'real-analysis']"
4038893,"Continuity with respect to Hausdorff metric, also with intersection?","Let $H$ be a real Hilbert space and let $v\in H$ , $\|v\|=1$ . Now, consider the affine subspaces $A(t) := tv + v^\perp$ of codimension $1$ , $t\in\mathbb R$ . Obviously $$
d_H(A(t),A(s)) = |t-s|
$$ for all $t,s\in\mathbb R$ . Hence, $t\mapsto A(t)$ is Lipschitz-continuous with respect to the Hausdorff metric. That was quite easy. However, what I really would like to show is that $f : t\mapsto A(t)\cap B$ is continuous, where $B$ is a bounded, closed, and convex subset of $H$ and $f$ is defined on $K := \{t\in\mathbb R : A(t)\cap B\neq\emptyset\}$ , which is a closed set. I've tried a lot, but I can't seem to prove this. So, I'd be happy if anyone had an idea. For clarification: the Hausdorff distance between two sets $A,B\subset H$ is defined by $$
d_H(A,B) := \max\left\{\sup_{a\in A}\operatorname{dist}(a,B),\,\sup_{b\in B}\operatorname{dist}(b,A)\right\}.
$$","['hausdorff-distance', 'real-analysis', 'functional-analysis', 'general-topology', 'convex-analysis']"
4038943,"Find $\int_0^4(g\circ f\circ g)(x)\mathrm{d}x$ where $f(x)=\sqrt[3]{x+\sqrt{x^2+1/27}}+\sqrt[3]{x-\sqrt{x^2+1/27}}$, $g(x)=x^3+x+1$","Let $$f(x)=\sqrt[3]{x+\sqrt{x^2+\frac{1}{27}}}+\sqrt[3]{x-\sqrt{x^2+\frac{1}{27}}}$$ and $$g(x)=x^3+x+1$$ then, find $$\int_0^4(g\circ f\circ g)(x) \mathrm dx$$ My attempt: Let $\displaystyle h(x)=\sqrt{x^2+\frac{1}{27}}$ $$(g\circ f)(x)=2x+3((2x)(x^2-[h(x)]^2))^{1/3}+(x+h(x))^{1/3}+(x-h(x))^{1/3}+1$$ Is finding $(g\circ f \circ g)(x)$ in term of $x$ necessary? Because it is quite a work to do.","['integration', 'definite-integrals', 'calculus', 'functions', 'algebra-precalculus']"
4038980,Intuitive Understanding of Independence of 3 Events,"I understand what it means to say that an even E is independent of an event F, it means that knowing that F has occurred does not change the probability that E will occur. Algebraically, it implies: $$P(E) = P\left(E \mid F\right)   \stackrel{\text{def}}{=} \frac{P\left(E \cap F\right)}{P\left(F\right)} \Leftrightarrow P\left(E \cap F \right) = P\left(E\right)  P\left(F\right)$$ Now I'm trying to tackle what it means for events E, F, and G to be independent, the definition stated in the book says: $$\begin{gathered}
  P\left(E \cap F \cap G\right) = P\left(E\right) P\left(F\right) P\left(G\right)\\
  P\left(E \cap F\right) = P\left(E\right) P\left(F\right) \\
  P\left(E \cap G\right) = P\left(E\right) P\left(G\right) \\
  P\left(F \cap G\right) = P\left(F\right) P\left(G\right)\end{gathered}$$ Based on my previous understanding the bottom three lines would be implying that E is independent of F, E is independent of G, and that F is independent of G. But I'm not sure what the first line is saying. The way I've made sense of it so far is that: $$ P\left(E \cap F \cap G\right)= P\left( E \cap \left( F \cap G  \right) \right) \stackrel{?}{=} P\left(E \right) P\left(F \cap G\right) \stackrel{\alpha}{=} P\left(E\right) P\left(F\right)P\left(G\right)$$ (Where $\alpha$ comes from the fact that E and F were independent.) In order for the equality with the question mark to hold, I think we would need $E$ to be independent from $F \cap G$ . I am a little confused by their definition and tried to figure it out algebraically, but that's what I've gotten up to. I am hoping someone can: Give me an intuitive understanding of what it means for 3 events to be independent Help me connect this understanding to the definition they have provided Bonus: Help me take that understanding to the independence of $n$ events",['probability']
