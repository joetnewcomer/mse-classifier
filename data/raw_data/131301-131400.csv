question_id,title,body,tags
2050978,Why do physicists get away with thinking of the Dirac Delta functional as a function?,"For instance they use it for finding solutions to things like Poisson's Equation, i.e. the method of Green's functions . Moreover in Quantum Mechanics, it's common practise to think of the delta functions $\delta_x$ as being a sort of standard basis for the vector space of square integrable functions but $\delta$ is obviously not a square integrable function itself so how can it form a basis for something that it's not a even an element of. (On a slightly separate issue, it also doesn't make sense to me why you can think of the functions $e^{ikx}$ (Fourier Basis) as a basis square integrable functions since $e^{ikx}$ is also not square integrable.
) Are physicists just really lucky that these things work out or is there some deeper underlying reason why it's okay to think in these terms? I've come across a lot of great books by a lot of great physicists who've simply assumed this to be the case. It's hard to believe that they were all naive enough to dismiss the mathematical fallacy in their arguments. Clearly I 'm missing something. My question is what is it? An elaborate answer would be much appreciated.","['quantum-mechanics', 'functional-analysis', 'dirac-delta', 'linear-algebra', 'vector-spaces']"
2050995,How to approximate the shape of a human face,"I am developing an Android application and the task is to check whether the user face is within the head shape. I am able to get facial landmark points (like nose, eyes etc) and have to check if they are within the shape. How can I approximate the shape ( without ears ) good enough to successfully check the above? Using a simple oval seems not accurate enough. EDIT : I suppose the most exact solution would be to define a set of points on the shape edges and interpolate the function with splines, with analytical approach the accuracy won't be so good.",['geometry']
2051015,Find the minimum-variance unbiased estimator for given $\tau(\theta)$,"Let $X = (X_1, \dots, X_n)$ - a sample from the distribution $U (0,\theta)$. Prove that $T(X) = X_{(n)}$ is complete and sufficient estimation for $\theta$ and find the minimum-variance unbiased estimator $T^*(X)$ for a differentiable function $\tau(\theta)$. The proof of sufficiency can be very easily carried out using the factorization criterion. I have done it. Next we need to prove completness. By the definition we need to prove that $\mathbb{P}(g(x) = 0) = 1$ from $$\mathbb{E}_{\theta}g(T(X)) = \displaystyle\int\limits_{[0,\theta]}g(x)\frac{n y^{n-1}}{\theta^n}dx = 0, \quad \forall \theta > 0$$ It can be easily done if $g(x)$ continuous.
 $\displaystyle\int\limits_{0}^\theta g(x)y^{n-1}dx = 0 $ $g(\theta)\theta^{n-1} = 0$. Than $g(\theta) = 0$. It works for continuous functions but how to prove it for all $g(x)$ such that $\mathbb{E}_{\theta}g(T(X))$ exists? And next I need to find the minimum-variance unbiased estimator $T^*(X)$ for a differentiable function $\tau(\theta)$. It seems like it is connected with the first question and can be done using something like Lehmann–Scheffé theorem, but I do not know how to do it exactly. Great thanks for the help!","['parameter-estimation', 'uniform-distribution', 'statistics', 'statistical-inference']"
2051042,"Evaluate $\displaystyle \lim_{(x,y)\to (0,0)}\frac{x^2}{|x|+|y|}\cos(y^2)$","$$\lim_{(x,y)\to (0,0)}\frac{x^2}{|x|+|y|}\cos(y^2)$$ I have tried the following paths:
$$(x,kx),(x,kx^2),(ky,y)$$ and I get that the limit is $0$ but according to WA there is not limit , how should I approach this?","['multivariable-calculus', 'real-analysis', 'limits']"
2051054,Finding existence of $ f : N → N $ with the following properties: $f(1) = 2$ and $f(f(n)) = f(n)+n (n ∈ N).$,"I have got a question related to functional equations which is as follow: Let $N =$ { ${1,2,3,...}$ }. Determine whether there exists a strictly increasing function $ f : N → N $ with the following properties: $f(1) = 2$ and $f(f(n)) = f(n)+n$ where $(n ∈ N)$ . I tried applying the method which I apply to every question of such kind. Put $n=1$ and then $f(f(1)) = f(1)+1\implies f(2)=3$ Put $n=2$ and then $f(f(2)) = f(2)+2\implies f(3)=5$ Now, if I put $n=3$ I will get $f(f(3)) = f(3)+3\implies f(5)=8$ . There are two problems now. $1.$ I don't see any pattern. $2.$ If I continue this way. I will miss $n=4,6,7......$ What to do now. Please help, any suggestion is heartily welcome. Edit: This is IMO1993 Contest Problem 5 (2nd problem on 2nd day).","['contest-math', 'functions', 'functional-equations']"
2051096,Integer solutions of $n^2+n = 2x^2+2x$,"I know that the integers solutions of the equation:
$$ n^2+n = 2x^2+2x $$
are $$n = \frac{1}{4} (-(3 - 2 \sqrt{2})^m - \sqrt{2} (3 - 2 \sqrt{2})^m - (3 + 2 \sqrt{2})^m + \sqrt{2} (3 + 2 \sqrt{2})^m + 2),$$
$$x = \frac{1}{8} (2 (3 - 2 \sqrt{2})^m + \sqrt{2} (3 - 2 \sqrt{2})^m + 2 (3 + 2 \sqrt{2})^m - \sqrt{2} (3 + 2 \sqrt{2})^m + 4),$$ $m \in \mathbb{Z}, m\ge0$ but I don't understand how. Someone can point me in the right direction to solve this problem.","['number-theory', 'pell-type-equations', 'diophantine-equations']"
2051109,"Derivative $\frac{d\left\{B(A_1, \ldots, A_M)\right\}}{d\left\{C(A_1, \ldots, A_M)\right\}}$ equals what?","Question : \begin{equation}
\dfrac{dB}{dC} = ?
\end{equation} Context The present problem might be considered a practical exercise with respect to the ongoing discussion in [1].  In addition, [2] offers a related question in that both the question here and there are derivatives with respect to a function. Given : \begin{align}
A & = f{\left(A_1, \ldots, A_M\right)} =   \sum_{k = 1}^{M}{A_k} = \textrm{constant  
}\\
B & = g{\left(A_1, \ldots, A_M\right)} =   \sum_{k = 1}^{M}{B_k(A_k)} 
\\
C & = h{\left(A_1, \ldots, A_M\right)} =   \sum_{k = 1}^{M}{C_k(A_k)} 
\end{align} Approach I: Ratio of Two Differentials : The differentials of $A$, $B$, and $C$, are respectively written as
\begin{align}
dA & =  \sum_{k = 1}^{M}{dA_k} = 0 \quad\quad  \textrm{Eq. 1}
\\
dB & =  \sum_{k = 1}^{M}{\dfrac{\partial{B_k}}{\partial{A_k}}}  \, dA_k \quad\quad  \textrm{Eq. 2}
\\
dC & =  \sum_{k = 1}^{M}{\dfrac{\partial{C_k}}{\partial{A_k}}}  \, dA_k \quad\quad  \textrm{Eq. 3}
\end{align} Next, arbitrarily singling out the $M$th differential, $dA_M$, we rewrite Eq. 1 as
\begin{align}
dA_M & =  -\sum_{k = 1}^{M-1}{dA_k}
\end{align}
and continue to rewrite Eqs. 2 and 3 as
\begin{align}
dB & =  \sum_{k = 1}^{M-1}\left[\dfrac{\partial{B_k}}{\partial{A_k}}  - \dfrac{\partial{B_M}}{\partial{A_M}} \right]  \, dA_k
\\
dC & =  \sum_{k = 1}^{M-1}\left[\dfrac{\partial{C_k}}{\partial{A_k}}  - \dfrac{\partial{C_M}}{\partial{A_M}} \right]  \, dA_k
\end{align} Naively or not, I write the derivative as the ratio of the differentials. My solution is thus
\begin{equation}
\dfrac{dB}{dC} = \dfrac{\sum_{k = 1}^{M-1}\left[\dfrac{\partial{B_k}}{\partial{A_k}}  - \dfrac{\partial{B_M}}{\partial{A_M}} \right]  \, dA_k}{\sum_{k = 1}^{M-1}\left[\dfrac{\partial{C_k}}{\partial{A_k}}  - \dfrac{\partial{C_M}}{\partial{A_M}} \right]  \, dA_k}.
\end{equation} Approach II: Chain Rule : From [3], let $A=A(A_1, A_2, \ldots, A_M) = \textrm{constant}$,   $B = B(A_1, A_2, \ldots, A_M)$, and $C =  C(A_1, A_2, \ldots, A_M)$, then the partial derivative of $C$ with respect to $B$, holding $A$ constant, written $\left(\dfrac{\partial{C}}{\partial{B}}\right)_A$, can be expressed as \begin{align}
\left(\dfrac{\partial{B}}{\partial{C}}\right)_A = \sum\limits_{k=1}^{M} {   \left(\dfrac{\partial{B}}{\partial{A_k}}\right)_{\substack{A_{1,\ldots, j, \ldots, M}\\ k \neq j }}  \, \left(\dfrac{\partial{A_k}}{\partial{C}}\right)_A    }
\end{align} The subscripts on the parenthesis above indicate which variables are considered constants with respect to the given partial derivative.  In particular, $\substack{A_{1,\ldots, M}\\ k \neq j }$ indicates that all of the $A_j; j\in 1,\ldots, j, \ldots, M$ are considered constants, save the partial derivative in question (i.e, the $k^\textrm{th}$ one). Discussion Question: Why did I start by asking for a a derivative and end up with a partial derivative? Answer: The initial question appears to be ill-posed. As opposed to finding the derivative, what I really saught all along was $\left(\dfrac{\partial\left\{B(A_1, \ldots, A_M)\right\}}{\partial\left\{C(A_1, \ldots, A_M)\right\}}\right)_A$. I am purposefully leaving the title as a misnomer because like myself, there might be others who would not think to search for a constrained partial derivative. Conclusion Beyond the solution to my question, this question offers a demonstration that the ratio of two differentials is not a derivative. Referencess [1] Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? [2] Differentiating with respect to a function [3] CRC Math Encyclopedia, Edition XX, 336.","['multivariable-calculus', 'partial-derivative', 'differential', 'derivatives']"
2051111,Eigenvectors for prime numbers matrices,"I have noticed that eigenvectors for matrices $2\times{2}$ made from $4$ consecutive big prime numbers, for example $\begin{bmatrix}
100003 & 100019 \\
100043 &100049 \\
 \end{bmatrix}$, have ""always"" approximate eigenvectors  presented below in the matrix $R=\begin{bmatrix}
  v_1 & v_2 \\
 \end{bmatrix} = \begin{bmatrix}
\dfrac{\sqrt{2}}{2}  &-\dfrac{\sqrt{2}}{2} \\
\dfrac{\sqrt{2}}{2} & \dfrac{\sqrt{2}}{2}\\
 \end{bmatrix} = Rotation(\pi/4) $ I suppose it's  not only characteristic for prime numbers but also for any big numbers with relatively small differences between consecutive numbers, but .... how to prove that it holds just for 4 consecutive big prime numbers? What are consequences of these approximate eigenvector forms? P.S. Please notice dear reader that I'm not asking whether property I have presented is valid exclusively for primes numbers (big ones), but ..  whether it is valid also for big prime numbers in any situation what requires however a little analysis of prime numbers properties. For example to use Legendre conjecture .","['matrices', 'number-theory', 'prime-numbers', 'linear-algebra']"
2051125,A bounded function in $\Bbb R$ with closed graph is continuous.,"It is known that if a function $f:\Bbb R\to \Bbb R$ is continuous then its graph is closed. Proof. Let $(x_n)_{n\in\Bbb N}$ be a sequence in $\Bbb R$ so that the sequence $(x_n,f(x_n))_{n\in\Bbb N}$ is convergent in $\Bbb R^2$ at a point $(x,y)\in\Bbb R^2$ . Then as the convergence in $\Bbb R^2$ is point-wise we have $$(x_n,f(x_n)) \xrightarrow{n\to \infty} (x,y) \Longrightarrow x_n\to x \ \  \& \ \  f(x_n)\to y $$ Now, from the continuity of $f$ we have that $$x_n \to x \Longrightarrow f(x_n)\to f(x)$$ and from the uniquence of the limits we assume that $y=f(x)$ . So, $\lim_{n\to\infty}(x_n,f(x_n))=(x,f(x))\in G(f)$ and so $G(f)$ is closed. We know that the converse is not true in general, that is if the graph of a real function $f$ is closed we cannot conclude that $f$ is continuous. One counter-example is the function: $$f: \Bbb R \to \Bbb R, \ \ f(x)=\begin{cases}
 \text{$\frac{1}{x} \ \ \ \ $ if } x \neq 0 \\ 
  \text{$0 \ \ \ \ \ $ if } x= 0
\end{cases}$$ $f$ is discontinuous at $x=0$ and $G(f)=\left\{ \left(0,0\right)\right\} \cup\left\{ (x,\frac{1}{x})\big|x\in\mathbb{R}\setminus\left\{ 0\right\} \right\} $ is closed because both of the sets are closed. BUT if we add that $f$ is bounded, then it can be proved that $f$ is continuous. I am having trouble in the proof. Here is my attempt: Attempt of a proof. Let $(x_n)_{n\in\Bbb N}$ be a real sequence that converges to some $x\in\Bbb R$ . We need to prove that $f(x_n)\xrightarrow{n\to \infty}f(x)$ .
We have that for all $n\in\Bbb N$ $(x_n,f(x_n))\in G(f)$ and that $((x_n,f(x_n))_{n\in\Bbb N}$ is bounded on $\Bbb R^2$ (since $(x_n)_{n\in\Bbb N}$ is convergent and $f$ is bounded). So from the Bolzano-Weierstrass theorem there exists a subsequence $(x_{k_n})_{n\in\Bbb N}$ of $(x_{n})_{n\in\Bbb N}$ so that $(x_{k_n},f(x_{k_n}))_{n\in\Bbb N}$ converges. Now, because $G(f)$ is a closed set $\exists x'\in\Bbb R :(x_{k_n},f(x_{k_n}))\to (x',f(x')) $ and so $x_{k_n}\to x'$ and $f(x_{k_n})\to f(x')$ . Moreover, $x_n\to x \ \ \& \ \ x_{k_n}\to x' \Longrightarrow x=x'$ and so $f(x_{k_n})\to f(x)$ . I cannot go any further than this.",['real-analysis']
2051160,Evaluating $\int_{ - \infty }^\infty {\frac{{{e^{7\pi x}}}}{{{{\left( {{e^{3\pi x}} + {e^{ - 3\pi x}}} \right)}^3}\left( {1 + {x^2}} \right)}}dx}$,"How to evaluate this integral? $$\int_{ - \infty }^\infty  {\frac{{{e^{7\pi x}}}}{{{{\left( {{e^{3\pi x}} + {e^{ - 3\pi x}}} \right)}^3}\left( {1 + {x^2}} \right)}}dx}$$ Maybe we can start by $$\int_0^\infty  {\frac{{dx}}{{({x^2} + 1)\cosh ax}}}  = \frac{1}{2}\left[ {{\psi _0}\left( {\frac{a}{{2\pi }} + \frac{3}{4}} \right) - {\psi _0}\left( {\frac{a}{{2\pi }} + \frac{1}{4}} \right)} \right]$$ in this . Then take the derivative with respect to $a$ , but I'm failed to solve it!","['real-analysis', 'calculus', 'complex-analysis', 'integration', 'sequences-and-series']"
2051195,what is the expected number of collisions?,"Suppose we use a hash function $H$ to hash $N$ distinct balls into $M$ distinct bins. Assuming simple uniform hashing, what is the expected number of collisions? Note that a collision is defined by adding a ball to an already occupied bin. If the already occupied bin has $k$ balls in it, then the number of collisions upon adding a new ball is $k.$ By using expectation, I tried as : => 1 × Probability of collision in first insertion  + 2 × Probability of collision in second insertion  + .......... + n × Probability of collision in nth insertion => $(1 ∗ 0) + (2 ∗ 1/m) + (3 ∗ 2/m) + (4 ∗ 3/m) + … + (n ∗ n−1/m)$ Actually, The answer is $(n^2 - n)/2m$ But, I am not getting the answer. Where am I wrong here ?","['combinatorics', 'probability-theory', 'probability', 'probability-distributions']"
2051199,Representing the continued fractions of non-square-root numbers ($\sqrt[a]{x}$ where $a>2$) NOT in simplest form,"The following question/topic is an extension of a question I asked a while ago: Click here In that question, I asked why the SIMPLE continued fraction of square roots are periodic and why this doesn't work for cubic roots, 4th roots, etc. I received many substantial answers which answered the question and as a result, I selected the best answer (I felt) from the 4 answers given. However, I feel like I only gave this question one approach: in SIMPLEST form... 
$$\sqrt[n]{x} = a_1+ \cfrac{1}{a_2 + \cfrac{1}{a_3 + \cfrac{1}{a_4 + \cfrac{1}{a_5 + \cfrac{1}{a_6 + \cfrac{1}{a_7 + \cdots}}}}}}$$ The above continued fraction is in simplest form (it is always $\dfrac{1}{a+\cdots}$) However, this is not the only way to represent continued fractions. I am going to use the following notation: $\sqrt{5} = 2+ \cfrac{1}{4 + \cfrac{1}{4 + \cfrac{1}{4 + \cfrac{1}{4 + \cdots}}}}$ $\sqrt5 = [2;\overline{4}]_1$ Where the subscript $_1$ represents the numerator of each term in the continued fraction Similarly:
$\sqrt7 = [2;\overline{1,1,1,4}]_1$ ...etc To showcase this ""notation""... and to show it does work for square roots, look at the following: Take the identity: 
$$\sqrt x = 1 + \frac{x-1}{1+\sqrt x }$$ Replacing $\sqrt x$ on the right hand side with $1 + \frac{x-1}{1+\sqrt x }$ gives us: $\sqrt{x} = 1+ \cfrac{x-1}{2 + \cfrac{x-1}{2 + \cfrac{x-1}{2 + \cfrac{x-1}{2 + \cdots}}}}$ $\sqrt{x} = [1;\overline{2}]_{x-1}$ Ok... enough with square roots does this work with other roots? I am not sure... but there must be a way to write cubic roots, etc. without having to use $1$ in the numerator of each term in the continued fraction... Now, $\pi$ isn't algebraic, but this is quite amazing: $$\pi = 3+ \cfrac{1^2}{6 + \cfrac{3^2}{6 + \cfrac{5^2}{6 + \cfrac{7^2}{6 + \cdots}}}}$$
$$\pi = [3;\overline{6}]_{(2n-1)^2}$$ Tl;Dr? Is there a way to represent continued fractions of non-square root numbers? Not in simplest form? Kind Regards Joshua Lochner","['number-theory', 'continued-fractions', 'irrational-numbers']"
2051237,Is $R^n$ projective as a $M_n(R)$-module?,"It is clear that $R^n$ is not free over $M_n(R)$. But is it projective?
I suspect that it should be projective because we can probably come up with a projective basis, but I'm not sure how to find the basis. Moreover, ideally if $R^n$ were projective over $M_n(R)$, we would get a big class of projective modules that are not free which would be interesting, I guess.","['abstract-algebra', 'modules', 'projective-module']"
2051241,"Showing $PSL(2,5)$ is isomorphic to $A_5$","Show $PSL(2,5)$ is isomorphic to $A_5$ The question I am trying to answer begins with us finding a subgroup of $SL(2,5)$ isomorphic $Q_8$, the quarternion group. We then deduce there that there is a subgroup of index 5 (given order of $SL(2,5)$ is 120) and so we can find a homomorphism from $SL(2,5)$ to $S_5$. The way I did this was considering the left coset action of $Sl(2,5)$ on $Sl(2,5) \ / Q_8$ i.e. define $ \theta_g (aH) = gaH $ and so the mapping $ \phi (g) = \theta_g $ is a homomorphism into $Sym( \{ \theta_g \}) \approx S_5 $. (Where H is my subgroup isomorphic to $Q_8$. So we now wish to show $ SL(2,5) \ / \{ \pm  I \} \approx A_5 $ My only idea to use the first isomorphism theorem, so I would like so somehow show every $ \theta_g $ is an even permutation, and also that the kernel of $ \phi $ is just $ \pm I $. That is, the only elements $g$ satisfying $ gaH=aH $ for all $a \in SL(2,5) $ are +- identity. However I am unsure of how to show both of these properties easily, or if they are even true to start with.
How can I finish this argument?","['finite-groups', 'group-theory']"
2051257,"$f(0)=0$ ; $f(x):=\int_0^x \cos \frac 1t \cos \frac 3t \cos \frac 5t \cos \frac 7t\,dt$, $\forall x \ne 0$. Is $f$ differentiable at $0$?","Let $f:\mathbb R \to \mathbb R$ be defined by $f(0)=0$ and
$$
f(x):=\int_0^x \cos \dfrac 1t \cos \dfrac 3t \cos \dfrac 5t \cos \dfrac 7t \,dt ,\quad\forall x \ne 0.
$$
Then is $f$ differentiable at $0$?","['derivatives', 'real-analysis', 'integration', 'riemann-integration']"
2051262,Issues with a proof: path-independence implies conservative vector field,"I'm a TA for calc 3, and one of my students asked me to help them understand a proof from lecture. The instructor was proving directly that if a vector field is path-independent, then it must be conservative (i.e. is a gradient of a potential function.) The proof (or sketch thereof) the student had in their notebook seemed to proceed as follows: Since the vector field $\mathbf{F}(x,y,z)=(M(x,y,z),N(x,y,z),P(x,y,z))$ is path-independent, we can construct a well-defined function, $$f(x,y,z) := \int_{(a,b,c)}^{(x,y,z)}\mathbf{F}\cdot d\mathbf{s},$$ where $(a,b,c)$ is some fixed point in the domain of $\mathbf{F}$, and the line-integral above should be interpreted as beginning at $(a,b,c)$ and ending at $(x,y,z)$. Our goal is to show that $\nabla f(x,y,z) = \mathbf{F}(x,y,z)$. Since the definition of $f$ is path-independent, we cleverly construct the following curves, joining $(a,b,c)$ to $(x,y,z)$:
$$\mathbf{c}_1(t) = (t,b,c), \quad \text{for }a\leq t \leq x$$
$$\mathbf{c}_2(t) = (x,t,c), \quad \text{for }b\leq t \leq y$$
$$\mathbf{c}_3(t) = (x,y,t), \quad \text{for }c\leq t \leq z$$ Since $\mathbf{c}_i'(t)= \mathbf{e}_i$, then $f$ takes the form:
$$f(x,y,z) = \int_{a}^{x}M(t,b,c)\,dt + \int_{b}^{y}N(x,t,c)\,dt + \int_{c}^{z}P(x,y,t)\,dt$$ It's at this point that the student's note start to breakdown, and I am having some trouble reconciling the rest of this proof. The issue I immediately see is that if you try and compute $\frac{\partial}{\partial x}f(x,y,z)$ directly, you get the following,
$$\partial_x f(x,y,z) = M(x,b,c) + \int_{b}^{y}\frac{\partial N}{\partial x}(x,t,c)\,dt + \int_{c}^{z}\frac{\partial P}{\partial x}(x,y,t)\,dt,$$
after applying the FTC to the first integral and differentiating under the integral on the other two. Ideally, we should have found that $\partial_x f(x,y,z) = M(x,y,z)$, but I do not see how this follows from the above computation. Similarly, we run into issues when we try computing $\partial_y f$. Computing $\partial_z f$, on the other hand, is straightforward since the only place $z$ occurs in $f(x,y,z)$ is on the last integral; therfore $\partial_z f = P$ directly by the FTC. What am I missing from this proof? Or was the instructor's idea wrong from the start, and should be prove the implication in some other way altogether?","['calculus', 'proof-verification', 'multivariable-calculus', 'integration', 'proof-explanation']"
2051273,Compute the value of Fresnel's Integral,"So, my teacher wants us to compute the value of the Fresnel integral: $$\int_0^\infty\cos(x^2)dx=\sqrt{\frac{\pi}{8}}$$ The problem is that we cannot use complex analysis to prove that and we should do that using the Euler identity: $$\int_0^\infty\cos(x^2)dx=\frac{1}{2}\int_0^\infty e^{iw^2}dw+\frac{1}{2}\int_0^\infty e^{-iw^2}dw$$ But I have that integral of $e^{-iw^2}$ and I cannot solve that :( I am an engineering student, so basically, I only have the ""basic"" calculus, just simple/double/triple integrals, some notions about series, ODE's and PDE's, but nothing as deep as in the regular Math degree, so probably there's no need to use hard stuff to figure this out.","['hyperbolic-functions', 'elementary-functions', 'fresnel-integrals', 'calculus', 'complex-analysis']"
2051278,Prove that $\sum\limits_{cyc}\frac{a}{\sqrt{a+3b}}\geq\sqrt{a+b+c+d}$,"Let $a$, $b$, $c$ and $d$ be positive numbers. Prove that:
$$\frac{a}{\sqrt{a+3b}}+\frac{b}{\sqrt{b+3c}}+\frac{c}{\sqrt{c+3d}}+\frac{d}{\sqrt{d+3a}}\geq\sqrt{a+b+c+d}$$
I tried Holder, AM-GM and more, but without success.","['multivariable-calculus', 'contest-math', 'inequality', 'radicals']"
2051289,What are roots of higher power like $n^9=512$?,"We know a value of $n$ in $n^2 = 4$ is $\pm2$ (two roots), then what about $n^9 = 512$, generic answer is $+2$, but it should have $9$ roots. What are those $8$ other roots and how can we find them ?","['algebra-precalculus', 'roots']"
2051327,How many solutions to $x_1 + x_2 + x_3 + x_4 + x_5 = 21$ given the following restrictions ...,"How many solutions to $x_1 + x_2 + x_3 + x_4 + x_5 = 21$ where $x_i$, $i = 1, 2, 3, 4, 5$ is a nonnegative integer such that $0 \le x_1 \le 3$, $1 \le x_2 \lt 4$ and $x_3 \ge 15$, My Approach My idea is to find the total number of solutions without restrictions and from that subtract the solutions with restrictions to arrive at the answer. Number of solutions without restrictions: $C(5-1 + 21, 21) = 12650$ Restriction 1: $x_3 \ge 15$
Number of solutions: $C(5-1+6,6)$ = 210 Restriction 2: $0 \le x_1 \le 3$
We can change the restriction to $x_1 \ge 4$ and subtract the number of solutions with this restriction from the total. $C(5-1 + 21, 21) - C(5-1 + 17, 17) = 6665$ Restriction 3: $1 \le x_2 \lt 4$
We can break this restriction down into two parts: when $x \ge 1$ and when $x \ge 5$. Then subtract case $2$ from case $1$. Case 1: $x \ge 1$.
$C(5-1 + 20, 20) = 10626$ Case 2: $x \ge 5$.
$C(5-1 + 21, 21) - C(5-1 + 16, 16) = 7805$. Case $1$ - Case $2$ is: $2851$. Now we can sum the restrictions and remove them from the total. $$12650 - (2851 + 210 + 6665) = 2924$$ However, the answer in the textbook is: $106$. Where is my reasoning incorrect? Thanks for your time! P.S. I know that there is this question however, I'm not looking for the answer per se. I am more interested in why my reasoning is flawed.","['combinatorics', 'discrete-mathematics']"
2051361,An interesting result in ratio and proportions,"If 
$$
\frac{a}{b}=\frac{c}{d}=k
$$
then 
$$
\frac{a+c}{b+d}=k
$$
Also
$$
\frac{a^2}{b^2}=\frac{c^2}{d^2}=k^2
$$
And
$$
\left( \frac{a+c}{b+d} \right)^2=k^2
$$
Also 
$$
\frac{a^2 + c ^2}{b^2+d^2}=k^2
$$
Hence
$$
\frac{a^2 + c ^2}{b^2+d^2}= \left( \frac{a+c}{b+d} \right)^2
$$
My question is... if the reverse is true. That is...
if 
$$
\frac{a^2 + c ^2}{b^2+d^2}= \left( \frac{a+c}{b+d} \right)^2
$$
Then can we assume that :
$$
\frac{a}{b}=\frac{c}{d}
$$
??","['algebra-precalculus', 'ratio']"
2051399,"Boolean Algebra, prime implicants, karnaugh map","This is the link to the question with my answer: https://i.sstatic.net/XAXU7.jpg I'm not sure if I identified the prime implicants and essential prime implicants correctly. I need to know if my answer is correct, that is there are: 6 prime implicants, and 5 essential prime implicants. Thanks in advance for the help!","['optimization', 'boolean-algebra', 'logic', 'binary-operations', 'discrete-mathematics']"
2051444,Show that the equation $\sqrt{ax+\alpha}+\sqrt{bx+\beta}+\sqrt{cx+\gamma}=0$ reduces to a simple equation.,I have got a question which looks like follow: Show that the equation $\sqrt{ax+\alpha}+\sqrt{bx+\beta}+\sqrt{cx+\gamma}=0$ reduces to a simple equation if $\sqrt{a}\pm\sqrt{b}\pm\sqrt{c}=0$. I am totally confused and don't even know from where should I start. Side note: I don't know what is a simple equation (I think it is something which is not filthy like given equation). Any hint/suggestion is heartily welcome. Thanks.,['algebra-precalculus']
2051472,Number of ways a laser will bounce on a circular reflective material in 4 times,"Is there a formula for getting the number of ways a laser will bounce on a circular reflective material in 4 times, returning to its original position? So the correct answer is 4, but is there a formula? Here's a picture showing the 4 ways:","['circles', 'geometry']"
2051480,Does this proof need the axiom of choice?,"I tried to prove this statement for a non-empty set $A$ $$A ~~\text{finite or countably infinite} ~\Leftrightarrow~ \exists\varphi : \mathbb{N} \rightarrow A ~~\text{surj}.$$ The $\Rightarrow$ is pretty straight forward and does not involve my question, so I will only go over what I came up with for the other direction. Proof (by contradiction)  We know that there exists a surjection $\varphi : \mathbb{N} \rightarrow A$ . So for every $a \in A$ the set $\varphi^{-1} (a)$ is non-empty. Since, by assumption, $A$ is not finite,  the axiom of choice is needed to choose one element out of every $\varphi^{-1} (a)$ . Collecting those and putting them into $N \subset \mathbb{N}$ , $\varphi$ induces a bijection $N \rightarrow A$ which contradicts $A$ not being countably infinite (or finite). Now I have 3 questions (aside from is the proof correct? ) Is the usage of the axiom of choice correct? Is the axiom needed to get one element out of every  $\varphi^{-1} (a)$ ? Is there a proof of this statment that does not rely on the axiom of choice?","['axiom-of-choice', 'alternative-proof', 'elementary-set-theory', 'proof-verification']"
2051508,Normal distribution can not be transformed into Laplace via additive transformation,"I am trying to find different methods of showing that there exists no random variable $V$ that can additively transform standard normal random variable into Laplace random variable. Formally,  we would like to show that there is no random
  variable $V$ independent of $N$(standard normal) such that 
  \begin{align} W=V+N, \end{align} where $W$ is zero mean and with
  Laplace parameter $b\ge 1$.  We want to show this is impossible for
  all $b\ge 1$. Proof 1 (Via Characteristic Functions): I have a proof via characteristic functions that goes as follows
\begin{align}
\phi_W(t)=\phi_V(t) \phi_N(t)   \rightarrow  \phi_V(t)=\frac{\phi_N(t)}{\phi_W(t)}
\end{align}
now we know that $\phi_N(t)=e^{-\frac{t^2}{2}}$ and $\phi_W(t)=\frac{1}{1+b^2 t^2}$, so
\begin{align}
\phi_V(t)= \frac{e^{\frac{t^2}{2}}}{1+b^2 t^2}
\end{align}
Clear this can not be a characteristic function since there are values of $t$ for which  $\phi_V(t) >1$. Proof 2 (Via analyticity after convolution) This proof was suggested by D.Thomine (see comments below).  Since convolution 'improves' regularity the pdf of $W$ must be analytic, however, the pdf of Lapalace distribution is not analytic at zero. My question: What would be some other methods of showing this? I was also thinking of the following approach. We know that 
\begin{align}
c_1 e^{-|w|/b} &=  E[ c_2 e^{-|X+w|^2/2} ] 
\end{align} Can we show that 
\begin{align}
E[ c_2 e^{-|X+w|^2/2} ] \le  c_3 e^{-w^2/2}
\end{align}
this would lead to a contraditiction since  $e^{-w^2/2}$ decays faster than $e^{-|w|/b}$. This question is related to something I asked here . Looking forward to your solutions. Thank you.","['characteristic-functions', 'probability-theory', 'probability-distributions']"
2051543,compute $\int_0^1 {{x^{k - 1}}{e^{ - x}}} dx$,compute $\int_0^1 {{x^{k - 1}}{e^{ - x}}} dx$ This what I did: By integration by part : $\int_0^1 {{x^{k - 1}}{e^{ - x}}} dx = \frac{{{e^{ - 1}}}}{k} + \frac{1}{k}\int_0^1 {{x^k}{e^{ - x}}} $ I'am stuck here Some help would be appreciated,"['real-analysis', 'integration', 'integration-by-parts']"
2051582,Find the power of operator,"I have an operator $U=x(t) + \int_0^1 x(st) \, ds$.
Goal is to find $U^2$ without iterated integrals. I start with:
$$U(Ux(t))=x(t) + \int_0^1 x(st)\,ds+\int_0^1 (x(qt) + \int_0^1 x(sqt)\,ds )\, dq= \\ =x(t) + 2\int_0^1 x(st) \, ds + \int_0^1 \int_0^1 x(sqt) \, ds\,dq.$$
I have a problem in transformation $\int_0^1 \int_0^1 x(sqt) \, ds\,dq$ . I have used integration by parts for the internal integral, but i do not know what to do next.","['functional-analysis', 'operator-theory']"
2051583,mixture distribution moment generating function,"Let $X$ and $Y$ be independent random variables, with known moment generating
functions $M_X(t)$ and $M_Y (t)$ and $I$ be such that $P(I = 1) =
1 − P(I = 0) = p \in (0, 1)$. Compute the moment generating function
of the random variable $S = IX + (1−I)Y $. I am given the hint that taking condition on I may help but still I have no idea how to compute. 
Any help would be appreciated.","['probability-theory', 'moment-generating-functions', 'probability-distributions']"
2051596,Does the order of a differential equation necessarily equal the number of arbitrary constants in the general solution?,"Consider the differential equation $(y')^2+3y'+2=0$.
The general solution seems to be $(y+x+c1)(y+2x+c2)=0$ with effective two arbitrary constants.But the order of the equation is 1. Can this discrepancy be avoided?Or is there some clause in the law relating order to the number of constants which i am missing?","['derivatives', 'integration', 'ordinary-differential-equations', 'calculus']"
2051605,"$\frac{1}{e}=$""Probability that every chocolate goes into a wrong spot"".","While watching a video by Po Shen Loh I found something strange. In the video, he said that: Suppose I have a box of chocolates having $100$ chocolates, and I drop them all on the ground, and then I try to put them all back in. What is the probability that every chocolate went back in a wrong spot? According to him, the probability is $\frac{1}{e}$ . Now the question is that how can we get that? To me, It is as interesting as Buffon's Needle problem, that is why I am eager to know the method to reach at $\frac{1}{e}$ . I shall be thankful if you guys can provide me idea about what is happening. Thanks","['probability', 'derangements']"
2051634,Constants in localizations of a differential ring,"Fix a commutative ring $A$, a derivation $\partial_A$ on $A$, and a multiplicative set $S$ in $A$. Then let $B = S^{-1}A$, $f : A \to B$ be the natural map, and $\partial_B$ be the natural extension of $\partial_A$ to $B$. We of course always have 
$$f(\ker \partial_A) \subseteq \ker \partial_B.$$
Broadly speaking, under what situations can we guarantee that this an equality? Here are some observations. If $A$ is a field, there's of course nothing to do. If $A$ is of positive characteristic $p$, there will likely be problems (even if $A$ is a domain). For example, if we take $A = \mathbb{F}_p[t]$ with $\partial_A$ being differentiation with respect to $t$, then $\ker \partial_A = \mathbb{F}_p[t^p]$. If we let $B = \mathrm{Frac}(A) = \mathbb{F}_p(t)$, then $\ker \partial_B = \mathbb{F}_p(t^p)$, which is of course strictly larger. If $A$ is not a domain, there will likely be problems (even in characteristic 0). For example, let $A$ be the ring of smooth functions $\mathbb{R} \to \mathbb{R}$ with $\partial_A$ being usual differentiation. Then $\ker \partial_A = \mathbb{R}$. Moreover, we know that there exist $f, g \in A$ such that the Wronskian $W(f,g) = fg' - f'g = 0$ despite the fact that $f$ and $g$ are $\mathbb{R}$-linearly independent. (An explicit example of such a pair can be found in Bôcher's note from 1900 ""On linear dependence of functions of one variable."") So, if we let $S$ be the multiplicative subset generated by $g$, note that $$\partial_B(f/g) = -W(f,g)/g^2 = 0,$$ so $f/g \in \ker \partial_B$ but it is not in $f(\ker \partial_A) = \mathbb{R}$ since $f$ and $g$ are $\mathbb{R}$-linearly independent. The subring of constants in a differential field is itself necessarily a field. So, if $A$ is a domain and $B = \mathrm{Frac}(A)$, we know that $\ker \partial_B$ must be a field, so there will be problems if $\ker \partial_A$ is not already a field. For example, let $A = \mathbb{Q}[x,y]$ and $\partial_A$ is differentiation with respect to $y$, so that $\ker \partial_A = \mathbb{Q}[x]$. Taking $B = \mathrm{Frac}(A) = \mathbb{Q}(x,y)$, we get $\ker \partial_B = \mathbb{Q}(x)$. So... putting together these observations... Do we have equality when $A$ is a domain such that $\ker \partial_A$ is a field of characteristic 0? (When $A$ is a domain, we may as well assume that $B = \mathrm{Frac}(A)$.) If the above conditions aren't sufficient, are there any further (non-tautological) conditions we can impose on $A$ and $\partial_A$ to guarantee equality? In addition to these specific questions, I'd love to hear anything people have to say about the broad question above also (whether that be in the form of general statements or fun counterexamples).","['abstract-algebra', 'differential-algebra', 'commutative-algebra']"
2051659,Prove that $\rho(I - M^{-1}A)<1$,"Let we have real square matrices $M$ and $A$ such that $A = A^T$ and $(Mx, x) > \frac{1}{2} (Ax, x) > 0, \forall x \ne 0 $. Prove that $\rho(I - M^{-1}A)<1$ where $\rho(A)$ is spectral radius. It should be somehow connected with Neumann series. I have no proof, just some details. For example $\rho(I - M^{-1}A)<1$ equivalent to $\rho(\frac{1}{2}M^{-1}A) < 1$. And if $\rho(\frac{1}{2}M^{-1}A) < 1$ than series $\sum\limits_{n=0}^{\infty}(\frac{1}{2}M^{-1}A)^n$ converges and equals $(I - \frac{1}{2}M^{-1}A)^{-1} = M(M - \frac{1}{2}A)^{-1}$, where $M$ and $M - \frac{1}{2}A$ are positive definite by the statement. Thanks for any help or ideas!","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2051666,Likelihood Function is a random variable,"I am taking my first statistics course and we are talking about finding a good estimate to unknown parameter $\theta$ given a sample $X_{1},...X_{n}\sim F_{\theta}$, and we talked about the likelihood function $L(\theta)$ and he claimed the maximum of likelihood function $L(\theta)$ is a random variable and the lecturer gave the example of two consecutive coin tosses. So we have $(T,T),(T,H),(H,T),(H,H)$. Assuming the probability of getting $T$ is an unknown parameter $p$. Then we will have to maximize $p^2,p(1-p),(1-p)^2$ respectively to find which $p$ is more likely to give us the  given sample space  and as a result we had $\hat{p}=\begin{cases}
1 & \left(T,T\right)\\
\frac{1}{2} & (H,T)\,or\,(T,H)\\
0 & \left(H,H\right)
\end{cases}$ But I didn't really understand how to see this as random variable. I mean if I look at $P(\hat{p}=1)=p^2$, it doesn't really makes sense since p is an unknow parameter. Now we are talking about given two estimations (two random variables), which gives a better estimation (Mean Squarred Error etc.) and we are usng the fact that the estimations are random variables, but since I don't get why it's a random variable it creates a problem for my understanding. Can someone explain why this is a random variable, or if I am misunderstanding something? Thanks","['maximum-likelihood', 'statistics']"
2051691,"What is wrong with the ""proof"" for $\ln(2) =\frac{1}{2}\ln(2)$?","I have got a question which is as follows: Is  $\ln(2)=\frac{1}{2}\ln(2)$?? The following argument seems suggesting that the answer is yes: We have the series $1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots$
  which has a mathematically determined value $\ln(2)=0.693$. Now, let's do some rearrangement: $$1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\frac{1}{6}+\frac{1}{7}-\frac{1}{8}+\frac{1}{9}-\frac{1}{10}+\frac{1}{11}-\frac{1}{12}......$$
   $$
 (1-\frac{1}{2})-\frac{1}{4}+(\frac{1}{3}-\frac{1}{6})-\frac{1}{8}+(\frac{1}{5}-\frac{1}{10})-\frac{1}{12}.......$$
  $$\frac{1}{2}-\frac{1}{4}+\frac{1}{6}-\frac{1}{8}+\frac{1}{10}-\frac{1}{12}......$$
   $$\frac{1}{2}(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\frac{1}{6}+\frac{1}{7}-\frac{1}{8}+\frac{1}{9}-\frac{1}{10}+\frac{1}{11}-\frac{1}{12}......)$$ $$\frac{1}{2}\ln(2).$$ I know that mathematics can't be wrong, and I have done something wrong. But here is my question : where does the argument above go wrong?","['real-analysis', 'fake-proofs', 'sequences-and-series']"
2051692,"Index subsets of a finite set by an integer, with larger indexes matching larger subsets?","Let $S$ be a finite set of $n$ elements. There are $2^n$ subsets of $S$. I want to index them by an integer in some simple way. That is, I want to find a mapping $I:\mathbb{N}\rightarrow \mathbb{P}(S)$, whose domain is the integers from 0 to $2^n - 1$, and whose image are all the subsets of $S$. The notation $\mathbb{P}(S)$ stands for the power set of $S$. One simple way to do it is to use binary notation. The drawback is that very large indices (like 10000....00) may refer to subsets with very few elements. I want instead that the subsets are ordered according to their cardinality. Also, the indexing function $I$ should be easy to compute and invert. I haven't gotten very far, but here's what I've noticed. There are $\binom{n}{j}$ subsets with $j$ elements. If $i$ is an index and $$\sum_{j=0}^k \binom{n}{j} \le i < \sum_{j=0}^{k+1} \binom{n}{j}$$ we could arrange things so that $i$ index subsets with $k$ elements. But I'm stuck here, because there is no closed formula for a partial sum of binomials.","['binomial-coefficients', 'discrete-mathematics']"
2051702,What is the point in taking Monte Carlo realizations of a data set?,"Let's say I have a dataset that is 100 elements long, $X = \{x_0...x_{100}\}$ and I do 1,000 Monte Carlo realizations of the data, $X_j, 0\leq j \leq1000$, sampling 10 points each time. If I then compute the variance (or any other estimator) on those ten points, and do that for every realization, I can then use that to tell me about the variance of the whole dataset. $Var(X) \approx \dfrac{\sum_{j=0}^{1000}Var(X_j)}{1000}$ So, why would I do this? If I take those 1,000 variances, and average them, shouldn't it just converge to the variance of the whole set, if I were to measure it once? Is the only reason to do it this way to be able to measure the error on the computed variance?","['monte-carlo', 'statistics', 'error-propagation', 'random']"
2051710,Convergence of directed family in topological space,"Let $X$ and $J\neq\emptyset$ be sets s.t for every $j\in J$ $(Z_j,\tau _j)$ is a topological space. Let $\tau $ be weakest topology on $X$ s.t for every $j\in J$, $f_j : X\to Z_j$ is continous. Let $x\in X$ and $(x_\alpha)_{\alpha\in\mathcal
A}$ Show that if for every $j\in J$, $f_j(x_\alpha)\to f_j(x)$, then $x_\alpha\to x$ By convergence, we mean $x_\alpha\to x\iff \forall U_x, \exists\alpha _0 : \forall\alpha (\alpha _0\preceq\alpha\Longrightarrow x_\alpha\in U_x)$, where $U_x$ is a neighborhood of $x$. Initial thought: Assume for a contradiction, for some $U_x\in\tau $, for every $\alpha$, exists $\alpha _0$ s.t $\alpha\preceq\alpha _0$ and $x_{\alpha _0}\notin U_x$. Due to continuity of $f_j,j\in J$ we have for every $V_{f_j(x)}\in\tau _j$ exists $\alpha _0$:
$$\forall\alpha (\alpha _0\preceq\alpha \Longrightarrow f_j(x_\alpha)\in V_{f_j(x)}) $$
Therefore $x_\alpha\in f_j^{-1}(V_{f_j(x)})\in\tau $, trouble is that potentially any preimage of $V_{f_j(x)}$ for any $j\in J$ might ""miss"" the fixed $U_x$ (aside from $x$ itself, of course, but isn't contained in $U_x$).  An idea
$$\bigcup_{j\in J}\bigcup_{V\in\mathcal{V}_{f_j(x)}}f_j^{-1}(V)\overset{?}=\bigcup_{U\in\mathcal{U}_x}U $$
Where $\mathcal{U}_x$ is the collection of all $\tau$ open neighborhoods $x$ and analogous description for $\mathcal{V}_{f_j(x)}$. It's obvious that $\subset$, but it's not so obvious that $\supset$ and it might not even be true. What to do? I'm probably over-thinking this problem waaay out of proportion.","['functional-analysis', 'general-topology', 'nets']"
2051726,Onto functions from Power set of Naturals,"Let's define that a function (from $\mathcal{P}(\mathbb{N})$ to $\mathcal{P}(\mathbb{N})$, $\mathbb{N}$ meaning the set of natural numbers throughout this proof) is ""OK"" if: $$\forall X,Y \in \mathcal{P}(\mathbb{N}). X \subset Y \leftrightarrow f(X) \subset f(Y)$$ while $X$ is a proper subset of $Y$, they are not equal (so as $f(X)$ and $f(Y)$). Does there exist an ""OK"" and onto function $f: \mathcal{P}(\mathbb{N}) \setminus \{\mathbb{N}\} \rightarrow \mathcal{P}(\mathbb{N})$? I tried disproving this claim, eventually getting stuck. I started with the assumption that $\mathbb{N}$ is in $Im(f)$, so there must be a subset of $\mathbb{N}$ (let's say $T$) so that $f(T) = \mathbb{N}$. $T$ is not $\mathbb{N}$ (because $\mathbb{N}$ is not in the domain), so there must be a natural number that is not inside $T$. and here I am stuck. Any suggestions? Thanks!","['elementary-set-theory', 'discrete-mathematics']"
2051825,Geometric reason for PCA eigen vectors being transposed,"I was trying to think of the reason behind the PCA eigen vectors being transposed, that is, why $y= U_D^T x$. Geometrically, is it because since we are taking the projection of the point $x$ into a reduced dimension hyperplane, we are essentailly saying that the component of x which is already in the required hyperplane need not be changed in any way, but the ones which are lying outside the hyperplane at a certain angle will need to have their dot products taken as part of their being projected on to the plane? What would be the interpretation in terms of maximizing variance?","['statistics', 'data-analysis', 'linear-algebra']"
2051839,Understanding a geometric argument in the proof of the strong maximum principle for elliptic operators in Evans's PDE,"Here is the strong maximum principle in Evans's Partial Differential Equations : Here $U\subset\mathbb{R}^n$ is open and bounded. Also, The proof is very short once one has Hopf's lemma. Here is my question : Would anyone elaborate the underlined sentence that why such $y$ exists? (I don't have any intuition at all why this should be true.)","['multivariable-calculus', 'real-analysis', 'metric-spaces', 'partial-differential-equations']"
2051874,Function that returns 1 given any positive number,"What would be a way to write a function that given any positive number the function would return one, but for a negative number or zero it would return zero. Also what mental process should I go through to approach such a problem? For example: f(45) = 1; f(10) = 1; f(1) = 1; f(0) = 0; f(-3) = 0;",['functions']
2051980,Holomorphic 1-forms on a smooth affine plane curve,"I'm looking at Miranda's Algebraic Curves and Riemann Surfaces. In particular, Chapter IV.I, question C : Let $X$ be a smooth affine plane curve defined by $f(u,v) =0$. Show that $du$ and $dv$ define holomorphic 1-forms on $X$, as do $p(u,v)du$ and $p(u,v)dv$ for any polynomial $p(u,v)$. Show that if $r(u,v)$ is any rational function, then $r(u,v)du$ and $r(u,v)dv$ are meromorphic 1-forms on $X$. Show that $(\partial f/\partial u)du = -(\partial f/\partial v)dv$ as holomorphic 1-forms on $X$. I understand what a holomorphic 1-form is, but I'm not sure how I'm supposed to show that these particular examples are 1-forms on $X$. The reason for this mainly is that I'm unsure of when something fails to be a holomorphic 1-form.","['complex-analysis', 'projective-space', 'plane-curves', 'algebraic-geometry']"
2051998,On the fractional derivatives of the Riemann zeta function and the derivatives of the derivatives,"It's been a while since the last fractional-calculus question, so here's my question for all of you. It can be found from the Riemann-Liouville definition of the fractional derivative that whenever $\Re(s)>1$, $$\begin{align}I_x^\alpha\zeta(x)&={\frac  {1}{\Gamma (\alpha )}}\int _{a}^{x}\zeta(t)(x-t)^{\alpha -1}\ dt\\&=\frac1{\Gamma(\alpha)}\int_a^x\sum_{n=1}^\infty\frac{(x-t)^{\alpha-1}}{n^t}\ dt\\&=\frac1{\Gamma(\alpha)}\sum_{n=1}^\infty\int_a^x\frac{(x-t)^{\alpha-1}}{n^t}\ dt\\&=\sum_{n=1}^\infty\frac{(-\ln n)^{-\alpha}}{n^x}\end{align}$$ Thanks to WolframAlpha for that last step.  I imagine the interchange between integral and sum can be made with rigor, but I can't see how at the moment.  Setting this into it's derivative form, I end up with $$D_x^\alpha\zeta(x)=\sum_{n=1}^\infty\frac{(-\ln n)^\alpha}{n^x}$$ which shall be my fractional derivative of the Riemann zeta function. I then wish to take the following: (is differentiation with respect to the fractional derivative nonsensical?) $$D_\alpha^\beta D_x^\alpha\zeta(x)=D_\alpha^\beta\sum_{n=1}^\infty\frac{(-\ln n)^\alpha}{n^x}$$ Again, using the Riemann-Liouville definition, I end up with $$D_\alpha^\beta\sum_{n=1}^\infty\frac{(-\ln n)^\alpha}{n^x}=\sum_{n=1}^\infty\frac{(\ln(-\ln n))^\beta(-\ln n)^\alpha}{n^x}$$ Which is a little weird since the summand is undefined at $n=1$.  What should I do about this? Also, my end goal is to get some crazy derivative of derivatives of the zeta function into $$\sum_{n=1?}^\infty\frac{\prod_{k=0}^p(\ \overbrace{\ln\ln\dots\ln\ln}^k\ n\ )^{a_k}}{n^x}$$ Any ideas? Simple idea: Rewriting the zeta function as $$\zeta(x)=1+\sum_{n=2}^\infty\frac1{n^x}$$ now removes the problem, and with linearity, should give us $$D_x^\alpha\zeta(x)=\frac1{\Gamma(1-\alpha)x^\alpha}+\sum_{n=2}^\infty\frac{(-\ln n)^\alpha}{n^x}$$ $$D_\alpha^\beta D_x^\alpha\zeta(x)=\left(D_\alpha^\beta\frac1{\Gamma(1-\alpha)x^\alpha}\right)+\sum_{n=2}^\infty\frac{(\ln(-\ln n))^\beta(-\ln n)^\alpha}{n^x}$$ Though this feels a tad bit unsafe.","['fractional-calculus', 'riemann-zeta', 'sequences-and-series']"
2052001,Solve quadratic congruence $x^2\equiv 51x+ 43 \pmod {\!187 =11\cdot 17 }$,"Given $x^2 - \overline{51}x - \overline{43} = \overline{0}$. Solve it in $\mathbb{Z}/187\mathbb{Z}$. First of all, does the $\overline{x}$ mean that $\overline x = \{x + z \ | z\in I\}$? I am getting really confused, because 187 is composite. I think I should somehow decompose this equation into two equations and solve them in $\mathbb{Z}/11\mathbb{Z}$ and $\mathbb{Z}/17\mathbb{Z}$, but I'm not sure. And what is the pattern to solve things like that?","['number-theory', 'modular-arithmetic']"
2052013,"""Functions"" that always equal zero and their gradients","I am currently enrolled in a university level Multivariable Calculus course in which my professor taught finding the tangent plane at a point on a surface the following way: To find the tangent plane at point $(x_0,y_0,z_0)$ on the surface $ z=x^2+y^2 $ set $f(x,y,z)=x^2+y^2-z=0$ and then find $\nabla f(x_0,y_0,z_0) $. According to his method, $\nabla f $ would be the following...
$$
\nabla f =  \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \\ \frac{\partial f}{\partial z} \end{bmatrix} = \begin{bmatrix}  2x \\ 2y \\ -1 \end{bmatrix}  
$$
While this indeed is an expression for the tangent plane's normal vector, I don't understand how this is a mathematically valid statement. If $f(x,y,z)=0$, than how can the rates of instantaneous change (i.e.$\frac{\partial f}{\partial x}$) ever be anything other than zero? Is $f(x,y,z)=x^2+y^2-z=0$ a valid function? Why would the partials not always be equal to zero? Am I thinking about partial derivatives incorrectly? Is this just a mathematical ""hack"" to serve as a shortcut? If this is the case, is there a more mathematically rigorous method to find the tangent plane of a surface? Thank you in advance.","['multivariable-calculus', 'partial-derivative', 'functions']"
2052057,Prove that the product of $\dfrac{a_k-a_l}{k-l}$ is an integer,"Let $a_1,a_2,\ldots,a_n$ be $n$ distinct integers. Show that the product of all the fractions of the form $\dfrac{a_k-a_l}{k-l}$, where $n \geq k > l$, is an integer. I thought about first determining how many differences $a_k-a_l$ are divisible by $b$. Assume that $n_0$ of the integers $a_1,a_2,\ldots,a_n$ are divisible by $b$, that $n_1$ of them yield the remainder $1$ upon division by $b$, that $n_2$ of them yield the remainder $2$, and so on up to $n_{b-1}$ integers the yield a remainder of $b-1$ when divided by $n$. It then follows that $$n_0+n_1+n_2+\cdots+n_{b-1} = n.$$ The difference $a_k-a_l$ is divisible by $b$ if and only if $a_k \equiv a_l \pmod{b}$. Now note that the number of differences $a_k-a_l$ divisible by $b$ such that $a_k \equiv a_l \equiv r \pmod{b}$ is $C^2_{n_r} = \dfrac{n_r(n_r-1)}{2}$. It follows that the number of differences divisible by $b$ is exactly \begin{align*}N &= \dfrac{n_0(n_0-1)}{2}+\dfrac{n_1(n_1-1)}{2}+\cdots+\dfrac{n_{b-1}(n_{b-1}-1)}{2}\\&= \dfrac{n_0^2+n_1^2+n_2^2+\cdots+n_{b-1}^2}{2}-\dfrac{n_0+n_1+n_2+\cdots+n_{b-1}}{2}\\&= \dfrac{n_0^2+n_1^2+n_2^2+\cdots+n_{b-1}^2}{2}-\dfrac{n}{2}.\end{align*}",['number-theory']
2052061,Probability that partition problem has a solution for sets of random integers,"The Birthday Problem wikipedia page has a section devoted to the problem of finding the minimum $N$ such that the probability $p_N$ that $N$ numbers randomly chosen with replacement from $\{1,2,...,1000000\}$ can be partitioned into two subsets whose sums differ by at most $1$ satisfies $p_N>\frac{1}{2}$. The answer given is $N=23$, and the explanation is that there are $2^{N-1}$ partitions and the distribution of the sum of the weights is Gaussian with width approximately $1000000\sqrt{N}$. It is then claimed that the critical value of $N$ is that for which $1000000\sqrt{N}\approx2^{N-1}$ which gives $N=23$ as the closest value. I do not follow this argument; in particular, I cannot see why the width approximation of $1000000\sqrt{N}$ is used or why $p_N$ will be greater than $\frac{1}{2}$ when the number of partitions is larger than the width. The page references this paper , which investigates the behaviour of the problem as $N\rightarrow\infty$ and the number of weights to choose from is increased proportional to $N$. Looking through the paper, I did not find any result which seemed to answer our problem, as most results were for asymptotically large $N$, and I couldn't find anything which seemed to justify wikipedia's working. Thus I would like the answer to the following problem: let $N$ weights be chosen with integer values randomly drawn from $\{1,2,...,M\}$ (I'd be happy to see results with or without replacement) and let $p(N,M)$ be the probability of existence of an equal-weight (with or without allowance for a discrepancy of 1 unit) two-subset partition of the weights. (Without allowance for discrepancy, $p(N,M)$ is the probability that the partition problem has a solution). Then what is $N_{min}(M)=\min{\left\{N\;|\;p(N,M)>\frac{1}{2}\right\}}$? I have attempted in the past to solve this by trying to find an exact expression for $p(N,M)$, by looking for expressions for the number of possible partitions with equal weights using combinations but I have never had success, and I don't know if this is an appropriate way to go about it. I have searched for any similar question on math.SE also but all of the results seem to be more concerned with complexity and other computational considerations relating to the partition problem. My question is: can anyone find a way of calculating $N_{min}(M)$ or even $p(N,M)$ or present a good approximation, or possibly elucidate wikipedia's calculation?","['combinatorics', 'probability', 'optimization']"
2052081,Proving countable set using a function that is one-to-one,"My problem reads: Prove that if there is a function $f\colon A\rightarrow \mathbb{N}$ that is one-to-one, then $A$ is countable Assuming $\mathbb{N}$ to be the set of natural numbers. I am not too sure how to go about proving this. Would I need the definition of denumerable in this case? or can I use the cardinality of A < or equal to N?","['proof-writing', 'elementary-set-theory']"
2052082,"$f(x)=x^\alpha$ uniformly continuous on [$0,\infty)$? using MVT","in spivak's calculus, there is a question asking for which of the following values of $\alpha$ is the function $f(x)=x^\alpha$ uniformly continuous on [$0,\infty)$? : $\alpha = 1/3, 1/2, 2, 3$ I know that $x^\alpha$ is uniformly continuous on [$0,\infty)$ for $\alpha = 1/3, 1/2$ but not for $\alpha = 2,3$ but this is not my interest. it was brought up in lecture that a proof of the result for general $\alpha \in [0,\infty)$ will be much easier after we learned the mean value theorem and now that i know what MVT is, how does one prove this with using MVT? All I know is that $x^\alpha$ is uniformly continuous on [$0,\infty)$ for $0 \le \alpha \le 1$ and not for $\alpha > 1$, and the definition of MVT is, $f$ is continuous on [$a,b$] and diff on ($a,b$) then there exists $x \in (a,b)$ such that $f'(x) = \frac{f(b)-f(a)}{b-a}$","['derivatives', 'real-analysis', 'continuity', 'uniform-continuity']"
2052095,Suppose that $p(z) = a_nz^n+\cdots+a_0$ and it has maximum modulus $1$ on the boundary of the unit disk.,"Suppose that $p(z) = a_nz^n+\cdots+a_0$ and it has maximum modulus $1$ on the boundary of the unit disk, show that $|p(z)| \leqslant max\{1,|z|^{n}\}$. How to show that $|p(z)| \leqslant |z|^n$?",['complex-analysis']
2052135,Folland Theorem 2.36: Why do we use Monotone Class Theorem?,"I am studying out of Folland and I think I understand the proof of Theorem 2.36, but I am not sure why we can't just look at disjoint unions of rectangles. Here is the Theorem: Let $(X,\mathcal{M}, \mu)$, $(Y,\mathcal{N}, v)$ be $\sigma$-finite measure spaces. Then for any $E\in\mathcal{M}\bigotimes\mathcal{N}$, $x\to v(E_x)$ and $y\to \mu(E^y)$ are measurable (on $X$ and $Y$ respectively) and $$\mu\times v (E)=\int \mu(E^y)\ dv=\int v(E_x)\ dx.$$ It is obvious for rectangles ($\chi_{E_x}=\chi_{A}v(B)$ if $E=A\times B$)  and for finite disjoint unions of rectangles, so why can we take an countable disjoint collection of rectangles (Theorem 2.15)? Wouldn't this just be taking the limit of a finite disjoint union of rectangles? If so, then the claim follows. So why use the Monotone Class Lemma? I know I have to be missing something.","['real-analysis', 'measure-theory', 'product-measure']"
2052179,How to find $\sum_{i=1}^n\left\lfloor i\sqrt{2}\right\rfloor$ A001951 A Beatty sequence: a(n) = floor(n*sqrt(2)).,"A001951 A Beatty sequence: a(n) = floor(n*sqrt(2)). If $n = 5$ then $$\left\lfloor1\sqrt{2}\right\rfloor+ \left\lfloor2\sqrt{2}\right\rfloor + \left\lfloor3\sqrt{2}\right\rfloor +\left\lfloor4 \sqrt{2}\right\rfloor+ \left\lfloor5\sqrt{2}\right\rfloor
= 1+2+4+5+7 = 19$$ Sequence from $1$ to $20$ is: $S=\{1,2,4,5,7,8,9,11,12,14,15,16,18,19,21,22,24,25,26,28\}$ I want to find answer for $n = 10^{100}$ .","['radicals', 'diophantine-approximation', 'summation', 'sequences-and-series', 'ceiling-and-floor-functions']"
2052182,Is there a probability distribution with a CDF shaped like the sinh or logit function?,"I am looking for a probability distribution with the shape of the CDF similar to the sinh or logit function. I need it to be centered at 0 and symmetric, meet the usual CDF restrictions. I understand that most likely it will need to be defined on a specific interval, that's okay. The important property is that F'(x) < 1, F''(x) < 0 close to the left of the mean, and F'(x)<1 , F''(x)>0 close to the right. For more farther away values, however defined, I need F'(x) > 1, F''(x) > 0. Needless to say, the easier the formula, the better!
I tried to play around with the sinh function but I couldn't get it to work. Any ideas? Here is an example of the cdf I am looking for:","['statistics', 'probability-distributions']"
2052220,Evaluate $\displaystyle \int_C \frac{1}{4z^2+4z-3}$ where $C=C^{+}_{1}(0)$.,"Evaluate $\displaystyle \int_C \frac{1}{4z^2+4z-3}$ where $C=C^{+}_{1}(0)$. I am not sure whether I understand how to use Cauchy's theorem which states that: If $f$ is a analytic in a simply connected domain $D$, and $C$ is a simple closed contour lying in $D$, then $\displaystyle \int_C f(z)dz=0$. (From this also follows the deformation of contours). Anyways, what I have done so far is in decomposing $f(z)=\frac{1}{4z^2+4z-3}=\frac{1}{4}\left(\frac{1}{2z-1}-\frac{1}{2z+3}\right)$. Now, I look at the singularities of $f$ which are $z=\frac{1}{2},-\frac{3}{2}$. My contour is the circle (with positive orientation) centered at $0$ with radius $1$ (so the integral part of $\frac{1}{4}\frac{1}{2z+3}$ just goes to $0$ by Cauchy). I then make a contour $C_{1/2}$ around the singularity $\frac{1}{2}$ so I can apply the deformation theorem. I'm then left with $\frac{1}{4}{2\pi i}=\frac{\pi}{2}i$. But apparently the answer is $\frac{\pi}{4}i$. Where did I go wrong here? Thanks.","['complex-analysis', 'integration', 'complex-numbers']"
2052226,"Is $f(x,y)=(xy)^{2/3}$ differentiable at $(0,0)$?","I looked at the graph of $f(x,y)=(xy)^{2/3}$ at the origin, and no matter how far I zoom in it has four corners along $x=\pm y$, so it doesn't look like it is differentiable as $z=0$ doesn't look like a tangent plane. I also found that its partial derivatives are not continuous at $(0,0)$, e.g. 
$$
\lim_{(ky^2,y) \to (0,0)} \frac{\partial f}{\partial x} = \lim_{(ky^2,y) \to (0,0)} \frac{2}{3}x^{-\frac{1}{3}}y^{\frac{2}{3}} = \frac{2}{3}k^{-\frac{1}{3}}
$$
(although $\frac{\partial f}{\partial x}(0,0) = \frac{\partial f}{\partial y}(0,0) = 0$) I also tried to prove/disprove its differentiability by definition, and for it to be differentiable I need
$$
\lim_{(x,y) \to (0,0)} \frac{f(x,y)-0}{\sqrt{x^2+y^2}} =0
$$
but then I got stuck. Am I even on the right track? [UPDATE] I need to do the same thing for $f(x,y)=(xy)^{\frac{1}{3}}$... I think the answer is that $(xy)^{2/3}$ is differentiable but $(xy)^{1/3}$ is not, according to my professor but I'm not sure","['derivatives', 'real-analysis', 'limits', 'calculus', 'multivariable-calculus']"
2052233,How does one get that $1^3+2^3+3^3+4^3+\cdots=\frac{1}{120}$? [duplicate],"This question already has answers here : Ramanujan Summation (3 answers) Closed 7 years ago . While watching interesting mathematics videos, I found one of the papers  of Srinivasa Ramanujan to G.H.Hardy in which he had written $1^3+2^3+3^3+4^3+\cdots=\frac{1}{120}$. The problem is that every term on the left is more than $\frac{1}{120}$ yet the sum is $\frac{1}{120}$. How is that ??? I know that there are many and much more interesting things  presented by Ramanujan (like $1-1+1-1+1...=\frac{1}{2}$ and $1+2+3+4.....=\frac{-1}{12}$) but for now I am interested in the summation in the title.
Any idea/hint is heartily welcome. Thanks. Here is the video I'm talking about.","['math-history', 'summation-method', 'ramanujan-summation', 'number-theory', 'summation']"
2052250,The Most general solution satisfying equations $\tan x=-1$ and $\cos x=1/\sqrt{2}$,"The most general value of $x$ satisfying the equations $\tan x=-1$ and $\cos x=1/\sqrt{2}$, is found to be $x=2n\pi+\frac{7\pi}{4}$. My approach: $$
\frac{\sin x}{\cos x}=-1\implies \sqrt{2}\sin x=-1\implies\sin x=\frac{-1}{\sqrt{2}}=\sin (\frac{\pi}{4}+\frac{\pi}{2})=\sin\frac{3\pi}{4}\\\implies x=n\pi+(-1)^n\frac{3\pi}{4}
$$
If I consider the cosine function
$$
\cos x=\frac{\sin x}{\tan x}=-\sin x=-\sin\frac{3\pi}{4}=\cos(\frac{3\pi}{4}+\frac{\pi}{2})=\cos\frac{5\pi}{4} \implies x=2n\pi+\frac{5\pi}{4}
$$
Is their anything wrong with my approach ?How do I compare different forms of general solutions without inputting for $n$ ?",['trigonometry']
2052286,Proving that $\lim\limits_{h \to 0}\frac{|x+h|-|x|}{h}$ does not exist at $x=0$,"In our exam, we are tasked to prove that $U(0)$ does not exist where, $$U(x) = \lim\limits_{h \to 0}\frac{|x+h|-|x|}{h}$$ What I did was to simplify the function first, so I initially performed the limit operation. That function above is essentially the derivative of $|x|$, so I got, $$U(x) = \frac{x}{|x|}$$ And after doing $U(0)$, it became $$U(0) = undefined$$ This completed my proof. My teacher marked it wrong. What is the incorrect step in what I did and what could be the right proof?","['derivatives', 'limits-without-lhopital', 'calculus', 'limits']"
2052318,Locus of centre of circle passing through concyclic points,Two rods of length $a$ and $b$ slide along the coordinate axes in a manner that their ends are always concyclic. Find the locus of the centre of the circle passing through these ends. Apart from figuring out a probable geometry given in the problem... I havent been able to go any further in the problem. Any clues or hints will be quite helpful... Thanks in advance!!... The answer given in the key is $4(x^2-y^2)=a^2-b^2$.,"['circles', 'locus', 'analytic-geometry', 'euclidean-geometry', 'geometry']"
2052332,Increasing sequence of closed subspaces of $L^2$ and error estimate of a product of orthogonal projections,"We define  an increasing sequence of closed subspaces
\begin{align*}
V_{0} \subset V_{1} \subset V_{\ell} \subset \dots
\end{align*}
of $L^2(I)$ where $I=(0,x_{max})$, and each $V_{\ell}$ is equipped with a $L^{2}$ basis $ \{\phi^{\ell}_i\}_{i=1}^{m_{\ell}} $ ( $\phi^{\ell}_i$ are piece-wise polynomials of order $p$). I define two types of orthogonal projections \begin{align*}
 P_L : & L^2(I) \rightarrow V_L\\
&f \rightarrow P_L f
\end{align*}
and  a nested projection for  $\ell\leq L$ \begin{align*}
 P_{\ell,\ell-1} : & V_{\ell}\rightarrow V_{\ell-1}\\
& f^{\ell}  \rightarrow P_{\ell,\ell-1} f^{\ell}=\tilde{f}^{\ell-1}
\end{align*} I have a problem where I am interested on deriving an upper bound estimate for \begin{align}
\mathrel{\Big|} \mathrel{\Big|} \tilde{f}^{\ell}- \tilde{f}^{\ell-1} \mathrel{\Big|} \mathrel{\Big|}_{L^2(I)}^2.
\end{align} A tentative way I did, is the following
\begin{align}
\mathrel{\Big|} \mathrel{\Big|} \tilde{f}^{\ell}- \tilde{f}^{\ell-1} \mathrel{\Big|} \mathrel{\Big|}_{L^2(I)}^2 &=
 \mathrel{\Big|} \mathrel{\Big|}\tilde{f}^{\ell}- P_{\ell,\ell-1} \tilde{f}^{\ell} \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2 \nonumber \\
 &\leq  \mathrel{\Big|} \mathrel{\Big|}\tilde{f}^{\ell}-  f \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2+  \mathrel{\Big|} \mathrel{\Big|} f - P_{\ell,\ell-1} \tilde{f}^{\ell} \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2 \nonumber\\
 & \le \mathrel{\Big|} \mathrel{\Big|} \left( P_{\ell+1,\ell} \dots P_{L,L-1}  P_{L}\right) f -  f \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2+  \mathrel{\Big|} \mathrel{\Big|} f- \left( P_{\ell,\ell-1} P_{\ell+1,\ell} \dots P_{L,L-1} P_{L} \right) f \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2 \nonumber\\
 & \le C h_{\ell-1}^{2p+2}  \mathrel{\Big|} \mathrel{\Big|} f^{(p+1)} \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2,
\end{align}
where $h_{\ell-1}$ is the discretization mesh size of $V_{\ell-1}$, and where I used the interpolation error estimate as an upper bound. I am not sure if this is correct, or if there are sharper bounds. Any hint or reference in this regard. Thanks.","['interpolation', 'reference-request', 'operator-theory', 'functional-analysis', 'linear-algebra']"
2052333,Discrete Math - Finding a specific function composition,"I have been given this task:
Find an example of $2$ functions. $g,f: \mathbb N \to\mathbb N$ such that for every $x$ that belongs to $\mathbb N$ this happens: $g(f(x)) = x + 2016$.
We also know that $g$ is not onto and is not $1-1$ and that $f$ is not onto. I'm having problems understanding how I'd describe $g$ in particular. I tried looking for examples but I can't find any. Would you help? Thanks","['function-and-relation-composition', 'discrete-mathematics']"
2052361,An interesting differential equations problem,"Suppose we have four ants, initially at rest, at the four corners of a square centered at the origin. They start walking clockwise, each ant walking directly toward the one in front of him. Suppose also that each ant walks with unit velocity, derive a differential equation that describes the trajectories. Thought: Here is the situation of the problem After some time $t$ , the ants are now at points $E,F,G,H$ . If we denote by $\mathbf{r(t)}$ the position of ant at $A$ , we know $\mathbf{r(0)} = (1,1)$ and after some $t$ , at point $E$ , we have $\mathbf{r(t)} = (x(t),y(t)) = E $ . We are given that $$ \frac{ y - 1 }{x - 1 } = 1 $$ Also, using the arclength formula, we know the path of ant $A$ is $$ \int\limits_0^t \sqrt{ (x')^2 + (y')^2 } dt $$ Am I on the right track?",['ordinary-differential-equations']
2052481,Explicit relationship between the complex derivative and the Jacobian of the corresponding 2d real function?,"As complex numbers can be represented in the complex plane $\mathbb{C} \cong \mathbb{R}^2$ there are two related notions of derivatives. For a real 2d function with continuous partial derivatives 
$ f:\left(
  \begin{array}{c}
    x \\
    y \\
  \end{array}
\right) 
\mapsto
\left(
  \begin{array}{c}
    u \\
    v \\
  \end{array}
\right) 
$ the derivative at a point $z_0 = \left(
  \begin{array}{c}
    x_0 \\
    y_0 \\
  \end{array}
\right) $ is defined as $$
\lim_{h \to 0}\frac{\|f(z_0+h)-f(z_0)-Df(z_0)h \|}{\|h\|} = 0
$$ where $Df$ is the Jacobian $\left(
  \begin{array}{cc}
    u_x & u_y \\
    v_x & v_y \\
  \end{array}\right)$ and has due to the Cauchy-Riemann equations the structure  $\left(
  \begin{array}{cc}
    a & b \\
    -b & a \\
  \end{array}\right)$ For a complex differentiable function $f(x,y) = u(x,y) + iv(x,y)$ the complex derivative is defined as $$
\lim_{h \to 0}\frac{|f(z_0+h)-f(z_0)-f'(z_0)h |}{|h|}
  = \lim_{h \to 0}\left|\frac{f(z_0+h)-f(z_0)}{h} -f'(z_0)\right| = 0
$$ where now $f'(z_0)\in \mathbb{C}$. Comparing the two we have $$
Df(z_0)h \cong f'(z_0)h = f'(z_0)(h_1 + ih_2)
$$ On the right hand side, the complex derivative $f'(z_0)$ is just a complex number multiplying $h=h_1 + ih_2$. How can $f'(z_0)$ explicitely be calculated from the partial derivatives that appear in the Jacobian of the corresponding 2d function? From the structure of the Jacobian and interpreting it as the matrix representation of a complex number, my first guess would be that $$
f'(x_0) = a + ib = u_x(x_0) + iu_x(x_0) = v_y(x_0) - iv_x(x_0)
$$",['complex-analysis']
2052488,"Prove the limit of $(x+y)/(x^2+y^2)$ as $(x,y)$ approaches $(0,0)$ does not exist","How to prove that the limit of
$$
\frac{x+y}{x^2+y^2}
$$
as $(x,y)$ approaches $(0,0)$ does not exist? I tried with $y = x$ , $x = 0$ , $y = 0$, which gives the limit of negative infinity and positive infinity, Somehow, I feel It's not correct.  Any better ways?","['multivariable-calculus', 'limits']"
2052557,Iterated Law of Expectation in Linear Regression,Suppose $E(Y|X)$ exists. There exists a disturbance term $\epsilon$ such that $Y=E(Y|X)+\epsilon$ where $E(\epsilon|X)=0$. I have trouble understanding the following simplifying process: $E(Y-E(Y|X)|X)=E(Y|X)-E(E(Y|X)|X)$. The second term on RHS simplifies to: $E(Y|X)$. Can someone explain this in detail? Thanks.,"['regression', 'statistics', 'probability', 'expectation']"
2052603,Is the set of positive rational sequences diverging to infinity countable?,"I came up with a positive proof for the title's question; however, it seems very counter-intuitive to me because the set of binary sequences seems much smaller, yet is uncountable. Moreover, we can forgo positivity as a condition, but I won't for simplicity. Perhaps I have made an elementary mistake? Proof . Suppose $q_n$ is a positive sequence of rationals diverging to infinity. Since it diverges there are only finitely many $q_n$ below any given $i\in\mathbb{N}$. The set of positive rationals below $i$, $Q_i$, is countable. The set of orderings of any finite subset of $Q_i$ is finite; in turn, for any finite subset of $Q_i$, the set of all its finite sequences is finite.  The number of subsets of $Q_i$ of a given size is countable; as is the number of sizes. In turn, the set of all ordered finite subsets of $Q_i$, $\tilde{Q}_i$ is countable. Then the set $\bigcup_{i\in\mathbb{N}} \tilde{Q}_i$ is countable, and the set of positive rational sequences diverging to infinity injects into it.","['sequences-and-series', 'elementary-set-theory']"
2052616,Is there anything like “cubic formula”?,"Just like if we have any quadratic equation which has complex roots, then we are not able to factorize it easily. So we apply quadratic formula and get the roots. Similarly if we have a cubic equation which has two complex roots (which we know conjugate of each other) and one fractional root, then we are not able to find its first root by hit & trial. So my question is like quadratic formula, is there exist any thing like cubic formula which help in solving cubic equations? For example, I have an equation $$2x^3+9x^2+9x-7=0\tag{1}$$ and I have to find its solution which I am not able to find because it has no integral solution. Its solutions are $\dfrac {1}{2}$, $\dfrac{-5\pm \sqrt{3}i}{2} $, I know these solutions because this equation is generated by myself. So how can I solve equations like these? Also while typing this question, I thought about the derivation of quadratic formula, which is derived by completing the square method. So I tried to apply ‘completing the cube’ method on the general equation $ax^3+bx^2+cx+d=0$ but it didn't help. So please help me in finding a cubic formula or to solve the equations like given in example by an alternative method.","['algebra-precalculus', 'cubics', 'roots', 'polynomials']"
2052618,$\sum_{n=1}^\infty \frac{1}{\sqrt{n^2+1}}$ Divergent? Did I do this right?,"$$\sum_{n=1}^\infty \frac{1}{\sqrt{n^2+1}}$$
$$a_n = \frac{1}{\sqrt{n^2+1}} <\frac{1}{\sqrt{n^2}} \le \frac{1}{n},  \forall n\ge1$$
Then by The Comparison Test with $b_n$, where $b_n$ is a divergent p-series  = $\frac{1}{n}$ and $a_n \le b_n$, Then
$\sum_{n=1}^\infty \frac{1}{\sqrt{n^2+1}}$ is Divergent.","['sequences-and-series', 'calculus']"
2052624,Limit of a sequence including infinite product. $\lim\limits_{n \to\infty}\prod_{k=1}^n \left(1+\frac{k}{n^2}\right)$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I need to find the limit of the following sequence:
$$\lim\limits_{n \to\infty}\prod_{k=1}^n \left(1+\frac{k}{n^2}\right)$$","['products', 'infinite-product', 'sequences-and-series', 'limits']"
2052642,Optimal $\mathbf{W}$'s columns are the generalized eigenvectors in $\mathbf{S}_\text{B}\mathbf{v}_i = \lambda_i \mathbf{S}_\text{W} \mathbf{v}_i$,"I am trying to understand Linear Discriminant Analysis (a.k.a. Fisher's discriminant analysis). I am reading this technical report and Christopher Bishop's Pattern Recognition and Machine Learning (section 4.1.4, p.187). Some definitions first: A data point $\mathbf{x}_n$ is defined as a column vector with $M$ components. $\mathbf{X}$ is defined as a $(M, N)$ matrix where columns are the data points $\mathbf{x}_n$ . $K$ is the number of classes. $\mathbf{S}_\text{B}$ : between-class covariance of the data before projection. $\mathbf{S}_\text{W}$ : within-class covariance of the data before projection. $\mathbf{s}_\text{B}$ : between-class covariance of the projected data. $\mathbf{s}_\text{W}$ : within-class covariance of the projected data. The general idea in Fisher's discriminant analysis is to project the data points in $\mathbf{X}$ to a $K - 1$ space that maximizes $\mathbf{s}_\text{B}$ and minimizes $\mathbf{s}_\text{W}$ . In the technical report, the discrimination criterion is defined as $$J(\mathbf{W}) = \frac{|\mathbf{s}_\text{B}|}{|\mathbf{s}_\text{W}|} = \frac{|\mathbf{W}^\mathsf{T}\mathbf{S}_\text{B}\mathbf{W}|}{|\mathbf{W}^\mathsf{T}\mathbf{S}_\text{W}\mathbf{W}|} = \frac{\text{det}(\mathbf{W}^\mathsf{T}\mathbf{S}_\text{B}\mathbf{W})}{\text{det}(\mathbf{W}^\mathsf{T}\mathbf{S}_\text{W}\mathbf{W})}$$ Question $(1)$ arises here: why does maximizing the ratio of the discriminants is a good criterion for maximizing $\mathbf{s}_\text{B}$ and minimizing $\mathbf{s}_\text{W}$ ? (there seems to be a basic linear algebra notion I am missing) Now, right after defining this criterion, they say that the columns of an optimal $\mathbf{W}$ are the generalized eigenvectors $\mathbf{v}_i$ that correspond to the nonzero eigenvalues $\lambda_i$ in $$\mathbf{S}_\text{B} \mathbf{v}_i = \lambda_i \mathbf{S}_\text{W} \mathbf{v}_i,$$ subject to the normalization constraint $$\mathbf{v}_i^\mathsf{T}\mathbf{S}_\text{W}\mathbf{v}_i = 1.$$ Now arises question $(2)$ : why is this true? To find the optimal $\mathbf{W}$ , I guess we need to solve $\frac{\partial J(\mathbf{W})}{\partial\mathbf{W}} = 0$ for $\mathbf{W}$ . I started by first noting (from the Matrix Cookbook) that $$\frac{\partial\text{det}(\mathbf{W}^\mathsf{T}\mathbf{A}\mathbf{W})}{\partial \mathbf{W}} = 2 \text{det}(\mathbf{W}^\mathsf{T}\mathbf{A}\mathbf{W})\mathbf{A}\mathbf{W}(\mathbf{W}^\mathsf{T}\mathbf{A}\mathbf{W})^{-1}$$ for any symmetric matrix $\mathbf{A}$ ( $\mathbf{S}_\text{B}$ and $\mathbf{S}_\text{W}$ do are symmetric, since they are covariance matrices). This leads me to the following $$\frac{\partial J(\mathbf{W})}{\partial\mathbf{W}} = \frac{2|\mathbf{W}^\mathsf{T} \mathbf{S}_\text{B}\mathbf{W}|\{ \mathbf{S}_\text{B}\mathbf{W}(\mathbf{W}^\mathsf{T} \mathbf{S}_\text{B}\mathbf{W})^{-1} - 
\mathbf{S}_\text{W}\mathbf{W}(\mathbf{W}^\mathsf{T} \mathbf{S}_\text{W}\mathbf{W})^{-1}
\}}
{|\mathbf{W}^\mathsf{T} \mathbf{S}_\text{W}\mathbf{W}|}$$ But then I'm not sure where I am going and would have liked some insight!","['discriminant', 'machine-learning', 'statistics', 'linear-algebra']"
2052652,"Lemma 1.1 from ""A course in minimal surfaces"" by Colding and Minicozzi.","I have just started reading the book ""A course in minimal surfaces"" by Colding and Minicozzi and I have a question about the proof of the very first Lemma1.1 . We have an open domain $\Omega \subset \mathbb{R}^2$ and a two-form $\omega$ defined on $\Omega \times \mathbb{R}$. We know that $\omega$ is closed ($d\omega = 0$). 
Let $u: \Omega \to \mathbb{R}$ be a $C^2$ function and let $Graph(u)$ be its graph. Let $\Sigma$ be any other surface with the same boundary of $Graph(u)$. Then in the book there is written that from Stokes' theorem we have
$$
\int_{Graph(u)} \omega = \int_{\Sigma} \omega.
$$ My question is: in order to apply Stokes' Lemma, we should know that $\omega$ is exact, right? We could use Poincaré's Lemma, but we don't know if $\Omega \times \mathbb{R}$ is contractible. Maybe Colding and Minicozzi are tacitly assuming that $\Omega$ is simply connected? Thank you!","['minimal-surfaces', 'differential-forms', 'stokes-theorem', 'submanifold', 'differential-geometry']"
2052690,Probability two students are paired,I'm wondering why my answer here is incorrect. The question is: John and Steve are members of a class of $20$ students. What is the probability that they are paired together? My thought was simply $1$ way to pair John and Steve and $20 \choose 2$ total pairs so $\frac{1}{190}$. But the correct answer is $\frac{1}{19}$. How is it that my method doesn't work?,['probability']
2052707,Squeeze Theorem for Abelian Groups,"Recently I saw this theorem : Assume that $A\leq B\leq C$ with $A\cong\mathbb{Z}^n\cong C$. Then $B\cong\mathbb{Z}^n$. I wonder whether it is possible to generalise this result to arbitrary finitely generated abelian groups. What I mean is this : If $A\leq B\leq C$ are finitely generated abelian groups with $A\cong C$ then $B\cong C$. Using the fundamental theorem of finitely generated abelian groups it seems like it is true but I could not prove it. Edit : If we remove the condition that $C$ is abelian then we know that the claim is not true for free groups. There are subgroups of $F_2$ which is not isomorphic to $F_2$ but contain a subgroup which is isomorphic to $F_2$. But I also do not know an example of a solvable group $C$ with there exists $A\leq B\leq C, A\cong C$ but $B\not\cong C$.","['abelian-groups', 'group-theory']"
2052748,Can a cubic equation have three complex roots?,"I have read somewhere that: If $x+ iy$ is a root of a cubic equation $ax^3+bx^2+cx+d=0$, then $x-iy$ is also a root of this equation. My question is: Can $k-il$ be the other root of this equation if $l\neq 0$? If yes, then it would seem that $k+ il$ should also be a root, but in that case the cubic equation would have four roots which is impossible. Where is the mistake in my logic? ($i$ denotes $\sqrt {-1}$, and is sometimes pronounced as $iota$.)","['algebra-precalculus', 'abstract-algebra']"
2052760,On the use of Fubini's theorem in the proof of Rademacher's theorem,"I am reading Lectures on Lipschitz Analysis by Heinonen, where the proof of Rademacher's theorem is presented in Chapter 3. Let $\, f:\mathbb{R}^n \to \mathbb{R}$ be Lipschitz.
In the first step of the proof, it is shown that the directional derivative of $f$ in direction $v \in \mathbb{R}^n$ exists for almost all $x \in \mathbb{R}^n$ . For any fixed $x,v \in \mathbb{R}^n$ the author defines a real-valued function $$ f_{x,v}(t)=f(x+tv)$$ which is Lipschitz, and hence differentiable at almost every $t$ . Then, it is claimed that after fixing $v$ , we can conclude from Fubini's theorem that $$ \lim_{t \to 0} \frac{f(x+tv) - f(x)}{t}$$ exists for almost every $x \in \mathbb{R}^n$ . Question: Why this conclusion follows from Fubini?","['derivatives', 'real-analysis', 'lipschitz-functions', 'measure-theory']"
2052817,Lattice of subgroups of $S_{n}$,"There is a method of calculating all the subgroups of dihedral group $D_{2n}$.I want to know that is there any method to find all the subgroups of $S_{4}, A_{4} $. I have found all the subgroups of both $S_{4}$ and $A_{4}$ somewher on google but want to know how to find them.
Secondly I want to know that how can we say that our lattice diagram is true?  And why it is called lattice of subgroups? Thanks in advance","['group-theory', 'symmetric-groups']"
2052827,Calculate a given integral,"Calculate the following integral:
$$\int \frac{x + \sqrt{x}}{\sqrt[4]{x} + \sqrt{x} + 1} dx$$ Here is what I did so far: The expression in the integral can be rewritten as: 
$$\int xx^{-1/4} \frac{x^{-1/2} + x^{-1}}{1 + x^{1/4} +x^{-1/4}} dx$$ By substituting $x^{1/4}$ with a variable $t$, we get $dt = \frac{4}{5} xx^{-1/4}dx$. Now, lets apply this to our integral:
$$\int \frac{5}{4} \frac{t^{-2} + t^{-4}}{1 + t + t^{-1}} dt = \frac{5}{4} \int \frac{t^2 + 1}{t^3(t^2 + t + 1)} dt$$ This all I did so far. I haven't succeded to solve the last integral yet. Thank you!","['indefinite-integrals', 'integration']"
2052846,Constrained combinations of marbles in jars,"I'm fairly new to combinations so I apologize upfront that I don't even know where to start with this one. My problem is of the form: 
If I have $N$ marbles, and $M$ jars that can only fit a max of $Z$ marbles each, for how many combinations may I distribute all marbles amongst the jars? (Assume $N$ is between $1$ and $M \cdot Z$, such that you are guaranteed to have at least one jar to place a marble.) Can someone please provide some insight, or point me in the right direction?","['combinations', 'combinatorics', 'statistics', 'probability']"
2052897,At what point are there gaps in the sequence of known prime numbers.,"I'm not sure how to properly word this question so please bear with me. If you look at the beginning of the prime number list it's obvious that the sequence is complete. At least up to a certain point. I was reading an article about a recently discovered prime number and it stated that Mersenne Prime's are currently being targeted. They referred to them as low hanging fruit and stated that many numbers are not being checked. So my question is at what point do we start having gaps in the ""sequence"" of prime numbers.","['algebra-precalculus', 'prime-numbers', 'arithmetic']"
2052981,"Prove $\lim_{x\to2} \sqrt{4x-x^2} = 2$ by definition with $\epsilon, \delta$","I need to prove using $\epsilon, \delta$: $$\lim_{x\to2} \sqrt{4x-x^2} = 2$$ Meaning I need to prove that for every $\epsilon > 0$, there is a $\delta >0$ such as that $0<|x-2|<δ→|f(x)-2|<ϵ$ I tried going like this:
$|\sqrt{4x-x^2} -2|<|\sqrt{4x}-2|<|2\sqrt x-2|=|2(\sqrt x-1)|=2|\sqrt x-1|<2|\sqrt x-1|$... But I don't know how to choose my $\delta$ from here.","['epsilon-delta', 'functions', 'limits']"
2053065,Finding $f(x)$ in a Fourier Integral,"I'm having trouble trying to find that function $f(x)$ or trying to find that it doesn't exist. $$\int_0^\infty f(x)\sin(\alpha x)dx=
\left\{
\begin{array}{lr}
\alpha & \alpha \in (0, 2\pi) \\
e^{-(\alpha - 2\pi)} & \alpha \in (2\pi, \infty)
\end{array}
\right.$$ I know that I am supposed to analyse that comparing to a Fourier Integral but I am stuck trying to analyse $\alpha $ and its values when <0,=0,>0.. Still, I don't have much idea of what to do in this question.. edit: we can assume that the function is odd Sorry for the latex and thank you in advance!","['periodic-functions', 'calculus', 'functions']"
2053095,Series of a square of modified Bessel function of first kind,"I need help with the following series:
$$
S(x)=\sum _{n=1}^{+\infty} \frac{(-1)^n \hspace{0.05cm} I_{n}^2 (x)}{2n-1}, \hspace{0.3cm}x \in \mathbb{R}_{>0}
$$
where $I_n(x)$ is the modified Bessel function of first kind. Is it possible to obtain a nice relation for it (i.e. not in the form of an infinite sum)? There is this formula (eq. 2.3 in the M. D. Rogers paper ):
$$
\sum _{n=-\infty }^{\infty } \frac{J_n^2(z)}{n+y}=J_{y}(z) J_{-y}(z)\frac{\pi}{\sin(\pi y)}
$$
If we set $y=-\frac{1}{2}$, $z=ix$ and use $J_n(ix)=i^nI_n(x)$, $J_{-1/2}(z)=\left(\frac2{\pi z}\right)^{1/2}\cos z$, $J_{1/2}(z)=\left(\frac2{\pi z}\right)^{1/2}\sin z$, and $\sin(2ix)=i\sinh(2x)$ , we get
$$
\sum _{n=-\infty}^{+\infty} \frac{(-1)^n \hspace{0.05cm} I_{n}^2 (x)}{2n-1}=-\frac{\sinh(2x)}{2x}
$$
But it's not the sum $S(x)$, which starts from $n=1$ and not $n=-\infty$. I don't know if it helps, but there is also a formula (from ""Integrals and Series Vol. 2: Special Functions"", also here a similar formula)
$$
\sum _{n=1}^{\infty } \frac{(-1)^n I_{n\nu}^2 (z)}{n^2-a^2}=\frac{I_{\nu}^2(z)}{2 a^2}-\frac{\pi  \csc (\pi  a) I_{\nu a}^2(z)}{2 a}. 
$$
but I am not sure if we can set $\nu=1$ (but even if we can, it's still not very helpful).","['bessel-functions', 'sequences-and-series']"
2053108,Showing that a matrix is invertible by factorisation,Let $P_5$ be the vector space of polynomials of degree $\leq$ 5 over $Q$. $P_5 = {a_0 + a_1x + a_2x^2 + a_3x^3 + a_4x^4 + a_5x^5: a_i \in Q}$ and let $D: P_5 \longrightarrow P_5$ be the linear map $D(\alpha) = \frac{d \alpha}{dx}$. By factorising the expression $D^6 - Id$ show that $D^4+D^2+Id: P_5 \longrightarrow P_5$ is invertible and write down its inverse. I am not quite sure how I can show that $D^4+D^2+Id$ is invertible but I tried factorising the given expression in the following way: $$ \begin{align*} D^6 - Id &= D^6 - Id^6\\ &= (D-Id) (D^5 + D^4Id + D^3Id^2 + D^2Id^3 + DId^4 + Id^5)\\ &= (D-Id)(D^5 + D^4 + D^3 + D^2 + D + Id)\\ &= (D-Id) (D^3(D^2+D+Id) + D^2 + D+Id)\\ &= (D-Id) (D+Id) (D^4+D^2+Id)\\ &= (D^2 - Id) (D^4 + D^2 + Id)\end{align*} $$ and now I am not sure what to do next Thank you in advance!,"['matrices', 'linear-algebra', 'vector-spaces']"
2053201,Probability Distribution Doubt,"1) A drawer full of AAA batteries contains 40 batteries, but 6 of them are dead. Suppose 4 batteries are randomly selected from the drawer to use. a) What is the probability that all four are good? 
(I used the Hypergeometric Distribution equation $ P(x) ={ \binom{k}{x} \binom{N - k}{n - x } \over \binom{N}{n}}$  assuming that the variables are N=40, k=34, n=4, x=4 ) $ P(x=4) ={ \binom{34}{4} \binom{40 - 34}{4 - 4 } \over \binom{40}{4}}$ $ P(x=4) = 0.507 $ would this be the correct way to answer this question? b) What is the probability that exactly one dead battery is selected amount the 4? 
(I assume the hypergeometric applies to this as well. Using $N=40$, $k=6$, $n=4$,$x=1$) $ P(x=1) ={ \binom{6}{1} \binom{40 - 4}{4 - 1 } \over \binom{40}{4}}$ I don't think this is right. I am confused how to set this equation right. I want to know what the variables for this equation stand for. I am confused on what each mean. So far from what i understand is N = total # of things, n = randomly select without replacement, x = # of successes we need)","['statistics', 'probability', 'probability-distributions']"
2053217,What is the length of the diagonal of a tesseract (four-dimensional cube) with side length $a$?,What is the length of the diagonal of a tesseract (four-dimensional cube) with side length $a$? Additional info: - Diagonal of a line: $a$ - Diagonal of a square: $\sqrt{2a^2}$ - Diagonal of a cube: $\sqrt{3a^2}$ - Diagonal of a tesseract: ??,['geometry']
2053221,"$n \cdot \text{lcm}(a,b,2016) = ab-2016$","Find the largest even number $n$ such that there exist positive integers $a,b$ with $n \cdot \text{lcm}(a,b,2016) = ab-2016$. I tried using the fact that $\text{lcm}(a,b,2016) \geq a,\text{lcm}(a,b,2016) \geq b,$ and $\text{lcm}(a,b,2016) \geq 2016$, but didn't see how to use this to solve the question. How should we approach it?",['number-theory']
2053229,"The connection between the Jacobian, Hessian and the gradient?","In this Wikipedia article they have this to say about the gradient: If $m = 1$, $\mathbf{f}$ is a scalar field and the Jacobian matrix is reduced to a row vector of partial derivatives of $\mathbf{f}$—i.e. the gradient of $\mathbf{f}$. As well as The Jacobian of the gradient of a scalar function of several variables has a special name: the Hessian matrix, which in a sense is the ""second derivative"" of the function in question. So I tried doing the calculations, and was stumped. If we let $f: \mathbb{R}^n \to \mathbb{R}$, then 
$$Df = \begin{bmatrix}
\frac{\partial f}{\partial x_1} & \dots & \frac{\partial f}{\partial x_n}
\end{bmatrix} = \nabla f$$
So far so good, but when I try to calculate the Jacobian matrix of the gradient I get
$$D^2f = \begin{bmatrix} 
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_2 x_1} & \dots & \frac{\partial^2 f}{\partial x_n x_1} \\
\frac{\partial^2 f}{\partial x_1 x_2} & \frac{\partial^2 f}{\partial x_2^2} & \dots & \frac{\partial^2 f}{\partial x_n x_2} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_1 x_n} & \frac{\partial^2 f}{\partial x_2 x_n} & \dots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}$$
Which according to this article, is not equal to the Hessian matrix but rather its transpose, and from what I can gather the Hessian is not generally symmetric. So I have two questions, is the gradient generally thought of as a row vector? And did I do something wrong when I calculated the Jacobian of the gradient of $f$, or is the Wikipedia article incorrect?","['multivariable-calculus', 'hessian-matrix', 'jacobian']"
2053273,Mixed Poisson processes and independent increments,"In reading Ross' Introduction to Probability Models , there's a part on mixed Poisson processes which states Suppose that $L$ is continuous with density function $g$. Because $P\{N(t+s) - N(s) = n \} = \int_0^{\infty} P\{N(t+s) - N(s) = n | L = \lambda \}\ g(\lambda)\ d\lambda = \int_0^{\infty} e^{-\lambda t}\frac{(\lambda t)^n}{n!}\ g(\lambda)\ d\lambda$ we see that a conditional Poisson process has stationary increments. However, because knowing how many events occur in an interval gives information about the possible value of $L$, which affects the distribution of the number of events in any other interval, it follows that a conditional Poisson process does not generally have independent increments. (...) I don't understand why ""it follows that a conditional Poisson process does not generally have independent increments"". Can someone give a concrete example? And in what instances does it have independent increments?","['poisson-process', 'statistics', 'probability']"
2053276,Tightness of a sequence of random variables with bounded mean and variance,"Suppose that $X_1, X_2, X_3,\ldots$ are real–valued random variables having means $\mu_1, \mu_2, \mu_3, \ldots $ and standard deviations $\sigma_1, \sigma_2, \sigma_3, \ldots$ and suppose that $|\mu_n|$ and $\sigma_n$ are bounded sequences. Show that the collection $(X_n)$ is tight. I was wondering how to prove it. I want to use the Chebyshev’s
inequality but I am sure how?","['probability-theory', 'measure-theory']"
2053319,Number of solutions of $x_1+2x_2+\cdots+kx_k=n$?,"Suppose $n$ be a given positive integer. Then the Diophantine equation $x=n$ has only $1$ solution. Just by inspection, I found that the Diophantine equation $x+2y=n$ has $\left\lfloor \dfrac{n}{2}+1\right\rfloor$ non-negative solutions for $(x,y).$ Also, according to this post the Diophantine equation $x+2y+3z=n$ has $\left\lfloor \dfrac{n^2}{12}+\dfrac{n}{2}+1 \right\rfloor$ non-negative solutions for $(x,y).$ Is there any closed form for the number of non-negative integer solutions for $(x_1,x_2,\cdots,x_k)$ of $$x_1+2x_2+3x_3+\cdots+kx_k=n$$ for a given $k\in\Bbb{N}$ ? How can I prove these formulas rigorously? EDIT After a very tedious calculation I found that the equation $w+2x+3y+4z=n$ has $\left\lfloor \dfrac{n^3}{144}+\dfrac{5n^2}{48}+\dfrac{(15+(-1)^n)n}{32}+1 \right\rfloor$ solutions. This solution completely agree with the approximation given by Rus May . However still I believe that we can do something more than this.","['diophantine-equations', 'linear-diophantine-equations', 'number-theory', 'generating-functions', 'combinatorics']"
2053325,How do I add summation formula's like this: $F(n+2) = 1 + \sum\limits_{i=0}^{n} F(i)$?,"I have a Fibonacci problem ... basically I have to show that: $$F(n+2) = 1 + \sum_{i=0}^{n} F(i)$$ Using strong induction, I must now show that: $$F(n+2) = F(n+1) + F(n)$$
$$F(n+2) = (1 + \sum_{i=0}^{n-1} F(i)) + (1 + \sum_{i=0}^{n-2} F(i))$$
$$F(n+2) = 2 + \sum_{i=0}^{n-1} F(i) +  \sum_{i=0}^{n-2} F(i)$$ How do I add these two summations to show that they equal the first equation (at the top)? Please provide the steps with explanation as to what you are doing. Thanks","['induction', 'summation', 'fibonacci-numbers', 'discrete-mathematics']"
2053470,"How to generalize Reshetnikov's $\arg\,B\left(\frac{-5+8\,\sqrt{-11}}{27};\,\frac12,\frac13\right)=\frac\pi3$?","We have, $$\arg z_1 = \frac{k\,\pi}3, \quad z_1 = \left(\tfrac{1+\sqrt{-3}}{2}\right)^k\tag1$$ $$\arg z_2=\frac{k\,\pi}3, \quad z_2 = \left( B\Big(\color{blue}{\tfrac{-5+8\,\sqrt{-11}}{27}};\,\tfrac12,\tfrac13\Big)\right)^k \tag2$$ for $k=1,2,3$ and incomplete beta function $B(z;a,b)$ . The second with $k=1$ is by V. Reshetnikov. But that cannot be an isolated result. Reshetnikov states (without giving details) that $(2)$ is equivalent to, $$B\left(\frac19;\,\frac16,\frac13\right) = \frac12\,B\left(\frac16,\frac13\right)
=\frac{1}{2\sqrt{\pi}}\,\Gamma\left(\frac16\right)\Gamma\left(\frac13\right)\tag3$$ Thus, $(3)$ is related to, $$I\left(\color{blue}{\frac19};\,\frac16,\frac13\right)=\frac12 \tag4$$ with regularized beta function $I(z;a,b)$ . The equation $$I\left(z;\,\frac16,\frac13\right)=\frac1n \tag5$$ is quite easy to solve. For example, the smallest real root of, $$-1 + 99 z - 243 z^2 + 81 z^3= 0\tag6$$ will yield $n=3$ . More specifically, $$I\left(\frac19\Big(1-4\sin\big(\tfrac{\pi}{18}\big)\Big)^2;\,\frac16,\frac13 \right)=\frac13\quad\quad\tag7$$ $$\quad I\left(\frac13\Big(1-\frac{\sqrt2}{\sqrt[4]3}\Big)^2;\,\frac16,\frac13\right) =\frac14\quad\tag8$$ and so on. Q: But how do we get from $\color{blue}{\frac19}$ of $(4)$ to $\color{blue}{\frac{-5+8\,\sqrt{-11}}{27}}$ of $(2)$ ? (I assume a hypergeometric transformation is involved.) If the answer can be found, then perhaps we can use an expression $P(z)$ of a root $z$ of $(6)$ to find some, $$\arg\,B\left( P(z);\,\frac12,\frac13\right)=\beta\,\pi\tag7$$ where $\beta$ is a rational/algebraic number?","['special-functions', 'complex-analysis', 'beta-function', 'closed-form']"
2053472,Partitioning Binomial Coefficients into Four Groups with Equal Sum,"It is well known that $$\sum_{k \textrm{ odd}} \binom{n}{k} = \sum_{k \textrm{ even}} \binom{n}{k} = 2^{n-1}.$$ One way of thinking about this is by ""toggling"" a specific element, e.g. $1.$ What I mean is that if you have an subset with an even number of elements and it contains $1,$ remove it. If it doesn't contain $1,$ add it in. This is a bijection between  subsets of even and odd size. I'm wondering for what (if any) $n$ is it possible to partition $\{0, 1, \ldots, n\}$ into four subsets $S_1\sqcup S_2 \sqcup S_3 \sqcup S_4$ such that $$\sum_{k\in S_1} \binom{n}{k} = \sum_{k\in S_2} \binom{n}{k} = \ldots = 2^{n-2}.$$ I would also be interested in the question for specific $n,$ e.g. $2^{m}$ or $2^{m}-1$ for some $m.$ Something to note is that you won't really be able to do this by hand since the middle terms themselves are close to $2^{n-2}$ for $n \le 16.$ When $n$ is large there is more room to play around since $\binom{2n}{n} \approx \frac{2^{2n}}{\sqrt{\pi n}}.$","['combinatorics', 'binomial-coefficients']"
2053480,"Chebyshev's Inequality: given probability, find $k$","Edit with Context: Book says the % of data captured within k standard deviations $= 1 - \dfrac{1}{k^2}$ . Dug a bit deeper and found it was derived using Chebyshev's but no direct derivation found $\ldots$ Within how many multiples of standard deviation will capture at least $\boldsymbol{75}$ % of the data in a distribution with a mean $\boldsymbol\mu$ ? I derived the formula below and got that $k$ must be equal to or less than $k$ 2. This doesn't make sense to me as a larger $k$ would capture more and more data, so it should be the other way around. Work is shown below. The inequality derivation: Let $v = |X-\mu|$ .
Let $y = k\sigma$ . Then $$P(v \geq y) \leq \frac{1}{k^2} = 1 - P(v < y) \leq \frac{1}{k^2}.$$ So $$k \leq \sqrt\frac{1}{1-P(v<y)}.$$","['statistics', 'probability']"
2053544,"Wrong Wolfram Alpha limit? $\lim\limits_{(x,y)\to(0,0)}\frac{{xy^3}}{{x^2+y^6}}$","Wolfram Alpha gives me: $\lim\limits_{(x,y)\to(0,0)} \dfrac{{xy^3}}{{x^2+y^6}}=0$. I assume $f\left( {x,y} \right) = \dfrac{{x{y^3}}}{{{x^2} + {y^6}}}$ I find that when $x=y^3$, $f\left( {x,y} \right) = \dfrac{1}{2}$, while when $x=0, f\left( {x,y} \right) = 0$. Hence the limit does not exist. Please correct me if I am wrong. Thank you.","['multivariable-calculus', 'limits']"
2053560,One form of poincare duality imply the other?,"I saw in my differential geometry class that Poincare duality states: $H_{DR}^k(X,\mathbb{R}) \times H_{DR}^{n - k}(X,\mathbb{R})  \rightarrow \mathbb{R}$
given by $([v],[r]) \mapsto \int_{X} n \wedge r$ is non-singular Prof mentioned that If X is compact oriented manifold of dimension n, then this is really the same as saying: $H_{DR}^i(X,\mathbb{R}) \cong H_{n - i}(X,\mathbb{R})$ where the right side of the equation we have singular homology. Can someone explain why this is the case ?","['homology-cohomology', 'manifolds', 'de-rham-cohomology', 'algebraic-topology', 'differential-geometry']"
2053584,Can an integral domain have an element that has no square root but has a square root in the field of fractions?,"A lemma states: Let $R$ be a UFD and $F=\operatorname{Frac}(R)$. Let $d\in R$, then equation $a^2=d$ has a root in $R$ iff it has a root in $F$. So I want to ask, is there a counterexample for this if $R$ is not a UFD?  I only know some non-UFD integral domains like ring of integers for some values.","['examples-counterexamples', 'abstract-algebra', 'unique-factorization-domains', 'number-theory', 'ring-theory']"
2053591,Hilbert's Nullstellensatz: changing $\mathbb{C}$ to $\overline{\mathbb{Q}}$ and stuck in proof,"(Weak form of) Hilbert's nullstellensatz: In $\mathbb{C}[x_1,\cdots,x_n]$, every maximal ideal is of the form $(x_1-a_1,\cdots,x_n-a_n)$ for some $(a_1,\cdots,a_n)\in\mathbb{C}^n$. Proof: (1) The set $\{\frac{1}{x-a}:a\in\mathbb{C}\}$ is linearly independent in $\mathbb{C}(x)$. (This is true if $\mathbb{C}$ is replaced by any field ; am I right?) (2) Let $M$ be a maximal ideal in $\mathbb{C}[x_1,\cdots,x_n]$. (3) Let $\pi_1: \mathbb{C}[x_1]\rightarrow \mathbb{C}[x_1,\cdots,x_n]/M$, $f(x_1)=x_1+M$ be natural ring homomorphism. (4) Suppose if possible $\ker\pi_1=0$. Then $\mathbb{C}[x_1]$ is isomorphic to subring of (quotient) field, so it should be field, i.e. $\mathbb{C}[x_1]=\mathbb{C}(x_1)$. Now $\mathbb{C}(x_1)$ is of uncountable dimension over $\mathbb{C}$ (by (1)). But $\mathbb{C}[x_1,\cdots,x_n]/M$ is of countable dimension over $\mathbb{C}$, so $\mathbb{C}(x_1)$ has countable dimension over $\mathbb{C}$. (5) Thus in (4) $\ker\pi_1$ is non-zero. (6) Then one shows that $\ker\pi_1=M\cap \mathbb{C}[x_1]$ contains some polynomial $x_1-a_1$; similarly, we obtain $M$ contains $x_i-a_i$ for some $a_i\in\mathbb{C}$ ($i=1,2,\cdots,n$). This completes proof. Question: The weak form is till valied if $\mathbb{C}$ is replaced by any algebraically closed field. Take $\overline{\mathbb{Q}}$, the algebraic closure of $\mathbb{Q}$. Then  $\overline{\mathbb{Q}}$ is countable, so  $\{\frac{1}{x-a}:a\in\overline{\mathbb{Q}}\}$ has countable dimension; thus in (4) we do not arrive directly at contradiction, and so above proof doesn't work. My question is can we modify the above proof for $\overline{\mathbb{Q}}$ instead of $\mathbb{C}$?","['ring-theory', 'algebraic-geometry', 'commutative-algebra']"
2053611,Show that for $|f_n| \le g_n$ $\forall n$: $\lim_{n\to \infty} {\int_E g_n } = \int_E g \Rightarrow \lim_{n\to \infty} {\int_E f_n } = \int_E f$,"Let $(f_n)_{n \in \Bbb N}$ be a sequence of measurable functions on $E$ , that converges almost everywhere pointwise towards $f$ . Let $(g_n)_{n \in \Bbb N}$ be a sequence of integrable functions on $E$ that converge almost everywhere on $E$ pointwise  towards $g$ . Also, suppose that $|f_n| \le g_n$ $\forall n \in \Bbb N$ . I have to show that: $$\lim_{n\to \infty} {\int_E g_n } = \int_E g \Rightarrow \lim_{n\to \infty} {\int_E f_n } = \int_E f$$ I don't really understand how i should show this. I don't see why the right hand side of the relation isn't just allways true due to dominated convergence (i can simply pick one $g_n$ ).
Any ideas or tipps on how to show this and on why it isn't already true because of dominated convergence?","['real-analysis', 'integration', 'lebesgue-integral']"
