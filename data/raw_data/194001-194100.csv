question_id,title,body,tags
3724359,Does there exist a gradient chain rule for this case?,"My question comes from this article in Wikipedia. I noticed that there is a chain rule defined for the composition of $f:\mathbb{R}\to\mathbb{R}$ and $ g: \mathbb{R}^n \to \mathbb{R}$ given by $$
\nabla (f \circ g) = (f' \circ g) \nabla g \tag{1}
$$ My question is if instead we had some functions $f: \mathbb{R}^m \to \mathbb{R}$ and $g: \mathbb{R}^n \to \mathbb{R}^m$ such that $(f \circ g): \mathbb{R}^n \to \mathbb{R}$ , does there exist an expression for $\nabla (f \circ g)$ similar to equation $(1)$ ? I tried looking for any resource who answered this but had no luck. If someone could point me in the right direction I would greatly appreciate it. Thank you!","['partial-derivative', 'multivariable-calculus', 'chain-rule']"
3724386,Does the Borel-Cantelli Lemma imply countable additivity?,"Let $(\Omega, \mathcal F, P)$ be a finitely additive probability space. If $P$ is not only finitely additive but also countably additive, then it satisfies the Borel-Cantelli Lemma: For all sequences $A_1, A_2,...$ in $\mathcal F$ , if $\sum_n P(A_n) < \infty$ , then $P(\limsup_n A_n) = 0$ . I'm wondering if the converse holds as well. Question. If $P$ (a finitely additive probability) satisfies the Borel-Cantelli Lemma, is $P$ countably additive? Suppose that $P$ satisfies the Borel-Cantelli Lemma and that $A_1, A_2,\ldots$ is a disjoint sequence in $\mathcal F$ . By finite additivity, $$\sum_n P(A_n) \leq P(\bigcup_n A_n) < \infty.$$ So, by the Borel-Cantelli Lemma $P(\limsup_n A_n)=0$ , which implies $P(\liminf_n A_n^c)=1$ . I tried using this fact to manipulate $$P(\bigcup_n A_n) = P(\bigcup_nA_n \cap \liminf_n A_n^c)$$ into something useful, but I wasn't able to get anywhere. I suspect the result doesn't hold, but it seems like coming up with a counterexample (a merely finitely additive probability that satisfies the Borel-Cantelli Lemma) will be pretty difficult.","['borel-cantelli-lemmas', 'measure-theory', 'probability-theory']"
3724407,Find integers $1+\sqrt2+\sqrt3+\sqrt6=\sqrt{a+\sqrt{b+\sqrt{c+\sqrt{d}}}}$,"Root numbers Problem (Math Quiz Facebook): Consider the following equation: $$1+\sqrt2+\sqrt3+\sqrt6=\sqrt{a+\sqrt{b+\sqrt{c+\sqrt{d}}}}$$ Where $a,\,b,\,c,\,d$ are integers. Find $a+b+c+d$ I've tried it like this: Let $w=\sqrt6,\, x=\sqrt3, \, y=\sqrt2, z=1$ $$\begin{align}
(y+z)^2 &= (y^2 + z^2) + 2yz\\
y+z &= \sqrt{(y^2 + z^2) + 2yz}\\
y+z &=  \sqrt{3 + \sqrt{8}}
\end{align}$$ Let $y+z=f$ $$\begin{align}
(x+f)^2 &= (x^2 + f^2) + 2xf\\
x+f &= \sqrt{(x^2 + f^2) + 2xf}\\
x+f &=  \sqrt{(9+\sqrt8) + 2\sqrt{9+3\sqrt8}}
\end{align}$$ And I don't think this going to work since there's still a root term on the bracket that is $9+\sqrt8$ . I need another way to make it as an integer.","['algebra-precalculus', 'integers', 'radicals']"
3724414,Find the angle $x$ in this triangle,"This image was doing the rounds on a popular text messaging application, so I decided to give it a try. From sine rule in $\triangle ABP$ : $$\frac{AB}{\sin(150^\circ)} = \frac{AP}{\sin(10^\circ} \\ \implies AP = 2AB \sin(10^\circ)$$ Applying sine rule again in $\triangle APC$ : $$\frac{AP}{\sin(60^\circ + x)} = \frac{AC}{\sin(x)}$$ Manipulating the equation and using some properties gives us $$x = \arctan\left(\frac{\sqrt 3}{4\sin(10^\circ) - 1}\right)$$ This gives $x = -80^\circ$ , but since it's an arctan, $x = 100^\circ$ . Also, since $\sin(x) = \sin(\pi - x)$ , $x = 80^\circ$ as well. My question is: Is there a way to solve this problem that does not require a calculator? I tried to chase angles but that did not work out in this case. This solution requires computing $\sin(10^\circ)$ as well as the $\arctan$ of that expression, which needs a calculator.","['trigonometry', 'triangles']"
3724548,"question on Natural Log, $\lim \limits_{n\to∞ }(1+\frac{1}{n} + \frac{1}{n^2})^n $","I'm curious what is the solution of this. Is this just same as ordinary natural log? $\lim \limits_{n\to∞ }(1+\frac{1}{n} + \frac{1}{n^2})^n =e?$ A few people says the $\frac{1}{n^2}$ just goes to $'0'$ , so it's same as $'e'$ .
But why? Why $\frac{1}{n}$ remains meaningful, while $\frac{1}{n^2}$ goes to zero?? I'm asking this because I'm stuck while deriving a formula.
I post the part of deriving as a picture. Look. From 1st, they go to 3rd line, by using 2nd. And I assume this suppose $\lim \limits_{n\to∞ }(1+\frac{1}{n} + \frac{1}{n^2})^n $ is just same as 'e'. Thank you genius","['limits', 'logarithms']"
3724568,On a real square matrix of order $10$,"Let $M_{10}$ be the set of $10×10$ real matrices; if $U\in M_{10}$ , then let $\rho(U)=rank(U)$ . Which of the followings are true for every $A\in M_{10}$ ? $(1)\rho(A^8)=\rho(A^9)$ $(2)\rho(A^9)=\rho(A^{10})$ $(3)\rho(A^{10})=\rho(A^{11})$ $(4)\rho(A^8)=\rho(A^7)$ I am able to discard options $(1),(2)$ & $ (4)$ by taking nilpotent matrices of order $9,10$ & $8$ respectively. This leaves $(3)$ as true but I am finding it difficult to write a general proof. I think characteristics polynomial can be used. Can you give any suggestions? Thanks for your time.","['matrix-rank', 'matrices', 'solution-verification', 'linear-algebra', 'characteristic-polynomial']"
3724604,The Petersburg game and fair game (Feller Vol.1),"(Feller Vol.1, P.252, Petersburg's paradox) Let $(X_k)$ be mutually independent random variables, each of which assume $2, 2^2, 2^3, ...$ with probability $2^{-1}, 2^{-2}, 2^{-3}, ...$ . We define $S_n = X_1 + \cdots + X_n$ . Let $e_n$ be the accumulated entrance fees, and $S_n$ be the accumulated gains. The author says that the game is fair if for every $\epsilon >0$ , $$P(|\frac{S_n}{e_n} -1|> \epsilon) \to 0.$$ Then, he claims that in the Petersburg game, $e_n = n \log_2 (n)$ makes it fair. The proof is given as follows: First, he introduces truncated variables $U_k$ and $V_k$ . If $n \log_2(n) \ge X_k$ , $U_k=X_k$ , $V_k =0$ . On the other hand, if $n \log_2(n) < X_k$ , $U_k = 0$ , $V_k = X_k$ . Then, it follows  that $$P(|\frac{S_n}{e_n} -1|> \epsilon) \le P(|U_1+ \cdots + U_n - e_n| > e_n \epsilon) + P(V_1 + \cdots + V_n \not=0).$$ The inequality holds since if neither of two events on the right does occur, the event on the left cannot happen. Thus, it is sufficient to prove that two probability on the right converges to $0$ . The author uses the following inequality $$P(V_1 + \cdots + V_n \not= 0) \le n \sum_{i=1}^n P(V_1 \not= 0) \le P(X_1 > n \log_2(n) ) \le \frac{2}{\log_2(n)} \to 0.$$ However, I don't understand how the last inequality holds. I was thinking about using Chebyshev's inequality, or the fact that $P(X_1 > n \log_2(n)) = P(X_1 \ge 2^{r+1})$ assuming that $r$ is the largest integer satisfying $2^{r} \le n \log_2(n)$ . But, I failed both attempts. I would appreciate if you give some help, and please let me know if you need more context.","['proof-explanation', 'law-of-large-numbers', 'probability-theory']"
3724626,$\operatorname{Lie}(G \times H)\cong \operatorname{Lie}(G)\oplus \operatorname{Lie}(H)$,"I am trying to solve an exercise from Lee's Introduction to smooth manifolds book . 8-23. (a) Given Lie algebras $\mathfrak g$ and $\mathfrak h$ , show that the direct sum $\mathfrak g\oplus \mathfrak h$ is a Lie algebra with the bracket defined by $$[(X, Y),(X',Y')]=([X,X'],[Y,Y']).$$ (b) Suppose $G$ and $H$ are Lie groups. Prove that $\operatorname{Lie}(G \times H)$ is isomorphic to $\operatorname{Lie}(G)\oplus \operatorname{Lie}(H)$ The first question I could solve by showing linearity of the lie bracket and the Jacobi identity, using that the jacobi identity is true in $\mathfrak g$ and $\mathfrak h$ . But how can I solve the second point?","['linear-algebra', 'smooth-manifolds', 'lie-groups', 'differential-geometry']"
3724637,"Is there a ""global"" convexity locally around a minimum?","$$\newcommand{\til}{\tilde}$$ Let $F:(0,\infty) \to [0,\infty)$ be a continuous function satisfying $F(1)=0$ , which is strictly increasing on $[1,\infty)$ , and strictly decreasing on $(0,1]$ . Suppose also that $F|_{[1-\epsilon,1+\epsilon]}$ is strictly convex, for some $\epsilon>0$ . Question: Does there exist a $\delta>0$ such that $F$ is convex at every point $y \in (1-\delta,1)$ ? By convexity at $y\,\,$ I mean that for any $x_1,x_2>0, \alpha \in [0,1]$ satisfying $\alpha x_1 + (1- \alpha)x_2 =y$ , $$
F(y)=F\big(\alpha x_1 + (1- \alpha)x_2 \big) \leq \alpha F(x_1) + (1-\alpha)F(x_2). \tag{1}
$$ Equivalently , $F$ admits a supporting line at $y$ , i.e. $\exists m \in \mathbb{R}$ such that $$
F(x) \ge F(y)+m (x-y) \, \, \, \text{ for every } \, \, x \in (0,\infty). \tag{2}
$$ Edit: I have a proof for a positive answer
if $F \in C^1$ , and an incomplete argument for the case where we do not assume $F \in C^1$ . I am interested to know the answer when $F$ is merely continuous. (Feel free to skip over my proofs/arguments below. The question stands as it is.) A (hopefully correct) proof assuming $F \in C^1$ : If there is no such $\delta$ , there exists $y_n \in (0,1)$ , $y_n \to 1$ such that $F$ is not convex at $y_n$ . Thus, $\exists x_n \in (0,\infty)$ such that $$
F(x_n) < F(y_n)+F'(y_n) (x_n-y_n) \tag{3}
$$ The tangent at $y_n$ is below the $x$ -axis for $x>1$ (since $F$ is convex at $(y_n,1]$ it decreases more slowly than its tangent after the tangency point). Thus, $F>0$ is above its tangent which is negative for $x>1$ , which implies $x_n \in (0,1)$ . We may assume that $x_n \to x_0$ ; taking limits on both sides of $(3)$ , we obtain $F(x_0) \le F(1)=0$ , since $|F'(y_n) (x_n-y_n)| \le |F'(y_n)| \to 0$ . This implies $x_0=1$ , so both $x_n,y_n \to 1$ . Looking again at inequality $(3)$ , this contradicts the convexity of $F|_{[1-\epsilon,1+\epsilon]}$ . We have used $F'(y_n) \to F'(1)=0$ . If we do not assume $F \in C^1$ , we need to replace $F'(y_n)$ with slopes of supporting lines $m_n$ , which do not necessarily converge to zero . An incomplete proof without assuming $F \in C^1$ : Assume there is no such $\delta$ . Then $\exists s_n \in [0,1]$ , $s_n \to 1$ such that $F$ is not convex at $s_n$ . Thus $\exists x_n,y_n \in (0,\infty), \alpha_n \in [0,1]$ , $x_n \le s_n \le y_n$ such that $$
s_n=\alpha_n x_n + (1- \alpha_n)y_n, \, \, \text{ and } \, \, 
F\left(s_n \right) > \alpha_n F(x_n) + (1-\alpha_n)F(y_n). \tag{4}
$$ $x_n \le s_n \le 1$ so $x_n$ is bounded. W.L.O.G we may assume that $y_n \le 1$ . Indeed, if $y_n >1$ , we can replace it with $\til y_n=1$ , and choose $\til x_n$ such that $\alpha_n \til x_n + (1- \alpha_n)\til y_n=s_n$ . Then $x_n \le \til x_n \le s_n \le 1=\til y_n \le y_n$ , hence $F(\til x_n) \le F(x_n), F(\til y_n) \le F(y_n)$ . Then inequality $(4)$ holds with $x_n,y_n$ replaced by $\til x_n,\til y_n$ . We now have $x_n \le s_n \le y_n \le 1$ , and $s_n \to 1$ so we may assume that $x_n \to x, y_n \to 1, \alpha_n \to\alpha$ . Taking limits of inequality $(4)$ we get $$
0=F(1) \ge \alpha F(x) + (1-\alpha)F(1) = \alpha F(x)\ge 0,
$$ so $\alpha F(x)=0$ . If $\alpha>0$ , then $F(x)=0$ and $x=1$ , so $x_n,y_n,s_n \to 1$ , thus they eventually lie at $(1-\epsilon,1]$ , which together with inequality $(4)$ contradicts the convexity of $F|_{[1-\epsilon,1+\epsilon]}$ . The problem is that if $\alpha=0$ we cannot deduce that $x=1$ !","['real-analysis', 'calculus', 'solution-verification', 'convex-analysis', 'probability']"
3724658,Does the floor function commute with any other functions?,"Basically what I'm asking is if there are any functions $f: \mathbb{R}\rightarrow \mathbb{R}$ such that \begin{align}\text{floor}(f(x)) = f(\text{floor}(x)),\text{ or } f \circ \text{floor} = \text{floor} \circ f. \end{align} I am, of course, aware of trivial examples like $f(x) = x$ , but I'm wondering if there's a whole class of functions? For example, for $g(x) = x^b$ , any function $f(x) = x^a$ will commute with $g$ in the way stated above; that is, $g\circ f = f\circ g$ for all $x \in \mathbb{R}.$",['functions']
3724688,$\frac{dy}{dx}=\frac{y^3}{e^x +y^2}$,"The curve $f(x,y)=0$ passes through $(0,2)$ and satisfies $$\frac{dy}{dx}=\frac{y^3}{e^x +y^2}$$ The line $x=\ln 5$ intersects the curve at $y=a$ and $y=b$ . Find the value of $\frac{4(a^2+b^2)}{53}$ It is the differential equation that I'm unable to crack. I'm almost sure that it is reducible to a Linear Differential Equation. My futile attempts include: Homogeneous form/reducible to homogeneous form Linear Differential treating $x$ as independent variable I am also toying with the idea that here $y$ could be the independent variable but I'm unable to proceed. Any help would be most welcome.","['calculus', 'ordinary-differential-equations']"
3724702,An expression subtly different from the one in Sherman-Morrison formula,"Let $0\ne x\in\mathbb{R},\,\mathbf{y} \in \mathbb{R}^{n},\,\mathbf{1}=(1,1, \cdots, 1)^{T} \in \mathbb{R}^{n}$ and assume that $\Sigma=x I_{n}+\mathbf{y} \mathbf{1}^{T}+\mathbf{1} \mathbf{y}^{T}$ is positive definite. Prove that $$
\Sigma^{-1}-\frac{\Sigma^{-1} \mathbf{1 1}^{T} \Sigma^{-1}}{\mathbf{1}^{T} \Sigma^{-1} \mathbf{1}}=\frac{1}{x}\left(I_{n}-\frac{1}{n} \mathbf{1 1}^{T}\right).
$$","['matrices', 'linear-algebra', 'positive-definite', 'symmetric-matrices']"
3724710,AF+BG theorem and Cohen Macaulay property,"I tried to solve the following exercise in Vakil's notes Question 1: According to the hint, I should try to show the intersection of affine cones is CM (since one dimensional scheme is CM iff it has no embedded points). This seems very plausible to me for it should be some affine lines glued together at the origin, how can I make this argument precise? Question 2: Then what? If I have another homogeneous polynomial $H$ , I hope to show that it is $0$ in $\mathbb{C}[x_0,x_1,x_2]/(f,g)$ . How does CM (or no embedded point) help me win?","['cohen-macaulay', 'algebraic-geometry']"
3724722,How do level surfaces are defined?,"I am studying the basics of differential geometry and I am focusing on distrributions. Studying from the notes of my professor, I have found that the concept of distribution and its integration is linked to the concept of level functions . So, suppose we have a smooth function $\lambda : \mathbb{R}^{n}\rightarrow \mathbb{R}$ . If we have this, we have that exist a set of points that satisfy: $I_c = \left \{ x\in {\mathbb{R}^{n}} : \lambda (x)=c \right \} $ with $c\in Im(\lambda )$ this define level functions, of dimension $(n-1)$ , but why it defines thes level functions? And why of dimension $n-1$ ? And , are these foliations? If I keep studying, I find an example, in which are considered two independent functions $\lambda _1$ and $\lambda _2$ , so: $rank\begin{pmatrix}
\frac{d\lambda _1}{dx}\\ 
\frac{d\lambda _2}{dx}
\end{pmatrix} =2$ and so it defines a foliation of dimension $(n-2)$ . But why the foliation is of dimension $n-2$ and not of dimension $2$ ? It ocntinues sayin that if we are considering a distribution of constant rank $k$ and the distribution $\Delta $ is involutive, then there exist $n-k$ indepedent functions such that: $\frac{d\lambda _1}{dx}(\tau _1(x)...\tau _k(x))=0$ where $\tau _1(x)...\tau _k(x)$ are vector fields. If it can help better understand the context, I am studying this in order to study the Frobenius theorem . I am really confused about the topics of differential geometry, since personally I find them hard to grasp. If something is wrong or unclear please tell me and I will try to correct. Can somebody please help me?","['dynamical-systems', 'control-theory', 'differential-geometry']"
3724757,Ito process and martingale.,"Suppose $X$ is an Ito process with $K=0$ .
Prove or disprove $X_t^2 - \langle X\rangle_t$ is a martingale. My attempt is: $X_t = X_0 + \int_0^t K_s ds + \int_0^t H_s dW_s$ this is Ito process. And so, If X is Ito process, with K=0, then $X_t = X_0 + \int^t_0 H_s dW_s$ or $dX_t = H_t dW_t$ And also $X_t^2 - \langle X\rangle_t =X_t^2 - dX_t dX_t = X_t^2 - H_t^2 d t$ Well I o order to be martingale, I need to show that $E[  X_t^2 - H_t^2 d t | F_s] =  X_s^2 - H_s^2 d s$ for $s\le t $ . But I cannot show this martingale part. Please help me to do this part. Thanks a lot.","['self-learning', 'probability', 'stochastic-processes', 'calculus', 'stochastic-calculus']"
3724829,"The least distance of $f\in\ell_\infty(K,\mathbb C)$ from $C(K,\mathbb C)$","Suppose that $K$ is a compact Hausdorff space. Consider a bounded function $f:K\to\mathbb R$ not necessarily continuous, that is, $f\in\ell_\infty(K,\mathbb R)$ . It's a well-known fact that the least distance of $f$ from some continuous $g:K\to \mathbb R$ is half of the maximum oscilation of $f$ . More precisely, we define the oscilation of $f$ at $k$ by $$
osc_f(k) = \inf_{U\in\mathcal V_k} \sup_{u,v\in U} |f(u)-f(v)|,
$$ where $\mathcal V_k$ is the set of open neighborhoods of $k$ , and we have the following result, which is the Proposition 1.18.(ii) of the book Geometric Nonlinear Functional Analysis , by Y. Benyaminni and J. Lindenstrauss: Let $f:K\to\mathbb R$ and put $\delta =\|osc_f\|_\infty$ , where $\|\cdot\|_\infty$ is the supremum norm. Then, there exists a continuous function $g:K\to\mathbb R$ such that $$
\|f-g\|_\infty=\frac{\delta}{2},
$$ and this is the least distance of a continuous function from $f$ . So, here is my question: Is there an analogous result for $\mathbb C$ , instead of $\mathbb R$ ? EXAMPLE: Let us see a function $f: K \to \mathbb C$ such that the least distance of this function to a continuous function is not $\delta/2$ . Put $K=[0,1]$ and define for each $k\in[0,1]$ : $$
f(k) = \left\{\begin{array}{rl} 1 &,\mbox{ if $k$ is rational}\\
e^{\frac{\pi i}{3}} &,\mbox{if $k$ is not rational, but is algebraic} \\
e^{-\frac{\pi i}{3}} &, \mbox{if $k$ is transcendental}\end{array}\right.
$$ The image of this function is equal to the vertices of an equilateral triangle centered at the origin of the complex plane. The oscillation of this function in any $k$ is equal to the length of the side of this triangle, that is, $\sqrt3$ . But there is no point whose distance is less than $\sqrt3/2$ to all the vertices of the triangle. The continuous function that has the least distance to $f$ is the zero function $k\mapsto 0$ , and this distance is equal to $1$ .","['complex-analysis', 'functional-analysis', 'analysis']"
3724862,"Why is the vector $(u_x, u_y, - 1)$ normal to the surface $u=u(x, y) $?","Studying the method of characteristics, the argument goes as follows: We are interested in the equation: $a(x, y)u_x+b(x, y) u_y=f(x, y, u)$ ; $(a(x, y), b(x, y), f(x, y, u))(u_x, u_y, - 1) =(a,b,f)\nabla{F}=0$ , where $F =u(x, y) - u=0$ . Hence, $(a, b, f)$ is orthogonal to $\nabla F$ . Then, they say that $\nabla F$ is orthogonal to the solution surface, and so we get that $(a, b, f) $ lies in the tangential plane to the solution surface. Therefore, we can build a characteristic curve, starting from a point on the boundary. We parametrise our unknown characteristic curve as ${x(t), y(t), z(t)} $ , and find the tangent vector to it at every point - $v=(x_t (t), y_t (t), z_t (t)) $ . Then, we find $x, y, z$ from the condition $v=c(a, b, f)$ . This is a system of ODEs, comprising the method of Characteristics. However, I don't understand why they say that $\nabla F=(u_x, u_y, - 1)$ is orthogonal to the solution surface.","['partial-differential-equations', 'vector-analysis', 'differential-geometry']"
3724905,Showing if an étale morphism is finite by looking at the fibers,"Let $f:X\to S$ be an étale morphism of schemes, with $S=\text{Spec}(A)$ , for $A$ a local domain, with residue field $k$ and field of fractions $K$ . Denote by $$\eta:=\text{Spec}(K)\hookrightarrow S \hookleftarrow s:=\text{Spec}(k)$$ the open and closed points. Question: Can we decide that $f$ is finite by just looking at the fibers $X_\eta\to \eta$ and $X_s\to s$ ? I know that $f$ could be not finite, for example it if is an open immersion (e.g. $X=\eta$ ). But in this case the special fiber will be empty. I suspect (but I don't have a reference) that a necessary condition for $X\to S$ to be finite is that $X_{\overline \eta}$ and $X_{\overline s}$ , the base change to the separable closures, have the same number of points (i.e. the degree of $X_\eta\to \eta$ and $X_s\to s$ are equal). Is this true? Is this condition sufficient?","['algebraic-geometry', 'schemes']"
3724930,Sampling from a distribution,"In many cases we use sampling from a distribution. Also in programming languages they implement it. But I wonder now what is the process of generating a sample from a probability distribution? What happens behind the scene that given the parameters a model, a function returns a sample? Also how can I know more on this topic? I want to understand it clearly.","['programming', 'statistics', 'probability-distributions', 'sampling', 'probability']"
3724937,System of differential equations depending on parameter,"This is the first time to get a question like this: How to solve the system $y'=3by+(1-2b)z$ , $z'=by+z+e^{4x}$ , where $b\in\mathbb{R}$ ?
Any help is welcome.","['systems-of-equations', 'derivatives', 'ordinary-differential-equations']"
3724964,"Calculate the following integral $\iint_{T}\frac{x^2\sin(xy)}{y}\,dx\,dy$","Calculate the following integral $$\iint_{T}\frac{x^2\sin(xy)}{y}\,dx\,dy\,,$$ where $$T=\{(x,y)\in\mathbb{R}^2:x^2<y<2x^2,y^2<x<2y^2\}$$ I found the $1/2\leq x\leq 1,1/2\leq y\leq 1$ but i got stuck on how set the limits of the integral","['integration', 'multivariable-calculus', 'calculus', 'multiple-integral', 'fubini-tonelli-theorems']"
3724980,A very interesting question: intersection point of $x^y=y^x$,"I've been investigating the Cartesian graph of $x^y=y^x$ . Obviously, part of the graph is comprised of the line $y=x$ but there is also a curve that is symmetrical about the line $y=x$ . (We can prove this symmetry by noting that the function $x^y=y^x$ is self-inverse; all self-inverse functions are symmetrical about the line $y=x$ .) An image of the graph is shown below: I decided to find the intersection point and arrived at an intriguing result: the intersection point between the two curves is at $(e,e)$ . The following is my method: If the gradient of the line $y=x$ is $1$ , the gradient of the curve at the intersection point must be $-1$ as it's a normal to the line (as it's symmetrical about the line).
This means that at that point $\frac{dy}{dx}=-1$ . Now to find $\frac{dy}{dx}$ . We have $x^y=y^x$ .
I then used a very powerful tehnique for differentiating these sorts of functions. We know that eg $$x^y=e^{\ln{x^y}}=e^{y\ln{x}}$$ and $$\frac{d}{dx}e^{f(x)}=f'(x)e^{f(x)}$$ Applying it to our function $x^y=y^x$ and using implicit differentiation and the product rule gives us: $$(\frac{dy}{dx}\times \ln x +\frac{y}{x})x^y=(\ln y +\frac{dy}{dx}\times \frac{x}{y})y^x$$ So $$\frac{dy}{dx}x^y\ln x +yx^{y-1}=\frac{dy}{dx}xy^{x-1}+y^x \ln y$$ Extensively rearranging gives: $$\frac{dy}{dx}=\frac{y^x \ln y -yx^{y-1}}{x^y \ln x -xy^{x-1}}$$ Let $\frac{dy}{dx}=-1$ : $$y^x \ln y -yx^{y-1}=xy^{x-1}-x^y\ln x$$ But we know $x=y$ since we are at the intersection point with the line $y=x$ : $$x^x \ln x -x^x=x^x-x^x\ln x$$ So $2x^x \ln x -2x^x=0$ $$2x^x(\ln x -1)=0$$ This means either $2x^x=0$ or $\ln x -1=0$ but we know $x^x$ is always greater than $0$ so $\ln x =1$ , leaving us with: $$x=y=e$$ So I have my answer, but is there any other method of getting it? I have heard there is. Any help will be very welcome. Thanks in advance","['exponential-diophantine-equations', 'alternative-proof', 'calculus', 'limits', 'exponential-function']"
3724997,"Given a finite set $X$, which of $\mathcal{P}(X\times X)\times\mathcal{P}(X\times X)$ and $\mathcal{P}({\mathcal{P}(X)})$ has more elements?","This is a problem that I found in the book ""Proofs and Fundamentals"" , by E. Bloch. Problem: Let $X$ be a finite set. Which of the two sets $\mathcal{P}(X \times X) \times \mathcal{P}(X \times X)$ and $\mathcal{P}({\mathcal{P}(X)})$ has more elements? Until now: I already proved that for a finite set $A$ with $a$ elements, $A \times A$ has $a^2$ elements and $\mathcal{P}(A)$ has $2^a$ elements. So, if $X$ has $n$ elements, then by the same argument, $\mathcal{P}(X \times X) \times \mathcal{P}(X \times X)$ has $2^{2n^2}$ and $\mathcal{P}({\mathcal{P}(X)})$ has $2^{2^n}$ . I'm having trouble with showing which one is greater than the other, I tried induction but I didn't know how to finish. I also tried to see, for what values of $n$ , one expression is bigger than the other, but I don't know how to solve that equation. The Point: I would really appreciate if someone could explain how can I deduce which one is greater than the other, and, if possible, to explain other methods that are suitable to solve this problem. Thank you very much for your attention","['elementary-set-theory', 'algebra-precalculus', 'inequality']"
3724999,Prove that the integral $\int_a^b \frac{\sin(x)}{x}dx$ is bounded uniformly.,"How do I show that exists a constant $M>0$ such that, for all $0\leq a \leq b < \infty$ , $$\left|\int_a^b \frac{\sin(x)}{x}dx\right| \leq M.$$ I just read on Richard Bass book's that is enough to prove the uniformly boundeness of the integral $\int_0^b \frac{\sin(x)}{x}dx$ , but I can't see why...","['calculus', 'measure-theory', 'analysis', 'real-analysis']"
3725062,$f(c)\int_{1}^{\infty} f(x) \phi(x) dx =g(c)\int_{1}^{\infty} g(x) \phi(x) dx$,"If $f$ and $g$ are continuous on $[1,\infty)$ and $\phi(x)\geq0$ and be integrable in $[1,\infty) $ . Also $ \int_{1}^{\infty}\phi(x) dx \neq 0$ and $ \int_{1}^{\infty} g(x) \phi(x) dx \neq 0$ then there exists some $c\in(1,\infty)$ such that $$
g(c)\int_1^\infty f(x) \phi(x) dx =
     f(c)\int_1^\infty g(x) \phi(x) dx
$$ MY TRY - Let $$
\frac{\int_1^\infty f(x) \phi(x) dx}
     {\int_1^\infty g(x) \phi(x) dx} = \lambda
$$ so $$
\int_{1}^{\infty}  (f(x)-\lambda g(x)) \phi(x)dx =0
$$ After this i am stuck.","['integration', 'improper-integrals', 'real-analysis', 'calculus', 'general-topology']"
3725131,how to prove the segment $FH=HE$,"Given a right-triangle $\triangle AGC$ $(\angle AGC=90^\circ)$ point $D$ is an arbitrary point on the altitude. $AE=AG$ and $CF=CG$ . How to prove that $FH=HE$ ? I found that circles centered at $A$ and $C$ are orthogonal circles. The altitude of triangle $\triangle AGC$ is the radical axis of the 2 circles and the circle of radius $FH$ is tangent to both (green) circles, but I couldn't prove $FH=HE$ . Please help.","['euclidean-geometry', 'geometry', 'plane-geometry']"
3725159,Orbit Space of Action of $SO(n)$ on $\mathbb{E}^n$,"I'm trying to solve problem 28, chapter 4 of M. A. Armstrong's Basic Topology : Describe the orbits of the natural action of $SO(n)$ on $\mathbb{E}^n$ as a group of linear
transformations, and identify the orbit space. I take the norm function as an identification map and try to use the fact that $\mathbb{E}^n$ with this identification is homeomorphic to $[0,1)$ . I know each orbit of action $SO(n)$ on $\mathbb{E}^n$ is the same as an equivalence class of identification space but I need to show that the identification space and orbit space of acting $SO(n)$ on $\mathbb{E}^n$ are homeomorphic to claim the orbit space is homeomorphic to $[0,1)$ . Can anyone help me to show that?","['general-topology', 'quotient-spaces']"
3725171,Shifting balls in urns that are already occupied by balls,"At the time $n=0$ we place $N$ balls in $k$ urns and change this in each step as follows: We choose one of the balls evenly distributed at random (meaning: each ball is chosen with a probability of $\frac{1}{N}$ ) and place it in an urn that is also randomly selected (meaning: every urn is chosen with probability $\frac{1}{k}$ ). Let $(X_n)_{n\in\mathbb{N}_0}$ be the stochastic process that describes the number of balls in the first urn after $n$ steps and $(\mathcal{F}_n)_{n\in\mathbb{N}_0}$ the natural Filtration of the stochastic process (meaning: $\mathcal{F}_n=\sigma(X_1,\ldots,X_n)$ for all $n\in\mathbb{N}$ ). Try to express $E[X_n|\mathcal{F}_n]$ using $X_n$ . Set $Z_n:=a_nX_n+b_n$ . Find $(a_n)_{n\in\mathbb{N}_0}$ and $(b_n)_{n\in\mathbb{N}_0}$ such that $(Z_n)_{n\in\mathbb{N}_0}$ defines a Martingale. It is mentioned as explicit example for this question I solved here .
So it makes sense that $E[X_n|\mathcal{F}_n]$ will be of the form $E[X_n|\mathcal{F}_n]=u_nY_n+v_n$ for some real-valued sequences $(u_n)_{n\in\mathbb{N}}$ and $(v_n)_{n\in\mathbb{N}}$ .  With that solution in mind Part 2 is quite simple, I just need to find $u_n$ and $v_n$ and the solution follows directly. So let us get to the point of finding $u_n$ and $v_n$ : Surely $P(X_0=m)=(\frac{1}{k})^m\cdot(1-\frac{1}{k})^{N-m}$ for $m\in\{0,1,\ldots,n\}$ . Now the probability of taking out a ball ... ... of the first urn and putting it back into the first urn is $\frac{X_n}{N}\cdot\frac{1}{k}$ . ... of the first urn and putting it into one of the other urns is $\frac{X_n}{N}\cdot(1-\frac{1}{k})$ . ... not form the first urn and putting it in the first urn is $(1-\frac{X_n}{N})\cdot\frac{1}{k}$ . ... not form the first urn and putting it not in the first urn is $(1-\frac{X_n}{N})\cdot(1-\frac{X_n}{N})$ . This brings us to the conclusion that: $$\begin{cases}
 & P(X_{n+1}=X_n)=(\frac{X_n}{N}\cdot\frac{1}{k})+(1-\frac{X_n}{N})\cdot(1-\frac{X_n}{N})\\ 
 & P(X_{n+1}=X_n-1)=\frac{X_n}{N}\cdot(1-\frac{1}{k})\\ 
 & P(X_{n+1}=X_n+1)=(1-\frac{X_n}{N})\cdot\frac{1}{k} 
\end{cases}$$ Now I am struggling to define $P(X_n=m)$ in general (do i even need it?). Somehow I think that I am on a wrong track. Any assistance or thoughts how to properly approach this problem would be much appreciated. Edit: After correcting my mistakes i get the desired form I mentioned at the beginning: $$\begin{align}E[X_{n+1}|\mathcal{F}_n]&=(X_{n}+1)P(X_{n+1}=X_n+1)+X_{n}P(X_{n+1}=X_n)\\&\hspace{5em}+(X_{n}-1)P(X_{n+1}=X_n-1)\\&=X_n\underbrace{[P(X_{n+1}=X_n+1) + P(X_{n+1}=X_n) + P(X_{n+1}=X_n-1)]}_{\substack{=1}} \\&\hspace{5em}+ P(X_{n+1}=X_n+1) - P(X_{n+1}=X_n-1) \\&= X_n +  P(X_{n+1}=X_n+1) - P(X_{n+1}=X_n-1)\\&=\underbrace{\frac{1}{Nk}(Nk+k-2)}_{\substack{=:u_n}}\cdot X_n+\underbrace{\frac{1}{k}}_{\substack{=:v_n}}\end{align}$$ ... which means that Part 2 follows with the statement here .","['conditional-expectation', 'martingales', 'probability-theory', 'filtrations']"
3725177,"Does $\iint_{\mathbb{R}^2}\frac{\sin(x^2+y^2)}{x^2+y^2}\,dx\,dy$ converge?","Consider $$\iint_{\mathbb{R}^2}\frac{\sin(x^2+y^2)}{x^2+y^2}\,dx\,dy$$ My try: changing to polar coordinates and then calculate the integral $$\int_{0}^{2\pi}\int_{0}^{\infty}\frac{\sin(r^2)}{r^2}\,dr\,d\theta\underset{r^2=u}{=}\pi\int_{0}^{\infty}\frac{\sin(u)}{u}\,du,$$ which converges but I saw in the solutions of a test that this integral diverges. Was I wrong?","['integration', 'multivariable-calculus', 'calculus', 'multiple-integral', 'fubini-tonelli-theorems']"
3725206,Definition of Lebesgue measurable functions- Why Borel sets? [duplicate],"This question already has answers here : Why this definition for Lebesgue measurable functions? (2 answers) Closed 4 years ago . I write $\mathcal{M}_{Bor}, \mathcal{M}_{Leb}$ for the Borel/ Lebesgue $\sigma$ - algebras on $\mathbb{R}$ . Let $A \in \mathcal{M}_{Leb}$ . Let $f: A\to \mathbb{R}$ . Then $f$ is Lebesgue- measurable iff $ \forall B \in \mathcal{M}_{Bor}: f^{-1}(B) \in \mathcal{M}_{Leb}$ . What goes wrong if we instead require $ \forall E \in \mathcal{M}_{Leb}: f^{-1}(E) \in \mathcal{M}_{Leb}$ , for $f$ to be measurable? Superficially this definition seems more natural, so why isn't it used?","['measure-theory', 'definition', 'lebesgue-measure']"
3725247,Book for The Lyapunov Function and topologically equivalent linear systems,"I encountered the concept of the Lyapunov Function from the book Ordinary Differential Equations by Arnold . The book states the following lemma: Let $A:\mathbb{R}^n \to \mathbb{R}^n$ be a linear operator, all of whose eigenvalues have positive real part. Then the system $$
\dot{x} = Ax, \quad x \in \mathbb{R}^n
$$ is topologically equivalent to the standard system $$
\dot{x} = x, \quad x \in \mathbb{R}^n
$$ The author states that the proof of this lemma is based on the construction of a special quadratic function, the so-called Lyapunov function. However, when I searched for the term Lyapunove Function, it seems that it is close related to a theory about equilibrium point, which is not like what this book described. I then think that I need more referrences to understand this topic. Any recommendations?","['lyapunov-functions', 'book-recommendation', 'ordinary-differential-equations', 'reference-request']"
3725260,Periodic center implies periodic in a nilpotent group,"Some background before I get into the problem - I've been out of school for a little while, but decided to head back to pursue a  doctorate in math. In the meantime, I've been going back through some old homework to try and brush up on some of the theory. For this particular course, I no longer have my lecture notes, and there are no doubt gaps in my knowledge that have grown over the years. From memory, we loosely followed Rotman's group theory text - as such my notation follows his (stuff operates on the left e.g. $x^y = yxy^{-1}$ , $[x,y] = xx^y$ , etc.). Problem statement: Prove that if $G$ is nilpotent and $Z(G)$ is periodic then $G$ is periodic. My solution idea: Lemma 1 : In any group $G$ , if $Z(G)$ is periodic then $Z_2(G)$ is periodic. Proof : Suppose $x \in G$ , $y \in Z_2(G)$ . Then $[x,y] \in Z(G)$ . Let $n = |[x,y]|$ . Then $[x,y]^n = 1 = [x,y^n]$ . Hence $y^n \in Z(G)$ . By hypothesis, there exists $m \in \mathbb{N}$ such that $(y^n)^m = 1$ so that $|y| \leq mn.$ $\square$ EDIT:: Lemma 2 : If $G$ is nilpotent of class $c+1$ , then $\frac{G}{Z(G)} = \bar{G}$ is nilpotent of class $m \leq c.$ Proof : For each $i \in \mathbb{N}$ , define $\bar{Z_i} = \frac{Z_{i+1}}{Z_1}$ , where $Z_1 = Z(G).$ By the correspondence theorem, $\bar{Z_i} \triangleleft \bar{G}$ . Moreover, $\frac{\bar{Z}_{i+1}}{\bar{Z_i}} \cong \frac{Z_{i+2}}{Z_{i+1}} = Z(\frac{G}{Z_{i+1}}) \cong Z(\frac{\bar{G}}{\bar{Z_i}}).$ Hence $\frac{\bar{Z}_{i+1}}{\bar{Z}_i} \leq \frac{\bar{G}}{\bar{Z}_i}.$ Therefore, $1 = \bar{Z}_0 \triangleleft \bar{Z}_1 \triangleleft  ... \triangleleft \bar{Z}_c = \bar{G}$ is a central series of $\frac{G}{Z_1}.$ Since $\bar{Z}_{c-1} \neq \bar{G}$ , we deduce that this series is has length $c$ . Thus $\bar{G}$ is nilpotent of class $m \leq c.$ $\square$ Returning to the main problem, we proceed by induction on the nilpotence class of $G$ . If $G$ has class 1, then $Z(G) = G$ and the result holds. Suppose now that the result holds for all groups of class $c$ . Then if $G$ has class $c+1$ , we have shown that $\bar{G}$ has class $c$ . Now $Z(\bar{G}) = \frac{Z_2}{Z_1}$ . By Lemma 1, $Z_2$ is periodic, so that $Z(\bar{G})$ is periodic. By induction, $\bar{G}$ is periodic. Thus if $g \in G$ , there exists $n \in \mathbb{N}$ such that $g^n \in Z(G).$ But then there exists $m \in \mathbb{N}$ such that $(g^n)^m = 1$ meaning $|g| \leq mn.$ Therefore, by induction, every nilpotent group is periodic. $\square$ EDIT: I updated Lemma 2 with a fix. Is my proof sound now?",['group-theory']
3725283,How can I integrate over the product of two multivariate normal distributions?,"Suppose that $\mathbf{y} \sim N(\mathbf{n},\sigma^2\mathbf{I})$ and $\mathbf{n} \sim N(\boldsymbol{\mu},\boldsymbol{\Sigma})$ . I want to integrate the following: $$\int [\mathbf{y}\mid\mathbf{n},\sigma^2][\mathbf{n} \mid 
\boldsymbol{\mu},\boldsymbol{\Sigma}] \, d\mathbf{n},$$ where $[\cdot]$ indicates a probability distribution. I take it that $\mathbf{y}$ and $\mathbf{n}$ have multivariate normal distributions, so I really want to solve the integral: $$\int (2\pi)^{-\frac{1}{2}(k+k)} \det(\sigma^2\mathbf{I})^{-1/2} \det(\boldsymbol{\Sigma})^{-1/2} \exp\left[-\frac{1}{2}((\mathbf{y}-\mathbf{n})^T (\sigma^2\mathbf{I})^{-1}(\mathbf{y}-\mathbf{n}) + (\mathbf{n}-\boldsymbol{\mu})^T(\boldsymbol{\Sigma})^{-1}(\mathbf{n}-\boldsymbol{\mu})\right] \, d\mathbf{n},$$ where $k$ is the dimension of y and of n . However, I don't know what to do next. Is there some standard trick that mathematicians use to complete this integration? I take it that the result is a multivariate normal. Thank you.","['integration', 'probability-distributions', 'normal-distribution', 'multivariate-polynomial']"
3725296,Geometric approach to $\lim_{n\to\infty}\left(\frac{x_{n+1}}{x_n}\right)^n$ where $x_1=1$ and $x_{n+1}=\sqrt{1+x^2_n}$?,"I was solving a question : Let $x_1=1$ and $x_{n+1} = \sqrt{1+x^2_n } \ \ \forall \ \ n\in \mathbb{N}$ Then evaluate $$\lim_{n \to \infty} \left( \frac{x_{n+1}}{x_n} \right)^n$$ The way I did it was : $$x_1^2=1$$ $$x_2^2=2$$ $$x_3^2=3$$ $$x_4^2=4$$ $$x_{n+1}^2=(n+1)$$ $$\therefore \lim_{n \to \infty} \left( \frac{x_{n+1}^2}{x_n^2} \right)^\frac{n}{2}$$ $$A = \lim_{n \to \infty} \left( \frac{n+1}{n} \right)^{\frac{n}{2}}$$ $$\ln(A) = \lim_{n \to \infty} \frac{1}{2} \frac{\ln\left( 1+ \frac{1}{n} \right)}{\frac{1}{n}} $$ Applying L'Hopital's rule : $$\ln(A) = \frac{1}{2}$$ $$A = \sqrt{e}$$ But what I think is that this could also be solved using a geometric approach. I believe so because the recurrence relation given is similar to the Pythagoras Theorem. $$x_{n+1}^2 = 1^2 + x_n^2$$ I tried to draw the diagram for something like this : However all I could conclude was the indeterminate form the was forming : For the the limit is of the secant of the base angle of the triangle ( $(\sec \alpha)^n$ ) with $x_n$ as the base. i.e. the limit becomes : $$\lim_{n \to \infty} \sec^n (\alpha_n)$$ which I can see that as $n$ approaches $\infty$ , $\alpha_n$ will keep decreasing until we can say $x_n \approx x_{n+1}$ , so it is a $(1)^\infty$ form. So my question is : How would one prove the above question using geometry?","['limits', 'calculus', 'geometry']"
3725301,"$\mid Df(u)\cdot w\mid \leq L \mid w\,\mid$","Suppose $f:C\to \mathbb{R}^n$ is a $C^1$ -function, where $C$ is a compact subset of $\mathbb{R}^m$ . I want to show that there exists an $L\in\mathbb{R}_{>0}$ such that $$\mid Df(u)\cdot w\mid\leq L\mid w\,\mid$$ for all $u\in C$ and $w\in \mathbb{R}^m$ , where $D$ denotes the Jacobian of $f$ at $u$ . I have already shown this using the extreme value theorem and the Heine-Borel-theorem, but my proof is too long for my purposes. I am looking for a proof, which is as short as possible. Could that be a well known fact?",['multivariable-calculus']
3725336,"Showing $X_n\sim \operatorname{Bin}\left(1,\frac{1}{n}\right)$ almost surely does not converge to $0$","I want to show that $ X_n\sim \operatorname{Bin}\left(1,\frac{1}{n}\right)$ almost surely does not converge to $0$ ; $X_n$ 's are independent. Therefore I got the hint to use Borel-Cantelli and showed that $\sum_{n=1}^{\infty}P(X_n=1)=\sum_{n=1}^{\infty}\frac{1}{n}=\infty$ and so $P(\limsup\limits_{x\rightarrow\infty}X_n)=1$ . Can I now follow that it is not possible that $X_n$ converges almost sure to $0$ because the probability that infinity $X_n=1$ happens is $1$ ?
Maybe you can help me with that","['convergence-divergence', 'probability-distributions', 'probability-theory', 'probability']"
3725338,An $n\times n$ matrix that has exactly one $1$ and one $-1$ in each row and column and others are $0$,"I came across the following Question Assume an $n\times n$ matrix that has exactly one $1$ and one $-1$ in
each row and column and others are $0$ . Prove that there is a way that
we can change the places of rows and columns in which it gives the
negative of the matrix. MY TRY- Call such matrix A.
All we need to do is to find some permutation matrices $P_{1}$ and $P_{2}$ such that $$P_{1}AP_{2} = -A$$ $A$ can be written as a difference of two permutation matrices i.e. $$A = P-Q$$ where P and Q are some permutation matrices Example of one such matrix of order $3\times3$ $$ \begin{pmatrix}
1 & 0 & -1 \\
-1 & 1 & 0 \\
0 & -1 & 1
\end{pmatrix} = \begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
 0 & 0 & 1
\end{pmatrix}-\begin{pmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
 0 & 1 & 0
\end{pmatrix}.$$ We could first turn every such matrix $A$ by multiplying
by appropriate permutation matrices to the form $I-R$ :- $$P^{T}A = P^{T}(P-Q) = I-R$$ Clearly the permutation matrix R shouldn't have $1$ at the same position as in $I$ . R lies in the class of traceless permutation matrices . Now If we are able to find matrices permutation $P_{1}$ and $P_{2}$ such that $$P_{1}(I-R)P_{2} = (R-I) = -(I-R)$$ we'll have $$P_{1}P^{T}AP_{2} = -P^{T}A \implies PP_{1}P^{T}AP_{2} = -A $$ and we would be done. But how could I proceed now to find $P_{1}$ and $P_{2}$ ? Would we need some extra equation from the fact that $R$ is a traceless permutation matrix? It was great to see other approaches to solve the problem by Michael Hoppe and user1551. But I am curious to see how would it be if we go this way?","['permutations', 'permutation-matrices', 'trace', 'matrices', 'linear-algebra']"
3725471,Proof that indicator function is random variable (Borel-measurable),"I realize this highly trivial but it's precisely why I decided to ask it. Also my explanation doesn't quite match the correct one. On a standard probability triple $(\Omega, \mathcal{F}, P)$ we define a function $$
X({\omega}) = 
\Bigg\{
\begin{array}{lr}
1 & \text{ if }  A \in \mathcal{F}\\
0 & otherwise
\end{array}
$$ We need to find $\{\omega: X(\omega) = 1\} \Leftrightarrow X^{-1}(\{1 \}) = A$ and obviously $A \in \mathcal{F}$ by construction of $X$ , so $X$ is a Borel-measurable random variable. I'm sure my solution is solid, but it's a bit different from the correct one. Also it seems somewhat trivial.","['borel-measures', 'solution-verification', 'probability-theory', 'random-variables']"
3725506,"Find all sequences $x_1,x_2,\dots,x_n$ of distinct positive integers such that $\frac{1}{2}=\frac{1}{x_1^2}+\frac{1}{x_2^2}+\dots+\frac{1}{x_n^2}$","Find all sequences $x_1,x_2,\dots,x_n$ of distinct positive integers such that $$\frac{1}{2}=\frac{1}{x_1^2}+\frac{1}{x_2^2}+\dots+\frac{1}{x_n^2}\tag{1}.$$ This is 3rd problem from 1st day of 16-th Hungary–Israel Binational Mathematical Competition 2005 . How to solve this? I couldn't find official solution, only couple of posts on AoPS with no complete solution, such as aops1 and aops2 . Some examples of $(x_1,x_2,\dots,x_n)$ found there are $$
(2,3,4,5,7,12,15,20,28,35),\\
(2,3,4,6,7,9,12,14,21,36,45,60),\\
(2,3,4,5,8,10,15,20,24,30,40,60,120)
$$ Since $\sum_{i=3}^{\infty} \frac{1}{i^2}<\frac{1}{2}$ , we know $\frac{1}{2^2}$ will be always in the sum. Also multiplying out $(1)$ to get rid of fractions and inspecting modulo $x_i^2$ , we see $2(x_1\cdots x_{i-1}x_{i+1}\cdots x_n)^2 \equiv 0 \pmod{x_i^2}$ , so divisors of each of $x_i$ must ""split"" among other $x_j$ 's (and $2$ ), but I don't think that helps much. Edit: As noted by WhatsUp in comments, there is relevant problem in Project Euler. There is actually also one that is almost identical, but with limited search range, specifically https://projecteuler.net/problem=152 . So it might help to think about this problem in similar manner, letting $x_n \leq M$ and then characterize solutions with these constrains. Caution, the following spoils the above mentioned Project Euler problem. We can convert this to more computationally manageable diophantine equation by multiplying $(1)$ by $2\text{lcm}(x_1,x_2,\dots,x_n)^2$ instead of $2(x_1x_2\cdots x_n)^2$ . For example considering $\text{lcm}(2,3,4,5,7,12,15,20,28,35)=420$ , first example can be written as $$420^2=2(210^2+140^2+105^2+84^2+60^2+35^2+28^2+21^2+15^2+12^2).$$ Then by the construction, all the squares on the right are divisors of the square on the left. Also, square on the left is just square of one of divisors of $\text{lcm} (2,3,\dots,M)$ , if we have $M \geq 35$ . This leads to following reformulation of the problem: Find all sequences $y_1,y_2,\dots,y_n$ of distinct positive integers such that $$m^2=2(y_1^2+y_2^2+\dots+y_n^2)\tag{2},$$ where $m=\text{lcm}(y_1,y_2,\dots,y_n)$ . So in principle, for fixed $M$ and $x_i \leq M$ , we can find $\text{lcm}(2,3,\dots,M)$ , and then enumerate its divisors effectively using prime factorization (using that its prime factors will be $p \leq M$ ). Then additionally, we can notice that not all primes $p$ can be present in these numbers. Indeed, if we have $p> M/2$ , then by earlier observation, it would have to divide at least two $x_i$ 's. Smallest of them can be $x_i=p$ , with next smallest $x_j=2p>M$ , impossible. In similar way, lots of other primes can be ruled out for $M=80$ , by realizing they would require to be split among at least $3$ of $x_i$ 's (try $37$ ). While these ideas allow us to significantly speed up the search, it is still hard to say which factorizations will satisfy $(2)$ without actually checking the values...","['contest-math', 'number-theory', 'elementary-number-theory', 'diophantine-equations']"
3725525,Why is $P(a \text{ and } b)$ maximized when $P(a \text{ or } b)$ is minimized?,"I can't seem to wrap my head around why $P(a \text{ and } b)$ is minimized when $P(a \text{ or } b)$ is maximized. This comes from PIE: $$P(a \text{ or } b) = P(A) + P(B) - P(a \text{ and } b).$$ Can someone please explain the intuition behind this? I'm even trying to picture the Venn diagram in my head, but this exact relationship doesn't make sense.",['probability']
3725640,"Proof that $f$ is Lebesgue-integrable in $[0,1]$.","Let $f:[0,1]\to\mathbb{R}$ be a non-negative function. For all $\epsilon\in(0,1]\,$ , let $f$ be Riemann-integrable in $[\epsilon,1]$ . Show that $f\in L_{1}[0,1]\,$ iff $\,\,\lim_{\,\epsilon \to 0}\int_{\epsilon}^{1}f(x)\,dx\,$ exists. Moreover, in that case $$\int_{[0,1]}f\,d\lambda=\lim_{\,\epsilon \to 0}\,\int_{\epsilon}^{1}f(x)\,dx$$ My attempt: First, I want to show that $f$ is a measurable function. Since $f$ is Riemann-integrable in $[\epsilon,1]$ , it is also Lebesgue-integrable and, in particular, it is measurable. Letting $\epsilon\to 0$ , $f$ is a measurable function in $(0,1]$ . Now, given any $t\in\mathbb{R}$ , define the set $A$ by $$A:= \{x\in[0,1] : f(x) \ge t  \}$$ We want to show that $A$ is a measurable set. Let $A_{1}:=A\,\cap\{0\}$ and $A_{2}:=A\cap (0,1]$ . Since $f$ is measurable in $(0,1]$ , $A_{2}$ is measurable. Now, the set $A_{1}$ will be empty or $A_{1}=\{0\}$ , and in both cases is a measurable set. Hence, $A=A_{1}\cup A_{2}$ is measurable. So, $f$ is a measurable function in $[0,1]$ . Now, for all $n\ge 1$ , let $E_{n}:=[1/n,1]$ . Note that $E_{n} \subseteq E_{n+1}$ and $\bigcup E_{n}=(0,1]$ . Further, since $f$ is non-negative, $(\int_{1/n}^{1} f(x)dx)_{n\ge 1}$ is an increasing sequence. So, it will converge iff it is bounded above. The claim then follows from $$\lim_{\,\epsilon \to 0}\,\int_{\epsilon}^{1}f(x)\,dx=\lim_{\,n \to \infty}\,\int_{1/n}^{1}f(x)\,dx=\lim_{\,n \to \infty}\,\int_{E_{n}} f\,d\lambda=\int_{(0,1]} f\,d\lambda=\int_{[0,1]} f\,d\lambda< +\infty$$ Did I get this right?","['measure-theory', 'lebesgue-integral']"
3725652,Computing derivatives? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $u: \Bbb R^2\to\Bbb R$ and define $f(t)=u(\sin(t),\cos(t))$ How may I calculate $f'(t)$ ? Note: I am expecting the answer to include symbols like ${\partial u}/ {\partial x}$ Is there a law for such kind of derivatives?","['partial-derivative', 'calculus', 'derivatives']"
3725661,Is the probability of getting at least the average number of heads monotone?,"Let $f(n,k)$ be the probability of getting heads at least $n$ times with $nk$ coins that independently show heads with probability $1/k$ . For $k$ fixed, is $f(n,k)$ monotone in $n$ ? With the central limit theorem it is easy to show that $f(n,k)$ converges to $1/2$ as $n\to\infty$ . By experimentation and intuition I expect this convergence to be monotone, but I can not come up with a proof.","['central-limit-theorem', 'probability']"
3725674,"If $R$ is a reduced Noetherian ring, then every prime ideal in the total quotient ring $K(R)$ is maximal.","I know that in $K(R)$ , the set of maximal ideals is the set of associated primes of $K(R)$ and that an ideal is maximal if and only if it is the localization of a maximal associated prime of $R$ . So, we know there are only finitely many maximal ideals in $K(R)$ . I'm not sure if that is helpful, but, We want to show that if $R$ is a reduced Noetherian ring, then every prime ideal in $K(R)$ is in fact maximal. I'm not sure how to use assumption that $R$ is reduced. All I know is that this means that there are no nilpotent elements in $R$ .","['ring-theory', 'abstract-algebra', 'commutative-algebra', 'noetherian']"
3725732,Mistake in computing the evolution of $\frac{\partial}{\partial t} R_{ik}$ - where did the term $-2g^{jp}g^{lq}R_{pq}R_{ijkl}$ go? Why did it vanish?,"I'm trying to prove that under the Ricci flow, the Ricci tensor evolves by the following equation: $$\frac{\partial}{\partial t} R_{i k}=\Delta R_{i k}+2 g^{p q} g^{r s} R_{p i k r} R_{q s}-2 g^{p q} R_{i p} R_{q k}$$ This is corollary $4.18$ of Ben Andrew's ""The Ricci Flow in Riemannian Geometry"" book. Now, the proof they give there starts like this: Where $(4.9)$ is given by: $$\frac{\partial}{\partial t} g^{i j}=-g^{i k} g^{j \ell} h_{k \ell}$$ and Theorem $4.14$ is: $$\begin{aligned}
\frac{\partial}{\partial t} R_{i j k \ell}=& \Delta R_{i j k \ell}+2\left(B_{i j k \ell}-B_{i j \ell k}-B_{i \ell j k}+B_{i k j \ell}\right) \\
&-g^{p q}\left(R_{p j k \ell} R_{q i}+R_{i p k \ell} R_{q j}+R_{i j k p} R_{q \ell}+R_{i j p \ell} R_{q k}\right)
\end{aligned}$$ where $$B_{i j k \ell}=g^{p r} g^{q s} R_{p i q j} R_{r k s \ell}=R_{i j}^{p q} R_{p k q \ell}$$ Now, I see where all the terms in my first picture (with the red outlined term) are coming from. I understand they ought to be here. But the term $-2g^{jp}g^{lq}R_{pq}R_{ijkl}$ outlined in red just vanished! And the authors never mention it again. What happened here? I've been staring at this for a long while now but can't figure this out. I would be really grateful for any help. I think this is probably a mistake on my part, because in Hamilton's $1982$ original paper, the same ""mistake"" is there, so I think there must be something going on here that I'm not seeing. Thanks in advance! UPDATE: I think a mistake was indeed committed by the authors, but overall the work is correct. First of all, the term I outlined in red has the wrong sign, it should be ${+2g^{jp}g^{lq}R_{pq}R_{ijkl}}$ rather than $\color{red}{-2g^{jp}g^{lq}R_{pq}R_{ijkl}}$ , as one can easily check (and in Hamilton's original paper the sign is correct). Secondly, the authors later compute that: $$\begin{array}{l}
2 g^{j \ell}\left(B_{i j k \ell}-B_{i j \ell k}-B_{i \ell j k}+B_{i k j \ell}\right) \\
\quad=2 g^{j \ell} B_{i j k \ell}-2 g^{j \ell}\left(B_{i \ell j k}+B_{i j \ell k}\right)+2 g^{p r} g^{q s} R_{p i q k} R_{r s} \\
\quad=2 g^{j \ell} B_{i j k \ell}-4 g^{j \ell} B_{i j \ell k}+2 g^{p r} g^{q s} R_{p i q k} R_{r s} \\
\quad=2 g^{j \ell}\left(B_{i j k \ell}-2 B_{i j \ell k}\right)+2 g^{p r} g^{q s} R_{p i q k} R_{r s}
\end{array}$$ and $$\begin{array}{l}
g^{j \ell} g^{p q}\left(R_{p j k \ell} R_{q i}+R_{i p k \ell} R_{q j}+R_{i j p \ell} R_{q k}+R_{i j k p} R_{q \ell}\right) \\
\quad=2 g^{p q} R_{p i} R_{q k}+g^{j \ell} g^{p q} R_{i p k \ell} R_{q j}+g^{j \ell} g^{p q} R_{i j k p} R_{q \ell} \\
\quad=2 g^{p q} R_{p i} R_{q k}+2 g^{p r} g^{q s} R_{p i q k} R_{r s}
\end{array}$$ Therefore, now taking into account the term I said was missing, we have: $$\begin{aligned} \frac{\partial}{\partial t} R_{i k}&=\Delta R_{i k}+2 g^{j \ell}\left(B_{i j k \ell}-2 B_{i j \ell k}\right)+2 g^{p r} g^{q s} R_{p i q k} R_{r s}-2 g^{p q} R_{p i} R_{q k} - 2 g^{p r} g^{q s} R_{p i q k} R_{r s} 
  \color{red}{ +2g^{jp}g^{lq}R_{pq}R_{ijkl}}\\ &=  \Delta R_{i k}+2 g^{j \ell}\left(B_{i j k \ell}-2 B_{i j \ell k}\right) \color{red}{ +2g^{jp}g^{lq}R_{pq}R_{ijkl}} -2 g^{p q} R_{p i} R_{q k} \end{aligned}$$ So all we have to do now is prove that: $$2g^{jp}g^{lq}R_{pq}R_{ijkl} =  2 g^{p r} g^{q s} R_{p i q k} R_{r s}$$ Which is indeed true, since: $$g^{p r} g^{q s} R_{p i q k} R_{r s} = R_{rs} R((e^r)^{\#}, e_i, (e^s)^{\#}, e_k) = R_{pq} R((e^p)^{\#}, e_i, (e^q)^{\#}, e_k)$$ and $$g^{jp}g^{lq} R_{pq} R_{ijkl} = R_{pq} R_(e_i, (e^p)^{\#}, e_k, (e^q)^{\#}) = R_{pq} R((e^p)^{\#}, e_i, (e^q)^{\#}, e_k)$$","['riemannian-geometry', 'ricci-flow', 'tensors', 'manifolds', 'differential-geometry']"
3725761,"Distance from one vertex of a rectangular prism to the plane determined by three other vertices, without using vectors or calculus","The objective is to find the shortest distance from the point $H$ to the plane $BDE$ . The prism $ABCD.EFGH$ has $AB=AD=5\sqrt{2}$ and $AE=12$ . I think that these numbers are badly selected by the author. The following shows my steps to solution but I feel it is too verbose, tedious, and time consuming. It can be solved easily with either vector (dot and cross products) or calculus (minimizing a distance function of two variables), but because this topic is for students who have not learned those subjects, I insist on only using not more than Pythagorean theorem and basic trigonometry. Analytic approach is not allowed! Finding $DE$ , $EP$ and $HP$ \begin{align*}
DE^2
&=DH^2+EH^2\\
&=12^2 + (5\sqrt2)^2 \\
&=194\\
DE
&=\sqrt{194}
\end{align*} \begin{align*}
\frac{1}{HP^2}
&=\frac{1}{DH^2}+\frac{1}{EH^2}\\
&=\frac{1}{122^2}+\frac{1}{(5\sqrt2)^2}\\
&=\frac{97}{3600}\\
HP
&=\frac{60}{\sqrt{97}}
\end{align*} \begin{align*}
DE \times EP &=EH^2\\
EP\sqrt{194}  &= (5\sqrt2)^2 \\
EP &=\frac{50}{\sqrt{194}}
\end{align*} Finding $BD$ , $\cos E$ , $EQ$ and $PQ$ \begin{align*}
BD 
&= \sqrt{CD^2+BC^2}\\
&= \sqrt{(5\sqrt2)^2+(5\sqrt2)^2}\\
&= \sqrt{50+50}\\
&= 10
\end{align*} \begin{gather*}
BE^2+DE^2-2\times BE\times DE \cos E = BD^2 \\
194 + 194 - 2\times 194 \cos E = 100\\
\cos E = \frac{72}{97}
\end{gather*} \begin{align*}
EQ 
&= EP \sec E\\
&= \frac{50}{\sqrt{194}}\times \frac{97}{72} \\
&= \frac{2425}{36\sqrt{194}}
\end{align*} \begin{align*}
PQ 
&= \sqrt{EQ^2-EP^2}\\
&= \sqrt{\left(\frac{2425}{36\sqrt{194}}\right)^2-\left(\frac{50}{\sqrt{194}}\right)^2}\\
&= \frac{1625}{36\sqrt{194}}
\end{align*} Finding $HQ$ \begin{align*}
HQ
&= \sqrt{EQ^2+EH^2}\\
&= \sqrt{\left(\frac{2425}{36\sqrt{194}}\right)^2 +\left(5\sqrt{2}\right)^2}\\
&= \frac{5\sqrt{15218}}{72}
\end{align*} Finding the altitude, of $\triangle HPQ$ , passing through $P$ and finding $HH'$ As the badly chosen numbers make the calculation a bit complicated. The process is left behind as your excercise. The altitude is $t = 1500/\sqrt{738073}$ . $HH'$ then can be found by equating the area from two different bases. \begin{align*}
PQ \times HH' &= HQ \times  t\\
\frac{1625}{36\sqrt{194}} \times HH' &= \frac{5\sqrt{15218}}{72} \times \frac{1500}{\sqrt{738073}}\\
HH' &= \frac{60}{13}
\end{align*} Thus the shortest distance from the point $H$ to the plane $BDE$ is $\tfrac{60}{13}$ . Question Is there any shorter way to solve it but with neither using vector nor using calculus?","['euclidean-geometry', 'geometry']"
3725798,Transitive Relations and functions,"A function by definition is a relation in which no two ordered pairs have the same first element, and every element in domain has an image in codomain.
A relaton $R$ is transitive if for all values $a, b, c$ : $a R b$ and $b R c$ implies $a R c$ .
So does it mean a function can never be transitive ?","['equivalence-relations', 'functions', 'relations']"
3725799,When to use each of the three double-angle identities for cosine?,"There are three double angle identities that are all equivalent to each other. The concept of the equations being equivalent sounds fair to me, except I noticed each one has a specific time when to be used precisely with the problem. How will I know when one of the three should be used according to the question? I noticed it does matter at times, but maybe I do not understand the concept as a whole, does anyone know which instances I should choose either, other, or? Perhaps I am mistaken, but if so, could someone clarify why? Thank you",['trigonometry']
3725958,Implicit differentiation: differential vs derivative,"When I search implicit differentiation for equation $x^2 + y^2 = r^2$ I find results of two versions: one using derivative and the other using differential. Version1: $\frac{d }{dx}(x^2 + y^2 = r^2) \Leftrightarrow 2x + 2y\frac{dy}{dx} = 0 $ Version2: $d(x^2 + y^2 = r^2) \Leftrightarrow 2xdx + 2ydy = 0 $ Using both methods, I can derive the result: $\frac{dy}{dx} = -\frac{x}{y}$ However, I am confused, could you please provide some answers to: Which one (derivative /differential) is the ""real"" implicit differentiation? What are the differences between using these two methods? When should differential be used rather than derivative?","['differential', 'calculus', 'derivatives']"
3725969,Real $2n$-plane bundle with a complex structure is a complex $n$-plane bundle,"I am trying to show that if $\xi=(E,B,\pi)$ is a real $2n$ -plane bundle with a complex structure $J:E\to E$ then $\xi$ becomes a complex $n$ -plane bundle if we define $(x+iy)v=xv+yJ(v)$ on each fiber. I only need to show that $\xi$ is locally trivial of complex sense. Suppose $h:\pi^{-1}(U)\to U\times\Bbb R^{2n}$ is a local trivialization. Identifying $\Bbb R^{2n}=\Bbb C^n$ by $(x_1,y_1,\dots,x_n,y_n)=(x_1+iy_1,\dots,x_n+iy_n)$ , we can view $h$ as a map $h:\pi^{-1}(U)\to U\times \Bbb C^n$ , and it remains to show that $h$ is fiberwise $\Bbb C$ -linear. To see this, it suffices to show that $h(J(v))=i\cdot h(v)$ where $v\in F_b$ and $F_b=\pi^{-1}(b)$ . But I can't understand why does this hold. (I'm reading the book Characteristic Classes of Milnor) Any hints?","['vector-bundles', 'almost-complex', 'characteristic-classes', 'differential-geometry']"
3726033,Why is the derivative of $f(x)^{g(x)}$ the same as the sum of the derivative of $x^n$ and $a^x$?,We did the derivation of the following formula in class today- $$(f(x)^{g(x)})'=g(x)f(x)^{g(x)-1}f'(x)+f(x)^{g(x)}ln(f(x))g'(x)$$ My question is that if there is a reason that this is just the sum of the derivative of $x^n$ and $a^x$ ?,"['calculus', 'derivatives']"
3726049,"$\cos(\alpha-\beta)+\cos(\beta-\gamma)+\cos(\gamma-\alpha)=\frac{-3}{2}$,show that $\cos\alpha+\cos\beta+\cos\gamma=\sin\alpha+\sin\beta+\sin\gamma=0$","I think that I've done a major part of the problem but I'm stuck at a point. Here's what I've done : It's given to us that $$\cos(\alpha-\beta)+\cos(\beta-\gamma)+\cos(\gamma-\alpha) = \dfrac{-3}{2}$$ Using the identity $\cos(A-B) = \cos A \cos B + \sin A \sin B$ , we obtain : $$\cos\alpha\cos\beta + \sin\alpha\sin\beta + \cos\beta\cos\gamma + \sin\beta\sin\gamma + \cos\gamma\cos\alpha + \sin\gamma\sin\alpha = \dfrac{-3}{2}$$ Multiplying both sides by $2$ , we obtain : $$2\cos\alpha\cos\beta + 2\cos\beta\cos\gamma + + 2\cos\gamma\cos\alpha + 2\sin\alpha\sin\beta + 2\sin\beta\sin\gamma + 2\sin\gamma\sin\alpha = -3$$ Adding $\sin^2\alpha+\sin^2\beta+\sin^2\gamma+\cos^2\alpha+\cos^2\beta+\cos^2\gamma$ to both sides, we obtain : $$\text{LHS : } (\cos^2\alpha + \cos^2\beta + \cos^2\gamma + 2\cos\alpha\cos\beta + 2\cos\beta\cos\gamma + 2\cos\gamma\cos\alpha)$$ $$ + (\sin^2\alpha + \sin^2\beta + \sin^2\gamma + 2\sin\alpha\sin\beta + 2\sin\beta\sin\gamma + 2\sin\gamma\sin\alpha)$$ $$\text{RHS : } -3 + (\cos^2\alpha + \sin^2\alpha) + (\cos^2\beta + \sin^2\beta) + (\cos^2\gamma + \sin^2\gamma)$$ On simplifying, $$\text {LHS : } (\cos\alpha + \cos\beta + \cos\gamma)^2 + (\sin\alpha + \sin\beta + \sin\gamma)^2$$ $$\text{RHS : } -3+1+1+1 = -3+3 = 0$$ So, we obtain : $$(\cos\alpha + \cos\beta + \cos\gamma)^2 + (\sin\alpha + \sin\beta + \sin\gamma)^2 = 0$$ $$\implies (\cos\alpha + \cos\beta + \cos\gamma)^2 = -(\sin\alpha + \sin\beta + \sin\gamma)^2$$ Now, square rooting both sides would involve $\iota$ i.e. $\sqrt{-1}$ but I haven't learnt about complex numbers yet and I think that the solution can be continued without using complex numbers but I don't know how. Any help would be appreciated. Thanks!",['trigonometry']
3726096,How to solve $f(x)=f^{-1}(x)$?,"It is a well-known fact that the graphs of $y=f(x)$ and $y=f^{-1}(x)$ are symmetrical about the line $y=x$ . Thus we can say the solution of $f(x)=x$ must satisfy $f(x)=f^{-1}(x)$ but converse is not true. But I have seen that many authors find the solution of $f(x)=x$ when we are rquired to solve $f(x)=f^{-1}(x)$ . But this method does not give us a guarantee to give complete answer. As an example, suppose we want to solve $(1-x)^3=1-x^{1/3}$ . We can easily observe that the equation is written in the form $f(x)=f^{-1}(x)$ where $f(x)=(1-x)^3$ . Let's see the solution of this equation by drawing their graphs by using a graphing calculator. We can see that the equation $f(x)=f^{-1}(x)$ has 5 solutions but the equation $f(x)=x$ has only one solution. The 'extra' solutions (except from $f(x)=x$ ) exist because there are some points on the graph of $y=f(x)$ which are image pairs (means, they themselves are symmetrical) about the line $y=x$ . In the given example, we notice that the points (C, F) and (D, E) are image pairs. My question is that if we are given to solve $f(x)=f^{-1}(x)$ then Do we have some easy method to solve it without actually solving $f(x)=f^{-1}(x)$ ? If we don't such an easy method, then can we have some easy method to figure out whether the graph has some image pairs or not? If we could figure out that the graph does not have any image pair then we can safely solve $f(x)=x$ .","['functions', 'graphing-functions', 'inverse-function']"
3726097,"Given a Ferrers diagram, prove that $\det(M)=1$","Let $\lambda$ be a Ferrers diagram corresponding to some
integer partition of $k$ . We number the rows and the columns, so that the
j'th leftmost box in the i'th upmost row is denoted as $(i,j)$ . Let $n$ be the largest number, such that the box $(n,n)$ is part of the
diagram. For each box $(i,j)\in \{1,\dots,n\}^2$ let $\ell(i,j)$ be the lowest
box in the $j$ 'th column of $\lambda$ , and let $r(i,j)$ be the rightmost
box in the $i$ 'th row of $\lambda$ . Note that $\ell(i,j)$ and/or $r(i,j)$ might be $(i,j)$ itself. We think about the diagram as a grid of vertices, such that the $(i,j)$ vertex is connected to $(i-1,j)$ and $(i,j+1)$ with directed
edges. We define $M\in \mathbb{R}^{n\times n}$ such that $M_{i,j}$ is
the number of directed paths from $\ell(i,j)$ to $r(i,j)$ . Namely, the
number of ""walks"" in which every step is either one move upward or one
move to the right. Note that we thus get a square matrix with positive
integer entries. Prove that for any $\lambda$ we have $\det(M)=1$ . Here is an example of a Ferrers diagram $\lambda$ in which $M$ is $3\times3$ , along with the corresponding numbers $M_{i,j}$ we put in each box $(i,j) \in \{1,\dots,n\}^2$ as defined above. $ \begin{align} 9&&3&&1&&☐ \\ 5&&2&&1&& \\ 1&&1&&1&& \\ ☐&&☐&&&& \\ ☐&&&&&& \\ \end{align} $ (See: https://i.sstatic.net/9RaP4.png ) I tried to prove it using induction. I tried to show that if you make the following row operation, you eventually get a triangular matrix with 1's on the diagonal. $$
\textrm{for $i=n,n-1,\dots,1$ do:}\\
\textrm{for $k=i-1,i,\dots,1$ do:}\\
R_{k} \longleftarrow R_{k}-M_{k,i}\cdot R_{i}
$$ But it didn't go well.","['integer-partitions', 'young-tableaux', 'combinatorics']"
3726122,Proving a subset of $H^1(\mathbb{R}^d)$ is compactly embedded in $L^2(\mathbb{R}^d)$.,"I was recenty reading about the weighted Lebesgue spaces and came accross an exercise that asks  to prove that $H^1(\mathbb{R}^d) \cap L^2(\mathbb{R}^d,|x|^2\,dx)$ is compactly embedded in $L^2(\mathbb{R}^d)$ . Where $H^1(\mathbb{R}^d)$ is the usual Sobolev space and $L^2(\mathbb{R}^d,|x|^2\,dx)$ is the weighted Lebesgue space containing functions $f$ for which $\int_{\mathbb{R}^d} |f|^2\,|x|^2\,dx< \infty$ The compact embedding theorems I know work on a bounded domain, not sure how to prove this one.","['sobolev-spaces', 'lebesgue-integral', 'functional-analysis']"
3726127,"Wrong codomain, why is this function called a composition?","I'm reading a book and it says: Definition: Let $\pmb f: \mathcal D_{\pmb f} \subset \mathbb R^n \to \mathbb R^m$ and $\pmb g:\mathcal D_{\pmb g} \subset\mathbb R^m \to \mathbb R^p$ be two maps and $\pmb x_0\ \in  \mathcal D_{\pmb f}$ a point such that $\pmb y_0=\pmb f(\pmb x_0)\in \mathcal D_{\pmb g}$ , so that the composite $$
\pmb h=\pmb g \circ \pmb f : \mathcal D_{\pmb h} \subset \mathbb R^n \to\mathbb R^p \tag 1
$$ for $\pmb x_0\in \mathcal D_h$ , is well defined. So far so good! And then: Theorem: Let $\pmb f$ be differentiable at $\pmb x_0 \in \mathcal D_{\pmb h}$ and $\pmb g$ differentiable at $\pmb y_0 = \pmb f(\pmb x_0)$ . Then $\pmb h = \pmb g\circ \pmb f$ is differentiable at $\pmb x_0$ and its Jacobian matrix is $$
\pmb J(\pmb g\circ \pmb f) (\pmb x_0) = \pmb J\pmb g(\pmb y_0)\pmb J\pmb f(\pmb x_0) \tag 2
$$ So far so good! But then this example is given: Let $\phi: I\subset \mathbb R\to \mathbb R$ be a differentiable map and $f:\mathbb R^2\to \mathbb R$ a scalar differentiable function. The composite $h(x)= f(x,\phi(x))$ is differentiable on $I$ , and $$
h'(x) = \frac{dh}{dx}(x)=
\frac{\partial f}{\partial x}(x,\phi(x))+\frac{\partial f}{\partial y}(x,\phi(x))\phi'(x) \tag 3
$$ as follows from the theorem above, since $h=f \circ \Phi$ , with $\Phi:I\to \mathbb R^2$ , $\Phi(x)=(x,\phi(x))$ Note: The book used $\Phi$ and not $\phi$ in the last line, is it a typo? Here is my problem: I don't follow why $h$ is a composition of $f$ and $\phi$ . Because, according to $(1)$ the co-domain of the ""inner function"", $\pmb f$ , should be equal to the domain of the ""outer function"", $\pmb g$ . But, in this example, the ""inner function"" is $\phi$ with the co-domain $I\subset \mathbb R$ and this is not equal to the domain of the ""outer function"", $f$ , which is $\mathbb R^2$ . So why is $h$ a composition? Can somebody shed some light here?","['notation', 'multivariable-calculus', 'functions', 'real-analysis']"
3726136,"Confusion with elimination of index variable if it does not appear in the summand - Concrete Mathematics, D.Knuth","Got confused by the statement as the author does not provide any details regarding this property.
So in the book ""Concrete Mathematics"" the authors state: An index variable that doesn’t appear in the summand (here j) can simply be eliminated if we multiply what’s left by the size of that variable’s index set (here $n-k$ ). Then the authors use this property in evaluating the particular sum: $ \begin{equation} S_n = \displaystyle\sum\limits_{1 \le j < k \le n}^{}{\frac{1}{k-j}}
\end{equation} $ = $ \begin{equation} S_n = \displaystyle\sum\limits_{1 \le j < k+j \le n}^{}{\frac{1}{k}} \end{equation} $ - replacing $k$ by $k+j$ $ \begin{equation} S_n = \displaystyle\sum\limits_{1 \le k \le n}^{} \displaystyle\sum\limits_{1 \le j  \le n-k}^{}{\frac{1}{k}} \end{equation} $ -summing first on j $ \begin{equation} S_n = \displaystyle\sum\limits_{1 \le k \le n}^{}{\frac{n-k}{k}} \end{equation} $ - the sum on $j$ is trivial I have underscored what makes me so confused. According to the authors' note, this is valid, however I seek for the clear explanation or proof that allow to eliminate the index variable by simply multiplying the summand by the upper bound. Could anyone shed more light on that?","['summation', 'factoring', 'discrete-mathematics', 'inequality', 'index-notation']"
3726183,Corners are cut off from an equilateral triangle to produce a regular hexagon. Are the sides trisected?,"The corners are cut off from an equilateral triangle to produce a regular hexagon. Are the sides of the triangle trisected? In the actual question, the first line was exactly the same . It was then asked to find the ratio of area of the resulting hexagon and the original triangle. In the several solutions of this problem which I found on the internet, they had started with taking the sides of triangle as trisected by this operation and hence the side length of the hexagon would also be equal to one-third of the side length of triangle. I have seen some variations of this problem where they had explicitly mentioned that the side was trisected and then hexagon was formed. On stackexchange, there are problems in which they started by trisecting the sides (they mentioned it in the title) and getting a regular polygon. My question is, if we cut off corners from the equilateral triangle to form regular hexagon, is it going to trisect the sides of triangle or not?","['proof-explanation', 'geometry']"
3726185,"Denote that expression is differentiated, without differentiating it","I'm trying to indicate that I'm working with the derivative of an expression, without differentiating it. This is how it would be done, if I differentiate both sides now: \begin{align}
y &= 5x^2\\
y' &= 10x
\end{align} However, I want to differentiate the right side later, but keep working with the expression. How can I denote that I'm referring to the derivative? I suppose this won't work, but I would like to do something in this fashion: $$
y = 5x^2\\
y'= (5x^2)'
$$ Is there any good way to do this?","['notation', 'calculus', 'derivatives']"
3726277,Is a circulant matrix irreducible?,"A circulant matrix $\mathbf{C} \in \mathbb{R}^{n \times n}$ is of the form \begin{equation}
\label{circulantmat}
    \mathbf{C} = \begin{pmatrix} 
    {c_0} & {c_1} & {\dots} & {c_{n-2}} & {c_{n-1}} \\
    {c_{n-1}} & {c_0} & {c_1} & {} & {c_{n-2}} \\
    {\vdots} & {c_{n-1}} & {c_0} & {\ddots} & {\vdots} \\
    {c_2} & {} & {\ddots} & {\ddots} & {c_1} \\
    {c_1} & {c_2} & {\dots} & {c_{n-1}} & {c_0} \\
    \end{pmatrix}.
\end{equation} In my case, I have that $\mathbf{C}$ has zero entries everywhere except $c_0$ , $c_1$ and $c_{n-1}$ , which are positive. I know that $\mathbf{C}$ is irreducible if and only if \begin{equation}
    (\mathbf{I}_n + \mathbf{C})^{n-1} > 0.
\end{equation} Is there a way to show that the above holds? By intuition, I know that as the non-zero entries of the circulant matrix neighbor each other, the directed graph of the adjacency matrix of $\mathbf{C}$ would be strongly connected, but I want to show this more mathematically. I have other circulant matrices where more neighboring points are non-zero, which I assume would mean that a similar result would hold.","['matrices', 'numerical-linear-algebra', 'matrix-calculus', 'linear-algebra']"
3726305,KL-Divergence of Uniform distributions,"Having $P=Unif[0,\theta_1]$ and $Q=Unif[0,\theta_2]$ where $0<\theta_1<\theta_2$ I would like to calculate the KL divergence $KL(P,Q)=?$ I know the uniform pdf: $\frac{1}{b-a}$ and that the distribution is continous, therefore I use the general KL divergence formula: $$KL(P,Q)=\int f_{\theta}(x)*ln(\frac{f_{\theta}(x)}{f_{\theta^*}(x)})$$ $$=\int\frac{1}{\theta_1}*ln(\frac{\frac{1}{\theta_1}}{\frac{1}{\theta_2}})$$ $$=\int\frac{1}{\theta_1}*ln(\frac{\theta_2}{\theta_1})$$ From here on I am not sure how to use the integral to get to the solution.","['statistics', 'uniform-distribution']"
3726333,An analytic continuation of the square root along the unit circle,"I want to find an analytic continuation of the square root along the unit circle but I am not sure whether I am doing it correctly. Let $C_0$ be the open disk of radius $1$ around $1$ , and let $f_0:C_0
\to \mathbb{C}$ be defined as $f_0(re^{i \varphi})=\sqrt{r} e^{i
\frac{\varphi}{2}}$ , where $\varphi \in (-\pi,\pi]$ . Let $\gamma:
[0,1] \to \mathbb{C}$ be the path given $\gamma(t)=e^{2 it \pi}$ . Find
an analytic continuation of $f_0$ along $\gamma$ , i.e. a sequence $(C_k,f_k)_{k=0}^{n}$ of analytic continuations $f_k$ of $f_0$ such
that the $C_k$ cover the image of $\gamma$ . Show that $f_n(1)=-f_0(1)$ . I tried to do this as follows. Since $f_0$ is holomorphic on $C_0$ we have the power series expansion $$
f_0(z)=\sum_{m=0}^{\infty} a^{(0)}_m (z-1)^m \tag{1}
$$ where $a^{(0)}_m=\frac{1}{m!} \frac{\partial^m}{z^m} \sqrt{z} \big|_{z=1}$ . I wanted to analyticaly continue this series by defining $C_1=\{z \in \mathbb{C} \ | \ |z-e^{i \frac{\pi}{4}}|<1 \}$ and considering the function $$
f_1: C_1 \to \mathbb{C}, \ f_1(z)=\sum_{m=0}^{\infty} a^{(1)}_m (z-e^{i \frac{\pi}{4}})^m. 
$$ where $a^{(1)}_m=\frac{1}{m!} \frac{\partial^m}{z^m} \sqrt{z} \big|_{z=e^{i \frac{\pi}{4}}}$ and $arg(z) \in (-\frac{3 \pi}{4}, \frac{5 \pi}{4}]$ . Let $z=re^{i\varphi} \in C_0 \cap C_1$ . With $z_1=e^{i \frac{\pi}{4}}$ I have $$
\sqrt{z}=\sqrt{z_1} \sqrt{\frac{z}{z_1}}
=\sqrt{z_1} \sqrt{1+\frac{z}{z_1}-1}
\underset{(1)}{=}\sqrt{z_1} \sum_{m=0}^{\infty} a^{(0)}_m (\frac{z}{z_1}-1)^m
=\sum_{m=0}^{\infty} \frac{\sqrt{z_1}}{z^m_1} a^{(0)}_m (z-z_1)^m 
$$ Since the series representation of $f_1$ is unique and since $f_0(z)=\sqrt{z}$ the functions $f_0,f_1$ agree on $C_0 \cap C_1$ . By iterating the steps above I can define disks $C_2, C_3,...C_8$ with centers $e^{i k\frac{\pi}{4}}$ and corresponding holomorphic functions $f_k$ , $k=2,...,8$ , each time requiring $arg(z) \in (-\pi+k \frac{\pi}{4},\pi+k \frac{\pi}{4}]$ for $z \in C_k$ . Upon considering the power series expansion centered at $e^{i \frac{8 \pi}{2}}=e^{i 2 \pi}$ I should get $f_8(e^{i 2 \pi})=\sqrt{1} e^{i \pi}=-1=-\sqrt{1} e^{i \cdot 0}=-f_0(1)$ . Am I on the right track here or is there an error?","['complex-analysis', 'power-series', 'analyticity']"
3726348,checking uniqueness and boundedness of an initial value problem,"Consider the initial value problem $$\frac{dy}{dx}=x^2+y^2$$ with $y(0)=1$ where $0≤x≤1$ . Then which of the following statements are
true? $(a)$ There exists a unique solution in $\displaystyle\bigg[0,\frac{\pi}{4}\bigg]$ . $(b)$ Every solution is bounded in $\displaystyle\bigg[0,\frac{\pi}{4}\bigg]$ . $(c)$ The solution exhibits a singularity at some point in $[0,1]$ . $(d)$ The solution becomes unbounded in some subinterval of $\displaystyle\bigg[\frac{\pi}{4},1\bigg]$ . For $(a)$ ; I started out by finding the largest interval of existence by Picard's theorem. Considering a rectangular strip $|x|≤h$ and $|y−1|≤k$ , we see that $|x^2+y^2|≤|x|^2+|(y−1)+1|^2≤h^2+k^2+1=M$ . Now the maximum interval of existence is $|x|≤h′$ where $\displaystyle h′=\min\bigg\{h,\frac{k}{M}\bigg\}=\min\bigg\{h,\frac{k}{h^2+k^2+1}\bigg\}$ . But I'm unable to check that minimum to see whether $\displaystyle h'>\frac{\pi}{4}$ or $\displaystyle h'<\frac{\pi}{4}$ . Also how to check the boundedness/singularity of solutions in the above intervals I don't understand. Any help is appreciated. I know the solution can be found using Bessel function. But that's not what I'm asking here. I want a method without explicitly finding the solution.",['ordinary-differential-equations']
3726484,Prove that a subset of $\mathbb{Z}$ is a subgroup.,"Came across an interesting question regarding subgroups of $\mathbb{Z}$ . Let $A \subseteq \mathbb{Z}$ such that $0 \in A$ , $A = -A$ (for every element in $A$ , its negative is also in $A$ ), and $A + 2A \subseteq A$ (for every $a,b \in A $ , $a+2b \in A$ ). We need to prove that this implies $A$ must be a subgroup of $\mathbb{Z}$ . Moreover, we need to show that this does not necessarily hold for $A \subseteq \mathbb{Z}^2 $ which has the same properties. Proving subgroups seems to be a challenge, since you get associativity, identity, and inverse for free. However, you cannot prove closure by manipulating the elements (atleast I cannot). I think the solution lies in the second part of the answer, where $A$ has some property that only exists as a subset of $\mathbb{Z}$ and not $\mathbb{Z}^2$ . Any help would be appreciated.","['group-theory', 'abstract-algebra']"
3726503,convex function of n convex function,"Assume $f$ is convex function that takes $n$ arguments, and we know $f$ is non-decreasing in every component.
Assume we have another $n$ convex functions: $h_1,...,h_n$ Does that composition $f(h_1(x_1),\ldots,h_n(x_n))$ is convex? I assume it does, but I also know that the composition of convex function isn't necessarily convex so I'm having hard time to prove it.","['convex-optimization', 'functions', 'functional-analysis', 'optimization', 'convex-analysis']"
3726507,A question about the main conjecture of Iwasawa theory.,"Let $p$ be an odd prime, and $K=\mathbb{Q}(\mu_{p})$ . Consider the cyclotomic extension of $K$ : $$K\subset K_1\subset K_2\subset\cdots\subset K_\infty ,$$ where $K_n=\mathbb{Q}(\mu_{p^{n+1}})$ and $K_\infty=\mathbb{Q}(\mu_{p^{\infty}})$ . The Iwasawa algebra is $$\Lambda=\varprojlim_n\mathbb{Z}_p[\text{Gal}(K_n/K)].$$ If $M$ is a $\Lambda$ -module, $\text{char}(M)$ is its characteristic ideal, and if it is also a $\mathbb{Z}_p[\Delta]$ -module, for $\Delta=\text{Gal}(K/\mathbb{Q})$ , then $M(\chi)$ is the $\chi$ -component, for $\chi$ a $p$ -adic character of $\Delta$ The following Main Conjecture holds: if $\chi$ is an even $p$ -adic character of $\Delta=\text{Gal}(K/\mathbb{Q})$ , then $$\text{char}(C_{\infty}(\chi))=\text{char}(E_{\infty}(\chi)/V_{\infty}(\chi)),$$ where $C_{\infty}$ is the inverse limit of the $p$ -parts of the ideal class groups of $K_n$ , $E_{\infty}$ is the inverse limit of the closure of the units of $K_n$ in the principal units of $\mathbb{Q}_p(\mu_{p^{n+1}})$ and $V_{\infty}$ is the inverse limit of the closure of the cyclotomic units of $K_n$ in the principal units of $\mathbb{Q}_p(\mu_{p^{n+1}})$ (all these under norm maps). This makes sense since we know that if $\chi$ is even, then the modules consider are finitely generated torsion $\Lambda$ -modules. My question: is it true that $$\text{char}(C_{\infty})=\text{char}(E_{\infty}/V_{\infty})?$$ Does this make sense? Is $E_{\infty}/V_{\infty}$ again finitely generated and torsion? This is true for $C_{\infty}$ . Also, we know that every module can be decomposed into the direct sum of its component with respect to the characters, but the main conjecture holds only if $\chi$ is even, therefore we cannot directly use this fact, since we lack information about $\chi$ odd.","['number-theory', 'algebraic-number-theory', 'cyclotomic-fields']"
3726532,Determine a polynomial function with some information about the function,"I am working through some exercises at the end of a textbook chapter on polynomial functions. Till now the questions have been about providing answers based on a given polynomial function. However, with this particular question I am to work backwards and define the polynomial based on some information about it: use the information about the graph of a polynomial function to determine the function. Assume the leading coefficient is $1$ or $–1$ . There may be more than one correct answer. The $y$ -intercept is $(0, 0)$ ,  the $x$ -intercepts are $(0,0)$ , $(2,0)$ , and the degree is 3.
End behavior: As $x$ approaches $-\infty$ , $y$ approaches $-\infty$ , as $x$ approaches $\infty$ , $y$ approaches $\infty$ . What I can tell is that since it's an odd degree, the functions will approach $-\infty$ or $+\infty$ either side of $x=0$ but that's already provided in the description. Tried writing it down as: $y = x(x-2)$ since the root of $(0,0)$ is $0$ (right) and the root of $(2,0)$ is $-2$ (right?). The provided answer is $x^3-4x^2-4x$ . How can I arrive at this solution with the information provided? Granular baby steps appreciated if possible?","['algebra-precalculus', 'functions', 'polynomials']"
3726541,Why is this theorem about derivatives true? $\frac{dy}{dx}= \frac{1}{dx/dy}$,$$\frac{dy}{dx}= \frac{1}{\;\frac{dx}{dy}\;}$$ Why is the above theorem true as long as $dx/dy$ is not zero? How can you prove it rigorously? I don’t think it is obvious by the definition of the derivative. I think this says $dx/d(x^2)$ will equal to $1/2x$ and so we can evaluate derivatives such as this. But I want a rigorous proof. Edit: by the answers I think you want the existence and differentiability of f inverse for something like this to even work ? Could the derivative still exist in such an example and fail to be able to be evaluated like this?Or does that have no meaning ?,"['calculus', 'derivatives', 'real-analysis']"
3726580,Finding derivative of integral?,How may I calculate derivative for the following function? $$I_n(x)=\int_{a}^{x} (x-t)^{n}f(t)dt$$ Note : I'm given that $f$ is continuous,"['integration', 'calculus', 'derivatives']"
3726611,Fourier transform of signum function,"If we treat fourier transform as an operator on $L^1(\mathbb{R})$ , then its image under fourier transform is the set of continuous functions which will vanish at infinity. It is well known that the fourier transform of signum function is $$\mathcal{F} (sgn)(u) =\frac{2}{ui}. $$ I know that signum function is not integrable over the real line. So, to evaluate its fourier transform, one can use limiting argument, say a sequence of functions that converges to signum function, because fourier transform is a bounded linear operator, and hence is continuous. What puzzles me is that when using continuity, don't we need to ensure that the fourier transform is defined on the limiting function? In this case, fourier transform of signum function is not defined due to reason given above. If this is the case, how would one obtain formula of the fourier transform of signum function?","['fourier-transform', 'real-analysis']"
3726632,Adding Background Evidence Variable to Bayes' Theorem,"A geometric interpretation of the Bayes' Theorem formula $$\Pr(H \mid E) = \frac{\Pr(H)\ \Pr(E\mid  H)}{\Pr(H)\ \Pr(E\mid  H)\ +\ \Pr(H^c)\ \Pr(E\mid  H^c)}$$ takes place in a unit square.  First, draw a vertical line through the square so that your assessment of a hypothesis's prior probability, $\Pr(H)$ , is the distance left of the line, and $\Pr(H^c)$ is the distance right of the line, as shown below in the left figure.  Then draw horizontal lines through the left rectangle at height $\Pr(E\mid  H)$ , and the right rectangle at height $\Pr(E\mid  H^c)$ , as in the center figure.  If the left line is higher, then the new evidence increased your epistemic probability of $H$ , and vice-versa. The areas of the new rectangles correspond to the above Bayes' Theorem formula, according to the pattern displayed in the right figure.  The numeric output is your new epistemic probability of $H$ . This output can then be treated as a new prior probability to be assessed against a new piece of evidence.  If the original was your prior probability at time $t_0$ , now your prior probability at $t_1$ is visualized by drawing a vertical line in a fresh unit square, as far to the right as necessary to match the output of the previous Bayes' Theorem iteration.  This process can continue indefinitely, assigning a new time step for each incoming piece of evidence. An alternative Bayes' Theorem formula $$\Pr(H\mid  (B\ \cap\ E)) = \frac{\Pr(H\mid  B)\ \Pr(E\mid (B\ \cap\ H))}{\Pr(H\mid  B)\ \Pr(E\mid  (B\ \cap\ H)) + \Pr(H^c\mid  B)\ \Pr(E\mid  (B\ \cap\ H^c))}$$ includes the variable $B$ to represent background evidence. This answer explains how to algebraically derive the alternative formula from the initial formula.  What I don't understand is $B$ 's purpose.  It seems to me that the initial Bayes's Theorem formula already described the iterable progression of the epistemic probability of $H$ from one time step to the next.  The notion of background evidence must have already been accounted for implicitly in $Pr(H)$ . How. if at all, does switching to the Bayes' Theorem formula with $B$ change the conceptual and/or geometric evolution described above?  Is the inclusion of $B$ a mathematically irrelevant substitution, similar to how differential equations are sometimes written with constants such as $\frac{k}{m}$ , even when they could be simplified by lumping, or should the presence of $B$ add something to the way the above mathematics is understood?","['statistics', 'conditional-probability', 'geometry', 'bayes-theorem', 'probability']"
3726639,I somehow deduced that $\tan x=\iota$ for any real value of $x$ by equating the value of $\tan(\frac{\pi}{2}+x)$ obtained using two identities.,"Let's assume that we're familiar with the identity : $\tan \Bigg (\dfrac{\pi}{2} + x \Bigg ) = -\cot x$ which we have derived using the unit circle . I was trying to equate the values of $\tan \Bigg ( \dfrac{\pi}{2} + x \Bigg )$ obtained using the above mentioned identity and the compound angle identity and I got a weird result. Have a look : $$\tan \Bigg ( \dfrac{\pi}{2} + x \Bigg ) = \dfrac{\tan\dfrac{\pi}{2} + \tan x}{1 - \tan \dfrac{\pi}{2} \tan x}$$ For the sake of simplicity, let us assume that $\tan \dfrac{\pi}{2} = a$ and $\tan x = b$ . $$ \therefore \tan \Bigg ( \dfrac{\pi}{2} + x \Bigg ) = \dfrac{a + b}{1 - ab} \implies -\cot x = \dfrac{a + b}{1 - ab}$$ Also, $$-\cot x = \dfrac{-1}{\tan x} = \dfrac{-1}{b}$$ $$ {\color{red} {\therefore \dfrac{-1}{b} = \dfrac{a + b}{1 - ab} \implies -1 + ab = ab + b^2 \implies -1 = b^2}}$$ This leads us to : $$\tan x = b = \sqrt{-1} = \iota$$ which is not true. So, what went wrong here? I think that the ${\color{red}{\text{highlighted part}}}$ was wrong because while cross-multiplying, I automatically made the assumption that $1 - ab$ has a real value which won't be the case if $\tan \Bigg ( \dfrac{\pi}{2} \Bigg )$ doesn't have a real value (which is actually the case as $\tan \Bigg ( \dfrac{\pi}{2} \Bigg ) = \dfrac{\sin \Bigg ( \dfrac{\pi}{2} \Bigg )}{\cos \Bigg ( \dfrac{\pi}{2} \Bigg )} = \dfrac{1}{0}$ which does not have a real value and approaches $\infty$ ) Was this the mistake I made? Thanks!","['trigonometry', 'fake-proofs']"
3726731,Can Cramer's Rule really distinguish between infinite no. of solutions and no solution?,"This is a question which was asked in a high-school exam held in India( JEE ADVANCED ). Going by Cramer's rule, for infinite solutions, I should get $D=D_1=D_2=D_3=0$ (where $D$ is the original determinant and $D_1, D_2, D_3$ are the respective coefficient determinants). Using these, I arrive at $\alpha^2$ =1, so that $\alpha$ =1,-1. But, $\alpha=1$ yields no solution here (if I write down the system of equations using $\alpha=1$ ). Why does it happen so? Is this a rare failure of Cramer's rule? How should we explain this unexpected result?","['matrices', 'determinant', 'contest-math']"
3726734,Eigenvalues of the one-dimensional Schrödinger operator,"I am stuck with the following problem: Let $Af = -f'' + uf$ be an unbounded operator, with $u \in \mathcal{S}(\mathbb{R},\mathbb{R})$ in the Schwartz space. The domain of $A$ is the linear subspace $H^2(\mathbb{R})$ of $L^2(\mathbb{R})$ . (i) Show that if $\lambda<0$ is an eigenvalue, then $\lambda > \inf_{x \in \mathbb{R}}\{u(x)\}$ . (ii) Show that if $\lambda_2 < \lambda_1$ are eigenvalues with corresponding eigenfunctions $f_2$ and $f_1$ and if $x_1 > x_2$ are two consecutive zeros of $f_1$ then the function $f_2$ must have at least one zero in the interval $(x_1,x_2)$ . For part (i) I attempted to solve it by proof of contradiction and showing that the corresponding ODE does not have a solution. I get $-f''+(u-\lambda)f =0$ but how do I know that this is not solvable? I think part (ii) follows from part (i). I would greatly appreciate any input, since I do not know how to approach this. Thank you!","['schwartz-space', 'spectral-theory', 'functional-analysis', 'ordinary-differential-equations']"
3726739,I'm having trouble identifying a 4-polytope from its system of inequalities,"In the euclidean space, the points $(x,y,z)$ belonging to a regular octahedron are those that satisfy the inequalities $$
\pm x\pm y \pm z \leq a
$$ where $a \geq 0$ . These eight inequalities can be divided into two groups of four according to the number (even or odd) of negative signs they contain. For example, the inequalities \begin{align}
x-y+z\leq a\\
-x+y+z \leq a\\
x+y-z\leq a\\
-x-y-z\leq a
\end{align} all have one or three negative signs and the points satisfying these form a tetrahedron. The other four inequalities correspond to the dual tetrahedron of the first, which shows that the intersection of two regular dual tetrahedra form a regular octahedron. Moreover, the vertices of the two tetrahedra can be seen as the eight vertices of a cube. I am wondering if there exists a similar relationship between regular polytopes in four dimensions. As it is another case of a regular cross-polytope, the hexadecachoron (or 16-cell) is defined by the sixteen inequalities $$
\pm x\pm y \pm z \pm w \leq a.
$$ If one were to take the eight inequalities containing an odd number of negative signs, say \begin{align}
x-y-z-w\leq a\\
-x+y-z-w\leq a\\
-x-y+z-w \leq a\\
-x-y-z+w\leq a\\
x+y+z-w \leq a\\
x+y-z+w \leq a\\
x-y+z+w \leq a\\
-x+y+z+w \leq a\\
\end{align} which 4-polytope would be obtained ? I doubt it would be a regular 5-cell, since (obviously) the number of cells and the number of hyperplanes don't add up. Besides, the intersection of the two 4-polytopes corresponding to the two sets of eight inequalities should technically correspond to the 16-cell. The tesseract, having eight cells, could be a candidate, but I have been unable to show that these eight inequalities define one (or any other 4-polytope). Any ideas ? Edit : I've discovered just now that 16-cells are four dimensional demihypercubes (see https://en.wikipedia.org/wiki/Demihypercube ), and so they are analogous to tetrahedra in that two of them can be combined to obtain the 16 vertices of a tesseract. I am still interested to know what type of polytope corresponds to the eight inequalities above, however.","['analytic-geometry', 'polyhedra', 'geometry', 'polytopes']"
3726767,Petersen's proof of Weitzenböck identity,"I realize that my question will sound silly, but I am reading Petersen's book on Riemannian geometry (third edition) and I can't quite figure out one step in his proof of the Weitzenböck identity for forms (Theorem 9.4.1, page 347).
My notation is as follows. Let $T$ be a tensor on a Riemannian manifold $M$ , with Levi-Civita connection $\nabla$ and let $R$ denote the curvature tensor with the following sign convention $$R(X,Y)T=\nabla^2_{X,Y}T-\nabla^2_{Y,X}T=\nabla_X(\nabla_Y T)-\nabla_Y(\nabla_X T)-\nabla_{[X,Y]}T$$ where $\nabla^2_{X,Y}T=\nabla_X(\nabla_YT)-\nabla_{\nabla_X Y}T$ .
Let $\{E_1,\dots,E_n\}$ be a local orthonormal frame. Then, for a $k$ -form $\omega$ on $M$ and local vector fields $X_0,X_1,\dots,X_k$ we have that its exterior derivative can be expressed as $$(d\omega)(X_0,\dots,X_k)=\sum_{i=0}^n(-1)^i(\nabla_{X_i}\omega)(X_0,\dots,\widehat{X}_i\dots,X_k)$$ and the $L^2$ -adjoint $\delta$ of $d$ is given by $$(\delta\omega)(X_2,\dots,X_k)=-\sum_{j=0}^n(\nabla_{E_j}\omega)(E_j,X_2,\dots, X_k)$$ Finally, let us define the tensor $\text{Ric}(\omega)$ by $$(\text{Ric}(\omega))(X_1,\dots,X_k)=\sum_{i,j=1}^n (R(E_j,X_i)\omega)(X_1,\dots,E_j,\dots,X_k)$$ where $E_j$ is in the $i$ -th slot, instead of $X_i$ . Let the ""rough Laplacian"" $\Delta=-\nabla^*\nabla$ be defined as $\sum_j \nabla^2_{E_j,E_j}$ , so that $$-(\Delta\omega)(X_1,\dots,X_k)=(\nabla^*\nabla\omega)(X_1,\dots,X_k)=-\sum_j(\nabla^2_{E_j,E_j}\omega)(X_1,\dots,X_k)$$ and let the De Rham Laplacian $\Delta_H$ be defined as usual by $\Delta_H=d\delta+\delta d$ , so that $$(\Delta_H\omega)(X_1,\dots,X_k)=(d(\delta\omega)+\delta(d\omega))(X_1,\dots,X_k)$$ The Weitzenböck identity in question is $$\Delta_H\omega=\nabla^*\nabla\omega+\text{Ric}(\omega)$$ I have no problems in showing that $$(\delta d\omega)(X_1,\dots,X_k)=(\nabla^*\nabla\omega)(X_1,\dots,X_k)+\sum(\nabla^2_{E_j,X_i}\omega)(X_1,\dots,E_j,\dots,X_k)$$ but I can't seem to get the other half of the formula, i.e. $$(d\delta\omega)(X_1,\dots,X_k)=-\sum(\nabla^2_{X_i,E_j}\omega)(X_1,\dots,E_j,\dots,X_k)$$ I'll explain the problem  in the basic case where $k=1$ , i.e. $\omega$ is a 1-form and $\delta\omega$ is a smooth function. Let $X$ be a local vector field. Then $$(d\delta\omega)(X)=X(\delta\omega)=-\sum X((\nabla_{E_j}\omega)(E_j))=-\sum\left((\nabla_X(\nabla_{E_j}\omega))(E_j)+(\nabla_{E_j}\omega)(\nabla_X E_j)\right)$$ and it is not clear to me that the term inside the sum is the second covariant derivative $(\nabla^2_{X,E_j}\omega)(E_j)$ . I know I could fix the problem by adapting the frame so that $\nabla_{E_i}E_j=0$ at a point and use tensoriality, but I was trying to see if I could get the full calculation in a general orthonormal frame to work out. What am I missing? Where is my mistake? P.S. I was super pedantic with brackets in order to avoid any notation misunderstandings. Sorry if the formulae are not very legible.","['proof-explanation', 'differential-forms', 'riemannian-geometry', 'differential-geometry']"
3726773,What is the underlying reason behind the definition of the discriminant as an expression of the roots?,"Background: The discriminant of a polynomial $A(x) = a_n x^n + a_{n-1} x^{n-1} + \cdots + a_0$ can be expressed in terms its roots as $$\text{Disc}(A)=a_{n}^{2n-2}\prod_{i<j}(r_i -r_j)^2\tag 1$$ so that for a quadratic $ax^2 + bx +c,$ the discriminant would predictably be $$a^2\left( \frac{-b-\sqrt{b^2-4a}}{2a} -  \frac{b-\sqrt{b^2-4a}}{2a} \right)^2=b^2-4a$$ The generalized form in $(1)$ I thing may be motivated by symmetric functions of the roots of a polynomial, $x_1, x_2, \dots, x_n,$ such as $$\begin{align}
S_1 &= x_1+x_2+\cdots+x_n=\sum x_i\\
S_2 &= x_1 x_2 + \cdots+ x_{n-1}x_n=\sum_{i<j}x_ix_j\\
S_3 &= x_1x_2x_3 +\cdots + x_{n-2} x_{n-1} x_n =\sum_{i<j<k} x_i x_j x_k\\
S_n &= \prod x_i
\end{align}$$ or $$\begin{align}
\sigma_1 &= S_1= x_1+x_2+\cdots+x_n=\sum x_i\\
\sigma_2 &= S_1^2 - 2 S_2= x_1^2+x_2^2+\cdots+x_n^2=\sum x_i^2\\
\sigma_3 &= S_1^3 - 3 S_1 S_2 + 3S_3= x_1^3+x_2^3+\cdots+x_n^3=\sum x_i^3\\
\end{align}$$ and Newton's recursive formulae $$\begin{align}
\sigma_1 &= S_1\\
\sigma_2 &= S_1 \sigma_1 - 2 S_2\\
\sigma_3 &= S_1 \sigma_2 -   S_2 \sigma_1 + 3 S_3\\
\sigma_4 &= S_1 \sigma_3 -   S_2 \sigma_2 +   S_3 \sigma_1 - 4 S_4\\
\end{align}$$ But is this even true? And if so, what is the link? For instance for a monic polynomial of degree $2$ $$\begin{align}
(x_1 - x_2)^2 &= (x_1 + x_2)^2 - 4 x_1 x_2\\
&= S_1^2 - 4 S_2
\end{align}$$ but what is the significance of that? Symmetry to achieve what purpose?","['group-theory', 'symmetric-polynomials', 'polynomials', 'discriminant']"
3726796,How to recognize the Laplace transform of a function with compact support?,"The question is pretty much self-contained in the title: is there some
criterion for recognizing the Laplace transforms of compact-supported
functions, other than the explicit computation of $\mathcal{L}^{-1}$ ? The question arises in a peculiar context: some integrals of oscillating functions can be converted into integrals of monotonic functions by exploiting the self-adjointness of the Laplace transform, for instance $$ \int_{0}^{+\infty}\frac{\sin(s)}{\sqrt{s}}\,ds = \int_{0}^{+\infty}\frac{dx}{\sqrt{\pi x}(1+x^2)} $$ and for numerical purposes the latter form is clearly more manageable than the former. On the other hand integrals of compact-supported functions are easier to handle through interpolation and quadrature, so it would be a nice thing to recognize in $\frac{1+e^{-\pi s}}{1+s^2}$ the Laplace transform of the chunk of the sine wave supported on $[0,\pi]$ , in order to compute $$ \int_{0}^{+\infty}\frac{1+e^{-\pi s}}{\sqrt{s}(1+s^2)}\,ds $$ by applying a quadrature scheme (as done here ) to $$ \int_{0}^{\pi}\frac{\sin(s)}{\sqrt{s}}\,ds. $$ The essence of the question is to understand which kinds of functions allow this trick.","['integration', 'complex-analysis', 'laplace-transform', 'approximation-theory']"
3726802,Is it possible to get a glimpse of whole of geometry before choosing a branch of it?,"Is it even possible to get a general idea of different fields in geometry before I choose a branch to focus upon, and if so, can someone give me a suggestion for topics that I must read before I settle on some? (Sorry if this question sounds too broad for this site.) Details: For context, I am a final year undergraduate student who seems to be interested in Geometry and Topology (especially, the branches like Differential Geometry rather than Algebraic Geometry.) I had always found geometry really interesting and thus during my undergraduate years I took courses and also tried to read about different fields in it. The courses included the ones like Manifolds and Algebraic Geometry . After spenting quite some time on them (during which I covered, the book An Introduction to Manifolds by Loring Tu and the first chapter of Hartshorne.) I seemed to find Manifolds more to my taste rather than Algebraic Geometry one which I found to have, I am sorry to say, no intuition at all, and not at all according to my taste. I also learned a bit of Algebraic topology with topics like Homotopy, Simplicial complexes, Homology etc. Nowadays, I am trying to read about Riemannian Geometry. Nowadays, I am afraid that even though I love the topics I usually read, and wants to read more about them, it might not even be possible for me to even get a proper glimpse of what is there in all of Geometry before, say, trying to settle on a topic for research (if all goes well, I want to start the PhD within two years), which is what my eventual aim is. So basically, is there a list of topics that I could go through before I choose something?","['general-topology', 'soft-question', 'geometry', 'differential-geometry']"
3726838,Constructing a proper variation through geodesics at cut locus,"Suppose $(M,g)$ is a complete, connected Riemannian manifold, $p \in M$ , and $\mathrm{Cut}(p)$ denotes the cut locus of $p$ , i.e. the set of all points $q \in M$ so that there is a minimizing geodesic from $p$ to $q$ that ceases to be minimizing past $q$ . I'm trying to prove the following (Problem 10-23 in John Lee's ""Introduction to Riemannian Manifolds""): Suppose $q \in \mathrm{Cut}(p)$ satisfies $d_g(p,q) = d_g(p,\mathrm{Cut}(p)) = \mathrm{inj}(p) =: b$ . Then either $q$ is conjugate to $p$ along some minimizing geodesic segment, or there are exactly two unit-speed geodesic segments $\gamma_0, \gamma_1 : [0,b] \to M$ such that $\dot\gamma_0(b) = -\dot\gamma_1(b)$ . By "" $p$ is conjugate to $q$ "", we mean there is a Jacobi field $J$ along a geodesic from $p$ to $q$ with $J(p) = 0$ and $J(q) = 0$ ; or equivalently, there is a proper smooth variation through geodesics from $p$ to $q$ . Furthermore, $\mathrm{inj}(p)$ denotes the injectivity radius at $p$ . What I've tried: By a previous result, assuming $q$ is not conjugate to $p$ along any minimizing geodesic, there are at least two minimizing unit-speed geodesics $\gamma_0$ and $\gamma_1$ from $p$ to $q$ . Suppose $\dot\gamma_0(b) \neq -\dot\gamma_1(b)$ . Let $\gamma : [0,2b] \to M$ be the closed loop based at $p$ given by $\gamma_0$ and (the reversed parametrization of) $\gamma_1$ , and let $\Gamma : [0,\delta] \times[0,2b] \to M$ be a smooth variation along $\gamma$ through unit-speed curves that coincide with $\gamma_0$ and $\gamma_1$ outside of an $\epsilon$ -neighborhood of $q$ , and inside this $\epsilon$ -neighborhood of $q$ , the variation field $V$ satisfies $V(b) = -\left(\dot\gamma_0(b) + \dot\gamma_1(b)\right)$ . Assume $\Gamma_0 = \gamma$ . (Essentially, we are slightly shortening and contracting the loop $\gamma$ by ""rounding the corner"" at $q$ .) One can show there is a small $s \in (0,\delta]$ such that $L_g(\Gamma_s) < L_g(\gamma) = 2b$ . Suppose the image of $\Gamma_s$ differs from the image of $\gamma$ on the interval $[b-\epsilon, a]$ for some $a \in (b-\epsilon, b+\epsilon]$ . Let $\sigma = \Gamma_s|_{[b-\epsilon,a]}$ . I've been able to show that $\sigma$ does not intersect $\mathrm{Cut}(p)$ . So $\sigma$ is a smooth curve in $M$ within the injectivity radius of $p$ . Because $\exp_p$ is a diffeomorphism near $0$ onto its image, and that image includes $\sigma$ , we can project $\sigma$ to a curve $\tilde\sigma$ on the unit sphere in the tangent space $T_pM$ . So let $\Sigma : [b-\delta, a] \times [0,b] \to M$ be the variation given by $$
\Sigma(s,t) = \exp_p\left(t\tilde\sigma(s)\right).
$$ Then $\Sigma$ is a variation through unit-speed geodesics that start at $p$ . My question: Can we show $\Sigma$ is a proper variation? If so, this would contradict our assumption that $q$ is not conjugate to $p$ along any minimizing geodesic. I know that by minimality, the first time each curve $\Sigma_s : [0,b] \to M$ can possibly intersect either $\gamma_0$ or $\gamma_1$ is at $t=b$ . Also, the curve $\Sigma(\cdot, b) : [b-\delta, a] \to M$ is a closed curve based at $q$ . So it suffices to show this curve is constant. Is there an obvious reason why this should be the case? EDIT: See comments for a solution sketch of the original problem. Constructing a proper variation through geodesics, as I was trying to do, turns out not to be the most straightforward approach; it's better to use the fact that $q$ is a regular value of $\exp_p : T_p M \to M$ if we assume $q$ is not conjugate to $p$ .","['calculus-of-variations', 'geodesic', 'riemannian-geometry', 'differential-geometry']"
3726852,Composition of local diffeomorphisms is a local diffeomorphism,"Let $F: M\rightarrow N$ , $G:N\rightarrow P$ be local diffeomorphisms, where $M,N,P$ are smooth manifolds. I would like to show that $G\circ F: M\rightarrow P$ is a local diffeomorphism. My attempt: Let $x\in M$ . Since $F:M\rightarrow N$ is a local diffeomorphism, there exists an open set $U$ of $x$ such that $F(U)$ is open in $N$ and $F|_U: U\rightarrow F(U)$ is a diffeomorphism. Similarly, since $F(x)\in N$ , there exists a neighborhood $V$ of $F(x)$ such that $G(V)$ is open in $P$ and $G|_V: V \rightarrow G(V)$ is a diffeomorphism. I thought of considering the set: $F|_U^{-1}(F(U)\cap V)$ , since this set is open in $U$ .However, I have not gotten far. May I have hints? Please do not use immersions","['smooth-functions', 'diffeomorphism', 'smooth-manifolds', 'differential-geometry']"
3726889,Dimensional analysis in combinatorics,"From HMMT: Fifteen freshmen are sitting in a circle around a table, but the course assistant (who remains to stand) has made only six copies of today's handout. No freshman should get more than one handout and any freshman who does not get one should be able to read a neighbor's. If the freshman are distinguishable but the handouts are not, how many ways are there to distribute the six handouts subject to the above conditions? The solution starts by considering the expected number of handouts to be received by any individual student. By linearity of expectation, there are 15 students and 6 handouts, so each student is expected to individually receive 6/15 handouts. Then, for an arbitrary individual student S, we compute the number of distributions of handouts in which S receives a handout, called $y$ . Also, let $x$ be the answer; $x$ is the number of ways to distribute the six handouts subject to the conditions of the problem. Now, the solution states that $y=\frac{6}{15}x \Longleftrightarrow x = \frac{15}{6}y$ , which is how we shall find the answer. This feels nearly obvious because with $y=\frac{6}{15}x$ we're multiplying the # distributions by the expected number of handouts per student, but if we apply some kind of ""dimensional analysis"" to this, the multiplication does not turn out to something like ""# distributions per student"". In particular, what would be the resulting meaning if we divided $x$ by the number of students as in $\frac{x}{15}$ , rather than dividing $x$ by the number of students, then multiplying by the number of handouts, as we do with the equation $y=\frac{6}{15}x$ ?",['combinatorics']
3726909,Inconsistent notation for vectors and points in textbooks,"Many books on calculus or advanced calculus distinguish between points and vectors. Usually points are denoted by italic letters like $P, Q$ , and $R$ , and vectors are denoted by bold letters such as $\mathbf{u}$ and $\mathbf{v}$ . And some textbooks put the components of a vector between two angle brackets, while the coordinates of points are simply placed between two parentheses. However the notation is not consistant through the books. At least I have not seen any textbook that is consistent through the text. Here are two samples: Sample 1: Marsden and Tromba in Vector Calculus As you can see from the following figure, the point $P$ is not bold but vectors $\mathbf{v}$ and $\mathbf{w}$ are bold. However, a few pages later, they denote points by bold letters So a student might ask: is $\mathbf{x}_0$ a point or a vector? Sample 2: Stewart Calculus The equation of a line passing through $P_0$ and parallel to a vector $\mathbf{v}$ is described by $$``\mathbf{r}_0=\mathbf{r}_0+t\mathbf{v},$$ where $\mathbf{r}$ is the position vector of a point $P(x,y,z)$ and $\mathbf{r}_0$ is the position vector of $P_0$ ."" So clearly here, he does not add a vector $t\mathbf{v}$ to a point $P_0$ . (I mean he could simply write the line is $\{P_0+t\mathbf{v}| t\in\mathbb{R}\}$ ). But when he talks about directional derivative, he adds a vector $\mathbf{u}$ to a point $\mathbf{x}_0$ : A student may ask: if $\mathbf{x}_0$ is a point why is it denoted by a bold letter? And what is the meaning of adding a vector to a point? Adding a vector to a point has not been defined in the textbook. What is the best and consistent notation? What are the benefits of using angle brackets and parentheses for vectors and points? How do you avoid confusing students?","['reference-request', 'notation', 'calculus', 'education', 'vector-analysis']"
3726925,One nappe of the hyperbola is an embedding. Pollack 1.3.8,"The problem asks to check that the map $f:\mathbb{R}^1\to \mathbb{R}^2$ given by $t\mapsto(\cosh(t),\sinh(t))$ is a closed embedding. I tried two different approaches to solve this problem. First , we can use the fact that $$f \text{ is a closed embedding if and only if } f(\mathbb{R}^1) \text{ is closed in }\mathbb{R}^2$$ and $f$ is a homeomorphism onto its image. We can see that $f(\mathbb{R}^1)$ is closed since its complement is open. Next, we want to show that $f$ is a homeomorphism. If we are going to look at it geometrically, then it is obvious as we can see that the image/preimage of open sets is going to be open. My first question: how to show this more precisely? I was thinking to define a function $g:f(\mathbb{R}^1)\to \mathbb{R}^1$ given by $g(x,y)=\ln(x+y)$ since $$(x,y)=(\cosh(t),\sinh(t))\text{, so } x+y=e^t\text{ i.e. }t=\ln(x+y)$$ So, we can see that $f$ and $g$ are continuous functions and inverses of each other i.e. $f$ is a homeomorphism. Does it work? (1) Second approach is to use the direct definition of the closed embedding. In other words, $$f\text{ is a closed embedding if and only if } f\text{ is an immersion and the preimage of every compact set is compact.}$$ $f$ is an immersion if the differential $df_a$ is injective for all $a\in\mathbb{R}^1$ i.e. the tangent vector is never zero. But, we can see that $df_a=\begin{bmatrix}\sinh(a)\\ \cosh(a)\end{bmatrix}$ , so $|df_a|\neq0$ To show that the preimage of a compact set is compact, I used the following. If we take any compact set $A$ in $\mathbb{R}^2$ , then we always can find a closed ball center at the origin that will contain $A$ . That closed ball will intersect $f(\mathbb{R}^1)$ at some point $(x_0,y_0)$ i.e. at some point $t_0\in \mathbb{R}^1$ as $(x_0,y_0)\in f(\mathbb{R}^1)$ . Then we can see that $$f^{-1}(A\cap f(\mathbb{R}^1))\subset[-t_0,t_0]$$ Since $A\cap f(\mathbb{R}^1)$ is closed, $f$ is continuous, and $[-t_0,t_0]$ is compact, then $f^{-1}(A\cap f(\mathbb{R}^1))$ is compact as the closed subset of the compact set. Does it work? (2)","['solution-verification', 'differential-geometry']"
3726935,Why is $|x|'=\frac{x}{|x|}$?,"Why is $|x|'=\frac{x}{|x|}$ ? May someone explain this clearly? If $x>0$ then it should be 1, and if $x<0$ then it's $-1$ ...","['calculus', 'derivatives', 'absolute-value']"
3726944,Discover and prove a theorem relating $\bigcap_{i \in J}A_i$ and $\bigcup_{X \in \mathcal{F}}(\bigcap_{i \in X}A_i)$.,"This is an exercise from Velleman's ""How To Prove It"": Suppose $\mathcal{F}$ is a nonempty family of sets. Let $I = \bigcup \mathcal{F}$ and $J = \bigcap \mathcal{F}$ . Suppose also that $J \neq \emptyset$ , and notice that it follows that for every $X \in \mathcal{F}$ , $X \neq \emptyset$ , and also that $I \neq \emptyset$ . Finally, suppose that $\{A_i | i \in I\}$ is an indexed family of sets. d. Discover and prove a theorem relating $\bigcap_{i \in J}A_i$ and $\bigcup_{X \in \mathcal{F}}(\bigcap_{i \in X}A_i)$ . After doing a few examples on paper, I decided that $\bigcup_{X \in \mathcal{F}}(\bigcap_{i \in X}A_i) \subseteq \bigcap_{i \in J}A_i$ . Here is a proof of this supposition: Proof: Let $y \in \bigcup_{X \in \mathcal{F}}(\bigcap_{i \in X}A_i)$ be arbitrary. Then we can choose an $X \in \mathcal{F}$ such that $y \in \bigcap_{i \in X}A_i$ . Now let $j \in J = \bigcap \mathcal{F}$ be arbitrary. Since $j \in \bigcap \mathcal{F}$ and $X \in \mathcal{F}$ , we must have $j \in X$ . Then since $j \in X$ and $y \in \bigcap_{i \in X}A_i$ , $y \in A_j$ . Since $j$ was arbitrary, $y \in \bigcap_{i \in J} A_i$ . Since $y$ was arbitrary, $\bigcup_{X \in \mathcal{F}}(\bigcap_{i \in X}A_i) \subseteq \bigcap_{i \in J}A_i$ . $\square$ I am struggling to understand how this is true intuitively. Right now, I am thinking of $\mathcal{F}$ as a  family of sets containing sets of indices, e.g., {{1,2}, {2,3}, {2,4}} (notice $\bigcap \mathcal{F} \neq \emptyset$ ). Then $y \in \bigcup_{X \in \mathcal{F}}(\bigcap_{i \in X}A_i)$ means that there is a set of indices in $\mathcal{F}$ such that $y$ is contained in $A_i$ for every index $i$ in that set. $y \in \bigcap_{i \in J}A_i$ means that for every index $i$ that is contained in all sets $X \in \mathcal{F}$ , we must have $y \in A_i$ . The formal proof seems to work out, but I am not seeing the relationship between these two sets clearly.",['elementary-set-theory']
3726952,"Prove: $\int_{0}^{\pi/2}{\sqrt{1 + \sqrt{1 + (\tan{x})^{2/3}}}\,dx} = \frac{\pi}{2} (3^{1/4} + 3^{3/4} - 2)$","I'm trying to prove $$I := \int_{0}^{\pi/2}{\sqrt{1 + \sqrt{1 + (\tan{x})^{2/3}}}\,dx} = \frac{\pi}{2} (3^{1/4} + 3^{3/4} - 2)$$ which is around $2.5063328837$ . Using the $u$ -substitution $u = \sqrt{-1 + \sqrt{1 + (\tan{x})^{2/3}}}$ , we have \begin{align*}
du
&= \frac{1}{6}\frac{1}{\sqrt{1 + (\tan{x})^{2/3}}}\frac{1}{\sqrt{-1 + \sqrt{1 + (\tan{x})^{2/3}}}} (\tan{x})^{-1/3} \sec^2{x}\,dx \\
&= \frac{1}{6 u (u^2 + 1)} \frac{1}{\sqrt{(u^2 + 1)^2 - 1}} \left(((u^2 + 1)^2 - 1)^3 + 1\right) \,dx \\
&= \frac{1}{6 u^2 (u^2 + 1)} \frac{1}{\sqrt{u^2 + 2}} \left(u^6 (u^2 + 2)^3 + 1\right) \,dx
\end{align*} and thus, $$I = \int_{0}^{\pi/2}{\sqrt{1 + \sqrt{1 + (\tan{x})^{2/3}}}\,dx} = 6 \int_{0}^{\infty}{\frac{u^2 (u^2 + 1) (u^2 + 2)}{u^6 (u^2 + 2)^3 + 1} \,du}$$ So we have the integral of a rational function, where the roots of the denominator can easily be found in closed form; hence using the method of residues, we can get an answer in closed form. Yet, I've done this and it's still not so obvious that the answer miraculously simplifies to $\frac{\pi}{2} (3^{1/4} + 3^{3/4} - 2)$ . My question is, seeing that the answer is so nice, is there a more clever way to solve this integral? I've tried different substitutions, integration by parts, the Feynman trick by putting a parameter in place of the exponent $2/3$ , etc., but they all seem to go nowhere. Thanks!","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
3727021,Find a power series that is convergent on the closed unit disk but diverges elsewhere.,"Question: Does there exist a power series centered at $z=0$ , $f(z)=\sum_{n=0}^\infty a_n z^n$ such that the domain of $f$ is exactly the unit disk $D^2\subset \mathbb{C}$ ? In other words, I'm looking for a power series whose radius of convergence $\rho=1$ such that the series also converges on the unit circle. Motivation: I'm thinking about a problem: ""does there exist a Laurent series that converges only on the unit circle but nowhere else?"" I realize that this problem reduces to the above question.","['complex-analysis', 'power-series', 'laurent-series']"
3727076,Conditions on the characteristic polynomial of a matrix with sines and cosines to has integer coefficients,"Let $A\in \mathsf{GL}(4,\mathbb{R})$ be the following matrix: $$A=\begin{pmatrix}
\cos a&-\sin a&0&0\\ \sin a&\cos a&0&0\\0&0&\cos b&-\sin b\\0&0&\sin b&\cos b
\end{pmatrix}$$ Assume that $A$ has finite order, i.e. $a$ and $b$ are rational multiples of $\pi$ . The problem is to give necessary conditions on the characteristic polynomial of $A$ to make it integer, i.e. $P_A(\lambda)\in\mathbb{Z}[\lambda]$ The characteristic polynomial is $P_A(\lambda)=\lambda^4-2\lambda^3(\cos a+\cos b)+\lambda^2(2+4\cos a\cos b)-2\lambda(\cos a+\cos b)+1$ (it is a symmetric polynomial). Now we want that \begin{cases}
2(\cos a+\cos b)\in\mathbb{Z}\\
4\cos a\cos b\in \mathbb{Z}
\end{cases} Question: In this case I've solved the system by hand and found the possible values of $a$ and $b$ but I wonder if there is a more efficient way to do this, or if there is some deeper theory involved, because I want to generalize to higher values (for example when there are three blocks, $a$ , $b$ , $c$ and so on). I see that in someway elementary symmetric polynomials appear but I don't know if this helps. Any comment or suggestion will be appreciated!","['matrices', 'linear-algebra', 'symmetric-polynomials', 'characteristic-polynomial']"
3727077,Is there a closed-form for $\sum_{n=0}^{\infty}\frac{n}{n^3+1}$?,"I'm reading a book on complex variables ( The Theory of Functions of a Complex Variable , Thorn 1953) and the following is shown: Let $f(z)$ be holomorphic and single valued in $\mathbb{C}$ except at a finite number of points $a_1,\ldots, a_k$ which are not integers or half-integers , where $f(z)$ has poles or essential singularities. Further, suppose there is some $d$ for which if $|z|>m(d)$ , $
|z^2 f(z)|<d
$ . Then $$
\sum_{n=-\infty}^{\infty} f(n) = -\pi \sum_{n=1}^{k} r_n \cot(\pi a_n),
$$ where $r_n$ is the residue of $f(z)$ at $z=a_n$ . Here is a sketch of the proof I supplied: the condition of no $a_n$ integers or half-integers is to prevent 'coupling' between $f$ and cotangent. Let $C_m$ be the counterclockwise rectangular contour with vertices $\pm(m+1/2)\pm m i$ ; choose $m>1$ big enough so that each $a_n$ is contained in the interior of $C_m$ and the growth condition is satisfied. Then $$
\frac{1}{2\pi i}\int _{C_m} \pi \cot(\pi z) f(z)\,dz = \sum_{n=-m}^{m}f(n) + \pi\sum_{n=1}^{k} \pi r_n \cot(\pi a_n)
$$ A brute force argument shows that along $C_m$ , $|\cot(\pi z)|<\coth(\pi m)<2$ . Now we use the growth condition: $$
\left|\frac{1}{2\pi i}\int _{C_m} \pi \cot(\pi z) f(z)\,dz\right|
\leq \frac{1}{2}\int _{C_m} \left| \cot(\pi z) f(z)\right|\,dz
$$ $$
<\int _{C_m} \left|f(z)\right|\,dz \leq (8m+2) \max_{z\in C_m} |f(z)| < \frac{(8m+2)d}{m^2}
$$ So as $m\to \infty$ , the LHS approaches $0$ and the result follows. Fine. Now, the series in my title is the second part of an exercise; the first part has you evaluate $\displaystyle{\sum_{n=0}^{\infty} \frac{1}{n^2+1}}$ , which is done by writing: $$
\sum_{n=0}^{\infty} \frac{1}{n^2+1} = \frac{1}{2}\left(\sum_{n=-\infty}^{\infty} \frac{1}{n^2+1}+1\right)
$$ $$
= \frac{1}{2}\left(-\pi \left(\frac{-i}{2}\cot(\pi i)+\frac{i}{2}\cot(-\pi i)\right)+1\right) = \frac{1+\pi\coth(\pi)}{2}
$$ But then the book claims $\displaystyle{\sum_{n=0}^{\infty}\frac{n}{n^3+1}}$ can be evaluated as well. I am skeptical because the singularity occurs at $n=-1$ , as opposed to $\pm i$ in the previous problem, and I don't think the same symmetry trick can work; further, this is the only relevant section of the chapter to this exercise. Mathematica gives the value $$
\frac{-1}{3}\sum_{k=0}^2 \frac{\psi(-\exp(2\pi i (2k+1)/6))}{\exp(2\pi i (2k+1)/6)},
$$ where $\psi$ is the digamma function, which is less than transparent, though I could buy how we could get there from the cotangent sum. I'm not sure if the sum has a nicer closed-form or if this is as best as can be done.","['complex-analysis', 'contour-integration', 'polylogarithm', 'closed-form', 'sequences-and-series']"
3727107,Show that the size of the Turan graph $T_r(n)$ is at least $(1 - \frac{1}{r}) \binom{n}{2}$.,"A Turan graph $T_r(n)$ is defined as the complete $r$ -partite graph of order $n$ such that the number of vertices in each of the $r$ classes is either $\lfloor \frac{n}{r}\rfloor$ or $\lceil \frac{n}{r} \rceil$ . For fixed $n$ and $r$ , $T_r(n)$ is unique up-to isomorphism. The size of $T_r(n)$ can be simply counted as: $\binom{n}{2} - (n \bmod r) \binom{\lceil \frac{n}{r} \rceil}{2} - (r - (n\bmod r))\binom{\lfloor \frac{n}{r}\rfloor}{2}$ . Here is what I have: assume $n = kr + s,\ 0 \leq s \leq r-1$ . Note that at least one class must have exactly $\lfloor \frac{n}{r} \rfloor$ vertices. Then, $\binom{n}{2} - (n \bmod r) \binom{\lceil \frac{n}{r} \rceil}{2} - (r - (n\bmod r))\binom{\lfloor \frac{n}{r}\rfloor}{2}$ $\geq$ $\binom{n}{2} -(r-1) \binom{\lceil \frac{n}{r} \rceil}{2} - \binom{\lfloor \frac{n}{r} \rfloor}{2}$ $=\binom{n}{2} - (r-1) \binom{k+1}{2} - \binom{k}{2}$ $= \frac{n(n-1)}{2} - \frac{(r-1)k(k+1)}{2} - \frac{k(k-1)}{2}$ $= \frac{n(n-1)}{2} - \frac{rk(k+1) - k(k+1)}{2} - \frac{k(k-1)}{2}$ $\geq \frac{n(n-1)}{2} - \frac{n(k+1) - k(k+1)}{2} - \frac{k(k-1)}{2}$ $= \frac{n(n-1)-n(k+1) + k(k+1) - k(k-1)}{2}$ $= \frac{n(n-1)-n(k+1) + 2k}{2}$ $= \binom{n}{2}(1 - \frac{k+1}{n-1} + \frac{2k}{n(n-1)})$ . But clearly, $(1 - \frac{k+1}{n-1} + \frac{2k}{n(n-1)})$ may be smaller than $1 - \frac{1}{r}$ , as seen by taking $n=31$ and $r=5$ .","['graph-theory', 'extremal-combinatorics', 'combinatorics', 'discrete-mathematics', 'extremal-graph-theory']"
3727109,"Algorithm to compute ""median""","Let $x_1,\dots,x_m\in\mathbb{R}^d$ be a finite set of points.  I define the $d$ -dimensional ""median"" $y\in\mathbb{R}^d$ to be the point minimizing the sum of distances to $x_j$ , $$
y = \arg\min_{z\in\mathbb{R}^d} \sum_{j=1}^m |z-x_j|.
$$ (Note that the median might not exist, for example if $x_j$ lie on a straight line). Is there an efficient algorithm to compute $y$ ?  For example, when $m=3$ and $d=2$ there is a construction for the Fermat point of a triangle, which is what I am calling the median here. Thanks!","['euclidean-geometry', 'statistics', 'median', 'geometry', 'calculus']"
3727116,Show that function distances are preserved,"If for all $x,y \in \mathbb{R}^n$ that satisfies $|x - y| = t$ also satisfies $|f(x) - f(y)|=t$ (for some constant $t \in \mathbb{R}^{+}$ ), show that $|f(x) - f(y)| = |x-y|$ for all values of $x,y \in \mathbb{R}^n$ . This seems very much intuitive to me and that $f(x) = x$ or that the range set is a copy of $\mathbb{R}^n$ . My initial goal was to show if $r \in l(x,y)$ , then $f(r) \in l(f(x),f(y))$ such that $|f(r)-f(z)| = |r - z|$ where $z \in \{x,y\}$ . Showing this is enough to solve the problem. The most intuitive idea, however that seems to work is creating equilateral triangles in some way and make it work. $\color{green}{\text{Edit :}}$ In $\mathbb{R}$ , by taking $f$ as the greatest integer function with a period of $t$ , we get a clear contradiction to the initial statement. The idea doesn't seem to directly work for higher dimensions, albeit the first answer claims so. Can this (or the claim in the original question) be proven for $n > 1$ ?","['combinatorial-geometry', 'analysis']"
3727155,How do I evaluate $\sum_{k = 1}^{\infty}\big[\frac{(-1)^{k - 1}}{k}\sum_{n = 0}^{\infty}\big\{\frac{1}{k(2^n) + 1}\big\}\big]$?,"Evaluate the following summation: $$\sum_{k = 1}^{\infty}\Bigg[\frac{(-1)^{k - 1}}{k}\sum_{n = 0}^{\infty}\Bigg\{\frac{1}{k(2^n) + 1}\Bigg\}\Bigg]$$ My attempts were to telescope by introducing the $k$ inside the inner summation and using partial fractions, but to no avail. I had noticed a logarithmic series across the summation but it leaves a more complicated sum. There is no algebraic identity I am aware of that simplifies the denominator, and I am unable to see a possible binomial series.","['algebra-precalculus', 'summation', 'sequences-and-series']"
3727221,Computing a line integral,"Let $C$ be the curve which obtains from the intersection of the plane $z=x$ and the cylinder $x^{2}+y^{2}=1$ , oriented counterclockwise. If $F$ is a vector field $F=(x,z,2y) \in \mathbb{R}^{3}$ , compute $$I= \oint_{C} F  \cdot dr $$ Hint: Use Stoke's Theorem. I have that, $\nabla \times F= i$ , then $$I= \int_{S} (\nabla \times F) \cdot dS= \int \int_{D} 1dA $$ where $D= \{(x,y)| \quad x^{2} + y^{2} \le 1 \}$ . Is this fine?, I don't know if I am using correctly the theorem.","['integration', 'multivariable-calculus', 'calculus', 'stokes-theorem']"
3727273,At least two points are $13$ apart,"Let $x_0,\dots,x_{37}$ be $38$ distinct integral points inside $[0,60]$ with $x_0=0$ (e.g. $0,1,2,\dots 37$ or $0,2,3,\dots 38$ , etc). Prove that there exists two points $x_i$ and $x_j$ such that $x_j-x_i=13$ . I thought of pigeonhole principle, but couldn't find a way to apply.",['discrete-mathematics']
3727287,How are the values of a function expressed in the formal language of ZFC set theory?,"I'm curious as to how the values $f(x)$ of a function $f$ , which as defined, are the corresponding unique $y$ such that $(x,y)\in f$ , are expressed formally in the language of ZFC (or an extension thereof). For example, with the help of the extension by definitions scheme, one can construct conservative extensions of ZFC with additional function symbols such as the power set operation, union, ordered pairs and Cartesian products with their respective definitional axioms. In particular, after proving $$\forall x\exists !y\forall z(\forall w(w\in z\rightarrow w\in x)\leftrightarrow z\in y)$$ in ZFC, we can add a new function symbol $\mathcal P$ to the signature and an additional axiom $$\forall x\forall z(\forall w(w\in z\rightarrow w\in x)\leftrightarrow z\in\mathcal P(x))$$ characterizing the power set of $x$ . In this case, we can work out the value of $\mathcal P(x)$ to be the unique $y$ that satisfies the former sentence. Likewise, we can add predicate symbols to the signature; for instance, the formula "" $f$ is a function"" is equivalent to $$\forall x(x\in f\rightarrow\exists y\exists z(x=(y,z)))\land\forall x\forall y\forall z(((x,y)\in f\land (x,z)\in f)\rightarrow y=z)$$ assuming we have extended the theory to capture the notion of ordered pairs. Moreover, we can add a ternary predicate so that the formula $f:X\to Y$ has its intended meaning in our supertheory, in a similar fashion. The problem however occurs where we define the term $f(x)$ as the unique $y$ such that $(x,y)\in f$ . How is this done formally? Do we add a new function symbol $f$ as a means to extend our supertheory so that $f(x)$ becomes a term? That can't be the case since $f$ would otherwise be undefined for most of the domain of discourse contrary to the behavior of function symbols in the signature such as $\mathcal P$ . Sure, the notation for $f(x)=y$ can be dismissed as an abbreviation for $(x,y)\in f$ , but there are formulas that treat the notation differently where $f(x)$ is treated as a legitimate term, such as the formal statement of AC : $$\forall X(X\ne\emptyset\rightarrow\exists f(f:X\to\bigcup X\land\forall A(A\in X\rightarrow f(A)\in A)))$$ which just can't be a well-formed formula unless $f$ is a function symbol. My thoughts are that in this example, $f(A)\in A$ could very well be just an abbreviation for $\forall z((A,z)\in f\rightarrow z\in A)$ . Does this suffice for everytime $f(x)$ is treated as a legitimate term?","['formal-languages', 'first-order-logic', 'functions', 'set-theory']"
3727330,Can you win the urn depletion game?,"In the urn depletion game , you are given several transparent urns containing various colored balls. (For the purposes of this problem, let us assume there are $k=2$ different available ball colors, red and blue.) You can easily see all of the contents of all of the urns, and pick out any ball from them at will. You win the game if you can remove all of the balls from the urns, subject to the following constraints: You may only remove one ball at a time. You may not pick from the same urn twice in a row. I will tell you, each time, what color of ball you must remove. Concretely, assume I give you a list in advance describing what color you must pick out each turn. The decision problem is: Given a setup of urns and colored balls, and given the ordered list of color requirements, is it possible to win? Example: You have urns containing [RB] [RB].  If the instructions are to remove them in the order red, blue, blue, red you can win. In contrast, if you must remove them in the order red, blue, red, blue, there is no way to win because you cannot draw from the same urn twice in a row. I am wondering whether this problem is in P, or whether, for example, it's NP-complete. It's a little similar to some other NP-complete problems, but it also seems at least superficially less expressive and I haven't been able to find a reduction. I've found several special cases that are in P. I know that if there's only one ball color ( $k=1$ ), then the problem is in P. My algorithm is to always remove a ball from the urn with the most balls (among the urns you're allowed to pick), breaking ties arbitrarily. If it's possible to win, this algorithm will win. (Note that it's still possible to have an unwinnable game even if $k=1$ , if there's too great a discrepancy in urn contents. For example, the game [R] [RRRR] is unwinnable.) I also know that if all balls have a unique color, then the problem is also in P. This is because then the color list uniquely determines the path you take (no branching factors), and you can check whether it's valid in polynomial time. More generally, if the color of the ball uniquely determines the urn it's in, then the problem is in P. And if there are only two urns, then no matter how many colors $k$ there are, the path must zigzag between them, and there are only two possible paths. You can check in polynomial time whether either path is legal. But I haven't solved the $k=2$ case, and I'm stumped on an algorithm or reduction. Edit: I've found if we allow for an unlimited number of colors, the problem becomes NP-complete, but I'm not  sure about just two colors. Edit: As @Artimis points out below, if we restrict to problems where the number of urns is at most U, or where the number of balls per urn is at most B, this special case can be shown to be in P, as there are a polynomial number of things to check. Hence if we are to show that the problem is NP-complete, the reduction must reasonably employ an unbounded number of urns and balls per urn.","['np-complete', 'computational-complexity', 'combinatorics', 'combinatorial-game-theory']"
3727356,Total Variation Distance between two uniform distributions,"Two distributions with $P=Unif([0,s])$ and $Q=Unif([0,t])$ where $0<s<t$ I have the general formula and use the uniform pdf for P and Q $$TV(P,Q) = 1/2 \int_{x\in E} |p_{\theta}-p_{\theta'}| dx$$ $$= 1/2 \int_{x\in E} |\frac{1}{s}-\frac{1}{t}| dx$$ Now I am having trouble with integrating. Which space do I integrate on, from s to t, since we want the distance of the two pdfs? And if so, how do I proceed?","['statistics', 'total-variation', 'probability']"
3727379,Is the Jordan normal form uniquely determined by the characteristic and minimal polynomial?,"I was looking into this answer to a question about obtaining the Jordan normal form given the characteristic and minimal polynomials of a matrix. In this answer, it is stated that ""The multiplicity of an eigenvalue as a root of the characteristic polynomial is the size of the block with that eigenvalue in the Jordan form. The size of the largest sub-block (Elementary Jordan Block) is the multiplicity of that eigenvalue as a root of the minimal polynomial"". I was then thinking of examples of matrices to apply this to, and I came up with the example of a matrix with characteristic polynomial $f(x) = (x-1)^4(x+1)$ and minimal polynomial $m(x) = (x-1)^2(x+1)$ . Using the method described in the answer, I know that the largest elementary Jordan Block for the eigenvalue $1$ should be of size $2$ . But given this, I can make $2$ distinct Jordan blocks for the eigenvalue $1$ : $$\begin{pmatrix}
1&1&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
\end{pmatrix} \qquad \text{and} \qquad \begin{pmatrix}
1&1&0&0\\
0&1&0&0\\
0&0&1&1\\
0&0&0&1\\
\end{pmatrix}
$$ where the first Jordan block has one elementary block of size $2$ and $2$ elementary blocks of size $1$ , and the second Jordan block is made up of $2$ elementary blocks, each one of size $2$ . Do the characteristic and minimal polynomial always uniquely determine the Jordan normal form? In which case my understanding is wrong, and I would ask if someone could tell me what am I missing. Or alternatively, when do the characteristic and minimal polynomial uniquely determine the Jordan normal form? Thank you!","['jordan-normal-form', 'matrices', 'minimal-polynomials', 'linear-algebra', 'characteristic-polynomial']"
