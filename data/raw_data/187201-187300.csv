question_id,title,body,tags
3478791,"Is there a way to prove that $x > \sin^2(x) , x >0$","The inequality of $$ x > \sin^2(x), \forall x \in  ( 0 , + \infty) $$ can be visualized, and easily, we can prove that it's true (via the graph). However, how can we prove that it's true algebraically?",['algebra-precalculus']
3478792,Jacobian and changing coordinates proof [duplicate],"This question already has an answer here : Wedge product and change of variables (1 answer) Closed 4 years ago . Consider a transformation between the coordinates given by $x^{a}$ to another system given by coordinates $x^{'a}$ . So a transformation of the type: $x^{a} \rightarrow x^{'a}$ . With that I can construct the Jacobian matrix: $\bigg[ \frac{\partial x^{'a}}{\partial x^{b}} \bigg]$ The determinant of this matrix is called the Jacobian determinant, hereafter J. I want to prove that: $dx^{'1}dx^{'2}...dx^{'N} = Jdx^{1}dx^{2}...dx^{N}$ where primed system is cartesian and the unprimed one is a general set of coordinates. How can I do that? I remembered that in transformation of coordinate is valid to write (using the summation convention) that: $dx^{'a} = \frac{\partial x^{'a}}{\partial x^{b}}dx^{b}$ And tried to apply that to proof that I need, but, I was not abble to go anywhere because of lot of terms shows up. 
Actually, in the case we considerer just 2 dimensions I ended up with something like that: $dx^{'1}dx^{'2} =  \bigg( \frac{\partial x^{'1}}{\partial x^{1}} \frac{\partial x^{'2}}{\partial x^{1}} \bigg)(dx^{1})^{2}+ \bigg( \frac{\partial x^{'1}}{\partial x^{2}} \frac{\partial x^{'2}}{\partial x^{2}} \bigg)(dx^{2})^{2} + \bigg[\bigg( \frac{\partial x^{'1}}{\partial x^{1}} \frac{\partial x^{'2}}{\partial x^{2}} \bigg) + \bigg( \frac{\partial x^{'1}}{\partial x^{2}} \frac{\partial x^{'2}}{\partial x^{1}} \bigg)\bigg]dx^{1}dx^{2} $ As is possible to see, the last two therms inside the brackets are almost the Jacobian determinant. So my question is: what is wrong in this approach? I did found this article with a different ideia of the proof. But why should I use that instead of the mine? And also: how could I generalize the ideia of this article for N-dimensions?","['analytic-geometry', 'differential-geometry', 'riemannian-geometry', 'algebraic-geometry', 'exterior-algebra']"
3478898,"Limit of integral, show limit exists and compute it","Suppose $f : [0,\infty) \rightarrow \mathbb{R}$ is continuous and such that $|f(x)| \leq 1 + x^{2}, \forall x \geq 0$ . Then how do I show that: $L = \lim_{n \rightarrow \infty} \int_{0}^{n} \frac{f(x/n)}{(1+x)^{4}} dx$ exists and how do I compute it? If I give a bound for it, I obtain that $|L| \leq \lim \int_{0}^{n} \frac{1+(x/n)^{2}}{(1+x)^{4}} dx = \lim_{n \rightarrow \infty} \frac{1}{3} - \frac{1}{3(n+1)^{3}} + \frac{-3n^{2} - 3n - 1 + (n+1)^{3} }{3n^{2}(n+1)^{3}}$ . Taking the limit as $n$ goes to infinity gives that $|L| \leq \frac{1}{3}$ . However, this bound does not hold for all $f(x)$ , as $f(x) = 0$ gives $L = 0$ . I am assuming that the limit depends on $f$ , but I don't know how to proceed.","['integration', 'limits', 'sequences-and-series', 'real-analysis']"
3478933,Area Between two Planes,"As a pool contractor I am searching for a better way to calculate required excavation and fill. Both pool floor and existing grades are represented by a simple slope, but they are not aligned. By this I mean the pool floor might slope at a right angle compared to the existing grades slope. I want to treat both the pool floor and existing grade as a two planes. Let pool depth be 3' to 5' and 20'length x 10'width...
  Problem A: Existing grade slopes from 4' to 6' over the same distance same direction.
  Problem B: Existing grade slopes from 2' to 4' over the same distance same direction.
  Problem C: Same as problem A but the existing grade slopes at a different angle (or direction) I am looking for formulas that will provide me the amount of earth to excavate and fill required.",['geometry']
3479002,Why can't I use Fermat's Small Theorem to calculate $12^{12^{12}} \pmod{13}$?,"The answer says it is 1, and I understand why. But why can't I use Fermat's Little Theorem on the exponent like this (by ""exponent"" I mean the entire $12^{12}$ ): $$12^{12} \equiv 1 \pmod{13}$$ Is this not a correct use of Fermat's small theorem? In such case the original problem becomes $$12^1 \equiv 12 \pmod{13}$$ But this is clearly the wrong answer to $12^{12^{12}} \pmod{13}$ .","['modular-arithmetic', 'discrete-mathematics']"
3479003,How to show that $\lim_{n\to\infty}\mathbb{P}\left(\bigg|\frac{1}{n}S_n-f(n)\bigg|>\varepsilon\right)=0$,"Consider a collection of independent events $(X_i)$ with $\mathbb{I}_{X_i}$ being the indicator random variable for $X_i$ . Let $$f(n)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{P}(X_i)\quad\text{and}\quad S_n=\sum_{i=1}^{n}\mathbb{I}_{X_i}.$$ I'm interested in showing that $$\lim_{n\to\infty}\mathbb{P}\left(\bigg|\frac{1}{n}S_n-f(n)\bigg|>\varepsilon\right)=0,\quad\forall\varepsilon>0$$ (in other words show convergence in probability). My second attempt: (using the hints given in the comments below) Let $\varepsilon>0$ . $$\lim_{n\to\infty}\mathbb{P}\left(\bigg|\frac{1}{n}S_n-f(n)\bigg|>\varepsilon\right)=\lim_{n\to\infty}\mathbb{P}\left(\frac{1}{n}\bigg|\sum_{i=1}^{n}\big[\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big]\bigg|>\varepsilon\right)$$ Let us use the Chebyshev's inequality, i.e., $\mathbb{P}(|Y|\geq a)\leq\frac{\mathbb{E}(Y^2)}{a^2}$ which leads to \begin{align}
\lim_{n\to\infty}\mathbb{P}\left(\frac{1}{n}\bigg|\sum_{i=1}^{n}\big[\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big]\bigg|>\varepsilon\right)& \leq\lim_{n\to\infty}\frac{\mathbb{E}\bigg(\frac{1}{n^2}\big(\sum_{i=1}^{n}\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big)^2\bigg)}{n\varepsilon}\\
& =\lim_{n\to\infty}\sum_{i=1}^{n}\frac{\mathbb{E}\bigg(\big(\sum_{i=1}^{n}\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big)^2\bigg)}{n^2\varepsilon}.
\end{align} Can we argue that as the term $\sum_{i=1}^{n}\mathbb{I}_{X_i}-\mathbb{P}(X_i)$ is just a summation of numbers, the expectation is constant and so taking the limit of $\frac{\text{constant}}{n^2}$ is equal to zero? I'd appreciate any hints or help.","['solution-verification', 'probability-theory', 'probability']"
3479022,Understanding Spectral Theorem,"I got the proof from mentioned here . Spectral Theorem : Let $T$ be a normal operator on a finite-dimensional complex inner product space $V$ or a self-adjoint operator on a finite-dimensional real inner product space $V$ .
  Let $c_1, \dotsc, c_k$ be the distinct characteristic values of $T$ .
  Let $W_j$ be the characteristic space associated with $c_j$ and $E_j$ the orthogonal projection of $V$ on $W_j$ .
  Then $W_j$ is orthogonal to $W_i$ when $i \neq j$ , $V$ is the direct sum of $W_1, \dotsc, W_k$ , and $$
  T = c_1 E_1 + \dotsb + c_k E_k$$ Proof : Let $\alpha$ be a vector in $W_j$ , $\beta$ a vector in $W_i$ , and suppose $i\neq j$ . Then $c_j⟨\alpha,\beta⟩=⟨T\alpha,\beta⟩=⟨\alpha,T^*\beta⟩=⟨\alpha,\overline{c_j}\beta⟩$ . Hence $(c_j-c_i)⟨\alpha,\beta⟩=0$ , and since $c_j-c_i\neq 0$ , it follows that $⟨\alpha,\beta⟩=0$ . Thus $W_j$ is orthogonal to $W_i$ when when $i\neq j$ . $\color{red}{\text{From the fact that } V \text{ has an orthonormal basis consisting of characteristic vectors, it}}$ $\color{red}{\text{follows that }V=W_1+\cdots+W_k}$ . If $\alpha_j$ belongs to $V_j(1\leq j\leq k)$ and $\color{red}{\alpha_1+\cdots+\alpha_k\stackrel{?}{=}0}$ , then \begin{align}
0&\color{red}{\stackrel{?}{=}}(\alpha_i,\sum_j\alpha_j)=\sum_j(\alpha_i,\alpha_j)\qquad(1)\\
&=\|\alpha_i\|^2
\end{align} For every $i$ , so that $V$ is the direct sum of $W_1,\cdots,W_k$ . Therefore $\color{red}{E_1+\cdots+E_k=I}$ and \begin{align}
T&=TE_1+\cdots+TE_k\\
&=c_1E_1+\cdots+c_kE_k
\end{align} I am not a follower of Hoffman and Kunze's linear algebra but for seeking my proof of Spectral Theorem I read that part. And I didn't manage to understand the red marked line. Like: $(1)$ Why $V$ space can be written as a sum of the $W_i$ space $?$ $(2)$ Why the sum of the vectors $\alpha_i$ is zero $?$ $(3)$ Why the equality hold for $(1)?$ $(4)$ Why the sum of eigenvectors $E_i$ is $I?$ Maybe all of my question require another reference theorem but I really want to know all of those Why . Thanks for your time and thanks in advance.","['inner-products', 'linear-algebra']"
3479036,Fourier Transform of indicator function of a cuboid,"In a proof of the Riemann-Lebesgue lemma I encountered the Fourier transform of the characteristic function $f$ of a cuboid $\prod_{k = 1}^{n} [a_k, b_k]$ . My lecture notes claim that $$
\mathcal{F}(f(\xi))
=\frac{1}{(2 \pi)^{\frac{n}{2}}} \prod_{i = 1}^{n} \frac{e^{-i \langle b_i, \xi \rangle} - e^{-i \langle a_i, \xi \rangle}}{i \xi_i}.
$$ Besides the horrible misuse of $i$ as index (I presume the $i$ in the denominator is the imaginary unit) I don't think this is correct, as for one, $a_i, b_i \in \mathbb{R}$ , so taking their scalar product with $\xi \in \mathbb{R}^n$ doesn't make so much sense.
Also, this doesn't work for $\xi_k = 0$ . If $\xi_k = 0$ for some $k \in \{1, \ldots, n\}$ , the corresponding factor is just $1$ , right? My approach to calculate $\mathcal{F}(f(\xi))$ would be as follows \begin{align}
(2 \pi)^{-\frac{n}{2}}\int_{a_1}^{b_1} \ldots \int_{a_n}^{b_n} e^{-i \langle x, \xi \rangle} d x_1 \ldots d x_n
& = (2 \pi)^{-\frac{n}{2}} \int_{a_1}^{b_1} \ldots \int_{a_n}^{b_n} \exp\left(-i \sum_{j = 1}^{n} x_j \xi_j\right) d x_1 \ldots d x_n \\
& = (2 \pi)^{-\frac{n}{2}} \prod_{k = 1}^{n} \int_{a_k}^{b_k} e^{-i x_k \xi_k} d x_k \\
& = (2 \pi)^{-\frac{n}{2}}\prod_{k = 1}^{n} \frac{i(e^{-i b_k \xi_k} - e^{-i a_k \xi_k})}{\xi_k}.
\end{align} Questions Is the result from the lecture notes correct? If not, is it just a typo that can be easily fixed? Is my calculation correct?","['fourier-analysis', 'proof-explanation', 'fourier-transform', 'solution-verification', 'functional-analysis']"
3479061,Why don't we study 'metric vector spaces' on their own?,I recently took a mastercourse on functional analysis and I was wondering why we 'skip' the metric structure on vector spaces. I have seen that $$\{\text{vector spaces}\}\supsetneq\{\text{topological vector spaces}\}\supsetneq\{\text{locally convex vector spaces}\}\supsetneq\{\text{normed vector spaces}\}\supsetneq\{\text{innerproduct spaces}\}.$$ Isn't it natural to include vector spaces endowed with a metric (that may not be induced by a norm) in this sequence? I can't seem to find any literature about it.,"['vector-spaces', 'category-theory', 'metric-spaces', 'functional-analysis', 'intuition']"
3479075,Is this a correct statement: $\sup \omega = \omega + 1$?,"I'm sorry if this is very obvious, but I am re-reading Rudin's Real Analysis, and thinking of $\omega$ . Would it be correct to say that the supremum of $\omega$ is $\omega + 1$ ?  And if so, how can I say $\omega$ is an unbounded set while also saying that it has a least upper bound in the ordinals? What's the best way to say this? Edit: $\omega = \{0, 1, 2, \dots \}$ $\omega + 1 = \{0, 1, 2, \dots, \omega \}$","['elementary-set-theory', 'real-analysis']"
3479089,Question about martingale convergence theorem,"Q) Let $X_n$ be a submartingale with $\sup X_n<\infty$ . Let $\xi_n = X_n - X_{n-1}$ and let $E(\sup \xi_n^+)<\infty$ . Show that $X_n$ converges a.s. Martingale convergence theorem: If $X_n$ is a submartingale with $\sup EX_n^+<\infty$ , then $X_n$ converges a.s. If I take $T = \inf\{n:X_n\geq K\}$ , then $$\begin{align}
X_{n\wedge T}&\leq K + \sup\xi_n^+ \\
E(X_{n\wedge T})&\leq K + E(\sup\xi_n^+)<\infty \\
\implies \sup E(X_{n\wedge T}) &< \infty
\end{align} 
$$ But how can I claim that $\sup E(X_{n\wedge T}^+) < \infty$ , so that I can say $X_{n\wedge T}$ converges a.s. by Martingale convergence theorem? If I have that, $X_n$ converges a.s. on $\{T=\infty\} = \{\sup X_n\leq K\}$ which would be true if I choose $K = \sup X_n +1 $ .","['martingales', 'probability-theory', 'real-analysis']"
3479170,How to prove if $P(A) = P(B) \cap P(C)$ then $A = B \cap C$,"Given three sets $A,B,C$ please help me to prove that if $P(A) = P(B) \cap P(C)$ then $A = B \cap C$",['elementary-set-theory']
3479228,What is the distribution of $X\vert Y$ where $Y$ is Bernoulli R.V?,"On good days customers arrive at an infinite server queue
  according to a Poission process with rate $12$ per hour, whereas on other days
  they arrive according to a Poisson process with rate $4$ per hour. The service times,
  on all days, are exponentially distributed with rate $1$ per hour. Every day at time $10$ hours the system is shut down and all those presently in service are forced to leave
  without completing service. Suppose that each day is, independently, a good day
  with probability $0.5$ and that we want to use simulation to estimate $\theta$ , the mean
  number of customers per day that do not have their services completed. My question regards only in finding $\mathbb{E}[X|Y]$ where $X$ denotes the number of costumers that do not have their services completed on any day and $Y$ denotes whether the day is good or ordinary. Therefore $Y$ is Bernoulli. (The hint given is that $X|Y$ follows a Poisson). I was not able to find an explicit form for the distribution of $X$ given $Y$ , but my reasoning in finding an approximate value of such goes as follows: The average number of people that arrive per hour on a good day is $12$ and they stay in the system approximately $1$ hour each. On average $108$ people visit the server. Suppose now that we allow people in the server at time $10$ , as soon as they walk in the server closes. But the average of people that walk is at $t=10$ is $12$ , therefore $$\mathbb{E}\left[X|Y=0\right]\approx12$$ and by the same reasoning $$\mathbb{E}\left[X|Y=1\right]\approx4$$ where $Y=0$ denotes that the day is gonna be good with probability $1/2$ and $Y=1$ denotes that it is going to be bad with probability $1/2$ .
My question is, how would I find the exact value of such expectations? Is it possible without any prior knowledge of stochastic processes? UDATE: After some research I found that $$\mathbb{E}\left[X\vert Y=0\right]=12\left(1-e^{-10}\right)$$ and $$\mathbb{E}\left[X\vert Y=1\right]=4\left(1-e^{-10}\right)$$ Which indeed agrees with my approximation. I am wondering how this answers was derived, as the source I found it on does not give any motivation.","['conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
3479232,Counting problem- Why is it wrong?,"Here's a problem from my textbook: On the island of Mumble, the Mumblian alphabet has only 5 letters, and every word in the Mumblian language has no more than three letters in it. How many words are possible if letters can be repeated? I know we can break this problem into cases and the answer is $155$ . But I want to know why my approach is incorrect. My Approach: For the first letter, there are 5 ways, since we can only have letters.
For the second and third letters, we have $6$ ways since we can also have no letters. So the final answer becomes $5*6*6$ .
Why am I getting the wrong answer?",['combinatorics']
3479248,Could this conjecture be proved ? (sum of even powers of cotangents in arithmetic progression ),"Having tried (in vain) to answer this question , I worked the explicit formulae of $$\color{blue}{S_k=\sum _{n=1}^m \Big[\cot \left(\frac{n \,\pi  }{2 m+1}\right)\Big]^k}$$ where $k$ is an even integer. To my surprise, the CAS I used were able to produce explicit results only for $k=2$ and $k=4$ . All the remaining was done by hand using pen and paper. What I noticed is that apparently $$\color{red}{S_{2k}=\frac{m (2 m-1)}{a_k} \left( \sum_{n=0}^{2(k-1)} b_n\, m^n \right)}$$ where the $a_k$ 's $$\color{blue}{\{3,45,945,14175,467775\}}$$ are the first terms of sequence $A171080$ in $OEIS$ : $$a_k = \prod_{q\;\text{odd prime}\\ \;\;\leq 2k+1}q^{\lfloor 2k/(q-1) \rfloor}$$ Concerning the coefficients $b_n$ , they are given in the following table $$\color{blue}{\left(
\begin{array}{cc}
 k & \text{list of the } b_n \\
 2 & \{1\} \\
 4 & \{-9,10,4\} \\
 6 & \{135,-252,8,112,32\} \\
 8 & \{-1575,3834,-1388,-2248,496,864,192\} \\
 10 & \{42525,-122760,77040,70400,-57920,-38720,14720,14080,2560\}
\end{array}
\right)}$$ My questions are : Why cannot we compute explicit forms for odd values of $k$ ? Could this conjecture be proved ? What are these polynomials ? Could we find their generic formula ?","['trigonometry', 'closed-form', 'polynomials', 'summation']"
3479273,How close are the closests cells of the same color in a periodically colored grid?,"In a square grid, if we have a coloring of the form $c(x, y) = (x + ny) \bmod m$ , what is the minimum (positive!) taxicab distance (i.e. sum of absolute value fo coordinates) between different cells of the same color? (In this example I colored all values except for 0 the same color. We are interested in the distance between yellow cells.) This is the same as minimizing the following function, $$d(m, n) = |mk + n\ell| + |\ell|$$ for fixed $0 \leq m < n$ , and $k, \ell$ are integers that can be chosen freely (not both 0). Ideally, I would like a formula for the minimum value of $d$ in terms of $m$ and $n$ . For the example shown above, $m = 7, n = 3$ , and we find the minimum of $d$ to be $3$ (with $k = -1$ and $\ell = 2$ ). It looks like this should be very easy but I find it tricky in the general case. Background: I came across this question: Minimum colors needed to color Z2 with connected subsets restriction , where a specific instance of this problem is used in the answer. This is also related to another question I asked: What is the minimum distance between vertices on an integer grid with the form $(m(m+2), 0)p + (m, 1)q$ ? (Although in that question the Euclidean distance rather than the taxicab distance is being minimized.) Update: I wrote a program to calculate the value of $d(m, n)$ . There are obviously patterns, although I have not worked out exactly what. Here is the same data arranged in a triangle; obviously factors play a role. One interesting observation: the maximum value in each row (for fixed $m$ ), is roughly $\sqrt{2m}$ , and in fact exactly $\sqrt{2m}$ for $m = 2, 8, 18, 32, ...$ (whenever $m$ is double a perfect square).","['discrete-geometry', 'combinatorics', 'integer-programming']"
3479275,How to write the inverse matrix $A^{-1}$ as a polynomial in $A$?,"I came a cross a question that gives a matrix $A$ , and asks to write $A^{-1}$ as a polynomial in $A$ with real coefficients. I don't know what this means, and googling didn't clarify very much. I found the inverse $A$ , but I do not know what it means to write it as a polynomial in $A$ .",['linear-algebra']
3479352,Is it possible to use set-builder notation to define the reals using the complex numbers?,"The set of complex numbers can be defined using the reals: $$\mathbb{C}=\{a+bi\,|\,a,b\in\mathbb{R}\}.$$ Could I do the opposite and define the reals using the complex numbers? $$\mathbb{R}=\{z\,|\,z\in\mathbb{C},\,\operatorname{Im}(z)=0\}.$$","['elementary-set-theory', 'real-numbers', 'complex-numbers']"
3479360,Proving a function is continuous everywhere,"Suppose $f:\mathbb{R} \rightarrow \mathbb{R}$ continuous at 0, and $f(a+b) = f(a) + f(b) \forall a,b \epsilon \mathbb{R}$ . Show that f is in fact continuous everywhere. Let $\epsilon > 0$ .
There exists $\delta_1>0$ such that $|x - 0| < \delta_1 \rightarrow |f(x) - f(0)|<\epsilon$ . We want to show that, for c other than 0, there exists $\delta>0$ such that $|x-c|<\delta \rightarrow |f(x) - f(c)|<\epsilon$ . We note that $|f(x) - f(c)| = |f(x-c)|$ . 
Let $y = x-c$ . Then, we want to prove $|y| < \delta \rightarrow |f(y)| < \epsilon$ .
But, we set $\delta = \delta_1$ .
Thus, $|f(y)| = |f(x-c)| < \epsilon.$ Thus $f(x)$ is continuous everywhere. Is this proof correct? If not, I would appreciate guidance towards the right direction.","['continuity', 'real-analysis']"
3479363,A magnificent series for $\pi-333/106$,"Stated here without proof is the magnificent series $$\frac{48}{371} \sum_{k=0}^\infty \frac{118720 k^2+762311 k+1409424}{(4 k+9) (4 k+11) (4 k+13) (4 k+15) (4 k+17) (4 k+19) (4 k+21) (4 k+23)} \\=\pi-\frac{333}{106},$$ which proves that $\pi>333/106$ . I can only assume that the series is proven using the integral $$\pi-\frac{333}{106}=\frac{1}{530}\int_0^1 \frac{x^5(1-x)^6(197+462x^2)}{1+x^2}dx.$$ My attempts have been so far to split up the integral as $$\begin{align}
530J&=\int_0^1 \frac{x^5(1-x)^6(197+462x^2)}{1+x^2}dx\\
&=197\int_0^1 \frac{x^5(1-x)^6}{1+x^2}dx+462\int_0^1\frac{x^{7}(1-x)^6}{1+x^2}dx\\
&=197J_1+462J_2.
\end{align}$$ Each remaining integral is turned into a series with $$\frac1{1+x^2}=\sum_{n\ge0}(-1)^n x^{2n}$$ so we have two series of the form $$f(p)=\sum_{n\ge0}(-1)^n\int_0^1 x^{p+2n}(1-x)^6dx=720\sum_{n\ge0}(-1)^n\frac{(p+2n)!}{(p+2n+7)!}.$$ Each factorial term is rewritten as $$\frac{s!}{(s+7)!}=\frac1{(s+1)(s+2)(s+3)(s+4)(s+5)(s+6)(s+7)},$$ so that $$f(p)=720\sum_{n\ge0}\frac{(-1)^n}{\prod_{k=1}^{7}(2n+p+k)}.$$ Then $$J_1=f(5)\\ J_2=f(7).$$ But how does one get from $530J=197f(5)+462f(7)$ to the series in question? Furthermore, how do we prove that $J=\pi-333/106$ ? I would assume that one would at some point use the binomial theorem then be left with a bunch of integrals like $$\int_0^1\frac{x^qdx}{1+x^2}$$ which I suppose are evaluable in terms of $\pi$ , but it seems like a great deal of cancellation/simplification would have to occur and I do not immediately see where this would happen. There has to be an easier way. Thanks!","['integration', 'real-analysis', 'pi', 'sequences-and-series', 'constants']"
3479388,"Counterintuitive result, Expected Value of Uniform Random Variable raised to increasing powers.","Out of curiosity, I've been playing with some simulations to simulate compounded interest rates from markets (such as stocks and cryptocurrencies). Let $r \sim \mathcal{U}(0.90,1.05)$ , be the return (as a percentage) of a single transaction. Let's say a stock was bought and sold, $r$ is the gain or loss for that particular transaction. I assumed a pessimistic distribution, $10\%$ loss and $5\%$ gain are the boundaries of the uniform distribution. The expected value for a single transaction is $\mathbb{E}[r]=0.975$ . I was curious to see the expected rate of return over multiple transactions, let's say $k$ transactions. This means I have to calculate $\mathbb{E}[r^k]$ . I do not know how to calculate $\mathbb{E}[r^k]$ manually and would appeciate if someone could teach me how. All I know is that it is not equal to $\mathbb{E}[r]^k$ . I used software to calculate $\mathbb{E}[r^k]$ for $k\in \{1,\dots,50\}$ , and I was surprised by the result. I plotted the result below. My intuition says that since $\mathbb{E}[r]=0.975$ then it is a losing game, and eventually would ruin the player, and indeed the plot shows increasing loss initially. However, I am baffled to see how it eventually became a positive rate of return. I cannot explain this neither intuitively nor do I have enough mathematical background in this area to reason about it. I appreciate your valuable insight!","['statistics', 'probability']"
3479464,Expected time to convergence,"Consider the following process: we place $n$ points labelled $1...n$ uniformly at random on the interval $[0,1]$ . At each time step, two points $i, j$ are selected uniformly at random and $i$ updates its position to be a point chosen uniformly at random in the interval between the positions of $i$ and $j$ (so the interval $[p(i),p(j)]$ if $p(i) < p(j)$ or $[p(j),p(i)]$ otherwise, where $p(x)$ denotes the position of the point labelled $x$ ). What is the expected time until all points are within distance $\varepsilon$ of each other for some fixed $\varepsilon > 0$ ? What is the expected time until all points are either to the left or right of $\frac{1}{2}$ ? Asymptotic bounds are also very interesting to me.","['random', 'probability', 'random-variables']"
3479547,Expected payout from two banks with limited money based on fair coin toss,"Suppose there are 2 banks with equal money units. A fair coin toss decides which bank loses 1 unit of money to the tosser i.e. if result is heads, Bank A loses 1 unit and if the result is tails, Bank B loses 1 unit. Bank A always wins with heads and bank B always wins with tails. The tosser is a third party in the game. The game stops when a bank runs out of money and then loses the toss. What is the expected payout of the tosser?","['expected-value', 'probability-theory', 'probability']"
3479552,What does it mean for a variety to be defined over a number field $K$,"Say I have the affine variety $X=\{y^2=\sqrt2x\}\subset\mathbb{C}^{2}$ . Clearly $X$ is ""definable"" over the field $K=\mathbb{Q}(\sqrt2)$ , and not over $\mathbb{Q}$ . My question is, how do I define this in algebraic geometry form? At first glance it seems something like ""there exists a morphism $X\to spec(K) $ which is locally of finite type"", but really this definiton doesn't seem that good, or even very comprehensible. My question is: Given a general variety (scheme) $X$ , what does it mean in algebraic geometric terms that $X$ is ""definable"" over $K$ ?","['number-theory', 'algebraic-geometry', 'diophantine-equations']"
3479574,Are Banach norms Fréchet differentiable?,"Suppose $(V, \|\cdot\|_V)$ and $(W, \|\cdot\|_W)$ are two Banach spaces and $f: V \to W$ is some function. We call a bounded linear operator $A \in B(V, W)$ Fréchet derivative of $f$ in $x \in V$ iff $$\lim_{h \to 0} \frac{\|f(x + h) - f(x) - Ah\|_W}{\|h\|_V} = 0$$ We call a $f$ Fréchet differentiable in $x$ iff there exists a Fréchet derivative of $f$ in $x$ . My question is: Suppose $(V, \|\cdot\|_V)$ is a Banach space. $f: V \to \mathbb{R}, v \mapsto \|v\|_V$ . Is it true, that $f$ is Fréchet differentiable $\forall x \in V \setminus \{0\}$ ? This statement is indeed true in the specific case, when $V$ is a Hilbert space. Proof: One can manually check, that $h \mapsto \frac{h}{2\sqrt{x_0}}$ is a Fréchet derivative for $x \mapsto \sqrt{|x|}$ in $x_0 \neq 0$ . One can also manually check, that $h \mapsto 2\langle v, h \rangle_V$ is a Fréchet derivative for $x \mapsto \langle x, x \rangle_V$ in all $v \in V$ . And it is a well known fact, that the composition of Fréchet derivatives of two functions is a Fréchet derivative of their composition. Thus, as $\|v\|_V = \sqrt{\langle v, v \rangle_V}$ , we have, that $h \mapsto \ \frac{\langle v, h \rangle_V}{\|v\|_V}$ is a Fréchet derivative of $\|v\|_V$ in all $v \in V \setminus \{0\}$ .","['banach-spaces', 'normed-spaces', 'frechet-derivative', 'linear-algebra', 'functional-analysis']"
3479617,Flux and Stokes Theorem,"I am trying to evaluate the flux of a cylinder $C$ which is closed at either the top or bottom using Stokes. The actual integral is $\iint_{\partial S} \mathrm{curl}\ F\ dS$ and using Stokes I can break the surface down into the outer part of the cylinder and either the bottom disk or the top disk and evaluate $\int F \cdot dr$ . However, since the bottom disk has normal down and the top disk has normal up, wouldn't that lead to two different orientations and answers depending on which I use? PS the line integral of the circular projection of the surface is given to be -2 so the answer is -4, but I am getting $0$ when I use the bottom disk.","['multivariable-calculus', 'line-integrals', 'stokes-theorem']"
3479619,"Find the directional derivative of the function $\phi=x^2-y^2+2$ at the point $P(1,2,3)$ .","Question: Find the directional derivative of the function $\phi=x^2-y^2+2$ at the point $P(1,2,3)$ in the direction of the st.line $PQ$ , where $Q$ is the point $(5,0,4)$ . $\dfrac{d\phi}{ds}=(l\hat{i}+m\hat{j}+n\hat{k})\cdot \left( \dfrac{\partial \phi_1}{\partial x}\hat{i}+ \dfrac{\partial \phi_2}{\partial y}\hat{j}+ \dfrac{\partial \phi_3}{\partial z}\hat{k}\right)=(\dfrac{4}{21}\hat{i}+\dfrac{-2}{21}\hat{j}+\dfrac{1}{21}\hat{k})\cdot (2x\hat{i}+-2y\hat{j})=\dfrac{4(2x+y)}{21}\implies \left[ \dfrac{d\phi}{ds}\right]_Q=\dfrac{4(10+0+y)}{21}=\dfrac{40}{21}$ . Is my approach correct?","['multivariable-calculus', 'derivatives']"
3479753,Do mixed moments determine joint distribution?,"Let $\vec{X}:=(X_1,X_2,\cdots,X_n)$ be an $n$ -dimensional random vector where $X_i$ takes values in $[N_i]:=\{0,1,\ldots,N_i\}$ for each $i=1,2,\ldots,n$ . Show that the distribution of $\vec{X}$ is uniquely determined by $$\left\lbrace\mathbb{E}\left[\prod_{i=1}^n X_i^{n_i}\right]\,:\,n_i\in[N_i],i=1,2,\ldots,n\right\rbrace.$$","['statistics', 'probability-distributions', 'linear-algebra', 'probability']"
3479773,Vector derivative of $ f(x)= (A+B\operatorname{diag}(x))^{-1} b$,"How to find a vector derivative with respect to $x\in \mathbb{R}^n$ of \begin{align}f(x)=  (A+B \operatorname{diag}(x))^{-1} b
\end{align} where $\operatorname{diag}(x)$ is a diagonal matrix where $x$ is a main diagonal, $A\in \mathbb{R}^{n \times n}$ , $B\in \mathbb{R}^{n \times n}$ , $b \in \mathbb{R}^n$ . This question is similar to what I have asked here .  However, there are some differences with matrix multiplication that lead to some confusion for me. I am also wondering if this can be shown using $\epsilon$ -definition of the derivative.","['matrices', 'jacobian', 'multivariable-calculus', 'linear-algebra', 'vector-analysis']"
3479781,Questions on $A^mBA^n=I$ and $\small B=\left[\begin{smallmatrix}1&-1&3&1\\1&1&2&1\\2&-1&3&2\\-1&-2&1&2\end{smallmatrix}\right]$,"Let $m,n\in \mathbb N$ and $A,B\in M_n(\mathbb R)$ so that: $$A^mBA^n=I$$ $$B=\begin{bmatrix} \;1&-1&\;3&\;1\\\;1&\;1&\;2&\;1\\\;2&-1&\;3&\;2\\-1&-2&\;1&\;2\end{bmatrix}$$ $(a)$ Is $A$ regular? $(b)$ Calculate $A^{m+n}$ $(a)$ $A$ is regular because $\det {(A^mBA^n)}=\det A\cdot\det {(A^{m-1}BA^n)}\ne0$ $(b)$ $A^{m+n}=A^mIA^n=A^m(B\frac{1}{B})A^n=\frac{1}{B}$ I inverted $B$ blockwise . As on Wikipedia (in notation here I used $B$ for a block-matrix, not to mix itwith the initial matrix): $${\begin{bmatrix} A&B\\C&D\end{bmatrix}}^{-1}=\begin{bmatrix} A^{-1}+A^{-1}B{(D-CA^{-1}B)}^{-1}CA^{-1} &- A^{-1}B{(D-CA^{-1}B)}^{-1}\\{(D-CA^{-1}B)}^{-1}CA^{-1} & {(D-CA^{-1}B)}^{-1}\end{bmatrix}$$ For the sake of simplicity, I transformed B into upper-triangular matrix: $$B=\begin{bmatrix} \;1&-1&\;3&\;1\\\;1&\;1&\;2&\;1\\\;2&-1&\;3&\;2\\-1&-2&\;1&\;2\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;2&-1&\;0\\0&\;1&-3&\;0\\\;0&-3&\;4&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;0&\;5&\;0\\\;0&\;1&-3&\;0\\\;0&\;0&-5&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;1&-3&\;0\\\;0&\;0&5&\;0\\\;0&\;0&-5&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;1&-3&\;0\\\;0&\;0&\;5&\;0\\\;0&\;0&\;0&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&\;0&\;0&\;1\\\;0&\;1&-3&\;0\\\;0&\;0&\;5&\;0\\\;0&\;0&\;0&\;3\end{bmatrix}$$ /edited: here I could've get $I_4$ , but I would like to go through different ways/ $$A=I, B=\begin{bmatrix}\;0&\;1\\-3&\;0\end{bmatrix}, C=0_2,D=\begin{bmatrix}\;5&\;0\\\;0&\;3\end{bmatrix}$$ Then, the Schur complement of A : $$D-CA^{-1}B=D$$ I got $$D^{-1}=\begin{bmatrix}\frac{1}{5}&\;0\\\;0&\frac{1}{3}\end{bmatrix}$$ Then $$-BD^{-1}=\begin{bmatrix}\;0&-\frac{1}{3}\\\frac{3}{5}&\;0\end{bmatrix}$$ Finally: $$B^{-1}=\begin{bmatrix}\;1&\;0&\;0&\frac{1}{3}\\\;0&\;1&-\frac{3}{5}&0\\\;0&\;0&\frac{1}{5}&\;0\\\;0&\;0&\;0&\frac{1}{3}\end{bmatrix}$$ This appears to be just a little different than $B$ transformed.
Is this correct?","['matrices', 'matrix-calculus', 'linear-algebra', 'inverse', 'solution-verification']"
3479802,A question on convergence of real sequence,Is there a sequence $\{x_n\}$ such that $\frac{x_n}{n}$ tends to $0$ but $\frac{x_n}{n^\alpha}$ does not tend to $0$ for any $0 < \alpha < 1$ ? More importantly is there a sequence of natural numbers with the above property?,"['calculus', 'sequences-and-series', 'real-analysis']"
3479812,"Minimizing a function related to ""The Median Minimizing the Sum of Absolute Deviations""","The function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ to minimize has the following form: $$f(x)=\displaystyle\sum_{i=1}^n\sum_{j=1}^n|s_{ij}-x_ix_j|$$ where the $s_{ij}$ 's are given real numbers between $0$ and $M>0$ . In order to find the least value of $f$ , I want to apply a grid search by discretizing every variable $x_i$ . My question is the following: in order to restrict my grid search, is it possible to find a lower and an upper bound on $x_i^{\ast}$ for
  an optimal solution $x^{\ast}$ ? I suspect that we have $0\leq x_i^{\ast} \leq M$ or maybe even better $0\leq x_i^{\ast} \leq \sqrt{M}$ but I am not able to prove it. It is clear that for the well-studied one-dimensional function $f(x)=\displaystyle\sum_{i=1}^n|s_{i}-x|$ , we have $\min_i(s_i)\leq x^{\ast} \leq \max_i(s_i)$ since $x^{\ast}$ is the median of the $s_i$ 's. Thank you very much!","['absolute-value', 'median', 'analysis', 'calculus', 'optimization']"
3479889,"If $f$ is a bounded non-decreasing function, then it converges as $x \to \infty$ and as $x \to -\infty$. (Obvious?)","Here's a dumb one. This is a result that seems intuitively true, but I cannot find an actual statement of it anywhere, which leads me to believe there is some weird counterexample out there somewhere. Remark. I am aware of the monotone convergence theorem for sequences , but I do not see a result anywhere for functions. Rather than try to modify the proof of the MCT for sequences, I'm just going to try to prove the result directly. Proposition. Let $f: \mathbb{R} \to \mathbb{R}$ be a bounded, non-decreasing function. Then $$\lim_{x\to -\infty}f(x) \qquad \text{ and } \qquad \lim_{x \to \infty}f(x) $$ exist. Proof of Proposition. Let $\alpha = \inf\{f(\mathbb{R})\}$ and $\beta = \sup\{f(\mathbb{R})\}$ . Since $f$ is bounded, both of these extrema exist and, clearly, $f(\mathbb{R}) \subset [\alpha, \beta]$ . Since $f$ is non-decreasing, $$ x \le y \implies \alpha \le f(x) \le f(y) \le \beta, $$ for all $x, y \in \mathbb{R}$ (so $f$ cannot exhibit any ""oscillating"" behavior""). So assume WLOG that $\beta$ is positive. Then, given $\epsilon > 0$ , there is a $b \in \mathbb{R}$ such that $f(x) > \beta - \epsilon$ whenever $x \ge b$ . Therefore $$ \lim_{x \to \infty}f(x) = \beta. $$ A similar proof shows that $f(x) \to \alpha$ as $x$ tends to $-\infty$ . Eh?","['limits', 'calculus', 'real-analysis']"
3479938,"""Independent observations"" via measure theory","I'm reading Chernoff's paper ""A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations,"" and am trying to understand it in terms of measure theory. On page 495, it says: "" $S_n$ is the sum of $n$ independent observations $X_1,X_2,\ldots,X_n$ on a chance variable $X$ ."" Which of the following is the correct interpretation? Let $M$ be a measurable space and let $X:M\to\mathbb{R}$ be a measurable function. Then $S_n:M\times\cdots\times M\to\mathbb{R}$ is the function $S_n(p_1,\ldots,p_n)=X(p_1)+\cdots+X(p_n)$ Let $(M,\mu)$ be a probability space and let $X:M\to\mathbb{R}$ be a measurable function. Let $X_1,\ldots,X_n$ be real-valued measurable functions on $M$ such that $(X_i)_\sharp\mu=X_\sharp\mu$ as measures on $\mathbb{R}$ for each $i$ and such that $(X_1,\ldots,X_n)_\sharp\mu=(X_1)_\sharp\mu\times\cdots\times(X_n)_\sharp\mu$ as measures on $\mathbb{R}^n$ . Define $S_n:M\to\mathbb{R}$ by $S_n(p)=X_1(p)+\cdots+X_n(p).$ Something else? (By $(X_1,\ldots,X_n)$ I mean the function $M\to\mathbb{R}^n$ given by $p\mapsto(X_1(p),\ldots,X_n(p))$ .) (I posted this on the stats stackexchange four days ago with no response)","['statistics', 'measure-theory', 'real-analysis', 'probability-theory', 'random-variables']"
3479999,Find the 66th derivative of this integral function.,"$$F\left(x\right)=\int _0^x\cos\left(t^3\right)dt$$ . Writing in the form of an infinite series, we get $$\sum _{n\ge 1}\left(\frac{\left(-1\right)^n}{\left(2n\right)!}\cdot \frac{x^{6n+1}}{6n+1}\right)$$ How can we find $F^{\left(66\right)}\left(0\right)$ ? Here's one solution that comes with this exercise, but which I can't comprehend: $$6n+1=66$$ $$n=\frac{65}{6}\notin \mathbb{N}.$$ Therefore $F^{\left(66\right)}\left(0\right)=0$ .","['derivatives', 'real-analysis']"
3480003,Basic binary operation on set,"Hey please help me answer this question:
Given a set $A$ with at least 2 elements which on it the binary operation * is defined in that manner: for every $a,b\in A,  a*b=b$ . Check if the binary operation * is commutative, associative and idempotent.","['elementary-set-theory', 'binary-operations']"
3480037,$\{\mathbb {E}^{\mathcal {B}}X~:~\mathcal {B}$ is a sub-$\sigma$-algebra of $\mathcal{F} \}$ is uniformly integrable,"Let $X\in L^{1}(\Omega , \mathcal{F},\mathbb {P}) $ . Show that : $$
H=\{\mathbb {E}^{\mathcal {B}}X~:~\mathcal {B}~\text{is a sub-$\sigma$-algebra of $\mathcal{F}$}\}
$$ is uniformly integrable (UI). My effort: $1. $ $\forall \mathcal {B}\subset  \mathcal{F} $ : $$
\int_{\Omega} |\mathbb {E}^{\mathcal {B}}X|d\mathbb {P}\leq \int_{\Omega} \mathbb {E}^{\mathcal {B}}|X|d\mathbb {P}= \int_{\Omega}|X|d\mathbb {P}<M~~(\text{because}~ X\in L^1)
$$ Then : $$
\sup_{\mathcal {B}\subset  \mathcal{F} }\int_{\Omega} |\mathbb {E}^{\mathcal {B}}X|d\mathbb{P}<+\infty
$$ $2. $ my problem is to show that : $\forall \epsilon >0, \exists \sigma >0$ such as: $$
[\forall A\in \mathcal{F}\, \text{such as:}\, \mathbb {P}(A)<\sigma ] \Rightarrow [\forall \mathcal {B}\subset  \mathcal{F} ~:~ \int_{A} |\mathbb {E}^{\mathcal {B}}X|d\mathbb {P}<\epsilon ]
$$","['integration', 'measure-theory', 'conditional-expectation', 'probability']"
3480058,"Convergence of $\prod_{k=1}^n \left( I_d + \frac{1}{n} A\left(\frac{k}{n}\right) \right)$ for $A : [0, 1] \rightarrow \mathcal{M}_d(\mathbf{R})$","The problem Let $d \geq 1$ and $A : [0, 1] \rightarrow \mathcal{M}_d(\mathbf{R})$ be a continuous function. For $n \geq 1$ , define: $$
E_n := \prod_{k=1}^n \left( I_d + \frac{1}{n} A \left( \frac{k}{n} \right)  \right)
$$ The goal is to study the convergence of $(E_n)_{n \geq 1}$ . I would like to find when (under which conditions on $A$ ) this sequence converges, to which limit, and when it does not converge. My try My first idea is to show that: $$
E_n \underset{n \to +\infty}{\longrightarrow} \exp \left( \int_0^1 A  \right)
$$ I have succeeded to prove it for $d = 1$ . It consists in taking the logarithm of $E_n$ , use the inequalities $x - \frac{x^2}{2} \leq \ln(1+x) \leq x$ for $x \in [0, 1]$ and finally the squeeze theorem and Riemann sums. The result is also true when $A$ is a constant matrix. But I have a hard time to generalize the proof with logarithms. I know one can define the logarithm of matrices not too far from $I_d$ , but nothing about the additivity of this logarithm.","['integration', 'matrix-exponential', 'matrices', 'linear-algebra', 'sequences-and-series']"
3480086,Issue about the generating function,"A rabbit initially stands at the position $0$ , and repeatedly
jumps on the real line. In each jump, the rabbit can jump to any
position corresponds to an integer but it cannot stand still. Let $N(a)$ be the number of ways to jump with a total distance of $2019$ and stop at the position $a$ . Determine all integers $a$ such that $N(a)$ is odd. Solution Consider the quantity $$T = (x+x^2+x^3+...)+(y+y^2+y^3+...) = \frac{x}{1-x}+\frac{y}{1-y}$$ and define generating functions $$F(x,y) = 1+T+T^2+...$$ It's clear that the coefficient of $x^my^n$ in $F$ equals to the number of ways to
jump with a total distance of $m+n$ and arrive at position $m-n$ . (i.e. variable $x$ corresponds
to positive jumps and variable $y$ corresponds to negative jumps). Now we evaluate $F(x,y)$ . To do this, we work in $\mathbb{Z}_2$ , so $$F(x,y) = \frac{1}{1-T} = \frac{(1-x)(1-y)}{1-xy}$$ Thus, we have $$F(x,y) = (1-x-y+xy)(1+(xy)+(xy)^2+(xy)^3+...)$$ It's clear that all odd coefficients are in form $x^ny^{n+1}$ and $x^{n+1}y^n$ , which corresponds to $N(1)$ and $N(-1)$ . Thus the answer is $\boxed{\{1,-1\}}$ . Edit after Donald Splutterwit answer. Can someone please explain What is $1$ in $F$ since rabbit must jump at least once? >Shouldn't it start with $T$ and not with $1$ ? Why there is no $1$ in $T$ since the rabbit can jump on the >spot? How come they never actually use $2019$ and $a$ ?","['proof-explanation', 'combinatorics', 'generating-functions']"
3480099,Convergence of partial quotients of $Q = \dfrac{1!}{1+\frac{2!}{\ddots}}$,"Consider the continued fraction $$S=\frac1{1+\dfrac2{1+\dfrac3{\ddots}}}$$ From here , we have $S = \sqrt{\frac2{e\pi}}(\text{erfc}(\frac{1}{\sqrt{2}}))^{-1}$ , where $\text{erfc}$ is the complementary error function .  I was wondering if $$Q = \frac{1!}{1+\dfrac{2!}{1+\dfrac{3!}{\ddots}}}$$ Exists, and if so, with what closed form. However, it very quickly becomes clear that $Q$ does not exist (at least, does not converge). If we let $$Q_n = \frac{1!}{1+\frac{2!}{\ddots1 +n!}}$$ Denote the $n$ -th partial quotient of $Q$ , then we have the first few partial quotients: $$1, \frac13, \frac79, \frac{31}{81},\frac{871}{1161}, \frac{23191}{59481},\frac{4413031}{5910921},\frac{939474151}{2404184841} \dots$$ which alternate between some two numbers, approximately $0.746$ and $0.39$ . I have two questions: Can I prove the even and odd-indexed partial quotients converge? Is there an analog for this to the Stern-Stolz theorem ? I don't expect there to be any nice/obtainable closed form for these partial quotients, but I ask anyone who might have insight to share.","['continued-fractions', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
3480129,Are there uncountably many disjoint uncountable real null sets?,"It is easy to think of a countable family of disjoint Cantor sets, and their union is of course a null set. 
It is equally trivial to define an uncountable family of Cantor sets but, how can it be ensured that they are pairwise disjoint? 
Would their union be a null set?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3480141,"Volume of a ""hyperbolic cube""-shaped solid in Euclidean space?","I'm searching for a formula for the volume (and, if available, also other information like area, etc.) of a so-called ""hyperbolic cube"": I couldn't find anything in Wikipedia, and also MathWorld's ""Hyperbolic Cube"" entry has no information about these solids apart from the picture. Clarification by @Blue. The goal is to find analogies for (what MathWorld calls) the ""hyperbolic octahedron"" (aka, a symmetric ""astroidal ellipsoid"" ), with Cartesian equation $x^{2/3}+y^{2/3}+z^{2/3}=1$ . That solid has volume 0.359038 (with ""apparently"" no known exact expression) and surface area $17\pi/12$ . OP wants the Euclidean volume (and surface area, etc) of a pointy-cornered, curvy-edged solid in Euclidean space, not the hyperbolic volume of the corresponding solid in hyperbolic space. (The use of the hyperbolic-geometry tag in the original version of this question was in error. But then, MathWorld's use of ""hyperbolic"" to describe these solids is somewhat misleading. I've edited the question and title to (hopefully) avoid further confusion with this terminology.)","['volume', '3d', 'geometry', 'solid-geometry', 'calculus']"
3480148,Finding limit of function,"Find the following limit : $$ \lim_{t \to(\pi/2)^-} \log\left(\frac{2 t}{\pi}\right) \log(\cos(t))$$ The indeterminate form is $0 \times\infty$ $$ \lim_{t \to(\pi/2)^-} \frac{ \log(\cos(t))}{\frac{1}{\log(\frac{2 t}{\pi})}}$$ And now it is in form $\frac{\infty}{\infty}$ , but l'Hospital's rule doesn't help me.
Any help or hint would be appreciated.","['limits', 'functions']"
3480168,Haar measure compact group,"Let $G$ be a compact group with Haar measure $m$ , then $m$ is left-invariant, in the sense that $\int_{G} f(x) \ dm(x) = \int_{G} f(s^{-1}x) \ dm(x)$ for all $s\in G$ and for all $f\in C(G)$ , and $m$ is also right-invariant. Why does $m$ satisfies the relation $\int_{G} f(x) \ dm(x) = \int_{G} f(x^{-1}) \ dm(x)$ for all $f\in C(G)$ ? My idea was to show that $f\mapsto m(\tilde{f})$ with $\tilde{f}(x) = f(x^{-1})$ is also a haar measure on $G$ and then the equality would follow from the uniqueness of the haar measure. But I don't know how to show the left and right invariance.","['haar-measure', 'functional-analysis']"
3480171,Clarification about guessing a particular solution in the method of undetermined coefficients,"Given the following Cauchy-Euler equation $$t^2y''-ty'-3y=4t^2+12,~t>0$$ one can first find the homogeneous roots through the characteristic equation $$r^2-2t-3=0$$ $$(r-3)(r+1)=0$$ $$r_1=3,~r_2=-1$$ to find the homogeneous solution of $$y_h(t)=c_1t^3+c_2t^{-1}$$ Next, one can find the particular solution by the method of undetermined coefficients . Since the right-hand side contains $4t^2+12$ , a perfectly reasonable guess for the particular solution is $$y_p(t)=At^2 + Bt+C \tag{1}$$ where upon differentiating twice and relating coefficients forms $$y_p(t)=-\frac{4}{3}t^2-4$$ However, one is also able to guess $$y_p(t)=At^2 + B \tag{2}$$ and arrive at the correct particular solution. One explanation for this is because the right-hand side of the equation has powers with exponents $2$ and $0$ , neither of which is a root of the characteristic polynomial. Since a multiple of $t$ is not present in the right-hand side of the equation, it isn't necessary to include a multiple of $t$ in the guess for the particular solution. Is this logic correct? Would the same logic apply to any similar second order Cauchy-Euler equation? For example, suppose the original equation was changed to $$t^2y''-ty'-3y=4t^4+4t^2+12,~t>0$$ then one could guess either $$y_p(t)=At^4 + Bt^3+Ct^2+Dt+E \tag{3}$$ or $$y_p(t)=At^4 + Bt^2+C \tag{4}$$ and still arrive at the correct particular solution of $$y_p(t)=\frac{4}{5}t^4-\frac{4}{3}t^2-4$$ Is it unnecessary to include a power of $t$ in the guess if it isn't included in the right-hand side of the original equation?",['ordinary-differential-equations']
3480187,"If $\left(1+\sqrt2\right)^{2011}=a+b\sqrt{2}$, for integers $a$ and $b$, then what is $\left(1-\sqrt2\right)^{2010}$ expressed using $a$ and $b$?","Being $ a $ and $ b $ integers such that $\left(1+\sqrt{2}\right)^{2011} =a+b\sqrt{2}, \left(1-\sqrt{2}\right)^{2010}$ equals: a) $a+2b+(a-b)\sqrt{2}$ b) $a-2b+(a-b)\sqrt{2}$ c) $a+2b+(b-a)\sqrt{2}$ d) $2b-a+(b-a)\sqrt{2}$ e) $a+2b-(a+b)\sqrt{2}$ Solution: Not going with any of the alternatives",['algebra-precalculus']
3480227,Questions on formulas for the reciprocal gamma function $\frac{1}{\Gamma(s)}$,"This question is related to the following formulas for the reciprocal gamma function $\frac{1}{\Gamma(s)}$ where formula (2) represents the analytic continuation of the sum over $k$ in formula (1). The parameter $f$ in formula (1) is assumed to be a positive integer. (1) $\quad\frac{1}{\Gamma(s)}=\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,2^{-s}\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\sum\limits_{k=1}^{f\,n}\frac{(n+i \pi k)^s+(n-i \pi k)^s}{\left(n^2+\pi^2 k^2\right)^s}\right),\quad\Re(s)>0$ (2) $\quad\frac{1}{\Gamma[s]}=\underset{N\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,(2 \pi)^{-s}\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right)\right),\quad s\in\mathbb{C}$ A conditional convergence requirement of the two formulas above is $M(N)=0$ where $M(N)=\sum_{n\le N}\mu(n)$ is the Mertens function . Note the condition $M(N)=0$ can be met for arbitrarily large magnitudes of $N$ since the Mertens function has an infinite number of integer zeros. See https://oeis.org/A028442 for the zeros of the Mertens function. Formulas (1) and (2) above for $\frac{1}{\Gamma(s)}$ are illustrated in Figures (1) to (6) following the questions below. Question (1) : Is it true formulas (1) and (2) for $\frac{1}{\Gamma(s)}$ converge for $\Re(s)>0$ and $s\in\mathbb{C}$ respectively? In response to the answer posted by reuns, I added information on the derivation of formula (1) for $\frac{1}{\Gamma(s)}$ to the end of my question below. I don't believe the derivation in the answer posted by reuns is equivalent to my derivation as I don't believe an expression in terms of $\zeta(s,nr/2\pi)$ is equivalent to an expression in terms of $\left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right)$ . The Mellin and Laplace transforms of $\frac{1}{\Gamma(s)}$ defined in formula (1) above are defined in formulas (3) and (4) below. I find it interesting that the $\Gamma(z)$ term appears in the Mellin transform of $\frac{1}{\Gamma(s)}$ illustrated in formula (3) below. I'm not sure the Laplace transform defined in formula (4) below is valid as Mathematica indicates the transform integral of the underlying term of the series in formula (1) for $\frac{1}{\Gamma(s)}$ is only valid for $n\le 1$ . Formulas (3) and (4) are illustrated in Figures (7) and (8) following the question below. (3) $\quad\mathcal{M}_s\left[\frac{1}{\Gamma (s)}\right](z)=\int\limits_0^\infty \frac{1}{\Gamma(s)} s^{z-1}\,ds=\\$ $\qquad\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,\Gamma(z)\sum\limits_{n=1}^N\frac{\mu(n)}{n}\sum\limits_{k=1}^{f n}\left(\left(\log(2)-\log\left(\frac{n}{n+i \pi k}\right)\right)^{-z}+\left(\log(2)-\log\left(\frac{n}{n-i \pi k}\right)\right)^{-z}\right)\right)$ (4) $\quad\mathcal{L}_s\left[\frac{1}{\Gamma (s)}\right](z)=\int\limits_0^\infty \frac{1}{\Gamma(s)} e^{-z s}\,ds=\\$ $\qquad\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\sum\limits_{n=1}^N\frac{\mu(n)}{n}\sum\limits_{k=1}^{f n} \left(\frac{1}{z+\log\left(2-\frac{2 i \pi k}{n}\right)}+\frac{1}{z+\log\left(2+\frac{2 i \pi k}{n}\right)}\right)\right)$ Question (2) : Are there a closed form expressions for the Mellin and Laplace transforms of $\frac{1}{\Gamma(s)}$ ? If not, are there other formulas for the Mellin and Laplace transforms of $\frac{1}{\Gamma(s)}$ that can be used to compare against formulas (3) and (4) above? Figure (1) below illustrates formula (1) for $\frac{1}{\Gamma(s)}$ for real $s$ , and figures (2) and (3) below illustrate the real and imaginary parts of formula (1) for $\frac{1}{\Gamma(s)}$ evaluated along the line $s=1+i t$ . All three plots are evaluated at $f=4$ and the orange and green curves represent the evaluation limits $N=39$ and $N=101$ respectively overlaid on the reference function in blue. Figure (1) : Illustration of formula (1) for $\frac{1}{\Gamma(s)}$ Figure (2) : Illustration of formula (1) for $\Re\left(\frac{1}{\Gamma(1+i t)}\right)$ Figure (3) : Illustration of formula (1) for $\Im\left(\frac{1}{\Gamma(1+i t)}\right)$ Figure (4) below illustrates formula (2) for $\frac{1}{\Gamma(s)}$ for real $s$ , and figures (5) and (6) below illustrate the real and imaginary parts of formula (2) for $\frac{1}{\Gamma(s)}$ evaluated along the line $s=i t$ . In all three plots the orange and green curves represent the evaluation limits $N=39$ and $N=101$ respectively overlaid on the reference function in blue. Figure (4) : Illustration of formula (2) for $\frac{1}{\Gamma(s)}$ Figure (5) : Illustration of formula (2) for $\Re\left(\frac{1}{\Gamma(i t)}\right)$ Figure (6) : Illustration of formula (2) for $\Im\left(\frac{1}{\Gamma(i t)}\right)$ Figures (7) and (8) below illustrates formulas (3) and (4) which are the Mellin and Laplace transforms of formula (1) for $\frac{1}{\Gamma(s)}$ . Both plots are evaluated at $f=4$ and the orange and green curves represent the evaluation limits $N=39$ and $N=101$ respectively. Figure (7) : Illustration of formula (3) which is the Mellin transform of formula (1) for $\frac{1}{\Gamma(s)}$ Figure (8) : Illustration of formula (4) which is the Laplace transform of formula (1) for $\frac{1}{\Gamma(s)}$ In response to the answer posted below by reuns below, actually I derived formula (1) from the relationship $$y^{-s}=e^2\int\limits_1^\infty x^{-3}\,\delta(\log(x)-1)\frac{\left(\frac{y}{\log(x)}\right)^{-s}}{\log(x)}\,dx\tag{5}$$ using the nested Fourier series representation of $\delta(x-1)$ . This leads to $$y^{-s}=\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,2^{-s}\,y^{-s}\,\Gamma(s)\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\sum\limits_{k=1}^{f\,n}\frac{(n+i \pi k)^s+(n-i \pi k)^s}{\left(\pi^2 k^2+n^2\right)^s}\right)\tag{6}$$ which is valid for $\Re(s)>0$ . Dividing both sides by $y^{-s}\,\Gamma(s)$ leads to $$\frac{1}{\Gamma(s)}=\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,2^{-s}\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\sum\limits_{k=1}^{f\,n}\frac{(n+i \pi k)^s+(n-i \pi k)^s}{\left(\pi^2 k^2+n^2\right)^s}\right)\tag{7}$$ which is also valid for $\Re(s)>0$ and identical to formula (1) above. The $M(N)=0$ restriction is because the nested Fourier series representation of $\delta(x-1)$ only converges at $x=0$ when $M(N)=0$ . For more information on derivation of formulas via Mellin convolutions using the nested Fourier series representation of $\delta(x-1)$ , see the answer I posted to my own question at https://math.stackexchange.com/q/2380164 . The analytic continuation $$\sum _{k=1}^{\infty } \frac{(n+i \pi  k)^s+(n-i \pi  k)^s}{\left(\pi^2 k^2+n^2\right)^s}=\pi^{-s} \left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right)\tag{8}$$ leads to formula (2) above for $\frac{1}{\Gamma(s)}$ .  I don't believe the derivation in the answer posted by reuns is equivalent to my derivation as I don't believe an expression in terms of $\zeta(s,nr/2\pi)$ is equivalent to an expression in terms of $\left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right)$ .","['number-theory', 'laplace-transform', 'complex-analysis', 'gamma-function', 'mellin-transform']"
3480255,Show that $A^4=A^2$ for an adjacency matrix with spectral radius $\leq1$,"Let $\Gamma$ be a finite, undirected graph. Let $A=(a_{ij})$ be its adjacency matrix (that is $a_{ij}=$ number of edges from vertex $v_i$ to vertex $v_j$ ) and thus $A$ is symmetric. Show that if the spectral radius of $A$ is less than or equal to $1$ , then $A^4=A^2$ . My try: Since $A$ is symmetric, eigenvectors $\{v_i\}_{i=1}^n$ of $A$ is a basis. To show $A^4=A^2$ , we only need to show that for any $i,j$ , $$
(A^4v_i,v_j)=(A^2v_i,v_j).
$$ This is equivalent to $$
\lambda_i^4(v_i,v_j)=\lambda_i^2(v_i,v_j).
$$ If $i=j$ , then we should have $$\lambda_i^4=\lambda_i^2.$$ But why this holds?","['spectral-graph-theory', 'adjacency-matrix', 'matrices', 'spectral-radius', 'linear-algebra']"
3480341,How small can the sum of the interior angles of a triangle in hyperbolic geometry get? Is there a lower bound?,"I know that in Euclidean geometry, the sum of the interior angles of a triangle is exactly $\pi$ . In  hyperbolic geometry, I know that the sum of the interior angles of a triangle is $\leq \pi$ , and I know that there exist triangles in hyperbolic geometry with interior angles that sum to strictly less than $\pi$ . However, what I don't know is ... How small can the sum of the interior angles of a triangle in hyperbolic geometry get? Is there a lower bound? Is there a hyperbolic triangle with interior angle sum equal to, say, $1/10000000$ ? I suspect it is something nice like the angles have to add up to be at least $\pi/2$ but I don't know if that's true.","['triangles', 'angle', 'hyperbolic-geometry', 'geometry']"
3480345,"How to show that limiting value of the function $f(x,y)=\frac{x^2-xy}{\sqrt{x}-\sqrt{y}}$ is zero by $\epsilon-\delta $ definition","I know that the limit of function when $(x,y)$ approaches $(0,0)$ is zero.
But how can I show this limiting value is correct using $\epsilon-\delta$ definition of the limit, i.e, $$0<\sqrt{(x-x_{0})^2+(y-y_{0})^2}<\delta \implies |f(x,y)-L|<\epsilon$$ Now in my case, if I take $$\left|\frac{x^2-xy}{\sqrt{x}-\sqrt{y}}-0\right|<\epsilon$$ I dont know how to proceed further to get the $\delta.$","['multivariable-calculus', 'calculus', 'inequality']"
3480348,"If with probability 1 we have $X_n=\mathcal{O}\left(u_n\right)$, when can we say $\mathbb{E}\left[X_n\right]=\mathcal{O}\left(u_n\right)$?","I hope that this one is not too trivial or maybe it's my lack of sleep that prevents me from seeing it. Assume we have a sequence of integrable random variables $\left(X_n\right)_n$ and let $\left(u_n\right)_n\in\left(\mathbb{R_+^{\star}}\right)^{\mathbb{N}^{\star}}$ such that with probability $1$ we have $X_n=\mathcal{O}\left(u_n\right)$ , more precisely there exists some $c>0$ such that $\mathbb{P}\left(\bigcup_{n \in \mathbb{N}^{\star}}\bigcap_{k\geq n}\left\{\left|X_k\right|\leq c u_k\right\}\right)=1$ . What (non-trivial) conditions might be enough to conclude that $\mathbb{E}\left[X_n\right]=\mathcal{O}\left(u_n\right)$ (not necessarily the same multiplicative constant), ie that $\left(\frac{\mathbb{E}\left[X_n\right]}{u_n}\right)_n$ is bounded? The only thing that I managed to find is to assume that $\frac{\left|X_n\right|}{u_n} \mathbf{1}_{\left\{\left|X_n\right| > c u_n\right\}}$ is dominated by some positive integrable random variable that is independent of $n$ and then apply reverse Fatou. EDIT: another possibility which follows from the last idea: assuming that $\inf_n u_n > 0$ and that $\left(X_n\right)_n$ is dominated by some $X$ which is positive and integrable and does not depend on $n$ , but then this kills the use of such a result for sequences converging to zero.",['probability-theory']
3480374,Proof: The cardinalities of two sets cannot be smaller than each other.,"I am working on these problems: Prove: (a) If $|A|<|B|$ and $|B|\leq|C|$ , then $|A|<|C|$ . (b) If $|A|\leq|B|$ and $|B|<|C|$ , then $|A|<|C|$ . As far as I know, it is clear that $|A|\leq|C|$ in both cases, so I just want to show that $|A|\neq|C|$ , for which I was trying to prove by contradiction. If I assume that $|A|=|C|$ , then the first one will yield: $(|A|<|B|)\land(|B|\leq|A|)$ . Similarly, the second one will yield: $(|A|\leq|B|)\land(|B|<|A|)$ . Yes, I know that in such case $|A|\neq|B|$ , so both of them reduce to $(|A|<|B|)\land(|B|<|A|)$ . Well, I have no idea how to prove that $|A|<|B|$ and $|B|<|A|$ cannot hold simultaneously. Of course, I don't think we can prove it by so-called ""transitivity"", because in order to show that $(|A|<|B|)\land(|B|<|C|)\implies|A|<|C|$ , we still need to show that $|A|\neq|C|$ , which will reduce to exactly the same problem. So I hope if anyone had some good ideas on this proof. Any help will be appreciated. P.S. It is not a homework problem. I am reading Introduction to Set Theory by K. Hrbacek and T. Jech to learn some ideas about axiomatic set theory by myself. This problem is from section 4.1. Update: It is just something like the law of trichotomy of cardinal numbers. But I don't want to use transfinite induction or AC. I wonder if there is some elementary approach to this proof. Update: In the book, $|A|=|B|$ if there is a bijection from $A$ to $B$ . $|A|\leq|B|$ if there is an injection from $A$ to $B$ . In particular, if $|A|\leq|B|$ and $|A|\neq|B|$ (the contropositive of $|A|=|B|$ ), then $|A|<|B|$ .","['elementary-set-theory', 'cardinals']"
3480376,Question about positive recurrence of a Markov chain,"Q) Let $\{X_n\}$ be an irreducible Markov chain on a countable set $S$ . Suppose that for some $x_0$ and a non-negative function $f:S\to(0,\infty)$ , there is a constant $0<\alpha<1$ s.t. $$\mathbb{E}_xf(X_1)\leq \alpha f(x)\text{ for all }x\neq x_0$$ Suppose also that $f(x_0)\leq f(x)$ for all $x$ . Show that $\{X_n\}$ is positive recurrent. Let $Y_n = f(X_n)/\alpha^n$ and I can show that $Y_n$ is a supermartingale using the hypothesis. But to show $X_n$ is positive recurrent, since $x_0$ is mentioned, I was thinking of the stopping time $$\tau = \inf\{n\geq 0:X_n = x_0\}$$ Then $Y_{n\wedge \tau}$ is also a supermartingale, $\mathbb{E}_xY_{n\wedge \tau}\leq f(x)$ . 1) How can I show that $\mathbb{E}_x\tau < \infty$ which shows that $x_0$ is positive recurrent? 2) How does that show the chain is positive recurrent?","['martingales', 'probability-theory', 'markov-chains', 'real-analysis']"
3480448,When does Mandelbrot's quadratic recurrence have explicit solutions?,"According to Mathworld's Quadratic Map , there is no general explicit solution to the following quadratic map : $$ z_{n+1} = z_n^2 + c ~, ~z_0 = 0$$ However, if $c=0$ , the solution is quite obvious. So I want to know if there are known explicit solutions for some other values of $c$ and with different $z_0$ value. (Actually, what I need to know is when $c=1/4$ and $z_0 \in (-1/2,1/2)$ , but just got curious about other cases too. $c$ and $z_0$ may range complex numbers.)","['recursion', 'recurrence-relations', 'discrete-mathematics', 'dynamical-systems']"
3480547,"For $0\le m\le M$, let $p$ satisfy $\left(1-\frac1M\right)^m(1-p)=\frac1e-\frac1M$. Why is it that $p=O\left(\frac{M-m}{M}\right)$?","For $M$ large enough, by taking logarithms and Taylor series, it can be checked that $$\left(1-\frac1M\right)^M \ge \frac1e - \frac1M$$ For $0\le m\le M$ , let $p$ be a parameter satisfying $$\left(1-\frac1M\right)^m(1-p)= \frac1e - \frac1M$$ Someone claims $$p=O\left(\frac{M-m}{M}\right)$$ I am wondering why it is true? **Further information: ** we can assume $M, m$ are nonnegative integers and $m<M$ .","['inequality', 'analysis']"
3480579,Meaning of $A^B$ where $A$ and $B$ are sets [duplicate],"This question already has answers here : Meaning of a set in the exponent (2 answers) Sets raised to exponents (1 answer) When can $\mathbb{N}$ (or any other set) be used as an exponent? (3 answers) Closed 4 years ago . I've tried to google this with a variety of search terms, but most results refer to the power set, which is $2^S$ , not $A^B$ . Does it mean the set of all maps (functions) from the set $B$ to the set $A$ ? What is the definition of this notation? Thank you for reading!","['elementary-set-theory', 'notation']"
3480585,An element $f$ that is integral over its affine coordinate ring: show there exists this open neighborhood,"(Recall first the following definition: Let $R$ be an integral domain and $K$ its field of fractions. An element $a \in K$ is called an integral element over $R$ if there exists a polynomial $g = x^n + a_{n-1} x^{n-1} + \ldots + a_1 x + a_0 \in R[x]$ such that $g(a) = 0$ . ) Problem: Consider an affine variety $X$ over $\mathbb{C}$ . Let $R = \mathbb{C}[x_1, \ldots, x_n] / I$ be its affine coordinate ring, and let $K$ be its field of fractions. Prove that if $f \in K$ is an integral element over $R$ , then for each point $x \in X$ there exists an open (in the usual Euclidean topology) neighborhood $U$ of $x$ and a real constant $B > 0$ such that $|f(y)| < B$ for all $y \in U$ where $f$ is regular. Show that this claim is false in general if $U$ is required to be Zariski open. Attempt: I'm given a hint that I should use the maximum principle for holomorphic functions. I don't really know how to find this open neighborhood $U$ . First, I believe that $K \cong \mathbb{C}(x_1, \ldots, x_n)/I$ (can someone confirm this)? So I assume $f \in K$ is integral over $R$ . By definition there exists a polynomial $p(t) \in R[t]$ such that $p(f) = 0$ . Can I assume that $f \in K$ is holomorphic? Any help with this problem is appreciated!","['complex-analysis', 'algebraic-geometry']"
3480673,This question seems simple but I can't wrap my head around it,Determine the two points on the graph $y = x^4 - 2x^2 - x$ which share a common tangent. I tried plotting the graph out but I couldn't really wrap my head around how $2$ points can have the same tangent. I could be missing something completely obvious but I would really appreciate if someone could solve it and walk me through it.,"['limits', 'derivatives']"
3480694,An elegant proof for a claim about orthogonal and positive-definite matrices,"Let $P$ be a real $n \times n$ symmetric positive-semidefinite matrix. Suppose that $\langle O,P \rangle = \langle I,P \rangle$ for some orthogonal matrix $O$ . Here $\langle , \rangle$ is the Euclidean (Frobenius) inner product. The assumption is equivalent to $\text{tr}(O^TP)=\text{tr}(P)$ . Then $OP=P$ holds. Since we can replace $O$ with $O^T$ the claim is equivalent to the seemingly surprising assertion $\text{tr}(OP)=\text{tr}(P) \implies OP=P$ . I have a proof, and I wonder whether there are other shorter proofs.
  Is there a proof which does not require diagonalizing $P$ ? Here is my proof: By orthogonally diagonalizing $P$ , we can reduce to the case where $P=\Sigma$ is diagonal, with non-negative entries $\sigma_i$ . The assumption implies $\sum_i \sigma_i O_{ii}=\sum_i \sigma_i$ where the sums run over all the indices $i$ where $\sigma_i \neq 0$ . Since $|O_{ii}| \le 1$ this forces $O_{ii}=1$ for all these $i$ . A direct check then implies that $O\Sigma=\Sigma$ : Indeed, $O\Sigma=\Sigma$ is equivalent to $O_{ij}\sigma_j=\Sigma_{ij}$ .
If $\sigma_j=0$ , then $\Sigma_{ij}=0$ so equality holds. If $\sigma_j \neq 0$ , then $O_{jj}=1$ so equality holds for $i=j$ . For $i \neq j$ , $\Sigma_{ij}=0$ (since $\Sigma$ is diagonal) and , and $O_{ij}=0$ , since whenever $O_{jj}=1$ , all the rest of the entries in the $j$ -th column of $O$ are zero. (Since $O$ is orthogonal).","['positive-semidefinite', 'matrices', 'orthogonal-matrices', 'matrix-calculus', 'linear-algebra']"
3480708,Delete some circles to isolate each one while still cover enough area.,"Problem Let $\mathcal C$ be a finite set of unit circles in the Euclidean plane such that the area of the union of the circles in $\mathcal C$ is $A$ . Then there exists a subset $\mathcal C'$ of $\mathcal C$ such that: $\bullet$ No two distinct circles in $\mathcal C'$ intersect, and $\bullet$ The area of the union of the circles in $\mathcal C'$ is at least $2A/9$ . My Attempt Let $G=(V, E)$ be a graph whose vertex set consists of $|\mathcal C|$ symbols, one for each member of $\mathcal C$ . Two vertices $u$ and $v$ are adjacent if the corresponding circles intersect. Note that for any vertex $v$ , the area of the union of the circles corresponding to the vertices in $v\cup N(v)$ , where $N(v)$ is the set of all the neighbors of $v$ , is no more than $9\pi$ . Now let $S$ be a maximal independent set in $G$ , and $\mathcal C'$ be the set of circles corresponding to the vertices in $S$ . We claim that the area of the union of the cirlces in $\mathcal C'$ is at least $A/9$ . Let us write $C_u$ to denote the member of $\mathcal C$ corresponding to a vertex $u\in V$ . Since $S$ is a maximal independent set, every vertex in $G$ is a neighbor of some vertex in $S$ .
Thus we have $$
\sum_{v\in S}9\cdot\text{area}(C_v)
\geq
\sum_{v\in S}\text{area}\left(\bigcup_{u\in\ v\cup N(v)} C_u\right)
\geq
\text{area}\left(\bigcup_{u\in V}C_u\right) = A$$ giving $$\sum_{v\in S}\text{area}(C_v)\geq A/9$$ Also, since $S$ is an independent set, no two circles in $\mathcal C'$ intersect. So instead of $2A/9$ , I am abnle to achieve $A/9$ . How can we improve this?","['contest-math', 'combinatorics', 'geometry']"
3480771,Confidence Interval question no standard divination given to find the standard error,"A survey of 800 Irish households finds that 54% of households have
  oil-fired central heating. Infer a 95% confidence interval for the
  percentage of the total population of Irish households that have
  oil-fired central heating. This is how I have worked on this problem using the formula to find the standard error Notation $N = 800$ $P = 54$ $95\% = 1.96$ Workings out (1) Find the standard error $$\frac{\sqrt{54(1-.54)}}{800} *100= 0.062$$ (2) Multiply the confidence interval by the standard error $54\% \pm 1.96 * 0.062$ (3) Answer $54\% \pm 0.015$ I would like to know if this is the correct anwser or am I even going the right way about it. The problems contains the standard deviation I can do but I am struggling with this","['statistics', 'confidence-interval']"
3480828,"Problem 8, chapter 1 - Rudin's functional analysis","Trying to solve this problem: Problem 8: a) Suppose $\mathcal{P}$ is a separating family of seminorms on a vector space $X$ . Let $\mathcal{Q}$ be the smallest family of seminorms on $X$ that contains $\mathcal{P}$ and is closed under max. [This means: if $p1,p2 \in\mathcal{Q}$ and $p=\max(p1,p2)$ then $p \in Q$ ]. If the construction of Theorem 1.37 is applied to $\mathcal{P}$ and $\mathcal{Q}$ show that the two resulting topologies coincide. The main difference is that $\mathcal{Q}$ leads directly to a base, rather than a subbase. b) Suppose $\mathcal{Q}$ as in part (a) and $\Lambda$ is a linear functional on $X$ . Show that $\Lambda$ is continuous if and only if there exists a $p\in Q$ such that $|\Lambda x|≤Mp(x)$ for all $x\in X$ and some constant $M < \infty$ . This is my attempt given my recent review of basis and subbasis of a topology. a) About the topology $\mathcal{Q}$ we have the following $$
\begin{array}{l}
p \in \mathcal{P} \Rightarrow q \in \mathcal{Q} \\
p_1, p_2 \in \mathcal{Q} \Rightarrow \max \left\{p_1,p_2\right\} \in \mathcal{Q}
\end{array}
$$ So the seminorms on $\mathcal{Q}$ are defined recursively. We can easily prove that for each $q \in \mathcal{Q}$ we have $p_1,\ldots, p_n \in \mathcal{P}$ such that $$
q = \max \left\{p_1, \ldots, p_n \right\}
$$ To prove that $\mathcal{P}$ and $\mathcal{Q}$ generate the same topology it suffice to show they generate the same basis. This is easy to show since if $V(q,m)$ is essentially a basis element $$
V(q,m) = \left\{x : q(x) < \frac{1}{m} \right\} = \left\{ x : p_1(x),\ldots, p_n(x) < \frac{1}{m}, p_i \in \mathcal{Q} \right\} = \\\bigcap_{i=1}^n \left\{x : p_i(x) < \frac{1}{m} \right\}
$$ So each $V(q,m)$ is actually a basis element generated by the subbasis defined by the seminorms of $\mathcal{P}$ , hence they generate the same topology. b) I'm stuck at the moment for this one. Suppose there are $M > 0$ and $p \in \mathcal{Q}$ such that for all $x \in X$ $$
\left| \Lambda x \right| \leq M p(x) \Leftrightarrow \left| \Lambda \left( \frac{x}{p(x)} \right) \right| \leq M
$$ Since $p \left( \frac{x}{p(x)} \right)= 1$ and for $0 < \epsilon < 1$ we have $$
p \left(\frac{\epsilon x}{p(x)} \right) < 1
$$ also the $V(p,n)$ are absorbing we can easily pick a basis element (which is open) such that $\Lambda$ is bounded. On the other way suppose $\Lambda$ is continuous then it is bounded in some neighborhood of $0$ , which implies there's an element of the local base where $\Lambda$ is bounded. This means we have a $p \in \mathcal{Q}$ and a natural $n$ such that $$
\left| \Lambda x \right| \leq M \; \text{for $x \in V(p,n)$}
$$ I define now a map from $X$ to $V(p,n)$ as $x \to \frac{x}{2np(x)}$ . This is well defined since $$
p\left( \frac{x}{2np(x)} \right) = \frac{1}{2n} \frac{p(x)}{p(x)} = \frac{1}{2n} < \frac{1}{n}.
$$ Therefore for each $x \in X$ $$
\left|\Lambda \left( \frac{x}{2np(x)} \right) \right| \leq M \iff \left|\Lambda \left( x \right) \right| \leq 2nM p(x)
$$ defining $M' = 2nM$ I have $$
\left|\Lambda \left( x \right) \right| \leq M' p(x)
$$ which is the result.","['proof-writing', 'topological-vector-spaces', 'functional-analysis']"
3480864,Why should it be $\sqrt[3]{6+x}=x$?,"Find all the real solutions to: $$x^3-\sqrt[3]{6+\sqrt[3]{x+6}}=6$$ Can you confirm the following solution? I don't understand line 3. Why should it be $\sqrt[3]{6+x}=x$ ? Thank you. $$
\begin{align}
x^3-\sqrt[3]{6+\sqrt[3]{x+6}} &= 6 \\
x^3 &= 6+ \sqrt[3]{6+\sqrt[3]{x+6}} \\
x &= \sqrt[3]{6+ \sqrt[3]{6+\sqrt[3]{x+6}}} \\
\sqrt[3]{6+x} &= x \\
x^3 &= 6+x \\
x^3-2x^2+2x^2-4x+3x-6 &= 0 \\
(x-2)(x^2+2x+3) &= 0 \\
x &= 2
\end{align}
$$","['cubics', 'algebra-precalculus', 'solution-verification', 'radicals']"
3480872,"Are there polynomials in $\mathbb{R}[x]$, other than $P(x)=x^2$, such that $P(\sin x)+P(\cos x)=1$ for all real $x$?","Are there any polynomials $P\in\mathbb R[x]$ , other than $P(x)=x^2$ , such that $$ P(\sin x) + P(\cos x) = 1,\quad x\in\mathbb R? $$ Notice that there are pairs of polynomials $P,Q$ with $P(\sin x)+Q(\cos x)=1$ : say, from $\sin 3x=3\sin x-4\sin^3 x$ and $\cos 3x=4\cos^3 x-3\cos x$ it follows that $$ (3\sin x-4\sin^3 x)^2+(4\cos^3 x-3\cos x)^2=1. $$ My motivation for this question is a pure curiosity; hope it counts.","['trigonometry', 'polynomials']"
3480912,Proof of Criterion for Uniform Integrability,"Defintion (Uniform Integrability): A family $\mathcal{F}$ of integrable functions is uniformly integrable if $\forall \varepsilon > 0 $ there is a $M_\varepsilon>0$ , such that $\int_{\{|f|>M_\varepsilon\}}|f|\,\mathrm{d}\mu < \varepsilon,\ \forall f\in\mathcal{F}$ Let $(X,\mathcal{A},\mu)$ be a finite measure space. A family of integrable functions $\mathcal{F}$ is uniformly integrable if and only if for all $\varepsilon > 0$ there exists a $\delta>0$ sucht that for all $A\in\mathcal{A}$ we have that $\mu(A)<\delta\ \Rightarrow\ \int_A|f|\,\mathrm{d}\mu<\varepsilon ,\ \forall f\in \mathcal{F}$ Here is my proof and I would like to know if there are errors and would be thankful for any improvements. "" $\Rightarrow$ "": Because of uniform integrability we can choose $\varepsilon/2 >0$ and get a $M_{\varepsilon/2}$ such that for arbitrary $A\in\mathcal{A}$ $\int_A |f|\,\mathrm{d}\mu =\int_{\{|f|>M_{\varepsilon/2}\}\cap A}|f|\,\mathrm{d}\mu + \int_{\{|f|\le M_{\varepsilon/2}\}\cap A}|f|\,\mathrm{d}\mu\le\mu(A) M_{\varepsilon/2} + \int_{\{|f|>M_{\varepsilon/2}\}\cap A}|f|\,\mathrm{d}\mu$ Note that we have $\mu(A)<\infty$ since the measure space is finite. Now we can pick a $\delta:=\frac{\varepsilon}{2}\frac{1}{M_{\varepsilon/2}}$ from which it follows by the previous equation that for all $A$ satisfying $\mu(A)<\delta$ $\int_A |f|\,\mathrm{d}\mu\le \varepsilon/2 + \varepsilon/2\le \varepsilon $ which is what we wanted to show. "" $\Leftarrow$ "": We have, since each member in $\mathcal{F}$ is integrable, that $\mu\left(\{|f|>m\}\right)\rightarrow 0$ for $m\rightarrow\infty$ and arbitrary $f\in\mathcal{F}$ . Any suggestions on how to prove this statement rigorously? This is equivalent to (just use the definition of a limit and exchange $\delta$ for $\epsilon$ ) $\forall \delta >0\ \exists M_\varepsilon\in \mathbb{N}\ \forall n\ge M_\varepsilon\colon \mu\left(\{|f|>n\}\right)\le |\mu\left(\{|f|>n\}\right)|<\delta$ which proves this direction.","['measure-theory', 'probability-theory']"
3480916,Calculating the integral of some bessel function,How can I calculate $$\int \frac {x\left(J_{ \frac 34}(\frac {x^2}{2})-J_{- \frac 54}(\frac {x^2}{2})\right)}{2J_{- \frac 14}(\frac {x^2}{2})}dx~?$$,"['integration', 'ordinary-differential-equations', 'bessel-functions']"
3480922,How do I begin with this integral? [Polar coordinates & line integrals],"Calculate the line integral of the scalar field over the curve L: $$
\int_L(x+y)\,ds
$$ with $L$ the right loop of $r^2=2\cos(2\theta)$ I've been going at it for over 3 hours now. With several people in a discord for math that were trying to help me but we can't seem to get the solution that should be 2sqrt(2). Can someone help me get to the solution? :) I tried the following: x = r.cos(θ) and y = r.sin(θ) and my $ds= \sqrt{r²\:+\:r'²}d\theta 
 =\sqrt{\frac{2}{cos(2\theta)}} .d\theta$ from there I got my boundaries as θ = $\frac{\pi }{4}$ -> r = 0 and as θ = 0 -> r = $\sqrt{2}$ from here I got that the integral should be the following: $\int _Lr\left(cos\theta +sin\theta \right)\:\cdot \sqrt{\frac{2}{cos\left(2\theta \right)}}\:.d\theta$ Now the issue is that I don't know what I have to fill in for the boudnaries of my integral. And even if this is correct what I'm doing.","['multivariable-calculus', 'calculus', 'vector-analysis']"
3480992,Smoothness and my typing speed,"I practice typing on a site called keybr.com. I get statistics of my typing speed in graph format as well. Here, I guess the straight green line is a line fitted to my typing speed data. There is some function called 'Smoothness'. From Wikipedia, I see that smoothing filters noise and modifies some data. I see that as I change the value of smoothness, the line changes its slope and my typing speed apparently decreases when smoothness is high. Can anyone give an intuitive explanation of why does this happen? What is the correct/optimum amount of smoothness so that I get the truest trend of my typing speed over time? According to the graph, does my typing speed actually increase or decrease with time?","['statistical-inference', 'statistics', 'graphing-functions']"
3480998,"Optimization $\max\{xy:(x,y) \in M \}$","Consider $\max\{xy:(x,y) \in M \}$ with the set $M := \left \{ (x,y) \in \mathbb{R}^2: x^2+y^2 \leq 4\right \} $ How or where do I have to sketch the level sets $N_c = \{(x,y) : xy = c \}$ for $c =0$ , $c=1$ , $c=4$ and $M$ ? When I write $x^2+y^2 = c$ in Desmos I get this for $c=1$ and this for $c=4$ For $c=0$ I get nothing and regarding $M$ I don't know how it's done.
Is $M$ just a circle $<= 4$ ?","['multivariable-calculus', 'calculus', 'optimization', 'lagrange-multiplier']"
3481048,Can I search for factors of $\ (11!)!+11!+1\ $ efficiently?,"Is the number $$(11!)!+11!+1$$ a prime number ? I do not expect that a probable-prime-test is feasible, but if someone actually wants to let it run, this would of course be very nice. The main hope is to find a factor to show that the number is not prime. If we do not find a factor, it will be difficult to check the number for primality. I highly expect a probable-prime-test to reveal that the number is composite.  ""Composite"" would be a surely correct result. Only if the result would be ""probable prime"", there would remain slight doubts, but I would be confident with such a test anyway. Motivation : $(n!)!+n!+1$ can only be prime if $\ n!+1\ $ is prime. This is because a non-trivial factor of $\ n!+1\ $ would also divide $\ (n!)!+n!+1\ $ . The cases $\ n=2,3\ $ are easy , but the case $\ n=11\ $ is the first non-trivial case. We only know that there is no factor upto $\ p=11!+1\ $ What I want to know : Can we calculate $$(11!)!\mod \ p$$ for $\ p\ $ having $\ 8-12\ $ digits with a trick ? I ask because pari/gp takes relatively long to calculate this residue directly. So, I am looking for an acceleration of this trial division.","['prime-factorization', 'factorial', 'number-theory', 'elementary-number-theory', 'prime-numbers']"
3481061,"Can it be shown, $n^4+(n+d)^4+(n+2d)^4\ne z^4$?","We know, $n^4+(n+d)^4= z^4$ has no solution in positive integers $n,d,z$ . Can it be shown, $n^4+(n+d)^4+(n+2d)^4= z^4$ has no solution in positive integers $n,d,z$ ? I am check upto $1\le n, d, z\le 150$ without finding a counter example. PARI/GP for(n=1,150,for(d=1,150,for(p=1,150,if(sum(q=0,2,(n+q*d)^4)==p^4,print([n,d,p]))))) Generalization over problem","['number-theory', 'elementary-number-theory', 'diophantine-equations']"
3481144,Calculating the genus of Quartic model $y^2=x^4+bx^3+cx^2+dx+e$ of elliptic curves.,This link says that $y^2=x^4+bx^3+cx^2+dx+e$ is an elliptic curve. How do we compute its genus (which should be 1)? Under what conditions is $y^2=p(x)$ an elliptic curve where $p(x)$ is polynomial in $x$ of degeree $\ge 5$ ?,"['algebraic-curves', 'number-theory', 'algebraic-geometry', 'elliptic-curves']"
3481147,Calculating differential equation with bessel function in it with ln,How can I solve that: $$\frac{d}{dx}\ln \left(\frac{xJ_{-\frac14}\left(\frac{x^2}{2}\right)}{2} \right)$$ without using the fact $$2J^{'}_{\nu}(z)=J_{\nu-1}(z)-J_{\nu+1}(z)$$ only the definition of the Bessel function,"['derivatives', 'ordinary-differential-equations', 'bessel-functions']"
3481174,Determine if this series $\sum_{n=1}^\infty \frac{u_n}{\sqrt{n}}$ (with $u_n$ periodic) is convergent/divergent,"Let $u_n$ be defined as follows: \begin{align}
u_{4k} &= 1 \\ 
u_{4k+1} &= -2 \\ 
u_{4k+2} &= 2 \\ 
u_{4k+3} &= -1
\end{align} Is the series $\sum_{n=1}^\infty \frac{u_n}{\sqrt{n}}$ convergent or divergent? I think it is convergent because you can compare it to a p-series with p = 3/2? But I am having trouble formalising the proof - I also tried to use something like the ratio test but I am dubious about whether this is a valid approach","['calculus', 'convergence-divergence', 'sequences-and-series']"
3481204,Cubic plane curves,"I understand that a cubic curve in $\mathbb P^2$ is given by a homogeneous degree $3$ polynomial, so that the space of cubic curves can be identified with $\mathbb P^9$ . However, there should be nine types of cubic curves: irreducible (elliptic) curves which may be either smooth, with a nodal singularity or a cusp singularity (3 types) the union of a conic and a line, which may either be tangent to the conic or meet the conic in two distinct points (2 types) the union of three distinct lines, which may either meet in one point or in three points (2 types) the union of a double line with a line (1 type) a triple line (1 type) I guess that $\mathbb P^9$ should admit a stratification by ""type"", but what are the dimensions of the components. Are there $0$ -dimensional components? What is a good reference for this stratification?","['algebraic-geometry', 'elliptic-curves']"
3481239,Why $r^{2} $ instead of $r^3$ in this change of variable?,"I'm taking a PDE's course, and several times there has been an integration over a sphere cropping up. Oftentimes, we change variables to shift the sphere we originally had to the unit sphere, and carry on calculations from there. I'll give an example: Let $ f: \mathbb{R}^3 \rightarrow   \mathbb{R}$ . We want to integrate $f$ over the surface of some ball in 3 dimensions, centered at $x$ and with radius $r$ ; that is, we are after $$ \int_{\partial B(x, r)} f(\sigma) \ d\sigma.$$ If we make the change of variables $$ \sigma = x + r\omega, $$ then the integral becomes $$ \int_{\partial B(0, 1)} f(x + r\omega)r^2 \ d\omega. $$ I don't understand why this is...I tried to justify it by using the change of variables theorem (the one involving the Jacobian determinant), but I get a different result, and I don't know what I'm misunderstanding here. Using the theorem, we can think of this change of variables as $$ T(\omega) = x + r \omega $$ for any $\omega \in \mathbb{R}^3.$ Then \begin{align} 
   && \int_{\partial B(x, r)} f(\sigma) \ d\sigma &=\int_{T^{-1}(\partial B(x, r)) = \partial B(0, 1)} f(T(\omega)) |J(\omega)| d \omega &&
\end{align} where $|J(\omega)|$ is the Jacobian determinant of $T$ . But $$T(\omega) = \langle x_1 + r \omega_1, x_2 + r \omega_2, x_3 + r \omega_3 \rangle$$ so that $$ T'(\omega) = J(\omega) = \frac{\partial }{\omega_j} ( x_i + r \omega_i)
= \delta_{ij} \cdot r,$$ or, in other words, a 3 by 3 matrix with only $r$ on the diagonal, and zero everywhere else. Doesn't that mean the Jacobian determinant should be $r^3$ and not $r^2$ ? I would hope that by this point I would have grasped this change of variables theorem, but it's very possible I have misunderstood something along the way. I would really appreciate some help! Thank you!","['multivariable-calculus', 'change-of-variable']"
3481317,Why combination with repetition doesn't use the same formula as unitary coefficient equation,"Unitary coefficient equation uses the formula $$ x_1 + x_2 + \ldots + x_k = m $$ $$ C(m + k - 1, k - 1) $$ (considering $x$ can be $0$ , i.e solutions in positive integers) But for combination with repetition, my book says that the formula is $C(m + k - 1, k)$ (note it is not $k-1$ ) and says that it is equivalent to proposing the problem as: $$x_1 + x_2 + \ldots + x_k = m$$ The weird thing is that in the example for combination with repetition is exactly the way of a equation with unitary coefficients. Example: How many different sets of three coins can be made if each coin can be 10, 25, 50 cents or 1 dollar. It says that this can be thought as $x_1 + x_2 + x_3 + x_4 = 3$ In this case is $C(6, 3)$ which seems to correlate with the first theorem. But another example says ""In a library there 20 math books each one with unlimited quantity. How many elections of 10 books can be done if repetitions are allowed?"" And puts as answer $C(29, 10)$ instead of $C(29, 9)$ As my thinking is that it should be $C(20 + 10 - 1, 10 - 1)$ I am a bit confused. Idiot-proof answer is much appreciated.","['combinatorics', 'discrete-mathematics']"
3481356,Question about a matrix in a problem of differential equations,"I am studyng differential equations from a book called linear ordinary differential equation by Earl A. Coddington this is the problem Consider the nonhomogeneous system \begin{equation}\label{equation:1}
X'=A(t)X+B(t),  \  A\in C(I,M_n(\mathcal F)), \ B \in C(I, \mathcal F^n  )\ 
\end{equation} on some interval $I$ , together with the corresponding homogeneous system \begin{equation}
X'=A(t)X
\end{equation} For $t,\tau \in I$ . let $S(t, r) \in M_n(\mathcal F)$ be the unique matrix satisfying \begin{equation}
\frac{\partial S}{\partial t}(t,\tau)=A(t)S(t,\tau), \ S(\tau,\tau)=I_n. 
\end{equation} show that if $X$ is any basis for $X'=A(t)X$ then \begin{equation}
S(t,\tau)=X(t)X^{-1}(\tau), \ t,\tau \in I 
\end{equation} $S(t,\tau)S(\tau,\sigma)=S(t,\sigma), \ t,\tau,\sigma \in I$ I'm confused that the matrix has two evaluations $S(t,\tau)$ I don't see what it means and the book dont have examples about this.","['linear-algebra', 'ordinary-differential-equations']"
3481388,Moduli functor $\mathcal{M}_g$ of smooth curves of genus $g$ not representable,"Let $S$ be a scheme. By a smooth curve of genus $g$ over $S$ we mean a proper, flat, family $C \to S$ whose geometric fibers are smooth, connected $1$ -dimensional schemes of genus $g$ . The moduli functor $\mathcal{M}_g$ of smooth curves of genus $g$ over a
noetherian base $S$ is the functor that sends each $S$ -scheme $B$ to the set $\mathcal{M}_g(B)$ of isomorphism classes of smooth and proper morphisms $C \to B$ (where $C$ is also
an $S$ -scheme) whose fibers are geometrically connected curves of genus $g$ . I have a question about the argument that this moduli functor $\mathcal{M}_g$ of smooth curves of genus $g$ over a
noetherian base $S$ is not representable (in spirit of Yoneda-Lemma this means that $\mathcal{M}_g$ isn't a sheaf).  I found it in Pedro Castillejo's paper Introduction to stacks and he refered for a ""detailed"" proof to Dan Edidin's ""Notes on the construction of the moduli space of curves"" . Now one argument from Edidin's paper I not understand: The key point is that some curves have non trivial isomorphisms, i.e. not identity maps, and it makes it possible to construct
non-trivial families $C \to B$ where each fiber has the same isomorphism class. The construction in Edindin's paper on page 3 I understand. What I not understand is why the existence of such non-trivial family $C \to B$ of isomorphic fiber imply that the functor $\mathcal{M}_g$ is not representable. In the paper on page 2 the argument is ...As a result, it is possible to construct
  non-trivial families $C \to B$ where each fiber has the same isomorphism
  class. Since the image of $B$ under the corresponding map to the moduli
  space is a point (???), if the moduli space represented the functor $\mathcal{M}_g$ then $C \to B$ would be isomorphic to the trivial product family (???) Assume $\mathcal{M}_g$ is representable by an $S$ -scheme $M$ , thus we have natural equivalence $\mathcal{M}_g \cong Hom_S(-, M)$ . Q_1: Why this imply that then the image of $B$ is a point of $M$ ? Q_2: I not see how the sheaf axiom would give a contradiction to representability.","['algebraic-stacks', 'algebraic-geometry', 'schemes']"
3481414,"Prove that the intersection of two compact sets is compact, using the Heine-Borel Criterion","Using the fact that: S is compact: every open cover has a finite subcover. Prove: Given $A,B \subset \Bbb R^n$ are compact sets, then $A\cap B$ is compact. Here is my attempt, using the fact that $A$ follows the above Heine-Borel criterion above and $A \cap B \subset A$ : Since $A$ is compact, every open cover of $A$ has a finite subcover. We wish to show that every open cover in $A \cap B$ has a finite  subcover. Since $A \cap B \subset A$ , then every open cover of A must be an open cover of $A \cap B$ (from the definition of an open cover shown below). A collection of sets ${U_\alpha}$ is an open cover os $S$ if $S$ is contained in $\bigcup U_\alpha$ . Since $A$ is compact, we know that every open cover has a finite subcover. Therefore, since $A \cap B \subset A$ and $A$ has a finite subcover for every open cover, $A \cap B$ has a finite subcover for every open cover. Is this the correct way to approach this problem? Thanks!","['elementary-set-theory', 'compactness']"
3481419,How to use Concentration Inequalitites to bound this probability from above?,"I'm attempting a problem in Rohatgi & Saleh's Intro to Probability and Statistics, from the Concentration inequalities section. It's actually the first exercise in the section, so the fact that I can't get it means I'm missing something obvious: We have a random variable X with PDF: $f(x;\lambda) = \frac{e^{-x} x^\lambda}{\lambda !}, x>0$ for some $\lambda \geq 0$ an integer. They ask that I show that $P\{0<X<2(\lambda + 1)\} > \frac{\lambda}{\lambda + 1}$ . I'd like to use the concentration/Markov/Chebyshev inequalities to for this but all the inequalities are in the wrong direction. Is there a nifty way I'm not thinking of to get them to mirror this result? Hints are greatly appreciated. Also please don't go easy on me if this is something obvious. I need to start being better at probability. Also here are the concentration inequalities I've been referring to: $$\text{If $E[h(X)]$ exists,  } \quad  P[h(X)\geq M] \leq \frac{E[h(X)]}{M} \quad \text{for any } M>0,\\
\text{If $E[|X|^k]$ exists,  } \quad  P[|X|\geq M] \leq \frac{E[|X|^k]}{M^k} \quad \text{for any } M>0,$$","['probability-theory', 'probability']"
3481428,"""Simple to state, but difficult to solve"" problems which require analyzing topology of simplicial complexes?","In a User's Guide to Discrete Morse Theory , Robin Forman writes: A number of questions from a variety of areas of mathematics lead one to the problem of analyzing the topology of a simplicial complex. In order to appreciate this statement, can I have some examples of ""simple to state, but difficult to solve"" problems which require one to analyze the topology of a simplicial complex? To me: ""simple to state"" means that the problem can be explained to an undergraduate student without domain-specific jargon. At first glance, it might even even seem rather easy to solve. ""difficult to solve"" means that the problem isn't easily solved using ""everyday tools"" provided by an undergraduate mathematics education. It requires some special insights, which help to illustrate the topic in question (in this case, discrete Morse theory).","['big-list', 'morse-theory', 'combinatorics', 'general-topology', 'simplicial-complex']"
3481443,Tautological bundle: algebraic geometry vs topology,"I'm going to compare the two construction of twisted sheaf/bundle $\mathcal{O}(1)$ from algebraic and topological viewpoint: 1) Algebraic construction (Hartshorne's Algebraic Geometry, p. 117): Definition . Let $S$ be a graded ring, and let $X = \operatorname{Proj} (S)$ . For any $n \in  \mathbb{Z}$ , we define the sheaf $\mathcal{O}_X(n)$ to be $S(n)^{\sim}$ . We call $\mathcal{O}_X(-1)$ the Tautological bundle. For $X= \mathbb{P}^n$ we can $\mathcal{O}_X(l)$ also characterize by cycle condition: 
the twisted sheaf $\mathcal O_{P^n}(l)$ is fully determined by it. as $\mathcal O_{P^n}(l)$ are invertible the restrictions $\mathcal O_{P^n}(l) \vert _{U_i}$ to $U_i := D_+(X_i)=Proj(k[X_1,...,X_n])_{(T_i)}= Spec(k[X_1/X_i,...,X_n/X_i])$ are generated by certain regular sections $s_i \in O_{P^n}(l)(U_i)$ . The cycle condition is noting but a family of $\phi_{ij} \in O_{P^n}(l)(U_i \cap U_j)^*$ such that $\phi_{ij} s_i = s_j$ . for $l \in \mathbb{Z}$ the cycle is given by $\phi_{ij} = (\frac{X_i}{X_j})^l \in O_{P^n}(l)(U_i \cap U_j)^*$ . Recall that $O_{P^n}(l)$ is uniqely determined by the the data $(\phi_{ij})_{ij}$ up to glocal section $a \in O_{P^n}(l)(X)^*$ , i.e. $(\phi_{ij})_{ij}$ and $(a \cdot \phi_{ij})_{ij}$ determine the same line bundle $O_{P^n}(l)$ for every $a \in O_{P^n}(l)(X)^*$ . 2) Topological construction: Let $V$ be a vector space of dimension $n$ , and $\Bbb P(V) = X$ be the space of its lines. Write $\mathcal O_X(-1)$ for the topological line bundle $L=\{(l,v) \in X \times V : v \in l\}$ with canonical projection to $X$ . Q: If we take $S= k[X_1,...,X_n]$ and thus $\operatorname{Proj}(S)= \mathbb{P}_k^n$ , how can I connect these both constructions explicitly and understand that the tautological bundle in both constructions in ""certain way"" coincide with each other. to be more precise: if we use the cycle condition for description of 1) for $l=1$ , how the data $\phi_{ij} = (\frac{X_i}{X_j})^{-1} \in O_{P^n}(-1)(U_i \cap U_j)^*$ is reflected in topological version $L=\{(l,v) \in X \times V : v \in l\}$ ? Assume $\operatorname{char}(k)=0$ . Then GAGA theorems provide correspondence $\mathcal{F} \to  \mathcal{F}^{an}$ that defines an exact functor from the category of sheaves over $ (X,\mathcal{O}_{X})$ to the category of sheaves of $ (X^{an},\mathcal{O}^{an}_{X})$ . The bundles of $ (X^{an},\mathcal{O}^{an}_{X})$ are the ""topological"" bundles and therefore we obtain identification between $Pic(X) = \mathbb{Z}$ and line bundles over $X$ . Thus formally we can establish such correspondence. The motivation of this question is more focused on intuitive approach to understand why $\mathcal{O}_X(-1)$ and $L=\{(l,v) \in X \times V : v \in l\}$ by these correspondence are the ""same"" . Is there any geometric intuition which makes this identifiction plausible focused on how the cycle condition is ""reflected"" in topological pendant $L$ ? I would very thankful if somebody could take some time to explain how one have to think intuitively about this identification.","['vector-bundles', 'algebraic-geometry', 'coherent-sheaves', 'line-bundles']"
3481449,Heuristics of Heine-Borel property,"A metric space $X$ is said to have the Heine-Borel property if every closed and bounded set is compact. There are two well-known facts about this property: Every Euclidean space $\mathbb{R}^n$ adopts the Heine-Borel property. No infinite dimensional Banach space adopts the Heine-Borel property. What is the intuition behind the difference between these two types of metric spaces? So far I've mostly dealt with Euclidean spaces, thus have a tendency to associate compactness being the same as closed and bounded. What are the heuristics behind this property failing for infinite dimensional cases?","['functional-analysis', 'intuition', 'real-analysis']"
3481476,Derivative of Inverse Function on Banach Space,"During the semester, I ran into a problem that dealt with finding the derivative of the inverse function on a Banach space (where $V$ , the vector space is a Banach algebra and the field $F = R$ or $C$ ). I had managed to show that the set of invertible elements of the Banach algebra was open, and then I needed to find the derivative of $f(x) = x^{-1}$ where $f : GL(V) \to GL(V)$ . Here is what I have thought so far: So the derivative satisfies $\lim_{h \to 0} |\frac{(x+h)^{-1} - x^{-1} - \lambda(h)}{|h|}|$ . We have that $(x+h)^{-1} - x^{-1} = x^{-1}((e + x^{-1}h)^{-1} - 1) = x^{-1}(\sum_{j=0}^{\infty} (x^{-1}h)^{j} - 1) = x^{-1}(\sum_{j=1}^{\infty} (x^{-1}h)^{j})$ . This is as far as I have been able to get. Any hints or helps would be greatly appreciated. Working off the hint from Thomas Shelby:
We show that $f$ is continuous on $GL(V)$ . Given some $x \in GL(V)$ (let $B(x, h) \subseteq GL(V)$ and an $\epsilon > 0$ we have two cases. If $||x||\epsilon \geq 1$ then choose $\delta = \min(1, h)$ else choose $\delta = \min(\frac{||x||^2\epsilon}{2(1-||x||\epsilon)}, h)$ . Then we have for $y$ such that $||x - y|| < \delta$ that $|f(x) - f(y)|| = ||x^{-1}(y-x)y^{-1}|| = ||x^{-1}||*||y^{-1}||*||x-y|| = \frac{||x-y||}{||x||*||y||}$ and since $||y|| \leq ||x - y|| + ||x||$ by triangle inequality we have that this is less than or equal to $\frac{\delta}{||x||(||x|| + \delta)} < \frac{\frac{||x||^2\epsilon}{1-||x||\epsilon}}{\frac{||x||^2}{1-||x||\epsilon}} = \epsilon$ . To conclude then we note that as Thomas Shelby wrote, $||\frac{(x+h)^{-1} - x^{-1} + x^{-1}hx}{||h||}|| = \frac{||(x+h)^{-1} - x^{-1}||*||h||*||x^{-1}||}{||h||} = ||(x+h)^{-1} - x^{-1}||*||x^{-1}||$ which goes to $0$ as $h \to 0$ by continuity of $f$ .","['banach-spaces', 'derivatives']"
3481486,End(M) for cyclic R-module M is a commutative ring where R is a PID.,"Show that End(M) for cyclic R-module M is a commutative ring where R is a PID. So I know the module M is an abelian group so by a previous result, End(M) is a ring under pointwise addition and function composition. What remains to show is that the function composition is commutative. To that end, let $\phi, \alpha \in End(M)$ . We want to show the ring End(M) is commutative so we must show $\phi(\alpha(m)) = \alpha(\phi(m)), \forall m \in M$ . We are given M is a cyclic R-module (R is a PID) so $M = Rx$ for some $x \in M$ . Then for any $m \in M$ we have that $ m = rx$ for some $r \in R$ . Then, $\phi(\alpha(m)) = \phi(\alpha(rx)) = \phi(r\alpha(x)) = r\phi(\alpha(x))$ Likewise, $\alpha(\phi(m)) = r\alpha(\phi(x))$ So, we just need to show $\alpha(\phi(x)) = \phi(\alpha(x))$ but I'm not sure what to do from here? Any hints?","['abstract-algebra', 'modules']"
3481529,"Example where $n^{-1/2}S_n\Rightarrow N(0,1)$ but its variance does not converge to 1","Let $\{X_n\}$ be a sequence of independent random variables such that $X_n$ takes the values $\pm n$ each with probability $1/2n^2$ and $\pm 1$ each with probability $1/2(1-1/n^2).$ Define $S_n=X_1+\cdots+X_n$ for $n\geq 1.$ Show that $S_n/\sqrt{n}$ converges in distribution to $N(0,1).$ This example is interesting because although $n^{-1/2} S_n \Rightarrow N(0,1),$ it holds that $\text{Var}[n^{-1/2} S_n]\to 2$ as $n\to \infty.$ My approach: Write $X_n = Z_n + (n-1)Z_n Y_n$ where $Z_n$ are iid, taking values $\pm 1$ with prob. $1/2$ and $Y_n\sim\text{Ber}(1/2n^2),$ such that $Y_n$ 's are independent themselves and also independent of the $Z_n$ 's. Then, $$\frac{S_n}{\sqrt{n}} = \frac{1}{\sqrt{n}} \sum_{k=1}^n Z_k + \frac{1}{\sqrt{n}} \sum_{k=1}^n (k-1)Z_kY_k.$$ Now the first part in the RHS converges weakly to $N(0,1).$ Hence it suffices to show that the other part converges to $0$ in probability (or in distribution). How can I show this?","['characteristic-functions', 'central-limit-theorem', 'probability-theory']"
3481562,Meaning of a term in stochastic gradient descent,"This is in reference to the first two pages of Robbins-Monro ""A stochastic approximation method,"" https://projecteuclid.org/euclid.aoms/1177729586 . What is the meaning of the RHS in (8)? As I understand it, there is a measure space $(\Sigma,\mu)$ and $x_n$ and $y_n$ are real-valued measurable functions on $\Sigma.$ The LHS of (8) is a conditional probability, and so for each number $y$ it is a function whose domain is $x_n(\Sigma)\subset\mathbb{R}.$ However, for each number $x$ and $y$ , $H(y\mid x)$ is a number, and so it seems that for each number $y$ , the RHS $H(y\mid x_n)$ is a measurable function on $\Sigma.$ So I don't understand the meaning of the equality.","['statistics', 'measure-theory', 'stochastic-approximation', 'stochastic-processes', 'probability']"
3481574,Reasoning behind the trigonometric substitution for $\sqrt{\frac{x-\alpha}{\beta-x}}$ and $\sqrt{(x-\alpha)(\beta-x)}$,"In my book , under the topic ""Evaluation of Integrals by using Trigonometric Substitutions"", it is given, in order to simplify integrals containing the expressions $\sqrt{\frac{x-\alpha}{\beta-x}}$ and $\sqrt{(x-\alpha)(\beta-x)}$ , the substitution $x=\alpha\cos^2\theta+\beta\sin^2\theta$ must be used. If I remember this form and the substitution, then it definitely helps simplify the integrand. The first expression gets simplified to $\tan\theta$ and the second to $\sin\theta\cos\theta(\alpha-\beta)$ . I understand that if we do this kind of substitution we greatly simplify the expression. But how do we determine what to substitute in the first place or in other words, if I forget the substitution, is there any way to determine which substitution works well to simplify the integrand? How did the author figure out this substitution is the best fit for this kind of expression? Was it a guess or is there any mathematical reasoning behind it?","['integration', 'substitution', 'soft-question']"
3481593,"Let $A,B$ be squared matrices. Given $A=I-AB$, Prove: $B^3=0$ if and only if $A=I-B+B^2$","Let $A,B$ be squared matrices. 
  Given $A=I-AB$ , Prove: $B^3=0  \iff A=I-B+B^2$ The question has 3 sections: Given $A=I-AB$ , Prove that $A$ is invertible and that $BA=AB$ . Prove that if $B$ is a symmetric matrix, then $A$ is symmetric Prove: $A=I-B+B^2$ if and only if $B^3=0$ . I proved the first two sections, and did the first direction of the third section, Thus we suppose that $A=I-B+B^2$ , and I proved that $B^3=0$ as follows: Suppose $A=I-B+B^2$ , then $A=I-AB=I-B+B^2$ $$-AB=-B+B^2$$ $$\text{ we'll substitute $A$ by $I-B+B^2$}$$ $$-(I-B+B^2)B= -B+B^2$$ $$-B+B^2-B^3 =-B+B^2 \Longrightarrow B^3 =0$$ As wished. Now let $B^3=0$ . Prove: $A=I-B+B^2$ . $$AB = I-A$$ We'll multiply by $B$ on the left side: $$AB^2 = B-AB$$ $$AB^2+AB=B$$ $$AB(B+I)=B$$ and from this point I'm stuck.","['matrices', 'linear-algebra']"
3481603,Application of the Nullstellensatz,"The question is as follows Let $k\subset K$ be algebraically closed fields. And $I \leq k[x_1,...x_n]$ an ideal. Show that if $f \in K[x_1 ,...x_n]$ vanishes on $Z(I)$ it vanishes on $Z_K(I)$ . Where $Z(I)$ is the set of zeros of $I$ in $k^n$ , $Z_K(I)$ is the set of zeros in $K^n$ . I've tried proving this following the proof of the nullstellensatz but I'm stuck, The only connection I can think of  between $Z(I)$ and $Z_K(I)$ is of inclusion which does not seem to help. Hints will be most welcome.","['zariski-topology', 'algebraic-geometry', 'abstract-algebra']"
3481607,Limit of the ratio of two non-Riemann sums.,"Let $\left[ {a,b} \right] \subset \mathbb{R}$ and $f,g:\left[ {a,b} \right] \to \mathbb{R}$ be two Riemann-integrable functions. Let $a = {x_0} < {x_1} < {x_2}... < {x_n} = b$ be a partition of $\left[ {a,b} \right]$ and let $\Delta x = \mathop {\max }\limits_{i = 0}^{n - 1} \left( {{x_{i + 1}} - {x_i}} \right)$ . Let ${t_i} \in \left[ {{x_i},{x_{i + 1}}} \right],\;i = 0,n - 1$ and let $k \in {\mathbb{N}^*}$ . I’d like to prove that $\mathop {\lim }\limits_{\Delta x \to 0} \frac{{\sum\limits_{i = 0}^{n - 1} {{{\left( {{x_{i + 1}} - {x_i}} \right)}^k}f\left( {{t_i}} \right)} }}{{\sum\limits_{i = 0}^{n - 1} {{{\left( {{x_{i + 1}} - {x_i}} \right)}^k}g\left( {{t_i}} \right)} }} = \frac{{\int\limits_a^b {f\left( x \right){\text{d}}x} }}{{\int\limits_a^b {g\left( x \right){\text{d}}x} }}$ It is obvious for equally spaced partitions ${x_{i + 1}} - {x_i} \equiv \Delta x$ $\mathop {\lim }\limits_{\Delta x \to 0} \frac{{\sum\limits_{i = 0}^{n - 1} {\Delta {x^k}f\left( {{t_i}} \right)} }}{{\sum\limits_{i = 0}^{n - 1} {\Delta {x^k}g\left( {{t_i}} \right)} }} = \mathop {\lim }\limits_{\Delta x \to 0} \frac{{\sum\limits_{i = 0}^{n - 1} {\Delta xf\left( {{t_i}} \right)} }}{{\sum\limits_{i = 0}^{n - 1} {\Delta xg\left( {{t_i}} \right)} }} = \frac{{\int\limits_a^b {f\left( x \right){\text{d}}x} }}{{\int\limits_a^b {g\left( x \right){\text{d}}x} }}$ But I don’t see how to do it in the general case?","['integration', 'riemann-sum', 'partitions-for-integration']"
3481616,"Prove: If $p$ is a prime number (but 2) and $2^m \not \equiv 1\bmod p$,$\quad\sum_{k=1}^{p-1} k^m \equiv 0\bmod p$","Prove: If $p$ is a prime number (but 2) and $2^m \not \equiv 1\bmod p$ , $\quad\sum_{k=1}^{p-1} k^m \equiv 0\bmod p$ It's easy to prove if $m \equiv 0\;$ or $\;1\bmod p$ , I tried Faulhaber's formula, but I can't prove this one.",['number-theory']
3481617,"$|x|=7$,$|y|=3$ then $|\langle x,y \rangle |=21$","Let $|G|=168$ . Let $x,y \in G$ with $|x|=7$ , $|y|=3$ . Show that if $yxy^{-1}=x^2$ then $|\langle x,y \rangle |=21$ . What I have done so far: since $\langle x \rangle \leq \langle x,y \rangle $ and $\langle y \rangle \leq \langle x,y \rangle $ then $7$ and $3$ divide $|\langle x,y \rangle |$ , and $21 \leq |\langle x,y \rangle |$ . Now, $|\langle x,y \rangle |$ must divide $168$ and is a multiple of $21$ so it is either $21$ or $84$ . I am not sure how to use $x^{-1}yx=y^2$ to conclude that it is of order $21$ . Any hints appreciated.","['group-theory', 'cyclic-groups', 'finite-groups']"
3481627,How many reduced transducers are there?,"Let’s define a transducer as a $5$ -tuple $(Q, A, B, \phi, \psi)$ , where $Q$ is the collection of states , $A$ is the input alphabet , $\phi: Q\times A \to Q$ is the transition function and $\psi: Q \times A \to B$ is the output function . Any transducer defines a transducer function $\overline{\psi}: Q\times A^* \to B^*$ described by the following recurrence: $\overline{\psi}(q, \Lambda) = \Lambda$ , where $\Lambda$ is the empty word. $\overline{\psi}(q, a \alpha) = \psi(q, a) \overline{\psi}(\phi(q, a), \alpha)$ , where $a \in A$ , $\alpha \in A^*$ . We call $q_1, q_2 \in Q$ distinguishable iff $\exists \alpha \in A^*$ such that $\overline{\psi}(q_1, \alpha) \neq \overline{\psi}(q_2, \alpha)$ . We call a transducer reduced iff any two its states are distinguishable. Suppose $n_t(k, m, n)$ is the number of distinct reduced deterministic transducers with input with $k$ states, $m$ -symbol input alphabet and $n$ -symbol output alphabet. Is there some sort of explicit formula (or at least asymptotic) for $n_t(k, m, n)$ ? I managed to derive only the following three particular cases: $n_t(1, m, n) = n^m$ Proof: As there is only one state, the transducer is fully determined by its output function. And it is in this case just a map from the input alphabet to the output one. $n_t(2, m, n) = (4n)^m(n^m - 1)$ Proof: Suppose, $q_1$ and $q_2$ are the only two states of our transducer. It is not hard to see, that in this particular case they are not distinguishable iff $\psi(q_1, a) = \psi(q_2, a)$ . Thus there are $n^m(n^m - 1)$ possible versions of $\psi$ , that result in our transducer being reduced. And there are $4^m$ ways to chose $\phi$ . $n_t(k, m, 1) = 0$ $\forall k \geq 2$ Proof: If $B$ is one-element, then $\overline{\psi}(q, \alpha)$ is determined only by the length of $\alpha$ .","['automata', 'combinatorics', 'discrete-mathematics']"
3481661,A weak to norm continuous operator between Banach spaces is a finite rank operator,"Let $X$ , $Y$ be Banach spaces. And $T:X\rightarrow Y$ a continuous operator when $X$ is endowed with the weak topology and $Y$ with the norm topology. I am trying to show that $T$ is finite rank i.e. that its range is a finite dimensional vector space. The first thing I tried was taking a basis in $X$ and looking at the image but I do not think the assumptions say anything useful about this. 
I also saw on some other post the same question for Hilbert spaces in which they looked at the inverse image of the (closed) unit ball, but I am not sure how they arrived at that idea and how I can use this. A subset $U$ of $X$ is open if there exist a $\rho>0$ and $\omega_1,\ldots,\omega_n\in X^*$ the dual space of $X$ such that $\{y\in X: \lvert\omega_i(x-y)\rvert<\rho\}\subset U $ for all $\omega_i$ .","['banach-spaces', 'operator-theory', 'functional-analysis']"
3481688,Asymptotes of the integral curve,"One of the first tasks in differential equation problem book. Obviously that, it’s separable equation, but integrals can’t be computed. The task is to proof that integral curve have two assymptoties","['integration', 'ordinary-differential-equations', 'asymptotics', 'real-analysis', 'calculus']"
3481696,The closest matrix to a given matrix in $\text{SO}(n)$ is unique iff the smallest singular value is a strict minimum,"$\newcommand{\SOn}{\operatorname{SO}_n}$ $\newcommand{\On}{\operatorname{O}_n}$ $\newcommand{\Sym}{\operatorname{Sym}_n}$ $\newcommand{\Skew}{\operatorname{Skew}_n}$ $\newcommand{\dist}{\operatorname{dist}}$ $\newcommand{\Sig}{\Sigma}$ $\newcommand{\sig}{\sigma}$ $\newcommand{\al}{\alpha}$ $\newcommand{\id}{\operatorname{Id}}$ This question is a reference request. Claim: Let $A $ be a $n \times n$ real matrix with non-positive determinant. Then there is a unique closest matrix $Q \in \SOn$ to $A$ (w.r.t the Euclidean  Frobenius norm) if and only if the smallest singular value of $A$ is strictly smaller than the rest of the singular values. Question: Is this claim ""known""? Where can I find a reference for it? Note that I am looking for a reference , not a proof. (I have a proof...) Also, I am specifically asking for the distance minimizer in $\SOn$ . If we replace $\SOn$ with $\On$ , then the minimizer(s) is the orthogonal polar factor from polar decomposition. (and is unique whenever the matrix $A$ is invertible).","['reference-request', 'orthogonal-matrices', 'matrix-calculus', 'linear-algebra', 'optimization']"
3481728,Cover map of the figure eight,"During an exam I was supposed to find a connected (and non trivial) cover map of the figure eight. In particular, each point in the figure eight has to have infinite pre-image. I know that there is a ‘standard’ one in $R^2$ that is not hard at all but during the exam for some reason I forgot about it or I was not able to build it again. Since I had fun thinking about it, I am still curious to know if it works or not. I came up with an alternative idea. Please let me know how you could formalize this idea in a better way or if there is any flaw in the reasoning :) Start with a triangle (full line) and map each segment to the ‘left circle’. Do you need an ‘orientation’ on the segments when you define this map? For each of the vertices of the triangle in previous step, construct another triangle (dashed line) and map each of its segments to the ‘right circle’. Then, continue with the same pattern. I think I missed the orientation but maybe the overall concept work. What do you think? From my reasoning it seems a covering space since it satisfies the required properties.","['general-topology', 'algebraic-topology', 'covering-spaces']"
3481766,"A connection between primes, right triangles and $\pi$.","This question is motivated by the question A mysterious connection between primes and $\pi$ in MO hence a similar title. Is the following claim true? Conjecture : The ratio of the sum of the squares of the hypotenuse to the
  sum of the area of all Pythagorean triangles in which the hypotenuse
  is a prime $\le n$ tends to $2\pi$ as $n \to \infty$ . Source code: p = 5
sn = sd = 0
target = step = 10^3

while True:
    if p%4 == 1:
        x = 1
        while x < p:
            y = (p^2 - x^2)^0.5
            if y%1 == 0:
                sn = sn + x^2 + y^2
                sd = sd + x*y
                i = i + 1
                break
            else:
                x = x + 1
    p = next_prime(p)
    if p > target:
        target = target + step
        print p, sn/sd.n()","['summation', 'number-theory', 'sequences-and-series', 'limits', 'prime-numbers']"
