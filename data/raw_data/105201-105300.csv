question_id,title,body,tags
1485723,Does $A\oplus \mathbb{Z}\cong B\oplus \mathbb{Z}$ imply $A\cong B$?,"If $A$ and $B$ are abelian groups, do we have that $A\oplus \mathbb{Z}\cong B\oplus \mathbb{Z}$ implies $A\cong B$? Motivation: I was just thinking about different ways of deducing equality from expressions by quotienting, then realized I didn't know the answer in this case.","['abstract-algebra', 'group-theory', 'abelian-groups', 'direct-sum']"
1485783,Second Course in Algebraic Number Theory - Lang versus Neukirch,"So the title pretty much says it all. I have completed a first course in Algebraic Number Theory (number fields, ideal factorization in the ring of integers, finiteness of the ideal class group, Dirichlet's units theorem) and I now want to move on to a second course. The two books that have been suggested to me are Lang and Neukirch (both called Algebraic Number Theory). I would like to hear people's opinions and comments about how these two compare for a graduate-level reading course in ANT. Many thanks!","['book-recommendation', 'number-theory', 'algebraic-number-theory', 'reference-request']"
1485818,"Is my proof correct? (group theory, Lagrange)","$G$ is a finite group. Prove, that $\exists k\in G:k\neq 1_G, k^2=1_G \iff |G|$ is even. My proof 1) $\Rightarrow$: $\exists k\in G: k^2 = 1_G\Rightarrow \exists G'\leq G$ s.t. $ G'=\{1_G,k\}$, then $|G'|=2$ Using Lagrange Theorem: $|G'|$ divides $|G|\Rightarrow |G|$ is even. 2) $\Leftarrow$: $|G|=2n\Rightarrow |G|\geq 2$ and $|G|=2n\Rightarrow\exists G'\leq G$ with $|G'|=2\Rightarrow\{1_G=1_{G'}, k\}= G'$ and $k=k^{-1}\Rightarrow |k|=2$ $\blacksquare$ Could you please review my proof? I'm currently learning how to write proofs using as few words as possible, so could you also check if the syntax is correct?","['abstract-algebra', 'group-theory', 'proof-verification']"
1485869,Closure of intersection of two sets,"I have a problem with the following exercise: Let be $A,B$ subsets of a topological space $X$. Prove that $$\overline{A\cap B}\subset \bar{A}\cap\bar{B}.$$ I only know is that $\bar{A}=A\cup\partial A$.",['general-topology']
1485879,Inverse of half a polynomial,"I am given the assignment of finding the inverse of the following function: $$f(x) = x^2 + 4x + 5$$
where $x > -2$. I can get rid of the $5$ obviously, but then I get stuck. Hints are appreciated.","['polynomials', 'limits']"
1485889,"Why there exist integer $b,c$ such $a=b+c$","Let $p$ is prime number,and $p>5,p\equiv 1\pmod 4$,
we Define:an integer $a$ is called a quadratic residue modulo $p$.:if there exists an integer $x$ such that:
$$x^2\equiv a\pmod p$$ show that: for every integer $a$,there exist integer $b,c$ such
$$a=b+c$$
and $b,c$ non-quadratic residues modulo $p$",['number-theory']
1485897,Example of non-surjective isometry of metric space,"I need an example of an isometry, $f$, from a metric space $X \rightarrow X$ that is not surjective. My example was letting the metric space be the set of non-negative real numbers with d$(x,y) := \max(x,y) - \min(x,y)$ if $x \neq y$, and $0$ if $x = y$. Then my isometry would be $f(x) = x+1$, so $0$ would not be in the image of $f$. Is this a valid example? I don't know whether this is a real metric space, I just came up with it on my own.","['metric-spaces', 'general-topology']"
1485924,Intuition behind the direct integral of a family of Hilbert spaces,"In order to understand better the mathematically rigorous version of Dirac's formalism in Quantum Mechanics I've been reading about direct integrals of Hilbert spaces, projector-valued measures and so on. Although the definition of the direct integrals of Hilbert spaces I've seem is quite clear, I simply couldn't get the idea behind it. The definition I have is the following: Definition 1 : Let $\mu$ be a Radon measure on the $\sigma$ -compact locally compact Hausdorff space $X$ . A measurable field of Hilbert spaces is a collection $\{H_x : x \in X\}$ of Hilbert spaces together with a linear subspace $\mathcal{S}$ of $\prod_{x\in X}H_x$ , whose elements are called measurable sections satisfying the following axioms: If $\eta\in \prod_{x\in X}H_x$ , then $\eta\in \mathcal{S}$ if and only if $x\mapsto \langle \xi(x),\eta(x)\rangle$ is measurable for all $\xi \in \mathcal{S}$ . There is a sequence $(\xi_n)$ in $\mathcal{S}$ such that for almost every $x\in X$ , the closed linear span of $\xi_n(x)$ is $H_x$ . Definition 2 : Let $\mu$ be a Radon measure on the $\sigma$ -compact locally compact Hausdorff space $X$ , and $\{H_x: x\in X\}$ a measurable field of Hilbert spaces over $X$ . We define the direct integral of the $H_x$ to be the set of all $\xi \in \mathcal{S}$ (modulo agreeing on measure zero sets) such that $\int_X |\xi(x)|^2d\mu(x) < \infty$ and we denote this direct integral by $$\int_X^{\oplus}H_x d\mu(x).$$ On the direct integral we have the inner product $$\langle \xi,\eta\rangle = \int_X \langle \xi(x),\eta(x)\rangle_{H_x}d\mu(x),$$ which turns the direct integral into a Hilbert space. Now, although the definition by itself is clear, I can't get the intuition behind it. We have a collection of Hilbert spaces indexed by one topological space with a Radon measure. We then consider a linear subspace of the product, satisfing two properties. This is my first question: what is the intuition behind the defining properties of a measurable section? What we are really defining there? After that, one simply picks all the measurable sections with the property that the integral of the square of the norm of the vectors at each point converge. This is my second question: what is the intuition behind this definition? What we are really trying to define with this construction of the direct integral? The direct sum, for example, can be understood in a intuitive way as the space of sums of elements of each space alone. In the case of the direct integral I'm not seeing one simple inution like that.","['definition', 'measure-theory', 'functional-analysis', 'hilbert-spaces', 'intuition']"
1485944,WTS: $X = (X \cap (C \setminus A)) \cup (X \cap B)$,"Let $A,B \subset C$, with $A \subset B$. I want to show that for every subset $X$ of $C$, it follows: $X = (X \cap (C \setminus A)) \cup (X \cap B)$. Let \begin{equation}\begin{split}
&~x \in (X \cap (C \setminus A)) \cup (X \cap B) \\
\Leftrightarrow&~x \in X \wedge (x \in C \setminus A \vee x\ \in B) \\
\Leftrightarrow&~x \in X \wedge~...\color{red}?... \\
\Leftrightarrow&~x \in X \wedge x \in C \\
\Leftrightarrow&~x \in X.
\end{split}\end{equation}",['elementary-set-theory']
1485955,Complexity of computing $ABx$ depends on the order of multiplication,"To calculate $y = ABx$ where $A$ and $B$ are two $N\times N$ matrices and $x$ is an vector in $N$-dimensional space. there are two methods. One is to first calculate $z=Bx$, and then $y = Az$. This way takes roughly $2N^2$ multiplications. The other is to first calculate $C = AB$, and then $y=Cx$. This way, however, takes $N^3+N^2$ multiplications! Am I correct on this? I do not understand why change of the order of the calculation introduces so much difference in the computation complexity.","['linear-algebra', 'computational-complexity', 'matrices']"
1485963,Stuck on solving this PDE of 2nd Order,"I'm trying to solve this second order PDE: $$-u_{xx}+2u_{xy}+3u_{yy}-(1/3)u_x+u_y=0$$ I've got up to $48V_{st}+4V_t=0$ and this implies $48V_s+4V=f(s)$. I'm stuck on how to find the $g(t)$ solution. Could I have some help, please?","['ordinary-differential-equations', 'partial-differential-equations']"
1485991,Path From Positive Dedekind Cuts to Reals?,"Don't spend a lot of time on this. I'm certain I could bang it out myself; but maybe there's an answer out there that someone already knows. Say we use Dedekind cuts to construct the reals. Addition is fine and the proof that a set bounded above has a supremum is just lovely. Then we get to defining multiplication and all hell breaks loose - not that there's anything deep or really difficult about it, but at the very least the elegance is lost in a mass of special  cases. So I say to myself this is just because of the historical accident that people figured out how to extend the positive rationals to the rationals before considering completeness; why not  get completeness first? So the plan is to use Dedekind cuts of positive rationals to construct the positive reals, and then go from there to the reals. (In all of this ""positive"" may mean strictly positive or non-negative, whatever works.) This leads to at least one question : For what values of whatever and the construction can one prove the following theorem? Theorem If $F$ is an ordered field then the set of positive elements is a whatever . Conversely, given a whatever $P$, the construction gives an ordered field $F$ such that $P$ is (isomorphic to) the set of positive elements of $F$. Or if there is no such general theorem we could be more specific. Say $P$ is the set of positive reals. I can think of at least two constructions that do in fact lead from $P$ to $\Bbb R$; the question is what do I need to prove about $P$ in order to prove that the result is in fact $\Bbb R$. One construction would be to say that a real is just an element of $P$ with a plus or minus sign attached. This seems likely to lead to the sort of special-casing that we're trying to avoid (already when we think about $0$ we see we need to add a special clause to the effect that $+0=-0$; blech). Another construction is to regard $(a,b)\in P^2$ as representing the real $a-b$. So we'd define $(a,b)\sim(c,d)$ if $a+d=c+b$, we'd define the sum and product of elements of $P^2$ in the obvious way (in particular $(a,b)(c,d)=(ac+bd,ad+bc)$), show these operations lift to the quotient $P^2/\sim$ and be on our way. I kind of like the second construction. What do I need to prove about $P$ to show that $P^2/\sim$ is an ordered field with positive cone $P$?","['abstract-algebra', 'real-numbers', 'real-analysis']"
1485994,"the rationals as a metric space, closed and open sets","If we consider the rationals, $\mathbb{Q}$, as a metric space with the usual metric of the real line, then is the set $B=\{q\in\mathbb{Q} \mid {q^2}\lt 2\}$ a closed set? I know the definition of a closed set is a set that contains all its limit points. And I understand that here $\sqrt{2}$ is a limit point but it's not in our set but it's also not in the rationals so it's not in our metric space at all. Then, does $B$ contain all its limit points? Alternatively, we can say $B$ every point in $B$ is an interior point (has a neighbourhood contained in $B$) so that makes $B$ open. So then is $B$ both open and closed? Thanks","['metric-spaces', 'general-topology']"
1485995,List of all matroids,"List all matroids $(E, S)$ with $E = \{1\}, E = \{1, 2\}, E = \{1, 2, 3\}$. I know that $(E, S)$ is called an independence system, and that according to the definition $S \subseteq 2^E$ and $S$ is closed under inclusion. The point is that, I don't exactly understand the problem above. What will be the matroids in question? EDIT: Matroids for $E = \{1, 2\}$ $\{ \emptyset \}, \{ \emptyset, \{1\} \}, \{ \emptyset, \{2\} \}, \{ \emptyset, \{1\}, \{2\}\}, \{ \emptyset, \{1\}, \{2\}, \{1, 2\}\}$ Matroids for $E = \{1, 2, 3\}$ $\{ \emptyset \}, \{ \emptyset, \{1\} \}, \{ \emptyset, \{2\} \}, \{ \emptyset, \{3\} \}, \{ \emptyset, \{1\}, \{2\}\}, \{ \emptyset, \{1\}, \{3\}\}, \{ \emptyset, \{2\}, \{3\}\}, \{ \emptyset, \{1\}, \{2\}, \{1, 2\}\}, \{ \emptyset, \{1\}, \{3\}, \{1, 3\}\}, \{ \emptyset, \{2\}, \{3\}, \{2, 3\}\}, \{ \emptyset, \{1\}, \{2\}, \{3\}\}, \{ \emptyset, \{1\}, \{2\}, \{3\}, \{1, 2, 3\}\}$","['graph-theory', 'discrete-mathematics', 'matroids']"
1486040,Complex differential forms on $CP^n$,"Why Complex projective spaces don't admit some differential forms? To be more specific, I know that the space of complex forms is decomposed as direct sum of holomorphic and anti-holomorphic part; given that, I don't understand why the complex projective line $CP^1$ for instance has only the scalar 0-form , and $dz \wedge \bar{dz}$. Why 1-forms like $dz$ and $\bar{dz}$ vanish?","['complex-geometry', 'differential-geometry', 'differential-forms']"
1486075,$\sum_{n=0}^{\infty} \sin (nx) = \cot(x/2)$?,"I noticed this trend playing around in desmos, that the series $\sin x + \sin 2x + \sin 3x +...$ tends to $\cot(\frac{x}{2})$ , is this identity correct? Here is an example: $a=18$ "" />","['trigonometric-series', 'trigonometry']"
1486080,The non-existence of $\lim \limits_{x \to 0} \sin {1 \over x}$,"Regarding to the definition of the limit of a function, employing a step by step approach using tautologies in logic, here it was proved that the limit of a function $f(x)$ does not exist at a point $x=a$  if and only if $$\forall L,\exists \varepsilon  > 0:\left( \forall \delta  > 0,\exists x:(0 < \left| x - a \right| < \delta  \wedge |f(x) - L| \ge \varepsilon \right))\tag{1}$$ or $$\nexists L:\left( {\forall \varepsilon  > 0,\exists \delta  > 0:\left( {\forall x,0 < \left| {x - a} \right| < \delta  \to \left| {f(x) - L} \right| < \varepsilon } \right)} \right)\tag{2}$$ Statements $(1)$ and $(2)$ are logically equivalent. I want to use just $(1)$ to prove that $\lim\limits_{x \to 0} \sin {1 \over x}$ does not exist . However, proofs using $(2)$ can also be interesting! My thought I just know that I must find an $\varepsilon $ which may depend on $L$ and one $x$ which may depend on both $L$ and ${\delta}$ such that for every $L$ and $\delta  > 0$ we must have $$0 < |x| < \delta  \wedge \left| \sin {1 \over x} - L \right| \ge \varepsilon \tag{3} $$ I don't know how to proceed!","['analysis', 'calculus', 'limits']"
1486104,Evaluating the limit $\lim_{n\to\infty} \left(\frac{1}{n^2}+\frac{2}{n^2}+...+\frac{n-1}{n^2}\right)$,"Evaluate the limit $$\lim_{n\to\infty} \left(\frac{1}{n^2}+\frac{2}{n^2}+...+\frac{n-1}{n^2}\right) .$$ My work: I started by computing the first 8 terms of the sequence $x_n$ ($0, 0.25, 0.222, 0.1875, 0.16, 0.139, 0.122, 0.1094$). From this, I determined that the sequence $x_n$ monotonically decreases to zero as n approaches infinity. which satisfies my first test for series convergance, if $\sum_{n=2}^\infty x_n$ converges, then $\lim_{n\to\infty}x_n=0$. Next, I rearranged the equation in an attempt to perform a comparison test. I re-wrote the equation as $\sum_{n=2}^\infty (\frac1{n}-\frac1{n^2})$. This was to no avail as the only series I could think to compare it to was $\frac1n$ which is always greater than the original series and is divergant, which does not prove convergance to a limit. The ratio test concluded with $\lim_{n\to\infty} \frac{x_{n+1}}{x_n}$ being equal to 1, which is also inconclusive (I can show my work if need be, but that would be a little tedious). I never ran the root test, but I doubt that this would be any help in this case. I see no other way to compute the limit, so any help would be appreciated!!","['sequences-and-series', 'calculus', 'limits', 'convergence-divergence']"
1486170,Let $G'$ be the commutator subgroup of a group $G$. Prove that $G'$ is the intersection of all the $K\mathrel{\unlhd}G$ for which $G/K$ is abelian.,"The Statement of the Problem: Let $G'$ be the commutator subgroup of a group $G$. Prove that $G'$ is the intersection of all the $K\mathrel{\unlhd}G$ for which $G/K$ is abelian. Where I Am: I feel like the wording here is kind of confusing me a bit. I assume that what I want to prove here is $G' = \cap K$, i.e. $G' \subset \cap K$ and $\cap K \subset G'$. But I didn't take the ""double-containment"" approach; rather, I just observed the following: \begin{align}
   (aK)(bK) & = (bK)(aK) & &\text{(because $G/K$ is abelian)} \\
   abK & = baK & & \text{(because $K$ is normal)} \\
   K & = b^{-1}a^{-1}baK & & \text{(multiplying both sides by $b^{-1}a^{-1}$)} \\
\ 
\end{align}
Clearly the RHS is in $K$, but the ""intersection"" thing is throwing me off. My choices of $a$ and $b$ were arbitrary, so certainly this proves that the RHS is also in the intersection of all normal $K$'s, right? Furthermore, since all of these statements are, as far as I can tell, ""iff,"" this means that I don't have to prove the other direction of containment, right? Am I missing something here?","['abstract-algebra', 'group-theory']"
1486188,I'm trying to calculate $e^{At}$. Where do I go wrong?,"Let $$A=\begin{bmatrix}0 & 1 & 0 & 0 \\ -1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & -4 & 0\end{bmatrix}$$ I want to determine $e^{At}$. I tried it using two methods. First via the Jordan form: The characteristic polynomial of $A$ is given by $$\det(I\xi-A)=\begin{vmatrix}
\xi & -1 & 0 & 0 \\ 1 & \xi & 0 & 0 \\ 0 & 0 & \xi & -1 \\ 0 & 0 & 4 & \xi
\end{vmatrix}=1\cdot\begin{vmatrix}
1 & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi
\end{vmatrix}+\xi\cdot\begin{vmatrix}
\xi & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi
\end{vmatrix}$$ $$\begin{vmatrix}
1 & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi
\end{vmatrix}=1\cdot\begin{vmatrix}
\xi & -1 \\ 4 & \xi
\end{vmatrix}=\xi^{2}+4\qquad\text{and}\qquad \begin{vmatrix}
\xi & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi
\end{vmatrix}=\xi\cdot\begin{vmatrix}
\xi & -1 \\ 4 & \xi
\end{vmatrix}=\xi(\xi^{2}+4)$$ $$\implies\det(I\xi-A)=\xi^{2}+4+\xi(\xi^{3}+4\xi)=\xi^{4}+5\xi^{2}+4=(\xi^{2}+4)(\xi^{2}+1)$$ Then the eigenvalues are given by $\lambda_{1,2}=\pm i$ and $\lambda_{3,4}=\pm 2i$. Now, computing the corresponding eigenvectors: For $\lambda=i$, we have $$\begin{cases}
-ix+y=0 \\ -x-iy=0 \\ iz+w=0 \\ -4z-iw=0
\end{cases}\implies\begin{cases}
x=-y \\ 4z=-w
\end{cases}$$ Then setting $x=a$ and $z=b$, for $a,b\in\mathbb{C}$, we obtain: $$v_{1}=\begin{pmatrix}
a \\ -a \\ b \\ -4b
\end{pmatrix}=a\begin{pmatrix}
1 \\ -1 \\ 0 \\ 0
\end{pmatrix}+b\begin{pmatrix}
0 \\ 0 \\ 1 \\ -4
\end{pmatrix}$$ Similarly, for $\lambda=-i$, we compute, for complex $c,d$: $$v_{2}=c\begin{pmatrix}
-1 \\ 1 \\ 0 \\ 0
\end{pmatrix}+d\begin{pmatrix}
0 \\ 0 \\ -1 \\ 4
\end{pmatrix}$$ For $\lambda=2i$, we get: $$v_{3}=j\begin{pmatrix}
1 \\ -1 \\ 0 \\ 0
\end{pmatrix}+k\begin{pmatrix}
0 \\ 0 \\ 1 \\ -2
\end{pmatrix}$$ And finally, for $\lambda=-2i$: $$v_{4}=m\begin{pmatrix}
-1 \\ 1 \\ 0 \\ 0
\end{pmatrix}+n\begin{pmatrix}
0 \\ 0 \\ -1 \\ 2
\end{pmatrix}$$ Define $S=\begin{bmatrix}
v_{1} & v_{2} & v_{3} & v_{4}
\end{bmatrix}$. Then $$S=\begin{bmatrix}
a & -c & j & -m \\ -a & c & -j & m \\ b & -d & k & -n \\ -4b & 4d & -2k & 2n
\end{bmatrix}$$ But I have to calculate $S^{-1}$ via this method (which seems a bit too complicated), since $e^{At}=Se^{S^{-1}ASt}S^{-1}$. Then I tried to compute $e^{At}$ using the theory of autonomous behaviours: So I said that every strong solution of $\frac{d}{dt}x=Ax$ is of the form $$x(t)=B_{10}e^{-it}+B_{20}e^{it}+B_{30}e^{-2it}+B_{40}e^{2it}$$ The vectors $B_{ij}$ should satisfy the relations $$(-i\cdot I-A)B_{10}=0$$ $$(i\cdot I-A)B_{20}=0$$ $$(-2i\cdot I-A)B_{30}=0$$ $$(2i\cdot I-A)B_{40}=0$$ Solving these equations yield $B_{10}=v_{1}$, $B_{20}=v_{2}$, $B_{30}=v_{3}$, $B_{40}=v_{4}$, as calculated previously. Hence every solution of $x$ can be written as $$x(t)=a\begin{bmatrix}-1 \\ 1 \\ 0 \\ 0\end{bmatrix}e^{-it}+b\begin{bmatrix}0 \\ 0 \\ -1 \\ 4\end{bmatrix}e^{-it}+c\begin{bmatrix}-1 \\ 1 \\ 0 \\ 0 \end{bmatrix}e^{it}+d\begin{bmatrix} 0 \\ 0 \\ 1 \\ 4 \end{bmatrix}e^{it} +j\begin{bmatrix} -1 \\ 1 \\ 0 \\ 0 \end{bmatrix}e^{-2it}+k\begin{bmatrix} 0 \\ 0 \\ -1 \\ 2 \end{bmatrix}e^{-2it}+m\begin{bmatrix} 1 \\ -1 \\ 0 \\ 0 \end{bmatrix}e^{2it}+n\begin{bmatrix}0 \\ 0 \\ 1 \\ -2 \end{bmatrix}e^{2it}$$ From this we can obtain four linearly independent solutions: $$x_{1}(t)=\begin{bmatrix}-e^{it} \\ e^{it} \\ 0 \\ 0\end{bmatrix}\qquad x_{2}(t)=\begin{bmatrix}e^{2it} \\ -e^{2it} \\ 0 \\ 0\end{bmatrix}\qquad x_{3}(t)=\begin{bmatrix}0 \\ 0 \\ e^{it} \\ 4e^{it}\end{bmatrix} \qquad x_{4}(t)=\begin{bmatrix} 0 \\ 0 \\ e^{2it} \\ -2e^{2it}\end{bmatrix}$$ The matrix $X$ is defined as $X=\begin{bmatrix}x_{1} & x_{2} & x_{3} & x_{4}\end{bmatrix}$ Then $e^{At}=\Phi(t):=X(t)X^{-1}(0)=\text{something really complicated, so I think I went wrong somewhere}$","['exponential-function', 'control-theory', 'linear-algebra', 'matrices']"
1486218,Evaluate the limit of $\lim_{n\to\infty}\frac{n^{1/3}\sin(n!)}{1+n}$,"I cannot seem to wrap my head around this problem due to its complexity. The answer seems to be that the limit goes to zero for the following reasons: $\sin(x)$ will never be greater than $1$, and thus will only lessen the value of the numerator. If $\sin(x)$ is omitted from the equation, the equation reads $\frac{n^{1/3}}{1+n}$. Therefore, because the power of the denominator is greater than that of the numerator, the denominator will reach infinity faster than the numerator, causing the limit to converge to zero. This answer seems a bit trivial, but I cannot disprove the rationale. Can someone confirm or deny my thinking process? If denied, could you theorize a potential, more rigorous proof?","['analysis', 'sequences-and-series', 'limits']"
1486241,Does a sub-$\sigma$-algebra of a countably generated $\sigma$-algebra have to be countably generated?,Assume that $A \subseteq B$ are $\sigma-$algebras and $B$ is a countably generated (separable) $\sigma-$algebra.  Now my question: Is it possible that $A$ is not countably generated? I'm testing many examples but I can't find any example.,"['examples-counterexamples', 'measure-theory']"
1486249,What does it mean for the determinant of $A^T A$ to be equal to zero?,Are there any statements that are equivalent to $\det(A^T A)=0$?,['linear-algebra']
1486259,What is an n-cell?,"In my Topology lecture notes, 'n-cell' seems to be mentioned a lot, but it never says what exactly it means. Does it mean $n$-dimensional space?","['simplicial-complex', 'terminology', 'general-topology']"
1486283,Solving an integral within an integral,"I have been trying for a while now to solve the following integral: $$ \int_0^t e^{-ct} \left(\int_0^t(1-M_r t)^a e^{ct} dt\right) (1-M_r t)^{-(a+1)} dt  $$ I know the answer is: $$ \frac {1}{M_r^2} [(1-M_r t) \ln( (1-M_r t) -1)] $$ the only approach I can think of is to use integration by parts and setting the integral within the integral to be the function which is differentiated. However, this does not seem to lead to the answer. Does anybody have ANY idea? A reference, term I could search. Literally anything that could point me to the right trail would be helpful.","['indefinite-integrals', 'definite-integrals', 'ordinary-differential-equations', 'integration', 'integration-by-parts']"
1486288,Showing $\left\lbrace x: \lim f_n(x) \text{ exists} \right\rbrace$ is measurable,"Assuming $\left\lbrace f_n\right\rbrace$ is measurable for all n, I was able to show using the fact that sequences of real numbers converge iff they are Cauchy that: $\left\lbrace x:  \lim f_n(x) \text{ exists} \right\rbrace =\bigcap_{k=1}^{\infty} \bigcup_{N=1}^{\infty} \bigcap_{m=N}^{\infty} \bigcap_{n=N}^{\infty} \left\lbrace x:  f_n(x)-f_m(x) < \frac{1}{k} \right\rbrace \cap \left\lbrace x:  f_m(x)-f_n(x) < \frac{1}{k} \right\rbrace$ Since $f_n$ is measurble for all n, $\left\lbrace x:  f_n(x)-f_m(x) < \frac{1}{k} \right\rbrace $ should be a measurable set for all $n,m,$ and $k$, and then since there's a bunch of countable intersections and unions of all these sets, I should have that $\left\lbrace x:  \lim f_n(x) exists \right\rbrace$ is measurable as well. What I don't understand is why $f_n$ being measurable immediately implies that $\left\lbrace x:  f_n(x)-f_m(x) < \frac{1}{k}\right\rbrace $ is measurable. We just started learning about measurable functions and I know that $f:X \rightarrow Y$ is $M,N-$measurable if $f^{-1}(E) \in M$ for all $E \in N$ where $M$ is the sigma-algebra on $X$ and $N$ the sigma algebra on $Y$, but I don't see how to apply that definition here.","['functions', 'measure-theory']"
1486383,How do you find the equation of a tangent line to an equation: $\sin$ in it?,"This is the question I need to answer, but I don't know how to. Find an equation of the tangent line to $y=10\sin(x)$ at $x=\pi$.","['calculus', 'derivatives']"
1486407,Strong Nullstellensatz without the Rabinowitsch trick,All the textbooks I've seen prove the Strong Nullstellensatz from the Weak Nullstellensatz using the Rabinowitsch trick. How did they prove the Strong Nullstellensatz before the Rabinowitsch trick was invented?,"['algebraic-geometry', 'commutative-algebra']"
1486409,Determining from its graph whether a vector field is conservative,"Given the graph of a vector field, how can I tell whether it is conservative or non-conservative?","['vector-fields', 'calculus', 'multivariable-calculus']"
1486413,Is my method for proving this implication correct?,"My professor put up the lecture slides from today's lecture and I decided to go over the proof again since I didn't quite catch it in class. I think I found something wrong with her proof. Please let me know if mine is correct or if I am wrong: Proving an Implication: Theorem: If $0\le x\le 2$ then $-x^3+4x+1>0$ Proof: Assume $0\le x\le 2$ Then $0\le x^2\le 4$ (since $0\le a,b\quad \& \quad a\le b\Rightarrow a^2\le b^2$) $-4\le -x^2\le 0$ $0\le -x^2+4\le 4$ $0\le x(-x^2+4)\le 4x$ $0\le -x^3+4x\le 4x$ $1\le -x^3+4x+1\le 4x+1$ More specifically, $-x^3+4x+1>0$ The proof from the lecture slides:",['discrete-mathematics']
1486440,$L^q(X) \subset L^p(X)$ if $p\leq q$ and $\mu(X) < \infty$.,"Let $(X,\mathfrak{M},\mu)$ be a measure space. Show that if $\mu(X) < \infty$ then $L^q(X) \subset L^p(X)$ for $1\le p \le q \le \infty$. Is it enough to define $$E_0 = \{x \in X \, : \, 0 \leq |f(x)| < 1\}$$ and
$E_1 = X \setminus E_0$ so that $\mu(E_0), \, \mu(E_1) < \infty$ and $E_0$ is measurable as $E_0 = \{x \in X : |f(x)| \geq 0 \} \cap \{x \in X : |f(x)| \leq 1 \}$ and $f$ is measurable. Therefore if $f \in L^q$, \begin{eqnarray*}
||f||^p_{L^p} &=& \int_{E_0}|f|^p \, d\mu + \int_{E_1} |f|^p \, d\mu \\
&\leq& \int_{E_0}|f|^p \, d\mu + \int_{E_1} |f|^q \, d\mu \\
&<& \mu(E_0) \,\, + ||f||_{L^q}^q \\
&<& \infty.
\end{eqnarray*} which implies $f \in L^p$. Doe this suffice? I've seen a proof using Holder's inequality (that has now been given as an answer for those interested), that is much shorter, but does this suffice?","['measure-theory', 'proof-verification', 'real-analysis', 'lp-spaces', 'analysis']"
1486457,Finding the commutator subgroup of $S_3$ and the commutator subgroup of that commutator subgroup.,"I'm finding this idea very frustrating. I need to find the commutator subgroup of $S_3$ (call it $S_3'$) and the commutator subgroup of that commutator subgroup (call it $S_3 ''$). So, I have that, for $S_3$, the commutator of $a,b \in S_3$ is given by $$ [a,b] :=aba^{-1}b^{-1}. $$ And the commutator subgroup is the subgroup generated by all such $[a,b]$, i.e., $$ S_3 ' = <[a,b]:a,b, \in S_3> .$$ Ok, I have read about everything I can on this and watched every single YouTube video on the subject I can find, and have yet to see it just worked out explicitly in a way that makes sense to me. For example: these elements $a,b$. Do they need to be distinct from one another?! Is this an ordered pair, or not?! I know that I only need to check the normal subgroups, of which there are only three, but that's still a lot of things to check if they do not need to be distinct and need to be ordered. I just feel like there are a lot of basic questions here that I'm not seeing addressed anywhere in the literature, which makes me feel like I'm ""missing the point,"" in a way. EDIT: for $S_3 ' = \{e, (123), (132) \}$ here are all of my computations in an attempt to determine $S_3 ''$: $$ [e, (123)] = e \\
   [e, (132)] = e \\
[(123, (132)] = e \\
[(132), (123)] = e \\
[(123), e] = e \\
[(132), e] = e \\
   $$ Ok. So, certainly the subgroup generated by $e$ is simply $e$, which means we get to conclude that $S_ 3 '$ is abelian? Is that it?","['abstract-algebra', 'group-theory']"
1486464,Find all five solutions of the equation $z^5+z^4+z^3+z^2+z+1 = 0$,$z^5+z^4+z^3+z^2+z+1 = 0$ I can't figure this out can someone offer any suggestions? Factoring it into $(z+1)(z^4+z^2+1)$ didn't do anything but show -1 is one solution. I solved for all roots of $z^4 = -4$ but the structure for this example was more simple.,"['roots', 'roots-of-unity', 'complex-analysis', 'complex-numbers']"
1486475,Show that conjugate by $g$ is isomorphism,"This is a question from Dummit and Foote. 
I am still a novice at algebra so any feedback on my work would be appreciated! Let $G$ be a group and let $G$ act on itself by left conjugation, so each
        $g \in G$ maps $G$ to $G$ by \begin{align*}
		x\mapsto gxg^{-1}
\end{align*}
For fixed $g \in G$, prove that conjugation by $g$ is an isomorphism from
        $G$ onto itself (i.e. an automorphism of $G$). Deduce that $x$ and 
        $gxg^{-1}$ have that same order for all $x \in G$ and that for any subset
        $A$ of $G$, $\vert A \vert=\vert gAg^{-1}\vert$ (here $gAg^{-1}=
		\{gag^{-1}\vert a \in A\}$.) Take $g \in G$. We show that $\sigma_{g}:G \rightarrow G$ defined by $\sigma_{g}(x) = gxg^{-1}$
        is a homomorphism and is bijective. 
        First, we show that $\sigma_g$ is homomorphism. Take $x,y \in G$. 
        \begin{align*}
			\sigma_g(xy) &= g(xy)g^{-1} \\
				&= gxg^{-1}gyg^{-1} \\
				&= \sigma_g(x) \sigma_g(y)
		\end{align*}
        Hence, $\sigma_g$ is homomorphism.
        Second, we show that injectivity by showing $ker(\sigma_g) = \{1\}$. 
        Suppose, $gxg^{-1}=1$. Then, 
        \begin{align*}
			xg^{-1} &= g^{-1} \\
			x &= g^{-1}g = 1
		\end{align*}
        Hence, $ker(\sigma_g) = \{1\}$.
        Now, we show $\sigma_g$ is surjective. 
        Take $g \in G$. Then, for $x = g^{-1}yg \in G$ (since $G$ is a group, $x \in G$),
        $g(g^{-1}yg)g^{-1} = y$. 
        Therefore, $\sigma_g$ is bijective and it follows that it is a isomorphism.
        Since $\sigma_g$ is isomorphism, it follows that for every $x \in G$, $|x| = |\sigma_g(x)|=|gxg^{-1}|$.
        Furthermore, since $\sigma_g$ is bijective on $G$, $\sigma_g$ is bijective on $A \subset G$. 
        In particular, $\sigma_g$ is injective when restricted to $A$. 
        Hence, $|A|= |im(\sigma_{g|A})|=|gAg^{-1}|$.","['abstract-algebra', 'proof-verification']"
1486487,"Conjectured closed form for $\int_0^1\frac{\operatorname{li}^4(x)}{x^4}\,dx$","It has been conjectured by Kirill that
$$\int_0^1\frac{\operatorname{li}^4(x)}{x^4}\,dx=\int_0^\infty\operatorname{Ei}^4(-x)\,e^{3x}\,dx\stackrel{\color{gray}?}=\frac{26\pi^4}{135}+\frac49\left(\pi^2\ln^22-\ln^42\right)-\frac{32}{3}\operatorname{Li}_4\!\left(\tfrac12\right).$$ Can we prove this conjecture?","['calculus', 'special-functions', 'closed-form', 'definite-integrals', 'integration']"
1486494,Describe the subfields of $\mathbb{C}$ of the form: $\mathbb{Q}(\alpha)$ where $\alpha$ is the real cube root of $2$.,"Describe the subfields of $\mathbb{C}$ of the form: $\mathbb{Q}(\alpha)$ where $\alpha$ is the real cube root of $2$. Let $\alpha$ be the real cube root of $2$, and consider $\mathbb{Q}(\alpha)$. As well as $\alpha$, the subfield $\mathbb{Q}(\alpha)$ must contain $\alpha^2$. We show that $$\alpha^2\neq j+k\alpha \text{ for } j,k \in \mathbb{Q}.$$ For a contradiction, suppose that $\alpha^2=j+k\alpha$. Then $$2=\alpha^3=\alpha(j+k\alpha)=j \alpha + k \alpha^2=j\alpha + k(j+k\alpha)=j\alpha+jk+k^2\alpha=jk+(j+k^2)\alpha.$$ Therefore $(j+k^2)\alpha=2-jk$. Since $\alpha$ is irrational, 
$j+k^2=0=2-jk.$ Note that $j+k^2=0 \iff -j=k^2$, so $$j+k^2=0=2-jk \iff k^3=2,$$ which is a contradiction because $k\in \mathbb{Q}$. \null
In fact, $\mathbb{Q}(\alpha)$ is precisely the set of all elements of $\mathbb{R}$ of the form $$p+q\alpha + r\alpha^2, \text{ where } p,q,r\in \mathbb{Q}.$$ To show this, we prove that the set of such elements is a subfield. We will show that every element of $\mathbb{Q}(\alpha)$ can be expressed in this way. Set 
$$X=\{p+q\alpha+r\alpha^2 | p,q,r \in \mathbb{Q} \}.$$ $X$ is a subgroup of the additive group $(\mathbb{Q}(\alpha), +)$. $1\in X$ is an identity element for multiplication. Multiplication between to elements: 
$$ (p+q\alpha+r\alpha^2)(p'+q'\alpha+r'\alpha^2) = p'p+(p'q+pq')\alpha+(p'r+pr'+qq')\alpha^2 +(r'q+rq')\alpha^3+rr'\alpha^4 $$ I know that I can't have $\alpha^4$, so I need to rewrite it. How would I do that? Answer: $$\alpha^4=2\alpha$$ How would I approach find the inverse of $p+q\alpha+r\alpha^2$?","['abstract-algebra', 'field-theory']"
1486502,Understanding the final part of the proof of Mills' theorem,"Mills proves that there is a real number $A$ such that $\lfloor A^{3^n}\rfloor$ is a prime number for every integer $n\ge 1$. After having proved that there is a prime between any two sufficiently large consecutive cubes, Mills constructs an infinite sequence of primes $P_0,P_1,P_2,$ $\cdots$ such that $(P_n)^3<P_{n+1}<(P_n+1)^3$. He then defines two functions that he applies to his sequence of primes $P_0,P_1,P_2,$ $\cdots$. Such functions are: $$u(n)=\sqrt[3^n]{P_n}$$
$$v(n)=\sqrt[3^n]{P_n+1}$$ He then proves the following three statements: 1) $$\sqrt[3^n]{P_n+1}>\sqrt[3^n]{P_n}$$ 2) $$\sqrt[3^{n+1}]{P_{n+1}}>\sqrt[3^n]{P_n}$$ 3) $$\sqrt[3^{n+1}]{P_{n+1}+1}<\sqrt[3^n]{P_n+1}$$ Unfortunately, I don't understand what follows, starting from ""It follows at once that the $u_n$ form a bounded monotone increasing sequence"". What is a bounded monotone increasing sequence? And what is $\lim_{n\to\infty}{u_n}$? I'd like to understand this final part of the proof. Thanks in advance!!","['prime-numbers', 'number-theory']"
1486514,"In the most basic of terms, what does contraction mapping mean?","So, I have been solving a number of exercises involving contraction mapping. However, I am struggling to understand what exactly contraction mapping is at its most basic. All I really understand from the concept is that if we have two sets, then one specific point on the set is being mapped closer to another - I'm not sure if I have the right definition. Can somebody please explain this to me in the most layman way possible?","['economics', 'metric-spaces', 'discrete-mathematics']"
1486515,"Given $\{A_n\}$, with $P\left(A_n\right)\to 1$, there exists a subsequence $\{n_k\}_{k\ge 1} \;$ such that $\; P\left(\bigcap_k A_{n_k}\right) > 0$","This is problem 4.21a is Resnick's Probability Path . Suppose $\{A_n\}$ is a sequence of events. If $P(A_n)\to 1$ as $n \to \infty$, prove that there exists a subsequence $\{n_k\}$ tending to infinity such that $P(\bigcap_k A_{n_k})>0$. (Hint: Use Borel-Cantelli.) Having searched a bit on here for related problems, I've gotten the impression that what most people mean by Borel-Cantelli is something a little more expansive than what Resnick means, which is that for some sequence of (not necessarily independent) events $\{A_n\}$, if $\sum P(A_n) < \infty$, then $P(\limsup_n A_n) = 0$. Since the $\{A_n\}$ in question are not independent, I can't use any results on sequences whose sums don't converge. Clearly, $P(A_n)\to 1$ implies that $\sum P(A_n) = \infty$, and so I can't directly apply Borel-Cantelli. Moreover, even though $P(A_n^c)\to 0$, this does not imply that $\sum P(A_n^c)<\infty$, so I can't do anything with the complements. I'm not sure what else to do:  I mean, if I could somehow determine that $P(\limsup_n A_n) = 1$, I could show that $\sum P(A_n) = \infty$ ... but then I could just reread the first sentence of this paragraph. Could anyone offer any suggestions?","['probability-theory', 'borel-cantelli-lemmas', 'real-analysis', 'limsup-and-liminf']"
1486516,Borel $\sigma$-algebra of continuous functions,"Let $B(C(T,\mathbb{R}))$ be the Borel sigma algebra of continuous functions mapping from the compact metric space $T$ to $S$ defined by the canonical metric $\|\cdot\|_\infty.$ Now I was wondering whether we can write this sigma algebra also as a sigma-algebra generated by some cylinder sets? If anything is unclear, please let me know.","['measure-theory', 'stochastic-calculus', 'real-analysis', 'stochastic-processes', 'analysis']"
1486533,How can I get eigenvalues of infinite dimensional linear operator?,"What I want to prove is that for infinite dimensional vector space, $0$ is the only eigenvalue doesn't imply $T$ is nilpotent. But I am not sure how to find eigenvalues of infinite dimensional linear operator $T$. Since we normally find eigenvalues by find zeros of characteristic polynomials, we even cannot find the characteristic polynomial for this situation. I am specifically interested in the differential operator on the vector space of all formal power series.","['infinite-matrices', 'operator-theory', 'functional-analysis', 'linear-transformations']"
1486537,Quotient of algebraic group by subgroup with trivial character group,Let $G$ be a linear algebraic group and $H$ be a closed subgroup. Suppose that all homomorphisms of algebraic groups $H\to\mathbb{G}_m$ are trivial. How to prove that $G/H$ is quasi-affine?,"['algebraic-groups', 'algebraic-geometry']"
1486611,Convolution of two Uniform random variables,"We have $X \sim \mathrm{Unif}[0,2]$ and $Y \sim \mathrm{Unif}[3,4]$. The random variables $X,Y$ are independent. We define a random variable $Z = X + Y$ and want to find the PDF of $Z$ using convolution.  Here is my work so far: The definition of convolution is: $f_Z(z) = \int_{-\infty}^{\infty}f_X(x)f_Y(z-x)\mathrm{d} x$ We know the PDF's of $X$ and $Y$ because they are just uniform distributions. The hard part for me is finding the limits of integration. We have to solve for the constraints. The integrand is nonzero when $3 \leq z-x \leq 4$ and when $0 \leq x \leq 2$. Together these constraints imply that $\max \{0, z-4\} \leq x \leq \min \{2, z-3 \}$. These constraints imply that there are three cases: Case 1 - $z \leq 4 \implies f_Z(z) = \int_0^{z-3}$ Case 2 - $4 \leq z \leq 5 \implies f_Z(z) = \int_{z-4}^{z-3}$ Case 3 - $z \geq 5 \implies f_Z(z) = \int_{z-4}^{2}$ My question is how to find the bounds of $Z$ i.e. what are the possible values of $Z$? Does $Z$ run from $0 \to 6$ since it is the sum of $X+Y$ and this sum will have some value for every value $\in [0,6]$?","['probability-theory', 'probability', 'probability-distributions', 'convolution']"
1486632,Why does $\sum_{n=1}^{\infty}\frac{\cos\frac{1}{n}}{n}$ diverge but $\sum_{n=1}^{\infty}\frac{\sin\frac{1}{n}}{n}$ converges?,"Why does $\sum_{n=1}^{\infty}\frac{\cos\frac{1}{n}}{n}$(I'm told it can be compared to the harmonic sequence, but I don't see the tangible comparison) diverge but $\sum_{n=1}^{\infty}\frac{\sin\frac{1}{n}}{n}$ converges?","['analysis', 'sequences-and-series', 'calculus', 'trigonometry']"
1486673,Is X a subset of the {X}?,"I'm new to this website. I'm trying to understand the precise definition of a subset. In particular, this problem is tripping me up: 
True or False? 
$\{\emptyset, \{\emptyset\}\} \subseteq \{\{\emptyset, \{\emptyset\}\}\}$
I know that any set X is a subset of itself. I'm taking a guess however, that the answer to this is false. Is this because $\{\{\emptyset, \{\emptyset\}\}\}$ contains only one element which is $\{\emptyset, \{\emptyset\}\}$?",['discrete-mathematics']
1486709,Linear Algebra Invertible Matrix Theorem Proof,"Part of the proof for this theorem asks you to show that if $A$ is an $n \times n$-matrix and there exists an $n \times n$-matrix $D$ such that $AD = I$ (the $n \times n$-identity matrix), then the equation $Ax = b$ has at least one solution for each $b$ in $\mathbb{R}^n$. In order to prove this, I simply started with the hypothesis that $AD = I$ and right-multiplied each side of the equation by the $\mathbb{R}^n$ vector $b$ to get: $AD(b) = I(b) = b.$ By the associative property for matrix multiplication, this leads to $A(Db) = b$, which means that for every $b$ in $\mathbb{R}^n$, the vector $Db$ is a solution so that $Ax = b$ has at least that one solution for every $b$ in $\mathbb{R}^n$. I'm pretty sure that's one way of proving it, but can anyone give me any hints about a different or possibly more insightful way of proving this step? It's definitely one of the least straight-forward inferences in the theorem.","['linear-transformations', 'linear-algebra', 'matrices']"
1486733,Trace Class: Counterexample,"This is a real question! Given a Hilbert space $\mathcal{H}$. Denote trace class by:*
$$\mathcal{B}_\textrm{Tr}(\mathcal{H}):=\{A\in\mathcal{B}(\mathcal{H}):\operatorname{Tr}|A|<\infty\}$$ Then it seems wrong:
$$A,B\in\mathcal{B}(\mathcal{H}):\quad BA\in\mathcal{B}_\textrm{Tr}(\mathcal{H})\implies AB\in\mathcal{B}_\textrm{Tr}(\mathcal{H})$$ Have a counterexample? *Remind: Definition","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
1486747,How is $\frac{ds}{dt}$ related to $\frac{dx}{dt}$?,"The problem states: Let $x$ and $y$ be differentiable functions of $t$, and let $s = \sqrt{4x^2+6y^2}$ be a function of $x$ and $y$. How is $\frac{ds}{dt}$ related to $\frac{dx}{dt}$ if $y$ is constant? My attempt: \begin{align}
\frac{ds}{dt} & = \frac{d}{dt} \left( \sqrt{4x^2+6y^2} \right) \\
 & = \frac{1}{2\sqrt{4x^2+6y^2}}*\left(8x\frac{dx}{dt}+12y\frac{dy}{dt}\right)
\end{align} If $y$ is constant, then $\frac{dy}{dt}=0$ \begin{align}
\frac{ds}{dt} & = \frac{1}{2\sqrt{4x^2+6y^2}}*\left(8x\frac{dx}{dt}+12y*0\right)\\
& =\left(\frac{8x}{2\sqrt{4x^2+6y^2}}\right)*\frac{dx}{dt}
\end{align} This is the relation I have found, but it is not correct. What is the relation between $\frac{ds}{dt}$ and $\frac{dx}{dt}$ ?","['calculus', 'derivatives']"
1486766,Equivalency of two forms of Lebesgue measurability,"Following is the CarathÃ©odory's criterion for Lebesgue measurability $A\subset \mathbb{R}$ is Lebesgue measurable $\textbf{iff}$ for any set B (measurable or not) $m_*(B)=m_*(B\cap A)+m_*(B -A)$. where $m_*(B)$ is the outer measure of $A$. How one could show that this is equivalent to other criteria of Lebesgue measurability such as $A\subset \mathbb{R}$ is Lebesgue measurable $\textbf{iff}$ for any $\epsilon>0$ there exists an open set $O$ that contains $A$ and $m_*(O-A)<\epsilon$. The above criterion can be found in Stein Shakarchi ""Real Analysis"" book printed 2007, page 21, Theorem 3.4.","['lebesgue-measure', 'real-analysis', 'measure-theory']"
1486767,Why in a directional derivative it has to be a unit vector [duplicate],"This question already has an answer here : why normalize and the definition of directional derivative (1 answer) Closed 3 years ago . Why when we compute the directional derivative we have to use the unit vector $\vec{\bf{u}}$ ? I know that using $2\vec{\bf{u}}$ would change the directional derivative as the second point is further apart. But why not use $\frac{\vec{\bf{u}}}{2}$ then? Wouldn't that gradient be ""more precise""?","['partial-derivative', 'calculus', 'multivariable-calculus', 'derivatives']"
1486776,Normalizer of G viewed as a subgroup of S_G,"If we take $G$ to be a group, we can view it a subgroup of $S_G$ by Cayley's theorem (we're considering the left action). Then the centralizer of $G$ in $S_G$ is all bijections $f$ such that $f(xy) = xf(y)$ and it's not hard to see that this is in fact the right translations. In other words $C(G) = G$. But how does one go about computing the normalizer N(G)? I keep reading that it should be $G \rtimes Aut(G)$ but I don't see where that's coming from. How does one end up with a semidirect product? Any insights would be useful. Thanks!","['abstract-algebra', 'finite-groups', 'permutations']"
1486784,Proving $\Gamma (\frac{1}{2}) = \sqrt{\pi}$,"There are already proofs out there, however, the problem I am working on requires doing the proof by using the gamma function with $y = \sqrt{2x}$ to show that $\Gamma(\frac{1}{2}) = \sqrt{\pi}$ . Here is what I have so far: $\Gamma(\frac{1}{2}) = \frac{1}{\sqrt{\beta}}\int_0^\infty x^{-\frac{1}{2}}e^{-\frac{x}{\beta}} \, dx.$ Substituting $x = \frac{y^2}{2}\, dx = y\,dy$ , we get $\Gamma(\frac{1}{2}) = \frac{\sqrt{2}}{\sqrt{\beta}}\int_0^\infty e^{-\frac{y^2}{2\beta}} \, dy$ . At this point, the book pulled some trick that I don't understand. Can anyone explain to me what the book did above? Thanks.","['gamma-function', 'statistics', 'special-functions']"
1486828,How many ways can 12 different pennies be distributed to four people without each person getting exactly 3?,Question: How many ways can $\mathbf{12}$ different pennies be distributed to four people without each person getting exactly $\mathbf{3}$? My thoughts: I'm really not too sure how to approach this question. Any recommendations for a less visual way of solving this problem - a more systematic approach.,"['discrete-mathematics', 'permutations', 'combinations', 'combinatorics', 'probability']"
1486833,How to deduce the recursive derivative formula of B-spline basis?,"Description Let $\vec{U}=\{u_0,u_1,\ldots,u_m\}$ denotes a non-decreasing sequence of real numbers, i.e, $u_i\leq u_{i+1} \quad i=0,1,2\ldots m-1$. and the $i$-th B-spline basis function of $p$-degree, denoted by $N_{i,p}(u)$, is defined as below: $$
N_{i,0}(u)=
\begin{cases}
   1 & u_i\leq u<u_{i+1}\\
   0 & otherwise
\end{cases}
$$
$$
N_{i,p}(u)=
\frac{u-u_i}{u_{i+p}-u_i}N_{i,p-1}(u)+\frac{u_{i+p+1}-u}{u_{i+p+1}-u_{i+1}}N_{i+1,p-1}(u)
$$ By reading the textbook The NURBS Book, I can know the following recursive formula about the derivetive of $N_{i,p}(u)$. $$
\frac{d}{du}N_{i,p}(u)=p\left[ \frac{N_{i,p-1}(u)}{u_{i+p}-u_i}-\frac{N_{i+1,p-1}(u)}{u_{i+p+1}-u_{i+1}} \right] \qquad (1)
$$ In addition, I can also understand the verification process by mathematical induction that the author given in the textbook pp.59-60. Namely, (1) Varifying the correctness of this recursive formula for $p=1$; (2) Assuming this formula is right for $p=k$, then proved that this formula is also right for $p=k+1$ with help of the assumption. However, I would like to know where this formula came from . The author just given the conclusion and proved it by mathematical induction . QUESTION How to deduce the derivative formmula of the B-spline basis function $N_{i,p}(u)$? Although the author has given a related reference The Computation of all the Derivatives of a B-spline Basis in the bibliography, I cannot download that paper by the libriary of our university. In addition, the reference just for another recursive formula(please see Eq.(2) ), not for Eq.(1) . $$
N_{i,p}^{(k)}=\frac{p}{p-k}\left(\frac{u-u_i}{u_{i+p}-u_i}N_{i,p-1}^{(k)}+\frac{u_{i+p+1}-u}{u_{i+p+1}-u_{i+1}}N_{i,p-1}^{(k)}\right)
\quad (2)
$$ where $k=0,1,\cdots,p-1$ Lastly, I discovered that Eq.(1) was useful than Eq.(2) , and it was implemented in Wolfram Mathematica . For instance, knots = {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1};
D[BSplineBasis[{3, knots}, 2, x], x]
(*9/2 BSplineBasis[{2, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 2, x] - 
    3 BSplineBasis[{2, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 3, x]*)
D[BSplineBasis[{3, knots}, 2, x], {x, 2}]
(*9/2 (6 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 2, x] - 
       3 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 3, x]) - 
  3 (3 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 3, x] - 
     3 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 4, x])*) Thanks a lot! :)","['recursion', 'derivatives']"
1486850,Determine whether the series $\sum_{n=1}^{+\infty}\left(1+\frac{1}{n}\right)a_{n}$ is convergent or divergent [duplicate],"This question already has an answer here : Too simple proof for convergence of $\sum_n a_n b_n$? (1 answer) Closed 8 years ago . If
$$\sum_{n=1}^{\infty}a_{n}$$divergent, determine whether the series
$$\sum_{n=1}^{+\infty}\left(1+\dfrac{1}{n}\right)a_{n}$$ is convergent or divergent. I know I have to use the ratio test.","['sequences-and-series', 'convergence-divergence', 'real-analysis']"
1486878,Proving whether the following are groups or not.,"In each case, I am asked to decide whether the indicated pair is a group or not. If so, prove it; if not, show which group axiom fails. (a) $(\dfrac{1}{2}\mathbb{Z}, +)$ where $\dfrac{1}{2} \mathbb{Z} = \{\dfrac{n}{2} | n \in \mathbb{Z}\}$. (b) $(\mathbb{R_{\geq 0}}, *)$ where $x * y = max\{x,y\}$. As for (a), it appears that $\dfrac{1}{2}\mathbb{Z}$ is closed with respect to addition since given $x,y \in \dfrac{1}{2}\mathbb{Z}$, we have $x = \dfrac{n_1}{2}, y = \dfrac{n_2}{2}$, for some $n_1,n_2 \in \mathbb{Z}$ and so $x + y = \dfrac{n_1 + n_2}{2}$ and clearly $n_1 + n_2 \in \mathbb{Z}$ so $x+y \in \dfrac{1}{2} \mathbb{Z}$. It's associative as well since $x,y,z \in \dfrac{1}{2}\mathbb{Z}$ is such that $x = \dfrac{n_1}{2}, y = \dfrac{n_2}{2}, z = \dfrac{n_3}{2} = \dfrac{n_1 + n_2 + n_3}{2}$ in either case. There's an identity element since $0 \in \dfrac{1}{2}\mathbb{Z}$ because $ 0 = \dfrac{0}{n}$. And there exists inverses since for any $x = \dfrac{n_0}{2} \in \dfrac{1}{2}\mathbb{Z}$ we have $x^{-1} = \dfrac{-n_0}{2}$ and $\dfrac{n_0 - n_0}{2} = 0 = e_{\dfrac{1}{2}\mathbb{Z}}$. So $(\dfrac{1}{2}\mathbb{Z},+)$ is a group. (b) This is not a group since we do not have a $unique$ identity element since while $\forall x \in \mathbb{R_{\geq 0}}$ we have $x*x = x$, there is no unique $e$ such that $e*x = x$ that will not depend on $x$. Is this reasoning correct?","['inverse', 'group-theory']"
1486893,Under what circumstances does a theory of infinite sums show that $1 - 1 + 1 - 1 + 1 -1 + \dots$ does not have a value?,"I am reading the excellent book How Not to Be Wrong by Jordan Ellenberg. He points out the argument by Guido Grandi from 1703 that: (1)  Let $T = 1 - 1 + 1 - 1 + 1 - 1 + \dots$ (2)  $-T = -1 + 1 - 1 + 1 - 1 + 1 - \dots$ (3)  So, $-T = T - 1$ and $T = \frac{1}{2}$ Ellenberg writes: Modern mathematicians would say that if we are to assign the Grandi series
  a value, it should be $\frac{1}{2}$, because, as it turns out, all
  interesting theories of infinite sums either give it a value of
  $\frac{1}{2}$ or decline, like Cauchy's theory, to give it a value at
  all. How does one ever establish that all ""interesting"" theories of infinite sums give it a value of $\frac{1}{2}$ or decline to give it a value at all?","['sequences-and-series', 'divergent-series']"
1486895,How to prove this $p^{j-\left\lfloor\frac{k}{p}\right\rfloor}\mid c_{j}$,"Let $p$ be a prime number and $g\in \mathbb{Z}[x]$. Let $$\binom{x}{k}=\dfrac{x(x-1)(x-2)\cdots(x-k+1)}{k!} \in \mathbb{Q}[x]$$ for every $k \geq 0$. Fix an integer $k$. Write the integer-valued polynomial $\binom{g(px)}{k}$ in the form
$$\binom{g(px)}{k}=\sum_{i=0}^{\infty}c_{i}\binom{x}{i}$$
for some fixed $c_0, c_1, c_2, \ldots \in \mathbb{Z}$, where all but finitely many $i\geq 0$ satisfy $c_i \neq 0$. show that
$$p^{j-\left\lfloor\frac{k}{p}\right\rfloor}\mid c_{j},\text{where } j=0,1,2,\ldots$$","['polynomials', 'number-theory', 'binomial-coefficients', 'algebraic-combinatorics']"
1486904,Infinitely many pairwise congruent solutions modulo $c$?,"The following is from my number theory textbook.. In particular, there exists an integer $c$ such that there are infinitely many solutions to the equation $x^2 - dy^2 = c$. Since there are only a finite number of classes modulo $c$, there even exist infinitely many pairwise congruent solutions modulo $c$. Why is the second sentence true? It's not very clear to me...","['number-theory', 'elementary-number-theory']"
1486911,"Is f(x,y) differentiable at the origin?","My book defines $f(x,y)$ is differentiable at $(a,b)$ when $\lim_{(x,y)\to(a,b)} \frac{R_{1,(a,b)} (x,y)}{\|(x,y)-(a,b)\|}=0$ where $R_{1,(a,b)}(x,y)=f(x,y)-L_{(a,b)} (x,y)$ and $L_{(a,b)}(x,y)=f(a,b)+\frac{âf}{âx}(x-a)+\frac{âf}{ây}(y-b)$ If $f(x,y)=\frac{x^3 +y^3}{x^2+y^2}$ for $(x,y) \ne (0,0)$ and $f(x,y)=0$ for $(x,y)=(0,0)$, what would $f(x,y)$ be in $R_{1,(0,0)} (x,y)$? Unsure of what to use since it $f(x,y)$ is defined separately. If I assume $f(x,y)$ is 0, I would get $\lim_{(x,y)\to(0,0)} \frac{R_{1,(a,b)} (x,y)}{\|(x,y)-(0,0)\|}=\frac{-x-y}{\sqrt{x^2 +y^2}}$ which equals $\frac{-\sqrt{2}x}{|x|}$ which is not $0$ when you approach with $y=x$ at $(x,y)=(0,0)$.","['calculus', 'multivariable-calculus']"
1486934,A Set of Trace Inequalities,"Let $\rho$ be a $N \times N$ positive semi-definite Hermitian matrix.  Let $S$ be a $N \times N$ matrix that squares to the identity.  Consider an arbitrary word constructed by multiplying $2n$ $\rho$'s together and an even number of $S$'s.   As I'm about to take the trace of the word, words are considered distinct only if they cannot be related by a cyclic permutation.  For example, when $n=4$, the distinct words would be $\rho^4$, $(\rho^2 S)^2$, $\rho S \rho^3 S$, and $(\rho S)^4$. I have reason to suspect that for fixed $n$, $\rho^{2n}$ is the word with the largest trace and $(\rho S)^{2n}$ is the word with the smallest trace.  Numerical experiments on random matrices along with multiplying out $2 \times 2$ matrices suggest that the trace decreases as the number of $S$'s increase.  Also, as we distribute the $S$'s more evenly among the $\rho$'s, the trace also appears to decrease. For certain words, relevant inequalities can be derived starting with the Lieb-Thirring inequality or the von Neumann inequality ( see this wikipedia page ).
For example, in the $2n=4$ case, let's define $\sigma = S \rho S$.  As $\sigma$ and $\rho$ are related by a similarity transformation, they have the same spectrum and in particular are both positive semi-definite.  Then by Lieb-Thirring, we have
$$
\mbox{tr} ((\rho S)^4 )= \mbox{tr}( (\rho \sigma)^2) \leq \mbox{tr} (\rho^2 \sigma^2) = \mbox{tr}((\rho^2 S)^2)
$$ But I haven't been able to find a proof for general $n$.  I was hoping someone might have a proof, or a suggestion of a strategy, or a counter-example.","['linear-algebra', 'inequality', 'matrices']"
1486938,Use Trigonometry to make a Marksman's Sight,"In the Civil War, marksmen used something called a ""stadia"" which is nothing more than a piece of brass with a triangular hole cut out and a slide with marks of varying distances. Line your target up within the hole, center the slide over it, and you have the distance you need to make your shot more accurate. http://i.ytimg.com/vi/FwGDPhNhXhg/maxresdefault.jpg Example of a brass stadia. I, however, am not a student of math, algebra, nor trigonometry. If I hold the Stadia at arm's length (30.5 inches away from my eye) and sight a deer that stands on average at 3.9 feet tall at 100 yards away, what is the relative height of the deer on the stadia?",['trigonometry']
1486940,"Given an integer-valued polynomial, can its sequential values be used to determine whether it is reducible?","Let $p(x)$ be an integer-valued polynomial of degree $n$.  Is it possible to use up to $n$ consecutive values to potentially identify $p(x)$ as irreducible over the set of integer-valued polynomials? I managed to prove that for $n=2$ there is a definite upper limit for the 3rd sequential value, but it is very difficult for higher degrees, even with induction.  I encountered this question while considering some primes appearing in polynomial sequences.  I have attempted to use Lagrange interpolation and binomial forms and I haven't made any progress past $n=2$. Here is a sketch for the proof that there is an upper bound for the described polynomial with $n=2$: We have fixed positive integers $v_1,v_2, v_1\le v_2$ and polynomial in binomial form $$p(x) = a{x\choose 2}+(v_2-v_1)x+v_1$$ (Notice that we have set $p(0) = v_1,p(1) = v_2$.)  Since we have $n=2$, we must have $p(2) = v_3$ for whatever positive integer value $v_3$ takes on, which means that we have $p(2) = a + 2v_2-v_1 = v_3$, and so we have $a = v_3-2v_2+v_1$ (which is correct by Finite Difference theory), giving us $$p(x) = (v_3-2v_2+v_1){x\choose 2}+(v_2-v_1)x+v_1$$ This shows that only the (binomial-form) quadratic term will change in coefficient as $v_3$ changes.  Now consider this value: $$p\left(\frac 12\right)=\frac 14v_1+v_2-\frac 14v_3$$ We again have linear change in the value at this point as $v_3$ changes, and it is clear that it is possible for this value to be negative. If $p(x)$ is composite with at least two non-constant integer-valued polynomial factors, and if $v_3$ is large enough to force $p\left(\frac 12\right)$ to be negative, then these factors are based exactly on the roots between $p(0)$ and $p(1)$, since each of these values is positive.  So we would have $$p(x) = (v_3-2v_2+v_1)(x-q_0)(x-q_1)=a(x-q_0)\cdot b(x-q_1)$$ In order for these factors to be considered ""integer-valued"", both $a$ and $b$ must be large enough that either the denominator of the rational $aq_0\le 2$ or the denominator of the rational $bq_1\le 2$.  (For example, ${x\choose 2}={x(x-1)\over 2}$ and these factors are considered ""integer-valued"".)  But we also notice that $q_0$ must be between $0$ and the intersection $(Q_0,0)$ of the line $y=0$ with the line segment connecting $(0,p(0))$ with $\left(\frac 12,p\left(\frac 12\right)\right)$, and this distance $Q_0$ is inverse-linear with $v_3$ (by similar triangles).  But this means that $a$ must be asymptotically linear with $v_3$ and also (by the same argument for $Q_1$) that $b$ must be asymptotically linear with $v_3$, and therefore that $$p(x) = (v_3-2v_2+v_1)(x-q_0)(x-q_1)=cv_3^2(x-q_0)(x-q_1)\tag 1$$ But $v_3$ is allowed to take on any value, and equation $(1)$ implies that $c$ must vary inverse-linearly with $v_3$, therefore there exists $N\in\Bbb N$ such that for all $v_3\gt N$ at least one of $a(x-q_0),b(x-q_1)$ cannot be integer-valued. I'm fairly certain that this same argument applies in the same way for any given value of $n$.","['polynomials', 'number-theory', 'lagrange-interpolation', 'integer-valued-polynomials']"
1486949,Why is this function measurable?,"If $f$ is Borel measurable on $\mathbb{R}$, why is then $f(x-y)$ Bore-measurable on $\mathbb{R}^2$? I tried showing that the set $\{(x,y): x-y \in f^{-1}(a,\infty)\}$, is measurable, we know that $f^{-1}(a,\infty)$ is in $\mathcal{B}$, but I am not quite sure how to finish the argument. Any tips?","['real-analysis', 'measure-theory']"
1486975,UMVUE for $\theta^2$,"Let $X_1,...X_n$ be a random sample with distribution $\text{Normal}(\theta,1)$. Find the UMVUE for $\theta^2$ What IÂ´ve done so far: I have already shown that $T=\sum_{i=1}^nX_i$ is a complete sufficient statistic for $\theta$. Let $\widehat{\theta^2}=\bar {X}^2-\frac{1}{n}$ be an estimator for $\theta^2$ $$E[\widehat{\theta^2}]=E[\bar {X}^2-\frac{1}{n}]=E[\bar{X}^2]-E^2[\bar X]+E^2[\bar X]+\frac{1}{n}=Var(\bar X)+E^2[\bar X]+\frac{1}{n}$$ I know that $\bar X$ has distribution $\text{Normal}(\theta, \frac{1}{n})$ It follows that :$$E[\widehat{\theta^2}]=\theta^2$$
Hence $\widehat{\theta^2}=\bar {X}^2-\frac{1}{n}$ is unbiased estimator Know I have to compute $g(T)=E[\widehat{\theta^2}|T]$ and by Lehmann-ScheffÃ© this will be the UMVUE for $\theta^2$ but the problem is how can I compute $$E[\widehat{\theta^2}|T]=E[\bar {X}^2-\frac{1}{n}|\sum_{i=1}^nX_i]$$ I know I need to find the joint density of $\bar {X}^2-\frac{1}{n}$ and $\sum_{i=1}^nX_i$ but is there an easy way to do it? Or is there another way to find the UMVUE? I would really appreciate if you can help me with this problem","['conditional-expectation', 'probability', 'statistics', 'statistical-inference']"
1486976,why this function is even function,"Let $f(x)$ be a continuous differentiable odd function, and for any
  $x,y\in R$, such  $$f(x-y)=f(x)g(y)-g(x)f(y)$$ show that $g(x)$ is an even function. How can I go about solving this? I think there is a specific function
$$\sin{(x-y)}=\sin{x}\cos{y}-\cos{x}\sin{y}$$ Thanks","['functional-equations', 'functions']"
1486994,Harmonics functions take minimum value on the boundary,"Let $u$ be harmonic on the bounded region $A$ and continuous on the closure of $A$. Then $u$ takes its minimum only on $\partial A$ unless $u$ is a constant. How can we prove this result? Do we have to use the Maximum modulus principles? in my notes, the proof only says that considering $v = - u$, the conclusion follows. But, how? I dont see it.",['complex-analysis']
1487001,Importance of $2$-groups in Finite Simple Groups,"In a forward to a book on Groups of Prime Power Order , Z. Janko says Of special interest are $2$-groups. In fact, if $G$ is a non-abelian  finite simple group and if the structure of its Sylow-$2$ subgroup is know, then the structure of $G$ is almost uniquely determined. However, the book is written on groups of prime power order, it does not contain any illustrative example of this fact. Can one give example(s) of infinite family of finite simple groups, which can be almost (in Janko's sense) characterized from structure of Sylow-$2$ subgroups?","['group-theory', 'finite-groups']"
1487014,Why is there only one solution to this ODE?,"Suppose $f:\mathbb{R}^2\rightarrow \mathbb{R}$ is continuous such that $f(x,y)<0$ if $xy>0$ and $f(x,y)>0$ if $xy<0$. Show that the following initial value problem has an unique solution \begin{align}
y'=f(x,y),\quad y(0)=0
\end{align} One solution is obviously $y(x)\equiv 0$, since $f(x,0)=0$, $\forall x\in\mathbb{R}$ because of continuity. I have problems to argue that there can't be another solution. Usually one knows uniqueness if the RHS is locally Lipschitz continuous which is not the case here. How can one prove the uniqueness? Best wishes",['ordinary-differential-equations']
1487025,Find the Lebesgue Measure of the following sets,Find the Lebesgue Measure of the following sets : $A=\{0<x\leq 1:x\sin (\dfrac{1}{x})\geq 0\}$ $B=\{0<x\leq 1:\sin(\dfrac{1}{x})\geq 0\}$ In order to find Lebesgue Measure of a set we have to cover the set by open balls in $\mathbb R^2$ and take the infimum of the area of the balls covered. But how to cover the graph of the above sets by the open balls? Any help will be great.,"['lebesgue-measure', 'real-analysis', 'measure-theory']"
1487058,Lax-Milgram theorem on Evans. If the mapping is injective why do we need to prove uniqueness again?,"This is the theorem and its proof (From Evans L., Partial Differential Equations, p. $297-299$) Theorem 1 (Lax-Milgram Theorem). Assume that$$
B: H Ã H â \mathbb{R} \tag{i}
$$
  is a bilinear mapping, for which there exists constant $Î±, Î² > 0$ such that$$
|B[u, v]| \le Î± \| u \| \| v \|
$$
  and $$\beta\|u\|^2\leq B[u,u]\ \,(u\in H).\tag{ii}$$ Finally, let $f:H\to\mathbb{R}$ be a bounded linear functional on $H$. Then there exists a unique element $u\in H$ such that $$B[u,v]=\langle f,v\rangle\tag{1}$$ for all $v\in H$. Proof. For each fixed element $u\in H$, the mapping $v\mapsto B[u,v]$ is a bounded linear functional on $H$; whence the Riesz Representation Theorem (D.3) asserts the existence of a unique element $w\in H$ satisfying $$B[u,v]=(w,v)\ \, (v\in H).\tag{2}$$ Let us write $Au=w$ whenever $(2)$ holds; so that $$B[u,v]=(Au,v)\ \, (u,v\in H).\tag{3}$$ We first claim $A:H\to H$ is a bounded linear operator. Indeed if $\lambda_1,\lambda_2\in\mathbb{R}$ and $u_1,u_2\in H$, we see for each $v\in H$ that $$\begin{align}(A(\lambda_1u_1+\lambda_2u_2),v)&=B[\lambda_1u_1+\lambda_2u_2,v]\ \text{ by }(3)\\ &=\lambda_1B[u_1,v]+\lambda_2 B[u_2,v] \\ &=\lambda_1(Au_1,v)+\lambda_2(Au_2,v)\ \text{ by }(3)\text{ again} \\&=(\lambda_1 Au_1+\lambda_2 Au_2,v). \end{align}$$ This equality obtains for each $v\in H$, and so $A$ is linear. Furthermore $$\|Au\|^2=(Au,Au)=B[u,Au]\leq\alpha\|u\|\,\|Au\|.$$ Consequently $\|Au\|\leq\alpha\|u\|$ for all $u\in H$, and so $A$ is bounded. Next we assert $$\left\{\begin{array}{l}A\text{ is one-to-one, and}\\ R(A),\text{ the range of }A,\text{ is closed in }H. \end{array}\right.\tag{4}$$ To prove this, let us compare $$\beta\|u\|^2\leq B[u,u]=(Au,u)\leq\|Au\|\,\|u\|.$$ Hence $\beta\|u\|\leq\|Au\|$. This inequality easily implies $(4)$. We demonstrate now $$R(A)=H.\tag{5}$$ For if not, then since $R(A)$ is closed, there would exist a nonzero element $w\in H$ with $w\in R(A)^{\bot}$. But this fact in turn implies the contradiction $\beta\|w\|^2\leq B[w,w]=(Aw,w)=0$. Next, we observe once more from the Riesz Representation Theorem that $$\langle f,v\rangle =(w,v)\quad\text{for all }v\in H$$ for some element $w\in H$. We then utilize $(4)$ and $(5)$ to find $u\in H$ satisfying $Au=w$. Then $$B[u,v]=(Au,v)=(w,v)=\langle f,v\rangle\quad(v\in H),$$ and this is $(1)$. Finally, we show there is at most one element $u\in H$ verifying $(1)$. For if both $B[u,v]=\langle f,v\rangle$ and $B[\tilde{u},v]=\langle f,v\rangle$, then $B[u-\tilde{u},v]=0\ (v\in H)$. We set $v=u-\tilde{u}$ to find $\beta\|u-\tilde{u}\|^2\leq B[u-\tilde{u},u-\tilde{u}]=0.$ $\tag*{$\square$}$ If we already know that $\langle f,v \rangle = (w,v)$ = $B[u,v] = (Au, v)$ and we know that $A$ is one-to-one, isn't that already a proof that there can be only one $u$ for which 
$$ B(u,v) = (w,v) \tag{2}$$
holds? Why do we need point 6 in the proof of the above theorem? Isn't that already explained?","['functional-analysis', 'partial-differential-equations']"
1487063,$L(\bigoplus_{i\in I} H_i)\cong \prod_{i\in I}L(H_i)$ as Banach spaces?,"Let $\{H_i:i\in I\}$ be a system of complex Hilbert spaces, $L(\bigoplus_{i\in I} H_i)$ the set of bounded linear maps $T:\bigoplus_{i\in I} H_i\to \bigoplus_{i\in I} H_i$, equipped with the operator norm. 
Let $\prod_{i\in I}L(H_i)$ the direct product of linear maps endowed with the norm $\|(T_i)_{i\in I}\|=sup_{i\in I}\|T_i\|$. Is $$L(\bigoplus_{i\in I} H_i)\cong \prod_{i\in I}L(H_i)$$ as banach spaces?
The map which comes to my mind constructed as follows: Is $$\iota_i: H_i\to \bigoplus_{i\in I} H_i$$ the canonical injection, we define for each $i\in I$ a map $$\varphi_i:L(\bigoplus_{i\in I} H_i)\to L(H_i),\; \alpha\mapsto \alpha\circ \iota_i$$ Then extend it to a map $$L(\bigoplus_{i\in I} H_i)\to \prod_{i\in I}L(H_i).$$ But I'm not sure if it's possible here or if the construction makes sense in this context.","['universal-property', 'banach-spaces', 'functional-analysis']"
1487068,If $G$ is a group of isometries of $X$ then prove that $X/G$ and $X/\bar{G}$ are homotopically equvalent,"Let $X$ be a connected, locally path connected, locally compact metric space. Let $G$ be a group of isometries of $X$ (that is a group of homeomorphisms of $X$ with itself that preserves distance). Let $\bar{G}$ be the closure of $G$ in Homeo$(X)$ endowed with the compact open topology. Let $(P)$ be the property that for all $x,x' \in X$ and a neighbourhood $V$ of $x$ in $X$, if $x' \in \bar{G}x$ then $x' \in GV$ I want to prove that $X/G$ and $X/\bar{G}$ are homotopically
  equivalent. I have shown that if $X$ and $G$ satisfy $(P)$ then they are homotopically equivalent (I have in fact shown this if $G$ is just a group of homeomorphisms of $X$ with itself). So to prove this result I only need to show that property $(P)$ is true when $G$ is a group of isometries of $X$. But I don't know how to do so. Is there a way to prove it without using $(P)$? Thank you.","['metric-spaces', 'general-topology', 'group-actions', 'algebraic-topology', 'homotopy-theory']"
1487075,The page numbers of an open book totalled $871$. At what pages was the book open?,"So far I have $x + (x-1) = 871$. This has led to one page being $435$ and the other, $436$. Is there anything needing alteration, or is this equation workable?","['word-problem', 'algebra-precalculus']"
1487085,If $x^2+4y^2=4.$ Then find range of $ x^2+y^2-xy$,"If $x^2+4y^2=4.$ Then find range of $ x^2+y^2-xy$ $\bf{My\; Try::}$ Given $$x^2+4y^2 = 4\Rightarrow \frac{x^2}{4}+\frac{y^2}{1} = 1$$, So parametric Coordinate for Ellipse are $x = 2\cos \phi$ and $y = \sin \phi$. Now Let $$f\left(x,y\right) = x^2+y^2-xy = 4-4y^2+y^2-xy = 4-3y^2-xy = 4 - 3\sin^2 \phi - 2\sin \phi \cdot \cos \phi$$ So $$f\left(\phi \right) = 4-\frac{3}{2}\left(1-\cos 2\phi \right) - \sin 2\phi = \frac{5}{2} + \frac{1}{2}\left(3\cos 2 \phi -2 \sin 2\phi \right)$$ Now Range of $$-\sqrt{13}\leq \left(3\cos 2 \phi - 2\sin 2\phi \right)\leq \sqrt{13}$$ So  $$\frac{1}{2}\cdot \left(5-\sqrt{13} \right) \leq f\left(\phi \right)\leq \frac{1}{2}\cdot \left(5+\sqrt{13} \right)$$ My question is can we solve it using Inequality or any other method, If yes Then plz explain here Thanks",['algebra-precalculus']
1487103,Probability of having at least one pair by drawing 4 shoes from 12 pairs.,"There are $12$ pairs of shoes in a cupboard. $4$ are drawn at random. What is the probability that there is at least one pair? My first attempt: If we chose a pair at first and then draw any two at random from the rest, then there will be at least one pair. 
We can choose one pair in ${12}\choose 4$ ways and chose any $2$ from the rest in ${22}\choose 2$ ways.
Therefore, the required probability= $\frac{{12\choose 4} \times {22\choose 2}}{{24\choose 4}}$ = $\frac{6}{23} =\frac{42}{161}$ But the given answer is $\frac{41}{161}$. Another attempt: Each of the 4 shoes we choose, will come from one of the pairs. We can choose the four pairs in ${12\choose 4}$ ways and can select a shoe from each of the pairs in $2$ ways so that no pair is obtained. Therefore, required probability =$1-$ $\frac{{12\choose 4} \times 2^4}{{24\choose 4}}$ = $\frac{41}{161}$ What is wrong with the first attempt?","['probability', 'combinatorics', 'permutations']"
1487116,Closure of set with certain attributes,"I am having a hard time solving the following problem: Suppose we have a nonempty set $E$ $\subseteq$ $(0,\infty)$ with the following attributes: 1) If $x\in E$ then $x/2\in E$ 2) If $x , y \in E$ then $\sqrt {x^2+y^2} \in E$. We have to prove that $\operatorname{cl}(E)=[0,\infty)$.",['general-topology']
1487127,exponential commutes integral,"Let $B(t)$ be a $n\times n$ function matrix with respect to $t$, consider $B(t)$ is differentiable , the following conclusion is obvious 
$$\text{if}\ \ B'B=BB',\ \ \text{then} \ \ B'e^B=e^BB'$$
So my question is if the converse direction correct? i.e. can be found some $B$ such that
$$B'B\neq BB'\ \ \text{but}\ \ B'e^B=e^BB'.$$. In deed, I found a complex matrix satisfy this condition
$$B=\begin{pmatrix} \frac{2i\pi}{t-1} & \frac{2i\pi}{t-1}\\
0 & \frac{2i\pi t}{t-1}\end{pmatrix}$$ However, I am wondering if there exist real matrix (I mean the coefficients are real ) example??? Updates Above is in differential form, now I consider the integral form:
Can we find some $B$ such that:
$$B\int_{t_0}^tB(s)ds\neq \int_{t_0}^tB(s)dsB\;\;but\;\;Be^{\int_{t_0}^tB(s)ds}=e^{\int_{t_0}^tB(s)ds}B$$
@John Ma, I tried you example, but failed to the integral case.","['ordinary-differential-equations', 'matrices']"
1487134,Using Lagrange multipliers to find the max and min,"I have the equation $$f(x,y)=x^2+y^2$$ and the constraint $$(x-1)^2+4y^2=4$$ So I must find the min and max. My try: So I get the equations:
$$2x=Î»(2x-2)$$
$$2y=Î»8y$$ and I get Î»=1/4 and x=1/3 Any help is appreciated","['optimization', 'lagrange-multiplier', 'multivariable-calculus']"
1487149,Prove the intersection of regular languages is regular.,"A, B are regular languages. Complement of a regular language is regular Union is regular Prove the intersection is regular. Using these definitions the proof in my book is: $\overline A$ is regular by 1 $\overline B$ is regular by 1 $\overline A \cup \overline B$ is regular by 2 $\overline{A\cup B}$ is regular by 1 $ A \cap B$ is regular by De Morgan's Law Would someone mind explaining steps 4 & 5 or more specifically how you arrive at 4 from 3 and 5 from 4? Thanks.","['regular-language', 'proof-verification', 'discrete-mathematics']"
1487159,"Some questions about the normal vector and Jacobian factor in surface integrals,","I have some short questions on some lingering confusing concepts, specific to surface integrals: a) Is the surface integral in the Divergence and Stokes's Theorem the same thing? Both require a unit-normal vector and a surface area component, $\vec n$dS, one is clearly a ""flux integral"", but is the one in Stoke's Theorem also a flux integral? b) In both theorems, the surface integrals require unit-normal vectors in the integrand.  But I remember working through some surface integrals where one shouldn't normalize the normal vector obtained -- I don't remember the example concretely now.  Why does this happen?  Why keep the magnitude of the normal vector... sometimes ?  I also have used a formula in the past, but not often: $\vec n$ = $<-f_x,-f_y,1$> - why doesn't this formula show up more often in surface integral problems?  Is this formula for a normal vector $\vec n$ = $<-f_x,-f_y,1$>...normalized / required to be normalized? c) And finally, why do some surface integrals have this factor in its $ds$ surface area component, $\large||\frac{d\vec v}{d\theta}\times\frac{d\vec v} {d\phi}||$,say, for integrating over a surface that is a sphere.  Does this factor just play the role of a ""Jacobian"", but for 3-D surfaces?  And then perhaps in flat surfaces in 3-D space, e.g., a plane disk in space, such a ""Jacobian"" factor is computed differently?  We know the Jacobian to be the determinant of the matrix of partial derivatives, e.g., yielding the variable ""r"" in polar and spherical coordinates integration.  But $\large||\frac{d\vec v}{d\theta}\times\frac{d\vec v} {d\phi}||$ is the magnitude of a determinant (cross-product of partial derivative vectors), though. Thanks so much in advance,","['surface-integrals', 'real-analysis', 'integration', 'stokes-theorem', 'multivariable-calculus']"
1487172,"If f is integrable, is it finite almost everywhere?","If $\int_\Omega f d\mu<\infty$, and $f$ is non-negative, can we conclude that $f$ is finite a.e. on $\Omega$? Is being finite a.e. the same as having a finite essential supremum, i.e. there exists an $M$ such that $f\leq M$ a.e.? I know that $f$ integrable does not mean that $f$ has a finite essential supremum, from the counter example $f=1/\sqrt x$ on $(0,1]$. Thanks for help.","['real-analysis', 'measure-theory']"
1487244,Meaning of ;-symbol in math,"Learning about the formal definition of the derivativ, I am asked to find the derivative at the indicated point from the graph of the function. However in the following problems, I do not understand what the "";""-symbol mean f(x)=5;x=1 f(x)=4x-3;x=-1 Of course the meaning of ; is something I should remember, but any help would be much appreciated as google search and lists of mathematical symbols didn't help!","['notation', 'functions']"
1487293,$\otimes$-Categorical Generalization of Lagrange,"I am reading M. Brandenburg's paper and came across the following result which is a generalization of Lagrange's theory in group theory: Let $\mathcal C$ be a $\otimes$-category and $A\to B$ a morphism of algebras (in $\mathcal C$). Then if $M$ is an $A$-module isomorphic to $T\otimes B$ as a $B$-module and $B_{|A}\cong S\otimes A$ as an $A$-module, then $M_{|A}\cong (T\otimes S)\otimes A$. The proof is a a simple one-liner. My question is how do we obtain Lagrange's theorem from it? At first, I thought we take $\mathcal C$ to be the cartesian closed category $\mathbf{Set}$ and let $A\to B$ be the inclusion of a subgroup in a finite group.  But we are assuming $B_{|A}\cong S\times A$ and so there would be nothing to show since that would imply the order of $A$ divided the order of $B$. I know this should be easy, any help?","['group-theory', 'category-theory']"
1487297,What exactly does the Hilbert scheme of points parametrize?,"The Hilbert scheme of points is defined as 
$$ \text{Hilb}^n(X) = \{ I \subset \mathbb{C}[x,y] \text{ such that } \text{dim}_{\mathbb{C}}/I = n  \} $$
or, in words, the Hilbert scheme of points consists of all ideals $I$ of polynomials on $x,y$ with complex coefficients $ \mathbb{C}[x,y]$ such that when we mod out the latter with the first, $ \mathbb{C}[x,y]/I$ the resulting space is $n$ dimensional. I struggle to understand this definition. Why does this definition require only polynomials in two variables and not, say, three? Is it possible to give me an example? I.e., can you give me some polynomial $\in  \mathbb{C}[x,y]$ and ideal of it and show me how the quotient space is $n$ dimensional? Also, what does the Hilbert-Chow morphism 
$$ \text{Hilb}^n(X) \to Sym^n(X) $$
actually mean? I struggle to undestand what for example would mean 
$$ \text{Hilb}^n{\mathbb{C}}^2 \to Sym^2\mathbb{C}^2 = \mathbb{C}^2/S_2 $$ Any help would be highly appreciated.","['mathematical-physics', 'algebraic-geometry', 'schemes', 'complex-analysis']"
1487307,Intuition of Gronwall lemma,"The Gronwall lemma is a well known and very useful statement which is used in many situations, in particular in the theory of differential equations. I have seen it so many times and even the proof is very easy to understand. But at the end of the day it is seems to me a very technical 'thing', I never have been able to develop some intuition for it. Neither the statement seems very intuitive nor the proof of it, so that it becomes impossible for me to remember it after some time. Can anyone 'explain' (whatever this means) to me the intuition behind the statement or does anyone has some picture in mind which helps to grasp the idea of Gronwall's lemma? I have in mind the integral form of this lemma.","['gronwall-type-inequality', 'real-analysis', 'ordinary-differential-equations']"
1487338,Finding right inverse matrix,"Given a $3\times 4$ matrix $A$ such as 
$$
        \begin{pmatrix}
        1 & 1 & 1 & 1 \\
        0 & 1 & 1 & 0 \\
        0 & 0 & 1 & 1 \\
        \end{pmatrix}
,$$ 
find a matrix $B_{4\times 3}$  such that $$AB =
        \begin{pmatrix}
        1 & 0 & 0  \\
        0 & 1 & 0  \\
        0 & 0 & 1  \\
        \end{pmatrix}
$$
Apart from simply multiplying $A$ with $B$ and generating a $12$ variable system of equations, is there any simpler way of finding $B$ ?","['inverse', 'linear-algebra', 'matrices']"
1487397,Prove that $\pi$ is in $\mathbb{R}$,How can I prove that the number $\pi$ exist or that it's real? I know how to prove the existence of $\sqrt{2}$ and I know how to prove that $\pi$ cannot be a rational number but how do we prove that it's even exist?,"['number-theory', 'real-analysis', 'discrete-mathematics']"
1487398,Why the $\partial_t(\Gamma_{ip}^k \Gamma_{jl}^p -\Gamma_{jp}^k\Gamma_{il}^p)$ vanish?,"In the red line part of below picture  ,why the $\partial_t(\Gamma_{ip}^k  \Gamma_{jl}^p  -\Gamma_{jp}^k\Gamma_{il}^p)$ vanish ?I know $\Gamma$ will vanish under normal coordinate. But if so, the RHS of red line will equal to $0$. And there is similar question in $\frac{\partial}{\partial t}\Gamma_{jl}^h$ of the third  line above red line . Thanks for any useful hint or answer. And the below picture is from 183th page of the paper","['differential-geometry', 'riemannian-geometry', 'ricci-flow']"
1487403,Fibonacci and Matrices [duplicate],"This question already has answers here : Proof of this result related to Fibonacci numbers: $\begin{pmatrix}1&1\\1&0\end{pmatrix}^n=\begin{pmatrix}F_{n+1}&F_n\\F_n&F_{n-1}\end{pmatrix}$? (4 answers) Closed 8 years ago . Consider Matrix $$ A = \begin{pmatrix} 1 & 1\\ 1 & 0 \end{pmatrix} $$ Investigate the sequence of powers of $A$
(i.e. $A^n$ for $n = 1, 2, 3, 4,\ldots$. Verify that $$A^n = \begin{pmatrix}F_{n+1} &F_n \\ F_n & F_{nâ1}\end{pmatrix}$$ for $n \geq 20$, where $F_n$ is the $n^{th}$ Fibonacci number. I don't get it, please help. Thank you!","['fibonacci-numbers', 'linear-algebra', 'matrices']"
1487411,"Class field theory for $p$-groups. (IV.6, exercise 3 from Neukirch's ANT.)","I will use notation as in a preious question of mine. 
This question is from Neukirch's book ""Algebraic number theory,"" page 305, exercise 3. Notation for the problem Let $G$ be a profinite $p$-group (so all quotients by open subgroups are $p$-groups). We will denote the closed subgroups of $G$ by $G_K$ and call the indices $K$ for fields. We say that $K$ is the fixed field of $G_K.$ We write $K \subset L$ if $G_L \subset G_K$ and we call $K \subset L$ a field extension. We say that it is a finite extension if $G_L$ is of finite index in $G_K$ and we say that it is Galois if $G_L$ is normal in $G_K$ and then we write $G(L|K) = G_K / G_L.$ We call this the Galois group of $K \subset L.$ Further, let us say that we have a continuous multiplicative $G$-module $A.$ We mean with this a multiplicative abelian group $A$ on which $\sigma \in G$ act as automorphisms on the right and they act continuously. We let $A_K = \{a \in A | a^\sigma = a, \forall \sigma\in G_K\}.$Clearly we then have that $A_K \subset A_L$ for any field extension $K \subset L.$ If $$K \subset L$$ is finite, then we have a norm map
$$N_{L|K}:A_L \rightarrow A_K,$$ $$N_{L|K}(a) = \Pi_\sigma a^\sigma$$ where $\sigma$ varies over a system of representatives of $G_K / G_L.$ Now, suppose further that we have a surjective map $$d:G \rightarrow \mathbb{Z}_p$$ where $p$ is a prime number. We also suppose we have a map $$v:A_k \rightarrow \mathbb{Z}_p$$ which we call a henselian $p$-valuation with respect to $d$ and assume that $v$ satisfies the following properties: $$v(A_k) = Z \supset \mathbb{Z}$$ and $$Z/nZ \cong \mathbb{Z}/n\mathbb{Z}$$
for all $n$ that are a power of $p.$ $$v(N_{K|k}(A_K))=f_k Z$$ for all finite extensions $k \subset K$ where
$$f_K = (d(G):d(G_K)).$$ Suppose that $A$ satisfies the class field axiom, which means that for every cyclic extension $L|K$ $$ |H^i(G(L|K),A_L)| = \begin{cases} [L:K] & \text{for } i= 0 \\ 1 & \text{for } i=-1. \end{cases}$$
Here $$H^0(G(L|K),A_L) = A_K/N_{L|K}(A_L)$$ while $$H^{-1}(G(L|K),A_L) = \text{ ker } N_{L|K} / I_{G(L|K)}$$
where $\text{ker } N_{L|K}$ is the kernel of $N_{L|K} : A_L \rightarrow A_K$ and $I_{G(L|K)}$ is the ideal generated by elements of the form $a^{\sigma-1} = a^\sigma \cdot a^{-1}$ for $a \in A_L$ and $g \in G(L|K).$ The problem statement With notation now set up, the question is as follows. Suppose that we have a p-class field thoery $(d:G \rightarrow \mathbb{Z}_p,v:A_k \rightarrow \mathbb{Z}_p)$ as above. Suppose we have another surjective homomorphism $d':G \rightarrow \mathbb{Z}_p.$ Let $\text{ker } d' = G_{\hat{K'}}.$ We then have that $\hat{K'} |K$ is an extension with galois group isomorphic to $\mathbb{Z}_p.$ Let now $$v':A_k \rightarrow \mathbb{Z}_p$$ be the composite of $$A_k \xrightarrow{(,\hat{K'}|K)} G(\hat{K'} |K) \xrightarrow{d'} \mathbb{Z}_p$$ where $$(,\hat{K'}|K)$$ is the norm residue symbol derived from the $p$-class field theory. Neukirch now claims that: $(d',v')$ is also a p-class field theory. I thought I had a proof of this, but I a now very unsure, since he states that the theorem does not hold for $\hat{\mathbb{Z}}$ class field theories, i.e if we replace $\mathbb{Z}_p$ with $\hat{\mathbb{Z}}$ above and $G$ is any profinite group, not neccesarily a $p$-group, the analogous statement does not hold. So my question is two-fold: How can I prove that $(d',v')$ is a p-class field theory? What goes wrong for $\hat{\mathbb{Z}}?$ What makes the case so different compared to $\mathbb{Z}_p?$","['class-field-theory', 'number-theory', 'local-field', 'algebraic-number-theory']"
1487428,"Let $k$ be a natural number . Then $3k+1$ , $4k+1$ and $6k+1$ cannot all be square numbers.","Let $k$ be a natural number. Then $3k+1$ , $4k+1$ and $6k+1$ cannot all be square numbers. I tried to prove this by supposing one of them is a square number and by substituting the corresponding $k$ value. But I failed to prove it. If we ignore one term, we can make the remaining terms squares.  For example, $3k+1$ and $4k+1$ are both squares if $k=56$ (they are $13^2$ and $15^2$ ); $3k+1$ and $6k+1$ are both squares if $k=8$ (they are $5^2$ and $7^2$ ); $4k+1$ and $6k+1$ are both squares if $k=20$ (they are $9^2$ and $11^2$ ).","['elliptic-curves', 'square-numbers', 'contest-math', 'number-theory', 'diophantine-equations']"
1487447,Do (the restrictions of) continuous maps (to the rationals) form a Borel set?,"Consider, within the Polish space $\mathbb{R}^\mathbb{Q}$ (with product topology), the subset of all those maps that can be extended to a continuous map on all of $\mathbb{R}$. It's easy to see that this set is co-analytic. But is it sharply so, or is it actually a Borel set?","['descriptive-set-theory', 'measure-theory', 'real-analysis', 'functional-analysis', 'general-topology']"
1487507,Find all values of N that satisfy Ï(N) = x for any given x value,"How would I determine all values of n in Ï(n) = x for any value of x where Ï is Euler's totient function? (and -1 if x is not a totient). Is there a simple formula for this? Or is this a lot more complicated than I think this is? Simply trying every number from x + 1 to the upper limit is too inefficient. I will be using numbers up to 1,000,000,000,000, so efficiency is key here.","['discrete-mathematics', 'totient-function', 'elementary-number-theory']"
1487551,Circularity in formal proof of De Morgan's laws?,"I was reading about De Morgan's laws on wikipedia: https://en.wikipedia.org/wiki/De_Morgan%27s_laws . When I looked at the formal proof for the law, it seems like a circular argument is used, can someone explain to me what I'm not getting. It's this bit: ""Let $x \in (A \cap B)^c$. Then, $x \not\in A \cap B$. Because $A \cap B = \{y | y \in A \text{ and } y \in B\}$, it must be the case that $x \not\in A$ or $x \not\in B$."" I can grasp why that must be the case. But is this deduction not done using De Morgan's laws? Ie De Morgan's laws state $\neg(P\land Q)\iff(\neg P)\lor(\neg Q)$ If $P$ stands for the statement ""$x \in A$"" and Q stands for ""$x \in B$"". Isn't the structure of the argument in the formal proof of De Morgan's Law the same as De Morgan's law itself?","['elementary-set-theory', 'logic']"
1487575,Calculate the limit of a function of two variables,"How to calculate this limit?
$$\lim_{(x,y) \rightarrow (0,0)}\frac{x^2+y^2}{\sqrt{x^2+y^2+9}-3}$$","['multivariable-calculus', 'limits']"
1487594,Affine-regular hexagon in convex body,"An affine-regular $n$-gon is a non-degenerate affine image of the regular $n$-gon. It seems to be a standard fact in combinatorial geometry that inside every convex compact set $K\subseteq \mathbb R^2$ with nonempty interior it is possible to inscribe at least one affine-regular hexagon so that its vertices belong to the boundary of $K$. This is stated, for example, here . How does one prove this? I am not able to find an accessible reference that contains the proof. Is there ""typically"" a unique inscribed hexagon?","['affine-geometry', 'combinatorial-geometry', 'geometry', 'convex-analysis', 'computational-geometry']"
1487596,A serie about $\sum_{n = 1}^\infty {\arctan \frac{{10n}}{{\left( {3{n^2} + 2} \right)\left( {9{n^2} - 1} \right)}}}$,How to prove $$\sum\limits_{n = 1}^\infty  {\arctan \frac{{10n}}{{\left( {3{n^2} + 2} \right)\left( {9{n^2} - 1} \right)}}}  = \ln 3 - \frac{\pi }{4}.$$ Add: Maybe we can follow this !,"['sequences-and-series', 'calculus', 'real-analysis']"
1487601,Taking the divergence of a field with a singularity when $\vec{r}=0$ produces a Dirac's delta.,"I'm currently taking a classical electrodynamics course. I have a mathematical background and I know that the classical theorems of integral calculus (Stokes, Gauss, ...) are just particular versions of Stokes' theorem for manifolds and that the curl, divergence, ... operators are particular forms of the exterior derivative. But in this course of physics that I'm taking most of the student don't know a lot of mathematics and because of this, the teacher solves most of the problems without justifying that he can apply the desired results from calculus. For instance, in one exercise I have to prove that a magnetic monopole does not satisfy Maxwell's equations. To do this we'll prove that $\nabla \vec{B} \neq 0$ . The field created by a magnetic monopole is: $$\vec{B}_m = \frac{\mu_0 q_m}{4 \pi} \frac{\vec{r}}{\lvert\vec{r}\rvert^3} $$ And the solution given by my teacher is that the divergence of this field is: $$\nabla \vec{B_m} = \mu_0 q_m \delta^{(3)}(\vec{r})$$ Where the superscript is to show that it is a 3-dimensional delta. I don't know how to justify this rigorously because the field has a singularity at $\vec{r}=0$ and the $\delta$ is not a function but a distribution. Is there any way to generalize Stokes theorem to be able to show this? What is the formal way to prove this statement? I would really appreciate any mathematical reference where I could find how to deal with this kind of singularities.","['singularity-theory', 'differential-geometry', 'physics', 'distribution-theory']"
1487613,Ricci tensor from Riemann tensor,"I have been studying differential geometry and general relativity, and I have a question about the Ricci tensor. So as I understand, to set things up: one defines the Riemann curvature operator as $R: (U,V,W) \rightarrow R(U,V)W$ mapping three vector fields to a fourth one: $$ R(U,V)W = \nabla_{U}\nabla_{V}W - \nabla_{V}\nabla_{U}W - \nabla_{[U,V]}W $$ with $[U,V]$ being the commutator. This has a nice geometrical interpretation as a measure of the noncommutativity of successive parallel transport along directions $U$ and $V$, versus $V$ and $U$. Then, in some coordinate system one computes: $$R(\partial_{\alpha},\partial_{\beta})\partial_{\gamma} = \nabla_{\partial_{\alpha}}\nabla_{\partial_{\beta}}\partial_{\gamma} - \nabla_{\partial_{\beta}}\nabla_{\partial_{\alpha}}\partial_{\gamma} $$ where the commutator term disappears; this has a nice expression in terms of the Christoffel symbols, and then $ R^{\delta}_{\gamma \alpha \beta} = dx^{\delta} \left(R(\partial_{\alpha},\partial_{\beta})\partial_{\gamma}\right) $ is the Riemann tensor in component notation. Everything is wonderful and beautiful thus far. And so now, enter the Ricci tensor. I know it is defined as the contraction of the Riemann tensor: $$ R_{\mu \nu} = R^{\delta}_{\mu \delta \nu} $$ My questions are the following: 1) What is the geometrical motivation behind this particular contraction of the Riemann tensor? (we are contracting in the index corresponding to the first ""test direction"" used in the Riemann curvature operator, is there a purpose for this choice?) 2) What is the geometrical interpretation of the Ricci tensor? Is there a coordinate-free way of defining it as a curvature operator, like the Riemann tensor? 3) Why contract the second covariant index and not another one? Is there a reason for this, or is it just a convention? Thanks in advance!","['general-relativity', 'differential-geometry']"
