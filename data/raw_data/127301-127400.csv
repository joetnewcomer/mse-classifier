question_id,title,body,tags
1943786,Proving $\lim \frac{x^n-a^n}{x-a}=n\cdot a^{n-1}$,"Please note that this question was asked by one of my students who doesn't know differentiation yet nor Lhopital nor mean value theorems. We teach limits before all these topics like differentiation , MVT , Lhopital , etc $$\lim_{ x \to a} \frac{x^n-a^n}{x-a}=n\cdot a^{n-1}$$ I can prove this result for $n \in \mathbb Z$ And for $n \in \mathbb Q $ , that is when $n =\frac{p}{q}$ , I can prove the result using the result for $n \in\mathbb Z$ . But my question is this : Since $\mathbb Z \subset \mathbb Q$ , why can't we prove this result only for $n \in \mathbb Q$ ? Is there a method to prove $$\lim_{ x \to a} \frac{x^\frac{p}{q}-a^\frac{p}{q}}{x-a}=\frac{p}{q}\cdot a^{\frac{p}{q}-1}$$ without the result for $n \in \mathbb Z $ ?","['real-analysis', 'limits-without-lhopital', 'calculus', 'limits']"
1943848,Approximation theory in Banach Spaces.,"This semester I am taking a class on approximation theory (centred primarily on the best approximation of an element in a space from an element in a subspace) and so far most of our work has been done in the realm of inner product spaces and Hilbert spaces. Naturally things follow fairly fluidly within the framework of an inner product space, but what about when approximation theory is practiced in a Banach space? What results in the approximation of functions are much harder, perhaps impossible, to obtain in a Banach Space? And are there any open problems in the area? (After class my lecturer told me that there are some things we can do when approximating functions in a Hilbert space that we can't do in a Banach space; also that there are many things that have not yet been shown in a Banach space that hold in a Hilbert space with regards to the approximation of functions.)","['banach-spaces', 'hilbert-spaces', 'functional-analysis', 'approximation-theory', 'approximation']"
1943975,Prove the following series $\sum\limits_{s=0}^\infty \frac{1}{(sn)!}$ [duplicate],"This question already has answers here : Sum of $\sum \limits_{n=0}^{\infty} \frac{1}{(kn)!}$ (3 answers) Closed 7 years ago . Prove that, $$\sum\limits_{s=0}^\infty \frac{1}{(sn)!}=\frac{1}{n}\sum\limits_{r=0}^{n-1}\exp\left(\cos\frac{2r\pi}{n}\right)\cos\left(\sin\frac{2r\pi}{n}\right)$$ 
I don't have a real idea on how to start approaching this question, some hints and suggestions would be helpful.","['trigonometry', 'sequences-and-series', 'exponential-sum']"
1944008,Formula For N-Dimensional Quaternions,"Is there an extensible formula for N-dimensional quaternions? What would the formula look like for example for 2, 3, 4 and 5 functional dimensions of rotation (where 3 would be the normal quaternion?) Examples of associated rotations and quaternion group cayley tables extending beyond the normal quaternion would also be very helpful.","['rotations', 'algebraic-geometry', 'quaternions', 'geometry', 'dimensional-analysis']"
1944010,What's the Disjunctive normal form of a tautology,What's the Disjunctive Normal form or a tautology with two variables P & Q is it P̅Q̅ + P̅Q + PQ̅  + PQ or is that completely wrong? Thanks :),"['propositional-calculus', 'computer-science', 'logic', 'discrete-mathematics']"
1944026,"$ab(a+b) + bc(b+c) + ac(a+c) \geq \frac{2}{3}(a^{2}+b^{2}+c^{2})+ 4abc$ for $\frac1a+\frac1b+\frac1c=3$ and $a,b,c>0$","Let $a$ , $b$ , and $c$ be positive real numbers with $\displaystyle \frac{1}{a}+\frac{1}{b}+\frac{1}{c}=3$ . Prove that: $$
  ab(a+b) + bc(b+c) + ac(a+c) \geq \frac{2}{3}(a^{2}+b^{2}+c^{2})+ 4abc.
$$ Let us consider the following proofs. $$
  a^{2}+b^{2}+c^{2} \geq ab+bc+ca
$$ By the Arithmetic Mean-Geometric Mean Inequality we have $$
a^{2}+b^{2} \geq 2ab,\ \  b^{2}+c^{2} \geq 2bc,\ \ c^{2}+a^{2} \geq 2ca \tag{1}
$$ If we add together all the inequalities $(1)$ , we obtain $$
  2a^{2}+2b^{2}+2c^{2} \geq 2ab+2bc+2ca
$$ By dividing both side by $2$ , the result follows. Now let us consider, $$
  ab(a+b) + bc(b+c) + ac(a+c) \geq 6abc \tag{2}
$$ I already have proved $(2)$ Then, 
We are given, $$
\frac{1}{a}+\frac{1}{b}+\frac{1}{c}=3 \implies bc+ac+ab=3abc \tag{3}
$$ Notice that we have $$
a^{2}+b^{2}+c^{2} \geq bc+ac+ab=3abc 
$$ So, $$
a^{2}+b^{2}+c^{2} \geq 3abc \tag{4}
$$ Let us multiply both side of $(4)$ by $\displaystyle\frac{2}{3}$ , yield $$
\frac{2}{3}(a^{2}+b^{2}+c^{2}) \geq 2abc 
$$ Here where I stopped. Would someone help me out ! Thank you so much","['inequality', 'polynomials', 'cauchy-schwarz-inequality', 'algebra-precalculus', 'sum-of-squares-method']"
1944063,Getting a matrix lexicographically sorted along both directions,"Let $A$ be some $2n\times 2n$ matrix such that each row and each column contains exactly $n$ times $0$ and $n$ times $1$. If I sort the rows of $A$ lexicographically (in the sense of the matrix treated as a list of words, its rows, not in the sense of rearranging each single row) and then its colums, will the rows stay sorted? If not, will the process of alternatingly sorting rows and colums eventually stabilize, giving some reordering of $A$ which is sorted along both directions? [EDIT] Jorge Fernández Hidalgo 's very nice answer works for arbitrary binary matrices and as well for matrices with values in $\{0,\dots,k-1\}$ by replacing $2$ with $k$ in the proof.  Since every $n\times n$ matrix can be replaced by an order equivalent matrix with values in $\{0,\dots,n^2-1\}$, the proof also works for arbitrary real valued matrices. However, I asked myself if it makes a difference whether one starts the process by sorting the rows or sorting the columns.  In more general terms, if $P_R,P_C,P_R',P_C'$ are permutation matrices such that $P_R AP_C$ and $P_R' AP_C'$ are both sorted by rows and columns, do we necessarily have $P_R AP_C=P_R' AP_C'$? The answer is (if my code is not wrong) that it makes a difference how we start, even when looking only at matrices of the type I described in the beginning (same number of $0$s and $1$s in each row and column).  The smallest counterexample I could find (of this type) has dimensions $6\times 6$. Here is some Matlab code, if anyone wants to fool around: function A=sortRowFirst(A)
  B=sortrows(sortrows(A)')';
  while(!isequal(A,B))
    A=B;
    B=sortrows(sortrows(A)')';
  endwhile
endfunction

function A=sortColFirst(A)
  B=sortrows(sortrows(A')');
  while(!isequal(A,B))
    A=B;
    B=sortrows(sortrows(A')');
  endwhile
endfunction

function A=randNice(n)
  A=[];
  for i=0:n-1
    a=[ones(1,i),-ones(1,i)];
    a=a(randperm(i*2));
    A=[A;a;-a];
    A=A';
    a=[a(randperm(i*2)),[-1,1](randperm(2))];
    A=[A;a;-a];
  endfor
endfunction","['matrices', 'sorting', 'linear-algebra', 'discrete-mathematics']"
1944098,Why can't both of these functions be solutions to this differential equation?,"I am studying for an ODE exam, and working on this problem. Let $\displaystyle x_1 = \begin{bmatrix} 1 \\ t \\ \end{bmatrix}$ and $\displaystyle x_2 = \begin{bmatrix} t \\ 2t^2 \\ \end{bmatrix}$, and call by (H) the homogeneous differential equation $$x' = A(t) x$$ where $A$ is a continuous $2 \times 2$ matrix-valued function of $t$. I want to show that $x_1$ and $x_2$ cannot both be solutions to (H). Here is how I have gone about it: I know that $x_1$ and $x_2$ are linearly independent, so if they are both solutions to (H) then $W(t) \neq 0$ on the domain for $t$. And I've calculated the $W(t) = t^2$, where $W$ is the Wronskian of $\displaystyle \begin{bmatrix} 1 & t \\ t & 2t^2 \\ \end{bmatrix}$. But since I am not given a domain, couldn't both $x_1$ and $x_2$ be solutions of (H) as long as the domain for $t$ does not include $0$, as that would be the only point where $W(t) = 0$? The only thing I can think to apply is that the maximum interval of existence of each of these solutions is $\mathbb{R}$, which includes $0$. Should I generally assume that if I am not given a domain, I want the maximum interval of existence? (If anyone is wondering, this is a problem from Q. Kong's A Short Course in Ordinary Differential Equations . There is a second part but I am wondering the same thing about that part.) Edit: I've confirmed that it should be assumed that $A(t)$ is continuous on $\mathbb{R}$.",['ordinary-differential-equations']
1944136,Rate of decline of rank of matrix powers,"Questions such as this one ( What is the limit of the rank of the power of a matrix? ) address the limit of the rank of matrix powers, but do not mention at what rate this limit is approached.  Is there a closed form expression for this rate?  If not in general, then for certain classes of matrices (e.g. Toeplitz)?","['matrices', 'abstract-algebra', 'linear-algebra']"
1944140,Finding the centre of the group of quaternions,"The group of quaternions is $Q = <i, j, k | i^2 = j^2 = k^2 = ijk = -1>$.
I know the centre of a group Q is a subgroup consisting of those elements that commute with each other. So $Z(Q) = \{x \in Q: xq=qx  \forall q \in Q\}$. The elements appear to be $1, i, j, k,-1, -i, -j, -k$. So intuitively, it looks like the centre would be $1$ or (and?) $-1$ because $1$ and $-1$ multiplied by anything will be the anything (or negative anything). Is this correct? Is there more of a process in finding the centre?",['group-theory']
1944197,How to integrate around a square,"I am trying to calculate the complex line integral of $f(z)=z^{-1}$ around the square of sides 2, centred at the origin counter clock wise. I know I cant use Cauchy theorem because of the singularity. I thought maybe since f is holomorphic I could use the fundamental theorem, but then even nif my $F(z)=ln(z)$ I don't know what to use as my endpoints. I tried breaking it up into parts, ie end points at $1+i$ , $-1+i$, $-1-i$ and $1-i$, but when I compute using FTC that gives me 0. However, the correct answer I am told is $i2\pi$ , by the way , I also know that i2pi is the result of doing this integration but around a circle of radius irrelevant, maybe that could tie in somehow? I am looking for help. Thanks","['complex-analysis', 'complex-numbers', 'calculus']"
1944198,Perfect Shuffle Card Permutation Problem,I'm given the problem where one can perform perfect shuffles (i.e. you split the deck into halves and then interweave them) on a deck of $52$ cards (both in and out shuffles) and I am supposed to determine whether all $52!$ possible deck orderings are possible through a composition of such shuffles. I know that given only in or out shuffles you cannot do so since they are cyclic and of order $8$ and $52$ but I really have no idea how to even begin to tackle this problem of composing them. Was hoping for any hints or thoughts on as to how I should attempt this problem? Thanks! EDIT: An out shuffle is when you interweave leaving the top card on the top while an in shuffle is when you interweave by putting the top card of the bottom half on top of the whole deck.,"['permutations', 'group-theory']"
1944201,The set of real sequences has no countable spanning set,"I'm working through some old Harvard Math 55 problem sets, and one problem in particular has got me stumped: If $F$ is field, then prove that the vector space $F^\infty$ over $F$ (the space of infinite tuples or sequences) has no countable spanning set. In the case that $F$ is countable or finite, we can show that the space spanned by a countable set is necessarily itself countable and therefore cannot span $F^\infty$ , which is uncountable. However, that argument fails when the field is uncountable, since the span of even a single vector is an uncountable set. Any hints on how to proceed?","['linear-algebra', 'vector-spaces']"
1944205,"Let X, Y be positive random variables on a sample space Ω. Assume that X(ω) ≥ Y (ω) for all ω ∈ Ω. Prove that EX ≥ EY .","The problem is: Let $X$, $Y$ be positive random variables on a sample space $\Omega$. Assume that $X(\omega)\geq Y (\omega)$ for all $\omega \in \Omega$.  Prove that $\operatorname{E} X \geq \operatorname{E}Y$ .
I have a little bit confused about how to use ω here. I did not treat random variable as function before. Thank you","['probability-theory', 'probability', 'expectation']"
1944219,Voisin's proof of the Hodge index theorem,"I'm afraid this question is specific to Voisin's proof of the hodge index thm in her book Hodge Theory and Complex Algebraic Geometry vol 1. Let $X$ be compact Kahler of even complex dimension $\dim_{\mathbf C}=n$. In particular, she states that the sign of the Hermitian form $H(\alpha,\beta)=\int_X\alpha\wedge\overline\beta$ on $L^rH_{\mathrm{prim}}^{a,b}$ is equal to $(-1)^a$, where $a+b=n-2r$ and $L$ denotes the Lefschetz operator on cohomology. To obtain this, she refers to prior results which state that (A) (for possibly $k\ne n$), the form $(-1)^{k(k-1)/2}i^{p-q-k}H_k$ is positive definite on $H_{\mathrm{prim}}^{p,q}:=H^k(X,\mathbf C)_{\mathrm{prim}}\cap H^{p,q}(X)$, where $H_n$ coincides with $H$ as denoted above, and (B) on $L^r H^{k-2r}(X,\mathbf C)_{\mathrm{prim}}$, $H_k$ induces the form $(-1)^rH_{k-2r}$ (here I have corrected what I believe is a typo in the book). When I try to do this computation, I see that combining (A) and (B) above with $a+b=n-2r$, I get that $H$ ($=H_n$) has sign
$$(-1)^r(-1)^{(n-2r)(n-2r-1)/2}i^{a-b-(n-2r)}=(-1)^{r-r+n/2}i^{-2b}=(-1)^{n/2}(-1)^a$$
on $L^rH_{\mathrm{prim}}^{a,b}$. What have I misunderstood?","['hodge-theory', 'complex-geometry', 'algebraic-geometry']"
1944220,Geometrical derivation of angle addition formulae in hyperbolic trigonometry,"The angle addition formulas in “regular” trigonometry state that $\sin(\alpha+\beta)=\sin\alpha\cos\beta + \cos\alpha\sin\beta$ and $\cos(\alpha+\beta)=\cos\alpha\cos\beta - \sin\alpha\sin\beta$ It is fairly straightforward and easy to derive these formulas from a figure like this: It is also easy to prove it using: $\cos(x) = \frac{e^{ix} + e^{-ix}}2$ and $\sin(x) = \frac{e^{ix} - e^{-ix}}{2i}$ Similar relations hold for the hyperbolic trig functions, namely $\sinh(\alpha+\beta)=\sinh\alpha\cosh\beta + \cosh\alpha\sinh\beta$ $\cosh(\alpha+\beta)=\cosh\alpha\cosh\beta + \sinh\alpha\sinh\beta$ which again can be computed using $\cosh(x) = \frac{e^{x} + e^{-x}}2$ $\sinh(x) = \frac{e^{x} - e^{-x}}2$ How would one prove the angle sum formulas for the hyperbolic functions from their geometric definition ?","['hyperbolic-functions', 'trigonometry', 'geometry']"
1944233,Natural numbers for set of polymonials,"I'm working on the following problem and have made it through everything but the final step.  I'm not sure how to show that each of my subsets are distinct, maybe I'm missing something here or maybe my approach is just wrong.  Any help would be appreciated, thanks! Problem: Let z be an integer and $f(t) = a_nt^n + ... + a_1t + a_0$ be a polynomial with integer coefficients. Then there is a unique way to write $f(t)$ in the form $b_n(t - z)^n + ... + b_1(t - z) + b_0$. For instance, if $z = 3$ and $f(t) = t^2 + 4t − 1$, then $f(t) = (t - 3)^2 + 10(t - 3) + 20$. So in this example $b_2 = 1$, $b_1 = 10$ and $b_0 = 20$.  Use this fact to construct infinitely many different subsets N of polynomials with integer coefficients which satisfy the following properties (natural numbers): i)  If m,n ∈ N then m+n ∈ N ii)  If m,n ∈ N then mn ∈ N iii)  0 $\notin$ N iv)  For every m ∈ Z, we have m ∈ N or m = 0 or −m ∈ N My attempt at a solution: Let $A_k$ where k is an integer be a subset of the polynomials with integer coefficients such that the elements of $A_k$ are the polynomials that when written uniquely in the form $b_n(t - k)^n + ... + b_1(t - k) + b_0$ have a leading coefficient $b_n > 0$. First we will show that $A_k$ satisfies the desired properties (i) If $m,n \in A_k$ then $ m+n \in A_k$:  Let $b_m$ be the leading coefficient of element m and $b_n$ be the leading coefficient of element n when written in the form given above.  Then in case 1: m is of higher degree than n.  Then the leading coefficient of m+n is $b_m$ which we know is $>$ 0 since $m \in A_k$.  In case 2: n is of higher degree than m.  Then the leading coefficient of m+n is $b_n$ which we know is $>$ 0 since $n \in A_k$.  And in case 3: n is of the same degree as m.  Then the leading coefficient of m+n is $b_n + b_m$ which we know is $>$ 0 since $n \in A_k$ and $m \in A_k$. (ii) If $m,n \in A_k$ then $mn \in A_k$:  Let $b_m$ be the leading coefficient of element m and $b_n$ be the leading coefficient of element n when written in the form given above.  Then the leading coefficient of m+n is $(b_m)(b_n)$ which we know is $>$ 0 since $m \in A_k$ and $n \in A_k$. (iii) $0 \notin A_k$:  The zero polynomial has a leading coefficient $b_n = 0 \ngtr 0$.  So $0 \notin A_k$. (iv) For every $m \in Z$, we have $m \in A_k$ or $m = 0$ or $−m \in A_k$:  Let $m \in Z$.  Suppose (iv) does not hold.  Since $m \notin A_k$, we know that $b_m \leq 0$.  So $b_m = 0$ or $b_m < 0$.  Since we assumed (iv) does not hold, we must have $b_m < 0$.  So $0 - b_m \in N$, which gives us $-b_m \in N$, so $-b_m > 0$.  Since the leading coefficient of $-m$ is $-b_m$, $-m \in A_k$.  This is a contradiction since we assumed (iv) does not hold.  So (iv) does hold for $A_k$. Now we will show that there are infinitely many subsets $A_k$.  Since k is any integer, we can get infinitely many $A_k$s by varying k since Z is an infinite set. All that remains is to show that each subset $A_k$ is different for different k.  This is where I'm stuck but I'm pretty sure that it's possible to show this.  Would appreciate any help on this, thanks! Update: still stuck, but I think the approach is correct. any ideas?  Thanks!","['real-analysis', 'polynomials', 'elementary-set-theory']"
1944262,Unbounded linear operator,"Let $(A, \|\cdot\|_A), (B, \|\cdot\|_B)$ be normed linear spaces. Consider $T \in L(A,B)$
The operator norm of $T$ is defined to be
$$\|T\| = \sup\{\|Tx\|_B: \|x\|_A \leq 1\}$$ $T$ is bounded if $\|T\| < \infty$ otherwise it is unbounded. So can someone give me an example of an unbounded linear operator? This seems very counterintuitive to me because, that means 
$$\exists \space x \in A, \|Tx\|_B = \infty$$
but then any scalar multiples of $Tx$ would have an infinite norm. Then what would $T(0)$ be?","['operator-theory', 'linear-algebra', 'linear-transformations']"
1944275,Rooted Binary Trees and Catalan Numbers,"To form a rooted binary tree, we start with a root node. We can then stop, or draw exactly $2$ branches from the root to new nodes. From each of these new nodes, we can then stop or draw exactly $2$ branches to new nodes, and so on. We refer to a node as a parent node if we have drawn branches from it. This diagram shows all distinct rooted binary trees with at most $0,$ $1,$ $2,$ or $3$ parent nodes: (Note that, in the diagram, the roots are at the top and the branches extend downward -- somewhat contrary to what you'd expect for something called a ""tree""!) Prove that the number of distinct rooted binary trees with exactly $n$ parent nodes is the $n^{\text{th}}$ Catalan number. To count the number of rooted binary trees, I think you do something with a power of 2, because there's two choices at each point.  But that's all I have.  And for the Catalan numbers, which is $C_n = \frac 1{n+1}\binom{2n}n,$ and I don't understand the other recurrence, if you could explain that, it would be great.  Can someone walk me through this problem?  Thanks in advance!","['combinatorics', 'graph-theory', 'trees', 'catalan-numbers']"
1944297,A complicated handshake problem,"I am struggling with the following problem. Suppose there are two distinct parties having the same number of people.  In the first party, each person shakes hands with every other person in that party exactly once.  Then one person leaves and everyone remaining shakes hands with everyone else once again exactly once.  This procedure continues (i.e. one person leaves after which the remaining people shake hands again) until there are two people remaining, who shake hands once. In the second party, a different handshake game is played.  One person is selected to be VIP and every non-VIP party member shakes hands with the VIP exactly once.  The VIP then leaves the party with one other person and two new people enter the party and both become VIPs.  All non-VIPs shake hands with each of these two VIPs exactly once (the VIPs do not shake with each other).  Then the two VIPs leave with one other person and three new people enter the party and become VIPs, and every non-VIP at the party shakes with each VIP exactly once (VIPs do not shake with each other).  This process continues until only one person remains and shakes with each of the new VIPs (who are one less in number than the original people at the party). If 165 total handshakes occurred in the the second party, how many total handshakes occurred in the first party? I am lost in how to approach this problem, particularly in expressing the number of handshakes in the first party.  However, for the second party, I gather that the total number of handshakes given $N$ people initially present, expressed as a dot product between two vectors, is $(1, 2,...,N-1) \cdot (N-1, N-2,...,1)$, which must equal 165.  I am not sure how to solve for $N$ and approach the first party.","['combinatorics', 'discrete-mathematics']"
1944301,Can A Knot Be Tied In Anything Other Than 3 Dimensions?,"I've heard that a knot can only be tied in 3 dimensions. Does this only apply to 1 dimensional topologies? What about a 2 dimensional topology in 4 dimensions or a 2, 3, or 4 dimensional topology in 5 dimensions?","['general-topology', 'dimension-theory-analysis']"
1944314,Is it true that $\lim_{n\to \infty} \dfrac{\int_{\frac{1}{n}}^{1}\frac{f(x)}{x}dx}{n}=0$?,"Suppose $f$ is $L^{1}[0,\infty)$ and non-negative, and $\int_{0}^{\infty}f(x)dx=1$. Is it true that $lim_{n\to \infty} \dfrac{\int_{\frac{1}{n}}^{1}\frac{f(x)}{x}dx}{n}=0$? I derive this problem from a probability problem where I am asked to compute a limit of an expectation and $f$ is the probability density function. However, I cannot make an estimate for this integral. Thanks!","['real-analysis', 'probability', 'analysis', 'probability-distributions']"
1944321,Closed subset of a manifold is a submanifold if there exists a smooth retract.,"My question is regarding a specific exercise that I'm not really sure how to approach. Suppose I have a smooth manifold $M$ and let $A$ be a closed subset. Now suppose that there exists a smooth function $f:M \to A$ such that $f|_A = id$ (smooth retract). I want to show that $A$ is a submanifold, i.e, it is Hausdorff, second countable and locally euclidean. Hausdorff and second countable are both hereditary properties, so only the latter is left. My attempt was to prove that if $(U, \varphi)$ is a chart around $x \in A \subset M$ for $M$, then $\left(f(U), \varphi|_{f(U)}\right)$ is the desired chart. However, I'm not sure if $f(U)$ is open. I probably should use that $A$ is closed and $f$ is a retract, but I couldn't connect both facts. There is no need for a solution, just a hint is enough. PS: John Lee suggested that I should ask a question instead of leaving the exercise for the community, so here it is. I rewrote my previous post here. :)","['manifolds', 'smooth-manifolds', 'differential-geometry']"
1944342,Finding all differentiable functions satisfying a property,"I am trying to find all differentiable functions $f: \mathbb{R} \rightarrow \mathbb{R}$ s.t. $$ \forall x \in \mathbb{R}, \; \forall n \in \mathbb{N}^{+},
f'(x)=\frac{f(x+n)-f(x)}{n}$$
I know that a sufficient class of functions with this property is all linear functions $f(x)=ax+b$ for some real constants $a$ and $b$ since
$$f'(x)=a=\frac{a(x+n)+b-(ax+b)}{n}=\frac{f(x+n)-f(x)}{n}$$
but what is the necessary class of functions with the property?","['calculus', 'functions']"
1944375,Proving an unspecified function is invertible,"Given $g: \mathbb{R} \rightarrow \mathbb{R}$ is a differentiable function with bounded derivative i.e. satisfying $|g'(x)|\leq K>0 ,  \forall x \in \mathbb{R}$, I am trying to show that for some constant $\epsilon>0$ small enough, the function $f: \mathbb{R} \rightarrow \mathbb{R}$ defined by $f(x):=x+\epsilon g(x)$ is invertible. My idea is to show that $f$ is bijective.  It occurred to me that  $f$ is injective since its derivative can be made to be strictly positive so that it is strictly increasing if $\epsilon<1/K \implies f'=1+\epsilon g'>1+\epsilon(-K)>1-1>0$.  However, how can I prove $f$ is surjective?",['functions']
1944401,Solve recurrence: T(n) = T(logn) + logn,"I am struggling to find a f(n) so that T(n) = $\Theta$(f(n)). I can only think of one upper bound, that is: $log^{*}(n) \cdot logn$ Any suggestion about what may f(n) be? Thanks.","['recurrence-relations', 'asymptotics', 'algorithms', 'computational-complexity', 'discrete-mathematics']"
1944458,Does there always exist a polygonal path between any two points in the interior of a rectifiable Jordan curve?,"My interest in this question comes from Cauchy's integral theorem. If you have a rectifiable Jordan curve, can any two points in its interior be connected by a path in the interior made up of a finite number of directed line segments? Here's my heuristic for believing it's true: let $f(t), t \in [a, b]$ be a parametrization of the curve $J$. Place a mesh $(x_i)_{1 \le i \le n}$ on $[a, b]$ and construct a polygonal curve $P$ approximating $J$ by connecting $f(x_j)$ to $f(x_{j+1})$ with a directed line segment (connecting $f(x_n)$ to $f(x_1)$) and traversing the line segments in order. My guess is that as the mesh gets finer, eventually $P$ will lie in the interior of $J$ and will not cross $J$ anywhere. Since $P$ is a closed polygonal curve its interior can be triangulated, and since triangles are convex this means any two points in the interior of $P$ can be connected with a polygonal path. Let $A$ be the intersection of the exterior of $P$ with the interior of $J$. I also think that as the mesh gets fine enough any connected component of $A$ (of which there will be a finite number) will also be convex. Since the interior of $J$ is connected and covered by a finite number of convex sets the original conjecture will then follow. I can't demonstrate either of these heuristics formally though.","['complex-analysis', 'general-topology']"
1944480,Riemann and Darboux Integral of a product of two functions,"I'm studying the Darboux definition of integrability, which I completely explained here . There's an exercise that asks me to prove that the Darboux Integrability is equivalent to Riemann Integrability, but this Riemann integral is defined as the following: It first defines a 'pointed' partition (I don't know how to say it in ensligh) by the following: a 'pointed' partition $[a,b]$ is a pair $P^*=(E,ξ)$, where $P=\{t_0, t_1, \cdots, t_n\}$ is a partition of $[a,b]$ and $ξ = (ξ_1, \cdots, ξ_n)$ is a list of $n$ chosen numbers such that $t_{i-1}\le ξ_i\le t_i$ for each $i=1,\cdots ,n$. Now, the Riemann Integral is defined as: $$\sum(f,P^*) = \sum_{i=1}^n f(ξ_i)(t_i-t_{i-1})$$ (I didn't understand the notation for the left hand side of the equation, by the way) Finally, I'm asked to prove the following: given $f,g:[a,b]\to \mathbb{R}$ integrable functions, for the entire partition $P=\{t_0, \cdots, t_n\}$ of $[a,b]$ let $P^* = (P,ξ)$ and $P^{\#} = (P, η)$ be pointed partitions of $P$, then: $$\lim_{|P|\to 0}\sum f(ξ_i)g(η_i)(t_i-t_{i-1}) = \int_a^b f(x)g(x) \ dx$$ I guess here I need to prove that the Riemann Integral of the product of two functions if the darboux integral of $f(x)g(x)$, but it seems too obvious, I just need to verify that $f,g$ are integrable, then their product is too, isn't it? I'm pretty sure this should be a hard question. Is there another interpretation that I'm missing?","['real-analysis', 'integration', 'calculus']"
1944486,"Prove that $\sqrt{3} \notin \mathbb{Q}[\sqrt 2,\sqrt[3]{2},\dots,\sqrt[n]{2},\dots]$","I'm studying chapter 7.2 Algebraic extensions in Abstract Algebra by S. Lovett, and I'm stuck with an exercise problem. Let $S = \{ \sqrt[n]{2} : n \in \mathbb{Z}$ with $n \geq 2  \}$. Prove that $\sqrt{3} \notin \mathbb{Q}[S]$. Here is my argument. Suppose that $\sqrt{3} \in \mathbb{Q}[S]$. Since $\mathbb{Q}[S] = \cup \mathbb{Q}[\sqrt[n]{2}]$, there exists an $n$ such that $\sqrt{3} \in \mathbb{Q}[\sqrt[n]{2}]$. If $n$ is odd, $[\mathbb{Q}[\sqrt[n]{2}]:\mathbb{Q}]=[\mathbb{Q}[\sqrt[n]{2}]:\mathbb{Q}[\sqrt{3}]][\mathbb{Q}[\sqrt{3}]:\mathbb{Q}]$ and $[\mathbb{Q}[\sqrt{3}]:\mathbb{Q}]=2$, so we have a contradiction. Now I'm stuck at proving $\sqrt{3} \notin \mathbb{Q}[\sqrt[n]{2}]$ for $n$ even. I appreciate any help on this part or suggestion of another approach on whole problem.","['abstract-algebra', 'field-theory']"
1944509,"Probability Theory, Symmetric Difference","I'm trying to show this property of the symmetric difference between two sets defined for two sets in a universe $A$ and $B$ by
$$
A\Delta B=(A\cap B^{c})\cup(B\cap A^{c})
$$
I need to show that 
$$
\mathbb{P}(A\Delta C)\leq \mathbb{P}(A\Delta B)+\mathbb{P}(B\Delta C)
$$
for sets $A, B,$ and $C$ in the universe. I showed in the first part of the problem that 
$$
\mathbb{P}(A\Delta B)=\mathbb{P}(A)+\mathbb{P}(B)-2\mathbb{P}(A\cap B)
$$
My idea was to note that
$$
\mathbb{P}(A\Delta C)\leq\mathbb{P}(A\cap C^{c})+\mathbb{P}(C\cap A^{c})
$$
by probability laws and then leverage the fact that for any set I can write it as a union with another set. That is
$$
\mathbb{P}(A)=\mathbb{P}(A\cap B)+\mathbb{P}(B^{c}\cap A)
$$
and likewise for $C$ to substitute in for $P(A)$ and $P(B)$ terms. However, I end up running in circles. My TA did say I was on the right track, though. Any suggestions would be helpful. Thanks.","['probability', 'elementary-set-theory']"
1944517,Characteristic Function of Variance Gamma Distribution,"I am having trouble on proofing the characteristic function of Variance Gamma distribution. The VG model is obtained from the normal distribution by mixing on the variance parameter. Let $R_t$ be the return and suppose the distribution of $\log(R_t)$ is normal with mean $\mu$ and a random variance $\sigma^2V$ . Both $\mu$ and $\sigma^2$ are known constants. With the distribution of $V$ is taken to be gamma with $c$ and $\gamma$ as the parameters. So the density function of $V$ is $$g(v)=\frac{c^\gamma v^{\gamma-1}e^{-cv}}{\Gamma(\gamma)}$$ If $X=\log(R_t)-\mu$ , then according to Madan and Seneta (1990) the density of $X$ is: $$f(x)=\int_0^\infty[{e^{-x^2/2\sigma^2v}}/(\sigma\sqrt{2v\pi})]g(v)\,dv$$ It is told in the journal that there is no closed form for $f(x)$ . However, the characteristic function of $X$ has the closed form by conditioning on $V$ , given by $$\phi{_X}(u)=[1+(\sigma^2v/m)(u^2/2]^{-m^2/v}$$ where $m=\gamma/c$ and $v=\gamma/c^2$ are both mean and the variance of $g(v)$ respectively. I have tried to do it by several integration techniques, but stuck on the double integral calculation and the complex integration. One of my friend suggest me to solved it using Maple software, but we still don't know which integration method that could solve this problem.","['characteristic-functions', 'statistics', 'integration', 'probability-distributions']"
1944550,Show the continuous embedding $ \ell^2 \subseteq c_0. $,"Show the continuous embedding $$ \ell^2 \subseteq c_0. $$ I couldn't find a question that showed this ""simple"" proof, and I'm having trouble doing it myself. For normed spaces $\mathcal{X},\mathcal{Y}$, let $x \in \mathcal{X}$. Then
\begin{equation}
   \mathcal{X} \subseteq \mathcal{Y} \iff \exists c > 0 \;:\;\Vert x \Vert_{\mathcal{Y}} \le c \Vert x \Vert_{\mathcal{X}} \quad \forall x\in\mathcal{X}.
\end{equation} We say normed space $\mathcal{X}$ continuously embeds into normed space $\mathcal{Y}$. The sequence spaces $\ell^2,c_0$ have norms $$\Vert x \Vert_2 = \left( \sum_{n} |x_n|^2 \right)^{1/2}$$ and $$\Vert x \Vert_{c_0} = \sup_{n} |x_n|,$$ respectively. We say $x \in \ell^2 \iff \Vert x \Vert_2 < \infty$ and $x \in c_0 \iff \lim_{n\to\infty} x_n = 0$. My attempt: Suppose $x \in \ell^2$. Then $\lim_{n \to \infty} |x_n| = 0$. Hence, there exists $k$ such that $x_k = \sup_{n} |x_n|$. Therefore, \begin{align}
   \mathrm{LHS}
   & = \sup_{n} |x_n| \\
   & = |x_k| \\
   & \le \sum_{n} |x_n| \text{ since $|x_n| \ge 0$ for $n\ne k$} \\
   & \le c \left( \sum_{n} |x_n|^2 \right)^{1/2}
\end{align}
since it is true that for all $u,v\in\mathbb{R}$, $u \ge 0$, $v \ge 0$, there exists $c \ge 0$ such that $u \le cv$. Hence, $\ell^2 \subseteq c_0$. Is the property of real numbers that I used enough to justify the ending of my ""proof""?","['functional-analysis', 'sequences-and-series']"
1944630,Is every positive definite matrix symmetric? [duplicate],"This question already has answers here : What is the agreed upon definition of a ""positive definite matrix""? (3 answers) Closed 7 years ago . whether every positive definite matrix has to be symmetric? If not, what will be the example?","['matrices', 'linear-algebra']"
1944637,Infinite descending set sequence,"In ZFC theory, the axiom of regularity guarantees there's no infinite descending sequence. But it seems I found one. Define $x_n=\{n, x_{n+1}\}$, then it is obvious that $x_0\ni x_1\ni x_2\ni\cdots$. And to see why's no contradiction with the axiom. Take any set $x_i$, there are only two elements $i$ and $x_{i+1}$. Neither of them intersects with $x_i$. So what's wrong with this construction?",['elementary-set-theory']
1944689,Nth power of the following matrix,"The matrix is 
$ \begin{pmatrix}0&1\\-1&0\end{pmatrix}^n $ 
for  $n=2 \implies \left(\begin{matrix}-1 & 0\\ 0 &-1\end{matrix}\right)$ for $n=3 \implies \begin{pmatrix}1&0\\0&1\end{pmatrix}$ for $n=4 \implies \begin{pmatrix}-1&0\\0&-1\end{pmatrix}$ so I assume that for every $n=2k $, where k is a natural number and bigger than $0$ the matrix will be 
$\begin{pmatrix}-1&0\\0&-1\end{pmatrix}$ and for every $n=2k+1$ where k is a natural number and bigger than $0 $the matrix will be $\begin{pmatrix}1&0\\0&1\end{pmatrix}$ How can I prove it? probably with induction and how can I get easily the inverses of the matrices?","['matrices', 'linear-algebra']"
1944690,Reference for measure theory book with 'many' examples of different Measures,"I post this question with some personal specifications. I hope it does not overlap with old posted questions I am looking for a clear way to learn measure theoretic probability theory.I have already done a course in measure theory so i already know all basic concepts of Measure theory which are usually covered in $1$ semester course like Caratheodory's extension theorem,integration,Fubini's theorem,$L^p$-spaces,Radon Nikodyn Theorem,Riesz Representation theorem for compact metric spaces etc but i realize that although i have done a course in measure theory but i don't still know many examples of measures other than some usual Lebesgue measure,counting measure,Jordan Measure and Haar Measure etc.In think any theory without lot of examples is mostly useless.So i am looking for a book of measure theory which discuss probability theory and also discusses many examples of spaces like Coin tossing space and discuss measures on different spaces.Most of the books which i have seen yet does not really give many examples of measures. Is there any book of Measure theory which develops measure theoretic probability theory as well as discuss many examples of 'different' measure?","['reference-request', 'probability-theory', 'probability', 'measure-theory']"
1944706,"Use of word ""axiom"" in definition of vector spaces","Consider the following definition of vector spaces: Why are the listed conditions called ""axioms""? My understanding of axioms is that they are base assumptions which are taken to be true. Thus, they're not really meant to be proven. Yet from this definition, it's necessary to show that the axioms are ""satisfied"" for a specific set in order to conclude that the set is a vector space. Is that somehow different than ""proving"" the axioms are true for the given set?","['axioms', 'linear-algebra', 'definition']"
1944743,"Compute the conditional expectation $E(Y|X)$ for a measurable function $Y$ and a random variable $X$ taking values on $[0,1)$","Good day, Currently I am working with ""Probability: Theory and Examples"" by Durrett and while getting familiar with conditional expectations I got to this problem: Consider the Lebesgue probability space on the interval $[0,1)$. (I.e. the state space is $Ω = [0, 1)$, the $\sigma$-field is the set of Lebesgue measurable sets and the measure is the Lebesgue measure.) We define the random variable $X$ as:
  $$X(w)=\begin{cases} 2w &, 0\leq w < 1/2 \\ 2w−1 &, 1/2\leq w<1 \end{cases}$$
  Compute the conditional expectation $E(Y |X)$ where $Y : [0, 1) \to \mathbb{R}$ is a measurable function. First off $Y$ is not defined further. I am a bit confused about the term ""measurable function"". Of what? A measurable function of $X$? But then it would just be $E(Y|X)=Y$. So I assume $Y$ to be a random variable not necessarily independent of $X$. Second let's define conditional expectation: $E(Y|X):=E(Y|\sigma(X))$ is a random variable $Z$ such that $Z$ is measurable w.r.t. $\sigma(X)$ and $E(1_A Y)=E(1_A Z)$ for all $A \in \sigma(X)$. Okay, let's pick a $A \in \sigma(X)$ then there exists a $B \in \mathcal{B}(\mathbb{\mathbb{R}})$ such that $A=X^{-1}(B)$. As Graham Kemp helpfully hinted, the correct inverse of $X$ is $$X^{-1}\{x\}~=~\{w\in[0;1):x=X(w)\}~=~\begin{cases}\{x/2, (1+x)/2\}&:& 0\leq x<1\\ \{\}&:&\text{otherwise}\end{cases}$$ Now I am not sure to do. Normally I would begin from $$E(1_A Y)= \int_A Y dP=\int_B y P_Y(dy)$$ but now I am at my end. Can someone take it from here and show me what to do? I am thankful for every help/hint.","['probability-theory', 'conditional-expectation', 'probability', 'random-variables']"
1944786,How to use the derivative (which has x and y in the answer) to approximate values of the function,"For ethical reasons, I won't ask the exact assignment question: For a function such as $4x^3 + 2y^3 -yx^2 = 49$. How could I use the derivative to estimate the values on the curve near a certain point? I am asked to create a table of values close to a point given to me. Since I have created the function above, I cannot give one as there is likely no pretty answer. So, if the solution requires this, how would I utilize it? The derivative of this function is similar in nature to the one in my assignment, that being it has values of x and y. The derivative for this particular function is
$$
f'(x)=\frac{2x(6x-y)}{x^2 -6y^2}
$$ I believe the solution lies in the using something similar to f(x) ≈ f(a) + f'(a)(x − a), but I do not know how to implement this. For my assignment in particular it asks me, after telling me to find values on the curve near x=1, y=2, to include the 0.96, 0.98, 1, 1.02, and 1.04 in my table of values. I can only assume this means the x values and find the subsequent y? I am not sure how to do this with the derivative considering if I put in a value of x, there are two unknowns: the value of y and the derivative itself. Sorry I have waffled a bit, and I apologise if my wording makes things confusing. It is a manifestation of my own confusion. Edit:
The next part of the question involves finding x values such that the tangent to the curve is horizontal, or vertical. How would I go about this? My initial response. for horizontal, is to make the derivative =0, but I have two variables? How could I find a vertical tangent?","['derivatives', 'calculus', 'approximation']"
1944807,"Arrangements of a,a,a,b,b,b,c,c,c in which no three consecutive letters are the same","Q: How many arrangements of a,a,a,b,b,b,c,c,c are there such that $\hspace{5mm}$ (i). no three consecutive letters are the same? $\hspace{5mm}$ (ii). no two consecutive letters are the same? A:(i). 1314. ${\hspace{5mm}}$ (ii). 174. I thought of using the General Principle of Inclusion and Exclusion along with letting $p_i$ denote a property that.. , and doing so, I will evaluate $E(0)$, which gives us the number of arrangements without any of the properties. How do I go about doing that? I am trying to use the method stated above in solving this question, but I am unable to generalize the properties $p_i$.","['permutations', 'combinatorics', 'inclusion-exclusion', 'combinations']"
1944818,How to come from $\frac{d}{dt} f(t) = \frac{2t}{1-t} f(t) + \frac{1}{(1-t)^2}$ to $f(t) =\frac{1-e^{-2t}}{2(1-t)^2}$,"From an answer on this question , I got the differential equation $$\frac{d}{dt} f(t) = \frac{2t}{1-t} f(t) + \frac{1}{(1-t)^2}$$ and I was even given the solution $$f(t) = \frac{1-e^{-2t}}{2(1-t)^2}.$$ It seems to be correct, however, I have trouble understanding how the solution was achieved. I tried building a power series $f(t) = \sum_{n=0}^\infty a_n t^n$ out of $f$ and finding the coefficients for it like in this tutorial , but I ended up in a huge ugly sum term $$ a_n = \frac{(2t)^n}{n!(1-t)^n} a_0 + \sum_{k=1}^n \frac{(2t)^{n-k}}{n!/k! (1-t)^{n-k}}. $$ Not fully shure if this is right. Somehow, the taylor series of the $e$-function sticks in there, but I don't think that this will lead to the solution above. So I am stuck here and any help would be appreciated.","['ordinary-differential-equations', 'power-series']"
1944859,Series $\sum_{n=2}^\infty \frac{14^n}{3^{3n+4}(3n+7)}$ Convergence or Divergence Using The Ratio Test,"I am trying to determine if the following series converges or diverges by using the ratio test, which I believe can be summarized as the following: $$L=\lim_{n\to \infty}  \left| \frac{a_{n+1}}{a_n} \right|$$ If $L < 1$ then the series converges, if $L > 1$, then it diverges, and if $L = 1$, then it is ambiguous. The series is below: $$\sum_{n=2}^\infty \frac{14^n}{3^{3n+4}(3n+7)}$$ I understand the ratio test in theory, but am not sure how to put it into practice for a series like this.","['sequences-and-series', 'convergence-divergence', 'limits']"
1944891,Differentiate $f(x)=\tanh(x)$,"Calculate the first derivative of the function $f:=
\tanh=\frac{\sinh}{\cosh}: \mathbb{R} \rightarrow \mathbb{R}$ I know that $\sinh(x)=\frac{1}{2}(e^{x} -e^{-x})$ and $\cosh(x) = \frac{1}{2}(e^{x}+e^{-x})$ $\Rightarrow$ $$f(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$ $$f'(x)= \frac{(e^{x}+e^{-x})(e^{x}+e^{-x})-((e^{x}-e^{-x})(e^{x}-e^{-x}))}{(e^{x}+e^{-x})^{2}}$$ $$f'(x)=1-\frac{(e^{x}-e^{-x})^{2}}{(e^{x}+e^{-x})^{2}}$$ $$f'(x)=1-\left(\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\right)^{2}$$ $$f'(x)=1-\left(\frac{\sinh(x)}{\cosh(x)}\right)^{2}$$ $$f'(x)=1-\tanh^{2}(x)$$ Is everything right? The calculation, the form etc.?","['functions', 'derivatives', 'calculus', 'analysis']"
1944894,"If $20x=\pi$, what is $\frac{\cos 4x - \cos 8x}{\cos 4x\cdot \cos 8x}$?","If $20x=\pi$,
what is $$\frac{\cos 4x - \cos 8x}{\cos 4x\cdot \cos 8x}?$$ I've tried using the factor formula on the numerator but I haven't managed to get anywhere with it... This is a multiple choice question with options $4$, $2$, $1$, $-1$, and $-2$. Thanks in advance!",['trigonometry']
1944895,Proof that $\|fx\| \leq \|f\|\cdot\|x\|$,"From the wiki article on the dual of a norm: $X$ and $Y$ are normed spaces, and we associate with each $f\in L(X,Y)$ (the space of bounded linear operators from $X$ to $Y$) the number
$$\|f\| = \sup\{|f(x)|:x\in X, \|x\| \leq 1\}.$$ At some point in the proof that $L(X,Y)$ is bounded, it seems like they use the inequality
$$\|fx\| \leq \|f\|\cdot\|x\|,$$
where $\|x\|\leq 1$. I can't see why this should holds. $\|f|\|$ is the supremum of what $|f(x)|$ can be, given that $\|x\|\leq 1$. So, given that $\|x\| \leq 1$, $|fx|$ should be bounded by $\|f\|$. But why should it be bounded by $\|f\|\cdot\|x\|$?","['functional-analysis', 'normed-spaces', 'linear-transformations']"
1944907,Notation of the identity element in a group,Is there any special reason why the identity element in a group is commonly denoted by $e$ in abstract algebra?,"['abstract-algebra', 'notation', 'group-theory']"
1944927,"Are the different ways of rigorizing the notion of ""differential"" mutually exclusive?","Let $C=\{(x,y):f(x,y)=0\}$ be the level set of a continuously differentiable function $f(x,y)$ of two variables. Using implicit differentiation, we get: $$\frac{dy}{dx} = - \frac{\displaystyle\frac{\partial f}{\partial x}}{\displaystyle\frac{\partial f}{\partial y}}$$ Thus, with a wink and a nudge, we define the differential of $f$ to be $$df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy. $$ I am aware of at least two different ways to make this notion rigorous: 1. Using multilinear algebra, interpret $dx$ and $dy$ as differential forms. 2. Using non-standard analysis, interpret $dx$ and $dy$ as infinitesimals (in the hypperreals). Questions: Are these two interpretations mutually exclusive? Is there a third way to think of them that unifies both? As far as I am aware, differential forms and infinitesimals are very different objects, so it seems like these two ways of understanding the concept of differential are intractable when considered together. The motivation for the notion of differential as stated above comes from p. 174, section 3.6.2., of Algebraic Geometry: A Problem Solving Approach .","['derivatives', 'differential', 'nonstandard-analysis', 'differential-forms', 'soft-question']"
1944930,why two solutions to DE are contradictory?,"Solution to the given differential equation $$\frac{dx}{dt}=4.9-0.196x$$
  is given by a) $x=25+ke^{-0.196t}\qquad$ b) $x=50+ke^{-0.196t}\qquad$ c) $x=50-ke^{-0.196t}\qquad$ d) $x=25-ke^{-0.196t}\qquad$ where k is some constant my try: method(1) $$\frac{dx}{4.9-0.196x}=dt$$
$$\int \frac{dx}{4.9-0.196x}=t+c$$ substituting $4.9-0.196 x=z$, $dx=-\frac{dz}{0.196}$, i get
$$\int \frac{dz}{z}=-0.196(t+c)$$
$$\ln z=\ln(4.9-0.196x)=-0.196(t+c) \tag{*}$$
$$0.196x=4.9-e^{-0.196(t+c)}=4.9-e^{-0.196c}\cdot e^{-0.196t}$$
$$x=25-ke^{-0.196t}\tag 1$$
method(2) $$\frac{dx}{dt}=4.9-0.196x$$
$$\frac{dx}{dt}+0.196x=4.9$$
now, use integration factor $I.F.=e^{\int 0.196\ dt}=e^{0.196 t}$
so the solution is 
$$x\cdot (I.F.)=\int (I.F.)\cdot 4.9\ dt+c$$
$$x\cdot e^{0.196 t}=\int e^{0.196 t}\cdot 4.9\ dt+c$$
$$x\cdot e^{0.196 t}=\frac{4.9}{0.196}e^{0.196 t}+c=25e^{0.196 t}+c$$
$$x=25+ke^{-0.196 t}\tag 2$$
now, you see i am getting two different solutions (1) & (2) to the same D.E., but i don't know which one is correct & why.
please explain me where i am wrong or which is the correct option & why?","['ordinary-differential-equations', 'calculus']"
1944940,Solving Summation Equations for Method of Moments Proof,"A part of the solution in a proof of a question related to the method of moments states that, $$\frac{1}n\sum_{i=1}^n{(y_i-\bar{y})^2}=\frac{1}n\sum_{i=1}^n{y_i^2-\bar{y}^2}$$ How does this follow?","['statistics', 'summation']"
1944967,Multivariate mean value theorem in statistics context,"I am trying to understand the proof of the following theorem. As first step, he uses the mean value theorem for a multivariate function. After research online, it seems that this theorem does not generalize to such functions. Could anyone guide me to what the author really says? Thanks,the theorem is below.","['multivariable-calculus', 'real-analysis', 'statistics', 'calculus']"
1944987,Is the trace of a unitary matrix always real?,"In a physics context I work with the SU(2) and SU(3) matrix groups. Let $U$ be such a special unitary matrix. There are expressions like $(U - \mathrm{h.c.})$ which turn out to be traceless. This means that the trace of $U$ is always real. I played around with Mathematica to generate matrices $U$ from the generators $\sigma_i$ and the matrix exponential function. The real part of the trace of an SU(2) matrix in terms of the three algebra components nicely oscillates: The imaginary part seems to be virtually zero, the numerical matrix exponential generates small imaginary parts of say $10^{10}$. From the color coding this seems rather zero: I know that $U = \exp(\mathrm i \alpha_i \sigma_i)$, that $U^{-1} = U^\dagger$ and a couple of other identities. However I cannot deduce that the trace is always real. Is it true that the trace of special unitary matrices is always real? How can one show it?","['trace', 'group-theory']"
1945049,"Measurability of $f\otimes g:(X\times Y,\mathcal{B}(X)\otimes \mathcal{B}(Y)) \to (H_1 \otimes H_2,\mathcal{B}(H_1\otimes H_2))$","Assume that $(X,d)$ and $(Y,\rho)$ are metric spaces, and that $f:X \to H_1$ and $g:Y\to H_2$ are isometries (especially Borel measurable) into Hilbert spaces. Now define the mapping
$$
f\otimes g:X\times Y \to H_1 \otimes H_2 \quad \quad \text{by} \quad f\otimes g(x,y) = \iota\circ f(x)\otimes g(y),
$$
where $H_1 \otimes H_2$ is the completion of the tensor product of $H_1$ and $H_2$ (i.e. a Hilbert space) and $\iota$ is the embedding into the completion. I need to show that $f\otimes g$ is $\mathcal{B}(X)\otimes \mathcal{B}(Y))/\mathcal{B}(H_1\otimes H_2)$-measurable or at the very least that $h^* \circ f\otimes g$ is measurable for every $h^* \in (H_1 \otimes H_2)^*$, i.e. $f\otimes g$ is weakly measurable. Can anybody show me how to proceed? For example can we find a generator for  $\mathcal{B}(H_1\otimes H_2)$ which allows for a smooth proof.","['general-topology', 'measure-theory', 'hilbert-spaces']"
1945059,Geometric structures which always exist,"It is well known fact that Riemannian metric always exist on any manifold. Another way to say that is that any manifold always admit $O(n)$ structure i.e. $O(n)$ reduction of structure group of frame bundle $L(M)$. One way to see that (next to the classical argument with partition of unity) is that $Gl(n)/O(n)$ is contactible, existance of a $G$ structure is equivalent to the existance of section of a fibre bundle $L(M)/G$ and fibre bundles with contractible fibre always possess one. My questions are: $\bullet$ What are another closed subgroups $G \subset Gl(n)$ with $Gl(n)/G$ contractible ? $\bullet$ What are another closed subgroups $G \subset Gl(n)$ such that any manifold admits $G$ structure?","['differential-geometry', 'lie-groups']"
1945079,in any triangle abc prove that bisectors of the interior angle A and the exterior angles at B and C are concurrent,"i assumed the exterior angles meet at point D, then joined A with D and tried proving AD is angle bisector of angle A. 
But i cant get to the proof. any idea will be helpful",['geometry']
1945113,Expected value of squared sum vs. square of expected sum,"Edit: A lot of my computations in the question below are utterly wrong, I just left them as they since the comments/answers applied to the original form. Edit: I have updated the question to hopefully better explain the draws I am referring to. In the actual problem, $x_{(i)}, i\in\{1,\ldots,n\}$ are the $n$ lowest out of $m$ i.i.d. draws from $U[0,1]$. I hope I am making a duly simplification below. Let $x_{(1)}\leq\ldots\leq x_{(n)}$ be $n$ i.i.d. draws of a random variable uniformly distributed in $[0,1]$. I know that (if I am not mistaken) \begin{alignat*}{2}
E[x_{(i)}]&=\frac{i}{n+1}\\
	E[\sum_{i=1}^n x_{(i)}] &=\frac{n}{2}, \\
	E[\sum_{i=1}^n (x_{(i)} ^2)] &= \sum_{i=1}^n\left(\frac{i}{n+1}\right)^2=\frac{n(2n+1)}{6(n+1)}.
\end{alignat*} I furthermore know that
\begin{align}
\underbrace{E[x_{(i)}^2]}_{\frac{2n+1}{6(n+1)}}\neq\underbrace{(E[x_{(i)}])^2}_{\frac{1}{4}}
\end{align}
and hence was expecting that
\begin{align}
E[\left(\sum_{i=1}^n x_{(i)}\right)^2]\neq\left(E[\sum_{i=1}^n x_{(i)}]\right)^2.
\end{align}
However, my calculation is
\begin{alignat*}{3}
	E[\left(\sum_{i=1}^n x_{(i)}\right)^2]%
	&= E[\sum_{i=1}^n (x_{(i)}^2)+2\sum_{i=1}^n x_{(i)} \sum_{j=1}^{i-1}x_{(j)}]  \\
	&= \frac{n(2n+1)}{6(n+1)} + 2\sum_{i=1}^n \frac{i}{n+1} \frac{(i-1)i}{2(n+1)}\\
	&= \frac{n(2n+1)}{6(n+1)} + \frac{1}{(n+1)^2}\sum_{i=1}^n (i^3-i^2)\\
	&= \frac{n(2n+1)}{6(n+1)} + \frac{1}{(n+1)^2}\left(\left(\frac{n(n+1)}{2}\right)^2-\frac{n(n+1)(2n+1)}{6}\right)\\
	&= (\ldots) = \left(\frac{n}{2}\right)^2=\left(E[\sum_{i=1}^n x_{(i)}]\right)^2
\end{alignat*} Are the two (i.e. expected value of squared sum, and square of expected sum) really equal or have I made a mistake in my calculations? If they are equal, is there an intuition why squares and expectations are interchangeable here but not in the case above (i.e. for the sum but not for the individual $x_{(i)}$)? (Apologies if the calculations are somewhat cumbersome, I have tried to simplify the original problem with more parameters to this form.)","['statistics', 'probability', 'order-statistics']"
1945162,Finite index proper subgroups of $Z[1/p]$,"I know how to prove that $\mathbb{Q}$ has no finite index proper subgroups: Let $H\leq\mathbb{Q}$ be a finite index subgroup. Let $x\in\mathbb{Q}$. Then $x+H=[\mathbb{Q}:H]\cdot(x/[\mathbb{Q}:H]+H)=H$ (by Lagrange's theorem). So $x\in H$, and thus $H=\mathbb{Q}$. Now, what about finite index subgroups of $G=\mathbb{Z}[1/p]$? An analogous argument shows that if $[G:H]=p^n$ then $H=G$. However, I still ask: Does $\mathbb{Z}[1/p]$ have finite index proper subgroups?",['group-theory']
1945196,Generalization of Lagrange's theorem $(1768)$?,"Here's the theorem : Let $p$ a prime number and $u_0,...,u_n$, a list of integers such that $p\not \mid u_n$. Then : $u_nx^n+...+u_1x+u_0 \equiv 0\pmod p$ admits at most $n$ solutions $\pmod p$. The proof can by done using induction on $n$ and the property of the prime number $p$. Now, I was wondering how it will work if we consider an integer $k$ instead of $p$. The statement will give : Let $k$ an integer and $u_0,...,u_n$, a list of integers such that $\gcd(k,u_n)=1$. Then how many solutions $\pmod k$ the equation : $u_nx^n+...+u_1x+u_0 \equiv 0\pmod k$ admits ? I think we can start with the decomposition theorem $k=p_1^{a_1}...p_l^{a_l}$. Maybe it will give a system in CRT style. Here's my attempt : First important fact : if $k\mid u_n\Leftrightarrow p_1^{a_1}...p_l^{a_l}\mid u_n\Rightarrow \exists i\in \{1,...,l\}, \ p_i^{l_i}\mid u_n$. For a factor $p_i^{a_i}$ we try to find the number of solutions
  $\pmod{p_i^{a_i}}$ of the equation : $u_nx^n+...+u_1x+u_0\equiv 0
 \pmod{p_i^{a_i}}$. By induction on $n$ I have : -For $n=0$ : the equation becomes : $u_1x\equiv -u_0 \pmod{p_i^{a_i}}$. The equation becomes $u_1x\equiv -u_0 \pmod{p_i^{a_i}}$. But we have
  $\gcd(u_1,p_i)=1$ and by property of Bézout we can deduce that
  $\gcd(u_1,p_i^{a_i})=1$. So $u_1$ has an inverse element and we can
  take $x\equiv -u_1^{-1}u_0 \pmod{p_i^{a_i}}$ which represents one
  solution (the only one). -For $n=n+1$ : the equation becomes $u_{n+1}x^{n+1}+u_nx^n+...+u_1x+u_0\equiv 0 \pmod{p_i^{a_i}}$.If  I
  consider $y$ a solution of the equation with the multiplicity $e=1$
  (for instance) we have the fact that we can factorize the equation by
  $(x-y)^{e}$ . It gives $(x-y)^{e}P(x)\equiv 0 \pmod{p_i^{a_i}}$ with $P$ a degree
  $n$ polynomial and with highest coefficient $u_{n+1}$ such that
  $\gcd(p_i^{a_i},u_{n+1})=1$. So the equation admits at most $n+1$
  solutions $\pmod{p_i^{a_i}}$. So there is at most $n$ solutions for $\pmod{p_i^{a_i}}$ and for each
  $i\in \{1,...,l\}$. If I want to use the CRT it gives a systeme of $l$ lines where each
  polynomials admit at most $n$ solutions. How can I conclude $\pmod k$
  (it's not a field) ? If we suppose that for each $p_i^{a_i}$ there are at most $n$
  solutions we can factorize the $l$
  lines with $n$ factors. Unfortunately this fact is false (look at $(x-1)(x-2)(x-4)\equiv 0 \pmod{9}$ which have $4$ solutions instead of $3$). Here is the main system : $\left\{\begin{array}{rl}
           u_{n}(x-x_{1_1})(x-x_{1_2})...(x-x_{1_n}) &\equiv 0  \pmod{p_1^{a_1}} \\
              &\vdots \\
           u_{n}(x-x_{i_1})(x-x_{i_2})...(x-x_{i_n}) &\equiv 0  \pmod{p_i^{a_i}} \\
              &\vdots \\
           u_{n}(x-x_{l_1})(x-x_{l_2})...(x-x_{l_n}) &\equiv 0  \pmod{p_l^{a_l}} \\
           \end{array}
            \right.$ And for instance by Euclid's lemma (for the case of $(a_i)_{i\{1,...,l\}}=1$) to count the number of systems :  for $u_{n}(x-x_{1_1})$ we have $(n^{(l-1)})$ systems possible with one solution. It's the same for each $u_{n}(x-x_{i_j})$ with $j\in \{1,...,n\}, \ i=1$ right.If it's the case it will give $n^l$ solutions. Thanks in advance !","['diophantine-equations', 'congruences', 'combinatorics', 'modular-arithmetic', 'elementary-number-theory']"
1945198,Showing that the union of transitive relations need not be transitive.,"I have a counterexample of the following statement but I am not sure if it is correct: Suppose $R_1$ and $R_2$ are relations on A. If $R_1$ and $R_2$ are transitive, then $R_1 \cup R_2$ is transitive. Prove or provide a counterexample. To me, this is of the form: If: $$((x,y)\in R_1 \land (y,z) \in R_1 \implies (x,z)\in R_1) \land ((x,y)\in R_2 \land (y,z) \in R_2 \implies (x,z)\in R_2) $$ Then: $$ ((x,y)\in R_1 \lor (x,y) \in R_2) \land ((y,z)\in R_1 \lor (y,z)\in R_2) \implies ((x,z)\in R_1 \lor (x,z)\in R_2)   $$ My counterexample: \begin{align*}
A&=\{ 1,2,3\} \\
R_1&= \{(1,2)\} \\
R_2&= \{(2,3)\} 
\end{align*} Then if we take $x=1,y=2,z=3$ , it should yield the following: If: $$(\underbrace{(x,y)\in R_1 \land (y,z) \in R_1}_{\text{false}} \implies \underbrace{(x,z)\in R_1}_{\text{false}}) \land (\underbrace{(x,y)\in R_2 \land (y,z) \in R_2}_{\text{false}} \implies \underbrace{(x,z)\in R_2}_{\text{false}}) $$ Then: $$ (\underbrace{(x,y)\in R_1 \lor (x,y) \in R_2)}_{\text{true}} \land (\underbrace{(y,z)\in R_1 \lor (y,z)\in R_2)}_{\text{true}} \implies (\underbrace{(x,z)\in R_1 \lor (x,z)\in R_2}_{\text{false}})   $$ I know it's a mess, but it's the best I can do to illustrate my idea clearly. Could anyone please help? Thank you so much!","['relations', 'elementary-set-theory', 'solution-verification']"
1945199,Fourier Series of $e^x$ [duplicate],"This question already has answers here : Fourier Series Representation $e^{ax}$ (3 answers) Closed 7 years ago . I am tying to integrate $a_n= \frac{1}{L} e^x\sin\left(\frac{n\pi x}{h}\right)dx$ and get the solution in sinh form.  I have gotten the long answer, but cannot figure out how to turn it into sinh.
Can someone help me with the steps?",['ordinary-differential-equations']
1945229,Lebesgue Measurable function raised to a power.,I would like to prove or give a counterexample to the following. Thanks for any help in advance. Let $f$ : R → R be a function. a) Suppose $f^{2}$ is Lebesgue measurable. Does it follow that $f$ is Lebesgue measurable? b) Suppose $f^{3}$ is Lebesgue measurable. Does it follow that $f$ is Lebesgue measurable?,"['lebesgue-measure', 'measure-theory']"
1945256,"Prove $f\in L_2$ if $\langle f,g\rangle< C\|g\|_{L_2}$ for any $g\in D$ and $\overline{D}=L_2$","Let $D$ be a subset of and dense in $L_2$. If $\langle f,g\rangle<C\|g\|_{L_2}$ for a constant $C$ and any $g\in D$, then $f$ must be in $L_2$. I read this statement in a paper. I think it is right, because the following argument makes sense. The fact that $\langle f,g\rangle< C\|g\|_{L_2}$ for any $g\in D$ implies that $f$ is a linear functional on $L_2$. It is known that the dual of $L_2$ is $L_2$. Thus $f$ should be in $L_2$. This argument is not rigorous and I think the duality of $L_2$ is a bit too advanced for this statement. Could anyone give a tip of an elementary proof that does not require the duality property? If needed, $D$ is the set of compact support functions in $C^{\infty}$ and $\langle f,g\rangle=\int f\bar{g}$. We can consider the problem in $L_2(\mathbb{R})$. But I guess that what $D$ is exactly does not matter, except that it is dense in $L_2$.","['functional-analysis', 'real-analysis']"
1945258,Outlier-resistant average of a set?,"I have a web application in which users can provide valuations for items.  These valuations are currently unrestricted, and there is no non-arbitrary criteria by which I could restrict them, since valuations are purely subjective.  For each item, I have an ""average valuation"", which is currently just a simple mean of all valuations for the item.  This was a naive choice that has inevitably lead to abuse, whereby a small subset of users intentionally provide extreme valuations to manipulate this ""average valuation"" to their benefit. I'd like to solve this problem mathematically, without imposing arbitrary restrictions on submitted valuations.  As far as I can tell, it's very commonplace for item valuations to follow a normal distribution, but I admittedly don't remember much of my statistics from decades ago.  I have been attempting to brush up on standard deviations, but it seems like I can't exclude valuations over 2σ because these extreme outliers raise the population standard deviation (I could be misunderstanding this concept, though).  So... What statistical approach can I take to determine the ""average valuation"", while remaining resistant to this kind of abuse? For reference, here is an example set of valuations for a particular item.  The outliers here are clearly the [317, 318, 630, 630, 640, 6511] subset: 60, 63, 63, 63, 63, 63.5, 63.8, 63.8, 63.9, 63.9, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64.2, 64.5, 64.5, 64.5, 64.5, 64.5, 64.9, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66.5, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 72, 74, 74, 75, 75, 75, 75, 78, 80, 85, 317, 318, 630, 630, 640, 6511 For this set of valuations, I would expect an ""average valuation"" somewhere around 65 or 66.","['statistics', 'standard-deviation']"
1945296,How logarithm is properly defined on a field?,"Given a field $(F,+,\times)$, an exponential function is  defined as a function $E:F\to F$ s.t. $E(x+y)=E(x)E(y)$ and $E(0)=1$ where $0$ is the additive identity and $1$ is the multiplicative identity. I am curious how logarithm is properly defined for $F$ and how the connection with the exponential function is made? Is it defined as a $L:F \to F$ function s.t. $L(xy)=L(x)+L(y)$ and $L(1) = 0$? Any explanation and reference is welcome. Thank you!","['abstract-algebra', 'field-theory']"
1945305,Complete ordered space,"In their paper A Method for Constructing Ordered Continua , Hart and van Mill give the following definition of ordered continuum : An ordered continuum is a compact, connected linearly ordered topological space, equivalently, a complete and densely linearly ordered set equipped with the order topology. My question is a about the word complete . I know that a linearly ordered space is connected if and only if it is densely ordered and Dedekind complete (every non empty subset that is bounded above admits a supremum). But that is not enough for this definition. So I think that complete here means complete lattice (every subset has both supremum and infimum), as complete lattices are compact. Is that correct? If it is, I have another question. Is completeness as a lattice equivalent to completeness in the sense of uniformity (in case the space is indeed uniformizable)? Thank you!","['complete-spaces', 'general-topology', 'order-topology']"
1945329,Can you transpose a matrix using matrix multiplication?,"Say you have a matrix A = \begin{bmatrix}a&b\\c&d\end{bmatrix} and I want it to look like $A^T$ = \begin{bmatrix}c&a\\d&b\end{bmatrix}
Can this be done via matrix multiplication? Something like a matrix T such that $T*A = A^T$.","['matrices', 'transpose', 'linear-algebra']"
1945351,"If $C$ is the Cantor set, then $C+C=[0,2]$.","Question : Prove that $C+C=\{x+y\mid x,y\in C\}=[0,2]$, using the following steps: We will show that $C\subseteq [0,2]$ and $[0,2]\subseteq C$. a) Show that for an arbitrary $n\in\mathbb{N}$ we can always find $x_n,y_n\in C_n$, where $$C_n=\left[0,\frac1{3^n}\right]\bigcup \dots \bigcup\left[\frac{3^{n}-1}{3^n},1\right]$$such that for a given $s\in[0,2]$ we have  $x_n+y_n=s$. b) Then we will set $x=\lim x_n$ and $y=\lim y_n$,  then $x+y=s$. My progress : Showing that $C+C\subseteq [0,2]$ is obvious, and I did part a) by showing that if $x_n,y_n$ are in different subintervals then concluding that $x_n+y_n$ covers $[0,2]$ (can be done using induction on $n$). My difficulty is in the second part. The sequence $x_n$ is bounded so it must have a convergent subsequence $(x_{n_{k}})$. If we set $x=\lim x_{n_{k}}$, then we can conclude $\lim y_{n_{k}}=y=s-x$, thus $x+y=s$. First I thought that $x,y$ will be in $C$ as it is closed. However $(x_{n_{k}})$ may not necessarily be in $C$, so $x$ can't be in $C$ for sure. Is this last thought correct? How do I overcome this last gap in my solution? Thanks for your help.","['cantor-set', 'real-analysis', 'sumset', 'elementary-set-theory']"
1945360,"How to prove from scratch that there exists $q^2\in(n,n+1)$?","Given $n$ a positive integer, how would you prove from scratch that there exists a rational number $q$ such that $n<q^2<n+1$? By ""from scratch"" I mean by not using any ""advanced"" tools like the density of the rational numbers in the real numbers. Just using the definition of rational numbers, how to prove that? I faced this problem while trying to verify that the Dedekind cut $(A,B)$ cannot be determined by a rational number, where: $B=\{x \in Q^+: x^2>2\}$ $A=Q\setminus B$ where $Q^+$ denotes the positive rationals. So, for the purposes of the problem, I still don't even know what the real numbers are.","['real-analysis', 'rational-numbers', 'elementary-number-theory']"
1945383,3-manifold examples of homomorphisms between fundamental groups that are not induced by continuous maps,"I am looking for some examples of closed, orientable 3-manifolds $M$ and $N$ and a homomorphism $\phi : \pi_1(M) \to \pi_1(N)$ such that $\phi$ is not induced by any continuous map $f : M \to N$.  Are there examples of this occurring for lens spaces?  I do not believe that this phenomenon can occur for maps between surfaces.","['algebraic-topology', 'geometric-topology', 'group-theory', 'geometric-group-theory']"
1945404,Inductive proof for $\binom{2n}{n}=\sum\limits_{k=0}^n\binom{n}{k}^2$,"I want to prove the following identity using induction (not double counting method). Although it is a specific version of Vandermonde's identity and its inductive proof is presented here , but I need a direct inductive proof on this, not the general form. $$\binom{2n}{n}=\sum\limits_{k=0}^n\binom{n}{k}^2$$ I have tried to simplify $\binom{2n+2}{n+1}$ using Pascal's theorem, but did not get any result. Any help?","['combinatorics', 'summation', 'binomial-coefficients', 'induction']"
1945439,New discovery of the unconventional matrix representation for the quaternion $H_8$,"Let us start with the notation for the quaternion group  of order 8 as $H_8=\{e,i,j,k,g,i^{-1},j^{-1},k^{-1}\}$ where $ij=k=j^{-1}i$, $i^2=j^2=k^2=g$ and $i^4=j^4=k^4=g^2=e$. 
One way to write the 4-by-4 matrix representation of the quaternion as $H_8$ is that $e=\begin{bmatrix}
 1 & 0 & 0 & 0 \\ 
 0 & 1 & 0 & 0 \\
 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & 1 
\end{bmatrix}, 
i=\begin{bmatrix}
 0 & -1 & 0 & 0 \\ 
 1 & 0 & 0 & 0 \\
 0 & 0 & 0 & -1 \\
 0 & 0 & 1 & 0 
\end{bmatrix},
j=\begin{bmatrix}
 0 & 0 & -1 & 0 \\ 
 0 & 0 & 0 & 1 \\
 1 & 0 & 0 & 0 \\
 0 & -1 & 0 & 0 
\end{bmatrix},
k=\begin{bmatrix}
 0 & 0 & 0 & -1 \\ 
 0 & 0 & -1 & 0 \\
 0 & 1 & 0 & 0 \\
 1 & 0 & 0 & 0 
\end{bmatrix}.$
$$g=i^2=j^2=k^2=-\begin{bmatrix}
 1 & 0 & 0 & 0 \\ 
 0 & 1 & 0 & 0 \\
 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & 1 
\end{bmatrix}=-e.$$
And we notice that the matrices of $i,j,k$ are skew symmetric. See for example the Ref: Matrix representation of quaternions
Farebrother-Groß-Troschke and please check the Wikipedia . Now let us consider the new notation for the quaternion group  of order 8 as $H_8=\{e,i',j',k',g',i'^{-1},j'^{-1},k'^{-1}\}$. My question is that can we choose the non-skew symmetric matrix representation such that we design and fix the new $$k'=\begin{bmatrix}
 0 & 0 & 0 & -1 \\ 
 0 & 0 & 1 & 0 \\
 0 & 1 & 0 & 0 \\
 1 & 0 & 0 & 0 
\end{bmatrix}$$ to be rather unconventional, and notice that the $g'=k'^2$ has a rather unconventional definition
  $$g'=k'^2=
\begin{bmatrix}
 -1 & 0 & 0 & 0 \\ 
 0 & 1 & 0 & 0 \\
 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & -1 
\end{bmatrix},$$ but the identity $$e=(g')^2=i'^4=j'^4=k'^4=\begin{bmatrix}
 1 & 0 & 0 & 0 \\ 
 0 & 1 & 0 & 0 \\
 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & 1 
\end{bmatrix}$$ is still the conventional identity matrix $e$. The questions are that does there exist such non-skew symmetric matrix representation of 4-by-4? For the real matrix or the complex matrix?  Given that we restrict the $k'=\begin{bmatrix}
 0 & 0 & 0 & -1 \\ 
 0 & 0 & 1 & 0 \\
 0 & 1 & 0 & 0 \\
 1 & 0 & 0 & 0 
\end{bmatrix}$, what will be the matrix representation form of $i',j'$? (The real matrix or the complex matrix?)","['finite-groups', 'abstract-algebra', 'noncommutative-algebra', 'representation-theory', 'group-theory']"
1945445,"What are the conditions for a compact, convex set to be homeomorphic to the closed unit ball in the plane?","A generalization of Brower's fixed point theorem says that any continuous map from compact, convex set in the plane $K$ to itself, $f:K \to K$, must have a fixed point. It is easy to see that any set homeomorphic to the closed unit disk must have a Brower fixed point theorem. However, the above generalization includes a wider array of allowed sets than just those homeomorphic to the closed unit disk. For instance, it includes the set $\{0\} \times [0,1]$. In some sense, that is not surprising as that just reduces to a lower dimensional case. But this makes me wonder, what additional sets are being added? Is it just lower dimensional cases? In other words, what are the conditions for a compact, convex set to be homeomorphic to the closed unit ball? Purely that it is 2 dimensional?","['fixed-point-theorems', 'general-topology']"
1945447,Poincaré recurrence but infinite measure,"I'm trying prove the following problem: Let $f : M → M$ be an invertible transformation and suppose that $µ$ is an invariant infinite measure. Let $B ⊂ M$ be a set with finite measure. Prove that, given any measurable set $A ⊂ M$ with positive measure, $µ$-almost every point $p ∈ A$ either returns to $A$ an infinite number of times or has only a finite number of iterates in $B$. Obs1: µ infinite measure means µ(M)=$\infty$; Obs2: f is a function preserving µ; Obs3: $f^{-1}$ isn't requered be measurable. Obs4: $µ(B)<\infty$, $µ(A)>0$ Obs5: $n$ iterates means $f^{n}(p)$ for a point $p$ What I tried:
Separate the set A in 9 parts, combining iterates that never return to B, return a finite times to B, and never return to B with the analogous to A, where the iterates are done over points of the set A. But I didn't get success in prove that the appropriate intersections have null measures. I hope any suggestions to proceed ahead, thank you!","['ergodic-theory', 'measure-theory']"
1945448,Methods for Finding Raw Moments of the Normal Distribution,"I'm having some trouble with finding raw moments for the normal distribution. Right now I am trying to find the 4th raw moment on my own. So far, I know of two methods: I can take the 4th derivative of the moment generating function for the normal distribution and evaluate it at 0. I can use the fact that $E(x^4)$ is an expectation of a function of x to write  $$E({X}^{4})=\int_{Sx}^{} {x}^{4} f(x) dx=\int_{-\infty}^{\infty} {x}^{4}\frac{{e}^{\frac{{(x-\mu )}^{2}}{2{\sigma }^{2}}}}{\sqrt{2\pi }\sigma } dx$$ I'm wondering if there's a 3rd method. We haven't covered integrating the normal pdf in class, and taking the 4th derivative of ${e}^{\frac{{t}^{2}{\sigma }^{2}}{2}+t\mu }$ seems really messy/inelegant, so I'm wondering if there is some conceptual piece about moment generating functions I am missing. Thanks in advance!","['probability', 'moment-generating-functions', 'normal-distribution']"
1945498,"Given subsequences converge, prove that the sequence converges.","I have looked through previous posts but have been struggling with this problem. The sequence is {$a_n$} and its subsequences {$a_{2k}$}, {$a_{2k+1}$}, {$a_{3k}$} converge. I have to prove that {$a_n$} converges. I know that a sequence converges if all of its subsequences converge. I'm suspecting that I have to prove that every subsequence belongs into these 3 subsequences. Thank you for your time and help.",['real-analysis']
1945511,Simple calcul of a limit superior,"What is the limit superior of :
$$
\limsup_{x\rightarrow 10} 1*\mathbb{I}_{x<10}(x)+2*\mathbb{I}_{x=10}(x)+\frac{1}{2}*\mathbb{I}_{x>10}(x) ?
$$
If I take the definition of a limit sup I arrive to 2, but I have a little doubt on my procedure.","['limsup-and-liminf', 'analysis']"
1945523,Showing that $\lim_{x \to 1} \left(\frac{23}{1-x^{23}}-\frac{11}{1-x^{11}} \right)=6$,"How does one evaluate the following limit?
$$\lim_{x \to 1} \left(\frac{23}{1-x^{23}}-\frac{11}{1-x^{11}} \right)$$
The answer is $6$. How does one justify this answer? Edit: So it really was just combine the fraction and use L'hopital's rule twice (because function and its first derivative are of indeterminate form at $x=1$). This problem is more straightforward than it seems at first.",['limits']
1945535,A question about straight lines that bisect the area of a triangle,"Let $K$ be a convex subset of the Euclidean plane $E(2)$ whose boundary is a triangle. Is it true that there cannot exist 4 pairwise distinct concurrent straight lines in $E(2)$, each of which bisects the area of $K$? Many results in the literature strongly suggest that this is true, but I have not so far been able to find a theorem that actually states or implies that it is.",['geometry']
1945550,"Explain why $E[X_1|X_1+X_2] = E[X_2|X_1+X_2]$ if $X_1$, $X_2$ are i.i.d.","I want to prove that if $X_1,X_2$ are i.i.d. random variables then $E[X_1| X_1+X_2] = E[X_2|X_1+X_2]$. I see that this is intuitive but I think it is by no means trivial yet everybody just states this property as though it was completely obvious. Do I really miss something obvious here? In my attempt to prove this I didn't get so far. It suffices to show $E[1_AX_1] = E[1_AX_2]$ for $A=\{X_1+X_2\in B\}$ where $B$ is any Borel set. I don't see how this is trivial and would appreciate help to let me see how it works.","['probability-theory', 'conditional-expectation']"
1945577,Diagonal entries of inverse larger than 1 over those for the original matrix,"A friend asked me if, given $A \in \mathcal{S}^n_{++}$ (positive definite), is it true $\forall$ j=1,...,n that
$$(A^{-1})_{jj} \geq \frac{1}{A_{jj}}$$
Not sure if it's true but we haven't found counterexamples yet. Attempt (assuming true): Let $A_{-ij} \in \mathbb{R}^{n-1 \times n-1}$ be A w/ the ith row and jth column removed.  It's equivalent to show $A_{jj}(A^{-1})_{jj} \geq 1$. Using this , it's equivalent to show
$$\frac{A_{jj} det(A_{-jj})}{\sum_{i=1}^n (-1)^{i+j}A_{ij}det(A_{-ij})} \geq 1$$
Note the numerator and denominator both have terms $A_{jj} det(A_{-jj})$ so it's equivalent to show $\sum_{i \neq j}(-1)^{i+j}A_{ij}det(A_{-ij}) \leq 0$, and here is where I'm stuck. I'm not sure if the attempt is the best approach, so alternate proposals are also much appreciated.","['matrices', 'linear-algebra']"
1945581,Efficiently computing Schur complement,"I would like to compute the Schur complement $A-B^TC^{-1}B$ , where $C = I+VV^T$ (diagonal plus low rank). The matrix $A$ has $10^3$ rows/columns, while $C$ has $10^6$ rows/columns. The Woodbury formula yields the following expression for the Schur complement: $$A-B^TB+B^TV(I+V^TV)^{-1}V^TB.$$ This expression can be evaluated without storing large matrices, but the result suffers from numerical inaccuracies. Is there a numerically more stable way of computing the Schur complement, without high memory requirements?","['matrices', 'numerical-linear-algebra', 'linear-algebra', 'schur-complement']"
1945612,Exercise about limit.,"Let be $g(t)=t\ln t$, $t>0$. How to show that $\displaystyle\lim_{t\rightarrow \infty} \frac{g^{-1}(t)}{t} = 0$ ? I'm trying to do using the L'Hospital rule, but I can not justify that $\displaystyle\lim_{t\rightarrow \infty}g^{-1}(t)=\infty$.","['derivatives', 'continuity', 'limits']"
1945619,PS for $4u_{n+1}-u_n = 5\cdot4^{-n}$?,How do you find particular solutions for: $$4u_{n+1}-u_n = 5\cdot4^{-n} \text{ ?}$$ Thanks for your time in advance.,"['recurrence-relations', 'discrete-mathematics']"
1945637,Double Integral Math Olympiad Problem,"I was taking a Math Olympiad test and one of the questions was to calculate the following double integral:
$$\int_0^\infty\int_0^\infty\frac{\log|(x+y)(1-xy)|}{(1+x^2)(1+y^2)}\ \mathrm{d}x\ \mathrm{d}y$$
Here, as usual, $\log a$ and $|a|$ are the natural logarithm and absolute value of $a$ respectively. I'm guessing that you're not supposed to solve it analytically, but rather find some symmetry argument or clever simplification that would make it straightforward. Since I don't even know where to start, any help is welcome. In case you want to know, this was taken from the 2016 Rio de Janeiro State Math Olympiad, known in Portuguese as OMERJ.","['multivariable-calculus', 'contest-math']"
1945664,Why I can't divide by y in this equation: 4y = y? [duplicate],"This question already has answers here : Why should you never divide both sides by a variable when solving an equation? (5 answers) Closed 7 years ago . I have this equation $$
  4y = y
$$ If I divide by y in both sides I would get this:
$$ 4 = 1$$ And this does not have sense. I know that the solution is 0 but why I get this answer when dividing by y . What's the logic behind?",['algebra-precalculus']
1945722,Proof of Kurtosis for a sum of independent random Variables,"I am trying to understand a proof for the Kurtosis of a sum of independent random variables, however, there is one part where I am quite stuck: Theorem: for $X_1, X_2, ..., X_n$ independent random variables with means $ \mu_1, ... , \mu_n$ and variance $\sigma_1^2, ... , \sigma_n^2 $  $E(X_i^4) < \infty$ Define $S_n = X_1 + ... + X_n$ and $S_n$ will be appropriately normal $ kurt(S_n) - 3 = (\sum_{i=1}^n\sigma_i^n)^{-2}\sum_{i=1}^n\sigma_i^4(kurt(X_i)-3)$ Proof (first part): assume WLOG that $E[X_i] = 0$ for all $i$ $ kurt(Sn) = \frac{E[(X_1 + ... + X_n )^4]}{(\sigma_1^2 + ... +\sigma_n^2)^2 } $ $ E[(X_1 + ... + X_n )^4] = \sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^n\sum_{l=1}^n E(X_iX_jX_kX_l) = \sum_{i=1}^n E(X_i^4) +6  \sum_{i<j}^n\sigma_i^2\sigma_j^2$ NOTE: $E(X_iX_jX_kX_l) = 0 $ unless $ i=j=k=l $ or if combinations of two pairs. Question: why is the NOTE: true? From my understanding the expected value of INDEPENDENT random variables is equal to the product of the expected values of the random variables. However I intuitively understand this case does not apply, or else, we would not get very far! So what is going on? Assuming this hickup true, i understand the rest of the proof, which I will not write. (as i am improvising the syntax)","['expectation', 'independence', 'statistics', 'proof-explanation', 'random-variables']"
1945754,Show that the numerator of $1+\frac12 +\frac13 +\cdots +\frac1{96}$ is divisible by $97$,"Let $\frac{x}{y}=1+\frac12 +\frac13 +\cdots +\frac1{96}$ where $\text{gcd}(x,y)=1$. Show that $97\;|\;x$. I try adding these together, but seems very long boring and don't think it is the right way to solving. Sorry for bad english.","['divisibility', 'algebra-precalculus', 'rational-numbers', 'summation', 'elementary-number-theory']"
1945768,Prove that the Dihedral group $D_n$ is isomorphic to $Z_n \rtimes_{\psi} Z_2$,"I consider the following map $\psi : Z_2 \rightarrow Aut(Z_n)$ where we map 
the identity element 0 to the identity map and $1 \mapsto \theta : Z_n \rightarrow Z_n$ where $\theta(x) = -x$. I am not sure how to proceed further. Any help would be nice. $D_n = ⟨ r, s : r^n = e = s^2, r^js = sr^{-j} ⟩ $","['abstract-algebra', 'semidirect-product', 'group-theory']"
1945789,topological structure on smooth manifolds,"In John Lee's Introduction to Smooth Manifolds , a smooth manifold is defined as a topological manifold with a smooth structure. In do Carmo's Riemannian Geometry , a differentiable (smooth) manifold is defined by giving the smooth structure on merely a set $M$ and the author makes a remark that such smooth structure induces a natural topology on $M$. Here is my question : In Lee's definition, what is the relation between the topological structure and the smooth structure? Must the topology of the manifold (in Lee's definition) induced by the smooth structure in the way that do Carmo mentions? The following are the definition and the remark by do Carmo mentioned above.","['manifolds', 'differential-geometry']"
1945848,Two-Term Exponential Curve Fitting,"I want to fit a pair of experimental data $(M_z(t), t),$ which can theoretically be modelled according to:$$M_z(t)=M_z(0) e^{-t/T_1} + M_0 (1-e^{-t/T_1}) \tag{1}.$$I want to use the equation of the fitting to solve for $T_1.$ The parameters $M_0,$ and $M_z$ are defined in the diagram below: What kind of fitting should be used here? Attempt: I have tried a two-term exponential fitting in Matlab, it is a good fit but I can't see how to deduce $T_1$ from the resulting equation: f(x) = a*exp(b*x) + c*exp(d*x)

   a =   2.642e+04  (2.623e+04, 2.662e+04)
   b =   1.347e-06  (-2.953e-05, 3.222e-05)
   c =   -2.45e+04  (-2.478e+04, -2.423e+04)
   d =    -0.07508  (-0.07726, -0.0729) Which exponent ($d$ or $b$) should be used for finding $T_1$? This was my experimental data: x=[2    5   10  20  30  50  100 200 400];
y=[5418.583 9479.431    14828.01    21052.6 23872.96    25784.02    26420.23    26445.85    26433.05]; And the resulting exponential fit: P.S. I can't do a log plot fitting because I don't know the value of $M_0.$","['regression', 'exponential-function', 'statistics', 'estimation', 'matlab']"
1945852,Motivations for the axioms of Galois category,"We know that any Galois category is equivalent to the category of  finite $\pi$-sets where $\pi$ is a unique profinite group. I think axioms for a Galois category were choosen so that the above theorem becomes true. But how would someone guess exactly axioms will give us the correct answer? Is there some kind of motivation behind choosing each axiom? SGA I, Le groupe fondamental : généralités. Exposé V .","['category-theory', 'galois-theory', 'algebraic-geometry']"
1945863,"A set of infinite sequences of only 0s and 1s with a metric, their convergence and the topology they induce?","The entire question is this: Let $S$ be the set of sequences of 0s and 1s. For $x = \{x_1, x_2, x_3,...\}$ and $y = \{y_1, y_2, y_3, ...\}$ define $$ d(x,y) = \sum_{j=1}^{\infty} \frac{|x_j -yj|}{2^j} $$ Explain why the infinite sum in the definition converges for all $x$ and $y$. Prove that $d(x,y)$ is a metric. Let $E$ be the subset of $S$ consisting of all sequences that are eventually 0. Thus, $x = (x_1, x_2, x_3...)$ is  in $E$ if there exists $N \geq 0$ such that $x_n = 0$ for all $n \geq N$. Prove that $E$ is dense in $S$ under the topology induced by $d$. My first instinct with the metric infinite sum convergence is that of course it converges because either x=y and then it's a sum of infinite 0s, converging to 0, or if x $\neq$ y then we can define a subsequence that is a subsequence of the geometric sequence that converges to 1, so it converges to 1. I also want this to be true because then it induces the discrete topology which is really easy to work in. But I'm not sure if the convergence is true? Because I was also thinking that we could have an x where each $x_i$ = 0 and $y = \{1, 0, 0, 0...\}$. Then this would converge to 1/2. So how does this thing converge? Am I overthinking it or underthinking it? I'm stronger with topology than analysis but if it's not the discrete topology I'm not sure where to go.","['general-topology', 'metric-spaces', 'convergence-divergence', 'analysis']"
1945866,How is the Jacobian the derivative if they have different dimensions,"I understand the proof that the Jacobian matrix is the best linear approximation of a function in the limit. But since it's a matrix it has different dimensions than the original function, shouldn'the it be a vector too?
Given the Jacobian how would I find or plot where it takes some given 'input'? Since the derivative is a function I suppose one could transform it into something like f'(x1,.....,×n)=(y1,.....,yn) I tried to be as clear as possible but I don't think I put it well into words. EDIT: Clarifiyng, my question is how do I find, explicitly, an arbitray element of image of the derivative. Does it need two vectors/points to be defined? If so, why not only one point as in single variable calculus?","['multivariable-calculus', 'matrix-calculus', 'derivatives']"
1945871,Prove that $n! + k$ is a composite number,"Recall that if $n$ is a positive integer, $n! = n(n-1)\cdots 3\cdot2\cdot1$. a) Let $n$, $k$ be positive integers with $1< k \le n$. Prove that $n! + k$ is composite. b) Using part a, find $100$ consecutive integers all of which are composite. c) In part b you gave a sequence of $100$ consecutive integers all of which are composite. You should be able to use the same idea to find $\ell$ consecutive integers which are composite for any positive integer $\ell$. In other words, you can find sequences of any length which consist of consecutive integers and contain no primes. Does this contradict the fact that there are an infinite number of prime numbers?",['discrete-mathematics']
1945891,(simple ?) proof of Cauchy integral theorem and formula,"Question :  Can you tell if this  proof of the Cauchy integral theorem/formula is correct, and if it is, is there a good reason for not using it in complex analysis courses ? Let $\gamma$ be a closed rectifiable piecewise $C^1$ contour parametrized by $t \in [a,b]$, and consider the dilatation of the contour around a point : $\gamma_{r,s}(t) = s + r(\gamma(t)-s)$.
Let
$$g(r) = \frac{1}{r}\int_{\textstyle \gamma_{r,0}} f(z) dz = \int_a^b f(r \gamma(t))\gamma'(t)dt$$
If $f(z)$ is continuously complex differentiable on $U$ a convex open containing $\gamma$  and the origin, then
$$r g'(r) = \int_a^b \gamma(t) f'(r \gamma(t)) r\gamma'(t)dt = \gamma(t) f(r\gamma(t))|_a^b - \int_a^b \gamma'(t) f(r \gamma(t))dt = -g(r)$$ so with $h(r) = r\ g(r)$ we have $h'(r) = g(r) + r g'(r) = 0$ 
$$ \implies  \int_\gamma f(z) dz = h(1) = \lim_{r \to 0} h(r) = \lim_{r \to 0}r\int_a^b f(r \gamma(t)\gamma'(t)dt = 0$$ If $f(z)$ is non-continuously complex differentiable, then as before we have $h'(r) = 0$. Letting $u = f(r\gamma(t)), v = \gamma(t)$ :
$$r \frac{\partial}{\partial r} [u \frac{\partial}{\partial t}v] = v \frac{\partial}{\partial t}u =  \frac{\partial}{\partial t}[uv]-u\frac{\partial}{\partial t}v $$
Also note that $\int_a^b \frac{\partial}{\partial t}[uv] dt = uv|_a^b = 0$, so that
$$rg'(r) = \int_a^b r \frac{\partial}{\partial r} [u \frac{\partial}{\partial t}v]dt = \int_a^b \left(\frac{\partial}{\partial t}[uv]-u\frac{\partial}{\partial t}v\right)dt$$ $$ = uv|_a^b - \int_a^b f(r\gamma(t))\gamma'(t)dt= -g(r)$$
and we have the Cauchy integral theorem for convex regions containing the origin. Corollaries : Shifting what was done before, we have that if $f(z)$ is holomorphic on any convex region $U \ni s$ then $\int_\gamma f(z) dz = \lim_{ r \to 0} \int_{\gamma_{r,s}} f(z) dz =0$ if $f(z)$ is holomorphic on a simply connected bounded open $U$ containing a closed rectifiable piecewise $C^1$ contour $\gamma$, then $U$ can be decomposed as an union of convex sub-regions $U_k$, and $\gamma = {\bigcup^{\bigoplus}}_{k=1}^K \gamma_k$   where each $\gamma_k \in U_k$ is a closed contour  and the symbol $\bigcup^{+}$ means that the union of two curves traversed in opposite direction cancel. Hence we have $$\int_\gamma f(z) dz = \sum_{k=1}^K \int_{\gamma_k} f(z) dz = \sum_{k=1}^K 0 = 0$$ if homotopically $\gamma \cong \gamma'$ on $U$ where $f(z)$ is holomorphic then $\gamma \cup \gamma'_{\ominus}$ ($\ominus$ means traversed in opposite direction) can be completed to yield $\lambda = \gamma \cup^+ \delta \cup^+ \gamma'_{\ominus} \cup^+ \delta_{\ominus}$ a closed contour supported on a simply connected region, so that $\int_\lambda f(z)dz = \int_\gamma f(z) dz +  \int_{\gamma'} f(z) dz = 0$ With $s \in U$ simply connected, by definition $\eta(\gamma,s)$ is the number of small circles $|z-s| = \epsilon$ to which a closed contour $\gamma \subset U$ is homotopically equivalent on $U \setminus \{s\}$, so that  : $$\int_\gamma \frac{f(z)}{z-s} dz  = \lim_{ r \to 0} \int_{\gamma}\frac{f(z)}{z-s} dz= \eta(\gamma,s) \int_{|z-s| = \epsilon}\frac{f(z)}{z-s} dz$$ 
$$ = \eta(\gamma,s)\ f(s) \lim_{\epsilon\to 0}\int_{|z-s| = \epsilon}\frac{1}{z-s} dz  = 2i \pi \ \eta(\gamma,s) \ f(s)$$","['cauchy-integral-formula', 'complex-analysis', 'proof-verification']"
1945899,Solutions to $x^2 = x + 1$ are irrational,"I'm trying to prove that all the solutions to the equation $x^2 = x + 1$ are irrational. This statement is equivalent to: If $x^2 = x + 1$, then $x$ is irrational. I want to prove this using contraposition. The contrapositive statement is: If $x$ is rational, then $x^2 \neq x + 1$. A rational number is one that can be described as a ratio. So, $r = \frac{k_1}{k_2}$, where $k_1$ and $k_2$ are integers and $k_2 \neq 0$. Let $x$ be a rational number. Using the above definition of a rational number we simplify $x^2 = x + 1$ to $(\frac{k_1}{k_2})^2 = \frac{k_1}{k_2} + 1$, where $k_1$ and $k_2$ are integers and $k_2 \neq 0$. All is left is to show that this equation cannot be satisfied for any combination of $k_1$ and $k_2$ (considering they are integers). How can we do this? Can someone point me in the right direction?","['algebra-precalculus', 'rationality-testing', 'golden-ratio', 'quadratics']"
1945900,Antiderivative of $\log(x)$ without Parts,"I understand how the antiderivative  of $\log(x)$ can be obtained by Integration by Parts (i.e. product rule), but I was wondering how-if at all- it could be obtained only using sum/difference rule and substitution/chain rule.","['logarithms', 'real-analysis', 'integration', 'calculus']"
1945935,Does $\sum\sqrt{n\arctan(1/n^3)}$ converge?,"Does the following series converge? Why? $$\sum\sqrt{n\arctan(1/n^3)}$$ The only way that comes to me is to look at the series: $$\arctan(x)=\sum\frac{(-1)^k}{2k+1}x^{2k+1}\Longrightarrow \arctan\left(\frac{1}{n^3}\right)=\sum\frac{(-1)^k}{2k+1}n^{-6k-3}$$ As a result (and if I got the computation right), we have: $$n\arctan(1/n^3)=\sum\frac{(-1)^k}{2k+1}n^{-6k-2}\approx \frac{1}{n^2}-\frac{1}{3n^8}+\cdots$$ Thus, by taking square root, the leading term ""becomes"" $\frac{1}{n}$ , so the series diverges. I know the answer is ""diverge"" but my argument (at least the last part?) is far from being rigorous, if not wrong. Any thought/suggestion/better ways of doing this?",['sequences-and-series']
1945955,"Does $K[\alpha_1, ..., \alpha_n]=K(\alpha_1, ..., \alpha_n)$ imply $\alpha_1, ..., \alpha_n$ are algebraic over $K$?","I know how to prove that if $\alpha_1, ..., \alpha_n$ are algebraic over $K$ , then $K[\alpha_1, ..., \alpha_n]$ is a field (i.e., $K[\alpha_1, ..., \alpha_n]=K(\alpha_1, ..., \alpha_n)$ ). I also know the converse is true for $n=1$ and also know how to prove it. However, I'm having real trouble to deal with the case $n\geq 2$ . I've tried to use the same strategy with $n=1$ , which envolves the surjective homomorphism $\psi:K[X]\to K[\alpha]$ with $F \mapsto F(\alpha)$ and the fact that $K[X]$ is a principal ideal domain. But that doesn't work with a similar map $\psi:K[X_1, ..., X_n]\to K[\alpha_1, ..., \alpha_n]$ since $K[X_1, ..., X_n]$ is not a principal domain. I couldn't disprove it either. I tried to find a small example with $K=\mathbb{R}$ and $n=2$ , but it also couldn't figure it out. Any ideas? Thanks!","['abstract-algebra', 'field-theory']"
1946007,"Prove $T$ is surjective if $(Tx,x)\geq k(x,x)$ in Hilbert space","Suppose $H$ is a Hilbert space, and $T$ is a continuous operator satisfies 
$$(Tx,x)\geq k(x,x), k>0,x\in H$$
How to prove that $T$ is onto?",['functional-analysis']
1946034,What time are the minute and hour hands of a clock perpendicular?,"At noon the minute and hour hands of a clock coincide. Assuming the hands of the clock move continuously: a) What is the first time $T_1$ when they are perpendicular? b) What is the next time $T_2$ when they again coincide? I believe I have solved this problem, but I would like someone to verify my answer. I got $T_1$ to be 12:16 P.M. and 21.818181... seconds ($\frac{180}{11}$ minutes after noon). I got $T_2$ to be 1:05 P.M. and 27.272727... seconds ($\frac{720}{11}$ minutes after noon). If there are any questions relating to this problem, please feel free to ask them, as I will do my best to answer them.","['algebra-precalculus', 'angle']"
