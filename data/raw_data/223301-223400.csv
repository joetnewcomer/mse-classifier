question_id,title,body,tags
4588551,What kind of function represents the Sophomore's Dream sum?,"I came across the following sum while studying the integral of $x^x$ : $$\int_0^1 x^x \,\text{d}x = \sum_{n=1}^\infty \frac{(-1)^{n-1}}{n^n} = 0.783430510712... $$ When I saw this, my first instinct was to think of the Dirichlet Eta Function : $$\eta({s}) = \sum_{n=1}^\infty \frac{(-1)^{n-1}}{n^s}$$ However, the term in the exponent of $n$ is, itself, changing with the sum, so this can't be it. I tried looking at other similar zeta functions , but most of them had the same issue. Is there a defined function that represents the former sum? I couldn't find one through my own research. Edit: (1) I now know that the official name for this sum is the Sophomore's Dream . However, I am still wondering if there is a way to represent this function using a zeta function, or something else that is similar.
This question's previous title was ""Is this sum a variation of a zeta function, or something else?"" (2) Another similar function might be the series expansion of the principal Lambert W Function : $$W_0(x) = \sum_{n=1}^\infty \frac{(-n)^{n-1}}{n!}, |x|<\frac1e$$","['summation', 'functions']"
4588561,Set of points at distance 1 has Lebesgue measure 0.,"This post shows that if $K$ is a compact subset of $\mathbb R^n$ , then the set of points in $\mathbb R^n$ which are at a distance $1$ from $K$ have Lebesgue measure 0. Does the same conclusion not follow if $K$ is replaced by any subset of $\mathbb R^n$ ? Here is my (possibly flawed) argument. Let use take $n=2$ for simplicity.
Let $S$ be an arbitrary subset of $\mathbb R^2$ and $A = \{x\in \mathbb R^2:\ d(x, S) = 1\}$ .
We want to show that $A$ has Lebesgue measure $0$ .
Let $I^2 = \{x\in \mathbb R^2:\ 0\leq x_1, x_2\leq 1\}$ .
For any $v\in \mathbb Z^n$ let $Q_v$ be the unit square defined as $Q_v = \{v+x:\ x\in  I^2\}$ . Thus $Q_0 = I^2$ . Let $S_v = Q_v\cap S$ for any $v\in \mathbb Z^2$ and let $A_v$ be the set of points in $\mathbb R^2$ which are at a distance $1$ from $S_v$ . Note that each $A_v$ has Lebesgue measure $0$ since $d(x, S_v) = d(x, \bar S_v)$ for any $x\in \mathbb R^2$ , and $\bar S_v$ is compact.
So we will be done if we can show that $A \subseteq \bigcup_{v\in \mathbb Z^2} A_v$ . Indeed, let $x\in A$ be arbitrary and assume that $x\notin A_v$ for any $v$ . Clearly, if $v$ is far away from $x$ then $d(x, S_v) > 2$ . Thus, it follows that there is $\epsilon>0$ such that $d(x, S_v) > 1 + \epsilon$ for all $v$ . But then $x$ cannot lie in $A$ , a contradiction, and we are done. I may be consistently overlooking a simple mistake.","['general-topology', 'solution-verification', 'measure-theory']"
4588661,What is the stationary distribution of this Markov chain?,"A Markov chain is shown in the figure above. I am writing the transition matrix as: $$P =	\begin{bmatrix}
0.6 & 0.4 & 0 & 0\\
0 & 0 & 0.5 & 0.5\\
0 & 0 & 0.25 & 0.75\\
0.7 & 0.3 & 0 & 0
\end{bmatrix}$$ I using given formula to find stationary distribution: $$AP=A$$ where $A=[w_1 \; w_2\; w_3\; w_4]$ is the stationary distribution. The answer for stationary distribution is given as: $$w_1=\frac{4}{12} \quad w_2=\frac{3}{12} \quad w_3=\frac{3}{12} \quad w_4=\frac{2}{12}$$ which I can easily check the formula $$AP=A$$ is not satisfying. Where am I going wrong?","['stochastic-processes', 'markov-chains', 'entropy', 'probability']"
4588669,Prove that there exists $c>0$ such that $f'(c) = 0$,"I would like some help on this problem I've been struggling to solve for a while. Let $f$ be a continuous function on $[0,+\infty)$ and differentiable on $(0,+\infty)$ such that $\lim \limits_{x \to +\infty}f(x) = f(0)$ .
Prove that there exists $c>0$ such that $f'(c) = 0$ . So this actually reminds me of Rolle's theorem but I can't use it since it has to be used on a closed interval. My intuition is that $f$ should be bounded and thus it changes its monotonicity which makes its derivative equal to 0 at a certain point. The problem is no matter how I tried I wasn't able to write a full rigorous proof so if you could help me on that I would greatly appreciate it!","['limits', 'calculus', 'derivatives', 'continuity']"
4588682,Calculating the volume of an elliptical region,"Problem : Calculate the volume of the finite body $K$ that is limited by the surfaces $$
z=2-x^2-y^2
\\z=y^2
$$ Answer : $\pi\sqrt2$ My Attempt : The surfaces intersect when $x^2+2y^2 = 2$ Therefore, the volume is: $$
\iint_{A}(2-x^2-y^2)-(y^2)=\iint_{A}2-x^2-2y^2
$$ where $A : x^2+2y^2 \le 2$ . The variable substitution $$
u = x
\\v = \sqrt2y
$$ transforms the ellipsis into a circle, and $\frac{d(x, y)}{d(u,v)} = \frac{1}{\sqrt2}$ . Consequently, the volume can be written as: $$
\iint_{B}(2-u^2-v^2)\frac{1}{\sqrt2}\,dudv
$$ where $B : u^2+v^2 \le 2$ Polar coordinates can now be used, and we finally have the volume: $$
\iint_{B}(2-u^2-v^2)\frac{1}{\sqrt2}\,dudv=\int_{0}^{2\pi}\int_{0}^{2}(2-r^2)\frac{r}{\sqrt2}drd\theta = 0
$$ which is clearly wrong! Where do I run off the rails?",['multivariable-calculus']
4588719,"Injective Lie Group Homomorphism, compact domain","Problem: Assume $F:G\rightarrow G$ is an injective Lie group homomorphism, where $G$ is finite dimensional. If $G$ is compact, then show that $F$ is surjective. Incomplete attempt: As $F$ is a Lie group homomorphism, it has constant rank. Hence, the differential $F_{*,e}:\mathfrak{g}\rightarrow \mathfrak{g}$ is an isomorphism. Hence by the Inverse function theorem, $F$ is a local diffeomorphism.
Since $G$ is compact, $F$ is closed. Since $F$ is a local diffeomorphism, $F$ is open.
Moreover, $F:G\rightarrow F(G)$ is a homeomorphism, and so $F(G_0)=G_0$ since $G_0$ is the only open connected subgroup of $G$ . Now why does it follow that $F$ is surjective?","['lie-algebras', 'lie-groups', 'differential-geometry']"
4588751,Is it always possible to cut out a piece of the square with $\frac{1}{5}$ of its area?,"Let there be a square that has $n+1$ notches on each edge (corners included)  to divide each edge into $n$ equal parts. We can make cuts on the square from notch to notch. Is it always possible to cut out a connected piece with area $\frac{1}{5}$ the area of the original square if $n≥2?$ It is possible for multiples of $2, 3,$ or $5,$ but I don't know any other numbers for which this is possible. If it is possible for $n,$ it is possible for any multiple of $n.$ $2$ : Make four cuts with slope $2$ and $-1/2$ and take the piece in the middle. $3$ : Make four cuts from a corner with slope $3$ and $-1/3$ and use a corner-to-corner cut to cut the center piece into two pieces, each with area $1/5.$ $5$ : It's obvious. Here's a link to the sequel.","['dissection', 'area', 'geometry']"
4588802,Roots of a set of nonlinear equations $ax + yz = b_1; ay + xz = b_2; az + xy = b_3$,"Let $a$ be a non-negative real number, $b_1, b_2, b_3$ be real numbers, and $x, y, z$ be variables. Is it possible to analytically find the root closest to origin $(0, 0, 0)$ of the set of nonlinear equations given by: $$\begin{cases}
ax + yz =  b_1\\
ay + xz =  b_2\\
az + xy =  b_3\;\;\; ?\end{cases}$$ Edit: This question is related to my research on quadrotor control. The roots of the set of nonlinear equations give the equilibrium points of the rigid body dynamics under a specific control input. I couldn't find any solution. I can only solve the problem when at least two of the $ b_1 $ , $ b_2 $ , and $ b_3 $ are zero.","['systems-of-equations', 'applications', 'polynomials', 'nonlinear-system', 'algebra-precalculus']"
4588867,Improper integrals with bounds 0 to infinity,"$$\int_{0}^{\infty }\frac{1}{x^{1/2}+x^{3/2}}dx$$ I managed to integrate correctly and get $2\arctan(\sqrt{x})+C$ , but I was wondering why my teacher split it up into two pieces, one from $0$ to $1$ with a limit at $0$ evaluating $2\arctan(\sqrt{x})+C$ and the other $1$ to $\infty$ with a limit at infinity evaluating $2\arctan(\sqrt{x})+C$ . Can I just take $2\arctan(\sqrt{x})$ and evaluate from $0$ to $\infty$ with a limit at infinity?","['integration', 'calculus']"
4588868,Show that scalene triangle $\triangle ABC$ is a right-triangle if $\sin(A)\cos(A)=\sin(B)\cos(B)$,"As the title suggests, this is a college entrance exam practice problem from Japan, it is as follows: Given a scalene triangle $\triangle ABC$ , prove that it is a right triangle if $\sin(A)\cos(A)=\sin(B)\cos(B)$ I found this problem pretty interesting, and after some thinking, I found a way to solve it, and I'll show my attempt here. I want to know, are there any other/better ways to solve this? Or is there anything about my solution that can be improved? Please let me know! Here's my attempt: Some have that: $$\sin(A)\cos(A)=\sin(B)\cos(B)$$ From Law of Sines we know that: $$\frac{a}{\sin(A)}=\frac{b}{\sin(B)}=\frac{c}{\sin(C)}=2R$$ And from Law of Cosines: $$\cos(B)=\frac{a^2+c^2-b^2}{2ac}$$ $$\cos(A)=\frac{b^2+c^2-a^2}{2bc}$$ Therefore: $$(\frac{a}{2R})(\frac{b^2+c^2-a^2}{2bc})=(\frac{b}{2R})(\frac{a^2+c^2-b^2}{2ac})$$ Dividing and multiplying by $a$ on the left side and by $b$ on the right side gives us: $$\frac{(a^2)(b^2+c^2-a^2)}{4Rabc}=\frac{(b^2)(a^2+c^2-b^2)}{4Rabc}$$ $$a^2b^2+a^2c^2-a^4=a^2b^2+b^2c^2-b^4$$ $$(a^2-b^2)c^2-(a^2-b^2)(a^2+b^2)=0$$ $$(a+b)(a-b)(-a^2-b^2+c^2)=0$$ Now, obviously the case $a=-b$ cannot be true. We also know that $a=b$ does not work in this particular case since this triangle is scalene, therefore we're left with: $$-a^2-b^2+c^2=0$$ $$a^2+b^2=c^2$$ And this proves that the triangle is right angled","['euclidean-geometry', 'geometry', 'solution-verification', 'triangles', 'trigonometry']"
4588873,Evaluate $\int_0^1\left(\frac{2x-1}{x^2-(1-i)x+\frac{1}{2}(1-i)}\right)^ndx$ for natural $n$,"I was trying to solve this integral the another integral came up: $$I_n=\int_0^1\left(\frac{2x-1}{x^2-(1-i)x+(1-i)/2}\right)^ndx$$ There seems to be a pattern in the results $$I_0=1$$ $$I_2=-2+\frac{\pi}{\sqrt2}$$ $$I_4=\frac{4}{3}-\frac{3\pi}{2\sqrt2}$$ $$I_6=\frac{8}{5}-\frac{9\pi}{8\sqrt2}$$ $$I_8=\frac{-16}{7}+\frac{61\pi}{16\sqrt3}$$ The odd inputs seem to be rational multiples of $\frac{\pi i}{\sqrt2}$ . The difference in the odd and even inputs makes me think they should be evaluated separately. Differentiating with respect to n seemed to make it worse. $$I'(n)=\int_0^1\left(\frac{2x-1}{x^2-(1-i)x+(1-i)/2}\right)^n\ln\left(\frac{2x-1}{x^2-(1-i)x+(1-i)/2}\right)dx$$ I tried integration by parts with $u=(\frac{2x-1}{x^2-(1-i)x+(1-i)/2})^{n-1}$ and $dv=\frac{2x-1}{x^2-(1-i)x+(1-i)/2}$ but it seemed to make the integrand so much messier. Edit: We can substitute $u=\frac{2x-1}{x^2-(1-i)x+(1-i)/2}$ , and the problem reduces to finding $$J_n=\int_{-1-i}^{-1+i}\frac{x^n}{\sqrt{-2x^2-4ix+4}}dx$$ For which I asked another question for","['integration', 'definite-integrals']"
4588899,How do you calculate average circulation across a möbius band?,"The surface is given by the parameterization shown in the picture. We are to find the average circulation across the mobius band with the circulation vector <-y,x,0> using stokes theorem if necessary. here's a picture of the problem","['multivariable-calculus', 'calculus', 'stokes-theorem', 'mobius-band']"
4588912,Use the formal definition of a limit for sequences to prove the limit of a sequence . Where Did I Go Wrong?,"The formal definition of limit of a sequence says that  for every $\epsilon>0$ , there is a number $N>0$ such that if $n > N$ it is true that $|(a_n) - L| < 	ε$ In this particular case I have to demonstrate by the definition that this limit exist: $$\lim \limits_{n \to \infty}\frac{(1-4n-7n^2)}{1+2n+n^2}=-7$$ This is what I did : $\|\frac{1-4n-7n^2}{1+2n+n^2} + 7\|$ = $\|\frac{10n+8}{(n+1)^2}\|$ In the definition, we have that $N > 0$ , and $n>N$ , this implies that $$\|\frac{10n+8}{(n+1)^2}\| = \frac{10n+8}{(n+1)^2}$$ I then used the fact that : $\frac{10n+8}{(n+1)^2}$ < $10n+8$ $10n+8 < ε$ if and only if $n < \frac{ε - 8}{10}$ I dont know what to do after that. Also, I don't think that makes sense, In these types of proofs, my teacher always had n > in the end and not vice versa. Also, it doesn't make sense that n needs to be very small since we are observing to what value the sequence converges to when n is VERY LARGE  I don't know where I went wrong and how to do it otherwise.","['limits', 'solution-verification', 'proof-explanation', 'epsilon-delta']"
4588924,"Suppose $X_n /a $ converges to standard normal in distribution. How to say normally that $X_n$ converges to normal $n(0, a^2)$?","Suppose $X_n /a $ converges to standard normal in distribution. How to say formally that $X_n$ converges to normal $n(0, a^2)$ in distribution? Slutsky's theorem? I need a formal statement. Thanks! My question is from Casella & Berger exercise 5.44. Let $X_i,i=1,2,...,$ be independent Bernoulli(p) random variables and let $Y_n= \frac{1}{n}\Sigma_{i=1}^n X_i$ . Show that $\sqrt{n}(Y_n-p) \rightarrow n[0,p(1-p)]$ in distribution. This question is not difficult for me. Since they are iid, thus they satisfy Lindeberg condition and qualify for using central limit theorem. $E(Y_n)=p$ and $Var(Y_n)=\frac{1}{n} p(1-p)$ , thus $\frac{\sqrt{n} (Y_n-p) }{\sqrt{p(1-p)}} \rightarrow n(0,1)$ in distribution. Now the last step and also my question is how to say $\sqrt{n}(Y_n-p) \rightarrow n[0,p(1-p)]$ in distribution? The solution I found in website is saying this step needs Slutsky's theorem.","['statistics', 'probability']"
4588936,Annihilator of canonical modules,"Let $R$ be a Cohen-Macaulay local ring with canonical module $\omega_R$ and $I$ an ideal of $R$ of height $g$ . Write $\omega_{R/I} = \operatorname{Ext}^g_R(R/I, \omega_R)$ . It is well-known that whenever $R/I$ is Cohen-Macaulay, the module $\omega_{R/I}$ is the canonical module of $R/I$ . In particular, $\omega_{R/I}$ is a faithful $R/I$ -module; in other words, its annihilator is $\operatorname{ann}_R(\omega_{R/I}) = I$ . I wonder under what conditions (of course, weaker than $R/I$ Cohen-Macaulay; e.g., $I$ is unmixed) we have $\operatorname{ann}_R(\omega_{R/I}) = I$ . A possibility could be to investigate when $\omega_{R/I}$ is a semidualizing module over $R/I$ (is it?), since such modules are faithful. Thank you in advance.","['homological-algebra', 'algebraic-geometry', 'commutative-algebra']"
4588953,"Calculate $\int_{0}^{\infty}\frac{1}{(x^2 + t^2)^n}dx$ for $t > 0, n \ge 1$","The problem is as follows: Show that for all $t > 0, n \ge 1$ , $$
\int_{0}^{\infty}\frac{1}{(x^2 + t^2)^n}dx = {2n-2 \choose n -1}\frac{\pi}{(2t)^{2n -1}}
$$ What I have so far: Let $f(t) = \int_{0}^{\infty}\frac{1}{(x^2 + t^2)^n}dx$ $$
\frac{df}{dt} = \int_{0}^{\infty}\frac{\partial}{\partial t}\frac{1}{(x^2+t^2)^n}dx
$$ After simplification $$
\frac{df}{dt} = \int_{0}^{\infty}-2nt(x^2+t^2)^{-n-1}dx
$$ However, I am not sure how to integrate this and WolframAlpha outputs a complicated solution, so any help would be greatly appreciated.","['integration', 'multivariable-calculus', 'calculus', 'partial-derivative']"
4589006,"$\langle Tx,x\rangle$ is continuous if and only if that $T$ is continuous.","$H$ is a Hilbert space , $T:H \longrightarrow H$ is a linear map , use $T$ to define a functional $$\phi_T:H \longrightarrow \mathbb C \quad, \quad\quad\phi_T(x)=\langle Tx,x\rangle$$ Then $\phi_T $ is continuous if and only if $T$ is continuous. "" $\Longleftarrow$ ""Use the continuity of inner product. "" $\Longrightarrow$ ""Feng gives a very beautiful proof below,using The Closed Graph Theorem.","['hilbert-spaces', 'functional-analysis']"
4589035,Prove that: $\sqrt [3]{36}<\ln 28<\sqrt [3]{37}$,"Prove that: $$\sqrt [3]{36}<\ln 28<\sqrt [3]{37}$$ This inequality is the result of an integral representation/inequality. I lost access to the article that mentioned this inequality.  Now I want to prove it myself.  Using series is the standard way.  However, this involves heavy calculations, so it's almost useless. We know that, $$\ln 28=2\ln 2+\ln 7$$ From here, it is necessary to evaluate the $\ln2$ and $\ln 7$ numbers independently. More work is required. If you've seen this inequality before, please share the solution with us.  Presumably this will be the Integral representation. If you have made any solution that requires as little computation as possible, please share it with us. The question is open to all solutions involving elementary techniques or advanced techniques. Less computational solutions are preferred.","['integration', 'inequality', 'logarithms', 'reference-request', 'algebra-precalculus']"
4589038,How Well Do Logarithms Preserve Properties Of A Function?,"This is a question I have always had, relating to probability and statistics. In many applications (e.g. estimating the parameters of a probability distribution function),  we almost always end up trying to optimize the ""log likelihood"" instead of just the ""likelihood"". From a computational standpoint, I have heard that this is much easier - for example, diffrentiating the log likelihood can remove exponent terms and thus make the optimization process simpler. From a mathematical standpoint, we are often told (without explanation) that optimizating the log likelihood function is equivalent to maximizing the original likelihood function - for example, the stationary points (i.e. where the derivatives are 0) on the log likelihood function are apparently equivalent to the stationary points on the original likelihood function. Therefore, optimizing the log likelihood function or the original likelihood function will result in identical parameter estimates. My question relates to the mathematics of this phenomenon : Does the logarithm of a function always preserve the stationary points of the original function - and if so, why does this happen? As a reference, I found the following quote ( Why we consider log likelihood instead of Likelihood in Gaussian Distribution ): ""Because the logarithm is monotonically increasing function of its argument, maximization of the log of a function is equivalent to maximization of the function itself."" Thus - how do I know that the above is true? I will assume that ""a logarithm is montonically increasing function of its argument"" is true by definition - can we mathematically prove that : Maximizing the log of a function is ALWAYS equivalent to maximizing the function itself? Given any montonically increasing function - does maximizing a function and any montonically increasing transformation of this original function ALWAYS results in identical results? Thanks!","['optimization', 'calculus', 'statistics', 'probability']"
4589049,Sequence of random variables and limit theorems,"Let $\{X_{n}\}_{n}$ be a sequence of random variables independent and iddentically distributed with distribution $P$ and defined in the same probability space $(\Omega,\sigma,\mathbb{P})$ . Let $A$ be a Borel-set such that $P(A) > 0$ . We want to show that $$
\mathbb{P}(\{\omega\in\Omega : \#(\{n\in\mathbb{N} : X_{n}(\omega)\in A\}) = \infty\}) = 1
$$ I'm stuck in this result. I think that the proof is based in the fact that the set $A$ has positive measure, but I'm not able to related with the probability.","['borel-sets', 'statistics', 'probability']"
4589054,Mackey's definition of group action,"I am reading Mackey's book 'induced representations of groups and quantum mechanics'. I am quite baffled by his definition of group action. His definition is, $$ (sx)y = s (xy ) ,  \quad \quad \forall s \in S, x \in G, y\in G  \\
s e = s , \quad \text{where } e \text{ is the identity element of G}. $$ This is at odds with what I usually encounter, namely, $$  x (y s) = (xy)s. $$ In his definition, the associative rule is broken. On the left hand side, first $x$ and then $y$ , but on the right hand side, first $y$ and then $x$ . How to reconcile his definition with the more common definition?","['group-theory', 'group-actions']"
4589072,Probability theory problem - the order of drawing tickets doesn't matter,"I found this problem in a textbook. It is given right after the theory about the Bayes' rule and the total probability rule. Problem: We have an urn with $N$ lottery tickets of which $M \le N $ are winning tickets. $K$ persons $K \le N$ take turns drawing tickets from the urn in order. Each person draws one ticket. Prove that each person (no matter of his order number) has a probability of $M/N$ for drawing a winning ticket. I can prove this statement for persons $1$ and $2$ using the total probability law, but I cannot quite formalize the proof for the $K$ -th person. I have the feeling that the total probability law has to be used here. I have this approach in mind which I am not sure if it's rigorous enough. Here it is: Obviously the probability of each ticket being a winning ticket is $M/N$ . Let's suppose it's person # $K$ 's turn to draw and he draws some ticket $A$ . Now we define these events: $H_1$ : ticket A is a winning ticket $H_2$ : ticket A is not a winning ticket $B$ : person # $K$ has drawn a winning ticket Using the total probability law we get: $P(B) = P(H_1) P(B|H_1) + P(H_2) P(B|H_2) = ( M/N ) \cdot 1 + ((N-M)/N) \cdot 0 = M/N$ But this solution is weird to me because I feel like I am already assuming what I need to prove. I don't know if this approach is valid, is it? If it's not, how can this problem be solved more rigorously? And finally, I was also thinking of another approach: some sort of induction by K. But it didn't lead me anywhere (at least for now). So... is the above approach valid and if not, what is the best way to solve this problem rigorously (without using any complex apparatus of course, because this problem is in the very beginning of the textbook, only basic things are known so far) ?","['solution-verification', 'probability-theory', 'probability']"
4589082,Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$,"I'm supposed to calculate: $$\lim_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!}$$ By using WolframAlpha, I might guess that the limit is $\frac{1}{2}$ , which is a pretty interesting and nice result. I wonder in which ways we may approach it.","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
4589092,Derivative of a matrix inverse with respect to a parameter,"Given some real symmetric positive semidefinite matrix $X\in\mathbb{R}^{n\times n}$ and a parameter $\lambda \in\mathbb{R}_+$ , I want to differentiate: $$f(\lambda) = (\lambda \mathrm{Id} + X)^{-1}$$ but I don't know where to start. Clearly $f:\mathbb{R}_+\to \mathbb{R}^{n\times n}$ but not sure how to handle the inverse.","['derivatives', 'multivariable-calculus', 'linear-algebra']"
4589098,Closed form for $\int \sqrt[n]{\tan x}\ dx$,"I was solving $\displaystyle\int\sqrt[n]{\tan x}\ dx$ . Here's my method: $$\begin{align}\int\sqrt[n]{\tan x}\ dx &= \int\frac{n \cdot t^n}{1 + (t)^{2n}}\tag{1}\ dt\\& = n \int\sum_{k=0}^\infty (-1)^k (t)^{2nk}\cdot t^n \ dt\tag{2} \\& = n\sum_{k=0}^\infty(-1)^k \int t^{2nk+n}\ dt \tag{3}\\& = n \sum_{k=0}^\infty(-1)^k \frac{t^{2nk +n+1}}{2nk +n+1} + C\tag{4}\\& =n \sum_{k=0}^\infty(-1)^k \frac{(\tan x)^{\frac{2nk +n+1}n}}{2nk +n+1} + C\tag{5} \\& = \boxed{n \sum_{k=0}^\infty(-1)^k \frac{(\tan x)^{2k +\frac{n+1}n}}{2nk +n+1} + C}\end{align}$$ Steps: $(1)$ Substitution: $\tan{x} = t^n$ $(2)$ Taylor series: $\displaystyle\frac{1}{1+t} = \sum_{k=0}^\infty (-1)^k (t)^k\implies \frac{1}{1+t^{2n}} = \sum_{k=0}^\infty (-1)^k (t^{2n})^k$ . $(3)$ Interchanged integral and summation symbols. $(4)$ Used power rule of integration. $(5)$ Undone the substitution. Source: I was practicing integral calculus and came across $\displaystyle \int \sqrt{\tan x}\ dx$ and $\displaystyle \int \sqrt[3]{\tan x}\ dx $ . Both of them were nice and I solved them. So I thought there would be definitely a general solution for $\displaystyle \int \sqrt[n]{\tan x}\ dx$ where $n\in \mathbb{Z}^+$ . My question: Answers for $\int \sqrt{\tan x}\ dx$ and $\sqrt[3]{\tan {x}} $ were looking good, at least elementary (Having closed form). I expected the same for $\int \sqrt[n]{\tan x}\ dx$ . Is there any closed form for $n \sum_{k=0}^\infty(-1)^k \frac{(\tan x)^{2k +\frac{n+1}n}}{2nk +n+1} + C$ ? And is my method right?","['integration', 'calculus', 'closed-form', 'power-series', 'indefinite-integrals']"
4589162,Writing $e^A$ as an expression of matrix A,"Let $A\in\mathscr{L}(x)$ and satisfies the condition $A^2=A$ . Write the function $e^A$ as expression of A. My attempt: Let's start with $e^x$ , which we  re-write as a series: $$f(x) = \sum\limits_{j=0}^\infty\bigg(\frac{1}{n!}\bigg)x^n$$ . Here $$\bigg(\frac{1}{n!}\bigg)=a_{n}$$ , so we get $$f(x) = \sum\limits_{j=0}^\infty\bigg(a_n\bigg)x^n$$ Since we have a that $A$ is a diagonizable matrix, we have the following.  Suppose $P^{-1}AP=D=\mathrm{diag}(\lambda_1,\dots,\lambda_n)$ . The given function $f(x)=e^x$ , we can define in terms of A as $$f(A) = \begin{pmatrix} e^{(\lambda_1)} & 0 & \cdots & 0 \\ 0 & e^{(\lambda_2)} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & e^{(\lambda_n)} \end{pmatrix}$$ . But to me, this seems odd, since $e=\sum\frac{1}{n!}$ , so shouldn't the right answer be: $$f(A) = \begin{pmatrix} \frac{1}{1!}^{x} & 0 & \cdots & 0 \\ 0 & \frac{1}{2!}^{x} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \frac{1}{n!}^{x} \end{pmatrix}$$ . Maybe I am confusing the matrix with the variable x here, but the first matrix doesn't seem to give the full answer. Any hints? Thanks","['matrices', 'linear-algebra', 'exponential-function']"
4589216,Non-symmetric example where Fubini's theorem fails but the iterated integrals are equal,"I was looking for measurable functions $f\colon \mathbb R^2 \to \mathbb R$ , for which the iterated integrals $$\int_{\mathbb R} \int_{\mathbb R} f(x,y) \, dy \, dx \quad \text{and} \quad \int_{\mathbb R} \int_{\mathbb R} f(x,y) \, dx \, dy $$ exist, are finite and equal, but yet $f$ is not integrable with respect to the $2$ -dimensional Lebesgue measure, that is $$\int_{\mathbb R^2}\lvert f(x,y) \rvert \, d(x,y) = \infty. $$ The standard (and the only I could find) counter example seems to be $$f(x,y) = \frac{xy}{(x^2+y^2)^2}, \quad x,y\in [-1,1].$$ My problem with this is twofold: $f$ satisfies $f(x,y) = f(y,x)$ , which (if I do not miss something) makes it obvious that the iterated integrals must be equal (without even calculating them), since $f(-x,y)=-f(x,y)$ and $f(x,-y)=-f(x,y)$ , the inner integral respectively vanishes because of symmetry reasons. I wonder if there are more interesting functions that do not satisfy one (or both) of the two points above, but yet provide a useful example for the original problem. Thanks in advance!","['integration', 'fubini-tonelli-theorems', 'analysis', 'real-analysis']"
4589235,"Asymptotic of $_3F_2(1, \frac{1}{2}+d+n, 1+c+n; 1+2c, 2+2n;1)$ as $n\to \infty$","Let $c,d$ be in a small neighborhood of $0$ ; I think the limit $$\begin{aligned}&\quad \lim_{n\to \infty} 4^{-n} n^{3c-d} {_3F_2}(1, \frac{1}{2}+d+n, 1+c+n; 1+2c, 2+2n;1) \\
&= \lim_{n\to \infty} 4^{-n} n^{3c-d} \sum_{k\geq 0} \frac{(1/2+d+n)_k (1+c+n)_k}{(1+2c)_k (2+2n)_k} := f(c,d)
\end{aligned}$$ exists and is nonzero. I would like to find it explicitly in terms of $c,d$ . Question: how to find $f(c,d)$ ? Using integral representation of $_3F_2$ , we have (here $c>0$ ): $$\int_{[0,1]^2} (1-x)^{2c-1} y^c (1-y)^{-c} (1-xy)^{-1/2-d} \left(\frac{y(1-y)}{1-xy}\right)^n dxdy \sim \sqrt{\frac{\pi}{n}}\frac{1}{4c} n^{d-3c} f(c,d)$$ this seems amendable via Laplace's method: the maximum of $\frac{y(1-y)}{1-xy}$ in $[0,1]^2$ is $1$ ; however, this maximum occurs at boundary. I'm not so familiar with such techniques, but perhaps an expert on asymptotic analysis here can say something. Any suggestion is appreciated. When $c=0$ , we have $f(0,d) = \frac{2 \Gamma \left(\frac{1}{2}-d\right)}{\sqrt{\pi }}$ , because in this case, $_3F_2$ can be exactly evaluated in terms of gamma function.","['asymptotics', 'real-analysis', 'sequences-and-series', 'limits', 'hypergeometric-function']"
4589237,Interpreting $\lim_{z\to x}\dfrac{\dfrac{f(z)-f(x)}{z-x}-f'(x)}{z-x}$,"I stumbled upon the limit as shown in the title some time ago. Evaluating the limit is not hard -- $\lim_{z\to x}\dfrac{\dfrac{f(z)-f(x)}{z-x}-f'(x)}{z-x}\\
=\lim_{z\to x}\dfrac{f(z)-f(x)-(z-x)f'(x)}{(z-x)^2}\\
\stackrel{\text{L'Hopital's}}{=}\dfrac{1}{2}f''(x)$ . What puzzles me is after I rearrange the equation -- $f''(x)=\lim_{z\to x}\dfrac{\dfrac{f(z)-f(x)}{z-x}-f'(x)}{\dfrac{1}{2}(z-x)}$ , I have a hard time convincing myself of the denominator. The second derivative can be regarded as the difference of the slopes $f'(a),f'(b)$ per the distance between them while $b$ approaches $a$ . In the equation above, we consider the slope at $x$ and another somewhere between $x$ and $z$ (by MVT) but we never know if it is halfway through. So how do we make sense of $\dfrac{1}{2}(z-x)$ ? Why is it not some other fractions or even a varying fraction depending on $z$ , $x$ , and the function itself?","['limits', 'calculus', 'derivatives', 'real-analysis']"
4589276,"In $\triangle ABC$, if the angles are in an A.P and $b:c=\sqrt{3}:\sqrt{2}$, compute $\angle A$.","As title suggests, we have a triangle $\triangle ABC$ with angles in an arithmetic progression, and sides $b$ and $c$ in a ratio of $\sqrt{3}:\sqrt{2}$ . This is a problem I saw from a mathematics textbook in India. While I don't exactly remember the name I do know it was for grade 11-12. I'm posting this here to see what the easiest possible way could be to solve it. I'm going to share my own approach here, please share your own as well as pointing out any flaws in mine if you spot any: Here's my method: So, since we know that $\angle A, \angle B$ and $\angle C$ are in arithmetic progression, we can say that the arithmetic mean between $A$ and $C$ is $B$ : $$B=\frac{A+C}{2}$$ $$2B=A+C$$ $$3B=A+B+C$$ $$3B=180^\circ$$ $$\angle B=60^\circ$$ Now we know that: $$\frac{b}{\sin{B}}=\frac{c}{\sin{C}}$$ $$\frac{c}{b}=\frac{\sin{C}}{\sin{B}}$$ $$\sin{C}=\sin{B}\left(\frac{\sqrt{2}}{\sqrt{3}}\right)$$ $$\sin{C}=\bigg(\frac{\sqrt{3}}{2}\bigg)\bigg(\frac{\sqrt{2}}{\sqrt{3}}\bigg)$$ $$\sin{C}=\frac{1}{\sqrt{2}}$$ Therefore, $\angle C=45^\circ$ and thus, $\angle A=180^\circ-45^\circ-60^\circ=75^\circ.$","['euclidean-geometry', 'geometry', 'solution-verification', 'triangles', 'trigonometry']"
4589355,A limit involving binomial coefficients and square roots,"I am trying to evaluate the following limit $$\lim_{n\to\infty}\frac{1}{2^n\sqrt{n}}\sum^n_{k=1}\binom{n}{k}\sqrt{k},$$ but to no avail. I experimented with some large values of $n,$ and it seems like the limit is $\frac{1}{\sqrt{2}}.$ In fact, I am able to prove that $$\frac{1}{2^n\sqrt{n}}\sum^n_{k=1}\binom{n}{k}\sqrt{k}\leq\frac{1}{\sqrt{2}}$$ using the fact that the square root function is concave, as follows. We show that $$\sum^n_{k=0}\binom{n}{k}\sqrt{k}\leq \sum^n_{k=0}\binom{n}{k}\sqrt{\frac{n}{2}}=2^n\sqrt{\frac{n}{2}}.$$ Indeed, for $k\leq\frac{n}{2},$ one has $\binom{n}{k}\sqrt{k}\leq\binom{n}{k}\sqrt{\frac{n}{2}}.$ But since square root is concave, we get $\sqrt{\frac{n}{2}}-\sqrt{k}\geq\sqrt{n-k}-\sqrt{\frac{n}{2}}.$ Moreover, $\binom{n}{k}=\binom{n}{n-k},$ so it follows that $$\sum^n_{k=0}\binom{n}{k}\left(\sqrt{k}-\sqrt{\frac{n}{2}}\right)\leq 0,$$ as desired. With this, I suspect that one may obtain a lower bound of the sum and apply squeeze theorem to obtain the final limit. Any ideas on this?","['limits', 'binomial-coefficients']"
4589397,Can the topological spaces $\mathbb{R}$ and $\mathbb{Q}$ be expressed as a colimit of a diagram of discrete/finite spaces?,"I am interested in knowing if $\mathbb{R}$ and $\mathbb{Q}$ are colimits in the category of topological spaces of a diagram $J$ of discrete or finite spaces. I would like to know also if it is possible choosing discrete or finite subspaces of $\mathbb{R}$ or $\mathbb{Q}$ . I am interested in the eight cases. If the answer is no, any non-trivial way of expressing $\mathbb{R}$ and $\mathbb{Q}$ as colimits is of interest. Almost-discrete topological spaces adresses a similar question but the answers do not treat the colimit case.","['general-topology', 'category-theory', 'limits-colimits', 'discrete-mathematics']"
4589405,Laurent Series for $\frac{z}{(z+2)(z+1)}$,"I need to find a Laurent series of $ f(z) = \frac{z}{(z+2)(z+1)}$ centered at $z_0 = -2$ The partial fractions of $f$ is $ \frac{2}{(z+2)} - \frac{1}{(z+1)} $ Now, $\frac{1}{(z+1)} = \frac{1}{(z+2)-1} =  \frac{-1}{1 -(z+2)} = - \frac{1}{1 -(z+2)} = - \sum_{n=0}^{\infty} (z+2)^n $ . So we have the Laurent Series $ \frac{2}{(z+2)} + \sum_{n=0}^{\infty} (z+2)^n $ centered at $z_0 = -2$ . The function has a pole of order 1. Its my resolution correct?","['complex-analysis', 'solution-verification']"
4589415,1-Lipschitzian Linear Operators on Hilbert Spaces and Fixed Points,"$\textbf{Question}$ Let $(\mathcal{H}, \langle \cdot \, | \, \cdot \rangle)$ be a real Hilbert space with induced norm $\|\cdot\| = \sqrt{\langle \cdot \, | \, \cdot \rangle}$ and let $$\mathscr{B}(\mathcal{H},\mathcal{H}) \equiv \mathscr{B}(\mathcal{H}) = \{ T : \mathcal{H} \rightarrow \mathcal{H} \, | \, T ~\textrm{is linear and continuous} \}.$$ Additionally, let $T \in \mathscr{B}(\mathcal{H})$ be 1-Lipschitzian, i.e., $$(\forall x \in \mathcal{H})(\forall y \in \mathcal{H}) ~~ \| T x - T y \| \leq \| x - y \|.$$ Show that $\textrm{Fix} \, T = \textrm{Fix} \, T^{*},$ where $T^{*}$ is the adjoint of $T$ (i.e., $T^{*} \in \mathscr{B}(\mathcal{H})$ is the unique operator that satisfies $\langle T x \, | \, y \rangle = \langle x \, | \, T^{*} y \rangle$ for every $x$ and $y$ in $\mathcal{H}$ ) and $\textrm{Fix} \, T = \{ x \in \mathcal{H} \, | \, T x = x \}.$ $\textit{Remark}$ : $0_{\mathcal{H}}$ designates the zero element in $\mathcal{H}.$ $\textbf{Incomplete Solution Attempt}$ Since $T$ is 1-Lipschitzian, one can show (which I have already done) that $\| T \| \leq 1$ (in fact, this is an equivalent characterization in this particular context). Now, take $x \in \textrm{Fix} \, T.$ Then $$ \| x \| = \|T x\| \leq \| T \| \| x \| \leq \| x \|. $$ Hence, $\| T \| \| x \| = \| x \|,$ or, equivalently, $(\| T \| - 1) \| x \| = 0.$ Therefore, either $\| T \| = 1$ or $\| x \| = 0.$ If $\| x \| = 0,$ then $x = 0_{\mathcal{H}}$ and consequently, $\textrm{Fix} \, T = \{ 0_{\mathcal{H}} \}$ (this follows from the linearity of $T;$ indeed, $(\forall z \in \mathcal{H}) ~~ T 0_{\mathcal{H}} = T (0 z) = 0 T z = 0_{\mathcal{H}}$ ).
Thus, $x \in \textrm{Fix} \, T \Rightarrow \| T \| = 1$ or $\textrm{Fix} \, T = \{ 0_{\mathcal{H}} \}.$ Similarly, since $\| T \| = \| T^{*} \|,$ $x \in \textrm{Fix} \, T^{*} \Rightarrow \| T^{*} \| = 1$ or $\textrm{Fix} \, T^{*} = \{ 0_{\mathcal{H}} \}.$ $\textbf{Issue(s)}$ By the above logic, I have only proved the following: either $\textrm{Fix} \, T = \textrm{Fix} \, T^{*} = \{ 0_{\mathcal{H}} \}$ or $\| T \| = \| T^{*} \| = 1.$ I am having trouble resolving the latter consequence ( $\| T \| = \| T^{*} \| = 1$ ), as the former is supposed to be the only true possibility. Any help is appreciated.","['operator-theory', 'fixed-points', 'lipschitz-functions', 'functional-analysis', 'adjoint-operators']"
4589462,Finding the rank of certain Mordell curves,"I am interested in the elliptic curve $y^2 = x^3 + 16r^2$ for various cubefree $r$ . (This is related to the elliptic curve $y^2 + ry = x^3$ , sums of two rational cubes A159843 , and the recent results of Alpöge, Bhargava, & Shnidman .) For example, let $r=54919$ . The analytic rank is 2 but I can't prove that its rank is > 0. Are there any tools I can use to compute these, any places I can look them up, or any good theory to attack them? I know very little about elliptic curves, unfortunately. The curves of interest to me all have large conductors (~ $10^9$ ). LMFDB has a database of elliptic curves but I can't seem to find the ones I'm looking for there, and can't even search it very well (only by computing some invariants* and searching through the resulting lists). Cremona's Elliptic Curve Data is a wealth of information, but it only has data for conductors less than half a million, so there's nothing there for me. PARI/GP has pretty good ability to work with elliptic curves and supplies ellrank as well as ellanalyticrank , but these run out of gas eventually. mwrank is now only available as eclib , and I haven't been able to build it (it ask me to recompile with -fPIC, and I'm trying, but it's not working). * There are some easy values: any curve in this family has $j$ -invariant 0 and discriminant $-432r^4$ .","['elliptic-curves', 'number-theory', 'computational-mathematics', 'algebraic-geometry', 'mordell-curves']"
4589466,What happens when you find the derivative of $f(x) =|x^2-1|$.,"Let $f(x) =|x^2-1|$ . I'm trying to see if this function has a point of inflection and even though looking at the graph tells me the answer already, I was just curious. What happens to the modulus part when you differentiate it? Would you just take the piecewise version of the function and differentiate each function?","['calculus', 'derivatives']"
4589510,Infected cubes puzzle in 3D with threshold 4,"(Now cross-posted to Puzzling SE .) 3D infected cubes puzzle with threshold $4$ : On an $n\times n\times n$ cube, some cells are infected; if a cell shares a face with $4$ infected cells, it becomes infected. What's the minimum number of initially infected cells required to infect the whole cube? The two-dimensional, threshold $2$ version is a classic. The solution to that puzzle (often simply called the ""infected squares puzzle"") is $n$ . The two-dimensional, threshold $3$ version is more interesting. When $n$ is of the form $2^k-1$ , the solution is $\frac{4^k-1}3=\frac13n^2+\frac23n$ with an interesting recursive pattern. When $n$ is not of that form, I believe that the solution is $\lceil\frac13n^2+\frac23n+\frac13\rceil$ for odd $n$ and $\lceil\frac13n^2+\frac23n+\frac43\rceil$ for even $n$ . (I don't have a proof but I think someone else does.) In summary: $\frac13n^2+\frac23n+O(1)$ . Up a dimension, the three-dimensional, threshold $3$ version is simple again. The answer is $n^2$ . In fact, the $d$ -dimensional, threshold $d$ version is solved for all $d$ : see here . The logical next step, then, is the three-dimensional, threshold $4$ version. After some thinking, I have some conjectural upper bounds: $n=1$ is $1$ , trivially. $n=2$ is $8$ . (In fact, for all $n\ge2$ , the $8$ vertices must start infected, as they only have three neighbors.) $n=3$ should be $14$ (corner cells and face cells). $n=4$ should be $33$ . $n=5$ should be $53$ (I previously wrote $52$ but I don't think that works actually). What more progress can be made? Are the solutions I found for $n\le5$ minimal? Is there a formula (even an asymptotic one) for general $n$ ? For what it's worth, I can manage a lower bound of $\frac14n^3+\frac34n^2$ . However, given the data above, this doesn't seem to be an especially close bound. A helpful observation: Consider the $(n+1)^3$ points that are vertices of a cell. I believe that this set (""the grid points"") must be connected through the infected cells: that is, the set of these grid points union the set of infected cells must be a connected set. (This observation is true of the two-dimensional, threshold 3 version as well. However, in that case, it was both a necessary and sufficient condition; in our case, this is still necessary but no longer sufficient.)","['cellular-automata', 'combinatorics', 'geometry']"
4589520,Evaluate $\int^{\pi}_{-\pi}\frac{1}{1+\sin^2\!\theta}\mathrm d\theta$ using the Cauchy residue theorem. [duplicate],"This question already has answers here : Solve $\int_{-\pi}^{\pi}\frac{1}{1+\sin^{^{2}}t}dt$ (4 answers) Closed 1 year ago . The goal is to evaluate $\displaystyle\int^{\pi}_{-\pi}\frac{1}{1+\sin^2\!\theta}\,\mathrm d\theta\;$ using the Cauchy residue theorem. Using the substitution $z=e^{i\theta}$ and $\;\sin\theta =\dfrac{z-1/z}{2i}\;$ with $\mathrm d\theta=\dfrac{\mathrm dz}{iz}\;,\;$ I rewrite the integral as $\displaystyle\int_{|z|=1}\frac{\mathrm dz}{\left[1+\left(\frac{z-1/z}{2i}\right)^2\right]iz}=\int_{|z|=1}\frac{\mathrm dz}{\left(1+\frac{z^2-2+\frac{1}{z^2}}{-4}\right)iz}=$ $\displaystyle\int_{|z|=1}\frac{\mathrm dz}{\left(-4z+z^3-2z+\frac{1}{z}\right)\frac{-i}{4}}=\int_{|z|=1}\frac{\mathrm dz}{\left(z^4-6z^2+1\right)\frac{-i}{4z}}=\int_{|z|=1}\frac{(-4z/i)\mathrm dz}{z^4-6z^2+1}$ I can't spot a mistake in my algebra, though my brain often gets stuck and I can't find the simplest errors. Do you see any error? My problem is that I can't factor the denominator $z^4-6z^2+1$ . I want to do this so that I can find out what the residues are and apply the Cauchy residue theorem. Please show me the algebraic steps to get this integral in my desired form.(where residue theorem is easily applied). Please also point out any mistakes I have made.",['complex-analysis']
4589524,Intersection of (Toric varieties) and (Flag varieties),"For me, a variety $X$ is assumed to be irreducible and normal over some algebraically closed field $k$ of characteristic zero. I call $X$ a toric variety if it has an algebraic torus $T$ which embeds $T\hookrightarrow X$ as an open subset such that the action of $T$ on itself (multiplication) extends to an algebraic action on all of $X$ . I call $X$ a flag variety if $X\cong G/P$ for some connected reductive linear algebraic group $G$ and some parabolic subgroup $P\subseteq G$ . Note that flag varieties are smooth and projective. My question is this: Which varieties $X$ are simultaneously a toric variety and a flag variety? Certainly such varieties $X$ need to be smooth and projective. Some examples: (1) If $X=\mathbb{P}^n$ , then this is certainly a toric variety, and it is a flag variety via $SL_n/P_n$ where $P_n$ is a maximal parabolic subgroup (not equal to all of $SL_n$ ). (2) Products of the above example, e.g. $\mathbb{P}^1\times\mathbb{P}^1$ , since toric and flag varieties are closed under taking products. (3) Points since a point is clearly toric (the trivial torus action) and is of the form $G/G$ . (4) As a non -example, the homogeneous space $SL_3/B$ is a flag variety but not a toric variety ( $B$ is the Borel subgroup). Indeed, it is clearly a flag variety, but its Cox ring is not a polynomial ring, so it cannot be toric (see Corollary 2.10 in ""Mori dream spaces and GIT"" by Hu and Keel; and see Example 4.1 in ""The Cox ring of a spherical embedding"" by Gagliardi for the Cox ring calculation). I'm not sure if there is a ""nice"" general answer to my question. Any help is appreciated.","['toric-varieties', 'algebraic-geometry', 'algebraic-groups', 'projective-varieties']"
4589554,"If $f$ is continuous on $[0,1]$, $a\in (0,1)$, the rational derivative of $f$ at $a: \lim_{\Bbb Q\ni r_n\to a}\frac{f(a+r_n)-f(a)}{r_n}=A$.","If $f$ is continuous on $[0,1]$ , $a\in (0,1)$ , the rational derivative of $f$ at $a: \lim\limits_{\Bbb Q\ni r_n\to0}\frac{f(a+r_n)-f(a)}{r_n}=A$ . Show $f$ is differentiable at $a$ . My attempt: For any $x_n\searrow 0$ , find $0<r_n<x_n<s_n$ such that $s_n\to 0$ . But $\dfrac{f(a+r_n)-f(a)}{r_n}, \dfrac{f(a+x_n)-f(a)}{x_n}, \dfrac{f(a+s_n)-f(a)}{s_n}$ is not compatible. How to use the continuity ?","['continuity', 'calculus', 'derivatives']"
4589565,Solving a simple trig equation: $12 \sin^2 (x)+ \cos(x)=11$,"The problem asks to determine all solutions to the equation $12 \sin^2 (x)+ \cos(x)=11$ over the interval $[0,2\pi)$ .  Seems straightforward to me but I was marked wrong and it's important I understand why and the only explanation I get is I should have added $\pi$ .  I need a better math explanation than that. I replaced $\sin^2x$ with $1-\cos^2x$ .  This yields: $-12 \cos^2x + \cos x +1=0$ .  Multiplying thru by $-1$ I get $12 \cos^2x-\cos x -1=0$ .  This yields $\cos x=1/3 $ and $\cos x=-1/4$ . Now here's the part I was marked wrong on.  Since they want the answers in radians I simply took (in radian mode) the inverse cosine of $1/3$ .  This yielded $x=1.231$ . I examined that same reference angle in quadrant 4 (the other place where cosine is positive).  So, I did $2\pi-1.231=5.052$ .  Counted totally wrong. I did the same thing for inverse cosine of $-1/4$ ; I got $x=1.82$ and $x=4.46$ Counted totally wrong. YET ... when I plug these values back into the original equation it does equal $11$ .  I don't understand where I went wrong.","['algebra-precalculus', 'trigonometry']"
4589614,How does Green's theorem and Stokes' theorem generalize the fundamental theorem of Calculus,"I've read in few places that Green's theorem $$
\oint_C L dx + M dy = \iint_{D} \left(\frac{\partial M}{\partial x} - \frac{\partial L}{\partial y}\right) dx dy
$$ is a generalization of fundamental theorem of calculus. And same with Stokes' theorem. I assume all these can be described in some differential geometry language which I am not so familiar with. Could someone explain how Green and Stokes theorem are generlisation of the FTC? It would be appreciated! thank you!","['multivariable-calculus', 'calculus', 'greens-theorem', 'real-analysis']"
4589630,"$ -\frac{1}{3}(1-x^2)G''=2(1-\frac{1}{3}G')G, x\in(-1,1)$ only admits zero solution.","Suppose there is a smooth function $u$ on $[-1,1]$ satisfying $$
-\frac{1}{3}((1-x^2)u')'+1=e^{2u}.
$$ Prove that $u\equiv 0$ . If we define $G(x)=(1-x^2)u'$ , simple conputation gives $$
-\frac{1}{3}(1-x^2)G''=2(1-\frac{1}{3}G')G, x\in(-1,1),
$$ and resonable to assume $G(1)=G(-1)=0$ . Thus it suffices to prove $G\equiv 0$ . I have tried to give some expressions of $\int_{-1}^1G^2$ , for example, by the ODE with respect to $G$ and the boundary value it is easy to show $$
\int_{-1}^1(1-x^2)(G')^2=5\int_{-1}^1G^2,
$$ but I have no idea how to continue. This problem is actually from the post Axially symmetric solution of a PDE on $S^2$ . Appreciate any help!","['boundary-value-problem', 'ordinary-differential-equations', 'partial-differential-equations']"
4589665,Show a closer correlation between random variables,"Thank you for paying attention on my post! The Original Question (informal): I have $(2n+1)$ random variables $X, Y_1, \cdots, Y_n, Z_1,\cdots,Z_n$ satisfying that $\forall i,j,Cov(X,Y_i)=\sigma^2\rho(X,Y_i)>Cov(X,Z_j)=\sigma^2\rho(X,Z_j)\ge 0$ , $\forall i,Var(X)=Var(Y_i)=Var(Z_i)=\sigma^2$ , and $\forall i\ne j, Cov(Y_i,Y_j)=Cov(Z_i,Z_j)$ (this condition can be removed in some situations), where $\sigma\in\mathbb{R}^+$ is a constant, $\rho$ denotes the Pearson's correlation coefficient. In addition, $\rho(X,Z_i)$ s are positive numbers convergent to $0$ in some situations. (We can discuss the case of $\forall i,\rho(X,Z_i)\equiv0$ .) And $\rho(X,Y_i)$ s are positive constants and not larger than $1$ . Is there a way to show that for any weighting factors $w_i$ s, the weighted variable $Y=\sum_{i}w_iY_i$ is more closely correlated to $X$ than $Z=\sum_{i}w_iZ_i$ ? My Efforts and Supplemental Information: There may be some intuitions: $(A1)$ All the $Y_i$ s are more closely related to $X$ , compared to $Z_i$ s; $(A2)$ $Y=\sum_{i}w_iY_i$ has a closer relation to $X$ , compared to $Z=\sum_{i}w_iZ_i$ . (For example, $Y_i$ s are actually scaled versions of $X$ , and $Z_i$ s are some values of random noise.) However, since $w_i$ s are not limited to be positive or negative, it is difficult to show this through some metrics like Pearson's correlation coefficient. In other words, I may know that if all $w_i$ s are non-negative, the above $(A2)$ can be revealed by the Pearson's correlation coefficient or covariance: \begin{align*}
Cov(X,Y)&=\sum_i{w_iCov(X,Y_i)}>\sum_i{w_iCov(X,Z_i)}=Cov(X,Z),\\
\rho(X,Y)&=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}=\frac{\sum_i{w_iCov(X,Y_i)}}{\sqrt{Var(X)Var(Y)}}\\
&>\frac{\sum_i{w_iCov(X,Z_i)}}{\sqrt{Var(X)Var(Z)}}=\frac{Cov(X,Z)}{\sqrt{Var(X)Var(Z)}}=\rho(X,Z).
\end{align*} But the $w_i$ s are not guaranteed to be non-negative. :( There are some things that should not be changed: $(B1)$ The weighting factors $w_i$ s are randomly produced and can be positive or negative. $(B2)$ The weighted summation form of $Y=\sum_{i}w_iY_i$ and $Z=\sum_{i}w_iZ_i$ should not be changed or modified. $(B3)$ I may want to show $(A2)$ through some metrics/indices/measures (not limited to Pearson's correlation coefficient or the covariance). Therefore, I am trying in the following aspects: $(C1)$ (Change the goal) I am finding some other variables $Y^\prime$ and $Z^\prime$ constructed upon $Y$ and $Z$ (for example, $Y^\prime=Y^2$ and $Z^\prime=Z^2$ ), and trying to show that $Y^\prime$ is more closely related to $X$ , compared to $Z^\prime$ . $(C2)$ (Change the metrics/indices/measures) Beyond Pearson's correlation coefficient and the covariance, I am trying to find some other metrics/indices/measures which can reflect that $Y$ is more closely related to $X$ , compared to $Z$ . But after a long struggle (about a week), I can still not find a solution to show something like $(A2)$ . Is there a way to modify something or made some operations like $(C1)$ and $(C2)$ to show some conclusions like $(A2)$ ? I am still thinking and finding ... Why do I try to find a solution toward a conclusion like $(A2)$ ? This is actually an intermediate step in my work. I am stuck by it for a long time. Now I am feeling that my thoughts may be in a muddle. I am refining and re-organizing my above words and sentences to make the expressions clearer. Any ideas/suggestions about ""explicitly showing a closer relation"" are very welcomed. Thank you very much for reading such a long informal post!","['measure-theory', 'statistics', 'correlation', 'probability-theory', 'random-variables']"
4589679,Solving Recurrence Relation with Exponential Relationship,"I've come across a recurrence relationship in a modeling problem given by the following: $$ a_{n+1} = a_{n} + \gamma a_{n} e^{-a_{n}^2} \qquad a_0, \gamma > 0. $$ My intuition tells me that the sequence has an accumulation point as a result of the exponential decay, however, I'm struggling to find a way to show this formally. Does anyone have any resources or means of solving this kind of relation or finding an upper bound?","['recurrence-relations', 'ordinary-differential-equations']"
4589716,Evaluate $\int\frac{1}{x-\sqrt{1-x^2}}dx$,"The integral $I$ in question is defined as follows $$
I \equiv \int\frac{1}{x-\sqrt{1-x^2}}dx
$$ To solve this, I tried the trig substitution $x = \sin\theta$ , with $dx = \cos\theta d\theta$ , and rewrote the integral as follows $$
\int\frac{\cos\theta}{\sin\theta-\sqrt{1-\sin^2\theta}}d\theta
$$ I used to identity $1 - \sin^2\theta = \cos^2\theta$ and simplified the denominator as follows $$
\int\frac{\cos\theta}{\sin\theta-\cos\theta}d\theta
$$ I then rewrote $\cos\theta$ as $\frac{\sin\theta + \cos\theta}{2} - \frac{\sin\theta - \cos\theta}{2}$ and rewrote the integrand as follows $$
\int\frac{1}{2}\frac{\sin\theta+\cos\theta}{\sin\theta - \cos\theta} - \frac{1}{2}d\theta
$$ I then split the integral as follows $$
\frac{1}{2}\int\frac{\sin\theta+\cos\theta}{\sin\theta-\cos\theta}d\theta - \frac{1}{2}\int1d\theta
$$ For the first integral, I substituted $\phi = \sin\theta-\cos\theta$ , with $d\theta = \frac{1}{\sin\theta+\cos\theta}d\phi$ We can then rewrite our integral as $$
\frac{1}{2}\int\frac{1}{\phi}d\phi
$$ This is trivial and after undoing the substitutions we have a result of $$
\frac{\ln({x - \cos(\arcsin(x))})}{2} 
$$ The second integral is also trivial and just evaluates to $\frac{x}{2}$ Combining everything together gives us a final simplified answer of $$
I = \frac{\ln({x - \sqrt{1-x^2}})-x}{2} + C
$$ However, both IntegralCalculator and WolframAlpha give very different answers, so if someone could tell me where I made a mistake or another approach entirely that would be greatly appreciated.","['integration', 'indefinite-integrals', 'calculus', 'trigonometric-integrals']"
4589734,Fourier transform of product: proof without invoking Fourier inversion theorem,"Let $f,g$ be Schwartz functions, then $f*g$ and $fg$ are both Schwartz functions. Denote the Fourier tranform of $f$ by $F(f)$ . It is very straightforward to prove $F(f*g)=F(f)F(g)$ from definition and Fubini theorem. However, in most textbooks on Fourier analysis, the other formula $F(fg)=F(f)*F(g)$ is often not proved by direct methods. Instead, they prove the Fourier inversion formula first, and then apply Fourier transform to both sides of $F^{-1}(f*g)=F^{-1}(f)F^{-1}(g)$ . I am wondering whether we can give a proof of $F(fg)=F(f)*F(g)$ without invoking Fourier inversion theorem. In other words, is Fourier inversion theorem essential to this formula? I have tried to prove it directly but find it very difficult. I have little knowledge in mathematical logics, so I don't know whether it is a valid question,  but it really bothers me. Thank you very much if you are willing to help.","['fourier-analysis', 'harmonic-analysis', 'fourier-transform', 'analysis', 'real-analysis']"
4589740,Study the character of recursive sequence,"I have tried yesterday to ask this question but probably it was not well written so I have decided to show you my attempt step by step. I have to study the character of the following: $$x_{n}=-x_{n-1}^2,\,\, x_1=x\in\mathbb{R}$$ First of all I have observed that for x=0 and x=-1 the sequence is constant so: if $x=0$ then $\lim_{n\to\infty}x_n=0$ , since $x_n=x_{n-1}$ iff $x_{n}=0,1$ . if $x=-1$ . $x_1=-1,\, x_2=-1\,\,x_3=-1....x_n=-1$ then $\lim_{n\to\infty}x_n=-1$ Then $x_{n}-x_{n-1}=-x_{n-1}^2-x_{n-1}$ . So: if $-x_{n-1}^2-x_{n-1}>0$ then $-1<x_{n-1}<0$ .
Thus if $-1<x_1<0$ then $-1<x_n<0$ for each $n$ . This means that the sequence is increasing and so it is convergent: $\lim_{n\to\infty}x_{n}=sup\{x_n\}=l$ . $\textbf{l=0}$ since $x_n<0$ . if $-x_{n-1}^2-x_{n-1}<0$ then $x_{n-1}<-1\, \vee x_{n-1}>0$ .
Thus if $x_1<-1$ then $x_n<-1$ for each $n$ : $\lim_{n\to\infty}x_{n}=inf\{x_n\}=l$ . Since the possible finite limits are $0,-1$ , they are not acceptable. So $\textbf{l=}$ $-\infty$ . Then?","['limits', 'solution-verification', 'sequences-and-series', 'real-analysis']"
4589755,Is $A^tA$ Zariski dense in symmetric matrices?,"Let $k$ be a field with char $(k)\neq 2$ , and $n$ be a positive integrer. Denote $\operatorname{M}_n(k)$ to be all $n\times n$ matrices over $k$ . Define $\mathcal{A}=\{A^tA\ |\ A\in\operatorname{M}_n(k)\}$ , and $\mathcal{S}=\{A\in\operatorname{M}_n(k)|\ A^t=A\ \text{i.e. } A\text{ is a symmetric matrix.}\}$ . Question1 : When does $\mathcal{A}=\mathcal{S}$ ? (For example, when $n=1$ , we know that $\mathcal{A}=\mathcal{S}$ iff $k$ has no extension of degree 2.) Question2 : Is $\mathcal{A}$ Zariski dense in $\mathcal{S}$ ? If not in general, when it is dense? (For example, when $n=1$ , we know that $\mathcal{A}$ in dense in $\mathcal{S}$ iff $|k|=\infty$ ) Question3 : Question 2, for $k=\mathbb{R}$ or $\mathbb{C}$ .","['bilinear-form', 'algebraic-geometry', 'linear-algebra']"
4589778,"I have a sum which I can really easily show to be convergent, but Wolfram Alpha says it diverges. Is this a bug in Wolfram or is my proof flawed?","The sum is extremely simple: $\sum_{n=1}^\infty \frac{1}{n^{2+(\cos{n})^2}}$ . The proof of convergence is elementary and is as follows: Since $(\cos{n})^2 \geq 0$ for all positive $n$ , we have $0<\frac{1}{n^{2+(\cos{n})^2}} \leq \frac{1}{n^2}$ . Then by direct comparison, the sum converges. This is trivial enough that I would expect a middle-schooler to be able to follow the argument, but somehow  WA gets it wrong. Since WA is very powerful, I'm now somewhat suspicious about my proof, (although it seems pretty airtight). Is this a Wolfram bug or is my proof somehow flawed? Here is the link to the sum in Wolfram: https://www.wolframalpha.com/input?i=sum+from+n%3D1+to+infinity+of+1%2F%28n%5E%282%2B%28cos%28n%29%29%5E2%29%29","['trigonometry', 'wolfram-alpha', 'convergence-divergence', 'sequences-and-series']"
4589809,Question about the proof of non-complete statistic,"Definition 6.2.21 (from Statistical Inference 2ed by George Casella ) Let $f(t|\theta )$ be a family of pdfs or pmfs for a statistic $T(\mathbf{X})$ . The family of probability distributions is called complete if $\mathbb {E}_{\theta } g(T) = 0$ for all $\theta$ implies $\mathbb{P}_{\theta }(g(T) = 0) = 1 $ for all $\theta$ . Equivalently, $T(\mathbf{X})$ is called a complete statistic . Let $U=\sum_{i=1}^n X_i,V=\sum_{i=1}^n X_i^2$ .
I think from Definition 6.2.21 , In order to prove that $T=(U,V)$ is not complete, at first, we need to find $ g_{\theta}(u,v)$ -the pdf of $\left (U,V \right )$ , And then to construct a function $g(u,v)=2u^2-(n+1)v\not\equiv 0$ such that $$\mathbb {E}_{\theta } g(T) = \iint_{\mathbf{R}^2}g(u,v)\cdot g_{\theta}(u,v)dudv=0,\text{for all} \quad \theta\in \Theta .$$ But in the Example 12 , $$g(x_{1},\cdots,x_{n})=2(\sum_{i=1}^{n}x_{i})^{2}-(n+1)\sum_{j=1}^{n}x^{2}_{j};$$ $$g_{\theta}(x_1,\cdots,x_n)=\prod_{i=1}^{n}\left [ \frac{1}{\sqrt{2\pi}\theta}\exp\left\{-\frac{(x_i-\theta)^2}{2\theta^2}\right\} \right ];$$ $$\mathbb {E}_{\theta } g\left\{T(\mathbf{X}) \right\}$$ $$=\int\cdots\int_{\mathbf{R}^n} g(x_{1},\cdots,x_{n})\cdot g_{\theta}(x_1,\cdots,x_n)dx_1\cdots dx_n=
\mathbb {E}_{\theta }\left\{2\left ( \sum_{i=1}^{n}{x_i}\right)^2-(n+1)\sum_{i=1}^{n}{x_i}^2  \right\}.$$ I don't understand this could prove that $T$ is not complete.Where is the $g_{\theta}(u,v)?$ Maybe we should verifty that $$\mathbb {E}_{\theta } g(T)=\mathbb{E}_{\theta } g\left\{T(\mathbf{X}) \right\} \quad \text{for all} \quad \theta\in \Theta.$$","['measure-theory', 'probability-theory', 'statistics']"
4589825,Wage x dollars in biased coin flip,"You have 100 dollars. You are playing a game where you wager x dollars on a biased coin flip with a 90 percent probability of heads. You make 2x if it’s heads and lose the x dollars if it’s tails. How much do you think you should bet on each flip if you are going to play for 100 flips? My approach:
If I wage x dollars then, 90% of them time (heads), I would win 2x - x = x dollars and 10% of the time (tails) I would lose x dollars.
Hence my Expected value E[x] = 0.9(x) + 0.1(-x) = 0.8x which is lesser than how much I wage.","['probability-theory', 'probability']"
4589859,How do I prove that $\sqrt[5]{80\sqrt 5+176}=1+\sqrt 5$?,How do I prove that $\sqrt[5]{80\sqrt 5+176}=1+\sqrt{5}$ ? I have no idea how to proceed except just raising both sides to the power of $5$ and expanding $ \left(1+\sqrt{5}\right)^{5}$ using the binomial theorem,"['algebra-precalculus', 'radicals']"
4589876,Why is the derivative of the real absolute squared different to complex absolute squared,"I know that for $x \in \mathbb{R}$ there is $$ \frac{\mathrm{d}}{\mathrm{d}x} |x|^2 = 2|x| \frac{\mathrm{d}}{\mathrm{d}x} |x| = 2|x| \frac{x}{|x|} = 2x $$ Which makes sense because $|x|^2 = x^2$ , but for $z\in\mathbb{C}$ there is $$ \frac{\partial}{\partial z} |z|^2 = \frac{\partial}{\partial z} (z\overline{z}) = \frac{\partial z}{\partial z} \overline{z} + \frac{\partial\overline{z}}{\partial z}z = \overline{z} $$ Both makes sense on their own but shouldn’t they agree on the real part? Why are these two different? This leads to the second part: which one should I use for $$ \frac{\partial}{\partial f_j} |\langle f_j,f_k \rangle|^2$$ Or does it depend on whether the inner product space is complex or real?","['complex-analysis', 'calculus', 'absolute-value', 'inner-products']"
4589902,"$X_n$ are independent exponential variables, if $\sum_{n=1}^\infty λ_n^{-1}=\infty$,show that $\sum_{n=1}^\infty X_n=\infty$ a.e.","Let $X_1,X_2,\cdots,X_n$ be independent nonnegative random variables, with $X_n$ having density $λ_n\exp(-λ_n x),x\geq 0,λ_n\geq 0$ ,if $\sum_{n=1}^\infty λ_n^{-1}=\infty$ , show that $\sum_{n=1}^\infty X_n=\infty$ a.e. Attempts: $\sum_{n=1}^\infty λ_n^{-1}=\infty$ implies $\sum_n E(X_n)=E(\sum_n X_n)=\infty$ ,also I get a hint to try to consider $\exp(-\sum_nX_n)$ ,and from $X_n$ are independent, $E(\exp(-\sum_nX_n))=\Pi_n E(\exp(-X_n))$ , but I don't know what to do next? A very similar question: Let $X_1, X_2, \ldots$ be independent r.v.'s with $0 \leq X_n \leq 1$ and $\sum_n E(X_n) = \infty$. Show $\sum_n X_n = \infty$ with probability 1? but differs in $0\leq X_n\leq 1$","['convergence-divergence', 'probability-theory', 'probability', 'real-analysis']"
4589935,Show that there is no continuously differentiable function on closed disks,"In Cartesian coordinates, the closed unit  disk  is given by $$
\bar{D}=\left\{(x, y) \mid x^2+y^2 \leq 1\right\},
$$ and its boundary is given by $$
\partial D=\left\{(x, y) \mid x^2+y^2=1\right\}.
$$ Prove that  there is no continuously differentiable function $f: \bar{D} \rightarrow \mathbb{R}^2 $ satisfying: $$
f\left(\bar{D}\right) \subseteq  \partial D \;\;\text{and}\;\; f(x,y)=f\left(x y, x^2-y^2\right).
$$ Here, I have no idea why the question emphasize $f$ need to be differentiable. If someone can help me understand how to think about it would be great.",['analysis']
4589968,Minimize function $\sqrt{x^2-4x+5}+\sqrt{4+x^2}$,"Suppose I want to find $$
\min f(x) = \min(\sqrt{x^2-4x+5}+\sqrt{4+x^2})
$$ I'd start with computing derivative and set it to $0$ $$
f'(x) = \frac{x-2}{\sqrt{x^2-4x+5}} + \frac{x}{\sqrt{4+x^2}}
$$ Then $$
\frac{x-2}{\sqrt{x^2-4x+5}} + \frac{x}{\sqrt{4+x^2}} = 0
$$ $$
(x-2)\sqrt{4+x^2} + x\sqrt{x^2-4x+5}=0
$$ My first instinct is to rewrite it as $$
(2-x)\sqrt{4+x^2} = x\sqrt{x^2-4x+5}
$$ and square both sides. The thing is, I get $$
3x^2 -16x +16=0
$$ with $x_1 = 4/3$ and $x_2 = 4$ . So $f'(x_1^-) <0$ and $f'(x_1^+) > 0$ thus in $x = 4/3$ there is a minimum of our function. Question: is it valid to square both sides of an equation as I did above? Is there a possibility that because of that I lose or introduce a solution that shouldn't exist and might mess everything up?","['optimization', 'maxima-minima', 'solution-verification', 'derivatives']"
4590061,How is this nonlinear differential equation solved with hyperbolic functions? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I am actually writing a research paper on how to determine the approximate region of where a rolling ball in a roulette wheel will fall through a differential equation. The differential equation is: $$\dfrac {dy}{dt}= -ay^2+b$$ where $(a)$ and $(b)$ are constants. I found the answer to this differential equation in another research paper and the answer the research paper shows, but without any steps leading to it is: $$y(t)= -b\coth(c-abt)$$ where $c$ is the constant arising from the integration.
I don't understand how this research paper managed to solve the nonlinear differential equation through the use of hyperbolic functions. Does anybody know how it has been done and is able to explain it please?","['integration', 'calculus', 'ordinary-differential-equations', 'hyperbolic-functions']"
4590101,"What are polar coordinates of the origin and why is $\arg(0) =$ undefined, an option?","I have come here from reading this and but it didn't really answered my question. Wikipedia says the cartesian co-ordinates are converted using $$x=r \cos \theta$$ $$y = r \sin \theta$$ where $r≥0$ but that doesn't make sense because those conversions are found using the definitions of $\sin$ and $\cos$ which are $$\sin \theta = \frac{y}{r},$$ $$\cos \theta =\frac{x}{r}$$ where $r>0$ and thus $r≠0$ . So for $(0,0)$ those conversions are not valid. Also from this answer, we may let $\theta = \arg (0,0)$ be undefined or $\mathbb{R}$ . My question is why is "" $\arg (0) =$ undefined"" even an option when all real numbers are perfect candidates for it? Why is the ""convention"" of leaving it undefined even a thing? What are we afraid of? Multiple values? Because surely, $\arg(z)$ has multiple values even when $z≠0$ . So why ""undefined"" for $z=0$ ?","['complex-analysis', 'calculus', 'polar-coordinates', 'trigonometry', 'algebra-precalculus']"
4590131,Find the eigenvalues of a 5x5 (symmetric) matrix containing a null 4x4 matrix,"Find the eigenvalues of $$A=\begin{bmatrix}
    0 & 1 & 1 & 1 &1 \\
    1 & 0& 0 & 0& 0\\
    1 & 0& 0 & 0& 0\\
    1 & 0& 0 & 0& 0\\
    1 & 0& 0 & 0& 0
\end{bmatrix}$$ It doesn't appear to be a block matrix. I can't find a way other than explicitly calculating the determinant of $\det (A-\lambda I)$ . Is there something else i'm missing?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4590153,Usual method of partial fractions decomposition over the reals seems to fail.,"I assumed that it would be straightforward to find the partial fraction decomposition over the reals of the rational function $$f(x) = \frac{1}{(x^2 +1)^2}.$$ However, when I try what I thought would be the usual method of writing it as $$\frac{1}{(x^2+1)^2} = \frac{Ax + B}{x^2 + 1} + \frac{Cx + D}{(x^2+1)^2},$$ I find that the only choice of constants is $A = B = C = 0$ , and $D = 1$ , simply reproducing what I started with. Typically, one might think that this would be a logical approach to finding the indefinite integral of $f(x)$ , but it seems to fail here. Could someone explain why this happens, or where I have made a mistake? For the record, the integral is elementary using the substitution $x = \tan \theta$ and  a trig identity in the result.","['algebra-precalculus', 'partial-fractions']"
4590161,Polar set. Continuous dual space of $c_0$.,"I need to find a Polar set in dual space for the following subset of $c_0$ .
Let $X = c_0$ ( $c_0$ — Banach space of sequences converging to zero). We have a set: $A_1 = \{x \in c_0: x_1+x_2>-2\}$ ( $x_1$ and $x_2$ — coordinates of vector $x$ ). My attempt:
Dual space of $c_0$ is $l_1$ . As I know Polar set of subset of $c_0$ in dual space: $A_1' = \{y \in l_1: \sup(|(x,y)|) < 1\}$ (sup here for all x from $A_1$ ). So in general case we got a module of a complex number : $|x_1y_1+x_2y_2|$ and we need other $y_i = 0$ for all $i = 3,4,5,...$ , because we have no limitation for other $x_i$ . Now we may assume that polar set contains vector such as $(y_1, y_2, 0, 0, ...)$ . Is there any way to find all of this $y_1, y_2$ ?(other than pick). Also is this logic right?","['general-topology', 'convex-analysis', 'functional-analysis']"
4590170,"Exercise from Isaacs' book ""Character Theory of Finite Groups""","I'm having trouble solving Exercise 5.19 from Isaacs' book. The condition of the problem is: Let $G$ be doubly transitive on $\Omega$ and let $H \subseteq G$ with $|G:H|<|\Omega|$ . Show that $H$ is transitive on $\Omega$ . I know various theorems and properties related to transitive and doubly transitive actions described in the fifth chapter of this book. However, I don't understand how to apply them. I would be very glad of any help with this problem!","['group-theory', 'group-actions', 'finite-groups']"
4590203,"If $x_n \to 0 \pmod{a}$ for every $a>0$, does it follow that $x_n \to 0$?","Let $x_n$ be a real-valued sequence such that $x_n \to 0\pmod{a}$ for every real $a>0$ : does it follow that $x_n \to 0$ ? Just so there is no ambiguity in the meaning of the question the hypothesis is that $$ \forall a>0 \; \forall \varepsilon>0 \; \exists n_0\in\mathbb{N} \; \forall n\geq n_0 \; (x_n \in \mathopen]-\varepsilon,+\varepsilon\mathclose[+a\mathbb{Z})$$ where $$\mathopen]-\varepsilon,+\varepsilon\mathclose[+a\mathbb{Z} := \{x\in\mathbb{R} \; : \; \exists u\in\mathbb{R}\; \exists k\in\mathbb{Z} \; (|u|<\varepsilon \; \land \; x=u+ka)\}$$ ( Edit: The question was initially posed with the notation “ $A$ ”: I changed this to $a$ to avoid a notation clash with the accepted answer which uses $A$ for something else (essentially the set of those $a$ ).) Addendum: The question has now been answered satisfactorily, but for completeness of MSE, let me still add some remarks that might help explain both whence the question is coming from and what the answer tells us. Suppose we're given a single $a>0$ , say $a>1$ for definiteness, and we want to find a sequence $x_n$ such that $x_n \to 0 \pmod{1}$ and $x_n \to 0 \pmod{a}$ yet $x_n \to +\infty$ in $\mathbb{R}$ ; in fact, let's even demand $x_n \equiv 0 \pmod{1}$ (i.e., $x_n\in\mathbb{Z}$ ).  Here's how one can do it: if $a$ is rational it's obvious (just take $x_n$ to be ever larger integers that are also multiples of $a$ ); and if $a$ is irrational, let $p_n/q_n$ be the convergents of its continued fraction: they satisfy $|q_n a-p_n| < \frac{1}{q_n}$ , which means that $p_n$ is both an integer, and also ever closer to $0$ mod $a$ , so we have $p_n \equiv 0 \pmod{a}$ and $p_n \to 0 \pmod{a}$ , yet $p_n \to +\infty$ in $\mathbb{R}$ , as promised. ∎ (For example, the sequence 1, 3, 7, 17, 41, 99, etc. tends to $0$ mod $\sqrt{2}$ , and is, of course, identically $0$ mod $1$ .) I suspect (thought I didn't check the details) that standard results on simultaneous Diophantine approximation (maybe applied to the $1/a$ : my $a$ is in some sense “backwards” in the question) will similarly give us a sequence $x_n$ tending to $+\infty$ in $\mathbb{R}$ but tending to $0$ mod finitely many $a$ 's (and exactly $0$ mod one).  The question was whether this “finitely many $a$ 's” can be strengthened to “all $a>0$ ”, and the accepted answer shows that it's not even possible for a dense $G_\delta$ of $a$ .","['general-topology', 'sequences-and-series', 'real-analysis']"
4590225,Can we find an inverse of a model for deadtime?,"This is kind of a real-world question, in that it comes from the work I do, but I'm just pursuing it for my own edification. When a radiation detector detects an event, it is insensitive to further events for a certain amount of time until it has recovered, the deadtime. The two standard models are paralyzable and non-paralyzable. If it is paralyzable, a new event that happens within the deadtime period resets the clock, so to speak, extending the time of insensitivity. That is modeled as $$m=n e^{-n \tau}$$ where m is the measured count rate, n the theoretical true count rate, and $\tau$ is the deadtime. For a Geiger-Müller counter $\tau$ is around $100 \mu s$ , a scintillator around $10 \mu s$ . So, given a true count rate, it predicts a measured count rate. The non-paralyzable model assumes that new events during the deadtime period are simply lost, and is modeled as $$n = \frac{m}{1-m \tau}$$ That, at least, is easy to express in terms of either $m$ or $n$ . No detector is really one or the other, and a better model is semi-paralyzable, with a $\tau_P$ for the non-paralyzable and $\tau_N$ for the paralyzable deadtimes, or $f$ for the fraction of paralysis (for a GM-counter $f$ is around 5%). So, $$m = \frac{n e^{-n \tau f}}{1 + n \tau (1-f)}$$ So it's both the above models put together. Again, the true (but unknown) count rate is the input.
Meanwhile, Ludlum Measurements, having sensible engineers, has a deadtime correction in some of their instruments that estimates true count rate, $n$ , from the measured count rate, $m$ , by including a quadratic term in the denominator of the non-paralyzable model and fitting to two deadtimes, $\tau_1$ and $\tau_2$ , $$n = \frac{m}{1 - m \tau_1 + m^2\tau_2}$$ What I would like to do is to use the improved deadtime model to find an estimate of the true count rate, so $n$ on the left-hand side and $m$ is the input, and I would like to compare the paralysis time, $\tau f$ to Ludlum's deadtime 2, $\tau_2$ which, since it multiplies $m^2$ , has to actually be a time-squared. I thought at first, it's just some algebra and a few terms of Taylor expansion, how hard can it be? But all I seem to get is a mess, and I don't know how to compare the semi-paralyzable model with Ludlum's. Edit: I brute-forced it with a spreadsheet. This time m, measured, is on the horizontal, and n, estimated true count rate, the vertical. I didn't make it look very pretty, I'm not very good with that software. But blue squares are a linear response for comparison, green triangles the hybrid model, which I assume to be essentially correct, orange diamonds the standard non-paralyzable model, and yellow triangles are Ludlum's correction in the Model 3000. I used 80 microsecond deadtime, 5% paralysis fraction (thinking of a 44-9 probe), and chose a deadtime 2 that illustrates the trend, although at 2e-9 it's pretty small. The takeaway is that it pulls you down below the true count rate, even below the imperfect non-paralyzable model.","['algebra-precalculus', 'inverse-function', 'applications']"
4590235,Positive Solution of Exponential Equation,"The equation is $2^{x+1}+2^{1/x^2}=6$ . By inspection I see that $1$ is a solution. However, after trying to algebraically isolate for $x$ , I was unable to deduce that $1$ is a solution. Given the simplicity of the value of the solution, I was wondering if it would be possible to do so? Also, I am only looking for the positive solution. However, when graphically analyzing the equation, I noticed that there exists a negative solution that Wolfram Alpha is incapable of giving an exact form for. Does there exist an exact form of the negative solution other than an infinite decimal?",['algebra-precalculus']
4590247,Finding the equation of a hyperbola given the foci and a tangent line [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question How would one calculate the equation of a hyperbola given the foci and a tangent line? So far I've only figured out that I can find the $c$ through the foci. I would be really thankful if someone could help.","['trigonometry', 'conic-sections', 'geometry']"
4590298,How much can you generalize the notion of an ideal/normal subgroup/kernel of a homomorphism?,"An ideal in a the kernel of a ring homomorphism. Similarly, a normal subgroup is the kernel of a group homomorphism. One thing that immediately jumps out at me is that rings and groups are both single-sorted equational theories and they both extend the theory of a single monoid $\langle 0, + \rangle$ where $+$ is constrained to be associative and $0$ is constrained to be an identity. A homomorphism $f : A \to B$ where $A$ and $B$ are algebraic structures of the same signature satisfies the rule: For every equation $\varphi(\vec{v})$ and variable context $\vec{v}$ such that $A, \vec{v} \models \varphi(\vec{v})$ , it holds that $B, f(\vec{v}) \models \varphi(\vec{v})$ . This notion of a homomorphism can be defined without referring to the underlying equational theory , we can consider the structures $A$ and $B$ is isolation without thinking about where we got them. I think this means that if I have an algebraic signature $\sigma$ extending $\langle 0, + \rangle$ and a theory $T$ extending $(a+b)+c \approx a + (b+c) \;\;\text{and}\;\; a+0 \approx a \;\;\text{and}\;\; 0+a \approx a$ , then I can define some kind of ideal. It might be possible to extend this even further to multi-sorted equational theories like the theory of a single category, but the most straightforward way to do this that I can think of produces a disjoint connection of monoid ideals on something else that's not very interesting. How far can you generalize the notion of an ideal / kernel of a homomorphism?","['abstract-algebra', 'logic']"
4590307,Fitting a painting through a door,"Deceptively tricky - will it fit through the door? I work in an art gallery and we regularly take delivery of large paintings housed in wooden crates. Our old building has a door measuring 180 x 234 cm (290 cm diagonal). What is the formula for calculating whether a painting will fit through our doors diagonally? For example, I am trying to calculate if we can receive a painting in a crate sized 281 x 347 x 23 cm. Its shortest side (281 cm) is within our diagonal dimension but I estimate that the 23 cm depth means that it will not fit. What is the formula for establishing this accurately? Ideally I would like to refer to a formula each time I find out the dimensions of a wooden crate. Door and Crate","['geometry', 'applications']"
4590321,Prove that $A^3\equiv I\mod p$.,"Let $p$ be a prime. Let $A$ be a $p\times p$ matrix whose $(i,j)$ th-coordinate is ${i+j-2\choose i-1}$ . Prove that $A^3\equiv I\mod p$ . Source: problem 10 from this problem set . We need to show that in $\mathbb{Z}_p, A^3 -I $ is the zero matrix, or equivalently that the minimal polynomial of $A$ in $M_p(\mathbb{Z}_p)$ divides $x^3-1=(x-1)(x^2+x+1).$ Clearly $A$ is not the identity matrix (e.g. $A_{1,2} = 1\neq 0$ ). I'm not sure if it's necessary to determine the eigenvalues or the characteristic polynomial of $A$ (in $\mathbb{Z}_p$ of course, since it doesn't seem necessary to consider other fields). I know the formula for the Vandermonde determinant, but I'm not sure if it's useful for this problem. Clearly modular arithmetic properties would be useful (e.g. $a^x\equiv b^x\mod n$ whenever $a\equiv b\mod n$ and if $f$ is a polynomial with integer coefficients, then $f(n)\equiv f(m)\mod a$ whenever $n\equiv m\mod a$ ), but they're not enough to make some progress on this problem. I'm not sure if it's useful to find the inverse of $A$ .","['contest-math', 'modular-arithmetic', 'elementary-number-theory', 'matrices', 'linear-algebra']"
4590337,Regular polygon of radius $1$ with diagonals: mysterious ring of radius $1/e$?,"I was playing with a geogebra applet that shows regular $n$ -gons of radius $1$ with their diagonals. For example, here is the $12$ -gon with its diagonals: For any value of $n$ , when I shrink the image, the image becomes darkened (due to the thickness of the lines), but sometimes there remains a faint single white ring , indicating a ring of exceptionally large cells. For example: $n=20$ $n=45$ $n=50$ Now here's the interesting thing: The radius of the white ring always seems to be approximately $1/e$ . (I used Perfect Screen Ruler .) For some $n$ -values, I cannot perceive a single white ring; I guess it still exists but is not perceivable due to limitations in pixelation and/or visual acuity. I can formalize my conjecture as follows: In a regular $n$ -gon of radius $1$ with it diagonals, if $d_n=$ distance between the centre and the centroid of one of the cells with the greatest area (excluding the centre cell when $n$ is odd), then $$\lim\limits_{n\to\infty}d_n=\frac{1}{e}$$ Question: Is my conjecture true? (This question was inspired by another question: Distribution of areas in regular $n$ -gon with diagonals, as $n\to\infty$ .)","['conjectures', 'area', 'geometry', 'polygons', 'limits']"
4590343,Existence of an analytic function by estimate,Does there exist an analytic function on the unit disc s.t. $|f\left(\frac1n\right)−\frac{(−1)^n}{n^2}|<\frac{1}{n^3}$ for all $n\geq2$ ? See my idea but I don’t know how to complete the proof. I know such analytic function doesn’t exist.,"['complex-analysis', 'analytic-functions']"
4590353,"13.	Let f : P ({1,2,3,4}) → {0,1,2,3,4} be the function that maps each subset S ⊆ {1,2,3,4} to its cardinality, f (S)= |S|. Is f onto or 1-1?","After working through this, I was sure that function f was one-to-one and not onto, but according to my answer key its onto but not one-to-one. I don't exactly understand how though. Here's my thought process: Function f maps elements of P{(1, 2, 3, 4)} to the set {0, 1, 2, 3, 4} Possible subsets of P({1,2,3,4)} are: {∅,{1},{2},{3},{4},{1,2},{1,3},{1,4},{2,3},{2,4},{3,4},{1,2,3},{1,2,4},{1,3,4},{2,3,4},{1,2,3,4} = 16 total subsets, each denoted as set S. Every subset set has elements that can ALL be uniquely mapped to elements in the set {0,1,2,3,4}, but 0 in this set can never be mapped to any elements of S. So, I would think the function is one-to-one, but not onto. Can someone explain where I'm going wrong? Thanks! :D",['elementary-set-theory']
4590371,how to prove this sum/combinatorics identity,"Prove $$\frac{n!}{x(x+1)(x+2)...(x+n)} = \frac{(-1)^0 \cdot {n \choose 0}}{x} +  \frac{(-1)^1 \cdot {n \choose 1}}{x+1} +  \frac{(-1)^2 \cdot {n \choose 2}}{x+2} + ... +  \frac{(-1)^n \cdot {n \choose n}}{x+n}$$ Prove true for n = 1, $$ \frac{1!}{x(x+1)} = \frac{1}{x} - \frac{1}{x+1} = \frac{1}{x(x+1)}$$ Assume true for n = k, $$\frac{k!}{x(x+1)(x+2)...(x+k)} = \frac{(-1)^0 \cdot {k \choose 0}}{x} +  \frac{(-1)^1 \cdot {k \choose 1}}{x+1} +  \frac{(-1)^2 \cdot {k \choose 2}}{x+2} + ... +  \frac{(-1)^k \cdot {k \choose k}}{x+k} \hspace{3em} (1)$$ Prove true for n = k + 1, $$\frac{(k+1)!}{x(x+1)(x+2)...(x+k+1)} = \frac{(-1)^0 \cdot {k+1 \choose 0}}{x} +  \frac{(-1)^1 \cdot {k+1 \choose 1}}{x+1} +  \frac{(-1)^2 \cdot {k+1 \choose 2}}{x+2} + ... +  \frac{(-1)^{k+1} \cdot {{k+1} \choose {k+1}}}{x+{(k+1)}} \hspace{3em} (2)$$ multiply both sides of $(1)$ by $\frac{k+1}{x + (k+1)}:$ $$\frac{k+1}{x + (k+1)} \cdot \frac{k!}{x(x+1)(x+2)...(x+k)} = \frac{k+1}{x + (k+1)} \cdot [ \frac{(-1)^0 \cdot {k \choose 0}}{x} +  \frac{(-1)^1 \cdot {k \choose 1}}{x+1} +  \frac{(-1)^2 \cdot {k \choose 2}}{x+2} + ... +  \frac{(-1)^k \cdot {k \choose k}}{x+k}] $$ $$\frac{(k+1)!}{x(x+1)(x+2)...(x+k+1)} = \frac{k+1}{x + (k+1)} \cdot [ \frac{(-1)^0 \cdot {k \choose 0}}{x} +  \frac{(-1)^1 \cdot {k \choose 1}}{x+1} +  \frac{(-1)^2 \cdot {k \choose 2}}{x+2} + ... +  \frac{(-1)^k \cdot {k \choose k}}{x+k}] $$ So LHS of $(2)$ is satifisfied: LHS = $  \frac{k+1}{x + (k+1)} \cdot [ \frac{(-1)^0 \cdot {k \choose 0}}{x} +  \frac{(-1)^1 \cdot {k \choose 1}}{x+1} +  \frac{(-1)^2 \cdot {k \choose 2}}{x+2} + ... +  \frac{(-1)^k \cdot {k \choose k}}{x+k}] $ = $  \frac{k+1}{x + (k+1)} \cdot [ \frac{{(-1)^0 \cdot {k \choose 0}}{} (x+1)(x+2)...(x+k) +  {(-1)^1 \cdot {k \choose 1}}{} x(x+2)...(x+k) +  {(-1)^2 \cdot {k \choose 2}}{} + ... +  {(-1)^k \cdot {k \choose k}}{} x(x+1)(x+2)...(x+k-1)}{ x(x+1)(x+2)...(x+k)}] $ = $ (k+1)[ \frac{{(-1)^0 \cdot {k \choose 0}}{} (x+1)(x+2)...(x+k+1) +  {(-1)^1 \cdot {k \choose 1}}{} x(x+2)...(x+k+1  ) +  {(-1)^2 \cdot {k \choose 2}}{} + ... +  {(-1)^k \cdot {k \choose k}}{} x(x+1)(x+2)...(x+k)}{ x(x+1)(x+2)...(x+k+1)}] $ = $ (k+1)[ \frac{(-1)^0 \cdot {k \choose 0}}{x} +  \frac{(-1)^1 \cdot {k \choose 1}}{x+1} +  \frac{(-1)^2 \cdot {k \choose 2}}{x+2} + ... +  \frac{(-1)^k \cdot {k \choose k}}{x+k+1}] $ The problem might be in the last two lines somewhere, or perhaps it cannot be achieved in this direction of reasoning.
Either way, if you have an idea on how to proceed with this line of thought, or another, i'm interested in seeing.
Thanks. EDIT:
another attempt: Since, ${n+1 \choose k} = {n \choose k} + {n \choose k -1}$ , $$ = \frac{(-1)^0 \cdot {k+1 \choose 0}}{x} + [ \frac{(-1)^1 \cdot {k+1 \choose 1}}{x+1} +  \frac{(-1)^2 \cdot {k+1 \choose 2}}{x+2} + ... + \frac{(-1)^k \cdot {k+1 \choose k}}{x+k} ]+  \frac{(-1)^{k+1} \cdot {{k+1} \choose {k+1}}}{x+{(k+1)}}$$ $$ = \frac{(-1)^0 \cdot {k+1 \choose 0}}{x} + [\frac{(-1)^0 \cdot {k \choose 0}}{x} + \frac{2(-1)^1 \cdot {k \choose 1}}{x+1} +  \frac{2(-1)^2 \cdot {k \choose 2}}{x+2} + ... +\frac{2(-1)^k \cdot {k \choose k-1}}{x+k}  + \frac{(-1)^k \cdot {k \choose k}}{x+k} ]+  \frac{(-1)^{k+1} \cdot {{k+1} \choose {k+1}}}{x+{(k+1)}}$$ $$ = \frac{(-1)^0 \cdot {k+1 \choose 0}}{x} + 2[\frac{(-1)^0 \cdot {k \choose 0}}{x}  + \frac{(-1)^1 \cdot {k \choose 1}}{x+1} +  \frac{(-1)^2 \cdot {k \choose 2}}{x+2} + ... +\frac{(-1)^k \cdot {k \choose k}}{x+k}  ]- \frac{(-1)^0 \cdot {k \choose 0}}{x} - \frac{(-1)^k \cdot {k \choose k}}{x+k} +  \frac{(-1)^{k+1} \cdot {{k+1} \choose {k+1}}}{x+{(k+1)}}$$ $$ = \frac{(-1)^0 \cdot {k+1 \choose 0}}{x} + 2[\frac{k!}{x(x+1)(x+2)...(x+k)}]- \frac{(-1)^0 \cdot {k \choose 0}}{x} - \frac{(-1)^k \cdot {k \choose k}}{x+k} +  \frac{(-1)^{k+1} \cdot {{k+1} \choose {k+1}}}{x+{(k+1)}}$$ $$ = 2[\frac{k!}{x(x+1)(x+2)...(x+k)}]- \frac{(-1)^k}{x+k} +  \frac{(-1)^{k+1} \cdot {{k+1} \choose {k+1}}}{x+{(k+1)}}$$ $$ = 2[\frac{k!}{x(x+1)(x+2)...(x+k)}]- (-1^k)[\frac{1}{x+k} -  \frac{1}{x+{(k+1)}}]$$ $$ = 2[\frac{k!}{x(x+1)(x+2)...(x+k)}]- (-1^k)[\frac{1}{(x+k)(x+k+1)} ]$$","['proof-explanation', 'summation', 'combinatorics', 'factorial']"
4590459,Find the value of $\lim _{x\to \infty }\left(1+\frac{4}{\ln x}\right)^{\ln\left(x^2+x\right)}$,"So this is my attempt to the question: Let, $$y=\left(1+\frac{4}{\ln x}\right)^{\ln\left(x^2+x\right)}$$ $$\ln y=\ln(x^2+x)\ln\left(1+\frac{4}{\ln x}\right)$$ $$\lim _{x\to \infty }\ln y=\lim _{x\to \infty }\ln(x^2+x)\ln\left(1+\frac{4}{\ln x}\right)$$ Solving for the right side, since it is in the form of $\infty\cdot0$ , we can change this into $$\lim _{x\to \infty }\frac{\ln(x^2+x)}{\frac{1}{\ln\left(1+\frac{4}{\ln x}\right)}}$$ So that we can then use the L'Hopital rule. However, after using the L'Hopital rule once, I found the form to be even more complicated and still becomes an indeterminate form. Is there another way to solve this problem?","['limits', 'calculus']"
4590474,"Why is $f(x,y)$ discontinuous at $(x,y)=0$","$\displaystyle f(x,y)=\begin{cases} \displaystyle\frac{x^3+y^3}{x-y},
 x\ne y \\ 0, x=y\end{cases}$ The questions is to check continuity at $(x,y)=(0,0)$ for $f(x,y)$ My attempt: $f(x,y)$ is continuous at $(0,0)$ iff $\displaystyle \lim_{(x,y) \to(0,0)} f(x,y) = f(0,0)$ Now for $y=mx,m\ne 1$ $\displaystyle \lim_{{(x,y) \to(0,0)}_{y=mx}} f(x,y) = \lim_{{(x,y) \to(0,0)}_{y=mx}} \frac{x^3+y^3}{x-y} =\lim_{x \to 0} \frac{x^3+m^3x^3}{x-mx}= \lim_{x \to 0} x^2\left(\frac{1+m^3}{1-m}\right)=0$ Now for $y=x$ $\displaystyle \lim_{{(x,y) \to(0,0)}_{y=x}} f(x,y) = \lim_{{(x,y) \to(0,0)}_{y=x}} 0 = 0$ From the above we can say $\displaystyle \lim_{(x,y) \to(0,0)} f(x,y)=0$ $\implies \displaystyle \lim_{(x,y) \to(0,0)} f(x,y)=f(0,0)$ So that means $f(x,y)$ is continuous at $(0,0)$ But the source from which I got this question says it's discontinuous at $(0,0)$ I am not able to find my mistake Although I can verify limit using $\epsilon,\delta$ method but I couldn't So I need your help to prove me correct/wrong Also using $\epsilon,\delta$ method would be apreciated","['limits', 'multivariable-calculus']"
4590491,$\bigcap_i \operatorname{Ann} \tilde{(M_i)} = \operatorname{Ann} \tilde{M}$?,"I’m reading Görtz–Wedhorn’s Algebraic Geometry , proof of the Proposition 7.34, and some question arises. Let $X$ be a scheme and $\mathcal{F}$ a quasi-coherent module of finite type. Define the annihilator $\newcommand{\Ann}{\operatorname{Ann}} \Ann \mathcal{F}$ of $\mathcal{F}$ as the kernel of the canonical homomorphism $\newcommand{\OX}{\mathcal{O}_X}\OX \to \mathcal{H}\mathrm{om}_{\OX}(\mathcal{F}, \mathcal{F})$ (Görtz–Wedhorn, 7.2.12, page 193). By Annihilator of a coherent sheaf, on affine subsets. , for each open subset $U$ of $X$ , $$
  \Ann(\mathcal{F})(U) := \{f \in \OX(U) \mid \forall x \in U, \forall m_x ∈ F_x, f_x \cdot m_x=0\} \,.
$$ Now let $X := \operatorname{Spec}A$ be an affine scheme and let $\mathcal{F} = \tilde{M}$ be a quasi-coherent module, where $M$ is an $A$ -module generated by a finite number of elements $t_1, \dotsc, t_n$ . Let $M_i := \langle t_i \rangle_A$ (the $A$ -module generated by the $t_i$ ). Question : Then, $$
  \bigcap_i \Ann \tilde{(M_i)} = \Ann \tilde{M} \,?
$$ Here, the intersection $\bigcap_i \mathcal{F}_i$ of a family ( $\mathcal{F}_i)_{i \in I}$ of $\OX$ -submodules of $\mathcal{F}$ is defined as the kernel of the canonical homomorphism $\mathcal{F} \to \prod_{i \in I} \mathcal{F} / \mathcal{F}_i$ (Görtz–Wedhorn, page 174). So, $$
  \bigcap_i \Ann \tilde{M_i}
  := \ker\biggl( \OX \xrightarrow{\psi} \prod_{i=1}^n \OX / \Ann\tilde{M_i} \biggr) \,.
$$ My first attempt is, for each affine open $U \subset X$ , I am trying to show that $\bigcap_i \Ann \tilde{(M_i)}(U) = \Ann \tilde{M}(U)$ . $\subseteq$ : Let $s \in \bigcap_i \Ann \tilde{(M_i)}(U)$ . To show $s\in \Ann \tilde{M}(U)$ , as above remark, we must show that for each $x \in U$ and $m_x \in M_x = M_{p_x}$ , it follows that $s_x \cdot m_x = 0$ . We have that $\psi_U(s) = 0$ . This means that $\psi_{U,i}(s) \in (\OX / \Ann \tilde{M_i})(U)$ is zero for all $i$ . Case 1: $(\OX / \Ann \tilde{M_i})(U) = \OX(U) / (\Ann \tilde{M_i})(U)$ . In this case, \begin{align*}
  \psi_U(s)
  &= ( \psi_{U,1}(s), \dotsc, \psi_{U,n}(s) ) \\
  &= ( s + \Ann(\tilde{M_1})(U), \dotsc, s + \Ann(\tilde{M_n})(U) ) \\
  &= (0, 0, \dotsc ,0) \,.
\end{align*} So, $s \in \Ann(\tilde{M_i})(U)$ for all $i$ . Again, by the above remark, this means that “for all $x \in U$ and $n_x \in (\tilde{M_i})_x = M_{i, p_x}$ , it follows that $s_x \cdot n_x =0$ for all $i$ ”. And this is equivalent to “for all $x \in U$ and $m_x \in M_{p_x}$ , it follows that $s_x \cdot m_x = 0$ ”, which is our desired result. $\supseteq$ : Similar argument (?), maybe reversing direction of deduction. An issue that makes me difficult is, since $U \mapsto \mathcal{F}(U) / \mathcal{G}(U)$ ( $\mathcal{G}$ is an $\OX$ -submodule of $\mathcal{F}$ ) is not sheaf in general, Case 1) does not always hold. Perhaps, assuming that each $\Ann M_i$ is quasi-coherent, Case 1) holds? Perhaps, in general, the following is true? Let $\mathcal{G} \subseteq \mathcal{F}$ be quasi-coherent sheaves on a scheme $X$ . Then for each affine open $U := \operatorname{Spec} B \subset X$ , $(\mathcal{F} / \mathcal{G})(U) \cong \mathcal{F}(U) / \mathcal{G}(U)$ ? I found an associated question Quotient, Tensor Quasicoherent Sheaves on Affine Open Subsets and do not understand completely well. Can we provide a more explicit proof for above claim? C.f. More and more think about sheafification, it seems that situations becomes more complicated. Is there method to handle this problem? Is there a way to bypass considering sheafification? Brutal force argument is needed? Can anyone help? I would like to learn how to handle an issue that maybe involves sheafification. EDIT: Further Progress! I’ve been trying to prove the above claim, based on the above linked question. Let me write my own proof: I think that one of key point to prove the above claim is using the next theorem: Theorem: Let $\mathcal{F}$ be a sheaf and $\mathcal{G}$ be a presheaf and $\mathcal{B}$ be a topological basis of $X$ . If $\mathcal{F}(U) = \mathcal{G}(U)$ for all $U \in \mathcal{B}$ , then $\mathcal{F} = \mathcal{G}^{\dagger}$ (the sheafification of $\mathcal{G}$ ). Let’s go back to the above claim. First, note that $\{ D(f) \}_{f \in B}$ forms a basis for $U := \operatorname{Spec} B$ . We show that for each $D(f)$ , $$
  (\mathcal{F} / \mathcal{G})^{\mathrm{pre}}|_U( D(f) )
  = \widetilde{(\mathcal{F}(U) / \mathcal{G}(U))}( D(f) )
$$ (where $\widetilde{(\mathcal{F}(U) / \mathcal{G}(U))}$ is the associated $\mathcal{O}_U$ -module).
This is true since \begin{align*}
  (\mathcal{F} / \mathcal{G})^{\mathrm{pre}}|_U( D(f) )
  &= (\mathcal{F} / \mathcal{G})^{\mathrm{pre}}( D(f) ) \\
  &:= \mathcal{F}(D(f)) / \mathcal{G}(D(f)) \\
  &= \mathcal{F}(U)_f / \mathcal{G}(U)_f \\
  &= (\mathcal{F}(U) / \mathcal{G}(U))_f \\
  &=: \widetilde{(\mathcal{F}(U) / \mathcal{G}(U))}(D(f)) \,.
\end{align*} (The third equality is by the quasi-coherence of the $\mathcal{F}$ and $\mathcal{G}$ (Görtz–Wedhorn, Theorem 7.16-(iv)) and the fourth equality is by the fact that “localization commutes with quotient of modules”.) Second, so, by the above theorem, $\widetilde{(\mathcal{F}(U) / \mathcal{G}(U))} = ((\mathcal{F} / \mathcal{G})^{\mathrm{pre}}|_U )^{\dagger} = (\mathcal{F} / \mathcal{G})|_U$ (sheafification commutes with restriction). So, third, we have that $$
  (\mathcal{F} / \mathcal{G})(U)
  = (\mathcal{F} / \mathcal{G})|_U(U)
  = \widetilde{(\mathcal{F}(U) / \mathcal{G}(U))}(U)
  = \mathcal{F}(U) / \mathcal{G}(U) \,.
$$ Is my argument correct? Are there any parts that I made mistake? Can anyone see? C.f. This question originates from following proof of the Görtz–Wedhorn, Proposition 7.24: Why is the underlined statement true? If our question is true, then since finite intersections of quasi-coherent submodules is again quasi-coherent, is suffices to show that each $\Ann \tilde{M_i}$ is quasi-coherent; i.e., we may assume that $M$ is generated by single element.",['algebraic-geometry']
4590518,"If it is known that $\lim _{h\to 0}(\frac{f(h)}{h}-f'(0))=0$, does this implies that $\lim _{h\to 0}(\frac{\frac{f(h)}{h}-f'(0)}{h})=0$?","So I am working on a problem as such: Suppose that $f$ is a function such that $f(0)=0$ and $f'(x)$ and $f''(x)$ exist for any real number $x$ . Let $g$ be a function  with $g(0)\!=\!\!f'(0)$ and $g(x)\!=\!\!f(x)/x$ for $x\neq0$ . Given that $g(x)$ is continuous for any real number $x$ ,
prove that $g'(x)$ exists for all $x\in\Bbb R$ . My approach to this problem is to divide the case into two case, which is for $x=0$ and $x\neq0$ . The function $g(x)$ can be written into a piecewise as such: $$g(x)=\begin{cases} 
      f(x)/x \;\;\;\;\;\; x\neq0 \\
      f'(0)\;\;\;\;\;\;\;\;\;x=0
   \end{cases}$$ For $x\neq0$ : $$g(x)=f(x)/x$$ $$g'(x)=\frac{f'(x)(x)+f(x)}{x^2}$$ Since $f'(x)$ exists for all real number $x$ , this implies that $f(x)$ exists for all real number $x$ . And since $x$ can never be $0$ , then $g'(x)$ exists for all $x\neq0$ . My question is for the case when $x\neq0$ My approach is to use the definition of limit, which is to find out whether $\;\lim\limits_{h\to 0}\dfrac{g(0+h)-g(0)}h\;$ exists or no. If it does, then the limit exists. Using what we know from the question,we can solve this equation: $$\lim\limits_{h\to 0}\dfrac{\frac{f(h)}{h}-f'(0)}{h}$$ And this is where my question comes from. Since we know that $g(x)$ is continous, we can say that $\;\lim\limits_{h\to 0}\left(\dfrac{f(h)}{h}-f'(0)\right)=0$ . But does this imply that $\;\lim\limits_{h\to 0}\dfrac{\frac{f(h)}{h}-f'(0)}{h}=0\;$ and hence $g'(0)$ exists? I am thinking of using the L'Hopital rule for the case $\mathbf{0/0}$ , but is it appropriate in this problem? I have tried it but seems that it does not take me anywhere. If no, how can I solve this problem?","['limits', 'calculus', 'derivatives', 'continuity']"
4590569,Circles packed between $y=1/x$ and $y=0$ in the first quadrant: What is the radius of the $n$th circle?,"I think question title is obvious.
assume we have a rectangular hyperbola chart.and we draw largest circle which fits under $y=1/x, y=0$ and $x=0$ .
then we continue drawing circles which are tangent to previous circle, $y= 1/x$ and $y=0$ .
Question is: what is radius of $n$ -th circle. Radius of first circle is $2 - \sqrt{2}$ . but even calculating second radius is impossible. It is over a month I am thinking on it. PS 1:
I try to find the line which connect all of circle's centers.
if we call if f(x), it is clear that $$\lim_{x\to \infty}\frac{f(x)}{(1/x)}=\frac{1}{2}$$ another thing is following formula. the centers of circles that are in equal distance from y = 0 and first circle. (second circle center in on this line:) $$y=\frac{\left(r_{0}-x\right)^{2}}{4r_{0}}$$ $$r_{0}=2-\sqrt{2}$$ and I get 6 formulas as follows:
there are three points which are hit points of three curves.
first circle and y=1/x hit point is (1,1)
but two other points should be found. each of points satisfy curve formulas. also in eqach derivative of both curves are equal. and also, distance of two first circle's centers is r0+r1 which r1=y1.","['circles', 'geometry']"
4590640,Show that function's inverse is not continuous at a point,"I have the following problem: Show a bijective function $f$ where $f'(0)$ is equal to $1$ and where the $f^{-1}(x)$ is not cont. at $f(1)$ . So far, I know that the derivative of an inverse function $f^{-1}(f(x))$ is $(f^{-1})'(f(x))=\frac{1}{f'(f^{-1}(f(x)))}=\frac{1}{f'(x)}$ , and that $f^{-1}$ not being continuous at $f(0)$ would mean that it is not differentiable there either. Don't know where to go from there however. Any help would be appreciated!","['calculus', 'derivatives', 'inverse']"
4590651,Is there anything special about the matrix $\frac{1}{2}\begin{pmatrix} e^{ix} & ie^{-ix} \\ ie^{ix} & e^{ix} \end{pmatrix}$ related to trigonometry?,"You can write the trigonometric formulas as such: $$ \cos x = \frac{e^{ix}+e^{-ix}}{2} $$ $$ \sin x = \frac{ie^{-ix}-ie^{ix}}{2} $$ This means you can write the rotation matrix as follows: $$  \begin{pmatrix}
\cos x & -\sin x \\
\sin x & \cos x
\end{pmatrix}
=
\frac{1}{2}\begin{pmatrix}
e^{ix}+e^{-ix} & ie^{ix}-ie^{-ix} \\
ie^{-ix}-ie^{ix} & e^{ix}+e^{-ix}
\end{pmatrix}
=
\frac{1}{2}\begin{pmatrix}
e^{ix} & ie^{ix} \\
ie^{-ix} & e^{ix}
\end{pmatrix}
+
\frac{1}{2}\begin{pmatrix}
e^{-ix} & -ie^{-ix} \\
-ie^{ix} & e^{-ix}
\end{pmatrix}
$$ Do those individual matrices have a special name? Or special properties? EDIT: you could make these matrices even nicer as follows: $$z=e^{ix}$$ $$\frac{1}{2}\begin{pmatrix}
z & z' \\
z'^{-1} & z
\end{pmatrix}
+
\frac{1}{2}\begin{pmatrix}
z^{-1} & (z^{-1})' \\
-z' & z^{-1}
\end{pmatrix}$$","['trigonometry', 'linear-algebra']"
4590677,"How might I have anticipated that $\frac14(\sqrt{5+2\sqrt5}+\sqrt{10+2\sqrt{5}})$ simplifies to a single surd (namely, $\frac14\sqrt{25+10\sqrt{5}}$)?","This is perhaps a silly question related to calculating with surds. I was working out the area of a regular pentagon ABCDE of side length 1 today and I ended up with the following expression : $$\frac{\sqrt{5+2\sqrt5}+\sqrt{10+2\sqrt{5}}}{4}$$ obtained by summing the areas of the triangles ABC, ACD and ADE. I checked my solution with Wolfram Alpha which gave me the following equivalent expression : $$\frac{\sqrt{25+10\sqrt{5}}}{4}$$ I was able to show that these two expressions are equivalent by squaring the numerator in my expression, which gave me $$15+4\sqrt5+2\sqrt{70+30\sqrt5},$$ and then ""noticing"" that $$\sqrt{70+30\sqrt5}=\sqrt{25+30\sqrt5+45}=5+3\sqrt5.$$ My question is the following : how could I have known beforehand that my sum of surds could be expressed as a single surd, and is there a way to systematize this type of calculation ? I would have liked to find the final, simplest expression on my own without the help of a computer. Thanks in advance !","['algebra-precalculus', 'area', 'geometry']"
4590808,Derivative of a multivariable composite functions,"Let $\;f(x,y,z)=xz+xy+yz\;$ and $\;g(t)=(e^t,\cos t,\sin t)$ . The thing is that I want to calculate $(f\circ g)'(1)$ . Previously, I was asked to calculate $g'$ and $\nabla f$ , so I do not know if that is necessary to calculate $(f\circ g)'(1)$ or I can simply composite the functions and apply chain rule for $t$ .","['multivariable-calculus', 'chain-rule']"
4590833,Can we construct a sine approximating function $f$ with $f'(n\pi) = 0$ and infinitely differentiable?,"Can we construct a sine approximating function $f(t) \approx \sin(t)$ with $f'(n\pi) = 0$ and infinitely differentiable? If this requirement alone makes it too easy - which I suspect it will be - maybe we can add the requirement that we want monotonicity on each interval $$[n\pi/2,(n+1)\pi/2]$$ Edit: I am thinking of considering an error $$E(f) =\left(\int_{-\pi}^{\pi}|f(t)-\sin(t)|^kdt\right)^{1/k}$$ And to show that we can find an $f$ satisfying $E(f) \leq \epsilon$ (and infinitely differentiable and piecewise monotonic) for every $\epsilon > 0$ .
Of particular interest would be for $k = 1,2$ . Total variation and mean square. Own work My suspicion is that if we can, then Fourier series would be a reasonable approach as such a function would be periodic with same period as the sine has.","['fourier-analysis', 'real-analysis', 'numerical-methods', 'trigonometry', 'soft-question']"
4590834,Finding the determinant of a matrix generated by an equation,"I have an equation that gives each entry of a square matrix of size $d+1 \times d+1$ with row $i$ and column $s$ . $$(-1)^{i+1}\sum_{j=0}^{s-1}\sum_{\ell=0}^{3(d-j)}{s-1\choose j} {3(d-j)\choose \ell } {3j\choose 3i-3-\ell}(-2)^\ell (-1)^{j}$$ So given $d=2$ we can get a matrix $$\begin{pmatrix}1 & 0 & 0\\
160 & 171 & 162\\
64 & 72 & 81\end{pmatrix}$$ My goal is to find the formula for the determinant of the matrix for any given $d$ . I was able to get the invariant factors by looking at the matrix's Smith normal form and create a sequence log 3 from it. $$\begin{pmatrix}1 & 0 & 0\\
0 & 9 & 0\\
0 & 0 & 243\end{pmatrix} =
\begin{pmatrix}3^0 & 0 & 0\\
0 & 3^2 & 0\\
0 & 0 & 3^5\end{pmatrix}
\text{which gives the sequence } \{0,2,5 \} \text{ log 3}.$$ If I stack the sequence of numbers as $d$ increases I can get this triangle, and removing the zeroes... My thought process is that if I can find a formula that will give the sum of each line then I'll have the determinant log 3. Looking through the online integer sequence database returned nothing. I did find regular patterns when taking the difference of each element in the sequence. Of note is that the sum of each row in this subsequence is $3d-1$ . Can someone let me know if I'm going about this the wrong way? I've been staring at triangles all week and am going crazy.","['elementary-number-theory', 'linear-algebra', 'discrete-mathematics']"
4590837,A Taylor expansion,"I have asked a similar question elsewhere but there has remained a small gap for me:
How can I derive this approximate equation via the Taylor expansion: $$\frac{y(x+h)-y(x-h)}{2}-\frac{2h}{12}(y'(x+h)+4y'(x)+y'(x-h))=-\frac{1}{80}h^5y^{(5)}(x)+O(h^7) \ ?$$ Do we have to assume that $$y(x+h)-y(x-h)=2hy'(x)+\frac{1}{3}h^3y^{'''}(x)+\frac{1}{360}h^5y^{(5)}(x)+O(h^7)$$ or does this follow ? I think that it is too trivial to ask but the calculation doesn't follow for me.","['functions', 'derivatives', 'taylor-expansion', 'real-analysis']"
4590839,"Is there any function that has a derivative almost everywhere, but where the derivative function is everywhere discontinuous?","This question asks whether there exists a function that has a derivative that is discontinuous everywhere. Is there any function that has a derivative almost everywhere, but where the derivative function is everywhere discontinuous, for the definition of continuity that takes into account only the inputs where the derivative is defined?","['limits', 'calculus', 'derivatives', 'continuity']"
4590857,Lebesgue-Radon-Nikodym: How to write $\bigcup_n (A_n \cap B_n)$?,"I am learning measure theory and often it helps to know some (basic) set theory. For e.g. one can write union of measurable sets as a disjoint union and then use countable additivity of the measure. I am reading the proof of special case of Lebesgue-Radon-Nikodym theorem from Folland: We have two $\sigma$ -finite measures $\mu$ and $\nu$ on $X$ and the space $X$ is written as $X=\cup_n E_n=\cup_n F_n$ of increasing union with both $\mu(E_n)$ and $\nu(F_n)$ finite for all $n$ . From here, we want to write $X=\cup_n X_n$ where $X_n$ are disjoint with both $\mu(X_n)$ and $\nu(X_n)$ finite for all $n$ I think $X_n= (E_n\cap F_n)\cap {G_n}^c$ , where $G_n=(\cup_{k=1}^{n-1} (E_k \cap F_k))$ . Clearly $X_n $ are disjoint. But how do I show that $\cup_n X_n=X$ ? In general, what is $\cup_n (A_n \cap B_n)$ ?","['elementary-set-theory', 'measure-theory', 'lebesgue-measure']"
4590867,Maximum number of negative coefficients $p(x)^2$ can have,"Here is the problem $2$ of the 83rd William Lowell Putnam Mathematical Competition. Let $n$ be an integer with $n \ge2$ . Over all real polynomials $p(x)$ of degree $n$ , what is the largest possible number of
negative coefficients of $p(x)^2$ ? I attempted to look at the case where $n = 2$ on WA and found that no matter what coefficients of $p(x) = ax^2 +bx+c$ the maximum negative coefficients of $p(x)^2$ is $2$ . I started to think it is the case that the maximum is just equal to n because we can simply alternate the signs to guarantee this result. But when I tried $n=3$ I found for example that $(x^3 -x^2 -x +1)^2 = x^6 -2x^5 -x^4 +4x^3 -x^2-2x+1$ has 4 negative coefficients and it seems that if we have negative coefficients for $x^2$ and $x$ this will be the case. I'm pretty much stuck here and any help is appreciated. Perhaps combinatoric may help here.","['contest-math', 'combinatorics', 'polynomials']"
4590899,Residue Theorem for Real Integral: Where did I go Wrong?,"So the integral is $$\int_{0}^{\infty} \frac{\sqrt{x}}{x^2 + 4x + 5} d x.$$ I did keyhole integration avoiding the positive real axis. Let this be $C$ . I defined the integrand as a complex valued function, and found that the poles are at $-2-i$ and $-2+i$ .
Computing the residues, we have $$Res(f, -2+i) = -\frac{\sqrt{-2-i}}{2i}$$ and $$Res(f, -2+i) = \frac{\sqrt{-2+i}}{2i}.$$ So then $\int_C f(z) = 2\pi i (\frac{\sqrt{-2+i}}{2i}-\frac{\sqrt{-2-i}}{2i}) = \pi (\sqrt{-2+i}-\sqrt{-2-i}).$ Now, after doing the keyhole part, the integral over the large circle and the small circle goes to 0 and so the only thing left are the integral over the line segment connecting the small circle to the large circle at angle $\epsilon$ and the integral over the line segment connecting the large circle to the small circle at angle $2\pi - \epsilon$ . Letting the radius of the small circle go to 0 and the radius of the large circle to infinity, we find that the two integrals are equal and $$\int_C f(z) dz = 2\int_{0}^{\infty} f(x) dx.$$ But $$\int_{0}^{\infty} f(x) dx = \frac{\pi}{2} (\sqrt{-2+i}-\sqrt{-2-i}),$$ which is an imaginary number. I've been trying to find my mistake but I can't seem to. Where did I go wrong? Wolfram tells me that the answer is $\sqrt{\frac{1}{2}(\sqrt{5} - 2)}\pi$ . My guess is that it's somewhere in the residue theorem step... Please help!","['integration', 'complex-analysis', 'calculus', 'residue-calculus']"
4590911,Number of Triangle Free Graph with prescribed number of edges,"Let $f(n, e)$ be the number of triangle-free graphs on $n$ vertices and $e$ edges. From empirical evidence, I am motivated to make the following conjecture. Conjecture : $f(n,e)$ is unimodal in $e$ . In other words, for each $n$ there exists a mode $m = m(n)$ such that $f(n, e) \leq f(n, e + 1)$ if $e < m$ , and $f(n, e) \geq f(n, e + 1)$ if $e \geq m$ . I am wondering if this conjecture has been mentioned in literature before since it is natural for such sequences to display some unimodality. Here's another way to phrase the question. Conjecture : Let $G$ be the $3$ -uniform linear hypergraph whose vertex set is $\binom{[n]}{2}$ , and whose edge set is $\{\{i,j\},\{j, k\}, \{k, i\}\}$ for distinct $i,j,k\in [n]$ . Then the independence polynomial $Z_G(x)$ is unimodal. It is known that if we instead consider the $2$ -uniform graph with the same vertex set and edge set $\{\{i,j\},\{j, k\}\}$ , then $Z_G(x)$ is equal to the matching polynomial $m_G(x)$ of $K_n$ , which is unimodal and log-concave. Edit: Cross-posted to overflow .","['extremal-graph-theory', 'independence', 'combinatorics']"
4590931,Prove that there do not exist a Lebesgue measurable set with the following property,"Prove that there do not exist a Lebesgue measurable set $A$ with the following property: $A\subset \mathbb R$ such that for all $0 < a < 1$ , $m(A\cap [0,a] ) = a/2$ . My attempt is the following and I'm wondering if it makes sense. Assume by contradiction that there exists a Lebesgue measurable set $A\subset\mathbb R$ such that $m(A\cap [0,a])=a/2$ for all $a\in(0,1)$ $(\star)$ . Note that for all $0<a<b<1$ we have that $m(A\cap [0,b])- m(A\cap [0,a])= m(A\cap (a,b))$ .
This is because for any two measurable sets $E,F$ we have that $m(E\cup F) -m(E)= m(F)- m(E\cap F)$ . Using the condition $(\star)$ , we have that $$m(A\cap (a,b)) = b/2-a/2= m((a,b))/2.\quad (\bullet)$$ Now, cover the interval $[0,1/2]$ using (almost disjoint) intervals $[1/2-1/3= 1/6,1/2], [1/6-1/4,1/6],\dots$ and denote each by $I_n$ for $n=1,2,\dots$ . These are intervals of length $1/(n+2)$ that only overlap at points which have measure zero. Then, by the additivity of measure we have that $$m(A\cap \cup_n I_n)=\frac{1}{2}\sum_n m(I_n).$$ Note that $\cup_n I_n= (0,1/2]$ and hence the left hand side of the above equation is $$m(A\cap \cup_n I_n)= m(A\cap [0,1/2])= 1/4.$$ Also, note that $m(I_n)= 1/(n+2)$ , and so the right hand side of the equation above is $$\frac{1}{2}\sum_n m(I_n)=\infty$$ being the harmonic sum. Hence, we get a contradiction. Edit : As pointed out in a comment the proof above is wrong. In a comment, it was suggested to use Lebesgue differentiation theorem.
Here's the Lebesgue differentiation theorem from Folland:
Suppose $f\in L^1_{loc}$ , then for a.e $x$ we have $$\lim_{r\to0}\frac{1}{m(E_r)}\int_{E_r}f(y)dy= f(x)$$ for every family $\{E_r\}_{r>0}$ that shrinks nicely to $x$ . By definition, a family $\{E_r\}_{r>0}$ shrinks nicely to $x$ if $E_r\subset B(r,x)$ and there is a constant $\alpha$ independent of $r$ such that $m(E_r)>\alpha m(B(x,r))$ . Fix an $a\in (0,1)$ . Let $E_r= (a-r/2,a+r/2)$ for $r>0$ , to be a family that shrinks nicely to $a$ . Then, $m(E_r)=r$ . Here, as suggested in the comment, we take $f(y)= \chi_{A\cap [0,a]}(y)$ . Then, the left hand side of the theorem becomes $$\lim_{r\to0}\frac{1}{m(E_r)}\int_{E_r}f(y)dy= \lim_{r\to0}\frac{1}{r}\int_{E_r}\chi_{A\cap [0,a]}(y)dy = \lim_{r\to0}\frac{1}{r}m({A\cap [0,a]\cap E_r}). $$ We note that ${A\cap [0,a]\cap E_r}= A\cap (a-r/2,a+r/2)$ and so $m({A\cap [0,a]\cap E_r})= m((a-r/2,a+r/2))/2= r/2$ (we used the formula $(\bullet)$ derived in the wrong solution section above.) Therefore, the left hand side of the theorem is $1/2$ . The right hand side however is \chi_{A\cap [0,a]}(a)= 1$. Therefore, we get a contradiction.","['measure-theory', 'solution-verification', 'lebesgue-measure', 'real-analysis']"
4590952,Analogous formula for finding “non-planar graphs” using defined faces instead of edges in $\Bbb R^3$?,"Given a graph, along with a set describing which edges are connected to form faces, how could we determine whether it is embeddable in 3D Euclidean space, a.k.a $\Bbb R^3$ ? What formula applies here? Assume faces can be curved in the same way that lines can be curved in a graph. Imagine a soccer ball with a hole that cut out one polygon—in the same way, what I’m imagining doesn’t require the edges formed by the vertices of this soccer ball to all form faces. I was reading about Euler’s formula for planar graphs, wherein v-e+f=c+1 (the number of vertices minus the number of edges plus the number of faces equals the number of connected components plus one). However, the usage of faces doesn’t work here, because there are an infinite number of planes in $\Bbb R^3$ , and the part of Euler’s proof that uses the number of faces increasing when a region becomes bound in by points doesn’t work here.
Since nonplanar graphs can be embedded in $\Bbb R^3$ , and the formula only applies to planar graphs, Euler’s formula can’t be the right first step. This would probably be defined by a hypergraph, but I don’t know much about them. Edit : Assume the new definition of “face” to be a set of three and only three edges. Any number of edges could technically be a face but only three are necessarily coplanar under shifts of the points. Faces with more edges can be subdivided into triangles. I realized earlier that if there exists an edge on the structure that has only one face connected to it, a volume is not enclosed by that structure. I found the contrapositive of this, namely, that a volume is enclosed if, for all edges in the structure, each edge has a greater-than-one number of faces connected to it. I also noticed that two volumes are enclosed (leaving three total when considering the “outside” region) when there exists an edge with three unique faces connected to it—assuming that the structure already satisfies the property of every edge having at least two unique faces connected to it. I don’t know how to prove this, but I can picture it. Edit : Two views about the number of edges a face should have: I know that an edge on a graph is allowed to curve, so perhaps faces on these edged-volumes I’ve defined should be able to as well. However, edges on regular graphs only ever connect two vertices, so faces on these volumes should probably be restricted to possessing the minimum number of edges that a face can have. Important Edit: Essentially, what I want to find is the simplest example of a set of surfaces connected at edges that cannot be embedded in a 3-dimensional Euclidean space without planes crossing.","['graph-theory', 'geometry', '3d', 'planar-graphs']"
4590997,"Without calculus, what is the maximum value of the rational function : $f(t)= \frac {30t} {t^2 + 2}$","Source : Stewart, Precalculus . The original question is to graph the function : $f(t)= \frac {30t} {t^2 + 2}$ . Desmos construction : https://www.desmos.com/calculator/19kybcl3hc It can be shown that $f(x)$ tends to $0$ as $t$ goes to infinity ( in both directions), so $y= 0$ is a horizonal asymptote. Also, near zero, the function looks like $y = t$ . The part of the question I am interested in here is : what is the
maximum value of $f(t)$ , not using calculus? Following a method shown by Sybermath ( ' Finding the maximum value of a rational function"", YT ) , I attempted this approach: (1) Set $f(t)= M$ , that is $f(t)= \frac {30t} {t^2 + 2}=M$ .
The question becomes: what is the maximum value of $M$ ? (2) $\frac {30t} {t^2 + 2}=M$ $ \iff 30t = M(t^2 +2)$ $\iff 30t = Mt^2 +2M$ $\iff -Mt^2 +30 t -2M = 0$ (3) Since we only want real values of $t$ , we require $\Delta = b^2 - 4 ac  \geq 0 $ $\iff 30^2 - 4(-M)(-2M) \geq 0 $ $\iff 30^2 -8M^2 \geq 0$ $\iff M^2 \leq 30^2/8$ $\iff  \sqrt{M^2} \leq \sqrt {30^2 / 8}$ $\iff |M|\leq \sqrt {30^2 / 8}$ $\iff -\sqrt {30^2 / 8} \leq M \leq \sqrt {30^2 / 8} \approx 10.6$ (3) So, the maximum value of $f(x)= M $ is $\sqrt {30^2 / 8} \approx 10.6.$ Is this answer correct? Are there other , desirably quicker methods to answer the question ( without calculus)?","['maxima-minima', 'algebra-precalculus', 'graphing-functions', 'rational-functions']"
4591006,What is the probability of sharing a birthday if a year has an infinite number of days?,"Here is the problem: Suppose that there are $k$ people. Each of them independently picks a uniformly random number from the set $\{1, 2,...,n\}$ . We say that a collision happens if there exist two people picking the same number. Let $k = \lceil n^\beta\rceil$ , where $\beta$ is  some  constant  that  does  not  change  with $n$ . Prove that there is a constant $\beta_0\in(0,1)$ such that if $\beta>\beta_0$ , then a collision happens with probability $1$ when $n\to\infty$ ; and if $\beta<\beta_0$ , then a collision happens with probability $0$ when $n\to\infty$ . Also find the value of $\beta_0$ . Here is my attempt: The sample space is $\Omega=\{1,2,\ldots,n\}^k$ . Let $A$ be the event that a collision happens. Then, $$
    \begin{align}
    \mathbf P(A)&=1-\frac{n(n-1)(n-2)\ldots(n-k+1)}{n^k}\\
    &=1-\frac{n-1}n\frac{n-2}n\ldots\frac{n-k+1}n\\
    &=1-\left(1-\frac1n\right)\left(1-\frac2n\right)\ldots\left(1-\frac{k-1}n\right)\\
    &=1-\prod_{i=1}^{k-1}\left(1-\frac in\right)\\
    &\ge1-\prod_{i=1}^{k-1}e^{-\frac in}\\
    &=1-e^{-\sum_{i=1}^{k-1}\frac in}\\
    &=1-e^{-\frac{k(k-1)}{2n}}\\
    &=1-e^{-\frac{k^2}{2n}}e^{\frac k{2n}}
    \end{align}
$$ We used the inequality $1-x\le e^{-x}$ . Thus, we have obtained a lower bound of $\mathbf P(A)$ . Let $k=n^\beta$ . Then, $\mathbf P(A)\ge1-e^{-\frac{n^{2\beta}}{2n}}e^{\frac {n^\beta}{2n}}$ . For any $\beta\in(0,1)$ , we have $\lim_{n\to\infty}e^\frac {n^\beta}{2n}=1$ . If $\beta>\frac12$ , then $\lim_{n\to\infty}e^{-\frac{n^{2\beta}}{2n}}=0$ , so the lower bound goes to $1$ , and $\lim_{n\to\infty}\mathbf P(A)=1$ . If $\beta<\frac12$ , then $\lim_{n\to\infty}e^{-\frac{n^{2\beta}}{2n}}=1$ , so $\lim_{n\to\infty}\mathbf P(A)\ge0$ . I guess that I have to find a upper bound for the probability which goes to zero if $\beta<\frac12$ , but I have been unable to find it. Here are my questions: Does such an upper bound exist? If yes, how can I find it? For simplicity, I used $k=n^\beta$ , which is not valid because $k$ must be a positive integer. What happens if I use $k=\lceil n^\beta\rceil$ ? When taking the limit, do we have to rely on the continuity property of probabilities? We cannot define a sequence of events $A_1,A_2,\ldots,A_n$ on the same probability space (although we could if it were $A_1,A_2,\ldots,A_k$ ).","['limits', 'birthday', 'probability']"
4591041,Optimising a business strategy,"My brother in year 8 was given this scenario: ""You sell chairs and make sales at monthly intervals. You start with $16,000. Each month, 17000 people buy chairs, The market average price for chairs is $72. Chairs cost $40 each when you buy 400 or more, and 35 dollars each when you buy 1000 or more. Premium chairs cost 45 dollars each when you buy 400 or more, and 40 dollars each when you buy 1000 or more. Your business sets the price of chairs. The price must be between $24-$ 150. For every $4 under the market average, your business gains 0.5% of the market. For each $0.50 over the market average, your business loses 0.05% of its customers when selling standard chairs, and 0.01% of its customers when selling standard chairs, and 0.01% of its customers when selling premium chairs. For every 1000 chairs sold in a month, your business must pay an employee $5000.10% of sales must go to the government for GST. Your business may take out one loan at a time. You can only loan out as much money as you currently have. The loan compounds monthly at a rate of 18% per month. You must pay the entire loan off in a single month. The loan will continue to compound until you repay it. Make the most profit possible within a year "" I think the purpose of this was for them to try different strategies and play around with them to see what works best. However, I'm wondering how this could be optimized to give the best possible result. As the problem is very convoluted I assume one could maybe use a program to simulate different situations however I wouldn't know how to do this. Feel free to simplify the problem if you wish to try it (eg. Don't use GST, premium chairs, or loans). I'm interested to see how a situation with multiple variables (the price each year) can be optimized.","['word-problem', 'finance', 'multivariable-calculus', 'calculus', 'optimization']"
4591151,How to find the value of $\lim_{n\to\infty}\int_{1}^{e}(\ln x)^n\ dx$,"In my textbook, the following relation is defined $I_n=\int_{1}^{e} (\ln x)^n\ dx$ . The first part of the exercise asks me to find a recurrence relationship, which I did: $$I_n = (n-1)(I_{n-2}-I_{n-1})$$ Then, as the title says, I need to find $$\lim_{n\to\infty}\int_{1}^{e}(\ln x)^n \ dx$$ I know the value is $0$ , but I am stuck at actually solving it. So far, I tried to find two other formulas whose limits are $0$ and squeeze mine between them, but this idea didn't help me much. I'm thinking that I could use the recurrence I found, but do now know how exactly.","['integration', 'limits', 'definite-integrals']"
4591167,On the usage of derivative in operator theory,"In quantum mechanics, we work with linear operators on Hilbert spaces $\mathscr{H}$ .
Suppose I have two bounded ones, defined on the same space $A, B: \mathscr{H}\to\mathscr{H}$ . It seems to me there is an ambiguity on the way to deal with the derivative. On one way, the operator $AB$ is usually interpreted as the composition $(A\circ B)f:=A(B(f))$ for every test function $f$ on $\mathscr{H}$ .
If so, the derivative $D$ operator on $AB$ should act as follows $$
D[AB]f = D[(A\circ B)f]= D[A(Bf)]D(B(f))
$$ On the other way, the following right examples treat $AB$ as if it were literally a product of operators instead of a composition. In other words, the preferred way to compute the derivative is the Leibniz rule $$
D[AB]= AD[B]+BD[A]
$$ $1^{\rm{st}}$ example, by E. Pisanty : The exponential of an operator $\hat A(t)$ does not obey the differential equation $$ \frac{d}{ dt}e^{\hat A(t)} \stackrel{?}{=} \frac{d \hat{A}}{ dt} e^{\hat A(t)} $$ that one might naively hope to satisfy.
To see why this does not work, consider the series expansion of the exponential \begin{align*}
\frac{d}{dt}e^{\hat A(t)}
& = \frac{d}{dt}\sum_{n=0}^\infty \frac{1}{n!} = \sum_{n=0}^\infty \frac{1}{n!} \frac{d}{dt} \hat A^n(t),
\end{align*} When we apply the product rule, we get the individual derivatives of each of the operators in the product, at their place within the product \begin{equation*}
\frac{d}{dt} \hat A^n(t)
=
\frac{d\hat A}{dt} \hat A^{n-1}(t)
+\hat A(t)\frac{d\hat A}{dt} \hat A^{n-2}(t)
+ \ \dots \
+\hat A^{n-2}(t)\frac{d\hat A}{dt} \hat A(t)
+\hat A^{n-1}(t)\frac{d\hat A}{dt} 
\end{equation*} This can simplify to just $n\frac{ d\hat A}{dt} \hat A^{n-1}(t)$ , in which case $\frac{d}{dt}e^{\hat A(t)}
= \frac{d \hat A}{dt} \sum_{n=1}^\infty \frac{\hat A^{n-1}(t)}{(n-1)!} = \frac{d\hat A}{dt}e^{\hat{A}(t)}$ ,  but only under the condition that $\hat A(t)$ commute with its derivative $$ \left[\frac{ d\hat A}{ dt} , \hat A(t)\right] \stackrel{?}{=} 0 $$ In this case, $A^n$ is seen as a product of $A$ with itself $n$ times, instead of $A \circ A \circ \dots \circ A$ $n$ times. $2^{\rm{nd}}$ example, by Wikipedia : The expectation value of an observable $A$ , which is a Hermitian linear operator, for a given Schrödinger state $\vert\psi(t)\rangle$ , is given by ${\displaystyle \langle A\rangle _{t}=\langle \psi (t)|A|\psi (t)\rangle .}$ In the Schrödinger picture, the state $\vert\psi(t)\rangle$ at time $t$ is related to the state $\vert\psi(0)\rangle$ at time $0$ by a unitary time-evolution operator $U(t)$ : ${\displaystyle |\psi (t)\rangle =U(t)|\psi (0)\rangle .}$ In the Heisenberg picture, all state vectors are considered to remain constant at their initial values $\vert \psi(t)\rangle$ , whereas operators evolve with time according to ${\displaystyle A(t):=U^{\dagger }(t)AU(t)\,.}$ The Schrödinger equation for the time-evolution operator is $${\displaystyle {\frac {d}{dt}}U(t)=-{\frac {iH}{\hbar }}U(t)}$$ where $H$ is the Hamiltonian and $\hbar$ is the reduced Planck constant.
It now follows that $${\displaystyle {\begin{aligned}{\frac {d}{dt}}A(t)&={\frac {i}{\hbar }}U^{\dagger }(t)HAU(t)+U^{\dagger }(t)\left({\frac {\partial A}{\partial t}}\right)U(t)+{\frac {i}{\hbar }}U^{\dagger }(t)A(-H)U(t)\\&={\frac {i}{\hbar }}U^{\dagger }(t)HU(t)U^{\dagger }(t)AU(t)+U^{\dagger }(t)\left({\frac {\partial A}{\partial t}}\right)U(t)-{\frac {i}{\hbar }}U^{\dagger }(t)AU(t)U^{\dagger }(t)HU(t)\\&={\frac {i}{\hbar }}\left(H(t)A(t)-A(t)H(t)\right)+U^{\dagger }(t)\left({\frac {\partial A}{\partial t}}\right)U(t),\end{aligned}}}$$ where differentiation was carried out according to the product rule. I really don't understand","['quantum-mechanics', 'operator-theory', 'derivatives', 'mathematical-physics']"
4591179,Integral In Ramanujan's Letter To G.H. Hardy,"In Ramanujan's first letter to G.H. Hardy he defines a function $\phi(n)$ as such $$
\phi(n) = \int_{0}^{\infty} \frac{\cos n x}{e^{2 \pi \sqrt{x}}-1} d x
$$ And then he gives a functional equation $$
\int_{0}^{\infty} \frac{\sin n x}{e^{2 \pi \sqrt{x}}-1} d x=\phi(n)-\frac{1}{2 n}+\phi\left(\frac{\pi^{2}}{n}\right) \sqrt{\frac{2 \pi^{3}}{n^{3}}}$$ How does one prove this functional equation? I am relatively unfamiliar with the methods one would use to solve this, and therefore do not have a proper attempt to share in this question. Thank you","['integration', 'functional-equations', 'calculus', 'definite-integrals']"
4591212,Minimum square sum of inscripted triangle,"Problem: In a right triangle $ABC$ , the hypotenuse is long $4$ and the angle in $B$ is $30°$ . Calling $N$ the midpoint on the side $AB$ (the hypotenuse), and $M$ the middle point on the side $CB$ , consider a random point on the side $AC$ , lets call it $P$ and $AP = x$ . Find the value of $x$ such that is minimum the sum of the squares of the sides of triangle $PNM$ . My Solution so far : I have to minimise $NM^2+NP^2 + PM^2$ and $NM$ is fixed and equal to $1$ , so I should only care about $NP$ and $MP$ $ABC$ is a right triangle with two angles of one angle of $30°$ then the other angle will be $60°$ and therefore the side $AC$ is half the hypotenuse, therefore $AC=2$ , and from this follow that $CB = 2 \sqrt{3}$ $PM$ is the hypothenuse of the right triangle $CPM$ and therefore its equation is $PM^2 = CM^2 + PC^2 = (\sqrt{3})^2 + (2-x)^2 = x^2 -4x + 7$ But now I'm stuck since I don't know how to correctly parametrise $PN$ as a function of $AP$ . Once found a way to write $PN$ as a function of $AP$ I'm done, since I can write them inside my, equation, take the derivative and find the minimum of the function of $AP$ Final Solution :
Thanks to @mathlove, the triangle $APN$ , can be solved by using the Law of Cosines , in our case used on the side $PN$ and with angle $\beta$ it results in: \begin{equation}\begin{aligned}
 PN^2 &= AP^2 + AN^2 - 2\cdot AP\cdot AN\cdot cos(\beta) = \\
 &= x^2 + 2^2 - 4xcos(60) =\\
&= x^2 -2x + 4
\end{aligned}\end{equation} Now I finally have an equation of both $PN^2$ and $PM^2$ both depending on $x$ and therefore I can find the minimum: \begin{equation}\begin{aligned}
PN^2+PM^2 &= x^2 -2x + 4 + x^2 -4x +7 = \\
& = 2x^2 + -6x + 11  
\end{aligned}\end{equation} Which by deriving in $x$ we obtain $4x -6 = 0$ which implies $x = 3/2$","['triangles', 'geometry']"
