question_id,title,body,tags
4655846,Expected value of a sum of geometric dice roll,"We roll a 6 sided fair dice until the number 3 is received. What is the expected value of the sum of the rolls? What I've done: I defined a random variable $X$ to count the rolls amount. $X\sim G\left(\dfrac{1}{6}\right)$ so we have $P(X=k)=\left(\dfrac{5}{6}\right)^{k-1}\cdot\dfrac{1}{6}$ Then I defined another random variable, $S$ , as the rolls sum.
Using the law of total expectation we get $ E\left(S \right) =\sum ^{k}_{i=1}E(  S| X= i) \cdot P( X= i) $ Now we define $x_{1},x_{2},\ldots ,x_{k}$ as the result of the $k$ -th roll. We note that $S=x_{1}+x_{2}+\ldots +x_{k}$ , so we get: $$E\left( S\right) =E\left( x_{1}+x_{2}+\ldots +x_{k}\right)$$ $$\Rightarrow E(  S| X= i) =E(  x_{1}+x_{2}+\ldots +x_{i}| X= i)$$ Using the linearity of expectation, it is equal to: $$E(  x_{1}|X=i) + E(  x_{2}|X=i) + ... + E(  x_{i}|X=i)$$ Now for each $1\leq t\leq i$ we have $$ E(  x_{t}|X=i)= \sum_{j=1}^{6} j \cdot \frac{P(x_t = j, X=k)}{P(X=k)} $$ So that would conclude as $$E(  S| X= i) = \sum_{t=1}^{i} \left( \sum_{j=1}^{6} j \cdot \frac{P(x_t = j, X=i)}{P(X=i)} \right) $$ But I'm not quite sure how to calculate the numerator here. Would appriciate a hint. Thanks!","['expected-value', 'probability']"
4655861,"How many numbers of base 3, of length $n$ are there that contain at least $1$ twos","My Attempts First I wrote down some numbers
Of length one, there is only $1$ valid number - $2$ Of Length two there are $4$ : $12, 20, 21, 22$ Of Length three there are $13$ : $112, 120, ... , 221, 222$ .... Of that I got a sequence ${1, 4, 13, 40, 121, 364, ...}$ I then found the relation $$a_n = \left\{ \begin{array}{cl}
a_1 & = \ 1 \\
a_n & = \ a_{n-1} * 3 + 1
\end{array} \right.$$ Which is $$ \frac{3^n - 1}{2}$$ Knowing that I wanted to prove this sequence holds for any string of length n. I tried to prove this via combinations -Of length one there are ${1}\choose{1}$ = 1 results -Of length two there are ${2}\choose{1}$ + ${2}\choose{2}$ = 3 ? That didn't work, so then I tried -The total strings of length 1 minus the total with NO twos is $3^1$ - $2^1$ = $1$ -Of Length 2 is $3^2$ - $2^2$ = $5$ ? This also doesn't work, so then I tried looking at the strings themselves again The strings of length $2$ contains... $3$ strings with one $2$ , $1$ with $2$ twos The strings of length $3$ contains $7$ with $1$ , $5$ with $2$ , and $1$ with $3$ Of length $3$ : $15$ with $1$ , $17$ with $2$ , $7$ with $3$ , $1$ with $4$ Now I have all these numbers, but I am at a loss what to do with them.
Where did I go wrong about attempting to do this? (Probably something very blatant) And how do I finish it?","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
4655914,Is it true that all matrices of this type are involutory?,"I came across a problem that suggested the following for a $3 \times 3$ matrix. $$\text{Let } H = I - \frac{2}{3}A$$ $$\text{Then } H^2 = I$$ For $H, A \in M_{3,3}$ and $A$ being a matrix of all ones. I played around with this and found that for $H, A \in M_{n,n}$ and $A$ being a matrix of all ones $$\text{Let } H = I - \frac{2}{n}A$$ $$\text{Then } H^2 = I$$ Works in the cases for $0 \leq n \leq 4$ , which makes me think it's probably true for any $n$ . Is that the case?","['matrices', 'involutions', 'linear-algebra']"
4655953,"Must two permutations in $S_n$ that only differ at two adjacent positions have different signs (i.e., one is even and one is odd)?","The Problem : Must two permutations in $S_n$ that only differ at two adjacent positions have different signs (i.e., one is even and one is odd)? For example, suppose $\sigma, \tau\in S_n$ such that $\sigma(x)=\tau(x) \forall x\in[n]-\{i, i+1\}$ , $\sigma(i)=\tau(i+1)=a$ and $\sigma(i+1)=\tau(i)=b$ . Then is it true that $\sigma$ and $\tau$ must have different signs (i.e., one is even and the other is odd)? Context : This problem arose when I was trying to verify the proof that a determinant function on $R$ given by $det(\alpha_{ij})=\sum_{\sigma\in S_n}\epsilon(\sigma)\alpha_{\sigma(1)1}\dots\alpha_{\sigma(n)n}$ is indeed a $n$ -multilinear alternating form. My Attempt : I have tried a few different cases, and the statement seems to be true (which is what I wish to see); but to show it rigorously proves to be trickier. I am currently trying to make use of $\tau=\sigma\sigma^{-1}\tau$ .","['permutations', 'permutation-cycles', 'abstract-algebra', 'symmetric-groups', 'group-theory']"
4656000,Conditional Poisson Distribution is Binomial,"I need to show that if Xi ∼ Poisson(λi), i = 1, 2, . . . , k are independent then the conditional distribution of X1 given X1 + X2 + . . .Xk is Binomial and determines the parameters of this Binomial distribution. I tried: $$ P(X1 = x| X1+...+Xk = y)$$ $$=\frac{P(X1 = x)P(X1+...+Xk = y|X1= x)}{P(X1 +...+Xk = y)}$$ $$=\frac{P(X1=x, X1+...+Xk = y)}{P(X1+...+Xk = y)}$$ $$=\frac{P(X1 = x)P(X2+...+Xk = y-x)}{\sum_{m=0}^yP(X1=m)P(X2+...+Xk = y-m) }$$ $$=\frac{λ1^xexp(-λ1)/x! *\frac{(λ2 + ...+λk)^{y-x}exp(-[λ2+...+λk])}{(y-x)!} }{\sum_{m=0}^y\frac{λ1^mexp(-λ1)}{m!} * \frac{(λ2 + ...+λk)^{y-m}exp(-[λ2+...+λk])}{(y-m)!} }$$ Then I am completely stuck. Am I on the right track? How do I get the $\frac{n!}{k!(n-k)!}$ for the $nCk$ in the Binomial distribution? Any help is greatly appreciated. --
Edit:
I have managed to get to the following: $$=\frac{\frac{λ1^xexp(-λ1)}{x!} *\frac{(λ2 + ...+λk)^{y-x}exp(-[λ2+...+λk])}{(y-x)!} }   { \frac{(λ1 + ...+λk)^{y}exp(-[λ1+...+λk])} { y!}}$$ $$= \frac{y!}{x!{y-x}!}*\frac{λ1^x (λ2+...+λk)^{y-x}}{(λ1+...+λk)^y}$$ where I think I got the $nCk$ but not the $p^k(1-p)^{n-k}$ bit. How can I simplify this step further? Sorry if this is really straightforward. What is $λ1+...+λk$ ? Can they sum up to 1?","['statistics', 'probability-distributions', 'binomial-distribution', 'poisson-distribution']"
4656001,Trouble solving Non-Homogeneous Recurrence relations,"The following is the prompt I am working on: Suppose you have an investment account which grows by 10% every year, and you want to,
in perpetuity, withdraw an increasing amount of money from this account, starting with 1 dollar at
the end of the first year, 2 dollars at the end of the second, 3 dollars at the end of the third, and so forth,
withdrawing $n after n years. a.) Taking a0 to be the initial balance in this account, produce a linear nonhomogeneous
recurrence to describe the balance after n years (note that the nth year involves both
increasing by the interest earned, and decreasing by the size of the withdrawal). For the problem I got the following answer $$B_n=(1.1*B_{n-1}) -n$$ b)Solve the recurrence above, putting the answer in terms of the initial balance a0. This is where I am struggling. I believe this is a nonhomogeneous recurrence relation that I need to find. But in almost all the examples I have worked so far, the non homogeneous part has a exponent. The issue could also be that the answer to a is incorrect, which would make any attempt at b incorrect.",['discrete-mathematics']
4656002,What does $g'(f(2))$ mean?,"Find $g'(f(2)),$ given $f(x)=\sqrt{x^2+5}$ and $g(x)=x^2+x$ . My first step was to find $g(f(x)),$ simplify if possible, then find the derivative $g'(f(x)).$ After I did this, I substituted $2$ for $x.$ A few other students we were saying to find the $g'(x),$ then find $f(2),$ then plug it into $g'(x).$ This also makes sense to me but I'm really unsure.","['calculus', 'derivatives', 'chain-rule']"
4656015,number of rational points of hyper elliptic curve $y^5=-x^2+x$ over $\Bbb{F}_{121}$,"Let $C$ be a curve given by $y^5=-x^2+x$ defined over $\overline{\Bbb{F}_{11}}$ . I want to calculate $\# C(\Bbb{F}_{11})$ and $\# C(\Bbb{F}_{11^2})$ . I calculated $\# C(\Bbb{F}_{11})＝\#\{(0,0),(1,0),(2,±3),(3,±4),(6,±5),(9, ±4),(10,±3),∞\}=13$ (where $∞=(1:0:0)$ in projective closure) by my hand. But I'm stuck with calculating $\# C(\Bbb{F}_{11^2})$ .
Maybe there is no method (in other words, theoretical way) to calculate $\# C(\Bbb{F}_{11^2})$ by hand, but calculation with computer is also appreciated. Background. If over $\Bbb{F}_p$ and $5$ does not divide $p-1$ , $y \mapsto y^5$ is bijective, so the number of rational point is $p+1$ . $p=5$ maybe(if my above counting is correct) a smallest example s.t. $p$ of $\# C(\Bbb{F}_p)\neq p+1$","['algebraic-curves', 'elliptic-curves', 'number-theory', 'finite-fields', 'arithmetic-geometry']"
4656016,Probability of sum of random variables,"I am trying to compute $P(X_1=x_1,\ldots,X_n=x_n\mid X_1+\cdots+X_n=x_1+\cdots+x_n)$ , where $X_i$ are independent samples from a Poisson distribution with mean $\lambda>0$ and $x_i$ are nonnegative values. $$P(X_1=x_1,\ldots,X_n=x_n\mid X_1+\cdots+X_n=x_1+....+x_n)=\frac{P(X_1=x_1,\ldots,X_n=x_n)}{P(X_1+\cdots+X_n=x_1+\cdots+x_n)}\;,$$ the numerator part is simple, but I have no idea for the denominator part; it looks like a sample mean to me, but I still do not know how to proceed further.","['statistics', 'probability']"
4656043,"Find the removed term from an arithmetic series, given the sum of the other terms","We are given a sequence made up of pairs of numbers, $(1, 2), (3, 4), …,(x, x+1)$ , out of which we must remove one pair.  The sum of the numbers which have not been removed is 156. Calculate the sum of the removed numbers. This is what I got so far: $(1+2)+(3+4)+(5+6)+...=3+7+11+...$ forms an arithmetic series. Let $S_n$ be the sum of the terms in this series up until the $n^{th}$ term. Let $n$ be the number of pairs. $S_n=\dfrac{n}{2}(2(3)+(n-1)(4))=n(2n+1)=156+y$ , where $y$ is the sum of the removed pair. Let $y$ be the $a^{th}$ term on the arithmetic series. $y=3+(a-1)(4)=4a-1$ $\therefore n(2n+1)=156+4a-1 \Rightarrow n(2n+1)=155+4a \Rightarrow a=\dfrac{n(2n+1)-155}{4}$ Since the sum of the numbers is even after $y$ is removed, $n$ must be odd. $a$ must be a positive integer, so $n(2n+1)-155$ must be divisible by 4. Now I'm stuck. The smallest $a$ I found was $4$ , which gives $y=15$ and is the correct answer, but I can't understand why it must be the smallest $a$ . Or maybe there is another way of solving this question?",['algebra-precalculus']
4656055,Does a distribution in the dual space of $C_0^\infty$ extend continuously to $C_b^\infty$?,"I want to understand distributions that are bounded linear functionals on smooth functions whose all derivatives vanish at infinity. Before asking questions, let me first define the terms used in this post. A continuous function $\phi$ on $\mathbb{R}^n$ is said to vanish at infinity if $\phi(x) \to 0$ as $|x| \to \infty$ . Denote $C_0$ the space of all continuous functions on $\mathbb{R}^n$ that vanish at infinity. This is a Banach space with the uniform norm. Denote $C_0^\infty$ the space of all smooth functions whose all derivatives vanish at infinity, i.e. $$ C_0^\infty = \{ \phi \in C^\infty : \forall \alpha, \partial^\alpha \phi \in C_0 \}.
$$ This is a Frechet space with the semi-norms $$ \|\phi\|_m = \sup\{ |\partial^\alpha \phi(x)| : |\alpha| \leq m, x \in \mathbb{R}^n \}.
$$ Clearly, Schwartz functions form a dense subspace in $C_0^\infty$ , and the injection $ \mathcal{S} \to C_0^\infty $ is continuous. Thus, we have the continuous injection $ (C_0^\infty)' \to \mathcal{S}' $ , i.e. any continuous linear functional on $C_0^\infty$ is a tempered distribution. Moreover, a linear functional $u$ on $C_0^\infty$ is continuous if and only if there exists $m \in \mathbb{N}, C > 0$ such that $$ 
\forall \phi \in C_0^\infty, |u(\phi)| \leq C \|\phi\|_m.
$$ Thus, any distribution that belongs to $(C_0^\infty)'$ has a finite order. Question 1 : Is there any special name for distributions that belong to $(C_0^\infty)'$ ? The next question is about the extension of distributions in $(C_0^\infty)'$ to smooth functions with bounded derivatives. Denote $C_b$ the space of all bounded continuous functions on $\mathbb{R}^n$ . Denote $C_b^\infty$ the space of all smooth functions with bounded derivatives, i.e. $$ C_b^\infty = \{ \phi \in C^\infty : \forall \alpha, \partial^\alpha \phi \in C_b \}.
$$ Clearly, we have the embeddings $C_0 \subset C_b$ and $C_0^\infty \subset C_b^\infty$ . Note that $C_0, C_0^\infty$ are not dense in $C_b, C_b^\infty$ because they are closed subspaces. Now, by Riesz-Markov representation theorem (and also by the fact that $\mathbb{R}^n$ is a locally compact Polish space), the dual space $(C_0)'$ is isomorphic to the space of all finite Borel measures on $\mathbb{R}^n$ . Here is the thing: any measure $\mu \in (C_0)'$ actually can be extended to $C_b$ by $$ \phi \in C_b \mapsto \int \phi d\mu $$ and the extension is a bounded linear functional on $C_b$ . Thus, we have the embedding $(C_0)' \subset (C_b)'$ . By analogy, I expect any distribution in $(C_0^\infty)'$ can be canonically extended to a continuous linear functional on $C_b^\infty$ . Question 2 : Can any distribution in $(C_0^\infty)'$ be canonically extended to a continuous linear functional on $C_b^\infty$ ? Edit: I expect $(C_0^\infty)'$ to be canonically identified with a closed subspace of $(C_b^\infty)'$ . Moreover, the extension should respect some distributional identities. For example, the Fourier transform of $u \in (C_0^\infty)'$ should be given by a smooth function $ \hat{u}(\xi) = \langle u, e^{ix \xi} \rangle $ .","['distribution-theory', 'functional-analysis', 'real-analysis']"
4656099,Hartshorne Chapter 1 Exercise 7.7 (a),"I am trying to solve part (a) of the following exercise of Hartshorne: Let $Y$ be a variety of dimension $r$ and degree $d>1$ in $\mathbb{P}^{n}$ . Let $P \in Y$ be a nonsingular point. Define $X$ to be the closure of the union of all lines $P Q$ , where $Q \in Y, Q \neq P$ . (a) Show that $X$ is a variety of dimension $r+1$ . Many answers solve this question by claiming that $X$ is birational to the cone on $Y$ , but I do not know the reason: How to prove a given map is a birational morphism rigorously? Moreover, I cannot see how to use the assumption that $P$ is a nonsingular point.","['algebraic-geometry', 'projective-varieties']"
4656146,Evaluation of $\int^{\infty}_{-\infty} dx \frac{x^2\cdot\exp\left(-x\right)}{\left(1+\exp\left(-x\right)\right)^2}$,"I can see that Wolfram Alpha can compute this, but I would like to understand how. The integrand has a poles at $\pm i\pi$ but because of the square at the bottom of the fraction, I cannot quite see how to apply residues here. The integral is also related to moments of logistic distribution https://en.wikipedia.org/wiki/Logistic_distribution (indeed this is how I got here), but cannot quite follow how the moments evaluate to results that depend on Bernoulli numbers. It would seem I am missing some transformation that would bring it into useful form. Can you please suggest how to proceed? Thanks","['integration', 'definite-integrals', 'bernoulli-numbers']"
4656162,Jacobian of a 1-form on a manifold,"Let $X$ be a smooth vector field on a manifold $M$ . The Jacobian of $X$ at a critical point $x^*$ is the linear map $$X'(x^*): T_{x^*}M \rightarrow T_{x^*}M$$ where $T_{x^*}M$ is the tangent space to $M$ at $x^*$ , defined by $$X'(x^*) \cdot u = \frac{d}{dt} \left( \text{d}_{x^*}\Theta_t \cdot u \right)|_{t=0}$$ for any $u \in T_{x^*}M$ . Here $\Theta_t: M \rightarrow M$ is the flow of $X$ and $\text{d}_{x^*}$ is the differential, so $\text{d}_{x^*}\Theta_t$ is a linear map of $T_{x^*}$ to itself, since $x^*$ is a critical point: $$\text{d}_{x^*}\Theta_t: T_{x^*}M \rightarrow T_{\Theta_t(x^*)}M = T_{x^*}M$$ Chosen a local chart at $x^*$ , the matrix representation of this linear map is the usual Jacobian matrix: $$\left[ X'(x^*) \right]_{ij} = \left( \frac{\partial X^i}{\partial x^j} \right)_{x = x^*}$$ See e.g. Abraham-Marsden (1978), Foundations of mechanics, p. 72 (page attached below). Question Is there an analogue coordinates-independent definition for the ""Jacobian"" of a 1-form $\alpha$ on a manifold $M$ as a linear map on a cotangent space and such that the local representative is the usual Jacobian matrix?","['vector-fields', 'reference-request', 'jacobian', 'differential-forms', 'differential-geometry']"
4656203,$X^n\not\Rightarrow X$ but finite dimensional distributions converge (Billingsley exercise 12.5),"Chapter 3, section 12 of Billingsley Convergence of Probability Measures contains the following theorem on p. 136: Theorem 12.6 . Suppose that $E \in \mathcal D$ and $T_0$ is a countable, dense
set in $[0,1]$ . Suppose further that, if $x, x_n \in E$ and $x_n(t) \to x(t)$ for $t \in T_0$ , then $x_n \to x$ in the Skorohod topology. If $P_nE = PE = 1$ and $P_n\pi_{t_1,\ldots,t_k}^{-1} \Rightarrow P\pi_{t_1,\ldots,t_k}^{-1}$ for all $k$ -tuples an $T_0$ , then $P_n \Rightarrow P$ . Here is Exercise 12.5 a couple pages later: Exercise 12.5 . Suppose that $\xi$ is uniformly distributed over $[\frac13,\frac23]$ , and consider the random
functions $X = 2I_{[\xi,1]}$ , $X^n = I_{[\xi-\frac1n,1]}+I_{[\xi+\frac1n,1]}$ .
Show that $X_n \not\Rightarrow X$ , even though $X^n_{t_1,\ldots,t_k} \Rightarrow X_{t_1,\ldots,t_k}$ for all $t_1,\ldots,t_k$ . Why does Theorem 12.6 not apply? Intuitively, it seems that we shouldn't have weak convergence with the Skorohod topology and metric since that only deals with tiny wiggles in space and time. Here, $X^n$ has sample paths that jump from 0 to 1 to 2, but $X$ has sample paths that jump directly from 0 to 2. So, even if sample path $x_n$ only takes the value 1 for a short time, it will always be Skorohod distance one from a path $x$ that jumps directly from 0 to 2. I'll consider both processes to have path space $E$ the cadlag paths which only take values in $\{0,1,2\}$ and have finitely many discontinuities. This should be a closed subset of $D[0,1]$ . To show $X_n \not\Rightarrow X$ , I tried to find a continuous function $f$ and show that $E^n(f)\not\to E(f)$ . I tried $f(x)=\inf \{t\in[0,1]\mid x(t)=1\}$ . If cadlag paths $x,y$ are close to each other by the Skorohod metric, then their jump times must be close. Hence, $f(x)$ and $f(y)$ must be close. So this $f$ seems like a continuous function (on this $E$ with the Skorohod metric/topology). This $f$ fails to be continuous if we let the paths take values close to rather than exactly one though since that allows $x,y$ to be close in the Skorohod metric but $f(x)$ and $f(y)$ not close. Let $x$ jump to one at $t_0$ , and let $y$ jump to $1+\epsilon$ at $t_0$ (both starting at zero). Then $f(y)=+\infty$ since it never takes the value one, but we can choose $\epsilon$ small to make these functions close in the Skorohod metric. Back to the problem, we get $E^n(f)=E(\xi-\frac1n)=\frac12-\frac1n\to\frac12$ , but $E(f)=+\infty$ since the random function $X$ never takes the value one. This shows that we do not get weak convergence. (Assuming my reasoning is correct...?) Now, why Theorem 12.6 does not apply is where I am confused. Clearly $E$ satisfies $P_nE=PE=1$ here, since both processes (random functions) always have sample paths in the space of cadlag sample paths which only take values in $\{0,1,2\}$ . Of course, most of those functions are not valid sample paths for either process. Really $X^n$ takes values in the set $E_{012}$ of functions that just jump $0$ to $1$ to $2$ , with exactly 2 jumps, and $X$ takes values in $E_{02}$ the set of paths that have a single jump from $0$ to $2$ . So I feel the language of Theorem 12.6 isn't precise enough, and should say something in the first sentence like the set $E$ not being able to be refined further and with $PE=1$ . Maybe it has to do with no countable $T_0$ can be found? What am I missing?","['stochastic-processes', 'skorohod-space', 'probability-theory', 'weak-convergence']"
4656240,Find the smallest angle possible from the diagram,"On the diagram above, $AB=BC=CD=DE$ . The measure of $\angle ADE$ is expressed in integer degrees. Find the smallest possible $\angle ADE$ . This is what I got so far: Let $\angle BAC=a$ .
So, $\angle CBD=2a$ and $\angle DCE=\angle DEC=3a$ Hence, $\angle ADE=180^\circ-a-3a=180^\circ-4a$ This was a bit iffy, but I thought that $0<\angle DCE<90^\circ$ , so $0<a<30^\circ$ . From there I got $60^\circ<180^\circ-4a<180^\circ \Rightarrow 60^\circ<\angle ADE<180^\circ$ . I got the smallest $\angle ADE$ as $\,61^\circ$ as a result, but it is wrong. How do you solve this question?","['triangles', 'geometry']"
4656250,Volume Under a Ceiling,"I have run into an interesting volume problem that I can't seem to figure out. I was recently in a school gym and noticed the interesting design of the ceiling and thought back to Calculus III and finding the volume under surfaces. I have been trying to come up with a rough equation that models the design of the ceiling but have not been able to. The floor is roughly 100ftx100ft and the center of the ceiling is roughly 28ft. I believe that the four curved sections are parabolic. The equations I have been playing with are $ f(x,y) = (-1/16)x^2 + 6$ and $ f(x,y) = (-1/16) y^2 + 6$ .These have just been jumping-off points and I figured their intersection region is roughly the shape of the ceiling. I'm not sure if trying to parameterize these equations is the way to go, or if I'm totally off base altogether. Any help would be appreciated!","['multivariable-calculus', 'surfaces', 'volume']"
4656267,Minimum value of f(θ)= a²sec²θ +b² cosec²θ using AM- GM inequality,"If we take and function $$f(θ)= \dfrac{a^2}{\cos^2\theta} + \dfrac{b^2}{\sin^2\theta}$$ And wish to find minimum value of function using AM -GM inequality i.e. if we have two no. p & q
Then $$AM \ge GM$$ $$\dfrac{p+q}{2}  \ge  \sqrt{pq} $$ Using here $$\dfrac{a^2}{\cos^2\theta} + \dfrac{b^2}{\sin^2\theta} \ge 2 \sqrt { \dfrac{a²b²}{\sin^2 \theta  \cos ^2 \theta}}$$ Multiply dividing by $4$ in square root $$\dfrac{a^2}{\cos^2\theta} + \dfrac{b^2}{\sin^2\theta} \ge 2 \sqrt { \dfrac{4a²b²}{4\sin^2 \theta  \cos ^2 \theta}}$$ by using $$2 \sin \theta \cos \theta=\sin (2\theta) $$ $$\dfrac{a^2}{\cos^2\theta} + \dfrac{b^2}{\sin^2\theta} \ge 2 \sqrt { \dfrac{4a²b²}{\sin^2 (2\theta)}}$$ But now i am totally confused how to deal it to find minimum value of function Please consider case when $a>b$ , $b>a $ Answer in the book is:- $(a+b)^2$","['functions', 'a.m.-g.m.-inequality']"
4656268,"If $A$ and $B$ are independent events of a sample space associated with a random experiment, show that $A$ and $B^c$ are also independent events.","If $A$ and $B$ are independent events of a sample space associated with a random experiment, $A$ and $B^c$ are also independent events. My attempt is to use the condition probability $P(A|B)$ or the probability of event $A$ given that event $B$ has occured: $$P(A \cap B^c) = P(A | B^c)P(B^c) = [1-P(A^c | B^c)]P(B^c)$$ $$=P(B^c)-P(B^c)P(A^c | B^c)$$ How could I continue this if I want to show that $P(A \cap B^c) = P(A)P(B^c)$ ?","['probability-theory', 'probability']"
4656279,Evaluate: $\sum\limits_{n\ge1}\sum\limits_{m\ge0}\sum\limits_{k=0}^{n-1}\frac{y^nm^k(-n)^m\delta_{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}$,"Context: The cube super root ssrt $_3(x)$ series expansion yielded part of it as: $$\sum_{n=1}^\infty\frac{y^n}{n!}\sum_{m=0}^\infty\frac{(-n)^m}{m!}\sum_{k=0}^{n-1}\binom{n-1}k\left.\frac{d^kt^m}{dt^k}\right|_{t=0}\left.\frac{d^{n-1-k}e^{tm}}{d^{n-1-k}}\right|_{t=0}=\lim_{t\to0}\sum_{n=1}^\infty\frac{y^n}{n!}\sum_{m=0}^\infty \sum_{k=0}^{n-1}\frac{(-n)^m \Gamma(n) m^{n-k+1}t^{m-k}}{k!(m-k)!\Gamma(n-k)}$$ The $m=0$ term is ignored outside the sum. The inner sum uses Hypergeometric $U(a,b,x)$ and cancels $t^a$ , so the limit was easier to find. Testing different cases of $m,n$ gave a pattern for the limit as a ratio of gamma functions: $$\lim_{t\to0}\sum_{m,n=1}^\infty\frac{y^n m^nn^m}{m^{m+2}(m-1)!n!}U(-m,n-m,t)=\lim_{t\to0}\sum_{m,n=1}^\infty\frac{y^n m^nn^m(m-(n+t))!\sin(\pi(n+t))}{m^{m+2}\pi\Gamma(m)}=\sum_{m,n=1}^\infty\frac{y^n(-1)^mm^n n^{m-1}}{\Gamma(n-m)\Gamma(m)m^{m+2}}$$ Problem: Also, switching the differentiation order,  due to the general Leibniz rule , gives the an equal sum; part of it had this limit: $$\sum_{n=1}^\infty\frac{y^n}{n!}\sum_{m=0}^\infty\frac{(-n)^m}{m!}\sum_{k=0}^{n-1}\binom{n-1}k\left.\frac{d^{n-1-k}t^m}{dt^{n-1-k}}\right|_{t=0}\left.\frac{d^ke^{tm}}{dt^k}\right|_{t=0}=\lim_{t\to0}\sum_{n=1}^\infty\sum_{m=0}^\infty\sum_{k=0}^{n-1}\frac{y^nm^k(-n)^m t^{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}$$ Taking $\lim\limits_{t\to0}t^{k+m-n+1}$ makes the sum diverge and seemingly removing any terms from the sum still makes the sum diverge, so we can’t substitute this confluent $\,_1\text F_1(a;b;x)$ identity: $$\sum_{k=0}^{n-1}\frac{m^k(-n)^m t^{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}=\frac{(-n)^mt^{m-n+1}\,_1\text F_1(1-n;m-n+2;-mt) }{n!(m-n+1)!}$$ However, noticing that the truncated sums give a polynomial, the limit is the constant term of it and therefore Kronecker $\delta_x$ appears: $$\lim_{t\to0}\sum\limits_{n\ge1}\sum\limits_{m\ge0}\sum\limits_{k=0}^{n-1}\frac{y^nm^k(-n)^mt^{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}= \sum\limits_{n\ge1}\sum\limits_{m\ge0}\sum\limits_{k=0}^{n-1}\frac{y^nm^k(-n)^m\delta_{k+m-n+1}}{(k+m-n+1)!\Gamma(n-k)k!n}$$ We can remove a sum like in this post or simplify, but how?","['kronecker-delta', 'real-analysis', 'limits', 'tetration', 'hypergeometric-function']"
4656293,"Expected number of rolls of a fair, 6-sided die before all the evens OR all the odds are rolled, whichever happens first. [duplicate]","This question already has answers here : Expected number of elements before 1 out of 2 hash tables are full? [duplicate] (2 answers) Closed last year . We're playing a game where we roll a fair, 6-sided dice over and over. The game ends when we have rolled all of the evens (2, 4, 6) OR all of the odds (1, 3, 5), we don't care which. On average how many rolls does it take to end the game? If we cared about even or odd, I think the answer would be 11 (for either scenario). I wrote a short Python script to simulate the game and believe the answer is 7.3, but I'm looking for an exact solution derived from probability theory.","['discrete-mathematics', 'probability-theory', 'probability']"
4656325,How to prove this characterisation of convergence by distribution?,"We solved the following problem in class and I do not understand what happens. Definition: A sequence of random variables $W_1, W_2, \ldots$ converges in distribution to random variable $W$ if for every $w \in \mathbb{R}$ s.t. $P(W = w) = 0$ , it is true that $\lim_{n \to \infty} P(W_n \leq w) = P(W \leq w)$ . Claim: Sequence of random variables $W_n$ converges weakly to $W$ iff for every $w \in \mathbb{R}$ , s.t. $P(W = w) = 0$ , it is true that $\lim_{n \to \infty} P(W_n < w) = P(W < w)$ . I do not understand the proof that we did. We only argued $(\Rightarrow)$ , as the converse is similar. We used the fact that $ \{ W_n \leq w \} = \cap_k \{ W_n < w + \frac{1}{k} \}$ . Then we wrote: \begin{multline*}
P(W \leq w) = \lim_{n \to \infty} P(W_n \leq w) = \lim_{n \to \infty} P \left( \cap_k \left( W_n < w + \frac{1}{k} \right) \right) = \lim_{n \to \infty} \left( \lim_{k \to \infty} P \left( W_n < w + \frac{1}{k} \right) \right) = \\ = \lim_{k \to \infty} \left( \lim_{n \to \infty} P \left( W_n < w + \frac{1}{k} \right) \right) = \lim_{k \to \infty} P \left(W < n + \frac{1}{k} \right)
\end{multline*} I have no idea why, first of all, $\lim_{k \to \infty} \left( \lim_{n \to \infty} P \left( W_n < w + \frac{1}{k} \right) \right) = \lim_{k \to \infty} P \left(W < n + \frac{1}{k} \right)$ . Then I also do not understand why what we did is enough; how does the claim (at least $(\Rightarrow)$ ) follow from what we have shown? We also stated that $F_X(x)$ has at most finitely many points of discontinuity and that this is crucial to our argument. Which makes me even more confused.","['measure-theory', 'statistics', 'weak-convergence', 'conditional-convergence', 'probability']"
4656386,Find the number of points inside a rectangle if the rectangle is divided into $210$ triangles.,"A number of points is drawn inside a rectangle. The rectangle is divided into 210 triangles whose vertices coincide with the vertices of the rectangle and/or the points drawn inside the rectangle. Find the number of points drawn. I have no idea how to interpret this question. My take is this: there are $210$ triangles that combines into a rectangle, then we find the total number of vertices that the triangles make inside the rectangle, excluding the vertices of the rectangle. If my take is correct, then I absolutely have no idea how to solve the problem. I tried finding any pattern, but I couldn't. This is primarily because I don't know what counts as a 'vertex'. A simple example is this: In the diagram on the left, there is only $1$ point, $A$ , inside the rectangle made up of $4$ triangles, but in the diagram on the right, there are $2$ points, $B$ and $C$ , inside the rectangle made up of the same number of triangles as the rectangle on the left. How do you solve the problem? If it helps, the final answer should be $104$ points.","['rectangles', 'triangles', 'geometry']"
4656424,Discussion on the Conditions for Pointwise/Uniform Convergence of Fourier Serieses.,"My book isn't very clear about the conditions for pointwise/uniform convergence of fourier series; so, after a bit of search, here I am with a summary of what I found. Please, it would be very appreciated if someone could discuss or check the validity of any of these points: Let's suppose that the function $\left[f(x)\right]$ has left-and-right derivatives in the point $\left[x_0\right]$ , and is here continuos, then its fourier series converges to $\left[f(x_0)\right]$ . Can somebody assure that this statement holds even if such derivatives are not equal? Let's suppose that the function $\left[f(x) \in C^1([-L,+L])\right]$ , then its fourier series converges pointwise over the whole segment. Can somebody assure that this convergence is also uniform? Let's suppose that $\left[\sum_{n=-\infty}^{+\infty} c_ne^{inx}\right]$ is the fourier series of the function $\left[f(x)\right]$ ; if $\left[\sum_{n=-\infty}^{+\infty} |c_n| \lt \infty\right]$ then the series converges absolutely, and therefore uniformly. (NOTE: this is a consequence of m-test.) Let's suppose that the function $\left[f(x)\right]$ has bounded variation over the segment $\left[-\pi, +\pi\right]$ , or its first derivative is such that $\left[\;|f'(x)|\le K\;\right]$ (otherwise it is lipshitz, or more generally it is $\alpha$ -holder with $0 \le \alpha \le 1$ ), then its fourier series converges uniformly. Note that this statement holds for $2\pi$ -periodic functions (or at least this is what I found). Can somebody assure that this statement holds even over a generic segment $\left[a,b\right]$ ? Moreover the condition of periodicity has to be contextualized with $\left[\; \lim_{x\to a^+}f(x)=\lim_{x\to b^-}f(x) \;\right]$ . Let's suppose that the function $\left[f(x)\right]$ is periodic with bounded variation over a certain segment; then its fourier series converges pointwise to $\left[\lim_{\epsilon\to 0} \frac{f(x+\epsilon)+f(x-\epsilon)}{2}\right]$ . In particular, if the function is continuous in $\left[(x)\right]$ , then its fourier series converges pointwise to $\left[f(x)\right]$ . Moreover, if the function is continuous everywhere over the segment, then its fourier series converges uniformly to $\left[f(x)\right]$ . Can somebody discuss about the periodicity condition? Is that necessary? Let's suppose that the function $\left[f(x) \in L^p, \;p\gt1 \right]$ ; then its fourier series converges for ""almost every"" $\left[(x)\right]$ . Can somebody assure that this type of convergence is a ""pointwise-one""? In addition to those, there is a very personal question I beg you to answer... Let's suppose that I'm asked to find the fourier series of some kind of function, over a certain segment $\left[a,b\right]$ . Let's say that I'm not interested in studying the properties of the function; instead, I'm required to solve the integral: $$\tilde b_n = \frac{1}{\sqrt{b-a}}\int_a^b dx \, f(x)e^{-i \frac{2n\pi}{b-a} x} \; \lt \infty \quad \vert \quad \forall \; n \in \mathbb{Z}$$ whose results are the coefficients for the function's fourier series: $$f(x) = \sum_{n=-\infty}^{+\infty} \tilde b_n \frac{e^{i \frac{2n\pi}{b-a} x}}{\sqrt{b-a}}$$ Why doesn't the simple existence of the fourier series coefficients imply the pointwise convergence of the series itself to the given function in the given segment?","['fourier-analysis', 'complex-analysis', 'calculus', 'sequences-and-series', 'convergence-divergence']"
4656427,Compose a function with a parameter and given local maximum points.,"I need to find a function $f_k: \mathbb R \rightarrow \mathbb R$ which includes the parameter $k \in \mathbb R$ . The local maximum point of this function changes with the parameter $k$ . This point is given by $M_k = \left(k-1,\quad e^{-k}\cdot\left(k^{2}+\left(k-1\right)\right)\right)$ . This has the result that all of these points lie on the curve $y= e^{-x-1} \cdot \left(x^2+3x+1\right)$ . What I have right now is $$
f_k(x) = e^{-k}\left(\left(x-k\right)^{3}-3\left(x-k\right)+k^{2}\right)
$$ which is incorrect, but the maximum is at $x = k-1$ (as is given by the point $M_k$ ). I have created a desmos file to show this: https://www.desmos.com/calculator/9l8ohitaxp (The black curve is where all the maximum points are on; the red graph is my incorrect $f_k$ . Change $k$ with the slider to visualize all the functions $f_k$ . You can see that $M_k$ deviates from the graph of $f_k$ ) As I understand it, I only need to change the height of the graph of $f_k$ at $x = k-1$ such that it meets the corresponding point. I have no clue how to even approach that. Thank you in advance.","['ordinary-differential-equations', 'function-and-relation-composition']"
4656460,Explain the general idea of topology for a sophomore student,"I am a sophomore and still taking calculus 2 and 3. However, I asked several questions in class but the professor always answers me: you need to take a higher level of mathematics, topology in specific, to be able to answer these questions. For example, when we were taking how to derive and integrate functions with multiple variables, I noticed that both differentiation and integration are generally expressed in terms of limits. In fact, this was a shock for me because in high school I used to treat derivatives, integrals, and limits such as every one of them have its own rules and I thought that they are somehow independent. It was the first time I see their proofs and understand what they really mean. So I've just wondered and asked if there is anything more general that can define limits. The professor answered: topology.
Topology again! It was the third time this semester I received the same mysterious answer. Therefore, I decided to search and read some books related to topology. However, most explanations and books require a higher level of mathematics than I have. Can anyone explain the general idea of topology and recommend some simple books?","['geometric-topology', 'general-topology', 'differential-topology', 'algebraic-topology']"
4656486,"Determine the continuous function such that $\lim\limits_{x\rightarrow a} \frac{f(x)}{x-a}= e^{a},$ and $(x-a)(y-a)f(x+y)=(x+y-a)f(x)f(y)$","Determine the continuous function $f:\mathbb{R}\to\mathbb{R}$ with the following properties: $$\lim\limits_{x\rightarrow a} \frac{f(x)}{x-a}= e^{a},$$ where $a$ is a real valued constant; $$(x-a)(y-a)f(x+y)=(x+y-a)f(x)f(y),$$ for any $x, y \in \mathbb{R}$ . What I have tried First of all, I have discovered the function $f_{1}=(x-a)e^{a}$ , that obviously respects all criteria. I have a feeling that this is the only function. Let's assume that $f_{2}$ is another function that checks all the criteria. Let $g:\mathbb{R}\to\mathbb{R}$ , $$g(x) = \begin{cases}\dfrac{f_{1}(x)}{f_{2}(x)}, & x \neq a \\
1, & \text{otherwise}
\end{cases}$$ . Obviously, $g$ is also continuous. What I am trying to prove is that $g$ is constant. Does anyone have any ideas?","['continuity', 'functions']"
4656541,Help with solving the differential equation $y'' - 12y' = \cos(2x)$ where $y(0) = y'(0) = 0$,"I'm currently struggling with solving the following differential equation: $$y'' - 12y' = \cos(2x)$$ with initial conditions $y(0) = 0$ and $y'(0) = 0$ . I have already found the homogeneous solution, but I'm having trouble guessing the particular solution. I know that if the equation also contained y, the particular solution would be $y_p = A \cos x + B \sin x$ . However, since $y$ is not included in the equation, I'm not sure how to proceed. I've tried guessing a solution using $e^{2ix}$ , but then $d$ still needs to be $y$ , and I'm not sure what to do next. Can anyone provide any insight or guidance on how to find the particular solution? Any help would be greatly appreciated. Thank you!","['initial-value-problems', 'calculus', 'ordinary-differential-equations']"
4656544,Prove that $A$ is invertible if $BA^2+A^2=I$,"I was asked this question: There's $A,B \in M_3 (\mathbb{R})$ a square matrices from order $3\times3$ from the real numbers, that satisifes: $BA^2+A^2=I$ . Prove that $A$ is invertible and that the matrices $A$ and $B$ have the commutative property. $(AB=BA=I)$ Given that $B+A^2B=O$ . Prove that $B$ isn't invertible and calculate $|A|$ , write the two possibilities. My first attempt (But then I saw that determinant does not preserve the addition, then this attempt is completely wrong):
Suppose $A$ isn't invertible, meaning $|A|=0$ . $BA^2+A^2=I \rightarrow A^2=I-BA^2$ And since we assumed that A isn't invertible: $|A^2|=|I|-B|A|^2 \rightarrow 0=1-B*0 \rightarrow 0=1$ Contradiction, therefore $A$ is invertible. My second attempt : $BA^2+A^2=I \rightarrow (B+I)A^2=I$ Now, we need to show that $A^2(B+I)=(B+I)A^2=I$ , which will prove that $A$ is invertible and found an inverse for it. But I wasn't sure how to continue from here... And since $A$ is invertible then: $BA^2+A^2=AB^2+A^2=I \rightarrow A^2(B+I)=(B+I)A^2=I$ Which proved that the commutative property exists. My attempt on the second question: (But it also relies on adding determinants, then it must be incorrect too)
We'll prove it by contradiction, $B$ is inverse, meaning $|B|\neq 0$ : $B+A^2B=O \rightarrow B=O-A^2B \rightarrow |B|=-|A^2||B|$ And from $1$ . we know that $|A|\neq 0$ , and since we assumed that $|B| \neq 0$ then the determinant of $B$ isn't necessarily always $0$ , therefore it is a contradiction. I'd love for guidance on how to continue. Thanks for the help!","['matrices', 'linear-algebra']"
4656551,Question about properly discontinuous actions in a metric space,"I was reading the Katok's book on Fuchsian groups and I read that she asserts the following: It is clear from the definition that $G$ acts properly and discontinuously on $X$ if and only if each orbit is discrete and the order of the stabilizer of each point is finite. I am trying to show that if each orbit is discrete and the order of the stabilizer of each point is finite, then $G$ acts properly and discontinuously on $X.$ In this SE discussion there's an answer that apparently show this fact using the following argument: Consider a compact $K\subset X, x\in X$ and the subset $K_x=K\cap Gx.$ Then $K_x$ is a discrete closed subspace of a compact space. But every discrete closed subspace of a compact space is finite. I am really stuck in proving that $K_x$ must be a closed subspace of $X.$ I tried to do this by showing that $Gx$ is a closed subset of $X$ (because if then, we would have that $K\cap G_x$ is a closed subset of $X$ contained in the compact space $K$ ), but I only achieve this by assuming that $G$ is a group of isometries instead of homeomorphisms . Is this true if we only assume that $G$ is as group of homeomorphisms? (the OP on the link cited above asserts that he could prove this fact only assuming that $G$ is a group of homeomorphisms of $X$ ). How can I prove it if so? Thanks in advance. Note: There are several discussions on SE concluding that Katok refers in general to isometries groups, for example, here ; but this particular SE discussion do not adress the question of wheter is true that $Gx\cap K$ is always closed or not (I think that the example $\{1/n:n \in \mathbb{N}\}$ may not be a counterexample because I don't know if this can be an orbit of a point by a group acting by homeomorphisms on $X$ ).","['general-topology', 'group-actions', 'topological-groups', 'group-theory']"
4656614,"Prove or disprove. $\bigcup_{i\in I} \bigcap_{j\in J} C_{i,j} = \bigcap_{j\in J} \bigcup_{i\in I} C_{i,j}.$ [duplicate]","This question already has answers here : Counterexample, union of intersections, intersection of unions (2 answers) Closed last year . Let $\{ C_{i,j} : i\in I \text{ and } j\in J \}$ be a family of sets. Assume the set $I$ (of indices $i$ ) is arbitrary, non-empty, and non-enumerable. Similarly, assume the set $J$ (of indices $j$ ) is arbitrary, non-empty, and non-enumerable. Prove or disprove the equality $$\bigcup_{i\in I} \bigcap_{j\in J} C_{i,j} = \bigcap_{j\in J} \bigcup_{i\in I} C_{i,j}.$$ Check my proof of this equality. Any errors? To prove that the equality is true, we need to show two inclusions. First, we will show that $$\bigcup_{i\in I} \bigcap_{j\in J} C_{i,j} \subseteq \bigcap_{j\in J} \bigcup_{i\in I} C_{i,j}.$$ Suppose $x$ belongs to $\bigcup_{i\in I} \bigcap_{j\in J} C_{i,j}$ . This means that there exists an index $i_0$ in $I$ such that $x \in \bigcap_{j\in J} C_{i_0,j}$ . For all $j \in J$ , we have $x \in C_{i_0,j}$ , therefore $x \in \bigcup_{i\in I} C_{i,j}$ for all $j \in J$ . This implies that $x$ belongs to $\bigcap_{j\in J} \bigcup_{i\in I} C_{i,j}$ , as desired. Now, let's show that $$\bigcap_{j\in J} \bigcup_{i\in I} C_{i,j} \subseteq \bigcup_{i\in I} \bigcap_{j\in J} C_{i,j}.$$ Suppose $x$ belongs to $\bigcap_{j\in J} \bigcup_{i\in I} C_{i,j}$ . This means that for all $j \in J$ , we have $x \in \bigcup_{i\in I} C_{i,j}$ . Therefore, for each $j \in J$ , there exists an index $i_j$ in $I$ such that $x \in C_{i_j,j}$ . If for each index $j$ we choose a unique index $i_j$ , we can define a function $\varphi: J\to I$ by setting $\varphi(j)=i_{j}$ . Then, for all $j \in J$ , we have $x \in C_{i_j,j}$ , which implies that $x$ belongs to $\bigcap_{j\in J} C_{i_j,j}$ . Therefore, $x$ belongs to $\bigcup_{i\in \varphi(J)} \bigcap_{j\in J} C_{i,j}$ . On the other hand, since $\varphi(J)\subset I$ , we have $$\bigcup_{i\in \varphi(J)} \bigcap_{j\in J} C_{i,j} \subseteq \bigcup_{i\in I} \bigcap_{j\in J} C_{i,j}$$ With this, we conclude that $$\bigcup_{i\in I} \bigcap_{j\in J} C_{i,j} = \bigcap_{j\in J} \bigcup_{i\in I} C_{i,j}$$ is true.","['elementary-set-theory', 'solution-verification']"
4656632,Find marginal PDF from joint PDF bound issue,"Given the joined density function for x and y is $$f_{XY}(x,y)=\frac{5}{16}{xy^{2}}$$ $$0<x<y,  0<y<2$$ find the marginal density function of Y. I used: $$f_Y(y)=\int_{-\infty}^\infty f_{XY}(x,y) dx $$ $$=\int_0^2 \frac{5}{16}{xy^{2}} dx ​$$ but had a second thought about the bounds of x: should I use $\int_0^y$ instead of $\int_0^2$ since $0<x<y$ ? Why or why not? What is the standard approach/rules?","['integration', 'statistics', 'probability-distributions']"
4656658,"Reference for the ""Quotient Variance""?","I have recently encountered the expression $$\frac{\mathbb{E}[X^2]}{\mathbb{E}[X]^2}$$ in my research. This can be thought of as an analogue of the variance $\mathbb{E}[X^2] - \mathbb{E}[X]^2$ where one divides instead of subtracting.
Does this quantity have some well-known name? I recall seeing it before in some probabilist's work, but tracking down a particular reference would take quite some time.","['probability', 'reference-request']"
4656659,"$\lim_{x\to -\infty} x^x$ exists? and if so, what's its value?","Disclaimer: I am completely and utterly new here. Please openly correct me if I'm doing anything wrong. I received the following question on a recent calc test: $$\lim_{x \to - \infty} x^x=?$$ $$a) 1 \quad b) \infty  \quad  c) 0   \quad  d) DNE$$ I selected $d) DNE$ as my answer as I believed the function $f(x) = x^x$ has a domain of $x \in (0, \infty)$ , and any negative value will cause the function to oscillate between positive and negative values and result in discontinuities. I was sincerely surprised when the test was returned and the correct answer was $c) 0$ . My instructor's argument was that the function did not have to be continuous for the limit to exist, despite it being outside of the domain. My argument was that for certain negative real numbers of the form $-\frac{2a+1}{2b}$ for $a, b \in \mathbb{N}$ , the function will yield imaginary results of the form: $$\frac{1}{\sqrt[2b]{\left(-\frac{2a+1}{2b}\right)^{2a+1}}}$$ .
My friend also had a counter-example where $\left(-\frac{1}{2}\right)^{-\frac{1}{2}}$ does not exist, but $\left(-\frac{2}{4}\right)^{-\frac{2}{4}}$ does. I have done some research on my own, and I believe by the epsilon-delta definition of a limit, this limit does not exist as this limit does not satisfy that for all real $x$ in the needed range, $\left|f(x) - L\right| < \epsilon$ . Please lend help! Edit: is there a (more) rigorous way to show that the limit does not exist?","['limits', 'calculus', 'exponential-function', 'algebra-precalculus']"
4656660,"Outer regularity of $\nu(E)=\int_E \phi\,d\mu$","Suppose that $\mu$ is a Radon measure on $X$ and $\phi \in C(X,(0,\infty))$ . Let $\nu(E)=\int_E \phi\,d\mu$ and let $\nu'$ be the Radon measure associated to the functional $I(f)=\int f\phi \,d \mu$ on $C_c(X)$ . Then show that $\nu=\nu'$ on open sets; $\nu$ is outer regular and $\nu=\nu'$ and hence $\nu$ is a Radon measure. I was able to show that $\nu$ and $\nu'$ agree on open sets. I am unable to show that $\nu$ is outer regular. But assuming outer-regularity, $\nu=\nu'$ right away and hence $\nu$ is Radon. But how do I show outer-regularity? Hint suggests the open sets $V_k=\{x: 2^k<\phi(x)<2^{k+2}\}$ with $k \in \mathbb{Z}$ cover $X$ but I don't see how that's helpful.","['measure-theory', 'real-analysis']"
4656688,Derivative of simple matrix product,"I have been trying for the past few days to find a solution to this very simple problem, yet every time I seem to hit a wall. (PS: please excuse the poor notations, it's my first time using this website). Let's say we have $C = AB$ , $A$ and $B$ matrices so that $C$ is defined. What is $\frac{\partial C}{\partial A}$ and $\frac{\partial C}{\partial B}$ , i.e., the derivatives of $C$ w.r.t $A$ and $B$ respectively? Let us assume size $A = m \times n$ , size $B = n \times p$ , size $C = m \times p$ By doing $C_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}$ for $i = 1, \dots, m$ and $j = 1, \dots, p$ Then $\frac{\partial C_{ij}}{\partial a_{ls}} = \frac{\partial (\sum_{k=1}^{n} a_{ik} b_{kj})}{\partial a_{ls}} = b_{sj}$ for $i = l$ Then shouldn't $\frac{\partial C}{\partial A}$ be the rate of change for each $C_{ij}$ w.r.t each element of $A$ ? So that $\frac{\partial C}{\partial A}$ is a matrix $D$ of size $C$ where each $D_{ij}$ is a matrix of size $A$ . For instance $C = AB$ , given $A$ of size $1 \times 3$ and $B$ of size $3 \times 2$ . $A = $$\begin{pmatrix}a_1 & a_2 & a_3 \end{pmatrix}$ $B = $$\begin{pmatrix}b_{11} & b_{12} \\ b_{21} & b_{22} \\ b_{31} & b_{32}\end{pmatrix}$ $C = \begin{pmatrix} a_1 b_{11} + a_2 b_{21} + a_3 b_{31} & a_1 b_{12} + a_2 b_{22} + a_3 b_{32} \end{pmatrix}$ Then $\frac{\partial C_1}{\partial B} = \frac{\partial (a_1 b_{11} + a_2 b_{21} + a_3 b_{31})}{\partial B} = \begin{pmatrix} a_1 & 0 & a_2 & 0 & a_3 & 0 \end{pmatrix}$ and $\frac{\partial C_2}{\partial B} = \frac{\partial (a_1 b_{12} + a_2 b_{22} + a_3 b_{32})}{\partial B} = \begin{pmatrix} 0 & a_1 & 0 & a_2 & 0 & a_3 \end{pmatrix}$ So that $\frac{\partial C}{\partial B} = \begin{pmatrix} a_1 & 0 & a_2 & 0 & a_3 & 0 \\ 0 & a_1 & 0 & a_2 & 0 & a_3 \end{pmatrix}$ , is of size $2 \times 6$ . $\frac{\partial C}{\partial A} = B^T$ . However, in the machine learning equations, it is given that $\frac{\partial C}{\partial B} = A^T$ . To be more precise, the context of this query is for machine learning and backpropagation. We have $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \hat{Y}} \frac{\partial \hat{Y}}{\partial \text{out}} \frac{\partial \text{out}}{\partial W}$ , where $L$ is a scalar function (the loss), $\hat{Y}$ is a $1 \times 2$ matrix (the activated output), $\text{out}$ is a $1 \times 2$ vector (pre-activation), and $W$ is a $3 \times 2$ matrix (the weights). If you could also explain why we actually write: $\frac{\partial L}{\partial W} = \frac{\partial \text{out}}{\partial W} \frac{\partial \hat{Y}}{\partial \text{out}} \frac{\partial L}{\partial \hat{Y}}$ , instead of the forward notation used earlier $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \hat{Y}} \frac{\partial \hat{Y}}{\partial \text{out}} \frac{\partial \text{out}}{\partial W}$ Is there an explanation beyond ""cause it fits for the matrix product so we rearrange the terms in the chain rule""? Asking Chat-GPT these questions yields different answers every time. Thank you for your time. Edit : the formatting was done using Chat-GPT.
I also checked the link : Not understanding derivative of a matrix-matrix product. and it seems that the result AX is a Kronecker product, which also seems to not be the right size. The part where I usually get stuck at is computing every $D_{ij}$ for $D = \frac{\partial C}{\partial B}$ . Other EDIT : Here is more context for the chain derivation above. $L(Y, \hat{Y}) = \frac{1}{2m} \sum_{i=1}^{m}(Y^{(i)} - \hat{Y}^{(i)})^2$ where $\hat{Y}$ is the predicted out $ \hat{Y} = \sigma_{o}(out) $ $ out = h_{2} W $ where $\sigma_{o}$ is the activation function for the out layer, $h_{2}$ is of size $1 \times 3$ , $W$ of size $3 \times 2$ , $out$ of size $1 \times 2$ and $\hat{y}$ of size $1 \times 2$ Here the activation function $\sigma_{o}(X)$ is applied element-wise to $X$ , a matrix.","['matrices', 'machine-learning', 'matrix-calculus', 'partial-derivative', 'derivatives']"
4656722,Proving the space which consists sequences of finitely many non-zero terms is incomplete with respect to $2$ -norm,"In order to prove the space is not complete w.r.t to the given norm, we have to find a Cauchy sequence in the space but it does not converge to an element in the given set. My attempt was taking $(Z^n) = ( (1,0,0, ... ) , (1, 1/2, 0, 0, ... ), ....)$ I tried to prove this is Cauchy and ended up with, here $m>n$ , $\lVert Z^n - Z^m \rVert  \leq \dfrac {(m-n)^{1/2}}{n}$ how could I make this arbitrarily small? ( $<\epsilon$ ) here the $n$ th term of the sequence would be $Z^n = (1,1/2,1/3,...,1/n,0,0,0...)$ So when $n \rightarrow \infty$ , $z^n$ has infinitely many non zero terms, would that be enough to say the space is incomeplete?","['banach-spaces', 'functional-analysis']"
4656752,Generating representatives for cosets,"Let $p$ be a prime number and let $n\geq 1$ be coprime to $p$ . Consider the groups $$G:=(\mathbb Z/n\mathbb Z)^\times\ni p,\quad H:=\langle p\rangle\leq G.$$ We may now form the quotient $G/H$ , giving us say $k$ cosets. It is often desirable to have a system of representatives $g_1,\dots,g_k$ of the cosets, and it would sometimes be even more useful to have all the $g_i$ be a power of the same $g$ . Hence the question: Do we always manage to find some $g\in G$ for which $1,g,\dots, g^{k-1}$ forms a system of representatives? In case $p$ generates $G$ , there is of course nothing to prove, just take $g=1$ . However, what if $p$ does not generate $G$ ? For instance, consider $m=11$ and $p=3$ . Then $\langle p\rangle=\{1,3,4,5,9\}$ , and the choice $g=1$ does not work. In this case, we have two cosets to consider, and the elements $2$ and $2^2$ lie in two different cosets, so that we can take $g=2$ . Is this always possible?","['elementary-number-theory', 'group-theory', 'modular-arithmetic']"
4656765,Is it always possible to construct a distribution with full support?,"Does any (arbitrary) measurable topological space admit a probability distribution with full support? If not, what is a counterexample?","['support-function', 'measure-theory', 'probability-distributions']"
4656767,Isosceles triangle generated by Intersecting circles and a point on one of them,"I am working on geometry exercises and struggling with a subproblem: From a point $O$ on circle $C_1$ , draw a smaller circle $C_2$ , so they intersect at $X$ and $Y$ .
Choose a point $R$ from $C_2$ , extend $XR$ to meet $C_1$ again at $S$ . Prove that $\triangle RSY$ is isosceles. Here is my progress: In triangles $OSY$ and $OSR$ , $\angle OSY = \angle OSR$ (angles subtended by equal radii of $OY$ and $OX$ of $C_2$ ), $OS$ is the common side, and $OY = OR$ being radii of $C_2$ . I wished to show that $\triangle OSY \cong \triangle OSR$ but here I got the side-side-angle condition which is not working. Please follow the scale on the diagram if in doubt.","['contest-math', 'geometry']"
4656799,Understanding the application of Yoneda's Lemma on Sheaves,"Let $\mathcal{G},\mathcal{G}'$ be sheafs on a topological space $X$ such that $$\operatorname{Hom}_{\mathcal{O}_X}(\mathcal{G},\mathcal{F})\cong\operatorname{Hom}_{\mathcal{O}_X}(\mathcal{G}',\mathcal{F})$$ for all sheaves $\mathcal{F}\in\operatorname{Sh}(X)$ . Then why does it follow that $\mathcal{G}\cong \mathcal{G}'$ ? I understand this goes along the lines explicitly as: taking $\mathcal{F}=\mathcal{G}$ , we get a morphism $\varphi:\mathcal{G}'\rightarrow\mathcal{G}$ corresponding the the identity on $\mathcal{G}$ from the above isomorphism; similarly, we have a morphism $\psi:\mathcal{G}\rightarrow\mathcal{G}'$ corresponding to the identity of $\mathcal{G}'$ . The claim is that from naturality of the morphisms, $\varphi$ and $\psi$ are inverses of each other, but I having trouble arriving to this conclusion. This is equivalent to saying that I can't exactly come to terms why naturality gives the claim, though the diagram hints something along its lines. How does this follow? I will be really grateful for any help!","['algebraic-geometry', 'category-theory', 'sheaf-theory']"
4656801,Conditional Probability With a Madman,"There are three six-chambered revolvers. The first has no bullets; the
second has one bullet; and the third has two bullets in consecutive
chambers. The cylinder advances automatically as the trigger is
pulled. A madman grabs a revolver at random, aims it at his own head,
pulls the trigger t times, and no shot is fired. He then aims at your
head and pulls the trigger once. What is the probability that you are
shot given t? The provided answers are $[0.167 (t = 0), 0.133 (t = 1), 0.154 (t = 2), 0.182 (t = 3), 0.222 (t = 4), 0.143 (t = 5), 0 (t ≥ 6)]$ I can't seem to match it. Can someone tell me where I'm wrong? My attempt: $\Pr(S|t = 0) = \dfrac{1}{3}[0 + 1/6 + 2/6] = 0.167$ $\Pr(S|t = 1) = \dfrac{1}{3}[0 + (5/6)(1/6) + (4/6)(2/6)] = 0.144..$ $\Pr(S|t = 2) = \dfrac{1}{3}[0 + (5/6)(4/5)(1/4) + (4/6)(3/5)(2/4)] = 0.122..$ I stopped here cause my answers weren't matching.","['conditional-probability', 'probability']"
4656817,Evaluating $\lim\limits_{x\rightarrow0}\frac{e^{-1/x^2}}{x}$,"Today in my analysis class, we were preparing for the final and this question came up: Evaluate $$\lim_{x\to0}\frac{e^{-1/x^2}}{x}$$ We tried taking the $\log$, using L'Hopitals and some other tricks but couldn't figure it out. I thought maybe viewing this limits as a sequence in the following way might help; $$\lim_{x\rightarrow0}\frac{e^{-1/x^2}}{x} = \lim_{n\rightarrow \infty}\frac{e^{-n^2}}{1/n} = \lim_{n\rightarrow \infty}\frac{n}{e^{n^2}}$$ But from them I'm not sure. Thank you.","['limits', 'real-analysis']"
4656947,First step of an alternative proof of Bennett's inequality - Moment Generating Function inequality,"I'm working on a problem that uses an alternative approach of proving Bennett's inequality. In the common approach, one uses Taylor expansion for the MGF to derive an expression which is then used to prove Bennett's inequality. However, this does not seem to be the case here. Show that for any $s > 0$ , and any random variable $X$ with $\mathbb{E}(X) = 0$ , $\mathbb{E}(X^2) = \sigma^2, X \leq c$ , $$ \mathbb{E}(e^{sX}) \leq e^{f(\sigma^2/c^2)}, $$ where $$ f(u) = \log \left( \frac{1}{1+u}e^{-csu} + \frac{u}{1+u}e^{cs} \right). $$ Can anyone see how to go about proving this first step? It looks like this result uses some kind of convexity argument but I cannot figure out how they go about deriving this exact expression.","['alternative-proof', 'moment-generating-functions', 'inequality', 'probability-theory', 'probability']"
4656963,Reference on the sum of absolute differences between $n$ samples from a random variable,"Let $X_1, X_2,\ldots X_n$ be $n$ samples taken of a random variable with a given distribution (so in particular it is i.i.d.). Is there literature on or a name for the random variable defined by $$Y = \sum_{i=1}^{n-1}\left|X_i-X_{i+1}\right|?$$ If necessary feel free to add further assumptions, such as $X_i$ being non-negative, or the $X_i$ being drawn from specific distributions.","['statistics', 'probability-distributions', 'reference-request', 'order-statistics', 'probability']"
4656967,Integral $\int_0^\infty\frac{dx}{\sqrt{1+\exp\left(\frac\pi2\left(x^2-\frac1{x^2}\right)\right) }}=\sqrt{\frac\pi2}$,"Someone posted the integral on a local chat group $$
\int_0^\infty\frac{dx}{\sqrt{1+\exp\left(\dfrac\pi2\left(x^2-\dfrac1{x^2}\right)\right) }}=\sqrt{\frac\pi2}
$$ It is interesting that the integrand is quite messy but the result is neat. Using CAS, I checked that the integral holds with high precision. The first thing that came into my mind was the Glasser's master theorem , but it is not in an acceptable form. I also tried substituting $\dfrac{x^4-1}{2x^2}=t$ but the integral becomes even worse. $$
\int_{-\infty}^\infty\frac{\sqrt{\sqrt{t^2+1}+t}}{\sqrt{e^{\pi  t}+1} \sqrt{t^2+1}}dt=\sqrt{2\pi}
$$ Are there any ways to work out the integral? Or perhaps there is an approach to find the result magically? Any help would be appreciated.",['integration']
4657006,Inequality involving changing order of limits and probability,"I read this paper , in Corollary 1 the author claims that $$\underset{\pi \in [0, 1]}{\sup}\ W_T(\pi) \overset{p}{\to} \infty$$ as $T \to \infty$ . Where $W_T(\pi)$ is Wald statistics but I think it should not matter for my question. The first step of the proof (formula A.35 in the paper) goes like this: Take any positive $c$ , $$\lim_{T \to \infty} \mathbb{P}(\underset{\pi \in [0, 1]}{\sup}\ W_T(\pi) < c) \le \lim_{\varepsilon \to 0}\sup\ \lim_{T \to \infty} \mathbb{P}(\underset{\pi \in [\varepsilon, 1-\varepsilon]}{\sup}\ W_T(\pi) < c)$$ and then he proves that the expression on the RHS is zero.
For me it is unclear where this first inequality comes from. I think, using the monotone convergence theorem we can get $$\lim_{T \to \infty} \mathbb{P}(\underset{\pi \in [0, 1]}{\sup}\ W_T(\pi) < c) = \lim_{T \to \infty} \lim_{\varepsilon \to 0} \mathbb{P}(\underset{\pi \in [\varepsilon, 1-\varepsilon]}{\sup}\ W_T(\pi) < c)$$ but I do not think that in general we can say that $$\lim_{T \to \infty} \lim_{\varepsilon \to 0} \mathbb{P}(\underset{\pi \in [\varepsilon, 1-\varepsilon]}{\sup}\ W_T(\pi) < c) \le \lim_{\varepsilon \to 0}\sup\ \lim_{T \to \infty} \mathbb{P}(\underset{\pi \in [\varepsilon, 1-\varepsilon]}{\sup}\ W_T(\pi) < c)$$ take for example $a_{T, \varepsilon} = 1\ \{ \frac{1}{\varepsilon} > T \}$ then $$1 = \lim_{T \to \infty} \lim_{\varepsilon \to 0} a_{T, \varepsilon} > \lim_{\varepsilon \to 0}\sup\ \lim_{T \to \infty} a_{T, \varepsilon} = 0$$ Do you have any ideas what is implicitly assumed in the proof to justify this step? Or am I missing something? Thanks so much!","['time-series', 'limsup-and-liminf', 'statistics', 'probability-theory']"
4657026,"Conflicting results for $\int\frac{|x+1|}{x}\,\mathrm dx$","I've encountered an obstacle while trying to solve the following integral: $$\int{\frac{\sqrt{x^2+2x+1}}{x}dx}$$ First thing we shall do is see that under the square root is actually $(x+1)^2$ . When I cancelled the square root I got the following integral: $$\int{\frac{|x+1|}{x}\,\mathrm dx}$$ After that I've separated the problem into two cases: $1.$ Case: $$\int{\frac{\sqrt{x^2+2x+1}}{x}dx}=-x-\ln|x|+C,\space x<-1$$ $2.$ Case: $$\int{\frac{\sqrt{x^2+2x+1}}{x}dx}=x+\ln|x|+C,\space x>-1, x\neq{0}$$ But this apparently isn't the correct solution and the only solution is: $$\int{\frac{\sqrt{x^2+2x+1}}{x}dx}=x+\ln|x|+C,\space x\neq{0}$$ Is this really the case, and if yes, then why?","['integration', 'absolute-value', 'calculus', 'indefinite-integrals', 'piecewise-continuity']"
4657051,Exponential stability of a time varying system,"I want to show that the system below is exponentially stable and I want to estimate its region of attraction \begin{align}
\dot x_1 &= -x_1+x_2+(x_1^2+x_2^2)\sin(t) \\
\dot x_2 &= -x_1-x_2+(x_1^2+x_2^2)\cos(t)
\end{align} I tried using a Lyapunov function $V(x_1,x_2)=\frac 12 (x_1^2+x_2^2)$ and I have found \begin{align}
\dot V(x_1,x_2) &= -x_1^2+x_1x_2+x_1(x_1^2+x_2^2)\sin(t)-x_2x_1-x_2^2+x_2(x_1^2+x_2^2)\cos(t) \\
&\le -x_1^2-x_2^2+x_1(x_1^2+x_2^2)+x_2(x_1^2+x_2^2)
\end{align} And when $x_1^2+x_2^2 \le 1$ we have \begin{align}
\dot V &\le -x_1^2-x_2^2+x_1+x_2
\end{align} so that $\dot V \le 0$ for $\lvert x_1 \rvert \ge 1$ and $\lvert x_2 \rvert \ge 1$ . Hence, we can only have the 2 conditions when $\dot V = 0$ and $x_1^2 + x_2^2 = 1$ . I suppose the region of attraction should be the unit circle. I'm not sure if I need to find a Lyapunov function with a negative definite derivative to ensure exponential stability, as this condition is more closely related to asymptotic stability, isn't it? EDIT: The equilibrium is $(0,0)$ , I linearised the system and got a Jacobian with a negative real part for the eigenvalues. Therefore, I can use the Lyapunov equation $A^TP+PA=-Q$ with A equal to the Jacobian and $Q=I$ and get a matrix $P$ so that $V=x^TPx$ and $\dot V = -x^TQx$ . However, I'm not sure if this gives the exponential stability?","['nonlinear-system', 'stability-theory', 'ordinary-differential-equations', 'lyapunov-functions']"
4657054,Goniometry exercise,"I have this exercise n. 161 Where am I going wrong with my steps? My solution :
We know that $\hat A+\hat B+\hat C=\pi \implies \frac{\hat A+\hat C}2=\frac{\pi}{2}-\frac{\hat B}{2}$ .
Hence $$\sin A\hat D C=\sin\biggl(\pi-\frac{\hat A+\hat C}2\biggr)=\sin\biggl(\pi -\frac{\pi}{2}+\frac{\hat B}{2}\biggr)=\sin\biggl(\frac{\pi}{2}+\frac{\hat B}{2}\biggr)=\cos\frac{\hat B}{2}$$ But $\cos\frac{\hat B}{2}=\sqrt{\frac{1+\frac{5}{13}}2}=\frac{3\sqrt{13}}{13}$ . $$\tan C\hat A D=\frac{\sin C\hat A D}{\cos C\hat A D}$$ $C\hat A D=\pi-A\hat CD-A\hat DC$ , but $\sin(C\hat A D)=\sin(\pi-(A\hat CD+A\hat DC))=\sin(A\hat CD+A\hat DC)$ . $$\sin(A\hat CD+A\hat DC)=\sin A\hat CD\cos A\hat DC+\sin A\hat DC\cos A\hat CD=$$ $$=\sqrt{1-\cos^2A\hat CD}\sqrt{1-\sin^2A\hat DC}+\frac{3\sqrt{13}}{13}\frac 45$$ $$=\sqrt{1-\frac{16}{25}}\, \cdot \sqrt{1-\frac{9}{13}}+\frac{3\sqrt{13}}{13}\frac 45=\frac{6}{5\sqrt{13}}+\frac{12}{5\sqrt{13}}$$ For $\cos C\hat A D$ the same steps. $$\cos(C\hat A D)=\cos(\pi-(A\hat CD+A\hat DC))=$$ $$-\cos(A\hat CD+A\hat DC) =-\cos A\hat CD\cos A\hat DC+\sin A\hat DC\sin A\hat CD$$ $$=-\frac 45\cdot \frac{\sqrt 5}{13}+\frac{3\sqrt{13}}{13}\cdot \frac 35$$ Wtih my steps $\tan C\hat A D \neq 6/17$ .","['algebra-precalculus', 'trigonometry']"
4657082,How to solve $\frac{\sin\theta+\cos\theta}{\sec\theta+\csc\theta} = 1 /\sqrt8$,"$$\frac{\sin\theta+\cos\theta}{\sec\theta+\csc\theta} = 1/ \sqrt8$$ Here is my steps $\frac{\sin\theta+\cos\theta}{\frac{\sin\theta+\cos\theta}{\sin\theta\cos\theta}} = 1/√8$ $\sin\theta+\cos\theta * \frac{\sin\theta\cos\theta}{\sin\theta+\cos\theta} = 1/√8$ And then I multiplied sin+cos with denominator to get same denominator and after this, $\frac{\sin^2\theta+(2)\sin\theta\cos\theta+\cos^2\theta*\sin\theta\cos\theta}{\sin\theta+\cos\theta} = 1/√8$ How to carry on?",['trigonometry']
4657112,Partial sum of alternating series involving binomials,"I ran across an interesting expression that I cannot prove (but tested numerically): $$
1 = \sum_{j=0}^{n} (-1)^j \binom{n+i}{n-j-1} \binom{n+j}{n-i-1} \binom{i+j}{i}
$$ for any $0 \leq i < n$ . In fact I cannot even prove the case with $i = 0$ -- Wolfram alpha knows it is true somehow, but gives no hints. I looked at at the identities involving binomials at wikipedia and mathworld. The closest identity that I found was Dixon's identity but I failed to rewrite the problem in to apply it. Rewriting the problem with the usual binomial identities (binomial complement, etc.) seems to get nowhere or least the strategy is unclear to me. By using the definition of binomial and rewriting in factorials did not work either. I am not sure how to approach this kind of proof, any suggestions?","['summation', 'binomial-coefficients', 'combinatorics']"
4657145,Topology to define continuous functions to the discrete plane,"If you have a function from $\mathbb R \to \mathbb R \times \mathbb R$ , you can define a continuous function as a function for which the preimage of every open subset in $\mathbb R \times \mathbb R$ is an open subset of $\mathbb R$ . In the discrete case, you can define functions that ""look continuous"" from $\mathbb N \to \mathbb N \times \mathbb N$ as functions so that for every step of 1, the image point is adjacent to the original point. Is it possible to consider topologies on $\mathbb N \to \mathbb N$ and $\mathbb N$ so that the definition of the continuity ""preimage of open is open"" in the first paragraph leads to exactly the sets of ""continuous"" functions defined in the second paragraph ?","['general-topology', 'discrete-mathematics']"
4657153,Distribution of Difference of Independent Inverse Gaussians,"Suppose, $X_1$ and $X_2$ are independent inverse Gaussian random variables with parameters $(\mu, \lambda_1)$ and $(\mu, \lambda_2)$ , respectively.  (See Wikipedia for the notation). I am interested in the following probability (and whether or not it has a closed form solution): $$ P(X_1 < X_2) = \int_0^\infty \int_0^y \exp \left(- \frac{\lambda_1(x-\mu)^2}{2\mu^2x} - \frac{\lambda_2(y-\mu)^2}{2\mu^2y} \right) \sqrt{\frac{\lambda_1 \lambda_2}{(2\pi)^2 x^3 y^3}} dx dy$$ $$= \int_0^\infty e^{-\frac{\lambda_2(y-\mu)^2}{2\mu^2y}}\left(\Phi \left(\sqrt{\frac{\lambda_1}{y}}\left(\frac{y}{\mu} -1\right) \right) + e^{2\lambda_1/\mu}\Phi \left(-\sqrt{\frac{\lambda_1}{y}}\left(\frac{y}{\mu} +1\right) \right) \right) \sqrt{\frac{\lambda_2}{2\pi y^3}}dy$$ where $\Phi$ is the cdf (Cumulative distribution function), of a standard normal random variable. I have no idea how to deal with this integral.  The reason I am interested in its closed form is due to the fact that for the case $\mu = 0$ , there is a very nice closed form solution relating to the hitting times of two independent Brownian motions ( see here ). I have tried consulting this table for hints of how to deal with the integral of the erf function , against some other complicated term, to no avail. Any help would be massively appreciated!","['probability-distributions', 'definite-integrals', 'normal-distribution', 'probability']"
4657181,Fundamental theorem of calculus with the derivative on the inside?,I know that: $$\frac{d}{dx}\int_{a}^{g(x)} f(t)dt = f(g(x))*g'(x)$$ But what about: $$\int_{a}^{g(x)}\frac{d}{dx}f(x)dx$$ An example of this would be: $$\int_{3}^{t^3}\frac{d}{dx}\frac{x}{x-2}dx$$ Do we apply the chain rule with the $t^3$ ? This is the question and solution:,"['integration', 'calculus', 'derivatives', 'leibniz-integral-rule']"
4657224,Examples of rings where every left ideal is two-sided but not every right ideal,"Is there an example of a ring where every left-ideal is two-sided but not every right ideal? WHAT FOLLOWS IS A FAILED EXAMPLE As an argument but not a proof that the example fails, consider: $ f(a) \;a\; f(a) = 1 f(a) = f(a) $ Thus $f(a) (a \; f(a)) = f(a) \times 1$ This suggests that $(a \; f(a))$ must also be $1$ (it certainly can't equal $f(a)$ since $f(a)$ can't be idempotent), but I'm not sure how to make this an actual argument. Let $F$ be the completely free algebra in the signature $(+, \times, f, \mathbb{Z}[w])$ where $+$ and $\times$ are binary functions, $f$ is a unary, and $\mathbb{Z}$ is a set of constants. Additionally, let the function $d : F \to \mathbb{N}$ be defined as follows: $d(\mathbb{Z})$ is 0 $d(f(x))$ is $1 + d(x)$ for all $x$ . $d(x+y)$ is the maximum of $d(x)$ and $d(y)$ for all $x$ and $y$ . $d(x\times y) = d(x+y)$ Next, I will build a set of equations, $\Delta$ . The intent of this construction is to make $f(x)$ be a left inverse but not a right inverse of $x$ for every $x$ . Let $\Sigma_0$ consist of the axioms of non-commutative rings, all variable-free true equations (i.e. of the form $t_1(\varnothing) = t_2(\varnothing)$ ) in $\mathbb{Z}$ , and the variable-free equation $f(0) = 0$ . Note that $\Delta_0$ is $f$ -free except for a single sentence. Let $\Delta_0$ be the equational deductive closure of $\Sigma_0$ . Note that $\Delta_0$ does not include the non-theorem $xy = yx$ because we can't conclude that it's true. We know that $xy = yx$ holds of all integers $\mathbb{Z}[w]$ , but can't conclude that only integers exist. Let $\Sigma_1$ be the union of $\Delta_0$ , sentences $f(x)x = 1$ for each $x$ such that $d(x) = 0$ and $x \neq 0$ is in $\Delta_0$ , and sentences $f(x)x = 0$ for each $x$ such that $d(x) = 0$ and $x = 0$ is in $\Delta_0$ . Let $\Delta_1$ be the equational deductive closure of $\Sigma_1$ . Analogously, let $\Sigma_{n+1}$ be the union of $\Delta_n$ , sentences $f(x)x = 1$ for each $x$ such that $d(x) = n$ and $x \neq 0$ is in $\Delta_n$ , and sentences $f(x)x = 0$ for each $x$ such that $d(x) = n$ and $x = 0$ is in $\Delta_n$ . Let $\Delta_{n+1}$ be the equational deductive closure of $\Sigma_{n+1}$ . I define $\Delta$ as $\cup_{n \in \mathbb{N}}\Delta_n$ . I define $A$ as $F/\Delta$ , which is well-defined since $F$ is a completely-free algebra and $\Delta$ is a set of equations. I claim that $A$ has precisely two left ideals. Let $I$ be a left ideal of $A$ . Suppose $I$ is $\{0\}$ , then $I$ is closed under left multiplication by arbitrary elements of $A$ . Suppose $I$ is not $\{0\}$ . It follows that $I$ has a nonzero element $c$ . $f(c)$ is a left inverse of $c$ and thus left multiplication by $rf(c)$ will hit the arbitrarily chosen ring element $r$ since $f(c)c = 1$ and thus $(rf(c))c = r(f(c)c) = r1 = r$ . Thus, if $I$ is not $(0)$ , then it is $(1)$ . Every left ideal of $A$ is two-sided since $(0)$ and $(1)$ are both two-sided ideals.","['ring-theory', 'abstract-algebra', 'noncommutative-algebra', 'examples-counterexamples']"
4657238,"Chebyshev's bias limit doesn't exist, but what about the lim sup and lim inf?","It's known that there is a bias towards primes being 3 mod 4 v.s. 1 mod 4, but that the primes 3 mod 4 aren't in the lead 100% of the time (more primes up to 26861 are 1 mod 4, the first time they get ahead); in fact, it's been proved the fraction of the time primes mod 3 are ahead does not tend to any limit (though see the earlier link that they do have a logarithmic density of about .9959) Is it known what the lim sup and lim inf of the fraction of time primes mod 3 are in the lead is? If the lim inf is greater than 1/2, that'd be a strong result in this direction. I'm also interested in any conjectures or heuristics that are relevant here if the answer is not known.","['number-theory', 'prime-numbers']"
4657239,Possibility of computing antiderivative using dual numbers,"It is known that, given a function $f(x)$ , plugging in the dual number $x+\varepsilon$ , where $\varepsilon^2=0$ , yields $f(x) + f'(x)\varepsilon$ . For example: $$f(x) = x^3\\f(x+\varepsilon)=(x+\varepsilon)(x+\varepsilon)(x+\varepsilon)\\f(x+\varepsilon) = (x^2+2x\varepsilon)(x+\varepsilon)\\f(x+\varepsilon) = x^3 + 3x^2\varepsilon$$ We observe that $x^3$ is the value of $f(x)$ and $3x^2$ is the derivative of $f(x)$ . I would like to know if the process is reversible in such a way that, given a few data points demonstrating the derivative of some function $g(x)$ , such as $$g(1+\varepsilon) = a_1 + \varepsilon\\g(2+\varepsilon) = a_2 + 4\varepsilon\\g(3+\varepsilon) = a_3 + 9\varepsilon\\g(4+\varepsilon) = a_4+16\varepsilon,$$ could we algorithmically recover $g(x)$ ? We as humans can see that the pattern among the dual parts is the series of squares, so $g'(x)=x^2$ , telling us that $g(x)$ must be $\int{g'(x)}dx$ , which would be $\frac{1}{3}x^3+C$ . I don't have much hope for it due to the significant hurdles in the way, such as the data-fitting aspect of trying to decide what function is being represented by the dual parts and the fact that we are losing information when we multiply $\varepsilon$ by itself. It would be extremely convenient to have a quick and simple way of computing antiderivatives though.","['integration', 'dual-numbers', 'derivatives']"
4657247,Sum of indicator of pairwise disjoint sets is the indicator of the union of those sets,"Let $(S,\Sigma,\mu)$ be a measure space.
Show that if $(A_n)$ is a pairwise disjoint sequence of sets in $\Sigma$ and $A = \bigcup_n A_n$ then $$1_A = \sum_n 1_{A_n}.$$ My try: To show that $1_A = \sum_n 1_{A_n}$ , we need to show that they are equal on every point $x\in S$ . Fix $x\in S$ . We have two cases to consider: either $x\in A$ or $x\not\in A$ . If $x\in A$ , then by definition of $A$ , there exists $n$ such that $x\in A_n$ . Therefore, $1_A(x) = 1$ and $1_{A_n}(x) = 1$ for that $n$ , and $1_{A_k}(x) = 0$ for all $k\neq n$ . Thus, we have $$\sum_n 1_{A_n}(x) = 1_{A_n}(x) = 1,$$ and so $1_A(x) = \sum_n 1_{A_n}(x)$ . If $x\not\in A$ , then $x\not\in A_n$ for all $n$ , and so $1_A(x) = 0$ and $1_{A_n}(x) = 0$ for all $n$ . Thus, we have $$\sum_n 1_{A_n}(x) = 0,$$ and so $1_A(x) = \sum_n 1_{A_n}(x)$ . Since we have shown that $1_A(x) = \sum_n 1_{A_n}(x)$ for every $x\in S$ , we conclude that $1_A = \sum_n 1_{A_n}$ . Question Does my logic make sense? Should I only discuss a point in $x \in A$ or I can use the union as well?","['measure-theory', 'solution-verification']"
4657266,Applications of the Triangle Inequality Proof,"I am trying to understand the proof that the forecast error for an ARIMA(p,d,q) model becomes larger when the forecast period grows larger. Suppose I take the ARIMA(p, q, d) : $$\nabla^d y_t = \sum_{i=1}^p \phi_i \nabla^d y_{t-i} + \sum_{j=1}^q \theta_j e_{t-j} + e_t$$ $$\nabla^d y_t = \phi_1 \nabla^d y_{t-1} + \dots + \phi_p \nabla^d y_{t-p} + e_t - \theta_1 e_{t-1} - \dots - \theta_q e_{t-q}$$ $$y_t - y_{t-d} = \phi_1 (y_{t-1} - y_{t-d-1}) + \dots + \phi_p (y_{t-p} - y_{t-d-p}) + e_t - \theta_1 e_{t-1} - \dots - \theta_q e_{t-q}$$ $$y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + e_t - \theta_1 e_{t-1} - \dots - \theta_q e_{t-q} + y_{t-d}$$ $$y_t = c + \sum_{i=1}^p \phi_i y_{t-i} + \sum_{j=1}^q \theta_j e_{t-j} + e_t + y_{t-d}$$ where: $y_t$ : the value of the time series at time t. $\nabla^d$ : the differencing operator, which takes the difference between the value of the time series at time t and the value at time t-d. The parameter d determines the order of differencing. $\phi_1, \dots, \phi_p$ : the autoregressive coefficients, which specify the dependence of the current value of the time series on its past values. The parameter p determines the order of the autoregressive model. $e_t$ : the error term at time t, which represents the deviation of the actual value of the time series from the predicted value based on past values. $\theta_1, \dots, \theta_q$ : the moving average coefficients, which specify the dependence of the current value of the time series on the past error terms. The parameter q determines the order of the moving average model. $y_{t-d}$ : the value of the time series at time t-d, which is used when performing differencing with parameter d. $c$ : a constant term, which represents the mean value of the time series when all other terms are zero. This term is optional and can be omitted from the model. $c = y_d - \phi_1 y_{d-1} - \dots - \phi_p y_{d-p}$ is a constant. The forecast at time $t+h$ , where $h$ is the number of time steps ahead, can be written as: $$\hat{y}{t+h} = c + \phi_1 y{t+h-1} + \cdots + \phi_p y_{t+h-p} - \theta_1 e_{t+h-1} - \cdots - \theta_q e_{t+h-q}$$ where: $y_t$ is the observed value at time $t$ $c$ is the constant term $\phi_1,\ldots,\phi_p$ are the autoregressive parameters $\theta_1,\ldots,\theta_q$ are the moving average parameters $e_t$ is the error term at time $t$ . The prediction error at time $t+h$ is then given by: $$e_{t+h} = y_{t+h} - \hat{y}_{t+h}$$ Here is where I get confused: Suppose we take the absolute value of this error and applying the triangle inequality -  we get: $$|e_{t+h}| = |y_{t+h} - \hat{y}{t+h}| \leq |y{t+h} - y_{t+h-1}| + |y_{t+h-1} - \hat{y}_{t+h-1}|$$ The first term on the right-hand side is simply the size of the change in the observed values between time $t+h$ and time $t+h-1$ . The second term is the prediction error at time $t+h-1$ . Substituting in the expression for $\hat{y}_{t+h-1}$ and taking the absolute value, we get: $$|y_{t+h-1} - \hat{y}{t+h-1}| = |e{t+h-1}| \leq |y_{t+h-1} - y_{t+h-2}| + |y_{t+h-2} - \hat{y}_{t+h-2}|$$ Repeating this process for $h-1$ steps, we get: $$|e_{t+h}| \leq |y_{t+h} - y_{t+h-1}| + \cdots + |y_{t+1} - y_t| + |e_{t+1}|$$ Now, assuming that the errors are uncorrelated and have constant variance, the expected value of the square of the prediction error at time $t+h$ is given by: $$\operatorname{E}(|e_{t+h}|^2) = h \sigma^2$$ where $\sigma^2$ is the variance of the error term. Thus, as the forecast horizon $h$ increases, the expected value of the prediction error grows linearly with $h$ . I am not sure that I fully understand how the triangle identity can be used in this proof - can someone please help me understand?","['geometry', 'probability']"
4657276,Adjoining a function to a ring: what is this called?,"There's a kind of construction of an extension of a ring, and that I've seen used e.g. here (although that may not be the best example) which is essentially adding a function into the ring, and taking the closure under the function. So for every element $r$ in $R_0$ you would adjoin elements $f(r)$ with no relations to make $R_1$ , and do this again to $R_1$ to make $R_2$ etc. so that $R_\omega$ , the limit of these, is closed under a free unary operation $f$ . I hope I have described this clearly. I would like to know what the terminology here is, and where I can learn more about using this construction and generalisations of it. E.g. with what I currently know, even defining a quotient by a relation, like $f(0)=1$ seems very messy, as you have to be able to describe all the other elements in the ring that have $f(0)$ as part of their construction. So I want to see how this is usually dealt with, and if there's a convenient way of defining variations like that which makes it easy to work with. Worse, proving that any two things aren't equal after taking a massive quotient seems a near-impossible task, but I'm sure there are neat ways of doing it that I'm just not aware of. Edit: @diracdeltafunk provided a nice universal property that makes this easier to work with. I would still like a reference for where to learn more about constructions like this, though.","['universal-algebra', 'ring-theory', 'abstract-algebra', 'reference-request']"
4657291,How to manipulate differential equation into Weierstrass P form?,"I have the diffeq $y'^2=\frac23 y^3+\alpha$ for some $\alpha\in\mathbb R$ . This has two solutions in the form of a Weierstrass P elliptic function which are $$6^{1/3}\wp\left(\frac{x\pm C}{6^{1/3}};0, \alpha\right)$$ The latter two arguments make sense, since they are just the $g$ constants in the weierstrass diffeq $y'^2=4y^3-g_2y-g_3$ , but I am having trouble getting the $6^{1/3}$ factors and the main argument. What I attempted to do was use the substitution $u(x)=6^{1/3}y(x)$ . This generates the differential $dy=\frac{du}{6^{1/3}}$ which I can substitute into my diffeq to get $$\left(\frac{dy}{dx}\right)^2=\frac23 y^3+\alpha\Longleftrightarrow \left(\frac{du}{6^{1/3}dx}\right)^2=\frac23\left(\frac{u}{6^{1/3}}\right)^3+\alpha\Longleftrightarrow\frac{1}{6^{2/3}}u'^2=4u^3+\alpha$$ This is close, but not quite correct as there is an extra factor on the LHS. Any hints as to how to proceed?
Thanks Alternatively, (since the diffeq is seperable) if anyone is able to reduce the integral (which is the square root of a cubic) down to an elliptic integral of the first kind and invert it, that works too, but I think this would take more work so I'm not too sure. Edit : Additionally, I have also tried solving the diffeq given here using the inverse Weierstrass P function $$\wp^{-1}(z; g_2, g_3)=\int_{\infty}^z\frac{dt}{\sqrt{4t^3-g_2t-g_3}}$$ which after seperation gives me something like $$\int\frac{du}{6^{1/3}\sqrt{4u^3+\alpha}}=\int\pm1dx$$ $$\wp^{-1}(u; 0, \alpha) = 6^{1/3}(C\pm x)\tag{???}$$ This is quite close but still not quite right since the factor of $6^{1/3}$ is not in the correct part of the equation. Furthermore, I'm not sure how to relate the bounds of the definite integral definition to the indefinite one (for example, there could be a negative I am missing or smth). Help would be appreciated!","['substitution', 'elliptic-functions', 'ordinary-differential-equations']"
4657298,Does $f=g$ (a.e.) and $g=h$ (a.e.) imply $f=g$ (a.e.) when the measure is not complete?,"Let $(S,\Sigma, \mu)$ be a measure space. Let $f,g,h:S\rightarrow \overline{\mathbb{R}}$ be functions. Suppose $f = g$ a.e. and $g = h$ a.e., show that $f = h$ a.e.. My try: I can show the above claim when the measure is complete as follows: Since $f = g$ a.e., we have $\mu(\{x\in S : f(x) \neq g(x)\}) = 0$ . Similarly, since $g = h$ a.e., we have $\mu(\{x\in S : g(x) \neq h(x)\}) = 0$ . Let $E = \{x\in S : f(x) \neq h(x)\}$ . Then we have: \begin{align*}
\mu(E) &= \mu(\{x\in S : f(x) \neq h(x)\}) \\
&= \mu(\{x\in S : f(x) \neq g(x) \text{ or } g(x) \neq h(x)\}) \\
&\leq \mu(\{x\in S : f(x) \neq g(x)\}) + \mu(\{x\in S : g(x) \neq h(x)\}) \\
&= 0 + 0 = 0.
\end{align*} Therefore, we have $\mu(E) = 0$ , which implies that $f = h$ almost everywhere on $S$ . Question I am not sure what I should do when the measure is not complete.",['measure-theory']
4657327,How to count - probability puzzle,"A $3 \times 3 \times 3$ big cube consists of $1 \times 1 \times 1$ smaller cubes. The big cube is painted black on the outside. Suppose we disassemble the cube and randomly put it back together. What is the probability of perfectly assembling the cube i.e. all faces are black again? Do you think my partial progress is correct? If not, could you provide a hint/clue (without giving away the entire solution)? Solution (My Attempt) . There are $8$ corner(vertex) cubes, having $3$ faces black - label them $V$ There are $12$ edge cubes, having $2$ faces black - label them $E$ There are $6$ center of face cubes, having $1$ face black - label it $C$ There is $1$ origin cube, that is not black - label it $O$ For the $1 \times 1 \times 1$ cube, there are $6$ choices for the top face, and for each such choice, $4$ choices for an adjacent face. This completely determines the orientation. So, there are $24$ distinguishable orientations of the unit cube. The probability that a $V$ cube is correctly oriented = $\frac{1}{24}$ . The probability that a $E$ cube is correctly oriented = $\frac{1}{24}$ . The probability that a $C$ cube is correctly oriented = $\frac{4}{24}$ . The probability that a $O$ cube is correctly oriented = $1$ . Furthermore, we can swap one corner cube for another, one edge cube for another and so forth leaving the outer appearance unchanged. So, there are $\frac{27!}{8!12!6!1!}$ favourable arrangements of the cubes. Putting it all together: $$P \{\text{Perfectly assembling the cube} \} = \frac{1}{24^8} \times \frac{1}{24^{12}} \times \left(\frac{4}{24}\right)^6 \times \frac{\frac{27!}{8!12!6!1!}}{27!}$$","['puzzle', 'solution-verification', 'combinatorics', 'probability']"
4657340,Are these conditions sufficient to guarantee positive definiteness?,"Let $f: \mathbb{C}^2 \to \mathbb{C}^n$ be the map $$ f(u, v) = (u^{n-1}, u^{n-2}v, \ldots, u^{n-k} v^{k-1}, \ldots, v^{n-1})^T$$ so that $f$ is the rational normal curve in homogeneous coordinates. Note that the image of $f$ can be described as: $$ \operatorname{Im}(f) = \{ c (1, z, z^2, \ldots, z^{n-1})^T; c, z \in \mathbb{C} \} \cup \{ d (0, 0, \ldots, 0, 1)^T ; d \in \mathbb{C} \}. $$ Let $H$ be an hermitian $n \times n$ matrix such that $$ (v, Hv) > 0, $$ for any $0 \neq v \in \operatorname{Im}(f)$ , where $(-, -)$ is the standard hermitian inner product on $\mathbb{C}^n$ . Are these conditions sufficient to guarantee that $H$ is positive definite? This question just popped in my head. Part of me thinks it would be too good to be true, in which case it should be easy to construct a counterexample, numerically. Another part of me thinks that the rational normal curve ""twists"" enough to guarantee that $H$ is positive definite. I will think about it during the day, but I think it is an interesting question.","['hermitian-matrices', 'algebraic-geometry', 'positive-definite']"
4657414,How to show $ \|fg\|_{L^2}\leq \|\partial_{x_1}f\|_{L^2}\|g\|_{L^2}+\|f\|_{L^2}\|\partial_{x_2}g\|_{L^2} $?,"Let $ f,g\in C_c^1(\mathbb{R}^2) $ , show that $$
\|fg\|_{L^2}\leq \|\partial_{x_1}f\|_{L^2}\|g\|_{L^2}+\|f\|_{L^2}\|\partial_{x_2}g\|_{L^2}.
$$ Here is my try. Without loss of generality, we can assume that $ \operatorname{supp}(f,g)\subset\mathbb{R}_+\times\mathbb{R}_+ $ . Then I want to use the fundamental theorem of Calculus to represent $ fg $ by the $ \partial_{x_1}fg $ and $ f\partial_{x_2}g $ . However I have some difficulty in this procedure. Can you give me some references or hints?","['calculus', 'sobolev-spaces', 'analysis', 'real-analysis']"
4657448,Why isn't the derivative of the volume of the cone its surface area?,"The derivative of the volume of a sphere, with respect to its radius, gives its surface area. I understand that this is because given an infinitesimal increase in radius, the change in volume will only occur at the surface. Similarly, the derivative of a circle's area with respect to its radius gives its circumference for similar reasons. A similar logic can be applied to other simple geometric shapes and the pattern holds.
For a cylinder, it becomes more interesting. Its volume is $\pi r^2h$ , with the height $h$ and radius $r$ being independent. Incremental increases in  a cylinders height will ""add a circle"" to the bottom of the cylinder, so it makes sense to me that the derivative of volume with respect to height is the are of that circle $\pi r^2$ . Incremental changes to its radius, will add a ""tube"" around the cylinder, whose area is $2\pi rh$ , the derivative of volume with respect to radius. I thought I had found a nice heuristic reason for why the derivatives of volumes give surface areas, but it breaks down for the cone. The volume is $\frac{\pi r^2 h}{3}$ . Assuming the angle $\theta$ of the cone remains constant, $h$ and $r$ are dependent ( $r=h\tan\theta$ ). Now, the derivative of the cone's volume with respect to height gives the area of the base of the cone, this intuitively makes sense, but the derivative with respect to radius doesn't seem to match any geometric quantity. $V=\dfrac{\pi r^3}{3\tan\theta}~$ and $~\dfrac{\text{d}V}{\text{d}r}=\dfrac{\pi r^2}{\tan\theta}$ Similarly, I'd hoped to find some way to drive the surface area of the cone without the base $\pi rl$ where $l$ is the distance from the point of the cone to the edge of the circle at its base. However, the derivative with respect to $l$ also doesn't seem to have any meaningful geometric significance. What's going on here? Is it an issue of not defining the shape rigorously enough? Do I need to parametrise the volume rigorously in some way?","['calculus', 'area', 'geometry', 'volume']"
4657456,How to find ${\rm rank}(2I_n-A)$ where A is a square matrix of size $n$ and $A^3 - 6A^2 + 12A = 0_n$?,"I think the rank has to be $n$ since anything else would be impossible to prove with so little information about the matrix. $$\det(2I_n-A) = -P_A(2) = -\det(A - 2I_n) \ .$$ So, if I can show the characteristic polynomial is non-zero at $2$ , it would prove the rank is $n$ . I have no idea how to do that though. Maybe the characteristic polynomial can be deduced from $$A^3 - 6A^2 + 12A = 0$$ somehow? Not sure how, since the Hamilton-Cayley theorem doesn't work in the opposite direction. If I factorize the polynomial I get $$(A^2 - 6A + 12I_n)A = 0_n \ .$$ So, at least one of the factors has to have the determinant $0$ , but I want to prove that $A$ 's determinant isn't $0$ , so I'm not sure if factorizing helps here.","['matrix-rank', 'matrices', 'linear-algebra', 'characteristic-polynomial', 'matrix-equations']"
4657510,Can a one-variable function generate a two-variable function with a certain property?,"Suppose a function $f$ of two variables is obeying $$
f(x,\,y) = f(xy,\,1)\;.\qquad\qquad\;\qquad (1)
$$ Using $f$ , we can define a function $\varphi(z)$ of one variable, $$
\varphi(z) \equiv f(z,\,1)\;.\qquad\qquad\qquad\qquad (2)
$$ Stated alternativaly, $$
\mbox{for}\;\forall\;f(x,\,y)\;\mbox{obeying (1)}\,,\;\;\exists\;\varphi(z)\;:\;\;\varphi(xy)=f(x,y)\;.
$$ Now, the inverse problem: from an arbitrary $\varphi(z)$ , can we build an $f(x,y)$ satisfying (1) and linked to $\varphi$ via (2)?","['multivariable-calculus', 'functions']"
4657541,Binomial Coefficient through multiplication only,"Is there any method to obtain $n \choose k$ , for all $n, k \in \mathbb{N}$ , using only products of natural numbers without using recursion on binomial coeffcients? A method that allows one to compute the following binomial coefficients through these operations, for instance: $$ {9 \choose 6} = 2 \cdot 2 \cdot 3 \cdot 7 = 84$$ $$ {10 \choose 4} = 2 \cdot 3 \cdot 5 \cdot 7 = 210$$","['binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4657548,Testing hypotheses about parameter function $\lambda^T\beta$,"I have a hypothetical situation, where the parameter function $\lambda^T\beta=\alpha_1-\alpha_2$ for the model $$y_{ij}=\mu+\alpha_i+\epsilon_{ij}, i=1,2, j=1,\cdots n_i$$ is estimated on two independent samples. So $\beta^T=(\mu,\alpha_1,\alpha_2)$ and $\lambda^T=(0,1,-1).$ The first sample has $n_1=2$ and $n_2=5$ , the second sample has $n_1=10$ and $n_2 = 20$ . From the first sample we know the estimate $\lambda^T\hat{\beta}$ and from the second sample we have the mean squared error $\text{(MSE}_2\text{)}$ . If I would like to test the hypothesis $H_0: \lambda^T\beta = 0,$ I would use the test statistic $$t := \frac{\lambda^T\hat{\beta}}{\sqrt{\text{MSE}_2\cdot \lambda^T(X^TX)^{-}\lambda}}.$$ For calculating $t$ , am I right to use the design matrix from the first sample, from where we have the estimate for the parameter function? I assume this because for deriving the formula of the statistic we need to use the fact that $$\frac{\lambda^T\hat{\beta}}{\sqrt{\lambda^T(X^TX)^{-}\lambda\sigma^2}}\sim N(0,1).$$ Am I right to assume this? A second topic of interest for me is how would one show that under $H_0$ the statistic $t$ indeed has t-distribution?","['statistics', 'parameter-estimation', 'hypothesis-testing']"
4657555,How to find out all the points where a function is not differentiable?,"I came across a question wherein I was asked to find out where all the function $\left|x^2 - 5x+ 6\right|$ is differentiable. I plotted the graph of the function and I was able to find out that the above function isn't differentiable at $2$ and $3$ . Now without plotting the graph of the same, how can I find out the points where the above function isn't differentiable (or any other function).","['calculus', 'derivatives']"
4657570,"The statistical average of a continuous value: $\overline{O} = \int O(x) \rho(x) dx$, but coordinate invariant","I am trying to solve a Lagrange multiplier problem for the following equation $$
L= - \int_{-\infty}^\infty \rho(x) \ln \frac{\rho(x)}{q(x)} dx + \alpha \left( 1- \int_{-\infty}^\infty \rho(x) dx \right) +\beta \left( \overline{O} - \int_{-\infty}^\infty  O(x) \rho(x)  dx \right) \tag{1}
$$ for $\frac{\partial L}{\partial \rho(w)} =0$ . Where the first term is the relative entropy, the second term is the constraint that it sums to one, and the last term is the constraint of an observable involving its average value. The problem solves for the probability measure that maximizes the relative entropy of a continuous parametrization x. Eventually, I wish to apply the probability measure to a curved manifold. This is why coordinate invariance is very interesting to me. An interesting property of the relative entropy is that its equations remains invariant with respect to a change in variable. Indeed, $$
\int_{-\infty}^\infty \rho(x)\ln \frac{\rho(x)}{q(x)} dx \to  \int_{-\infty}^\infty \rho(y(x)) \left|\frac{\partial y}{\partial x}\right|\ln \frac{\rho(y(x))\left|\frac{\partial y}{\partial x}\right|}{q(y(x)) \left|\frac{\partial y}{\partial x}\right|} dx  = \int_{-\infty}^\infty \rho(y)\ln \frac{\rho(y)}{q(y)} dy \tag{2}
$$ and $$
\int_{-\infty}^\infty \rho(x)dx \to \int_{-\infty}^\infty \rho(y(x)) \left|\frac{\partial y}{\partial x}\right| dx = \int_{-\infty}^\infty \rho(y) dy \tag{3}
$$ However, the last term isn't. Indeed: $$
\int_{-\infty}^\infty O(x) \rho (x)  dx \to \int_{-\infty}^\infty O(y(x)) \left|\frac{\partial y}{\partial x}\right|  \rho (y(x))  \left|\frac{\partial y}{\partial x}\right| dx =  \int_{-\infty}^\infty   O(y) \rho (y(x)) \left|\frac{\partial y}{\partial x}\right| dy  \tag{4}
$$ How can I modify the integral that contains O so that it is coordinate invariant? What is the expression for the average of a function O(x) over a continuous parametrization x, such that the average value is coordinate invariant? edit: It appears to me that the solution is to consider the observable to be the ratio between two quantities: $$
\overline{O} = \int_{-\infty}^\infty \frac{A(x)}{B(x)} \rho(x) dx
$$ where $O(x) := A(x)/B(x)$ . In this case, it is invariant for the same reason that equation (2) is: $$
\int_{-\infty}^\infty \frac{A(x)}{B(x)} \rho(x) dx \to \int_{-\infty}^\infty \frac{A(y(x))  \left| \frac{dy}{dx} \right| }{B(y(x))  \left| \frac{dy}{dx} \right|} \rho(y(x)) \left| \frac{dy}{dx} \right| dx = \int_{-\infty}^\infty \frac{A(y) }{B(y) } \rho(y)  dy
$$ Can anyone weight in?","['entropy', 'statistics', 'lagrange-multiplier', 'ordinary-differential-equations']"
4657576,Why are points and vectors illustrated differently?,"In linear algebra a vector space $(V,+,\cdot)$ is defined by the usual axioms and an element of the set $V$ is called a vector. Of particular interest in school is the three dimensional real vector space $(\mathbb{R}^3,+,\cdot)$ . This space is usually visualized by a three dimensional space and its elements, the vectors, are usually visualized by arrows pointing from the origin $0$ to its coordinates. Using the definition above, a vector, that is, an element of $\mathbb{R}^3$ is given by a $3$ -tuple $(x,y,z)$ where $x,y,z \in \mathbb{R}$ . However, a point is usually defined to be an element of $\mathbb{R}^3$ as well. This means that the usual definition of ""point in $(\mathbb{R}^3,+,\cdot)$ "" coincides with the definition of a vector in $\mathbb{R}^3$ . The usual illustrations of this vector space usually distinguish between points, drawn as points, and vectors, drawn as arrows as described above. Question: $(1)$ Why is this distinction in the illustrations justified using the abstract definition provided above? I know that technically speaking $\mathbb{R}^3$ and $(\mathbb{R}^3,+,\cdot)$ are different objects, but elements of their respective sets are the same objects. So if one were to define points to be elements of $\mathbb{R}^3$ they would still coincide with vectors as elements of $(\mathbb{R}^3,+,\cdot)$ . So again, I am unsure why a distinction in terminology and in illustration is justified. Question: $(2)$ So using this definition are points and vectors of $(\mathbb{R}^3,+,\cdot)$ the same thing formally? Why is there a special notation for vectors if that is the case? The operations $+$ and $\cdot$ are functions and so in particular defined for elements of $\mathbb{R}^3$ . The distinction between points and vectors is common however. As an example one can regard the problem of calculating the distance between a ""point"" and a $2$ -dimensional subspace of $(\mathbb{R}^3,+,\cdot)$ . If one wanted to make a distinction between the two concepts mathematically, one could perhaps define vectors to be triples $(v,+,\cdot)$ where $v$ is an element of $V$ and $+,\cdot$ are the operations on $V$ . This would certainly give mathematically different concepts of points and vectors, but this is artificial to me. This would however lead to the question why vectors and points can be draw in the same space, namely the three dimensional space, since they belong to different sets.","['linear-algebra', 'geometry']"
4657665,Checking the stability of equilibrium point,"Is equilibrium point $x = 0$ stable for $\ddot{x}+\dot{x}=-x\sin x$ ? It is a case of a bigger problem, where we need to check the stability of critical points $x \in [-\frac \pi 2, \frac \pi 2]$ for $\ddot{x}+\dot{x} = x(a-1)(\sin x-a)$ . I've done with all other situations, but can not do anything with this case. Obviously, we need to say $\dot{x}=y$ , $\dot{y} =-y-x\sin x$ . Linear approximation doesn't help here. So, it seems we need to find Lyapunov function $V$ in order to use Lyapunov theorem or Chetaev theorem. I've tried some, but I didn't manage to find any. I think that we shoud in some way modify $V = \frac{y^2}2 - x\cos x + \sin x$ , because $\dot{V} = -y^2$ .","['differential', 'nonlinear-system', 'derivatives', 'nonlinear-dynamics', 'dynamical-systems']"
4657670,Better understanding of integration of differential forms.,"On page $100$ of Spivak's Calculus on Manifolds the following definition is made: If $\omega$ is a $k$ -form on $\mathbb{R}^k$ , then $\omega = f\ dx_1\land\ldots\land dx_k$ for a unique function $f:\mathbb{R}^k\to\mathbb{R}$ . We define $$\int_{[0,1]^k}\omega := \int_{[0,1]^k}f.$$ I wish to get a better grasp of the above definition. From my understanding of this post one may interpret $\omega$ as a function that, at each point $p\in\mathbb{R}^k$ , takes in $k$ vectors $v^1,\ldots,v^k$ representing a $k$ -dimensional parallelotope $P$ and spits out a number proportional to its hypervolume. Such number being $$ f(p) \ A(v^1,\ldots,v^k)$$ for a point $p\in P$ , a function $f:\mathbb{R}^k\to\mathbb{R}$ , and the alternating $k$ -tensor $A:{(\mathbb{R}^k)}^k\to\mathbb{R}$ defined by $$A = x_1\land\ldots\land x_k = \text{Alt}(x_1\otimes\ldots\otimes x_k) = \sum_{\sigma\in\mathbb{S}_k}\text{sgn}(\sigma) \prod_{j=1}^k v^{\sigma(j)}_j$$ although the explicit computation of $A$ seems secondary to the fact it is multilinear and alternating. How should one interpret the numbers $f(p)$ and $A(v_1,\ldots,v_k)$ at the moment of computing the integral? It would also probably help if someone could provide an example where concrete values are given to the numbers above.","['integration', 'real-analysis', 'multivariable-calculus', 'definition', 'differential-forms']"
4657709,Solving $ y'' + a y = f(x) $ with zero initial conditions,"Given the following initial value problem (IVP) $$ y'' + a y = f(x), \qquad  y(0)= y'(0) = 0$$ show that $$y = \frac1a \int_{0}^{x}f(t)\sin(ax-at)\ {\rm d}t$$ My attempt To solve the given initial value problem $$y''+ay=f(x)$$ we first solve the homogeneous equation $$y''+ay=0$$ The characteristic equation is $r^2+a=0$ , which has roots $r=\pm\sqrt{-a}$ . Depending on the sign of $a$ , these roots can be written as complex numbers or real numbers. If $a<0$ , we can write the roots as $r=\pm i\sqrt{a}$ . In this case, the general solution to the homogeneous equation is $$y_h(x) = c_1\cdot \cos\sqrt{-a}x + c_2\sin\sqrt{-a}x$$ If $a>0$ , we can write the roots as $r=±\sqrt(a)$ . In this case, the general solution to the homogeneous equation is $$y_h(x) = c_1\cdot e^{-\sqrt{a}x)} + c_2e^{\sqrt{a}x}$$ Proceeding as such only the former case yields the correct result. What about the latter?","['initial-value-problems', 'calculus', 'convolution', 'ordinary-differential-equations']"
4657710,Do functions over a ring have an odd and an even part?,"All functions $f(x):\mathbb{R}\to\mathbb{R}$ can be decomposed into an even and an odd part $f(x)=E(x)+O(x)$ . The proof I see here , and on Wikipedia requires $2$ to have an inverse, however I want to know if this property is true of functions in a ring. For a ring $R$ for every function $f(x):R\to R$ , can I find even and odd functions $E(x)$ and $O(x)$ such that $f(x)=E(x)+O(x)$ ? Using a similar method to the ordinary proof I can find that $2E(x)$ and $2O(x)$ are defined uniquely by $f$ , but this only shows that $2f$ has a decomposition into an even and an odd part.
Because $2$ does not necessarily have an inverse, I'm not certain how to solve this. I think that a decomposition seems possible, but it would not necessarily be unique?","['even-and-odd-functions', 'ring-theory', 'abstract-algebra']"
4657728,"Show that $\forall (a, b, c) \ \in \mathbf{R}: \frac{a^2}{b^2} + \frac{b^2}{c^2} + \frac{c^2}{a^2} \ge \frac{b}{a} + \frac{c}{b} + \frac{a}{c}$","Show that \begin{equation}
\forall (a, b, c) \ \in \mathbf{R}: \frac{a^2}{b^2} + \frac{b^2}{c^2} + \frac{c^2}{a^2} \ge \frac{b}{a} + \frac{c}{b} + \frac{a}{c}
\end{equation} My approach was as follows: multiplying both sides by $a^2b^2c^2$ , we get \begin{equation}
a^4c^2 + b^4a^2 + c^4b^2 \ge b^3c^2a + c^3a^2b + a^3b^2c
\end{equation} Now, introduce the following notation: \begin{equation}
\begin{bmatrix}
 a_1 & a_2 & \dots & a_n \\
 b_1 & b_2 & \dots & b_n
\end{bmatrix} = a_1b_1 + a_2b_2 + \dots + a_nb_n.
\end{equation} Thus, we can write the above inequality as: \begin{gather*}
a^4c^2 + b^4a^2 + c^4b^2 \ge b^3c^2a + c^3a^2b + a^3b^2c  \\
\iff a^3c^2a + b^3a^2b + c^3b^2c \ge b^3c^2a + c^3a^2b + a^3b^2c \\
\iff 
\begin{bmatrix}
a^3 & b^3  & c^3 \\
c^2a & a^2b & b^2c
\end{bmatrix} \ge
\begin{bmatrix}
a^3 & b^3 & c^3 \\
b^2c & c^2a & a^2b
\end{bmatrix}
\end{gather*} Now, assume that $a \le b\le c$ (note that the inequality is not symmetric, but the same argument as is about to be made could be made assuming any other order of the variables as well). If we look at the $RHS$ of the above inequality, we see that we have the two arrays $A = (a^3, b^3, c^3), B = (b^2c, c^2a, a^2b)$ . If $a \le b \le c$ , we have that $a^3 \le b^3 \le c^3$ , which is equivalent to $A$ being strictly increasing. Moreover, we see that the last element of $B$ ( $a^2b$ ) is the smallest element in $B$ (this is trivial to show). Now, according to the rearrangement inequality, the smallest possible sum (for $n=2$ ) is achieved when $A$ is increasing and $B$ is decreasing, which means that the sum in $RHS$ is either the smallest possible sum or the second smallest, depending on if $b^2c \ge c^2a$ or not. But the problem is that this very same argument about smallest or second smallest sum can be made about the $LHS$ as well... Does anyone have any ideas on how to proceed?","['contest-math', 'rearrangement-inequality', 'algebra-precalculus', 'inequality']"
4657739,Roots and analytic continuation of $\zeta(s)=\sum_{n>0} \frac{H_n^{-s}}{n} $?,Let $$\zeta(s)=\sum_{n>0} \frac{H_n^{-s}}{n} $$ Where $H_n$ are the harmonic numbers. This is well defined for $\Re (s)>1$ . But what about analytic continuation? And where is $$\zeta(s) = 0$$ ?? Is $\Re(s) = 1$ a natural boundary ? And can we continue the function beyond $\Re(s) > 1$ ? Plots are appreciated too :),"['complex-analysis', 'zeta-functions', 'roots', 'analytic-continuation']"
4657778,Existence of sequence of test functions converging to a constant,"I read somewhere that it is possible to find a sequence of test functions $\phi_{\epsilon} \in C^{\infty}_{c}(0,1)$ such that $\phi_{\epsilon} \ge 0$ , $\phi_{\epsilon} \to 1$ as $\epsilon \to 0$ , $\|\phi_{\epsilon}'\|_{L^{2}(0,1)} \lesssim  1 $ . I wish to know if this claim is actually true or not.
I am doubting this mainly because of the final statement which says that the derivatives are uniformly bounded. The reason I doubt this is because if this sequence is converging to $1$ and each element has compact support then surely the derivatives would have to be 'really large' near the endpoints $x=0$ and $x=1$ ? Intuitively I don't see how you could cook up a sequence of functions which have compact support, converge to $1$ AND have derivative uniformly bounded in $L^{2}$ .","['distribution-theory', 'functional-analysis', 'analysis', 'real-analysis']"
4657787,Exercise on multivariable calculus: am I reasoning right?,"Consider $$f(x, y) = \begin{cases} x^2+y^2 & x^2+y^2 \geq 1 \\\\ \frac{1}{x^2+y^2} & 0 < x^2+y^2 < 1\end{cases}
$$ Now, the domain of $f$ is: $D: \mathbb{R}^2\backslash\{(0, 0)\}$ When it's asked to check for the continuity of the function in all the points of its domain, I guess that it's really asked to only check for what happens when $x^2+y^2 = 1$ . Question: could I turn this into a one dimensional problem where $x^2+y^2 = m$ and hence studying the continuity of $f(m)$ which turns to be trivial? At least using directional limits or sequences... I don't have any idea on how it would be to prove continuity in this case, as if I had to use the same method of $2$ variables function in the usual way (that is, with majorisation and distance). Is it doable?","['continuity', 'multivariable-calculus', 'real-analysis']"
4657859,"Inequality $|a|\langle x+a\rangle^{-1} \leq \langle x \rangle$ for all $x,a \in \mathbb R^d$ with strange tangency behavior","While working on a problem in harmonic analysis, attempting to apply DCT, we stumbled across the following interesting inequality: $|a|\langle x+a\rangle^{-1} \leq \langle x \rangle$ for all $x,a \in \mathbb R^d$ , where $\langle x\rangle$ is the Japanese bracket of $x$ , defined to be $\sqrt{1+|x|^2}$ . Writing everything in coordinates, this is the inequality $$(a_1^2+\ldots a_n^2) \leq (1+(x_1+a_1)^2 + \ldots + (x_n+a_n)^2)(1+x_1^2+\ldots + x_n^2).$$ Looking at the graphs on Desmos the graphs look tangent! However, for small values of $a$ , they are clearly not tangent, so intuitively one would expect them not to become tangent suddenly for larger values of $a$ . Indeed for $a<2$ , the curves do not touch, and for $a=2$ , they do touch (seemingly tangently) at $x=-1$ . Trying $a=3$ , they do touch (again seemingly tangently) at $x=-1-\varphi$ where $\varphi$ is the golden ratio (I recognized this by seeing on Desmos they meet at $x=-2.618$ , and miraculously it's the actual golden ratio). The other tangent point is $x=\frac{3-\sqrt 5}2$ . This is certainly the most cursed inequality regarding quadratics I have ever come across. It seems like the inf of the difference between these function is a smooth (?) function with compact support! Very strange. Can anyone prove this inequality? The inequality is true for large $x_i$ , so the minimum of the difference of RHS-LHS occurs when the gradient is $0$ , so one could calculate the gradient and get $n$ polynomial equations and try to find the zeroes, but this does not seem to be a workable solution.","['harmonic-analysis', 'multivariable-calculus', 'algebra-precalculus', 'inequality']"
4657896,Where the tensor used in the definition of the octonion product come from?,"The octonion multiplication table is hard to remember. It's also not uniquely defined, but I'm assuming the definition that Wikipedia chose is fairly standard. One of the presentations uses an interesting tensor $\varepsilon_{ijk}$ . Where does this tensor come from? I'm trying to understand this definition : $e_0e_j = e_j$ $e_ie_0 = e_i$ $e_ie_j = -\delta_{ij}e_0 + \varepsilon_{ijk}e_k$ when $0 \not\in \{i, j\}$ According to the article: $\delta_{ij}$ is the Kronecker delta (equal to 1 if and only if $i = j$ ). and $\varepsilon_{ijk}$ is a completely antisymmetric tensor with value 1 when $ijk = 123, 145, 176, 246, 257, 347, 365$ The Kronecker delta makes sense; it's like an equality indicator function. The $\varepsilon_{ijk}$ makes some sense if I we look at the article on completely antisymmetric tensors . Apparently, $\varepsilon_{ijk}$ takes the value $1$ at its base cases, and permuting any of its arguments changes the sign of the result by the sign of the permutation. Also, $\varepsilon$ takes the value $0$ if any of its indices are repeated or are not among the triples listed above. For example $\varepsilon_{135}$ is zero. $\varepsilon_{ijk}e_k$ also appears to be making use of the Einstein summation convention . As a quick check $-1 = e_4e_4 = -\delta_{44} + \varepsilon_{44k}e_k = -1$ , which makes sense. However, for cases when $i$ and $j$ are not equal, it seems like a small miracle that $\varepsilon_{ijk} e_k$ even refers to a standard root of unity, rather than, say, something like $e_1 - e_6$ or something. Where does this tensor come from?","['abstract-algebra', 'octonions']"
4657900,"Exercise 18 on p.39 in Exercises 2B in ""Measure, Integration & Real Analysis"" by Sheldon Axler.","I am reading ""Measure, Integration & Real Analysis"" by Sheldon Axler. The following exercise is Exercise 18 on p.39 in Exercises 2B in this book. Exercise 18 Suppose $f:\mathbb{R}\to\mathbb{R}$ is differentiable at every element of $\mathbb{R}$ . Prove that $f'$ is a Borel measurable function from $\mathbb{R}$ to $\mathbb{R}$ . I was not able to solve Exercise 18. I found the following answer: https://math.stackexchange.com/a/1803668/384082 I cannot understand this answer. Let $A_n:=\{x:\frac{f(x+\frac{1}{n})-f(x)}{\frac{1}{n}}>a\}.$ hmakholm left over Monica wrote the following equality: $\{x:f'(x)>a\}=\bigcup_{k=1}^\infty \bigcap_{n=k}^\infty A_n.$ I cannot prove that $\{x:f'(x)>a\}\supset\bigcup_{k=1}^\infty \bigcap_{n=k}^\infty A_n.$ Let $f(x):=\frac{1}{3}x^3$ . Let $a:=0$ . Then, $0\in A_n$ for any $n\in\{1,2,\dots\}$ . So, $0\in\bigcup_{k=1}^\infty \bigcap_{n=k}^\infty A_n$ . But, $\{x:f'(x)>a\}=\{x:x^2>0\}\not\ni 0$ . So, I guess hmakholm left over Monica's answer needs to be modified. Am I wrong?","['borel-measures', 'measure-theory', 'measurable-functions', 'real-analysis']"
4657906,Finite projective plane with transitive collineation group and non-trivial elation is a Moufang plane,"Let $P$ be a finite projective plane, $\Gamma$ a collineation group and suppose $\Gamma$ is transitive on the points of $P.$ If $P$ contains a non-trivial elation, show that $P$ is a Moufang plane. I solved it, but I'm suspicious about the last step. Here's my solution: Let $\alpha$ be a $(A,l)$ elation, then $|\Gamma_{(l,l)}|>1$ so $l$ is a translation line by the previous problem. Now we use the following result extensively: If $\alpha$ is a collineation and $l$ a translation line, then $\alpha(l)$ is a translation line. Given any $B \not\in l,$ pick $\beta$ such that $\beta(A) = B,$ then $\beta(l)$ passes through $B$ so there's a translation line through every point of $P.$ If there's 3 non-concurrent translation lines, then every line is a translation line by a previous result, so $P$ is a Moufang plane. Otherwise, every translation line concurs at $X \in l$ and every line through $X$ is a translation line. At this point I wasted so much time trying to generate new lines from the existing ones and couldn't ever derive a contradiction. After 1 hour I had the idea to take a line not containing $X$ and turn it into one that passes through $X.$ Pick $Y, Z$ such that $X, Y, Z$ are not collinear and $\beta$ such that $\beta(Y) = X,$ then $\beta(YZ) = X\beta(Z)$ is a translation line, so $\beta^{-1}(\beta(YZ)) = YZ$ is as well. It seems suspicious that no matter what you do, it's impossible to create any more lines once you have them concur at a point, and the only way to continue is to take a new line, make it concur, and undo the automorphism. Is there another proof or is this essentially the only way?","['projective-geometry', 'geometry']"
4657945,Expected number of minutes for two ants to fall off rope,"This question is based off of this post Let's say we have a $1$ foot rope and we randomly and uniformly place $2$ ants on it. They each move $1$ ft per second towards a random end and switch direction upon collision. What would the expected length of time be for the last ant to fall off the rope. Suppose on average the ants will be placed at $\frac13$ and $\frac23$ feet on this rope (since ants are IID). It is with probability $0.5$ that they will go in the same direction in which case it will take $\frac23$ of a minute for the last ant to fall. If the ants travel towards each other (probability $\frac14$ ) it will take $\frac23$ minutes for them to fall. If the ants move away from each other, it will take $\frac13$ for them to fall. This gives us $\frac34\cdot\frac23 + \frac14\cdot\frac13 = \frac7{12}$ However, based off the solution I believe the correct answer is $\frac23$ . Why is this approach invalid?","['expected-value', 'independence', 'uniform-distribution', 'probability']"
4657950,Integral of function with two linearily scaled variables,"I have a known function $w = w(x,y)$ and I want to find $N(a,b)$ where $a$ and $b$ are variables independent of $x$ . \begin{equation}
N(a,b) = \int_{-\infty}^{\infty} w(ax,bx) dx
\end{equation} Assuming the integral converges, is there a way to simplify this integral? For example, in the 1D case I can use substitution to prove that: \begin{equation}
\int_{-\infty}^{\infty} f(ax) dx = \frac{1}{a}\int_{-\infty}^{\infty} f(x) dx
\end{equation} Is there a similar type of simplification I can do for the 2D case? Ideally, I'd like to suck the $a$ and $b$ outside the integral.","['multivariable-calculus', 'calculus']"
4658060,Uniqueness of trigonometric functions from functional equation,"Take the functional equation $$f(x+y)=f(x)f(y).$$ This is a variation on the Cauchy equation, and we know that it has a unique continuous solution over the real numbers $$f(x)=e^{\alpha x}.$$ We then look at the pair of equations $$u(x+y)=u(x)u(y)-v(x)v(y),$$ $$v(x+y)=v(x)u(y)+u(x)v(y).$$ Moving over to complex numbers and setting $f(z)=u(z)+iv(z)$ , this becomes the exponential equation above. Fixing that $u,v$ are real for real arguments fixes the trigonometric functions, $$u(x)=e^{\alpha x}\cos\beta x,\quad v(x)=e^{\alpha x}\sin\beta x$$ as defined via complex exponentials. Further specifying a bounded image gives us what we need. So the trig functions must be the only continuous bounded solutions to the pair of equations above. Specifying that it's just bounded is probably enough, actually, given the Cauchy-ness involved. Can this uniqueness be established by working solely over the reals? Preferably just by elementary methods. It doesn't need to be established that $\cos x=\Re e^{ix}$ , for example. EDIT: A possible direction assuming continuity could be the following. As pointed out by an answer below, we can focus only on solutions for which $u^2+v^2=1$ . With that in mind, what would be interesting to show is that given two nontrivial continuous solutions $(u_1,v_1)$ and $(u_2,v_2)$ , we must have $$u_2(x)=u_1(\alpha x),$$ $$v_2(x)=v_1(\alpha x),$$ where $$\alpha=\lim_{x\rightarrow0}\frac{v_2(x)}{v_1(x)},$$ provided of course that we can show this limit exists.","['complex-analysis', 'functional-equations', 'real-analysis']"
4658075,"Proving $ d_W(X,Y) \leq \vert \lambda_1-\lambda_2\vert $ for the Wasserstein metric","Suppose, that $X \sim \text{Pois}(\lambda_1)$ and $Y \sim \text{Pois}(\lambda_2)$ , and consider the Wasserstein metric $$
d_W(X,Y) = \sup_{h \in \text{Lip}(1)} \vert E[h(X)]-E[h(Y)] \vert,
$$ where $\text{Lip}(1)$ denotes the real functions which are Lipschitz with constant atmost 1. I want to prove the following upper bound: $$
d_W(X,Y) \leq \vert \lambda_1-\lambda_2\vert
$$ I am not sure whether or not this holds for a more general class of distribution (if we replace the $\lambda$ 's with expectations), and by extension whether or not we need to use properties of the Poisson distribution. As a generic attempt, the taking an arbitrary $h \in \text{Lip}(1)$ , we get $$
\vert E[h(X)]-E[h(Y)] \vert \leq E[\vert h(X) - h(Y)\vert ] \leq E[\vert X - Y \vert ], 
$$ but this is not useful. We would need to not move the absolute value inside the integral, but then I am coming up blank for ways to do this estimate. Also I tried googling for any nice results for the Lipschitz functions and the Poisson distribution but could not find anything. I guess we could try to use that the Poisson distribution is discrete, but I don't think sums are preferable to $E$ here.","['probability-theory', 'poisson-distribution', 'upper-lower-bounds']"
4658130,Category theory textbook for Banach algebras and Banach spaces,"Do you know of a textbook at the graduate level about the common categories of functional analysis (such as the 2 categories of Banach spaces, the 2 categories of Banach algebras, the category of Banach modules, the category of C $^*$ -algebras, the category of W $^*$ -algebras) which is suitable for a student who has taken a basic course in category theory? The ideal book covers the basic themes such as injective, projective, flat objects; limits & colimits; tensor products; injective envelopes; etc. of each category in the category theoretical context, with their context-specific equivalent definitions. It would be instructive if the book contains examples for the (existence and) non-existence results. I can't blame if you think that I have way too many criteria for the ideal book, so I can't match up with a great book that would fill up the gap in my life and my bookshelf.","['operator-algebras', 'book-recommendation', 'category-theory', 'reference-request', 'functional-analysis']"
4658187,Is there any way to quickly check if a partial order is transitive?,Is there any fail safe and quick way to check if a partial order $\preceq$ on a set $A$ is transitive without checking through trial and error?,"['graph-theory', 'order-theory', 'set-theory', 'discrete-mathematics']"
4658198,Solutions of $dx/dt = \lambda x$ with initial condition,"I'm working through Robinson's Introduction to Ordinary Differential Equations (Section 8.2) and have a few questions. The section is on finding the solution of the simplest possible linear differential equation, $$\frac{dx}{dt} = \lambda x$$ with the initial condition $$x(t_0) = x_0.$$ Here's the first issue I encountered. If $x_0 = 0$ , the author claims $x(t) = 0$ for all time. Could anyone provide a formal proof of this? I can see that $x(t_0) = x_0 = 0$ leads to $\dot{x}(t_0) = 0$ , but why would this imply $x(t) \equiv 0$ ? I can sort of see why this is visually by drawing a phase diagram like the one in figure 8.1: in the case $x_0 = 0$ , the line has zero slope and goes through 0 , so $x$ must be the constant zero solution. However, I'd really appreciate a more detailed explanation. If $x_0 \neq 0$ , he goes on to integrate the equation: $$\int_{x = x_0}^{x(t)} \frac{1}{x}dx = \int_{t=t_0}^t \tau d\tau$$ and arrives at $$ \frac{|x|}{|x_0| } = e^{\lambda(t-t_0)} \tag{$*$}$$ Next he writes: To work out what to do about the modulus signs, the easiest thing is
to draw the phase diagram. For the case $\lambda > 0$ this is shown in Figure
8.1, from which we can see that $x(t)$ and $x_0$ have the same sign. It follows that we can remove the modulus signs and multiply up to give $$x = x_0 e^{\lambda(t-t_0)}$$ The figure is reproduced below: The way I understand this is: if $\lambda >0$ , then from the differential equation, $x$ and $\dot{x}$ always have the same sign. So if $x(t_0) = x_0 > 0$ , then $\forall t$ , $x(t) > 0 $ .
However, if $x(t_0) = x_0 < 0$ , then $\dot{x}<0$ and hence we will have $x(t) < 0 $ for all $t$ . So $x(t)$ and $x_0$ have the same sign and we can simply remove the modulus signs. The case $\lambda < 0$ was left to the reader. Here is my reasoning: If $x_0 < 0$ , then $\dot{x}(t_0) > 0$ , so $x(t)$ is increasing (at least until $x(t) = 0 \iff \dot{x}(t) = 0 $ ). But from ( $*$ ), $x=0 \iff e^{\lambda(t-t_0)} = 0$ , which is not satisfied by any $t\in \mathbb{R}$ . So $x$ never reaches $0$ . Hence the solution must start negative, be increasing and tend to $0$ : $x = -|x_0| e^{\lambda(t-t_0)} = x_0 e^{\lambda(t-t_0)}$ If $x_0 > 0$ , then $\dot{x}(t_0) < 0$ , so $x(t)$ is decreasing (at least until $x(t) = 0 \iff \dot{x}(t) = 0 $ ). But once again we can't have $x=0$ for any $t$ , so the solutions starts positive, is decreasing and tends to $0$ : $x =  x_0 e^{\lambda(t-t_0)}$ So in every single case ( $\lambda > 0$ or $\lambda <0$ ) we obtain $$x = x_0 e^{\lambda(t-t_0)}$$ Is my reasoning correct? Is there any simpler way to arrive to the same conclusions?",['ordinary-differential-equations']
4658216,Parametrization of integer solutions of the equation $a^2+b^2=c^2+d^2=2x^2$,"I need the general form of integer solutions to this equation $$a^2+b^2=c^2+d^2=2x^2$$ Here is my partial solution:- The parametrization of the integer solutions of the equation $$p^2+q^2=2y^2$$ is the following:- $$p=k(m^2-n^2+2mn)$$ $$q=k(m^2-n^2-2mn)$$ $$y=k(m^2+n^2)$$ where $k,m,n\in\mathbb Z$ . The proof can be seen here . If we have two triplets of that satisfy the equations $p^2+q^2=2y^2$ and $r^2+s^2=2z^2$ , we can multiply $p$ , $q$ and $y$ with $z$ to get $$(pz)^2+(qz)^2=2(yz)^2$$ Similarly we can multiply $c$ , $d$ and $z$ with $y$ to get $$(ry)^2+(sy)^2=2(yz)^2$$ Combining the two equations, we get $$(pz)^2+(qz)^2=(ry)^2+(sy)^2=2(yz)^2$$ If we set it like the following:- $$a=pz$$ $$b=qz$$ $$c=ry$$ $$d=sy$$ $$x=yz$$ we get integer solutions to $a^2+b^2=c^2+d^2=2x^2$ We can replace $p$ with $k(m^2-n^2+2mn)$ , $q$ with $k(m^2-n^2-2mn)$ and $y$ with $k(m^2+n^2)$ . Similarly, we can replace $r$ with $l(u^2-v^2+2uv)$ , $s$ with $l(u^2-v^2-2uv)$ and $z$ with $l(u^2+v^2)$ and adding a scale factor $t$ to get $$a=k(m^2-n^2+2mn)l(u^2+v^2)t$$ $$b=k(m^2-n^2-2mn)l(u^2+v^2)t$$ $$c=k(u^2-v^2+2uv)l(m^2+n^2)t$$ $$d=k(u^2-v^2-2uv)l(m^2+n^2)t$$ $$x=k(u^2+v^2)l(m^2+n^2)t$$ This is a nice set of integer solutions to the equation $a^2+b^2=c^2+d^2=2x^2$ . But I don't know if every integer solution will be in the form given above. If not, what is the parametrization of integer solutions to the equation that doesn't miss any integer solution? I also want the proof too, just to convince myself that the answer is true. Edit :I have found from the comments that my set of equations misses a lot of solutions. So can somebody come up with a parametrization for this? (Where it is guaranteed that $a,b,c,d,x\in\mathbb Z$ ) Update :I realized that my set of equations would miss some solutions where $k(m^2+n^2)$ and $l(u^2+v^2)$ are not relatively prime. So, instead of multiplying $p$ , $q$ and $y$ with $z$ to get $(pz)^2+(qz)^2=2(yz)^2$ and multiplying $c$ , $d$ and $z$ with $y$ to get $(ry)^2+(sy)^2=2(yz)^2$ , we should multiply $p$ , $q$ and $y$ with $\frac{LCM(y, z)}{y}$ and multiply $c$ , $d$ and $z$ with $\frac{LCM(y, z)}{z}$ to get $$\left(\frac{pLCM(y, z)}{y}\right)^2+\left(\frac{qLCM(y, z)}{y}\right)^2=2(LCM(y, z))^2$$ and $$\left(\frac{rLCM(y, z)}{z}\right)^2+\left(\frac{sLCM(y, z)}{z}\right)^2=2(LCM(y, z))^2$$ Putting everything in terms of $m, n, u$ and $v$ , we get $$a=tk(m^2-n^2+2mn)\left(\frac{LCM(k(m^2+n^2),l(u^2+v^2))}{k(m^2+n^2)}\right)$$ $$b=tk(m^2-n^2-2mn)\left(\frac{LCM(k(m^2+n^2),l(u^2+v^2))}{k(m^2+n^2)}\right)$$ $$c=tl(u^2-v^2+2uv)\left(\frac{LCM(k(m^2+n^2),l(u^2+v^2))}{l(u^2+v^2)}\right)$$ $$d=tl(u^2-v^2-2uv)\left(\frac{LCM(k(m^2+n^2),l(u^2+v^2))}{l(u^2+v^2)}\right)$$ $$x=t(LCM(k(m^2+n^2),l(u^2+v^2)))$$ Now, these set of equations include solutions like $(|a|,|b|,|c|,|d|,|x|)=(5,35,17,31,25)$ and $(|a|,|b|,|c|,|d|,|x|)=(1,7,5,5,5)$ which were missing with my earlier set of equations. But, I am still not sure if this set of equations misses any solutions. So, if this set of equation misses any solutions, I need the true parametrization of the integer solutions of the equation that doesn't miss any solution and its proof. Or if this is an actual parametrization of the equation that includes every integer solution, I need the proof for that.","['sums-of-squares', 'elementary-number-theory', 'diophantine-equations', 'algebra-precalculus', 'parametrization']"
4658294,Why is the graph of $\frac{1}{x}$ a curve? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question I don't understand why the graph of $y=\frac{1}{x}$ is a curve. Consider if $x$ doubles. The subsequent value of $y$ would half. When $x$ were to triple, the value of $y$ would divide by $3$ . It seems to me that the relationship is linear. However, when you plot this it is most obviously a curve. What's going on?","['curves', 'algebra-precalculus', 'functions']"
4658382,Integrate $\int \sqrt{(x+a)(x+b)} \space dx$,"Integrate $\int \sqrt{(x+a)(x+b)} \space dx$ I've tried to use first Euler substitution: $$\sqrt{x^2 + (a+b)x + ab} = x + t \implies x = \frac{t^2 - ab}{a + b - 2t}$$ $$dx = \frac{-2t^2 + 2(a+b)t - 2ab}{(a + b - 2t)^2}dt$$ Then: $$(x + a) = \frac{t^2 - 2at + a^2}{a + b - 2t} = \frac{(t-a)^2}{a + b - 2t}$$ $$(x + b) = \frac{t^2 - 2bt + b^2}{a + b - 2t} = \frac{(t-b)^2}{a + b - 2t}$$ $$\sqrt{(x+a)(x+b)} = \frac{(t-a)(t-b)}{a+b-2t} = \frac{t^2 - (a+b)t + ab}{a + b - 2t}$$ Then we have multiplication by $dx$ : $$\int \frac{t^2 - (a+b)t + ab}{a + b - 2t} \cdot \frac{-2t^2 + 2(a+b)t - 2ab}{(a + b - 2t)^2}dt = -2 \int \frac{(t^2 -(a+b)t + ab)^2}{(a+b - 2t)^3}dt$$ Substitution $ t = \sqrt{(x+a)(x+b)} - x$ doesn't help.
Second Euler substitution $\sqrt{t^2 - (a+b)t + ab} = t + l$ won't help too. Okay, another idea. $$(x+a)(x+b) =\left (x + \frac{a+b}{2}\right)^2 - \left(\frac{a-b}{2}\right)^2$$ Then $$u =  x + \frac{a+b}{2}, dx = du  ; \sqrt{u^2 - (\frac{a -b}{2})^2} = \frac{a-b}{2}\sqrt{\frac{4 u^2}{(a-b)^2} - 1}$$ If we substitute $s = \sec^{-1}  \frac{2u}{a-b}$ : $$\frac{a-b}{2}\sqrt{\frac{4 u^2}{(a-b)^2} - 1} = \frac{a-b}{2}\sqrt{\sec^2 s - 1} = \frac{a-b}{2} \tan s$$ As for the $ds$ : $$\sec s = \frac{2u}{a-b} \implies u = \frac{(a-b)}{2} \sec(s)$$ $$ du = \left(\frac{(a-b)}{2} \sec(s)\right)' du = \frac{(a-b)}{2} \sec u \tan u \cdot ds$$ As a result: $$\int \sqrt{(x+a)(x+b)} dx = \int \frac{(a-b)^2}{4} \tan s \tan u \sec u \cdot ds$$ Something is wrong. I have a formula for finding this integral, but still I would like to ""understand"" this problem. I want to find it step by step. Any ideas for solution?","['integration', 'indefinite-integrals', 'calculus']"
4658471,Find the volume of the solid $Q$ cut from the sphere $x^2 + y^2 + z^2 ≤ 4$ by the cylinder $r = 2 \sin θ$ .,"I set up the integral as $$\int_0^{\pi}\int_0^{2\sin\theta}\int_{-\sqrt{4-r^2}}^{\sqrt{4-r^2}} r dzdrd\theta$$ and got $\frac{16\pi}3$ , but the answer is $\frac{16(3\pi-4)}9  \approx 9.64$ , which is also what my calculator got too. What am I doing wrong? $$\begin{split}
\int_0^{\pi}\int_0^{2\sin\theta}\int_{-\sqrt{4-r^2}}^{\sqrt{4-r^2}} r dzdrd\theta &= 
 \int_0^{\pi}\int_0^{2\sin\theta} r(2\sqrt{4-r^2})drd\theta \\
&= -\frac23\int_0^{\pi} (8\cos^3\theta - 8)d\theta \\
&= \frac{16}3\int_0^\pi (1-\cos^3{\theta})d\theta \\
&= \frac{16\pi}3
\end{split}$$","['integration', 'multivariable-calculus', 'change-of-variable']"
4658496,Prove that regions formed by a planar map vertices even degree can be colored with two colors such that no two neighboring regions have the same color,"Prove that regions formed by a planar map all of whose vertices have
even degree can be colored with two colors such that no two
neighboring regions have the same color. I would like a proof using induction. This problem seems too much abstract to me, I can't think in a way to apply the reduction step, there's so much ways of drawing an edge for this kind of example.","['coloring', 'graph-theory', 'solution-verification', 'discrete-mathematics', 'planar-graphs']"
4658542,Why do we trust the Classification of Finite Simple Groups?,"It seems to me there are a two main reasons to believe a theorem/conjecture to be true: Because it has a correct proof (e.g. the Feit-Thompson Theorem, Dirichlet's Theorem) Because there is an intuitive reason/heuristic for it to be true (e.g. $P \ne NP$ , Dirichlet's Theorem) The former is of course usually the gold standard. However very long proofs may still be regarded as suspicious after a proof has been produced. The longer a proof is, the more likely it is to have undiscovered errors, so that even if no specific gap is known, people don't trust that there isn't a gap. Computer verification of proofs may be used to alleviate these concerns, e.g. the Feit-Thompson theorem was verified with Coq in 2012. But it seems neither condition convincingly applies to CFSG. It has a proof, but this proof is tens of thousands of pages spread across hundreds of articles, and the second generation is still going to be thousands of pages long. Moreover, at least one mistake have previously been discovered in the proof: the gap filled by Harada and Solomon in 2008. I am aware CFSG has likely been submitted to significantly more scrutiny than the average proof, but even so, it seems plausible (from my uninformed perspective) that an error could have slipped through. And CFSG has not been computer verified. There also seems to be no intuitive reason to expect the theorem to be true, as this MathOverflow question suggests. Apparently it wasn't known if there were finitely many sporadic groups until late in the proof, suggesting even to those familiar with the proof there is nothing intuitive about it. Though maybe there is something intuitive about it to experts now. However, the theorem is widely trusted by experts, so I would like to to understand why. What details I am interested in depends on what the answer is, but I am not asking about the structure of the proof itself . My impression is that it is widely believed there may be mistakes in the proof, but any mistake would be fixable. I don't understand how one can be confident that unfixable mistakes don't exist without being confident that no mistakes exist, unless there is an intuitive reason the theorem must be true. So if my impression is correct, an explanation of how you can be confident about one without the other would probably be satisfactory. This might not be anything specific to CFSG. $$ $$ So my first question is: Am I correct that it is widely believed there may be errors in the proof of CFSG? If so, how can we be confident those mistakes are fixable? Is there something intuitive about the proof itself that suggests it should be true? (if there is, I accept that I won't be able to understand what the intuitive thing is) If not, how can we be confident there are no errors without computer verification? This is not a duplicate of How confident can we be about the validity of the classification of finite simple groups? as that confirms that we are confident in the result, but doesn't explain where this confidence comes from. I do not think this question is opinion based, as I am asking why experts trust it, rather than if the person responding trusts it.","['finite-groups', 'group-theory', 'simple-groups', 'soft-question']"
4658543,Solving $\sqrt{x^2+20\sqrt2 x - 100} \space + $ $\sqrt{x^2 + 25} $ $= 15$ by hand,"I was solving a Euclidean geometry problem and I got the equation $\sqrt{x^2+20\sqrt2 x - 100} + \sqrt{x^2 + 25} = 15$ . I asked Wolfram Alpha and it says the answer is $60\sqrt3-70\sqrt2$ . But I'm wondering is it possible to solve it by hand? (without using softwares like Wolfram Alpha) Before I check the answer with WA, I thought the answer should be in the form $x=a\sqrt2$ and substituted it in the equation. (I justified it as each term on the LHS should be a non-negative integer) not sure why the answer is in the form $x= a\sqrt 2 +b \sqrt 3$ .
I also tried squaring both sides of the original equation but it seems that makes things more complicated.","['algebra-precalculus', 'radicals']"
