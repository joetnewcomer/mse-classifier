question_id,title,body,tags
4703366,Proving a trig identity with a median in triangle ABC,"In a given triangle $ABC$ , $\angle BAC=\alpha, \angle ABC = \beta, \angle ACB = \gamma, \angle BAM = \theta$ , where $M$ is the midpoint of side $BC$ . Prove the identity $$\frac{\sin\gamma}{\sin\beta}\cdot\frac{\sin\left(\beta-\theta\right)}{\sin\left(\beta+\theta\right)}=\cos\alpha.$$ $$ $$ I have checked this identity empirically, but I am having trouble proving the general statement. I tried to get an expression for $\sin\theta$ by realizing that $ $ $2 \cdot \frac{1}{2} AB \cdot AM \cdot \sin\theta = \frac{1}{2} AB \cdot AC \cdot \sin\alpha$ $ $ since the area of $\triangle ABC$ is twice the area of $\triangle ABM$ . However, this leads to an ungodly mess when computing $AM$ using Stewart's theorem and using the sine compound angle formula on the LHS fraction. $$ $$ My question: is there a clean(ish) way to prove this identity without resorting to extremely messy computations? My motivation: the proof of this identity leads to a proof of a symmedian property I am investigating.","['euclidean-geometry', 'trigonometry', 'geometry']"
4703369,Combinatorics: How to apply the implicit function scheme?,"A rooted dissection of a convex polygon with a distinguished edge (the
root) is a set of non-crossing diagonals of the polygon. Let $D_n$ be the class of rooted dissections of regular $(n + 2)$ -gons. The ordinary generating function $D(z)$ of $\mathcal{D} = \cup_n D_n$ satisfies $$D(z) = (1+D(z)) \biggl( \frac{1}{1-z(1+D(z))} - 1  \biggr).$$ Use the implicit function scheme to determine an asymptotic formula for $[z^n]D(z)$ . Remark: You can find the relevant section from the Flajolet & Sedgewick book page 467, 468 below. I suppose that the exercise is meant to be solved using $$G(z, D(z)) := (1+D(z)) \biggl( \frac{1}{1-z(1+D(z))} - 1  \biggr),$$ i.e. $$G(z, w) := (1+w) \biggl( \frac{1}{1-z(1+w)} - 1  \biggr).$$ However, I do not understand how to apply the scheme. Is it enough to compute the (regular) derivatives $G_z$ and $G_{ww}$ , solve the characteristic system, and then plug the results into the formula? So far I have: $$\frac{\partial}{\partial w} G(z,w) = -\frac{(w+1)z(wz+z-2)}{(-wz-z+1)^2},$$ so by $G(r,s) = s$ implying $z = \frac{w^2+2w-\sqrt{(w+1)^3} +1}{(w+1)^3}$ . Is this correct so far?","['analytic-combinatorics', 'combinatorics']"
4703399,Evaluating $\frac{d^n}{dt^n}e^{a(t-e^t)}$ as a single series to extend region of convergence for super root function,"$\def\srt{\operatorname{srt}}$ Introduction: There is a multiple series expansion for the super root $\srt_n(z)$ valid near $0.7<|z|<1.4$ . However, for around $0<|z|<1.3$ , there is this expansion: $$\sqrt[k]z_s=\srt_k(z)=z-\sum_{n=1}^\infty\frac1{n!}\frac{d^{n-1}}{dt^{n-1}}e^{(n+1)t-e^t-\overbrace{ne^{t-e^{\dots}}}^{k-2\ “e^t”\text s}}\bigg|_{\ln(-\ln(z))}$$ Unfortunately, there is no obvious way to expand it as $k-1$ sums preserving convergence. A starting point is a method for finding a single series expansion for $\frac{d^{n-1}}{dt^{n-1}}e^{(n+1)(t-e^t)}$ , when $k=3$ , or the derivatives in: $$\ln(-\ln(\srt_3(e^z)))=\ln(-z)+\sum_{n=1}^\infty\frac1{n!}\left.\frac{d^{n-1}}{dt^{n-1}}e^{n(t-e^t)}\right|_{\ln(-z)}$$ converging around $0<|z|<1.3$ Attempt 1: Trying $\srt_3(z)$ via $e^y$ Maclaurin expansion and changing summation order gives: $$\srt_3(z)=1-\sum_{n=1}^\infty\sum_{m=0}^n\frac{(-1)^m m^{n-m} n^{m-2}}{(m-1)!(n-m)!}\ln^n(z)$$ converges for about $0.4<|z|<1.3$ . If $n\gg m$ , the region of convergence is a bit larger , but not the original $0<|z|<1.3$ : Attempt 2: Using Stirling S2 , general Leibniz rule , and factorial power $a^{(b)}$ : $$\frac{d^{n-1}}{dt^{n-1}}e^{n(t-e^t)}=\sum_{k=0}^{n-1}\sum_{m=0}^n s_{n-1}^{(k)}e^{-ne^t}e^{(k-m+n)t}\binom k nn^{(m)} (-n)^{k-m}$$ but this is not a single series expansion for: $$\frac{d^n}{dt^n}e^{a(t-e^t)} =ae^{a(t- e^t)}\sum_{m=0}^{n-1} (-1)^m a_m e^{am}$$ Pattern : Creating a table and using the OEIS shows that: $$\frac{d^{n-1}}{dt^{n-1}}e^{(n+1)(t-e^t)}=(-1)^{n+1} ne^{(n+1)(t-e^t)}\left((-1)^{n+1} (n+1)^{n-2}+(-1)^n ((n+2)^{n-1}-(n+1)^{n-1})e^t+?e^{2t}+\dots+(n+1)^{n-2} e^{n-1}\right)$$ and $$\frac{d^n}{dt^n}e^{t-e^t}=(-1)^n\left((-1)^n+(-1)^{n+1}S_{n+1}^{(2)}e^t+(-1)^nS_{n+1}^{(3)}e^{2t}+(-1)^{n+1}S_{n+1}^{(4)}e^{3t}+\dots+e^{nt}\right)$$ Question: What are methods for finding $\frac{d^n}{dt^n}e^{a(t-e^t)}$ as a single series so that a series expansion of $\srt_3(z)$ converges around $0<|z|<1.3$ ?","['roots', 'lagrange-inversion', 'closed-form', 'derivatives', 'tetration']"
4703430,"My friend starts at $(1,0)$ and moves up. I start at $(0,0)$ and continuously approach them at the same speed. What's the equation of my movement?","My friend starts at $(1,0)$ and moves up. I start at $(0,0)$ and continuously approach them at the same speed. What's the equation of my movement? Furthermore, let's say my equation of my movement is $f(x)$ . Then, what is $\int_0^1 f(x)dx$ ? I've determined that the equations can be determined by the following system of differential equations, where $x$ and $y$ represent coordinates and $t$ represents time: $$\frac{dx}{dt} = \cos\left(\arctan\left(\frac{t-y}{1-x}\right)\right)$$ $$\frac{dy}{dt} = \sin\left(\arctan\left(\frac{t-y}{1-x}\right)\right)$$ $$t_0 = 0, x_0 = 0, y_0 = 0$$ However, I do not know how to solve these equations. I am aware that $\frac{dy}{dx} = \frac{t-y}{1-x}$ , but am unable to find $t$ in terms of $x$ and $y$ .
Here is approximately the graph of my movement in a simulation:","['integration', 'ordinary-differential-equations']"
4703460,Choosing number of subdivisions for approximating double integrals in context of image processing,"I have a few double integrals that I have to approximate that are based on getting motion parameters from a sequence of images, and I have chosen to use Riemann sums for that purpose. These integrals are over the domain $R = [-w,w] \times [-h,h]$ , where $2w$ and $2h$ are the image width and height respectively. What would be an appropriate number of subdivisions $m$ and $n$ to choose to ensure that I can use code to evaluate the integrals in all cases? I am confused as we don't have access to all necessary values of $x_{ij}$ and $y_{ij}$ as $m$ and $n$ increase. I was thinking of considering the image frames as my grids of values, but I don't know what the pixel widths and heights would be. I am not even sure if this is the correct approach for approximating the integrals, so please suggest if there are other appropriate approaches. Here is one of the integrals that I am approximating: $$\iint ((u-u_r)\beta - (v - v_r)\alpha)(-xy\beta + (y^2 + 1)\alpha) \ dxdy$$","['multivariable-calculus', 'approximation', 'computer-vision']"
4703468,How to find $ \int_0^1 \frac{x^n}{(1-x) \ln ^n(1-x)} d x? $,"Latest news Thanks to Gary Liang and metamorphy who had given me links of relevant materials so that the closed form of our integral can be found as $$
\boxed{\int_0^1 \frac{x^{n+1}}{(1-x) \ln ^{n+1}(1-x)} d x =-\frac{n+1}{n !} \sum_{k=1}^n\left(\begin{array}{l}
n \\
k
\end{array}\right)(-1)^{n-k}(k+1)^{n-1} \ln (k+1)}
$$ By this formula, we can evaluate the integral by finding the series instead of evaluating multiple integrals. Couple of days ago, I encountered the integral $$ J_2=\int_0^1 \frac{x^2}{(1-x) \ln ^2(1-x)} d x. $$ For convenience, I first transformed the integral by the substitution $x\mapsto 1-x$ and then made use of double integral. $$ \begin{aligned}
J_2  &=\int_0^1 \frac{(1-x)^2}{x \ln ^2x} d x =-\int_0^1(1-x)^2 d\left(\frac{1}{\ln x}\right) \\&=2 \int_0^1 \frac{x-1}{\ln x} d x=2 \int_0^1 \int_0^1 x^t d t d x\\&=2 \int_0^1 \int_0^1 x^t d x d t= 2 \int_0^1\left[\frac{x^{t+1}}{t+1}\right]_0^1 d t\\&= 2 \int_0^1 \frac{1}{t+1} d t= 2 \ln 2\\ \end{aligned}
$$ Then I tried generalise $J_2$ by raising the power by $2$ to $n+1$ and used similar technique to get $$J_{n+1}=\int_0^1 \frac{x^{n+1}}{(1-x) \ln ^{n+1}(1-x)} d x= -\frac{n+1}{n}\int_0^1 \left(\frac{1-x}{\ln  x}\right)^{n} d x $$ Replacing $\phi$ by $1$ in my post , we have $$
\begin{aligned}
J_{n+1}&= (-1)^{n+1}\frac{n+1}{n}\int_0^1\left(\frac{x-1}{\ln x}\right)^n d x \\
& = (-1)^{n+1}\frac{n+1}{n}\int_0^1 \left(\underbrace{\int_0^1 \int_0^1 \cdots \int_0^1}_{n \text { integral signs }} x^{t_1+t_2+\ldots+t_n} d t_1 d t_2 \cdots d t_n\right) dx \\&= (-1)^{n+1}\frac{n+1}{n} \underbrace{\int_0^1\int_0^1\cdots \int_0^1}_{n \text { integral signs } }\left(\int_0^1 x^{t_1+t_2+\ldots+t_n} d x\right) d t_1 d t_2 \cdots d t_n
\\ \int_0^1 \frac{x^{n+1}}{(1-x) \ln ^{n+1}(1-x)} d x&= (-1)^{n+1}\frac{n+1}{n}\int_0^1\int_0^1 \cdots \int_0^1\frac{1}{1+t_1+t_2+\cdots+t_n} d t_1 d t_2 \cdots d t_n \blacksquare\\
\end{aligned}
$$ For confirmation, we start with $$J_2=2\int_0^1 \frac{1}{1+t_1} d t_1=2\ln 2$$ $$
\begin{aligned}
J_3&=-\frac{3}{2} \int_0^1 \int_0^1 \frac{1}{1+t_1+t_2} d t_1 d t_2 \\&=-\frac{3}{2} \int_0^1\left[\ln \left(1+t_1+t_2\right)\right]_0^1 d t_2 \\
&=-\frac{3}{2} \int_0^1\left[\ln \left(2+t_2\right)-\ln \left(1+t_2\right)\right] d t_2 \\
&=-\frac{3}{2}\left[\left(2+t_2\right)\left(\ln \left(2+t_2\right)-1\right) -\left(1+t_2\right)\left(\ln \left(1+t_2\right)-1\right)\right]\,_0^1\\
&=\frac{3}{2} \ln \left(\frac{16}{27}\right)
\end{aligned}
$$ and so on. We can find $J_{n}$ as long as we could repeatedly find $\int_0^1x^k\ln x dx$ . My question is how to simplify the multiple integral or obtain any other closed form.  Your comments and alternative methods are highly appreciated.","['integration', 'calculus', 'multiple-integral', 'definite-integrals']"
4703487,The set of unitary operators on a complex separable Hilbert space is connected,"In my functional analysis class I was given a problem: Let $H$ be complex separable Hilbert space. Prove that the set of unitary operators on $H$ is connected (as topological space with operator norm). I know the proof that unitary operators are path-connected , so they are connected. This proof is not short. And I wonder if it is possible to prove that unitary operators are connected without proving path-connectedness? It would give shorter proof of this problem. I do not have any ideas how to prove connectedness of unitary operators without proving path-connectedness, so any hints are appreciated. Thanks!","['operator-theory', 'functional-analysis']"
4703488,A limit containing an integral,"I got stuck evaluating the limit of an expression containing an integral. $$\lim_{n\to+\infty} \frac{e^{(n+1)^2}}{(n+1)\int_{n}^{n+1}e^{x^2}dx}$$ Using Mathematica, I can get the reference answer of $2$ . But I don't know how to compute it by hand. I tried taking Taylor series at $x=n+1$ to cancel $e^{(n+1)^2}$ but it doesn't work - the denominator becomes a polynomial of $n$ , so the limit becomes $0$ , which is certainly wrong. Could somebody help me?","['integration', 'limits', 'definite-integrals']"
4703567,"Does the series $\sum_{1}^{\infty } \frac{\left ( -1 \right )^{n} }{n}e^{-\frac{x}{n} } $ converges uniformly on $\left [ 0,\infty \right )$?","Does the series $\sum_{1}^{\infty } \frac{\left ( -1 \right )^{n} }{n}e^{-\frac{x}{n} } $ converges uniformly on $\left [ 0,\infty  \right )$ ? Prove or disprove it. My first attempt is try to use Weierstrass M-test: $$\left |\frac{\left ( -1 \right )^{n} }{n}e^{-\frac{x}{n} }  \right | =\frac{1}{n}e^{-\frac{x}{n} } =\frac{1}{ne^{\frac{x}{n}} }< \frac{1}{ne^{\frac{0}{n}} }=\frac{1}{n}......(1)    $$ but unfortunately $\sum_{1}^{\infty } \frac{1}{n} $ is divergent, so (1) tell me nothing. My second attempt is try to use Dirichlet test: $$\left | \sum_{n=1}^{k} \left ( -1 \right ) ^n \right | < 2 \qquad \forall  x\in \left [0,\infty   \right ) ...\space this \space condition \space is \space OK$$ Next assume $b(n)=\frac{e^{\frac{-x}{n}}}{n} $ , then... $$b'{(n)} =\frac{ \left (e^{-\frac{x}{n}}\times \frac{x}{n^2}\times n  \right ) - \left ( e^{-\frac{x}{n}}\times1 \right ) }{n^2}= \frac{e^{-\frac{x}{n}}\times \left ( \frac{x}{n}-1  \right )  }{n^2}$$ sadly, it seems not to decrease monotonically to zero, so this condition has failed. What should I do in order to test the uniform convergence of this series on $\left [ 0,\infty  \right )$ ?","['real-analysis', 'calculus', 'uniform-convergence', 'sequences-and-series', 'convergence-divergence']"
4703586,Find the limit $\lim\limits_{n\to\infty}n\sin(2\pi\sqrt{1+n^2})$,"Determine the following limit: $$\lim_{n\to\infty}n\sin(2\pi\sqrt{1+n^2}).$$ My Approach: I have myself solved the problem over $n\in\mathbb N$ , but do not have any clue on how to solve it over $n\in\mathbb R$ . My solution for $n\in\mathbb N$ is as follows but my question is is the following problem even solvable over $n\in\mathbb R$ ? Anyways, here is my solution for $n\in\mathbb N$ . \begin{align*}
\lim_{n\to\infty}n\sin(2\pi\sqrt{1+n^2})&=\lim_{n\to\infty}n\sin(2\pi\sqrt{1+n^2}-2n\pi)\\
&=\lim_{n\to\infty}n\sin(2\pi(\sqrt{1+n^2}-n))\\
&=\lim_{n\to\infty}n\sin\left(2\pi\left(\dfrac{1}{\sqrt{1+n^2}+n}\right)\right)\\
&=\left(\lim_{n\to\infty}\dfrac{\sin\left(\dfrac{2\pi}{\sqrt{1+n^2}+n}\right)}{\dfrac{2\pi}{\sqrt{1+n^2}+n}}\right)\cdot\lim_{n\to\infty}\left(n\cdot\dfrac{2\pi}{\sqrt{1+n^2}+n} \right)\\
&=\pi
,\end{align*} which completes the proof when $n\in\mathbb N$ . But the problem is when I used that $-2n\pi$ , I used the fact that $n\in\mathbb N$ , and nowhere it is told what $n$ is which is why I doubt if this problem is solvable. Thank you.","['real-analysis', 'calculus', 'functions', 'limits', 'algebra-precalculus']"
4703588,A determinantal inequality,"The following is an exercise from the 2023 international selection at École Normale Supérieure (ENS): Prove that for any complex numbers $a_1, \dots, a_n$ and positive semi-definite complex matrices $A_1, \dots ,A_n$ , the following inequality is satisfied: $$ \det \left( \left| a_1 \right| A_{1\ } + \left| a_2 \right| A_2 + \dots + \left| a_n \right| A_n \right) \geq | \det(a_1 A_1 + \dots + a_n A_n)| $$ So far, I have tried using the Hadamard inequality on determinants but it doesn't seem to lead anywhere. Same for expanding the determinant using the permutations formula. Since it's an exercise from ENS, it is supposed to be somehow hard and I don't know how to ""start"". The exercises from ENS are known to be very hard to approach since it's literally the best mathematics department of France. Any hints, please?","['permutations', 'determinant', 'linear-algebra']"
4703616,An open set containing a closed ball should contain an open ball between them,"In this question I am probably missing some easy counterexamples: Let $(X,d)$ be a metric space, fix $x_0\in X$ , and suppose that an open set $G$ contains the closed ball $F:=\{x \in X: d(x,x_0)\le 1\}$ . Does there exist an open ball $B$ centered in $x_0$ such that $F\subseteq B\subseteq G$ ? Idea for a possible counterexample: Pick an infinite dimensional Banach space $X$ , set $x_0=0$ , and fix infinitely many points $z_i$ on the unit sphere (the boundary of $F$ ). Then construct $G$ as the union between $F$ and all the open balls with centers $z_i$ and radii $r_i$ with $\inf_i r_i=0$ (the union of the latter open balls should contain the unit sphere).","['general-topology', 'metric-spaces']"
4703664,Convergence of sequences in the one-point compactification,"Consider a 1st countable and Hausdorff space $X$ and its one-point compactification $X^* = X \cup \{\infty\}$ . The topology $\mathcal{T}^\ast$ contains the topology $\mathcal{T}$ of $X$ and all sets of the form $(X \setminus K) \cup \{\infty\} \subset X^\ast$ where $K \subseteq X$ is any compact (and closed) set. Supposedly, one can then show: a sequence $x_n \in X$ converges to $\infty \in X^*$ iff it has no convergent subsequence. For the backwards implication I came up with the following reasoning: Any subsequence $x_{n_k}$ of $x_n$ does not converge, i.e. for all neighborhoods $U\subset X$ of any point $x \in X$ the subsequence leaves the neighborhood for large enough $k$ . Therefore $x_{n_k} \notin U$ and so eventually $x_{n_k} \in (X \setminus K) \cup \{\infty\}$ . However, the converse seems wrong to me. If $x_n \to \infty \in X^\ast$ then for large enough $n$ the sequence is contained in the neighborhoods $U = (X \setminus K) \cup \{ \infty \} \subseteq X^\ast$ . So to me it looks like the sequence simply leaves all compact neighborhoods in $X$ . How does this imply that there can be no subsequence converging in $X$ somewhere outside of those compact sets? If $X$ were locally compact then any point would have a compact neighborhood, but here $X$ is only 1st countable and Hausdorff, so why does no converging subsequence exist?","['general-topology', 'compactification']"
4703671,"Prove that if $M$ is simply connected and $H^2(g) = 0$, then $M$ is symplectomorphic to an adjoint orbit.","A manifold $M$ is said to be homogeneous if there exists a transitive action $G ↷ M$ . Let $(M,ω)$ be a homogeneous symplectic manifold, i.e., there exists a transitive and symplectic action $G ↷ M$ . Prove that if $M$ is simply connected and $H^2(g) = 0$ , then $M$ is symplectomorphic to an adjoint orbit. Here $g$ is the lie algera of $G$ (the action group) and $H^*(g)$ is the  Chevalley-Eilenberg cohomology of the lie algebra $g$ . I would appreciate it if someone could help me understand and prove this statement. Specifically, I would like to know the steps and reasoning behind the proof. Could you please provide a detailed explanation or point me to any relevant resources that can help me grasp this concept? Thank you in advance for your assistance!","['symplectic-geometry', 'manifolds', 'group-actions', 'lie-groups', 'differential-geometry']"
4703679,Cartesian product of union,"I would like to know if the way I solve this exercise is correct, and if there is a shorter way to make the demonstration. Here is the exercise : Given : $E$ and $F$ two sets, $I$ an indexed family, $(A_i)_{i \in I}$ an indexed family of $E$ , $(B_i)_{i \in I}$ an indexed family of $F$ , Show that : $\bigcup_{i \in I} (A_i \times B) = \left( \bigcup_{i \in I} A_i \right) \times B$ Then I wrote : If $x = (a,b) \in \bigcup_{i \in I} (A_i \times B)$ , then $\exists i \in I, (a \in A_i \wedge b \in B)$ , then $(\exists i \in I, a \in A_i) \wedge (\exists i \in I, b \in B)$ , then $a \in \left( \bigcup_{i \in I} A_i \right) \wedge b \in B$ , so $x \in \left( \bigcup_{i \in I} A_i \right) \times B$ If $x = (a,b) \in \left( \bigcup_{i \in I} A_i \right) \times B$ , then $(\exists i \in I, a \in A_i) \wedge (b \in B)$ , then $(\exists i \in I, a \in A_i) \wedge (\forall i \in I, b \in B)$ , then $\exists i \in I, (a \in A_i \wedge b \in B)$ , so $x \in \bigcup_{i \in I} (A_i \times B)$ As a conclusion : $\bigcup_{i \in I} (A_i \times B) = \left( \bigcup_{i \in I} A_i \right) \times B$ Can anyone confirm everything is correct ? Is there a way to show it using equivalence ? Thanks !",['elementary-set-theory']
4703715,Inverse function of $x \mapsto \coth x - 1/x$,"Consider the function $f$ defined over $\mathbb{R}$ as $$f(x) = \coth x - \frac{1}{x}$$ if $x \neq 0$ and $f(0)=0$ . Since the function $\coth$ can be developed in series as $\coth x = \frac{1}{x} + \frac{x}{3} - \frac{x^3}{45} + \cdots$ around the origin, the function $f$ can easily be shown to be infinitely smooth over $\mathbb{R}$ , strictly increasing, with limits $\pm 1$ in $\pm \infty$ . The function $f$ is therefore a smooth bijection from $\mathbb{R} \rightarrow (-1,1)$ . My question is the following: Is there an analytic expression for the inverse $f^{-1} : (-1,1) \rightarrow \mathbb{R}$ of $f$ such that $f^{-1}(y) = x$ if and only if $f(x) = y$ ? Context: The function $f$ naturally appears in an optimization problem I am considering. It would be useful for me to understand and possibly compute what the inverse is.","['elementary-functions', 'functions', 'hyperbolic-functions']"
4703741,Is $y=|x|$ a differential manifold?,"In the Euclidean space $\mathbb R^2$ , the function of absolute values $y=|x|$ determines a $1$ -dimentional topological manifold $M$ . Define the natural projection map $$p:\mathbb R^2\to \mathbb R,$$ $$(x,y)\mapsto x,$$ let $f:=p|_M$ be the restriction of $p$ on $M$ , then $M$ can be covered by two charts $U,V$ , where $U=f^{-1}(-1,+\infty)$ and $V=f^{-1}(-\infty,1)$ , the transition map between $U$ and $V$ is the identity map, which is clearly a smooth function, so according to the definition of differential (or smooth) manifolds (see for example Wiki page ), can we get the conclusion:
the function image of $y=|x|$ is a differential manifold?",['differential-geometry']
4703760,Evaluating $\sqrt{x+\sqrt{\sqrt{x+\sqrt{\sqrt{\sqrt{x+...}}}}}}$,"How would one evaluate the infinitely nested radical $$\sqrt{x+\sqrt{\sqrt{x+\sqrt{\sqrt{\sqrt{x+...}}}}}}$$ due to the fact that each term cannot be evaluated individually to create a product or sum, and using repeated difference of two squares to find a pattern also doesn't seem to work, I am not sure where to even start. Any solutions or help would be appreciated $$\text{it can be written as follows to make the pattern more obvious}$$ $$f(x) = \left(x+\left(x+\left(x+...\right)^{\frac{1}{8}}\right)^{\frac{1}{4}}\right)^{\frac{1}{2}}$$ and defined by the limit of the recursive sequence of functions (credit to @Andrea Marino) $$
\left\{\begin{array}{rcl}
f_1(x,y) & = & y^{1/2}\\
f_n(x,y) & = & f_{n - 1}(x,x + y^{1/2^n}) \textrm{ if } n \geqslant 1 \end{array}\right.
$$ update: This questions is bountied for anyone who can find a closed form, or at least an accurate approximation","['radicals', 'sequences-and-series']"
4703788,Diagonal door brace orientation,"I'm trying to come up with functions that describe the orientation of a brace on a door, such that there is equal interface x between the brace and all 4 door members, provided door dimensions a , b , and brace width c . Given a , b , c , what function describes x ?  Or r (brace rotation around center)?",['geometry']
4703811,Doubt about exponential distribution [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I'm given a certain seed number, lambda and k number of observations. I'm suppossed to find sum(k), divide it in unit intervals, check how many events occured in each interval, calculate the mean of events and finally calculate the absolute deviation between the experimental value I got of the mean and the theoretical expected value. In exponential distribution, lambda is the expected value. Does this still apply in each interval? Is my theoretical expected value equal to lambda? Here follows my code in R set.seed(4731)
    k <- 4979
    lambda <- 9
    data <- rexp(k, lambda)
    sum <- cumsum(data)
    T <- ceiling(sum[k])
    subintervals <- cut(sum, breaks = seq(0, T, 1))
    num_events <- table(subintervals)
    mean <- mean(num_events)
    expected <- lambda
    absolute_deviation <- abs(mean - expected)
    round(absolute_deviation, 4)",['statistics']
4703812,Is the curve $t \mapsto t \sin \frac 1 t$ rectifiable?,"Let $a\le 0\le b $ and let $ \gamma : \left[ a,b \right] \to \mathbb{R}$ be defined by $\gamma(t)= t\text{sin}\frac{1}{t}$ for $  t\neq 0 $ and $\gamma(t)=0$ for $t=0$ . I want to investigate that if this function is rectifiable or not. I know that for paths or smooth curves being rectifiable is guaranteed. So, since $\gamma'(t) = \sin \frac{1}{t} - \frac{1}{t} \cos \frac{1}{t}$ for $  t\neq 0 $ , $\gamma'$ is continuous and bounded for $  t\neq 0 $ , hence a smooth curve and hence rectifiable. However, I highly doubt that this line of thinking is correct because the function is piecewisely defined, which is my main problem when attacking the problem. I know that there are two equivalent definitions for being rectifiable, one with the sup and the other is a convergence of the net of lengths of polygonal paths. However, I could not find a way to proceed from either definition.","['arc-length', 'analysis', 'real-analysis', 'multivariable-calculus', 'vector-analysis']"
4703828,Is this relation analysis correct?,"In my previous assignment, I was given two sets, A and B, both containing the elements 1, 2, 3, and 4. Additionally, I was provided with a relation C, which consisted of pairs of elements: (1, 1), (3, 4), (2, 2), and (3, 3). I argued that relation C was transitive based on the definition of transitivity. I believed that by considering (3, 3) as (a, b) and (3, 4) as (b, c), I could conclude that (a, c) would be (3, 4), which already existed in the set. To my surprise, my teacher disagreed and claimed that relation C was not transitive, reflexive, or symmetric. Now, I am wondering if I can contest my grade because I strongly believe that my reasoning is correct. I even sought assistance by posting a question on the Mathematics Stack Exchange website months ago, which can be found at the provided link: Transitive relation of non function","['relations', 'discrete-mathematics']"
4703848,How do you calculate the Expectation $E(X^4)$? [duplicate],"This question already has answers here : Methods for Finding Raw Moments of the Normal Distribution (4 answers) Closed last year . Suppose $X$ is a normal random variable with, $\mu_x\ {\ne}\ 0\ \text{or}\ 1$ and $\sigma_x\ {\ne}\ 0\ \text{or}\ 1$ . I would like to show that the expectation $E(X^4)$ has the following equality: $E(X^4)\ {=}\ \mu_x^4+6\sigma_x^2\mu_x^2+3\sigma_x^4$ . I tried to prove this by calculating $E[(x-\mu_x)^4]$ which leads me to $E[(x-\mu_x)^4]\ {=}\ E(x^4-4x^3\mu_x+6x^2\mu_x^2-4x\mu_x^3+\mu_x^4)$ $E(x^4)\ {=}\ 4\mu_xE(x^3)-6\mu_x^2\sigma_x^2-3\mu_x^4+E[(x-\mu_x)^4]$ where I used $E(x^2)\ {=}\ \sigma_x^2+\mu_x^2$ and $E(x)\ {=}\ \mu_x$ to combine terms. This result didn't seem to help. I've looked into using the generating function approach but for $\mu_x\ {\ne}\ 0\ \text{or}\ 1$ and $\sigma_x\ {\ne}\ 0\ \text{or}\ 1$ these calculations become tedious very quickly. Is there an easy and quick way to show this?","['statistics', 'probability-theory', 'probability', 'random-variables']"
4703867,"Affine Functions, Möbius and Hyperbolic","I haven't been able to figure out the relationship between objects in an Exercise of Do Carmo's book on Riemannian Geometry and this may be a bit vague, but I am posting out of despair. I want to understand how the hyperbolic metric arises on the upper half plane $\mathbb{H}$ , but struggle to understand why Do Carmo starts with $g(t) = yt +x$ . The following steps aim to make my question more precise, but not well defined: The functions $f(t) = yt +x$ ( $x,y \in \mathbb{R}, y>0$ ) define a group under composition, which can be understood as the upper half plane. Under composition, these functions define a group. We use this to define a left invariant metric, that at $e = (0,1)$ coincides with the usual scalar product on $\mathbb{R}^2$ ( $\langle u,v \rangle_{(f_2t+f_1)} = f_2^{-2}\langle u, v \rangle_e$ ). So far, we have $(G,\circ)$ and $\mathbb{R} \times \mathbb{R}_{>0}$ and our defined metric. Now, in part (b) of the Exercise, we add the complex upper half plane (which I write $\mathbb{H}$ ). It inherits complex multiplication and addition. We are asked to prove that $f_1 + if_2 = z \mapsto \frac{az + b}{cz + d}$ is an isometry of $G$ . Q: Why introduce the metric $g_{ij} = \delta_{ij}/y^2$ through these affine functions ( $G,\circ)$ ? It seems Do Carmo motivates the appearance of this metric as ""the one left-invariant under composition of real affine Transformations"" and then drops it and uses it on the complex upper half plane, which has a completely different structure to these affine transformations. Through my approaches so far: I've had the Idea of looking at the equivalence relation $(f,g) \sim (\lambda f, \lambda g), \lambda \in \mathbb{C}^\times$ . In this case: $(f_2t + f_1, g_2t + g_1) \sim (\frac{f_2t + f_1}{g_2t + g_1},1)$ which looks like a Möbius transformation, only $t \in \mathbb{R}$ in the Exercise. This didn't satisfy me but looks promising as $\mathrm{SL}_2(\mathbb{Z})$ is very close (acting on each component). If we add $z \mapsto -1/z$ we can then express the Möbius transformation as a composition of transformations of $G$ . But then, why start with $G$ first?  (random guess, Does adding this inversion correspond to adding a point at infinity?) In euclidian spaces, such affine transformations are our ""change of coordinates"", which leave the metric invariant. Here, we show that the Möbius transormations in a more general setting when we add that inversion. These actions seem to give us a way to ""straighten"" two points to a ""line"". Pointing in some direction for me to investigate would make me happy. The context is a talk that I will give on closed geodesics in $\mathbb{H}/\mathrm{SL}_2(\mathbb{Z})$ and their relationship to class Numbers. P.S. Having solved the Exercise, the discussion is not meant to be about a specific solution. Just the motivation behind the structure of the exercise.","['riemannian-geometry', 'number-theory', 'abstract-algebra', 'hyperbolic-geometry', 'differential-geometry']"
4703871,When is a solution $P(f'(x)) = Q(f(x))$ periodic or double periodic?,"Consider the differential equation $$P(f '(x)) = Q(f(x))$$ Where $P(x),Q(x)$ are polynomials. Examples are $f'(x) = 1 + f(x)^2$ where we get a tan solution and $f'(x)^2 = 4 f(x)^3 - g_2 f(x) - g_3$ where we get a Weierstrass elliptic function solution.
One of them is periodic , the other double periodic. In general, When is a solution $P(f'(x)) = Q(f(x))$ periodic or double periodic ? I looked at some famous elliptic functions and most of them are defined with 2 or 3 functions like the Dixon elliptic functions with $cm'(x) = - sm^2(x),sm'(x) = cm^2(x)$ what is related to the Fermat curve $x^3 + y^3 = 1$ and the Eisenstein integers. Or the lemniscate elliptic functions with $sl'(x) = (1+ sl^2(x)) cl(x) , cl'(x) = -(1 + cl^2(x)) sl(x)$ . However the lemniscate elliptic function $sl$ also satisfies a selfreference one : $(sl'(x))^2 = 1 - sl(x)^4$ thereby satifying the type of differential equation I was looking for. It is basically just that solving $$P(x) + Q(y) = 1$$ for $x$ or $y$ results in at least one function that satisfies : $$P(f '(x)) = Q(f(x))$$ with the right initial conditions. Usually what I find is that the degrees of $P$ and $Q$ are between $2$ and $4$ . So, what is going on ? Does every pair of polynomials $P,Q$ with degrees between $2$ and $4$ give double periodic functions ?
Are degrees above $4$ possible to get double periodic functions ? And when do we get periodic functions that are not double periodic ? Does the Fermat curve $$x^5 + y^5 = 1 $$ or $$x^7 + y^7 = 1$$ and their related differential equations give us double periodic functions ? I want to point out that a function of a periodic function is also periodic and the same applies to the double periodic case. Also we get the trivial case for a polynomial $M(x)$ : $$ M(P(f'(x))) = M(Q(f(x)))$$ which has as its solutions the same function $f$ as if $M$ was the identity function. What basicly is an answer to my question of bounded degree, but I am looking for more insightful and general results ofcourse. Some ideas I had were plugging in a fourier series with variable coefficients.
But I was dealing with infinitely many variables and not sure if my fourier series was even valid ; was it still analytic and did it still agree with the function it was describing ??
Another ideas was an analogue for fourier series, a series expansion for double periodic functions.
But I got stuck there too.
Not sure if that was going in the right direction or not.
Even if that works, I want to prove it does. I tried some (complex analysis and geometric function theory ) theorems but they had problems with the poles and analytic continuation around those.
The taylor radius was too small and fourier requires $L^2$ spaces anyways.
I might be able to solve a specific case but I want the general idea. I am ofcourse slightly aware of some basic results such as relating the period(if it exists) with some coefficients ( such as $g_2,g_3$ in the Weierstrass case ) and rewriting the equations as an integral. Or writing functions in terms of eachother. Or some infinite sums.
But that does not give me the insight I seek. I am not an expert at the addition formula's but I also understand that an addition formula implies periodic or double periodic.
But again that does not give me what I seek. How to look at this ?","['fourier-analysis', 'ordinary-differential-equations', 'periodic-functions', 'elliptic-functions', 'polynomials']"
4703940,Is $5^1$ the maximal power of 5 that divides evenly into ${1000 \choose 500}$?,"I am trying to solve an exercise from Hua Loo Keng's Introduction to Number Theory Chapter 1 Section 12 to further my understanding of the material. Here are the steps I took to arrive at this solution. Did I arrive at the correct answer or did I make a mistake? Thanks in advance for your time and consideration! (also this is my first post on here so apologies if this is poorly worded) The maximal power of a prime number p that divides n! is given by the series $$\sum_{i=1}^{\infty}\lfloor\frac{n}{p^i}\rfloor.$$ Please note that there are finitely many non-zero terms in the series. ${n\choose r} = \frac{n!}{r!(n-r)!}$ and always produces an integer solution. Thus, $${1000 \choose 500} = \frac{1000!}{500!(1000-500)!} = \frac{1000!}{500!500!}.$$ If we calculate the maximal power of the prime number 5 that divides evenly into 1000!, we calculate $$\sum_{i=1}^{\infty}\lfloor\frac{1000}{5^i}\rfloor = \lfloor\frac{1000}{5^1}\rfloor + \lfloor\frac{1000}{5^2}\rfloor + \lfloor\frac{1000}{5^3}\rfloor + \lfloor\frac{1000}{5^4}\rfloor + \lfloor\frac{1000}{5^5}\rfloor + \lfloor\frac{1000}{5^6}\rfloor + . . .$$ $$=$$ $$200 + 40 + 8 + 1 + 0 + . . . + 0 + . . . = 249$$ The maximal power of 5 that divides evenly into 1000! is 249. We can say that for some positive integer a, the product $a5^{249} = 1000!$ such that a isn't divisible by 5 itself. Similarly, we can calculate the maximal power of the prime number 5 that divides evenly into 500! to be $$\sum_{i=1}^{\infty}\lfloor\frac{500}{5^i}\rfloor = \lfloor\frac{500}{5^1}\rfloor + \lfloor\frac{500}{5^2}\rfloor + \lfloor\frac{500}{5^3}\rfloor + \lfloor\frac{500}{5^4}\rfloor + . . .$$ $$=$$ $$100+ 20+ 4 + + 0 + . . . + 0 + . . . = 124$$ The maximal power of 5 that divides evenly into 500! is 124. We can then that for some integer b, the product $b5^{124} = 500!$ such that b isn't divisible by 5 itself. Thus, we can state that $${1000 \choose 500} = \frac{1000!}{500!500!} = \frac{a5^{249}}{(b5^{124})(b5^{124})} = \frac{a5^{249}}{b^25^{248}} = \frac{a}{b^2}5.$$ Neither a nor b are divisible by 5, which means their quotients or products with other numbers will not result in a integer divisible by 5. Does this mean that the maximal power of 5 that divides evenly into ${1000 \choose 500}$ is 1 (or $5^1$ )?","['number-theory', 'prime-numbers']"
4703981,is this submartingale proof correct?,"The question is: ""Let $(X_n)$ be a sequence of independent nonnegative random variables. Show that $M_n = \prod_{j=1}^{n} X_j$ is a submartingale (supermartingale) with respect to $\mathcal{F}_n = \sigma(X_1, X_2, \ldots, X_n)$ , if $E(X_n) \geq 1$ ( $E(X_n) \leq 1$ ) holds for all $n$ ."" Is the following proof correct? In the following proof, how did we get this step: \begin{align*}
E(M_n | \mathcal{F}_{n-1}) &\leq E(X_n) \cdot \prod_{j=1}^{n-1} E(X_j) \\
&\leq E(X_n) \cdot \prod_{j=1}^{n-1} 1
\end{align*} ==== Let $(X_n)$ be a sequence of independent nonnegative random variables. We want to show that $M_n = \prod_{j=1}^{n} X_j$ is a submartingale (supermartingale) with respect to $\mathcal{F}_n = \sigma(X_1, X_2, \ldots, X_n)$ if $E(X_n) \geq 1$ ( $E(X_n) \leq 1$ ) holds for all $n$ . To prove this, we consider the conditional expectation $E(M_n | \mathcal{F}_{n-1})$ . By the properties of conditional expectation, we have: \begin{align*}
E(M_n | \mathcal{F}_{n-1}) &= E\left(\prod_{j=1}^{n} X_j \bigg| \mathcal{F}_{n-1}\right) \\
&= E\left(\prod_{j=1}^{n} X_j \bigg| X_1, X_2, \ldots, X_{n-1}\right) \quad \text{(independence of } X_1, X_2, \ldots, X_n) \\
&= E\left(E\left(\prod_{j=1}^{n} X_j \bigg| X_1, X_2, \ldots, X_{n-1}\right)\right) \quad \text{(tower property)} \\
&= E\left(X_n \cdot \prod_{j=1}^{n-1} X_j \bigg| X_1, X_2, \ldots, X_{n-1}\right) \\
&= E(X_n) \cdot E\left(\prod_{j=1}^{n-1} X_j \bigg| X_1, X_2, \ldots, X_{n-1}\right) \\
&= E(X_n) \cdot \prod_{j=1}^{n-1} E(X_j) \quad \text{(independence of } X_j \text{ for } j > n-1)
\end{align*} Now, if $E(X_n) \geq 1$ for all $n$ , we can bound the conditional expectation as follows: \begin{align*}
E(M_n | \mathcal{F}_{n-1}) &\leq E(X_n) \cdot \prod_{j=1}^{n-1} E(X_j) \\
&\leq E(X_n) \cdot \prod_{j=1}^{n-1} 1 = E(X_n) \cdot (n-1)
\end{align*} Since $E(X_n) \geq 1$ , it follows that $E(X_n) \cdot (n-1) \geq 1 \cdot (n-1) = n-1$ . Therefore, we have: \begin{align*}
E(M_n | \mathcal{F}_{n-1}) &\leq n-1
\end{align*} This inequality holds for all $n$ , which satisfies the submartingale property. To show that $M_n$ is a supermartingale when $E(X_n) \leq 1$ holds for all $n$ , we can reverse the inequality. Starting from: \begin{align*}
E(M_n | \mathcal{F}_{n-1}) = E(X_n) \cdot \prod_{j=1}^{n-1} E(X_j)
\end{align*} Since $E(X_n) \leq 1$ , we have: \begin{align*}
E(M_n | \mathcal{F}_{n-1}) \geq E(X_n) \cdot \prod_{j=1}^{n-1} E(X_j)
\end{align*} Again, using the assumption $E(X_j) \leq 1$ for all $j$ , we have: \begin{align*}
E(M_n | \mathcal{F}_{n-1}) \geq E(X_n) \cdot \prod_{j=1}^{n-1} 1 = E(X_n) \cdot (n-1) = n-1
\end{align*} Therefore, we have: \begin{align*}
E(M_n | \mathcal{F}_{n-1}) \geq n-1
\end{align*} This inequality holds for all $n$ , satisfying the supermartingale property. In conclusion, the sequence $M_n = \prod_{j=1}^{n} X_j$ is a submartingale when $E(X_n) \geq 1$ holds for all $n$ , and it is a supermartingale when $E(X_n) \leq 1$ holds for all $n$ .","['proof-explanation', 'sequences-and-series', 'martingales', 'probability-theory', 'probability']"
4704062,Check whether a set is well-defined or not,"This is probably a very basic question: how do we check whether a set is well-defined or not? For example, $X$ is a set. Let $\tau = \{U\subset X|$ $U^c$ is finite or is $X\}$ , $\tau$ is a topology on $X$ . My way of checking that it is well-defined is: take a specific $U\subset X$ , the set will tell me whether it's in the set or not, so it is well-defined. Is it a good way to check? Another example is: Suppose that $U$ is a subset of an inner product space $V$ , $U^{\perp} = \{v: \langle u,v\rangle = 0$ for all $u\in U \}$ is called the orthogonal complement of $U$ . How do we check that $U^{\perp}$ is well-defined (exists and is unique)? At some point, I was suddenly unsure about sets whose elements can not be listed/don't have a pattern. Is it that if take a specific element, we can tell whether it's in the set or not, then the set is well-defined? But I feel like I fail to see the essence of some sets in this way..",['elementary-set-theory']
4704084,"Find $\int_0^\infty f(x)dx$ given that $f(0)=1$ and for all $a>0$, the arc length from $0$ to $a$ equals the $x$-intercept of the tangent at $a$.","I made up the following problem. A curve $y=f(x)$ passes through $(0,1)$ , and for all $a>0$ , the arc length of $y=f(x)$ from $x=0$ to $x=a$ equals the $x$ -intercept of the tangent to $y=f(x)$ at $x=a$ . That is, in the graph below, the red arc and the red line segment have the same length. What is the area under the curve, $A=\int_0^\infty f(x)dx$ ? My attempt We have $$\int_0^a \sqrt{1+(f'(x))^2}dx=a-\frac{f(a)}{f'(a)}$$ Differentiating both sides with respect to $a$ gives $$\sqrt{1+(f'(a))^2}=1-\frac{(f'(a))^2-f(a)f''(a)}{(f'(a))^2}$$ $$(y')^6+(y')^4-(yy'')^2=0$$ But I don't know how to solve this differential equation. Then I approximated the curve with a sequence of triangles as shown below. Each of the colored line segments has length $\epsilon$ , which approaches $0$ . Let $h_n$ be the height, and let $\theta_n$ be the lower-right angle, of the $n$ th triangle from the left. Let $\theta_0=\pi/2$ . $h_n=1-\epsilon\sum\limits_{k=0}^{n-1} \sin{\theta_k}$ $\theta_n = \arctan{\left(\dfrac{h_n}{n\epsilon -\epsilon\sum\limits_{k=1}^{n-1}\cos{\theta_k}}\right)}$ I made an Excel simulation using these two equations, and it suggests that $A=1/3$ . (The length of the longest side of the triangles seems to approach $1/2$ .) I tried to rearrange the triangles in some clever way to get a total area of $1/3$ , but I haven't found a way.","['integration', 'improper-integrals', 'definite-integrals', 'ordinary-differential-equations', 'calculus']"
4704085,Relation between flow and exponential map,"In Proposition 1.7.12 in Hamilton's Mathematical Gauge Theory he states Let $G$ be a Lie group and $X$ a left-invariant vector field. Then its flow $\phi_t(p)$ through a point $p \in G$ is defined for all $t \in \mathbb{R}$ , $$\phi:\mathbb{R} \times G \rightarrow G\\ (t,p) \mapsto \phi_t(p),$$ and given by $$\phi_t(p) = p \cdot \exp tX = R_{\exp tX}(p) = L_p (\exp tX).$$ In his notation $L_x$ and $R_x$ are left and right multiplication by $x$ , and $D_e$ is the differential evaluated at $e$ . His proof is Define $\phi_t(p)$ for all $t \in \mathbb{R}$ by the right-hand side. It is clear that $$\phi_0(p) = p \cdot \exp(0) = p.$$ Furthermore, $$\frac{d}{dt}\Big|_{t = s} \phi_t(p) = 
 \frac{d}{d\tau} \Big|_{\tau = 0} L_p(\exp sX \cdot \exp \tau X)  \\ = D_e L_{p \exp sX}(X_e) \\ = X_{p \exp s X}  \\ = X_{\phi_s(p)},$$ since $X$ is left-invariant. This implies the claim by uniqueness of solutions of ordinary differential equations. I am having some trouble understanding what he did in the first two lines/equalities. The first seems to be a change of variables. The second appears to be using the chain rule, but I am not seeing how he obtains $X_e$ or why he is evaluating the differential at $e$ .","['differential', 'vector-fields', 'smooth-manifolds', 'lie-groups', 'differential-geometry']"
4704091,"If an arithmetic function is multiplicative, non-zero at a prime, and ""prime-linear"", is it the identity?","Let $f:\mathbb{N}\to\mathbb{N}\cup\{0\}$ be a function. Let $f(1)=1,$ and $f(ab)=f(a)f(b)$ whenever $\gcd(a,b)=1.$ Note that I am assuming that $f$ is multiplicative but not completely multiplicative. Let $\mathbb{P}$ be the set of all primes. Assume there exists a $p_0\in\mathbb{P}$ such that $f(p_0)≠0.$ Further, let $f(p+q)=f(p)+f(q)$ whenever $p,q\in\mathbb{P}.$ This is what I mean by a function that is multiplicative, non-zero at a prime, and ""prime-linear"". Is it true that $f=\mathrm{id}_{\mathbb{N}}?$ I think $f$ is indeed the identity function. I do not have a full proof of this, but I've made some progress: We begin by showing that $f(2)=2.$ Let $p_0$ (as given in the question) not be $2.$ Then, $f(2p_0)=f(2)f(p_0)$ since $\gcd(2,p_0)=1.$ Also, $f(2p_0)=f(p_0+p_0)=f(p_0)+f(p_0)=2f(p_0).$ So, $f(2)f(p_0)=2f(p_0).$ Since $f(p_0)≠0,$ we get that $f(2)=2.$ Suppose $p_0=2.$ Then, $f(10)=f(2)f(5)=f(5+5)=f(5)+f(5)=2f(5).$ Now, we show that $f(5)≠0.$ For the sake of contradiction, let $f(5)=0.$ Then, $f(5)=f(2)+f(3)=0.$ This gives $f(2)=-f(3).$ Since $f(2)\geq0,$ we get that $f(2)=0.$ This contradicts the fact that $f(p_0)=f(2)$ is unequal to $0.$ So, $f(5)≠0.$ Then, from $f(2)f(5)=2f(5),$ we can cancel out $f(5)$ and get $f(2)=2.$ From this, we also get $f(4)=4.$ Now, $f(12)=f(4)f(3)=4f(3)=f(7)+f(5)=f(3)+f(4)+f(2)+f(3)=6+2f(3).$ So, $4f(3)=6+2f(3).$ Hence, $f(3)=3.$ So far, we have: $$f(1)=1,$$ $$f(2)=2,$$ $$f(3)=3,$$ $$f(4)=f(2+2)=f(2)+f(2)=4,$$ $$f(5)=f(2+3)=f(2)+f(3)=5,$$ $$f(6)=f(2)f(3)=6,$$ $$f(7)=f(2)+f(5)=7,$$ $$f(8)=f(3)+f(5)=8,$$ $$f(9)=f(2)+f(7)=9,$$ $$f(10)=f(2)f(5)=10.$$ Now, I proceed by induction. I treat $1$ through $10$ as my base cases. Now, let $f(n)=n$ for each $n\in\{1,2,3,\ldots,k\}$ for some natural $k>10.$ Consider $f(k+1).$ If $k+1$ is composite and not a perfect prime power, then, $k+1=ab$ for some $a,b\in\{1,2,3,\ldots,k\},$ and $\gcd(a,b)=1.$ So, $f(k+1)=k+1.$ If $k+1$ is prime, it is odd since $k+1>2.$ So, $k$ is even. This implies that $k+4$ is even too. So, $k+4$ is composite. If $k+4$ is not a prime power, $f(k+4)=k+4$ since it can be written as $ab$ with $a,b\in\{1,2,3,\ldots,k\},$ and $\gcd(a,b)=1.$ Now, $f(k+4)=f(k+1+3)=f(k+1)+f(3)=f(k+1)+3=k+4.$ So, $f(k+1)=k+1.$ If $k+4$ is a prime power, it must be a power of $2$ since it's even. In that case, $k+6$ is not a prime power as it too, must be a power of $2$ if it is, and powers of $2$ cannot be seperated by $2$ unless they are $2$ and $4.$ So, $k+6=ab$ with $a,b\in\{1,2,3,\ldots,k\},$ and $\gcd(a,b)=1.$ Hence, $f(k+6)=k+6.$ Now, $f(k+6)=f(k+1+5)=f(k+1)+5=k+6.$ Hence, $f(k+1)=k+1.$ However, I am unable to deal with the case where $k+1$ is itself a prime power. Note: As an interesting aside, if we assume Goldbach's conjecture, then, using the induction assumption, we may show that $f(2a)=2a$ for all $2a$ that can be written as a sum of primes $p$ and $q$ with $p,q\in\{1,2,3,\ldots,k\}.$","['number-theory', 'elementary-number-theory', 'goldbachs-conjecture', 'arithmetic-functions', 'functions']"
4704117,Suppose $A$ and $B$ are two sets with $B\subset A.$ Let $f:A\to B$ be injective. Then show that $\exists$ a bijection $h:A\to B.$,"Suppose $A$ and $B$ are two sets with $B\subset A.$ Let $f:A\to B$ be injective. Then show that $\exists$ a bijection $h:A\to B.$ The proof given was : Let $$X=(A-B)\cup f(A-B)\cup  f^2(A-B)\cup \cdots=\bigcup_{n=0}^{\infty}f^n(A- B);f^0(A-B)=A-B.$$ Then clearly if $x\in X,x\in f^n(A-B)$ for some $n.$ Then $f(x)\in f^{n+1}(A-B)\implies f(x)\in X.$ Thus, $f(X)\subset X.$ We define, $$h:A\to B$$ by $$h(x)=f(x)\text{, if }x\in X,$$ $$x\text{ otherwise. }$$ To show that $h$ is one-one, let $x,y\in A$ and $h(x)=h(y).$ If $x,y\in X$ then $f(x)=f(y)\implies x=y$ If $x,y\in A-X$ then, $x=y,$ by definition of $h.$ If $x\in X$ and $y\in A-X$ then $$h(x)=h(y)\implies f(x)=y\implies y\in f(X)\subset X,$$ a contradiction. So, the last case can't occur. Thus, $h$ is one-one. To show that $h$ is onto, let $y\in B.$ If $y\in X$ then $y\in f^n(A-B)$ for some $n\geq 1\implies y=f^n(x),$ for some $x\in A-B$ $$\implies y=f(x')$$ where $x'=f^{n-1}(x)\in f(A-B)\subset X.$ $$\implies y=h(x').$$ If $y\notin X$ thhen, $h(y)=y.$ Thus, $h$ is onto. This completes the proof. However, I find this proof much convoluted,  which was not at all necessary. I tried devising an alternative proff, which is quite straight-foerward in it's working and might even be considered simpler. Here it is : Notation: If S is a set, then by $|S|$ I mean, Cardinality of the set $S$ Given, $B$ is a  subset of $A$ and $f:A\to B$ is an injective mapping. Since, $B$ is a subset of $A,$ we have, $|B|\leq |A|.$ Also, $f$ is an injective mapping from $A$ to $B$ implies, $|A|\leq |B|.$ Hence, $|A|=|B|$ is the only  possible conclusion. Again, as $f$ is an injection from $A$ to $B$ (and with the (now) established fact that, $|A|=|B|,$ ) $f$ is a bijection. Thus, $f=h$ and $h:A\to B$ is a bijection. I hope that  my  alternative approach is a correct one. Is there any way, I can improve the proof? I am looking, for some specific suggestions that might make my proof look more readable and understandable. Any suggestions/remarks against ways of improving it, will be highly appreciated. Also, please do point out if any requisite changes should be made or not. Lastly, if there are any errors in my  proof, please correct me.","['elementary-set-theory', 'proof-writing', 'alternative-proof']"
4704183,"find limit of $ \lim\limits_{(x,y)\to (2,0)} \frac{x+y-2}{x^2+y^2-4} $","I have a problem with finding a limit of this: $$\lim_{(x,y)\to(2,0)}\frac{x+y-2}{x^2+y^2-4}$$ I was thinking about using this: $\;\dfrac{1}{x^2+y^2}\leqslant\dfrac{1}{2|xy|}$ I also looked at some sequences to determine, whether the limit exists or not and my intuition is that it exists.
Any ideas how to calculate this limit? (Sorry, there was a mistake in the first question)","['limits', 'analysis']"
4704240,Is there something called function of two dependent variables?,"While reading in Stewart calculus in the section that talks about implicit differentiation in multivarible functions he wrote this : if $F(x,y)=0$ that define implicitly a differentiable function in $x$ , $y=f(x)$ where $F(x,f(x))=0$ and then he applied the chain rule \begin{gather} \frac{\partial F}{\partial x}+\frac{\partial F}{\partial y}\frac{d y}{dx}=0 \end{gather} so he treated $F$ as being function of $2$ variables although one of them is depending on the other I think this like the composite function in single variable calc, sometimes we write functions in terms of composition between others \begin{gather} h(x)=f(x^2)=sin(x^2) \end{gather} the function $h$ it's input is $x$ while the function $f$ it's input it $x^2$ and that is obvious while taking the chain rule we write \begin{gather} \frac{d h(x)}{dx}=\frac{d f(x^2)}{d(x^2)}\frac{d(x^2) }{dx} \end{gather} which is an indication that the term $(x^2)$ is the variable to $f$ even that it's connected to $x$ then if it applicable to single variable calc , then it can also be applicable to multivariable ? correct me if I wrote anything wrong .","['multivariable-calculus', 'calculus', 'functions']"
4704332,"""There are two different people who have visited exactly the same websites""","Let $W(x,y)$ mean that student x has visited website y, where the domain for $x$ consists of all students in your school and the domain for $y$ consists of all websites. Express the statement $∃x∃y∀z((x≠y)∧(W(x,z)↔W(y,z)))$ using a simple English sentence. Answer: There are two different people who have visited exactly the same websites. I think the given answer is wrong. Let's suppose that the domain of x and y (students) is {Eric, John, Albin} and the domain of z (websites) is {A, B, C}. According to the answer, if, for example, Eric and Albin both visited only website A (which means that both of them did not visit website B and C), the statement ∃x∃y∀z((x≠y)∧(W(x,z)↔W(y,z))) should be true. Let's see if it's true... (Eric≠Albin)∧(W(Eric,A)↔W(Albin,A)) (Eric≠Albin)∧((W(Eric,A)→W(Albin,A))∧(W(Albin,A)→W(Eric,A))) Eric is not equal to Albin, so Eric≠Albin is true. Moving to the next, Since Eric visited website A, we have to test whether Albin also visited website A. Because I already assumed that Albin visited website A, the compound proposition ((W(Eric,A)→W(Albin,A)) is true. Next compound proposition (W(Albin,A)→W(Eric,A)) should also be true, so the statement will return True. However, what if Eric, Albin, and John did not visit any websites? If we express the compound proposition, if becomes: (I only wrote the case where x and y are not equal since the statment will definitely return false if x and y are equal due to ""x≠y"") Reminder: domain of x,y = {Eric, John, Albin}, domain of z = {A, B, C}. {(W(Eric,A)→W(John,A))∧(W(John,A)→W(Eric,A)) ∧ (W(Eric,B)→W(John,B))∧(W(John,B)→W(Eric,B)) ∧ (W(Eric,C)→W(John,C))∧(W(John,C)→W(Eric,C))} ∨ {(W(Eric,A)→W(Albin,A))∧(W(Albin,A)→W(Eric,A)) ∧ (W(Eric,B)→W(Albin,B))∧(W(Albin,B)→W(Eric,B)) ∧ (W(Eric,C)→W(Albin,C))∧(W(Albin,C)→W(Eric,C))}. Now, let's suppose that Eric, Albin, and John did not visit any websites. If the proposition means ""There are two different people who have visited exactly the same websites."", then the proposition should return FALSE since no one has visited any websites. Checking from the first row, (W(Eric,A)→W(John,A))∧(W(John,A)→W(Eric,A)) returns TRUE since Eric did not visit website A. Therefore, W(Eric,A)→W(John,A) is vacuously true. Applying the same logic to (W(John,A)→W(Eric,A), it is also vacuously true. Therefore, the proposition returns TRUE. Applying the same logic to every single compound propositions, they all returns TRUE. Therefore, the compound propositon ∃x∃y∀z((x≠y)∧(W(x,z)↔W(y,z))) returns TRUE when nobody visited websites, which is a false. Is there anything that I could have interpreted the question in a wrong way? If so, I would be very appreciated to nofify my mistake and understand it in the right way.","['predicate-logic', 'discrete-mathematics', 'logic-translation']"
4704385,Continuous (or Lipschitz) map sends open sets onto Borel sets,"Let $(X,\|\cdot\|_{X})$ be a separable Hilbert space and let $(Y,\|\cdot\|_{Y})$ be a Banach space endowed with the Borel $\sigma$ -field. Let $f:X\to Y$ . Is it true that: if $f$ is continuous, then $f(A)$ is Borel for all open sets $A$ ; if $f$ is Lipschitz, then $f(A)$ is Borel for all open sets $A$ . Clearly (1) $\implies$ (2), so these are two statements of different strengths. If $\dim(X)<+\infty$ , then (1) is trivial. In fact, any open set can be written as the countable union of compact sets, and compactness is preserved under continuity. But what about the case $\dim(X)=+\infty$ ? My attempt was: let $\{e_{i}\}_{i=1}^{+\infty}$ be an orthonormal basis for $X$ . Since $X$ is separable, it suffices to prove that (1) or (2) hold whenever $A$ is an open ball (and, ultimately, one can focus directly on the unit ball). Furthermore, the statement holds for all open sets iff it does for closed sets. So let $A$ be the closed unit ball. Then, $A=\overline{\cup_{n=1}^{+\infty}K_{n}}$ , where $$K_{n}=\left\{\sum_{i=1}^{n}\alpha_{i}e_{i}\;\;\text{with}\;\;\sum_{i=1}^{n}|\alpha_{i}|^{2}\le1\right\}$$ are compact. However, here I get stuck (and I'm not even sure this is the correct way to go...).","['borel-sets', 'measure-theory', 'functional-analysis']"
4704388,"Exercise 3.6.8 from ""How to prove it""","The exercise: Let $\mathit{U}$ be any set. (a) Prove that for every $\mathit{A} \in \mathscr{P}(\mathit{U})$ there is a unique $\mathit{B} \in \mathscr{P}(\mathit{U})$ such that for every $\mathit{C} \in \mathscr{P}(\mathit{U})$ , $\mathit{C} \setminus \mathit{A} = \mathit{C} \cap \mathit{B}$ . How do I start solving this? On my own, I tried to do it like this: Let $A$ be arbitrary. Let $B$ be just a variable (since at this point i have no idea what it's supposed to be) Let $C$ be arbitrary. Suppose $a\in C\setminus A$ . I need to prove $a\in C\cap B$ . But I have no idea what $B$ even is, so im stuck. I think the question right now is, what should $B$ defined as such that when $a\in C\setminus A$ , then also $a\in B$ ? And $B$ also needs to be unique, which needs to be kept in mind.","['elementary-set-theory', 'proof-writing', 'logic']"
4704390,What is $f$ in $f(x)+f(1-x)=f(x^2)$?,"Find all $f(x)$ such that $f(x)+f(1-x)=f(x^2)$ is satisfied This is a question I made up, and my solution is as follows: Let $x=1$ (or $x=0$ ), then $f(0)+f(1)=f(1)$ which means that $f(0)=0$ . Substitute $-x$ for $x$ . We get that $$f(x)+f(1-x)=f(-x)+f(1+x)=f(x^2)$$ Note that substituting $x+1$ into the original functional equation yields $f(x+1)+f(-x)=f((x+1)^2)$ , meaning that $$f(x^2)=f((x+1)^2)$$ Thus we need to find a function that is the same when the input is the square of an integer. Two such functions are $$\frac{a\lfloor\sqrt{x}\rfloor}{\sqrt{x}}\text{ and }\frac{a\lceil\sqrt{x}\rceil}{\sqrt{x}}$$ Where $a$ is any real number. I am not sure how to prove that these are (not) the only functions that solve the functional equation. (The case $f(x)=0$ is included in the two functions when $a=0$ ). So what are the other functions, if there are any? Edit: The functions in my solution are only true when $x$ is an integer (meaning that, according to @BrunoB's comment, the functions aren't solutions), so this means that $x\in\mathbb{Z}$ . I would like to see a solution for $x\in\mathbb{R}$ or maybe even for $x\in\mathbb{C}$ .","['functional-equations', 'proof-writing', 'functions']"
4704413,Laplace transform problem involving piecewise function - Could you tell me where I'm going wrong?,"I attempted to solve the following second-order differential equation using the Laplace transform method: $$2y'' + 3y' + y = g(t)$$ with the initial conditions $y(0) = y'(0) = 0$ . The input function $g(t)$ is defined as follows: $$g(t)=\begin{cases} t, & 4<t<5 \\ 0,& \text{otherwise}\end{cases}$$ First, I applied the Laplace transform to both sides of the equation to obtain the transformed equation in the s-domain. Then, I rearranged the equation to solve for $Y(s)$ , the Laplace transform of $y(t)$ . Next, I applied partial fraction decomposition to express the transformed equation in a simpler form. I found the inverse Laplace transform of each term using tables and formulas. However, the resulting solution did not match the expected value of $y(4.5) = 0.20438$ . I'm looking for guidance and suggestions on how to correctly solve this differential equation and find the accurate value of $y(4.5)$ . If there are any alternative methods or insights that could be helpful, please share them with me. Thank you for your assistance and expertise in advance! Steps: Step 1: Define the differential equation: $$2y"" + 3y' + y = g(t)$$ Step 2: Define the input function $g(t)$ : $$g(t)=\begin{cases} t, & 4<t<5 \\ 0,& \text{otherwise}\end{cases}$$ Step 3: Apply the Laplace transform to both sides of the differential equation. Use the following formulas: $$L{y""} = s^2Y(s) - sy(0) - y'(0)$$ $$L{y'} = sY(s) - y(0)$$ $$L{y} = Y(s)$$ After applying the Laplace transform, the differential equation becomes: $$2(s^2Y(s) - sy(0) - y'(0)) + 3(sY(s) - y(0)) + Y(s) = G(s)$$ Step 4: Apply the initial conditions: $y(0) = 0$ $y'(0) = 0$ Substitute these initial conditions into the Laplace transformed equation. Step 5: Substitute the Laplace transform of the input function $g(t)$ into the equation. Use the following formula for the Laplace transform of $g(t)$ : $$L(g(t)) = G(s) = 1/s^2 - (e^{-4s}/s^2) + (e^{-5s}/s^2)$$ For each interval of $g(t)$ , substitute the corresponding Laplace transform into the equation. $$Y(s)[2s^2 + 3s + 1] = 1/s^2 - (e^{-4s}/s^2) + (e^{-5s}/s^2)$$ $$Y(s) =\frac{1/s^2 - (e^{-4s}/s^2) + (e^{-5s}/s^2)}{2s^2 + 3s + 1}$$ Stuck at this point!","['initial-value-problems', 'calculus', 'laplace-transform', 'ordinary-differential-equations']"
4704430,Chain rule in multivarible calculus,"I recently was reading spivak calculus on manifold and I've arrived to chain rule he wrote that \begin{gather}F(x)=f(g_{1}(x),...,g_{m}(x))\end{gather} then he took the derivative which lead to \begin{gather} \frac{d F(x)}{dx}=\sum_{j=1}^{m}\frac{\partial f}{\partial g_{j}}.\frac{dg_{j} }{dx}\end{gather} let's talk about the case that $x$ is a scalar and $g_{1}(x)=x$ and $g_{2}(x)=x^2$ \begin{gather} \frac{d F(x)}{dx}=\frac{\partial f}{\partial x}.\frac{d x}{dx}+\frac{\partial f}{\partial (x^2)}.2x \end{gather} doesn't that mean that we've considered $f$ as being function of $2$ variables ,because we took the partial derivatives w.r.t  variables inside it ?",['multivariable-calculus']
4704439,A few questions concerning the Poincare-Bendixson theorem,"I have a few questions concerning the Poincare-Bendixson result:( $G\subset \mathbb{R}^2$ open, $f\in C_{loc}^{1-}(G,\mathbb{R}^2)$ ) Let $u$ be a solution of $u'=f(u)$ so that $\overline{u(\mathbb{R^+})}\subset G$ compact. Assume $\omega(u)\cap E=\varnothing$ , where $E$ is the set of equilibria. Then $\omega(u)=u(\mathbb{R}^+)$ , i.e. $u$ is a periodic solution or $\omega(u)$ is a periodic orbit. Question According to https://sites.me.ucsb.edu/~moehlis/APC591/tutorials/tutorial3/node2.html a periodic orbit is a solution for which there exists $0<T<\infty$ so that $u(x+T)=u(x)$ for all $x$ . So if I have a periodic orbit why is this not a periodic solution? By Poincare-Bendixson we either have $\omega(u)=u(\mathbb{R}^+)$ or $\omega(u)=u([0,T])$ where $T$ is the period. But if I understand correctly if we have a periodic orbit it already holds $u([0,T])=u(\mathbb{R}^+)$ . It seems that this is false, is there a good example or picture to see this? Moreover I don't really get the point of homoclines: I'm imagining it as a loop which I run through once (then the period is infinity) or more than ones and then I have the corresponding period, is this the right picture? For me homoclinic orbits are periodic solutions. Depending on how often I run through the loop the omega limit set could contain only one point but also the complete loop, is this correct?","['periodic-functions', 'ordinary-differential-equations', 'dynamical-systems']"
4704481,Why is $\binom{k}{2}$ + $\binom{n−k}{2}$ maximized when $k = 1$ or $k = n - 1$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I was looking at another answer on this stack exchange ""An alternate proof strategy: if $G$ is the disconnected union of a $k$ -vertex graph and an $(n−k)$ -vertex graph, then it has at most $\binom{k}{2}$ + $\binom{n−k}{2}$ edges; this is maximized when $k=1$ or $k=n−1$ , in which case the bound on the number of edges is $(n−1)(n−2)2$ "" and I know it truly is maximized when $k=1$ or $k=n-1$ but I was hoping to get an analytical or combinatorial proof as to why?
.","['combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
4704483,Different definitions for a connection,"I'm learning Riemannian geometry and in particular about connections and I have now seen multiple different definitions for this and trying to understand how they are all the same. The first one and the one I'm relatively comfortable with is that if $\pi : E \to M$ is a smooth vector bundle over a smooth manifold $M$ , then a connection is a map $$\nabla : \mathfrak{X}(M) \times \Gamma(E) \to \Gamma(E), (X,Y) \mapsto \nabla_XY$$ satisfying certain the product rule and linearity over $\mathbb{R}$ in $Y$ as well as linearity over $C^\infty(M)$ in $X$ . The second one I have is that a connection on a smooth vector bundle $\xi$ is an $\mathbb{R}$ -linear map $$\nabla: \Omega^0(\xi) \to \Omega^1(M) \otimes_{\Omega^0(M)} \Omega^0(\xi)$$ which satisfies the Leibnitz rule $\nabla(f\cdot s)=df \otimes s + f \cdot \nabla s.$ The last one is from Wikipedia which states that for a smooth vector bundle $E \to M$ a connection is an $\mathbb{R}$ -linear map $$\nabla:\Gamma(E) \to \Gamma(T^*M \otimes E)$$ that satisfies the same kinda properties as the two above. Now I think that most of my confusion here is due to not understanding the tensor product properly. If anyone can provide some idea on why the two latter ones should coincide with the first one that would be much appreciated. Also for the record I'm quite well acquainted with differential forms so there is no need to explain what $\Omega^k(M)$ 's are here.","['riemannian-geometry', 'differential-geometry']"
4704507,Flat limit of the union of two lines,"I am confused about a computation in exercise 24.4.L in Vakil's Foundations of Algebraic Geometry, where we are asked to explicitly compute a flat limit. Let $X = \mathbb{A}^3 \times \mathbb{A}^1 \to Z = \mathbb{A}^1$ over a field $k$ , where the coordinates on $\mathbb{A}^3$ are $x, y, z$ , and the coordinates on $\mathbb{A}^1$ are $t$ . Define $Y$ away from $t = 0$ as the union of the two lines $y = z = 0$ and $x = z - t = 0$ . Find the flat limit at $t = 0$ . I think I am expected to get the union of two lines, plus some ""nonreducedness"" at the origin which points normal to the $xz$ -plane due to the ""motion"" of the line $x = z - t = 0$ as we move $t \to 0$ . However, my computation gives me the reduced union instead, and I'm not sure what I'm doing wrong. Let $A = k[x, y, z, t]$ and $B = k[x, y, z, t, 1/t]/((y, z) \cap (x, z-t))$ , so $B$ is the coordinate ring of $Y$ . We first compute the scheme-theoretic closure of $Y$ in $X$ , i.e. the kernel of the homomorphism $A \to B$ . This is just the pullback of the ideal $(y, z) \cap (x, z-t) \subseteq A_t$ to $A$ . Since $(y, z)$ and $(x, z-t)$ are both prime in $A_t$ and the primes of $A_t$ correspond bijectively to prime ideals in $A$ not containing $t$ via extension and contraction, the pullback of $(y, z) \cap (x, z - t)$ in $A$ is just $(y, z) \cap (x, z - t)$ . The fiber is then computed by appending the equation $t = 0$ , so the coordinate ring of $Y'\vert_0$ is $k[x, y, z, t]/((y, z) \cap (x, z - t), t) \cong k[x, y, z]/((y, z) \cap (x, z)) \cong k[x, y]/(xy)$ , the reduced union of the two lines. Surely the problem here arises from an erroneous computation of the pullback of $(y, z) \cap (x, z- t)$ to $A$ , but I don't feel like my reasoning was flawed. What exactly is going wrong here?","['algebraic-geometry', 'solution-verification', 'commutative-algebra']"
4704530,"Find the number of collections of $16$ distinct subsets of $\{1,2,3,4,5\}$ with the some specific property","Find the number of collections of $16$ distinct subsets of $\{1,2,3,4,5\}$ with the property that for any two subsets $X$ and $Y$ in the collection, $X \cap Y \not= \emptyset$ Solution given: Denote by $\mathcal C$ a collection of 16 distinct subsets of $\left\{ 1, 2, 3, 4, 5 \right\}$ . Denote $N = \min \left\{ |S|: S \in \mathcal C \right\}$ . Case 1: $N = 0$ . This entails $\emptyset \in \mathcal C$ . Hence, for any other set $A \in \mathcal C$ , we have $\emptyset \cap A = \emptyset$ . This is infeasible. Case 2: $N = 1$ . Let $\{a_1\} \in \mathcal C$ . To get $\{a_1\} \cap A \neq \emptyset$ for all $A \in \mathcal C$ . We must have $a_1 \in \mathcal A$ . The total number of subsets of $\left\{ 1, 2, 3, 4, 5 \right\}$ that contain $a_1$ is $2^4 = 16$ . Because $\mathcal C$ contains 16 subsets. We must have $\mathcal C = \left\{ \{a_1\} \cup A : \forall \ A \subseteq \left\{ 1, 2, 3, 4, 5 \right\} \backslash \left\{a_1 \right\} \right\}$ . Therefore, for any $X, Y \in \mathcal C$ , we must have $X \cap Y \supseteq \{a_1\}$ . So this is feasible. Now, we count the number of $\mathcal C$ in this case. We only need to determine $a_1$ . Therefore, the number of solutions is 5. Case 3: $N = 2$ . Case 3.1: There is exactly one subset in $\mathcal C$ that contains 2 elements. Denote this subset as $\left\{ a_1, a_2 \right\}$ . We then put all subsets of $\left\{ 1, 2, 3, 4, 5 \right\}$ that contain at least three elements into $\mathcal C$ , except $\left\{ a_3, a_4, a_5 \right\}$ . This satisfies $X \cap Y \neq \emptyset$ for any $X, Y \in \mathcal C$ . Now, we count the number of $\mathcal C$ in this case. We only need to determine $\left\{ a_1, a_2 \right\}$ . Therefore, the number of solutions is $\binom{5}{2} = 10$ . Case 3.2: There are exactly two subsets in $\mathcal C$ that contain 2 elements. They must take the form $\left\{ a_1, a_2 \right\}$ and $\left\{ a_1, a_3 \right\}$ . We then put all subsets of $\left\{ 1, 2, 3, 4, 5 \right\}$ that contain at least three elements into $\mathcal C$ , except $\left\{ a_3, a_4, a_5 \right\}$ and $\left\{ a_2, a_4, a_5 \right\}$ . This satisfies $X \cap Y \neq \emptyset$ for any $X, Y \in \mathcal C$ . Now, we count the number of $\mathcal C$ in this case. We only need to determine $\left\{ a_1, a_2 \right\}$ and $\left\{ a_1, a_3 \right\}$ . Therefore, the number of solutions is $5 \cdot \binom{4}{2} = 30$ . Case 3.3: There are exactly three subsets in $\mathcal C$ that contain 2 elements. They take the form $\left\{ a_1, a_2 \right\}$ , $\left\{ a_1, a_3 \right\}$ , $\left\{ a_1, a_4 \right\}$ . We then put all subsets of $\left\{ 1, 2, 3, 4, 5 \right\}$ that contain at least three elements into $\mathcal C$ , except $\left\{ a_3, a_4, a_5 \right\}$ , $\left\{ a_2, a_4, a_5 \right\}$ , $\left\{ a_2, a_3, a_5 \right\}$ . This satisfies $X \cap Y \neq \emptyset$ for any $X, Y \in \mathcal C$ . Now, we count the number of $\mathcal C$ in this case. We only need to determine $\left\{ a_1, a_2 \right\}$ , $\left\{ a_1, a_3 \right\}$ , $\left\{ a_1, a_4 \right\}$ . Therefore, the number of solutions is $5 \cdot \binom{4}{3} = 20$ . Case 3.4: There are exactly three subsets in $\mathcal C$ that contain 2 elements. They take the form $\left\{ a_1, a_2 \right\}$ , $\left\{ a_1, a_3 \right\}$ , $\left\{ a_2, a_3 \right\}$ . We then put all subsets of $\left\{ 1, 2, 3, 4, 5 \right\}$ that contain at least three elements into $\mathcal C$ , except $\left\{ a_3, a_4, a_5 \right\}$ , $\left\{ a_2, a_4, a_5 \right\}$ , $\left\{ a_1, a_4, a_5 \right\}$ . This satisfies $X \cap Y \neq \emptyset$ for any $X, Y \in \mathcal C$ . Now, we count the number of $\mathcal C$ in this case. We only need to determine $\left\{ a_1, a_2 \right\}$ , $\left\{ a_1, a_3 \right\}$ , $\left\{ a_2, a_3 \right\}$ . Therefore, the number of solutions is $\binom{5}{3} = 10$ . Case 3.5: There are exactly four subsets in $\mathcal C$ that contain 2 elements. They take the form $\left\{ a_1, a_2 \right\}$ , $\left\{ a_1, a_3 \right\}$ , $\left\{ a_1, a_4 \right\}$ , $\left\{ a_1, a_5 \right\}$ . We then put all subsets of $\left\{ 1, 2, 3, 4, 5 \right\}$ that contain at least three elements into $\mathcal C$ , except $\left\{ a_3, a_4, a_5 \right\}$ , $\left\{ a_2, a_4, a_5 \right\}$ , $\left\{ a_1, a_4, a_5 \right\}$ , $\left\{ a_2, a_3, a_4 \right\}$ . This satisfies $X \cap Y \neq \emptyset$ for any $X, Y \in \mathcal C$ . Now, we count the number of $\mathcal C$ in this case. We only need to determine $\left\{ a_1, a_2 \right\}$ , $\left\{ a_1, a_3 \right\}$ , $\left\{ a_1, a_4 \right\}$ , $\left\{ a_1, a_5 \right\}$ . Therefore, the number of solutions is 5. Putting all subcases together, the number of solutions is this case is $10 + 30 + 20 + 10 + 5 = 75$ . Case 4: $N \geq 3$ . The number of subsets of $\left\{ 1, 2, 3, 4, 5 \right\}$ that contain at least three elements is $\sum_{i=3}^5 \binom{5}{3} = 16$ . Because $\mathcal C$ has 16 elements, we must select all such subsets into $\mathcal C$ . Therefore, the number of solutions in this case is 1. Putting all cases together, the total number of $\mathcal C$ is $5 + 75 + 1 = \boxed{\textbf{(081) }}$ . Though I Did same approach to reach the solution but I want to know is there any general way to solve this problem. (recursion  or Some other) Since Given set contains only Five elements so I think author of problem wants us to make cases.","['algebra-precalculus', 'combinatorics', 'recurrence-relations']"
4704580,$\lim_{x\to \infty}{\bigl( \frac{(1+x)^x}{x^xe}\bigr)}^x$,"$$\lim\limits_{x\to \infty}{\left( \frac{(1+x)^x}{x^xe}\right)}^x$$ $$=\lim\limits_{x\to \infty}{\left( 1+\frac1x\right)}^{x^2}e^{-x}$$ Now since $\lim\limits_{x\to \infty}{\left( 1+\frac1x\right)}^{x}$ is $e$ when $x \to \infty$ , and that raised to the power of $x$ is $e^x$ $$=\lim\limits_{x\to \infty}e^xe^{-x}=1$$ However, the answer is given $\frac {1}{\sqrt e}$ . How? And where am I going wrong?","['limits', 'calculus', 'limits-without-lhopital', 'algebra-precalculus']"
4704630,Proving that Sobolev space embeds continuously into space of continuous functions,"We say $f \in L^2([-\pi, \pi])$ is an element of the Sobolev space $H^s(\mathbb{T})$ of order $s \geq 0$ if $$\sum_{n \in \mathbb{Z}} |\hat{f}(n)|^2 (1 + |n|^2)^s < \infty.$$ I am trying to prove the following: Let $s > \frac{1}{2}$ . If $f \in H^s(\mathbb{T})$ then $\exists g \in C([-\pi, \pi])$ such
that $f = g$ a.e. I have already shown that $H^s(\mathbb{T})$ is a Hilbert space with Hermitian inner product $$\langle f, g \rangle_{H^s(\mathbb{T})} := \sum_{n \in \mathbb{Z}} \hat{f}(n) \overline{\hat{g}(n)} (1 + |n|^2)^s,$$ and I believe I can also show that $\exists C(s) > 0$ such that $$||f||_{\infty} \leq C(s) ||f||_{H^s(\mathbb{T})},$$ with an added hint that the Weierstrass M-test may be useful, but I am overall not sure how to proceed.","['sobolev-spaces', 'functional-analysis']"
4704774,Asymptotic Riemann-Roch formula with exceptional divisor involved,"Let $X$ be a projective surface, let $D$ be a curve on $X$ viewed as a Cartier divisor. Let $P$ be a point on $D$ , and let $\pi: \tilde{X} \to X$ be the blow up $X$ at $P$ with exceptional divisor $E$ . We know that $\pi^*D.E=0$ and $E^2=-1$ . For sufficiently large integer $N$ , consider the dimensions of $H^0(X,\mathcal{O}_X(N\pi^*D)),H^0(X,\mathcal{O}_X(N(\pi^*D-E)))$ and $H^0(X,\mathcal{O}_X(N(\pi^*D+E)))$ .
Using asymptotic Riemann-Roch, I get: $h^0(X,\mathcal{O}_X(N\pi^*D))=N^2\pi^*D^2/2 + O(N)$ $h^0(X,\mathcal{O}_X(N(\pi^*D-E)))=h^0(X,\mathcal{O}_X(N(\pi^*D+E)))=N^2(\pi^*D^2-1)/2 + O(N)$ . That confuses me, since $E$ being effective, I should expect $$H^0(X,\mathcal{O}_X(N(\pi^*D-E))) \subset H^0(X,\mathcal{O}_X(N\pi^*D)) \subset H^0(X,\mathcal{O}_X(N(\pi^*D+E))).$$ But this contradicts the dimension count. Where I have things wrong? Any comment is appreciated!",['algebraic-geometry']
4704790,Can $a$ be negative here?,"If $f(x+y)=f(x)f(y);\forall x,y\in\mathbb R$ and $f(4)=16$ then find $f(3)$ I took $f(x)$ as $a^x$ . Since $f(4)=16$ , I got $a=2$ . Thus, $f(3)=8$ , which is the correct answer. But I wonder if $a$ can be $-2$ here.","['contest-math', 'functions']"
4704836,Weil restriction of a base change,"$\DeclareMathOperator{\Hom}{Hom}$ $\DeclareMathOperator{\Res}{Res}$ Let $G$ be an algebraic group over $K$ and let $L/K$ be a finite separable extension. Consider the Weil restriction $G' := \Res^L_K(G \times_K L)$ of the base change $G \times_K L$ . Is it true that $G'$ is isomorphic to $G$ as algebraic groups over $K$ ? Is it at least true if $G$ is an elliptic curve (1-dimensional projective algebraic group)? If not, what is the relation between $G$ and $G'$ ? I believe that $G' \times_K L$ is isomorphic to $G^{[L:K]}$ (at least if $L/K$ is Galois). But I am not sure.","['extension-field', 'algebraic-groups', 'algebraic-geometry', 'elliptic-curves']"
4704840,"If $f(x) = x^3 + ax^2 + bx + c$ has three distinct integral roots and $f(x^2+2x+2)$ has no real roots, then...","Suppose that $f(x) = x^3 + ax^2 + bx + c$ has three distinct integral roots and $f(x^2+2x+2)$ has no real roots. What is the minimum value of $a$ ? What is the minimum value of $b$ ? What is the minimum value of $c$ ? In the case when $a$ , $b$ and $c$ take their minimum values, if the roots of $f'(x) = K$ are equal, then what is $K$ ? My attempt: I factorised $g(x) = x^2+2x+2$ as $(x+1)^2 + 1$ . For all values of $g(x)$ , $f(x)$ will always give a non-zero value. I tried getting multiple linear equations in $a$ , $b$ and, $c$ , but it got me nowhere. I don't see any other way to obtain minimum values of said variables, let alone attempt question 4. This is a problem from a revision worksheet for our polynomials class. Currently, I have no other ideas as to how to approach solving this problem. Any help would be appreciated.",['algebra-precalculus']
4704852,Restricted Cotangent Bundle,"Let $(\mathcal{M},g)$ be a four-dimensional Lorentzian manifold and $\Sigma$ a Riemannian hypersurface such that $\mathcal{M}=\mathbb{R}\times\Sigma$ . How to show (if true) that $$T^{\ast}\mathcal{M}\vert_{\Sigma}\cong (\Sigma\times\mathbb{R})\oplus T^{\ast}\Sigma,$$ where $\Sigma\times\mathbb{R}$ denotes the trivial bundle. My motivation for this comes as follows: Lets take a section of $T^{\ast}\mathcal{M}$ , which is a $1$ -form $A=A_{\mu}\mathrm{d}x^{\mu}$ . Then, I can naturally write $A=A_{0}\mathrm{d}t+A_{\Sigma}$ (where $t$ is the ""time""-coordinate corresponding to $\mathbb{R}$ ), where $A_{0}\in C^{\infty}(\mathcal{M})$ and $A_{\Sigma}$ is a ""time-dependent"" $1$ -form on $\Sigma$ , i.e. a one-parameter family in $\Omega^{1}(\Sigma)$ labelled by the time-parameter $t$ . Now, if I restrict this section to $\Sigma$ , then I naturally get a map $$\Gamma^{\infty}(\Sigma,T^{\ast}\mathcal{M})=\Gamma^{\infty}(T^{\ast}\mathcal{M}\vert_{\Sigma})\xrightarrow{\cong}C^{\infty}(\Sigma)\oplus \Gamma^{\infty}(T^{\ast}\Sigma)\\ A=A_{0}\mathrm{d}t+A_{\Sigma}\mapsto (A_{0},A_{\Sigma}),$$ so I would expect the above splitting on the level of boundles.","['riemannian-geometry', 'vector-bundles', 'differential-topology', 'differential-forms', 'differential-geometry']"
4704857,Are complex analytic subsets of affine space necessarily zero sets of a globally defined holomorphic function?,"Let me fix some terminology. Let's say $U \subset \mathbb{C}^m$ is said to be a complex analytic subset if it can be covered by open subsets $V_i$ such that $U\cap V_i$ is the zero set of holomorphic functions $f_1,...,f_n$ on $V_i$ . Is it true that if $U$ is closed, there is a single chart $V\subset \mathbb{C}^m$ with holomorphic functions $f_1,...,f_n$ on $V$ such that $U=\{z\in V:f_i(z)=0,\forall i\}$ ? I guess there are two issues. The first problem is that if you have $f_i$ on $V$ and $g_i$ on $V'$ that agree on the intersection is there a unique $V$ and $V'$ that is holomorphic on $V\cup V'$ . I think basiaclly this is resolved by using analytic continuation for holomorphic functions in several variables by Hartog's theorem. The second issue is something that is bothering me. If your $U$ is locally cut out by one holomorphic function, we can patch these together to get a holomorphic function on one chart. But say on $V_1$ , it is the zero set of $f_1,...,f_n$ and on $V_2$ , it is the zero set of $g_1,...,g_n$ how do I know that for each $f_i$ there is a $g_j$ where they agree on the intersection?","['complex-analysis', 'algebraic-geometry', 'differential-geometry']"
4704882,Calculate the integral $\int\limits_{0}^{3}\frac{\sqrt{\arcsin \frac{x}{3}}}{\sqrt[4]{9-x^2}}dx$,"Calculate the integral $$\int\limits_{0}^{3}\frac{\sqrt{\arcsin \frac{x}{3}}}{\sqrt[4]{9-x^2}}dx$$ I was only able to get a grade, but it gives me nothing. I can't think of a way to solve this integral. You can't use numerical methods if they don't give you an approximately accurate answer through non elementary functions. I was able to get a numerical approximation on the computer, but how do I solve the integral analytically? $$\int\limits_{0}^{3}\frac{\sqrt{\arcsin \frac{x}{3}}}{\sqrt[4]{9-x^2}}dx \approx \sum_{i=0}^{n-1} \frac{h}{6} \left(\frac{\sqrt{\arcsin \frac{x_i}{3}}}{\sqrt[4]{9-x_i^2}} + 4\frac{\sqrt{\arcsin \frac{x_i + x_{i+1}}{6}}}{\sqrt[4]{9-(x_i + x_{i+1})^2}} + \frac{\sqrt{\arcsin \frac{x_{i+1}}{3}}}{\sqrt[4]{9-x_{i+1}^2}}\right)$$ where $h = (3-0)/n$ , $x_i = 0 + i \cdot h$ , and $n$ is the number of sub-sections into which the interval $[0, 3]$ is divided","['integration', 'calculus', 'real-analysis']"
4704886,A counterexample for weak convergence,"Find two sequence of probability measures $\{\mu_n\}$ and $\{\nu_n\}$ such that $$\forall f\in C_K:\int f\mathrm{d}\mu_n-\int f\mathrm{d}\nu_n\rightarrow 0.$$ but for no finite $(a,b)$ is it true that $$\mu_n(a,b)-\nu_n(a,b)\rightarrow0.$$ It is an exercise in Kai Lai Chung's book, here is a hint: let $\mu_n=\delta_{r_n}$ and $\nu_n=\delta_{s_n}$ and choose $\{r_n\}$ and $\{s_n\}$ suitable.","['probability-distributions', 'probability-theory', 'weak-convergence']"
4704965,"Is there a cojoin, the dual construction of the join of topological spaces?","The join $X\star Y$ of topological spaces $X$ and $Y$ can alternatively be written as a homotopy pushout of the canonical diagram $X\leftarrow X\times Y\rightarrow Y$ : \begin{equation}
X\star Y
=X\stackrel{h}{+}_{X\times Y}Y.
\end{equation} What about the dual construction, the homotopy pullback of the canonical diagram $X\rightarrow X+Y\leftarrow Y$ , written as $X\stackrel{h}{\times}_{X+Y}Y$ ? I haven't found anything about it yet. I suspect it to be empty (and therefore uninteresing) because of disjoint images in the mentioned diagram, but is that true and if yes, how can it be argued elegantly? I think, there should be a short argument involving cofibrations.","['pullback', 'general-topology', 'category-theory', 'algebraic-topology']"
4704980,$\sqrt{y + 1} - \sqrt{y - 1} = \sqrt{4y - 1}$. Find the value of $y$.,"I tried this way
Squaring both sides $(y+1)+ (y-1) - 2\sqrt{(y+1)(y-1)}=(4y-1)$ $1-2y =2\sqrt{(y+1)(y-1)}$ Again squaring both side. $1+4y^2-4y =4(y^2-1)$ $4y^2-4y+1=4y^2-4$ $y=5/4$ But on putting the value result was. 1=2. Which was wrong. I want to know where is the mistake.","['algebra-precalculus', 'radicals']"
4704982,Is $T$ totally bounded when $C_u(T)$ is separable?,"I'm seeking help with a question regarding the space of bounded and uniformly continuous functions $C_u(T,d)$ , where $(T,d)$ is a metric space. In this context, $C_u(T)$ is a closed subspace of $C_b(T)$ , therefore it is a Banach space as well. In the second edition of Giné and Nickl's Mathematical Foundations of Infinite-Dimensional Statistical Models (2021), page 17 , a statement reads, The Banach space $C_u(T,d)$ is separable if (and only if) $(T,d)$ is totally bounded. While I've been struggling with the proof of the ""only if"" part for the past three weeks, I've also attempted to construct counterexamples. I'm now seeking additional ideas or guidance to approach this problem. Here is what I have tried so far: I proved $C_b(T,d)$ is separable if and only if $(T,d)$ is compact, following Conway's A Course in Functional Analysis (2007) Theorem V.6.6 (p.140). If we can prove ""when $C_u(T,d)$ is separable, the completion of $T$ , denoted as $\overline{T}$ , is compact,"" we can prove the version for $C_u(T,d)$ . However, obtaining the Banach space isomorphism $C_u(T)\simeq_{\mathrm{Ban}}C_b(\overline{T})$ proves challenging, although $C_u(T)\simeq_{\mathrm{Ban}}C_u(\overline{T})$ is a relatively straightforward result. Abandoning the use of the $C_b(T)$ version's result, I examined other paths. If $C_u(T)$ is separable, the closed unit ball of the dual space $B^*\subset C_u(T)^*$ is metrizable. Coupled with the fact $B^*$ is always $w^*$ -compact, we find $B^*$ to be $w^*$ -sequentially compact. Hoping this would offer a proof to the original problem, I turned to the property of a metric space being totally bounded if and only if every sequence has a Cauchy subsequence. For an arbitrary sequence $\{x_n\}\subset T$ , we obtain a $w^*$ -convergent subsequence $\{\delta_{x_{n_k}}\}\subset B^*$ . However, attempts to prove $\{x_{n_k}\}\subset T$ as a Cauchy sequence by evaluating at some specially constructed $f_1,f_2,\cdots\in C_u(T)$ have been unsuccessful. Thank you for any suggestions or insights you can provide. Update: I also posted the same query on mathoverflow","['function-spaces', 'functional-analysis']"
4704988,Steps in proof of Proposition 2.4.17 in Liu,"I am going through the proof of Proposition 4.17 in Liu's Algebraic Geometry and Arithmetic Curves: First question (SOLVED): I can't verify why $V(f\vert_U)$ contains $W\cap U$ . If we write $\phi:W\cong\operatorname{Spec}A$ and $\psi:U\cong\operatorname{Spec}B$ , then formally $W=V(f\vert_W)$ means that $$
\phi(W)=V({\phi^\#}^{-1}f\vert_W)=V(\mathfrak p).
$$ where for notational convenience $a={\phi^\#}^{-1}f\vert_W$ . And $W\cap U\subset V(f\vert_U)$ means that $$
\psi(W\cap U)=\psi(W)\cap\operatorname{Spec}B=V({\psi^\#}^{-1}f\vert_V)=V(b)
$$ with $b={\psi^\#}^{-1}f\vert_V$ . So let $x\in W$ . We need to show that $b\in\psi(x)$ . We know that $$
a\in\phi(x).
$$ Attempt (SOLVED): Let's say for notational convenience that $W=\operatorname{Spec}A$ (equality as opposed to an isomorphism). We still work with an explicit isomorphism $\psi:U\cong\operatorname{Spec}(B)$ . So we want to show that $$
\psi(\psi^{-1}(U)\cap W)=V(b)
$$ where $b=(\psi^{-1})^\#f\vert_W$ . Let $x\in \psi^{-1}(U)\cap W$ . Then $x$ is given by a prime $\mathfrak p$ of $A$ , and write $f\vert_W=a\in A$ . Then we know that $a\in\mathfrak p$ .  Write $\mathfrak q=\psi(\mathfrak p)$ . Choose some $g\in A$ such that $$
\mathfrak p\in\operatorname{Spec}(A_g)\subset \psi^{-1}(U)\cap W.
$$ Note that we have a sheaf map $$
\Psi=\psi\vert_{\operatorname{Spec}(A_g)}:\operatorname{Spec} A_g\to\psi(\operatorname{Spec}A_g)\subset\operatorname{Spec}B,
$$ which corresponds to a map of rings $$
G:B\to A_g,
$$ i.e., $\Psi=\operatorname{Spec}(G)$ . So we have $$
G^{-1}(\mathfrak p)=\mathfrak q.
$$ To conclude that $b\in\mathfrak q$ , I would need that $G(b)=a$ . This is true, if we remember that $b=(\psi^{-1})^\# f\vert_U$ and $a=f\vert_W$ and that $G$ corresponds to $\psi^\#$ (along with restriction). More precisely, we have $$
a/1\in A_g=\mathcal O_{\operatorname{Spec}A}(\operatorname{Spec}A_g),
$$ and $$
G:\sigma\mapsto \sigma\vert_{\psi(\operatorname{Spec}A_g)}\mapsto \psi^\#\sigma\vert_{\psi(\operatorname{Spec}A_g)}.
$$ so recalling the definition of $b$ , we get indeed $$
G:b\mapsto f\vert_{\operatorname{Spec}A_g}=a/1\in A_g.
$$ It's clear to me how to adapt this proof for the 'general' case where we have an isomorphism $\phi:U\cong\operatorname{Spec}A$ (instead of equality), which I won't write down. Second question (SOLVED): How can I show that $\xi_1$ and $\xi_2$ would be generic points of $U_1\cap U_2$ ? Attempt: (SOLVED) Let $Z_i=\overline{\{\xi_i\}}$ . I know that $U_1\cap U_2$ is irreducible, and hence contains a generic point $\xi$ (since $U$ is a scheme), i.e. $$
\operatorname{Cl}_{U_1\cap U_2}(\xi)=U_1\cap U_2.
$$ It follows that $$
\operatorname{Cl}_{U_1}(\{\xi\})=\operatorname{Cl}_{U_1}\operatorname{Cl}_{U_1\cap U_2}(\{\xi\})=\operatorname{Cl}_{U_1}(U_1\cap U_2)=U_1,
$$ where in the last step I used that $U_1\cap U_2$ is dense in $U_1$ (which follows from irreducibility of $U_1$ ). Therefore $$
\operatorname{Cl}_{Z_1\cap U_1}(\{\xi\})=\operatorname{Cl}_{U_1}(\{\xi\})\cap U_1\cap Z_1=U_1\cap Z_1.
$$ Now using that $U_1\cap Z_1$ is dense in $Z_1$ (by irreducibility of $Z_1$ ), we conclude that $$
\operatorname{Cl}_{Z_1}(\{\xi_i\})=\operatorname{Cl}_{Z_1}(\operatorname{Cl}_{Z_1\cap U_1}(\{\xi\}))=Z_1.
$$ and hence $\xi=\xi_1$ by uniqueness of the generic point. The case for $Z_2$ is similar.",['algebraic-geometry']
4705073,Is convergent sequence in $L^2$ almost surely bounded?,"Let $(f_n)_{n\ge 1}$ be a sequence of function that converges to some $f\in L^2(\mathbb{R})$ , i.e., $||f_n-f||_{L^2(\mathbb{R})}=0$ . Clearly the sequence of norms $(||f_n||_{L^2(\mathbb{R})})_{n\ge 1}$ is bounded. Moreover, it is well-known that $(f_n)$ has a convergent subsequence that converges almost surely. I would like to prove/disprove the following statement: There exists $g\in L^2(\mathbb{R})$ such that $|f_n|\le g$ a.s. for all $n\ge1$ . Edit : As S.L. commented below, this statement is false if we endow $\mathbb{R}$ with the Lebesgue measure. I'm still wondering whether the above statement holds when we consider a probability measure on $\mathbb{R}$ , for example, the Gaussian measure $\mu(A)=\int_A e^{-x^2/2}dx$ .","['measure-theory', 'convergence-divergence', 'probability-theory', 'almost-everywhere']"
4705124,How to show the continuity of the Laplacian of the heat kernel (in t),"Let $K_t(x):=(4\pi t)^{-\frac{d}{2}}e^{-\frac{x^2}{4t}}$ be the heat kernel for $t > 0$ . How can I show that the function $t \mapsto (\Delta K_t)*f$ is continuous in $L^p(\mathbb{R}^d)$ (associated with the $L^p$ norm) for $f \in C_c^{\infty}(\mathbb{R}^d)$ ? I have proven the the continuity of this function $t \mapsto(\Delta K_t)*f(x)$ for every $x$ but clearly this is not sufficient.
I have that $\lvert| \Delta K_t |\rvert_{1}\leq \frac{d}{2t}+ \frac{1}{2}(4 \pi t)^{-d/2}(\frac{\pi}{t})^{1/2}$ , but I can't find the correct bound function to use this.","['functional-analysis', 'heat-equation', 'partial-differential-equations']"
4705143,Prove the limit of the expectations of max over $X_i/n$ goes to $0$ for positive integer-valued r.v. with identical distributions but NOT independent,"Let $X_1,X_2,...$ be positive integer-valued random variables with the same distribution but they can be dependent on each other. We want to prove that $\lim_{n\rightarrow \infty} E[\max_{i<n} X_i/n] = 0$ . I keep running into issues with the lack of independence. I want to do the classic ""if max is less than $\epsilon$ then they all have to be so raise the probability of one to the $n$ "" type of trick, but it obviously fails if they are dependent. Maybe the idea can be used, but not sure how. Clearly $$E\left[ \max_{i<n} \frac{X_i}{n} \right] = \sum_{m=0}^n P\left(\max_{i<n} \frac{X_i}{n}>m \right)$$ which seems like it should be the key, but I keep hitting a wall on bounding this above in a way that allows the limit to work. This is from Ross and Pekoz ""A Second Course in Probability"" Chapter 1, Problem 12. Just working on it as a summer project for myself.","['expected-value', 'probability-limit-theorems', 'probability-theory']"
4705184,Help check that function is non-negative,"Consider the function: $$f_a(x) = [\phi(a+x)(a+x) + \phi(a-x)(a-x)][\Phi(a-x)-\Phi(-a-x)]+[\phi(a+x)-\phi(a-x)]^2$$ where $a>0$ is a real number and $\phi(\cdot), \Phi(\cdot)$ denote the probability density function and the cumulative distribution function of a standard normal random variable, respectively. I would like to prove that the function is non negative. The graph (blue) of $f_3$ is shown below. The claim looks right, and it is easy to check it for the interval $x \in [-a,a]$ . Any help to prove the remaining part of the real line will be of great help. Note that by the symmetry of the function we only need to check it for $x>0$ , or as I have already done a little, for $x>a$ . Edit 1: We also see that the function is such that $f_a(x) \to 0$ as $x \to \infty$ . I am thinking we could check that $f_a''(0)>0$ and that for $x>0$ the function has a unique critical point (or not more than one). Then it will follow our claim, since if we prove this then there can not be some $x^*>0$ such that the function changes sign since then there would be another critical point. But of course to analyse the derivative and to prove doesn't look easy. Edit 2: Changed the first sign, had a typo. Edit 3: Graph (red) of the function $\phi(a+x)(a+x) + \phi(a-x)(a-x)$ for $a=3$ again. Edit 4: In black the same function divided by $[\Phi(a-x)-\Phi(-a-x)]^2$ is plotted. Since this is a non negative function, clearly the original is non negative iff this quotient is. This graph seems to indicate that the $f_a(x)$ is indeed non negative.",['analysis']
4705204,Uniformly Sampling from a High-Dimensional Unit Sphere [duplicate],"This question already has answers here : Algorithm to generate an uniform distribution of points in the volume of an hypersphere/on the surface of an hypersphere. (3 answers) Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved This Question is supposed to gather techniques on how to uniformly sample points on $\mathbb S^{d-1}$ for large $d$ . There are a few things to keep in mind for this problem, mainly as the dimension $d$ increases: The computation of the norm of vectors in $\mathbb R^d$ becomes increasingly expensive. Hence methods relying on normalization of vectors become less efficient. The volume of the $d$ -ball in relation to larger sets containing the $d$ -ball decreases. As a consequence, acceptance-rejection sampling methods become less efficient as we reject too many samples. To give two examples of ""naive"" methods one might try, which do work well in low dimensions but not in large dimensions, consider the following: Normalizing a symmetric distribution: First, sample a random vector $X\in\mathbb R^d$ from a distribution which is invariant under symmetry transformations of $\mathbb S^{d-1}$ , then normalize $X$ , i.e. compute $X/\Vert X\Vert$ . This normalized vector will be uniformly distributed on $\mathbb S^{d-1}$ . This method suffers from Problem 1. Rejecting points from the unit cube: Sample a random vector $X$ uniformly in $[-1,1]^d$ . If the random vector lies inside the $d$ -ball, keep the sample, if not, discard the sample. This will generate a uniform distribution on the $d$ -ball. A uniform distribution on $\mathbb S^{d-1}$ can then be achieved by normalizing the samples. This acceptance-rejection method suffers from Problem 2, since as $d$ increases, fewer points in the unit cube are located in the unit $d$ -ball and hence we will reject a lot of samples. (This method also suffers from Problem 1, but that's beside the point.) Question: What are sampling techniques that achieve a uniform distribution on $\mathbb S^{d-1}$ but avoid Problems 1 and 2 for large $d$ ?","['statistics', 'uniform-distribution', 'probability', 'sampling']"
4705212,Is $f(g(x))$ discontinuous?,"Question: Let $f(x) = \frac{1}{15x^2+8x+1} $ and $ g(x)= \frac{1}{(x-1)(x-2)} $ , then the number of points of discontinuity of $f(g(x))$ is? The answer key claimed that the answer is $1$ , but I don't agree with it. I claim that the answer should be zero, since $f(g(x))$ simplifies to $h(x)=\frac{x^{2}-3x+2}{(5x^{2}-15x+11)(3x^{2}-9x+7)}$ . So $h(1)=h(2)=0$ , which upon plotting on Desmos gives rise to same results. However, there is a counter argument that since $g(x)$ is not defined at $x=1,2$ , how can $f(g(x))$ be defined at those points? And hence there are $2$ points of discontinuity. Which of the arguments is correct?","['limits', 'functions', 'continuity', 'function-and-relation-composition']"
4705215,How to change from cartesian coordinate to cylindrical coordinates,"Consider the triple integral $$\iiint_{K} \frac{z}{2+ x^2 + y^2} dV$$ , where K is the region defined by $z \geq \sqrt{x^2 + y^2}$ and $x^2 + y^2 + z^2 \leq 9$ . The question then asks me to rewrite the region using cylindrical coordinates which I did but the problem is that my bounds for r is wrong. Here is what I did. $$\vec{r}(r,\theta, z) = (r\cos(\theta),r\sin(\theta), z)$$ $$0\leq r \leq 3$$ $$r\leq z\leq\sqrt{9-r^2}$$ $$0\leq \theta \leq 2\pi$$ The reason why I gave r the bounds it has is because if you see the equation: $$x^2 + y^2 + z^2 \leq 9$$ Here you can see that the radius needs to be less than 3 and then if you look at: $$z \geq \sqrt{x^2 + y^2} \iff 0 \geq x^2 + y^2 - z^2  $$ Here you can see that the radius is greater than $0$ . But is it though? I just assumed because this is not a sphere but a hyperboloid of 2 sheets. But like said the correct bounds for r is $0 \leq r \leq \frac{3}{\sqrt{2}}$ . And I don't see why at all?","['cylindrical-coordinates', 'definite-integrals', 'multivariable-calculus', 'calculus', 'spherical-coordinates']"
4705249,What to call the set of all sets of summands of a number?,"The number 3 can be summed by $1 + 1 + 1$ or $1 + 2$ . The set of all multisets of summands would be $\left\{\left\{1,1,1\right\},\left\{1,2\right\}\right\}$ Is there a general term for this kind of comprehensive set of possible decompositions? Maybe no specific word for summands exists, but there might be a word for this concept for factors? E.g. 12 is 2×6, 2×3×2, 3×4, etc.","['elementary-set-theory', 'factoring', 'terminology']"
4705297,How to change coordinates and variables when doing double integral?,"Let D be the bounded region in the first quadrant of $R^2$ bounded by curves $x^2 + 16y^2 = 16, x^2 + 16y^2 = 1, x = y$ and the positive y−axis.
Describe $D$ in the $(u, v)$ −plane when $u = x^2 + 16y^2$ and $v =\frac yx,$ and calculate the integral $$\iint_{D} \frac{y}{x} dA$$ So in order to solve the integral I used the new coordinates $1\leq u \leq 16$ and $1\leq v \leq \infty$ . Afterwards I calculated the the determinant of the jacobian and got $$ |\det J| = 1+32\left(\frac{y}{x}\right)^2 = 2 + 32v^2$$ Then I did: $$\iint_{D} v (2 + 32v^2) dudv$$ But I got the wrong answer and instead in the answers they divided the determinant of jacobian. $$\iint_{D} \frac{v}{(2 + 32v^2)} dudv$$ Can someone explain why?","['integration', 'definite-integrals', 'improper-integrals', 'multivariable-calculus', 'calculus']"
4705301,Determining the marginal distribution for a markov chain,"Let ${X_n : n = 0, 1, 2, . . .}$ denote a Markov chain with the states $S = {1, 2, 3}$ and transition matrix P given by $$
\begin{bmatrix}
0 & 0.5 & 0.5 \\
0.1 & 0 & 0.9 \\
0.8 & 0.2 & 0
\end{bmatrix}
$$ Determine whether the Markov chain has a marginal distribution. Determine the it and explain how you found it. Also determine whether the boundary distribution is unique. So i know that an irreducible and aperiodic markov chain has a unique solution, which also is the limiting distribution. In this case, the stationary distribution is a limiting distribution. To find the stationary distribution i need to solve the following: \begin{align*}
    \pi_1 &= \frac{1}{2}\pi_2 + \frac{1}{2}\pi_3   \\
    \pi_2 &= \frac{1}{10}\pi_1 + \frac{9}{10}\pi_3 \\
    \pi_3 &= \frac{8}{10}\pi_1 + \frac{2}{10}\pi_2 \\
    1 &= \pi_1 + \pi_2 + \pi_3
\end{align*} I have solved it and got that $\pi_1 = \frac{1}{3}$ , $\pi_2 = \frac{1}{3}$ and $\pi_3 = \frac{1}{3}$ Now that i have found my stationary distribution, which is also the limiting distribution what do i do with it? Is my limiting distribution the marginal distribution? I know that the desired marginal distribution that i need to find is $(0.35, 0.25, 0.4)$ but i can't manage to get this.","['statistics', 'marginal-distribution', 'probability-distributions', 'markov-chains', 'probability']"
4705323,Cool clock question [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question A clock has a second, minute, and hour hand. A fly initially rides on the second hand
of the clock starting at noon. Every time the hand the fly is currently riding crosses
with another, the fly will then switch to riding the other hand. Once the clock strikes
midnight, how many revolutions has the fly taken?","['elementary-number-theory', 'algebra-precalculus']"
4705345,Help with Theorem 2.9.7 from Federer's Geometric Measure Theory,"Suppose $\phi$ and $\psi$ are Borel regular measures (outer measures) on a metric space $X$ such that $\phi(A),\psi(A)<\infty$ for every bounded subset $A\subseteq X$ . One defines a Borel regular measure $$\psi_{\phi}(A)=\inf \{ \psi(B) \, ; \, B \, \text{is a Borel set and} \, \phi(A-B)=0 \}$$ whenever $A\subseteq X$ . The first step in the proof of Theorem 2.9.7 is to show that if $A\subseteq X$ is $\phi$ -measurable then it is also $\psi_{\phi}$ -measurable. It's clear to me that $A$ is contained in a Borel set $B$ such that $\phi(B-A)=0$ , but why does this imply $\psi_{\phi}(B-A)=0$ ? Any help would be appreciated.","['measure-theory', 'outer-measure', 'borel-measures', 'geometric-measure-theory', 'real-analysis']"
4705368,A suspicion on orthonormal basis coefficients,"Let's assume I have a $2$ dimensional vector space with inner product, and a basis where the inner product can be represented as $$
\begin{bmatrix}x &y\end{bmatrix}
\begin{bmatrix}E &F\\F &G\end{bmatrix}
\begin{bmatrix}x\\y\end{bmatrix}$$ In this basis, I express an orthonormal basis $\{e_1;e_2\}$ as $\{(e_{1x},e_{1y});(e_{2x},e_{2y})\}$ . I have the ""suspicion"", and I could not find any counterexample, that the inverse of the matrix above can be written as $$
\begin{bmatrix}
e_{1x}^2+e_{2x}^2 & e_{1x}e_{1y}+e_{2x}e_{2y}\\
e_{1x}e_{1y}+e_{2x}e_{2y} & 
e_{1y}^2+e_{2y}^2 
\end{bmatrix}
$$ for any orthonormal basis $\{e_1;e_2\}$ . I tried to set up the equalities that derive from orthogonality and normality of the various vectors and arrange them in a identity matrix, but I am stuck there and I can't find any way to extraxt an inverse of my $EFFG$ matrix. I feel there may be a straightforward way to prove this (or I am wrong upfront). What could be a method to prove this?
Also, how would this extend to $n>2$ ? thanks","['matrices', 'linear-algebra', 'orthonormal']"
4705378,Smallest value of $d$ such that $\frac{\left(\sum_{i=1}^d i^{-p}\right)^2}{\sum_{i=1}^d i^{-2p}}\ge \frac{1}{2}\frac{\zeta(p)^2}{\zeta(2p)}$,"I'm looking for $d^*$ , the smallest value of $d$ such that the following holds: $$\frac{\left(\sum_{i=1}^d i^{-p}\right)^2}{\sum_{i=1}^d i^{-2p}}\ge \frac{1}{2}\frac{\zeta(p)^2}{\zeta(2p)}$$ with $\zeta$ referring to Riemann Zeta function . Empirically this seems to scale as the function below, why? $$d^*\approx c_1 \exp\left(\frac{c_2}{p-1}\right)$$ Notebook Edit Mathematica user found a way to express $c_1$ and $c_2$ in terms of Euler gamma constant and $\sqrt{2}$ Motivation: $\frac{\zeta(p)^2}{\zeta(2p)}$ gives ""effective rank"" of infinite-dimensional linear regression problem with eigenvalues $1^{-p},2^{-p},\ldots$ (ie, how many observations are needed for prediction). This question gives $d$ such that finite-dimensional truncation after $d$ terms has similar effective rank.","['riemann-zeta', 'statistics', 'sequences-and-series', 'real-analysis']"
4705389,"In a finite field, can all of the $\mathbb{F}_p$-translates of a square also be square?","Let $F$ be a finite field of $q = p^d$ elements, and suppose $\alpha\in F^\times$ is a square which generates $F$ over $\mathbb{F}_p$ . I'm interested in understanding to what extent it is possible that $\alpha,\alpha+1,\ldots,\alpha+(p-1)$ are all squares. This is trivially true for $p = 2$ , and false if $d = 1$ , and $p \ge 3$ . Can it happen for arbitrarily large primes $p$ that you can find $\alpha$ such that $\alpha,\alpha+1,\ldots,\alpha+(p-1)$ are all squares?","['number-theory', 'abstract-algebra', 'algebraic-number-theory']"
4705441,Disconnectedness in the subspace topology again,"Here is a question that is likely somewhat known (in the vein of Counterexamples in Topology). It comes from a past PhD qualifying exam. The question is basically about the situation where $X$ is a topological space and $Y$ is a disconnected subspace (in the subspace topology). In this case, combining the definitions, one can find two open sets $U$ and $V$ in $X$ such that $Y\subset U\cup V$ , $U\cap V\cap Y=\emptyset$ and $Y\cap U$ and $Y\cap V$ both non-empty. The general question is when can one expect $U$ and $V$ to be disjoint, rather than $U\cap V\cap Y$ to be empty. Specifically, If $X$ is compact Hausdorff and $Y$ is a disconnected subspace, can one find disjoint open subsets $U,V$ of $X$ such that $Y\subset U\cup V$ and $Y\cap U$ and $Y\cap V$ are both non-empty? This question gives a negative answer if one only requires $X$ to be Hausdorff, and states the fact that if $X$ is metrizable there is a positive answer. I do see a proof in one additionally assumes that $Y$ is a closed subspace of $X$ .","['general-topology', 'connectedness']"
4705452,Index Notation - An Inconsistency with a deeper meaning?,"Background Suppose we have a linear transformation $T : V \to W$ where $V$ and $W$ are finite dimensional vector spaces with bases $(e_i)$ and $(f_i)$ respectively. Using index notation, it is tempting to define the components of $T$ with respect to the bases $(e_i)$ and $(f_i)$ as $$
T(e_i) = {T_i}^j f_j
$$ If we carry out the application of $T$ to $v = v^i e_i$ , we obtain $$
T(v) = T(v^i e_i) = v^i T(e_i) = v^i {T_i}^j f_j
$$ So far so good. All the indices are matching up nicely. But consider the composition with $S : W \to U$ where $U$ has basis $(g_i)$ . We have $$
(S \circ T)(v) = S(v^i {T_i}^j f_j) = v^i {T_i}^j S(f_j) = v^i {T_i}^j {S_j}^k g_k
$$ from which we conclude $$
{(S \circ T)_i}^k = {T_i}^j {S_j}^k
$$ Oh no! It looks like passing to components is a contravariant functor. Not a disaster, but certainly unexpected. This is ""unexpected"" because this is not the usual way to define the components of a linear transformation with respect to a basis. Usually, one defines $$
T(e_i) = {T^j}_i f_j
$$ Defined in this usual way, passing to components is a covariant functor, but we had to write down the somewhat ugly and less obvious expression ${T^j}_i f_j$ . I call this ugly because the $i$ is between the two $j$ 's with this convention. We can make this look a little less ugly if we agree to write basis elements to the left, i.e. $T(e_i) = f_j {T^j}_i$ . That looks a bit better, but it certainly isn't the usual convention (I've seen it in one textbook: ""The Geometry of Physics"" by Frankel). This convention also runs into problems when we want to think of vectors as differential operators, since $e_i v^i f$ looks like it should mean $e_i (v^i f)$ , but it actually means $v^i (e_i f)$ . It seems like no matter what we do, we have to deal with some ugliness in how we define passing to components with index notation. Question Is there significance to the fact that the most ""obvious"" (I suppose a matter of opinion) way to write down the indices for a linear transformation makes passing to components a contravariant functor? Or maybe more to the point: is there significance to the fact that no matter what we do we have to deal with some ugliness / non-obviousness in how we pass to index notation, especially when we want to start thinking of vectors as differential operators? I've run into this sort of thing before and concluded that the problem comes about because we apply functions to the left and not the right (this is, for example, why we have to read commutative diagrams ""backward"" when writing down the identities they imply). But it seems like there's a deeper problem here. Apologies if this is an extremely pedantic question, but it's something that's been nagging me for some time now. Hopefully the question is clear enough.","['index-notation', 'linear-algebra', 'vector-spaces', 'differential-geometry']"
4705464,"If P and Q can never be true, are they still ""necessary"" and ""sufficient"" for each other?","If we are given that propositions P and Q can never be true, is it still accurate to say that P and Q are necessary and sufficient for each other, and why? I am conflicted here, as the statement P iff Q is true here and I have learned that this also means P and Q are necessary and sufficient for each other (for context, I have learnt that P is necessary for Q if P must be true to conclude Q is true, and that P is sufficient for Q if Q must be true to conclude that P is true); however, it seems counterintuitive to stipulate conditions for propositions being true (by deeming other propositions as necessary or sufficient for them) if these are never relevant to the situation, as these propositions are always false anyway. I would appreciate an answer grounded in an explanation of necessity and sufficiency.","['propositional-calculus', 'logic', 'discrete-mathematics']"
4705488,"Proving T is self-adjoint iff $\left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R}$","I'm struggling with proving $\left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle \iff \left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R}$ for all $x,y \in H$ where $H$ is a Hilbert space over $\Bbb{C}$ , and $T$ is a bounded linear operator from $H$ to $H$ . I've proven $\left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle \implies \left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R}$ as follows: $$\left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle$$ $$\left \langle Tx,y \right \rangle=\overline{\left \langle Ty,x \right \rangle}$$ So $Im(\left \langle Tx,y \right \rangle)=-Im(\left \langle Ty,x \right \rangle)$ , so the imaginary parts of $\left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle$ cancel out and the result is a real number. For the other direction $\left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R} \implies \left \langle Tx,y \right \rangle=\left \langle x,Ty \right \rangle$ , I've gotten this far: $$\left \langle Tx,y \right \rangle + \left \langle Ty,x \right \rangle \in \Bbb{R}$$ $$\left \langle Tx,y \right \rangle + \overline{\left \langle x,Ty \right \rangle} \in \Bbb{R}$$ $$Im\left \langle Tx,y \right \rangle=-Im\overline{\left \langle x,Ty \right \rangle}$$ $$Im\left \langle Tx,y \right \rangle=Im{\left \langle x,Ty \right \rangle}$$ So I've proved $\left \langle Tx,y \right \rangle$ and $\left \langle x,Ty \right \rangle$ have the same imaginary parts but I'm stuck on proving they have the same real parts. I've been substituting the polarization identity and parallelogram law, equating the imaginary parts, etc, for a while, but haven't gotten anything useful. Any sort of push in the right direction would be hugely appreciated. Thank you :)","['hilbert-spaces', 'inner-products', 'linear-algebra']"
4705490,"$f$ is continuous on $[0, 1]$, $g(0)=1, g(1)=0$, and $f+g$ is monotonic increasing. Prove: $g$ can take any value between $[0, 1]$.","Function $f$ is continuous on $[0, 1]$ . Function $g$ is defined on $[0, 1]$ and $g(0)=1, g(1)=0$ . Function $f+g$ is monotonic increasing. Prove: $g$ can take any value between $[0, 1]$ . My attempt: Since $f+g$ is monotonic, then $f(0)+g(0)\le f(1)+g(1)\Rightarrow f(0)+1\le f(1)$ . We also have: $$f(0)+1\le f(x)+g(x)\le f(1)~~~~~\forall x\in [0,1]$$ move terms, $$f(0)+1-f(x)\le g(x)\le f(1)-f(x)$$ since $f$ is continuous, it can take any value between $[f(0), f(1)]$ , the LHS can take any value between $[f(0)+1-f(1), 1]$ , and $[0,1]\subseteq [f(0)+1-f(1), 1]$ . The RHS can take any value between $[0, f(1)-f(0)]$ , and $[0,1]\subseteq [0, f(1)-f(0)]$ . So for any value $a\in [0,1]$ , there exists $x_1$ , such that $a\le g(x_1)$ .Also there exists a point $x_2$ such that $g(x_2)\le a$ . I am stuck here. I can't conclude $x_1=x_2$ . How to proceed next? Thank you a lot.","['monotone-functions', 'analysis', 'real-analysis', 'continuity', 'functions']"
4705491,"Does the equality $\|\nabla u\|^{p}_{L^{p}(\mathbb{R}^{n})}=[u]^{p}_{W^{s,p}(\Omega)}$ hold in the fractional Sobolev space?","I have been reading the Hitchhiker’s guide to the fractional Sobolev spaces and there they define the fractional Sobolev space and its norm in the following way. $$W^{s,p}(\Omega):=\Biggl\{ u\in L^{p}(\Omega):(x,y)\mapsto\frac{|u(x)-u(y)|}{|x-y|^{\frac{n}{p}+s}}\in L^{p}(\Omega\times\Omega)\Biggl\},$$ and $$\|u\|^{p}_{W^{s,p}(\Omega)}:=\int_{\Omega}|u|^{p}dx+\int_{\Omega}\int_{\Omega}\frac{|u(x)-u(y)|^{p}}{|x-y|^{n+sp}}\,dx\,dy=\|u\|^{p}_{L^{p}(\Omega)}+[u]^{p}_{W^{s,p}(\Omega)}.$$ Where $[u]_{W^{s,p}(\Omega)}$ is the Gagliardo semi-norm.
I understand the definition of both the norm and the space, my problem arises in the Proposition 2.2 specifically in the ecuation (2.6), where the next inequality is given. $$C_{1}(n,s,p)\|\nabla \overset{\backsim}{u}\|^{p}_{L^{p}(\mathbb{R}^{n})}
              \leq C_{1}(n,s,p)\| \overset{\backsim}{u}\|^{p}_{W^{1,p}(\Omega)}.$$ I asked a profesor and he gave me this $$C_{1}(n,s,p)\|\nabla \overset{\backsim}{u}\|^{p}_{L^{p}(\mathbb{R}^{n})}
              \leq C_{1}(n,s,p)\biggl(\|\nabla \overset{\backsim}{u}\|^{p}_{L^{p}(\mathbb{R}^{n})}+\| \overset{\backsim}{u}\|^{p}_{L^{p}(\mathbb{R}^{n})}\biggl)=C_{1}(n,s,p)\| \overset{\backsim}{u}\|^{p}_{W^{1,p}(\Omega)}.$$ Stating that the term $\|\nabla \overset{\backsim}{u}\|^{p}_{L^{p}(\mathbb{R}^{n})}+\| \overset{\backsim}{u}\|^{p}_{L^{p}(\mathbb{R}^{n})}$ is exactly the definition of the fractional Sobolev norm, but as stated above the definition would be $\|\overset{\backsim}{u}\|^{p}_{L^{p}(\Omega)}+[\overset{\backsim}{u}]^{p}_{W^{s,p}(\Omega)}$ . Does that mean that the equality $\|\nabla \overset{\backsim}{u}\|^{p}_{L^{p}(\mathbb{R}^{n})}=[\overset{\backsim}{u}]^{p}_{W^{s,p}(\Omega)}$ holds? or is it that there is another definition for the norm in the fractional Sobolev space? P.D. I tried to prove the equality and I could not achive anything.","['fractional-sobolev-spaces', 'functional-analysis', 'fractional-calculus']"
4705518,"$P$ is a point in $\triangle ABC$, with $\angle PBC=\angle PCB=24^\circ$, $\angle ABP=30^\circ$, $\angle ACP=54^\circ$. What is the $\angle BAP$?","Let $P$ be a point inside a triangle $\triangle ABC$ with $\angle PBC=\angle PCB=24^\circ$ , $\angle ABP=30^\circ$ and $\angle ACP=54^\circ$ . What is the $\angle BAP$ ? I can calculate $\angle BAP=18^\circ$ by assuming $BC=1$ and calculating all the lengths involved in the picture using formula of sines and cosines. But since the answer is an integer $18^\circ$ , I believe there must be a more elementary ways (i.e. without using sines/cosines or invloving irrational numbers) to obtain the answers. Any suggestions are welcome! Thank you in advance!","['euclidean-geometry', 'geometry']"
4705580,"Finding a conditionally convergent series of functions in $C[0,1]$ with supremum norm w.r.t Faber-Schauder system.","Let, \begin{align*}
        \alpha(x) &= 1 \\
        \beta(x) &= x\\
        s_{n,k}(x) &= \max\{1-|2(2^nx-k)-1|, 0\} \text{   for  } 0 \le n \text{ and } 0 \le k \le 2^{n}-1
    \end{align*} We define the Faber-Schauder system \begin{equation*}
    S := \{ \alpha, \beta \} \cup \{ s_{n,k} \}_{n \ge 0, 2^{n}-1 \ge k \ge 0 }
\end{equation*} $s_{n,k}$ denotes the hat function supported on the interval $[\frac{k}{2^n},\frac{k+1}{2^n}]$ and whose peak is 1 and attained at $\frac{2k+1}{2^{n+1}}$ . S is a basis for $C[0,1]$ with the supremum norm. It is known that any basis for $C[0,1]$ is conditional. I am trying to find a concrete example of a series of functions which converges conditionally. My strategy is to use some sort of linear combinations of the basis functions i.e, $g_M = \sum_{n = 0}^{M} \sum_{k = 0}^{2^{n}-1} c_{n,k} s_{n,k}$ . I will show that $g_M$ uniformly converges(show that it is uniformly cauchy). Then I show that $(-1)^n g_M$ doesn't uniformly converge(ideally goes to infinity). Vice versa is fine too, i.e, $(-1)^n g_M$ uniformly converges and $(-1)^n g_M$ doesn't uniformly converge. This shows that $g_M$ (or $(-1)^n g_M$ ) is conditionally convergent. Or maybe, I could use another characterization of conditional convergence. I tried couple of linear combinatons to construct $g_M$ . For example, $g_M = \sum_{n = 0}^{M} s_{n,0}$ which didn't work.  Another example is constructed by firs considering interval $[0,1]$ and its corresponding function $t_0 = s_{0,0}$ then diveding it by half and taking the left part, $[0,\frac{1}{2}]$ , which corresponds to $t_1 = s_{1,0}$ then divide half and take the right part, $[\frac{1}{4},\frac{1}{2}]$ , which correspond to $t_2 = s_{2,1}$ and so on. We always halve the current interval and alternate between taking the left part and right part with its corresponding function. $g_M = \sum_{n=0}^{M} t_n$ .In this case, $g_M$ goes to infinity which is good. But I don't think $(-1)^ng_{M}$ uniformly converges. I am looking forward for your ideas.","['metric-spaces', 'real-analysis', 'conditional-convergence', 'functional-analysis', 'sequences-and-series']"
4705629,How do mathematicians work with non-linear equations?,"I'm asking this as an absolute beginner (second year undergrad student). Suppose I write an equation out of the blue such as $$\log(\tan(\sin x + 23e^{2x+83} +\tan x))= 1000$$ , which is non-linear, how would a mathematician attempt to solve it? I am encountering subjects which get more and more abstract(linear algebra, group theory) and I wonder if they are applicable to such problems.","['trigonometry', 'linear-algebra']"
4705655,Is a projected subspace equivalent to a truncated subspace?,"I am from physics with humble mathematical background so apologies if you find this trivial. Given a vector $v=\begin{pmatrix} a \\ \alpha \\ b \\ \beta \end{pmatrix}$ that lives in $\mathbb{R}^{4\times 1}$ , I define a projector $P$ with $P_{11}=P_{33}=1$ and all other elements zero, such that $u:=Pv = \begin{pmatrix} a \\ 0\\ b \\ 0 \end{pmatrix}$ . My question is whether the following replacement is true: $$\begin{pmatrix} a \\ 0\\ b \\ 0 \end{pmatrix} \overset{?}{\equiv} \begin{pmatrix} a \\   b   \end{pmatrix}=:w$$ Since $u$ and $w$ belong to different spaces, to me it seems that this equivalence is not true. I ""invented"" the word truncated for the vector $w$ , only to help me frame the question title. Also, can a similar thing be said about the matrices, e.g. $$ P \begin{pmatrix} a_{11} & a_{12} & a_{13} & a_{14} \\
                  a_{21} & a_{22} & a_{23} & a_{24}\\
                 a_{31} & a_{32} & a_{33} & a_{34}\\
                 a_{41} & a_{42} & a_{43} & a_{44}   \end{pmatrix}P = \begin{pmatrix} a_{11} & 0 & a_{13} & 0 \\
                  0 & 0 & 0 & 0\\
                 a_{31} & 0 & a_{33} & 0\\
                 0 & 0 & 0 & 0   \end{pmatrix} \overset{?}{\equiv} \begin{pmatrix}  a_{11} & a_{13} \\ a_{31} & a_{33} \end{pmatrix}$$ EDIT: Thanks to the comment by @ronno and answer by @Nils. What I mean by the ""equivalence""  is the that expectation values should be the same in the two representations.","['projection', 'linear-algebra', 'linear-transformations', 'projective-space']"
4705678,What exactly is a universal central extension?,"Let $G$ be a group. In An introduction to homological algebra , Chapter 6.9 Weibel defines a universal central extension as a central extension $$0 \to A \to X \to G \to 1,$$ which is initial with respect to all central extensions. In other words, if $0 \to B \to Y \to G \to 1$ is any other central extension, then there is a unique homomorphism $X \to Y$ which is compatible with the two maps to $G$ . The first basic lemma (6.9.2) Weibel proves is that if there exists a universal central extension, then both $X$ and $G$ are perfect (i.e. $[G,G] = G$ ). Now both Wikipedia and this notes (Exercise 10) claim that the extension $$0 \to \mathbb Z \to \mathcal B_3 \to \operatorname{PSl}_2(\mathbb Z) \to 1$$ is universal. (For the definition of the extension, see my other question .) But that apparently contradicts Weibel's lemma, because the group $\mathcal B_3$ is not perfect, it's abelization is $\mathbb Z$ (by exercise 7 of the above notes). So I guess there is a different notion of universal central extension? This section in Wikipedia seems to also indicate that, but I don't quite understand what they mean, and it seems to mostly care about finite groups? Does that make a difference?","['representation-theory', 'central-extensions', 'abstract-algebra', 'group-cohomology', 'group-theory']"
4705689,How to use Lagrange's multipliers to solve a problem?,"A carpenter wants to manufacture a box with a volume of $1 \text{m}^3$ . The box has sides parallel to the coordinate plane, and the side lengths are given by $x$ , $y$ , and $z$ , where $0 \leq x \leq 10$ , $0 \leq y \leq 10$ , and $0 \leq z \leq 10$ . The front and top sides of the box will be made of an expensive type of wood that costs $900$ per $\text{m}^2$ , while the bottom and other sides will be made of a cheaper type of wood that costs $300$ per $\text{m}^2$ . The carpenter wants to choose the side lengths to minimize the cost. a) Formulate the carpenter's problem mathematically as a minimization problem with constraints. b) Use the Lagrange multiplier method to solve the minimization problem in part (a) of the question. No points will be awarded for using other methods. A) : $$f(x,y,z) = 1200xz+1200xy+600yz$$ $$g(x,y,z) = xyz = 1$$ My expression of the the function was correct but when it comes to the constraints there is something wrong. B) Here I used my constraint and function and used Lagrange multipliers to get: $$x= 2^{-\frac23}$$ $$y= 2^{\frac13}$$ $$z= 2^{\frac13}$$ $$f(x,y,z) = 1800 \times 2^{\frac23} $$ So far I'm correct and I thought it was the answer but they continued in the answers They said: Here we have to check edge points, i.e. $x = 10, y = 10 or z = 10. If x = 10$ , then we get
minimization problems. After this they did a computation that took over one page to complete but I couldn't understand anything because I'm totally confused. Why aren't we done where I was ""done""? What is my constraint to begin with?
What is the point of $0 \leq x \leq 10$ , $0 \leq y \leq 10$ , and $0 \leq z \leq 10$ ?","['lagrange-multiplier', 'multivariable-calculus', 'calculus', 'partial-derivative', 'optimization']"
4705703,Evaluate $\int_{-\infty}^\infty \frac{\sin{(x\sin{x})}}{x^2}dx$.,"Evaluate $$\int_{-\infty}^\infty \frac{\sin{(x\sin{x})}}{x^2}dx.$$ Numerical investigation suggests that it equals $\pi$ . Let $f(\alpha)=\frac{1}{\pi}\int_{-\alpha}^\alpha \frac{\sin{(x\sin{x})}}{x^2}dx$ . Wolfram (free version) gives me the following results: $f(10)=0.992293$ $f(20)=0.996916$ $f(30)=0.998604$ $f(100)=0.999761$ $f(200)=0.999916$ $f(300)=0.999934$ But with even larger values of $\alpha$ , Wolfram behaves erratically: $f(1\times10^5)=2.12907$ $f(2\times10^5)=0.574468$ $f(3\times10^5)=0.351411$ And finally, Wolfram gives: $f(\infty)=0.998775$ But in the comments, @NN2 said that the paid version of Wolfram gives some detailed warnings about the accuracy of this last result. Perhaps someone with more reliable computing power could shed some light on $f(\infty)$ . My attempt I tried to use some of the techniques for proving $\int_0^\infty {\sin{x}\over x}dx=\pi/2$ , to no avail. For example, this one sets up $I(a)=\int_0^\infty e^{-ax}\frac{\sin x}{x}dx$ , then evaluates $I'(a)$ via integration by parts. If we just have $\sin{x}$ this is easy, but the fact that $\int_0^\infty{\sin{(x\sin{x})}\over x^2}dx$ involves a sine within a sine, seems to make everything difficult. Context I've been searching (in vain) for a definite integral with no numbers that equals $\pi/7$ , and I stumbled upon this integral. I was surprised that it seems to admit a closed form, $\pi$ .","['improper-integrals', 'wolfram-alpha', 'definite-integrals', 'complex-analysis', 'calculus']"
4705712,Uniqueness of the square root for non-symmetric definite matrices,"When $A$ is positive semi-definite, the positive semi-definite square root $B$ , which is $BB=A$ is unique. I have heard that this holds true for a symmetric matrix. What about under weaker conditions? In particular, is it true when $A,\ B$ may not be symmetric but $x^TAx>0,\ x^TBx>0$ ? or may not be symmetric and may not be $x^TAx>0,\ x^TBx>0$ but all eigenvalues are positive? It's hard for me to think of a way to prove. For example, I guess this is true for the normal matrix of real numbers such as $$A=\begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 1& 0 & 1\end{pmatrix},\ B=\frac{1}{3}\begin{pmatrix}\sqrt{2}+\sqrt{3} & \sqrt{2} & \sqrt{2}-\sqrt{3}\\ \sqrt{2}-\sqrt{3} & \sqrt{2}+\sqrt{3}& \sqrt{2} \\\sqrt{2}  &\sqrt{2}-\sqrt{3} & \sqrt{2}+\sqrt{3}\end{pmatrix}$$","['positive-semidefinite', 'matrices', 'linear-algebra', 'radicals', 'positive-definite']"
4705758,find the area of five quadrilaterals and one triangle,"In the figure below, all line segments between two distinct points are of length 2, except line segments involving I,H, or J are not necessarily 2. Also, $\angle FHE = \angle GIA = 90^\circ$ . Find the areas of quadrilaterals $FJHE, GIAJ, BIGC,GJFC, FHDC$ and of triangle $AJE$ . I know how to find the area of the entire pentagon, and I think the three quadrilaterals $BIGC, GJFC, FHDC$ are congruent. I also think the two other quadrilaterals are congruent. Also, I think $\angle FCD = 30^\circ$ . One can probably find a lot of angles via angle-chasing. For instance, $\angle AJE = 90^\circ$ . But then if I assume $\angle GED = 60^\circ$ and try to apply the Pythagorean theorem twice in triangles $FJE$ and $FEH$ , I get a contradiction. I'm not sure what I'm doing wrong.","['contest-math', 'area', 'geometry']"
4705802,Double summation with binomial coefficients,"Question : Find the value of the following expression : $$ \frac{\sum_{i=0}^{2024}\sum_{r=0}^{2024}(-1)^r{2024 \choose r}(2024-r)^i}
{\sum_{r=0}^{2025}(-1)^r\binom{2025}{r}(2025-r)^{2025}} $$ I am not able to think of a way to manipulate and solve this algebraically , so I decided to test this on smaller summation boundaries to find a pattern in the answers  For ex : $$ \frac{\sum_{i=0}^{2}\sum_{r=0}^{2}(-1)^r{2 \choose r}(2-r)^i}
{\sum_{r=0}^{3}(-1)^r\binom{3}{r}(3-r)^{3}} $$ On  substituting 2024 with 2 and 2025 with 3 and solving the resultant expression (summing each case) I got the answer 2/6 which reduced to 1/3 . Here I observed that 3 is the upper boundary for the summation in the denominator , so I checked for 2 more cases which turned out to be quite tedious but : 2024 --> 3 and 2025 --> 4 Answer - 6/24 --> 1/4 2024 --> 4 and 2025 --> 5 Answer - 24/120 --> 1/5 A pattern is formed where the reciprocal of the upper boundary of the summation in denominator is the answer . There is also the pattern of factorials where 2 is factorial of 2 , 6 is of 3 , 24 is of 4 and 120 is of 5 . So I can guess the answer to my original question to be 1/2025 following the pattern , which also turns out to be the correct answer
But this is definitely not how the question was intended to be solved , so I need a way to solve this with just algebraic manipulations in mind. There could be a way to generalize this as : $$ \frac{\sum_{i=0}^{k}\sum_{r=0}^{k}(-1)^r{k \choose r}(k-r)^i}
{\sum_{r=0}^{k+1}(-1)^r\binom{k+1}{r}(k+1-r)^{k+1}} = \frac{k!}{(k+1)!}=\frac{1}{k+1} $$","['algebra-precalculus', 'binomial-coefficients', 'binomial-theorem', 'summation']"
4705860,"A Lagrange optimisation that I don’t understand. $f(x, y, z) = x^2 + y^2$ is constrained by $5x^2 + 6xy + 5y^2 = 1$","The function $f(x, y, z) = x^2 + y^2$ is constrained by $5x^2 + 6xy + 5y^2 = 1$ . Using Lagrange, maximise and minimise this function. My original Lagrange equation was $$L = x^2 + y^2 + \lambda(1 - 5x^2 - 6xy - 5y^2).$$ After partially differentiating I got: $$\begin{aligned}
L_x &= 2x - 10\lambda x - 6\lambda y ,\\
L_y &= 2y - 6\lambda x - 10\lambda y , \\
L_\lambda &= 1 - 5x^2 - 6xy - 5y^2 .
\end{aligned}$$ As I’m attempting to find a minimum and maximum, I set all the equations equal to $0$ . Rearranging the partial derivative w.r.t. $x$ gave me $$\lambda = \frac{2x}{10x + 6y}$$ and rearranging the partial derivative w.r.t. $y$ gave me $\lambda = 2y/(10y + 6x)$ . Putting these two equations equal to one another gives $$\frac{2y}{10y + 6x} = \frac{2x}{10x +6y}.$$ Multiplying both sides out led to $20xy + 12y^2 = 20xy + 12x^2$ . As such I could equate $x$ to $y$ . Subbing in $x$ for $y$ into the constraint gave me $16x^2 = 1$ and as such $x$ (and $y$ ) $= \pm \frac14$ . However, I don’t think either $\left(\frac14, \frac14\right)$ or $\left(-\frac14, -\frac14\right)$ are the minimum and maximum points. If I have done something wrong please correct me but as of right now I am unsure what to do. Also, $z$ being listed in the function but actually having no part in any of the question confused me too. Was just hoping someone could help me understand.","['optimization', 'multivariable-calculus', 'qcqp', 'lagrange-multiplier']"
4705895,Coclosed form is sum of coexact and harmonic form.,"Let $(\mathcal{M},g)$ be a compact and connected Riemannian manifold, $\mathrm{d}$ and $\delta$ differential and codifferential, respectively, and $\Delta:=\delta\mathrm{d}+\mathrm{d}\delta$ the corresponding de Rham-Hodge Laplacian. Is it true that $$\mathrm{ker}(\delta)=\mathrm{ran}(\delta)\oplus\mathrm{ker}(\Delta)$$ Direction "" $\supset$ "" is clear, since $\delta^{2}=0$ and every harmonic form is both closed and coclosed. However, I am struggeling with the other direction. Let $\omega\in\mathrm{ker}(\delta)$ . My idea was to use condradiction: Assume $\omega\notin\mathrm{ran}(\delta)\oplus\mathrm{ker}(\Delta)$ . Is it possible to argue by Hodge decomposition that then $\omega\in\mathrm{ran}(\mathrm{d})$ ? Because then the claim follows, since $\omega$ would be both closed and coclosed and hence in particular harmonic. EDIT: At least for $1$ -forms, it should be true: Take $A\in\Omega^{1}(\mathcal{M})$ such that $\delta A=0$ . By Hodge decomposition, $A=\mathrm{d}f+\delta F+h$ for $h$ harmonic. Then $\delta A=\Delta f=0$ and hence $f=\mathrm{const}$ , since any harmonic $1$ -form on a compact manifold is necessarily constant. It follows that $A=\delta F+h\in\mathrm{ran}(\delta)\oplus\mathrm{ker}(\Delta)$ .","['hodge-theory', 'differential-topology', 'riemannian-geometry', 'differential-geometry']"
4705908,Generating residues with $ a^n + b^n \mod p $,"Say there exist some non-zero distinct residues $a,b$ such that $$ a^n + b^n \mod p $$ generates all nonzero residues for some $n$ . Does such a pair $a,b$ exist for every odd prime $p > 13$ ? Or for all sufficiently large prime $p$ ? And if so, how many pairs exist as a function of $p$ ? I consider $3$ cases : $a$ is a primitive root, $b$ is not (or visa versa by symmetry) Neither $a,b$ are primitive roots. Both $a,b$ are primitive roots. And I would like some insight about these cases. edit How about $a^n + b^n = a^n + (ac)^n = a^n(1 + c^n) \mod p$ ?
Does that help ?
Im not certain what and if "" factoring "" helps. edit 2 To be clear it is not forbidden that $a^n + b^n = 0$ but it must generate all nonzero residues for some $n$ .
And this does not violate pigeonhole principles.
See my "" answer "" that is more of a comment. $n$ can go from $0$ to $p$ , after that it repeats by fermat's little rule. edit 3 It is clear from the ""answers and comments"" that our best bet for having a solution seems to be the primes $p>13$ such that $(p-1)/2$ is also a prime.
These primes are called Sophie Germain primes. ( for number theory fans, I noticed something weird about those primes , see : Why does this ratio $5$ occur relating prime twins and sophie germain primes? , see also https://en.wikipedia.org/wiki/Safe_and_Sophie_Germain_primes ) edit 4 ok based on some mistakes, edit 3 was exgaggerated. $p = 4 m + 3$ are already very good candidates. So perhaps we should look at those in particular $3,7,11,19,23,...$","['number-theory', 'representation-theory', 'modular-arithmetic', 'prime-numbers']"
4705912,Zero subscheme and zero locus,"On this page on StacksProject about coherent sheaves on projective spaces they define a hypersurface to be the zero scheme $Z(s)$ of a global section $ s \in \Gamma(\mathbb{P}_k^n,\mathcal{O}(d))$ , which is defined here . I know that those global sections can be identified with homogeneous polynomials of degree $d$ in $k[X_0, \ldots, X_n]$ , so let $f_d$ be the homogeneous polynomial corresponding to $s$ . My question is, does this zero scheme coincide with the closed subscheme $V_{+}(f_d)=Proj(k[X_0, \ldots,X_n]/(f_d)) $ ? This would make sense since this last scheme is sometimes called ""zero locus"", but I am not able to show this from the abstract definition of $Z(s)$ in stacks project. I know that $Z(s)$ is, by definition, a closed subscheme of $\mathbb{P}_k^n$ , so it must be of the form $Proj(k[X_0, \ldots, X_n]/I)$ for some homogeneous ideal $I$ , but I do not know how to show $I=(f_d)$ . EDIT: In fact, the case $d=1$ is showed here .","['algebraic-geometry', 'projective-space']"
4705923,Help - calculation of a multivariable limit,"Can we prove that $  \displaystyle \lim_{(x,y) \to (0,0)}\frac{x^2 y^2(y-x)}{x^3+y^3}=0$ without using $\epsilon - \delta$ definition? My original idea was to use polar coordinates: $x=r\cos\theta$ , $y=r\sin\theta$ . Then : $$\lim_{ r\to 0^+}\frac{r^5( \cos^2\theta \sin^2\theta)(\sin\theta -\cos\theta)}{r^3(\cos^3\theta+\sin^3\theta)} = \lim_{r \to 0^+} r^2\frac{( \cos^2\theta \sin^2\theta)(\sin\theta -\cos\theta)}{\cos^3\theta+\sin^3\theta}.$$ Now i think the problem is that the expression inside the limit isnt bounded ( for $\theta= -\pi/4 \rightarrow \infty$ ).","['limits', 'multivariable-calculus']"
4705996,Markov processes associated with generators being differential operators of higher order $n>2$,"Let $P$ be a $C_0$ -semigroup on the space of all bounded Borel functions on $\Bbb R^n$ with $P_t1= 1$ for all $t$ . It corresponds to a continuous time Markov process $X$ such that $\mathbb E(f(X_t)|X_0 = x) = P_tf(x)$ . A generator of $P$ is given by $$
Af(x):=\lim_{t\to 0}\frac{P_tf(x) - f(x)}t.
$$ From what I know if $A$ is a first order differential operator, $X$ is a solution of an ODE if $A$ is a second order differential operator, $X$ is an Ito diffusion if $A$ has an integral term, $X$ has jumps. Yet, I have never saw an example of what happens when $A$ is a differential operator of an order higher than $2$ . Which kind of stochastic process will that correspond to? I'd be happy to see an example for $A = \frac{d^3}{d^3x}$ and $A = \frac{d^4}{d^4x}$ .","['stochastic-processes', 'probability-theory', 'functional-analysis']"
4706005,"Understanding the ""abuse of notation"" in the differential of tangent vectors","I am reading John Lee's Smooth Manifolds book, current looking at the bottom of Page 63 in which we are working out what the differential looks like in the special case that it's along the transition map between two charts. Let $M$ be a smooth manifold and and $(U,\psi)$ and $(V,\phi)$ be two smooth coordinate charts with coordinate functions $(x^i)$ and $(\tilde x^i)$ respectively. It goes on to mention that we are engaging in a typical abuse of notation by writing the transition map as follows: $$\phi\circ\psi^{-1}(x):=(\tilde x^1(x),...,\tilde x^n(x)).$$ It then mentions that here we are thinking of the $\tilde x^i$ in $\tilde x^i(x)$ as a coordinate function, but $x$ as representing a point. I want to make sure I have understood what exactly about this is abuse of notation (my background is physics so I am no stranger to abuse of notation). I want to say that we are doing two things that are ""questionable notation"" here, the first is that (as the author mentions) before, the letter $x$ would have represented the coordinate functions on $U$ but now represent a point. And also that $\tilde x^j$ is no longer a function $$\tilde x^j:V\rightarrow \Bbb R^N$$ but instead the different function $$\tilde x^j:\psi(U)\rightarrow \phi(V).$$ Which leads on to me now looking at the final equation we end up with (which is much more recognisable to a physicist), $$\frac{\partial \tilde x^{j}}{\partial x^{i}}(\psi(p)) \frac{\partial}{\partial \tilde x^{j}}|_{\psi(p)},$$ in which $\tilde x^k$ is now simultaneously two objects in the same expression. in the first ""half"" of the expression, $\frac{\partial \tilde x^j}{\partial x^i}(\psi(p))$ , it is the transition map, and in the second ""half"", $\frac{\partial}{\partial \tilde x^j}\biggr|_{\psi(p)}$ , it is the (strictly completely different) coordinate function on $V$ . My question is, is my reasoning above correct? Thanks in advance :)","['coordinate-systems', 'pushforward', 'tangent-spaces', 'smooth-manifolds', 'differential-geometry']"
4706013,Integral shrinks with measure of the set,"I would like to prove the following: My idea is to pass to a function $f$ in $L^1$ that bounds all the $f_n$ (whose existence is guaranteed because the sequence is Cauchy) and show that the size of the integral of $f$ can be made small by integrating over a sequence of sets with measure going to zero. However, I'm not sure if this is equivalent to solving the above problem. How should I proceed?","['measure-theory', 'real-analysis']"
4706024,What is the simple way to solve the equation $x+\sqrt{a+\sqrt{x}}=a$ for all real $x$?,"Solve the equation $$x+\sqrt{a+\sqrt{x}}=a$$ for all real $x$ and nonzero parameter $a$ , prove that the equation has a solution if and only if $a\ge 1$ . My attempts and thoughts. $$\sqrt {a+\sqrt x}=a-x$$ $$a+\sqrt x=(a-x)^2$$ $$\sqrt x =(a-x)^2-a$$ $$x=\big((a-x)^2-a\big)^2$$ $$x=(a^2-2ax+x^2-a)^2$$ But the last expansion seems terrible. I haven't been able to prove why $a\ge 1$ has to be. My other concern is that not all roots of the last equation may be valid.For example, say $\sqrt {x}=-1$ . We have $x=1$ but $x=1$ doesn't satisfy $\sqrt {x}=-1$ . My questions . Is there a method that doesn't square the equation and works easily in general? How can we get rid of excess roots? How can we prove that $a\ge 1$ ?","['contest-math', 'algebra-precalculus', 'nested-radicals']"
4706121,Patio Tiling Proof,"Call a square grid n squares wide, with a single square removed from the top left, a patio. It appears that the minimum number of squares necessary to tile a patio 2^k squares wide is 3k. This is done by adding 3 larger squares to the bottom right of the patio, each time the patio doubles in width. Can 3k be proven minimal? Here are some patios of width n = 4 through 8, in case my definition of patios was unclear.","['optimization', 'geometry', 'tiling']"
