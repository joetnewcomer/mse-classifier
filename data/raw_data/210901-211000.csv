question_id,title,body,tags
4241644,Derivative of $\sin^{-1}\frac{x+\sqrt{1-x^2}}{\sqrt 2}$,I'm a calculus beginner. I was asked to find the derivative of the function: $$\sin^{-1}\frac{x+\sqrt{1-x^2}}{\sqrt 2}.$$ I'm able to solve it in the following way: I first calculate the derivative of $\frac{x+\sqrt{1-x^2}}{\sqrt 2}$ and get $\frac{1}{\sqrt 2}(1-\frac{x}{\sqrt{1-x^2}})$ . Then the derivative of the given function is $\frac{1}{\sqrt{1-(\frac{x+\sqrt{1-x^2}}{\sqrt 2}})^2}\cdot \frac{1}{\sqrt 2}(1-\frac{x}{\sqrt{1-x^2}})$ . Simplifying this gives the final answer $\frac{1}{\sqrt{1-x^2}}$ . But the simplication process is quite lengthy and involves some bizarre calculations. Is there tricks/ways to solve these kinds of derivatives that do not involve too much calculations like above?,"['calculus', 'inverse-function', 'derivatives', 'trigonometry']"
4241689,Solve differential equation $y'=a \sin(bt-y)$,"I'm looking for a solution to $y'=a \sin(bt-y)$ , where a and b are constants.
I've looked at previous posts were b=1, which leads to nice solutions. I'm looking for a solution when b is an arbitrary constant.
To kick off:
Substitute $u=bt-y$ $u'=b-y'$ -> $y'=b-u'$ . $u'=b-a\sin(u)$ $\int\frac{du}{b-a\sin(u)}=\int dt$ And this is where I'm stuck. Differential/integral equation solvers give me some solutions including tans and arctans. Plotting those solutions does not match my numerical approach.
Maybe it has something to do with the domains of the tan and arctan? I look forward to your help!","['integration', 'ordinary-differential-equations']"
4241691,"ODE: $y''y+ax+by+c=0,y=k\pm\sqrt2\int\sqrt{a\int\ln(y)dx-(ax+c)\,\ln(y)-by+K}dx,\int\frac{dy}{\sqrt{K-(ax+c)\,\ln(y)+a\int\ln(y)dx-by}}=k\pm\sqrt2x$","Imagine we had a differential equation like: $$y’’-\frac xy=0$$ Now let’s standardize the signs. Note we do not need a constant for the first term because of the zero product property. We can generalize any way we want, but this way of adding constants will generalize linearly and intuitively: $$y’’+\frac {ax+c}{y}+b=0\implies y’’y+ax+by+c=0\implies y=y(a,b,c;x)\ne 0$$ where a,b,c are  any constants Notice that we can then do: $$y’’+a\frac {x+\frac ca}{y}+b=0\implies y’’y+a\left(x+\frac ca\right)+by=0$$ It can be shown that an implicit solution for $y=y(0,b,c;x)$ is the function $y(x)$ which satisfies the following functional integral equation. Be careful with the squared terms when solving here: $$y’’y+by+c=0\implies\left(\int_1^{y(x)}\frac{dt}{\sqrt{c_1-2(bt+c\ln(t)))}}\right)^2=(x+c_2)^2$$ This might remind you of an inverse gamma-type function which I did not see coming. Another nice thing is a closed form for $y(0,0,c;x)$ which uses the Inverse Error function : $$y’’y+c=0\implies y(x)=e^{\frac{c_1-2c\left(\operatorname{erf}^{-1}\left(\pm \sqrt{\frac 2\pi}\sqrt{ce^{-\frac{c_1}{c}}(x+c_2)^2}\right)\right)^2}{2c}} \implies \left(\int_1^{y(x)}\frac{dt}{\sqrt{c_1-2c\ln(t))}}\right)^2=(x+c_2)^2$$ Here is what the sample solution family for $y’’y+x+y+1=0\ $ looks like: Here is what the sample solution family for $y’’y-x-y-1=0\ $ looks like: Related problems: Approach Zero Search Results This almost looks like the Airy Differential Equation , but it is not related : $$y’’-xy=0\implies y=\operatorname{Ai}(x)+i\operatorname {Bi}(x)$$ Unfortunately, we cannot use the Principle of Superposition to find a more general solution as the equation is nonlinear. @Eli showed that the following is a particular solution: $$y=-\frac{ax+c}b\ne 0\implies -\frac{ax+c}b \frac{d^2}{dx^2} \frac{-ax-c}b +ax-b\frac{ax+b}b +c=0+ax-ax-c+c=0$$ The problem is that this is not the general solution. One attempt at finding such a general solution will use our aforementioned a=0 using a Generalized Puisex Series , with machine help, at t=0, but the convergence may be a problem. I have listed out a few of many known terms: $$y’’y+by+c=0\implies\left(\int_1^{y(x)}\frac{dt}{\sqrt{k-2(bt+c\ln(t)))}}\right)^2=(x+c_2)^2\implies \int_1^{y(x)}\frac{dt}{\sqrt{k-2(bt+c\ln(t))}}=\int_1^{y(x)}\left(\frac 1{\sqrt{k - 2 c \ln(t)}}+ \frac{b t}{(k - 2 c \ln(t))^\frac32} + \frac{3 b^2 t^2}{(2 (k - 2 c \ln(t))^\frac52)} +O\left(t^3\right)\right) $$ Then one may use an inversion theorem to find the inverse and find $y(x)$ with correct convergence. My question is how to either find a closed form for $y(x)=y(a,b,c;x)$ or a general series representation for the general case without any initial values. Please do not give any implicit solutions as the goal is to find $y(x)$ . You can even use an inversion theorem like the Lagrange Inversion Theorem . Please correct me and give me feedback! Here is a general integral solution for which the main integral we need to put in terms of x. Here is a reference problem for the technique used by @Ron Gordon to solve the general solution and somehow integrate a general $y’y’’$ : Solution to Differential Equation: $y'' = \frac{c_1}{y} - \frac{c_2}{y^2} $ . $$y’’y+ax+by+c=0\mathop \implies^{y\ne0} y’’+a\frac x{y’}+b+ \frac cy=0\implies -y’’y’= a\frac {x y’}{y}+by’+ c\frac {y’}y$$ Now we integrate and use the referenced integration technique along with Logarithmic Differentiation . One also uses Integration by Parts of x and $\frac {y’}{y}$ : $$ \int -y’’y’dx= a\int \frac {x y’}{y}dx+b\int y’dx+ c\int \frac {y’}y dx\implies c_1-\frac{y’^{\,2}}2=ax\,\ln(y)-\int \ln(y) dx+c_2+by+c_3+c\,\ln(y)+c_4\implies y’^{\,2}=-2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)+c_1\implies y’=\pm\sqrt{c_1 -2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)} \implies y=c_2\pm \sqrt 2\int\sqrt{a\int \ln(y)dx -ax\,\ln(y)-by-c\,\ln(y)+c_1} dx$$ Here is another way to solve: $$\frac{y’}{\sqrt{c_1 -2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)} }=\pm1\implies \int \frac{\frac{dy}{dx}}{\sqrt{c_1 -2ax\,\ln(y)+2\int \ln(y)dx-2by-2c\,\ln(y)}}dx=\pm\int dx  \implies \int \frac{dy}{\sqrt{c_1 -2ax\,\ln(y)+2a\int \ln(y)dx-2by-2c\,\ln(y)}}=c_2\pm x\implies \int \frac{dy}{\sqrt{c_1 -ax\,\ln(y)+a\int \ln(y)dx-by-c\,\ln(y)}}=c_2\pm \sqrt2 x$$ This is where I got stuck. If we could integrate, by substitution in terms of x maybe, with a series expansion and use a series reversion, then it should output $y(x)$ explicitly.","['special-functions', 'ordinary-differential-equations', 'integro-differential-equations', 'sequences-and-series', 'error-function']"
4241711,What is $\mathcal{R}$?,"First of all, I am asking this question entirely out of curiosity. It basically randomly popped out of my mind. So I am asking for the value of an infinite series. Let's call it, $\mathcal{R}=\sum_{n=1}^{\infty}\mathcal{R}_n$ Now I will try to explain what are these $\mathcal{R}_i$ 's Let's take a 1-ball of unit length (1-volume). It's radius will be $r_1=\frac{1}{2}$ So the first term of my series is $\mathcal{R}_1=1$ Now I will turn the length of this line into the circumference of a circle or a $2$ -ball. It's radius will be $r_2$ , you can just do a little calculation and $r_2=\frac{1}{2\pi}$ And the area of this circle would be, $\pi r_2^2=\frac{1}{4\pi}$ This will be $\mathcal{R}_2=\frac{1}{4\pi}$ Now let's turn the area of this circle into the surface area of a sphere or a $3$ -ball, It's radius would be $r_3$ Again, $r_3=\frac{1}{4\pi}$ And the volume of this sphere will be, $\frac{4}{3}\pi r_3^3=\frac{1}{48\pi^2}$ This will be, $\mathcal{R}_3=\frac{1}{48\pi^2}$ In general we take an $n$ -ball whose radius is $r_n$ , then we turn the $n$ -volume of this ball into the $n$ -surface area of an $n+1$ -ball whose radius is $r_{n+1}$ . And $\mathcal{R}$ is the sum over the $n$ -volumes of these $n$ -balls with radius $r_n$ . So we can write, $\mathcal{R}=\textstyle\displaystyle\sum_{n=1}^{\infty}V_n(r_n)$ From this we can deduce the formula for $r_n$ Agian in general, we take an $n$ -ball with radius $r_n$ . Then we set it's $n$ -volume to equal the $n$ -surface area of an $n+1$ -ball. And we define the $n+1$ -ball's radius to be $r_{n+1}$ . From this we can calculate a recurrence relation for $r_n$ , $V_n(r_n)=S_n(r_{n+1})$ $\textstyle\displaystyle{\frac{\pi^\frac{n}{2}}{\Gamma(\frac{n}{2}+1)}r_n^n=\frac{2\sqrt{\pi}\pi^\frac{n}{2}}{\Gamma(\frac{n+1}{2})}r_{n+1}^n}$ $\textstyle\displaystyle{r_{n+1}=r_n\sqrt[n]{\frac{\Gamma(\frac{n+1}{2})}{2\sqrt{\pi}\Gamma(\frac{n+2}{2})}}}$ with the initial condition of $r_1=\frac{1}{2}$ If we keep expanding $r_n$ using the formula then we get, $\textstyle\displaystyle{r_n=r_1\prod_{k=1}^{n-1}\sqrt[k]{\frac{\Gamma(\frac{k+1}{2})}{2\sqrt{\pi}\Gamma(\frac{k+2}{2})}}}$ $\textstyle\displaystyle{r_n=\frac{1}{2(2\sqrt{\pi})^{H_{n-1}}}\prod_{k=1}^{n-1}\sqrt[k]{\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k+2}{2})}}}$ So finally, $\textstyle\displaystyle{V_n(r_n)=\frac{\pi^\frac{n}{2}}{\Gamma(\frac{n}{2}+1)}r_n^n}$ $\textstyle\displaystyle{\begin{align}\mathcal{R}_n=&\frac{\sqrt{\pi}^{n(1-H_{n-1})}}{2^{n(1+H_{n-1})}\Gamma(\frac{n+1}{2})}\prod_{k=1}^{n}\left(\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k+2}{2})}\right)^\frac{n}{k}\end{align}}$ So, $\textstyle\displaystyle{\mathcal{R}=\sum_{n=1}^{\infty}\frac{\sqrt{\pi}^{n(1-H_{n-1})}}{2^{n(1+H_{n-1})}\Gamma(\frac{n+1}{2})}\prod_{k=1}^{n}\left(\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k+2}{2})}\right)^\frac{n}{k}}$ Here $H_0=0$ $\mathcal{R}\approx 1.0817135\dots$ $\underline{\text{My Question}:-}$ Is there a closed form for $\mathcal{R}$ ? Or maybe we can write $\mathcal{R}$ in terms of some advance function? Are there some interesting properties of $\mathcal{R}$ ? A list of the first few terms of $\mathcal{R}$ :- $\{\mathcal{R}_n\}_{n=1}^{\infty}$ $=\{$ $\mathcal{R}_1$ , $\mathcal{R}_2$ , $\mathcal{R}_3$ , $\mathcal{R}_4$ , $\mathcal{R}_5$ , $\mathcal{R}_6$ , $\mathcal{R}_7$ , $\mathcal{R}_8$ , $\mathcal{R}_9$ , $\mathcal{R}_{10}$ , $\dots$$\}$ $\textstyle\displaystyle{=\left\{1, \frac{1}{2^2\pi}, \frac{1}{2^43\pi^2}, \frac{1}{2^\frac{23}{3}3^\frac{4}{3}\pi^\frac{10}{3}}, \frac{1}{2^\frac{31}{3}3^\frac{17}{12}5\pi^\frac{14}{3}}, \frac{1}{2^\frac{72}{5}3^\frac{27}{10}5^\frac{6}{5}\pi^\frac{31}{5}}, \frac{1}{2^\frac{163}{10}3^\frac{179}{60}5^\frac{37}{30}7\pi^\frac{116}{15}}, \cdots\right\}}$ You can yourself find more terms by just plugging the formula in Wolfram alpha. Notice the terms at the denominator have a very weird property, they are all prime numbers to the power of some rational numbers which is really weird. The more peculiar part is that they are not random prime numbers, but the first $n$ prime numbers. So there will definitely be some involvement of prime numbers in the evaluation of $\mathcal{R}$ . Observing some patterns:- Notice that every $n^\text{th}$ $2$ terms namely the ${2n-1}^\text{st}$ and ${2n}^\text{th}$ terms have the first $n$ primes in their denominator. Also notice that when the $n^\text{th}$ prime first appears at the ${2n-1}^\text{st}$ term it is being raised to the first power and at the ${2n}^\text{th}$ term it is being raised to the power of $\frac{p_n+1}{p_n}$ . (If we say that the $n^\text{th}$ prime is $p_n$ ). I can't prove these observations, but if they are true to infinity then we can write- $\textstyle\displaystyle{\mathcal{R}=\sum_{n=1}^{\infty}\left(\frac{1}{p_n\pi^{q_{2n-1}}\prod_{k=1}^{n-1}p_k^{r_{k,2n-1}}}+\frac{1}{p_n^{\frac{p_n+1}{p_n}}\pi^{q_{2n}}\prod_{k=1}^{n-1}p_k^{r_{k,2n}}}\right)}$ For some rational sequence $q_n$ and $r_{m,n}$ It seems like $\forall n\in\mathbb{N}, q_{n+1}>q_n$ And the greatest common divisor of the denominators of $r_{m,n}$ and $q_n$ is not $1$ , namely If $\textstyle\displaystyle{r_{m,n}=\frac{s_{m,n}}{t_{m,n}}}$ and $\textstyle\displaystyle{q_n=\frac{a_n}{b_n}}$ such that $\operatorname{gcd}(s_{m,n},t_{m,n},)=\operatorname{gcd}(a_n,b_n)=1$ Then, $\operatorname{gcd}(t_{1,n},\dots,t_{m,n},b_n)\neq 1$ for $n\gt 3$ I don't know if my observation are true to infinity or not, do they even simplify $\mathcal{R}$ , I don't know. But it increases my hope for getting a closed form for $\mathcal{R}$ . If I am able to spot more patterns then I will add it to the post. Working with the formula itself:- I hadn't really worked with the formula yet. I spent the last 2 days trying to find more patterns, I think I may have found another one but I need work on that a little bit more before I add that to the question. I am sure is the following progress or regress, but whatever. $\textstyle\displaystyle{\mathcal{R}_n=\frac{\sqrt{\pi}^{n(1-H_{n-1})}}{2^{n(1+H_{n-1})}\Gamma(\frac{n+1}{2})}\prod_{k=1}^{n}\left(\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k+2}{2})}\right)^\frac{n}{k}}$ $\textstyle\displaystyle{=\frac{\sqrt{\pi}^{n(1-H_{n})}}{2^{n\left(\frac{1}{2}+H_{n-1}\right)}(n-1)!!}\prod_{k=1}^{n}\left(\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k+2}{2})}\right)^\frac{n}{k}}$ Now I am going to break $\mathcal{R}$ into two infinite sums as following $\mathcal{R}=E+O$ Where, $\textstyle\displaystyle{E=\sum_{n=1}^{\infty}\frac{\pi^{n(1-H_{2n})}}{2^{n(1-2H_{2n-1})}(2n-1)!!}\prod_{k=1}^{2n}\left(\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k+2}{2})}\right)^\frac{2n}{k}}$ $\textstyle\displaystyle{=\sum_{n=1}^{\infty}\frac{(n-1)!\pi^{n(1-H_{2n})}}{4^{nH_{2n}}(2n-1)!}\prod_{k=1}^{2n}\left(\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k+2}{2})}\right)^\frac{2n}{k}}$ Here i replaced every $n$ with $2n$ And, $\textstyle\displaystyle{O=\sum_{n=1}^{\infty}\frac{\sqrt{\pi}^{2n(1-H_{2n})+H_{2n-1}}}{2^{(2n-1)\left(\frac{1}{2}+H_{2n-2}\right)}(2n-2)!}\prod_{k=1}^{2n-1}\left(\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k+2}{2})}\right)^\frac{2n-1}{k}}$ $\textstyle\displaystyle{=\frac{1}{2\sqrt{2}}\sum_{n=1}^{\infty}\frac{\sqrt{\pi}^{2n(1-H_{2n})+H_{2n-1}}}{2^{(2n-1)(1-H_{2n-1})}(n-1)!}\prod_{k=1}^{2n-1}\left(\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k+2}{2})}\right)^\frac{2n-1}{k}}$ Here i replaced every $n$ with $2n-1$ . Before I do anything with these $E$ and $O$ . I want to first take care of the product inside both of them. Let, $\textstyle\displaystyle{P_{n}=\prod_{k=1}^{2n}\left(\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k+2}{2})}\right)^\frac{2}{k}}$ $\textstyle\displaystyle{=\prod_{k=1}^{n}\left(\frac{\Gamma(\frac{2k+1}{2})}{\Gamma(\frac{2k+2}{2})}\right)^\frac{2}{2k}\prod_{k=1}^{n}\left(\frac{\Gamma(\frac{2k}{2})}{\Gamma(\frac{2k+1}{2})}\right)^\frac{2}{2k-1}}$ $\textstyle\displaystyle{=\prod_{k=1}^{n}\left(\frac{\Gamma(k+\frac{1}{2})}{k!}\right)^\frac{1}{k}\prod_{k=1}^{n}\left(\frac{(k-1)!}{\Gamma(k+\frac{1}{2})}\right)^\frac{2}{2k-1}}$ $\textstyle\displaystyle{=\prod_{k=1}^{n}\left(\frac{(2k)!\sqrt{\pi}}{4^kk!^2}\right)^\frac{1}{k}\prod_{k=1}^{n}\left(\frac{4^kk!(k-1)!}{(2k)!\sqrt{\pi}}\right)^\frac{2}{2k-1}}$ $\textstyle\displaystyle{=\prod_{k=1}^{n}\left(\frac{\sqrt{\pi}}{4^k}\right)^\frac{1}{k}\prod_{k=1}^{n}\left(\frac{4^k}{\sqrt{\pi}}\right)^\frac{2}{2k-1}\prod_{k=1}^{n}\left(\frac{(2k)!}{k!^2}\right)^\frac{1}{k}\prod_{k=1}^{n}\left(\frac{k!(k-1)!}{(2k)!}\right)^\frac{2}{2k-1}}$ Since, $\sum_{k=1}^{n}\frac{1}{2k-1}=H_{2n}-\frac{1}{2}H_n$ And by Wolfram Alpha we have, $\prod_{k=1}^{n}16^\frac{k}{2k-1}=2^{2n+\psi(n+\frac{1}{2})-\psi(\frac{1}{2})}$ Also notice, $\frac{k!(k-1)!}{(2k)!}=B(k,k+1)$ and, $\frac{(2k)!}{k!^2}=\frac{1}{kB(k,k+1)}$ And finally again from Wolfram Alpha , $\prod_{k=1}^{n}k^{-\frac{1}{k}}=e^{\gamma_{1}(n+1)-\gamma_1}$ We have- $\textstyle\displaystyle{P_{n}^n=2^{n(\psi(n+\frac{1}{2})-2+\gamma+\ln(4))}\pi^{n(H_n-H_{2n})}e^{n(\gamma_1(n+1)-\gamma_1)}\prod_{k=1}^{n}B(k,k+1)^\frac{n}{k(2k-1)}}$ So, $\textstyle\displaystyle{E=\sum_{n=1}^{\infty}\frac{(n-1)!}{(2n-1)!}\pi^{n(1+H_n-2H_{2n})}e^{n(\gamma_1(n+1)-\gamma_1)}2^{n(\psi(n+\frac{1}{2})-2(n+1)H_{2n}+\gamma+\ln(4)-2)}\prod_{k=1}^{n}B(k,k+1)^\frac{n}{k(2k-1)}}$ Now replacing $n$ with $n-\frac{1}{2}$ should give us $P_{2n-1}$ which corresponds to the product inside $O$ . I am not sure would it work or not because $n-\frac{1}{2}$ won't be an integer anymore. And I don't know what $\prod_{k=1}^{m}$ means when $m\not\in\mathbb{Z}$ . So let's leave that idea and re do the process again. Let, $\textstyle\displaystyle{Q_n=\prod_{k=1}^{2n-1}\left(\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k+2}{2})}\right)^\frac{1}{k}}$ $\textstyle\displaystyle{=\prod_{k=1}^{n}\left(\frac{\Gamma(\frac{2k}{2})}{\Gamma(\frac{2k+1}{2})}\right)^\frac{1}{2k-1}\prod_{k=1}^{n-1}\left(\frac{\Gamma(\frac{2k+1}{2})}{\Gamma(\frac{2k+2}{2})}\right)^\frac{1}{2k}}$ $\textstyle\displaystyle{=\prod_{k=1}^{n}\left(\frac{4^k}{\sqrt{\pi}}\right)^\frac{1}{2k-1}\prod_{k=1}^{n-1}\left(\frac{\sqrt{\pi}}{4^k}\right)^\frac{1}{2k}\prod_{k=1}^{n}\left(\frac{k!(k-1)!}{(2k)!}\right)^\frac{1}{2k-1}\prod_{k=1}^{n-1}\left(\frac{(2k)!}{k!^2}\right)^\frac{1}{2k}}$ $\textstyle\displaystyle{=\sqrt{\pi}^{H_{n}-H_{2n}+\frac{1}{2n}}2^{\frac{1}{2}\psi(n+\frac{1}{2})+\frac{\gamma}{2}+\ln(2)+1}\frac{\sqrt[n]{n!}}{(2n)!^\frac{1}{2n}}e^{\gamma_1(n+1)-\gamma_1}\prod_{k=1}^{n-1}B(k,k+1)^\frac{1}{2k(2k-1)}}$ We have- $\textstyle\displaystyle{Q_n^{2n-1}=\sqrt{\pi}^{(2n-1)(H_{n}-H_{2n}+\frac{1}{2n})}2^{(2n-1)(\frac{1}{2}\psi(n+\frac{1}{2})+\frac{\gamma}{2}+\ln(2)+1)}e^{n(\gamma_1(n+1)-\gamma_1)}\frac{\sqrt[n]{n!}^{2n-1}}{(2n)!^\frac{2n-1}{2n}}\prod_{k=1}^{n-1}B(k,k+1)^\frac{2n-1}{2k(2k-1)}}$ So, $\textstyle\displaystyle{O=\frac{\sqrt{\pi}}{2\sqrt{2}}\sum_{n=1}^{\infty}\frac{1}{(n-1)!}\sqrt{\pi}^{(2n-1)H_n-4nH_{2n}+2n-\frac{1}{4n}}e^{n(\gamma_1(n+1)-\gamma_1)}2^{(2n-1)(\frac{1}{2}\psi(n+\frac{1}{2})+H_{2n-1}+\frac{\gamma}{2}+\ln(2)}\frac{\sqrt[n]{n!}^{2n-1}}{(2n)!^\frac{2n-1}{2n}}\prod_{k=1}^{n-1}B(k,k+1)^\frac{2n-1}{2k(2k-1)}}$ So finally, $\textstyle\displaystyle{E=\sum_{n=1}^{\infty}\frac{(n-1)!}{(2n-1)!}\pi^{n(1+H_n-2H_{2n})}e^{n(\gamma_1(n+1)-\gamma_1)}2^{n(\psi(n+\frac{1}{2})-2(n+1)H_{2n}+\gamma+\ln(4)-2)}\prod_{k=1}^{n}B(k,k+1)^\frac{n}{k(2k-1)}}$ $\textstyle\displaystyle{O=\frac{\sqrt{\pi}}{2\sqrt{2}}\sum_{n=1}^{\infty}\frac{1}{(n-1)!}\sqrt{\pi}^{(2n-1)H_n-4nH_{2n}+2n-\frac{1}{4n}}e^{n(\gamma_1(n+1)-\gamma_1)}2^{(2n-1)(\frac{1}{2}\psi(n+\frac{1}{2})+H_{2n-1}+\frac{\gamma}{2}+\ln(2)}\frac{\sqrt[n]{n!}^{2n-1}}{(2n)!^\frac{2n-1}{2n}}\prod_{k=1}^{n-1}B(k,k+1)^\frac{2n-1}{2k(2k-1)}}$ The real thing is that was this really a simplification or a complexification. And please tell me if there are any mistakes.","['spheres', 'gamma-function', 'closed-form', 'products', 'sequences-and-series']"
4241749,Domain of square root of x squared,"What is the domain of $f(x)=(\sqrt{x})^2$ ? Is it all real numbers, or are negative numbers still excluded, even after the square? Edit: What I'm really wondering is whether $\lim\limits_{x \to 0} f(x)$ is defined. Sorry for the confusion.",['functions']
4241804,Evaluating $\lim_{k\to \infty} \frac{\sum_{n=0}^{k}x^{\frac{n}{k}}}{k+1}$ for all $x$,"So I was really bored in my math class (I'm a high school sophomore taking precalculus in school) and my mind wandered to this expression: $$\lim_{k\to \infty} \frac{\sum_{n=0}^{k}x^{\frac{n}{k}}}{k+1}$$ I have no idea how to even begin to evaluate this. I did some playing around with it on Desmos and it looks something like this for large values of $k$ , which I don't recognize the shape of at all. I do notice that the graph should have the points $(0,0)$ and $(1,1)$ , and it seems like if $j>k$ then $\frac{\sum_{n=0}^{k}x^{\frac{n}{k}}}{k+1}\ge\frac{\sum_{n=0}^{j}x^{\frac{n}{j}}}{j+1}$ for all $x$ with equality at $x=1$ . Also, please feel free to add any suitable tags---I didn't really know which ones to add. (If you're wondering how I derived this expression, I was thinking about expressions like $\frac{x^0+x^1}{2}$ , $\frac{x^0+x^{\frac{1}{2}}+x^1}{3}$ , $\frac{x^0+x^{\frac{1}{3}}+x^{\frac{2}{3}}+x^1}{4}$ , and then I thought about what would happen if we added more and more terms...)","['limits', 'summation']"
4241902,Weak convergence in $L^2$ implies boundness,"Let $f$ , $f_n \in L^2$ , we say that $f_n \rightharpoonup f$ converges weakly in $L^2$ if $$\lim_{n \to \infty} \langle (f_n-f),g \rangle_{L^2}=0\,\,\,\forall\,\,\, g \in L^2. $$ I know that using the Uniform boundedness principle ( Banach-Steinhaus Theorem) the sequence $(\lVert f_n \rVert_2)$ is bounded. But I wish to know a way to prove this boundness without using the uniform boundness principle. Given the particularity of the situation, I think that is not that hard, but nevertheless I still need help, thank you in advance! I was able to prove, using Riesz- Fréchet representation theorem that for each linear functional $\varphi:L^2 \to \mathbb{R}$ , the sequence $(\varphi(f_n))_n$ is bounded. There exist some particular $\varphi \in L^2$ which implies the boundness of the sequence $(\lVert f_n \rVert_2)$ ?","['measure-theory', 'weak-convergence', 'analysis', 'real-analysis', 'functional-analysis']"
4241914,Kernel of the polynomial to function map?,"It's common to think of polynomials as functions, but it was recently brought to my attention that this isn't exactly right. Consider, for example $x$ and $x^7$ in $\mathbb{Z} / 7\mathbb{Z} [x]$ . These are distinct polynomials, but as functions $\mathbb{Z} / 7\mathbb{Z} \to \mathbb{Z} / 7\mathbb{Z}$ they are identical. This made me wonder: how can we describe the kernel of the map $
\varphi : \mathbb{Z} / n\mathbb{Z} [x] \to A
$ given by $\varphi(p) = (x \mapsto p(x))$ where \begin{align}
A = \{p : \mathbb{Z} / n\mathbb{Z} &\to \mathbb{Z} / n\mathbb{Z} \\&\mid p \text{ is a polynomial with coefficients in } \mathbb{Z} / n\mathbb{Z}\}
\end{align} (Is there a more standard notation for $A$ ?) The only thing I know is that when $n$ is prime, $x^n - x \in \ker \varphi$ . This is just Fermat's little theorem. Can we give an explicit characterization of $\ker \varphi$ ? How should we think of elements of $\mathbb{Z} / n\mathbb{Z} [x]$ if not as functions $\mathbb{Z} / n\mathbb{Z} \to \mathbb{Z} / n\mathbb{Z}$ .","['functions', 'abstract-algebra', 'polynomial-rings']"
4241915,The 'inscribed-circumscribed' 6 point circle,"Here is a construction I came upon recently: A'B'C' is the contact triangle. X( 1 ) is the Incenter. A'',B'',C'' are the midpoints of the sides of the contact triangle. A''',B''',C''' are the midpoints of the segments AX(1), BX(1), CX(1) . Then quadrilaterals A''A'''C''C''', B''B'''C''C''', A''A'''B''B''' are cyclic. Let Ob, Oa, Oc be the circumcenters of these 3 quadrilaterals. Finally by drawing circles with centers at Oa, Ob, Oc and passing through the incenter we get the ' inscribed-circumscribed' six point circle with its center at the midpoint of X(1) and X(3) - the point X(1385) in the ETC: There is an even easier way to get a smaller concentric six point circle: circumcircles of the quadrilaterals A''A'''C''C''', B''B'''C''C''', A''A'''B''B''' cut the sides of the triangle ABC at six concyclic points. The mere fact that the point  X(1385) is already described in the ETC supposedly makes the construction not particularly inspiring for a geometer.  However as this ' X(1385)-circle ' has never been previously mentioned (?) in the literature, there might still be a chance that it has some 'nice' properties of its own. Specifically I am interested whether any Kimberling center belongs to this 'inscribed-circumscribed' circle?  Or, alternatively, does its central function correspond to any known center?","['euclidean-geometry', 'conjectures', 'circles', 'geometry', 'triangles']"
4241937,How do we reduce the inversion formula for the $n$-dimensional Fourier transform to the $1$-dimensional case using Fubini's theorem?,"Please consider the following theorem from Measures, Integrals and Martingales (2nd edition) : Question 1 : The proof of the claim is straightforward, but I don't understand the remark in the ""caution"" paragraph. Isn't the right-hand side of (19.6) equal to the Bochner integral $\int\frac{e^{{\rm i}b\xi}-e^{{\rm i}a\xi}}{{\rm i}\xi}\hat\mu(\xi)\:\lambda({\rm d}\xi)$ , where $\lambda$ denotes the Lebesgue measure on $\mathcal B(\mathbb R)$ ? Moreover, I would like to understand how this identity can be generalized to the $n$ -dimensional case. In the book, there is the following corollary (without proof), but I'm not sure how we obtain it: Question 2 : How exactly do we need to apply Fubini's theorem here in order to use the former result (19.6)? Do we need to apply (19.6) somehow to the finite measures $\mu_i$ , where $$\mu_1(B):=\mu\left(B\times\times_{i=2}^n(a_i,b_i)\right)\;\;\;\text{for }B\in\mathcal B(\mathbb R)$$ and $\mu_2,\ldots,\mu_n$ are defined in the obvious analogous way?","['measure-theory', 'fubini-tonelli-theorems', 'fourier-transform', 'real-analysis']"
4241939,$A$ upper triangular $n\times n$ over $\mathbb{R}$. Show that $I-A$ is invertible and express inverse of $I-A$ as a function of $A$.,"Let $A$ be a strictly upper triangular $n\times n$ matrix with real entries.  Show that $I-A$ is invertible and express the inverse of $I-A$ as a function of $A$ . $A$ is upper triangular, so the diagonal is all $0$ , everything below the diagonal is all $0$ , and then we have entries in the upper part, say, $a_{1,2}$ , as the entry in row $1$ column $2$ .  If we consider $I-A$ , then $I-A$ has $1$ in every entry of the diagonal, only $0$ below the diagonal, and negative whatever the entry of $A$ was in the upper part.  So, det $(I-A)=1n=1$ , so $I-A$ is invertible.  Consider $$X=I+A+A^2+ \cdots +A^{n-1}$$ Then, I want to show that $(I-A)X=X(I-A)=I$ . I can see why this ""might"" work, since we would have $I$ and then add all the powers of $A$ , then subtract all the powers of $A$ ... but are there any unexpected issues I should be careful of here?  Thank you much!","['matrices', 'determinant', 'linear-algebra', 'inverse']"
4241958,How to convert an informal proof to something formal?,"Q: Prove that for a 0-2 non-empty binary tree, num of external nodes=num of internal nodes+1 Here is a proof I came up with while in class a completely diff proof was discussed. Is this correct? Proof: $n_e$ = num of external nodes $n_i$ = num of internal nodes Induction Hypothesis: For a 0-2 non-empty finite binary tree, $n_e$ = $n_i$ +1 Proof: By induction on $n_i$ Base Case: $n_i$ =0 in case of binary tree with 1 vertex. $n_e$ =1, so P(0) is true Induction Step: For a tree with $n_i$ internal nodes, we have $n_e$ = $n_i$ +1. Now we add 1 internal node. This can be done only by converting a leaf (ie external node) to an internal node. (Does this require a proof?) In this case, we have $n_i'$ = $n_i$ +1. And by property of 0-2 btree, we have to add 2 children to this internal node, in this process, $n_e'$ = $n_e$ -1+2= $n_e$ +1. Hence, $n_e'$ = $n_e$ +1= $n_i$ +2= $n_i'$ +1, hence proved. Another of my proofs on the same proposition can be found here. How to write formal proofs? My prof says I write ""too much story/informally"" while structuring proofs. But I can't seem to rectify myself. Any help?","['graph-theory', 'induction', 'discrete-mathematics']"
4242030,Understanding the confidence interval and statistical significance,"I am struggling to understand confidence intervals and their relationships to a null hypothesis. The basic definition of the confidence interval is: (1−α), where α is the statistical significance. So let's say I have two cases: I've allocated a 70 percent confidence level of meeting the probability of my null hypothesis, this means I have a statistical significance of of .30. I've allocated a 95 percent confidence level of meeting the probability of my null hypothesis. α = .05. Is it always better to have the case of a 95 percent confidence level, as I have the higher probability of not rejecting my null hypothesis? With the 70 percent confidence region, I always have a higher probably of falling outside the (1-α) region which rejects my null hypothesis. To me having a 95 percent confidence is always better. Is there a reason to ever prefer 70 percent confidence interval? Would the 95 percent confidence scenario require less resources to sample from?","['statistics', 'confidence-interval', 'probability']"
4242050,New way to obtain partial continued fractions of square roots?,"Some background: I was working on a problem that asked for the sum of the cubes of the roots of a cubic, $x^3 - 5x^2 + 5x - 1$ . I found that this factored into $(x-1)(x^2 - 4x + 1)$ , meaning that its roots are $1$ and $2 \pm \sqrt{3}$ . After finding the sum of the cubes of them, I re-checked my work by manually cubing the roots and adding them together. While I was looking at the powers of $2 + \sqrt{3}$ , I noticed that the integer component of the sum approached the same value as the irrational component. For example, $(2 + \sqrt{3})^3 = 26 + 15\sqrt{3} = \sqrt{676} + \sqrt{675}$ , which is way closer than $2$ and $\sqrt{3}$ are. Moreover, when I looked at the powers of more expressions of the form $a + \sqrt{b}$ , where $a$ and $b$ were integers and $b$ was not a perfect square, I noticed that when $a = \lfloor \sqrt{b} \rfloor$ , the quotient of the coefficients of the rational and irrational terms after simplification is equal to a partial sum of the infinite fraction of $\sqrt{b}$ . For example, the continued fraction of $\sqrt{2}$ is $$1 + \cfrac{1}{2 + \cfrac{1}{2 + \cfrac{1}{2 + \cfrac{1}{2 + \cdots}}}}$$ and the first few partial fractions are $$1, \frac{3}{2}, \frac{7}{5}, \frac{17}{12}, \frac{41}{29}, \cdots$$ Let $p_{a\sqrt{b}}$ denote the $a^{\text{th}}$ partial sum of the infinite fraction of $\sqrt{b}$ . Looking at the powers of $1 + \sqrt{2}$ , and letting $r_{c}$ be the ratio of the coefficients of the rational and irrational terms (i.e. $r_{c} = \frac{\text{coefficient}_{\text{rational}}}{\text{coefficient}_{\text{irrational}}}$ ), we find \begin{align*}&(1 + \sqrt{2})^1 = 1 + 1\sqrt{2} \Rightarrow r_{c} = \frac{1}{1} = 1 = p_{1\sqrt{2}}\\ &(1 + \sqrt{2})^2 = 3 + 2\sqrt{2} \Rightarrow r_{c} = \frac{3}{2} = p_{2\sqrt{2}} \\ &(1 + \sqrt{2})^3 = 7 + 5\sqrt{2} \Rightarrow r_{c} = \frac{7}{5} = p_{3\sqrt{2}} \end{align*} etc. This pattern also appears to hold for other values of $b$ , as long as $a = \lfloor \sqrt{b} \rfloor$ . For example, the continued fraction of $\sqrt{5}$ is $$2 + \cfrac{1}{4 + \cfrac{1}{4 + \cfrac{1}{4 + \cfrac{1}{4 + \cdots}}}}$$ and the first few partial fractions are $$2, \frac{9}{4}, \frac{38}{17}, \frac{161}{72}, \frac{682}{305}, \cdots$$ Looking at the powers of $2 + \sqrt{5}$ , we have \begin{align*}&(2 + \sqrt{5})^1 = 2 + 1\sqrt{5} \Rightarrow r_{c} = \frac{2}{1} = 2 = p_{1\sqrt{5}}\\ &(2 + \sqrt{5})^2 = 9 + 4\sqrt{2} \Rightarrow r_{c} = \frac{9}{4} = p_{2\sqrt{5}} \\ &(2 + \sqrt{5})^3 = 38 + 17\sqrt{5} \Rightarrow r_{c} = \frac{38}{17} = p_{3\sqrt{5}} \end{align*} Does this method actually work for all possible $b$ , and if so, has anyone else catalogued this yet? As someone who pursues math for fun, I find this to be a very interesting result! Also, would there be an elegant way to prove that $r_c$ approaches $\sqrt{b}$ as $n$ , the exponent, increases to infinity? I think that the binomial theorem would be very helpful, especially the fact that $\binom{n}{0} + \binom{n}{2} + \cdots = \binom{n}{1} + \binom{n}{3} + \cdots$ . Thank you for taking the time to read this exceptionally long post, and any help would be appreciated! Edit: Some of the comments are mentioning that the aforementioned process fits some patterns. Could someone explain how those patterns work/are generated?","['approximation', 'radicals', 'combinatorics', 'discrete-mathematics', 'continued-fractions']"
4242060,How to show Riemann zeta function $\zeta(s)$ is holomorphic (except at s=1)?,"The Riemann zeta function $\zeta(s)$ is known to be holomorphic, except at $s=1$ . I am not trained to university maths, but I interpret this to mean: it is complex differentiable at every point (smoothly changing values, no discontinuities) and is equal to Taylor series developed around any point in its domain Question: How do we show $\zeta(s)$ is holomorphic? Is it enough to show that the series which represent it, $\zeta(s)=\sum 1/n^s$ valid for $\Re(s)>1$ for example, are infinitely differentiable , except at the known pole at $s=1$ ? Is it also correct to say that by the principle of analytic continuation, any other series, such as $\zeta(s)=(1-s^{1-s})^{-1}\eta(s)$ valid over larger domains, are also holomorphic, except at any new poles. By separate analysis, there are no new poles in $0<\Re(s)\leq 1$ . Apologies if this question seems naive, I am self-teaching.","['complex-analysis', 'riemann-zeta']"
4242076,'Conceptual' proof of Holder's inequality or Minkowski's inequality,"Holder's inequality and Minkowski's inequality are two basic inequalities about $L^p$ spaces, stating that $||fg||_1 \leq ||f||_p||g||_q$ when $p, q$ are conjugate, and that $||f+g||_p \leq ||f||_p + ||g||_p.$ Holder's inequality is closely related to the fact that $L^p, L^q$ are each others' duals for $p, q$ conjugate (and $1 < p, q < \infty$ ). However, the only proofs I know of these inequalities are very 'algebra heavy' and I find it hard to think of what the 'ideas' of the proofs are, or how to compress them in my head beyond (for Holder) 'do funky algebra and apply Young's inequality' and (for Minkowski) 'do funky algebra and apply Holder's.' Is there some more intuitive/conceptual proof of either inequality? Say, some abstract way of seeing the dual of $L^p$ is $L^q$ in a way that allows you to deduce Holder, or some other proof that doesn't use any 'clever' algebraic manipulations (or, perhaps, any motivations for these clever manipulations?).","['intuition', 'analysis']"
4242086,This matrix defined by a polynomial $f$ has rank less than $\deg(f)+1$,"Let $f\in \mathbb{R}[x]$ be a polynomial of degree $d$ . Let $x_1, \dots, x_n$ be real numbers, I want to show the matrix $A$ given by $A_{ij} = f(x_i + x_j)$ has rank $\le d+1$ . My attempt: I tried writing the polynomial as $f(t)=\sum_{k=0}^d c_k t^k$ , then I can write $A = \sum_{k=0}^d c_k B^{(k)},$ where $B^{(k)}$ is a matrix with entries $B^{(k)}_{ij} = (x_i+x_j)^k$ . But this does not seem to be helpful. Thank you in advance.",['linear-algebra']
4242089,"Lebesgue integral of two functions are equal, then the functions are equal a.e.","Let $\psi,\phi$ be non negative measurable functions on the measure space $(X,\Sigma,\mu)$ . Such that, $\psi \le \phi$ . If, $$\int_{X} \psi ~ d\mu = \int_{X} \phi ~ d\mu,$$ then $\psi = \phi$ a.e. on $X$ . Let, $g(x) = \phi(x) - \psi(x).$ Then $\forall \lambda \in \mathbb{R}^{+},$ $$ \mu(\{ x \in X: g(x) \ge \lambda \} ) \le \frac{1}{\lambda}\int_{X}g(x) \le \frac{1}{\lambda}(\int_{X}\phi(x) ~ d\mu  ~  - ~ \int_{X}\psi(x) ~ d\mu  ) = 0.$$ Let, $V = \{x \in X: g(x) \ne 0 \}$ and $E_{n} = \{x \in X: g(x) \ge \frac{1}{n} \}$ for all  natural numbers $n \ge 1$ . We have that, $$V = \bigcup^{\infty}_{n=1}E_n $$ Then, $$\mu(V) = \mu(\bigcup^{\infty}_{n=1}E_n) = \lim_{n \rightarrow \infty} \mu(E_n) = 0. $$ Therefore, $\psi = \phi$ a.e. on $X$ . Is this proof correct?","['solution-verification', 'functional-analysis']"
4242093,Does the graph contain a Hamiltonian and an Euler cycle?,"Question: Let $G=(V_n,E_n)$ such that: G's vertices are words over $\sigma=\{a,b,c,d\}$ with length of $n$ , such that there aren't two adjacent equal chars. An edge is defined to be between two vertices that are different by only one char. A. Does the graph contain an Euler cycle? Find a pattern. B. Does the graph contain a Hamiltonian cycle This can be proven by induction. $Solution.A.$ Now, when $n=1$ , we have 4 vertices: $$v_1= \ 'a'$$ $$v_2= \ 'b'$$ $$v_3= \ 'c'$$ $$v_4= \ 'd'$$ Therefore, for each $v\in \{v_1,v_2,v_3,v_4\}$ , $N(v)=\{v_1,v_2,v_3,v_4\}/ \{v\}$ so we get that their degree is 3, so by a theorem we get that there isn't an Euler cycle. In addition, when $n=4$ , considering the string $""abad""$ we have 2 options to replace the edges of the string. In order to replace the second char we have 2 options, replacing it by $'c'$ and $'d'$ . For the third char, we can replace it only by $'c'$ . In total, we got 7 edges with this vertex, so by a theorem, we get that there isn't an Euler cycle. I cannot find here a pattern, because if we take a look at $n=2$ we get an Euler cycle. $Solution.B.$ First, we examine whether each vertex has at least $\frac{n}{2}$ neighbors. Hence, we should take the vertex to have the least number of neighbors. This vertex should be the string with disjoint chars. i.e. the string ""abcd"" when $n=4$ . The first and last chars has always 2 neighbors, so we get that the least degree is: $$2+2+\binom{n-2}{n-3} \cdotp 1=4+n-2=n+2\geq \frac{n}{2}$$ Thus, we get that the graph always has a Hamiltonian cycle. I don't get why I didn't get a pattern in $A$ , and how $B$ can be proven by induction. In addition, is my answer correct?","['graph-theory', 'eulerian-path', 'discrete-mathematics', 'hamiltonian-path']"
4242184,Is the answer of a problem about the equation of a straight line given by my book wrong?,"This is the problem and the answer given by my book: My solution: $3x+\sqrt{3}y+2=0...(i)$ $x\cos\alpha+y\sin\alpha=p...(ii)$ Since (i) & (ii) are equations of the same straight line, $$\frac{3}{\cos\alpha}=\frac{\sqrt{3}}{\sin\alpha}=\frac{2}{-p}$$ $$\implies -3p=2\cos\alpha...(i)$$ $$\implies -\sqrt{3}p=2\sin\alpha...(ii)$$ $(i)^2+(ii)^2:-$ $$9p^2+3p^2=4\cos^2\alpha+4\sin^2\alpha$$ $$\implies 12p^2=4$$ $$\implies p^2=\frac{1}{3}$$ $$\implies \sqrt{p^2}=\sqrt{\frac{1}{3}}$$ $$\implies |p|=\frac{1}{\sqrt{3}}$$ $$\implies p=\pm \frac{1}{\sqrt{3}}$$ Why did the book pick only the negative value of $p$ ? Related","['coordinate-systems', 'trigonometry', 'geometry']"
4242188,Prove $\int_0^1 \ln(x+1) \ln(x)dx=2-2\ln(2)-\frac{\pi^2}{12}$ [duplicate],"This question already has answers here : Evaluate $ \int_{0}^{1} \ln(x)\ln(1-x)\,dx $ (9 answers) Closed 2 years ago . Prove that $$I:=\int_0^1 \ln(x+1) \ln(x)dx=2-2\ln(2)-\frac{\pi^2}{12}$$ I've tried integration by parts and u substitution and cannot make it work but I have one potential path I use $$\ln(x+1)=\sum^{\infty}_{n=1} \frac{(-1)^{n-1}x^n}{n}
$$ $$\Rightarrow I=\int_0^1 \ln(x)\sum^{\infty}_{n=1} \frac{(-1)^{n-1}x^n}{n}dx=\sum^{\infty}_{n=1} \frac{(-1)^{n-1}}{n} \int_0^1 x^{n-1} \ln(x)dx= 
$$ How does one proceed from here?
Thank you for your time","['integration', 'calculus', 'real-analysis']"
4242214,Prove $\int^{\infty}_0 \frac{\sin(\sqrt{x})\ln^2\big(\frac{1}{\sqrt{x}}\big)}{x}dx=\frac{\pi^3}{12}+\gamma^2 \pi$,"I came across the following integral Prove $$\int^{\infty}_0 \frac{\sin(\sqrt{x})\ln^2\big(\frac{1}{\sqrt{x}}\big)}{x}dx=\frac{\pi^3}{12}+\gamma^2 \pi$$ where $\gamma$ is the euler-mascheroni constant.
I simplify the integral.
let $x=u^2 \Rightarrow dx=2udu$ $$\Rightarrow\int^{\infty}_0 \frac{\sin(\sqrt{x})\ln^2\big(\frac{1}{\sqrt{x}}\big)}{x}dx\
= 2\int^{\infty}_0 \frac{\sin(u)\ln^2(u)}{u}du$$ How does one proceed from here?
Is there a particular formula or theorem that I need to use? Am I in too deep water if I can't figure this out? Thank you for your time","['integration', 'calculus', 'improper-integrals', 'real-analysis']"
4242230,A problem on Mean Value Theorem from Thomas Calculus.,"The question If $f:[0,4] \rightarrow \mathbb{R}$ is differentiable, then prove that $$
[f(4)]^2-[f(0)]^2=8f'(a)f(b) \text{ for } a,b \in (0,4)
$$ My Solution Let's choose $b$ such that $$
f(b)=\frac{f(4)+f(0)}{2} \tag{1}
$$ Since $f$ is differentiable in it domain, $$
\exists \text{ } c \in (0,4) \text{ | } f'(c)=\frac{f(4)-f(0)}{4-0} \tag{2}
$$ If we consider $a=c$ and multiply (1) and (2), we get $$
f'(a)f(b)=\frac{[f(4)]^2-[f(0)]^2}{8}
$$ and we're done. My doubt This solution has $b$ locked to one particular value and $a$ could assume a couple of values. I'm not entirely sure if the question is asking us to prove all values of $a$ and $b$ in $(0,4)$ satisfies the given assertion. I'm pretty sure that all values in $(0,4)$ would not satisfy the given assertion. Any comments is highly appreciated.",['derivatives']
4242241,Is the function here an onto function for its given domain?,"I have been given to prove that $f:(- \infty, -1] \rightarrow (- \infty, 3]$ defined by $f(x) = x^3-3x+2$ is not onto. Now I found that the function is increasing in its given domain, and it's range is $(- \infty, 4]$ The definition of an onto function is that ""if $f:A \rightarrow B$ then every element of B must have a pre image in A"" Here, that implies, every element of $(- \infty, 3]$ must have a pre image in the domain right? Isn't that what's happening? Then why isn't this an onto function exactly?","['calculus', 'functions', 'algebra-precalculus']"
4242258,Geometry of the map $\vec{u}\times(\vec{r}\times\vec{u})$,A linear map $f\colon \mathbb{R}^3 \to \mathbb{R}^3$ is defined as $\vec{u}\times(\vec{r}\times\vec{u})$ where $\vec{u}$ is a unit vector. What would its geometry look like? I know that I can rewrite this map as $(\vec{u}\cdot\vec{u})\vec{r}-(\vec{u}\cdot\vec{r})\vec{u} = \vec{r} - (\vec{u}\cdot\vec{r})\vec{u}$ However I am not sure what to do from here.,"['functions', 'linear-algebra', 'vectors']"
4242300,Eigenvalues of adjoints of Hecke operators,"I am trying to work through Diamond and Shurman's A First Course in Modular Forms but am stuck in one of the exercises. Exercise 5.11.2 asks us to show that given a normalized eigenform $f\in S_k(\Gamma_1(N))$ with eigenvalues $a_p$ for Hecke operators $T_p$ , $f$ has eigenvalue $\overline{a_p
}$ under the adjoint $T_p^*$ with respect to the Petersson inner product. Assuming that $T_p^*f= b_p f$ , we see that $a_p||f||^2 = \langle a_pf,f\rangle = \langle T_pf,f\rangle = \langle f,T_p^*f\rangle = \langle f,b_pf\rangle=\overline{b_p}||f||^2$ , which gives us $b_p = \overline{a_p}$ as needed. However, I do not see why $f$ has to be an eigenvector for the operators $T_p^*$ for all $p$ . For $p\nmid N$ , $T_p^* = \langle p\rangle^{-1}T_p$ , and since $f$ is an eigenvector for both $\langle p\rangle$ and $T_p$ , so in this case I can see that $f$ is an eigenform. But what about the case $p\mid N$ ? The given hint just assumes that $T_p^*f= b_p f$ for all $p$ and proceeds as in the last paragraph.","['number-theory', 'modular-forms', 'hecke-algebras']"
4242319,Just infinite group,"A group is said to be just infinite if it is infinite and every proper quotient is finite.
In this paper http://onlinelibrary.wiley.com/doi/10.1002/cpa.3160210604/pdf I read that infinite dihedral group $D_\infty$ is just infinite and its proper quotients are dihedral groups of type $D_{2n}$ . Why is this? I ask also if is there a name for the dual notion: a group infinite with every not trivial (normal?) subgroup finite.
For exemple $\mathbb{Z}(p^\infty)$ and Tarsky group have this property; are there any other interesting examples?","['group-theory', 'infinite-groups', 'dihedral-groups']"
4242392,Verifying solutions to logarithms,"I was working on a logarithm question and I know how to solve it. $\log_2(y^2) = 4+\log_2(y+5)$ Now after working out the solution you will find that the quadratic yields $y=-4, y=20$ now you are supposed to verify your solutions to rid of any that do not comply with logs requirements for $\log_a(b)$ $a,b>0$ log laws state that $\log_a(b^c)= (c)\log_a(b)$ from the equation above $\log_2(y^2) = 4+\log_2(y+5)$ both solutions work however when you pull out the squared $2\log_2(y) = 4+\log_2(y+5)$ since $y=-4$ is not positive, it is not valid. but since $2\log_2(y)$ and $\log_2(y^2)$ are equivalent what is the problem here with depending on which form is in the equation, the solutions can be different?","['algebra-precalculus', 'logarithms']"
4242440,Solving exact second order differential equation,"I just started reading about second order differential equations and I have issue with exact equations. In the book I'm reading from, says, The equation $p(x)y''+q(x)y'+r(x)y=s(x)$ is exact if we have $p''-q'+r=0$ . These type of equations can be solve with the following
method, $$\frac d{dx}(py'+(q-p)y)=s(x)\xrightarrow{\int} \text{ first order
 linear differential equation} \Rightarrow \ldots$$ I tried to find elementary proofs for why exact equations can be solve as mentioned in the book but some of them was too advanced for me like Wikipedia and I found a similar question on this site here which states that If an equation $P(x)y''+Q(x)y'+R(x)y=0$ can be written in the form: $$[P(x)y']'+[f(x)y]'=0$$ then the equation is said to be exact. And it cause more confusion for me because in the book says RHS is a function $s(x)$ but here RHS is zero. Can you please prove the statement in the book?",['ordinary-differential-equations']
4242445,"How many functions $g:A\to A$ are there such that $g(g(x))=x$ and $g(x)-x$ is not divisible by $3$ where $A=\{1,2,3,\dots,12\}$?","My friend shared the following problem with me. The problem is from Bangladesh Mathematical Olympiad 2021 (Divisional round). Let $ A=\{1,2,3,\dots,12\}$ . How many functions $g:A\to A$ are there such that $g(g(x))=x$ and $g(x)-x$ is not divisible by $3$ ? Here is my progress in solving the problem: We partition the sets modulo $3$ . Let $A_0=\{3,6,9,12\}$ , $A_1=\{1,4,7,10\}$ , $A_2=\{2,5,8,11\}$ . Because $g(g(x))=x$ the function will be fixed if we fix the values of the function for $6$ numbers. Let $g(1)=k$ , then $k\notin A_1$ and $g(k)=1$ . This means $k$ can take $8$ values. Similar argument leads that $g(4)$ , $g(7)$ , $g(10)$ can take $7$ , $6$ and $5$ values respectively. I can't proceed from here because I need to find the number of values the function can take for $2$ more numbers for the function to be fixed. But I can say that the final answer will be greater than $8\cdot7\cdot6\cdot5=1680$ from my approach. From the comments, I realized that my approach is wrong. Then how do I solve the problem?","['contest-math', 'functions', 'combinatorics']"
4242450,"Self-adjoint operators, distributions and spectral theorem","Suppose we have a self-adjoint operator, $L$ ,  which maps elements $u \in C^2 \to C^2$ on $(0,\infty)$ using the standard inner product. Furthermore, we have a trigonometric system over an infinite set of real numbers, $\theta_i$ , which converges to $L$ as a distribution- when integrated against a test function. What i mean by this is that suppose, $ Lu=v$ , then we have, for example, a cosine series over all $\theta_i$ , which takes in $u$ and outputs a distribution which converges to $v$ . Perhaps something like $$\sum_{i=1}^{\infty}\omega_i\cos(\theta_i) \approx{Lu}$$ $$ \omega_i=\langle\cos(\theta_i),u\rangle$$ where $\approx$ is only achieved after integrating the LHS against a test function.
Here are my questions: is it possible to use the spectral theorem to draw conclusions about the set of numbers, $\theta_i$ ? Specifically, can we say that the elements in the set are unique? while the cosine series is orthogonal, i am unsure if we can consider them eigenfunctions as the convergence is only as a distribution and is only achieved after we integrate against a test function. is it ever possible to have a self-adjoint operator which sends distributions to distributions? my instinct says no, because we can not take inner product of two distributions. EDIT (based off of comments from paul garrett):
Let's assume that $C^2$ is the space of all twice-continuously-differentiable functions and the 'standard inner product' is $\langle f,g\rangle=\int_{0}^{\infty}f(x)g(x)dx$ . Furthermore, $L$ is not necessarily bounded.
paul garrett also pointed out that the distinct consines in the example are problematic because they are not in $L^2$ . The essence of my confusion is whether or not these $\theta$ 's are unique, or if a collection of functions can be regarded as eigenfunctions if they only converge as a distribution- eg. the cosines above will not converge to $Lu$ , but we can prove that they will as a distribution.","['eigenvalues-eigenvectors', 'distribution-theory', 'linear-algebra', 'functional-analysis', 'linear-transformations']"
4242496,"$f$ surjective iff for every $g:B \rightarrow C, h:B \rightarrow C$ exist $g \circ f=h \circ f \implies g=h.$","$ f: A \rightarrow B.$ Prove / Disprove: $f$ surjective iff for every $g:B \rightarrow C, h:B \rightarrow C$ exist $g \circ f=h \circ f \implies g=h.$ My solution : One direct: $f \space surjective \implies \forall \space b\in B \space \exists \space a\in  A \space 
   f(a)=b.$ Suppose $g \neq h$ so $\exists \space b \in B$ so $g(b)\neq h(b) $ . Lets take $a \in A$ that $f(a)=b.$ $(g \circ f)(a)=g(f(a))= g(b) \neq h(b) = h(f(a)) =  (h \circ f)(a)$ Contradiction to $g \circ f=h \circ f. $ Second direct: $g \circ f=h \circ f \implies g=h \implies  $ Suppose $f$ is not surjective so $\exists b\in B$ that $\forall \space a \space f(a) \neq b. $ And the condition of $g \circ f=h \circ f \implies g=h  $ doesnt exist for every $g,h : B 
   \rightarrow C$ , because $(g \circ f)(a)$ and $(h \circ f)(a)$ dont defined I am not sure about the second direct. Is my proof correct ?",['elementary-set-theory']
4242529,Existence and uniqueness in a vector-field,"Let $f: [0,1]^n \rightarrow \mathbb{R}_+^n$ be a vector-field where the function $f_i$ for each dimension $i$ is defined as: $$
f_i(\vec{x}) = \sqrt{ \sum_{j=1}^n x_i \cdot x_j \cdot C_{i,j}}
$$ Where we are given the $n \times n$ matrix denoted $C$ with elements $C_{i,j} \in [0,1]$ for all $i$ and $j$ , and the diagonal equals 1 so $C_{i,i}=1$ , and the other elements are symmetrical so $C_{i,j} = C_{j,i}$ . We are also given $\vec{a} = [a_1, a_2, .., a_n]$ with $a_i \in [0,1]$ for all $i$ . Question 1: Does a solution $\vec{x} \in [0,1]^n$ exist that causes $f(\vec{x})$ to equal $\vec{a}$ ? Question 2: Is $\vec{x}$ unique? Question 3: How can we find $\vec{x}$ ? Please explain in a manner that can be understood by amateur mathematicians like myself. Note that I have found a couple of numerical algorithms to solve this problem, one is shown in another question , and another way is to derive the inverse of $f_i(\vec{x})$ and use that to find each $x_i$ that causes $f_i(\vec{x}) = a_i$ , but this causes $f_j(\vec{x}) \neq a_j$ for the other dimensions $j$ , so we need to do this for several iterations and then it converges to the correct solution. However, I am having trouble formally proving the convergence of those algorithms, so I would like to know if we can at least claim that a unique solution even exists, using only mathematical arguments? Thanks!","['multivariable-calculus', 'functions']"
4242534,Inverse of integral transform $f(s)=\int_0^\infty g(x) \exp(-s g(x)) \mathbb{d}x$,"Given $g(x)$ defined for positive reals, say $f(s)$ is defined as below $$f(s)=\int_0^\infty g(x) \exp(-s g(x))  \mathbb{d}x.$$ Is there a relationship to named integral transforms, or a generic approach to obtain $g$ from $f$ ? For instance, we can show the following holds through trial and error ( notebook ) $$
\begin{align}
f(s) & = \frac{\sqrt{\pi }\ \text{erf}\left(\sqrt{s}\right)}{2 \sqrt{s}} &g(x)&=\frac{1}{(1+x)^2}\\
f(s)& = \frac{\Gamma \left(\frac{2}{3}\right)-\Gamma \left(\frac{2}{3},s\right)}{3 s^{2/3}} &g(x)&=\frac{1}{(x+1)^3}\\
f(s)& =\frac{-\cosh (s)+\sinh (s)+1}{s}  &g(x)&=\frac{1}{e^{x}}\\
f(s)&=\frac{e^{\left.\frac{1}{4}\right/s} \sqrt{\pi } (2 s+1)}{4 s^{5/2}} &g(x)&=\log ^2(x)\\
f(s) & = \frac{e^{-s} \left(-s+e^s-1\right)}{s^2} & g(x)&=\text{max}(0, 1-x)\\
f(s) & = \frac{1}{(s-1)^2} &g(x)&=\log(1+x)\\
f(s)&=\frac{e^{-s} (s+1)}{s^2} &g(x)&=1+x\\
\end{align}.$$ Edit Sep 7 Partial solution below works for increasing $g(x)=x+1$ , but not for decreasing ones like $g(x)=(1+x)^{-2}$ Motivation: if loss after running $s$ steps of gradient descent steps on quadratic $Q$ decays as $f(s)$ , then $i$ th eigenvalue of Q quadratic decays as $g(i)$ , hence knowing shape of loss curve over time would let us infer shape of quadratic $Q$ ( background )","['integration', 'laplace-transform', 'complex-analysis', 'mellin-transform', 'integral-transforms']"
4242559,Identifying a polynomial by its value at specific points,"I am writing this question as a self-answer, because I suspect, although I
do not know for sure, what the OP (i.e. original poster) was trying to ask
in this question which is now closed (make that now deleted). I had intended to give a long-winded response comment, in an answer box, but the question was closed (and then deleted). Further, the question that I think that the OP was trying to ask seems worthwhile. So I will guess what the question was, present the question below, and then provide
an answer.  Note that I might be misinterpreting the OP's actual intent. The Problem : Find a polynomial $f(y)$ in the variable $y$ that satisfies the
following $3$ constraints: $f(1) = 12.$ $f(2) = 6.$ $f(3) = 18.$ Note: before I posted, I searched for duplicates, and found this question .
However, that question didn't provide an explicit step-by-step solution.
Therefore, I feel that this question is not precisely a duplicate.",['algebra-precalculus']
4242562,"Number of monotonically increasing functions from the set $\{1,2,3,4,5,6\}$ to itself with the property that $f(x)\ge x$","While working on P&C problems, I stumbled upon this problem. The number of monotonically increasing functions from the set $\{1,2,3,4,5,6\}$ to itself with the property that $f(x)\ge x$ for all x, is equal to $\frac{2}{k}.C^{11}_5$ , where k equals...? Using dummy variables I could figure out number of non-decreasing functions as $C^{11}_6$ , but it doesn't take into consideration the condition $f(x) \ge x.$ How to proceed further?","['permutations', 'combinatorics']"
4242572,Isomorphisms in tangent bundles,"Given the tangent bundle $T$ , is the following isomorphism $$\Lambda^{d-1}T\simeq T^*\otimes\Lambda^dT^* $$ true on a $d$ -dimensional manifold? i.e. are $(d-1)$ -vectors equivalent to objects which are tensor products of a 1 form and a $d$ form? And if so, why?","['fiber-bundles', 'tangent-bundle', 'differential-geometry']"
4242578,How to cram a segment and a triangle as tightly as possible?,You are given a triangle and a segment on a plane. The segment is longer than any of the triangle's sides. How can you translate and/or rotate the triangle or the segment such that the resulting convex hull has minimal area and the segment doesn't cut the triangle? The segment has to be tangent to the triangle otherwise moving it towards the triangle until it collides with it will reduce the area of the convex hull. I thought it would be pretty simple to prove that the segment would have to overlap one of the triangle's sides but failed so far. Could you help me to prove or disprove this?,['geometry']
4242580,On reduced functors and closed complements of open subfunctors,"In the following, I adopt the functorial perspective on schemes. That is, for any ring $R$ , the category of $R$ -schemes is considered a (full) sub-category of the category $\mathcal{C}$ of all functors from $R$ -algebras to sets, affine schemes being those functors isomorphic to $\operatorname{Hom}(A,-)$ for some $R$ -algebra $A$ . The objects of $\mathcal{C}$ are called $R$ -functors for short. Many definitions and basic results on $R$ -schemes naturally generalize to $R$ -functors. For example, an open (resp. closed) subfunctor of some $R$ -functor $F$ is, by definition, a subfunctor $U$ such that for any $R$ -algebra $A$ and for any morphism $\varphi \colon \operatorname{Hom}(A,-) \to F$ , the preimage $\varphi^{-1}(U)$ is open (resp. closed) in $\operatorname{Hom}(A,-)$ in the usual sense. It is easy to show that any closed subfunctor $C \subseteq F$ has a unique open ""complement"" $O \subseteq F$ , where complementary means that $C \cap O = \emptyset$ and $C(K) \cup O(K) = F(K)$ for all fields $K$ (in the category of $R$ -algebras). Question 1: Does the converse hold? That is: does any open subfunctor have a (non-unique) closed complement? Looking at the sub-category of schemes, there is a canonical construction for closed complements as we know that any open subscheme has a unique reduced closed complement. Having the notion of a reduced functor would certainly help a lot for proving the converse true. This leads me to the following question: Question 2: Is there a meaningful generalization of reduced schemes to $R$ -functors? Let's see an example: Let $F$ be the $R$ -functor of all elements of finite multiplicative order, that is, $$ F(A) = \{ x \in A : x^n = 1 \text{ for some } n \in \mathbb{N} \} $$ for all $R$ -algebras $A$ .
We define a subfunctor $O \subseteq F$ which, roughly speaking, consists of all elements of composite order. More formally: $$ O(A) = \{ x \in F(A) : x^p-1 \in A^\times \text{ for all primes } p \}. $$ Using the definition mentioned above, one can easily show that $O$ is open in $F$ (although $O$ is defined by an infinite intersection, the pre-image of $O$ into any affine scheme becomes a finite intersection of open subschemes, which is open again). For any field $K$ , the set $O(K)$ indeed consists of all elements of composite order in $K$ . So if there is any closed complement $C$ of $O$ in $F$ then $C(K)$ would be the set of all elements of prime order in $K$ - together with $1 \in K$ . I have not found such a complement so far. Question 3: Does $O$ have a closed complement in $F$ ? Update 1 (some thoughts) Both open and closed subfunctors of an $R$ -functor $F$ can be regarded as compatible collections of ideals. More concretely, any open subfunctor of $F$ is equivalent to the assignment of radical ideals $I_{A,a} \subseteq A$ for all $R$ -algebras $A$ and all elements $a \in F(A)$ such that for all morphisms $f \colon A \to B$ of $R$ -algebras, we have $\sqrt{\langle f(I_{A,a}) \rangle} = I_{B,F(f)(a)}$ . Let's call such an assignment an open ideal collection of $F$ . The corresponding open subfunctor $O$ of $F$ is given by $O(A) = \{ a \in F(A) : I_{A,a} = A \}$ . Analogously, a closed subfunctor of $F$ is equivalent to the assignment of ideals (not necessarily radicals) $I_{A,a} \subseteq A$ for all $R$ -algebras $A$ and all elements $a \in F(A)$ such that for all morphisms $f \colon A \to B$ of $R$ -algebras, we have $\langle f(I_{A,a}) \rangle = I_{B,F(f)(a)}$ .
Let's call such an assignment a closed ideal collection of $F$ .
The corresponding closed subfunctor $C$ of $F$ is given by $C(A) = \{ a \in F(A) : I_{A,a} = 0 \}$ . From this point of view, we immediately see that closed subfunctors have open complements, as we can canonically transform closed ideal collections into open ideal collections by simply taking radicals. If all open subfunctors would have closed complements, and if there were such a thing as reduced functors, we should also be able to canonically transform closed ideal collections into open ideal collections. For example, maybe an open collection $(I_{A,a})$ could be transformed into a closed collection by defining $J_{A,a}$ to be those ideal generated by $f(I_{B,b})$ for all morphisms $f \colon B \to A$ with $F(f)(b) = a$ ? Concerning reduced functors, I actually think we could define the reduction of $F$ as the intersection of all closed subfunctors $C$ of $F$ such that $C(K) = F(K)$ for all fields $K$ . Then, as usual, a functor would be reduced if it coincides with its reduction. Can someone confirm that this definition generalizes reduced schemes? Update 2 (Reduction of question 1 to a single functor!) My previous thoughts (update 1) can be reformulated in a very neat way. Let us consider two specific $R$ -functors. For any $R$ -algebra $A$ , let $\mathcal{R}(A)$ be the set of radical ideals of $A$ , and let $\mathcal{I}(A)$ be the set of ideals of $A$ . For any morphism $f \colon A \to B$ of $R$ -algebras, let $\mathcal{R}(f) \colon \mathcal{R}(A) \to \mathcal{R}(B)$ be the map $I \mapsto \sqrt{\langle f(I) \rangle}$ , and let $\mathcal{I}(f) \colon \mathcal{I}(A) \to \mathcal{I}(B)$ be the map $I \mapsto \langle f(I) \rangle$ . So both $\mathcal{I}$ and $\mathcal{R}$ are $R$ -functors. We obviously have a canonical morphism $\phi \colon \mathcal{I} \to \mathcal{R}$ given by $I \mapsto \sqrt{I}$ . My previous considerations show that an open subfunctor of a functor $F$ is actually the same as a morphism $F \to \mathcal{R}$ . Similarly, a closed subfunctor of $F$ corresponds to a morphism $F \to \mathcal{I}$ . Any closed subfunctor of $F$ given by $F \to \mathcal{I}$ can be transformed to its unique open complement by composition with $\phi$ . Conversely, given any open subfunctor of $F$ corresponding to $\alpha \colon F \to \mathcal{R}$ , finding a closed complement means finding a morphism $\beta \colon F \to \mathcal{I}$ such that $\alpha = \phi \circ \beta$ . Of course this characterization holds for $F = \mathcal{R}$ in particular. In this way we get an equivalent reformulation to question 1: Question 1': Does the canonical morphism $\mathcal{I} \to \mathcal{R}$ have a section? Note that the identity morphism of $\mathcal{R}$ defines an open subfunctor of $\mathcal{R}$ , namely $O(A) = \{A\}$ for all $R$ -algebras $A$ . So we can reformulate question 1 even further so that there is only one functor involved: Question 1'': Does the open subfunctor $O \subseteq \mathcal{R}$ have a closed complement? This is really surprising to me: Every open subfunctor of every $R$ -functor has closed complements if and only if one specific open subfunctor of one specific functor has closed complements! However, I doubt that this is the case: If there was a closed complement, there would also be a canonical closed complement (the intersection of all closed complements). So there would be a canonical section of $\phi \colon \mathcal{I} \to \mathcal{R}$ . This appears to me as highly unlikely...","['functors', 'category-theory', 'algebraic-geometry', 'abstract-algebra', 'schemes']"
4242603,What will be the maximum value of expression: 7sin²x + 5cos²x +√2[sin(x/2) + cos(x/2)]?,"This is what I've managed to do: $=5 + 2\sin²x +√2[\sin(x/2) + \sin(π/2 - x/2)]$ $=5 + 2\sin²x + √2[2.\sin((x/2 + π/2 -x/2)/2)·\cos((x/2 - π/2 + x/2)/2)]$ $=5 + 2\sin²x + √2[2\sin(π/4)\cos(x/2 - π/4)]$ $=5 + 2\sin²x + 2\cos(x/2 - π/4)$ From here I've tried to reduce everything to $\sin(x/2)$ and $\cos(x/2)$ but unfortunately I can't do anything further.
Please help and provide a solution.",['trigonometry']
4242608,Applying binomial expansion and using algebra to find the value of $a+b$,"Q) In the expansion of $f(x)=(1 + ax)^4 (1 + bx)^5$ where $a$ and $b$ are positive integers,
the coefficient of $x^2$ is 66.
Evaluate $a+b$ . My working: After expanding the expression I simplified it and got $5b^2+10ab+3a^2=33$ After further simplification, I managed to get $5(a+b)^2-2a^2=33$ but I'm not sure what to do next.",['algebra-precalculus']
4242617,How is sigma-algebra related to topology?,"The axioms of $\sigma$ -algebra and of open sets seem quite similar to me. Topology has subsets closed under finite intersection and arbitrary union. $\sigma$ -algebra has subsets closed under complementation and under countable union (implying it is also closed under countable intersections).
The definition of a $\sigma$ -algebra also implies that it also includes the empty subset. So both definitions include empty set and the whole set. My question is: does it even make sense to try to relate these concepts? I know $\sigma$ -algebra from measure theory, while in topology, we do not measure things, but perhaps measurable spaces and topological spaces could share some interesting properties? Remark: I am using the definition of topology via open sets.","['general-topology', 'measure-theory']"
4242624,"Given i.i.d. random variables $X, Y \sim N(0, 1)$, find the conditional distribution of $X$ given that $X + Y > 0.$","Given i.i.d. random variables $X, Y \sim N(0, 1)$ , find the conditional distribution of $X$ given that $X + Y > 0.$ First approach: Find the conditional CDF of $X$ given $X+Y >0$ : $F_X(x | X+Y >0 ) = \mathbb{P}[X<x | X+Y > 0 ] = \mathbb{P}[X+Y >0 | X<x] \cdot \frac{\mathbb{P}[X<x]}{\mathbb{P}[X+Y >0 ]}$ . Notice that $X+Y \sim N(0,2)$ and so $\mathbb{P}[X+Y >0 ] = \frac{1}{2}$ However I cannot seem to calculate nicely $\mathbb{P}[X+Y >0 | X<x]$ Is there a better way to approach this problem? Can someone calculate $\mathbb{P}[X+Y >0 | X<x] $ , I am going to differentiate it anyways so It can be as an integral.","['statistics', 'probability-distributions', 'bayesian', 'probability']"
4242631,"Show $\{y \mid \limsup\limits_{i \rightarrow \infty}d(x_i,y) \leq 1\}$ and $\{y \mid \liminf\limits_{i \rightarrow \infty}d(x_i,y) \leq 1\}$ are Borel","I was struggling immensely to show that $\{y \mid \limsup\limits_{i \rightarrow \infty}d(x_i,y) \leq 1\}$ and $\{y \mid \liminf\limits_{i \rightarrow \infty}d(x_i,y) \leq 1\}$ are Borel. My first attempt consisted of falsely writing: $\{y \mid \limsup\limits_{i \rightarrow \infty}d(x_i,y) \leq 1\}=\bigcap\limits_{N=1}^{\infty}\bigcup\limits_{n \geq N}\{y \mid d(y,x_n) \leq 1\}$ and $\{y \mid \liminf\limits_{i \rightarrow \infty}d(x_i,y) \leq 1\}=\bigcup\limits_{N=1}^{\infty}\bigcap\limits_{n \geq N}\{y \mid d(y,x_n) \leq 1\}$ . However, people were giving me counterxamples to show they are not true. My question is, how can I properly write the sets $\{y \mid \limsup\limits_{i \rightarrow \infty}d(x_i,y) \leq 1\}$ and $\{y \mid \liminf\limits_{i \rightarrow \infty}d(x_i,y) \leq 1\}$ as union/intersections of open/closed sets in order to show they are Borel?","['elementary-set-theory', 'real-analysis']"
4242744,Circles and Sine Waves,All examples I've come across use unit circles to relate with sine waves. How about circles that are not unit circles? Won't the amplitude of the sine wave not correspond to the vertical displacement of the line vector that is spinning in the circle? Since the radius/line vector is not 1 unit anymore?,"['trigonometry', 'circles']"
4242810,Complex proof of the Fundamental Theorem of Algebra,"I am self-studying Stein's Complex Analysis text, in which he has as proof for the Fundamental Theorem of Algebra. For some reason, something about it is just not clicking. Here is his proof itself (here $P$ is a non-constant polynomial): If $P$ has no roots, then $1/P(z)$ is a bounded holomorphic function. To see this, we can of course assume $a_n \neq 0$ and write $$\frac{P(z)}{z^n} = a_n + \Bigl(\frac{a_{n-1}}{z} + \cdots + \frac{a_0}{z^n} \Bigr)$$ whenever $z \neq 0$ . Since each term in the parentheses goes to 0 as $|z| \rightarrow \infty$ we conclude that there exists $R > 0$ so that if $c = |a_n|/2$ , then $$|P(z)| \geq c|z|^n, \quad \text{whenever } |z| > R$$ In particular, $P$ is bounded from below when $|z| > R$ . Since $P$ is continuous and has no roots in the disc $|z| \leq R$ , it is bounded from below in that disk as well, thereby proving our claim. By Liouville's theorem we then conclude that $1/P$ is constant. This contradicts our assumption that $P$ is non-constant and proves the corollary. How does he conclude that there exists $R > 0 $ so that if $c = |a_n|/2$ then $|P(z)| \geq c|z|^n$ for $|z| > R$ ? Why is he choosing $c$ as he did? Also, how does he use this to deduce that $P$ is bounded from below?","['complex-analysis', 'proof-explanation', 'polynomials']"
4242821,Different definitions of the tangent space at a point of a smooth manifold: Biduals?,"There are two different definitions of the tangent space $T_pM$ at a point $p$ of a smooth manifold $M$ . Define $T_pM$ as the set of all equivalence classes of curves (smooth functions) $u : (\mathbb R, 0) \to (M,p)$ , where $u \sim v$ if $u, v$ have the same derivative at $0$ . Define $T_pM$ as the set of all derivations $d : C^\infty(M) \to \mathbb R$ at $p$ . Here $C^\infty(M)$ denotes the set of all smooth functions $M \to \mathbb R$ and a derivation at $p$ has the property $d(f\cdot g) = df \cdot g(p) + f(p)\cdot dg$ . Concerning 1. it is worth to mention that having the same derivative at some point $q$ of a smooth manifold $N$ is an equivalence relation on the set of smooth maps $f : N  \to M$ which can be defined without previously introducing the concepts of tangent spaces and  differentials $d_qf : T_qN  \to T_{f(q)}M$ . It works via charts around $q$ and $f(q)$ . It is well-known that both definitions are equivalent. Definition 1. is certainly more intuitive, but its disadvantage is that there is no ""intrinsic"" definition of a vector space structure on $T_pM$ . Only scalar multiplication has an obvious interpretation on the level of curves, addition has to be defined via using charts and observing that each curve with range in an open subset of $\mathbb R^n$ is equivalent to a ""locally linear curve"" determined by a vector $x \in \mathbb R^n$ . Definition 2. is less intuitive, but its advantage is that the vector space structure on $T_pM$ is provided for free. My question: It seems to me that the tangent space in the sense of 2. is something like the bidual of the tangent space in the sense of 1.  My intuition says that $(T_pM)^*$ is something like equivalence classes of smooth maps $M \to \mathbb R$ (i.e. maps in $C^\infty(M)$ , the equivalence relation again being ""same derivative at $p$ ""), thus $(T_pM)^{**}$ would be something like suitable equivalence classes of maps in $(C^\infty(M))^*$ which could be related to derivations. I have no idea how this could be made precice. Perhaps somebody can do this?","['tangent-spaces', 'differential-geometry']"
4242840,Confusion in Cauchy Integral Formula,"I am trying to calculate a simple contour integral in three different ways and am getting three different results. $$\int_{\vert z \vert = 2} \frac{1}{z^2+1}dz$$ Method $1$ : Write $\frac{1}{z^2 + 1} = \frac{1}{(z+i)(z-i)} = \frac{1/(z+i)}{z-i}$ . Since $i \in B_2(0)$ , we can apply the Cauchy Integral Formula to get $$\int_{\vert z \vert = 2} \frac{1}{z^2+1}dz = \int_{\vert z \vert = 2} \frac{1/(z+i)}{(z-i)}dz = 2\pi i f(i)$$ where $f(z) = \frac{1}{z+i}$ . Hence, the integral evaluates to $\pi$ since $f(i) = \frac{1}{2i}$ . Method $2$ : This is almost identical to the above. Write $\frac{1}{z^2 + 1} = \frac{1}{(z+i)(z-i)} = \frac{1/(z-i)}{z-(-i)}$ . Since $-i \in B_2(0)$ , we can apply the Cauchy Integral Formula to get $$\int_{\vert z \vert = 2} \frac{1}{z^2+1}dz = \int_{\vert z \vert = 2} \frac{1/(z-i)}{(z-(-i))}dz = 2\pi i g(-i)$$ where $g(z) = \frac{1}{z-i}$ . Hence, the integral evaluates to $-\pi$ since $g(-i) = -\frac{1}{2i}$ . Method $3$ : Using partial fractions, $\frac{1}{z^2+1} = \frac{1}{(z+i)(z-i)} = -\frac{1}{2i}\left(\frac{1}{z+i} - \frac{1}{z-i}\right)$ . So, $$\int_{\vert z \vert = 2} \frac{1}{z^2+1}dz = -\frac{1}{2i} \left(\int_{\vert z \vert = 2} \frac{1}{z+i}dz - \int_{\vert z \vert = 2}\frac{1}{z-i}dz\right) = 0$$ since both integrals are $2\pi i$ . Which of these three methods is correct and why are the other two wrong? (By symmetry I feel like the third one is correct, though I also think it may be that the first two are correct but somehow represent integrating in opposite directions along the contour.)","['complex-analysis', 'complex-integration']"
4242868,Prove that $\sum_{i=0}^{m}{\binom{n-i}{k}}=\binom{n+1}{k+1}-\binom{n-m}{k+1}$,"Question I need to prove the following expression for $n>m+k$ : $$
\sum_{i=0}^{m}{\binom{n-i}{k}}=\binom{n+1}{k+1}-\binom{n-m}{k+1}
$$ My Solution Define $S$ as a set containing the first $n+1$ positive integers. From $S$ we create subsets with $k+1$ elements. Since $\binom{n-i}{k}$ is the number of subsets with $n-i+1$ as their greatest element, the left hand side of the equation gives the number of subsets where the greatest element is greater than $n-m$ . The right hand side is an alternative method to count such subsets; we count the number of all possible subsets then subtract the number of subsets with all elements less than or equal to $n-m$ Because both left hand side and right hand side count the same objects, they must be equal. I want to know if my solution is correct and if there’s any alternative","['solution-verification', 'combinatorics', 'binomial-coefficients', 'combinatorial-proofs']"
4242885,"Proper name for a ""polytope"" when defined by linear functionals","Is there a name for the set of functions $f:[0,1]\to\mathbb{R}$ which satisfy a finite set of equations $\int_0^1 f(x)w_i(x)dx\geq c_i$ for $i=1,\ldots,n$ ? If $f$ had a discrete domain and the integral was replaced with summation, the feasible set could have been called a polytope ..","['functional-inequalities', 'linear-algebra', 'functional-analysis']"
4242902,"Showing $\int_0^\infty\frac{Li_2\frac{(1-x^4)^2}{(1+x^4)^2}}{1+x^4}dx=\sqrt2\,\Re\left(\int_0^1\frac{Li_2\frac{(1+x^4)^2}{(1-x^4)^2}}{1+x^2}dx\right)$","A beautiful equality: $$\int_{0}^{\infty }\frac{\operatorname{Li}_{2}\left(\frac{(1-x^{4})^{2}}{(1+x^{4})^{2}}\right)}{1+x^{4}}dx=\sqrt{2} \Re \left(\int_{0}^{1 }\frac{\operatorname{Li}_{2}\left(\frac{(1+x^{4})^{2}}{(1-x^{4})^{2}}\right)}{1+x^{2}}dx\right)$$ This question was proposed by Sujeethan Balendran in RMM(Romanian Mathematical Magazine)
I did by using complex number but i didn't by
Real method by using $$\operatorname{Li}_{2}(a)=-a\int_{0}^{1}\frac{\ln x}{1-ax}dx$$ I believe some of you know some nice proofs of this, can you please share it with us?","['integration', 'calculus', 'complex-numbers', 'real-analysis']"
4242913,How would you simplify $\frac{\frac{6y}{y + 6}}{\;\frac{5}{7y + 42}\;}$ using LCD method?,"Peace to all. While in class I was taught to solve complex fractions by an ""alternative method"" in which you would: Multiply the numerator and denominator by the LCD Apply the distributive property Factor and Simplify An example: $\dfrac{4 - \dfrac 6 {x}} {\dfrac 2 {x} - \dfrac 3 {x^2}}$ Multiply the numerator and denominator by the LCD (x^2): $\dfrac{4 - \dfrac 6 {x}} {\dfrac 2 {x} - \dfrac 3 {x^2}}$ Apply the distributive property: $$\dfrac{x^2 × (4) - x^2 × \dfrac 6 {x}} {{x^2} × \dfrac 2x - x^2 × \dfrac 3 {x^2}}$$ Factor and Simplify $$\dfrac{4x^2 - 6x} {2x-3}$$ $$\dfrac{2x(2x - 3)}{2x-3}$$ A:2x This example is very straightforward and simple.  However, when I put it to use it becomes very difficult. For example for the equation: $\dfrac{\dfrac{6y}{(y + 6)}}{\dfrac{5}{(7y + 42)}}$ .  I get $y + 42$ as the LCD.  I'm not too confident of this because when  I begin to distribute I get very large numbers.  How would one apply this to the prior equation?","['fractions', 'algebra-precalculus']"
4242922,"For $a_i \in \mathbb{C}$, does $(1+a_1^k)(1+a_2^k)\cdots (1+a_n^k)=1$ for any positive integer $k$ imply $a_1=\cdots =a_n =0$?","For $a_i \in \mathbb{C}$ , if $(1+a_1^k)(1+a_2^k)\cdots (1+a_n^k)=1$ holds for all positive integers $k$ , does it follow that $a_1=\cdots =a_n =0$ ? In fact, I want to prove $|I+A^k|=1$ for any positive integer $k$ ,where $A$ is a $n \times n$ matrix implies $A$ is nilpotent.","['matrix-equations', 'linear-algebra']"
4242924,Find the area of triangle ABC given area of a subtriangle,"I'm stuck with this question: Given the lines in triangles and the intersection point, I thought there could be some implications from those info. My intuition tells me $\Delta ANP=12$ and $\Delta NBC=24$ , but could that be true? I find no theorems and properties to back that up. I attempted Pick's Theorem, though. However, the calculation didn't turn out well.","['euclidean-geometry', 'triangles', 'area', 'geometry']"
4242965,How do I know when squaring is valid or not?,"Question: The straight-line $x\cos\alpha+y\sin\alpha=p$ intersects the x & y axes at A & B respectively. Considering $\alpha$ as a variable, show that the equation of the locus that the middle point of AB is a part of is $p^2(x^2+y^2)=4x^2y^2$ . Solution: $$x\cos\alpha+y\sin\alpha=p$$ $$\implies \frac{x}{\sec\alpha}+\frac{y}{\csc\alpha}=p$$ $$\implies \frac{x}{p\sec\alpha}+\frac{y}{p\csc\alpha}=1$$ $\therefore$ The coordinates of A & B are $(p\sec\alpha,0)$ & $(0, p\csc\alpha)$ . Moreover, the midpoint of AB is $(\frac{p\sec\alpha}{2}, \frac{p\csc\alpha}{2})$ . Let, $(x,y)$ is the midpoint of any straight line AB. So, $$(x,y)=(\frac{p\sec\alpha}{2}, \frac{p\csc\alpha}{2})$$ Now, $$x=\frac{p\sec\alpha}{2}$$ $$\implies2x\cos\alpha=p$$ $$\implies\cos\alpha=\frac{p}{2x}...(i)$$ Again, $$y=\frac{p\csc\alpha}{2}$$ $$\implies 2y\sin\alpha=p$$ $$\implies \sin\alpha=\frac{p}{2y}...(ii)$$ $(i)^2+(ii)^2:-$ $$\frac{p^2}{4x^2}+\frac{p^2}{4y^2}=1$$ $$\implies p^2(\frac{y^2+x^2}{4x^2y^2})=1$$ $$\implies p^2(x^2+y^2)=4x^2y^2(showed)$$ Now, my problem is with the step when we square and add (i) & (ii). How do we know that we are not introducing extraneous roots? I have a bigger question. How do I know when squaring is valid or not so that I don't have to come to Math SE every time I encounter squaring in my book? This might help you in answering my question.","['locus', 'trigonometry', 'radicals']"
4242967,Definition of a circle on a manifold,"I'm reading Tristan Needham's Visual Differential Geometry and Forms, specifically the start of the book where he's giving a rough intuitive idea of what non-Euclidean geometry is. He gives the example of a crookneck squash: The analogy for a geodesic curve between, say $c$ and $d$ is a string stretched taut between the two points (imagine the string on the inside of the surface), with the disclaimer that it is between two sufficiently close points that we can find a unique length-minimizing geodesic segment. The string analogy makes sense. Now on to defining a circle on such a surface: (the figure) shows how we may then define, for example, a “circle of radius $r$ and centre $c$ ” as the locus of points at distance $r$ from $c$ . To construct this geodesic circle we may take a piece of string of length $r$ , hold one end fixed at $c$ , then (keeping the string taut) drag the other end round on the surface. Maybe I'm nitpicking, but how do we know that the piece of string is length $r$ anyways? Seems like the above definition assumes that we've already measured the string in the ambient space and then use it to construct a circle on the surface. The book states that there's intrinsic and extrinsic geometry. How would we construct a circle in intrinsic geometry? Since in that case, we only have the surface to work with and nothing else - no ambient space.",['differential-geometry']
4243005,Prove $\int_{-\infty}^{\infty} e^{2x}x^2 e^{-e^{x}}dx=\gamma^2 -2\gamma+\zeta(2)$,I was having trouble with this integral Prove $$\int_{-\infty}^{\infty} e^{2x}x^2 e^{-e^{x}}dx=\gamma^2 -2\gamma+\zeta(2)$$ Where $\gamma$ is the euler mascheroni constant. Let $u=e^x\rightarrow \ln(u)= x$ thus $du=e^x dx$ $$\int_{-\infty}^{\infty} e^{2x}x^2 e^{-e^{x}}dx= \int_{0}^{\infty} u\ln^2(u) e^{-u}du$$ Now $$\Gamma(n)=\int_0^\infty x^{n-1}e^xdx$$ $$\Gamma'(n)=\int_0^\infty x^{n-1}e^x\ln(x)dx$$ $$\Gamma''(n)=\int_0^\infty x^{n-1}e^x\ln^2(x)dx\Rightarrow \Gamma''(2)=\int_0^\infty x e^x\ln^2(x)dx$$ How do I proceed from here? Do I have to use $\Gamma'(n)=\psi(n)\Gamma(n)$ ? How do I solve this integral ? Thank you for your time,"['integration', 'improper-integrals', 'real-analysis', 'calculus', 'riemann-zeta']"
4243045,"Asymptotic behavior of $\int_0^\pi\int_0^R e^{-at\cos\theta}\sin^2\theta J_0(at\sin\theta)\sin\theta \, \mathrm{d}t\mathrm{d}\theta$ as $R \to \infty$","Consider the two following functions defined by double integrals \begin{align}
\varphi_a (R) &= \int_0^\frac{\pi}{2} \int_0^R e^{-at\cos\theta} \sin^2\theta \,
J_0(at\sin\theta) \sin\theta \, \mathrm{d}t \, \mathrm{d}\theta \, , \\
\psi_a (R) &= \int_0^\frac{\pi}{2} \int_0^R e^{-at\cos\theta} \sin^2\theta \,
 J_1(at\sin\theta) \cos\theta \, \mathrm{d}t \, \mathrm{d}\theta \, ,
\end{align} wherein $a, R \in \mathbb{R}_+^*$ .
It can be shown that $\lim_{R\to\infty} \varphi_a = 2/ (3a)$ and that $\lim_{R\to\infty} \psi_a = -1/ (3a)$ But how do $\varphi_a(R)$ and $\psi_a(R)$ behave asymptotically to leading order as $R \to \infty$ ? Any help or suggestions are highly appreciated. Thank you,","['integration', 'asymptotics', 'real-analysis', 'complex-analysis', 'indefinite-integrals']"
4243048,Pointwise prove.,"Prove that $$f_n =\begin{cases} n\sin(nx) &\text{for} \space 0 \leq nx \leq \pi\\ 0 & \text{otherwise}\end{cases}$$ converges pointwise to 0 as $n \to \infty$ , being n a integer and for all x satisfying the properties. I retired this question from a book about complex numbers/functions exercise. Now i just get a counterpoint, see: Adote $x = \pi/2n$ , so that $f_n = nsin(\pi/2)=n$ . Now this certainly does not tends to 0. What am i doing wrong? I mean, i just found a counterprove to what i should prove, so i think or the enunciate is wrong or am i wrong, so, if i am wrong, where is my error?","['complex-analysis', 'limits']"
4243057,Is a rational function which maps all circles/lines to circles/lines a Möbius transformation?,"It is well-known that Möbius transformations map circles and lines to circles and lines. (Here and in the following, “line” means a line in the extended complex plane $\hat{\Bbb C}$ , including the point at infinity.) My question is if that property characterizes Möbius transformations, i.e. if the following converse statement is true: Let $f$ be a rational function which maps circles and lines to circles and lines. Then $f$ is a Möbius transformation. Just to be clear: The property is that if $C$ is a circle or a line then $f(C)$ is also a circle or a line. If we consider $f$ as a mapping of the Riemann sphere onto itself then it means that circles on the sphere are mapped to circles. Context In a now deleted question the following was asked: Suppose $f$ is a continuous function on the extended complex plane which is analytic except possibly at one point and maps lines and circles to lines and circles. Does it follow that $f$ is necessarily a Möbius transformation? It is not difficult to see that under those conditions, $f$ has a removable singularity at the possible exception point, so that it is analytic in all of the extended complex plane, and therefore a rational function. That leads to the above question. This is posted as a self-answered question because I figured out a solution which seems not to be posted before. Of course other answers are most welcome. A previous related questions is Is a function that maps circles to circles necessarily a Möbius transformation? where the following examples are given: a finite Blaschke product maps the unit circle onto itself. The functions $z \mapsto z^p$ map all lines through the origin to lines, and all circles with center at the origin to circles. That are not counterexamples to the above conjecture because not all circles and lines are mapped to circles and lines.","['complex-analysis', 'mobius-transformation']"
4243068,Decomposing the Fubini-Study metric - Irreducibility of $\mathbb{CP}^n$,"I was wondering whether $(\mathbb{CP}^n, g_{FS})$ , where $g_{FS}$ denotes the Fubini-Study metric, is holonomy irreducible, that is, whether or not the tangent space can be split up into subspaces invariant under the holonomy action. I computed that the holonomy group is $U(n)$ . My line of thinking was the following: If $(\mathbb{CP}^n, g_{FS})$ would not be irreducible, then the de Rham Theorem would imply (because $(\mathbb{CP}^n, g_{FS})$ is simply connected and complete) that $(\mathbb{CP}^n, g_{FS})$ is globally a product, which in turn implies that its holonomy group is a product. So there are two things that might go wrong: Either $(\mathbb{CP}^n, g_{FS})$ can not be a product manifold, that is, either $\mathbb{CP}^n$ can not even topologically be or the metric $g_{FS}$ can not split; or $U(n)$ can not be split into a product. But $U(n)$ is reducible, so the latter might be possible. What I am left with is the first option and I have currently no idea how to show that that's not possible. So my question is: Why can the Fubini-Study metric $g_{FS}$ not be realized as a product metric, $h_1 + h_2 = g$ ?
Or: Why can $\mathbb{C}\mathbb{P}^n$ not be written as a product?","['complex-analysis', 'holonomy', 'riemannian-geometry', 'projective-space']"
4243073,What is the Frobenius Reciprocity theorem saying?,I am studying representations of Lie groups and I still cannot find the intuition behind Frobenius Reciprocity theorem. Why would group homomorphisms from representation of $G$ to the $\mathrm{Ind}^G_H(V)$ be the same as group homomorphisms from the same representation restricted on $H$ to $V$ ? I think my problem is that I cannot visualize the $\mathrm{Ind}^G_H(V)$ (definition below) and hence don´t understand what role it plays in the relation. The definition of $Ind^G_H(V)$ : Source of the images: Sepanski - Compact Lie Groups,"['lie-algebras', 'algebraic-groups', 'representation-theory', 'group-theory', 'lie-groups']"
4243075,Ideal triangulation of compact 3-manifolds,So I know that the figure-8 knot complement decompose into two ideal tetrahedra. But as far as I‘m concerned the knot complement is a non-compact 3-manifold. Is there an easy example for an ideal triangulation of a compact 3-manifold that admits a hyperbolic structure like the figure-8 knot complement? Is that even possible? And are there any general theorems about triangulation like „All compact 3-manifolds decompose into tetrahedra…“? I try to get an overview about this topic.,"['knot-theory', 'hyperbolic-geometry', 'geometry']"
4243101,Why does this non-stiff ode requires a stiff solver?,"This post is related to another one in physics.stackexchange. But it might be more of a quadrature than a physics problem. The following set of ordinary differential equations describe the motion of two mass points. They are coupled by a couple of geometric constraints yielding in the following equations in $x_1(t), z_2(t)$ (see Theoretische Physik , Bartelmann et al., 1e, 2015): $$
\begin{eqnarray}
m_1 \ddot{x}_1 & = & -\frac{\dot{x}_1^2 + \dot{z}_2^2 - z_2g}{2(x_1^2/m_1 + z_2^2/m_2)}x_1, \\
m_2 \ddot{z}_2 & = & -\frac{\dot{x}_1^2 + \dot{z}_2^2 - z_2g}{2(x_1^2/m_1 + z_2^2/m_2)}z_2 - m_2g.
\end{eqnarray}
$$ Integrating with a Runge-Kutta scheme of order 4, and initial conditions chosen consistently with the constriants to be $\mathbf{x}_0 = (x_{10}, z_{20}) = (0, -L)$ , violates one of the geometric constraints $x_1^2 + z_2^2 - L^2 = 0$ for time $t > 0$ . If integrated with an implicit Runge-Kutta method of the Radau IIA family of order 5, considered a stiff solver, the error is reduced by an order of magnitude of 3. If I consider a characteristic of the system, stating that the expression $E := m/2(\dot{x}_1^2 + \dot{z}_2^2 + (x_1 + z_2)g)$ forms an invariant/is preserved, if $m_1 = m_2 = m$ , and considering the geometric constraint $x_1^2 + z_2^2 = L^2$ , the same system can be expressed as $$
\begin{eqnarray}
m\ddot{x}_1 & = & -\frac{E - (x_1/2 + z_2)g}{L^2}x_1, \\
m\ddot{z}_2 & = & -\frac{E - (x_1/2 + z_2)g}{L^2}z_2,
\end{eqnarray}
$$ i.e. a system of ordinary differential equations which can be split into a decoupled linear, and a nonlinear part. Question: As can be seen from the linear part of the model, the eigenfrequencies are identical. Particularly in the case of $g=0$ , how come I need a stiff solver to account for numerical instabilities then?","['numerical-methods', 'ordinary-differential-equations']"
4243102,Is my second order ODE solution correct?,"$$xy'' + 2y' - xy = e^x$$ Now, I solved the homogenous equation correctly using reduction of order, I even verified my solution on wolframalpha. $$y_h = \frac{e^xC_1}{x} + \frac{e^{-x}C_1}{x}$$ However, next I tried to find the particular solution using variation of parameters. I calculated that the wronskian is $$W = \frac{-2}{x^2}$$ and that $$W_1 = -\frac{1}{x^2}$$ and $$W_2 = \frac{e^{2x}}{x^2}$$ This means that $C_1' = \frac{1}{2}$ and $C_2' = -\frac{e^{2x}}{2}.$ So that $$C_1 = \frac{x}{2} + K_1$$ $$C_2 = -\frac{e^{2x}}{4} + K_2$$ Plugging this into my solution I get that $$\frac{K_1 e^x}{x} + \frac{K_2 e^{-x}}{x} + \frac{e^x}{2} - \frac{e^x}{4 x}$$ However, the WolframAlpha solution is $$\frac{K_1 e^x}{x} + \frac{K_2 e^{-x}}{x} + \frac{e^x}{2}$$ Where did I go wrong? I tried to find my mistake so I can't rule out an error made due to lack of concentration but I seriously can't find it.","['calculus', 'ordinary-differential-equations']"
4243108,The way to prove that $σ(a) = 3^k$ has no solution?,"$\sigma(n)$ = sum of divisors of n is a divisors function . How to prove there are no such $a$ and $k \ge 2$ satisfy $\sigma(a) = 3^k$ . This proplem can be simplify to the case when $a$ is a power of prime ( $a=p^\alpha$ ) because if $a = p_0^{\alpha_{0}}p_1^{\alpha_{1}}...p_n^{\alpha_{n}}$ , then $$\sigma(a) = \sigma(p_0^{\alpha_{0}})\sigma(p_1^{\alpha_{1}})...\sigma(p_n^{\alpha_{n}})=\prod_{i=0}^n \frac{p_i^{\alpha_i + 1} - 1}{p_i - 1}$$","['number-theory', 'elementary-number-theory', 'divisor-sum', 'discrete-mathematics', 'prime-numbers']"
4243156,"Prove (only with definition) that $f(x)=\frac{x^3+1}{x^2}, x>0$ is not uniformly continuous","I want to prove (only with definition) that $f(x)=\dfrac{x^3+1}{x^2}, x>0$ is not uniformly continuous. Obviously, $f$ has the problem near at zero. I suppose that $f$ is uniformly continuous. So, for all $\epsilon >0$ , exists some $\delta>0$ , such that if $x,y>0, |x-y|<\delta\Rightarrow |f(x)-f(y)|<\epsilon$ .
I tried to take some $0<x<\delta$ and $y=x/2$ . Therefore, $|x-y|<\delta$ , and I want to prove that $|f(x)-f(y)| $ is bigger than a fix number $\epsilon >0$ . However,it seems that this choice about $x$ and $ y$ did not work. Any ideas, what's a better choice for $x$ and $ y$ ? Thanks","['calculus', 'uniform-continuity', 'analysis', 'epsilon-delta']"
4243172,How to compute the Laurent expansion of $\frac{1}{(\cos(z)-1)^4}$?,"I am trying to expand the Laurent series of: $$\frac{1}{(\cos(z)-1)^4}$$ About $z=0$ . If I simply expand $\cos(z)-1$ , I get: $$-\frac{z^2}{2}+\frac{z^4}{24}-\frac{z^6}{720}+\frac{z^8}{40320}-\frac{z^{10}}{3628800}+\dots$$ Which is not very useful when I substitute in the original expression, I'd get: $$\frac{1}{\left(-\frac{z^{10}}{3628800}+\frac{z^8}{40320}-\frac{z^6}{720}+\frac{z^4}{24}-\frac{z^2}{2}+\dots\right)^4}$$ And I don't know what to do with that. I tried to first ""expand"" $1/x$ with the following substitution $x=1-y$ which yields: $$1+y+y^2+y^3+y^4+\dots$$ And using that $y=x-1$ , we obtain: $$2- x + (1 - x)^2 + (1 - x)^3 + (1 - x)^4 +\dots$$ But that also didn't went too well. Can you help?","['complex-analysis', 'sequences-and-series']"
4243204,Last attempt about the Cirtoaje's inequality $ x^{\left(2\left(1-x\right)\right)^{k}}+(1-x)^{\left(2\left(x\right)\right)^{k}}\leq 1$,"Claim : Let $0.5-\varepsilon_k\leq x\leq 0.5+\varepsilon_k$ and $0<\varepsilon_k\leq 0.25$ with $k\geq 2$ then prove or disprove that : $$\frac{\left(2^{-\left(2\left(1-x\right)\right)^{k}}\cdot x\right)}{1-2^{\left(k-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(k-1\right)\ln\left(2\right)+1\right)}}\geq x^{\left(2\left(1-x\right)\right)^{k}}$$ Background : Again and I can say it's my last attempt on it I try to show : Let $0<x<1$ and $k\geq 1$ then we have : $$ x^{\left(2\left(1-x\right)\right)^{k}}+(1-x)^{\left(2\left(x\right)\right)^{k}}\leq 1$$ To show it we can directly use the Bernoulli's inequality on $0.5+\beta_k\leq x\leq1$ and use the claim if proved on $0.5-\varepsilon_k\leq x\leq 0.5+\varepsilon_k$ so the two inequalities complements each other .And the last inequality resulting from the claim is really easier than the Cirtoaje's inequality . To finish I have tried to show the claim using logarithm and derivatives without reach the goal Case $k=2$ : As first step I simply use convexity because we have the inequality on $x\in[0.5,0.55]$ : $$\left(\frac{\left(r\left(0.5\right)-r\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+r\left(0.5\right)\right)^{2}\cdot x\leq \left(2^{-\left(2\left(1-x\right)\right)^{2}}\cdot x\right)$$ Where : $$r\left(x\right)=\left(2^{-\left(2^{0.5}\left(1-x\right)\right)^{2}}\right)$$ Then it seems we have on $x\in[0.54,0.55]$ : $$\left(\frac{\left(r\left(0.5\right)-r\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+r\left(0.5\right)\right)^{2}\geq \left(1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}\right)\cdot x^{\left(4\left(1-x\right)^{2}\right)-1}$$ But It's really not convincing...because the derivatives are not equal at $x=0.5$ A better way would be to re-write the inequality as : $$f\left(x\right)=\frac{0.5}{1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}}\geq g(x)=\left(\left(2x\right)^{\left(\left(2\left(1-x\right)\right)^{2}-1\right)}\right)$$ Then take the logarithm ,make the difference and see what happens with : $$\frac{d}{dx}(\ln\left(f\left(x)\right)-\ln\left(g\left(x\right)\right)\right)$$ It seems that the derivative of the difference admits three roots whose at $x=0.5$ .It seems it's also the case in general . We can also substitute $y=2x$ . Last edit in this case ( $k=2$ ) : Using concavity and chord  we have $x\in[0.5,0.55]$ : $\left(\frac{\left(f\left(0.5\right)-f\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+f\left(0.5\right)\right)^{\frac{1}{a\ln^{2}\left(1-x\right)}}\leq \frac{1}{1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}}\quad (I)$ Where : $$f\left(x\right)=\left(\frac{1}{1-2^{\left(2-1\right)\ln\left(2\right)}\left(\left(1-x\right)\cdot2\cdot x\right)^{\left(\left(2-1\right)\ln\left(2\right)+1\right)}}\right)^{a\ln^{2}\left(1-x\right)}$$ And $a$ evaluate as follow let: $$g\left(x\right)=\left(\frac{\left(f\left(0.5\right)-f\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+f\left(0.5\right)\right)^{\frac{1}{a\ln^{2}\left(1-x\right)}}$$ $$h(x)=(g(x))'$$ Then : $$h(0.5)=0$$ Now it seems we have $x\in[0.5,0.55]$ : $$\left(\frac{\left(f\left(0.5\right)-f\left(0.55\right)\right)}{-0.05}\left(x-0.5\right)+f\left(0.5\right)\right)^{\frac{1}{a\ln^{2}\left(1-x\right)}}\cdot0.5\geq  \left(2x\right)^{\left(\left(2\left(1-x\right)\right)^{2}-1\right)}$$ Ps : I change the last edit because before it was useless and not interesting now it's better . PPS: The inequality $(I)$ must be restricted on $x\in[0.5,0.515]$ where the function $f(x)$ seems to be concave . Question : How to prove or disprove the claim ? How to determine $\varepsilon_k,\beta_k$ with some accuracy  ? Thanks for your effort in this sense .","['inequality', 'derivatives']"
4243238,weak convergence of a function in $L^2$,"Suppose that $\textbf{a}:\mathbb R^n \to \mathbb R^n$ is a smooth map. We have a condition on $\textbf{a}$ : $\forall p\in \mathbb R^n,\,|\textbf{a}(p)|\le C\,(1+|p|)$ . Let $U$ be a bounded, open, connected subset in $\mathbb R^n$ . Recall that $H^1(U)=W^{1,2}(U)$ . Suppose $u,v\in H_0^1(U)$ , which is the closure of $C_c^\infty(U)$ in $H^1(U)$ . Let $\lambda>0.$ Now I want to prove that $$\int_U \textbf{a}(\mathrm Du-\lambda \mathrm Dv)\cdot \mathrm Dv \,\mathrm dx \to \int_U \textbf{a}(\mathrm Du)\cdot \mathrm Dv \,\mathrm dx \qquad\text{ as } \lambda \to 0.$$ The main problem which I encounter is that $(\mathrm Du-\lambda \mathrm Dv)(x)$ and $\mathrm Du(x)$ may not be bounded in $U$ . If both of them are bounded, then I can prove the convergence by Holder inequality and the smoothness of $\textbf{a}$ easily because we can use the mean value theorem in $\mathbb R^n$ to dominate the term $\textbf{a}(\mathrm Du-\lambda \mathrm Dv) - \textbf{a}(\mathrm Du)$ . I also thought of the norm continuity of $L^2$ . By the condition on $\textbf{a}$ , we know $\textbf{a}(\mathrm Du)$ is in $L^2(U)$ . So we know $\| \textbf{a}(\mathrm Du+h) -\textbf{a}(\mathrm Du)\|_{L^2} \to 0$ when $|h|\to 0.$ By Holder inequality, we are done. Can we regard $(\lambda Dv)(x)$ as a constant vector $h$ ? If we can, then why? Thanks. Any other method is also welcome. My purpose is to prove that $$\int_U \textbf{a}(\mathrm Du-\lambda \mathrm Dv)\cdot \mathrm Dv \,\mathrm dx \to \int_U \textbf{a}(\mathrm Du)\cdot \mathrm Dv \,\mathrm dx \qquad\text{ as } \lambda \to 0.$$ Thanks again. Edit: Here is a possibly useless condition on $\textbf{a}$ : $\textbf{a}$ is a monotone operator, which means $$(\textbf{a}(p)- \textbf{a}(q))\cdot (p-q)\ge 0$$ for all $p,q\in \mathbb R^n.$","['weak-convergence', 'analysis', 'real-analysis', 'lp-spaces', 'sobolev-spaces']"
4243250,solving trigonometry,Solve $16 \cos^2 x + 6 \sin x = 17$ for $0 < x < 2\pi$ Steps: $16(1-\sin^2x)+6\sin x = 17$ $16\sin^2x-6\sin x + 1 = 0$ let $y = \sin x$ $16y^2 - 6y + 1 = 0$ I was not able to solve this quadratic equation. It has no real roots. What was done incorrectly?,['trigonometry']
4243277,30 points in the plane 12 red segments and 17 blue segments from each point,"Given 30 points in the plane no 3 of which are collinear, we color the segments formed by the pairs of these points red and blue such that: from every point there emerge 12 red segments and 17 blue segments.
How many triangles that have all three sides of the same colour are there? N.B. i think the phrasing of the problem is bad - the number of monochromatic triangles surely cannot be constant right? However it’s rather hard for me to construct two examples with a different number of triangles. Maybe the problem wanted the minimum number of such triangles","['ramsey-theory', 'combinatorics', 'extremal-combinatorics', 'coloring']"
4243307,Expected Value and Variance of Random Variable Divided by Another Random Variable,"Let $N$ be a random variable taking values $1,2,...,n$ , with known probabilities $p_1,p_2,...,p_n$ , where $\sum_i p_i = 1$ . Furthermore let $X \sim binomial(N,\theta)$ . Consider now the estimator $\frac{X}{N}$ and show that $E(\frac{X}{N}) = \theta$ , and $Var(\frac{X}{N}) = \theta(1-\theta)E(\frac{1}{N})$ So far Im struggling to find the expected value. I know that $E(\frac{X}{N}) = E(X) \cdot E(\frac{1}{N}) = n \theta E(\frac{1}{N}).$ The formula for $E(\frac{1}{N})$ is $E(\frac{1}{N}) = \sum_i \frac{1}{i} p_i$ but not sure how to determine this sum. Also not sure what formula to apply to calculate the variance. Would appreciate any help.","['expected-value', 'statistics', 'variance', 'random-variables']"
4243315,Is Riemann integration only concerned with rates of change?,"Consider the Riemann integrable function $f(x)$ . Whenever I see the expression $$
\int_a^b f(x) \ dx
$$ I read this as $$
\int_a^b \frac{dF(x)}{dx} \ dx
$$ where $F(x)$ is the antiderivative of $f(x)$ . In other words, I consider every Riemann integral to be concerned with integrating a ""rate of change"", which in this case is $\frac{dF(x)}{dx}$ . Is this thinking correct? That is, do all Riemann integrals take in a ""rate of change"" as an input?","['integration', 'calculus', 'riemann-integration']"
4243323,Largest possible side of a triangle when $\cos(3P)+ \cos(3Q)+ \cos(3R) = 1$.,"In triangle $PQR$ , $\cos(3P)+ \cos(3Q)+ \cos(3R) = 1$ . Two sides of the
triangle have lengths of $15 cm$ and $18 cm$ . If the length of the
third side of the triangle PQR is $\sqrt{m}$ cm, then the largest
possible value that $'m'$ can take is? At first I thought the information given i.e. $\cos(3P)+ \cos(3Q)+ \cos(3R) = 1$ to be a distraction and I went with the usual way to solve this problem that sum of two sides is always greater than the third side. So; $15+18>\sqrt{m}$ $\Rightarrow m<1089$ So highest value that $m$ can take is $1088$ but this was the wrong answer. Then I used the cosine rule as follows :- $\cos P=\frac{15^2+18^2-m}{2.15.18}$ $\Rightarrow -1 \le \frac{15^2+18^2-m}{2.15.18} \le1 $ $\Rightarrow9 \le m \le1089$ So from here I got the highest value of $m$ to be 1089 but this was also wrong. Now I am stuck as I am not able to think how to solve this question. I think I need to use the given condition some how to get to the answer but how? Please help me on this !!! Thanks in advance !!!","['contest-math', 'geometry', 'maxima-minima', 'geometric-inequalities', 'trigonometry']"
4243328,How do I make sense of combinations with repetition? [duplicate],"This question already has answers here : Combination with repetitions. (9 answers) Closed 2 years ago . I'm a non-math major but have to study a course in probability and statistics for graduate school. I was able to understand permutations with repetition and then permutations without repetition using examples, and then trying to generalize. But I can't seem to find any examples of combinations with repetition that explain how the formula works, without going into multisets and things like that. Help would be much appreciated.","['combinations', 'combinatorics']"
4243333,Weak convergence of conditioned random variable,"I have two sequences of real random variables, $X_n$ and $\mu_n$ . I know that $\mu_n$ converges weakly (in distribution) to a law $\mathcal N(0,1)$ For every $\eta \in \mathbb R$ , the law of $X_n\mid\mu_n=\eta$ converges in distribution to a law $\mathcal N(\eta,1)$ Can I say that the sequence $X_n$ itself converges in distribution to a law $\mathcal N(0,2)$ , which is what we get when $$X|\mu \sim \mathcal N(\mu,1) \qquad \mu\sim \mathcal N(0,1)?$$ For now, the only idea that come to my mind is to use the fact that $X_n$ converges in distribution to a r.v. with the same law of $X$ if and only if $$\forall h: \mathbb R \to \mathbb R,\ h\in C^0_0(\mathbb R)\qquad \mathbb E[h(X_n)]\to \mathbb E[h(X)]$$ but then, one has $$ \lim_n \mathbb E[h(X_n)] = \lim_n \int_\mathbb R \mathbb E[h(X_n)\mid\mu_n=\eta]dP^{\mu_n}(\eta)$$ and since both the function of $\eta$ and the measure depend on $n$ I don't know how to apply any convergence theorem. Have you got any idea? Edit: What can be relevant, in this case, is the uniform convergence of the sequence of functions $$f_n(\eta) = \mathbb E[h(X_n)\mid\mu_n=\eta]$$ in fact, if we have that $$f(\eta):=\mathbb E[h(\mathcal N(\eta,1))]\qquad  \|f_n-f\|_\infty \to 0$$ then, $$\lim_n \int_\mathbb R \bigg ( \mathbb E[h(X_n)\mid\mu_n=\eta]-f(\eta) \bigg )dP^{\mu_n}(\eta)\le \lim_n \int_\mathbb R \|f_n-f\|_\infty dP^{\mu_n}(\eta)=0$$ Note that, since $f_n$ is uniformly bounded by $\sup h$ and converges pointwise to $f$ by assumption, it is sufficient to prove equicontinuity of $f_n$ and apply Ascoli-Atzerlà theorem. Still, the general question is opened.","['weak-convergence', 'conditional-expectation', 'functional-analysis', 'convergence-divergence', 'probability-theory']"
4243385,"Are the inverse of ""nearly"" diagonal nonsingular matrices also ""nearly"" diagonal?","Assumption: Given a sequence of square matrices $P^k, k=1,2, ..., n$ with $\lim_{k \to \infty} P^k_{ij} = 0$ for $i \neq j$ (but the diagonal element of $P^k$ may not converge). Moreover, the determinant of the sequence is a constant, suppose $\det P_k = 1, k=1,2, ... n$ . Question: Let $U^k = (P^k)^{-1}$ . I am wondering if $\lim_{k \to \infty} U^k_{ij} = 0$ , for $i \neq j$ ? Currently, I think perhaps we can start from the equaiton: $$
U^k_{ij} = \frac{C^k_{ji}}{\det P^k}
$$ , where $C^k_{ji}$ is the $(j, i)$ cofactor of $P^k$ . But I can not go through further. Also, it might by easier if we suppose the diagonal elements of $P^k, k=1,2, ..., n$ are uniformly bounded so that $P^k$ has a convergent subsequence.","['matrices', 'calculus', 'matrix-calculus', 'linear-algebra', 'matrix-equations']"
4243422,Why does the Levi-Civita connection on a flat manifold satisfy the flatness criterion?,"According to Lee's book on Riemannian manifolds, a connection $\nabla$ on a smooth manifold $M$ is said to satisfy the flatness criterion if whenever $X,Y,Z$ are smooth vector fields defined on an open subset of $M$ , the following identity holds: $$\nabla_X\nabla_Y Z-\nabla_Y\nabla_X Z=\nabla_{[X,Y]}Z.\tag{7.3}$$ After introducing this criterion, Lee says: Proposition 7.2. If $(M,g)$ is a flat Riemannian or pseudo-Riemannian manifold, then its Levi-Civita connection satisfies the flatness criterion. Its proof is shown below. Proof. We just showed that the Euclidean connection on $\mathbb{R}^n$ satisfies (7.3). By naturality, the Levi-Civita connection on every manifold that is locally isometric to a Euclidean or pseudo-Euclidean space must also satisfy the same identity. $\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\square$ I think the naturality here refers to the naturality of the Levi-Civita connection on a manifold: Proposition 5.13. Suppose $(M,g)$ and $(\widetilde{M},\widetilde{g})$ are Riemannian or pseudo-Riemannian manifolds, and let $\nabla$ denote the Levi-Civita connection of $g$ and $\widetilde{\nabla}$ that of $\widetilde{g}$ . If $\phi:M\to\widetilde{M}$ is an isometry, then $\phi^*\widetilde{\nabla}=\nabla$ . But I don't know how to apply this proposition in understanding the proof of Proposition 7.2. Thank you.","['connections', 'riemannian-geometry', 'differential-geometry']"
4243466,"$\iiint_M (x+y+z)\,dx\,dy\,dz$ over $M=\{(x,y,z)\in\mathbb{R^3}: 0≤z≤(x^2+y^2)^2≤81\}$","$$\iiint_M (x+y+z)\,dx\,dy\,dz$$ over $M=\{(x,y,z)\in\mathbb{R^3}: 0≤z≤(x^2+y^2)^2≤81\}$ . How would I express this with the correct bounds? Once I have the bounds I can continue on my own but I need the bounds, since this is the very first time encountering this type of boundering with the $M$ . Any hints about the change of bounds would really help.","['definite-integrals', 'bounds-of-integration', 'real-analysis', 'multivariable-calculus', 'multiple-integral']"
4243473,Lebesgue Integral and Lebesgue measure,"So far I have learned the definition of Lebesgue integration, and say we do Lebesgue integration of measurable function $f$ in the measure space $(X,M,\mu)$ . I notice that $\mu$ does not have to be the Lebesgue measure $m$ . Is that correct? In other words, is it ok to understand Lebesgue integration as a general technique and Lebesgue measure as just one tool that we can use for that technique, but the Lebesgue measure is not the only one we can use?","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
4243477,Show that the collection of sets for which the inner measure equals the outer measure $\mu_*(A) = \mu^*(A)$ is a $\sigma$-algebra.,"On space $\Omega$ we have algebra $\mathcal{A} \subset \mathcal{P}(\Omega)$ with measure $\mu: \mathcal{A} \to [0,1]$ and we define the inner measure $\mu_*: \mathcal{P}(\Omega) \to [0,1]$ and outer measure $\mu^*: \mathcal{P}(\Omega) \to [0,1]$ as: \begin{align*}
  \mu_*(A) &= \text{sup} \left\{ \sum\limits_{i=1}^\infty \mu(B_i) : \text{for all disjoint sequences such that} \; B_i \in \mathcal{A}, \; \cup_{i = 1}^\infty B_i \subset A \right\} \\
  \mu^*(A) &= \text{inf} \left\{ \sum\limits_{i=1}^\infty \mu(C_i) : \text{for all sequences such that} \; C_i \in \mathcal{A}, A \subset \cup_{i=1}^\infty C_i \right\} \\
\end{align*} Show that the collection of sets $\mathcal{B}$ where $\mu_*(A) = \mu^*(A)$ is a $\sigma$ algebra. A few properties: Both inner and outer measure are monotonic such that if $A \subset B$ , then $\mu_*(A) \le \mu_*(B)$ and $\mu^*(A) \le \mu^*(B)$ For any $A \in \mathcal{A}$ , $\mu_*(A) = \mu(A) = \mu^*(A)$ such that $\mathcal{A} \subset \mathcal{B}$ . For any $A \subset \Omega$ , $\mu_*(A) \le \mu^*(A)$ . The inner measure is superadditive for disjoint $A_i \subset \Omega$ and the outer measure is subadditive for any $A_i \subset \Omega$ such that: \begin{align*}
  \mu_* \left( \cup_{i \ge 1} A_i \right) \ge \sum_{i \ge 1} \mu_*(A_i) \\
  \mu^* \left( \cup_{i \ge 1} A_i \right) \le \sum_{i \ge 1} \mu^*(A_i) \\
\end{align*} For any $A \in \Omega$ , $\mu_*(A) \le \mu^*(A)$ so, where $A_i$ are disjoint, we have: \begin{align*}
  \sum_{i \ge 1} \mu_*(A_i) \le \mu_* \left( \cup_{i \ge 1} A_i \right) \le \mu^* \left( \cup_{i \ge 1} A_i \right) \le \sum_{i \ge 1} \mu^*(A_i) \\
\end{align*} For any disjoint countable $A_i \in \mathcal{B}$ such that $\mu_*(A_i) = \mu^*(A_i)$ , then $\sum_{i \ge 1} \mu_*(A_i) = \sum_{i \ge 1} \mu^*(A_i)$ such that: \begin{align*}
  \mu_* \left( \cup_{i \ge 1} A_i \right) &= \mu^* \left( \cup_{i \ge 1} A_i \right) \\
\end{align*} Therefore, $\mathcal{B}$ is closed over countable disjoint union. From the definitions of inner/outer measure, for any such sequences $B_i, C_i \in \mathcal{A} \subset \mathcal{B}$ , we have: \begin{align*}
  \cup_{i = 1}^\infty B_i \subset A \subset \cup_{i=1}^\infty C_i \\
\end{align*} With monotonicity of both the inner and outer measures: \begin{align*}
  \mu_*(\cup_{i = 1}^\infty B_i) &\le \mu_*(A) \le \mu_*(\cup_{i=1}^\infty C_i) \\
  \mu^*(\cup_{i = 1}^\infty B_i) &\le \mu^*(A) \le \mu^*(\cup_{i=1}^\infty C_i) \\
\end{align*} Combining with $\mu_*(A) \le \mu^*(A)$ and also $\mu_*(B_i) = \mu^*(B_i)$ and $\mu_*(C_i) = \mu^*(C_i)$ \begin{align*}
  \mu_*(\cup_{i = 1}^\infty B_i) = \mu^*(\cup_{i = 1}^\infty B_i) \le \mu_*(A) \le \mu^*(A) \le \mu_*(\cup_{i=1}^\infty C_i) = \mu^*(\cup_{i=1}^\infty C_i) \\
\end{align*} For any $E \in \mathcal{B}$ such that $\mu_*(E) = \mu^*(E)$ superadditivity and subadditivity give us: \begin{align*}
  \mu_*(E) + \mu_*(E^c) \le \mu(\Omega) \le \mu^*(E) + \mu^*(E^c) \\
  \mu_*(E^c) \le \mu(\Omega) - \mu_*(E) \le \mu^*(E^c) \\
\end{align*} From there, how do we demonstrate that $\mu_*(E^) = \mu^*(E^c)$ ?
Now, I'm stuck on what to do. I need to show that $\mathcal{B}$ is closed over complement and countable (not necessarily disjoint) union to say that it is a $\sigma$ -measure. BTW, with stackexchange search, I see this question asked twice before with no good solutions: Sets $Q$ such that outer measure $\mu^*$ equals inner measure $\mu_*$ form an $\sigma$-algebra. Sets such that inner measure equals outer measure is a sigma algebra","['measure-theory', 'probability-theory', 'outer-measure']"
4243493,Solve the equation $2\sin{ix} -3i\cos{ix}=3i$,"I have got an answer for this, but I know that it is wrong because it contains $\cos^{-1}\left(-\frac{3}{\sqrt{5}}\right)$ , which is undefined. So I was wondering if the issue is with the method I am using or if I have made a mistake with the numbers. I rewrote the equation as $$3i\cosh{x}-2i\sinh{x}=-3i$$ by using $\cos{ix}=\cosh{x}$ and $\sin{ix}=i\sinh{x}$ . I then compared this to the compound angle formula for $\cos{a+bi}$ , which is: $$r\cos(a+bi)=r\cos{a}\cosh{b}-ir\sin{a}\sinh{b}$$ So, letting $r\cos{a}=3i$ and $r\sin{a}=2$ gives me $a=\tan^{-1}\frac{2}{3i}$ and $r=i\sqrt{5}$ . This means that my original equation becomes: $$\cos\left(\tan^{-1}\left(-\frac{2}{3}i\right)+ix\right)=-\frac{3}{\sqrt{5}}$$ which is impossible because the $\cos$ function will only produce values between $-1$ and $1$ . I can't see anything that is wrong with what I have done so I was wondering if I am missing something.","['complex-analysis', 'trigonometry', 'complex-numbers']"
4243505,Intuition for why triangles have unique incircles,"It's easy to figure out why triangles have unique circumcircles; take two points on a side and look at the family of circles passing through them, only one of which (and one of which always) passes through the other point; I can't produce a similar reasoning for incircles, however. Can someone help me figure it out?","['circles', 'geometry', 'triangles', 'intuition', 'triangle-centres']"
4243512,Does a random set of points in the plane contain a large empty convex polygon?,"Suppose I choose $n$ points uniformly at random from the unit square $[0,1]\times [0,1]$ , obtaining a set of points $S=\{p_1,\ldots, p_n\}\subset [0,1]\times [0,1]$ . Then $S$ may contain subsets which span an empty convex polygon. For example, in the illustration below, we have an empty convex polygon with $6$ corners. A polygon is ""empty"" if it contains no other point of $S$ . Some empty convex hulls may have many corners, some fewer. I am curious about the asymptotic behaviour of this phenomenon. To this end, let $$ f(S)= \max \{|T|:T\subseteq S\text{ and }T\text{ is convex and } S\cap \text{conv}(T)=T\}$$ Question. How does the expectation $\mathbb E[f]$ grow as $n\to\infty$ ? Conjecture. $\mathbb E[f]=\Theta(\sqrt{n})$ , due to the birthday paradox. I think this is a Ramsey Theory type question, but I am not equipped to answer it. I would be happy with either a lower bound or an upper bound, or pointers to finding them.","['geometry', 'polygons', 'combinatorial-geometry', 'ramsey-theory', 'probability-theory']"
4243562,Understanding $\log p/q = \sum_{k=1}^{\infty} \frac{1}{2k-1}(\frac{p-q}{p+q})^{2k-1}$,"Problem This problem comes straight from the Taylor Formula Chapter of Edwin Wilson's Advanced Calculus Textbook : The part that I am concerned with is part $(\gamma)$ . What I need: Either a hint pointing me in the right direction or a guide to a resources that will hellp illustrate why this relationship is true. Personal Work and Ideas: The problem comes down to proving that: $$\log\frac{p}{q} = 2\bigg[\sum\limits_{k=1}^{\infty}\frac{1}{2k-1}(\frac{p-q}{p+q})^{2k-1}\bigg]$$ which on its face seems to be some manipulation of the series $$\text{ (1)   }\log x = \log a + \sum\limits_{k=1}^\infty \frac{(-1)^{k-1}}{k}  (\frac{x-a}{a})^k$$ Origin of This formula of course comes from taylor expansion of $\log x$ and is derived from these applying these two pieces of information: $$f^{(k)}(x) = \begin{cases} 
      \log x & k=0 \\
      \frac{(-1)^{k-1}}{(x)^k}(k-1)!& k>0\\ 
   \end{cases} $$ and $$f(x) = f(a) + \sum\limits_{k=1}^{\infty} f(a)\frac{(x-a)^k}{k!}$$ Some ideas I have tried are the following ( Caution this mostly reads like scratch work. ): IDEA 1 Idea : Set $x=p$ and $a=q$ Motivation: As a first attempt I am trying to see how far simple substitution takes me. So I am going to plug in $x=p$ and $a=q$ to see how far it goes.. Attempt Assume that we plug $x=p$ and $a=q$ in to (1) then we obtain: $$\log (\frac{p}{q}) = \sum\limits_{k=1}^{\infty}\frac{(-1)^{k-1}}{k}(\frac{p-q}{q})^{k}$$ Problems The biggest problems with this approach are two fold. No introduction to $p+q$ as a denominator term. The k-values in parity with what the solution is giving us (all k-values of the solution are odd). IDEA 2 Idea : Sum pairs of summands Motivation: The issue with the parity of the $k$ values seems to be addressed if we sum pairs of items. This possibly might introduce $p+q$ to the denominator. Attempt Let $b_k =\frac{(-1)^{k-1}}{k}(\frac{p-q}{q})^{k} $ then $$b_{2k}+b_{2k+1} = \frac{(-1)^{2k-1}}{2k}(\frac{p-q}{q})^{2k} +\frac{(-1)^{2k}}{2k+1}(\frac{p-q}{q})^{2k+1}   $$ $$=(\frac{p-q}{q})^{2k}\bigg[-\frac{1}{2k} +\frac{\frac{p-q}{q}}{2k+1}\bigg]$$ $$=(\frac{p-q}{q})^{2k}\bigg[\frac{-q(2k+1) +(p-q)(2k)}{(2qk)(2k+1)}\bigg]$$ Problems: There doesn't seem to be a good way that this turns to the denominator to $p+q$ . IDEA 3 Idea: Take the difference of to taylor series. Namely $\log p$ and $\log q$ with the center being at p+q. Motivation The goal of this is to introduce the $p+q$ term to the denominator. Attempt $$\log p = \log(p+q) + \sum\limits_{k=1}^{\infty} \frac{(-1)^{k-1}}{k}  (\frac{p-(p+q)}{p+q})^k$$ or $$\log p = =\log(p+q) + \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}  (\frac{q}{p+q})^k$$ $$\log q=\log(p+q) + \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}  (\frac{p}{p+q})^k$$ which after subtraction gives us this: $$\log \frac{p}{q}= \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}\bigg[  (\frac{q}{p+q})^k-(\frac{p}{p+q})^k\bigg]= \sum\limits_{k=1}^{\infty} \frac{(-1)^{2k-1}}{k}(\frac{1}{p+q})^k\bigg[  q^k-p^k\bigg]$$ Notes: Since this is my current idea and I am muddling through, I will rename the problem section notes. I am not sure if this route will work or not. Might end up having a problem with the coeffients. I am still thinking through possible next steps.","['analysis', 'real-analysis']"
4243576,"If $\lambda_n\to\lambda$ setwisely, can we show $\int f\:{\rm d}\lambda_n\to\int f\:{\rm d}\lambda$?","Let $(E,\mathcal E)$ be a measurable space and $\lambda,\lambda_n$ be measures on $(E,\mathcal E)$ for $n\in\mathbb N$ such that $(\lambda_n)_{n\in\mathbb N}$ is nondecreasing and $$\lambda_n\xrightarrow{n\to\infty}\lambda\tag1$$ (""setwisely""). If $f:E\to[0,\infty]$ is $\mathcal E$ -measurable, are we able to show that $\int f\:{\rm d}\lambda_n\to\int f\:{\rm d}\lambda$ ? Clearly, we can find $\mathcal E$ -measurable $f_k:E\to[0,\infty]$ with $|f_k(E)|\in\mathbb N$ for $k\in\mathbb N$ such that $(f_k)_{k\in\mathbb N}$ is nondecreasing and $$f_k\xrightarrow{k\to\infty}f\tag2.$$ By $(1)$ , $$\lambda_ng_k\xrightarrow{n\to\infty}\lambda g_k\;\;\;\text{for all }k\in\mathbb N\tag3.$$ And by the monotone convergence theorem, $$\int g_k\:{\rm d}\lambda_n\xrightarrow{k\to\infty}\int g\:{\rm d}\lambda_n\tag4\;\;\;\text{for all }n\in\mathbb N;$$ similarly, $$\int g_k\:{\rm d}\lambda\xrightarrow{k\to\infty}\int g\:{\rm d}\lambda\tag5.$$ Can we conclude? Maybe using that $(\lambda_n)_{n\in\mathbb N}$ is nondecreasing?","['measure-theory', 'real-analysis']"
4243580,"Compute $\int_{0}^{1}\int_{0}^{z}\int_{0}^{\sqrt{z^2-x^2}} \dfrac{e^{z^2}}{\sqrt{x^2+y^2}}\, dy\, dx\, dz$ using this change of coorninates.","Compute $\int_{0}^{1}\int_{0}^{z}\int_{0}^{\sqrt{z^2-x^2}} \dfrac{e^{z^2}}{\sqrt{x^2+y^2}}\, dy\, dx\, dz$ using this change of coorninates. I could say that: $0\leq 1, 0\leq x\leq z, 0\leq y \leq \sqrt{z^2-x^2}$ I believe that cylindrical and not spherical coordinates should be used. But the calculation has become difficult for me, I think there are more things to consider. If $x=r\cos\theta, y=r\sinθ, z=z$ , then $$\int_{0}^{1}\int_{0}^{z}\int_{0}^{\sqrt{z^2-r^2\cos \theta}} e^{z^2}\, dr\, d\theta\, dz$$ The limit of integration $\sqrt{z^2-r^2\cos \theta}$ stays like this, or I must do something else.","['iterated-integrals', 'multivariable-calculus', 'multiple-integral']"
4243599,"$A\subset (b,c)\subset \mathbb{R}$ is Lebesgue measurable if and only if $|A|+ |(b, c)\setminus A|= c −b$","I am trying to prove the following statement and I am having difficulties proving the leftward implication so I would appreciate an hint about how to do it: ""Suppose $b < c$ and $A\subset (b, c)$ . Prove that $A$ is Lebesgue measurable if and
only if $|A|+ |(b, c)\setminus A|= c −b$ ."" What I have done: $\fbox{$\Rightarrow$}$ : $(b, c)\setminus A$ and $A$ are disjoint subsets of $\mathbb{R}$ of which $A$ is Lebesgue measurable: since we know that if $A$ and $B$ are disjoint subsets of $\mathbb{R}$ and $B$ is Lebesgue measurable then $|A\cup B|=|A|+|B|$ it follows that $c-b=|(b,c)|=|((b,c)\setminus A)\cup A|=|(b,c)\setminus A|+|A|$ , as desired. $\fbox{$\Leftarrow$}$ : For the leftward implication I have tried to argue by contradiction by using the fact that if $A$ is not Lebesgue measurable then there exists some $\bar{\varepsilon}>0$ such that $|G\setminus A|\geq\bar{\varepsilon}$ for every $G\supset A$ open so in particular $|(b,c)\setminus A|\geq\bar{\varepsilon}$ but I haven't been able to obtain a contradiction and I am now stuck so an hint about how to better understand this problem is welcome. EDIT: I guess that another way to see the leftward implication is to note that $|A|+ |(b, c)\setminus A|=c−b=|(b,c)|=|A\cup (b,c)\setminus A|$ so outer measure is additive and it wouldn't be if $A$ were not Lebesgue measurable (cfr Vitali set). Now, while compelling, this is not a rigorous explanation: how to make it so? Thanks DEF. (Lebesgue measurable set): A set $A\subset\mathbb{R}$ is called Lebesgue measurable if there exists a Borel set $B\subset A$ such that $|A\setminus B|= 0$ . $|\cdot|$ denotes outer measure","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4243608,"Let $G$ be a torsion-free abelian group, prove that for $g\neq h\in G$ there exists a homomorphism $\phi:G\to\Bbb{R}$ such that $\phi(g)\neq\phi(h)$","I have the following question: Let $G$ be a torsion-free abelian group, prove that for every distinct elements $g, h \in G$ there exists a homomorphism $\phi: G \rightarrow \mathbb{R}$ such that $\phi(g)\neq \phi (h)$ . I have some observations, if I take $\phi(g)=a$ and $\phi(h)=b$ , for $a\neq  b\in \mathbb{R}$ and define $\phi(g^n h^m)= na + mb $ for every $m, n\in \mathbb{N}$ , then $\phi: \langle g, h\rangle\rightarrow \mathbb{R}$ is an homomorphism (it is well defined since $G$ is torsion free abelian). Therefore, if $G$ is finitely generated, by a similar agument I can conclude the problem. For the case where $G$ is not finitely generated I don't have any progress yet. I will appreciate any hint! thanks!","['group-homomorphism', 'free-abelian-group', 'torsion-groups', 'abstract-algebra', 'group-theory']"
4243635,Conflicting definitions of tensors?,"I am currently taking a course in continuum mechanics using P. Chadwick's Continuum Mechanics: Concise Theory and Problems and I am having trouble reconciling the following definitions of a tensor: In the book, Chadwick defines a tensor in the following way: A tensor is a linear transformation or the Euclidean vector space $E$ into itself. Since I have some experience using tensors, the definition I am familiar with is: Let $V$ be a vector space and $V^*$ be its duel. Then a $(p,q)$ tensor is defined as $T:\underbrace{V^{*}\times...\times V^{*}}_\text{p-copies}\times \underbrace{V\times...\times V}_\text{q-copies} \to \mathbb{R}$ . In other words, a tensor is a multilinear map . Now that both of these definitions are on the table, here is my question: How does Chadwick's definition relate (if at all) to the definition that I am familiar with? To me his definition isn't very precise because according to what he wrote a tensor should be an object of the form $T : E \to E$ (linearly of course). Is there something that I am not seeing or is this another way of defining tensors? note: this class is centered towards engineers (I am a mathematician by training) if that helps put things in context a little better.","['tensors', 'linear-algebra', 'differential-geometry']"
4243661,"Prove that for one vertex of a convex pentagon, the sum of distances to the other four is greater than the perimeter","The problem is from the journal 'Crux Mathematicorum', originally proposed by Paul Erdős and Esther Szekeres for the case of a convex $n$ -gon with $n > 5$ , and can be found here together with a proof for that case (pdf page 20) . Unfortunately they leave the case $n = 5$ open to the reader, so I would like to know how to prove: Any convex pentagon has a vertex whose sum of distances to the other four vertices is greater than the perimeter of the pentagon. I was not able to extend the method from the pdf above to the $n = 5$ case, because it relies on comparing the perimeter of the pentagon to that of a regular $n$ -gon centered on the centroid of the original polygon and then using the inequality $\sin(\frac \pi n) \leq \frac 1 2 $ , which is not true for $n = 5$ . Thanks in advance for any help! EDIT: I would prefer a proof that would be feasible in the context of a math competition like the IMO or Putnam, but any kind of result is appreciated. One more result that might be helpful: If for two distinct vertices $U$ and $V$ we denote by $s_U$ and $s_V$ the sum of distances from $U$ and from $V$ respectively, one can show that $s_U + s_V > 3\vert UV\vert + p$ , where $p$ is the perimeter of the pentagon and $\vert UV\vert$ is the distance from $U$ to $V$ . Therefore if we had $\vert UV\vert \geq \frac{p}{3}$ , this would give us a proof, so we can assume wlog that the distance between any two vertices is at most $\frac{p}{3}$ . EDIT 2: If we could prove the inequality in this post , we would have a proof using the inequality from the pdf. EDIT 3: A proof of the result from the first edit was requested: Label the vertices $U_1,\ldots, U_5$ such that consecutive vertices have consecutive indices (mod $5$ ), then there are two cases, non-consecutive and consecutive vertices. Case 1: The vertices are non-consecutive, say $U_1$ and $U_3$ . Let $P$ be the point of intersection of the segments $U_1U_4$ and $U_3U_5$ . Using the triangle inequality we get $$
\begin{align}
|U_1P| + |PU_3| &> |U_1U_3|,\\
|U_4P| + |PU_5| &> |U_4U_5| \\
\implies |U_1U_4| + |U_3U_5| &> |U_1U_3| + |U_4U_5| \\
\implies s_{U_1} + s_{U_3} &= |U_1U_2| + |U_1U_3| + |U_1U_4| + |U_1U_5| \\
& \hspace{5mm}+ |U_3U_1| + |U_3U_2| + |U_3U_4| + |U_3U_5|\\
&> 3|U_1U_3| + |U_1U_2| + |U_2U_3| + |U_3U_4| + |U_4U_5| + |U_5U_1| \\
&= 3|U_1U_3| + p.
\end{align}
$$ Case 2: The vertices are consecutive, say $U_1$ and $U_2$ . Just like before we get $$
\begin{align}
|U_1U_3| + |U_2U_4| &> |U_1U_2| + |U_3U_4|,\\
|U_1U_4| + |U_2U_5| &> |U_1U_2| + |U_4U_5| \\
\implies s_{U_1} + s_{U_2} &= |U_1U_2| + |U_1U_3| + |U_1U_4| + |U_1U_5| \\
& \hspace{5mm}+ |U_2U_1| + |U_2U_3| + |U_2U_4| + |U_2U_5|\\
&> 3|U_1U_2| + |U_1U_2| + |U_2U_3| + |U_3U_4| + |U_4U_5| + |U_5U_1| \\
&= 3|U_1U_2| + p. 
\end{align}
$$ $\tag*{$\square$}$","['contest-math', 'geometry', 'polygons']"
4243686,If there is a periodic solution then every solution is periodic with the same period.,"Let $A$ be a $2\times 2 $ real matrix. Suppose the ordinary differential equation $$x'=Ax$$ has a periodic nonconstant solution $\varphi(t,x_0)$ such that $\varphi(0,x_0)=x_0$ for some $x_0 \neq (0,0),$ i.e., there exists $\tau >0$ such that $\varphi(t+\tau,x_0)=\varphi(t,x_0)$ for every $t \in \mathbb{R}.$ Prove that for every initial value $x \neq(0,0)$ we have that the solution $\varphi(t,x)$ is periodic with period $\tau$ . I think I can solve it by separating in 3 cases: $A$ is similar to $\begin{bmatrix}\lambda_1 & 0 \\ 0 & \lambda_2\end{bmatrix}$ , where $\lambda_1, \lambda_2 \in \mathbb{R}$ are the eigenvalues of $A$ . In that case, the solution will be constant, or it will go to infinity or 0. That is, it is not periodic. $A$ is similar to $\begin{bmatrix}\lambda & 1 \\ 0 & \lambda\end{bmatrix}$ , where $\lambda \in \mathbb{R}$ is the eigenvalue of $A$ with multiplicity 2. Similarly, the solution will be constant, or it will go to infinity or 0 depending on the sign of $\lambda$ . $A$ is similar to $\begin{bmatrix}a & -b \\ b & a\end{bmatrix}$ , where $a\pm bi$ are the eigenvalues of $A$ . Then we would conclude that the only way to get periodic solutions is with $a=0$ and $b\neq 0$ . And then we see that in this case all the solutions in fact will be periodic with the same period. I have two questions. Is that solution correct? Is there a cleaner or simpler solution?","['solution-verification', 'ordinary-differential-equations']"
4243762,Possible Solution to Ordered Games Problem,"This a followup to a question I asked here . I am hoping to get some feedback on a possible proof of the claim (showing that the probability of winning the match for either player is equal). Specifically, the ""Reversing Arrows"" section below and any glaring errors. Grid Representation Let us start by visualizing the problem using a grid. Consider the case where $n = 3$ . We have P1 starting on Char 1 and P2 starting on Char 3. We can represent this via the diagram below, where each box represents a possible matchup; moving to the right is a win for P2 and moving down is a win for P1. This means that P1 winning the match is represented by a sequence of down and right moves leading off the bottom of the grid whereas P2 winning the match is represented by a similar sequence leading off the right side of the grid. We also consider any set of probabilities (with the assumption of no 100/0 matchups). Now consider generalizing to a set of unique characters (so every matchup is unique). For convenience, let us label P2's characters 3 to 1 and P1's characters 4 through 6. The grid now looks like the following: Ordered Games BO5 as Motivation Taking a quick detour, let us consider a ""1-D"" version of this problem with ordered games in a best of five. Suppose we have two players (P1 and P2) who play games against each other. They play a match with 5 games (labeled 1 through 5) where each game has a unique probability for P1 or P2 to win (which is where the ordering comes into play). So Game 1 has probability $p_1$ for P1 and $q_1 = 1-p_1$ for P2, Game 2 is $p_2$ and $q_2$ and so on, with the assumption that $p_i$ is not necessarily equal to $p_j$ for all $i$ and $j$ (and standard assumption of no 100/0 matchups). In this situation an interesting symmetry arises: the probability for P1 to win when playing the games in order from 1 to 5 is the same as the probability of P1 winning when playing the games in the reverse order (or any other order for that matter). This is because once P1 has won three games, essentially the rest of the games in that particular match don't matter (playing them doesn't change the fact that P1 has already won). Mathematically this is accounted for in the probability when extending every match to five games and considering all the possible W/L sequences. Connection to Grid Idea (Reversing Arrows) How does this relate to the ""2-D"" version of the problem? Looking at our diagram, we have a parallel to the ""1-D"" example: P1 winning a ""2-D"" match consists of P1 winning valid BO5s (each path corresponding to a particular configuration) starting with the (3,4) matchup (the specific probabilities per game changing based on the W/L record). With this motivation, consider changing the order of the matchups -- specifically, let us reverse the order of the matchups. This amounts to reversing the arrows on the grid in a consistent manner. Instead of starting with P1 on 4 and P2 on 3, we start P1 on 6 and P2 on 1 -- the last possible matchup. In every game, we have two states that we can go to based on the W/L probability in the forward direction. When reversing we have to choose which probability leads to which outcome. For example, winning (5, 1) or losing (6, 2) both lead to the state (6, 1). Reversed, we have winning (6, 1) leads to (5, 1) and losing (6, 1) leads to (6, 2). This assignment counts the paths in the same way since we have the winner switching character. The following diagram shows the original paths on the left and the new reversed paths on the right with probabilities. Now consider the probability that P1 wins in this new grid. When reversing the arrows we must maintain the total probability of P1 winning because (1) every path to victory traverses the same P1 win matchups (the reversed arrows in the vertical direction are the same for every path) and (2) we counted the paths the same way. So the probability of P1 winning in the ""forward"" configuration is the same as the probability of P1 winning in the ""reverse"" configuration. Application to Original Problem Finally we consider our original case (with P1 playing 1 to 3 and P2 playing 3 to 1). It is clear to see that by symmetry, the paths defined by reversing arrows corresponding to P1 winning are exactly the paths that correspond to P2 winning (shown below). By the reverse arrow argument in the previous section, the total probabilities of winning are the same (being a specific case of the general claim above with 4 = 1, 5 = 2, and 6 = 3) and since the sum of their probabilities must add up to 1, the probability of P1 and P2 winning must both be 0.5. Therefore the claim is proved for $n = 3.$ We can extend this analysis to other $n$ with some minor modifications.","['discrete-mathematics', 'combinatorics', 'probability', 'word-problem']"
4243834,Finding smooth behaviour of infinite sum,"Define $$E(z) = \sum_{n,m=-\infty}^\infty \frac{z^2}{((n^2 + m^2)z^2 + 1)^{3/2}} = \sum_{k = 0}^\infty \frac{r_2(k) z^2}{(kz^2 + 1)^{3/2}} \text{ for } z \neq 0$$ $$E(0) = \lim_{z \to 0} E(z) = 2 \pi$$ where $r_2(k)$ is the number of ways of writing $k$ as a sum of two squares of integers . Is $E(z)$ smooth at $z = 0$ , and can we evaluate its derivatives $(\partial_z)^n E(z)|_{z = 0}$ ? Footnote 1: There is a physical motivation for these equations, $E(z)$ is the electric field generated by a $2d$ lattice of point charges where the lattice spacing is $z$ and the charge density is held fixed. $E(0)$ here is the continuum limit. Footnote 2: Here is a plot of the behavior of the sum in the second representation I wrote above from $k = 0$ to $k = N$ , as a function of $z \in [-2,2]$ . The black line is $2 \pi$ . It looks like the limit will be very flat at $z = 0$ (possibly all derivatives vanish? That would be cool)","['summation', 'electromagnetism', 'taylor-expansion', 'sequences-and-series', 'fourier-series']"
4243918,"Prove that $||A||\leq\sqrt{\sum_{i,j}a_{ij}^2}\leq\sqrt{n}\,||A||$. I cannot prove the second inequality.","I am reading ""Multivariable Mathematics"" by Theodore Shifrin. The following exercise is in this book (Exercise 5.1.5 on p.201): Suppose $A$ is an $m\times n$ matrix. Prove that $||A||\leq\sqrt{\sum_{i,j}a_{ij}^2}\leq\sqrt{n}\,||A||$ . In this book, the definition of $||A||$ is as follows: $||A||:=\max_{||\mathbf{x}||=1} ||A\mathbf{x}||$ . I proved the first inequality but I cannot prove the second inequality. My partial solution is here: Let $||\mathbf{x}||=1$ . $$||A\mathbf{x}||=\sqrt{(a_{11}x_1+\dots+a_{1n}x_n)^2+\dots +(a_{m1}x_1+\dots+a_{mn}x_n)^2}.$$ By the Cauchy–Schwarz inequality, $$(a_{i1}x_1+\dots+a_{in}x_n)^2\leq(a_{i1}^2+\dots+a_{in}^2)\cdot(x_1^2+\dots+x_n^2)$$ for any $i$ such that $1\leq i \leq m$ . So, $$||A\mathbf{x}||=\sqrt{(a_{11}x_1+\dots+a_{1n}x_n)^2+\dots +(a_{m1}x_1+\dots+a_{mn}x_n)^2}\leq\sqrt{(a_{11}^2+\dots+a_{1n}^2+\dots+a_{m1}^2+\dots+a_{mn}^2)\cdot(x_1^2+\dots+x_n^2)}=\sqrt{a_{11}^2+\dots+a_{1n}^2+\dots+a_{m1}^2+\dots+a_{mn}^2}=\sqrt{\sum_{i,j}a_{ij}^2}.$$ $$||A||\geq \left\lVert A\begin{bmatrix}
\frac{1}{\sqrt{n}} \\
\vdots \\
\frac{1}{\sqrt{n}} \\
\end{bmatrix}\right\rVert
=
\frac{1}{\sqrt{n}} \left\lVert A\begin{bmatrix}
1 \\
\vdots \\
1 \\
\end{bmatrix}\right\rVert
=
\frac{1}{\sqrt{n}} \left\lVert \begin{bmatrix}
a_{11}+\dots+a_{1n} \\
\vdots \\
a_{m1}+\dots+a_{mn} \\
\end{bmatrix}\right\rVert
=
\frac{1}{\sqrt{n}}\sqrt{(a_{11}^2+\dots+a_{1n}^2+\dots+a_{m1}^2+\dots+a_{mn}^2)+2(a_{11}a_{12}+\dots+a_{1{n-1}}a_{1n})+\dots+2(a_{m1}a_{m2}+\dots+a_{m{n-1}}a_{mn})}.$$ But we cannot say $$2(a_{11}a_{12}+\dots+a_{1{n-1}}a_{1n})+\dots+2(a_{m1}a_{m2}+\dots+a_{m{n-1}}a_{mn})\geq 0.$$","['multivariable-calculus', 'normed-spaces', 'linear-algebra', 'inequality']"
4243956,Functional equation $f(px)+p=[f(x)]^2$,"Let $p\in\mathbb{N}$ and $p>1$ . Consider the functional equation $$f(px)+p=[f(x)]^2$$ I need to find all functions $f:\mathbb{R}\to\mathbb{R}$ that is continuous at $0$ and satisfies above functional equation for all $x\in\mathbb{R}$ . For $p=2$ , it can be showed that there exists a solution, the function $f:\mathbb{R}\to\mathbb{R}$ defined by $f(x)=2\cos(\alpha x)$ where $\alpha\in\mathbb{R}\cup i\mathbb{R}$ is arbitrary. How can we find solutions for $p>2$ ?
It possible to prove the existence since there exists constant solutions.","['nonlinear-analysis', 'functional-equations', 'real-analysis', 'continuity', 'functions']"
4244026,When can we extend a measure from a sub $\sigma$-algebra to the $\sigma$-algebra?,"Let $(X,\mathcal F)$ be a Borel measure space and let $\mathcal G\subseteq F$ be a Borel sub $\sigma$ -algebra (or maybe the topology generating $\mathcal G$ is included in the topology generating $\mathcal F$ ). Suppose that $(X,\mathcal G,\mu)$ is a measure space. Is it always possible to extend $\mu$ to the measure space $(X,\mathcal F,\mu)$ such that for any $\mathcal G$ measurable function $f$ on $X$ , $\mu(f)$ is preserved. Here is my motivation for this problem. If we take the Borel $\sigma$ -algebra generated by a Banach space $X$ to be $\mathcal F$ and the Borel $\sigma$ -algebra generated by the weak topology to be $\mathcal G$ then the constraint is satisfied, now it is easier to get statement like for all measure $\mu$ on $X$ , there is $x\in X$ such that for all continuous affine function $f$ on $X$ , $\mu(f)=f(x)$ (and things about uniqueness) in the weak topology and I want to see what they imply in the strong topology.","['general-topology', 'functional-analysis', 'measure-theory']"
4244041,Supermartingale is also a supermartingale w.r.t. right-continuous filtration,"I came across the following theorem in some online lecture notes. Let $(\Omega, \mathcal{G}, P)$ be a probability space Thereom: If $(X_t)_{t\geq0}$ is a supermartingale with right-continuous paths (a.s.) w.r.t. a filtration $(\mathcal{G}_t)$ , then it is also a supermartingale w.r.t. the filtration $(\mathcal{G}_{t+}) = \cap_{s > t}\mathcal{G_s}$ . The proof given goes like this: Proof :
For any $u$ between $s$ and $t$ \begin{equation}
    \mathbb{E}[X_t \rvert \mathcal{G}_u] \leq X_u \; \; \text{a.s.},
\end{equation} since $(X_t)$ is a supermartingale. So for any $A \in \mathcal{G_{s+} \subset \mathcal{G_u}}$ , \begin{equation}
    \mathbb{E}[1_A  X_t] \leq \mathbb{E}[1_A X_u].
\end{equation} Letting $u \searrow s$ , $X_u \rightarrow X_s$ (by r-c) and \begin{equation}
    \mathbb{E}[1_A  X_t] \leq \mathbb{E}[1_A X_s], \tag{a}
\end{equation} for any $A \in \mathcal{G}_{s+}$ . My question: How are the limit and expectation interchanged in (a)? Clearly Monotone convergence does not apply, so it must be Dominated convergence? But I can't see why. Thanks in advance.","['stochastic-processes', 'probability-theory', 'martingales']"
4244054,How to find the most square-ish factorization of an integer?,"Given an integer $n\in\Bbb N$ , what's the best (known) way to decompose it as the product of two integers in such a way that the factors are the sides of a rectangle that's as close as possible to a square?  The full factorization of $n$ into prime factors can be assumed to be known if that's helpful. More formally: Given $n\in \Bbb N$ , find a divisor $d|n$ such that $ |d-n/d| \stackrel{!}= \min$ . or Given $n\in \Bbb N$ , find $a, b\in\Bbb N$ such that $a\cdot b=n$ and $a+b \stackrel{!}= \min$ . Using the factorization $n = \displaystyle \prod_{j=1}^k p_j$ where primes might occur more than once, this can be restated using logarithms: Find $k$ values $b_j\in\{-1,1\}$ such that $\displaystyle \Bigg|\sum_{j=1}^k b_j\log p_j \Bigg|\stackrel{!}= \min$ . This looks like a knapsack problem which are known to be hard in general.  For $n > 1$ there are $2^{k-1}$ possible decompositions into different pairs of factors, and checking all of them is expensive when $n$ has many factors.  Moreover: If someone comes up with a specific decomposition asserting it's the most square-ish one: Is it easier to check whether the solution is correct, or is it as expensive as determining the best solution in the first place? As an example, let $H_D(x)\in \Bbb Z[x]$ be the Hilbert class polynomial for discriminant $D$ and let $n=H_{-71}(0)$ : $$n = 737707086760731113357714241006081263 = 11^9 \cdot 17^6 \cdot 23^3 \cdot 41^3 \cdot 47^3 \cdot 53^3$$ See [ 1 ].  This $n$ has $2^{26} = 67\,108\,864$ different decompositions, which is doable with brute force on a modern computer. But is there a smarter approach? Two more demanding test cases are $H_{-119}(0) = (11^4 \cdot 17 \cdot 23^2 \cdot 29^2 \cdot 47 \cdot 59 \cdot 83 \cdot 89)^3$ with $2^{38}\approx 2.7\cdot 10^{11}$ decompositions and $H_{-907}(0) = (2^{19} \cdot 3^3 \cdot 5^3 \cdot 131 \cdot 137 \cdot 167)^3$ with $2^{83}\approx 9.7\cdot 10^{24}$ decompositions. Note 1 As Jukka Kohonen mentions in a comment, the upper bound for the complexity of a brute-force approach can be reduced: Let $n=\displaystyle\prod_{i=1}^m p_i^{e_i}$ with pairwise different primes $p_i$ be the factorization of $n$ . Then the task is: Find $m$ integers $a_i$ with $0\leqslant a_i \leqslant e_i$ such that $\Bigg|\log n - 2\displaystyle \sum_{i=1}^m a_i\log p_i \Bigg| \stackrel!= \min$ . If all $a_i$ are replaced by $e_i-a_i$ then the value to be minimized stays the same. If all $e_i$ are even, then $n$ is a perfect square and the solution is clear. If at least one $e_i$ is odd, then a brute-force approach has to test $\dfrac12\displaystyle\prod_{i=1}^m (1+e_i)$ combinations of the $a_i$ . The examples above are all in case 2., and the corrected number of tries are: $H_{-71}(0)$ : $\frac12\cdot 10 \cdot 7 \cdot 4 \cdot 4 \cdot 4 \cdot 4 = 8960$ possibilities. $H_{-119}(0)$ : $\frac12\cdot 13 \cdot 4 \cdot 7 \cdot 7 \cdot 4 \cdot 4 \cdot 4 \cdot 4 = 326144$ possibilities. $H_{-907}(0)$ : $\frac12\cdot 58 \cdot 10 \cdot 10 \cdot 4 \cdot 4 \cdot 4 = 185600$ possibilities. Conclusion: The work that's needed to get the complete factorization of $n$ is in general much higher than brute-force checking which pair of divisors is the best choice. Related: How to get Most square rectagle : No answers, asked for explicit formula. Is there a name for the ""most square"" factorization of an integer? : Asks for nomenclature.","['discrete-optimization', 'elementary-number-theory', 'combinatorics', 'factoring']"
