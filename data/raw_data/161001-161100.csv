question_id,title,body,tags
2786982,Are operators functions? (cont.),"Undergraduate here. Some time ago I had a discussion with a friend on whether or not an operator is a function. The debate centered on this issue: I believed all operators are functions, while my friend believed not all operators are functions. I acknowledge the past questions asked regarding this issue, but intend for this question to build on previous discussions. Argument: All operators are functions My argument that an operator is a function is this: since the input and output of an operator is a function, then it fits the definition of a function: ""A relation $f$ is a function provided $(a,b),(a,c)\in f$ implies $b=c$."" In addition, the majority of online resources claim that operators are functions: On Quora , answers state that the operator is either ""a special type of function"" or exactly a function. In StackExchange, here , and here , the operator is also ""a special kind of function,"" etc. Wikipedia states that an operator is a mapping, and since mappings are functions, operators are therefore functions. (I'll note that the reliability of Wikipedia is debated and questioned here .) On my conceptual interpretation of the operator and the literature surrounding it, I came to support the argument that all operators are functions. Argument: Not all operators are functions My friend argued that there exist operators that do not conform to the definition of the function. He lists these operators as examples: The indefinite integral operator $\int$, in which $\int dt(f(t))=F(t)+C$ an arbitrary constant $C$. A square root operator $S$, where $S(f)=\pm\sqrt f$. A less-than operator $T$, where $T(f)=g$ for all $g<f$. A random number generator $U$, where $U(f)=x$ for some random number $x$. Since for any of these functions, the input of a function can correspond to an output of two or more distinct functions, it follows that not all operators are functions. We agreed that if all operators are functions, then the indefinite integral operator and co. are not operators. However, I also couldn’t ignore the literature online. I consulted with two professors. One suggested a correction on the indefinite integral, and it return an equivalence class of functions, instead of multiple distinct functions, for some function input, so technically there is one output. Although this argument fits the integral operator as a function, it seems to contradict the definition given by Wikipedia, where domain and codomain are the same. The other said that the definition of an operator depends on the math field we are working in. Our (frankly enjoyable) debate thus stopped without conclusion, but I believed that progress would be made if we were given a rigorous (and preferably, widely accepted) definition of an operator, instead of working with what we think what an operator is. Thank you for reading, and my key questions are below: How are operators defined? If definitions vary depending on field, please give a working definition in your field. Are operators functions? Depending on the definition, this answer may differ. If operators are not functions, please give some counterexample. (Some extra context: we both were in Intro to Quantum and read Gasiorowicz, and where I was first exposed to the idea of operators and operator manipulations such as $(\frac{d}{dx})^2=\frac{d^2}{dx^2}$. I thought if I really understand what an operator is, then this type of manipulation would be intuitive and founded. I also understand that there exists a relation between operators and transformations.)","['terminology', 'operator-theory', 'functions', 'definition']"
2786999,"Can I use Leibniz' rule to differentiate when integral is $\int_0^\gamma f(\gamma,s)dF(s)$, instead of $\int_0^\gamma f(\gamma,s)ds$ ($F$ is a CDF)","Specifically, suppose I have a CDF $F$ and an integral
$$
\int_0^\gamma (\gamma-s)dF(s)
$$
and I want to find
$$
\frac{d}{d\gamma} \int_0^\gamma (\gamma-s)dF(s)
$$ (Note that the upper limit $\gamma$ indicates that $s=\gamma$) If the integral was $\int_0^\gamma (\gamma-s)ds$ then I can apply Liebniz rule. Can I still use Liebniz rule even though my integral is of the form $\int_0^\gamma f(\gamma,s)dF(s)$, instead of $\int_0^\gamma f(\gamma,s)ds$ (if $F$ is differentiable than I can (rigorously/correctly) write $dF(s) = f(s)ds$ and have the standard form. But if $F$ has mass point(s)... then I don't know)","['derivatives', 'integration', 'calculus']"
2787011,Convergence of a Series with Infinitely Many Increasing Terms,"I'm working on the following problem: Let $\{ c_n \}_{n=1}^{\infty}$ be a sequence such that $c_n = \frac{1}{n^2}$ if $n$ is odd, and $c_n = \frac{1}{n^4}$ if $n$ is even. Prove that (i) $c_{n+1} > c_n$ for infinitely many $n$ and (ii) $\sum_{n=1}^{\infty} (-1)^{n-1} c_n$ converges. My attempt: (i) We claim that when $n$ is even, 
$$c_{n+1} > c_n$$
and proceed via induction. First, note that for $n=2$, 
$$c_3 = \frac{1}{9} > \frac{1}{16} = c_2.$$
Now, suppose that $c_{k+1} > c_k$ for some even $k$. Then, we prove that 
\begin{align*}
c_{k+3} &> c_{k+2} \\
\frac{1}{(k+3)^2} &> \frac{1}{(k+2)^4} \\
\end{align*}
by showing that 
$$(k+2)^4 > (k+3)^2.$$
Accordingly,
\begin{align*}
(k+2)^4 &= k^4 + 8k^3 + 24k^2 + 32k + 16 \\
&>^{I.H.} (k^2 + 2k + 1) + 8k^3 + 24k^2 + 32k + 16 \\
&= 8k^3 + 25k^2 + 34k + 17. \\
\end{align*}
Now,
\begin{align*}
8k^3 + 25k^2 + 34k + 17 &> (k+3)^2 \\
\Rightarrow 8k^3 + 24k^2 + 28k + 8 &> 0 \\
\end{align*}
holds as $k>0$ and each coefficient of the polynomial is positive. Thus, $c_{n+1} > c_n$ for each even $n$. (ii) To see that $\sum_{n=1}^{\infty} (-1)^{n-1} c_n$ converges, we first note that $$ |(-1)^{n-1} c_n | = c_n \leq \frac{1}{n^2} \qquad \forall n \in \mathbb{N} $$ Further, $\sum_{n=1}^{\infty} \frac{1}{n^2}$ converges via $p$-Test with $p = 2 > 1$, so $$ \sum_{n=1}^{\infty} (-1)^{n-1} c_n $$ converges via the Weierstrass M-test. Is this sound reasoning? The inductive portion seems long, as the result is somewhat intuitive. Is there an shorter way to prove this?","['sequences-and-series', 'analysis']"
2787031,Maximizing $3\sin^2 x + 8\sin x\cos x + 9\cos^2 x$. What went wrong?,"Let $f(x) = 3\sin^2 x + 8\sin x\cos x + 9\cos^2 x$. For some $x \in \left[0,\frac{\pi}{2}\right]$, $f$ attains its maximum value, $m$. Compute $m + 100 \cos^2 x$. What I did was rewrite the equation as $f(x)=6\cos^2x+8\sin x\cos x+3$. Then I let $\mathbf{a}=\left<6\cos x,8\cos x\right>$ and $\mathbf{b}=\left<\cos x,\sin x\right>$. Using Cauchy-Schwarz, I got that the maximum occurs when $\tan x=\frac{4}{3}$, and that the maximum value is $10\cos x$. However, that produces a maximum of $9$ for $f(x)$, instead of the actual answer of $11$. What did I do wrong, and how do I go about finding the second part? Thanks!","['algebra-precalculus', 'inequality', 'trigonometry', 'cauchy-schwarz-inequality']"
2787058,$\mathrm{rank}(AB-BA)=1$ implies $A$ and $B$ are simultaneously triangularisable,"Let $A$ and $B$ in $M_n(\mathbb C)$ such that the rank of $AB-BA$ is $1$ . Prove that $A$ and $B$ are simultaneously triangularisable. This generalizes the classical case $AB = BA$ . By induction on $n$ , it suffices to show that $A$ and $B$ have a common eigenvector.
So, it would be sufficient to find a eigenspace of $A$ which is stable by $B$ since matrices are complex. Do you have ideas for that? Thank you.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2787066,Spectrum of multiplication operator,"How do I show that given $a$ continuous on $[0,1]$, then the operator $$A_a:L^2(0,1) \to L^2(0,1)$$ with $$A_a: f \to af$$ has spectrum exactly $a([0,1])$? I can show that $a([0,1])^c \subset \text{Res}(A_a)$, but I do not see how to show the reverse.","['functional-analysis', 'real-analysis', 'spectral-theory', 'analysis']"
2787103,How do I show that partial derivatives are continuous?,"The theorem says that for $f$ to be differentiable, partial derivatives of $f$ exist and are continuous. For example, let $f(x,y)=x^2+2xy+y^2.$ Let $(a,b)\in R^2.$ Then, I know that partial derivatives exist and $f_x(a,b)=2a+b,$ and $f_y(a,b) = a+2b$. In order to test the continuity, $$\lim_{(x,y)\to(a,b)}f_x(x,y)=\lim_{(x,y)\to(a,b)}2x+y=2a+b=f_x(a,b). $$
$$\lim_{(x,y)\to(a,b)}f_y(x,y)=\lim_{(x,y)\to(a,b)}x+2y=a+2b=f_y(a,b).$$ Is this the right way to check whether partial derivatives are continuous on $(a,b)$ for the multivariable case?","['multivariable-calculus', 'real-analysis']"
2787111,Continuity of Un-plottable Piece-wise Defined Function,"I'm working on the following problem: Consider the function
$$f(x) = 
\begin{cases}
x + x^2 & x \in \mathbb{Q} \\
x - x^2 & x \notin \mathbb{Q} \\
\end{cases}
$$
Prove that $f$ is nowhere continuous except at $x=0$. My attempt: First, we show that $f$ is continuous at $x=0$. Accordingly, fix $\epsilon > 0$, and choose $\delta < \frac{-1 + \sqrt{1 + 4 \epsilon}}{2}$ (note that since the quantity on the left hand side is strictly positive, the Archimedean Principle guarantees the existence of such a $\delta$). 
Then, if 
$$ |x - 0| = |x| < \delta$$
then
$$ |f(x) - f(0)| = |f(x)| =  
\begin{cases}
|x + x^2| & x \in \mathbb{Q} \\
|x - x^2| & x \notin \mathbb{Q} \\
\end{cases}
$$
In either case, the triangle inequality gives that
\begin{align*}
|f(x)| &\leq |x| + |x|^2 \\
&< \delta + \delta^2 \\
&= \frac{-2 + 2\sqrt{1+4 \epsilon} + 1 - 2\sqrt{1 + 4 \epsilon} + 1 + 4 \epsilon}{4} \\
&= \frac{4\epsilon}{4} \\
&= \epsilon
\end{align*}
Thus, for arbitrary $\epsilon>0$, $\exists \delta > 0$ s.t.
$$ |x - 0| < \delta $$
implies
$$ |f(x) - f(0)| < \epsilon,$$ 
so $f$ is continuous at $x=0$. Now, consider some real $x_0 \neq 0$. It follows from the Sequential Density of Irrationals/Rationals that there exists some sequence $u_n$ of rational numbers that converges to $x_0$, and there exists some sequence $v_n$ of irrational numbers that also converges to $x_0$. Now, since the limits $\lim_{n \rightarrow \infty} u_n = x_0$ and  $\lim_{n \rightarrow \infty} v_n = x_0$ exist, we conclude that $$\lim_{n \rightarrow \infty} f(u_n) = \lim_{n \rightarrow \infty} u_n + u_n^2 = x_0 + x_0^2$$ 
and 
$$\lim_{n \rightarrow \infty} f(v_n) = \lim_{n \rightarrow \infty} v_n + v_n^2 = x_0 - x_0^2.$$
Since these limits are distinct for $x_0 \neq 0$, it follows that either the image of $u_n$ or the image of $v_n$ under $f$ does not converge to $f(x_0)$. Thus, $f$ is not continuous for any $x_0 \neq 0$. Is this a valid proof? I'm concerned I jumped in a bit too eagerly - is it necessary to include the former proof of continuity? Or does it suffice to say the images limit to the same value only when $x_0$ is identically 0?","['continuity', 'piecewise-continuity', 'analysis']"
2787114,"Does the following definition of ""approach independent"" derivative of a vector valued function $f$ implies the continuity of $f$?","Before going to my question, let me give two prelimilary definitions Definition 1. Let $S\subseteq\mathbb{R}^n$ be a non=empty open set in $\mathbb{R}^n$ under the usual topology on $\mathbb{R}^n$ and $f:S\to \mathbb{R}^m$. Let $\mathbf{c}\in S$ and $g:U(\subseteq \mathbb{R})\to S$ is such that, $U$ is open in $\mathbb{R}$ under the usual topology on $\mathbb{R}$ $g(0)=\mathbf{c}$ $g$ is continuous at $0$ Then $f$ will be said to have derivative along the cruve $g$ at the point $\mathbf{c}$ if, $$\displaystyle\lim_{h\to 0}\dfrac{(f\circ g)(h)-(f\circ g)(0)}{h}$$ exists. Definition 2. Let $S\subseteq\mathbb{R}^n$ be a non=empty open set in $\mathbb{R}^n$ under the usual topology on $\mathbb{R}^n$ and $f:S\to \mathbb{R}^m$. Let $\mathbf{c}\in S$. Then $f$ will be said to have approach independent derivative at $\mathbf{c}$ if, $$\displaystyle\lim_{h\to 0}\dfrac{(f\circ g)(h)-(f\circ g)(0)}{h}$$ exists for all $g$ satisfying the properties listed in the previous definition. Question If $f$ has approach independent derivative at $\bf{c}$ then is it continuous at $\mathbf{c}$? I was trying to find a counter example of such a function $f$ but till now I have not been able to find such an example. Any help will be appreciated.","['multivariable-calculus', 'real-analysis', 'definition', 'derivatives']"
2787131,Volume of cannoli,"Problem statement (see pic below): 1. Wrap unit circle over cylinder with radius $r=1/\pi$, edges of unit circle will just touch each other. 2. Remove cylinder, fill cannoli with cream)). 3. Scrape off the excess of cream with straight edge held perpendicular to the cannoli axis and touch symmetic points of the edges. Find volume of cannoli. This lead me to a difficult integral which I can't solve so far. Here are my steps: since figure is simmetrical we can integrate from 0 to R=1: $V=2\int_0^R S(h)dh$, where $S(h)$ is area at heigh $h$ $S(h)$ is a segment of circle with radius $r=1/\pi$ Length of this segment is: $l(h)=2\sqrt{R^2-h^2}$ (top pic) The angle of corresponding segment is $\phi={l(h)/ r}$ and it's area is: $S(h)={1\over 2}r^2(\phi-\sin\phi)=r\sqrt{R^2-h^2}-{1 \over 2}r^2 \sin{2\sqrt{R^2-h^2} \over r}$ (right picture) $V=2r\int_0^R \sqrt{R^2-h^2}dh-r^2\int_0^R\sin{2\sqrt{R^2-h^2} \over r}dh$ Am I being wrong somewhere?
If 'no' then how to take 2nd integral?","['volume', '3d', 'integration']"
2787144,Distribution of maximum proportion of heads in an infinite sequence of flips,"Let $X_1, X_2, X_3, \dots$ be independent variables each of which is $1$ with probability $1/2$ and $0$ otherwise.  Let
$$S_n=\sum_{i=1}^n X_i$$
and define
$$Y = \sup_{n \geq 1} \frac{S_n}{n}$$ With probability $1$, ""$\sup$"" here can be replaced by $\max$, since $S_n/n$ is approaching $1/2$ by the Law of Large Numbers.  In particular, $Y$ is rational with probability $1$.  I am curious what is known and what has been studied about the distribution of $Y$.  For example, are there closed forms for or bounds on The expectation and variance of $Y$? The probability that $Y=q$, for specific rational $q$ between $1/2$ and $1$? The probability that $Y \leq \frac{1}{2}+\epsilon$, for small $\epsilon$? I feel like this is something that has to be well studied, but I'm not sure the specific terms to look under for it.","['reference-request', 'probability-theory', 'probability-distributions']"
2787157,Prove by induction that $x^n-y^n$ is divisble by $x-y$ for $ n \ge 1 $,"I'm new to inductive proofs so I need some commentary on my proof since the book only gives a hint in the back. In ""Discrete Mathematics with Applications"" by Epp Third Edition in section 4.3 problem 13 states For any integer $ n \ge 1,  x^n - y^n$ is divisible by $(x - y)$ 
  where x and y are any integers with $ x \ne y $ My Proof is as follows. let $ Q(n) = x^n - y^n $ Then the base case is $ Q(1) = x^1 - y^1 $ Now $ Q(n + 1) = x^{n+1} - y^{n+1} = (x^n + y^n)(x-y)$ So now we can see $(x-y)$ is a factor and in turn divisible by $(x-y)$. I have just one hesitation. I didn't substitute from the inductive hypotheses. In every other inductive proof I've done this was a necessary step. My intuition on induction tells me that I have basically set up all of the dominoes but failed to knock down the first one (the substitution). Is this necessary for a valid proof or does this hold?","['induction', 'proof-verification', 'discrete-mathematics']"
2787188,"Can someone explain to me why $\mathcal P(\mathcal P(\emptyset)) = \{\emptyset,\{\emptyset\}\}$?","Would someone explain the reasoning of the answer of $$\mathcal P(\mathcal P(\emptyset)) = \{\emptyset,\{\emptyset\}\}$$ I am having trouble understanding this",['elementary-set-theory']
2787215,Summation of $\sum_{r=1}^{n} \frac{\cos (2rx)}{\sin((2r+1)x \sin((2r-1)x}$,Summation of $$S=\sum_{r=1}^{n} \frac{\cos (2rx)}{\sin((2r+1)x) \sin((2r-1)x)}$$ My Try: $$S=\sum_{r=1}^{n} \frac{\cos (2rx) \sin((2r+1)x-(2r-1)x)}{\sin 2x \:\sin((2r+1)x \sin((2r-1)x}$$ $$S=\sum_{r=1}^{n} \frac{\cos (2rx) \left(\sin((2r+1)x \cos (2r-1)x-\cos(2r-1)x)\sin(2r+1)x\right)}{\sin 2x \:\sin((2r+1)x \sin((2r-1)x)}$$ $$S=\sum_{r=1}^n \frac{\cos(2rx)}{\sin 2x}\left(\cot(2r-1)x-\cot(2r+1)x\right)$$ Any clue here?,"['telescopic-series', 'summation', 'trigonometry', 'sequences-and-series']"
2787285,"Carmo Curves and Surfaces, $f:V \subset S \to R$ no-where zero differentiable allows easier computation of curvature.","How can I solve this problem? I know that the curvature $K$ is given by $\det(dN_p)$ for any point $p$ in the surface. Here I will write capital $V$'s for the vectors in the problem statement. We should show that $$\langle d(fN)(V_1)\times d(fN)(V_2),fN\rangle=f^3\det(dN_p)$$I know that $$V_1=\alpha_1'(s)=x_uu_1'(s)+x_vv_1'(s)$$ $$V_2=\alpha_2'(s)=x_uu_2'(s)+x_vv_2'(s)$$ for curves $\alpha_1,\alpha_2$ mapping into $V$ from $R^2$.
Therefore I think I can write $dfN(V_i)$ which I assume means $d(f \circ N)(V_i)$ as
$$dfN(V_i)=dN(V_i)dfN(V_i)=(N_uu_i'(s)+N_vv_i'(s))dfN(V_i)$$ by the chain rule but obviously this doesn't make sense. I feel like I completely misunderstand something very fundamental, perhaps it should be
$$dfN(V_i)=df(dN(V_i))=df(N_uu_i'(s)+N_vv_i'(s))$$
Furthermore I think that $N_uu_i'(s)+N_vv_i'(s)$ lies in the tangent plane and so $V_1,V_2$ are made up of the basis vectors $N_u,N_v$ so we can rewrite $$N_uu_i'(s)+N_vv_i'(s)=a_iV_1+b_iV_2$$ which would make sense given the hint in the book. Then can write $$dfN(V_i)=df(a_iV_1+b_iV_2)$$ assuming the second way I wrote the differential is correct. But this is probably completely wrong and I can't see how I could carry on from here anyway. Can anybody help me out?","['multivariable-calculus', 'differential-geometry', 'derivatives']"
2787307,Find the Limit of $‎\prod‎_{n=1}^{‎\infty}\frac{(1+‎\frac{1}{n}‎)^n}{(1+‎\frac{1}{n+x})^{n+x}}$,"‎Consider the following productions‎
‎$$‎‎‎‎‎\prod‎_{n=1}^{‎\infty}\frac{1+‎\frac{1}{n}‎}{1+‎\frac{1}{n+x}}$$
and
$$‎‎‎‎‎\prod‎_{n=1}^{‎\infty}\frac{(1+‎\frac{1}{n}‎)^n}{(1+‎\frac{1}{n+x})^{n+x}}$$
I know that the above are convergent.
Can anyone find the limit of these products? ‎","['special-functions', 'real-analysis', 'sequences-and-series', 'calculus']"
2787340,Finding a 'colouring'/configuration of a cube such that the stabilizer is isomorphic to a given group,"I'm making a few old exams to practice for a group theory exam. In every old exam I practiced, there is a question such as the following: Suppose we have a cube and on each face, we draw an arrow starting from the middle of the face, towards one of the four vertices of that face. Then first, I have to find the number of ways to do this up to rotation symmetry. Thus this is a kind of 'standard' exercise where you should use Burnside's counting lemma. But then, they ask for the following: there is a twodimensional printout for a cube given: and then they ask to draw arrows on this printout such that the stabilizer$^*$ is isomorphic to $S_3$. Now I know that the rotation of a cube is $S_4$ and that we can classify these rotations as follows: The identity permutation $6$ rotations over $90$ degrees through a line through two opposite faces. $3$ rotations over $180$ degrees through a line through two opposite faces. $8$ rotations over $120$ degrees through a main diagonal of the cube. $6$ rotations over $180$ degrees through the middle of two opposite edges. Now the solution should be the following but I really don't see how I should come up with this solution. I even don't see why this solution is correct. There are also similar exercises where you have to fill in the printout such that the stabilizer is isomorphic to $V_4$ and $A_4$. I cannot find anything silimar on the internet and this is never explained in class. Any help is much appreciated! $^*$: of this configuration of arrows, the rotation group of the cube $S_4$ acts on the set of arrow configurations","['finite-groups', 'abstract-algebra', 'group-theory']"
2787368,Intuition behind the Minkowski dimension and measure,"Can someone please explain the intuition behind the Minkowski dimension and measure? I am using the definition of the Minkowski measure as 
$$M^{\alpha}(K)=\lim_{\epsilon\rightarrow0}\frac{\mu(V_{\epsilon}(K))}{\epsilon^{n-\alpha}} $$
Where $K\subset\mathbb{R^n}$, $\alpha\in(0,\infty)$, and $\mu$ is the $n$-dimensional Lebesgue measure and $V_{\epsilon}$ is an $\epsilon$-neighborhood of $K$. And the definition of the Minkowski dimension is 
$$\dim_{M}(K)=\lim_{\epsilon\rightarrow0}\frac{N_{\epsilon}(K)}{\ln(\frac1{\epsilon})}$$
Where $N_{\epsilon}(K)$ is the minimum number of non-overlapping $n$-dimensional boxes of side length $\epsilon$ necessary to cover $K$. I know these are largely used when working with fractals, but how does the notion of counting boxes relate.","['real-analysis', 'fractals', 'dimension-theory-analysis', 'measure-theory', 'general-topology']"
2787400,Show that these circles are linked (their linking number is $\pm 1$),"There are two circles given in 3 dimensions:
$$c_0(\phi) = (\cos(\phi), \sin(\phi),0)\ \ \ \phi \in [0,2\pi[$$
$$c_1(\phi) = \frac{1}{\sqrt{2}-\sin(\phi)}(\cos(\phi), \sin(\phi), \cos(\phi))\ \ \ \phi \in [0,2\pi[$$
Show that their linking number is $\pm 1$, where the linking number is defined by:
$$L(c_0,c_1) = \frac{1}{4\pi}\int_0^{2\pi}\int_0^{2\pi}\frac{(c_0(t)-c_1(s),\dot c_0(t),\dot c_1(s))}{|c_0(t)-c_1(s)|^3}\,dt\ ds$$
Where
$$\text{if } \ a,b,c:\Bbb{R} \to \Bbb{R}^3$$
$$\text{then } \ (a(t),b(t),c(t)) =
\begin{align}\begin{vmatrix}
a_1(t) & b_1(t) & c_1(t) \\ 
a_2(t) & b_2(t) & c_2(t) \\ 
a_3(t) & b_3(t) & c_3(t) \\
\end{vmatrix}
\end{align}$$
And
$$\text{if } \ c: \Bbb{R} \to \Bbb{R}^3, \ c(t) = (c_1(t),c_2(t),c_3(t))$$
$$\text{then } \ \dot c(t) = (c_1'(t),c_2'(t),c_3'(t))$$
Usually the integral for the linking number is very difficult to calculate, so there are other options too. For example, since we know that our curves are circles, their linking number can only be $L(c_0,c_1) \in \{-1,0,+1\}$. Showing that an integral is equal to a specific value can be done with this following technique:
$$L(c_0,c_1) \stackrel{?}{=} a \in \{-1,0,+1\} \iff \\ 
\iff \forall \varepsilon \in \Bbb{R}^+: a - \varepsilon \stackrel{?}{\le} L(c_0,c_1) \stackrel{?}{\le} a + \varepsilon $$
So far I've shown that the circles don't intersect, meaning that there are no $\phi \in \ [0,2\pi[$ values for which $|c_0(\phi)-c_1(\phi)| = 0$. That was relatively easy to show. But the big question is:
$$L(c_0,c_1) \stackrel{?}{=} \pm 1$$
I would appreciate any insight on how I could possibly show this. In other words, how can we show that the above integral is approximately equal to $1$, with perhaps numerical methods?","['circles', 'multivariable-calculus', 'knot-theory', 'definite-integrals', 'differential-geometry']"
2787402,$L^2$ equality implies $L^1$ equality or a.e. equality?,"Suppose $f\in L^1 \cap L^2$ and $g\in L^2$. If I know that $f=g$ in $L^2$, then does it follow that $f=g$ in $L^1$ or a.e? Does it follow that $g\in L^1$? More generally, if $f\in L^s \cap L^r, g\in L^s$ and $f=g$ in $L^s$, then does it follow that $g\in L^r$? In particular, I have the Lebesgue measure on $\mathbb R$ in mind but I am also curious about general results. Thanks and regards.","['real-analysis', 'functional-analysis', 'integration', 'measure-theory', 'analysis']"
2787419,Any collection of neighborhoods of a dense set cover the space?,"Trying to solve an exercise, I'm trying to see if this is true: Let $X$ a Hausdorff space and $D\subset X$ dense. Also let a function $$f:D\to\wp(X),\quad x\mapsto U_x$$ where $U_x$ is a neighborhood of $x$. Then we can say that $X=\bigcup f(D)$? I'm unable to prove it so probably it's false. My first idea was taking each $U_x$ open, then $U:=\bigcup f(D)$ is also open and $U^\complement$ closed. Then if my assertion would be true, it must be the case that $U^\complement=\emptyset$, but I'm unable to show this. Can someone confirm if there exists some Hausdorff space where my hypothesis doesn't work? Or, by the contrary, if my hypothesis is true?","['general-topology', 'examples-counterexamples', 'analysis']"
2787474,How to construct a Poisson process not based on Lebesgue measure?,"It is clear to me that I can build a suitable underlying probability space for a homogeneous Poisson point process . It is enough to have a pobability space $(\Omega,\mathcal A,P)$ with on it iid random variables $X_1,X_2,\dots$ having exponential distribution. So I could do already with $\mathbb R^{\mathbb N}$ applied with product measure. Then $N_t$ can be defined as the cardinality of the set $\{n\mid S_n\leq t\}$ where $S_n:=X_1+\dots+X_n$. If $A$ is a measurable subset of $[0,\infty)$ then I can define random variable $\hat A$ as the (random) cardinality of $\{n\mid S_n\in A\}$ and then $\hat A$ has Poisson-distribution with a (multiple of) $\lambda(A)$ as parameter, where $\lambda$ denotes the Lebesgue measure. In that sense it can be called a Poisson process on base of the Lebesgue measure . Now my question: How to build up a probability space allowing me to construct a Poisson process based on an arbitrary chosen measure $\nu$ on $[0,\infty)$ with the property that $\nu([0,t])<\infty$ for each $t$? So this means that for a measurable $A\subseteq[0,\infty)$ the random cardinality of $\{n\mid S_n\in A\}$ has Poisson-distribution with parameter $\nu(A)$. Further if two such sets $A,B$ are disjoint then $\{n\mid S_n\in A\}$ and $\{n\mid S_n\in B\}$ must be independent (as is also the case described above).","['stochastic-processes', 'probability-theory', 'probability']"
2787569,Is $Bw_0B$ a dense open subset of $G$?,"Let $G=GL_n(\mathbb{C})$ and $B$ the Borel subgroup of $G$ containing all upper triangular matrices in $G$. Let $w_0$ be the longest word in the Weyl group $W$ of $G$. We have the Bruhat decomposition $G = \cup_{w \in W} BwB$. Is $Bw_0B$ a dense open subset of $G$? I think that $Bw_0B$ is open because $Bw_0B = \{g \in G: \Delta_{\{n-i+1, \ldots, n\}, \{1,\ldots,i\}}(g) \ne 0, i=1,2,\ldots,n \}$. How to show that $Bw_0B$ is dense in $G$? Thank you very much.","['algebraic-groups', 'algebraic-geometry', 'commutative-algebra']"
2787579,Closeness of points in the irreducible decomposition of a C$^{*}$-algebra representation,"Suppose $X$ and $Y$ are compact metric spaces. Let $\varphi\colon C(X)\to M_{n}(C(Y))$ be any $*$-homomorphism. If $\pi$ is an irreducible representation of $M_{n}(C(Y))$, then $\pi$ is unitarily equivalent to a point evaluation $\textrm{ev}_{y}$. The $*$-homomorphism $\textrm{ev}_{y}\circ\varphi\colon C(X)\to M_{n}(\mathbb{C})$ is a representation of $C(X)$. Since it's a finite-dimensional representation, we can find a unitary $u_{y}\in M_{n}(\mathbb{C})$ and a set of points $X_{y}=\{x^{y}_{1},\ldots,x^{y}_{n}\}\subset X$ such that for all $f\in C(X)$,
$$
(\varphi\circ f)(y)=(\textrm{ev}_{y}\circ\varphi)(f)=u_{y}
\begin{pmatrix}
f(x^{y}_{1}) & 0 & \cdots & 0\\
0 & f(x^{y}_{2}) & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & f(x^{y}_{n})
\end{pmatrix}
u_{y}^{*}.
$$ My question is: Does the Hausdorff distance of the set $\{x_{1}^{y},\ldots,x_{n}^{y}\}$ vary continuously in $y$? More precisely, if $y_{m}\to y$ in $Y$, does $D_{m}:=\max_{1\leq i\leq n}\min_{1\leq j\leq n}d_{X}(x^{y}_{i},x^{y_{m}}_{j})\to 0$ as $m\to \infty$? This question is motivated by the simpler case where $\varphi$ gives rise to continuous functions $\lambda_{1},\ldots,\lambda_{n}:Y\to X$ satisfying
$$
\varphi(f)=\begin{pmatrix}
f\circ\lambda_{1} & 0 & \cdots & 0\\
0 & f\circ\lambda_{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & f\circ\lambda_{n}
\end{pmatrix}.
$$ In this case for each $y\in Y$, $X_{y}=\{\lambda_{1}(y),\ldots,\lambda_{n}(y)\}$, and the result holds.","['c-star-algebras', 'operator-theory', 'functional-analysis', 'representation-theory', 'operator-algebras']"
2787581,How many numbers below $n$ with a square number of distinct prime factors?,"Is there a nice closed-form expression for, or asymptotic formula for the number of numbers below $n$ with a square number of distinct prime factors? Motivation: As a student who studies mathematics in my spare time, I encountered some questions asking about the number of numbers below certain limits that adhere to certain elementary factor properties and later was thinking about it, when it occurred to me that there were many, seemingly simple or natural, questions to ask about ""counting functions"" that nothing I know can help with... So, more generally, on top of an answer to the above, are there nice toolkits or pieces of machinery that can, with some property, tell me the number of numbers below $n$ with that property? (even if this toolkit only works for a specific subset of properties, I would be interested)","['number-theory', 'prime-factorization', 'prime-numbers', 'functions']"
2787602,Explicit Countable Orthonormal Basis for $L^2(\mathbb{R})$,"I was working with my thesis and I needed to use the existence of a countable orthonormal basis of $L^2(\mathbb{R}).$ It should be true that the Hilbert Space $L^2(\mathbb{R})$ is separable, this because the measure space $(\mathbb{R},\lambda),$ where $\lambda$ is the Lebesgue measure, is $\sigma-$finite. Now, this implies that there exists a countable orthonormal basis, but this comes from an abstract type of reasoning, i.e. the Zorn's Lemma for the existence of an orthonormal basis and the use of separability to say that it is countable. The question that came up to me is: is there an explicit representation of this basis? The answer should be no, otherwise the use of the continuous Fourier transform doesn't make much sense, but who knows, perhaps there are some good reasons to prefer the continuous transform even if there is a countable basis. My idea was to take inspiration from the system $\{sin(\frac{n x}{m})\}_{n,m\in\mathbb{N}},$ but I don't know how to continue.","['functional-analysis', 'orthonormal', 'fourier-analysis', 'separable-spaces']"
2787647,Intuition for why a integral with jump discontinuity is continuous but not differentiable?,"Suppose we have a function $f:[b,d]\to\mathbb{R}$ that has a jump discontinuity at some point $b<a<d$ and continuous otherwise. Define $F(x) = \int_b^x$ Then, from other answers on the site , I know that the jump discontinuity does not affect the value of $F(x)$ $F(x)$ is continuous (since the jump discontinuity did not affect the value of $F(x)$, and $F(x)$ would be continuous/differentiable otherwise) $F(x)$ is not differentiable (at $a$) When i think of a function not being differentiable I normally imagine a kink or a jump. But I don't see how/why $F(x)$ would have a kink/jump (if that is the problem for differentiability). It can't jump because the discontinuity doesn't affect the value of the integral. For a kink I have no intuition I tried computing an example where $$g:[-2,3]\to \mathbb{R} = \begin{cases} 1 & x\not = 0\\ 5 & x=0\end{cases}$$ Then we have, for$G(x)\equiv \int_{-2}^xg$ , that
$$
G(x) = \begin{cases} \int_{-2}^{x} 1 = (x) +2 =2+x & x< 0\\
\\lim_{\epsilon\to 0} \bigg(\int_{-2}^{-\epsilon}1ds + \int_{\epsilon}^x 1 ds = -\epsilon +2 + x-\epsilon = 2+x-2\epsilon\bigg ) = 2+x & x\geq 0 \end{cases}
$$ So the integral is $G(x) = 2+x$, which is continuous and differentiable. So I am messing up the calculation somewhere, but I don't see where? (perhaps I cannot write the integral as the limit as $\epsilon\to 0$, precisely because the discontinuity?)","['derivatives', 'integration', 'continuity']"
2787688,Space of Linear and Bounded Operators is a Banach Space,"I have some questions about proof of the theorem below is written in my notes. If someone help me about it I will be appreciated. Thanks Theorem : Let $(X,\|.\|_X)$ is a normed space and $(Y,\|.\|_Y)$ is a Banach space then $(B(X,Y),\|.\|_{op})$ is a Banach space I have problem about some writtens : 1) $\|T_n(x)-T_m(x)\|_Y \leq \|T_n-T_m\|_{op}$ $\|T_n(x)-T_m(x)\|_Y \leq sup_{\|x\|_X \leq 1}\|T_n(x)-T_m(x)\|_Y$ it is true only for some $x \in X$ not for all. How can we use it? 2) $T_n(x) \to y , \exists y \in Y$ since $Y$ is a Banach space.It's OK for me but after this statement ""Since $\forall x \in X$  $\exists y \in Y$ we can write $lim_{n\to \infty}T_n(x)=T(x)$"" is written. How can we write it? Thanks in advance :)","['banach-spaces', 'normed-spaces', 'operator-theory', 'functional-analysis', 'analysis']"
2787720,What is an example of a groupoid which is not a semigroup?,"I know that groupoid refers to an algebraic structure with a binary operation. The only necessary condition is closure. However, I couldn't find any easy-to-understand example of a groupoid which is not a semigroup. I did come across some examples of (certain type of) matrices but then matrix multiplication is always associative (thus making it a semi-group). So, could someone please provide me an example of a groupoid which isn't a semigroup?","['abstract-algebra', 'magma', 'semigroups']"
2787761,Well posedness of PDEs of the form $u_{tt} + \left[ P ( \partial_x)\right]^2u=0$,"Let $P$ be a polynomial with real coefficients. I want to find the necessary&sufficient conditions on $P$ in order for the problem 
$$u_{tt} + \left[ P ( \partial_x)\right]^2u=0$$ with $u (0,x) = f(x)$ $ u_t(0,x) = 0$ to be well-posed for every positive period of the data $f$. I am tempted to decompose the PDE to the form 
$$P_1(\partial_t,\partial_x) \cdot \overline{P_1(\partial_t,\partial_x)}u=0$$ where $P_1(\partial_t,\partial_x) = \partial_t+iP(\partial_x)$ and $\overline{P_1(\partial_t,\partial_x)} =\partial_t - iP(\partial_x) $ is it's complex conjugate. But I don't know how to take it from there...","['real-analysis', 'fourier-series', 'partial-differential-equations', 'ordinary-differential-equations', 'analysis']"
2787808,ODE power series solution $y'+xy=1+x$,"So i have been told to find the power series solution to the following ode $$y'+yx=1-x$$
Using the substitution $y=\sum_{n=0}^{\infty}a_nx^n$, i can rewrite the equation as the following;
$$\sum_{n=1}^{\infty}na_nx^{n-1}+\sum_{n=0}^{\infty}a_nx^{n+1}=1+x$$
That is
$$\sum_{n=0}^{\infty}(n+1)a_{n+1}x^{n}+\sum_{n=1}^{\infty}a_{n-1}x^{n}=1+x$$
I then combined the series and the non homogeneous terms to obtain
$$(a_1-1)+x(2a_2+a_0-1)+\sum_{n=2}^{\infty}[(n+1)a_{n+1}+a_{n-1}]x^n=0$$
Setting $a_1=1$ and $2a_2+a_0-1=0$, I can say that $(n+1)a_{n+1}+a_{n-1}=0$, meaning i can say $a_n=\frac{-a_{n-2}}{n}$ for $n=3,4,5...$ I began subbing in the values for n to try and get a relation, i got $a_3=-\frac{1}{3}$, $a_4=\frac{a_0-1}{4\cdot2}$, $a_5=\frac{1}{5\cdot3}$, $a_6=-\frac{a_0-1}{6\cdot4\cdot2}$... This is where I am stumped, I said that $$a_{2k}=\frac{(-1)^k(a_0-1)}{2k!!}, k=2,3,4...$$
And
$$a_{2k+1}=\frac{(-1)^n}{(2k-1)!!},k=1,2,3..$$
Am i correct in saying this? And if so how to i get to an answer from these statements? Any help would be greatly appriciated.","['ordinary-differential-equations', 'power-series']"
2787813,Estimate of the speed of convergence to normal of the average of binomially distributioned variables,"Let us fix $n$, and consider a sequence of identical, independent random variables with binomial distributions with $n$ trials and probability of success $p=1/2$. Are there estimate on how fast the average of the sequence will converge to a normal distribution? (I mean, more accurate than application of the standard results.)","['statistics', 'probability']"
2787834,Showing that a sequence in $\ell^\infty(\mathbb{N})^*$ has a weak* convergent subnet,"I am currently looking at the sequence of functionals in the closed unit ball of $\ell^\infty(\mathbb{N})^*$ which are given by $e_n(x) := x_n$. I know that the closed unit ball is weak*-compact by the Banach Alaoglu theorem and I have shown that it does not have a weak*-convergent subsequence. I would now like to show that I can still find a convergent subnet. It is clear to me that in compact spaces, every net has a convergent subnet, but I am struggling to prove the following: Let $N_k= \{n \in \mathbb{N}: n \ge k\}$ be a filterbasis and let $\mathcal{U}$ denote the utrafilter of the filter that is induced by the sets $N_k$. Furthermore, let $I := \{(U,u) \ | \ U \in \mathcal{U}, u \in U \}$ be a directed set with the relation $(U,u) \triangleleft (V,v) $ iff $V \subseteq U$.I am trying to view the net over the set $I$ as a subnet of the sequence $e_n$ and then I would like to show that this subnet does indeed converge in the weak*-topology. I am not quite sure how to do this. I was thinking, since weak*-convergence is basically pointwise convergence it would be easier to look at a sequence $x \in \ell^\infty$, since the $e_n$ are just evaluating the sequences at a certain index. My idea was that any bounded sequence has a convergent subsequence, $x_{n_k}$ so I could look at the set $\{n_k :k\in \mathbb{N}\}$ and maybe use the fact that for any set $A \subseteq \mathbb{N}$ either $A \in \mathcal{U}$ or $A^C \in \mathcal{U}$, since $\mathcal{U}$ is an ultrafilter.","['functional-analysis', 'general-topology', 'weak-convergence', 'sequences-and-series']"
2787850,Jordan measure and Riemann integral,"I am studying Riemann integral, and I haven't understood what is the relation between the concepts of Riemann integral and Jordan measure. Is Riemann integral a special case of Jordan Measure? What is the precise definition of Jordan Measure?
Thank so much to who will help me!","['real-analysis', 'measure-theory', 'analysis']"
2787856,Vertex and axis of $n$-dimensional paraboloid,"Consider a surface defined by the form $$ x^\text{T}Ax+b^\text{T}x+c=0, $$ where $A\in\mathbb{R}^{n\times{n}}$ is non-zero symmetric positive semi-definite, $b\in\mathbb{R}^n$ and $c\in\mathbb{R}$. Suppose that $\det(A)=0$, and that
$$ \det\begin{pmatrix}A&b\\b^\text{T}&c\end{pmatrix}\neq0. $$ Question 1: Does the set $S=\{x:x^\text{T}Ax+b^\text{T}x+c=0\}$ have a well-defined notion of vertex and axis of symmetry (analogous to a parabola in 2D)? Question 2: If so, is there a way to express the vertex and axis of symmetry in terms of the data $(A,b,c)$? I seems like most of the work on these problems is done in 2D and 3D. I looked here but unfortunately it wasn't very helpful.","['quadrics', 'conic-sections', 'algebraic-geometry']"
2787870,If the solutions of $X'=AX$ have a constant norm then $A$ is skew symmetric.,"I have a differential equation $X'=AX$ where $A\in\mathcal M_n(\Bbb R)$. The question is to prove that if all the solutions have a constant norm then $A$ is skew-symmetric matrix. What I have tried so far: let $\varphi(t)=\Vert X(t)\Vert^2$ where $X$ is a solution. Then
$$0=\varphi'(t)=2\langle X'(t),X(t)\rangle=2\langle AX(t),X(t)\rangle=2\langle X(t),A^TX(t)\rangle$$
How can I complete my answer? Any other suggestion?",['ordinary-differential-equations']
2787878,Building intuition for Riemann-Stieltjes integral,"Soft question: how can I build geometric intuition for and/or visualize the Riemann–Stieltjes integral? This is pretty easy with Riemann-integral, but in case of Riemann-Stieltjes integral I always have to rely on symbolics.","['real-analysis', 'measure-theory', 'riemann-integration']"
2787898,Conditional expectation of product of conditionally independent random variables,"I would like to show the following statement using the general definition of conditional expectation. I believe it is true as it was also pointed out in other posts. Let $X,Y$ be conditionally independent random variables w.r.t a sigma algebra $\mathcal{G}$. Furthermore the expected values of $X,Y$ and $XY$ exist.Then $$\mathbb{E}(XY\mid \mathcal{G})= \mathbb{E}(X\mid \mathcal{G})\mathbb{E}(Y\mid \mathcal{G}).$$ Using the definition of conditional expectation that is used on wikipedia it suffices to show that $(1)$ $\mathbb{E}(X\mid \mathcal{G})\mathbb{E}(Y\mid \mathcal{G})$ is $\mathcal{G}$-measurable and $(2)$ that $\mathbb{E}(\mathbb{E}(X\mid \mathcal{G})\mathbb{E}(Y\mid \mathcal{G})\mathbf{1}_A)=\mathbb{E}(XY\mathbf{1}_A)$ for all $A \in \mathcal{G}$. $(1)$ is clear as the product of two $\mathcal{G}$-measurable functions is $\mathcal{G}$-measurable. For the second part I am able to deduce $\mathbb{E}(XY\mathbf{1}_A)=\mathbb{E}(X\mathbf{1}_A)\mathbb{E}(Y\mathbf{1}_A)/\mathbb{P}(A)=\mathbb{E}(\mathbb{E}(X\mid \mathcal{G})\mathbf{1}_A)\mathbb{E}(\mathbb{E}(Y\mid \mathcal{G})\mathbf{1}_A)/\mathbb{P}(A)$ using conditional independence and the definition of conditional expectation. But then I am lacking any idea to show that the latter equals the LHS in $(2)$. Do you have any ideas how to proceed?","['independence', 'probability-theory', 'conditional-expectation']"
2787911,Why is $\tan^2(y)=x^2$?,"I'm following a proof in a book about the derivative of an arc tangent as follows: By definition, $y = \tan^{-1}(x) \Rightarrow \tan(y)=x$ Therefore, using implicit derivation: $$
\sec^2(y)\frac{dy}{dx}=1 \\
\frac{dy}{dx}=\frac{1}{\sec^2(y)}=\frac{1}{1+\tan^2(y)}=\frac{1}{1+x^2}
$$ Thus completing the proof. I follow well the implicit derivation, and the trigonometric identity $\sec^2(x)=1+\tan^2(x)$, but I don't follow how did it jump to the conclusion that $\tan^2(y)=x^2$. I can't find a reference for this. What would be the reason for this? Thanks.","['derivatives', 'trigonometry', 'calculus']"
2787992,Taking derivative of a differential equation,"When I was reading my lecture note examples, I came across some expression about a DE like this. Edit: $y$ is a function of $x$.
$$ y'=y-x-1$$
And for 
$$f=y-x-1\to f'= \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y} f $$
I have some experience with both multi-variable calculus and DE, but this second expression had me lost. What exactly happened there?","['multivariable-calculus', 'ordinary-differential-equations', 'derivatives']"
2788010,Game: two stacks of coins and reducing by divisor of number of coins on the stack,"Player $A$ and $B$ play a game: There are two stacks of coins, first stack has $a=12$ coins, second - has $b=8$ coins. Each player choose a stack and remove $d$ coins, if choose first stack $d|a$ , otherwise $d|b.$ Player win if other player can't move. Which player has a winning strategy? How change the answer if $a=20,b=28$ ? I know answer by trials if $a=5,b=3$ - $B$ has winning strategy: this player take one coin from this stack, which was choosed by $A$ at last move, if it is possible. If it isn't possible, it means that $A$ get last coin on the stack. I have problem to see a general pattern. I thought that in situation that $a=12,b=8$ $B$ get not one coin, but $gcd(12,8)=4$ coins by analogy to situation described above; I don't see at moment any reason.","['number-theory', 'combinatorics', 'contest-math', 'game-theory']"
2788015,Show that if $X$ is locally compact and $\sigma$-compact then is separable,"I'm trying to solve an exercise that says Show that a locally compact space is $\sigma$ -compact if and only if is separable. Here locally compact means that also is Hausdorff. I had shown that separability imply $\sigma$ -compactness but I'm stuck in the other direction. Assuming that $X$ is $\sigma$ -compact it seems enough to show that a compact Hausdorff space is separable. However I don't have a clue about how to do it. My first thought was try to show that a compact Hausdorff space is first countable, what would imply that it is second countable and from here the proof is almost done. However it seems that my assumption is not true, so I'm again in the starting point. Some hint will be appreciated, thank you. EDIT: it seems that the exercise is wrong. Searching in the web I found a ""sketch"" for a proof that a compact Hausdorff space is not separable: Another natural example: take more than |R| copies of the unit interval
and take their product. This is compact Hausdorff (Tychonov theorem) but
not separable (proof not too hard, but omitted). Hope this helped, Henno Brandsma My knowledge of topology is little and the exercise appear in a book of analysis (this is a part of the exercise 18 on page 57 of Analysis III of Amann and Escher.) My hope is that @HennoBrandsma (an user of this web) appear and clarify the question :)","['general-topology', 'compactness', 'separable-spaces']"
2788018,"Finest locally convex topology - Conway, ex 20, sec 1, chapter 4","I am a beginner in the field of functional analysis and started with Conway's book. On the chapter of topological vector spaces, there is an exercise which I can't crack and would appreciate some help Let $X$ be a infinite dimensional vector space and let $\tau$ be the collection of all subsets $W$ of $X$ such that, if $x\in W$, then there is a convex balanced set $U$ with $x+U\subset W$ and $U\cap M$ open in $M$ for every finite dimensional linear manifold in $X$ (the topology in $M$ is the unique linear topology). a) $(X,\tau)$ is Locally convex space. b) a set $F$ is closed in $X$ if and only if $F\cap M$ is closed for every finite dimensional subspace. ... The first part was ok, but the second part I am having trouble to prove the converse. So far, my conclusion is that b) is equivalent to $O$ is open iff $O\cap M$ is open for any finite dimensional subspace $M$. Which would imply that $\tau$ is the finest linear topology (since every subspace of a TVS is a TVS with induced topology and there is only one linear topology in finite dimensional spaces, right?!). If this makes sense, it means that the strongest linear topology is a locally convex topology, which makes me believe I am missing something here I looked in some free online notes ( http://www.math.uni-konstanz.de/~infusino/Lect12.pdf ) and this topology is called the finest locally convex topology, and its characterization given in b) makes sense in countable dimensional TVS (also in the notes). On Conway's book however, no such countability is mentioned... Anyways, I would appreciate some tips on how to solve this problem and further comments or references on the topic.","['functional-analysis', 'topological-vector-spaces']"
2788024,Is there a non-unital ring such that one of its quotient ring is unital?,"Suppose $R$ is a non unital ring, but it has an ideal $I$ such that $R/I$ is unital, does this kind of animal exist? This problem arouse when I try to show “$R$ is unital, $R/I$ is division ring, then $I$ is maximal. “ I am wondering whether the condition $R$ is unital can be dropped or not. Thank you in advance. By the way, is this kind of problem meaningful? I am pretty addicted to this kind of problems but it seems that nobody cares about it.","['abstract-algebra', 'ring-theory']"
2788149,Diffeomorphism of Lens Spaces,"Let $p,q \in \mathbb{Z}$, with $p > 0$ and $gcd(p,q) = 1$. The Lens space $L(p,q)$ is defined by $\mathbb{S}^3/\mathbb{Z}_p$ where the action is given by $n \cdot (z_1,z_2) = (e^{2\pi i n /p}z_1,e^{2\pi i n q/p}z_2)$. Here we are identifying $\mathbb{S}^3$ with the unit sphere of $\mathbb{C} \times \mathbb{C}$. I wish to show that if $q + q^\prime\equiv 0 \;(mod\; p)$ or $qq^\prime \equiv 1 \;(mod \; p)$ then $L(p,q)$ is diffeomorphic to $L(p,q^\prime)$. I tried defining a map from $\mathbb{S}^3$ to $\mathbb{S}^3$ that could induce such diffeomorphisms but I wasn't able to find it. Any sugestion?","['differential-topology', 'low-dimensional-topology', 'smooth-manifolds', 'manifolds', 'differential-geometry']"
2788232,Integrating $\int_0^1 \frac {\log(1-x)\log^2(1+x)}x \mathrm{d}x$,"Question: Integrate$$\int\limits_0^1dx\,\frac {\log(1-x)\log^2(1+x)}x=-\frac {\pi^4}{240}$$ I'm curious as to if there is a way to integrate this. I've tried using integration by parts to get$$I=-\frac {\pi^2}6\log^22+2\int\limits_0^1dx\,\frac {\operatorname{Li}_2(x)\log(1+x)}{1+x}$$However, I'm not sure how to continue even further. The polylog in the second integrand seems a bit intimidating and I don't see how the first term even helps.","['integration', 'definite-integrals']"
2788367,"Are $(2,28)$ and $(5,3207)$ the only solutions $(m,n)\in\mathbb{N}^2$?","I noticed something as I was playing around with prime numbers. By denoting $p_i$ the $i^{\text{th}}$ prime number, I discovered the following: $$
\begin{align}\prod_{i=1}^2\left(p_i^{ \ 2}+i\right)&= 28^2-27^2 \\ \prod_{i=1}^5\left(p_i^{ \ 2}+i\right)&=3207^2-27^2.\end{align}
$$ Given the general equation $$\prod_{i=1}^m\left(p_i^{ \ 2}+i\right)=n^2-27^2\tag*{$\big(\left(m,n\right)\in\mathbb{N}^2\big)$}$$ I have tested for $5< m\leqslant 32$ and it appears that for $m > 5$, there does not exist such $n$. I know that $n^2-27^2=(n+27)(n-27)$, but this does not help me make much progress. Is this true? Can it be verified? Thus far, it remains a conjecture that the only solutions $(m,n)$ are $(2,28)$ and $(5,3207)$. Source of Testing: $$\text{Alpertron $-$ Integer Factorization Calculator.}$$ Thank you in advance.","['square-numbers', 'number-theory', 'products', 'summation', 'prime-numbers']"
2788397,Why does this seem to converge to $\pi$,"Let $X, Y$ be vectors of length $n$ such that each element is an iid Uniform random variable on $[0,1]$. Define $Z$ by $$Z = \sum_{i=1}^{n}\varphi(i)$$ where $$\varphi(i) =  \begin{cases}
X_{i}^{2} + Y_{i}^{2} & \text{if $X_{i}^{2} + Y_{i}^{2}<1$}\\
0 &\text{otherwise}.
\end{cases}$$ It seems that (I don't think its necessary to put the expectation here, but I sort of used it later on when describing equation (1).) $$\mathbb{E}[\lim_{n\rightarrow \infty}8Z/n] = \pi$$ but I am not sure why, or even if my conjecture actually holds. (I feel like there is a painfully obvious geometric intuition that is just going right over my head.) I haven't been able to get very far into the problem. But below is my best shot at it. As I progress through this work, I get the impression that I am writing increasingly dubious equations, but this is the closest I could get to anything even containing a constant multiple of $\pi$. It looks to me that $\mathbb{P}[\varphi(i)\neq 0] = \pi/4$ and that this is a Bernoulli random variable. From there, I could see that if $\varphi(i)$ were instead defined by $$\bar{\varphi}(i) =  \begin{cases}
1 & \text{if $X_{i}^{2} + Y_{i}^{2}<1$}\\
0 &\text{otherwise}.
\end{cases}$$ I think we get that \begin{equation}
\mathbb{E}[Z] = \mathbb{E}[n \bar{\varphi}(1)] = n\pi/4, \quad (1)
\end{equation} but I have no idea how to handle the sum of $X^{2}_{i} + Y_{i}^{2}$'s here. Is there a reasonable geometric intuition for why $Z/n$ seems to converge to $\pi/8$? If I naively try to write $$\mathbb{E}[Z] = \mathbb{E}[n\varphi(1)] = \mathbb{E}[n(X^{2} + Y^{2})]\mathbb{P}[\varphi(i)\neq 0]$$ Then since $\mathbb{E}[X^{2}] - \mathbb{E}[X]^{2} = 1/12$, $\mathbb{E}[X^{2}] = (1/12) + (1/4) = 1/3.$ Then we would obtain $$\mathbb{E}[n\varphi(1)] = (\mathbb{E}[(X^{2}]+ \mathbb{E}[Y^{2}])\frac{n\pi}{4} = \frac{2}{3} \frac{n\pi}{4} = \frac{n\pi}{6}$$ which does not seem to hold in my numerical experiments.","['monte-carlo', 'probability-theory']"
2788404,Interpolating $f$ and approximating its derivative,"We are given $x_0 = 1, x_1 = \frac{4}{3}$ and $x_2 = 2$.
Find a parabola which agrees with function $f(x) = (x + 1)\sin(x)$ in the given points. Afterwards derive a formula for the
approximation of the $f'$ i.e. the derivative in $x_1$. What is the approximation of $f'(x_1)$ for function f? I did the first step using interpolation and got the polynomial $p(x) = -1.0647x^2 + 4.2390x - 1.4914.$ Now I do not know how to use this to approximate the derivative. Should I just compute the derivative of the polynomial $p$? What about the approximation then?","['derivatives', 'numerical-methods', 'interpolation', 'approximation']"
2788412,$L^p$ norm convergence implies $L^p$ convergence,"I've been struggling with the following, and can't seem to determine a good direction to follow: Let $f_n,f\in L^p$ where $f_n\rightarrow f$ almost everywhere and $\infty>p\geq1$. Then $\underset{n\rightarrow \infty}{\lim}\Vert f_n-f\Vert_p=0$ if and only if $\underset{n\rightarrow \infty}{\lim}\Vert f_n\Vert_p=\Vert f\Vert_p$. Assuming that there is $L^p$ convergence it is easy to show that $\Vert f_n\Vert\rightarrow \Vert f\Vert$. However I've been stuck on the other direction, attempting to construct a dominating function (but failing). It seems to me that there should be a simple answer I'm overlooking.","['real-analysis', 'lp-spaces', 'measure-theory']"
2788415,Non-linear system of equations over the positive integers — more unknowns than equations,"This exercise appeared on a german online tutoring board and caught my attention but stumbled me for hours. The task is to find 6 distinct positive three digit integers satisfying: $x_{1}+x_{2}+x_{3}+x_{4}+x_{5}+x_{6}=4.198$ $x_{1}^{2}+x_{2}^{2}+x_{3}^{2}+x_{4}^{2}+x_{5}^{2}+x_{6}^{2}=3.215.224$ $x_{1}^{3}+x_{2}^{3}+x_{3}^{3}+x_{4}^{3}+x_{5}^{3}+x_{6}^{3}=2.600.350.972$ According to the power mean inequality or Cauchy-Schwarz the numbers must lie relatively closely together. However a brief search lead nowhere.
For simplicity I set $4.198=A$, $3.215.224=B$ and $2.600.350.972=C$ and then my approach was to manipulate the three equations and perhaps use that no square is negative. For example $6B-A^{2}=\sum_{i<j}(x_i-x_j)^{2}=(x_{1}-x_{2})^2+(x_{1}-x_{3})^2+...+(x_{2}-x_{3})^2+...+(x_{5}-x_{6})^2$  where every distinct pair appears exactly once. If now $6B-A^{2}$ would result in something like $5*1^{2}+4*2^{2}+3*3^{2}+2*4^{2}+1*5^{2}=105$
 we could tell exactly what our $x$ were. Unfortunately it gives $1.668.140$ and we cannot conclude much. Similar reasoning with factoring to $\sum_{i<j}(x_i-x_j)^{4}$ didn't help. If there exists such a factorization, my intuition says it would only make sense if the $x$ form an arithmetic sequence, otherwise we would get different factors on the right side that can't appear on the left side. (does this sound reasonable?) But substituting gives no solution. Also, I don't know what other, more sophisticated factorization would lead me to the solution. I'm running out of ideas, how can this problem be solved? Links to the original problem: https://www.geocaching.com/geocache/GC69JE0_lotto-mal-anders https://www.gutefrage.net/frage/matherechenart-gesucht-mehrere-variablen-mit-festen-ergebnis?foundIn=unknown_listing","['diophantine-equations', 'systems-of-equations', 'algebra-precalculus', 'natural-numbers', 'elementary-number-theory']"
2788449,"If $(x+1)(x+3)(x+5)(x+7)= 5760$, what are the possible values of $x$?","I was doing some questions related to quadratic equations and I stumbled upon this question: Solving the equation
$$(x+1)(x+3)(x+5)(x+7)= 5760.$$ One of the hints was to convert it into a quadratic equation. I tried to but couldn't get anything meaningful. Is there any way in which I can solve bi-quadratic equations fast? (Sorry if this strikes as basic, I am just a highschooler.)","['algebra-precalculus', 'polynomials', 'quadratics']"
2788455,Checking directly the integral of $1/(z-a)$ around a disc centered at the origin is $2\pi i$,We know by Cauchy's integral formula that $$\int\limits_{|z|=1} \frac{1}{z-a} \ dz = 2 \pi i$$ for $|a| < 1$. We can try to calculate this directly. If $a = 0$ then simply substituting in $z = e^{i\theta}$ works. However if $a \neq 0$ say $a = \frac{1}{2}$ then upon substituting we get $$\int\limits_{|z|=1} \frac{1}{z-\frac{1}{2}} \ dz = i\int\limits_{0}^{2\pi} \frac{e^{i\theta}}{e^{i\theta}-\frac{1}{2}} \ d\theta = i\int\limits_{0}^{2\pi} 1+\frac{1}{2e^{i\theta}-1} \ d\theta = 2 \pi i + i\int\limits_{0}^{2\pi} \frac{1}{2e^{i\theta}-1} \ d\theta$$ From the integral formula we know that the last integral must be zero. Is there an elementary way of seeing this?,"['complex-analysis', 'integration']"
2788460,Proving the formula of a function much similar to the Dirichlet kernel.,"Many of us know about the Dirichlet kernel which is on lower level stated as $$D_n(\theta) =\frac {\sin \left(n+\frac 12\right)\theta}{\sin \frac {\theta}{2}}=1+2\sum_{r=1}^n \cos (r\theta)$$ I have a devised another similar formula for $$\frac {\cos \left(n+\frac 12\right)\theta}{\cos \frac {\theta}{2}}$$ which I have verified and proved using some simple trigonometry and telescoping series but I want to know if I could somehow prove the result using complex numbers which I know is possible in the case of Dirichlet kernel. So the similar formula I devised be defined as $$F_n(\theta)=
\frac {\cos \left(n+\frac 12\right)\theta}{\cos \frac {\theta}{2}}=  (-1)^n\left(1+2\sum_{r=1}^n (-1)^r \cos(r\theta)\right) $$ Any hints or some kind of help would be very beneficial.","['complex-numbers', 'trigonometry', 'trigonometric-series', 'calculus', 'complex-analysis']"
2788462,Finding a pivotal function from uniform distribution,"Let $X_1,...,X_n$ be a random sample from a uniform distribution $(0,\theta)$. I have found the maximum likelihood estimator to be $Y_n = \text{max}\{X_1,...,X_n\}$ The solution then claims that $\frac{Y_n}{\theta}$ is a pivot. I don't understand how to show that it is a pivot.",['statistics']
2788470,Reciprocal of a convex decreasing function:,"I have the following math problem that I haven't been able to solve. I would be very grateful of any idea or solution you can give me. Thanks! Let $f(x): [0,\infty)\rightarrow (0,\infty)$ such that $f'(x)<0$ and  $f''(x)>0$. What are the necessary and sufficient conditions for $1/f(x)$ to be concave. I approach to this problem by using derivatives and got to the equation: $$2(f'(x))^2 -f''(x)f(x)\leq0$$ Yet I feel something smarter can be done that gives a more precise characterization. Thanks!","['derivatives', 'convex-analysis']"
2788509,What's the expected value of working modules?,"An electric system is made up of $k$ modules. In each module, there are $N$ resistors in series. What is the expected value of working modules if $m$ resistor break down? A module is not working if there is at least $1$ bad resistor in it. I can calculate the trivial cases (for example, $m=1$ , or $m>(k-1)*N$ ), but I can't find out the general solution. For $m\leq N$ , I think I can use the binomial distribution: The probability for a module to have $n$ bad resistor is $$p(n)=\binom{N}{n}\left(\frac{1}{N}\right)^n\left(1-\frac{1}{N}\right)^{N-n}$$ and we would need to sum up this probability to cover all of the possible cases and multiple them with $1, 2, \dots N$ to get the expected value, but it seems to be a really big sum. Bounty: I'd really like to have a solution to this problem, so I started a bounty.",['probability']
2788523,Integral $\int_0^1 \frac{\log^4(x)}{(1-x)^4}dx$,"I am trying to evaluate $$\int_0^1 \frac{\log^4(x)}{(1-x)^4}\,dx$$ This type of integral already has two answers here: Closed form for $\int_0^1 \frac {\log^n(x)}{(1-x)^m} dx$ . However I desire to evaluate it with a different method. Starting with: $$I(a)=\int_0^1 \frac{\ln^n(x)}{a-x}\,dx$$ we have that $$\frac{1}{a-x}=\sum_{j=0}^{\infty} \frac{x^j}{a^{j+1}}$$ therefore $$I(a)=\sum_{j=0}^{\infty} \frac{1}{a^{j+1}} \int_0^1 x^j\ln^n(x) \,dx$$ using the substitution $$x=e^{-y}$$ $$I(a)=\sum_{j=0}^{\infty} \frac{(-1)^n}{a^{j+1}}\int_0^{\infty}y^ne^{-y(j+1)}\,dy $$ now with $(j+1)y=t\,$ and using gamma function: $$I(a)=\sum_{j=0}^{\infty} \frac{(-1)^n n!}{a^{j+1} (1+j)^{n+1}}= \sum_{j=1}^{\infty} \frac{(-1)^n n!}{a^{j} j^{n+1}}=(-1)^n n!\operatorname{Li}_{n+1}\left(\frac{1}{a}\right)$$ Where $\operatorname{Li}_n(z)=\sum_{k=1}^{\infty}\frac{z^k}{k^n}$ is the Polylogarithm. Now we have to derivate $I(a)$ four times with respect to $a$ using $\frac{d}{dz} \left(\operatorname{Li}_n(z))= \frac{1}{z}\operatorname{Li}_{n-1}(z)\right)$ then plug in $a=1$ and $n=4$ in order to get the desired integral, but I am struggling here. $$4!\int_0^1 \frac{\log^4(x)}{(1-x)^4}\,dx=(-1)^nn!\frac{d^4}{da^4}\left(\operatorname{Li}_{n+1}\left(\frac{1}{a}\right)\right)$$ For the first derivate: $$\frac{d}{da} (\operatorname{Li}_{n+1}(1/a)) = \frac{1}{\frac{1}{a}}\operatorname{Li}_{n}(1/a)\frac{-1}{a^2} = - \frac{1}{a}\operatorname{Li}_{n}(1/a)$$ Second time: $$\frac{d}{da} \left(-\frac{1}{a}Li_n(1/a)\right) = \frac{1}{a^2}\left(\operatorname{Li}_{n}(1/a) + \operatorname{Li}_{n-1}(1/a)\right)$$ Now when I get to the fourth one: $$\frac{1}{a^4}\left(6\operatorname{Li}_{n}\left(\frac{1}{a}\right)+12\operatorname{Li}_{n-1}\left(\frac{1}{a}\right)+6\operatorname{Li}_{n-2}\left(\frac{1}{a}\right)+\operatorname{Li}_{n-3}\left(\frac{1}{a}\right)\right)$$ But obviously this is wrong because if one would plug in $n=4$ and $a=1$ the last term will be $\operatorname{Li}_1(1)$ which is not nice. Could you help me finish this or perhaps find another closed form for $\displaystyle{\int_0^1 \frac {\log^n(x)}{(1-x)^m}\,dx}$ using this method?","['derivatives', 'proof-verification', 'closed-form', 'integration', 'polylogarithm']"
2788577,"If $f$ is a measurable random field, then $(ω,x)↦E[f(x)\mid F](ω)$ has a measurable version $g$ and $E[f(X)\mid F]=g(X)$ for all $F$-measurable $X$","Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $\mathcal F\subseteq\mathcal A$ be a $\sigma$-algebra on $(\Omega,\mathcal A)$ $(E,\mathcal E)$ be a measurable space $f:\Omega\times E\to\mathbb R$ be $\mathcal A\otimes\mathcal E$-measurable I'll write $f(x)$ instead of $f(\;\cdot\;,x)$ for $x\in E$ and $f(X)$ instead of $f(\;\cdot\;,X(\;\cdot\;))$ for $X:\Omega\to E$. Assume $$\operatorname E\left[\left|f(x)\right|\right]<\infty\;\;\;\text{for all }x\in E.\tag1$$ By a monotone class argument, it's easy to show that there is a $\mathcal F\otimes\mathcal E$-measurable $g:\Omega\times E\to\mathbb R$ with $$\operatorname E\left[f(x)\mid\mathcal F\right]=g(x)\;\;\;\text{almost surely for all }x\in E.\tag2$$ I've found the following exercise in the book Stochastic Flows and Stochastic Differential Equations (Exercise 1.4.11): Let $X:\Omega\to E$ be $\mathcal F$-measurable with $$\operatorname E\left[\left|f(X)\right|\right]<\infty.\tag3$$ Show that $$\operatorname E\left[f(X)\mid\mathcal F\right]=g(X)\;\;\;\text{almost surely}.\tag4$$ How can we prove that? Unfortunately, I don't have any idea for an argument. I don't think that the author has forgotten to add a crucial assumption, but nevertheless I've tried to consider the case where $E$ is a separable metric space, $\mathcal E$ is the Borel $\sigma$-algebra on $E$ and $f(\omega,\;\cdot\;)$ is continuous for allmost all $\omega\in\Omega$. With that assumption it's easy to see that $(4)$ is at least satisfied if $X$ has finite range. Now, since $E$ is separable, we can find a sequence $(X_n)_{n\in\mathbb N}$ of $\mathcal F$-measurable functions $X_n:\Omega\to E$ with finite range such that $$d(X_n,X)\le d(X_{n+1},X)\;\;\;\text{for all }n\in\mathbb N\tag5$$ and $$d(X_n,X)\xrightarrow{n\to\infty}0.\tag6$$ However, I don't know how we need to proceed from here. So, the question is: How can we prove the claim in the general case and, if that's not possible, how do we need to proceed in the described special case?","['stochastic-processes', 'probability-theory', 'conditional-expectation', 'stochastic-analysis']"
2788596,How to calculate the Eisentein series $G_{4}(\tau)$ and $G_{6}(\tau)$,"I would like to calculate the $j$-invariance of tori $\Sigma=\mathbb{C}/L$, where lattice $L:=<1,i>$. And I try to show that the ellipict $y^2=x^3-x$ is isomorphism with $\mathbb{C}/L$. From the Weierstrass function $\mathscr{P}$, we have $\forall \tau \in \mathbb{H}$ 
$$g_{2}(\tau)=60G_{4}(\tau), g_{3}(\tau)=140G_{6}(\tau),$$
Here, we know $$G_{4}(\tau)=G_{4}(L)=\sum_{(m,n)\in \mathbb{Z}^2 \setminus (0,0)} \frac{1}{(m+ni)^4},$$
$$G_{6}(\tau)=\sum_{(m,n)\in \mathbb{Z}^2 \setminus (0,0)} \frac{1}{(m+ni)^6}$$ Is there an answer to two series? EDIT: I look up a theorem: If $k \geq 4$ is even, and $Im(\tau)>0$, then
$$E_{k}(\tau)=2\zeta(k)+\frac{2(-1)^{\frac{k}{2}}}{(k-1)!}\sum_{r=1}^{\infty}\sigma_{k-1}(r)e^{2\pi i \tau r}，$$
where divisor function $\sigma_{l}=\sum_{d|r}d^l.$ But how to get the answer $j$-invariance of tori:
$$ j=\frac{g_{2}^3(\tau)}{\Delta},$$
where $\Delta(\tau)=g_{2}^3(\tau)-27g_{3}^2(\tau)$.","['riemann-surfaces', 'algebraic-geometry', 'elliptic-curves', 'algebraic-curves', 'sequences-and-series']"
2788615,"Integrating $\int^\infty_ 0\frac{x^2}{e^{x^2} - 1}\,\mathrm dx$","How can I integrate
$$\int^{\infty}_ 0 \frac{x^2}{{e^{x^2}} - 1}\,\mathrm{d}x?$$ I have tried using the Gaussian Integral Formula and Integration by parts, but have made no progress so far.","['improper-integrals', 'integration', 'definite-integrals']"
2788617,Solution to Dirichlet boundary value problem on upper halve plane using Green's function,"Currently I am studying for an exam about partial differential equations. While looking through some of the exercises concerning Green's function on the plane, I came across a rather impenetrable-seeming integral connected to a Dirichlet BVP. As the textbook we are using (Partial Differential Equations, Peter Olver) does not provide clear examples on this topic, I figured that the internet would be a fine next step towards a solution. I also think that some experienced mathematicians out there might enjoy an exercise, and explaining it. Let me be clear: this exercise will not be examined. The problem is as follows: Solve for $u$ where
\begin{align}
-\Delta u (x,y)= \frac{1}{1+y}\text{ on }\{(x,y):y>0\}\text{ and }u(x,0) = 0\text{ for all }x\in \mathbb{R}.
\end{align} Now, we are asked to use the Green's function for the upper half plane to find a solution. Using the method of images, one finds such a function quite easily:
\begin{align}
G(x,y;\theta,\eta) = \frac{1}{4\pi}\log\left(\frac{(x-\theta)^2 + (y-\eta)^2}{(x-\theta)^2+(y+\eta)^2}\right).
\end{align}
By Green's representation formula, the solution is then given by
\begin{align}u(x,y) = \frac{1}{4\pi}\int_{-\infty}^{\infty}\int_0^{\infty}\frac{1}{1+\eta}\log\left(\frac{(x-\theta)^2 + (y-\eta)^2}{(x-\theta)^2+(y+\eta)^2}\right)\,\mathrm{d}\eta\,\mathrm{d\theta}.
\end{align}
I am wondering, did the author perhaps choose this example problem because the resulting integral would be solvable? If so, could someone shed light on the problem? And if not, is there a better way to solve it? My thanks.","['partial-differential-equations', 'multivariable-calculus', 'greens-function', 'integration', 'boundary-value-problem']"
2788631,Mathematical notation for matrix slicing,"Suppose I have a matrix $M$. From that matrix, I create a matrix $N$ by only keeping some rows of $M$. The rows to keep are given in a vector or set called $r$. Then I do the same with columns in vector/set $c$ in order to obtain matrix $Q$. In a notation similar to Matlab or Numpy, this is realized by slicing: N = M[r, :] # keep only rows whose index is in r
Q = N[:, c] # keep only columns whose index is in c How does the mathematical notation for such a construct look? I'm also happy with a solution that does the slicing in one step.","['matrices', 'notation', 'linear-algebra']"
2788649,An equation for Fibonacci numbers [duplicate],"This question already has answers here : Prove $f_{n+1}^2+f_n^2=f_{2n+1}$ (3 answers) Closed 6 years ago . Let $F(n)$ be $n$-th Fibonacci-number. Then the following holds:
$$F(n)^2+F(n+1)^2=F(2n+1).$$
I have proved, but I want a brief proof.
Do you know anything about it?","['induction', 'fibonacci-numbers', 'alternative-proof', 'discrete-mathematics']"
2788675,$T:X\to X$ is absolutely summing if and only if $\sum_{n=1}^\infty \|Tx_n\|<\infty$ whenever $\sum_{n=1}^\infty x_n$ is unconditionally convergent.,"The following proposition is extracted from Topics in Banach Space Theory by Albiac and Kalton . Proposition $8.2.2$: An operator $T:X\to X$ is absolutely summing if and only if $\sum_{n=1}^\infty \|Tx_n\|<\infty$ whenever $\sum_{n=1}^\infty x_n$ is unconditionally convergent. In the book, the authors do not provide a proof of the above theorem and they say the proof is routine.
However, I do not know how to prove it. 
Any hint is appreciated.
If anyone knows of any reference containing a proof of the proposition, may I have it? Definitions : Let $X$ and $Y$ be Banach spaces.
$T$ is said to be absolutely summing if there is a constant $C$ such that for all choices of $(x_k)_{k=1}^n$ in $X,$
$$\sum_{k=1}^n\|Tx_k\|\leq C\sup\bigg\{ \sum_{k=1}^n|x^*(x_k)|:x^*\in X^*, \|x^*\|\leq 1 \bigg\}.$$ We say that $\sum_{n=1}^\infty x_n$ is unconditionally convergent if one of the following holds: $(1)$ $\sum_{n=1}^\infty x_{\pi(n)}$ converges for every permutation $\pi$ of $\mathbb{N}.$ $(2)$ The series $\sum_{k=1}^\infty x_{n_k}$ converges for every increasing sequence $(n_k)_{k=1}^\infty$ $(3)$ The series $\sum_{n=1}^\infty \varepsilon_n x_n$ converges for every choice of signs $(\varepsilon_n).$ $(4)$ For every $\varepsilon>0,$ there exists $n$ such that if $F$ is a finite subset of $\{n+1,n+2,...\},$ then 
$$\bigg\| \sum_{j\in F} x_j \bigg\|<\varepsilon.$$","['real-analysis', 'banach-spaces', 'reference-request', 'functional-analysis', 'sequences-and-series']"
2788765,Find $\lim \mathbb{E} \frac{X_{n}^{2}}{\log(1+X_{n}^{2})}$,"Consider $Z_{n} = \frac{X_{n}^{2}}{\log(1+X_{n}^{2})}$, in which $X_n$ denotes a Gaussian random variable with zero expected value and variance equals $1/n$. Now we want find 
$\lim_{n \to \infty} \mathbb{E} \frac{X_{n}^{2}}{\log(1+X_{n}^{2})}$.
I thought about find characteristic function of $Z_{n}$ and so it will be easy to find expectation. But I guess there should be easier approach. Any ideas ?",['probability-theory']
2788799,Another corollary of the Lagrange's Mean Value Theorem,"I'm having trouble at proving the following theorem, stated as a corollary of the Lagrange's Mean Value Theorem in the book I'm reading (""Curso de Análise"", Elon Lages Lima): Let $f: \left(a, b\right) \rightarrow \mathbb{R}$ be a differentiable function except, possibly, at $c \in \left(a, b\right)$, with $f$ continuous at $c$. If the limit: $$\lim_{x \rightarrow c} f'(x)=L$$ exists, then $f'(c)$ exists and its value is $f'(c)=L$. My attempt at proving this was: ""As long as $c \neq a$ and $c\neq b$, then there exists $A \in \left(a, c\right), B \in \left(c, b\right)$ such that $f$ is continuous at $(A, c)\cup(c,B)$ (because $f$ is differentiable in these open intervals), $f$ is continuous at $A$, $B$ and $c$ (because $f$ is differentiable at $A$ and $B$, when the intervals under consideration are $(a,c)$ and $(c,b)$ and $f$ is supposedly continuous at $c$). Then $f$ is (uniformly) continuous in $\left[A,B\right]$. In particular, $f$ is uniformly continuous in every closed interval contained in $\left[A,B\right]$. Let $\epsilon > 0$ be given, and let's keep fixed a corresponding $\delta > 0$ which satisfies the uniform continuity. Then $f$ is (uniformly) continuous at $[c-\delta, c]$ and $[c, c+ \delta]$ and differentiable at $(c-\delta, c)$ and $(c, c+ \delta)$, for $\epsilon$ small enough (which makes $\delta$ to be small enough and the intervals $[c-\delta, c]$ and $[c, c+ \delta]$ to be contained in $[A,B]$). By the Lagrange's Mean Value Theorem, we have: $$\exists c_1 \in (c- \delta, c): \ f'(c_1)[c-(c-\delta)]=\delta f'(c_1)=f(c)-f(c - \delta)$$
$$\exists c_2 \in (c, c+ \delta): \ f'(c_2)[(c+\delta)-c]=\delta f'(c_2)=f(c + \delta)- f(c)$$ By uniform continuity of $f$, $\epsilon \rightarrow 0 \implies \delta \rightarrow 0$ and, as $c\in (a,b)$ is fixed, $c_1=c_1(\delta(\epsilon), f), c_2=c_2(\delta(\epsilon),f )$, with: $$\lim_{\epsilon \rightarrow 0} c_1(\delta(\epsilon), f)= c=\lim_{\epsilon \rightarrow 0} c_2(\delta(\epsilon),f)$$ Then, we finally have: $$L=\lim_{x \rightarrow c^{-}} f'(x)=\lim_{\epsilon \rightarrow 0} f'(c_1(\delta(\epsilon), f))=\lim_{\epsilon \rightarrow 0} \frac{f(c) - f(c-\delta(\epsilon))}{\delta}=\lim_{x \rightarrow c^{-}} \frac{f(c) - f(x)}{c-x}$$ $$L=\lim_{x \rightarrow c^{+}} f'(x)=\lim_{\epsilon \rightarrow 0} f'(c_2(\delta(\epsilon), f))=\lim_{\epsilon \rightarrow 0} \frac{f(c+\delta(\epsilon))-f(c)}{\delta}=\lim_{x \rightarrow c^{-}} \frac{f(x) - f(c)}{x-c}$$ Since both lateral limits exist and are equal, the theorem is proved and: $$L=\lim_{x \rightarrow c} \frac{f(x)-f(c)}{x-c}\equiv f'(c)$$ q.e.d."" Am I missing anything? Or is the theorem correctly proved?","['derivatives', 'real-analysis', 'uniform-continuity']"
2788802,The Grigorcuk Group solvable? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question The description of the Grigorcuk Group is given here https://en.wikipedia.org/wiki/Grigorchuk_group . Is this group Solvable? Thanks for any help.","['abstract-algebra', 'infinite-groups', 'group-theory']"
2788896,"Spivak Calculus 4-th Ed., Chapter 2, Exercise 13a, Understanding the proof of $\sqrt3$ being irrational.","Problem Statement: Prove that $\sqrt3$ is irrational. Hint: To treat $\sqrt3$, for example, use the fact that every integer is of the form $3n$, $3n+1$ or $3n+2$. Solution from the ""Answers"" Chapter: Since
$$
\\
(3n+1)^2 = 9n^2 +6n + 1 = 3(3n^2+2n)+1
\\
(3n+2)^2 = 9n^2 +12n + 4 = 3(3n^2+4n +1)+1
$$
He proceeds then to state that if $k^2$ is divisible by 3, then so is $k$.
I have a hard time understanding what $k$ he is talking about in these equations, it hasn't been defined earlier. He then proceeds: Supose $\sqrt3$ were rational, and let $\sqrt3 = p/q$, where $p$ and $q$ have no common factor. Then $p^2=3q^2$, so $p^2$ is divisible by 3, so $p$ must be. Thus, $p=3p'$ for some natural number $p'$ and consequently $(3p')^2=3q^2$, or $3(p')^2=q^2$. Thus, $q$ is also divisible by 3, a contradiction. I have a hard time linking the first part where he states that "" $k^2$ being divisible by 3 leads to $k$ being divisible by 3 "" and the second part. Where does the first conclusion come from? NOTE: Problem statement in the book in the same manner requests the proof of $\sqrt5$ and $\sqrt6$ being irrational. However, in this question I'm interested in the proof for the case of $\sqrt3$.","['rationality-testing', 'calculus', 'proof-explanation']"
2788924,Finding critical points of a triple variable function,"What are the critical points of the function: $f(x,y,z)=\frac{1}{z^2+x^2+1+y^2}$? Identify as a local minima, maxima, or saddle points. So I know you have to take the gradient and set it equal to $(0,0,0)$. That will get you all your critical points. How do I identify it as a local minima, maxima, or a saddle point?","['multivariable-calculus', 'maxima-minima']"
2788939,"$\int_H c_1 (\mathcal O_{\mathbb P^2(k)}|_H)$, integral of restriction of Chern class to a projective hypersurface","Let $H$ be a degree $d$ hypersurface of $\mathbb P^2$. I want to calculate the integral $\int_H c_1 (\mathcal O(k)|_H)$. I think I got a proof for this, but im am not sure if I got all the details right. I would appreciate if you could take a look on it: First we note that as $c_1(\mathcal O(k))= k \cdot c_1(\mathcal O(1))$, it suffices to consider the case $k=1$. If we denote by $i$ the inclusion map, then $\mathcal O(1)|_C$   The integral above is the pairing 
$$\langle c_1(i^*\mathcal O(1)),[H]\rangle = \langle i^*c_1(\mathcal O(1)),[H]\rangle= \langle c_1(\mathcal O(1)),i_*[H]\rangle.$$ $i_*[H]$ is Poincaré dual to $c_1(\mathcal O[H])$. As $H$ is of degree $d$, we have  $\mathcal O[H])=\mathcal O(d)$. Thus 
$$\langle c_1(i^*\mathcal O(1)),[H]\rangle  = \langle c_1(\mathcal O(1))\cup c_1(\mathcal O(d)),[\mathbb P^2]\rangle  $$ As further $c_1(\mathcal O(1))$ is Poincaré dual to a linear hyperplane which intersects the degree $d$ hypersurface $H$ in exactly $d$ points (counting multiplicities), the Poincaré dual $P$ of $c_1(\mathcal O(1))\cup c_1(\mathcal O(d))$ is the linear combination of these $d$ points. Applying the definition of Poincaré duality on the constant $1$-function, we get 
$$  \langle c_1(\mathcal O(1))\cup c_1(\mathcal O(d)),[\mathbb P^2]\rangle = \langle P, 1|_P\rangle =d. $$ Summarizing we get $\int_H c_1 (\mathcal O(k)|_H)=d\cdot k.$ Is what I wrote right? Is there a shorter proof for this?","['algebraic-topology', 'characteristic-classes', 'complex-geometry', 'algebraic-geometry']"
2788960,Let $X$ locally compact and $\sigma$-compact. Then any open subset is also $\sigma$-compact,"Im stuck with this exercise Show that if $X$ is locally compact and $\sigma$-compact then any open subset is also $\sigma$-compact. Note: here locally compactness imply that the space is also Hausdorff. This exercise is the last part of a longer exercise where the previous part was this . If the linked exercise would be true (but it is not) then a locally compact and $\sigma$-compact space would be separable, and because I also know that open sets of locally compact spaces are also locally compact then the proof would be (I guess) easier to prove following this path. However, as I said, the statement of $X$ being locally compact and $\sigma$-compact imply that it is also separable is not true in general. Then its possible that also this part of the exercise is wrong (that is, that the statement to be proved would be false). In any case I dont find something to work to prove the statement. My first attempt was trying to show that if $X$ is Lindelöf, $\sigma$-compact and locally compact then any open set is also Lindelöf, but I dont found a way to show it, thus I'm lost again in the starting point. The exercise appear (as stated in the linked question) in a book of analysis so it is supposed that it can be solved using elementary notions of topology. Can someone confirm if the statement to be proved is really true or if, by the contrary, it is other mistake in the book? If it would be solvable, can someone give me a hint? Thank you.","['general-topology', 'analysis']"
2789036,How to prove isomorphism with the Dihedral group,"I have a group that I'm trying to prove is isomorphic to the Dihedral group. I know that it is finite, that it is generated by two elements $\alpha$ and $\beta$ such that: $\alpha^2=\beta^n=1$ and that $\alpha\beta\alpha=\beta^{-1}$ .
EDIT: also, $\alpha\neq \alpha^2$ and $\beta\neq \beta^2\neq\ldots\neq\beta^n$ . I also know that is has at least $2n$ unique elements. EDIT: Is this assumption redundant? EDIT: It is redundant for $n$ >2. For $n=2$ , $\alpha\neq\beta$ is enough (i.e. $D_2\cong C_2\times C_2$ ). Is this enough in order to imply that this group is the Dihedral group with $2n$ elements? Will appreciate any help :)","['finite-groups', 'dihedral-groups', 'group-theory']"
2789060,Matrix version of Pythagoras theorem,"Can I find a solution for $C_{n\times n}$, explicitly, for the given $A_{n\times n}$ and $B_{n\times n}$ such that
$AA^{T} + BB^{T} = CC^{T}$? Here $A^{T}$ denotes the transpose of $A$ and all the matrices ($A$, $B$, and $C$) are real. I think there is a solution for $C$ as there are $n^2$ unknowns (the $n^2$ components $C$) and $n^2$ equations (comparing the components of LHS and RHS). Note: If it can not be solved (explicitly), then one can assume mild conditions (like positive definiteness, symmetry) on $A$ and $B$.","['matrices', 'matrix-equations', 'linear-algebra', 'pythagorean-triples']"
2789064,How do you prove that a plane intersecting a cone gives an ellipse?,"I know why it can also give a parabola when the plane's slant is equal to the cone's slant or an hyperbola when the plane's slant is parallel to the cone's axis, but I don't mathematically understand how it can give an ellipse. What I've tried so far is substituting the equation of a plane $z=my-c \ $ into the equation of a cone, resulting in 
\begin{array}{rcl}
\dfrac{(h-my+c)^2}{h^2} &=& \dfrac{x^2+y^2}{r^2} \\[0,4cm]
\iff \dfrac{(h+c)^2-2(h+c)my+m^2y^2}{h^2}  &=& \dfrac{x^2+y^2}{r^2} \\[0,4cm]
\iff \dfrac{h^2+2hc+c^2-2hmy-2cmy+m^2y^2}{h^2} &=& \dfrac{x^2+y^2}{r^2} \\[0,4cm]
\iff r^2\left(1+\dfrac{2c}{h}+\dfrac{c^2}{h^2}-\dfrac{2my}{h}-\dfrac{2cmy}{h^2} + \dfrac{m^2y^2}{h^2}\right) &=& x^2+y^2 \\[0,4cm]
\iff r^2+\dfrac{2cr^2}{h}+\dfrac{c^2r^2}{h^2} - \dfrac{2r^2my}{h} - \dfrac{2r^2cmy}{h^2} + \dfrac{r^2m^2y^2}{h^2} &=& x^2+y^2 \\[0,4cm]
\iff A - By - Cy + Dy^2 &=& x^2+y^2 \\[0,4cm]
\iff \alpha y^2+\beta y + \gamma &=& x^2+y^2 \\[0,4cm]
\end{array}
but I don't see how this last equation can be transformed into an equation of an ellipse. Thanks in advance.","['conic-sections', 'geometry']"
2789068,Is there any alternative way to simulate a continuous time model instead of discrete?,"Normally I use the exponential matrix method to turn continuous time model to discrete time model. In this case, it's SS-models: $$\dot x = Ax + Bu + Eu \\ y = Cx + Du$$ $$\begin{bmatrix}
A_d & B_d &E_d \\ 
0 &I  &I 
\end{bmatrix} = e^{\begin{bmatrix}
A & B & E\\ 
0 & 0 & 0
\end{bmatrix}}T_s$$ Where $T_s$ is the sampling time. But I have a problem with it comes to this and it's because of the simulation of nonlinear systems. If I have a nonlinear model: $$\dot x = f(x, u)$$ And I want to linearize it in the state $x_0 = 0$. I know that it will be nothing left of the dynamics. Assume that we linearize at the initial state and we have some damping in our system, then the damping factor is going to be zero. I going to do nonlinear numerical simulation of a discrete system. So my procedure is going to be: Linearize the system in state $x$. E.g If we have the damping $Bx_2^2$ the it would be linearized to $2Bx_2$. Turn it to discrete Do the simulation and find state $x$ Jump to stage 1. A lot of times, my simulation breaks. To do a simulation, I need to linearize the system first and then turn it to a discrete form, as I have been learned. Question: Is there any simpler way to simulate a nonlinear continuous time model?
Is there an alternative way to turn the model to discrete? Nonlinear simulation seems to be so sensitive to against everything. EDIT: Here is the code. Assume that you know A, B and sample time h. % Compute sizes
a1 = size(A,2) + size(B,2) - size(A,1);
b1 = size(A,2);
a2 = size(A,2) + size(B,2) - size(B,1);
b2 = size(B,2);
% Compute square matrix
M = [A B; zeros(a1, b1)  zeros(a2, b2)];
M = expm(M*h);
% Find the discrete matrecies
Ad = M(1:size(A,1), 1:size(A,2));
Bd = M(1:size(B,1), (size(A,2) + 1):(size(A,2) + size(B,2)));","['control-theory', 'simulation', 'ordinary-differential-equations', 'discrete-mathematics']"
2789094,"Deskew and rotate a photographed rectangular image (aka ""perspective correction"")","I photographed a rectangular shape, but: the camera angle is not perfect the image is possibly rotated. Given the coordinates of 8 points $ A_1 (x_1, y_1), ..., A_8 (x_8, y_8)$, how to transform the input coordinates into perfectly rectangular output coordinates? Note: 1) Additional information: the deskewed rectangle has a width $W = 21\ cm$ and a height $H = 29.7\ cm$. Also $A_1 A_2 A_4 A_3$ is a square of 1 cm x 1 cm, and $A_5 A_6 A_8 A_7$ too. 2) The problem will probably have multiple solutions, up to rotations of 90 or 180 degrees 3) It's probably possible with only 6 points (or even less?) but I'd like to make use of all the 8 points to improve quality (it's an overdetermined problem, so using least-squares could probably help, but I don't know how to do it in this context)","['coordinate-systems', 'rectangles', 'linear-transformations', 'geometry']"
2789097,Bound $|a-b|^p $ by $|a^p - b^p| $.,"I assume that $a>0$ and $b>0.$ I want to know if there is a bound of the type $|a-b|^p \leq K |a^p - b^p|$ where $p>1$ and $K$ is a constant depending on $p.$ I would like to know the explicit form of the constant $K$ if there exists one.
I know that $(a+b)^p \leq 2^p (a^p + b^p)$ but this doesn't help.","['algebra-precalculus', 'inequality', 'upper-lower-bounds', 'calculus']"
2789108,Preimage set of complex polynomial is connected.,"Let $f(z) \in \mathbb{C}[z]$ with deg$(f) \geq 1$. Then $\{z \in \mathbb{C} : |f(z)| > 1\}$ is connected. I know that $f(z)$ is holomorphic, so continuous on $\mathbb{C}.$ I dont think that preimage of connected set under continuous map is always connected. Right now I just know that $\{z \in \mathbb{C} : |f(z)| > 1\}$ is open since it is exactly the set $|f|^{-1}((1,\infty)).$","['complex-analysis', 'general-topology', 'holomorphic-functions']"
2789121,Any order isomorphism $F$ between $\mathcal{P}(A)$ and $\mathcal{P}(B)$ W.R.T. $\subseteq$ must be $F:X\mapsto\{f(x)\mid x\in X\}$ for some $f:A\to B$,"I'd like to prove that if there exists an order isomorphism $F:\left\langle \mathcal{P}\left(A\right),\subseteq\right\rangle \to\left\langle \mathcal{P}\left(B\right),\subseteq\right\rangle $ for some nonempty $A$ and $B$ then there must be some $f:A\to B\ $   S.T. $F$ is defined by $F:X\mapsto\left\{ f\left(x\right)\mid x\in X\right\} $ My thought was to explicitly build $f$ out of $F$ by defining $f\left(x\right)=F\left(\left\{ x\right\} \right)$, which would work exactly as requested. However, I'm having trouble proving that $F$ must take singletons to singletons. I suspect that in general, $F$ must preserve cardinality, but I was not successful in proving this.","['order-theory', 'elementary-set-theory']"
2789143,"Why is $\operatorname{Var}[X]≤(b−a)^2/4$ if $X$ is a random variable with values between $[a,b]$",I don't know how to start here. I tried to proof it with $$\operatorname{Var}(X) =  E(X^2) - [E(X)]^2$$ but this wasn't a good idea in the end.,"['variance', 'probability']"
2789200,"Show that $f$ is differentiable and determine $f'(t)$, for all $t \in I$. (Differentiable path)","Let $X: I \to \mathbb{R}^{n^{2}}$ a differentiable path whose values ​​are matrices $n \times n$. Fixed $k \in \mathbb{N}$, let $f: I \to \mathbb{R}^{n^{2}}$ the path given by the rule $f(t) = X(t)^{k}$. Show that $f$ is differentiable and determine $f'(t)$, for all $t \in I$. Since $X$ is differentiable, we have
$$X(a+t) - X(a) = X'(a) + r(t)$$
where $\lim_{t \to 0} \frac{r(t)}{t} = 0$, with $a \in I$. My idea is find $v$ such that
$$r(t) = f(a+t) - f(a) - v$$
implies $\lim_{t \to 0} \frac{r(t)}{t} = 0$ for show that
$$\lim_{t \to 0} \frac{X(a+t)^{k} - X(a)^{k}-v}{t} = 0.$$
I thought $v$ could be something that would allow you to write $X(a+t)^{k} - X(a)^{k} - v = (X(a+t) - X(a))^{k}$ or $v = kX(a+t)^{k-1}\cdot X'(t)$, but I couldn't conclude anything. Thanks for the any hint!","['matrices', 'real-analysis', 'derivatives']"
2789233,A longer series is better for a better team: Can you see this at a glance?,"Here is problem 6 from chapter 2 of Introduction to Probability by Bertsekas and Tsitsiklis: The Celtics and the Lakers are set to play a playoff series of $n$
  basketball games, where $n$ is odd. The Celtics have a probability $p$
  of winning any one game, independent of other games. For any positive
  integer $k$, find the values for $p$ for which $n = 2k + 1$ is better
  for the Celtics than $n = 2k-1$. When I read this problem statement, I quickly felt certain based on intuition that a longer series is better when $p > 1/2$. Question: Is there a short proof that allows us to see this result at a glance ? Here is a solution to the problem which seems overly complicated, given how obvious the result is intuitively. The calculation below is surely not what my brain did in order to be certain that the answer must be $p > 1/2$. Imagine that the two teams play $2k + 1$ games, and let the random
  variable $N$ be the number of games won by the Celtics during the
  first $2k -1$ games. The probability $p_{2k+1}$ of the Celtics winning
  the ""best of $2k+1$"" series (which requires winning at least $k + 1$
  games in the series) is $$ \tag{1}p_{2k+1} = P(N \geq k+1) + P(N = k)(1 - (1-p)^2) + P(N = k-1)p^2. $$ On the other hand, the probability
  $p_{2k-1}$ of the Celtics winning a ""best of $2k - 1$"" series is  $$ \tag{2} p_{2k-1} = P(N \geq k + 1) + P(N=k). $$ Notice that $P(N=k) = \binom{2k-1}{k}p^k(1-p)^{k-1}$ and $$ P(N = k-1) = \binom{2k-1}{k-1}p^{k-1}(1-p)^k = \binom{2k-1}{k}p^{k-1}(1-p)^k. $$
  Comparing $(1)$ and $(2)$, we see that \begin{align} p_{2k+1} > p_{2k-1}  &\iff P(N=k-1)p^2 > P(N=k)(1-p)^2 \\ &\iff p^{k+1}(1-p)^k > p^k(1-p)^{k+1} \\ &\iff p > \frac12. \end{align} If there is no simpler proof, then why are we so certain at the outset of what the answer must be?","['probability', 'alternative-proof']"
2789290,Interpreting the Jordan Normal Form,"What's the best way to interpret Jordan Normal Form (e.g. in terms of a linear map)? For instance, how should we interpret those $1$'s?","['matrices', 'jordan-normal-form', 'matrix-decomposition']"
2789301,Measure of sets on $\mathbb R^{n+m}$ isomorphic to $\mathbb R^n\times \mathbb R^m$,"Does the natural isomorphism between $\mathbb R^n \times \mathbb R^m$ and $\mathbb R^{n+m}$ preserve the measure of sets ? Let $\lambda^p$ be the Lebesgue measure on $\mathbb R^{p}$, does for $\lambda^{n+m}$-measurable set $A\subseteq \mathbb R^{n+m}$ we have
$$ \lambda^{n+m}(A) = \lambda^n\otimes\lambda^m(\iota(A)) $$
where $\iota : \mathbb R^{n+m} \to \mathbb R^n\times \mathbb R^m$ denotes the natural isomorphism ? Thanks for answers.","['lebesgue-measure', 'measure-theory']"
2789309,A trigonometric integral (guessed from a combinatorics formula),"In class, I defined the binomial coefficient using an integral: $$\binom{n}{k} = \displaystyle \int_0^{2\pi}\dfrac{dt}{2\pi} e^{-ikt}(1+e^{it})^n.$$ I succeeded in demonstrating many standard properties of the binomial coefficient directly using integration: Pascal's identity, Vandermonde identity, Hockey stick. But I could not show that $$\sum_{k=0}^n \binom{n}{k}=2^n.$$ It turns out I have to show the following: $$\int_{0}^{2\pi}\dfrac{dt}{2\pi}\dfrac{\sin\left(\dfrac{(n+1)t}{2}\right)}{\sin\dfrac{t}2}\cos^n\dfrac{t}{2}=1$$ I do not know how to perform this integration! I need help. It is better if the solution did not involve contour integration.",['integration']
2789325,"For which n differential operator $C^{(n)}[0, 1] \rightarrow C[0,1]$ is compact","I'm trying to learn functional analysis mostly on my own, and I'm stuck with the following task on operator compactness. Consider the operator $Tx(t)=\frac{dx}{dt}$, given $T$ maps $C^{(n)}[0,1]$ to $C[0,1]$. The norm in $C^{(n)}[0,1]$ is given by (standard) $||f||=sup|f^{(n)}| + \dots + sup|f^{'}| + sup|f|$. The task is to find out for which $n$ the operator is compact and bounded. I have successfully managed to prove that under the given norm, $T$ is always bounded. Now, I'm trying to analyse its compactness. It may be possible to make use of Arzela-Ascoli theorem (Let $X$ be a compact metric space and $\{f_{n}\}$ a uniformly bounded equicontinuous sequence of real-valued functions on $X$. Then $\{f_{n}\}$ has a subsequence that converges uniformly on $X$ to a continuous function $f$ on $X$), but I'm unable to move forward with this.","['compact-operators', 'operator-theory', 'functional-analysis', 'compactness', 'analysis']"
2789342,Derivatives of function defined by cases,"I have the following problem that I'm not sure how to tackle. For function $f$, find values for $a$ and $b$ such that $f'(3)$ exists. The function is defined as: $$ 
f(x)=
\begin{cases}
\hfill x^2-4x+8 & \text{if $x \leq 3$} \\
ax+b & \text{if $x \gt 3$} \\
\end{cases}
$$ So I'm a bit at a loss here. Both cases of the function are polynomials, so I would assume that they are always derivable, and therefore $f'(3)$ would exist, but I'm not sure how to tackle it. Should I derive both equations and then use the results to create a lineal system of equations? How would you recommend me to proceed? Also, I'm thinking that maybe $f(x)$ is not continuous in $f(3)$ because the limit approaching by the right might be different from the limit approaching from the left. Does it mean it is not derivable and thus $f'(3)$ actually does not exist?","['derivatives', 'calculus']"
2789393,Series of Reciprocal of Iterated Functions,"Have infinite series of the form
$$I(f;z)=\sum_{n=1}^{\infty}\frac{1}{f^n(z)}$$
where
$$f^n=\underbrace{f\circ f\circ\dots\circ f}_{n\text{ times}}$$
been studied? One interesting example of this that spurred my interest in these series is Sylvester's sequence . One way to define Sylvester's sequence is $s_0=2$ and $s_n=s_{n-1}^2-s_{n-1}+1$, and it has the known property that
$$\sum_{n=0}^{\infty}\frac{1}{s_n}=1$$
This corresponds to the fact that $I(z^2-z+1; \varphi)=1$ where $\varphi$ is the golden ratio. In fact, we have the more general fact that
$$I(z^2-z+1; z)=\frac{1}{z(z-1)}$$
In studying these series I have classified all rational $f$ such that $I(f;z)$ is rational in $z$, one such example being the one above. Does anyone know any way this could be used? Or know any literature that makes use of these series? One thing I have considered with this series is that when $f$ is rational, $I(f;z)$ converges on the escaping set minus points whose orbit contains zero. Thus for those such rational $f$ for which $I(f;z)$ has a known closed form the Julia set could maybe be described by looking at where this series (or its partial sums) strays ""too far"" from the value it ought to converge to.","['reference-request', 'recreational-mathematics', 'sequences-and-series']"
2789402,Holomorphic function with analytic square root but no holomorphic logarithm,Give an example of a domain $U$ and a holomorphic function $f:U \to \Bbb C$ which has a holomorphic square-root but for which there does not exist a holomorphic function $F$ such that $f = \exp F$. I am struggling to come up with an appropriate example. Could I have a hint?,"['logarithms', 'complex-analysis', 'examples-counterexamples', 'exponential-function']"
2789412,UFD containg a special element,"Does anyone know an example of a unique factorization domain $R$ that is (i) not a Dedekind domain (or equivalently, not a principal ideal domain) and (ii) contains some irreducible element $r \in R$ such that the quotient $R/rR$ is finite? I am grateful for any suggestions.",['abstract-algebra']
2789421,A series in a Hilbert space that converges unconditionally but only in weak topology,"Let $H$ be a Hilbert space. I'm looking for an example of a series $\sum x_n$ with the following two properties: $\sum x_n$ does not converge, i.e., partial sums do not have a limit in the norm topology of $H$; $\sum x_n$ converges unconditionally in the weak topology, that is for every bijection $\sigma:\mathbb{N}\to\mathbb{N}$ the partial sums of the rearranged series $\sum x_{\sigma(n)}$ converge weakly. Progress so far : considered some weakly but not strongly convergent sequences, and corresponding series. In all cases the weak convergence turned out to be conditional. For example: let $e_n$ be an orthonormal basis, and consider $x_1=e_1$, $x_n = e_n - e_{n-1}$ for $n>1$. Then the partial sums of $\sum x_n$ are precisely the vectors $e_n$ which converge to zero weakly but not in the norm. However, this weak convergence is conditional: if we take only even-numbered elements
$$x_2+x_4+x_6+\dots = (e_2-e_1) + (e_4-e_3) + (e_6-e_4)+\dots$$
the norm of partial sums tends to $\infty$; and including an odd-numbered $x_n$ once in a million terms will not change that. Since a weakly convergent sequence must be bounded, the rearranged series does not converge weakly.","['functional-analysis', 'weak-convergence', 'hilbert-spaces']"
2789482,How to solve $2c x + x\dot x + (c^2-1)t = cd$,"After computation on a certain problem, for a specific case I came across the differential equation $$2c x + x \dot x + (c^2-1) t = cd$$
with initial conditions $0 < x(0) = d $ and  $1 < - \dot x(0) = c $. Does this have a closed form? This differential equation doesn't appear to conform to solution by factors, integrating factors, Clairaut's equation, homogeneous methods, differentiating methods, Riccati's equation, or any other special form or method I know of.",['ordinary-differential-equations']
2789484,What makes radians superior to turns/revolutions?,"1. THE CONTEXT OF THE PROBLEM This question came to me when I was exploring complex exponents. The key identity to computing expressions with complex exponents is the Euler's identity: $$e^{i\theta}=\cos\theta+i\sin\theta$$ This enables us to compute, for example what $2^i$ is by some algebraic manipulation. The computation is as follows: $$2^i=e^{\ln2^i}=e^{i\ln2}=\cos\ln2+i\sin\ln2\;\approx\;0.769+0.639i$$ 2. THE PROBLEM A question arises in my head. We compute sines and cosines with radian values. And since $\ln2\approx0.693$ then $\sin\ln 2$ will be the $y$ coordinate of the unit circle when the angle is $0.693$ radians . But if we use turns instead of radians, then $\sin\ln 2$ will be the $y$ coordinate of the unit circle when the angle is $0.693$ turns . So the value of sine when using turns or radians is different. Similarly, the value of cosine is different. But that creates a problem. When using radians , $2^i$ computes to about $0.769+0.639i$. But when using turns , $2^i$ computes to about $0.994+0.110i$. 3. POSSIBLE IMPLICATIONS The problem above illustrates that $2^i$ and any expression with complex exponent is a generalisation that directly depends on what units we use for angles. There are only two possibilities of ""the state of truth"" that this fact implies. Either complex exponents are a concept completely made up by humans which would mean that expressions like these are really undefined to the code of the universe (we only make it defined because we think that radians are the true angle units), or there must be a unit that is the truest unit for measuring angles, whether that would be radians or something else. If the first statement is true, then this would mean that we can accept $2^i$ to be $0.769+0.639i$. After all, this concept would be defined by radians only because we defined it like this. However, if the second statement is true, then there are even more questions to be asked. If there is a ""truest"" unit for measuring angles then what is it? Perhaps radians are really the truest unit, meaning that $2^i$ is unambiguously $0.769+0.639i$, but if so, what justifies this fact? What makes radians truer than turns?","['angle', 'trigonometry', 'complex-numbers', 'foundations']"
2789497,"Evaluate $\int_0^{\pi/2}\ln^2(\sin\theta)\,\mathrm d\theta$","How can I evaluate $$\int_0^{\pi/2}\ln^2(\sin\theta)\,\mathrm d\theta$$ With Wolfram Alpha ( see ) I get to the next result 
$$ \int_0^{\pi/2}\ln^2(\sin\theta)\,\mathrm d\theta = \frac{1}{24} \left(\pi^3 + 3\pi \ln^2(4)\right)$$
How to prove these ?","['improper-integrals', 'integration', 'definite-integrals', 'calculus']"
2789503,How can I show what the induced metric is?,"I'm unclear on how to solve any of these problems, can someone please help me with what to do? a) Let S be the upper portion of a cone in $\mathbb{R}^3$ parametrized by $\phi(u^1,u^2) = (u^1\cos u^2, u^1\sin u^2)$; $u^1 >0$. Show that the induced metric on S from $\phi$ is $$\begin{pmatrix} 2 & 0 \\\ 0 & (u^1)^2\end{pmatrix}$$ $\phi u_1 = <\cos u^2, \sin u^2, 1>$ and $\phi u_2 = <-u^1\sin u^2, u^1\cos u^2, 0>$ Now do all I have to do is dot product these together? So, $g_{11} = \phi u_1 \cdot \phi u_1 = \cos^2 u^2 + \sin^2 u^2 +1$ $g_{12} = g_{21} = \phi u_1 \cdot \phi u_2 = -u^1\sin u^2 \cos u^2+u^1\cos u^2 \sin u^2$ $g_{22} = \phi u_2 \cdot \phi u_2 = (u^1)^2\sin^2 u^2 +(u^1)^2 \cos u^2$ b) Let $X = (u^1)\partial_1 +\partial_2$, be a vector field on S. Find the length of the vector field X with respect to the metric g. c) Suppose $f: S \to \mathbb{R}$ by $f(u^1\cos u^2, u^1\sin u^2, u^1) = (u^1)^2 -(u^1)^2-(u^1)(u^2)$Find X(f), where $X = (u^1)\partial_1 +\partial_2$",['differential-geometry']
2789511,"Bloch's theorem in one dimension, confusion about proof","I was looking at the derivation of Bloch's theorem in Griffith's QM: If $V(x+a)=V(x)$ for any $x$ and some $a$, and
$\psi$ solves
$$
H\psi =\lambda \psi
$$
for $H=-\frac{\hbar^2}{2\pi}\frac{d^2}{dx^2}+V(x)$
then
$$
\psi(x+a)=e^{ika}\psi(x)
$$ The proof relies on the fact that since the shift operator $D\psi(x)=\psi(x+a)$ commutes with the Hamiltonian, we may take an eigenfunction of $H$ to serve as an eigenfunction for $D$. The wikipedia article on the theorem (on a 3 dimensional crystal lattice but the principle is the same) once again asserts the existence of a simultaneous eigenbasis for the two operators. What am I missing here? I know that this is a fact for matrices, but we are working over $L^2(\mathbb{R})$ here, where I don't believe this is true for unbounded self adjoint operators, and it is not clear to me why it should be true with this particular operator (indeed I think it is not). Thanks for any help and I apologize if I am being naive.","['functional-analysis', 'quantum-mechanics', 'mathematical-physics', 'unbounded-operators']"
2789523,Solving second order ODE:Numerical and Analytical-Help,"$$\frac{d^2x}{dt^2} - a^2x + \frac{k}{x^2} = 0$$
Can anyone help me with solving this ODE ? What type of ODE is this? Is there a general solution for this? If not, please give me some help on solving this numerically. Thank you","['numerical-methods', 'ordinary-differential-equations']"
2789564,"If three sides of an acute triangle is $3$ cm, $4$ cm, and $x$ cm. What are the possible values of $x$?","Problem: If three sides of an acute triangle is 3 cm, 4 cm, and $x$ cm. What are the possible values of $x$? 2 of the choices: (A) $1<x<5$, and (B) $0<x<7$ Solution: According to triangle theorems, 2 sides of a triangle is greater than the 3rd side and their difference is less than the 3rd side. From one of the choices, (ranges $0<x<7$) when $x = 6$: $3 + 4 > 6$ $6 + 4 > 3$ $3 + 6 > 4$ From the above, it means $6$ is a possible value for $x$, and since choice A doesn't have $6$ in it, I chose B as the correct answer. However that isn't the case, the correct answer according to the book is A, which I don't know why. So how was it $1<x<5$? Any help would be appreciated.","['trigonometry', 'geometry']"
2789572,How many possible rankings in a game?,"This is an interesting problem that I have no easy solution for, but an inkling that there may be one! Consider an $n$ player game. If we disallow draws, we know that the number of permutations of $n$ players in that $n$ player game will be $n!$. That's the easy problem. But now let us permit and consider the possibility of draws. What then? I model this tentatively in my own mind as a roll of $n$ $n$-sided dice. For for a four player game 4 dice with 4 sides each (d4 in gamers' parlance). Now that would appear to present $n^n$ permutations. And that seems a good starting point, albeit overestimating the number of rankings considerably because ... ... from a ranking perspective a load of these are invalid or identical depending on how we choose to classify it. But let's consider that to limit the results to valid rankings: A 1 must appear All numbers must be contiguous (adjacent) All numbers must be ascending (order is relevant) We might address the first issue by pinning one outcome to 1, and so only rolling 3 4-sided dice for the remaining 3 players (or $n-1$ $n$-sided dice for the remaining $n-1$ players). Which presents 1/4 as may permutations or a total of $n^{n-1}$ permutations generally. Now from the results we want to exclude all those that have gaps, are not contiguous. On that I am stuck. How to enumerate those if possible. To clarify, this is an illegal ranking we wish not to count (4 player game): 1334 only because it is double counting: 1223 And we also want to enforce ascending order so that these are excluded: 3314 4331 again, only because they double count: 1223 (which is a winner, a tie for second place and someone in 3rd place). which is the same ranking. I figure the best way to avoid this kind of double counting is to demand this ascending contiguity of ranks. How many illegal results then are there in the sample space of $n^{n-1}$ rolls of the $n$-sided die? Or is there a better way to model it?","['combinatorics', 'combinatorial-game-theory']"
