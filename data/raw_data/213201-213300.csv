question_id,title,body,tags
4308484,"What is known about the 'Double log Eulers constant', $\lim_{n \to \infty}{\sum_{k=2}^n\frac{1}{k\ln{k}}-\ln\ln{n}}$?","The Euler constant is defined as $$\gamma = \lim_{n \to \infty}{\sum_{k=1}^n\frac{1}{k}-\ln{n}}$$
Let $$q = \lim_{n \to \infty}{\sum_{k=2}^n\frac{1}{k\ln{k}}-\ln\ln{n}}$$
I managed to prove that $$\frac{1}{3\ln{3}}+\frac{1}{2\ln{2}}-\ln\ln{3} \geq q \geq \frac{1}{2\ln{2}}-\ln\ln{3}$$
Is there something known about the constant $q$? For instance, is $q$ expressible in terms of $\gamma$?","['asymptotics', 'analytic-number-theory', 'euler-mascheroni-constant', 'limits', 'constants']"
4308524,The tensor product of the canonical line bundle and k(x) for a closed point x,"I am reading the book “Fourier-Mukai transforms in algebraic geometry” by Daniel Huybrechts. At the beginning of the page 91, it is written that if $X$ is a smooth projective variety with a canonical bundle $\omega_X$ , then for a closed point $x\in X$ , we have $k(x)\cong k(x)\otimes \omega_X$ . My question is that why is this true?","['algebraic-geometry', 'line-bundles', 'projective-varieties']"
4308576,Rigid nonagons and rational solutions of a hyperelliptic equation,"Here is a rational bracing of the regular nonagon of side $1$ with $12$ extra rods (extensions of the nonagon sides don't count), fulfilling a long-held dream of mine: I found it in a similar manner to the rationally braced pentagon by considering the following framework where $a,b,c$ and the blue edge length are all rational – rods are interpreted as vectors pointing from bottom to top: $a,b,c>0$ as depicted, while the bottom-most point can be fixed relative to the horizontal rod by building an equilateral triangle on it, so there's no problem in making the first vector length $a\sqrt3$ . Then the blue rod as a complex number is $$a(z^3-z^6)+b+z+z^2+z^3+cz^4$$ where $z=e^{2i\pi/9}$ , and its squared length is in $\mathbb Q(z)$ : $$3a^2+b^2+c^2+3a-b-c+3+(1+2ac)z-(1+2ac)z^2+(-1-3a-b-c+ac+bc)z^4+(-2-3a-b-c-ac+bc)z^5$$ Since $\{1,z,z^2,z^3,z^4,z^5\}$ is a $\mathbb Q$ -basis for $\mathbb Q(z)$ yet the expression has to be a rational square, the coefficients of $z$ to $z^5$ must be zero, leading to $b$ and $c$ expressed in terms of $a$ : $$b=\frac{1-3a-6a^2}{1+2a}\qquad c=-\frac1{2a}$$ Substituting gives the blue rod length as $$\frac{\sqrt{1+6a+24a^2+48a^3+144a^4+288a^5+192a^6}}{2a(1+2a)}$$ so all rational bracings under this framework correspond to rational points on the hyperelliptic curve $y^2=1+6a+24a^2+48a^3+144a^4+288a^5+192a^6$ . Using PARI/GP I have found four pairs of rational points on the curve with $a=-\frac12,0,\frac14,\frac32$ . The first two values cause a divide-by-zero upon substituting, the third gives the bracing shown at top and the last gives a vertex-edge-degenerate and very large but still rigid bracing. The question here, since this is a hyperelliptic curve, is natural: Does $a\in\left\{-\frac12,0,\frac14,\frac32\right\}$ describe the complete set of rational points on $y^2=1+6a+24a^2+48a^3+144a^4+288a^5+192a^6$ ? I suspect the problem can be resolved with current methods – although the rank of the Mordell–Weil group of the Jacobian is $2$ , the sextic in $a$ is also a defining polynomial for $\mathbb Q(z)$ , which means it has Galois group $C_6$ and (I think) ""elliptic curve Chabauty"" as mentioned in this MathOverflow answer (and explicated for $y^2=x^6+x^2+1$ here , though I can't follow the whole thing) can be applied. It may help that the substitution $a=\frac1{2(t-1)}$ produces a ""minimal"" birationally equivalent curve $$y^2=t^6-3t^5+6t^4-8t^3+12t^2-6t+1$$","['number-theory', 'recreational-mathematics', 'geometry', 'diophantine-equations']"
4308584,Group of all permutations of $\mathbb N$ that don't change the limit of series,"Clearly all bijections functions $\varphi : \mathbb N \to \mathbb N$ form a group, let's call it $S(\mathbb N)$ . Now let's define $H \subset S(\mathbb N)$ to be the set of all elements $\varphi \in S(\mathbb N)$ such that for every real valued sequence $(a_n)_{n \in \mathbb N}$ for which $$\sum_{n=1}^\infty a_n$$ converges we have $$\sum_{n=1}^\infty a_n = \sum_{n=1}^\infty a_{\varphi(n)}.$$ Clearly $H$ forms a proper subgroup of $S(\mathbb N)$ . For fixed $n \in \mathbb N$ the group $S_n$ is a subgroup of $H$ . But this is not everything. For example the map $$
\varphi(n) := n+(-1)^{n+1}
$$ is also in $H$ without being a finite permuation. Question 1 : Does the group $H$ have a special name? Is it mentioned somewhere in literature? Question 2 : Is $H$ simply the group of all ""infinite products of disjoint cycles""?","['permutations', 'absolute-convergence', 'sequences-and-series', 'group-theory', 'convergence-divergence']"
4308625,My solution to compute type II error,"Suppose there is a coin, we flipped it for 100 times and want to check if it is fair. Let $X_{1}, X_{2}, ..., X_{100}$ be the outcome of each experiment, which is a series of random variables following i.i.d. Bernoulli distributions with parameter p. For $i^{th}$ experiment, if we get a head, then $X_{i} = 1$ , otherwise $X_{i} = 0$ . Our hypothesis: $H_{0}: p = \frac{1}{2}$ against $H_{1}: p \neq \frac{1}{2}$ Let $X = \sum\limits^{100}_{i=1}X_{i}$ , if $40\leq X \leq 60$ then don't reject the null hypothesis $H_{0}$ , otherwise reject the null hypothesis. Obviously, $X \sim Bin(100, p)$ . $\mathbb{P}(40\leq X \leq 60) = \sum\limits^{60}_{x=40} \binom{100}{x}p^{x}(1-p)^{100-x}$ . Type II error is the probability that we can't reject the null hypothesis given that the alternative hypothesis is true. Suppose P is an uniform random variable defined on $(0,\frac{1}{2})\cup(\frac{1}{2}, 1)$ , we have: $\mathbb{P}(40\leq X \leq 60)\\
= \int^{1}_{0}\mathbb{P}(40\leq X \leq 60 | p \in (0,\frac{1}{2})\cup(\frac{1}{2}, 1))f_{P}(p)\, dp\\
= \int^{1}_{0}\mathbb{P}(40\leq X \leq 60 | p \in (0,\frac{1}{2})\cup(\frac{1}{2}, 1))\, dp\\
= \sum\limits^{60}_{x=40} \int^{1}_{0}\binom{100}{x}p^{x}(1-p)^{100-x}\, dp\\
= \sum\limits^{60}_{x=40}\int^{1}_{0}\frac{100!}{x!(100-x)!}p^{x}(1-p)^{100-x}\, dp\\
= \sum\limits^{60}_{x=40}\frac{100!}{101!}\int^{1}_{0}\frac{\Gamma(102)}{\Gamma(x+1)\Gamma(101-x)}p^{x}(1-p)^{100-x}\, dp\\
= \sum\limits^{60}_{x=40}\frac{100!}{101!} = \frac{21}{101}$ What's wrong with my solution?","['statistics', 'probability-distributions', 'hypothesis-testing', 'probability-theory', 'probability']"
4308647,Expectation of inverse of sum of iid random variables,"Description: There is an indicator function called $I_{i}^{k}$ as follows: \begin{align*}
 \begin{split}
 I_{i}^{k}= \left\{
 \begin{array}{lr}
 1, \; {\rm if \; the  \; event \;  happened\; at\; time \;k\; for\; player\; i;}\\
 0, \; {\rm if  \; not.}
 \end{array}
 \right.
 \end{split}		
 \end{align*} The event happened at least once, so $\mathbb{E}[I_{i}^{k}]=p_{i}>0$ .  Besides,  the indicators $I_{i}^{k}, \left\lbrace i=1,\dots N \right\rbrace $ are independent and
identically distributed (i.i.d.) with a fixed distribution across
time steps. Now, we Let $\Gamma_{i}^{k}=\sum_{t=1}^{k}I_{i}^{t}$ and $\gamma_{i,k}={1}/{(\Gamma_{i}^{k})^{a}}$ , $a\in (1/2, 1]$ for all $i$ and $k\geq 1$ . Can we give the upper bound of the following? \begin{align*}
(a) \sum_{k=1}^{K}\mathbb{E}\left[ \gamma_{i,k}^{2}\right] \leq ?   \quad\quad  
(b) \sum_{k=1}^{K}\mathbb{E}\left[ \left| 
\gamma_{i,k}-\frac{1}{k^{a}p_{i}^{a}} \right| \right] 
\leq ?
\end{align*} where $K$ is some constant. PS: I think maybe it can be proved by Taylor expansion , but I don't know how to calculate it, maybe the following link is useful: https://stats.stackexchange.com/questions/399261/expectation-of-inverse-of-sum-of-positive-iid-variables/399289#399289","['statistics', 'expected-value', 'distribution-tails', 'inverse', 'probability']"
4308692,Sum of variables in graph optimum,"Given a connected (undirected) graph $G$ with vertex set $V$ of size at least $2$ , we are allowed to put a real number $x_v$ on each $v\in V$ . The constraint is that, for any $W\subseteq V$ such that the induced subgraphs on both $W$ and $V\setminus W$ are connected, $\displaystyle\left|\sum_{v\in W}x_v\right|\le 1$ . We want to maximize $\displaystyle\sum_{v\in V} |x_v|$ . Is it true that for any maximizing solution, the sum of the $x_v$ 's is $0$ ? It is true for several graphs as seen here , as well as all graphs of size up to $4$ . One can observe that if $\{x_v\}_{v\in V}$ is a maximizing solution, then so is $\{-x_v\}_{v\in V}$ . But that does not imply that the sum of $x_v$ 's must be $0$ .","['graph-theory', 'optimization', 'inequality', 'combinatorics']"
4308716,"Cahill, Physical Mathematics, Exercise 7.14","Problem: Integrate the ODE $(xy+1)dx+2x^{2}(2xy-1)dy=0$ . Hint given in the book (Kevin Cahill, Physical Mathematics Exercise 7.14, page 332, Second edition): Use the variable $v(x)=xy(x)$ instead of $y(x)$ . Now I tried solving this equation using the hint and separating the variables since: \begin{equation}
\frac{dv}{dx}=f(x)g(v)
\end{equation} where $f(x)=\frac{1}{x}$ and $g(v)=\frac{-(v+1)+2v(2v-1)}{2(2v-1)}$ . Separating and integrating gives me: $x=(v-1)^{2/5}+(4v+1)^{6}+c$ . But this is not helpful since I need to find $v=v(x)$ and from that $y=y(x)$ and inverting the above relation seems difficult. Have I done anything wrong here? Is there any other method to solve this problem? Thanks in advance.",['ordinary-differential-equations']
4308721,Spherical harmonics - Computing the variance of Poisson noise integrated over $\ell$ on a defined quantity?,"It is an astrophysics context but actually, it is mostly a mathematics issue. From spherical harmonics with Legendre deccomposition, I have the following definition of
the standard deviation of a $C_\ell$ noised with a Poisson Noise $N_p$ : \begin{equation}
\sigma({C_\ell})(\ell)=\sqrt{\frac{2}{(2 \ell+1) f_{sky}}}\left[C_\ell(\ell)+\dfrac{1}{N_{p}}\right]\label{1}\tag{1}
\end{equation} Now I consider the quantity : $$\sum_{\ell=1}^{N} \sum_{m=-\ell}^{\ell} a_{\ell m}^{2}$$ I want to estimate the variance expression of Poisson Noise of this qantity. For that, I take the definition of $a_{lm}$ following a normal distribution with mean equal to zero and take also the definition of a $C_\ell=\langle a_{lm}^2 \rangle=\dfrac{1}{2\ell+1}\sum_{m=-\ell}^{\ell}\,a_{\ell m}^2 = \text{Var}(a_{lm})$ . I use $\stackrel{d}{=}$ to denote equality in distribution : $$
\begin{split}
Z & \equiv \sum_{\ell=\ell_{\min }}^{\ell_{\max }} \sum_{m=-\ell}^{\ell} a_{\ell, m}^{2} \\
& =\sum_{\ell=\ell_{\min }}^{\ell_{\max }} \sum_{m=-\ell}^{\ell} C_{\ell} \cdot\left(\frac{a_{\ell, m}}{\sqrt{C_{\ell}}}\right)^{2} \\
& \sim \sum_{\ell=\ell_{\min }}^{\ell_{\max }} \sum_{m=-\ell}^{\ell} C_{\ell} \cdot \mathrm{ChiSq}(1) \\
&=\sum_{\ell=\ell_{\min }}^{\ell_{\max }} C_{\ell} \cdot \mathrm{ChiSq}(2 \ell+1).
\end{split}
$$ So Finally we have : $$
\sum_{\ell=1}^{N} \sum_{m=-\ell}^{\ell} a_{\ell m}^{2}  \stackrel{d}{=} \sum_{\ell=1}^{N}\,C_\ell \chi^{2}(2\ell+1).
$$ To compute the Poisson Noise of each $a_{\ell m}^2$ , a colleague suggests me that for getting an optimal variance expression, I must use the Inverse-variance_weighting , to take only the quantity below: $$
\dfrac{2}{f_{sky}\,N_p^2},\label{2}\tag{2}
$$ and to do the summation to get the variance of Poisson Noise (to make the link with formula \eqref{1} and be consistent with it) : $$
\text{Var}(N_{p,int})=\sum_{\ell=1}^{N} \dfrac{2}{f_{sky}\,N_p^2}. \label{3}\tag{3}
$$ But I have difficulties to understand the Inverse-variance_weighting applied in my case : I don't understand where are implied the weights regarding the formula \eqref{3}. I wrote above $\text{Var}(N_{p,int})$ , make caution, this is just to express the ""integrated Poisson variance"" over $\ell$ in the quantity $$
\sum_{\ell=1}^{N} \sum_{m=-\ell}^{\ell} a_{\ell m}^{2}.
$$ Unfortunately, when I do numerical computation with this variance formula \eqref{3}, I don't get the same variance than with another valid method. Correct results are got by introducing a factor $\sqrt{2\ell+1}$ into formula \eqref{3}. But I don't know how to justify it, it is just fine-tuned from my part for the moment and it is not rigorous I admit. QUESTION : Is formula \eqref{2} that expresses the variance of Shot Noise on the quantity $\sum_{\ell=\ell_{min}}^{\ell_{max}} \sum_{m=-\ell}^\ell a_{\ell,m}^2$ correct ? If yes, how could integrate it correctly by summing it on multipole $\ell$ and to remain consistent with the standard deviation formula \eqref{1} ? (I talk about the pre-factor $\sqrt{\frac{2}{(2 \ell+1) f_{sky}}}$ that disturbs me in the expression of integrated Poisson Noise) Finally, from all given informations above, how to correctly compute analytically the variance of $$\sum_{\ell=1}^{N} \sum_{m=-\ell}^{\ell} a_{\ell m}^{2}\:?$$","['statistics', 'variance', 'poisson-distribution', 'noise', 'spherical-harmonics']"
4308729,How to deal with products of “shifted” Kronecker deltas?,"How does one deal with expressions like $$
\sum_\mu s_\mu \delta_{2 \mu-1,j} \delta_{2\mu,k} \quad \text{ or } \quad \sum_\mu a_\mu\delta_{2\mu,j} \delta_{2\mu -1,k} \quad ?
$$ The usual rules would tell me set $\mu=\frac{j+1}{2}$ so that the first expression would be equal to $$
\sum_\mu s_\mu \delta_{2 \mu-1,j} \delta_{2\mu,k} \stackrel{?}{=} s_{\frac{j+1}{2}}\delta_{j+1,k}
$$ but it should be $$
\sum_\mu s_\mu \delta_{2 \mu-1,j} \delta_{2\mu,k} = s_{(j+1)/2}\delta_{j+1,k}\delta_{j\in \mathbb{O}\
}
$$ where $\mathbb{O}$ here denotes the set of (positive) odd numbers. So, what are the rules to tackle these problems fast?","['differential-geometry', 'tensors', 'notation', 'linear-algebra', 'index-notation']"
4308730,Sum of two independent random variables: distribution function and quantile function,"If $X,Y$ are two independent random variables with CDFs $F_X,F_Y$ , their sum has CDF $F_X \star F_Y$ ( $\star$ is the convolution product). What can be said about the quantile function of $X+Y$ ? The quantile function should be the inverse of $F_X \star F_Y$ . Can we express that using the CDFs, quantiles (or even densities) of $X$ and $Y$ ?","['statistics', 'quantile-function', 'probability-distributions', 'probability-theory', 'probability']"
4308736,"If $x;y$ $\in T$($x$ and $y$ can be the same), then $x^2-y \in T $ Prove that : $T = \mathbb Z $","Consider the set of integers $T$ with the following properties: There exist two integers $a, b \in T $ satisfying : $\gcd(a,b)=\gcd(a-2,b-2)=1$ If $x;$$y$ $\in T$ ( $x$ and y can be the same), then $x^2-y \in T $ Prove that: $T = \mathbb Z$ I find that if $1 \in T  \Rightarrow 1^2-1 = 0 \in T \Rightarrow 0^2-1 = -1 \in T $ $ \Rightarrow 1^2+1 = 2 \in T \Rightarrow 0^2-2 =-2  \Rightarrow 1^2+2 = 3 \in T $ With the same algorithm, we should be able to prove $T = \mathbb Z $ . So we only need to prove that for $2$ given numbers $a, b$ satisfying: $\gcd(a,b)=\gcd(a-2,b-2)=1$ , there exists an algorithm for the occurrence of the number $1$ in set $T$ . $\gcd(a,b)=\gcd(a-2,b-2)=1$ suggest me to use Bezout theorem, but I have no idea at all, would love to get help from everyone. Thanks very much!","['number-theory', 'elementary-number-theory', 'gcd-and-lcm', 'combinatorics', 'algebra-precalculus']"
4308750,Proof verification: $\lim_{x\to 2} \frac{\sqrt{x^2 + 5} - 3}{x - 2} = \frac23$,"The question is as follows: Prove that $\displaystyle\lim_{x\to 2} \dfrac{\sqrt{x^2 + 5} - 3}{x - 2} = \dfrac23$ . My proof is: Fix $\varepsilon > 0$ . Note that $$\begin{array}{rcl}\left|\dfrac{\sqrt{x^2 + 5} - 3}{x - 2} - \dfrac23\right| &=& \left|\dfrac{x + 2}{\sqrt{x^2 + 5} + 3} - \dfrac23\right|\\&=& \dfrac13\left|\dfrac{3x - 2\sqrt{x^2 + 5}}{3 + \sqrt{x^2 + 5}}\right|\\&=& \dfrac13 \left|\dfrac{5x^2 - 20}{\left(3 + \sqrt{x^2 + 5}\right)\left(3x + 2\sqrt{x^2 + 5}\right)}\right|\\&=& \dfrac{5|x - 2|\cdot|x + 2|}{3\left|3 + \sqrt{x^2 + 5}\right|\cdot \left|3x + 2\sqrt{x^2 + 5}\right|}\end{array}$$ Pick $\delta = \min\left\{1, \dfrac{21\varepsilon}5\right\}$ . Suppose that $0 < |x - 2| < \delta < 1$ . Then $1 < x < 3$ . Therefore, $|x + 2| < 5$ , $\left|3 + \sqrt{x^2 + 5}\right| > 3 + \sqrt 6 > 5$ , and $\left|3x + 2 \sqrt{x^2 + 5}\right| > 3 + 2\sqrt 6 > 7$ . Therefore, $$\left|\dfrac{\sqrt{x^2 + 5} - 3}{x - 2} - \dfrac23\right| < \dfrac{5 \cdot 5}{3 \cdot 5 \cdot 7} |x - 2| < \dfrac5{21} \delta < \varepsilon$$ Hence, $\displaystyle\lim_{x\to 2} \dfrac{\sqrt{x^2 + 5} - 3}{x - 2} =\dfrac23$ . Is my proof correct?","['calculus', 'proof-writing', 'solution-verification', 'analysis']"
4308791,Inequalities for $\mathcal C^1$ functions satisfying $f^\prime(x) \leq af(x)+b$,"Let $f : \mathbb{R} \to \mathbb{R}$ be a class $C^1$ function which vanishes in $0$ and which satisfies: $$\forall x \geq 0,\quad  f'(x) \leq af(x)+b \quad [a > 0, b \geqslant 0] $$ We try to show that: $$\forall x \geq 0, \quad f(x) \leq {b \frac{e^{ax}-1}{a}}$$ and $$f'(x) \leq be^{ax}$$ Could you direct me to the demo please? I don't see how to start ...
Thank you","['inequality', 'derivatives', 'exponential-function']"
4308823,Martingale and card withdrawal,"I have deck of 52 shuffled cards from which I draw cards one by one and an indicator $I_k$ where $ I_k \in \{0, 1\}$ that shows the k-th card is a King or not. My filtration is $\mathcal{F}_n = \sigma(I_1, I_2,...,I_n)$ . I also have a probability estimate of the last card being a King $G_n = \mathbb{E}(I_{52}| \mathcal{F}_n)\;\;\forall n \in$ {0,1,...,51}. I wish to know how to express $G_n$ in terms of $I_1,I_2,...I_n$ . I have tried solving it myself as follows: Number of kings drawn after n draws would be $\sum^n_{k=1} I_k$ . The remaining number of kings would be $4 - \sum^n_{k=1} I_k$ . Since the remaining 52 - n cards have equal chance to be a king, then $\mathbb{E}(I_{52}| \mathcal{F}_n) = \frac{4 - \sum^n_{k=1} I_k}{52 - n}$ . Secondly I would also like to know how to show if $G_n$ is a martingale. Thank you in advance","['conditional-expectation', 'martingales', 'probability-theory', 'card-games']"
4308854,How to determine $\sup_{x \in \mathbb{R}} \left|\frac{\sin \left(x^{2}+1\right)}{x^{2}+1}\right|$,"It is clear (especially using a graphic calculator) that the sup of the function is the value of the function for $x=0$ , but I want to prove it analytically. First, because of Weierstrass, the sup of $f(x)=\frac{\sin (x^2+1)}{x^2+1}$ is equal to its max (the function is continuous everywhere on $\mathbb{R}$ and it's bounded since $f(x) \rightarrow 0$ for $x \rightarrow \pm \infty$ ), so I tried to do the derivative of the function to find the stationary points and determine the maximum. The problem is that the derivative is $$f'(x)=2x\dfrac{(x^2+1)\cos (x^2+1)-\sin (x^2+1)}{(x^2+1)^2}$$ So the stationary points are $x=0$ and the solutions of the equation $$\tan (x^2+1)=x^2+1 \quad (1).$$ Then, I thought  that, since we should find the sup of the absolute value of $f$ , we could define the sequence $\{|f(x_n)|\}_{n \in \mathbb{N}}$ where $\{x_n\}_{n \in \mathbb{N}}$ are the stationary points of the function such that $x_0=0$ and $x_n$ is a solution of the equation $(1)$ for every $n>0$ (in particular, we can take $\{x_n\} \subset [0,+\infty)$ whitout loss of generality because  the function is even and so we have that $f(x_n)=f(-x_n)$ for every $n \in \mathbb{N}$ ) and verify that is a decreasing sequence, so that we have that $f(x_0)=f(0)$ is the sup of $|f|$ . The problem with this approach seems to be that there are no analytical solution to $(1)$ . What do you suggest?","['absolute-value', 'real-analysis', 'optimization', 'trigonometry', 'supremum-and-infimum']"
4308909,"For each integer a>1, is there an integer power p such that the decimal expansion of $a^p$ contains the digit 0?","This isn't homework, but I'm trying to work some other math problems for fun in my spare time, and noticed that I had trouble solving this slightly adjacent question. I've noticed that the ways we can get a 0 in a decimal expansion is either a) with no carry-in through 2 5, 4 5, or 5*6, or b) with a carry-in through basically any other product of numbers, but I'm not sure how to proceed from here. Any thoughts appreciated. Thanks.","['number-theory', 'elementary-number-theory']"
4308921,"Does convergence in distribution of marginals, and same conditional laws, imply convergence in distribution of the joint?","Suppose we have a sequence of random variables in a Polish space, such that $X_n$ converges in distribution to $X$ . Suppose also that we have random variables $Y_n,Y$ such that $\mathcal{L}(Y_n|X_n)=\mathcal{L}(Y|X)$ for all $n$ .
Is this enough to conclude that $(X_n,Y_n)$ converges in distribution to $(X,Y)$ ? Something I've been thinking about is showing whether the function $g(x)=\mathbb{E}[f(Y,X)|X=x]$ is continuous in $x$ when $f$ is continuous (or bounded and continuous), so that using averaging property and DCT I can show that $\lim_n\mathbb{E}[f(X_n,Y_n)])=\mathbb{E}[f(X,Y)])$ .  But I haven't been able to get around this. Does anyone know if this works? If it doesn't work in general, do you have a counter example? And in the latter case, does anyone know of conditions that would ensure the desired convergence? A partial answer I found is that If we can write $Y_n=F(X_n,U_n)$ (and thus $Y=F(X,U)$ ) for $X_n \perp U_n$ , $X \perp U$ , and  for a $F$ such that $x\rightarrow F(x,u)$ is continuous for all $u$ , then I have the result I want as follows. For $f$ bounded and continuous, we can rewrite $f(X_n,Y_n)=\tilde{f}(X_n,U_n)$ , with $\tilde{f}$ is bounded, and continuous in the first argument. Then, $$ \mathbb{E}[\tilde{f}(X_n,U_n)]=\mathbb{E}[\mathbb{E}[\tilde{f}(X_n,U_n)| X_n]].$$ Now, $x\rightarrow \mathbb{E}[\tilde{f}(X_n,U_n) | X_n=x]$ is continuous by Dominated Convergence Theorem. Suppose $x_i\rightarrow x$ , then: $$ \lim_{i\rightarrow \infty} \mathbb{E} [\tilde{f}(X_n,U_n) | X_n=x_i]= \lim_{i\rightarrow i} \int \tilde{f}(x_i,u) \mu(du)\overset{DCT}{=} \int \lim_{i\rightarrow i}\tilde{f}(x_i,u) \mu(du)= \int \tilde{f}(x,u) \mu(du),$$ where $\mu=\mathcal{L}(U_n)=\mathcal{L}(U_n|X_n)$ (independence). Then, since $X_n$ converges in distribution to $X$ , it follows from the fact that $x\rightarrow \mathbb{E}[\tilde{f}(X_n,U_n) | X_n=x]$ is bounded and continuous that $(X_n,Y_n)$ converges in distribution to $(X,Y)$ .",['probability-theory']
4308977,"$f(n) = f(2n)$, and $f(2n + 1) = f(n) + 1$, find expression of such $f$","The question is a. $f(n) = f(2n)$ b. $f(2n + 1) = f(n) + 1$ , with $f(1)=1$ , find expression of such $f$ that defined on positive integers Got an initial idea about the pattern but find the difficulty to find the exact form. It's related to exponent of 2 (obviously). the number of times it can be expressed as addition of 2 exponents for example, for $13 = 2^3+2^2+2^0, f(13) = 3$ and $16=2^4, f(16)=1; 15 = 2^3+2^2+2^1+2^0, f(15)=4$ in other words, it's the sum of all digits in its binary form, right? if so what's the formula of it. during such time as I wrote my thoughts down, it seems I come up with a formula $$a-\sum_{i=1}^\infty  \lfloor a/2^i\rfloor.$$ which seems to be the answer to the question","['number-theory', 'recurrence-relations', 'elementary-number-theory']"
4309034,What are the consistency criteria for Kolmogorov's existence theorem?,"I've been trying to better understand Kolmogorov's Existence Theorem, and I was reading a couple of references to get a better picture of what is going on. But then I got really confused, cause it seemed that each reference used a variation and I was not able to piece them together. The main thing that confused me were the consistency conditions. If I read in Wikipedia and book like ""Measure Theory and Probability Theory"" (by Athreya), then one consistency condition is: $$
\mu_{(t_1,...,t_{n+1})}(B_1 \times ...\times B_n \times \mathbb R) = 
\mu_{(t_1,...,t_n)}(B_1 \times ...\times B_n);
$$ And the second condition is permutation. The issue is, in books like ""Stochastic Calculus"" (by Baldi) and some lecture notes,
the theorem is stated only with the first consistency condition, i.e. without permutation. Why is that so? I'm guessing there might be some ""hidden"" difference in the way the theorems are stated, but I could not figure out. Here is the theorem as stated by Baldi: Consistency : Let $\pi = (t_1,...,t_n)$ and $\pi'=(t_1,...,t_{i-1},t_{i+1}, ..., t_n)$ . Let $p_i : E^n \to E^{n-1}$ be the map defined as $(x_1,...,x_n)\mapsto (x_1,...,x_{i-1},x_{i+1},...x_n)$ . Tjhe the image of $\mu_\pi$ through $p_i$ is equal to $\mu_{\pi'}$ . Theorem (Kolmogorov's Existence - Baldi) Let $E$ be a complete separable metric space,
with finite-dimensional distributions $(\mu_\pi)$ satisfying the consistency condition. Let $\Omega = E^T$ (the set of all paths from $T\to E$ ) and define $X_t(\omega) = \omega(t), \mathcal F_t( \sigma(X_s, s \leq t), \mathcal F = \sigma(X_t,t \in T)$ . Then there exists a unique probability $P$ on $(\Omega, \mathcal F)$ with the $(\mu_\pi)$ family of finite-dimensional distributions. Now, Baldi is doing things in quite a general context. For a simpler version, some lecture notes I'm using state Kolmogorov's theorem as the following: Theorem (Kolmogorov's Existence - Notes) For each $n \geq 1$ consider a probability measure $P_n \in \mathbb R^n$ such that $$
P_{n+1}(A\times \mathbb R) = P_n(A), \forall A \in \mathcal B(\mathbb R^n).
$$ Then there exists a unique probability measure $P$ in the space of infinite sequences $(\Omega, \mathcal F)$ such that $P(A \times \mathbb R \times ...) = P_n(A)$ for every $n$ and every $A \in \mathcal B(\mathbb R^n))$ . Finally, Durrett also expresses the theorem similarly without the permutation: Theorem (Kolmogorov's Existence - Durrett) Suppose we are given probability measure on $(\mathbb R^n, \mathcal B(\mathbb R^n))$ that are consistent, that is $$
\mu_{n+1}((a_1,b_1]\times ... \times (a_n,b_n] \times \mathbb R)
= \mu_n((a_1,b_1]\times...\times (a_n,b_n]).
$$ There there is a unique probability measure on $(\mathbb R^\mathbb N, \mathcal B(\mathbb R^\mathbb N))$ . Why there is no permutation condition in these versions?","['stochastic-processes', 'measure-theory', 'probability-theory']"
4309089,Weighted summation of symmetric Bernoulli RV. Characteristic function inequality,"Let $$S_n = \sum_{k=1}^n \frac{X_k}{\sqrt{k}}$$ where $X_1, X_2, \ldots$ are iid symmetric Bernoullis with parameter $\frac{1}{2}$ : $$X_k =
\begin{cases}
1 &p=\frac{1}{2}\\
-1 &p=\frac{1}{2}
\end{cases}
$$ I found that the characteristic function for $S_n$ is $$\varphi_n(t)=\prod_{k=1}^n \cos\left(\frac{t}{\sqrt{k}}\right)$$ and have proved the following inequality $$|\mathbb{E}[\exp\{it(S_{n+m}-S_n)\}-1]| \leq |t|\cdot \mathbb{1}_{|\Delta S| < 1 } + 2\mathbb{P}(|S_{n+m}-S_n| \geq 1)\cdot \mathbb{1}_{|\Delta S| \geq 1 }, \ \forall \ t \in \mathbb{R}, \ n,m > 0$$ where $\Delta S = S_{n+m}-S_{n}$ . Now I am looking to use this inequality to prove that there exists a subsequence $n_1, n_2,\ldots$ such that $$\mathbb{P}(|S_{n_{k+1}}-S_{n_{k}}| \geq 1) \geq \frac{1}{4}$$ I started with \begin{split}
        \mathbb{P}(|\Delta S_n| \geq 1) &\geq\frac{1}{2} \left( |\mathbb{E}[e^{it\Delta S_n} - 1]|\right)\\
        &=\frac{1}{2} \left( |\mathbb{E}[\cos(t\Delta S_n) + i\sin(t\Delta S_n) - 1]|\right)\\
        &=\frac{1}{2} \left( \left|\mathbb{E}\left[\frac{e^{it\Delta S_n} + e^{-it\Delta S_n}}{2} + i \frac{e^{it\Delta S_n} - e^{-it\Delta S_n}}{2i}- 1\right]\right|\right)\\
        & =\frac{1}{2} \left( \left|\mathbb{E}\left[\frac{e^{it\Delta S_n} + e^{-it\Delta S_n} + e^{it\Delta S_n} - e^{-it\Delta S_n} - 2}{2}\right]\right|\right)\\
        & = \frac{1}{2}\left( \left|\mathbb{E}\left[\frac{2e^{it\Delta S_n} - 2}{2}\right]\right|\right)
         \end{split} where $\Delta S_n = S_{n_{k+1}}-S_{n_{k}}$ . I think my intuition is right to deduce a lower bound for $\mathbb{P}(|\Delta S_n| \geq 1)$ with trig identities, but I'm stuck. Any help is welcome, thanks!","['characteristic-functions', 'random-walk', 'expected-value', 'probability-theory', 'complex-numbers']"
4309106,Is ${\rm Core}_G(H) \leqslant C_G(A)$?,"Let $G$ be a group and let $A$ be a normal abelian subgroup of $G$ . Suppose $G$ splits over $A$ with complement $H$ . Prove that ${\rm Core}_G(H) \leqslant C_G(A)$ . I have been trying to prove that the core is contained in the centralizer but I have not been successful. Any hints would be very useful. I clarify some of the notation: $G$ splits over $A$ with complement $H$ means that $G=AH$ and $A\cap H=1$ . ${\rm Core}_G(H)=\bigcap_{g\in G}H^g$ . $C_G(A)=\{g\in G:ag=ga, \forall a\in A\}$","['group-theory', 'normal-subgroups', 'abelian-groups', 'solvable-groups']"
4309162,Derivative w.r.t. the weight matrix in a linear layer?,"I'm deriving the back-propagation equations in a neural network. I have a single linear layer. I want to calculate the derivative of the loss function w.r.t. the weight matrix $W$ . The Loss is some function on the output $Y$ , $L(Y)$ . $Y$ is given as $Y=XW^T$ where $Y, X, W$ are all matrices. The answer is as below: $$
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial Y} \frac{\partial Y}{\partial W} =?=(\frac{\partial L}{\partial Y})^T X
$$ I now want to derive that answer using index notation but am completely stuck. I first start by rewriting $Y$ for a single element: $$
Y_{ij} = \sum^k X_{i,k}W_{j,k}
$$ But I don't really know how to complete the derivation using only index notation. And why does $\partial L / \partial Y$ become transposed, while it is before $\partial Y / \partial W$ . I can derive that its necessary by investigating the shapes of the matrices, but that's not really sound reasoning imo.","['matrices', 'jacobian', 'partial-derivative', 'neural-networks']"
4309198,"On which tangent bundles of $\mathbb R^2$ does position, velocity, acceleration live?","I am studying differential geometry, and am unable to align what I have learnt in classical mechanics with how differential geometers describe the situation. Consider a particle travelling in a circle. We draw its position, its velocity, and its acceleration all on the same plane, and are given by the equations: \begin{align*}
&p, v, a: [0, 2 \pi) \to \mathbb R^2 \\
&p(\theta) \equiv (\cos \theta, \sin \theta) \\
&v(\theta) \equiv (-\sin \theta, \cos \theta) \\
&a(\theta) \equiv (-\cos \theta, - \sin \theta) 
\end{align*} and is drawn with $p$ as a curve, $v, a $ as vector fields on the curve: Let's now turn to differential geometry: The manifold $M$ is the surface $\mathbb R^2$ . The path of the particle $p: [0, 2 \pi] \to M$ is a curve that lives on the surface of the manifold. The velocity vectors are a vector field $v: [0, 2 \pi] \to TM$ . The normal vectors are a vector file $a: [0, 2 \pi] \to T^2M$ , since they are derivatives of the vector field $v$ . However, at least in the physics picture, there is a ""natural"" identification between $M$ , $TM$ , and $T^2M$ , since I can ""draw"" the velocity and acceleration next to each other, and even take the dot product between velocity and acceleration! So where am I missing the link between classical mechanics and differential geometry? In particular, I'd like to understand: Why can we draw the tangent and normal vectors next to each other on the circle? This must give some identification of $TM$ and $T^2 M$ Is this special for curves? Does the same thing happen for vector fields of surfaces?","['curves', 'classical-mechanics', 'differential-geometry']"
4309221,"The Expectation and Variance of the number of $k$ size sets containing exactly $m$ edges in $G(n, p)$","Let $X$ be the number of sets of size $k$ containing exactly $m$ edges in $G(n,p)$ . Find $EX$ and $Var X$ . My Attempt Given some indicators $X_1, \dots, X_{\binom{n}{k}}$ we have that $X = \sum X_i$ . Where, $$X_i = P(X_i = m \hspace{0.2cm} edges) = \binom{\binom{k}{2}}{m} p^m (1-p)^{\binom{k}{2} -m}$$ $$EX = \sum E X_i = \binom{n}{k} \binom{\binom{k}{2}}{m} p^m (1-p)^{\binom{k}{2} -m}$$ To find the Variance I need to apply $$Var X = EX + \sum_{i \neq j} EX_i X_j - (EX)^2$$ But I am not quite sure on how to compute $\sum_{i \neq j} EX_i X_j $ .  Any help regarding computing and solving the variance is much appreciated.","['graph-theory', 'random-graphs', 'probability-theory']"
4309255,Cardinality of hypersphere over $\mathbb{F}_p$ is always divisible by $p$?,"I recently read an interesting post here (the second answer to this ) about proving quadratic reciprocity by considering the set $$S_p^{n-1} = \{(x_1,\dots,x_n)\in\mathbb{F}_p^n : x_1^2+\dots +x_n^2 = 1\}$$ Which is just the equation for an $(n-1)-$ sphere with entries from a finite field.
Then, consider $$N_p(n):= |S_p^{n-1}|$$ which just gives how many possible solutions to $x_1^2+\dots +x_n^2 = 1$ there are in $\mathbb{F}_p$ .
It turns out that $$N_p(2) \equiv 1 \mod p \iff p \equiv 3 \mod 4$$ and $$N_p(2) \equiv -1 \mod p \iff p \equiv 1 \mod 4$$ for all $p$ , which is interesting.
After reading the proof in the link above, I see why this is the case.
What I do not understand is that for $n>2$ , I wrote some code and it seems $$N_p(n) \equiv 0 \mod p$$ for all $p$ .
So my question is Why is it the case that $p$ divides $N_p(n)$ for all $n>2$ ? What does having a certain amount of solutions on a hypersphere over $\mathbb{F}_p$ have to do with being divisible by $p$ ?","['algebraic-geometry', 'finite-fields', 'abstract-algebra']"
4309373,Trying to prove $\left(1-\frac13+\frac15-\frac17+\cdots\right)^2=\frac38\left(\frac1{1^2}+\frac1{2^2}+\frac1{3^2}+\frac1{4^2}+\cdots\right)$,"It has been more than 7 days I have been trying to prove this following result using Harmonic Numbers Let me add this Proving $\left(1-\frac13+\frac15-\frac17+\cdots\right)^2=\frac38\left(\frac1{1^2}+\frac1{2^2}+\frac1{3^2}+\frac1{4^2}+\cdots\right)$ doesn't answer my question as I'm looking for the proof-based using Harmonic Numbers . If possible, I would like to avoid integration for the series so that I at least could get the intuitional background. My attempts:
I could prove The LHS: $\left(1-\frac13+\frac15-\frac17+\cdots\right)^2= \lim_{x\to 1}(\tan^{-1}x)^2$ which because of the this square of the series I was looking for a non-squared series which later $$\tan^{-1}\frac1x = \sum_{k\ge0}\frac{(-1)^k}{(2k+1)x^{2k}} $$ Which later for $(\tan^{-1}x)^2 =(\sum_{k\ge0}\frac{(-1)^k}{(2k+1)x^{2k}} )^2 $ Or $$(\tan^{-1}x)^2 = \sum_{q\ge0}\sum_{p\ge0} \frac {(-1)^{p+q}}{(2p+1)(2q+1)}x^{2{(p+q+1)}}$$ In the end I could find this beautiful formula(For me at least) $$\color{blue}{(\tan^{-1}(x))^2 = \sum_{k =1}^{\infty}\frac {(-1)^{k-1}}{k}h_kx^{2k} }$$ $$\text{ Where }  h_k = \sum_{i =1}^{k}\frac 1{2i-1} = H_{2k} - \frac12H_k$$ Which later using $$\sum_{k =1}^x a_kb_k = S_xb_x - \sum_{k =1}^{x-1}S_k(b_{k+1} - b_k)$$ $$\sum_{k =1}^\infty\frac{(-1)^{k-1}(H_{2k} - \frac12H_{k})}{k} = \lim_{x\to\infty}\left(H'_x(H_{2x} -\frac12H_x) - \sum_{k =1}^{x -1}\frac {H'_k}{2k+1}\right) $$ I tried to use a few other identities so that I at least could end up to $\zeta(2), Li_n(x)$ but I'm stuck and don't know how to go forward Also, $$f(x) = \sum_{k =1}^{\infty}\frac {h_kx^{2k-1}}{2k-1} \text { , } |x|< 1$$ $$f\left(\frac x{2-x}\right) = \frac18\log^2(1-x) + \frac12Li_2(x)$$ Where, $Li_n(1) = \zeta(n)$ I do not want to use facts like $\frac π4,\frac{π^2}{6}$ I'm looking for the complete proof that bridges square of alternate odd harmonic number series with a sum of reciprocals of squared natural numbers. Why did I add this problem to MSE?
I'm really not able to find time for solving maths and can't live stable without solving it at the same time.
Even If I'll have time most of all my time goes into finding things that are already(invented) as I lack mathematical background most of all of the above formulas are from Ramanujan's lost notebook and most of all my time goes in shuffling pages...
I Hope MSE can help me live obsessed-free life...haha","['summation-method', 'summation', 'alternative-proof', 'harmonic-numbers', 'sequences-and-series']"
4309395,Relationship Between a Parabola and the Normal Distribution,"In this video ( https://www.youtube.com/watch?v=m62I5_ow3O8 ), at 4:05 the author talks about taking the (2nd Order) Taylor Expansion of the Likelihood Function (in statistics) for some ""model parameter"". Naturally, the 2nd Order Taylor Expansion will be a ""parabola shaped function"". Based on this 2nd Order Expansion, the author is interested evaluating the ""value of the model parameter"" that brings the value of the (2nd Order Expanded) Likelihood Function to $0$ . The author states that instead of directly taking the 2nd Order Expansion of the Likelihood Function, it is more advantageous to take the 2nd Order Expansion of the (negative) Logarithm of the Likelihood Function ( I didn't quite understand the reason behind this, why exactly is it advantageous? ). However, the author states that the 2nd Order Expansion of the (negative) Logarithm of the Likelihood Function will no longer be shaped like a parabola - but rather, take the form of a Gaussian Distribution (i.e. an ""inverted parabola""). In general terms, I am trying to understand the mathematical logic behind this. Suppose I have some arbitrary 2nd Order Function : $f(x) = x^2$ . If I take the second derivative of the of the negative logarithm of this function, I get the following result: Second derivative: $-\log(x^2) = \frac{2}{x^2}$ If I were to plot this function: I can see that the ""Red Function"" somewhat resembles a ""bell curved shape"" normal distribution. My Question: Is my understanding of the above correct? And is there any reason as to why the negative logarithm of the 2nd order expansion is ""more advantageous"" compared to just the 2nd order expansion? Thanks!","['graphing-functions', 'derivatives', 'probability', 'logarithms']"
4309430,"Prove that for $f\colon S^1 \to S^1$, $\Vert f(x)-x\Vert<1$ implies $f$ is surjective.","I'm working on a topology problem that asks Let $f:S^1\to S^1$ be continuous where $S^1$ is the unit circle. Suppose that $\Vert f(x)-x\Vert<1$ for all $x$ , then $f$ is surjective. So far, I'm able to show that if $f$ is not surjective, it has degree zero, but from there it gets more complicated. My intuition is that so long as $f$ is continuous, mapping $S_1$ to an interval would mean there is some point that will be ""too far"" from its image, which is the desired contradiction, but I'm not sure how to make this notion rigorous, or even if its entirely correct. What should I be thinking about in order to figure out how to solve this? Thanks!","['general-topology', 'algebraic-topology']"
4309441,Find the minimum value of the constant term.,"Let $f(x)$ be a polynomial function with non-negative coefficients such that $f(1)=f’(1)=f’’(1)=f’’’(1)=1$ . Find the minimum value of $f(0)$ . By Taylor’s formula, we can obtain $$f(x)=1+(x-1)+\frac{1}{2!}(x-1)^2+\frac{1}{3!}(x-1)^3+\cdots+\frac{f^{(n)}(1)}{n!}(x-1)^n.$$ Hence $$f(0)=\frac{1}{2}-\frac{1}{6}+\sum_{k=4}^n\frac{f^{(k)}(1)}{k!}(-1)^k.$$ This will help?","['calculus', 'functions', 'polynomials', 'inequality']"
4309442,Simple objects with zero dimensional support,"I am reading the book “Fourier-Mukai transforms in algebraic geometry” by Daniel Huybrechts. Before stating my question, let me state some definitions. Here, $X$ is a smooth projective variety. Definition: In a k-linear triangulated category $\mathcal{D}$ , an object $P\in \mathcal{D}$ is called simple if $Hom(P,P)$ is a field. Definition: The support of a complex $\mathcal{F}^{\bullet}\in D^b(X)$ is the union of all its cohomology sheaves, i.e. it is the closed subset $$supp(\mathcal{F}^{\bullet}):=\cup \ supp(H^i(\mathcal{F}^{\bullet}))$$ . And for support of complexes, we have the following lemma (it is stated on page 65 as Lemma 3.9 in Huybrecht’s book): Lemma: Suppose $\mathcal{F}^{\bullet}\in D^b(X)$ and $supp(\mathcal{F}^{\bullet})=Z_1\cup Z_2$ where $Z_1,Z_2$ are disjoint closed subsets of $X$ . Then, $\mathcal{F}^{\bullet}\cong \mathcal{F}_1^{\bullet}\oplus\mathcal{F}_2^{\bullet}$ Now, my question is about the proof of Lemma 4.5, on page 92, in Huybrecht’s book. This lemma is as follows: Lemma: Suppose $\mathcal{F}^{\bullet}$ is a simple object in $D^b(X)$ with zero dimensional support. If $Hom(\mathcal{F}^{\bullet},\mathcal{F}[i])=0$ for $i<0$ , then $$\mathcal{F}^{\bullet}\cong k(x)[m]$$ for some closed point $x\in X$ and some integer $m$ . In its proof, to show $\mathcal{F}^{\bullet}$ is concentrated in exactly one closed point, he uses Lemma 3.9, which I stated before, and he get the contradiction if the assumption of Lemma 3.9 holds and then he conclude that we may assume that the support of all cohomology sheaves $H^i(\mathcal{F}^{\bullet})$ consists of the same closed point $x\in X$ . My question is that why is this true?","['derived-categories', 'algebraic-geometry']"
4309444,Can a ratio of sinusoidal functions with the same frequency always be written as a tangent function?,"In general, two sinusoidal functions $y_1$ and $y_2$ with angular frequency $\omega$ can be written as $y_1 = A\sin(\omega x) + B \cos(\omega x)$ , $y_2 = C\sin(\omega x) + D \cos(\omega x)$ . Where $A, B, C, D$ are real numbers and it is not the case that $A=B=0$ or $C=D=0$ . Can the ratio $\frac{y_1}{y_2}$ always be written in the form $$ \frac{y_1}{y_2} = E \tan(\omega x + h) + k$$ for some constants $E, h$ , and $k$ ? Edit:
For example $$\frac{12 \sin (x) + 3 \cos(x)}{  4 \sin(x) + 2 \cos(x)}$$ has a graph like this: Which looks like the tangent function transformed by scaling and shifting.","['trigonometry', 'ratio']"
4309472,Number of group homomorphisms from quaternion group to itself.,"I have to find all group homomorphisms from quaternion group $Q_8=\{\pm 1, \pm i, \pm j,\pm k\}$ to itself . I tried it as Since $Aut(Q_8)\cong S_4$ so there are $24$ automorphisms which are also homomorphisms. One is trivial homomorphism which maps all elements to identity and there are $4$ homomorphisms whose range are inside $\{\pm 1\}$ as they are factor through $\frac{Q_8}{\{\pm 1\}}\cong \mathbb Z_2\times\mathbb Z_2.$ Now there is  no onto maps from $Q_8$ onto $\{\pm 1,\pm i\}$ , $\{\pm 1,\pm j\}$ , $\{\pm 1,\pm k\}$ as these are cyclic groups of order $ 4$ but $\frac{Q_8}{\{\pm 1\}}\cong \mathbb Z_2\times\mathbb Z_2$ has no element of order $4$ . So I conclude that in  total there are $24+1( trivial)+3=28$ group homomorphisms. Please comment if any mistakes. Thank you.","['group-homomorphism', 'group-theory', 'finite-groups', 'quaternions']"
4309506,"""Improper"" Stokes theorem","In this question I will use Stokes' theorem with manifolds (well, domains in manifolds), rather than with chains. Let $M$ be a smooth manifold of dimension $m$ . A subset $D\subseteq M$ is called a regular domain if 1) $D$ is compact, 2) $D$ is the closure of its interior, 3) the topological boundary $\partial D$ is a smooth hypersurface in $M$ (everything should also be valid when $\partial M$ is a ""manifold with corners"", i.e. it is piecewise smooth instead of smooth, but I now ignore that case for simplicity). Stokes' theorem then states that $$ \int_{\partial D}\omega=\int_D d\omega, $$ where $\omega$ is a $C^1$ $m-1$ -form and the pullback to $\partial D$ on the left hand side is implicit. The compactness of $D$ as well as the regularity of $\omega$ ensures that both integrals are convergent. In the textbook G. de Rham: Differentiable Manifolds , there is a counterexample that shows that Stokes' theorem need not be valid when $D$ is noncompact (and $\omega$ has no compact support), even if everything converges. As an example, let $D=(-\infty,0]$ and $f$ is a function such that $f(x)= -1$ for $x\le -1$ and $f(x)=0$ for $x\ge 0$ . The boundary of $D$ is $\partial D=+\{0\}$ , where the $+$ sign means that this point is assigned a positive oriantation, then $$ \int_D df=\int_{-\infty}^0 df=1, $$ but $$ \int_{\partial D}f=f(0)=0, $$ which shows that Stokes' theorem does not apply to this case. The problem is that the integral $\int_D df$ is calculated as $\int_D df=\int_{-\infty}^0f^\prime(x)dx=f(0)-\lim_{r\rightarrow\infty}f(-r)$ , i.e. it is an improper Riemann integral, and a version of Stokes' theorem is obeyed with $D$ having a ""fictious"" boundary piece $-\{-\infty\}$ (the outer sign denotes orientation). But of course as a manifold with boundary, $\partial D=+\{0\}$ , and the ""boundary piece at infinity"" is missing here and this is why Stokes' theorem fails. A similar situation is often encountered in theoretical physics, where one often integrates over the entirety of $\mathbb R^3$ . As a manifold, $\partial\mathbb R^3=\varnothing$ and Stokes' theorem says that $\int_{\mathbb R^3}d\omega=0$ if $\omega$ has compact support and does not say anything otherwise. However in physics, such and integral would be considered with $\mathbb R^3$ having a fictious ""boundary at infinity"". In fact, what happens (often implicitly) is that instead of integrating over $\mathbb R^3$ , one integrates over the closed ball $\bar B_r\equiv \bar B_r(0)$ of radius $r$ , which is a regular domain whose boundary is $S_r\equiv S_r(0)$ the $2$ -sphere of radius $r$ . One may then take apply Stokes' theorem to $\mathbb R^3$ itself by $$ \lim_{r\rightarrow\infty}\int_{\bar B_r}d\omega=\lim_{r\rightarrow\infty}\int_{S_r}\omega, $$ with the compact closed balls $\bar B_r$ providing an exhaustion of $\mathbb R^3$ with compact sets. Now, unlike calculus texts where improper Riemann integrals are frequently treated, I have never ever seen a textbook on differential geometry, where integration and Stokes' theorem are treated in a way that improper integrals are incorporated. Ideally there would be a formalism where the Stokes' theorem $\int_M d\omega=\int_{\partial M}\omega$ could be interpreted for any $m$ -manifold (with or without boundary) $M$ and any $m-1$ -form $\omega$ such that the left hand side converges and where $\partial M$ is allowed to have ""fictious"" pieces at infinity. Probably such a version of Stokes' theorem would be defined in terms of exhaustions by regular domains, but of course there are questions I don't know the answer to such as Does every $M$ admit such an exhaustion? or Is the value of the integral independent of the exhaustion? I am looking for resources (textbooks, papers) where a coherent theory of ""improper integrals"" on manifolds (including Stokes' theorem!) is elaborated.","['integration', 'improper-integrals', 'stokes-theorem', 'differential-topology', 'differential-geometry']"
4309536,Relation between $S$-ideal class group and usual ideal class group,"For $\mathcal{O}_{K}$ , the integer ring of a global field, we denote $S$ to be any set of primes of a global field $K.$ Let $$\mathcal{O}_{K,S}:=\{x\in K\mid v_{\mathfrak{p}}\geq 0\text{ for }\mathfrak{p}\notin S\}$$ be the ring of $S$ -integers of $K$ (see Neukirch, Schmidt, Wingberg Cohomology of Number Fields , Ch. VIII, § 3). Ideal class group of $\mathcal{O}_{K,S}$ is called $S$ -ideal class group . Neukirch, Schmidt, Wingberg, Cohomology of Number Fields , Ch. VIII, § 3, p. $452$ states that $S$ -ideal class group is the quotient of the usual ideal class group $Cl_K$ of $K$ by the subgroup generated by the classes of all prime ideals in $S$ . without no more explanation.
How can I prove this statement? My try and thought: Let $X＝\operatorname{Spec}(\mathcal{O}_{K})$ , $X_S＝\operatorname{Spec}(\mathcal{O}_{K,S})$ .
Natural map $X_S→X$ induces natural surjective map $f:\operatorname{Pic}(X)→\operatorname{Pic}(X_S)$ .Thus, $\operatorname{Pic}X/\ker f$ is isomorphic to $\operatorname{Pic}(X_S)$ (Hartshone, Proposition $Ⅱ6.5$ ).
So, I need to prove $\ker f$ is generated by the classes of all prime ideals in $S$ .
This is algebraic geometrical point of view, I want to accomplish this kind of proof, but another algebraic number theoretical approach is also appreciated.","['algebraic-number-theory', 'ideal-class-group', 'number-theory', 'algebraic-geometry', 'arithmetic-geometry']"
4309562,Infinite Sum of Infinite Product,"I've got an expression, $$
\left( \prod^{n}_{i = 1} \mu a_i \right) \left(\sum^{n}_{j=1} \frac{1 - a_j}{\prod^{j}_{k=1} \mu a_k} \right) 
$$ Where each $0 < a_j < 1 / \mu$ and $\mu > 1$ , so for all $a_j$ , $0 < 1 - a_j$ . I'm trying to find an analytic solution, or determine if one exists, for this expression. I know beforehand that this converges as $n \rightarrow \infty$ . Both the case of finite $n$ and for infinite $n$ terms are useful. What methods or approaches are helpful here? Pointing to similar problems is also helpful. I'm not sure what to study to understand when looking for simplifications of sums of products.","['summation', 'analysis', 'upper-lower-bounds', 'products', 'sequences-and-series']"
4309598,What is this lognormal-like distribution?,"I have a distribution, marked with the blue line, which looks remarkably like a lognormal distribution. . I used the median and mod values to get the mu and sigma values to estimate its lognormal counterpart. The resultant lognormal distribution is given with the orange line. Note that it is shifted slightly to the right to fit best to the distribution. The distribution given by the blue line has the formula: $$
dist(x)=\frac{\mathrm{d}}{\mathrm{d}x}\left(1-\sum_{i=-\infty}^\infty(-1)^{i+1}\left[\Phi\left(\frac{(2i+1)c}{\sqrt{x}}\right)-\Phi\left(\frac{(2i-1)c}{\sqrt{x}}\right)\right]\right),
$$ where $\Phi()$ is the CDF of the standard normal distribution and $c=50$ for this particular case. The above equation is the probability of first incidence happening at $x=i$ to the boundaries $\{-c,c\}$ for a brownian motion starting at $0$ . My questions are: Is this a lognormal distribution? Would I get a better approximation if I use some other measures to get mu and sigma values, i.e., if I use variance and mean? If it is not a lognormal distribution, what it could be? I think I need something slightly more skewed. Thanks in advance! Appendix: Here is a python snippet to obtain the above formula. #Performs the summation of the above CDF upto ss
from scipy.stats import norm
def cdf_of_dist(c,m,ss):
    aa=c/(m)**0.5
    P1=lambda s: ((-1)**(s+1))*(norm.cdf((2*s+1)*aa)-norm.cdf((2*s-1)*aa))
    res=1
    for i in range(ss):
        res+=P1(i)+P1(-i)
    return res-P1(0)
#Takes derivative of the CDF
def dist(c,ss):
    derivative_array=[0]
    for i in range(m-1):
        derivative_array.append(cdf_of_dist(c,i+1,ss)-cdf_of_dist(c,i,ss))
    return derivative_array","['statistics', 'probability-distributions']"
4309605,Is $\limsup_{\epsilon \to 0^+}\frac{1}{\epsilon}\sup_{y \in B(x; \epsilon)}|f(y)-f(x)|$ related to any standard notion of gradient?,"Let $f:\mathbb R^d \to \mathbb R$ be a function and fix scalar $\epsilon > 0$ . Given a pint $x \in \mathbb R^d$ , define $$
\Delta_f(x;\epsilon) := \sup_{y \in B(x; \epsilon)}|f(y)-f(x)|,
$$ where $B(x;\epsilon):=\{y \in \mathbb R^d \mid \|y-x\| \le \epsilon\}$ . Finally, define $\partial^{-} f(x) := \liminf_{\epsilon \to 0^+}\frac{\Delta_f(x;\epsilon)}{\epsilon}$ and $\partial^+ f(x) :=\limsup_{\epsilon \to 0^+}\frac{\Delta_f(x;\epsilon)}{\epsilon}$ Question 1. Is there a way to link the quantities $\partial^{\pm} f(x)$ with any known notion of derivative ? The strong slope of $f$ at $x$ , denoted $|\partial^+ | f(x)$ is defined by $$
|\partial^+| f(x) := \limsup_{y \to x}\frac{(f(y)-f(x))_+}{\|y-x\|},
$$ where $(t)_+ := \max(t,0)$ . This notion of gradient is common in gradient-flow literature, and can be defined for functions on arbitrary metric spaces (e.g spaces of probability measures). Question 2. Is $|\partial^+| f(x)$ related in any way to $\partial^{\pm} f(x)$ ?","['gradient-flows', 'nonlinear-optimization', 'analysis', 'multivariable-calculus', 'derivatives']"
4309625,Are tail bounds on hypergeometric distribution weaker than Chernoff?,"From a previous question , I learned about the tail bounds of the hypergeometric distribution that can be summarized as follows: Let $N$ be the overall number of balls, let $K$ be the number of red balls, and considering drawing $n$ balls at random without replacement. Denote the number of drawn red balls by $X$ , then $X\sim\mbox{Hypergeometric}(N,K,n)$ and we have that for any $0<t<nK/N$ : $$
\Pr[|X-\mathbb E[X]|\ge n\cdot t]\le 2e^{-2t^2\cdot n}.
$$ Denote by $p=K/N$ the probability that a specific drawn ball is red, and denote by $Y\sim \mbox{Bin}(n,p)$ a binomial random variable (representing drawing with replacement). Using the Chernoff bound , we can write, for any $\delta\in(0,1)$ : $$
\Pr[|Y-\mathbb E[Y]|\ge np\cdot \delta]\le 2e^{-\delta^2\cdot np/3}.
$$ To make the comparison easier, let us set $\delta=t/p$ . Then: $$
\Pr[|Y-\mathbb E[Y]|\ge n\cdot t]\le 2e^{-(t/p)^2\cdot np/3}=2e^{-t^2\cdot n/3p}.
$$ That is, the exponent bound for $Y$ seems tighter by a $\Theta(1/p)$ factor (which is important when $p$ is small).
This seems to contradict the intuition that $X$ ""should be"" more concentrated around its mean, as was also suggested by the answer to my previous question. Is $X$ truly more concentrated around its mean? Can we get a bound of $\Pr[|X-\mathbb E[X]|\ge n\cdot t]\le e^{-\Theta(t^2\cdot n/p)}$ ? Edit: This paper claims that binomial tail bounds also apply to hypergeometric random variables. However, it is 10 years old and (as far as I can tell) hasn't been peer reviewed.","['probability-distributions', 'probability-theory', 'probability', 'upper-lower-bounds']"
4309657,Is every semi-Riemannian group geodesically complete?,"I recently found out from this answer that every Lie group equipped with a left-invariant Riemannian metric is a (geodesically) complete Riemannian manifold. I wonder whether the same holds also for a semi-Riemannian group, i.e., a Lie group equipped with a bi-invariant semi-Riemannian metric. Any semisimple and any compact Lie group admits such a metric. Question . Is every semi-Riemannian group geodesically complete? If the answer is no , as I suspect, does it then change anything to assume that the group has dimension three?","['lie-algebras', 'riemannian-geometry', 'semi-riemannian-geometry', 'lie-groups', 'differential-geometry']"
4309724,Show that this is a random inner product,"Fix a probability space $(\Omega,\mathcal A,P)$ and let $\mathcal F $ be a sub $\sigma$ -algebra of $\mathcal A$ . Let $\mathcal P^+$ denote the set of all random variables $X$ on $(\Omega,\mathcal A,P)$ satisfying $E[X^2\mathcal |\mathcal F]<\infty$ a.s., where $E[X^2\mathcal |\mathcal F]$ is a nonnegative extended real-valued conditional expectation. From the conditional Minkowski's inequality and the pull-out property of conditional expectations, we have that $Z_1X_1+Z_2X_2\in \mathcal P^+$ whenever $X_1,X_2\in P^+$ and $Z_1,Z_2$ are $\mathcal F$ -measurable. For $X,Y$ in $\mathcal P^+$ define $\langle X,Y\rangle:=E[(XY)^+\mathcal |\mathcal F]-E[(XY)^-\mathcal |\mathcal F]$ , where $E[(XY)^+\mathcal |\mathcal F]$ , $E[(XY)^-\mathcal |\mathcal F]$ are chosen to be nonnegative real-valued conditional expectations. This is possible because, by the conditional Hölder's inequality, we have $E[|XY|\mathcal |\mathcal F]\leq E[X^2\mathcal |\mathcal F]^{1/2} E[Y^2\mathcal |\mathcal F]^{1/2}<\infty$ a.s.. For $X$ in $\mathcal P^+$ also define $\|X\|:=E[X^2\mathcal |\mathcal F]^{1/2}$ with $E[X^2\mathcal |\mathcal F]$ is chosen nonnegative and real-valued. Note that $\langle X,Y\rangle$ and $\|X\|$ are only defined a.s.. Now am asked to show the following properties for $X,Y,Z$ in $\mathcal P^+$ : $\langle X,Y\rangle=\langle Y,X\rangle$ a.s.. $\langle X+Y,Z\rangle=\langle X,Z\rangle+\langle Y,Z\rangle$ a.s.. $\langle ZX,Y\rangle=Z\langle Y,X\rangle$ a.s. if $Z$ is $\mathcal F$ -measurable. $\langle X,X\rangle\geq 0$ a.s. and $\langle X,X\rangle= 0$ a.s. if and only if $X=0$ a.s.. $\|X\|=\sqrt{\langle X,X\rangle}$ a.s.. $|\langle X,Y\rangle|\leq \|X\|\|Y\|$ a.s.. $\|X+Y\|\leq \|X\|+\|Y\|$ a.s. $\|ZX\|=|Z|\|X\|$ a.s. if $Z$ is $\mathcal F$ -measurable. $\|X\|\geq 0$ and $\|X\|=0$ a.s. if and only if $X=0$ a.s.. My attempt: Clear. We have the identity $$((X+Y)Z)^++(XZ)^-+(YZ)^-=((X+Y)Z)^-+(XZ)^++(YZ)^+$$ Taking conditional expectations on both sides and using linearity we get $$E[((X+Y)Z)^+|\mathcal F]+E[(XZ)^-|\mathcal F]+E[(YZ)^-|\mathcal F]=E[((X+Y)Z)^-|\mathcal F]+E[(XZ)^+|\mathcal F]+E[(YZ)^+|\mathcal F] \quad \text{a.s.}$$ Since everything is finite a.s. we get the result by subtracting. First suppose $Z\geq 0$ . Then $E[(ZXY)^+\mathcal |\mathcal F]=ZE[(XY)^+\mathcal |\mathcal F]$ and $E[(ZXY)^-\mathcal |\mathcal F]=ZE[(XY)^-\mathcal |\mathcal F]$ a.s. by the pull-out property. Taking difference we get $\langle ZX,Y\rangle=Z\langle Y,X\rangle$ a.s.. Similarly if $Z\leq 0$ then $\langle ZX,Y\rangle=Z\langle Y,X\rangle$ a.s.. Finally for arbitrary $Z$ we have $\langle ZX,Y\rangle=\langle Z^+X,Y\rangle+\langle -Z^-X,Y\rangle=Z^+\langle X,Y\rangle-Z^-\langle X,Y\rangle=Z\langle X,Y\rangle$ a.s. using 2. and the previous results. $\langle X,X\rangle=E[X^2|\mathcal F]\geq 0$ a.s. and $E[X^2|\mathcal F]=0$ a.s. if and only if $X=0$ a.s.. $\sqrt{\langle X,X\rangle}=E[X^2|\mathcal F]^{1/2}=\|X\|$ a.s.. $|\langle X,Y\rangle|\leq E[(XY)^+\mathcal |\mathcal F]+E[(XY)^-\mathcal |\mathcal F]=E[|XY|\mathcal |\mathcal F]\leq E[X^2\mathcal |\mathcal F]^{1/2} E[Y^2\mathcal |\mathcal F]^{1/2}=\|X\|\|Y\|$ a.s. by the conditional Hölder's inequality. $\|X+Y\|=E[(X+Y)^2|\mathcal F]^{1/2}\leq E[X^2\mathcal |\mathcal F]^{1/2}+ E[Y^2\mathcal |\mathcal F]^{1/2}=\|X\|+ \|Y\|$ a.s. by the conditional Minkowski's inequality. $\|X\|=\sqrt{\langle ZX,ZX\rangle}=\sqrt{Z^2\langle X,X\rangle}=|Z|\|X\|$ a.s. by 5,3,1. Follows from 5 and 4. Am I missing something?","['inner-products', 'normed-spaces', 'conditional-expectation', 'solution-verification', 'probability-theory']"
4309727,Is there a numerical method to solve the integral of a function containing a constant?,"Is there a numerical method to solve the integral of a function containing a constant? Such as: $$\int_0^{\pi} x^2 \cos(tx) e^{a\cos(x)} dx , \qquad t\in\mathbb Z^+, a\in\mathbb R$$ While working on one of the statistical derivations, I encountered integrals as shown in the example, and I tried to solve them analytically, but I did not reach any result even after resorting to modified Bessel function.
I do not have a method other than numerical methods, but I do not have enough experience in it.... I look forward to your experience.","['integration', 'numerical-methods']"
4309730,Finding a consistent estimator for area under simple regression line,"I am trying to solve the following problem: Take the following simple linear regression model, where $x_i \in \mathbb R$ : $y_i=\beta_0 + x_i \beta_1 + \epsilon_i$ Given that: $\mathbb E[\epsilon_i]=0$ $\mathbb E[\epsilon_i|x_i]=0$ $\beta_0 >0$ $\beta_1 <0$ Let $\theta_0$ represent the area under the regression line. Propose a consistent estimator of $\theta_0$ . I have began by finding the integral of $y_i$ with respect to $x_i$ . That is, $\theta_0= \int \beta_0 + x_i \beta_1 + \epsilon_i$ $dx_i=\beta_0x_i + {x_i^2\over{2}}{\beta_1} + \epsilon_ix_i$ I am considering proposing an MLE and proceeding to find the derivative of the log-likelihood of this expression. However, since this expression is rather complicated and I foresee the MLE computation turning incredibly thorny, I suspect I may be approaching this incorrectly. Any thoughts?","['regression', 'statistics', 'parameter-estimation', 'maximum-likelihood']"
4309733,Is $BV$ the dual of a separable Banach space?,"In literature I am reading right now, it says $BV(\Omega)$ , where $\Omega \subset \mathbb{R}^n$ an open bounded set, is the dual of a separable space. Is it a dual separable Banach space, or not a Banach space at all? I am asking, because I want to use the weak-* compactness theorem Banach Alaoglu on this space, but then $BV(\Omega)$ needs to be the dual of a separable Banach space.","['banach-spaces', 'functional-analysis', 'bounded-variation']"
4309739,"show this $(f(x))''+2\ge 0$,for any real numbers, where $f(x)=\frac{x^2-1}{x^{2n}-1}$","When I did a question today, I turned to the following question let $n$ be postive integer,and $x\in R$ , let $f(x)=\dfrac{x^2-1}{x^{2n}-1}$ ,show this $$(f(x))''+2\ge 0 \tag{1}$$ For example $n=2$ then $$(f(x))''+2=\dfrac{2x^2(x^4+3x^2+6)}{(x^2+1)^3}\ge 0$$ $n=3$ ,then $$(f(x))''+2=\dfrac{2x^4(x^8+3x^6+6x^4+17x^2+15)}{(x^4+x^2+1)^3}\ge 0$$ and when $n=4$ ,see links1 $n=5$ see links2 , $n=6$ see links3 always hold,so How to prove for $(1)$ ,maybe this usefull: \begin{align*}(f(x))''+2&=(f(x)+x^2)''=\left(\dfrac{x^{2n+2}-1}{x^{2n}-1}\right)''\\
&=\dfrac{x^{2n-2}(4n^2(x^2-1)(x^{2n}+1)-2n(3x^2+1)(x^{2n}-1)+2x^2(x^{2n}-1)^2)}{(x^{2n}-1)^3}\end{align*} see links4","['calculus', 'inequality']"
4309743,Deriving variance for running statistics,"From the definition of the variance: $$\sigma^2 = \frac{1}{N-1}\sum_{i=0}^{N-1}(x_i-\mu)^2 \tag{1}$$ and mean: $$\mu = \frac{1}{N}\sum^{N-1}_{i=0}x_i \tag{2}$$ how is it possible to derive the variance for running statistics, which is: $$\sigma^2 = \frac{1}{N-1}\left[\sum^{N-1}_{i=0}x_i^2-\frac{1}{N}\left(\sum_{i=0}^{N-1}x_i \right)^2\right] \tag{3}$$ why the term $2x_i\mu$ is not considered? Starting from $(1)$ , I get: $$\sigma^2 = \frac{1}{N-1}\left[\sum^{N-1}_{i=0}x_i^2-\sum_{i=0}^{N-1}2x_i \mu\right]+\frac{\mu^2}{N-1} $$","['statistics', 'signal-processing', 'variance', 'standard-deviation']"
4309778,Chain Rule Puzzle,"Here is an old question:
If $\boldsymbol{s_r}$ denote the sum of the $\boldsymbol{r}$ th powers of the roots of the
equation $$\boldsymbol{x^n+p_1x^{n-1}+\cdots +p_n=0}$$ prove that if the coefficients be expressed in terms of $\boldsymbol{s_r}$ then will $$\boldsymbol{\frac{dp_{r+k}}{ds_r}=-\frac{p_k}{r}}$$ [Brioschi.] If we just consider the case $n=3$ and attempt to verify it for $\frac{dp_3}{ds_2}$ then we have Newton's identities, $$-3p_3=s_1p_2+s_2p_1+s_3$$ $$-2p_2=p_1s_1+s_2$$ $$-p_1=s_1$$ Which give $$\frac{dp_2}{ds_2}=-\frac{1}{2}$$ $$\frac{dp_3}{ds_2}=-\frac{p_1}{2}$$ So this seems to verify the question. However a different solution is possible, using the chain rule and this does not seem to work, $$\frac{dp_3}{ds_2}=\frac{dp_3}{dx_1}\frac{dx_1}{ds_2}+\frac{dp_3}{dx_2}\frac{dx_2}{ds_2}+\frac{dp_3}{dx_2}\frac{dx_3}{ds_2}$$ And taking $$\frac{dx_i}{ds_2}=1/\frac{ds_2}{dx_i}=\frac{1}{2x_i}$$ (this is legitimate right ?)
I have the sum $$\frac{dp_3}{ds_2}=\frac{x_2x_3}{2x_1}+\frac{x_1x_3}{2x_2}+\frac{x_2x_2}{2x_3}$$ and this is not equal to the previous answer. What is the error in this second method ?","['calculus', 'derivatives', 'chain-rule']"
4309801,Weak convergence of atomic measure to Lebesgue measure,"Let $G$ be the open $n$ -ball in $\mathbb{R}^n$ and $G^\Delta$ the set of points in $\mathbb{R}^n$ with distance less than $\Delta>0$ from $G$ . Let $G_T=\{Tx: x\in G \}$ and $G_T^\Delta = \{ Tx: x \in G^\Delta\}$ . Let $\partial G$ be the boundary of $G$ , $n(\sigma)$ the outer unit normal vector at the point $\sigma \in G $ and $u>0$ . Every point $y \in G^\Delta \setminus G$ can be uniquely identified by the pair $(u,\sigma)\in ([0,\infty),\partial G)$ through the relationship $y=\sigma + n(\sigma)u$ . Let $T>0$ . $\mu_T$ is the measure in $G^\Delta \setminus G$ consisting of atoms of weight $T^{-(n-1)}$ at the points $$
\{ \omega \in G^\Delta \setminus G: T\omega \in \mathbb{Z}^n \}.
$$ $\bar\mu_T$ is the measure in $G_T^\Delta \setminus G_T$ consisting of atoms of weight $T^{-(n-1)}$ at the points $$
\mathbb{Z}^n\cap (G_T^\Delta \setminus G_T).
$$ For more clarity I added a small sketch for the $2$ -dimensional case. I would like to understand the proof of the statement: The measure $\mu_T$ converges weakly on the set $[0,\Delta)\times \partial G$ as $T\rightarrow \infty$ to the measure $\mu$ equal to the direct product of the Lebesgue measures on $[0,\Delta)$ and $\partial G$ . Proof: The proof uses the following lemma Lemma 1: Let $$\phi(t_1,...,t_r)\in C^2((a_1,b_1)\times...\times(a_r,b_r)),\\
\bigg|\frac{\partial^2 \phi}{\partial t_1^2} \bigg|\geq C >0,\quad \xi=\prod_{j=1}^m(b_j-a_j),\quad T \in (0,+\infty),
$$ let $P_T$ be the measure on $[0,1]$ constituted by atoms of weight $T^{-r}$ at the points $\{T\psi(l_1/T,...,l_r/T) \}$ , where $\{ \}$ stands for the operation of taking the fractional part and $l_j$ runs through independent integral values in the intervals $(Ta_j,Tb_j)$ . Then $\xi^{-1}P_T$ converges weakly to the Lebesgue measure on [0,1] as $T \rightarrow \infty$ . Proof: We consider the conditional measure $P'_T$ for fixed $l_2,l_3,...,l_r$ . The author of the paper claims that $$
\int_0^1e^{2\pi i m t} dP'_T(t)<C_1(\phi)\frac{\sqrt{|m|}}{\sqrt{T}}
$$ for integral $m \neq 0$ holds. Question 1: Why is this inequality correct? Hence $$
\int_0^1 e^{2\pi i m t}dP_T(t) \rightarrow 0
$$ as $T\rightarrow \infty$ . This proves Lemma 1. Question 2: Why does the lemma follow from this (Levy continuity theorem, Weyl criterion, something else...) Main part of the proof: Each sufficiently small open subset of $\partial G$ can be represented as the graph $\Gamma$ of a 2-smooth function $\phi(\tau)$ for $\tau \in V$ , where V is an open subset of the hyperplane perpendicular to one of the basis vectors e of the space $\mathbb{R}^n$ in which $\mathbb{Z}^n$ is imbedded canonically. Let $t(\sigma)$ be the cosine of the angle between e and $n(\sigma)$ . We suppose that $t(\sigma)>0$ for $\sigma \in \Gamma$ . Let $\Pi$ be an open (n-1)-dimensional parallelepiped with edges parallel to the basis vector of $\mathbb{R}^n$ , $\Pi\subset V$ ; $\Gamma(\Pi)$ is the graph of the function $\phi(\tau)$ for $\tau \in \Pi$ , $\xi$ is the Lebesgue measure of $\Pi$ for dimension $n-1$ , $\eta = \max_{\sigma \in \Gamma(\Pi)}t(\sigma)$ , and $a,b\in \mathbb{R}, 0<a<b<\Delta\eta^{-1}$ . We introduce the sets $F\subset [0,\Delta)\times\partial G$ and $H_T \subset \mathbb{R}^n$ in the following way: $$
F=\{(u,\sigma): \sigma \in \Gamma(\Pi), at(\sigma)<u<bt(\sigma) \}
$$ $$
H_T=\{ (p+qe): p\in T \Pi, a <T\phi(p/T)-q<b \}.
$$ Question 3: Why these two particular sets? For sufficiently large T, $H_T \subset G_T^\Delta \setminus G_T$ . We define a map $W: G_T^\Delta \setminus G_T \rightarrow [0,\infty)\times \partial G$ by the formula $$
W(y)=\bigg(u,\frac{\omega}{T} \bigg).
$$ We put $F_T=W(H_T)$ . It follows from Lemma 1 that $$
\lim_{T\rightarrow \infty} \mu_T(F_T)=\lim_{T\rightarrow \infty}\bar\mu_T(H_T)=\xi(b-a). \quad (*)
$$ Question 4: I don't see the connection to the fractional part in Lemma 1. In addition, $\mu(F)=\xi(b-a)$ . Question 5: Why? We introduce $a_i,b_i$ and $\Pi_i$ , i=1,2, subject to the same conditions as $a,b$ and $\Pi$ . Let $a_1 <a<a_2$ , $b_1<b<b_2$ and $\Pi_1 \subset \Pi \subset \Pi_2$ , and let the boundaries of $\Pi_1$ , $\Pi$ and $\Pi_2$ be mutually disjoint. We define $H_{T,i}$ , $F_{T,i}$ and $\xi_i$ as above. For large enough T we have $F_{T,1}\subset F \subset F_{T,2}$ . We apply (*)for $i=1,2$ and let $a_i$ to a and $b_i$ to b and $\Pi_i$ to $\Pi$ , and we obtain $$
\lim_{T\rightarrow \infty} \mu_T(F)=\xi(b-a).
$$ If $t(\sigma)<0$ , we replace the basis vector e by -e and carry out the same reasoning. Because of the arbitrariness of the choice of a, b and $\Pi$ the statement is proved. Question 6: Why? What theorem is used here?","['measure-theory', 'lebesgue-measure']"
4309803,Average length of consecutive numbers which have an increasing number of divisors,"Consider the nine consecutive natural numbers starting from $1584614377$ . n = 1584614377 no. of divisors: 2
n = 1584614378 no. of divisors: 4
n = 1584614379 no. of divisors: 8
n = 1584614380 no. of divisors: 12
n = 1584614381 no. of divisors: 14
n = 1584614382 no. of divisors: 16
n = 1584614383 no. of divisors: 32
n = 1584614384 no. of divisors: 40
n = 1584614385 no. of divisors: 48 Their number of divisors is strictly increasing. This is an examples of a increasing sequence of the number of divisors of length $9$ . For every positive integer $n \ge 1$ , we calculate far we can go before the sequence of increasing divisors breaks. For some $n$ the sequence will break immediately at $n+1$ while for others it might go longer. The longest such sequence I have found so far is the above sequence of length $9$ . On an average how far we can go before the sequence breaks? Let $I_n$ be the average of length of increasing sequence for the first $n$ natural numbers. Update 12-12-21 :  Experimental data shows that $I_{10^6} \approx 1.537$ and $I_{1.2 \times 10^{10}} \approx 1.5537$ . It will be nice if it converges to well known constant such as $\pi/2$ . Question 1 : What is $\lim_{n \to \infty}I_n$ ? Instead of an increasing sequence of divisors, let is consider a decreasing sequence of divisors and let $D_n$ , be the average of length of decreasing sequence for the first $n$ natural numbers. Question 2 : Is it true that $\lim_{n \to \infty}I_n = \lim_{n \to \infty}D_n$ ? Update 20-Jan-22 : Posted in MO since it is unanswered in MSE.","['number-theory', 'analytic-number-theory', 'sequences-and-series', 'limits', 'prime-numbers']"
4309808,Pigeonhole principle: Coloring $11$ points of a $5\times 5$ square grid,"We have a $
5 \times 5$ grid of points, all colored white. Now 11 of the 25 points are colored black. Prove that it is possible to find 3 points all colored black such that no two of them share the same row or column. I strongly feel this can be proved using the Pigeonhole Principle. Hence, I tried to attempt by finding the number of triplets of points which satisfy the condition of not sharing rows or columns. This comes out to be 600 out of total of 2300 ways to pick triplets. The number of triplets with 3 black points is 165. I could not proceed any further, with such values. I also thought 11 black points must occupy at least 3 distinct rows and columns, using Pigeonhole principle. I think I am close to the solution with this approach but am not able to connect the result to be proved with this observation.","['contest-math', 'coloring', 'pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
4309823,What's the role of domain compactness in the Ascoli-Arzela theorem?,"The problem From what I've read in the literature I get the idea that domain compactness is quite crucial in the theorem, and yet looking at the proof it seems to me as an unnecessary assumption. I surmise I'm probably missing an important part of the picture. In order to fix some notation and say what I mean precisely, allow me to repeat the proof down here. I will assume we are all familiar with the notions of equicontinuity and uniform boundedness. The theorem If a family $\mathcal{F}$ of functions from [a,b] to $\mathbb{R}$ is uniformly bounded and  uniformly equicontinuous, then every sequence of functions in $\mathcal{F}$ admits a convergent subsequence. Proof. Let us fix a sequence $(f_n)\subseteq\mathcal{F}$ and an enumeration $(q_n)$ of the rationals in $[a,b]$ . Since the numeric sequence $\big(f_n(q_0)\big)_{n\in\mathbb{N}}$ is bounded, by the Bolzano-Weierstrass theorem, it admits a convergent subsequence $\big(f_{n_k}(q_0)\big)_{k\in\mathbb{N}}$ , which defines a corresponding subsequence of functions $(f_{n_k})_{k\in\mathbb{N}}$ . By the same token, the numeric sequence $\big(f_{n_k}(q_1)\big)_{k\in\mathbb{N}}$ admits a convergent subsequence $\big(f_{n_k}^{(1)}(q_1)\big)_{k\in\mathbb{N}}$ , to which corresponds the subsequence of functions $(f_{n_k}^{(1)})_{k\in\mathbb{N}}$ , and so on. Iterating this process by induction, we get an infinite chain of nested subsequences \begin{equation}
(f_{n_k})_{k\in\mathbb{N}}\supseteq (f_{n_k}^{(1)})_{k\in\mathbb{N}}\supseteq (f_{n_k}^{(2)})_{k\in\mathbb{N}}\supseteq (f_{n_k}^{(3)})_{k\in\mathbb{N}}\supseteq \,...
\end{equation} Consider now the `diagonal' subsequence $(f_{n_k}^{(k)})_{k\in\mathbb{N}}$ . By construction, this sequence of functions converges at all rational points in $[a,b]$ . Therefore, given a $q_m$ and an $\epsilon>0$ there is a $\nu\in\mathbb{N}$ such that \begin{equation}
|f_{n_k}^{(k)}(q_m)-f_{n_h}^{(h)}(q_m)| \le \frac{\epsilon}{3},
\end{equation} for all $k,h\ge\nu$ . Furthermore by the uniform equicontinuity of $\mathcal{F}$ , for any $x\in [a,b]$ there is an interval $J_x$ such that \begin{equation}
|f_{n_k}^{(k)}(y)-f_{n_k}^{(k)}(z)| \le \frac{\epsilon}{3},
\end{equation} for all $y,z\in J_x$ and all $k\in\mathbb{N}$ . (#) These intervals form an open cover of $[a,b]$ from which we can extract a finite subcover $\{J_{x_1},\,...,J_{x_p}\}$ . By the density of $\mathbb{Q}$ , each of these intervals contains a point of $(q_m)$ , and thus there is an $M\in\mathbb{N}$ such that every interval of the finite subcover contains a rational $q_m$ with $0\le m\le M$ . Moreover, for any $x\in[a,b]$ there is a $J_{x_j}$ with $1\le j\le p$ containing a $q_m$ with $1\le m\le M$ . (#) But then, chosen a $q_m$ lying in the same interval as $x$ and $k,h$ sufficiently large, \begin{align}
&|f_{n_k}^{(k)}(x)-f_{n_h}^{(h)}(x)| \le\nonumber\\
&|f_{n_k}^{(k)}(x)-f_{n_k}^{(h)}(q_m)| + |f_{n_k}^{(k)}(q_m)-f_{n_h}^{(h)}(q_m)| + |f_{n_h}^{(h)}(q_m)-f_{n_h}^{(h)}(x)| \le\nonumber\\
&\hspace{3.5cm}\le\frac{\epsilon}{3} + \frac{\epsilon}{3} + \frac{\epsilon}{3} = \epsilon,
\end{align} therefore the sequence is uniformly Cauchy and thus $(f_{n_k}^{(k)})_{k\in\mathbb{N}}$ converges uniformly. The question The entire part enclosed by the symbol (#) seems completely inessential to me. Why can't we just say that by the density of rationals, $J_x$ contains some $q_m$ and be done with it? If that was the case we would not even need to assume compactness to begin with! What am I missing here?","['arzela-ascoli', 'functional-analysis', 'compactness', 'real-analysis']"
4309836,Homomorphism between coherent sheaves concentrated in one point,"I am reading the book “Fourier-Mukai transforms in algebraic geometry"" by Daniel Huybrechts. In the proof of Lemma 4.5 in page 92, he uses Lemma: If $M$ is a finite module over a local ring $(A,m)$ such that $\operatorname{supp}(M)=\{m\}$ , then there exists a surjection $M\twoheadrightarrow A/m$ and an injection $A/m\hookrightarrow M$ . and he concludes that if $\mathcal{F}$ and $\mathcal{G}$ are two coherent sheaves over a smooth projective variety $X$ such that $\operatorname{Supp}(\mathcal{F})=\operatorname{Supp}(\mathcal{G})=\{x\}$ , then there exists a homomorphism $\mathcal{F}\to \mathcal{G}$ . My question is that why is this true?","['algebraic-geometry', 'coherent-sheaves', 'sheaf-theory']"
4309849,Formula(s) for sharing multiple golden geese,"Please have mercy on a simple, non-math person like me. I'm trying to find a formula (or even be told it's impossible) for a unique and difficult compound interest scenario pertaining to the Golden Goose. At least it's unique and difficult to me. Setup: Suppose I have a ""Golden Goose"" which is actually just a regular goose except it lays a solid gold egg every day. Also if I wait and accumulate 100 golden eggs, I can exchange them for another goose. Scenario: So I've got a golden goose and I've got a friend, Jimmy, that also has a golden goose! We agree to pool our eggs so we can collectively buy more geese more quickly. So instead of each of each us waiting 100 days to get another goose, we combine our eggs and reach 100 eggs in 50 days. Jimmy then buys a goose bringing us to 3 geese total. We wait 34 more days, combine our 100 eggs again and then I get a goose. So at the end of this 84 day egg-pooling-cycle we each finish with 1 more goose than we started with. We each got a 2nd goose 16 days earlier than if we waited on our own! Now let's try it again adding a 3rd friend with more geese than either of us. We have another friend, Sally, with 4 golden geese. So I have 2, Jimmy has 2, and Sally has 4, which brings us to 8 total. We decide to pool our eggs again to double our geese. Since Sally has double the geese we each do and is therefore contributing double the eggs, it's only fair that she gets double the geese during this next egg pooling cycle. So in 13 days (we're rounding up from 12.xxx days) we accumulate 100 eggs and Sally gets a goose, bring the new total to 9 geese. In 12 more days, I get a goose. New total = 10 geese In 10 more days, Sally gets a goose. New total = 11 geese In 10 more days, Jimmy gets a goose. New total = 12 geese In 9 more days, Sally gets a goose. New total = 13 geese In 8 more days, I gets a goose. New total = 14 geese In 8 more days, Sally gets a goose. New total = 15 geese In 7 more days, Jimmy gets a goose. Final total = 16 geese So at the end of this cycle, which lasted a total of about 77 days, Sally finishes with 8 geese, I have 4, and Jimmy has 4. So we all figure, wow! This is great! The more people with geese we pool together the more everyone gets geese more quickly. Right? Right? WRONG!! If Jimmy and I would have simply pooled our 4 geese without Sally (or if Sally would have accumulated on her own without us), we/she would have reached 8 total geese in 77 days still. We didn't save any time by adding Sally. So now we're left asking ourselves Was our math right and fair? Did everyone get the number of geese they were supposed to? Is there a scenario where pooling eggs is inefficient for some or all participants? What if it was 10 people each with 1 goose? 20 people each with 1 goose? 3 people each with 1 goose and 2 people with 4 geese? Is there a ratio for pooling eggs that is MOST efficient? And now the real question: What is a formula to find the best ratio and/or length of time for pooling eggs and gaining geese based on the number of participants AND the number of geese each participant begins with? Note : Assume the geese all lay one egg at the same time, in unison, in the morning. So if you acquired 100 geese in the morning after they had already laid their eggs for their previous owner, you would have to wait 1 full day for them to lay again next morning. I was surprised to know that there was already a question about the Golden Goose , but it's not super related.","['modular-arithmetic', 'word-problem', 'economics', 'sequences-and-series', 'problem-solving']"
4309878,Why does the probability of an event change in a binomial experiment with the proportional change of successes and failures?,"Let us assume the following two binomial experiments, assuming coin tosses with a fair coin $(p = 0.5)$ : General: $\binom{n}{k}p^{k}(1-p)^{n-k}$ $\binom{10}{9} \cdot 0.5^{9} \cdot 0.5^{1} = 0.009766$ $\binom{20}{18} \cdot 0.5^{18} \cdot 0.5^{2} = 0.0001812$ Why is it that in the second case the probability of the event decreases, although the successes and failures have been proportionally (doubled) changed here? That seems counterintuitive to me. Is there an intuitive explanation for this? Many thanks in advance! Note: I‘m an undergraduate economics student.","['binomial-distribution', 'probability-distributions', 'probability-theory', 'probability']"
4309880,Convergence of power of EDF: $F(x_n)^n \to 1 \overset{?}{\implies} \hat F_n(x_n)^n \overset{\text{a.s.}}{\longrightarrow} 1$,"Situation Let $X_1, X_2, \dots$ be i.i.d. random variables on a shared probability space, with distribution function $F$ . Denote the empirical distribution function (EDF) as $\hat F_n(\cdot) := \frac 1n \sum_{i=1}^n \mathbf 1\{X_i \leq \cdot\}$ . Suppose we have a sequence $x_m \uparrow \infty$ such that $F(x_m)^m \to 1$ as $m \to \infty$ . Additionally, let $m \equiv m_n = o(n)$ with $m_n \to \infty$ . Question Does it follow that $$\hat F_n(x_m)^m \to 1$$ in any sense (almost surely, in probability, ...) as $n \to \infty$ ? Thoughts It would be nice to have a.s. convergence, but I think the answer is no. Here's my attempt so far: Suppose that $\hat \Omega$ is a set with $\mathbb P(\hat \Omega) = 1$ and $\|\hat F_n^\omega - F\|_\infty \to 0$ for all $\omega \in \hat \Omega$ . (Glivenko-Cantelli tells us this is possible.) Let $\omega \in \hat \Omega$ . Let $\varepsilon > 0$ be arbitrarily small. There exists $N_\omega^\varepsilon \in \mathbb N$ such that $\| \hat F_n^\omega - F \|_\infty \leq \varepsilon$ for all $n \geq N_\omega^\varepsilon$ . So we have for all $n \geq N_\omega^\varepsilon$ $$
\begin{aligned}
0 \leq 1 - \hat F_n^\omega(x_m)^m &\leq 1 - \big( F(x_m) - \varepsilon \big)^m \\
&= 1 - \sum_{k=0}^m \binom{m}{k} F(x_m)^{m-k} \cdot (-\varepsilon)^k \\
&= 1 - \Big[ F(x_m)^m + \sum_{k=1}^m \binom{m}{k} F(x_m)^{m-k} \cdot (-\varepsilon)^k \Big] \\
&= \underbrace{ 1 - F(x_m)^m}_{\to 0} - \sum_{k=1}^m \binom{m}{k} \cdot F(x_m)^{m-k} \cdot (-\varepsilon)^k
\end{aligned}
$$ So the issue is the ""error term"" $$
\text{err} := - \sum_{k=1}^m \binom{m}{k} \cdot F(x_m)^{m-k} \cdot (-\varepsilon)^k,
$$ which we need to vanish to achieve the desired limit. But I don't see how to do that, after all, $$
\begin{aligned}
|\text{err}| &\leq \sum_{k=1}^m \binom{m}{k} \left| (-\varepsilon)^k \right| \\
&= \sum_{k=1}^m \binom{m}{k} \varepsilon^k \\
&= (1 + \varepsilon)^m - 1
\end{aligned}
$$ As $n$ increases, so does $m \equiv m_n$ , driving the right hand side up. On the other hand, higher $n$ 's allow us to choose a smaller $\varepsilon$ , driving the RHS back toward zero. It seems that the speed of convergence (which determines the smallest $\varepsilon >0$ for the $n > N$ ) determines whether convergence is achieved. More information In the particular example I'm looking at we have $F(x) > 1 - \frac{c}{\log^\delta(x)}$ for large $x$ , where $c>0$ and $\delta > 2$ ; and $x_m := \xi \cdot \text e^m$ for $\xi \in (0,1)$ . Maybe the specifics here are helpful.","['statistics', 'probability-limit-theorems', 'probability-theory', 'real-analysis']"
4309944,On one example involving the mean value theorem,"Let me first remind you of what the theorem states (mainly for you to understand the notation): If $f(x)$ is continuous in the closed interval $[x_1;x_2]$ and differentiable at every point of the open interval $(x_1;x_2)$ , then there is at least one point $\xi \in (x_1;x_2)$ such that $$ \frac{f(x_2)-f(x_1)}{x_2-x_1} = f’(\xi)$$ I have been asked to determine the value of $\xi = \xi(x_1,x_2)$ , given that $$ f(x) = \frac{1}{x^2 + 1}.$$ The problem is that when I obtained the derivative $f’(x)$ and, given the definition of $f(x)$ , rewrote the expression from the theorem and simplified it, I arrived at the following equation: $$\xi^4 + 2\xi^2 - 2p\xi + 1 = 0,$$ where $$ p \equiv \frac{(x_1^2 + 1)(x_2^2 + 1)}{x_1 + x_2}.$$ The form of the equation, which is a quartic (though a depressed one, but that does not make the matters easier), suggests that there exist several roots of rather complicated form, but when I look at the graph of $f(x)$ , I see that almost always there is a singular value of $\xi$ (checking by means of the geometric interpretation of the mean value theorem) which satisfies the theorem. The questions is whether it is possible to avoid the cumbersome encounter with the quartic, or it is something I shall inevitably face in order to solve the problem? I also desire to ask you to propose hints only . I want to solve the problem myself, for otherwise I will not be able to learn.","['derivatives', 'real-analysis']"
4309988,Solve BVP $y''-y=t^2$ by green's function,"Construct the green's function for the following boundary value problem and use it to find the solution of $$y''-y=t^2,\quad y(0)=0,y(1)=0$$ I got the complementary solution as: $$y_c=c_1e^t+c_2e^{-t}$$ Following this answer , let $y_1=e^t$ and $y_2=e^{-t}$ . Then $$\textrm{Wronskian}=W(y_1,y_2)=-2$$ $$G(t,u)=\frac{y_1(u)y_2(t)-y_1(t)y_2(u)}{W(y_1,y_2)}=\frac{e^ue^{-t}-e^te^{-u}}{-2}$$ Thus, $$
\begin{align}
y_p&=\int_0^tG(t,u)f(u)\:du\\
&=\int _0^t\:\left(\frac{e^ue^{-t}-e^te^{-u}}{-2}\cdot \:u^2\right)\:du\\
&=e^{-t}\left(-2e^t-t^2e^t+e^{2t}+1\right)
\end{align}
$$ But the solution mentioned in the book was, $$-\frac{\sinh (1-t)}{\sinh 1}\left[t^2\cosh t-2t\sinh t+2\cosh t-2\right]-\frac{\sinh t}{\sinh 1}\left[t^2\cosh(1-t)+2t\sinh (1-t)+2\cosh(1-t)-3\right]$$ Did I do anything wrong when following that answer? Any help will be appreciated.","['boundary-value-problem', 'greens-function', 'ordinary-differential-equations']"
4310003,Question about size of a set,"Suppose you have a non empty set $X$ , and suppose that for every function $f : X \rightarrow X$ , if $f$ is surjective, then it is also injective.  Does it necessarily follow that $X$ is finite ? Every example I've been able to think of leads me to believe this is true.  Is it ? Or could anyone provide a counterexample?",['elementary-set-theory']
4310005,Express This function as a piecewise function,"I need to express this function as a piecewise function. Now, it is not discontinuous so I'm lost. But my attempted solution is to break this into parts, first part is $f(t)=0, -\infty<t\leq a$ , from $a \rightarrow b$ , it is $f(t)=t, a\leq t \leq b$ . The function here is $\frac{1}{b-a}$ because it increases by 1 from a to b. So the slope is assumed to be $\frac{1}{b-a}$ . Am I on the right track?",['functions']
4310062,Extracting coefficients,"I stuck at the following problem: Let \begin{equation}
f(z) = \frac{1 - \sqrt{1 - 4z}}{2}.
\end{equation} Find $[z^n]$ of $f(z)^r$ for $r \in \mathbb{N}$ . Where $[z^n]f(z)$ is the $n$ -th coefficient of the power series $f(z) = \sum_{n \geq 0}{a_n z^n}$ therefore $[z^n]f(z) = a_n$ .
So far I got \begin{equation}
\sqrt{1 - 4z} = \sum_{n \geq 0}\binom{1/2}{n}(-4 x)^n.
\end{equation} and therefore \begin{align*}
  f(z) = \frac{1 - \sqrt{1 - 4z}}{2} &= \frac{\sum_{n \geq 1}\binom{1/2}{n}(-4)^n z^n}{2} \\
  &= \sum_{n \geq 1}\binom{1/2}{n}(-1)^n 2^{n} z^n \\
  &= \sum_{n \geq 1}\binom{1/2}{n}(-2)^n z^n \\
  &= \sum_{n \geq 0}a_n z^n
\end{align*} with coefficients $a_0 = 0$ and $a_n = \binom{1/2}{n}(-2)^n$ . I wanted to use cauchys integral formula for $g(z) = f(z)^r$ to extract the $n$ coefficient
but then I get \begin{align}
  \left( \frac{1 - \sqrt{1 - 4z}}{2}\right)^r &= \frac{1}{2^r} \left(1 - \sqrt{ 1 - 4z} \right)^r \\
  &= \frac{1}{2^r} \sum^{r}_{m=0}\binom{r}{m}(-1)^m \sqrt{1 - 4z}^m \\
  &= \frac{1}{2^r} \sum^{r}_{m=0}\binom{r}{m}(-1)^m (1 - 4z)^{m/2} \\
  &= \frac{1}{2^r} \sum^{r}_{m=0}\binom{r}{m}(-1)^m \sum_{k \geq 0}\binom{m/2}{k}(-1)^k 4^k z^k \\
  &= \frac{1}{2^r}\sum^{r}_{m=0} \sum_{k \geq 0}\binom{r}{m} \binom{m/2}{k} (-1)^{m+k} 4^k z^k.
\end{align} Therefore I should have \begin{align*}
  [z^n] (f(z))^r &= [z^n] \sum^{r}_{m=0} \sum_{k \geq 0}\frac{1}{2^r} \binom{r}{m} \binom{m/2}{k} (-1)^{m+k} 4^k z^k \\
  &= \sum^{r}_{m=0}[z^n] \sum_{k \geq 0}{\frac{1}{2^r} \binom{r}{m} \binom{m/2}{k} (-1)^{m+k} 4^k z^k} \\
  &= \sum^{r}_{m=0}\frac{1}{2^r}\binom{r}{m} \binom{m/2}{n} (-1)^{m + n} 4^n \\
  &= \sum^{r}_{m=0}\frac{1}{2^r}\binom{r}{m} \binom{m/2}{n} (-1)^m (-4)^n \\
  &= \frac{1}{2^r} \sum^{r}_{m=0} \binom{r}{m} \binom{m/2}{n} (-1)^m (-4)^n \\
  &= \frac{1}{2^r} \sum^{r}_{m=2 n} \binom{r}{m} \binom{m/2}{n} (-1)^m (-4)^n 
\end{align*} For $r \geq 2n$ otherwise the coefficient vanishes. I would be thankful if anyone can give me a hint.","['complex-analysis', 'binomial-coefficients']"
4310085,Assessing bias and consistency of modified OLS estimator,"Given the (multivariate) linear regression model $\mathbf y=\mathbf X \mathbf\beta_0 + \epsilon$ and $\mathbb E[\epsilon|\mathbf X]=0$ for $\beta_0 \in \mathbb R^k$ , determine if the following estimator is unbiased a/o consistent: $\hat\beta_j = (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \mathbf y$ where $\lambda>0$ I notice that this is basically just the OLS estimator, but with the addition of a scaled identity matrix inside the inverse. I just do not know how to get rid of this, I feel like there is an algebra trick that I'm missing, or perhaps it involves spectral decomposition. For now, this is where I have arrived: $\hat\beta_j = (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \mathbf X\beta_0 + (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \epsilon$ and $\mathbb E[\hat\beta_j] = (\mathbf X^T \mathbf X + \lambda \mathbf I_k)^{-1} \mathbf X^T \mathbf X\beta_0$ This would seem to suggest that the estimator is biased, but I'm not sure this is correct. I know that it is possible to be both biased and consistent, but I also have no idea how to prove this because I can't isolate $\beta_0$ algebraically in order to take the limit necessary.","['regression', 'statistics', 'parameter-estimation', 'linear-regression']"
4310107,How to use differentials to estimate the relativistic increase in mass from at 90% speed of light to 92%,"the question below is from a section on differentials in an old calculus text I am teaching myself with (Calculus, Varberg & Purcell, 6th edition.) The text gives an answer (9.47%) but does not explain how it is derived. That is what I wish to learn. I understand how to calculate the answer using the equation, but not how to estimate it using differentials. I believe dv = .02, and I have to multiply that by the derivative of the equation below to find dm. But I am confused by the c^2. (which is in fact a constant, further confounding me.)  Must I first express v in terms of c? (for example .9c^2 / c^2, in order to differentiate?  Or possibly I should try implicit differentiation?  Here's the question- any help would be greatly appreciated.  thank you! ""Einstein's Special Theory of Relativity says that the mass m of an object moving at a velocity v is given by the formula: m = m0 (1- v^2/c^2)^(-1/2) Here m0 is the rest mass (mass at velocity 0) and c is the speed of light. Use differentials to estimate the percentage increase of the object as its velocity increases from v=0.90c to 0.92c.""",['derivatives']
4310118,$\sigma$-algebra generated by functions,"I'm working on an exercise that asks you to find $\sigma(u)$ , where $u$ is defined as $u:\mathbb{R}\rightarrow \mathbb{R}, u(x) = x$ $u:\mathbb{R}\rightarrow \mathbb{R}, u(x) = |x|$ $u:\mathbb{R^2}\rightarrow \mathbb{R}, u(x,y) = x^2+y^2$ I know that $$\sigma(u) = u^{-1}(\mathcal{B}) = \{u^{-1}(B):B\in\mathcal{B}\}$$ with $\mathcal{B}$ the Borel $\sigma$ -algebra. And that for $B\in\mathcal{B}$ $$u^{-1}(B) = \{x\in\mathbb{R}:u(x)\in B\}$$ For $u(x)=x$ it's clear to me that $\sigma(u)=\mathcal{B}$ For $u(x)=|x|$ what I did was the following, $$|x|\in (a,b) \implies (x < -a \lor  x > a) \land (-b < x < b)$$ So I think that $$\sigma(u) = \sigma\{((-\infty,-a)\cup(a,\infty))\cap(-b,b):a,b\in\mathbb{R}\}$$ I'm not quite sure about this last part, and even if I'm right I don't know if there is a better way to define the set. Finally, for $u(x,y)=x^2+y^2$ I'm a little bit lost. I know that $a<x^2+y^2<b$ creates 'discs' on the plane but I'm not sure where to go from there. Any help would be really appreciated!","['functions', 'measure-theory', 'abstract-algebra']"
4310150,Using complex analysis for evaluating summations: some general queries,"So, I was reading some old course materials, and came across the following summation, which is to be evaluated through complex analysis: $$\sum_{-\infty}^{\infty}\frac{1}{n^2+a^2}, \ \ \ \ \ \ \ a\ne 0$$ I am fine with the mathematical manipulation for the most part. Instead, I am more curious as to the motivations between a couple of points in the method (below). In my course notes, the following is written: ""Firstly, one considers the integral $$I_N(a) = \oint_{\mathcal{C}} \frac{\pi \cot(\pi z)}{z^2+a^2} dz$$ Where the contour $\mathcal{C}$ is the circle of radius $N+1/2$ ( $N$ integer), and centred on the origin. This contour contains $2N+1$ poles of the function $\pi\cot(\pi z)$ (integers from $-N$ to $N$ ), as well as $\pm ia$ from $(z^2+a^2)^{-1}$ (assuming $N+1/2 > |a|$ ) leading to: $$I_N(a) = \sum_{-N}^{N} \frac{2i\pi}{n^2+a^2}+2i\pi\left(\frac{\pi\cot(i\pi a)}{2ia} + \frac{\pi\cot(-i\pi a)}{-2i\pi a}\right) = \sum_{-N}^{N}\frac{2i\pi}{n^2+a^2}-2i\pi\frac{\pi\coth{\pi a}}{a}$$ But $I_N(a)\propto N^{-1}$ as $N\rightarrow \infty$ since the numerator is of order $1$ , the denominator $N^2$ , and the contour length is of order $N$ , leading (in the limit) to: $$\sum_{-\infty}^{\infty}\frac{1}{n^2+a^2} = \frac{\pi}{a}\coth(\pi a),\ \ \ \ \ \ a\ne 0$$ "" Q1) EDIT: Now understood thanks to the answers and comments below What caught my attention revisiting this, is how seemingly ""out of nowhere"" the function $\pi\cot(\pi z)$ seems to have appeared. For sums like this (and any other that can be analysed similarly), how is such an integral as $I_N(a)$ above even determined- what motivates the decision to choose this integral? (i.e. why is the $\pi\cot(\pi z)$ required to correctly determine this sum? Is it arbitrary? Are there other integrals that would yield the same results?) Q2) EDIT: Now understood thanks to Ron Gordon's answer (below) Is there any significance to the contour being of radius $N+1/2$ in this example? If I understand correctly, radius $N$ would cause the perimeter of the contour to lie on another pole; but why can we not use, say $N+1/4$ , or $N-1/2$ as the radius, or some other non-integer radius? What motivates this decision? Q3) EDIT: Now understood thanks to Ron Gordon's answer + comments (below) $2N+1$ poles of $\pi\cot(\pi z)$ within the contour? Simple thing here- I think I've confused myself slightly, thinking that there are $2N$ integers in the range $[-N, N]$ ... Am I correct in thinking that the $+1$ accounts for $0$ being a pole? (and if the radius is different, as per Q2) above, is there an easy way to determine how many poles you have for a periodic function like this?) Q4) EDIT: Now understood thanks to Ron Gordon's answer (below) ""...and the contour length is of order $N$ ""- just a sanity check really- is the significance of the contour length contributing in the limit of $I_N(a)$ ( $N\rightarrow\infty$ ) because it is effectively a line integral? Not sure if I'm just overthinking, or misunderstanding exactly what's going on here, so a clarification would be appreciated. Any sort of pointers in the right direction, or things for me to think about to try and understand this would be very welcome!
Thanks :)","['complex-analysis', 'summation', 'complex-integration', 'sequences-and-series']"
4310165,Counterintuitive Charecterization of Openness in Euclidian Topology,"I came across an interesting problem and after playing with it for a few days I made some progress but resorted to looking up some solutions by others. My question is about their solutions , so please read along to the end before answering or commenting. The original problem: Prove or disprove: Consider $\mathbb{R}^2$ with the Euclidean topology
and let $X$ be a subset. If for every $a \in X$ and $v \in
 \mathbb{R}^2$ there exists $d > 0$ such that $a + tv \in X$ for every $0 \leq t < d$ then $X$ is open. If you haven't seen this problem before you might want to stop now and try it before I spoil the fun, since I am about to list some solutions below. It is a fun problem. Why the problem is interesting: The standard method of showing a set is open is to cover the set in open balls. The hypothesis of this problem naively makes us think we can do that. Given a point $a$ in $X$ and a point $v$ in the plane, by hypothesis there is some open ray starting at $a$ in the direction of $v$ that is contained in $X$ . So if we center a circle around $a$ and construct an open ray in the direction of every point on the circle, we hope to have created something that can be reduced to an open disk around $a$ and contained in $X$ . [Technically to use the hypothesis as stated we first need to  relocate the set $X$ so that $a$ is located at the origin, this makes $a+tv$ actually represent the ray from $a$ in the direction of $v$ , but the ability to construct an open disk around $a$ doesn't depend on the location of $X$ in the plane, so I won't mention this detail again.] This intuition motivated me to try and prove the claim: so let $a \in X$ and let $S$ be a circle of radius one centered on the point $a$ . Let $v$ be an arbitrary point on $S$ and let $$R_v = \{ a+tv \, | \, 0 \leq t < d_v \}$$ be the set of points on the open ray in the direction of $v$ that exists by hypothesis. Now let $$D = \{d_v \, | \, v \in S \}.$$ If $d = \mathrm{min}(D)$ then we can consider $$R_{v_d} =\{ a+tv \, | \, 0 \leq t < d \}$$ and finally $$\bigcup_{v \in S} R_{v_d}$$ is an open disk centered on $a$ contained in $X$ . Problem: most members of this community will have already noticed the issue, the existence of $\mathrm{min}(D)$ is not guarunteed, and it is fairly clear we will not be able to establish such an existence. At this point I abandon trying to prove the claim and begin looking for a set which fails to be open, satisfies the hypothesis, and where $\mathrm{min}(D)$ does not exist. The question: I found hunting for a counterexample much more difficult than attempting to prove the proposition. I have built a large tool belt of standard methods and techniques for trying to prove openness, and trying to prove claims in general, but trying to find a counterexample seemed much more creatively demanding. I am looking for a set that is not open, that satisfies the hypothesis, and something where $\mathrm{min}(D)$ will not exist, since if $\mathrm{min}(D)$ always existed the theorem would be true. After struggling for a few days I looked up solutions: these are what I found: Solution 1: $\mathbb{R}^2 - \{(x,y) \, | \, x^2 + y^2 = 1 \wedge (x,y) \neq (0,1) \}$ . In words: the plane kickout a circle and add back in a point. Solution 2: $\mathbb{R}^2 - \{(x,y) \, | \, y = x^2 \wedge x > 0 \}$ In both solutions there is a point that causes the set to not be open, (0,1) and (0,0) respectively so I agree neither set is open. I intuitively see how the constructions are intended to meet the hypothesis yet fail to have a minimum $d$ : as we swing the ray around, the shape of the curve removed forces the ray to be smaller and smaller, hopefully causing no minimum. I think any counter example to this problem will involve a point with a removed arc leading up to it contained in some larger open set (perhaps $\mathbb{R}^2$ itself). But what I am having trouble fully convincing myself of is: Is there not one single small direction when we pass the ray from
below the curve removed to above the curve removed where the length of
the ray must be 0? How can we prove that these solutions fully satisfy
the hypothesis: that is, for every direction we have the open ray of
non zero length? What have a tried: Without fail someone will ask it - so lets just cover it now. I worked with solution 1 and considered the point (0,1). I can parameterize the ray from the point to the removed circle as $$r(t) = (0,1) + t*(v - (0,1))$$ where $v = (cos(x), sin(x)), 0 \leq x < 2\pi$ . It is sufficient to derive an expression for the length of $r(t)$ and show that I can swing $r(t)$ around a full $2\pi$ radians and always have nonzero length. I am not convinced that it can move beyond the point of being parallel to the x axis without going to 0!","['general-topology', 'geometry', 'real-analysis']"
4310211,Existence of solution via divergence and curl,"Let us consider a bounded domain $V$ of $\mathbb{R}^3$ with surface $S$ and let $C$ be a divergence-free vector field and $d$ be some real function on $V$ . Let everything be sufficiently well-behaved (smoothness and all that). Now I want to find the solution $F$ to the following $$
\nabla\cdot F=d, \quad \nabla\times F=C
$$ Where $F$ may possibly satisfy some boundary condition on $S$ , e.g., $F\cdot \hat{n}=0$ on $S$ . How would one show that such a solution exists (possibly through explicit construction)? Background . Now from the Wiki page on Helmholtz decomposition, if we were working in $\mathbb{R}^3$ and $C$ decays sufficiently fast at $\infty$ , then this $F$ can be easily constructed with the Green functions/Newtonian potential, i.e., if $$
G(r)=\frac{1}{4\pi |r|}
$$ And $G*f$ denotes the convolution, then we have $F=-\nabla \phi+\nabla \times A$ where $\phi=G*d$ and $A=G*C$ . Indeed, notice that $$
\nabla\times(\nabla\times (G*C))=\nabla(\nabla\cdot(G*C))-\nabla^2(G*C)
$$ The second term is $=C$ since $-\nabla^2 G$ is the dirac-delta function. The first term goes to zero due to the decay of $C$ , indeed, approximating $G*C$ by restricting to some bounded domain $V$ of $\mathbb{R}^3$ with surface $S$ and eventually taking $V \to \mathbb{R}^3$ , we have \begin{align}
\nabla\cdot(G*C)&= -\int (\nabla'G(r-r')) C(r') \\
&= -\int\nabla'\cdot(G(r-r')C(r'))\\
&= -\oint_S \hat{n}'\cdot G(r-r')C(r') \to 0
\end{align} where I have used the fact that $\nabla \cdot C=0$ and $C$ decays sufficiently fast at $\infty$ . However, since we're working on a bounded domain, this term doesn't quite disappear, so the method doesn't quite work.","['integration', 'partial-differential-equations', 'differential-geometry']"
4310246,Alternative proof of an intereting identity of Catalan's Numbers and central binomial coefficients,Some time ago i got from Polya's Urn Scheme that for the n-th Catalan number $C_n = \frac{1}{n+1}\binom{2n}{n}$ and the central binomial coefficient takes place the identity $$\sum_{n = 0}^\infty\frac{C_{n+k}}{4^n} = 2\binom{2k}{k}$$ I'm looking for a non-probabilistic proof of that result.,"['number-theory', 'catalan-numbers', 'combinatorics', 'probability-theory']"
4310247,Probability distribution function of the gestation period,"The problem is :
The ""gestation period"" for humans averages 40 weeks, with a standard deviation
of 10 days. Using CERN's root package, calculate and plot the
probability that a woman will give birth tomorrow, given that she has
made it all the way to today without giving birth yet, as a function of
the day. You can start about two weeks before the due date, and go
until two weeks afterward. I am trying to solve this problem mathematically before solving it numerically, the approach I am taking is:
Let the Guassian distribution give ideally the normalized number of women in a population N0 who give birth during each unit (day) with mean 40 weeks and standard deviation ten days ( probability distribution for large numbers of women of giving birth at 40 weeks with standard deviation 10 days). This can be represented by a normalized distribution  N0 * ( 1/ 10 sqrt(2pi))* exp{ -((x- 280)/10) (sqaure)} [ 280 days is basically 40 weeks: the mean ] The number who give birth during a period dx at time T is
dN = N0 * (1/100 * Sqrt(2pi)) * exp ((T-280)/10) square dT The number of women who have given birth until a time T is then:
N(T) = N0 * Integral of the exponential distribution until T which is the error function of T with some modifications for the coefficients. therefore the chance (probability)that a women will be giving birth at T having NOT done so up to that point is :    dN(T)/ [ N0- N(T)]    giving at T a function  F(T)dT where F(T) is the probability asked for.  Since the denominator is the likelihood that no birth has taken place as it is the number who have not given birth up to that point. N0 will cancel out. I don’t know if my approach works because the distribution I should get must go to 1 after the mean because logically the probability of giving birth should not decrease. Is there a flaw in the logic above ? Any help would be appreciated.","['statistics', 'conditional-probability', 'means', 'probability-distributions', 'probability']"
4310262,What schemes correspond to varieties in the sense of Weil?,"Out of (perhaps morbid) curiosity I am trying to learn the basics of Weil's foundations of algebraic geometry.
I tried to ask a question earlier but it turned out I had misunderstood some more basic points, so I want to confirm them before re-asking my original question. Let $\mathbb{K}$ be a universal domain in the sense of Weil, i.e. an algebraically closed field of infinite transcendence degree over its prime field.
For convenience, by quantity we will mean an element of $\mathbb{K}$ .
A point in $n$ -space is an $n$ -tuple of quantities.
A variety in $n$ -space in the sense of Weil is a pair $(k, P)$ where: $k$ is a subfield of $\mathbb{K}$ such that $\mathbb{K}$ has infinite transcendence degree over $k$ . $P$ is a point in $n$ -space. The subfield $k (P) \subset \mathbb{K}$ generated by $k$ and the components of $P$ is separably generated over $k$ , and $k$ is algebraically closed in $k (P)$ . If $V$ is a variety given by data $(k, P)$ as above, Weil says: $k$ is a field of definition of $V$ . $V$ is defined over $k$ . $V$ is the locus of $P$ over $k$ . $P$ is a generic point of $V$ over $k$ . A point $Q$ in $n$ -space is in $V$ if: For every polynomial $F$ in $n$ variables with coefficients in the field of definition $k$ , $F (P) = 0$ implies $F (Q) = 0$ . Weil says two varieties in $n$ -space are equivalent if they have the same points.
Note that equivalent varieties need not have the same field of definition. Question 1. Is a variety in $n$ -space in the sense of Weil, up to equivalence, the same information as a closed $\mathbb{K}$ -subscheme of $\mathbb{A}^n_\mathbb{K}$ that is integral and of finite type over $\mathbb{K}$ ? More precisely, suppose we are given a variety $V$ in $n$ -space, with a field of definition $k$ and a generic point $P$ over $k$ .
Let $I$ be the set of polynomials $F$ in $n$ variables with coefficients in $k$ such that $F (P) = 0$ .
Then $I$ defines a closed $\mathbb{K}$ -subscheme $\tilde{V}_\mathbb{K}$ of $\mathbb{A}^n_\mathbb{K}$ .
I ask: Is this $\tilde{V}_\mathbb{K}$ integral and of finite type over $\mathbb{K}$ ?
(Yes: see this question .) Is the map $V \mapsto \tilde{V}_\mathbb{K}$ well defined and injective up to equivalence of varieties? Does every closed $\mathbb{K}$ -subscheme of $\mathbb{A}^n_\mathbb{K}$ that is integral and of finite type over $\mathbb{K}$ arise as $\tilde{V}_\mathbb{K}$ for some $V$ ? Furthermore, $I$ also defines a closed subscheme $\tilde{V}_k$ of $\mathbb{A}^n_k$ , and $\tilde{V}_\mathbb{K} \cong \tilde{V}_k \times_{\operatorname{Spec} k} \operatorname{Spec} \mathbb{K}$ .
So $\tilde{V}_k$ is a geometrically integral affine scheme of finite type over $k$ . Question 2. Is a variety in $n$ -space defined over $k$ , up to equivalence, the same information as a closed $k$ -subscheme of $\mathbb{A}^n_k$ that is a geometrically integral affine scheme of finite type over $k$ ?","['affine-schemes', 'algebraic-geometry', 'affine-varieties']"
4310284,Number of permutations generated by using $k$ serial stacks,"Suppose given $k\geq 2$ stacks $S_1,S_2,\dots,S_k$ , and we have $n$ numbers that we can push a number to $S_1$ and pop and push it into $S_2$ and finally we can pop from $S_k$ . How many permutations can generated by this method of using $k$ stacks? Also I read this post but i can't figured out how many stacks we need to generate $n!$ permutation or how many permutation generated by using $k$ stacks?","['permutations', 'combinatorics', 'discrete-mathematics']"
4310410,Two different definitions of monotone operator,"I want to know the relationship between the following two definitions of monotone operator: An operator $f \in \mathcal{P}(V) \to \mathcal{P}(V)$ is monotone with respect to $V$ if and only if, for all $A, B \subseteq V$ , $A \subseteq B$ implies $f(A) \subseteq f(B)$ Let $(A, \leq)$ be a partial order. An operation $f \in A \to A$ is monotone if and only if $a \leq b$ implies $f(a) \leq f(b)$ . Are these simply equivalent definitions or does one of the definitions subsume the other? If we consider $(\mathcal{P}(V), \subseteq)$ we get a partial order with $f \in \mathcal{P}(V) \to \mathcal{P}(V)$ monotone by the second definition. But then how can we go from the second definition to the first? I suppose we could do this if we could ensure that a function $f \in A \to A$ can be lifted to a function $f \in \mathcal{P}(A) \to \mathcal{P}(A)$ , which is monotone by the first definition and that the definitions are only guaranteed to be equivalent when $\leq \;=_{df}\; \subseteq$ . Is the following proof going from 2. to 1. correct? Suppose $(A, \leq)$ is a partial order and $f \in A \to A$ is monotone. Consider the function $f' \in \mathcal{P}(A) \to \mathcal{P}(A)$ such that, for all $X \subseteq A: f'X = \{f(x) \mid x \in X \}$ and suppose that $X,Y \subseteq A$ and $X \subseteq Y$ . Then $f'(X) \subseteq f'(Y)$ .","['monotone-functions', 'order-theory', 'abstract-algebra', 'discrete-mathematics', 'elementary-set-theory']"
4310417,Why $x^{n}=1$'s solution is equal to regular polygon in complex plane?,"My major is Electrical Engineering. Although I'm not majoring mathamatics, but I got one question. In my circuit theory class, I just learned Euler's formula to understand R-L-C circuit's
analysis But one day, another professor told me that you have to make regular polygon program unless
you got low score. so, I have to make regular polygon drawing program. my solution is let's find $x^{n}=1$ solution. here is my $x^{n}=1$ solution (Let me skip my prove) $x_k=e^{\frac{i(2k-2)\pi}{n}}$ (n, k is must be natural number) $(1\le k \le n, n\ge1)$ but why $x^n=1$ solution equals to drawing regular polygon?? I mean, if I solve ${x^3=1}$ equation, I got regular triangle (in complex plane) if I solve ${x^4=1}$ equation, I got regular square (in complex plane) if I solve ${x^5=1}$ equation, I got regular pentagon (in complex plane) but why? Do I have to learn Galois theory? Galois proved that over 5 dimension equation, there is no general quadratic formula. but I found general equation in some cases. why? Some people mentioned that you have to learn group theory to understand
equation. Do I have to learn group theory to understand it? Thank you for reading my noob article.","['galois-theory', 'trigonometry', 'linear-algebra']"
4310444,Understanding determinant $=0$,"I am playing around with determinants to see if I can get a better grasp of it and would appreciate some thoughts. $$A=\begin{pmatrix}a_1&b_1&c_1\\a_2&b_2&c_2\\a_3&b_3&c_3\end{pmatrix}$$ Column vectors $\vec{a},\vec{b},\vec{c}$ If det(A)=0, then $\vec{a},\vec{b},\vec{c}$ are linearly dependent and can be written $\alpha\vec{a}=\beta\vec{b}+\gamma\vec{c}$ . It also means that $A\vec{x}=0$ has a non trivial solution $$\vec{x_0}=\begin{pmatrix}\alpha\\-\beta\\-\gamma\end{pmatrix}$$ and all vectors $t\vec{x_0},\ t\in R$ Suppose we have an equation $A\vec{x}=\vec{y}$ , with $\vec{x_1}$ as a solution, then all $\vec{x}=\vec{x_1}+t\vec{x_0}$ are also solutions. This explanations makes it clear in my mind why, if there is one solution, there are infinite solutions in any dimension when det(A)=0. Suppose $\alpha\ne0$ , one of $\alpha,\beta,\gamma$ must be non zero. We could then describe $$A=\begin{pmatrix}\beta_1b_1+\gamma_1c_1&b_1&c_1\\\beta_1b_2+\gamma_1c_2&b_2&c_2\\\beta_1b_3+\gamma_3c_1&b_3&c_3\end{pmatrix}, \beta_1=\frac{\beta}{\alpha},\gamma_1=\frac{\gamma}{\alpha}$$ Then $$A\vec{x}=\vec{y}\Leftrightarrow$$ $$(\beta_1x_1+x_2)\vec{b}+(\gamma_1x_1+x_3)\vec{c}=\vec{y}$$ This is an equations system with two vectors in three dimensions describing a third, which can only lie in the plane the two vectors, $\vec{b},\vec{c}$ , span. This makes sense in two and three dimensions, but the thing I am having trouble visualising is why you need an equal number of linearly independent vectors to the number of the dimension to describe all vectors in higher than three dimensions. It makes sense, but in a ephemeral way that is not completely satisfying to me. When it comes to higher than three dimensions I start to view the equation $A\vec{x}=\vec{y}$ as an equations systems instead of geometric representations. So that is where my thoughts go. You need an equal number of equations to variables to have a possible single solution. But how to state this cleanly and intuitively? Edit: I thing the solution I am seeking is something like: if you have less vectors than the number of dimensions or if the vectors are linearly dependent, then you can not use Gaussian elimination to ever get an overly triangular matrix without zeros in the diagonal.","['matrices', 'determinant', 'linear-algebra', 'vector-spaces']"
4310453,How many subgroups does $\mathbb{Z}_{13}\times\mathbb{Z}_{13}$ have?,"How many subgroups does $\mathbb{Z}_{13}\times\mathbb{Z}_{13}$ have ? My attempt:
Firstly, we observe that the possible orders for an element of this group are $1$ and $13$ only. So, we would need to find the subgroups of order $13$ other than the group $$\mathbb{Z}_{13}\times\mathbb{Z}_{13}$$ itself and the trivial group order $1$ to determine the total number groups. Now to find the number of subgroups of order $13$ , let us first find the number of cyclic subgroups of order $13$ first (moreover, group of order $13$ is already cyclic, fortunately), let us find the number of elements of order $13 $ in $\mathbb{Z}_{13}\times\mathbb{Z}_{13}$ Suppose $o(a,b)=13$ then ${\rm lcm}(o(a),o(b))= 13$ . The possiblities are: $o(a)=1,o(b)=13$ there are $12$ such elements $o(a)=13, o(b)=1$ there are $12$ such elements $o(a)=13, o(b)=13$ there are $144$ such elements So, there are a total of $144+12+12=168$ elements of order $13$ in $\mathbb{Z}_{13}\times\mathbb{Z}_{13}$ . So, there are $\frac{168}{\phi(13)}=\frac{168}{12}=14$ cyclic subgroups of order $13$ in $\mathbb{Z}_{13}\times\mathbb{Z}_{13}$ (which are in fact the only subgroups of order $13$ ) So, we found out that there are a total of $14+2=16$ subgroups in $\mathbb{Z}_{13}\times\mathbb{Z}_{13}$ I was fortunate enough to have a group $\mathbb{Z}_{p}\times\mathbb{Z}_{p}$ where $p$ is prime so that the subgroups of order $p$ are cyclic only, so that the computation becomes easy. My question is how to tackle such direct sum problems where my p is not a prime and I have to find the number of non-cyclic subgroups ? How to find the number of subgroups (cyclic or non-cyclic) for any group of type $G_1\times G_2$ ? My focus is mainly of non-cyclic type*","['direct-product', 'cyclic-groups', 'abstract-algebra', 'group-theory', 'abelian-groups']"
4310459,Joint distribution from marginals,"I have a question about a joint distribution calculated in a paper I am reading. There are three random variable a, b and c such that $$ a,b,c \in \{+1,-1\} $$ and then the joint distribution is given by: $$ p(a,b,c) = \frac{1}{8}(1 + aE_{A} + bE_{B} + cE_{C} + abE_{AB} + acE_{AC} + bcE_{BC} + abcE_{ABC})$$ where $E_{A}$ , $E_{B}$ and $E_{C}$ are  the  single-party  marginals, $E_{AB}$ , $E_{BC}$ and $E_{AC}$ the two-party marginals, and $E_{ABC}$ is the three-body correlator. I feel like this should be something I should have seen in an introductory course on probability but I can't seem to prove it. Also if I convince myself that it's just adding up all the possible cases, I think 1 should be part of the other expected values (i.e. it is already considered in them) so I don't see the point in adding it separately.
Also would we have the same expression if the possible values were $\{+1,0\}$ (or any other set of size 2) ? I am used to the notation $E$ as the expected value, but this is totally unrelated to that and it is about marginals. Am I correct? I would be pleased to see a complete proof or a link to study this fact. It also adds: Note that the positivity of $p(a,b,c)$ implies  constraints  on  marginals,  in  particular $p(+ + +) + p(−−−) ≥ 0$ implies $$ E_{AB} + E_{AC} + E_{BC} ≥ −1.$$ which I don't understand.","['expected-value', 'marginal-distribution', 'probability-distributions', 'probability']"
4310469,"Finding $\cot(a+b)$, if $\tan^3a+2\tan^3b=6$, $\tan^4a+2\tan^4b=18$, $\tan^5a+2\tan^5b=30$, and $\tan^6a+2\tan^6b=66$","A ""Brain Teaser"" with Trigonometry: If $$\begin{align}
\tan^3 (\alpha) +2\tan^3 (\beta)&=\phantom{0}6 \\
\tan^4 (\alpha) +2\tan^4 (\beta)&=18 \\
\tan^5 (\alpha) +2\tan^5 (\beta)&=30 \\
\tan^6 (\alpha) +2\tan^6 (\beta)&=66
\end{align}$$ Then find $\cot(\alpha+\beta)$ . Answer is $3$ . I have tried to form terms of $\tan(\alpha+\beta)$ by multiplying 1st term with $\tan(\alpha) +\tan(\beta)$ and again 1st term with $\tan^2(\alpha) +\tan^2(\beta)$ to get relationship with different terms hoping to extract $\tan(\alpha) \tan(\beta)$ and cancel out terms but no use. Any suggestions???","['algebra-precalculus', 'systems-of-equations', 'trigonometry']"
4310548,Which line bundle on $A$ is the pull-back of the Poincaré bundle via a given morphism $A \to A^*$?,"Let $A$ be an Abelian variety (over an algebraically closed field of characteristic zero), and denote by $A^*$ its dual, which parameterizes degree zero line bundles on $A$ . If $Q$ is a point of $A^*$ , the corresponding degree zero line bundle on $A$ arises via pull-back of the Poincaré bundle $\mathcal{P} \to A \times A^*$ along $A \xrightarrow{\sim} A \times \{Q\} \xrightarrow{(\mathrm{Id}_A,Q)} A \times A^*$ . Question: If I have a morphism $\lambda: A \to A^*$ , I can pull-back $\mathcal{P}$ along the graph morphism $\Gamma_\lambda :A \xrightarrow{(\mathrm{Id}_A,\lambda)} A \times A^*$ . Is there any way to know what line bundle on $A$ I obtain, i.e., what point of $A^*$ corresponds to it? A similar question, possibly of secondary importance: the Poincaré bundle is itself a degree zero line bundle on $A \times A^*$ , so it should correspond to a point on $(A \times A^*)^* \cong A \times A^*$ . But which one?","['algebraic-geometry', 'abelian-varieties', 'line-bundles']"
4310584,Edgematching tiles,"Consider a 3×3 grid. Now, look at the patterns which generate 1 to 7 dots around the edges, taking into account rotations and reflections. Turns out there are 49 patterns, as seen in the set below made with index cards and a hole punch. By hand, I managed to lay out a 7×7 grid where all edges match. Is it possible to do edge-matching on a cylinder or torus with this set?","['recreational-mathematics', 'puzzle', 'combinatorics', 'tiling']"
4310602,Is the closed form of $\int_0^1 \frac{x\ln^a(1+x)}{1+x^2}dx$ known in the literature?,"We know how hard these integrals $$\int_0^1 \frac{x\ln(1+x)}{1+x^2}dx;
\int_0^1 \frac{x\ln^2(1+x)}{1+x^2}dx;
\int_0^1 \frac{x\ln^3(1+x)}{1+x^2}dx;
...$$ can be. So I decided to come up with a generalization and I succeeded: With $x=1/(1+y)$ , we have $$\int_0^1 \frac{y\ln^a(1+y)}{1+y^2}dy=(-1)^a\left[\int_{1/2}^1\frac{\ln^a(x)}{x}dx+\int_{1/2}^1\frac{1-2x}{x^2+(1-x)^2}\ln^a(x)dx\right]$$ $$=(-1)^a\left[\frac{(-1)^a}{a+1}\ln^{a+1}(2)+\Re\int_{1/2}^1\frac{1+i}{1-(1+i)x}\ln^a(x)dx\right].$$ For the remaining integral, we need to find $\int_{1/2}^1\frac{z}{1-zx}\ln^a(x)dx:$ $$\int_{1/2}^1\frac{z}{1-zx}\ln^a(x)dx=\int_{0}^1\frac{z}{1-zx}\ln^a(x)dx-\underbrace{\int_0^{1/2}\frac{z}{1-zx}\ln^a(x)dx}_{x=y/2}$$ $$=(-a)^aa!\,\text{Li}_{a+1}(z)-\int_0^{1}\frac{\frac{z}2}{1-\frac{z}2y}\ln^a(y/2)dy$$ $$=(-a)^aa!\,\text{Li}_{a+1}(z)-\sum_{k=0}^a{a\choose k}(-\ln(2))^{a-k}\int_0^1 \frac{\frac{z}2}{1-\frac{z}2y}\ln^k(y)dy$$ $$=(-a)^aa!\,\text{Li}_{a+1}(z)-\sum_{k=0}^a{a\choose k}(-\ln(2))^{a-k}(-1)^kk!\,\text{Li}_{k+1}\left(\frac{z}2\right)$$ $$=(-1)^aa!\,\left[\text{Li}_{a+1}(z)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+1}\left(\frac{z}2\right)\right].\quad\quad (*)$$ Using this integral, we get $$\int_0^1\frac{x\ln^a(1+x)}{1+x^2}dx=\frac{\ln^{a+1}(2)}{a+1}+a!\,\Re\left[\text{Li}_{a+1}(1+i)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+1}\left(\frac{1+i}2\right)\right].$$ Question : Is this generalization known in the mathematical literature? and if so, may I get the reference? Thanks, Bonus : Following the same approach above, using $\frac{1}{x^2+(1-x)^2}=\Im\,\frac{1+i}{1-(1+i)x}$ , also gives $$\int_0^1\frac{\ln^a(1+x)}{1+x^2}dx=a!\,\Im\left[\text{Li}_{a+1}(1+i)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+1}\left(\frac{1+i}2\right)\right].$$ Also if we divide the identity in $(*)$ by $z$ then integrate $\int_0^1$ w.r.t $z$ , we obtain $$\int_{1/2}^1\frac{\ln(1-x)\ln^a(x)}{x}dx=(-1)^{a-1}a!\,\left[\zeta(a+2)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+2}\left(\frac{1}2\right)\right].$$","['integration', 'reference-request', 'real-analysis', 'calculus', 'polylogarithm']"
4310678,Random area covered by disks: what do I need to prove normal covergence?,"Consider a square $\mathcal{L}$ of side $L$ , a Poisson point process $\mathcal{P} \subset \mathcal{L}$ of expected $n$ points, and unit disks with their centres at the points of $\mathcal{P}$ . The disks can overlap with the boundary $\partial \mathcal{L}$ . Let the total area of the square $\mathcal{L}$ covered by the union of the disks be the random variable $\mathcal{A}$ . In the `dense' limit where $n \to \infty$ at fixed $L$ , consider the random uncovered area $L^2 - A$ . Does this converge somehow to a Gaussian distribution? The area will be small, but random, and so a sort of central limit theorem may well apply. I imagine I could bound the deviation of $f_{L^2-A}(x)$ from that of the corresponding Gaussian distribution with the same mean and variance as $L^2 - A$ . What would I need to prove this? Just bounds on some of the moments of $L^2 - A$ ? Below is an image of the square. The area $L^2 - A$ is shown in white.","['geometry', 'probability', 'normal-distribution']"
4310760,How to generate Gaussian random variables with given covariance and autocorrelation structure,"Given a pos. def. covariance matrix $\Sigma\in \mathbb{R}^{d\times d}$ and first order autocorrelations $a\in (-1,1)^d$ for each dimension, I would like to generate Gaussian random variables in time $X_t \sim N(0,\Sigma)$ such that in each dimension $i$ the lag-1-autocorrelation $a_i$ is met. Is there a simple characterization which combinations of $\Sigma$ and $a$ are valid? How could I generate such a process efficiently? I'd prefer a solution using the following VAR(1) model: $$ X_t = A \cdot X_{t-1} + \varepsilon_t, $$ where $\varepsilon_t \sim N(0,\Sigma_\varepsilon)$ iid and $A= \mathrm{diag}(a)$ . Then by $X_t = \sum_{k=0}^\infty A^k \varepsilon_{t-k}$ , it would hold that $$X_t\sim N\left(0, \sum_{k=0}^\infty A^k \Sigma_\varepsilon A^k \right)= N\left(0,\Bigl(\frac{(\Sigma_{\varepsilon})_{ij}}{1-a_i a_j} \Bigr)_{ij}\right).~$$ Now if the next step is well-defined, it holds that $(\Sigma_\varepsilon)_{ij} = \Sigma_{ij} (1-a_i a_j)$ . Unfortunately $\Sigma$ and $a$ may be incompatible such that the resulting $\Sigma_\varepsilon$ is not pos. semidefinite. So can anyone characterize the conditions on $\Sigma$ and $A$ (or better on $\Sigma$ and $a$ ) such that there exists a positive definite covariance matrix $\Sigma_\varepsilon$ that allows me to generate the corresponding VAR-process? For example if $A$ and $\Sigma$ share the same eigenvectors then there exists a pos. semidefinite matrix $\Sigma_\varepsilon$ for the VAR(1)-process above. But my particular setting is the covariance matrix of $d$ points on the sphere $S^2$ of an isotropic random field, hence the eigenvector basis will not be the standard basis and $A$ not diagonal (as preferred). Another idea, instead of VAR(1), is to use a spatio-temporal covariance function for $d$ grid points and time such that the autocorrelation depends on the location. Which combinations of $\Sigma$ and $a$ are allowed now and how do I find a plausible covariance function?","['statistics', 'probability-distributions', 'linear-algebra', 'time-series', 'probability']"
4310774,"Events in a measure preserving system satisfying $\mu (A \cap T^{-n}B) = \mu(A)\mu(B)$ for all $n$ large imply $\mu(A) \in \{0,1\}$","Let $(X, \mathscr B_X, \mu, T)$ be a measure-preserving system such that the condition in the title is satisfied for all $n$ larger than $N \in \Bbb N$ . How can we conclude that $A$ is trivial? Any help\hints are appreciated.","['ergodic-theory', 'probability']"
4310801,Integral of inverse Laplace transform,"Let $f$ be some integrable function defined on $[0,\infty)$ which is not analytically tractable, and $\hat{f}$ is its Laplace transform which can be expressed in some analytic form, something like $\hat{f}(s)=1-\frac{1}{1+e^{-s}}$ . Now I want to get some approximation of the improper integral $\int_0^T f(t)\, dt$ with some large $T$ by using the Laplace transform $\hat{f}$ (because $\hat{f}$ is analytically tractable, it is easier to handle with it). Then the Bromwich integral gives \begin{align*}
\int_0^T f(t)\, dt &= \int_0^T \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{iyt} \hat{f}(iy)\, dy\, dt   &\text{(by Bromwich)}\\
&= \int_{-\infty}^{\infty} \frac{1}{2\pi} \underbrace{\int_0^T e^{iyt}\,dt}_{=(e^{iyT}-1)/(iy)} \hat{f}(iy)\,dy &\text{(by Fubini)} \\
&= \underbrace{\frac{1}{2\pi}\int_{-\infty}^{\infty} \frac{e^{iyT}\hat{f}(iy)}{iy}\,dy}_{(I)} - \underbrace{\frac{1}{2\pi} \int_{-\infty}^{\infty} \frac{\hat{f}(iy)}{iy}\, dy}_{(II)}.
\end{align*} Then my question is the following. Question . $\;$ Since $f$ is integrable, $\int_0^{\infty}f(t)\, dt = \hat{f}(0)$ is well-defined, so that $(I) + (II) \rightarrow \hat{f}(0)$ as $T \rightarrow \infty$ .
Then in terms of $\hat{f}(0)$ , what is the limit value of $(I)$ as $T \rightarrow \infty$ ? (Is it $\hat{f}(0)$ or not?) and what is $(II)$ ? For answering this, Here is what I have done . Since the residue of $\hat{f}(z)/z$ at $z=0$ is $\hat{f}(0)$ , I suspect that somehow I must use residue theorem to $(I)$ or $(II)$ with proper contour. Because of Bromwich integral restrict to integrate the right half-plane, $y$ must have nonnegative real part so that the contour of $(I)$ and $(II)$ may pass the right-side of the singularity $z=0$ . Here is the contour image: By the Riemmann-Lebesgue lemma , $\hat{f}(z) \rightarrow 0$ as $|z| \rightarrow \infty$ , so the contour integration of $\hat{f}(z)/z$ with infinite radius arc (the dashed line) is always zero.
This means that $(II)$ is zero so that $(I) \rightarrow \hat{f}(0)$ automatically.
But if this is the case, I'm not sure how to show that $(I)$ goes to $\hat{f}(0)$ by direct calculation of $(I)$ . Thanks,","['complex-analysis', 'contour-integration', 'fourier-analysis', 'laplace-transform']"
4310803,Proving that $3^{(3^4)}>4^{(4^3)}$ without a calculator,"Is there a slick elementary way of proving that $3^{(3^4)}>4^{(4^3)}$ without using a calculator? Here is what I was thinking: $$4^4=256>243=3^5,$$ hence $$4^{4^3}=4^{64}=(4^4)^{16}=(3^5)^{16}\cdot\left(\dfrac{256}{243}\right)^{16}=3^{80}\cdot\left(\dfrac{256}{243}\right)^{16}<3^{81}=3^{3^4}\,.$$ It is possible to prove that $\left(\dfrac{256}{243}\right)^{16}<3$ without a calculator by making a comparison such as $\dfrac{256}{243}=1+\dfrac{13}{243}<1+\dfrac{15}{240}=\dfrac{17}{16}$ and $\left(1+\dfrac{1}{16}\right)^{16}<e$ so $\left(\dfrac{256}{243}\right)^{16}<\left(1+\dfrac{1}{16}\right)^{16}<e<3$ . Is there a more elegant elementary proof? Ideally I'm looking for a proof that doesn't rely on calculus. Source of problem: I made up this question but it is inspired by a similar question that appeared in the British Mathematical Olympiad round 1 in 2014.","['contest-math', 'elementary-number-theory', 'algebra-precalculus', 'number-comparison']"
4310816,How to prove $\lfloor{\pi}\rfloor+\lfloor{\pi^e}\rfloor=\lfloor{e}\rfloor+\lfloor{e^\pi}\rfloor$?,"How to prove $\lfloor{\pi}\rfloor+\lfloor{\pi^e}\rfloor=\lfloor{e}\rfloor+\lfloor{e^\pi}\rfloor$ ?(without using the calculator) $e$ is Euler's number.
I tried to solve this question. But I could only get to the following inequality: $e^\pi >\pi^e$ . I got this inequality by deriving from the function $f(x)=e^x-x^e$ . so $f'(x) \geq 0$ for $x > e$ . This means that $f$ is an increasing function, and $f(e) = 0$ .Thus, $f(\pi) > 0$ , so $e^{\pi} - \pi^{e} > 0$ . This implies that $e^{\pi}$ is greater than $\pi^{e}$ .","['calculus', 'exponential-function', 'algebra-precalculus']"
4310889,transitive relation question,"I need to show if the following relation is transitive : $$
R\subseteq \mathcal{P}(\mathbb{N})\times\mathcal{P}(\mathbb{N}) \space \text{with} \space XRY \space :\Leftrightarrow \exists x \in\mathbb{N}:\space x\in X \space \land \space x\in Y
$$ The answer says that it is not transitive because there is: $$ \{1,2\}R\{2,3\}\space \text{and} \space \{2,3\}R\{3,4\} \space \text{but}\space \space \text{not} \space
 \{1,2\} \cap \{3,4\}=\emptyset $$ what does intersection have to do with it?","['elementary-set-theory', 'relations']"
4310894,Measure-theoretic proof of the inverse Fourier transform,"I would like to prove this statement: Proposition. (Inverse Fourier transform) Let $u \in L^1_\mathbb{R}(\lambda^n)$ and let us define its Fourier transform as $$\hat{u}(\xi):=(2\pi)^{-n}\int_{\mathbb{R}^n} u(x)e^{-i\langle x,\xi\rangle}dx,\,\xi \in \mathbb{R}^n$$ then if $\hat{u} \in L^1_\mathbb{C}(\lambda^n)$ we have the inversion formula $$u(x)=\int_{\mathbb{R}^n} \hat{u}(\xi)e^{i\langle x,\xi\rangle}d\xi$$ To prove this I would like to use the following lemma: Lemma. Let $\mu$ be a finite measure on $(\mathbb{R}^n,\mathcal{B}(\mathbb{R}^n))$ and assume $$\hat{\mu}(\xi):=(2\pi)^{-n}\int_{\mathbb{R}^n}e^{-i\langle x,\xi\rangle}\mu(dx)\in L^1_\mathbb{C}(\lambda^n)$$ in this case, $\mu(dx)=u(x)dx$ where $$u(x)=\int_{\mathbb{R}^n}\hat{\mu}(\xi)e^{i\langle x,\xi\rangle}d\xi$$ I thought about decomposing $u\in L^1_\mathbb{R}(\lambda^n)$ in positive and negative parts, yielding $u=u^+-u^-$ . As $u^\pm\geq 0,\,u^\pm \in L^1_\mathbb{R}(\lambda^n)$ , we can construct the finite measures $\mu^\pm(dx)=u^\pm(x)dx$ . Assume $\widehat{\mu^\pm}\in L^1_\mathbb{C}(\lambda^n)$ . Using the above lemma: $$u(x)=\int_{\mathbb{R}^n} (\widehat{\mu^+}(\xi)-\widehat{\mu^-}(\xi))e^{i\langle x,\xi\rangle}d\xi$$ My idea is to show $\widehat{\mu^+}(\xi)-\widehat{\mu^-}(\xi)=\hat{u}(\xi)$ : $$\begin{aligned}\widehat{\mu^+}(\xi)-\widehat{\mu^-}(\xi)&=(2\pi)^{-n}\bigg(\int_{\mathbb{R}^n} e^{-i\langle x,\xi\rangle}\mu^+(dx)-\int_{\mathbb{R}^n} e^{-i\langle x,\xi\rangle}\mu^-(dx)\bigg)=\\
&=(2\pi)^{-n}\bigg(\int_{\mathbb{R}^n} e^{-i\langle x,\xi\rangle}u^+(x)dx-\int_{\mathbb{R}^n} e^{-i\langle x,\xi\rangle}u^-(x)dx\bigg)=\\
&=(2\pi)^{-n}\bigg(\int_{\mathbb{R}^n} e^{-i\langle x,\xi\rangle}(u^+(x)-u^-(x))dx\bigg)=\hat{u}(\xi)\end{aligned}$$ so assuming $\widehat{\mu^\pm}\in L^1_\mathbb{C}(\lambda^n)$ is equivalent to assuming $\hat{u}\in L^1_\mathbb{C}(\lambda^n)$ . Is this approach correct, or did I miss something? Thank you very much for your help.","['measure-theory', 'lebesgue-integral', 'fourier-transform', 'real-analysis']"
4310923,Does the empty set permit zero or one equivalence classes?,"Let $\sim$ be an equivalence relation on $\varnothing$ (it must in fact be the empty relation). Does $\varnothing\big/\sim$ have no elements, or one, namely $\varnothing$ ? Put differently, does $\varnothing$ get partioned into one equivalence class, namely $\varnothing,$ or no equivalence classes, because equivalence classes are defined relative to elements of the original set, of which there are none? I think the answer is the latter, i.e., there are no equivalence classes, because equivalence classes cannot be empty by definition. But I must admit, even after checking the definitions , I'm a bit confused.","['elementary-set-theory', 'equivalence-relations']"
4310932,What's so special about square root cancellation?,"I am reading a lecture about analytic number theory and I found a statement I do not quite understand. We were studying a sum and got the result $$\sum_{n\leq x} n^{-it} = O(x^{1/2}\vert t\vert^\epsilon)$$ for $\vert t\vert \geq x^{1/2}$ . The author concluded, that because of this square root cancellation, ""the functions $n^{-it}$ , $1\leq n\leq x$ behave like independent identically distributed variables provided $\vert t\vert \geq x^{1/2}$ "". In another context with the Riemann Hypothesis I have read, that the hypothesis is equivalent to $$\pi(x)=\int_2^x\frac{dt}{\log t} + O(x^{1/2+\epsilon})$$ for all $\epsilon>0$ and that this error term would imply that the primes would be distributed ""as uniformly as possible"" (this is just a literal translation since the book is not in english, but I think it is clear what I mean). However, I don't understand how these $x^{1/2}$ error terms (namely the square root cancellation) implies these statistical statements. What is so special with square root cancellation? The statement about the prime number theorem was just an example and this is not a questing about analytic number theory, but a question about these statistical conclusion. I'd appreciate any explanation and any help on this!","['analytic-number-theory', 'statistics', 'real-analysis']"
4310995,"If $f(a)=f(b)$ then $\forall\alpha$, $f(x+\alpha)=f(x)$ for some $x$","Given that $f$ is continuous in $[a,b]$ with $f(a)=f(b)$ . Prove or disprove: given any $\alpha\in[0,{b-a\over2}]$ , there exists some $x\in[a,b]$ such that $f(x+\alpha)=f(x)$ . If this were true then I'd suppose we prove by contradiction. Suppose $f(x)\ne f(x+\alpha_0)$ for some $\alpha_0\in[0,{b-a\over2}]$ for all $x$ , then since $f$ is continuous, suppose w.l.o.g. $f(x+\alpha_0)-f(x)>0,\forall x\in[a,b]$ . How should I proceed?","['continuity', 'analysis', 'real-analysis']"
4310998,How to solve $3\sec(x)-2\cot(x)>0$ (trigonometric inequality)?,"I am struggling to find the solution, here is what I already tried (it's for a pre-calculus class so no calculus): $3\sec(x)-2\cot(x)=0 \rightarrow 3(\frac{1}{\cos(x)})=2\left(\frac{\cos(x)}{\sin(x)}\right) \rightarrow 3\sin(x)=2\cos(x)^2 \rightarrow 3\sqrt{1-\cos(x)^2}=2\cos(x)^2 \rightarrow 9(1-\cos(x)^2)=4\cos(x)^4 \rightarrow 4\cos(x)^4+9\cos(x)^2-9=0$ Then I found the roots ( $\pm \sqrt{3}/2$ ) and the respective $rad$ values for $x$ ( $\frac{\pi}{6},\frac{5\pi}{6},\frac{11\pi}{6},\frac{7\pi}{6})$ , but the problem is that when I graph it I find that the functions are different, although they share the roots (but cosine one has more roots in between!) and some patterns (maximum/minimum values and the roots of the cosine function seem to have some relation to when the original one is positive or negative). Did I do the transformations right? I though they were supposed to be equal at least on the roots, why are they related in such a weird way? But anyway, I think my biggest problem right now is that I have no intuition for associating the roots to the original function so that I know when its positive or negative. Edit: The solution set is for all $\mathbb{R}$ . Edit 2 : Thank you all. So I did it using $\cos(x)^2=1-\sin(x)^2$ and now the roots are right( $\frac{\pi}{6},\frac{5\pi}{6}$ ). My only problem now is associating this with the original function. I don't know how to aproach it. Looking at the graph, the tangent and the secant parts of the function seem to be independent for some reason.","['algebra-precalculus', 'trigonometry']"
4311014,Fubini's theorem for conditional measures,"I have an integration that looks like: \begin{align}\label{eq1}\tag{1}
\int_{f \in F} \left[\int_{x \in \mathbb{R}} \chi_{\{x \in A\}} \mathrm{d} \gamma(x|f)\right] \mathrm{d} \mu(f),
\end{align} where $F$ is some interval, $\gamma(\cdot | f)$ is a distinct probability measure supported on $\mathbb{R}$ for all $f \in F$ , $\mu$ is a probability measure on $F$ , and $\chi$ is the indicator. I would like to change the order of integration, but this doesn't look feasible due to the $\color{red}{\text{red}}$ term \begin{align}
\int_{x \in \mathbb{R}} \left[ \int_{f \in F}  \chi_{\{x \in A\}} \mathrm{d} \mu(f) \right] \color{red}{\mathrm{d} \gamma(x|f)}.
\end{align} Hence, I write the integration \eqref{eq1} by using an additional index: \begin{align}
& \int_{f \in F} \left[\color{blue}{\int_{(f',x)\in F \times \mathbb{R}} \chi_{\{x \in A\}}\chi_{\{ f = f' \}} \mathrm{d} \gamma(x|f)}\right] \mathrm{d} \mu(f)\label{eq2}\tag{2} \\
\iff &\int_{(f',x)\in F \times \mathbb{R}} \left[\int_{f \in F}  \chi_{\{x \in A\}}\chi_{\{ f = f' \}} \mathrm{d} \mu(f) \right] \mathrm{d} \gamma(x|f')\label{eq3}\tag{3}
\end{align} I was wondering if equation~\eqref{eq3} is correct, which depends on the correctness of the $\color{blue}{\text{blue}}$ part in~\eqref{eq2}.","['integration', 'measure-theory', 'simple-functions', 'fubini-tonelli-theorems']"
4311037,Concrete realization of quotient $C^*$-algebra,"Let $A$ be a $C^*$ -subalgebra of the bounded operators $B(H)$ on some Hilbert space $H$ . Let $J$ be a proper closed $2$ -sided ideal of $A$ . Then one knows that $A/J$ is another $C^*$ -algebra. By the Gelfand-Naimark theorem, one also knows that $A/J$ is $*$ -isomorphic to a $C^*$ -subalgebra of $B(H')$ for some Hilbert space $H'$ . Question: Is there a ""natural"" choice of $H'$ ? This seems like a natural and basic question, but having looked around a bit, it seems that an answer may not be straightforward. In particular, in the case where $A=B(H)$ and $J=K(H)$ for $H$ separable, the Gelfand-Naimark-Segal construction gives a non-separable example of $H'$ . But I would be interested if there are any references where this is discussed further.","['operator-algebras', 'operator-theory', 'hilbert-spaces', 'functional-analysis', 'quotient-spaces']"
4311047,"Value of $b$ in smallest triplet of perfect squares $(a,b,c)$ where $a+5k=b, b+5k=c.$","Find the value of $b$ in the smallest triplet of perfect squares $(a,b,c)$ such that $a+5k=b$ and $b+5k=c$ for positive integers $a,b,c,k.$ I started by defining $a=a_1^2, b=b_1^2, c=c_1^2$ for convenience as they are perfect squares. Then, we know that $$a_1^2=a_1^2, \;(a_1^2+5k)=b_1^2, \;(a_1^2+10k)=c_1^2.$$ I'm not completely sure on how to continue from here, but I assume we have to solve the equaions in the problem and then try to minimize $b$ from there. May I have some help? Thanks in advance. When I say smallest triplet, I mean a triplet that has its value of $b$ as minimal as possible.","['algebra-precalculus', 'square-numbers']"
4311123,Is this graph a Reuleaux triangle?,"Given the following function: $f(\theta) =
\begin{cases}
\cos(\theta) + \omega\sin(\theta),  & \text{if $0\le\theta<\frac{\pi}{2}$} \\
\omega\cos(\theta-\frac{\pi}{2}) + \omega^2\sin(\theta-\frac{\pi}{2}),  & \text{if $\frac{\pi}{2}\le\theta<\pi$} \\
\omega^2\cos(\theta-\pi) + \sin(\theta-\pi),  & \text{if $\pi\le\theta<\frac{3\pi}{2}$}
\end{cases}$ where $\omega=-\frac{1}{2} + i\frac{\sqrt{3}}{2}$ , is the graph of the function a Reuleaux triangle? If so, how does one prove it? I tried graphing it and it sure looks like one: Edit: A huge apology to everyone. The original function I wrote had a gigantic mistake because I misinterpreted the output of the program I wrote. I've already fixed it above. As you can see, the actual function I graphed has a weird domain of just $0\le\theta\lt3\pi/2$ , which is probably why I got confused. Either way, it turns out that dxiv's conclusion is still correct, so I accepted it.","['geometry', 'complex-numbers']"
4311269,What is the logic behind the use of the harmonic mean?,"I'm a self-taught student in mathematics, and I only began self-study about half a year ago. I was thinking about the harmonic mean, and the logic behind its use.
Firstly, let's begin with the arithmetic mean which equals the sum of the set of numbers divided by the number of numbers.
For example: say you have a set $(4, 3, 2)$ , and you want the mean. You simply divide: $\frac{4+3+2}{3}$ . You get 3 in this case. Thus, in a way, the logic behind is to generalize it. It's to take all of the numbers, and distribute them equally to these 3 ""slots"" if that make sense. In doing this, you ""generalize"" all the numbers to one single number if that makes sense. The geometric mean is easy to understand too. Imagine you have a set of numbers which are constantly being multiplied.
Say you have a set whose first number is multiplied by 1.3, 1.2, and 1.5.
So, starting with 4: $(4, 8, 12, 36)$ .
We can ""generalize"" this as we did with the arithmetic mean, because: $4*8*12*36=x^4$ .
So, we can ""generalize"" those numbers to the variable x like we did with the arithmetic mean.
In the words of a Stackoverflow user by the name of Domagoj Pandzha (who I thank for allowing me to rationalize the geometric mean): ""We are, essentially looking for a value which, when multiplied by itself 3 times (the number of percentages per respective years), gives the same final value as if we simply repeatedly multiplied each year with the respective 1.3, 1.4 and 1.5."" So, $\sqrt[4]{x^4}$ = $\sqrt[4]{4*8*12*36}$ Makes sense to me to use it, since we're trying to average out the products here. But the harmonic mean I struggle with in my mind. What is the logic behind its use? Thanks.","['algebra-precalculus', 'statistics']"
4311291,$AB=Y = y + y'(x) (X-x)$ $PG= Y = y - \frac{1}{y'(x)} (X-x)$ Find the curves such that $PM=MG$.,"$C$ is a curve of $y(x)$ function on $XY$ , there is point $P:=(x,y)$ .
A parallel line and perpendicular line of C pass through the point $P$ . $AB=Y = y + y'(x) (X-x)$ $PG= Y = y - \frac{1}{y'(x)} (X-x)$ Find the curves such that $PM=MG$ .
I need to find the formula of the curves family and don't know how to do it. My solution : I will find $M$ and $G$ : $0=y - \frac{1}{y'(x)} (X-x) \implies X_G=yy'(x)+x \implies G=(yy'(x)+x,0).$ $Y_M = y - \frac{1}{y'(x)} (0-x) \implies Y_M=y+\frac{x}{y'(x)} \implies M=(0,y+\frac{x}{y'(x)}).$ Find $PM : \sqrt{(x-0)^2+(y-(y+\frac{x}{y'(x)})^2} = \sqrt{x^2+(\frac{x}{y'(x)})^2}$ Find $MG$ : $\sqrt{(0-(yy'(x)+x))^2+(y+\frac{x}{y'(x)}-0)^2}$ $PM=MG : \sqrt{x^2+(\frac{x}{y'(x)})^2}= \sqrt{(0-(yy'(x)+x))^2+(y+\frac{x}{y'(x)}-0)^2}$ $ \implies x^2+(\frac{x}{y'(x)})^2=(y\cdot y'(x)+x)^2+(y+\frac{x}{y'(x))}^2$ $ \implies x^2 + \frac{x^2}{(y'(x))^2}=y^2\cdot (y'(x))^2+2y\cdot y'(x)\cdot x+x^2+y^2+\frac{2xy}{y'(x)}+\frac{x^2}{(y'(x))^2} $ $\implies 0=y^2\cdot y'(x)+2y\cdot y'(x)\cdot x+y^2+\frac{2xy}{y'(x)} \implies y'(x)=\sqrt{\frac{x}{-y-x}}.$ Is this part of my solution correct?",['ordinary-differential-equations']
4311292,"How many integers not divisible by $7$ are there between $[1-8888]$ using the set $\{0,1,2,3,4,5,6,7,8\}$","How many integers not divisible by $7$ are there between $[1-8888]$ using the set $\{0,1,2,3,4,5,6,7,8\}$ .For example $49$ cannot be used because of $9$ . $\mathbf{\text{My attempt:}}$ I said that if we cannot use $9$ , then we can think the question like it was written in base $9$ . Then, if I subtract the number of integers divisible by $7$ from the total, we can reach the answer. Total number of integers : $(8888)_9 - (1)_9=(8887)_9=6559$ The number of integers divisible by $7$ : $$\frac{(8887)_9 - (7)_9}{(7)_9}+(1)_9 = (1251)_9=937$$ So my answer is $6559-937=5622$ Am I correct? If not, can you correct me or give another approach? $\mathbf{\text{EDIT:}}$ I realize that i made a mistake in calculating the numbers of integers in $[1-8888]$ that do not contain integer $9$ . The right calculation : $$\frac{(8888)_9 - (1)_9}{(1)_9} + (1)_9 =6560 $$ Now , the answer is $6560 -937 =5623$ , but it still contradict with the answer given by @HennoBrandsma , so i want to understand what my mistake is.","['contest-math', 'elementary-number-theory', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
4311316,Why isn't this a counterexample of Banach-Steinhaus theorem?,"The theorem from Wikipedia is as follows Let $X$ be Banach space, $Y$ be a normed space and $F$ be family of linear bounded operators $f:X \to Y$ such that $\forall x \in X \sup_{f \in F} \|f(x)\|_Y < \infty$ . Then $$
\sup_{f \in F ,\ \|x\| = 1} \| f(x)\|_Y = \sup_{f \in F} \| f\| < \infty
$$ I thought the following ""counterexample"" Let $X = Y$ be a Banach space. It normalised linear (Hamel) basis $A = \{e_\alpha\}_{\alpha\in J}$ is uncountble ( $|A| \ge 2^{\aleph_0}$ ) and let $J = \{e_j\}_{j \in \mathbb N} \subset A$ be a (countable) sequence thereof. Now let $\{f_n\}_{n\in \mathbb N}$ the countable family of operators, such that $$
f_n(e_\alpha) = 
\begin{cases}
e_\alpha & e_\alpha \in A\backslash J \\
\frac{1}{\frac{1}{j+1} + \frac{1}{n+1}}e_j & e_\alpha = e_j \in J.
\end{cases}
$$ i.e. all what an operator does here is that it scales a countably infinite basis vectors by bounded sequence, from this I believe that these operators are bounded with $\|f_n\| = n+1$ (which may turn out not to be the case, as the comments below suggest). Considering $$
\|f_n(e_\alpha)\| = 
\begin{cases}
1 & e_\alpha \in A\backslash J \\
\frac{1}{\frac{1}{j+1} + \frac{1}{n+1}} & e_\alpha = e_j \in J
\end{cases}
$$ we find $\sup_{n \in \mathbb N}\|f_n(e_\alpha)\| = 1$ or $j+1$ (depending on $\alpha$ ), I thought that is enough to conclude, (since $x = \sum x_k e_k$ ) that condition $$
\sup_{n \in \mathbb N}\|f_n(x)\| \le \sup_{n \in \mathbb N} \sum_k |x_k| \|f_n(e_k)\| < \infty
$$ for every $x \in X$ is statisfied. Now if we take the sequence $(\|f_n\|)_{n \in \mathbb N}$ it is not bounded. What is the mistake ? Added: to disprove this is a counerexample, I think, it is sufficient to show that in any norm on $X$ which has the Hamel basis $\{e_\alpha\}_{\alpha \in A}$ normalised we have (infinitely many elements in) $\{f_n\}_{n \in \mathbb N}$ not bounded. Otherwise there is a norm such that (most of) these maps are bounded and the sequence $(\|f_n\|)_{n \in \mathbb N}$ is not bounded.","['hamel-basis', 'banach-spaces', 'functional-analysis', 'examples-counterexamples']"
4311318,Prove or disprove a Hölder type bound on antiderivative,"Let $f:(0,1]\rightarrow \mathbb R_{\ge0}$ be a continuous function such that for every $t_0\in [0,1]$ , $$\int_0^{t_0} [f(t)-f(t_0)]\,dt\le c_1 t_0^\gamma$$ for $0<\gamma<1$ and $c_1>0$ . Does it follow that $$\int_0^{t_0} f(t)dt\le c_2t_0^\gamma$$ for some $c_2>0$ ? I know that this fails to hold if for example $\gamma=1$ we can have $f(t)=-\log t$ but I have no idea how to prove this for $0<\gamma<1$ . Any help is appreciated.","['integration', 'calculus', 'functional-analysis', 'upper-lower-bounds']"
