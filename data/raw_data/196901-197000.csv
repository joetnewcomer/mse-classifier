question_id,title,body,tags
3795656,Base-Exponent Invariants,"A sum of powers is called a base-exponent invariant if its value does not change if each base and exponent are switched.  The simplest example is $2^4$ , which of course is equal to $4^2$ .  Another base-exponent invariant is $$2^{5} + 2^{7} + 2^{9} + 5^{3} + 5^{4}=5^{2} + 7^{2} + 9^{2} + 3^{5} + 4^{5}$$ There are lots of other examples with $5$ summands known. ( See answer to question $4$ .) We are interested in base-exponent invariants in which all the bases and exponents are integers at least $2$ , and where no power appears more than once, even after the bases and exponents have been switched.  Is there a sum of $2$ , $3$ , or $4$ powers which is a base-exponent invariant? I'm also interested in a general sum of powers expression involving a variable that still remains true if the bases and exponents are switched, leading to infinitely many examples of a given length.  Dean Hickerson found this expression involving a sum of $20$ powers which works: $$ 2^{2n} + 2^{2n+8}+ 2^{2n+16} + 2^{2n+32} + 2^{2n+34} + 4^{n+1} + 4^{n+2} + 4^{n+10} + 4^{n+14} + 4^{n+18} + n^{4} + (n+4)^{4} + (n+8)^{4} + (n+16)^{4} + (n+17)^{4} + (2n+2)^{2} + (2n+4)^{2} + (2n+20)^{2} + (2n+28)^{2} + (2n+36)^{2} $$ Is there such an expression involving fewer than $20$ powers?","['exponential-diophantine-equations', 'number-theory', 'recreational-mathematics', 'exponentiation']"
3795690,"If two zero sets are homeomorphic, is the ring of polynomials over the sets also homeomorphic?","I'm very sorry to have made this obvious mistake, I should have asked the ideals also be prime. I have fixed this. Let $R$ be the ring of complex polynomials in $n$ variables and let $I$ and $J$ be prime ideals of $R$ . Consider $V(I)$ and $V(J)$ , the zero set of the ideals, that is, the set of points sent to zero by all polynomials in the ideal. Gives each of these sets, $V(I)$ and $V(J)$ , the subspace topology induced by the usual topology on $\mathbb{C}^n$ and then assume $V(I)$ and $V(J)$ are homeomorphic. Now consider the rings $R/I$ and $R/J$ . Must they be isomorphic as rings? If so does this result have a name and can you please provide a proof or point to where I can find a proof? If not I would like a counter example. To help explain the question further, here is a concrete example: Say $R$ is the ring of complex polynomials in two variables and say we have the ideals generated by the polynomials $x^2+y^2-1$ and $x^2+y^2-2$ . The two topologies are homeomorphic in this case and also the quotient rings are ring-isomorphic. Must it always be the case that homeomorphism implies ring-isomorphism? If not, is it the case that a stronger condition, like diffeomorphism is instead required?","['ring-theory', 'abstract-algebra']"
3795722,Homomorphism of $k$-algebras induce homomorphism of maximal spectrum,"For $k$ a algebraically closed field, we define an affine $k$ -algebra to be a finitely generated $k$ -algebra that is reduced (i.e. $\sqrt{(0)} = (0)$ ). For an affinie $k$ -algebra $A$ , we define $\operatorname{specm} A$ to be the set of maximal ideals.  Then we have the following proposition: If $\alpha: A \rightarrow B$ is a homomorphism of affine $k$ -algebras, then $\alpha$ induces a continuous map of topological spaces $\phi: \operatorname{specm} B \rightarrow \operatorname{specm} A$ where for a maximal ideal $m \subset B$ , $$
\phi(m) = \alpha^{-1}(m).
$$ I am having trouble understanding the first half of the proof which goes as follows: Proof: For any $h \in A$ , $\alpha(h)$ is invertible in $B_{\alpha(h)}$ (which denotes the localization of $B$ at $\alpha(h)$ ), so the homomorphism $A \rightarrow B \rightarrow B_{\alpha(h)}$ extends to a homomorphism $$
\frac{g}{h^m} \rightarrow \frac{\alpha(g)}{\alpha(h)^m}: A_h \rightarrow B_{\alpha(h)}
$$ For any maximal ideal $n \in B$ , $m = \alpha^{-1}(n)$ is maximal in $A$ because $A/m \rightarrow B/n$ is an injective map of k-algebras which implies that $A/m$ is $k$ . I do not believe step 1 is used anywhere else in the proof so it seems that step 2 must be a consequence of step 1.  Could someone explain how?  In particular, is step 1 the reason that the map in step 2 is injective?  Thank you!","['algebraic-geometry', 'commutative-algebra']"
3795734,Generalizing fields to more than two operations: Are these definitions equivalent?,"See the previous question Can a ""generalized field"" with three operations be infinite? We have a set $S$ , and $n$ operations $\times_0,\times_1,\times_2,\cdots,\times_{n-1}$ on $S$ . Each operation $\times_k$ is commutative, is associative, has an identity $e_k\in S$ , and distributes over the previous operation $\times_{k-1}$ . Also, the identities are all distinct. Denote $S_k=S\setminus\{e_0,e_1,e_2,\cdots,e_{k-1}\}$ (understanding that $S_0=S$ , etc.). The whole structure is called an $n$ -field if it has some further properties. Given the above properties, are (some combinations of) the following further properties equivalent? $(1)$ Each $\times_k$ is invertible in the sense that, for any $a\in S_k$ , there exists $b\in S$ such that $a\times_kb=e_k$ . $(2)$ Each $\times_k$ is invertible in the sense that, for any $a\in S_k$ , there exists $b\in S_k$ such that $a\times_kb=e_k$ . $(3)$ Each identity is null with respect to the higher operations; for any $k<l$ and any $a\in S_k$ , $e_k\times_la=e_k$ . $(4)$ Recursively, both of the structures $(S_0,\times_0,\times_1,\cdots,\times_{n-2},e_0,e_1,\cdots,e_{n-2})$ and $(S_1,\times_1,\times_2,\cdots,\times_{n-1},e_1,e_2,\cdots,e_{n-1})$ are $(n-1)$ -fields. (And a $1$ -field is an abelian group.) $(5)$ All $(n-1)$ of the structures $(S_k,\times_k,\times_{k+1},e_k,e_{k+1})$ are fields. (Or if $n=1$ the structure is an abelian group.) Clearly $(2)$ implies $(1)$ , and $(1)$ and $(3)$ together imply $(2)$ . Ideally, I want $(1)$ alone to imply all the others. To prove $(4)$ for the second structure (the first is easy), we'd only need to show that $S_1$ is closed under $\times_1,\times_2,\cdots,\times_{n-1}$ ; that is, if $a\neq e_0\neq b$ , then $a\times_kb\neq e_0$ . But this follows from $(3)$ and invertibility. My linked question proves that $(1)$ implies the others in the case $n=3$ , and shows that no $n$ -field exists for $n>4$ provided that it's true for $n=4$ . So let's focus on $4$ -fields, and assume property $(1)$ . We know that the sub-structure $(S_0,\times_0,\times_1,\times_2,e_0,e_1,e_2)$ is a $3$ -field, which implies, for all $a\in S$ , $$e_0\times_0a=e_1\times_1a=e_2\times_2a=a,$$ $$e_0\times_1a=e_0\times_2a=e_0,$$ $$a\neq e_0\implies e_1\times_2a=e_1.$$ The last two lines are property $(3)$ for this sub-structure. To complete $(3)$ , we need to consider the final operation $\times_3$ : $$e_0\times_3a\overset?=e_0,$$ $$a\neq e_0\overset?\implies e_1\times_3a=e_1,$$ $$e_0\neq a\neq e_1\overset?\implies e_2\times_3a=e_2.$$ For the last one, defining $x=e_2\times_3a$ and using the given algebraic laws, $$e_2\times_3a=(e_2\times_2e_2)\times_3a=(e_2\times_3a)\times_2(e_2\times_3a),$$ we see that $x=x\times_2x$ is its own square. If $x\in S_2$ (meaning it's not $e_0$ or $e_1$ ), then it's $\times_2$ -invertible, and dividing gives $e_2=x$ . If instead $x=e_0$ or $e_1$ , then $$a=a\times_3e_3=a\times_3(e_3\times_2e_2)=(a\times_3e_3)\times_2(a\times_3e_2)=a\times_2x,$$ so we get $a=a\times_2e_0=e_0$ , or $a=a\times_2e_1=e_1$ , a contradiction. Therefore $x=e_2\times_3a=e_2$ . In fact we can prove $(2)$ from $(1)$ , at least in the case $n=4$ . We already know that $\times_0,\times_1,\times_2$ are invertible on their respective spaces $S_0,S_1,S_2$ . It remains to consider $a\in S_3$ : is its $\times_3$ -inverse $b$ also in $S_3$ ? Suppose contrarily that $b=e_0$ , $e_1$ , or $e_2$ . Then, from the known properties of $3$ -fields, $b=b\times_2b$ , and thus $$e_3=a\times_3b=a\times_3(b\times_2b)=(a\times_3b)\times_2(a\times_3b)=e_3\times_2e_3;$$ but $e_3\in S_2$ is invertible; dividing gives $e_2=e_3$ , a contradiction. So we must have $b\in S_3$ .","['group-theory', 'field-theory', 'definition', 'abstract-algebra', 'abelian-groups']"
3795773,Word Problem about Statistics,The median of 5 numbers is 15. The mode is 12. The mean is 15. What are the 5 numbers? I've done some algebra and now I know that the last 2 digit numbers must add up to 36. How can I determine those two digits without having to use trial and error which would consume a lot of time?,"['word-problem', 'statistics']"
3795846,It is really necessary to be always very rigorous in math?,"I am an undergraduate student trying to read the text "" Calculus on Manifolds "" by Spivak. I have some doubts, in general, about how to write the exercises in this book (or another books) that involves lots of cumbersome notation. Before giving an example, I want to say that as an undergraduate, some teachers asks to students to write very clear proofs and to prove every assertion we made. I think that sometimes, making that very-clear-proofs are very very inefficient or hard. Now, the example that I would like to share is the following (at the end, I will add some comments that I really want to discuss; thanks in advance for taking your time reading this). I'll share with you the "" $(\Leftarrow)$ "" part of my proof. Suppose that for all subrectangles $S_i$ of $P,$ the function $f\vert_{S_i}$ is integrable over $S_i.$ If $S$ is a subrectangle of $P,$ then for all $\varepsilon>0$ we can find a partition $P_S$ of $S$ s.t. $U(f\vert_S,P_S)-L(f\vert_S,P_S) < \varepsilon/N$ (where $N$ is the number of subrectangles of $P$ ). Lets write $P_S=\{ P_{1,S}, \ldots, P_{n,S} \}.$ Let $Q=\{ \cup_S P_{1,S}, \ldots, \cup_S P_{n,S} \}.$ If we write $A=[a_1,b_1]\times \cdots \times [a_n,b_n],$ is not hard to verify that $\cup_S P_{i,S}$ is a partition of the interval $[a_i,b_i].$ Hence $Q$ is a partition of $A.$ Now, for all subrectangle $S_i$ of $P \quad (1 \leq i \leq N),$ there are subrectangles $S_{1,i}, \ldots, S_{\alpha,i}$ of $Q$ such that $S_i=S_{1,i}\cup \cdots \cup S_{\alpha,i},$ which determine a refinement $Q_{S_i}$ of the partition $P_{S_i}.$ Then we have $U(f\vert_{S_i}, Q_{S_i})-L(f\vert_{S_i},Q_{S_i})<\varepsilon/N.$ Therefore \begin{align*}
U(f,Q)-L(f,Q) =& \sum_{R}[M_R(f)-m_R(f)]v(R) \\
=& \sum_{i}\sum_{j}  [M_{S_{j,i}}(f) - m_{S_{j,i}}(f)]v(S_{j,i}) \\
=& \sum_{i} \left[ U(f, Q_{S_i})-L(f,Q_{S_i}) \right] = \sum_{i} \left[ U(f\vert_{S_i}, Q_{S_i})-L(f\vert_{S_i},Q_{S_i}) \right] \\
<& \sum_{i} \varepsilon /N =\varepsilon.
\end{align*} Then $f$ is integrable. Note that there are several parts of my proof that are not proper and rigorously proved. For example, in this part: "" is not hard to verify that $\cup_S P_{i,S}$ is a partition of the interval $[a_i,b_i].$ Hence $Q$ is a partition of $A.$ Now, for all subrectangle $S_i$ of $P \quad (1 \leq i \leq N),$ there are subrectangles $S_{1,i}, \ldots, S_{\alpha,i}$ of $Q$ such that $S_i=S_{1,i}\cup \cdots \cup S_{\alpha,i},$ which determine a refinement $Q_{S_i}$ of the partition $P_{S_i}$ "". I tried to write proofs of that assertions (that, I consider, are intuitive if you draw rectangles in the plane), but I spent tons of time and made very cumbersome notation (that make the proof very unclear and really hard to follow) in proving those statements. So, it would be insanely hard to prove every assertion that I made in a proof. As a self-learning student, what do you think that I should do in that kind of situations? It should be enough to me having intuitively clear those ideas that requires lots of notation machinery and time? or it should be better that I prove literally any assertion that I made? What should you recommend? If you have some extra comments, it will be great if you share with me your thoughts. Thanks a lot if you read all the text I wrote.","['education', 'multivariable-calculus', 'vector-analysis']"
3795863,Is it possible to intersect two lattices and that their intersection is not a lattice?,"Draw the line diagram of two lattices whose intersection is not a lattice. I have tried to do it with division, with containment relation, and yet I still cannot solve this exercise, I appreciate any help.","['elementary-set-theory', 'lattice-orders']"
3795927,"$\phi_{w}$ is differentiable in $a,$ if $ r (h) $ depends on $ h$","Definition: Let $V\subseteq{\mathbb{R}^{m}}$ an open set, $a\in V$ y $f\colon V\to\mathbb{R}^{n}$ a function. We will say that f is differentiable in $a,$ if exists a linear transformation $f′(a)\colon\mathbb{R}^{m}\to\mathbb{R}^{n}$ such that \begin{equation}
f(a+h)=f(a)+f'(a)(h)+r(h),\qquad\lim_{h\rightarrow 0}{\dfrac{r(h)}{\lVert h\rVert}}=0.
\end{equation} Let $V\subseteq{\mathbb{R}^{m}}$ and $a\in V$ be. They are equivalent (1). Then the function $A\colon V\to\mathcal{L}(\mathbb{R}^{m},\mathbb{R}^{p})$ is differentiable in $a$ (2). For all $w\in\mathbb{R}^{m}$ the function $\phi_{w}\colon V\to\mathbb{R}^{p}$ given by $\phi_{w}(x)=A(x)\cdot w$ is differentiable in $a.$ In this case, $\phi_{w}'(a)\cdot v=(A'(a)\cdot v)\cdot w.$ My attempt: Suppose $ (1). $ Let $w\in\mathbb{R}^{m}.$ Let's see what $\phi_{w}\colon V\to\mathbb{R}^{p}$ given by $\phi_{w}(x)=A(x)\cdot w$ is differentiable in $a.$ By $(1),$ exists a linear transformation $A′(a)\colon\mathbb{R}^{m}\to\mathcal{L}(\mathbb{R}^{m},\mathbb{R}^{p})$ such that \begin{equation}
A(a+h)=A(a)+A'(a)(h)+r(h),\qquad\lim_{h\rightarrow 0}{\dfrac{r(h)}{\lVert h\rVert}}=0.
\end{equation} From this it follows that \begin{equation*}
    A(x+h)\cdot w=A(a)\cdot w+A'(a)(h)\cdot w+r(h)\cdot w, \qquad\lim_{h\rightarrow{0}}{\dfrac{r(h)}{\lVert h\rVert}}=0,
\end{equation*} that is to say, \begin{equation*}
    \phi_{w}(x+h)=\phi_{w}(a)+(A'(a)\cdot h)\cdot w+r(h)\cdot w,\qquad\lim_{h\rightarrow{0}}{\dfrac{r(h)}{\lVert h\rVert}}=0
\end{equation*} and $\phi_{w}$ is f is differentiable in $a.$ I did not want to continue because several doubts arose with that test that I wrote. In this case, is the function $ r (h) $ a linear transformation? Also, can't I conclude that $ \phi_{w} $ is differentiable in $ a $ because that $ r(h) $ depends on $ h $ ?
Any suggestion will be welcome. Also, I would like to know if you have seen that exercise in a book.","['partial-derivative', 'multivariable-calculus', 'derivatives', 'real-analysis']"
3795938,"If $f: \mathbb N \rightarrow \mathbb N \times \mathbb N $ such as $f(n)=(n,n+1)$ Is it surjective and/or injective?","If $f: \mathbb N \rightarrow \mathbb N \times \mathbb N $ such as $f(n)=(n,n+1)$ Is it surjective and/or injective? I know that it is surjective $\Leftrightarrow \forall (a,b) \in \mathbb N \times \mathbb N \exists c \in \mathbb N:f(c)=f(a,b)$ It is obviously injective because if $(n,n+1)=(m,m+1) \rightarrow n=m$ I can see that it is not surjective but do not know how to prove it, can I get some help?","['elementary-set-theory', 'functions']"
3795944,Solutions to Monge–Ampère equation,"I am looking for examples of solutions to the following Monge-Ampere equation (also called a determinant Hessian PDE, and note that this is analogous to finding a surface given its Gaussian curvature) $$\phi_{aa}\phi_{bb}-\phi_{ab}^2 = K(b).$$ An example of a solution to this is $\phi=\sin a\ e^b$ and $K(b)=-e^{2b}$ . Are there any other non-trivial (ie non polynomial examples)? Note, one can linearize the above equation using the theory of 2 forms and a partial Legendre transform, but it's not obvious that the solutions may be transformed back in closed form.","['partial-differential-equations', 'differential-geometry']"
3795947,Necessity to prove certain obvious and trivial results in trigonometry?,"I'm currently doing high school second year trigonometry but this is just something that came to my mind. The figure below shows a right angled triangle $ABC$ , where $\angle A = \alpha$ , $\angle B = \dfrac{\pi}{2}$ or $90^\circ$ . Now, in introductory trigonometry, the trigonometric functions of an angle are defined as ratios or sides of a right angled triangle. For example : $\sin\alpha = \dfrac{BC}{AC}$ . In the textbook I learned trigonometry from, this were the contents of the first few pages : What is trigonometry? It's applications in other fields of Mathematics and outside of Mathematics Definition of trigonometric functions (or ratios) Examples and questions The first example was as follows : You are provided with two  triangles. Let those triangles be $XYZ$ and $PQR$ . Both of these triangles are right angled triangles. $\angle Y = \angle Q = 90^\circ$ . Also, $\angle X = \angle P$ . Let these two angles be equal to $\varphi$ . Now, $XZ = 5 \text{ units}$ , $YZ = 3 \text{ units}$ and $PR = 10 \text{ units}$ . Find $QR$ . The solution was as follows : $$\sin\varphi = \dfrac{\text{Perpendicular}}{\text{Hypotenuse}} = \dfrac{3}{5} \text{, obtained from } \Delta XYZ$$ $$\text{From }\Delta PQR \text{, } \sin\varphi = \dfrac{QR}{10}$$ $$\therefore \sin\varphi ~ \dfrac{3}{5} = \dfrac{QR}{10} \implies QR = \dfrac{10 \cdot 3}{5} = 6 \text{ units}$$ I think that prior to this,. the following statement should have been proved : The trigonometric ratios of an angle are unique and do not depend on the triangle chosen. I don't think think that this solution would be valid before proving that the sine of $\varphi$ obtained from both the triangles is unique. The statement that I have mentioned above can easily be proved using similarity but what I want to ask in this question is ""Should statements like this, which are assumed to be obvious and trivial be proved before attempting questions like the example I gave and would the solution to this example be rendered as invalid if this statement has not been proved beforehand?"" Thank You!","['trigonometry', 'soft-question']"
3795992,Percentage of contribution to the average,"I want to calculate how an individual Business Unit (BU) of a company contributed to the overall Average  Salary of the company. Average Salary is defined as the average (arithmetic mean) salary of the employees of the company.
Suppose the BU-wise breakup of the salaries and the count of employees of the company is as follows: $$\begin{array}{|c|c|c|} 
\hline
\textbf{BU} & \textbf{Total Salary (in thousands)} & \textbf{No of Employees}& \textbf{Average Salary (in thousands)} \\ \hline
\text{A} & 500 & 8 & 62.5 \\ \hline
\text{B} & 700 & 10 &  70\\ \hline
\text{C} & 630 & 7 &  90\\ \hline
\end{array}$$ I can calculate the average salary of an employee for the company by : (500 + 700 + 630)/(8 + 10 +7) = 73.2 I want to calculate how much did each BU contribute to this average. Is there a way to calculate the individual contribution percentages of each BU?","['statistics', 'percentages', 'fractions', 'average', 'algebra-precalculus']"
3796046,Precalculus algebra problem about rational and irrational numbers.,"Let $ a, b $ be irrational numbers. We know that $ a + b $ , $ a^3 + b^3 $ and $ a^2 + b $ are rational. I have proved that $ ab $ , $ a + b^2 $ are also rational.
I tried to find some examples: $ (1 - \sqrt{x}, 1 + \sqrt{x}) $ , $ (1 - \sqrt[3]{x}, 1 + \sqrt[3]{x}) $ , $ (1 - \sqrt[6]{x}, 1 + \sqrt[6]{x}) $ , even trigonometric functions.","['irrational-numbers', 'algebra-precalculus', 'roots', 'radicals']"
3796062,$\left(\frac{\partial u}{\partial x}\right)^2+\left(\frac{\partial u}{\partial y}\right)^2=\left|2\frac{\partial u}{\partial z}\right|^2$,"I'm reading Complex Analysis by Stein and Shakarchi (page 13). The goal is to prove that $\partial f/\partial z=2\, \partial u/\partial z$ where $f=u+iv$ and $z=x+iy$ . The proof in the book states that $$\begin{align}\det J_F (x_0,y_0)&=\frac{\partial u}{\partial x}\frac{\partial v}{\partial y}-\frac{\partial v}{\partial x}\frac{\partial u}{\partial y}\\&=\left(\frac{\partial u}{\partial x}\right)^2+\left(\frac{\partial u}{\partial y}\right)^2\\&\overset{?}{=}\left|2\frac{\partial u}{\partial z}\right|^2.\end{align}$$ I understand everything except for the last step marked with ""?"". What I get instead is $$\begin{align}\left(\frac{\partial u}{\partial x}\right)^2+\left(\frac{\partial u}{\partial y}\right)^2&=\frac{\partial u^2}{\partial x^2}+\frac{\partial u^2}{\partial y^2}\\&=\frac{\partial u^2\partial y^2+\partial u^2\partial x^2}{\partial x^2\partial y^2}\\&=\frac{\partial u^2(\partial x^2+\partial y^2)}{\partial x^2\partial y^2}\\&=\frac{\partial u^2\partial z^2}{\partial x^2\partial y^2}\end{align}$$ but I don't understand why should that equal $\left|2\frac{\partial u}{\partial z}\right|^2$ .","['complex-analysis', 'cauchy-riemann-equations']"
3796091,Simply connected domain under analytic function,"Let $f$ be analytic and one-to-one that is defined on a simpliy connected domain $D$ , I am struggling with showing that $f(D)$ is simply connected as well, I'll appreciate hints!",['complex-analysis']
3796098,Moment generating functions of two random variable,"Let $X$ and $Y$ be independent random variable with respective moment generating function $M_x(t) = \frac{(8+e^t)^2}{81} $ and $M_y(t) = \frac{(1+3e^t)^3}{64} , -\infty<t<\infty $ Then $ P(X+Y = 1) $ equals I know that using moment generating function we can find probability $M_x(t) = P(X=0)e^{t*0} + P(X=1)e^{t*1}.....P(X=n)e^{t*n}$ Comparing this mgf we can get the particular probability.
But how do we do this question?","['moment-generating-functions', 'probability-theory', 'probability']"
3796132,"Deriving the gradient of $f: \mathbb{R^3} \rightarrow \mathbb{R}$ at any coordinates $(u_1,u_2,u_3)$","Let $(u_1,u_2,u_3)$ be an coordinates on $\mathbb{R^3}$ s.t there is parametrization  as the following: $$\vec{r}\left(u_{1}, u_{2}, u_{3}\right)=x_{1}\left(u_{1}, u_{2}, u_{3}\right) \hat{i}+x_{2}\left(u_{1}, u_{2}, u_{3}\right) \hat{j}+x_{3}\left(u_{1}, u_{2}, u_{3}\right) \hat{k}, \quad\left(u_{1}, u_{2}, u_{3}\right) \in D$$ assume that $\vec{r} \in C^1$ and let $$\vec{r}_{u_{1}}=\frac{\partial \vec{r}}{\partial u_{1}}, \quad \vec{r}_{u_{2}}=\frac{\partial \vec{r}}{\partial u_{2}}, \quad \vec{r}_{u_{3}}=\frac{\partial \vec{r}}{\partial u_{3}}$$ define the basis $$B=\left\{\vec{r}_{u_{1}}, \vec{r}_{u_{2}}, \vec{r}_{u_{3}}\right\}$$ for $\mathbb{R^3}$ and Let $$\tilde{B}=\left\{\vec{f}_{u_{1}}, \vec{f}_{u_{2}}, \vec{f}_{u_{3}}\right\}$$ be the dual basis s.t $$\left\langle f_{u_{i}}, \vec{r}_{u_{j}}\right\rangle=\delta_{i j}$$ define the matrix $g$ s.t $$g_{i j}=\left\langle\vec{r}_{u_{i}}, \vec{r}_{u_{j}}\right\rangle$$ Let $$f: \mathbb{R^3} \rightarrow \mathbb{R}$$ and $f \in C^1$ assume that $f$ is the function of the coordinates $(u_1,u_2,u_3)$ s.t $$f\left(u_{1}, u_{2}, u_{3}\right)=f\left(\vec{r}\left(u_{1}, u_{2}, u_{3}\right)\right)$$ show that the grad of $f$ in $(u_1,u_2,u_3)$ coordinates is $$\vec{\nabla} f=\sum_{i=1}^{3}(\vec{\nabla} f)_{i} \vec{r}_{u_{i}}=\sum_{i=1}^{3} \sum_{i=1}^{3}\left(g^{-1}\right)_{i j} \frac{\partial f}{\partial u_{j}} \vec{r}_{u_{i}}$$ while $\left(g^{-1}\right)_{i j}$ is the entries of the inverse matrix $g$ . I understood so far that the matrix g is the multiply of $J^TJ$ but I am not sure how it can help me solve this problem","['coordinate-systems', 'multivariable-calculus', 'vector-analysis', 'differential-geometry']"
3796173,Derivative with summation operator in the exponent of $e$,"What is the derivative when the summation operator is in the exponent of $e$ , such as: $$\frac{d}{d\mathbf{x}}\left(\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)=\frac{d}{d\mathbf{x}}\left(\frac{1}{1+e^{-\sum_{i} a_ix_i+b_i}}\right),$$ where $\mathbf{a}=[a_1,a_2,\cdots,a_n]$ and $\mathbf{b}=[b_1,b_2,\cdots,b_n]$ are constant vectors. I tried the following using chain rule: $\frac{d}{d\mathbf{x}}\left(\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)=\left(\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)\times\left(1-\frac{1}{1+e^{-\mathbf{\hat{x}}}}\right)\times\mathbf{a}^T$","['summation', 'derivatives', 'vector-spaces']"
3796174,Locally closed subspace,"Came across this definition of a locally closed subspace in a book: Any subspace $A$ of a space $X$ is called locally closed if
each point of $A$ is contained in an open subset $G$ of $X$ such that $G ∩ A$ is closed in $G$ . If all points of $A$ are contained in $G$ then $G∩A$ is closed in $G$ should essentially just mean $A$ is closed in $G$ but is this really the case?","['general-topology', 'definition']"
3796192,"What distinguishes the terms ""relation"", ""function"" and ""mapping""?","In this context the first domain $\theta_1\left(f\right)$ and second domain $\theta_2\left(f\right)$ represent the set of pre-image elements, and the set of image elements, AKA range . The following is from BBFSK , Part A, Section 8.4: An important class of relations consists of the functions , defined by the requirement of uniqueness $\forall_{x}\forall_{y}\forall_{z}\left(\left(xry\land xrz\right)\implies y=z\right).$ [ $\dots$ ] The function $f$ is a mapping of the first domain $\theta_1\left(f\right)$ onto the second domain $\theta_2\left(f\right)$ : if $\theta_2\left(f\right)$ is contained in a set $\mathcal{A},$ we say that $f$ is a mapping into $\mathcal{A}.$ Apparently that is where they introduce the term mapping , and by the emphasis using italics, I assume it is intended to be a definition. Is it correct to understand this as: the term mapping means a correspondence between two sets, or between a set and itself such that the set of image elements is the second domain $\theta_2\left(f\right)$ of a function $f$ .  Specifically, for every argument (pre-image) element there is exactly one image element (definition of function ).  In other words, all mappings are single-valued. Furthermore, this distinguishes between the term mapping and function in that a mapping has a codomain which is not ncessarily covered by image elements, whereas a function ncessarily covers its second domain. I am particularly interested in this question as it pertains to computer science, and such fields as relational database schema and UML.  I used to think that there was such a thing as a many-to-many mapping. Apparently the use of the term relation regarding many-to-many correspondences is consistent with mathematical usage, but the term mapping should be restricted to many-to-one relations, where many may be one. Is this correct?","['elementary-set-theory', 'definition', 'relations', 'computer-science']"
3796229,How to evaluate $\int_0^{\pi/2} x\ln^2(\sin x)\textrm{d}x$ in a different way?,"The following problem \begin{align}
&\int_{0}^{\pi/2}
x\ln^{2}\left(\sin\left(x\right)\right)\,{\rm d}x \\[5mm] = & \
\frac{1}{2}\ln^{2}\left(2\right)\zeta\left(2\right)
- \frac{19}{32}\,\zeta\left(4\right) +
\frac{1}{24}\,\ln^{4}\left(2\right)
+ \operatorname{Li}_{4}\left(\frac{1}{2}\right)
\label{1}\tag1
\end{align} was already solved in this solution. The question here is how to prove $(\ref{1})$ by utilizing the Fourier series of \begin{align}
&\tan\left(x\right)
\ln\left(\sin\left(x\right)\right) =
-\sum_{n=1}^\infty\left[%
\psi\left(\frac{n+1}{2}\right) -
\psi\left(\frac{n}{2}\right)-\frac1n\right]\sin(2nx)
\\[5mm] = & \
-\sum_{n=1}^\infty\left(\int_{0}^{1}\frac{1 - t}{1 + t}\,\,t^{n - 1}\,\,{\rm d}t\right)
\sin\left(2nx\right),\quad 0 < x <\frac{\pi}{2}
\end{align} I wonder what kind of clever manipulation we need to do to create the integrand in $(1)$ . I am sure it would be an amazing solution. Thank you in advance. This Fourier series can be found in the book, Almost Impossible Integrals, Sums and series , page $243$ ,  Eq $(3.281)$ .","['integration', 'digamma-function', 'real-analysis', 'alternative-proof', 'polylogarithm']"
3796312,Does there exist a a general counting function related to the prime counting function?,"Does there exist a a general counting function related to the prime counting function? Say for example I wanted all the positive integer multiples of three less than or equal to N, is there a multiples of 3 less than or equal to N counting function? Sometimes, I might want all positive integer multiples of a product of primes, for example 15, does there exist a positive multiples of 3 and 5 less than or equal to N counting function?","['distribution-of-primes', 'integers', 'functions', 'combinatorics', 'sequences-and-series']"
3796320,Evaluating $\left| \frac{\tan40^\circ + \tan100^\circ + \tan160^\circ}{\tan20^\circ\tan40^\circ\tan80^\circ} \right| $,"How do I find the value of the following expression? $$
\left| \frac{\tan40^\circ + \tan100^\circ + \tan160^\circ}{\tan20^\circ\tan40^\circ\tan80^\circ} \right|
$$ I tried writing the numerator as $\tan 40^\circ - \tan80^\circ -\tan20^\circ,$ but then the expression was getting complicated.",['trigonometry']
3796428,Dot product properties,"I want to prove or contradict the following claim: If we take two vectors $\mathbf{v}_1$ and $\mathbf{v}_2$ in $\mathbb{R}^{d}$ ( $d$ isn't neccesarily 2, so geometric proofs aren't available) and the angle between them, which is defined by $\cos(\alpha_{\mathbf{v}_1,\mathbf{v}_2}) = \frac{\mathbf{v}_1^T\mathbf{v}_2}{\Vert \mathbf{v}_1 \Vert \Vert \mathbf{v}_2 \Vert}$ the following holds: For any vector $\mathbf{u}$ s.t. $\text{sgn}(\mathbf{v}_1^T\mathbf{u}) = \text{sgn}(\mathbf{v}_2^T\mathbf{u}) = 1$ if we denote $\tilde{\mathbf{v}}_1 = \mathbf{v}_1+\mathbf{u}$ and $\tilde{\mathbf{v}}_2 = \mathbf{v}_2+\mathbf{u}$ we'll get $\alpha_{\tilde{\mathbf{v}}_1,\tilde{\mathbf{v}}_2}<\alpha_{\mathbf{v}_1,\mathbf{v}_2}$ For any vector $\mathbf{u}$ s.t. $\text{sgn}(\mathbf{v}_1^T\mathbf{u}) = \text{sgn}(\mathbf{v}_2^T\mathbf{u}) = -1$ if we denote $\tilde{\mathbf{v}}_1 = \mathbf{v}_1-\mathbf{u}$ and $\tilde{\mathbf{v}}_2 = \mathbf{v}_2-\mathbf{u}$ we'll get $\alpha_{\tilde{\mathbf{v}}_1,\tilde{\mathbf{v}}_2}<\alpha_{\mathbf{v}_1,\mathbf{v}_2}$ I am pretty confident the above holds, since I ran a lot of numerical simulations and it does seem to hold, i.e. I believe the claim needs to be proved and not contradicted. I attempted to use the algebraic definition of cosine with some algebraic tricks (triangle inequality etc) and it didn't work, same with the generalized cosine inequality (for vectors).",['linear-algebra']
3796449,An Integral involved $L^2$ function,"Define, $$g_n :[0,1] \to \mathbb{R} \, ,$$ such that $g(x) = n$ on $\left[0, \dfrac{1}{n^3}\right]$ and $0$ otherwise. I need to prove that, if $f \in L^2[0,1] ,$ then $\lim_{n \to \infty} \int f g_n \, d\mu = 0$ My attempt: My idea is to use MCT. $g \to 0$ pointwise a.e. Since $f \in L^2[0,1]$ , $|f|^2 \in L^1[0,1]$ , though it seems that this approach doesn't work since there is no $f^2$ to work with. Alternatively, can I use $L^2[0,1] \subset L^2[0,1]$ ?. Any hint would be appreciated.","['integration', 'measure-theory', 'functional-analysis', 'real-analysis']"
3796471,Monodromy element: Why that name?,"Let $(H,R)$ be a quasitriangular Hopf algebra, i.e. $R$ is a choice of an universal $R$ -matrix for the Hopf algebra H. (You can find a definition of the term quasitriangular Hopf algebra on wikipedia .). One calls the element $Q:=R_{21}\cdot R_{12} \in H \otimes H$ the monodromy element of the quasitriangular Hopf algebra $(H,R)$ . Questions Why is $Q$ called that way? Is it related to the monodromy group? Or to any other concept presented on here ?","['singularity', 'abstract-algebra', 'hopf-algebras', 'algebraic-topology', 'terminology']"
3796479,"Calculate $\int_Sxyz \cdot dS$ where $S$ is the triangle $(1,0,0),(0,2,0),(0,1,1)$","Calculate the integral: $$\int_Sxyz \cdot dS$$ where $S$ is the surface of the triangle with vertices: $P(1,0,0) ,Q(0,2,0),R(0,1,1).$ I've find the  plane where $P,Q,R$ are points in it: $$Π :(x,y,z)=r\vec {PQ}+t\vec {PR} +(1,0,0)$$ $$\text{Hence } ,\text{ Π: }2x+y+z=2$$ $$\Rightarrow z=2-y-2x$$ $$z_x^2=4,z_y^2=1$$ $$\int_SxyzdS=\int_0^1\int_0^{2-2x}xy\cdot (2-y-2x)\sqrt{1+4+1}\cdot dydx=\frac{\sqrt{6}}{15}.$$ But the answer is $\frac{\sqrt{6}}{30}$ What am i doing wrong? Thank you.","['integration', 'multivariable-calculus', 'multiple-integral']"
3796523,How to solve $\frac{dx}{dt} = ax^2 + bx + c$?,"It's been a while since I've solved ODEs analytically, but this looks like a first order nonlinear ODE that is separable. When I separate the terms, I see \begin{align}
    \frac{1}{ax^2 + bx + c}dx = dt \\
\end{align} The LHS it'll be quite difficult to evaluate. Is there a simple solution to this problem that I am not seeing?",['ordinary-differential-equations']
3796539,"Given a field $\mathbb F$, is there a smallest field $\mathbb G\supseteq\mathbb F$ where every element in $\mathbb G$ has an $n$th root for all $n$?","The base field $\mathbb F$ is probably not important, but I'm using the rational expressions over the binary field, $\mathbb F=\mathbb F_2(x)$ . One subfield of $\mathbb G$ consists of ratios of polynomials in roots of $x$ , with coefficients in $\mathbb F_2$ , such as $$\frac{x^{2/5}+x^{-1/3}+x^2}{x^{-2/3}+1}=\frac{x^{16/15}+x^{1/3}+x^{8/3}}{1+x^{2/3}}=\frac{\sqrt[15]x^{40}+\sqrt[15]x^{16}+\sqrt[15]x^5}{\sqrt[15]x^{10}+1},$$ and such expressions are added and multiplied using the usual rules for rational functions, and $x^ax^b=x^{a+b}$ for $a,b\in\mathbb Q$ . Squaring and square roots are linear over $\mathbb F_2$ , so we can simply halve the exponents on $x$ to get a square root of the expression. Thus, any $2^n$ th root always exists. For example, $$x+1=(x^{1/2}+1)^2=(x^{1/4}+1)^4=(x^{1/8}+1)^8.$$ But this doesn't work for other $n$ th roots; it's not possible to write $$x+1=\frac{p(x^{1/m})^3}{q(x^{1/m})^3}$$ where $p,q$ are polynomials and $m\in\mathbb N$ . So it's necessary to formally adjoin $\sqrt[3]{x+1}$ to the field, or instead $\sqrt[3]{x^{1/2}+1}$ , etc. Is there a unique, well-defined field $\mathbb G$ of such algebraic expressions over $\mathbb F$ ? Note that I don't want $n$ different $n$ th roots of each element, just a single root (unless $\mathbb F$ already has roots of unity; but I chose $\mathbb F_2$ to avoid this). Given the algebraic closure $\mathbb A\supseteq\mathbb F$ , we might just take the intersection $\mathbb G\overset?=\bigcap\{\mathbb B\}$ of all intermediate fields $\mathbb A\supseteq\mathbb B\supseteq\mathbb F$ with the property $\forall n\in\mathbb N,\,\forall a\in\mathbb B,\,\exists b\in\mathbb B,\,a=b^n$ . But this doesn't work because different fields have different roots of $a$ , so their intersection contains no root of $a$ . Presumably there's some way to use the axiom of choice to construct $\mathbb G$ , either through $\mathbb A$ , or directly from $\mathbb F$ . Is this the case? Can the Wiki proof of existence (I haven't followed it in detail) be modified to give $n$ th roots of everything without introducing new roots of unity? And what about uniqueness? Is there a simpler construction of $\mathbb G$ for the special case of $\mathbb F_2(x)$ , that doesn't use the axiom of choice? Here I don't require uniqueness. See for example this answer ; we would be using polynomials of the form $x^p-a$ which are irreducible over the field defined by the previous polynomials. Having several $n$ th roots of $a\neq0$ is equivalent to having an $n$ th root of unity: If $x_1^n=x_2^n=a$ and $x_1\neq x_2$ , then $(x_1/x_2)^n=1$ and $(x_1/x_2)\neq1$ . Conversely, if $\omega^n=1$ and $\omega\neq1$ , and $x_1^n=a$ , then $(\omega x_1)^n=a$ and $x_1\neq\omega x_1$ . If $\mathbb F$ has a primitive $mn$ th root of unity, then it also has a primitive $n$ th root of unity; so we need only consider prime numbers. Fix two primes $p\neq q$ . If $\mathbb F$ has a primitive $p$ th root of unity $\omega_1$ , then $\mathbb G$ should have a primitive $p^n$ th root of unity $\omega_n$ for all $n$ , since non-primitive $p^n$ th roots of unity can never reach $\omega_{n-1}$ as a $p$ th power. If $\mathbb F$ doesn't have a primitive $q$ th root of unity, then $\mathbb G$ shouldn't either, since we already have $1^q=1$ and $(\omega_n^r)^q=\omega_n$ where $r=q^{-1}\bmod p^n$ .","['axiom-of-choice', 'field-theory', 'abstract-algebra', 'extension-field', 'radicals']"
3796553,About a differential form in submanifolds of hyperboloid diffeomorphics to $S^1$,"Let $$\omega=xz\,dx+yz\,dy-z^2\,dz$$ be a $1$ -form in $\Bbb{R}^3$ and let $H$ be the hyperboloid $x^2+y^2-z^2=1$ . I want to show that, if $M\subset H$ is a surface that is diffeomorphic to the sphere $S^1$ , so $\int_M \omega=0.$ My idea is to find a $0$ -form $\eta$ such that, in $M$ , $d\eta=\omega$ . My first try was $$\eta=z\frac{x^2}{2}+z\frac{y^2}{2}-\frac{z^3}{3}\implies  d\eta=xz\,dx+yz\,dy+\left(\frac{x^2+y^2-2z^2}{2}\right)\, dz $$ My hope was to use $x^2+y^2-z^2=1$ in the equation of $d\eta$ to cancel some terms and get $d\eta=\omega$ , but I didn't get it. What can I do?","['multivariable-calculus', 'differential-forms']"
3796569,How can I prove $\int_{0}^{1} \frac {x-1}{\log(x) (1+x^3)}dx=\frac {\log3}{2}$,"Question:- Prove that $$\int_0^1 \frac {x-1}{\log(x) (1+x^3)} \, dx = \frac {\log(3)}{2}$$ I saw this problem as an comment on a youtube video few hours ago but I don't know how to prove this one as integration by parts doesn't works here. Also I wasn't able to figure out any proper subsitution that would simplify the integral. Can someone suggests me some hints?","['integration', 'calculus', 'definite-integrals']"
3796602,"Square-integrability in lemma 4.30 of Folland's ""A Course in Abstract Harmonic Analysis""","Update: I decided to post the question on MathOverflow here . In lemma 4.30 of Folland's ""A Course in Abstract Harmonic Analysis"" (Second Edition) one needs to show the square-integrability of the function $f$ defined below and I don't understand how Folland deduces it from the inequality below. For context, $G$ is a locally compact abelian Hausdorff group and $dx$ is the Haar measure on $G$ (note that Folland defines Radon measures to be outer regular and inner regular on open sets). Furthermore $f$ is continuous, bounded and a linear combination of functions of positive type (this is encoded in the notation $f \in \mathcal{B}(G)$ ). Here is what i have tried so far: With Plancherel's theorem we see (as in Folland's proof) that $$(L^1(G) \cap L^2(G), \|\cdot\|_2) \to \mathbb{C}, \ k \mapsto \int_G f(x) \cdot k(x) \, dx$$ defines a bounded linear functional which extends to a bounded linear functional $F \in L^2(G)^*$ by the BLT theorem (I removed the complex conjugation for linearity; this should not make a difference in the argumentation). Now Riesz's theorem yields an $r \in \mathcal{L}^2(G)$ such that $F$ is given by integration against $r$ , i.e. $$F(k) = \int_G r(x) \cdot k(x) \, dx \ \text{ for all } k \in L^2(G).$$ In particular we have $$\int_G f(x) \cdot k(x) \, dx = \int_G r(x) \cdot k(x) \, dx \ \text{ for all } k \in L^1(G) \cap L^2(G).$$ With this we can show that the set $N := \{x \in G: r(x) \neq f(x)\}$ is locally null with respect to the Haar measure $dx$ since for any Borel set $A \subseteq N$ with finite Haar measure we can set $$k(x) := 1_A(x) \cdot \frac{|f(x) - r(x)|}{(f(x) - r(x)) + 1_{G \setminus N}(x)}$$ to obtain a function $k \in L^1(G) \cap L^2(G)$ , so $$0 = \int_G (f(x) - r(x)) \cdot k(x) \, dx = \int_A |f(x) - r(x)| \, dx,$$ i.e. $A \cap N = A$ has Haar measure $0$ . To show $f \in L^2(G)$ we can now equivalently show that $N$ has Haar measure $0$ (it is clear that this is sufficient and it also is necessary by the injectivity of the map in Riesz's theorem), but I haven't managed to reach this conclusion - the claim would follow from the above argumentation if our Haar measure was inner regular on $N$ , but I also don't see why this would be the case. EDIT: It is sufficient to show that $N$ is $\sigma$ -finite and this problem can be reduced further: The set $$R := \{x \in G: r(x) \neq 0\} = \bigcup_{n \in \mathbb{N}} \{x \in G: |r(x)| \geq \tfrac{1}{n}\}$$ is $\sigma$ -finite since $r \in \mathcal{L}^2(G)$ , so $R \cap N$ is again $\sigma$ -finite and locally null. Hence $R \cap N$ has Haar measure $0$ and we only need to worry about the set $$M := (G \setminus R) \cap N = \{x \in G: r(x) = 0 \neq f(x)\}.$$","['measure-theory', 'harmonic-analysis', 'proof-explanation', 'lp-spaces', 'locally-compact-groups']"
3796615,Show that power set is a set.,"I came across following proposition author wants the reader to prove: Proposition 1 . For arbitrary set $X$ , $\{A \mid A \subseteq X\}$ is a set. My attempt (mainly based on hints given by the author): I'll first state the power axiom presented in the book (which seems to be different from what is written the wikipedia article ): Power set axiom . Let $X$ and $Y$ be sets. Then there exists a set, denoted $Y^{X}$ , which consists of all the functions from $X$ to $Y$ , thus $$f \in Y^{X} \iff \text{(f is a function with domain $X$ and range Y)}$$ Using power set axiom and replacement axiom, we can construct following set $$S = \{Z \mid Z = f^{-1}(\{1\}) \text{ for some } f \in \{0,1\}^X \}$$ Now we need to show that for arbitrary $A \in S$ , $A \in S$ iff $A \subseteq X$ $(\rightarrow)$ Take some $A \in S$ and take some $a \in A$ . Since $A \in S$ , exists some $f: X \rightarrow Y$ such that $f^{-1}(\{1\}) = A$ . By definition of the backward image, we can conclude that $a$ is in the domain of $f$ , that is $a \in X$ . $(\leftarrow)$ Take arbitrary subset of $X$ , say $A$ . We can define $f: X \rightarrow Y$ such that $f(x) = 1$ iff $x \in A$ , and $f(x) = 0$ otherwise. We see that $f \in \{0,1\}^{X}$ and it is true that $A = f^{-1}(\{1\})$ . Hence $A \in S$ . Hence $S = \{A \mid A \subseteq X\}$ , which means that $\{A \mid A \subseteq X\}$ is a set. $\blacksquare$ Question 1. Is it correct? Question 2. If the proof above is correct, are there more concise alternatives? Before seeing hints by the author (that is, we need to use power set axiom and replacement axiom), I'd thought following argument would be sufficient: ""Set is a collection of objects. Subset is an object. Hence collection of subsets of a particular set is a set.""","['elementary-set-theory', 'solution-verification']"
3796633,Conditional statement contradicts truth table,"I have spent all day reading over SE and other sites trying to understand this but I am having trouble. The conditional statement: If you are a guitar player, then you are a musician. a → b \begin{array}{|c|c|c|}
\hline
a& b & a→b \\ \hline
T & T & T \\ \hline
T & F & F \\ \hline
F & T & T \\ \hline
F & F & T \\ \hline
\end{array} If-then form: If you are a guitar player, then you are a musician.
True, guitars players are musicians. Converse : If you are a musician, then you are a guitar player.
False, not all musicians play the guitar. Inverse : If you are not a guitar player, then you are not a musician.
False, even if you don’t play a guitar, you can still be a musician. Contrapositive : If you are not a musician, then you are not a guitar player
True, a person who is not a musician cannot be a guitar player. Looking at the truth table above, the last row shows that F,F=T. The inverse statement says this too but there it's false whereas in the truth table it's True. The converse statement doesn't seem to agree with the truth table either. I understand that the converse is b → a and the inverse is ~a → ~b and the contrapositive is ~b → ~a What I don't understand is this (apologies for showing a different example)
If it is raining, there are clouds in the sky
a = Raining,    b = Clouds Contrapositive: If there are no clouds in the sky, then it is not raining. (I understand that this is logically equivalent to the conditional statement) I don’t understand what the use of the truth table is. It’s useful in showing that if it rains, it’s cloudy and that you can’t have rain and then no clouds. But in these two examples you are given if ""a"" is true or false and then if ""b"" is true of false. What happens when you are told like in the contrapositive that ""b"" is false and ""a"" is false (this is in the opposite order, given ""b"" first then ""a"")?
Can you still look at the truth table, look at the last row and say that the conditional statement is True overall? What has really confused me is also that logically if I know that it is raining then there should be clouds but I also know that just because there are clouds doesn’t necessarily mean it will be raining. This is the same as saying all squares are rectangles but not all rectangles are squares. I don’t see where this is in the truth table. Sorry again for all of my confusion, I am probably making this more confusing than it is, but I need a step by step explanation Thank you for your time and answers","['logic', 'discrete-mathematics']"
3796686,Solve the system of linear inequalities with parameters,"Solve the system of inequalities \begin{cases}
0\leq \phantom{-2\;}x+2\,y-3\,b+3\,a \leq 2 \\
0\leq -2\,x-3\,y+6\,b\phantom{\;+3a\;\,} \leq 1 \\[4pt]
0 \leq x \leq 1 \\   
0 \leq y \leq 2 \\[4pt]
0\leq a \leq 1 \\
0\leq b \leq 1 \tag{*}
\end{cases} Here $x,y$ are unknown variables and $a,b$ are parameters. My attempt. By adding the inequalities with some coeficients I separated the variables and get the simple system \begin{cases}
0 \leq y+6a \leq 5,\\
0 \leq -x+9a+3b\leq 8.\\ \tag{**}
\end{cases} and I am able to solve it. But the solutions of the last system  are not solution of the initial system! Maple and wolframAlpha cant solve the system. Any help? P.S.1 For $a=\frac{63}{100}$ and $b=\frac{59}{100}$ ( as at G Cab's picture  below ) Maple gives the solutions \begin{gather*}
 \left\{ x=1,{\frac{9}{50}}\leq y,y\leq {\frac{11}{25}} \right\} , \left\{ x=-3/2\,y+{\frac{127}{100}},{\frac{9}{50}}<y,y<{\frac{11}{25}} \right\} ,  \left\{ {\frac{9}{50}}<y,x<1,y<{\frac{11}{25}},-3/2\,y+{
\frac{127}{100}}<x \right\} , \left\{ y={\frac{11}{25}},{\frac{61}{100
}}\leq x,x<1 \right\} , \left\{ x=-3/2\,y+{\frac{127}{100}},{\frac{11}
{25}}<y,y<{\frac{127}{150}} \right\} ,  \left\{ {\frac{11}{25}}<y,x<-2
\,y+{\frac{47}{25}},y<{\frac{127}{150}},-3/2\,y+{\frac{127}{100}}<x
 \right\} , \left\{ x=-2\,y+{\frac{47}{25}},{\frac{11}{25}}<y,y<{\frac
{127}{150}} \right\} , \left\{ x=0,{\frac{127}{150}}\leq y,y\leq {
\frac{47}{50}} \right\} , \left\{ y={\frac{127}{150}},x\leq {\frac{14}
{75}},0<x \right\} , \left\{ 0<x,{\frac{127}{150}}<y,x<-2\,y+{\frac{47
}{25}},y<{\frac{47}{50}} \right\} , \\ \left\{ x=-2\,y+{\frac{47}{25}},{
\frac{127}{150}}<y,y<{\frac{47}{50}} \right\}
\end{gather*} P.P.S. I am interested in finding not the whole set of solutions but in finding at least one solution from the area, if it exists,  but in terms $a$ and $b$ , for example, $x=(a+b)/2, y=(2a-b)/10$ is a solution.","['algebra-precalculus', 'systems-of-equations']"
3796725,Prove that there are no integer solutions to $x\left(y^{2}-1\right)=y\left(2+\frac{1}{x}\right)$,"I have struggled on this problem for quite a bit of time now, asked some of my peers and teachers, and I am yet to find the solution. Here is the problem: Prove that there are no integer solutions to the equation $$x\left(y^{2}-1\right)=y\left(2+\frac{1}{x}\right)$$ Here is what I have tried: Expanding, moving things around, factoring (I wasn't able to factor it into something useful) Expanding, converting to a cubic equation (Too difficult to solve) Expanding, converting to a quadratic, using the quadratic formula (I was unable to simplify it enough) It would be great if you guys could help!","['number-theory', 'algebra-precalculus', 'elementary-number-theory', 'diophantine-equations']"
3796740,"Uniqueness of Solutions to First-Order, Linear, Homogeneous, Boundary-Value PDE","Consider a homogeneous, linear, first-order PDE $$L u \equiv \left( \sum_{i = 1}^d f^i(x) \frac{\partial}{\partial x^i} + c(x) \right) u(x) = 0$$ on some compact domain $\Omega \subset \mathbb{R}^d$ .  Obviously this system always has $u = 0$ as a solution; my question is what sorts of conditions on the coefficients $f^i(x)$ and $c(x)$ are sufficient in order to guarantee that the zero solution is unique subject to the boundary condition $u|_{\partial \Omega} = 0$ . I know that well-posedness of first-order PDEs is usually studied via the method of characteristics, but as I understand that's typically useful in thinking of the PDE as an initial value problem in which boundary conditions are specified on an initial-value surface and evolved from there.  Because here I'm treating the system as a Dirichlet problem, the inhomogenous problem $Lu = g$ , $u|_{\partial \Omega} = h$ may not in general be well-posed; but that's OK because I just care about uniqueness of the zero solution to the homogeneous problem. I have one partial result from Oleinik and Radkevic ( https://www.springer.com/gp/book/9781468489675 ), which consider second-order linear PDEs with nonnegative characteristic form, of which the equation I gave above is a special case (since its characteristic form is identically zero).  Then from e.g. Theorem 1.6.2 of this book I can conclude that the zero solution is unique if $c^* < 0$ in $\Omega \cup \partial \Omega$ , where $c^* \equiv c - \sum_{i = 1}^d \partial_i f^i$ is the zero-derivative term of the adjoint $L^*$ of $L$ .  But because the operator $L$ I care about is genuinely a first-order operator, while the condition $c^* < 0$ comes from considering second-order operators, I imagine there must be much more general sufficient conditions for the uniqueness of the zero solution than just $c^* < 0$ .","['linear-pde', 'homogeneous-equation', 'ordinary-differential-equations', 'partial-differential-equations']"
3796937,Prove that $2^{n}+1$ is not a cube of an integer for all $n\in\mathbb{N}$ [duplicate],"This question already has answers here : Prove that $2^n +1$ in never a perfect cube (2 answers) Closed 3 years ago . Prove that $2^n+1$ is not a cube for any $n\in\mathbb{N}$ . I managed to prove this statement but I would like to know if there any other approaches different from mine. If existed $k\in\mathbb{N}$ such that $2^n+1=k^3$ then $k=2l+1$ for some $l\in\mathbb{N}$ . Then $(2l+1)^3=2^n+1 \iff 4l^3+6l^2+3l=2^{n-1}$ . As I am looking for an integer solution, from the Rational Root Theorem $l$ would need to be of the form $2^j$ for $j=1,...,n-1$ . But then $$4(2^j)^3+6(2^j)^2+3\times2^j=2^{n-1} \iff 2^{2j+2}+3(2^{j+1}+1)=2^{n-1-j}$$ the LHS is odd which implies that $j=n-1$ . Absurd. Thank you in advance.","['number-theory', 'algebra-precalculus', 'elementary-number-theory']"
3796943,"Spivak vs uncommon calc texts (Silverman, Lax, Sasane) for a rigorous introductions","Long story short, I'm an aspiring physics person, probably theoretical, looking to study introductory at or approaching the rigor of a math major. Spivak is often recommended, but I was wondering if anyone had experience with the books of Lax, Silverman, or Sasane. I've been out of undergrad for almost a decade but already reviewed and feel totally comfortable with pre-calc. I have no deadlines and can devote as much time as needed to a book, but I want to set off on the right foot. My ideal book, if it exists: Gets to the why . I hate knowing how to do a thing without knowing why it works. Hence the rigor. Should be as as self-study-friendly as possible. As in, I shouldn't have to rely on outside resources to understand problems in the book. Isn't too dry. It helps if the book has some sort of beauty and/or humor. Has some applications. This isn't totally essential, but I would like to some relevance to the real world to keep me entertained. Likewise, I do appreciate historical applications and reasoning for context. The length of the book itself could not matter less to me. If anything, the wordier, probably the better. Spivak seems to be the go-to recommendation. I started it (just on Chapter 2 now) and have liked it, but some of the calc books that most caught my eye are ones with few reviews or impressions. So I was hoping people here might've read one/some of these and compared to spivak. -Calculus with Applications by Lax and Terell(Second Edition, 2014): Strong emphasis on applications, with proofs. -Modern Calculus and Analytic Geometry by Silverman: At first glance, looks great. It's an old book ('69) but seems to have been very well written, with plenty of explanations between proofs. -The How and Why of One Variable Calculus by Sasane: Somewhat similar to the above but is quite a new book (2015ish). I like that it has full solutions to every problem. Any chance someone has read these? Or have another perhaps better-than-spivak recommendation? I think I'll be okay whichever book I end up choosing, but it'd be nice to hear some opinions from people with more experience.","['calculus', 'book-recommendation', 'reference-request']"
3797004,Does a ring isomorphic to its opposite always admit an involution?,"Let $R$ be a ring with identity such that $R\cong R^{\mathrm{op}}$ , then is it necessary that $R$ always admits an involution? An involution is an anti-homomorphism $l\colon R\to R$ such that $l\circ l=\mathrm{id}_R$ .","['ring-theory', 'abstract-algebra']"
3797041,Application of the change of variables theorem on the n-ball.,"Let f: $\mathbb{R} \to \mathbb{R}$ be continuous function and $z \in \mathbb{R}^n$ . Show that $$
\int_Bf(\langle x,z \rangle)=\int_Bf(x_n|z|)
$$ where $x=(x_1,...,x_n)$ and $B=\{x\in \mathbb{R}^n ; |x| \leq 1\}$ . My idea is to apply the change of variables theorem. I tried to find an orthogonal tranformation $h(x)=Qx$ such that $|detDh|=|detQ|=|\pm1|=1$ . But I couldn't find such transformation. I would appreciate any tips on how to find this transformation or a new idea on how to solve this problem.","['multivariable-calculus', 'change-of-variable']"
3797059,How does $\sum_{i=1}^\infty \sum_{j=1}^\infty a_{ij} = \sum_{j=1}^\infty \sum_{i=1}^\infty a_{ij}$ follow from the monotone convergence theorem?,"In Rudin's Real and Complex analysis, he says that the equality $$\sum_{i=1}^\infty \sum_{j=1}^\infty a_{ij} = \sum_{j=1}^\infty \sum_{i=1}^\infty a_{ij}$$ for $a_{i,j} \ge 0$ follows from this corollary of the monotone convergence theorem (via counting measure on a countable set): If $f_n: X \to [0, \infty]$ is measurable and $f = \sum f_n$ , then $$\int_X f =\sum_{n=1}^\infty \int_X fn $$ However, I'm having a hard time seeing this. I'm guessing you use indicator functions for each point in the countable set, but I don't see any obvious manipulations to make it true. Any help would be appreciated.","['measure-theory', 'real-analysis']"
3797100,"Solving a coupled system of linear ODEs (one second order, the other first order)","I have two coupled ODEs for $T(x)$ and $t(x)$ : $$\frac{d^2 T(x)}{d x^2}-\beta (T(x)-t(x))+K=0 \tag 1$$ $$\frac{d t(x)}{dx}-\alpha(T(x)-t(x))=0 \tag 2$$ $\alpha, \beta$ and $K$ are constants $>0$ . Also, it is known that $t(x=0)=t_i$ . Additionally, for $(1)$ we know: $$\frac{d T(x=0)}{d x}=\frac{d T(x=L)}{d x} = 0$$ I need to determine $T(x)$ and $t(x)$ .  Can anyone suggest a way towards moving ahead with this problem ? Probably this system of coupled equations can be solved using the matrix method, but I am not aware of it. I normally solve a single equation using either the method of integrating factor or using a characteristic equation and finding the roots.",['ordinary-differential-equations']
3797155,A differential inequality with boundary values,"Let $f:[0,1] \to \mathbb{R}$ be twice differentiable function on $(0,1)$ such that $f(0)=f(1)=0$ and $f''+2f'+f \ge 0$ Then which of the following values cannot be attained by $f$ ? $(a)\quad \pi$ $(b) \quad e$ $(c) \quad e^{\pi}$ $(d) \quad {\pi}^e$ My first thinking was to take equality with zero . Then $f(x)=(a+bx)e^{-x}$ whefe $a,b$ are arbitary constants With the boundary value , we have $a=0=b$ so $f=0$ . So no conclusiions Again taking the differential equation $f''+2f'+f=e^{\pi x}$ We have the general solution as $f(x)=(a+bx)e^{-x}+\frac{e^{\pi x 
} }{(\pi+1)^2}$ With the boundary values $a=-\frac 1{(\pi+1)^2}$ and $b=\frac{e^{\pi+1}}{(\pi+1)^2}+ 
 \frac 1{(\pi+1)^2} $ But what to conclude from this this? I am totally confused since I am new to this type of problem. Please help me solve this question. Thanks for your time.","['boundary-value-problem', 'inequality', 'ordinary-differential-equations', 'real-analysis']"
3797187,How to find this $\prod_{k=-\infty}^{+\infty}\frac{x^2+(4k+1-y)^2}{x^2+(4k+3-y)^2}$,"let $x,y$ be real numbers,show that $$\prod_{k=-\infty}^{+\infty}\dfrac{x^2+(4k+1-y)^2}{x^2+(4k+3-y)^2}=\dfrac{1+e^{\pi x}-2e^{\frac{\pi}{2}x}\sin{\dfrac{\pi}{2}y}}{1+e^{\pi x}+2e^{\frac{\pi}{2}x}\sin{\dfrac{\pi}{2}y}}$$ This is a question that a physicist asked me. He said that he could prove the equation in a physical way, but not in a mathematical way. I thought about it for a while and might need to use the following conclusion to solve it, but it didn't work out $$\prod_{k=1}^{+\infty}\left(1+\dfrac{1}{k^2}\right)=\dfrac{\sinh{\pi/2}}{\pi/2}$$ ？",['sequences-and-series']
3797245,inclusion map in smooth manifold,"Given smooth manifold $M$ and it's submanifold $S$ (e.g. open subset of $M$ ) we have inclusion map $i:S\to M$ . And we treat $i$ as $i(x) = x$ typically. For example $i:S^n \to \mathbb{R}^{n+1}$ is valid to define $i(x) = x$ But it seems not for example inclusion $i:\mathbb{R}^n \to \mathbb{R}^{n+1}$ as $(x_1,...,x_n) \to (x_1,...,x_n,0)$ So I was a bit confused what is the definition for inclusion here?Should we treat it as $i(x) = x$ ? Is this ""inclusion"" a topological embedding by default setting or not? I found an explanation here","['manifolds', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
3797253,Is Gromov's conjecture about optimal Betti number estimate true for compact manifolds?,"Gromov's conjecture state that: Conjecture: Any complete manifold $(M, g)$ with $\sec\geq 0$ and for any field $\Bbb F$ of coefficients satisfies $$\sum_{i=0}^n b_i(M,\Bbb F)\leq 2^n.$$ Is Gromov's conjecture true for compact non-negatively curved manifolds?","['differential-topology', 'conjectures', 'riemannian-geometry', 'differential-geometry']"
3797262,Is it true that $ \lim_{x\to0}\frac{f'\left(x\right)-\frac{f\left(x\right)-f\left(0\right)}{x}}{x}=\frac{f''\left(0\right)}{2} $ [duplicate],"This question already has answers here : Let f be a function twice differentiable and with derivatives continuous on an interval $[a,b]$ containing $0$. Prove the following statement: (3 answers) Closed 3 years ago . Let $ f $ be a function such that $ f'' $ exists at $ x=0 $ . Is it true that : $$ \lim_{x\to0}\frac{f'\left(x\right)-\frac{f\left(x\right)-f\left(0\right)}{x}}{x}=\frac{f''\left(0\right)}{2} ~~?$$ Im pretty sure that in order for this to be true, $ f'' $ should be continuous, which is not given. But I'm struggling to find a counterexample. I need to find a function that is twice differentiable, but $ f'' $ is not continuous (assuming I understood the situation). I'd appreciate some help. Thanks in advance.","['limits', 'calculus', 'derivatives']"
3797263,Calculate the curvature of torus using the shape operator and check the Gauss-Bonnet theorem $\iint_{S} K(\vec{r}) d S=4 \pi(1-g)$,"I am trying to check the Gauss-Bonnet theorem which says $$\iint_{S} K(\vec{r}) d S=4 \pi(1-g),$$ where $g$ is the genus. For a torus, $g=1$ . I am given a hint that I can get $K$ from the shape operator $$A(\vec{X})=-\left(X^{1} \hat{N}_{1}+X^{2} \hat{N}_{2}\right)$$ $$K\left(\vec{r}\left(u_{1}, u_{2}\right)\right)=\operatorname{det}\left([A]_{B}^{B}\right)$$ while $B$ is the basis for a tangent plane. I think my main problem is to derive the curvature for the torus.","['curvature', 'differential-geometry']"
3797278,prove that sum of lengths of sides of pentagon is less than sum of lengths of diagonals of pentagon,"Let $ABCDE$ be pentagon. Prove that sum of lengths of sides of pentagon is less than sum of lengths of diagonals of pentagon APPROACH 1 I tried using triangle inequality but it does not lead to a proof.One thing i noticed that the statement is not true for quadrilaterals.I proved some extreme cases like when one vertex is collinear with two other vertices. APPROACH 2 If we consider sides of pentagon as vectors,then diagonals are just vector sum of sides.I thought this might help. I ended with the following inequality to prove $\vert{\vec a}\vert+\vert{\vec b}\vert+\vert{\vec c}\vert+\vert{\vec d}\vert+\vert{\vec e}\vert < \vert{\vec a + \vec b}\vert+\vert{\vec c + \vec b}\vert+\vert{\vec c + \vec d}\vert+\vert{\vec d + \vec e}\vert+\vert{\vec e + \vec a}\vert$ Again i stuck here.I want to ask whether my approaches can actually lead to a proof or i may ness some other approach?",['geometry']
3797362,Is list of prime numbers a sequence,"Let $f:\mathbb{N}\to\mathbb{R}$ such that $f(n)=p$ (where $p$ is the $n$ th prime number). My doubt is whether this is a function and hence a sequence.
I got this doubt because we don't know all the primes, right? So after a certain stage, we don't know what is the output of the function.",['sequences-and-series']
3797376,"Prove that $f(W)$ is the graph of $y_{n+1} = \varphi(y_1,\cdots,y_n)$","Let $f: U \subseteq \mathbb R^n \rightarrow \mathbb R^{n+1}$ be of class $C^k,k\geq 1,$ and $U$ open. If for every $x\in U$ , $$f(x) = (f_1(x),\cdots, f_{n+1}(x)) \text{    and   }\det\bigg(\frac{\partial f_i}{\partial x_j}\bigg)_{1\leq i,j \leq n} \neq0,$$ then, for every $x\in U$ , there exists a neighbourhood $W\subseteq U$ of $x$ such that $f(W)$ is the graph of a $C^k$ function $y_{n+1} = \varphi (y_1,\cdots,y_n)$ . Saying that $f(W)$ is the graph of $\varphi$ is the same as saying the following sets are equal: $\{(f_1(x),\cdots,f_{n+1}(x)): x\in W\} = \{((y_1,\cdots,y_n,\varphi(y_1,\cdots,y_n)): (y_1,\cdots,y_n)\in \text{Domain of $\varphi$}\}$ . In other words, I have to prove that locally there is a function $\varphi$ depending on the previous coordinates $f_1,\cdots,f_n$ . The first thing that I've noticed is that $f'(x)$ is an injective linear transformation. Indeed, we have $n = \dim \ker f'(x) + \dim \text{im}f'(x) \geq n + \dim \ker f'(x) \geq n \implies \dim\ker f'(x) =0,$ since $f'(x)$ has at least $n$ linearly independent lines. Now I don't know how to proceed exactly. Initially, I was wondering of using the local immersion theorem (since $f'(x)$ is injective), but I couldnt see a way to use this theorem to express $f_{n+1}$ in terms of the others. I also considered the function $\pi:\mathbb R^{n+1} \rightarrow \mathbb R^n, (x_1,\cdots, x_{n+1}) \mapsto (x_1,\cdots, x_n).$ So, $\pi(f(x)) = (f_1(x),\cdots,f_n(x)).$ Writing $g = \pi \circ f$ , its derivative $g'(x)$ is invertible, hence, it is a local diffeomorphism with inverse $h$ . Therefore, $\pi \circ f \circ h = g \circ h = I_d$ and we have $\pi(f(h(x_1,\dots,x_n) ) = (x_1,\cdots,x_n).$ If I could ""get rid"" of $\pi$ somehow, this equation would give me that $f(h(x_1,\cdots,x_n)) = (x_1,\cdots,x_n,\varphi(x_1,\cdots,x_n))$ and thats what whe need to show. But I cant find a clear way to say or jusitfy this. Any insight, hint? Thank you.","['multivariable-calculus', 'real-analysis']"
3797446,What is constraining the Kullback-Leibler divergence to not be infinitely negative?,"I'm trying to understand t-SNE, based on this video , and everything is clear except for one key part of the logic right at the time point linked to above: namely, minimizing the Kullback-Leibler divergence. The stated problem is that we want to choose $ q_{ij} $ such that $$
\sum\limits_{i}\sum\limits_{j \neq i} p_{ij} {\rm log} \left( \frac{p_{ij}}{q_{ij}} \right)
$$ is as small as possible, and $$
q_{ij}=\frac{(1+|y_i-y_j|^2)^{-1}}{ \sum\limits_{k} \sum\limits_{l\neq k} \left( 1+|y_{k}-y_{l}|^2 \right)^{-1}}
$$ (I'm a little unsure about those $^{-1}$ exponents in the expression, but I'm copying what's in the video)( Edit: they're right. The quantities are weighted inversely to squared distances ) I understand that $q_{ij}$ is constrained to the range $0..1$ , but ... these aren't probabilities ( Edit: They actually are referred to as ""probabilities of choosing each other as neighbours"" ), they're just normalized distances, so why is this a sensible comparison? does this just work for any set of numbers that are confined to the 0..1 range and which add up to one? (I updated this question because I missed something obvious, but I'm still stuck. Thanks for any help you can offer)","['entropy', 'probability']"
3797447,Closedness in scheme theory,"Let $X$ be a scheme and $(U_i)_{i\in I}$ an open covering, (1) Given a subset $Z\subset X$ , why does $Z\cap U_i$ is closed for all $i$ implies that $Z$ is closed? (2) Let $A$ be a ring. Given a morphism schemes $f:X\rightarrow \operatorname{spec} A$ such that $f_Y:X \times_{\operatorname{spec}A} Y\rightarrow Y$ is closed for all affine $A$ schemes $Y$ , does this imply that $f$ is universally closed?","['algebraic-geometry', 'schemes']"
3797449,Axiomatic definition of groups,"To define natural numbers one can either: use the Peano axioms in second-order logic; encode them in set theory as von Neumann ordinals . The relation between those two definitions of natural numbers is that
(2) satisfies (1). Now, groups are usually defined as sets equipped with operations and axioms . This is akin to (2) above since it is an encoding of groups in set theory. What would be a definition of groups akin to (1)?","['foundations', 'axioms', 'model-theory', 'category-theory', 'group-theory']"
3797520,Elliptic curves and scheme theory,"Recall that an elliptic curve over a field $k$ i.e a proper smooth connected curve of genus $1$ equipped with a distinguished $k$ -rational point, i'll be really grateful for any help in understanding the following part of our course (1) Why does $E$ , the closure of the vanishing locus of the equation (4), define an irreducible algebraic curve? (2) Why is $E$ smooth over $k$ if and only if $E-\{0\}$ is smooth over $k$ ? (3) Given an affine and smooth Weirstrass equation, why does its shematic closure in $\mathbb{P}_k^2$ define an elliptic curve? It is stated that this is consequence of Bezout theorem but i failed to understand how (4) Let $(E,0)$ be an elliptic curve, using Riemann-Roch we construct an isomorphism into $\mathrm{Proj}\,k[X,Y,Z]/Y^2Z+a_1XYZ+a_3YZ^2-X^3-a_2X^2Z-a_4XZ^2-a_6Z^3$ , why does $0$ map to the infinity point $O=[0:1:0]$ ?","['algebraic-geometry', 'schemes', 'elliptic-curves']"
3797536,How to compute a $\sum_{n=0}^{\infty}{\frac{2^n}{(2n+1){2n\choose n}}}$,"I need help with the following excersise: Evaluate $$\sum_{n=0}^{\infty}{\frac{2^n}{(2n+1){2n\choose n}}}\\\text{Hint: Use identity}\int_0^{\pi/2}{\sin^{2k+1}x\;dx}=\frac{2^{2k}k!^2}{(2k+1)!}$$ My attempt: $$\sum_{n=0}^{\infty}{\frac{2^n}{(2n+1){2n\choose n}}}=\sum_{n=0}^{\infty}{\frac{2^n}{(2n+1){\frac{2n!}{(2n-n)!n!}}}}\\=\sum_{n=0}^{\infty}{\frac{2^nn!^2}{(2n+1)!}}=\sum_{n=0}^{\infty}{\frac{2^n2^nn!^2}{2^n(2n+1)!}}=\sum_{n=0}^{\infty}{\frac{2^{2n}n!^2}{2^n(2n+1)!}}$$ Applying the identity $$\sum_{n=0}^{\infty}{\frac{2^{2n}n!^2}{2^n(2n+1)!}}=\sum_{n=0}^\infty{\frac{1}{2^{2n}}\int_0^{\pi/2}{\sin^{2n+1}x\;dx}}$$ And here I am stuck, since I am not sure if I can do any change regarding the sum and integral, any help or tips is helpful. Thanks!","['integration', 'calculus', 'summation']"
3797564,Eigenvalues and null space,"I would like to understand the relationships between the null space and the eigenvalues of a matrix better. First of all, we know that an $n \times n$ matrix will have $n$ eigenvalues, though the eigenvalues can be complex and repeated. Next, we know that if $A$ has the eigenvalue 0, then the corresponding eigenvector is in the null space $N(A)$ , since $A\textbf{x}=0\textbf{x}=\textbf{0}$ . This implies that all eigenvectors that correspond to the eigenvalue 0 exactly span $N(A)$ . Using the above-mentioned two conclusions, and assume we have an $n \times n$ matrix with rank $r$ , now we know the dimension of the null space is $n-r$ . From this, can we conclude that there will be at least $n-r$ eigenvalues that equal to 0? and exact $n-r$ independent eigenvectors to span the null space?","['matrices', 'linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
3797602,Spivak's Calculus 5-15-vi $\lim_{x \rightarrow 0}\frac{\tan^2(x)+2x}{x+x^2}$,Evaluate the following in terms of $\alpha = \displaystyle \lim_{x \rightarrow 0}\frac{\sin x}{x}:$ $$\lim_{x \rightarrow 0}\frac{\tan^2 (x)+2x}{x+x^2}$$ I'm stuck on this one. I've tried using $\tan x=\frac{\sin x}{\cos x}$ followed by $\cos^2(x)=1 - \sin^2(x)$ to get everything in terms of $x$ and $\sin$ . Then I tried in terms of $x$ and $\cos$ (since $\cos$ is on the denominator). Also tried partial fractions. Help.,"['limits', 'calculus', 'trigonometry']"
3797636,The differences between Lagrange and Leibniz's derivative notations,"One problem I have found when learning calculus is that there are many different ways to denote the derivative. If $y=f(x)=x^2$ , then we could write \begin{align}
f'(x)&=2x \\
y'&=2x \\
\frac{df}{dx}(x)&=2x \\
\frac{df(x)}{dx}&=2x \\
\frac{d}{dx}f(x)&=2x \\
\frac{dy}{dx}&=2x
\end{align} And this is just Lagrange and Leibniz's notations alone. What I find troubling is that they all seem to be suggesting subtly different things about what the derivative actually is . Is it a function, a limit of a quotient, or both? In the interests of keeping my post brief, I'll focus my attention on $f'(x)=2x$ and $\frac{dy}{dx}=2x$ , as these seem to be the most common notations. $$ f'(x)=2x $$ It does make sense to think of the derivative as the gradient function: $$
f'\colon x\mapsto\lim_{\Delta x \to 0}\frac{f(x+\Delta x)-f(x)}{\Delta x}
$$ In this case the limit expression is equal to $2x$ , and so we can write $$
f' \colon x \mapsto 2x
$$ However, this notation seems a little counter-intuitive when we consider what it means to differentiate a function with respect to a variable other than $x$ . If I ask what is the derivative of $f(x)$ with respect to $\frac{x}{2}$ , does this question make sense? Is it simply $f'(\frac{x}{2})$ ? Or do we have to express $x^2$ in terms of $\frac{x}{2}$ ? And how can we can express this derivative using Lagrange's notation? $$ \frac{dy}{dx}=2x $$ There are many things which are nice about Leibniz's notation, including the fact that it is explicit which variable you are differentiating with respect to. However, in this case, it is unclear whether we are talking about a function, or something else entirely. There are other issues. Some people say they dislike the Leibniz formulation of the chain rule $$
\frac{dy}{dx}=\frac{dy}{du}\frac{du}{dx}
$$ saying that they find it to be inaccurate. I don't really understand why this is the case. Could someone please elaborate?","['notation', 'derivatives', 'terminology']"
3797643,What is the Euler characteristic of a square? (Confusion with Gauss-Bonnet theorem),"Highschooler here, trying to learn about the Euler characteristic, Gaussian curvature and the Gauss-Bonnet theorem linking them together. As per the Gauss-Bonnet theorem: total curvature $= 2 \pi \times$ euler characteristic. Here's my confusion. A square (for example a flat sheet of paper) has a Gaussian curvature of zero. But following the formula $\chi = V - E + F$ , I calculate that a square's Euler characteristic is $1$ . This is because vertices $V = 4$ , edges $E = 4$ and faces $F = 1$ . Therefore $\chi = 4 - 4 + 1\Rightarrow \chi = 1$ . So I get the equation $0 = 2\pi  1$ , i.e. $0 = 2\pi$ . Where is my mistake?","['curvature', 'differential-geometry']"
3797659,How to show $f^{-1}(0) \subset \mathbb{R}^m \times \mathbb{R}^n$ is smooth,"Let $f : \mathbb{R}^m \times \mathbb{R}^n \to \mathbb{R}^m$ be a smooth map. I'm wondering about smoothness of the set $f^{-1}(0)$ under the following conditions: (1) For all $x \in \mathbb{R}^m$ , $f^{-1}(0) \cap (\{x\} \times \mathbb{R}^n)$ is smooth of dimension $r$ (if non-empty). The dimension stays the same for all $x$ . (2) For all $(x, y) \in f^{-1}(0)$ the restriction of the projection $\mathbb{R}^m \times \mathbb{R}^n \to \mathbb{R}^m$ to the kernel of $df_{(x,y)}$ is surjective. In other words, for all $u \in \mathbb{R}^m$ there exists $v \in \mathbb{R}^n$ such that $df_{(x,y)}(u, v) = 0$ . Why is $f^{-1}(0)$ a smooth submanifold of $\mathbb{R}^m \times \mathbb{R}^n$ ? (Maybe it isn't, but I've seen that claim somewhere, that's why I'm asking.) I'm trying to apply the transversality theorem, but it's not obvious how since $df_{(x,y)}$ may not be surjective. Intuitively, by (1), non-smoothness can only arise in the transverse directions $\mathbb{R}^m \times \{y\}$ but because $\ker df_{(x,y)} \to \mathbb{R}^m$ is surjective that cannot happen.","['smooth-manifolds', 'differential-geometry']"
3797677,How to evaluate $\int _0^1\frac{\ln ^2\left(1-x\right)\ln ^5\left(1+x\right)}{1+x}\:dx$,"Before you think I haven't tried anything, please read. I've been trying to evaluate $$\int _0^1\frac{\ln ^2\left(1-x\right)\ln ^5\left(1+x\right)}{1+x}\:dx$$ But I can't find a way to simplify it. Integration by parts is not valid since we face convergence issues. Subbing the $1-x$ term is also not quite helpful here. I also tried to use the sub $\frac{1}{1+x}$ but this is not useful either. Using algebraic identities isn't that useful either, since we run into similar difficulty integrals. Since these attempts do not lead to anything, how can I approach it?","['integration', 'definite-integrals', 'harmonic-numbers', 'polylogarithm', 'closed-form']"
3797711,How to approach $\sum _{n=1}^{\infty } \frac{16^n}{n^4 \binom{2 n}{n}^2}$?,@User mentioned in the comments that $$\sum _{n=1}^{\infty } \frac{16^n}{n^3 \binom{2 n}{n}^2}=8\pi\text{G}-14 \zeta (3)\tag1$$ $$\small{\sum _{n=1}^{\infty } \frac{16^n}{n^4 \binom{2 n}{n}^2}=64 \pi  \Im(\text{Li}_3(1+i))+64 \text{Li}_4\left(\frac{1}{2}\right)-233 \zeta(4)-40  \ln ^2(2)\zeta(2)+\frac{8}{3}\ln ^4(2)}\tag2$$ I was able to prove $(1)$ but had some difficulty proving $(2)$ . Any idea? I am going to show my proof of $(1)$ hoping it helps you prove $(2)$ : We showed in this question that $$\sum_{n=1}^\infty\frac{4^ny^n}{n^2{2n\choose n}}=2\int_0^y \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}dx$$ multiply both sides by $\frac{1}{y\sqrt{1-y}}$ then $\int_0^1$ with respect to $y$ and use $\int_0^1\frac{y^{n-1}}{\sqrt{1-y}}dy=\frac{4^n}{n{2n\choose n}}$ we obtain $$\sum _{n=1}^{\infty } \frac{16^n}{n^3 \binom{2 n}{n}^2}=2\int_0^1\int_0^y \frac{\arcsin \sqrt{x}}{y\sqrt{x}\sqrt{1-x}\sqrt{1-y}}dxdy$$ $$=2\int_0^1\frac{\arcsin\sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(\int_x^1\frac{dy}{y\sqrt{1-y}}\right)dx$$ $$=2\int_0^1\frac{\arcsin\sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(2\ln(1+\sqrt{1-x})-\ln x\right)dx$$ $$\overset{\sqrt{x}=\sin \theta}{=}8\int_0^{\pi/2}x\ln(1+\cos x)dx-8\int_0^{\pi/2}x\ln(\sin x)dx$$ $$=8\int_0^{\pi/2}x\ln(2\cos^2\frac x2)dx-8\int_0^{\pi/2}x\ln(\sin x)dx$$ $$=32\int_0^{\pi/4}x\ln(2\cos^2x)dx-8\int_0^{\pi/2}x\ln(\sin x)dx$$ $$=32\underbrace{\int_0^{\pi/4}x\ln(2)dx}_{\frac3{16}\ln(2)\zeta(2)}+64\underbrace{\int_0^{\pi/4}x\ln(\cos x)dx}_{\frac{\pi}{8}\text{G}-\frac3{16}\ln(2)\zeta(2)-\frac{21}{128}\zeta(3)}-8\underbrace{\int_0^{\pi/2}x\ln(\sin x)dx}_{\frac7{16}\zeta(3)-\frac34\ln(2)\zeta(2)}$$ $$=8\pi\text{G}-14 \zeta (3)$$ The last two integrals follow from using the Fourier series of $\ln(\cos x)$ and $\ln(\sin x)$ . All approaches are appreciated. Thank you. Addendum: Here is an easier way to prove $(1)$ : We have $$\arcsin^2(x)=\frac12\sum_{n=1}^\infty\frac{(2x)^{2n}}{n^2{2n\choose n}}$$ or $$\sum_{n=1}^\infty\frac{4^nx^n}{n^2{2n\choose n}}=2\arcsin^2(\sqrt{x})$$ Divide both sides by $x\sqrt{1-x}$ then $\int_0^1$ and use $\int_0^1\frac{x^{n-1}}{\sqrt{1-x}}dx=\frac{4^n}{n{2n\choose n}}$ we have $$\sum_{n=1}^\infty\frac{16^n}{n^3{2n\choose n}^2}=2\int_0^1\frac{\arcsin^2(\sqrt{x})}{x\sqrt{1-x}}dx$$ $$\overset{\sqrt{x}=\sin x}{=}4\int_0^{\pi/2}x^2 \csc(x)dx$$ $$\overset{IBP}{=}-8\int_0^{\pi/4} x\ln(\tan\frac x2)dx=8\pi\text{G}-14\zeta(3)$$ where the last result follows from the Fourier series of $\ln(\tan\frac x2)$ .,"['integration', 'real-analysis', 'harmonic-numbers', 'polylogarithm', 'sequences-and-series']"
3797746,Discuss existence and uniqueness for a Cauchy problem,"I don't know what's happening with this exercise. I need a help becuase I'm quite puzzled. Consider the Cauchy problem \begin{cases}
y'=\frac{2}{t} y + 2 t \sqrt{y} \\
y(1)=0
\end{cases} Study the existence and uniqueness Here $$f(t,y)=\frac{2}{t} y + 2 t \sqrt{y}$$ Since $y\geq0$ (I have the square root), I consider as open neigbourhood $K = \{t: |t-1|< r_1 \} \times \{y: 0 < y < r_2 \}$ , but in this way I am in trouble with $$f_y(t,y)= \frac{2}{t} + \frac{t}{\sqrt{y}}$$ because it's discontinuous at $y=0$ . So I should look for a weaker condition as Lipschitz continuity: I take $(t,y_1)$ and $(t,y_2)$ in $K$ : $$|\frac{2}{t} \bigl(y_1 - y_2 \bigr) + 2t \bigl( \sqrt{y_1} - \sqrt{y_2} \bigr)|  \leq |\frac{2}{t} \bigl(y_1 - y_2 \bigr)| + |2t \bigl( \sqrt{y_1} - \sqrt{y_2} \bigr)| $$ but the second term of the inequality is quite problematic: it is like proving that $x \mapsto \sqrt{x}$ is Lipschitz for $x\geq0$ , which is known to be false. So, I can't apply the theorem actually...Am I wrong? If so, what are my mistakes?","['cauchy-problem', 'lipschitz-functions', 'ordinary-differential-equations', 'real-analysis']"
3797837,Find exact confidence interval for uniform distribution,"In my homework I have $X_1,...,X_n$ which are all uniformly distributed on $(0,\theta)$ I have concluded that the $MLE=\hat{\theta}=max(X_1,...,X_n)$ because: $L_x(\theta)= \frac{1}{\theta^n}$ so the likelihood is a decreasing function. However we also know that $\theta > \max\left(X_1,...,X_n\right)$ , hence $\hat{\theta}=\max\left(X_1,...,X_n\right)$ But I need to find an exact $95$ % confidence interval for $\theta$ . I think the book wants me to find a pivot I have tried to set $P\left(X \in \left(0,X_n\right)\right)=\left(\frac{X_n}{\theta}\right)^n$ for some $\theta>X_n$ Therefore $P(X \in (X_n,\theta))=1-\left(\frac{X_n}{\theta}\right)^n=0.95$ which gives me $\theta=\frac{X_n}{0.05^{1/n}}$ However I am pretty sure this is not the way to go and it is not a standard method. There should be some more standard method but using $L,\ell_X, \ell_X'=0$ just gives me MLE=0 which is of no use Any help/hint would be appreciated","['confidence-interval', 'statistics', 'uniform-distribution']"
3797852,Proving Tonelli's Theorem for $n$ Factors,"Am trying to prove the following extension of Tonelli's theorem: Proposition. Let $(\Omega_j,\mathcal{A}_j,\mu_j)$ $j=1,\dots,n$ be $\sigma$ -finite measure spaces. Let $f\to[0,\infty]$ be an $\mathcal{A}_1\otimes \dots\otimes\mathcal{A}_n$ measurable function on $\Omega_1\times\dots\times\Omega_n$ . Then for every permutation $j_1,\dots,j_n$ of $1,\dots,n$ we have $$\int f(\omega_1,\dots,\omega_n) \,d (\mu_1 \otimes \dots \otimes \mu_n)=\int \dots \int f(\omega_1,\dots,\omega_n)\,d\mu_{j_1}\dots d\mu_{j_n}$$ where each integral on th RHS is measurable with respect to the product of the $\mathcal{A}_j$ corresponding to coordinates in which integration has not yet occured. My book says it's a simple induction but somehow my proof seems complicated. I believe it is sufficient to consider the case of the identity permutation. This is because we have the equality $$\int f(\omega_1,\dots,\omega_n) \,d (\mu_1 \otimes \dots \otimes \mu_n)=\int f(\omega_{1},\dots,\omega_{n}) \,d (\mu_{j_1} \otimes \dots \otimes \mu_{j_n})$$ see here . In other words, it does not matter whether we regard $f$ as a function on $\Omega_1\times\dots\times\Omega_n$ or on $\Omega_{j_1}\times\dots\times\Omega_{j_n}$ . Is this correct? The comments seem to indicate that are multiple possible approaches here. Any proof outline is greatly appreciated.","['measure-theory', 'lebesgue-integral', 'real-analysis', 'measurable-functions', 'fubini-tonelli-theorems']"
3797887,"If $T_t$ is the flow generated by the autonomous velocity $v$ and $\left.v\right|_{\partial\Omega}=0$, then $T_t(\partial\Omega)=\partial\Omega$","Let $d\in\mathbb N$ and $v\in C_c^\infty(\mathbb R^d,\mathbb R^d)$ . We know that, for any $\tau>0$ , there is a unique solution $X^x\in C^0([0,\tau],\mathbb R^d)$ of \begin{align}X'(t)&=v(X(t))\tag1&\text{for all }t\in[0,\tau],\\X(0)&=x\end{align} for all $x\in\mathbb R^d$ . It is easy to show that $$T_t(x):=X^x(t)\;\;\;\text{for }t\in[0,\tau]$$ is a $C^1$ -diffeomorphism from $\mathbb R^d$ onto $\mathbb R^d$ . Now let $\Omega\subseteq\mathbb R^d$ . How can we show that, if $\left.v\right|_{\partial\Omega}=0$ , then $$T_t(\partial\Omega)=\partial\Omega\tag2$$ for all $t\in[0,\tau]$ ? if $\Omega$ is closed or open, then $$T_t(\Omega)=\Omega\tag3$$ for all $t\in[0,\tau]$ ? It's clear to me that any homeomorphism maps boundary (interior) points to boundary (interior) points. I guess we need to use this somehow. EDIT : From the comments it is clear that $(2)$ holds, since it should generally hold that if $B$ is any subset of $\mathbb R^d$ with $\left.v\right|_B=0$ , then $T_t(x)=x$ for all $x\in B$ . But how can we prove $(3)$ ? EDIT 2 : If $f$ is any homeomorphism between topological spaces $E_1$ and $E_2$ and $B_1\subseteq E_1$ , then we know that $f(B_1^\circ)=f(B_1)^\circ$ , $f(\partial B_1)=\partial f(B_1)$ and $f(\overline{B_1})=\overline{f(B_1)}$ . If $B_1$ os open, then $B_1=B_1^\circ$ and if $B_1$ is closed, then $B_1=\overline{B_1}$ . I think we need to use this for $(3)$ . EDIT 3 : Let $x\in\Omega^\circ$ . Then there is a $\varepsilon>0$ with $B_\varepsilon(x)\subseteq\Omega$ . Maybe we can at least show that there is a $t\in[0,\tau]$ (sufficiently small) such that $\left\|X^x(s)-x\right\|<\varepsilon$ for all $s\in[0,t]$ . Then it would follow that $$T_s(\Omega^\circ)\subseteq\Omega^\circ\;\;\;\text{for all }s\in[0,t].\tag4$$ From pure intuition, for a sufficiently small $t$ , the velocity should not be able to move the point $x$ outside of the ball $B_\varepsilon(x)$ . So, $(4)$ should hold. (How would we need to argue that it must even be an equality? This seems trivial, by bijectivity though.)","['diffeomorphism', 'numerical-methods', 'general-topology', 'differential-topology', 'differential-geometry']"
3797897,Expected Value of flipping through a book,"Suppose that a book has $N$ pages, and we read through the book as follows. We start from page 0, and if we're on page $i$ , we randomly flip to a page $i + 1, i + 2, ..., N$ with equal probability. What is the expected value of number of flips we need to finish the book? Intuition tells me that we can, on average, expect to halve the number of pages remaining.  This  yields $\log_2(N)$ , but I'm having trouble formalizing it. If $N = 26$ , what is the probability we flip to page 13 at some point? Assume we start at page 0. I let $P_i$ be the probability we land on page 13 eventually, starting from page $i$ . Then, $P_{13} = 1$ , and in general, $$P_{i} = \frac{1}{26 - i}\sum_{k = i + 1}^{13}P_k$$ Evaluating terms like $P_{12}, P_{11}, P_{10}$ , I see that all of these values are $\frac{1}{14}$ , including $P_0$ . Is there a more intuitive reason for such a simple answer?","['expected-value', 'probability']"
3797900,Infinite group acting differentiably on continuous space,"I am interested in infinite groups $G$ that act on $\mathbb{R}^n$ , with some informal properties: For each $g \in G$ , the map $x \mapsto gx$ is smooth and differentiable with respect to $x$ . For each $x \in \mathbb{R}^n$ , the map $g \mapsto gx$ is smooth and differentiable with respect to $g$ . Is there a standard concept that is a reasonable match to this or way to formalize this? Should I perhaps be looking at Lie groups?  I confess I've tried learning about them and the examples look like a match but even understanding the definition of a Lie group requires machinery that I am not familiar with. I realize this is specified in a somewhat vague and imprecise way.  I have the sense that my situation falls within some well-studied mathematical concept, but I'm not entirely sure what.","['mathematical-modeling', 'multivariable-calculus', 'group-theory', 'lie-groups', 'differential-geometry']"
3797988,"Showing that $S_n -\lfloor S_n \rfloor \sim U[0,1]$","$k \in \mathbb{N}$ is fixed $(X_n)_{n \geq 1}$ are all independent and follow an uniform law on $[0,k]$ We define $f(x)=x -\lfloor x \rfloor$ $S_n= \sum_{i=1}^{n} X_i$ $Z_n= f(S_n)$ We want to show that $\forall n \geq 1, S_n -\lfloor S_n \rfloor  \sim U[0,1]$ Here are the steps : I have found a density of $S_2$ Show that $Z_2 \sim U[0,1]$ 3.(a) Express $f(f(S_n) + X_{n+1})$ with $Z_{n+1}$ 3.(b) Deduce that $Z_n \sim U[0,1]$ My attempt: 1. $f_{S_2}(s)=
\begin{cases}
\frac{1}{k^2} s  \quad \text{si}  \quad  0 \leq s\leq k \\
\frac{1}{k} (2-\frac{s}{k}) \quad \text{si}  \quad k  \leq s \leq 2k\\
\end{cases}
$ $F_{S_2}(s)=
\begin{cases}
\frac{s^2}{2 k^2}  \quad \text{si}  \quad 0 \leq s\leq k \\
2\frac{s}{k}-\frac{s^2}{2 k^2} -1 \quad \text{si}  \quad  k  \leq s \leq 2k\\
\end{cases}
$ For this question, let $Z=Z_2$ $0\leq Z \leq 1 $ For $a \leq 1$ $0\leq Z \leq a \iff Z \in  \bigcup_{j=0}^{j=k-1} [j,j+a]$ $F_Z(a)= \sum_{j=0}^{j=2k-1} F(j+a)-F(j)$ $
\begin{align*}
f_Z(a) &= \sum_{j=0}^{j=2k-1} f_S(a+j)  \\
&= \sum_{j=0}^{j=k-1} f_S(a+j) + \sum_{j=k}^{j=2k-1} f_S(a+j) \\
&= \sum_{j=0}^{k-1} \big( \frac{a}{k^2} + \frac{j}{k^2} \big) + \sum_{j=k}^{2k-1} \big( \frac{2}{k} - \frac{a}{k^2} - \frac{j}{k^2}) \\
&=  \big( \sum_{j=0}^{k-1}  \frac{a}{k^2} - \sum_{j=k}^{2k-1}\frac{a}{k^2}  \big)
+  \sum_{j=0}^{k-1} \frac{j}{k^2} -  \sum_{j=0}^{k-1} \frac{j+k}{k^2} + \sum_{j=k}^{2k-1}   \frac{2}{k} \\
&= -1 +\sum_{j=k}^{2k-1}  \ \frac{2}{k} \\
&=-1+2=1\\
\end{align*}
  $ 3. $f ( f(S_n) + X_{n+1})= f( S_n - \lfloor S_n \rfloor + X_{n+1} )$ Let $Z_n= S_n - \lfloor  S_n\rfloor $ $S_{n+1} = S_n+ X_{n+1}  =  Z_n +  \lfloor  S_n\rfloor + X_{n+1}$ $ S_{n+1} -  \lfloor  S_{n+1}\rfloor = f(  Z_n + X_{n+1} )$ because $f(x+p)=f(x)$ for all integer $p$ so : $f ( f(S_n) + X_{n+1}) = Z_{n+1}$","['ceiling-and-floor-functions', 'probability-distributions', 'uniform-distribution', 'probability']"
3798018,How would you calculate a derivative of $ f(x)= \frac{\sqrt{x+1}}{2-x}$ by the limit definition?,"I have a function defined as follows: $$
f(x)= \frac{\sqrt{x+1}}{2-x}
$$ I tried to calculate the derivative using the limit definition using four methods, but I was unsuccessful in any. Could someone help me calculate it and explain the method? $$
1) \lim_{h\to 0} =\frac{\frac{\sqrt{(x+h)+1}}{2-(x+h)}-\frac{\sqrt{x+1}}{2-x}}h
$$ $$
2)\lim_{z\to x} =\frac{\frac{\sqrt{z+1}}{2-z}-\frac{\sqrt{x+1}}{2-x}}{z-x}
$$ $$
3)\;f(x)= \frac{\sqrt{x+1}}{2-x}; u=\sqrt{x+1}
$$ $$
\lim_{h\to 0} =\frac{\frac{u+h}{3-(u+h)^2}-\frac{u}{3-u^2}}h
$$ $$
4)\;f(x)= \frac{\sqrt{x+1}}{2-x}; u={x+1};
$$ $$
\lim_{h\to 0} =\frac{\frac{\sqrt{u+h}}{3-(u+h)}-\frac {\sqrt{u}}{3-u}}h
$$","['limits', 'derivatives']"
3798028,Prove that $f(x) = \frac{5x^{2}}{1 + x^{2}}$ is bounded,"I have only had experience bounding functions over certain ranges of x, so I am having trouble trying to show it is true for all x. I would appreciate any guidance or tips for these types of problems as I feel  my proof is somewhat trivial and not rigorous. Proof. To show that $\frac{5x^{2}}{1 + x^{2}}$ is bounded we show that there exists an $M \in \mathbb{R}$ such that $$\mid \frac{5x^{2}}{1 + x^{2}} \mid \leq M  \: \text{for all $x \in \mathbb{R}$}$$ As $|5x^{2}|$ is already bounded from above, we seek to bound $|1 + x^{2}|$ from below. Using the triangle inequality, $|1 + x^{2}| \geq |1| + |x^{2}| > |x^{2}|$ . Thus, $M = |\frac{5x^{2}}{x^{2}}| = |5| = 5$ $\square$","['functions', 'solution-verification']"
3798042,"Can I claim that the range of $\sin(x)$ in the given equation must be necessarily $[-1,1]$?","I have this equation $$a \sin (x)+b \cos (x)+c=0$$ I want to obtain $\sin (x)$ in terms of other parameters. Substituting $\;\cos (x)=\sqrt{1-\sin ^2(x)}$ , and then, solving the quadratic equation, I get $$\sin (x)=\frac{-a c\pm \sqrt{a^2 b^2+b^4-b^2 c^2}}{a^2+b^2}$$ My question is since I have substituted $\;\cos (x)=\sqrt{1-\sin ^2(x)}$ in the original equation, then, can I claim that the range of the result must be necessarily in $[-1,1]$ ? Since when I evaluate the result for random parameters, I see that sometimes it gives imaginary values, and, in those cases, the real part is greater than $1$ and smaller than $-1$ .","['trigonometry', 'real-analysis']"
3798089,Showing $\langle \forall x:: P.x \rangle \land \langle \exists x:: Q.x \rangle \implies \langle \exists y: P.y : Q.y \rangle $ is a theorem.,"A possible proof using a Natural Deduction system like Fitch is not difficult. But, I would like to prove it using Equational Logic. This system uses common rules of propositional logic like DeMorgan, etc..., and Predicate logic rules used in books like: A Logical Approach to Discrete Math (David Gries). Complete book can be found here: https://link.springer.com/book/10.1007/978-1-4757-3837-7 List of rules on page 503. Programming in the 1990s (Edward Cohen). My proof starts as follows: I assume the antecedent holds $$\langle \forall x:: P.x \rangle \land \langle \exists x:: Q.x \rangle$$ and try to reach $$\langle \exists x:: P.x \land Q.x \rangle$$ Attempted proof: $$
\begin{align*}
  & \langle \forall x:: P.x \rangle \land \langle \exists x:: Q.x \rangle\\
  \implies & \{\text{Instantiation } (\forall)\}\\
  & P.a \land \langle \exists x:: Q.x \rangle \\
  \equiv & \{\text{Distributivity of} \land \text{over} \exists \} \\
  & \langle \exists x:: P.a \land Q.x \rangle 
\end{align*}
$$ Of course, the proof is not complete. Does someone know if this proof is possible in this context ?","['alternative-proof', 'predicate-logic', 'logic', 'discrete-mathematics']"
3798229,"Given $a,b\in\{1,2,3,4,5,6,7,8,9,10\}$ and $b>\frac{a^4}{a^2+1}$, prove $b\geq a^2$","Given that $$ a,b \in \{ 1,2,3,4,5,6,7,8,9,10 \}$$ and $$b> \frac{a^4}{a^2+1}$$ How can I prove $b\geq a^2$ since I'm looking for all possible values of $(a,b)$ (and I actually know all though by some brute force) ? So far I can go to the canonical form of the original inequality is this $$b> a^2-1+ \frac{1}{a^2+1}.$$ Any help will be much appreciated :) PS: I already solve this the way I wanted and I've seen my mistakes as well. Thanks to all who helped me and who edited my problem especially to @quasi. I'm SO satisfied rn since it's actually part of a more intricate probabilistic problem. I know it's kind of unfair but I'm more comfortable of my own solution and I put it below...","['algebra-precalculus', 'functional-inequalities', 'inequality']"
3798265,Primal and Dual Solution not same,"Suppose, we have following convex optimization problem: $$\
\min_x\psi(x) \  \ s.t. \ \phi(x) \leq 0
$$ We can write the primal problem as: $$\
\min_x\max_{\lambda\geq 0} \psi(x) + \lambda\phi(x)
$$ and the dual problem as: $$\
\max_{\lambda\geq 0}\min_x \psi(x) + \lambda\phi(x)
$$ Let's say $(\bar x, \bar \lambda)$ is the solution to primal problem and $(x^*, \lambda^*)$ solution to dual problem. Assuming $\psi$ and $\phi$ are not strictly convex, primal solution need not be the same as dual solution i.e. $x^* \neq \bar x$ and $\lambda^* \neq \bar\lambda$ . However strong duality tell us that, $\mathcal L (\bar x, \bar \lambda) = \mathcal L(x^*, \lambda^*)$ . Can you point me to some interesting example for the same or give an intuition? I understand the problem does not have unique saddle point and hence this issue. I still have conceptual doubts, will $(x^*, \lambda^*)$ not satisfy the K.K.T conditions for the original constrained optimization problem? Edit: I am confused because often people talk about methods for getting primal solution from the dual solution. Paper Page 56 (Applying Minimax theorem line) tells that the approximate solution is in the convex hull of dual iterates. Well my question is why not take the last iterate of dual?","['convex-optimization', 'lagrange-multiplier', 'multivariable-calculus', 'optimization', 'convex-analysis']"
3798273,"Show that the function is continuous on $[-1,1]$","$f(x)=\mid{x}\mid$ Let $a\in(-1,1)$ $\mid f(x)-f(a)\mid=\mid\mid x\mid-\mid a\mid\mid\leq\mid x-a\mid$ Let $\epsilon>0$ be given and define $\delta=\epsilon$ , whenever $\mid x-a\mid<\delta,\space \mid f(x)-f(a)\mid<\epsilon.$ $\therefore f(x)$ is continuous on the interval $(-1,1)$ Also, $\lim_{x\to-1^+}f(x)=1=\lim_{x\to-1}f(x)$ and $\lim_{x\to1^-}f(x)=1=\lim_{x\to1}f(x)$ $\therefore f(x) $ is continuous on the right side of $-1$ and on the left side of $1$ . Thus, we conclude that $f(x)=\mid x\mid$ is continuous on the interval $[-1,1]$ . Is my proof correct?","['limits', 'solution-verification', 'continuity']"
3798290,Is the series $\sum_\limits{n=1}^\infty\frac{n}{n^3+1}$ convergent or divergent? [duplicate],"This question already has an answer here : Convergent series and divergent series $\sum\limits_{n=1}^\infty \frac{n}{n^3+1}$ and $\sum\limits_{n=1}^\infty \frac{n^2+1}{n^3+1}$ (1 answer) Closed 3 years ago . Is the series $$\sum_\limits{n=1}^\infty\frac{n}{n^3+1}$$ convergent or divergent? My answer is the following. Could anyone tell me if it is correct? Since $$0<\frac{n}{n^3+1}<\frac{1}{n^2}\;\;,\;\;\;\;\forall n\in\mathbb{N}$$ and the series $$\sum_\limits{n=1}^\infty\frac{1}{n^2}$$ is convergent, by applying comparison test, we get that the series $$\sum_\limits{n=1}^\infty\frac{n}{n^3+1}$$ is convergent too.","['calculus', 'sequences-and-series', 'real-analysis']"
3798330,Triangle greater than (probability),"This one is a follow up of my previous question. But a different problem. And this one should have a more interesting answer. I don't really know how to approach this problem nonetheless reach a solution, so again help is appreciated. Question: You have a circle with radius $R$ . If three points are randomly chosen inside this circle.
What is the probability that the three points form a triangle with an area greater than $\displaystyle \frac{R^2}{5}$ ? Edit: Is anyone trying or maybe found an approach that might work? Is there any similar problems you've seen before that could work as a guide towards solving this one? What do you consider being the difficulties? I literally don't have any idea on where to even start.","['geometric-probability', 'probability']"
3798363,"""Second order quantization operator"" and its role in the construction of Hida spaces.","As premise I know barely anything about physics and thus this question might be absolutely trivial or ill-posed. I am starting to study the White noise theory and when constructing the infinite-dimensional analogue of the Schwartz space $S(\mathbb R^n)$ , i.e. $(S)$ one way is to grab a random variable $\psi\in L^2(S(R),\mu)$ with chaotic representation in term of Wiener-Ito integrals: $$\psi=\sum_n I_n(f_n).$$ At this point an operator is introduced, namely $$\Gamma (A) \psi= \sum_n I_n(A^{\otimes n} f_n),$$ where $A$ is the Hamiltonian of a harmonic oscillator, $A=-d^2/dx^2 +x^2+1$ . The author of the notes I'm reading states that this is the ""second order quantization operator"", also T.Hida makes allusion to such operator. I went to Wikipedia to read about this ""second order quantization"" and  honestly I don't quite understand if the two things are related at all. Do you mind giving a brief (and ""for a
dumb"") explanation on why this operator is called like that?
I suspect it has to do with the Fock space, since the Homogeneous chaoses are related via an isometry with the  Hilbert  space symmetric tensor power. Thanks in advance.","['quantum-mechanics', 'functional-analysis', 'stochastic-calculus']"
3798409,"Prove that tangent spaces, modeled as equivalence classes of curves, are vector spaces","Starting with these definitions A curve on a manifold $\mathcal M$ is a smooth (i.e. $C^{\infty}$ ) map $\sigma $ from some open interval $(-\epsilon,\epsilon)$ of the real line into $\mathcal M$ Two curves $\sigma_1$ and $\sigma_2$ are tangent at a point $p$ in $\mathcal M$ if (a) $\sigma_1(0) = \sigma_2(0) = p$ and (b)
In some local coordinate system $(x^1,x^2,\ldots,x^m)$ around $p$ , two
curves are tangent in the usual sense as curves in $\mathbb R^m$ , $$
(x^i \circ \sigma_1)'(0) = (x^i \circ \sigma_2)'(0) 
$$ here, $i=1,\ldots,m$ The tangent vector is defined as the equivalence class of curves in $\mathcal M$ where the equivalence relation between two curves is that they are tangent at point $p$ . The tangent space is $T_p\mathcal M$ to $\mathcal M$ at point $p$ is the set of all tangent vectors at point $p$ I am trying to prove the tangent space at the point $p$ in a manifold $\mathcal M$ is a vector space. I'm starting with $v_1 \in T_p\mathcal M$ , and $v_2 \in T_p\mathcal M$ , and I have the following
definitions $$
v_1 + v_2 := [\phi^{-1}\circ \ (\phi\ \circ \sigma_1 + \phi\ \circ \sigma_2 )] \\
r \ v_1 := [\phi^{-1}\circ \ (r  \phi\ \circ \sigma_1)]\ \forall r \in \mathbb R
$$ I want to show that $v_1 + v_2 \in T_p \mathcal M$ and $r \ v_1 \in T_p \mathcal M$ As $v_1 ,v_2 \in T_p\mathcal M$ , then $$ \sigma_1(0) = \sigma_2(0) = p $$ Now, for $v_1 + v_2$ be a vector at $p$ , $\phi^{-1}\circ \ (\phi\ \circ \sigma_1 + \phi\ \circ \sigma_2 )(0) = p$ $$
\phi^{-1}\circ \ (\phi\ \circ \sigma_1 + \phi\ \circ \sigma_2 )(0) = \phi^{-1} \ (\phi\ ( \sigma_1(0)) + \phi\ (\sigma_2(0)) ) \\
= \phi^{-1}((\phi\ ( p) + \phi\ (p) )) \\
=  \phi^{-1}( \  2\phi\ ( p) ) \neq p
$$ I can't prove the closure relations starting from the definitions, what I'm doing wrong? Edit: The book I am following ""Isham, Chris J. Modern differential geometry for physicists. Vol. 61. World Scientific, 1999."" , takes a special chart $(U,\phi)$ such that $\phi(p) = \mathbf 0 \in \mathcal M$ , using this choice $$
\phi^{-1}\circ \ (\phi\ \circ \sigma_1 + \phi\ \circ \sigma_2 )(0) = 
\phi^{-1}( \  2\phi\ ( p) ) = \phi^{-1}(0) = p
$$ So, the closure is proven under addition. But this chart is a special choice. But the definitions hold for any charts around $p$ , so another choice of charts should give the same result.","['manifolds', 'differential-geometry']"
3798429,"How to solve $\sqrt{x!y!}=xy$ for $(x,y)\in\mathbb{Z}_{\geq0}\times\mathbb{Z}_{\geq0}$?","How to solve $\sqrt{x!y!}=xy$ for $(x,y)\in\mathbb{Z}_{\geq0}\times\mathbb{Z}_{\geq0}$ ? In this task they are asking to find ordered pair couple in $\mathbb{Z}$ that satisfies the above equation,
So that, i started to take squaring both sides to get $x!y!=xy\times xy$ then i tired to use the definition of factorial $x!=x(x-1)(x-2)....2\times 1$ Finally i obtain that $(x-1)!(y-1)!=xy \text{ or } xy=0 $ Now is there any shortcut to kill this problem easily ?","['elementary-number-theory', 'inequality', 'factorial', 'discrete-mathematics']"
3798441,Why did I get $J \bar{v} = \sqrt{-1}\bar{v}$ ??,"Q. I cannot deduce the equation $J \bar{v}= -\sqrt{-1}\bar{v}$ for $v$ satisfying $Jv= \sqrt{-1}v$ . Details Let $V$ be a vector space on $\mathbb{R}$ equipped with a complex structure $J$ . Using $J$ , the action of $\mathbb{C}$ to $V$ is defined by $(a+b\sqrt{-1})v := av + bJv$ for $v \in V$ and $a,b \in \mathbb{R}$ . Let $V_{\mathbb{C}}:= V \otimes_ \mathbb{R} \mathbb{C}$ . For $v= w\otimes_ \mathbb{R} \alpha \in V_{\mathbb{C}}$ , its complex conjugate is defined by $\bar{v}:= w\otimes_ \mathbb{R} \bar{\alpha} \in V_{\mathbb{C}}$ . I cannot understand the statement that $Jv= \sqrt{-1}v$ implies $J \bar{v}= -\sqrt{-1}\bar{v}$ , where $v \in V_{\mathbb{C}}$ . In my view, $J \bar{v} = J( w\otimes_ \mathbb{R} \bar{\alpha} )= (J w) \otimes_ \mathbb{R} \bar{\alpha} =  (\sqrt{-1} w) \otimes_ \mathbb{R} \bar{\alpha}$ I get stuck, because $\sqrt{-1}$ cannot pass the $\otimes_ \mathbb{R}$ . So,.. but maybe $J \bar{v} = J( w\otimes_ \mathbb{R} \bar{\alpha} )= (J w) \otimes_ \mathbb{R} \bar{\alpha} =  (\sqrt{-1} w) \otimes_ \mathbb{R} \bar{\alpha}:=  \sqrt{-1}( w \otimes_ \mathbb{R} \bar{\alpha})=  \sqrt{-1} \bar{v}$ ?? Where do I misunderstand??","['complex-geometry', 'almost-complex', 'differential-geometry']"
3798450,Is this function definition related to the Collatz conjecture valid?,Is the following function definitions related to the Collatz Conjecture valid?  Is there a simpler or more standard way to define $C_n(x)$ ? Here's what I did: Let: $C(x) = \dfrac{3x+1}{2^w}$ where $w$ is the highest power of $2$ that divides $3x+1$ Goal: Define $C_n(x)$ where: $$C_n(x) = C_1(C_2(C_3(\dots C_n(x)\dots)))$$ Define $w_i$ as the highest power of $2$ that divides $C_i(x)$ : Claim: $$C_n(x) = \frac{3^n x + 3^{n-1} + \sum\limits_{i=1}^{n-1}3^{n-1-i}2^{\left(\sum\limits_{k=1}^{i}w_k\right)}}{2^{\left(\sum\limits_{j=1}^{n} w_i\right)}}$$ Argument : (1)  Base Case: $n=2$ : $C_2(x) = C(C(x)) = C\left(\dfrac{3x+1}{2^{w_1}}\right) = \dfrac{3\left(\frac{3x+1}{2^{w_1}}\right)+1}{2^{w_2}} = \frac{3^2x + 3 + 2^{w_1}}{2^{w_1 + w_2}}$ (2)  Assume that it is true up to $n$ so that: $$C_n(x) = \frac{3^n x + 3^{n-1} + \sum\limits_{i=1}^{n-1}3^{n-1-i}2^{\left(\sum\limits_{k=1}^{i}w_k\right)}}{2^{\left(\sum\limits_{j=1}^{n} w_i\right)}}$$ (3)  Then: $$C_{n+1}(x) = C(C_n(x)) = \frac{3\left(\frac{3^n x + 3^{n-1} + \sum\limits_{i=1}^{n-1}3^{n-1-i}2^{\left(\sum\limits_{k=1}^{i}w_k\right)}}{2^{\left(\sum\limits_{j=1}^{n} w_i\right)}}\right)+1}{2^{w_{n+1}}} = \frac{3^{n+1} x + 3^{n} + \sum\limits_{i=1}^{n}3^{n-i}2^{\left(\sum\limits_{k=1}^{i}w_k\right)}}{2^{\left(\sum\limits_{j=1}^{n+1} w_i\right)}}$$,"['collatz-conjecture', 'functions', 'solution-verification']"
3798526,Find the maximum value of $x^2y$ given constraints,"Find the maximum value of $${ x }^{ 2 }y$$ subject to the constraint $$x+y+\sqrt { 2{ x }^{ 2 }+2xy+3{ y }^{ 2 } } =k$$ where k is a constant.
I tried it by substituting value of x and then differentiating w.r.t $x$ but not able to proceed further.","['optimization', 'derivatives', 'constraints']"
3798620,Parametrized permutation function,"I'm looking for a way to construct a function that would work the way depicted in the following picture: That is, I want it to permute the elements in a given array to put them in some different order depending on a parameter I give it, so that the numbers of my choice from the original set (those I marked with grey background) would all end up as a continuous range at the beginning of the output array (to the left of the thick line), while all other elements (red background) would end up at the remaining positions in that array (to the right of the thick line). The order of those elements in each of the output ranges (grey or red) doesn't matter to me. They can be put in any arbitrary order by that function, whatever is simpler to compute for a particular choice of selected input elements (grey). The only thing that matters is that all those selected elements (grey) end up on one side of the boundary, while other elements (red) end up on the other side of that boundary, and the two ranges are continuous. This function needs to be parametrizable so that, out of all possible permutations of this array, I could choose that particular permutation that puts the elements in that particular order by just specifying some numerical parameter (or parameters) in the function's formula. One numerical parameter is preferable, since there is only one permutation that puts all the elements in this particular order, and this number could be the ""identifying number"" of that permutation, but if that would be hard to achieve, several numerical parameters is acceptable, as long as it doesn't exceed the number of chosen elements (which would probably make it not worth the effort anyway). Is there a way to construct a formula for such a function in a systematic way, given a subset of ""chosen elements"" from the input array? Maybe something based on modular arithmetics or finite fields? A quick web search gave me a term called ""permutation polynomials"" that at first glance seems to be related to this problem somehow, but all the resources I could find about them are some thick math that seems to require a lot of background in that field to even understand what's going on (I'm just an IT engineer / programmer looking for a solution for some programming problem, not a professional mathematician :q ) Of course, any function could be put into a lookup table. But that's not what I'm looking for, because that would require a lookup table of the same size as the entire input set, which would be an overkill. Edit: One thing that comes to my mind is modular exponentiation, since in prime moduli, when a primitive root is chosen as the base, and the exponent is our $x$ , then every power of that base is unique (maximal period) and the resulting sequence is some permutation of the original sequence (however, it always starts and ends with 1, and there's always $N-1$ in the middle). But this way I can only obtain some permutations, not every possible permutation. Raising this exponential function to some other power $p$ only selects every $p$ th element from this sequence, so this way I can only get a sequence for another primitive root (provided that $p$ is coprime to the size of the modulus less one, because otherwise the period breaks into shorter cycles, like for some other base that is not a primitive root). Maybe there is some other way to shuffle those numbers than exponentiation?","['permutations', 'functions', 'algorithms']"
3798628,Solve $\lim_{x\rightarrow +\infty}\frac{1}{x}\log\left(\frac{x+1}{1+x^2}\right) = 0$ without L'Hôpital's rule,How would you solve the limit $$\lim_{x\rightarrow +\infty}\frac{1}{x}\log\left(\frac{x+1}{1+x^2}\right) = 0$$ without using L'Hôpital's rule?,"['limits', 'limits-without-lhopital']"
3798636,"Surjection between same finite set, show that there can't be two different inputs produce same output","Is it possible to prove that there can't be two different inputs produce same output for surjection between same finite set without firstly proving
such surjection is also an injection. I'm asking this because I'm trying to use this result to prove that such surjection is also an injection. Here's the working proof of injection. Goal: $∀n ∈ ℕ, ∀ f, f: n ⟹ n → injective f$ Here ""f: n ⟹ n"" denotes that f is an surjection from n to n. Prove by induction, n = 0 is true vacuously. For n = k, assume $∀ f, f: k ⟹ k → injective f$ so we need to prove $∀ f, f: k⁺ ⟹ k⁺ → injective f$ Use exclude middle to $∀p ∈ k, f(p) ∈ k$ Case I: $∀p ∈ k, f(p) ∈ k$ We have $f(k) = k$ , or else $f(k) ∈ k$ contradict surjection since nothing maps to k. So we have $f ↾ k : k ⟹ k$ where ""↾"" denotes restriction. By induction hypothesis, $injective (f ↾ k)$ Therefore $f = f ↾ k ∪ \{<k, k>\}$ is injective. Case II: $¬ ∀p ∈ k, f(p) ∈ k$ which means $∃p ∈ k，f(p) = k$ If we can prove $f(k) ∈ k$ ，then it can be reduced to Case I by swapping values on k and p. To prove $f(k) ∈ k$ , notice $f(k) ∈ k⁺$ we only need to prove $f(k) ≠ k$ which leads to what title asks.",['elementary-set-theory']
3798672,Find the ratio of the area of the original triangle to the area of the equilateral triangle at Gergonne point,"If $G$ is the Gergonne point of $\triangle PQR$ and $A, D, E$ are the contact points of the incircle such that $\triangle PGE$ is equilateral. What is the ratio of area $\dfrac{\triangle PQR}{\triangle PGE}$ ? My attempt: Let's put this in $X$ - $Y$ coordinates with $C$ as origin and line $PQ$ being parallel to the $X$ -axis and $EC$ being the $Y$ -axis ( $C$ is the center of the incircle and radius $r$ ). $\angle APQ = \angle PER = \alpha = 60^0$ ; Say, $PG = EG = PE = 1$ unit Coordinates of points: $E(0,-r), P(-1,-r), C(0,0)$ . The equation of the line $PR$ : $y + r = m(x+1)$ As line $PR$ is tangent to the circle, the perpendicular from center $C$ will be equal to the radius. $r = \dfrac{r - m}{\sqrt{m^2+1}}$ or, $m = \dfrac{2r}{1-r^2}$ So, the equation of the line $PR$ : $y + r = \dfrac{2r}{1-r^2} (x+1)$ ...(1) Equation of line $ER$ : $y + r = -x \tan\alpha$ ...(2) Equating (1) and (2), we get the intersection point $R (x_0,y_0)$ [ $\dfrac{-2r}{2r+(1-r^2)\tan \alpha}, \dfrac{2r\tan \alpha}{2r+(1-r^2)\tan \alpha} - r$ ], where $\tan \alpha = \tan 60^0 = \sqrt3$ Equation of line $QR$ : $y - y_0 = m_0 (x - x_0)$ Again equating perpendicular distance from the center and the radius $r = \dfrac{m_0x_0 - y_0}{\sqrt{m_0^2+1}}$ or, $(x_0^2-r^2)m_0^2 - 2x_0y_0m_0 + (y_0^2 - r^2) = 0$ As there are two lines passing through $(x_0,y_0)$ that are tangent to the circle ( $PR$ and $QR$ ), the quadratic equation will give us two values of $m_0$ , one of them being $m_0 = \dfrac{2r}{1-r^2}$ as we have established before. This is how far I reached. I tried to replace values of ( $x_0, y_0$ ) in the quadratic equation and we already know one of the roots but it is becoming very tedious to find the slope of line $QR$ . Once I know the slope, I can find the point $A$ and as we know the equation of line $PA$ , I should be able to find the value of $r$ as well as coordinates of points $P, Q, R$ . That will lead to us finding the area of $\triangle PQR$ and the ratio to $\triangle PGE$ (area of $\triangle PGE = \dfrac {\sqrt3}{4}$ units). Any suggestions on how to simplify the solution? Are there any known properties of gergonne point that I am missing which makes the solution simpler?","['analytic-geometry', 'geometry']"
3798679,Is it true that $\angle ACK=\angle BCL$ in a circle?,"Note: this is the core problem I extracted from the following question, which I have been struggling for hours and days and am at the verge of giving up. Prove two angles add up to 90 degrees The problem: In a circle, $MN$ is a diameter. $\triangle ABC$ is a right triangle such that $AB\perp MN$ and $\angle ACB = 90^{\circ}$ . $MA, MB$ intersect the circle at $K, L$ . Prove that $\angle ACK=\angle BCL$ . What I have tried: Notice that this problem has a ""looser"" condition than the original problem, so the statement is not guaranteed to be true $100$ percent. But I have drawn out $5$ different pictures with very high precision and manually compared the two angles, which are always the same. This leads me into believing that this question is the core part of the original problem. From the given, what I can tell is $A,B,L,K$ are co-cyclic and I did not really go anywhere beyond that despite spending days. If I draw lines parallel to $AC$ and $BC$ from $M$ and intersect them with the circle, I get a triangle similar to $\triangle ABC$ which pass through the center of the circle, which is nice but unhelpful at all. The two angles seems so remote. I have a feeling that there is a theorem that can solve this problem in several lines but just that I don't know the theorem. The stricter condition: If I include one more condition that $KB$ and $LA$ meets the circle at $P,Q$ where $C$ lies on line $PQ$ and $PQ$ is perpendicular to $MN$ then this problem is equivalent to the original problem. But I don't believe that is necessary for this specific result based on experimental observation. (They are necessary for the original problem though)","['contest-math', 'euclidean-geometry', 'geometry']"
3798682,Testing goodness of fit using Kolmogorov-Smirnov test,"I want to check if two probability distributions (experimental and theoretical) are same. The distributions are not normal distributions so I decided to use KS test. I used the MATLAB function KStest2 and got p-value = 1! Now, it means that I can't reject the null hypothesis that the two distributions are same. I have two main concerns: Does it mean I can accept the null hypothesis? I'm confused about the statement 'fail to reject the null hypothesis' What is the p-value for the hypothesis that distributions are same. Can I calculate it as 1-p? As I'm interested in testing whether my theory is correct and want to give a p-value for that. https://se.mathworks.com/help/stats/kstest2.html","['statistical-inference', 'statistics', 'p-value', 'hypothesis-testing', 'mathematical-physics']"
3798684,"Prove that if lines $FP$ and $GQ$ intersect at $M$, then $\angle MAC = 90^\circ$.","In convex cyclic quadrilateral $ABCD$ , we know that lines $AC$ and $BD$ intersect at $E$ , lines $AB$ and $CD$ intersect at $F$ , and lines $BC$ and $DA$ intersect at $G$ . Suppose that the circumcircle of $\triangle ABE$ intersects line $CB$ at $B$ and $P$ , and the circumcircle of $\triangle ADE$ intersects line $CD$ at $D$ and $Q$ , where $C,B,P,G$ and $C,Q,D,F$ are collinear in that order. Prove that if lines $FP$ and $GQ$ intersect at $M$ , then $\angle MAC = 90^\circ$ . My Progress : Claim : $PBQD$ is cyclic Proof : Note that $CQ\cdot CD=CE\cdot CA=CB\cdot CP \implies PQDB$ is cyclic. Claim : $APQC$ is cyclic Proof : angle chase! Note that for this to be true , it is enough to show that $\angle AEB=\angle AQC$ or it is enough to show that $\angle AEB=\angle AQC $ or it is enough to show that $\angle AED=\angle AQD$ which is true since $AEDQ$ is cyclic. Claim : $E\in PQ$ Proof : So enough to show that $\angle AEQ+\angle AEP=180 $ or enough to show that $180- \angle ADC + \angle AEP=180 $ or enough to show that $\angle ADC= \angle ABC$ , which is true since $ABCD$ is cyclic. after that I am stuck. I observed that $FG , AM, PQ$ concur but was not able to prove. Can someone give hints? Thanks in advance.","['contest-math', 'euclidean-geometry', 'geometry']"
3798750,"Show that $(P,S,1)$ is a Peano system if and only if it satisfies the Iteration Theorem","You can skip the first section(hints and useful things) if you want, I will keep editing this along I got more findings related to the question. Hints and useful things: I find out that the book from 1967 which the author mentions is Algebra(first edition) - Maclane and Birkhoff , but I dont have access to this book therefore I cant say what exactly is the hint in pages 67-70. If someone have access to this first edition please let me know what is the subject in these pages. I have found a slide presentation talking about Induction, iteration, recursion, andwell ordering , In the page 42, It shows the Homomorphism Theorem (Dedekind 1888) in the same way the concept  of isomorphism(3) here in my question, which in fact is obtained using the Iteration Theorem. Later in pages 50-51, It talks about the Converse of Homomorphism Theorem where they states a very similar proposition to my interpretation of the question: $$(N,S,1) \text{ is a Peano system} \Leftrightarrow (N,S,1) \text{ satisfies the Homomorphism Theorem}$$ The only difference is that in the question it uses Iteration Theorem instead of Homomorphism Theorem, then in the same slide the author says ""This is a categorical characterization of $(\mathbb{N},0,S)$ (Lawvere 1965)"". As the book which Im following give a definition of categorical (4) I added it to the concepts and definitions. Question: Consider a Peano structure $(P,S,1)$ , where $1 \in P$ and $S:P\rightarrow P$ , Show that $(P,S,1)$ is a Peano system if and only if it satisfies the Iteration Theorem, that is for any set $W$ , any object $c$ in $W$ , and any singulary operation $g$ on $W$ , there is a unique function $F:P\rightarrow W$ such that $F(1)=c$ and $F(S(x)) = g(F(x))$ for all $x$ in $P$ . (see Maclane and Birkhoff [1967], pp. 67-70. Concepts and definitions: By a Peano System we mean a set $P$ , a particular element $1$ in $P$ , and a singulary operation $S$ on $P$ such that the following axioms are satisfied. (P1) $1$ is not the successor $S(x)$ of any object $x$ in $P$ . In Symbols: $$(\forall x)(S(x) \neq 1)$$ (P2) Different objects in $P$ have different successors. This can be formulated as follows: $$(\forall x)(\forall y)(x \neq y \Rightarrow S(x) \neq S(y))$$ (P3) Principle of mathematical induction: Any subset of $P$ containing $1$ and closed under $S$ must be identical with $P$ . This can be symbolically rendered as follows: $$(\forall B)([B \subseteq P \land 1 \in B \land (\forall x)(x \in B \Rightarrow S(x) \in B)] \Rightarrow P=B)$$ Such a Peano System will be denoted by the ordered triple (P,S,1): $P$ is called underlying set , $S$ the successor operation , and $1$ the distinguished element . Iteration Theorem: Consider any Peano system $(P,S,1).$ Let $W$ be an arbitrary set, let $c$ be a fixed element of $W$ , and let $g$ be a singulary opration on $W$ . Then, there is a unique function $F:P \rightarrow W$ such that: $F(1) = c$ $F(S(x)) = g(F(x))$ for all $x$ in $P$ Consider any two Peano systems $\mathscr{P}=(P,S,1)$ and $\mathscr{P}^* = (P^*,S^*,1^*)$ , we say $\mathscr{P}$ and $\mathscr{P}^*$ are isomorphic if and only if there is a one-one correspondence $H$ between $P$ and $P^*$ $$H: P \xrightarrow[onto]{1-1} P^*$$ such that: $H(1) = 1^*$ , and $H(S(x)) = S^*(H(x))$ for all $x$ in $P$ . Such a function $H$ is called an isomorphism between $\mathscr{P}$ and $\mathscr{P}^*$ A set of axioms is said to be categorical if and only if any two models for those axioms are isomorphic. Proved Theorems: Here I will list previus proved Theorem in the text book which I believe its useful for this question. Iteration Theorem (defined above) Any two Peano systems are isomorphic (and the isomorphism is unique) My atempt: My interpretation of the question is that it should be proved an equivalence between a system be a Peano system and satisfying the Iteration Theorem: $$(P',S',1') \text{ is a Peano system} \Leftrightarrow (P',S',1') \text{ satisfies the Iteration Theorem}$$ $[\Rightarrow]$ We have by the definition of Iteration Theorem that it holds for any Peano system, as the Theorem is already proved we are done and just need to prove the converse. $[\Leftarrow]$ First assume some system $(P',S',1')$ , where $P'$ is a set, $1' \in P'$ and $S'$ is a singulary operation on $P'$ and that this system satisfies the Iteration Theorem. From here I thinked about two approaches, where one is too prove that is possible to define an bijection between $P'$ and an already known set in another Peano system like $\mathbb{N}$ to establish an isomorphism, and the other approach is to prove that if any of Peano Axioms dont holds true for $(P',S',1')$ then the Iteration Theorem cannot be satisfied. Tryng the first approach, consider $(\mathbb{N}, S, 1)$ as the Standard Peano system, where $\mathbb{N}$ is the set of natural numbers and $S$ is defined as $S(x) = x+1$ , now using the Iteration Theorem in $(P',S',1')$ , let $W=\mathbb{N}$ , $c=1$ and $g=S$ , therefore we got a unique function $F:P' \rightarrow \mathbb{N}$ such that: $F(1') = 1$ , and $F(S'(x)) = S(F(x))$ for all $x$ in $P'$ We must show that $F$ is one-one and from $P'$ onto $\mathbb{N}$ , first lets  show that $F$ is onto by induction on $\mathbb{N}$ . Let $\mathscr{R}(F)$ be the range of $F$ . We have that $1 \in \mathscr{R}(F)$ , since $F(1') = 1$ . Now assume $x \in \mathscr{R}(F)$ thus $(\exists u)(u \in P' \land x = F(u))$ and as $S$ is closed under $\mathbb{N} $ we have S(x) defined: $$S(x) = S(F(u))$$ and by definition of $F$ it follows $$S(x) = F(S'(u))$$ Thus we have $\mathscr{R}(F) \subseteq \mathbb{N} \land 1 \in \mathscr{R}(F)$ and we have shown that $x \in \mathscr{R}(F) \Rightarrow S(x) \in \mathscr{R}(F)$ , therefore by Mathematical induction on $\mathbb{N}$ we have that $\mathscr{R}(F) = \mathbb{N}$ , thus we got $F: P' \xrightarrow[onto]{}  \mathbb{N}$ Now we must show that $F$ is one-one, the first thing I notice is that if $F$ is from $P'$ onto $\mathbb{N}$ is must have at least one restriction $F'$ of $F$ where $F'$ is one-one, by the nature of the Iteration Theorem if we peek some element $x$ from $P'$ we have that $F(S'(x)) = S(F(x))$ , but it we take $F(S'(S'(x)))$ we got first $S(F(S'(x))$ from which follows $S(S(F(x))$ not matter how many times we repeat this proccess we will have distinct objects because the nature of successor operation on $\mathbb{N}$ , lets denote the $n$ applications of $S'$ to some $x$ by $S'^n(x)$ and $n$ applications of $S$ to $x$ by $S^n(x)$ , therefore we can write $F(S'^n(x)) = S^n(F(x))$ . From this we can see that for any $a,b \in \mathbb{N}$ and some $x \in P'$ we have that $a \neq b \Rightarrow F(S'^a(x)) \neq F(S'^b(x))$ from which follows $a \neq b \Rightarrow S'^a(x) \neq S'^b(x)$ . Now for any $x \in P'$ let $P_x = \{y : y \in P' \land n \in \mathbb{N} \land (y = S'^n(x) \lor y=x) \}$ , thus let $x=1$ in this definition and we will have $$P_{1'} = \{y : y \in P' \land n \in \mathbb{N} \land (y = S'^n(1') \lor y=1') \}$$ Now let $F$ be restricted to $F': P_{1'}\rightarrow \mathbb{N}$ , we can prove that $F'$ is onto in the same way that was done before and as all elements in $P_{1'}$ is obtained from a successive applications of $S'$ to $1'$ it follows that $F'$ is one-one because $a,b \in \mathbb{N}$ we have that $a \neq b \Rightarrow F(S'^a(1')) \neq F(S'^b(1'))$ , therefore we have $F':P_{1'} \xrightarrow[onto]{1-1} \mathbb{N}.$ Now I suppose its possible to show that $F'=F$ or in other way that $P_{1'} = P'$ because it will prove that exists an isomorphism between $(P',S',1')$ and $(\mathbb{N}, S, 1)$ from which will follows that (P',S',1') is Peano system. I tried to show those both but I failed in all attempts and its not worth to put what  tried at this part because I dont get any useful conclusions, thus I moved to the another approach where I dont make much progress but at least I got some useful things about singulary operation $S'$ . From now I will just show two useful properties that $S'$ must have considering the function $F:P' \rightarrow \mathbb{N}$ established before using the Iteration Theorem. $(\forall x)(S'(x) \neq 1')$ First assume $1'$ is in the range of $S$ , thus $1' = S'(u)$ for some $u \in P'$ , from which follows: $$F(1') = F(S'(u))$$ but as F(1') = 1 and F(S'(u)) = S(F(u)) we got $$1 = S(F(u))$$ , which is a contradiction because by Axiom (P1) on $(\mathbb{N},S,1)$ $1$ is not a successor, therefore $1'$ is not in the range of $S'$ , if we suppose $S'$ will act as a sucessor operation on $P'$ this show that Axiom (P1) holds true for $(P',S',1')$ . $(\forall x)(x \neq S'(x))$ Assume $S'(x) = x$ , thus F(S'(x)) = F(x), from which follows S(F(x)) = F(x), which is a contradiction on $(\mathbb{N},S,1)$ because no one natural number is its own sucessor, therefore we have $x \neq S'(x)$ for all $x$ in $P'$ . Thats all I tried and managed to do, I think if its possible to prove that $S'$ is one-one it will be possible to prove that $F$ is one-one too and finish this proof without going along all Peano Axioms in the second apporach, but when if $S'$ is proved to be one-one, the only axiom which will be needed to prove is the Mathematical Induction. Im not certain that those approach can really work in that case, so any feedback is welcome, and another approaches too, but I will like if some of those can be finished or corrected.","['elementary-set-theory', 'solution-verification', 'peano-axioms']"
3798800,Find all complex solutions (real and non real) of $2x^3-3x^2+32x+17$,"I am to find the solutions of $2x^3-3x^2+32x+17$ . My textbook says the solutions are $\frac{-1}{2}$ , $1\pm4i$ I got $\frac{-1}{2}$ and $1\pm i \sqrt{17}$ First I used the fundamental theorem of algebra to find candidate zeros and verified using synthetic division that $\frac{-1}{2}$ is a zero. I then had: $(x+\frac{1}{2})(2x^2-4x+34)$ Then, using the quadratic formula with $(2x^2-4x+34)$ to find the zeros: $a=2$ $b=-4$ $c=34$ $$\frac{4\pm\sqrt{4^2-4(2)(34)}}{4}$$ $$\frac{4\pm\sqrt{-272}}{4}$$ $$\frac{4\pm4i\sqrt{17}}{4}$$ $$1\pm\sqrt{17}i$$ Where did I go wrong and how can I arrive at $1\pm4i$ ?","['algebra-precalculus', 'solution-verification', 'roots', 'polynomials']"
3798807,"What meaning should I assign to ""assign""?","Since a few days ago, I have been working with Schaum's Outline of General Topology by Seymour Lipschutz. So far I have been studying the first chapters about sets and functions to review, and to make sure that I know his notation. My question is more philosophical in nature, and would perhaps strike you as ""silly"" but I will pose it anyway, and would love to hear your thoughts! In chapter 2, he defines a function in this way (rather wordily): Suppose that to each element of a set $A$ there is assigned a unique element of a set $B$ ; the collection, $f$ , of such assignments is called a function from $A$ into $B$ ... (p.17, emphasis added) Very standard I guess, but what caught my attention was the word assign . What does it really mean to ""assign"" something to something else? Where (i.e., in what kind of set) is this assignment stored? My intuition about assignment was (and still is) that it is simply a pair of elements; i.e., an assignment of elements $a \in A$ to $b \in B$ is simply a subset of $A \times B$ . However, the (philosophical) problem comes from Lipschutz's next statement: To each function $f: A \rightarrow B$ there corresponds the relation in $A \times B$ given by $$\{ ( a,f(a) ) \vert a \in A\}.$$ (p.17, emphasis added) Thus to the function corresponds a relation, i.e., the function and the relation are seen as different objects. The problem is then metaphorically swept under the carpet by not ""distinguish[ing] between a function and its graph."" I interpret this a bit colorfully as ""they are different, but we are not supposed to ask questions about it."" I remember when I studied my first algebra course, then a function $f: A \rightarrow B$ was indeed defined as a special case of a relation, i.e., as a subset of $A \times B$ . I never thought much about it then, but now I see that doing it this way avoids any reference to some ""assignment"" and we know exactly which set the function ""lives in"" -- $A \times B$ , so we don't have to think about where the ""assignment"" is ""stored."" But by making a difference between the function and the relation (although there is a map between them, which is my interpretation of ""corresponds"") the (philosophical) question of the nature of this ""assignment"" arises (at least in my mind). I guess another way to phrase what I'm thinking about is that in my mind, ""assignment"" is done by means of a map, or function, between sets; but what does it mean then to define functions in terms of some ""assignment"" operation? I apologize in advance for taking up your time with this! (I feel so stupid for thinking about these kinds of things instead of actually working with the topology problems...). But I'm wondering if there is some definition or notion of what ""assignment"" means in this context? Or perhaps it is just some language that we shouldn't think more about? Or perhaps there is something I have missed, not being a native English speaker? If you have any insights I would love to hear them :)","['elementary-set-theory', 'definition', 'functions']"
3798822,"If $f(x)=\big\lfloor x\lfloor x\rfloor\big\rfloor$ for all $x\geq 0$, then for an integer $n$, solve for $x\geq 0$ such that $f(x)=n$.","Let $f(x) = \big\lfloor x \lfloor x \rfloor \big\rfloor$ for $x \ge 0.$ (a) Find all $x \ge 0$ such that $f(x) = 1.$ (b) Find all $x \ge 0$ such that $f(x) = 3.$ (c) Find all $x \ge 0$ such that $f(x) = 5.$ (d) Find the number of possible values of $f(x)$ for $0 \le x \le 10.$ Attempt. I've solved part (a), but I'm stuck on how to solved Part (b), (c), and (d).
The answer I got for (a) is $1 \leq x < 2$ .  If $0\le x<1$ , then $\lfloor x\rfloor=0$ , so $f(x)=0\ne 1$ .  If $x\ge 2$ , then $\lfloor x\rfloor=2$ and $x\lfloor x\rfloor \ge 4$ , so $f(x)\ge 4$ .  Hence, when $f(x)=1$ we must have $1\le x<2$ .  This means $\lfloor x\rfloor=1$ so $1\le x\lfloor x\rfloor<2$ and $f(x)=1$ . Could someone help me out with the other parts of the question? Thanks! (Also, there is a hint to divide into cases based on the value of $\lfloor x \rfloor.$ but I don't exactly understand.)","['elementary-number-theory', 'algebra-precalculus', 'functions', 'ceiling-and-floor-functions']"
3798876,"What does ""discrete"" really mean, in plain English?","Can someone explain what a ""discrete"" function really means, in a philosophical sense, in plain English? As a guess, does discrete mean there are only points with known values, and nothing in between?
And if that's the case, is it possible to truly know what's in between the points somehow? I mean, linear interpolation would be ""fudging it"" of course, simplifying a curve to a series of lines. Polynomial interpolation maybe? Is there a way to 100% accurately represent what would go in between the dots? Come to mention it, aren't all computed values ""discrete""? I.e., when the graphing calculator, or desmos.com or whatever, draws out a graph, isn't it actually plotting a series of output values of an equation, only at small enough increments that you can't see the gaps? So what I'm asking is, is there actually a deeper, fundamental difference between a discrete function like y_0 = 10
y_(i+1) = C/2 + y_i vs a ""regular"" function like y = x or is it just a matter of similar patterns being represented differently by the computer? Bc both functions can go on forever. And even though the first progresses in discrete ""steps,"" the pattern that it represents must exist at a smaller scale, just maybe not ""captured"" by the ""lens"" of this equation? Idk.","['calculus', 'linear-algebra', 'polynomials', 'discrete-mathematics', 'lagrange-interpolation']"
3798898,"Is there a rectangular pyramid whose altitude, slant height, and ALL edge lengths are all integers, or rationals?","f so, have any suggestions on methods to find all such pyramids? but besides guess-and-check don't know where to begin. Background: I am a high school math teacher and got curious about this when generating problems. Not a homework set for me.","['number-theory', 'integers', 'elementary-number-theory', 'pythagorean-triples']"
3798904,Let $f$ be a differentiable function with no point $x$ such that $f(x)=0=f'(x)$ show that $f$ has finitely many zeros.,"Let $f: [0, 1] \rightarrow R$ be a differentiable function. Assume there is no
point $x$ in $[0,1]$ such that $f(x) = 0 = f'(x)$ . Show that $f$ has only a finite
number of zeros in $[0, 1]$ . My proof. Assume otherwise. Keep bisecting the interval choosing the subinterval with infinitely many zeros. (This is fairly. standard so I won't go into it). We obtain $(x_n)$ such that $f(x_n)=0$ for all $n$ . Moreover, $x_n\rightarrow x$ as $n\rightarrow \infty$ . We see immediately that $f(x)=0$ . Our goal is to show $f'(x)=0$ as well. We know, $$
\lim_{h\rightarrow 0}\dfrac{f(x+h)}{h}=L
$$ There's a subsequence $x_{n_k}$ , $h_k = x_{n_k}-x\ge 0$ , (if not we will use $x-x_{n_k}$ and the proof will be similar) and we observe for every $h$ , there's $N$ such that if $k\ge N$ , $h_k=x_{n_k}-x\le h$ . Thus we observe, $$
\lim_{k\rightarrow \infty }\dfrac{f(x+h_k)}{h_k}=L=0
$$ The last part is due to the fact $f(x+h_k)=0$ . Contradiction! I am only looking for proof verification. $\textbf{Please only provide hints if my proof is wrong. Complete solutions won't benefit me at all!}$","['solution-verification', 'derivatives']"
