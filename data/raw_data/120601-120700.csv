question_id,title,body,tags
1804878,Solving $y^y = x$ for large $x$,"I was playing around with recurrence relations and noticed that $\sqrt x$ has the fun property that
$$\frac{x}{f(x)} = f(x)$$
($\sqrt{x}$ and its negation are the only functions $f(x)$ that satisfy this it). That got me thinking about what functions satisfy
$$\sqrt[f(x)]{x} = f(x).$$ These functions need to satisfy
$$x = f(x)^{f(x)}.$$ If we let $y = f(x)$, this boils down to solving
$$y^y = x.$$ I am having trouble seeing how to solve this. My initial thought was to take the log of both sides, giving $$y \log y = \log x,$$ and then tried seeing if the change-of-basis formula would help, since the above statement implies that $$y = \log_y x,$$ but this didn't seem to offer any clarity. Is there a nice way to solve this equation? Or is there a known name for a function of $x$ that's specifically designed to have this property?","['algebra-precalculus', 'special-functions', 'logarithms']"
1804886,Random Binary matrix,"This is a question from Strang's ""Linear Algebra and its Applications"", right in the first chapter (I'm studying it by myself). I couldn't solve it, it isn't in the Solutions Manual, and my research suggests that there shouldn't be a simple solution for it . However, its presence in the very first chapter suggests me that I'm missing something. Here it goes: 1.6: a) There are sixteen 2x2 matrices whose entries are 1's and 0's. How many are invertible? b) (Much harder!) If you put 1's and 0's at random into the entries of a 10 by 10 matrix, is it more likely to be invertible or singular? From what I can tell, this can't be solved by elementary linear algebra so... it shouldn't be there? I'm guessing there is a clever computational way to exhaust all cases?",['linear-algebra']
1804909,Decomposition of a representation into a direct sum of irreducible ones,"I'm studying representation theory and in the book (Fulton and Harris) the author makes the following proposition with the following proof: Proposition : For any representation $V$ of a finite group $G$ , there is a decomposition $$V = V_1^{\oplus a_1}\oplus\cdots \oplus V_k^{\oplus a_k},$$ where the $V_i$ are distinct irreducible representations. The decomposition of $V$ into a direct sum of the $k$ factors is unique, as are the $V_i$ that occur and their multiplicities. Proof : It follows from Schur's lemma that if $W$ is another representation of $G$ , with a decomposition $W = \bigoplus W_j^{\oplus b_j}$ , and $\varphi : V\to W$ is a map of representations, then $\varphi$ must map the factor $V_i^{\oplus a_i}$ into the factor $W_j^{\oplus b_j}$ for which $W_j\simeq V_i$ ; when applied to the identity map of $V$ to $V$ , the stated uniqueness follows. I must confess I didn't understand. The fact that we can decompose $V$ like this I do understand that follows from the fact that if $V$ has a proper nonzero subrepresentation $W$ then there is another subrepresentation $W'$ such that $V = W\oplus W'$ . In that case, if either $W$ or $W'$ are not irreducible we can apply the same idea to them, until we have the desired decomposition. Now, this proof of uniqueness I really can't understand. I mean, uniqueness means that if we have $$V = V_1^{\oplus a_1}\oplus\cdots \oplus V_k^{\oplus a_k}\simeq W_1^{\oplus b_1}\oplus\cdots \oplus W_{r}^{\oplus b_r},$$ then we have $k = r$ , $a_i = b_i$ and $W_i\simeq V_i$ . I can't understand how this argument the author presents shows all of this. Indeed the whole point is that $V$ has these two decompositions then they are isomorphic, so that there exists one isomorphism $$\varphi : V_1^{\oplus a_1}\oplus\cdots \oplus V_k^{\oplus a_k}\to W_1^{\oplus b_1}\oplus\cdots \oplus W_r^{\oplus b_r}.$$ If we restrict it to $V_i$ we get one isomorphism $\varphi : V_i\to \varphi(V_i)$ . But why $\varphi(V_i)=W_j$ for some $j$ ? I mean, couldn't $\varphi(V_i)$ be some other subspace of the direct sum of the $W_i$ which is not one of the $W_i$ themselves? So how to understand this proof about the decomposition of a representation? What really is the argument used in this proof?","['finite-groups', 'representation-theory', 'proof-explanation', 'group-theory', 'linear-algebra']"
1804918,Switch from $a\cdot \sin(t) + b \cdot \cos(t)$ to $c \cdot \cos(t+f)$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How could I switch from $a\cdot \sin(t) + b \cdot \cos(t)$ to $c \cdot \cos(t+f)$?
Thank you for your time.","['trigonometry', 'education']"
1804936,Does $1-\frac{1}{2^\omega}$ equal 1? What about $1-\frac{1}{2^{\aleph_0}}$?,"(Correct if I'm wrong on any of this.) Recently, I've been learning about transfinite ordinals and cardinals. For some cases, I understand the difference between ordinals and cardinals, for instance that $$|\{1,2,3,\ldots,\omega\}| = \aleph_0$$ I get that ordinals index a set, while cardinal numbers are the sizes of sets. For some cases, however, I'm still uncertain about which to use. An equation that would seem to me to work is $$1-\frac{1}{2^\omega} = 1$$ After all, $$\lim_{x\to\infty}{1-\frac{1}{2^x}}=1$$ However, I'm not certain that $\omega$, an ordinal number, is the right infinity to be using.  Does this work: $$1-\frac{1}{2^{\aleph_0}}=1$$ Or perhaps, am I making a fallacy and does neither work?","['cardinals', 'ordinals', 'elementary-set-theory', 'limits']"
1804953,Birthday line to get ticket in a unique setup,"At a movie theater, the whimsical manager announces that a free ticket will be given to the first person in line whose birthday is the same as someone in line who has already bought a ticket.  You have the option of choosing any position in the line. Assuming that you don't know anyone else's birthday, and that birthdays are uniformly distributed throughout the year (365 days year), what position in line gives you the best chance of getting free ticket?","['probability', 'probability-distributions']"
1804968,"let $f(x)=(3(x+x^2))/14$ and $x$ between $0$ and $2$ , zero otherwise be the pdf for a random variable $X$ ,Find the median and the mode?","let f(x)=(3(x+x^2))/14 and x between 0 and 2  , zero otherwise be the pdf for a random variable X
Find the median and the mode ` Could you please help me Is it correct or not?","['statistics', 'probability']"
1804973,Is this greedy sequence optimal?,"Consider this strictly increasing sequence $(x_n)$ of 21 natural numbers:
$$1, 3, 4, 11, 12, 27, 28, 59, 60, 123, 124, 251, 252, 507, 508, 1019, 1020, 2043, 2044, 4091, 4092$$
i.e., $$x_n=\begin{cases}1&n=1\\2^{k+2}-5&n=2k\\2^{k+2}-4&n=2k+1>1\end{cases}$$
It has the property $$\tag{$\star$} \text{There is no index $k$ with $x_{k-1}x_{k+1}<2x_k^2<4x_{k-1}x_{k+1}$.}$$ Questions: Is there a sequence with property $(\star)$ and $x_{21}<4092$? What is the minimal possibly value of $x_{21}$? More general, for $n\gg 1$, what is the minimal possible value of $x_n$? Is there an infinite sequence with property $(\star)$ that achieves the minimal value  for all $n\gg 1$? Note that numbers from $1$ to $2046$ can (approximately) be interpreted as showing that $x_{21}\ge 2047$. In my answer to that question, I improved this first to $x_{21}\ge 2559$ and then to $x_{21}\ge 3071$. There's probably still room for improvement along the methods I used,  but the case distinctions started to become convoluted. The sequence above was constructed greedily , i.e., starting with $x_1=1$ and $x_2=3$ we recursively find the smallest $x_{k+1}$ that is $>x_k$ and is either $\ge \frac{2x_k^2}{x_{k-1}}$ or $\le  \frac{x_k^2}{2x_{k-1}}$. Note that being too greedy is bad: Starting with $x_1=1$, $x_2=2$, we arrive at the much worse $1,2,8,9,21,22,47,\ldots, x_{21}=6647$.","['number-theory', 'sequences-and-series']"
1804998,Finding the stable and unstable manifold of this system,"Consider the system $$\begin{cases}\dot{x} = x \\ \dot{y} = -y + x^2\end{cases}$$ This has fixed point $\overline{X} = (0,0)$, which is a saddle point. The aim is to find the equation of the stable and unstable manifold for the system. My lecture notes finds the unstable manifold in the following way: Use the trick: $\dot{x}\frac{dy}{dx} = \dot{y}$. For the system this gives $x\frac{dy}{dx}=-y+x^2$. We insert our power series $y(x) = \sum^\infty_{k=2}a_kx^k$ into the equation to give $\sum^\infty_{k=2}a_kkx^k = \sum^\infty_{k=2}(-a_k)x^k + x^2$. Comparing coefficients gives $a_2 = \frac{1}{3}$ and $a_k = 0$, for all $k > 2$. Hence, $y(x) = \frac{x^2}{3}$. This is correct and I'm fine working through this. But I don't know how to use this to find the stable manifold (which turns out to be $x = 0$). I've tried interchanging $x$ and $y$ to try and end up with an equation in $x$ but I can't actually get anywhere with it. How would I go about finding the stable manifold?","['manifolds', 'ordinary-differential-equations', 'dynamical-systems']"
1805025,How can $e^a = 0$ when integrating?,"I was doing a question which asked me to turn the following into polar equation:$$(y + x − x(x^2 + y^2))\frac{dy}{dx} = y − x − y(x^2 + y^2)$$ With a lot of messy algebra I can get it down to:
$$\frac{1}{2}\ln \left\lvert\frac{{r^{2} - 1}}{r^2}\right\rvert = \theta + k.$$
Which is definitely correct. From there:
$$\pm{e^{2\theta+2k}}=\frac{r^2-1}{r^2}$$
let $e^{2k}=A$
$$\pm{}Ae^{2\theta}=1-\frac{1}{r^2}$$
$$1\mp{Ae^{2\theta}}=\frac{1}{r^2}$$
$$r^2=\frac{1}{1\mp{Ae^{2\theta}}}$$
As $ A>0$ and $e^{2k}\neq0$, where $C\neq0$
$$r^2=\frac{1}{1+Ce^{2\theta}}$$
At this point, I'm asked to sketch the graph for the different possible values of C, which I assumed meant for $C>0$ and $C<0$, however apparently $C=0$ is valid. I'm struggling to grasp why it's possible. Would that mean that $k=-\infty$? If anyone can explain this / correct any issues with my working, I'd greatly appreciate it","['integration', 'exponential-function', 'polar-coordinates']"
1805030,"Conjecture $\sum_{n=1}^\infty\frac{\ln(n+2)}{n\,(n+1)}\,\stackrel{\color{gray}?}=\,{\large\int}_0^1\frac{x\,(\ln x-1)}{\ln(1-x)}\,dx$","Numerical calculations suggest that
$$\sum_{n=1}^\infty\frac{\ln(n+2)}{n\,(n+1)}\,\stackrel{\color{gray}?}=\,\int_0^1\frac{x\,(\ln x-1)}{\ln(1-x)}\,dx=1.553767373413083673460727...$$
How can we prove it? Is there a closed form expression for this value?","['closed-form', 'integration', 'definite-integrals', 'summation', 'sequences-and-series']"
1805032,Prove $(x+2y+z) ( \frac{x}{y} +\frac{2y}{z}+\frac{z}{x}) > 12$ for $x^2+y^2+z^2=3$,"Let $x,y,z > 0$ and $x^2+y^2+z^2=3$ , prove that $$(x+2y+z)\cdot \left( \frac{x}{y} +\frac{2y}{z}+\frac{z}{x}\right) > 12.$$ The coefficient $2$ destroys the symmetry of this inequality and makes the use of C-S to be impossible. I guess someone experiences with classical inequality can help. Thanks","['algebra-precalculus', 'inequality']"
1805033,"Is there any conclusion about a group, if the group has unique element of order $n>1$?","If a group $G$ has an unique element of order $n>1$, then which of the following is true: Order of $G$ is $n$. Order of $Z(G)$ is greater than $n$. $Z(G)=G$ $G=S_2$ (I've seen that (1) can not be true. Because order of $\mathbb{Z}_5$ is $5$, but there are $4$ elements of order $5$ in $\mathbb{Z}_5$.) Firstly, we know that in any group the number of elements of order $n$ is $\phi (n)$ or its multiple, if $n$ divides the order of the group.
Now, $\phi (n)=1$ gives $n=1$, $n=2$.
But, according to the question, n>1.
So $n=2$ only.
i.e. it's given that the group contain an unique element of order $2$. But which option is correct between $2, 3, 4$?","['abstract-algebra', 'group-theory']"
1805035,Prove $\sum_{k=0}^{n}\frac{n!}{k!}(n-k)n^k=n^{n+1}$ for any $n\in\mathbb N$.,"I want to prove the following: $$\sum_{k=0}^{n}\frac{n!}{k!}(n-k)n^k=n^{n+1}\quad\text{for any $n\in\mathbb N$.}$$ I tried induction and invoking the binomial theorem, to little avail. I’m looking for some quick and dirty solution. Thanks for any hints. As the answers below reveal, the following update I had added earlier is not really of much use, so I struck it. Update: After some rearrangements, the left-hand side above can be rewritten as $$\require{enclose}
     \enclose{horizontalstrike}{n\sum_{k=0}^{n-1}\binom{n-1}{k}n^k(n-k)!}$$ This form seems to suggest resorting to the binomial theorem.","['algebra-precalculus', 'combinatorics']"
1805078,Candies withdrawal probability for a particular subsequence,"You are taking out candies one by one from a jar that has 10 red candies, 20 blue candies, and 30 green candies in it. What is the probability that there are at least 1 blue candy and 1 green candy left in the jar when you have taken out all the red candies?","['combinatorics', 'probability']"
1805091,Can $A \times B$ give you a circle of radius $1$.,I've heard that $A \times B$ cannot be a circle of radius $1$. However I think this is false as I know from $\sin^2 x+\cos^2 x=1$. So can we take the set $A=\{\sin \theta | \theta \in R \}$ and $B=\{\cos \theta | \theta \in R\}$ or does this not work?,"['parametrization', 'elementary-set-theory']"
1805110,"Find an injective function that maps $\mathbb{R} \to (-\infty, 0]$","I'm looking for any ideas as to a function which maps $\mathbb{R} \to (-\infty, 0]$. I considered $-|x|$ but realised that is not injective.",['functions']
1805114,Dividing a Checkerboard into L-Shaped Regions,"In preparation for the GRE Math-Subject test, and honestly for the fun of it, I've been working through a select number of my texts. The first of which is Saracino's Abstract Algebra text. I was hoping this wouldn't happen, but I've been very quickly stumped. The following is Exercise 0.22, and I'm looking for a few hints. ""Prove that the follow statement is true for any integer $n \geq 1$: If the number of squares in a ""checkerboard"" is $2^n \times 2^n$ and we remove any one square, then the remaining part of the board can be broken up into L-shaped regions each consisting of 3 squares."" Before I wrote this proof, I went ahead and pulled out a sheet of paper and did a little testing, because I'm not exactly sure if all of these have to be correct. I found that I could remove pretty much any square on the $2^2 \times 2^2$ board and not be able to break it up. This leads me to believe that the statement is false. Any advice?","['induction', 'proof-writing', 'elementary-set-theory']"
1805161,Parametric equations for intersection between plane and circle,"So I was looking at this question Determine Circle of Intersection of Plane and Sphere but I need to know how to find a parametric equation for intersections such as these.  My particular question is to find a parametric equation for the intersection between the plane $$x+y+z=1$$ and unit sphere centered at the origin. I started out my question by substituting 
$$
z=-x-y+1$$ into 
$$x^2+y^2+z^2=1  $$ deriving 
$$x^2+y^2+(-x-y+1)^2=1$$
and getting $$2x^2+2y^2+2xy-2x-2y=0$$ but I am unsure how to proceed from here. I also tried to use the vector equation of the plane 
$$r(u, v)=(0,0,1)+(2,1,-3)u+(1,1,-2)v$$ but I am not sure how that would help.","['algebra-precalculus', 'multivariable-calculus']"
1805162,Group extension that doesn't realize a coupling,"Let $E$ be an extension of $N$ by $G$: $$N \hookrightarrow E \twoheadrightarrow G$$ If $N$ is abelian, then $E$ uniquely defines an action of $G$ on $N$. More generally, it defines a unique class $\chi$ on: $$\text{Out}(N) = \text{Aut}(N)/\text{Inn}(N)$$ We call the pair $(G, \chi)$ a coupling of $G$ to $N$. Robinson says: [...] principal aims of the theory of group extensions may be summarized as follows: (i) to decide which couplings of $G$ to $N$ give rise to an extension of $N$ by $G$; Unfortunately, I'm failing to find a counter example, a coupling of $G$ to $N$ that does not gives rise to an extension of $N$ by $G$. So far, I've looked only at finite, abelian groups $N$. Can someone point me to such counter example?","['group-actions', 'group-extensions', 'coupling', 'group-theory']"
1805175,Why not define infinite derivatives?,"Is there any particular reason ""infinite"" derivatives are not well-defined? For example, $x \mapsto x^{\frac 13}$ at $x=0$. More precisely, what is wrong with the following definition of differentiability? Let $f:I \to \mathbb{R}$ be a  real function and let $c$  be an interior point of $I$. If $f$ is continuous at $c$ and the limit
  $$\lim_{h \to 0} \frac{f(c+h) - f(c)}{h}$$ exists and is equal to $L$, where $L \in \mathbb{R} \cup \{+\infty, -\infty\}$, $f$ is said to be differentiable at $c$. There doesn't seem to be anything obviously wrong with this. The most important theorem, the mean value theorem, still holds with this definition, according to Wikipedia. Furthermore, unless I'm mistaken, with this definition there is the nice property that if $f:(a,b) \to \mathbb{R}$ is injective and differentiable, $f^{-1}$ is as well. I am assuming, of course, that there are problems since we use the stricter definition: what are they?","['derivatives', 'real-analysis', 'calculus']"
1805181,Prove that a $k$-regular bipartite graph has a perfect matching,Prove that a $k$-regular bipartite graph has a perfect matching by using Hall's theorem. Let $S$ be any subset of the left side of the graph. The only thing I know is the number of things leaving the subset is $|S|\times k$.,"['bipartite-graphs', 'combinatorics', 'graph-theory', 'matching-theory']"
1805184,The Dirac delta does not belong in L2,"I need to prove that Dirac's delta does not belong in $L^2(\mathbb{R})$. First, I found the next definition of Dirac's delta $$\delta :D(\mathbb R)\to \mathbb R$$ is defined by: $$\langle \delta,\varphi \rangle=\int_{-\infty}^{+\infty}\varphi(x)\delta(x)\,\mathrm{d}x = \varphi(0),$$ and $$\delta(x)= \begin{cases} 1,& x= 0\\ 0 ,& x\ne 0. \end{cases} \\$$ The space $L^2(\mathbb{R})=\{f:f \text{ is measurable and } \|f\|_{2}<+\infty \}$. I'm thinking suppose otherwise, i.e, Dirac's delta in $L^2$, but I have problems to prove that Dirac's delta is measurable, but I suspect that in calculating of $\|f\|_2$ I'll find the contradiction. Could you give me any suggestions??","['measure-theory', 'analysis']"
1805191,Calculating SVD by hand: resolving sign ambiguities in the range vectors.,"When calculating the SVD of the matrix $$A = \begin{bmatrix}3&1&1\\-1&3&1\end{bmatrix}$$ I followed these steps $$A A^{T} = \begin{bmatrix}3&1&1\\-1&3&1\end{bmatrix} \begin{bmatrix}3&-1\\1&3\\1&1\end{bmatrix} = \begin{bmatrix}11&1\\1&11\end{bmatrix}$$ $$\det(A A^{T} - \lambda I) = (11-\lambda)^{2} - 1 = 0$$ Hence, the eigenvalues are $\lambda_{1} = 12$ and $\lambda_{2} = 10$. When $\lambda_{1} = 12$: $$ \begin{bmatrix}11-\lambda_{1}&1\\1&11-\lambda_{1}\end{bmatrix} \begin{bmatrix}x_{1}\\x_{2}\end{bmatrix} = \begin{bmatrix}0\\0\end{bmatrix}$$ $$x_{1} = x_{2} \implies u_{1} = \begin{bmatrix}t\\t\end{bmatrix}$$ And for $\lambda_{2} = 10$: $$x_{1} = -x_{2} \implies u_{2} = \begin {bmatrix}t\\-t\end{bmatrix}$$ Now $$U = \begin {bmatrix} u_{1}&u_{2} \end{bmatrix}$$ $u_{1}$ and $u_{2}$ are orthonormal.
So the for $u_{1} = \begin{bmatrix}t\\t\end{bmatrix}$ , $u_{2} = \begin{bmatrix}t\\-t\end{bmatrix}$ I know $\left| t  \right| = \frac{1}{\sqrt{2}}$ and $u_{1}.u_{2}=0$. My question how can we decide about the sign? For example I think both $U=  \begin{bmatrix}\frac{1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &\frac{-1}{\sqrt{2}} \end{bmatrix}$ and $U=\begin{bmatrix}\frac{-1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \end{bmatrix}$ could be answers. Then Which one should I choose? ====== Update1: Based on answers posted I rewrite:
$u_{1} = sgn (t_1) \begin{bmatrix} \frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\end{bmatrix}$ $u_{2} = sgn (t_2) \begin{bmatrix} \frac{1}{\sqrt{2}}\\ \frac{-1}{\sqrt{2}}\end{bmatrix}$ $$U= \begin{bmatrix} \frac{1}{\sqrt{2}}& \frac{-1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_1)&0 \\ 0& \operatorname{sgn} (t_2) \end{bmatrix}$$ ====== Update2: I continued by calculation of $V$ as follow: $ A^{T} A = \begin{bmatrix}3&-1\\1&\\1&1\end{bmatrix} \begin{bmatrix}3&1&1\\-1&3&1\end{bmatrix}  = \begin{bmatrix}10&0&2\\0&10&4\\2&4&2\end{bmatrix}$ $det( A^{T} A- \lambda I)=0$ $\lambda_{1} = 12 , v_1 =  sgn(t_3) \begin{bmatrix}t_{3}\\ 2t_{3} \\ t_{3} \end{bmatrix}$ $\lambda_{2} = 10 ,  V_{2} = sgn(t_4) \begin{bmatrix}t_{4}\\ -0.5t_{4} \\ 0 \end{bmatrix}$ $\lambda_{3} = 0 ,  V_{3} = sgn(t_5) \begin{bmatrix}t_{5}\\ 2t_{5} \\ -5t_{5} \end{bmatrix}$ $V= \begin{bmatrix}\frac{1}{\sqrt{6}} &\frac{2}{\sqrt{5}} &\frac{1}{\sqrt{30}}\\ \frac{2}{\sqrt{6}}&\frac{-1}{\sqrt{5}}&\frac{2}{\sqrt{30}}\\ \frac{1}{\sqrt{6}}& 0& \frac{-5}{\sqrt{30}}\end{bmatrix}\begin{bmatrix} \operatorname{sgn} (t_3)&0&0 \\ 0& \operatorname{sgn} (t_4)&0\\ 0&0& \operatorname{sgn} (t_5) \end{bmatrix}$ I try to check if all possible answers for U and V are valid : $A = U\Sigma V^{*}$
 $A = \begin{bmatrix} \frac{1}{\sqrt{2}}& \frac{-1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_1)&0 \\ 0& \operatorname{sgn} (t_2) \end{bmatrix} \Sigma  (\begin{bmatrix}\frac{1}{\sqrt{6}} &\frac{2}{\sqrt{5}} &\frac{1}{\sqrt{30}}\\ \frac{2}{\sqrt{6}}&\frac{-1}{\sqrt{5}}&\frac{2}{\sqrt{30}}\\ \frac{1}{\sqrt{6}}& 0& \frac{-5}{\sqrt{30}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_3)&0&0 \\ 0& \operatorname{sgn} (t_4)&0\\ 0&0& \operatorname{sgn} (t_5) \end{bmatrix} )^{*} $
 $A = \begin{bmatrix} \frac{1}{\sqrt{2}}& \frac{-1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_1)&0 \\ 0& \operatorname{sgn} (t_2) \end{bmatrix} \Sigma  \begin{bmatrix} \operatorname{sgn} (t_3)&0&0 \\ 0& \operatorname{sgn} (t_4)&0\\ 0&0& \operatorname{sgn} (t_5) \end{bmatrix} \begin{bmatrix}\frac{1}{\sqrt{6}} &\frac{2}{\sqrt{5}} &\frac{1}{\sqrt{30}}\\ \frac{2}{\sqrt{6}}&\frac{-1}{\sqrt{5}}&\frac{2}{\sqrt{30}}\\ \frac{1}{\sqrt{6}}& 0& \frac{-5}{\sqrt{30}}\end{bmatrix}^{*} $ When I assigned $U=  \begin{bmatrix}\frac{-1}{\sqrt{2}} &\frac{-1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \end{bmatrix}$ 
and $V= \begin{bmatrix}\frac{-1}{\sqrt{6}} &\frac{-2}{\sqrt{5}} &\frac{-1}{\sqrt{30}}\\ \frac{-2}{\sqrt{6}}&\frac{1}{\sqrt{5}} & \frac{-2}{\sqrt{30}} \\ \frac{-1}{\sqrt{6}}& 0& \frac{5}{\sqrt{30}}\end{bmatrix}$ and $\Sigma = \begin{bmatrix}\sqrt{20}&0&0\\ 0&\sqrt{10}&0\end{bmatrix}  $in $A = U\Sigma V^{*}$
I got the A. But when I updated U as $U = \begin{bmatrix}\frac{1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &\frac{-1}{\sqrt{2}} \end{bmatrix}$, it produced -A.
This probably means certain version of $U$ and $V$ will reproduce A. I haven't  figured how should I choose them.","['eigenvalues-eigenvectors', 'matrices', 'matrix-decomposition', 'svd', 'linear-algebra']"
1805212,"Integral ${\large\int}_0^\infty\big(2J_0(2x)^2+2Y_0(2x)^2-J_0(x)^2-Y_0(x)^2\big)\,dx$","I'm interested in the following definite integral:
$$\int_0^\infty\big(2J_0(2x)^2-J_0(x)^2+2Y_0(2x)^2-Y_0(x)^2\big)\,dx,\tag1$$
where $J_\nu$ and $Y_\nu$ are the Bessel functions of the first and the second kind. Mathematica evaluates this integral symbolically to $\frac{\ln2}\pi$, but the result of a numerical integration looks more like $\frac{\ln4}\pi$, so I suspect the symbolic result is incorrect. Moreover, it looks like both components converge and make equal contribution:
$$\int_0^\infty\big(2J_0(2x)^2-J_0(x)^2\big)\,dx\stackrel?=\int_0^\infty\big(2Y_0(2x)^2-Y_0(x)^2\big)\,dx\stackrel?=\frac{\ln2}\pi,\tag2$$
but it is more difficult to check numerically, because both integrands here are oscillating (unlike $(1)$ where the integrand looks monotonic). How can we find values of these integrals and prove them correct? Can we generalize results for values of the index $\nu$ other than $0$?","['bessel-functions', 'closed-form', 'integration', 'definite-integrals', 'special-functions']"
1805247,"Given a rocket with constant acceleration after t = 4, when will it hit the ground?","A rocket is launched straight upward. During the first four seconds of powered flight, its height is given by: $h(t) = 16.1t^2 − 1.75t^3$ The function is valid when $0 ≤ t ≤ 4$ $t$ in seconds and $h$ in feet. At the instant when $t = 4$ seconds, the fuel cuts off. From that point in time onward, the rocket has constant acceleration of $-32.2 ft/s^2$. When does it hit the ground? Given this information I have hypothesized: $h'(t) = -32.2t$ $h''(t) = -32.2$ At the instant fuel cuts off $h(4)=145.6$ Taking two anti-derivatives, the $h(t)$ for $t > 4$ is now: $h(t)= -16.1t^2+403.2$ Solving for $h(t)=0$ at $t ≥ 4$ gives $t=5.004$ but this is not the correct answer. Can someone help me understand what I'm missing? Graph of $h(t)$ and $y=0$ intersection provided: https://www.desmos.com/calculator/hddvv1de1z","['derivatives', 'physics', 'calculus']"
1805277,"If two coins are flipped and one gets head, what is the probability that both get head?","I have a doubt because I think that once the result of the first coin is obtained, just simply await the outcome of the second, which is completely independent of the previous one, and then we have a chance of $\frac12$ to get a head again. But someone else tells me that as the possible events are: Head - Head Head - Tail Tail - Head Tail - Tail then when we get a head we restrict ourselves to the first three cases, so the probability would be $\frac13$. What is the right way? I know there's a difference between saying ""first came head"" to say ""one of the two came head"", but if we have the first fact, aren't we supposed to know which one is that came head?","['combinatorics', 'probability', 'discrete-mathematics']"
1805283,Find the range of $a$ for $a^2 - bc - 8a + 7 = 0$ and $b^2 + c^2 + bc - 6 a + b = 0$.,"Let $a, b, c$ be real numbers such that \begin{cases}
a^2 - bc - 8a + 7 = 0\\
b^2 + c^2 + bc - 6 a + b = 0\\
\end{cases} Find the range of $a$. By adding the two equations I have $$(a - 7)^2 + (b + \frac 12)^2 + c^2 = \frac {169}4,$$
thus $a \in [\frac 12, \frac {27}2]$. Then I found that $a$ cannot obtain the boundaries so $a \in (\frac 12, \frac {27}2)$. But this is only a rough restriction. Exactly how do I solve this? Edit: When I got this question from my friend, it was a $b$, but after I saw the official solution, it should be a $6$ instead. So the official answer I posted is actually not the answer to the question here but the answer to the original question. I'm really sorry for this mistake.","['algebra-precalculus', 'calculus']"
1805296,"Represent a Uniform[0,1] random variable as a sum of independent Bernoulli(1/2) random variables","With $X \sim U[0,1]$, Lecturer says that $X = \sum_{k\ge 1} B_k(\frac{1}{2}) 2^{-k}$ where the $B_k(\frac{1}{2})$ are independent Bernoulli random variables with parameter $1/2$. I have no idea how we can express uniformly distributed RV like that. Can anyone please explain? Thanks,",['probability-theory']
1805303,"If $\lambda$ is the eigen-value of a $n\times n$ non-singular orthogonal matrix $A$, then prove that $\frac{1}{\lambda}$ is also an eigen-value.","QUESTION : If $\lambda$ is the eigen-value of a $n\times n$ non-singular matrix $A$ and $A$ is a real
  orthogonal matrix, then prove that $\frac{1}{\lambda}$ is an
  eigen-value of the matrix $A$. MY ATTEMPT: Since $\lambda$ is the eigen-value of a $n\times n$ matrix $A$, we have 
$$|A-\lambda I_n|=0$$
Also since $A$ is a real orthogonal matrix,we have 
$$AA^T=A^TA=I_n$$
So we can conclude that $$|A-\lambda( AA^T)|=0$$ Or, $$|\lambda A\left(\frac{1}{\lambda}I_n- A^T\right)|=0$$ Or, $$\left|\lambda A\right|\cdot \left|\frac{1}{\lambda}I_n- A^T\right|=0$$ Or, since $A$ is non-singular, $$\left|A^T-\frac{1}{\lambda} I_n\right|=0$$ So, we can conclude that $\frac{1}{\lambda}$ is an eigen-value of the matrix $A^T$. But how do I prove that $\frac{1}{\lambda}$ is an eigen-value of the matrix $A$ ? Is my working faulty? Or is there a mistake in the question? Please help.","['matrices', 'eigenvalues-eigenvectors', 'algebra-precalculus', 'linear-algebra']"
1805331,Finding side of a parallelogram,"In a parallelogram $ABCD$, the bisector of angle $ABC$ intersects $AD$ at $P$ where $PD=5, BP=6$ and $CP=6$. Find $AB$. I tried equating areas and heron's formula but no luck..",['geometry']
1805333,prove the result of a Laplace transformation,"I have to prove the next problem $$\mathcal{L}
 \left(\int_{0}^{t}\frac{1-e^{-u}}{u}du,s\right) = \frac{1}{s}\log\left(1+\frac{1}{s}\right)$$ I'm quite new in the subject and I have troubles with this one. There's no need to put the step by step, just the initial ones to know that I'm going in the right way 
(but if you want to,that would be great) Thanks!","['laplace-method', 'ordinary-differential-equations', 'integers', 'laplace-transform']"
1805457,How to prove that the image of a prime ideal is also a prime ideal,"If $f:A\rightarrow R$ be a ring homomorphism, where $A$ and $R$ are commutative rings. If $f$ is surjective  and $P$ is a prime ideal in $A$,  how to prove that $f(P)$ is a prime ideal in $R$? This a an exercise I came across while self-studying joseph rotman's book advanced modern algebra. I searched this website and find similar questions but they have another condition: $P$ contains the kernel of $f$. However, I want to know how to prove this result without that condition. Give thanks to any useful help！","['abstract-algebra', 'ring-theory']"
1805495,The quotient norm on $\ell^{\infty}(\mathbb{N}) / c_0 (\mathbb{N})$ is given by $\limsup_{n \in \mathbb{N}} |x_n|$.,"I try to show that the norm on the quotient space $\ell^{\infty}(\mathbb{N}) / c_0 (\mathbb{N})$ is given by $\limsup_{n \in \mathbb{N}} |x_n|$, where $x = (x_n)_{n \in \mathbb{N}} \in \ell^{\infty} (\mathbb{N})$. My attempt is the following: By definition, the quotient norm $\| \cdot \|_{\ell^{\infty} / c_0}$ is defined as
  $$ \| x + c_0 (\mathbb{N}) \|_{\ell^{\infty} / c_0} = d( x + c_0 (\mathbb{N}), 0 + c_0 (\mathbb{N}) ), $$
  which can be re-written as 
  $$ \| x + c_0 (\mathbb{N}) \|_{\ell^{\infty} / c_0} = \inf \{ \| x + y \|_{\infty} \; | \; y \in c_0 (\mathbb{N}) \}. $$
  Since $0 = (0,0,...) \in c_0 (\mathbb{N})$, it follows that
  $$ \| x + c_0 (\mathbb{N}) \|_{\ell^{\infty} / c_0} \leq \| x \|_{\infty} = \sup_{n \in \mathbb{N}} |x_n|. $$ I thought about using Bolzano-Weiserstrass' Theorem at a certain point, but I am not sure how to obtain the wished equality from here. Any help is appreciated. Thanks in advance.","['functional-analysis', 'normed-spaces', 'banach-spaces']"
1805525,Summation of L1-function values finite,"For $f \in L^1(\mathbb{R})$, we can proof, that $\sum_{n\in \mathbb{Z}} |f(t+na)|<\infty$ ALMOST EVERYWHERE for $t\in [0,a]$ ($\star$ proof below) My Question: If we assume $f\in L^1(\mathbb{R})\cap C(\mathbb{R})$, can we conclude
$$\sum_{n\in \mathbb{Z}} |f(t+na)|<\infty \text{ for all }t\in [0,a]\ \ \ ?$$ I would especially need to show: $\sum_{n\in \mathbb{Z}} |f(na)|<\infty$ $\star$ proof: Define $f_n=\chi_{(na,(n+1)a]}\cdot f \Rightarrow\ \sum_{n\in \mathbb{Z}}|f_n|=|f|\ \ \forall x\in \mathbb{R}$ If we use monotone convergence and shifting invariance of the Lebesgue integral we get:
$$\infty>\int_\mathbb{R} |f(t)|dt=\int_\mathbb{R}\sum_{n\in \mathbb{Z}}|f_n(t)|dt = \sum_{n\in \mathbb{Z}}\int_\mathbb{R}|f_n(t)|dt=\sum_{n\in \mathbb{Z}}\int_{(na,(n+1)a]}|f(t)|dt=$$
$$=\sum_{n\in \mathbb{Z}}\int_0^a|f(t+na)|dt=\int_0^a\sum_{n\in \mathbb{Z}}|f(t+na)|dt$$
So we can conclude: $\sum_{n\in \mathbb{Z}}|f(t+na)|<\infty$ a.e. for $t\in[0,a]$","['real-analysis', 'functional-analysis', 'convergence-divergence', 'sampling-theory', 'sequences-and-series']"
1805528,"Solve for integers $x, y, z$ such that $x + y = 1 - z$ and $x^3 + y^3 = 1 - z^2$.","Solve for integers $x, y, z$ such that $x + y = 1 - z$ and $x^3 + y^3 = 1 - z^2$ . I think we'll have to use number theory to do it. Simply solving the equations won't do. If we divide the second equation by the first, we get: $$x^2 - xy + y^2 = 1 + z.$$ Also, since they are integers $z^2 \ge z \implies -z^2 \le -z$ . This implies $$x + y = 1 - z \ge 1 - z^2 = x^3 + y^3.$$ This shows that atleast one of $x$ and $y$ is negative with the additive inverse of the negative being larger than that of the positive. I have tried but am not able to proceed further. Can you help me with this? Thanks.","['diophantine-equations', 'polynomials', 'number-theory', 'cubics', 'elementary-number-theory']"
1805550,Evaluating $\int_{-1}^{1} \frac{dx}{(e^x+1)(x^2+1)}$ [duplicate],"This question already has an answer here : Evaluate the integral $\int_{-1}^1\frac{dx}{(e^x+1)(x^2+1)}$. (1 answer) Closed 5 years ago . I would like to evaluate the following integral:
$$
\int_{-1}^{1}  \frac{dx}{(e^x+1)(x^2+1)} 
$$
I tried various methods but without success.","['real-analysis', 'calculus', 'closed-form', 'integration', 'definite-integrals']"
1805587,"Prove that $\mathbb P(X>Y) =\frac{b}{a + b}$ if $X, Y$ are exponentially distributed with parameters $a$ and $b$.","Let $X, Y$ be an exponentially distributed random variables with parameters $a, b$. Then $X$ has pdf: 
  $$f_X(x) =\begin{cases} a e^{-a x},& x\geq 0\\
0,& \text{otherwise}.\end{cases}$$ Suppose $X$ and $Y$ independent. Show that $$\mathbb P(X>Y) = \frac{b}{a+b}.$$ Now I thought the following:
$$f(x,y) = f_X(x)\ f_Y(y) = abe^{-ax -by},\qquad\text{for } x,y > 0.$$
And then $$\mathbb P(X>Y) = \int_0^\infty \int_0^x a b e^{-ax -by}\,dydx$$
However, if I solve this (manually or using Wolframalpha), I can't seem to end up with $\frac{b}{a+b}$. Any ideas?","['independence', 'probability-distributions', 'integration', 'probability', 'exponential-distribution']"
1805602,Esscher Transform extended,"This problem is almost solved, dont get scared by the massive text The Esscher-transform is a well know tool in the financial section.
I posted this in statistics also, since it relates to continuous sampling.
Im not sure if my approach is right, so it would be nice, if someone could check this. Given a Levy-process $(X_t)_{t\geq 0}$ under $P$ on $(\Omega,\mathcal{F})$ Let be $u$ be real such that the cumulant generating function of $X_t$ given by $\phi_{t}(u)=log(E[exp(uX_t)])$ is finite. We have with levy-properties $\phi_{t}(u)=t\phi_{1}(u)$. We know that the Esscher-transform $$
Z_{t}=e^{uX_t-t\phi_{1}(u)} \tag 1
$$
is a positive Martingale wrt to $\mathcal{F}_t=\sigma(X_s:s\leq t)$ and this density defines a measure $Q|\mathcal{F}_t$. We can define a measure $Q$ (by extension for $t\rightarrow \infty$) under which $(X_t)_{t\geq 0}$ is again a Levyprocess and its density on $\mathcal{F}_t$ is given by
$$
\frac{dQ}{dP}|_{\mathcal{F}_t}=\frac{dQ}{dP}((X_{s})_{0\leq s \leq t})=Z_{t} 
$$ Now however the restirction of $dQ/dP$ on $\mathcal{F}_t$ first says that $Z_t$ is an $\mathcal{F}_t$ measurable function and can thus be expressed almost surely as an function of the sample path $(X_s)_{s\leq t}$. Like in sequential statistics, we observe on $[0,t]$ and the induced likelihood-ratio $Z_t$ depends on the process observed on this interval. I want the understanding, why one says, that the induced measure $Z_t$ of the trajectory $(X_s)_{s\leq t}$ is only a function of the last $X_t$.
So i want an understanding how we get this expression of $Z_t$. Typically in my understanding one has first to construct $Z_t$ in a common sense
$$
Z_{t}=e^{\int_{0}^{t}udX_s-\int_{0}^{t}\phi_{1}(u)ds} \tag 2
$$
where you literally see, that the density is really a function of $(X_s)_{s\leq t}$, but which in the end really coincides with (1) then. This connects to my 2nd approach. There are possibly  approaches: Maybe since the cumulant generating function of $X_t$ $\phi_t$ is $t\cdot \phi_1$. So that the one dimensional distribution of $(X_t)_{t\geq 0}$ totally determine the law of the process, such that $X_t$ is enough to describe? First approach: This is not so clear for me. Maybe one can help me here. Maybe one can argue with the bayes forumla of conditional expectation, since the density process $Z_{t}$ is a Martingale wrt to $\mathcal{F}_t$ or general by the conditional expectation.
Where for the second assertion we have for $s<t$ that
$$
E_{Q}[Z_{t}|\mathcal{F}_s]=e^{uX_s-s\phi_1(u)}=Z_s
$$
or for $A \in \mathcal{F}_s$
$$
Q_{|\mathcal{F}_t}(A)=E_{P}[L_t 1_{A}]=E_{P}[E_{P}[L_t 1_A|\mathcal{F_s}]=E_{P}[L_s 1_A]=Q_{|\mathcal{F}_s}(A)
$$
which states, that the measures are consistent. Second approach:
We construct (2). But i dont know if the way is correct. We know that the induced measure of a path $(X_s)_{s\leq t}$, is generated by its finite dimensional distributions.
Basawa ""Statistical inference for Stochastic Processes"" p. 181 recommends to calculate the finite dimensional distributions  of $X=(X_{t_0},X_{t_1},\ldots X_{t_n})$ for $0=t_0<t_1<\ldots<t_n=t$ and let $\max_{1\leq i \leq n} t_i -t_{i-1}\rightarrow 0$ as $n\rightarrow \infty$ For a sequece $K_{i}$ of independent random variables on $(\Omega,\mathcal{F},P)$ with finite mean we have with $S_{n}=\sum_{i=1}^{n}K_{i}$
$$
Z_n=\frac{e^{uS_n}}{E[e^{uS_n}]}
$$
Where $Z_{n}$ can be regarded as the density of the $n$-times product space wrt to $P^{\otimes n}$ and $Z_{n}=\prod_{i=1}^{n} e^{uK_{i}}/[Ee^{uK_{i}}]$
on $(\prod_{i=1}^{n} \Omega ,\otimes_{i=1}^{n} \mathcal{F})$. Lets $(K_{1},\ldots,K_{n})=(X_{t_1}-X_0(=0),X_{t_2}-X_{t_1},\ldots,X_{t_n}-X_{t_{n-1}})$ the increments of the increments of the levy process.
Then
$$
Z_n=e^{(uS_{n})-t_1\phi_{1}(u)-(t_2-t_1)\phi_{1}(u)-\ldots (t_n-t_{n-1})\phi_{1}(u)}
$$
Letting $\max_{1\leq i \leq n} t_i -t_{i-1}\rightarrow \infty$ as $n\rightarrow \infty$ we can conclude (2). The law of $(X_s)_{s\leq t}$ is completely determined by the law of finite dimensional distributions. $Z_{t}$ is generated by sampling the increments on $[0,t]$. Since $X_t$ is Markov, the finite dimensional distribution is the product of the incremental densitys. This should show (2) i hope?","['stochastic-processes', 'probability-theory', 'statistics', 'probability', 'measure-theory']"
1805630,Hausdorff space if no net converges to two different values,"Having trouble to prove the following statements regarding topological spaces: Let $(X,\mathcal T)$ be a topological space. a) Show that for all mutually distinct points $x,y \in X$ there are open sets $U,V$ with $U\cap V = \emptyset$ and $x\in U, y \in V$ if, and only if, no net in $X$ converges to two different values. b) Let $\mathcal T'$ be a topology on $X$ and for all nets in $X$ it holds that they converge in $(X, \mathcal T)$ if, and only if, they converge in $(X, \mathcal T')$. Show that $\mathcal T = \mathcal T'$. Anyone can help?","['general-topology', 'nets', 'separation-axioms', 'limits']"
1805643,A curious approximation to $\cos (\alpha/3)$,"The following curious approximation $\cos\left ( \frac{\alpha}{3} \right ) \approx \frac{1}{2}\sqrt{\frac{2\cos\alpha}{\sqrt{\cos\alpha+3}}+3}$ is accurate for an angle $\alpha$ between $0^\circ$ and $120^\circ$ In fact, for $\alpha = 90^\circ$, the result is exact. How can we derive it?","['approximation-theory', 'real-analysis', 'trigonometry', 'approximation']"
1805648,"Prove that If a closed curve on the unit sphere has length less than $2\pi$, then it is contained in some hemisphere.","I found the following proof in a book: ""Pick any point P on the curve, travel half way around the curve to the
point $Q$, and let $N$ (standing for North Pole) be the point half-way between $P$ and $Q$. (Since the distance $d(P,Q)$ from $P$ to $Q$ is less than $\pi$, $N$ is uniquely defined). $N$ determines an “equator” and if the curve lies entirely in the northern hemisphere, we are done. Otherwise, the curve crosses the equator, and let $E$ be one of the points at which it does so. Then, we observe that $d(E,P) + d(E,Q) = \pi$, since if you poke $P$ through the equatorial plane to $P'$ on the other side, $P'$ is antipodal to $Q$; hence, $d(E,P') + d(E,Q) = \pi$.
However, for any point $X$ on the curve, $d(P,X)+d(X,Q)$ must be less
than $\pi$, and this provides the desired contradiction."" I don't quite understand how this works. I guess the idea is to construct a hemisphere so that the curve lies in it. But how does $N$ determine an equator? Don't we need at least need two Points?
And how does $d(E,P') + d(E,Q) = \pi$ indicate $d(E,P) + d(E,Q) = \pi$ ? Thanks for any help.",['geometry']
1805659,Meeting probability of two bankers: uniform distribution puzzle,"Two bankers each arrive at the station at some random time between 5PM
  and 6PM (arrival time for each of them is uniformly distributed). They
  stay exactly five minutes and then leave. What is the probability that
  they will meet on a given day? I am not sure how to go about modelling this problem as uniform distribution and solving it. Appreciate any help. Here is how I start with it: Assume banker A arrives X minutes after 5PM and B arrives Y minutes after 5PM. Both X and Y are uniformly distributed between 5PM and 6PM. So pdf of X, Y is $\frac{1}{60}$. Now A and B will meet if $|X - Y| < 5$. So required probability is $P(|X - Y| < 5)$ = Integral of joint distribution function of $|X - Y|$ from $0$ to $5$? Now not sure how to write the equation from this point onwards and solve it. Answer: $\frac {23}{144}$","['puzzle', 'uniform-distribution', 'probability', 'probability-distributions']"
1805660,Formula for the general term of the Taylor series of $\tan(x)$ at $x = 0$,"Today we were taught different expansions; one of them was the series expansion of $\tan(x)$ , $$\tan(x)=x+\frac{x^3}{3}+\frac{2x^5}{15} + \cdots .$$ So, with curiosity, I asked my sir about next term. He said to get general formula divide series expansion of $\sin x,\cos x$ . His reply didn't satisfy me. Do we have a general term for this expansion in elementary functions? If not, then why?","['taylor-expansion', 'trigonometry']"
1805708,Exterior derivative of complex differential form,"I have this question, from several complex variables: Start with the differential form: $$\omega(z)=\sum_{\nu=1}^{n} \frac{(-1)^{\nu-1}\bar{z}_{\nu}}{|z|^{2n}} d\bar{z}[\nu] \wedge dz, $$ where $dz= dz_1 \wedge ... \wedge dz_n$ and $d\bar{z}[\nu] = dz_1 \wedge ...  dz_{\nu - 1} \wedge dz_{\nu +1} ... \wedge dz_n$. Since $d(\bar{z}_{\nu} d\bar{z}[\nu]) = d\bar{z}_{\nu} \wedge d\bar{z}[\nu] = (-1)^{\nu -1} d\bar{z}$, we have $$d\omega =\sum_{\nu=1}^{n} \frac{\partial}{\partial \bar{z}_{\nu}} \bigg( \frac{\bar{z}_{\nu}}{|z|^{2n}} \bigg) d\bar{z} \wedge dz.$$ It's this line, right at the beginning, which I can't seem to compute. I'm pretty new to differential forms and exterior derivatives, so I can't see exactly how this is arrived at. I dont understand the first computation either ( $d(\bar{z}_{\nu} d\bar{z}[\nu]) = d\bar{z}_{\nu} \wedge d\bar{z}[\nu]$ etc. ).
Any help or hints would be great, thanks.","['differential-forms', 'differential-geometry', 'exterior-algebra']"
1805778,Eigenvalue of a matrix and a polynomial of that matrix,"Let $A$ be a $n \times n$ matrix over $F$, and let $c_1, ... c_n$ be its eigenvalues. Show that for every polynomial $g(x) \in F[x]$, the eigenvalues of $g(A)$ are $g(c_1), ... , g(c_n)$. I think by making $A$ an upper triangular matrix the question might be solver easily. But I have no idea how to show how many times each eigenvalue has been repeated.","['matrices', 'eigenvalues-eigenvectors', 'polynomials', 'linear-algebra']"
1805790,"Prove that, at least one of the matrices $A+B$ and $A-B$ has to be singular","Problem: Let $A$ and $B$ be real orthogonal matrices, $n$x$n$, where $n$ is an odd number. Prove that, at least one of the matrices $A+B$ and $A-B$ has to be singular. What have I done so far: -Since  matrices $A$ and $B$ are real orthogonal, it means that their determinants are $-1$ or $+1$ First I observed matrix $A+B$ :
$$A+B=AI+BI=ABB^T+BAA^T=AB(B^T+A^T)=AB(A+B)^T\Rightarrow$$ $$det(A+B)=det(AB)det(A+B)^T=det(A)det(B)det(A+B)$$ So, if $detA=detB$ we have $det(A+B)=det(A+B)$ which tells me nothing. 
But, if  $detA=-detB$ we have $det(A+B)=-det(A+B)$ which can only hapen if $det(A+B)=0$ making $A+B$ singular. Then, I tried the same for $A-B$ :
$$A-B=AI-BI=ABB^T-BAA^T=AB(B^T-A^T)=AB(A-B)^T\Rightarrow$$ $$det(A-B)=det(AB)det(A-B)^T=det(A)det(B)det(A-B)$$ So, if $detA=-detB$ we have $det(A-B)=-det(A-B)$ which can happen if $det(A-B)=0$ making $A-B$ singular. Is this correct or should I do matrix $A-B$ differently? I am also a little confused why does it have to be said that $n$ is an odd number? What should that tell me? Any help is greatly appreciated.","['matrices', 'linear-algebra']"
1805815,"If the series $\sum_{k=1}^{\infty} a_k$ is Cesàro summable and $n a_n \to 0$ as $n \to \infty$, then the series converges","I'm learning about Fourier series, specifically Cesàro summable sequences and series, and need help with the following problem: Show that if the series $\sum_{k=1}^{\infty} a_k$ is Cesàro summable and $n a_n \to 0$ as $n \to \infty$, then the series converges. The definition that is given for Cesàro summable series is as follows: Cesàro summable series. A series $\sum_{k=1}^{\infty} a_k$ is called Cesàro summable if the sequence of partial sum $S_n$ is Cesàro convergent, i.e. if
$${S_0 + S_1 + \ldots + S_n \over n + 1}$$
converges as $n \to \infty$. I'm sorry for the lack of effort on my part but I have no idea how to prove the above statement.","['real-analysis', 'fourier-series', 'fourier-analysis', 'convergence-divergence', 'sequences-and-series']"
1805843,Why all irreducible representations appear in the regular representation?,"Let $G$ be a finite group and $R$ the regular representation. That is, as a vector space $R = F(G)$ is the free vector space with basis $G$ . If the basis is $\{e_g : g \in G\}$ the action is defined by $$g \cdot e_{g'}=e_{gg'}$$ and extended by linearity. Now, in the book I'm studying the author states the following corolary: Corollary 2.18 : Any irreducible representation $V$ of $G$ over an algebraically closed field of characteristic $0$ appears in the regular representation $\dim V$ times . The ""proof"" for this is a little argument before the statement: We know tha the character of $R$ is simply $$\chi_R(g)=\begin{cases}0, & g\neq e, \\ |G|, & g= e\end{cases}$$ Thus, we see first of all that $R$ is not irreducible if $G\neq \{e\}$ . In fact, if we set $R = \bigoplus V_i^{\oplus a_i}$ , with $V_i$ distinct irreducibles, then: $$a_i = (\chi_{V_i},\chi_R)=\dfrac{1}{|G|}\chi_{V_i}(e)|G|=\dim V_i.$$ All I get from that is: if we decompose $R$ into a direct sum of irreducible representations, the multiplicites are the dimensions. But what guarantees that any irreducible representation of $G$ appears in that decomposition of $R$ ? Why all irreducible representations of $G$ appear in the direct sum decomposition of the regular representation?","['finite-groups', 'abstract-algebra', 'representation-theory', 'group-theory', 'linear-algebra']"
1805891,Norms on $\mathcal{P}_N$ Vector Space of Polynomials up to Order N,"$\|p\|_\infty :=\sup_{x\in [0,1]}|p(x)|$ and $\|p\|_{L^1}:=\int_0^1 |p(x)| dx$ . As the space of real-valued polynomials on $[0,1]$ up to order $N$ is a $N+1$ dimensional vector space and $\|\cdot\|_{\infty}$ and $\|\cdot\|_{L^1}$ are norms for it, there is a constant $C_N$ s.t. $\|\cdot\|_\infty \leq C_N \|\cdot\|_{L^1}$ . Is there an easy way how to compute this $C_N$ ?","['real-analysis', 'polynomials', 'normed-spaces', 'analysis', 'vector-spaces']"
1805930,Find the order of $U_{2n}$,"Let $n$ be an odd integer and let $k$ be the number of elements in $U_n.$ What is the order of $U_{2n}$? I have said $\left\lvert U_{2n}\right\rvert=\varphi(2n)=\varphi(2)\varphi(n)=\varphi(n)=k.$ (Since $\operatorname{hcf}(2,n)=1$). Could someone verify whether this is correct or not?","['group-theory', 'totient-function']"
1805946,GMM with full and diagonal covariances,"I have Gaussian Mixture Model-- distribution  with probability density function, that is a weighted sum of Gaussian probability density functions:
\begin{equation}
p(X)=\sum_{i=1}^k \omega_i\mathcal{N}(X,\mu_i,\Sigma_i)=\sum_{i=1}^k \omega_ip_i(X),
\end{equation} where $k$ is the number of components, $\mathcal{N}(X,\mu_i,\Sigma_i), i=1,...,k$ are Gaussian densities
with expectations (vectors) $\mu_i,i=1,...,k$ and covariance matrices $\Sigma_i,i=1,...,k$, $\omega_i,i=1,...,k$ are weights: $\sum_{i=1}^k \omega_i=1.$ Covariance matrices $\Sigma_i,i=1,...,k$,are full -- have correlation elements (non-zero non-diagonal elements). How I can approximate this GMM via GMM with components with diagonal covariances. It is understood, that it will be more components in the weighted sum, but they will be diagonal. 
Here on page 2 in is written, that it is possible (but without proof) : https://www.ll.mit.edu/mission/cybersec/publications/publication-files/full_papers/0802_Reynolds_Biometrics-GMM.pdf ""It is also important to note that because the component Gaussian are
 acting together to model the overall feature density, full covariance
 matrices are not necessary even if the features are not statistically
 independent. The linear combination of diagonal covariance basis Gaussians
 is capable of modeling the correlations between feature vector elements.
 The effect of using a set of M full covariance matrix Gaussians can be
 equally obtained by using a larger set of diagonal covariance Gaussians. "" But how it can be done and what can be say if to compare cost of calculations for these 2 cases? Is it faster to use in calculations more components, but diagonal?
Thank you.","['matrices', 'probability-distributions', 'statistics', 'stochastic-analysis', 'stochastic-calculus']"
1805953,Is this transformation of a Markov process again Markovian?,"Let $(X_t)_{t\in\mathbb{N}_0}$ be a stationary Markov process valued in $\mathbb{R}$ and $c\in\mathbb{R}$. Is the process $(Y_t)_{t\in\mathbb{N}_0}$ defined by
$$
Y_t={\bf 1}{(X_t<c)}
$$
again a Markov process (valued in $\{0,1\}$)? Here ${\bf 1}$ denotes the characteristic function, i.e. it is $1$ if $X_t<c$ and $0$ otherwise. The main example I'm interested in is $X_t$ being an $\rm{AR}(1)$ process, so I would also appreciate answers for this special case. I tried, for example, to show that
$$
\mathbb{P}(Y_t=0 \ \mid \ Y_{t-1}=0,\dots, Y_0=0)=\mathbb{P}(Y_t=0 \ \mid \ Y_{t-1}=0)
$$
using the Markov property of $Y_t$ and marginalization, but I was not able to show the equality of the resulting integrals, which lead me to believe that $Y_t$ is actually not a Markov process. I would really appreciate any help or a reference for these type of results/proofs/counterexamples.","['stochastic-processes', 'probability-theory', 'markov-chains', 'markov-process', 'probability']"
1805980,Evaluate $\lim_{x\to 1^{+}}(\sqrt{x}-1)^{x^2+2x-3}$ using L'Hôpital,"I have solved before problems with L’Hopital’s Rule but this one is giving me a headache... Here it is: $$\lim_{x\to 1^{+}}(\sqrt{x}-1)^{x^2+2x-3}$$ I know that first you need to $ \log$ it so you can get the $x^2+2x-3$ upfront and then you find the derivative till it is not longer $0/0$ or $\infty/\infty$, but I am doing all that and still can't find solution. If someone can solved it I would really appreciate it. 
Thank you",['limits']
1805985,Comparing morphisms of local nature for equivalent topologies,"Let $S$ be a scheme. It is known that the smooth topology on $\textrm{Sch}_S$ is equivalent to the étale topology, basically because every smooth covering can be refined to an étale covering. Question . Is it true, then, that all properties P of morphisms of schemes that are of local nature for the étale topology are also of
  local nature for the smooth topology? I believe this would simplify a lot some arguments, which is also why I believe the answer is no... Let me give you one example. Let P be a property of morphisms of schemes, assumed to be local on the source and target for the étale topology. Definition . A map of Deligne-Mumford $f:\mathcal X\to \mathcal Y$ has P if, given a commutative diagram $$\require{AMScd}
\begin{CD}
X @>{x}>> \mathcal X\\
@V{g}VV @VV{f}V \\
Y @>{y}>> \mathcal Y
\end{CD}$$
where $X,Y$ are schemes and $x,y$ are $\color{red}{\textrm{étale}}$ and surjective, the map $g$ has P. My question above is equivalent to the following: Question' . Can we replace $\color{red}{\textrm{étale}}$ by $\color{red}{\textrm{smooth}}$ in the above definition?",['algebraic-geometry']
1805986,The dual of the space of all the bounded functions,"I 'd like to know what is the dual space of the space of all the bounded functions on the set $X$, where $X$ can be any set. 
Also, I don't assume that the function $f$ is measurable relative to any sigma-field.  (Thus, underlying sigma-algebra is just the power set $2^X$). My conjecture is the dual space should be the space of all the finite measures with at most countable support. 
At least, integral with respect to such measures are bounded linear functional. I found in Conway's functional analysis book, Chapter VIII, section 2, Exercises 3 and 4 states that 
if $(X,B,\mu)$ is a a-finite measure space, the maximal ideal space of $L^\infty(X, B,\mu)$ is totally disconnected. Does this, combined with Gelfand representation, lead to the answer to my question? If yes, how?",['functional-analysis']
1805989,What is... A Parsimonious History?,"Interpreting historical mathematicians involves a recognition of the fact that most of them viewed the continuum as not being made out of points. Rather they viewed points as marking locations on a continuum taken more or less as a primitive notion. Modern foundational theories starting around 1870 are based on a continuum made of points and therefore cannot serve as a basis for interpreting the thinking of the earlier mathematicians as far as the foundations are concerned. What one can however seek to interpret are the techniques and procedures (rather than foundations) of the earlier authors, using techniques and procedures available in modern frameworks. In the case of analysis, the modern frameworks available are those developed by Weierstrass and his followers around 1870 and based on an Archimedean continuum, as well as more recently those developed by Robinson and his followers, and based on a continuum containing infinitesimals, and other frameworks such as the one developed by Lawvere, Kock, and others. I was therefore a bit puzzled by the following comment by a historian expressed here : Recently there have been attempts to argue that Leibniz, Euler, and even Cauchy could have been thinking in some informal version of rigorous modern non-standard analysis, in which infinite and infinitesimal quantities do exist. However, a historical interpretation such as the one sketched above that aims to understand Leibniz on his own terms, and that confers upon him both insight and consistency, has a lot to recommend it over an interpretation that has only been possible to defend in the last few decades. It is parsimonious and requires no expert defence for which modern concepts seem essential and therefore create more problems than they solve (e.g. with infinite series). The same can be said of non-standard readings of Euler; etc. Question 1. Is this historian choosing one foundational framework over another in interpreting the techniques and procedures of the historical authors? Question 2. What exactly is a Parsimonious History? Question 3. Gray and Bottazzini reportedly make a rather poetic proposal in the following terms: "" The best policy is to read on in a spirit of dialogue with the earlier authors. "" The proposal of such a conversation with, say, Euler sounds intriguing. I am only wondering about Gray's comment here that "" Euler’s attempts at explaining the foundations of calculus in terms of differentials, which are and are not zero, are dreadfully weak. "" Isn't such an opening line in a conversation likely to be a conversation-stopper? Question 4. In connection with the work of Laugwitz mentioned by one of the responders, one could ask why Gray does not cite explicitly the work of any of the authors that he wishes explicitly to criticize for using modern infinitesimals? Laugwitz's article ( Laugwitz, Detlef. Definite values of infinite sums: aspects of the foundations of infinitesimal analysis around 1820. Arch. Hist. Exact Sci.  39  (1989),  no. 3, 195–245 ) appeared in Archive for History of Exact Sciences , clearly a reputable journal since Jeremy Gray happens to be its Editor-in-Chief . Similarly, the article McKinzie, Mark; Tuckey, Curtis. Hidden lemmas in Euler's summation of the reciprocals of the squares. Arch. Hist. Exact Sci.  51  (1997),  no. 1, 29–57 appeared in the same journal and exploited Robinson's framework to clarify some of Euler's procedures; it, too, is being stonewalled by the Editor-in-Chief.","['real-analysis', 'math-history', 'nonstandard-analysis', 'soft-question', 'philosophy']"
1805995,Using pdf or marginal pdfs to calculate expected value,"I have a little doubt concerning the calculation of expected values when dealing with marginal distributions. Consider, for instance, a real bidimensional random variable $(X,Y)$ with pdf $f(x,y)$, and its marginal distributions $X$ and $Y$ having marginal pdfs $f_x(x)$ and $f_y(y)$, respectively. We then know the expected value of each component can be computed as $$\mathbb{E}[X]=\int_{\mathbb{R}}xf_x(x)dx\qquad\mathbb{E}[Y]=\int_{\mathbb{R}}yf_y(y)dy$$ But then, when you need to compute the expected value of some combination of both, you need to use the joint pdf, for example $$\mathbb{E}[XY]=\iint_{\mathbb{R}^2}xyf(x,y)dxdy$$ But, we could think of $\mathbb{E}[X]$ and $\mathbb{E}[Y]$ as particular cases of combinations between both, in which case we would do $$\mathbb{E}[X]=\iint_{\mathbb{R}^2}xf(x,y)dxdy\qquad\mathbb{E}[Y]=\iint_{\mathbb{R}^2}yf(x,y)dxdy$$ Are both of these ways of calculating $\mathbb{E}[X]$ and $\mathbb{E}[Y]$ (and similar expressions) correct? This would of course imply the following holds $$\int_{\mathbb{R}}xf_x(x)dx=\iint_{\mathbb{R}^2}xf(x,y)dxdy$$
$$\int_{\mathbb{R}}yf_y(y)dy=\iint_{\mathbb{R}^2}yf(x,y)dxdy$$","['probability-theory', 'expectation', 'density-function']"
1806004,Prove that function has only one maximum,"I have a function $f_n(x)$ with an integer parameter $n \in \{3,4,5,\dots\}$ and $x\in ]0,1[$, and i want to show that $f_n(x)$ has only one critical point for every value of $n$. The function is
$$ f_n(x)=(1 - x)^n + (1 + x)^n + \frac{
 x [(1 - x)^n - (1 + x)^n]}{ \sqrt{1 - x^2}-1}.  $$
Setting the derivative to zero, gives after some simplification and case differentiations the equation $$ \left(\frac{1-x}{1+x}\right)^n= \frac{1+n \left(1-x -\sqrt{1-x^2}\right)}{1+n \left(1+x -\sqrt{1-x^2}\right)}. \quad \quad (1)$$
So if there is only one $x \in ]0,1[$ that fullfills equation (1), the problem is solved. However i struggle to prove that. (The fact is obvious from plotting the function) Maybe there is a better way to show that, instead of setting the derivative to zero? Just as an example consider the plot of the function for $n=5$, Update (with the help of MotylaNogaTomkaMazura) If you parametrize $x=\cos(2t)$ with $t \in [0,\pi/4]$, the original function can be written as
$$ g_k(t)=\frac{\cos ^k(t)-\sin ^k(t)}{\cos (t)-\sin (t)},$$
with $k=2n+1$ and omitting a factor $2^{1 + n}$. The maximum of $g_k(t)$ is the maximum of $f_n(x)$. An equivalent form is
$$ g_k(t)=\frac{1}{\sin(t)} \sum_{n=1}^k \cos(t)^{k - n} \sin(t)^n.$$ Setting the derivative to zero gives
$$ k \left( \frac{\tan(t)}{\tan^k(t)-1}-\frac{\cot(t)}{\cot^k(t)-1} \right)=\frac{1}{\cot(t)-1}-\frac{1}{\tan(t)-1}.$$ Update 2 Another way to rewrite the function $f_n(x)$ is $$h_m(t)=\cos ^m(t) \cdot \sum _{k=0}^m \tan ^k(t),\quad \quad \quad (2)$$ with $x=\cos(2t)$, $t \in [0,\pi/4]$ and $m=2n$, again with omitting the factor $2^{1 + n}$. So proving that $h_m(t)$ has only one maximum is equivalent to the original problem. Now we have the product of two functions one is monotonically increasing and convex, and the other is monotonically decreasing and convex. Is this enough to prove that the function has one and only one maximum for all $n$?",['real-analysis']
1806015,Ordinal Arithmetic Identity,Let $\alpha$ be an Ordinal and $S$ a set of Ordinals. Is it the case that $$\alpha\bigcup\limits_{x\in S} x=\bigcup\limits_{x\in S} \alpha x$$ and if so how could one prove this?,"['elementary-set-theory', 'ordinals']"
1806071,Real and imaginary part of an analytic function are harmonic?,"Let $f:\Bbb{C}\to\Bbb{C}$ be an analytic function. For $z = x+iy,$ let $u,v : \mathbb{R}^2  \rightarrow \mathbb{R}^2 $ be such that $u(x,y) = \operatorname{Re}f(z)$ and $v(x,y) = \operatorname{Im}f(z)$ .
Then which of the following are correct, 1. $\dfrac{\partial ^2u}{\partial x^2} + \dfrac{\partial ^2u}{\partial y^2} = 0$ 2. $\dfrac{\partial ^2v}{\partial x^2} + \dfrac{\partial ^2v}{\partial y^2} = 0$ 3. $\dfrac{\partial ^2u}{\partial x \partial y} - \dfrac{\partial ^2u}{\partial y \partial x} = 0$ My try: Since $\operatorname{Re} f(z)$ and $\operatorname{Im}f(z)$ parts of an analytic function satisfies Cauchy Riemann Equation. We can conclude all the options holds. But it's displayed option $2$ is incorrect. Is that so?","['complex-analysis', 'analytic-functions', 'complex-numbers']"
1806072,Using Green's Theorem to Express the Integral $I=\int_C (Pdx+Qdy)$ as an expression of $I_i=\int _{C_i} (Pdx+Qdy)$,"Let $p_1,...p_n$ be points in $\mathbb{R}^n$. Let $P(x,y), Q(x,y)$ be functions with continuous derivatives in $ D=\mathbb{R}^2\setminus\{p_1,...p_n\}$ such that $Q_x-P_y=1$ for all $(x,y)\in D$. For all $i$ let $C_i$ be a circle of radius $r_i$ around $p_i$ such that $C_i$ doesn't contain $p_j$ for all $j\neq i$. Let $C$ be a circle of radius $R$ around $(0,0)$, such that $\forall i=1,...n, p_i \in C$. Let $I=\int_C (Pdx+Qdy),  I_i =\int_C(Pdx+Qdy)$. Express $I$ as a function of $I_1,...I_n, r_1,...r_n, R$. I was able to prove this considering that $C_1,...C_n$ are all disjoint and $\cup_{i=1}^nC_i \subseteq C$, but wasn't able to prove this when they were not disjoint. Here's the proof for disjoint $C_1,...C_n$ and $\cup_{i=1}^nC_i \subseteq C$: Let $A$ be $C\setminus \cup_{i=1}^nC_i$. $A$ is a Green region, since it's boundary is $C\cup \cup_{i=1}^nC_i$, and therefore: $I+\sum_{i=1}^nI_i=\int_C(Pdx+Qdy)+\sum_{i=1}^n\int_{C_i}(Pdx+Qdy)=\int_{\partial A}(Pdx+Qdy)=\iint_A(Q_x-P_y)dxdy=\iint_A1dxdy=\mu (A)$ And therefore, $I=\mu(A)-\sum_{i=1}^nI_i$, but since $C_1,..C_n$ are all disjoint and if we write $B$ to be the interior of $C$ and $B_i$ to be the interior of $C_i$, then, we get that $\mu(A)-\mu(B)-\sum_{i=1}^n\mu(B_i)=\pi(R-\sum_{i=1}^nr_i^2)$, such that $I=\pi(R-\sum_{i=1}^nr_i^2)-\sum_{i=1}^nI_i$. In the case that $C,C_1,...C_n$ are not disjoint I looked at the case of only two that intersect. Let's assume that $B_1\cap B_2=E\neq \emptyset$. Since $B_1$ only contains $p_1$ of the set $\{p_1,...p_n\}$ and $B_2$ only contains $p_2$ of $\{p_1,...p_n\}$, then $E$ doesn't contain any of the points $\{p_1,...p_n\}$ and therefore, $P,Q$ and their derivatives are continuous on $E$. Also, if $E$ contains more than one point, then it can be proven that $E$'s boundary is a Jordan curve, and therefore, Green's theorem applies on $E$. 
Therefore, I figured out that $\int_{C_1\cup C_2}(Pdx+Qdy)=\int_{C_1}(Pdx+Qdy)+\int_{C_2}(Pdx+Qdy)-\iint_E(Q_x-P_y)dxdy=I_1+I_2-\mu(E)$. From here on, I couldn't find a direction that worked.","['line-integrals', 'greens-theorem', 'calculus', 'multivariable-calculus', 'integration']"
1806096,Compute the Galois group of $p(x)=x^4+ax^3+bx^2+cx+d$,"Compute the Galois group of the following polynomial:
  $$p(x)=x^4+ax^3+bx^2+cx+d$$ Step 1: Calculate cubic resolvent Step 2: Calculate the discrimimant $\gamma$ of the cubic resolvent Step 3: If $\gamma$ is a square then the Galois group is $A_4$, else $S_4$ Is this correct for all $a, b, c, d \in \mathbb{Z}$? How do we know if the Galois group is, for example, $\mathbb{Z_4}$?","['finite-groups', 'abstract-algebra', 'galois-theory', 'group-theory']"
1806122,A question about alternate series involving unit fractions,"I don't know exactly how to classify this question. It is not from any homeworks, just something I've been wondering about. Let $A\subseteq\mathbb N$ be a subset that contains at least $n$ elements; then $A(n)$ is the $n$-th element of $A$, and 
let $R$ be a function $R:P(\mathbb N) \rightarrow [0,1]$.
If $A$ is not empty and the cardinality of $A$ is $|A|$, then $$R(A)=\sum_{n=1}^{|A|}\frac{(-1)^{n+1}}{A(n)} $$ and $R(\emptyset )=0$.
 Pierce expansion shows that $R$ is onto, and I know that $R$ isn't one-to-one.
My question is whether for every real number $r$ between $0$ and $1$ the inverse image $R^{-1}[\{r\}]$ is finite. Or is it only finite for $0$, $\frac{1}{2}$ and $1$?","['sequences-and-series', 'functions']"
1806132,Prove $\int_{0}^{\infty}\frac{2x}{x^8+2x^4+1}dx=\frac{\pi}{4}$,"$$\int_{0}^{\infty}\frac{2x}{x^8+2x^4+1}dx=\frac{\pi}{4}$$ $u=x^4$ $\rightarrow$ $du=4x^3dx$ $x \rightarrow \infty$, $u\rightarrow \infty$ $x\rightarrow 0$, $u\rightarrow 0$ $$=\int_{0}^{\infty}\frac{2x}{u^2+2u+1}\cdot\frac{du}{4x^3}$$ $$=\frac{1}{2}\int_{0}^{\infty}\frac{1}{u^2+2u+1}\cdot\frac{du}{x^2}$$ $$=\frac{1}{2}\int_{0}^{\infty}\frac{1}{u^2+2u+1}\cdot\frac{du}
{\sqrt{u}}$$ Convert to partial fractions $$\int_{0}^{\infty}\frac{1}{\sqrt{u}}-\frac{2}{u+1}+\frac{1}{(u+1)^2}du$$ $$=\left.2\sqrt{u}\right|_{0}^{\infty}-\left.2\ln(1+x)\right|_{0}^{\infty} -\left.\frac{1}{1+u}\right|_{0}^{\infty}$$ Where did I went wrong during my calculation?",['integration']
1806164,Proof of $\sum_{n=1}^\infty \frac{1}{n(n!)} = \int_0^1\frac{e^x-1}{x}dx$.,"I want to prove that $$\sum_{n=1}^\infty \frac{1}{n\cdot n!} = \int_0^1\frac{e^x-1}{x} \,dx.$$ I tried this: $$g(x)=
\begin{cases}
\frac{e^x-1}{x} , x\neq0\\
1,x=0\\
\end{cases}$$ $\lim_{n\to\infty}\frac{e^x-1}{x}=1$, so $g(x)$ is continuous in $[0,1]$. Therfore $$(1)\int_0^1g(x)dx=\int_0^1\frac{e^x-1}{x}dx$$
Next, 
$$e^x-1=\sum_{n=1}^\infty\frac{x^n}{n!}$$
And this series converges uniformly in $[0,1]$. If $0\lt x\le1$ divide by $x$:
$$\Rightarrow(2)\frac{e^x-1}{x}=\sum_{n=1}^\infty\frac{x^{n-1}}{n!}$$
In addition,
$$(3)\sum_{n=1}^\infty\frac{0^{n-1}}{n!}=0^0=1$$
From $(2),(3)$ we can conclude that for every $x\in[0,1]$:
$$(4)g(x)=\sum_{n=1}^\infty\frac{x^{n-1}}{n!}$$
This series converges uniformly in $[0,1]$, so from $(1),(4)$:
$$\int_0^1\frac{e^x-1}{x} \, dx = \int_0^1 g(x) \, dx =\int_0^1\sum_{n=1}^\infty\frac{x^{n-1}}{n!} =\sum_{n=1}^\infty \int_0^1 \frac{x^{n-1}}{n!} =\sum_{n=1}^\infty\frac{1}{n\cdot n!}$$
I'm not so sure that all the steps i took in my proof are 100% correct,","['real-analysis', 'integration', 'sequences-and-series', 'proof-verification']"
1806166,Does the ring of integers for the field $\mathbb{Q}(\sqrt{-1+2\sqrt{2}})$ have a power basis?,"Specifically I am interested in the the ring of integers for the field $\mathbb{Q}(\sqrt{-1+2\sqrt{2}})$. Does this ring of integers have a power basis? More generally, for any Salem number $s$, does the ring of integers for $\mathbb{Q}(s)$ have a power basis? Does anyone know of a sage command or some clever way to determine if any given ring of integers for an algebraic number field has a power basis? Thanks!","['number-theory', 'algebraic-number-theory', 'field-theory']"
1806177,For which values of $a\in\mathbb{C}$ does $\sum\limits_{n=1}^\infty\frac{a^n}{n}$ converge?,"For which complex values of $a$ does $\sum\limits_{n=1}^\infty\frac{a^n}{n}$ converge? Clearly when $|a|>1$ it does not and when $|a|<1$ it does, so we only have to see what happens when $|a|=1$. I am rather stumped with this problem. I have shown if $a$ has an even order then the sum converges. It is also clear if $a=1$ then it diverges. I think this is the only value for which it diverges, but I am not sure, any help is appreciated.","['complex-analysis', 'sequences-and-series', 'power-series']"
1806184,On the convergence of a more complex iterated radical,"After exploring Ramanujan's famed $$3=\sqrt{1+2\sqrt{1+3\sqrt{1+\cdots}}} $$ and $$4=\sqrt{6+2\sqrt{7+3\sqrt{8+\cdots}}},$$ both of which can be expressed more generally by $$x+n+a=\sqrt{ax+(n+a)^2+x\sqrt{a(x+n)+(n+a)^2+(x+n)\sqrt{a(x+2n)+\cdots}}},$$ I came across a nested radical of the form \begin{equation}P=\sqrt{a+\sqrt{ab+\sqrt{ab^2+\sqrt{ab^3+\cdots}}}} \end{equation} and am struggling to derive a formula for determining to what value $P$ converges. I know from a simple reformulation of $$\sqrt{a+b\sqrt{a+b\sqrt{a+\cdots}}}$$ that $$\sqrt{a+\sqrt{ab^2+\sqrt{ab^6+\sqrt{ab^{14}+\cdots}}}} $$ converges to $$\frac{b+\sqrt{b^2+4a}}{2}. $$ This result, however, fails to be helpful. Any advice on determining the convergence of $P$ would be appreciated. UPDATE: Let \begin{equation} P=\sqrt{a+\sqrt{ab+\sqrt{ab^2+\sqrt{ab^3+\cdots}}}}.\end{equation} Notice $$P(x)=\sqrt{ab^x+P(x+1)} $$ denotes the recurrence relation for $P$. My new attempt was to consider Ramanujan's recurrence relation: $$\psi(x)=\alpha+\beta+x=\sqrt{\alpha x +(\alpha+\beta)^2+x\psi(x+1)}, $$ let $\beta=1$, then set \begin{equation}ab^x=\alpha x +(\alpha+1)^2.\end{equation}
Because $\alpha\geq0$, the above produces $$\alpha=\frac{-(x+2)+\sqrt{(x+2)^2-4(1-ab^x)}}{2}. $$ Through Ramanujan's formula, setting $x=1$ then implies that $$\psi(1)=\frac{1+\sqrt{5+4ab}}{2}=\sqrt{ab+\psi(2)}.$$ I realize, as suggest in Infinitely nested radical problem? , which is nearly identical to my new approach, that $P$ and $\psi$ are not in complete equitable forms. That post also suggests that this may not be the correct approach to solving this problem. Could someone in different words than the cited post please explain why this method is not sufficient and perhaps propose a new method or resource for computing radicals of this form?","['recurrence-relations', 'real-analysis', 'nested-radicals']"
1806250,Definition of $\pi_0$,"$\pi_0(X)$, for a topological space $X$, is the space of homotopy classes of maps $S^0\to X$. I suppose here $S^0$ may be taken as the set $\{\pm1\}$ with the discrete topology. I am wondering, are there instances in which $\pi_0(X)$, is not a set which is bijective to the set of path-connected components of $X$? In particular, what happens when $X$ is not locally path-connected? This question arose because in some book (Rørdam's) they make a distinction between $X/\tilde{}~h$, the set of path-connected components of $X$, and $\pi_0(X)$. But I was not aware there was a difference. Here is the relevant excerpt from the book:","['algebraic-topology', 'general-topology']"
1806269,Logic Puzzle (Valid and Invalid Arguments),"I have been given a logic puzzle and I am having a tough time figuring out how to set it up and solve. Here is the puzzle: a) The Statement ""If Dr. Jones did not commit the murder then neither Ms. Scarlet nor Mr. Green committed the murder"" is false. I interpret that as: $$ \lnot D \to (\lnot S \lor \lnot G)$$ 
Now isn't the ONLY way for conditional statements to be false is if the hypothesis is true and the condition is false? Thus leaving $ \lnot D $ b) Either Mr.Green did not commit the murder or the weapon was a candlestick. I read that as: $$ \lnot G\lor C $$ c) If the weapon was a candlestick then Dr. Jones commited the murder. That reads $$ C \to D $$
Question is who committed the murder?
The thing that I am having most trouble with is figuring out what laws to use in order to solve this. A point in the right direction would be greatly appreciated.","['boolean-algebra', 'logic', 'discrete-mathematics']"
1806291,Proving $\int_{0}^{\infty}\frac{4x^2}{(x^4+2x^2+2)^2}dx\stackrel?=\frac{\pi}{4}\sqrt{5\sqrt2-7}$,"$$\int_{0}^{\infty}\frac{4x^2}{(x^4+2x^2+2)^2}dx=\frac{\pi}{15}$$ $$\int_{0}^{\infty}\frac{4x^2}{[(1+(1+x^2)^2]^2}dx=\frac{\pi}{15}$$ $u=\tan(z)$ $\rightarrow$ $du=\sec^2(z)$ $u$ $\rightarrow \infty$, $\tan(z)=\frac{\pi}{2}$ $u$ $\rightarrow 0$, $\tan(z)=0$ $$\int_{0}^{\pi \over 2}\frac{4\tan^2(z)}{[(1+(1+\tan^2(z))^2]^2}\frac{du}{\sec^2(z)}=\frac{\pi}{15}$$ $1+\tan^2(z)=\sec^2(z)$ $$\int_{0}^{\pi \over 2}\frac{4\sin^2(z)}{[(1+\sec^4(z)]^2}dx=\frac{\pi}{15}$$ $$\int_{0}^{\pi \over 2}\frac{4\sin^2(z)}{[(1+i\sec^2(z))(1-i\sec^2(z))]^2}dx=\frac{\pi}{15}$$ Hopeless! Any suggestion? Try again 
$$\int_{0}^{\pi \over 2}\frac{4\sin^2(z)}{[(1+\sec^4(z)]^2}dx=\frac{\pi}{15}$$ $$\int_{0}^{\infty}\frac{\sin^2(2z)\cos^6(z)}{(1+\cos^4(z))^2}dx=\frac{\pi}{15}$$ $\sin^2(2z)=\frac{1-\cos(4z)}{2}$ No more I gave up. Any hints?","['integration', 'definite-integrals', 'closed-form']"
1806292,Showing that $f$ is not measurable,"I'm learning about measure theory, specifically measurable functions, and need help with the following problem: Let $N$ be a non-measurable subset of $[0, 1]$ and define $$f(x) = \begin{cases} x, & x \in N  \\ -x, & x \in [0,1] \setminus N \end{cases}$$ $(1)$ Show that $f(x)$ is a non-measurable function. $(2)$ Show that $\forall a \in \mathbb R$ the set $\{f(x) = a\}$ is measurable. This problem looks rather simple but I think I'm missing some key points. I tried to prove $(1)$ by contradiction, that is assuming $f$ is measurable but I didn't get far. For $(2)$ my guess is that $m(\{f(x) = a\}) = 0$ (since by definition the Lebesgue measure of a single point is $0$ ) but I don't know how to show this properly.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1806317,Size of the vocabulary in Laplace smoothing for a trigram language model,"Let's say we have a text document with $N$ unique words making up a vocabulary $V$, $|V| = N$. For a bigram language model with add-one smoothing, we define a conditional probability of any word $w_{i}$ given  the preceeding word $w_{i-1}$ as: $$P(w_{i}|w_{i-1}) = \frac{count(w_{i-1}w_{i}) + 1}{count(w_{i-1}) + |V|}$$ As far as I understand (or not) the conditional probability, and basing on a 3rd point of this Wikipedia article , $w_{i-1}$ might be assumed to be ""constant"" here, so by summing this expression for all possible $w_{i}$ we should obtain 1, and so it is, which is obvious. However, I do not understand the answers given for this question saying that for n-gram model the size of the vocabulary should be the count of the unique (n-1)-grams occuring in a document, for example, given a 3-gram model (let $V_{2}$ be the dictionary of bigrams): $$P(w_{i}|w_{i-2}w_{i-1}) = \frac{count(w_{i-2}w_{i-1}w_{i}) + 1}{count(w_{i-2}w_{i-1}) + |V_{2}|}$$ It just doesn't add up to 1 when we try to sum it for every possible $w_{i}$. Therefore - should the $|V|$ really be equal to the count of unique (n-1)-grams given an n-gram language model or should it be the count of unique unigrams?","['machine-learning', 'probability']"
1806343,For which values of $x$ does $\sum_{n=1}^{\infty}\frac{ x^n}{x^{2n} -1}$ converge?,"Of course it does not converge for $x = \pm 1$. I tried to use the limit form of the comparison test, using a divisor ""b"" series of $x^n$, and $\frac{1}{x^{2n} -1}$ goes to $0$ as $n$ goes to $\infty$ for $|x| > 1$, BUT my test series converges only for $|x| < 1$ so I am likely using this incorrectly. for $|x| < 1$ I don't know how to proceed.","['sequences-and-series', 'calculus']"
1806376,How to Prove the Stochastic Fubini Theorem? (Exercise 2.19 in Chapter IV of Revuz and Yor),"Here is the theorem statement: Let $B$ and $C$ be two independent standard Brownian motions. If $\phi$ is square integrable on the unit square ($\phi \in L^2([0,1]^2)$ ), by suitable filtrations, give a meaning to the integrals $$\int_0^1 dB_u \int_0^1 dC_s \phi(u,s) \quad \text{and} \quad \int_0^1 dC_s \int_0^1 dB_u \phi(u,s)$$ and prove that they are almost surely equal. (This is the first part of Exercise 2.19 in Chapter IV of Revuz and Yor) What I tried: There are always so many measure-theoretic assumptions and results implicit in the theory of stochastic processes that are always brushed under the rug that I basically have no idea what is going on. Nevertheless, here is my pitiful attempt at a solution: In order to give meaning to the integrals, we need to first choose a suitable filtration. We can simply use the natural filtration generated by $B$ and $C$ on $\mathbb{R}^2$ though -- this is because under such a filtration, both processes are adapted, and since they are also continuous, they also generated the predictable sigma algebra, which implies they are also progressive(ly measurable). The inner integrals, being continuous and adapted, will again be suitably measurable. The almost sure equality follows from the progressive measurability of the processes on $\mathbb{R}^2$ allowing us to switch the order of integration for the simple predictable integrals/stochastic Riemann sums (by the regular Fubini theorem) and then follows by the uniqueness of limits in $L^2$ (using the dominated convergence theorem to get the double stochastic integrals as limits).","['stochastic-processes', 'probability-theory', 'stochastic-integrals', 'stochastic-analysis', 'stochastic-calculus']"
1806404,when to use restrictions (domain and range) on trig functions,"I'm doing math practice problems from ""Precalculus for Dummies 1,000 Practice Problems Book"" and I'm confused about when to apply restrictions to trig function questions. This book has all the solutions step by step in the back so I know how the problem is solved. What confuses me is why the restrictions are used in some problems and not others. The restrictions that I'm talking about are: The restrictions for the regular trig functions should be the the same but the range and domain's values are switched if I'm not mistaken. These were the main problems that made me confused: Find an exact value of $y$, $y=\arcsin\left(\frac{\sqrt{3}}{2}\right)$ if I solve for $y$ it equals $2\pi/3$ and $4\pi/3$, but since $\cos(x)$ has the restrictions where domain is $[-1,1]$ and the range is $[0,\pi]$ the final answer is just $2\pi/3$ Find an exact value of $y$, $y= \cos(\arctan(-1))$ in this case, while solving for y, I get to a step that looks like this: 
$y=\cos(7\pi/4)$. The answers/solutions in the back then show $y= \sqrt{2}/2$ is the answer. However, I thought $\cos(x)$'s domain should have been restricted to $[-1,1]$. Why is the $7\pi/4$ allowed? At this point I was thinking, then do the restrictions only apply to inverse trig functions? But that didn't make sense either because I should be able to rewrite $y=\cos(7\pi/4)$ as $\arccos(y)=7\pi/4$; the ""$7\pi/4$"" part is not part of $\arccos$'s range restriction, $[0,\pi]$ Find all solutions of the equation in the interval $[0^\circ,360^\circ)$. $2cos^2x-2=3\cos x$ I get to a step where $\cos x= -1/2$ and $2$. I solve the $\cos x= -1/2$ part which equals $120^\circ$ and $240^\circ$; however, the book says I can't solve for $\cos x = 2$ because ""There are no solutions for the second factor, $\cos x = 2$, because $\cos(x)$ is $[-1,1]$"" This confuses me because question 2 allows for $\cos x$'s domain to be outside of the restriction while question 3 doesn't allow for $\cos x$'s range to be outside of the restriction.","['algebra-precalculus', 'trigonometry']"
1806414,Prove $\frac{xy}{5y^3+4}+\frac{yz}{5z^3+4}+\frac{zx}{5x^3+4} \leqslant \frac13$ for positives $x + y + z = 3$,"$x,y,z >0$ and $x+y+z=3$ , prove $$\tag{1}\frac{xy}{5y^3+4}+\frac{yz}{5z^3+4}+\frac{zx}{5x^3+4} \leqslant \frac13$$ My first attempt is to use Jensen's inequality. Hence I consider the function $$f(x) =\frac{x}{5x^3+4}$$ Compute second derivative we have $$\tag{2}f''(x)=\frac{30x^2(5x^3-8)}{(5x^3+4)^3}$$ $(2)$ shows that the function is neither concave or convex. So I don't think Jensen useful here. The author of $(1)$ is the same as of this inequality .","['algebra-precalculus', 'inequality']"
1806423,Eigenvalues of Matrix vs Eigenvalues of Operator,"I'm having some trouble reconciling the concept of eigenvalues of operators with eigenvalues of matrices: Say you have an $n\times n$ matrix $A$. It represents a linear operator $T:V\to V$ with respect to some basis $\{e_i\}$ in the background. Now my understanding is that 1.) Whatever the basis is, it has no effect on the eigenvalues of $A$. I.e. solutions to $\text{det}(A-\lambda I)=0$ gives the same solutions regardless if we have $\{e_i\}$ or $\{e_i'\}$ as the basis in the background, so long as we keep the entries of $A$ the same in both cases. 2.) The eigenvalues of $A$ are the same as the eigenvalues of $T$ as long as we use the basis $\{e_i\}$ in which $A$ represents $T$. However, if we keep the entries of $A$ the same, and change the basis in the background, then $A$ represents a different linear operator $T'$. This seems contradictory since 
\begin{align}
\{\text{eigenvalues of} \ T\}&=\{\text{eigenvalues of} \ A \ \text{with respect to basis} \{e_i\}\} \ \ \text{by 2}\\
&=\{\text{eigenvalues of} \ A \ \text{with respect to basis} \{e_i'\}\} \ \ \text{by 1} \\
&=\{\text{eigenvalues of} \ T'\} \ \ \text{by 2}
\end{align} But there's no reason the two operators $T$ and $T'$ should have the same eigenvalues. Can someone point out what's wrong here? Any help would be appreciated.",['linear-algebra']
1806432,Choosing Functions for the Squeeze Theorem,"Evaluate $$\lim_{n\to \infty}\dfrac{1}{\sqrt{n^2}}+\dfrac{1}{\sqrt{n^2+1}}+...+\dfrac{1}{\sqrt{n^2+2n}}$$ $$$$
I came across the the question on this site itself but had a few doubts on the given solution. As I do not yet have 50 reputation points, I cannot comment over there. Could somebody please help me?
$$$$
From what I understand of the Squeeze Theorem, the three functions are related as 
$$g(x)\le f(x)\le h(x)$$
$$$$
Now in the selection of terms, the following inequality has been used:
$$\displaystyle \frac{1}{n+1} \leq \frac{1}{\sqrt{n^2+k}} \leq \frac{1}{n}  $$when $0 \leq k \leq 2n$
$$$$
This inequality lead to the one used as the three functions for the application of the Squeeze theorem: $$\frac{2n+1}{n+1} \leq S(n) \leq \frac{2n+1}{n}$$
where $S(n)=\dfrac{1}{\sqrt{n^2}}+\dfrac{1}{\sqrt{n^2+1}}+...+\dfrac{1}{\sqrt{n^2+2n}}$
$$$$
I don't understand how $$\displaystyle \frac{1}{n+1} \leq \frac{1}{\sqrt{n^2+k}} $$
$$$$
Isn't $$n^2+k\le n^2+2n<(n+1)^2 \Rightarrow n^2+k<(n+1)^2$$
$$\Rightarrow \sqrt{n^2+k}< (n+1)$$$$\Rightarrow \displaystyle \frac{1}{n+1} < \frac{1}{\sqrt{n^2+k}}$$ $$$$
Thus shouldn't the resulting set of inequalities be $$\displaystyle \frac{1}{n+1} < \frac{1}{\sqrt{n^2+k}} \leq \frac{1}{n}  $$ $$\Longrightarrow \frac{2n+1}{n+1} < S(n) \leq \frac{2n+1}{n}$$ $$$$ In this case there is a $<$ sign instead of the $\le$ sign. How then can the Squeeze Theorem be applied?
Many thanks in advance.
$$$$
EDIT: Also, since Limits preserve Inequalities, how can $$\lim_{n\to \infty} \frac{2n+1}{n+1} = \lim_{n\to \infty}\frac{2n+1}{n}$$ when $$\frac{2n+1}{n+1} <  \frac{2n+1}{n}$$","['sequences-and-series', 'calculus', 'limits']"
1806447,Sketching the global phase portrait for a version of the Lotka-Volterra system,"I'm trying to sketch the phase portrait for a version of Lotka-Volterra given by $$\begin{cases} \dot{x} = x(3-x-2y)\\ \dot{y} = y(2-x-y) \end{cases}.$$ I can sketch this just fine except for the brown curve (shown below). The fixed points are: $(0,0), (0,2), (3,0), (1,1)$. The linearised system is given by $Df(x,y) = \begin{pmatrix}
3-2x-2x & -2x \\ 
-y & 2-x-2y
\end{pmatrix}$. So we have: $Df(0,0) = \begin{pmatrix}3 & 0 \\ 0 & 2 \end{pmatrix}$, $\lambda_1 = 3, \lambda_2 = 2, e_1 = \begin{pmatrix}1 \\ 0 \end{pmatrix}, e_2 = \begin{pmatrix}0 \\ 1 \end{pmatrix}$ $Df(0,2) = \begin{pmatrix}-1 & 0 \\ 2 & -2 \end{pmatrix}$, $\lambda_1 = -1, \lambda_2 = -2, e_1 = \begin{pmatrix}1 \\ -2 \end{pmatrix}, e_2 = \begin{pmatrix}0 \\ 1 \end{pmatrix}$ $Df(3,0) = \begin{pmatrix}3 & -6 \\ 0 & -1 \end{pmatrix}$, $\lambda_1 = -3, \lambda_2 = -1, e_1 = \begin{pmatrix}1 \\ 0 \end{pmatrix}, e_2 = \begin{pmatrix}3 \\ -1 \end{pmatrix}$ $Df(1,1) = \begin{pmatrix}-1 & -2 \\ -1 & -1 \end{pmatrix}$, $\lambda_1 = -1 + \sqrt{2}, \lambda_2 = -1-\sqrt{2}, e_1 = \begin{pmatrix}1 \\ -\frac{1}{\sqrt{2}} \end{pmatrix}, e_2 = \begin{pmatrix}1 \\ \frac{1}{\sqrt{2}} \end{pmatrix}$ So respectively we have an unstable node, a stable node, a stable node and a saddle point. I'm having difficulty sketching the stable manifold for the saddle point. I know that the manifold must contain $(1,1)$ and is tangent to $(1,1) + E^s$, where $E^s$ is the space spanned by the eigenvector $\begin{pmatrix}1 \\ \frac{1}{\sqrt{2}} \end{pmatrix}$, i.e. the manifold must be tangent to the line $y = \frac{1}{\sqrt{2}}(x-1)+1$. Here is a sketch of the phase portrait by the lecturer: Since the manifold needs to be tangent to $y = \frac{1}{\sqrt{2}}(x-1)+1$, why is the brown curve not concave downward then concave upward (rather than the way it's drawn, i.e. concave upward then concave downward)?","['manifolds', 'ordinary-differential-equations', 'dynamical-systems']"
1806454,What are vector norms used for?,"I'm currently working with a computer science problem that requires me to build vectors that can return their own norms. Based on Wolfram Alpha's description, I think I have an idea of how this is accomplished for the simple $L^2$ norm ($\sqrt{a^2+b^2+c^2}$, and so on) required by the exercise, but I've no notion of what this is actually useful for or why I would want to find it, outside of it being required by the exercise. Any insight is appreciated!","['real-analysis', 'normed-spaces', 'general-topology', 'linear-algebra', 'vectors']"
1806518,Find the maximal value of $a+b-c+d$,"Let $a, b, c, d$ be real numbers satisfying inequality $$f(x)=a\cos x+b\cos 2x+c\cos 3x+d\cos 4x\le 1$$ holds for $x\in\Bbb{R}$. Find the maximal value of $a+b-c+d$ and determine the values of $a,b,c,d$ when that maximum is attained. Try $f(0)=a+b+c+d\le 1$, $f(\pi)=-a+b-c+d\le 1$ and
$$f\left(\dfrac{\pi}{2}\right)=-b+d\le 1$$","['inequality', 'trigonometry']"
1806589,Unfair coins connected in a game,"I would like to ask the following question. There are 3 coins ( $A,B$ and $C$ ) that are biased with probability of tails equal to $t_a, t_b$ and $t_c$ respectively.   The coins are tossed: $m_a$ times for coin- $A$ , $m_b$ times for coin- $B$ , and $m_c$ times for coin- $C$ , though with no specific order (the order is uniformly random).   There are $k$ slots that are filled with the coins when tails is a result.   When all $k$ slots are filled the game stops. What is the function / expression that I should use to find the probability that $s_a$ slots from coin A, $s_b$ slots from coin B and $s_c$ slots from coin C are filled with $s_a + s_b + s_c \leqslant k$ ?","['probability-theory', 'probability']"
1806623,Spaces of functions similar to $L^p$ but with different local and global sizes,"Obviously much of analysis on $\mathbb R^n$ considers $L^p$ spaces or other Banach spaces derived from them. The definition of $L^p(\mathbb R^n)$ looks very natural, but I've been bothered for some time that what it measures might not so natural. My issue specifically is that it measures the ""local"" size of $f$ (say $f\cdot\mathbf1_{|f|>1}$) with the same $p$ that it measures the ""global"" size (say $f\cdot\mathbf1_{|f|\leq1}$). I intuitively feel like the local and global sizes have nothing to do with each other and therefore it is arbitrary to say they should be measured with the same exponent. It seems to me then that analysis would benefit from some Banach space $X^{p,q}$ that lets you measure the function locally in $p$ and globally in $q$. To illustrate my concern, consider $|x|^{-a}$ ($a>0$), which are not in any $L^p$ spaces! But we have $|x|^{-a}\in X^{p,q}$ as long as $n/q<a<n/p$. It's then clear how $L^p=X^{p,p}$ is just a special case, and there's no $a$ to satisfy $n/p<a<n/p$. An explicit norm I thought of that might do the job is
$$\|\varphi\|_{X^{p,q}}=\inf_{K\subset\subset\mathbb R^n}\left(\|\varphi\cdot\mathbf1_K\|_{L^p}+\|\varphi\cdot\mathbf1_{K^c}\|_{L^q}\right).$$ The closest I've seen to this is what Lemarie-Rieusset (Recent Developments in the Navier-Stokes Problem, 2002) calls $WL^\infty$ with norm
$$\|\varphi\|_{WL^\infty}=\sum_{k\in\mathbb Z^n}\sup_{x-k\in[0,1]^n}|\varphi(x)|,$$
which resembles what my above notation would call $X^{\infty,1}$. So please tell me, am I wrong that this is something natural to consider? It seems that analysts have gotten pretty far without it. Perhaps there's something very special about measuring the local and global parts of a function in the same way. Or maybe such a space is known and I haven't come across it.","['functional-analysis', 'real-analysis', 'lp-spaces']"
1806669,"The Boolean Pythagorean triples problem, a $200$-terabyte proof, and $d=163$","I came across this interesting math article, "" Computer cracks 200-terabyte maths proof "" where one phrase caught my attention and I quote, ""... all triples could be multi-coloured in integers up to $7824$"" . Alternatively, from page 2 of this paper , Theorem 1 . The set {$1,\dots,7824$}  can be partitioned into two parts, such that no part contains a Pythagorean triple. This is impossible for {$1,\dots,7825$}. The number $N=7824$ was awfully familiar . A quick factorization showed that it was in fact, $$N = 7824 = 2^4 \times 3\times \color{blue}{163}$$ Questions : Does anybody know why the largest Heegner number $163$ figures in the largest $N$ that can be multi-colored in the Boolean Pythagorean triples problem? A272709 is the sequence $2, 4, 8, 16, 24, 48, 96, 192,....0,0,0,0,0\dots$ where the zeros start at $a(7825)$. What is the exact value of $a(7824)$? (In the comments, it just says $a(7824)\geq8$.)","['number-theory', 'diophantine-equations', 'pythagorean-triples']"
1806670,Martini Glass - Extension,"This is my extension to the very interesting question on the martini glass from 538.com by the Riddler as posted here earlier by MP Droid. Recap of original configuration. A martini glass has the shape of an inverted right circular cone with sides of unit length.  The glass is positioned upright and filled with martini up to a level $p(<1)$ on each side.  See diagram below. Extension: The extension is as follows: (1) Refer to the first diagram above. As in the original question, when tilted such that the glass is just short of overflowing, what is the angle of tilt $\alpha$ from the vertical? Now assume that the martini glass is covered with a lid such that the martini does not overflow when tilted more than $\alpha$. (2) Refer to the second diagram above. If the martini glass is further tilted such that the right side (now the bottom) of the glass is parallel to the water level, i.e. horizontal, what is the length $u$ of the left side of the glass? what is the angle of tilt $\beta$ from the vertical? what is the shape of the top surface of the martini? (3) Refer to the third diagram above. If the martini glass is further tilted such that the top surface of the martini touches the apex of the inverted cone of the glass, what is the length $v$ of the right side? what is the angle of tilt $\gamma$? what is the shape of the top surface of the martini?","['conic-sections', 'trigonometry', 'geometry']"
1806684,How can I prove that $(a + b )\oplus(a + c)$ is not possible to simplify. Or is it?,"I was trying to simplify the following expression $(a + b )\oplus(a + c)$, where $+$ is just a simple addition of two numbers and $\oplus$ is a binary xor operation . By simplifying I mean exanding or finding any alternative form. After a couple of attempts I was not able to find any simplification and started to think that it is impossible. But I do not have any justification for this except of handwaving like this 'xor operates on individual bits, and is incompatible with normal summation'. This looks closer to a joke then to a normal proof. So how can one prove that you can't simplify this expression? Or can it be simplified?","['boolean-algebra', 'summation', 'discrete-mathematics']"
1806702,Inverse tangents in a cyclic order,"If
  $$\theta= \tan^{-1}\left(\frac{a(a+b+c)}{bc}\right)+\tan^{-1}\left(\frac{b(a+b+c)}{ac}\right)+\tan^{-1}\left(\frac{c(a+b+c)}{ab}\right)$$then find $\tan\theta$ I tried to use these as sides of a triangle and use their properties, but other than that I am clueless. I cannot think of a substitution either. The answer happens to be zero. Any help is appreciated. Thanks in advance!!",['trigonometry']
1806716,Limit theorem for changed time,"This post seems long, but its almost everything proofed in this post. Only one step seems to be left, for the desired proof. I would be very gratefull for any help. The setup Given a Levy-Process $U_{t}$ with  with $E(U_t)=0$ (then $U_t$ is a martingale). Let $U_t$ have finite variance $Var(U_t)=tVar(U_1)$ and $Var(U_1)=\sigma^{2}$ and the limit theorem  holds:
\begin{align}
F_t:=\sqrt{t}\left(\frac{U_t}{t}-E(U_1) \right)=\frac{U_t}{\sqrt{t}}\xrightarrow{d}\mathcal{N}(0,\sigma^{2})\quad as \,\,t\rightarrow \infty.\tag1
\end{align}
Let $K_t$ a non-decreasing positive ($K_{t}>0$ a.s.) process with cadlag-paths with the property that $K_{t}\rightarrow \infty$ almost sureley, as $t\rightarrow \infty$. I want to show that 
\begin{align}
F_{K_t}:=\frac{U_{K_t}}{\sqrt{K_{t}}} \xrightarrow{d}\mathcal{N}(0,\sigma^{2})\quad as \,\,t\rightarrow \infty. \tag2
\end{align}
For this one requires a positive non-random cadlag-function $a(t)$ with $a(t)\rightarrow \infty$ as $t\rightarrow \infty$ such that
\begin{align}
\frac{K_{t}}{a(t)}\rightarrow \theta\quad P\, a.s. \tag3
\end{align}
holds. Where $\theta$ is a positive random-variable.
Then the convergence in distribution of $F_{t}\xrightarrow{d} \mathcal{N}(0,\sigma^{2})$ implies the convergence in distribution of $F_{K_t}\xrightarrow{d} \mathcal{N}(0,\sigma^{2})$. The original question from my old account is posted here . 
However with my reputation here, i am able to start a bounty for the question. The suggestion of the proof are the following: For simplicity it is said, that $\theta=1$ and $\sigma^{2}=1$ So that we have $K_{t}\in ((1-\epsilon)a(t),(1+\epsilon)a(t))$ for large $t$. For $0<\theta<\infty$ we could do the same procedure and get the same result. This is how we go on:
For small $m$ we have
$$
P(U_{K_t}<x\sqrt{K_t})\leq P\left(K_{t}\notin ((1-\epsilon) a(t),(1+\epsilon) a(t))\right)+P\left(U_{a_t}<x\sqrt{(1+\epsilon)a(t)}+m\cdot \sqrt{\epsilon a(t))}\right)+ P\left(\sup_{s\in ((1-\epsilon)a(t),(1+\epsilon)a(t))}|U_{s}-U_{a(t)}|>m\cdot \sqrt{\epsilon a(t))}\right)
$$
The first term converges to 0 due to (3). The second term converges to $\Phi(x+m)$ (Why?) by the central limit theorem (1) applied to $U_{a(t)}$. The third term is bounded by martingale inequalitys $L^{2}$ by a factor
$$\frac{1}{(m\cdot \sqrt{\epsilon a(t))})^{2}}$$ Otherwise we can state
$$
P(U_{K_t}<x\sqrt{K_t})\geq Z\xrightarrow{d} \Phi(x-m)
$$
So we have sandwiched it and the desired result (2) holds for arbitrary $0<\theta<\infty$. HOWEVER (hopefolly the last step) We have with $(3)$ convegence to a strict positive finite randomvariable $\theta$. What is left to proof, considering, that $\theta$ is regarded as a random variable? Some additional stuffto understand the inequalities $$
P(U_{k_t}<x \sqrt{K_t})\\
\leq P[U_{k_t}<x \sqrt{K_t},K_{t}\in((1-\epsilon) a(t),(1+\epsilon) a(t))]+P[U_{K_t}<x\sqrt{K_t},K_{t}\notin ((1-\epsilon) a(t),(1+\epsilon) a(t))] \\ \leq P[K_{t}\notin ((1-\epsilon) a(t),(1+\epsilon) a(t))]+ P[U_{k_t}<x \sqrt{K_t},K_{t}\in((1-\epsilon) a(t),(1+\epsilon) a(t))] \\
\leq P[K_{t}\notin ((1-\epsilon) a(t),(1+\epsilon) a(t))]  \\
+P(U_{K_{t}}<x\sqrt{(1+ \epsilon)a(t)},|U_{K_t}-U_{a(t)}|\leq m\sqrt{\epsilon a(t)},|U_{K_t}-U_{a(t)}|> m\sqrt{\epsilon a(t)}]
\\ \leq P[U_{a(t)}<x\sqrt{(1+\epsilon)a(t)}+m \sqrt{\epsilon a(t)}] + P\left(\sup_{s\in ((1-\epsilon)a(t),(1+\epsilon)a(t))}|U_{s}-U_{a(t)}|>m\cdot \sqrt{\epsilon a(t))}\right)+P\left(K_{t}\notin ((1-\epsilon) a(t),(1+\epsilon) a(t))\right)
$$","['stochastic-processes', 'probability-theory', 'probability-distributions', 'probability', 'martingales']"
1806739,Are matrices 2D by definition?,"On the one hand, I read on Wikipedia that [A] matrix (plural matrices) is a rectangular array of numbers,
  symbols, or expressions, arranged in rows and columns. However, googling ""3D matrix"" produces a huge number of hits, including many from this website. I also find some people complaining about alleged misuse of terminology, e.g. here Just to gratify a pet peeve, there is no such thing as a ""3D matrix"".
  Matrices are 2D by definition. What's the most appropriate terminology here, say for a high school or undergraduate mathematics teaching context?","['matrices', 'terminology']"
1806781,"Geometric median (or Fermat-Weber problem), including continuous case","For a finite set $X\subset \mathbb R^n$ the geometric median is defined as the point in $\mathbb R^n$ for which the sum of distances to all points of $X$ attains its minimum. Here is a wiki article: https://en.wikipedia.org/wiki/Geometric_median Question 1. Is there some standard contemporary mathematical book (or article or survey) that contains all the basic information on this topic? I was able to find the English translation of a classical article of Weizsfeld of 1937 http://link.springer.com/article/10.1007%2Fs10479-008-0352-z#/page-1 but I would like to learn of further sources. Question 2. Clearly one can consider geometric medians of infinite sets, for example domains in $\mathbb R^n$ where one should minimize the integral of distance. What terminology is used in this case? Is this minimizing point still called the geometric median? Are there some nice sources? Question 2a. I am interested in particular if there is some nice characterization of the geometric median (in the sense of Qustion 2) of a solid triangle in $\mathbb R^2$?","['optimization', 'median', 'discrete-geometry', 'reference-request', 'geometry']"
1806792,How to find the inverse of $f(x)=x+ \frac{x^{3}}{1+x^{2}}$?,"I know that given,  $f(x)=x+ \frac{x^{3}}{1+x^{2}}$ I should set $y=x+ \frac{x^{3}}{1+x^{2}}$ and solve in terms of $x$, then just swap the $x$'s and $y$'s. I know that, since the derivative is always positive, and since the function is composed of polynomials, it is continuous and one-to-one, so that an inverse exists. But I cant seem to figure out the algebra to solve in terms of $x$? I just end up going in circles. Any help would be greatly appreciated.","['algebra-precalculus', 'inverse-function', 'functions']"
1806793,Number theory: Solving $2^n-1\equiv0\pmod{n+1}$.,"If $n$ satisfies the congruence
$$2^n-1\equiv0\pmod{n+1},$$
then what is $n$? Or if you can't know what $n$ is, then what can be said about $n$? Thank you in advance.","['number-theory', 'modular-arithmetic']"
1806821,Are there infinite (consecutive) pairs of Achilles numbers?,"This was proven true for powerful numbers, but has this been proven for Achilles numbers? I've found a total of 2 sites that claim this to be true but do not provide any sort of reasoning, nor proof as reference, and one site cites the other. EDIT: I've been asked to explain Achilles numbers and this problem in general so here is a brief explanation: Q: What is an Achilles number? A: An Achilles number is a number that is powerful but cannot be expressed in the form of a perfect power. (e.g. $72$) Q: What is a powerful number? A: A positive integer $m$ where, if a prime number $p$ divides $m$, then $p^2$ also divides $m$. (e.g. $25$) Q: What is a perfect power? A: A positive integer $n$ that can be written as $m^k$, where $m>1$ and $k\geq2$ and both $m$ and $k$ are $\in\mathbb{N}$. (e.g. $8$)","['number-theory', 'proof-explanation']"
1806836,Why are Grothendieck's and Hartshorne's definitions of quasi-coherence equivalent?,"Hartshorne's Algebraic Geometry defines an $\mathcal O_X$-module $\mathscr F$ to be quasi-coherent if there is an open affine cover $(U_i=\operatorname{Spec} A_i)_{i\in\mathcal I}$ of $X$ such that each $\mathscr F\rvert_{U_i}$ is isomorphic to the sheaf of modules $\widetilde{M_i}$ associated to some $A_i$-module $M_i$.
It is later proved that this is equivalent to the statement that for any open affine $U=\operatorname{Spec} A\subseteq X$, there exists an $A$-module $M$ with $\mathscr F\rvert_U\cong\widetilde M$. On the other hand, Grothendieck's Éléments de Géométrie Algébrique defines $\mathscr F$ to be quasi-coherent if every point $x\in X$ admits an open neighborhood $U\subseteq X$ such that $\mathscr F\rvert_U$ is isomorphic to the cokernel of a morphism of free $\mathcal O_U$-modules. Now I wonder how those definitions are equal. I tried proving it by myself, but both directions seem quite nontrivial to me... Any hints?","['quasicoherent-sheaves', 'modules', 'algebraic-geometry', 'definition']"
1806842,Saturation of a measure Folland Problem 1.3.16,"Exercise 16 - Let $(X,M,\mu)$ be a measure space. A set $E\subset X$ is called locally measurable if $E\cap A\in M$ for all $A\in M$ such that $\mu(A) < \infty$. Let $\tilde{M}$ be the collection of all locally measurable sets. Clearly, $M\subset \tilde{M}$, if $M = \tilde{M}$, then $\mu$ is saturated. a.) If $\mu$ is $\sigma$-finite, then $\mu$ is saturated. Proof - Suppose $\mu$ is $\sigma$-finite. Let $A\in\tilde{M}$, and let $X = \bigcup_{1}^{\infty}E_j$ where $E_j\in M$ and $\mu(E_j) < \infty$ for all $j$. We know $M\subset \tilde{M}$ what we want to show is that $\tilde{M}\subset M$. We can write $$A = A\cap X = A\cap \left(\bigcup_{1}^{\infty}E_j\right) = \bigcup_{1}^{\infty}E_j\cap A$$ Since $\mu(E_j)<\infty$ we have $E_j\cap A\in M$ for all $j$. Therefore, $\tilde{M}\subset M$, thus $\mu$ is saturated. b.) $\tilde{M}$ is $\sigma$-algebra. Proof - i.) $\emptyset\in M\subset \tilde{M}$, so $\emptyset\in \tilde{M}$. ii.) Let $B\in \tilde{M}$. Take any $E\in M$ such that $\mu(E) < \infty$. Then $$E\setminus B = E\cap B^c = E\cap (E\cap B)^c$$
since $E\in M$ and $(E\cap B)\in M$ then $(E\cap (E\cap B)^c\in M$. Thus we have $B^c\in \tilde{M}$. iii.) Let $\{B_j\}_{1}^{\infty}\in \tilde{M}$. Take any $E\in M$ with $\mu(E)< \infty$. Then, $$\left(\bigcup_{1}^{\infty}B_j\right)\cap E = \bigcup_{1}^{\infty}(B_j\cap E)\in M$$ 
so, by definition of $\tilde{M}$, $\bigcup_{1}^{\infty}B_j\in \tilde{M}$. Therefore $\tilde{M}$ us a $\sigma$-algebra. c.) Define $\tilde{\mu}$ on $\tilde{M}$ by $\tilde{\mu}(E) = \mu(E)$ if $E\in M$ and $\tilde{\mu}(E) = \infty$ otherwise. Then $\tilde{\mu}$ is a saturated measure on $\tilde{M}$, called the saturation of $\mu$. Step 1: Show that $\tilde{\mu}$ is a measure on $\tilde{M}$. Proof - i.) $\tilde{\mu}(\emptyset) = \mu(\emptyset) = 0$. ii.) Let $\{E_j\}_{1}^{\infty}\in \tilde{M}$ that is pairwise disjoint. Let $$E = \bigcup_{1}^{\infty}E_j$$
If $E \in M$ then \begin{align*} \tilde{\mu}(E) = \tilde{\mu}\left(\bigcup_{1}^{\infty}E_j\right) &= \mu\left(\bigcup_{1}^{\infty}E_j\right)\\
&= \sum_{1}^{\infty}\mu(E_j)\\
&= \sum_{1}^{\infty}\tilde{\mu}(E_j)
\end{align*}
If $E\notin M$ then $\tilde{\mu}(E) = \infty$... not sure where to go from here. Step 2 - $\tilde{\mu}$ is saturated. Proof - Let $E\subset X$ such that $E\cap A\in \tilde{M}$ when $\tilde{\mu}(A) < \infty$. Choose a $B\in M$ such that $\mu(B)<\infty$. Then, clearly $\tilde{\mu}(B)<\infty$ and $E\cap B\in \tilde{M}$. So, $E\cap B = (E\cap B)\cap B\in M$ so $E\in \tilde{M}$ it thus follows that $\tilde{\mu}$ is saturated. d.) If $\mu$ is complete, so is $\tilde{\mu}$. Proof - Suppose $\mu$ is complete. Let $A\subset X$ and suppose there is a $B\in \tilde{M}$ such that $A\subset B$ and $\mu(B) = 0$. Since $B\in\tilde{M}$ and $\mu(B) = 0$ then $\tilde{\mu}(B) < \infty$ and hence $B\in M$. This, since $A\subset B$ we have $A\in M$ by completeness of $\mu$. Therefore, $A\in \tilde{M}$ and $\tilde{\mu}$ is complete. e.) Suppose that $\mu$ is semifinite. For $E\in\tilde{M}$, define $\underline{\mu}(E) = \sup\{\mu(A):A\in M, A\subset E\}$. Then $\underline{\mu}$ is a saturated measure on $\tilde{M}$ that extends $\mu$. Step 1 - $\underline{\mu}$ is a measure. Proof - i.) $\overline{\mu}(\emptyset) = \mu(\emptyset) = 0$ ii.) Let $\{E_j\}_{1}^{\infty}$ be a sequence of disjoint sets in $\tilde{M}$. Set, $$E = \bigcup_{1}^{\infty}E_j$$ then by definition of $\tilde{M}$ there is an $A\in M$ and $A\subset E$. Case 1 - $\mu(A) < \infty$. Then $$\mu(A) = \mu\left(\bigcup_{1}^{\infty}E_j\cap A\right) = \sum_{1}^{\infty}\mu(E_j\cap A)\leq \sum_{1}^{\infty}\underline{\mu}(E_j)$$ Case 2 - $\mu(A) = \infty$. By semifiniteness, for all $C>0$ there exists a $F\subset A$ such that $F\in M$ and $\mu(F) = C$. Then by case 1, $\leq \sum_{1}^{\infty}\underline{\mu}(E_j) = \infty$. Therefore, $\mu(A) \leq \sum_{1}^{\infty}\underline{\mu}(E_j)$. Taking the supremum over all $A$ we have $$\underline{\mu}\left(\bigcup_{1}^{\infty}E_j\right)\leq \sum_{1}^{\infty}\underline{\mu}(E_j)$$ Now we need to show the reverse inequality. By the definition of supremum there exists a sequence $\{B_i\}_{1}^{\infty}\in M$ and $B_i\subset E_i$ for all $i$. Thus, $\underline{\mu}(E_i)\leq \mu(B_i) + \epsilon 2^{-i}$. Therefore, 
\begin{align*}
\sum_{1}^{\infty}\underline{\mu}(E_i) &\leq \sum_{1}^{\infty}\mu(B_i) + \epsilon\\
&= \mu\left(\bigcup_{1}^{\infty}B_i\right) + \epsilon \ \ \text{is this true because of case 1?}\\
&\leq \underline{\mu}\left(\bigcup_{1}^{\infty}E_i\right) + \epsilon
\end{align*}
Since this holds for all $\epsilon > 0$, we have $$\sum_{1}^{\infty}\underline{\mu}(E_j)\leq \underline{\mu}\left(\bigcup_{1}^{\infty}E_j\right)$$ Therefore, $$\underline{\mu}\left(\bigcup_{1}^{\infty}E_j\right) = \sum_{1}^{\infty}\underline{\mu}(E_j)$$
and hence $\underline{\mu}$ is a measure. Step 2 - $\underline{\mu}$ is saturated. Proof - Let $E\subset X$ be such that $E\cap A\in \tilde{M}$ when $\underline{\mu}(A)< \infty$. Take any $B\in M$ such that $\mu(B) < \infty$. Then $\underline{\mu}(B) < \infty$ so $E\cap B\in \tilde{M}$. Thus, $E\cap B = (E\cap B)\cap B\in M$ and $E\in\tilde{M}$, hence, $\underline{\mu}$ is saturated. Step 3 - $\underline{\mu}$ is an extention of $\mu$. Proof - Let $E\in M$. For any $A\in M$ such that $A\subset E$, we have by monotonicity that $\mu(A)\leq \mu(E)$. Since $\underline{\mu}(E)$ is the supremum over all such $A$, we must have that $\underline{\mu}(E)\leq \mu(E)$. OTOH.... not sure how to show the reverse inequality. f.) Let $X_1$ and $X_2$ be disjoint uncountable sets, $X = X_1\cup X_2$, and $M$ the $\sigma$-alegbra of countable or co-countable sets in $X$. Let $\mu_0$ be counting measure on $\mathcal{P}(X_1)$ and define $\mu$ on $M$ by $\mu(E) = \mu_0(E\cap X_1)$. Then $\mu$ is a measure on $M$, $\tilde{M} = \mathcal{P}(X)$, and in the notation of parts (c) and (e), $\tilde{\mu}\neq \underline{\mu}$. Step 1 - $\mu$ is a measure on $M$. Proof - i.) $\mu(\emptyset) = \mu_0(\emptyset\cap X_1) = 0$ ii.) Let $\{E_j\}_{1}^{\infty}$ be a sequence of disjoint sets in $M$, then 
\begin{align*}
\mu\left(\bigcup_{1}^{\infty}E_j\right) &= \mu_0\left(\bigcup_{1}^{\infty}E_j\cap X_1\right)\\
&= \sum_{1}^{\infty}\mu_0(E_j\cap X_1)\\
&= \sum_{1}^{\infty}\mu(E_j) \ \ \ \text{is this true because} \ \mu_0 \ \text{is a counting measure?}
\end{align*}
Therefore $\mu$ is a measure on $M$. Step 2 - $\tilde{M} = \mathcal{P}(X)$ Proof - Step 3 - $\tilde{\mu}\neq \underline{\mu}$ Proof - Take $y_1,y_2\in X_1$. Let $E = \{y_1,y_2\}\cup X_2$. Then $E\notin M$, so $\tilde{\mu}(E) = \infty$. However, $\underline{\mu}(E) = 2$. I will re-edit my question and include these other proofs as I continue to do them. I am pretty sure my proof for $\tilde{\mu}$ is a measure is incorrect. But I am not really sure how to do it. Any suggestions on any of these is greatly appreciated.","['real-analysis', 'measure-theory', 'proof-verification']"
1806856,Have I discovered an analytic function allowing quick factorization?,"So I have this apparently smooth, parametrized function: The function has a single parameter $ m $ and approaches infinity at every $x$ that divides $m$. It is then defined for real $x$ apart from a finite set of arguments.
The animation above shows plots of $f_{m}(x)$ (in white) for $x\in(0, 17>$, while each frame shifts $m$ by $0.05$ starting from $2$ and ending in $17$.
In green plotted is a hyperbola $(x, {m\over x})$. Explanation Consider a lattice of integer-valued points $(a, b)$: (the origin $(0, 0)$ is at the left-bottom corner of the picture) Let us draw a hyperbola $(x, {m\over x})$ for real $x$ and for $m = 10$. It is trivial to observe that intersections of the hyperbola and the lattice lead us to factors of $m$, in this case, $10$.
For example, the green curve above intersects red dots at $(2, 5)$ and $(5, 2)$. Let us therefore define the function whose results were presented in the first animation: $$f_{m}(x) = \sum_{a, b  \in  \mathbb{Z}} {1\over\lVert (x, {m\over x}) - (a, b)\rVert ^{3}}
$$ What this function does intuitively : Choose a constant $m$ whose divisors you wish to seek. Given $x$, calculate point $(x, {m\over x})$ on the real plane. For every possible $(a, b)$ where $a$ and $b$ are integers, calculate distance of $(a, b)$ from $(x, {m\over x})$ and add inverse of cube of the result to the total sum. Notice that if $x$ and $m\over x$ are integers (and therefore $x$ divides $m$), there exists an integer pair $(a, b)$ whose distance to ($x$, $m\over x$) is zero and when that happens, the series diverges due to addition of a $1 \over \lVert(0, 0)\rVert ^{3} $ term. Plot of $f_{11}(x), x > 0$: Notably, the function only ever approaches infinity at $x\in \{0, 1, 11\}$ ultimately indicating primality of $11$. A little note: I've obtained the plots by coding a procedure that considers $(a, b)$ until distance inverses become less than some epsilon, in which case there is no point in further traversal. Important update #1 I have updated the definition of $f_{m}(x)$ and replaced plots with ones generated by the new implementation, as it was previously using exponent of 2. According to Daniel Fischer, $f_{m}(x)$ with exponent not bigger than $2$ in the denominator must necessarily diverge for every $x$ in its domain. I have therefore decided to use cubes. Important update #2 It appears that in order to finely approximate $f_{m}(x)$, it is enough to simply sum inverses of distances to the four closest lattice points , instead of taking an infinite sum. Let $H = (x, {m\over x} )$ (actual point on hyperbola) Let $h = (\lfloor x \rfloor, \lfloor {m\over x} \rfloor)$ (the closest lattice point to the left-bottom) Our new function is therefore $$f_{m}(x) = {1\over \mathbf{d}(H, h) }
+ {1\over \mathbf{d}(H, h + (0, 1)) }
+ {1\over \mathbf{d}(H, h + (1, 0)) }
+ {1\over \mathbf{d}(H, h + (1, 1)) }
$$ In fact, the plots look almost identical.
Consider $f_{17}(x):$ For me, it appears to yield a perfectly acceptable estimate. I believe it is especially worthy of our attention that the function looks like it remains smooth even in presence of floor function . Questions: How can we qualify this function? Can we know that it is smooth? Analytic maybe? Is there a way to approximate this function without infinite series? Or maybe a simpler, exact formula? - Partially solved - four closest lattice points finely approximate the result while probably preserving smoothness. I've chosen square of the distance on the premise that it must converge equally well as the series $ \sum\limits_{n=1}^\infty {1 \over n^{s}}$ for $s > 1$. Is it actually the case that $f_{m}(x)$ must always converge for $x > 0$ and $x$ not dividing $m$? Solved - $f_{m}(x)$ must always diverge with the exponent not bigger than $2$ in the denominator . Is this function of any use? I know very well that the infinite series are far from efficient, but maybe there's an approximation that would sufficiently well predict factors? Or maybe, is it ultimately an obscure encoding of trial division?","['divisibility', 'prime-factorization', 'factoring', 'prime-numbers', 'sequences-and-series']"
1806870,Smallest number of $n$-simplices in a triangulation of the sphere,"Let $X$ be a simplicial complex homeomorphic to $S^n$. I proved that there must be at least $(n+2)$ vertices in $X$ and that there must be at least one $n$-simplex in $X$. Now I want to prove that there are at least $(n+2)$ n-simplices in $X$. My idea was to assume that there are fewer than $(n+2)$ n-simplices and then proving that the simplicial boundary map $\partial_n:C_n^{\Delta}(X)\to C_{n-1}^{\Delta}(X)$ is injective, contradicting $H_n^{\Delta}(X)=\Bbb Z$. This quickly proved to be very messy and I am not sure if that's the best way to go about it. I appreciate all help.","['algebraic-topology', 'combinatorics', 'general-topology']"
1806871,What is the difference between $\frac{\mathrm{d}}{\mathrm{d}x}$ and $\frac{\partial}{\partial x}$?,"Is there not any difference between $\frac{\mathrm{d}}{\mathrm{d}x}$ and $\frac{\partial}{\partial x}$ as long as your function has one variable? $f(x) = x^3\implies \left\{\begin{align}&\dfrac{\mathrm{d}}{\mathrm{d}x}f = \dfrac{\mathrm{d}f}{\mathrm{d}x}=\dfrac{\mathrm{d}(x\mapsto x^3)}{\mathrm{d}x} = x\mapsto 3x^2&\color{green}{\checkmark}\\&\dfrac{\partial}{\partial x}f = \dfrac{\partial f}{\partial x}= \dfrac{\partial(x\mapsto x^3)}{\partial x} = x\mapsto 3x^2&\color{green}{\checkmark}\end{align}\right.$ And if so, why does this change with two (or more) variables? $\require{cancel}
f(x,y) = yx^3\implies \left\{\begin{align}&\color{grey}{\cancel{\dfrac{\mathrm{d}}{\mathrm{d}x}f = }} \dfrac{\mathrm{d}f}{\mathrm{d}x}=\dfrac{\mathrm{d}((x,y)\mapsto yx^3)}{\mathrm{d}x} \neq x\mapsto 3yx^2&\color{green}{\checkmark}\\&\dfrac{\partial}{\partial x}f = \dfrac{\partial f}{\partial x} =\dfrac{\partial((x,y)\mapsto x^3)}{\partial x} = x\mapsto 3yx^2&\color{red}{\mathcal{X}}\end{align}\right.$ I get that it is supposed to be something like this $f(x,y) = yx^3\implies \left\{\begin{align}&\color{grey}{\cancel{\dfrac{\mathrm{d}}{\mathrm{d}x}f = }} \dfrac{\mathrm{d}f}{\mathrm{d}x}=\dfrac{\mathrm{d}((x,y)\mapsto yx^3)}{\mathrm{d}x} \neq\\&\cdots\quad (x,y)\mapsto 3y\dfrac{\mathrm{d}\color{red}{(x\mapsto x^3)}}{\mathrm{d}x}+\dfrac{\mathrm{d}\color{red}{(y\mapsto y)}}{\mathrm{d}x}x^3 =\\&\cdots\quad (x,y)\mapsto 3y\color{red}{(x\mapsto x^2)}+\dfrac{\mathrm{d}\color{red}{(y\mapsto y)}}{\mathrm{d}x}x^3&\color{green}{\checkmark}\\&\dfrac{\partial}{\partial x}f = \dfrac{\partial f}{\partial x} = \dfrac{\partial(x,y)\mapsto x^3)}{\partial x} = x\mapsto 3yx^2&\color{green}{\checkmark}\end{align}\right.$","['multivariable-calculus', 'partial-derivative', 'notation', 'derivatives']"
