question_id,title,body,tags
3793869,"USAMO $1989$, Problem $2$","Problem $2$ from USAMO, $1989$ : The $20$ members of a local tennis club have scheduled exactly $14$ two-person games among themselves, with each member playing in at least one game. Prove that within this schedule there must be a set of six games with $12$ distinct players. My attempt at a solution: Since there are $20$ players, each of whom has played at least one game, the least number of matches which must be organised to accommodate all players would be $10$ . Now, $4$ more matches are left, which can be played by at most $8$ of these $20$ players. Thus, at least $12$ of the players get to play no more than $1$ match, and hence there must exist a set of six games with $12$ distinct players . This, I believe, will be the guiding principle of a rigorous proof. I have the following doubts clouding my mind: Is my reasoning sound, i.e., free of any pitfalls? If correct, how to frame the above reasoning in a mathematically rigorous proof? Edit: As pointed out by Ben in the comments, the statement in italics is unjustified. I overlooked a lot of possibilities while framing this proof, it seems. Thus, I would like to get some hints to proceed with more reasonable proof.","['contest-math', 'recreational-mathematics', 'solution-verification', 'combinatorics']"
3793883,Prove that there exists $c$ such that $\int_{1}^{\infty} \frac{(1-x+[x])(x^a-x^{1-a})}{x^2}dx=(c^a-c^{1-a})\int_{1}^{\infty} \frac{(1-x+[x])}{x^2}dx$,"$I = \int_{1}^{\infty}$$\frac{(1-x+[x])(x^a-x^{1-a})}{x^2} dx$ where $[x]$ denotes Greatest integer function and $a\in \mathbb{C}$ is a constant. Prove that there exists some $c \in (1,\infty)$ such that $$\int_{1}^{\infty}\frac{(1-x+[x])(x^a-x^{1-a})}{x^2}dx =  (c^a-c^{1-a})\int_{1}^{\infty}\frac{(1-x+[x])}{x^2}dx.$$ My try: $$0<1-x+[x]\leq 1 \Rightarrow 0< \frac{1-x+[x]}{x^2} \leq \frac{1}{x^2}$$ $$0<\int_{1}^{\infty}\frac{1-x+[x]}{x^2}dx \leq 1.$$ Consider, $$\frac{\int_{1}^{\infty}\frac{(1-x+[x])(x^a-x^{1-a})}{x^2}dx }{\int_{1}^{\infty}\frac{(1-x+[x])}{x^2}dx} = \lambda \Rightarrow \int_{1}^{\infty}\frac{(1-x+[x])[(x^a-x^{1-a})-\lambda]}{x^2}dx=0$$","['integration', 'real-analysis', 'maxima-minima', 'calculus', 'derivatives']"
3793887,"Solve the ODE $(x-1)y'' - xy' + y = 1$ subject to boundary conditions $y(0)=0,y(1)=2$ using Greens function","I'm looking to solve the BVP $$(x-1)\frac{d^2y}{dx^2} - x\frac{dy}{dx} + y = 1$$ subject to the conditions $y(0)=0,y(1)=2$ , firstly by getting the problem into self adjoint form and then by finding the Greens function. I think I have the self adjoint form for the problem, namely $$\frac{d}{dx}(\frac{e^{-x}}{x-1}\frac{dy}{dx}) +\frac{e^{-x}}{(x-1)^2}y=\frac{e^{-x}}{(x-1)^2}$$ where the boundary conditions remain unchanged as I have just multiplied through by a factor and then used the chain rule to simplify. Then, I have noted that $y=x$ and $y=e^x$ are solutions to the homogeneous form of the above which are linearly independent since they have non-zero Wronskian on the domain we are considering. Here I've also noted that $u=2x$ satisfies the homogeneous problem and the inhomogeneous boundary conditions, so then $\hat{y} = y - u$ is a solution to the inhomogeneous problem with the homogeneous boundary conditions (which we can find by the Greens function and use to get the solution $y$ of the problem we are considering). It is in finding the Greens function that I'm struggling, as I seem to always get a wrong answer. I wonder if my understanding of the method is wrong. Could someone provide a step by step process of how to get the Greens function in this case, and then how to get a solution from that?","['boundary-value-problem', 'self-adjoint-operators', 'greens-function', 'ordinary-differential-equations']"
3793917,How to prove this statement in set theory?,"I need to prove that $((A \cap B) \cup C = A \cap (B \cup C)) \iff (C \subset A)$ While proving, I was trying to use distributions and intersect both sides of the left equation set $\bar{B}$ . It works for $\Rightarrow$ , but not sure for $\Leftarrow$ It would be good to get at least 1 hint if my mind is wrong. Thanks in advice",['elementary-set-theory']
3793920,Lower a polytope into the water $-$ are vertices at the water level connected to those at the bottom?,"Suppose we take a convex polyope $P$ and a face $A$ with vertices $a_1,\ldots, a_n$ . We hold the polytope with $A$ flush with the surface and slowly lower it, keeping $A$ parallel to the surface thoughout. We continue lowering until the water level reaches some vertex $b_1$ not belonging to $A$ . Then let $b_1,\ldots, b_m$ be all the vertices at the water level. I wonder: Is every $b_i$ joined by an edge to some $a_i$ ? Seems physically obvious. But so do many facts about polytopes, such as the linear-inequalities/convex-hull definitions being equivalent. If you consider the part of the polytope between the water level and the plane spanned by $A$ you get a smaller polytope $Q$ . This $Q$ has all $a_i,b_j$ as vertices but might have extra vertices created when edges of $A$ pass through the water. Nevertheless all the vertices are contained in one of the two planes. This suggests the following perhaps easier question. Suppose $P_1,P_2$ are two parallel planes, and $P$ is a polytope whose every vertex is in either $P_1$ or $P_2$ . Is each vertex in $P_1$ joined by an edge to a vertex of $P_2$ ?","['polytopes', 'combinatorial-geometry', 'geometry']"
3793961,Evaluate the limit $\lim_{n \to \infty} \left(3^n+1\right)^{\frac1n} $,How do you evaluate this sequence limit using the squeeze/sandwich theorem? $$\lim_{n \to \infty} \left(3^n+1\right)^{\frac1n} $$ I don't really know where to start. I've tried using the fact that $\lim_{n \to \infty} \left(3^n\right)^{\frac1n} = 3$ (which is the correct answer) but I don't know where to go from there. Thanks!,"['limits', 'sequences-and-series']"
3793962,Is it possible to write a metric a space as a countable disjoint union of compact sets?,"Let $ (X,d)$ be a metric space and let $\mu $ be a Radon $\sigma$ -finite measure on the Borel $\sigma$ -algebra. I read that it's possible to find countable disjoint compact sets $\lbrace K_n\rbrace_{\mathbb{N}}$ and a $\mu$ -null set $N$ such that $$ X=\bigcup_{\mathbb{N}}K_n\cup N. $$ I've tried to reach some results using inner regularity of $\mu$ , but nothing. Is this statement true? How can i prove it?","['measurable-sets', 'measure-theory']"
3794022,Need help minimizing the Kullback-Leibler divergence between discrete observations and a complicated distribution.,"TL;DR: I want to fit the probability distribution $P(x) = \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \int_{x}^{W} N_1(z) \hspace{0.1cm} dz$ to a bunch of discrete observations in order to obtain $N_0$ and $N_1$ which produced them. I fail at solving the following equations and need help at how to approach this: $$\frac{\partial}{\partial \mu_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right), \hspace{0.5cm}\frac{\partial}{\partial \sigma_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right)$$ My motivation comes from this paper: Probabilistic Object Detection: Definition and Evaluation . The authors define a probability distribution over all pixels $(u', v')$ of an image as a product of integrals of two 2D-Gaussian distributions $N_0$ and $N_1$ , which represent bounding box corners. The exact formula is: $$P(u', v') = \int \int_{0, 0}^{u', v'} N_0(u, v) \hspace{0.1cm} du \hspace{0.1cm} dv \int \int_{u', v'}^{H, W} N_1(u, v) \hspace{0.1cm} du \hspace{0.1cm} dv$$ where $H$ and $W$ are width and height of the image. This procedure allows one to get from two bounding-box gaussians to a probability distribution over all pixels - however, I wanted to solve the opposite problem of trying to reconstruct the two gaussians given the pixel-wise probability distribution. To do that, I first want to solve a simpler, 1D version of this problem. Here is an image showing the 1D problem: Visualization of the 1D problem In a 1D setting, the element-wise probability distribution simplifies to this calculation: $$P(x) = \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \int_{x}^{W} N_1(z) \hspace{0.1cm} dz$$ In this example there are 16 discrete probability valuse at each of 16 positions of the 1D array. In my toy example I just made up two gaussians to calculate the resulting $P(x)$ . The ""observations"" are discretizations of that $P$ with some gaussian noise added. My approach to reverse engineer the two gaussians from the observations was to minimize the Kullback-Leibler divergence between observations and the combined integrated distribution. This leads to my problem: After some reformulation of the KL-divergence I'm stuck at actually minimizing it, because the logarithm doesn't act directly on the gaussians but on the integrals. I don't know how to approach the problem from here, or if an analytical solution even exists at all. I'll write down the steps I have done so far. $W$ is the width of the 1D array and $O(x)$ the observation at position x: $$D_{KL} (O||P) = \sum_{x=1}^W O(x) ln \left(  \frac{O(x)}{\int_{0}^{x} N_0(z) \hspace{0.1cm} dz \int_{x}^{W} N_1(z) \hspace{0.1cm} dz} \right)$$ $$= \sum_{x=1}^W O(x) \left[ln (O(x) - ln \left( \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \right) - ln \left( \int_{x}^{W} N_1(z) \hspace{0.1cm} dz \right)\right]$$ $$= \sum_{x=1}^W O(x) ln(O(x) - \sum_{x=1}^W O(x)\hspace{0.1cm} ln \left( \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \right)- \sum_{x=1}^W O(x)\hspace{0.1cm} ln \left( \int_{x}^{W} N_1(z) \hspace{0.1cm} dz \right)$$ If I want to minimize this term, that brings me to the following subproblem (each of the optimization variables $\mu_1, \mu_2, \sigma_1, \sigma_2$ is only in one of the terms, so for each only one of the addends is relevant): $$max \hspace{0.1cm} \mu_1,\sigma_1 \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right) \rightarrow$$ $$\frac{\partial}{\partial \mu_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right)$$ $$\frac{\partial}{\partial \sigma_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right)$$ This is the point at which I don't really know what to do anymore. If anybody can point me at how to solve this or a completely different approach to get to the two original gaussians without iterative numeric approaches, I'd be really thankful!","['multivariable-calculus', 'calculus']"
3794028,"Show that if $a,b \in \mathbb{R}^n$, then $|||a|| - ||b||| \leqslant ||a+b||$","Show that if $a,b \in \mathbb{R}^n$ , then $$|\|a\| - \|b\|| \leqslant \|a+b\|$$ We have that $$||a|| = ||a+b-b||\leqslant||a+b||+||-b|| = ||a+b||+||b||$$ and that $$||b|| = ||b+a-a||\leqslant||b+a||+||-a|| = ||b+a||+||a||$$ however I don't see how I can continue from here. If i take $||a||-||b||$ I get that $$||a||-||b|| = ||a+b||+||b|| -(||b+a||+||a||) = ||b||-||a||$$ which doesn't help at all. What shold I do here?","['algebra-precalculus', 'absolute-value', 'triangle-inequality', 'inequality']"
3794036,What's the right definitition of continuously differentiable?,"Suppose $V$ and $W$ are Banach spaces, $U\subset V$ is open, and $F:U\to W$ is a differentiable function. Then the derivative of $F$ is the map $$ DF:U\to B(V;W) $$ where $B(V;W)$ is the Banach space of continuous linear maps $V\to W$ . We say that $F$ is of class $\mathcal{C}^1$ at a point $x_0\in U$ if the mapping $$ U\ni x\mapsto DF(x) \in B(V;W) $$ is continuous at $x_0$ ; we say that $F$ is of class $\mathcal{C}^1$ on $U$ if $F$ is of class $\mathcal{C}^1$ at each point in $U$ . If $X$ is an arbitrary subset of the Banach space $V$ and $f:X\to W$ is a map, then we say that $f$ is of class $\mathcal{C}^1$ on $X$ if there exists an open subset $U$ of $V$ where $X\subset U$ and a function $F:U\to W$ of class $\mathcal{C}^1$ on $U$ where $F|_X=f$ . (Informally, we can extend $f$ to an open set on which it is of class $\mathcal{C}^1$ .) See this answer for a function $f$ which is continuously differentiable at only a single point. Namely, if $g(t)=t^2\sin(1/t)$ for $t\in\mathbb{R}$ then the function $$ f(t) = \sum_{n\geq 1} \frac{g(t-1/n)}{2^n} $$ is continuously differentiable at $t=0$ . However, $f$ has discontinuities arbitrarily close to the origin so $f$ cannot be of class $\mathcal{C}^1$ on any open set containing $0$ . That is, $f$ is a function which is of class $\mathcal{C}^1$ at $0$ , but $f$ is not $\mathcal{C}^1$ on $\{0\}$ . This does not seem right to me. Of course, it is not ""typical"" for a function we encounter to behave this way. However, this example still bothers me. What can we do? Can we slightly modify the above definitions so that this does not happen? Is the answer I referenced somehow incorrect? (I couldn't prove the results he stated...)","['banach-spaces', 'frechet-derivative', 'continuity', 'functional-analysis', 'derivatives']"
3794044,"Prove that $_4F_3\left(\frac13,\frac13,\frac23,\frac23;1,\frac43,\frac43;1\right)=\frac{\Gamma \left(\frac13\right)^6}{36 \pi ^2}$","I found an interesting problem about generalized hypergeometric series in MO, stating that: $$\, _4F_3\left(\frac{1}{3},\frac{1}{3},\frac{2}{3},\frac{2}{3};1,\frac{4}{3},\frac{4}{3};1\right)=\sum_{n=0}^\infty \left(\frac{(\frac13)_k (\frac23)_k}{(1)_k (\frac43)_k}\right)^2=\frac{\Gamma \left(\frac{1}{3}\right)^6}{36 \pi ^2}$$ This numerically agrees, but I found no proof using either elementary properties of hypergeometric functions (e.g. cyclic sum) or classical Gamma formulas (e.g. Dougall formula). I bet it has something to do with modular forms and elliptic $K$ integral, but the exact relation remain elusive. How to prove this identity ? What will be its motivation? Can we generate other Gamma evaluation of high order hypergeometric series using the method of proving it? Any help will be appreciated.","['gamma-function', 'closed-form', 'sequences-and-series', 'elliptic-integrals', 'hypergeometric-function']"
3794094,Computing $\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}$,"I managed to find $$\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}=3\zeta(2)\zeta(3)-8\underbrace{\int_0^{\pi/2}x^2\tan x\ln^2(\sin x)dx}_{I}\tag1$$ but I could not finish $I$ : My attempt: In the book, Almost Impossible Integrals, Sums and series , page $243$ ,  Eq $(3.281)$ we have $$\tan x\ln(\sin x)=-\sum_{n=1}^\infty\left(\psi\left(\frac{n+1}{2}\right)-\psi\left(\frac{n}{2}\right)-\frac1n\right)\sin(2nx)$$ $$=-\sum_{n=1}^\infty\left(\int_0^1\frac{1-t}{1+t}t^{n-1}dt\right)\sin(2nx),\quad 0<x<\frac{\pi}{2}$$ So $$I=-\sum_{n=1}^\infty\left(\int_0^1\frac{1-t}{1+t}t^{n-1}dt\right)\underbrace{\left(\int_0^{\pi/2}x^2\sin(2nx)\ln(\sin x)dx\right)}_{J}$$ For $J$ , we use the Fourier series of $\ln(\sin x)=-\ln(2)-\sum_{k=1}^\infty\frac{\cos(2kx)}{k}$ $$J=-\ln(2)\underbrace{\int_0^{\pi/2}x^2\sin(2nx)dx}_{J_1}-\sum_{k=1}^\infty \frac1k\underbrace{\int_0^{\pi/2}x^2\sin(2nx)\cos(2kx)dx}_{J_2}$$ $$J_1=\frac{\cos(n\pi)}{4n^3}-\frac{1}{4n^3}-\frac{3\zeta(2)\cos(n\pi)}{4n}+\frac{\pi\sin(n\pi)}{4n^2}$$ $$=\frac{(-1)^n}{4n^3}-\frac{1}{4n^3}-\frac{3\zeta(2)(-1)^n}{4n}$$ The last result follows from $\cos(n\pi)=(-1)^n$ and $\sin(n\pi)=0$ for $n=1,2,3,..$ $$J_2=\frac18\left(\frac{1}{(k-n)^3}-\frac{\pi\sin(\pi(k-n))}{(k-n)^3}-\frac{(1-3(k-n)^2\zeta(2))\cos(\pi(k-n))}{(k-n)^3}\right)$$ $$-\frac18\left(\frac{1}{(k+n)^3}-\frac{\pi\sin(\pi(k+n))}{(k+n)^3}-\frac{(1-3(k+n)^2\zeta(2))\cos(\pi(k+n))}{(k+n)^3}\right)$$ I stopped here as I can not simplify $J_2$ . But I think it can be simplified considering $n,k\in\mathbb{Z}>0$ . Any idea? Also do you have a different path? Thank you. Addendum I also found that $$\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}=\int_0^1\frac{\text{Li}_4(x)-\ln(1-x)\text{Li}_3(x)-\frac12\text{Li}_2^2(x)}{x\sqrt{1-x}}dx$$ $$=\int_0^{\pi/2}\frac{2\text{Li}_4(\sin^2x)-4\ln(\cos x)\text{Li}_3(\sin^2x)-\text{Li}_2^2(\sin^2x)}{\sin x}dx$$ Proof of $(1)$ : Differentiating both sides of $\int_0^1 x^{n-1}\ln(1-x)dx=-\frac{H_n}{n}$ with respect to $n$ gives $$\int_0^1 x^{n-1}\ln x\ln(1-x)dx=\frac{H_n}{n^2}+\frac{H_n^{(2)}}{n}-\frac{\zeta(2)}{n}$$ multiply both sides by $\frac{4^n}{n^2{2n\choose n}}$ then $\sum_{n=1}^\infty$ we get $$\sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}}+\sum_{n=1}^\infty\frac{4^nH_n^{(2)}}{n^3{2n\choose n}}-\sum_{n=1}^\infty\frac{\zeta(2)4^n}{n^3{2n\choose n}}=\int_0^1\frac{\ln x\ln(1-x)}{x}\left(\sum_{n=1}^\infty\frac{(4x)^n}{n^2{2n\choose n}}\right)dx$$ $$=\int_0^1\frac{\ln x\ln(1-x)}{x}\left(2\arcsin^2(\sqrt{x})\right)dx$$ $$\overset{\sqrt{x}=\sin\theta}{=}16\int_0^{\pi/2}x^2\cot x\ln(\sin x)\ln(\cos x)dx\tag1$$ Next, differentiate both sides of $\int_0^1 x^{n-1}\ln(1-x)dx=-\frac{H_n}{n}$ with respect to $n$ twice we get $$-\frac12\int_0^1 x^{n-1}\ln^2x\ln(1-x)dx=\frac{H_n}{n^3}+\frac{H_n^{(2)}}{n^2}+\frac{H_n^{(3)}}{n}-\frac{\zeta(2)}{n^2}-\frac{\zeta(3)}{n}$$ multiply both sides by $\frac{4^n}{n{2n\choose n}}$ then $\sum_{n=1}^\infty$ we have $$\sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}}+\sum_{n=1}^\infty\frac{4^nH_n^{(2)}}{n^3{2n\choose n}}+\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}-\sum_{n=1}^\infty\frac{\zeta(2)4^n}{n^3{2n\choose n}}-\sum_{n=1}^\infty\frac{\zeta(3)4^n}{n^2{2n\choose n}}$$ $$=-\frac12\int_0^1\frac{\ln x\ln(1-x)}{x}\left(\sum_{n=1}^\infty\frac{(4x)^n}{n{2n\choose n}}\right)dx$$ $$=-\frac12\int_0^1\frac{\ln x\ln(1-x)}{x}\left(\frac{2\sqrt{x}\arcsin(\sqrt{x})}{\sqrt{1-x}}\right)dx$$ $$\overset{\sqrt{x}=\sin\theta}{=}-16\int_0^{\pi/2}x\ln(\cos x)\ln^2(\sin x)dx$$ $$\overset{IBP}{=}16\int_0^{\pi/2}x^2\cot x\ln(\sin x)\ln(\cos x)dx-8\int_0^{\pi/2}x^2\tan x\ln^2(\sin x)dx\tag2$$ Subtracting $(2)$ from $(1)$ and using $\sum_{n=1}^\infty\frac{\zeta(3)4^n}{n^2{2n\choose n}}=3\zeta(2)\zeta(3)$ yields $$\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}=3\zeta(2)\zeta(3)-8\int_0^{\pi/2}x^2\tan x\ln^2(\sin x)dx$$","['integration', 'real-analysis', 'harmonic-numbers', 'binomial-coefficients', 'sequences-and-series']"
3794113,If two graphs are isomorphic then will their determinants be equal?,"10 vertices graphs G1 and G2 I'm solving the exercises in chapter 2 from Graph Theory with Algorithms and its Applications. So far, for isomorphism I just write all the edges and try to find the patterns, which for small graphs is enough. For these 2 10-vertices 30-edges graph I think I need something more scalable. I tried 'translating' both graphs to a unified structure, but off course if one vertx changes name the graph will also look different. I wrote the adjacency matrix trying to find a pattern, and there's a column that makes me suspicious these two may not be isomorphic. However, I could always change the name of a vertex to make columns look a bit more similar. Anyway, I found that two graphs (A,B) are isomorphic if A = PBP(t) where P is a permutation matrix, and P(t) is its transpose. Since det(P) = 1 or -1 = det(P(t)) and det(AB) = det(A)*det(B) then if A and B are isomorphic: det(A) = 1|-1*det(B)*1|-1
det(A)=det(B) could two adjacency matrices have the same determinant and not be isomorphic? or is equality in the determinant a sufficient condition (and much more scalable) for isomorphism? Best, Sergio","['graph-isomorphism', 'determinant', 'graph-theory', 'adjacency-matrix', 'linear-algebra']"
3794153,"Positive reals satisfy $ \sum_{i=1}^{24} x_i = 1 $, determine maximum of following quantity","So, positive reals satisfy the following $$ \sum_{i=1}^{24} x_i = 1 $$ And I need to find maximum of the following quantity. $$ \left( \sum_{i=1}^{24} \sqrt{x_i}\right) \left(\sum_{i=1}^{24} \frac{1}{\sqrt{1+x_i} } \right) $$ Now, using Cauchy Schwarz inequality, I got $$ \left( \sum_{i=1}^{24} \sqrt{x_i}\right)^2 \leqslant \underbrace{(1+1+\cdots + 1)}_{\text{24 times}} \left( \sum_{i=1}^{24} x_i \right) $$ This leads to $$ \left( \sum_{i=1}^{24} \sqrt{x_i}\right) \leqslant  \sqrt{24} $$ I am stuck with other part. I can get the minimum of the following using similar technique. $$ \left(\sum_{i=1}^{24} \frac{1}{\sqrt{1+x_i} } \right) $$ But I need to have maximum of this quantity, so that I can combine the two. Any hints will help.","['maxima-minima', 'algebra-precalculus', 'cauchy-schwarz-inequality']"
3794171,Show that $\angle BOC=\angle AOD$.,"Let $E$ and $F$ be the intersections of opposite sides of a convex quadrilateral $ABCD$ . The two diagonals meet at $P$ . Let $O$ be the foot of the perpendicular from $P$ to $EF$ . Show that $\angle BOC=\angle AOD$ . Here's the diagram: I defined $X=OD\cap EP, Y=EP\cap FC,Z=FP\cap EB,W=FP\cap EC $ . Now, by a known lemma , we have $(Y,X;P,E)=-1$ and by apollonius lemma , we get $PO$ bisects $\angle XOY \implies \angle XOP =\angle POY $ . Similarly, we know that $(F,P;Z,W)=-1 \implies PO$ bisects $\angle ZOW \implies \angle ZOP =\angle WOP$ . But this angle equalities lead me no where.Can someone give some hints ? Thanks in advance !","['euclidean-geometry', 'geometry']"
3794174,Greatest integer less than or equal to $\sum_{n=1}^{9999}\frac{1}{n^{1/4}}$,This is a PhD entrance question of TIFR 2020. The question requires the explicit answer. I know that the partial sums are evaluated using Abel's formula in Number Theory but I believe there may be better methods for series of this form. Can anyone share their thoughts?,"['contest-math', 'number-theory', 'calculus', 'real-analysis']"
3794176,Degree of a field extension by a transcendental element,"Let $F$ be a field, and let $F(x)$ be the field of fractions of the polynomial ring $F[x]$ . I'm interested in the degree of the field extension $[F(x) : F]$ . Obviously it is infinite, but what exactly is its cardinality? Is it $\aleph_0$ ? Does it depend on the field $F$ ?","['cardinals', 'field-theory', 'abstract-algebra', 'transcendental-numbers', 'extension-field']"
3794178,"Prove that $\omega(v,u)=0$ $\forall u \in \mathbb R^m$.","if $\omega:\mathbb R^m\times \mathbb R^m\to \mathbb R$ one alternant 2-tensor. Prove that if $m$ is odd then exist $v\in \mathbb R^m$ such that $\omega(v,u)=0$ $\forall u\in \mathbb R^m$ .
I dont have idea how to start just use the definition,hence $\omega$ is alternant 2-tensor then $\omega(x,y)=-\omega(y,x)$ . So, for example $\omega(x,x)=0\quad\forall x\in \mathbb R^m$ , if i suppose that exist $\tilde{v}$ then $\omega(\tilde{v},u)=\omega(\tilde{v},u)+\omega(u,u)$ imply $\omega(\tilde{v}+u,u)=\omega(\tilde{v},u)=-\omega(u,\tilde{v})$ . But what can i do with all equalitys, how to use that $\mathbb R^m$ where $m$ is odd, somebody can help me please, thank you.","['multivariable-calculus', 'tensors']"
3794198,Show that $S^2$ is a biased estimator of $\sigma^2$,How did they get from equation (3) to equation (4)? $$S^2 = \frac{1}{n} \sum (X_i - \bar{X})^2 \tag{0}$$ $$E[S^2] = E\Big[\frac{1}{n} \sum (X_i - \bar{X})^2 \Big]\tag{1}$$ $$E[S^2] = E\Bigg[\frac{1}{n} \sum \limits_{i=1}^{n}\Big[~[(X_i - \mu)-(\bar{X}-\mu)]^2~\Bigg]\tag{2}$$ $$E[S^2] = \Bigg[ \frac{1}{n} \sum \limits_{i=1}^{n} \Big[~(X_i-\mu)^2-2(X_i-\mu)(\bar{X}-\mu)+(\bar{X}-\mu)^2~\Big] ~\Bigg]\tag{3}$$ $$E[S^2] = E\Bigg[~\frac{1}{n} \Big[~\sum \limits_{i=1}^{n} (X_i - \mu)^2 - n(\bar{X} - \mu)^2 \Big]~\Bigg]\tag{4}$$ $$E[S^2] = \frac{1}{n} \sum \limits_{i=1}^{n} E[X_i-\mu)^2] - E[(\bar{X}-\mu)^2]\tag{5}$$ $$E[S^2] = \sigma^2 - \sigma_X^2\tag{6}$$ $$E[S^2] = \sigma^2 - \frac{1}{n}\sigma^2\tag{7}$$ $$E[S^2] = \frac{n-1}{n}\sigma^2\tag{8}$$ Equation (8) shows that $S^2$ is a biased estimator of $\sigma^2$,['statistics']
3794228,"Is there a bijective function $f:[0,1] \to [0,1]$ such that the graph of $f$ in $\mathbb{R}^2$ is a dense subset of $[0,1] \times [0,1]$?","Is there a bijective function $f:[0,1] \to [0,1]$ such that the graph of $f$ in $\mathbb{R}^2$ is a dense subset of $[0,1] \times [0,1]$ ? (Exact same as title). I think the question is not affected much if we ask the same question but for a function $f:(0,1) \to [0,1]$ or $f:[0,1) \to (0,1]$ etc, as opposed to $f:[0,1] \to [0,1]$ , which was in the original question. All that really matters is that the domain and range are bounded, connected subsets of $\mathbb{R}^2$ . I suspect the answer to the question is yes, but I don't know how to construct such a function. The first thing to note is that, if such a function exists it must be nowhere continuous, else the graph of f would not be dense throughout all of $[0,1] \times [0,1]$ . However, it is not clear if the graph of our function would be a totally disconnected subset of $[0,1] \times [0,1]$ . Can a nowhere continuous function have a connected graph? I have actually not read the answers to the above question in any detail, and anyway, it may not be relevant to answer the question here (although it might). My attempt: Let $f_{ Conway_{(0,1)} }:(0,1) \to \mathbb{R} $ be the Conway base-13 function , but with domain restricted to $(0,1)$ . Now define $f_{Conway_{(0,1)}bounded}(x) = \frac{1}{\pi} \arctan(f_{ Conway_{(0,1)} }(x)) + \frac{1}{2}$ with domain $(0,1)$ and range $(0,1)$ . Then the function is well-defined, and the graph of $f_{Conway_{(0,1)}bounded}:(0,1) \to (0,1)$ is a dense subset of $[0,1] \times [0,1]$ . Now we can easily modify our function $f_{Conway_{(0,1)}bounded}$ so that it has domain $[0,1]$ and range $[0,1]$ , and I will assume the reader can do this and leave the details for brevity. But the point is, these missing two points in the domain, $0$ and $1$ , are not a problem. The problem is that our function is not injective. Note that we cannot answer the question by only removing points from the graph of $f_{Conway_{(0,1)}bounded}$ , for then you would be removing lots of points from the domain, and so this would not be a function with domain $(0,1)$ . So maybe doing something clever to $f_{Conway_{(0,1)}bounded}$ , or perhaps coming up with an entirely different way to construct a function to answer the question is necessary.","['general-topology', 'real-analysis']"
3794325,Finding $\lim_{x \to \infty} (x + \frac{2x^{3}}{3} - \frac{2(x^2+1)^{\frac{3}{2}}}{3})$,$\lim_{x \to \infty} (x + \frac{2x^{3}}{3} - \frac{2(x^2+1)^{\frac{3}{2}}}{3})$ this limit  according to wolframalpha is equal to $0$ . So this is my work thus far $\lim_{x \to \infty} (x + \frac{2x^{3}}{3} - \frac{2(x^2+1)^{\frac{3}{2}}}{3})$ output is $\infty - \infty$ which is indeterminate form. So next I basically but it on the same denominator: $\frac{1}{3}$ $((3x + 2x^3 - 2(x^2+1)^{\frac{3}{2}})$ and turned $2(x^2+1)^{\frac{3}{2}}$ into something easier to work with $2\sqrt{x^2+1}+2x^{2}\sqrt{x^2+1}$ now the limit is $\frac{1}{3} \lim_{x \to \infty} ((3x + 2x^3-2\sqrt{x^2+1} -2x^{2}\sqrt{x^2+1})$ and this is where I am stuck to do next and lost.,"['limits', 'calculus', 'limits-without-lhopital']"
3794380,Show that an increasing function has derivative $0$ a.e.,"Let $0<p<1$ and define $F:[0,1]\rightarrow[0,1]$ by $$F(x)=\begin{cases}
pF(2x),&x\in\left[0,\frac12\right]\\
p+qF(2x-1),&x\in\left[\frac12,1\right]
\end{cases}$$ where $q=1-p$ .  I would like to prove that $F'(x)=0$ a.e. I am working my way through ""How to Gamble If You Must"" by Kyle Siegerst, which is basically a series of exercises. $F(x)$ is the probability that a gambler starting with a bankroll $0\leq x\leq 1$ will reach his target of $1$ if he engages in ""bold play"" in the game of red and black.  When his bankroll is $\leq\frac12$ he bets it all, winning the amount bet with probability $p$ , and losing it with probability $q$ .  When his bankroll is $>\frac12$ , he bets just enough to reach the target, that is, $1-x$ . In the exercises, I have shown that there is a unique function $F$ satisfying the functional equation above, and that it is continuous and strictly increasing.  Following exercise $33$ , the author remarks that when $p\neq\frac12$ , $F'(X)=0$ a.e., so that $F$ is a devil's staircase.  I have been trying to prove this statement.  (I know that an increasing function is differentiable a.e.  It's the value that I'm having trouble with.) Vague $50$ -year-old memories of measure theory have led me to Proposition 3.31 in Folland's ""Real Analysis"", to wit If $F\in NBV, \text{ then }F\in L^1(m).$ Moreover, $\mu_F\perp m \text{ iff } F' =0$ a.e., and $\mu_F \ll m \text{ iff } F(x)=\int_{-\infty}^xF'(t)dt. $ Here $m$ is Lebesgue measure, and a.e. is with respect to Lebesgue measure. $\mu_F$ is the Borel measure defined by $\mu_F([a,b])=F(b)-F(a)$ .  Folland uses $NBV$ to mean that $F$ is of bounded variation, $F(-\infty)=0$ and $F$ is right continuous.  This is no problem, as we can extend $F$ to $\mathbb{R}$ by defining $F(x)=0$ for $x<0$ and $F(x)=1$ for $x>1$ . So it seems to come down to showing $\mu_F\perp m$ .  This means that there is an $E\subset[0,1]$ with $m(E)=0$ and $\mu_F(E)=1$ if I'm not mistaken.  I don't see how to prove this.  Indeed it doesn't seem at all likely to me, so I must misunderstand something. In exercise 29, I proved that $$F(x)=\sum_{n=1}^\infty p_{x_1}\cdots p_{x_{n-1}}px_n$$ where $x_i$ is bit number $i$ of $x$ , and $p_0=p,\ p_1=q$ .  (When $x$ is a dyadic rational, we take the terminating representation.)   If we represent wins by $1$ and losses by $0$ , this means that the gambler reaches the goal if and only if the first time a bit in his bankroll matches the corresponding game bit, those bits are both $1$ .  This is the most concrete representation of $F$ in the paper, but I don't see how it helps. Can you cast any light on this for me?","['measure-theory', 'probability', 'radon-nikodym']"
3794418,Difficulty in understanding significance of Grelling's Paradox.,"Background: I'm a math rookie, yet to enrol in university. I randomly started reading Mendelson's Introduction to Mathematical Logic , when I stumbled upon this paradox in the introductory section: Grelling's Paradox: An adjective is called autological if the property denoted by the adjective holds for the adjective itself. An adjective is called heterological if the property denoted by the adjective does not apply to the adjective itself. For example, 'polysyllabic' and 'English' are autological, whereas 'monosyllabic' and 'French' are heterological. Consider the adjective 'heterological'. If 'heterological' is heterological, then it is not heterological. If 'heterological' is not heterological, then it is heterological. In either case, heterological is both heterological and not heterological. I'd like to understand the following: What is the source of logical fallacy in this paradox? If I formulate a set $A$ of all adjectives and subsets $A_a$ and $A_h$ corresponding to autological and heterological adjectives, respectively, then it could be the case that $\text{(heterological)}\in A-(A_a\cup A_h)$ , i.e., it belongs to neither of the two sets(unless $A_a\cap A_h=\emptyset$ and $A_a\cup A_h=A$ ). On a lighter note, I'd like to know about the mathematical significance of this paradox, and how it's dealt with in modern set theories. Although I understand the answer(s) could be very abstract, please add a simpler analogy along with a necessary technical explanation, if possible.","['elementary-set-theory', 'logic']"
3794424,$L^1$ convergence for convex combinations of non-negative random variables,"I want to solve the following problem but I can't put all the tools together properly: My attempt: We consider the sets $$A_n \equiv \{E\left(1-\exp(-g) \right) : g \in \text{conv}(f_j : j \ge n) \}$$ and define $s_n \equiv \sup(A_n)$ .  Noting that $g \ge 0 \quad \forall g \in \text{conv}(f_j : j \ge n)$ , $1-\exp(-g) \in [0,1]$ so that $\{s_n\}_{n \in \mathbb{N}}$ is a bounded, decreasing sequence (as $A_{n+1} \subseteq A_n$ ) and so has a limit $s = \inf_n s_n$ . By definition of the supremum $s_n$ , there exist $g_n \in \text{conv}(f_j : j \ge n)$ such that $$s_n - \frac{1}{n} \leq E(1- \exp(-g_n)) \leq s_n $$ The idea is to show that $g_n$ converges in $L^1$ , which would be shown if it were Cauchy in $L^1$ , or even if it converged in probability (since $|g_n| \leq K$ , we have a uniformly integrable family so that convergence in probability implies $L^1$ convergence). I do not know how to do this.  Any ideas? Some of my (maybe useless ones): We clearly have $E( 1- \exp(-g_n)) \rightarrow s$ , and if we could show that $1-\exp(-g_n)$ is Cauchy in measure (or $L^1$ ) then we would have $1-\exp(-g_{n})$ converges to a limit $X$ in probability, which would imply that $g_{n}$ converges to $-\log(1-X)$ in probability by the continuous mapping theorem and then we would be done.","['convergence-divergence', 'lebesgue-integral', 'probability-theory']"
3794442,Dirichlet problem in unbounded domain,"In bounded domains of $\mathbb{R}^2$ , the Dirichlet problem has a unique solution:
the equation $\triangle u=0$ with prescribed boundary value has a unique solution. This is not true if the domain is unbounded. Is there any result for existence and uniqueness for unbounded domains under further assumptions on the domain and the function? If there is a reference where this question is treated it would be of help.","['harmonic-functions', 'elliptic-equations', 'reference-request', 'complex-analysis', 'potential-theory']"
3794444,"Repeated roots of $x^3 + xy + z = 0$ for fixed values $(y,z)$. (Reid, Undergraduate Algebraic Geometry, Exercise 0.1)","Show that for fixed values of $(y,z)$ , $x$ is a repeated root of $x^3 + xy + z = 0$ if and only if $x = \frac{-3z}{2y}$ and $4y^3 + 27z^3 = 0$ . I know that the discriminant of this cubic is $-4y^3 - 27z^2$ , and thus, our polynomial only has a repeated root iff $4y^3 + 27z^3 = 0$ . However, I'm not sure how to approach the other part of the problem. I suppose one could use the analog of the quadratic formula for cubics, but I was hoping to find a ""cleaner"" approach or one that is more helpful while continuing through the text. I would also appreciate any insight of different methods for solving like problems.","['algebraic-geometry', 'abstract-algebra']"
3794445,Ramanujan Identity related to JacobiFunction [duplicate],"This question already has answers here : Proof of a Ramanujan Integral (4 answers) Closed 3 years ago . The following identity is allegedly due to Ramanujan $$\int_0^\infty \frac{{\rm d}x}{(1+x^2)(1+r^2x^2)(1+r^4x^2)\cdots} = \frac{\pi/2}{\sum_{n=0}^\infty r^{\frac{n(n+1)}{2}}} \, $$ but how do you prove this? The denominator of the right side is related to the Jacobi Function, so maybe one could proceed via modular forms?","['integration', 'modular-function']"
3794458,Proof of the spectral decomposition theorem for normal operators on a finite-dimensional vector space,"So, I was reading the book Nielsen and Chuang and it's introductory chapter on Quantum Mechanics and it had a theorem called the ""Spectral Decomposition Theorem"" which states that an Operator $M$ is a normal operator if and only if it can be diagonalized in the orthonormal basis (which turn out to be the eigen-vectors). Now, I have some trouble understanding the forward proof, which was to prove that if I have a Normal operator, it can be diagonalized. I am attaching the proof that is written in the book. Ultimately, it uses Projectors onto eigenspaces of a particular eigen value $P_\lambda=\sum_i |\lambda ; i\rangle \langle\lambda;i |$ and it's Orthogonal complement $Q_\lambda \equiv I-P_\lambda$ and rewrite $M=(P_\lambda+Q_\lambda)M(P_\lambda+Q_\lambda)$ which then simplifies to $M=P_\lambda MP_\lambda +Q_\lambda M Q_\lambda$ and using principle of mathematical induction, one can indeed prove that M can be diagonalized with respect to some orthonormal basis. The part of the proof that I don't understand is that somehow, it implies that the operator $M$ can be diagonalized with respect to eigenvectors of this Normal Operator and that they are necessarily orthogonal. Also, if I try to write down $M$ in outer-product representation, it somehow simplifies as $M=\sum_i \lambda_i |e_i\rangle\langle e_i|$ where $\lambda_i$ 's are the eigen-values and $|e_i\rangle$ 's are the eigenvectors. I have literally no idea how this decomposition was implied by the above arguments. Any sort of help in the understanding of this is appreciated. P.S.- I would really appreciate if you could explain the arguments with respect to the way that it is given in the proof in this book.","['quantum-mechanics', 'linear-algebra']"
3794478,Derivatives of Polygamma Functions,"I would like to know if there's a quicker way to verify: $$\partial_z^{n-1}\psi(tz)t^n = \partial_z^n\ln[\Gamma(tz)], \,\,\\ n \in \mathbb{N}^+, t \in \mathbb{C}_+\tag{1}\label{1}$$ That's true for $t=1$ . But what about a complex $t$ ? For complex $t$ , $(1)$ holds for $n=1,2,3$ . These computations were done by hand. I searched any information that would help me on DLMF ( https://dlmf.nist.gov/5.15 ), but nothing relevant was found. Thanks","['multivariable-calculus', 'polygamma', 'special-functions', 'digamma-function']"
3794496,What is the relation between Lipschitz and $BMO$ spaces?,"Let $0 < \alpha < 1$ . The space $\text{Lip}_{\alpha}(\mathbb{R}^n)$ of the Lipschitz continuous functions with order $\alpha$ is $$
\text{Lip}_{\alpha}(\mathbb{R}^n)=\{f: |f(x)-f(y)|\le C|x-y|^{\alpha} \quad\text{  for a.e. }\,x, y \in \mathbb{R}^n\}.
$$ The smallest such constant $C$ is called the $\text{Lip}_{\alpha}(\mathbb{R}^n)$ norm of $f$ and is denoted by $\|f\|_{\text{Lip}_{\alpha}(\mathbb{R}^n)}$ . This norm has the following integral representation: $$
\|f\|_{\text{Lip}_{\alpha}(\mathbb{R}^n)}\thickapprox\sup_{B}\frac{1}{|B|^{1+\alpha/n}}\int_{B}|f(x)-f_{B}|dx,
$$ where $f_{B}=\frac{1}{|B|}\int_{B}f(y)dy$ . It is clear that for $\alpha=0$ the right hand side is the definition of $BMO$ norm of $f$ . According to these facts can we say that Lipschitz space includes $BMO$ space? What is the relation between Lipschitz and $BMO$ spaces?","['harmonic-analysis', 'functional-analysis', 'real-analysis']"
3794507,"difference between $S^2$, $\sigma_x^2$. and $\sigma^2$?","On $(5),$ $(6),$ and $(7),$ what's the difference between $S^2$ and $\sigma_x^2$ ? Also, why does: $$\sigma_X^2 = \sum \limits_{i=1}^{n} \frac{1}{n^2} \sigma^2 = \frac{\sigma^2}{n}$$ ? I'm assuming $\sigma^2$ is the population variance. It seems like S is a random variable since I can take the expectation of it, but, $\sigma_x$ is the same thing except not a random variable? Let $(X_1, \cdots, X_n)$ be a random sample of $X$ having unknown mean $\mu$ , and variance $\sigma_x^2$ \begin{align}
S^2 &= \frac{1}{n} \sum (X_i - \bar{X})^2 \tag{0}\\[4ex]
E[S^2] &= E\Big[\frac{1}{n} \sum (X_i - \bar{X})^2 \Big]\tag{1}\\[2ex]
&= E\Bigg[\frac{1}{n} \sum \limits_{i=1}^{n}\Big[~[(X_i - \mu)-(\bar{X}-\mu)]^2~\Bigg]\tag{2}\\[2ex]
&= E\Bigg[ \frac{1}{n} \sum \limits_{i=1}^{n} \Big[~(X_i-\mu)^2-2(X_i-\mu)(\bar{X}-\mu)+(\bar{X}-\mu)^2~\Big] ~\Bigg]\tag{3}\\[2ex]
&= E\Bigg[~\frac{1}{n} \Big[~\sum \limits_{i=1}^{n} (X_i - \mu)^2 - n(\bar{X} - \mu)^2 \Big]~\Bigg]\tag{4}\\[2ex]
&= \frac{1}{n} \sum \limits_{i=1}^{n} E\big[(X_i-\mu)^2\big] - E\big[(\bar{X}-\mu)^2\big]\tag{5}\\[2ex]
&= \sigma^2 - \sigma_X^2\tag{6}\\[2ex]
&= \sigma^2 - \frac{1}{n}\sigma^2\tag{7}\\[2ex]
&= \frac{n-1}{n}\sigma^2\tag{8}
\end{align} Equation (8) shows that $S^2$ is a biased estimator of $\sigma^2$","['expected-value', 'statistics', 'standard-deviation', 'sampling']"
3794526,A simple geometric proof of Zariski's connectedness theorem?,"By Zariski's connectedness theorem , we will mean the following: Let $f : X \to Y$ be a proper map between irreducible normal varieties (over $\mathbb{C}$ ) with $f_{\ast} \mathcal{O}_X = \mathcal{O}_Y$ . Then $f$ is surjective and the fibers of $f$ are connected. A proof is given here The proof seems to use quite heavy machinery and is not very enlightening from a geometric point of view. Is there a simple geometric proof that the fibers of an algebraic fiber space are connected? Lazarsfeld's first book gives the following proof: Any projective morphism $g : V \to W$ of irreducible varieties admits a Stein factorization, i.e., $g = a \circ b$ , where $a$ is an algebraic fiber space and $b$ is a finite map. In particular, $g$ is a fiber space if and only if the finite part of its Stein factorization is trivial. In particular, all the fibers are connected. The implication of not being finite $\implies$ fibers are connected is not clear to me.","['complex-geometry', 'algebraic-geometry']"
3794543,Is there an efficient way of showing $\int_{-1}^{1} \ln\left(\frac{2(1+\sqrt{1-x^2})}{1+x^2}\right)dx = 2$?,"Hi I recently came across the following integral: $$
\int_{-1}^{1} \ln\left(\frac{2(1+\sqrt{1-x^2})}{1+x^2}\right)dx
$$ When an integral calculator finds the antiderivative of the equation (if it even can) it comes out as this crazy formula. Anyways the definite integral happens to be equal to 2 and I was wondering if there might be an elegant way of showing that this is the case. Thank you.","['integration', 'calculus', 'definite-integrals']"
3794553,Coefficient of $x^7y^6$ in $(xy+x+3y+3)^8$,"Find the coefficient of $x^7y^6$ in $(xy+x+3y+3)^8$ . My solution: Factor $(xy+x+3y+3)^8$ into $(x+3)^8(y+1)^8$ . To get an $x^7y^6$ term, we need to find the coefficient of $x^7$ in the first factor and $y^6$ in the second factor. Using the binomial theorem, we get the coefficient of $x^7$ to be $17496$ and $y^6$ to be $28$ . Multiplying the two gets us an answer of $489888.$ However, this is wrong. This is the answer key's approach: $x^7y^6 = (xy)^6 \cdot x = (xy)^5 \cdot x^2 \cdot y$ . Now, $(xy)^6\cdot x$ can be formed by choosing $6$ $xy$ 's, $1$ $x$ , and $1$ $3$ , which can be done in $\binom{8}{6}\binom{3}{2}\binom{1}{1} = 56$ ways. $(xy)^5\cdot x^2\cdot y$ can be formed by choosing $5$ $xy$ 's, $2$ $x$ 's, and $1$ $3y$ , which can be done in $\binom{8}{5}\binom{3}{2}\binom{1}{1} = 168$ ways. Thus the final coefficient is $3(56+168) = 672$ ways. I completely understand their approach, but fail to understand why mine doesn't work. Don't we just calculate the number of ways to get $x^7$ , and $y^6$ , then multiply them? Interestingly enough, I noticed that when you calculate the coefficient of $x^1$ (which is $x^{8-7}$ ) and $y^2$ (which is $y^{8-6}$ ), you get $3\cdot \binom{8}{1} \cdot \binom{8}{2} = 672$ , which is the answer. I'm $99\%$ sure this isn't a coincidence, but why does this method work and not the other? I know that I did not get any calculations wrong, because I double-checked everything with WolframAlpha; the error must be in my process. Thanks in advance! (Question from PuMaC 2017 Algebra B)","['algebra-precalculus', 'binomial-coefficients', 'binomial-theorem']"
3794556,probability group students to maximize expectation,"Given $3n$ people that the $i^{\text{th}}$ person can pass a test with probability $p_i$ , now you are required to divide them to $n$ groups that each group has $3$ people. The score of one group equals $1$ if at least two people pass the test, $0$ otherwise. In order to maximize the expectation of total score, how do you group them? I've thought about this problem for a bit, and I think intuitively it makes sense to group two large $p_i$ with a small $p_i$ . Also, I've thought about in the optimal arrangement, swapping any two $p_i$ from different groups should lower the expectation. I can write out mathematically the difference in expectation when swapping two of the students, but it doesn't seem to give any obvious result. I've hit a wall.","['optimization', 'probability', 'algorithms']"
3794565,Expected value of game when flipping a coin,"You start off with \$ $10,000$ . You flip a fair coin. If you get heads, you get paid \$ $1$ . If you get tails, you pay your friend half your current money. What is the expected amount of money you have after $n$ rounds ?. So let's define the initial amount as $x_{0} = 10000$ . Then in round 1, we expect $$
x_{1} = {1 \over 2}\left({x \over 2} + x + 1\right)
$$ Note in round $1$ , we could have $2$ possible values for $x_{1}$ . $5000$ and $10,001$ . So in round $2$ , we could have $4$ possible values. So clearly at round $n$ , we have $2^{n}$ possible values, all with equal probability. Now this is one of the parts that I think I'm doing correctly, but don't know how to justify. To simplify things, I am claiming that instead of having $2^{n}$ possible values in round $n$ , we have a single value, which is the average of the $2^{n}$ values. So, for example, I can collapse the $2$ values for round $1$ into $\left(10001 + 5000\right)/2 = 7500.5$ . Then by doing so, it becomes clear that our recursion is $$
x_{n} = {1 \over 2}\left({x_{n - 1} \over 2} + x_{n - 1} + 1\right)
$$ My first question is: how can I justify the ""collapsing"" ?.
If you write out a few terms, you'll see that $$
x_{n} = 0.75^{n}\, x_{0} + \sum_{i = 0}^{n - 1}0.75^{i} \times 0.5
$$ My second question is, am I done here, or do I need to prove that the simplified $x_{n}$ that depends only on $x_{0}$ holds by induction ?. I found this formula by writing out a few items, and eyeballing/inducing things because there's a very clear pattern, so I feel like that's enough proof ?.","['expected-value', 'induction', 'probability', 'gambling']"
3794603,Counting the number of decimals that satisfy a condition,"This is supposedly a problem from a $\textbf{Chinese Math Olympiad team selection test}$ but it looks like an interesting combination of combinoatorics and number theory. $\textbf{Problem:}$ We have two positive decimal fractional numbers $X=0.x_1 x_2 ... x_k$ and $Y=0.y_1 y_2 ... y_k$ . here $x_k$ and $y_k$ can be zero. Let $S$ be the number of decimals $0.z_1 z_2 ... z_k$ that satisfy $0.z_1 z_2 ... z_k < X$ and $0.z_k z_{k-1} ...z_1 < Y$ (and similarly $z_k$ and $z_1$ can both be zero), and we view $0.100$ and $0.1$ as the same number. Prove that $|S - 10^k XY | \leq 9k$ So obviously $10^k$ is the number of decimal $0.z_1 z_2 ... z_k$ . What's really awkward is the multiplication of $XY$ . I cannot come up with a combinatoric explanation for $XY$ to convert this into an enumeration problem","['contest-math', 'number-theory', 'combinatorics', 'elementary-number-theory']"
3794614,Why do we relate functions and its derivatives in Diff Eq?,"I understand that a differential equation possesses both a function and its derivative. I also understand that we solve for an unknown function. But why do we put them in one long equation? My basic understanding is that we do it to explain complex ""things"" that change over time. However, I cannot seem to understand or even phrase it better than that.","['differential', 'derivatives', 'ordinary-differential-equations']"
3794645,Statistics problem about Possibility principle,"I have a question about the following problem from the first edition of Introduction to Probability by Joseph K. Blitzstein and Jessica Hwang. This is problem 68 from Chapter 4. Each of 111 people names his or her 5 favorite movies out of a list of
11 movies. Show that there are 2 movies such that at least 21 of the people name
both of these movies as favorites. The possibility principle states that The possibility principle: Let A be the event that a randomly chosen
object in a collection has a certain property. If P(A) > 0, then there
exists an object with the property. Is it enough to state that the probability of two specific movies being chosen by at least 21 people is ${111\choose 21}{(2/11)}^{21} $ (where 2/11 is the probability of the two movies being chosen by a specific person) and therefore since this probability is greater than 0 then by the possibility principle there exist two  movies that are chosen by at least 21 people? But then it could also be said that there are 5 movies that are chosen by all 111 people.","['expected-value', 'random', 'statistics', 'probability']"
3794651,Commuting of derivation on localization and canonical map,"This problem is from van der Put's ""Galois Theory of Linear Differential Equations"". Show that there exists a unique derivation $\partial$ on $RS^{-1}$ (the localization of $R$ with respect to $S$ ) such that the canonical map for $R \rightarrow RS^{-1}$ commutes with $\partial$ where $R$ is a commutative ring and $S \subset R$ is a multiplicative subset. Here is my attempt with my rough understanding of the concepts in this problem. Let $\phi : R \rightarrow RS^{-1}$ be the canonical map. We want to show that $\partial (\phi (x)) = \phi (\partial (x))$ for $x \in R$ . Observe that by definition, $\phi (\partial (x))$ maps $x \mapsto [\partial (x)],$ the equivalence class of $\partial (x)$ in $RS^{-1}$ (this IS how the canonical map works, right?). On the other hand, $\partial (\phi (x))$ maps $x \mapsto \partial ([x]) = [\partial (x)],$ which is the same equivalence class in $RS^{-1}$ as mapped to by $\phi (\partial (x)).$ Thus, we conclude that $\phi$ and $\partial$ commute. But I'm not sure how to show that there is a unique $\partial$ that satisfies this problem. Could someone please help? On a tangent: Consider the polynomial ring $R[X_1, X_2, \dots ,X_n ]$ and a multiplicative subset $S \subset R[X_1, X_2, \dots ,X_n ]$ . Let $a_1, a_2, \dots , a_n \in R[X_1, X_2, \dots ,X_n ]S^{-1}$ be given. Prove that there exists a unique derivation $\partial $ on $R[X_1, X_2, \dots X_n] S^{-1}$ such that the canonical map $R \rightarrow R[X_1, X_2, \dots ,X_n ] S^{-1}$ commutes with $\partial$ and $\partial (X_i) = a_i$ for all $i$ . (Is the assumption $\mathbb{Q} \subset R$ useful at all?)","['ring-theory', 'abstract-algebra', 'algebraic-number-theory']"
3794653,"If there exists a bijection $\varphi$ from $A$ to $B$, to what extent are the two sets interchangeable?","I am seeking understanding of part of Proposition 6.7 from page 48 of Analysis I by Amann and Escher. Quote: Comments and questions: The part that I don't follow is how we can assume ""without loss of generality"" that $X$ is the natural numbers and $A$ is an infinite subset thereof. I am somewhat familiar with the idea that a (group) isomorphism, as special type of bijective function between groups, ""preserves structure"" in the sense that if you are unconcerned with the specific details of each group (such as the names of the elements), then the groups can be considered interchangeable. That is what the text reminds me of. Is it that, whatever the names of the elements of $X$ and $A$ , since they can be put into bijection with $\mathbb N$ and an infinite subset thereof respectively, we could assign an arbitrary order to the elements of $X$ , and then we might as well be working with $\mathbb N$ ? I appreciate any help.","['elementary-set-theory', 'proof-explanation', 'functions']"
3794665,What can we say about this function when it approaches zero from above?,"Define the function $f:(0,\infty)\rightarrow \mathbb{R}$ by $$f(x)=\sum\limits_{n=1}^\infty\frac{1}{2^n}\cdot \frac{1}{\left(\frac{1}{n}-x\right)^2}I_{\{x: x \ne \frac{1}{n}\}}(x)$$ . What can we say about this function as it approaches zero? I think it is pretty clear that $\limsup\limits_{x \rightarrow 0^+}f(x)=\infty$ , this is because no matter how close we are to zero, we can choose an $n^*$ closer to zero, and then approach it. But what about $\liminf\limits_{x \rightarrow 0^+}f(x)$ ?, is it possible to see what this is? As we get closer to zero, the tail in the sum has elements of $(1/n-x)^2$ that becomes very small, but $2^n$ becomes very big, so they might cancel them out?","['limits', 'calculus', 'limsup-and-liminf']"
3794692,Contiguous hypergeometric function question,"I have come upon the hypergeometric function $$_2F_1\left(k+\frac{1}{2},k+\frac{1}{2};\frac{3}{2},z\right)$$ where $k \geq 1$ is an integer, and I believe that this is equal to $$\frac{p(z)}{(1-z)^{(4k-1)/2}}$$ where $p$ is a polynomial of degree $k-1$ (Wolframalpha confirms the first few values). I understand that this must follow from some relationship involving contiguous hypergeometric functions, but I don't know how, and don't have a good reference (library at my uni is closed for COVID-19). I actually don't care about the coefficients in the polynomial, because I'm just trying to show an integral is finite. Is anyone able to put me on the right track? Many thanks,
Greg","['special-functions', 'analysis', 'hypergeometric-function']"
3794702,Can we use Bayes' Theorem on CDFs,"I'm learning probability on my own, so I'd like to know when I can use Bayes' Rule. From my understanding it's only valid if our probability axioms hold: $P(x) \geq 0$ $P(S) = 1$ where S is the entire sample space $P(A \cup B) = P(A) + P(B)$ if A and B are disjoint events. So the first axiom and the last one are definitely true for CDFs and PDFs. But the second one is a little harder to prove. I guess my question really boils down to can I use Bayes' Theorem for CDFs and PDFs.","['self-learning', 'bayes-theorem', 'probability-theory', 'probability']"
3794709,"Closed form of hypergeometric $\, _4F_3\left(\frac{3}{8},\frac{5}{8},\frac{7}{8},\frac{9}{8};\frac{5}{6},\frac{7}{6},\frac{9}{6};z\right)$","Let $f(z)=\sqrt[3]{\sqrt{z^2-z^3}+z}$ , $g(z)=\sqrt{\frac{f(z)}{z}+\frac{1}{f(z)}}$ , then how to prove that for $\ 0<z<1$ : $$\small \, _4F_3\left(\frac{3}{8},\frac{5}{8},\frac{7}{8},\frac{9}{8};\frac{5}{6},\frac{7}{6},\frac{9}{6};z\right)=\frac{4 \sqrt[4]{2} }{3 \sqrt{3}}\left(g(z)-\sqrt{-\frac{f(z)}{z}+\frac{2 \sqrt{2}}{z g(z)}-\frac{1}{f(z)}}\right)^{3/2}$$ This formula is collected from Wolfram Functions site but no proof is offered there. I think it will be worthy to find a rigorous proof, as well as to investigate the motivation of this formula in order to generate similar algebraic closed-forms of generalised hypergeometric series (very likely, their $_2F_1$ counterparts are those in Schwarz's list). Any help will be appreciated!","['integration', 'closed-form', 'sequences-and-series', 'radicals', 'hypergeometric-function']"
3794723,Differential inequality regarding volume comparison,"Let $(M,g)$ be a complete $n$ -dimensional Riemannian manifold and let $p \in M$ . Consider $(t,\Theta)$ , the geodesic spherical coordinates around $p$ , where $t \in (0,\text{conj}_p(\Theta))$ and $\Theta$ is a unit vector in $T_pM$ . Let $A_p(t,\Theta)$ be the density of the volume measure in these coordinates, i.e. \begin{equation*}
     d\operatorname{Vol} = A_p(t,\Theta) dt  d\Theta
\end{equation*} A well-known theorem of Gromov states that if $\operatorname{Ric}(M) \geqslant (n-1)\kappa$ ,  then the map \begin{equation}
    t \mapsto \frac{{A}_p(t,\Theta)}{sn^{n-1}_{\kappa}(t)}
\end{equation} is non-increasing in $t$ . As usual, $sn_{\kappa}$ is given by \begin{align*}
 sn_{\kappa}(t) = \begin{cases} 
\frac{\sin{\sqrt{k}t}}{\sqrt{k}} & k > 0\\
t & k = 0\\
\frac{\sinh{\sqrt{-k}t}}{\sqrt{-k}} & k < 0
\end{cases}
\end{align*} Now I would like to prove a similar result when the sectional curvature of $M$ is bounded from above. That is, if $ \text{sec}(M) \leqslant \kappa$ , then \begin{equation*}
     \frac{d^2}{dt^2}\left(\frac{A_p(t,\Theta)}{sn^{n-2}_{\kappa}(t)}\right) + \kappa \left(\frac{A_p(t,\Theta)}{sn^{n-2}_{\kappa}(t)}\right) \geqslant 0
\end{equation*} I'm trying to mimic the argument given by Gromov, letting $\varphi(t) = A_p(t,\Theta)^{\frac{1}{n-2}}$ and calculate that $(\log \varphi(t))' = \frac{1}{n-2}\text{tr}(\text{II}(t))$ , where $\text{II}(t)$ is the second fundamental form of $\partial B(p,t)$ . But since we are not proving a statement about monotonicity, I don't know how I can get rid of the power $(n-2)$ . Differentiating such expression directly seems intimidating and tedious, and I believe there's a shortcut to the problem since it is very similar to the estimate of the norm of Jacobi fields. Any insight of the problem will be appreciated.","['riemannian-geometry', 'differential-geometry']"
3794739,What does the $f^!$ do for line bundles on a curve?,"I'm trying to understand what does inverse exceptional image $f^!$ of a coherent sheaf look like. Let's say for the sake of this question that $f:X\rightarrow Y$ is a finite flat morphism between schemes. In this case, my understanding is that $f^!$ is a functor from coherent sheaves on $Y$ to coherent sheaves on $X$ , and that if $\mathcal{F}$ is a coherent sheaf on $Y$ then $f^!\mathcal{F}$ can be described as $\underline{Hom}_{\mathcal{O}_Y}(f_*(\mathcal{O}_X),\mathcal{F})$ . If $X$ and $Y$ are affine then it's not hard to work out what this does for a coherent sheaf on Y. As another basic case, I'm trying to understand what happens if $\mathcal{L}$ is a line bundle on $Y$ , in terms of divisors. Say $D=\sum{P_i}$ is a divisor on $Y$ , and $\mathcal{L}(D)$ is the corresponding line bundle. My understanding is that if $f$ is etale then $f^! = f^*$ , so that in that case $f^!(\mathcal{L}(D))=\mathcal{L}(D')$ where $D'=\sum_i\sum_{f(Q)=P_i}{Q}$ . Can we understand $f^!(\mathcal{L}(D))$ in general (by which I mean, when $f$ is assumed finite flat but not necessarily etale)? (can we say something more specific than $f^!\mathcal{L}(D)=\underline{Hom}_{\mathcal{O}_Y}(f_*(\mathcal{O}_X),\mathcal{L}(D))$ , in terms of the divisor $D$ )? And how should I think of the functor $f^!$ for general coherent sheaves, in this setting? Thanks!",['algebraic-geometry']
3794745,"If $V_n(a)$ counts sign changes in the sequence $\cos a, \cos2a,\cos3a,\ldots,\cos na,$ show that $\lim_{n\to\infty}\frac{V_n(a)}n=\frac{a}\pi$","Let $0\leq\alpha\leq \pi $ . $V_n (\alpha) $ denote the number of sign changes in the sequence $\cos\alpha,\cos2\alpha,\cos3\alpha,\ldots,\cos n\alpha $ . Then prove that $$\lim\limits_{n\to\infty}\dfrac{V_n (\alpha)}{n}=\dfrac{\alpha}{\pi}.$$ I saw a hint where $\dfrac{V_n (\alpha)}{n}$ is considered as the probability. I mean how this expression is a probability of something. If it is, how can I progress further in this way? Update: I have a solution to this problem In $n\alpha$ rotation the number of times full circle rotation occures $=\bigg\lfloor\dfrac{n\alpha}{2\pi}\bigg\rfloor$ In one full circle rotation sign change occures 2 times. Hence in $\bigg\lfloor\dfrac{n\alpha}{2\pi}\bigg\rfloor$ full rotation sign change occures $=2\bigg\lfloor\dfrac{n\alpha}{2\pi}\bigg\rfloor$ Now the rest angle is $n\alpha-\bigg\lfloor\dfrac{n\alpha}{2\pi}\bigg\rfloor\times2\pi$ If we consider 0 as a change of sign in case of $\cos\left( \dfrac{\pi}{2}\right)$ and $\cos\left(\dfrac{3\pi}{2}\right)$ then:- (1) If $0\leq n\alpha-\bigg\lfloor\dfrac{n\alpha }{2\pi }\bigg\rfloor\times 2\pi<\dfrac{\pi}{2 }$ sign changes 0 times (2) If $\dfrac{\pi}{2 }\leq n\alpha-\bigg\lfloor\dfrac{n\alpha }{2\pi }\bigg\rfloor\times 2\pi<\dfrac{3\pi}{2 }$ sign changes 1 times (3) If $\dfrac{3\pi}{2 }\leq n\alpha-\bigg\lfloor\dfrac{n\alpha }{2\pi }\bigg\rfloor\times 2\pi<2\pi$ sign changes 2 times Let $f$ be a function such that $$f\left(\left\lfloor \dfrac{n\alpha-\bigg\lfloor\dfrac{n\alpha }{2\pi }\bigg\rfloor\times 2\pi}{\dfrac{\pi}{2}}\right\rfloor\right)=\begin{cases}0,\text{ when }\left\lfloor \dfrac{n\alpha-\bigg\lfloor\dfrac{n\alpha }{2\pi }\bigg\rfloor\times 2\pi}{\dfrac{\pi}{2}}\right\rfloor=0\\ 1,\text{ when }\left\lfloor \dfrac{n\alpha-\bigg\lfloor\dfrac{n\alpha }{2\pi }\bigg\rfloor\times 2\pi}{\dfrac{\pi}{2}}\right\rfloor=1\\ 1,\text{ when }\left\lfloor \dfrac{n\alpha-\bigg\lfloor\dfrac{n\alpha }{2\pi }\bigg\rfloor\times 2\pi}{\dfrac{\pi}{2}}\right\rfloor=2\\ 2,\text{ when } \left\lfloor \dfrac{n\alpha-\bigg\lfloor\dfrac{n\alpha }{2\pi }\bigg\rfloor\times 2\pi}{\dfrac{\pi}{2}}\right\rfloor=3\end{cases}$$ Therefore $\dfrac{V_n(\alpha)}{n}=\dfrac{2\bigg\lfloor\dfrac{n\alpha}{2\pi}\bigg\rfloor+ f\left(\left\lfloor \dfrac{n\alpha-\bigg\lfloor\dfrac{n\alpha }{2\pi }\bigg\rfloor\times 2\pi}{\dfrac{\pi}{2}}\right\rfloor\right)}{n}$ Hence $$\dfrac{V_n(\alpha)}{n}\geq \dfrac{2\bigg\lfloor\dfrac{n\alpha}{2\pi}\bigg\rfloor}{n}$$ and $$\dfrac{2\bigg\lfloor\dfrac{n\alpha}{2\pi}\bigg\rfloor+ 2}{n}\leq \dfrac{V_n(\alpha)}{n}$$ $\lim\limits_{n\to \infty}\dfrac{2\bigg\lfloor\dfrac{n\alpha}{2\pi}\bigg\rfloor}{n}=\dfrac{\alpha}{\pi}$ and $\lim\limits_{n\to\infty} \dfrac{2\bigg\lfloor\dfrac{n\alpha}{2\pi}\bigg\rfloor+ 2}{n}=\dfrac{\alpha}{\pi}$ Hence by Sandwich Theorem We get $\lim\limits_{n\to \infty}\dfrac{V_n(\alpha)}{n}=\dfrac{\alpha}{\pi}$ [Proved] Is this correct?","['limits', 'trigonometry']"
3794763,Showing a normed vector space is the direct sum of a closed subspace and a one dimensional subspace.,"Below is exercise 7 from chaper IV Banach spaces in Lang's Real and Functional Analysis: Let $F$ be a closed subspace of a normed vector space $E$ , and let $v\in E, v\notin F$ . Show that $F+ \Bbb{R}v$ is closed. If $E=F+ \Bbb{R}v$ , show that $E$ is the direct sum of $F$ and $\Bbb Rv$ (meaning the map $\phi(f,rv)= f+rv$ is a toplinear isomorphism from $F\times \Bbb Rv$ to $E$ , i.e. a homeomorphism and isomorphism). I can prove $F+ \Bbb{R}v$ is closed by looking at the quotient space $E/F$ . As the image of $F+ \Bbb{R}v$ under the quotient map $\rho$ is homeomorphic to $\Bbb R$ , it is automatically closed in $E/F$ , whose inverse image is closed in $E$ by continuity of $\rho$ . But $\rho^{-1}(\rho(F+ \Bbb{R}v))=F+ \Bbb{R}v$ , thereby proving the closeness of $F+ \Bbb{R}v$ . But I am stuck at showing the latter statement. It suffices to show that $\phi$ is an open map, which amounts to showing $U_1+U_2$ is open if $U_1$ and $U_2$ are open subsets of $F$ and $\Bbb Rv$ , respectively. Lang mentions that this is an easy consequence of the open mapping theorem, which is a more general result. However, doesn't that assume completeness of $E$ ? I try to use the quotient space technique, but that doesn't seem to apply here as $U_1+U_2$ needs not be saturated. How should I proceed? Thanks in advance.","['banach-spaces', 'direct-sum', 'open-map', 'functional-analysis']"
3794770,Prove $\lim_{x\to\infty} \frac{1}{x} = 1$ is false,"$$\lim_{x\to\infty} \frac{1}{x} = 1$$ Given $\epsilon > 0$ $$\left|\frac{1}{x} - 1\right| < \epsilon.$$ Rewrite it as $$-\epsilon < \frac{1}{x} - 1 < \epsilon$$ $$-\epsilon + 1< \frac{1}{x} < \epsilon + 1$$ If epsilon is very small, then on both sides we are getting value close to $1$ , but the function gets closer to zero, hence both sides false.
If $\epsilon$ is big, then on the right side we are getting big positive value, but with $ n \in (0,1)$ the function gets bigger too. Hence right side fails.
Is this  a sound proof? And if yes, how would I rewrite it with math symbols?","['limits', 'epsilon-delta', 'real-analysis']"
3794773,"$ f $ is differentiable in $ (0,0). $","Definition: Let $V\subseteq{\mathbb{R}^{m}}$ an open set, $a\in V$ y $f\colon V\to\mathbb{R}^{n}$ a function. We will say that $f$ is differentiable in $a,$ if exists a linear transformation $f'(a)\colon\mathbb{R}^{m}\to\mathbb{R}^{n}$ such that \begin{equation}
f(a+h)=f(a)+f'(a)(h)+r(h),\qquad\lim_{h\rightarrow 0}{\dfrac{r(h)}{\lVert h\rVert}}=0.
\end{equation} Let $ a \in \mathbb {R}$ be. Define the function $ f \colon \mathbb {R}^ {2} \to \mathbb {R} $ given by \begin{equation}
f(x,y)=\left\{\begin{matrix}
\dfrac{x\sin^{2}(x)+axy^{2}}{x^{2}+2y^{2}+3y^{4}} & (x,y)\neq(0,0)\\ 
0 & (x,y)=(0,0)
\end{matrix}\right.
\end{equation} Find the value of $ a $ so that $ f $ is differentiable by $ (0,0). $ My attempt: We observed that \begin{equation}
\dfrac{\partial f}{\partial x}(0,0)=0=\dfrac{\partial f}{\partial y}(0,0).
\end{equation} If $(x,y)\in\mathbb{R}^{2}\setminus\{(0,0)\},$ then \begin{equation}
\dfrac{\partial f}{\partial x}(x,y)=\dfrac{\sin^{2}(x)(2y^{2}+3y^{4}-x^{2})+x\sin(2x)(x^{2}+2y^{2}+3y^{4})+ay^{2}(2y^{2}+3y^{4}-x^{2})}{(x^{2}+2y^{2}+3y^{4})^{2}}
\end{equation} \begin{equation}
\dfrac{\partial f}{\partial y}(x,y)=\dfrac{2axy(x^{2}-3y^{4})-4xy\sin^{2}(x)(1+3y^{2})}{(x^{2}+2y^{2}+3y^{4})^{2}}
\end{equation} If $\dfrac{\partial f}{\partial y}(x,y)=0,$ then \begin{align}
2axy(x^{2}-3y^{4})-4xy\sin^{2}(x)(1+3y^{2})=0&\quad\Longleftrightarrow\quad a(x^{2}-3y^{4})=2\sin^{2}(x)(1+3y^{2})\\
&\quad\Longleftrightarrow\quad a=\dfrac{2\sin^{2}(x)(1+3y^{2})}{x^{2}-3y^{4}}
\end{align} \begin{equation}
f(x,y)=\left\{\begin{matrix}
x\sin^{2}(x) & (x,y)\neq(0,0)\\ 
0 & (x,y)=(0,0)
\end{matrix}\right.
\end{equation} \begin{equation}
\dfrac{\partial f}{\partial x}(0,0)=0=\dfrac{\partial f}{\partial y}(0,0)
\end{equation} From this it follows that $\dfrac{\partial f}{\partial x}(x,y)$ and $\dfrac{\partial f}{\partial y}(x,y)$ are continuous by $(0,0)$ y $f$ is differentiable by $(0,0).$ Are my arguments correct?
Any suggestion is welcome.","['partial-derivative', 'multivariable-calculus', 'derivatives', 'real-analysis']"
3794786,Can I pass the limit under the integral?,"I couldn't find the question on the website. I need to determine the integral $\lim_{n\rightarrow \infty }\int_{\left[
0,1\right] }f_{n}d\mu $ , if it exists, where $\mu $ is the Lebesgue measure
and $$
f_{n}=\frac{\sin \left( \left( x+\frac{1}{n}\right) ^{2}\right) -\sin \left(
\left( x-\frac{1}{n}\right) ^{2}\right) }{\sin \frac{1}{n}}
$$ Now I can see that $$\frac{\sin \left( \left( x+\frac{1}{n}\right)
^{2}\right) -\sin \left( \left( x-\frac{1}{n}\right) ^{2}\right) }{\sin 
\frac{1}{n}}=\frac{\sin \left( \left( x+\frac{1}{n}\right) ^{2}\right) -\sin
\left( \left( x-\frac{1}{n}\right) ^{2}\right) }{\frac{2}{n}}2\frac{\frac{1}{n}}{\sin \frac{1}{n}}$$ and using the fact that $g^{\prime }\left( x\right)
=\lim_{h\rightarrow 0}\frac{g\left( x+h\right) -g\left( x-h\right) }{2h}$ for $g\left( x\right) =\sin \left( x^{2}\right) $ and that $\frac{\frac{1}{n}}{\sin \frac{1}{n}}\longrightarrow 1$ , we conclude that $f_{n}\longrightarrow 4x\cos x^{2}$ but I am having trouble moving the limit
under the integral sign. What I think is that, since this sequence of
functions converges pointwise, they must be bounded by a finite function $%
m\left( x\right) $ , which is Lebesgue integrable and then we can apply the
DCT. Is this okay? I tried to show that convergence is uniform but my hand
wouldn't move.","['limits', 'measure-theory', 'lebesgue-integral']"
3794791,Line in polar coordinates,I just wanted to clarify something. A line in polar coordinates has the parameterization of $\theta = k\pi$ for $k \in \mathbb{R}$ right? Or am I missing something?,"['trigonometry', 'polar-coordinates']"
3794828,Limit of a convex function,"I would need a check on the following exercise: Let $f:\mathbb{R} \rightarrow \mathbb{R}$ a convex function. Prove that $\lim_{x \rightarrow \infty} f(x)$ and $\lim_{x \rightarrow - \infty} f(x)$ exist Show that if both the limits are finite, then $f$ is constant. My attempt: i ) I know that if $f$ is convex, then $$f(t x_1 + (1-t)x_2)< t f(x_1) + (1-t) f(x_2)$$ If I fix an arbitrary $N>0$ , then I have that for $x>x_2 \colon \quad f(x)>N$ , thanks to the convexity, therefore this proves the limit to $+ \infty$ is $+\infty$ . The same argument applies to $\lim_{x \rightarrow -\infty}f(x)$ : it suffices to note that for $x<x_1 \colon \quad f(x)>N$ . ii) Graphically it's obvious, but I have some problem in make it formal. If the limit is finite, say $L$ , then for every $\varepsilon >0$ there exists an $M(\varepsilon)$ such that for $$x>M(\varepsilon) \colon \quad |f(x)-L|\leq \varepsilon$$ Assume $f (x) \ne c$ . By definition of convexity, it has to hold (for $t \in [0,1]$ ) $$t f(M)+(1-t)f(M+1) \leq f(t M + (1-t)(M+1))$$ Now, by definition of limit, $f(M)$ and $f(M+1)$ are less than $L-\varepsilon$ . Also, the argument in the rhs of the inequality can be simplified: $$L-\varepsilon <t f(M)+(1-t)f(M+1) \leq f(t M + (1-t)(M+1)) = f(M-t)$$ Therefore $$L-\varepsilon < f(M-t)$$ , which is a contradiction because $M-t<M$ and hence it can be greater than $L-\varepsilon$ . So $f$ has to be equal to $c$ . Indeed in this case, it is still (trivially) convex, and the limits are of course finite.","['proof-writing', 'real-analysis', 'solution-verification', 'limits', 'convexity-inequality']"
3794830,Why does $N$ need to be normal? (Robinson exercise),"In Robinson Ed. 2, Ex. 1.3.16 he asks: if $H\le K\le G$ and $N\lhd G$ and $KN=HN$ and $K\cap N =H\cap N$ then show $H=K$ .  My question is: does $N$ need to be normal in $G$ ? Here's my attempted proof:  It's enough to show $K\le H$ . Let $k\in K$ .  Then there's an $h\in H$ such that $kN = hN$ .  So $h^{-1}k\in N$ but it's also in $K$ , and therefore $h^{-1}k\in K\cap N = H\cap N$ and so $h^{-1}k\in H$ therefore $k\in H$ . I'm sure I went wrong somewhere.  Where did I use normality? [I'm returning to maths and group theory after many years, and I do remember always struggling with the intuitive meaning of normality in the past, even though the definition is simple.]","['group-theory', 'normal-subgroups']"
3794846,How to compute $\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx$ or $\sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}}$,How to tackle $$I=\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx\ ?$$ This integral came up while I was working on finding $\sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}}$ . First attempt : By writing $\text{Li}_2(x^2)=-\int_0^1\frac{x^2\ln(y)}{1-x^2y}dy$ we have $$I=-\int_0^1\ln(y)\left(\int_0^1\frac{x\arcsin^2(x)}{1-x^2y}dx\right)dy$$ and Mathematica gave a complicated expression for the inner integral and that made me stop. Second attempt: $x=\sin\theta$ $$I=\int_0^{\pi/2}\theta^2\cot\theta\ \text{Li}_2(\sin^2\theta)d\theta$$ $$=\sum_{n=1}^\infty\frac{1}{n^2}\int_0^{\pi/2}\theta^2\cot\theta \sin^{2n}(\theta) d\theta$$ and I have no idea how to continue. Any suggestion? Thanks How $I$ appeared in my calculations: Since $$\frac{\arcsin x}{\sqrt{1-x^2}}=\sum_{n=1}^\infty\frac{(2x)^{2n-1}}{n{2n\choose n}}$$ we can write $$\frac{2\sqrt{x}\arcsin \sqrt{x}}{\sqrt{1-x}}=\sum_{n=1}^\infty\frac{2^{2n}x^{n}}{n{2n\choose n}}$$ Divide both sides by $x$ then $\int_0^y$ we have $$\sum_{n=1}^\infty\frac{2^{2n}y^n}{n^2{2n\choose n}}=2\int_0^y \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}dx$$ Next multiply both sides by $\frac{\text{Li}_2(y)}{y}$ then $\sum_{n=1}^\infty$ and use that $\int_0^1 y^{n-1}\text{Li}_2(y)dy=\frac{\zeta(2)}{n^2}-\frac{H_n}{n^2}$ we get $$\sum_{n=1}^\infty\frac{\zeta(2)2^{2n}}{n^3{2n\choose n}}-\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^4{2n\choose n}}=2\int_0^1\int_0^y \frac{\arcsin \sqrt{x}\text{Li}_2(y)}{y\sqrt{x}\sqrt{1-x}}dxdy$$ $$=2\int_0^1 \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(\int_x^1\frac{\text{Li}_2(y)}{y}dy\right)dx$$ $$=2\int_0^1 \frac{\arcsin \sqrt{x}}{\sqrt{x}\sqrt{1-x}}\left(\zeta(3)-\text{Li}_3(x)\right)dx$$ $$\overset{\sqrt{x}\to x}{=}4\int_0^1\frac{\arcsin x}{\sqrt{1-x^2}}(\zeta(3)-\text{Li}_3(x^2))dx$$ $$\overset{\text{IBP}}{=}4\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx$$ Substitute $\sum_{n=1}^\infty\frac{\zeta(2)2^{2n}}{n^3{2n\choose n}}=15\ln(2)\zeta(4)-\frac72\zeta(2)\zeta(3)$ we get $$\sum_{n=1}^\infty\frac{2^{2n}H_n}{n^4{2n\choose n}}=15\ln(2)\zeta(4)-\frac72\zeta(2)\zeta(3)-4\int_0^1\frac{\text{Li}_2(x^2)\arcsin^2(x)}{x}dx$$,"['integration', 'real-analysis', 'harmonic-numbers', 'polylogarithm', 'trigonometry']"
3794862,Does it imply to other side?,"I've got this kind of statement in a book: $(A \subset B) \Rightarrow ((A \setminus C) \subset (B \setminus C))$ Isn't it true for other side? I assumed that $x \in A$ . Then $x \in (A \setminus C) \Rightarrow x \in (B \setminus C)$ . So $x \in B$ and $x \notin C$ . And it has to be $A \subset B$ . But someone gave me a contradiction by setting $A=\{1\}, B=\{2\}, C=\{1,2\}$ . Could you also tell me where I may be wrong? Thank you",['elementary-set-theory']
3794865,Let $BA= BC$ and $BD = AC$. Find angle $BDC$. (See image),"Let $BA= BC$ and $BD = AC$ . Find angle $BDC$ . This is probability a very elementary question, and I'm thinking of using sine rule to brute force it. However, I seem to feel like there should be an elegant solution to this that involves shifting the triangles around (perhaps side BD to AC). Is there any simple way of doing this?","['euclidean-geometry', 'triangles', 'geometry']"
3794866,Show that if $x(t)$ satisfies $\frac{dx}{dt} = x^2-x^6$ and $x(0) >0$ then $\lim_{t\to\infty} x(t) = 1.$,"Show that if $x(t)$ satisfies $\frac{dx}{dt} = x^2-x^6$ and $x(0)  = c >0$ then $\lim_{t\to\infty} x(t) = 1.$ (Taken from ""Berkeley problems in mathematics"") Here's my attempt: The function $f(t,x) = x^2-x^6$ is continuous everywhere and locally Lipschitz. By Picard's theorem there exists an $\epsilon>0$ such that there is a unique solution $x(t)$ on $[- \epsilon, \epsilon].$ By differentiability of $x(t)$ with $x'(0) \neq 0$ ( assuming $x$ is non-constant) and as $x(0)>0$ the inverse function theorem applies and there is a local continuous inverse $x^{-1}(y)$ satisfying $$
\frac{dt}{dx} = \frac{1}{x^2-x^6} \implies x^{-1}(y) = \int^{y}_{c}\frac{d \xi}{\xi^2-\xi^6}.
$$ As $\frac{1}{\xi ^2- \xi^6}$ ""blows up at $\xi=1$ rapidly enough"" we have $x^{-1}(1) = \infty$ and by continuity of $x$ , $ \lim_{t \to \infty}x(t) = 1$ . $\blacksquare$ Is this correct? Many thanks! EDIT: In response to enzotib: Assume $0 < c <1$ . The integral $x^{-1}(y)$ is continuous and non-zero on $(c,1)$ . Clearly $x^{-1}(c) = 0, $ and as shown $x^{-1}(1) = + \infty$ . By IVT, $ x^{-1} : (c,1) \to [0, +\infty)$ . Applying the inverse function theorem a second time we see that the unique solution on $[0,  \epsilon]$ can be extended to a unique solution on $[0, \infty)$ . Taking should now be justified. The case $c >1$ is much the same and the case $c =1$ forces $x(t) =1$ which solves the problem trivially.","['calculus', 'solution-verification', 'ordinary-differential-equations', 'real-analysis']"
3794897,Does a nontrivial finite solvable group have a subgroup of prime power index for each prime divisor?,"It is well-known that every maximal subgroup of $G$ is of prime power index if $G$ is a nontrivial finite solvable group. My question is: Can we prove that for each prime $r\in\pi(G)$ there exists a maximal subgroup of $G$ of index a power of $r$ ? I tried to prove it but I found that I made a mistake in my proof. Here is my attempt: Define $$\pi^*:=\{r\in\pi(G)\mid~\mbox{There is no maximal subgroup }H\mbox{ of }G\mbox{ such that }|G:H|\mbox{ is a power of }r\}.$$ We claim that $\pi^*$ is an empty set.
Assume that $\pi^*$ is non-empty. Then the indices of the maximal subgroups are exactly powers of primes in $\pi(G)\setminus\pi^*$ . Take a Sylow $q$ -subgroup $S_q$ for each $q\in\pi(G)$ . For $p\in\pi(G)\setminus\pi^*$ , take an arbitrary maximal subgroup $M$ of $G$ such that $|G:M|$ is a power of $p$ . We have $$\left|\prod_{q\in\pi(G)\setminus\pi^*}S_q\right|_p=|G|_p>|M|_p.$$ It implies that $\prod\limits_{q\in\pi(G)\setminus\pi^*}S_q$ is not contained in any maximal subgroup of $G$ . But $\prod\limits_{q\in\pi(G)\setminus\pi^*}S_q$ is properly contained in $G$ , which is a contradiction. My mistake : $\prod\limits_{q\in\pi(G)\setminus\pi^*}S_q$ is not necessarily a subgroup of $G$ , so in fact I cannot get any contradiction. Could you give me some ideas? I think maybe I should prove it in a different way. Any help is appreciated. Thanks!","['group-theory', 'abstract-algebra', 'finite-groups', 'solvable-groups']"
3794914,The ratio of the area of two regular polygons,"The polygons in the figure below are all regular polygons(regular heptagon),   share a vertex and the orange line crosses the three vertices of the two regular polygons, the area of the small regular polygon and the large regular polygon is denoted as $S_1$ , $S_2$ ,  what is $\frac{S_1}{S_2}$ ? Additional question (regular nine-sided polygon)","['euclidean-geometry', 'area', 'geometry', 'polygons', 'ratio']"
3794918,Quotients of abelian groups - residual finiteness and elements of order $p$,"Suppose $A$ is an abelian group and $\pi$ is a set of prime numbers. A $\pi$ -number is a product of primes from $\pi$ . Assume that for each $p \in \pi$ , $A_p = \{a \in A : \exists i\in\mathbb{N} \text{ s.t } a^{p^i} = 0\}$ has finite exponent. Assume also that $A$ is $\pi$ -reduced; there are no non-trivial subgroups of $A$ which are $\pi$ -divisible. That is, for any $H \leq A$ there is $h \in H$ and $m$ a $\pi$ -number such that for any $x \in H$ , $x^m \neq h$ . Let $j \in \mathbb{N}$ , $p \in \pi$ and $m = p^jn$ a $\pi$ -number where $n$ is relatively prime to $p$ . why is $A/A^m$ residually finite? why does $A^{p^j}/A^m$ have no element of order $p$ ? Here is the context from Infinite Soluble Groups:","['group-theory', 'abstract-algebra', 'abelian-groups']"
3794965,Variety over a finite field as a scheme,"In Hartshorne's book, a functor $t$ is defined (II.2.6) from the category of varieties over an algebraically closed field $\kappa$ (Ch I) to the category of schemes over $\operatorname{Spec}\kappa$ . I was wondering if it is possible to define in a similar fashion a functor $t'$ from the category of varieties over arbitrary fields $k$ (that is, locally ringed spaces that are locally isomorphic to set of zeros of some polynomials over $k$ ) to the category of functors over $\operatorname{Spec}k$ such that for every affine variety $V$ with ring of regular functions $A(V)$ $$ t'(V) = \operatorname{Spec}A(V). $$ In particular I am interested in the case when $V$ is a variety defined over some finite field $\mathbb{F}_q$ .","['algebraic-geometry', 'schemes']"
3795010,Topological Markov Property and $D$-condition,"I was reading two different articles and I came across these two definitions below: $(1)$ A subshift $X \subseteq \mathcal{A}^{\mathbb{Z}^d}$ has the topological Markov property (TMP) if for any finite subset $\Lambda$ of $\mathbb{Z}^d$ there exists a finite subset $\Lambda^{\prime}$ of $\mathbb{Z}^d$ with $\Lambda \subset \Lambda^{\prime}$ such that for any $x,y \in X$ , if $x_{\Lambda^{\prime}\setminus \Lambda} = y_{\Lambda^{\prime}\setminus \Lambda}$ , then $x_{\Lambda}y_{\Lambda^c} \in X$ . $(2)$ Given $\{T_n\}$ a Folner sequence for $\mathbb{Z}^d$ , the pais $(X, \{T_n\})$ satisfies the $D$ -condition if for all $n \in \mathbb{N}$ , there exists $T_n \subseteq \tilde{T}_n$ such that $\frac{|\tilde{T}_n|}{|T_n|} \to 1$ , and for all $x, y \in X$ , there exists $z \in X$ such that $z_{T_n} = x_{T_n}$ and $z_{\tilde{T}_n^c} = y_{\tilde{T}_n^c}$ . I was wondering what is the relation (and if there is, of course) between these definitions in the sense that one implies the other. If $\{T_n\}$ is a Folner sequence and $\Lambda$ is a finite set, we also have that $\{F_n\} = \{T_n\} \cup \Lambda$ is a Folner sequence. But I am not sure if assuming that (X, {T_n}) satisfies the $D$ -condition, we would obtain that $(X, \{F_n\})$ also satisfies the $D$ -condition. I guess not. This is one of the difficulties I came across while trying to prove that $D$ -condition implies TMP, which is the direction I think it might be true.","['general-topology', 'probability', 'dynamical-systems']"
3795035,Probability of getting an odd number of balls,"We have $n$ balls such that $k \ge 1$ of them are black and the rest are white. Consider the following procedure: We first put all the balls in the bucket $B_0$ . Then, we select each one of them with probability $1/2$ and put the selected ones in $B_1$ . Next, we select each ball in $B_1$ with probability $1/2$ and put the selected ones in $B_2$ . We continue doing that for $\Theta(\log{n})$ iterations. What is the probability that at least one of these buckets contains an odd number of black balls? Well, if $k$ is odd then it is easy to see that the probability is $1$ , but how can we analyze this when $k$ is even?","['discrete-mathematics', 'combinatorics', 'probability-theory', 'probability']"
3795047,A twice differentiable function satisfying a differential equation,"The question is : Let $f:\mathbb{R} \to \mathbb{R}$ be twice differentiable function satisfying $f(x)+f''(x)=-xg(x)f'(x), x\in \mathbb{R} $ where $g(x) \ge 0, \forall x\in \mathbb{R}$ Which of the following is\are true? $(1)$ If $f(0)=f'(0)=1$ , then $f(3)\lt 3$ $(2)$ If $f(0)=f'(0)=2$ , then $f(4)\lt 4$ $(3)$ If $f(0)=f'(0)=3$ , then $f(3)=5$ $(4)$ If $f(0)=f'(0)=3$ , then $f(3)=6$ My thoughts:- I will first discuss about $(3)$ and $(4)$ Let $g(x)=0$ Then with some computation , we can show $f(x)=3(\sin x+\cos x)$ as a suitable candidate to discard $(3)$ and $(4)$ Here , for option $(3)$ $f(3)=5$ $\Rightarrow \sin 3+\cos 3=\frac 53$ On squaring both sides $1+\sin 6=\frac{25}9$ $\sin 6=\frac {16}9 \gt 1$ , a contradiction Similarly $f(3)= 6$ will give the contradiction $\sin 3+\cos 3=2$ ( implying $\sin 3=\cos 3=1$ which is an impossibility) . Thus we are left with $(1)$ and $(2)$ Note:A slight variant of the above example satisfies the condition in $(1)$ and $(2)$ I tried with simple examples like $g(x)=1 $ and $f(x)=x$ or like quadratics but couldn't reach conclusions . Please help with the options $(1)$ and $(2)$ . Thanks for your time.","['ordinary-differential-equations', 'sturm-liouville', 'real-analysis', 'solution-verification', 'derivatives']"
3795076,Asymptotical behavior of the SIR epidemic model,"The SIR epidemic model presents three differential equations
for three time-dependent variables $s(t), i(t), r(t)$ : $$\begin{align}
	\frac{ds}{dt} & = - \beta i s 			 \\
	\frac{di}{dt} & = \beta i s - \gamma i	 \\
	\frac{dr}{dt} & = \gamma i 
\end{align}$$ It is assumed that the variables are non-negative, $s(t) + i(t) + r(t) = 1$ ,
and the coefficients $\beta, \gamma$ are positive.
In the literature it is claimed or taken as self-evident that $$
	\lim_{t \to \infty} i(t) = 0
$$ How can this behavior be proven rigorously?","['limits', 'stability-in-odes', 'ordinary-differential-equations', 'dynamical-systems']"
3795093,"If $x + y = 5xy$ , $y + z = 6yz$ , $z + x = 7zx$ . Find $x + y + z$ .","If $x + y = 5xy$ , $y + z = 6yz$ , $z + x = 7zx$ . Find $x + y + z$ . What I Tried : I used some clever ways to get $x + y + z = 26xyz$ , but I suppose we have some solution as a number . All all $3$ to get :- $$2(x + y + z) = 5xy + 6yz + 7zx$$ Or, $$ 2(x + y + z) = (xy + xy + xy + xy + xy) + (yz + yz + yz + yz + yz + yz) + (zx + zx + zx + zx + zx + zx + zx)$$ That is, $$ 2(x + y + z) = (xy + zx) + (xy + zx) + (xy + zx) + (yz + zx) + (yz + zx) + (yz + zx) + (yz + zx) + (xy + yz) + (xy + yz)$$ Now see that $(xy + zx) = x(y + z) = 6xyz$ , similarly $(yz + zx) = 5xyz$ and $(xy + yz) = 7xyz$ So $$2(x + y + z) = 3(6xyz) + 4(5xyz) + 2(7xyz)$$ $$\Rightarrow (x + y + z) = \frac{52xyz}{2} = 26xyz$$ I tried till this , then I have no idea . Can anyone help?","['algebra-precalculus', 'problem-solving', 'polynomials']"
3795119,Probability of selecting a poker hand,"I am trying to solve a probability problem about five-card poker hand. I have access to the answer which is different from what I had come up with. The question is: What is the probability that a five-card poker hand has exactly two cards of same value, but no other cards duplicated? My answer to this question was as follows: $\binom{13}{1} \binom{4}{2} \binom{48}{1}\binom{44}{1} \binom{40}{1}$ .
Which means: First select a card number then select its two suits ie. $\binom{13}{1} \binom{4}{2}$ . These will be the two cards of same value. Select three other cards which are not duplicate as: $\binom{48}{1}\binom{44}{1} \binom{40}{1}$ . The correct answer doesn't match my answer. This answer is provided in book AOPS and is as: $\binom{13}{1} \binom{4}{2}\binom{12}{3}\binom{4}{1}\binom{4}{1}\binom{4}{1}$ . So question is, what am I doing wrong? Thanks","['combinatorics', 'probability']"
3795149,What is the difference between a Hamiltonian Path and a Hamiltonian Cycle?,"The title says it all. I've seen confusing definitions of this, and would appreciate if someone can succinctly clear this up with definitions and examples.","['graph-theory', 'hamiltonian-path', 'discrete-mathematics', 'computer-science']"
3795171,"Expected stopping time of Brownian motion breaking out of [a,-b] channel","Let $W(t)$ be a Standard Brownian motion. Let $\tau$ be the first time that $W(t)$ hits either level "" $a$ "" or level "" $-b$ "". What is the most straightforward way to compute $\mathbb{E}[\tau]$ ? I am able to show the probability that $W(t)$ hits level "" $a$ "" before "" $-b$ "" and vice versa, but I am unable to easily compute the expectation of the stopping time $\tau$ . To show the probability that $W(t)$ hits "" $a$ "" before "" $b$ "", I assume $\mathbb{E[\tau]}\leq \infty$ , so that by Doob's optional stopping theorem, $\mathbb{E}[W_{\tau}]=W(0)=0$ (i.e. stopped process is a martingale). Then: $$ 0=\mathbb{E}[W_{\tau}]=a*\mathbb{P}(W_{\tau}=a)+ (-b)\mathbb{P}(W_{\tau}=-b) $$ By definition of $\tau$ , we have that $\mathbb{P}(W_{\tau}=a)+\mathbb{P}(W_{\tau}=-b)=1$ , so that: $$ a*\mathbb{P}(W_{\tau}=a)+ (-b)\mathbb{P}(W_{\tau}=-b) = \\ = a*\mathbb{P}(W_{\tau}=a)-b(1-\mathbb{P}(W_{\tau}=a)$$ Solving for $\mathbb{P}(W_{\tau}=a)$ gives: $\mathbb{P}(W_{\tau}=a)=\frac{b}{a+b}$ Question 1 : how could I easily show that $\mathbb{E}[\tau]\leq \infty$ , so that I can verify that I can indeed use Doob's optional stopping theorem? Question 2 : how can I compute $\mathbb{E}[\tau]$ in the simpliest possible way?","['stochastic-processes', 'martingales', 'stopping-times', 'brownian-motion', 'probability-theory']"
3795183,Integrating a 'twisted' rational function,"For $x\in [0,1]$ , let $$
P_n (x) = \prod_{k=1}^{n} (x^k+1)^{(-1)^k}.
$$ For example, $\displaystyle{P_4(x) = \frac{(x^2+1)(x^4+1)}{(x+1)(x^3+1)}}$ . Of note: $P_n(1)=1/2$ if $n$ is odd and $1$ if $n$ is even, so we cannot expect uniform convergence on $[0,1)$ . I am interested in the limit $\lim_{n\to\infty}P_n(x)$ , if it exists, and several related integrals, namely: Whether $P(x):=\lim_{n\to\infty}P_n(x)$ exists and if so what it is $I_n:=\int_0^1 P_n(x)\,dx$ (this seems to be the natural range of integration since we want to avoid negative numbers and the even-index version blows up for $x>1$ ) $I:=\int_0^1 P(x)\,dx$ I calculated the first few values of $I_n$ by hand: $$
\left\{\log (2),\log (4)-\frac{1}{2},\frac{1}{27} \left(9+2 \sqrt{3} \pi \right),\frac{5}{2}+\frac{\pi }{9
   \sqrt{3}}-\frac{8 \log (2)}{3}\right\}
$$ Then I computed $20$ values using a CAS; the sequence appears to be alternating with the odd values increasing and the even values decreasing (as expected). I got $I_{1000}\approx 0.79496$ and $I_{1001}\approx 0.794376$ , so I would guess the limit $I$ is somewhere in between them. I've seen infinite products before, mostly in the context of some introductory material I've read on hypergeometric series, so feel free to use them in your answer!","['integration', 'definite-integrals', 'infinite-product']"
3795202,Circular reasoning in proving $\lim_{x\to a}(\sin x) = \sin a$,"I just started learning about epsilon-delta limit proofs, and I want to know how to prove using the epsilon-delta definition of a limit that $\lim_{x\to a}(\sin x) = \sin a$ I tried and failed, so I looked it up online and found the trick is to use the identity $\sin x < x$ . I cannot find any proofs that do not use this identity. I had never seen this identity before, so I searched for its proof and found this proof that uses the mean value theorem. Again, I haven't yet learnt the mean value theorem, but according to the website, it requires a continuous (and differentiable) function $f$ . But the concept of continuity is defined using the epsilon-delta limit definition! In fact, the fact that $\sin(x)$ is continuous is exactly the statement that I'm trying to prove above: $\lim_{x\to a}(\sin x) = \sin a$ This is clearly circular reasoning. My question is how does one escape it? Either there must be a way to prove $\lim_{x\to a}(\sin x) = \sin a$ without the identity $\sin x < x$ , or we need to prove $\sin x < x$ without the fact that sin is continuous. Or I suppose there could be a 3rd option? I can't find any answers on how to do it, which I find most strange...","['epsilon-delta', 'proof-writing', 'fake-proofs', 'alternative-proof', 'trigonometry']"
3795224,Hexagon Rearrangement Problem (How many ways to rearrange these hexagons so that every pair of adjacent hexagons become non-adjacent?),"Here is the detail:
There are $\displaystyle \frac{n(n+1)}{2}$ ( $n$ layers) hexagons forming a triangle. Denote them as $H_i$ from top to bottom and from left to right. Now I want to find the number of ways to rearrange them so that every pair of adjacent hexagon become non-adjacent. For example, when $n=4$ , one of the feasible arrangement is like https://s1.ax1x.com/2020/08/18/dMw4yR.png , and I can find that there are 18 ways to do this by enumerating. For $n>4$ , I don't know what to do. I would appreciate it if someone can help me with this problem.","['graph-theory', 'combinations', 'combinatorics', 'permutations']"
3795225,Is the converse of Ramanujan's Master Theorem also true?,"Ramunajan's Master Theorem states that if a complex-valued function $f(x)$ has an expansion of the form $$\displaystyle f(x)=\sum _{k=0}^{\infty }{\frac {\,\varphi (k)\,}{k!}}(-x)^{k}$$ then the Mellin transform of $f(x)$ is given by $$\displaystyle \int _{0}^{\infty }x^{s-1}\,f(x)\,\operatorname {d} x=\Gamma (s)\,\varphi (-s)$$ Here $\varphi(s)$ is some function (say analytic or integrable). Now, what about the converse of this? Say that we know that the Mellin transform of $f(x)$ is equal to $\Gamma (s)\,\varphi (-s)$ , is it then true that $f(x)$ has an infinite expansion in the form given above? I couldn't find anything about this question on Wikipedia or somewhere else.","['integration', 'mellin-transform', 'sequences-and-series']"
3795231,Taking matrix derivative $\| \left| \mathbf{X}\mathbf{W}\right|-\mathbf{1}_{n \times K} \| ^2_F$ with respect to W,"I am trying to take the matrix derivative of the following function with respect to $\bf W$ : \begin{equation}
\| \left| \mathbf{X}\mathbf{W}\right|-\mathbf{1}_{n \times K} \| ^2_F \\
\end{equation} Where $\mathbf{X}$ is $n \times d$ , $\mathbf{W}$ is $d \times K$ and $\mathbf{1}_{n \times K}$ is a marix with all elements one. $\| \cdot \|_F$ is the Frobenius norm and $\left| \mathbf{X}\mathbf{W}\right|$ is the element wise absolute value of $\mathbf{X}\mathbf{W}$ . Any helps is highly appreciated.","['matrices', 'multivariable-calculus', 'matrix-calculus', 'linear-algebra', 'derivatives']"
3795243,Every holomorphic function on a compact complex manifold is locally constant,"We know that if $X$ is a compact connected complex manifold,then every holomorphic function on $X$ is constant. Now,supposed that $X$ is not necessarily connect, then we can choose a connected component. We know that connected component is closed subset and every closed subset of a compact set is also compact. So the connected component is also compact, then we can deduced that every holomorphic function on the connected component is constant. Then We can deduced that every holomorphic function on $X$ is locally constant. I think this may be not right but I can't find where is the problem in my proof in the above.","['complex-geometry', 'complex-manifolds', 'differential-geometry']"
3795285,"Visualising the scheme $\mathrm{Spec} \, k[x,y_1,y_2,\dots,y_n]/(y_1^2,\dots,y_n^2)$","Let $k$ be an algebraically closed field (for me I am using $k=\mathbb C$ ). I know that $\mathrm{Spec} \, k[x]/(x^2)$ consists of simply the prime ideal $(x)$ . Indeed, any ideal $\mathfrak p$ of $k[x]/(x^2)$ is an ideal of $k[x]$ such that $(x^2) \subset \mathfrak p$ . If we now consider $\mathrm{Spec} \, k[x,y]/(y^2)$ , now the prime ideals of $k[x,y]$ are $(0)$ , $(x-a,y-b)$ for $a,b \in k$ and irreducible polynomials $f(x,y)$ generating $(f(x,y))$ . Clearly $(y^2)\not\subset (0)$ . As for the irreducible polynomials, we have $(y^2) \subset k[x,y]f(x,y)$ , so I think it is right to say that the ideals in bijection with these are of the form $(a+f(x)y+g(x))$ where $a,b \in k$ and $f,g$ irreducible. I guess $(x-a,y-b)$ would also be prime ideals of the quotient ring since quotienting by them gives an integral domain. Now I am interested in understanding the generalisation $\mathrm{Spec} \, k[x,y_1,y_2,\dots,y_n]/(y_1^2,\dots,y_n^2)$ . In particular: Can we classify all elements of the spectrum of this ring, for $n \geq 1$ ? Can we visualise this scheme, and has it been studied in some context in the literature?","['ring-theory', 'algebraic-geometry', 'schemes', 'commutative-algebra']"
3795291,Notation for set union that results in a multiset,"Is there a notation that replaces the ""union"" operator $A\cup B$ and emphasizes that the outcome should be considered a multi-set rather than a set? For example, if $A = \{1,2,3\}$ and $B =\{3,4,5\}$ , then: $$A ? B = \{1,2,3,3,4,5\}$$ since the $3$ is counted twice. What operator can I used instead of the "" $?$ ""?","['elementary-set-theory', 'notation', 'multisets']"
3795297,Is it possible that $|| f^{(n)} ||_1 \to \infty$ exponentially for a compactly supported $C^\infty$-functions?,"Suppose that $f \in C^\infty(\mathbb R)$ has compact support in $[-T,T]$ where $T>0$ . Is it possible that the $L^1$ -norm of its derivatives are growing exponentially? That means $$
\| f^{(n)} \|_1 \to \infty
$$ exponentially. For me it's hard to imagine that such a function could exist. Is there a way to construct such a function or argue that it cannot exist?","['derivatives', 'smooth-functions', 'real-analysis']"
3795327,"$h^{p,q}$ of a complex torus.","As we know, a compact Khler surface with trivial canonical bundle is a K3 surface or a torus of dimension 2. I know $h^{0,2}$ of a K3 surface is 1, and I know $h^{0,2}$ of a torus must not be zero (otherwise it is always algebraic), but I don't know how to compute $h^{0,2}$ of a complex torus of dimension 2. By the way, is there a general method to compute all the Hodge numbers of a complex torus of dimension $n$ ? Any comments are welcome!","['hodge-theory', 'kahler-manifolds', 'complex-geometry', 'algebraic-geometry', 'differential-geometry']"
3795370,Concentration of the Norm for Sub-gaussians,"I am reading Theorem 3.1.1 in HDP book by Vershynin. The theorem states that $ \text{Let } X=\left(X_1,\ldots,X_n \right) \text{be a random vector with independent, sub-gaussian coordinates } X_i \text{ that satisfy } \mathbb{E}X_i^2=1. \text{Then}$ $$ \| \| X\|_2-\sqrt{n}\|\|_{\psi_2} \leq CK^2$$ $ \text{where } K=\max_i{\|X_i\|_{\psi_2}} \text{ and } C \text{ is an absolute constant.}$ The $\psi_2$ norm is the Orlicz norm with Orlicz function $\psi(x)=e^{x^2}-1. $ I found a place that I don't understand in the proof. The whole proof only showed that $ \| X \|_2 -\sqrt{n} $ is a sub-gaussian random variable. And in the last sentence, the author just said it is equivalent to the conclusion of the theorem. I would like to ask about the equivalence in the last sentence. I've tried to look at the centering property of sub-gaussian, but it seems that $\sqrt n \neq \mathbb{E}\|X\|_2 $ . Any hint or idea is appreciated.","['concentration-of-measure', 'probability-theory', 'orlicz-spaces']"
3795377,Suppose $\angle BAC = 60^\circ$ and $\angle ABC = 20^\circ$. A point $E$ inside $ABC$ satisfies $\angle EAB=20^\circ$ and $\angle ECB=30^\circ$.,"Problem statement: In a triangle $ABC$ with angles $\angle BAC = 60^\circ$ and $\angle ABC = 20^\circ$ , a point $E$ inside the triangle is given such that $\angle EAB = 20^\circ$ and $\angle ECB = 30^\circ$ . Prove that $E$ lies on an angle bisector of $\angle ABC$ . I drew a picture in Geogebra for this problem and this is what I did so far: All angles drawn were found out by using the fact that the sum of angles in a triangle is $180^\circ$ . I also noticed that $AD=BD$ , $AC=CD=AE$ by using isosceles triangles. I tried proving that $E$ lies on an angle bisector of $\angle ABC$ by proving that $GE=EI$ and so I drew perpendicular bisectors from $E$ to the sides $AB$ and $BC$ , and I noticed two pairs of similar triangles: $\triangle AHE \sim \triangle AEF$ and $\triangle CJE \sim \triangle CED$ , but I'm not sure if this is useful in any way. I'm stuck since and I don't know how to continue on from this. I'm not sure if this is even the right approach to the problem. Is there a way of approaching this problem that I missed?","['contest-math', 'euclidean-geometry', 'triangles', 'plane-geometry', 'trigonometry']"
3795383,"Is $f(x,y)=\frac{xy^3}{x^2+y^6}$ differentiable at $(0,0)$? [duplicate]","This question already has an answer here : Prove that $\lim_{(x,y)\rightarrow(0,0)} \frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}}$ does not exist (1 answer) Closed 3 years ago . Is the following function differentiable at $(0,0)$ ? $$ \
 f(x,y) =
  \begin{cases}
   \frac{xy^3}{x^2+y^6} & \text{if } (x,y) \ne (0,0), \\
   0       & \text{if } (x,y) = (0,0).
  \end{cases}
$$ I found that both of the partial derivatives are $0$ , and then tried to calculate the following limit: $$\lim_{(x,y) \to (0,0)} \frac{\frac{xy^3}{x^2+y^6}}{\sqrt{x^2+y^2}} = \lim_{(x,y) \to (0,0)} \frac{xy^3}{(x^2+y^6) \sqrt{x^2+y^2}}$$ And then I got stuck. I tried the squeeze theorem, but I still couldn't calculate it. How can I calculate this limit?","['limits', 'multivariable-calculus', 'derivatives']"
3795411,Prove $ac\cos B+ab\cos C-bc\cos A-a^2 \le \frac{c^2}{8\cos^2(90^\circ-C)}$ for $\triangle ABC$,"Triangle $\triangle ABC$ has sides $a$ , $b$ , and $c$ , and circumradius $R$ . Prove that $$ac \cos B + ab \cos C - bc \cos A - a^2 \le \frac{c^2}{8\cos^2(90^\circ - C)}$$ When does equality occur? I came across this question in a different forum and I thought it was interesting. I made a bit of progress but not much: I changed $R^2$ to the fraction in the inequality. I think that there is probably another use of Law of Sines or Law of Cosines but I can't find one. Edit: A lot of people have questions about if the problem is right; here is the original problem: Triangle $\triangle ABC$ has sides $a$ , $b$ , and $c$ , and circumradius $R$ . Prove that $b^2 + c^2 - a^2 \ge -R^2$ When does equality occur?","['inequality', 'geometry', 'a.m.-g.m.-inequality', 'geometric-inequalities', 'trigonometry']"
3795446,Limit of hypergeometric distribution when sample size grows with population size,"Consider choosing $Mn/6$ balls from a population consisting of $M$ balls of each of $n$ colors (so $Mn$ balls in total). So the density function of the sample is given by a multivariate hypergeometric distribution: $$f(x_1,\ldots, x_n) = \frac{\binom{M}{x_1}\cdots\binom{M}{x_n}}{\binom{Mn}{Mn/6}}.$$ Can one say anything about the limiting behavior of the distribution as $M\to\infty$ , where the number of colors $n$ is fixed? Since the sample size grows at the same rate as the population size, this wouldn't converge to a binomial/multinomial distribution as it would if the sample size were fixed. Any help is appreciated! (The $1/6$ in $Mn/6$ is arbitrary, I'm just curious in general about the case where the sample size is always a fixed fraction of the population size). I guess it wouldn't surprise me if nothing really useful can be said, in which case I have a related question. Suppose you consider the same scenario, but instead of starting off with $M$ balls of each color, we only started off with, say, $5M/6$ balls of each color. So the modified density function would be: $$g(x_1,\ldots, x_n) = \frac{\binom{5M/6}{x_1}\cdots\binom{5M/6}{x_n}}{\binom{5Mn/6}{Mn/6}}.$$ As $M\to\infty$ , is there any meaningful relationship between $f$ and $g$ that can be made? It vaguely seems to me like as $M$ grows large the two densities should look more and more alike, but it's possible that that intuition is awry.","['probability-distributions', 'probability-theory']"
3795454,Using the Snowflake Method to Factor Trinomials,"In school, students may be taught different ways to factor the trinomial $$ax^2+bx+c$$ where $a \neq 1,0$ . Possible methods include the classic Guess and Check Method, Grouping, Box Method , and the Snowflake Method , which is the one I'm focused on right now. If the Snowflake Method is used correctly, factoring trinomials can happen much quicker than using the traditional Guess and Check Method. Indeed, the Snowflake Method works for factoring the following trinomial: $$5x^2-x-18$$ First we set up the snowflake: To briefly summarize, we label and fill in the ""wings"" as seen above. Then we find the factors of $c$ that add to $b$ and multiply to $ac$ and put them in the empty wings. This creates fractions which I circled, and they must be reduced if possible. This gives us the correct factored form of $\boxed{(x-2)(5x+9)}$ . Now, here is my problem. I tried to use the Snowflake Method to factor $$7x^2+37x+36$$ I set up the snowflake as follows: There was no ""nice"" factor pair here because no pair multiplied to get $ac=252$ . However, I noticed that $(7)(36)=252$ , so I chose the pair $(36,1)$ . This would imply that the factored form is $$(7x+36)(7x+1)$$ but clearly this is incorrect. The answer should be $$(7x+9)(x+4)$$ I don't see how the Snowflake Method can produce this. It seems to impossible to produce the $(x+4)$ term because if we divide $7$ by any of the factors of $36$ , we will not get $4$ . I would like to understand: Why did the Snowflake Method not work for this example? Is there some restriction when using the Snowflake Method that I missed?","['algebra-precalculus', 'factoring', 'polynomials']"
3795458,A non-linear integrable differential equation,"I'm trying to solve a question from a tutorial on mathematics for physicst which was never done due to the pandemic so I don't know the answer or a proper method of solving it. Nevertheless here is the question and my attempt at solving it. Feedback, suggestions on how to approach it and further readings recommendations would be hugely appreciated. Let the equation of movement be: $$m\ddot{x}(t) + V'(x(t))= 0\tag1$$ and, $$E = \frac{m}{2}\left(\dot{x}(t)\right)^2 + V(x(t))\tag2$$ where $V(x)$ is a known derivable potential and $E$ is independent of $t$ . By integration of the equation giving $\dot{x}$ , Express the solution with the initial condition $x(t_0)=x_0$ in the form of t(x). From the equation $(1)$ $$\dot{x}^2 = \frac{E-V}{m/2} \implies
\pm\int_\left(x_0\right)^x\sqrt{\frac{m/2}{E-V}}dx = t+Cste$$ Taking the positive root and from the initial condition we know $Cste=-t_0$ $$t(x)=\int_\left(x_0\right)^x\sqrt{\frac{m/2}{E-V}}dx+t_0$$ 2. Let a increasing potential at infinity be: $$V(x\rightarrow\infty)= \frac{-C}{x^\left(2a\right)}$$ where $C>0$ and $a>0$ . We consider a particle of initial velocity $v_0>0$ . Give the asymptotic behavior of $x(t)$ when $E>0$ and $E=0$ . I tried substituting the expression of $V(x)$ at infinity in the integral: $$t(x)=\int_\left(x_0\right)^x\sqrt{\frac{m/2}{E+\frac{C}{x^\left(2a\right)}}}dx+t_0$$ I was trying to convert it in the form of $\arcsin(x)+c=\int\frac{1}{\sqrt{1-x^2}}dx$ by substitution but it has become apparent to me that it isn't possible perhaps I'm not allowed to directly substitute the expression of $V(x)$ at infinity. I also think there is a way around this question without having to compute the integral but i can't seem to find one. Hope somebody can help me.","['nonlinear-analysis', 'ordinary-differential-equations']"
3795474,Computing a 2 variable integral - switching the order of integration,I have to compute this integral: $$\int_0^1 dy \int_{\sqrt{y}}^{1} e^{\frac{y}{x}} dx$$ Because we have not learn how to compute $\int e^{a}{x} dx$ (because it has something with gamma function etc..) it makes me think only of one option and is to flip the $dx \Leftrightarrow dy$ $\sqrt{y} = x \Rightarrow  y = x^2$ and thus $$ \int_0^1 dx \int_{x^2}^1 e^{\frac{y}{x}}dy = \int_0^1 dx (\frac{1}{x}e^{\frac{1}{x}} - \frac{1}{x}e^x)$$ Which again leads me to this gamma function.. ( $\Gamma$ ...) and we don't know how to work with it (not in our syllabus) Any help would be appreciated!! Thanks!,"['integration', 'multivariable-calculus', 'calculus']"
3795482,$2^{\aleph_0} \geq \aleph_1$,I've read that $2^{\aleph_0} \geq \aleph_1$ by Cantor's theorem. Can someone please elaborate further? I know that $|\mathbb{R}| = 2^{\aleph_0}$ but I cannot find a connection to $\aleph_1$ especially not with Cantor's theorem.,"['elementary-set-theory', 'cardinals', 'set-theory']"
3795494,"Number of permutations of $D,D,D,O,O,O,G,G,G$ such that no two $D$ are adjacent and no two $G$ are adjacent","I have the following problem.  I already solved it but I am not happy with my solution.  I hope for a more slick solution and maybe somebody can help with it.  Is there a way to solve this problem with just the stars/bars method? Compute the number of ways to permute all letters from $D,D,D,O,O,O,G,G,G$ such that no two $D$ are adjacent and no two $G$ are adjacent. My attempt. For $k=2,3$ , let $\Delta_k$ denote the set of permutations s.t. only $k$ of $D$ are adjacent, and let $\Gamma_k$ denote the set of permutations s.t. only $k$ of $G$ are adjacent.  Let $U$ be the set of all permutations without any conditions. Then $$|U|=\frac{(3+3+3)!}{3!3!3!}=1680.$$ We have $$|\Delta_3|=\frac{(1+3+3)!}{1!3!3!}=140$$ (as $\Delta_3$ is the set of permutations of $DDD,O,O,O,G,G,G$ ), and $$|\Delta_2|+2|\Delta_3|=\frac{(1+1+3+3)!}{1!1!3!3!}=1120$$ since this number counts the permutations of $DD,D,O,O,O,G,G,G$ .  Therefore $$|\Delta_2|=1120-2(140)=840.$$ Similarly $|\Gamma_3|=140$ and $|\Gamma_2|=840$ . We want to find $|\Delta_i\cap\Gamma_j|$ .  Since $\Delta_3\cap\Gamma_3$ is the set of permutations of $DDD,O,O,O,GGG$ , $$|\Delta_3\cap\Gamma_3|=\frac{(1+3+1)!}{1!3!1!}=20.$$ Since $|\Delta_2\cap\Gamma_3|+2|\Delta_3\cap\Gamma_3|$ counts the permutations of $DD,D,O,O,O,GGG$ , we have $$|\Delta_2\cap\Gamma_3|+2|\Delta_3\cap\Gamma_3|=\frac{(1+1+3+1)!}{1!1!3!1!}=120$$ so $$|\Delta_2\cap\Gamma_3|=120-2(20)=80.$$ Similarly $|\Delta_3\cap\Gamma_2|=80$ . We now want to find $|\Delta_2\cap\Gamma_2|$ .  Since $|\Delta_2\cap\Gamma_2|+2|\Delta_3\cap\Gamma_2|+2|\Delta_2\cap\Gamma_3|+4|\Delta_2\cap\Gamma_3|$ counts the number of permutations of $DD,D,O,O,O,GG,G$ , we get $$|\Delta_2\cap\Gamma_2|+2|\Delta_3\cap\Gamma_2|+2|\Delta_2\cap\Gamma_3|+4|\Delta_2\cap\Gamma_3|=\frac{(1+1+3+1+1)!}{1!1!3!1!1!}=840.$$ Hence $$|\Delta_2\cap\Gamma_2|=840-2(80)-2(80)-4(20)=440.$$ By inclusion-exclusion principle we have $$|\Delta_2\cup\Delta_3\cup\Gamma_2\cup\Gamma_3|=\sum_{k=2}^3|\Delta_k|+\sum_{k=2}^3|\Gamma_k|-\sum_{i=2}^3\sum_{j=2}^3|\Delta_i\cap \Gamma_j|.$$ Therefore $$|\Delta_2\cup\Delta_3\cup\Gamma_2\cup\Gamma_3|=2(840)+2(140)-(440+80+80+20)=1340.$$ The question asks for the size of $U\setminus(\Delta_2\cup\Delta_3\cup\Gamma_2\cup\Gamma_3)$ , which is $$|U|-|\Delta_2\cup\Delta_3\cup\Gamma_2\cup\Gamma_3|=1680-1340=340.$$","['permutations', 'combinatorics-on-words', 'alternative-proof', 'solution-verification', 'combinatorics']"
3795500,Almost Sure Convergence and Lacunary Sequences,"Is there an example of a sequence $X_n$ of random variables so that for every lacunary sequence $n_k$ it holds that $X_{n_k}$ converges almost surely to $0$ , but $X_n$ does not converge almost surely to $0$ ? A sequence $n_k$ is lacunary when there exists a $\lambda > 1$ so that $n_{k+1} > \lambda n_k$ for all $k$ .",['probability-theory']
3795526,Proof of the Fundamental Theorem of Space curves using Rigid Transformation by Peter Baxandall ( Vector Calculus ),"I am reading Vector calculus by Peter Baxandall which proves the Fundamental theorem of Space curves ( Curves with equal torsion and curvature are identical except probably their position) in the following manner: In the proof, the author says : Pick any $p \in E$ . Hold $C_g$ fixed and move $C_h$ rigidly in $\Bbb R^3$ until $T_h(p) = T_g(p) , \cdots$ . I do not see very clearly the motive and mechanism by which the author is able to do so. I understand rigid transformation as something which preserves the length of the curve. However, we may even have to employ rotation to make the unit tangent vector $T_g$ and $T_h$ the same. But, in the last line, he ultimately says that $C_h$ is a translation of $C_g$ . Also, I couldn't find where has the author used the fact that the torsion and curvatures of the two curves are equal . $$\phi = T_g \cdot T_h + N_g \cdot N_h + B_g \cdot B_h \\ \implies \phi' = T_g' \cdot T_h + T_g \cdot T_h' + N_g' \cdot N_h + N_g \cdot N_h' +  B_g' \cdot B_h + B_g \cdot B_h'$$ . But since, we have already have : $T_g=T_h,N_g=N_h,B_g=B_h$ , thus : $T_gT_h'=0=T_g'T_h$ . Similarily, for others, each dot product turns out to be $0$ . We haven't seemed to use the fact that the torsions and curvatures of the two curves are equal ? Could someone please explain what's actually going on. Thanks a lot! NOTE : $T,N,B$ represent the tangent, normal and bi-normal unit - vector respectively","['curves', 'multivariable-calculus']"
3795551,"Prove that any injective function from $\{ 1, \dots, n \}$ to itself is bijective.","This is Exercise 1 from page 50 of Analysis I by Amann and Escher. I have found similar questions here and here but neither of those questions has a solution that uses what's hinted at in the text. Exercise: My attempt: It seems simple to argue that, since an injective function sends each element in its domain to a different element in the codomain, it has to ""hit"" all the elements in $\{ 1, \dots, n \}$ . I am not sure if this is formal enough, and at any rate it doesn't use the hint given. If I use the hint, then the base case of an injective function $\{ 1 \} \to \{ 1 \}$ is definitely bijective. Assume that any injective function from $\{ 1, \dots, n \}$ to $\{ 1, \dots, n \}$ is bijective, and consider the injective function $f \colon \{ 1 , \dots, n + 1 \} \to \{ 1 , \dots, n + 1 \}$ as described. We want to show that $f$ is bijective. It seems to me that there are at least two basic ways to show that $f$ is bijective. First, we can show that it's surjective, which involves considering some element $l \in \{ 1 , \dots, n + 1 \}$ and showing there exists an element $m$ in the same set such that $f(m) = l$ . The second way is to show that there exists a function $i$ such that $f \circ i$ is the identity function. However, an inductive proof should really use the inductive assumption, and I'm not sure either of these tactics do. I find the hint given pretty mystifying, but I have collected a few thoughts regarding the hint below. I see that $g$ is bijective. It is almost the identity function except that it sends $k$ to $n + 1$ and $n + 1$ to $k$ . Since $f$ and $g$ are injective, $h$ is also injective. I also see that $g$ undoes what $f$ does to $n + 1$ , hence $h(n + 1) = n + 1$ . The function $h$ is almost the same as $f$ , except for the swapping done by $g$ as described in 1. The restriction $h \mid \{ 1, \dots, n\}$ does not send any element to $n + 1$ , because the only element that $h$ sends to $n + 1$ is $n + 1$ , and $n + 1$ is outside the restriction. I have no idea how to cobble this into a proof. I appreciate any help.","['elementary-set-theory', 'induction', 'functions', 'solution-verification']"
3795555,A sum of series with the inverse squared central binomial coefficient,"A nice challenge by Cornel Valean: Show that $$2\sum _{n=1}^{\infty }\frac{2^{4 n}}{\displaystyle n^3 \binom{2 n}{n}^2}-\sum _{n=1}^{\infty }\frac{2^{4 n}}{\displaystyle n^4 \binom{2 n}{n}^2}+\sum _{n=1}^{\infty }\frac{2^{4 n} H_n^{(2)}}{\displaystyle n^2 (2 n+1) \binom{2 n}{n}^2}=\frac{\pi^3}{3}.$$ I have to say that I am not experienced in series involving squared central binomial coefficient, so I leave it for people who are experts in such series. All approaches are appreciated. Thank you.","['integration', 'real-analysis', 'harmonic-numbers', 'binomial-coefficients', 'sequences-and-series']"
3795564,"Find all finite groups $G$ s.t for any $a,b\in G$ either $a$ is a power of $b$ or $b$ is a power of $a$","Find all finite groups $G$ s.t for any $a,b\in G$ either $a$ is a power of $b$ or $b$ is a power of $a$ I think i showed that all such groups are $Z_{p^n}$ for $p$ prime, is this correct? I first showed that the group must be cyclic by considering the element of the largest order $\langle a\rangle$ and achiveing contradiction if $\langle a\rangle\not= G$ ., and then that if $Z_n$ with $n$ composite then it does not have this property. as there are two disjoint cyclic subgroups of coprime orders. Is this correct? Are all groups such groups $Z_{p^n}$ ?",['abstract-algebra']
3795565,Show that $f(x) = x|x|$ is continuous and differentiable - solution verification?,"Another exercise I did without any solutions. I highly doubt this is correct so pls correct me :) Let $f: \mathbf{R} \rightarrow \mathbf{R}$ be given by $f(x):=x|x| .$ Show that $f$ is continuous and differentiable on $\mathrm{R}$ $$
\begin{array}{l}
\text { Continuous: } \lim _{x \rightarrow c} f(x)=f(c) \\
\begin{aligned}
\lim _{x \rightarrow c} x \cdot|x| &=\lim _{x \rightarrow c} x \cdot \lim _{x \rightarrow c}|x|=f(c) \\
&=\lim _{x \rightarrow c} c \cdot \lim _{x \rightarrow c}|c|=f(c) \\
&=c \cdot|c|=f(c)=c \cdot|c|
\end{aligned}
\end{array}
$$ So $f(x)$ is continuous Differentiable: show $f^{\prime}(x)$ exists atall $x \in \mathbb{R}$ : $$
\begin{array}{l}\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h} \\ 
\lim _{h \rightarrow 0} \frac{(x \cdot|x|)+h-(x \cdot|x|)}{h} \\ =\lim _{h \rightarrow 0} \frac{h}{h}=1\end{array}
$$ $$
So f(x) \text { is differentiable }
$$","['continuity', 'calculus', 'solution-verification', 'derivatives']"
3795602,Inverse image; cut of domain of an increasing function,"I want to proof the following set theory exercise (from C. Pinter) Let $A$ and $B$ be partially ordered classes, and let $f : A  B$ be an increasing function; assume $\vec{f}(A)=B$ . Prove that if $(L, U)$ is a cut of $B$ , then $(f^{-1}(L), f^{-1}(U))$ is a cut of $A$ . I tried it by contradiction, but it came to nothing. I appreciate tips to proof it","['elementary-set-theory', 'order-theory']"
3795618,Multiplicity of Complex Conjugates of Repeated Complex Eigenvalues,"I know that for a real-valued matrix, complex eigenvalues come in complex conjugate pairs. However, I'm wondering what happens for repeated complex eigenvalues (i.e. complex eigenvalues with multiplicity greater than 1). In that case, does the complex conjugate of the repeated complex eigenvalue have the same multiplicity as that eigenvalue? If that statement holds, how can we show that it's true?","['matrices', 'linear-algebra', 'complex-numbers', 'eigenvalues-eigenvectors']"
3795638,Find the probability that at least one valve is defective when two valves are drawn at random.,"Problem Statement: A factory A produces $10\ \%$ defective valves and another factory $B$ produces $\mbox{$20\ \%$}$ defective valves. A bag contains $4$ valves of factory $A$ and $5$ valves of factory B. If two valves are drawn at random from the
bag, find the probability that at least one valve is defective. This question has been asked here once. Find the probability that atleast one valve is defective. Now, clearly Probability of drawing atleast one defective valve)=1- Probability that both valves drawn are non-defective valves Using this, the required probability is $
=1-\left(\frac{\binom{4}{2}}{\binom{9}{2}}(0.9)^2+\frac{\binom{5}{2}}{\binom{9}{2}}(0.8)^2+\frac{\binom{4}{1}\binom{5}{1}}{\binom{9}{2}}(0.9)(0.8)\right)=\frac{517}{1800},
\label{1}\tag{1}
$ as OP has also shown in the above linked post. However, this is where the confusion arises: Alternatively, let's consider the following mutually exclusive events: Both valves are defective and are from factory $A$ Both valves are defective and are from factory $B$ Both valves are defective (one from $A$ and the other from $B$ ) One valve is drawn from $A$ and is defective while the other is drawn from $B$ and is non-defective. One valve is drawn from $B$ and is defective while the other is drawn from $A$ and is non-defective. Now the required probability = Sum of all the probabilities of mutually exclusive events listed above. Let's denote probability for the ith event listed above by $P(i)$ , where $i=1,2,3,4,5$ $P(1)=\frac{^4C_2}{^9C_2}(0.1)^2\;\;,P(2)=\frac{^5C_2}{^9C_2}(0.2)^2\;\;,P(3)=\frac{^4C_1 \times ^5C_1}{^9C_2}(0.1)(0.2)\;\;$ $P(4)=\frac{^4C_1\times ^5C_1}{^9C_2}(0.1)(0.8)\;\; ,P(5)=\frac{^4C_1 \times ^5C_1}{^9C_2}(0.9)(0.2)\;\;$ Therefore, the required probability is $$P=\sum_{i=1}^{5}P(i)=1/600+1/90+1/90+2/45+1/10=303/1800
\label{2}\tag{2}$$ I want to know why answers in (\ref{1}) and (\ref{2}) above are different . In fact, in the link above, OP had mentioned that answer given in his book was $303/1800$ , whereas the comments and answers to that post mentioned the answer as wrong. But $(2)$ clearly shows that there is nothing wrong with the answer. Please help. Thanks.","['solution-verification', 'combinatorics', 'probability']"
