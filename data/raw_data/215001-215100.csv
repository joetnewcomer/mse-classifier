question_id,title,body,tags
4360475,Kelly betting: why do we maximize the expected value of the logarithm of wealth?,"Introduction The Kelly betting criterion is a betting strategy for repeated games of chance which works by wagering a fixed proportion of one's bankroll each time. That is, suppose I play a game of chance where I wager an amount $a$ , win with probability $p$ and payout $(1+b)a$ , and lose with probability $(1-p)$ with payout $0$ . The Kelly betting criteria then advises wagering the proporition $$p+\frac{p-1}{b}$$ of my bankroll at each opportunity, and this formula is derived from maximizing the expected value of the logarithm of wealth. See Wikipedia for more details. Question How do we get to the point of deciding to maximize the expected value of the logarithm of wealth? I'm also interested in why one should choose such a strategy - why should we just bet a fixed amount each time and not adjust our strategy to events as they occur? Work My (probably somewhat naïve) attempt at working this out has an error somewhere but I'm not sure where it is. My idea is that if one bets a fixed proportion, one can actually calculate the expected change in bankroll over some fixed term: if we bet a proportion $f$ of our bankroll each time, winning multiplies our stash by $1+bf$ and losing multiplies it by $1-f$ . Starting with, say, one unit of money, and running for $n$ trials, the expected value to finish with is $$2^{-n}\sum_{i=0}^{n} \binom{n}{i} p^i(1-p)^{n-i}(1+bf)^i(1-f)^{n-i}=2^{-n}(1+f(bp+p-1))^n$$ which doesn't maximize at all like the Kelly criteria. What error am I making here?","['gambling', 'probability']"
4360477,Prove uniqueness of solutions of different OLS matrix cases,"Let $D = \{(x_1, y_2), (x_2, y_2), \ldots , (x_n, y_n)\}$ where $x_i \in \mathbb{R}^d$ and $y_i \in \mathbb{R}$ . One may use linear regression to predict $y$ as $w^Tx$ for some parameter vector $w \in \mathbb{R}^{d}$ . Consider matrix $X \in \mathbb{R}^{n\times d}$ with $x_i$ as rows and the vector $y \in \mathbb{R}^n$ as the vector of $y_i$ . Given an OLS loss function $$ \arg\min_w \hat{R}(w)=\arg\min_w \sum^n_{i=1}(y_i - w^Tx_i)^2$$ a) Show that for $n < d$ that the OLS loss function does not admit a unique solution b) Under what assumptions on $X$ does this equation admit a unique solution $w^*$ ? i. There exists a unique solution if $n \geq d$ and the columns of $X$ are independent ii. There exists a unique solution $\iff$ $X^TX$ is invertible iii. For $n > d$ , there will always be a unique solution if $X$ is full rank. I'm a little unsure how to write a proof on this question. My initial idea is to derive a unique solution under the assumption that $X^TX$ is invertible and use the properties of linear systems to that a) will have $\mathrm{rank}(w^*) < d$ and thus be inconsistent and b) will hold under assumptions ii. Is there a better/more formal way for proving a) and b)?","['statistics', 'matrix-rank', 'linear-algebra', 'linear-regression', 'matrix-equations']"
4360564,Algebraic proof that $\sum\limits_{k=0}^{n} {n \choose k}\cdot \frac{(-1)^k}{(k+1)(k+2)} = \frac{1}{n+2}$,"I tried evaluating the integral $\int\limits_{[0,1]^n} \min(x^1,x^2,\ldots,x^n) \lvert d^nx\rvert$ . In my first attempt, I used a recursive approach and managed to defined the integral as being $I_n^k = \int\limits_{[0,1]^n} \min(x^1,x^2,\ldots,x^n)^k \lvert d^nx\rvert$ , where I could evaluate $I_n^k = I_{n-1}^ k - \frac{k}{k+1} \cdot I_{n-1}^{k+1}$ , and $I_1^k = \frac{1}{k+1}$ . Using this recursive approach, I managed to extract the $I_n^1 = \sum\limits_{k=0}^{n-1} {n-1 \choose k}\cdot \frac{(-1)^k}{(k+1)(k+2)}$ I plugged numbers into this sum for multiple values of $n$ , and saw that I constantly get $\frac{1}{n+1}$ . During my second attempt, I managed to eventually find the integral by splitting the area into $n!$ areas in which there exists some order for each element $x_0$ such that $x_0^{i_1} \leq x_0^{i_2} \leq \ldots \leq x_0^{i_n}$ , and proved that the integral over each of these area is $\frac{1}{(n+1)!}$ which gave me showed me more definitively that the integral is equal to $\frac{1}{n+1}$ . That said, after trying for a while, I couldn't come up with any combinatorial/algebraic proof that the sum I found is indeed $\frac{1}{n+1}$ . I tried evaluating it as a telescoping sum, giving me the expression $\sum_{k=0}^{\frac{n}{2}}{n \choose 2k}\cdot\frac{\left(4k+3-n\right)}{\left(2k+3\right)\left(2k+2\right)\left(2k+1\right)}$ , but couldn't expand this sum to anything useful either. I haven't worked much with sums of this form and was wondering whether I'm missing something that can help me show this without the integral.","['summation', 'binomial-coefficients', 'combinatorics']"
4360572,Finding the approximate value of improper integral using the Monte-Carlo method,"Find the approximate value of the improper integral $$
\int_{-3}^{\infty} \left( \int_{0.5}^{\infty} \frac{2+\sin(x+y)}{e^{0.4x}+0.4y^2} \,dx \right) \, dy
$$ using the Monte-Carlo method. I have managed to define the necessary functions in R in order to apply the MC method but the problem lies in the function under the integral itself. I have tried to use well-known distributions (exponential and normal) to rewrite the function and get an idea from which distributions should I generate $x$ and $y$ but due to lack of luck in doing that it seems like a wrong direction, perhaps there is an easier way. So any help, hints, tips and tricks on how to solve the problem would be greatly appreciated.","['integration', 'monte-carlo', 'probability-distributions', 'improper-integrals']"
4360651,$\sum_{\sigma\in S_n} e^{-\frac{i\pi}{2m} \sum_{1\leq i<j\leq n} \mathrm{sgn}(\sigma(i)-\sigma(j))}$ is zero for $n\geq 2m$?,"For $m,n\in \mathbb N$ , I am interested in $$f(m,n) = \sum_{\sigma\in S_n} e^{-\frac{i\pi}{2m} \sum_{1\leq i<j\leq n} \mathrm{sgn}(\sigma(i)-\sigma(j))},$$ where $S_n$ is the group of all permutations. This function is motivated from physics. By numerical method, I compute the above function for small $m,n$ as follows: I observe several remarkable things: $f(m,n)=0$ for $n\geq 2m$ . $f(m,2m-2)=f(m,2m-1)$ . $f(m,n) \in \mathbb R$ . Why this fact holds? Also, is there any other pattern of $f(m,n)$ for $n<2m$ ? Any result on $f(m,n)$ will be appreciated. Remark: I used the following Mathematica code to obtain the above table: f[m_, n_] := 
 Sum[ Exp[-I Pi/(2 m) Sum[ 
      Sign[x[[i]] - x[[j]]], {i, 1, n}, {j, i + 1, n}]], {x, 
    Permutations[Range[n]]}] // ComplexExpand Motivation: I briefly explain how $f(m,n)$ came up in physics. In the edge of fractional quantum Hall liquid, the fundamental excitation is anyon. The anyon has charge $1/m$ -th of the electron, and is described by the ""anyon operator"" $\psi(x) = e^{i\frac{1}{\sqrt m}\phi(x)}$ . If there are $n$ anyons, we consider certain correlator $$\langle \psi(x_1) \cdots \psi(x_n) \psi^\dagger(y_1) \cdots \psi^\dagger(y_n)\rangle,$$ which can be understood as how $n$ anyons want to occupy the same state.
When I tried to evaluate this expression, I came up with $f(m,n)$ , mainly due to the commutation relation $$\psi(x) \psi(x') = e^{-\frac{i\pi}{m}\mathrm{sgn}(x-x')} \psi(x') \psi(x).$$ The fact that $f(m,n)=0$ for $n\geq 2m$ can be naturally interpreted to the ""Pauli exclusion of $2m$ anyons""; $2m-1$ anyons can occupy the same state, but not for $2m$ anyons.","['permutations', 'summation', 'combinatorics']"
4360653,"Revisit : $20\choose 5$ subsets without 3,4 or 5 consecutive numbers","Addendum-2 just added to my question. Addendum just added to my question. $\underline{\textbf{Overview}}$ This is a self-answer question of this original question .
I strongly suspect that the original question will soon be closed and then deleted. I’m trying to get the amount of combinations of 5 numbers from one to twenty
without duplicates and without 3,4,and 5 consecutive running numbers. $\underline{\textbf{Clarification}}$ Let $N = \{1,2,\cdots, 20\}$ . How many distinct subsets of $N$ are there where: The subset has exactly $5$ elements. The subset does not contain $3$ consecutive elements. Here, consecutive elements are elements $(k), (k+1), (k+2).$ For example, both of the following sets are satisfactory: $\{1, 2, 4, 5, 7\}$ $\{1, 3, 5, 7, 9\}$ . Further, each of the following sets are unsatisfactory: $\{1, 2, 3, 14, 18\}$ $\{2, 3, 4, 5, 17\}$ $\{8, 9, 10, 11, 12\}$ . $\underline{\textbf{My Background}}$ About $50$ years ago I took a Probability course in college and did ok.  I have
forgotten much of the theory, and usually rely exclusively on intuition to attack
Probability (or Combinatorics) problems. If relevant, some decades ago I survived but have forgotten much of: ""Real Analysis : Volume 1 : 2nd Ed."" (Apostol, 1966). The first $(2/3)$ of ""Elementary Number Theory"" (Uspensky and Heaslett, 1938)
[through quadratic reciprocity]. $\underline{\textbf{Problem Relevance}}$ In my experience, there are three typical approaches to this type of problem: The Direct Approach Recursion Inclusion-Exclusion This particular problem interested me, because of the challenge involved in providing three distinct solutions, one for each of the above approaches.  However, exploring Inclusion-Exclusion ,
I concluded that the math involved was too ugly to be reasonably feasible. However, I was able to find two distinct Direct Approaches to offer. $\underline{\textbf{My Work}}$ See my self - answers. For clarity, I have provided a separate answer for : A Direct Approach An Alternate Direct Approach Recursion Addendum Given the answers provided by others, it seems to me that the one pending challenge is to find some elegant solution that is based primarily on Inclusion-Exclusion. I would be very interested if someone could present such a solution. Edit Mike Earnest added an Inclusion-Exclusion response to his answer. Addendum-2 Finally conquered my own private Inclusion-Exclusion challenge for this problem.  Just added a separate Inclusion-Exclusion answer.","['combinatorics-on-words', 'inclusion-exclusion', 'combinatorics', 'recurrence-relations']"
4360685,About Fourier transform of a probability measure on $\mathbb{Z}_2^d$,"I am reading a paper by Diaconis and Graham and in page 219, in equation 2.5, Fourier transform of a probability measure $Q$ on $\mathbb{Z}^d_2$ is defined as: For $y\in \mathbb{Z}^d_2$ , the Fourier transform of $Q$ at $y$ is defined by: $$
\hat{Q}(y) = \sum_{x\in \mathbb{Z}_2^d} (-1)^{y^t x}Q(x)
$$ where $x^ty$ is a dot product of the vectors $x,y$ and here the dot product is taken mod $2$ . May I know why the Fourier transform is defined this way? Let's say $Q$ is a probability measure on $\mathbb{Z}_k^d$ , $k>2$ is an integer, then would the Fourier transform be defined the same way where we sum over $x\in \mathbb{Z}^d_k$ and the dot product is taken mod $k$ ? I know that the Fourier transform of a probability measure generally is $\hat{Q}(y)=\int e^{iyx}dQ(x)$ but because the state space is $\mathbb{Z}^d_2$ , somehow $e^{iyx}$ translates to $(-1)^{y^tx}$ which is don't understand?","['fourier-transform', 'probability-theory']"
4360704,Stochastic integral change of variable.,"Define $\alpha_t=\frac12\ln(1+\frac23 t^3)$ . If $B_t$ is a Brownian motion, prove that there exists another Brownian motion $\tilde{B}_r$ such that $$\int_0^{\alpha_t}e^sdB_s=\int_0^t rd\tilde{B}_r.$$ I would like to use this specific Lemma 4 which I found here Lemma 4 Let X be a semimartingale and ${\xi}$ be a predictable, X-integrable process. Suppose that ${\{\tau_t\}_{t\ge 0}}$ are finite stopping times such that ${t\mapsto\tau_t}$ is continuous and increasing. Define the time-changes ${\mathcal{\tilde F}_t=\mathcal{F}_{\tau_t}}, {\tilde X_t=X_{\tau_t}}$ and ${\tilde \xi_t=\xi_{\tau_t}}$ .
With respect to the filtration ${\mathcal{\tilde F}_t}, {\tilde X}$ is a semimartingale, ${\tilde\xi}$ is predictable and ${\tilde X}$ -integrable, and $\displaystyle \int_0^t\tilde\xi\,d\tilde X=\int_{\tau_0}^{\tau_t}\xi\,dX$ . By the lemma we get $$\int_0^{\alpha_t}e^sdB_s=\int_0^te^{\alpha_s}dB_{\alpha_s}=\int_0^t \sqrt{1+\frac23s^3}dB_{\alpha_s}$$ Now I want to somehow take care of $dB_{\alpha_s}$ . Maybe I can find a martingale $M_s$ with quadratic variation $\langle M\rangle_s=\alpha_s$ and then I would have $B_{\alpha_s}=B_{\langle M \rangle_s}=M_s$ but how do I find this martingale?","['martingales', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4360720,Generators of $H^1_{dR}(S^1)$ and angular forms,"I am trying to understand and ""visualize"" the generator of the first de Rham cohomology group of the circle and the intuition behind the concept of angular form. $H^1(S^1)$ can be computed from Mayer-Vietoris sequence obtaining $\dim H^1(S^1)= 1$ and from the coboundary map we obtain a generator $\omega_{MV}$ of $H^1(S^1)$ as a bump form with support (in a connected component of) the intersection of the two open sets used in M-V, that is, a bump form on the circle (say with integral equal to 1). Thus $H^1(S^1) = \langle [\omega_{MV}] \rangle$ . This form is sometimes called ""angular form"" but intuitively does not look like a ""differential"" of an angle (I know that the angle is a multifunction and hence we should use the universal cover of $S^1$ to properly discuss it, but we can get away with some careful intuition nonetheless).
Intuitively the angle ""function"" has a linear growth on the circle minus a point, thus I'd expect its ""differential"" to be a ""constant"" form. Of course constant does not make sense for a 1-form. We can restate this by asking that this angular form should evaluate to a constant function when paired with the vector field $X = x \frac{\partial}{\partial y} - y \frac{\partial}{\partial x}$ restricted to $S^1$ . Of course such a form can be seen as the restriction of $xdy - ydx$ to $S^1$ . Without referring to the ambient space, we can construct a ""constant"" form $\omega_c$ gluing the forms $c\cdot dx$ on $U,V \simeq \mathbb{R}$ as follows.
Say $U \simeq_{\varphi_U} (0-\varepsilon,\pi +\varepsilon)$ and $V \simeq_{\varphi_V} (\pi-\varepsilon,2\pi +\varepsilon)$ just to fix the ideas. We have $$\varphi_U(U\cap V) = (0- \varepsilon,0+\varepsilon) \sqcup (\pi -\varepsilon,\pi+\varepsilon)$$ $$\varphi_V(U\cap V) = (2\pi- \varepsilon,2\pi +\varepsilon) \sqcup (\pi -\varepsilon,\pi+\varepsilon)$$ and the transition functions are the identity on the first compoment and the translation $\pm 2\pi$ on the second component, thus their jacobian is always the identity. Moreover on $\mathbb{R}$ the forms $dx$ are translation invariant, i.e. $dx_p = dx_{p+q}$ , thus in the end $\varphi_{UV}^*dx = dx$ .
This defines $\omega_c$ as $\omega_c|_U = c \cdot \varphi_U^*dx$ and $\omega_c|_V = c \cdot \varphi_U^*dx$ . These forms are closed by definition and they are intuitively exact since if there was a function on $S^1$ with a constant differential, it must be constantly increasing, but this is impossible on $S^1$ . Thus $H^1(S^1) = \langle [\omega_{c}] \rangle$ . This form is what I would call angular form , of course, since $H^1$ is 1-dimensional, $[\omega_{c}] = [\omega_{MV}]$ thus by extension we call also $\omega_{MV}$ an angular form. Since $S^1$ is compact, we also have $H_c^1(S^1) = \langle [\omega_{c}] \rangle = \langle [\omega_{MV}] \rangle$ . Does my reasoning make sense? Thanks EDIT: I've rewritten the construction of ""constant"" 1-forms to make it clearer. Seems to me it that the use of partition of unity it not even necessary.","['differential-topology', 'de-rham-cohomology', 'differential-geometry']"
4360729,"Integral $\int_0^{\pi/3}\frac{x}{\cos(x)}\,dx$","I am trying to compute the integral $$\int_0^{\pi/3}\frac{x}{\cos(x)}\,dx \tag{1}$$ Context: Originally I was trying to prove the following result: $$\sum_{n=0}^\infty\frac{1}{(2n+1)^2\binom{2n}{n}}=\frac83\beta(2)-\frac{\pi}3\ln(2+\sqrt{3})\tag{2}$$ Where $\beta(2)$ is the Catalan´s constant To this end I started with the well known result $$\frac{\arcsin(x)}{\sqrt{1-x^2}}=\sum_{n=0}^\infty\frac{4^n x^{2n+1}}{(2n+1)\binom{2n}{n}} \tag{3}$$ Dividing both sides of $(3)$ by $x$ and integrating from $0$ to $1/2$ we obtain $$\int_0^{1/2}\frac{\arcsin(x)}{x\sqrt{1-x^2}}\,dx=\frac12\sum_{n=0}^\infty\frac{ 1}{(2n+1)^2\binom{2n}{n}} \tag{4}$$ So the task reduces to compute the integral in $(4)$ . Therefore $$
\begin{aligned}
\sum_{n=0}^\infty\frac{ 1}{(2n+1)^2\binom{2n}{n}}&=2\int_0^{1/2}\frac{\arcsin(x)}{x\sqrt{1-x^2}}\,dx\\
&=2\int_0^{\pi/6}\frac{x}{\sin(x)}\,dx &(x \to \sin(x))\\
&=2\int_0^{\pi/2}\frac{x}{\sin(x)}\,dx-2\int_{\pi/6}^{\pi/2}\frac{x}{\sin(x)}\,dx\\
&=4\beta(2)-2\int_{\pi/6}^{\pi/2}\frac{x}{\sin(x)}\,dx\\
&=4\beta(2)-2\int_{0}^{\pi/3}\frac{\left(\frac{\pi}{2}-x\right)}{\cos(x)}\,dx & (x \to \frac{\pi}{2}-x)\\
&=4\beta(2)-\pi\int_{0}^{\pi/3}\sec(x)\,dx+2\int_{0}^{\pi/3}\frac{x}{\cos(x)}\,dx\\
&=4\beta(2)-\pi\ln\left(\sec(x)+\tan(x) \right)\Big|_0^{\pi/3}+2\int_{0}^{\pi/3}\frac{x}{\cos(x)}\,dx\\
&=4\beta(2)-\pi\ln\left(2+\sqrt{3}\right)+2\int_{0}^{\pi/3}\frac{x}{\cos(x)}\,dx\\
&=4\beta(2)-\pi\ln\left(2+\sqrt{3}\right)+4\int_{0}^{\pi/3}\frac{x}{e^{ix}+e^{-ix}}\,dx\\
&=4\beta(2)-\pi\ln\left(2+\sqrt{3}\right)+4\int_{0}^{\pi/3}\frac{xe^{-ix}}{1+e^{-2ix}}\,dx\\
&=4\beta(2)-\pi\ln\left(2+\sqrt{3}\right)+4\int_{0}^{\pi/3}xe^{-ix}\sum_{k=0}^\infty(-1)^ke^{-2ikx}\,dx\\
&=4\beta(2)-\pi\ln\left(2+\sqrt{3}\right)+4\sum_{k=0}^\infty(-1)^k\int_{0}^{\pi/3}xe^{-ix(2k+1)}\,dx\\
\end{aligned}
$$ The integral in the last line is $(1)$ . I integrated by parts, but ended up with some nasty series not very promising.","['integration', 'catalans-constant', 'sequences-and-series']"
4360739,"If $x^2 \geq y^2$, is the same true for their absolute values?","Apologies to ask a question that may be extremely obvious. I've got a multi-variable function: $f(x,y) = \sqrt{x^2 - y^2}$ I've been asked to find (and sketch) the domain of this function: Because of the $\sqrt{}$ , we know that $x^2 - y^2 \ge 0$ This means that $x^2 \ge y^2$ Would it be wrong to therefore write the domain as: $D(f) = $ { $(x,y): |x| \ge |y|$ }? The model answer was written as: $D(f) = $ { $x \ge \pm y, x \ge 0$ } ∪ { $x \le \pm y, x \le 0 $ } Would I be right in assuming that the answer was written like this to make sketching the domain easier , and both answers are in fact correct? Any clarity on this would be of great help!","['multivariable-calculus', 'functions']"
4360745,"Find all function $f$ such that, for any sequence $(x_n)$ Cesàro-convergent, the sequence $(f (x_n))$ is also Cesàro-convergent","Let's say that a function $f$ is Cesàro-continuous at $x_0$ iff for any sequence $(u_n)_\Bbb{N}$ whose Cesàro mean converges to $x_0$ , the Cesàro mean of the sequence $(f(u_n))_\Bbb{N}$ converges to $f(x_0)$ . At first, I was interested in the following question Find all functions $f : \mathbb{R} \to \mathbb{R}$ Cesàro-continuous. I realized that it is a classic question and the answer is that $f$ must be affine. Now I'm interested in this question Find all function $f : \mathbb{R} \to \mathbb{R}$ such that, for any sequence $(x_n)$ Cesàro-convergent, the sequence, the sequence $(f(x_n))$ is also Cesàro-convergent? It seems to me that the two questions are equivalent, but I do not know if it's true and how to prove it. I need help. The idea of the question came to me by this question .","['convergence-divergence', 'functional-analysis', 'sequences-and-series']"
4360751,Is $\int_0^T\Delta_t^2(\omega)dt$ a Riemann integral or an Itô integral?,"I am studying stochastic integrals w.r.t. brownian motion and for a generic integrand process $\{\Delta_t\}_{t\in[0,T]}$ the following condition is required: $E\big[\int_0^T\Delta_t^2(\omega)dt\big]<\infty$ . I don’t quite understand what kind of integral that is, and how would you assure the condition is satisfied for a specific process. The same integral is also used to define the quadratic variation of the Itô integral: $[I,I]_t(\omega)=\int_0^t\Delta^2_u(\omega)du$ , a.s.","['stochastic-integrals', 'stochastic-calculus', 'stochastic-processes', 'probability-theory', 'probability']"
4360769,Why do I have to perform polynomial division when trying to find slant asymptotes,"When trying to find the slant asymptote of $\frac{2x^2+x}{x-3}$ , the way I thought was correct is to divide everything by $x$ to get $\frac{2x+1}{1-\frac{3}x}$ . All that was left was to say that as $x$ tends to $\infty$ , $\frac{3}x$ tends to $0$ , so the asymptote is $2x+1$ . Spoiler: it was not.
If I would do it the polynomial division way, I would get that the asymptote is $2x+7$ . My question is: what is wrong with my way? Helpful link though it did not answer my question here","['limits', 'rational-functions', 'asymptotics', 'real-analysis']"
4360774,Writing a random variable as the sum of independent random variables,"Let $X_i$ be independent random variables with $X_i \sim Po(1)$ . We know (for instance by looking at characteristic functions) that then $\sum_{i=1}^{n} X_i \sim Po(n)$ . I am interested in the converse of that implication.
Let $Y \sim Po(n)$ . Can we find independent random variables $Y_1, ..., Y_n$ with $Y_i \sim Po(1)$ such that $\sum_{i=1}^{n} Y_i=Y$ ?","['statistics', 'poisson-distribution', 'independence', 'probability', 'random-variables']"
4360811,volume of region in a probability simplex with shannon entropy > c?,"Let $S_n = \{(p_1, \cdots, p_n)| p_i\geq 0, \sum_{i=1}^n p_i = 1\}$ . I.e. $S_n$ is a $n-1$ -simplex.  Let $$
E(p_1, \cdots, p_n) = -\sum_{i=1}^n p_i\ln p_i
$$ be the Shannon entropy. I am stuck with the question, what is the $n-1$ -Volume of $\{(p_1, \cdots, p_n)\in S_n \mid E(p_1, \cdots, p_n)>a\}$ ? I mean I am interest in integral $$
\int_{X}dV
$$ where $X= \{(p_1, \cdots, p_{n-1}) \mid p_i\geq 0, p_n = 1-\sum_{i=1}^{n-1}p_i\geq 0, E(p_1, \cdots, p_n)>a\}\subset {\mathbb R}^{n-1}$ Any non-trivial bounds are also interesting.","['entropy', 'probability-distributions', 'probability-theory', 'information-theory']"
4360816,Prove: $\frac{f(x+g(x))-f(g(x))}{x} \to f'(0)$ as x approaches $0$,"I was given this question: Prove $$\frac{f(x+g(x))-f(g(x))}{x} \xrightarrow{x\to0} f'(0)$$ if $|g(x)|\le|x|$ and $f(x)$ is differentiable at zero. There's a very similar question here: Prove $\lim_{x \rightarrow 0} \frac{f(x+g(x))-f(g(x))}{x}=f'(0)$ But in there f is differentiable everywhere, nevertheless here $g(x)\to 0$ and smaller than $x$ , but I couldn't find a way to solve it as I can't use the mean value theorem, or say anything about $f'(x)$ without the definition. You can't use integrals in your answer because we haven't learned them yet in class. Any help is greatly appreciated","['functions', 'derivatives', 'real-analysis']"
4360853,Proving a transformation of the interval is ergodic,"Consider the function $f(x):[0,1]\rightarrow[0,1]$ given by $$\begin{cases}2x & 0 \leq x \leq\frac{1}{2}\\x-\frac{1}{2 } &\frac{1}{2}< x\leq 1 \end{cases}$$ I found the measure with density given by $\rho=\frac{4}{3}\chi_{[0,\frac{1}{2}]}+\frac{2}{3}\chi_{(\frac{1}{2},1]}$ (with respect to Lebsegue measure) is invariant for this transformation.
My question now is: how can I prove this system with this measure is ergodic?
I thought to use the approach with invariant functions and Fourier series, but I'm not sure on how to write Fourier expansion with a measure different than Lebesgue's. I also thought to exploit a possible conjugacy with symbolic shift, but wasn't able to prove that $[0,\frac{1}{2}]$ and $(\frac{1}{2},1]$ constitute a Markov partition of the unit interval.
Any ideas?","['measure-theory', 'ergodic-theory', 'dynamical-systems']"
4360876,About Usage of Lebesgue Dominated Convergence Theorem,"Let $(X, \mathcal{M}, u) $ be a measure space, $f$ and each $f_n$ be integrable and non-negative, $f_n$ converges to $f$ a.e. and $\lim \int f_n$ = $ \int f.
$ Prove that for each measurable set $A \in \mathcal{M} $ , $\lim \int_A f_n$ = $ \int_Af$ My attempt : Here, I first define $g_n = f_n \chi_A $ and $g = f\chi_A$ . Of course, $\lim g_n$ = $ g$ a.e. since $\lim f_n$ = $ f$ . Also, $g_n \leq 2f$ , where f is integrable, by dominated convergence theorem we can put lim inside and get the result : $\lim \int g_n$ = $\lim \int_A f_n$ = $ \int g$ = $ \int_Af$ My question is the following : I am note sure if $2f$ dominates $g_n$ s and also I wonder where we will use the fact that $\lim \int f_n$ = $ \int f.
$ ? I will be glad if you help","['measure-theory', 'lebesgue-integral', 'real-analysis']"
4360883,Integral curve definition in terms of the tangent vector: why $D \varphi_{t} (d/dt) = X_{\varphi(t)}$?,"I have this definition of an integral curve from these notes , page 29, on differentiable manifolds. With a manifold $M$ , an integral curve of a vector field $X$ is a smooth map $\varphi: (\alpha,\beta) \subset \mathbb{R} \to M$ such that $$
D \varphi_{t} \left(\frac{d}{dt}\right) = X_{\varphi(t)}
$$ An example is given where $M = \mathbb{R}^{2}$ , and so the derivative of the smooth function $\varphi(t) = (x(t),y(t))$ is $$
D \varphi\left(\frac{d}{dt}\right) = \frac{dx}{dt}\frac{\partial}{\partial x} + \frac{dy}{dt}\frac{\partial}{\partial y}
$$ What is going on here with the $d/dt$ in the brackets on the final line? i.e. this part $$
D\varphi\left(\underbrace{\frac{d}{dt}}_{\text{this part here}}\right) = \dots
$$ I am assuming there is a function $f: M \to \mathbb{R}$ on which $D\varphi(d/dt)$ acts, as $D\varphi(df/dt)$ , but I thought $\varphi$ takes in real numbers i.e. the time parameter, since its defined as a map $(\alpha,\beta) \subset \mathbf{R}$ ? Perhaps it's not a function, just a term in brackets to multiply $D\varphi$ by?","['tangent-spaces', 'smooth-manifolds', 'differential-geometry']"
4360965,Dual of the Sobolev space,"It is well known that for a given bounded domain $\Omega$ , the Sobolev space $W^{1,2}(\Omega)$ is a Hilbert space, which is the space given by $$
W^{1,2}(\Omega)=\{u\in L^2(\Omega):\nabla u\in L^2(\Omega)\}
$$ under the norm $$
\|u\|_{W^{1,2}(\Omega)}=\|u\|_{L^2(\Omega)}+\|\nabla u\|_{L^2(\Omega)}.
$$ Then by Riesz representation theorem, the dual of this space should be isomorphic to the space itself. But I have seen in PDE books, the dual of $W^{1,2}(\Omega)$ is a bigger space than $W^{1,2}(\Omega)$ , which is also not isomorphic to $W^{1,2}(\Omega)$ , if I understood correctly. I could not understand the reason. Can someone please help me to understand the concept of it? Thank you.","['sobolev-spaces', 'functional-analysis', 'riesz-representation-theorem', 'partial-differential-equations']"
4360976,Natural group action on mapping torus,"Let $ (F,g) $ be a Riemannian manifold. Let $ G:=Iso(F,g) $ be the isometry group. Let $ M $ be the mapping torus of some isometry of $ F $ . So we have a bundle $$
F \to M \to S^1
$$ $ M $ has Riemannian cover $ F \times \mathbb{R} $ and there is natural action of $ G \times \mathbb{R} $ on $ F \times \mathbb{R} $ . If the mapping torus is trivial $ M\cong  F \times S^1 $ then this action on the cover descends to a natural action on the mapping torus. What if the mapping torus is nontrivial? When is there a natural action of $ G \times \mathbb{R} $ on $ M $ ? I am especially interested in the case where $ F $ is Riemannian homogeneous and this action on the mapping torus is transitive. I was inspired to ask this by a claim in this question https://mathoverflow.net/questions/410547/exact-condition-for-smooth-homogeneous-to-imply-riemannian-homogeneous and a similar claim in this question https://mathoverflow.net/questions/413409/mapping-torus-of-orientation-reversing-isometry-of-the-sphere that there is a natural action of the group $ O_{n+1}(\mathbb{R}) \times \mathbb{R} $ on the mapping torus of the antipodal map on $ S^n $ .","['riemannian-geometry', 'smooth-manifolds', 'geometric-topology', 'lie-groups', 'differential-geometry']"
4360997,Quadratic-trigonometric integral -- part 2,"Problem I need to compute the following integral \begin{equation*}\int_{t_\text{s}}^{t_\text{e}} \cos(a+b\tau+c\tau^2)\text{ d}\tau\end{equation*} where $t_{\text{s}}<t_{\text{e}}$ and $a,b,c>0$ are given parameters. Remark Actually, thanks to the help of @egglog and @Bobby Laspy , I've got a clear solution expressed in terms of the so-called Fresnel integrals \begin{equation*}C(t)\triangleq \int_0^t \cos(\tau^2)\text{ d}\tau \qquad S(t)\triangleq \int_0^t \sin(\tau^2)\text{ d}\tau\end{equation*} Indeed this post is a continuation of this previous one . Questions From what I've understood, the Fresnel integrals are somewhat related to $\text{erf}(\cdot)$ function which, as an engineer, is more familiar to me. Wikipedia presents an expression of $C(\cdot)$ and an expression of $S(\cdot)$ in terms of the $\text{erf}(\cdot)$ function, which are \begin{equation*}\begin{aligned}
S(z)&=\sqrt{\frac{\pi}{2}}\frac{1+i}{4}\left[\text{erf}\left(\frac{1+i}{\sqrt{2}}z\right)-i\,\text{erf}\left(\frac{1-i}{\sqrt{2}}z\right)\right]\\
C(z)&=\sqrt{\frac{\pi}{2}}\frac{1-i}{4}\left[\text{erf}\left(\frac{1+i}{\sqrt{2}}z\right)+i\,\text{erf}\left(\frac{1-i}{\sqrt{2}}z\right)\right]
\end{aligned}\end{equation*} where, I believe, $i$ is the imaginary unit and the input $z$ is complex.
A first question is the following: 1) what are the expressions of the (not complex) $C(\cdot)$ and $S(\cdot)$ in terms of $\text{erf}(\cdot)$ ? The answer to the first question can be obtained from the two expressions above by simply putting inside them $z=t$ real, but I would like to have an explanation (because I don't know if I have correctly understood the underlying theory) and so I'm trying to find a proof for the expressions above. Moreover, it is not clear if the output of the previous expressions is real if $z=t$ is real, so a second question is the following: 2) Why for $z=t$ real the expressions above give a real output? My attempt to prove the expressions, which is below, give rise to a third question: 3) In the derivations of the results are involved the square roots of $\text{j}$ (imaginary unit  expressed in my favorite notation) and $-\text{j}$ . Both $\text{j}$ and $-\text{j}$ have two square roots, so what roots I have to consider and why? Finally, I have also a fourth question about the utility of the $\text{erf}(\cdot)$ function. The expression above requires the computation of $\text{erf}(\cdot)$ with complex input. The $\text{erf}(\cdot)$ is not expressed in terms of elementary functions, so in a practical scenario one have to use some numerical approximation. But when the input is complex what happens? There are some numerical approximation for this more complicated case? It seems to me that the $\text{erf}(\cdot)$ function yields the computation of the Fresnel integrals more complicated because there are numerical approximations, that works with real inputs, of $C(\cdot)$ and $S(\cdot)$ . The question is thus the following: 4) What is the utility of expressing the Fresnel integrals in terms of the $\text{erf}(\cdot)$ function? Cosine integral derivation I consider the case of the cosine integral function (I believe that the sine case is analogous with some minor adjustments), so my objective is to express the integral \begin{equation*}C(t)\triangleq \int_0^t \text{cos}\left(\tau^2\right)\text{ d}\tau\end{equation*} in terms of the error function \begin{equation*}\text{erf}(t)\triangleq \frac{2}{\sqrt{\pi}}\int_0^t \exp\left(-\tau^2\right)\text{ d}\tau\end{equation*} So, in order to do that I'm tempted to express the cosine as the as the combination of two complex exponentials \begin{equation*}\begin{aligned}
C(t)&= \int_0^t \frac{\text{exp}\left[\text{j}\left(\tau^2\right)\right]+\text{exp}\left[-\text{j}\left(\tau^2\right)\right]}{2}\text{ d}\tau
\end{aligned}\end{equation*} where $\text{j}$ is the imaginary unit (I prefer this notation than $i$ ). Now I have the exponentials with quadratic arguments, but I'm still far from the result. Unfortunately it is not clear what to do, so I have to exploit my fantasy to find a way to rearrange the integral in a way such that the $\text{erf}(\cdot)$ function turns out somewhere. Firstly, I split the integrand, \begin{equation*}\begin{aligned}
C(t)&= \frac{1}{2}\left[\underbrace{\int_0^t \text{exp}\left(\text{j}\tau^2\right)\text{ d}\tau}_{\triangleq I_1}+\underbrace{\int_0^t\text{exp}\left(-\text{j}\tau^2\right)\text{ d}\tau}_{\triangleq I_2}\right]
\end{aligned}\end{equation*} so that I have to compute two independent integrals $I_1$ and $I_2$ . Second integral I start from the second integral because is easier. Thanks to the
following dirty trick \begin{equation*}\begin{aligned} I_2&=
   \frac{1}{\sqrt{\text{j}}}\int_0^t\text{exp}\left(-\text{j}\tau^2\right)\sqrt{\text{j}}\text{
   d}\tau \end{aligned}\end{equation*} I can use the change of variable \begin{equation*} \alpha(\tau)\triangleq \sqrt{\text{j}}\,\tau
   \end{equation*} which implies $\text{d}\alpha=\sqrt{\text{j}}\,\text{d}\tau$ ,  to write finally \begin{equation*}\begin{aligned} I_2&=
   \frac{1}{\sqrt{\text{j}}}\int_0^{\sqrt{\text{j}}t}\text{exp}\left(-\alpha^2\right)\text{
   d}\alpha=\frac{1}{\sqrt{\text{j}}}\frac{\sqrt{\pi}}{2}\text{erf}\left(\sqrt{\text{j}}t\right)
   \end{aligned}\end{equation*} First integral For the first integral I use the same procedure, but starting from a
different trick \begin{equation*}\begin{aligned} I_1&=
   \frac{1}{\sqrt{-\text{j}}}\int_0^t\text{exp}\left(\text{j}\tau^2\right)\sqrt{-\text{j}}\text{
   d}\tau \end{aligned}\end{equation*} so that the change of variable \begin{equation*} \beta(\tau)\triangleq \sqrt{-\text{j}}\,\tau
   \end{equation*} allows to write \begin{equation*}\begin{aligned}
   I_1&=
   \frac{1}{\sqrt{-\text{j}}}\int_0^{\sqrt{-\text{j}}t}\text{exp}\left(-\beta^2\right)\text{
   d}\beta=
   \frac{1}{\sqrt{-\text{j}}}\frac{\sqrt{\pi}}{2}\text{erf}\left(\sqrt{-\text{j}}t\right)\\
   \end{aligned}\end{equation*} In conclusion, the implicit result is \begin{equation*}C(t)=\frac{\sqrt{\pi}}{4}\left[\frac{1}{\sqrt{-\text{j}}}\text{erf}\left(\sqrt{-\text{j}}t\right)+\frac{1}{\sqrt{\text{j}}}\text{erf}\left(\sqrt{\text{j}}t\right)\right]\end{equation*} here I'm saying ""implicit"" because the square roots of $\text{j}$ and $-\text{j}$ are left implicit. This is a problem because both $\text{j}$ and $-\text{j}$ have not one square root, but two square roots! \begin{equation*}\begin{aligned}
\sqrt{\text{j}}&=\sqrt{\exp\left(\text{j}\frac{\pi}{2}\right)}=\pm\exp\left(\text{j}\frac{\pi}{4}\right)=\pm\left(\frac{\sqrt{2}}{2}+\text{j}\frac{\sqrt{2}}{2}\right)\\
\sqrt{-\text{j}}&=\sqrt{\exp\left(-\text{j}\frac{\pi}{2}\right)}=\pm\exp\left(-\text{j}\frac{\pi}{4}\right)=\pm\left(\frac{\sqrt{2}}{2}-\text{j}\frac{\sqrt{2}}{2}\right)\\\end{aligned}
\end{equation*} so, which one I have to consider? This is not so clear. It seems that Wikipedia consider the first roots (the one with the positive sign outside), but why? Final observation Such square roots are born from the change of variables $\alpha$ and $\beta$ , so in principle I can decide by myself what root to use because any root leads to the same integral in $\text{d}\alpha$ and the same integral in $\text{d}\beta$ . But this fact must imply that the ending result for $C(\cdot)$ is independent from the choice of the roots.
This is actually true? I cannot see it clearly because the inputs of the $\text{erf}(\cdot)$ functions changes if the roots of $\sqrt{\text{j}}$ and $\sqrt{-\text{j}}$ change.","['integration', 'trigonometric-integrals', 'fresnel-integrals', 'complex-integration', 'error-function']"
4361031,Show that a particular process is white noise,"Given $0< p < 1$ and $T_t \overset{i.i.d.}{\sim} t _5 $ , Student's-t distribution with 5 degrees of freedom; $B_t \overset{i.i.d.}{\sim} B(1,p)$ , Bernoulli distrution. Define: $$\epsilon_t = B_t T_t, \, \forall\, t $$ I want to show that $\{\epsilon_t\}_{t \in \mathbb{Z}}$ is a white noise process. First, my lecture notes does not suppose any thing about the independence or dependence between $T_t$ and $B_t$ . There is a possibility that my reading notes are considering $T_t$ and $B_t$ independent, but not written. This would be a fault. So, is it possible to show that $\{\epsilon_t\}_{t \in \mathbb{Z}}$ is white noise not assuming the independence of $T_t$ and $B_t$ ? If they are independent, then $E(\epsilon_t) = E(T_t B_t) = E(T_t)E(B_t) = 0$ , because $E(T_t)=0$ . But if $T_t$ and $B_t$ are dependent? How about the other properties? $E(\epsilon_t^2) = \sigma^2 < \infty\,\, \forall t $ ; $E(\epsilon_t \epsilon_s) = 0, \,\,\forall s \neq t.$ Some help?","['stochastic-processes', 'probability']"
4361050,Show that $f(x) = x^3 + 3\sin x + 2\cos x$ is one-to-one.,"How would I show that $f(x) = x^3 + 3\sin x + 2\cos x$ is one to one? Showing that the function is strictly increasing seemed to be the way to go, but then I need to show that $f'(x) = 3x^2 + 3\cos x - 2\sin x > 0$ . I graphed the derivative function and it is indeed strictly positive. Thoughts?","['calculus', 'functions']"
4361071,Weak solution to ODE,"I have to find a weak solution to $$(a(x)u')' - u = x,\\ a(x) = \begin{cases}1 \qquad x<\frac 12\\ 4\qquad x>\frac 12\end{cases}$$ with $x\in(0,1)$ and $u$ vanishing at the boundaries, i.e. $u(0)=u(1)=0$ . I'm slightly lost on how to approach this problem. I've tried to multiply the equation with a test function $\varphi$ that also vanishes at the boundary and then integrating, which yielded $$\int_0^1[a(x)u'\varphi'+u\varphi+x\varphi]\text{d}x = 0$$ However, I'm not sure how to proceed or if it makes more sense to approach the problem differently. Any help is appreciated.","['ordinary-differential-equations', 'distribution-theory']"
4361079,"Functional equation for cosine on $[0,\frac{\pi}{2}]$","This was part of last week's homework assignment in my Clac 1 class: Let $f:[0,\frac{\pi}{2}]\rightarrow \mathbb{R}^+\cup\{0\}$ be continuous, $$f(\frac{\pi}{2})=0 \hspace{2mm} \textrm{and} \hspace{2mm} f(x+y)= f(x)f(y)-\sqrt{1-f(x)^2}\sqrt{1-f(y)^2}$$ Show that $f$ is uniquely determined and well-defined. I tried proving this by first showing that $f(\frac{x}{2})$ , $f(\frac{x}{2^k})$ and in turn $f(\frac{\pi}{2^k})$ and $f(\frac{j\hspace{1mm}\pi}{2^k})$ are uniquely determined. From that I can also see that $\lim_{x\to 0}f(x)=1$ and since $f$ is continuous $f(x)=1$ . After this I appear to be stuck tho. Any hints would be greatly appreciated, keep in mind that I am not allowed to use any advanced (anything beyond 1st semester) theorems.","['functional-equations', 'calculus', 'trigonometry', 'real-analysis']"
4361082,On smooth structure of quotient manifold,"Let $M$ be a smooth manifold of dimension $m>1$ . Let $\sim$ be an equivalence relation on $M$ such that the graph of this relation makes a smooth, closed and $(2m-1)$ dimensional submanifold of $M \times M$ .
Show that if $\pi : M \to M/\sim$ be an open map then $M/\sim$ has a smooth structure such that with respect to it $\pi$ would be a smooth immersion. My attempt : Since the graph of $\sim$ is closed in $M \times M$ , and $\pi : M \to M/\sim$ is an open map, $M/\sim$ is Hausdorff. $M/\sim$ is second countable as well since open elements in $M/\sim$ (i.e. all elements of each equivalence class), form an open set in $M$ and vice versa (since $\pi^{-1}(U) \subset M$ is open in $M$ ), Therefore the image of a basis of $M$ in $M/\sim$ is a basis (topological basis). We construct smooth structure of $M/\sim$ from structure of $M$ : Let $U\subset M/\sim$ is open then $\pi^{-1}(U) \subset M$ is open in $M$ .
We know that $f : N \to M \times M$ ( $N$ is the set of equivalent pairs) is smooth submanifold of $M \times M$ with $2m-1$ as its dimension. So by rank theorem, for each $x \in M$ , $f^{-1}( {\{ x}\} \times M)$ is a smooth submanifold of $N$ but its second component is the set of elements equivalent to $x$ . And its dimension is $2m-2$ , so $M/\sim$ is a manifold with a smooth structure. Am I rigorously correct?","['solution-verification', 'smooth-manifolds', 'differential-geometry']"
4361095,How to find joint distribution of two normally distributed random variables?,"I have a problem with this exercise. Let $Y_1$ and $Y_2$ be independent random variables with $Y_1∼N(1,3)$ and $Y_2∼N(2,5)$ .
If $W_1=Y_1+2Y_2$ and $W_2=4Y_1−Y_2$ , what is the joint distribution of $W_1$ and $W_2$ ? So I know that $W_1∼N(5,23)$ and $W_2∼N(2,53)$ . But I also need the correlation coefficient for $W_1$ and $W_2$ for the variance-covariance matrix, how do I calculate this? Do I need to calculate the covariance first, and in that case, how do I do that?","['statistics', 'covariance', 'normal-distribution', 'correlation', 'random-variables']"
4361173,Show $\mathcal L^p \otimes \mathcal L^q \subset \mathcal L^{p+q}$,"How can I show that $\mathcal L^p \otimes \mathcal L^q \subset \mathcal L^{p+q}$ where $p,q \in \mathbb Z_{\geq 1}$ ? From my understanding, $\mathcal L^p = \mathcal L(\mathbb R^p)$ is the $\sigma$ -algebra of Lebesgue measurable sets of $\mathbb R^p$ with the Lebesgue measure $\lambda^p$ and $\mathcal L^q = \mathcal L(\mathbb R^q)$ is the $\sigma$ -algebra of Lebesgue measurable sets of $\mathbb R^q$ with the Lebesgue measure $\lambda^q$ . $\mathcal L^p \otimes \mathcal L^q$ is the product $\sigma$ -algebra defined as $\mathcal L^p \otimes \mathcal L^q = \sigma\{A \times B: A \in \mathcal L(\mathbb R^p), B \in \mathcal L(\mathbb R^q) \}$ I have also the hint that every intersection of a set from $\mathcal L^p \otimes \mathcal L^q$ is Lebesgue measurable. Any help would be appreciated.","['measure-theory', 'lebesgue-measure']"
4361175,"$\int{x^k dx}$ as $k \rightarrow -1$ ""paradox"" [duplicate]","This question already has answers here : Demystify integration of $\int \frac{1}{x} \mathrm dx$ (11 answers) Closed 2 years ago . While I was studying integrals by my own, I learnt these two rules for integrating $f(x) = x^k$ : if $k \neq -1$ , then $\int{x^k dx}=\frac {x^{k+1}}{k+1}+c$ ; if $k =- 1$ , then $\int{x^{-1} dx} = \ln {|x|} + c$ . What I find interesting is that, for a fixed $x_0$ , the function $g(x_0, k)$ (defined below) has a discontinuity at $-1$ , but it is still defined. Let $g(x_0,k)=\int_1^{x_0} {x^k dx}$ and $x_0 \in (0, +\infty)$ . Notice that $\lim_{k\rightarrow -1} g(x_0, k) = \pm\infty$ , but $g(x_0, -1) = \ln x_0 +c$ . If you graph $^1$ $g(x_0, k)$ (with $x_0 = e$ and $k$ represented by the $x$ -axis), you get this: My question is: why ? Why is $g(x_0, -1)$ well defined? I mean: it makes sense that $1/x$ should have a primitive; also I can graphically calculate the area underneath it I understand the proofs for $\int{x^{-1} dx} = \ln {|x|} + c$ $\int{x^{-1} dx} = \ln {|x|} + c$ just works, so it must be correct But it seems like this result is completely out of context when you study $x^k$ . What am I missing out? And, is there any relationship between $\int{x^k dx}$ (with $x\ne -1$ ) and $\int{x^{-1} dx}$ at all? If there are none, what's special about $x^{-1}$ ? NOTES: graph $^1$ : done with GeoGebra. I added the point manually, as GeoGebra was graphing $h(x) = \frac {e^{x+1}} {x + 1}$ for every $x$ , instead of $h(x) = \ln e$ when $x = -1$ .","['indefinite-integrals', 'calculus']"
4361192,Singapore Math 6th grade probability problem [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Karen has 12 cards, each written with a letter from the word PENNSYLVANIA. She picks a card at random. Without replacing she picks another card at random. Find the probability she picks two vowels. I was sure the answer is $\frac{5}{12}\times\frac{4}{11}$ :-)",['statistics']
4361203,"An order that lacks ""infinite"" transitivity?","The ordering on the real line "" $<$ "" possesses the property that if we have a sequence $(a_n)_{n=1}^{\infty}$ such that $a_n < a_{n+1}$ and if we have a finite limit $a$ , then $a_1 < a_2 < a_3 < \dots < a$ and we can conclude that $a_1 < a$ - even though transitivity of "" $<$ "" is a statement of finitely many elements. Can we have an ordering, not necessarily on the reals, where ""infinite"" transitivity is not necessarily followed? We'd keep all the other properties: anti-symmetric and transitive; but we could have an $""a""$ such that $a_0$ and $a$ are incomparable but there exists an infinite chain  such that $a_0 < a_1 < \dots < a$ . Have these orderings been discussed already? Attempt: An attempt I've thought about is taking the extended reals ( $\mathbb{R} \cup{\{\pm \infty\}}$ ) and defining the normal order $<$ on the finite numbers but every finite number is ""greater than"" $\infty$ , $-\infty > \infty$ , and every finite number is ""less than"" $-\infty$ . So if we had a sequence diverging to $\infty$ , we'd simultaneously arrive at the smallest element while getting larger; which wouldn't be infinite transitive. However, I'm curious about a case where our limit $a$ is still larger than at least one element in our sequence. Motivation: Went on a wikipedia rabbit hole after reviewing Zorn's lemma.","['order-theory', 'sequences-and-series', 'relations', 'real-analysis']"
4361216,How many holes does my sock that has two extra holes have in it?,I have a pair of Nike Elite socks that have a hole on the heel and another hole at the ball of my foot. Here is the question: Topologically how many holes are in my sock? My friend and I have been arguing about this all night.,['general-topology']
4361226,How to show positivity of Fresnel C integral?,"The Fresnel $C$ -integral is defined as follows. $$C(x) = \int_0^x \cos(t^2) \, dt $$ From the plot found on Wikipedia it seems to be non-negative for all $x \geq 0$ however it is not obvious to me why this is. For example, if you make the change of variables $t = u^\frac{1}{2}$ then you find $$C(x) = \frac{1}{2} \int_0^{x^2} u^{-\frac{1}{2}} \cos(u) \, du$$ My first thought was to split it into positive and negative regions and make a crude bound. For example, $\cos(u)$ will be positive on $[0,\pi/2]$ and negative on $[\pi/2,3\pi/2]$ . A crude lower bound on the first region would be $\left(\frac{\pi}{2}\right)^{-\frac{1}{2}} \int_0^\frac{\pi}{2} \cos(u) \, du$ and on the second region would be $\left(\frac{\pi}{2}\right)^{-\frac{1}{2}} \int_\frac{\pi}{2}^\frac{3\pi}{2} \cos(u) \, du$ . However adding these two bounds together yields something negative and so it doesn't work (however, this approach does work to show the non-negativity of the Fresnel $S$ -integral).","['special-functions', 'analysis', 'fresnel-integrals']"
4361230,Topology of sets defined by real-valued functions,"Suppose I have a topological space $S$ and a continuous real-valued function $f:S \to \mathbb R$ . I can define sets like: \begin{align}
A &= \{x \in S : f(x) = 0 \} \\
B &= \{x \in S : f(x) \le 0 \} \\
C &= \{x \in S : f(x) < 0 \} \\
\end{align} What can we say about the topological relationships between $A, B, C$ ? For example, is it true that $A$ is the boundary of $B$ , or that $C$ is the interior of $B$ . If those statements are not true, and we want them to be true, what extra conditions do we have to impose on $f$ or on the topology of $S$ . I’d be happy to assume that $S$ is a metric space, or even a normed space, for example. Context: these sorts of sets are used to model shapes in computer systems, so I’m interested in their topology.","['implicit-function', 'general-topology']"
4361237,Different definitions of the archimedean property,"In some textbooks I have seen the archimedean property defined as: for some positive real $x$ , real number $y$ , there exists a natural $n$ such that $nx>y$ . In other textbooks the archimedean property is defined as: for any real $x$ , we can find a natural $n$ such that $x \leq n$ I'm guessing I can prove that the two definitions are equivalent, but its just my guess, so if my reasoning is incorrect please correct me. Also if there are any other ways of looking at how the two definitions are equivalent I would love to learn more. My attempt at proving: To see how the first definition implies the second, we let $x=1$ in the first definition, then we have $n>y$ . But then this means for any real $y$ , we can find n such that n satisfies $n \geq y$ (since n satisfies $n>y$ ), which is exactly the second definition. To see how the the second definition implies the first, for any positive real $x$ , real $y$ , we pick a real $z$ such that $z=y/x$ . Then we can find $n$ such that $n \geq z=y/x$ . Since $n+1>n$ , we have $n+1>y/x$ . Since $x>0$ , we multiply both sides of the inequality and it does not change the order, to get $x(n+1)>y$ . Since $n+1$ is a positive integer, this is equivalent to the first definition.","['real-numbers', 'proof-writing', 'solution-verification', 'real-analysis']"
4361332,Is there a function that is the envelope of the sum of ceilings of reciprocal functions,"TL;DR: Given a sum of ceilings of reciprocal functions $$y_1 = T = \sum^{n-1}_i \Big\lceil \frac{p_i}{k} \Big\rceil$$ is there a corresponding form for a function that envelopes the $T$ on the left? Or in other words, is there a form for the function $L$ that touches the left endpoint of each line segment in the graph of $y_1 = T$ ? More formally, Given $y_2 = h$ for some integer $h$ , we have that $y_1$ and $y_2$ intersect in a line segment, say at $y=h$ from $x=x_{h^-}$ to $x=x_{h^+}$ . The left envelope of $y_1 = T$ would be the function $L$ such that for each integer $h$ , we have $L(x_{h^-}) = h$ or in other words for each integer $h$ , we have $L(x) = h$ iff $x = x_{h^-}$ . For example, in this Desmos graph , we have $$
y_1 = T =
\Big\lceil \frac{3}{x} \Big\rceil
+ \Big\lceil \frac{6}{x} \Big\rceil
+ \Big\lceil \frac{7}{x} \Big\rceil
+ \Big\lceil \frac{11}{x} \Big\rceil
$$ and $y_1 = T$ intersects $y_2 = h = 8$ in a line segment at $y = 8$ from $x=x_{h^-}=3.667$ to $x=x_{h^+}=5.5$ . The left envelope of this $T$ would have $L(3.667) = 8$ . Given a sum of two reciprocals $$
T_2 =
\Big\lceil \frac{p_0}{k} \Big\rceil
+ \Big\lceil \frac{p_1}{k} \Big\rceil
$$ we can note that $p_0 = ak + b$ and $p_1 = ck + d$ for some integers $0 \leq b, d < k$ and so we have $$
\Big\lceil \frac{ak + b}{k} \Big\rceil =
\begin{cases}
a & \text{if } b = 0\\
a+1 & \text{if } b \neq 0
\end{cases}
$$ and similarly for $p_1$ . Overall we have four cases for the value of $T_2$ : $d = 0$ $d \neq 0$ $b = 0$ $a+c$ $a+(c+1)$ $b \neq 0$ $(a+1)+c$ $(a+1)+(c+1)$ Note, asking whether $b = 0$ or $b \neq 0$ is the same as asking whether $p_0$ is divisible by $k$ or not, respectively. We can express the above in python3 as follows: a, b = divmod(p[0], k)
c, d = divmod(p[1], k)
T = a + (b != 0) + c + (d != 0)

# Alternative
# T = a + bool(b) + c + bool(d) Also, each time we add a $\big\lceil \frac{p_i}{k} \big\rceil$ term to the $T$ sum, we introduce two new cases (i.e. whether $p_i$ is divisible by $k$ or not). Thus a $T$ sum of $n$ terms would have $2^n$ cases. Is there any way around this? Perhaps there's a more clever way to combine terms...??? Motivating problem: I was doing Leetcode #875: Koko Eating Bananas (description in footnote [1]). The answer can be found using binary search over $k_{min} <= k <= k_{max}$ where k_min = sum(piles) / h and k_max = max(piles) (see footnote [2]), but I was wondering if there's an even more mathematical way to find it. Having chosen a $k$ , $p_i = $ pile[i] takes $t_i = \big\lceil \frac{p_i}{k} \big\rceil$ hours to consume and all $n$ piles take $$T = \sum^{n-1}_i \Big\lceil \frac{p_i}{k} \Big\rceil$$ hours to consume. We want to minimize $k$ such that $T \leq h$ . This can be done graphically: Given example 1: Input: piles = [3,6,7,11], h = 8
Output: 4 We graph $$
\begin{aligned}
y_1 = T &= \sum^{n-1}_i \Big\lceil \frac{p_i}{x} \Big\rceil\\
&=
\Big\lceil \frac{3}{x} \Big\rceil
+ \Big\lceil \frac{6}{x} \Big\rceil
+ \Big\lceil \frac{7}{x} \Big\rceil
+ \Big\lceil \frac{11}{x} \Big\rceil
\end{aligned}
$$ where $p_i$ represents piles[i] , $y$ represents T , and $x$ represents k . We also graph $y_2 \leq h$ to represent the constraint. The minimum value of $k$ is minimum value of $x$ where $y_1$ and $y_2$ intersect. See the Example 1 Desmos graph , which shows the minimum value of $k$ is $3.667$ , though since the problem statement calls for $k$ being an integer, the actual value is $4$ . Here is another examples: Input: piles = [30,11,23,4,20], h = 6
Output: 23 Likewise, here's the Example 3 Desmos graph , which shows the minimum value of $k$ is $23$ . [1]: Leetcode #875: Koko Eating Bananas Koko loves to eat bananas. There are $n$ piles of bananas, the $i$ th pile
has piles[i] bananas ( piles is $0$ -indexed). The guards have gone and will come back in $h$ hours. Koko can decide her bananas-per-hour eating speed of $k$ . Each hour, she
chooses some pile of bananas and eats $k$ bananas from that pile. If the
pile has less than $k$ bananas, she eats all of them instead and will
not eat any more bananas during this hour. Koko likes to eat slowly but still wants to finish eating all the
bananas before the guards return. Return the minimum integer $k$ such that she can eat all the bananas
within $h$ hours. [2]: Binary Search solution class Solution:
    def minEatingSpeed(self, piles: List[int], h: int) -> int:
        k_min = (sum(piles) + h-1) // h # ceil(sum(piles) / h) 
        k_max = max(piles)
        
        l, r = k_min, k_max + 1
        while l < r:
            k = (l+r) // 2
            t = sum((p + k-1) // k for p in piles) # sum(ceil(p / k) for p in piles)
            if t <= h:
                r = k
            else: # t > h
                l = k + 1
        return l","['envelope', 'curves', 'ceiling-and-floor-functions', 'functions']"
4361378,Can I Find the Eigenvalues of a Matrix this Way?,"I have a matrix $A = xx^T - yy^T,$ where both $x$ and $y$ are linearly independent $n$ -column vectors, $n\geq 2$ . To find the eigenvalues, I reasoned this way: Since $x$ and $y$ are linearly independent vectors (given), then $rank(A) = 2.$ So, we have two non-zero eigenvalues and $(n-2)$ eigenvalues, each with a value of zero. Since both $x$ and $y$ are linearly independent, they form a basis for $V = span(x,y).$ Therefore, $Ax = (x.x)x - (x.y)y$ and $Ay = (x.y)y - (y.y)y$ The matrix $A$ relative to $V$ is: $A = \begin{bmatrix}x.x&x.y\\-x.y&-y.y\end{bmatrix}.$ Now, $Av =$ $\lambda$$v$ Therefore, $(A - \lambda I_{2})v = 0$ , where $A$ has been restricted to a $2 \times 2$ matrix. When we solve this, we get the $\lambda$ 's. Is this correct so far? Thanks.","['matrices', 'solution-verification', 'linear-algebra', 'eigenvalues-eigenvectors']"
4361385,Properties of periodic Sturm-Liouville Problem,"Consider the Regular Sturm Liouville(RSLP) problem $$(p(x)y’)’+(q(x)+\lambda r(x))y=0$$ where $p,q$ and $r$ are functions such that $p$ has continuous derivative, $q$ and $r$ are continuous, and $p(x)>0$ and $r(x)>0$ for all $x$ on a real interval $a\leq x\leq b$ and $\lambda $ is a parameter independent of $x$ and with boundary conditions $$A_1y(a)+A_2y’(a)=0$$ $$B_1y(b)+B_2y(b)=0$$ I know properties of this regular Sturm Liouville problems as follows( Given in Shepley L. Ross Differential Equations) $1$ . Eigen values of RSLP are reals and can be arrange in an increasing order. $2$ . Eigen values of RSLP are simple. $3$ . Eigen function $\phi_n(x)$ corresponding to $n$ -th eigen value $\lambda_n$ has exactly $n-1$ -zeros in the open interval $(a,b)$ . $4$ . Eigen functions corresponding to different eigen values are orthogonal with respect to weight function $r$ on $[a,b]$ . Now I only know that for Periodic Strum Liouville problem eigen values many not be simple I.e. there many be two linearly independent eigen functions corresponding to an eigen value . I want to know whether all others $3$ properties written above are true for periodic Sturm Liouville problem . By a periodic Sturm Liouville problem  I mean the above differential equation with boundary conditions $y(a)=y(b), y’(a)=y’(b)$ and one more condition as $p(a)=p(b)$ .Thank you .","['boundary-value-problem', 'ordinary-differential-equations']"
4361393,A map homotopic to a $\mathbb{Z}_2$-equivariant map is $\mathbb{Z}_2$-equivariant.,"Assume an antipodal homeomorphism $\nu_n: S^n \rightarrow S^n$ mapping $x$ to $-x$ , (it can be seen as an action of $\mathbb{Z}_2$ on $S^n$ ). What I want to prove or reject is: Suppose $h:S^2 \rightarrow S^1$ is a $\mathbb{Z}_2$ -equivariant map. If $g:S^2 \rightarrow S^1$ is homotopic to $h$ , then $g$ is a $\mathbb{Z}_2$ -equivariant map too. This claim might be true in general for any $G$ -equivariant map on any topological space. The general statement would be: Suppose $X$ and $Y$ are two topological spaces equipped with an action of the group $G$ (i.e. $X$ and $Y$ are $G$ -spaces). If $h:X \rightarrow Y$ is a $G$ -equivariant map homotopic to the map $g:X \rightarrow Y$ , then $g$ is $G$ -equivariant too. I am trying to prove the statement for $G=\mathbb{Z}_2$ , $X=S^2$ , and $Y=S^1$ but it might be wrong. So, either a proof or a counter example would help me. I know equivariant homotopy theory is interested in studying the homotopy classes of equivariant maps between $X$ and $Y$ but I guess my question does not fit into this subject because we do not know yet if the map $g$ is $G$ -equivariant or not. I have not seen this question anywhere. This is my own thought. I would appreciate any hint or help.","['general-topology', 'abstract-algebra', 'algebraic-topology']"
4361418,Difficulty in evaluating $\lim_{t\to x}\frac{t^2f(x)-x^2f(t)}{t-x}$,"We have the differentiable function $f(x)$ , on interval $(0,\infty)$ such that $f(1)=1$ and $\lim_{t\to x}\frac{t^2f(x)-x^2f(t)}{t-x}=1\,\,\forall x>0$ , so, we need to find $f(x)$ . Applying L'Hopital's rule and substituting $t\rightarrow x$ quickly simplified the equation to: $$2xf(x)-x^2f'(x)=1$$ Now, this equation is probably some type of differential equation, and further, I can't solve it and am stuck here. I haven't studied much of differential equations so far. Please help in solving this equation or suggest some other method. Thanks in advance.","['limits', 'calculus', 'ordinary-differential-equations']"
4361447,Exercise about almost sure convergence,"I have the following problem: Let $X_n$ , $n \geq 1$ be independent r.v. identically distributed with the probability function: $P(X_{i} = k) = P(X_{i} = - k) = \frac{p}{2} q^{k-1}$ , $k = 1,2,...,m$ , $P(X_{i} = m + 1) = P(X_{i} = -m -1) = \frac{q^{m}}{2}$ where $m>1$ , $p\in (0,1)$ and $q = 1-p$ . Calculate the almost surely limit of $Z_{n} = \frac{\frac{X_{1}}{X_{2}} + \frac{X_{3}}{X_{4}} +...+ \frac{X_{2n-1}}{X_{2n}}}{X_{1}^{2} + X_{2}^{2} + ... + X_{n}^{2}}$ . The exercise also adds a suggestion that is as follows: Prove that the numerator's limit is $0$ and that the denominator's limit is always greater than $0$ . So far I think I can say that the denominator's limit must be greater than $0$ because for all $x\in \mathbb{R}-\{0\}$ , $x^{2} > 0$ . I know that $X_{i} \neq 0$ and, therefore, $X_{1}^{2} + X_{2}^{2} + ... + X_{n}^{2} > 0$ . Let me know if there's any mistake in my reasoning and also if you find any way to proceed to find the answer to the problem.","['convergence-divergence', 'probability-limit-theorems', 'probability-theory', 'random-variables']"
4361455,An informal debate on the cardinality of infinite sets,"A friend and I have been engaged in a lively discussion on prime numbers. I'll cut to the chase right away. They ask: If you were to write out all of the even numbers and then all of the pairs of primes, you believe they would correspond one-to-one? Clearly, they are referring to a bijection, which is from my understanding the definition for equal cardinality. Furthermore, I believe that the evens and pairs of primes have the same cardinality. Mind you, in this case we are referring to a set of two primes $\{p, q\}$ rather than an ordered pair, or twin primes, for that matter. Set-theoretically, my reasoning is as follows: 1: Both the evens and primes are countably infinite. 2: Any set made from pairing elements of a countably infinite set is also countably infinite. 3: There is only one cardinality which is countably infinite. 4: Therefore the evens and primes pairs have the same cardinality. Is my reasoning valid? If not, I'd be grateful for corrections. If so, I'd be grateful for any elaboration which may aid in a convincing argument to end the discussion. I find my reason quite sound, but I shall give them the benefit of the doubt as they have a higher education on mathematics. Hence, perhaps they are referring to something more abstract, of which you may enlighten me. Edit: I should also add some of their points too, for context: ""Are you suggesting that if we have 5 objects, and consider how many ways we can combine them into two objects that we will have five such combinations?"" ""The number of combinations is not equal to the number of objects."" ""Even in dealing with the infinities, the combinations of paired primes are infinitely greater than theircounterparts in the infinite set of even numbers"" Edit 2: The previous title was regarding a 'formal' debate. I realize now I was using 'formally' informally! In fact, the opposite may be more appropriate. An intuitive argument in mostly plain language or with aid of some visualization is optimal, as those are the kind they appreciate the most.","['elementary-set-theory', 'cardinals']"
4361540,"If G is n regular, then G has n disjoint perfect matchings.","Let G be a bipartite simple graph show that: If G is n regular, then G has n paarwise disjoint perfect matchings. It's firstly easy to show that G has a perfect matching by using Hall’s Theorem, $|N(S)| \geq |S|$ with $N$ the potential mathings and $S$ a arbitary subset of one part of the bipartite graph G. But how to show that there is n perfect matchings and besides disjoint. I've seen a answer by multiplying d to $|N(S)| \geq |S|$ , but I don't why should we do this. If anyone could help me with some tipps, I would be very grateful.","['graph-theory', 'matching-theory', 'bipartite-graphs', 'discrete-mathematics']"
4361562,Mapping logarithmic ranges to linear,"I'm trying to map the table 3.2.5 from RFC1951 from its distance values to code values. The table as follows (ignore the bits column, ranges are inclusive): Code Bits Dist  Code Bits   Dist     Code Bits Distance
         ---- ---- ----  ---- ----  ------    ---- ---- --------
           0   0    1     10   4     33-48    20    9   1025-1536
           1   0    2     11   4     49-64    21    9   1537-2048
           2   0    3     12   5     65-96    22   10   2049-3072
           3   0    4     13   5     97-128   23   10   3073-4096
           4   1   5,6    14   6    129-192   24   11   4097-6144
           5   1   7,8    15   6    193-256   25   11   6145-8192
           6   2   9-12   16   7    257-384   26   12  8193-12288
           7   2  13-16   17   7    385-512   27   12 12289-16384
           8   3  17-24   18   8    513-768   28   13 16385-24576
           9   3  25-32   19   8   769-1024   29   13 24577-32768 I need a mathematical function that maps the distance values to the code values e.g. f(777) = 19, f(9000) = 26. Both input and output values are integer and I know there must be some expression that would work using Log2 functions. The range of distance doubles every 2 codes (except code 0 and 1) e.g. there are 4096 distinct values for codes 26 and 27, 8192 for 28 and 29, etc. Exceptions can be made for codes 0 and 1","['computer-arithmetic', 'functions', 'graphing-functions', 'logarithms']"
4361575,Which of the two quantities $\sin 28^{\circ}$ and $\tan 21^{\circ}$ is bigger .,I have been asked that which of the two quantities $\sin 28^{\circ}$ and $\tan 21^{\circ}$ is bigger without resorting to calculator. My Attempt : I tried taking $f(x)$ to be $f(x)=\sin 4x-\tan 3x$ $f'(x)=4\cos 4x-3\sec^23x=\cos 4x(4-3\sec^23x\sec 4x)$ but to no avail. I also tried solving $\tan^2 21^{\circ}-\sin^228^{\circ}=\tan^2 21^{\circ}-\sin^221^{\circ}+\sin^221^{\circ}-\sin^228^{\circ}=\tan^2 21^{\circ}\sin^221^{\circ}+\sin^221^{\circ}-\sin^228^{\circ}$ but again no luck. There doesn't appear to be a general way of doing this,"['triangles', 'algebra-precalculus', 'number-comparison', 'trigonometry']"
4361600,"Some problems in Cheeger's ""A lower bound for the smallest eigenvalue of the Laplacian""","Pictures below is from Cheeger's "" A lower bound for the smallest eigenvalue of the Laplacian "" $f$ is the eigenfunction  of smallest eigenvalue  of Laplacian on $M$ , where $M$ is Riemannian manifold with $\partial M =\varnothing $ . 1, I don't know why $V(M_1)\le V(M_2)$ means that $h_1 \ge h$ . From the first below picture, the definition of $h_1$ and $h$ are different. I don't know how to get $h_1 \ge h$ ? 2, $M_1$ is a part of $M$ , in my view, $\dim M_1 = \dim M$ . There should be not any submanifold $A$ of $M$ such that $\partial A = M_1$ . 3, What is the critical levels ?  Is it the set of critical points ? Why the regions of $M_1$ lying between the critical levels of $f^2$ have a natural product strcture ? I can't see it. In fact, I don't know the image of level surface and orthogonal trajectories. PS: I feel my problem is too much to it is hard to answer. If so, is there any book contain the Cheeger's paper and not hard to read ?","['eigenfunctions', 'partial-differential-equations', 'differential-geometry']"
4361652,Prove that $s_p(n-1)+s_p(n)+1-s_p(2n)\ge v_p(n)\cdot (p-1)$ for $p|n$,"Prove that $\frac{(2n)!}{n!(n+1)!}$ is an integer. Now, one way of proving this is $$\frac{(2n+1)!}{n!(n+1)!}=\frac{2n+1}{n}\cdot \frac{(2n)!}{(n-1)!(n+1)}=\frac{2n+1}{n}\cdot \binom{2n-1}{n+1}\in \Bbb Z$$ but $\gcd(n,2n+1)=1\implies n|\binom{2n}{n-1}$ The other possible way I think is using $v_p.$ Let $p$ be an odd prime dividing $n$ then $$v_p\left(\frac{(2n)!}{(n-1)!(n+1)!}\right)=v_p(2n!)-v_p(n-1!)-v_p(n+1!)=\frac{2n-s_p(2n)-n+1+s_p(n-1)-n-1+s_p(n+1!)}{p-1}=\frac{s_p(n-1)+s_p(n+1)-s_p(2n)}{p-1}$$ where $s_p$ denotes the sum of digits of $n$ is base $p$ . Since $p|n\implies s_p(n+1)=s_p(n)+1.$ And it is well known that $p-1|x-s_p(x).$ Now, to show that $n|\left(\frac{(2n)!}{n!(n+1)!}\right)$ enough to show that $$s_p(n-1)+s_p(n)+1-s_p(2n)\ge v_p(n)\cdot (p-1)$$ When $2|n$ note that $v_2(\frac{(2n)!}{n!(n+1)!})=v_2(\frac{(2n+1)!}{n!(n+1)!})=v_2(\binom{2n+1}{n})$ Any idea on how to prove this without using the first proof? Also, this identity is definitely true because of the first proof.","['number-theory', 'elementary-number-theory']"
4361671,Why was the concept of first/second categories in metric spaces introduced?,"Let $(X, d) $ be a metric space. $X$ is of first category if it can be expressed as a countable union of nowhere dense subsets. Otherwise, $X$ is called a metric space of second category . A first category set is intuitively ""small "" and a second category set is ""large"". A second category set with no isolated point is uncountable. Cardinality is purely a set-theoretic characterisation and first/second category is related to the topology on the underlying set. However I am interested to know whether there are any deeper relations between the cardinality and whether it is first or second category. Why was the concept of first/second categories in metric spaces introduced?","['metric-spaces', 'elementary-set-theory', 'general-topology', 'soft-question', 'baire-category']"
4361692,Let $a$ and $b$ be positive integers such that $an + 1$ s a cube if and only if $ bn + 1$ is a cube. Prove that $a = b.$,"Let $a$ and $b$ be positive integers such that $an + 1$ is a cube if and only if $bn + 1$ is a cube. Prove that $a = b.$ By choosing $p_n^3 \equiv 1 \mod b$ , we find that there are infinitely many numbers $n$ such that $:bn+1=p_n^3\Rightarrow an+1=q_n^3$ So we have : $(a-b)n = q_n^3-p_n^3 \Rightarrow (a-b)$ is a divisor of an infinite number of numbers $T$ of the form $:T=p^3-q^3$ This is a quite hard problem for me. Any assistance would be appreciated.","['number-theory', 'elementary-number-theory']"
4361731,Finite group containing a normal copy of $S_3$ is a cartesian product,"Suppose that $G$ is a finite group and $N \triangleleft G$ is an isomorphic copy of $S_3$ . Is it true that we can write $G \simeq H \times N$ ? I cannot find a counterexample (at least using standard examples). I'm trying to prove that but I did not have good ideas. Probably this is not a hard question, so I'm satisfied with a hint.","['group-theory', 'abstract-algebra', 'finite-groups']"
4361737,"Let $G$ be a group of order $7105$, show that the order of the center of $G$ is divisible by $35$.","Let $G$ be a group of order $5\cdot 7^2 \cdot 29$ , show that the order of the center $Z(G)$ of $G$ is divisible by $35$ . Now, $G$ contains only a $5$ -sylow, $P_5$ , and since the conjugacy action of $G/P_5$ on $P_5$ is the trivial homomorphism, then $P_5 \subseteq Z(G)$ , so $5$ divides the order of the center. The number of $7$ -sylow could be either $1$ or $29$ , in the first case we can conclude as above. What about the second case?","['group-theory', 'abstract-algebra', 'finite-groups', 'sylow-theory']"
4361780,Showing that a series of random variables converges almost surely.,"Let $X_1,X_2,\dots$ be independent and identically distributed random variables with $$\mathbb{P}(X_n=1)=\mathbb{P}(X_n=-1)=1/2$$ for all $n\geq1$ . Show that the series $$\sum_nX_n/n$$ converges almost surely. My idea was to define the random process $(S_n)_{n\geq1}$ by $$S_n = \sum_{k=1}^nX_k/k.$$ It is simple enough to show that this is a martingale, and so if I can show that this process is $L^1$ -bounded then I can use Doob's almost sure martingale convergence theorem to obtain the result. But I don't know how to show $L^1$ -boundedness (or if it even is possible, in which case I need a new idea), so any help would be great!","['stochastic-processes', 'probability-limit-theorems', 'probability-theory', 'martingales']"
4361803,Morse function induced on fibered product,"Let $A,B$ and $C$ be three smooth manifolds. Suppose that $F:A\to C$ and $G:B\to C$ are smooth and transverse functions, making the fibered product $$S=A\underset{F,C,G}{\times}B=\{(x,y)\in A\times B\,;\,F(x)=G(y)\}$$ a manifold with tangent space at $(x,y)\in S$ $$T_{(x,y)}S=T_xA\underset{dF_x,T_zC,dG_y}{\times}T_yB$$ where $z=F(x)=G(y)$ . Suppose that we have a smooth function $f:A\to\mathbb{R}$ , and note $\pi_A:S\to A$ and $\pi_B:S\to B$ the canonical projections. My question is: Is there a (generic) property we can ask $F$ to check in order to make $\tilde{f}=f\circ\pi_A:S\to\mathbb{R}$ a Morse function? So far analysing this question we have that $d\tilde{f}:S\to T^*S$ is given by $$(x,y)\in S\mapsto \left[(\dot{a},\dot{b})\in T_{(x,y)}S\mapsto df_x(\dot{a})\right].$$ Thus $(x,y)\in S$ is critical for $\tilde{f}$ iff for all $(\dot a,\dot b)\in T_xA\times T_yB$ , $dF_x(\dot a)=dG_y(\dot b)$ implies that $df_x(\dot a)=0$ (remark: if $f$ has a critical point $x_0\in A$ , then $\pi_A^{-1}(x_0)$ is a set of critical points. Since this set is generically a manifold of dimension $\dim S-\dim A=\dim A+\dim B-\dim C-\dim A=\dim B-\dim C$ , the function $\tilde{f}$ would be at most Morse-Bott as soon as $\dim B>\dim C$ , so to avoid this case we can require $A$ to be non compact and $f$ to have no critical points). Then, $\tilde{f}$ will be a Morse function iff $d\tilde{f}$ is transverse to the zero section $0_{T^*S}\subset T^* S$ . But so far I can't manage to reformulate this transversality condition as another one directly involving $F$ (I think the more likely one would be a transversality between the $2$ -jet of $F$ , $j^2F$ , and some submanifold of the $2$ -jet space). The fact that the choice of $F$ only changes implicitely $S$ and its tangent space make it difficult to me to make $F$ appear while studying the condition on $\tilde{f}$ . I will be happy to receive any idea or comment about this problem, and thank you for having read it.","['transversality', 'morse-theory', 'differential-geometry']"
4361823,How to prove this inequality with the simplest means?,"$$ x^2+5y^2+6z^2 \geq 4\sqrt{5xyz^2},\ \text{ if }\ xy>0 $$ I was trying to prove it. The right hand side should be nonnegative, so I can square it on both sides.
But once I have done it I get to a point, where I do not see how to show, that the statement is greater or equal to $0.$ $$x^4+25y^4+36z^4+10x^2y^2+12x^2z^2+60y^2z^2-80xyz^2 \geq 0. $$ Thank you","['elementary-number-theory', 'inequality', 'discrete-mathematics']"
4361851,Euclidean norm of a solution of matrix differential equation,"I try to solve a problem I have found in a book where I'm asked to find the solution to a differential equation of the form $$
x'=Ax
$$ where $A$ is a matrix. The answer is the exponential of $e^{At}$ . Then I'm stuck with the second request that is to prove that $\|x(t)\|_2$ is increasing (as a function of $t$ ) by considering $A+A^T$ . What information can give $A+A^T$ ? I'm not posting the expression for $A$ because I'm looking for a general answer. Thanks in advance for any suggestion. Thanks to the suggestion given, one can observe that the square of the norm is increasing if and only if the norm is increasing but the transpose of $x(t)$ is $e^{A^Tt}$ . So $$
\langle x(t),x^T(t)\rangle=\langle e^{At},e^{A^Tt}\rangle=e^{(A^T+A)t}
$$ and if I prove that $A^T+A$ is positive definite I have concluded. It's all correct?","['systems-of-equations', 'linear-algebra', 'ordinary-differential-equations']"
4361857,Hartshorne Exercise II.6.1 Proving that $\operatorname{Cl}(X\times\mathbb{P}^n)\cong \operatorname{Cl}(X)\times \mathbb{Z}$.,"I have been stuck on this computation for some time (at least a year now since I started learning AG seriously). The trick is to use Proposition II.6.5 and take the closed set $Z$ to be the hyperplane at $\infty$ to get a sequence $$
\mathbb{Z}\rightarrow \operatorname{Cl}(X\times \mathbb{P}^n)\rightarrow \operatorname{Cl}(X\times\mathbb{A}^n)\cong \operatorname{Cl}(X)\rightarrow 0
$$ with an inductive application of Proposition II.6.6. Now we prove that there is a section of the map on the right $\operatorname{Cl}(X\times \mathbb{P}^n)\rightarrow \operatorname{Cl}(X\times\mathbb{A}^n)$ . To complete the proof, it then suffices to show that the left hand side map is injective. Here, I am stuck since I cannot seem to find a description of the closed set $Z$ and the generic point of $X\times\mathbb{P}^n$ . I've looked explanation like this but it flew over my head and wasn't really enlightening. Any thoughts / suggestions?",['algebraic-geometry']
4361928,How to model 2 correlated Geometric Brownian Motions?,"Let's say we have 2 GBM processes: $$S_1(t)=S_1(0)\cdot \exp{(\mu_1t-\frac{1}{2}\sigma_1^2t+\sigma_1\cdot W(t))}\text{  and  } S_2(t)=S_2(0)\cdot \exp{(\mu_2t-\frac{1}{2}\sigma_2^2t+\sigma_2\cdot W(t))}$$ such that $\mathbb{E}[dW_1;dW_2]=\rho$ . How can I model them together? Formulas above won't take the correlation into account. Since $S(t)$ are meant to be stock prices, I can look at daily returns directly as: $$\frac{S(t+1)-S(t)}{S(t)}=\exp{(\mu-\frac{1}{2}\sigma^2+\sigma \cdot N(0;1))}-1=\exp{(N(\mu-\frac{1}{2}\sigma^2;\sigma^2))}-1$$ which is close to normal (transposed lognormal in fact). If it was in fact normally distributed then I would use 2-D $N(M;\Sigma)$ with $M=(\mu_1-\frac{1}{2}\sigma_1^2;\mu_2-\frac{1}{2}\sigma_2^2)$ and $\Sigma$ as the covariance matrix. But I want to be precise with the model and not assume (incorectly) that returns are normal if prices are GBM. So I'm struggling on how to include the correlation/covariance in the direct formulas for $S_1,S_2$ .","['stochastic-processes', 'brownian-motion', 'probability-theory', 'normal-distribution']"
4361955,"Do sets, whose power sets have the same cardinality, have the same cardinality?",Is it generally true that if $|P(A)|=|P(B)|$ then $|A|=|B|$?  Why? Thanks.,['set-theory']
4362012,"If $\varphi: A \to B$ is surjective, then is $\varphi^{**}: A^{**}\to B^{**}$ surjective?","Let $A$ and $B$ be $C^*$ -algebras and consider a $*$ -homomorphism $\varphi: A \to B$ . Then the biduals $A^{**}$ and $B^{**}$ carry natural $C^*$ -structures (coming from the enveloping von Neumann algebra) so that the map $$\varphi^{**}: A^{**}\to B^{**}$$ is again a $*$ -morphism. If $\varphi$ is isometric (i.e. injective), it is easily verified (using general properties of adjoint maps on normed spaces) that $\varphi^{**}: A^{**}\to B^{**}$ is again isometric. I'm now wondering if surjectivity of $\varphi$ implies surjectivity of $\varphi^{**}$ . We can't use the straightforward route: we have $\varphi$ surjective $\implies $ $\varphi^*$ injective but this does not allow us to conclude that $\varphi^{**}$ is surjective. I'm starting to think there may be a counterexample!","['von-neumann-algebras', 'c-star-algebras', 'banach-spaces', 'functional-analysis']"
4362014,How to determine logical form and / or type of proof? (beginner),"I have some very confusion when analysing the logical form / structure of a proof. In Basic Mathematics, there is a proof of the rule $0a = 0$ that is done by using the rules of addition and multiplication and the rule $1a = a$ It begins on one line: $$0a + a = 0a + 1a = (0 +1)a = 1a = a$$ Is the first statement $0a + a= 0a +1a$ ? The reason he can  begin with this is because of the previously established rule that $1a = a$ ? Is this basically a biconditional, one object in and of itself? His second statement would be $0a + 1a = (0 + 1)a$ and he can justify it by distributivity? Sometimes these inline equations mess me up, I used to think of the leftmost side as continually 'transforming' after each equal sign. Is it actually a series of statements like I described above? I have found as I've been learning more I've been questioning everything and possibly overthinking things, is this common when people are introduced to logic and proofs?","['algebra-precalculus', 'proof-writing', 'logic']"
4362048,Infer boundedness from differential inequality $\frac{dx}{dt} \leq x(t)^2 + y(t)$?,"The original problem comes from a paper saying that $$\frac{d}{dt}\|u\|_{H^s} \leq C\|u\|_{H^s}^2 + \|f(t)\|_{H^s},$$ where $\|\cdot\|_{H^s}$ denotes the Sobolev norm of order $s$ , and $f(t) \in L^1(0,T;H^s)$ . Then the author concludes that $x \in L^\infty(0,T; H^s)$ . But I think it is fine to simplify it into a real-valued differential inequality for positive function $x(t)$ : $$\frac{dx}{dt} \leq x(t)^2 + y(t),$$ where $y(t)$ is some integrable function. How do we see $x(t)$ is also bounded in some interval, say $[0,T]$ ? If the power of $x(t)$ in the RHS is $1$ , then we could use Gronwall inequality to conclude the result. But it is not the case here. Could anyone help with it? I appreciate any hint and suggestion! Edit: On another paper I read that: $x(t) \leq g(t)$ , where $g(t)$ solves $$
\frac{dg}{dt} = g(t)^2 + y(t), g(0) = x(0).
$$ And $T$ is chosen as the any number such that $\sup_{t\in[0,T]} g(t) < \infty$ . Questions : Does the ODE $$
\frac{dg}{dt} = g(t)^2 + y(t), g(0) = x(0)
$$ has a solution? Why $x(t) \leq g(t)$ on $[0,T]$ ?","['analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
4362060,Path to proving partial fractions and the fundamental theorem of algebra,"As I've learned Calculus, I've tried to follow along with proofs of the rules that I use.  In most cases, like say the Power Rule, I'm able to follow along with the proofs using concepts I understand, or things I've already proved like the Binomial Theorem.  In other cases, like the Extreme Value Theorem the concepts used to prove it seem to require math that's beyond me, so I save proving them for later. I recently learned Partial Fraction Decomposition in the context of Integrals, and I became enamored with this technique.  I searched for proofs of why it works, and most of them seem to rely on the Fundamental Theorem of Algebra.  Most of the proofs of the FTA I've seen rely on either Number Theory or Complex Analysis.  I'm interested in proving the FTA regardless of whether I need it to prove partial fractions. If I want to: Prove the Fundamental Theorem of Algebra Prove why Partial Fractions work What would be the recommended math path for me following Calc?  Would it be to study Number Theory concepts like Euclidean Division and GCD, or to focus on complex numbers to prove the FTA? Also, am I correct to assume that proving FTA should come before proving Partial Fractions always work because the latter proof will follow easily from the former?","['number-theory', 'linear-algebra', 'polynomials', 'education', 'partial-fractions']"
4362107,"P(D | w) vs P(y | x, w) - likelihood notation","In certain ML books/lectures/slides/notes about Bayesian inference, you often see the likelihood written as $P(\mathcal{D} | w) = \Pi_{i \in N} P(y_i | x_i, w)$ , where $\mathcal{D} = {(x_1, y_1),~ ...~,(x_n, y_n)}$ . It then follows that the equation for Bayes rule is: $$ P(w | \mathcal{D}) = \frac{P(\mathcal{D} | w) \cdot p(w)}{P(\mathcal{D})} $$ I'm confused because if you separate out the covariates from the response (inputs from targets), you could get the following equation for Bayes rule: $$ P(w | x_i, y_i) = \frac{P(y_i | x_i, w) \cdot p(w | x_i)}{P(x_i, y_i)} $$ These two equations don't match up, in particular $p(w) \neq p(w | x_i)$ . So what is the justification for this? In both a frequentist sense and a Bayesian sense? I know that, for example wikipedia , defines the likehood function as the joint probability of the observed data, which would allow for this confusion to make sense. And when performing MLE, you maximize the response/targets in conjunction with the covariates/inputs, so it makes sense intuitively for this notation, but what would be the theoretical justification? Edit:
I suppose I'm more confused by why would $P(y_i | x_i, w)$ have the same formulation for the likelihood function as $P(\mathcal{D} | w)$ .","['statistics', 'bayesian', 'maximum-likelihood', 'probability-theory', 'probability']"
4362120,Sum of reciprocals of primes: easy proof that $\sum_{p\leq x} \frac{1}{p} < C\log\log x$ for some constant $C$?,"$\sum_{p\leq x} \frac{1}{p} \sim \log\log x$ follows from the PNT , and there's also a short proof not relying on the PNT . However that proof still requires some nontrivial steps. I was wondering if there is a simpler proof which shows the weaker $$C_1\log\log x < \sum_{p\leq x} \frac{1}{p} < C_2\log\log x$$ for some constants $C_1, C_2$ . What made me wonder is that if I start from a proof that $\sum_p \frac{1}{p}$ diverges (which I take from Bateman & Diamond's ANT book): $$\sum_{n\leq x} \frac{1}{n} < \prod_{p\leq x} \left(1-\frac{1}{p}\right)^{-1}<\prod_{p\leq x} e^{2/p} = \exp(2\sum_{p\leq x}\frac{1}{p})$$ and take the $\log$ of each side I get $$\sum_{p\leq x} \frac{1}{p} > \frac{1}{2}\log\left(\sum_{n\leq x}\frac{1}{n}\right) > \frac{1}{2}\log\log x$$ which gives one side of the required inequality. Is there a way to get a similarly easy proof of the other side? Relatedly, is there a proof of $\sum_{p\leq x}\frac{1}{p} \sim \log\log x$ which is simpler than the one in Merten's theorems (and which does not rely on the PNT)? Mertens gives more, $$\sum_{p\leq x}\frac{1}{p}=\log \log x + M + O\left(\frac{1}{\log n}\right)$$ which is why I ask if there is a simpler proof which does not give the asymptotic term.","['analytic-number-theory', 'number-theory', 'prime-numbers']"
4362130,Finding $\int_{1}^{\infty} \frac{1}{1+x^2} \frac{\operatorname{Li}_2\left ( \frac{1-x}{2} \right ) }{\pi^2+\ln^2\left(\frac{x-1}{2}\right)}\text{d}x$,"Prove the integral $$\int_{1}^{\infty} \frac{1}{1+x^2}
\frac{\operatorname{Li}_2\left ( \frac{1-x}{2}  \right ) }{
\pi^2+\ln^2\left ( \frac{x-1}{2}  \right ) }\text{d}x
=\frac{96C\ln2+7\pi^3}{12(\pi^2+4\ln^2(2))} 
-\frac{\pi}{24}(3+4\pi)$$ where $\operatorname{Li}_2$ is dilogarithm and $C$ is Catalan's constant . (I checked it high precision. So I believe that it's absolutely true.) How we prove that? I tried to use functional equations of $\operatorname{Li}_2$ but I get nothing useful. Any suggestion will be appreciated.","['integration', 'definite-integrals', 'polylogarithm', 'closed-form', 'catalans-constant']"
4362134,A curious family of Chebyshev-like polynomials,"Consider the family $f_n(x)$ of functions of $x$ for $0\leq x\leq1$ , each indexed by a variable $n \in \mathbb{N}$ , described by the following equation: $$f_n(x) = \sin^2\left(n \arcsin\left(\sqrt{x}\right)\right)$$ Evaluation in numerical algebra programs yields the following polynomial forms of each function $f_n(x)$ for the first few $n$ : $f_1(x) = x$ $f_2(x) = 4x - 4x^2$ $f_3(x) = 16 x^3 - 24 x^2 + 9 x$ $f_4(x) = -64 x^4 + 128 x^3 - 80 x^2 + 16 x$ and so on. Is there any way to analytically derive the polynomial form of this family of functions in terms of $n$ and $x$ (i.e., $f_n(x) = g(n,x)$ where $g$ is a polynomial in $x$ )? Or perhaps this family of polynomials is related to another ""named"" family of polynomials under an appropriate transformation? COMMENT: The ""Chebyshev-like"" qualifier in the title comes from the fact that the Chebyshev polynomials $T_n(x)$ can be defined by a similar trigonometric identity: $T_n(x) = \cos(n\arccos(x))$ .","['trigonometry', 'polynomials']"
4362143,Why should I care about Gauss-Bonnet (and Gaussian curvature)?,"The Gauss-Bonnet Theorem (for orientable surfaces without boundary) states that for surface $M$ , with Gaussian curvature at a point $K$ , we have $$\int_M K\ dA=2\pi\chi(M).$$ Right now, this just says to me that the integral of something I don’t care about is equal to the ratio of a circumference to radius of a (Euclidean) circle times something I do care about. I get why normal curvature is interesting. I get Gaussian curvature is important in the Theorema Egregium and classification of surfaces of constant curvature. In the former, it helps classify surfaces up to isometry and show that developable surfaces are ruled. In the latter, it tells us that Riemannian manifolds with transitive isometry groups locally have a structure from a very small selection. But that’s just Gaussian curvature giving us other things. I don’t understand why I should like it for its own sake, and as such, why I should care about Gauss-Bonnet. I get that it links differential geometry and topology, but maybe there are other ways to do this, and this one feels a little convoluted, and topology is already linked to differential manifolds, and thus Riemannian manifolds, through Poincare-Hopf.","['curvature', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
4362174,Duality of linear independence and span,"I have observed the following phenomenon in linear algebra. Let $V$ , $W$ be two vector spaces over a field $F$ . Let $T:V\to W$ be a linear map. On object level, we have If $S'\subseteq S\subseteq V$ subsets, then $S$ is a linearly independent set $\implies$ $S'$ is a linearly independent set. $S'$ is a spanning set $\implies$ $S$ is a spanning set. On morphism level, we have If $S\subseteq V$ is a subset, then If $T$ is injective on $S$ , then $T(S)$ is a linearly independent set $\implies$ $S$ is a linearly independent set. If $T$ is surjective, then $S$ is a spanning set $\implies$ $T(S)$ is a spanning set. I have a strong feeling linear independence is ""dual"" to span, but cannot find a categorical formulation about this anywhere. Is there a systematic way to formulate this phenomenon in category theory? Thanks a lot in advance.","['linear-transformations', 'linear-algebra', 'vector-spaces', 'category-theory']"
4362224,Expected value of random expressions,"I'm trying to solve this math puzzle: write numbers $1$ to $N$ in a row. Randomly insert $+$ or $\times$ between two adjacent numbers with equal probability. What is the expected value of the expression if the expression is evaluated as an ordinary arithmetic expression? For instance, $1 + 2 \times 3$ will be evaluated as $1 + (2\times3)$ . Initially I thought that it was a simple recursion, but then I realized that $\times$ would change the precedence of the entire expression and then got stuck. It's easy to find a solution for a small enough $N$ with code, but I'm curious how one can solve it with math. Thanks,","['puzzle', 'probability']"
4362225,Why does having an integral model make étale cohomology unramified?,"Let $K$ be a number field, and $\mathcal O$ its ring of integers. Fix a finite set $S$ of rational primes, and let $\mathcal O_S$ be the ring of integers with these primes inverted. Let $X$ be a smooth proper scheme over $K$ . Suppose that $X$ arises as the base change of a smooth proper $\mathfrak X$ defined over $\mathcal O_S$ . Fix a non-archimedean place $v$ of $K$ not lying over any prime in $S$ , and write $K_v$ for the completion of $K$ at $v$ . Let us consider the étale cohomology groups $\mathrm{H}^i(X_v \times_{K_v} \overline K_v, \mathbf{Q}_\ell)$ as Galois representations of $\mathrm{Gal}(\overline K_v/K_v)$ . It seems that the existence of $\mathfrak X$ implies that these representations are unramified at $v$ (i.e. the inertia group $I_v$ acts trivially). Why is this the case?","['etale-cohomology', 'algebraic-geometry', 'galois-representations']"
4362269,Torsion at base change of group schemes,"I am slightly confused about how torsion behaves with base change of group schemes. These notes ( https://www.math.uni-bonn.de/ag/alggeom/veranstaltungen/Vorlesungen/2015ws_vl_morrow/2015ws_vl_p-div%20final.pdf , remark 6.1) seem to imply that torsion commutes with base change. However, this blog post ( https://simpletonsymposium.wordpress.com/2013/08/09/p-divisible-groups/?fbclid=IwAR20ydyqu1uCbfDY-3Cu83ThWSrGTjFfYFddc4Kdrhg7nEaNsNhbpq1MFas ) suggest with the example at the end that this is not always the case, since we don't always have $$ R[[x]]/[p](x) \otimes_R K \cong K[[x]] /[p](x)$$ and the LHS is just the $p$ -torsors of the group scheme whereas the right hand side are the $p$ -torsors of the group scheme after changing base. Are they both right and I am misinterpreting something?","['group-schemes', 'algebraic-geometry', 'schemes']"
4362326,Calculate the area of ​the shaded region BPIQ,"In the figure, $P$ , $Q$ and $I$ are the incenters of the
triangles $\triangle AHB$ , $\triangle BHC$ and $\triangle ABC$ respectively. Calculate the area of ​​the shaded region if $MN = a$ . (Answer: $\frac{a^2}{2}$ ) My progress: $S_{BPQI} = S_{\triangle BPQ}-S_{\triangle BQI}.$ $P$ is incenter, therefore $BP$ is angle bisector of $\angle ABH$ . Let $\angle ABP = \angle PBI = \alpha$ . But $JM \parallel JB \implies \angle BPM = \alpha$ . Therefore $ \triangle MPB$ is isosceles. Similarly $\triangle BNQ$ is isosceles. ....??","['euclidean-geometry', 'area', 'geometry', 'plane-geometry']"
4362350,"If normal to curve $x=3\cos\theta-\cos^3{\theta}$ and $y=3\sin\theta-\sin^3{\theta}$ at point $P(\theta)$ passes through the origin, then $\theta=$","If normal to curve $x=3\cos\theta-\cos^3{\theta}$ and $y=3\sin\theta-\sin^3{\theta}$ at point $P(\theta)$ passes through the origin, then $\theta=$ (A) $0$ (B) $\dfrac{\pi}{4}$ (C) $\dfrac{\pi}{2}$ (D) $\dfrac{\pi}{6}$ My Approach: I got the slope of normal as $\tan^3{\theta}.$ So equation of normal will be $$y-3\sin\theta +\sin^3{\theta}=\tan^3{\theta \cdot(x-3\cos\theta+3\cos^3{\theta})}$$ Because it passes through origin so I put $x=0,y=0$ , which lead me to the equation, $$3\sin\theta \cdot \cos{\theta} \cdot\cos{2\theta}=0$$ So value of $\theta$ will be $\theta=0, \dfrac{\pi}{2},\dfrac{\pi}{4}.$ But answer given is only $\theta = \dfrac{\pi}{4}$ My doubt: Why option (A) and (C) are wrong.","['trigonometry', 'derivatives', 'polar-coordinates']"
4362371,Solution to $\frac{1}{6}g''=v g^3+k g^3-k g$,"I have a nice problem with a not-so simple solution. Consider the following differential equation: $$\frac{1}{6}g''=v g^3+kg^3 - k g$$ with boundary conditions $g(0)=0$ and $g(\infty)=1$ , where $k>0$ and $v \geq 0$ . What i've tried so far:
Multiply by $g'$ and integrate once: $$(g')^2=3\left[(k+v)g^4-2k g^2+(k-v)\right]$$ where I've added the integration constant $(v+k)$ to ensure that $g(\infty)=1$ and thus $g'(\infty)=0$ . which has a nice solution for $v=0$ , $g'=3k(1-g^2)$ , thus $g=\tanh{(3k x)}$ . However,such a nice factorization of the polynomial does not hold for $v \neq 0$ . This is where your help may come in, if anyone has any suggestions, it would be greatly appreciated.","['calculus', 'ordinary-differential-equations', 'real-analysis']"
4362400,Assumptions on a scheme X needed to construct the Picard scheme $\operatorname{Pic}(X)$,"Let $X$ be a scheme. I believe that Grothendieck was the first to put a scheme structure on the Picard group $\operatorname{Pic}(X)$ , where he assumed that X is projective and reduced. Then Mumford constructed a Picard scheme in the non-reduced case, still assuming projectivity, and possibly with other assumptions. I would like to know why the projective hypothesis is necessary, and what are the key differences between the reduced and non-reduced cases. Many thanks!","['algebraic-geometry', 'moduli-space', 'picard-scheme']"
4362426,Extremely slowly divergent series - Putnam 2008 A.4 generalization,"INTRODUCTION Starting from Putnam 2008, Exercise A.4, I tried to generalize the result to series that diverge slowlier than the one presented there. Since I am not too familiar with the necessary instruments (e.g. the Lambert function), I wanted to show my partial results and ask you to comment them, find any mistake, or give some insight on the situation. (I HAVE DONE SOME EDITING on the last two paragraphs, to account for a mistake I discovered) ORIGINAL SETTING The original divergent series is constructed as follows. Let $$f(x)= 
\begin{cases}
x & (x \leq e)\\
x\log f(x) & (x>e).
\end{cases}
$$ Then the series $$\sum_{n=1}^{\infty} \frac1{f(n)}$$ diverges. The function is basically a product of nested logarithms , where the number of logarithmic factors grows with $x$ , in such a way that $f(x)$ is continuous and monotonically increasing. Thus divergence is proved via divergence of the integral $$\int_e^{\infty} \frac1{f(x)}dx.$$ GENERALIZATION (CLAIM) Let $\alpha$ be a real number such that $$1< \alpha\leq \frac{e}{e-1}\tag{1}\label{1}$$ and $$\beta = e^{-\frac{\alpha}{\alpha-1}\cdot W\left(-\frac{\alpha-1}\alpha\right)},\tag{2}\label{2}$$ where $W(x)$ is the principal branch of the Lambert function. Let also $E_1 = e^\beta$ and $$E_k = e^{E_{k-1}}$$ for $k>1$ . Similarly define $L_1(x) = \log x$ and $$L_k(x) =\log L_{k-1}(x)$$ for $k>1$ . If $f(x)$ is defined as $$f_\alpha(x)=\begin{cases}
x\log^\alpha x & \left(e < x \leq e^\beta\right)\\
x\log x\log^\alpha\log x & \left( e^\beta <x \leq e^{e^\beta}\right)\\
x\log x\log\log x \log^\alpha \log\log x & \left(e^{e^\beta} < x \leq e^{e^{e^\beta}}\right)\\
\vdots & \vdots \\
x\cdot \prod_{k=1}^{m}L_k(x) \cdot L_{m+1}^\alpha(x) & (E_m < x \leq E_{m+1})\ \ \ (m\geq 1)\\
\vdots & \vdots.
\end{cases}
$$ Then the series $$\sum_{n=3}\frac1{f_\alpha(n)}\tag{3}\label{3}$$ diverges. SOME NOTES In order to guarantee monotonicity of the piecewise function $f_\alpha(x)$ we need the constituent functions to behave properly. This gives limitations on $\alpha$ given by \eqref{1} and the relationship between $\alpha$ and the connection points given by \eqref{2}. First of all we wish the component to intersect at some point, and this happens if, for some $\overline x$ , $$x\prod_{k=1}^m L_k(\overline x) L_{m+1}^\alpha(\overline x) = x\prod_{k=1}^{m+1}L_k(\overline x) L_{m+2}^\alpha(\overline x) $$ that is $$L_{m+1}^{\alpha-1}(\overline x) = L_{m+2}^{\alpha} (\overline x),$$ Letting $\xi = L_{m+1}(\overline x)$ we obtain the equation $$ \xi^{\alpha-1} = \log^\alpha\xi.$$ This equation has only one solution when $\alpha = \frac{e}{e-1}$ (the two red lines in the Figure below, that are tangent in point $B$ ), and has to solutions
for $0< \alpha <\frac{e}{e-1}$ . In the latter case, the smallest one is $$\beta = e^{-\frac{\alpha}{\alpha-1}\cdot W\left(-\frac{\alpha-1}\alpha\right)},$$ (the larger one being given by the lower branch of the Lambert function) from which we obtain the connection point $$\overline x = E_{m+1}.$$ In the Figure, the case $\alpha = \frac32$ is depicted ( $A$ marks the intersection between the two lines). Note that, from the asymptotic beahviour of the logarithmic function, we can also deduce that in a right neighborhood of $\beta$ we have $$x\prod_{k=1}^m L_k(x) L_{m+1}^\alpha(x) < x\prod_{k=1}^{m+1}L_k(x) L_{m+2}^\alpha(x).$$ Below a plot of the function $$\beta = \beta(\alpha)$$ (equation \eqref{2}) for $0 <\alpha \leq \frac{e}{e-1}$ . I haven't studied this function formally but we surely have $$\lim_{\alpha \to \frac{e}{e-1}}\beta(\alpha)= e^{-e\cdot W\left(-\frac1{e}\right)}=e^e,\tag{4}\label{4}$$ and it seems we get $$\lim_{\alpha \to 1} \beta(\alpha) = e \tag{5}\label{5}.$$ In particular \eqref{5} shows that we can think of the series \eqref{3} as a generalization of the one proposed in the Putnam competition (where $\alpha = 1$ , $\beta =e$ ). It appears that the reasoning (also in what follows) can be extended to the more general
case $0<\alpha\leq \frac{e}{e-1}$ . However the divergence when $0<\alpha< 1$ is trivial. When $\alpha > 1$ , though, each constituent function would give rise by itself to a convergent series (by Condensation Test), and yet our series diverge. PROOF OF DIVERGENCE (SKETCH) As stated we need to prove that the integral diverges. Now, we have $$\mathcal I_0 = \int_e^{e^\beta} \frac1{x \log^{\alpha} x}dx = -\frac1{\alpha-1}\left[\frac1{\log^{\alpha-1}x}\right]^{e^\beta}_e=\frac1{\alpha-1}\left(1-\frac1{\beta^{\alpha-1}}\right),$$ which, using \eqref{2}, can be expressed as $$\mathcal I_0 = \frac1{\alpha-1}\left[1-e^{\alpha\cdot W\left(-\frac{\alpha-1}{\alpha}\right)}\right].$$ For all the other intervals we get $$\mathcal I_1 = \int_{E_m}^{E_{m+1}} \frac1{x\prod_{k=1}^m L_m(x)\cdot L_{m+1}^{\alpha}(x)}dx =-\frac1{\alpha-1}\left[\frac1{L_{m+1}^{\alpha-1}(x)}\right]_{E_m}^{E_{m+1}}=\frac1{\alpha-1}\left(\frac1{\log^{\alpha-1}\beta}-\frac1{\beta^{\alpha-1}}\right).$$ Using again \eqref{2} yields $$\mathcal I_1 = \frac1{\alpha-1}\left[\frac1{\left(-\frac{\alpha}{\alpha-1}\cdot W\left(-\frac{\alpha-1}{\alpha}\right)\right)^{\alpha-1}}-e^{\alpha\cdot W\left(-\frac{\alpha-1}{\alpha}\right)}\right].$$ A plot of $\mathcal I_0$ (yellow) and $\mathcal I_1$ (green) as functions of $\alpha$ is depicted below. Here again a more formal study of these functions is required. From what I have done so far, it appears that both are limited functions of $\alpha$ . We conclude that the integral diverges, and so does the series. Also, as $\alpha$ approaches $\frac{e}{e-1}$ the ""speed"" at which the series diverges rapidly decreases, since the integral increments are less then $1$ in the (larger and larger) intervals $[E_m, E_{m+1}]$ . QUESTIONS First of all, I am interested in spotting any error in my approach. Secondarily, since I have been largely qualitative in some steps of the reasoning, I was wondering if everything can be made more formal, in the last two paragraphs above especially (study of $\beta(\alpha)$ , $\mathcal I_0(\alpha)$ , and $\mathcal I_1(\alpha)$ , for example). Of course any other insight, comment, suggestion is more than welcome.","['convergence-divergence', 'solution-verification', 'sequences-and-series', 'real-analysis']"
4362451,A rare integral involving $\operatorname{Li}_2$,"A rare but interesting integral problem: $$\int_{0}^{1} 
\frac{\operatorname{Li}_2(-x)-
\operatorname{Li}_2(1-x)+\ln(x)\ln(1+x)+\pi x\ln(1+x)
-\pi x\ln(x)}{1+x^2}\frac{\text{d}x}{\sqrt{1-x^2} } 
=\frac{\pi^3}{48\sqrt{2} }.$$ Where $\operatorname{Li}_2$ is dilogarithm . The integral without the factor $\frac{1}{\sqrt{1-x^2} }$ is much easier(it can be expressed using $\operatorname{Li}_3$ , $\ln(2)$ , Catalan's constant and $\pi$ ).
However, the same idea dosen't work for this one. Any suggestion will be much appreciated.","['integration', 'definite-integrals', 'real-analysis', 'polylogarithm', 'closed-form']"
4362461,The solution set of equation $\sin^{-1}{\sqrt{1-x^2}}\;+\cos^{-1}x=\cot^{-1}{\dfrac{\sqrt{1-x^2}}{x}}-\sin^{-1}x$,"Find the solution set of equation $\sin^{-1}{\sqrt{1-x^2}}\;+\cos^{-1}x=\cot^{-1}{\dfrac{\sqrt{1-x^2}}{x}}-\sin^{-1}x$ My Approach: $\sin^{-1}{\sqrt{1-x^2}}\;+\cos^{-1}x \; +\sin^{-1}x=\cot^{-1}{\dfrac{\sqrt{1-x^2}}{x}}$ $\implies$$\sin^{-1}{\sqrt{1-x^2}}\;+ \frac{\pi}{2}=\cot^{-1}{\dfrac{\sqrt{1-x^2}}{x}}$ $\implies$$\sin^{-1}{\sqrt{1-x^2}}\;+ \frac{\pi}{2}-\cot^{-1}{\dfrac{\sqrt{1-x^2}}{x}}=0$ $\implies$$\sin^{-1}{\sqrt{1-x^2}}\;+\tan^{-1}{\dfrac{\sqrt{1-x^2}}{x}}=0$ $\implies$$2\sin^{-1}{\sqrt{1-x^2}}=0$ I am obtaining two values of $x$ and those value are $x=1,{-1}.$ But given answer is $(-1,0)\cup \{1\}$ . What am I doing wrong?","['trigonometry', 'inverse-function']"
4362519,Doubt regarding a differential equation.,"There's a problem in a book regarding exact differential equation : Solve : $$x\,dx+y\,dy = \frac{a^2(x\,dy-y\,dx)}{x^2+y^2}$$ The author further proceeds to rearrange above in the form $M\,dx+N\,dy=0$ where $$M=x+\frac{a^2y}{x^2+y^2} ; N= y-\frac{a^2x}{x^2+y^2}$$ Which further implies $$M_y=N_x=\frac{a^2(x^2-y^2)}{(x^2+y^2)^2}$$ This is the required condition for the given equation to be exact and the solution is obtained using standard formula. But, what I did is as follows : $$x\,dx+y\,dy = \frac{a^2(x\,dy-y\,dx)}{x^2+y^2}$$ $$\to (x^2+y^2)x\,dx + (x^2+y^2)y\,dy=a^2(x\,dy -y\,dx)$$ Therefore $$(x^3+xy^2+a^2y)\,dx + (y^3+yx^2-a^2x)\,dy =0$$ Comparing it with the equation $M\,dx+N\,dy=0$ we get, $$M=x^3+xy^2+a^2y; N=y^3+yx^2-a^2x$$ But $$M_y=2xy+a^2; N_x=2xy-a^2$$ Evidently, $M_y\neq N_x$ . So what am I doing wrong here? At first I thought I shouldn't/couldn't just simply multiply both sides of the given equation with the denominator ( $x^2+y^2$ ) of right hand side if it's zero, but since as it's already in denominator isn't it understood that it isn't/can't be zero ?? Any help is really appreciated.","['differential', 'ordinary-differential-equations']"
4362572,Doubt regarding Conditional Expectation,"Let $X,Y$ be IID random variable that are uniformly distributed on $[0,1]$ . Does the following equation hold? $$\text{E}[X\vert X\geq Y] = \int_{-\infty}^{\infty} \text{E}[X\vert X\geq y]f_Y(y)\,\text{d}y$$ This would then imply that $$\text{E}[X\vert X\geq Y] = \int_0^1 \text{E}[X\vert X\geq y]\,\text{d}y$$ where $$f_{X\vert X\geq y}(x) = \begin{cases}\frac{1}{1-y}&y\leq x\leq 1\\0&\text{otherwise}
\end{cases}$$ such that $$\text{E}[X\vert X\geq y] = \int_y^1 x\,\frac{1}{1-y}\,\text{d}x = \frac{1+y}{2}$$ Finally, we obtain $$\text{E}[X\vert X\geq Y] = \int_0^1 \frac{1+y}{2}\,\text{d}y = \frac{3}{4}$$ which is not correct. Hence, there must be an error somewhere. I appreciate your help! For completeness, the (presumably) correct solution is given below. $$\text{E}[X\vert X\geq Y] = \int_{-\infty}^{\infty} x f_{X\vert X\geq Y}(x)\,\text{d}x$$ Using Bayes' rule, we obtain \begin{align}
f_{X\vert X\geq Y}(x) &= \frac{\text{Pr}[X\geq Y\vert X = x] f_X(x)}{\text{Pr}[X\geq Y]}\\
&= \frac{\text{Pr}[Y\leq x] f_X(x)}{1/2}\\
&= \begin{cases}
2x& x\in[0,1]\\
0&\text{otherwise}
\end{cases}
\end{align} such that $$\text{E}[X\vert X\geq Y] = \int_{-\infty}^{\infty} x \cdot 2x\,\text{d}x = \frac{2}{3}$$ Answer All credit goes to @Thomas who found the mistake in my initial approach. Thank you @Thomas and thanks to those who added helpful answers or comments. The main issue lies in my very first equation, namely $$\text{E}[X\vert X\geq Y] = \int_{-\infty}^{\infty} \text{E}[X\vert X\geq y]f_Y(y)\,\text{d}y$$ As @Thomas suggested, the density $f_Y(y)$ should also be conditioned on $X\geq Y$ , that is, $f_{Y\vert X\geq Y}(y)$ . The corrected equation is given as $$\text{E}[X\vert X\geq Y] = \int_{-\infty}^{\infty} \text{E}[X\vert X\geq y]f_{Y\vert X\geq Y}(y)\,\text{d}y$$ with \begin{align}
f_{Y\vert X\geq Y}(y) &= \frac{P(X\geq Y\vert Y = y)f_Y(y)}{P(X\geq Y)}\\
&= \frac{P(X\geq y)f_Y(y)}{P(X\geq Y)}\\
&= \begin{cases}
\frac{1-y}{1/2} &0\leq y \leq 1\\
0 &\text{otherwise}
\end{cases}\\
&= \begin{cases}
2(1-y) &0\leq y \leq 1\\
0 &\text{otherwise}
\end{cases}
\end{align} This results in the following solution. \begin{align}
\text{E}[X\vert X\geq Y] &= \int_{-\infty}^{\infty} \text{E}[X\vert X\geq y]f_{Y\vert X\geq Y}(y)\,\text{d}y\\
&= \int_0^1 \text{E}[X\vert X\geq y]\,2(1-y)\,\text{d}y\\
&= \int_0^1 \frac{1+y}{2}\,2(1-y)\,\text{d}y\\
&= \int_0^1 (1-y^2)\,\text{d}y\\
&= \left[y - \frac{1}{3}\,y^3\right]_0^1\\
&= \frac{2}{3}
\end{align} where $\text{E}[X\vert X\geq y] = \frac{1+y}{2}$ follows from the computations above (initial approach).","['expected-value', 'conditional-probability', 'probability-theory', 'probability']"
4362594,Mean and characteristic function of $Y=\sum_{n=1}^{\infty}\prod_{k=1}^{n}X_k$,"I have problems with this exercise. Let $X_1, X_2, \ldots $ r.v. independent and equally distributed exponential with parameter $\lambda > 1$ . Verify if random variable $$Y=\sum_{n=1}^{\infty}\prod_{k=1}^{n}X_k$$ has finite mean and calculate the characteristic function. First of all, I need to calculate $$E[Y]=E\left[\sum_{n=1}^{\infty}\prod_{k=1}^{n}X_k\right]=\sum_{n=1}^{\infty}E\left[\prod_{k=1}^{n}X_k\right],$$ this is true becase the r.v. are positive. $$\sum_{n=1}^{\infty}E\left[\prod_{k=1}^{n}X_k\right]=\sum_{n=1}^{\infty}\prod_{k=1}^{n}E\left[X_1\right],$$ this is true since r.v. are independent and equally distributed. $$\sum_{n=1}^{\infty}\prod_{k=1}^{n}E\left[X_1\right]=\sum_{n=1}^{\infty}\prod_{k=1}^{n}1/\lambda=\sum_{n=1}^{\infty}(1/\lambda)^n=\frac{1}{\lambda-1},$$ since r.v. is exponential distributed. Am I right? Or there is a wrong step? If I am right how I can calculate the characteristic function with that expected value? Any help?","['expected-value', 'characteristic-functions', 'probability-theory']"
4362678,Encoding information by hiding digits,"Good evening, I have been stumbled by this problem. Two magicians, call them A and B, play a trick: while B is not looking, A asks a spectator to write an n-digit number on a board. A then cancels two digits. B comes back, being able to see the remaining n-2 digits, in order, and the positions of the two cancelled digits. He guesses, correctly, the two cancelled digits in order. For which n is this possible? An easy induction can prove that if the trick works for $n$ , it works for all numbers greater than $n$ . I have found a way to make it work for $n = 20$ : supposing the sum of the first $10$ digits is $k \mod10$ and the sum of the other $10$ digits is $l \mod 10$ , A just hides the $k$ -th and $(10+l)$ -th digit. However, I have not been able to prove this is a lower bound. In fact, the ""natural"" argument leads me to think the true bound is $n \geq 15$ : there are $10^n$ possible numbers, and $b$ can receive $10^{(n-2)}{{n}\choose{2}}$ different strings. For the trick to work, we then need ${{n}\choose{2}} \geq 100$ which implies $n \geq 15$ . Is there a way to explicitly construct the algorithm for the trick in case $n = 15$ ?
This is a variation of a well-known problem, which has already been asked about and answered on this site, in which the cancelled digits are adjacent, which results, much more easily, in $n \geq 101$ . Edit: I think the most straightforward solution is to forsake every hope of an explicit construction and apply Hall's marriage theorem (this is akin to finding a perfect matching on the bipartite graph between the set of numbers and the set of strings with two numbers hidden, and this graph satisfies the marriage condition by a double counting argument). I have thus proven that $15$ is the lower bound, but still have not been able to describe the strategy for $15 \leq n < 20$ Edit 2: The magicians are collaborating (there would be no solution if they were not), the spectator is not","['graph-theory', 'combinatorics', 'contest-math']"
4362698,Solving trigonometric equation $\sin(2x) = \frac{1}{2}$,"I have tried to solve trigonometric equations in $2$ different ways. I know that only the Case $1$ is the correct way of solving the equation, but I don’t understand why case $2$ is an incorrect way of solving the equation. After looking at the respective unit circle of case $1$ and $2$ I feel like case $2$ should be correct since both the answers of case $2, 15, 165$ have the same $y$ coordinates on the unit circle i.e. they have the same sin theta value. Whereas the values of case $1$ have different $y$ coordinates and hence different value of $\sin (\theta)$ . Can someone explain why case $2$ is incorrect by using the unit circle in the explanation. Case $1$ : $\sin (2x)$ = $\frac{1}{2}$ $\arcsin (\frac{1}{2}) = 2x$ $30 = 2x$ $180 - 30 = 2x $ $150 = 2x $ $X = 15, 75$ Case $2$ : $\sin (2x) = \frac{1}{2}$ $\arcsin (\frac{1}{2}) = 2x$ $30 = 2x$ $X = 15$ $X = 180 - 15$ $X = 165$","['algebra-precalculus', 'solution-verification', 'roots', 'trigonometry']"
4362746,Nontrivial 'classical' examples of contracting differential transformations on compact Riemannian manifolds,"Let $M$ be a Riemannian manifold. A differentiable map $f:M\to M$ is called contracting (self-made definition) if the linear map $D_x f:T_x M \to T_x M$ has the operator norm bounded by $1$ w.r.t. the norms on $T_x M$ induced by Riemannian metric on it for all $x\in M$ . The only examples that I am aware of are either isometries or constant maps. I wonder if there are any relatively nontrivial, natural, classical examples of contracting maps on compact Riemannian manifolds. (Please don't worry too much about the words ""natural"" and ""classical"" too much...)","['riemannian-geometry', 'differential-geometry']"
4362803,"""Set"" vs ""Space"" in definitions?","I have seen many definitions in two versions: sometimes referred to sets, sometimes to spaces. Some examples : closed set/space, compact set/space, $F_{\sigma\delta}$ set/space. I asked one of my lecturers about this and they said that if I introduce some definition with ""space"", I need to explain it more. So I am thinking, defining something as a set is more general, while defining on a space requires looking also at the space structure? Or does this depend on particular case? I am particularly interested in the case with $F_{σδ}$ sets vs spaces, but I think it is important question in general. Thank you for your advice.","['borel-sets', 'general-topology', 'definition', 'terminology']"
4362815,Why do we need surface and line integrals when we already have the Lebesgue integral?,"I really don't understand why we need surface and line integrals when we can already integrate on Lebesgue measurable subsets of $\mathbb{R}^3$ . Aren't surfaces and curves Lebesgue measurable sets? I will write here exactly how surface integrals and line integrals were defined to me because I suspect that my lecturer doesn't take the canonical approach (see this other question of mine about the notation for an inner product that I am going to use and that he uses). A (space) curve $\gamma$ is a $C^1$ class function defined on some compact set $[a, b]\subset \mathbb{R}$ that takes values in $\mathbb{R}^3$ , i.e. $\gamma:[a, b]\to \mathbb{R}^3$ is a $C^1$ class function putting words into notation. We say that a function $F:\gamma([a, b])\to \mathbb{R}^3$ is integrable on $\gamma$ if the function $(F\circ \gamma | \gamma')_{\mathbb{R}^3}:[a, b]\to \mathbb{R}$ is $\lambda_1$ -integrable on [a, b], where $\lambda_1$ is the Lebesgue measure on $[a, b]$ (so the one dimensional Lebesgue measure, hence the $1$ ). In this case, we define $$\int_{\gamma}(F|dl)_{\mathbb{R}^3}=\int_{[a,b]}(F\circ \gamma | \gamma')d\lambda_1.$$ Similarly, a surface in $\mathbb{R}^3$ is a $C^1$ class function $\sigma:[a_1, b_1]\times [a_2, b_2]\to \mathbb{R}^3$ and we say that a function $F:\sigma([a_1, b_1]\times[a_2, b_2])\to \mathbb{R}^3$ is integrable on $\sigma$ if $(F\circ \sigma | \frac{\partial \sigma}{\partial u}\times \frac{\partial \sigma}{\partial v}):[a_1, b_1]\times [a_2, b_2]\to \mathbb{R}$ is $\lambda_2$ -integrable on $[a_1, b_1]\times[a_2, b_2]$ (here $\lambda_2$ is the Lebesgue measure on $\mathbb{R}^2$ ). In this case, we define $$\int_{\sigma}(F|ds)_{\mathbb{R}^3}=\int_{\sigma}\left(F\circ \sigma | \frac{\partial \sigma}{\partial u}\times \frac{\partial \sigma}{\partial v}\right)_\mathbb{R}^3 d\lambda_2.$$ What I don't get is why we need these definitions to integrate on, say, $\gamma$ . Basically, won't $\gamma([a, b])\subset \mathbb{R}^3$ be Lebesgue measurable because it is a compact set (it is the continuous image of a compact set)? Don't I just know already how to integrate on $\gamma([a, b])$ since I know how to do Lebesgue integrals? The definition given by my professor actually looks like some kind of change of variable to me, i.e. I believe that we can in fact write $\displaystyle \int_{\gamma([a, b])}F(x, y, z) dxdydz$ without any further ado and he just makes the change of variable $(x, y, z)=\gamma(t)$ , giving him that this equals $\displaystyle \int_{[a,b]}(F\circ \gamma | \gamma')d\lambda_1$ , and he just decides to call this the definition of the integral on $\gamma$ purely for computational convenience, even though the notion is allready well defined. Am I correct? This is purely intuitive, I haven't learned the change of variable for the Lebesgue integral. I don't really see any other point in calling this a definition if we are using the Lebesgue measure and our curves and surfaces are $C^1$ so that we can make different changes of variable. If we were to use Riemann integrals instead of Lebesgue integrals, I think that we indeed need to define what it means to integrate on some curve $\gamma$ or some surface $\sigma$ simply because compact sets are not necessarily Jordan measurable (and as far as I am concerned we can't really do Riemann integrals on sets that are not Jordan mesurable even if they are bounded - this is simply because we will end up with some continuous functions not being integrable and this is definitely not something we want). So yeah, in that case we would be integrating on some ""new"" sets, but in my setting I don't really see the point. EDIT: I should also say that I am basically assuming some regularity on $F$ , i.e. that it is Lebesgue integrable, but this feels natural.","['integration', 'measure-theory', 'real-analysis', 'multivariable-calculus', 'calculus']"
4362818,What types of axioms retain consistency of ZFC under additional axioms about its consistency?,"My question is pretty simple. If ZFC does not prove that it is not consistent, then can we add the axiom to ZFC that it proves it is not consistent and is consistent and achieve equiconsistency? I think I got confused because obviously we can add the axiom to ZFC that it is not consistent and achieve equiconsistency, but can we also do the same when we say ZFC proves it is not inconsistent (while it does not) and that it is consistent? This was inspired by the list of options of consistency of ZFC, where they say ZFC could be consistent but prove it is not. To clarify, as the comments have pointed out, I want to know if ZFC + ZFC proves not(Con(ZFC)) + Con(ZFC) is consistent if ZFC does not prove not(Con(ZFC)).","['elementary-set-theory', 'logic', 'metalogic']"
4362895,How does one show that $\left|\operatorname{sgn}(a)|a|^{1/2}-\operatorname{sgn}(b)|b|^{1/2}\right|^2\le2|a-b|$ using more elementary techniques?,"$\newcommand{\sgn}{\operatorname{sgn}}$ It was left as an exercise to show that: $\forall a,b\in\Bbb R,$ $$\left|\sgn(a)|a|^{1/2}-\sgn(b)|b|^{1/2}\right|^2\le2\cdot|a-b|$$ The only techniques I have seen used to solve these problems (they seem to occur often in the functional analysis of $\mathcal{L}^p$ spaces - I've seen this method used in the proofs of Banach-Saks and some other theorems) is to do a division and show that the resulting rational function is bounded and positive in the region of interest by taking limits. My solution given below works, but it felt like more work than should have been necessary for this problem - I am asking if there are simpler approaches. I then am reducing this problem to showing that: $$\frac{|\sgn(a)|a|^{1/2}-\sgn(b)|b|^{1/2}|^2
}{|a-b|}\le2$$ Always. Since the expression is symmetric in $a,b$ , assume wlog that $a\ge b$ . Note of course that the expression is always positive, so we need only show that it is bounded from above. Consider then the above as a function $f(a)$ defined on regions where $a\ge b$ . If $b=0$ the expression is clearly $1\le 2$ for all $a$ , so instead take $b\gt 0$ as a first case. For $a\ge b\gt 0$ : $$f(a)=\frac{a+b-2\sqrt{ab}}{a-b}=\frac{(a^{1/2}-b^{1/2})^2}{(a^{1/2}-b^{1/2})(a^{1/2}+b^{1/2})}=\frac{a^{1/2}-b^{1/2}}{a^{1/2}+b^{1/2}}$$ As $a\to b^+$ the limit is $0\le 2$ ; now take its derivative: $$f'(a\ge b\gt0)=\frac{\sqrt{b/a}}{a+b+2\sqrt{ab}}\gt0$$ So as $f'$ is continuous we have that $f$ is a strictly increasing function, beginning (in the range $a\ge b\gt0$ ) at $0$ and has its upper bound at $\lim_{a\to\infty}f(a)=1\lt2$ . Therefore $0\le f\lt 2$ when $a\ge b\ge0$ . Now suppose $b\lt0$ . There are two cases; $a\gt0,b\le a\lt 0$ . $a=0$ is excluded as the function is clearly $1$ in this case also. First take $a\gt0$ : $$f(a)=\frac{a-b+2\sqrt{a|b|}}{a-b}\ge1$$ And: $$f'(a)=\frac{(1+\sqrt{|b|/a})(a-b)-a+b-2\sqrt{a|b|}}{(a-b)^2}=\frac{-b\sqrt{|b|/a}-\sqrt{a|b|}}{(a-b)^2}$$ Let the numerator be $h(a)$ . $h'(a)=-\frac{b}{2a}\sqrt{\frac{|b|}{a}}-\frac{1}{2}\sqrt{\frac{|b|}{a}}=0\iff a=-b$ , and as $a$ exceeds this point $h'(a)\lt 0$ clearly, so this is a maximum of $h$ , and $f(-b)=2$ and thus when $a\gt0,\,f\le 2$ . Now suppose $b\le a\lt 0$ : $$f(a)=\frac{a+b+2\sqrt{|a||b|}}{b-a}$$ And: $$f'(a)=\frac{(1-\sqrt{|b|/|a|})(b-a)+a+b+2\sqrt{|a||b|}}{(b-a)^2}=\frac{2b+\sqrt{|a||b|}-b\sqrt{|b|/|a|}}{(b-a)^2}$$ Call the numerator $g(a)$ . $g'(a)=-\frac{1}{2}\sqrt{|b|/|a|}-\frac{b}{2|a|}\sqrt{|b|/|a|}\gt0$ since $|b|\gt|a|$ . Therefore $g$ is increasing; take $\lim_{a\to b^+}g(a)=0$ , which implies that $g$ increases beyond $0$ and is always positive. Thus $f'$ is positive and $f$ increases. As $a\to 0^-$ , $f\to 1$ , and this implies that $f\le 1$ in the region $b\le a\le0$ . Having considered all cases for $a\ge b$ wlog, this concludes that $f\le 2$ . This was tedious work for me; I made many mistakes with absolute value derivatives along the way... was there a more elementary way of seeing this inequality? Royden leaves it as an exercise and I'm not great with inequalities - I presume one did not need to take this calculus approach. Note: it is left as another exercise to show that: $$\left|\sgn(a)\cdot|a|^2-\sgn(b)\cdot|b|^2\right|\le2\cdot|a-b|(|a|+|b|)$$ Which definitely suggests these problems have a simpler solution, since calculating the above using my calculus method would be very tedious.","['inequality', 'solution-verification', 'absolute-value', 'real-analysis']"
4362896,"Rudin Functional Analysis, Lemma 4.22","I am having difficulty understanding Rudin's proof on Lemma 4.22 of his Functional Analysis book. The assumption is that $M$ is a subspace of a normed space $X$ and $M$ is not dense in X. Rudin then claims that there exists $x_1 \in X$ whose distance from $M$ is 1, that is, $\inf \{||x_1 - y||: y \in M\} = 1$ . It is not so obvious to me why such an $x_1$ exists. May someone explain the logic to me? Does it have anything to do with the assumption "" $M$ is not dense in X""? Thanks in advance to everyone who's trying to help out. This is the screenshot of the whole Lemma and proof given by Rudin:","['normed-spaces', 'functional-analysis']"
4362903,Harmonic functions and coordinate transfomation,"Related to a question about coordinates, I've been given the following comment to consider: ""Local coordinate functions are given by suitable harmonic functions on subsets of the metric space, where ""harmonic"" is defined in metric terms."" Trying to comprehend and to confirm this statement leads me to the following: Let function $h : \mathbb R^2 \rightarrow \mathbb R$ be a harmonic function in terms of arguments $(x, y)$ ; i.e. $\frac{\partial^2}{\partial x}\left[ h \right]+ \frac{\partial^2}{\partial y}\left[ h \right] = 0.$ Now define another function, $f : \mathbb R^2 \rightarrow \mathbb R$ through $$
\begin{align*}
f\huge[ & \, x - \text{Sgn}[ \, y \, ] \, \sqrt{ \, \text{Abs}[ \, y \, ] \, }, & ~ \\ ~ & \text{Sgn}[ \, y \, ] \, \left( \sqrt{ \text{Abs}[ \, y \, ] \, \left(\text{Abs}[ \, y \, ] + \frac{1}{4}\right)} + \left( \frac{1}{4} \right) \, \text{Ln} \! \left[ \sqrt{4 \, \text{Abs}[ \, y \, ] + 1} + \sqrt{4 \, \text{Abs}[ \, y \, ]} \, \right] \right) \, {\huge]} := h[ \, x, y \, ].
\end{align*}
$$ Also, introduce new variables: $$ p := x - \text{Sgn}[ \, y \, ] \, \sqrt{ \, \text{Abs}[ \, y \, ] \, } $$ and $$ q := \text{Sgn}[ \, y \, ] \, \left( \sqrt{ \text{Abs}[ \, y \, ] \, \left(\text{Abs}[ \, y \, ] + \frac{1}{4}\right)} + \left( \frac{1}{4} \right) \, \text{Ln} \left[ \sqrt{4 \, \text{Abs}[ \, y \, ] + 1} + \sqrt{4 \, \text{Abs}[ \, y \, ]} \right] \right). $$ (As a motivation for these particular choices note that $$\int_0^k \! \! \! \sqrt{1 + (2 \, x)^2} \, {\rm d}x \, = \, k \, \sqrt{k^2 + \frac{1}{4}} + \left(\frac{1}{4}\right) \, \text{Ln} \left[ \sqrt{4 \, k^2 + 1 } + 2 \, k \right],$$ where $\frac{d}{dx} \left[ x^2 \right]  = 2 \, x$ , of course. See also this "" visual suggestion of the appearance of $(p, q)$ -coordinate lines relative to $(x, y)$ -coordinate lines"".) Consequently, $f[ \, p, \, q \, ] := h[ \, x, \, y \, ],$ and there is a one-to-one correspondence (map) $\psi : \{(x, y)\} \leftrightarrow \{(p, q)\}$ as described above. My question: Is function $f$ , as function $f[ \, p, \, q \, ]$ , a harmonic function in terms of arguments $(p, q)$ , too ? -- especially considering the particular choice of map $\psi$ between arguments $(p, q)$ of function $f$ , and $(x, y)$ of function $h$ .","['coordinate-systems', 'partial-differential-equations', 'differential-geometry']"
4362925,Finding level curves to function,"I am supposed to find and draw a few level curves for the function $g(x,y) = e^{\sqrt{x^2-y^2}}$ . I have already calculated the domain of the function: $Df=\lbrace(x,y) : y ≤ ±|x|\rbrace$ In order to find a few level curves, I began by calculating the following for a constant c: $e^{\sqrt{x^2-y^2}}=c$ , This gives $\sqrt{x^2-y^2}=\ln(c)$ and $c>0$ . The first level curve, when $c=1$ : $$g(x,y)=e^{\sqrt{x^2-y^2}}=1\implies x^2-y^2=0\implies y=±x,$$ which I can I can easily draw, but I am having trouble finding more level curves. It feels like they get very complicated e.g. when $c=1$ . How do I find values of $c$ that result in level curves that aren't too hard to draw?",['multivariable-calculus']
4362932,Is there a reason the prime factors of $|M_{24}|$ are all one less than the factors of $24$?,"Wikipedia says of the Mathieu group $M_{24}$ , a $5$ -transitive permutation group acting on $24$ points, $$ |M_{24}|= 2^{10}\cdot3^3\cdot5\cdot7\cdot11\cdot23. $$ The prime factors $2,3,5,7,11,23$ are all one less than one of the factors $3,4,6,8,12,24$ of $24$ . Am I crazy? Is this a coincidence, or does it admit an explanation? (I suppose it could be a combination of both: maybe the factors $11$ and $23$ for some $24$ -related reason and $2,3,5,7$ because of the law of small numbers, for example.)","['group-theory', 'abstract-algebra', 'finite-groups']"
4362966,Generate random values using an empirical cumulative distribution function,"I have a set of data points that I have used to generate my empirical cumulative distribution function (CDF), which looks like this (to simplify things I have reduced the number of points for this question but it shouldn't matter): Given this data and plot, I need to somehow generate random values which follow this distribution. I admit that I am quite rusty when it comes to probability, but as far as I understand I need to first generate the probability density function (PDF), and then from there I can do what I want. Is that accurate? Or what is the best way to get what I want? If interested in giving an example for the answer, here are the X and Y array for the data points of the ECDF plot. I am using Python but I guess a language agnostic answer would also be really helpful x = [107.6697676209896, 430.70331251794784, 1975.0646306785532, 7793.524079329409, 27569.66699567533, 62566.73646946178, 222847.1449910263, 832591.8949493016, 2827054.7454871265, 10000733.572934577] y = [0, 0.04812202549318534, 0.09825964339269633, 0.14190143419466905, 0.27204351414405636, 0.46590411495145756, 0.6008552899988212, 0.6796719668120879, 0.8400864397710662, 1] Thanks a lot!","['statistics', 'python', 'cumulative-distribution-functions', 'probability-distributions', 'probability']"
4362971,How to prove T is a closable operator,"Let $T:H\to H$ a densely defined operator, with $H$ a Hilbert space such that: $$Re(x,Tx)\geq 0, \forall x\in Dom(T) $$ I want to prove that $T$ is a closable operator, that means... that there exists a closed extension $S:H\to H$ where, $Dom(T)\subset Dom(S)$ , alternatively $T$ is closed if its graph $\Gamma(T)$ is closed in the direct sum $H⊕H$ . First of all, If I prove that $T$ is bounded (equivalent to be continuous)I would finish because every linear continuous opearator is closable, neverless I don't know if $Re(x,Tx)\geq 0$ let me what I want. All I can imagine with that hypothesis is that we're working in a half-plane, and see what happens with the adjoint of $T%$ if it's self-adjoint i.e. if $(x,Tx)=(Tx,x) \forall x\in H$ , because of Hellinger-Toeplitz theorem tell me that every symmetric operator is bounded, but symmetric implies self-adjointness... Any idea to undertand what $Re(x,Tx)\geq 0$ is telling me would be appreciated.","['hilbert-spaces', 'closed-map', 'functional-analysis', 'unbounded-operators', 'adjoint-operators']"
4363008,"Which is the difference between the logical propositions $\exists a, b$ and $\exists a \exists b$?","I was wondering if there is any difference between $\exists a,b$ and $\exists a \exists b$ . I can't imagine there would be any but my homework uses them both.","['logic', 'discrete-mathematics']"
4363021,Finding the extrema of a functional (calculus of variations),"I am trying to understand the basics of calculus of variations. To that effect I am trying to solve a problem stated as follows: Find an extremal for $$I(y) = \int^1_0y'^2 - y^2 + 2xy dx$$ So I calculated: \begin{align*}
\frac{\partial F}{\partial y'} &= 2y' \implies  \frac{d}{dx}\bigg(\frac{\partial F}{\partial y'}\bigg) = 2y'' \\
	\frac{\partial F}{\partial y} &= -2y + 2x
\end{align*} Which gives the Euler equation $2y'' +2y - 2x = 0$ (I think, unless I messed up my math). Ok now I need to try to find solutions to this differential equation, however, I only know how to solve linear second order DE's. The $x$ term is throwing me off and I am not sure how to solve this. I know $y(x) = x$ is a solution by inspection, but inspection is a poor man's approach to solving DE's.","['optimization', 'calculus', 'ordinary-differential-equations', 'calculus-of-variations']"
4363065,Why is an elliptic curve with $j$-invariant in $K$ defined over $K$?,"I often see in the literature some arguments like this: ""to show that an elliptic curve $E$ is defined over $\mathbb{F}_p$ ,
we show that the $j$ -invariant of the curve is in $\mathbb{F}_p$ ."" But I thought in general there are elliptic curves not define over $\mathbb{F}_p$ , which still have $j$ -invariant $\mathbb{F}_p$ ? What do they mean by ""defined over"" when they make a such implication? $E$ is really defined over $\mathbb{F}_p$ , i.e., the weierstrass equation of $E$ has coefficients in $\mathbb{F}_p$ . $E$ is not defined over $\mathbb{F}_p$ , but it is isomorphic to a curve defined over $\mathbb{F}_p$ . Is it the case that "" $E$ is defined over $K$ if and only if $j\in K$ "" when $K$ is a finite field?
I'm sure this is not true when the characteristic of $K$ is $0$ , but I haven't thought about explicitly constructing an elliptic curve not defined over $\mathbb{F}_q$ but $j \in \mathbb{F}_q$ , $q =p^r$ .","['algebraic-geometry', 'elliptic-curves', 'projective-varieties']"
4363098,"Upper bound for a series $\sum_{n=0}^{\infty}2^n r^{2^n}$ for $r\in (0,1)$","It is clear that for $r\in (0,1)$ fixed, the series $$f(r)=\sum_{n=0}^{\infty}2^n r^{2^n}$$ is convergent, and also it is not to hard show that $f(r)$ is not uniformly convergent in $(0,1)$ . Since $\sum_{n=0}^{\infty}r^n=\frac{1}{1-r}$ , I think this sum function $f(r)$ may be similar. So can I find some upper bound such as $\frac{1}{1-r^\alpha}$ to dominate $f(r)$ ? Sincerely thanks!","['power-series', 'sequences-and-series', 'analysis', 'real-analysis']"
