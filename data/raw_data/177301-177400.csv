question_id,title,body,tags
3196621,How to prove $\lim_{n\to\infty}\sup A(t)\le\frac{a}{b}$?,"Suppose $A(t)>0(t\ge 0)$ , $a, b>0$ , let $$
A'(t)\le aA-bA^2.
$$ Prove $\lim_{n\to\infty}\sup A(t)\le\frac{a}{b}$ . Using Taylor formula $$
A(0)=A(t)-tA'(t)+o(t)\ge (1-ta)A(t) +tbA^2(t)+o(t).
$$ then $$
\frac{A(0)+o(t)-A(t)}{tA(t)}\ge -a+bA(t).
$$ therefore, I only need to prove $$
\lim_{t\to\infty}\sup \frac{A(0)+o(t)-A(t)}{tA(t)}=0.
$$ but I have no idea about the above formula. Could you please give me any hints? Thanks in advance!","['calculus', 'ordinary-differential-equations', 'inequality']"
3196634,How to remove this numerical artifact?,"I am trying to solve a differential equation: $$\frac{d f}{d\theta} = \frac{1}{c}(\text{max}(\sin\theta, 0) - f^4)~,$$ subject to periodic boundary condition, whic would imply $f(0)=f(2\pi)$ and $f'(0)= f'(2\pi)$ . To solve this numerically, I have set up an equation: $$f_i = f_{i-1}+\frac{1}{c}(\theta_i-\theta_{i-1})\left(\max(\sin\theta_i,0)-f_{i-1}^4\right)~.$$ Now, I want to solve this for specific grids. Suppose, I set up my grid points in $\theta = (0, 2\pi)$ to be $n$ equally spaced floats. Then I have small python program which would calculate $f$ for each grid points in $\theta$ . Here is the program: import numpy as np
import matplotlib.pyplot as plt
n=100
m = 500
a = np.linspace(0.01, 2*np.pi, n)
b = np.linspace(0.01, 2*np.pi, m)
arr = np.sin(a)
arr1 = np.sin(b)
index = np.where(arr<0)
index1 = np.where(arr1<0)
arr[index] = 0
arr1[index1] = 0
epsilon = 0.03
final_arr_l = np.ones(arr1.size)
final_arr_n = np.ones(arr.size)
for j in range(1,100*arr.size):
    i = j%arr.size
    step = final_arr_n[i-1]+ 1./epsilon*2*np.pi/n*(arr[i] - final_arr_n[i-1]**4)
    if (step>=0):
        final_arr_n[i] = step
    else:
        final_arr_n[i] = 0.2*final_arr_n[i-1]
for j in range(1,100*arr1.size):
    i = j%arr1.size
    final_arr_l[i] = final_arr_l[i-1]+1./epsilon*2*np.pi/m*(arr1[i] - final_arr_l[i-1]**4)

plt.plot(b, final_arr_l)
plt.plot(a, final_arr_n)
plt.grid(); plt.show() My major problem is for small $c$ , in the above case when $c=0.03$ , the numerical equation does not converge to a reasonable value (it is highly oscillatory) if I choose $N$ to be not so large. The main reason for that is since $\frac{1}{c}*(\theta_i-\theta_{i-1})>1$ , $f$ tends to be driven to negative infinity when $N$ is not so large, i.e. $\theta_i-\theta_{i-1}$ is not so small. Here is an example with $c=0.03$ showing the behaviour when $N=100$ versus $N=500$ . In my code, I have applied some adhoc criteria for small $N$ to avoid divergences: step = final_arr_n[i-1]+ 1./epsilon*2*np.pi/n*(max(np.sin(a[i]), 0) - final_arr_n[i-1]**4)
if (step>=0):
    final_arr_n[i] = step
else:
    final_arr_n[i] = 0.2*final_arr_n[i-1] what I would like to know: is there any good mathematical trick to solve this numerical equation with not so large $N$ and still make it converge for small $c$ ?","['convergence-divergence', 'ordinary-differential-equations', 'numerical-methods']"
3196706,degree extension over field of $p$-adic numbers,"Let $K = \mathbb{Q}(\theta)$ be a numberfield and $[K:\mathbb{Q}]=n$ . When $\mathbb{Q}_p$ is the field of $p$ -adic numbers and $K_p=\mathbb{Q}_p(\theta)$ , what about $[K_p : \mathbb{Q}_p]$ ?","['number-theory', 'p-adic-number-theory', 'algebraic-number-theory']"
3196738,Why does the rotating wave approximation work?,"Consider two coupled oscillators with position coordinates $X_a$ and $X_b$ . In general, the motion is described by a system of coupled first order linear differential equations: $$
\frac{d}{dt}
\begin{pmatrix} a \\ b \\ a^* \\ b^* \end{pmatrix}
= -i
\begin{bmatrix}
  \omega_a & g & 0 & -g \\
  g & \omega_b & -g & 0 \\
  0 & g & -\omega_a & -g \\
  g & 0 & -g & -\omega_b
\end{bmatrix}
\begin{pmatrix} a \\ b \\ a^* \\ b^* \end{pmatrix}
$$ where $a \equiv X_a + i \dot X_a$ and similarly for $b$ .
It is common in the analysis of this problem to drop the anti-diagonal terms in the matrix, thus decoupling the upper left and lower right blocks.
Dropping the anti-diagonal terms is called the ""rotating wave approximation"" (RWA) and is supposedly good when $g \ll \omega_a, \omega_b$ . Why is the RWA justified? Physically, dropping the anti-diagonal means that the variables $a$ and $b$ which would be purely clockwise-rotating in the uncoupled $(g=0)$ case couple only to each other and not to the counterclockwise-rotating terms $a^*$ and $b^*$ .
Perhaps more interestingly I noticed that the matrix can be written in an algebraic form $^{[a]}$ $$
-i \sigma_z \otimes
\left(
  g \sigma_x + \frac{\Delta}{2} \sigma_z
  + \frac{S}{2} \mathbb{I}
\right)
- i g (\sigma_y \otimes \sigma_x)
$$ where $\Delta \equiv (\omega_a - \omega_b) / 2$ and $S \equiv (\omega_a + \omega_b) / 2$ .
In this form, the RWA corresponds precisely to dropping the $-ig(\sigma_y \otimes \sigma_x)$ term.
I could imagine that this algebraic representation might help explain why the RWA works. Another final observation is that the eigenvalues of the matrix change only sightly when using the RWA in the limit of $g \ll \omega_a, \omega_b$ , which seems to suggest that the RWA is indeed a good approximation.
But still, why does it work? $[a]$ The $\sigma$ 's refer to the Pauli matrices . This question is more or less a rewrite of question 467342 from the Physics site. Here I'm trying to draw more specific attention to the question of why the RWA works at all, and to get insight from the mathematics community.","['ordinary-differential-equations', 'dynamical-systems']"
3196785,Are there infinitely many solutions such that the digit sum of a prime power is a smaller power of the same prime?,"While discussing prime powers and divisors,  I came up with the following problem. Examples $\to$ prime $p=3$ digit sum (in base ten) of $p^3=27$ is $p^2=9$ , a power of $p$ ,. $\to$ prime $p=7$ digit sum of $p^4=2401$ is $p^1=7$ , a power of $p$ . We can get even crazier: $\to$ prime $p=5$ digit sum of $p^{208}$ is equal to $p^4=625$ . and so on . The PARI/GP code used to generate the examples is below: sfun(p,k,n)={for(q=2,n,if(sumdigits(p^q,10)==p^k,print(q)))} . Questions Main question: Denote the digit sum (in base ten) of an integer $m$ as $s(m)$ . Are there infinitely many solutions such that $$s(p^n)=p^k$$ where $p$ is a prime, and $k,n$ are positive integers? When $p=3$ , it has the nice property that $s(3^n)$ is always divisible by $3$ , so the first few solutions are not very hard to find. For example, when $k=3$ , we obtain $n=9,10,11,13,16,17,21$ . Some experimentation with the code reveals that the solutions get sparser as $p$ increases, as expected. But the solutions themselves are very unexpected, such as $$s(5^{4938})=5^6,\quad s(89^{898})=89^2.$$ Thus I believe in the finitude of the solutions, but I think evaluating them will be out of the question. To disprove my claim, it suffices to show that for every $p$ there is a solution, since we know that the number of primes is infinite.","['number-theory', 'conjectures', 'prime-numbers', 'decimal-expansion']"
3196848,Infinity norm is actually a norm : triangle inequality,"I have to prove the following assertion : 
Let $V$ be a finit dimentional vector space with dimension $n$ over the field $K$ which is the field of real numbers or complex numbers. Let the map defined by: $\mid \mid . \mid \mid _{\infty} \quad : v \mapsto \mid \mid v \mid \mid_{\infty}=\max\{\mid v_1\mid,...,\mid v_n \mid \} \quad \forall v \in V$ where $v_k \quad k=1,...,n $ are the componant of the vector $v$ . Show that this map is a norm. I proved the posivity of this norm. I also proved that : $\mid \mid \lambda v \mid \mid _{\infty}=\mid \lambda \mid \mid \mid v \mid \mid \quad \forall v \in V, \forall \lambda \in K$ Now, I have to prove the triangle inequality. I have already done something but i am not sure what i have done is correct : 
I have to prove that : $$\forall v, w \in V \quad \mid \mid v+w \mid \mid _{\infty}\leq \mid \mid v \mid \mid_{\infty} + \mid \mid w \mid \mid _{\infty}$$ I know that in the real or complex numbers, we have this inequality for all $x$ and $y$ : $\mid x+y\mid \leq \mid x \mid +\mid y\mid $ Therefore : $v=(v_1,...,v_n)$ , $w=(w_1,...,w_n)$ : $$\forall k \in {1,...,n} \quad \mid v_k + w_k \mid \leq \mid v_k \mid +\mid w_k \mid $$ Then, $$\max\{\mid v_1 +w_1\mid,... \mid v_n +w_n\mid \} \leq \max\{\mid v_1 \mid +\mid w_1 \mid ,...,\mid v_n\mid+\mid w_n \mid \}$$ Now the problem is : can i say that $$\max\{\mid v_1 \mid +\mid w_1 \mid ,...,\mid v_n\mid+\mid w_n \mid \} \leq \max\{\mid v_1\mid ,...\mid v_n\mid \}+\max\{\mid w_1\mid ,...\mid w_n\mid \}$$ which would prove the inequality ? Is it trivial ? If it is not, i don't know how to prove the last inequality...","['inequality', 'normed-spaces', 'analysis']"
3196854,Triple sum $\sum\limits_{a=1}^{\infty} \sum\limits_{b=1}^{\infty} \sum\limits_{c=1}^{\infty} \frac{\cos a \cos b \cos c}{a^2 + b^2 + c^2}$,"We have poor water heating system in our countryside house (currently it takes 4 hours to warm up the water), and my father has decided to improve it; he bought a water tank and placed it up in the attic, and the last stage is to buy a correct heating element. So he gave me a call and asked to calculate which heating element to buy (more precisely, he asked what electric power does the heating element need to make the descending water hot in 10 minutes; hot was defined as 40 Celsius). The water tank is huge and the heater is placed at one of its dead ends; it wouldn't be correct to say the temperature is uniform and does not vary from one point to another. The only person for whom I can abandon my work is my father, so I took a standard heat equation : $$\frac{\partial T}{\partial t} = a^2 \left( \frac{\partial^2 T}{\partial x^2} + \frac{\partial^2 T}{\partial y^2} + \frac{\partial^2 T}{\partial z^2} \right) + f (x,y,z,t)$$ with initial and boundary conditions $$T(x,y,z,0) = T_0, \hskip 20 pt T(x,y,z,t)\Big|_{\text{boundary}} = T_0$$ and with $$f(x,y,z,t) = \frac{P}{c\rho} \delta(x - x_0) \delta (y - y_0) \delta (z - z_0)$$ for this case (point-like heater located at $(x_0, y_0, z_0)$ , $P$ is heater electric power, $c$ is water specific heat capacity, $\rho$ is water density), $a = \sqrt{\kappa/c\rho}$ is water thermal diffusivity, $\kappa$ is water thermal conductivity. After a tough computation I got the solution: $$T (x,y,z,t) = T_0 +  \frac{P}{lwh \kappa} \sum\limits_{n=1}^{\infty} \sum\limits_{u=1}^{\infty} \sum\limits_{v=1}^{\infty}  \frac{\sin \left(\frac{\pi n x}{l} \right) \sin \left(\frac{\pi u y}{w} \right) \sin \left(\frac{\pi v z}{h} \right) \sin \left(\frac{\pi n x_0}{l} \right) \sin \left(\frac{\pi u y_0}{w} \right) \sin \left(\frac{\pi v z_0}{h} \right)}{\left( \frac{\pi n}{l} \right)^2  + \left( \frac{\pi u}{w} \right)^2  + \left( \frac{\pi v}{h} \right)^2  } \cdot $$ $$\cdot \left(1 - \exp\left( - \left( \frac{\pi n a}{l} \right)^2 t - \left( \frac{\pi u a}{w} \right)^2 t - \left( \frac{\pi v a}{h} \right)^2 t \right) \right)$$ So guys, if you will ever install a water tank in your attic, you know where to look. Here $l$ , $w$ , $h$ are length, width and height of the tank. My father's question was ""which $P$ do I need for achieving $T_{\text{hot}} = 40$ Celsius from $T_0 = 20$ Celsius in $t = 10$ minutes in a point $(x,y,z) = (x_1, y_1, z_1)$ ?"" To resolve it, to find the $P$ , I need to numerically compute this triple sum somehow. Here go my thoughts on the subject: the triple sum $\sum\limits_{n=1}^{\infty} \sum\limits_{u=1}^{\infty} \sum\limits_{v=1}^{\infty} \frac{1}{n^2 + u^2 + v^2}$ does not converge; you need to have a power of at least $(3+\varepsilon)$ in denominator trigonometric functions in the numerator really matter: $\sum\limits_{n=1}^{\infty} \frac{1}{n}$ does not converge but $\sum\limits_{n=1}^{\infty} \frac{\cos n}{n}$ does one can utilise the formula for sine product ( $\sin \left(\frac{\pi n x_1}{l}\right) \sin \left(\frac{\pi n x_0}{l}\right) = \dots$ ) and rewrite the numerator into 8 terms of three cosines thus, what I'm interested in is the sum $\sum\limits_{n=1}^{\infty} \sum\limits_{u=1}^{\infty} \sum\limits_{v=1}^{\infty} \frac{\cos n \cos u \cos v}{n^2 + u^2 + v^2}$ clearly, for numeric computation (e.g. in Mathematica) it would be wonderful to find the precise $N$ , $U$ , $V$ such that $\sum\limits_{n=1}^{\infty} \sum\limits_{u=1}^{\infty} \sum\limits_{v=1}^{\infty} (\dots) \approx \sum\limits_{n=1}^{N} \sum\limits_{u=1}^{U} \sum\limits_{v=1}^{V} (\dots)$ There's a decent identity $\int\limits_0^{\infty} \frac{sin at}{a} e^{-bt} dt = \frac{1}{a^2 + b^2}$ , which is proved by integration by parts twice. It allows you to split $\frac{1}{a^2 + b^2}$ into factors, into multipliers. Given this split, computing $\sum\limits_{a=1}^{\infty} \sum\limits_{b=1}^{\infty} \frac{1}{a^2 + b^2}$ is relatively easy (well, one can calculate both sums and then prove the resulting integral does not converge). However, I failed to split $\frac{1}{a^2 + b^2 + c^2}$ in this manner. With Fourier transform (see wiki , end of the page, row 502) $$\frac{1}{a^2 + b^2 + c^2} = \iiint\limits_{\mathbb{R}^3} \frac{e^{-2\pi i (a \xi_a + b\xi_a + c\xi_c)}}{\xi_a^2 + \xi_b^2 + \xi_c^2} d\xi_a d\xi_b d\xi_c $$ the triple sum $\sum\limits_{a=1}^{\infty} \sum\limits_{b=1}^{\infty} \sum\limits_{c=1}^{\infty} \frac{\cos a \cos b \cos c}{a^2 + b^2 + c^2}$ could be reduced to ordinary sums such as $\sum\limits_{a=1}^{\infty} \cos a \cdot e^{-2\pi i a \xi_a}$ . However, the latter sum does not converge.","['summation', 'fourier-transform', 'sequences-and-series', 'fourier-series', 'convergence-divergence']"
3196923,Can we cancel the equality mark here?,"Problem Let $f(x)$ satisfy that $f(1)=1$ and $f'(x)=\dfrac{1}{x^2+f^2(x)}$ . Prove that $\lim\limits_{x \to +\infty}f(x)$ exists and is less than $1+\dfrac{\pi}{4}.$ Proof Since $f'(x)=\dfrac{1}{x^2+f'(x)}>0$ , $f(x)$ is strictly increasing. Thus, $f(x)>f(1)=1$ holds for all $x>1$ , and $\lim\limits_{x \to +\infty}f(x)$ equals either the positive infinity or some finite value. Notice that, $\forall x>1:$ \begin{align*}
 f(x)-f(1)&=\int_1^x f'(t){\rm d}t=\int_1^x \frac{1}{t^2+f^2(t)}{\rm d}t<\int_1^x\frac{1}{t^2+1}{\rm d}t=\arctan x-\frac{\pi}{4}.
\end{align*} Therefore $$f(x)<\arctan x-\frac{\pi}{4}+1<\frac{\pi}{2}-\frac{\pi}{4}+1=1+\frac{\pi}{4},$$ which implies that $f(x)$ is bounded upward. Thus, $\lim\limits_{x \to +\infty}f(x)$ exists. Take the limits as $x \to +\infty$ , we have $\lim\limits_{x \to +\infty}f(x)\leq 1+\dfrac{\pi}{4}.$ Can we cancel the equality mark here? In another word, can we obtain $\lim\limits_{x \to +\infty}f(x)<1+\dfrac{\pi}{4}$ ?","['limits', 'calculus', 'ordinary-differential-equations']"
3196942,Simple process in Itô calculus,"For the definition of Itô integral, one uses simple stochastic processes. I have found two definitions for simple stochastic process, given a filtration $(\mathcal{F}_t)_{t\geq0}$ , an interval $[0,T]$ and a sample space $\Omega$ : $u_t=\sum_{i=1}^p \phi_i 1_{(t_{i-1},t_i]}(t)$ , $u_t=\sum_{i=1}^p \phi_i 1_{[t_{i-1},t_i)}(t)$ , where $0=t_0<t_1<\ldots<t_p=T$ is a partition, and $\phi_i$ is an $\mathcal{F}_{t_{i-1}}$ -measurable random variable such that $E[\phi_i^2]<\infty$ , for $i=0,\ldots,p$ . The first definition corresponds to the book Introduction to Stochastic Calculus Applied to Finance , by Lamberton and Lapeyre. The second definition corresponds to my lecture notes. In both cases, given a general adapted stochastic process $u$ in $L^2(\Omega\times[0,T])$ , it is approximated by simple processes $(u^n)_{n=1}^\infty$ in $L^2(\Omega\times[0,T])$ and the Itô integral is defined as a limit in $L^2(\Omega)$ of $\int_0^T u_t^n \,dB_t$ . My question is whether the two definitions are equivalent to define the Itô integral.","['stochastic-integrals', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3196952,Is the projection $p: X\times_Z Y\to Y$ also an immersion?,"Let $f:X\to Z$ be an immersion of schemes and $g: Y\to Z$ is any morphism of schemes, is the projection $p: X\times_Z Y\to Y$ also an immersion?",['algebraic-geometry']
3196965,$\lim_{\epsilon\to0}\frac{\cos(\epsilon-n\frac{\pi}{2})}{\epsilon^n}$,"We were doing generalized integrals in class and this integral came out. I tried using integration by parts and got something repeating. We're gonna let $\epsilon \rightarrow 0$ and $x\rightarrow\infty$ $$\int_0^{\infty}\frac{-\cos t}{t}dt=\Big[\frac{-\sin t}{t}\Big]_{\epsilon}^{x}+\Big[\frac{\cos t}{t^2}\Big]_{\epsilon}^{x}+\Big[\frac{2\sin t}{t^3}\Big]_{\epsilon}^{x}+\Big[\frac{-6\cos t}{t^4}\Big]_{\epsilon}^{x}+\int_{\epsilon}^x\frac{-24\cos t}{t^5}dt$$ $$=\sum_{n=1}^{\infty}\Big[\frac{(-1)^{n+1}(n-1)!\cos{(t-n\frac{\pi}{2})}}{t^n}\Big]^x_{\epsilon}$$ $$=\sum_{n=1}^{\infty}(-1)^{n+1}(n-1)!\Big(\lim_{x\to\infty}\frac{\cos(x-n\frac{\pi}{2})}{x^n}-\lim_{\epsilon\to0}\frac{\cos(\epsilon-n\frac{\pi}{2})}{\epsilon^n}\Big)$$ $$=\sum_{n=1}^{\infty}(-1)^{n}(n-1)!\lim_{\epsilon\to0}\frac{\cos(\epsilon-n\frac{\pi}{2})}{\epsilon^n}$$ The first limit is $0$ by squeeze theorem but what about the second one, I don't know how to start. Please share your work, thank you for your time!","['limits', 'calculus', 'sequences-and-series']"
3196991,Finding the determinant of a tridiagonal matrix,$$\begin{vmatrix}x&1&0&0&⋯\\-n&x-2&2&0&⋯\\0&-(n-1)&x-4&3&⋯\\⋮&⋱&⋱&⋱&⋮\\0&⋯&-2&x-2(n-1)&n\\0&0&⋯&-1&x-2n\end{vmatrix}_{(n+1)×(n+1)}$$ Find the value of the above determinant. This problem comes from an advanced algebra book. I want to solve it with elementary transformation knowledge. I have been trying to solve it for a long time.,"['determinant', 'linear-algebra', 'tridiagonal-matrices']"
3197026,Distribution of prime numbers modulo $4$,"Are primes equally likely to be equivalent to $1$ or $3$ modulo $4,$ or is there a skew in one direction? That is my specific question, but I would be interested to know if there exists a trend more generally, say for modulo any even.","['number-theory', 'prime-numbers']"
3197028,"Prove that when writing up all even numbers in a column, then chaining $2n+1$ from them, we get all natural numbers exactly once.","Here is the idea: Write up all even numbers in the first column, then get numbers in the second column by taking the number to the left ( $n$ ), and calculating $2n+1$ . And keep repeating this. Here is how it's done: $$\begin{matrix}
0 & 1 & 3 & 7 & 15 & 31 & 63 & \dots \\
2 & 5 & 11 & 23 & 47 & 95 & 191 & \dots \\
4 & 9 & 19 & 39 & 79 & 159 & 319 & \dots \\
6 & 13 & 27 & 55 & 111 & 223 & 447 & \dots \\
8 & 17 & 35 & 71 & 143 & 287 & 575 & \dots \\
10 & 21 & 43 & 87 & 175 & 351 & 703 & \dots \\
12 & 25 & 51 & 103 & 207 & 415 & 831 & \dots \\
14 & 29 & 59 & 119 & 239 & 479 & 959 & \dots \\
16 & 33 & 67 & 135 & 271 & 543 & 1087 & \dots \\
18 & 37 & 75 & 151 & 303 & 607 & 1215 & \dots \\
20 & 41 & 83 & 167 & 335 & 671 & 1343 & \dots \\
22 & 45 & 91 & 183 & 367 & 735 & 1471 & \dots \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots
\end{matrix}$$ And this is what it looks like to only write up the first $50$ numbers gotten this way: $$\begin{matrix}
0 & 1 & 3 & 7 & 15 & 31 & \dots\\
2 & 5 & 11 & 23 & 47 & \dots & \\
4 & 9 & 19 & 39 & \dots & & & \\
6 & 13 & 27 & \dots & & & & \\
8 & 17 & 35 & \dots & & & & \\
10 & 21 & 43 & \dots & & & & \\
12 & 25 & \dots & & & & & \\
14 & 29 & \dots & & & & & \\
16 & 33 & \dots & & & & & \\
18 & 37 & \dots & & & & & \\
20 & 41 & \dots & & & & & \\
22 & 45 & \dots & & & & & \\
24 & 49 & \dots & & & & & \\
26 & \dots & & & & & & \\
28 & \dots & & & & & & \\
30 & \dots & & & & & & \\
32 & \dots & & & & & & \\
34 & \dots & & & & & & \\
36 & \dots & & & & & & \\
38 & \dots & & & & & & \\
40 & \dots & & & & & & \\
42 & \dots & & & & & & \\
44 & \dots & & & & & & \\
46 & \dots & & & & & & \\
48 & \dots & & & & & & \\
50 & \dots & & & & & & \\
\end{matrix}$$ Question: Do we get all natrual numbers exactly once when writing up all rows this way? Bonus question: When writing up numbers in order, what curve do we approximate when connecting the end of each row? (What curve do we get closer and closer to by adding more numbers in order to our list?) $\quad\quad\quad\quad\quad\quad\quad\quad$ My guess is that the red line approximates $\ln(x), x \in [0,1]$ .","['infinite-matrices', 'algebra-precalculus', 'natural-numbers', 'sequences-and-series']"
3197039,Can we always perturb a map to have distinct singular values?,"Let $\mathbb{D}^n$ be the closed $n$ -dimensional unit ball, and let $f:\mathbb{D}^n \to \mathbb{R}^n$ be smooth. Question: Do there there exist $f_n \in C^{\infty}(\mathbb{D}^n, \mathbb{R}^n)$ such that $f_n \to f$ in $W^{1,2}(\mathbb{D}^n, \mathbb{R}^n)$ with the property that for every $p \in \mathbb{D}^n $ , the singular values of $(df_n)_p$ are all distinct? I am fine with the $f_n$ being $C^1$ , if that matters.","['svd', 'real-analysis', 'multivariable-calculus', 'sobolev-spaces', 'perturbation-theory']"
3197068,What is the derivative of the real part of a complex variable?,"If I have the complex variable $z=x+iy$ and the function $f(z)=z$ , is it possible to calculate $\frac{d\Re{f(z)}}{dz}$ , or in this particular case $\frac{dx}{dz}$ ? It should be equal to $\frac{1}{2}$ , but I don't know why.","['complex-analysis', 'derivatives', 'complex-numbers']"
3197078,A new stopping time built from a stopping time,"Let $T$ be a stopping time for the filtration $(\mathcal{F_n})_{n \in \mathbb{N}}.$ For all $n \in \mathbb{N}  \cup \left\{+\infty \right\},$ we set $\phi(n)=\inf\left\{k \in \mathbb{N};\left\{T=n \right\} \in \mathcal{F_k}\right\}$ . Show that $\phi(T)$ is a stopping time and that $\phi(T) \leq T.$ I wrote $\left\{w;\phi(T(w))=n \right\}=\left\{w;T^{-1} (\left\{T(w) \right\}) \in \mathcal{F_n} \ and \ T^{-1}(\left\{T(w) \right\}) \notin \bigcup_{k=0}^{n-1}{\mathcal{F}_k} \right\} $ by I don't see how this will help us to prove that $\left\{w;\phi(T(w))=n \right\} \in \mathcal{F}_n$ .","['martingales', 'stopping-times', 'probability-theory']"
3197089,Simplify an equation where a fox chases a rabbit,"I'm stuck on the following question and don't know how the book got its answer because it has multiple variables (its precalc so I don't know how to do calculus yet) and I don't know how to eliminate one completely.  Any help is much appreciated. Question: Suppose Fritzy the Fox, positioned at a point $(x,y)$ in the first quadrant, spots Chewbacca the Bunny at $(0,0)$ .
Chewbacca begins to run along a fence (the positive $y$ -axis) towards his warren.
Fritzy, of course, takes chase and constantly adjusts his direction so that he is always running directly at Chewbacca.  If Chewbacca's speed is $v_1$ and Fritzy's speed is $v_2$ , the path Fritzy will take to intercept Chewbacca, provided $v_2$ is directly proportional to, but not equal to $v_1$ is modelled by: $$
y
= \frac{1}{2}\left(\frac{x^{1+v_1/v_2}}{1+v_1/v_2}-\frac{x^{1-v_1/v_2}}{1-v_1/v_2}\right) + \frac{v_1v_2}{v_2^2-v_1^2}
$$ a.) Determine the path that Fritzy will take if he runs exactly twice as fast as Chewbacca; that is $v_2$ = $2v_1$ .  Use your calculator to graph this path for $x \ge 0$ .  What is the significance of the y-intercept of the graph? So in order to graph this with the provisions of a (where $v_2$ = $2v_1$ ) I believe I need to reduce this function down to one variable.  So here is my attempt: $$
y
= \frac{1}{2}\left(\frac{x^{3v_1/2v_1}}{3v_1/2v_1}-\frac{x^{v_1/2v_1}}{v_1/2v_1}\right) + \frac{3v_1}{v_1^2}
$$ $$
y
= \frac{1}{2}\left(\frac{x^{3v_1/2v_1} - 3x^{v_1/2v_1}}{3v_1/2v_1}\right) + \frac{3v_1}{v_1^2}
$$ From here I believe I can get rid of the $v_1$ in the first term because it is present in both numerators and denominators.  So here goes: $$
y
= \frac{1}{2}\left(\frac{x^{3/2} - 3x^{1/2}}{3/2}\right) + \frac{3v_1}{v_1^2}.
$$ But I don't know how you get rid of the $v_1$ in the second term because it is squared in the denominator. I maybe even be on the completely wrong path as the book answer is: $$
y
= \frac{1}{3} x^{3/2} - \sqrt{x} + \frac{2}{3}.
$$ Can anyone help me understand what I have to do to get the book answer?",['algebra-precalculus']
3197125,"Are 2, 3 the only prime numbers that don't have the digit 1 and are palindromes whose squares are also palindromes?","While thinking about prime numbers, I noticed that: $(1)$ Very few prime numbers have squares that are palindromes. Ex: $2$ , $3$ , $11$ , $101$ , $307$ $(2)$ Even rarer are prime numbers that are palindromes whose square are palindromes. Ex. $2$ , $3$ , $11$ This inspired me to ask the following questions: $(1)$ Are $2, 3$ the only prime numbers that don't have the digit $1$ and are palindromes whose squares are also palindromes? $(2)$ If not, then are there a finite number of these types of these prime numbers.","['number-theory', 'conjectures', 'palindrome', 'prime-numbers']"
3197140,Implicit derivative: why do we keep the $\frac{dy}{dx}$?,I just started learning calculus and I'm studying implicit derivatives and I have a question regarding the differenciation of the y variable. I'll use an example: Applying implicit derivative to $5y^2 = x^2$ $ \frac{d}{dx} (5y^2) = \frac{d}{dx} (x^2) $ $10y \frac{dy}{dx} = 2x$ Why do we keep the $ \frac{dy}{dx} $ after differentiating $5y^2$ ? The book I'm following does not explain the reason for this. Thanks in advance.,"['calculus', 'derivatives']"
3197165,How is stability for a numerical solution generalized to a system of ODEs?,"Consider the system of ODEs $$y' = \begin{bmatrix}-6&4\\4&-6\end{bmatrix}y, \quad t\in[t_0, t_e], \quad y(t_0)=y_0$$ I'm asked for what stepsize the explicit Euler method generates a stable solution. If it would be one ODE I would just find for what stepsize $h$ the stability function $R(z)$ in $$y_{n+1} = R(\lambda h) y_{n}$$ would be less than $1$ , but I get confused when it's a system of ODEs. Do I write $$y_{n+1} = \left(1 + h \begin{bmatrix}-6&4\\4&-6\end{bmatrix}\right) y_n$$ and use the eigenvalues somehow?","['numerical-methods', 'systems-of-equations', 'ordinary-differential-equations']"
3197261,"Necessary and sufficient conditions that $\langle \zeta, (ij), \lvert\lvert k\, \ell \rvert\rvert, \xi_M\rangle$ generates $\mathscr{P}_n.$","Throughout I use cycle notation and write maps $m:X\to Y$ on the right of their arguments (e.g. $xm=y$ for $m(x)=y$ ). Let $\zeta=(12\dots n)$ . This question is inspired by the following questions: Necessary and Sufficient conditions to generate $S_n$ Showing the full transformation semigroup $\mathscr{T}_n=\langle\zeta, \tau, \pi\rangle$. Showing $\mathscr{P}_n=\langle\zeta, \tau, \pi, \xi\rangle$. Definition 1: The full transformation semigroup $\mathscr{T}_n$ on the set $N=\{1, \dots,n\}$ is given by all maps $\alpha:N\to N$ , together with composition of transformations. Definition 2: For $i\ne j$ in $N$ , let $\lvert\lvert i\, j \rvert\rvert$ denote the map $\phi\in\mathscr{T}_n$ for which $$i\phi =j,\quad x\phi=x\quad (x\ne i).$$ Definition 3: The partial transformation semigroup $\mathscr{P}_n$ is the set of all partial maps from $\{1, 2, \dots , n\}$ to itself, together with composition of partial transformations. Definition 4: For each subset $M$ of $N=\{1, 2, \dots , n\}$ , define $\xi_{M}$ by the identity $1_{N\setminus M}$ . Let me interject by saying I'm not sure if this question makes much sense, so if that's the case, I'm sorry. Question: Are there necessary and sufficient conditions on $1\le i\le j\le n$ , $k, \ell\in N$ , and $M\subseteq N$ such that $$\langle \zeta, (ij), \lvert\lvert k\, \ell \rvert\rvert, \xi_M\rangle$$ generates $\mathscr{P}_n$ ? Thoughts: I don't have much to offer besides pointing to the lemmas of the questions that inspired this question. This is not something I think I could solve myself. The question arose out of curiosity. I did a module in semigroup theory back in, oh, 2014/2015, and I read about a third of Howie's ""Fundamentals of Semigroup Theory"" (before starting a PhD in combinatorial group theory). I also did a module in formal languages & automata circa 2012/2013; plenty of monoids were studied then; and my Master's dissertation was on inverse semigroups. Please help :)","['permutations', 'monoid', 'finitely-generated', 'functions', 'semigroups']"
3197291,Optimising unrounding using maximum likelihood,"I have a bunch of rounded random independent numbers. I want to replace them with unrounded numbers such that the unrounded numbers are 'most likely' to have been generated by some (continuous) distribution, eg, a lognormal distribution. What should be my objective function?","['statistics', 'maximum-likelihood']"
3197336,How to show that two trigonometric polynomials of degree $n$ combined have at most $2n$ zeros?,"I am already aware of this question: Prove the following trigonometric polynomial has $2n$ zeros But it's not the same. Let be $P(x) = \sum_{k=0}^{n} a_k \cos (kx)$ and $\tilde{P}(x) = \sum_{k=0}^n a_k \cos((n - k)x)$ . If we denote $Z(Q)$ the number of zeros of $Q$ (we count without multiplicities, so: $Z(X^2) = 1$ ), then, how can we show that, that over $[0, 2\pi[$ : \begin{equation*}
Z(P) + Z(\tilde{P}) \leq 2n
\end{equation*} I know that I can show that: $\sum_{k=0}^n a_k \cos(kx)$ have at most $2n$ zeros, using an imaginary exponential form and the fact that $x \mapsto e^{ix}$ is a bijection from $[0, 2\pi[$ to $\mathbb{U}$ the set of complex of module $1$ . But I don't know how to relate those two polynomials to an exponential imaginary form, I tried to expand $\cos((n - k)x)$ to create $\sin, \cos$ expressions, but without luck.","['trigonometry', 'roots', 'polynomials', 'real-analysis']"
3197390,Showing a mapping is bijective if and only if a matrix is invertible,"Let $\mathbf{A}$ be an $n\times n$ matrix and let $\mathbf{c}$ and $x_{\star}$ be point in $\mathbb{R}^{n}$ . Define the affine mapping $\mathbf{G} : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ by $$\mathbf{G(x)} = \mathbf{c + A(x - x_{\star})} $$ for $\mathbf{x}$ in $\mathbb{R}^{n}$ . Show that the mapping $\mathbf{G} : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ is one-to-one
  and onto if and only if $\mathbf{A}$ is invertible. I am not too sure about how to approach this problem. I've also got the following theorem that I think might help: Let $\mathcal{O}$ be an open subset of $\mathbb{R}^{n}$ and suppose $\mathbf{F} : \mathcal{O} \rightarrow \mathbb{R}^{n}$ is continuously differentiable. Let $x_{\star}$ be a point in $\mathcal{O}$ at which the derivative matrix $\mathbf{DF(x_{\star})}$ is invertible. Then there is a neighborhood $U$ of $x_{\star}$ and a neighborhood $V$ of its image $\mathbf{F(x_{\star})}$ such that $\mathbf{F} : U \rightarrow V$ is one-to-one and onto. I've tried taking the derivative of both sides of the equation, etc, but didn't get anywhere. Any help is appreciated.","['matrices', 'multivariable-calculus', 'functions', 'linear-algebra', 'derivatives']"
3197399,Derivatives without limits,"Update: H/t David K for pointing out that my assumption that I can force $a^2+b^2=r^2$ is wrong. This led to analyzing a cubic equation which is now moot, but I think the bulk of the question remains sensible. I think I'm making some mistakes in the below, but I'm not sure where. Even if there are mistakes, I'm thinking there might not be any practical point to it, but I'm curious. I think it's possible to find the slope of a tangent line without using limits in some cases. Further it's possible to generalize the procedure to many common curves. I am having trouble with a proof of this and I'm not sure where I am going wrong. Proof we can find the slope of a tangent line to a parabola without taking a limit: First we consider a circle. If we know the center of a circle and we know a point on that circle, we know the equation of a line connecting the center to that point. The slope of the line perpendicular to this line is the slope of the tangent line to the circle at that point. So we have already proven the thesis in the case of a circle. A circle is tangent to some other curve if it only intersects that curve at a single point. A circle can be constructed to be tangent to certain points on certain curves. Tangent circles share tangent lines with the curves to which they are tangent. So the procedure above can find tangents to those curves by constructing the correct tangent circle. Parabola case:
Consider $y=x^2$ . We want a circle such that $(x-a)^2+(x^2-b)^2=r^2$ So: $$x^4+(1-2b)x^2-2ax+(a^2+b^2-r^2)=0$$ We either have 2 complex roots or no real solutions. Assuming we have any real solutions, having a unique one requires that we have a repeated root. So to find the tangent to the curve at $x_0$ we want to divide the quartic by $(x-x_0)^2$ . We'd want to have the remainder terms be zero and the discriminant of the resulting quotient to be negative. EDIT: Turns out Descartes developed this method centuries ago bearing some fruit. It's algebraically more intense than Newton's and Leibniz's approach to finding slopes of tangent lines, so fell by the wayside. It's called Descartes' Method of Normals","['calculus', 'derivatives']"
3197429,Why aren't these two solutions equivalent? Combinatorics problem,"I was given the following fact: there is a set $S$ of $11$ people, among which there are $4$ professors and $7$ students, $S=\{p_1, p_2, p_3,p_4, s_1, s_2,...,s_7\}$ We are requested to form from it a group of $5$ people, and we must have at least 3 professors. I find that the two answers I will expose should be equivalent, but are not, and I can't figure out why. Answer 1 The group of $5$ people must have at least $3$ professors. This means that three of the $5$ people will necessarily be a subset of $S_p$ , the subset of $S$ containing only the professors. There are $\binom{4}{3}$ subsets of $S_p$ , and therefore I have $\binom{4}{3}$ alternatives for the three professors that must be in the group. Now that I've made sure this $3$ professors are in the group, I have $11-3=8$ people left to choose from. The remaining two persons of the group can either be professors or students, so I can pick any of those $8$ . So for the two remaining places I have $\binom{8}{2}$ alternatives. At last, I have $\binom{4}{3} \binom{8}{2} = 112$ ways of forming a group of $5$ people in which there will definitely be at least $3$ professors. Answer 2 There are $4$ professors and, in my group of $5$ people, I must have at least $3$ of them. So I'll either have $3$ or $4$ professors. If I have $3$ professors, I'll choose them from the $4$ professors, and fill the remaining two places with $2$ of $7$ students. This is $\binom{4}{3} \binom{7}{2}$ . If on the other hand I have $4$ professors, I'll have $\binom{4}{4}$ alternatives for choosing them, and $\binom{7}{1}$ ways of choosing a student for the remaining last place. So at last there are $\binom{4}{3}\binom{7}{2}+\binom{4}{4}\binom{7}{1} = 91$ ways of making the group. Doubt As you can see, the answers are different. Answer $1$ says there are $112$ ways of making the group; answer two says $91$ . However, both reasonings seem okay to me and I can't see why should they differ nor where. Perhaps someone can clear this up for me.","['combinatorics', 'problem-solving', 'discrete-mathematics']"
3197565,Sum of two random variables uniformly distributed on circles,"Suppose we have two independent random variables $U_1$ and $U_2$ unfiorm  on \begin{align}
S_i = \left\{ (s_1,s_2) \in \mathbb{R}: \sqrt{s_1^2+s_2^2} =r_i \right\} 
\end{align} respectily. Assume $r_1 \ge r_2$ . Question: How to find the pdf of $U_1+U_2$ ? We know that it would be distributed on \begin{align}
S_3 = \left\{ (s_1,s_2) \in \mathbb{R}: r_1-r_2 \le  \sqrt{s_1^2+s_2^2} \le r_1+r_2 \right\} 
\end{align} In other words, show that the sum of two random variables on the circles results in a random variable distributed on an annulus. The question now is how to find the pdf of $U_1+U_2$ ? Can this, for example, be done by using characteristic functions?","['probability-theory', 'probability']"
3197636,Rigid motions taking a regular $n$-gon back to itself carry vertices to vertices,"I have been reading Keith Conrad's expository paper Dihedral groups I and I have two questions about Theorem $2.2$ , which deals with the size of $D_n$ . In the first part of the proof you can read An element $g$ of $D_n$ is a rigid motion taking the $n$ -gon back to itself, and it must carry vertices to vertices (how are vertices unlike other points in terms of their distance relationships with all points on the polygon?) and $g$ must preserve adjacency of vertices. I think I understand why $g$ must preserve adjacency of vertices: if $A$ and $B$ are vertices, then $g(A)$ and $g(B)$ are vertices, and the distance of $g(A)$ from $g(B)$ equals the distance of $A$ from $B$ . Therefore, $g(A)$ and $g(B)$ are adjacent. Is this reasoning correct? I think (but I'm not sure) that proving that $g$ preserves adjacency of vertices requires the fact that $g$ carries vertices to vertices. Why is that true? In particular, what is the answer to the author's question? (How are vertices unlike other points in terms of their distance relationships with all points on the polygon?) Does this question bear any relation to Lemma 2.1 in Conrad's notes? EDIT: I'm still confused. I don't completely understand @zipirovich's answer : how can I rigorously show that the maximum $d(P)$ exists and that the value of $d(P)$ is largest when $P$ is a vertex, and smaller for any other point in the polygon? Maybe my comment is a possible answer, but I think I'm using the fact that each rigid motion in $D_n$ extends uniquely to an isometry of $\mathbb{R}^2$ , and I don't know if I'm answering Conrad's question ""How are vertices unlike other points in terms of their distance relationships with all points on the polygon?"".","['dihedral-groups', 'solution-verification', 'geometry', 'symmetry']"
3197658,Construct a nonabelian group of order 44,"Let $G$ be a group s.t. $|G|=44=2^211$ . Using Sylow's Theorems, I have deduced that there is a unique Sylow $11$ -subgroup of $G$ ; we shall call it $R$ . Let $P$ be a Sylow $2$ -subgroup of $G$ . Then we have $G=P\rtimes R$ and a homomorphism $$\gamma: P \rightarrow Aut(R)=Aut(\mathbb{Z_{11}})\cong(\mathbb{Z_{10}},+) .$$ Is this all correct so far? So what about $\gamma(p)=\phi_p$ where $\phi_p(r)=r^5$ . I thought this because $\tilde{5}\in\mathbb{Z_{10}}$ has order $4$ so the order of any element of $P$ could divide it... or something... So I was thinking the group would be something like $$G= \langle p,r | p^4=r^{11} prp^{-1}=r^5 \rangle .$$ Any insight is greatly appreciated! Thanks! I would like to know both where I went wrong and how to do it correctly. Did I do the above right? Identifying $\mathbb{Z_{11}}$ with the additive group of $\mathbb{Z_{10}}$ ? Or should I look at it multiplicatively, because I don't understand how that isomorphism works so it doesn't make sense to define the conjugation that makes the semi-direct product well defined based on elements of the additive group $\mathbb{Z_{10}}$ , but instead realize that $10 \in U(\mathbb{Z_{11}})$ has order $2$ so we can have a group presentation something like: $G = \langle p, r | p^2=r^{11}=1 , prp^{-1}=r^{10} \rangle$ Insight appreciated! I understand the dihedral group of the $22$ -gon works now, thank you. Can somebody help me with my approach in constructing a non-abelian group of order $44$ via the methods I've been using? Thanks!","['group-presentation', 'group-theory', 'abstract-algebra', 'sylow-theory']"
3197709,Cartan homotopy formula and curl,"In Topological Methods in Hydrodynamics , V. I. Arnol'd writes that the following expression $$curl(\mathbf a \times \mathbf b)=[\mathbf a, \mathbf b]+ \mathbf a \ div \ \mathbf b - \mathbf b \ div \ \mathbf a$$ could be obtained ""repeatedly applying"" the Cartan homotopy formula $$L_v = i_vd+di_v$$ And (in another book) adds some hints: $i_{curl(\mathbf a \times \mathbf b)}\tau = di_{\mathbf a} i_{\mathbf
   b}\tau$ $div \ \mathbf a = di_{\mathbf a} \tau$ $[\mathbf a, \mathbf b] = L_{\mathbf a} \mathbf b$ (where $\mathbf a, \mathbf b$ are two vectors in $R^3$ , $i$ is the interior product, and $\tau$ the volume element). I really could not figure how to proceed. Any suggestions?","['differential-forms', 'calculus', 'vector-analysis', 'differential-geometry']"
3197746,ODE in $\mathbb{R}^n$ defined by the gradient of a function,"I'm studying for an exam and I got stuck in this question: Let $x: I \to \mathbb{R}^n$ be a differentiable parametrized curve (I is an interval) in $\mathbb{R}^n$ and $f: \mathbb{R}^n \to \mathbb{R}$ be a differentiable function such that $\frac{dx}{dt}(t) = - \nabla f(x(t))$ . Show that either $x$ is a constant function or $f \circ x$ is strictly decreasing. I tried differentiating $f \circ x$ and arrived to the result that $\frac{d (f \circ x)}{dt} = - \Vert \nabla f(x(t)) \Vert ^2 \leq 0$ , which proves that $f \circ x$ is monotonically decreasing. I concluded that to show the desired result, it must be the case that if $\nabla f(x(c)) = 0$ for some $c \in I$ , then $\nabla f(x(t)) = 0$ for all $t \in I$ , but I can't understand or prove it. Can anyone shed some light?","['multivariable-calculus', 'ordinary-differential-equations']"
3197756,derivative of quaternion product,"I want to calculate the derivative of quaternion product. Say $p$ and $q$ are unit quaternions. And I want to calculate $\frac{\partial p\bigotimes q}{\partial q}$ . From one reference Quaternion kinematics for the error-state Kalman filter I found that I can calculate this by converting p to a left-quaternion-product matrix: $p\bigotimes q=[p]_{R}q$ Then, the partial derivative is just $[p]_{R}$ . My question is that what if I calculate this by the definition of partial derivative like the following $$\frac{\partial p\bigotimes q}{\partial q}
=\frac{p\bigotimes q\bigotimes dq\ominus p\bigotimes q}{dq}
=\frac{q^{*}\bigotimes p^{*}\bigotimes p\bigotimes q\bigotimes dq}{dq}
=\frac{dq}{dq}
=I.$$ What is the problem of this derivation?
Also I would like to ask what is the common way to calculate quaternion derivative. Just treat them as normal 4 by 1 vector and calculate vector derivative? Thanks","['derivatives', 'quaternions']"
3197758,Is there a way to perform this integration such that the answer is $e^{-|y|}$?,"Consider the function $f(y)=e^{-|y|}e^{y}$ I am trying to integrate this function with respect to another variable (such as $x$ ) so that the result from the integration is $e^{-|y|}$ ? The function $f(y)$ can be changed in anyway as long as 1) the powers of $y$ and $|y|$ stay equal to one. 2) and the boundaries of integration do not include $y$ in them. 3) The result of the integral is $e^{-|y|}$ . Of course the answer may be of the form $A e^{-a|y+b|}$ where $A$ , $a$ and $b$ are constants. So for example we can add a constant or $x$ inside the $||$ or multiply $y$ . So for example the integral $$\int^{c_2}_{c1}e^{-|x y+a|}e^{y/x}dx$$ or $$\int^{c_2}_{c1}(e^{-|ay+x|}e^{y+b}+d)dx$$ Is there a way to integrate this so that the answer is $e^{-|y|}$ ? We are free to choose where to put the constants or $x$ as long as the three conditions are satisfied.","['integration', 'calculus', 'definite-integrals']"
3197775,Intriguing Limit,"Prove that: $$L=\lim_{n\to\infty} \frac {\sqrt 2 n^{\left(n-\frac 12\right)}}{n!}\left(\frac {(2\sqrt[n] {n} -1)^n}{n^2}\right)^{ \frac {n\left(n-\frac 12\right)}{\ln^2 n}}=\sqrt {\frac {e}{\pi}}$$ My method: Properties I am going to use : 1)Stirling's approximation: $$n!\sim\sqrt {2\pi n} \left(\frac ne\right)^n$$ 2)Property 2 : $$\sqrt[n] {n}\sim 1+\frac {\ln n}{n}+\frac{\ln^2 n}{2n^2}$$ 3)Property 3: For all continuous and differentiable functions $f,g$ (In their domain respectively),  if $\lim_{x\to\infty} g(x)=0$ then for large enough $x$ we have $$(1+g(x))^{f(x)}\sim e^{f(x)\cdot g(x)}$$ Using Stirling's approximation we get $$L=\lim_{n\to\infty} \frac {e^n}{\sqrt{\pi} n}\left(\frac {\displaystyle (2\sqrt[n]{n} -1)^n}{n^2}\right)^{\frac {n\left(n-\frac 12\right)}{\ln^2n}}$$ Using Property 2 we get $$L=\lim_{n\to\infty} \frac {e^n}{\sqrt {\pi} n}\left(\frac { \left(1+\frac {2\ln n}{n}+\frac{\ln^2n}{n^2}\right)^n}{n^2}\right)^{\frac {n\left(n-\frac 12\right)}{\ln^2n}}$$ And using the property 3 we get $$L=\lim_{n\to\infty} \frac {e^n}{\sqrt{\pi} n} \displaystyle \frac {e^{\frac {n(2n-1)}{\ln n}}\cdot e^{ \left(n-\frac 12\right)}}{ n^{\frac {n(2n-1)}{\ln^2 n}}}$$ Using that $$n^{\frac {n(2n-1)}{\ln^2 n}}=e^{\frac {n(2n-1)}{\ln n}}$$ Using this alongwith previous results we get $$L=\lim_{n\to\infty} \frac {e^n}{\sqrt{\pi} n} \displaystyle \frac {e^{\frac {n(2n-1)}{\ln n}}}{e^{ \frac {n(2n-1)}{\ln n}}}\cdot e^{ \left(n-\frac 12\right)}=\lim_{n\to\infty} \frac {e^{2n}}{\sqrt{e\pi} n}$$ Which  clearly doesn't converge.Can someone please point out my mistake in above working. Also some new suggestions to solve this question will be quite beneficial.","['limits', 'calculus']"
3197802,Partial Derivative Disambiguation,"There are at least two substantially different meanings to $\frac{\partial}{\partial x}f(x,\ y,\ z(x))$ .  The $\partial x$ could mean ""with respect to $x$ the independent variable,"" or it could mean ""with respect to the $x$ the first parameter of $f$ .""  I think this can be understood in light of a net income calculation. Suppose $x$ is an individual's taxable gross income, $y$ is her non-taxable gross income (gifts received, etc.), $z$ is her income tax, and $f$ is her net income, all over the same year.  Since net income depends on taxable gross income, non-taxable gross income, and income tax, as given by $f = x + y - z$ , and income tax depends on taxable gross income, as given by $z = .15x$ (using a single 15% tax bracket for simplicity), we can write the overall equation as $f(x,\ y,\ z(x)) = x + y - z(x)$ where $z(x) = .15x$ , and then consider the meaning of $\frac{\partial}{\partial x}f(x,\ y,\ z(x))$ . If we interpret $\partial x$ to mean ""with respect to $x$ the independent variable,"" then $\frac{\partial}{\partial x}f(x,\ y,\ z(x))$ represents the change in net income relative to a reported change in taxable gross income, whereas if we interpret $\partial x$ to mean ""with respect to the $x$ the first parameter of $f$ ,"" then $\frac{\partial}{\partial x}f(x,\ y,\ z(x))$ represents the change in net income relative to an unreported change in taxable gross income. I have given, just as I have learned, a binary explanation of this difference.  The $\partial x$ refers to either an independent variable or a parameter of $f$ .  My question is whether it is also acceptable for it to refer to something in between.  Let's assign a new color to the contents of each nested layer of a function's parentheses, so that the above example becomes $f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{)}})$ .  This disambiguates things by allowing us to refer to the change in net income relative to a reported change in taxable gross income with $\frac{\partial}{\partial \color{tan}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{)}})$ and the change in net income relative to an unreported change in taxable gross income with $\frac{\partial}{\partial \color{blue}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{)}})$ .  I think colors are less misleading than subscripts in this case, because $\color{tan}{\textrm{x}}$ and $\color{blue}{\textrm{x}}$ are the same algebraic entity; it's just that when the calculus eats an algebraic expression and spits out a new one, it sometimes chews up the two $x$ 's a bit differently. With this setup, the question can be asked quite succinctly; can $\frac{\partial}{\partial x}f(x,\ y,\ z(x),\ a(x,\ z(x)), b(x,\ z(x))$ also mean $\frac{\partial}{\partial \color{orange}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{), a(}}\color{orange}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{orange}{\textrm{)}}\color{blue}{\textrm{), b(}}\color{orange}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{orange}{\textrm{)}}\color{blue}{\textrm{)}}$ and/or $\frac{\partial}{\partial \color{lime}{\textrm{x}}}f(\color{blue}{\textrm{x, y, z(}}\color{tan}{\textrm{x}}\color{blue}{\textrm{), a(}}\color{lime}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{lime}{\textrm{)}}\color{blue}{\textrm{), b(}}\color{red}{\textrm{x, z(}}\color{tan}{\textrm{x}}\color{red}{\textrm{)}}\color{blue}{\textrm{)}}$ , and/or have some other meaning drawn via a similar color hierarchy, or is a partial derivative unable to be taken with respect to orange or green (or red) $x$ , since they are neither independent variables nor parameters of $f$ ?","['multivariable-calculus', 'calculus', 'functions', 'definition', 'partial-derivative']"
3197817,Fourier Transform of Airy Equation,"I am trying to find $Y(k)$ of the equation $y''(x)-xy(x)=0$ and hence show that $$y(x)=\sqrt{\frac{2}{\pi}}\int_0^{\infty}\cos\left(\frac{k^3}{3}+kx\right) \ dk,$$ given $Y(0)=1$ . Here, we use the following definition of the Fourier transform: $$F(k)=\mathcal{F}(f(x))=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-ikx}f(x) \ dx.$$ It is easy to show that $$\mathcal{F}(xy(x))=i\frac{dY(k)}{dk},$$ where $Y(k)=\mathcal{F}(y(x))$ . My working is as follows: \begin{align}
\mathcal{F}(y''(x))-\mathcal{F}(xy(x))&=0 \\
-k^2\mathcal{F}(y(x))-i\frac{dY(k)}{dk}&=0 \\
i\frac{dY(k)}{dk}+k^2Y(k)&=0 \\
\implies Y(k)&=Ae^{ik^2} \\
\implies Y(k)&=e^{ik^2} \\ 
y(x)&=\mathcal{F}^{-1}(e^{ik^2}) \\
y(x)&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{i(k^2+kx)} \ dk \\
y(x)&=\sqrt{\frac{2}{\pi}}\int_0^{\infty}\cos(k^2+kx) \ dk \ \ \text{(sine is odd)}
\end{align} I don't know where/if I've made an error in the argument of $\cos$ .","['calculus', 'proof-verification', 'fourier-transform', 'ordinary-differential-equations']"
3197853,Number of subgroups of an abelian p-group,"Let $p$ be a prime number and let $n\in \mathbb{N}$ . I know that every abelian group of order $p^n$ is uniquely a direct sum of cyclic groups of order $p^{\alpha_i}$ where $\sum \alpha_i = n$ . Now the question: Among all abelian groups of order $p^n$ which one has the most number of subgroups? Actually, I am looking for the Max number of subgroups so a close upper bound for the maximum number of subgroups would also be appreciated. ADDED LATER: So far two persons submitted a solution, suggesting that the maximum number of subgroups is $2^n$ (Which is not true, consider $\mathbb{Z}_2\times\mathbb{Z}_2$ , an abelian group with $2^2$ elements and $5$ subgroups). They deleted their solution because there were some gaps.","['abelian-groups', 'group-theory', 'finite-groups']"
3197868,Finding the conditional expectation of independent exponential random variables,"Let $X$ and $Y$ be independent exponential random variables with respective rates $\lambda$ and $\mu$ . Let $M = \text{min}(X,Y)$ . Find (a) $E(MX|M=X)$ (b) $E(MX|M=Y)$ (c) Cov $(X,M)$ (a) I first tried $\displaystyle E(MX|M=X) = E(X^2) = \int_{0}^{\infty} x^2 f(x) dx = \int_{0}^{\infty} x^2 \lambda e^{-\lambda x} \, dx = \frac{2}{\lambda ^2}$ , which does not agree with the textbook answer. I then tried $\displaystyle E(MX|M=X) = E(M^2) = \int_0^\infty m^2 f(m) \,dm $ , where $f(m)$ is the pdf of $M$ which I know from here is equal to $\displaystyle (\lambda + \mu) e^{-(\lambda + \mu)m}$ $$\therefore E(MX|M=X) = \int_{0}^{\infty} m^2 (\lambda + \mu) e^{-(\lambda + \mu)m} dm = \frac{2}{(\lambda+\mu)^2}, $$ which agrees with the textbook answer. Why is my first attempt not correct? \begin{align}
\\[15pt]
\end{align} (b) On this part I first tried $E(MX|M=Y) = E(XY)$ and using the fact that $$E(g(X,Y)) := \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} g(x,y)f(x,y) \, dxdy \tag{*}$$ to write $$E(MX|M=Y) = E(XY) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xy f(x,y) dx dy$$ but I was not able to find the joint pdf $f(x,y)$ . Alternatively, I tried $E(MX|M=Y) = E(MY|Y<X)$ , but couldn't figure out where to go from here. My guess is to use the memoryless property of exponentials, but I'm not sure how to apply that. \begin{align}
\\[15pt]
\end{align} (c) $ \;\text{Cov}(X,M) = E(MX)-E(X)E(M)$ , where $\displaystyle E(X) = \frac{1}{\lambda}$ and $ E(M) = \int_{0}^{\infty} m f(m) dm 
= \frac{1}{\lambda + \mu}$ I'm not sure how to calculate $E(MX)$ . If I use equation (*), then I would again be stuck trying to find the joint pdf $f(x,m)$ like in part (b). Using a different approach: $M = \text{min}(X,Y) = \frac{X+Y-|X-Y|}{2}$ so that \begin{align}
E(MX) &= E\left(\frac{ X^2 + XY - X(|X-Y|) }{2}\right) \\
&= \frac{1}{2} \left( E(X^2) + E(XY) - E \left( X\sqrt{(X-Y)^2} \right)\right) \\
&= \frac{1}{2} \left( E(X^2) + E(X)E(Y) - \iint_{0}^{\infty} x\sqrt{(x-y)^2} f(x,y) dxdy \right),
\end{align} and again I'm stuck.","['statistics', 'probability-distributions', 'conditional-expectation', 'probability-theory', 'random-variables']"
3197878,"If $f(x,y)=9-x^2-y^2$ if $x^2+y^2\leq9$ and $f(x,y)=0$ if $x^2+y^2>9$ study what happens at $(3,0)$","If $$f(x,y)=\begin{cases}9-x^2-y^2&\text{if }x^2+y^2\leq9\\0&\text{if }x^2+y^2>9\end{cases}$$ study the continuity and existence of partial derivative with respect to $y$ at point $(3,0)$ . The graph of the domain of $f$ is: Continuity study at $(3,0)$ : $f(3,0)=9-3^2-0=0$ , but I do not know how to find $$\lim_{(x,y)\to(3,0)}f(x,y).$$ I tried the following: $$\lim_{(x,y)\to(3,0)}f(x,y)=\left\{\begin{array}{l}\displaystyle\underset{C_1\colon x^2+y^2=9}{\lim_{(x,y)\to(3,0)}}f(x,y)=\underset{C_1\colon x^2+y^2=9}{\lim_{(x,y)\to(3,0)}}(9-(x^2+y^2))=9-9=0\\\displaystyle\underset{C_2\colon x^2+y^2\neq9}{\lim_{(x,y)\to(3,0)}}f(x,y)=\underset{C_2\colon x^2+y^2\neq9}{\lim_{(x,y)\to(3,0)}}\text{??}=\text{??????}\end{array}\right.$$ but then I realized that the ""curve"" $C_2$ is actually NOT a curve but a set of infinite points, as shown in the previous image. Existence of partial derivative with respect to $y$ at $(3,0)$ : I know that I need to study whether $$\frac\partial{\partial y}f(3,0)=f'((3,0);(0,1))=f_y(3,0)=\lim_{h\to0}\frac{f((3,0)+h(0,1))-f(3,0)}{h}=\lim_{h\to0}\frac{f(3,h)}{h}$$ exists or not, but I am not able to even find that limit. Any help? Thanks!!","['limits', 'multivariable-calculus', 'piecewise-continuity']"
3197969,Manifolds that admit Lorentzian metrics?,"John Lee says in ""Riemannian Manifolds: An Introduction to Curvature"": With some more sophisticated tools from algebraic topology, it can be
  shown that every noncompact connected smooth manifold admits a Lorentz
  metric, and a compact connected smooth manifold admits a Lorentz
  metric if and only if its Euler characteristic is zero (see [O’N83, p.
  149]). The reference is [O’N83] Barrett O’Neill, Semi-Riemannian Geometry with Applications to
  General Relativity, Academic Press, New York, 1983. But I can't get access to this book. Is there another more available reference? It could be lecture notes, does not have to be a published text. Thank you.","['semi-riemannian-geometry', 'reference-request', 'differential-geometry']"
3198019,Computing how many distinct digital products are below $10^n$,"Given a number $n$ , its digital product is the product of its digit. So the digital product of $15$ is $1\times 5=5$ , and the digital product of $760$ is $0$ , etc. I recently saw a nice video on Numberphile where they discussed persistence, namely how many iterations of digital product do you need in order to reach a single digit (e.g. $75\to 35\to 15\to 5$ has persistence of $3$ ). Playing a bit with the idea, it occurred to me that an efficient way of writing a code to check for this would include having a dictionary. This is because if I know that $35$ requires two steps, I don't need to iterate three steps from $75$ , I can just notice that it's one more than $35$ . Of course, you don't want to store this information for every number, because then you'd be wasting a lot of memory when testing this with very large numbers. So my first quest was to wonder what would be the maximal size of a dictionary we need. Well, If we test all the way to $10^n$ , the largest digital product would be $10^n-1$ , which is $9^n$ . This is great, since $\frac{9^n}{10^n}\to 0$ , so you're being relatively efficient. But in the real world, this is still a massive size of memory allocation on a personal computer. The next step, therefore, is to ask, how many distinct values can we get from a digital product ? I tested this up to $10^8$ , and the answer may surprise you. It turns out that there are only $2026$ values that you get from digital products below $10^8$ . About half of those are actually $0$ , since the longer the number the harder it is to avoid $0$ , but we are still talking about $43000000$ numbers whose product is non-zero. Here is a basic state of the results: $$\begin{array}{c|l}
\text{Below} & \text{Distinct digital products}\\\hline
10^0 & 1\\
10^1 & 10\\
10^2 & 37\\
10^3 & 101\\ 
10^4 & 226\\
10^5 & 442\\
10^6 & 785\\
10^7 & 1297\\
10^8 & 2026
\end{array}$$ This is quite a slow growth rate, and again, it makes sense. The longer your numbers are, the more likely they are to have $0$ inside, and if not a $0$ , then at least $1$ which can then be omitted and not change the value of the product. Obviously, the first question is whether or not there are only finitely many digital products. Of course not. $10^n-1$ has a digital product of $9^{n-1}$ , which would provide us with infinitely many distinct values. Is there a relatively straightforward to recursively compute how many distinct digital products are there below $10^n$ ? Or at least a reasonable upper bound? (Note that there is some intuition as to why this can be done recursively. For example, if $k$ has small digits ( $1$ to $4$ , then $10k+2$ has the same digital product as $2k$ , so if $2$ appears in a number of length $n$ , and all the digits are small, it does not add a new value. There are a lot of cases to test, and , which is why I hope someone more talented than me when it comes to finite combinatorics can see through them.)","['recursive-algorithms', 'combinatorics', 'inclusion-exclusion']"
3198033,"If $f(x)f(y)+f(xy)\le -\frac{1}{4},\forall x,y\in[0,1)$, show that $f(x)=-\frac{1}{2}$","Let $f:[0,1) \to \mathbb{R}$ be a function such that $$f(x)f(y)+f(xy)\le -\dfrac{1}{4} \quad \forall\, x,y\in[0,1).$$ Show that $$f(x)=-\dfrac{1}{2} \quad \forall\, x \in[0,1).$$ I have proved that $f(0)=-\dfrac{1}{2}$ : if $x=y=0$ , we have $$f^2(0)+f(0)\le-\dfrac{1}{4}\Longrightarrow \left( f(0)+\frac{1}{2} \right)^2\le 0\Longrightarrow f(0)=-\dfrac{1}{2}.$$ But I can't prove $f(x)$ be constant. Thanks.","['contest-math', 'functional-equations', 'functions']"
3198068,"if $(\{n\alpha\}-c)(\{n\beta\}-c)\ge 0,\forall n\in N^{+}$ then have $\{\alpha\}=\{\beta\}$?","The following question was asked by one of my students.He didn't know the conclusion was correct. I thought for a long time and felt right.Because we seem to be using  Kronecker theorem It follows from Kronecker's density theorem stating that if $\theta$ is an irrational real number, $\alpha$ is a real number, and $\varepsilon>0$ is any positive real number, then there exist integers $h,k$ with $0<k$ such that $$
|k\theta-h-\alpha|<\varepsilon.
$$ Problem : Suppose $c\in (0,1)$ and irrational numbers $\alpha,\beta$ are such that $$(\{n\alpha\}-c)(\{n\beta\}-c)\ge 0,\forall n\in N^{+}.$$ Prove or disprove $$\{\alpha\}=\{\beta\}.$$","['number-theory', 'diophantine-approximation']"
3198089,"If $X_n$ and $Y_n$ are independent does $(X_n,Y_n)\overset{d}{\rightarrow}(X,Y)$?","More formally: If $X_n\overset{d}{\rightarrow}X$ and $Y_n\overset{d}{\rightarrow}Y$ and also $X_i$ and $Y_j$ are independent for all i,j; does $(X_n,Y_n)\overset{d}{\rightarrow}(X,Y)$ ? I am aware of the Cramer-Wold theorem to prove asymptotic convergence of vector of random variables but I can't quite figure out how to apply it here. (To be honest I don't even know if the statement is true but it feels like it should be).","['random', 'statistics', 'probability']"
3198096,A lower bound on $|S_n - S_{n+1}|$,"I am reading ""Durrett: Theory and examples"", fourth edition. In page 67, he proves theorem 2.3.7 (an application of Borel Cantelli lemma). During that demonstration, he arrives to a point where: $X_1,..., X_n$ are i.i.d. random variables, and $S_n$ their sum. $S_n/n$ converges to a finite limit. So $S_n/(n(n+1))$ goes to zero. $|X_n| \geq n$ infinitely often. So, using that $S_n/n - S_{n+1}/(n+1) = S_n/(n(n+1)) - X_{n+1}/(n+1)$ he states that $|S_n/n - S_{n+1}/(n+1)| > 2/3$ infinitely often. Why does he use two thirds as the lower bound? Since $S_n/(n(n+1))$ goes to zero and $X_{n+1}/(n+1)$ is above 1 infinitely often, I could use $1-\epsilon$ as a lower bound. Am I wrong? Is there any reason to choose $\epsilon = 1/3$ Thank you very much.",['probability-theory']
3198106,"Is $A^2-B^2$ positive definite too when $A-B,B$ is positive definite?","Denote $A,B\in M_n(\mathbb{R})$ If $A-B,B$ is positive definite, it's easy to see $A^2-B^2$ is symmetric. Now the question is: Prove or disprove: $A^2-B^2$ is positive definite. I have checked some easy examples(mostly 2x2), and now I believe this is true. But we know here $A,B$ do not necessarily commute. I tried to write $A$ as $A=P^\mathrm{T}P$ , and then I know all the eigenvalues of $(P^\mathrm{T})^{-1}BP^{-1}$ all lay in the interval $(0,1)$ . Another question show me that when $A,B$ is positive definite, $AB+BA$ can also be no positive definite.
I can't move forward. Hint will also be appreciated.","['linear-algebra', 'positive-definite']"
3198118,Intuition behind determinant of a matrix with $2$ equal rows [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question In my linear algebra course, we have just proved that if a matrix $A$ contains $2$ equal rows, then $\det(A)=0$ . I understand how the proof works, but could somebody offer a more intuitive explanation of why this is the case?","['determinant', 'linear-algebra', 'intuition']"
3198126,2nd order linear ODE non constant coefficient solution methods,"I'm working on a project and I've come across a seemingly standard ODE, however I have no idea how to solve it.  The equation in question is $$
\epsilon'' + \left[g(\tau) + \frac{\beta}{\tau}\right]\epsilon' + \left[\frac{\gamma}{\tau^{4/3}}\right]\epsilon = 0
$$ where $g(\tau) = \alpha \cos(\tau)$ or $\delta\cos(\tau)\sin(\tau)$ (there are two cases).  I've tried a series solution but unsurprisingly it's pretty messy and doesn't really solve anything, and I've tried Laplace transforms but unfortunately they're divergent for negative powers of $\tau$ . Does anybody have recommendations for methods I can try, or even how to make a decent ansatz?  This is just a step in my project and it's really presenting a roadblock for me, so any help is appreciated.",['ordinary-differential-equations']
3198159,Can't solve system of linear equations (that need simplification first),"I'm self-studying from Stroud & Booth's amazing ""Engineering Mathematics"", and am stuck on a problem at the end of the ""Linear Equations"" chapter. I've a system of two linear equations: $$\frac{3x+2}{4} - \frac{x+2y}{2} = \frac{x-3}{12}$$ $$\frac{2y+1}{5} + \frac{x-3y}{4} = \frac{3x+1}{10}$$ So, these two first need to be simplified. I assume that the LSM for the first one (for 2, 4 and 12) is 12, so we have: $$12\frac{3x+2}{4} - 12\frac{x+2y}{2} = 12\frac{x-3}{12}$$ Simplifying further, we have: $$3(3x+2) - 6(x+2y) = (x-3)$$ $$9x + 6 - 6x - 12 y = x - 3$$ $$9x -6x -x - 12y = -3 +6$$ Finally, we get our first simplified linear equation: $$2x - 12y = 3$$ Now, onto the second one. The LSM of 5, 4 and 10 is 20, so we have: $$20\frac{2y+1}{5} + 20\frac{x-3y}{4} = 20\frac{3x+1}{10}$$ Simplifying further, we have: $$4(2y+1) + 5(x-3y) = 2(3x + 1)$$ $$8y + 4 + 5x - 15y = 6x + 2$$ $$5x - 6x + 8y - 15y = 2 - 4$$ We get our second simplified linear equation: $$-x -7y = 2$$ Now we can solve our system of linear equations: $$2x - 12y = 3$$ $$-x -7y = 4$$ Multiplying the second one by 2: $$2x - 12y = 3$$ $$-2x -14y = 4$$ Now, we add the two equations, and get: $$-26y = 7$$ Solving for $y$ , we get: $$y = -\frac{7}{26}$$ which I'm fairly certain is not a correct answer. Can anyone see where I'm going wrong here?","['self-learning', 'algebra-precalculus']"
3198203,5 digits numbers such that when the sum of digits divided by 4 leaves remainder 2.,"How many 5 digits numbers such that when the sum of digit divided by 4 leaves remainder 2. Example:-
Consider a 5 digit number- $(x1,x2,x3,x4,x5)$ Then $(x1+x2+x3+x4+x5)$ must be of form $(4n+2)$ I tried this
(x+x²+x³...+x^9)(1+x+x²+x³....+x^9)⁴ In this sum of coefficient of x^(2,6,10,14....42) But this involve lot of calculation.! Please some one provide me something different and smarter solution.","['permutations', 'number-theory', 'combinations']"
3198238,"Bifurcation analysis, limit cycle collapses on two symmetric fixed points","Coming back on the system I already mentioned in another post, this time I am working on some bifurcation analysis of a 2D System. The system is defined by the following equations. I am assuming $\tau_a >1$ to be kept fixed. \begin{equation}
\begin{aligned}
\dot d_{1} &= - d_1 - e_1 + \varepsilon f(d_1) \\
\tau_a \dot{e}_{1}&=g_a f(d_1) - e_{1}\\
f(d_1) &= \frac{e^{d_1}-1}{e^{d_1}+1}
\end{aligned}
\label{eq:2D_sync}
\end{equation} In particular, one can show that a Hopf bifurcation occurs when $\epsilon>\epsilon^*,\ \epsilon^*=2(1 + \frac{1}{\tau_a})$ . I mentioning it here for the sake of completeness, although it is not the main problem I am trying to solve. In orange are plotted some trajectories of the system forward in time In blue are plotted some trajectories of the system backward in time In red are plotted the nullclines After Hopf bifurcation: By increasing the parameter $\epsilon$ , the size of the limit cycle increases until two fixed points emerges ""from inside"" the limit cycle. In the picture below you can see the intersection of the two nullclines. By further increasing $\epsilon$ the limit cycle collapses on the fixed points that become stable. Zooming on the new stable fixed points. It looks again as an Hopf bifurcation but I did not find anything to classify it properly. Do you have any suggestion on some more rigorous analysis/example? UPDATE Numerically I computed the value of $\epsilon^{o}=13.12$ at which the off-origin equilibria bifurcates. It is a subcritical Hopf bifurcation. In the figure below I plot the system's evolution with $\epsilon=13.23$ . and just before the catastrophe ( $\epsilon=13.246$ ) (the two blue limit cycles also merge, it seems). In the plot below, only two trajectories are plotted (forward in time in orange, backward in time in blue). The initial transient has been removed as well. I would say that the unstable (in blue) and stable (in orange) limit cycles collides and annihilate but I think I need something more rigorous or a similar system to motivate this intuitive consideration.","['ordinary-differential-equations', 'bifurcation', 'catastrophe-theory', 'limit-cycles', 'dynamical-systems']"
3198241,Simple Geometric Proof of Intersecting Circles,"The following problem has been bothering quite a while. I guess there is a gap in my school knowledge of geometry, but I do not know how to show that: Prove or disprove that given two circles with centers $O_1, O_2$ of the same radius $r$ , which intersect at two points $A, B$ , the intersection of the circles is entirely contained in the circle with the center $M$ at the midpoint of the line $O_1O_2$ and radius $AM$ . In the picture below: $M$ would be the origin, and $A$ and $B$ are two points, where the circles intersect. The desired circle that is supposed to contain the intersection is drawn in green.","['euclidean-geometry', 'geometry']"
3198259,$\frac1n\sum _{k=1}^na_k\to0$ if and only if $\frac1n\sum _{k=1}^na^2_k\to0$ [duplicate],"This question already has answers here : Sequence such that $\lim\limits_{n\to \infty} \frac{x_1^2+x_2^2+...+x_n^2}{n}=0$ (2 answers) Closed 5 years ago . If $(a_n)$ is a sequence in $(0,1)$ , show that $\frac1n\sum _{k=1}^na_k\to0$ if and only if $\frac1n\sum _{k=1}^na^2_k\to0$ My try: $\implies$ : Since $a_k\in (0,1)$ , we have $0\le\frac1n\sum _{k=1}^na^2_k\le \frac1n\sum _{k=1}^na_k$ , and if RHS goes to zero, then by squeeze theorem, $\frac1n\sum _{k=1}^na^2_k\to0$ How to show other direction?","['limits', 'sequences-and-series', 'real-analysis']"
3198266,How to obtain recursion relations from this,"I'm trying to solve a problem using  the power series solution. Finally (and after substitution of differentations) I have come up with $$
-\frac1{2\mu}\sum_{i=2}^p i(i-1)a_i r^{(i-1)}+\frac1{2\mu}\omega \sum_{i=1}^p i\,a_i \,r^{(i+1)}-\frac{(l+1)}{\mu}\sum_{i=1}^p i\,a_i\,r^{(i-1)}-\frac{(4\mu^2-1)}{8\mu}\omega^2\sum_{i=0}^p a_i\,r^{(i+3)}+\frac{(2l+3)}{4\mu}\omega\sum_{i=0}^p a_i\,r^{i+1}-\sum_{i=0}^p a_i\,r^{i}=0
$$ According the power series solutions method now it's time to equalize the summation limits and powers of $r$ , but here the powers include a variable called $l$ . For this reason I'm confused how to proceed this calculation and get recursion relations? Addendum: As a user said there is no need to include $l$ 's in the summations. So I divided out them. Now the problem is equalizing the powers and limits.","['recurrence-relations', 'ordinary-differential-equations']"
3198278,"Let $\mu(X)=1$, $0 \leq f \leq k$, and $m=\int_X f d\mu$. Show $\int_X |f-m|^2 d\mu \leq \frac{k^2}{4}$.","Let $\mu(X)=1$ for $\mu$ a positive measure. Let $0 \leq f \leq k$ for some $k\in\mathbb{R}$ and let $m=\int_X f d\mu$ . Show $\int_X |f-m|^2 d\mu \leq \frac{k^2}{4}$ . My attempt: I tried to expand the integrand to see where it would take me. In general, we have $|a|^2=a^2$ , so then $|f-m|^2=(f-m)^2=f^2-2fm+m^2$ . Thus, $$ \int_X |f-m|^2 d\mu = \int_X f^2 d\mu - 2m\int_X f d\mu + \int_X m^2 d\mu$$ $$ = \int_X f^2 d\mu - 2m^2 + m^2 = \int_X f^2 d\mu -m^2. $$ I thought about using the fact that $f\leq k$ to obtain $$ \int_X f^2 d\mu -m^2 \leq k^2 - m^2, $$ but I do not think it is true that $k^2 - m^2 \leq \frac{k^2}{4}$ , because that would require $k\leq m\sqrt{\frac{4}{3}}$ . That means that I need a tighter bound on $\int_X f^2 d\mu - m^2$ . If I can show that $$ \int_X f^2 d\mu \leq \frac{k^2}{4} + m^2 $$ then I am done, but I am not quite sure how to do that.","['integration', 'measure-theory', 'analysis', 'real-analysis', 'probability-theory']"
3198279,"Maximum and minimum of $\frac{1}{n} \cot(n \pi \phi)$, $\phi$ Golden ratio","Studying aspects of this problem I stumbled on this question. Designating the golden ration by $\phi=\frac{1+\sqrt{5}}{2} \simeq 1.61803$ and letting $$a(n) = \frac{1}{n} \cot(n \pi \phi)$$ (i) prove that $a(n)$ is bounded from above and from below (ii) calculate $\max (a(n))$ and $\min (a(n))$ with $n =1,2,3,...$ (iii) solve the similar problem when $\cot$ is replaced by $\csc$ , i.e. consider $$b(n) = \frac{1}{n \sin(\pi \phi n)}$$ In this case check the validity of my conjecture that $b(1)< b(n) < b(3)$ for $n\gt 3$ (iv) Extension: the same if $\phi$ is replaced by other irrational quantities like $\sqrt{2}$ , $2^{\frac{1}{3}}$ , $\log{2}$ , $\gamma$ , $\pi$ , $e$ . Here except for the case $\sqrt{2}$ I have no indication that the extremes exist at all, i.e. that $a(n)$ is bounded if $n \to \infty$ . What makes this question interesting (IMHO)? One aspect is this: The expression $b(n)$ , when considered as a function of real $n\gt 0$ has simple poles at $$n_{k} = k/ \phi, k=1,2,3,...$$ An integer $n$ can become very close to an $n_k$ . I found it surprising that the rather modest damping factor $\frac{1}{n}$ is able to cancel the steep rise in the vicinity of the poles. What have I done so far? The modest part I did up to now is in the reference above. Addtionally here are graphs of the quantities in question. Remark: the choice of Fibonacci numbers as the upper limit of the range is made plausible in the quoted investigation.","['maxima-minima', 'fibonacci-numbers', 'sequences-and-series']"
3198291,Words with repeated blocks of letters,"If I have the word BARBARIANISM, how many arrangements of this word's letters contain two identical blocks of 3 letters (e.g. 'BAR' repeated twice in the original word, or 'AIR' repeated twice in 'BAIRBANSAIRM'). I think there are two ways in which we can create such a block of 3 letters. One include the letter 'A', and one without the letter 'A'. That is ${3\choose 1}\times3!$ and $3!$ ways respectively. I think the number of ways we can arrange the other letters (not in the block of 3) with and without the letter 'A' being used in said block of 3 is $\frac{6!}{2!}$ and $\frac{6!}{3!}$ respectively. There are ${7\choose2}$ ways we can arrange the 2 blocks of 3 within the new word, so by putting it all together I think that \begin{equation*}
{7\choose2}\Big(\frac{6!}{3!}\times3! + \frac{6!}{2!}\times{3\choose 1}\times3!\Big) = 151200 \mbox{ ways}
\end{equation*} is the answer. Is my method correct? I feel like this number is too large and I may have double counted. Thanks.",['combinatorics']
3198310,Open Affine subsets of $\mathbb{A}^n_k$ are principal,"I have heard that affine open subsets of $\mathbb{A}^n_k$ are all principal (i.e of the form $D_f$ ). I know this is not true if one removes the assumption the open subset is affine. I have tried to prove this fact using quasi-compactedness of $\mathbb{A}_n^k$ , but this is getting nowhere and I don't see how the assumption that the open is affine will be used with this method.","['affine-schemes', 'algebraic-geometry']"
3198316,"Show that, in a group of n people, everyone has the same number of friends if..","Question: Consider a group of n people with the following properties: • no person is friends with everyone, • any pair of strangers share exactly one friend in common, • no three people are mutually friends. Show that everyone has the same number of friends. I want to solve this using Ramsey's theorem but I'm struggling to formulate it in a way that would make it straightforward.. Any help would be greatly appreciated.","['graph-theory', 'combinatorics', 'ramsey-theory']"
3198331,Multivariate Kolmogorov distance bounded by Wasserstein distance,"I'm trying to find a bound for the multivariate Kolmogorov distance in terms of the Wasserstein distance.
Denoting by $F$ and $G$ two cumulative distribution functions (cdf) on $\mathbb{R}^n$ the Kolmogorov and Wasserstein distances between $F$ and $G$ are given by \begin{align}
d_{K}(F,G) &:= \sup_{\mathbf{x} \in \mathbb{R}^n} \left| F(\mathbf{x})- G(\mathbf{x}) \right|,\\
d_{W_1}(F,G) &:= \inf_{M_{F,G}} \left\{ \int_{\mathbb{R}^{n}\times\mathbb{R}^{n}} \Vert \mathbf{x}-\mathbf{y} \Vert dM_{F,G}(\mathbf{x},\mathbf{y}); M_{F,G} \text{ is a cdf with margins } F \text{ and } G \right\},
\end{align} where $\Vert \mathbf{x} \Vert$ is a vector norm on $\mathbb{R}^n$ . If $G$ has a density $g$ bounded by $C := \sup_{x\in\mathbb{R}} g(x)$ we have in the univariate case the following bound (see Lemma 1 in https://statweb.stanford.edu/~souravc/Lecture2.pdf ): \begin{align}
d_{K}(F,G) \leq 2 \sqrt{C} \sqrt{d_{W_1}(F,G)}.
\end{align} In the multivariate ( $n$ -dimensional) case I used the same approach to get the following result.
If $G$ has a density $g$ bounded by $C := \sup_{\mathbf{x}\in\mathbb{R}^n} g(x)$ and $G$ is compactly supported on a set $D \subsetneq \mathbb{R}^n$ where $K = \max_{i=1,\ldots,n} \{ \sup_{\mathbf{x,y}\in D}|x_i-y_i| \}$ is the longest possible distance in one direction in $D$ then \begin{align}
d_{K}(F,G) \leq 2 \sqrt{CnK^{n-1}} \sqrt{d_{W_1}(F,G)}.
\end{align} However, I would like to get rid of the bounded support assumption for $G$ (density, smoothness or moment assumptions for $F$ and $G$ would be fine) and I was wondering if anyone knows of a less restrictive result of the same flavor?","['statistics', 'optimal-transport', 'multiple-integral', 'inequality', 'probability-theory']"
3198350,What is the base measure in measure theory?,"I see the term ""base measure"" used frequently about measures. I do not completely get what that exactly means: Some examples are: Let $\cal F$ be the space of all probability density functions with
  respect to a base measure $\nu$ What is the base measure? Sometimes when a probabilistic function is integrated, the dx is called a base measure. $$\int_{\cal X}  ....  dx$$ Can someone explain in simple words or refer me to a simple reference to read about ""base measures"".","['measure-theory', 'probability', 'reference-request']"
3198379,Find : $\int_0^{\pi/4}x\ln(\sin x)\mathrm dx$,I'm try to find this integral $$\int_0^{\pi/4}x\ln(\sin x)\mathrm dx$$ My try use : $\ln(\sin x)=-\ln2-\sum\limits_{n=1}^{\infty}\frac{\cos (2nx)}{n}$ But I don't know how to complete summation ... I will happy if someone help me Thanks!,"['integration', 'trigonometry', 'definite-integrals']"
3198586,Product of vector fields is not a vector field,"Let $M$ be a manifold and $X,Y$ be vector fields on $M$ . The bracket $[X,Y]:=XY-YX$ is a vector field when $X,Y$ are smooth, but why is $XY$ not a vector field when $X,Y$ are smooth? By definition, a smooth vector field takes a smooth map on $M$ to a smooth map on $M$ . So if $f$ is a smooth map on $M$ then $XY(f)$ is $X(Y(f))$ and by hypothesis $Y(f)$ is a smooth map on $M$ so that $X(Y(f))$ is a smooth map on $M$ as well.","['manifolds', 'vector-fields', 'smooth-manifolds', 'differential-geometry']"
3198648,Combinatorial proof for $\sum_{k=0}^p (-1)^k {n \choose k} = (-1)^p {n-1 \choose p}$ [duplicate],"This question already has answers here : Combinatorial Proof of Combinatorial Identity involving $(-1)^k \binom {n-1}{k}$ (1 answer) Combinatorial proof that $\sum_{j=0}^k (-1)^j {\binom n j}=(-1)^k \binom{n-1}{k}$ (3 answers) Closed 5 years ago . I am trying to give a combinatorial proof for: $$\sum_{k=0}^p (-1)^k {n \choose k} = (-1)^p {n-1 \choose p}$$ Where $p$ and $n$ are natural numbers. We could easily see that if $p=n$ this reduces to the fact that a set has as many subsets of even cardinality, as those with odd cardinality. However, this formula suggests a relation between the even and odd cardinalities less that $p$ . Note: I'm not interested in an algebraic proof, as Pascal's identity gives a telescopic sum on the RHS. Thank you in advance.","['summation', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
3198660,"Prove that if $f$ is entire and $\vert f(z^2) \vert \leq 2 \vert f(z) \vert$, then $f$ is constant",I'm not sure if this requires Liouville's theorem or the use of the integral formula for the Taylor coefficients but I cant get either to work. By the formula for the Taylor coefficients for $f(z^2)$ we have that: $$\vert a_n\vert  = \Big\vert \frac{1}{2 \pi i} \int_{\vert z\vert =R} \frac{f(z^2)}{z^{n+1}} dz\Big\vert \leq \frac{1}{2 \pi} \int_{\vert z \vert =R} \frac{\vert f(z^2) \vert}{\vert z \vert^{n+1}} \vert dz \vert \leq \frac{1}{2 \pi} \int_{\vert z \vert =R} \frac{2\vert f(z) \vert}{\vert z \vert^{n+1}} \vert dz \vert  $$ and then this can be bounded by $$ \frac{1}{2 \pi } \frac{M_f(R)}{R^{n+1}} 2 \pi R =  \frac{M_f(R)}{R^{n}} $$ where $M_f(R)$ is the max of $\vert f\vert $ on the circle $\vert z \vert =R$ . I had wanted to take the limit as $R \to \infty$ and have that it equals $0$ unless $n=0$ but this isn't clear to me.,['complex-analysis']
3198674,Number of permutations differing in at least $d$ spots in pairwise comparisons,"A friend and I were thinking about this problem today but we were unable to come up with a solution. Problem: Consider the the numbers $S=\{1,\ldots,n\}$ . Given $2\le d \le n$ what is the maximal number of permutations $p(d)$ of $S$ you can choose such that the following holds: Whenever
  you compare two chosen permutations, they differ in at least $d$ spots? Note that we always have $2\le d\le n$ since two permutations cannot differ in exactly one spot. Note also that $n\le p(d)\le n!$ for all $d:$ $p$ is non-increasing and for $d=n$ one can choose the natural order first and proceed cyclically by writing \begin{matrix}
1 & 2 & \ldots & n-1 & n \\
2 & 3 & \ldots & n   & 1 \\
\ldots & \ldots & \ldots & \ldots & \ldots \\
n & 1 & \ldots & n-2 & n-1 \\
\end{matrix} so that $p(n) \ge n.$ Actually $p(n)=n$ since when writing the $n$ permutations differing in $n$ spots in pairwise comparisons into the rows of a matrix, the first column will contain all $n$ numbers. Thus it is not possible to add a row to the matrix that differs from all previous rows at the first index. Example: for $n=3$ all possible permutations are: \begin{matrix}
1 & 2 & 3\\
1 & 3 & 2\\
2 & 1 & 3\\
2 & 3 & 1\\
3 & 1 & 2\\
3 & 2 & 1
\end{matrix} For $d=3$ we get $p(d) = 3$ since $n=3,$ by the reasoning outlines above, and for $d=2$ we get $p(d) = 6$ since all distinct permutations differ in at least two spots.","['permutations', 'combinatorics', 'discrete-mathematics', 'elementary-set-theory', 'recreational-mathematics']"
3198680,Prove that $SU(n)$ is simply-connected (using Van Kampen),"I have some problems to prove that $SU(n)$ is simply-connected, for $U(n) = \{ M \in GL_n(C) \; | \; ^{t}\overline{M}M = Id \;\}$ . In fact, there is some indications (to follow if it's possible). First, let's define $N=(0, \dots, 1)$ the North pole of $S^{2n-1} \subset \mathbb{C}$ . We suppose $n \geq 2$ . Then : Let $e_N : SU(n) \rightarrow \mathbb{S}^{2n-1}$ the application which evaluates the matrix at the $N$ , i.e : $e_N(A) = A \cdot N$ . Prove that it exists a continuous section : $s : \mathbb{S}^{2n-1} -{N} \rightarrow SU(n)$ of $s$ , i.e $e_N \circ s = Id$ . Then, show by induction that $SU(n)$ is simply connected, using Van Kampen. For the first point, so to find the section, let's $z \in \mathbb{S}^{2n-1} -N$ . Then, we want to say that $s(z)(N) = z$ and $s(z) = Id$ over $Vect(N,z)^{\perp}$ . Then, we have defined $s$ over $N$ and $Vect(N, z)$ , and as we should send an orthonormal basis on an orthonormal basis, and as $dim(Vect(N, z)^{\perp} + Vect(N))=2n-1$ , finally $s$ is totally determined. Then, I don't succeed to do the second part, i.e showing that $SU(n)$ is simply connected. Actually, let's suppose the result is true for $SU(n-1)$ . We can identify $SU(n-1)$ with $SU(Vect(N)^{\perp}) \subset SU(n)$ (by the ""natural"" inclusion). Then, I would like to consider : $S^{2n-1} - N \times SU(n-1)$ and $S^{2n-1} - S \times SU(n-1)$ (actually, the matrix associated, the first column would be the vector of $S^{2n-1} - N$ and the others columns would be the one of the element of $SU(n-1)$ ) because each factor are simply connected, but it's not a open cover of $SU(n)$ , as it's not even elements of $SU(n)$ . So, it should be here that we have to used the section. But I don't figure out how to do it. Someone could help me ? Thank you very much !","['algebraic-topology', 'analysis']"
3198692,"The multiplicative group of units in $\mathbb{Z}_{p}$ is isomorphic to $\mathbb{Z}_{p} \times C_{n}$ with $n=\max\{p-1,2\}$.","Problem. (a) Write down explicitly the rules for addition and multiplication in $\mathbb{Z}_{p}$ . (b) Show that $(\mathbb{Z}_{p^{i}},\varphi_{ij})$ where $\varphi:\mathbb{Z}_{p^{j}} \to \mathbb{Z}_{p^{i}}$ is given by $\varphi_{ij}(n + p^{j}\mathbb{Z}) = n + p^{i}\mathbb{Z}$ is a inverse system of finite rings and groups. (c) Show that $$\mathbb{Z}_{p} = \varprojlim_{i}\mathbb{Z}_{p^{i}}.$$ (d) Show that the multiplicative group of units in $\mathbb{Z}_{p}$ is isomorphic to the direct product of $\mathbb{Z}_{p}$ with a cyclic group of order $\max\{p-1,2\}$ . I have no problems with (a), (b) and (c). For item (d), I dont know how to approach this problem. I take an arbitrary cylic group of order $\max\{p-1,2\}$ and I tried to construct a isomorphism between the groups, but it doesn't works (at least, I cannot see how to do that). I would like some hints and approaches to follow. Also, in the item (c), I used the inverse system given in (b) and I built a bijection between $\mathbb{Z}_{p}$ and $\varprojlim \mathbb{Z}_{p^{i}}$ , considering $\varprojlim \mathbb{Z}_{p^{i}}$ as a subgroup of $\prod_{i}\mathbb{Z}_{p^{i}}$ . Generally, this is the standard approach to problems like that. But I would like to know any other alternative approach. Thanks for the advance. Edit. I read the references in comment below, but it uses some things that the book doesn't comment on (like exact sequences, Hensel's lemma, for example). Basically, the book defines $\mathbb{Z}_{p}$ as the set of formal infinite sums and proves only the necessary results to find the completion of $\mathbb{Z}$ , I mean: the book doesn't develop until now $p$ -adic theory. So, I would like to know if there is another proof.","['p-adic-number-theory', 'group-theory', 'abstract-algebra', 'profinite-groups']"
3198694,Let $X_1$ and $X_2$ be uniform on $n$-spheres. What is the distribution of $\| X_1+X_2\|$?,"Suppose we have two  independent random variables $X_1$ and $X_2$ distribution on $n-1$ -sphere of radius $r_1 $ and radius $r_2$ , respectivly. Assume $r_1>r_2$ . Recall, that the $n-1$ -sphere of radius $r$ is defined as \begin{align}
S_{n-1}= \{ x \in \mathbb{R}^n : \|x\|=r \}.
\end{align} We have to find the distribution of \begin{align}
U=X_1+X_2
\end{align} We can see that $U$ will be distributed on an annulus \begin{align}
A=\{ x:  r_1-r_2 \le \| x\|\le r_1+r_2  \}
\end{align} It is not difficult to see that $U$ has a uniform spherical angle. Therefore, the question is what is the distribution of the magnitude of $U$ that is $\| U\|$ ? This question is an extension of the question previously asked here . For the bounty: I would like to see the exact expression for the distribution of $U$ .","['probability-theory', 'probability']"
3198703,"If $H_1 \subset H_2 \subset G$ and $G/H_2,\ H_2/H_1$ are compact then $G/H_1$ is compact.","I'm trying to solve the following exercise of the book ""Grupos de Lie - Luiz A. B. San Martin  (exercise 18, page 55)"": Exercise: Let $G$ be a topological group (if necessary, $G$ is Hausdorff) and $H_1 \subset H_2\subset G$ closed subgroups of $G$ . Show that if $G/H_2$ and $H_2/H_1$ are compact, then $G/H_1$ is compact. Some comments... It is easy to see that the function \begin{align*}
\pi: G/H_1 &\to G/H_2 \\
g H_1 & \mapsto g H_2,
\end{align*} is a continuous and open function, nevertheless $\pi$ also satisfies $g_1 \cdot \pi(g_2) = \pi(g_1\cdot g_2)$ , $\forall \ g_1 \in G,\ $ ( $g_1 \cdot (g_2 H) = (g_1 \cdot g_2) H$ ). Although I am aware of the following theorem: Theorem: Let $G$ be a topological group, and $H$ a closed subgroup of $G$ , if $H$ and $G/H$ are compact then $G$ is compact. I can't apply it to solve my problem, once neither $G/H_1$ nor $H_2/H_1$ are topological groups. So, I tried to adapt the proof of the theorem cited above for my case, and it is necessary to show that the function $ \pi $ is a closed function, 
 which I was not able to conclude. Can anyone help me?","['general-topology', 'topological-groups', 'group-theory']"
3198750,How to compute a Jacobian using polar coordinates?,"Consider the transformation $F$ of $\mathbb R^2\setminus\{(0,0)\}$ onto itself defined as $$
F(x, y):=\left( \frac{x}{x^2+y^2}, \frac{y}{x^2+y^2}\right).$$ Its Jacobian matrix is $$\tag{1}
\begin{bmatrix} \frac{y^2-x^2}{(x^2+y^2)^2} & -\frac{2xy}{(x^2+y^2)^2} \\ -\frac{2xy}{(x^2+y^2)^2} & \frac{x^2-y^2}{(x^2+y^2)^2} \end{bmatrix},\quad \text{and its determinant equals}\ \frac{-1}{(x^2+y^2)^2}.$$ The following alternative computation is wrong at (!) and (!!), and I cannot see why. Let $\phi\colon (0, \infty)\times (-\pi, \pi)\to \mathbb R^2$ be the map $$\phi(r, \theta) =(r\cos \theta, r\sin \theta).$$ Let moreover $$\tag{2}\tilde{F}:=\phi^{-1}\circ F\circ \phi;$$ then, by an easy direct computation, $$\tilde{F}(r, \theta)=\left( \frac1r, \theta\right).$$ The Jacobian matrix of $\tilde{F}$ is, thus, $$\tag{!}\begin{bmatrix} \frac{-1}{r^2} & 0 \\ 0 & 1\end{bmatrix} , \quad \text{and its determinant equals }\ \frac{-1}{r^2}.$$ On the other hand, by (2) and by the chain rule, the Jacobian determinants of $F$ and $\tilde{F}$ are equal. We conclude that the Jacobian determinant of $F$ is $$\tag{!!} \frac{-1}{r^2}=\frac{-1}{x^2+y^2}.$$ The result (!!) is off by a factor of $r^{-2}$ from the correct one, which is given in (1). Equation (!) must also be wrong. Indeed, computing the Jacobian matrix from (2) using the chain rule, and using that $$
D\phi = \begin{bmatrix} \cos \theta & \sin \theta \\ 
-r\sin \theta & r\cos \theta\end{bmatrix}$$ and that $$\tag{!!!}
D(\phi^{-1})= \begin{bmatrix} \frac{x}{\sqrt{x^2+y^2}} &  \frac{y}{\sqrt{x^2+y^2}} \\ -\frac{y}{x^2+y^2} & \frac{x}{x^2+y^2}\end{bmatrix},$$ I obtain the result $$
\begin{bmatrix} \frac{x}{\sqrt{x^2+y^2}} &  \frac{y}{\sqrt{x^2+y^2}} \\ -\frac{y}{x^2+y^2} & \frac{x}{x^2+y^2}\end{bmatrix} \begin{bmatrix} \frac{y^2-x^2}{(x^2+y^2)^2} & -\frac{2xy}{(x^2+y^2)^2} \\ -\frac{2xy}{(x^2+y^2)^2} & \frac{x^2-y^2}{(x^2+y^2)^2} \end{bmatrix}\begin{bmatrix} \cos\theta & -r\sin\theta \\ \sin \theta & r\cos \theta\end{bmatrix} = \begin{bmatrix} -\frac1{r^2} & 0 \\ 0 & \frac{1}{r^2}\end{bmatrix},$$ which is different from the matrix in (!), and which gives the correct determinant of $-1/r^4$ , as it should be. Can you help me spot the mistake? SOLUTION ( added at a later time ). As answerers pointed out, there is a mistake in (!!!). The correct matrix to be used is $$ D(\phi^{-1})|_{F\circ \phi(r, \theta)} 
= \begin{bmatrix} \frac{\frac{x}{x^2 + y^2}}{\sqrt{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}} &  \frac{\frac{y}{x^2 + y^2}}{\sqrt{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}}  \\ -\frac{\frac{y}{x^2 + y^2}}{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}  & \frac{\frac{x}{x^2 + y^2}}{\left(\frac{x}{x^2 + y^2} \right)^2+\left( \frac{y}{x^2 + y^2}\right)^2}\end{bmatrix}  = \begin{bmatrix}\cos\theta & \sin \theta \\ - r\sin \theta & r\cos\theta \end{bmatrix}.$$ Had I used this matrix, I would have found the correct result for the Jacobian matrix of $\tilde{F}$ , which is the equation marked (!). Thus, (!) is actually correct . My fundamental misunderstanding was the assumption that, because of (2), the Jacobian determinant should be invariant for coordinate changes. This is not true; what follows from (2) is only that $$
\det D\tilde{F}|_{(r, \theta)}= \det D\phi^{-1}|_{F\circ\phi(r, \theta)}\det D\phi|_{(r, \theta)} \det DF|_{\phi(r, \theta)}.
$$ The first two factors in the right-hand side need not cancel , as I erroneously thought.","['multivariable-calculus', 'calculus', 'differential-geometry']"
3198776,Elegant way to evaluate $\int_0^\pi\frac{\sin\left(M+\frac{1}{2}\right)\theta\;\sin\left(N+\frac{1}{2}\right)\theta}{\sin^2(\theta/2)}d\theta$?,"Solve in terms of $M, N$ $$I(M. N) = \int_0^\pi\frac{\sin\left[\left(M + \frac{1}{2}\right)\theta\right]\sin\left[\left(N + \frac{1}{2}\right)\theta\right]}{\sin^2\left(\frac{\theta}{2}\right)}d\theta$$ where $M, N$ are non-negative integers. I've tried solving it by using trigonometric identities and brute force, but it gets extremely annoying. Is there a simpler way?","['trigonometry', 'definite-integrals']"
3198813,Using chain rule to calculate Fréchet derivative of $F(X) = \det(A^T (I - X) A)$,"Let $\mathbb{M}^n$ be the set of real $n \times n$ matrices, and let $A$ be a fixed real $n \times n$ matrix. Define the function $F: \mathbb{M}^n \rightarrow \mathbb{R}$ by $$ F(X) = \det( A^T (I-X) A) $$ What is the Fréchet derivative (total derivative) of $F$ at a matrix $X$ ? I am having difficulty working it out explicitly. I know that we can apply a chain rule, as $F = G \circ H$ where $H(X) = A^T(I-X)A$ and $G(Y) = \det(Y)$ . Yet, I am confused how to apply the chain rule here. I am looking for an expression for the Fréchet derivative and a derivation would be wonderful as well.","['frechet-derivative', 'matrix-calculus', 'derivatives']"
3198856,How to change this Runge-Kutta method implementation from first order ode solver to system of ODEs solver?,"I implemented 4-step Runge-Kutta method (k1..k4) ODE solver for a function $u'(x) = f(x,u(x))$ with initial condition $u(x_0) = u_0$ But it solves just ODEs of the first order. How could I change the code, so that it also takes ODEs of higher order? ${\bf u}'(x)={\bf f}(x,{\bf u}(x))$ with initial condition ${\bf u}(x_0)= {\bf u}_0$ ? All as vectors here not scalars. Here is the code for my solution, which takes first order ODEs: def euler(f, x, y, h):
    yn = y + h*f(x,y)
    xn = x + h
    return xn, yn

def rk4(f, x, y, h):
    k1 = h*f(x,y)
    k2 = h*f(x+(1/2)*h,y+(1/2)*k1) 
    k3 = h*f(x+(1/2)*h,y+(1/2)*k2)
    k4 = h*f(x+h,y+k3)
    yn = y + ((1/6)*(k1+(2*k2)+(2*k3)+k4))
    xn = x + h
    return xn, yn

def integrate(method, f, t0, y0, tend, h): # Depending on the 'method' (euler or rk4) this method solves the ode f)
    T = [t0]
    Y = [y0]
    t = t0
    y = y0 
    while (t < tend):
        h = min(h, tend-t)
        t, y = method(f, t, y, h)
        T.append(t)
        Y.append(y)
    return np.array(T), np.array(Y)

#def f1(t, y):   # this is an 1st order ODE
    #return (t*y) + t

# Usage example
xv, yv = integrate(method=rk4, f=f1, t0=0, y0=2, tend=1, h=0.5)
yv[-1] # Output: 6.15203857421875 Is there a solution, to make this work also for a given system of ODEs? (dynamically, without knowing the input ODE if it's 1st or higher order)for example this one: $$u''(x)+u(x)+u(x)^3 =0 ~~{\rm with}~~ u(0)=u'(0)=0 $$ or this one: $$u_1'(x) = 98u_1(x) + 198u_2(x) ~~{\rm and}~~ u_2'(x) = −99u_1(x) − 199u_2(x) \\~~{\rm with}~~ u_1(0) = 1 ~~{\rm and}~~ u_2(0) = 0$$ EDIT: changed the code to this now, to try if the 1st given example 2nd order ODE works: def f(t,y):
    return np.array([ y[1], -y[0]-(y[0])**3 ])

Y0 = np.array([0.0,0.0])

xv, yv = integrate(method=rk4, f=f, t0=0.0, y0=Y0, tend=100.0, h=h)
print(""[ %20.15f, %20.15f]""%(yv[-1,0], yv[-1,1]) )
print(yv) but I get an output consisting of 0 vectors like this: [    0.000000000000000,    0.000000000000000]
[[0. 0.]
 [0. 0.]
 [0. 0.]
 ...
 [0. 0.]
 [0. 0.]
 [0. 0.]]","['runge-kutta-methods', 'numerical-methods', 'python', 'ordinary-differential-equations']"
3198872,determine maximum remaining teams in a round robin tournament,"Let's say I have 10 teams competing in a round-robin tournament. Ties are not allowed, and only teams that have at least 7 wins can move on to a new tournament. How would I go about proving the maximum number of teams that could reach 7 wins and move on?","['graph-theory', 'discrete-mathematics']"
3198918,Are these square matrices always diagonalisable?,"When trying to solve a physics problem on decoupling a system of ODEs, I found myself needing to address the following problem: Let $A_n\in M_n(\mathbb R)$ be the matrix with all $1$ s above its main diagonal, all $-1$ s below its diagonal, and $0$ s everywhere else. Is $A_n$ always diagonalisable? If so, what is its diagonalisation (equivalently: what are its eigenvalues and corresponding eigenvectors)? For example, $$A_3=\begin{bmatrix}0&1&0\\-1&0&1\\0&-1&0\end{bmatrix},\quad A_5=\begin{bmatrix}0&1&0&0&0\\-1&0&1&0&0\\0&-1&0&1&0\\0&0&-1&0&1\\0&0&0&-1&0\end{bmatrix}.$$ Assuming my code is correct, Mathematica has been able to verify that $A_n$ is always diagonalisable up to $n=1000$ . If we use $\chi_n(t)\in\mathbb Z[t]$ to denote the characteristic polynomial of $A_n$ , a straightforward evaluation also shows that $$\chi_n(t)=-t\chi_{n-1}(t)+\chi_{n-2}(t)\tag{1}$$ for all $n\geq4$ . Furthermore, note that $A_n=-A_n^t$ so that, in the case where the dimension is even, $$\det(A_{2n}-\lambda I)=\det(A_{2n}^t-\lambda I)=\det(-A_{2n}-\lambda I)=\det(A_{2n}+\lambda I).$$ This implies that whenever $\lambda$ is an eigenvalue of $A_{2n}$ , so is $-\lambda$ . In other words, $\chi_{2n}(t)$ is always of the form $(t^2-\lambda _1^2)(t^2-\lambda_2^2)\dotsm(t^2-\lambda_n^2)$ for some $\lambda_i$ . And this is where I am stuck. In order for $A_n$ to be diagonalisable, we must have that all the eigenvalues are distinct, but trying to use the recurrence $(1)$ and strong induction, or trying to use the formula for the even case have not helped at all. It seems like the most probable line of attack would be to somehow show that $$\chi_{2n}'(t)=2t\sum_{k=1}^n\frac{\chi_{2n}(t)}{t^2-\lambda_k^2}$$ never shares a common zero with $\chi_{2n}$ (which would resolve the even case), though I don't see how to make this work. Note: I do not have any clue how to actually find the eigenvalues/eigenvectors even in the case where the $A_n$ are diagonalisable. As such even if someone cannot answer the second part of the question, but can prove that the $A_n$ are diagonalisable, I would appreciate that as an answer as well. Above I tried to look at the special case where the dimension is even, though of course the proof for all odd and even $n$ is more valuable. Even if this is not possible, for my purposes I just need an unbounded subset $S\subseteq\mathbb Z$ for which the conclusion is proven for $n\in S$ , so any such approach is welcome too. Thank you in advance!","['eigenvalues-eigenvectors', 'toeplitz-matrices', 'linear-algebra', 'diagonalization', 'tridiagonal-matrices']"
3198960,Does every subgroup of an abelian group have to be abelian?,"My original problem is to show that E/L is an abelian extension over L and L/F is an abelian extension over F, given that E/F is an abelian extension over F and that L is a normal extension of F such that $F\subseteq L \subseteq E$ . So far I have proved that E is a normal extension of F, E is a normal extension of L, and L is a normal extension of F. I know that to prove abelian extension I must also prove that Gal(E/L) is an abelian group. I have shown that Gal(E/L) $\subseteq$ Gal (E/F). In my mind it makes sense that I cannot lose commutativity therefore my subgroup must be Abelian too. How do I show this in a proof? Is it enough to show two elements in the subgroup must also exist in the larger group and that they must be commutative in the larger group? I feel like I know what needs to be done, just not how to phrase it.","['galois-theory', 'group-theory', 'abstract-algebra', 'abelian-groups']"
3200009,"Finding the global minimum of $\int_{0}^{1} \left( ax+b+\frac{1}{1+x^{2}} \right)^{2}\,dx$ having just the local minimum.","In order to calculate the values of $a$ and $b$ such we get the minimum possible for: $$\int_{0}^{1} \left( ax+b+\frac{1}{1+x^{2}} \right)^{2}\,dx$$ I got the help of @TheSimpliFire among others to get the respectively $a$ and $b$ here: Find $a$ and $b$ for which $\int_{0}^{1}( ax+b+\frac{1}{1+x^{2}} )^{2}\,dx$ takes its minimum possible value. Then, as we found were the partial derivatives of $a$ and $b$ are zero it is not hard to prove the founded $(a,b)$ satisfies that for: $$D(a,b) = f_{xx}'(a,b)f_{yy}'(a,b)-f_{xy}(a,b)^2 
$$ As for $p=(a,b)$ ; $$D(p) > 0 \land f_{xx}'(p) > 0 \implies \text{minimum}$$ i s satisfied, then $(a,b)$ are locally minimum. My question is how to verify $(a,b)$ is also the global minimum?? Thanks!!!","['integration', 'real-analysis', 'multivariable-calculus', 'calculus', 'derivatives']"
3200133,Laurent series of $f(z)=\frac{4z-z^2}{(z^2-4)(z+1)}$ in different annulus,"Given $f(z)=\dfrac{4z-z^2}{(z^2-4)(z+1)}$ I need to find the Laurent series in the annulus: $A_{1,2}(0),\;A_{2,\infty}(0),\;A_{0,1}(-1)$ I found the following partial fractions: $f(z)=\dfrac{-3}{(z+2)}+\dfrac{1}{3(z-2)}+\dfrac{5}{3(z+3)}$ , the power series of these fractions are: $\dfrac{-3}{(z+2)}=\displaystyle{\frac{-3}{2}\sum_{n=0}^\infty \left( \frac{-z}{2} \right)^n} $ $\dfrac{1}{3(z-2)}=\displaystyle{\frac{-1}{6}\sum_{n=0}^\infty \left( \frac{-z}{2} \right)^n} $ $\dfrac{5}{3(z+1)}=\displaystyle{\frac{5}{3}\sum_{n=0}^\infty \left( -z \right)^n} $ and the principle parts are: $\dfrac{-3}{(z+2)}=\displaystyle{\frac{-3}{2}\sum_{n=0}^\infty \left( \frac{1}{-2z} \right)^n} $ $\dfrac{1}{3(z-2)}=\displaystyle{\frac{-1}{6}\sum_{n=0}^\infty \left( \frac{1}{-2z} \right)^n} $ $\dfrac{5}{3(z+1)}=\displaystyle{\frac{5}{3}\sum_{n=0}^\infty \left( \frac{1}{-z} \right)^n} $ In the first annuli I take the principle part only of $\dfrac{5}{3(z+1)}$ , in the second annuli I take the principle part of all fraction. About the third one, I have $0<\vert z-1\vert<1$ , I denoted $w=z-1$ and then I took the power series for all fractions and simply switched the $w$ back to $z-1$ at the end. Is it the right way of doing it? I received $\displaystyle{\frac{-3}{2}\sum_{n=0}^\infty \left( \frac{1-z}{2} \right)^n - \frac{1}{6}\sum_{n=0}^\infty \left( \frac{1-z}{2} \right)^n + \frac{5}{3}\sum_{n=0}^\infty \left( 1-z \right)^n}$","['complex-analysis', 'power-series', 'laurent-series']"
3200151,What is the correct way to solve the equation: $x^4-x^3+x^2-x+1=0$,"Given the equation: $x^4-x^3+x^2-x+1=0$ we need to find both its real and complex roots. What is the easiest and correct method for solving the equation? Here is my approach, but it gives wrong result on the end. Since the equation is symmetric we can group the terms. $$x^4+1 - (x^3+x)+x^2=0 \text{ Divide everything by } x^2 \\(x^2+\frac{1}{x^2})-(x+\frac{1}{x})+1=0\\ \text{ Let } t = x + \frac{1}{x}, \text{ we can see that } x^2 + \frac{1}{x^2}  = t^2 - 2 \\ \text{back in our equation: } t^2 - 2 - t + 1 = 0 \\ t_{12} = \frac{1 \pm \sqrt{5}}{2} \\ \text{however if we go back in } x+\frac{1}{x} = t_{12} \text{ we don't get the correct result }$$ . As given in the textbook the solutions are: $x_{12}=\frac{1+\sqrt5 \pm \sqrt{10-2\sqrt5}}{4}, x_{34}=\frac{1-\sqrt5 \pm \sqrt{10-2\sqrt5}}{4}$ Can someone say if those are the correct solutions or not?","['algebra-precalculus', 'roots', 'polynomials']"
3200181,Is $Aut(K/\mathbb F_q)$ finite?,I do not know whether this result is true or not. I did not find any reference about it. Let $K/\mathbb F_q$ be a function field defined over a finite field. Is $Aut(K/\mathbb F_q)$ finite where $Aut(K/\mathbb F_q)$ denotes the set of automorphisms of $K$ that are identity on $\mathbb F_q$ . All I found is about function field defined over an algebraically closed field. So when the genus is $\ge2$ one deduces that $Aut(K/\mathbb F_q)$ is finite. The genus $0$ case is easy. But that says nothing about the genus $1$ case. Any reference or proof would be welcome for the genus $1$ case.,"['galois-theory', 'algebraic-geometry']"
3200256,Number of possible chess pairs where order and position matter,"Given 11 chess players and 5 distinct tables, in how many ways can we pair them to play (color does matter)? My problem is that I have found two approaches, both of which give different numbers, and I am not sure what is missing in one or double-counted in the other. The first approach is to just take any permutation of the $11$ players, as the tables are distinct there are 11 unique spots (one of them is not playing), so we can just place the players according to the permutation. This gives $11!=39916800$ possible games. The second approach is to first choose pairs, the first player has 10 choices, the next has 8, ... and then we multiply by $2^5$ to account for the colors of each pair, and finally multiply by the number of ways to seat them at tables (so multiply by $5!$ ), giving $$10\cdot8\cdot6\cdot4\cdot2\cdot2^5\cdot120=14745600$$ My intuition is that the first approach is correct, but then I am not sure which pairings are missing in the second one..",['discrete-mathematics']
3200258,Finding the greatest lower bound of a finite lattice,"For the lattice below. What is the Greatest Lower Bound of $a_2$ and $a_4$ (i.e., $a_2 \land a_4$ ) and $a_3$ and $a_4$ (i.e. $a_3 \land a_4$ )? I thought it was $a_2 \land a_4 = a_2$ and $a_3 \land a_4= a_3$ , but I got it wrong on the homework and I don't know what the right answer is. Can someone tell me what the Greatest Lower Bound is and why?","['graph-theory', 'lattice-orders', 'discrete-mathematics']"
3200298,Cardinality of a power set $2^{2^A}$,"The question I want to answer is : if $A$ is finite set of cardinality $2$ , find $2^{2^A}$ I know that $|2^{2^{A}}| = 2^{2^{|A|}}$ , but does this mean that there are 16 elements in the power set? Such as $$\{\varnothing\}, \{\{\varnothing\}\},\{\{0\}\}, \{\{1\}\},\{\{0,1\}\},\{\varnothing,\{0\}\}, \{\{\varnothing\},1\}, \{\varnothing,\{0,1\}\},\{\{0\},\{1\}\}, \{\{0\},\{0,1\}\}, \{\{1\},\{0,1\}\}\}, \{\varnothing,\{0\},\{1\}\}, \{\varnothing,\{0\},\{1\}\}, \{ \varnothing,\{1\},\{0,1\}\}, \{\{0\},\{1\},\{0,1\}\}, \{ \varnothing,\{0\},\{1\},\{0,1\}\}$$ Please correct me if I’m wrong, thank you so much!","['elementary-set-theory', 'discrete-mathematics']"
3200312,Derivative of matrix as a function of a vector w.r.t a vector,"I want to compute the derivative of the matrix $ diag(x)M $ with respect to $ x $ , where $ x \in \mathbb{C}^{n \times 1} $ and $ M \in \mathbb{C}^{n \times m} $ . This is how I have approached it, but I have not been successful. First, $$ Y = diag(x) $$ Then, $$ Z = Y M  $$ The differential of $ Z $ is $$
   dZ = dY M
$$ If I am not mistaken $ dY = (I_{n \times n} \otimes 1_{n \times 1}) dx $ . So $$
   dZ = (I_{n \times n} \otimes 1_{n \times 1}) (dx) M
$$ But the dimensions do not make much sense in this last expression. Would you please help me to find the right way?","['matrices', 'derivatives', 'calculus', 'linear-algebra']"
3200380,$A\otimes \mathbf{Q}\cong B\otimes \mathbf{Q}$ implies $A$ and $B$ are $\mathcal{C}$-isomorphic,"I am trying to solve exercise 211 on Davis-Kirk : Let $\mathcal{C}$ be the class of torsion abelian groups. Show that for any abelian groups $A,~B$ , $A\otimes \mathbf{Q}\cong B\otimes \mathbf{Q}$ implies $A$ and $B$ are $\mathcal{C}$ -isomorphic, that is, there exists an abelian group $C$ and group homomorphisms $f:C\to A$ , $g:C\to B$ such that $\ker f,\mathrm{coker} f$ , $\ker g,\mathrm{coker} g$ are abelian torsion groups. My attempt: Consider the exact sequence $$0\to\mathbf{Z}\to \mathbf{Q}\to \mathbf{Q}/\mathbf{Z} \to 0   $$ and the indcued exact sequence $$0\to \mathrm{Tor}(\mathbf{Q}/\mathbf{Z},A)\to A\to \mathbf{Q}\otimes A \to \mathbf{Q}/\mathbf{Z} \otimes A \to 0 $$ It is tempting to take the map $A\to \mathbf{Q}\otimes A$ whose kernel and cokernel are all torsion groups. But the definition ask us to find a map whose target is $A$ , not the source. Moreover, we cannot conclude $\mathrm{Tor}(\mathbf{Q}/\mathbf{Z},A)\cong \mathrm{Tor}(\mathbf{Q}/\mathbf{Z},B)$ . Besides from this ""canonical map"", I have no idea how to construct those two maps. Any hints and answer are welcome!","['algebraic-topology', 'homological-algebra', 'abstract-algebra', 'group-theory', 'commutative-algebra']"
3200411,"Proof of : "" Union of all R-equivalence classes on A is included in A"" based on "" R is a subset of A² "". ( Q° on Ayres, Problems Modern Algebra)","I'm studying Ayres Theory and Problems of Modern Algebra . In Chapter 2, Solved Problem 6, the question is asked to prove that "" An equivalence relation R on a set S effects a partition of S."" Ayers notes T(a) , T(b), T(c), etc.  the equivalence classes of ( respectively) the elements a, b, c , etc belonging to the set S. Aiming at proving first that the union of all equivalent classes is equal to S (which is the first condition for the set of equivalence classes to be a partition of S), Ayres only says : "" Since p belongs to [p] , it is clear that S is the union of all the distinct subsets T(a), T(b), T(c)."" My question is: knowing that to prove a set equality, a reciprocal inclusion has to be shown, does the argument given by Ayres prove inclusion in both directions ? May I develop Ayre's argument in the following way? (1) Proving that S is included in the union of the family of equivalence classes. Suppose p belongs to A. Since pRp ( given reflexivity) , p belongs to the R-equivalence class [p]. But the union of the set of R-equivalence classes on A is { x | x belongs to at least one R-equivalence class on A}. So, knowing that [p] is an R-equivalence class on A and that p belongs to [p], it follows that p belongs to at least one R-equivalence class on A , and, therefore, to the union of the set of R-equivalence classes on A. (2) Proving that the union of R-equivalence classes on A is included in A. Suppose that x belongs to the union of R-equivalence classes. It means that x belongs to at least one equivalence class ( by the definition of "" union"") , say the class [a]. So there exists an element a belonging to A such that : xRa , that is such that (x,a) belongs to R. But R ( being a relation on A) is a subset of A². So (x,a) belongs to A² . But A² is the set of all pairs such that both their first and second element belong to A. So the first element of (x,a) belongs to A. So x belongs to A. Since x was arbitrary, we can conclude that for all x, if x belongs to the union of the set of R-equivalence classes on A, then x belongs to A; in other words, that the union of the set of R-equiv classes is included in A.","['elementary-set-theory', 'abstract-algebra']"
3200458,Proving or disproving $12\mid x$ given $x^2+2\mid y^2-2$,"Let $x$ , $y$ be positive integers such that $x^2+2\mid y^2-2$ . Prove or disprove that $12\mid x$ . This conclusion comes when I was dealing with another problem, and I feel it is right because when $x=12$ and $y=32$ , $$\frac{32^2-2}{12^2+2}=7.$$ Let me explain the origin of this example  when $x=12$ can consider $\sqrt{146k+2}\in \Bbb Z$ , I try $k=1,2,\cdots$ ,when $k=7$ is such it but when $x=24$ , I can't find $y\le 48$ no example such it,because I want to find $k$ such $\sqrt{478k+2}\in \Bbb Z$","['number-theory', 'quadratic-residues', 'divisibility']"
3200529,Finding all continuous function which maps any sequence in geometric progression to another geometric progression,Find all continuous functions $f:\mathbb{R}\rightarrow\mathbb{R}$ such that for any geometric progression $x_n$ the sequence $f(x_n)$ is also a geometric progression. I tried first by taking constant sequences. But it does not helps much.,"['continuity', 'functions', 'geometric-progressions']"
3200569,"Geometrically explained, why do Linear Transformations Take a circle to an ellipse","Short Version: How can it be geometrically shown that non-singular 2D linear transformations take circles to ellipses? (Also, its probably important to state I'd prefer an explanation that doesn't use SVD, as I don't really understand it yet...although I see it everywhere) Long Version: Let's use the definition of an ellipse as being a circle stretched in two perpendicular directions. The two directions its stretched in will correspond to the two axes of the ellipse. We begin by defining the circle as the endpoints of all possible 2D unit vectors. The ellipse (or at least I""m TOLD it's an ellipse) is the shape drawn by the endpoints of all these vectors after they have all been transformed in the same way (aka multiplied by the same nonsingular matrix $A$ ) . For linear transformations represented by diagonal matrices, it's easy to see. We're just stretching the circle in the X and Y directions. For linear transformations represented by symmetric matrices...its a little harder, but I can see the transformation because the eigenvectors of the symmetric matrix are perpendicular, and if we change to a basis where those eigenvectors are the basis vectors, the transformation can be represented by a diagonal matrix (as for WHY symmetric matrices can be decomposed this way I don't yet really understand - but for the purpose of this question I'm just accepting that they can; I'm accepting that the eigenvectors of symmetric matrices are perpendicular to one another) . So, just like diagonal matrices, symmetric matrices also correspond to stretching a unit circle in perpendicular directions - but unless the symmetric matrix is diagonal, these are perpendicular directions different from the X and Y directions. Buuut...what about for nonsymmetric matrices? Thanks! EDIT: I've now learned of the polar decomposition of any real matrix, and that provides a beautiful explanation for why any real matrix takes a circle to an ellipse! $A=QS$ , where $Q$ is an orthogonal matrix (rotation) and $S$ is a symmetric matrix (stretching in the direction of the eigenvectors). The symmetric matrix will definitely correspond to making an ellipse (since it scales in orthogonal directions, although perhaps not our regular $x$ and $y$ directions) and all the orthonormal matrix will do is rotate this ellipse. However, all the explanations I've seen so far that PROVE that polar decompositions of real matrices are always possible use an algebraic explanation instead of a geometric one ...so they aren't really what I'm looking for. Thanks again!","['intuition', 'linear-algebra', 'linear-transformations']"
3200595,Hodge theory: $\Delta \alpha = 0$ iff $d\alpha = d^* \alpha = 0$ on a noncompact manifold?,"Let $M$ be a Riemannian manifold (connected, oriented). One can define the co-differential $d^* : \Omega^k(M, \mathbb{R}) \to \Omega^{k-1}(M, \mathbb{R})$ even if $M$ is not compact (for example use the definition with the Hodge star, there is also a definition with the covariant derivative). $d$ and $d^*$ are formally self-adjoint, in the sense that $\langle d \alpha, \beta \rangle_{L^2} = \langle \alpha, d^* \beta \rangle_{L^2}$ if $\alpha$ or $\beta$ has compact support. One can then define the Hodge Laplacian $\Delta = d d^* + d^* d$ . If $M$ is compact, then \begin{equation}
\Delta \alpha = 0\quad \Leftrightarrow \quad d\alpha = 0 \text{ and } d^* \alpha = 0~.
\end{equation} What if $M$ is not assumed compact? The direction $\Leftarrow$ still obviously holds. What about $\Rightarrow$ ? Thoughts: In the compact case, it is easy to show $\Rightarrow$ : one just writes $\langle \Delta \alpha, \alpha \rangle_{L^2} = \langle d \alpha, d\alpha \rangle_{L^2} + \langle d^* \alpha, d^* \alpha \rangle_{L^2}$ and quickly concludes. I still the argument might still work in the noncompact case by evaluating $\langle \Delta \alpha, \varphi \alpha \rangle_{L^2} $ for a wisely chosen bump function $\varphi$ , but I can't quite make it work. Otherwise maybe it can be proven by direct computation somehow, after all $\Delta \alpha$ , $d \alpha$ , and $d^* \alpha$ are all local quantities.","['hodge-theory', 'riemannian-geometry', 'laplacian', 'differential-forms', 'differential-geometry']"
3200620,Proving an identity of quadratic form,"Let $n$ be even and $x_1,x_2,⋯,x_n$ be reals. Show that $$\sum_{1\le i<j\le n}\min(|i-j|,n-|i-j|)x_ix_j\\=\sum_{j=1}^{\frac n2}(x_j+x_{j+1}+⋯+x_{j+\frac n2-1})(x_{j+\frac n2}+x_{j+\frac n2+1}+⋯+x_{j+n-1}),$$ where $x_{n+k}=x_k$ for $k=1,2,\cdots,n-1$ . This identity looks interesting, which comes from one of my students. I think is right, but I am unable to prove it. When $n=2$ , $$\text{LHS}=\sum_{1\le i<j\le 2}\min(|2-1|,2-|2-1|)x_{i}x_{j}=x_{1}x_{2},\\\text{RHS}=x_{1}x_{2}.$$ When $n=4$ , $$\text{LHS}=x_{1}x_{2}+2x_{1}x_{3}+x_{1}x_{4}+x_{2}x_{3}+2x_{2}x_{4}+x_{3}x_{4},\\\text{RHS}=\sum_{j=1}^{2}(x_{j}+x_{j+1})(x_{j+2}+x_{j+3})=(x_{1}+x_{2})(x_{3}+x_{4})+(x_{2}+x_{3})(x_{4}+x_{1})=\text{LHS}.$$","['algebra-precalculus', 'quadratic-forms']"
3200626,Probability of even number of events occuring,"As part of trying to fresh up on my basic probability theory I came along Ex. 1.46 in Grimmet's probability book with the second part troubling me. If $A_1$ , $A_2$ , . . . , $A_m$ are independent and $P(A_i) = p$ for $i = 1, 2, . . . , m$ , find the probability that (a) none of the $A_i$ occur, (b) an even number of the $A_i$ occur. a) none of $A_i$ Just computing the probability of the event $E=\Omega \ \backslash ( \cup_i A_i ) $ (i.e. 'all events without the union of all $A_i$ ') with $q=1-p$ and using the event's independence: $$P \bigg ( \bigg [\bigcup_i^m A_i \bigg]^c \bigg) = P \bigg ( \bigg [\bigcap_i^m A_i^c \bigg ] \bigg ) = P(A^c)^m = q^m = (1-p)^m$$ which agrees with the answer in the back of the book. b) even number of $A_i$ I expected this one to be as easy as part (a), but apparently I am overlooking something. My idea was to build sets that consist of an even number of sets without the rest and then computing and summing up the probability of them. E.g. for $m=4$ this would be: $$ 
E_1 = A_1 \cap A_2 \backslash  ( A_3 \cup A_4)  \\
E_2 = A_1 \cap A_3 \backslash  ( A_2 \cup A_4)  \\
 ... \\
E_6 = A_3 \cap A_4 \backslash  ( A_1 \cup A_2)  \\
E_7 = (A_1 \cap A_2 \cap A_3 \cap A_4)
$$ where there are 6 combinations to get two events out of four, $6=$ $4 \choose 2$ . The probability for such an event in detail is $$
\begin{align}
P(E_1) & =P(A_1 \cap A_2 \backslash  ( A_3 \cup A_4))\\\
&= P(A_1 \cap A_2) - P((A_1 \cap A_2) \cap (A_3 \cup A_4))\\
&= P(A_1) P(A_2) - P(A_1 \cap A_2) P(A_3 \cup A_4)\\
&= p^2 - p^2 \ P(A_3 \cup A_4)\\
&= p^2 - p^2 \ ( P(A_3)+P(A_4)-P(A_3 \cap A_4))\\
&= p^2 - p^2 \ ( p+p-p^2) = p^2 ( 1-2p+p^2)\\
&= p^2 (1-p)^2 \\
&= p^2 q^2
\end{align}
$$ which is true for $E_1$ to $E_6$ , where we have each two events happening ( $p^2$ ) while two other events are not happening (the $q^2$ ), and $E_7$ just giving $P(E_7)=p^4$ because four events are happening. So for the case of m=4 we should get that the probability for an even number of events to occur should be $$
P(\text{even number})=\sum_i P(E_i)=P(E_1)+ ... +P(E_7) = 6 p^2 q^2 + p^4 $$ Apart from the back of the book giving a different answer anyway, namely $$
P(\text{even number})_{book}=\frac{1}{2} [1+(q-p)^m],$$ I would not even know how to generalize my example for $m=4$ to any m. Studying the book's answer and expanding the bracket term gives $$
(q-p)^4=p^4 - 4 p^3 q + 6 p^2 q^2 - 4 p q^3 + q^4
$$ in which two parts of the sum agree with my result. It looks like the event of 4 times happening 'not $A_i$ ' ( $q^4$ ) is also counted as an even event of the $A_i$ 's, while the uneven events are being subtracted ( $p^3q$ and $pq^3$ ). But even then, I do not see how to get there or where the $\frac{1}{2}$ comes from. Does anyone know where I am going wrong?
Thanks!","['independence', 'probability-theory', 'probability']"
3200628,"Prove that $g(x):=\int_0^1f(x,y)dy$ is Borel measurable.","Let $f: [0,1]\times [0,1]\to\mathbb R$ satisfy: (i) for each $x\in [0,1]$ , the function $y\mapsto f(x,y)$ is Riemann integrable on $[0,1]$ ; and (ii) for each $y\in [0,1]$ , the function $x\mapsto f(x,y)$ is Borel measurable. Show that the function $g(x):=\int_0^1f(x,y)dy$ is Borel measurable. (Note: In general, $f$ will not be Borel measurable as a function on $[0,1]\times [0,1].$ ) My attempt: By the definition of Borel measurable, we need to show that for every open set $U$ in $\mathbb R^1$ , $$g^{-1}(U)=\{x\in [0,1]:g(x)=\int_0^1f(x,y)dy\in U\}$$ is a Borel set in $[0,1]$ . How can we use (ii)? $f(x,y)$ is Borel measurable with regard to $x$ for every fixed $y$ , but now $y$ is a variable in the integral and we are no longer to directly quote the measurablity of $f(x,y)$ . How to move on?","['measure-theory', 'measurable-functions', 'real-analysis']"
3200678,Find closed form for quadruple integral,"I am trying to find a closed form of the following integral $$
\int _0^{\infty }\int _0^x\int _0^y\int _0^z \exp \left( -\frac{a x^2}{2}-\frac{b y^2}{2}-\frac{c z^2}{2}-\frac{d w^2}{2} \right) \,\mathrm{d}w\,\mathrm{d}z\,\mathrm{d}y\,\mathrm{d}x
$$ where $a,b,c,d>0$ are some constants. My idea is to change variable to to polar system by letting $$
x=\frac{r \cos (\alpha ) \cos (\beta ) \cos (\theta )}{\sqrt{a}}, \quad
y=\frac{r \cos (\alpha ) \cos (\beta ) \sin (\theta )}{\sqrt{a}}
$$ $$
z=\frac{r \sin (\alpha ) \cos (\beta )}{\sqrt{c}}, \quad w=\frac{r \sin (\beta )}{\sqrt{d}}
$$ This reduces the original integral into $$
\int_0^{\tan ^{-1}\left(\sqrt{\frac{b}{a}}\right)} \frac{\sin (\theta ) \tan ^{-1}\left(\sin (\theta ) \sqrt{\frac{d}{b+c \sin ^2(\theta )}}\right)}{\sqrt{a b d \left(b+c \sin ^2(\theta )\right)}} \, d\theta
$$ But then I get stuck here. PS: I am interested in this because I found that $$
\int _0^{\infty }\int _0^x \exp \left(-\frac{a x^2}{2}-\frac{b y^2}{2}\right)\,\mathrm{d}y\,\mathrm{d}x
=
\frac{\tan ^{-1}\left(\sqrt{\frac{b}{a}}\right)}{\sqrt{a b}}
$$ and $$
\int _0^{\infty }\int _0^x\int _0^y \exp \left( -\frac{a x^2}{2}-\frac{b y^2}{2}-\frac{c z^2}{2} \right) \,\mathrm{d}z\,\mathrm{d}y\,\mathrm{d}x
=
\frac{\sqrt{\pi/2 } }{\sqrt{a b c}}
\left(\tan ^{-1}\left(\sqrt{\frac{c}{b}}\right)-\tan ^{-1}\left(\sqrt{\frac{a c}{b (a+b+c)}}\right)\right)
$$ So I am trying to generalize this. Maybe this is already known?","['integration', 'multivariable-calculus', 'multiple-integral', 'definite-integrals']"
3200697,"Transform coordinate system for the gradient of a function at a specific x, y, z value.","I have the gradient of a function $f(x,y,z)$ at a specific value of $x, y,$ and $z$ in the vector form: $$
\nabla f(x,y,z)\Bigr|_{\substack{x=x_1\\y=y_1\\z=z_1}} =\begin{pmatrix}
\frac{\partial{f}}{\partial{x}}\\
\frac{\partial{f}}{\partial{y}}\\
\frac{\partial{f}}{\partial{z}}
\end{pmatrix}_{\substack{x=x_1\\y=y_1\\z=z_1}}
$$ where I know the numerical value of each derivative but I don't know the explicit form of the function $f$ . I need to transform this gradient vector into a new coordinate system of three vectors, $\vec{p} ,\vec{q},$ and $\vec{r}$ , defined by: $$
\vec{p}=\begin{pmatrix}
x_p\\
y_p\\
z_p
\end{pmatrix}; \,\,
\vec{q}=\begin{pmatrix}
x_q\\
y_q\\
z_q
\end{pmatrix}; \,\,
\vec{r}=\begin{pmatrix}
x_r\\
y_r\\
z_r
\end{pmatrix}
$$ So I need to transform the vector $\nabla \vec{f}(x,y,z)$ to $\nabla \vec{f}(p,q,r)$ (or the equivalent unit vectors), for that specific value of variables. Something like: $$
\nabla f(p,q,r)\Bigr|_{\substack{x=x_1\\y=y_1\\z=z_1}} =\begin{pmatrix}
\frac{\partial{f}}{\partial{p}}\\
\frac{\partial{f}}{\partial{q}}\\
\frac{\partial{f}}{\partial{r}}
\end{pmatrix}_{\substack{x=x_1\\y=y_1\\z=z_1}}
$$ For any regular vector I know I can construct a transformation matrix from vectors $\vec{p} ,\vec{q},$ and $\vec{r}$ , and multiply the original vector. But given that vector $\nabla \vec{f}(x,y,z)$ comes from a gradient, I'm not sure if there are any special precautions I have to take to account for the chain rule. Is it as simple as a regular coordinate transformation? PS: Along similar lines, how can I transform the hessian of $f$ in $x,y,z$ coordinates ( $f_{xy}$ : matrix of second derivatives), into a hessian of $f$ in $p,q,r$ coordinates, when I only have the hessian at a specific value of $x,y,$ and $z$ ? Thanks!","['multivariable-calculus', 'vectors']"
3200710,Determining the ideals of a quotient ring,"Given an ideal $I = \langle x^3 - x\rangle \subseteq \Bbb{R}[x]$ , determine the ideals in the quotient ring $\Bbb{R}[x]/I$ . I understand that the quotient ring is of the form $k[x_1...x_n]/I$ where I is an ideal in $k[x_1...x_n]$ . I also understand that the ideals of the quotient ring are in one-to-one correspondence with the ideals of $k[x_1...x_n]$ containing $I$ . However, I'm really confused on how to actually go about finding the ideals of the quotient ring given a specific ideal. Any hints or suggestions would be appreciated, thank you.","['ring-theory', 'abstract-algebra', 'polynomials', 'quotient-spaces', 'ideals']"
3200813,Why is this method for solving linear equations systems using determinants works?,"I'm new here and english is not my native language but I'll try to explain my question the best I can.
While studying methods for solving systems of linear equations I came across a method so called ""Gauss algorithm"" for which I doubt if it is it's real name or not, but I can't understand WHY it works.
I would really appreciate a proper explanation or maybe a hint for what is happening inside the method process that accounts for the logic behind it. The process in question is the following and involves determinants: Suppose we have the following system of linear equations, for instance \begin{cases}
\ x+2y-z=-5 \\2x-1y+2z=8 \\3x+3y+4z=5 \
\end{cases} Then the result of appliying the method continues as follow \begin{array}{ccc|c}
x & y & z & i.t. \\
\hline
1 & 2 & -1 & -5 \\
2 & -1 & 2 & 8 \\
3 & 3 & 4 & 5 \\
\hline
 & -5 & 4 & 18 \\
 & -3 & 7 & 20 \\
\hline
 &  & -23 & -46 \\
\end{array} Where for example the coefficients $-5$ and $4$ , and the independent term $18$ , of the first row of the reduced system has been computed with the follow determinants: \begin{equation}
 \begin{vmatrix}
    1 & 2  \\
    2 & -1  \\
    \end{vmatrix}=-5
\hspace{1cm}
\begin{vmatrix}
    1 & -1  \\
    2 & 2  \\
    \end{vmatrix}=4
\hspace{1cm}
\begin{vmatrix}
    1 & -5  \\
    2 & 8  \\
    \end{vmatrix}=18
\end{equation} Where those determinants were computed using the first and the second row of the system of equations matrix. Then the other coefficients -3 and 7 and the independant term 20 were found by computing the respective determinants as above but using the first and third row of the system of equations matrix. Appliying this method and if the system is compatible, then as above you can see that \begin{equation} -23z = -46\\
z=2\end{equation} and then by substitution in the subsequent former equations the other variables can be found. Well that's the method I was studying and will appreciate to know more about it, it's a pity I don't know the name of it but maybe you can help. Thanks in advance!","['systems-of-equations', 'linear-algebra']"
3200864,Expected value of a binary decimal generated by coin flips,"A coin is flipped infinitely many times. Heads is $1$ , tails is $0$ . The string created by the heads and tails is turned into its respective string of ones and zeros. If I write $1.$ before the binary string it creates a decimal in base 2.
I've been trying to figure out the expected value of the aforementioned decimal in base 10. So far, I have that the number generated, $N$ , can be represented by $$N=1+\sum_{n=1}^{\infty}\left(\frac{X(\omega)}{2^n}\right)$$ where $X(\omega)$ is a random variable such that $$X(\omega)= 
\begin{cases}
1, \text{if $\omega = heads$} \\
0, \text{if $\omega = tails$} \\
\end{cases}$$ but I have no idea where to go from here. Am I going down the wrong path, or is this line of reasoning correct? How can I find the expected value that N will be?","['expected-value', 'probability-theory', 'probability']"
