question_id,title,body,tags
751734,Boundary of Ball of radius R has zero measure,"If $\mu$ is a Radon measure on $\mathbb{R}^n$ and $B_r$ is a closed ball of radius $r$. Why is $\mu(\partial B_r) = 0$? Or how can I prove that there is at least one $r_0 > 0$ such that
$\mu(\partial B_{r_0}) = 0$?",['measure-theory']
751735,Convolution of a continuous function and uniform continuity,"Suppose $f$ is a continuous function, and let the convolution $f_n(x) := f \star \varphi_n(x)$ where $\varphi_n$ are smooth test functions. We know $f_n \in C^\infty$. We know that if $f$ is uniformly continuous, $f_n \to f$ uniformly. We only have continuity of $f$... how can I show that $f_n$ (or there is a sequence $f_n$) that converges uniformly to $f$ on compact subsets? We can consider the convolution of $f|_K$ onto a compact subset $K \subset \mathbb{R}$, but this gives us a different sequence $f_n(K)$ for each $K$. I want a single sequence that converges uniformly on the compact subsets.","['convolution', 'functional-analysis', 'functions']"
751739,Hausdorff dimension of $\mathrm{R}^d$,"I assume the Hausdorff Dimension of $\mathrm{R}^d$ is $d$ . To prove this I guess one has to prove these two statements: the $\alpha$ -dimensional Hausdorff measure of $\mathrm{R}^d$ is $0$ for $\alpha \gt d$ . the $\alpha$ -dimensional Hausdorff measure of $\mathrm{R}^d$ is $\infty$ for $\alpha \lt d$ . Unfortunately I am unable to prove anything here. I guess the trick is to use a clever partition of $\mathrm{R}^d$ , but I don't come up with one. How can I prove this statement?","['dimension-theory-analysis', 'measure-theory']"
751759,Action of $SO_n$ on $\mathbb{S}^{n-1}$ induces fibre bundle.,"Real compact Lie group $SO_n$ acts smoothly and transitively on $\mathbb{S}^{n-1} \subseteq \mathbb{R}^n$ with obvious action. Isotropy subgroup of each point in $\mathbb{S}^{n-1}$ is isomoprhic to $SO_{n-1}$ . Let $\{e_1,\ldots,e_n\}$ be standard orthonormal base for $\mathbb{R}^n$ . I want to see that map $$p \colon SO_n \to \mathbb{S}^{n-1}$$ $$A \mapsto A e_n$$ is fibre bundle. Map $p$ is obviously continuous surjection. I suppose one could take open cover $\{U^+,U^-\}$ of $\mathbb{S}^{n-1}$ , where $U^\pm := \mathbb{S}^{n-1} \setminus \{\pm e_n\}$ . Now, I should construct two homeomorphisms $\alpha_1$ , $\alpha_2$ , such that the following diagrams commute: (source: presheaf.com ) (source: presheaf.com ) I cannot make this happen. How to define $\alpha_1$ and $\alpha_2$ ? Or should I use some other cover? Any help would be appreciated. The reason I want this is that I would like to use homotopy lifting property (or exact sequence of homotopy groups) to calculate fundamental groups of $SO_n$ by induction.","['differential-geometry', 'fiber-bundles', 'lie-groups', 'group-actions', 'fibration']"
751770,Roots of Taylor's series.,"Show that there is exactly one value of x which satisfies the equation $$2\cos^2 (x^3+x)=2^x+2^{-x} $$ I solved this using Taylor's series: $$2^x+2^{-x}=2\{1+\frac {x^2 \{\ln2\}^2}{2!}+\frac {x^4 \{\ln2\}^4}{4!}....\} $$ $$\cos^2 (x^3+x)=1+\cos (2x^3+2x)=2-\frac{{2x^3+2x}^2}{2!}.... $$ Equating the two gives 
$$ x^2\{{\ln2}^2+.....\}$$
$$\implies x=0 $$ But I can't seem to be able to find an argument justifying the existence if only one root. If I said that as the other term is an infinite series hence it's roots are undefined,  would that be correct?",['functions']
751771,Proof that $\det(A)=\det(A^T)$ using permutations.,"I'm reading a proof for the identity $\det(A) = \det(A^T)$ and I'm trying to udnerstand why the following rows are equivalent: $$\eqalign{
  & \det ({A}) = \sum\limits_{\pi  \in {S_n}} {{\mathop{\rm sgn}} (\pi ) \cdot {a_{\pi (1),1}}} ...{a_{\pi (n),n}}  \cr 
  & \det ({A^T}) = \sum\limits_{\pi  \in {S_n}} {{\mathop{\rm sgn}} ({\pi ^{ - 1}}) \cdot {a_{1,{\pi ^{ - 1}}(1)}}} ...{a_{n,{\pi ^{ - 1}}(n)}} \cr} $$ Intuitively, I can understand that each term appears in both of the summations. How to understand it algebraically?","['permutations', 'linear-algebra', 'transpose', 'determinant']"
751781,How to parametrize a curve by its arc length,"I am reading on Wikipedia that ''...Any regular curve may be parametrized by the arc length (the natural parametrization) and...'' I know that if $a(t) = (x(t),y(t),z(t))$ is a curve (say, smooth) then it is regular iff for all $t$: $a' (t) \neq 0$. I also know the definition of arc length: The arc length of a curve $a$ between $t_0$ and $t$ is defined as $$ l = \int_{t_0}^t |a'(t)|dt$$ But what is the parametrization of $a$ using its arc lenght?","['differential-geometry', 'real-analysis']"
751799,"The zeros of $2\,\xi(s)-1$. Is there anything known about the curves they lie on?","Take the well known integral: $$\displaystyle \pi^\frac{-s}{2}\,\Gamma\left(\frac{s}{2}\right)\, \zeta(s) =\int_1^{\infty} \left({x}^{\frac{s}{2}-1} + {x}^{\frac{-s}{2}-\frac12}\right)\,\psi(x)\, \text{d}x + \frac{1}{s\,(s-1)}$$ with $\displaystyle \psi(x)=\sum_{n=1}^{\infty}e^{-\pi\,n^2\,x}$. Make a simple change and rewrite it into: $$\displaystyle s\,(s-1)\,\pi^\frac{-s}{2}\,\Gamma\left(\frac{s}{2}\right)\, \zeta(s) - 1 =s\,(s-1)\,\int_1^{\infty} \left({x}^{\frac{s}{2}-1} + {x}^{\frac{-s}{2}-\frac12}\right)\,\psi(x)\, \text{d}x$$ With $\xi(s)=$ Riemann xi-function, the left hand side becomes $2\,\xi(s)-1$ and below  are its zeros: The zeros are symmetrical around the line $\Re(s)=\frac12$, however based on the irregular patterns in the imaginary parts of the non-trivial zeros $\rho$, I was actually surprised by their apparent regularity. I therefore have two questions: 1) Can anything be derived about the ""tangent shaped"" curves on which these zeros seem to reside? 2) The function $2\,\xi(s)-1$ remains entire, so could a Hadamard product of all its zeros ($\mu$) exist for it? If so, this would then yield something of the form: $$\displaystyle \prod_{n=1}^{\infty} \Bigl(1-\frac{s}{\rho_n}\Bigr)\Bigl(1-\frac{s}{{1-\rho_n}}\Bigr) = \Bigl(z^m \, e^{g(z)}\,\prod_{n=1}^{\infty}\Bigl(1-\frac{s}{\mu_n}\Bigr)\Bigl(1-\frac{s}{{1-\mu_n}}\Bigr)\Bigr) + 1$$","['riemann-zeta', 'number-theory']"
751806,Give an example of non-normal subspace of a normal space.,"We know that a closed subspace of normal space is normal. My question was: why should other subspaces not work and then I came up with a counterexample.
It is peculiar that any subspace of regular space is regular.","['general-topology', 'separation-axioms', 'examples-counterexamples']"
751826,$\frac{5\pi^3}{154}=\int_{\pi/6}^{\pi/2}\bigg[\Re\big(\text{Li}_2(4\sin^2\theta)\big) +\text{Li}_2\bigg(\frac{1}{4\sin^2\theta}\bigg) \bigg]d\theta$,"I am trying to prove  $$
\int_{\pi/6}^{\pi/2}\bigg[\Re\big(\text{Li}_2(4\sin^2\theta)\big) +\text{Li}_2\bigg(\frac{1}{4\sin^2\theta}\bigg) \bigg]d\theta=\frac{5\pi^3}{54}.
$$
Clearly, this closed form result is very nice.  I am very rusty with working with dilogarithm integrals and am not sure where to start this.  Some information that may help is, note the dilogarithm function is given by 
$$
Li_2(z)=\sum_{n=1}^\infty \frac{z^n}{n^2}.
$$
This integral is related to the dilogarithm representation 
$$
\frac{1}{\pi}\int_0^\pi \Re\big(Li_2(4 \sin^2 \theta)\big)d\theta=\zeta(2)=\frac{\pi^2}{6}
$$
which is strongly related to the log-sine integrals I have been posting.","['special-functions', 'integration', 'definite-integrals', 'real-analysis', 'contour-integration']"
751852,LogSine Integrals $\int_0^{\pi/3}\theta \ln^2\big(2\sin\frac{\theta}{2}\big)d\theta$.,"Hi this will soon end my posts on Log Sine integrals, and we can progress into other classes of integrals. Given the generalized log sine integral , $$\rm{Ls}_n^{(k)}(\sigma) = -\int_0^{\sigma}\theta^k \Big(\ln\big(2\sin\tfrac{\theta}{2}\big)\Big)^{n-1-k}\,d\theta$$ The log sine integral I am trying to calculate is given by $k=1$ , $n=4$ , $$
\rm{Ls}_4^{(1)}\big(\tfrac{\pi}{3}\big)=-\int_0^{\pi/3}\theta\, \ln^2\big(2\sin\tfrac{\theta}{2}\big)\,d\theta=-\,\frac{17\pi^4}{6480}.
$$ What a beautiful closed form!
This integral is known as one of many ""Log Sine integrals at $\pi/3$ .""  It also seems that many people have not been responding to these log sine integral posts.  This is quite puzzling as these integrals are fundamental to mathematical analysis and are quite old.. If anybody wants literature on these integrals, please let me know. I am not quite sure how to evaluate this.  Perhaps  a change of variables or logarithmic expansion may work.  Thanks for your help. EDIT:  This is also of interest when discussing Mahler measures. http://en.wikipedia.org/wiki/Mahler_measure","['integration', 'definite-integrals', 'real-analysis', 'analysis', 'contour-integration']"
751871,bessel function maximizer,"I try to find global maximum for $ \frac{J_2(x)}{x^2} $  I suspect it happens at x=0 ( plotting the graph) where the value of the function is $ \frac{1}{8} $ 
I know local maximizers are at   zeros of $ J_3(x) $ since $ \frac{d}{dx}\frac{J_2(x)}{x^2}=-\frac{J_3(x)}{x^2} $. 
My problem is to evaluate $ \frac{J_2(x)}{x^2} $  at the zeros of  $ J_3(x) $ or compare it to 
$ \frac{1}{8} $ .
thanks.","['special-functions', 'analysis']"
751886,"If a nonempty set of real numbers is open and closed, is it $\mathbb{R}$? Why/Why not?","In other words, are $\emptyset$ and $\mathbb{R}$ the only open and closed sets in $\mathbb{R}$? Why/Why not? I tried by assuming a set is equal to its interior points and contains its limit points. A bounded set will not do since stuff like $[1,4]$ and $\{5\}$ will not work, though that is not really proof. Help please? Anyway, it must then be unbounded. If $a$ is a real number then $(a,\infty)$, $(-\infty,a)$, $[a,\infty)$ and $(-\infty,a]$ don't seem to cut it so it must be $\mathbb{R}$.","['general-topology', 'connectedness', 'real-analysis', 'analysis']"
751894,Calculate the summation of double continued fractions,"A few month ago, my brother had given me this question:
\begin{equation}
  \cfrac{1}{2
          + \cfrac{1}{3
          + \cfrac{1}{4 + \cfrac{1}{\cdots+\frac{1}{2005}} } } }+\cfrac{1}{1
          + \cfrac{1}{2
          + \cfrac{1}{3 + \cfrac{1}{\cdots+\frac{1}{2005}} } } }
\end{equation}
He said to me, ""this is called continued fraction and the answer equals 1."" Since then, I learned from many websites about CF but I failed to prove the summation of those CFs equals 1. I only see this pattern:
\begin{equation}
  \cfrac{1}{2
          + \frac{1}{3}
}=\frac{\color{green}3}{\color{red}7}\text{ and }\cfrac{1}{1
          + \cfrac{1}{2+\cfrac{1}{3}}
}=\frac{\color{red}7}{10}=\frac{\color{red}7}{\color{green}3+\color{red}7}\text{, etc.}
\end{equation}
Let's say the first CF equals $\frac{a}{b}$, then the second CF will equal $\frac{b}{a+b}$. Hence
\begin{equation}
\frac{a}{b}+\frac{b}{a+b}=\frac{a^2+ab+b^2}{ab+b^2}=1+\frac{a^2}{ab+b^2}>1
\end{equation}
Is this correct? Did my big brother trick me all this time? Someone here please help me, preferably someone with a doctorate degree in math or a college math professor so I  can be sure that I'm right and can argue with him. I want to win this time because he always shows off his smartness to me. BTW, I'm just an 8th grade student, so please be nice to me. Thank you. :)","['arithmetic', 'algebra-precalculus']"
751895,Can we find some constraint about order of $xy$ in a group $G$?,"Can we determine order of $xy$ in $G$ if we know order of $x$ and $y$ ? I know that answer is yes for abelian groups and I guess the answer is no for nonabelian case.
That is why I am looking for useful relation about $|xy|$. I have found one which says that $|xy|=|yx|$ for any group. Any useful relation about $|xy|$ would be appreciated, thanks.","['group-theory', 'abstract-algebra']"
751908,Proof that $ p $ | ${ p^n \choose k } $ for any prime $p$ and $ k < p^n$,"I know how to prove the fact that $ p$ | ${ p \choose k } $ (when writing it as a fraction, $p$ cannot be divided by any of the $1\times2\times...\times k$ or $1\times2\times...\times(p-k)$ because $p$ is prime). When I try to apply the same rationale for $ p$ | ${ p^n \choose k } $ I get stumped because $p^n$ is in fact divisible by one of the $k! = 1\times2\times...\times k$, where $0 < k < p^n$. If we take the example of $2^3$ and $k = 7$, we can easily see that $k! = 1 \times 2 \times ... \times 6 \times 7$, and $2$ clearly divides $2^3$. How else can I approach this?",['combinatorics']
751910,"Ordinary differential equation $y'(t)=\sin(f(t,y))$","One whose solution never makes me happy is the following:
$$y'(t)=\sin(y+t)\text{.}$$
I would start by substituting $z(t)=y(t)+t$ to get an ODE in $z(t)$, but then I'm not sure about how to substitute back my solution to check if it's correct or not$\dots$","['trigonometry', 'ordinary-differential-equations']"
751945,Rising Sun Inequality (Dunford-Schwartz maximal inequality),"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be an absolutely integrable function, and let $f^*:\mathbb{R} \rightarrow \mathbb{R}$ be the one-sided signed Hardy-Littlewood maximal function
$$f^*(x)  := \sup_{h>o} \frac{1}{h}\int_{[x,x+h]} f(t) dt.$$
Establish the rising sun inequality
$$\lambda \mu (\{f^* > \lambda\})\leq \int_{\{f^*>\lambda\}} f(t)dt,$$
furthermore, the above is in fact equal when $\lambda > 0$. I have been stuck on this for the past couple of days. I am still trying to show the case when $\lambda = 0$. Here is the hint from the problem: when $f$ is compactly supported on the compact interval $[a,b]$, then $F(x) = \int_a^x f(t)dt-(x-a)\lambda$ is continuous on the compact interval; apply the Rising Sun lemma to $F(x)$. Side note: for $\lambda >0$
$$\mu (\{x\in \mathbb{R} : \sup_{h>o} \frac{1}{h}\int_{[x,x+h]} |f(t)| dt > \lambda\})\leq \frac{1}{\lambda}\int_\mathbb{R}|f(t)|dt$$
is called One-sided Hardy-Littlewood maximal inequality. This can be proven with the hint from above instead of the standard method using Vitali cover lemma.","['ergodic-theory', 'inequality', 'functional-analysis', 'integration']"
751948,Mellin Transform of the floor function,"I have from integral transform tables that: $$\operatorname{Mellin}(\lfloor x\rfloor) = -\dfrac{\zeta(-z)}{z},\quad \operatorname{Re}(z)<-1$$ How can this be proved?","['integration', 'integral-transforms']"
751966,Sites or youtube videos to learn algebraic geometry,"Is there any sites or free lecture videos to learn algebraic geometry? or should I call abstract algebra? I want to understand about rings, ideals, and real spectrum of rings but my understanding on it is near 0. If anyone have some recommendation video or anything  (as long as it is free :) ) for commoners (for people whose major is not math) to learn things that I mentioned above, please let me know. Many thanks in advance. Cheers!","['algebraic-geometry', 'abstract-algebra']"
751971,Application of chain rule,"The equations $u=f(x,y),x=X(t),y=Y(t)$ define $u$ as a function of $t$, say $u=F(t)$. Compute $F'(t)$ in terms of $t$ if, $$f(x,y)=\log [(1+e^{x^2})/(1+e^{y^2})] , X(t)=e , Y(t)^t=e^{-t}.$$ From the chain rule we have - $$F'(t) = \frac{\partial f}{\partial x}X'(t)+\frac{\partial f}{\partial y}Y'(t).$$ Since $X(t)$ and $Y(t)$ are constant here and respectively equal to $e$ and $e^{-1}$, does this not mean that $F'(t)=0$?","['multivariable-calculus', 'derivatives']"
751991,Invariant measure of Euler-Maruyama Discretisation of an Ito diffusion,"Let $(X_t)_{t \geq 0}$ be a diffusion process with dynamics governed by the stochastic differential equation
\begin{equation}
dX_t = b(X_t)dt + \sigma(X_t)dW_t, ~~ X_0 = x_0,
\end{equation}
where $b,\sigma$ are Lipschitz and $(W_t)_{t \geq 0}$ is a standard $d$-dimensional Wiener process.  Assume $(X_t)_{t \geq 0}$ possesses a unique invariant probability measure $\mu(\cdot)$. The Euler-Maruayama approximation of this SDE with step size $h$ is a Markov chain defined recursively as $Y_0 = x_0$, and for $n \in \mathbb{Z}^+$
\begin{equation}
Y_{(n+1)h}|Y_{nh} = Y_{nh} + b(Y_{nh})h + \sqrt{h}\sigma(Y_{nh})\varepsilon, ~~ \varepsilon \sim \mathcal{N}(0,I_{d \times d}).
\end{equation} I can see that under certain conditions $\{Y_i\}_{i \in \mathbb{Z}^+}$ will also possess a unique invariant probability measure, $\mu^h(\cdot)$.  I've seen certain results discussing how the ergodicity properties of $(X_t)_{t \geq 0}$ and $\{Y_i\}_{i \in \mathbb{Z}^+}$ can differ (e.g. scenarios in which the first can be exponentially ergodic but the second transient, see: http://projecteuclid.org/euclid.bj/1178291835 ). What I've been unable to find are results discussing how $\mu^h(\cdot)$ and $\mu(\cdot)$ differ.  Can we make some statement about how 'close' the two measures are, in total variation
\begin{equation}
\| \mu^h(\cdot) - \mu(\cdot) \|_{TV}
\end{equation}
or some other appropriate distance.  And can we say anything about the distance
\begin{equation}
\|\mathbb{E}_{\mu^h}f - \mathbb{E}_{\mu}f\|
\end{equation}
for some suitable class of functions $f$, assuming both processes are defined on the same space which has norm $\|\cdot\|$?  I've searched around online a bit, the closest I've come is here http://www.newton.ac.uk/preprints/NI03065.pdf , where the result is alluded to, which makes me think it might be fairly well known.  So if anyone could point me in the right direction or explain why it's obvious then that would be great, thanks! Also asked on Math overflow here: https://mathoverflow.net/questions/163443/invariant-measure-of-euler-maruyama-discretisation-of-an-ito-diffusion/163489?noredirect=1#163489","['stochastic-processes', 'markov-chains', 'stochastic-calculus', 'numerical-methods', 'probability']"
751999,Problem : Solve $|x^2+x-4| =|x^2-4| +|x|$,"Problem : Solve $|x^2+x-4| =|x^2-4| +|x|$ We can find the critical point of each modulus function individually then we  get : $x =\pm 2;$ and $x = 0$ $x = \frac{-1 \pm \sqrt{17}}{2}$ So there are six intervals on the number line ( as per definition of modulus function) for which we can check the behaviour of mod. function which are : $(-\infty , -2.55) , (-2.5 , -2), (-2,0) , (0,1.45),(1.45, 2), (2, \infty)$ Please suggest whether this is the correct method of approaching this problem. Thanks .","['algebra-precalculus', 'functions']"
752006,Is every subgroup of the product of two cyclic groups is again a product of two cyclic groups?,"Well, this is my question. Is every subgroup of the product of two cyclic groups is again a product of two cyclic groups (maybe one being trivial)? Thanks!","['group-theory', 'abstract-algebra']"
752025,Manipulating An Equation into A Workable Form,"The question asks me to find the arc length of 
$$y= (x-x^2)^{1/2} + \sin^{-1}(x^{1/2})$$
I know I need to take the derivative:
$$\frac{1-2x}{2(x-x^2)^{1/2}} + \frac{1}{(1-x)^{1/2}}$$
I've tried manipulating it to simplify it, since I need some equation that is easy to square, but I've gotten nowhere. I tried getting a common denominator but that didn't really help. 
I also tried rationalizing the denominator but got stuck as well. Please help! Ok, sorry! The derivative of $$\sin^{-1}(x^{1/2})$$ should be $$\frac{1}{2(x)^{1/2}(1-x)^{1/2}}$$
That makes more sense!","['plane-curves', 'algebra-precalculus', 'derivatives']"
752036,Find linear transformation given kernel,"Find linear transformation $F$ in canonical bases given $ F: \Bbb R^4 \to \Bbb R^3 $ $ \ker F=\operatorname{span}\left\{\begin{bmatrix}1\\2\\3\\4\end{bmatrix}, \begin{bmatrix}0\\1\\1\\1\end{bmatrix} \right\} $ I tried expanding these two vectors to a basis of $\Bbb R^4$, I tried adding two linear dependent vectors, but nothing seems to work. I would be glad for a little hint.","['linear-algebra', 'transformation']"
752038,An Inequality Problem with not nice conditions,"How to show that $\dfrac{a^3}{a^2+b^2} + \dfrac{b^3}{b^2+c^2} + \dfrac{c^3}{c^2+a^2} \ge \dfrac32$, where $a^2+b^2+c^2=3$, and $a,b,c > 0$ ?","['inequality', 'algebra-precalculus']"
752056,Does the rank-nullity theorem hold for infinite dimensional $V$?,"The rank nullity theorem states that for vector spaces $V$ and $W$ with $V$ finite dimensional, and $T: V \to W$ a linear map, $$\dim V = \dim \ker T + \dim \operatorname{im} T.$$ Does this hold for infinite dimensional $V$?  According to this , the statement is false.  But according to this , page 4, the statement is still true.  I'm thoroughly confused.",['linear-algebra']
752073,Continuous injective map is strictly monotonic,"Show that if $f: \mathbb{R} \to \mathbb{R}$ is a continuous injective map, then it is strictly monotonic. Could someone give me a proof for this? I have the intuition for why it's true - I'm just having trouble expressing that intuition in a rigorous manner. Basically consider two points $x_1, x_2 \in \mathbb{R}$. By the problem statement, $f$ is continuous on $[x_1, \, x_2]$. WLOG, assume that $f$ is strictly increasing. It there exists a point where it is not increasing, then $f$ hits a value twice, and it's not injective.",['real-analysis']
752081,"Directional, differential and lie derivatives on manifolds intuition?","Trying to translate elementary multivariable calculus into the language of manifolds: Is the directional derivative on a manifold just a way of finding the rate of change of a vector in a single direction, in a fixed basis, where the vector is expressed in terms of linear combinations of basis vectors (i.e. to find the rate of change of a vector in the tangent space moving along one of the directions the basis points out)? Is the differential then just a way of finding the rate of change of that same vector in a single direction, where the vector is just expressed in terms of it's coordinates in an invariant fashion (i.e. to find the rate of change of an equivalent vector in the cotangent space that is dual to the original vector in one direction directed out by the basis)? If I want to take a second derivative in another direction given by a fixed basis, am I forced to define the lie derivative of a one-form? i.e. is the lie derivative just a fancy way of taking second derivatives of scalar-valued functions in a single direction (while also interpretable as first, second, ... derivatives of vector-valued functions)? In terms of vector-valued functions of vector fields, I've never understood why the second derivative naturally ends up with us having to define a bilinear form, intuitively why does this necessarily arise in taking the second derivative of something like $\vec{F}(x,y) = (x^2+y^2,2xy)$? How does this intuitive example translate into the language of lie derivatives of vector fields? Is the covariant derivative just a way to do all of the above in an arbitrary basis, i.e. in a random direction no matter what basis we're given? What does the commutator actually do, thinking along these lines?","['multivariable-calculus', 'manifolds']"
752086,Sequential Criterion for Limits,"Prove that if for every sequence ($x_n$) in A that converges to c such that $x_n \ne c$ for all $n \in \mathbb{N}$, the sequence $(f(x_n))$ converges to L, then $lim_{x\to c} f = L$.","['real-analysis', 'limits']"
752112,Alternatives to Fisher information,"The Fisher information matrix is defined as the following: $$\mathcal{I}(\theta)=E[(\frac{\partial \log f(x;\theta)}{\partial \theta})^2]=-E[\frac{\partial^2 \log f(x;\theta)}{\partial \theta \partial \theta^T}]$$ Where $f(x;\theta)$ is the probablity distribution function (pdf) of some random (vector) variable $x$ parameterized by (vector) parameter $\theta$.  For an unbiased estimator $\mathcal{I}(\theta)^{-1}$ is an lower bound for the MSE of estimating the parameter $\theta$. The reason that Fisher chose this definition for the Information measure was very intuitive:
We are interested in relative changes in $f(x;\theta)$, i.e. $\frac{\partial f(x;\theta)/\partial \theta}{f(x;\theta)}=\frac{\partial \log f(x;\theta)}{\partial \theta}$. In particular, we are interested in average of this quantity regardless of its sign. Therefore, $E[(\frac{\partial \log f(x;\theta)}{\partial \theta})^2]$, is a natural and usually tractable choice (with closed-form solution in many cases). Here is my specific question: Are there alternatives to this definition? For example, $\mathcal{I}_a(\theta)=E[|\frac{\partial \log f(x;\theta)}{\partial \theta}|]$, is the first that comes to my mind. Are there other established formulations, or even totally different approaches that address the problem of lower-bounding the error of parameter estimation? Thanks!","['statistics', 'measure-theory', 'parameter-estimation']"
752123,Computing a Lie Bracket: General Questions,"I'm asked to compute the following Lie Bracket: $\left [ -y \dfrac{\partial}{\partial x} + x\dfrac{\partial}{\partial y} , \dfrac{\partial}{\partial x} \right] $ on $\mathbb{R}^2$. Just writing it out, I get $\left( -y \dfrac{\partial}{\partial x} + x\dfrac{\partial}{\partial y} \right) \dfrac{\partial}{\partial x} - \dfrac{\partial}{\partial x} \left(-y \dfrac{\partial}{\partial x} + x\dfrac{\partial}{\partial y} \right)$. How can I simplify this? I know this is a very trivial question, but I'm getting stuck for some stupid reason. Any help would be greatful :)","['multivariable-calculus', 'lie-algebras', 'differential-geometry']"
752129,Finite-case symmetry leads to infinite-case asymmetry,"Formulas for sines or cosines of sums superficially appear to have a certain symmetry , specifically it looks as if sine and cosine play something like symmetrical roles:
$$
\begin{align}
& \sin(\alpha+\beta+\gamma+\delta+\varepsilon+\zeta) \\[8pt]
= & \underbrace{\sin\alpha\  \overbrace{\cos\beta\cos\gamma\cos\delta\cos\varepsilon\cos\zeta}^{\text{5 cosines}}+\cdots\cdots}_{\text{6 terms}} \\[8pt]
& \underbrace{{} - \overbrace{\sin\alpha\sin\beta\sin\gamma}^{\text{3 sines}}\  \  \overbrace{\cos\delta\cos\varepsilon\cos\zeta}^{\text{3 cosines}}-\cdots\cdots}_{\text{20 terms}} \\[8pt]
& \underbrace{\overbrace{{} + \sin\alpha\sin\beta\sin\gamma\sin\delta\sin\varepsilon}^{\text{5 sines}}\ \ \cos\zeta+ \cdots\cdots}_{\text{6 terms}} 
\end{align}
$$ But that superficial appearance vanishes when there are infinitely many terms: every term has only finitely many sine factors and cofinitely many cosine factors: $$
\sin\left( \sum_{k=1}^\infty \alpha_k \right) = \sum_{\text{odd }k} (-1)^{(k-1)/2} \sum_{\begin{smallmatrix} A \subseteq\{1,2,3,\ldots\} \\  |A|=k \end{smallmatrix}} \prod_{k\in A} \sin\alpha_k \prod_{k\not\in A}\cos\alpha_k.
$$ This is of course an instance of the fact that when one enumerates the finite subsets of a set, then the set of complements is the same as the set enumerated if the set is finite, but quite different if the set is infinite. But until one looks at the infinite case, one doesn't know that sine rather than cosine is the one that will appear only finitely many times. So I have a somewhat open-ended question: Where else in mathematics is symmetry in finite cases replaced by extreme asymmetry in infinite cases?","['trigonometry', 'set-theory', 'compactness', 'big-picture']"
752133,How to show that there is no $3\times3$ real matrix $A$ such that $A^2+I=0$?,"Question: show that there is no $3\times3$ real matrix $A$ such that $A^2+I=0$? Is it because: 
$$\det(A^2)=\det(-I)\\
\implies \det(A)\det(A)=-1\\
\implies \det(A)=-i$$
How to continue?","['matrices', 'linear-algebra', 'determinant']"
752137,The Lie derivative by an infinitesimal action is invertible at an isolated zero point,"Let a compact Lie group $G$ act on a manifold $M$. Fix $X \in \mathfrak g$ and we write $X_M$ for the infinitesimal action of $X$. Assume that $p \in M$ is a zero point of $X_M$. Define a linear map $L_p : T_pM \to T_pM$ by $L_p(v) = \mathcal L(X_M)v$. Here $v$ on the right hand side is an extension of the tangent vector $v$ to a vector field. Since $X_M(p)=0$, $L_p$ is well-defined. I want to show that $L_p$ is invertible if $p$ is an isolated zero point. Here is a proof (Lemma 45). Proof . Pick a $G$-invariant Riemannian metric and suppose that $v \in T_pM-0$ is annihilated by $L_p$. Taking the exponential map with respect to the metric, we see that all the points on the geodesic $\exp_p (tv)$, $t \in \mathbb R$, would be fixed by $\exp(sX) \in G$, $s \in \mathbb R$, contradicting to $p$ being isolated. (I changed the proof slightly due to notation.) Could you tell me why $\exp_p(tv)$ is fixed by $\exp(sX)$?","['riemannian-geometry', 'lie-groups', 'differential-geometry', 'geodesic']"
752155,Analysis of stability of a linearized ODE with a periodic solution,"I am asked to find the stability of the following ODE:
\begin{equation*}
\dot{y} = y^{2} + 2\cos(t)\sin(t) - \sin^{4}(t)
\end{equation*}
by linearizing around a particular solution $\eta = \sin^{2}(t)$,
and investigating the solution. My understanding is that I should look at the first variational equation
\begin{equation*}
\dot{y} = A(t)y
\end{equation*}
where
\begin{equation*}
A(t) = df_{y}(t,\eta), \quad A(t+T) = A(t), \quad f(t,y) = \dot{y}(t).
\end{equation*} Thus, I get
\begin{alignat*}{2}
f(t,y) &= y^{2} + 2\cos(t)\sin(t) - \sin^{4}(t) &&\Rightarrow \\
f_{y}(t,y) &= 2y &&\Rightarrow \\
A(t) &= 2\sin^{2}(t).
\end{alignat*} Now $A(t)$ is periodic with period $T=2\pi$, and my next understanding is that stability of the solution depends on the sign of the eigenvalue for this one-dimensional system which are the Floquet multipliers (or characteristic exponents).  This is given by the formula (using notion from Teschl's ODE text):
\begin{alignat*}{2}
\bar{a} &= \frac{1}{T}\int_{0}^{T}A(s)ds \\
&= 1
\end{alignat*}
which seems to imply that the solution is not stable.  Is this understanding correct?  Either way, I think part of my confusion comes from not understanding the connection between the first variation and the principal matrix solution of the original equation.  Any help towards that would be greatly appreciated.",['ordinary-differential-equations']
752246,Series convergent but not absolutely? $\sum_{n=1}^{\infty} \frac{\cos(n^p \pi)}{n^p}$,"For which real numbers $p>0$ does the series
$$\sum_{n=1}^{\infty} \frac{\cos(n^p \pi)}{n^p}$$
converge? Obviously it converges absolutely for $p>1$ but what about $0<p<1$? I have the feeling that something qualitative happens at $p=1/2$. Help wanted, thanks.","['absolute-convergence', 'convergence-divergence', 'sequences-and-series']"
752298,Prove that $\sum_{k=0}^n k^2{n \choose k} = {(n+n^2)2^{n-2}}$,Prove that: $$\sum_{k=0}^n k^2{n \choose k} = {(n+n^2)2^{n-2}}$$  i know that: $$\sum_{k=0}^n {n \choose k} = {2^n}$$ how to get the (n + n^2)?,"['induction', 'algebra-precalculus', 'binomial-coefficients', 'combinatorics']"
752313,Second partial derivative test is inconclusive,"I am trying to find the critical points of the function: $f(x,y)=2x^4-3x^2y+y^2$ and find the Max, Min and saddle points. What I've done so far is: $f_x=8x^3-6xy=0  ,  f_y=-3x^2+2y=0  ,  f_{xx}=24x^2-6y  ,  f_{yy}=2  ,  f_{xy}=-6x$ So (0,0) is the only critical point. But using the second partial derivative test: $\Delta(0,0)=f_{xx} . f_{yy} - f^2_{xy}=0$ Which is inconclusive. Without using MATLAB or similar software and based on calculation, how can we determine whether (0,0) is Max, Min or saddle point? Or the general question is what to do when it's inconclusive (without using software)?","['partial-derivative', 'derivatives']"
752322,explicit expression sought,"Consider the equation
$$
\cos^2\phi + \alpha\sin\phi\cos\phi-\beta=0\;,
$$
where $\alpha,\beta\in\mathbb{R}$. I need to find an explicit expression for $\phi$. I have tried completing the square, but that did not go far. Any ideas are welcome.","['trigonometry', 'calculus', 'real-analysis']"
752351,What is the relationship of the EMD (Earth movers Distance) and total variation (and other probability measures)?,"I was trying to understand different methods for comparing probability distribution and saw the following paper/reference: http://arxiv.org/abs/math/0209021 In it it defines and compares and explains some of the relationships between each probability measure. I was wondering if someone knew or understood better the relationships EMD has with other the other metrics? The reason I am asking specifically about total variation and EMD is because I heard someone refer to them being the same thing, but I was not sure about it and after reading a little about both of them, I am a little skeptical they are the same thing in every case. Can anyone clarify that point? Maybe I was thinking there might be special probability distributions in  which they are the same ... Also if you have more knowledge in general about this, feel free to share that too (i.e. if you want to compare EMD to other measures in addition to total variation).","['probability-theory', 'measure-theory']"
752358,"Get the $n$th term of a sequence $1$, $2$, $4$, $7$, $13$, $24$, $\ldots$","I have a sequence: $1$ , $2$ , $4$ , $7$ , $13$ , $24$ , $44$ , $81$ ,  and I think it's like a Fibonacci sequence, however you add three numbers together and not two (""Tribonacci""?).  So: $$ v_n = v_{n-1} + v_{n-2} + v_{n-3}, $$ where $v_0=1$ , $v_1=1$ and $v_2=2$ . I begin from the proof of the $n$ th term of the Fibonacci sequence, and I find that: $$ \begin{bmatrix}1 & 1 & 1\\1 & 0 & 0\\0 & 1 & 0 \end{bmatrix}^n \begin{bmatrix}2 \\1\\ 1\end{bmatrix} = \begin{bmatrix}v_{n+2} \\v_{n+1}\\ v_n\end{bmatrix} = \mathbf{A}^n\mathbf{x} $$ $$ \mathbf{A}^n = \mathbf{PD}^n\mathbf{P}^{-1}$$ and $$ \mathbf{D}^n = \begin{bmatrix}\lambda_0^n &0&0\\0&\lambda_1^n&0\\0&0&\lambda_2^n\end{bmatrix}. $$ From $\det(\lambda I-A)=0$ , I got that $\lambda_0=0$ , $\lambda_1=1$ and $\mathbf{x}=0$ I got stuck here. Can anybody help me where I make a mistake?","['recurrence-relations', 'sequences-and-series', 'discrete-mathematics']"
752373,Is the Axiom of Choice implicitly used when defining a binary operation on a quotient object?,"Let's say you have a group $(G,\cdot)$ and you have a normal subgroup $N$ (note we are considering this only as a set). And now we want to define a binary operation $\star$ on $G/N$ such that $(G/N, \star)$ is a group. Every element of $G/N$ looks like $aN$ for some $a\in G$. Thus it seems perfectly natural to want to exploit the group-theoretic structure of the elements of the cosets and define $aN\star bN:= a\cdot bN$. Most textbooks do this and then they quickly turn around and prove this operation is well-defined. I understand all of this and why they do it, but doesn't this construct pre-suppose a choice function on $G/N$? It seems to me that the definition of the binary operation $\star$ is a quick abbreviation of all of this: let $f$ be a choice function for $G/N$. Then for all $A$ and $B$ in $G/N$ define $A\star B:= (f(A)\cdot f(B))N$. And the usual well-defined check follows up to show that this operation does not depend on the choice-function $f$, shows that this operation is associative, and has an identity. But the independence of the choice-function does not excuse the need for the existence of a choice function. And we are not always guaranteed a choice function (in AC's absence). It bothers that we appear to need an extremely strong principle for something so fundamental but trivial in Group Theory. (Of course, there is a similar problem with rings and ideals in Ring Theory) I have a couple questions. Is my supposition correct? That is, in the usual construction of the quotient operation, is there an assumption of a choice-function? If that is the case, could we---at the cost of being long-winded and perhaps tedious---define a quotient operation that does not presuppose a choice function?","['group-theory', 'abstract-algebra', 'axiom-of-choice']"
752389,Proving a set of $2\times 3$ matrices is a manifold?,"The way I have always been told to check if something is a manifold (I haven't had a whole lot of experience with them), is to check if the derivative of the function representing the loci of the graph satisfies the implicit function theorem. For example, the circle is a manifold because it can be expressed as $F(x,y)=x^2+y^2-1=0$ and this has a derivative of $D_F=[2x\  2y]$ which satisfies the implicit function theorem since it is onto. However, how could one show something more abstract is a manifold? Specifically, how can you show (even just hints are fine) that the set of all 2x3 matrices with rank one (subset of $\text{Mat}(2, 3)$) form a manifold in $\mathbb{R}^4$? Also, the notion of manifold for me I think is properly called an embedded submanifold or something of the sort","['matrices', 'manifolds', 'multivariable-calculus']"
752484,mapping from $2^A$ to $P(A)$,Let $2^A$ denote the set of all functions from set $A$ into two-element set 2. How to show that there exists one-to-one and onto mapping from $2^A$ to $P(A)$ (power set)?,['elementary-set-theory']
752493,Finding the inverse of trig functions,I'm supposed to find the inverse of $$f(x) = \cos(x)+x$$ I usually just substitute $x$ for $y$ and then re-arrange. What do I do in this scenario?,"['trigonometry', 'inverse', 'algebra-precalculus']"
752494,Any two disjoint open sets are the interior and exterior of some set,"For a topological space $X$, given any two open sets $A,B$, there is a set $S\subseteq X$ such that $\DeclareMathOperator{\ntr}{int}\ntr S=A$ and $\DeclareMathOperator{\ext}{ext}\ext S=B$. Is this true for $X=\Bbb R^2$? If so, what topological properties on $X$ are needed for a general topological space to satisfy this? If not, are there any restrictions that I forgot to add?",['general-topology']
752498,Orthogonal Parametrization of a Regular Surface,"I was just wondering whether or not it is always possible to parametrize a regular surface $S$ via a function $X$ of local coordinates $u$, $v$ such that $X$ is an orthogonal parametrization- that is to say, such that the first fundamental form has $F=0$ everywhere ($\forall (u,v) \in R$, where the domain of $X$ is $R$).  If so, prove it.  If not, give a counterexample and explain the most general case of a surface (known to you) of when it is possible.","['surfaces', 'differential-geometry']"
752505,Homeomorphism between compactification of real line and unit circle.,"I'm currently working on a topology assignment which is, unfortunately, due today.
As part of that, I need to show that one-point compactification of the real line, $\mathbb{R}\cup\infty$ is homeomorphic to the unit circle.
I've come so far to have defined a function $f$ with $(x,y)=(\cos(2\arctan(t)),\sin(2\arctan(t)))$, which should map the reals onto the unit circle and be bijective and continuous. Now since my domain is compact, if I can show that the unit circle is Hausdorff, I can conclude that my function is a homeomorphism, correct? However, which topology would I use for that? Also, I am having a hard time trying to prove that f is surjective. Thought about dividing it up into two functions, from reals to $(-\pi,\pi)$ and then to the unit circle, but that doesn't really seem to work either. Any thoughts? Topology is confusing... (Intuitively, I absolutely see why all this should be the case, however I am struggling with the formal stuff.) Edit: Okay I have tried the approach of defining an inverse function, suggested and then proving that it is continuous. So far I've got: $$g=\tan\left(\frac{1}{2}\arctan\left(\frac{y}{x}\right)\right)$$ Problem is, that this function is not bijective, so there is something missing (case distinction?) but I can't figure out what... At least within $\left(\frac{-\pi}{2},\frac{\pi}{2}\right)$ it seems to work, and it's easy to show that combining f and g gives the identity.
Any tips maybe? Cheers Tom",['general-topology']
752508,Is it possible to triangularize a matrix only by adding scalar multiples of rows to each other?,"I am working on showing if $B$ is a $s \times s$ matrix, $D$ is a $t \times t$  matrix, $C$ is a $s \times t$ matrix, and $0$ is a $t \times s$ zero matrix, then $\det(A)=\det(B)\det(D)$, where $$A = \begin{bmatrix}B &C\\ 0& D\end{bmatrix}$$ I understand two more complicated proofs, but I figured out if I could triangularize a matrix by just adding scalar multiples of row i to row j (in that sense my determinant will not change), then I can make to proof much more easier. However, is that generally true? After put in some thought, I realized I can use all elementary row operations in my proof. It will not affect the proof. A scratch of my proof will be I am trying to triangularize B and D and A as well, then because the determinant is just the product of diagonal element I can show $\det(A)=\det(B)\det(D)$. If I used row swaps and multiply a row by a scalar, I will have shown $K\det(A)=K\det(B)\det(D)$, where K is some constant due to the row operations. This then gives the result I want. So, the question no longer have anything to do with the proof now. But still, is it possible?","['matrices', 'linear-algebra']"
752521,How do I show a mapping is a homomorphism?,"I don't want to make this question too broad, or non-specific.  I'll will discuss a simple situation so we can all share a common context, but my question is less about this particular group, and more about the strategy of showing that a particular mapping you're interested in is a valid homomorphism. Let $G$ be a group, and let a non-identity $(\ne 1)$ element $a \in G$ be such that $a^2=1$.  Then $K:=\left\{ 1, a \right\} \lt G$.  I don't see right away that $K \lhd G$, so I want to find a homomorphism with kernel $K$. If we were to show that $K$ is normal, we would need $gK = Kg$ $\forall g \in G$, but since $g1 = 1g$, this would imply we need $ga = ag$ $\forall g \in G$.  (I point this out because I'll refer back to it later.  I'm willing to hear arguments about $K$ being normal, but that's not the point of this question.) Defining a mapping $\phi:G \rightarrow G$ such that $\phi(a)=1$ (and necessarily $\phi(1) = 1$) would certainly put $a$ in the kernel if $\phi$ is indeed a homomorphism as we'd like.  But how would I clearly show that this is a homomorphism, with exactly $K$ as its kernel?  Surely I can't just note that $\phi(ag) = \phi(a)\phi(g) = \phi(g) = \phi(g)\phi(a)=\phi(ga)$.  This would depend on $\phi$ being a homomorphism, and would be an example of assuming the conclusion.  If $\phi$ were a homomorphism and well defined, this would show ... well, I guess it would not show that $ag = ga$, but it would show that $ag$ and $ga$ are both members of the same equivalence class in the quotient group formed by modding $K$ out of $G$. I freely admit that my background is strong in analysis and weak in algebra.  When I watch an algebraist at work, everything looks quick and simple and leaves the impression that I'm watching magic, as if we could prove anything we want to, which clearly isn't so. So I suppose my clear, narrowly focussed question is this: What is the list of criteria that one must show to prove that a particular mapping is a well defined homomorphism, and as an example how would they be used to show that this is (or is not) a homomorphism?","['group-theory', 'abstract-algebra', 'normal-subgroups']"
752528,Umbilic Points of an Ellipsoid,"I have an ellipsoid given by $S = \{ (x,y,z): \frac{x^{2}}{a^{2}} + \frac{y^{2}}{b^{2}} + \frac{z^{2}}{c^{2}} = 1$, for some fixed $a,b,c \in \mathbb{R}^{+} \}$.  I need to find the umbilic points of this ellipsoid.  However, everytime I try a parametrization $X$, the expressions for the first and second fundamental forms quickly become rather messy and the Gaussian curvature $K$ and the mean curvature $H$ are nigh impossible to work with by hand.  I ask for help in finding a parametrization $X$ that the quantities therefrom derived are nice and tidy enough to work with by the time that I get to calculating the umbilic points.  Also, if you have any tricks in which way to determine the umbilic points might be easiest in this case (which method to choose), that would be very helpful too. I have tried $X_{1}(u,v) = (a \cos{u} \sin{v}, b \sin{u} \sin{v}, c \cos{v})$ and $X_{2}(u,v) = (u, v, \pm c \sqrt{1 - \frac{u^{2}}{a^{2}} - \frac{v^{2}}{b^{2}}})$ to no avail, along with a few others that I swiftly gave up on.","['surfaces', 'parametric', 'differential-geometry']"
752549,proof a function is an isomorphism,"When we prove a function is an isomorphism, we need to prove it's a bijection and it's closed under an operation. In one example I had no problem proving the first part, but in the second part, I proved that $f^{-1}(ab)=f^{-1}(a)f^{-1}(b)$, so my question is does it also follow that $f(ab) = f(a)f(b)$?","['group-theory', 'abstract-algebra']"
752589,Is statistical dependence transitive?,"Take any three random variables $X_1$, $X_2$, and $X_3$. Is it possible for $X_1$ and $X_2$ to be dependent, $X_2$ and $X_3$ to be dependent, but $X_1$ and $X_3$ to be independent? Is it possible for $X_1$ and $X_2$ to be independent, $X_2$ and $X_3$ to be independent, but $X_1$ and $X_3$ to be dependent?","['statistics', 'probability', 'relations']"
752725,Verify Stokes's Formula for...,"Verify Stokes's Formula for $\textbf{F}(x,y,z)=(3y,-xz,yz^2)$, where $S$ is the surface of the paraboloid $2z=x^2+y^2$ bounded by the plane $z=2$. So I need to compute the integral using the formula $\iint_S \text{curl}~\textbf{F} \dot ~\eta ~d\sigma$ and the formula $\int_\Gamma \textbf{F}~\dot~\textbf{T}~ds$ and get the same answer. I have a formula that says $$\int_\Gamma \textbf{F} \cdot \textbf{T}~ds = \int_a^b \textbf{F}(\gamma(t)) \cdot \gamma'(t)dt,$$ so I thought that the line that I am integrating across is $4=x^2+y^2$, so I said $\gamma(t)=(2\text{cos}(t),2\text{sin}(t),2)$, with $0 \leq t \leq 2\pi$. From here it's a pretty straight forward computation, but I'm not sure I set it up correctly. Does this look right? Now for the first integral, I have $$\iint\limits_S \text{curl}~\textbf{F} \cdot \eta ~d\sigma = \iint\limits_R \text{curl}~\textbf{F}(\textbf{r}(u,v)) \cdot \frac{\partial \textbf{r}}{\partial u}\times\frac{\partial \textbf{r}}{\partial v} du \, dv.$$ However, I can't figure out what $\textbf{r}(u,v)$ should be. I tried using polar coordinates but it was pretty ugly. Any tips?",['multivariable-calculus']
752726,Equivalence relations and power sets.,"Let $\mathcal{A}$ be the class of all sets and define the relation $R$ on $\mathcal{A}$ as: $A\space R\space B$ iff there is a bijective function $f:A \to B$. Prove that $R$ is an equivalence relation on $\mathcal{A}$. I know I need to prove bijection by proving injection and surjection, but I don't even know where to start. Any help would be greatly appreciated.","['relations', 'equivalence-relations', 'proof-writing', 'abstract-algebra']"
752732,How to establish $\sum_{d|n}d\phi(d)$,I am focusing on #5(b). I do not understand how they go from what I have to the answer. Those are r's at the end.,"['functions', 'number-theory']"
752739,What to do with the boundary values of a bin in a histogram?,"Suppose I want to make a simple frequency histogram of the following data: $$\{3, 3, 4, 5, 5, 6, 7, 7, 8, 10, 11\}$$ I'm supposed to use bins of size $5$, starting with zero.  Here's my question: Is there a standard way to handle the boundary values of the bins?  My textbook has the two $5$ values go into the right-hand bin; is this always the case, or do some textbooks put the fives into the left-hand bin? Thanks!",['statistics']
752774,Generalized Cross Product,"I know that the cross product can be generalized as $$\text{cross}(x_0,...,x_{n-1})=\det\begin{vmatrix}&x_0&\\&x_1&\\&\vdots&\\e_1&\cdots&e_n\end{vmatrix}$$ where $e_i$ is the $i$'th standard unit vector. We have $n-1$ vectors in $n$-dimensional Euclidean Space, so there is a one-dimensional orthogonal complement to that set (if they are independent) and the cross product above gives a vector in that subspace. I also  know that the ""area""/""n-volume"" of an n-parallelopiped spanned by the vectors $v_1,...,v_n$ is given by $$\sqrt{\det A^TA}$$ where $A=\begin{bmatrix}v_0&\cdots&v_n\end{bmatrix}$. In three dimensions this reduces to $$\sqrt{\det\begin{bmatrix}a_0&a_1&a_2\\b_0&b_1&b_2\end{bmatrix}\begin{bmatrix}a_0&b_0\\a_1&b_1\\a_2&b_2\end{bmatrix}}=\sqrt{||a||^2||b||^2-(a\cdot b)^2}=||a\times b||$$ I am wondering if it is true in general that, taking the cross product as defined above, $$||\text{cross}(x_0,...,x_{n-1})||=\sqrt{\det A^TA}\;\;\;\;\;\; A=\begin{bmatrix}x_0&\cdots&x_{n-1}\end{bmatrix}$$ The algebra seems horrific but I can't find any nice way to prove (or disprove) it.","['cross-product', 'linear-algebra']"
752778,"How many sets can you get by taking closures, complements, and intersections?","The Kuratowski closure-complement problem yields 14 sets which can be formed by taking the closure and the complement of a single set. But if I want to also include such sets as the frontier or boundary, I also need to be able to take intersections between previously generated elements. In this case, how many different sets can you get?",['general-topology']
752815,"How does the base of a group determine the ""sort"" of the elements in the group","I'm trying to study groups in Mathematica, and I've asked a question on Mathematica.SE that perhaps only someone from Math.SE could answer. Related: How does GroupActionBase affect the order of elements in a group? We typically don't think of elements of groups, or even sets to have an inherent ""sort order"", but a computer must select one in order to operate on, and display the group.  Mathematica uses a command called GroupActionBase to determine the sort order of the elements in a group, but it doesn't specify how.  At first I thought the term base in this context was specific to Mathematica, but I was surprised when I looked it up and saw that it was an algebra term.  That page has an example that I've copied from the help file (the help file doesn't say much more than what I've copied to that question).  Can someone familiar with group bases clue me in on how Mathematica might be using them to determine what order to represent and display elements in?  (Mathematica treats every group as its isomorphic subgroup of some $S_n$, so elements are represented in cycle form.) Edit Here are some additional examples I've generated that might help. As noted on the other site, if I don't provide a complete base, Mathematica extends it somehow (unspecified) to form a complete base.  I also note that if I'm understanding a base correctly, a base for $S_n$ would consist of $n$ (or $n-1$?) points with at most $n!$ different ways to order them, but the number of elements in $S_n$ itself is $n!$, so there are $(n!)!$ different ways to order them.  This would seem to me that I can't force any order I want, and the examples below seem to follow some similar pattern indicating that. In[1]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {1}] // TableForm
Out[1]//TableForm=
        Cycles[{}]
        Cycles[{{2,3}}]
        Cycles[{{1,2}}]
        Cycles[{{1,2,3}}]
        Cycles[{{1,3,2}}]
        Cycles[{{1,3}}]

In[2]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {2}] // TableForm
Out[2]//TableForm=
        Cycles[{}]
        Cycles[{{1,3}}]
        Cycles[{{1,2}}]
        Cycles[{{1,3,2}}]
        Cycles[{{1,2,3}}]
        Cycles[{{2,3}}]

In[3]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {3}] // TableForm
Out[3]//TableForm=
        Cycles[{}]
        Cycles[{{1,2}}]
        Cycles[{{1,3}}]
        Cycles[{{1,2,3}}]
        Cycles[{{1,3,2}}]
        Cycles[{{2,3}}]

In[4]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {1, 2}] // TableForm
Out[4]//TableForm=
        Cycles[{}]
        Cycles[{{2,3}}]
        Cycles[{{1,2}}]
        Cycles[{{1,2,3}}]
        Cycles[{{1,3,2}}]
        Cycles[{{1,3}}]

In[5]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {1, 3}] // TableForm
Out[5]//TableForm=
        Cycles[{}]
        Cycles[{{2,3}}]
        Cycles[{{1,3}}]
        Cycles[{{1,3,2}}]
        Cycles[{{1,2,3}}]
        Cycles[{{1,2}}]

In[6]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {2, 3}] // TableForm
Out[6]//TableForm=
        Cycles[{}]
        Cycles[{{1,3}}]
        Cycles[{{2,3}}]
        Cycles[{{1,2,3}}]
        Cycles[{{1,3,2}}]
        Cycles[{{1,2}}]

In[7]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {1, 2, 3}] // TableForm
Out[7]//TableForm=
        Cycles[{}]
        Cycles[{{2,3}}]
        Cycles[{{1,2}}]
        Cycles[{{1,2,3}}]
        Cycles[{{1,3,2}}]
        Cycles[{{1,3}}]

In[8]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {1, 3, 2}] // TableForm
Out[8]//TableForm=
        Cycles[{}]
        Cycles[{{2,3}}]
        Cycles[{{1,3}}]
        Cycles[{{1,3,2}}]
        Cycles[{{1,2,3}}]
        Cycles[{{1,2}}]

In[9]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {2, 1, 3}] // TableForm
Out[9]//TableForm=
        Cycles[{}]
        Cycles[{{1,3}}]
        Cycles[{{1,2}}]
        Cycles[{{1,3,2}}]
        Cycles[{{1,2,3}}]
        Cycles[{{2,3}}]

In[10]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {2, 3, 1}] // TableForm
Out[10]//TableForm=
        Cycles[{}]
        Cycles[{{1,3}}]
        Cycles[{{2,3}}]
        Cycles[{{1,2,3}}]
        Cycles[{{1,3,2}}]
        Cycles[{{1,2}}]

In[11]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {3, 1, 2}] // TableForm
Out[11]//TableForm=
        Cycles[{}]
        Cycles[{{1,2}}]
        Cycles[{{1,3}}]
        Cycles[{{1,2,3}}]
        Cycles[{{1,3,2}}]
        Cycles[{{2,3}}]

In[12]:= GroupElements[SymmetricGroup[3], GroupActionBase -> {3, 2, 1}] // TableForm
Out[12]//TableForm=
        Cycles[{}]
        Cycles[{{1,2}}]
        Cycles[{{2,3}}]
        Cycles[{{1,3,2}}]
        Cycles[{{1,2,3}}]
        Cycles[{{1,3}}]","['mathematica', 'abstract-algebra', 'finite-groups', 'math-software', 'group-theory']"
752855,Differentiable functions defined on a regular surface,"First, recall a general definition of a differentiable function as follows. Suppose $f :D\subset \mathbb R^3 \rightarrow \mathbb R$. Then $f$ is differentiable at
$\mathbf a \in D$ if there is a linear function $L:\mathbb R^3 \rightarrow \mathbb R$ such that $$\lim_{\mathbf x\rightarrow\mathbf a}\frac{\|f(\mathbf x)-[f(\mathbf a)+L(\mathbf x-\mathbf a)]\|}{\|\mathbf x-\mathbf a\|} = 0.$$ Now consider a definition of a differentiable function defined on a regular surface given in most differential geometry textbook. Let $f:V\subset S\subset \mathbb R^3\rightarrow \mathbb R$ be a function defined on an open subset $V$ of a regular surface $S$ (which itself is a subset of $\mathbb R^3$). Then $f$ is differentiable at $\mathbf p\in V$ if for some parameterization $\mathbf x: U\subset\mathbb R^2\rightarrow S$ with $\mathbf p\in \mathbf x(U)\subset V$, the composition $f\circ\mathbf x:U\subset \mathbb R^2\rightarrow \mathbb R$ is differentiable at $\mathbf x^{-1}(\mathbf p)$. My question is whether the two definitions are consistent with each other. To be more specific, if the two definitions are consistent, then Why do we need a new definition for differentiable functions defined on a regular surface which is in fact a subset of $\mathbb R^3$ anyway? What is the corresponding linear function $L$ in the second definition? Thank you!","['self-learning', 'differential-geometry']"
752895,"Ahlfors ""Prove the formula of Gauss""","He says: Prove the formula of Gauss:
  $$
(2\pi)^\frac{n-1}{2} \Gamma(z) = n^{z - \frac{1}{2}}\Gamma(z/n)\Gamma(\frac{z+1}{n})\cdots\Gamma(\frac{z+n-1}{n})
$$ This is an exercise out of Ahlfors. By taking the logarithmic derivative, it's easy to show the left & right hand sides are the the same up to a multiplicative constant. After that I'm lost. It's easy using another identity when $n$ is even to use induction. But when $n$ is odd I am lost. It's obvious when $n$ is a power of 2.","['gamma-function', 'complex-analysis']"
752957,Exponentials of rational numbers,Does there exist an $$0<x<1$$ such that $$\forall q \in \mathbb{Q^+}$$ $$q^x \in \mathbb{Q^+}$$,"['real-analysis', 'number-theory']"
752968,Fixed Point Involutions,"In recent reading on Riemann surfaces and complex manifolds (primary Miranda with a few random finds online), I encountered the notion of involutions, in particular fixed point involutions. We recall that an involution on a complex manifold $X$ is an element $f \in \mathrm{Aut}(X)$ of order two. I want to prove the following results: (i) The fixed point set of any involution on $\mathbb{P}^{2}$ contains a line. (ii) Every involution on a non-hyperelliptic Riemann surface of genus 3 (i.e. a canonically embedded degree 4 curve) has a fixed point. (iii) Every involution on a Riemann surface of even genus has a fixed point. Could someone provide some guidance on this? Thanks very much!","['riemann-surfaces', 'algebraic-geometry', 'manifolds']"
752989,Lefschetz duality for non-compact relative manifolds,"I'd like to use the formulation of Lefschetz duality stated here , but I can't seem to find a reference for this particular version of it, and it doesn't seem quite right to me. The exact statement in question is:
Let $X$ be a Hausdorff topological space, and let $A \subset X$ be a subspace such that the complement $X - A$ is an orientable topological $n$-manifold. Then, for any abelian group $G$ and any $i$,
$$H_i(X, A; G) \cong H_c^{n - i}(X - A; G),$$
where $H_i$ and $H_c^{n - i}$ denote singular homology and compactly supported singular cohomology, respectively. The problem is, by Poincar duality, $H_c^{n - i}(X - A; G) \cong H_i(X - A; G)$, so if the above statement is true, then $H_i(X - A; G) \cong H_i(X, A; G)$. But that definitely doesn't seem right; for example, if $X = \mathbb{R}^2$ and $A = \{(0, 0)\}$, then $H_1(X - A; G) \cong G$ because $X - A$ is homotopic to a circle, but $H_1(X, A; G) \cong \tilde{H}_1(X; G) = 0$. Is the above statement true in this generality? If so, where can I find a proof, and where is my attempt at a counterexample mistaken? If not, what's the correct formulation? (The main case I'm interested in is where $X$ is a complex projective variety and $A$ is a closed subvariety containing the singular locus of $X$.)","['algebraic-topology', 'algebraic-geometry', 'reference-request']"
752995,Real analysis question involving a linear ODE,Where do I start with this one? This question is really quite difficult..,"['derivatives', 'ordinary-differential-equations', 'integration', 'real-analysis']"
753105,Proving that Markov Chain Monte Carlo converges,"I am trying to understand how the very basic Markov Chain Monte Carlo approach works: We try to approximately calculate the expected value $E_{\pi(x)}[X]$ by drawing sequential samples from a Markov Chain $(x_0,x_1,...)$ with the stationary distribution $\pi(x)$ and  transition matrix $T(x_i|x_{i-1})$. So, according to the MCMC approach, it should be $E_{\pi(x)}[X] \approx \frac{1}{N-N_0} \sum_{i=N_0}^{N} x_i$ where $N_0$ is the point where we assume that $p(x_{N_0})$ is close enough to the stationary distribution $\pi(x)$. We simulate the Markov Chain such that $x_0$ comes from an initial distribution $p(x_0)$ and each $x_i$ comes from $T(x_i|x_{i-1})$ . I am trying to show that $\frac{1}{N-N_0} \sum_{i=N_0}^{N} x_i \rightarrow E_{\pi(x)}[X]$ as $N \rightarrow \infty$. What I am doing currently is the following: 1-In order to simplify the problem I assume that $x_0$ comes already from the stationary distribution so we won't need a $N_0$ value. I have $S_N = \frac{1}{N} \sum_{i=0}^{N-1} x_i$ where $x_0$ comes from $\pi(x_0)$ and each $x_i$ comes from $T(x_i|x_{i-1})$ and $(x_0,x_1,...)$ is a Markov Chain. 2-The convergence of the regular Monte Carlo is shown with the Law of Large Numbers, so I try the same approach. By using the Weak Law of Large Numbers, I want to show that $P(|S_N - E_{\pi(x)}[X]| \geq \epsilon) \rightarrow 0$ as $ N \rightarrow \infty$ where $\epsilon > 0$. 3-Weak Law of Large Numbers can be proven by using Chebyshev's Inequality. For $S_N$, I write the inequality as $P(|S_N - E[S_N]| \geq \epsilon) \leq \frac{V[S_N]}{\epsilon^2}$ where $V[S_N]$ is the variance of $S_N$ and again $\epsilon > 0$. 4-I first want to show that the expected value of $S_N$, $E[S_N]$ is equal to the expected value we are after: $E_{\pi(x)}[X]$. I showed this by using the fact that the marginal of each $x_i$ in the Markov Chain is equal to the stationary distribution $\pi(x)$: $E[S_N]=E[\frac{1}{N} (x_0 + x_1 + ... + x_{N-1})] = \frac{1}{N} (E[x_0] + E[x_1] + ... + E[x_{N-1}]) = \frac{1}{N} (E_{\pi(x)}[x] + E_{\pi(x)}[x] + ... + E_{\pi(x)}[x]) = E_{\pi(x)}[x]$ 5-Now I need to evaluate the variance of $S_N$, $V[S_N]$ in order to complete the Chebyshev's Inequality. But I failed to form a closed form expression for $V[S_N]$ as I did for $E[S_N]$ and I became stuck. I have actually two questions. First one: Is my way of proving the convergence of Markov Chain Monte Carlo is correct to begin with? Second one is, how can I carry on with the proof from the 5. step? It seems that the variance of the Monte Carlo sum doesn't have a closed form solution since $(x_0,x_1,...)$ are not i.i.d and come from a Markov Chain instead. Please note that I am a Computer Engineer, not exactly from a Mathematician background, so I could have done something very naive. Thanks in advance.","['markov-chains', 'self-learning', 'monte-carlo', 'simulation', 'probability']"
753106,"$ \lim\limits_{x \to +\infty}x\, e^{-x^2}\int_{0}^{x}e^{t^2}dt $","Hello every one Please I need your help for the 3rd question, I tried but i fail every time. for every real $ x $, we put $ f(x)=e^{-x^2}\int_{0}^{x}e^{t^2}dt $. Show that $ f $ is odd of class $ C^{\infty} $ on $ \mathbb{R} $. Show that $ f $ is a solution of the functional equation $ y'+2xy=1$. Prove that $ \lim\limits_{x \to +\infty}2xf(x)=1 $. thanks.","['integration', 'real-analysis', 'limits']"
753113,"How to find an equation of the plane, given its normal vector and a point on the plane? [duplicate]","This question already has answers here : Find plane by normal and instance point + distance between origin and plane (2 answers) Closed 10 years ago . I have a question regarding vectors: Find the equation of the plane perpendicular to the vector $\vec{n}\space=(2,3,6)$ and which goes through the point $ A(1,5,3)$. (A cartesian and parametric equation). Also find the distance between the beginning of axis and this plane. I'm not really sure where to start. Any help would be appreciated.","['multivariable-calculus', 'linear-algebra', 'vectors']"
753130,Is there a simple and fast way of computing the residue at an essential singularity?,"Is there a simple and fast way of computing the residue at an essential singularity ? I mean if we have a pole of order $n$ at $c$ we can use the formula : $$\mathrm{Res}(f,c) = \frac{1}{(n-1)!} \lim_{z \to c} \frac{d^{n-1}}{dz^{n-1}}\left( (z-c)^{n}f(z) \right) $$ Is there a similar formula for an essential singularity (without using Laurent series) ?","['residue-calculus', 'complex-analysis', 'analysis']"
753134,d-uple embedding,"When one restricts the $d$-uple embedding $\mathbb{P}^n \hookrightarrow \mathbb{P}^N$ to $\mathbb{P}^{n-1} \hookrightarrow \mathbb{P}^n$, does this yield the $d$-uple embedding $\mathbb{P}^{n-1} \hookrightarrow \mathbb{P}^N$? The degree of $\mathbb{P}^n$ in $\mathbb{P}^N$ is $d^n$. Is the degree of $\mathbb{P}^{n-1}$ under the above embedding equal to $d^{n-1}$?",['algebraic-geometry']
753141,Rationalization of denominator with nth roots,"I would need your help to solve a problem with the rationalization: \begin{equation}
\sqrt{a}-\sqrt{b}\over \sqrt[4]a+\sqrt[4]b
\end{equation} I think I'm doing something wrong with the products and signs of the fourth roots.","['radicals', 'algebra-precalculus']"
753178,union of cartesian products problem!,$$\bigcup_{i \in I}A _{i}\times \bigcup_{j \in J}B _{j} = \bigcup_{(i\times j) \in I\times J}A_{i} \times B_{j}$$ $$\bigcup_{k\in N} \mathbb{N}\times\{k\} = \mathbb{N}\times\mathbb{N}$$ i'm not yet skilled at proving something like this. how can I prove this??,['elementary-set-theory']
753186,GCD of $a^n + b^n$ and $c^n + d^n$,"Prove or disprove that there does not exists any integers $a,b,c,d > 1$ such that $a,b,c,d$ are pairwise coprime, and  $a^n + b^n$ and $c^n + d^n$ are also coprime for all integer  $n > 1$. I came up with this problem after asking about a related problem this afternoon. I was thinking something along the line of Euler's Theorem or F.L.T, though I am completely lost on how to even start tackling this problem.","['divisibility', 'number-theory']"
753210,A question on gradients: $f$ must assume equal value at two points,"If $\nabla f(x,y,z)$ is always parallel to $x i+y j+z k$, show that $f$ must assume equal values at the points $(0,0,a)$ and $(0,0,-a)$.",['multivariable-calculus']
753245,Historical context: The Fresnel integrals,"The evaluation of the Fresnel integrals has been done a plethora of 
times both on this site, and numerous other places. The two main ways of 
evalutating these integrals has either been with some algebraic manipulations. 
$$
\int_{-\infty}^{\infty} \sin(x^2)\mathrm{d}x
= \int_{0}^{\infty} \frac{\sin t}{\sqrt{t}}\mathrm{d}t
=\frac{2}{\sqrt{\pi}}\int_0^\infty \int_0^\infty e^{-tx^2}\sin{t} \, \mathrm{d}x\,\mathrm{d}t
=\frac{2}{\sqrt{\pi}}\int_0^{\infty} \frac{\mathrm{d}x}{1+x^4}
=\sqrt{\frac{\pi}{8}}
$$
Where it was amongst other things used that 
$1/\sqrt{t} = \pi^{-1}\int_{-\infty}^{\infty}e^{-tx^2}\mathrm{d}x$ 
and $\int_0^{\infty}e^{-\alpha t}\sin \beta t \mathrm{d}t
= \beta/(\alpha^2 + \beta^2)$. Or for an example http://www.jstor.org/stable/2320230 , http://www.math.binghamton.edu/loya/papers/LoyaMathMag.pdf Another path to take is complex analysis usually using 
path in the figure below I have seen several proofs using the indented path, but the sources are few. 
For a proof se p.32 here or for a proof on stack.exchange see here .
My question is asking for refferences for these methods to evaluate the Fresnel integrals and in particular using complex analysis. What are the earliest occurrences of using complex analysis to evaluate the Fresnel integrals?","['reference-request', 'complex-analysis']"
753276,Sum of fractions of squared sines,"I'm trying to prove the following approximate identity for $p$ integer:
$$
\sum_{l=1}^m\frac{\sin^2\left(\frac{\pi l}{p}\right)}{\sin^2\left(\frac{\pi l}{mp}\right)}\sim \frac{m^2(p-1)}{2}+O(m)
$$ Things I have tried: Convert to an integral through a Riemann sum, however, the function $1/\sin^2(x)$ and it's derivatives are unbounded for small $x$ I tried to relate it to this problem , but I found it impossible to eliminate the numerator, simply averaging over one period of $p$ doesn't work. Any help would be much appreciated.","['trigonometry', 'sequences-and-series']"
753284,On the convexity of a particular form of integrals,"EDIT: I made some critical corrections below. Let $\mathcal{H}\colon\mathbf{w}\cdot\mathbf{x}+c=0$ be a hypeplane in $\mathbb{R}^n$. Also, let $g\colon\mathbb{R}^n\to\mathbb{R}_+$, be a non-negative, real-valued function. I would like to decide on the convexity of the function $f\colon\mathbb{R}^n\times\mathbb{R}\to\mathbb{R}$, given by
$$
f(\mathbf{a},b)=\int_{\Omega}\! (\mathbf{a}\cdot\mathbf{x}+b)g(\mathbf{x}) \,\mathrm{d}\mathbf{x},
$$
where $\mathbf{x}\in\mathbb{R}^n$, $b\in\mathbb{R}$, and $\Omega$ is the half-space defined by $\mathcal{H}$ as $\Omega=\{\mathbf{x}\in\mathbb{R}^n\rvert\mathbf{w}\cdot\mathbf{x}+c\geq0\}$. What I have thought so far is to express the integral as a sum (if such a thing is feasible) and use the property of the affine (and consequently convex) quantity $(\mathbf{a}\cdot\mathbf{x}+b)\cdot k$, where $k\in\mathbb{R}$. Albeit, I am not sure at all this is going to work. In addition, I am not sure in what direction should I work to... Is there any appropriate argument I could use instead?","['functional-analysis', 'convex-analysis', 'locally-convex-spaces', 'integration']"
753340,Is the polar of the polar set the original set?,"For each $Q \subset \Bbb R^n$, denote $Q^*:=\{z \in \Bbb R^n:z\cdot x \leq 1,\;\;\text{for all}\; x \in Q\}$. Let $P:=\{x \in \Bbb R^n: Ax \leq b\}$, for the matrix $A$ and the vector $b$. It is clear that $P \subset P^{**}$. My question: $P = P^{**}$? There are some particular cases that the equality happens. But I did  not succeed in the general case. I would like to collect ideas here. Many thanks.","['inner-products', 'discrete-mathematics', 'linear-programming']"
753351,Is the property of having connected complement inherited by the closure of components of interior?,"Let $E$ be a compact set in the plane whose complement $\Omega$ is connected. Let $A$ is the interior of $E$, $B$ is some component of $A$. Can I assert the complement of the closure of $B$ is connected? In other words: if a compact set does not disconnect the plane, then neither do the closures of the components of its interior. This is easy to see for the components themselves, but the interior-closure steps complicate the problem.","['general-topology', 'complex-analysis']"
753381,Limit $\frac{x^2y}{x^4+y^2}$ is found using polar coordinates but it is not supposed to exist.,"Consider the following 2-variable function: $$f(x,y) = \frac{x^2y}{x^4+y^2}$$ I would like to find the limit of this function as $(x,y) \rightarrow (0,0)$. I used polar coordinates instead of solving explicitly in $\mathbb R^2 $, and it went as the following: $$ x = r \cos \theta, \qquad y = r\sin\theta $$ Hence, $$\lim_{(x,y) \to (0,0)} \frac{x^2y}{x^4 + y^2} = \lim_{r \to 0}\frac{r^2\cos^2\theta(r\sin\theta)}{r^4\cos^4\theta + r^2\sin^2\theta}$$ This simplifies to, $$ \lim_{r \to 0} \frac{r^3 \cos^2\theta\sin\theta}{r^2(r^2\cos^4\theta + \sin^2\theta)}$$ Simplifying $r^3/r^2$, we finally get; $$\lim_{r \to 0} \frac{r (\cos^2\theta\sin\theta)}{r^2\cos^4\theta + \sin^2\theta}$$ Now from the above, we find that as $r \to 0$ the limit is $0$. I wanted to verify this answer so I checked on Wolfram Alpha. Explicitly without changing to polar coordinates, it said that the limit does not exist at $(0,0)$ and rightly so. Then how is it that with polar coordinates, the limit exists and is $0$? Am I doing something wrong in this method? Also, what should I do in this situation, and when should I NOT use polar coordinates to find limits of multi-variable functions?","['multivariable-calculus', 'polar-coordinates', 'calculus', 'limits']"
753428,Schwarzschild metric tensor normal vectors,"The Euclidean Schwarzschild metric describing a  manifold (a black hole, though this is not relevant to the question) is given by, $$\mathrm{d}s^2 = \left( 1-\frac{2GM}{r}\right)\mathrm{d}\tau^2 + \left( 1-\frac{2GM}{r}\right)^{-1} \mathrm{d}r^2 + r^2 \mathrm{d}\theta^2 + r^2 \sin^2 \theta \, \mathrm{d}\phi^2$$ where $\tau$ is periodic, hence the manifold is topologically equivalent to $\mathbb{R}^2 \times S^2$. The boundary of the manifold is described by a metric, $$(\mathrm{boundary}):\mathrm{d}s^2=\left( 1-\frac{2GM}{R}\right)\mathrm{d}\tau^2 + R^2 \mathrm{d}\theta^2 + R^2 \sin^2 \theta \, \mathrm{d}\phi^2$$ where we introduce a radial 'cut off' or regulator $R > GM$. The lecturer then stated that the inward pointing unit normal vector was given by, $$n^a = -\delta^a_r \sqrt{1-\frac{2GM}{R}}$$ How does one obtain this normal? In addition, how does one, in general for any metric, find the normals to the boundary? I also saw another lecturer write that the relevant normal was, $$n=-\sqrt{1-\frac{2GM}{R}} \frac{\partial}{\partial r}$$ How is this a normal vector? It has no indices, and the $\partial / \partial r$ suggests it is an operator, rather than a vector.","['general-relativity', 'riemannian-geometry', 'differential-geometry']"
753436,How do I calculate variance for sum of dice?,"I'll post my work, but I'm not sure how to calculate variance. The question asks for the expected sum of 3 dice rolls and the variance. I think I got the expected sum. Any help would be awesome :) thanks!","['discrete-mathematics', 'probability', 'combinatorics']"
753437,Why must a field whose a group of units is cyclic be finite?,"Let $F$ be a field and $F^\times$ be its group of units. If $F^\times$ is cyclic, then show that $F$ is finite. I'm a bit stuck. I know that I can represent $F^\times = \langle u \rangle$ for some $u \in F^\times$ and that we must have that $|F^\times| = o(u)$ , where $o(u)$ denotes the order of $u$ in $F^\times$ .  I tried assuming $o(u) = \infty$ , but I'm not sure exactly where to go from there. I was wondering if I could get a hint.","['cyclic-groups', 'finite-fields', 'abstract-algebra', 'group-theory', 'field-theory']"
753485,Find an $n\times n$ integer matrix with determinant 1 and $n$ distinct eigenvalues,"Pretty much what the title suggests: for any positive integer $n$, I'm looking for an $n$-by-$n$ matrix with integer entries, determinant $1$ and $n$ eigenvalues. In case it is absolutely useless to come up with such a matrix, I'm looking for a proof that such a matrix exists.","['matrices', 'eigenvalues-eigenvectors', 'determinant']"
753502,Derivative of $\; y={(1+e^x)}^{0.5}\; $ using the definition of the derivative,"$$y={(1+e^x)}^{0.5} =f(x)$$ $$\frac{dy}{dx}= \lim_{h\to0}\frac {f(x+h)-f(x)}{h}$$ My attempt I got down to 
$$\lim_{h\to0}\frac{(1+e^xe^h)^{0.5}-(1+e^x)^{0.5}}{h}$$ I can't see where to go from here","['derivatives', 'limits']"
753524,Why is the inverse of this function not a function?,"Why does $F^{-1}$ need to be defined on all of $Y$? I can have this function: $g(x)=x,\quad x\ne 3$ and even though it is not defined for all $x$ in its domain, it is still a function, right?","['discrete-mathematics', 'functions']"
753533,Is this space complete?,"Let $X$ be the space of measurable functions $f:[0,1] \rightarrow \mathbb{R}$. I want to find out whether this space is complete under the metric $d(f,g):= \int_{[0,1]} \frac{|f-g|}{1 + |f-g|}$. Does anybody here have an idea how to tackle this problem?","['measure-theory', 'real-analysis', 'lebesgue-integral', 'metric-spaces', 'functional-analysis']"
753565,Prove that no function exists such that...,"The exercise goes like this: Find a continous function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that $\forall c \in \mathbb{R}$ the equation $f(x)=c$ has exactly 3 solutions; Prove that no continuous function $f: \mathbb{R} \rightarrow \mathbb{R}$ exists such that the equation $f(x)=c$ has exactly two solutions $\forall c \in \mathbb{R}$; For what $n \in \mathbb{N}$ it's true that a continous function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that $\forall c \in \mathbb{R}$ the equation $f(x) = c$ has exactly $n$ solutions exists? For the first point I built kind of a zigzag function which is easily generalizable to every odd $n$. It seems true to me that for even natural numbers such a function doesn't exist, because in some ways it would have to ""jump"", but I failed to formalize the argument.","['functions', 'calculus', 'real-analysis']"
753580,Interpolation and Taylor's Theorem,"I just answered a question where I used the fact that a $(n+1)$ -times (continuously) differentiable function $f$ interpolated by a $n$ th degree polynomial $p(x)$ through the $n+1$ points $x_0,...,x_n$ has error given by $$f(x)-p_n(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^n{(x-x_i)}$$ This seems eerily similar to Taylor's Theorem $$f(x)-T_n(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}$$ for some $\xi\in (a,x)$ . The wiki page http://en.wikipedia.org/wiki/Polynomial_interpolation#Interpolation_error says that ""the remainder term in the Lagrange form of the Taylor theorem is a special case of interpolation error when all interpolation nodes $x_i$ are identical"" but the article it cites for that is https://math.okstate.edu/people/binegar/4513-F98/4513-l16.pdf where I find ""Although this formula for the error is somewhat reminiscent of the error term associated
with an nth order Taylor expansion, this theorem has little to do with Taylor expansions."" Obviously the claim given in the wikipedia article is not justified, but I am not inclined to dismiss such a remarkable resemblance to mere chance. The proof is as follows: write $R(x)$ for the remainder at $x$ and let $$g(t)=R(t)-\frac{R(x)}{W(x)}W(t),\;\;W(t)=\prod_{i=0}^n{(t-x_i)}$$ Then $g(x)=0$ - take $x\in (x_0,x_n)$ - and $g(x_i)=0,\;\;i=0,...,i=n$ because $R(x_i)=0$ . Thus it has $(n+2)$ zeros in the interval and by repeated application of Rolle's Theorem there exists a $\xi$ such that $$g^{(n+1)}(\xi)=f^{(n+1)}(\xi)-\frac{R(x)}{W(x)}(n+1)!=0$$ from which the error formula follows. We can do the same with Taylor's formula. Let $$g(t)=R(t)-\frac{R(x)}{W(x)}W(t),\;\;W(t)=(t-a)^{n+1}$$ Then $g(a)=g(x)=0$ so $\exists c_0\in (a,x): g'(c_0)=0$ . Now $g'(a)=g'(c_0)=0$ so $\exists c_1\in (a,c_0): g''(c_1)=0$ etc. until finally $$\exists\xi: g^{(n+1)}(\xi)=f^{(n+1)}(\xi)-\frac{R(x)}{W(x)}(n+1)!=0\implies R(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-a)^{n+1}$$ This simply cannot be a coincidence - can anyone explanation the connection?","['truncation-error', 'power-series', 'calculus', 'interpolation', 'numerical-methods']"
753623,A connected linearly ordered set is a linear continuum,"Let $X$ be a linearly ordered set in the order topology which is connected. Show that $X$ is a linear continuum. A linear continuum is a linearly ordered set $X$ with more than one element that is densely ordered ( i.e. , between any two members there is another), and has the least upper bound property ( i.e. , every nonempty subset with an upper bound has a least upper bound) I know that $X$ has the intermediate value property because it is connected, but I don't know why it satisfies the least upper bound property.","['general-topology', 'connectedness', 'order-theory']"
753652,How is this subgroup normal?,"Let $G$ be a group, and let $U$ be a subset of $G$. Let $\hat{U}$ be the smallest subgroup of $G$ containing $U$. Then $\hat{U}$ is the intersection of the collection of all the subgroups of $G$ containing $U$. (Right?) This collection is obviously non-empty as $G$ itself is a subgroup of itself which contains $U$. Now my question is this: If $gug^{-1} \in U$ for all $g \in G$ and $u \in U$, then is the subgroup $\hat{U}$ a normal subgroup of $G$? If the answer is yes, then one way of proving this is to find a homomorphism $ \phi \colon G \to G$ with kernel equal to $\hat{U}$. If the answer is no, then what counter-example can we give?","['group-theory', 'abstract-algebra', 'normal-subgroups']"
753661,Difference between conditional and intersection in probability.,"I am having hard time figuring out if it is a conditional probability or an ""and"" probability under the following types of problems. When a student is absent, the probability of the student being sick is .6 In such a sentence, I am not quite sure if the probability is conditional or the probability when the student is sick and absent. Which one would it be? As a matter of fact, is there a rule of thumb to be able to tell if it is conditional or not? I feel as though every time I deal with these kind of problems I get stuck.","['probability', 'actuarial-science']"
753674,Two continuous functions agree on an open subset of an irreducible space.,"I have the varieties $X,Y,Z$ where $X$ is complete. I have the morphism of varieties $f:X\times Y\rightarrow Z$, I have a closed subset $W\subset Y$ such that $f=g\circ\textrm{pr}_Y$ on $X\times(Y\setminus W)$ where $g(y)=f(x_0,y)$ for some $x_0\in X$. Is this sufficient to say that $f=g\circ\textrm{pr}_Y$ on all of $X\times Y$?",['algebraic-geometry']
753685,What is $\mathbb{R}^\mathbb{R}$,I do not know what it is. $\mathbb{R}$ is the set of real numbers. How come $\mathbb{R}\times\mathbb{R}\times \ldots $? Thanks.,"['notation', 'elementary-set-theory', 'real-analysis']"
753695,Inverse of product of matrices,"Let $n>m$ and let $A$ and $B$ be $m\times n$ and $n\times n$ matrices. $B$ is invertible. If $A$ is square ( i.e. $n=m$ ) and invertible, then obviously $$
\left(ABA^T\right)^{-1} = A^{-T}B^{-1}A^{-1}
$$ But, if $A$ is not square, can we say something (assuming that $ABA^T$ is invertible)?","['matrices', 'linear-algebra']"
753769,Is there a formal definition of convergence of series?,"One is often asked to check if a given series converges or not in one's first year of uni. Is there a formal definition that allows us to check this? We are only given a bunch of tests that are troublesome to remember (I've never liked cramming in math), and a Google-search only yields results regarding sequences.","['sequences-and-series', 'calculus']"
753797,Solving a differential equation?,"I'm trying to analyze the transient state of a RC circuit. My book gives me the following differential equation: $$\frac{d(v(t))}{dt} + av(t) = c$$ for some constants $a$ and $c$. The book thens proceeds to solve it, and says that: $$v(t) = K_1 + K_2e^{-t/\tau}$$ for some constants $\tau, K_1, K_2$. We haven't learned differential equations yet, so I wasn't able to follow along the solution of the differential equation. However, doesn't: $v(t) = c/a$ also satisfy the differential equation? Why isn't this a valid solution to the equation? If it is a valid question, what could possibly motivate the book to not include it?","['ordinary-differential-equations', 'calculus', 'physics']"
