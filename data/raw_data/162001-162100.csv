question_id,title,body,tags
2811143,Integration of this expression $ ( u(t)^2 )' \leq 2M u(t)^2 $,"undoubtedly, this is really simple but I don't know how to do this. I'm still learning math so I would be gratefull if you are compassionate to me :) How do you compute this expression please : $$ ( u(t)^2 )' \leq 2M u(t)^2 $$
where $ u \in \mathcal C^1( [a,b[) \ ; \ M \in \mathbb R. $ the result has to be : $$ u(t) ^2 \leq u(a)^2 e^{2M (t - a) } $$ I have any method for expression where you have directly the derivative in it. I don't even know what has to be the boundary of the integral. Is there any method ? If I had to do this, I would either proceed like this : if we replace u by x, we have : $$ ( x^2 )' \leq 2M x^2 \implies x^2 \leq 2/3 M x^3 + cst$$ but
that's surely false... or $$ ( u(t)^2 )' \leq 2M u(t)^2 \implies  \frac{ ( u(t)^2 )' } { u(t)^2
   } \leq 2M   \implies ln(u(t)^2) \leq 2Mt + u(a) ^2 $$","['real-analysis', 'integration', 'ordinary-differential-equations', 'inequality']"
2811188,What does it mean for a ring to have an involution? Are there any examples?,"A ring $R$ is said to be a ring with an involution if there exists a mapping 
$*\colon R  \to  R$ such that for every $a, b \in R$: $a^{**} = a$, $(a + b)^* = b^* +  a^*$, $(ab)^* = b^*a^*$. Can anyone please explain this definition with an example?","['involutions', 'ring-theory', 'commutative-algebra', 'linear-algebra', 'definition']"
2811213,The pigeonhole principle and a professor who knows $9$ jokes and tells $3$ jokes per lecture,"A professor knows $9$ jokes and tells $3$ jokes per lecture.
  Prove that in a course of $13$ lectures there is going to be a pair of jokes that will be told together in at least $2$ lectures. I've started with counting how many possibilities there are to tell jokes in a lecture. Let $$J := \{1,2,\dots,9\}$$ The amount of all different possible combinations for jokes is $9 \choose 3$ and for each lecture there are going to be $3$ unique pairs of jokes $\left(\frac{3!}{2!}=3\right)$. I'm not sure how to continue from here to get to the PHP, I think I might be doing something wrong here, any advice how to abstract it properly? This is an exercise from the Tel-Aviv University entry test preparation and I'm not a student yet so elementry combinatorics should do here.","['permutations', 'combinatorics', 'pigeonhole-principle', 'combinations']"
2811239,Prove that $1+\frac{1}{2}+\frac{1}{3}+...+\frac{1}{2^n-1}<n$ for $n\geq{2}$,"I tried using mathematical induction to prove this, but the problem I faced was that there are a lot of numbers between $\frac{1}{2^k-1}$ and $\frac{1}{2^{k+1}-1}$. Is it possible to prove this with induction or is there a better method?","['inequality', 'harmonic-numbers', 'sequences-and-series', 'analysis', 'discrete-mathematics']"
2811244,Estimator for $\theta$ using the method of moments,"Exercise : Using the method of moments, find the estimator for $\theta$ for a random sample $X_1, \dots, X_n$ that follows the distribution with pdf $f(x) = \theta x^{-2}, \; 0 < \theta \leq x < \infty$, where $\theta$ is the unknown parameter. Attempt : $$m_1' = \bar{X}$$ $$μ_1' = \int_\theta^xu\theta u^{-2}\mathrm{d}u=\int_\theta^x\theta u^{-1}\mathrm{d}u=\big[\theta \ln u\big]_\theta^x =\theta\ln x - \theta\ln\theta$$ Thus, the estimator is given by : $$m_1' = μ_1' \Rightarrow \theta\ln x - \theta\ln\theta = \bar{X}$$ But we cannot solve that equation with respect to $\theta$, so we'll take the moments of bigger order : $$m_2' = \frac{1}{n}\sum_{i=1}^nX_i^2$$
$$μ_2' = \int_\theta^x u^2 \theta u^{-2} \mathrm{d}u = \int_\theta^x \theta \mathrm{d}u = \big[\theta u\big]_\theta^x =\theta x - \theta ^2 $$ Thus the solution of the following quadratic equation for $\theta >0$, gives as the estimator of $\theta$ : $$\theta^2-x\theta +\frac{1}{n}\sum_{i=1}^nX_i^2$$ Question : Is my solution correct ? Shall the upper bound be $x$ or $\infty$ ?","['parameter-estimation', 'statistics', 'probability', 'probability-distributions']"
2811249,Derivative of the nuclear norm ${\left\| {XA} \right\|_*}$ with respect to $X$,"The nuclear norm (also known as trace norm) is defined as \begin{equation}
{\left\| M \right\|_*} = \mbox{tr} \left( {\sqrt {{M^T}M} } \right) = \sum\limits_{i = 1}^{\min \left\{ {m,n} \right\}} {{\sigma _i}\left( M \right)} 
\end{equation}
where ${\sigma _i}\left( M \right)$ denotes the $i$-th singular value of $M$. My question is how to compute the derivative of ${\left\| {XA} \right\|_*}$ with respect to $X$, i.e.,
\begin{equation}
\frac{{\partial {{\left\| {XA} \right\|}_*}}}{{\partial X}}
\end{equation}
In fact, I want to use it for the gradient descent optimization algorithm. Note that there is a similar question , according to which the sub-gradient of ${\left\| X \right\|_*}$  is $U{V^T}$, where $U\Sigma {V^T}$ is the SVD decomposition of $X$. I hope this is helpful. Thanks a lot for your help.","['derivatives', 'nuclear-norm', 'matrices', 'matrix-calculus', 'linear-algebra']"
2811251,Actual example of tensor contraction,"So I'm having trouble to compute tensor contractions with ""actual"" numbers from the matrix representations of the tensors. I have only seen abstract theoretical examples on the internet so I'm asking for a bit of help on how to find the contractions given the expressions of the tensors and the pair of indices where we will carry out the contraction. I'll show a simple example and I hope you can help me. Let's suppose we have two tensors of the type (1,1) (that means 1 contravariant, 1 covariant). They will be called Y and Z and knowing their coordinate forms, we can represent them through matrices in this way: Y = \begin{pmatrix}1&-1\\2&3\end{pmatrix} Z = \begin{pmatrix}-1&0\\1&2\end{pmatrix} Now, we could compute the Kronecker product Y  x  Z in order to get a type (2,2) tensor (2 contravariant, 2 covariant). The result would be: \begin{pmatrix}-1&0&1&0\\1&2&-1&-2\\-2&0&-3&0\\2&4&3&6\end{pmatrix} So, how could we get the contractions of this (2,2) tensor (with actual numbers instead of the parameters that you see in other examples) ?.
Note that here we have 2 contravariant indices and 2 covariant indices so there are 4 possible contractions depending on the pair of indices that you choose, I guess.
As far as I know, all possible contractions with the selection of pairs of indices are the following: - 1st covariant index and 1st contravariant index. - 1st covariant index and 2nd contravariant index. - 2nd covariant index and 1st contravariant index. - 2nd covariant index and 2nd contravariant index. So what matrices would be the result of those contractions. Just in case I didn't state it clearly before, these matrices are the representation of tensor of which we know the coefficients of their coordinate forms (explicit forms in a specific basis, and that info is known). So we want to perform contractions in the last 4x4 matrix that I showed (which represents a (2,2) tensor) and there a 4 different possibilities. My question has to do with the fact that I can't find a way to do this with actual numbers and I can't figure out the result for each possible contraction (I also don't know what the differences would be in the calculation of a contraction with a certain pair of indices or another). I would really appreciate that someone could find a specific result for the contractions that I proposed (I guess the calculation is easy but I just don't know how it could be done). Thank you really much for reading.","['tensor-products', 'tensors', 'tensor-decomposition', 'tensor-rank', 'linear-algebra']"
2811255,Regarding Research in Artificial Intelligence [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 6 years ago . Improve this question Which mathematical areas are important for research purposes in artificial intelligence? Specifically, If I have Masters in Statistics how much it will be beneficial for research in artificial intelligence?","['statistics', 'artificial-intelligence']"
2811279,Prescribing the dimension of intersections of sub-vector spaces,"Let $n$ be a positive integer. For each subset $S$ of $\{1,\dots,n\}$ let $d_S$ be a nonnegative integer. Assume that the $(d_S)$ satisfy:
$$
S\subset T\implies d_S\ge d_T,
$$ 
$$
d_{S\cap T}\ge d_S+d_T-d_{S\cup T}
$$ 
for all $S,T\subset\{1,\dots,n\}$. [In this post $X\subset Y$ denotes what some people designate by $X\subseteq Y$.] Is there a tuple $(V,V_1,\dots,V_n)$, such that $V$ is a finite dimensional $\mathbb Q$-vector space $V$ and the $V_i$ are sub-vector spaces of $V$, satisfying 
$$
\dim\left(\bigcap_{s\in S}V_s\right)=d_S
$$ 
for all $S\subset\{1,\dots,n\}\ ?$ EDIT 1. The conditions 
$$
S\subset T\implies d_S\ge d_T,
$$ 
$$
d_{S\cap T}\ge d_S+d_T-d_{S\cup T}
$$ 
for all $S,T\subset\{1,\dots,n\}$ stated above are necessary. The first one follows from the fact that the dimension of a subspace can't exceed the dimension of the ambient space, the second one follows from the first one coupled to the equality $(\star)\quad\dim(U+W)=\dim U+\dim W-\dim(U\cap W)$ for finite dimensional subspaces $U$ and $W$. There is an similar question for finite sets, with cardinalities instead of dimensions; that is, one can ask which families of nonnegative integers indexed by subsets of $\{1,\dots,n\}$ come (in the obvious sense) from a tuple $(V,V_1,\dots,V_n)$ of finite sets . In this setting, we still have the trivial obstruction that the cardinality of a subset can't exceed the cardinality of the ambient set, but there are other obstructions, given by the inclusion-exclusion principle for $k$ finite subsets, for all $k$ varying from $2$ to $n$. In the case of finite dimensional vector spaces, however, the inclusion-exclusion principle holds for $k=2$ (see $(\star)$), but not for $k\ge3$: see Sebastien Palcoux's posts in this thread (see also the references therein). It seems that (for dimensional vector spaces) no obstructions other that the trivial one and the one coming from $(\star)$ have been found so far. This is perhaps because they don't exist, but, for the time being, it seems that the existence of subtler obstructions can't be ruled out. The ground field has been chosen to be $\mathbb Q$, but this was only for the sake of concreteness. I'd be very interested in any result involving another field. EDIT 2. In fact, if the ground field is finite, there are obvious obstructions coming from the fact that the cardinality of a union of subspaces can't exceed the cardinality the ambient space. To avoid this kind of complications, let's assume that ground field is infinite . EDIT 3. Here is a proof of the existence of $V$ and the $V_i$ in the case $n=3$ (assuming that the ground field is infinite). Let $K$ be the ground field, which is assumed to be infinite . For each set $Z$ let $|Z|$ be the cardinality of $Z$, and let $KZ$ be the free vector space over $Z$. If there are sets $Z,Z_1,Z_2,Z_3$ such that 
$$
\left|\bigcap_{i\in S}Z_i\right|=d_S
$$ 
for all $S\subset\{1,2,3\}$, we solve our problem by applying the functor $X\mapsto KX$. So we can assume from now on that no such sets exist. There are still sets $Z_1,Z_2,Z_3$ such that 
$$
\left|\bigcap_{i\in S}Z_i\right|=d_S
$$ 
for all nonempty subset $S$ of $\{1,2,3\}$. We define $Z$ by $Z:=Z_1\cup Z_2\cup Z_3$. Setting $r:=|Z|-d_\varnothing$, we get 
$$
1\le r\le|Z\setminus(Z_i\cup Z_j)|
$$ 
for all $i,j$. As $K$ is infinite, there is an $r$ dimensional subspace $W$ in $KZ$ such that $W\cap(K\,(Z_i\cup Z_j))=0$ for all $i,j$. Let $\pi:KZ\to(KZ)/W$ be the canonical projection. Then it suffices to set 
$$
V:=(KZ)/W,\quad V_i:=\pi(KZ_i).
$$","['abstract-algebra', 'linear-algebra', 'matroids']"
2811323,About a metric space,"If we define a metric $d(x,y)=\begin{cases}|x|+|y|,&\text{if }x\neq y\\0,&\text{if }x=y\end{cases}$ on $\mathbb{R}$, which of following options are true? Every compact set is finite, $\mathbb{N}$ is open, $(\mathbb{R},d)$ is connected, $\mathbb{Z}$ is open. I think we have $ B_r(n)=\{ n\}, n\in\mathbb{N}, r <n$ and $ B_r(0)=(-r,r)$  (note that $ B_r(x_0)= \{y\in\mathbb{R} | d(y,x_0)<r \}$) so $ \mathbb{N}$ is open and $ \mathbb{Z}$ is not open. $(\mathbb{R},d)$ is not connected because $ \mathbb{N}$ is open and close. Note that $ k(x,y)=\lvert x+y\rvert  \le \lvert x\rvert+\lvert y\rvert =d(x,y) $ then every open set in $(\mathbb{R},k)$ is open in $(\mathbb{R},d)$. Now is a compact set in $(\mathbb{R},k)$ also compact in $(\mathbb{R},d)$ ?","['general-topology', 'metric-spaces', 'compactness', 'connectedness']"
2811340,Origin of double cosets and Mackey formula as Beck-Chevalley condition,"I recently met the Mackey double coset formula for the first time. After asking around I was told that the origin of double cosets is the following fact. Fact. Let $G_1,G_2\leq G$ be subgroups. Consider the 2-pullback of the diagram $\mathbf BG_1 \rightarrow \mathbf BG\leftarrow \mathbf BG_2$. The 2-pullback is a groupoid; Its connected components are in natural bijection with the double cosets $G_1\!\!\setminus \!G /G_2 $. Question 1. Where can I find a reference for this? (Also, is the 2-pullback meant to be strict? Perhaps it does not matter...) Snooping around the nlab page on double cosets I read that the Mackey double coset ""amounts to the Beck-Chevalley condition for the homotopy pullback square."" Question 2. What is a maximally readable reference for this viewpoint? I have only seen the Beck-Chevalley condition in the context of 1-categorical (bi)fibrations, and only for the purpose of monadic descent. I don't know any $\infty$-category theory.","['homotopy-theory', 'reference-request', 'representation-theory', 'category-theory', 'group-theory']"
2811370,Proving $A \cup B \cup C = (A-B)\cup(B-C)\cup(C-A)\cup(A \cap B \cap C)$ - not sure why this reasoning is failing.,"This seems like a fairly elementary problem but I am not sure where my reasoning is incorrect, whether it's down to a simple error in Boolean algebra or if I'm misunderstanding something else. We want to show that $A \cup B \cup C = (A-B)\cup(B-C)\cup(C-A)\cup(A \cap B \cap C)$. Let $x \in A \cup B \cup C$. Then: $(x \in A) \lor (x \in B) \lor (x \in C) \\
\leftrightarrow (x \in A \land \mathbf T) \lor (x \in B  \land \mathbf T) \lor (x \in C  \land \mathbf T) \\
\leftrightarrow (x \in A \land (x \in B \lor x\notin B)) \lor (x \in B  \land (x \in C \lor x\notin C)) \lor (x \in C  \land (x \in A \lor x\notin A)) \\
\leftrightarrow ((x \in A \land x \in B) \lor (x\in A \land x\notin B)) \lor ((x \in B \land x \in C) \lor (x\in B \land x\notin C)) \lor ((x \in C \land x \in A) \lor (x\in C \land x\notin A)) \\
\leftrightarrow (x \in A \land x \in B) \lor (x\in(A-B)) \lor (x \in B \land x \in C) \lor (x\in (B-C)) \lor (x \in C \land x \in A) \lor (x\in (C- A)) \\
\leftrightarrow x\in(A-B) \lor x\in (B-C) \lor x\in (C- A) \lor (x \in A \land x \in B) \lor(x \in B \land x \in C) \lor (x \in C \land x \in A)$ ...which almost works, but obviously $(x \in A \land x \in B) \lor(x \in B \land x \in C) \lor (x \in C \land x \in A) \neq x\in A \land x\in B \land x\in C$. I suspect I am erroneously equivocating some Boolean algebra operation onto set operations.","['proof-verification', 'logic', 'elementary-set-theory', 'discrete-mathematics']"
2811384,formula for $\sum_{n = 1}^{\infty}{\frac{1}{a^n+b^n}}$,"Is there some sort of formula for the infinte sum:
\begin{equation}
\sum_{n = 1}^{\infty}{\frac{1}{a^n+b^n}}
\end{equation} (one can assume $a,b>1$) What I've got so far : $$\sum_{n = 1}^{\infty}{\frac{1}{a^n+b^n}}\le\sum_{n = 1}^{\infty}{\frac{1}{a^n}}=\frac{1}{a-1}<\infty$$ Therefore, the series converges. if:
\begin{equation}
f(a,b)=\sum_{n = 1}^{\infty}{\frac{1}{a^n+b^n}}
\end{equation}
then: $f(a,b)=f(b,a)$ $f(a,0)=\sum_{n = 1}^{\infty}{\frac{1}{a^n+0^n}}=\sum_{n = 1}^{\infty} {\frac{1}{a^n}}=\frac{1}{a-1}$ $f(a,a)=\sum_{n = 1}^{\infty}{\frac{1}{a^n+a^n}}=\sum_{n = 1}^{\infty}{\frac{1}{2a^n}}=\frac{1}{2}\sum_{n = 1}^{\infty}{\frac{1}{a^n}}=\frac{1}{2}\cdot\frac{1}{a-1}$ For every constant $b$, $\lim_{a\to\infty}{f(a,b)}=0$ Also: \begin{equation}
\sum_{n = 1}^{\infty}{\frac{1}{a^n+b^n}}=\sum_{n = 1}^{\infty}{\prod_{m=0}^{n-1}{\frac{1}{a-b\cdot e^{\frac{1+2m}{n}\pi i}}}}
\end{equation} Maybe that would help... Does anyone know the answer? EDIT: I calculated the first few values: $f(1, 0)=\infty$ $f(2, 0)=1.0$ $f(2, 1)=0.7644997803484442$ $f(3, 0)=0.5$ $f(3, 1)=0.40406326728086184$ $f(3, 2)=0.32135438719750625$ $f(4, 0)=0.3333333333333333$ $f(4, 1)=0.27940026240596016$ $f(4, 2)=0.2355002196515558$ $f(4, 3)=0.1978825074467063$",['sequences-and-series']
2811389,I roll three dice and keep only the largest two,"I know that rolling two 6-sided dice I will get 7 as medium result. But what if I roll three dice and get only the largest two? How much is that going to increase the medium result? And what about with 4 dice, 5 etc...? Thank you!","['statistics', 'probability', 'dice', 'probability-distributions']"
2811398,How do I calculate how many ways 14 non-attacking bishops can be placed on a chessboard?,"It's possible to place up to 14 non-attacking bishops on a chessboard. How can I calculate the number of valid configurations, without writing a program to brute force it? I've checked Non Attacking Chess Pieces , but it doesn't cover the variation I'm interested in, and a quick Google hasn't yielded useful results. UPDATE: Proof that only 14 bishops can be placed: If we divide the chessboard into diagonals, and we treat diagonal 1 and 15 as the same diagonal, then the maximum number of bishops per diagonal is 1, therefore 14. Here is an example configuration of 14 bishops to show it's possible: NOTE: Not a duplicate of this question because I am asking about the number of arrangements of 14 bishops, not the maximum number of bishops on a chess board.","['combinatorics', 'recreational-mathematics']"
2811458,Exchangeable matrix,"An infinite random sequence $(X_i)_{i \ge 1}$ is defined exchangeable if its distribution is invariant to finite permutations of the indexes, that is
$$(X_i)_{i \ge 1} \overset{d}{=} (X_{\sigma(i)})_{i \ge 1},$$
for any finite permutation $\sigma$ of $\mathbb{N}$. An infinite random matrix $(X_{ij})_{ij}$ is defined row-column exchangeable (RCE) if its distribution is invariant to two different permutations of the row indexes and column indexes, that is $$(X_{i j})_{ij} \overset{d}{=} (X_{\sigma(i) \sigma^{\prime}(j)})_{ij},$$ for any pairs of finite permutations $\sigma$ and $\sigma^{\prime}$ of $\mathbb{N}$. This is equivalent to ask that the sequence of rows $(X_{i\cdot})_{i \ge 1}$ are exchangeable and also the sequence of the columns $(X_{\cdot j})_{j \ge 1}$, see Aldus1983 . Question: if fixed an arbitrary row $i \in \mathbb{N}$, $(X_{ij})_{j \ge 1}$ is an exchangeable sequence and fixed an arbitrary column $j \in \mathbb{N}$, $(X_{ij})_{i \ge 1}$ is an exchangeable sequence, the random matrix $(X_{ij})_{ij}$ is row-column exchangeable? Guess: I believe that it is false. More precisely, RCE implies immediately exchangeability inside an arbitrary row and inside an arbitrary column, but I think that from the marginal assumption we can not recover the global one, but I can not find a counterexample.","['independence', 'probability-theory', 'probability']"
2811476,Cool way of finding $\cos\left(\frac{\pi}{5}\right)$,"while trying to solve a problem, I stumbled upon a way of finding $\cos\left(\frac{\pi}{5}\right)$ using identities and the cubic formula. Is it possible to find other values of sine or cosine in a similar way ? Consider
$$\cos\left(\frac{\pi}{5}\right) - \cos\left(\frac{2\pi}{5}\right).$$
Using the difference of cosines identity, we have
$$\cos\left(\frac{\pi}{5}\right) - \cos\left(\frac{2\pi}{5}\right) = -2\sin\left(\frac{3\pi}{10}\right)\sin\left(-\frac{\pi}{10}\right).$$ Now we change the RHS using the identity $\sin(x) = \cos\left(\frac{\pi}{2}-x\right)$ and the fact that $\sin(x)$ is odd. $$-2\sin\left(\frac{3\pi}{10}\right)\sin\left(-\frac{\pi}{10}\right) = 2\cos\left(\frac{\pi}{5}\right) \cos\left(\frac{2\pi}{5}\right)$$
So,
$$\cos\left(\frac{\pi}{5}\right) - \cos\left(\frac{2\pi}{5}\right) = 2\cos\left(\frac{\pi}{5}\right) \cos\left(\frac{2\pi}{5}\right)$$ Now we make use of the identity $\cos(2x)=2\cos^2(x)-1$.
$$\cos\left(\frac{\pi}{5}\right) - 2\cos^2\left(\frac{\pi}{5}\right)+1 = 2\cos\left(\frac{\pi}{5}\right) \left(2\cos^2\left(\frac{\pi}{5}\right)-1\right)$$
Let $y=\cos\left(\frac{\pi}{5}\right)$ and we have
$$y-y^2+1=2y(2y^2-1)$$
$$4y^3+2y^2-3y-1=0$$
which has the correct solution
$$ y=\frac{\sqrt{5}+1}{4} =\cos\left(\dfrac{\pi}{5}\right) $$
One of the roots is also $\sin\left(\dfrac{\pi}{10}\right)$ which I'm guessing is because you end up with the same cubic if you apply the above to sin too.","['algebra-precalculus', 'trigonometry']"
2811524,Help me to understand the definition of orientation forms,"Can anyone clarify the definition of orientation forms on a smooth manifold $M^n$ ? I'm reading Lee's book ""Introduction to smooth manifolds"" (second edition) on pg. 381, and I am confused about how I verify that a differential $n$ -form on $M$ is a form of orientation. Is an $n$ -form $\omega$ an orientation form when $\omega_p$ is not a null $n$ -covector for all $p\in M$ or $\omega_p(v_1,...,v_n)\neq0$ for all $\{v_1,...,v_n\}$ basis of $T_p M$ ?","['manifolds', 'differential-forms', 'smooth-manifolds', 'differential-geometry']"
2811530,On the Normality of the Sum of Two Normal Operators,"It is well kown that Theorem: Let $A$ and $B$ be two normal operators. If $A$ commutes with $B$, then
  $A + B$ is normal. Indeed the proof follows by using the Fuglede theorem since the commutativity of $A$ and $B$ implies the commutativity of $A$ and $B^*$. The converse of this theorem is not always true. I hope to construct
  a counterexample.","['functional-analysis', 'examples-counterexamples', 'operator-theory', 'normal-operator']"
2811562,"Proof that if $f:S^{n} \to \mathbb{R} $ is continuous, then is not injective","I have to prove that if $f:S^{n} \to \mathbb{R} $ (where $S^{n}= \{(x_1, ..., x_{n+1})\in\mathbb{R}^{n+1} |  x_1^2+...+x_{n+1}^2=1\} $ is continuous, then is not injective. If possible, I would like it to be proven by using connectivity arguments. My attempt: suppose $f$ is injective. Let $p\in f(S^n)$ and, since $f$ is injective, there exists only one $q\in S^n$ such that $f(q)=p$. We consider now $f_{|S^n \setminus\{q\}}:{S^n \setminus\{q\}} \to f(S^n\setminus\{q\})=f(S^n)\setminus\{p\} $, which is continuous and bijective (because $f$ was continuous and injective). Then, since ${S^n \setminus\{q\}}$ is path-connected and $f$ is continuous and exhaustive, we have that $f(S^n)\setminus\{p\}$ is path-connected as well.  Here I would like to use that $\mathbb{R}\setminus \{p\}$ is not path-connected and arrive at a contradiction, but this is wrong.","['continuity', 'general-topology', 'connectedness']"
2811595,Is $[G_p \cap G_q:G_{pq}]$ always finite?,"Suppose $G$ is a group, $G_n = \langle\{g^n| g \in G\}\rangle$. Suppose $p$ and $q$ are coprime integers. It is not hard to notice, that $G_{pq} \leq G_p \cap G_q$ (""$\leq$"" sign means here ""Is a subgroup of"") and there are cases where inequality holds ($G_{pq}$ is a proper subgroup). For example in a free group $G = F[a, b]$ the element $a^3(ab^2)^3$ is an element of both $G_2$ and $G_3$ but not of $G_6$. However, I failed to find any example where $G_{pq}$ is of infinite index in $G_q \cap G_p$. The proof that there isn't one, didn't come to my mind either.  And thus I am asking a question:
Is the index of $G_{pq}$ in $G_p \cap G_q$ always finite? Any help will be appreciated.","['abstract-algebra', 'group-theory']"
2811615,random variable takes only rational values with probability one,"I have found an old exercise that seems very interesting: let $X_{1},X_{2},...$ be i.i.d. bernoullian random variables with $\mathbb{P}(X_{n}=1) = \mathbb{P}(X_{n}=0) = 1/2$. Define $S_{n} := X_{1} + ... + X_{n}$. It is to show that the random variable $$
M := \sup_\limits{n \in \mathbb{N}}\frac{S_{n}}{n}
$$
with probability $1$ only takes rational values in the intervall $(1/2,1]$. Anyone has an idea, how to prove it?","['probability-theory', 'probability']"
2811672,"Prove that for any two sets $A$ and $B$, $A\notin B$ or $B\notin A$","A quick preface to the below question: this is my first post on math.se! I am excited to begin participating in such a wonderful community. Any feedback as to how I can improve this or subsequent posts is certainly feedback from which I could benefit. ( Exercise 3.2.2 of Tao's Analysis I ) Use the axiom of regularity (and the singleton set axiom, which guarantees the existence of the singleton set) to show that if $A$ is a set, then $A\notin A{. \kern 0.05em}^\text{1}$ Furthermore, show that if $A$ and $B$ are sets, then $A\notin B$ or $B\notin A$. Thoughts: Consider some set $A$ and the singleton $\{A\}$. By the axiom of regularity, the only element of $\{A\}$, namely $A$, is not a set or disjoint from $\{A\}$. Clearly, $\{A\}\cap A=\emptyset$ and thus, $A\notin A$. Moreover, suppose $B$ is some other set such that $\neg(A\notin B \lor B\notin A)$, i.e. $A\in B\land B\in A$. This implies, in very vauge notation, that $A=\{\{A,\dots\},\dots\}$ and $B=\{\{B,\dots\},\dots\}$. Can I get a contradiction out of this? In the case of $A$, we get that $x\in\{A,\dots\}\lor x\in \{a\}$ for some $x\in A$ and all other objects $a\in A$. How can I prove that there exists no element of $A$ (and $B$) that is not a set unless it is disjoint from $A$. I would greatly appreciate any hints as to how I can complete my argument. 1 This first problem has been discussed on this site times before (see here and here ). I include a solution only because the context demands it, but to avoid a duplicate question, I ask that it not be discussed here.",['elementary-set-theory']
2811680,Solution of the functional equation $f(x) + f(y) =f\left(x\sqrt{1-y^2 }+y\sqrt{1-x^2 }\right) $,"If $$f(x) +  f(y) =f\left(x\sqrt{1-y^2 }+y\sqrt{1-x^2 }\right)\text,$$ prove that $$f(4x^3 -3x) + 3f(x) =0\text.$$ I started by substituting $x = y$ , in the expression and I get $$2f(x)=f\left(2x\sqrt{1-x^2}\right)\text.$$ Then, I also substitute $y = \sqrt{(1-x^2)}$ and I get $$f(x) +f\left(x\sqrt{1-x^2}\right) = 0\text.$$ How shall I proceed further and solve this problem?","['functions', 'functional-equations']"
2811683,"How many trees over ${1,2,3,...n}$ with conditions","I’m stuck on this question in graph theory.
The question is: How many labeled trees are there over $V={0,1,2,...n}$
with which vertices 1,2,3 are leaves, and distance between any two of these leaves is 3 or more. I tried using Cayley theorem but I don’t know how to apply it in this specific question.","['combinatorics', 'graph-theory', 'trees']"
2811685,Lifting idempotents modulo an ideal and its radical,"The following result is a direct consequence of Proposition 27.1 of F. W. Anderson and K .R. Fuller's ""Rings and categories of modules"". But I cannot prove it. Is there any hint? Definition: Let $A$ be an ideal in a ring $R$ and let $g+A$ be an idempotent element of $R/A$. We say that this idempotent can be lifted  modulo $A$ in case there is 
an idempotent $e^2=e\in R$ such that $g + A = e + A$. Fact: Let $I$ be an ideal of a commutative ring with identity. Then every idempotent of $R/I$ can be lifted modulo $I$ if and only if every idempotent of $R/\sqrt{I}$ can be lifted modulo $\sqrt{I}$, where $\sqrt{I}$ is the radical of $I$ in $R$.","['idempotents', 'algebraic-geometry', 'commutative-algebra']"
2811709,Differential Equation with y(-x),"Please how can I solve
$$y''(x)+y'(-x)=e^x$$ I tried everything I  could I can't even find the complementary solution
Any help would be gladly appreciated Thanks In Advance","['parity', 'ordinary-differential-equations']"
2811751,Is $[G_p \cap G_q:G_{pq}]$ always finite? v2.0,"Suppose $G$ is a finitely generated group. $G_n = \langle\{g^n| g \in G\}\rangle$. Suppose $p$ and $q$ are coprime integers. Is the index of $G_{pq}$ in $G_p \cap G_q$ always finite? I have recently asked a similar question about arbitrary groups and received an infinitely generated counterexample: Is $[G_p \cap G_q:G_{pq}]$ always finite? However, the question, whether the statement is true under this additional condition, or is there a finitely generated counterexample, seems to be quite interesting and still remains unanswered.","['abstract-algebra', 'group-theory', 'finitely-generated']"
2811777,Necessary and sufficient condition for diagonalizable matrix,"Let $A\in M_n(\mathbb R)$ and $B=\begin{pmatrix}A & A\\0 & A\end{pmatrix}$. Prove that $B$ is diagonalizable if and only $A = 0$. We see that for any polynomial $P$, $P(B) = \begin{pmatrix}P(A) & AP'(A)\\0 & P(A)\end{pmatrix}$ where $P'$ is the derivative of P $B$ is diagonalizable $\iff \exists P$ a polynomial with single roots which annihilates B Then B diagonalizable if and only if A diagonalizable because $P(A)=0$. 
A annihilates the polynomial $XP'$ too. From there, I don't see how to prove that $A=0$","['matrices', 'diagonalization', 'block-matrices', 'linear-algebra']"
2811800,Why should I take the time order to solve this differential equation?,In a textbook on quantum field theory I come across the following differential equation. $$ i \partial_t U(t) = H(t) U(t) $$ I would say that the solution to this equation would be $$U(t) = e^{-i \int_0^t dt H(t) }$$ Since $$ \partial_t e^{-i \int_0^t dt H(t) } =  \partial_t ( -i\int_0^t dt H(t))e^{-i\int_0^t dt H(t) } =  -i H(t) e^{-i\int_0^t dt H(t) } = -i H(t) U(t)$$ exactly as desired. However the solution in the textbook states: $$ T(e^{-i\int_0^t dt H(t) })$$ where T stands for the time order parameter. In other words the solution should be $$ U(t) = 1 -i  \int_0^t dt_1 H(t_1)  + \frac{1}{2}(-i)^2  \int_0^{t} \int_0^{t} dt_t dt_2 T(H(t_1)H(t_2)) + \cdots $$ where $T(H(t_1)H(t_2))$ equals $H(t_1)H(t_2)$ if $t_1<t_2$ and $H(t_2)H(t_1)$ otherwise. So why is this the correct solution? What goes wrong in the reasoning above?,['ordinary-differential-equations']
2811847,Maximum Likelihood Estimator for Poisson Distribution,"Studying for my upcoming exams I came upon this weird MLE exercise : Let the random variable $X$ follow the Poisson Distribution with unknown parameter $\theta >0$. In $50$ observations of $X$, you only know that $20$ of them are zero. Find the Maximum Likelihood Estimator of $\theta$ using only this fact. How would one proceed to finding the MLE using the fact that you have $20$ zero observations ? I have never came upon such a problem of MLE.","['probability-distributions', 'maximum-likelihood', 'statistics', 'probability', 'poisson-distribution']"
2811902,every nonsurjective continuous function from $S^2$ to $S^2$ there exist a fixed point?,"can someone please help me to show for every nonsurjective continuous function from $S^2$ to $S^2$ there exist a fixed point?
i think since the fuction is not surjective it doesn't contain at least one point of $S^2$ so it's like a function from $S^2$ to $R^2$ and by Borsuk-Ulam there exist a point x s.t. f(x)=f(-x) but i'm not sure if this can help...","['algebraic-topology', 'fixed-point-theorems', 'functions']"
2811913,Dodecahedron volume,"How can I derive the volume of a regular dodecahedron? A. P. Kiselev's Stereometry suggests that we cut it into a cube and congruent ""tent-like"" solids. I'm finding some difficulty to compute the volume of these ""tent-like"" solids.","['polyhedra', 'geometry']"
2811946,Uniqueness of solutions of Cauchy problem with nonzero function,"Consider the initial value problem
$$y'(x)=f(x,y(x)), \\ y(x_0)=y_0,$$
where the function $f \colon D \to \mathbb R$ is defined and continuous on some open set $D \subseteq \mathbb R \times \mathbb R$ and $(x_0, y_0) \in D$. Is the following statement true? This problem cannot have two distinct solutions on some interval $[x_0, x_1]$ if $$\forall (x,y) \in D \colon f(x, y) \ne 0.$$ If $f$ does not depend on $x$, the answer seems to be positive, i.e. we have some kind of uniqueness theorem here, but my intuition tells me that it is, generally, wrong. Can you provide a counterexample?","['cauchy-problem', 'ordinary-differential-equations']"
2811948,Generating a System of Recurrences from a Light Bulb Word Problem,"Question A row of $n$ light bulbs must be turned on. Initially they are all off. The first bulb can always be turned on or off. For $i> 1$, the $i$th bulb can be switched (turned on or off) only when $i-1$ is on and all other earlier bulbs are off. Let $a_n$ be the number of switches needed to turn all light bulbs on and let $b_n$ be the number of switches needed to turn on the $n$th bulb for the first time. Find recurrences for $a_n$ and $b_n$. The above question is from Aigner's A course in Enumeration . My Attempt The initial conditions are $a_0=b_0=0$ and computing the first couple of values gives us for example $a_1=b_1=1$, $a_2=b_2=2$, and $a_3=5;\, b_3=4$. We note that $a_n=b_n+a_{n-2}$ for $n\geq 2$. Indeed, to switch on all lights we must first switch on the $n$th light for the first time (and after doing so light $n-1$ and $n$ are on, all others off) and then switch on the reamining $n-2$ lights. I was unable to come up with a recurrence for $b_n$. My Problem I looked in the back of the book and the following recurrence for $b_n$ is given 
$$
b_n=a_{n-1}+a_{n-2}+1\quad (n\geq 2)\tag{1}
$$ How does one explain the recurrence above (combinatorially) ? My idea is that to get to the state that light $n$ is on for the first time it must be the case that we get to the state when only $n-2$ and $n-1$ are on and then we turn off $n-2$ and turn on $n$. But I am unable to relate it to the number of switches to have an initial segment of lights on. Any help is appreciated.","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
2811951,Why is it important for a matrix to be square?,"I am currently trying to self-study linear algebra. I've noticed that a lot of the definitions for terms (like eigenvectors, characteristic polynomials, determinants, and so on) require a square matrix instead of just any real-valued matrix. For example, Wolfram has this in its definition of the characteristic polynomial: The characteristic polynomial is the polynomial left-hand side of the characteristic equation $\det(A - I\lambda) = 0$, where $A$ is a square matrix. Why must the matrix be square? What happens if the matrix is not square? And why do square matrices come up so frequently in these definitions? Sorry if this is a really simple question, but I feel like I'm missing something fundamental.","['matrices', 'linear-algebra']"
2811990,Calculation to show $|\mathrm{d}r|^2_{\bar g} = 1$ implies sectional curvatures tend to $-1$.,"$\textbf{tl;dr:}$ Given that $r$ is a definining function for the boundary of a conformally compact manifold, how does one show that the sectional curvatures tend to $-1$ if $|\mathrm{d}r|^2_{\bar g} = 1$ on the boundary? $\textbf{Definitions:}$
Let $(M, g)$ be a pseudo-Riemannian manifold. Suppose $M$ can be identified with the interior of a smooth compact manifold with boundary $\overline M$. Let $\Sigma$ denote $\partial M$ so that $\overline M = \Sigma \sqcup M$. We say that $r$ is a defining function for $\Sigma$ if $r: M \to \mathbb{R}$ is smooth, $\mathcal Z(r) = \Sigma$, and $\mathrm{d}r \neq 0$ on $\Sigma$. ($\mathcal Z(r)$ denotes the zero locus of $r$, that is, the set of points in $M$ where $r$ vanishes.) We say that $(M, g)$ is conformally compact if there is a defining function $r$ for $\Sigma$ such that
$$\bar g = r^2 g$$
on $M$, and $g$ is a metric on $\overline M$. We say that $g$ is asymptotically hyperbolic if $|\mathrm{d}r|_{\bar g}^2 = 1$. Question: My interpretation of asymptotically hyperbolic is that the sectional curvatures should tend to $-1$, so I set out to do some calculations. Does $|\mathrm{d}r|_{\bar g}^2 = 1$ really ensure that sectional curvatures tend to $-1$? I start by using the conformal transformation rule for the Riemann curvature, and write
$$\bar R_{ijkl} = r^2(R_{ijkl} + (\Upsilon_{ij} - \Upsilon_i \Upsilon_j + \frac{1}{2}\Upsilon^2 g_{ij})\circledast g_{kl})$$
where I have used $\circledast$ to denote the Kulkarni-Nomizu product. Note that $\Upsilon_{i} = r^{-1}\partial_{i}r$. I now proceed to write each term on the right as $r^kA_{ijkl}$ where $A_{ijkl}$ is bounded (in the sense that each component of the tensor is finite). First we note the following:
\begin{align*}
\Upsilon_{ij} &= \nabla_{i}\Upsilon_j = \nabla_i(r^{-1}\partial_j r) = r^{-1}\Phi_{ij} - r^{-2}\Phi_i \Phi_j\\
\Upsilon_i \Upsilon_j &= r^{-2}\Phi_i \Phi_j\\
\Upsilon^2 &= r^{-2}g^{ij}\Phi_i \Phi_j = r^{-2}r^{2}\bar g^{ij}\Phi_i\Phi_j = |\mathrm{d}r|^2_{\bar g}
\end{align*}
where $\Phi_i = \partial_i r$. Therefore, we have
\begin{align*}
R_{ijkl}
&= r^{-2}\bar R_{ijkl} - (\Upsilon_{ij} - \Upsilon_i \Upsilon_j + \frac{1}{2}\Upsilon^2 g_{ij})\circledast g_{kl}\\
&= r^{-2}\bar R_{ijkl} - (r^{-1}\Phi_{ij} - r^{-2}\Phi_i \Phi_j - r^{-2}\Phi_i \Phi_j + \frac{1}{2}|\mathrm{d}r|^2_{\bar g} g_{ij})\circledast g_{kl}\\
&= r^{-2}\bar R_{ijkl} - (r^{-3}\Phi_{ij} - r^{-4}\Phi_i \Phi_j - r^{-4}\Phi_i \Phi_j + r^{-4}\frac{1}{2}|\mathrm{d}r|^2_{\bar g} \bar g_{ij})\circledast \bar g_{kl}.
\end{align*}
If the two terms in the ""centre"" of the expression ($r^{-4}\Phi_i \Phi_j$) were to cancel, it now becomes clear that the $|\mathrm{d}r|^2_{\bar g}$ term dominates as $r$ vanishes, and the expression directly implies that the sectional curvature tends to $-|\mathrm{d}r|^2_{\bar g}$. However, the signs are negative for both terms and the limits don't work out. Did I make a calculation mistake somewhere? Any pointers would be appreciated!","['riemannian-geometry', 'compactification', 'general-relativity', 'conformal-geometry', 'differential-geometry']"
2812010,Number as the sum of digits of some degree,"We will say that the measure of a number is equal to the maximum degree in which it is possible to represent a number in the form of a sum of digits copied (You can not rearrange the numbers). For example, for $55$ this will be $5$, because $$ 55^1 = 55, \quad 55 = 55$$ $$ 55^2 = 3025, \quad 30+25 = 55 $$ $$ 55^3 = 166375, \quad 1+6+6+37+5 = 55$$ $$ 55^4 = 9150625, \quad 9+15+0+6+25 = 55 $$$$55 ^ 5 = 503284375, \quad 5 + 0 + 3 + 28 + 4 + 3 + 7 + 5 = 55.$$
  Let $a_n$ be a sequence of numbers such that all smaller ones have a measure less than. What is the asymptotics of this sequence? Is it possible to somehow build numbers with a given measure? If not, what measures can not be built? The task was put in a Russian forum, I put it a little different question, I will be glad to any help in its solution :)","['number-theory', 'asymptotics', 'sequences-and-series', 'elementary-number-theory']"
2812030,"Prove that if $E(X^2+Y^2) \lt \infty$, $E(X|Y)=Y$, $E(Y|X)=X$, then $P(X=Y)=1$ [duplicate]","This question already has answers here : If $E[X|Y]=Y$ almost surely and $E[Y|X]=X$ almost surely then $X=Y$ almost surely (4 answers) Closed 6 years ago . Suppose that X and Y are random variables on a common probability space such that $E(X^2+Y^2) \lt \infty$, $E(X|Y)=Y$, $E(Y|X)=X$. Prove that $$P(X=Y)=1$$ My work:
$E(X)=E(E(X|Y))=E(Y)$ and $E(Y)=E(E(Y|X))=E(X)$ But I don't know what to do next and I'm sure how to use the condition $E(X^2+Y^2) \lt \infty$. Thanks in advance.","['probability-theory', 'conditional-expectation', 'probability', 'expectation']"
2812047,What is the essential difference between classical and quantum information geometry?,"This question may be a little subjective, but I would like to understand, from a geometric perspective, how the structure of quantum theory differs from that of classical probability theory. I have a good understanding of classical information geometry, and a reasonable understanding of quantum theory, for finite Hilbert spaces at least. (I am mostly only interested in finite spaces for now.) I understand information geometry mostly from the perspective of Amari , who essentially derives everything from the ""Pythagorean theorem"". This says that we can treat the Kullback-Leibler divergence as a kind of squared distance. That is, if we have classical probability distributions $P$, $Q$ and $R$ such that
$$
D_{KL}(R\|Q) + D_{KL}(Q\|P) = D_{KL}(R\|P)
$$
then we may interpret $P$, $Q$ and $R$ as points that lie on the corners of a right triangle, with $Q$ at the right angle. I like to think of this as a definition of orthogonality rather than a theorem, and much of the rest of information geometry (mixture families and exponential families, the Fisher metric, dually flat spaces, and so on) can be seen as following from it. Unfortunately Amari's book doesn't cover quantum information geometry. Here we have the quantum relative entropy, $\mathrm{Tr}\,\rho(\log \rho - \log \sigma)$, which generalises the Kullback-Leibler divergence. Presumably a similar Pythagorean interpretation can be made, with presumably gives rise to a similar dually flat structure, with quantum analogs of the Fisher metric, mixture and exponential families, and so on. Here is my question: looking at these two objects purely as geometric spaces , is it possible to point at one or a few key defining differences between them? Here is one observation as a possible starting point. In the quantum case, if we fix a basis and only consider distributions that are mixtures of the eigenstates, then we have a manifold that is exactly equivalent to the classical probability simplex. However, this works for any unitary basis, and so the quantum case has a kind of rotational symmetry that's lacking in the classical case. I suspect this is not the only difference, however, and so I am looking for a characterisation of the differences between the classical probability simplex and its quantum analog, in terms of their properties as geometric manifolds. (I would also appreciate pointers to good introductions to quantum information geometry, ideally pitched at a similar level to Amari's book and available online.)","['quantum-mechanics', 'information-geometry', 'quantum-information', 'information-theory', 'differential-geometry']"
2812068,Convergence in distribution implies convergence of $L^p$ norms under additional assumption,"The following is an exercise (4.5.2) from Chung's book, ""A Course in Probability Theory"". If $\{X_n\}$ is dominated by some $Z$ in $L^p$ and converges in distribution to $X$, then prove that $$\lim\limits_{n\rightarrow\infty}E[|X_n|^p]=E[|X|^p]$$ I applied the Skorohod's Representation theorem. Since $X_n$ converges in distribution to $X$, there exists a sequence of random variables $\{Y_n\}$ with the same distribution as $\{X_n\}$ such that $Y_n \rightarrow Y$ a.s and $Y$ has the same distribution as $X$. By the continuous mapping theorem, $$ \lim\limits_{n\rightarrow\infty}|Y_n|^p = |Y|^p $$ almost surely. Then, $$ \lim\limits_{n\rightarrow\infty}E[|X_n|^p] = \lim\limits_{n\rightarrow\infty}E[|Y_n|^p] = E[|Y|^p] = E[|X|^p]$$ where I applied the Dominated Convergence Theorem in the second step. Am I allowed to apply the Dominated Convergence Theorem here? We know that $\{X_n\}$ are dominated by $Z$, but does that mean the $\{Y_n\}$ are as well?","['weak-convergence', 'probability-theory', 'lp-spaces']"
2812075,"Why do mathematicians refer to sets as ""topological spaces"" or ""measurable spaces"" without referencing any topology or measurable sets?","In a theorem such as this, found in Rudin: Let $u$ and $v$ be real measurable functions on a measurable space
$X$, let $\phi$ be a continuous mapping of the plane into a topological space $Y$, and define $h(x) = \phi (u(x), v(x))$ for all $x \in X$. Then $h: X \to Y$ is measurable. Isn't it redundant/unnecessary to refer to $Y$ as a topological space? Every set can be a topological space, because you can generate a topology, no matter how trivial, from every set, right? Since the theorem makes no explicit use of any topology on $Y$, why does $Y$ need to be labeled as a topological space? In the same way, why are any spaces labeled as ""measurable spaces"" when every set, at least trivially, is a measure space? Note: 
I am aware that both a topological space and a measurable space refer more technically to the ordered pairs of a set and a collection of subsets, known as the topology or measure set respectively. However, when no explicit reference is made to these collections of subsets I don't understand how sets can be meaningfully classified as ""topological"" or ""measurable.""","['terminology', 'measure-theory']"
2812092,Proving martingale property of $N_t = Z(M_{t\wedge s} - M_{t \wedge r})$ for martingale $M$,"(Stochastic calculus and Brownian motion, LeGall, page 80). Suppose $M = (M_t)$ is a martingale. Also, let $Z$ be a bounded random variable which is $\mathcal{F}_r$ adapted. Then we like to show that for any $0 \leq r < s$ , $$N_t = Z(M_{t\wedge s} - M_{t \wedge r})$$ is a martingale. My attempt: $Z(M_{t\wedge s} - M_{t \wedge r}) \in L_1$ , should be fine since both $Z$ and $M$ are bounded. I am not sure how to show it is adapted.
Finally we need to show the martingale identity. Suppose $v \geq r$ , then we have $$\mathbb{E} \{Z(M_{t\wedge s} - M_{t \wedge r}) \mid \mathcal{F}_v \} = Z \mathbb{E} \{(M_{t\wedge s} - M_{t \wedge r}) \mid \mathcal{F}_v \} = Z (M_{v\wedge s} - M_{v \wedge r})$$ where in the first equality we use the fact that $Z \in \mathcal{F}_r$ , and $v \geq r$ . This proves the result for this case. However, I don't know how to get the result for general $v$ . Thanks for you helps in advance.","['probability-theory', 'martingales', 'local-martingales']"
2812114,Solving $f(x)= f''(x)$,"I am studying limits, continuity and derivability and I have got a continuous function $f\colon \mathbb R \to \mathbb R$ such that $f''(x)= f(x)$. How do I find $f(x)$ from this information? This is the first time I have come across such a problem, that's the reason I am stuck. I know that its obvious that it should be $e^x$, but what's the formal method of finding the function? Trigonometric functions like sin or cos don't satisfy the relation. Wolfram alpha gives the solution to be $f(x)= c_1e^x + c_2 e^{-x}$","['ordinary-differential-equations', 'functions']"
2812241,Summation of an infinite Exponential series,"Q. Find the value of - $$ \lim_{n\to\infty} \sum_{r=0}^{n} \frac{2^r}{5^{2^r} +1} $$ My attempt - I seem to be clueless to this problem. Though I think that later terms would be much small and negligible( Imagine how large would be $ 5^{2^r} $ after 3-4 terms), so I calculated the sum of first 3-4 terms and my answer was just around the actual answer    ( just a difference of $0.002$ ). But I wonder if there is an actual method to solve this problem? If it is, would you please share it to me? Any help would be appreciated","['summation', 'exponential-function', 'limits']"
2812295,Sum of a matrix with its transpose [duplicate],"This question already has answers here : How to prove $A+ A^T$ symmetric, $A-A^T$ skew-symmetric. [closed] (5 answers) Closed 6 years ago . I've a question that many of yours could consider stupid: if i sum a matrix with its transpose, I obtain a particular result? E.g. $A + A^T = B$, $B$ has some particular properties?",['matrices']
2812319,"Minimise the functional $\int_D \| \operatorname{grad}u(M)\|^2\,\mathrm dM$","I have the following problem: Let $k \in \mathbb{N}^*$ and $D$ be the closed disk of radius $1$ in $\mathbb{C}$ and $g\colon \mathbb{U} \to \mathbb{R}:z \mapsto \sin(k\mathrm{Arg}(z))$. Now let $E$ be the set of functions from $D$ to $\mathbb{R}$ which are continuous on $D$, $C^1$ on $\overset{\circ}{D}$ and whose partial derivatives can be continuously extend to $D$ and whose restrictions at $\mathbb{U}$ is $g$. Find the minimum of the functional: 
  $$u \in E \mapsto\int_D \| \operatorname{grad}u(M)\|^2 \,\mathrm{d}M.$$ I would like to have some thoughts on it, the problem is that I do not know at all how to abord it and where to look at. I know that most of the time in order to find the maximum of a function we need to find the critical point of this function in order to see if these points are relatives minimum and then ajust. But here it is not clear to me how I should find the critical points or if I should apply an other technique. Thank you.","['multivariable-calculus', 'real-analysis', 'calculus-of-variations', 'maxima-minima']"
2812346,Trigonometric functions limit to complex infinity,"It is well-known that trigonometric functions oscillate on the real axis and the limit does not exist as the argument approaches infinity. However, I suspect that a limiting value exist if the argument approaches any complex infinity that is not real, i.e. $$\lim_{r\to\infty} f(re^{i\theta})$$ is suspected to exist for $\theta \ne n\pi$ , where $f$ is a trigonometric function. I confirmed it is the case for $\tan (z)$ by decomposing it into real and imaginary parts. For real part: $$\lim_{r\to\infty}\frac{\sin 2r\cos\theta}{\cos 2r\cos\theta+\cosh 2r\sin \theta}=0$$ which is straightforward. For imaginary part: $$\lim_{r\to\infty}\frac{\sinh 2r\sin\theta}{\cos 2r\cos\theta+\cosh 2r\sin \theta}=\text{sgn}(\sin\theta)$$ Thus $$\lim_{r\to\infty}\tan(re^{i\theta})= \text{sgn}(\sin\theta)i$$ My questions are: Are the above calculations correct? Is there an easier way to compute the limit, other than decomposing it into real and imaginary parts? Indeed, the limits should be well known. Are there some reliable references that summarize the results for various trigonometric functions? Thanks in advance.","['reference-request', 'trigonometry', 'limits']"
2812378,Compute limits using central limit theorem,"I want to find the limit of expressions such as: $\lim_{n \to \infty} P(\frac{|S_{n}|}{n} \le \epsilon) $. I do not know how to proceed. Using the central limit theorem, I can rewrite the expression: $$\lim_{n \to \infty} P(|S_{n}|\le n \cdot \epsilon) = \lim_{n \to \infty} P(- n \epsilon \le |S_{n}| \le n \epsilon) = \int_{-\infty}^{\infty}\frac{1}{2\pi}e^{-x^{2}/2}=1$$ Is that correct? If so, how do I proceed in the case  $\lim_{n \to \infty} P(\frac{|S_{n}|}{n} =0) $? Your support is very much appreciated! Thank you! Best, Jolle","['random-walk', 'probability-theory', 'probability', 'central-limit-theorem']"
2812415,Find $f(3)$ if $f(f(x))=3+2x$,A function $f\colon \mathbb{R} \to \mathbb{R}$ is defined as $f(f(x))=3+2x$ Find $f(3)$ if $f(0)=3$ My try: Method $1.$ Put $x=0$ we get $f(f(0))=3$ $\implies$ $f(3)=3$ Method $2.$ Replace $x$ with $f(x)$ we get $$f(f(f(x)))=3+2f(x)$$ $\implies$ $$f(3+2x)=3+2f(x)$$ Put $x=0$ $$f(3)=9$$ I feel Method $2.$ is Correct since $f(f(x))=3+2x$ is Injective which means $f(x)$ should be Injective.,"['algebra-precalculus', 'calculus', 'functions']"
2812428,Upper bound on ratio of positive polynomials,"Let $a_i,b_i,c_i,d_i>0$ for all $1\leq i\leq n$.  What is a sharp upper bound on $$\sup_{x_1,\ldots,x_n>0}\frac{\sum_{i,j=1}^n a_ib_jx_ix_j}{\sum_{i,j=1}^n c_id_jx_ix_j}$$ This quantity is always bounded as there is the bound:
$$ \frac{\sum_{i,j=1}^n a_ib_jx_ix_j}{\sum_{i,j=1}^n c_id_jx_ix_j}\leq \max_{i,j}\frac{a_ib_jx_ix_j}{c_id_jx_ix_j}=\max_{i,j}\frac{a_ib_j}{c_id_j}.$$
So, we get 
$$\sup_{x_1,\ldots,x_n>0}\frac{\sum_{i,j=1}^n a_ib_jx_ix_j}{\sum_{i,j=1}^n c_id_jx_ix_j}\leq \min\Big\{\max_{i,j}\frac{a_ib_j}{c_id_j},\max_{i,j}\frac{b_ia_j}{c_id_j}\Big\}.$$
This bound is tight as it is attained for instance when $a_i= r b_i= s c_i = t d_i$ for every $i$ and some $r,s,t>0$. However it is quite conservative. Can we do better ?","['inequality', 'polynomials', 'geometry', 'linear-algebra', 'analysis']"
2812438,Which specific smooth structure are we using in general relativity?,"In this lecture by Fredric Schuller it is said that in the case of a non compact four dimensional manifold there is a non countable infinity of differentiable or smooth manifolds that are NOT diffeomorphic. Differentiable structures definition and classification - Lec 07 - Frederic Schuller My question is that how this fact from math can be related to or affect the study of black holes, say finding the Schwarzschild solution or the study of cosmology, say solving for the FLRW metric. I mean in which part of the calculations we specify which specific smooth structure, i.e. $C^{\infty}$-compatible maximal atlas are we using to take the chart from it and put a coordinate system?","['manifolds', 'general-relativity', 'differential-geometry', 'differential-topology']"
2812465,An analytic function $f$ on $\{z \in\mathbb{C} : |z|>1 \}$,"Let $\Gamma$ denote the positively oriented circle of radius $2$ with center at the origin. Let $f$ be an analytic function on $\{z \in\mathbb{C} : |z|>1 \}$, and let $$\lim_{z\to \infty}f(z)=0.$$ Prove that $$ f(z)=\frac{1}{2\pi i} \int_{\Gamma} \frac{f(\xi)}{z-\xi}d\xi,  $$ for all $z\in\mathbb{C}$ with $|z|>2$. I have no idea how to even start. Any hint will be appreciated.","['complex-analysis', 'analytic-functions', 'contour-integration']"
2812472,How to show that $\frac{d^n}{dx^n} (x^2-1)^n = 2^n \cdot n!$ for $x=1$,"I am trying to show that $$
\frac{d^n}{dx^n} (x^2-1)^n = 2^n \cdot n!,
$$ for $x = 1$. I tried to prove it by induction but I failed because I lack  axioms and rules for this type of derivatives. Can someone give me a hint?","['derivatives', 'real-analysis', 'legendre-polynomials']"
2812483,Does not existence of partial derivatives at a point tells us that the function is not differentiable?,"This question arose after my calculus test in which the told us: Show that $$f(x,y)=\begin{cases}\frac{2x^3}{x^2 +y^2}&\text{ if }(x,y)\neq(0,0)\\0&\text{ if }(x,y)=(0,0).\end{cases}$$ is not differentiable at $(0,0)$. I showed that the partials do not exist in $(0,0)$ thus the function is not differentiable.","['multivariable-calculus', 'partial-derivative', 'derivatives']"
2812518,Implicitly differentiating $y = y^2 x$,"I am self learning calculus and have a problem that may be simple here but  I cant find an answer on the web so here it is: If we have the equation: $$y = y^2x$$ and differentiate it (implicitly and using product rule) we get: $$\frac{dy}{dx} = \frac{y^2}{1-2xy}.$$ However, $y =y^2x$ can be simplified to $1 = xy$, i.e. $y = \frac{1}{x} $, which when differentiated using the power rule gives $\frac{dy}{dx} = -\frac{1}{x^2}$. The two differentials are different, as
$\frac{dy}{dx} = -\frac{1}{x^2}$ does not equal $\frac{dy}{dx} = \frac{y^2}{1-2xy}$. Why is this when we are essentially differentiating the same equation? (I plotted the graph on wolfram alpha and they are indeed not the same.)","['derivatives', 'calculus']"
2812527,"Can $n+1$ , $2n+1$ , $3n+1$ all be perfect squares , if $n$ is a positive integer?","Can the numbers $n+1$ , $2n+1$ and $3n+1$ be simultaneously perfect squares for any positive integer $n$ ? I tried to find that out and arrived at the equation system $$c^2-3a^2=-2$$ $$b^2-2a^2=-1$$ by setting $$n+1=a^2$$ $$2n+1=b^2$$ $$3n+1=c^2$$ and I conjecture that the only solution in positive integers is $c=a=b=1$ , corresponding to $n=0$. But how can I prove this conjecture ? Or did I miss a solution ?","['number-theory', 'pell-type-equations', 'square-numbers', 'elementary-number-theory']"
2812564,Separating theorem,"I have convex the set $C:=C(x_1,\dots,x_n) \in \mathbb{R}^n$ of convex combinations of $x_i$'s. I know that there exists an $x_i$ such that $\Vert x_i \Vert > 0$ and $ 0 \notin \mathring{C}(x_1,\dots,x_n) = \{\sum_{i=1}^{n}\lambda_i x_i : \lambda_i \in (0,1) \text{ and sum up to 1}\}$. I also know that \begin{align*} \exists y \in C \>\forall c \in \mathring{C} : \> \langle y , c \rangle > 0. \end{align*}  Now I am supposed to show that $\langle y , x_i \rangle > 0$ holds. Can somebody give me a hint for this task?","['real-analysis', 'inequality', 'convex-analysis', 'functional-analysis', 'inner-products']"
2812573,Proving the Jacobi method converges for diagonally-column dominant matrices,"The Jacobi method is an iterative method for approaching the solution of the linear system $Ax=b$, with $A\in\mathbb{C}^{n\times n}$, where we write $A=K-L$, with $K=\mathrm{diag}(a_{11},\ldots,a_{nn})$, and where we use the fixed point iteration $$\alpha_{j+1}=K^{-1}L\alpha_j+K^{-1}b,$$ so that we have for a $j\in\mathbb{N}$: $$\alpha-\alpha_{j+1}=K^{-1}L(\alpha-\alpha_{j}).$$ Now my syllabus provides a proof for convergence for the case that $A$ is diagonally-row dominant, but I and our teacher both couldn't see a way to rewrite the proof to a proof for the diagonally-column dominant case. The proof for the diagonally-row dominant is given using the $\|\cdot\|_{\infty}$ norm, and I found on the internet that the diagonally-column dominant case can be proved using the $\|\cdot\|_{1}$ norm. The proof strategy would be to show that $$\|\alpha-\alpha_{j+1}\|\leq \rho^j\|\alpha-\alpha_1\|,$$ for some matrix norm $\|\cdot\|$ and a $0\leq\rho<1$, from which convergence would follow immediately. Does anyone have an idea how to continue? Using the $\|\cdot\|_{1}$ norm as suggested by the internet we want to have something like $$\|\alpha-\alpha_{j+1}\|_1\leq\|(K^{-1}L)^j\|_1\cdot\|\alpha-\alpha_1\|_1,$$ and somehow conclude $\|K^{-1}L\|_1$ should be bounded by $1$. I know about the characterization $$r:=\max_{i\in\{1,\ldots,n\}}\Bigg\{|a_{ii}|^{-1}\sum_{\substack{i&=1\\ i&\neq j}}|a_{ji}|\Bigg\}.$$ I'd like to proof this without Gershgorin's theorem, since this is not covered in my course. It is fine if the case follows from the diagonally-row dominant case.","['numerical-methods', 'numerical-linear-algebra', 'linear-algebra']"
2812584,Why my cosine interpolation of a cube's face doesn't work?,"NB please : executable use cases are available at the end of this question. I begin this question by showing you the problems of my program, then I explain how the latter works, and finally I end up with these both executable use cases. What I want to do I want to draw the bottom face of a cube. To do that, I interpolate between the 4 vertices of this face. I use two kinds of interpolation : Linear Cosine I know it's typically the aim of GPUs, but I wanted to implement it myself to learn and try the ""deep"" mechanisms of 3D rendering. Results Each point of the following both pictures are interpolated between the 4 vertices of the bottom face (except the 8 vertices of my cube). With linear interpolation It's OK, no problem. With cosine interpolation Error. As you can see, there is some weird problem : face's points are inclined towards left. How do I interpolate between 4 points ? I test all the combinations of interpolation's weights and, if the sum of the weights equals 1, I draw the resulting point. I know this method is not very good (a lot of consumption of CPU and RAM), and there are P duplicates. But this problem is (1) secondary and (2) out of the range of this StackOverflow question. In other words : the definition of the interpolation between 4 points is : P = aA + bB + cC + dD . a + b + c + d must equals 1 to draw P , otherwise it's not drawn. (a;b;c;d) \in [0;1]^4 by the way. A concrete example is (step = 1) : P = 0a + 0b + 0c + 0d is NOT drawn P = 0a + 0b + 0c + 1d is drawn P = 0a + 0b + 1c + 0d is drawn P = 0a + 0b + 1c + 1d is NOT drawn etc. (""I test all the combinations""). (Last is : P = 1a + 1b + 1c + 1d , which is not drawn). To test all these combinations, I iterates on the weights, with a step, using recursion. Case of the linear interpolation The above definition doesn't change : P = aA + bB + cC + dD . Case of the cosine interpolation The above definition does change : P = aA + bB + cC + dD becomes P = a'A + b'B + c'C + d'D , with : a' = 0.5(1 - cos(a * \pi)) and the idea is the same for the three other weights. This new definition is not random, I could explain how I found it, but it's out of the range of this topic (in résumé : a simple remap of the cosine function). Question Why does it works well with the linear interpolation, and not the cosine one ? Indeed, the cosine face's points are inclined towards left. By the way : interpolations between two points work well Linear and cosine interpolations both work very well when used between two points, as you can see below. Linear interpolation It's OK, no problem. Cosine interpolation It's OK, no problem. Implementation and Executable I wrote the program in Scala. First I show you the functions, then two use cases you can execute along with the given functions. Tests all the combinations of interpolations between n points of k coordinates def computeAllPossibleInterpolatedPoints(step : Double, points : Seq[Seq[Double]], transform: (Double) => Double) : Seq[Seq[Double]] = {
  var returned_object : Seq[Seq[Double]] = Seq.empty[Seq[Double]]
  recursiveInterpolation(0, Seq.empty[Double])

  def recursiveInterpolation(current_weight_id : Int, building_weights : Seq[Double]) : Unit = {

    (.0 to 1.0 by step).foreach(current_step => {
      if (current_weight_id < points.size - 1) {
        recursiveInterpolation(current_weight_id + 1, building_weights :+ current_step)
      } else {
        val found_solution = (building_weights :+ current_step).map(transform)
        if(BigDecimal(found_solution.sum).setScale(5, BigDecimal.RoundingMode.HALF_UP).toDouble == 1.0) {
          returned_object = returned_object :+ interpolation(found_solution, points)
        }
      }
    })
  }

  returned_object
} Interpolates between n points of k coordinates def interpolation(weights: Seq[Double], points: Seq[Seq[Double]], transform: (Double) => Double = null) : Seq[Double] = {
  var transformed_weights = weights
  if(transform != null) {
    transformed_weights = weights.map(transform)
  }
  if(BigDecimal(transformed_weights.sum).setScale(5, BigDecimal.RoundingMode.HALF_UP).toDouble != 1.0) {
    println(""ERROR : `SUM(weights) != 1`. Returning `null`."")
    return null
  }
  if(transformed_weights.exists(weight => BigDecimal(weight).setScale(5, BigDecimal.RoundingMode.HALF_UP).toDouble < 0)
    ||
    transformed_weights.exists(weight => BigDecimal(weight).setScale(5, BigDecimal.RoundingMode.HALF_UP).toDouble > 1)) {
    println(""ERROR : `EXISTS(weight) / weight < -1 OR weight > 1`. Returning `null`."")
    return null
  }

  transformed_weights.zip(points).map(
    weight_point => weight_point._2.map(coordinate => weight_point._1 * coordinate)
  ).reduce((point_a : Seq[Double], point_b : Seq[Double]) => point_a.zip(point_b).map(coordinate_points => coordinate_points._1 + coordinate_points._2))
} Interpolation functions def linear(weight : Double) : Double = {
  weight
}

def cosine(weight : Double) : Double = {
  (1 - Math.cos(weight * Math.PI)) * 0.5
} Two use cases : between 2 points, each made of 2 coordinates (first use case : linear interpolation, second use case : cosine interpolation) Note that you can add 2 new points to each of the following functions, to interpolate a face instead of a segment (for now, the below code indeed interpolates points of a segment). val interpolated_points : Seq[Seq[Int]] = interpolator.computeAllPossibleInterpolatedPoints(0.05, Seq(Seq(22, 22), Seq(33, 33)), interpolator.linear).map(_.map(_.intValue()))

val interpolated_points : Seq[Seq[Int]] = interpolator.computeAllPossibleInterpolatedPoints(0.05, Seq(Seq(22, 22), Seq(33, 33)), interpolator.cosine).map(_.map(_.intValue()))","['3d', 'interpolation', 'geometry']"
2812590,"Along a finite ring extension, the restriction of invertible ideals cannot be zero","Let $S := k[x] \subseteq R$ be a finite ring extension where $k$ is a field. Moreover, let $R$ be one-dimensional, noetherian and reduced. Let $I$ be an invertible ideal of $R$. Do we have $I \cap S \neq \{0\}$? My attempt: The answer is yes if $I$ contains (properly) a prime ideal $P \subsetneq I$ since then by incomparability we have $0 \subseteq P \cap S \subsetneq I \cap S$. But do we know that invertible ideals always contain a prime?","['abstract-algebra', 'algebraic-number-theory', 'algebraic-geometry', 'commutative-algebra']"
2812599,Difference equation solution,My question : $y_{n+1}= ay_n+b$ ;  $y_0 = \alpha$ We solved this difference equation in a class and got that $y_n = a^nc + \beta$ Can someone please explain the way how to solve it? And what is the was to solve differential equations in general? Is there some good literature with difference equations theory and examples?,"['recurrence-relations', 'ordinary-differential-equations']"
2812602,"$w \mapsto \frac{1}{2i\pi} \int_{\gamma}\frac{1}{z-w}\mathrm{d}z$ from $\mathbb{C} \setminus \gamma([a,b]) \to \mathbb{C}$ is continuous","Let $\gamma: [a,b] \rightarrow \mathbb{C}$ be continuous and piecewise continuously differentiable but not necessarily closed. I want to show that $$w \mapsto \frac{1}{2i\pi} \int_{\gamma}\frac{1}{z-w}\mathrm{d}z$$ from $\mathbb{C} \setminus \gamma([a,b]) \to \mathbb{C}$ is continuous. $f(w)=\frac{1}{2i\pi} \int_{\gamma}\frac{1}{z-w}\mathrm{d}z=\frac{1}{2i\pi} \int_{a}^b\frac{1}{\gamma(t)-w}\gamma'(t)\mathrm{d}t=\frac{1}{2\pi i}\sum_{i=1}^n\int_{\xi_{i-1}}^{\xi_i}\frac{\gamma'(t)}{\gamma(t)-w}\mathrm{d}t$ $a=\xi_1<...<\xi_n=b.$ Now, because $\gamma$ is piecewise continuously differentiable for every $i=1...n$ $\gamma'(t)$ is continuous on $[\xi_{i-1},\xi_i]$ and because $\mathbb{C} \setminus \gamma([a,b])$ the denominator is also continuous therefore $f(w)$ is continuous. Is that correct? Maybe there is a way arguing using parameter integrals","['continuity', 'complex-analysis', 'line-integrals']"
2812608,"Let $f$ be holomorphic in $\mathbb{C}\setminus\{i,2i\}$.","Let $f$ be holomorphic in $\mathbb{C}\setminus\{i,2i\}$. Show that if $f$ has an non avoidable singularity in $z = i$ and $z = 2i$, then, the Laurent series of $f$ in $\{1 <|z| < 2 \}$ has infinite positive and negative terms. I've know that if $f$ has a singularity in $z_0$, then: If is avoidable, then the Laurent series of $f$ in $z_0$ has all positive terms If is a pole, then the Laurent series of $f$ in $z_0$ has finite negative terms If is essential, then the Laurent series of $f$ in $z_0$ has infinite negative terms. In this case, $z_0 = 1$, but I don't have to evaluate in $z_0$, but in $z_1 = i$ and $z_2 = 2i$. I tried by the absurd. If $$\sum_{n=k_0}^\infty a_n z^n$$ is the Laurent series in the domain, then I should show that $$\sum_{n=k_0}^\infty a_nw^n$$ converges to a number for $w=i$ and $w=2i$. I'm assuming that the case que has finite positive terms is analogous","['laurent-series', 'complex-analysis']"
2812616,Find real part of $\frac{1}{1-e^{i\pi/7}}$,"How can you find
$$\operatorname{Re}\left(\frac{1}{1-e^{i\pi/7}}\right).$$ I put it into wolframalpha and got $\frac{1}{2}$, but I have no idea where to begin. I though maybe we could use the fact that $$\frac{1}{z}=\frac{\bar{z}}{|z|^2},$$ where $\bar{z}$ is the conjugate of $z$. Unfortunately, the magnitude doesn't seem to be a nice number. I feel like this might be a trigonometry question in disguise, but converting $e^{i\pi/7}=\cos\left(\frac{\pi}{7}\right)+i\sin\left(\frac{\pi}{7}\right)$ hasn't been very fruitful.","['exponential-function', 'trigonometry', 'complex-numbers']"
2812656,"Given $f(x,y)=5+2x+4y+x^2+y^2+(x^2y^4)^\frac15$, show that it is differentiable at $(0,0)$.","I was given the function: $f(x,y)=5+2x+4y+x^2+y^2+(x^2y^4)^\frac15$ I need to show it is differentiable at $(0,0)$. I started using the method of differentials and infinitesimal functions: $\Delta f=f(0+\Delta x,0+\Delta y)-f(0,0)$ $\Delta f=2\Delta x + 4 \Delta y + (\Delta x)^2 + (\Delta y)^2 + ((\Delta x)^2(\Delta y)^4)^\frac15$ Now I cannot see a way how I can obtain infinitesimal functions* $\alpha$ and $\beta$ such that: $\Delta f=2\Delta x + 4 \Delta y + \alpha\Delta x + \beta \Delta y$, where $\alpha,\beta=o(\rho)$ (Where $\rho^2 = x^2+y^2$, and $o$ is the Landau little-$o$ notation defined as $\alpha=o(\rho)\iff\lim \alpha/\rho=0$). I managed to get 3 ""infinitesimal functions"": $\alpha=\Delta x$, $\beta=\Delta y$, and $\gamma=(\Delta x^2\Delta y^{-1})^\frac15$ but I am having trouble showing $\gamma=o(\rho)$ because frankly it doesn't seem to be. Any hints? If this is not possible, are there any other methods that one would recommend? *My prof defines infinitesimal functions as: $\alpha(x)$ is infinitesimal at $a$ if $\lim_{x\rightarrow a}\alpha(x)=0$","['derivatives', 'real-analysis', 'infinitesimals', 'analysis']"
2812688,"Why does every symplectic matrix $M\in{\rm Sp}(2n,{\Bbb R})$ admit a symplectic eigenbasis?","I tripped over the following assertion. Let $M \in \mathrm{Sp}(2n)$ be a symplectic real matrix which is diagonalizable. Then we can write it down as $M = S^{-1}D\ S$ where $S \in \mathrm{Sp}(2n)$ and $D$ is diagonal. I don't see why I can diagonalize $M$ by a symplectic matrix, I haven't found any reference of this statement somewhere else, Does anyone knows a reference or a proof about this statement? I tried to prove and the only thing I could get was that $\lambda$ and $\lambda^{-1}$ are eigenvalues of $M$ and probably this could get to the symplecticity of the eigenvectos of $S$.","['eigenvalues-eigenvectors', 'matrices', 'symplectic-linear-algebra', 'symplectic-geometry', 'linear-algebra']"
2812719,"Prob. 21, Exercises 8.9, in Apostol's CALCULUS vol. II: If the directional derivative of a scalar field vanishes . . .","Here is Prob. 21, Exercises 8.9, in the book Calculus volume II by Tom M. Apostol, 2nd edition: A set $S$ in $\mathbb{R}^n$ is called convex if for every pair of points $\mathbf{a}$ and $\mathbf{a}$ in $S$ the line segment from $\mathbf{a}$ to $\mathbf{b}$ is also in $S$; in other words, $t \mathbf{a} + (1-t) \mathbf{b} \in S$ for each $t$ in the interval $0 \leq t \leq 1$. (a) Prove that every $n$-ball is convex. (b) If $f^\prime ( \mathbf{x}; \mathbf{y} ) = 0$ for every $\mathbf{x}$ in an open convex set $S$ and for every $\mathbf{y}$ in $\mathbb{R}^n$, prove that $f$ is constant on $S$. Part (a) Let $\mathbf{p}$ be a given point in $\mathbb{R}^n$, and let $\delta$ be a given positive real number. Then the $n$-ball $B( \mathbf{p} ; \delta )$ is given by 
  $$ B( \mathbf{p} ; \delta ) \colon= \{ \ \mathbf{x} \in \mathbb{R}^n \ \colon \ \lVert \mathbf{x} - \mathbf{p} \rVert < \delta \ \}, $$
  where 
  $$ \lVert \mathbf{y} \rVert = \sqrt{ \sum_{i=1}^n \left\lvert y_i \right\rvert^2 } $$
  for any point 
  $$ \mathbf{y} \colon= \left( y_1, \ldots, y_n \right) $$
  in $\mathbb{R}^n$. Suppose $\mathbb{a}$ and $\mathbb{b}$ are any two points in $B( \mathbf{p} ; \delta )$ and suppose that $t$ is any real number such that $0 \leq t \leq 1$. Then $\mathbb{a}$ and $\mathbb{b}$ are in $\mathbb{R}^n$, and we also have 
  $$ \lVert \mathbf{a} - \mathbf{p} \rVert < \delta, \qquad \mbox{ and } \qquad \lVert \mathbf{b} - \mathbf{p} \rVert < \delta. $$
  So 
  $$
\begin{align}
\lVert t \mathbf{a} + (1-t) \mathbf{b} - \mathbf{p} \rVert &= \lVert t \mathbf{a} + (1-t) \mathbf{b} - [t + (1-t)] \mathbf{p} \rVert \\
&= \lVert t ( \mathbf{a} - \mathbf{p} ) + (1-t) ( \mathbf{b} - \mathbf{p} ) \rVert \\
&\leq \lVert t ( \mathbf{a} - \mathbf{p} ) \rVert  + \lVert  (1-t) ( \mathbf{b} - \mathbf{p} ) \rVert \\
&= \lvert t \rvert  \lVert  \mathbf{a} - \mathbf{p}  \rVert  + \lvert 1-t \rvert  \lVert  \mathbf{b} - \mathbf{p}  \rVert \\
&=  t \lVert  \mathbf{a} - \mathbf{p}  \rVert  + ( 1-t )  \lVert  \mathbf{b} - \mathbf{p}  \rVert \ [ \mbox{ because } \ t \in [0, 1] \ \mbox{ and } \ 1-t \in [0, 1] \ ] \\
&< t \delta + (1-t) \delta \\
&= \delta,
\end{align}
$$
  which shows that the point $t \mathbf{a} + (1-t) \mathbf{b}$ is also in $B( \mathbf{p}; \delta )$. Hence $B( \mathbf{p}; \delta )$ is convex. Part (b) Suppose that $S$ is an open convex set in $\mathbb{R}^n$, suppose that $f^\prime( \mathbf{x}; \mathbf{y} ) = 0$ for every $\mathbf{x} \in S$ and for every $\mathbf{y} \in \mathbb{R}^n$, and suppose that $\mathbf{a}$ and $\mathbf{b}$ are any two points in $S$. Then, for every $t \in [0, 1]$, the point $t \mathbf{a} + (1-t) \mathbf{b}$ is also in $S$. Now if we put 
  $$ \mathbf{y} \colon= \mathbf{b} - \mathbf{a}, $$
  then, by the mean-value theorem for scalar fields, we can conclude that,  for some real number $\theta \in (0, 1)$, we have 
  $$ f( \mathbf{b} ) - f( \mathbf{a} ) = f( \mathbf{a} + \mathbf{y} ) - f( \mathbf{a} ) = f^\prime ( \mathbf{a} + \theta \mathbf{y} ; \mathbf{y} ). \tag{1}   $$
  Now 
  $$ \mathbf{a} + \theta \mathbf{y} = \mathbf{a} + \theta (\mathbf{b} - \mathbf{a} ) = \theta \mathbf{b} + (1- \theta) \mathbf{a} \in S, $$
  and thus 
  $$ f^\prime ( \mathbf{a} + \theta \mathbf{y} ; \mathbf{y} ) = 0,  $$
  by our hypothesis. Therefore (1) gives 
  $$  f( \mathbf{b} ) = f( \mathbf{a} ) $$
  for any two points $\mathbf{a}$ and $\mathbf{b}$ in $S$. Hence $f$ is constant on set $S$. Is this proof correct? If so, then do we really need our set $S$ to be open in $\mathbb{R}^n$? If not, then where is it that I'm going wrong?","['derivatives', 'real-analysis', 'proof-verification', 'multivariable-calculus', 'analysis']"
2812754,ELO Rating as a sorting algorithm,"Let $N$ be the number of players each with a true rating $R_i$, where $i \in \{1,2,...,N\}$. Two players, $A$ and $B$, are selected at random and made to play a game. Let their current ratings be $R_A$ and $R_B$, respectively. We use the ELO rating system to calculate the expectation of a player to win the game as $$E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}, \\
E_B = \frac{1}{1 + 10^{(R_A - R_B)/400}}.$$ Their new ratings are given by the expressions $$R'_A = R_A + K(S_A - E_A),\\
R'_B = R_B + K(S_B - E_B).$$ If $A/B$ wins, $S_{A/B} = 1$. If $A/B$ loses, $S_{A/B} = 0$. $K$ is a constant (say 32). If each of these $N$ players start with a base rating of 1400, what order of games are required for the players to arrive at an accurate ranking? More generally, how does ELO rating serve as a sorting algorithm?","['sorting', 'statistics', 'probability']"
2812797,Connectedness of a subset of structured matrices satisfying prescribed conditions on roots of polynomials,"I am extracting information of me proving an application problem, if there is anything not clear, please let me know. Suppose we parameterize a structured matrix in following form
\begin{align}
\label{matrix:1}
\tag{$\star$}
\begin{pmatrix}
0 & -a_1 & 0 & 0 & -b_1 \\
1 & -a_2 & 0 & 0 & -b_2 \\
0 & -a_3 & 0 & 0 & -b_3 \\
0 & -a_4 & 1 & 0 & -b_4 \\
0 & -a_5 & 0 &1 & -b_5
\end{pmatrix},
\end{align}
where the upper left $2 \times 2$ and lower right $3 \times 3$ blocks are in companion form . I would like to claim that the set of matrices of above form and with eigenvalues modules bounded in some interval $(-M, M)$ is connected. If we let
    \begin{align*}
 E = \{ A \in \mathcal{M}(5 \times 5; \mathbb R): \text{ $A$ is of form \eqref{matrix:1} and } \rho(A) < M \},
\end{align*}
    we want to prove $E$ is connected. My approach is to first prove the set
\begin{align*}
 F = \{ B \in \mathcal{M}(5 \times 5; \mathbb R): \text{ structure of $B$ is of the form \eqref{matrix:2} and } \rho(B) < M \}
\end{align*}
is connected,
where
\begin{align}
\label{matrix:2}
\tag{$\ast$}
\begin{pmatrix}
0 & -a_1 & 0 & 0 & 0 \\
1 & -a_2 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & -b_3 \\
0 & 0 & 1 & 0 & -b_4 \\
0 &  0 & 0 &1 & -b_5
\end{pmatrix}.
\end{align}
If a matrix $C$ has the form \eqref{matrix:2}, then the characteristic polynomial $p_C(t) = (t^2+a_2 t + a_1)(t^3 + b_5 t^2 + b_4 t +b_3)$. If I am not mistaken, the set $F$ should be a continuous image of roots lying in $(-M, M)$ which should be connected. Now I would like to show for a general element in $E$, we can find a continuous path to the set $F$. Since for any $A \in E$, the characteristic polynomial 
$$p_A(t) = (t^2+a_2 t + a_1)(t^3 + b_5 t^2 + b_4 t +b_3) + \text{ polynomial terms involving entries of } a, b.$$
Intuitively, we can first choose some element $D \in F$ with $p_D(t) = p_A(t)$ and then continuously transform the coefficients there. But now I could not prove the whole path will only contain elements of $E$. Would anyone help me with this approach or provide some other approaches? Or given examples to show the set is actually not connected?","['polynomials', 'connectedness', 'abstract-algebra', 'general-topology', 'linear-algebra']"
2812799,Two supermartingales and a stopping time,"I have the following task to do: Let $(X_n)$ and $(Y_n)$ be two supermartingales on the probability space $(\Omega, \mathcal{A}, P)$ and $T$ be a stopping time regarding a filtration $(\mathcal{F}_n)$ and $X_T \leq Y_T$ on $\{T< \infty\}$. Define $Z_n = Y_n$ on $\{n<T\}$ and $Z_n = X_n$ on $\{T \leq n\}$. Proof that $(Z_n)$ is a supermartingale. I tried to use $Z_n = Y_nI_{\{n<T\}} + X_nI_{\{T\leq n\}}$ and plug that into $\mathbb{E}[Z_n|\mathcal{F_{n-1}}]$ and use some properties of the conditional expectation, but I don't seem to get to $\leq Z_{n-1}$. Any help is appreciated.","['probability-theory', 'stopping-times', 'probability', 'martingales', 'conditional-expectation']"
2812805,Summation $\sum_{n=1}^\infty \frac{1}{2^n+1}$,So I was was messing around with infinite series and I came across one that is deceptively similar to the familiar $\sum_{n=1}^\infty \frac{1}{2^n} = 1 $ Simply add $1$ to each denominator of each term of the series. This is $\sum_{n=1}^\infty \frac{1}{2^n+1} \approx 0.76449...$ I have used all my weaponry on trying to crack it and the best I've come up with is the equivalent series $\sum_{n=1}^\infty \frac{(-1)^n}{2^n-1} $ Could somebody help me retrieve some sort of compact value for this expression ( e.g. $\sum_{n=1}^\infty \frac{1}{n^2} $ can be rewritten as $\frac{\pi^2}{6}$) or is this pretty much impossible? Thanks!,"['summation', 'exponential-function', 'limits']"
2812813,Finding the cauchy integral for $I(n)=\int_\gamma \frac{\cos{z}}{(z-z_0)^3}{dz}$ when $\gamma: |z|=3.$,"$$\int_\gamma \frac{\cos z}{(z-z_0)^3}dz,\quad  \gamma \colon |z|=3$$ I was able to find the solution for this problem which included 
$z=2 \in \gamma, -3<z<3$, therefore $-3<2<3$. And when comparing to Cauchy formula we find that $n+1=3 \Rightarrow n=2$. Now the rest of the solution is obvious. What I wasn't able to understand was how did we determine that $<=2 \in \gamma$  and that it is the point that we are looking for. Please note that English isn't my first language so I'm having problems translating the exact mathematical words to English so pardon the quality of this post. Any help would be greatly appreciated.","['cauchy-integral-formula', 'complex-analysis']"
2812843,Every affine set can be expressed as the solution set of a system of linear equations,"This is not a duplicate of How to prove: Every affine set can be expressed as the solution set of a system of linear equations . I have already read the provided answer , but I don't understand the logic of the answer. Here are the definitions and theorems I have proved. Definition . A set $C \subseteq \mathbb{R}^n$ is affine if $$x_1, x_2 \in C \text{ and }\theta \in \mathbb{R} \implies \theta x_1+(1-\theta)x_2\in C\text{.}$$ Theorem . If $C$ is affine and $x_1, \dots, x_k \in C$, $\sum_{i=1}^{k}\theta_i = 1$, then $\sum_{i=1}^{k}\theta_i x_i \in C$. Theorem . Let $C$ be affine and $x_0 \in C$. Then $$V = C - x_0 = \{x-x_0:x \in C\}$$
  is a subspace of $\mathbb{R}^n$. Theorem . The set $C = \{x : Ax = b\}$ is affine, with $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$. Theorem . With $C = \{x : Ax = b\}$, the set $V$ (defined above) is the nullspace of $A$. What I want to prove : Every affine set can be expressed as the solution set of a system of linear equations. What I think this means : If $C \subseteq \mathbb{R}^n$ is affine, then $C = \{x: Ax = b\}$ for some $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$. This means that we need to show that $C \subseteq \{x: Ax = b\}$ and $\{x: Ax = b\} \subseteq C$. How does this imply the answer in the link above ? Fix an element $x_0 \in C$. Claim: The set $K = \{x-x_0: x \in C\}$ is a linear subspace of $\Bbb R^n$. Claim: There exists a linear map $T$ whose kernel is precisely $K$. Claim: $C$ is the solution to the linear system $Tx = Tx_0$ on $x$. Please note that I'm very rusty on linear transformations and the associated terminology.","['linear-algebra', 'systems-of-equations']"
2812845,Convergence or not of infinite series: $\sum^{\infty}_{n=1}\frac{n}{1+n^2}$ [duplicate],This question already has an answer here : Prove that $\sum_{n=1}^\infty\frac{n}{n^2+1}$ is divergent (1 answer) Closed 6 years ago . How can we prove that the series $\displaystyle \sum^{\infty}_{n=1}\frac{n}{1+n^2}$ is convergent or divergent? Solution I try: $$\lim_{m\rightarrow \infty}\sum^{m}_{n=1}\frac{n}{1+n^2}<\lim_{m\rightarrow \infty}\sum^{m}_{n=1}\frac{n}{n^2}$$ Did not know how I can solve that problem from that point.,"['real-analysis', 'sequences-and-series', 'calculus']"
2812856,Rudin's proof on the Analytic Incompleteness of Rationals [duplicate],"This question already has answers here : Choice of $q$ in Baby Rudin's Example 1.1 (16 answers) Closed 6 years ago . In Rudin's classical ""Principles of Mathematical Analysis,"" he gave a proof like this: Claim: Let $A= \{p\in \mathbb{Q} | p>0, p^2 <2\}$ . Then A contains no largest number. Proof: Given any $p\in A$ . Let $q = p-\frac{p^2 -2}{p+2}$ . Later Rudin claimed that $q^2<2,$ and $q>p$ . My instructor asks us to think about a question on our own: Why is such $q$ a natural choice in this proof? I can see that in this way, $q>p$ is for sure. However, how does it become a natural choice?","['real-analysis', 'irrational-numbers', 'proof-explanation']"
2812878,How to derive this integral identity?,There is a particular integral which comes up frequently in physics which a friend and I spent hours trying to figure out with no success. I don't know if this integral has a name so googling has been fruitless. Arfken states the solution in the back flap but I have no idea how to find it $$\int_{-\infty}^{\infty}{\frac{e^{i\mathbf{k}^{.}\mathbf{r}}}{k^{2}+m^{2}}\frac{dk^{3}}{(2\pi)^{3}}}=\frac{e^{-mr}}{4\pi r}$$ Limits go from -inf to inf for all k. Note that k and r are vectors. Presumably r on the right hand side is $\lvert\mathbf{r}\rvert$ (I could be wrong). If someone could show how this is derived or provide a link to its derivation it would be greatly appreciated.,"['complex-analysis', 'fourier-analysis']"
2812882,Trigonometric equations solutions within a range,"I wanted to ask a question about solving the following trigonometric equation: $$ \tan\left(\theta + \frac{\pi}{6}\right) = \tan (\pi - \theta)$$ in the interval $0 ≤ \theta ≤ \pi$ This is the following route I took. $$ \theta + \frac{\pi}{6} = \pi - \theta$$
$$ 2 \theta = \frac{5 \pi}{6}$$
$$ \theta = \frac{5 \pi}{12}$$ Now, using the interval above, I then clarified that there are no further solutions as the next solution would be $+ \pi$ so would be $\frac{17 \pi}{12}$ and hence out of this range. However, I apparently made a slight ""error"". According to a friend in the lecture hall today, there is another solution, $\frac{11 \pi}{12}$ as follows: $$ 2 \theta = \frac{5 \pi}{6}, \frac{11 \pi}{6}$$
$$ \theta = \frac{5 \pi}{12}, \frac{11 \pi}{12}$$ because in the range $0 ≤ 2 \theta ≤ \pi$, the result $\frac{11 \pi}{6}$ is possible. I'm confused. How can it be that when I solved for $\theta = \frac{5 \pi}{12}$ there were no more solutions available in the interval but using $2 \theta = \frac{5 \pi}{6}$ there were solutions possible?",['trigonometry']
2812906,Statements on the behavior of solutions to $y' = \sin(xy)$ for large $y(0)$,"Consider the following initial-value problem involving a nonlinear first-order ODE: $y' = \sin(xy), \quad y(0) = y_0$. For large enough values of $y_0$, the solutions to this equation appear to converge onto a specific shape under the appropriate normalized scale—dividing both $x$ and $y$ by $y_0$. Can any statements be made about the form of the function corresponding to this shape/normalized solution? See below for a graph of numerically computed solutions to this ODE that demonstrate this phenomenon. I've plotted the solutions for even larger $y_0$ below so that the shape these solutions are converging to can be viewed more clearly. Only one line is visible as both solutions are virtually the same under the normalized scaling. A comment: this question is motivated by a statement in Bender & Orszag's Advanced Mathematical Methods for Scientists and Engineers , where they use the IVP above when $y_0 = 1$ as an example of an initial-value problem with a unique solution near $x = 0$ that is not known to be representable in terms of elementary functions. Perhaps this strange shape is representable with elementary functions? UPDATE 6/10/2018 Although it is not clearly visible in the graphs below, the function appears to have a slight kink at the point where $y = x$, indicating the solution's derivative is not itself differentiable. In addition, a paper by Wendell Mills, Boris Weisfeiler and Allan M. Krall discusses how solutions to $y' = sin(xy)$ asymptotically tend to a function $y \propto 1/x$ as $x \rightarrow \infty$. Intrigued by this, I found numerically that for large $y_0$, the solution past the $y = x$ point (about 0.7929) matches $y = 0.63 y_0^2 / x$ essentially perfectly. See below for a comparison plot. That being said, I still have no clue regarding: What these constants—0.7929 and 0.63—are and where they come from. (I would assume the 0.63 constant has some factor of $\pi$ in it due to the arguments from the Mills-Weisfeiler-Krall paper.) How to prove the solution past $x = y$ approaches this specific $\propto 1/x$ form for large $y_0$. An expression for the part of the solution before $x = y$. Even more intriguing: Perhaps the solution for large $y_0$ can be defined piecewise in terms of elementary functions? UPDATE 7/31/2018 It seems as if the solution of $y' = \cos(xy),\ y(0) = y_0$ converges onto this unknown function as well for large $y_0$. See the graph below for numerical evidence; only one line is visible as both solutions are essentially the same. Could the asymptotic form of these solutions have some exponential component in the region where $y > x$?","['nonlinear-analysis', 'asymptotics', 'ordinary-differential-equations']"
2812907,How to prove the sum of angles is $\pi$ radians,"I made a cardboard triangular tile out of 6 other triangles. Here are the triplets shown in order of CAB, BAC, CAB, BAC, CAB, BAC. The slashes show sides-C joined and the vertical lines show sides-B joined. I found the arcsines of the sine values and my best calculations show that the exterior acute angles add up to $\pi$ radians. The problem is, unless this is proven, I cannot ’know’ that the peaks all meet at a central point and that the figure is 2D and not a pyramid. What approach can I take to prove my conjecture? / 565 403 396 | 396 1053 1125 / 1125 675 900 | 900 1925 2125 / 2125 2107 276 | 276 493 565 \","['trigonometry', 'triangles']"
2812917,Why doesn't integration by substitution work in this case?,"I am asked to solve $\int_0^\frac{\pi}{4}\tan x\, \mathrm{d}x$. This is what I did: $$
\begin{align}
&\int_0^\frac{\pi}{4}\tan x \,\mathrm{d}x&\\
= {} &\int_0^\frac{\pi}{4}\frac{\sin x}{\cos x}\, \mathrm{d}x&\\
= {} &\int_0^\frac{\pi}{4}\sin x\frac{-1}{-\cos x} \,\mathrm{d}x&\\
= {} &-\int_0^\frac{\pi}{4}\varphi'(x)\frac{1}{\varphi(x)} \,\mathrm{d}x &\text{where $\varphi(x):=-\cos x$} \\
= {} &-\int_{\varphi(0)}^{\varphi(\frac{\pi}{4})} \frac{1}{z} \,\mathrm{d}z \\
= {} &-\int_{-1}^{-\frac{1}{\sqrt{2}}} \frac{1}{z}\, \mathrm{d}z \\
= {} &\Big[\ln z\Big]_{-1}^{-\frac{1}{\sqrt{2}}} \\
= {} &\ln \left(-\frac{1}{\sqrt{2}}\right) - \ln (-1)
\end{align}
$$ And this is where I am stuck, because the solution is definitely not a complex number. I know the correct answer (it's $-\ln\left(\frac{1}{\sqrt{2}}\right)$, my problem is that I don't know where I made a mistake.","['substitution', 'real-analysis', 'integration', 'definite-integrals']"
2812949,What do the solvable groups in a free product of cyclic groups look like?,"Let $G = H_1 * H_2 * \dots * H_n$ such that $H_i$ is cyclic for $1 \leq i \leq n$ . So far, I proved that every abelian subgroup of $G$ is either cyclic or conjugate to a $H_i$ .
Next, I want to classify the solvable subgroups of $G$ . The hint I received is to look at the normaliser of abelian subgroups. However, I am totally stuck at this point. The abelian subgroups themselves are solvable, but how is their normaliser related to the solvable subgroups of $G$ ?","['free-product', 'solvable-groups', 'group-theory', 'cyclic-groups']"
2812969,Is the Trefoil knot isotopic to the unknot (torus) in ambient $\mathbb{R}^4$?,"I am watching this video on topology: https://youtu.be/CJBfpvWBmSs One of the claims is in an earlier video: https://youtu.be/CZqeAC07_UE?t=1037 Suppose we have manifolds with dimensions $L$ and $K$ in ambient with a dimension $M$. If $K + L - M = -1$, then the manifolds must cross to ""change sides"". If $K + L - M < -1$, then the crossing can be avoided. The meaning of ""changing sides"" is left somewhat undefined in the video. Consequently, in one of his later examples , the lecturer says that to untie the Trefoil knot, you need the ambient space $\mathbb{R}^6$ (one of the students also gives the answer 6). However, it seems to me that it is possible to untie a Trefoil knot in $\mathbb{R}^4$. If you pull out one part of the knot into the direction of the 4-th dimension, then you can obviously circumvent the adjacent part without crossing it. In fact, this is observed in one of the comments below the video too. Is this intuition correct? Can you untie the Trefoil knot in 4 dimensions? If it is, then according to this definition , the Trefoil knot is isotopic to the unknot in $\mathbb{R}^4$. EDIT: When I said ""Trefoil knot"", I actually meant the following 2-dimensional surface:","['knot-theory', 'general-topology']"
2812988,Precisely defining the conditional distribution of $X$ given that $Y = y$,"Let $(\Omega,\Sigma,P)$ be a probability space and let $X\colon \Omega \to \mathbb R$ and $Y\colon \Omega \to \mathbb R$ be continuous random variables with density functions $f_X$ and $f_Y$, respectively. I would like to precisely define the conditional distribution of $X$ given that $Y = y$, where $y \in \mathbb R$ and $f_Y(y) > 0$. The difficulty in doing this is that the event $Y = y$ has probability $0$. As a first step, we can attempt to define $P(A \mid Y = y)$, where $A \subset \Omega$ is an event. A key idea is to note that while the event $Y = y$ has measure $0$, the event $Y \in [y, y + \Delta y]$ has positive probability for any number $\Delta y > 0$. This suggests that we can define
$$
\tag{1} P(A \mid Y = y) = \lim_{\Delta y \to 0} P(A \mid Y \in [y,y + \Delta y]).
$$ Question: Does the limit on the right exist? If so, is the function $P(\cdot \mid Y = y)$ defined in equation (1) a probability measure? Additional question: Is this the standard approach to rigorously defining the conditional distribution of $X$ given that $Y = y$? If not, what is the standard approach? Please recommend a book that explains this clearly, as it is glossed over in most probability books I have looked at.",['probability']
2813015,Use Borel-Cantelli to determine $\operatorname{lim sup_{n \to \infty}}$ for some i.i.d. $X_n$,"Let $X_1, X_2, \ldots$ be i.i.d. nonnegative random variables. I want to use the Borel–Cantelli lemma, to show that
\begin{equation*}
  \limsup_{n\rightarrow\infty} \frac{1}{n} X_n =
  \begin{cases}
    0 \text{ a.s.}, & \text{ if } \mathbb{E}[X_1] < \infty,\\
    \infty \text{ a.s.}, & \text{ if } \mathbb{E}[X_1] = \infty.
  \end{cases}
\end{equation*} The second case actually works quite nicely when you use that $\mathbb E[X] \le \sum_{n=0}^{\infty} \mathbb P[X \ge n]$. But I don't know how the first case works. On a different thread I saw this estimation: $ \sum_{n=1}^\infty \mathbb{P}\bigl[X_n/n \geq \varepsilon \bigr] = \frac{1}{\varepsilon} \sum_{n=1}^\infty \mathbb{P}\bigl[X_1 \geq n  \varepsilon\bigr] \cdot \varepsilon \leq \frac{1}{\varepsilon}\cdot \underbrace{\mathbb{E}[X_1]}_{<\infty} < \infty$ but I don't see why I can get a lower bound for my expected value here. Additionally it also resembles the estimation I used for the second case, but the other way around.","['probability-theory', 'borel-cantelli-lemmas']"
2813030,"Can independence go one way? I.e., so that P(A|B) = P(A), but P(B|A) ≠ P(B)","As I understand it, independence of A and B can be informally established by asking whether learning something about one of those events tells you something new about the other. This must be borne out mathematically, however. For example: If P(A|B) = P(A), then A and B are independent. And if P(A & B) equals P(A) x P(B), then A and B are independent. The above imply that P(B|A) = P(B) It’s this last statement that confuses me, at least in application to certain cases. For example: You devise a way to randomly choose a number from all real numbers, uniformly distributed. The probability of the chosen number being prime is 0, given that 0% of the reals are prime. Similarly, choosing the number 2 from the set of all real numbers has a probability of 0. Likewise, the probability of choosing a 2 from the set of all prime numbers has a probability of 0. Given that 2 is a prime number, it seems that choosing a 2 and choosing a prime number must be dependent events, at least in one direction (namely, if I know I’ve chosen a 2, then I’m certain I’ve chosen a prime number). Here’s what I mean: P(2|prime number) = P(2) = 0 (passes for independence) P(prime number|2) = 1 (i.e., not 0, or P(prime number), and so fails for independence) But can also test as follows: P(2 & prime number) = P(2) x P(prime number) = 0
P(2 & prime number) = P(2) x P(prime number|2) = 0
P(2 & prime number) = p(prime number) x P(2|prime number) = 0 Everything here comes out to 0, as I suppose it should. This also aligns with my understanding that anything with probability 0 is independent from any other event. (Right?) And yet, I’m stuck with the intuition that: If I learn I got a 2, I know I got a prime number, wherein learning I got a prime is insufficient for updating my beliefs about getting a 2 (provided I really do believe that the probability of pulling a 2 from the primes is 0), and the same goes for learning I got an even, a natural, an integer, and so on. Yet I learn I got all those things if I learn I got a 2. I’ve thought of other examples, though all of them deal with some single event occurring out of a set of infinite possible outcomes. E.g., pulling from the natural numbers: P(2|even) = P(2) = 0; but P(even|2) = 1 (rather than the P(even) = 1/2). So I imagine there’s something I’m naive about in the domain of infinite possible outcomes. What am I missing?","['independence', 'probability']"
2813059,Bessel function and ODE.,"Define the Bessel function $J_{0}$ by
  $$J_{0}(x) = \frac{1}{\pi}\int_{-1}^{1}\frac{\cos xt}{\sqrt{1-t^{2}}}\mathrm{d}t.$$
  Prove that $J_{0}'' + (\frac{1}{x})J_{0}' + J_{0} = 0$ for all $x > 0$. The hint of my professor was to use the Leibniz Rule (permutation of partial derivatives), but I don't see how that can help. Can someone help me? I appreciate it!","['derivatives', 'real-analysis']"
2813060,(Linear Regression) Proving that Linear Regression is linear invariant,"I want to ask how to show that Linear Regression is linear invariant? The problem is specified in the following picture: Here is the ""solution"" for the problem. But I really get confused by its second step: Why $(Z^TZ)^{-1} = Z^{-1}Z^{-T}$?  The matrix Z is not a square matrix. I am wondering how this is legit? I will also greatly appreciate if anyone can give alternative solution to this problem. Thanks!","['matrices', 'linear-regression', 'linear-algebra']"
2813080,Can nonstandard analysis give a uniform probability distribution over the integers?,"There exists no uniform probability distribution over the non-negative integers. This is because we would need to have $p(i)=q$ for all $i$, for some real number $0\le q\le 1$. But normalisation requires
$$\sum_{i=1}^\infty q = 1,\tag{*}
$$
and there exists no real number $q$ with this property, since the sum diverges for $q\ne 0$. However, there are various formulations of non-standard analysis that extend the real numbers to include infinitesimals. These include the hyperreal numbers , the surreal numbers , and I believe others. Does there exist such a system in which equation $(*)$ can be satisfied by an infinitesimal $q$? If so, does it allow uniform probability distributions to be defined on infinite supports, such as the integers, reals etc.?","['nonstandard-analysis', 'probability-theory']"
2813115,"Precise definition of an ""algebraic function""","Remark. I'd like to avoid the ""ring of formal expressions"" viewpoint for this question. I know we can avoid these kinds of questions by working ""purely algebraically"" and in particular by taking the algebraic closure of $K(x).$ But I don't want to do this here. If I understand the wikipedia article correctly, a function $f$ is algebraic iff there's a polynomial $P(x,y)$ such that the equation $P(x,f(x)) = 0$ is true for all $x$ in the domain of $f$. For example, the function $$[0,\infty) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$$ is algebraic because $y^2-x$ has this property. However that article is pretty vague and I'm not sure I quite understand what's being said. In particular, which of the following functions would be considered algebraic? $(0,\infty) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$ $(0,1) \cup (2,3) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$ $(0,1) \cup (2,3) \rightarrow \mathbb{R}, x \mapsto \begin{cases}\sqrt{x} & x \in (0,1), \\ -\sqrt{x} & x \in (2,3)\end{cases}$ $(0,e) \rightarrow \mathbb{R}, x\mapsto \sqrt{x}$ $[0,\infty) \rightarrow \mathbb{R}, x \mapsto \begin{cases}\sqrt{x} & x \in \mathbb{Q}, \\ -\sqrt{x} & x \notin \mathbb{Q}\end{cases}$ These pertain to different issues, namely: Does a function have to be defined on the largest possible domain to be considered algebraic? Does it have to be defined on a connected set? If defined on a disconnected set, do we require that it be extensible to an algebraic function defined on a connected set? Can non-algebraic real numbers be used to define the domain of an ""algebraic"" function? Do they have to be continuous? Question: What are the standards here?","['algebraic-curves', 'real-analysis', 'polynomials', 'algebraic-geometry']"
2813129,Do any authors take the sheaf-theoretic viewpoint on multivalued functions and/or indefinite integrals?,"It seems to me that multivalued functions and/or indefinite integrals can be thought of as sheaves. For example: The real square-root function can be viewed as the sheaf $\mathcal{F}$ defined on the open sets of $\mathbb{R}$ by letting $\mathcal{F}(U)$ denote the set of all continuous function $f : U \rightarrow \mathbb{R}$ satisfying $\forall x \in U(f(x)^2 = x).$ The complex square-root function can be viewed as the sheaf $\mathcal{F}$ defined on the open sets of $\mathbb{C}$ by letting $\mathcal{F}(U)$ denote the set of all continuous function $f : U \rightarrow \mathbb{C}$ satisfying $\forall z \in U(f(z)^2 = z).$ The complex logarithm function can be viewed as the sheaf $\mathcal{F}$ defined on the open sets of $\mathbb{C}$ by letting $\mathcal{F}(U)$ denote the set of all continuous function $f : U \rightarrow \mathbb{C}$ satisfying $\forall z \in U(e^{f(z)} = z).$ If $f$ is a continuous real-valued function, we can define $\int f(x) dx$ to be the sheaf $\mathcal{F}$ such that $\mathcal{F}(U)$ is the set of all differentiable functions $F$ on $U$ satifying $F'=f$. More generally, if $\mathfrak{f}$ is a sheaf of continuous functions, we can define $\int \mathfrak{f}(x)dx$ to be the sheaf $\mathcal{F}$ such that $\mathcal{F}(U)$ is the set of all differentiable function $F : U \rightarrow \mathbb{R}$ satisfying $F' \in \mathfrak{f}(U).$ Similar statements to the above probably hold for the complex case, allowing us to prove claims like $$\log(z) \subseteq \int \frac{1}{z}dz,$$ etc. where $\log$ is viewed as the sheaf-theoretic inverse of $z \mapsto e^z$ as described above, and the $\subseteq$ in this context really means something like: for all open $U \subseteq \mathbb{C}$, we have $$(z \mapsto \log(z))(U) \subseteq \left(z \mapsto \int \frac{1}{z}dz\right)(U).$$ Question. Do any published books or articles take this point of view?","['real-analysis', 'sheaf-theory', 'calculus', 'complex-analysis', 'multivalued-functions']"
2813132,Kernel of nilpotent operators on infinite dimensional vector spaces,Suppose $T$ is a nilpotent linear transformations on an infinite dimensional vector space. By nilpotent I mean that $T^k=0$ for some $k$. Does it follow that the kernel of $T$ must be infinite dimensional?,"['functional-analysis', 'linear-algebra', 'operator-theory']"
2813133,Uniform convergence of sequence of complex polynomials,Show that there does not exist any sequence of complex polynomials that converges to $\frac{1}{z^2}$ uniformly on the annulus $A=\{z\in\mathbb{C}:1<|z|<2\}$. I don't know how to even think in this problem. Can someone please give me hint or way ?,"['complex-analysis', 'uniform-convergence']"
2813158,Every path homotopic to a Piecewise linear path (Hatcher 1.1.4),"I am stuck in the question from exercise 1 of Hatcher. A subspace $X\subseteq \mathbb{R}^n$ is said to be star-shaped if there is a point $x_0\in X$ such that, for each $x \in X$ , the line segment from $x_0$ to $x$ lies in $X$.
Show that if a subspace $X \subseteq \mathbb{R}^n$ is locally star-shaped, in the sense that every point of $X$ has a star-shaped neighborhood in $X$, then every path in $X$ is homotopic in $X$ to a piecewise linear path , that is , a path consisting of a finite number of straight line segments traversed at constant speed.show this applies in particular when $X$ is open or when $X$ is a union of finitely many closed convex sets. I have been unable to get the meaning of the question properly and I am not able to figure out a possible approach. Kindly help me out. I have followed this link Exercise 1.1.4 in Hatcher's Algebraic Topology, star-shaped but hasn't been of much help to me. TIA","['algebraic-topology', 'general-topology']"
2813169,Quasinilpotent operators satisfying a polynomial identity,"Suppose $T$ is a bounded quasinilpotent operator (that is, spectrum is zero) on an infinite dimensional Banach space such that $p(T)=0$ for some polynomial $p$. Does it follow that $T$ is actually nilpotent? In the finite dimensional case, quasinilpotent and nilpotent are the same, but not in the infinite dimensional case.","['banach-spaces', 'hilbert-spaces', 'operator-theory', 'functional-analysis', 'linear-algebra']"
2813187,Square root of even degree line bundle,"Let $C$ be a curve of genus $g$. Let $\mathcal{L}$ be a line bundle on $C$ of degree 2n. I would like to know a proof of the following statements: To prove that there is a line bundle $\mathcal{M}$ on $C$ such that $\mathcal{M}^{2}= \mathcal{L}$. To prove that there are $2^{2g}$ many such line bundle which are square 
roots of $\mathcal{L}$.","['riemann-surfaces', 'vector-bundles', 'algebraic-geometry']"
2813188,What is the relationship between different theorems all called Hilbert's Nullstellensatz?,"The following statements are all named the Hilbert's Nullstellensatz, but they appear at first to be completely unrelated to each other. What is the relationship between them exactly? (Theorem 1.3A on page 4 of Hartshorne's Algebraic Geometry) Let $k$ be an algebraically closed field, $a$ be an ideal in $A = k[x_1, ... x_n]$, and $f \in A$ be a polynomial which vanishes on all points of $Z(a)$, then $f^r \in a$ for some integer $r > 0$ (""weak Nullstellensatz"" listed on Wikipedia) An ideal $I \subseteq k[x_1, ... x_n]$ contain 1 iff the polynomials in $I$ do not contain any common zeroes in $k^n$ (Theorem 3.2.4 on page 107 of Vakil's notes) The only maximal ideals in the ring $k[x_1, ... x_n]$ are of the form $(x_1 - a_1, ... x_n - a_n)$, for $(a_1, ... a_n) \in k^n$ (Theorem 3.2.5 on page 107 of Vakil's notes) If $k$ is any field, every maximal ideal of $k[x_1, ... x_n]$ has residue field a finite extension of $k$. Any field extension of $k$ that is finitely generated as a ring is also finitely generated as a module (Theorem 3.7.1 on page 128 of Vakil's notes) Let $A$ be a commutative ring with identity, then $V(.)$ and $I(.)$ give an inclusion reversing bijection between closed subsets of $Spec(A)$ and radical ideals of $A$",['algebraic-geometry']
2813197,Ball rolling along a see-saw,"Suppose we have a see-saw which is 2 metres long, whose mid-point is connected to a fulcrum which is 0.5 metres tall. The see-saw has a mass of 1kg, and has uniform thickness and density. The see-saw starts with one side up, and hence has a slope of 30 degrees (or pi/6 radians). You place a ball of mass $m$ on top of the see-saw. It starts rolling down the see-saw, but as it rolls, it starts pushing that side of the see-saw down. If the ball is too light, the see-saw will not swivel much and the ball will roll off the right hand side of the see-saw. If the ball is too heavy, the ball won't roll much, and instead will push the see-saw down quickly, and will end up rolling off the left hand side of the see saw. The question is: what must the mass of the ball be such that it gets stuck in a dynamic equilibrium. That is, for the first phase (up until the ball reaches the fulcrum) the ball's gravitational potential energy is converted into kinetic energy both of the ball, and of the see-saw. When the ball is at the fulcrum, the see-saw is perfectly flat and the ball and see-saw have both reached their maximum velocity. In the second phase, the ball starts rolling up the right hand side of the see-saw.  The right side of the see saw is still swivelling up, although it's slowing down as the ball presses on it. The ball reaches the top of the see-saw, and then starts rolling down again; repeating the process. NB Assume no friction or air resistance. Model the see-saw as being arbitrarily thin. Neglect the rotational energy of the ball (ie assume the ball is a point mass). I have not attempted to solve this problem analytically (this would require differential equations, I'd imagine). I have used a spreadsheet model, though, to determine that the mass of the ball should be about 0.13 kg, in which case it reaches the fulcrum at around t=0.7 seconds (this was with a model in which time has a 'granularity' of 0.01 seconds). Bonus points: Determine the curve that the ball traces. I think it should be a parabola.","['physics', 'curves', 'calculus', 'geometry', 'ordinary-differential-equations']"
2813208,Prove that $\frac{dy}{dx}+\sec^2(\frac{\pi}{4}-x)=0$,"If $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$, prove that $\frac{dy}{dx}+\sec^2(\frac{\pi}{4}-x)=0$ My attempts : Attempt 1 : $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$ $\implies y=\sqrt{\frac{\sin^2 x+\cos^2 x-2\sin x \cos x}{\sin^2 x+\cos^2 x+2\sin x \cos x}}$ $\implies y=\sqrt{\frac{(\sin x -\cos x)^2}{(\sin x+\cos x)^2}}$ $\implies y=\frac{\sin x -\cos x}{\sin x +\cos x}$ $\therefore \frac{dy}{dx}=\frac{(\sin x +\cos x)(\cos x+\sin x)-(\sin x -\cos x)(\cos x -\sin x)}{(\sin x +\cos x)^2}$ $\implies \frac{dy}{dx}=\frac{(\sin x+\cos x)^2+(\sin x -\cos x)^2}{(\sin x+\cos x)^2}$ $\implies \frac{dy}{dx}=\frac{2(\sin^2 x+\cos^2 x)}{(\cos(\frac{\pi}{2}-x)+\cos x)^2}$ $\implies \frac{dy}{dx}=\frac{2}{(2\cos(\frac{\pi}{4})\cos(\frac{\pi}{4}-x))^2}$ $\implies \frac{dy}{dx}=\frac{2}{4\cos^2(\frac{\pi}{4})\cos^2(\frac{\pi}{4}-x)}$ $\implies \frac{dy}{dx}=\frac{2}{4\times\frac{1}{2}\cos^2(\frac{\pi}{4}-x)}$ $\implies \frac{dy}{dx}=\sec^2(\frac{\pi}{4}-x)$ $\implies \frac{dy}{dx}-\sec^2(\frac{\pi}{4}-x)=0$ But i have to prove $\frac{dy}{dx}+\sec^2(\frac{\pi}{4}-x)=0$ Attempt 2 : $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$ $\implies y=\sqrt{\frac{\cos^2 x+\sin^2 x-2\cos x \sin x}{\cos^2 x+\sin^2 x+2\cos x \sin x}}$ $\implies y=\sqrt{\frac{(\cos x -\sin x)^2}{(\cos x+\sin x)^2}}$ $\implies y=\frac{\cos x -\sin x}{\cos x +\sin x}$ $\therefore \frac{dy}{dx}=\frac{(\cos x +\sin x)(-\sin x-\cos x)-(\cos x -\sin x)(-\sin x +\cos x)}{(\cos x +\sin x)^2}$ $\implies \frac{dy}{dx}=\frac{-(\cos x+\sin x)^2+(\cos x -\sin x)^2}{(\cos x+\sin x)^2}$ $\implies \frac{dy}{dx}=\frac{(\cos x -\sin x)^2-(\cos x+\sin x)^2}{(\cos x+\sin x)^2}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{(\cos x+\cos(\frac{\pi}{2}-x))^2}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{(2\cos(\frac{\pi}{4})\cos(x-\frac{\pi}{4}))^2}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{4\cos^2(\frac{\pi}{4})\cos^2(x-\frac{\pi}{4})}$ $\implies \frac{dy}{dx}=\frac{-4\sin x\cos x}{4\times \frac{1}{2}\cos^2(x-\frac{\pi}{4})}$ $\implies \frac{dy}{dx}=\frac{-2\sin x\cos x}{\cos^2(x-\frac{\pi}{4})}$ Attempt 3 : $y=\sqrt{\frac{1-\sin 2x}{1+\sin 2x}}$ $\implies y=\sqrt{\frac{(1-\sin 2x)(1-\sin 2x)}{(1+\sin 2x)(1-\sin 2x)}}$ $\implies y=\frac{1-\sin 2x}{\cos 2x}$ $\therefore \frac{dy}{dx}=\frac{\cos 2x(-2\cos 2x)-(1-\sin 2x)(-2\sin 2x)}{\cos^2 2x}$ $\implies \frac{dy}{dx}=\frac{-2\cos^2 2x+2\sin 2x(1-\sin 2x)}{\cos^2 2x}$ My questions : (i) Why does Attempt 2 give a result different from that given by Attempt 1 . (ii) How do i prove the result?","['derivatives', 'trigonometry', 'calculus', 'proof-verification']"
2813243,Group with proper subgroups infinite cyclic,"Suppose $G$ is an infinite group such that any proper non-trivial subgroup of $G$ is infinite cyclic. Is $G$ itself then infinite cyclic? If we would only require the proper subgroups to be cyclic, then the Tarski monster groups would yield some counter-examples. Are there analogous examples of Tarski monsters where proper subgroups are infinite cyclic?",['group-theory']
