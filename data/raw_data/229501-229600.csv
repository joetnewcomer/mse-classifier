question_id,title,body,tags
4754996,"Bondy, Murty Graph Theory exercise 16.2.12 (by Noga Alon)","Let $G = (U\sqcup V, E)$ be a bipartite graph in which each vertex of $U$ is of odd degree. Suppose that any two vertices of $U$ have an even number of common neighbours. Show that $G$ has a matching covering every vertex of $U$ . Let $S=\{u_1,\ldots,u_t\}\subseteq U$ . We must show that $|N(S)|\ge |S|=t$ , where $N(S)$ is the set of neighbors of $S$ . In order to use the information about common neighbors, I thought it would be useful to use inclusion-exclusion. We have $$\begin{align}
|N(S)|  &=   \left| \bigcup_{i=1}^t N(u_i) \right| 
\\&= \sum_{i=1}^t |N(u_i)|-\sum_{1\le i<j\le t} |N(u_i)\cap N(u_j)|+\cdots \\&+ (-1)^{t+1} |N(u_1)\cap N(u_2)\cap\ldots \cap N(u_t)| \end{align}$$ We first sum over odd numbers; the second one sums over even numbers. But I don't really see how to continue.","['graph-theory', 'bipartite-graphs', 'discrete-mathematics']"
4755028,Approximation of gamma function via Riemann sums at integer points,"I found something curious. We know that the gamma function is defined as $$ \Gamma(n+1) := \int_{t=0}^\infty t^n \exp(-t) dt,$$ and it has the property that $\Gamma(n+1) = n!$ for non-negative integer $n$ . I recently came across the following Riemann sum approximation of the gamma function: $$ S(n) := \sum_{k=1}^\infty k^n \exp(-k). $$ A priori there is no reason to suspect that $S(n)$ should approximate $\Gamma(n+1)$ at all, since the points are integer, so they are not getting closer. However, I plugged in a couple of values of this sum into Wolfram Alpha (e.g., https://www.wolframalpha.com/input?i=sum+exp%28-n%29+n%5E6 ) and saw that, to my surprise, $S(n)$ has a closed form (some messy formula), and numerically $S(n)$ is pretty close to $n!$ , seemingly improving as $n$ gets large. Is this known? How do we derive the closed form for the sums in general? I tried searching but couldn't find anything. Any pointers would be super helpful! Ultimately I want to prove that $S(n)$ approximates $n!$ as $n$ gets large so any ideas for showing that would also be helpful.","['factorial', 'approximation', 'number-theory', 'gamma-function', 'combinatorics']"
4755071,Must a closed set with empty interior in a separable compact space of cardinality at most $2^{\aleph_{0}}$ be separable?,"Consider a separable compact Hausdorff space $X$ with $|X| \leq 2^{\aleph_{0}}$ . If $A\subseteq X$ is a closed set with empty interior, must $A$ be separable? Each example I've been able to come up with in this setting is either countable or can be proven to be separable. EDIT: Edited question to reflect the question at base.","['general-topology', 'separable-spaces', 'analysis']"
4755125,Solution of ODE with discontinuous coefficients,"Can anyone please point to any references regarding solutions of ODE of the form $$(a+1_{x=0})f'(x)-f(x)=h(x)-\int_0^\infty h(t)e^{-t}dt \tag{1}$$ with $f(0)=0$ , where $h$ is a given function, a is some constant and $1_{x=0}$ is the indicator function. I only need the solution for $x\geq 0$ .  I see that when the coefficient is some continuous function of $x$ , say $b(x)$ , that is $$b(x)f'(x)-f(x)=h(x)-\int_0^\infty h(t)e^{-t}dt$$ the solution is given by $$-e^{-\int_0^x\frac{1}{b(u)}du}\int_{x}^\infty\frac{1}{b(y)}\left(h(y)-\int_0^\infty h(t)e^{-t}dt\right)e^{\int_0^y\frac{1}{b(u)}du}dy$$ I am not sure how to proceed when the coefficient is not continuous.","['calculus', 'ordinary-differential-equations', 'real-analysis']"
4755149,"Let $p$ be Riemann integrable function with $p(x) \ge 0,$ for all $x \in [a, b]$. If $\int_{a}^{b} p(x)dx = 0,$ then the set Y where p(x) = 0 is dense","I'm studying for a Master's admission exam and I came across this question: Let $p : [a, b] \longrightarrow \mathbb{R}$ integrable, with $p(x) \ge 0,$ for every $x \in [a, b]$ . If $\int_{a}^{b} p(x)dx = 0,$ then the set $Y = \{x \mid p(x) = 0\}$ is dense in $[a, b]$ . I saw that you already asked about this question here (I can't put the link) and they talked about using measurement theory. I could not understand the solutions presented, so I would like to try to present a solution. I don't have any knowledge about Measure Theory. Suppose that $Y$ is not dense in $[a, b]$ . Then there exists a point $\alpha \in [a, b]$ and $\varepsilon > 0$ such that $$(\alpha - \varepsilon, \alpha + \varepsilon) \ \cap \ Y = \emptyset.$$ Let $[\beta_1, \beta_2] \subset (\alpha - \varepsilon, \alpha + \varepsilon) \subset [a, b].$ Then $$ [\beta_1, \beta_2]\ \cap \ Y = \emptyset.$$ We say that $f$ is Riemann-integrable iff the set of points of discontinuities of f has measure zero. Since $p$ is integrable and $[a, b]$ don't have measure zero, $p$ it must be continuous at at least one point $x \in [a, b]$ . Again, let's assume that $p$ is discontinuous at all points in the interval $(\alpha - \varepsilon, \alpha + \varepsilon)$ . Then $p$ is discontinuous in $[\beta_1, \beta_2]$ . But $p$ is integrable in $[\beta_1, \beta_2]$ and $[\beta_1, \beta_2]$ has non-zero measure, wich is a contradiction. Therefore, there must be some point $y \in $ $[\beta_1, \beta_2]$ where $p$ is continuous. Note that $p(y) > 0$ . Soon $$\int_{\beta_1}^{\beta_2} p(x)dx > 0.$$ Following that $$\int_{a}^{b} p(x)dx = \int_{a}^{\beta_1} p(x)dx + \int_{\beta_1}^{\beta_2} p(x)dx + \int_{\beta_2}^{b} p(x)dx$$ $$\Rightarrow 0 = \int_{a}^{\beta_1} p(x)dx + \int_{\beta_1}^{\beta_2} p(x)dx + \int_{\beta_2}^{b} p(x)dx$$ which again is a contradiction, since $\int_{a}^{\beta_1} p(x)dx, \int_{\beta_2}^{b} p(x)dx \geq 0 $ . I really believe this solution is wrong. That's why I'm presenting in the hope that someone can help me improve. All the solutions that were presented were quite sophisticated and either used measurement theory or were difficult for me. I believe that the error of my solution is when I say that there is a point $y$ where p is continuous.","['integration', 'real-analysis']"
4755184,Printing neatly,"I'm working on the following problem (which is not my actual question) Consider the problem of neatly printing a paragraph with a monospaced font (all
characters having the same width). The input text is a sequence of $n$ words of
lengths $l_1, l_2,...,l_n$ , measured in characters, which are to be printed neatly on a
number of lines that hold a maximum of $M$ characters each. No word exceeds
the line length, so that $l_i\le M$ for $i=1, 2, ... ,n$ . The criterion of neatness is
as follows. If a given line contains words $i$ through $j$ , where $i \le j$ , and exactly
one space appears between words, then the number of extra space characters at the
end of the line is $M-j+i-\sum_{k=i}^j l_k$ which must be nonnegative so that the
words fit on the line. The goal is to minimize the sum, over all lines except the last,
of the cubes of the numbers of extra space characters at the ends of lines. Give a
dynamic-programming algorithm to print a paragraph of $n$ words neatly. Analyze
the running time and space requirements of your algorithm. [1] in order to appreciate dynamic programming (which I found can run in $O(n)$ time), I wonder what if we use a brute force search instead? So my actual question is what's the size of the search space if we're to use brute force search? I tried this: Words must be printed in fixed order. Suppose no empty line is allowed, we know the $i+1$ th words must come right after the $i$ th word and that they may either be on the same line or on adjacent line. We want to find the total number of arrangement of the word sequence. Consider the set of all arrangements $A_i$ of a sequence of $i$ words and the binary variable $b(a,i+1)$ is $1$ only if the $i+1$ th word can be on the same line as the $i$ th word in the arrangement $a\in A_i$ . Then $$
|A_n| = |A_{n-1}| + \sum_{a\in A_{n-1}} b(a,n)
$$ which of course implies $|A_{n-1}|  \le |A_n| \le 2|A_{n-1}| $ and since $|A_1|=1$ we have $1 \le |A_n| \le 2^{n-1}=O(2^n)$ . However I'm not sure this is correct especially I haven't well defined what ""arrangement"" is using $M$ and $l_1,...,l_n$ ... [1] Introduction To Algorithms, MIT","['recurrence-relations', 'combinatorics', 'discrete-mathematics', 'algorithms', 'computer-science']"
4755246,The boundary of any manifold that is itself a boundary of a higher-dimensional manifold must always be empty?,"I have seen it asserted several times that it is well-known that the boundary of any manifold that is itself a boundary of a higher-dimensional manifold must always be empty. Yet, despite it supposedly being well-known, I have not been able to find any proof of this theorem. I have also searched through two popular topology textbooks – Topology by Munkres and Algebraic Topology by Hatcher – but have not been able to find this theorem. Is there any reputable source, such as a well-known topology textbook, that discusses this theorem and provides a proof? Or, if anyone here has a (relatively basic) proof of this theorem, I'd appreciate it.","['manifolds', 'general-topology', 'submanifold', 'manifolds-with-boundary']"
4755266,Bounding an unknown IVP solution,"Given the Following IVP: $$y'=f(x,y)=\frac 1 {2+{\cos(xy)}}$$ $$y(0)=\frac 1 2$$ I need to show that the IVP has a single solution $u(x)$ that is defined in $(-\infty,\infty)$ , increasing, and that $u(x)<x$ for all $x>1$ I was able to show, using the fact that $f_y(x,y)=\frac {x \sin(xy)} {{2+\cos(xy)}^2}$ is bounded in $[a,b]\times(-\infty,\infty)$ for all $a$ , $b$ , that there exist a single solution, and to show that it's derivative is positive, so that it is increasing, but I couldn't get the inequality right. I Tried using the Mean Value Theorem but the bound that I got wasn't tight enough ( $u(x)<x+\frac 1 2$ ). I also tried to bound integral up to a general point $x>1$ but this didn't work either. Any assistance would be appreciated.","['initial-value-problems', 'ordinary-differential-equations']"
4755272,"Counting the pentagons and hexagons on the surface of a football (aka, soccer ball)","For the Americans: I'm talking about soccer :-) As you might know, a football consists of a collection of regular pentagons and hexagons, but how much of which ones? The construction is simple: for every pentagon, there's a hexagon at each side and for every hexagon, the polygons at its sides are altering between a pentagon and a hexagon. Let's say we have the following numbers: $p$ : the number of points $s$ : the number of sides $u$ : the number of surfaces. There is the well-known formula: $$p - s + u = 2$$ Let's now say: $v$ : the number of pentagons (""vijfhoek"" in Dutch) $h$ : the number of hexagons Then we know that: $$v + h = u$$ Every point is related to three segments and three surfaces, and every segment is related to two surfaces. But how to enter this in a formula to calculate the number of pentagons and hexagons? Does anybody have an idea? Thanks in advance","['recreational-mathematics', 'geometry', 'polygons']"
4755281,Intersection of a small Riemannian sphere and a submanifold,"Suppose $N$ is a closed submanifold in a Riemannian manifold $(M, g)$ . For $q \in M \setminus N$ , consider the sphere $$S = S\big(q, d(q,N)\big) = \{x \in M | d(q, x) = d(q, N)\},$$ where $d(q,N) = \min \{d(q, y) | y \in N\}$ is the distance from $q$ to $N$ . Clearly, $S \cap N$ consists of precisely those points of $N$ where the distance $d(q,N)$ is attained. Now, it seems very intuitive to me that if $d(q, N)$ is sufficiently small, the intersection is a singleton. My reasoning is that for $d(q,N)$ sufficiently small, the sphere is ""too much curved"", compared to the (compact) submanifold $N$ , which makes sure that the sphere can only ""touch"" $N$ at a single point. Is my reasoning remotely correct? If so, could someone please point out how to make this rigorous, or maybe point to any reference? Thanks in advance. Cheers!","['riemannian-geometry', 'differential-geometry']"
4755284,What is the difference between a general solution of a differential equation and a family of solutions to a differential equation?,"I'm having trouble learning differential equations. I'm a bit confused about the difference between a general solution of a differential equation and a family of solutions to a differential equation. I've repeatedly read obscure textbooks and concluded the following: The general solution of a differential equation refers to the set of all solutions that satisfy the differential equation, and it usually contains some arbitrary constants, which can be determined according to the initial conditions or boundary conditions. For example, the general solution to the differential equation $y′=y$ is $y=Ce^{x}$ , where $C$ is an arbitrary constant. a family of solutions to a differential equation refers to a class of special solutions that satisfy differential equations, and there is a certain relationship between them, usually in the form of parameterization. For example, the family of differential equations $y′=y$ is $y=ae^{x+b}$ , where a and b are parameters (1)The general solution of differential equations contains all the solutions satisfying the differential equations, while the family of solutions to a differential equation to a differential equation contains only a part of the solutions satisfying the differential equations. (2)Any constant in the general solution of differential equations can determine the unique solution through initial conditions or boundary conditions, but the parameters in the  family of solutions to a differential equation cannot determine the unique solution through initial conditions or boundary conditions, but need additional parameter values. (3)The family of solutions to a differential equation can be obtained by some transformation of the general solution of differential equations, but not necessarily vice versa May I ask if there are any mistakes in the summary of the above three points?","['ordinary-differential-equations', 'terminology']"
4755315,Is the set open or closed?,"Let f be a smooth function with $f: Ω⊂R^m→R$ Is the set $\{(x,y)∈Ω×R|y=f(x)\}$ open? A set is considered open if, for every point within the set, you can find a small enough region around that point that is entirely contained within the set. I am thinking that the set is open since in our case, for any point $(x, f(x))$ on the graph, you can find a small enough neighborhood around it (in the domain $Ω$ ) such that all the points in that neighborhood, when combined with the corresponding y values (i.e., $f(x)$ ), lie within the graph. Am I right that the set is open? Its not really mathematically shown since I argued in a more informal way.","['general-topology', 'functions', 'smooth-functions']"
4755357,Prove that $x^{2020} + x^{1011} + 2 x^{1010} + x^2 + x + 1$ does not have a real root,Prove that $$x^{2020} + x^{1011} + 2 x^{1010} + x^2 + x + 1$$ does not have a real root. The degree of the polynomial is $2020$ . That's a enough big number. Usually I know some ways for small degree polynomials but it didn't work here. I've been trying to try the sum of squares method but haven't been able to. I tried putting the $x^{1010}$ out of the parenthesis $$x^{1010}(x^{1010} + x + 2) + x^2 + x + 1$$ That didn't work either. I'm looking for a derivative-free solution.,"['algebra-precalculus', 'polynomials']"
4755402,How many groups $G_i$ (up to isomorphism) are there such that $G\cong G_i \times \mathbb{Z}$?,"Recall that a group $G$ is polycyclic if it has a finite series
of subgroups $G = G_0 \rhd G_1 \rhd \cdots \rhd G_k = 1$ for which each factor $G_{i−1}/G_i$ (where $i = 1, \ldots , k$ ) is finite cyclic or infinite cyclic. A group $G$ is called virtually polycyclic if there exists a polycyclic group $H \lhd G$ such that $G/H$ is finite. Assume that $G$ is a virtually polycyclic group. How many groups $G_i$ (up to isomorphism) are there such that $G\cong G_i \times \mathbb{Z}$ ? It is a  finite number? My try : I think it's finite.  Since $G\cong G_i \times \mathbb{Z}$ and $G$ is virtually polycyclic, $G_i \times \mathbb{Z}$ is virtually polycyclic. So we can comclude that $G_i$ 's are virtually polycyclic with Hirsch length $<h(G)$ . So the class of $G_i$ 's is classified into finitely many subclasses $\mathcal{W}_i$ in each of which two groups have the same Hirsch length. My probelm is that I can't prove that every class has a finite number of elements.",['group-theory']
4755404,Limit question regarding logarthmic function.,$\displaystyle{\lim_{x \to 0}}f(x)$ where $f(x)$ = $\frac{\ln(1+x^2+x)+\ln(1+x^2-x)}{secx-cosx}$ What I found was the two ways Way 1 $L$ = $\frac{\frac{\ln(1+x^2+x)(x+x^2)}{(x+x^2)}+\frac{\ln(1+x^2-x)(x^2-x)}{(x^2-x)}}{secx-cosx}$ $L$ = $\frac{2x^2}{secx-cosx}$$=$$2$ Way2- $\frac{\ln(1+x^2+x)+\ln(1+x^2-x)}{secx-cosx}$ $L$ = $\frac{\ln((1+x^2+x^4)}{secx-cosx}$ $L$ = $\frac{(x^2+x^4)}{secx-cosx}$ =1 Which method is correct and why?,['limits']
4755448,A clue for the geometry problem,"With respect to the picture $AB=DC  , \ B=10\alpha ,A_1=\alpha , c=2\alpha$ so need $\alpha$ I tried first to $A=A_1+A_2=\alpha +\beta$ so $$\beta =180-13\alpha$$ then draw a line $AF=AB$ as below or $DG=AB=DC$ but got nothing,
(seems like parallel lines, but I have no evidence to show this!) . I get stuck on this after about 2 hours. Actually, I was far from geometry for years. Now I want a clue or a hint to take over and find a solution. Thanks in advance Remark: I tried to write $sine/cosine  $ theorem ,but needs the relation for $5x $ arcs and become more complicated.","['euclidean-geometry', 'triangles', 'geometry']"
4755460,Existence of unique solution of initial value problem,"Let us take initial value problem $y'+\frac{{2}}{{t}}y=4t$ , $y(1)=2$ where $p(t)=\frac{{2}}{{t}}$ and $q(t)=4t$ . Here $q(t)$ is continuous for all $t$ while $p(t)$ is continuous only for $t<0$ or $t>0$ . The interval $t>0$ contains the initial point; consequently theorem guarantees that the initial value problem has unique solution on the interval $0<t<\infty$ , but if the initial condition $y(1)=2$ is changed to $y(-1)=2$ the existence of solution will be on the interval $-\infty<t<0$ containing the initial point. Now my question is that is it necessary to look upon the interval which necessarily must contain initial point for the existence of solution of initial value problem? Is it valid or does there exist solution on those interval which doesn't contain initial point but those coefficients $p(t)$ and $q(t)$ are continuous for all the points contained in those interval?","['initial-value-problems', 'ordinary-differential-equations']"
4755471,Doubling metric spaces,"A metric space $(X,d)$ is said to be doubling if there exists a constant integer $N\ge 1$ such that every closed ball of radius $R$ can be covered by at most $N$ closed balls of radius $R/2$ . Proposition. If a metric space $(X,d)$ is doubling, then there exist constants $C\ge 1, s>0$ such that for every $x\in X$ , $0<r<R$ it holds the following implication: if $Y$ is a subset
of the ball $\mathbb{B}(x,R)$ such that $d(y,y')>r$ for every $y,y'\in Y, y\ne y'$ ,
then $Y$ has cardinality less or equal than $C(R/r)^s$ . My questions are the following ones. How can I prove the Proposition? My idea is to use the following remark: every doubling metric space with doubling constant $N$ has de Groot dimension $N$ , that is to say, for each $x\in X, r>0$ , it holds that the cardinality of $Y$ is less or equal than $N$ for every set $Y\subseteq\mathbb{B}(x,r)$ such that $d(y,y')>r$ for every $y,y'\in Y$ with $y\ne y'$ . How can I prove that if there exists a doubling Borel outer measure over $(X,d)$ , then the metric space is doubling? I know that there exists a similar question to the current on MathStack but I'm looking for a more complete and detailed question. Can anyone help me please?","['general-topology', 'metric-spaces', 'measure-theory', 'real-analysis']"
4755505,Relation between range of $\sin(\pi x)$ and $\sin(b^n \pi x)$ on a monotonic region when $|b| > 1$,"If $|b| > 1$ then $\sin(b^n \pi x)$ compresses the $\sin(\pi x)$ graph horizontally by a factor of $b^n$ : The idea that comes to mind is that: If $x_0 \in [x_1,x_2]$ and $\sin(\pi x)$ is monotonic in that region without changing the sign and $|b| > 1$ and $x \neq \frac{p}{b^q} $ when $p,q \in \mathbb{Z}$ , then for infinity $n$ in $\mathbb{N}$ , also $\sin(b^n \pi x_0)$ is between $\sin(x_1)$ and $\sin(x_2)$ . Is above guess true? I want to find prove or disprove for that. I think it must be provable by Archimedean property ; but do not know how.","['calculus', 'functions', 'trigonometry', 'real-analysis']"
4755529,Reciprocal of power series has a power series; Hunter,"I'm reading these lecture notes by Hunter, specifically chapter 10 on power series, and I have some elementary questions about a proof on the proposition stating that the reciprocal of a convergent power series that is nonzero at its center also has a power series. Everything is real here. The following proposition is used in the proof: Proposition 10.15. If $R, S>0$ and the functions $$
f(x)=\sum_{n=0}^{\infty} a_{n} x^{n} \quad \text { in }|x|<R, \quad g(x)=\sum_{n=0}^{\infty} b_{n} x^{n} \quad \text { in }|x|<S
$$ are sums of convergent power series, then $$
\begin{aligned}
(f+g)(x) & =\sum_{n=0}^{\infty}\left(a_{n}+b_{n}\right) x^{n} & & \text { in }|x|<T, \\
(f g)(x) & =\sum_{n=0}^{\infty} c_{n} x^{n} & & \text { in }|x|<T
\end{aligned}
$$ where $T=\min (R, S)$ and $$
c_{n}=\sum_{k=0}^{n} a_{n-k} b_{k} .
$$ Now follows the proposition and its proof that I have some questions about. Proposition 10.16. If $R>0$ and $$
f(x)=\sum_{n=0}^{\infty} a_{n} x^{n} \quad \text { in }|x|<R,
$$ is the sum of a power series with $a_{0} \neq 0$ , then there exists $S>0$ such that $$
\frac{1}{f(x)}=\sum_{n=0}^{\infty} b_{n} x^{n} \quad \text { in }|x|<S .
$$ The coefficients $b_{n}$ are determined recursively by $$
b_{0}=\frac{1}{a_{0}}, \quad b_{n}=-\frac{1}{a_{0}} \sum_{k=0}^{n-1} a_{n-k} b_{k}, \quad \text { for } n \geq 1 .
$$ Proof. First, we look for a formal power series expansion (i.e., without regard to its convergence) $$
g(x)=\sum_{n=0}^{\infty} b_{n} x^{n}
$$ such that the formal Cauchy product $f g$ is equal to 1 . This condition is satisfied if $$
\left(\sum_{n=0}^{\infty} a_{n} x^{n}\right)\left(\sum_{n=0}^{\infty} b_{n} x^{n}\right)=\sum_{n=0}^{\infty}\left(\sum_{k=0}^{n} a_{n-k} b_{k}\right) x^{n}=1 .
$$ Matching the coefficients of $x^{n}$ , we find that $$
a_{0} b_{0}=1, \quad a_{0} b_{n}+\sum_{k=0}^{n-1} a_{n-k} b_{k}=0 \quad \text { for } n \geq 1,
$$ which gives the stated recursion relation. To complete the proof, we need to show that the formal power series for $g$ has a nonzero radius of convergence. In that case, Proposition 10.15 shows that $f g=1$ inside the common interval of convergence of $f$ and $g$ , so $1 / f=g$ has a power series expansion. I'm a bit confused about the last paragraph. I don't see why $1/f=g$ has a power series expansion. The whole proposition is about showing that $1/f$ can be written as a power series. This has not been proven yet, right? We assume without loss of generality that $a_{0}=1$ ; otherwise replace $f$ by $f / a_{0}$ .
The power series for $f$ converges absolutely and uniformly on compact sets inside its interval of convergence, so the function $$
\sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n}
$$ is continuous in $|x|<R$ and vanishes at $x=0$ . It follows that there exists $\delta>0$ such that $$
\sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n} \leq 1 \quad \text { for }|x| \leq \delta \tag{1}
$$ Then $f(x) \neq 0$ for $|x|<\delta$ , since $$
|f(x)| \geq 1-\sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n}>0 \tag{2}
$$ so $1 / f(x)$ is well defined. The author states a power series converges absolutely and uniformly on compact sets within its radius of convergence. The latter implies that a power series therefor is continuous within its radius of convergence. That said, I'd like to verify why $h(x)=\sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n}$ is continuous, as claimed above. If $f(x)=\sum_{n=0}^{\infty}a_{n}x^{n}$ converges for $|x|<R$ , so does $g(x)=\sum_{n=0}^{\infty}\left|a_{n}\right||x|^{n}$ on $|x|<R$ according to the author. $g(x)$ is just another power series, so it is continuous on $|x|<R$ . Hence, since $a_0=1$ , $h(x)=g(x)-1$ and this is continuous since it is the difference of two continuous functions. Is this reasoning correct? I am a little confused about $(1)$ and $(2)$ . I suppose $(1)$ is just the definition of continuity with $<$ replaced by $\leq$ . However, where do the two inequalities in $(2)$ follow from? Why do they hold for $|x|<\delta$ ? We claim that $$
\left|b_{n}\right| \leq \frac{1}{\delta^{n}} \quad \text { for } n=0,1,2, \ldots
$$ The proof is by induction. Since $b_{0}=1$ , this inequality is true for $n=0$ . If $n \geq 1$ and the inequality holds for $b_{k}$ with $0 \leq k \leq n-1$ , then by taking the absolute value of the recursion relation for $b_{n}$ , we get $$
\left|b_{n}\right| \leq \sum_{k=1}^{n}\left|a_{k}\right|\left|b_{n-k}\right| \leq \sum_{k=1}^{n} \frac{\left|a_{k}\right|}{\delta^{n-k}} \leq \frac{1}{\delta^{n}} \sum_{k=1}^{\infty}\left|a_{k}\right| \delta^{k} \leq \frac{1}{\delta^{n}},
$$ so the inequality holds for $b_{k}$ with $0 \leq k \leq n$ , and the claim follows. We then get that $$
\limsup _{n \rightarrow \infty}\left|b_{n}\right|^{1 / n} \leq \frac{1}{\delta}
$$ so the Hadamard formula in Theorem 10.6 implies that the radius of convergence of $\sum b_{n} x^{n}$ is greater than or equal to $\delta>0$ , which completes the proof.","['proof-explanation', 'real-analysis', 'calculus', 'sequences-and-series', 'power-series']"
4755562,Possible error in Stanley's combinatorics volume 1,Let $E(n)$ be the number of functions $f:[n]\to [n]$ without fixed points. An exercise (ex. 10 chapter 2) of Stanley combinatorics volume 1 asks to prove that $E(n)/n!\to 1/e$ . But this seems blatantly false. Because clearly $E(n)=(n-1)^n$ and $(n-1)^n/n!\to \infty$ . Did the author mean $E(n)/n^n$ ? I'm uncertain because in the solution he insists on saying $E(n)/n!$ .,"['limits', 'combinatorics', 'discrete-mathematics']"
4755581,Showing that an unknown IVP solution is an even function,"Given the following IVP: $$y'=f(x,y)=4x^3 e^{-|x^2+y|}$$ $$y(x_0)=y_0$$ I need to show that the IVP has a single solution in $(-\infty,\infty)$ , and that it is an even function. I was able to show, after some struggles that $f(x,y)$ fulfills Lipschiz condition with respect to $y$ in every closed vertical infinite rectangle of the form $[a,b]\times(-\infty,\infty)$ (using showing it separately for the functions in the composition of $4x^3e^z$ with $|x^2+y|$ ), thus there is a single unique solution in $(-\infty,\infty)$ . Yet, I fail to show that the solution $u(x)$ is an even function. The problem arises when I tried to show that the derivative of the solution, $u'(x)$ , is odd, since $f(x,u)$ depends on $u(x)$ itself, which leads me to a cyclic argument. Any hint would be appreciated.","['initial-value-problems', 'ordinary-differential-equations']"
4755633,Notions of measurability restricted to an event.,"Suppose on a probability space $(\Omega,\mathcal{A},\mathbb{P})$ we have $\mathcal{F}$ and $\mathcal{G}$ , independent sub-sigma-algebras of $\mathcal{A}$ . Consider $F\cap G\in \sigma(\mathcal{F},\mathcal{G})$ where $F\in \sigma(\mathcal{F},\mathcal{G})$ and $G\in\mathcal{G}$ . If we can show, due to the structure of $\mathcal{G}$ , that $F\cap G\in \mathcal{F}\cap G$ (where $\mathcal{F}\cap G$ denotes the trace sigma-algebra on the event $G$ ) can we conclude that $F\in\mathcal{F}$ ? Although I do not think so, to give some context, I came across two papers that make essentially the above reasoning, with the sole difference that the final claim is that $\mathbb{1}_F$ is $\mathcal{F}$ -measurable 'on $G$ ' (a claim that does not seem to make sense to me, as I have never met a notion of measurability restricted to an event, and therefore I interpreted as given above) and then go on to use the fact in computing $\mathbb{E}(X\mathbb{1}_F\mathbb{1}_G|\mathcal{F})=\mathbb{P}(G)X\mathbb{1}_F$ , having exploited that $X$ is a $\mathcal{F}$ -measurable random variable. Clearly this step is what suggest my interpretation of their claim. However, I failed to prove the theoretical fact above. Any ideas?","['measure-theory', 'conditional-probability', 'conditional-expectation', 'probability-theory', 'probability']"
4755637,Where are the zeros of a slightly perturbed Riemann Zeta function?,"We seek to understand the locations of the zeros when we introduce a minor perturbation to the Riemann Zeta function: ${\displaystyle \zeta (s)=\sum _{n=1}^{\infty }{\frac {1}{n^{s}}}={\frac {1}{1^{s}}}+{\frac {1}{2^{s}}}+{\frac {1}{3^{s}}}+{\frac {1}{4^{s}}}+\cdots}$ For instance, consider the following slightly perturbed Zeta function: ${\displaystyle N(s)={\frac {1}{1^{s}}}+{\frac {2}{2^{s}}}+{\frac {1}{3^{s}}}+{\frac {1}{4^{s}}}+\cdots}$ Where are its zeros located on the complex plane? (We can assume the Riemann Hypothesis is true.)","['number-theory', 'riemann-hypothesis', 'complex-analysis', 'analytic-number-theory', 'riemann-zeta']"
4755662,Why are coordinate maps diffeomorphisms? (A seeming counterexample?),"In Tu's ""An Introduction to Manifolds"", Prop 6.10 says: If $(U,\phi)$ is a chart on the manifold $M$ of dimension $n$ , then the coordinate map $\phi:U\to\phi(U)\subset\mathbb{R}^n$ is a diffeomorphism. However, I come up with a seeming counterexample from exercise 6.1. Consider $(\mathbb{R},\psi)$ where $\psi = x^{\frac{1}{3}}$ is a homeomorphism but not a diffeomorphism. I don't see why this didn't hold water. Definitely something is wrong... Any help will be appreciated! Thanks in advance!","['manifolds', 'smooth-functions', 'differential-geometry']"
4755663,Must injective path-connected maps/“path-Darboux” functions be continuous?,"(Inspired by https://mathoverflow.net/questions/235893/does-there-exist-a-bijection-of-mathbbrn-to-itself-such-that-the-forward-m?rq=1 ) Let us define a Darboux function or connected map to be a map between topological spaces that preserves connected sets (i.e. the image of any connected set is connected); and a path-Darboux function or path-connected map to be a map that preserves path-connected sets. Conjecture: every injective path-connected map $f:[0,1]\hookrightarrow \mathbb R^2$ has to be continuous. I make several pathetic attempts at this Conjecture below and draw some pictures, but honestly I do not expect nor wish anyone to do anything more than barely skim through them. Preliminary Remarks: The converse is of course true: all continuous functions are path-connected maps. Injectivity is important because of the example of ""everywhere locally surjective"" functions (functions whose images of any nonempty open set is the entire codomain), like the Conway Base-13 function, or another example using the Riemann rearrangement theorem on the alternating harmonic series . The Conjecture is true for $f: [0,1] \hookrightarrow \mathbb R$ , because we know all discontinuities of Darboux functions are essential , but essential discontinuities contradict injectivity. If one considers ""connected"" instead of ""path-connected"", the Conjecture is false, by considering the graph of the topologist sine curve $g$ with $g(0):=(0,0)$ ( $g(t):= (t,\sin(\frac 1t))$ for $t\in (0,1]$ ). Let us now assume $f$ is not (right-)continuous at $0$ , and normalize $f(0)=0$ . I think user TheEmptyFunction's answer to the above linked MO post https://mathoverflow.net/a/353018/112504 also applies here to tell us that there are arbitrarily small radii $\epsilon>0$ s.t. there is a sequence $y_n^\epsilon \searrow 0$ s.t. $f(y_n^\epsilon) \in C(0,\epsilon)$ ---- (illustrated below as the blue points, which I think we can normalize via pre-composing with a homeomorphism of $[0,1]$ and post-composing with a homeomorphism of $\mathbb R^2$ to be $\{\frac 1m: m \in \mathbb N^+\}$ in the domain, and $i\cdot e^{-\pi/2 \cdot \frac 1m})$ in the codomain, with $\epsilon=1$ ) By mapping $(\frac 1{m+1},\frac 1m]$ to the line segment between $0\in \mathbb C$ (not inclusive) and $i\cdot e^{-\pi/2 \cdot \frac 1m} \in \mathbb C$ (inclusive) in a linear manner say (illustrated above), we get an injective map $[0,1]\to \mathbb R^2$ that maps any interval $[0,a\rangle$ (where the right-hand bracket $\rangle$ denotes both "" $)$ "" and "" $]$ "") to a path-connected set in $\mathbb R^2$ (perhaps we could say this function is a ""initial-segment path-connected map""); but any interval $(0,a\rangle$ would not map to a path-connected set. This example is a ""near miss"" to the Conjecture. $$$$ One idea: projection onto 1st and 2nd coordinates are both continuous maps, hence preserve path-connectedness. Thus if $f:=[t\mapsto (x(t),y(t))]:[0,1] \to \mathbb R^2$ is a path-connected map, $x,y:[0,1]\to \mathbb R$ must be as well. As I mentioned above, a well-known class of non-continuous path-connected maps $[0,1]\to \mathbb R$ are the ""everywhere locally surjective"" functions. By postcomposing with a homeomorphism $\mathbb R \to (0,1)$ , we can think of $x,y$ as ""everywhere locally surjective"" maps $x,y:[0,1]\to (0,1)$ . Then to make $f$ bijective, we need $y$ to be injective when restricted on each fiber $x^{-1}(r)$ for all $r\in \mathbb R$ (where each such fiber is dense in the domain), and vice versa ( $x$ injective when restricted to each fiber $y^{-1}(r)$ for all $r\in \mathbb R$ ). Maybe something like this could work if we can partition $[0,1]$ into disjoint classes $C_\alpha$ where each $C_\alpha \cap x^{-1}(r)$ contains exactly 1 element (so each $C_\alpha$ contains exactly one element that gets mapped to $r$ , for each real number $r$ ), and if $y$ attains a different constant value $c_\alpha$ on each $C_\alpha$ . And to make $y$ ""everywhere locally surjective"" we need each $C_\alpha$ to be dense in $[0,1]$ . However I doubt that such a function preserves path-connectedness; it sends any interval $(a,b)$ (or $[a,b)$ , etc.) to a set with lots of pieces of horizontal and vertical lines; if the pieces are too fragmented, we won't get path connectivity. I can imagine that if $y$ was monotone on $x^{-1}(r)$ or something, then we would see a longer vertical line piece at $x=r$ ; but unfortunately as I commented here this desired monotonicity is not possible. $$\newcommand{\im}{\operatorname{im}} \newcommand{\harmonicint}[1]{[\frac 1{#1+1},\frac 1{#1}]} \newcommand{\harmonicintt}[1]{[1/({#1+1}),1/{#1}]}$$ Infinitely Many Discontinuities, and Trees In working with the previous near-miss example, I got the impression that in fact MiniConjecture: one point $p$ of discontinuity of $f$ begets infinitely many points of discontinuity converging to $p$ , making such a potential discontinuous $f$ quite difficult to imagine. Unfortunately I could not prove this MiniConjecture . Attempted proof that 1 discontinuity begets infinitely many: Since we know that each $[0, \frac 1m]$ maps to a path-connected set containing $f(\frac 1m)$ and $f(0)=0$ , let us define : paths $P_m : [-\frac 1m, 0] \to f([0, \frac 1m])\subseteq \mathbb R^2$ , which we can assume to be injective by e.g. Equivalence of Path-Connectedness and Arc-Connectedness for Hausdorff Spaces , or using Zorn's lemma (article by Jeremy Brazas) --- illustrated below in orange. and paths $F_m: [-\frac 1m, - \frac 1{m+1}] \to f([\frac 1{m+1},\frac 1m]) \subseteq \mathbb R^2$ going from $f(\frac 1m)$ to $f(\frac 1{m+1})$ (which we can again assume injective; and furthermore because $f$ is injective, the paths $F_m$ will be disjoint except touching perhaps at endpoints $f(\frac 1m)$ ) --- illustrated below in green. Now if $f|_{[1/(m+1),1/m]}$ is continuous as a function $[\frac 1{m+1},\frac 1m]\hookrightarrow \mathbb R^2$ , then $f|_{[1/(m+1),1/m]}$ is in fact a homeomorphism onto its image $f(\harmonicint m) \supseteq \im(F_m)$ . So its inverse is a continuous map, mapping the path-connected set $\im(F_m)$ to path-connected set in $\harmonicint m$ containing $\frac 1{m+1},\frac 1m$ (since the path $F_m$ goes through those points), meaning $f^{-1}$ maps $\im(F_m) \subseteq f(\harmonicint m)$ to all of $\harmonicint m$ , implying that $f|_{\harmonicintt m}$ is in fact a continuous bijection ( and hence homeomorphism) $\harmonicint m \to \im(F_m) = f(\harmonicint m)$ . So if $f$ is continuous on $(0,\eta)$ for some $\eta>\frac 1M >0$ , then
all $\harmonicint m$ inside $(0,\eta)$ will be mapped by $f$ to a path from $f(\frac 1{m+1})$ to $f(\frac 1m)$ ; in other words, $f$ is composed of all these paths $F_m$ stitched together at the endpoints. Now if I had the injective continuous path $P_M :[-\frac 1M, 0] \hookrightarrow f([0,\frac 1M])$ that goes through $f(\frac 1M)$ and $f(0)=0$ , the fact that $f((0,\frac 1M])$ is the continuous image of $(0,1]$ (so a path/""ray"" with a perhaps infinite ""tail"" that may wiggle a lot, but still pass through all of the points $f(\frac 1m)$ illustrated below), intuitively $P_M$ must end up travelling along this ""ray"". Then one would hope to get a contradiction because if $P_M$ travels along this ray, then it would be forced to continue to come across the points $f(\frac 1m)$ (blue points in below pictures), and because the points $f(\frac 1m)$ are so far from $f(0)=0$ , this would contradict that $P_M$ is a continuous path $[-\frac 1M, 0] \hookrightarrow f([0,\frac 1M])$ . Sadly after making many attempts in which I kept finding mistakes, I gave up trying to prove this MiniConjecture , though I suspect that it can be proven by making the above intuition/hand-waving rigorous. $$$$ In any case, assuming $f:[0,1] \hookrightarrow \mathbb R^2$ has a discontinuity at $p\in [0,1]$ (e.g. $p=0$ as I normalized above), the paths $F_m$ and $P_m$ that are guaranteed (by path-connectedness of the map $f$ ) make us have a picture that looks like what follows, with the green $F_m$ paths and orange $P_m$ paths. If indeed we have infinitely many points of discontinuity (illustrated in red) converging to $p$ from the right, then each of those points will lead to another version of this tree structure in the codomain (which I have suggested in the small circles around the red points in the codomain). All images of intervals $[p, p+\eta\rangle$ (where "" $\rangle$ "" refers to both "" $)$ "" and "" $]$ "") will map to such a tree. Subtracting 2 such intervals leads to general intervals $\langle p+\delta, p+\eta\rangle$ which get mapped to a tree like above, with some portion (portions on the left in the above picture) cut off. But if the tree is super ""well-connected"" with threads weaving throughout the ""canopy"", I can imagine that the set remains path-connected. So perhaps the Conjecture above is false, if one can construct an injective map from $[0,1]$ to this fractal tree-like structure. $%[![simplified path connected tree thingy][8]][8]$","['path-connected', 'real-analysis', 'continuity', 'general-topology', 'set-theory']"
4755675,"Book suggestion on ""Banach space geometry for machine learning""","Is there any book for a Mathematics student who can learn Machine learning in the aspect of Banach space geometry? Or, one can understand the connection between Geometry of Banach spaces and Machine learning? Any suggestions in terms of article, note, book regarding the development of such study is always welcomed. It will be very much helpful if anyone can suggest a good book which contains the discussion about the topic of Geometry of banach spaces that is required for Machine learning, Neural network such things. Thank you in advance.","['machine-learning', 'banach-spaces', 'operator-theory', 'functional-analysis']"
4755677,Is a stationary point for length functional automatically a local minimum?,"Given a Lagrangian $L:TM \rightarrow \mathbb{R}$ defined on a tangent bundle, Hamilton's principle states that a curve $\gamma:[a,b] \rightarrow M$ is a stationary point of the action functional $S[\gamma]:=\int_a^b L(\gamma(t),\gamma'(t)) dt$ among variations with fixed endpoints iff the Euler-Lagrange equations $\frac{d}{dt}\left(\frac{\partial L}{\partial q'} \right) - \frac{\partial L}{\partial q} = 0$ are satisfied. Now we consider the Lagrangian given by $L(q,q')= \sqrt{g(q',q')}$ , that is the length of a tangent vector. Then the action functional becomes arc length. The Euler-Lagrange equations then become the geodesic equations. My question is the following:
Is a stationary point of the length functional $S_l[\gamma]:=\int_a^b\sqrt{(g(\gamma'(t), \gamma'(t)))} dt$ automatically a local minimum for this length functional $S_l$ among all variations with fixed endpoints? In other words, if $\gamma$ is a stationary point of $S_l$ (equivalently, if it satisfies the Euler-Lagrange equation, that is in this case the geodesic equations) is it then true that for any variation $f:(-\epsilon, \epsilon) \times [a,b] \rightarrow M$ of $\gamma$ with fixed endpoints (that is $f(0,t)=\gamma(t)$ for all $t \in [a,b]$ and $f(s,a)=\gamma(a)$ and $f(s,b) = \gamma(b)$ for all $s \in ( -\epsilon, \epsilon)$ ) we have that for all small enough $s$ $$S_l(f(s, \cdot))= \int_a^b \sqrt{g \left(\frac{\partial f}{\partial t}(s,t)),\frac{\partial f}{\partial t}(s,t) \right)} dt \geq S_l(f(0, \cdot))=S_l[\gamma] \ \ \ ?$$ (This is different from the fact that a geodesic is not necessarily a global minimum of the length functional among all admissible path connecting two given points. And it is different from the property of geodesics to be length-minimizing within a geodesic ball.)","['calculus-of-variations', 'geodesic', 'riemannian-geometry', 'differential-geometry']"
4755684,Solution of Bernoulli's differential equation,"I am trying to solve the following Bernoulli's equation: $\frac{dx}{dt}=a(t)x-b(t)x^2$ . with $x(0)=x_0$ and $a(t)>0$ , $b(t)>0$ . On the way to solution, I make the substitution $u=1/x$ implying $u'=-x^{-2}\frac{dx}{dt}$ to transform it to the following equation in $u$ . $u'+a(t)u-b(t)=0$ , which is a linear first order ODE, can be solved via Integrating factor: $e^{\int a(t)dt}$ . Leading to the following: $\frac{d}{dt}\left[e^{\int a(t)dt)}*u\right]=b(t)e^{\int a(t)dt}$ $\implies u=\frac{1}{e^{\int a(t)dt}}\int b(t)e^{\int a(\tau)d\tau}dt$ Lets call $\int a(t)dt=\alpha(t)$ . $\implies u=\frac{1}{e^\alpha}\int b(t)e^{\alpha}dt$ . Integrate by parts: $\int b(t)e^{\alpha(t)}dt=b\int e^{\alpha(t)}dt-\int b(t)\left[\int e^{\alpha(\tau)}d\tau\right]dt+c$ which means: $u=\frac{1}{e^{\alpha(t)}}\left[b\int e^{\alpha(t)}dt-\int b(t)\left[\int e^{\alpha(\tau)}d\tau\right]dt \right]$ , this leads us to the solution $x(t)$ by substituting back $u=x^{-1}$ and $\alpha=\int a(t) dt$ i.e., $x(t)=\frac{e^{\int a(\tau)d\tau}}{\frac{1}{x_0}+b\int e^{\alpha(t)}dt-\int b(t)\left[\int e^{\alpha(\tau)}d\tau\right]dt}$ .  where $c=1/x_0$ . However, the solution that I needed to come to is the following: $x(t)=\frac{e^{\int a(\tau)d\tau}}{\frac{1}{x_0}+\int b(t)e^{\int a(\tau)d\tau}dt}$ . So I have one extra term in my solution $b(t)\int e^{\int a dt} $ , that emerges from the 'uv' term in integration by parts. I would appreciate if someone can point out where I am doing a mistake?","['calculus', 'ordinary-differential-equations']"
4755698,Integral of Binomials [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 10 months ago . Improve this question Is there a way to integrate a binomial coefficient? I've tried to put the following into Wolfram Alpha, but it doesn't return a result. I have a small suspicion that I'm going a bit over my head here, but does anyone know if this is even possible? $$F(n) = \int_1^n \binom{n}{x} \,dx$$","['integration', 'definite-integrals', 'binomial-distribution', 'gamma-function', 'calculus']"
4755747,Evaluate $\int_{0}^{2\pi}\frac{x^{2}\sin x}{8+\sin^{2}x}dx$,"Prove that $$\int_{a}^{2a}f(x)dx = \int_{0}^{a}f(a+x)dx = \int_{0}^{a}f(2a-x)dx.$$ Evaluate $$\int_{0}^{2\pi}\frac{x^{2}\sin x}{8+\sin^{2}x}dx.$$ I broke down the integral into $$\int_{0}^{2\pi}\frac{x^{2}\sin x}{8+\sin^{2}x}dx=\int_{0}^{\pi}\frac{x^{2}\sin x}{8+\sin^{2}x}dx+\int_{0}^{\pi}\frac{(\pi+x)^{2}\sin(\pi+x)}{8+\sin^{2}(\pi+x)}dx$$ or $$\int_{0}^{2\pi}\frac{x^{2}\sin x}{8+\sin^{2}x}dx=\int_{0}^{\pi}\frac{x^{2}\sin x}{8+\sin^{2}x}dx+\int_{0}^{\pi}\frac{(2\pi-x)^{2}\sin(2\pi-x)}{8+\sin^{2}(2\pi-x)}dx$$ But the answer is still zero. The actual answer for this is $$\frac{-2\pi^2}{3}\ln 2$$ Using Desmos Calculator, I come up with this. Can anyone here to tell me how to do with this improper integral?","['integration', 'calculus']"
4755759,Eulerian polynomials evaluated in $-1$,Let $$A_n(x):=\sum_{\sigma \in S_n} x^{1+\text{des}(\sigma)}$$ where $\text{des}(\sigma)$ is the number of descents of $\sigma$ . These are called the Eulerian polynomials. I need to express $A_n(-1)$ in terms of Euler numbers. This is an exercise in Stanley's combinatorics vol 1 (exercise 135 chapter 1). In the solution the author suggests using the known fact that $$\sum_{d\geq 0} A_d(x) \frac{t^d}{d!}=\frac{1-x}{1-xe^{(1-x)t}}$$ and evaluate this equality in $x=-1$ . Then the author suggests to compare the obtained expression with the Taylor-Maclaurin expansion of the tangent function. My attempt Evaluating in $x=-1$ $$\sum_{d\geq 0} A_d(-1) \frac{t^d}{d!}=\frac{2}{1+e^{2t}}$$ So $$\sum_{d\geq 0} A_d(-1)\frac{t^d}{d!}+\left(\sum_{d\geq 0} A_d(-1)\frac{t^d}{d!}\right)\left(\sum_{j\geq 0} 2^j \frac{t^j}{j!} \right)=2$$ Equating the coefficients on both sides I get messy equations and I really don't know how to use the Taylor series of the tangent.,"['permutations', 'combinatorics', 'discrete-mathematics', 'generating-functions']"
4755794,Almost sure convergence of means of arbitrary sample sets?,"Suppose $\def\N{\mathbb{N}}(X_n)_{n\in\N}$ are i.i.d. integrable random variables with mean $\mu$ . Let us write, for finite $I \subset \N$ $$\overline{X}_I = \frac1{|I|}\sum_{i\in I} X_i$$ (and $\overline{X}_\emptyset = 0$ .) Suppose $(I_n)_{n\in\N}$ are finite subsets of $\N$ with $\lim_n|I_n| = +\infty$ . Does it hold that $$\overline{X}_{I_n}\xrightarrow[n\to\infty]{~\text{a.s.}~}\mu ~ ?$$ The strong law of large numbers is the case $I_n = \{1,\dots,n\}$ . I am mostly interested in the case where the $I_n$ are pairwise disjoint.","['convergence-divergence', 'law-of-large-numbers', 'probability-theory', 'reference-request']"
4755815,Estimating $\int_0^1 a^x\ \frac{x(1-x)}{\sin(\pi x)} dx$,"I doubt a closed form exists, so I am trying to approximate the integral: $$I(a)=\int_0^1 a^x\ \frac{x(1-x)}{\sin(\pi x)}  dx$$ I am therefore looking for a function $F(a)$ that provides $$F(a)\simeq I(a)$$ But I want this to be true for every value of $a>0$ , and so something like a truncated Taylor series of $I$ would be useless, since for large $a$ it would require adding more and more terms. Up to this day I found only two promising approximations of $I(a)$ : $\Large{1)}$ Using the inequality (which is a quite good approximation for small values of $x,y$ ) $$\frac{x-y}{\log x-\log y}<\left(\frac{x^{\frac13}+y^{\frac23}}{2} \right)^3 $$ I got $$F_1(a)=\frac{1+a}{8\pi}+\frac{a^{\frac13}+a^{\frac23}}{6\sqrt3}$$ with a percentage error of roughly $1\%$ for small $a$ , growing larger as $a$ grows. $\Large{2)}$ Using the famous ancient approximation for $\sin x$ : $$\sin x \simeq \frac{16(\pi-x)x}{5\pi^2-4(\pi-x)x}$$ I got $$F_2(a)=\frac{a-1}{2\log^3a}-\frac{a+1}{4\log^2a}+\frac{5(a-1)}{16\log a}$$ and this one is tremendously precise, even for very large values of $a$ . However, I am still on the lookout for other, even better, approximations for $I$ . These two are the only ones I was able to get. Regarding the first one, I tried even sharper bounds known for the logarithmic mean, but then I wasn't able to integrate. The generalized mean of exponent $\frac13$ was the best I could get. I like the result because it features only polynomials, no exponentials or logarithms and trig functions. But, since the two means start to differ very fast past around $10$ , the result is good only for small values of $a$ . If someone has an idea to get another approximation I'd be happy to hear it.","['integration', 'improper-integrals', 'definite-integrals', 'real-analysis', 'calculus']"
4755822,"Finding a closed form for the series $\sum_{n=0}^{\infty}\frac{\binom{2n}{n}^3}{64^n(n+1)^k}$ for $k=1,2,3,4$","Context: This question is related to Calculate $\sum_{n = 0}^\infty \frac{C_n^2}{16^n}$ and Is there a closed form for a give infinite sum? .
We have also: $$\sum_{n=0}^{\infty}\frac{\binom{2n}{n}^3}{64^n(n+1)^2}=\frac{\Gamma(\frac{1}{4})^4}{2\pi^3}-\frac{96\pi}{\Gamma(\frac{1}{4})^4},\tag{1}$$ $$\sum_{n=0}^{\infty}\frac{\binom{2n}{n}^3}{64^n(n+1)^3}=8-\frac{384\pi}{\Gamma{(\frac{1}{4})}^4}\tag{2}.$$ Both can be obtained in an elementary way (avoiding hypergeometric functions).
I have derived: $$I=-\frac{32}{\pi^2}\int_{0}^{\pi/2}\int_{0}^{\pi/2}\sqrt{1-\sin^2{(k)}\sin^2{(x)}}dxdk\\=4\sum_{n=0}^{\infty}\frac{\binom{2n}{n}^3}{64^n(n+1)}-\frac{2\Gamma(\frac{1}{4})^4}{\pi^3}.\tag{3}$$ I tried several different ways to compute: $$S=\sum_{n=0}^{\infty}\frac{\binom{2n}{n}^3}{64^n(n+1)}\tag{4}.$$ It seems that is easier than $(1)$ and $(2)$ , but it still eludes me. Updated 1: Thanks to Mariusz Iwaniuk and KStarGamer we have (see https://functions.wolfram.com/HypergeometricFunctions/Hypergeometric3F2/03/08/05/01/01/06/0002/ ): $$I=\int_{0}^{\pi/2}E(\sin({k}))dk=\frac{\Gamma(\frac{3}{4})^4}{2\pi}+\frac{\pi^3}{8\Gamma(\frac{3}{4})^4} ,$$ where $E(k)$ is the complete elliptic integral of the second kind. This integral implies: $${_3F_2(-1/2,1/2,1/2;1,1;1)}=\frac{\pi}{2\Gamma(3/4)^4}+\frac{2\Gamma(3/4)^4}{\pi^3},$$ and Wolfram and Mathematica are unable to deal with it. Updated 2: The natural question is to ask about the closed form of: $$S'=\sum_{n=0}^{\infty}\frac{\binom{2n}{n}^3}{64^n(n+1)^ 4}={_5F_4(1/2,1/2,1/2,1,1;2,2,2,2;1)}\tag{5},$$ If we find the closed form of $S'$ also we will get the closed form for: $$I'=\int_{0}^{\pi/2}k\cot(k)K(\sin(k))dk\\=\frac{3\pi^2\log{2}}{4}+\frac{5\Gamma(3/4)^4}{\pi}+\frac{\pi^3}{4\Gamma(3/4)^4}-\pi G+\frac{\pi^2S'}{64}-\frac{3\pi^2}{4} \tag{6},$$ where $K(k)$ is the complete elliptic integral of the first kind and G is Catalan's constant. Updated 3: It can be done using $(6)$ also: $$
I'=\frac{3\pi^2\log{2}}{4}-\frac{\pi^2 S_{2}}{64}-\pi G \tag{7},
$$ where: $$S_{2}={_5F_4(1,1,3/2,3/2,3/2;2,2,2,2;1)}.$$ As @SetnessRamesory has suggested probably $S'$ and $S_{2}$ don't have easy closed forms. But at least we know from $(6)$ and $(7)$ this nice relation: $${_5F_4(1/2,1/2,1/2,1,1;2,2,2,2;1)}+{_5F_4(1,1,3/2,3/2,3/2;2,2,2,2;1)}=-\frac{320\Gamma{(3/4)}^4}{\pi^3}-\frac{16\pi}{\Gamma{(3/4)}^4}+48.\tag{8}$$ This question is going to be updated for a while. Thanks for your feedback!","['catalan-numbers', 'closed-form', 'sequences-and-series', 'elliptic-integrals', 'hypergeometric-function']"
4755835,Image of the standardization of permutations of a finite multiset,"Let $M$ be a multiset $\{1^{m_1},2^{m_2},...\}$ whose cardinality $\#M:=m_1+m_2+...=:n$ . Let $\Sigma:S_M\to S_n$ be the standardization map defined in Stanley combinatorics volume 1 ( $S_M$ is the set of permutations of $M$ ). The author states in the solution of exercise 56 that $$\text{Im}(\Sigma)=\{\sigma\in S_n:D(\sigma^{-1})=\{m_1,m_1+m_2,...\}\cap [n-1]\}.$$ But this just seems wrong. Let $M=\{1^2,2^1,3^2\}$ so $\#M=5$ . Then let $w\in S_M$ be $$w=12313$$ Applying standardization we get $$\Sigma(w)=13425$$ The inverse permutation of $\Sigma(w)$ is $14235$ and its only descent is $\{2\}$ . I think that the condition should be relaxed to $D(\sigma^{-1})\subseteq\{m_1,m_1+m_2,...\}\cap [n-1]$","['permutations', 'multisets', 'combinatorics', 'discrete-mathematics']"
4755869,Problem 6.1 in Loring Tu: Why is the identity function on a particular manifold not smooth?,"I'm currently reading from Loring Tu's Introduction to Manifolds , and I've come across the following problem: My confusion lies primarily with the last comment mentioned in Tu's hint: the identity map $\mathbb{R'} \to \mathbb{R}$ is not smooth (after looking in Tu's list of errata for the book, he meant to say $\mathbb{R'} \to \mathbb{R}$ instead of $\mathbb{R} \to \mathbb{R}$ ). This perplexes me a little bit for the following reasons: Let $i: \mathbb{R'} \to \mathbb{R}$ be the map given by $i(x) = x$ . Tu gives a very clear set of criteria to determine when such a map (an $\mathbb{R}$ -valued function defined on a manifold) is smooth: So, at least in my mind, we have the atlas $ \{ (\mathbb{R}, \psi) \} $ on $\mathbb{R'}$ where $\psi(x) = x^{1/3}$ . It's not necessarily the smooth structure on $\mathbb{R'}$ , but it's an atlas contained in the smooth structure. It is clear that the inverse of $\psi$ is the map $\psi^{-1}: \mathbb{R} \to \mathbb{R'}$ given by $\psi^{-1}(x) = x^3$ . By proposition $6.3(\text{ii})$ , $i(x) = x$ will be $C^{\infty}$ if the map $i \circ \psi^{-1}: \mathbb{R} \to \mathbb{R}$ is $C^\infty$ . But $i \circ \psi^{-1}(x)$ is precisely $i \circ \psi^{-1}(x) = x^3$ which is clearly smooth. What am I missing here? I've been trying to find which assumptions I'm making are incorrect, but I can't see them. Any help with this would be greatly appreciated! (Note, I'm not asking for help solving problem 6.1 as I've already found another diffeomorphism between $\mathbb{R}$ and $\mathbb{R'}$ . I would just like clarity on identity's lack of smoothness).","['differential-topology', 'derivatives', 'smooth-manifolds']"
4755919,Find area of triangle given one angle and the lengths its altitude divides the opposite side into,"In triangle $\triangle ADC$ , $DB$ is perpendicular to $AC$ at $B$ so that $AB=2$ and $BC=3$ as shown in the figure. Furthermore, $\angle ADC=45^\circ$ . Find the area of $\Delta ADC$ . Tried the following things: Drawing medians and altitudes of the right triangles formed. Apollonius's theorem . Stewart's theorem . Sine rule . Cosine rule . Noticing that $3+2$ equals to $5$ , I extended segment $BD$ to form a 3-4-5 triangle and try to proceed. Spent around 1.5 hours on it. In each of the cases, I wasn't able to proceed after a certain point.","['triangles', 'area', 'geometry']"
4755926,Prove that $e^\frac{\pi}{2}=1+\frac{1}{1}+\frac{1}{2}+\frac{1}{3}+\frac{5}{24}+\frac{1}{6}+\frac{85}{720}+\frac{520}{5040}+\frac{3145}{40320}+\cdots$,"I misused the Taylor series to represent $e^x$ as a combination of sin(x) functions Assume: $e^x=a_0+a_1\sin(x)+a_2\sin^2(x)+a_3\sin^3(x)+\cdots$ The sequence coefficients are: $a_0=1;a_1=1;a_2=\frac{1}{2};a_3=\frac{2}{6};a_4=\frac{5}{24};a_5=\frac{20}{120};a_6=\frac{85}{720};a_7=\frac{520}{5040};a_8=\frac{3145}{40320}\cdots$ Therefore we have, $e^\tfrac{\pi}{2}=1+\frac{1}{1}+\frac{1}{2}+\frac{2}{6}+\frac{5}{24}+\frac{20}{120}+\frac{85}{720}+\frac{520}{5040}+\frac{3145}{40320}+\cdots$ Additionally we have, $e^\tfrac{\pi}{6}=1+\frac{1}{2}+\frac{1}{2\times2^2}+\frac{2}{6\times2^3}+\frac{5}{24\times2^4}+\frac{20}{120\times2^5}+\frac{85}{720\times2^6}+\frac{518}{5040\times2^7}+\frac{3145}{40320\times2^8}+\cdots$ The summation values do seem to be nearly equal to the LHS after a few terms. However I am skeptical and would like to know if this misrepresentation of the Taylor series is valid. I would appreciate any other proofs for the derived value of $e^\tfrac{\pi}{2}$ or $e^\tfrac{\pi}{6} $ to corroborate the summation.","['summation', 'taylor-expansion', 'sequences-and-series']"
4755933,"Problem doing a line integral $\int_C P(x,y)dx+Q(x,y)dy$","Evaluate $$\int_C P(x,y)dx+Q(x,y)dy$$ where $P(x,y) =  y^2 $ , $Q(x,y) = x$ , and $C$ is the part of the graph $x = y^3$ from $(-1,-1)$ to $(1,1)$ . I was trying the parametrization: $$x = t $$ $$y = \sqrt[3] t$$ $$dx = 1$$ $$dy = \frac{1}{3}t^{-2/3}$$ from which: $$\int y^2dx+xdy=\int_{-1}^1 \left(t^{2/3}+ \frac{1}{3}t^{1/3}\right) dt= \frac{17}{10} $$ which is not the correct answer, correct answer is $\dfrac{6}{5}$ .","['multivariable-calculus', 'calculus', 'line-integrals', 'parametrization']"
4755951,Density with respect to direct sum of measures,"For each $n \ge 1$ , let $\Gamma_{n} \subset \mathbb{R}^{n}$ be a (non-empty) measurable and bounded set and equip $\Gamma_{n}$ with its Borel $\sigma$ -algebra $\mathbb{B}_{n}$ . Define $\Omega_{n} := \Gamma_{n} \times \{n\}$ and $\Omega := \bigcup_{n=1}^{\infty}\Omega_{n}$ . For each $E \subset \Omega$ , set: $$E_{n}:= \{ x \in \bigcup_{n=1}^{\infty}\Gamma_{n} : (x,n) \in E\}$$ We equip $\Omega$ with the $\sigma$ -algebra: $$\mathcal{F} = \{E \subset \Omega: \mbox{$E_{n} \in \mathbb{B}_{n}$ for all $n$}\}$$ If we equip each $(\Omega_{n},\mathbb{B}_{n})$ with a measure, say, $\mu_{n}$ , we can define a new measure $\mu$ on $(\Omega, \mathcal{F})$ by setting: $$\mu = \sum_{n=1}^{\infty}\mu_{n}$$ Suppose each $\mu_{n}$ has density $\rho_{n}$ with respect to the standard Lebesgue measure $\lambda_{n}$ on $(\Gamma_{n},\mathbb{B}_{n})$ , that is, $d\mu_{n} = \rho_{n}d\lambda_{n}$ . Of course, one can define a new function $\rho$ on $\Omega$ by setting $\rho(x,n) := \rho_{n}(x)$ . We can also let $\lambda$ be the associated measure: $$\lambda = \sum_{n=1}^{\infty}\lambda_{n}$$ Question: Is it true that $\mu$ has density $\rho$ with respect to $\lambda$ ? In other words, does $$\int_{\Omega}f(x)d\mu(x) = \int_{\Omega}f(x)\rho(x)d\lambda(x) = \sum_{n=1}^{\infty}\int_{\Omega_{n}}f(x,n)\rho_{n}(x)d\lambda_{n}(x)$$ hold?","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis', 'probability-theory']"
4755964,Derivative comparison: direct formula vs Jacobian,"Let's use the following defininition of the derivative (Hubbard, 5th ed.) Let $U \subset \mathbb R^n$ be open and let $f:U \to R^m$ ; let $a \in U$ , $ h \in \mathbb R^n$ If there exists a linear transformation $[Df(a)]:\mathbb R^n \to R^m$ such that $$\lim_{h \to [0]} {( f(a+h) - f(a) - [Df(a)]h ) \over {|h|}} =[0]$$ then $f$ is differentiable at $a$ and $[Df(a)]$ is unique derivative of $f$ at $a$ . Now, let's solve the following task: let $ A = \begin{bmatrix} a & b \\\\
c & d
\end{bmatrix}
$ .
Compute the derivative for the function $f(A) = A^{-1}$ Remark: I'll be using 2x2 matrix -> 4x1 vector transformation by traversing top left, top right, bottom left, bottom right. If we use direct definition of the derivative, and come up with the $[Df(a)]H = -A^{-1}HA^{-1}$ , where $H = \begin{bmatrix} h_1 & h_2 \\\\
h_3 & h_4
\end{bmatrix} \to [0]$ But there derivative times H is wired with H: $-A^{-1}HA^{-1}$ .
Moreover, this derivative times H is a $2 \times 2$ matrix, so it has 4 entries in total: $$ {1 \over (ad-bc)^2}
\begin{bmatrix} -bch_4 +bdh_3 +cdh_2 -d^2 h_1 & abh_4-adh_2-b^2 h_3+bdh_1 \\\\
ach_4-adh_3-c^2 h_2+cdh_1 & -a^2 h_4+abh_3+ach_2-bch_1
\end{bmatrix}
$$ Now, another approach giving pure derivative by computing Jacobian. If we interpret $A$ as vector of 4 entries, then $A^{-1}$ is also vector of 4 entries. So $$[Df(A)] = -{1 \over (ad-bc)^2} \begin{bmatrix} d^2 & -cd & -db & bc \\\\
-bd & ad & b^2 & -ab \\\\
-dc & c^2 & ad & -ac \\\\
bc & -ac & -ab & a^2
\end{bmatrix}$$ What I see is: if we factor out ""-"" from the first approach matrix, we'll get top left element of first approach matrix coeffs equal to first row, top right - to second row, bottom left - to 3rd, and bottom right - to 4th. So we factor out ""-"", then each item of resulting first-approach matrix is dot product between some vector and H: $$
-{1 \over (ad-bc)^2}
\begin{bmatrix} d^2 \\\\ -cd \\\\ -bd \\\\ bc  \\\\ 
                -bd \\\\ ad  \\\\ b^2 \\\\ -ab \\\\ 
                -cd \\\\ c^2 \\\\ ad  \\\\ -ac \\\\
                bc  \\\\ -ac \\\\ -ab \\\\ a^2
\end{bmatrix} \bullet \begin{bmatrix} h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4  \\\\ 
                h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4  \\\\ 
                h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4  \\\\ 
                h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4
 \end{bmatrix}
$$ Now we rearrange first vector into 4x4 matrix and we're done. Now, my questions are: Even though I matched direct and Jacobian approaches, I just guessed. As I calculated Jacobian as well and I knew what I had to match and how to get from direct to Jacobian. But what If I didn't have Jacobian? Would my way of factoring out H work?(I think it will, as D is linear, but just to make sure) while meaning of $[Df(a)]$ is clear to me, what is meaning or intuitive interpretation of $[Df(a)]H$ (to me, this meaning now is given any arbitrary H it will give good enough approximation to $f(A+H) - f(A)$ . But as $H \to [0]$ , lim of $[Df(a)]H = [0]$ which makes no sense to me...",['multivariable-calculus']
4755967,Condition about functional equation has a solution,"Problem : Let $P(x, y)$ be a polynomial respect to $x, y$ . Find a condition about $P$ where functional equation of $f$ $$f(x+y)=f(x)+f(y)+P(x,y)$$ has a solution. (Where $f\colon \mathbb{R}\to\mathbb{R}$ be a differentiable function.) This problem came from some functional equation problems below : P1 : Find a differentiable $f$ which satisfies $$f(x+y)=f(x)+f(y)+2xy$$ S1 : Set $x=y=0$ then we get $f(0)=0$ . From definition of derivative, $$f'(x)=\lim_{y\to 0}\frac{f(x+y)-f(x)}{y} = \lim_{y\to 0} \frac{f(y)+2xy}{y} = 2x+f'(0)$$ This implies $$f(x)=x^2 + cx$$ for some constant $c$ . Plugging this to original functional equation, $$\text{LHS} = x^2 + 2xy + y^2 + cx + cy$$ $$\text{RHS} = x^2 + y^2 + cx + cy + 2xy$$ which are same, so we find a solution of original functional equation. P2 : Find a differentiable $f$ which satisfies $$f(x+y)=f(x)+f(y)+3x^2y$$ S2 : Similar way, $f(0)=0$ and from definition of derivative, $$f'(x)=f'(0)+3x^2$$ which implies $$f(x)=x^3 + cx$$ for some constant c. However If we plug this to original functional equation $$\text{LHS} = x^3 + 3x^2y + 3xy^2 + y^3 + cx + cy$$ $$\text{RHS} = x^3 + 3x^2y + y^3 +cx + cy$$ which are not same. So, I want to know a condition of $P(x, y)$ which makes a functional equation of $f$ (which likes below form) has a solution : $$f(x+y)=f(x)+f(y)+P(x,y)$$ Where $f$ is a differentiable function. I find that WLOG $P(0, 0) = 0$ Because, if $P(0, 0) = a \neq 0$ , add $a$ both side then we can write as $$(f(x+y)+a) = (f(x)+a) + (f(y)+a) + Q(x, y)$$ Where $Q(x, y)$ is a polynomial respect to $x, y$ which satisfies $Q(0, 0) = 0$ . Substitution $g(x) = f(x)+a$ changes this equation to : $$g(x+y)=g(x)+g(y)+Q(x, y)$$ So I can assume that WLOG $P(0,0)=0$ . Added Conjecture : $P(x,y)$ must be constant or middle term(Cross -terms) of binomial expansion, i.e. $$ (x+y)^2 = x^2 + y^2 + \mathbf{2xy}$$ or $$(x+y)^3 = x^3 + y^3 + \mathbf{3x^2y + 3xy^2}$$ I don't have a proof but I made some try and common thing of $P(x,y)$ is this. Thanks for your help!","['functional-equations', 'calculus', 'polynomials', 'real-analysis']"
4755987,"Impossible? Finding a closed loop on a six-sided die that crosses face $n$ exactly $n$ times, without returning to a face from one step before","I'm new here, so please tell me if I'm doing anything wrong. Here is a puzzle that I came up with, which I believe falls someone in the realm of graph theory or topology (I could be very wrong, please correct me!): Given a standard six-sided die (with opposite faces adding to seven), is it possible to draw a single non-crossing closed loop on those faces such that the loop crosses through face $n$ exactly $n$ times? The answer to this question is yes: I found a solution with code and a friend. However, if I add one more restriction ... The loop can't return to the face it was at one face before (eg. the loop can't go through face 1 and then face 5 and then immediately back to face 1), ... it becomes very difficult to find a solution. So difficult, in fact, I started wondering if it is possible at all. However, I can't prove one way or the other whether there is a solution or not. So: Is this version of my problem impossible? Or is there a solution? Also, if you know, what is this type of problem called? It certainly seems related to graph theory and topology, but I don't know if there are any keywords that describe this problem. Thanks! Added by @Blue. Here's a different view of the sample path. (Circled nodes indicate turn-arounds that the revised rules ask to avoid.) Here's the path from @DanielMathias' comment to @JimFerry's answer , having just one turn-around:","['graph-theory', 'general-topology', 'puzzle', 'dice']"
4756042,Interpretation of Ricci and Scalar curvature.,"Let $(M,g)$ be a SR-manifold and for $p \in M$ , let $\{e_1,\ldots,e_n\}$ be a pseudo-orthonormal basis for $T_pM$ . Now, in my notes, it says that, if $v \in T_pM$ is such that $g(v,v) \neq 0$ and $$\{e_2,\ldots,e_n\}$$ is an orthonormal basis of $v^{\perp}$ then $$\operatorname{Ric}(v,v) = g(v,v) \sum_{i = 2}^{n} K(v,e_i)$$ where $K(-,-)$ is the sectional curvature of non-degenerate two-planes at $p \in M$ . Then it says that from this, we can interpret $\operatorname{Ric}(v,v)$ as the ”mean” of all sectional curvatures of two-planes containing $v$ . Now, for the scalar curvature $\operatorname{scal} = \operatorname{tr}_g(\operatorname{Ric}) \in C^{\infty}(M)$ we have $$\operatorname{scal}(p) = \sum_{i,j = 1}^{n} \epsilon_i \epsilon_j R(e_j,e_i,e_i,e_j) = \sum_{\substack{i,j = 1, \\ i \neq j}}^{n} \epsilon_i \epsilon_j K(e_i,e_j)(\underbrace{g(e_i,e_i)}_{=\epsilon_i}\underbrace{g(e_j,e_j)}_{=\epsilon_j}-\underbrace{g(e_i,e_j)}_{= 0}) = \sum_{i \neq j} K(e_i,e_j)$$ so that $\operatorname{scal}(p)$ is the ”mean” over all $i \neq j$ sectional curvatures of non-degenerate two-planes. My question is this: In what sense are these two descriptions ”means”? My mind is naturally drawn to the arithmetic mean, but there are of course other means, and I am not sure there is a mathematical definition of mean independent of the specific mean one is referring to, but since we don’t divide by something, it can’t be the arithmetic mean, I presume. Any thoughts on in which sense these are means? And for the scalar curvature, I presume they must mean all non-degenerate two-planes spanned by the basis vectors? Because could there not potentially be other two-planes?","['riemannian-geometry', 'differential-geometry']"
4756044,"Finding the cardinality of $\{(a, S) \mid a ∈ S, S ∈ P(A)\}$ for $A =\{1, 2, 3, \cdots , n\}$.","This is exercise 1.6a in Introductory Mathematics: Algebra And Analysis by G.C. Smith. Let $A =\{1, 2, 3, \cdots , n\}$ . What is the cardinality of $\{(a, S) \mid a ∈ S, S ∈ P(A)\}$ ? The answer given is $n \cdot 2^{n - 1}$ since there are $n$ choices for $a$ , and $2^{n-1}$ choices for $S$ , since we can
choose the set $S \setminus \{a\}$ in $2^{n - 1}$ ways. I understand that the cardinality of $P(A)$ is $2^n$ and that there are that many possible $S$ sets, $2^n - 1$ if we exclude the empty set. The number of elements in each $S$ set ranges from $1$ to $n$ , every set yields $k$ pairs which is the number of elements it contains. The number of sets containing $k$ elements is $\binom n k$ . So I think the answer should be $$
1 \cdot \binom n 1
+ 2 \cdot \binom n 2 
+ \cdots
+ n \cdot \binom n n
$$ which I think is correct. I just don't see how that is equal to $n \cdot 2^{n - 1}$ . If you could explain the reasoning behind the author's answer it would be appreciated as well.","['binomial-coefficients', 'combinatorics']"
4756045,"Suppose $f(t) = f(-t)$ and $f \in C^\infty$, show that there's a $h(t) \in C^\infty$ with $h(t^2) = f(t)$","So the hint of the exercise is that I should use Taylor's theorem (I'm not assuming $f$ is analytic). Then it's clear that all odd derivatives of $f$ at $0$ is $0$ so in some sense $h$ can be defined by the Taylor coefficients $h_k = f_{2k}$ . More generally, can I show that if $f \in C^{2k}$ and is even, then there exists $h \in C^k$ with $h(t^2) = f(t)$ ?","['calculus', 'real-analysis']"
4756065,Cosmological evolution of a hyperbolic space form,"Please consider the following metric $$ ds^2 =-dt^2+f(t)( {\frac {a^2}{r^2+a^2}}dr^2+ r^2 d\Omega_{n-1})$$ where $d\Omega_{n-1}$ is the metric of the hyper-sphere $S^{n-1}$ Making ""experimental symbolic computation"" with the package GrTensor, I am obtaining that $$f(t) = {\frac {1}{8}}{\frac {\left( 2\,{{\rm e}^{-2\,\sqrt {2}\sqrt {{\frac {\Lambda}{n \left( n-1 \right) }}}t}}-\frac{1}{2}\,n \left( n-1 \right)  \right) ^{2}{{\rm e}^{2\,\sqrt {2}\sqrt {{\frac {\Lambda}{n \left( n-1 \right) }}}
t}}
}{{\Lambda}^{\frac{3}{2}}{a}^{3}}}
  $$ is a solution for the Einstein equation with cosmological constant: $$ G_{{\mu,\nu}}+\Lambda\,g_{{\mu,\nu}}=0 $$ My question is: how to derive this solution using only pen and paper?","['general-relativity', 'differential-geometry']"
4756102,What do they mean by radius of convergence?,"I am looking at a couple different page on the definition of Radius of Convergence, specifically for Taylor series. I first learned it as follows: For a power series $$\sum_{k=0}^\infty a_k (z-z_0)^k$$ the radius of convergence is a unique real number $R\in\mathbb R \cup \{0,\infty\}$ where the sum converges when $|z-z_0|<R$ and diverges when $|z-z_0|>R$ . However, I was told a different definition/convention specific to Taylor series: For a function $f:U\to\mathbb C$ and any $z_0\in U$ we say the radius of convergence for the Taylor series centered at $z_0$ is the largest $R$ for which the Taylor series converges to $f$ on $D(z_0;R)$ . So it not only needs to converge, it has to converge to $f$ . This two definitions are clearly different, for example, consider the function $g:\mathbb C\setminus\{1\}$ where $g(z)=0$ . The Taylor series of $g$ centered at $z_0=0$ is $0$ . Using the first definition we know that the radius of convergence of this Taylor series is $\infty$ . Using the second definition we see that the radius of convergence is actually $1$ , since it does not converge to $g$ at $z=1$ . This is confusing already but upon a bit of searching, it seems like both of these definition contradicts with this fact in the wikipedia article: https://en.wikipedia.org/wiki/Analyticity_of_holomorphic_functions which says: ""the radius of convergence is always the distance from the center $a$ to the nearest non-removable singularity;"" Using the same $g$ as above, this fact produces contradictory result with the second definition. Consider $h:\mathbb C\setminus \mathbb R^-\to \mathbb C$ where $h(z)=0$ , $h$ has no removable singularity since none of them are isolated. Now the Taylor series of $h$ at $z_0=1$ is $0$ and has radius of convergence $\infty$ . However, the distance between $z_0$ and the nearest non-removable singularity is $1$ , since $\mathbb R^-$ is a set of non-removable singularities. So $h$ shows that the fact is contradictory with the first definition. Given those, I have the following questions: Is anything I have stated incorrect? Is it a convention to define radius of convergence differently for Taylor series? (instead of the series converging, it has to converge to the function where the series is defined on) Is the fact listed in wikipedia correct? Is it yet another convention for ""radius of convergence""?",['complex-analysis']
4756109,A first-order linear differential equation,"I have a first-order linear ODE as follows: $\frac{dx}{dt}=-\alpha(x-x_0)-\beta(t)$ , and I am trying at a solution of this via integrating the factor method. Here's a try: $\frac{dx}{dt}=-\alpha x+\underbrace{\alpha x_0-\beta(t)}_{f(t)}$ Is it OK to assume the braced term as $f(t)$ if $x_0$ is constant? Thus: $\frac{dx}{dt}=-\alpha x+f(t)$ Next, the integrating factor is: $e^{\int\alpha dt}=e^{\alpha t}$ (only if $\alpha$ is a constant?) Also, can we take a constant as an I.F. , like $\alpha$ in this case? This can then lead to $\frac{d}{dt}\left[x e^{\alpha t} \right]=(\alpha x_0-\beta(t))e^{\alpha t}$ leading to $x(t)=e^{-\alpha t}\left[\int\limits_0^t (\alpha x_0-\beta(t'))e^{\alpha t'}dt' \right]$ Finally, can I write the above as follows? $x(t)=\int\limits_0^t e^{-\alpha(t-t')}\left[\alpha x_0-\beta(t') \right]dt'$ because the integration measure is over $t'$ that runs from $0$ to $t$ ?",['ordinary-differential-equations']
4756123,Number of ways to reach a certain point on an hexagonal grid by taking halving steps.,"Imagine you begin at the center of a hexagon, with center-to-vertices distance ( $=$ radius) $1$ , and can step in any of the directions of the vertices of the hexagon. Every time you take a step, the distance you step is halved, beginning with $\frac{1}{2}$ . The image depicts all the points you can reach in $1$ or $2$ steps. If $A$ through $F$ are the vectors from the origin to the vertices of the hexagon, and $S_i$ is the direction of the step you take on step $i$ ( $S_i$ will always be $A$ , $B$ , $C$ , $D$ , $E$ , or $F$ ) your position after $n$ steps will be given by: $$
\sum_{i=1}^{n}\frac{S_i}{2^i}
$$ What I want to know is, given a step count $n$ , and a point, in either rectangular or hexagonal coordinates, how many ways are there to reach it. For example, this image shows all points that can be reached in $1$ or $2$ steps : Points $G$ through $L$ can be reached in $1$ step, whereas the rest can be reached in $2$ . You will note that, for example, point $P_1$ will be accessible via jumping to $I$ (in the direction $C$ ), then jumping to $P_1$ (in the direction $E$ ), or by jumping to $J$ (in the direction $D$ ), and then to $P_1$ (in the direction $B$ ). In this manner, the point can be reached in $2$ ways. The point $K_1$ , by contrast, can only be reached in $1$ way, by jumping first to $J$ , then to $K_1$ , both times in the direction of $B$ . So the desired algorithm would output $2$ when given $P_1$ as input, and output $1$ when given $K_1$ as input, if it was also given $n=2$ . For reference: $A=\left(1,0\right), B=\left(\frac{1}{2}, \frac{\sqrt{3}}{2}\right), C=\left(-\frac{1}{2}, \frac{\sqrt{3}}{2}\right), D=(-1, 0), E=\left(-\frac{1}{2}, -\frac{\sqrt{3}}{2}\right), F=\left(\frac{1}{2}, -\frac{\sqrt{3}}{2}\right),$","['coordinate-systems', 'vectors', 'combinatorics', 'geometry']"
4756126,"Prove that $\,\int_0^1 \sqrt{1+x^2}\,\mathrm dx\le \int_0^1 \sqrt{1+x}\,\mathrm dx\,,\,$ proof verification.","Prove that $\displaystyle\int_0^1 \sqrt{1+x^2}\mathrm dx\le \int_0^1 \sqrt{1+x}\,\mathrm dx$ So these are my first days of integral calulus. I think that's well done and is all clear, but would like to know if there's a better way to work it, or even if I've omitted something, would like to know what you will add or improve, thanks! My proof: Let $f(x)=\sqrt{1+x^2}$ and $g(x)=\sqrt{1+x}$ continuous functions on the interval $[0,1]$ , then, for both functions, the condition $x\in [0,1] \to 0\le x\le 1$ must be satisfied. So, we start by building the functions based on this condition: $$x\in [0,1] \to 0\le x\le 1$$ $$0\le x^2\le x$$ $$1\le 1+x^2\le 1+x$$ $$1\le \sqrt{1+x^2}\le \sqrt{1+x}$$ So, that implies that: $$f(x)\le g(x), \forall x\in[0,1]$$ (For this last step, I'm just applying a known proposition: If $f(x)\ge g(x), $ then $\int_a^b f(x)\,\mathrm dx\ge \int_a^b g(x)\,\mathrm dx$ ) $$\int_0^1 \sqrt{1+x^2}\,\mathrm dx\le \int_0^1 \sqrt{1+x}\,\mathrm dx$$","['calculus', 'solution-verification', 'definite-integrals', 'proof-writing']"
4756172,Any known discontinuous functions which show similar behaviour,"I come from a physics background, and as a result of one of my simulations, I get the following function: On the x axis, the sampled points are of the form $m/n$ with $n \in [1,20]$ and $m \in {1,2,...,n-1}$ . Basically just all the rational numbers less than $1$ with the maximum possible denominator equal to 20. I feel that I have seen a similar curve somewhere, but I can't remember. Is there any known discontinuous function that looks like this? A table of values the function takes: $$\begin{array}{c|c|c|} 
m & n & f(m,n) \\ \hline
1 & 2 & 1\\ \hline
1 & 3 & 2\\ \hline
2 & 3 & 1\\ \hline
1 & 4 & 3\\ \hline
2 & 4 & 1\\ \hline
3 & 4 & 1\\ \hline
1 & 5 & 4\\ \hline
2 & 5 & 2\\ \hline
3 & 5 & 3\\ \hline
4 & 5 & 1\\ \hline
1 & 6 & 5\\ \hline
2 & 6 & 2\\ \hline
3 & 6 & 1\\ \hline
4 & 6 & 1\\ \hline
5 & 6 & 1\\ \hline
1 & 7 & 6\\ \hline
2 & 7 & 3\\ \hline
3 & 7 & 2\\ \hline
4 & 7 & 5\\ \hline
5 & 7 & 4\\ \hline
6 & 7 & 1\\ \hline
\end{array}$$ $f(m,n)$ is only a function of $m/n$ and not $m$ and $n$ separately. A pattern I have noticed is always $f(n-1,n)=1$ , and $f(1,n)=n-1$ . Other than these two cases, I have not been able to find any patterns. Edit: I have found an analytical statement to evaluate this function. But its form looks somewhat complicated, so I find it difficult to believe that there can be closed-form expressions for it. The function (in a non-mathematician's language) is as follows: For 2 coprime integers $m$ and $n$ , and $x=m/n$ , $f(x)$ = $n_{max}$ where $n_{max}$ is the maximum natural number $<n$ , such that $\sin(j \pi x) \sin{ \pi(\text{Ceil}(nx) - (n-j)x)}>0$ , for all $j=1,2,3...(n-1)$ . The Ceil(x) function returns the smallest integer more than x. This function can probably be simplified further (the sines can be removed to put a condition on the arguments), but because of the Ceil function, I could not make much progress. Another interesting feature is that the plot of $x$ vs $1/f(x)$ looks similar to Thomae's function: https://en.wikipedia.org/wiki/Thomae%27s_function .","['continuity', 'functions', 'piecewise-continuity']"
4756186,Solving the o.d.e. $\frac{dy}{dx}=\frac{-x+\sqrt{x^2+y^2}}{y}$,I'm having trouble working out this following ODE: $$\frac{dy}{dx}=\frac{-x+\sqrt{x^2+y^2}}{y}$$ I know that the equation is homogenous and therefore we are able to solve it by subbing $v=\frac{y}{x}$ but also looking for a way to solve by substituting $v=x^2+y^2$ . This is my first time using MSE so please let me know if I'm asking questions in the correct way or how i can format them better :),['ordinary-differential-equations']
4756200,Question on compactness of generator of unitary group,"Let $\mathcal{H}$ be complex Hilbert space. Suppose we have a unitary group $\{U_t\},t\in\mathbb{R}$ and by Stone's Theorem we have a unique infinitesimal generator $A:\mathcal{D}(A)\rightarrow\mathcal{H}$ , which is a self-adjoint operator: $$U_t = e^{itA} \quad t\in\mathbb{R}.$$ So, I wonder is this operator $A$ compact or not? The reason why I ask this is that in P.Walter's Spectral Theorem for Unitary Operators he said ""there exists a unique finite Borel measure $\mu_f$ "", however, in other materials, the term ""spectral measure"" is used. I noticed that the Borel measure is a spectral measure if there exists an orthogonal basis formed by spectrum of $A$ , but then I also see here that the compact self-adjoint operator $A$ can have such result. So, can someone explain me more about the details here why P.Walters theorem is different from others?","['compact-operators', 'functional-calculus', 'operator-theory', 'real-analysis', 'functional-analysis']"
4756201,No group of order $112$ is centerless.,"No one of the 38 nonabelian groups of order $112$ (up to isomorphism) is centerless . Indeed, this answer uses advanced tecniques ( of course w.r.t. my level of knowledge , such as solvability, Burnside's $p^aq^b$ -theorem, etc.) to show a stronger result, namely that every group of order $112$ has an element of order $2$ in its center. Is there any ""lower-tech"" argument to prove the (weaker than the above mentioned one) result in the title? For example, I expected that the class equation in the centerless case to lead to some contradiction, also considering the possibility that any between the $2$ -Sylow or the $7$ -Sylow was normal, but unsuccessfully as far as I could go (both $16=1+2^3+7$ and $7=1+2+2+2$ are allowed sizes of union of conjuagacy classes). The only I could get to, is that the number of conjugacy classes of size $7$ must be odd, but that's seemingly not enough to get any contradiction.","['group-theory', 'finite-groups']"
4756213,"If $f: A\rightarrow A $ is one-to-one, why should its range have cardinality |A|?","I am reading a proof of the following proposition: for a finite set $A, f: A\rightarrow A $ is a bijection if there is an inverse function $g : A\rightarrow A$ such that $\forall x \in A  \ g(f(x))=x$ . When showing that $f$ is onto after it has showed that $f$ is one-to-one, the proof mentions how the fact that $f$ is one-to-one means the cardinality of its range is $|A|$ . I do not understand why this is the case from the mere fact that $f$ is one-to-one. Using the definition of one-to-one, I don't see anything saying that $f$ must map every single element of $A$ onto its range- only that if 2 elements of its domain are mapped to the same element in its range, these elements are equivalent. Did the proof commit a logical error here, or is there something I'm missing? Why must the cardinality of $f$ 's range here be $|A|$ ?","['elementary-set-theory', 'functions']"
4756220,Between us my sister and I know everything -- how many sisters do I have?,"My name is Aaron. I used to think my twin brother Bernard and I were pretty smart. You see between us, we know everything. That was until we met the triplets Charlotte, Denise, and Esther. Between any two of the sisters, they knew everything. I asked Charlotte did she know are there any quadruplets or quintuplets who, between any pair, know everything. She doesn't know but that means we can ask Denise or Esther when we see them next. Unfortunately they are on holiday in Fiji. So I will ask the internet instead. Problem: Suppose we have a set $F=\{1,2,\ldots, n\}$ of facts . Consider a family (haha, family) $\{F_1,\ldots, F_m\}$ of distinct subsets of $F$ where $F_i \neq F_j$ for $i\neq j$ , i.e., each $F_i$ is distinct. We call the family knowledgeable to mean that for any distinct $F_i,F_j$ we have $F_i \cup F_j = F$ . What is the largest cardinality of a knowledgeable family? The answer is at least $n+1$ , since you can have sets $F_i = F \setminus \{i\}$ and $F_{n+1}=F$ . Is the answer exactly $n+1$ ?","['combinatorics', 'extremal-combinatorics']"
4756249,Why is there independence in this proof of the existence of a countable sequence of iids?,"Consider this Proposition 2.4.1. Let $(\Omega, \mathcal{F}, \mathbb{P})$ be the probability space of Lebesgue measure on the Borel subsets of $(0,1)$ . Let $(F_n: n \in \mathbb{N})$ be a sequence of distribution functions. Then there exists a sequence $(X_n: n \in \mathbb{N})$ of independent random variables on $(\Omega, \mathcal{F}, \mathbb{P})$ such that $X_n$ has distribution function $F_{X_n}=F_n$ for all $n$ . Proof: Choose a bijection $m: \mathbb{N}^2 \rightarrow \mathbb{N}$ and set $Y_{k, n}=R_{m(k, n)}$ , where $R_m$ is the $m$ th Rademacher function. Set $$Y_n=\sum_{k=1}^{\infty} 2^{-k} Y_{k, n} . $$ Then $Y_1, Y_2, \ldots$ are independent and, for all $n$ , for $i 2^{-k}=0 . y_1 \ldots y_k$ , we have $$ \mathbb{P}(i 2^{-k}<Y_n \leq(i+1) 2^{-k})=\mathbb{P}(Y_{1, n}=y_1, \ldots, Y_{k, n}=y_k)=2^{-k} $$ so $\mathbb{P}(Y_n \leq x)=x$ for all $x \in[0,1]$ . Set $$ G_n(y)=\inf \{x: y \leq F_n(x)\} $$ then, by Lemma 2.2.1, $G_n$ is Borel and $G_n(y) \leq x$ if and only if $y \leq F_n(x)$ . So, if we set $X_n=G_n(Y_n)$ , then $X_1, X_2, \ldots$ are independent random variables on $\Omega$ and $$ \mathbb{P}(X_n \leq x)=\mathbb{P}(G_n(Y_n) \leq x)=\mathbb{P}(Y_n \leq F_n(x))=F_n(x) . $$ Question: I am struggling to see why are $Y_n$ independent. I can see why this would be true if the sums were finite as independence would follow from the fact that the Rademacher functions are independent and then talk about finite intersections. However, I do not see how one argues for the independence of the infinite sums.","['independence', 'probability-theory', 'random-variables']"
4756251,Lower bound for greatest LCM,"Let $k>1$ be an integer. I am looking to prove or disprove the following conjecture $(\mathscr{C}_k)$ : There exists a constant $C_k>0$ such that for any integer $n\geq k$ , if $a_1,\cdots,a_n$ are $n$ distinct positive integers, then we can find $k$ terms among them whose LCM is greater than $C_kn^k$ . I think we can prove (see below $^{\star}$ ) that $(\mathscr{C}_2)$ is true by associating this result (proved here ) : $(\clubsuit)$ Let $a_1,a_2,...,a_{p+1}$ be a sequence of distinct positive integers where $p$ is prime. Then we can find two numbers from this sequence such that largest of them divided by their GCD is $\geq p+1$ . with Bertrand–Chebyshev theorem . Any help will be appreciated in case $k>2$ $(^{\star})$ For $n$ large enough, there is a prime $p$ such that $\frac{n}{4}<p<\frac{n}{2}$ . If $a_1,\cdots,a_n$ are distinct positive integers, then the integers $a_{\left\lfloor\frac{n}{2}\right\rfloor},\cdots,a_n$ are distinct and $\geq \left\lfloor\frac{n}{2}\right\rfloor$ , and the number of these terms is $n-\left\lfloor\frac{n}{2}\right\rfloor+1\geq p+1$ . So, from $(\clubsuit)$ , we can find integers $i$ and $j$ such that $p+1\geq j>i\geq\left\lfloor\frac{n}{2}\right\rfloor$ such as $\frac{a_j}{\gcd(a_i,a_j)}\geq p+1$ , so $\text{lcm}(a_i,a_j)\geq(p+1)a_i>\left(\frac{n}{4}+1\right)\left\lfloor\frac{n}{2}\right\rfloor>\left(\frac{n}{4}+1\right)\left(\frac{n}{2}-1\right)$ , hence $\text{lcm}(a_i,a_j)\geq\frac18n^2$ for $n\geq4$ . (Edit) In fact I just realized that we could prove $(\mathscr{C}_2)$ using a weaker result than $(\clubsuit)$ and much easier to prove: Let $a_1,a_2,...,a_{p+1}$ be a sequence of distinct positive integers where $p$ is prime, then we can find two numbers from this sequence such that largest of them divided by their GCD is $\geq p$ (not $\geq p+1$ ).","['divisibility', 'number-theory', 'gcd-and-lcm', 'elementary-number-theory', 'prime-numbers']"
4756290,Generalisation of the idea of decomposition into even and odd functions,"We know that a function $f(z)$ can be decomposed into $\frac{f(z)+f(-z)}{2}$ and $\frac{f(z)-f(-z)}{2}$ . These are called the even and odd components. I have made a generalisaiton of this. Suppose $f(z)$ is our function on the complex plane, and $e^{\frac{i2\pi q}{n}}$ are the $nth$ roots of unity, $q=0,1,2,3...n-1$ . We can decompose the function into $n$ functions $g_k(z)$ , $k=0,1,2,3....n-1$ given by: $g_k(z)= \frac{1}{n} \sum _{q=0} ^{n-1} e^{\frac{2\pi qk}{n}} f(e^{2\pi i \frac{q}{n}} z)$ Adding these back together: $$\sum _{k=0}^{n-1} g_k (z)$$ $$= \sum _{k=0}^{n-1} \frac{1}{n} \sum _{q=0} ^{n-1} e^{\frac{2\pi qk}{n}} f(e^{2\pi i \frac{q}{n}} z)$$ $$= \sum _{q=0}^{n-1} \frac{1}{n}( \sum _{k=0} ^{n-1} e^{\frac{2\pi qk}{n}} f(e^{2\pi i \frac{q}{n}} z))$$ If we look at the first term of this sum corresponding to $q=0$ , we get $\frac{1}{n} nf(z)=f(z)$ . The terms for other values of $q$ are 0. This means that the decomposition $g_k (z)$ adds upto $f(z)$ . For $n=2$ , this is the same as the decomposition of $f(z)$ into even and odd functions. This is a generalisation of the decomposition into even and odd functions. Each of the functions $g_k(z)$ has a rotational symmetry of rotation of the argument $z$ by $e^{\frac{i2\pi k}{n}}$ . Is there a name for this generalisation?","['complex-analysis', 'functions', 'even-and-odd-functions']"
4756314,Finding solution of non-homogeneous ODE when we know the solutions of the homogeneous counterpart,"Given ODE is: $$y''+Py'+Qy=0$$ It has two solutions $f(x)$ and $xf(x)$ . I need to find the solution to the following non-homogeneous ODE: $$y''+Py'+Qy=f(x)$$ where $f(x)$ on the RHS is the solution of the first ODE. I need to find its solution. I tried to apply variation of parameters method, but failed to proceed and also found this question similar. But being a beginner who's self-learning ODE, I am stuck. An elaboration of its solution would really help me to understand the working of such questions. EDIT I reconsidered the variation of parameter method and seemed to find a legit solution! $$y=f(x)\left(\int \frac{-xf(x).f(x) dx}{f(x)^2} +c_1\right) + xf(x)\left( \int \frac{f(x). f(x) dx}{f(x)^2} +c_2\right)$$ P.S. The denominator is the wronskian  of $f(x)$ and $xf(x)$ which gives $f(x)^2$ On solving, it gives: $$y=f(x) \cdot \left(\frac {x^2}{2}+c_2 x+ c_1\right)$$ Thanks for the valuable comment!","['ordinary-differential-equations', 'fundamental-solution']"
4756340,Prove that $\ln x\leq\frac{x^{x+1/x}-1}{2}$ [ not solved ],"Prove that $$\ln x\leq\frac{x^{x+1/x}-1}{2}$$ is true for every positive real number, without calculus/derivative . (i.e. using some inequalities) My progress. For $x\geq 1$ using $x+1/x\geq 2$ we obtain $$\frac{x^{x+1/x}-1}{2}\geq\frac{x^{2}-1}{2}$$ So it suffices to prove $$\ln x\leq\frac {x^2-1}{2}$$ or $$x^2\geq 2\ln x+1$$ The inequality seems easier now. But, I am stuck here. I don't have any approach for $0<x<1$ .","['algebra-precalculus', 'logarithms', 'inequality']"
4756385,How to evaluate the integral $\int_1^9\frac{dx}{x\sqrt{81-x^2}}$,"So I was recently looking at a new book on Calculus [1] that I got and decided to look at the techniques for integration that it listed. I then decided after a while to make my own integral and evaluate it using the techniques that it listed. Here is the integral I came up with [2] : $$\int_1^{9}\dfrac{dx}{x\sqrt{81-x^2}}$$ which I thought that I might be able to evaluate. Here is my attempt at doing so: We use the substitution $x=9\sin(\theta)\implies\theta=\sin^{-1}\left(\dfrac x9\right)$ , and then we can have $dx=9\cos(\theta)d\theta$ , and then $\sqrt{81-x^2}$ becomes [3] $$\sqrt{81-81\sin^2(\theta)}=9\sqrt{\sin^2\theta}\implies9\sqrt{\cos^2(\theta)}\implies9|\cos(\theta)|=\sqrt{81-\dfrac{x^2}9}$$ $$\because|\cos(\theta)|=\sqrt{a^2-\dfrac{x^2}a}\because\text{the inverse sine function}$$ $$\text{oscillates between }-\dfrac\pi2\text{ and }\dfrac\pi2\text{ which implies}$$ $$\cos(\theta)=|\cos(\theta)|=\sqrt{a^2-\dfrac{x^2}a}=\sqrt{81-\dfrac{x^2}9}$$ This implies the integral $I(t)$ is now $$\int_1^9\dfrac{\require{cancel}\cancel{9\cos(\theta)}d\theta}{9\sin(\theta)\cancel{(9\cos(\theta))}}$$ $$=\dfrac19\int_1^9\csc(\theta)d\theta$$ Now there’s only one problem: Here’s what we get when we evaluate the integral: $$\dfrac19\left[\operatorname{Ln}\left(\sin\left(\dfrac\theta2\right)\right)-\operatorname{Ln}\left(\cos\left(\dfrac\theta2\right)\right)\right]_1^9$$ There’s only one problem. I know how to evaluate any $\sin(\dfrac x2)$ , but I don’t know how to evaluate any $\cos(\dfrac x2)$ . Then, I decided to go back in the book to the part on trigonometric functions and saw definition ( $16.13$ ): ( $16.13$ ) $\quad\cos^2(u)=\dfrac{1+\cos u}2$ This was great because now I could finally integrate the function. Then, $$\dfrac19\left[\operatorname{Ln}\left(\sin\left(\dfrac\theta2\right)\right)-\operatorname{Ln}\left(\cos\left(\dfrac\theta2\right)\right)\right]_1^9$$ $$=\dfrac19\left[\ln\left(\sqrt{\dfrac12(1-\cos(\theta))}\right)-\ln\left(\sqrt{\dfrac12(1+\cos(\theta))}\right)\right]_1^9$$ $$\dfrac1{18}\left[\ln\left(\dfrac{1-\cos(\theta)}2\right)-\ln\left(\dfrac{1+\cos(\theta)}2\right)\right]_1^9$$ $$\implies\dfrac1{18}\left[\ln\left(\dfrac{1-\cos(\theta)}{1+\cos(\theta)}\right)\right]_1^9$$ $$\implies\dfrac1{18}\left[\ln\left(\dfrac{9-\sqrt{81-x^2}}{9+\sqrt{81-x^2}}\right)\right]_1^9$$ which converges to $$-\dfrac1{18}\ln\left(\dfrac{(9-\sqrt{80})^2}{161}\right)$$ or approximately $0.603108$ My question Did I evaluate the integral correctly, or what could I do to evaluate it correctly? Notes [1] Calculus by Elliot Mendelson, PHD. [2] Based off of Chapter $32$ “Techniques of Integration II” example $32.12$ [3] This is done using Strategy II: “If $\sqrt{a^2-x^2}$ occurs in an integrated, try the substitution $x=a\sin\theta$ .”","['integration', 'calculus', 'solution-verification', 'definite-integrals']"
4756401,Minimum number of straight lines to cover $n \times n$ grid?,"I want to know the minimum number of lines needed to touch every square of an $n \times n$ grid. The only added rule is that the line has to pass inside the square, not on the edge/corner. I have found a solution for $n-1$ lines for all $2<n\le 10$ but not with fewer lines. I figure you can represent the lines by the squares they pass through, thus making a finite amount of ""lines"", but I don't have many further ideas.
Here is an example of what I mean by ""covering a square"". The line just needs to pass through any part of the square: Here is also a solution for a $3 \times 3$ grid with 2 lines:","['combinatorics', 'discrete-mathematics', 'computational-geometry']"
4756402,how to evaluate $\int_{5 n \pi}^{\frac{5\pi (n^2 + 1)}{n}}\frac{\frac{8 n^2 x}{25}dx}{((a^2 + b^2) + (a^2 - b^2) cos\frac{2 n x}{5})^2}$,"$$\int_{5 n \pi}^{\frac{5\pi (n^2 + 1)}{n}}\frac{\frac{8 n^2 x}{25}dx}{((a^2 + b^2) + (a^2 - b^2) cos\frac{2 n x}{5})^2}$$ for $n   \in \mathbb{N}\space\space\text{and } a,b >0$ I know this problem has to be solved by properties of definite integrals. so the first thing I tried was. $$\frac{1}{k}\cdot\int_{ak}^{bk}f\left(\frac{x}{k}\right)dx$$ so by $k=\frac{2n}{5}$ $$\frac{5}{2n}\int_{2\pi n^2 }^{2\pi (n^2 + 1)}\frac{\frac{4nx}{5}dx}{((a^2 + b^2) + (a^2 - b^2) cosx)^2}$$ $$\frac{5}{2n}\cdot\frac{4n}{5}\int_{2\pi n^2 }^{2\pi (n^2 + 1)}\frac{xdx}{((a^2 + b^2) + (a^2 - b^2) cosx)^2}$$ $$2\int_{2\pi n^2 }^{2\pi (n^2 + 1)}\frac{xdx}{(a^2(1+cosx) + b^2(1-cosx))^2}$$ now I have no clue how to solve it further. The function has no period nor did the  Weierstrass Substitution work did I mess up at any point? or was my entire method wrong? Kindly help me. PS: It's a high school-level problem. $$\text{Solution: }\frac{\pi^2(2n^2+1)(a^2+b^2)}{4a^3b^3}$$","['integration', 'calculus', 'definite-integrals', 'trigonometric-integrals']"
4756414,"Prove $_4 F_3\left(2,\frac32,\frac32,\frac32;\frac52,\frac52,\frac52;1\right)=\frac{27}{16}\left(\pi^2-7\zeta(3)\right) $","How to prove the following result about the generalized hypergeometric function $_4 F_3$ ? $$_4 F_3\left(2,\frac32,\frac32,\frac32;\frac52,\frac52,\frac52;1\right)\stackrel{?}=\frac{27}{16}\left(\pi^2-7\zeta(3)\right) $$ Apart from the definition, I know very little about hypergeometric functions. $$_4 F_3(a_1,a_2,a_3,a_4;b_1,b_2,b_3;z)=\sum_{n=0}^\infty \frac{(a_1)_n(a_2)_n(a_3)_n(a_4)_n}{(b_1)_n(b_2)_n(b_3)_n}\frac{z^n}{n!} $$ This particular $_4 F_3$ is not in one of the many known closed forms provided by Wolfram . I also tried getting an idea by reading Jack D'aurizio's paper Surprising identities for the hypergeometric $_4 F_3$ function , but I couldn't see how to attack this series. Could you please suggest any ideas on how to prove it?","['real-analysis', 'calculus', 'closed-form', 'sequences-and-series', 'hypergeometric-function']"
4756420,Walking around a cube to return to starting point,"Suppose I have a cube of side length $1$ unit and I am supposed to walk $6$ units in total, starting from one vertex of the cube. I can move from one vertex to only any other adjacent vertex through the edge of the cube. In how many ways can I start and end at the same vertex? My work Suppose I start from any one of the vertices. Then I have noted that one way to satisfy given constraints is by moving to any adjacent vertex and moving back to the origin a total of 3 times. This gives me 27 possible ways for this case. I also think that it has something to do with how many edges of a face are traversed which determines if the condition can be satisfied. But I can not work out anything concrete. However if I try to do some casework I immediately find that the number of cases are too many to count and hence I’m unable to conclude anything useful. Help is appreciated.",['combinatorics']
4756578,(Path) connected components of zero-sets of limiting functions.,"$\newcommand{\set}[1]{\{#1\}}$ $\newcommand{\R}{\mathbb R}$ $\newcommand{\bd}{\text{Bd}}$ Let $X$ be a compact metric space and $f_n:X\to \R$ be a continuous function on $X$ , one for each $n$ .
Assume $f_n$ converges uniformly to a function $f:X\to \R$ . Let $Z_n$ be the zero-set of $f_n$ , that is $Z_n$ is the set of all the points $x$ in $X$ such that $f_n(x) = 0$ .
Let $Z$ be the zero-set of $f$ . Assume that there are two points $p$ and $q$ in $X$ such that $p, q\in Z_n$ for each $n$ .
It follows that $p, q\in Z$ . Lemma. If $p$ and $q$ are in the same connected component of $Z_n$ for each $n$ , then they are also in the same connected component of $Z$ . (A proof of the lemma is at the end (I haven't seen it in a book or paper so there could be a mistake with my reasoning)). Question. Can we replace component with path component in the above lemma? In other words, if, for each $n$ , there was a path $\gamma_n:[0, 1]\to X$ such that $f_n\circ \gamma_n$ is identically $0$ and $\gamma_n(0) = p, \gamma_n(1)= q$ , then can we say that there is a path $\gamma:[0, 1] \to X$ with $f\circ \gamma$ being identically zero and $\gamma(0) = p, \gamma(1) = q$ . Proof of Lemma. Suppose not.
First note that $f$ is continuous since it the uniform limit of a sequence of continuous functions on a compact metric space.
Therefore $Z$ , like each $Z_n$ , is a closed subset of $X$ . Since we have assumed that $p$ and $q$ are in different components of $Z$ , there exist disjoint open sets $U$ and $V$ of $X$ such that $p\in U, q\in V$ , $(U\cap Z) \cap (V\cap Z) = \emptyset$ , and $Z\subseteq U\cup V$ We claim that there exist $U'$ and $V'$ open in $X$ such that $\bar U'\subseteq U, \bar V'\subseteq V$ , $p\in U', q\in V'$ , $(Z\cap U')\cap(Z\cap V') = \emptyset$ , $Z\subseteq U'\cup V'$ To see this, for each point $x\in \bd(U)$ , where $\bd(U)$ means the boundary of $U$ , we can find a neighborhood $O_x$ of $x$ such that $\bar O_x$ is disjoint from $Z$ .
Since $\bd(U)$ is compact, there exist finitely many points $x_1,\ldots, x_n$ in $\bd(U)$ such that $O_{x_1}\cup \cdots \cup O_{x_n}$ contains $\bd(U)$ .
Define $U' = U \setminus (\bar O_{x_1}\cup \cdots \cup \bar O_{x_n})$ .
By a similar procedure we construct $V'$ .
Then $U'$ and $V'$ satisfy the requirement of the claim. Since $f_n$ converges uniformly to $f$ we get that $Z_n\subseteq U'\cup V'$ for each large enough $n$ .
By passing to a tail we may assume that $Z_n\subseteq U'\cup V'$ for each $n$ .
Now using the hypothesis that $p$ and $q$ are in the same component of $Z_n$ , we deduce that $(U'\cap Z_n) \cap (V'\cap Z_n)$ cannot be empty.
So let $x_n \in (U'\cap Z_n)\cap (V'\cap Z_n)$ be chosen for each $n$ .
Passing to a subsequence, we may assume that $x_n\to x$ for some $x\in X$ .
Then $x$ must lie in $\overline{U'\cap V'}$ , and hence, in particular, it must lie in $U\cap V$ . Using $f_n(x_n) = 0$ for each $n$ , and the uniform convergence of $f_n$ to $f$ , we deduce that $f(x) = 0$ , that is $x\in Z$ .
Therefore $x\in (U\cap Z) \cap (V\cap Z)$ , contradicting the choice of $U$ and $V$ . $\blacksquare$","['connectedness', 'path-connected', 'uniform-convergence', 'general-topology', 'compactness']"
4756699,"How to understand Euler's criterion for exactness of differential equation as presented in the book ""Physical chemistry"" by Silbey, Alberty, Bawendy?","While reading a chapter about thermodynamics in a book, I encountered a small section on exact and inexact differentials. Since the book is a chemistry book, I am not sure if the math details/assumptions are being partially omitted from the reader. Says the book, There is a simple test to see whether a differential is exact. For a system with just two independent degrees of freedom, the total
differential $dz$ of a quantity $z$ may be determined by the
differentials $dx$ and $dy$ in two other quantities $x$ and $y$ . In
general, $$dz=M(x,y)dx+N(x,y)dy\tag{2.17}$$ where $M$ and $N$ are functions of the independent variables $x$ and $y$ . To show the test for exactness we now consider a function $z$ that has
an exact differential. If $z$ has a definite value at each point in
the $xy$ -plane, then it must be a function of $x$ and $y$ . If $z=f(x,y)$ , then $$dz=\left (\frac{\partial z}{\partial x}\right )_ydx +\left
 (\frac{\partial z}{\partial y}\right )_x dy\tag{2.18}$$ Comparing (2.17) and (2.18) we find $$M(x,y)=\left (\frac{\partial z}{\partial x}\right )_y\tag{2.19}$$ $$N(x,y)=\left (\frac{\partial z}{\partial y}\right )_x\tag{2.20}$$ Since the mixed partial derivatives are equal, $$\left [ \frac{\partial}{\partial y} \left (\frac{\partial
 z}{\partial x}\right )_y\right ]=\left [ \frac{\partial}{\partial x}
 \left (\frac{\partial z}{\partial y}\right )_x \right ]\tag{2.21}$$ then $$\left (\frac{\partial M}{\partial y}\right )_x=\left (\frac{\partial
 N}{\partial y}\right )_y\tag{2.22}$$ This equation must be satisfied if $dz$ is an exact differential. It
is Euler's criterion for exactness. Here is what I think is being said, in my own words and with more assumptions made explicit. (2.17) denotes a general differential. We don't know if it is exact or not. We wish to find some test that enables us to determine if this differential equation is exact or not. Then we are given a function $z=z(x,y)$ that has, by assumption, an exact differential (equivalently, is differentiable). The exact (total) differential of $z$ is $$dz=\left (\frac{\partial z}{\partial x}\right )_ydx +\left
(\frac{\partial z}{\partial y}\right )_x dy\tag{2.18}$$ It is stated that the mixed partial derivatives of $z$ are equal. This occurs if $D_1 z$ , $D_2 z$ , and one of the mixed partial derivatives exist and, in addition, the mixed partial derivative is continuous in the open set $S$ that contains the point in consideration in this analysis. This is implicit in the snippet. The snippet contains two conditions for the functions $M(x,y)$ and $N(x,y)$ . $M$ and $N$ are the partial derivatives of some function (called $z$ as well, but I am going to call it $\varphi$ ) The mixed partials of $\varphi$ are equal. Euler's criterion for exactness seems to be stated as being requirement 2). As far as I know, given $M$ and $N$ , if all we know is that $D_2 M=D_1 N$ , then all we have is a necessary condition for $\langle M, N \rangle$ to be a gradient on an open set $S$ (unless the set $S$ is convex, in which case we have a sufficient condition). My questions are: is my interpretation correct? Also, the way the book presents all of this, doesn't 2) require 1) to be true? Shouldn't Euler's criterion be 1) and 2) together? What is Euler's criterion for exactness after all?","['multivariable-calculus', 'ordinary-differential-equations']"
4756819,"If for a.e. $x \in X$ the sequence $(f_n(x, \cdot))_n$ is Cauchy in $L^p_{\text{loc}} (Y)$, then $(f_n)$ is Cauchy in $(L^0 (Z), \rho_Z)$","Let $T>0$ and $p \in [1, \infty)$ , $X :=[0, T]$ and $Y:= \mathbb R^d$ , $\cal A$ the Lebesgue $\sigma$ -algebra of $X$ , $\cal B$ the Lebesgue $\sigma$ -algebra of $Y$ , $\mu, \nu$ complete finite measures on $X, Y$ respectively, $\mu, \nu$ absolutely continuous, $Z :=X \times Y$ , $\cal C$ the product $\sigma$ -algebra of $\cal A$ and $\cal B$ , $\lambda$ the product measure of $\mu$ and $\nu$ , $(Z, \overline{\mathcal C}, \overline{\lambda})$ the completion of $(Z, \mathcal C, \lambda)$ , $L^0 (X) := L^0 (X, \mathbb R)$ the space of measurable functions from $X$ to $\mathbb R$ . Let $L^p_{\text{loc}} (Y)$ be the space of measurable functions $f:Y \to \mathbb R$ such that $$
\|f\|_{L^p_{\text{loc}}} := \sup_{y \in Y} \|1_{B(y, 1)} f\|_{L^p} < \infty,
$$ where $B(y, 1)$ is the open unit ball centered at $y$ . Let $(y_m)$ be a countable dense subset of $Y$ . A sphere is a $\nu$ -null set. Then by dominated convergence theorem (DCT), $$
\|f\|_{L^p_{\text{loc}}} = \sup_{m \in \mathbb N} \|1_{B(y_m, 1)} f\|_{L^p}
\quad \forall f \in L^p_{\text{loc}} (Y).
$$ Let $\rho_Y$ be a pseudometric on $L^0(Y)$ defined by $$
\rho_Y (f, g) := \int_Y \min\{|f-g|, 1\} \, \mathrm d \nu
\quad \forall f, g \in L^0 (Y).
$$ We define $\rho_Z$ on $L^0(Z)$ similarly. I would like to prove an analogue of this result, i.e., Theorem Let $f_n \in L^0(Z)$ for all $n \in \mathbb N$ . Assume that for $\mu$ -a.e. $x \in X$ , the sequence $(f_n(x, \cdot))_n$ is a Cauchy sequence in $(L^p_{\text{loc}} (Y), \| \cdot\|_{L^p_{\text{loc}} (Y)})$ . Then $(f_n)$ is a Cauchy sequence in $(L^0 (Z), \rho_Z)$ . Could you have a check on my below attempt? Thank you so much for your help! For simplicity, we write $$
\nu (|f - g| > \delta) := \nu (\{y \in Y : |f (y) - g(y)| > \delta\})
\quad \forall \delta >0, \forall f,g \in L^0 (Y).
$$ Let $\hat \rho_Y$ be another pseudometric on $L^0(Y)$ defined by $$
\hat \rho_Y (f, g) := \inf_{\delta >0} \{ \nu (|f - g| > \delta)  +\delta \}
\quad \forall f, g \in L^0 (Y).
$$ We define $\hat\rho_Z$ on $L^0(Z)$ similarly. We have some useful properties, i.e., Lemma 1 $(L^0(Y), \rho_Y)$ is complete. Lemma 2 $(L^0(Y), \hat \rho_Y)$ is complete. Lemma 3 The following statements are equivalent: (S1) $\rho_Y (f_n, f) \to 0$ . (S2) Every subsequence of $(f_n)$ has a further subsequence that converges to $f$ $\nu$ -a.e. (S3) $f_n \to f$ in measure, i.e., $$
\nu (\{y \in Y : |f_n (y)-f(y)| > \varepsilon\}) \xrightarrow{n \to \infty} 0
\quad \forall \varepsilon>0.
$$ (S4) $\hat \rho_Y (f_n, f) \to 0$ . Proof The proof of the completeness of $\rho_Y$ can be found here . The proof of the completeness of $\hat\rho_Y$ can be found here . The proof of (S1) $\iff$ (S2) can be found here . The proof of (S3) $\iff$ (S4) can be found here . The proof of (S1) $\iff$ (S3) can be found here . Now we are going to prove our theorem. Assume the contrary that $(f_n)$ is not a Cauchy sequence in $(L^0 (Z), \rho_Z)$ . By above Lemmas $(f_n)$ is not a Cauchy sequence in $(L^0 (Z), \hat \rho_Z)$ . There is $\varepsilon >0$ and two subsequences $\varphi, \psi$ of $\mathbb N$ such that $$
\hat \rho_Z (f_{\varphi (n)}, f_{\psi (n)}) \ge \varepsilon
\quad \forall n \in \mathbb N.
$$ Then $$
\lambda (|f_{\varphi (n)} - f_{\psi (n)}| > \delta)  +\delta \ge \varepsilon
\quad \forall n \in \mathbb N, \forall \delta >0.
$$ We pick $\delta := \varepsilon/2$ and get $$
\lambda (|f_{\varphi (n)} - f_{\psi (n)}| > \varepsilon/2)  \ge \varepsilon/2
\quad \forall n \in \mathbb N.
$$ Let $g_n := f_{\varphi (n)} - f_{\psi (n)}$ . Then $g_n \in L^0(Z)$ . We define $$
h_n: X \to \mathbb R, x \mapsto \| g_n (x, \cdot) \|_{L^p_{\text{loc}}}.
$$ It has been verified that $h_n$ indeed belongs to $L^0 (X)$ . By our assumption, $h_n \to 0$ $\mu$ -a.e. Lemma 4 Let $g_n \in L^0(Z)$ for all $n \in \mathbb N$ such that for $\mu$ -a.e. $x \in X$ we have $(g_n(x, \cdot))_n \subset L^p_{\text{loc}}  (Y)$ . We define $$
h_n: X \to \mathbb R, x \mapsto \| g_n (x, \cdot) \|_{L^p_{\text{loc}}}.
$$ If $\rho_{X} (h_n, 0) \to 0$ then $\rho_Z (g_n, 0) \to 0$ . By Lemma 4 , $g_n \to 0$ in measure, which is a contradiction. This completes the proof.","['banach-spaces', 'measure-theory', 'solution-verification', 'lp-spaces', 'functional-analysis']"
4756827,Can't logically find critical points but everything works,"I'm trying to find the critical points of: $$ f(x,y) = (x^2 + 2y^2)e^{1-x^2-y^2} $$ I applied the product rule and chain rule for each of the partial derivatives to get these two: $$f_x(x,y) = 2x(e^{1-x^2-y^2}) + (x^2 + 2y^2)(-2x)(e^{1-x^2-y^2})$$ $$f_y(x,y) = 4y(e^{1-x^2-y^2}) + (x^2 + 2y^2)(-2y)(e^{1-x^2-y^2})$$ When I set those two to zero I can reduce them to: $$x^2 + 2y^2 = 1$$ $$x^2 + 2y^2 = 2$$ Which I'm pretty sure is right. Trying to solve those two I only get nonsense like $1 = 2$ or the square root of a negative. I looked at the answer and the points are $(1,0), (-1,0), (0,1), (0,-1), (0,0)$ I can see that those work out with the two formulas right above but I can't get there logically. Is there a flaw in my reasoning so far or do I just need to keep working to solve those two partials ? Also if $y$ is undefined for a value of x does that make it critical value of $x$ ?",['multivariable-calculus']
4756840,Why can we prove facts about Euclidean geometry using coordinate method?,"It's easy to show that coordinate geometry based on real number axioms satisfies the Euclidean postulates. But how do we go the other way around? Say we prove an arbitrary * statement about Euclidean geometry using coordinates. How can we translate it to a proof that solely relies on Euclid's postulates? By ""coordinate geometry"" Euclidean space with Euclidean norm is meant specifically. * there I'm assuming the statement is provable in Euclidean geometry","['real-numbers', 'euclidean-geometry', 'analytic-geometry', 'axioms', 'geometry']"
4756849,Olympiad Algebra Polynomial Regarding Exponential Functions,"There exists a degree 4 polynomial M(y) such that M(y)=2 y for y = 1,2,3,4,5. Find M(6). This problem appeared on my IOQM (Indian Olympiad Qualifiers for Mathematics) mock test paper. The official solution is very incomprehensive and it uses the combinatorial formula of 2 y (i.e. - 

∑ n C k ) Which I also don't understand. Does anyone have a different solution?","['contest-math', 'algebra-precalculus', 'polynomials']"
4756880,Combinatorial proof for an integral,"The normalizing constant of the Dirichlet distribution implies that $$
\int_{\Delta_k} x_1^{n_1}\dots x_k^{n_k} dx_1 \dots dx_k = \frac{n_1! \dots n_k!}{(n_1+\dots+n_k+k-1)!}
$$ where $$
\Delta_k = \{(x_1, \dots, x_k) \in [0,1]^k: x_1+\dots+x_k=1\}
$$ for $n_1,\dots,n_k \in \mathbb N$ . Is there a combinatorial proof for this?","['integration', 'combinatorial-proofs', 'reference-request', 'combinatorics', 'probability']"
4756883,What is so special about $\mathbb{Q}(\sqrt{398})$ that it has several good prime generating polynomials?,"Going through an old post of mine from 2014, I realized there was a curiosity that hasn't been fully explained up to now. Consider the following simple prime-generating polynomials. ( My thanks to user25406 for the simpler form .) $$F_1(n)=(2n+1)^2-398$$ $$F_2(n)=(2n+1)^2-4\times398$$ $$\;F_3(n)=(2n+1)^2-16\times398$$ $$\,F_4(n)=2(2n+1)^2-\tfrac12\times398$$ The discriminant (square-free) of these quadratics is $d=398.$ This is the largest even $d$ of real quadratic fields with class number 1 and related cfrac of period 4 . ( A050953 ) Just like Euler's $F(n) = n^2+n+41=\big(n+\frac12\big)^2+\frac{163}4$ , the four polynomials above are remarkably good at producing primes (counting negative values): $$\begin{array}{|c|c|c|c|}
\hline
F_k(n)&\text{Range of n}&\text{Total primes}\\
\hline
F_1(n)& 0\text{ to }26& 27\\
\hline
F_2(n)& 1\text{ to }35& \color{red}{35}\\
\hline
F_3(n)& 23\text{ to }53& 31\\
\hline
F_4(n)& 0\text{ to }30& 31\\
\hline
\hline
\end{array}$$ with the second almost catching up with the 40 primes of Euler's polynomial. Here is is a plot of the first three (which are just squares of odd numbers subtracted by $398\times4^m$ ), For period 1 and 3, see "" Relatives of Heegner numbers? "" For period 2, see "" Prime-generating polynomials like $F(n) = 7n^2+49n+41$ ? "" Questions: Being class number 1 probably helps, but what is it that makes $d=398$ so special it is involved in four similar polynomials very good at producing primes within a range? (The cfrac expansion of $\sqrt{398}$ is $19;\overline{1,18,1,38},$ hence has period 4.) I tested $e^{\pi\sqrt{398}}$ and is nothing remarkable. After all, the j-function $j(\tau)$ uses complex arguments $\tau$ . So is there some special function that uses real arguments involving $\sqrt{398}$ such that we get a Ramanujan-type result similar to $e^{\pi\sqrt{163}} \approx 640320^3$ ?","['class-field-theory', 'number-theory', 'special-functions', 'prime-numbers']"
4756896,Soft question: Which topological properties are transferred by bijective continuous maps?,"Note The following question was asked by @Tian Vlašić, who then closed it after several interesting comments had been made. I was interested enough that I wanted to revive it. Let us say that a property $P$ of topological spaces is transferred by bijective continuous maps if for every pair $X,Y$ of topological spaces, the statements $P(X)$ and there exists a bijective continuous map $f: X \rightarrow Y$ implies the statement $P(Y)$ . It is clear that every property of topological spaces that is transferred by bijective continuous maps is a topological property. I have noticed that the following well-known topological properties are in fact transferred by bijective continuous maps: the cardinality of the underlaying set of the topological space the cardinality of the topology of the topological space Hausdorffness of the topological space (Wrong! See comments) I was quite surprised that Hausdorffness of the topological space is a property of topological spaces transferred by bijective continuous maps (note that this is a stronger statement than the statement that Hausdorffness of the topological space is a topological property). My question is the following. What are some other well-known topological properties that are transferred by bijective continuous maps? Note Some comments  (from F. Shrike) on this question already noted the following: Cardinality is pretty trivially transferred Compactness and (path-)connectivity are transported (even if you remove injectivity as an assumption). A curious question is whether or not arc-connectivity is transported Compactness and connectivity are easy from surjectivity, they have extremely well known proofs (and duplicates on this site). As for arc-connectivity, I have no idea if this is true. It's true if the domain is Hausdorff by a difficult theorem I don't know the proof of + your claim that Hausdorffness is transported.",['general-topology']
4756909,Multivariable Calc. Integration by Parts,"$$
\iint_S f(\nabla \times \mathbf{A}) \cdot d\mathbf{S} = \oint_{\partial S} f \mathbf{A} \cdot d\mathbf{r} - \iint_S \mathbf{A} \times (\nabla f) \cdot d\mathbf{S}
$$ Is anything wrong with this. My textbook says it should be $$
\iint_S f(\nabla \times \mathbf{A}) \cdot d\mathbf{S} = \oint_{\partial S} f \mathbf{A} \cdot d\mathbf{r} + \iint_S \mathbf{A} \times (\nabla f) \cdot d\mathbf{S}
$$ Which of the two is correct? I used integration by parts and Stoke's theorem.",['multivariable-calculus']
4756953,The Weak Besicovitch Covering Property and the Lebesgue Differentation Property,"Some preliminary terminology. Before making the question let me introduce some terminology. Notation. Let $X$ be a set and $A$ a subset of $X$ . I denote by $\chi_A\colon X\to\{0,1\}$ the characteristic function of $A$ . Definition $1$ . We say that a metric space $(X,d)$ satisfies the Weak Besicovitch Covering Property $1$ (WBCP1) if there exists a constant $K\in\mathbb{N}^+$ such that every finite Besicovitch family of balls of $(X,d)$ has cardinality $\le K$ . Recall that a Besicovitch family of balls of $(X,d)$ is a family $\mathcal{F}$ of closed balls of $(X,d)$ such that the centre of each ball does not belong to any other ball of the family and such that $\bigcap\mathcal{F}\ne\emptyset$ . Proposition $a$ . The WBCP $1$ is equivalent to require that there exists a constant $K\in\mathbb{N}^+$ such that for every finite family of closed balls of $(X,d)$ there exists a subfamily $\mathcal{G}\subseteq\mathcal{F}$ such that \begin{equation}
\chi_C\le\sum_{G\in\mathcal{G}}\chi_G\le K
\end{equation} where $C$ is the set of all the centres of the balls of $\mathcal{F}$ . Definition $2$ . We say that a metric space $(X,d)$ satisfies the Weak Besicovitch Covering Property $2$ (WBCP2) if there exists a constant $N\in\mathbb{N}^+$ such that for every bounded set $A$ of $X$ and for every family of closed balls $\mathcal{F}$ of $(X,d)$ such that each point of $A$ is the centre of some ball of $\mathcal{F}$ and such that either $\sup\{r_B\mid B\in\mathcal{F}\}=+\infty$ or $\{r_B\mid B\in\mathcal{F}\}$ is a discrete subset of $(0,+\infty)$ , there exists a countable subfamily $\mathcal{G}\subseteq\mathcal{F}$ such that it holds \begin{equation}
\chi_A\le\sum_{G\in\mathcal{G}}\chi_G\le N.
\end{equation} Now, we are ready for the question. How can I prove the following proposition? It is taken from page 109 of the "" New Trends on Analysis and Geometry in Metric Spaces "", Levico Terme, Italy 2017. Proposition $b$ . The WBCP $1$ implies the WBCP $2$ in every metric space $(X,d)$ . A possible way to prove it could be to follow the idea of the proof of the fact that in every doubling metric space $(X,d)$ the WBCP $1$ is equivalent to the BCP (see Proposition 3.7 of the Article "" BESICOVITCH COVERING PROPERTY ON GRADED GROUPS AND
APPLICATIONS TO MEASURE DIFFERENTIATION "" of Le Donne and Rigot.). I recall the BCP in the following definition. Definition 3. We say that a metric space $(X,d)$ satisfies the Besicovitch Covering Property (BCP) if there exists a constant $N\in\mathbb{N}^+$ such that for every bounded set $A$ of $X$ and for every family of closed balls $\mathcal{F}$ of $(X,d)$ such that each point of $A$ is the centre of some ball of $\mathcal{F}$ , there exists a countable subfamily $\mathcal{G}\subseteq\mathcal{F}$ such that it holds \begin{equation}
\chi_A\le\sum_{G\in\mathcal{G}}\chi_G\le N.
\end{equation} Why I need the proof of Proposition b? Because I'm trying to prove the implication 1 $\implies$ 2 of the Preiss Theorem that is the following one. Theorem. Let $(X,d)$ be a separable complete metric space. Then the following are equivalent. $(X,d)$ is $\sigma$ -finite dimensional, that is to say, there exists a sequence $\{X_n\}_{n\in\N}$ of subsets of $X$ such that $X=\bigcup_{n=0}^{+\infty}X_n$ and a sequence $\{s_n\}_{n\in\N}\subseteq(0,+\infty]$ such that every set $X_n$ has finite Nagata dimension inside $X$ on scale $s_n$ . $(X,d)$ has the Lebesegue Differentation Property, that is to say, for every locally finite Borel measure $\mu$ on $X$ it exists \begin{equation}
\lim_{r\to0^+}\frac{1}{\mu(\mathbb{B}(x,r))}\int_{\mathbb{B}(x,r)}f\,d\mu=f(x)\quad\text{ for }\mu-a.e.\, x\in X
\end{equation} for every $\mu$ -measurable function $f\colon X\to\overline{\mathbb{R}}$ such that \begin{equation}
\int_{A}|f|\,d\mu<+\infty\text{ for all $\mu$-measurable bounded set $A\subseteq X$}.
\end{equation} If anyone knows where I can find a detalied Proof of the latter theorem, please let me know becasuse it would be very usefull for me. I know that David Preiss proved it the "" Dimension of metrics and differentiation of measures "", General topology and its relations
to modern analysis and algebra, V (Prague, 1981), Sigma Ser. Pure Math., vol. 3, Heldermann,
Berlin, 1983, pp. 565–568. But I cannot find this article anywhere.","['measure-theory', 'metric-spaces', 'geometric-measure-theory', 'real-analysis']"
4757018,Inequality involving the Gagliardo seminorm,"Does the following estimate hold for any value of $p \in [1,+\infty)$ ? $$\int\int_{\mathbb{R}^N \times \mathbb{R}^N} \dfrac{|u(x) - u(y)|^p}{|x - y|^{N + sp}}dxdy \leq 2\left[\left(\int\int_{\mathbb{R}^N \times \{|y| \geq 2|x|\}} \dfrac{|u(x)|^p}{|x - y|^{N + sp}}dxdy\right)^{1/p} + \left(\int\int_{\mathbb{R}^N \times \{|y| \geq 2|x|\}} \dfrac{|u(y)|^p}{|x - y|^{N + sp}}dxdy\right)^{1/p}\right]^{p} + 2\int\int_{\mathbb{R}^N \times \{|y| < 2|x|\}} \dfrac{|u(x) - u(y)|^p}{|x - y|^{N + sp}}dxdy$$ Based on some calculations performed using the traditional inequality $|a + b|^p \leq 2^p(|a|^p + |b|^p)$ , I noticed that the constant $2^p$ appears in the first part of the second inequality expression. This presence of the constant $2^p$ represents a problem in my specific case.","['fractional-sobolev-spaces', 'sobolev-spaces', 'analysis']"
4757031,Number of words where each letter is adjacent to the same letter.,"Let $a_n$ (for $n \geq 2$ ) denote the number of words of length $n$ over a 7-letter alphabet in which each letter is the same as the previous or next one. Find a compact formula for $a_n$ . The conclusion is that each letter must appear in a ""group"" of the same letters of size 2 or more. My first approach was by recursion and I got: $$a_n = 7a_{n-2} + 7a_{n-3},  a_0 = 1, a_1 = 0, a_2 = 7$$ since every string of length $n$ can be created from a string of length $n-2$ by adding two identical letters, or from a string of length $n-3$ by adding three identical letters. We can achieve a group of any length in this way. Each time, we choose the color of the added letters in 7 ways. The second way is to use stars and bars, dividing into elements $\geq 2$ . At the start, we can assign 2 elements to each of the $k$ groups, so we have the standard stars and bars but for $n-2k$ elements: $$\binom{n-2k+k-1}{k-1} = \binom{n-k-1}{k-1}.$$ Then we sum over all possible numbers of groups and assign a color to each group in 7 ways: $$a_n = \sum_{k=1}^{\left\lfloor\frac{n}{2}\right\rfloor}\binom{n-k-1}{k-1} \cdot 7^k.$$ There are at most $\left\lfloor\frac{n}{2}\right\rfloor$ groups. However, first of all, these two solutions give different values, and secondly, I suspect that they count some things more than once, because numbers they give are slightly larger than when manually counting the possibilities. Is any of these formulas good? If not, what could be the way to find the number of these sequences?
Thanks.","['summation', 'combinatorics', 'recurrence-relations', 'discrete-mathematics']"
4757066,Intuition for the inverse Hessian matrix,"Consider a function $f:\mathbb{R}^n\to\mathbb{R}$ and denote by $H$ its Hessian matrix . I understand that $H$ provides a measure of the curvature of $f$ in all directions and plays a role in the Taylor approximation of $f$ . But I keep encountering the inverse of $H$ , and I was wondering if there is some intuition about $H^{-1}$ ? What does an element $H^{-1}_{ij}$ tell us about about the shape of $f$ ? Is there a geometric interpretation? EDIT: Some examples where the inverse Hessian plays a role. For instance, in maximum likelihood estimation the negative of the expected value of the Hessian of the log likelihood function is the Fisher information , and the inverse of that quantity gives us the covariance matrix of the estimated parameter. In Newton's optimization method , the inverse Hessian of the function tells us how big a step to take toward the optimum.","['multivariable-calculus', 'approximation', 'curvature', 'hessian-matrix']"
4757091,Understanding the notion of i.i.d. in the Hilbert Space of Square-Integrable Random Variables,"Let $X_1, \ldots, X_n$ be square-integrable random variables in $L^2(\Omega, \mathcal{F}, \mathbb{P})$ with the usual inner product (and induced norm): $$ \langle X, Y \rangle = \int XY \; d\mathbb{P}$$ I am tring to understand what it means for them to be independent and identically distributed from a ""functional / inner product"" perspective.  For simplicity, assume $\mathbb{E} X_i = \langle X_i, \mathbf{1}_\Omega \rangle = 0$ and $X_i \neq 0$ almost everywhere for each $i$ . If they are identically distributed, this means that the pushforward measure is the same for each $X_i$ .  In other words, $\mathbb{P} \circ X^{-1}_i = \mathbb{P} \circ X^{-1}_j$ , which I think in turn implies that $X_i = X_j$ almost everywhere.  Let's call the common function $X$ . However, if they are independent this implies that they have covariance $0$ , i.e. $$ \langle X_i, X_j \rangle = 0 $$ However, since each $X_i = X$ and $X_i \neq 0$ almost everywhere, this would seem to imply that $$\langle X_i, X_j \rangle = \langle X, X \rangle = \lVert X \rVert^2 > 0 $$ which seems to be a contradiction.  Can someone clarify where my logic is going wrong?","['measure-theory', 'independence', 'hilbert-spaces', 'probability-theory', 'random-variables']"
4757129,Finding all functionals over $\mathbb{N}$ such that $f(f(n)) = an$ for which $a\in\mathbb{Z}^+$,"I want to find all functions $f:\mathbb{N}\rightarrow \mathbb{N}$ for which there is a $a\in\mathbb{Z}^+$ satisfying $$f(f(n)) = an$$ for all $n\in\mathbb{N}$ My attempt:
First, taking the case $a=1$ , we have $f(f(n)) = n$ . I suspect there are probably many or infinitely many family of functions that satisfy this, but I haven't had any luck here. In the second case where $a>1$ , take $f(x) = y$ . Then $f(y) = ay$ . Repeating this composition, it looks like $f(f(a^{k-1}n)) = a^kn$ for every integer $k\geq 1$ . So, proceeding by induction on $k$ , it is clear that this property holds for $k=1$ since, $$f(f(a^0n)) = f(f(n)) = an$$ Suppose that $f(f(a^{k-1}n)) = a^kn$ for some $k>1$ . We want to show that $$f(f(a^kn)) = a^{k+1}n$$ Since $f(f(n)) = an$ , then taking the formula from the induction step we should have \begin{align*}
f(f(a^kn)) &= a \big(a^k n\big) \\
&= a^{k+1}n
\end{align*} Hence $f(f(a^kn)) = a^kn$ satisfies the functional for every integer $k\geq 1$ . Again, I have not been able to find precisely what functions $f$ are a valid solution.","['number-theory', 'algebra-precalculus', 'functional-equations']"
4757136,"Is it true that if a given equivalence relation is closed, then each equivalence class is closed?","Let $X$ be a topological space and $R$ a closed equivalence relation on $X$ . Then is it true that each equivalence class (under the relation $R$ ) is closed subset in $X$ . Here by a closed equivalence relation $R$ , we mean that $R$ is closed subset of $X \times X$ under the product topology.","['equivalence-relations', 'general-topology']"
4757145,What are some obscure radical identities?,"So there are several trigonometric identities, some very well know, such as $\cos(x) = 1 - 2\sin^2(\frac{x}{2})$ and some more obscure like $\tan(\frac{\theta}{2} + \frac{\pi}{4}) = \sec(\theta)+\tan(\theta)$ . You can also find some logarithmic identities online But looking for radical identities (stuff involving square roots), if I google I'm lead to politics and ethnicity (???)... I want to find some cool identities involving square roots that are always true for any $x>0$ . I am aware they exist, I've seen some, but I don't remember them... What are some cool obscure identities involving square roots?","['radicals', 'functions', 'radical-equations']"
4757220,Can I deduce one set distributive law from the other?,"I am asked to prove the distributive laws $$A\cup (B\cap C)=(A\cup B)\cap (A\cup C)$$ $$A\cap (B\cup C)=(A\cap B)\cup (A\cap C)$$ I proved the first one the boring way; I would like to avoid doing the same for the second. Is there some clever way to deduce the second from the first? I would guess that it is possible by using the logical contrapositive of the first statement, but I didn't quite manage to do it.",['elementary-set-theory']
4757279,"Vanishing of the second cohomology module of a pair $(M^*, \partial M^*)$","I am currently reading the paper $\textit{Classification of the actions of the circle on 3-manifolds}$ , by Frank Raymond.
In the proof of Lemma 2, the author enunciates the following fact without a proof:
If $M^*$ is a non-compact surface with boundary, and $U^*$ is a small enough closed collared neighborhood of $\partial M^*$ , then $$\text{H}^2(M^*, U^*;\mathbb{Z}) = 0.$$ The author seems to use the fact that $M^*\setminus U^*$ is non-compact and connected. I have unsuccessfully tried to use the usual tools from a basic Algebraic Topology course for a proof of the claim.
Namely, the Long Exact Sequence in Cohomology for the given pair is $$0\rightarrow \text{H}^0(M^*, U^*)\rightarrow \text{H}^0(M^*)\rightarrow \text{H}^0(U^*)\rightarrow \text{H}^1(M^*, U^*)\rightarrow \text{H}^1(M^*)\xrightarrow{\phi} \text{H}^1(U^*)\xrightarrow{\delta}\text{H}^2(M^*, U^*)\rightarrow \text{H}^2(M^*)\rightarrow \text{H}^2(U^*)\rightarrow 0.$$ The two rightmost terms vanish, since $M^*$ is a non-compact surface and $U^*$ deformation retracts onto a 1-manifold. Thus, since $\delta$ is onto, $\text{H}^2(M^*, U^*) = 0 \iff \delta = 0 \iff \phi $ is onto. Hence, it would suffice to prove that the inclusion $U^*\hookrightarrow{} M^*$ induces a surjection in 1-dimensional cohomology. This seems to have a geometric interpretation in terms of cocycles, or even 1-forms if we consider the de Rham cohomology. The Excision Theorem is not very promising in my opinion, since there does not seem to be an appropriate way to excise a subspace from the pair $(M^*, U^*)$ so that it fundamentally simplifies. A colleague suggested to prove that the pair above has the homotopy type of a 1-dimensional CW pair, but this seems even harder to prove. By itself, this would be a very interesting fact to learn. Another reasonable approach could be to use the fact that $(M^*, U^*)$ is a $\textit{good pair}$ in the sense of Hatcher's book, so that the quotient map $$q:(M^*, U^*)\rightarrow (M^*/U^*, U^*/U^*)$$ induces isomorphisms in homology. However, the non-compact CW space $M^*/U^*\cong M^*/\partial M^*\cong M^*\cup_{\partial M^*} C(\partial M^*)$ can be a little wild if $\partial M^*$ has infinitely many components, where $CX$ denotes the cone over $X$ . I am very curious about the two main approaches I propose, but even the smallest hint towards a proof of the original question is greatly appreciated.","['differential-topology', 'algebraic-topology', 'differential-geometry']"
4757291,Increasing quintic polynomial,"If $$
f(x) = \frac{x^5}{5} + \frac{x^4}{4} + x^3 + \frac{kx^2}{2} + x
$$ is a real valued function, then find the interval for $k$ such that $f(x)$ is increasing for all $x$ . My attempt: For $f'(x) \geq 0$ , I have been able to ascertain that $y=-kx$ needs to be tangent to the curve $$
x^4 + x^3 + 3x^2 + 1.
$$ But then I get a very complicated set of equations, which seem difficult to solve without the help of a computer.","['calculus', 'functions', 'derivatives']"
4757297,Axiomatizing convergent sequences and limits,"The usual exposition of limits of sequences starts with a definition and then derives properties like linearity. I'm curious if, conversely, a reasonable set of these properties characterize the subspace $C_{\text{std}} \subset \mathbf{R}^\mathbf{N}$ of convergent sequences and the map $L_{\text{std}}: C_{\text{std}} \to \mathbf{R}, (a_n) \mapsto \lim_{n \to \infty} a_n$ . Here is an example list of properties (when I say equivalently, I mean equivalent assuming the previous conditions): $C$ is a vector subspace and $L: C \to \mathbf{R}$ is $\mathbf{R}$ -linear. $C$ contains all monotone bounded sequences. If $a$ is eventually the constant $a_\infty$ , ie $a_n = a_\infty$ for large $n$ , then $La = a_\infty$ (equivalently, $L$ vanishes on the subspace of eventually $0$ sequences and takes the constant $1$ sequence to $1$ ). If $a \in (\mathbf{R}_{\ge 0})^\mathbf{N} \cap C$ then $La \ge 0$ (equivalently $L$ is monotonic). If $a, b \in C$ then $ab := (a_n b_n) \in C$ and $L(ab) = (L a)(L b)$ . For $a \in C$ every subsequence $a'$ of $a$ is also in $C$ and $La = La'$ . But all of these are also satisfied by $$C = \operatorname{span}\{a \text{ monotone bounded}\} = \{ a - b \mid a, b \text{ increasing and bounded} \}$$ (see https://math.stackexchange.com/a/4377050/32766 for an alternate characterization) and $L = L_{\text{std}}|C$ . So, is there a similar list of ""well-known"" properties of $(C_{\text{std}}, L_{\text{std}})$ that implies $C \supseteq C_{\text{std}}$ and $L$ is an extension of $L_{\text{std}}$ ? The implication should of course be ""non-trivial"", ie the list should not just include a definition of $\lim$ . Given such a list, we could define $(L_{\text{std}}, C_{\text{std}})$ as the minimal pair satisfying that list. Note that (1)-(4) are enough to show that $L$ restricted to $C \cap C_{\text{std}}$ agrees with $L_{\text{std}}$ : if $a_\infty = \lim a_n$ then $a_n$ is eventually bounded between $a_\infty - \epsilon$ and $a_\infty + \epsilon$ and hence so is $La$ . We don't even need all of (2), just that eventually constant sequences are in $C$ . (1), (3), (4) and (6) also imply that $C \subseteq C_{\text{std}}$ . First note that if $a \in C$ then it has to be bounded, otherwise for each $N > 0$ either $a$ or $-a$ has a subsequence above $N$ , so $|La| \ge N$ by monotonicity. Now if $a \in C$ is not convergent then it must have convergent subsequences with distinct limits, which forces a contradiction between (6) and that $L$ agrees with $L_{\text{std}}$ on $C \cap C_{\text{std}}$ , as shown above. However I'd be generally interested if an answer provides a list that allows for $C \supsetneq C_{\text{std}}$ . Remarks: (1), (3) and (5) can be combined to: $C$ is a unital subalgebra of $\mathbf{R}^\mathbf{N}$ containing the ideal $C_0$ of eventually $0$ sequences and $L: C \to C/C_0 \to \mathbf{R}$ is a unital algebra homomorphism. It is plausible to me that (4) is implied by the other conditions. It could also be replaced by the (not obviously equivalent) condition $a \in C\cap (\mathbf{R}_{\ge 0})^{\mathbf{N}} \implies (\sqrt{a_n}) \in C$ .","['limits', 'sequences-and-series', 'real-analysis']"
4757318,What is the difference/relationship between the gradient and the Jacobian?,"What is the difference/relationship between the gradient and the Jacobian? I think it has to do with vectors/covectors, tangent spaces / contangent spaces, but I'm not sure what's going on. (A gentle and readable but thorough reference in addition to an answer would be appreciated too.) Let $R$ be the real numbers. Let $f: R^2 \longrightarrow R$ be a smooth function. Let $p \in R^2$ . (The Jacobian is typically defined for (differentiable?) functions $f: R^n \longrightarrow R^m$ , but just set $m$ to 1.) Now the gradient is a vector field, whose value at $p$ is a (column) vector (in particular, not a covector): $$\nabla f(p) := \left[{\partial f \over \partial x_1}(p) \ \ {\partial f \over \partial x_2}(p)\right]^T$$ (with the convention that vectors are column vectors and covectors are row vectors). And the Jacobian $Jf(p)$ is the $1 \times 2$ matrix (or just the row vector, ie. covector? what's the rigorous difference?): $$Jf(p) := \left[\left[{\partial f \over \partial x_1}(p) \ \ {\partial f \over \partial x_2}(p)\right]\right].$$ In particular, since the gradient and the Jacobian are just ""transposes"" (duals) of each other, when would you use one or the other? What's the point? There are related questions here , here , here , but I'm hoping for more insights.","['multivariable-calculus', 'calculus', 'differential-forms', 'differential-geometry']"
4757337,Root space decomposition of Lie subgroups,"Let $\varphi \colon G \to \tilde{G}$ be a group homomorphism between two real semisimple Lie groups. For example, $\varphi$ could be an inclusion of a subgroup $G \subseteq \tilde{G}$ . Let $\mathfrak{g}, \tilde{\mathfrak{g}}$ denote the Lie algebras with Cartan decompositions $\mathfrak{g} = \mathfrak{k} \oplus \mathfrak{p} , \tilde{\mathfrak{g}} = \tilde{\mathfrak{k}} \oplus \tilde{\mathfrak{p}}$ . Let $\mathfrak{a}, \tilde{\mathfrak{a}}$ be maximal abelian subspaces of $\mathfrak{p}, \tilde{\mathfrak{p}}$ . Then we have root space decompositions \begin{align}
\mathfrak{g} &= \mathfrak{g}_0 \bigoplus_{\alpha \in \Sigma} \mathfrak{g}_\alpha \\
\tilde{\mathfrak{g}} &= \tilde{\mathfrak{g}}_0 \bigoplus_{\alpha \in \tilde{\Sigma}} \tilde{\mathfrak{g}}_\alpha \\
\end{align} Let us assume that $G$ and $\tilde{G}$ are compatible, in the sense that $D_0\varphi(\mathfrak{p}) \subseteq \tilde{\mathfrak{p}}$ and $D_0\varphi(\mathfrak{a}) \subseteq \tilde{\mathfrak{a}}$ . What can we say about the root systems $\Sigma, \tilde{\Sigma}$ ? Concretely, this amounts to: Do we have $D_0\varphi \mathfrak{g}_0 = \tilde{\mathfrak{g}}_0 \cap D_0\varphi (\mathfrak{g})$ ? Is there a relation between $\Sigma$ and $\tilde{\Sigma}$ ? Do we get a group homomorphism between the Weyl groups $W \to \tilde{W}$ ? Looking at the hyperplanes $M_\alpha = \{H \in \mathfrak{a} \colon \alpha(H) = 0\}$ , $\tilde{M}_\alpha = \{H \in \tilde{\mathfrak{a}} \colon \alpha(H) = 0\}$ is it true that $D_0\varphi^{-1}\tilde{M}_\alpha \cap \mathfrak{a}$ is always a hyperplane $M_\beta$ for some $\beta \in \Sigma$ or all of $\mathfrak{a}$ ? Do any of the above questions hold when $\varphi$ is just an inclusion? Or a surjection? Experiments on representations $\operatorname{SL}(2,\mathbb{R}) \subseteq \operatorname{SL}(n,\mathbb{R})$ and $\operatorname{Sp}(2\cdot 2, \mathbb{R}) \subseteq \operatorname{SL}(4,\mathbb{R})$ seem to suggest that questions 1, 3 and 4 have a positive answer.","['lie-algebras', 'root-systems', 'semisimple-lie-algebras', 'group-theory', 'lie-groups']"
4757359,Smallest eigenvalue of random walk on a graph,"Say $A$ is an $n \times n$ adjacency matrix for a connected undirected graph with all diagonals 1 (i.e. all self loops are present). I consider the uniform random walk on the graph according to transition matrix $D^{-1}A$ . The rate of convergence to equilibrium depends on $\lambda_* = \max(|\lambda_2|,|\lambda_n|)$ where $\lambda_i \geq \lambda_{i+1}$ are the eigenvalues of $D^{-1}A$ , so I am interested in finding under what conditions $\lambda_* = \lambda_2$ . Intuitively it feels like this should be the case in a ""typical"" example of $A$ , since $\lambda_n$ near -1 would mean near-periodicity whereas $\lambda_2$ near 1 is the more likely case of near-unconnectedness, if I understand correctly. However I haven't been able to find much conrete analysis of when $\lambda_2 = \lambda_*$ - are there any well known results?","['graph-theory', 'markov-chains', 'probability']"
4757360,Prove that $\sigma(\mathcal{C_{1}})=\sigma \left \{ \bigcup_{\alpha\in \Gamma_{1}} X_{\alpha}^{-1}\left ( \mathcal{B(\mathbb{R})} \right ) \right \}.$,"Let $\left(\Omega,\mathcal{F},P\right)$ be a probability space and let $\left\{X_\alpha:\alpha\in \Gamma_{1}\right\}$ be a collection of random variables on $\left(\Omega,\mathcal{F},P\right)$ and $\left\{Y_\beta:\beta\in \Gamma_{2}\right\}$ be another collection of random variables on $\left(\Omega,\mathcal{F},P\right)$ where both $\Gamma_{1}$ and $\Gamma_{2}$ are nonempty set. Whether we have the following conclusion: $\sigma \left \{ \bigcup_{\alpha\in \Gamma_{1}} X_{\alpha}^{-1}\left ( \mathcal{B(\mathbb{R})} \right ) \right \}$ and $\sigma \left \{ \bigcup_{\beta\in \Gamma_{2}} Y_{\beta}^{-1}\left ( \mathcal{B(\mathbb{R})} \right ) \right \}$ are independent w.r.t. $P$ $\Leftrightarrow $ For each finite subset of $\Gamma_{1}$ , $\mathcal{I}$ and each finite subset of $\Gamma_{2}$ , $\mathcal{J},$ $\sigma \left \{ \bigcup_{i\in\mathcal{I} } X_{i}^{-1}\left ( \mathcal{B(\mathbb{R})} \right ) \right \}$ and $\sigma \left \{ \bigcup_{j\in\mathcal{J} } Y_{j}^{-1}\left ( \mathcal{B(\mathbb{R})} \right ) \right \}$ are independent w.r.t. $P$ . "" $\Rightarrow $ "" is obviously. "" $\Leftarrow $ "" Let $$\mathcal{C_{1}}=\left\{A:A\in\sigma\left(\bigcup_{i\in \mathcal{I}} X_{i}^{-1}\left ( \mathcal{B(\mathbb{R})} \right)\right),\mathcal{I}\text { is any finite subset of  }\Gamma_{1}\right\};$$ $$\mathcal{C_{2}}=\left\{B:B\in\sigma\left(\bigcup_{j\in \mathcal{J}} Y_{j}^{-1}\left ( \mathcal{B(\mathbb{R})} \right)\right),\mathcal{J}\text { is any finite subset of  }\Gamma_{2}\right\}.$$ Since $\mathcal{C_{1}}$ and $\mathcal{C_{2}}$ are $\pi$ -systems,also $\mathcal{C_{1}}$ and $\mathcal{C_{2}}$ are independent w.r.t. $P$ , we have $\sigma(\mathcal{C_{1}})$ and $\sigma(\mathcal{C_{2}})$ are independent w.r.t. $P$ .
If we can prove that $$\sigma(\mathcal{C_{1}})=\sigma \left \{ \bigcup_{\alpha\in \Gamma_{1}} X_{\alpha}^{-1}\left ( \mathcal{B(\mathbb{R})} \right ) \right \}\text{ and }\sigma(\mathcal{C_{2}})=\sigma \left \{ \bigcup_{\beta\in \Gamma_{2}} Y_{\beta}^{-1}\left ( \mathcal{B(\mathbb{R})} \right ) \right \},$$ then the equivalence holds. The next question is how to prove that $\sigma(\mathcal{C_{1}})=\sigma \left \{ \bigcup_{\alpha\in \Gamma_{1}} X_{\alpha}^{-1}\left ( \mathcal{B(\mathbb{R})} \right ) \right \}.$","['measure-theory', 'probability-theory', 'real-analysis']"
4757382,Why does a covariance matrix have to be positive semi definite on an intuitive level,"If I have a covariance matrix of some random vector $X$ with expectation $\mu \in \mathbb{R}^n$ it is not that difficult to show that its covariance matrix is positive semi definite; given $$
Cov(X) = \mathbb{E}[(X-\mu) (X-\mu)^T]
$$ For any vector $z\in \mathbb{R}^n$ we have $$
z^T Cov(X) z = z^T \mathbb{E}[(X-\mu) (X-\mu)^T] z =  \mathbb{E}[z^T(X-\mu) (X-\mu)^T z]
$$ which is just the inner product squared $$
\mathbb{E}[z^T(X-\mu) (X-\mu)^T z] = \mathbb{E}[<z,(X-\mu)>^2] \geq 0
$$ and hence always greater than or equal $0$ . I don't really understand however why a non PSD cannot function as a covariance matrix on an intuitive level.
Suppose you have a non-PSD matrix. Can you prove by contradiction that it can not be the covariance matrix of some random vector X?","['statistics', 'covariance', 'probability']"
4757385,Is it possible to determine sides in a triangle given angle of median and opposite side length?,"Questions In the triangle $ABC$ below we know the values of $\alpha$ , $\beta$ and $\vert BD\vert$ in addition to knowing that AD is the median in the triangle, i.e. $\vert BD\vert = \vert DC\vert$ . Is it possible to analytically determine the other side lengths, i.e. $\vert AB\vert$ , $\vert AD \vert$ and $\vert AC \vert$ ? If no, why not? Is the solution ambiguous? My Approach Since none of the triangles have enough information on their own, I have tried to utilize that the triangles $ABD$ and $ADC$ have the same areas to give me some insigts about the relationship between them: \begin{align*}
\frac{1}{2} \vert AB \vert \vert AD\vert \sin(\alpha) = \frac{1}{2} \vert AD \vert \vert AC\vert \sin(\beta) 
\end{align*} simplifying to \begin{align} \vert AB \vert \sin(\alpha) =\vert AC\vert \sin(\beta) \qquad(1), \end{align} or equivalently $$\frac{\sin(\alpha)}{\sin(\beta)} = \frac{\vert AB\vert}{\vert AC\vert}\qquad (2).$$ This is convenient, since I can compute the ratio on the left hand side. To gain more information about $\vert AB\vert$ and $\vert AC\vert$ , I derive and expression for each of them using the sinus theorem on the triangles ABD and ABC, resulting in the two equations: \begin{align}
\vert BD \vert &= \frac{\sin(\alpha)}{\sin(\delta)}\vert AB\vert \\
\vert BD \vert &= \frac{\sin(\beta)}{\sin(\zeta)}\vert AC\vert 
\end{align} or in combination \begin{align}
\frac{\vert AB\vert}{\vert AC \vert} = \frac{\sin(\beta)\sin(\delta)}{\sin(\alpha)\sin(\zeta)},
\end{align} which can be substituted into Eq. (2) resulting in the relation \begin{align}
\left(\frac{\sin(\alpha)}{\sin(\beta)} \right)^2=\frac{\sin(\delta)}{\sin(\zeta)},
\end{align} or, when substituting $\delta = \pi - \zeta$ \begin{align}
\left(\frac{\sin(\alpha)}{\sin(\beta)} \right)^2=\frac{\sin(\pi - \zeta)}{\sin(\zeta)}.\qquad (3)
\end{align} So when I go down this path my question becomes: Can I determine $\zeta$ analytically from Eq. (3)?","['trigonometry', 'geometry', 'median']"
4757390,Full subcategories of $\textbf{Grp}$ that are monoidal closed,"Motivation: One of the reason that the category of abelian groups is nice is that, unlike the category of groups, it has a monoidal closed structure. Are there other full subcategories of $\textbf{Grp}$ that aren't a full subcategory of $\textbf{Ab}$ and admit a monoidal closed structure? Is a classification of those subcategories known?","['group-theory', 'category-theory']"
4757417,Convergence of random vectors to vector with independent marginals when marginal distribution function converges at points of continuity,"Let $\xi_n, \xi, \eta_n, \eta$ be random vectors in $\mathbb{R}^{d}$ such that $\xi$ and $\eta$ are independent, $\xi_n \overset{d}{\to} \xi$ and $\eta_n \overset{d}{\to} \eta$ . Denote by $C(F_{\xi})$ the set of points of continuity of distribution function $F_{\xi}$ of $\xi$ . Let $C(F_{\eta})$ have an obvious meaning. Prove of disprove that if $$\mathbb{P} \Big( (\xi_n, \eta_n) \in (-\infty,x]\times (-\infty,y] \Big) \to \mathbb{P} \Big( (\xi, \eta) \in (-\infty,x]\times (-\infty,y] \Big) \quad (*)$$ for all $x \in C(F_{\xi})$ and $y \in C(F_{\eta})$ then $(\xi_n, \eta_n) \overset{d}{\to}  (\xi, \eta)$ . We suppose that $z \le x$ iff $z_i \le x_i$ for all $1 \le i \le d$ . I suppose that the answer is positive, but I'm not sure that my proof is correct and moreover, it looks that there should be a more simple proof. My attempt to make a simple proof (not successful). We know that $\xi_n \overset{d}{\to} \xi$ so $$\mathbb{P} \Big( \xi_n \in (-\infty,x] \Big) \to \mathbb{P} \Big( \xi \in (-\infty,x] \Big)$$ for all $x \in C(F_{\xi})$ .
Hypothesis: if $(x,y) \in C(F_{(\xi,\eta)})$ then $x \in C(F_{\xi})$ and $y \in C(F_{\eta})$ . If the hypothesis is true then it follows from (*) that the answer in the original problem is positive.
Unfortunatelly the hypothesis is not true: counterexample is $\xi_n = \xi = 1, \eta_n = \eta = 0$ , $x = y = 0$ . So it's not a solution. Addition Theorem 2.2 from Billingsley, ""Convergence of probability measures"" looks like the answer, if I am right, because we can take $$\mathcal{U} = \{ (a,b] \times (c,d] | (P(\xi = a) = P(\xi = b) = 0 = P(\eta = c) = P(\eta = d) = 0)  \},$$ then $\mathcal{U}$ is a $\pi$ -system and each open set in $\mathbb{R}^d \times \mathbb{R}^d$ is a countable union of sets from $\mathcal{U}$ so if $$\mathbb{P} \Big( (\xi_n, \eta_n) \in (a,b]\times (c,d] \Big) \to \mathbb{P} \Big( (\xi, \eta) \in (a,b]\times (c,d] \Big) \quad (**)$$ for all $(a,b]\times (c,d] \in U$ then $(*)$ is true. Hence it's sufficient to prove $(**)$ but it looks like it follows immediately from $(*)$ . Am I right? And is there a more simple proof? Some more facts about problem. If $\xi_n$ and $\eta_n$ are independent then the answer in the original problem is positive. Moreover, $(x,y) \in C(F_{(\xi,\eta)})$ iff $\mathbb{P} \Big( (\xi,\eta) \in \partial((-\infty,x] \times (-\infty,y]) \Big) = 0$ , where $\partial X$ is a boundary of $X$ . Hence if $(x,y) \in C(F_{(\xi,\eta)})$ then either $x \in C(F_{\xi})$ or $y \in C(F_{\eta})$ . Billingsley, ""Convergence of probability measures"", doesn't contain the theorem in the form I need. Any help is appreciated.","['probability-limit-theorems', 'probability-distributions', 'limits', 'convergence-divergence', 'probability-theory']"
4757438,"Showing sequence of trigonometric integrals is positive, decreasing","For integer $n\geq 0$ , consider the integral $$I_n := 2\int_{-\infty}^{\infty}\cos^n(x)\sin(x)xe^{-x^2}dx.$$ Show that each $I_n>0$ and $I_{n+1}< I_n$ . I believe this should be true based on computing values of $n$ in Mathematica, but I do not know how to prove this. For $n=0$ , it's straightforward from integration by parts and using the Fourier transform of $e^{-x^2}$ . For $n=1$ , one can write $\cos(x)\sin(x)=\frac12\sin(2x)$ and use integration by parts again plus the the Fourier transform of $e^{-x^2}$ . If it helps, one can use integration by parts and trig identities to rewrite $$I_n = (n+1)\int_{-\infty}^{\infty}\cos^{n+1}(x)e^{-x^2}dx - n\int_{-\infty}^{\infty}\cos^{n-1}(x)e^{-x^2}dx. $$","['integration', 'calculus', 'trigonometric-integrals']"
4757444,Dimension formula/algorithm for quiver varieties?,"The title says the question. For a quiver and a dimension vector and a stability vector, we can construct a moduli space of semistable quiver representations with the given dimension vector. The question is about the dimension of this moduli space. We can consider over $\mathbb{C}$ . A quick search discovers nothing relevant. I reckon this an open problem for a general quiver and dimension vector. So I also want to know anything results for special quivers.","['quiver', 'geometric-invariant-theory', 'algebraic-geometry', 'combinatorics']"
