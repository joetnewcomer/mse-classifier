question_id,title,body,tags
2533257,Suppose $f$ is continuous on R such that $\lim_{h\to 0} \frac{f(x+h)-f(x-h)}{h} = 0$ for all $x\in\mathbb R$. Prove that $f$ is constant. [duplicate],"This question already has an answer here : Are constants the only continuous functions with ""symmetric derivative"" zero? (1 answer) Closed 6 years ago . I'm having a bit of trouble with the title problem out of Davidson and Donsig's Real Analysis. I'll state it again: Suppose $f$ is continuous on $\mathbb{R}$ such that $$\lim_{h\to 0} \frac{f(x+h)-f(x-h)}{h} = 0\  \forall x\in\mathbb{R}.$$ Prove that $f$ is constant. They provide the following hint which I have been trying to apply. HINT: Fix $\epsilon > 0$. For each $x$, find a $\delta > 0$ so that $|f(x+h)-f(x-h)| \leq \epsilon h$ for $0\leq h \leq \delta$. Let $\Delta$ be the supremum of all such $\delta$. Show that $\Delta = \infty$. Here's how I've started. Fix $\epsilon > 0,\ x\in\mathbb{R}$. By the definition of the limit, 
$$(\forall x\in\mathbb{R})(\forall\epsilon>0)(\exists\delta>0)(0<|h|\leq\delta\implies\Bigg|\frac{f(x+h)-f(x-h)}{h}\Bigg|<\epsilon).$$ Therefore, we immediately get a $\delta>0$ for our $x,\epsilon$ such that $$|f(x+h)-f(x-h)|\leq\epsilon h,\ 0\leq h \leq\delta.$$ I don't know how to proceed from here. I'm not even sure conceptually how showing that $\Delta=\infty$ would give us that $f$ is constant. Any help would be appreciated!","['continuity', 'real-analysis', 'limits']"
2533270,prove that $2\sqrt5 +\sqrt{11}$ is irrational,"how would you prove that $2\sqrt5 +\sqrt{11}$ is irrational?
I started with a proof by contradiction that assumes that $2\sqrt5 +\sqrt{11}$ is rational and therefore there exist integers $a$ and $b$ such that $\frac{a}{b}=2\sqrt5 +\sqrt{11}$ and squaring both sides yields
$\frac{a^2}{b^2}=31 +4\sqrt5\sqrt{11}$ and from this point im stuck as i dont know how to continue to arrive at a contradiction.","['algebra-precalculus', 'proof-writing', 'rational-numbers', 'irrational-numbers']"
2533273,Does chain rule enable you yo calculat derivative of $|x^2|$ at x = 0,"The question states as following: 
""Does the Chain Rules anble you to calculate the derivates of |x^2| and |x|^2 at x=0? Do these functoins have derivatives at x = 0? why?"" So using the chain rule for both of these I got two different answer. For $|x|^2$ I got $f'(x)= 2|x|*\frac {x}{|x|}$ i.e $2x$ For $|x^2|$ using the chain rule I got $f'(x)=\frac {2x}{|x|}$ Two very different answers. And and only one of these derivatives is differentiable at x = 0. Which I know is strange since I know for $y = x^2$ => $y' = 2x$ $y'$ is differentiable at x = 0. So how am I not able to reach the same conclusion using the chainrule? Thank you in advance!","['derivatives', 'chain-rule']"
2533280,Poisson random variable is not sub gaussian,"I am reading a chapter on concentration inequalities, and I am struggling to make connections on sub-Gaussian random variables.  A random variable is sub-Gaussian if there exists $C > 0$ such that $P(|X| \geq t) \leq 2\exp(-t^2/C^2)$.  The text gives numerous equivalent conditions to this one. I have an exercise:  If $X \sim Poi(\lambda)$, show that $X$ is not sub-Gaussian. My setup is $P(|X| \geq t) = P(X \geq t) = \sum_{k=t}^{\infty} \frac{e^{-\lambda}\lambda^{k}}{k!}$ since the Poisson random variable is supported on nonnegative integers.  My first thought was to use Stirling's formula on the $k!$, but I wouldn't know how to sum the resulting series.  I appreciate any help on this question!",['probability']
2533282,Finding an expression for $\prod_{x=1}^n(1-f(x))$,"Question: Say I have $\prod_{x=1}^n(1-f(x))$ where $1 > f(x) > 0$. I am looking to find a way to express this product using $f(x)$. Expanding gives me $1-\sum(\text{odd pairs})+\sum(\text{even pairs})$, however I am not sure how to reduce further. Context: For the case:
$$f(x)=\frac{1}{\sqrt{x^2-3x+2a+\frac{1}{4}}}$$
this product gives the probability that $a$ isn't eliminated by any $x<a$, related to my question here . Any advice/pointers to useful topics are appreciated!","['statistics', 'elementary-number-theory']"
2533294,Approximation of sums with integrals,"Consider a finite sum of a function $f(x)$ over discrete values of $x$. 
$$S=\sum_{x=a}^{b} f(x)\tag{1}$$ Now suppose that, instead of having only certain values of $i$, this variable can vary continuously in the interval $[a,b]$, i.e.  $x \in [a,b] \subset \mathbb{R}$. In many occasion, studying physics mainly, I read on textbooks that such sum can be in that case ""approximated"" with an integral. Nevertheless I'm quite sure that I cannot write $$S \approx \int_{a}^b f(x) dx\tag{2}$$ Since this would not work dimensionally. I could imagine to divide by a constant factor for dimensions but I don't think it would work either, because the integral of a function itself is proportional to its integral average on the interval $[a,b]$, which I do not think can be proportional to the ""sum"" $S$. So I can imagine to divide $[a,b]$ in sub-intervals and creating a density function $\frac{dn}{dx}$ which sholud give the ""relative number of allowed values of $x$ in the interval $dx$"" and then finally $$S \approx \int_{a}^b  \frac{dn}{dx} f(x) dx\tag{3}$$ But again if there are no reason for $x$ not to be spread uniformly I should set $\frac{dn}{dx}=constant$ and I would get the same of above. So what integral should I think of when I read on a physics book that a discrete sum like $(1)$ can be subsituted by an integral if the variable is allowed to vary continously?","['summation', 'integration', 'physics', 'approximation']"
2533342,Geometric reason why conjugation by an element in $B_3$ inverts this element?,"Let $B_3$ be the braid group on three strands. I was looking at an element in $B_3$, which I will write in the standard presentation: $$(\sigma_2\sigma_1\sigma_2)^{-1}\sigma_1^3\sigma_2^{-3}(\sigma_2\sigma_1\sigma_2)$$ and I was able to explicitly show it equal to $\sigma_2^{3}\sigma_1^{-3}$, which inverts the element by conjugation. I was wondering if  one see this geometrically? (Via some diagram) Or rather, if there is some phenomenon that explains this, or if it is a mere coincedence. If the following is known: what do inner automorphisms of $B_3$ look like in general? Maybe the semi direct product presentation is more promising for understanding it.","['braid-groups', 'group-theory']"
2533360,Show that the set $\mathbb{N}\times\mathbb{N}\times\mathbb{N}$ is countable infinite,"I've proven that $f:\mathbb{N}\times\mathbb{N}\to\mathbb{N}$ given by $f(m,n)=2^{m-1}(2n-1)$ is a bijection. How do I show that the function $g:\mathbb{N}\times\mathbb{N}\times\mathbb{N}\to\mathbb{N}$ given by $g(k,m,n)=f(k,f(m,n))$ is a bijection by writing it as a composition of functions $\mathbb{N}\times\mathbb{N}\times\mathbb{N}\to\mathbb{N}\times\mathbb{N}\to\mathbb{N}$? Then do I conclude with the fact the $\mathbb{N}\times\mathbb{N}$ and $\mathbb{N}$ is denumerable a composition of such would be denumerable and thus $\mathbb{N}\times\mathbb{N}\times\mathbb{N}$ is?",['discrete-mathematics']
2533367,"Toward Explicit Formula from Recursion : Is Generating function ""the only"" answer?","I am trying to draw the explicit formula of $S_n$ that is defined as below: $S_n$ is the number of words of length n using 0,1 and 2 such that no two consecutive 0's occur. Myself, as I had learned from the basic skills in combinatorics, just easily get to the point constructing recursive relation such that: $$S_n = 2S_{n-1} + 2S_{n-2}$$ since $S_n$ splits up to disjoint two cases: one with no 0 at the last posit, and always 0 at the last locus, then there exist bijection between the former one and 1 or 2 at the n-th posit multiplied with $S_{n-1} $ cases, and also another bijection between the latter one and 1 or 2 at the n-1-th posit multiplied with $S_{n-2}$. So If my given recursion is correct, next step is how could I go further into the formulating with what. I superficially knows the concept of $OGF$, and $EGF$, and their formal definition. Generating function contains its sequential information in a form of coefficients with a corresponding polynomial degrees as an index (as far as I understand). Now if I define $S_n$ a functional form, $s(n)$, generating function would be : $$g(x) = \sum_{n=0}^{\infty}s(n)x^n$$ Then let's little bit refer to a few terms of $S_n$: $$1, 3, 8, 22, ...$$ And revise the $g(x)$: $$g(x) = \sum_{n=0}^{\infty}s(n)x^n =\sum_{n=2}^{\infty}s(n)x^n+3x+1=2\sum_{n=2}^{\infty}s(n-1)x^n+2\sum_{n=2}^{\infty}s(n-2)x^n+3x+1 $$ Now, it looks like the problem has been changed into solve the functional equation(I am not sure this is right term) 1) What should be the next step? 2) Is the generating function the only approach toward the explicit formula?","['generating-functions', 'combinatorics', 'recursion', 'recurrence-relations']"
2533369,Derivative of $\sin^{-1}(x)$,"I can find this using the fact that $\sin(\sin^{-1}(x)) = x$, for all $x\in[-1,1].$ Now, differentiate. $$\frac{d}{d\sin^{-1}(x)}\sin(\sin^{-1}(x))\cdot \frac{d}{dx} \sin^{-1}(x)= \frac{d}{dx} x= 1$$ $$\cos(\sin^{-1}(x))\cdot \frac{d}{dx} \sin^{-1}(x) = 1$$ $$\frac{d}{dx} \sin^{-1}(x) = \frac{1}{\cos(\sin^{-1}(x))}$$ $$\frac{d}{dx} \sin^{-1}(x) = \frac{1}{\sqrt{1-\sin^2(\sin^{-1}(x))}}$$ $$\frac{d}{dx} \sin^{-1}(x) = \frac{1}{\sqrt{1-x^2}}$$ However, what if I wanted to differentiate this like $\ \sin^{-1}(\sin(x))$ without knowing the fact that $\ \frac{d}{dx}\sin^{-1}(x) = \frac{1}{\sqrt{1-x^2}}$ ? Is there a solution for it? I keep getting stuck at a certain step when I try this...","['derivatives', 'trigonometry']"
2533377,"Given $f(x)=\frac{\sin \pi x}{\pi\sin x}$ and $f'(x_0)=0$,find value of $(f(x_0))^2(1+(\pi^2-1)\sin^2x_0)$","Let $$f(x)=\frac{\sin (\pi x)}{\pi\sin x}$$ and let $x_0\in(0,\pi)$ such that $f'(x_0)=0$
  Then find the value of
  $$(f(x_0))^2(1+(\pi^2-1)\sin^2x_0)$$ My attempt:
$$f'(x_0)=\lim_{x \to x_0}\frac{f(x)-f(x_0)}{x-x_0}$$
$$=\lim_{x\to x_0}\frac{\frac{\sin (\pi x)}{\pi \sin x}-\frac{\sin (\pi x_0)}{\pi \sin x_0}}{x-x_0}$$
$$\lim_{x \to x_0}\frac{\sin(\pi x)\sin x_0-\sin (\pi x_0)\sin x}{\pi \sin x\sin x_0(x-x_0)}$$
Applying L'Hopital's rule and then subsituting the value of limit,I got $$\frac{\pi\cos (\pi x_0)\sin x_0-\cos x_0\sin(\pi x_0)}{\pi\sin^2 x_0}=0$$
How will I make into the form asked in the question?","['derivatives', 'calculus']"
2533422,Who has to buy the beer?,"There are $115$ members in a team in which everyone has a smart phone. No member of the team wishes to buy beer for every other member of the team. But one Friday, after the work hours, everyone expresses the desire to have beer. The team leader proposes a game, which everyone accepts to play. The game is that one team member would send a message "" Beer "" to one of the other team members, who has to forward it to some one other than the person from whom he/she received the message. At the end of the game whoever is the last person to be with the message has to buy beer for everyone else. Someone arbitrarily sets a time-limit for the game. The team leader starts the game by sending the message to someone of his/her team. Assuming that anyone in the team could send message to anyone without any difficulty, what is the probability that at the end of the game, when $2009$ messages have been passed, it's the team-leader who is found to be the one with the last message, and hence, the one to buy beer for everyone else? So, there are $115$ vertices and there is an edge from one to every other vertex. The graph is, thus, $114-$ regular and simple. So, the problem is actually this: Can there be a path starting from a vertex $u$ and after having traversed $2009$ edges, such that one can not leave a vertex by the same edge one came from, culminating back to $u$? Since, the path $u-u$ must contain some cycle, which can be of length $3$ to $115$,the question is does there exist a non-negative solution of: $2x_1+3x_2+...+115x_{114}=2009$ such that such a $u-u$ path exists? For instance, $x_1=1000$ and $x_2=3$ can not be a solution as it would imply that one took the edge $v-v$, $v\in V(G)$, consecutively which is not allowed. How do we solve this?","['graph-theory', 'probability', 'discrete-mathematics']"
2533458,"To check if $f_x$ and $f$ are bounded for $f(x,y)=\frac{x^2y}{x^2+y^2}$","Let $$f(x,y)=\frac{x^2y}{x^2+y^2}$$ for$(x,y) \ne(0,0)$ I want to check if $f_x$ and $f$ are bounded I calculated $f_x=\frac{2xy^3}{(x^2+y^2)^2}$ To check if its bounded below or above,do I have to calculate limits? If so,at which points?","['multivariable-calculus', 'limits']"
2533474,Is the multiplicity of an eigenvalue equal to the dimension of its associated eigenspace?,"This is a silly question, but if someone could provide a short proof as to why if it is true, or a counterexample and short explanation if it's false I would appreciate it. EDIT: By multiplicity I mean, when solving the characteristic polynomial, the roots (i.e the eigenvalues) can be repeated, so I was referring to the algebraic multiplicity.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2533533,Does the boundary of metric balls have measure $0$ in a metric measure space with Radon measure?,"Let $(X,d,m)$ be a metric measure space such that $m$ is a Radon measure and let $\overline{B}(x,r):=\{y\in X \mid d(x,y)\leq r\}$. Is it true that the ""boundary"" of $\overline{B}$ (meaning the set $\{y\in X \mid d(x,y)=r\}$) has $m$ measure $0$?","['metric-spaces', 'measure-theory']"
2533570,Dominated convergence theorem to show $F$ is integrable.,"$(\Omega,\mathscr{F},\mu)$ measure space. We have a map $f : \Omega \times \mathbb{R} \rightarrow \mathbb{R}$ with these three properties: a) For all $x \in \mathbb{R}$ :  $f(x,\star) : \Omega \rightarrow \mathbb{R}$ is integrable. b) For all $\omega \in \Omega$: $f(\star, \omega) : \mathbb{R}\rightarrow \mathbb{R}$ is differentiable. c) There exists an integrable function $g$, so that for all $x \in \mathbb{R}$ : $|\frac{\partial}{\partial x} f(x, \star) | \le g$ $\mu$-almost everywhere. Show that: $F: \mathbb{R}\rightarrow \mathbb{R}$  is differentiable. $$F(x) := \int_{\Omega} f(x,\omega) d\mu(\omega).$$ And show that for all $x \in \mathbb{R}$: $$F '(x) = \int_{\Omega} \frac{\partial}{\partial x} f(x,\omega) d\mu(\omega)$$ So what I have tried: I'm pretty sure, that we have to use the dominated convergence theorem for Lebesgue integration. First of all we could start with $\lim_{x \rightarrow x_o} \frac{F(x)-F(x_o)}{x-x_o}$. Then we get $\lim_{x \rightarrow x_o} \frac{\int_{\Omega} f(x,\omega) d\mu(\omega) - \int_{\Omega} f(x_o,\omega) d\mu(\omega)}{x-x_o}$.  But what do I now? Maybe:  $\lim_{x \rightarrow x_o} \int_{\Omega} \frac{  f(x,\omega) dµ(\omega) - f(x_o,\omega) d\mu(\omega)}{x-x_0}$. Is this right? Could I use now the dominated convergence theorem? Then I could use the fact that $f$ is differentiable? But I'm not sure. Thank you for your reply.","['derivatives', 'integration', 'lebesgue-integral']"
2533583,If $\lambda$ has algebraic multiplicity $m$ has $\lambda^k$ the same multiplicity?,Let $A\in M_n(\mathbb{C})$ and $\lambda$ a eigenvalue of $A$ with the algebraic multiplicity equal to $m$. I know that $\lambda^k$ is an eigenvalue of $A^k$. What can I say about its algebraic multiplicity?,"['matrices', 'abstract-algebra', 'eigenvalues-eigenvectors', 'linear-algebra']"
2533598,What is limit of $ \lim \limits_{n \to \infty} \sqrt[n]{\sqrt[2^n]{a}-1}$ given $a>1$?,"What is the limit of $\displaystyle{ \lim \limits_{n \to \infty} \sqrt[n]{\sqrt[2^n]{a}-1}}$ given $a>1$ ? I did some computations and I feel its $\frac{1}{2}$ , i don't know how to prove it I did used Bernoulli inequality","['radicals', 'real-analysis', 'limits', 'indeterminate-forms', 'sequences-and-series']"
2533610,how do you recognize a positive (semi)definite matrix?,"I understand that the definition of (semi-)definiteness of matrix $A$ is $$\forall z_{\neq0}\in\mathbb R^k: z^TAz>0$$ I also know that this doesn't mean that all elements of a negative definite matrix $A$ are negative (in fact, they may all be positive or 0, such as with a 180 degree rotation in $\mathbb R^2$). Nevertheless, I'm wondering if there is a way to recognize, just by looking at the matrix, whether it is likely going to be a positive definite matrix? Is there a way to see this just from the matrix itself, or does it always require some form of computation first?",['matrices']
2533647,Autocorrelation function of derivative,"I have a question, I am stuck on for quite some time now.
Imagine you can choose a two dimensional autocorrelation function $C_V(x,y)$. From this I can create the two dimensional random process $V(x,y)$ (using the Wiener–Khinchin theorem and phase-randomization). So far so good. What I want in addition, is that the $x$-Integral of the autocorrelation function of the $y$-derivative is zero. So: $\int_{-\infty}^{\infty} C_F(x)\,$d$x = 0$. With: $C_F(s_x) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}F(x+s_x,y+s_y)F(x,y)\,\text{d}x\,\text{d}y \, |_{s_y=0}$ $F(x,y)=\frac{\partial V(x,y)}{\partial y}$. So the question is, how to choose $C_V(x,y)$ to obtain this (or is it even possible?). Also $C_V$ can be assumed to be radial symmetric. To put this in perspective:
I want to look at a particle, moving in the random (but correlated) potential V(x,y), which experiences the sideway Force F(x,y). Currently I am interested in a force with the specific setup above. What I have done so far is calculating :
$\int_{-\infty}^{\infty} \frac{\partial^n C_V(x,y)}{\partial y^n}|_{y=0}\,$d$x$
and tried to express it in terms of $C_F$ to connect these two functions. Also played around a lot with mathematica and numerics but did not find any expression linking these two. 
The correlation function I usually start with (because of specific reasons to my problem) is :
$C_V(x,y)=(1-a(x^2+y^2)) \cdot e^{-b(x^2+y^2)}$
But I am not bound on this form. I hope someone has an idea or knows where to read up something about this.
Thanks :)","['random-walk', 'signal-processing', 'probability', 'correlation']"
2533652,Solution of non-linear second order differential equation,"I am working with a system, which is described by the following differential equation $y'' = (1+y)^{3/2}$ Does this differential equation have a closed-form solution? And more generally: Is there a place, which contains a complete list of second-order differential equations, which do have analytical solutions?",['ordinary-differential-equations']
2533668,Tough integral involving logarithm,"Let $$ I = \int_0^{\infty} \ln{\left(\frac{a^s + x^s}{b^s + x^s}\right)}dx$$ assuming $a>0$, $b>0$, and $s>1$. (It would be great if someone could explain why $s$ should satisfy this to guarantee convergence.) Integration by parts yields(after an annoying limit) $$I = s(a^s - b^s) \int_0^{\infty} \frac{x^s}{(a^s + x^s)(b^s + x^s)}dx$$ I believe this form of the integral might be ripe for contour integration, however juggling the potential branch cuts of $x^s$ along with poles at solutions of $x^s = -a^s$, and  $x^s = -b^s$ makes me think this might not be the best solution. Perhaps there might be a better way to approach the original integral?","['complex-analysis', 'integration', 'contour-integration']"
2533671,"In the ring of even integers , do 4 and 6 have a lcm and gcd? If they have , what are they?","I have a solution, but I have a doubt in it. regarding gcd :
2 is not gcd for 4 and 6 because 2 does not divides 6 , as it gives 3 which does not belongs to set of even integers ring. regarding lcm : (i have doubt)
the solutions shows 12 is not lcm because 4|12 (= 3) which does not belongs to set of even integers ring but then why not 24 is a solution to lcm of 4 and 6 in a set of even integers??","['abstract-algebra', 'ring-theory']"
2533673,Sobolev spaces on Riemannian manifold and the Laplacian,"Craioveanu, Puta and Rassias define the Sobolev space $H_k(M)$ (pg. 106) on a compact Riemannian manifold $M$ as the completion of $C^\infty(M)$ with respect to the norm
$$ \|f\|_{H_k(M)} = \|f \|_{H_{k-1}(M)} + \int_M \langle \nabla^k f, \nabla^k f \rangle_M\ d\mu_g
$$
where $\nabla^k$ is the iterated covariant derivative with respect to the Levi-Civita connection on $M$, $d\mu_g$ is the Borel measure induced by the volume form, and $H_0(M)$ is defined as $L_2(M)$. Let $\Delta_g$ be the Laplace-Beltrami operator on $M$ with eigenvalues $0 = \lambda_0 < \lambda_1 \leq \lambda_2 \ldots$ and associated eigenfunctions $\{\varphi_j\}$ assuming Dirichlet boundary conditions. These eigenfunctions form an orthonormal basis for $L_2(M)$. The book then proves (pg. 134) that
\begin{equation} H_1(M) = \left\{ f = \sum_{j=0}^\infty \alpha_j \varphi_j\ \middle|\ \sum_{j=0}^\infty \lambda_j \alpha_j^2 < \infty \right\} \tag{1}\end{equation}
They state without proof that a similar result holds for $H_2(M)$, that is 
\begin{equation} H_2(M) = \left\{ f = \sum_{j=0}^\infty \alpha_j \varphi_j\ \middle|\ \sum_{j=0}^\infty \lambda_j^2 \alpha_j^2 < \infty \right\}\tag{2}\end{equation} 
and use this fact to extend the domain and range of $\Delta_g$ from $C^\infty(M)$ to $H_2(M)$ and $L_2(M)$ respectively. Questions: How does one prove (2)? Can it be generalised to $H_k(M)$? The proof of the first statement uses Green's formula and the fact the covariant derivative for functions coincides with the gradient. Is there an analogue of Green's formula for iterated covariant derivatives i.e. something like
$$ \int_M \langle \nabla^k f, \nabla^k g \rangle_M = \int_M \langle \Delta^k f , g \rangle_M $$ when $f$ and $g$ vanish on the boundary of $M$?","['laplacian', 'riemannian-geometry', 'differential-geometry', 'sobolev-spaces']"
2533688,Proving that $\bigl(1+\frac{r}{2^n}\bigr)^{2^n}<\frac{1}{1-r}$ by induction,"The question sets a condition where $r\in\mathbb{R}$ and $0<r<1$ with a sequence of rational numbers $a_1,a_2,a_3,\dotsc$ given by $$a_n=\Bigl(1+\frac{r}{2^n}\Bigr)^{2^n}$$ The question then asks to: Prove that $a_n<\dfrac{1}{1-r}$ for all $n\in\mathbb{N}$. How I approached this question is to first put the information I have in place $$\Bigl(1+\frac{r}{2^n}\Bigr)^{2^n}<\frac{1}{1-r}$$ where we can then take a base case of $n=1$.$$\Bigl(1+\frac{r}{2}\Bigr)^2<\frac{1}{1-r}$$ Here I expanded the equation to look like $$1+r+\frac{r^2}{4}<\frac{1}{1-r}$$ But at this point, I really didn't know how I would continue with proving the result. Any help would be appreciated.","['induction', 'sequences-and-series', 'discrete-mathematics']"
2533698,"If $a,b,c$ are three complex numbers Find possible values of $\lvert a+b+c \rvert$ [duplicate]","This question already has an answer here : Find the possible values of |A + B + C | (1 answer) Closed 6 years ago . Given three complex numbers $a,b,c$ such that $\lvert a \rvert=\lvert b \rvert=\lvert c \rvert=1$ and $$\frac{a^2}{bc}+\frac{b^2}{ac}+\frac{c^2}{ab}=-1$$ Find which of the following are possible values of $\lvert a+b+c \rvert$ A)0 B)2 C)1.5 D)3 My try: I assumed $a=e^{ix}$,$b=e^{iy}$, $c=e^{iz}$ Then we have $$\cos (2x-y-z)+\cos (2y-x-z)+\cos (2z-x-y)=-1$$ $$\sin(2x-y-z)+\sin (2y-x-z)+\sin (2z-x-y)=0$$ Squaring and adding we get $$3+2(\cos(3x-3y)+\cos(3y-3z)+\cos (3z-3x)=1$$ So $$\cos(3x-3y)+\cos(3y-3z)+\cos (3z-3x)=-1$$ any clue here?","['complex-analysis', 'trigonometry', 'complex-numbers']"
2533714,"Prove that, for prime $p$, the sum of products of numbers taken $r<p-1$ at a time from the set $\{1,2,\dots,p-1\}$ is always divisible by $p$.","Prove that, for prime $p$, the sum of products of numbers taken $r<p-1$ at a time from the set $\{1,2,\dots,p-1\}$ is always divisible by $p$. One way too prove is considering coefficients of polynomial:
$$f(x)=(x-1)(x-2)...(x-p+1)-x^{p-1}+1.$$
Any other proof will be appreciated.
The above proof of this on Introduction to Analytic Number Theory
 by Tom M. Apostol (Theorem 5.23).","['number-theory', 'alternative-proof']"
2533715,Estimator examples of non-normal probability distributions?,"The maximum likelihood estimators for expectation $\mu$ and variance $\sigma^{2}$ of a normal distribution are:
$$\hat{\mu} = \frac{1}{n}\sum_{i=1}^{n} \Big(x_{i}\Big)$$
and
$$\hat{\sigma^{2}} = \frac{1}{n}\sum_{i=1}^{n}\Big((\hat{\mu}-x_{i})^{2}\Big)$$
They are used so frequently in statistics that it is easy to think that they apply to all probability distributions. What would be the mean, variance (or other parameter of the distribution) estimators for some of the non-normal probability distributions?","['statistics', 'estimation', 'probability']"
2533759,Why this equality is true?,"Let $E$ be a complex Hilbert space. Let ${\bf S} = (S_1,...,S_d) \in \mathcal{L}(E)^d$. We recall that $\|{\bf S}\|$ is defined by
\begin{eqnarray*}
\|{\bf S}\|
&:=&\sup\left\{\bigg(\displaystyle\sum_{k=1}^d\|S_kx\|^2\bigg)^{\frac{1}{2}},\;x\in E,\;\|x\|=1\;\right\},
\end{eqnarray*} If the operators $S_k$ are commuting, why we have
$$\displaystyle\sup_{\|x\|=1}\displaystyle\sum_{|\alpha|=n}\frac{n!}{\alpha!}\|{\bf S}^{\alpha}x\|^2=||{\bf S}^n||^2\;?? \;,$$
with $n\in\mathbb{N}^*,\;$ $\alpha = (\alpha_1, \alpha_2,...,\alpha_d) \in \mathbb{N}^d;\;\alpha!: =\alpha_1!...\alpha_d!,\;|\alpha|:=\displaystyle\sum_{j=1}^d\alpha_j$;  ${\bf S}^\alpha:=S_1^{\alpha_1} \cdots S_d^{\alpha_d}$ and ${\bf S}^n:={\bf S}\diamond{\bf S}\diamond\cdots\diamond{\bf S}$. Note that ${\bf S}^2 :={\bf S}\diamond{\bf S}= (S_1 S_1,\cdots,S_1 S_d,S_2S_1,\cdots,S_2S_d,S_dS_1\cdots,S_d S_d)$. Thank you!!","['functional-analysis', 'multinomial-coefficients']"
2533775,Conditions required for Taylor's theorem for multivariate functions,"It's easy to find proofs for the following version of the Taylor's theorem for multivariate functions: Let $U$ be an open neighbourhood of $\mathbf a\in\mathbb R^n$ and $f:U\to\mathbb R^m$ a $C^k$ function. Then
$$f(\mathbf a+\mathbf h)=\sum^k_{i=0}{1\over i!}D^if(\mathbf a)\mathbf h^i+o(\|\mathbf h\|^k)$$
where $\mathbf h^i=(\mathbf h,\cdots,\mathbf h)\in(\mathbb R^n)^i$. For example, see http://math.stanford.edu/~conrad/diffgeomPage/handouts/taylor.pdf I remember a long time ago I saw on the internet a proof of this theorem only assuming $f$ is $k$-times differentiable, not assuming it to be $C^k$. I didn't really pay much attention to the proof back then. Now I am again thinking about this question, but I failed to find anywhere this version of Taylor's theorem with the weaker condition despite considerable effort. And I could not prove it or disprove it myself. So I wonder does the above hold under this weaker condition? Could anyone point me to any literature on the subject? It seems plausible that it might be true, since we know that the $k=1$ case is true by definition of being differentiable, and moreover the $n=m=1$ case is also true by the one-dimensional Taylor's theorem. And if this is true, does it also hold if $f$ is just $k$-times differentiable at $\mathbf a$, instead of on the whole of $U$?","['multivariable-calculus', 'real-analysis', 'analysis']"
2533810,1st Order Nonlinear PDE: Understanding Envelopes and Monge Cones,"I have a question about envelopes of surfaces. In a book I am reading the following: Suppose $S_a$ is a one parameter family of surfaces in $R^3$ given by $z=w(x,y;a)$ where $w$ depends smoothly on $x,y$ and the real parameter $a$. Consider also the equation $\partial_a w(x,y;a)=0$. For a fixed values of $a$, these two equations determine a curve $\gamma_a$. The envelope $E$ of the family of surfaces $S_a$ is just the union of these curves $\gamma_a$. The equation for $E$ is found simply by solving $\partial_a w(x,y,a)=0$ for $a$ as a function of $x$ and $y$, $a=f(x,y)$, and then substituting into $z=w(x,y,f(x,y))$. Moreover, along $\gamma_a$, $a$ is constant and we have $$dz = w_xdx + w_ydy \\0 = w_{ax}dx + w_{ay}dy$$
  For instance, if $S_a$ is a one-parameter family of 2-spheres: $(x-a)^2+y^2+z^2 = 1$, then the envelope is a cylinder of radius 1. Can anyone provide an ""intuitive"" geometric reason for why taking the derivative with respect to the parameter, setting it equal to zero, and plugging it back into $F$ gives the envelope? I see that it works in the example of the sphere, I obtain a cylinder $y^2 + z^2 = 1$. In the procedure descirbed above they use the notation $$dz = w_xdx + w_ydy \\0 = w_{ax}dx + w_{ay}dy$$ Is this a formal expression? When I read it as $$\frac{dz}{dt} = w_x\frac{dx}{dt} + w_y\frac{dy}{dt} \\0 = w_{ax}\frac{dx}{dt} + w_{ay}\frac{dy}{dt}$$ It makes sense to me. I looked this up and saw some junk on cotangent spaces, but couldn't understand how it was related to the discussion above. They introduce a notion of Monge Cone in the following way: Consider the 1st order PDE: $F(x,y,z,p,q)=0$. At any point $(x_0,y_0,z_0)$, $F$ establishes a functional relation between $p$ and $q$. Assuming $F_q(x_0,y_0,z_0,p,q)\neq 0$, implicit function theorem gives us: $F(x_0,y_0,z_0,p,q(p))=0$ for all $p$. The possible tangent planes to the graph $z=u(x,y)$ are given by: $$(z-z_0) = p(x-x_0)+q(p)(y-y_0)$$ which, as $p$ varies, describe a one-parameter family of planes through the point $(x_0,y_0,z_0)$. Using the equations in #2, they solve for the envelope of planes at $(x_0,y_0,z_0)$ (parameter is $p$) and find: $$dz = pdx + qdy \\ 0 = dx + \frac{dq}{dp}dy$$ How do I see this is a cone at $(x_0,y_0,z_0)$?","['characteristics', 'analysis', 'partial-differential-equations']"
2533837,$f'(x) = [f(x)]^{2}$. Prove $f(x) = 0 $,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a differentiable function such that $f(0) = 0$ and $f'(x) = [f(x)]^{2}$, $\forall x \in \mathbb{R}$. Show that $f(x) = 0$,  $\forall x \in \mathbb{R} $. I first (unsuccessfully) tried using the Mean Value Theorem, but this is in the Integrals chapter so the solution probably involves them. Can't really see where integrals come in here though. What I've got so far: (i) Since $f$ is differentiable, thus it is continuous and, hence,  integrable. Therefore $f^2$ is also integrable and as $f'=f^2$, $f'$ is too. (ii) $f' \geq 0$, $\forall x \in \mathbb{R}$","['real-analysis', 'ordinary-differential-equations']"
2533871,"Condition for 3 distinct normals from a point $(h,k)$ to the parabola $y^2=4ax$","What are the conditions in terms of $h,k$ to be able to draw three
  distinct normals to the parabola $y^2= 4ax$ ? Normal to the parabola from $(h,k)$ is given by: $am^3 +(2a-h)m+k=0$. This equation can yield three distinct slopes $m_1,m_2,m_3$ if $\Delta>0$ ( source ). The $\Delta$(discriminant) of the equation is given by $-4a (2a-h)^3-27a^2k^2$. For it to be greater than $0$ when $a>0$, I derived $h>2a$ and $k<-2\sqrt{2}a$. How do I derive the other constraints on $h,k$ i.e. $k>4\sqrt{2}{a}$ and $h>8a$
?","['algebra-precalculus', 'proof-writing', 'analytic-geometry']"
2533891,What are some interesting functions that are equivariant under rotations in SO(3)?,"I'm interested in machine learning on 3D point clouds. Are there any interesting functions that are equivariant under rotations in SO(3)? The PointNet paper: https://arxiv.org/abs/1612.00593 already found one way to design functions that are invariant to permutations of the inputs (symmetric), but I haven't found a class of functions that is both symmetric and also equivariant under SO(3). The functions don't have to be complex themselves as long as they can be composed with each other to approximate any continuous function that is symmetric and equivariant under SO(3).","['group-theory', 'equivariant-maps']"
2533893,"Can we generalize $\sum_{n=0}^\infty \binom{4n}{n}\frac{1}{(3n+1)\,2^{4n+1}}=\frac1{T}$ for tribonacci constant $T$?","(Inspired by this post .) Given the tribonacci constant $\Phi_3$, the tetranacci constant $\Phi_4$, etc. How do prove that, $$\sum_{n=0}^\infty \binom{4n}{n}\frac{1}{(3n+1)\,2^{4n+1}}=\frac{1}{2}{\;}_3F_2 \left(\frac14,\frac24,\frac34; \color{blue}{\frac23,\frac43};\frac{4^4}{\color{red}{2^4}\,3^3} \right)=\frac1{\Phi_3}$$ $$\sum_{n=0}^\infty \binom{5n}{n}\frac{1}{(4n+1)\,2^{5n+1}}=\frac{1}{2}{\;}_4F_3 \left(\frac15,\frac25,\frac35,\frac45; \color{blue}{\frac24,\frac34,\frac54};\frac{5^5}{\color{red}{2^5}\,4^4} \right)=\frac1{\Phi_4}$$ $$\sum_{n=0}^\infty \binom{6n}{n}\frac{1}{(5n+1)\,2^{6n+1}}=\frac{1}{2}{\;}_5F_4 \left(\frac16,\frac26,\frac36,\frac46,\frac56; \color{blue}{\frac25,\frac35,\frac45,\frac65};\frac{6^6}{\color{red}{2^6}\,5^5} \right)=\frac1{\Phi_5}$$ and so on? And what is the corresponding hypergeometric formula for $\displaystyle \frac1{\Phi_2}$ with golden ratio $\Phi_2$? ( Courtesy of Jack D'Aurizio ) By the Lagrange inversion theorem , a solution to,
$$x^k-x+1=0$$
is given by,
$$x=\sum_{n=0}^\infty \binom{kn}{n}\frac{1}{(k-1)n+1}$$ Similarly, given the general polynomial of the $k$-nacci constants as,
  $$x^k(2-x)-1=0$$
  how do we show that,
  $$\frac1{x}=\frac1{\Phi_k}=\sum_{n=0}^\infty \binom{(k+1)n}{n}\frac{1}{(kn+1)\,2^{(k+1)n+1}}$$","['fibonacci-numbers', 'binomial-coefficients', 'hypergeometric-function', 'constants', 'sequences-and-series']"
2533921,$n$ vertices from a regular polygon with $2n$ sides are red and the other $n$ vertices are blue.,"$n$ vertices from a regular polygon with $2n$ sides are chosen and colored red. The other $n$ vertices are colored blue. Afterwards, the $\binom{n}{2}$ lengths of the segments formed with all pairs of red vertices are ordered in a non-decreasing sequence, and the same procedure is done with the $\binom{n}{2}$ lengths of the segments formed with all pairs of blue vertices. Prove that both sequences are identical. Say $\mathcal{R}=\{r_1,r_2,...r_n\}$ the set of red vertices and $\mathcal{B}=\{b_1,b_2,...b_n\}$ the set of blue vertices.
Let $$R(x) = \sum _{i=1}^{n} x^{r_i}\;\;\;\wedge \;\;\; B(x) = \sum _{i=1}^{n} x^{b_i}$$ then we have to prove that $$B(x)B({1\over x}) = R(x)R({1\over x})$$ where the exponents are taken modulo $2n$ . Is this reasonable aproach?","['polynomials', 'generating-functions', 'combinatorics', 'contest-math', 'discrete-mathematics']"
2533941,Some T2 spaces must have a small dense?,"If a Hausdorff space $\ X\ $ admits a dense subset $ A \hookrightarrow  X\ $ such that $$|X|^{|A|}\ =\ |X|$$ then indeed $$|X| \leq |\text{End}_{\text{Top}}(X)|  \leq |X|^{|A|}\ = \ |X|.$$ It is the case of $\mathbb{Q} \hookrightarrow \mathbb{R}$. Thus, if there is a  small  enough dense subspace, there are  not so many endomorphisms. Is the converse true? Namely, suppose $|\text{End}_{\text{Top}}(X)|  = \ |X|.$ Is it true that there exists a dense subset $A$ such that $|X|^{|A|}\ =\ |X|$? UPDATE: Solved on MO .",['general-topology']
2533958,Separating the complements of two sets in each other,"Let $U,V\subseteq \mathbf R^n$ be open sets with non-empty intersection. Do there exist disjoint open sets $A,B\subseteq U\cup V$ such that $U\setminus V\subseteq A$ and $V\setminus U\subseteq B$? Drawing pictures and taking intuition from $\mathbf R^1$ and $\mathbf R^2$ it seems that the answer should ""obviously"" be yes, by splitting $U\cap V$ ""down the middle"" by a closed set, then taking the two pieces and adding them to $U\setminus V$ and $V\setminus U$, respectively (see picture below). This is ""oviously"" not a proof, so I was hoping someone could help me with the right terminology here. Unfortunately the closures of $U\setminus V$ and $V\setminus U$ intersect, so I can't use the fact that any two non-intersecting closed sets are separable. However $\overline{U\setminus V}\cap V\setminus U = \emptyset$ and $\overline{V\setminus U}\cap U\setminus V = \emptyset$, but I'm not sure if this helps. Hour later edit: I think I worked it out, maybe someone can spot an error in my attempt. Let $W := \overline {U\cap V} \cap (U\cup V)$, which is closed in $U\cup V$. Note that
\begin{align*}
U' & := (U\setminus V)\setminus \text{int}(U\setminus V) \subseteq W \text{ is closed,} \\
V' & := (V\setminus U)\setminus \text{int}(V\setminus U) \subseteq W \text{ is closed,}
\end{align*}
and moreover $U'\cap V' = \emptyset$. Hence there exist open sets $U'',V''\subseteq W$ (open in $W$) with $U'\subseteq U''$, $V'\subseteq V''$ and $U''\cap V'' = \emptyset$. Then $(U'')^c, (V'')^c$ are both closed in $W$, so $(U'')^c\cap W, (V'')^c\cap W$ are both closed in $U\cup V$. Since $U\setminus V$, $V\setminus U$ are both closed in $U\cup V$, we have that
\begin{align*}
A & := \left(((U'')^c \cap W) \cup (V\setminus U)\right)^c ,\\
B & := \left(((V'')^c \cap W) \cup (U\setminus V)\right)^c ,
\end{align*}
are both open in $U\cup V$, with $U\setminus V \subseteq A$ and $V\setminus U \subseteq B$, and most importantly, $A\cap B = \emptyset.$",['general-topology']
2533960,Combinatorical proof of the identity $\binom{2n}{2} = 2\binom{n}{2}+n^2$,"So, the given expression is
$$\binom{2n}{2} = 2\binom{n}{2}+n^2$$ The task is to give a combinatorical proof for it. Left side of the identity is obviously equal to the number of options for choosing 2 elements out of the set with cardinality $2n$. What issues me is that I can't think of any way to separate that into two disjoint cases which would have $2\binom{n}{2}$ and $n^2$ different options (what is, I believe, meant to happen). Any hints would be helpful.","['combinations', 'combinatorics']"
2533987,Inequality about Eigenvalues of Symmetric Block Matrix,"Let $ M = \left( \begin{array}{cc} A & B \\ B^T & C \end{array} \right)$ be a real symmetric matrix ($A, C$ are symmetric matrices). Let $\lambda_m$ and $\lambda_M$ denote the minimum and maximum eigenvalues of $M$, and $\mu_A$ and $\mu_C$ denote, respectively, the maximum eigenvalues of $A$ and $C$. Show that $\lambda_m+\lambda_M \leq \mu_A + \mu_C$. I've just got a hint from others: consider the positive semidefinite matrix $ M - \lambda_m I$. Maybe there are other methods.","['eigenvalues-eigenvectors', 'symmetric-matrices', 'linear-algebra']"
2534006,Tautology of $\mathcal{L}_X(J)=0$ and flow of $X$ consisting holomorphic transformation where $J$ is complex structure,"Let $(M,J)$ be a complex manifold with complex structure $J$. Suppose $X$ is a real vector field over $M$. $\mathcal{L}_XJ=0$ iff flow of $X$ consists of holomorphic transformation of $M$ The following is a dumb question. The reference is http://moroianu.perso.math.cnrs.fr/tex/kg.pdf pg 14 Lemma 2.7. Q: Why this is tautological here? I do not see this is obvious. The flow is made of 1 parameter group along some other real vector field.","['complex-geometry', 'several-complex-variables', 'differential-geometry', 'lie-derivative']"
2534043,"Show that $\zeta(2,2) = \frac{3}{4} \zeta(4)$ by integrating $\int \frac{dx}{x} \wedge \frac{dx}{x} \wedge \frac{dx}{1-x} \wedge \frac{dx}{1-x}$","On Wikipedia there's an OK discussion of multiple zeta values ( MZV ).  We have an identity: $$ \zeta(2,2) = \sum_{m > n > 0} \frac{1}{m^2 \, n^2}  = \frac{3}{4}  \sum_{n > 0} \frac{1}{ n^4}  = \frac{3}{4} \zeta(4) $$ This formula might already be on the Math.Stackexchange site (link?) Also notice that somehow the zeta function turns partly into an element of $\mathbb{Q}$.  I have already found an argument using one of the special cases of the shuffle formula. $$ \zeta(2)^2 = 2 \, \zeta(2,2) + \zeta(4)  $$ Certainly, there still a search for even more $\zeta$-functions of this kind.  Both sides of this equation have integral formulas (due to Drinfiel'd): \begin{eqnarray*} \zeta(2,2) &\stackrel{?}{=}& \int_{1 > x_1 > x_2 > x_3  > x_4 > 0}  
\frac{dx_1}{x_1} \wedge
\frac{dx_2}{1-x_2} \wedge
\frac{dx_3}{x_3} \wedge
\frac{dx_4}{1-x_4}  \tag{$*$}\\ \\
\zeta(4) &\stackrel{?}{=}& \int_{1 > x_1 > x_2 > x_3  > x_4 > 0}
\frac{dx_1}{x_1} \wedge \;\;
\frac{dx_2}{x_2} \;\;\wedge 
\frac{dx_3}{x_3} \wedge
\frac{dx_4}{1-x_4} \tag{$**$}
 \end{eqnarray*} Even now, I'm not totally convinced the integrals on the right side, represent the infinite series on the left side.  But also, I wonder if there's a stronger notion of equivalence than just that they evaluate to the same number.  We have just shown that $\zeta(2,2)$ and $\zeta(4)$ are periods These notes of José Ignacio Burgos Gil and Javier Fresán indicate the following: From the modern point of view, periods appear
  when comparing de Rham and Betti cohomology of algebraic varieties over
  number fields. So, my other question is whether these two 4-forms are the same.  Possibly over $\mathbb{C} \backslash \{ 0,1\}$ or $\mathbb{P}^1 \backslash \{ 0,1, \infty\}$ : \begin{eqnarray*} \omega_1 &=&   
\frac{dxt_1}{x_1} \wedge
\frac{dx_2}{1-x_2} \wedge
\frac{dx_3}{x_3} \wedge
\frac{dx_4}{1-x_4} \\ \\
\omega_2 &=& 
\frac{dx_1}{x_1} \wedge \;\;
\frac{dx_2}{x_2} \;\;\wedge 
\frac{dx_3}{x_3} \wedge
\frac{dx_4}{1-x_4}
 \end{eqnarray*} And I would ask is there a sense in which $\omega_1 = \omega_2$?  Mostly I just want help with the integrals.","['number-theory', 'multiple-integral', 'homology-cohomology', 'zeta-functions']"
2534053,"On the harmonic number ($H_n$) upper and lower ""classical"" bounds: which of those is closest to $H_n$?","It is a well-known fact that the harmonic number $$\displaystyle H_n = \sum_{k=1}^n \frac{1}{k}$$ satisfies the following inequality: $$\displaystyle \ln(n) + \frac{1}{n} \;\leq \; H_n \; \leq \; \ln(n) + 1$$ as it is stated on page 26 of this notes . Is it true that $H_n$ is closer to $\ln(n) + 1$ than $H_n$ is to $\displaystyle \ln(n) + \frac{1}{n}$ ? If so, how to prove that?","['real-analysis', 'calculus', 'discrete-mathematics']"
2534097,Limit in the category of sets,"In Vakil's notes on algebraic geometry, to define inverse limit, he started with a functor $F:C\to S$ where $C$ is a small category and $S$ is any category (small means objects and morphisms are sets). Later there is an exercise which asks us to prove that in the category of sets the following is the inverse limit $$\left\{(a_i )_{i∈I} ∈ \prod_{i\in I} A_i: F(m)(a_j ) = a_k \text{ for   all }m ∈ Mor_I (j,k) \in  Mor(I)\right\}.$$ Now my question is 1) Why is the condition of small category on the indexing set and not on the category $S$? 2) What does he mean by $F(m)(a_j ) = a_k $ for all  $m\in Mor_I (j,k) \in Mor(I)$. (shouldn't it be : $ F(m)(A_j ) = A_k $ for   all  $m\in Mor_I (j,k) \in Mor(I)$ as the functor maps the indexing category to sets and $a_i$ is just an element of the set $A_i$?)","['category-theory', 'limits-colimits', 'elementary-set-theory']"
2534109,Computing Type II Error for a One-Sided Normal Test.,"For a random sample $X_1, X_2, \ldots, X_{49}$ taken from a population having standard deviation $4$ , the sample mean is found to be $20.3$ .
Test the claim that the mean of the population is not less than $21$ at $1\%$ level of significance. Find the probability of Type II error if the population mean is $19.1$ . So to test for $H_0: \mu\geq 21$ versus alternate hypothesis $H_1: \mu < 21$ we calculate the test statistic $Z = \dfrac{20.3-21}{\frac{4}{\sqrt{49}}}=-1.225$ and since $-z_{0.01} = -2.326<-1.225$ , we cannot reject the null hypothesis at 1% level of significance. I am confused how to do the second part. I know that the Type 2 error will be $P(\text{accept} \ H_0 \mid \mu = 19.1)$ . How do I do this?","['statistics', 'probability', 'hypothesis-testing']"
2534119,Approximation of exponential function by power series,"Let $x \in (-\frac{1}{2},\frac{1}{2}), n \in \mathbb{N}$ How can I choose a $n$ that the the inequality is valid? $$\left|e^x-\sum_{k=0}^n \frac{x^k}{k!}\right| \leq \frac{|e^x|}{10^{16}}$$ My ideas:
Try some values for $n$ and verify the inequality for value greater than $-1/2$ and less than $1/2$ because of the monotony of the exponential function... But I could not find a $n$.",['analysis']
2534155,Finding out nth term of a sequence by method of diference,"Let us consider $$S_n=t_1+t_2+...+t_n$$ and let $∆_{t_1}=t_2-t_1,∆_{t_2}=t_3-t_2,...,∆_{t_{n-1}}=t_n-t_{n-1}$ be the first order difference. Similarly $∆^2_{t_1}=∆_{t_2}-∆_{t_1},...∆^2_{t_{n-2}}=∆_{t_{n-1}}-∆_{t_{n-2}}$ be the second other difference. Similarly we calculate $∆^{n-1}_{t_1}$ . The question is to prove that $$t_n=t_1+\binom{n-1}{1} ∆_{t_1}+\binom{n-1}{2}∆^2_{t_1}+...+∆^{n-1}_{t_1}$$ I tried to verify it for simple cases and tried to proof via induction but couldn't proceed.Any ideas?",['sequences-and-series']
2534203,Solution of the equation below,"The solution of the equation $\sin 7x + \cos 2x = -2$ is/are? My Approach: For the above equation to hold true, Both $\sin 7x$ and $\cos 2x$ have to be -1. $$\sin 7x = -1$$
$$x = \frac{n\pi}{7} + (-1)^n(-\frac{\pi}{14})$$ Also, $$\cos 2x = -1$$
$$x = n\pi \pm \frac{\pi}{2}$$ But the answer is $2n\pi + \frac{\pi}{2}$ Can anybody help me?",['trigonometry']
2534220,What is an elegant way to find the third eigenvalue of $A=\left[\begin{smallmatrix} 51&-12 & -21\\ 60 & -40&28\\ 57&-68&1 \end{smallmatrix}\right]$?,"We had this in an exam today: Given the matrix
$$A=\begin{bmatrix}
51&-12 & -21\\
60 & -40&28\\
57&-68&1
\end{bmatrix}$$
Here is the precise question as asked in our exam: Someone tells you (accurately) that $-48$ and $24$  are eigenvalues of the matrix $A$. Without using a computer, calculator or writing  anything down find the third eigenvalue of $A$. I have used the classical method (through the characteristic polynomial of A) but this leaded to a some tremendous computation and finally got the answer. But the question intentionally asked to do not make any use of such computation. How can one elegantly find the third eigenvalue of $A$?","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2534267,"Proving the binomial identity $\sum_{j=0}^k {n \choose j}\cdot {m \choose k-j}={n+m \choose k}$ when $n,m$ are negative integers","What I'm really interested in is the identity:
$$\displaystyle \sum_{j=0}^k {j+r-1 \choose j}\cdot {k-j +s-1 \choose k-j}={k+r+s-1 \choose k}$$ It arises in the proof of the convolution formula of two negative binomial Random Variables, for example. What I know is the identity:$\displaystyle { j+r-1 \choose j} = {-r \choose j} (-1)^j $ Hence what I need to show is $$ \displaystyle \sum_{j=0}^k {-r \choose j} \cdot {-s \choose k-j}={-r-s \choose k}  $$ aka the  binomial convolution formula for $m = -r, n = -s$ when $s$ and $r$ are positive intergers. How do I do that?","['combinatorics', 'binomial-coefficients', 'random-variables']"
2534294,The function $g$ is continuous in discontinuity point,"Let $g \in V[a, b]$, where $V[a, b]$ is the set of functions of bounded variation on $[a,b]$. 
  Show that existence of the Riemann-Stieltjes integral $\int_a^b f dg$ implies that $g$ is continuous in any discontinuity point of $f$. May be I can show that the integral exist when $g$ is monotone increasing and continuous by substracting the upper and lower Riemann-Stieltjes sums and showing this quantity is smaller than any $\epsilon$?","['real-analysis', 'integration', 'stieltjes-integral']"
2534303,Is a quotient of topological vector space a topological vector space,"I should prove or disprove that quotient of topological vector space is a topological vector space. I think that it is. May I reduce my prove to proof of following statement? $X$ - topological vector space, $M \subset X$ - linear subspace of $X$. Let $A$ - vector addition in $X/M$, W - neighbourhood of origin $o$ in $X/M$. Then i should prove that $A^{-1}(W)$ is a neighbourhood of $(o, o)$ in $X/M \times X/M$. But I have no idea how can I prove this fact.","['functional-analysis', 'abstract-algebra', 'topological-vector-spaces']"
2534330,Determine $\lim_{n \to \infty} (2^{n+1} \sin(\frac{\pi}{2^{n+1}}))$,"I was able to calculate
$$\lim_{n \to \infty} (2^{n+1} \sin(\frac{\pi}{2^{n+1}})) = \pi$$
using L'Hopital Rule, but how to get $\pi$ without it?","['algebra-precalculus', 'calculus']"
2534369,"Is f(x,y) integrable? Question 3-7 from Spivak's Calculus on Manifolds","I am trying to work my through the exercises in Spivak's Calculus on Manifolds. I am currently working on the exercises in Chapter 3 which deals with Integration. I am having trouble with the following question: Let: \begin{equation}
  f(x,y)=\begin{cases}
    0, & \text{if $x$ is irrational}.\\
    0, & \text{if $x$ is rational, $y$ is irrational}. \\
    1/q, & \text{if $x$ is rational, $y=p/q$ in lowest terms}.
  \end{cases}
\end{equation} Show that $f$ is integrable  on $A = [0,1] \times [0,1]$ and $\int_A f = 0$. I was thinking of trying to prove that this set is Jordan Measurable and that it's Jordan measure is zero and that it is therefore Riemann Integrable but I am not sure how to do this or if it is even the best way to solve this problem. If I could show that $f$ is continuous on $A$ up to a set of Jordan Measure $0$, then $f$ would be integrable but again, I'm not sure I can do this or if its even appropriate for this problem. Any assistance that anyone could provide would be greatly appreciated. Thank you.","['multivariable-calculus', 'real-analysis', 'integration', 'riemann-integration']"
2534415,Every Linear Functional is zero or surjective,"Is the Following Proof Correct ? Theorem. Given that $\mathcal{L}(V,\mathbf{F})$ represent the set of all linear functionals from the finite dimensional vector space $V$ to the scalar field $\mathbf{F}$, every member  of $\mathcal{L}(V,\mathbf{F})$ is either surjective or Zero. Proof. Let $\phi\in \mathcal{L}(V,\mathbf{F})$ and $c\in\mathbf{F}$ assume that $\phi\neq 0$. We may assume without loss of generality that 
$$\phi = A_1\phi_1+A_2\phi_2+.\ .\ .\ A_m\phi_m$$ where $A_1,A_2,...A_m\in\mathbf{F}$ and that $\phi_1,\phi_2,...,\phi_m$ is the basis of the dual of $V$ which has the basis $v_1,v_2,...,v_m$ furthermore since $\phi\neq 0$ it follows that the set $\alpha$
$$\alpha = \{A\in\{A_1,A_2,...,A_m\}\big|A\neq 0\} = \{B_1,B_2,...,B_{|\alpha|}\}$$
is such that $\alpha\neq\varnothing$, where $|\alpha|\leq \dim V.$ Now let $$c=\sum_{j=1}^{|\alpha|}C_j$$ where $C_j\in\mathbf{F}$ and consider the vector $v\in V$ such that 
$$v = \sum_{j=1}^{|\alpha|}\frac{C_j}{B_j}v_j$$
where $v_j$ and therefore $\phi_j$ corresponds to $B_j$ applying $\phi$ to $v$ yields
$$\phi\left(\sum_{j=1}^{|\alpha|}\frac{C_j}{B_j}v_j\right) = c$$ since $c$ was arbitrary it follows that $\phi$ is indeed surjective $\blacksquare$","['linear-algebra', 'proof-verification']"
2534490,How do I prove this relation is symmetric?,"The problem defines a relation on the integers as follows: For integers $a,b$, $a \sim b$ if $ab$ is even. In other words, $ab$ is even $ \Rightarrow a\sim b$. To prove $\sim$ is symmetric, I need to show $a\sim b \Rightarrow b\sim a$. My original approach was to assume $a\sim b$, and from this deduce $ab$ is even. By the commutative property of multiplication, $ba$ is even as well, so $b\sim a$. However, if I assume $a\sim b$, I'm not sure if I can deduce $ab$ is even. According to the statement, $a\sim b$ being true does not guarantee $ab$ is even, as $ab$ can be odd and the statement can still hold. Does this concern warrant me changing my original approach, or is there a way to still deduce $ab$ is even from the fact $a\sim b$?","['proof-verification', 'proof-writing', 'contest-math', 'relations', 'discrete-mathematics']"
2534526,Radius of a circle that overlaps half of a smaller circles area [Geometry puzzle],"I thought of this question whilst doing some physics. There is a circle $C_1$ with radius $r$ and a circle $C_2$ with radius $\lambda$. The centre of $C_2$ is on the circumference of $C_1$. What value of $\lambda$ in terms of $r$ will give the area $A$ equal to the area $B$? My attempt:
Taking the centre of $C_2$ as the origin and thinking of them as circles on a graph gives an equation for each circle. C1: $ (x+r)^2+y^2=r^2 $ C2:  $x^2+y^2=\lambda^2$ If solve it as a simultaneous equation you can find that their point of intersection is $-\lambda^2/2r$ Then you can do $\int_{-r}^{-\lambda^2/2r} \sqrt{\lambda^2-x^2} dx$ to find area $D$ and $\int_{-\lambda^2/2r}^0 \sqrt{-x^2-2xr} dx$ to find area $E$.
Adding these areas together and doubling them should give half the circle, which is $0.5\pi r^2$. Then you should be able to solve the equation for $\lambda$. However when I attempted this (after slogging through about two pages of algebra) I put the equation in Wolfram Alpha and got values for $\lambda$ which were either complex or smaller than $r$. I would like to know if my method is correct and I just made a mistake, or if there is another method to this problem.","['calculus', 'geometry']"
2534574,Converting this particular double integral to an iterated polar,"In class, the professor converted this double integral: $$\int_{-4}^0 \int_{-\sqrt{16-x^2}}^{\sqrt{16-x^2}} f(x,y) \,dy\,dx$$ into the following iterated polar: $$\int_{\pi/2}^{\pi} \int_{0}^{4} f(r,\theta) r\,dr\,d\theta$$ My question is this: why is the outer integral for the iterated polar from $\pi$/2 to $\pi$? Shouldn't it extend from $\pi$/2 to $\frac{3{\pi}}{2}$? In other words, shouldn't the iterated polar read: $$\int_{\pi/2}^{\frac{3{\pi}}{2}} \int_{0}^{4} f(r,\theta) r\,dr\,d\theta$$ Any insight would be deeply appreciated. Thank you. Edit 1: Reformatted iterated polar to represent proper values.",['multivariable-calculus']
2534605,Find the cardinality of the set,"Let $A = \{0,1,2,3\}$.
  Find the cardinality of the set
  $$\left\{f\mid f:A\rightarrow A\land f(x) = ax+b\land a,b\in A\land\text{$f$ is one-to-one}\right\}$$ The only method i can think of is to list out all the possible functions exhaustively. What are the better methods?","['functions', 'discrete-mathematics']"
2534625,Stuck on finding the integrating factor for an exact differential equation.,"The following DV is given $\sin(y)\cos(x) + \cos(y)[\sin(y)-\sin(x)]y'=0$ where $y$ is a function of $x$. This is in the form  $M(x,y(x))+N(x,y(x))y'=0.$ This equation is not exact. To make it exact I have to calculate the integrating factor $\mu(x)$.
Now when I use the formula $(M_y-N_x)/N$ to find the integrating factor, I get the expression $2\cos(x)/(\sin(y)-\sin(x))$ which is not only depending on $x$ or $y$. The problem states that there should be an integrating factor so what am I doing wrong. Can someone help me out?","['ordinary-differential-equations', 'calculus']"
2534644,Points of intersection between line and ellipse,"I'm trying to find out if a line intersects an ellipse. I tried to do this without finding the intersection points at first, but this didn't work out well. So now I'm just trying to find the two intersection points and then check if they are on the line segment. First I tried to implement an algorithm to find a line, perpendicular to the line segment, from the center of the ellipse to the line segment. If either end point was in the ellipse or the intersection point was both on the line segment and in the ellipse, then they intersected. However this failed around the perimeter of the ellipse in some situations. So next I tried to implement an algorithm discussed in: Coordinates of the intersection points between an ellipse and a chord line I used Hamed's answer. However, it doesn't appear to be working for me. I was hoping someone could tell me what I'm doing wrong. The $x$ values are coming out much higher than they should, and it often determines that there aren't real roots when there are. I'm not seeing the difference between my implementation and the solution. I'm just curious if I implemented it incorrectly, the equation is wrong or maybe there is something else I'm missing? Thanks in advance, I appreciate any assistance I can get as I've hit a road block at this point. Edit : $a$ and $b$ are of arbitrary size, $a$ can be larger than $b$ and vice versa. the ellipse is at the origin $(0,0)$ and the line is a line segment which is defined by end points. you'll also notice I multiplied things by themselves rather than use the built in function for squaring something, as I've read it's expensive in terms of computing power. Edit : here is an example public function lineTest( test_ellipse:Geometry_Ellipse, test_line:Geometry_Line ):Object {
        // get the variables that define the line and ellipse
        var m:Number = test_line.getSlope();
        var c:Number = test_line.getY_Intercept();
        var a:Number = test_ellipse.getRadius_X();
        var b:Number = test_ellipse.getRadius_Y();

        // check for division by 0 and sqrt of negative values
        if( (b*b)+(m*a)*(m*a)-(c*c) >= 0 && (b*b)+(m*a)*(m*a) != 0 ) {
            // get intersection points
            var point_1:Geometry_Coordinate = new Geometry_Coordinate(
                ( (m*c*a)*(m*c*a) + (a*b)*Math.sqrt((b*b)+(m*a)*(m*a)-(c*c)) ) / ( (b*b)+(m*a)*(m*a) ),
                ( (c*b)*(c*b) - ((m*a)*b)*Math.sqrt((b*b)+(m*a)*(m*a)-(c*c)) ) / ( (b*b)+(m*a)*(m*a) ),
                0
            );
            var point_2:Geometry_Coordinate = new Geometry_Coordinate(
                ( (m*c*a)*(m*c*a) - (a*b)*Math.sqrt((b*b)+(m*a)*(m*a)-(c*c)) ) / ( (b*b)+(m*a)*(m*a) ),
                ( (c*b)*(c*b) + ((m*a)*b)*Math.sqrt((b*b)+(m*a)*(m*a)-(c*c)) ) / ( (b*b)+(m*a)*(m*a) ),
                0
            );

            // check if points are on line segment
            // ...

            return { ""result"":true, ""points"":new Array(point_1, point_2) };
        } else {
            trace(""no real values"");
        }

        return { ""result"":false, ""points"":new Array() };
    }","['conic-sections', 'geometry']"
2534645,"For any two sets , the power sets : $\mathcal P(A\setminus B) = \mathcal P(A) \setminus \mathcal P(B)$?","For any two sets , the power sets : $\mathcal P(A\setminus B) = \mathcal P(A) \setminus \mathcal P(B)$? Is this proof correct? $$\begin{split}x \in \mathcal P(A\setminus B) &\iff x \subseteq A\setminus B  \\ &\iff x \subseteq A \land x \nsubseteq B \\ & \iff x \in\mathcal P(A) \land x \notin \mathcal P(B) \\ & \iff x \in \mathcal P(A) \setminus \mathcal P(B)\end{split}$$","['proof-writing', 'proof-verification', 'discrete-mathematics']"
2534649,Banach Steinhaus Theorem: Necessity of Completeness,I came across this proof of the Banach-Steinhaus theorem in my textbook: I was wondering where exactly the author of the proof used the completeness of $V$. Does this have to do with the initial construction of $U \subset V$?,"['real-analysis', 'banach-spaces', 'functional-analysis', 'linear-transformations', 'metric-spaces']"
2534664,Functional equation on $\mathbb{R}^+$: $f(x)f\big(yf(x)\big)=f(x+y)$,"Let $f:\mathbb{R}^+\rightarrow\mathbb{R}^+$ be a function satisfying $$f(x)f\big(yf(x)\big)=f(x+y),\forall x,y\in\mathbb{R}^+$$ If $f(1)=\frac{1}{152}$ , evaluate $f(4)$ . By inspection, we can see $f(x)=\frac{1}{151x+1}$ is a solution, from which we can easily get the answer. But how can we show that this is the only solution? Here is my work: Because $f$ is the reciprocal of a linear function, it would probably help to define $g(x)=\frac{1}{f(x)}$ (note that this is well defined as we are working in $\mathbb{R}^+$ ). Then the given equation becomes $$g(x)g\left(\frac{y}{g(x)}\right)=g(x+y)$$ If we take $y=g(x)$ , then this becomes $$g(x)g(1)=g\big(x+g(x)\big)\implies 152g(x)=g\big(x+g(x)\big)$$ Not sure where to go from here. Any thoughts?","['contest-math', 'functions', 'functional-equations']"
2534680,What is the meaning of limit of Fibonacci sequence?,"For general Fibonacci sequence with $F_1=F_2=1$, It is known that limit of $\frac {F_{n+1}} {F_n} $ exists. I am wondering what this limit implies and why it is important to compare these two sequences. Thanks","['fibonacci-numbers', 'sequences-and-series']"
2534686,On Pandigital numbers,"The other day I ran across a very nice video from Numberphile in YouTube, which proves that the following formula is approximate to $e$ by $18457734525360901453873570$ digits, which is pretty amazing. \begin{align*}
\left(1+9^{-4^{7\cdot 6}}\right)^{3^{2^{85}}} \approx e
\end{align*} I got excited about this, and thought about writing an article for my school magazine. Does anyone know any good references (articles, books, videos) that have interesting facts and curiosities concerning pandigital numbers and formulas? Maybe something like the topic addressed in this this question is interesting as well, computing algorithms to find pandigital formulas... Thanks in advance, Miguel","['algebra-precalculus', 'recreational-mathematics', 'number-systems']"
2534709,What to teach in a lecture about diagonalization of matrices in 50 minutes?,"I have to give a lecture of 50 minutes with the theme ""diagonalization of matrices"" (basic undergrad level) for a public contest. I would like suggestions about the contents of this class. I've been thinking to give the pages 181-187 from Hoffman and Kunze's linear algebra book (maybe I will not have time to do all these pages). What do you think is the best approach? I don't know if I start this class with eigenvalues as Hoffman and Kunze do and goes through very basic theorems about eigenvalues and diagonalizable matrices or begin with the definition of diagonalizable matrices and prove deeper theorems.","['linear-algebra', 'soft-question']"
2534730,"Given a $3 \times 3$ matrix, left multiply or right multiply unitary matrices (e.g. householder reflector) to introduce zeros.","Given a $3 \times 3$ matrix $A$, we would like to left-multiply or right-multiply unitary matrices to introduce zero elements in specific forms as the following, $\left( {\begin{array}{*{20}{c}}
  {\text{x}}&{\text{x}}&0 \\ 
  {\text{x}}&0&{\text{x}} \\ 
  0&{\text{x}}&{\text{x}} 
\end{array}} \right)$ and $\left( {\begin{array}{*{20}{c}}
  {\text{x}}&{\text{x}}&0 \\ 
  {\text{0}}&0&{\text{x}} \\ 
  0&{\text{0}}&{\text{x}} 
\end{array}} \right)$ where $\text x$ represents a non-zero element. My attempt : For the first form, First apply a householder reflector for the 1st column of the 2nd and 3rd rows but keeps the 1st row unchanged,then we have $\left( {\begin{array}{*{20}{c}}
  {\text{x}}&{\text{x}}&{\text{x}} \\ 
  {\text{x}}&{\text{x}}&{\text{x}} \\ 
  {\text{0}}&{\text{x}}&{\text{x}} 
\end{array}} \right)$, then similarly householder on the last column of the first two rows but keeps the 3rd row unchanged, and we have $\left( {\begin{array}{*{20}{c}}
  {\text{x}}&{\text{x}}&0 \\ 
  {\text{x}}&{\text{x}}&{\text{x}} \\ 
  {\text{0}}&{\text{x}}&{\text{x}} 
\end{array}} \right)$. I cannot figure out how to make the center element zero while preserving the zeros introduced earlier. For the second form, First apply householder reflectors to make it upper triangular $\left( {\begin{array}{*{20}{c}}
  {\text{x}}&{\text{x}}&{\text{x}} \\ 
  {\text{0}}&{\text{x}}&{\text{x}} \\ 
  {\text{0}}&{\text{0}}&{\text{x}} 
\end{array}} \right)$, then apply householder on the first row of the last two columns to have $\left( {\begin{array}{*{20}{c}}
  {\text{x}}&{\text{x}}&{\text{0}} \\ 
  {\text{0}}&{\text{x}}&{\text{x}} \\ 
  {\text{0}}&{\text{x}}&{\text{x}} 
\end{array}} \right)$, then householder again to make it $\left( {\begin{array}{*{20}{c}}
  {\text{x}}&{\text{x}}&{\text{0}} \\ 
  {\text{0}}&{\text{x}}&{\text{x}} \\ 
  {\text{0}}&{\text{0}}&{\text{x}} 
\end{array}} \right)$. I have the same problem to make the center element zero.","['numerical-linear-algebra', 'linear-algebra']"
2534761,Example of limit of a net of Borel functions not Borel,"I am stuck in the following problem on Reed and Simon functional analysis. Give an example to show that a pointwise limit of a net of Borel functions on $\mathbb{R}$ may not be Borel. First of all, that net cannot be a sequence in the usual sense, because from real analysis we have pointwise sequence limit of Borel functions is Borel. I try to find some non-Borel measurable function, like the characteristic function of this . Then I try to imagine if it could be written as limit of Borel function in some sense. However I'm not sure what type of net should I look for, and how to characterize the convergence of such a net.","['functional-analysis', 'real-analysis', 'borel-sets']"
2534794,Prove $\lim_{n\to\infty} 2^n \sqrt{2-x_n} = \pi$,"Given the sequence $x_1 = 0$, $x_{n+1} = \sqrt{2+x_n}$, proove:
$$\lim_{n\to\infty} 2^n \sqrt{2-x_n} = \pi$$ I have used the relations
$$2\cos({\frac x 2}) = \sqrt{2 + 2\cos(x)}$$
and
$$2\sin({\frac x 2}) = \sqrt{2 - 2\cos(x)}$$ Observe:
$x_1 = 0 = 2\cos(\alpha) \iff \alpha = \frac \pi 2$ I then hoped we can set $x_n = 2\cos(\frac{\alpha}{2^{n-1}})$ but if we can do this, why is that? I assummed we can, so: $\lim_{n\to\infty} 2^n \sqrt{2-x_n} = \lim_{n\to\infty} 2^n \sqrt{2-2\cos(\frac{\alpha}{2^{n-1}})} = \lim_{n\to\infty} 2^n \sqrt{4 \sin^2(\frac{\alpha}{2^n})}$ We know $$\alpha = \frac \pi 2$$ By substituting: $ = \lim_{n\to\infty} 2^{n+1} \sin(\frac{\pi}{2^{n+1}})$ Again, by substituting
$$\frac{\pi}{2^{n+1}} = \frac 1 m$$ $= \lim_{m\to\infty} \pi m \sin(\frac 1 m)= \pi$ Is this proof correct, and if not, how to prove it? The even more important question is the why is that? part mentioned above.","['algebra-precalculus', 'calculus']"
2534855,On the way of evaluate $\int_{0}^{\pi}\frac{dx}{1+2\sin^{2}x}$. [duplicate],"This question already has answers here : Evaluating $\int\limits_0^{\pi} \frac{dx}{1+2\sin^2x}$ (5 answers) Closed 6 years ago . Evaluate
  $$\int_{0}^{\pi}\frac{dx}{1+2\sin^2x}$$ My approach
$$\Longrightarrow\int_{0}^{\pi}\frac{dx}{1+2\sin^{2}x}=2\int_{0}^{\frac{\pi}{2}}\frac{\sec^{2}x}{1+3\tan^{2}x}dx=\frac{2}{\sqrt{3}}\left[\tan^{-1}\left(\sqrt{3}\tan x\right)\right]_{0}^{\frac{\pi}{2}}$$
$\tan x$ is undefined at $\dfrac{\pi}{2}$. So I need to solve $\lim_{x\rightarrow\frac{\pi}{2}}$$\left[\tan^{-1}\left(\sqrt{3}\tan x\right)\right]$. I don't know how to solve.","['definite-integrals', 'integration', 'trigonometry', 'calculus']"
2534923,Does SO(3) preserve the cross product?,"Let $g\in SO(3)$ and $p,q\in\mathbb{R}^3$ . I wondered whether it is true that $$g(p\times q)=gp\times gq$$ I am not sure how to prove this. I guess I will use at some point that the last row $g_3$ of $g$ can be obtained by $g_3=g_1\times g_2$ . But I assume there is an easier proof than writing everything out.","['orthogonal-matrices', 'cross-product', 'linear-algebra', 'linear-groups']"
2534942,Uniform Lipshitz continuity implies Continuous Differentiability,"Mallat in his book on wavelets makes the following definition - 
A function $f : [a,b] \to \mathbb{R}$ is $(C, \alpha)$-Lipschitz at $v \in [a, b]$ if there is a polynomial $p_v$ of degree at most $\lfloor \alpha \rfloor$ such that $|f(x) - p_v(x)| \leq C|x - v|^\alpha$ for any $x \in [a, b]$. It is uniformly Lipschitz if it is Lipschitz for all $v \in [a ,b]$ with a constant that is independent of $v$. 
Continuous differentiability implies Lipschitz continuity as above, since one can use the Taylor polynomial of the function. Question: Apparently, the converse is also true - uniform $(C, \alpha)$-Lipschitz continuity implies that the function is $\lfloor \alpha \rfloor$ times continuously differentiable. I'm having difficulty proving this. It is clear that if $\alpha > 1$ then the function is differentiable. I'm guessing the derivative is uniformly $\alpha-1$ Lipschitz which would complete the proof by induction, but I'm having trouble showing this. Any hints would be appreciated!","['derivatives', 'real-analysis', 'limits', 'continuity', 'lipschitz-functions']"
2534969,What is the significance of differential operators over other operators in group theory?,"When studying the representation of the group $SO(3)$ on the space $L^2(\mathbf{S}^2)$, we have the spherical functions as its basis: $$L^2(\mathbf{S}^2) = \text{span} \left \{ Y^l_m, l \in \mathbf{N}^+, -l \leqslant m \leqslant l \right \}$$
$$f(\theta,\varphi)=\sum_{\ell=0}^\infty \sum_{m=-\ell}^\ell f_\ell^m \, Y_\ell^m(\theta,\varphi)$$ But then suddenly the laplacian comes from nowhere and we says that its eigenfunctions are the spherical functions. I know it has important applications in physics, but from the group-theoretic point of view, is it just an operator like any other else?","['physics', 'laplacian', 'differential-operators', 'spherical-harmonics', 'group-theory']"
2535095,Proving set of polynomials is linearly independent,"If $n$ is a positive integer, is there an easy way to prove that the set $$\{(nx)^n,((n-1)x+y)^n, ((n-2)x+2y)^n,\ldots, (x+(n-1)y)^n, (ny)^n\}$$ of polynomials in two variables, $x$ and $y$, is linearly independent? I have verified it manually for $n=1,2,3$ by expanding and considering vectors of coefficients.","['polynomials', 'linear-algebra']"
2535113,"""Inverse"" homomorphism of a group monomorphism","Assume $\varphi:H\rightarrow G$ is a finite group monomorphism. Is it true that there exists a homomorphism
$$
\psi:G\rightarrow H
$$
such that
$$
\psi\circ\varphi=id_H?
$$","['abstract-algebra', 'group-theory']"
2535130,"Are the symbols ""$|$"" and ""$:$"" in the definition of a set exactly the same thing?","When I read some math books, I often see two different symbols to represent a specific condition for a set. Some of them use ""$|$"" before the condition, while others use ""$:$"". For example of the first case, we define the outer measure induced by $\mu$ as
\begin{align*}
\mu^*(E) = \inf \Big\{\sum_{i=1}^\infty \mu(E_i) \; \big| \; E_i \in \mathbb{R}, \; E \subset \bigcup_{i=1}^\infty E_i \Big\}
\end{align*}
that I find in a textbook where it uses ""$|$"" before the condition. As an example of the second case, we define the Sobolev space with $D^α$ denoting a weak derivative of order |α|
\begin{align*}
W_p^k(Ω) = \{ u ∈ L_p (Ω) : D^α u ∈ L_p (Ω), \; |α| ≤ k \} 
\end{align*}
that I find in a lecture note where it uses ""$:$"" before the condition. My question is quite simple: Are the symbols ""$|$"" and ""$:$"" here totally interchangeable with exactly the same meaning? Otherwise, do they emphasize on anything different? If they are the same, is the existence of two different notations due to some historical reason?","['math-history', 'notation', 'terminology', 'elementary-set-theory', 'definition']"
2535182,Elliptic pseudodifferential operator has at most finite number of non-positive eigenvalues,Let $P \in OPS^m(M)$ ($m$ can be negative of positive) be a self-adjoint elliptic pseudodifferential operator on a compact manifold $M$. How to show that $P$ has only finitely many non-positive eigenvalues (in $L^2(M)$)? Does it somehow follow from the Gårding inequality?,"['functional-analysis', 'pseudo-differential-operators', 'spectral-theory']"
2535224,Range of a function involving integration,"Find the range of the function for $\alpha \in \mathbb{R}$,
$$f(\alpha) = \int_{\tan^{-1}(\alpha)}^{\cot^{-1}(\alpha)}{\frac{\tan(x)}{\tan(x) + \cot(x)}}dx$$ My attempt: Please verify if my way of solution is correct. $$f(\alpha) = \int_{\tan^{-1}(\alpha)}^{\cot^{-1}(\alpha)}{\frac{\tan(x)}{\tan(x) + \cot(x)}}dx$$
$$\text{or } f(\alpha) = \int_{\tan^{-1}(\alpha)}^{\cot^{-1}(\alpha)}{2\sin^2(x)}dx$$
$$\text{or }\, f(\alpha) = \frac{1}{2}\int_{\tan^{-1}(\alpha)}^{\cot^{-1}(\alpha)}\left({1-\cos(2x)}\right)dx$$
$$\text{or }\, f(\alpha) = \frac{1}{2}\left({x-0.5\sin(2x)}\right)|_{\tan^{-1}(\alpha)}^{\cot^{-1}(\alpha)}$$
$$\text{or }\, f(\alpha) = \frac{\pi}{4} - \tan^{-1}{\alpha}$$ So range is $(\frac{-\pi}{4},\frac{3\pi}{4})$","['integration', 'inverse-function', 'functions']"
2535288,If two matrices have the same eigenvalues and eigenvectors are they equal?,"The question stems from a problem i stumbled upon while working with eigenvalues. Asking to explain why $A^{100}$ is close to $A^\infty$ $$A=
  \left[ \begin{array}{cc}
   .6 & .2 \\
   .4 & .8
  \end{array} \right]
$$ 
$$A^\infty=
  \left[ \begin{array}{cc}
   1/3 & 1/3 \\
   2/3 & 2/3
  \end{array} \right]
$$ 
Answer being given that (skipping calculations) that $A$ has eigenvalues $\lambda_1=1$ and $\lambda_2=0.4$ with eigenvectors $x_1=(1,2)$ and $x_2=(1,-1)$, and $A^{\infty}$ has eigenvalues $\lambda_1=1$ and $\lambda_2=0$, with same eigenvectors, while $A^{100}$ has eigenvalues $\lambda_1=1$ and $\lambda_2=(0.4)^{100}$ with same eigenvectors as the others, concluding that as the eigenvectors are the same and the eigenvalues are close comparing $A^\infty$ and $A^{100}$ they must be close. Creating the basis for my question, how can one conclude that two matrices with same eigenvectors and close/equal eigenvalues are close/equal to each other? My initial thoughts is that two matrices with equal eigenvectors and eigenvalues founds the basis for the same transformation which is why they are equal - Am I completly off?","['eigenvalues-eigenvectors', 'diagonalization', 'linear-algebra', 'linear-transformations']"
2535312,Do we need differentiability in problem 2-35 in Spivak?,"In Calculus on Manifolds by Spivak, Problem 2-35 goes as follows: If $f: \mathbb{R}^n \to \mathbb{R}$ is differentiable and $f(0) =0$,
  prove that there exist $g_i: \mathbb{R}^n \to \mathbb{R}$ such that
  $f(x) = \sum_{i=1}^n x^ig_i(x)$. My question is: do we really need differentiability? Can we just let
$$g_i(x)=\begin{cases} {f(x)}/\left({x^i\cdot\left|\{j|x^j\neq 0\}\right|}\right) & x^i\neq 0\\
0 & x^i=0 \end{cases}$$ so that $$x^ig_i(x)=\begin{cases} {f(x)}/\left({\left|\{j|x^j\neq 0\}\right|}\right) & x^i\neq 0\\
0 & x^i=0 \end{cases}$$ so $\sum_{i=1}^n x^ig_i(x)={f(x)}$?","['multivariable-calculus', 'differential-geometry', 'proof-verification']"
2535325,"Periods of periodic solutions of the (Hamiltonian) system $\dot{x}=y$, $\dot{y}=-x-x^2$","I'm preparing for a scholarship examination (no solutions available) and in older tests I'm coming across problems like the following. Consider the (Hamiltonian) system
  $$\begin{cases}\dot{x}=y \\ \dot{y}=-x-x^2 \end{cases} $$
  (a) Find the initial conditions under which the motion is periodic and estimate the set of possible periods; (b) Prove that there exist periodic orbits for which the average value of the position is not $0$ (if $x(t)$ is a periodic orbit of period $T$, the average value is defined as $T^{-1}\int_0^Tx(t)dt$). The approach I've been taught in my undergraduate ODE course is to draw a phase portrait given by the level sets of the Hamiltonian, which gives me full information on the support of the solutions. For instance, I can find the region of the plane which encloses all periodic solutions. However, I've never been taught how to study the graphs of solutions, or how to obtain quantitative results on the periods of periodic solutions. Can anyone give me an idea on how this could be accomplished? EDIT: Maybe an idea would be to study the linearized system about the critical point $(0,0)$, which is
$$\begin{cases}\dot{x}=y \\ \dot{y}=-x\end{cases} $$
in this case, the solutions can be explicitly calculated from $x(t)=a\cos t+b\sin t$, $a,b\in \mathbb{R}$, so their period is $2\pi$. Intuitively, I would say that the nonlinear system would also have periods $2\pi$ near the origin, but this still doesn't give me any estimate on the periods on the whole region.","['ordinary-differential-equations', 'dynamical-systems']"
2535350,How did Euler prove $\frac{d}{dx}e^{ix}=ie^{ix}$?,"Surely, $\frac{d}{dx}e^{ax}$ is $ae^{ax}$ when $a$ is a real number. However, it does not mean $\frac{d}{dx}e^{ax}$ is $ae^{ax}$ when $a$ is a complex number, I think. if he didn’t prove it, Euler’s formula is not a formula but a definition of $e^{ix}$. Please tell me whether Euler’s formula is a formula or a definition.
Additionally, I want to know whether Euler thought this equation as a formula or a definition when he found this.","['complex-analysis', 'calculus']"
2535396,Cases where ANY 2 of 3 +/- choices select one of four possible elements,"Edited 1/21/2018 to add the following : Here is a DropBox link https://www.dropbox.com/s/7rtt0iqmgimsgzu/Zumkeller_edge-magic.pdf?dl=0 to a PDF showing how my team used biomolecular first principles to arrive at a set of 240 biomolecular objects (which we believe to be an instantiation of the roots of $E_8$), and more generally, how we arrived at related sets of biomolecular objects with the cardinalities of the Zumkeller numbers (176,240,336) and the correponding edge-magic injection label numbers (11,15,21). The ""first principles"" which we used are those described in the original MSE question below. Original Question Reference: https://www.ncbi.nlm.nih.gov/pubmed/8054766 Background: The four DNA bases (t,c,a,g) and the four RNA bases (u,c,a,g) all share the following fundamental biochemical properties: 1) each base is a pyrimidine (Y) or purine (R) 2) each base is weak (W) or strong (S) 3) each base is keto (K) or amino (M) For the sake of this discussion, let: +Y = Y and -Y = R +W = W and -W = S +K = K and -K = M a = + or - Then the properties aY, aW, aK are distributed among the four bases (DNA or RNA) such that you only need to specify the value of a for two of these three properties in order to uniquely specify one of the four bases (DNA or RNA): t   c   a   g

Y    +   +   -   -   -R
W    +   -   +   -   -S
K    +   -   -   +   -M Question: What is the mathematical structure that best captures this redundancy in the specification of the four bases (DNA or RNA) by the three properties aY, aW, aK?","['combinatorics', 'combinatorial-geometry']"
2535402,Finite families of finite sets closed under union and intersection,"At first I thought that any finite family of finite sets closed under union and intersection must be a power set. But then I came up with the following counterexamples: $F_1=\{\emptyset, \{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},\{1,2,3\},\{1,2,3,4\} \}$. $F_2=\{\emptyset,\{1\},\{2\},\{3,4\},\{1,2\},\{1,3,4\},\{2,3,4\},\{1,2,3,4\}\}$. So my question is what types of finite families of finite sets are closed under union and intersection? Is there any literature on this subject? I could only find this: https://en.wikipedia.org/wiki/Ring_of_sets",['elementary-set-theory']
2535426,Find a set of parametric equations for the tangent line,"Find a set of parametric equations for the tangent line to the curve of intersection of the surface $x^2 + z^2 = 2$ and the surface $x^2 + y^2 - z^2 = 1$ at the point $(1, 1, 1)$.",['multivariable-calculus']
2535443,Problem with finding sum of arccot series [duplicate],"This question already has answers here : Explicitly finding the sum of $\arctan(1/(n^2+n+1))$ (2 answers) Closed 6 years ago . I have to find $$\sum_{n=1}^\infty \operatorname{arccot}(n^2-n+1)$$ and trying to compute sum of this series. I'm trying to take the derivative of member of the series and compute another sum, but this is not very successful. Is it right way to try take the derivative or it is useless in this situation? I can't see how to simplify this monster.","['algebra-precalculus', 'sequences-and-series', 'trigonometric-series']"
2535502,Geometric intuition and visualization of matrix exponential,"What is the geometric intuition and visualization of the matrix exponential $e^{\textbf{X}}$ ?
Does that explain why matrix exponential is only valid with square matrices ?","['matrices', 'matrix-exponential']"
2535507,Geometric proof of a trig identity on $\cos t \cos u\cos v$,"Consider the following trigonometric identity, valid for any set of angles $u,v,t$: $$\cos t⋅\cos u⋅\cos v =\frac14\left[\cos(t + u + v)+\cos(t + u - v)+\cos(u+v-t)+\cos(v + t - u)\right]$$ This identity and its derivation have previously appeared as a question on this site (though the version quoted in the question is in fact missing a term). But the solutions to the linked question, while valid, were all algebraic in nature.  Is there a geometric proof/demonstration of the above identity? I would also be satisfied with a proof-without-words of a special case e.g. $u=u+v$, $u+v+t=\pi$, $u+v+t=2\pi$, etc.","['trigonometry', 'alternative-proof', 'proof-without-words', 'geometry']"
2535522,Representation of elements by words on finite groups.,"Consider a finite group $G$ of order $n$, that is generated by two elements $a,b$. I would like to find some good upper bound $m$, such that for any element $g \in G$ there is an expression of $g$ by a word of some length shorter than $m$. By word, I mean a sequence of symbols on the alphabet $\{a,b,a^{-1},b^{-1}\}$. Clearly, this isnt true for non-finite groups - for any $j$, the free group on $\{a,b\}$ has elements that cannot expressed by a word of length less than $j$. For commutative groups finite groups, the answer is also clear - any element can be written as $a^ib^j$, and both $i,j$ are bound by the order of $n$. Is there some ""good"" bound for finite groups that are generated by two elements?","['finite-groups', 'group-theory', 'group-presentation']"
2535532,Continuity of the inverse Operator,"I was wondering whether the inverse Operator $A\mapsto A^{-1}$ is continuous on the set of continuously invertible linear operators $G\subset L(X)$, where X is supposed to be a Banach space (Can this may be weakend?). If this is not the case, do anyone has a counterexample. Thanks in advance. Best
Aristo",['functional-analysis']
2535555,Do all finite rings which are not fields have a non-trivial zero divisor?,"I know that any commutative ring without zero divisors must be a field, but if we have a finite ring, which isn't necessarily commutative or unital, must there exist a zero divisor?
I guess $\mathbb{Z}/n\mathbb{Z}$ for even $n$ will always have a zero divisor, but what about other finite rings?","['finite-groups', 'abstract-algebra', 'ring-theory']"
2535581,prove that $\displaystyle A\cdot B= 2^{-6}\bigg(\csc \frac{\pi}{22}-1\bigg)$,"If $\displaystyle A= \prod^{5}_{k=1}\cos \left(\frac{k\pi}{11}\right)$ and $\displaystyle B= \sum^{5}_{k=1}\cos \left(\frac{k\pi}{11}\right)$ then show that $\displaystyle A\cdot B= 2^{-6}\bigg(\csc \frac{\pi}{22}-1\bigg)$ Attempt: $\displaystyle B= \sum^{5}_{k=1}\cos \left(\frac{k\pi}{11}\right)=\cos \frac{\pi}{11}+\cos \frac{2\pi}{11}+\cos \frac{3\pi}{11}+\cos \frac{4\pi}{11}+\cos \frac{5\pi}{11}$ $\displaystyle =\frac{\sin \frac{5\pi}{22}}{\sin \frac{\pi}{22}}\bigg[\cos \bigg(\frac{\pi}{11}+4\frac{\pi}{22}\bigg)\bigg] =\frac{\sin \frac{5\pi}{22}}{\sin \frac{\pi}{22}}\bigg[\cos \frac{6\pi}{22}\bigg]=\frac{1}{2}\frac{-\sin(\frac{11\pi}{22})+\sin\frac{\pi}{11}}{\sin \frac{\pi}{22}}=\frac{1}{2}\bigg[-\csc \frac{\pi}{22}+1\bigg]$ could some help me how to solve it, thanks",['trigonometry']
2535591,Calculating Eigenvectors: Is my book wrong?,"I have a covariance matrix:
$$
   S= \begin{pmatrix}
    16 & 10 \\
    10 & 25
    \end{pmatrix}
$$ I calculate my eigenvalues correctly (the same as what the book finds); $\lambda_1 = 31.47$ , $\lambda_2 = 9.53$ But now it comes to calculating eigenvectors:
I do everything as I was taught way back in Elementary Linear Algebra. $S X = \lambda v$ {where v is the eigenvector} $(S - I \lambda)v$ Get Row-Echelon Form But when I do this I get the following reduced matrix: $$
    \begin{pmatrix}
    1 & -.646412 & 0 \\
    0 & 0 &0 
    \end{pmatrix}
$$ But this result doesn't seem consistent with my textbook which says that the eigenvectors are; $(0.54 , 0.84)^T$ and $(0.84 , -0.54)$ I looked online for calculators and found one consistent with the book and a few consistent with my result: Consistent with Book: http://comnuan.com/cmnn01002/ Consistent with Me: http://www.arndt-bruenner.de/mathe/scripts/engl_eigenwert2.htm Any ideas? Additional Information: This problem stems from Principal Component Analysis","['eigenvalues-eigenvectors', 'linear-algebra']"
2535622,Existence of second derivative implies symmetric second derivative,"I would like to know if the following statement is true. Let $f:(a,b)\subset\mathbb{R}  \rightarrow \mathbb{R}$, such $f'$
  exits for all $x \in (a,b)$ and also exists $f''(x_0)$ for some
  $a<x_0<b$. Then $ \lim_{h \to 0} \frac{f(x_0+h)-2f(x_0)+f(x_0-h)}{h^2} = f''(x_0)$. Obs: I'm not assuming that $f'$ is continuos in some neighborhood of $x_0$. However, I think that requirement of the existence of $f'$ in a neighborhood of $x_0$ is a necessity for the existence of $f''(x_0)$.","['derivatives', 'calculus']"
2535650,$P \in \mathbb C[X]$ such that $P(\mathbb R\setminus\mathbb Q) \subset \mathbb R\setminus\mathbb Q$ [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 6 years ago . Improve this question Which polynomials $P(X)$ in $\mathbb C[X]$ satisfy $5) \ P(\mathbb R\setminus\mathbb Q) \subset \mathbb R\setminus\mathbb Q$? The followings have already been solved: $1) \ P(\mathbb C) \subset \mathbb R$? $2) \ P(\mathbb U) \subset \mathbb U$ with $\mathbb U$ being the unit circle $\big\{z\in\mathbb{Z}\,\big|\,|z|=1\big\}$? $2') \ P(\mathbb U) = \mathbb U$? $3) \ P(\mathbb Z) \subset \mathbb Z$? $4) \ P(\mathbb Q) \subset \mathbb Q$? $4') \ P(\mathbb Q) = \mathbb Q$?","['real-analysis', 'polynomials', 'complex-numbers', 'abstract-algebra', 'complex-analysis']"
2535651,Is the Axiom of Replacement necessary to construct any function in ZFC?,"A while back, I read a source that I believe mentioned something along the lines of needing the Axiom of Replacement to define functions in ZFC Set Theory. I was convinced the entire time up to that point that to define a function $f \colon X \longrightarrow Y$, one mearly considers it as a subset of the product $X \times Y$, and thus only needs the Axiom of Powerset in order to define the Cartesian product, and the Axiom Schema of Comprehension to define subsets satisfying a particular predicate. However, the more that I think about it, the more I am confused in trying to define a function by a single predicate as a subset $f = \{(x, y) \in X \times Y \mid P(x, y)\}$, where $P(x,y)$ describes that each $x \in Y$ is associated to a $\textit{unique}$ $y \in Y$. So which is it? What is the Axiom of Replacement really needed for? Thanks in advance.","['axioms', 'set-theory', 'functions']"
2535654,Moment of inertia of a sphere,"My physics textbook said that the moment of inertia of a sphere is $\frac25mr^2$ where $m$ and $r$ are the mass and the radius of the sphere respectively. I wanted to verify the result by finding it myself, however, I always end up with $\frac35mr^2$ instead of $\frac25mr^2$. What is wrong with my method? My approach involved cutting up a sphere into an infinite amount of hollow sphere shells, each of which having volume $4\pi r^2dr$. Then, calling $d$ the density of the sphere and $R$ the radius, the moment of inertia should be:
$$\int_{0}^{R}4\pi r^2\cdot d\cdot r^2\cdot dr$$
Calculating this integral gives the following:
$$4\pi d\frac{R^5}5$$
Using $m=d\cdot\frac43\pi r^3$, we can simplify this to:
$$\frac35mR^2$$ This is clearly not the correct answer. Where have I gone wrong in my method? Is cutting the sphere into hollow shells conceptually wrong in the first place?","['physics', 'integration', 'calculus']"
2535676,Is there a stationary point process generated by i.i.d. waiting times of any distribution?,"Let $\mathcal{G}$ be the set of non-empty closed subsets of $\mathbb{R}$, equipped with its natural Polish topology, as induced by the metric
$$ d(A,B) \ = \ d_H\Big(\overline{\mathrm{arc}\tan(A)} \, , \, \overline{\mathrm{arc}\tan(B)}\Big) $$
where $d_H$ is the Hausdorff metric. For any probability measure $\nu$ on $(0,\infty)$ and any $t \in \mathbb{R}$, define the probability measure $\mathbb{P}_{\nu,t}$ on $\mathcal{G}$ to be the image measure of $\nu^{\otimes \mathbb{N}}$ under the map
$$ \hspace{40mm} (x_i)_{i \geq 1} \mapsto \left\{ t + \sum_{i=1}^n x_i \, : \, n \geq 1 \right\} \hspace{6mm} \textrm{(defined $\nu^{\otimes \mathbb{N}}$-almost everywhere).}$$ Is it the case that for every probability measure $\nu$ on $(0,\infty)$, $\,\mathbb{P}_{\nu,t}$ is weakly convergent as $t \to -\infty\,$? For example, in the case that $\nu=\mathrm{exponential}(\lambda)$, I believe we have that $\mathbb{P}_{\nu,t}$ converges weakly to the law of a two-sided Poisson process of intensity $\lambda$ as $t \to -\infty$. But this is the ""easy case"", since a one-sided Poisson point process is already stationary (as a one-sided process).","['stochastic-processes', 'probability-theory']"
2535687,"2,3,5,6,7,10,11 Counting with Restrictions","The sequence 2, 3, 5, 6, 7, 10, 11, $\ldots$ contains all the positive integers from least to greatest that are neither squares nor cubes nor perfect fifth powers (in the form of $x^{5}$, where $x$ is an integer). What is the $1000^{\mathrm{th}}$ term of the sequence? I've seen problems where I need to count with restrictions (like cubes and squares). I have never seen a problem with this degree.  Here is my thought process. Find intersection of squares and cubes. This is simple enough. For every 6th square, there will lie a cube. We can do this for everything else (intersection of 2 and 4, 2 and 5, 3 and 4, 3 and 5), but it will be tedious. Now, count out the numbers. This solution is very tedious...Can someone guide me through the solution?",['combinatorics']
