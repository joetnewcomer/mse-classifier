question_id,title,body,tags
2861461,"Calculate $\int_{S_A} x^T B x \, dx$ where $S_A$ is an ellipsoid","Suppose that $S_A \subset \mathbb{R}^n$ denotes an ellipsoid centered at the origin, \begin{align}
S_A= \{ x \in \mathbb{R}^n :  x^T A x \le 1    \} ,
\end{align} where matrix $A$ is symmetric and positive definite. Let matrix $B$ be positive definite.  Can the following integral be calculated in closed form? \begin{align}
\int_{S_A}   x^T B x  \, dx
\end{align} I understand we have to switch to spherical coordinates here, but there a few details that are not clear to me.  For example, how to change the coordinates in this case.","['integration', 'ellipsoids', 'definite-integrals', 'multivariable-calculus', 'quadratic-forms']"
2861464,Question about series rearrangement in Baby Rudin (theorem 3.54).,"I have trouble following the following theorem in Rudin: Let $\sum a_n$ be a series of real numbers which converges, but not absolutely. Suppose $$-\infty \leq \alpha \leq \beta \leq +\infty.$$ Then there exists a rearrangement $\sum a_n^\prime$ with partial sums $s_n^\prime$ such that $$\lim_{n\to\infty}\inf s_n^\prime = \alpha, \ \ \ \mbox{ and } \ \ \ \lim_{n\to\infty}\sup s_n^\prime = \beta. \tag{24}$$ Here's the proof: Let $$p_n = \frac{|a_n| + a_n}{2}, \ q_n = \frac{|a_n| - a_n}{2} \ (n = 1, 2, 3, \ldots). $$ Then $p_n - q_n = a_n$ , $p_n + q_n = |a_n|$ , $p_n \geq 0$ , $q_n \geq 0$ . The series $\sum p_n$ , $\sum q_n$ must both diverge. For if both were convergent, then $$\sum \left( p_n + q_n \right) = \sum |a_n|$$ would converge, contrary to hypothesis. Since $$ \sum_{n=1}^N a_n = \sum_{n=1}^N \left( p_n - q_n \right) = \sum_{n=1}^N p_n - \sum_{n=1}^N q_n,$$ divergence of $\sum p_n$ and convergence of $\sum q_n$ (or vice versa) implies divergence of $\sum a_n$ , again contrary to hypothesis. Now let $P_1, P_2, P_3, \ldots$ denote the non-negative terms of $\sum a_n$ , in the order in which they occur, and let $Q_1, Q_2, Q_3, \ldots$ be the absolute values of the negative terms of $\sum a_n$ , also in their original order. The series $\sum P_n$ , $\sum Q_n$ differ from $\sum p_n$ , $\sum q_n$ only by zero terms, and are therefore divergent. We shall construct sequences $\{m_n \}$ , $\{k_n\}$ , such that the series $$ P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} + \cdots \tag{25}, $$ which clearly is a rearrangement of $\sum a_n$ , satisfies (24). Choose real-valued sequences $\{ \alpha_n \}$ , $\{ \beta_n \}$ such that $\alpha_n \rightarrow \alpha$ , $\beta_n \rightarrow \beta$ , $\alpha_n < \beta_n$ , $\beta_1 > 0$ . Let $m_1$ , $k_1$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} > \beta_1,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} < \alpha_1;$$ let $m_2$ , $k_2$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} > \beta_2,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} < \alpha_2;$$ and continue in this way. This is possible since $\sum P_n$ , $\sum Q_n$ diverge. If $x_n$ , $y_n$ denote the partial sums of (25) whose last terms are $P_{m_n}$ , $-Q_{k_n}$ , then $$ | x_n - \beta_n | \leq P_{m_n}, \ \ \ |y_n - \alpha_n | \leq Q_{k_n}. $$ Since $P_n \rightarrow 0$ , $Q_n \rightarrow 0$ as $n \rightarrow \infty$ , we see that $x_n \rightarrow \beta$ , $y_n \rightarrow \alpha$ . Finally, it is clear that no number less than $\alpha$ or greater than $\beta$ can be a subsequential limit of the partial sums of (25) . I don't understand the last two lines of the proof (put in bold). I'm aware that this question was already asked on this forum, but I didn't understand the answers provided in this question, so that's why I'm writing my own question. Thanks in advance.","['proof-explanation', 'sequences-and-series', 'real-analysis']"
2861487,"How does one complexify a real $n$-dimensional Riemannian manifold $(M,g)$?","If $V$ is a real vector space, then the complexification of $V$ is formally defined as $V^{\mathbb{C}}=V\otimes_{\mathbb{R}}\mathbb{C}$. Is there an analogous complexification operation for a real $n$-dimensional Riemannian manifold $(M,g)$? Idea: The notion of complexification for Lie groups exists, so perhaps one can ""complexify"" a real Riemannian manifold by realizing it as a Lie group (or the quotient of one). It seems that under complexification of a real manifold some additional information must be added to determine a complex structure. The reason I ask this is because I am looking through the Riemannian holonomy section of this article and it states that ""the complexified holonomies $SO(n,\mathbb{C})$, $G_2(\mathbb{C})$, and $Spin(7,\mathbb{C})$ may be realized from complexifying real analytic Riemannian manifolds."" Any help would be much appreciated!","['riemannian-geometry', 'differential-geometry']"
2861500,Why the transpose of a singular matrix is singular? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have to prove this lemma without using the concept of rank neither the concept of determinant: $A$ is a singular matrix iff $A^T$ is singular Unfortunately i've only found proofs that contains rank and determinant. Can you help me ?",['linear-algebra']
2861557,Proof on positive sequence with $\limsup_{n} a_n^{1/n}=1$ and $\liminf_{n}a_n^{1/n} <1$,"Does a positive sequence $\{a_n\}$ with $\limsup_{n} a_n^{1/n}=1$ and $\liminf_{n}a_n^{1/n} <1$ must have a subsequence $\{a_{n_i}\}$ satisfying $\lim_{i} a_{n_i}^{1/n_i}=1$ and $\lim_{i} |a_{n_i}^2-a_{n_i-1}a_{n_i+1}|^{1/n_i}=1$. So Here are the hypothesis:
\begin{equation} \limsup_{n} a_{n}^{1/n}=1 \tag1 \end{equation}
and 
\begin{equation} \liminf_{n} a_{n}^{1/n}<1 \tag2 \end{equation}
How to derive the conclusion: there is a subsequence $\{a_{n_i}\}$ fulfilling both of
\begin{equation} \lim_{i} a_{n_i}^{1/n_i}=1 \tag3 \end{equation}
and
\begin{equation} \lim_{i} |a_{n_i}^2-a_{n_i-1}a_{n_i+1}|^{1/n_i}=1 \tag4 \end{equation} My understanding to this problem It is obvious that $(1)$ implies $(3)$, but how $(1)$ together with $(2)$ imply $(3)$ and $(4)$ is not so clear, although trivial examples such as
$$1,\delta^2,1,\delta^4,\cdots,1,\delta^{2n},\cdots$$
(where $0<\delta<1$) strongly support this proposition. Given $(3)$ holds, $(4)$ may be replaced with the equivalent
\begin{equation}\lim_{i} \left|1-\frac{a_{n_i+1}}{a_{n_i}} \frac{a_{n_i-1}}{a_{n_i}}\right|^{1/n_i} =1 \end{equation} So this suggests that $\dfrac{a_{n_i+1}}{a_{n_i}} \dfrac{a_{n_i-1}}{a_{n_i}}$ must be ""small"" enough. It seems somehow related to the ratio test vs the root test in convergence of series. What I expect A proof (or a counter-example) is of course appreciated, but I also want to know the origin of this problem (in what literature did it emerge). Discussions giving an insight to this problem is welcomed as well.",['sequences-and-series']
2861588,Why the Jauge of $C$ is $\inf\{r>0\mid v/r\in C\}$ and not $\sup\{r>0\mid rv\in K\}$?,"Why the Jauge $p:\mathbb R^n\to \mathbb R$ of a set $C\subset \mathbb R^n$ is defined as $$p(v)=\inf\{r>0\mid v/r\in C\}$$ and not as $$p(v)=\inf\{r>0\mid rv\notin C\} \quad \text{or}\quad p(v)=\sup\{r>0\mid rv\in C\}\ \ ?$$ Because both explain the same concept as : the first time we enter in $C$ or go out of $C$, and it looks more easy to work with $rv$ than with $\frac{v}{r}$. So I was wondering what is the motivation to use $\frac{v}{r}$ instead of $rv$.",['functional-analysis']
2861612,"exponential of second derivative, not positive?","I'm interested in whether the following function $h(x,y)=e^{\partial_x\partial_y}f(x)g(y)$, is positive in the sense that $h(x,y)\geq 0$, whenever $f(x)\geq 0$ and $g(y)\geq 0$ (positive and real). I would have thought that it was positive because the derivatives are in the exponential, but it appears not to be. Any idea why it isn't positive? Maybe because ${\partial_x\partial_y}$ is not a normal operator? But I think it is even self-adjoint (or at least symmetric), so should have real eigenvalues. To see that it is not positive one can expand the exponential $h(x,y)=f(x)g(y)+f'(x)g'(y)+\frac{1}{2}f''(x)g''(y)+...$ and take (for example) $f(x)=x^2$ and $g(y)=y^2$. Then the series terminates at second order and is $(xy+2)^2-2$, which is negative at $x=-\sqrt{2}$, $y=\sqrt{2}$ etc. To see that $\partial_x\partial_y$ is symmetric we have $\int dx dy \psi^*(x,y)\partial_x\partial_y \phi(x,y)=\int dx dy \partial_x\partial_y \psi^*(x,y) \phi(x,y)$ after integrating by parts twice, plus a boundary term. So for the boundary term to vanish we should have that $\psi^*(x,y)$ is zero at infinity, so maybe  $e^{\partial_x\partial_y}f(x)g(y)$ is positive on functions which decay to zero  at infinity? This is not the case with polynomials which are the examples I know that give negative values.","['multivariable-calculus', 'linear-algebra', 'functional-analysis', 'derivatives', 'exponential-function']"
2861616,Inner regular content is premeasure,"Let $(X,\mathcal{T})$ be a topological space and $\mu$ be a content on a semiring $\mathfrak{J}$ over $X$. Suppose that $\mu$ is inner regular, i.e., for every $\epsilon > 0$, $A \in \mathfrak{J}$, there is a $K\in \mathfrak{J}$ such that $\overline{K}$ is compact, $\overline{K} \subseteq A$ and $\mu(A)\leq \mu(K)+\epsilon$. I want to show that this implies that $\mu$ is a premeasure. My attempt: First of all, I showed that if $\mu$ is an inner regular content on $\mathfrak{J}$, then its extension to the ring $\mathfrak{R}$ (which I denote by $\mu$ as well) generated by $\mathfrak{J}$ is inner regular as well. This allows us to use the following lemma: Lemma: Let $\mu$ be a content on a ring $\mathfrak{R}$. If for every sequence $(A_n)_{n\in\mathbb{N}}$ of sets in $\mathfrak{R}$ satisfying $\mu(A_1) < \infty$ and $A_n \downarrow \emptyset$ we have $\mu(A_n)\downarrow 0$, then $\mu$ is a premeasure on $\mathfrak{R}$. So, in order to use the lemma, let $A_n$ be defined as in the lemma. Take $K_n \in \mathfrak{R}$ such that $\overline{K_n}$ is compact, $\overline{K_n} \subseteq A_n$ and $\mu(A_n) \leq \mu(K_n) + \epsilon$ for all $n \in \mathbb{N}$. We have $$\bigcap_{n=1}^{\infty} \overline{K_n} \subseteq \bigcap_{n=1}^{\infty} A_n = \emptyset.$$
Due to the compactness of the $\overline{K_n}$, this means that the family $(\overline{K_n})_{n\in\mathbb{N}}$ does not have the finite intersection property. Hence, there is a $N \in \mathbb{N}$ such that 
$$ \bigcap_{n=1}^{N} K_n \subseteq \bigcap_{n=1}^{N} \overline{K_n} = \emptyset.$$
I've tried to use $\cap_{n=1}^{N} A_{n} = A_{N}$ and estimate $\mu(\cap_{n=1}^{N} A_n)$ by something depending on $\mu(\cap_{n=1}^{N} K_n)$ plus something depending on epsilon, but without success. I've also tried to make the $K_n$ disjoint, that is, define $C_n := K_n \setminus \cup_{i=1}^{n-1}K_i$ and then use that $$\mu(A_1) \geq \mu\left(\bigcup_{n=1}^{N} C_n\right) = \sum_{n=1}^{N} \mu(C_n).$$ As $N$ was arbitrary, this shows that $\mu(C_n) \to 0$, but I've been unable to conclude from this.","['general-topology', 'measure-theory', 'analysis', 'borel-measures']"
2861630,"If matrix $A$ has entries $A_{ij}=\sin(\theta_i - \theta_j)$, why does $\|A\|_* = n$ always hold?","If we let $\theta\in\mathbb{R}^n$ be a vector that contains $n$ arbitrary phases $\theta_i\in[0,2\pi)$ for $i\in[n]$, then we can define a matrix $X\in\mathbb{R}^{n\times n}$, where
\begin{align*}
X_{ij} = \theta_i - \theta_j.
\end{align*}
Then the matrices that I consider are the antisymmetric matrix $A=\sin(X)$ and the symmetric matrix $B=\cos(X)$. Through numerical experiments (by randomly sampling the phase vector $\theta$) I find that the nuclear norm of $A$ and $B$ are always $n$, i.e.
\begin{align*}
\|A\|_* = \|B\|_* = n.
\end{align*} Moreover, performing SVD on $A$ yields the largest two singular value $\sigma_1 = \sigma_2 = n/2$ and all the other $\sigma_3 = \ldots = \sigma_n = 0$. Further, if we look at the matrix $A\circ B$, where
\begin{align*}
(A\circ B)_{ij} = \sin(\theta_i - \theta_j)\cos(\theta_i - \theta_j) = \sin(2(\theta_i - \theta_j))/2,
\end{align*}
then 
\begin{align*}
\|A\circ B\|_* = n/2
\end{align*}
with $\sigma_1 = \sigma_2 = n/4$ and $\sigma_3 = \ldots = \sigma_n = 0$. Is there any way to see why $A$ and $B$ have these properties?","['nuclear-norm', 'eigenvalues-eigenvectors', 'linear-algebra', 'trigonometry', 'spectral-theory']"
2861708,Issue with $\int_0^{\infty}\frac{\cos(x^2)}{x^2}dx$,"I have stumbled into a issue with  gamma function, I will show the approach first: 
$$\Gamma(x)=\int_0^{\infty}t^{x-1}e^{-t}dt$$ subtituting $t=iu^2$ gives:
$$\Gamma(x)=2\int_0^{\infty}(iu^2)^{x-1}e^{-iu^2}iudu\rightarrow\frac{\Gamma(x)}{2i^x}=\int_0^{\infty}u^{2x-1}e^{-iu^2}du$$ Doing the same thing using $t=-iu^2\,$results in$$\frac{\Gamma(x)}{-2i^x}=\int_0^{\infty}u^{2x-1}e^{iu^2}du$$ Now, summing those two  and using that $i^x=e^{\frac{i\pi}{2}x} \,$gives
$$\frac{\Gamma(x)}{2}(e^{\frac{i\pi}{2}x}+e^{\frac{-i\pi}{2}x})=\int_0^{\infty}u^{2x-1}(e^{iu^2}+e^{-iu^2})du$$ which is just $$\frac{\Gamma(x)}{2}\cos(\frac{\pi}{2}x)=\int_0^{\infty}u^{2x-1}\cos(u^2)du$$ plugging $x=-\frac{1}{2}$ we get that$$\int_0^{\infty}\frac{\cos(x^2)}{x^2}dx=-\sqrt{\frac{\pi}{2}}$$ Well, obviously this integral diverges... But if Instead of summing we subtract we get that $$\frac{\Gamma(x)}{2}\sin(\frac{\pi}{2}x)=\int_0^{\infty}u^{2x-1}\sin(u^2)du$$ simmilarly with $$x=-\frac12 \rightarrow \int_0^{\infty}\frac{\sin(x^2)}{x^2}dx=\sqrt{\frac{\pi}{2}}$$ So its not that completely garbage. Now my question is, what goes wrong when I use the first substitution? And how do I prove that I am allowed to use this substitution for the sine integral?","['integration', 'complex-analysis', 'substitution']"
2861742,Define $*$ on $\mathbb{Z}$ by $a*b = a+b$. Show $*$ is a binary operation on $\mathbb{Z}^+$.,"Let $S = \mathbb{Z}^+$ . Define $*$ on $\mathbb{Z}$ by $a*b = a+b$ . Show $*$ is a binary operation on $\mathbb{Z}^+$ . In our course on Abstract Algebra our book says the following in it. $*$ is a binary operation on $\mathbb{Z}^+$ . Because $*$ is well defined: if $(a, b) = (c, d)$ , then $a = c$ and $b=d$ . Hence we have $$*(a, b)= a+b = c+d = *(c, d)$$ $\mathbb{Z}^+$ is closed under $*$ . If $a, b \in \mathbb{Z}^+$ , then $a*b = a+b \in \mathbb{Z}^+$ The above seems wrong to me. I see two errors. Firstly if $*$ is a well-defined function then there is no need to show that $\mathbb{Z}^+$ is closed under $*$ and secondly they haven't actually shown well definedness of $*$ , since all they've done is just take two equal elements in $\mathbb{Z}^+ \times \mathbb{Z}^+$ and shown their outputs are equal. Now the way I see it, all I have to show is that $* : \mathbb{Z}^+ \times \mathbb{Z}^+ \to \mathbb{Z}^+$ defined by $$*(a, b) = a*b = a+b$$ is actually a well-defined function. To do this (in more familiar notation compared to the answer given here: https://math.stackexchange.com/a/313182/266135 ) I need to show the following $*(a, b) = a+b \in \mathbb{Z}^+$ for all $(a, b) \in \mathbb{Z}^+ \times \mathbb{Z}^+$ $\forall (x, y) \in \mathbb{Z}^+ \times \mathbb{Z}^+$ , there exists a $z \in \mathbb{Z}^+$ such that $*(x, y) = x+y =z$ If $*(a, b) =c$ and $*(a, b)=d$ then $c=d$ . Now 1. above is satisfied easily since we know that by properties of the integers that the addition of any two positive integers is again a positive integer. Since 1. is satisfied 2. is redudant and there's nothing to prove (please correct me if I am wrong on this). So all that's left to prove is 3. To prove 3. all I'd need to do is suppose that $*(a, b) = c$ and $*(a, b) = d$ , then I'd have $a+b=c$ and $a+b=d$ , but then trivially I'd have $c=d$ (because if you add the same two positive integers $a, b$ together then you get the same unique result $a+b$ always). Am I correct in everything I've said?","['elementary-set-theory', 'binary-operations', 'functions']"
2861764,Limit of $\frac{1}{\ln n}\sum_{j=1}^n\frac{x_j}{j}$ in two different cases.,"I have two related problems: (1) For a sequence $\{x_n\}$ with $\lim_{n \to \infty} x_n = X$, show that $\frac{1}{\ln n}\sum_{j=1}^n\frac{x_j}{j}$ converges to $X$. (2) For a sequence $\{x_n\}$ with convergent arithmetic means, $\lim_{n \to \infty} \frac{1}{n} \sum_{j=1}^n x_j = X$, show that $\frac{1}{\ln n}\sum_{j=1}^n\frac{x_j}{j}$ converges to $X$. Attempt: For (1) I used the Cesaro Stolz theorem: $$\lim_{n \to \infty}\frac{1}{\ln n}\sum_{j=1}^n\frac{x_j}{j} = \lim_{n \to \infty}\frac{\frac{x_{n+1}}{n+1}}{\ln(n+1)-\ln(n)} = \lim_{n \to \infty}\frac{x_{n+1}}{\ln(1+1/n)^{n+1}} = \frac{X}{\ln(e)} = X$$ For (2) I know if all $x_n > 0$ then $\frac{1}{\ln n}\sum_{j=1}^n\frac{x_j}{j} < \frac{n}{\ln n}\frac{1}{n}\sum_{j=1}^n x_j$ but this does not seem to help.  Also it is not given that all $x_n >0$ Thank you for any assistance.","['convergence-divergence', 'sequences-and-series']"
2861768,Calculate $\lim_{n \rightarrow \infty } \frac{n (1- na_n)}{\log n} $,"Given the recursive sequence $\{a_n\}$ defined by setting $0 < a_1 < 1, \; a_{n+1} = a_n(1-a_n) , \; n \ge 1 $ Calculate :  $$\lim_{n \rightarrow \infty } \frac{n (1- na_n)}{\log n} $$ My attempts :  $$\lim_{n \rightarrow \infty } \frac{n (1- n a_n)}{\log n} =\lim_{n \rightarrow \infty }\frac {n \left (\frac{1}{n a_n} -1 \right) n a_n} {\log n}= \lim_{n \rightarrow \infty }  \frac {\frac{1}{a_n} - n}{ \log n}$$ Now I am not able to proceed further. Please help me. Thank You.","['limits', 'real-analysis']"
2861815,Differences between homeomorphic and topologically conjugate dynamical systems.,"I have begun studying homeomorphisms between dynamical systems. I have a few related questions about homeomorphic and topologically conjugate dynamical systems. Questions How can I determine if a homeomorphism is orientation presevering? If two dynamical systems are Topologically Conjugate (that is $f \circ h=h \circ g$), does this imply that $h$ is an orientation preserving homeomorphism? (Answered see comment below this post) What properties are preserved between two topologically conjugate dynamical systems, compared to two dynamical systems which are only homeomorphic to each other? Definitions $f:X \rightarrow X$, $g:Y \rightarrow Y$ and $h:Y \rightarrow X$ are continuous functions on smooth orientable manifolds, $X$ and $Y$. Topologically Conjugate: $f \circ h=h \circ g$ and $h$ is homeomorphsim. Homeomorphism What it means for a manifold to be orientable . Notes Partial answers are appreciated. If you need any clarification please ask.","['differential-topology', 'dynamical-systems', 'differential-geometry']"
2861926,x$A^{100 }$ where $A = \begin{bmatrix} 1 &2 \\ 3& 4 \end{bmatrix}$ [duplicate],"This question already has answers here : Finding a 2x2 Matrix raised to the power of 1000 (6 answers) Closed 5 years ago . Compute $A^{100 }$ where   $A = \begin{bmatrix}  1 &2 \\ 3& 4 \end{bmatrix}$. I can  calculate $A^{100}$  using a calculator, but  my question  is that  is  there  any  short formula/method   or  is their any trick  to find the  $A^{100}$?",['matrices']
2861941,"Find the second degree Taylor polynomial of $f(x,y)=e^{-x^2-y^2}cos(xy)$ at $x_0=0$, $y_0=0$.","I would like to either confirm my solution is correct, or find the error in it. I used the following MATLAB code to try to check my answer, but the solution it gave differs from mine. f = exp(-(x^2+y^2)) cos(x y); taylor(f,[x,y],[0,0]) ans = x^4/2 + (x^2*y^2)/2 - x^2 + y^4/2 - y^2 + 1 My solution: The second degree Taylor polynomial at the point $(a,b)$ is given by
$$p_2(x,y)=f(a,b)+Df(a,b)\left[\begin{array}{c}
x-a\\y-b
\end{array}\right]+\frac{1}{2}[x-a\mbox{ }y-b]Hf(a,b)\left[\begin{array}{c}
x-a\\y-b
\end{array}\right]$$
where $Df(a,b)$ is the Jacobian matrix and $Hf(a,b)$ is the Hessian matrix (i.e. first and second order partial derivatives, respectively). Thus, we begin by computing the partial derivatives:
\begin{equation*}
\begin{split}
\frac{\partial f}{\partial x}(x,y)=-e^{-x^2-y^2}(y\mbox{ sin}(xy)+2x\mbox{ cos}(xy))\\
\frac{\partial f}{\partial y}(x,y)=-e^{-x^2-y^2}(x\mbox{ sin}(xy)+2y\mbox{ cos}(xy))\\
\frac{\partial^2 f}{\partial x^2}(x,y)=e^{-x^2-y^2}((4x^2-y^2-2)\mbox{cos}(xy)+4xy\mbox{ sin}(xy))\\
\frac{\partial^2 f}{\partial y^2}(x,y)=e^{-x^2-y^2}(4xy\mbox{ sin}(xy)-(x^2-4y^2+2)\mbox{cos}(xy))\\
\frac{\partial^2 f}{\partial x\partial y}(x,y)=\frac{\partial^2 f}{\partial y\partial x}(x,y)=e^{-x^2-y^2}((2x^2+2y^2-1)\mbox{sin}(xy)+3xy\mbox{ cos}(xy))
\end{split}
\end{equation*}
Then, at the point $(x_0,y_0)=(0,0)$,
\begin{equation*}
\begin{split}
f(0,0)=1\\
\frac{\partial f}{\partial x}(0,0)=\frac{\partial f}{\partial y}(0,0)=0\\
\frac{\partial^2 f}{\partial x^2}(0,0)=\frac{\partial^2 f}{\partial y^2}(0,0)=-2\\
\frac{\partial^2 f}{\partial x\partial y}(0,0)=\frac{\partial^2 f}{\partial y\partial x}(0,0)=0
\end{split}
\end{equation*}
Therefore, the second degree Taylor polynomial at the point $(0,0)$ is
\begin{equation*}
\begin{split}
p_2(x,y)=f(0,0)+Df(0,0)\left[\begin{array}{c}
x-0\\y-0
\end{array}\right]+\frac{1}{2}[x-0\mbox{ }y-0]Hf(0,0)\left[
\begin{array}{c}
x-0\\y-0
\end{array}\right]\\
=1+[0\mbox{ }0]\left[
\begin{array}{c}
x\\y
\end{array}\right]+\frac{1}{2}[x\mbox{ }y]\begin{bmatrix}
-2&0\\0&-2
\end{bmatrix}\left[
\begin{array}{c}
x\\y
\end{array}\right]=1-x^2-y^2
\end{split}
\end{equation*}","['multivariable-calculus', 'proof-verification', 'matlab']"
2861966,Determining the last column so that the resulting matrix is an orthogonal matrix,"Determine the last column so that the resulting matrix is an orthogonal
  matrix $$\begin{bmatrix} \dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{6}} & ? \\ \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{6}} & ? \\ 0 & \dfrac{2}{\sqrt{6}} & ? \end{bmatrix}$$ Can anyone please provide hints to solve this?","['matrices', 'orthogonal-matrices', 'linear-algebra']"
2861970,Is injective co-isometry an isometry?,"Let $E$ be a normed space. Let $T:E\to E$ be an injective continuous linear map, such that $T^{*}$ is an isometry. Does it follows that $T$ itself is an isometry (in fact it is then an isometric isomorphism)? This is true if $E$ is reflexive. Indeed, if $T$ is injective, $T^{*}$ has a dense range. An isometry with a dense range must be an isometric isomorphism, and so $T=T^{**}$ is also an isometric isomorphism. However, if $E$ is not reflexive we cannot conclude that $T^{*}$ has a dense image, only weak* dense, and so I expect that there is a counterexample.","['banach-spaces', 'functional-analysis']"
2861980,Exponential conditional probability,"There are two types of claims that are made to an insurance company. Let $N_i(t)$ denote the number of type $i$ claims made by time $t$, and suppose that $\{N_1(t), t \ge 0\}$ and $\{N_2(t), t \ge 0\}$ are independent Poisson processes with rates $λ_1 = 10$ and $λ_2 = 1$. The amounts of successive type 1 claims are independent exponential
random variables with mean 1000 dollars whereas the amounts from type 2 claims are independent exponential random variables with mean 5000 dollars. A claim for 4000 dollars has just been received; what is the probability it is a type 1 claim? Here's my approach to the problem: \begin{align}
& P(\text{claim} = \text{type 1}\mid 4000) \\[10pt]
= {} & \frac{P(4000\mid \text{claim} = \text{type 1})(\text{claim} = \text{type 1})}{P(4000\mid \text{claim} = \text{type 1})P(\text{claim} = \text{type 1}) + P(4000\mid\text{claim} = \text{type 2})P(\text{claim} = \text{type 2})}
\end{align} by Bayes' formula. My question is, how do I calculate $P(4000\mid \text{claim} = \text{type 1})$ and $P(4000\mid \text{claim} = \text{type 2})$? The dollar value of claims is exponentially distributed, and since the distribution is continuous I can't fix a value to the pdf, I need a range. Any tips on how to calculate these two values?","['exponential-distribution', 'statistics', 'bayes-theorem', 'probability']"
2861998,$A$ is a symmetric matrix such that $A^4=A$. Prove that $A$ is idempotent,"Let $A$ be a real symmetric matrix such that $A^4=A$. Prove that $A$ is idempotent. I have tried using eigenvalues and only inferred that the eigen values may be $0,1$. But I cannot proceed with this.","['idempotents', 'matrices', 'linear-algebra', 'symmetric-matrices', 'matrix-equations']"
2862028,Why is $\sin(x+y)=c$ not a smooth curve for $c = \pm 1$?,"I understand that, for $c= 1$ and $c=-1$, the derivative of the function $f(x,y)=\sin(x+y)-c$ is a zero vector, and hence the locus which is given by $\sin(x+y)-c=0$ fails to satisfy the definition of the smooth curve. However, I have the following doubt: Suppose $c = 1$. Then, $$ \sin(x+y) = 1  \implies y = -x + \arcsin(1)$$ So, $$y = -x + (4n+1)\pi/2, \quad\forall n\in\mathbb{Z}$$ satisfies the equation. These are a set of parallel lines.
But, why is it failing to satisfy the definition of a smooth curve? I tried to plot the function for $c=1$ in WolframAlpha, but the plot did not contain any points. Most likely, I am doing some silly mistake in the above calculations. Can anyone help in identifying the flaw in my thinking?","['multivariable-calculus', 'algebra-precalculus', 'trigonometry']"
2862050,The normalizer of permutation,"Let $\sigma = (1 2 \dots 9) \in S_{10}$ . a) Calculate the size of the normalizer $N_{S_{10}}(<\sigma >)$ . b) Describe exactly the elements in $N_{S_{10}}(<\sigma >)$ . I am not sure how to approach this. I understand that we look for permutations that fixes $10$ , and yet I can't see what to do further... I know that $\tau \sigma \tau^{-1} =(\tau(1) \dots \tau(9))$ by definition, and also that for $\tau$ to be in $N_{S_{10}}(<\sigma >)$ than it is required that $\tau \sigma \tau^{-1} =(\tau(1) \dots \tau(9)) = \sigma^i$ for some $i$ . How can I continue from here?","['permutations', 'group-theory', 'normal-subgroups']"
2862063,Advice on thinking as an algebraist or topologist to prove theorems in analysis,"Browsing through stackexchange, I'm often struck by the elegance of topological/algebraic proofs for claims in analysis. Time and again, I can come up with one proof but would not have thought of the other, with the ones I think of coming from an analyst's, or sometimes more sacrilegiously, an engineer's perspective. I'm good at juggling around epsilons, establishing inequalities, or proving one of many types of  convergence, but whenever the key step is to invent new spaces as simple as a quotient space, I get stuck. Infuriatingly, I tend to understand the proof once it is presented - I just cannot come up with it on my own. Let me illustrate with an example: The claim to be proven is the following: Let $T: X \rightarrow Y $ be a linear mapping between two normed
  linear spaces. Assume you have proved that if $X$ is finite dimensional,
  $T$ is continuous. Prove that if $Y$ is finite dimensional, $T$ is
  continuous if and only if $\ker(T)$ is closed. If $T$ is continuous, then since $\ker(T) = T^{-1}(0)$ is the preimage of a closed set, it is closed. So far so good. For the second portion I have two different proofs: The first proof is gritty, and in a style of an analysis textbook: Without loss of generality, $T$ is surjective. Let $\{e_i\}_{i=1}^n$ be a basis for $Y$. Then there exists $\{u_i\}_{i=1}^n$ such that $Tu_i = e_i$. If $T$ were not continuous, there is a sequence $\{x_j\}_{j=1}^\infty \rightarrow 0$ such that $\|Tx_j\|=1$. The unit sphere in $X$ in $Y$ is compact, hence there is a subsequence (for simplicity just $\{x_j\}_{j=1}^\infty$) and a $y \in Y$ with $\|y\| =1$ such that $Tx_j \rightarrow y$. Since $Y$ is finite-dimensional, we have for some appropriate coefficients that 
$$Tx_j = \sum_{i=1}^n \alpha_{i,j} e_i, \quad y = \sum_{i=1}^n \alpha_i e_i , \text{ where } \alpha_{i,j} \rightarrow \alpha_i \text{ as } j \rightarrow \infty .$$
Denote $w_j = \sum_{i=1}^n \alpha_{i,j} u_i, w = \sum_{i=1}^n \alpha_i u_i $. Note that $T(w_j - x_j) = 0$, so $w_j - x_j \in \ker(T)$. Since $\ker(T)$ is closed $w = \lim w_j - x_j \in \ker(T)$, but $Tw = y \neq 0$, a contradiction. Hence $T$ must be continuous. The second proof is somewhat topological, and much more clean and elegant: Since $\ker(T)$ is closed, $X/\ker(T)$ is a normed linear space. Define $\bar{T}:X/\ker(T) \rightarrow Y$ by $\bar{T}(x + \ker(T)) = T(x)$, which is linear, and continuous since $X/ker(T)$ is finite-dimensional. Define $\pi: X \rightarrow X/\ker(T)$ in the obvious way. Then note that $T = \bar{T} \circ \pi$ is a composition of continuous function, hence continuous. # This is definitely not a perfect example, but in my imperfect vocabulary, the first, gritty proof is very detailed, full of analysis concepts such as convergence, or simple linear algebra, whereas the second deals with properties of the space as a whole, without relying much on the objects within it. I realize that it is hard to compare these 'styles' in proof, and the first style can go a long way. Nevertheless, I feel that to become a better student of mathematics, and even of analysis, I should get a better handle on algebraic or topological techniques. I've taken courses in abstract algebra and topology, but my intuition for these has not grown sufficiently for me to think of questions in analysis in largely algebraic or topological ways, at least beyond standard linear algebra. In the end, I am sure that I want to study analysis, mostly for applied purposes, and I'm aware that there are practical constraints - I won't be in school forever. Would you still agree that developing greater topological intuition is worthwhile? Why or why not? If you feel that it is worth it, how should I go about cultivating said intuition (keeping in mind that eventually, I will enter applied rather than pure math)?","['normed-spaces', 'real-analysis', 'functional-analysis', 'general-topology', 'soft-question']"
2862068,Proving that $\cot^220^\circ + \cot^240^\circ + \cot^280^\circ = 9$. [duplicate],This question already has answers here : Evaluate $\tan^{2}(20^{\circ}) + \tan^{2}(40^{\circ}) + \tan^{2}(80^{\circ})$ (6 answers) Closed 5 years ago . Prove that $\cot^220^\circ + \cot^240^\circ + \cot^280^\circ = 9$. I tried bringing them all to $\cot^220^\circ$ but it didn't work. How do I proceed?,['trigonometry']
2862076,Value of $\lim_{n \to \infty} \left({\frac{(n+1)(n+2)(n+3)...(3n)}{n{^{2n}}}}\right)^{1/n}$ [duplicate],"This question already has answers here : $\lim_{n \to \infty} (\frac{(n+1)(n+2)\dots(3n)}{n^{2n}})^{\frac{1}{n}}$ is equal to : (5 answers) Closed 3 years ago . I was asked to evaluate the following expression: $\lim_{n \to \infty} \left({\frac{(n+1)(n+2)(n+3)...(3n)}{n{^{2n}}}}\right)^{1/n}$ My first step was to assume that the limit existed, and set that value to $y$. $ y = \lim_{n \to \infty} \left({\frac{(n+1)(n+2)(n+3)...(3n)}{n{^{2n}}}}\right)^{1/n}$ And then, I took the natural logarithm of both sides of the equation. I obtained the expression: $ \ln y = \lim_{n \to \infty} \frac{1}{n} \cdot \left(\ln(1+\frac{1}{n}) + \ln(1+\frac{2}{n}) + ... + \ln(1+\frac{2n}{n})\right) $ This simplified to: $ \ln y = \lim_{n \to \infty} \frac{1}{n} \cdot \sum_{k = 1}^{\color{Red}{2n}} \ln(1+\frac{k}{n}) $ I realize that this is similar to the form of a Riemann sum, which can then be manipulated to give the expression in the form of a definite integral. However, the part bolded in red, which is $ 2n$, throws me off. I have only seen Riemann sums be evaluated when the upper limit is $ n - k $, where $k$ is a constant. Therefore, how would I go about evaluating this expression? Thank you for all help in advance.","['limits', 'calculus']"
2862078,A problem about conditional probablity,"Consider probability space
$\left( \Omega\mathcal{,F,}\mathbb{P} \right)$. Given an event
$A\mathcal{\in F}$ and a $\sigma$-algebra $\mathcal{G \subseteq F}$, we
define the conditional probability given $\sigma$ -algebra $\mathcal{G}$, denoted by $\mathbb{P}\left( A|\mathcal{G} \right)$, as a
non-negative
$\left( \Omega\mathcal{,G} \right) \rightarrow \left( \mathbb{R}\mathcal{,B}\left( \mathbb{R} \right) \right)$
random variable s.t.
$\forall G \in \mathcal{G,}\mathbb{P}\left( A \cap G \right) = \int_{G}^{}{\mathbb{P}\left( A \middle| \mathcal{G} \right)d\mathbb{P}}$.
Existence and uniqueness is proved below. For any $A \in \mathcal{F}$ and $\mathcal{G \subseteq F}$, the
  conditional probability exists and is a.s. unique. Notice
  $\mathbb{P}_{A}\left( \cdot \right):=\mathbb{P}\left( A\bigcap_{}^{}\left( \cdot \right) \right)$
  is a measure for $\left( \Omega\mathcal{,G} \right)$ by checking those
  axioms, and we can also verify $\mathbb{P}_{A}\mathbb{\ll P}$. Also,
  $\mathbb{P}$ is also a valid measure for
  $\left( \Omega\mathcal{,G} \right)$, then by Randon-Nikodym
  Theorem we have a non-negative $\mathcal{G}$-measurable function
  $\mathbb{P}\left( A \middle| \mathcal{G} \right) = \frac{d\mathbb{P}_{A}}{d\mathbb{P}}$
  exists and is a.s. unique. We can further show given any $\omega \in \Omega$, then
    $\mathbb{P}_{\mathcal{G}}^{\left( \omega \right)}\left( \cdot \right):=\mathbb{P}\left( \cdot \middle| \mathcal{G} \right)\left( \omega \right)$
    is a.s. a valid measure on $\left( \Omega\mathcal{,F} \right)$. First, $\mathbb{P}_{\mathcal{G}}^{\left( \omega \right)} \geq 0$
      by definition. Secondly, $\mathbb{P}_{\mathcal{G}}^{\left( \omega \right)}\left( \varnothing \right) = 0$ because by definition $0 = \mathbb{P}\left( \varnothing \cap G \right) = \int_{G}^{}{\mathbb{P}\left( \varnothing \middle| \mathcal{G} \right)d\mathbb{P,\forall}G \in \mathcal{G \Rightarrow}\mathbb{P}\left( \varnothing \middle| \mathcal{G} \right) \equiv 0}$ a.s.
      Thirdly, for any
      $A_{1},A_{2}\mathcal{\in F,}A_{1}\bigcap A_{2} = \varnothing$, then
      $A_{1}\bigcap G$ is disjoint form $A_{2}\bigcap G$ and $$\int_{G}^{}{\mathbb{P}\left( A_{1}\bigcup A_{2} \middle| \mathcal{G} \right)d\mathbb{P}}\mathbb{= P}\left( \left( A_{1}\bigcup A_{2} \right) \cap G \right)\mathbb{= P}\left( \left( A_{1}\bigcap G \right)\bigcup\left( A_{2}\bigcap G \right) \right)\mathbb{= P}\left( A_{1}\bigcap G \right)\mathbb{+ P}\left( A_{2}\bigcap G \right) = \int_{G}^{}{\mathbb{P}\left( A_{1} \middle| \mathcal{G} \right)d\mathbb{P}} + \int_{G}^{}{\mathbb{P}\left( A_{2} \middle| \mathcal{G} \right)d\mathbb{P}} = \int_{G}^{}{\left( \mathbb{P}\left( A_{1} \middle| \mathcal{G} \right)\mathbb{+ P}\left( A_{2} \middle| \mathcal{G} \right) \right)d\mathbb{P}}$$ Then by uniqueness we have a.s.
  $\mathbb{P}\left( A_{1}\bigcup A_{2} \middle| \mathcal{G} \right) = \mathbb{P}\left( A_{1} \middle| \mathcal{G} \right)\mathbb{+ P}\left( A_{2} \middle| \mathcal{G} \right)$. I need help with showing the following, Given any random variable $Y:\left( \Omega\mathcal{,F} \right) \rightarrow \left( S,\mathcal{E} \right)$, let $\sigma(Y)$ be its generated $\sigma$-algebra, then $\mathbb{P}_{\sigma\left( Y \right)}^{\left( \omega_{1} \right)} = \mathbb{P}_{\sigma\left( Y \right)}^{\left( \omega_{2} \right)}$ a.s.
  if $Y\left( \omega_{1} \right) = Y\left( \omega_{2} \right)$","['measure-theory', 'probability-theory']"
2862096,How to prove that if $f*\chi_A=0$ a.e. for all $A$ of finite measure then $f=0$ a.e.,"Let $G$ a locally compact abelian group and let $\mu_G$ an Haar measure of $G$. Suppose that $f\in L^1(\mu_G)$ is such that for all measurable $A$ of finite $\mu_G$-measure it happens that $f*\chi_A=0$ $\mu_G$-a.e.. I want to prove that $f=0$ $\mu_G$-a.e.. In $(\mathbb{R}^n,+)$ I would proceed taking $B_r$ as the ball centered in the origin of radius $r>0$ and then using Lebesgue differentiation theorem to get that:
$$0=\frac{1}{|B_r|}f*\chi_{B_r}\rightarrow f, r\rightarrow0 \ a.e.$$ However, for example, in $(\mathbb{Z},+)$ we don't have at our disposal Lebesgue differentiation theorem but here the result can be neatly proved convolving with indicator function of $\{0\}$. So, how can I prove the result in general? Should I try to decompose the group as a product $G=G_1\times G_2$ where $G_1$ and $\hat{G_2}$ are discrete (if such a decomposition can be performed at all) and try to use a technique that mix the previous two strategies (if such a mixed strategy can be implemented at all), or is there another simpler (maybe obvious) path?","['measure-theory', 'locally-compact-groups', 'haar-measure']"
2862101,the maximum value of $\frac{\sin A}{A}+\frac{\sin B}{B}+\frac{\sin C}{C}$,"For any acute angled triangle ABC , find the maximum value of $\frac{\sin A}{A}+\frac{\sin B}{B}+\frac{\sin C}{C}$ . Attempt: As $A+B+C=\pi$ $C=\pi -(A+B)$ After differentiating it $dA+dB+dC=0$ Now :
$\frac{\sin A}{A}+\frac{\sin B}{B}+\frac{\sin C}{C}$ $\frac{\sin A}{A}+\frac{\sin B}{B}+\frac{\sin (A+B)}{\pi-(A+B)}$ $(\frac{A\cos A-\sin A}{A^2})dA + (\frac{B\cos B- \sin B}{B^2})dB + (\frac{C\cos C-\sin c}{C^2})dC =0$ But could not solve further .","['trigonometry', 'inequality']"
2862111,Second marble is of same color,"A bag contains 3 white, 4 black, and 2 red marbles. Two marbles are drawn from the bag. If replacement is not allowed, what is the probability that the second marble drawn will be red? I disagree with the given answer $\frac29$. Can someone please point why my solution is wrong? P(Both Red) + P(Second is Red): P(Both Red) $=\frac{\binom22}{\binom92}$ P(Second is Red) $=\frac{\binom71\binom21}{\binom92}$ This comes out to be $\frac5{12}$.","['combinatorics', 'probability']"
2862129,Minimization problem with latent function and splines,"I have a dataset consisting of pairs $(x_i, y_i)$. I want to determine the function $f$, so $$
f(x)f(y) = 1
$$ with the constraint that $f(x) \leq x$, $f'(x) \geq 0$ and $f''(x) \geq 0$. I was thinking that using splines in some way while constraining the parameters should let me find $f$ satisfying the constraints but I'm unsure how to define the minimization problem in the context of these splines. Any ideas? Data is available here: ""x"" ""y""
0.8 1.111
0.76 1.163
0.98 0.92  
0.66 1.316
0.9 1
0.78 1.136
1.031 0.87
1.042 0.86
0.85 1.053
1.087 0.82
0.83 1.075
1.099 0.81
0.93 0.97
0.4 2
0.34 2.273
1.053 0.85
1.075 0.83
1 0.9
0.89 1.01
0.91 0.99
0.92 0.98
0.95 0.95
0.82 1.087
0.86 1.042
0.88 1.02
0.41 1.961
0.72 1.22
0.96 0.94
0.7 1.25
1.02 0.88
1.111 0.8
0.81 1.099
1.136 0.78
0.94 0.96
1.19 0.74
0.31 2.439
0.39 2.041
1.25 0.7
0.99 0.91
0.87 1.031
0.97 0.93
1.064 0.84
0.44 1.852
0.84 1.064
0.38 2.083
1.163 0.76
0.68 1.282
0.42 1.923
0.33 2.326
0.75 1.176
0.62 1.389
0.77 1.149
0.61 1.408
0.74 1.19
0.51 1.639
0.6 1.429
0.58 1.471
1.176 0.75
1.124 0.79
0.5 1.667
1.01 0.89
0.46 1.786
1.205 0.73
0.65 1.333
0.48 1.724
0.55 1.538
0.54 1.563
0.37 2.128
0.79 1.124
0.45 1.818
1.149 0.77
0.73 1.205
0.3 2.5
1.22 0.72
0.28 2.632
0.71 1.235
0.35 2.222
0.64 1.351
0.53 1.587
0.63 1.37
0.36 2.174
0.49 1.695
0.32 2.381
0.56 1.515
0.59 1.449
0.67 1.299
0.43 1.887
0.25 2.857
0.69 1.266
0.47 1.754
0.52 1.613","['regression', 'optimization', 'statistics', 'spline']"
2862199,Problem book for differential equations?? Are there?,"Is there a problem book which gathers the most relevant exercises in differential equations (historical problems, eventually providing counter-examples for some theorems and properties if some conditions are neglected)? I'm interested in both ODE and PDE. Thanks a lot!","['ordinary-differential-equations', 'book-recommendation', 'reference-request', 'real-analysis', 'partial-differential-equations']"
2862257,Sum of digits of sum of digits of sum of digits of $7^{7^{7^7}}$,"On the back of a mathematical magazine, I came across some ""quick facts"" about the number 7. Most of them were real life related ones, like ""Rome was buit on 7 hills"", ""the neck of most mamals is made out of 7 bones"", ""a ladybird has 7 black spots on its back"" etc. But the last one was very intriguing: denote by $A$ the sum of digits of the number $7^{7^{7^7}}$, by $B$ the sum of digits of $A$, and by $C$ the sum of digits of $B$. What is the sum of digits of $C$? After doing some research on the power of usual computers, I concluded that it would not be possible to calculculate this number and find the sums like this. So the solution must be pure mathematical. But I could not manage to find any path to a solution. If it helps, we have that $7^7=823543$ which has sum of digits $25$, but online big number calculators can't even compute $7^{7^7}$, and I figured it was not worth it to write such a program myself. Edit : After some good hours I want to draw some conclusions about the question as well as proposing some extensions. We saw that for the proposed number $^4 7$, the searched sum is $7$. Mees de Vries found a rigorous proof for that, but which doesn't seem to work in a general case; Henry gave a quite tedious solution which may be generalizable with some extra explanations needed, and Gottfried Helms came up with a computer algorithm which he used to deduce that the sum of digits (repeated as many times as necessary) will repeat with period 3 across consecutive powers of 7. Now, based on the fact that from Euler's Theorem we will get that $^n 7$ has residue $7$ mod 9, for any $n\in\mathbb{N}^*$, I came to believe that if we repeatedly do sum of digits for larger $^n 7$ we will also get 7. The question is  how many times we would have to do the sum-of-digits. Also, I would like to notice that the number 7 here is not a simple coincidence. It is because it is $\varphi(9)+1$, and the sum-of-digits conserves mod 9. It would be interesting to find a similar property for 3, which has totient function 2, and also ""conserves"" the residue of the sum-of-digits (here it seems that the sum will be 9 - at least this is how it is for $3^3$ and $3^{3^3}$). Thanks for all your answers!","['number-theory', 'big-numbers']"
2862270,"If $E$ is an infinite vector space, how to interprete $\sum_{i}x_ie_i$?","Let $E$ a vector space of infinite dimension and let $(e_i)_{i\mathbb N}$ a basis. How is interpreted  $$\sum_{i\in\mathbb N}x_ie_i \ \ ?$$ Suppose now that $E$ is a Banach space with norm $\|\cdot \|$. I suppose that $$\sum_{i\in\mathbb N}x_ie_i=\lim_{n\to \infty }\sum_{i=1}^n x_ie_i,$$
in the $\|\cdot \|$ sense, i.e. $$\forall \varepsilon>0, \exists N: \forall n\in \mathbb N, n\geq N\implies \left\|\sum_{i\in\mathbb N}x_ie_i-\sum_{i=1}^nx_ie_i\right\|<\varepsilon.$$ But since $E$ is a vector space, $\sum_{i\in\mathbb N}x_ie_i\in E$ and thus exist (by definition of a vector space), but I guess it could happen that $$\lim_{n\to \infty }\sum_{i=1}^n x_ie_i$$ doesn't exist or is infinite, does it ? So how can we manage this case ?","['summation', 'linear-algebra', 'functional-analysis', 'vector-spaces']"
2862332,Theorem A: subset of a countable set is countable.,"Theorem 3 A subset of a countable set is countable. In particular, every set of natural numbers is countable. Proof Let $B$ be a countable set and $A$ a nonempty subset of $B$ . First consider the case that $B$ is finite. Let $f$ be a one-to-one correspondence between $\{1,\dots,n\}$ and $B$ . Define $g(1)$ to be the first natural number $j$ , $1\le j\le n$ , for which $f(j)$ belongs to $A$ . If $A=\{f(g(1))\}$ the proof is complete since $f\circ g$ is a one-to-one correspondence between $\{1\}$ and $A$ . Otherwise, define $g(2)$ to be the first natural number $j$ , $1\le j\le n$ , for which $f(j)$ belongs to $A\sim\{f(g(1))\}$ . The pigeonhole principle tells us that this inductive selection process terminates after at most $N$ selections, where $N\le n$ . Therefore $f\circ g$ is a one-to-one correspondence between $\{1,\dots,N\}$ and $A$ . Thus $A$ is finite. Can you elaborate on the statement ""Define $g(1)$ to be the first natural number $j$ , $1\le j\le n\dots$ I also don't understand the next sentence ""If $A=\{f(g(1))\}$ , the proof is complete."" I really appreciate if you explain these in easy terms.",['elementary-set-theory']
2862368,Isn't $E\cong E^{**}\iff \dim(E)<\infty $ wrong?,"Let $E$ a $K$-vector space and denote $E^{**}$ its bidual. Let $$\Phi: E\longrightarrow E^{**}$$
defined by $$\Phi(x)=\left<f,x\right>,\quad f\in E^*.$$ A theorem in my course says $$\Phi\text{ is an isomorphism}\iff E\text{ has finite dimension}.$$ The implication (i.e. $\Rightarrow$) is not correct, no ? For example, if $p\in (1,\infty )$ we have that $L^p(\mathbb R)$ is reflexive, which mean exactly that $\Phi$ is bijective, right ? So I have a truble with this theorem. Moreover, the proof looks correct : The fact that $\Phi$ is injective is fine. We show the contrapositive, i.e. we suppose $E$ has infinite dimension and we prove that $\Phi$ is not surjective. Fo the surjectivity, let $F=\text{Span}(e_1^*,...,e_n^*)\neq E$. I know that there is a non zero linear form $f:E^*\to K$ s.t. $F\subset \ker(f)$. For all $x=(x_i)_{i\in I}\in E$, $$\Phi(x)(e_{i}^*)=x_i.$$
Therefore, if $\Phi (x)(F)=0$, then $x=0$ and thus $\Phi (x)=0$. Therefore, $f$ is not in the range of $\Phi$ and thus its not surjective. What do you think ? By the way, I don't understand why $\Phi(x)(F)=0$ implies that $\Phi(x)=0$... Indeed, we can have $g\notin F$ s.t. $\Phi(x)(g)\neq 0$ no ?",['functional-analysis']
2862383,Decay estimate for the heat equation: $\sup_{t>0}\int_{\mathbb{R}} t^\alpha |u_x|^2\ dx$,"Let $u$ be a solution of the heat equation $$u_t - u_{xx} = 0, \quad t>0, x \in \mathbb{R}$$
with initial data $u(0,\cdot) = u_0$.
Fix $\alpha >0$. How can I estimate (without using explicitly the heat kernel)
$$\sup_{t>0}\int_{\mathbb{R}} t^\alpha |u_x|^2 \  dx,$$
in terms of the initial data?  Could you point out a reference where such an estimate is obtained? Is it fair to call what we obtain a decay estimate?","['ordinary-differential-equations', 'reference-request', 'real-analysis', 'calculus', 'partial-differential-equations']"
2862389,How to determine if a set of five $2\times2$ matrices is independent,"$$S=\bigg\{\left[\begin{matrix}1&2\\2&1\end{matrix}\right], \left[\begin{matrix}2&1\\-1&2\end{matrix}\right], \left[\begin{matrix}0&1\\1&2\end{matrix}\right],\left[\begin{matrix}1&0\\1&1\end{matrix}\right], 
\left[\begin{matrix}1&4\\0&3\end{matrix}\right]\bigg\}$$ How can I determine if a set of five $2\times2$ matrices are independent?","['matrices', 'linear-algebra', 'vector-spaces']"
2862402,Farkas' lemma proof explanation,"I have been studying the proof of the following variant of Farkas' Lemma: A system of linear equations $A \mathbf{x} = \mathbf{b}$ in $d$ variables has a solution iff for all $\mathbf{\lambda} \in \mathbb{R}^d, \lambda^T A = \mathbf{0}^T$ implies $\lambda^T \mathbf{b} = 0$. For the direction $\Rightarrow$ the proof is easy: Suppose that $A\mathbf{x} = \mathbf{b}$ has a solution $\bar{\mathbf{x}}$. Then $\lambda^T A = \mathbf{0}^T \Rightarrow \lambda^TA\bar{\mathbf{x}}=\lambda^{T}\mathbf{b}=0$ For the other direction the proof that the notes give proceeds as follows: The implication $\lambda^TA= \mathbf{0}^T \Rightarrow \lambda^T\mathbf{b}=0$ means that both matrices $A \in \mathbb{R}^{n\times d}$ and $(A|\mathbf{b}) \in \mathbb{R}^{n\times (d+1)}$ have the same linear dependencies among their rows, therefore the same row rank, which means that they have the same column rank. That means that $\mathbf{b}$ is a linear combination of the columns of $A$ which implies that $A\mathbf{x} = \mathbf{b}$ has a solution. What I am missing in the second part of the proof is how the starting claim means that both matrices have the same linear dependencies among their rows. Can anyone give an intuitive explanation?","['convex-optimization', 'matrix-rank', 'linear-algebra', 'linear-programming']"
2862441,Decomposing surfaces into pairs of pants,"I apologise if this question is badly worded/doens't make sense - if I knew how to formulate this question well, I'd probably be half-way to answering it myself. Let $F_g$ be a closed surface of genus $g$, for $g \geq 2$. Let K be a pants decomposition of our surface, i.e. a set of closed curves on $F_g$ such that when we cut along the curves, we obtain a number of disjoint pairs of pants. Furthermore, suppose all curves in K are geodesics on $F_g$. Finally, suppose $F_g$ admits some hyperbolic metric. Each pair of pants contains three seams, i.e. lines of shortest distance between each pair of boundary components. Question : Suppose $P_i, P_j$ are two pairs of pants in $F_g$ which are glued along a common boundary component $\delta$. Do the end-points of the seams of $P_i$ in $\delta$ match up with the end-points of the seams of $P_j$?","['geometric-topology', 'general-topology', 'surfaces']"
2862447,We have $n$ real numbers around the circle and among any consecutive 3 one is AM of the other two. Then all the numbers are the same or $3\mid n$.,"There are $n$ real numbers around the circle and among any consecutive 3 one is arithmetic mean of the other two. Prove that all the numbers are the same or $3\mid n$. Hint was to use a linear algebra. It is obviously that if among some three consecutive numbers some two are the same then all are three the same: Say we have $$(a,a,b)\implies b ={a+a\over 2}= a\;\;\;{\rm or}\;\;\;a ={a+b\over 2} \implies a=b$$ But then all the numbers are the same. So we can assume that among any consecutive 3 there are all different. Any way, if all the number are $a_1,a_2,....,a_n$ then for any three consecutive (inidices are modulo $n$) we have $$a_{i-1}+a_i+a_{i+1} \equiv_3 0$$","['vector-spaces', 'linear-algebra', 'combinatorics', 'algebraic-combinatorics', 'hamel-basis']"
2862510,"Solution of the nonlinear ODE $y'' y' =A y' y + B (x-1) y$, with $y(0) = 1$, $y(1) = 0$","What analytical techniques are available for finding solutions to the nonlinear ODE
$$y'' y' =A y' y + B (x-1) y,$$ with boundary conditions $$y(0) = 1, \quad y(1) = 0,$$
where $A$ and $B$ are positive, real constants? Unfortunately, neither $A$ nor $B$ are necessarily small. Does assuming that $y'(x) \neq 0$ allow further progress?",['ordinary-differential-equations']
2862519,"Does "" All continuous functions are bounded "" or "" All continuous functions attain a maximum "" or together imply the domain is compact?","Let $(X,d)$ be an infinite metric space satisfying H1 or H2 or both. H1: (All continuous functions on X to $\mathbb{R}$ are bounded.) If $f: X\to\mathbb{R}$ is continuous on $X$, then $f(x)$ is bounded. H2: (All continuous functions on $X$ to $\mathbb{R}$ attain a maximum.) If $f: X\to\mathbb{R}$ is continuous on $X$, then there exists at least one point $p \in X $ such that $f(p) \geq f(x)$ for every $x \in X.  $ Question Does H1 only or does H2 only imply $X$ is compact?
Do H1 and H2 together imply $X$ is compact? Note: I know "" If $X$ is compact then all continuous functions on $X$ to $\mathbb{R}$ are bounded "" and "" If $X$ is compact then all continuous functions on $X$ to $\mathbb{R}$ attain a maximum "". I am just curious about whether or not the converse still hold.","['continuity', 'general-topology', 'compactness', 'real-analysis']"
2862535,What is the motivation behind defining tensor product?,"In my undergraduate math course we have tensor calculus. I am not getting the motivations of defining such thing, definition of tensor product and feeling lack of interest in the topic. Can anyone explain why tensor product is defined as it is? or, provide a link. I am using the definition of Tensor product as it is defined here .","['multivariable-calculus', 'tensors', 'tensor-products']"
2862541,"What does it mean that ""the central limit theorem does not hold far away from the peak""?","So I know nothing about large deviations theory, and I'm reading some notes . They claim that: The CLT does not hold far away from the peak I am not sure how to parse this statement. There are many statements of the CLT but here is the one I know: Let $X_n$ be a sequence of i.i.d. random variables with mean $0$ and variance $\sigma^2<\infty$ . Then the following sum: $$\frac{1}{\sqrt{n}}\sum_{k=1}^n X_n$$ converges in distribution as $n\to\infty$ to $N(0,\sigma^2)$ . Why do the notes say that central limit theorem doesn't hold away from $0$ ? There's nothing in the central limit theorem that says ""only for some interval around $0$ "". Does it just mean that the convergence rate is very slow and impractical?","['large-deviation-theory', 'probability-theory']"
2862545,When is $m^2k^2(c^2+1)^2-4mc(c^2-c+1)$ a perfect square?,"Suppose, $m,k,c$ are positive integers Conjecture : The expression $$m^2k^2(c^2+1)^2-4mc(c^2-c+1)$$ is a perfect square if and only if $m=k=1$ In the case $m=k=1$ , we get $(c-1)^4$ which is a perfect square ($0$ and $1$ are allowed) The hard part is to show that otherwise the expression cannot be a perfect square. I tried to compare $(mk(c^2+1)\pm 1)^2$ with the given expression but this led to nowhere. The conjecture is true for $m,k,c\le 1\ 600$ I arrived at this problem by trying to prove that for positive integers $a,b,c$ with $c^2+1\mid a+b$ and $ab\mid c(c^2-c+1)$ we have $a=c$ , $b=c^2-c+1$ or vice versa.","['number-theory', 'square-numbers', 'elementary-number-theory']"
2862549,"$\lim_{(x,y) \to (0,0)}\frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}} = 0 \Longleftrightarrow \frac{a}{c} + \frac{b}{d} > 1$","Let $a,b,c,d$ be positive real numbers. Show that the limit
  $$\lim_{(x,y) \to (0,0)}\frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}} = 0$$
  if only if
  $$\frac{a}{c} + \frac{b}{d} > 1.$$ This question seems to be basically about algebraic manipulation, but I cannot seem to find a good way. Can anybody help me?","['limits', 'real-analysis']"
2862551,Exponential function of a Hermitian matrix,"Given $$H = \begin{pmatrix}\sin \theta & 0 & \cos \theta \\ 0 & 1 & 0 \\ \cos \theta & 0 & -\sin \theta \end{pmatrix}$$ where $\theta=\pi/6$, then what is $\exp{ \left( i \frac{\pi}{2} H \right)}$? I tried to calculate in the following way 
$e^{(i\pi H)/2}=[e^{(i\pi/2)}]^H=i^H$. I do not know how to proceed.","['matrices', 'matrix-exponential', 'linear-algebra']"
2862553,First-Order Stochastic Dominance,"Consider two cumulative distribution functions $F(x)$ and $G(x)$ for $x\in[a,b]$ where $G(x)$ has the first-order stochastic dominance over $F(x)$. That is, $F(x)>G(x)$ for all $x\in(a,b)$. We assume $a<0$ and $b>0$. Let $f(x)$ and $g(x)$ be the probability density function of $F(x)$ and $G(x)$ respectively. Suppose the expected value of $x$ under $F(x)$ is positive:
$$
\int_{a}^{b}xf(x)dx=\int_{a}^{0}xf(x)dx+\int_{0}^{b}xf(x)dx>0.
$$ Under this condition, does $f(x)-g(x)>0$ always hold in any interval of $0<x<b$? Graphical Expression of the Question is Here.","['economics', 'calculus', 'probability-distributions', 'statistics']"
2862563,Sum over all inverse zeta nontrivial zeros,"Starting from the Hadamard product for the Riemann Zeta Function (assuming the product is taken over matching pairs of zeros)
$$\zeta(s)=\frac{e^{(\log(2\pi)-1-\gamma/2)s}}{2(s-1)\Gamma(1+s/2)}\prod_{\rho}\left(1-\frac{s}{\rho} \right)e^{s/\rho}$$
can one derive the exact value of $\sum_{\rho} \frac{1}{\rho}$
to be
$$\sum_{\rho} \frac{1}{\rho} = -\log(2\sqrt{\pi})+1+\gamma/2$$
What implications does this have?","['complex-analysis', 'riemann-zeta']"
2862571,Minimal rotation matrix for two vectors in $\mathbb{R}^n$,"Suppose we have two normal vectors $v, u \in \mathbb{R}^n, \|u\|_2=1, \|v\|_2=1$ . We would like to find a rotation matrix $R\in\mathbb{R}^{n \times n}$ that satisfies $R u = v$ . This clearly does not uniquely determine matrix $R$ because there are only $n$ equations while there are $n(n-1)/2$ degrees of freedom for $R$ . However we can make it unique by forcing it to only rotate vectors in the $Span\{u,v\}$ . In other words: $$\forall w \in Span\{u,v\}^\perp : R w = w$$ Intuitively, it only rotates vectors in the $Span\{u,v\}$ plane while leaving every other direction untouched. In this sense, the rotation matrix is minimal. In the very simple case that $Span\{u,v\} = Span\{e_i,e_j\}$ , i.e., the linear space spanned by $u,v$ is the same as the one spanned by $e_i, e_j$ we can compute $R$ as follows: $R = I_n - M$ , in which $M\in \mathbb{R}^{n\times n}$ is only zeros except for its $i$ -th and $j$ -th rows and columns: $$M_{i, j\times i, j} = \begin{bmatrix}
    1-\cos \theta       &  \sin \theta\\
    -\sin\theta       & 1-\cos\theta \\
\end{bmatrix}$$ in which $\theta = \cos^{-1} \langle u, v \rangle$ Is there a way to compute the rotation matrix for general normal vectors $u, v$ ?","['linear-algebra', 'rotations']"
2862590,Probability that a stick randomly broken in three places can form a triangle,"I like questions about geometric probability, and two of my favourite questions here on math.SE are Probability that a stick randomly broken in two places can form a triangle and Probability that a stick randomly broken in five places can form a tetrahedron . I wondered about the probability that a stick randomly broken in three places can form a triangle. More generally, we can ask for the probability distribution of the number of triangles that can be formed from the four pieces. Since each triple of the pieces has probability $\frac14$ of forming a triangle, the expected number of triangles we can form is $\frac14\cdot4=1$, which is already a rather nice result. I tried various ways of applying inclusion–exclusion, with or without first ordering the segments by size, but it all seemed too complicated and unenlightening, and I ended up writing a program to output all the inequalities in order to let qhull compute the volumes of the polytopes they define. The result (confirmed by simulations) is: \begin{array}{c|c|c}
&\text{probability}&\text{probability}\\
\#\triangle&\text{(reduced)}&\text{(unreduced)}\\\hline
0&\frac37&\frac{45}{105}\\
1&\frac{11}{35}&\frac{33}{105}\\
2&\frac{16}{105}&\frac{16}{105}\\
3&\frac4{105}&\frac4{105}\\
4&\frac1{15}&\frac7{105}
\end{array} You can check that the expected number of triangles comes out as $1$. While the overall distribution is somewhat complicated, the probabilities that we can form all triangles ($\frac1{15}$), any triangle ($\frac47$) and no triangles ($\frac37$) come out as nice low fractions, so I thought that maybe there's hope to find an elegant way to compute (one of) them after all. Do you see one? (The three places where the stick is broken are independently uniformly chosen along its length.)","['alternative-proof', 'inclusion-exclusion', 'geometric-probability', 'probability']"
2862596,Garage Door Puzzle,"When I drive home, I open my garage door from my truck. I cannot see the garage door until I turn into my driveway. I want to know the range of my opener. How close can I get to knowing the farthest distance at which it will open in one drive down the road towards my house? Assumptions for simplicity:
The door takes exactly 10 seconds to open or close fully. (And it closes at a uniform rate).
The road leading past the house is straight.
The distance from the road to the garage is negligibile.
I can click the opener as many times as I want.
If I click the opener while the door is closing, it begins opening and vice versa.
I can discern differences in the height of the door as small as 6 inches, meaning I can tell the difference between 1/2 and 1/3rd, but not between 1/5th and 1/6th. (These numbers are largely arbitrary - basically, I want a practical solution that doesn't involve measuring the final height of the door with lasers and levels)
Finally, I know how far away I am at any time. I know the opener doesn't work at 150 feet and does work at 50 feet. [Edit credit: joriki] This is my first time posting here, so please let me know if there are any additional details I ought to add. Thanks!","['puzzle', 'geometry']"
2862598,"How to find all solutions of the ODE $x'=3x^{\frac{2}{3}}, x(0)=0$","Problem: Find all the solutions of the IVP $$x'=3x^{\frac{2}{3}}, x(0)=0$$ for $t\geq 0$. Here $3x^{\frac{2}{3}}$ is not $C^1$, so the existence and uniqueness theorem does not apply here. My guess the solutions is $$x=\left\{\begin{matrix}
0 & \text{if }0\leq t< t_{0} \\ 
 (t-t_{0})^3 &  \text{ if }t\geq t_{0}  
\end{matrix}\right.$$
$t_{0}\in\mathbb{R^+}$ or $t_{0}\rightarrow+\infty$. But my professor told me that there are a lot more! My main question is: How can I find all the solutions, and then prove that they are all the solutions, rigorously?","['analysis', 'ordinary-differential-equations', 'real-analysis']"
2862607,Studying the convergence of the series $\sum_{n=1}^\infty\sin\frac1{n}\log\left(1+\sin\frac1{n}\right)$,"Study the convergence of the series $$\sum_{n=1}^\infty\sin\frac1{n}\log\left(1+\sin\frac1{n}\right)$$ This is what I came up with $$\lim_{x\to \infty}\frac{\sin\frac1{n}\log\left(1+\sin\frac1{n}\right)}
{\sin^2\frac1{n}}= 1 $$ 
This implies that $$\sin\frac1{n}\log\left(1+\sin\frac1{n}\right) \sim {\sin^2\frac1{n}}$$ using the inequality $\sin{x}\lt x$   $\left(0\le x \lt \pi\right)$ $${\sin^2\frac1{n}} \lt \frac1{n^2}$$ Since  $\sum_{n=1}^\infty\frac1{n^2}$   converges so does $\sum_{n=1}^\infty\sin^2\frac1{n}$ this implies the convergence of $$\sum_{n=1}^\infty\sin\frac1{n}\log\left(1+\sin\frac1{n}\right)$$ Is this right?","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
2862610,Does weakly sequentially closed imply weakly closed?,"As stated, I know of course weakly closed implies weakly sequentially closed, but is the converse also true? In other words, I want to know if in weak topology, a subset is weakly closed if and only if it is sequentially closed. aka they are equivalent definitions just analogous to the case in norm topology. I also know that it is not true to say sequentially closed imply  closed in ALL topological spaces. But is this true specifically in weak topology? Answers appreciated.","['general-topology', 'functional-analysis']"
2862620,UK Lottery Odds Calculation Error,"I found the probability topic in stats and mechanics to be very interesting, and I attempted to try using it to calculate the odds of winning the UK National Lottery, but failed. My calculation was (1/59*1/58*1/57*1/56*1/55*1/54). The reason was that in the NL there are 6 balls dropped, and the possible outcomes range from 1 to 59. I reasoned that since all 6 need to match, I could assume that each outcome could be treated as an isolated selection, and that my first ball match odds were 1/59 as a result, then if the first ball matches (which it must), the next is 1/58 and so on. The odds I calculated were orders of magnitude less likely than the correct value. What is wrong with my attempt, and why is the correct formula quoted as ""59!/(6!*(59-6)!)""?","['statistics', 'lotteries', 'probability']"
2862676,"""Lift"" of an immersion","Let $f : \Sigma^k \to M^n$ be an immersion between differentiable manifolds and let $\pi : \tilde{M} \to M$ be a finite-to-one covering map. Let $\tilde{\Sigma} = \pi^{-1}(f(\Sigma))$. Is it true that $\tilde{\Sigma}$ is immersed somehow into $\tilde{M}$? If $f$ is an embedding, I know how to prove that $\tilde{\Sigma}$ is embedded into $\tilde{M}$, since we can use the fact that $\pi$ is transversal to $f(\Sigma) \cong \Sigma$ (as $\pi$ is a submersion) to conclude that $\pi^{-1}(f(\Sigma))$ is a submanifold of $\tilde{M}$. What about the general case?","['submanifold', 'smooth-manifolds', 'covering-spaces', 'manifolds', 'differential-geometry']"
2862698,Probability of a sequence of coin tosses ending in $HHT$ (I win) or $THH$(You win),"I'm still learning undergraduate probability. I was asked this probability puzzle in a recent quantitative developer interview. I solved the first part of the question, using brute-force. I think, brute-force very quickly becomes unwieldy for the sequence ending $THH$ - not sure if my answer is correct. A coin is flipped infinitely until you or I win. If at any point, the last three tosses in the sequence are $HHT$, I win. If at any point, the last three tosses in the sequence are $THH$, you win. Which sequence is more likely? Solution. $\begin{aligned}
P(xHHT)&=P(H^2T)+P(H^3T)+P(H^4T)+\ldots \\
&=\frac{1}{2^3}+\frac{1}{2^4} + \frac{1}{2^5} + \ldots \\
&=\frac{1/8}{1-1/2}\\
&=\frac{1}{4}
\end{aligned}$ For the second part, $P(xTHH)$, I have drawn a state-diagram, but there are just too many possible combinations for a sequence ending in $THH$. Is there an easier, or perhaps an intuitive way to look at this? Any hints in the right direction would be great!",['probability']
2862702,"A bag contains 3 red, 4 blue, and 5 green balls. [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Peter draws a ball from the bag, and then Angelina draws a ball. What is the probability that Angelina got a green ball? So far I have this: Scenario A: 1st ball is not green, 2nd green: 7/12 * 5/11 = 35/132 Scenario B: 1st ball is green, 2nd green: 5/12 * 4/11 = 20/132 --> $$\frac{55}{132} = \frac{5}{12}$$","['combinatorics', 'probability']"
2862730,Powers of Irreducible Polynomials in Partial Fractions,"Fractions are normally decomposed by finding the numerator that goes with each individual factor of the denominator of the original fraction.  However, when a denominator factor is repeated, the solution algorithm involves finding numerators for ascending powers of that factor.  So there appears to be a difference in how we treat repeated and non-repeated factors. I thought it might be insightful to work out a couple of examples with almost-repeated factors.  We can find, either trivially or via partial fraction decomposition, that $$\frac{x + 5}{x^2} = \frac{1}{x} + \frac{5}{x^2}$$ Compare this to the decomposition of a similar fraction $$\frac{x + 5}{(x + \varepsilon)(x)} = \frac{\varepsilon - 5}{\varepsilon(x + \varepsilon)} + \frac{5}{\varepsilon x}$$ where $\varepsilon$ is an arbitrarily small constant.  To me, it's not particularly clear that the right-hand side (RHS) of each equation relates to that of the other.  We can't set $\varepsilon$ to $0$ in the second RHS, but even using limits to let $\varepsilon$ approach $0$ doesn't yield the first RHS, as I would have guessed.  Furthermore, the solution process does not harmonize the degree disparity between the top and bottom RHS; the top has the form $\frac{degree 0}{degree 1} + \frac{degree 0}{degree 2}$ , while the bottom has the form $\frac{degree 0}{degree 1} + \frac{degree 0}{degree 1}$ . A less trivial example, comparing $$\frac{x^2 + 1}{(x^2)(x + 3)} = -\frac{1}{9x} + \frac{1}{3x^2} + \frac{10}{9(x + 3)}$$ to $$\frac{x^2 + 1}{(x + \varepsilon)(x)(x + 3)} = \frac{1}{3\varepsilon x} + \frac{\varepsilon^2 + 1}{\varepsilon(\varepsilon - 3)(x + \varepsilon)} + \frac{10}{3(3 - \varepsilon)(x + 3)}$$ reveals a similar absence of clarity.  Confusingly, one gets the urge to let some of the $\varepsilon$ approach $0$ while letting others approach $3$ , but even arbitrarily allowing this much freedom won't allow us to reach the top RHS in this pair. Why is this analysis not providing more insight into the repeated factor rule for partial fraction decomposition, and will some other analysis in a similar vein work better?","['fractions', 'calculus', 'partial-fractions', 'intuition', 'limits']"
2862733,Sequence formed by quotient of sum and the product of odd squares is divergent: Why?,"Trust me when I tell you this is not homework. Can you suggest an solid argument to prove
$$ S_n = \frac{1}{3^25^27^2\cdots (2n-1)^2}\sum_{m=3}^{\infty} 2^{(2n)m}e^{-2^{m/2}} 
$$
diverges as $n \rightarrow \infty$ ? I have calculated $S_1,S_2,S_3,S_4$ and $S_5$ 
$$ S_1 = \sum_{m=3}^{\infty} 2^{2m}e^{-2^{m/2}} =  13.63 $$
$$ S_2 = \frac{1}{3^2}\sum_{m=3}^{\infty} 2^{4m}e^{-2^{m/2}} =  1611.5$$
$$ S_3 = \frac{1}{3^25^2}\sum_{m=3}^{\infty} 2^{6m}e^{-2^{m/2}} =  511684$$
$$ S_4 = \frac{1}{3^25^27^2}\sum_{m=3}^{\infty} 2^{8m}e^{-2^{m/2}} =  3.42 \times 10^8 $$
$$ S_5 = \frac{1}{3^25^27^29^2}\sum_{m=3}^{\infty} 2^{10m}e^{-2^{m/2}} =  \frac{3.513 \times 10^{17}}{3^25^27^29^2} = 3.934 \times 10^{11} $$
and I am convinced it diverges, however, I'd like a more solid argument to rest my conjecture $S_n \rightarrow \infty$ as $n \rightarrow \infty$. Thanks in advance. I will award 200pts bounty for a quality answer.","['divergent-series', 'sequences-and-series']"
2862736,Can a spiral have its centroid at the origin?,"A spiral is a curve $\gamma$ with the polar equation $r=f(\theta)$ where $f$ is a continuous positive strictly monotone function on some interval $[a, b]$, $-\infty<a<b<\infty$. Best known examples are the logarithmic spiral and the Archimedean spiral . Problem : Find a spiral whose centroid is the origin of the coordinate system. Progress so far : We want $$\int_\gamma x\,ds = \int_\gamma y \,ds = 0 \tag1$$
Note that $x = f(\theta)\cos\theta$, $y = f(\theta)\sin\theta$, and $ds = \sqrt{(f'(\theta))^2 + f(\theta)^2}\,d\theta$. Thus, we need the function 
$$g(\theta) = f(\theta) \sqrt{(f'(\theta))^2 + f(\theta)^2} $$ 
to be orthogonal to both $\cos \theta$ and $\sin\theta$ on the interval $[a, b]$, meaning $$\int_a^b g(\theta)\cos\theta\,d\theta = \int_a^b g(\theta)\sin\theta\,d\theta  = 0\tag2$$ A natural way to satisfy (2) is to take $[a, b] = [0, 2\pi]$ and $g$ to be constant (say $g\equiv 1$ as scaling does not matter). However this fails, because solving the equation $g\equiv 1$ for $f$ (as an autonomous ODE) yields $f(\theta) = \sqrt{\sin 2\theta}$ (up to a shift), which is not even defined, let alone monotone, on any interval of length $2\pi$. Note : It is not required for $[a, b]$ to have length $2\pi$ or a multiple of $2\pi$; it can be any nontrivial finite interval.","['plane-curves', 'multivariable-calculus', 'centroid', 'polar-coordinates']"
2862794,Matrix equation with symmetric and positive definite matrices [duplicate],"This question already has answers here : Find $C$, if $A=CBC$, where $A$,$B$,$C$ are symmetric matrices. (3 answers) Closed last year . Let matrices $P_0$ and $P_1$ be symmetric and positive definite. Is it possible to find a symmetric matrix $S$ such that $SP_0S=P_1$? If $P_0=I$, this is always possible: $S=\sqrt{P_1}$.","['matrices', 'matrix-equations', 'positive-definite', 'symmetric-matrices']"
2862796,limit of sequence involving the fractional part,"Using the pigeonhole principle, any sequence of the form $(\{\frac{n}{r}\})_{n\geq1}$ where $r$ is an irrational number is dense in the unit interval. Then prove that the following limit does not exit in $[0;\infty]$ $$\lim_{n\to\infty}n\bigg\{\frac{n}{r}\bigg\}$$","['limits', 'fractional-part', 'sequences-and-series']"
2862820,"Calculate the derivative of $f(x)=\int_x^{x^2}\tan(x+y)\,dy$ on the open interval $(0,\frac{\pi}{4})$.","This appears to be a chain rule question, but I'm struggling with the setup. My attempt is as follows: Let $F:\mathbb{R}^2\rightarrow\mathbb{R}$ , $F(u,v)=\int_u^{u^2}$ tan $(v+y)dy$ , $u:\mathbb{R}\rightarrow\mathbb{R}^2$ , and $u(x)=(u(x),v(x))=(x,x)$ . Then writing $f(x)=(F\circ u)(x)$ allows for use of the chain rule: $D(F\circ u)(x)=DF(u(x),v(x))Du(x)=\left[\frac{\partial F}{\partial u}\mbox{ }\frac{\partial F}{\partial v}\right]\begin{bmatrix}u'(x)\\v'(x)\end{bmatrix}=\frac{\partial F}{\partial u}(u(x))u'(x)+\frac{\partial F}{\partial v}(u(x))v'(x)$ Then by the Fundamental Theorem of Calculus, $\frac{\partial F}{\partial u}=\frac{\partial}{\partial u}\int_u^{u^2}tan(v+y)dy=\mbox{tan}(v+u)=\mbox{tan}(x+x)=\mbox{tan(2x)}$ And similarly, $\frac{\partial F}{\partial v}=\frac{\partial}{\partial v}\int_u^{u^2}\mbox{tan}(v+y)dy=\int_u^{u^2}\frac{\partial}{\partial v}\mbox{tan}(v+y)dy=\int_x^{x^2}\frac{\partial}{\partial x}\mbox{tan}(x+y)dy=\int_x^{x^2}\mbox{sec}^2(x+y)dy$ However at this point, I'm relatively convinced I'm making a major mistake, either with my entire substitution or with the way I'm handling the boundaries. Any advice is much appreciated.","['multivariable-calculus', 'proof-verification', 'real-analysis']"
2862826,$\sum a_n$ converges iff $\sum \frac{a_n}{1+a_n}$ converges.,"This is a modification of a problem in Rudin. Let $(a_n)$ be a sequence of positive numbers (that is $a_n \geq 0)$. Then $\sum a_n$ converges iff $\sum \frac{a_n}{1+a_n}$ converges. My attempt : $\Rightarrow$ $$\frac{a_n}{1+a_n} \leq a_n$$ and this follows from comparison test. $\Leftarrow$ Since $$a_n = \frac{a_n}{1+a_n} (1+a_n) = \frac{a_n}{1+a_n} + \frac{a_n^2}{1+a_n}$$ it suffices to show that $\sum \frac{a_n^2}{1+a_n}$ converges. For this, it suffices to show that $(a_n)$ is bounded, because then the result follows from the comparison test. Indeed, let $M$ be an upperbound. Then $$\frac{a_n^2}{1+a_n} \leq \frac{Ma_n}{1+a_n}$$ We will prove that $a_n \to 0$, and this will prove the boundedness. Let $\epsilon > 0$. Choose $N$ such that $\frac{a_n}{1+a_n} < \frac{\epsilon}{1+ \epsilon}$ for $n \geq N$, which is possible since $\frac{a_n}{1+a_n} \to 0$ since the series converges. Then, $n \geq N$ implies that $a_n < \epsilon$ and the result follows. Is this correct? Is there an easier way?","['proof-verification', 'sequences-and-series', 'real-analysis']"
2862907,Prove that: Probability of connectivity of a random graph is increasing with the size of the graph,"In a random graph $G(n, p)$, the exact probability of the graph being connected can be written as: 
$$
f(n) = 1-\sum\limits_{i=1}^{n-1}f(i){n-1 \choose i-1}(1-p)^{i(n-i)}
$$ This probability is claimed to converge to 1 , when $n \rightarrow +\infty$. Empirically simulating this function, indeed it converges to 1, for different values of $p$. Additionally, I conjecture that this function is an strictly increasing with $n$ (for $n > 2$ and $p > 0.5$). Any thoughts on how we can prove/disprove this conjecture? Here is an effort: If I prove that $f(n) - f(n-1) > 0$ for any $n > 2$, I'm done (proving that it's strictly increasing). 
$$
f(n) - f(n-1) = \left(1-\sum\limits_{i=1}^{n-1}f(i){n-1 \choose i-1}(1-p)^{i(n-i)} \right) - \left(1-\sum\limits_{i=1}^{n-2}f(i){n-2 \choose i-1}(1-p)^{i(n-1-i)} \right) \\
=  \sum\limits_{i=1}^{n-2}f(i){n-2 \choose i-1}(1-p)^{i(n-1-i)} 
 - \sum\limits_{i=1}^{n-1}f(i){n-1 \choose i-1}(1-p)^{i(n-i)} \\ 
=  \sum\limits_{i=1}^{n-2}f(i){n-2 \choose i-1}(1-p)^{i(n-1-i)} 
 - \sum\limits_{i=1}^{n-2}f(i){n-1 \choose i-1}(1-p)^{i(n-i)} - (n-1) \cdot f(n-1)(1-p)^{n-1} \\ 
=  \sum\limits_{i=1}^{n-2}f(i){n-2 \choose i-1}(1-p)^{i(n-1-i)} 
 - \sum\limits_{i=1}^{n-2}f(i){n-2 \choose i-1}(1-p)^{i(n-1-i)} \times \frac{n-1}{n-i} (1-p)^{i} - (n-1) \cdot f(n-1)(1-p)^{n-1} \\ 
= \left\lbrace \sum\limits_{i=1}^{n-2}f(i){n-2 \choose i-1}(1-p)^{i(n-1-i)} \times  \left[ 1 - \frac{n-1}{n-i} (1-p)^{i}\right] \right\rbrace - (n-1) \cdot f(n-1)(1-p)^{n-1} \\ 
$$ Since $p > 0.5$, $1-p < 0.5$ and $\frac{n-1}{n-i} (1-p)^{i} < 1$ (for any $i$). Hence the first term is positive. However, this is not enough for proving the strict increasing behavior of $f(n)$; we have to show that this is first term is strictly biggeer than the 2nd (negative) term. Let's try the first few terms. One can see that the first few terms of this function is ($q = 1-p$): 
$$
f(2) = 1-q \\ 
f(3) = 1 - 3q^2 + 2q^3 \\ 
f(4) = 1 - 4q^3 - 3q^4 + 12q^5 - 6q^6 \\ 
f(5) = 1 - 5q^4 - 10q^6 + 20q^7 + 30q^8 - 60q^9 + 24q^{10} \\ 
$$ Here I plot the differences $f(i) - f(i-1)$: 
$$
f(3) - f(2) = -3q^2 + 2q3  + q 
$$ $$
f(4) - f(3) = - 4q^3 - 3q^4 + 12q^5 - 6q^6 - (- 3q^2 + 2q^3) 
$$ $$
f(5) - f(4) = - 5q^4 - 10q^6 + 20q^7 + 30q^8 - 60q^9 + 24q^{10} -  (- 4q^3 - 3q^4 + 12q^5 - 6q^6)
$$ Visuallly all the difference functions are positive for $q < 0.5 (i.e. $p > 0.5$)$ (i.e. conjecture is supported).","['random-graphs', 'functions', 'approximation', 'recursion']"
2862966,Vanishing of terms in power series expansion,"I am looking to find the power series expansion around $0$ of the rational function defined by $$f(z)=\prod_{l=0}^n (1-lz)^{(-1)^l {n\choose l}}.$$
By considering small $n$, I am led believe that the coefficients $a_r$ of the power series expansion satisfy $a_0=1$, $a_r=0$ for $0<r<n$, and $a_n=1+(-1)^{n-1}(n-1)! z^n$. I would like to see a quick proof of this statement (using any methods you like, the shorter the better), having tried e.g. residue theorem and induction in vain. Presumably this comes down to a certain identity between binomial coefficients?","['complex-analysis', 'abstract-algebra']"
2862982,Approximation of derivative using combinations,"I have sets $A = \left\{a_{1},a_{2},a_{3},\dots,a_{n} \right\}$ and $B = \left\{b_{1},b_{2},b_{3},\dots,b_{n} \right\}$. If they are time-series sorted by their indices, I can take the differences,
$$
d = \frac{\Delta A}{\Delta B} = \left\{\frac{a_2-a_1}{b_2-b_1},\frac{a_3-a_2}{b_3-b_2}, \dots, \frac{a_n-a_{n-1}}{b_n-b_{n-1}}\right\}
$$
and take the mean finite difference $\bar{d}$ to approximate the derivative $\frac{dA(t)}{dB(t)}$. If they are not sorted by time (i.e., they are just sequences but not time-series), can I take all combinations of differences in the numerator and denominator, take their ratio, and finally their mean to approximate $\bar{d}$?
That is, can we say:
$$
\bar{d} \approx \frac{1}{n}\sum_{i\neq j}\frac{a_i-a_j}{b_i-b_j}
$$ It will be awesome if someone could suggest an approximation. This is needed in my biology research. Thanks.",['derivatives']
2862992,Connected components of real matrices in $M_n(\mathbb R)$ with constant rank $k$,"Let $E = \{A \in M_n(\mathbb R): \text{rank}(A) = k\}$. I would like to determine the connected components of $E$. There is a similar question asked here , but the underlying field is $\mathbb C$. For $A \in E$, let $A = U\Sigma V^T$ be the singular value decomposition. If we don't require $U, V$ to be orthogonal, then
\begin{align*}
A = M \begin{pmatrix}
I_{k \times k} & 0 \\
0 & 0
\end{pmatrix} N =: M\hat{I}N,
\end{align*}
where $M,N \in GL_n(\mathbb R)$. But $GL_n(\mathbb R)$ has two connected components defined by the sign of determinants, does this mean there are four connected components in $E$ by enumerating all the possibilities of sign of determinants of $M,N$? If the dimension $n$ is odd, then I think there should be two connected components. Let $\phi: GL_n(\mathbb R) \times GL_n(\mathbb R) \to M_n(\mathbb R)$ be given by $S \times T \mapsto S \hat{I} T$. We can view $E$ as the continuous image of $\phi$. Since $A = M \hat{I} N = (-M) \hat{I} (-N)$ and $\det(-M) = (-1)^{n}\det(M) = -\det(M)$, then the four cases mentioned above should correspond to two connected images.","['matrices', 'general-topology', 'linear-algebra', 'connectedness']"
2863008,Can we deduce Surjectivity from a property of Unitary linear map?,"Many books define the unitary linear map $U: (\mathbb{H}_1,\left<\right>_1) \rightarrow (\mathbb{H}_2,\left<\right>_2)$ as a bijective linear map such that
$\left< x,y \right>_1=\left<Ux,Uy  \right>_2$. And I have been trying to disprove that if $U: (\mathbb{H}_1,\left<\right>_1) \rightarrow (\mathbb{H}_2,\left<\right>_2)$ is a linear map such that
$\left< x,y \right>_1=\left<Ux,Uy  \right>_2$ then $U$ is surjective. I am pretty sure that the statement above is false since definition specify the bijectivity of unitary map and otherwise they don't need to do that. I hope someone can help me to come up with the counter example.","['hilbert-spaces', 'functional-analysis']"
2863051,Prove that $\log_23>\log_35>\log_47$.,"Using calculus, prove that $\log_23>\log_35>\log_47$ . My try : If $\log_x(2x-1)$ is decreasing function then we can say that $\log_23>\log_35>\log_47$. $f(x)=\log_x(2x-1)$ $f(x)=\dfrac{\ln(2x-1)}{\ln x}$ $f'(x)=\dfrac{2x\ln x-(x-1)\ln(2x-1)}{(\ln x)^2x(2x-1)}$","['calculus', 'logarithms']"
2863055,Compactly supported bounded functions with zero integral are dense in Lp spaces,"As in the title, I want to know if compactly supported bounded functions with zero mean, i.e. $\int f dx= 0$, are dense in $L^p$ for any $ 1 <p< \infty $. I feel like there should be some counterexamples to this but could not think of one. Thanks in advance.",['real-analysis']
2863056,"On Convolution: Show that $g_a *g_b = g_{\min(a,b)}$","For $a>0$, I have been given following  the functions 
$$f_a(x)=\frac{a}{π(x^2+a^2)}$$
 and 
$$g_a(x)=\frac{\sin(ax)}{π x}~~x\neq0,\qquad g_a(0)= \frac{a}{π}. $$ Question Show that, $$f_a *f_b =  f_{a+b}$$
and $$g_a *g_b =  g_{\min(a,b)}$$ I was able to prove that $f_a *f_b =  f_{a+b}$ through Fourier Transform. Can anyone help to show that $g_a *g_b =  g_{\min(a,b)}$ or any hint?","['fourier-analysis', 'convolution', 'analysis', 'real-analysis', 'calculus']"
2863073,Convergence of slowly decreasing sequences,"Let $A$ be a large number, say $A=100$, and $a$ be a small number, say $a=0.01$. Let $x_1=1$ and define $x_{n+1}-x_n=-a \exp(-A/x_n)$. Is it true that $x_n\to 0$? We know that $x_n$ is strictly decreasing. But I do not know how to show that $x_n>0$ for all $n$, since mathematical induction fails here. Note that $x=0$ is a fixed point for the iteration.","['analysis', 'sequences-and-series']"
2863122,"Evaluating $\int _0^\infty\,\frac{\ln x}{(1+x^2)^2}\,\text{d}x$ using the Residue Theorem","To evaluate this integral
$$\int\limits_0^\infty\frac{\ln x}{(1+x^2)^2} \text{d}x\,,$$ Let us consider the following function.
$$f(z)=\frac{\ln^2 z}{(1+z^2)^2}=\frac{\ln^2 z}{(z+i)^2(z-i)^2}\,.$$
The integral over $C_R$ and $C_\epsilon$ vanish as $R$ approaches $\infty$ and $\epsilon$ approaches $0$. So we only need to evaluate the residues
$$\oint\limits_C f(z)dz = 2\pi i \sum \text{Res}\big(f(z)\big)\,.$$ The poles are $\pm i$ and of second order. Here is our chosen contour . Let us recall the residue of an $m$-th order of pole is
$$\text{Res}[f(z);z_0]=\frac{1}{(m-1)!}\lim_{z\to z_0} \frac{\text{d}^{m-1}}{\text{d}z^{m-1}}\,(z-z_0)^m\,f(z)\,.$$ Consequently the residues are
$$\text{Res}[f(z);i]=\lim_{z \to i}\Bigg(\frac{2\ln z}{z(z+i)^2}-\frac{2\ln^2 z}{(z+i)^3}\Bigg)=\frac{2\cdot \left(\frac{i\pi}{2}\right)}{-4i}-\frac{2\cdot (-\frac{\pi^2}{4})}{-8i}=-\frac{\pi}{4}+i\frac{\pi^2}{16}$$
As for the other pole
$$\text{Res}[f(z);-i]=\lim_{z \to -i}\Bigg(\frac{2\ln z}{z(z-i)^2}-\frac{2\ln^2 z}{(z-i)^3}\Bigg)=\frac{2\cdot \left(\frac{i3\pi}{2}\right)}{4i}-\frac{2\cdot \left(-\frac{9\pi^2}{4}\right)}{8i}=\frac{3\pi}{4}-i\frac{9\pi^2}{16}\,.$$ Adding these two together and remembering the residue theorem, we have
$$\oint\limits_C f(z)\,\text{d}z = 2\pi i \cdot \left(\frac{\pi}{2}-i\frac{\pi^2}{2}\right)=i\pi^2+\pi^3$$
On the other hand, for the integrals over $C_+$ and $C_-$, $z=x$ and $z=xe^{2\pi i}$ respectively. Hence, we can write
$$\int\limits_\epsilon^R\frac{\ln^2 x}{(1+x^2)^2} \,\text{d}x + \int\limits_R^\epsilon \frac{(\ln x +2\pi i)^2}{(1+x^2)^2} \,\text{d}x=i\pi^2+\pi^3\,.$$
If we let $R \to \infty$ and $\epsilon \to 0$ and change the direction of the second integral, we get
$$-4\pi i\int\limits_0^\infty\frac{\ln x}{(1+x^2)^2}\,\text{d}x + 4\pi^2\int\limits_0^\infty\frac{\text{d}x}{(1+x^2)^2}=i\pi^2+\pi^3\,.$$ Comparing the real and the imaginary parts
$$\int\limits_0^\infty\frac{\ln x}{(1+x^2)^2}\,\text{d}x = -\frac{\pi}{4}$$
$$\int\limits_0^\infty\frac{\text{d}x}{(1+x^2)^2} = \frac{\pi}{4}$$","['integration', 'definite-integrals', 'improper-integrals', 'complex-analysis', 'contour-integration']"
2863140,Non piecewise $C^1$ solutions of conservation laws,"I studied the following theorem:(Rankine-Hugoniot condition) Let $u:\mathbb{R} \times [0,+\infty) \rightarrow \mathbb{R} $ be a piecewise $C^1$ function. Then $u$ is a weak solution if and only if the two of the following conditions are satisfied: i) $u$ is a classical solution of in the domain where $u$ is $C^1$ ii) $u$ satisfies the jump condition $$(u_+ -u_-) \eta_t +\sum\limits_{j=1}^d{f_j(u_+) -f_j(u_-)} \eta_x=0$$
My doubts... i)Can we find an example of the  conservation law where solutions are not piecewise $C^1$(i.e the solution does not have a piece wise $C^1$ representative) so that we cannot apply Rankine-Hugoniot condition across the jump? ii) Is there any weaker versions of this theorem so that piecewise $C^1$ can be relaxed?","['regularity-theory-of-pdes', 'hyperbolic-equations', 'ordinary-differential-equations', 'partial-differential-equations']"
2863159,Maximal order of a torsion element in a hyperbolic group,"Suppose that $G = \langle X \rangle$ is a $\delta$-hyperbolic group, i.e. all geodesic triangles are $\delta$-thin (the inverse image of a point under the projections onto a tripod has diameter bounded by $\delta$). It is a standard result that every finite subgroup of $G$ is conjugate to a subgroup contained in the ball of radius $2 \delta +1$ around the identity. It follows that every finite subgroup of $G$ is of size at most $|X|^{2\delta + 1} + 1$. Can this result be improved if we only considered cyclic subgroups, i.e. can we get a better bound on the maximal order of a torsion element than $|X|^{2\delta + 1} + 1$? EDIT:
Following the discussion in the comments, does the situation change if we assumed the group $G$ to be one-ended?","['group-theory', 'hyperbolic-geometry']"
2863171,Second order non- linear differential equation solution [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question EDIT1: Please suggest a substitution for solving or any method of solving: $$y'' (x)\cot( y(x) ) = y'(x){^2} +c  $$ Thanks in advance.",['ordinary-differential-equations']
2863179,Explanation of and alternative proof for Cantor's Theorem,"I'll state the Cantor's theorem proof as is it is in my study texts: Theorem (Cantor): Let $X$ be any set. Then $|X|<|\mathcal{P}(X)|$ Proof: Define map $\varphi:X\rightarrow\mathcal{P}(X)$ by $\varphi:x\mapsto\{x\}$. $\varphi$ is injective, thus $|X|\leq|\mathcal{P}(X)|$. Now suppose there is a bijection (surjection) $\psi:X\rightarrow\mathcal{P}(X)$. Denote the set $A=\{x\in X, x\notin \psi(x)\}$. By assumption, $\psi$ is surjection so we find some $a\in X$ such that $\psi(a)=A$. Then we have two cases, either $a\in A$, but the, by definition of $A$: $a\notin \psi(a)$ which is a contradiction. So $a\notin A$ but then $a\in \psi(a)=A$ which is a contradiction aswell thus such surjection cannot exist. So now, for my question. The definition of $A$ seems really weird to me. Because, in the first place. Assume $X=\{1,2\}$ then $\mathcal{P}(X)=\{\{1,2\},\{1\},\{2\},\emptyset\}$. In the proof $A$ is supposed to be the set of all members of $X$ (thus numbers) that are not in the range of $\psi$ but, the range of $\psi$ are sets, aren't they? Thus $A=\{1,2\}$ and the case is $\forall a\in X:a\in{A}$ thus the second case of the proof applies. Basically this leads me to the following idea: instead of constructing this set $A$, we can say that: Estabilish injection and thus $|X|\leq|\mathcal{P}(X)|$ by $\varphi:x\mapsto\{x\}$ thus $\forall{S}\in\varphi(x):|S|=1$, but for any set $X:$ $\emptyset\subset\mathcal{P}(X)$ but $|\emptyset|=0$ so $\emptyset\notin ran(\varphi)$ thus $\varphi$ is not surjective.","['elementary-set-theory', 'proof-explanation']"
2863220,Sufficient condition for a matrix to be diagonalizable and similar matrices,"my question is about diagonalizable matrices and similar matrices. I have a trouble proving a matrix is diagonalizable. I know some options to do that: Matrix $A$ $(n \times n)$, is diagonalizable if: Number of eigenvectors equals to number of eigenvalues. There exists an invertible matrix $B$ and a diagonal matrix $D$ such that: $D=B^{-1}AB$. But i have a trouble to determine it according the second option, Do i really need to search if there exists an invertible matrix $B$ and a diagonal matrix $D$ such that: $D=B^{-1}AB?$ I really sorry to ask an additional question here: If a matrix has a row of $0$'s (one of its eigenvalues is $0$), That matrix is diagonalizable? in general, given a matrix, how do i know if is a diagonalizable matrix? Are there some additional formulas to do that? Thanks for help!!","['matrices', 'diagonalization', 'similar-matrices', 'linear-algebra']"
2863266,Correct Notation of Mean Value Theorem for Vector-Valued Function,"Let ${\bf f}({\bf x})$ be a function ${\bf f}: \mathbb{R}^n \to \mathbb{R}^m$ with continuous derivatives ${\bf H}({\bf x})$ . We wish to approximate ${\bf f}({\bf x}_0)$ by ${\bf f}({\bf x})$ . It is well known that for $m > 1$ , we cannot guarantee the existence of a vector $\bf \tilde{x}$ between ${\bf x}_0$ and ${\bf x}$ such that $${\bf f}({\bf x}) = {\bf f}({\bf x}_0) + {\bf H}({\bf \tilde{x}})({\bf x}-{\bf x}_0) $$ Hence, the MVT cannot be directly applied to vector-valued function. However, a straightforward modification to the MVT can yield fruitful results, but seems ignored by many sources (including this very site). We can apply the MVT to each of the $m$ components of ${\bf f}$ separately, and thus we can write $${f_k}({\bf x}) = f_k({\bf x}_0) + {\bf h}_k({\bf \tilde{x}}_k)({\bf x}-{\bf x}_0) , k = 1,...,m$$ I have seen some authors combine the $m$ results and write $${\bf f}({\bf x}) = {\bf f}({\bf x}_0) + {\bf H}({\bf {x}_*})({\bf x}-{\bf x}_0) $$ where ${\bf {x}_*} =[{\bf \tilde{x}}_1, ...,{\bf \tilde{x}}_m]^T$ is now a $m \times n$ matrix, and ${\bf H}({\bf {x}_*}) = [{\bf h}_1({\bf \tilde{x}}_1),...,{\bf h}_m({\bf \tilde{x}}_m)]^T$ . Although I remember seeing this notation multiple times, I can't remember how exactly how it was expressed, or in which article I viewed it. Could someone kindly reminds me how to properly use this notation for vector-valued functions?","['partial-derivative', 'functions', 'derivatives', 'vectors']"
2863271,Ratio of two binomial distributions,"How to estimate
$$
E\left[\frac{X}{X+Y}\right]
$$
for two independent random variables $X\sim Bin(n,p)$ and $Y\sim Bin(m,p)$ ?
Are there any connection with $\frac{n}{n+m}$ e.g., $1-\varepsilon\leq E\left[\frac{X}{X+Y}\right]/\frac{n}{n+m}\leq 1+\varepsilon$?","['binomial-distribution', 'probability', 'ratio']"
2863300,A bug's journey,"I encountered this question here (question 6) http://sections.maa.org/iowa/Activities/Contest/Problems/Probs98.htm The Question: A bug is crawling on the coordinate plane from (7,11) to (-17, -3). The bug travels at constant speed one unit per second everywhere but quadrant II (negative x- and positive y- coordinates), where it travels at 1/2 units per second.  What path should the bug take to complete its journey in minimal time? I'm thinking that the way to solve would be to somehow dilate the quadrant II region by 2, or do some clever reflections. Then the answer would be given by a straight line path. If I try to compute an answer by calculus and Snell's law, it starts to look very very tedious. I tried to simplify the question by placing the end point inside quadrant II, but I couldn't determine the exact path to take. Is there an elegant way to do this problem? Thanks for the help!","['calculus', 'recreational-mathematics']"
2863309,How to calculate the indefinite integral of $\frac{1}{x^{2/3}(1+x^{2/3})}$?,"$$\int \frac{dx}{x^{2/3}(1+x^{2/3})}.$$ I substituted, $$t=\frac{1}{x^{1/3}}$$ $$\frac{dt}{dx} = -\frac{1}{3x^{4/3}}$$ $$\frac{dt}{dx} = -\frac{t^4}{3}$$ Rewriting the question, $$\int \frac{dx}{x^{2/3}+x^{4/3}}$$ $$-\frac{1}{3} \int \frac{dt}{t^4\Bigl(\frac{1}{t^2}+\frac{1}{t^4}\Bigr)}$$ We have, $$-\frac{1}{3} \int \frac{dt}{t^2 + 1}$$ $$-\frac{1}{3}\tan^{-1}t+C$$ $$-\frac{1}{3}\tan^{-1}\Biggl(\frac{1}{x^{1/3}}\Biggr)+C$$ But the answer given is $$3\tan^{-1}x^{1/3}+C$$ Where am I wrong? Any help would be appreciated.","['integration', 'indefinite-integrals', 'trigonometry']"
2863328,Does $A$ is similar to a symmetric matrix $\implies $ $A$ is symmetric?,"Let $A\in \mathbb R^{n\times n}$. Is it true that $A$ similar to a symmetric matrix $\implies $ $A$ symmetric ? Let $B$ symmetric s.t. $A=PBP^{-1}$. Then $$A^T=(P^{-1})^TB^T P^T=(P^{-1})^T BP^T.$$ For me there is no reason that $P$ is orthogonal, so I would say it's false a priori. But in the same time, this theorem should be true since   operator is self adjoint $\iff$ it's diagonalizable. I also know that matrices in any basis of Self Adjoint operator are symmetric. But if A is similar to a symmetric matrix, then it's diagonalizable and thus self adjoint, and thus, it should be symmetric in any basis... this is wrong ? If yes, why ?","['matrices', 'linear-algebra', 'symmetric-matrices']"
2863363,Is the statement that $ \operatorname{Aut}( \operatorname{Hol}(Z_n)) \cong \operatorname{Hol}(Z_n)$ true for every odd $n$?,"Is the statement that $ \operatorname{Aut}(\operatorname{Hol}(Z_n)) \cong  \operatorname{Hol}(Z_n)$ true for every odd $n$? $Hol$ stands here for group holomorph. This problem appeared, when I stumbled upon the following MO question: https://mathoverflow.net/questions/258886/conditions-for-a-finite-group-to-be-isomorphic-to-its-automorphism-group OP of that question provided us with the complete list of groups $G$, such that $|G| \leq 506$ and $ \operatorname{Aut}(G) \cong G$. Among those groups there are some looking like holomorphs of all cyclic groups of odd orders up to $23$. Does anybody know, if that pattern continues or is it just a coincidence?","['automorphism-group', 'cyclic-groups', 'finite-groups', 'holomorph', 'group-theory']"
2863436,Existence of a similar positive definite matrix,"Assume $A$ is a real (non-symmetric) positive definite matrix, that is,
$$ x^T A x > 0 $$
for all real non-zero real $x$. It is easy to prove that the eigenvalues of $A$ have positive real part. Conversely, assume $A$ is a (non-symmetric) real matrix, whose eigenvalues have positive real part. It does not necessarily follow that $A$ is positive definite. Nonetheless, does there exist a similar matrix $B$ which is positive definite? More precisely, does there exist an invertible matrix $P$ such that $P^{-1} A P$ is positive definite?","['change-of-basis', 'linear-algebra', 'positive-definite']"
2863440,Does Wolfram MathWorld misstate Glasser's master theorem?,"Citing from Wikipedia and one of its references, MathWorld , following hold: Glasser's master Theorem For $f$ integrable, $\Phi(x) = |a|x - \sum_{i=1}^N \frac{|\alpha_i|}{x-\beta_i}$ and $a$ , $\alpha_i$ , $\beta_i$ arbitrary real constants the identity \begin{equation}
    \mathrm{PV}\int_{-\infty}^\infty f(\Phi(x)) dx =
    \mathrm{PV} \int_{-\infty}^\infty f(x) dx
    \label{Glasser}
    \tag{1}
\end{equation} holds. Now consider \begin{align*}
    \Phi_1(x) &= |a|x - \sum_{i=1}^N \frac{|\alpha_i|}{x-\beta_i} \\
    \Phi_2(x) &= x - \sum_{i=1}^N \frac{|a\alpha_i|}{x-|a|\beta_i}
\end{align*} Then, by Glasser's theorem \ref{Glasser} $$\mathrm{PV}\int_{-\infty}^\infty f(\Phi_1(x)) dx =
\mathrm{PV} \int_{-\infty}^\infty f(x) dx =
\mathrm{PV}\int_{-\infty}^\infty f(\Phi_2(x)).$$ However, under the change of variables $y = |a| x$ \begin{equation}
    \mathrm{PV}\int_{-\infty}^\infty f(\Phi_1(x)) dx =
    \frac{1}{|a|}\mathrm{PV} \int_{-\infty}^\infty f(\Phi_2(y)) dy.
    \label{my Idea}
    \tag{2}
\end{equation} Thus, I assume Glasser's theorem only holds for $|a| = 1$ ; a quick numerical check seems to support Eq. \ref{my Idea}. Is Wikipedia and MathWorld wrong about this?","['integration', 'solution-verification']"
2863446,Finding roots of unity,"Exercise Let $w, z$ are the unit roots of equation $z^5 = 1$ for $z \in \mathbb{C}$. Prove that $(w^{21} + z^{14})^5$ is always a real number. So, first I can do: $w^{21} = w$ $z^{14} = z^4$ So then I have: $(w + z^{4})^5$ But I'm not sure what I can do with this. I've tried using Newton's Binomial and also trying to write the roots as $e^{i(\frac{\phi + 2k\pi}{5})}$ but because the roots are not multiplied but in a sum I don't think that will be useful. Using Newton's Binomial I ended with a summatory but with different coefficient, so I wasn't able to apply the typical rule to solve these kind of exercises: If w belongs to Gn: $\sum_{i = 0}^{n-1} w^{i} = 0$","['complex-analysis', 'roots-of-unity']"
2863475,Why do we have to calculate 1-binomcdf for P(X>11)?,Why do we have to calculate 1-binomcdf for P(X>11)? Can't we just calculate binomcdf with left and right bound like the normalcdf?,['statistics']
2863530,What can we say about invertible matrix $P$ under a condition that $A=PAP^{-1}$?,"Given an integral matrix $A$, we shall consider the set $$\{P:\text{invertible}|PAP^{-1}=A\}.$$
We can check that the set is a group under the matrix multiplication. Can we say something algebraic for $P$? I am considering the concept of commutator to find some property for $P$. If you have another viewpoint or some comment, can you give it to me?","['abstract-algebra', 'linear-algebra', 'inverse']"
2863536,Finding similar scalene triangles,"Given a scalene triangle $P$, does there exist a similar scalene triangle $P'$, such that when $P'$ is put inside $P$, all three of $P'$ vertices's touch all three of the inner edges of $P$? If so, is there a function that you can use to find the smallest possible $P'$ that will still touch all of the inner edges of $P$?","['triangles', 'geometry']"
2863544,Why is the derivative of these functions a secant line?,"I have been trying to understand the relationship between derivatives of functions, their tangents, and secant lines. I came across this thread , but it has not really helped me understand why certain functions, when derived, give a secant line. For example, if I have a function $$f(x) = x^3 + 2x^2 + 3$$ 
and its derivative, $$f'(x) = 3x^2 + 4x$$ when I graph them, the derivative is a second-degree function, so it will be a parabola and intersect the original function twice. From my understanding, this is a secant line, since it intersects the function two times, whereas a tangent only intersects once at a certain point. Since this is the derivative, how does it give me a secant line? It may be false that the tangent line can only intersect at one point, but that still does not make sense to me. If that is true, how would you know the difference between a tangent line and the secant line? In my case here, how would I find the tangent line? it seems to me that functions with a degree greater than two will have this complication as well. $$f(x) = x^n, n > 2$$ So, if someone would help me understand that relationship between these three concepts and why the tangent line may, in most cases, intersect the function more than once it would help me very much. Thank you.","['calculus', 'derivatives']"
2863551,Are there further primes of the form $\varphi(n)^{\varphi(\varphi(n))}+1$?,"For positive integers $n$ , define $$f(n):=\varphi(n)^{\varphi(\varphi(n))}+1$$ where $\varphi(n)$ denotes the totient function. According to my calculation, for the following positive integers $n$ , $f(n)$ is a prime number : $$[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 18, 97, 119, 153, 194, 195, 208, 224, 23
8, 260, 280, 288, 306, 312, 336, 360, 390, 420]$$ and upto $n=10^4$, no further prime occurs. For $n>6$ , we have $\varphi(\varphi(n))>1$ and $\varphi(n)>1$ hence $\varphi(\varphi(n))$ must be a power of $2$. The number is then a generalized Fermat-number. Do further primes $f(n)$ exist ?","['number-theory', 'totient-function', 'elementary-number-theory', 'prime-numbers']"
2863613,Tetrahedron Centers,"For Triangle Centers , as seen at the Encyclopedia of Triangle Centers , the various centers each have a triangle center function $f(a,b,c)$ that is homogeneous, bisymmetric, and cyclic within barycentric or trilinear coordinates.  Here are functions for centers known to Euclid . name               trilinear  barycentric
X_1   incenter       I   1          a                 angle bisectors  
X_2   centroid       G   1/a        1                 medians
X_3   circumcenter   O   cos(A)     a^2(b^2+c^2-a)    perpendicular bisectors  
X_4   orthocenter    H   sec(A)     tan(A)            altitudes Are there tetrahedron center functions similar to the triangle center functions in barycentric or trilinear coordinates? The barycentric centroid and  trilinear incenter are known. I've made an elaborate Tetrahedron Centers demonstration, and posted code showing $4 \pi =2 \sum dihedral - \sum solid $ . At fermat point I give exact coordinates of centers for a specific tetrahedron. At Dihedral Constant Center some exact coordinates are calculated for a new tetrahedron center. Some available items for tetrahedron $ABCD$ are : volume, total surface area, total perimeter, dihedral constant. solid angle $A$, face area $A$ ($\triangle BCD$), perimeter $A$ ... $B$ ... $C$ ... $D$ edge length $ab$, dihedral angle $ab$ ... $ac$ ... $ad$ ... $bc$ ... $bd$ ... $cd$ I'd like to get a list of tetrahedron center functions started.","['solid-geometry', 'triangles', 'polyhedra', 'geometry']"
2863649,Homomorphisms from $\mathbb Q$ to a finitely generated abelian group,"Let $G$ be a finitely generated abelian group. Prove that there is no non-trivial homomophism from $\mathbb Q$ to $G$. I believe this has to do with divisibility. $\mathbb Q$ is divisible. The image of it under a group homomorphism must be divisible. If $G$ were finite, this would imply that the image is the trivial subgroup since any divisible finite group is trivial. But $G$ is not necessarily finite. What should the argument be like instead?","['group-homomorphism', 'group-theory', 'abstract-algebra']"
2863651,Lee's proof that the normal bundle is embedded,"First I'll fix some of the definitions and context for the forthcoming question. Suppose $M\subseteq \Bbb R^n$ is an embedded $m$-dimensional submanifold. For each $x\in M$, we define the normal space to $\pmb M$ at $\pmb x$ to be the $(n-m)$-dimensional subspace $N_xM\subseteq T_x\Bbb R^n$ consisting of all vectors that are orthogonal to $T_xM$ with respect to the Euclidean dot product. The normal bundle of $\pmb{M}$ , denoted by $NM$, is the subset of $T\Bbb R^n\approx \Bbb R^n\times\Bbb R^n$ consisting of vectors that are normal to $M$:
$$
NM = \big\{(x,v) \in \Bbb R^n\times\Bbb R^n : x\in M,\ v\in N_xM \big\}.
$$ The statement and part of the proof of Theorem 6.23 in Lee's Introduction to Smooth Manifolds is reproduced below: Theorem 6.23. If $M\subseteq \Bbb R^n$ is an embedded $m$-dimensional submanifold, then $NM$ is an embedded $n$-dimensional submanifold of $T\Bbb R^n\approx \Bbb R^n\times\Bbb R^n$. Proof. Let $x_0$ be any point of $M$, and let $(U,\varphi)$ be a slice chart for $M$ in $\Bbb R^n$ centered at $x_0$. Write $\widehat U = \varphi(U)\subseteq \Bbb R^n$, and write the coordinate functions of $\varphi$ as $\big(u^1,\dots,u^n\big)$, so that $M\cap U$ is the set where $u^{m+1}=\dotsb=u^n=0$. At each point $x\in U$, the vectors $E_j|_x = (d\varphi_x)^{-1}\big(\partial/\partial u^j|_{\varphi(x)}\big)$ form a basis for $T_x\Bbb R^n$. We can expand each $E_j|_x$ in terms of the standard frame [emphasis added] as
  $$
E_j\big|_x = E_j^i(x)\frac{\partial}{\partial x^i}\bigg|_x,
$$
  where each $E_j^i(x)$ is a partial derivative of $\varphi^{-1}$ evaluated at $\varphi(x)$, and thus is a smooth function of $x$. The proof goes on, but my question is about this part of the proof. Is $\partial/\partial u^j|_{\varphi(x)}$ literally the partial derivative operator in the $e_j = (0,\dots,0,\underbrace{1}_{\text{$j$th component}},0,\dots,0)$ direction? If not, what is it precisely, using Lee's notation? I suspect that if $\partial/\partial u^j|_{\varphi(x)}$ is not literally the partial derivative operator in the $e_j$ direction, then
$\partial/\partial x^j|_x = (d\varphi_x)^{-1}\big(\partial / \partial x^j|_{\varphi(x)}\big)$, where $\partial / \partial x^j|_{\varphi(x)}$ is
literally the partial derivative operator in the $e_j$ direction, and that $\partial/\partial u^j|_{\varphi(x)}$ must be something else. What is $E^i_j(x)$ explicitly? Note that Lee hasn't yet defined standard frame at this point in the text, so I suppose a complete answer to this question would also address what the standard frame actually is.","['proof-explanation', 'smooth-manifolds', 'differential-geometry']"
2863661,Is there a perfect square that is the sum of $3$ perfect squares?,"This is part of a bigger question, but it boils down to: Is there a square number that is equal to the sum of three different square numbers? I could only find a special case where two of the three are equal? https://pir2.forumeiros.com/t86615-soma-de-tres-quadrados (in portuguese). Any clue?","['number-theory', 'square-numbers']"
2863690,Unpacking a comment about sets of morphisms,"In a comment on this MathOverflow question , Theo Johnson-Freyd states: OTOH, in any category, if there are monomorphisms $A\to B$ and $B\to A$ , then for any $X$ the sets $\operatorname{Hom}(X,A)$ and $\operatorname{Hom}(X,B)$ are isomorphic, by unpacking the word ""monomorphism"" and applying Schroeder-Cantor-Bernstein (one of those rare results with non-alphabetized names). If this isomorphism could be made natural in $X$ , then by Yoneda we would have an isomorphism $A\cong B$ . But of course it cannot, in general. I'm new to category theory and having a hard time unpacking this. I know what a monomorphism is, but it's not clear to me how ""unpacking the word monomorphism and applying SCB"" implies the set of morphisms between $X$ and $A$ and $X$ and $B$ are isomorphic. I'm also confused what it means for this isomorphism to be ""made natural"" in $X$ , and then how does the Yoneda lemma show that there is an ismorphism between $A$ and $B$ ? I can only assume he's referring to the Yoneda embedding, but I still don't see how natural transofmations come in here.","['elementary-set-theory', 'yoneda-lemma', 'category-theory']"
