question_id,title,body,tags
1904461,Trying to show $\Vert A (\sum_{j=1}^n v_j)\Vert_2^2 = \sum_{j=1}^n\lambda_j^2$?,"I have been reading through some lecture notes and I think the author must be mistaken in a particular statement he makes. Suppose $A$ is a symmetric positive definite matrix. Then there exists an orthonormal basis $v_1, \dots, v_n \in \mathbb{R}^n$ of eigenvectors with eigenvalues $0 < \lambda_1 \le \dots \le \lambda_n < \infty$. As the eigenvectors are orthonormal this means that we can write: $$\Vert A (\sum_{j=1}^n v_j)\Vert_2^2 = \sum_{j=1}^n\lambda_j^2$$ I don't think this is correct? I have $$\Vert A (\sum_{j=1}^n v_j)\Vert_2^2 = \Vert \sum_{j=1}^n \lambda_j v_j \Vert_2^2.$$
Taking the $2$ norm gives $$\sum_{i=1}^n |\sum_{j=1}^n \lambda_j v_{j_i}|^2,$$ and I don't think this can be reduced further?","['matrices', 'normed-spaces', 'eigenvalues-eigenvectors', 'orthogonality']"
1904477,"Does the perimeter of a polygon necessarily decrease if more edges are added to it, with the constraint of constant area?","A circle has the lowest perimeter for a 2D shape of a given area. To my understanding, it can also be approximated by a polygon of infinite sides. So, if I take an n-sided polygon and gradually add edges to it, keeping my area constant, will the perimeter also gradually decrease(Since I am approaching a circle)? Thanks!","['area', 'geometry']"
1904510,"What was the original German word Cantor used for ""countable"" and/or ""uncountable""","Apologies if this is slightly off-topic - it's about history of mathematics, but linguistic history specifically:  I suspect I'd get a better answer here than the language SE site due to domain knowledge. I was explaining the concept of countably vs. uncountably infinite sets to a friend, and he had a (probably-not-uncommon?) viscerally negative reaction to that terminology. So I started wondering: how closely do those English terms - ""countable"" and ""uncountable"" - hew to whatever the original German terms Cantor published? I.e., is this a problem of translation or just a normal specialized-jargon-can-be-confusing issue?","['reference-request', 'math-history', 'cardinals', 'elementary-set-theory']"
1904525,First-Order Logic into Set Theory?,"It may sound silly, but I just came to wonder whether you can ""translate"" all well-formed formulas in FOL (without individual constants such as $a,b,\ldots$) into equivalent formulas in set theory (plus Boolean operators) given a first-order model. NOT the other way around, which is to reduce set-theory into first-order logic. For example, it seems like the following equivalences trivially hold (I will assume that first-order predicates are no more than sets): $$\exists x \, Fx \text{ iff }\neg(F = \emptyset)$$
$$\exists x (Px \wedge \exists y (Rxy)) \text{ iff }  \neg(P= \emptyset)\wedge \neg\{(P \cap X)=\emptyset\}\wedge \neg (Y=\emptyset) \text{ where }R=X\times Y$$ Can every well-formed formula be translated into an  equivalent set-theoretic formula in a similar vein? Or to put it another way, can logic formulas with first-order quantifiers ($\exists, \forall$) be suitably translated into set-theoretic formulas without first-order quantifiers? If that is the case, will extending the first-order logic (e.g. into infinitary logic) compromise such set-theoretic translatablity?","['predicate-logic', 'logic', 'elementary-set-theory']"
1904532,Width of a tilted cube,"I'm projecting a cube onto a hexagon (for RGB to HSL conversion) and I want to calculate the width of the hexagon. Cube is, first, tilted by 45° ccw on the x axis, and then, tilted by 35.26° ccw on the y axis. Black corner is at the bottom and the white corner is at the top. Both (black and white) corners pass through the z axis. I have four values at hand, but don't know what to do with them, atm. I can get the width of the hexagon with this 3D software, but I need to be able to calculate it mathematically.","['trigonometry', 'geometry']"
1904536,Can we approximate an $L^1 $function pointwise almost everywhere by a continous function?,"Using this result Under what condition can converge in $L^1$ imply converge a.e.? . Can one infer that we can approximate an $L^1$ a.e by a continuous function on a finite measure space? I.e find $h$ cts s.t $\mid h-g \mid < \epsilon $ a.e Letting $=f_{n}=g-c_{n}$ where $c_{n}$ is a sequence of continuous approximating to $g$. Other suggestions which are simplier if this is true would be appreciated! I might have an alternative solution which is less messy;
Since the measure is finite convergece in $L^1$ implies convergnce in measure which implies there is a subseq convering a.e we can pick/find some cts function from this subsequence that are arbitrarly close a.e to out $L^1$ function. Finite measure was added after Ian's comments!","['lp-spaces', 'measure-theory', 'proof-verification']"
1904553,"Can the limit $\lim_{x\to0}\left(\frac{1}{x^5}\int_0^xe^{-t^2}\,dt-\frac{1}{x^4}+\frac{1}{3x^2}\right)$ be calculated?","$$\displaystyle\lim_{x\to0}\left(\frac{1}{x^5}\int_0^xe^{-t^2}\,dt-\frac{1}{x^4}+\frac{1}{3x^2}\right)$$ I have this limit to be calculated. Since the first term takes the form $\frac 00$, I apply the L'Hospital rule. But after that all the terms are taking the form $\frac 10$. So, according to me the limit is $ ∞$. But in my book it is given 1/10. How should I solve it?","['taylor-expansion', 'integration', 'calculus', 'limits']"
1904566,Inequality of product of matrices,"Is this inequality true?
$X^T P Y \ge \lambda_{min}(P)X^TY $ 
if
  $P$ is a positive definite matrix and $Y=sgn(X)$ where $X$ is a vector, $sgn(X)$ is a vector which its elements are the sign of the elements of the vector $X$, $\lambda_{min}(P)$ is the minimum eigenvalue of $P$.
Generally, can you give my an inequality that relates $X^T P Y$ to $X^T Y$?","['matrices', 'inequality']"
1904575,prove that integral over set of small measure is small,"This is exercise 4.18 from ""A User-Friendly Introduction to Lebesgue Measure and Integration"" by Gail S. Nelson: Let $(X,\mathcal{B},\mu)$ be a complete measure space. Let $f$ be integrable with respect to the measure $\mu$ .  For $A\in\mathcal{B}$ we define $\int_{A} f d\mu=\int f\cdot\mathcal{X}_{A} d\mu$ .  Prove: Given $\epsilon>0$ there is $\delta>0$ such that if $A\in\mathcal{B}$ and $\mu(A)<\delta$ , then $\Big|\int_{A} f d\mu\Big|<\epsilon$ . Note: here, $\mathcal{X}_{A}(x)$ is characteristic function of set $A$ , that is equal $1$ if $x\in A$ and equal $0$ if $x\notin A$ . At the moment, I really don't have an idea on how to start the proof.  Some additional definitions: A function is integrable with respect to the measure $\mu$ if its positive and negative parts are integrable, and non-negative function $f$ is integrable if $\sup\Big\{\int \Phi d\mu\ \Big|\ \textrm{$\Phi$ is a simple function with $0\le\Phi\le f$}\Big\}$ is finite (and integral $\int f d\mu$ is then equal to this supremum). A simple function $\Phi(x)$ is a function defined as $\Phi(x)=\sum_{k=1}^{n}a_{k}\cdot\mathcal{X}_{E_{k}}(x)$ , where $a_{k}$ are real constants and $E_{k}$ are pairwise disjoint measurable sets.",['measure-theory']
1904577,"Gradient of a function on product $(U\times R, g_M \oplus ds^2)$","Consider $U$ an open subset of $\mathbb{R}^n$. It is clear that $U$ is a smooth submanifold of $\mathbb{R}^n$. Now, consider that $U$ is equipped with a Riemannian smooth metric $g$. The pair $(U,g)$ is a Riemannian manifold. Define a smooth function $f : U \times \mathbb{R} \to \mathbb{R}$. Consider that $\mathbb{R}$ is seen as a Riemannian manifold too, equipped with the canonical metric. The product $U \times \mathbb{R}$ can be equipped with the product metric $\tilde{g}$ defined as follows : Let $(p,t) \in U \times \mathbb{R}$. Then we have the following identification :
$$ \mathrm{T}_{(p,t)}\big( U \times \mathbb{R} \big) \simeq \mathrm{T}_pU \oplus \mathrm{T}_{t}\mathbb{R} \simeq \mathrm{T}_pU \oplus \mathbb{R}$$
since $\mathrm{T}_{t}\mathbb{R} \simeq \mathbb{R}$. For all $(p,t) \in U \times \mathbb{R}$, for all $(u_{1}+v_{1},u_{2}+v_{2}) \in \mathrm{T}_{(p,t)}\big( U \times \mathbb{R} \big)$, 
$$ \tilde{g}_{(p,t)}(u_{1}+v_{1},u_{2}+v_{2}) = g_{p}(u_{1},u_{2}) + v_{1}v_{2} $$ My question is : what is the gradient of $f$ (at a point $(p,t)$) ? Since $g$ is a smooth Riemannian metric on $M$, it is necessarily of the form : $\forall p \in U, \, \forall (u,v) \in \mathrm{T}_{p}U, \, g_{p}(u,v) = u^{\top}G(p)v$, with $G$ a smooth function such that $G(p)$ is a $n \times n$ symmetric positive definite matrix for all $p$. For $(p,t) \in U \times \mathbb{R}$, is the gradient $\nabla_{(p,t)}f$ of $f$ of the following form ? 
$$ \nabla_{(p,t)}f = \begin{bmatrix} G(p)^{-1}\mathbf{A} \\ a \end{bmatrix} $$
with :
$$ \mathbf{A} = \begin{bmatrix} \frac{\partial f}{\partial x_{1}}(p,t) \\ \vdots \\ \frac{\partial f}{\partial x_{n}}(p,t) \end{bmatrix} \quad \text{and} \quad a = \frac{\partial f}{\partial x_{n+1}}(p,t). $$","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
1904643,Tips on identifying pigeon and pigeonhole,"I always have trouble trying to exactly identifying the exact pigeon and the pigeonholes for questions with slightly more integers. For example, questions like this. Eleven integer are chosen from 1 to 20 inclusive. Use pigeonhole principle to prove the selection include integer a and b such that b = a + 1 To me, it's a little confusing because of the number of integers and the integer a and b. Is it safe for me to assume the pigeon = 2 because of the integer a and b while the pigeonhole = 11 because of the chosen eleven integer? The above seems to be less confusing compared to another question that uses generalized pigeonhole A fruit basket that contains 10 apples, 8 oranges and 9 banana. If someone pick some fruits without looking, use the generalized pigeonhole principle to determine how many must you pick to be sure of getting at least 5 fruits of the same type With so many potential values lingering in the question, is there any way to simply and identify the pigeon and pigeonhole to use the generalized pigeonhole to prove it?","['pigeonhole-principle', 'discrete-mathematics']"
1904645,Suprema and infima of measures,"Let $(E,\mathcal E)$ be a measurable space and $(\mu_i)_{i\in I}$ be a family of measures. Let: $$\sup_{i\in I} \mu_i : \mathcal E \to [0\, ..\infty], A \mapsto \sup_{i\in I} \mu_i(A)$$ $$\inf_{i\in I} \mu_i : \mathcal E \to [0\, ..\infty], A \mapsto \inf_{i\in I} \mu_i(A)$$ Is $\sup_{i\in I} \mu_i $ / $\inf_{i\in I} \mu_i $ a measure? If not generally what if $I = \mathbb{N}$ and / or  we require the $\mu_i$'s to be localizable, $\sigma$-finite or finite? If not, are there other ways to define suprema and infima in the lattice of measures (satisfying some property)?","['real-analysis', 'measure-theory']"
1904655,Verify this identity: $\sin6\alpha + \sin2\alpha = \frac{\cos^22\alpha-\cos2\alpha\cos6\alpha}{\sin2\alpha}$,Say I want to verify the following identity: $$\sin6\alpha + \sin2\alpha = \frac{\cos^22\alpha-\cos2\alpha\cos6\alpha}{\sin2\alpha}$$ I'm working on the first member of the identity so that it will eventually match the second one. $$\sin6\alpha + \sin2\alpha =$$ $$\sin(2\times3\alpha)+\sin2\alpha=$$ $$2\sin3\alpha\cos3\alpha+2\sin\alpha\cos\alpha$$ That doesn't to be taking me anywhere. Any hints?,['trigonometry']
1904724,Does $\pi$ span an infinite grid?,"This is similar to a random walk using $\pi$ as the input. Consider taking steps on an infinite grid based on the digits of $\pi$ (though we could use any number).  If we express the number in base $8$, then each digit $0$ to $7$ represents a step to one of the $8$ neighbors including diagonals (base 4 would be an alternative with up/down left/right).  One possible mapping of these steps could be from the current position (or pixel) $P$ to one of these neighbors as
$$
\begin{array}  \\ \hline
0 & 1 & 2 \\
7 & P & 3  \\
6 & 5 & 4  \\ \hline
\end{array}
$$
Considering $\pi$ in base 8 we have 
$$
\pi_8 = 3.1103755 \dots
$$ 
so the first step would be one cell to the right (3).  The second and third steps both move one cell up (1).  The fourth step moves up and to the left, $etc$. The first 1022 steps (starting at green position ending at black) looks like this: While discussing this game a friend of mine claimed that as we continued steps toward infinity, $\pi$ would fill the plane.  Clearly a bold claim, and I am suspicious.  Is it provable?  If it doesn't work in base $8$, could it work in the base $4$ case where we move up/down or left right?","['number-theory', 'pi']"
1904748,"How do I calculate $u(w)=\int_0^\infty \frac{1-\cos(wt)}{t}\,e^{-t}\,dt$?","How do I calculate $$u(w)=\int_0^\infty \frac{1-\cos(wt)}{t}\,e^{-t}\,dt$$ I tried to do it, I use partial integration but I get lost. Is there any nice simple way to calculate it?","['derivatives', 'integration', 'calculus']"
1904756,"$|f|\leq |g|$ on $\mathbb{C}$, then $f=cg$. [duplicate]","This question already has an answer here : Property of Entire Functions (1 answer) Closed 9 months ago . Let $f$ and $ g$ be entire functions with $|f(z)|\leq |g(z)|$ for all $z\in \mathbb{C}$. Is it true that $f=cg$ for some $c\in \mathbb{C}$? My attempt: If $g=0$, we are done. Suppose then that $g$ is not identically zero. Let $h=f/g$. Fix an arbitrary $z_0$. If $g(z_0)\neq 0$, then $g$ is nonzero on a neighborhood of $ z_0$ and $h$ is holomorphic at $z_0$. If $g(z_0)=0$, then $g$ is nonzero on a neighborhood of $z_0$ except at $z_0$ (by analytic continuation). Then $h$ is holomorphic on that neighborhood of $z_0 $ except at $z_0$. As $| h|\leq 1$, $h$ has an avoidable singularity at $z_0$, therefore it can be assumed to be holomorphic at $z_0$. Thus, $h$ is entire and by Liouville constant.",['complex-analysis']
1904771,"Why $f(x,y)=y^2$ instead of $f(y)=y^2$?","If I have a function $f(x,y)=y^2$, is it equivalent with $f(y)=y^2$? Are there any differences? Is $f(x,y)=y^2$ a multivariable function despite no $x$?","['multivariable-calculus', 'calculus', 'functions']"
1904774,How to prove that if $\sum _{n=1}^{\infty }a_n\:$ converges then $\sum _{n=1}^{\infty }a_na_{2n}\:$ converges?,"How to prove that if $\sum _{n=1}^{\infty }a_n\:$ converges then $\sum _{n=1}^{\infty }a_na_{2n}\:$ converges? Note: $a_n \in \mathbb R$ I tried to prove it using cauchy criterion
The idea was to use the partial sums. let $\epsilon$>0 There exists $N_1$ such that for any $n>N_1$ and $p\ge1$ $$\left|\sum \:\:\:\:_{k=n+1}^{n+p}a_k\:\right|<\epsilon \:$$
There exists $N_2$ such that for any $n>N_2$ and $p\ge1$ $$\left|\sum \:\:\:\:_{k=n+1}^{n+p}a_{2k}\:\right|<\epsilon \:$$
We will take $N=max\left\{N_1,N_2\right\}$ and then for any $n>N$ and $p\ge1$: $$\left|\sum \:\:_{k=n+1}^{n+p}a_ka_{2k}\:\right|\le \left|\sum \:\:\:_{k=n+1}^{n+p}\left(max\left\{a_k,a_{2k}\right\}\right)^2\:\right|\le ...<\epsilon $$ But I didn't manage to show the part with the three dots. Maybe it's not the way to prove this. Can I get help please ?","['cauchy-sequences', 'sequences-and-series', 'calculus']"
1904790,Confusion about proving homotopy invariance of the contour integral,"This is a technical question about a certain proof of the following theorem. (I'm reading volume 2 of Stein and Shakarchi, pp. 93-95, but I imagine that any other complex analysis textbook has a similar proof.) Theorem: Let $f:\Omega\to\mathbb C$ be a holomorphic function on some domain $\Omega$ and let $\gamma_0,\gamma_1:[a,b]\to\mathbb C$ be closed, piecewise $C^1$ curves. If $\gamma_0$ and $\gamma_1$ are homotopic then 
  $$
\int_{\gamma_0} f(z)\,dz = \int_{\gamma_1} f(z)\,dz.
$$ Here's a (very rough) sketch of the proof, for reference. I'm afraid that even this sketch is too detailed; you might just want to jump to the paragraph starting with ""The Problem"" if you're familiar with this standard proof. Proof: To begin, use a compactness argument to reduce to the case where 
$\sup_{t\in[a,b]}|\gamma_0(t)-\gamma_1(t)|$ is small enough so that we can cover $\gamma_0$ and $\gamma_1$ with discs $D_1,D_2,\dots,D_n$ as in the picture below (except for my labeling being off by one). Specifically, the discs have the following property: there are times $a=t_0 < t_1 < \cdots < t_n = b$ such that if $t_{k-1} \leq t \leq t_k$ then $\gamma_0(t)$ and $\gamma_1(t)$ are contained in $D_k$. Also, choose points of the two arcs in each disc, $z_k=\gamma_0(t_k)$ and $w_k=\gamma_1(t_k)$. This has the effect of dividing both arcs into $n$ smaller arcs. (After this reduction, when I write $\gamma_0$ and $\gamma_1$ I really mean intermediate curves in the homotopy.) To finish, find a primitive (i.e., an antiderivative) $F_k$ of $f$ on each disc $D_k$. The integral of $f$ on each small arc is the difference of the values of $F_k$ on the endpoints of the arc . Now add up some equations, etc etc (omitted details). The Problem: The italicized line worries me. What happens if the intermediate arcs in the homotopy are not $C^1$? In order to use the relationship between primitives and contour integrals we need the contours to have some regularity properties, right? It seems like this proof should break if the homotopy passed through contours that were nowhere differentiable or had unbounded variation, for example. Do we need to assume that the homotopy is through $C^1$ curves?
If so, it seems that we would have to also prove that homotopic curves are $C^1$ homotopic (like the proof of the De Rham theorem).","['complex-analysis', 'proof-verification']"
1904818,"Find the sum of an alternating, non-geometric series","Looking to the following series: $$\sum_{n=1}^\infty \frac{(-1)^n(4n)}{4n^2-1}$$ It converges according to Leibniz criteria. However it does not seem to be a telescopic series (if you take partial fractions, you end up with two positive terms), neither a geometric one, so I can not figure out a way to find its sum. Maybe I am missing something here. Thanks for your time and I appreciate any help.","['sequences-and-series', 'calculus']"
1904821,"What is the exact definition of ""induced homomorphism""?","I am working through Hatcher on my own, but having trouble finding a general definition of ""induced homomorphism,"" at least without resorting to category theory (of which I have no knowledge). I understand Hatcher's specific uses, for example with fundamental groups (p. 34) and homology groups (p.111).  But in his discussions, he also seems to imply that there are certain properties of induced homomorphisms, generally.  Is this correct? Any (non-category-theory) definition and properties would be much appreciated.","['algebraic-topology', 'abstract-algebra', 'terminology', 'definition']"
1904835,Evaluate the integral $\int_{-1}^1 x \ln \frac{x}{1-e^{-a x}} dx$,"How would you evaluate this definite integral with a combination of logarithmic and exponential functions: $$I(a)=\int_{-1}^1 x \ln \frac{x}{1-e^{-a x}} dx, \qquad a \geq 0$$ Mathematica didin't solve it for the general case. My solution is hidden below (spoiler tag doesn't work properly for multiple lines for some reason). Substitute $x=-t$ : $$I(a)=\int_{-1}^1 t \ln \frac{e^{a t}-1}{t} dt$$ $$2 I(a)=\int_{-1}^1 x \ln \frac{e^{a x}-1}{1-e^{-a x}} dx=a \int_{-1}^1 x^2 dx+\int_{-1}^1 x \ln 1 dx=\frac{2a}{3}$$ The answer is: $$I(a)=\frac{a}{3}$$ That's not how I actually found this solution though. But how would you, at a first glance, attempt to solve it?","['integration', 'definite-integrals']"
1904842,In terms of functions why am I allowed to separate $f$ and $g$ from $x$?,"While studying Linear Algebra I came across a function notation that looked like this: $(f + g)(x)$. I also saw that this was the same thing as saying $f(x) + g(x)$. I understand the function notation $f(x)$ and $g(x)$ (which are literally functions) but I do not understand the notation $(f + g)(x)$. Could anyone please fill me in with some introductory info of what this notation means? My guess is that it is a composite function. However I saw that a composite function does not have a plus but a circle in it so I may be wrong. Also from an algebraic perspective, it sort of looks like you can apply the distributive law on $(f + g)(x)$ to make it into $f(x) + g(x)$. That sounds a bit strange to me because notation wise it doesn't make sense. I have heard that this means $(f + g)$ ""evaluated at $x$"", which I don't understand. I would greatly appreciate it if someone gave an overview of the $(f + g)(x)$ notation. Thank you in advance everyone.","['linear-algebra', 'functions']"
1904891,Tilings and meromorphic functions,"This question and its answer by ""J.M."" were quite informative and inspire other questions. If a function is meromorphic on $\mathbb C$ and doubly periodic then, as we all learned at our mother's knee, there is a fundamental domain that is a parallelogram.  So someone asked whether one could do the same with the standard tiling of the plane by regular hexagons, so that the restriction of the function to one hexagon is just a shift of its restriction to any of the other hexagons.  ""J.M."" 's answer was that that is exactly what happens with ""Dixon's elliptic functions"" .  This in no way conflicts with the existence of a fundamental domain that is a parallelogram --- indeed a rectangle (that's an exercise whose solution may take you five seconds). All this immediately inspires two other questions: What about other periodic tilings?  For example, there is a periodic tiling by hexagons, squares, and triangles.  Might the restriction of some doubly periodic meromorphic function to any of those be a shift or maybe a shift followed by a rotation, of the restriction to any of the others?  For that tiling, rotation as well as translation becomes relevant. What about aperiodic tilings?  We'd want a function meromorpic on the whole plane whose restriction to any tile is a shift-plus-rotation of its restriction to any of the infinitely many tiles of the same shape.  For which tilings does such a thing exist? [Aperiodic tilings won't work here. That follows immediately from some basic stuff from complex variables.]","['complex-analysis', 'tiling', 'meromorphic-functions']"
1904905,"Applying MVT and IVT (?) to show something about f'''? (edit: actually, Taylor's Theorem using Lagrange remainder)","I believe that the correct answer to this question will involve applying both the Mean Value Theorem and the Intermediate Value Theorem, perhaps several times.  However, I can't see how to proceed.  I went down what I thought were the typical paths and just reached dead ends. Does someone maybe have a hint?  I think that with the appropriate nudge I should be able to solve this question.  If not, I'll edit and ask if someone knows how to solve the whole thing. Here's the question: Let $f: \mathbf R \to \mathbf R$ be such that $f$, $f'$, $f''$, and $f'''$ exist and are continuous on $\mathbf R$ and satisfying $f(-3)=-1$, $f(0)=0=f'(0)$, and $f(3)=8$.  Prove that there exists $\xi \in (-3, 3)$ such that $f'''(\xi) \ge 1$.","['derivatives', 'real-analysis', 'taylor-expansion', 'calculus']"
1904939,Bochner-Sobolev space vs. Sobolev space on product via Fubini-Tonelli?,"Let $\Omega \subset \mathbb{R}^n$ be a domain. There are two different types of Sobolev spaces on $\Omega \times \mathbb{R}$ that are used in PDE Theory: one is the Bochner-Sobolev space $W^{1,p}(\mathbb{R}, W^{1,q}(\Omega))$ of Banach-space valued functions; the other is the usual Sobolev space $W^{1,p}(\Omega \times \mathbb{R})$ on the product space. It seems to me that there should be some kind of simple relation between these two kinds of spaces coming from Fubini-Tonelli. However, none of the standard references that I've looked at seem to state any kind of result along these lines, which makes me somewhat concerned. Can someone give a reference for a theorem that relates these two? Background: I'm reading a PDE paper which states all of its results in terms of Bochner-Sobolev spaces and I'd like to understand how these results translate back into ordinary Sobolev spaces.","['functional-analysis', 'partial-differential-equations']"
1904941,Every Riemannian metric is conformally related to a complete metric,"If $(M,g)$ is a Riemannian manifold, there is a metric $\tilde g=hg$ on $M$ which is complete, where $h$ is a positive smooth function. I've been given the hint to let $f:M\to \Bbb R$ be a smooth exhaustion function, which means that $f^{-1}(-\infty,c]$ is compact for all $c\in\Bbb R$. Then one should pick $h$ so that $f$ is bounded on $\tilde g$-bounded sets ($\tilde g$-bounded means bounded wrt. the distance function induced by $\tilde g$). (I don't want to repeat the classical proof by Nomizu and Ozeki.) Assuming such an $h$ can be found, I understand how to complete the proof. I think that $h$ should be of the form $\mathrm e^{-2f}$ or $f^{-2}$ so that distances wrt. $\tilde g$ are shorter than wrt. $g$, so that the bounded sets are ""shrunk."" I tried to assume the opposite, namely I chose one of those $h$s and assumed $f$ is unbounded on a $\tilde g$-bounded set $B$. Letting $(y_n)$ be a sequence in $B$ such that $f(y_n)>n$ for all $n$, I tried to contradict that $B$ is bounded, but nothing seems to work. (I just end up with trivial inequalities and the like.) I'd appreciate a hint on how to start. My $h$ is probably wrong, but even if I had the right $h$, I'm not sure how to link $\tilde g$-boundedness to the boundedness of $f$.","['riemannian-geometry', 'differential-geometry']"
1904957,Transforming solvable equations to the de Moivre analogues,"( This question was inspired by this post . ) I. Quintic Given a solvable quintic $$F(x)=0\tag1$$ the problem is to transform it to the de Moivre form $$y^5+5ay^3+5a^2y+b=0\tag2$$ using only a third -degree Tschirnhausen transformation $$y=x^3+c_1x^2+c_2x+c_3\tag3$$ Eliminating $x$ between $(1)$ and $(3)$ yields the quintic $$y^5+d_1y^4+d_2y^3+d_3y^2+d_4y+d_5=0.\tag4$$
The three unknowns $c_i$ allow us to solve the system
$$d_1 = d_3 =0$$
$$d_2^2=5d_4$$ Its final equation turns out to be of $12$-deg, but seems to be factorable and solvable when $(1)$ is solvable. And since the quintic roots $x,y$ are related by $$y=x^3+c_1x^2+c_2x+c_3=2\sqrt{-a}\;\cos\left(-\tfrac{2\pi\,k}{5}+\tfrac{1}{5}\,\arccos\big(\tfrac{-b}{2\sqrt{-a^5}}\big)\right)\tag5$$ then such transformations may yield unusual trigonometric identities like, $$\color{brown}{x^3-\phi\,x^2-\tfrac{7+\sqrt{5}}{2}x+\tfrac{5+4\sqrt{5}}{5} = 2\,\phi\sqrt{\tfrac{11}{5}}\cos\left(-\tfrac{6\pi}{5}+\tfrac{1}{5}\,\cos^{-1}\big(\tfrac{-89-25\sqrt{5}}{44\sqrt{11}}\big)\right)=-4.7985\dots}$$ where $x=2\cos\big(\tfrac{2\pi}{11}\big)$ and the golden ratio $\phi=\frac{1+\sqrt{5}}{2}$. Similar relations in radicals can be found using other quintics. II. Septic Similarly, given the solvable septic
$$F(x) =0\tag6$$
we transform it to the de Moivre quintic's seventh-degree analogue
$$y^7+7ay^5+14a^2y^3+7a^3y+b=0\tag7$$
using a fifth -degree Tschirnhausen 
$$y=x^5+c_1x^4+c_2x^3+c_3x^2+c_4x+c_5.\tag8$$
Eliminating $x$ between $(6)$ and $(8)$ yields the septic
$$y^7+d_1y^6+d_2y^5+d_3y^4+d_4y^3+d_5y^2+d_6y+d_7=0.\tag9$$
The five unknowns $c_i$ allow us to solve the system
$$d_1 = d_3 = d_5 =0$$
$$2d_2^2=7d_4$$
$$d_2^3=49d_6$$
The septic roots $x,y$ are then related by
$$y=x^5+c_1x^4+c_2x^3+c_3x^2+c_4x+c_5=2\sqrt{-a}\;\cos\left(-\tfrac{2\pi\,k}{7}+\tfrac{1}{7}\,\arccos\big(\tfrac{-b}{2\sqrt{-a^7}}\big)\right)\tag{10}$$ III. Questions: If $F(x)=0$ is a solvable equation, will the coefficients $c_i$ of the Tschirnhausen transformation turn out to be radicals ? (I'm of the opinion they are.) For $p=5$, it seems the degree of the $c_i$'s minimal polynomial are quadratics. For $p=7$, are the $c_i$ roots of cubics ?","['minimal-polynomials', 'radicals', 'trigonometry', 'algebraic-geometry']"
1905007,$G$ be a group that has finite number of not normal subgroups. Show that every infinite subgroup of $G$ is normal subgroup.,"Let $G$ be a group that has finite number of not normal subgroups. Show that every infinite subgroup of $G$ is normal subgroup. I would appreciate if someone can give me a hint! My attempt:
I think I solve it! Assume $H$ be an infinite and not normal subgroup of $G$, so exist a member like $g$ that $gHg^{-1}\nsubseteq H $ ,  let consider $ghg^{-1}=x \in gHg^{-1}-H$ so $gxg^{-1}\notin \ \lt{x}\gt $ because $x=g^{-1}x^{n}g={(g^{-1}xg)}^{n}=h^n\in H$ and it is contradiction. so $\lt {x} \gt $ is not normal. If  $gHg^{-1}-H$ is infinite $\lt{x}\gt $ is not normal when $x\in gHg^{-1}-H$ and because $G$  has a finite number of not normal subgroups so exists distinct $x_1,x_2,\dots$ that $\lt{x_1}\gt=\lt{x_2}\gt=\lt{x_3}\gt=\dots $
so $\lt{x_1}\gt$ has infinite generator but every cyclic group has finite genrator and its contradiction. So $gHg^{-1}-H$ is finite now let $\alpha\in gHg^{-1}-H$ and $ gHg^{-1} \cap {H}=K$ so
$$\alpha K \subseteq gHg^{-1}= (gHg^{-1}-H )\cup K $$ and since $gHg^{-1}-H$ is finite and $\alpha K$ is infinite so $\alpha K \cap K \neq \emptyset$ so implies $\alpha K=K$ and $\alpha \in H$ and its contradiction.",['group-theory']
1905019,Finding all integer $n$ so that period of $\cos( n x)\sin(5x/n)$ is $3\pi$,"I want to find all integer $n$ so that the function has period $3\pi$. I am unable to take a start, as there is no general rule for periid of product of periodic functions. Please give a starting.","['periodic-functions', 'trigonometry', 'functions']"
1905021,Is rank of submatrix less than or equal to rank of matrix?,"OK, so I realize this might be a stupid question but an answer can certainly help me in my matrix theory class, I need to know if in general the rank of a submatrix is less than or equal to the rank of the larger matrix? Is it true in general?","['matrices', 'matrix-rank', 'linear-algebra']"
1905063,Is A276175 integer-only?,"The terms of the sequence A276123 , defined by $a_0=a_1=a_2=1$ and $$a_n=\dfrac{(a_{n-1}+1)(a_{n-2}+1)}{a_{n-3}}\;,$$ are all integers (it's easy to prove that for all $n\geq2$, $a_n=\frac{9-3(-1)^n}{2}a_{n-1}-a_{n-2}-1$). But is it also true for the sequence A276175 defined by $a_0=a_1=a_2=a_3=1$ and $$a_n=\dfrac{(a_{n-1}+1)(a_{n-2}+1)(a_{n-3}+1)}{a_{n-4}} \;\;?$$ Update : I crossposted to MO .","['number-theory', 'cluster-algebra', 'sequences-and-series', 'elementary-number-theory']"
1905103,"Every finite subgroup of $GL(n,\mathbb{R})$ is conjugate to a subgroup of $O(n)$","A finite subgroup $G$ of $GL(n,\mathbb{R})$ is conjugate to a subgroup of $O(n)$. Proof. Let's define $\beta \colon \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ such that 
$$ \beta(v,w) = \sum_{g\in G} gv \cdot gw $$
where $\cdot$ is the usual dot product. Then $\beta$ is a bilinear symmetric form. Moreover
$$\beta(v,v) = \sum_{g\in G} \|gv\|^2 > 0, \quad \forall v\neq \mathbf 0 $$ and
$$ \beta(hv,hw) = \sum_{g\in G} ghv \cdot ghw = \sum_{k\in Gh=G} kv \cdot kw = \beta(v,w)$$
Hence $\beta$ is a $G$-invariant inner product and then $G$ is a subgroup of a conjugate of $O(n)$. I can't see how the sentences '$\beta$ is a $G$-invariant inner product' and '$G$ is a subgroup of a conjugate of $O(n)$' are related.","['linear-algebra', 'inner-products']"
1905107,Prove that the $1*1$ tile should be in the center,"We have a $5*5$ room and we want to fill it with $8$,$3*1$ tiles and $1$,$1*1$ tile.Prove that the $1*1$ tile should be in the center. My attempt :I colored it with three colors every $1*3$ tile should fill one of each color because the tiles with the color $B$ are one more than the others the $1*1$ tile should be in one of the tiles with the color $B$.The center is also colored with $B$ but there are also other tiles colored with $B$.I don't know how to do to show that the $1*1$ tile should be in the center that is colored with $B$.Any hints ?",['combinatorics']
1905118,"If $A$ is a set with $m$ elements and $C \subset A$ is a set with $1$ element, then $A-C$ is a set with $m-1$ elements","If $A$ is a set with $m$ elements and $C \subset A$ is a set with $1$ element, then $A-C$ is a set with $m-1$ elements Let $f:\mathbb{N_m} \to A$ and $g:\mathbb{N_1} \to C$ . $f,g$ are both bijections I need to find a bijective map form $N_{m-1} \to (A-C)$ . Now intuitively i can see that in our new map we can define as same way as in $f$ . But how to remove $1$ element from $f$ map and leave others as it is? Thanks",['elementary-set-theory']
1905186,"Let $R$ be a commutative Noetherian ring (with unity), and let $I$ be an ideal of $R$ such that $R/I \cong R$. Then is $I=(0)$?","Let $R$ be a commutative Noetherian ring (with unity), and let $I$ be an ideal of $R$ such that $R/I \cong R$ . Then is it true that $I=(0)$ ? I know that a surjective ring endomorphism of a Noetherian ring is also injective, and since there is a natural surjection from $R$ onto $R/I$ we get a surjection from $R$ onto $R$ , but the problem is I can not determine the map explicitly and I am not sure about the statement. Please help. Thanks in advance.","['ring-homomorphism', 'noetherian', 'abstract-algebra', 'ring-theory', 'ideals']"
1905189,Proving $\sum_{x=0}^{n-1} \cos\left(k +x{2\pi\over n}\right) =\sum_{x=0}^{n-1} \sin\left(k +x{2\pi\over n}\right) =0. $,"Is there anybody who can help me show the following? $$
\sum_{x=0}^{n-1} \cos\left(k +x{2\pi\over n}\right) =0 
\qquad\hbox{and}\qquad 
\sum_{x=0}^{n-1} \sin\left(k +x{2\pi\over n}\right) =0
$$ I can only prove this claim for the sine expression when $k=0$, by grouping the output values such that they all cancel out (for both $n$ even and odd). I have thought about using complex numbers in exponential form to prove it, but I can't do it ... :P I would really appreciate an algerbraic proof.","['summation', 'trigonometry', 'linear-algebra']"
1905217,Is it true that if $n$ is a Carmichael number then $n-1$ cannot be square free?,"It is known that a positive composite integer $n$ is a Carmichael number if and only if $n$ is square-free, and for all prime divisors $p$ of $n$, it is true that $(p-1)\mid (n-1)$. Question: Is it true that if $n$ is a Carmichael number then $n-1$ cannot be square free? I am looking for a proof or disproof of this.","['divisibility', 'number-theory', 'prime-numbers', 'modular-arithmetic', 'elementary-number-theory']"
1905286,$f\in L^1_{\textrm{loc}}(U)$ with $ \int_Ufg\ dx=0 $ for every $g\in C_c^\infty(U)$ implies $f=0$ a.e.,"Suppose $U$ is an open subset of $\mathbb{R}^n$, and $f:U\to\mathbb{R}$ is a measurable [Edited: locally integrable] function such that
  $$
\int_Ufg\ dx=0,\quad \textrm{for every } g\in C_c^\infty(U).
$$
   Then $f=0$ a.e. Suppose otherwise $f\neq 0$ on some measurable subset $V$ of $U$ such that $V$ has positive Lebesgue measure. To get a contradiction, how shall I handle the sign of $f$ on $V$? [Added:] Could anyone come up with a handy theorem that gives the above proposition? [Added:] This argument is used in the proof of uniqueness of weak derivatives in Evans's Partial Differential Equations:","['real-analysis', 'partial-differential-equations', 'reference-request', 'functional-analysis', 'measure-theory']"
1905294,"""Lebesgue"" measurabillity on Riemannian manifolds","Let $X$ be a smooth oriented manifold of positive dimension; Let $g_1,g_2$  be two Riemannian metrics on $X$. Define $\Lambda_1,\Lambda_2:C_c(X) \to \mathbb{R}$ by $$ \Lambda_i(f)=\int_X f \, Vol_{g_i},$$ where $Vol_{g_i}$ is the Riemannian volume form of $g_i$. The discussion here implies that for each $i$, there is a unique $\sigma$-algebra $\mathfrak B_i$, and a unique measure $\mu_i$ such that $I(f)=\int_X f d \mu$ for all $f \in C_c(X)$, and the conditions (a)-(f)** are satisfied. (This is essentially Riesz Representation theorem, with the additional observation that $X$ is $\sigma$-compact $\Rightarrow$ uniqueness of the $\mathfrak B_i$). Questions: (1) Is it true that $\mathfrak B_1= \mathfrak B_2$? (independence of the metric chosen) Assuming this is true, is there a way to define $\mathfrak B_i$ without passing through a Riemannian metric? (This is a natural expectation now since $\mathfrak B_i$ is an invariant of $X$ as a differentiable manifold, thus unrelated to the added Riemannian structure) My guess is that $\mathfrak B_i$ will be the completion of the Borel $\sigma$-algebra w.r.t a suitable measure (which should be any one of the ""Lebesgue"" measures $\mu_i$ mentioned above. On a first glance, this does seem to go through a Riemannian metric, since the $\mu_i$ was induced by it. However, the completion w.r.t a measure $\mu$ is dependent only on the  subsets that have $\mu$-measure zero , and this is independent of the Riemannian metric chosen, and can be defined invariantly (see Lee's book on smooth manifolds, chapter 6: A subset of a manifold has measure zero if its image under every coordinate chart has measure zero  in $\mathbb{R}^n$) (2) A function $f:X \to \mathbb{R}$ is measurable w.tr.t $\mathfrak B_i$ if and only if it is measurable after (pre)composing with a coordinate chart diffeomorphism? (3) Is this the standard way to define $L^p(X)$ spaces (from the perspective of measurability, are the elements of $L^p(X)$ exactly the measurable functions in the sense above, with the additional requirement of finiteness of the integral?) **
(a) $\mathfrak{B_i}$ contains all Borel sets, (b) $\mu(V)=\sup \{I(f): f \in C_c(X), 0\leq f \leq 1, \operatorname{supp} f \subset V\}$ for each open $V$, (c) $\mu(K) < \infty$  for compact $K$, (d) $\mu(E)=\inf \{\mu(V): E \subset V, \ V \mbox{ open}\}$ for each $E \in \mathfrak{B}$, (e) $\mu(E)=\sup \{\mu(K): K \subset E, \ K \mbox{ compact} \}$ for each open $E$ and for each $E\in \mathfrak{B}$ such that $\mu(E)< \infty$, (f) $\mu$ is a complete measure on $\mathfrak{B}$.","['smooth-manifolds', 'real-analysis', 'lebesgue-measure', 'measure-theory']"
1905300,Showing that a curve $\Gamma^*$ is spherical,"I'm having troubles with this differential geometry problem: Let $\Gamma : \overrightarrow{x} = \overrightarrow{x}(s)$ be a curve of $E^3$ with natural parameter $s$ which does not pass through the origin $O$. Now consider the curve $$\Gamma^* : \overrightarrow{x}^*(s) = \frac{\overrightarrow{x}(s)}{f(s)},$$ where $f(s) = \left|\overrightarrow{x}(s)\right|$. I need to show that $\Gamma^*$ is spherical and that $s$ is a natural parameter of $\Gamma^*$ if and only if $f^2 + (f')^2 = 1$. Usually when we want to show that a curve is spherical it is sufficient to show that $$\frac{\rho(s)}{\tau(s)} + (\rho'(s)\tau(s))' = 0$$ where $\rho(s) = \frac 1{\kappa(s)}$ is the radius of curvature and $\tau(s) = \frac 1{\sigma(s)}$ is the radius of torsion. However here I have no way of calculating these scalar quantities. I'm also lost with the second part of the question.",['differential-geometry']
1905311,"$L \log L$-convexity in $L^{1,\infty}$","It can be shown that the Lorentz space $L^{1,\infty}(\mathbb{R}^n)$ (also called weak $L^1$) is not normable and exhibits a lack of convexity. Namely, there exist a sequence $(u_n)\subseteq L^{1,\infty}(\mathbb{R}^n)$ of nonnegative functions and a sequence of real numbers $\lambda_n>0$ such that $\|u_n\|_{1,\infty}\le 1$, $\sum\lambda_n<\infty$, $\sum \lambda_n u_n\not\in L^{1,\infty}$. I read without reference that, however, there is a sort of $L\log L$-convexity: if $(u_n)\subseteq L^{1,\infty}$ is a bounded sequence, $\lambda_n>0$
  and $\sum|\lambda_n|(1+|\log\lambda_n|)<\infty$ (or equivalently
  $\lambda_n\to 0$ and $\sum|\lambda_n\log\lambda_n|<\infty$), then
  $\sum\lambda_n u_n\in L^{1,\infty}$ (meaning that the partial sums converge in the topology of $L^{1,\infty}$). Where can I find a proof of this fact?","['functional-analysis', 'reference-request', 'real-analysis']"
1905316,Understanding a particular proof of the derivative of $e^x$,"There are probably more efficient and easier proofs for same thing. This is proof I have to study for my exam. Theorem: $(e^x)'=e^x$ Proof: For $x\ge 0$ we have defined $f_0(x)=\lim_{n\to +\infty}\left(1+\frac{x}{n}\right)^n.$ Exponential function is now defined by $$ f(x)=e^x=
\begin{cases}
f_0(x),  & x\ge0 \\[4pt]
\dfrac{1}{f_0(-x)}, & x\lt0
\end{cases}$$ For $x,c\gt0$ is $$\frac{\left(1+\frac{x}{n}\right)^n-\left(1+\frac{c}{n}\right)^n}{x-c}=\frac{1}{n}\left[\left(1+\frac{x}{n}\right)^{n-1}+\left(1+\frac{x}{n}\right)^{n-2}\left(1+\frac{c}{n}\right)+\cdots+\left(1+\frac{x}{n}\right)\left(1+\frac{c}{n}\right)^{n-2}+\left(1+\frac{c}{n}\right)^{n-1}\right]$$ We have $$\left(1+\frac{\min{(x,c)}}{n}\right)^{n-1} \le \frac{\left(1+\frac{x}{n}\right)^n-\left(1+\frac{c}{n}\right)^n}{x-c}\le  \left(1+\frac{\max{(x,c)}}{n}\right)^{n-1}$$ For $n \rightarrow +\infty$ we have
$$f_0(\min{(x,c))} \le \frac{f_0(x)-f_0(c)}{x-c} \le f_0(\max{(x,c))}$$ Functions $x \rightarrow \max{(x,c)}$ and $x \rightarrow \min{(x,c)}$ are continuous so for $x \rightarrow c$ is $$f_0^{'}(c) = \lim_{x\to c} \frac{f_0(x)-f_0(c)}{x-c} = f_0(c)$$ Now , for $ x\lt 0$ we have $f(x)=\frac{1}{f_0(-x)}$ , so$$ f'(x)=-\frac{f_0^{'}(-x)(-1)}{[f_0(-x)]^2}=\frac{f_0(-x)}{[f_0(-x)]^2}=\frac{1}{f_0(-x)}=f(x)$$ In case $c=0$ we have to separately look at left and right limit of $\frac{f(x)-1}{x}$. For $x\gt 0$ we have $1\le \frac{f_0(x)-1}{x} \le f_0(x)$, because of continuity of $f_0$ in $0$ and $f_0(0) = 1$ we have $$ \lim_{x\to 0+} \frac{f(x)-1}{x}=1=f(0) $$ For $x<0$ we have $$\lim_{x\to 0-} \frac{f(x)-1}{x} = \lim_{x\to 0-} \frac{\frac{1}{f_0(-x)}-1}{x} = \lim_{-x\to 0+}\frac{f_0(-x)-1}{-x} \frac{1}{\lim_{-x\to 0+}f_0(-x)}=1=f(0)$$ Thus, $f'(0)=f(0)$. So, for $\forall x \in \mathbb{R}$ is $(e^x)'=e^x$. I don't understand couple of things about this proof: 1. How does $(1+\frac{\min{(x,c)}}{n})^{n-1}$ go to $f_0(\min{x,c})$. We haven't got exponent $n$, but $n-1$? 2. Why is this valid: $ 1\le \frac{f_0(x)-1}{x} \le f_0(x)$ for $x\gt0$ 3. Why is this valid : $\lim_{x\to 0-} \frac{\frac{1}{f_0(-x)}-1}{x} = \lim_{-x\to 0+}\frac{f_0(-x)-1}{-x} \frac{1}{\lim_{-x\to 0+}f_0(-x)}=1$ And one personal question, what do you think about this proof, and did your teacher in college requested something similar for you to know for exam?","['derivatives', 'real-analysis', 'exponential-function']"
1905366,How can I find the limit of a function that is inside a limit?,"For example: Given $\lim_{x\to 4} \frac{5xf(x)−1}{x−4}= 8$, find $\lim_{x\to 4} f(x)$. A good hint would be appreciated!","['calculus', 'limits']"
1905371,Show that the linear combination is unique?,"I have the following question:
Let S be a subset of the vector space $\Bbb R^3$ defined by $$
S =  \left\{  ~
\begin{bmatrix}2\\-1\\ 0\end{bmatrix},
\begin{bmatrix}1\\3\\ -2\end{bmatrix}, 
\begin{bmatrix}1\\1\\ 4\end{bmatrix} 
~\right\}
$$ show that 
$v= \begin{bmatrix}-4\\4\\ -6\end{bmatrix}$ is in $\text{span}(S)$ by constructing it as a linear combination of the vectors in $S$. Show that this linear combination is unique. This is what I got after solving part 1: 
$$
c_1\begin{bmatrix}2\\-1\\ 0\end{bmatrix}
 + c_2\begin{bmatrix}1\\3\\ -2\end{bmatrix} 
+ c_3\begin{bmatrix}1\\1\\ 4\end{bmatrix} = \begin{bmatrix}-4\\4\\ -6\end{bmatrix}$$ after solving this I got  $c_1= -2$, $c_2= 1$, $c_3= -1$. How do I show that the linear combination is unique?","['linear-algebra', 'vector-spaces']"
1905391,Is this an adjunction?,"(Write $\langle S\rangle$ for the submonoid generated by $S$.) Let $A$ denote a set and $S$ denote a subset of $A^*$ equipped with a distinguished well-ordering. Let $M$ denote a monoid and $f : S \rightarrow M$ denote a function. Then we get a corresponding function $f' : \langle S\rangle \rightarrow M,$ defined recursively as follows. Firstly, $f'(1) = 1$. Secondly, for all $x \in S$ and $y \in \langle S \rangle$, if for all $x' \in S$ and $y' \in \langle S \rangle$ satisfying $x'y' = xy$ we have $x \leq x'$, then $f'(xy) = f(x) f'(y).$ That's perhaps a little hard to understand, so let me give an example. Let $A = \{b,c\}$ and $S = \{bc \leq b \leq c\}$. Let $M = \{a,b,c\}^*$ and define that $f:S \rightarrow M$ be given by $$bc \mapsto a, b \mapsto b, c \mapsto c.$$ Then the corresponding function $$f' : \langle bc,b,c\rangle \rightarrow M$$ just has the effect of replacing each subword of the form $bc$ with the symbol $a$. For example: $$f'(bbcc) = f(b)f'(bcc) = f(b)f(bc)f'(c) = f(b)f(bc)f(c)f'(1) = f(b)f(bc)f(c) = bac$$ (The initial motivation for this question is that the function $f'$ that replaces $bc$'s with $a$' came up naturally while solving a problem about binomial paths on the latest combinatorics assignment.) Now, getting from $f : S \rightarrow M$ to $f' : \langle S \rangle \rightarrow M$ kind of looks like an adjunction, in the sense of category theory. I haven't been able to formalize this. So: Question. Is this an adjunction? If so, what are the relevant categories and functors? (Lesser question: in monoid theory, does what I'm denoting $f'$ have a name?) Addendum. The motivation behind $f'$ was requested, so here it is. Let $A_k$ denote the set of all $k$-length words $x$ in the alphabet $\{U,D\}$ such that for every prefix $p$ of $x,$ we have $|p|_U \geq |p|_D$. For example, $UUDD \in A_4$ has this property, because the prefixes of this word are $$\{1,U,UU,UUD,UUDD\}$$ and it can be seen that there's always at least as many $U$'s as $D$'s. (This can be visualized as a zig-zag pattern ($U$ = up and one step to the right, $D$ = down and one step to the right) in $\mathbb{R}^2$ that starts at the origin and never goes below the $x$-axis.) Okay. Here's some numbers: $$|A_1| = |\{U\}| = 1$$ $$|A_3| = |\{UUU;UUD,UDU\}| = 3$$ $$|A_5| = |\{UUUUU;UDUUU,UUDUU,UUUDU,UUUUD;UDUDU,UUDDU,UDUUD,UUDUD,UUUDD\}| = 10$$ The problem is to show that: Theorem. If $k$ is odd, then $|A_k| = \frac{1}{2}\binom{k+1}{(k+1)/2}.$ Our solution was as follows. Let $B_k$ denote the set of all $k$-length words $x$ in the alphabet $\{U,D\}$ such that $|x|_U = |x|_D$ and $x_0 = D$. (This can be visualized as a length $2k$ zig-zag pattern that starts at the origin, whose first step is $D$ and which ends somewhere on the $x$-axis). For $k$ odd, $B_k$ is empty. But if $k$ is even, we have: $$|B_k| = \frac{1}{2}\binom{k}{k/2}$$ Now let $k$ denote a fixed but arbitrary odd number. We're going to define a function $$\varphi:A_k \rightarrow B_{k+1}.$$ If this function $f$ can be shown to be a bijection, then we can argue as follows: $$|A_k| = |B_{k+1}| = \frac{1}{2}\binom{k+1}{(k+1)/2}$$ which proves the result. Informally, our proposed $\varphi$ was as follows: Consider $x \in A_k$. Change $x$ by adjoining a $D$ symbol on the left. Note that the result is necessarily an element of $A_{k+1}$, due to the oddness of $k$. Now go through and start changing $U$'s into $D$'s from left-to-right, but with the condition that patterns of the form $UD$ are skipped over; they're not to be touched. Stop when the number of $D$'s is large enough that it equals the number of $U$'s, which is now smaller. Call the result $\varphi(x)$. Here's an illustration of $\varphi$ for $k=5$, for example. The first arrow indicates adjoining a $D$ to the beginning of the word, and the second arrow indicates the rest of the process. $$UUUUU \mapsto DUUUUU \mapsto DDDUUU$$ $$UUUUU \mapsto DUDUUU \mapsto DUDDUU$$ $$UUUUU \mapsto DUUDUU \mapsto DDUDUU$$ $$UUUUU \mapsto DUUUDU \mapsto DDUUDU$$ $$UUUUU \mapsto DUUUUD \mapsto DDUUUD$$ $$UUUUU \mapsto DUDUDU \mapsto DUDUDU$$ $$UUUUU \mapsto DUUDDU \mapsto DUUDDU$$ $$UUUUU \mapsto DUDUUD \mapsto DUDUUD$$ $$UUUUU \mapsto DUUDUD \mapsto DUUDUD$$ The question then becomes: how can we formally define $\varphi$? Let $A$ denote the set $\{U,D\}$ and $S$ denote the poset $\{UD\leq U\leq D\}$. Define $f : S \rightarrow \{A,U,D\}^*$ as follows: $$f(UD) = A,f(U)=U,f(D)=D$$ Then we get a corresponding function $$f' : \{U,D\}^* \rightarrow \{A,U,D\}^*$$ by applying the construction under question. It follows that $f'$ has the effect of removing $UD$ patterns and replacing them with $A$'s. For example: $$f'(DUDUUU) = Df'(UDUUU) = DAf'(UUU) = \cdots = DAUUUf'(1) = DAUUU.$$ Once we've got the word in this form, it becomes easy to define the function that replaces $U$'s with $D$'s without touching $UD$'s, because we've replaced all the $UD$'s with $A$'s. Omitting certain details that the reader can easily work out for themselves, the full chain for $UDUUU \in A_5$ ends up looking something like this: $$UDUUU \mapsto DUDUUU \mapsto DAUUU \mapsto DADUU \mapsto DUDDUU$$","['binomial-coefficients', 'monoid', 'terminology', 'category-theory', 'combinatorics']"
1905392,Curvature tensor of a conformally flat manifold,"Let $M$ be a manifold of dimension $n>3$ and $g$ a Riemannian metric on $M$ which is conformally equivalent to a flat one. Are there formulas (different from $R(u,v)w=\nabla_u\nabla_vw-\nabla_v\nabla_uw-\nabla_{[u,v]}w$) that allow to compute easily the curvature tensor of a conformally flat manifold such as $(M,g)$?","['riemannian-geometry', 'differential-geometry']"
1905397,Is a Sudoku a Cayley table for a group?,"I want to know if the popular Sudoku puzzle is a Cayley table for a group. Methods I've looked at: Someone I've spoken to told me they're not because counting the number of puzzle solutions against the number of tables with certain permutations of elements, rows and columns, the solutions are bigger than the tables, but I can't see why because I don't know how to count the different tables for a group of order 9, and then permute the rows, columns and elements in different ways.  Also I believe the rotations/reflections will matter in comparing these numbers too.  It would also be nice if there was a way to know if the operation is associative just from the table.","['sudoku', 'finite-groups', 'permutations', 'combinatorics', 'group-theory']"
1905416,Closed 1-form on a Riemann surface,"Let $R$ be a Riemann surface of genus $g\ge 2$ and $p\in R$ a point. I need to find a way to produce a closed 1-form $\omega$ on $R$ which satisfies the following conditions: $\omega$ is not exact, not holomorphic and not anti-holomorphic $\omega$ is zero in $p$ $\int_R\omega\wedge \overline{\omega}>0$ Can you help me? The answer of this question Constructing one-forms on a Riemann surface using the uniformization theorem suggests to use the uniformization theorem, but they do it for meromorphic quadratic forms and I don't know how to apply it to my case.","['complex-geometry', 'differential-forms', 'riemann-surfaces', 'differential-geometry']"
1905422,Why does $\frac{1}{x} < 4$ have two answers?,"Solving $\frac{1}{x} < 4$ gives me $x > \frac{1}{4}$. The book however states the answer is: $x < 0$ or $x > \frac{1}{4}$. My questions are: Why does this inequality has two answers (preferably the intuition behind it)? When using Wolfram Alpha it gives me two answers, but when using $1 < 4x$ it only gives me one answer. Aren't the two forms equivalent?","['algebra-precalculus', 'inequality']"
1905462,Number of Differentiable Structures on a Smooth Manifold,"On John Lee's book, Introduction to Smooth Manifolds, I stumbled upon the next problem (problem 1.6): Let $M$ be a nonempty topological manifold of dimension $n \geq 1$. If $M$ has a smooth structure, show that it has uncountably many distinct ones. The trick in this exercise was to use the function $F_s(x) = |x|^{s-1}x$, where $s \in \mathbb{R}$ and $s>0$. This function defines an homeomorphism from $\mathbb{B}^n$ to itself, and is a diffeomorphism iff $s=1$. Now, reading Loring W. Tu's book, An Introduction to Manifolds, he writes: ""It is known that in dimension $< 4$ every topological manifold has a unique differentiable structure and in dimension $>4$ every compact topological manifold has a finite number of differentiable structures. [...]"" Can someone help me explain how this last ""known fact"" and problem 1.6 in Lee's book don't contradict each other? Thanks in advance","['smooth-manifolds', 'differential-geometry', 'differential-topology']"
1905481,"Conditional distribution of $U$ given $\max(U,V)$ for $U$, $V$ i.i.d. uniform on $(0,1)$","Suppose $U,V$ are i.i.d. random variables following Unif$(0,1)$, what is the conditional distribution of $U$ given $Z:=\max(U,V)$ ? I tried writing $Z=\Bbb{I}\cdot V+(1-\Bbb{I})\cdot U$ where $\Bbb{I}=\begin{cases}1&U<V\\0&U>V\end{cases}$ But I am not getting anywhere.","['uniform-distribution', 'probability-theory', 'conditional-expectation', 'probability-distributions']"
1905484,Relation between braided Hopf algebra and usual Hopf algebra.,"What are the relations between braided Hopf algebra and usual Hopf algebra? Acoording to wikipedia , a braided Hopf algebra is defined as follows. Let H be a Hopf algebra over a field $k$, and assume that the antipode of $H$ is bijective. A Yetter–Drinfeld module $R$ over $H$ is called a braided bialgebra in the Yetter–Drinfeld category ${}_{H}^{H}{\mathcal  {YD}}$ if
${\displaystyle (R,\cdot ,\eta )}$ is a unital associative algebra, where the multiplication map ${\displaystyle \cdot :R\times R\to R}$ and the unit ${\displaystyle \eta :k\to R}$ are maps of Yetter–Drinfeld modules,
${\displaystyle (R,\Delta ,\varepsilon )}$ is a coassociative coalgebra with counit ${\displaystyle \varepsilon }$, and both $\Delta$  and ${\displaystyle \varepsilon }$  are maps of Yetter–Drinfeld modules,
the maps ${\displaystyle \Delta :R\to R\otimes R}$ and ${\displaystyle \varepsilon :R\to k}$ are algebra maps in the category ${}_{H}^{H}{\mathcal  {YD}}$, where the algebra structure of ${\displaystyle R\otimes R}$ is determined by the unit ${\displaystyle \eta \otimes \eta (1):k\to R\otimes R}$ and the multiplication map
\begin{align}
(R\otimes R)\times (R\otimes R)\to R\otimes R,\quad (r\otimes s,t\otimes u)\mapsto \sum _{i}rt_{i}\otimes s_{i}u,
\end{align}
and
$$
c(s\otimes t)=\sum _{i}t_{i}\otimes s_{i}. (R\otimes R)\times (R\otimes R)\to R\otimes R,\quad (r\otimes s,t\otimes u)\mapsto \sum _{i}rt_{i}\otimes s_{i}u,
$$
and
$$
c(s\otimes t)=\sum _{i}t_{i}\otimes s_{i}. 
$$
Here $c$ is the canonical braiding in the Yetter–Drinfeld category ${\displaystyle {}_{H}^{H}{\mathcal {YD}}}$.
A braided bialgebra in ${\displaystyle {}_{H}^{H}{\mathcal {YD}}}$ is called a braided Hopf algebra, if there is a morphism ${\displaystyle S:R\to R}$ of Yetter–Drinfeld modules such that
$$
{\displaystyle S(r^{(1)})r^{(2)}=r^{(1)}S(r^{(2)})=\eta (\varepsilon (r))} {\displaystyle S(r^{(1)})r^{(2)}=r^{(1)}S(r^{(2)})=\eta (\varepsilon (r))}$$ 
for all ${\displaystyle r\in R,}$
where $${\displaystyle \Delta _{R}(r)=r^{(1)}\otimes r^{(2)}}$$ is the Sweedler notation. The usual Hopf algebra is a braided Hopf algebra when the braiding is permutation. Is this correct? Thank you very much.","['abstract-algebra', 'representation-theory', 'hopf-algebras']"
1905497,Blow up $\{x_1^4+x_2^2+x_3^2+x_4^2=0\}\subset\mathbb{C}^4$ twice at the origin,"I know the definition of the blow up at a point, but have only computed extremely easy cases. I'm reading a paper where the variety
$$x_1^4+x_2^2+x_3^2+x_4^2=0$$
is blown up twice at the origin. Now I tried to compute what this would look like, but to be honest I have no real idea what to do here. So according to my definition the blow up we would get the variety
$$
\begin{equation*}
V(x_1y_2=x_2y_1,\\
x_1y_3=x_3y_1,\\
x_1y_4=x_4y_1,\\
x_2y_3=x_3y_2,\\
x_2y_4=x_4y_2,\\
x_3y_4=x_4y_3,\\
x_1^4+x_2^2+x_3^2+x_4^2=0
)
\end{equation*}\subset \mathbb{C}^4\times \mathbb{P}^3$$
But now what? This seem quite ugly and unworkable to me. And Now To be honest I don't even know what it would mean to blow up the 'origin' here again, since I don't know what the origin in $\mathbb{C}^4\times \mathbb{P}^3$ is? It would be much appreciated if someone could work out this computation in some detail.","['algebraic-geometry', 'blowup']"
1905513,Lottery System vs combinatorics and geometry,"Let $\mathbb{P}=\{0,\dots,8\}$ be the set of $9$ points in a coordinate geometry over a field $(K,+,\cdot)$. Let moreover consider the set of lines in this geometry:
$$\mathbb{B}=\{\{0,1,2\}, \{3,4,5\}, \{6,7,8\}, \{0,3,6\}, \{1,4,7\}, \{2,5,8\}, \{0,4,8\}, \{3,7,2\}, \{1,5,6\}, \{2,4,6\}, \{0,5,7\}, \{1,3,8\}\}$$
The set above represents all the lines over the square below. If one plays the lottery giving all the sheets of $\mathbb{B}$ he will be surely guess $2$ numbers of the $3$ extracted. Claim: Any other set $\mathbb{B}$ with the above characteristic has still $12$ elements. How to prove the claim? I know that this geometry is isomorphic to the one on $GF(3)$, does it helps?","['finite-fields', 'combinatorics']"
1905533,Find perpendicular distance from point to line in 3D?,"I have a Line going through points B and C; how do I find the perpendicular distance to A? $$A= (4,2,1)$$ $$B= (1,0,1)$$ $$C = (1,2,0)$$",['geometry']
1905543,Find the rank of the tensor,"I need to find the rank of the tensor $t = a \otimes a \otimes b + a \otimes b \otimes a + b \otimes a \otimes a$. For simplicity let $a = (1, 0)^T$ and $b = (0, 1)^T$. I know the answer, rank equals 3. And moreover as I know rank equals 3 for every pair of linear independent vectors $a$ and $b$. But I do not know how to prove that rank equals 3 even in this particular case. I tried to write an assumption that $t = u_1 \otimes v_1 \otimes w_1 + u_2 \otimes v_2 \otimes w_2$ and write elementwise equations. But I got eight equations, so it looks difficult to solve:
\begin{cases}
t_{111} = u_{11}v_{11}w_{11} + u_{21}v_{21}w_{21} = 0\\
t_{112} = u_{11}v_{11}w_{12} + u_{21}v_{21}w_{22} = 1\\
t_{121} = u_{11}v_{12}w_{11} + u_{21}v_{22}w_{21} = 1\\
t_{122} = u_{11}v_{12}w_{12} + u_{21}v_{22}w_{22} = 0\\
t_{211} = u_{12}v_{11}w_{11} + u_{22}v_{21}w_{21} = 1\\
t_{212} = u_{12}v_{11}w_{12} + u_{22}v_{21}w_{22} = 0\\
t_{221} = u_{12}v_{12}w_{11} + u_{22}v_{22}w_{21} = 0\\
t_{222} = u_{12}v_{12}w_{12} + u_{22}v_{22}w_{22} = 0\\
\end{cases} Thanks for the help, do not judge strictly it is my first experience with tensors.","['tensors', 'linear-algebra']"
1905544,Geometric characterization of closed forms,"I was reading the introduction to Bott & Tu and I did not understand the following geometric characterization of closed forms that is explained there.  Let $M$ be a smooth manifold and $\omega \in \Omega^k(M)$ be a $k$ -form of $M$ . The authors claim that a form is closed (i.e. $d\omega = 0$ ) if and only if given any two $k$ -dimensional submanifolds $\Sigma_1$ and $\Sigma_2$ such that the embeddings of these two submanifolds are homotopic relative to their boundaries, then $$
\int_{\Sigma_1} \omega = \int_{\Sigma_2} \omega
$$ and conversely if the above integral equality holds for all such embedded submanifolds then $\omega$ is closed.  Why are these claims true? I suppose this gives a very concrete geometric interpretation of a form being closed.  Is there a similar geometric interpretation of exact forms (i.e. with no mention of the exterior derivative)?","['de-rham-cohomology', 'differential-geometry', 'differential-topology']"
1905547,Let $p$ be a prime of the form $3k+2$. Show that if $x^3 \equiv 1 \pmod p$ then $x \equiv 1 \pmod p$.,"Let $p$ be a prime of the form $3k+2$ . Show that if $x^3 \equiv 1 \pmod p$ then $x \equiv 1 \pmod p$ . What seems like and is probably an incredibly easy question and I'm struggling to get anywhere. I've tried showing that since $p$ is prime and $p|x^3-1$ then either $p|x-1$ or $p|x^2+x+1$ so we're done if we can show that the latter case can never come about, however I'm struggling to show this. I'm doubtful this is the correct approach but I can't see anything else. Any help is appreciated. Thank you.","['number-theory', 'prime-numbers', 'modular-arithmetic']"
1905560,Discontinuous at infinitely many points,"While doing a worksheet on real analysis I came across the following problem. $Q$. Let $f$ be a function defined on $[0,1]$ with the following property. For every $y \in R$, either there is no $x$ in $[0,1]$ for which $f(x)=y$ or there are exactly two values of $x$ in $[0,1]$ for which $f(x)=y$. (a) Prove that $f$ cannot be continuous on $[0,1]$. (b) Construct a function $f$ which has the above property. (c) Prove that any such function with this property has infinitely many discontinuous on $[0,1]$. I really have absolutely no idea how to solve the problem. Even constructing the function is proving pretty difficult. Any help would be appreciated asap.","['continuity', 'real-analysis']"
1905596,Two-Sample Hotelling's $T^2$ test example work through.,"Let $\textrm{x}_{11},\ldots,\textrm{x}_{1n_1}$ and $\textrm{x}_{21},\ldots,\textrm{x}_{2n_2}$ be two observed samples where $\textrm{x}_{ij}$ is a $p$ vector from $\sim N_p (\mu_1,\Sigma)$ and $\sim N_p(\mu_2,\Sigma)$ for the two samples respectevely. From these samples I can find: $\bar{\textrm{x}}_1,\bar{\textrm{x}}_2,S_1,S_2$ where $$S_1=\frac{1}{n_1} \sum_j (\textrm{x}_{1j}-\bar{\textrm{x}}_1)(\textrm{x}_{1j}-\bar{\textrm{x}}_1)'$$
$$S_2=\frac{1}{n_2} \sum_j (\textrm{x}_{2j}-\bar{\textrm{x}}_2)(\textrm{x}_{2j}-\bar{\textrm{x}}_2)'$$
I have that if I let $\mu_1-\mu_2=\delta,$ $$ \bar{\textrm{x}}_1-\bar{\textrm{x}}_2 \sim N_p\left( \delta, \frac{n_1+n_2}{n_1n_2}\Sigma \right)$$ Assume weighted covariance matrix $S=\frac{1}{n_1+n_2}(n_1S_1+n_2S_2)$. How is the $S$ distributed? I know it should be Wishart distribution, but I'm not sure how. I think that $n_1S_1 \sim (n_1,\Sigma)$ or $(n_1-1,S_1)$ if $S_1$ is an estimate of $\Sigma$ and the same argument for $n_2S_2 \sim (n_2,\Sigma)$
My guess is: $S\sim (n_1+n_2-2,\Sigma)$, but I don't fully understand why. Now I'm having trouble with with $T^2$ distribution. My notes only tell me what to do when $\textrm{x}\sim N(\mu,\Sigma)$. But in our case $ \bar{\textrm{x}}_1-\bar{\textrm{x}}_2 \sim N_p(\delta, \frac{n_1+n_2}{n_1n_2}\Sigma)$. Thus I tried to bring it into the form $N(\mu,\Sigma)$. $$\frac{\sqrt{n_1n_2}(\bar{\textrm{x}}_1-\bar{\textrm{x}}_2)}{\sqrt{n_1+n_2}} \sim N_p(\delta, \Sigma)$$ and by the theorem in the book I should try: $$t^2=(n_1+n_2-2) \left(\frac{\sqrt{n_1n_2} (\bar{\textrm{x}}_1-\bar{\textrm{x}}_2)}{\sqrt{n_1+n_2}} -\delta\right) S^{-1} \left( \frac{\sqrt{n_1n_2} (\bar{\textrm{x}}_1-\bar{\textrm{x}}_2)}{\sqrt{n_1+n_2}}-\delta\right)'$$ But my book has the solution: $$t^2=\frac{n_1n_2(n_1+n_2-2)}{(n_1+n_2)^2}(\bar{\textrm{x}}_1-\bar{\textrm{x}}_2-\delta)S^{-1}(\bar{\textrm{x}}_1-\bar{\textrm{x}}_2-\delta)'\sim T^2_{p,n_1+n_2-2}$$","['statistics', 'hypothesis-testing', 'statistical-inference']"
1905598,Bounded function and second derivative implies bounded derivative.,"Suppose that $f$ is a twice differentiable real-valued function on the real line such that $|f(x)| \le 1$ and $|f''(x)|\le 1$ for all $x$. Find, with proof, a constant $b$ such that $|f'(x)| < b$ for all $x$. My instinct is to use the Mean Value Theorem, which gives me
$$\frac{|f(x)-f(y)|}{|x-y|}=|f'(\xi)|, \quad \text{for some } \xi \text{ between } x \text{ and } y,$$
and
$$\frac{|f'(s)-f'(t)|}{|s-t|}=|f''(\nu)| \le 1, \quad \text{for some } \nu \text{ between } s \text{ and } t.$$ From here, I have been unable to find a expression for the upper bound of $|f'(x)|$ in terms of $|f(x)|$ and $|f''(x)|$. Any suggestions on where to go from here, or perhaps a different way to approach the problem?","['derivatives', 'real-analysis']"
1905603,Prove that $\left(\forall x\in \mathbb R\left(f'(x)=f(x)^2\right)\land f(0)=0\right)\implies f=\bf 0$,"Let $f:\mathbb{R}\to\mathbb{R}$ differentiable, $f(0)=0$ and $f'(x)=f(x)^2\; \forall x\in\mathbb{R}$. Show that $f(x) = 0\; \forall x\in\mathbb{R}$. Some thoughts: It can be shown that $f^{(n)}(x) = n!f(x)^{n+1}$, therefore $f^{(n)}(0)=0$. I thought of using Taylor series but that is only useful if the function is analytical. I also tried something with $f(x) = \int_0^x f(t)^2$ but no luck. Solution Since $f'(x)=f(x)^2$, $f$ is increasing. Then $f(x) \geq 0$ for $x>0$ and $f(x) \leq 0$ for $x<0$. Suppose that for some $a>0$ we have $f(a) > 0$, then $\forall x\in(a,\infty),\; f(x)>0$. Then in this interval, we can proceed similarly to the answer provided by @zhw below, to get $f(x) = \frac{-1}{x+c}$, but for $x$ sufficiently large, $f(x)$ would be negative, contradiction. The case for the negative part is similar.","['derivatives', 'calculus']"
1905640,When is $\sin(x) = \cos(x)$?,"How do I solve the following equation? $$\cos(x) - \sin(x) = 0$$ I need to find the minimum and maximum of this function: $$f(x) = \frac{\tan(x)}{(1+\tan(x)^2}$$ I differentiated it, and in order to find the stationary points I need to put the numerator equal to zero. But I can't find a way to solve this trigonometric equation.",['trigonometry']
1905648,Christoffel Symbol in terms of $\sqrt{|g|}$,"I'm trying to understand the conversion of $$ \Gamma^{\mu}_{\mu \nu} = \frac{1}{\sqrt{|g|}} \partial_{\nu}(\sqrt{|g|}) $$ where $g=\det{g_{uv}}$ Working it out, I get to this form of the connection $$ \Gamma^{\mu}_{\mu \nu} = \frac{1}{2} g^{\mu\lambda} \partial_{\nu} g_{\mu\lambda}$$ from here, I'm stuck on how to apply the determinant to the metric and remove the $1/2$ From my general relativity text book, Carroll takes the coordinate transformation and applies the determinant to both sides. $$ g_{\mu' \nu'} = \frac{\partial x^{\mu}}{\partial x^{\mu'}} \frac{\partial x^{\nu}}{\partial x^{\nu'}} g_{\mu \nu}   $$ $$ g(x^{\mu'}) =  \Big| \frac{\partial x^{\mu'}}{\partial x^{\mu}} \Big|^{-2}\ g(x^{\mu})   $$ Why is the Jacobian raised to the power of -2? And where does the square root come in?","['tensors', 'differential-geometry']"
1905652,Proofs of determinants of block matrices [duplicate],"This question already has answers here : Determinant of a block lower triangular matrix (7 answers) Closed 7 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved I know that there are three important results when taking the Determinants of Block matrices $$\begin{align}\det \begin{bmatrix}
A & B \\
0 & D 
\end{bmatrix} &= \det(A) \cdot \det(D) \ \ \ \ & (1) \\ \\
\det \begin{bmatrix}
A & B \\
C & D 
\end{bmatrix} &\neq AD - CB & (2) \\ \\
\det \begin{bmatrix}
A & B \\
C & D 
\end{bmatrix} &= \det \begin{bmatrix}
A & B \\
0 & D - CA^{-1}B
\end{bmatrix} \\ \\
&= \underbrace{\det(A)\cdot \det\left(D-CA^{-1}B\right)}_\text{if $A^{-1}$ exists} \\ \\
&= \underbrace{\det\left(AD-CB\right)}_\text{if $AC=CA$} & (3)
\end{align}$$ Now I understand in result $(3)$, that all that row operations are being performed to bring it into the form we see in $(1)$, but I can't seem to convince myself that result $(1)$ is true in the first place. Furthermore in result $(3)$, I understand that,  $\det(A)\cdot \det\left(D-CA^{-1}B\right) = \det\left(A(D-CA^{-1}B)\right)= \det(AD-CB)$, via the product rule for determinants I also understand that we need $A^{-1}$ to exist, for the initial row operation to reduce the matrix into an upper triangular form $U$, and I understand that we require $AC = CA$, to allow commutativity when we multiply $ACA^{-1}B$ to reduce it to $CB$. Can someone provide proofs for results $(1)$ and $(2)$, as I can't seem to find proofs for them in any of the textbooks I have at my disposal","['block-matrices', 'matrices', 'determinant', 'proof-explanation', 'linear-algebra']"
1905678,What does the second antiderivative of a function represent?,If the antiderivative represents the area under the curve from $f(0)$ to $f(x)$ what does the second antiderivative or the antiderivative of the antiderivative represent?,['calculus']
1905696,A property of the area of quadrilaterals,"I did not know this property of quadrilaterals proposed me a student. I could not prove to the first attempt but I could do after devoting some time. I want to share it here to see if anyone has any proof different from mine. In the four vertices of a quadrilateral $ABCD$ whose area is $S$, they are drawn parallel to two rectangular straight lines which determines two rectangles (green in the figure below) whose areas are $R$ and $r$. Prove that $$ R + r = 2S$$ Explanation of parallel lines: The quadrilateral and a pair of orthogonal directions are given. $A$ and $C$ are opposite vertices. For one rectangle, draw ""horizontal"" lines through $A$ and $C$ and ""vertical"" lines through $B$ and $D$. For the other rectangle, draw ""vertical"" lines through $A$ and $C$ and ""horizontal"" lines through $B$ and $D$.",['geometry']
1905712,Commutator of two Lie subalgebras,"Let $\mathfrak{g}$ be a Lie algebra, and $\mathfrak{h},\mathfrak{m}$ two Lie subalgebras. Is it true that $[\mathfrak{h},\mathfrak{m}]$ is a Lie algebra in itself?. I have tried to develop a commutator of the form $[[a,b],[a',b']]$ (where $a,a'\in\mathfrak{h}$, and $b,b'\in\mathfrak{m}$) by applying repeatedly Jacobi's rule, but it seems that one of the subalgebras must be an ideal.","['abstract-algebra', 'lie-algebras']"
1905744,What is arctan (4/3)?,"I'm refreshing my memory on sines, cosines, SOHCAHTOA, and writing complex numbers in polar form - I chose the complex number $3+4i$ to try and write it in polar form.  The modulus is easily computed to be $5$.  However, I'm having some trouble with computing its argument.  Based on drawing out the right triangle, thinking of $\tan(\theta)$, then the argument must be $\tan^{-1} (\frac{4}{3})$. However, how do we actually compute this? I don't think it's $\frac{1}{\tan(\theta)}$ = $\frac{\cos(\theta)}{\sin(\theta)}$, because that just seems weird and more like cotangent. Where have I gone wrong? Thanks,","['algebra-precalculus', 'complex-analysis', 'complex-numbers', 'geometry']"
1905761,Prove the integral $\int_0^1 \frac{H_t}{t}dt=\sum_{k=1}^{\infty} \frac{\ln (1+\frac{1}{k})}{k}$,"By numerical results it follows that: $$\int_0^1 \frac{H_t}{t}dt=\sum_{k=1}^{\infty} \frac{\ln (1+\frac{1}{k})}{k}=1.25774688694436963$$ Here $H_t$ is the harmonic number , which is the generalization of harmonic sum and has an integral representation: $$H_t=\int_0^1 \frac{1-y^t}{1-y}~dy$$ If anyone has doubts about convergence, we have : $$\lim_{t \to 0} \frac{H_t}{t} = \frac{\pi^2}{6}$$ Which would be another nice thing to prove, although I'm sure this proof is not hard to find. It is also interesting that the related integral gives Euler-Mascheroni constant: $$\int_0^1 H_t dt=\gamma$$","['integration', 'definite-integrals', 'sequences-and-series']"
1905776,Why does a function and its inverse always intersect on the line y=x [duplicate],"This question already has answers here : Will inverse functions, and functions always meet at the line $y=x$? (5 answers) Closed 7 years ago . I've been working through a textbook and noticed that if a function intersects with its inverse function it's always on the line y = x. Why is this?",['functions']
1905785,Why isn't conditional convergence an issue when defining $\sigma$-additivity of signed measures,"A signed measure $\mu$ has to satisfy
$$\mu(\bigcup_{n=1}^\infty A_n)=\sum_{n=1}^\infty\mu(A_n)$$
for $A_n$ disjoint and measurable. Clearly the LHS does not depend on rearrangements of the sequence $(A_n)$, but that's not so clear (to me) for the RHS. What happens if you choose $A_n$ so that the RHS converges conditionally? Doesn't Riemann's rearrangement theorem lead to a problem? Either I'm missing something silly or we just ""define away"" this possibility. I looked at the Wikipedia page and my probability book (Shiryaev) and neither addresses the issue.",['measure-theory']
1905798,"How do I integrate $\int\dfrac{e^x - e^{-x}}{e^x + e^{-x}}\,dx$?","$$\displaystyle\int\frac{e^x - e^{-x}}{e^x + e^{-x}}\,dx$$ In Mathematica: Integrate[(E^x - E^-x)/(E^x + E^-x), x] I'm using the substitution $u=e^x + e^{-x}$, so $du = e^x + e^{-x}$ $e^{-x} = u - e^x$ $e^{x} = u - e^{-x}$ Substituting into the the equation gives: $$\int\frac{2e^x-u}{u}du$$ But I'm not sure where to go from here to get the supposed answer of: $$\log(1 + e^{2x}) - x$$","['ordinary-differential-equations', 'calculus']"
1905837,Is Riesz representation theorem general?,"It is my first course to learn functional analysis. I am looking some video about functional analysis and it seems that this theorem is mainly used in Hilbert space: https://en.wikipedia.org/wiki/Riesz_representation_theorem Book: Functions, spaces, and expansion, By Ole p. 70. However, I also found that this theorem can also be applied to $l^p$ space: Riesz Representation Theorem for $l_p$ Can this theorem be applied to any vector space? (like topological space?)","['functional-analysis', 'riesz-representation-theorem', 'topological-vector-spaces', 'inner-products']"
1905863,Solving $\frac{dy}{dx} = \frac{ay+b}{cy+d}$,"I'm on the section of my book about separable equations, and it asks me to solve this: $$\frac{dy}{dx} = \frac{ay+b}{cy+d}$$ So I must separate it into something like: $f(y)\frac{dy}{dx} + g(x) = constant$ *note that there are no $g(x)$ but I don't think it's possible. Is there something I'm missing?","['integration', 'ordinary-differential-equations', 'calculus']"
1905905,"Find the linear approximation and the derivative of $m$ at $(A,B)\in \mbox{GL}(V)\times \mbox{GL}(V)$ where $m$ is ""composition"" map.","Let $V$ be a finite dimensional vector space over $\mathbb{R}$. Let  $\mbox{GL}(V)\subset\mbox{End}(V)$ denote the subset of invertible maps. Let 
$$m:\mbox{GL}(V)\times \mbox{GL}(V)\longrightarrow \mbox{GL}(V)$$
denote the ""composition"" map defined by
$$(A,B)\mapsto m(A,B)=A\circ B.$$ Find the linear approximation to $m$ at $(A,B)\in \mbox{GL}(V)\times \mbox{GL}(V)$. Give a formula for the derivative of $m$ at $(A,B)\in \mbox{GL}(V)\times \mbox{GL}(V)$. My attempt and questions: We know that $\mbox{GL}(V)$ is an open subset of $\mbox{End}(V)$, therefore $\mbox{GL}(V)\times \mbox{GL}(V)$ is an open subset of $\mbox{End}(V)\times \mbox{End}(V)$, also we know that $\mbox{End}(V)\times \mbox{End}(V)$ is a vector space. Therefore, for $(\tilde{A},\tilde{B})\in \mbox{GL}(V)\times \mbox{GL}(V)$ we have $$m(\tilde{A},\tilde{B})=A\circ B + A\circ (\tilde{A}-A)+(\tilde{B}-B)\circ B+(\tilde{A}-A)\circ (\tilde{B}-B).$$ Then, for $(\tilde{A},\tilde{B})$ sufficiently close to  $(A,B)$ we have $$m(\tilde{A},\tilde{B})\thickapprox A\circ B + A\circ (\tilde{A}-A)+(\tilde{B}-B)\circ B.$$ Is this the linear approximation to $m$ at $(A,B)\in \mbox{GL}(V)\times \mbox{GL}(V)$? In this item, we know that $T_{(A,B)}\left(\mbox{GL}(V)\times \mbox{GL}(V)\right)=\mbox{End}(V)\times \mbox{End}(V)$ and $T_{m(A,B)}\mbox{GL}(V)=\mbox{End}(V)$. Therefore, given $(K,S)\in T_{(A,B)}\left(\mbox{GL}(V)\times \mbox{GL}(V)\right)$, i. e, $K, S \in \mbox{GL}(V)$, let  $c(t):=(A+tK,B+tS)$ be a curve, note that $c(0)=(A,B)$ and $c'(0)=(K,S)$. Therefore, $$\begin{array}{rcl}\left.Dm\right|_{(A,B)}(K,S)&=&\left.\frac{d}{dt}\right|_{t=0}m(c(t))=\left.\frac{d}{dt}\right|_{t=0}m(A+tK,B+tS) \\ &=& \left.\frac{d}{dt}\right|_{t=0}\left(A\circ B+t K\circ B+t A\circ S+t^{2} K\circ S\right) \\ &=&K\circ B + A \circ S.\end{array}$$   Therefore, the formula for the derivative of $m$ at $(A,B)$ is $$\begin{array}{rcl}
\left.Dm\right|_{(A,B)}:T_{(A,B)}\left(\mbox{GL}(V)\times \mbox{GL}(V)\right) &\rightarrow & T_{m(A,B)}\mbox{GL}(V) \\
(K,S) &\mapsto & K\circ B + A \circ S.
\end{array}$$ Is my answer correct?","['derivatives', 'differential-geometry', 'proof-verification', 'lie-groups']"
1905935,How do you show that two sets are disjoint?,"Here's a problem I am trying to solve for recreation. 
$$
A\cap B\subset C' \text{ and } A\cup C\subset B. \hspace{2 mm}\text{    Show that $A$ and $C$ are disjoint.}
$$ I can clearly see how A and C would be disjoint. Essentially, if my understanding is correct, A and C are non-overlapping sets within the bound of set B . But, I'm not exactly clear on how I would prove this by set logic. If you could provide some guidance, I would really appreciate it.",['elementary-set-theory']
1905947,"difference between hyperplane and plane, examples, pictures","What is the difference between these two objects? In 2D?
In 3D?
In 4D? It would be great if anyone can give me some examples distinguish the two concepts and pictures.","['vector-spaces', 'linear-algebra', 'geometry']"
1905977,Deriving the Covariance of Multivariate Gaussian,"I've been having trouble figuring out how to directly prove that the covariance of the multivariate Gaussian distribution $p(x) = \dfrac{1}{{{(2\pi)}^{\frac{n}{2}}}|\Sigma|^{\frac{1}{2}}}\exp\{{-\dfrac{1}{2}(x - \mu)^T \Sigma^{-1}(x-\mu)}\}$ where $x \in \mathbb{R}^n$ and $\Sigma$ is positive definite, is actually $\Sigma$. That is to say, given $\Sigma$ and the above density function, I want to prove that the covariance is $\Sigma$. I've been able to prove that the mean is $\mu$ by diagonalizing the matrix $\Sigma$ and integrating, but I'm struggling to do it for the proof that the covariance is $\Sigma$. Can anyone help?","['probability-theory', 'probability', 'probability-distributions']"
1906059,"Does there exist a non-constant function $f:\mathbb N^2 \rightarrow \mathbb N$ such that $f(x,y)+f(y,x)=f(x^2,y^2)+1$","Does there exist a non-constant function $f:\mathbb N^2 \rightarrow \mathbb N$ such that 
  $$f(x,y)+f(y,x)=f(x^2,y^2)+1$$
  for all positive integers $x,y$? I think that such a function does not exist. But I do not know how to prove","['functions', 'functional-equations']"
1906066,Proving that symmetric matrix can be shifted to be positive definite,"In my studies of matrix analysis, particularly in positive semidefinite matrices, I have come across the following question: Let $ I $ be the $ n \times n $ identity matrix, and let $ v $ be a length n real column vector, we are asked to prove there exists a positive $ \delta > 0 $ such that the matrix $ I - \delta vv^T $ is positive semi definite. Now, I know for any positive $ \delta $ we know that $ I - \delta vv^T $ is a real symmetric matrix and thus its positive semi definiteness is equivalent to non-negativity of its eigenvalues, but how do I prove there exists a positive $ \delta > 0 $ such that the eigenvalues of $ I - \delta vv^T $ are non negative and I have no real idea on how to do this, or even if this is the right approach, I would certainly appreciate any help on this, I thank all helpers.","['matrices', 'eigenvalues-eigenvectors', 'positive-definite', 'linear-algebra']"
1906101,Help finding out details of a function,"So i have this function:
$$f(x)= (2-x)e^{-\sqrt{1-x}}$$ And it's domain is $x\leq 1$ if investigated that correctly.
Now i am confused about it's codomain, i came to an assumption it's $y\in(0,1]$
But is it really accurate enough?
I am trying to draw it with using derivatives, but i guess this part is important as well, i'm presuming it won't be even nor odd function because of the exponential part.
I will use only 1st and 2nd derivative so that shouldn't be a problem.
So my main question is if i did the codomain and domain part correctly 
Thank you in advance. Edit: i got the first derivative  like this:
$$f'(x)=2(-x+1)^{\frac{1}{2}}e^{(-x+1)^{\frac{1}{2}}}$$ If someone can check it it would be great, cause i feel like it could be wrong, but well this function seems like it won't have any zeros at this domain.","['calculus', 'functions']"
1906134,A problem about oriented face in Square grid,"Consider a $n \times n$ square grid (finite) (a square is divided into smaller squares by lines parallel to its sides). The boundary of the square is oriented, (clockwise or anticlockwise) that is, a direction is chosen on it and fixed, such that if you move in that direction along the boundary, the internal points of the square always stay on your left or on your right (depending on the orientation). For each of the internal edges of the subdivision, a direction is specified, such that for each interior vertex, there are exactly two edges coming to the vertex and two edges going away from it (see diagram below). Then my question is that does it follow that there is atleast one oriented face in the subdivision? (For example in the figure, there is exactly one such face, namely in the extreme lower right corner).","['graph-theory', 'logic', 'discrete-mathematics']"
1906149,Basis of cohomology of curve,"Let $R$ be a Riemann surface of genus g and $p\in R$ a point. I'm searching for a way to compute linearly independent differential 1-forms on $R$ which: are closed and not exact, not holomorphic, not anti-holomorphic are zero on $p$ Is there a standard way to do so? Thank you","['riemann-surfaces', 'complex-geometry', 'algebraic-geometry', 'differential-forms', 'differential-geometry']"
1906173,Proof that linearly dependent rows/columns of $A \implies \det(A) = 0$,"I have seen in a few textbooks of proofs of $\det(A) = 0$ if the rows/columns of $A$ are linearly dependent from the properties of the determinant, but I have yet to see a proof that $\det(A) = 0$ from the definition of the determinant: $$\det(A) = \sum \text{sgn}(P)a_{1\alpha}a_{2\beta}...a_{n\omega}$$ where $(\alpha, \beta, ..., \omega)$ are permutations of the columns of $A$. And since all the properties of $\det$ follow from it's definition, I find that proving $\det(A) = 0$ if the rows/columns of $A$ are linearly dependent from the properties of the determinant, to be somewhat unsatisfactory. Can someone provide a proof from the Leibniz Expansion definition of the determinant, that I provided above, of the result that $\det(A) = 0$ if the rows/columns of $A$ are linearly dependent?","['matrices', 'proof-explanation', 'linear-algebra', 'determinant']"
1906210,Why are angles defined as positive counter clockwise? [duplicate],"This question already has answers here : Why are quadrants defined the way they are? (3 answers) Closed 7 years ago . A rather peculiar question and off topic in every way but though.
In almost every situation clockwise is considered to be positive but not when it comes to angles. Why is that? 
Euler's fault or ...",['geometry']
1906241,When not to treat dy/dx as a fraction in single-variable calculus?,"While I do know that $\frac{dy}{dx}$ isn't a fraction and shouldn't be treated as such, in many situations, doing things like multiplying both sides by $dx$ and integrating, cancelling terms, doing things like $\frac{dy}{dx} = \frac{1}{\frac{dx}{dy}}$ works out just fine. So I wanted to know: Are there any particular cases (in single-variable calculus) we have to look out for, where treating $\frac{dy}{dx}$ as a fraction gives incorrect answers, in particular, at an introductory level? Note: Please provide specific instances and examples where treating $\frac{dy}{dx}$ as a fraction fails","['derivatives', 'differential', 'calculus']"
1906263,"Why is the unit sphere strictly convex w.r.t ""continuous"" combinations?","Let $\mu$ be a probability measure $\mathbb{R}^n$ (on the Borel or Lebesgue $\sigma$-algebra, I do not really care) which is supported on $\mathbb{S}^{n-1}$. (You can just think on a measure on the sphere itself). Suppose that for some unit vector $v \in \mathbb{S}^{n-1}$, it holds that $$ v= \int_{\mathbb{S}^{n-1}} x \, d\mu(x)$$
(That is $v$ is a ""continuous"" convex combination of the points of the sphere). How to prove that $\mu = \delta_v$? (I know this holds for finite combinations, and I am not sure how to generalize to the continuous case)","['convex-analysis', 'spheres', 'spherical-geometry', 'probability', 'measure-theory']"
1906275,Is $\sqrt{\lambda_{\max}(A^2)} = \lambda_{\max}(A)$?,"I'm trying to prove a statement on the condition number of a matrix in the 2-norm for a symmetric positive definite matrix $A$. I nearly have the proof completed, if $\sqrt{\lambda_{\max}(A^2)} = \lambda_{\max}(A)$ then the proof is finished. But I'm not sure if this is the case? Is $\sqrt{\lambda_{\max}(A^2)} = \lambda_{\max}(A)$ for a symmetric positive definite matrix?","['matrices', 'linear-algebra']"
1906295,Intuition on an unexpected finite dimensional space,"We had this exercise that uses Riesz' lemma to prove that a functional space actually has a finite dimension. I was curious to know what this space ""looks"" like, if we can find natural elements that would generate it. Here are the assumptions: $E = \mathcal{C}^0([0,1], \mathbb{R}) $ is the space of continuous functions on $[0, 1]$, endowed with the norm $ ||f||_\infty = \sup\{|f(t)|, t \in [0, 1] \} $. We have a closed linear subspace $F$ of $E$ such that $F \subset \mathcal{C}^1([0,1], \mathbb{R})$ (the space of continuously differentiable functions with continuous derivatives) and
$$ \exists C > 0, \forall f \in F, \; ||f'||_\infty \leq C \, ||f||_\infty $$ You can show that the unit ball of that space is compact for $ ||\cdot||_\infty$ (using Ascoli), hence by Riesz' lemma the space has a finite dimension (!) If I follow the conclusion then I imagine that you could build up a finite family of functions that would generate the whole space. Is that true? What would these functions look like? Are they useful somewhere else? Are they known?",['functional-analysis']
1906297,How to solve $x = e^{1/(4x)}$?,I need to find the solution to this equation : $$ x = e^{1/(4x)} $$ Can you give me a hint ?,"['logarithms', 'functions']"
1906318,Orthogonal eigenvectors in symmetrical matrices with repeated eigenvalues and diagonalization,"Symmetrical matrices have orthogonal eigenvectors. However, there is the special case when eigenvalues are repeated. The ultimate scenario is that of the identity matrix. Professor Strang mentions here that ""if an eigenvalue is repeated, then there is a whole plane of eigenvectors, and in that plane we can choose perpendicular ones""... a ""real substantial freedom."" And he goes on to note that symmetrical matrices can be diagonalized as ${\bf A = Q\Lambda Q'}$. Although he does mention ""... I also mean that there is a full set of them [eigenvectors]"", it sounds (in the video) as though the ${\bf Q\Lambda Q'}$ is not necessarily jeopardized by the presence of repeat eigenvalues. However, and in general for square matrices (not limiting ourselves to symmetrical), repeated eigenvalues can render the matrix non-diagonalizable as $\bf{A=S\Lambda S^{-1}}$. How does all this come together into a question? The ${\bf A'A}$ matrix has many properties shared by positive semidefinite matrices . Among them is its being diagonalizable. Now its eigenvalues do not have to necessarily be distinct (real, yes; but not necessarily of algebraic multiplicity of $1$). So... Is it correct to say that symmetrical matrices have always orthogonal eigenvectors (or we can choose them so that they are), guaranteeing the ${\bf Q\Lambda Q'}$ decomposition, regardless of the possible presence of repeat eigenvalues? If (1) is not true, are we then stuck with a caveat to the assertion that ${\bf A'A}$ matrices are diagonalizable? Can we say that ${\bf A'A}$ is diagonalizable as a blanket statement?",['linear-algebra']
1906329,Graphical explanation of Riesz's lemma,Does there exist an intuitive graphical explanation of Riesz's lemma?,['functional-analysis']
1906360,"Compute $\lim_{t\rightarrow0}\int_B \frac{l(t,x,y)}{t^2}d(x,y)$.","This is a problem that I saw on the Internet. I'd like some hints to solve it. Let $B$ be a ball in $\mathbb{R}^2$. Given $(x,y)\in B$, consider the circle of radius $t>0$ and centre $(x,y)$. Let $l(t,x,y)$ be the length of the arc of that circle that is outside of $B$. Find $$\lim_{t\rightarrow0}\int_B \frac{l(t,x,y)}{t^2}d(x,y).$$ Here is a picture: in black the boundary of the Ball $B$. In $\color{red}{\text{red}}$ (+ blue) the circle considered (centered at some $x,y$ with radius $t$). In $\color{blue}{\text{blue}}$ the arc whose length we define to be $l(t,x,y)$","['contest-math', 'real-analysis', 'integration']"
1906389,How to transform this equation?,"Let and I am reading a paper where the author states that in terms of the coordinates ($\theta$, $\phi$) the second equation is translated to Does anyone know how this happens? Thanks.","['trigonometry', 'complex-numbers']"
1906390,"If $\operatorname{rank} (AB) = \operatorname{rank} (BA)$ for any $B$, then is $A$ invertible?",Let $A$ and $B$ be two (nonzero) real square matrices and suppose that $\operatorname{rank} (AB) = \operatorname{rank} (BA)$ for any $B$ . Can one prove that $A$ is invertible? (The converse is true and a simple linear algebra question but I'm stuck on this one.),"['matrices', 'matrix-rank', 'linear-algebra', 'inverse']"
1906448,how find $f(f(f(2^{1388})))$ ? when `f(x)` is sum of digits of `x`,We define f(n) = sum of digits of n . Is there a simple and logical way for calculate $f(f(f(2^{1388})))$? thanks in advance.,"['number-theory', 'elementary-number-theory']"
1906451,How to raise a number to a power geometrically.,"There are methods to add two lines of arbitrary lengths or multiply them together known since Greek times; and more advanced methods based on the concepts of bases and units. But, I have not been able to find a way to exponentiate a number geometrically without using algebra. I would love if someone could somehow illustrate the concept. Basically I am asking is it possible to draw the graph of a^x geometrically. On questions raised by Aretino and RickyDemer I want to clarify that:
I am talking about Euclidean geometry (so a collapsible compass,straight-edge are allowed); although, Cartesian geometry is fine, too. Also, is there a book that can teach a basic concept as this? You know, a book on Euclidean geometry that teaches exponentiation, multiplication etc.","['exponentiation', 'geometry']"
1906508,Finding set of triplets which sum to N with additional constraints.,"Given a number n, we construct a set S with triplets $(x_i, y_i, z_i)$ such that $x_i+y_i+z_i = n$  $\forall$ $ 1 \le i \le |S| $. Also $x_i\ge 0, y_i\ge 0, z_i\ge 0$. Additionally we must satisfy the constraint that all $x_i's$ in S are unique and all $y_i's$ in S are unique and all $z_i's$ in S are unique. Find the maximum size of set S and find any such set S. This problem was part of a contest here: Beautiful Set 3 which has ended. There is tutorial section which is very terse. Although I understand what is being done there it is not very clear why that algorithm would give a correct answer. So if you help in understand the tutorial or give another answer/algorithm, I would greatly appreciate it.","['computer-science', 'discrete-mathematics']"
