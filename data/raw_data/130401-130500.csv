question_id,title,body,tags
2026453,"For every $n$, $\sum\limits_{m=0}^{\infty}\left(-\frac{1}{2}\right)^m\sum\limits_{l=0}^m\left(\frac{1}{2^{n-1}}-2\right)^{m-l} {n \choose l}=1$","Show that for every $n$ we have that $$\sum_{m=0}^{\infty}\left[\left(-\frac{1}{2}\right)^m\sum_{l=0}^m\left(\left[\frac{1}{2^{n-1}}-2\right]^{(m-l)}{n \choose l}  \right)\right]=1$$ Hint: Represent the function $$F(x) =\sum_{m=0}^{\infty}\left[x^m\sum_{l=0}^m\left(\left[\frac{1}{2^{n-1}}-2\right]^{(m-l)}{n \choose l}  \right)\right]$$ as the product of two generating functions. I'm really stuck. We know that $$1 - \frac{1}{2} + \frac{1}{4} - \frac{1}{8} +... = \frac{1}{1+\frac{1}{2}} =\frac{2}{3}$$ So I thought it would be enough to show that $\forall m$ $$\sum_{l=0}^m\left(\left[\frac{1}{2^{n-1}}-2\right]^{(m-l)} {n \choose l}  \right) = \frac{3}{2}$$ But 1) I've had a hard time trying to show that the sum above has a constant value independent of $m$ (by transforming $2^{n-1}$ in a sum of binomial coefficients, and seeing if something gets cancelled out). 2) The hint confuses me. As noticed, I wasn't applying it in my only attempt, and at the same time I don't know how to take advantage of that fact.","['generating-functions', 'combinatorics', 'discrete-mathematics']"
2026512,Is $f+g$ and $fg$ maximum where $f$ and $g$ are maximum?,"If $g$ and $f$ are two real functions and I know that both $f$ and $g$ have a maximum in $x_0 \in \mathbb{R}$ can I say that $f+g$ has a maximum in $x_0$? If this is true, can the same be said if I have $fg$? That is, if  both $f$ and $g$ have a maximum in $x_0 \in \mathbb{R}$ can I say that $fg$ has a maximum in $x_0$?","['derivatives', 'real-analysis', 'calculus', 'functions']"
2026541,Prove that a function containing $n!$ is decreasing,"Taking the derivative of the function $f(n)=n!$ with $n \in \mathbb{N}$ is not possible. But is there some alternative way to understand if a function with $n!$ is decreasing (besides the definition of decreasing, i.e. proving that $f(n+1)<f(n)$ directly, which is hard to do in some cases). For example: suppose that I need to use Leibniz rule of convergence for this series.
$$\sum_{n \geq 1} (-1)^n \frac{\mathrm{sin}(n!)}{n^5}$$
Firstly I must prove that $ \frac{\mathrm{sin}(n!)}{n^5}$ is decreasing, but how can I do without taking the derivative?","['derivatives', 'calculus', 'factorial', 'sequences-and-series', 'elementary-number-theory']"
2026584,Convergence of $\sum_{n=1}^\infty \frac{\cos(n)}{n}$,Does this series converge $$\sum_{n=1}^\infty \frac{\cos(n)}{n}$$ someone has told me that I have to apply Dirichlet's test but I don't know how to calculate the sum $$\left|\sum_{n=1}^{N} \cos(n)\right|$$,"['sequences-and-series', 'calculus']"
2026629,Proving the composition of a symmetric relation is commutative,"How can I show that $R \circ S = S \circ R$ if $S, R$ are symmetric? I'm working on a longer proof and this is one of the lemmas I think I need to show, since in general a composition of relations isn't necessarily commutative. I'm struggling with how to use composition in a proof in general so I'm not really sure where to start with this.","['relations', 'proof-writing', 'symmetry', 'elementary-set-theory']"
2026637,Find slope using X-Intercept and Y-intercept,"My mathematics teacher explained how to find slope using 2 different points but I was never taught how to use the X-Intercept and the Y-Intercept to find the slope. Example question:
What is the slope of a line that has an X-Intercept of 8 and a Y-Intercept of 11?","['algebra-precalculus', 'slope']"
2026654,Difference of fourth powers in three ways,"I'd like to find a number that's the difference of fourth powers in three ways or more. I.e.: $$k=a^4-b^4=c^4-d^4=e^4-f^4$$
Is this possible? There seem to be plenty of examples of differences of fourth powers in two ways. The smallest:
$$300783360=133^4−59^4=158^4−134^4$$ I've checked numbers $a,b$ up to ~$10,000$ with a python script with no results.","['number-theory', 'perfect-powers']"
2026682,Product of immersions is immersion?,"If $U\subseteq \mathbb{R}^n$ and $V\subseteq \mathbb{R}^m$, let's define immersions $f\colon U\longrightarrow \mathbb{R}^n$ and $g\colon V\longrightarrow \mathbb{R}^m$, if I define
$$h\colon U\times V\longrightarrow \mathbb{R}^{m+n}$$
With $h((u,v))=(f(u),g(v))$, then it is an immersion too? Why?",['differential-geometry']
2026684,Find the Method of Moments estimator of $\theta$ and derive its asymptotic distribution,"Let $Y_1, \ldots , Y_n$ be a random sample from the uniform distribution on the interval $(0, \theta)$
with an unknown $\theta > 1$. Suppose we only observe for $i = 1, \ldots , n$ $$X_i= \begin{cases} Y_i & \text{if } Y_i \leq 1, \\ 1 & \text{if } Y_i > 1. \end{cases}$$ Find the Method of Moments estimator of $\theta$ and derive its asymptotic distribution So I found the expected value like usual when finding an MME, however I was unable to solve for $\theta.$ This is what I got: $$ \operatorname{E}(X_i) = \frac{\theta-1} 2 +\frac 1 \theta$$ is my $\operatorname{E}(X_i)$ wrong? Because I can't seem to solve for $\theta$ when equating it to the sample mean","['censoring', 'statistics', 'probability', 'statistical-inference']"
2026710,Prove or disprove: All radial vector fields are conservative,"This was a question a calculus student had asked me, and unfortunately I believe I gave an incorrect proof. DEFINITION : A radial vector field is defined by ${\bf F}(x,y) = g(x,y){\bf r},$ where $g$ is a scalar function and ${\bf r}= \langle x,y\rangle$ is a vector. At the time, I did not know the complete definition of a radial vector field. I proved it like so: PROOF : Let $a,b\in\mathbb R$. Take ${\bf F}(x,y) = ax{\mathrm i} + by\mathrm j$. Consider the function $f(x,y) = {1\over2}ax^2 + {1\over2}by^2$. Then clearly $\nabla f(x,y) = ax\mathrm i + by\mathrm j = {\bf F}(x,y)$, hence all radial vector fields are conservative. However, this is not correct because it only works for fixed constants $a,b$. For a general function $g(x,y)$, what might we be able to do? The more I think about it, the less I think the conjecture is true but I can't think of a counter example.","['fake-proofs', 'calculus', 'multivariable-calculus', 'proof-writing', 'vectors']"
2026727,Prove $\lim \limits_{x \to 3}{(x^2+x-4)} = 8$ via the precise definition of a limit.,"Exercise Prove the statement using the $\epsilon$, $\delta$ definition of a limit:
  $$\lim \limits_{x \to 3}{(x^2+x-4)} = 8$$ The Precise Definition of a Limit In case you're not familiar with the definition of ""The Precise Definition of a Limit"", here it is. Let $f$ be a function defined on some open interval that contains the number $a$, except possible $a$ itself. Then we say that the limit of $f(x)$ as $x$ approaches $a$ is $L$, and we write
  $$\lim \limits_{x \to a}{f(x)} = L$$
  if for every number $\epsilon > 0$ there is a number $\delta > 0$ such that
  $$\text{if } 0 < |x - a| < \delta \text{ then } |f(x) - L| < \epsilon$$ Attempt $\lim \limits_{x \to 3}{(x^2+x-4)} = 8 \implies \text{if } 0 < |x - 3| < \delta \text{ then } |x^2+x-4 - 8| < \epsilon$ $|x^2+x-4 - 8| < \epsilon \implies |x^2+x-12| < \epsilon \implies |(x-3)(x+4)| < \epsilon$ From here, I quickly get lost. I notice that $|(x-3)(x+4)| < \epsilon \implies |x-3||x+4| < \epsilon \implies |x-3| < \frac{\epsilon}{|x+4|}$, which means that $\delta = \frac{\epsilon}{|x+4|}$. In the other exercises, I often end the proof here, because I've solved for $\delta$ purely in terms of $\epsilon$. However, here I have $x$ on the RHS.","['functions', 'calculus', 'limits']"
2026760,Does a converging expected value imply convergence almost surely?,"If ${X_n}$ is a sequence of random variables defined on the same probability space, does $\lim_{n\to\infty}\mathbb{E}(|X_n|)=0$ imply that $X_n$ converges to $0$ almost surely? I have a sense that this is true -- since the expected value of the absolute value of the sequence (which is all positive) converges to 0, the sequence itself converges to 0, intuitively -- but recalling the formal definition of almost sure convergence ($X_n\to X$ iff $\mathbb{P}\left(\{\omega\in\Omega|\lim_n X_n(\omega)=X(\omega)\})=1\right)$.","['probability-theory', 'convergence-divergence', 'random-variables']"
2026773,Dense subset of Compact Topological Group,"Let $G$ be a compact topological group and $a \in G$. Suppose $\{a^k\}_{k \in \mathbb{Z}}$ is dense in $G$. Show $G$ is Abelian. My thoughts for this were that for any $x,y \in G$, $x$ is close to some $a^m$ and $y$ is close to some $a^n$. Since $a^m$ and $a^n$ commute, this should mean $x$ and $y$ commute, but I've had trouble making this work. Note: $G$ might not need to be compact for this to work. EDIT: new idea: if for any $x,y \in G$ there are sequences $(a^{n_k})_k$ and $(a^{m_k})_k$ converging to $x,y$ respectively, then since multiplication is continuous, we would be done. EDIT: I think I solved it. The edit above provides a solution assuming the existence of such sequences. We don't know such sequences exist but we do know such a net exists. And we can apply continuity of multiplication for that net, and be done.","['general-topology', 'topological-groups', 'group-theory']"
2026783,"P(x) is an odd polynomial. When (x−3) is factored out, the remainder is 6.","$P(x)$ is an odd polynomial. When $(x−3)$ is factored out, the remainder is $6$ . What is the remainder when $P(x)$ is divided by $(x^2-9)$ ? Using the remainder theorem, when $x=3$ , $P(x)=6$ , and if the function is odd then when $x=-3$ , $P(x)=-6$ . I would very much appreciate if someone can guide me through the solving process, since that is what I care about the most. Thanks in advance.","['algebra-precalculus', 'polynomials']"
2026842,$\mathbb R^3$ minus a line is connected.,"Let $S\subseteq \mathbb R^3$ be homeomorphic to $\mathbb R$. Prove that $\mathbb R^3 \setminus S$ is connected. I haven't been able to solve this, although my topology skills are pretty weak. My friend told me he managed to prove this using results from his ""dimension topology"" class. Although I think this should be solvable using more mainstream stuff like homology or something. But I'm not sure. Regards.","['contest-math', 'general-topology']"
2026858,Existence of complete sufficient statistic,"I'm trying to solve a problem in problem list of qualifying exam. Here is my problem : Let $X_1, \ldots, X_n$ be a random sample from a p.d.f $f(x,\theta) = \theta f_1(x) 1_{(-\infty,0)} + (1-\theta) f_2(x) 1_{(0,\infty)}$
where $f_1 \geq 0 , f_2 \geq 0$ and $\int_{-\infty}^0 f_1=0$,  $\int_0^\infty f_2=0.$
Prove or disprove that there exists a complete sufficient statistic for $\theta$. I'm trying to show that there is only one sufficient statistic $T(X_1, \ldots, X_n) =(X_1, \ldots, X_n)$ for $\theta$ and that such $T$ is not complete. But I cannot even show the uniqueness of such sufficient statistic. Anyone can help me?",['statistics']
2026902,Show that any cyclic group with square free order is the Galois group over $\mathbb{Q}$ of some field extension,"Show that any cyclic group with square free order is the Galois group over $\mathbb{Q}$ of some field extension. I'm curious because I (believe) know the proof when $G$ is a finite cyclic group of any order, so I'm wondering if perhaps the square-free case is easier. This is off an old qualifying exam, and the hint is to consider $x^n-1$ for suitable $n$. In any case, an outline of the proof for a cyclic group is as follows:
$G \cong \mathbb{Z}_n$ for some $n$. Then, there exists a prime $p$ such that $n \mid p-1$. Writing $p-1=n\cdot m$, we know $\mathbb{Z}_{m}$ is a subgroup of $\mathbb{Z}_{p-1}$. Then one considers $\zeta$ a primitive $p$th root of unity. We know $Gal(\mathbb{Q}(\zeta)/\mathbb{Q}) \cong \mathbb{Z}_{p-1}$. As this is abelian, $\mathbb{Z}_m$ is a normal subgroup of this group, and so if $K$ is the fixed field of $\mathbb{Z}_m$, it has Galois group isomorphic to $Gal(\mathbb{Q}(\zeta)/\mathbb{Q}) / \mathbb{Z}_m \cong \mathbb{Z}_n \cong G$. Assuming the above is correct, how would square free order change this at all? Thanks!","['abstract-algebra', 'galois-theory', 'field-theory']"
2026904,Can the formula for the exterior derivative be extended to vector valued differential forms?,"If $\omega$ is a real $k$-form then the exterior derivative $d\omega$ can be expressed as $
\begin{align*}
d\omega(X_0,\dots,X_k)&=\sum_{i=0}^k (-1)^iX_i(\omega(X_0,\dots,\hat{X}_i,\dots,X_k)) \\
&\quad+\sum_{j=1}^k\sum_{i=0}^{j-1}(-1)^{i+j}\omega([X_i,X_j],X_0,\dots,\hat{X}_i,\dots,\hat{X}_j,\dots,X_k)
\end{align*}
$ where $X_0,\dots,X_k$ are vector fields and $\hat{X}_i$ indicates the omission of the argument $X_i$. Is there a way to extend this formula to the case when $\omega$ takes values in some finite dimensional real vector space $V$? The problem appears to be defining $X_i(\omega(X_0,\dots,\hat{X}_i,\dots,X_k))$ when $\omega$ is $V$-valued.","['differential-forms', 'differential-geometry']"
2026941,What could be said about the cardinality of $\bigcup_{i\in I} A_i$ if $I$ and all the $A_i$ have cardinality $2^{\aleph_0}$,"The countable union of a countable set is countable. Does the same hold for sets with cardinality $|\mathbb R|$. More specifically, if $A_i$ are sets of the same cardinality as the real numbers, and $I$ is an index set also with cardinality $|\mathbb R|$, is $|\bigcup_{i\in I} A_i| = |\mathbb R|$?","['cardinals', 'elementary-set-theory']"
2026971,"Using ZFC axioms, prove that the set $\{ \emptyset \}$ exists.","As the title describes, I want to prove that the set $\{ \emptyset \}$ exists, using ZFC axioms. I have an answer that I wish to check if I understood ZFC correctly. Is it that simple as: 1) The empty set axiom - There is a set having no elements.  we get $\{ \}$. 2) The Power Set axiom - For every set $A$, there is a set $B$ whose elements are the subsets of $A$. We get $\{ \emptyset \}$. Thanks.","['elementary-set-theory', 'proof-verification']"
2026977,Finding the largest possible value of $|f(2)|$ and |f'(1)|?,"Hi I was doing the following problem: Let $f$ be analytic in the right half plane $\{z\in\mathbb{C}:\text{Re}(z)>0\}$ with $|f(z)|<1$ for all $z$. If $f(1)=0$, (a) What is he largest value of $|f(2)|$? (b) What is the largest possible value of $|f'(1)|$? I got so far the following The transformation $T(z)=\frac{z-1}{z+1}$ maps $P=\{z\in\mathbb{C}:\text{Re}(z)>0\}$ to unit disk and hence $f\circ T^{-1}$ maps the unit disk to itself and $$f\circ T^{-1}(0)=f(T^{-1}(0))=f(1)=0$$ Therefore Schwarz lemma can be applied and by Schwarz lemma we have $|f\circ T^{-1}(z)|\le|z|$ Then $$|f(2)|=|f\circ T^{-1}\circ T(2)|=\Bigl|\:f\circ T^{-1}\Big(\frac{1}{3}\Big)\Bigr|\le\frac{1}{3}$$ Hence the largest possible value will be $\frac{1}{3}$ But I couldn't do part (b). Anyone can help me with that. Any help would be highly appreciated. Thanks in advance.",['complex-analysis']
2027006,Prove that the Sorgenfrey line is not connected,"Problem: Let $ \mathbb{R}_l $ denote the topological space whose underlying set is the real line $ \mathbb{R} $ and the topology is generated by the half closed intervals $ [a,b) $. Prove that the topological space $ \mathbb{R}_l $ is not connected. My proof: By definition, a connected space has no non-empty proper subset that are both open and closed. Any half-closed set $ [a,b) $ is by definition open in $ \mathbb{R}_l $, but every set $ [a,b) $ is also closed since its compliment 
$$\mathbb{R}_l\backslash[a,b)=(-\infty,a)\cup [b,\infty)=\bigcup_{\substack{n=1}}^\infty [-n,a)\cup [b,\infty)$$
is open. It follows that the topological space $ \mathbb{R}_l $ is not connected. Question: Is my proof correct?","['general-topology', 'sorgenfrey-line', 'connectedness']"
2027019,An example of a Banach space isomorphic but not isometric to a dual Banach space,"I am wondering the following question:
Let $X$ be a separable Banach space which is linearly isomorphic to a dual Banach space $Y^*$. Is there a Banach space $Z$ such that $X$ is lineraly isometric to the dual of $Z$: $X=Z^*$. I think that the answer is no, but I do not have a counterexample. Since $L_1$ is not isometric to any dual Banach space, maybe one can find a dual Banach space which is isomorphic to $L_1$... To finish, do that change anythink if I suppose $X$ to be almost linearly isometric to $Y^*$ ? By almost linearly isometric I mean that for every $\varepsilon >0$ there exist $T$: $X \to Y$ a linear isomorphism satisfying $\|T\|  \|T^{-1}\| \leq 1+\varepsilon$.","['banach-spaces', 'duality-theorems', 'isometry', 'vector-space-isomorphism', 'functional-analysis']"
2027088,Curvature of pullback connection,"Let $E\to M$ be a vector bundle over a smooth (compact) manifold $M$. Let $\nabla^E$ be a connection on $E$. Given a smooth map $f\colon N\to M$, where $N$ is another smooth (compact) manifold, we have the pullback connection $\nabla^{f^*E}$ on the pullback vector bundle $f^*E$. $\nabla^{f^*E}$ is uniquely determined by $\nabla^{f^*E}_X(f^*s)=(\nabla^E_{df(X)}s)\circ f$ where $X\in TN$ and $s$ is a section of $E$. Now, regarding the curvature, do we have $R^{f^*E}(X,Y)s=R^E(df(X),df(Y))s$ for $X,Y\in TN$, $s$ a section of $f^*E$? If yes, what is the best way to prove this identity? Can it be shown by just using the definition of curvature ($\nabla_X\nabla_Y-\nabla_Y\nabla_X-\nabla_{[X,Y]}$) and a computation in local coordinates? To me that seems messy, but maybe there is some way to simplify the calcuations that I don't see.","['riemannian-geometry', 'curvature', 'differential-geometry', 'vector-bundles', 'connections']"
2027111,Normal Derivative: Identity,"I don't really get this identity :
$$ \frac{\partial\Phi}{\partial n} = \vec n . \nabla \Phi $$
So gradient is:
$$ \nabla \Phi = \frac{\partial\Phi}{\partial x}  \vec i +\frac{\partial\Phi}{\partial y} \vec j +\frac{\partial\Phi}{\partial z}  \vec k  $$
Normal vector is: $$ \vec n = n_x \vec i + n_y \vec j+ n_z \vec k $$
RHS of identity: $$\vec n . \nabla \Phi = n_x \frac{\partial\Phi}{\partial x} +  n_y \frac{\partial\Phi}{\partial y} +  n_z \frac{\partial\Phi}{\partial z} $$ So must be: $$ \frac{\partial\Phi}{\partial n} =  n_x \frac{\partial\Phi}{\partial x} +  n_y \frac{\partial\Phi}{\partial y} +  n_z \frac{\partial\Phi}{\partial z} $$ Now is my question, what is $n_x $ , so the components of the normal vector Can this be true:
$$n_x = \frac{\partial x}{\partial n} $$","['derivatives', 'partial-derivative', 'vector-analysis']"
2027117,A limit without invoking L'Hopital: $\lim_{x \to 0} \frac{x \cos x - \sin x}{x^2}$,"The following limit $$\ell=\lim_{x \rightarrow 0} \frac{x \cos x - \sin x}{x^2}$$ is a nice candidate for L'Hopital's Rule. This was given at a school before L'Hopital's Rule was covered. I wonder how we can skip the rule and use basic limits such as: $$\lim_{x \rightarrow 0} \frac{\sin x}{x} \quad , \quad \lim_{x \rightarrow 0} \frac{\cos x -1}{x^2}$$","['real-analysis', 'trigonometry', 'calculus', 'limits-without-lhopital']"
2027136,Symmetric tensor decomposition in higher tensor powers,"Let $V$ be a vector space over a field $k$ of characteristic $0$ (not assumed to be algebraically closed, if it makes any difference). Consider the sequence of tensor powers 
$$V^{\otimes 2} = V\otimes V, \quad V^{\otimes 3} = V\otimes V\otimes V,\dots $$
It's well-known that there is a ""braiding"" representation of $S_n$ on $V^{\otimes n}$ given by permuting the tensor factors; i.e., $\sigma\in S_n$ acts linearly and on basis vectors via
$$\sigma(v_{i_1}\otimes\dots\otimes v_{i_n}) = v_{i_{\sigma(1)}}\otimes\dots\otimes v_{i_{\sigma(n)}}.$$
The symmetric tensors are defined as the subspace $\text{Sym}^n (V)\subseteq V^{\otimes n}$ on which $S_n$ acts trivially. Now for $n=2$ there is a very nice decomposition: namely $S_2=C_2$ is cyclic of order $2$, and the nontrivial automorphism $\epsilon$ of $V\otimes V$ has order $2$. Therefore its eigenvalues are $\pm 1$; $\text{Sym}^2 (V)$ is the $+1$ eigenspace and $\Lambda^2 (V)$ (the alternating square) is the $-1$ eigenspace. This decomposes $V\otimes V$ as a direct sum:
$$V\otimes V = \text{Sym}^2 (V)\oplus \Lambda^2 (V).$$ My questions are: Does a generalisation of this decomposition hold in the higher tensor powers? i.e. Can we write $V^{\otimes n} = \text{Sym}^n (V)\oplus W$ for some explicit direct sum $W$? As a specialisation, assume that $\Lambda^2 (V) \cong k$ is one-dimensional, so that $\dim V = 2$, say with basis vectors $v_1$ and $v_2$. In this case I have tried working out the decomposition of the higher tensor powers into symmetric tensors plus ""alternating"" parts. For example, $V^{\otimes 3} = \text{Sym}^3 (V)\oplus W$ for a $4$-dimensional space $W$ on which $S_3$ acts nontrivially. I calculated that
$$W = (V\otimes\Lambda^2 (V)) \oplus (\Lambda^2 (V)\otimes V)\cong V\oplus V.$$
I want to convince myself (and make precise) that in the decomposition $V^{\otimes n} = \text{Sym}^n (V)\oplus W$ for larger $n$, the direct summands appearing in $W$ are ""all things we've seen before"" (e.g. for $n=3$ there were just two copies of $V$, rather than some horrible ""higher-alternating-square"" thing), and so the only genuinely new things (representations of the symmetric group?) occurring in this sequence are the symmetric tensors $\text{Sym}^n (V)$. Can anyone explain how this holds in general?","['representation-theory', 'linear-algebra', 'symmetric-groups', 'tensor-products']"
2027156,Confusion on dependence of induced first fundamental form on the tangent space,"In section 2.5 of do Carmo, given an embedded regular surface $S\subset\mathbb R^3$, the author defines the first fundamental form $\mathrm I_p:T_pS\to \mathbb R$ as the quadratic form $\mathrm I_p(w)= \left\langle w,w \right\rangle _{\mathbb R^3}$. In a first course on differential geometry, the lecturer gave the same definition. I really feel I'm missing the the idea here, because it seems to me $\mathrm I_p$ ""does not depend on $p$"" in the sense the inner product stays the same. The lecturer said the whole point of the first fundamental form is to capture the local geometry of a regular surface at a given point in an intrinsic fashion, which definitely seems like a great thing, but the very definition seems to be independent of $p$. The only dependence is through the domain, but not the geometry. A fellow student told me the dependence is through $E,F,G$, but I don't understand: While $E,F,G$ are calculated via parametrizations of coordinate neighborhoods of $p$, they must be independent of parametrization... Their definition uses an inner product which is the same at every $p$... What am I missing here? What's the picture I should have in mind? I think I understand this answer , which explains in what says the fundamental form allows computations without further reference to an embedding in Euclidean space, but fear I am missing something else. (My fellow student spoke with confidence.)","['riemannian-geometry', 'differential-geometry']"
2027168,Growth order of entire functions,"This is an exercise 5 in chapter 5-Stein complex analysis. Let $\alpha\gt1$. Prove that $F_\alpha(z)=\int_{-\infty}^{\infty}e^{-|t|^\alpha}e^{2\pi izt}dt$ is of growth order $\alpha\over{\alpha-1}$. Actually, I found the following link. But, I can't understand some answer. how can i calculate growth order of this entire function? Is there any good elementary method without ""Laplace method"" to exact computation about order of growth $\frac{\alpha}{\alpha-1}$ ? 
(I already proved the inequality case.) I tried the substitution $t=|z|^{\frac{\alpha}{\alpha-1}} \mu$. 
Then,  $|z|^{\frac{\alpha}{\alpha-1}} $ could be extracted out of the integrals.
The integral is finite. Then, did I get what I want? I can't assure this.",['complex-analysis']
2027191,Prove that a regular Lindelöf topological space is normal,"That a closed subset of a Lindelöf space is Lindelöf, is already proven. I use this in my proof of the following: Prove that a regular Lindelöf topological space is normal. Here is my proof: Let $ X $ be a regular Lindelöf topological space and let two disjoint closed sets $ A $ and $ B $ be given. Since $ X $ is regular, for each $ x\in A $ there exists open disjoint sets $ U_x $ and $ V_x $ such that $ x\in U_x $ and $ B\subset V_x $. Let $ \mathcal{U} $ and $ \mathcal{V} $ be the set of all such sets $ U_x $ and $ V_x $, respectively, for every $ x\in A $. The set of sets defined by $ \mathcal{W}=\{U_x\cap A \} $ for all $ x\in A $ is an open covering of $ A $. Since $ A $ is a closed set in a Lindelöf space, $ A $ is Lindelöf as well by the previous problem and there exists a countable subcollection $ \mathcal{A} $ of $ \mathcal{W} $ that covers $ A $. Let $ U_0 $ be the intersection of all sets of $ \mathcal{A} $ and let $ V_0 $ be the intersection of all $ V_x $ corresponding to the sets of $ \mathcal{A} $. Since $ U_x\cap V_x=\emptyset $ for all $ x\in A $, it is clear that $ U_0\cap V_0=\emptyset $. Also, since a countable intersection of open sets is open, $ U_0 $ and $ V_0 $ are both open. Since $ V\subseteq V_x $ for each $ x\in A $, $ V\subseteq V_0 $ as well. In summary, $ U_0 $ and $ V_0 $ are disjoint open sets containing $ A $ and $ B $, respectively, and hence $ X $ is normal. Question: Is my proof correct? I think it has a couple of mistakes. Can I be sure that every set in $\mathcal{W}$ is open in $A$? And it is simply wrong that a countable intersection of open sets is open, right? How can I correct my proof, and is there a different and better proof?","['general-topology', 'separation-axioms', 'elementary-set-theory']"
2027239,Let $f: \mathbb N \rightarrow \mathbb N$ are increasing function such that $f\left(f(n)\right)=3n$. Find $f(2017)$ [duplicate],"This question already has an answer here : Finding the value of $f(2001)$ given the functional equation $f\bigl(f(n)\bigr)= 3n$ and strict monotonicity of $f:\mathbb N\to\mathbb N$ (1 answer) Closed 3 years ago . Let $f: \mathbb N \rightarrow \mathbb N$ are increasing function such that 
  $$f\left(f(n)\right)=3n$$
  for any positive integer $n$. Find $f(2017)$ My work so far: 1) If $m\not=n$ then $f(m)\not= f(n)$ 2) $f(3n)=3f(n)$","['functions', 'functional-equations']"
2027308,Find the set of numbers for which some sum over permutations is independent of the initial value,"Question I have a sequence of natural numbers $0 = k_0 < k_1 < k_2 < \dots < k_n$ and we want to find the set of numbers which satisfy some property, so we want to find (in function of $n$):
$$
\mathcal{A}_n := \{ (k_1,\dots,k_n) \in \mathbb{N}^{n} \mid (k_0,\dots,k_n) \mbox{ satisfy property }A\}
$$
The property $A$ we are looking at is the following: let $\sigma$ be an arbitrary permutation on $\{k_0,\dots,k_n\}$ and $k \in \{k_0,\dots,k_n\}$ be arbitrary, we define:
$$
V(\sigma,k) := \sum_{i=0}^{k_n-1} \delta_{\{\sigma^{k_n-i-1}(k) > i\}}
$$
where we define $\delta_{\{a > b\}}$ to be zero if $a \leq b$ and one if $a > b$. We say that $(k_0,\dots,k_n)$ satisfy property $A$ if:
$$
\exists \sigma: \forall k,l \in \{k_0,\dots,k_n\}: V(\sigma,k) = V(\sigma,l),
$$
i.e. there exists some permutation s.t. $V(\sigma,k)$ doesn't depend on the initial state $k$. Simplified version I would already be quite happy if it was possible to show that $\forall n: \mathcal{A}_n \neq \emptyset$. Lower dimensional example I have done the calculations for some values of $n$. $n=1$ We have:
$$
\mathcal{A}_n = \times 2\mathbb{N},
$$
indeed, for the trivial permutation this sum is dependant on $k$ as we have:
$$
V(\mbox{id},k) = \begin{cases}
k_1 & \mbox{ if } k = k_1\\
0 & \mbox{ if } k = k_0.
\end{cases}
$$
and the only other permutation $\sigma$ we have is defined by sending $0$ to $k_1$ and $k_1$ to $0$, the sum then just counts the number of times $\sigma^{k_1-i-1}$, for $k_1$ odd this the sum differs by one depending on we start at $k$ or not (as we start and end with the same value), whilst for even $k_1$ we start and end with the same value and thus the sum yields the same result in both cases (i.e. starting from $k_1$ or $0$). $n=2$ In this case the solution is still quite elegant as we have:
$$
\mathcal{A}_n = \{(k_1,k_2) \mid k_1,k_2 \mbox{ and } 0 \mbox{ are distinct modulo } 3.\}
$$
which can be seen analogously to the 2 dimensional case (except for using other permutations). $n=3$ Numerically I suspect that $\mathcal{A}_n$ can be characterized by the matrix:
$$
M_3
\begin{pmatrix}
1 & 2 & 1\\ 
1 & 3 & 0\\ 
1 & 0 & 3\\ 
1 & 1 & 2\\ 
2 & 3 & 3\\ 
2 & 0 & 2\\ 
2 & 2 & 0\\
3 & 0 & 1\\
3 & 1 & 0\\
3 & 2 & 3\\
0 & 1 & 3\\
\end{pmatrix}
$$
where $(k_1,k_2,k_3)$ is in $\mathcal{A}_n$ if the vector $(\mod(k_1,4),\mod(k_2,4),\mod(k_3,4))$ is a row in the matrix $M_3$.
Numerically I can compute these matrices for pretty large $n$ but I would like to have some general expression for it, but I feel like I lack the algebraic insights to do this. Origin Of The problem I have a supply process with $n+1$ different lead times (with one equal to $0$) and I cycle through all the different lead times. I wonder if we have for any number of different lead times that we can cycle through these lead times in such a way that the inventory remains the same over time.","['matrices', 'abstract-algebra', 'modular-arithmetic', 'permutations']"
2027323,Law of Iterated Expectation applied to a ratio?,"Consider the random variables $\epsilon_1, X_1, X:=(X_1,\dots,X_n)$ with $X_1,...,X_n$ i.i.d. Is it true that
$$\mathbb E\left(\frac{\epsilon_1}{X_1}\right)=\mathbb E\left(\frac{\mathbb E(\epsilon_1\mid X)}{X_1} \right)$$
Sketch of proof?","['expectation', 'conditional-expectation', 'probability', 'random-variables']"
2027324,Derivative of an ellipse,"I'm in my last year of high school and I'm currently studying on conics.
Usually to get the centre of an ellipse for example I use the canonical form to get the following form $((x+k)/a)^2 + ((y+k)/b)^2=1$ then consider the $x+k$ and the $y+k$ at the coordinates of the centre.
My Math teacher showed us a method to find this center without using the canonical form, just by using derivatives on the initial form which is: $ax^2 +by^2 + cx + dy + f = 0$. So his method consists of deriving this equation once with respect to $x$ and consider the $y$ a constant then have an equation which is $2ax + c = 0$ and the derive the same equation with respect to $y$ and considering $x$ a constant and having: $2by + d = 0$, and finally by solving these two equations he will have the center of the ellipse.
Can I have an explanation on why he used this method? like what do the derivatives have to do with the ellipse, where did this method come from. Sorry for any English mistakes and for the wordiness, Thank you in advance.","['derivatives', 'conic-sections']"
2027361,A non-existence example of a sufficient statistic,"In parametric statistical inference, a statistic $T$ of some variable $X$ (thought of as experimental or observational data) is sufficient for the parameter $\theta$ if is captures the essential information in the data $X$ about the parameter $\theta$ (there are several equivalent definitions for this). It is implicitly understood that the random variable $X$ has a probability distribution $f(x;\theta)$, where the parameter $\theta$ is of course unknown. Related is the concept of a minimal sufficient statistic , which captures nothing more than the essential. Is it true that any random variable $X$ as above has a sufficient statistic? Suppose $X$ has a sufficient statistic $T$. Must it also have a minimal sufficient statistic? I'd be glad to have either a concise proof or a simple counter-example for each of these two.","['statistics', 'statistical-inference']"
2027368,Is it required to use brackets inside an integral?,"Is it necessary for someone to write $$\int (x^2+2x) \,\mathrm{d}x$$ Instead of $$\int x^2+2x \,\mathrm{d}x$$ With the second one, it's quite obvious which terms we are taking the integral of. Is it still necessary to use brackets in this case?","['integration', 'notation']"
2027398,Series Converging Uniformly,"Given $$\sum\limits_{n=1}^\mathbb{∞}\frac1{ne^{nx}}$$ why does this series not converge uniformly for $x$ in $(0,∞)$ but converges uniformly for $x$ in $[𝛿,∞)$ for $𝛿 > 0$?","['real-analysis', 'calculus']"
2027405,Prove that $\pm1\pm2\pm\ldots\pm(4n+1)$ yields all odd numbers up to $(2n+1)(4n+1)$,"Problem : Prove that for different choices of signs $+$ and $-$ the expression $$\pm1\pm2\pm3\pm4\pm5...\pm(4n+1)$$ yields all odd positive integers less than or equal to $(2n+1)(4n+1).$ My Attempt : Let $n=1$, then we have the following expression to work with $$\pm1\pm2\pm3\pm4\pm5.$$
Let $+$ be represented by $0$ and $-$ be represented by $1.$ A binary string of length $5$ therefore represents the permutation of the operations. So $$+1+2+3+4+5\text{ is equivalent to }00000$$ 
$$+1-2+3+4+5\text{ is equivalent to }01000$$ and so on. With this representation we observe that:
$$15=00000$$
$$13=10000$$
$$11=01000$$
$$9=00100$$
$$7=00010$$
$$5=00001$$
$$3=10001$$
$$1=01001.$$ 
Notice how $1$ traverses in each number. Ignoring the trivial case when the odd number is equal to $(2n+1)(4n+1)$ (which in this case is $15$) we observe that there are $4*1+1$ numbers that can be written with only one $1$ since the numeral $1$ has $4*1+1$ places to move on. The remaining places $$\frac{1*(1+3)}{2}=2$$ require a two $1$s. This observation motivates the following Proof: Given any $n\geq 1$ we find $4n+1.$ Then we have $4n^2+3n$ odd numbers that are strictly less than $(2n+1)(4n+1).$ Let $a_i$ denote these odd numbers where $1\leq i\leq 4n^2+3n$ and $a_1<a_2<....<a_{4n^2+3n}.$ We begin by covering the last $4n+1$ odd numbers by writing 
$$a_{4n^2+3n}=\underbrace{1000000...0}_{4n+1}=8n^2+6n-1$$
$$..$$
$$a_{4n^2+3n-(4n)}=00000...1=8n^2+2n+1$$
$$a_{4n^2+3n-(4n+1)}=\underbrace{10000...1}_{4n+1}=8n^2+2n-1$$
$$..$$
and so on. Essentially $4n^2+3n=(4n+1)+(4n)+(4n-1)+(4n-2)+...+(4n-(n-1))+\underbrace{\frac{n(3+n)}{2}}_{\text{need }n+1\text{ 1s}}=4n^2+3n.$ Now I don't know how to compile these observations into a formal proof. What proof technique would be best suited in this scenario, maybe induction? Moreover, should I inclucde Lemmas regarding the zero and one arrangments?","['number-theory', 'proof-writing']"
2027429,"Is $C[0,1]$ not complete with the $L^2$ distance?","$$X=C[0,1]\qquad d(f,g)=\left(\int_{0}^{1}\vert f-g \vert^{2}dx\right)^{1/2}$$
The complete metric space is defined that every Cauchy sequence should be convergent on my book, so I think that the main goal is to either find at least one Cauchy sequence that is not convergent or to prove that all Cauchy sequences are convergent. I have completely no idea how to deal with the given distance here, could anybody give me some idea?","['real-analysis', 'cauchy-sequences']"
2027497,How to prove the intersection of functions is a function,"My question reads: 
Prove that, if f and g are functions, then f ∩ g is a function by showing that f∩g=g|A.","['elementary-set-theory', 'proof-writing', 'functions', 'proof-explanation']"
2027522,Prove that the number is the product of two successive positive integers,Prove that the number $\underbrace{11\cdots 1}_{100\text{ digits}}\underbrace{22\cdots 2}_{100\text{ digits}}$ is the product of two successive positive integers. What is the general method for this class of proof (big integers)?,['discrete-mathematics']
2027529,Fundamental group of $S^1\vee S^1$,"I want to show that $\pi_1(S^1\vee S^1) = \mathbb{Z}*\mathbb{Z}. $ I know that it follows from the Seifert-van Kampen theorem, but we haven't talked about that in class. We are given the following hint: Define a group homomorphism $\mathbb{Z}*\mathbb{Z}\to\pi_1(S^1\vee S^1)$ via the two inclusions $S^1\to S^1\vee S^1,$ and show that this is an isomorphism. We've also never talked about free products, so I am very confused as to how to define this homomorphism.","['algebraic-topology', 'general-topology']"
2027556,Understanding definition of tensor product,"The definition I have of a tensor product of vector finite dimensional vector spaces $V,W$ over a field $F$ is as follows: Let $v_1, ..., v_m$ be a basis for $V$ and let $w_1,...,w_n$ be a basis for $W$. We define $V \otimes W$ to be the set of formal linear combinations of the mn symbols $v_i \otimes w_j$. That is, a typical element of $V \otimes W$ is $$\sum c_{ij}(v_i \otimes w_j).$$ The space $V \otimes W$ is clearly a finite dimensional vector space of dimension mn. We define bilinear map $$B: V \times W \to V \otimes W$$ here is the formula $$B(\sum a_iv_i, \sum b_jw_j) = \sum_{i,j}a_ib_j(v_i \otimes w_j). $$ Why does $V \otimes W$ have to be a formal linear combinations of symbols $v_{i} \otimes w_j$, what would be wrong in defining $V \otimes W$ simply as a linear combination of symbols $v_i \otimes w_j$? Thanks.","['functional-analysis', 'tensor-products', 'definition', 'vector-spaces']"
2027656,How to figure out an equation from a plot?,I am trying to figure out the equations $y=f(x)$ of the red dashed lines in this log-linear graph where the $x$ values were plotted on a logarithmic scale while the $y$ values were kept linear. I tried to figure it out from the table below which revealed a geometric serie starting from the third $X$ value (i.e $x=25$). Any ideas or suggestions will be greatly appreciated. Many thanks.,"['algebra-precalculus', 'calculus']"
2027810,"How can $\frac{x^3-4x^2+4x}{x^2-4}$ be both $0$ and ""undefined"" when $x = 2$?",Suppose I have a function defined as $$F(x)= \frac{x^3-4x^2+4x}{x^2-4}$$ Now I want to find the value of $F(2)$. I can do it in 2 ways: Put $x=2$ and solve the function. It will give: $$F(2)=\frac{0}{0}$$ which is not defined. Solve $F(x)$ first and then put $x=2$. $$F(x)= \frac{x(x-2)^2}{(x-2)(x+2)}=\frac{x(x-2)}{x+2}$$ It will give $${F(2)=\frac{0}{4}}$$ which is zero. How can zero equal not defined?,"['algebra-precalculus', 'functions']"
2027861,In Hilbert space: $x_n → x$ if and only if $x_n \to x$ weakly and $\Vert x_n \Vert → \Vert x \Vert$.,"Assume that $H$ is a $\mathbb K$-Hilbert space, $(x_n)_{n \ge 1}$ a sequence in $H$ and $x ∈ H$.
  Show that $x_n → x$ if and only if $x_n \to x$ weakly and $\Vert x_n \Vert → \Vert x \Vert$. I'm trying to prove this statement. 
The $\Rightarrow$ is basically clear, since a  strongly convergent sequence is also weakly convergent. But how can I show the other direction $\Leftarrow$ ?","['hilbert-spaces', 'functional-analysis', 'weak-convergence', 'convergence-divergence', 'sequences-and-series']"
2027966,Find the limit in which constraints regard (ir)rationality... again.,"I recently posted this question , and received an adequate answer. However, I now have a similar question, which requires some verification. Exercise If
  $$f(x) = \begin{cases}
0 \space\space\space\space \text{if $x$ is rational}\\ 
1 \space\space\space\space \text{if $x$ is irrational}
\end{cases}
$$
  prove that $\lim \limits_{x \to 0}{f(x)}$ does not exist. Attempt It seems like my proof that I used in my previous question is valid (which wasn't true for that one) for this one: For any tiny rational number $\delta$ near $0$, there is a smaller irrational number, and vice versa. Therefore, as $\delta$ continues to ""shrink"", $f(0 + \delta)$ will oscillate between $0$ and $1$. Request Is the proof above a valid solution?","['real-analysis', 'limits', 'functions', 'proof-verification', 'calculus']"
2028001,The probability of an ant rarely visiting the points with both coordinates even,"A point of the lattice $\mathbb{Z}^3$ in $\mathbb{R}^3$ is painted white if at least one of its coordinates is odd. An ant is moving in $\mathbb{R}^3$. At each integer time $t$ the ant is
at a point in $\mathbb{Z}^3$ and it chooses one of points in $\mathbb{Z}^3$ at distance $1$ with uniform
probability, and it moves there before time $t + 1$. For an integer $n$, denote by $P_n$
the probability that among the previous $n$ integer times the ant was at least $90\%$
of the time at a white point. Prove that $P_n$ decreases exponentially with $n$. Can
you compute the rate?","['stochastic-processes', 'markov-chains', 'random-walk', 'probability']"
2028034,A Perron-like formula,"From basic Dirichlet series ,if $f(s)=\sum_{n=1}^{\infty}\frac{a_n}{n^s},s=\sigma+it , \sigma>\sigma_0 $  where $\sigma_0$ is the abscissa of convergence, we know that : $$ \sum_{n<x}\frac{a_n}{n^s}=\frac{1}{2\pi i}\int_{c-i\infty}^{c+i \infty}f(s+z) \frac{x^z}{z}dz,  c>0 ,c>\sigma-\sigma_0$$. I tried using some other integrand to create a similar formula and i came to this: $$\frac{1}{\pi}\sum_{n=1}^{\infty}\frac{a_n}{n^s}\log\left(1+\frac{1}{n^2}\right)=$$ $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}-\frac{f(s+2z)}{z\sin(\pi z)} dz$$ , for $\Re{z}>0  $ . In order to get the result , one has to compute the integral on the right side. I used as a contour the semicircle from the right side of the vertical line $(c-iT,c+iT)$ which contains the poles $1,2,3,...,n,...$ of the integrand function. I think my result is right , but can somebody confirm it?","['complex-analysis', 'dirichlet-series']"
2028048,Solve for $x$ in $\log_4{(x+4)} \le \log_2{(2x+5)}$,I would like to solve  $$\log_4{(x+4)} \le \log_2{(2x+5)}$$ for $x$. I did: $$\log_4{(x+4)} \le \log_2{(2x+5)} \Leftrightarrow \frac{\log(x+4)}{\log(4)} \le \frac{\log(2x+5)}{\log(2)}\\ \Leftrightarrow \frac{\log(x+4)}{2\log(2)} \le \frac{\log(2x+5)}{\log(2)} \Leftrightarrow \frac{1}{2} \cdot \frac{\log(x+4)}{\log(2)} \le \frac{\log(2x+5)}{\log(2)} \Leftrightarrow\ ???$$ What do I do next?,"['algebra-precalculus', 'inequality', 'calculus', 'logarithms']"
2028068,PDF of product of components of uniform vector,"Given $(X, Y) \sim \mathbb{U}([0, 1]^2)$ a uniformly distributed random vector, I have to calculate the PDF of $XY$. I've seen about product distribution, but the deal here is that $X$ and $Y$ aren't independent. I tried using a test function (ie computing $\mathbb{E}(\phi(XY))$ for any $\phi$ positive measurable) but I found myself stuck having to compute the integral of $\frac{1}{x}$ over $[0, 1]$, which is ... not right. The only thing I know is that the PDF of $(X, Y)$ is $f_{X,Y}(x, y) = \mathbb{1}_{[0, 1]^2}(x, y)$, so I'm really stuck. EDIT : as it turns out, $X$ and $Y$ are indeed independent. Although that doesn't really help, since I still somehow have to integrate $\frac{1}{x}$ over $[0, 1]$ ...","['probability-theory', 'measure-theory']"
2028076,"Given the following linear transformation, find the matrix associated to $\varphi$ through a given base.","Text of the exercise : Consider the linear transformation $\varphi:\mathbb{R}^3 \rightarrow \mathbb{R}^3$ defined by: $\varphi(1,0,0)  = (0,1,0)\\ \varphi(0,1,1) = (1,1,0) \\\varphi(0,0,1) =(1,0,0)$ Is $B={(0,1,1),(0,1,0),(1,0,0)}$ a basis of $\mathbb{R}^3$? If the answer is yes, find the matrix $M_{\varphi}^B$ associated to $\varphi$ through $B$. Reasoning : $B$ is a basis of $\mathbb{R}^3$ since it is a set of three linearly independent vectors (proof is given by the fact that the matrix constituted by their coordinates is not singular) in a three-dimensional space. I know from the definition of $\varphi$ the result of the transformation of two of the vectors of $B$; precisely $\varphi(1,0,0)=(0,1,0)$ and $\varphi(0,1,1)=(1,1,0)$. In order to infer the last one, I could use the definind property of a linear mapping, i.e. that a function $f:V\rightarrow W$ is linear if $\forall \textbf{v}_{1},\textbf{v}_{2}\in V$, $\forall\lambda_{1},\lambda_{2}\in \mathbb{K}$ then $f(\lambda_{1}\textbf{v}_{1}+\lambda_{2}\textbf{v}_{2}=\lambda_{1}f(\textbf{v}_{1})+\lambda_{1}f(\textbf{v}_{1})$, so that:
\begin{equation}
\varphi(0,1,0)=\varphi(0,1,1)-\varphi(0,0,1)=(1,1,0)-(1,0,0)=(0,1,0)
\end{equation}
The asked matrix is thus:
\begin{equation}
M_{\varphi}^B=\begin{bmatrix}
1&0&0\\1&1&1\\0&0&0
\end{bmatrix}
\end{equation} I'd greatly appreciate a feedback. Thank you all!","['matrices', 'linear-algebra', 'linear-transformations']"
2028094,Is there anything interesting about this matrix update?,"I have a symmetric, real matrix $M$ (not necessarily full rank) and I am performing the following rank one update $$M - \frac{M\textbf{1}\textbf{1}^TM}{\textbf{1}^TM\textbf{1}}$$ where $\textbf{1}$ is the vector of all ones. Is there anything interesting I can say about this matrix (properties, theorems that use a similar update, perturbation effects) or is it just a boring old rank one update?","['matrices', 'numerical-linear-algebra', 'linear-algebra']"
2028103,Proving there is no solution: $-\sin(n\pi \cot^{-1} x)=\sin(n\pi \tan^{-1} x)$,"Let $n$ be a integer. Prove that this equation has no solutions:
  $$-\sin(n\pi \cot^{-1} x)=\sin(n\pi \tan^{-1} x)$$ I tried this:
$$-n\pi \cot^{-1} x+k\pi=n\pi \tan^{-1} x, k\in\mathbb{Z}\Leftrightarrow$$
$$\frac{k}{n}= \tan^{-1}x +\cot^{-1}x$$
But $\tan^{-1}x +\cot^{-1}x=\dfrac{\pi}{2}$, and $\pi$ is irrational, so this is a contradiction, because $k$, $n$ are integers. How I can prove this without using that $\pi$ is irrational?",['trigonometry']
2028137,Finding an $x \in \mathbb R$ so that $\|I - x A\|_2$ becomes minimal,"Let $A \in \mathbb R^{n \times n}$, and let $I$ denote the ($n \times n$-)identity matrix. Then I want to find a $x \in \mathbb{R}$, so that: $$\|I - x A\|_2$$ becomes minimal (where $\|\cdot\|_2$ denotes the Euclidean matrix norm). Let's assume $A ≠ 0$ to exclude the trivial case where any $x$ can be chosen. I think it's obvious that such a minimum exists, because $\|I - x A\|$ can be considered a real (continuous) function with respect to $x$ which is of course always positive, and $\lim_{\|x\| \to \infty} \|I - x A \|_2 = \infty$. So there must be (at least one) minimum, and it must somehow depend on $A$. I'm not really sure on how to find it, though. I've tried to write out $\|I - x A\|_2 = \sqrt{\sum_{i=0}^n \sum_{j=0}^n |\delta_{ij}- x a_{ij}|^2}$ and thought about differentiating that thing, but from some of the squares we could pull out $x$ and from others (where $\delta_{ij} = 1$) we can't, so I'm afraid that $\frac d{dx}$ of that thing would quickly become very nasty, let alone setting it $= 0$ and solving it for $x$, so I frankly suspect there's an easier way. Could we somehow bring the eigenvalues, the spectral radius or something like that into the equation, or use one of the properties of the matrix norm?","['matrices', 'normed-spaces', 'linear-algebra']"
2028165,How to understand the proof of the uniform boundedness principle?,"The uniform boundedness principle is a quite important result in functional analysis. It can be stated as: Let $X$ be a Banach space and $Y$ be a normed vector space. Suppose $F$ is a collection of continuous linear operators from $X$ to $Y$ . If for all $x\in X$ one has $\sup_{T\in F}\|T(x)\|_Y<\infty$ then $\sup_{T\in F}\|T\|<\infty$ . This result can be proved as follows, using Baire category theorem, according to Wikipedia: Suppose that for every $x$ in the Banach space $X$ , one has: $$\sup_{T\in F} \|T(x)\|_Y<\infty.$$ For every integer $n\in \mathbb{N}$ , let $$X_n=\{x\in X:\sup_{T\in F}\|T(x)\|_Y\leq n\}.$$ The set $X_n$ is a closed set and by the assumption, $$\bigcup_{n\in \mathbb{N}}X_n=X\neq \emptyset.$$ By the Baire category theorem for the non-empty complete metric space $X$ , there exists $m$ such that $X_m$ has non empty interior, i.e., there exists $x_0\in X_m$ and $\epsilon > 0$ such that $$\overline{B_{\epsilon}(x_0)}=\{x\in X : \|x-x_0\|\leq \epsilon\}\subset X_m.$$ Let $u\in X$ with $\|u\|\leq 1$ and $T\in F$ . One has that: $$\|T(u)\|_Y=\epsilon^{-1}\|T(x_0+\epsilon u)-T(x_0)\|_Y$$ $$\leq \epsilon^{-1}(\|T(x_0+\epsilon u)\|_Y+\|T(x_0)\|_Y)$$ $$\leq \epsilon^{-1}(m+m), \quad \text{since $x_0+\epsilon u$, $x_0\in X_m$}$$ Taking the supremum over $u$ in the unit ball of $X$ it follows that $$\sup_{T\in F}\|T\|\leq 2\epsilon^{-1}m < \infty.$$ Now I want to understand this proof but it is being a little bit hard to do it. There are some points to it: First, I can't get any intuition about that proof. How can I get some intuition on why to follow the steps from this proof? Second, we defined a countable collection of closed sets $X_n$ whose union equals $X$ . Baire Category theorem says that if we have a countable collection of open dense sets has dense intersection. How does Baire Category theorem applies to the collection $X_n$ to show that there is $m$ such that $X_m$ has non-empty interior? Why would finding this $X_m$ be able to furnish the proof? I mean, in general I can't understand what is going on here with this proof. I want to understand it correctly, and mainly, how Baire Category theorem is being used. How can one understand this proof?","['real-analysis', 'functional-analysis', 'proof-explanation', 'general-topology', 'metric-spaces']"
2028169,Resultant contains all common roots as linear factors?,"Let $f,g \in \mathbb{C}[x,y,z]$ be homogeneous polynomials, so they define projective plane curves $C$ and $D$ in $\mathbb{C}P^2$. We are interested in Bezout's theorem applied to $C \cap D$. Write $f$ and $g$ as polynomials in $z$:
$$
f(x,y,z) = \sum_{i = 0}^ma_i(x,y)z^{m-i}, \quad g(x,y,z) = \sum_{j = 0}^nb_j(x,y)z^{n-j},
$$
where $a_i, b_j \in \mathbb{C}[x,y]$ are homogeneous of degree equal to their index. The resultant of $R(f,g)$ is a homogeneous polynomial in $x,y$, so we may factor it into linear factors. $R(f,g)$ vanishes at some $x_0,y_0$ if and only if $f(x_0,y_0,z), g(x_0,y_0,z) \in \mathbb{C}[z]$ have a common root. My question is: if $f(x_0,y_0,z)$ and $g(x_0,y_0,z)$ have many distinct common roots $z_1,\dotsc,z_k$, then we have different points $P_i = (x_0:y_0:z_i) \in C \cap D$. But how do we define the intersection multiplicity of each $P_i$? Section 4.2 of this book defines it to be the multiplicity of the factor $(x_0y - y_0x)$ in $R(f,g)$, but that would be the same for all $P_i$ which is very strange.","['resultant', 'projective-space', 'algebraic-geometry', 'definition']"
2028212,ellipse uniform perimeter travel?,I'm trying to come up with a way that as I progress linearly from $0$ to $360$ degrees or $0$ to $2\pi$ radians I have the the corresponding position (P) on the perimeter of an ellipse to also travel on it at a linear speed movement. Thanks,"['conic-sections', 'trigonometry', 'geometry']"
2028225,Integration with a little bit of Leibnitz rule,"The problem is simple. But the solution is where I get my doubts. Also I would appreciate if anyone could fill me in with all the necessary points where I need to check for convergation for the procedure to correct. $$\int_0^\infty \frac{\cos(bx)-\cos(cx)}x e^{-ax} \,dx $$ for all $a,b,c \in \rm I\!R  ; a>0$ From here it's obvious that we can rewrite this as
$$\frac d{dt} \left.\int_0^\infty \frac{cos(tx)}x e^{-ax}\right|_c^b \,dx =\int_0^\infty \int_c^b {\sin(tx)} e^{-ax} \,dx \,dt= \int_c^b I\,dt$$ Then via double per partes 
$$I=\int_0^\infty {\sin(tx)} e^{-ax}\,dx = \left.\frac{-1}{a}\sin(tx)e^{-ax}\right|_{t=0}^\infty + \frac ta \int_0^\infty {cos(tx)} e^{-ax}dx =$$
$$=\left. \frac{-\sin(tx)e^{-ax}}a \right|_{t=0}^\infty - \left. \frac t a {\cos(tx)e^{-ax}}\right|_{t=0}^\infty - \frac{t^2}{a^2}\int_0^\infty {\sin(tx)} e^{-ax}\,dx$$
$$I=\frac ta-\frac{t^2}{a^2}I$$
$$I=\frac{\frac ta}{1+\frac{t^2}{a^2}}$$
$$\int_c^b\frac{\frac ta}{1+\frac{t^2}{a^2}}=\int_{c/a}^{b/a}\frac{u}{1+u^2}=\frac12\ln(\frac dc)$$ I find it odd that my final result is not dependant at all of parameter $a$ Just does not sound right. And I would appreciate filling out where covnergation or any other possible breaking points need to be checked. EDIT: A typo...probably more to come.","['derivatives', 'definite-integrals']"
2028229,A variation of Borel Cantelli Lemma,If $P(A_n) \rightarrow 0$ and $\sum_{n=1}^{\infty}{P(A_n^c\cap A_{n+1}})<\infty$ then $P(A_n \text{ i.o.})=0$. How to prove this? Thanks.,"['probability-theory', 'limsup-and-liminf', 'borel-cantelli-lemmas']"
2028249,Why do sequences need to converge with respect to a norm?,"Suppose there is a sequence $x_n$ in $\mathbb{R}^n$, which converges to $x$, with respect to $\lVert\cdot\rVert_2$, say. Then we can write down vectors $x_1, x_2, x_3, ...$, and so on, and, with each $x_i$, we will be closer and closer to $x$. Now, suppose there is another sequence, $y_n$, which converges to the same point $x$, but now with respect to $\lVert\cdot\rVert_\infty$. Then we can write this sequence down as numbers $y_1, y_2, ...$, and with each $y_i$ we will be closer and closer to $x$. My point is that why do we need sequences to converge with respect to certain norms if we can just write them down as numbers, one by one, without any norms, and the numbers (or vectors) will just be closer and closer to the point of convergence? I understand that convergence with respect to a norm $\lVert\cdot\rVert$ means that $\lim\limits_{n\to\infty}\lVert x_n-x \rVert=0$. But if we can write down approaching vectors one by one, without any norms, and the vectors will be approaching a certain point, then shouldn't this mean that convergence must be satisfied with respect to any norm, and the norm should not be relevant to convergence at all?","['normed-spaces', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
2028288,Why don't quaternions contradict the Fundamental Theorem of Algebra? [duplicate],"This question already has answers here : Why are the solutions of polynomial equations so unconstrained over the quaternions? (6 answers) Closed 7 years ago . I don't pretend to know anything much about the Fundamental Theorem of Algebra (FTA), but I do know what it states: for any polynomial with degree $n$, there are exactly $n$ solutions (roots). Well, when it comes to quaternions, apparently $i^2=j^2=k^2=-1$, but $i\ne j\ne k\ne i$. So now, we have apparently found three solutions to the second -degree polynomial $x^2=-1$. I'm not aware of the justification of the FTA, nor I am I aware of Hamilton's justification for quaternions. However, I know a contradiction when I see one. What am I missing here?","['abstract-algebra', 'quaternions', 'polynomials']"
2028293,Splitting $r^k (r+n)!$ as a sum of factorials,"I wanted to split the expression $r^k (r+n)!$ as a sum of factorials, where $k ,n \ \in \ \mathbb{Z} \ ;\ k>0$ . For example, $r(r+n)! = (r+n+1)! - (n+1)(r+n)!$ $r^2(r+n)! = (r+n+2)! - (2n+3)(r+n+1)! + (n+1)^2 (r+n)!$ And in general, If $$ r^k (r+n)! = \sum_{m=0}^{k} \lambda_{m} (r+n+m)! $$ where $ \lambda_{m} $ is independent of $r$ , find a closed form expression for $\lambda_{m}$ . My Try : 1) I tried to form a two variable recurrence for the expression and solve it such that we can express it in a summation form. Let $r^k (r+n)! = A(k,n)$ Since $ r^k(r+n)! = r^{k-1}(\overline{r+n+1} - \overline{n+1})(r+n)! = r^{k-1}(r+n+1)! - r^{k-1} (n+1)(r+n)! $ $ \implies A(k,n) = A(k-1 , n+1) - (n+1)A(k-1,n) $ we can fix the initial conditions as $A(k,0) = r^k r!$ and $A(0,n) = (r+n)!$ . I tried solving the above using generating functions, but eventually we'll have to find a Mac Lauren Series expansion for $\dfrac{1}{\Gamma \left( n+1 + \frac{2}{x} \right)}$ 2) The expression can be rewritten as $$ r^k = \sum_{m=0}^{k} \lambda_{m} (r+n+m)_{m}  $$ where $(x)_{y}$ denotes the falling factorial . This resembles the identity $$ r^k = \sum_{m=0}^{k} {k\brace m} (r)_{m} $$ where $ \displaystyle {a\brace b} $ denotes the Stirling Numbers of the Second Kind . So maybe the the coefficients can be expressed in terms of Stirling Numbers. 3) I also tried plugging in values of $r$ and making a system of $k+1$ equations, but it became tedious to solve. Update (26th November, 2016) : The user @Marko Riedel has discovered a remarkable closed form expression, namely $$ \lambda_m = (-1)^{k+m}
\sum_{p=0}^{k-m} {k\choose p} {k+1-p\brace m+1} n^p $$ I have put a bounty since I feel that there might be alternate solutions to prove (or maybe even simplify) the above proposition. I'm also looking for any references/links to this problem. Also, the great answers so far reveal that the identity is valid even when $n$ is not an integer. Any help will be greatly appreciated.","['recurrence-relations', 'stirling-numbers', 'generating-functions', 'combinatorics', 'sequences-and-series']"
2028308,How to determine the average number of dice showing the most common value?,"Suppose the situation that tossing $n$ dice simultaneously. Each die has $k$ faces. How can I determine the average number of dice showing the most common value among them? For example, $n=10$ and $k=3$, the result of tossing is 1-3-2-1-1-1-2-1-2-3, respectively. The most common value is ""1"" with 5 dice. Trying again, the result is 3-2-3-2-1-3-3-3-3-3. The most common value is ""3"" with 7 dice. Try in this way 1,000 times and average the number of dice showing the most common value. That is $\frac{{5 + 7 +  \cdots }}{{1,000}}$. Do there already exist any function (with parameter $n$, $k$) to determine the average? Note that the number of dice showing ""i"" is not independent of the number of dice showing ""j"" with $i\ne j$ that is because the total number of dice is fixed. I cannot solve this problem because the independent assumption cannot be used. Thank you.","['combinatorics', 'means', 'probability', 'average']"
2028324,"If $x>0$, then find the greatest value of the expression $ \frac{x^{100}}{1+x+x^2+x^3+\cdots+x^{200}}$","If $x>0$, then find the greatest value of the expression $ \dfrac{x^{100}}{1+x+x^2+x^3+\cdots+x^{200}}$ This expression simplifies to $ \frac{(x^{100})(x-1)}{x^{200}-1}$ using sum of n terms of GP. Now one can find the maxima by equating the derivative to zero. But is there any other way not involving calculus to get the maximum value?","['geometric-progressions', 'sequences-and-series']"
2028363,"If $a+b+c=3$, find the greatest value of $a^2b^3c^2$.","If $a+b+c=3$, and $a,b,c>0$ find the greatest value of $a^2b^3c^2$. I have no idea as to how I can solve this question. I only require a small hint to start this question. It would be great if someone could help me with this.","['geometric-progressions', 'arithmetic-progressions', 'sequences-and-series']"
2028383,Closed form solution to nonstandard Kolmogorov Forward Equation,"I am interested in obtaining a closed form solution for the stationary distribution $q(x,y)$ to the following Kolmogorov Forward Equation $$ \frac{\partial}{\partial t} q(x,y,t) = - \frac{\partial}{\partial x} q(x,y,t) a(x,y) $$ $$+ \lambda_1 \Big( \delta(y)\int_{-\infty}^{\infty}q(x,z,t)dz  - q(x,y,t) \Big) + \lambda_2 \Big( \frac{\partial}{\partial y}\big(F(y) \int_0^y q(x,z,t)dz \big)  - q(x,y,t) \Big)  $$ Where $q(x,y)$ is a joint density and $F(x)$ is a CDF with positive support. $\delta(x)$ is the Dirac Delta function. I was able to solve for the marginal density of $y$ in closed form (its CDF is $\frac{\lambda_1}{\lambda_1+\lambda_2(1-F(y))}$ over $y>0$) but that is all. What tools would you recommend? I am not too familiar with calculations involving Laplace transform in this context. Thank you","['partial-differential-equations', 'stochastic-processes', 'real-analysis', 'probability-theory']"
2028384,What is motivation to study Harmonic functions?,Recall that a function $f:U (\subset \mathbb R^n) \to \mathbb R$ is called harmonic if $\Delta f=0$ where $\Delta$ is Laplacian operator. What is a good motivation to study harmonic functions?,"['complex-analysis', 'real-analysis', 'harmonic-functions', 'functions']"
2028404,What is the difference between Rudin's *Principles of Mathematical Analysis* and *Real and Complex Analysis* books?,What's the difference between these two Rudin's books: Principles of Mathematical Analysis Real and Complex Analysis ? I want to reread by myself undergraduate analysis (single and multivariable analysis) to remember my undergraduate courses. Is one better to self-study than another? Isn't Principle is just a synthesis of the Real and Complex analysis Book? Are the exercises hard in both books?,"['real-analysis', 'reference-request', 'complex-analysis', 'book-recommendation', 'analysis']"
2028417,How many values of $2^{2^{2^{.^{.^{.^{2}}}}}}$ depending on parenthesis?,Suppose we have a power tower consisting of $2$ occurring $n$ times: $$\huge2^{2^{2^{.^{.^{.^{2}}}}}}$$ How many values can we generate by placing any number of parenthesis? It is fairly simple for the first few values of $n$: There is $1$ value for $n=1$: $2=2$ There is $1$ value for $n=2$: $4=2^{2}$ There is $1$ value for $n=3$: $16=({2^{2})^{2}}=2^{(2^{2})}$ There are $2$ values for $n=4$: $256=(({2^{2})^{2}})^2=(2^{(2^{2})})^2=(2^{2})^{(2^{2})}$ $65536=2^{(({2^{2})^{2}})}=2^{(2^{(2^{2})})}$ Any idea how to formulate a general solution? I'm thinking that it might be feasible using a recurrence relation. Thanks,"['combinatorics', 'recurrence-relations', 'power-towers', 'elementary-number-theory']"
2028418,Gamma Distribution Sum,"I'm trying to show that the sum of three independent gamma distributions $X_1\sim \Gamma(2,5)$, $X_2\sim \Gamma(3,5)$, and $X_3\sim\Gamma(1.5,2.5)$ are a gamma distribution. $Y=X_1+X_2+2X_3$. I've multiplied the moment generating functions of $X_1$ and $X_2$ and I've squared the moment generating function of $X_3$ and am left with gamma distributions that I cannot multiply together to give me another Gamma distribution. How would I proceed? $\frac{1}{2.5^3\left(\frac{1}{2.5}-t\right)^3}\left(\frac{1}{5^5\left(\frac{1}{5}-t\right)^5}\right)$","['gamma-distribution', 'statistics']"
2028428,"For i.i.d. standard normal variables and $\gamma > 1$, $|X_n| \le (2 \gamma \log{n})^{1/2}$ eventually, almost surely","I am a beginner in probability theory (also a beginner in analysis) and am currently having trouble with regard to the following problem. I would like to show: Let $X_n$ be a sequence of standard normal random variables. Show that 
  for any $\gamma > 1$, $|X_n| \le (2 \gamma \log{n})^{1/2}$ for all but finitely many $n$ almost surely. My attempt is the following: Consider $\gamma > 1$ and $\{ x: |X_n| \le (2 \gamma \log{n} )^{1/2} \text{ for all but finitely many $n$} \}$.
Then, $\{ x: |X_n| \le (2 \gamma \log{n} )^{1/2} \text{ for all but finitely many $n$} \} 
= \bigcup_{N=1}^\infty \bigcap_{n=N}^\infty \{ x: |X_n| \le (2 \gamma \log{n} )^{1/2} \}$.
We want to show $P(\bigcup_{N=1}^\infty \bigcap_{n=N}^\infty \{ x: |X_n| \le (2 \gamma \log{n} )^{1/2} \}) = 1$;
or to show
$P(\bigcap_{N=1}^\infty \bigcup_{n=N}^\infty \{ x: |X_n| > (2 \gamma \log{n} )^{1/2} \}) = 0$; I show this in the edit. By way of contradiction, suppose $P(\bigcap_{N=1}^\infty \bigcup_{n=N}^\infty \{ x: |X_n| > (2 \gamma \log{n} )^{1/2} \}) > 0$.
Then, since the infinitely often events are tail events, $P(\bigcap_{N=1}^\infty \bigcup_{n=N}^\infty \{ x: |X_n| > (2 \gamma \log{n} )^{1/2} \}) = 1$ by the Kolomolgorov Zero-One Law EDIT: By Borel-Cantelli Lemma, it suffices to show $\sum_{n \ge 1} P(|X_n| > (2 \gamma \log{n})^{1/2}) < \infty$.
Since $X_n$ has standard normal distribution,
\begin{align*}
\sum_{n \ge 1} P(|X_n| > (2 \gamma \log{n})^{1/2}) 
&\le \sum_{n \ge 1} 2 \dfrac{exp( -2\gamma \log{n/2} ) } { \sqrt{2\gamma} \sqrt{2 \pi \log{n}}} \\
&= \sum_{n\ge1} \dfrac{2}{\sqrt{2\gamma} n^\gamma \sqrt{2\pi \log{n}}} \\ 
&< \infty
\end{align*}
if $\sqrt{2 \gamma} > \sqrt{2}$.
Since $\gamma > 1$ by hypothesis, $\sum_{n \ge 1} P(|X_n| > (2 \gamma \log{n})^{1/2}) < \infty$
and by Borel-Cantelli Lemma, $P(\bigcap_{N=1}^\infty \bigcup_{M=N}^\infty \{ |X_n| > (2 \gamma \log{n} )^{1/2} \}) = 0$. From here, I am stuck.
Is there another approach? Any insight?
Thank you in advance.","['probability-limit-theorems', 'probability-theory', 'normal-distribution']"
2028460,Reading the centralizer off of the character table,"Assume that I am given the table of irreducible characters of a finite group $G$. I realize that we can see the order of the centralizer of any element $g \in G$ by summing the squares in the corresponding column: $$|C(g)| = \sum_{\chi} |\chi(g)|^2.$$ I need to know the centralizer more exactly: what conjugacy classes occur in the centralizer and how often. For example, from the second column in the character table of S4 we can see that the centralizer of $(1 2)$ has order $1^2 + 1^2 + 1^2 + 1^2.$ It is $$\{(), \, (1 2), \, (3 4), \, (1 2)(3 4)\}$$ so I would like to read off the tuple $(1,2,1,0,0)$ from this table somehow. If that is impossible then it would still be useful to know that there are three conjugacy classes occuring in this centralizer. Can that information be read off the character table?","['finite-groups', 'group-theory', 'characters']"
2028461,Multiple integration,"We know that the double integration is used to find the area of a planer region and volume of a solid object in space.
My question is why we use triple integration to find again the volume a body?
(I know the importance of triple integration, any boddy plz)","['multivariable-calculus', 'multiple-integral', 'calculus']"
2028512,Nice identity of telescoping expectation in martingale,"Given $\left\{A_n\right\}$ be a zero mean and $F_n$- martingale with $E(A_n^2)<\infty$. Prove that $$E(A_{n+r}-A_n)^2 = \sum_{k=1}^{r} E(A_{n+k}-A_{n+k-1})^2$$ My attempt: We rewrite the LHS as: 
\begin{align}E(A_{n+r} - A_n)^2= &E(A_{n+r} - A_{n+r-1})^2 + E(A_{n+r} - A_{n+r-1})^2 \\[0.2cm]&+ 2E(A_{n+r} - A_{n+r-1})(A_{n+r-1}-A_n) + E(A_{n+r-1}-A_n)^2\end{align}
Now, we will show that $E(A_{n+r} - A_{n+r-1})(A_{n+r-1}-A_n) = 0$ as follows:
\begin{align}E(A_{n+r}A_{n+r-1})& = EE(A_{n+r}A_{n+r-1}\mid F_{n+r-1}) = E(A_{n+r-1}E(A_{n+r}\mid F_{n+r-1})) \\[0.2cm]&= E(A_{n+r-1}A_{n+r-1})\end{align} \begin{align}E(A_{n+r-1}A_r) - E(A_{n+r}A_r) &= EE(A_{n+r-1}-A_{n+r})A_r\mid F_{n+r-1})\\[0.2cm]& = E((A_{n+r-1}-A_{n+r})E(A_r\mid F_{n+r-1})) = 0\end{align} since $E(A_r\mid F_{n+r-1}) =$ some constants, and $E((A_{n+r-1}-A_{n+r}) = 0-0=0$. Now repeat the whole process above for $E(A_{n+r-1}-A_n)^2$ and the following terms, we achieve what we want. My question: Could anyone please help verify if my solution above is correct? I am quite skeptical about the part of proving $$E(A_{n+r-1}A_r) - E(A_{n+r}A_r) = 0$$ So if someone could help, I would really appreciate.","['probability-theory', 'probability', 'martingales']"
2028524,(f of g) versus (f as a function of g),"I've run into a problem dealing with notation. Say I have two functions: $f(x)=x+2$ $g(x)=\frac x3$ From basic function composition, we know that $f$ of $g$ is: $f(g(x))=\frac x3 +2$ However, to find $f$ as a function of $g$ , we find that $x=3g(x)$ and substitute: $f(g(x))=3g(x)+2$ Thus we see that two different ideas share the same notation. Am I making a mistake in the notation of these ideas? If not, then is there some way to differentiate the two? I ran across this problem when dealing with parametric equations and indicating ""y(t) as a function of x(t)"" as y(x(t)), which could also mean ""y of x(t)"". Any help would be greatly appreciated.","['function-and-relation-composition', 'notation', 'functions']"
2028555,"Nondecreasing, singular function fails a weaker condition than absolutely continuous","Suppose $f:[0,1]\to\mathbb{R}$ is a nondecreasing function, and $f'(x)=0$ almost everywhere on $[0,1]$. For any $\epsilon>0$, prove that there exists finitely many pairwise disjoint intervals $[a_k,b_k]$, $k=1,\dots, n$ in $[0,1]$ with $$\sum_{k=1}^n(b_k-a_k)<\epsilon$$ and $$\sum_{k=1}^n(f(b_k)-f(a_k))>f(1)-f(0)-\epsilon.$$ My attempt: I think proving by contradiction may be the way to go here. Suppose to the contrary there exists $\epsilon>0$ such that for any finitely many pairwise disjoint $[a_k,b_k]\subseteq[0,1]$ with $\sum_{k=1}^n(b_k-a_k)<\epsilon$, we have $$0\leq\sum_{k=1}^n(f(b_k)-f(a_k))\leq f(1)-f(0)-\epsilon.$$ This looks like a weaker condition than absolutely continuous. However, I am not sure how to proceed here, other than noting that $f(1)-f(0)>0$, so that $f$ is not constant. Also, I can see that $f$ cannot be absolutely continuous, otherwise it will have a contradiction, since an absolutely continuous and singular function must be constant. Thanks for any help! Update: I have another idea, let $E$ be the set where $f'\neq 0$. Then $|E|=0$. My idea is to cover $E$ in the Vitali sense by intervals $[a_k,b_k]$ and use Vitali Covering Theorem to prove that $\sum(b_k-a_k)<\epsilon$, but $[a_k, b_k]$ covers nearly all of $E$, where all the changes occurs, so that $\sum(f(b_k)-f(a_k))>f(1)-f(0)-\epsilon$. At the moment, I have no idea how to make the above idea rigorous though.","['real-analysis', 'lebesgue-measure', 'lebesgue-integral', 'measure-theory', 'analysis']"
2028558,"The geometry behind the lemniscatic integral: $u' = \int^t_0(4t(1-t^2))^{-1/2}\,dt$","From a translated version of this (page 111): One can easily recognise that in this case, the lemniscatic integral $$u' = \int^t_0\frac{dt}{\sqrt{4t(1-t^2)}}$$ represents the interior of each of the two half-planes in which the plane is divided by the real axis conformally on the interior of a square with sides $\int^1_0\frac{dt}{\sqrt{4t(1-t^2)}}$. I guess that it's saying that the integral conformally maps half-planes to a square with sides $\int^1_0\frac{dt}{\sqrt{4t(1-t^2)}}$, but I can't see the reason why.","['indefinite-integrals', 'integration', 'definite-integrals', 'geometry']"
2028572,"What is exactly the relation between vectors, matrices, and tensors?","I am trying to understand what Tensors are (I have no physics or pure math background, and am starting with machine learning). In an introduction to Tensors it is said that tensors are a generalization of scalars, vectors and matrices: Scalars are 0-order tensors, vectors are 1-order tensors, and matrices are 2-order tensors. n-order tensors are simply an n-dimensional array of numbers. However, this does not say anything about the algebraic or geometric properties of tensors, and it seems from reading around the internet that tensors are algebraically defined, rather than defined as an array of numbers. Similarly, vectors are defined as elements of a set that satisfy the vector space axioms. I understand this definition of a vector space. And I understand that matrices are representations of linear transformations of vectors. Question: I am trying to understand what a Tensor is more intuitively, and what the algebraic and intuitive/geometric relation is between tensors on the one hand, and vectors/matrices on the other. (taking into account that matrices are representations of linear transformations of vectors)","['matrices', 'tensors', 'vectors']"
2028637,Is this a ruled surface?,"I'd like to know if the contour surfaces of the following $f$ are ruled: $f(x, y, z)$ is a 3-dim scalar field (real-valued function) defined in the 1st octant $x > 0,\, y > 0,\,z > 0$ that exhibits the following scaling property: $$
f(x, y, z) = f\left(~ \frac{z_0}z \, x, \frac{z_0}z \, y,\, z_0 ~\right) \qquad \text{or} \qquad f(x, y, z) = f(\, sx,\, sy,\, s z) \tag*{$s \equiv \frac{z_0}z$}
$$
That is, for any point, the value of $f$ can also be found on a reference plane $z = z_0$ by scaling radially $(x, y) \mapsto (sx, sy)$. Of course, any plane $z = z_0 > 0$ can act as the reference plane, and each contour surface $f(x, y, z) = p$ is in fact ""swept out"" by rays originating from the origin and passing through $(x, y, z)$. This reminds me of ruled surface for the following reason: given a contour surface $f(x, y, z) = p$, there is a level curve on the reference plane $g(x, y)=f(x, y, z_0)=p$ which can be parametrized into $g\left(\, x(u),\, y(u) \, \right) \to \beta(u)$ as in
$$ f = \alpha(u) + v \beta(u) \qquad \text{with $v = z$}$$
where $(u, v)$ are the parameters, with $\alpha(u) = \{0, 0, 0\}$ being a trivial base curve (directrix) and $\beta(u) = \{ x(u), y(u), z_0 \}$ the director. There is also the alternative parametrization of interpolation/extrapolation of two non-intersecting curves on the contour surface: just take two planar level curves at two different heigh $z = z_1$ and $z = z_2$. However, I'm not sure if the base curve is allowed to be degenerate. In addition, I have trouble constructing a parametrization with rulings orthogonal to the rays. Formally,my question are: Does the scaling property above implies it to be a ruled surface? If it is ruled, is it necessary that there exists a parametrization with two orthogonal rulings? I have very little knowledge about even intro level differential geometry, and I basically would like to know if my field $~f~$ belongs to this type of function (ruled surface) so that I should go ahead and learn the relevant tools and study it that way. p.s. I don't know if this is relevant, but
the field $f$ and its 1st derivatives are all continuous, but there are some boundaries of regions where the 2nd derivatives are not continuous with infinite jumps. The boundaries are like $z = y$ and $z = \sqrt{x^2 + y^2}$.","['3d', 'parametrization', 'differential-geometry', 'surfaces']"
2028646,Variance of aggregated distributions of binomial random variables,"Let two random variables: $$x_1 \sim Bin(100, 0.5) \\ x_2 \sim Bin(100, 0.6)$$ Now, we define a third random variable, $x_{12}$ which it's distribution is the aggregated distributions of $x_1$ and $x_2$, so it's not quite like $x_1 + x_2$ even though empirically the variance seems like the sum of the two variances. Is that the case? How can I show it? Thanks.","['statistics', 'probability']"
2028648,For each $f \in P_{n-1}$ there exists $g \in P_n$ such that $f(x)=g(x+1)-g(x)$,"Assume that $P_n$ & $P_{n-1}$ are defined as the vector spaces of the polynomials of degree $n$ & $n-1$ over field $\mathbb R$, respectively. Prove that for each $f \in P_{n-1}$ there exists a polynomial $g \in P_n$ such that $f(x)=g(x+1)-g(x)$. My try : I defined a linear-map $T: P_{n} \to P_{n-1}$ such that $T(g)=g(x+1)-g(x)$. I need to prove that $T$ is surjective. That's what i don't know how to show ... Thanks in advance.","['linear-algebra', 'vector-spaces']"
2028698,Why the null space of pseudo inverse equals the null space of the matrix transpose?,"The pseudoinverse $A^+$ of A is the matrix for which $x = A^+Ax$ for all x in the row space of A. The nullspace of $A^+$ is the nullspace of $A^T$. I don't understand this cause the above seems to imply that $A^+=A^T$ which doesn't make sense as $x = A^+Ax$ while $A^TA$ gives a matrix which is not an identity matrix. Here is the source -Page 2 on ""Finding the pseudo Inverse""","['matrices', 'pseudoinverse', 'linear-algebra']"
2028702,Convergence of an independent but not identically distributed sequence of discrete random variables,"Suppose $\{X_k\}_{k=1}^\infty$ is a sequence of independent random variables  such that
  $$
P\left(X_k=1-\frac1k\right)=1-\frac1k\qquad P\left(X_k=2-\frac1k\right)=\frac1k.
$$
  Define for each positive integer $n$, $Y_n=\prod_{k=1}^n X_k$. Using the martingale convergence theorem, one can show that $Y_n\to Y$ a.s. for some $Y$. What is the distribution of $Y$? Several approaches have been tried, but I don't know how to go on due to the difficulties in computation: Let $Z_n=\log Y_n$ and calculate for every $t$ the limit of
$$
\prod_{k=1}^n\Bigg[\left(1-\frac1k\right)e^{it\log(1-1/k)}+\frac1ke^{it\log(2-1/k)}\Bigg].
$$ Check the Lindeberg or Lyapunov condition in order to use the CLT:
$$
E(Y_n)=1,\qquad EY^2_n=\prod_{k=1}^n\left(1+\frac1k-\frac{1}{k^2}\right),\quad\cdots
$$",['probability-theory']
2028751,Prove that an open subset of a Baire space is a Baire space,"Problem: Prove that if $X$ is a Baire space and $U\subset X$ is an open set, then the subspace $U$ is a Baire space. My proof: Let $ \{A_n \} $ be any countable collection of closed sets of $ U $, each of which has empty interior. Since $ U $ is open, all the sets $ A_n $ is closed in $ X $ as well. Since $ X $ is a Baire space, $ \bigcup A_n $ is has empty interior in $ X $. Since $ A_n\subset U $ for each $ n $, $ \bigcup A_n\subset U $ as well, and hence $ A_n $ has empty interior in $ U $. It follows that $ U $ is a Baire space. Question: Is my proof correct? I am claiming that a set closed in an open set is closed in $X$ as well. I that true?","['general-topology', 'baire-category']"
2028759,Calculation of Connection on regular submanifold,"I am studying Riemannian Geometry from the book by M.P. do Carmo and I am trying to get a complete picture of Connections by working out some examples and in particular I to calculate for the class of sub-manifolds given as a regular level set of a regular value, I'm trying to calculate it for that case. As a corollary, I wish to see the case of sphere, or some other simple manifold. If $f:\Bbb{R}^n \to \Bbb{R}^k$  where $ (k<n)$ and $p\in\Bbb{R}^k$ be a regular value, then $M = f^{-1}(p)$ will be a regular submanifold of $\Bbb{R}^n$ of dimension $n-k$. Now how to give a connection on $M$? Any hint will be appreciated. It will be really helpful if one can give a reference to a book, online material or online lecture notes in which similar calculations are done or where sufficient hints are provided.","['riemannian-geometry', 'differential-geometry']"
2028849,"How many positive integers less than 1,000,000 have the sum of their digits equal to 19?","How many positive integers less than $1,000,000$ have the sum of their digits equal to $19$ ? I tried to answer it by using stars and bars combinatorics method. The question says that sum of $6$ digit numbers { considering the number as $abcdef$ } is 19. Hence, I can write it as $a + b + c + d + e + f = 19$ ,but this assumes that $a,b,c,d,e,f >= 0$ . But, here each digit lies between 0 and 9. So, how stars and bars method will be modified according to this domain ?","['permutations', 'combinatorics', 'combinations']"
2028877,There is no diffeomorphism between one quadrant in the plane and the half plane,"I am trying to prove rigourosly that the unit square $[0,1]\times [0,1]\subset \mathbb R^2$ is not a differentiable manifold with boundary (I've searched for a rigouros proof, but haven't found anyone) Corners are obviously the problem, so it is reduced to check that otherwise, there would be some open subset around, for example, $(0,0)$, diffeomorphic to $H = \{(x,y)\in\mathbb R^2 : y \geq 0\}$. How can I prove easily that $Q = \{(x,y)\in\mathbb R^2 : x\geq 0, y\geq 0 \}$ is not diffeomorphic to $H$? Any hint? Thank you in advance.","['multivariable-calculus', 'smooth-manifolds', 'manifolds-with-boundary', 'differential-topology']"
2028881,Prove that $\log_2 3$ is irrational!,"So, I have been told that for every $x\in\mathbb{N}$, $\ln{x}$ is irrational by using the fact that $e$ is transcendental number. But, how to prove that $\log_{2}{3}$ is irrational number? My idea is to rewrite the form $$\log_2 3 = \frac{\ln 3}{\ln 2}$$ But, if both $x, y$ irrational, it isn't necessary that $\frac x y$ irrational. Please, help me!","['number-theory', 'irrational-numbers']"
2028914,Show that for stopping times two conditional expectations are equal.,"Let $(\Omega, \mathcal S, P)$ be a probability space with a filtration $\mathfrak F = \{\mathcal F(t)\}_{t \in \mathcal T}$. Let $\tau, \sigma : \Omega \to \mathcal T \cup \{\infty\}$ be stopping times, $X$ a random variable with $E[|X|] < \infty$. Show that
$$
 1_{\{\sigma = \tau\}} E[X \mid \mathfrak F(\sigma)] = 1_{\{\sigma = \tau\}} E[X \mid \mathfrak F(\tau)] \quad a.s.
$$
where $1_{\{\sigma = \tau\}}$ denotes the indicator function, the expectation are condititional expectations, and $\mathfrak F(\sigma)$ denotes the $\sigma$-algebra of the stopping time past, given by
$$
 \mathfrak F(\sigma) := \{ A \in \mathcal S : S \cap \{\sigma \le t \} \le \mathfrak F(t) \mbox{ for all } t \in \mathcal T \}
$$
see for example Definition 20 here . How to show this? I tried to use the definition of conditional expectation, with this we have
$$
 E[1_F X] = E[1_F E[X\mid \mathfrak F(\sigma)]]
$$
for each $F \in \mathfrak F(\sigma)$, and similar
$$
 E[1_F X] = E[1_F E[X\mid \mathfrak F(\tau)]]
$$
for each $F \in \mathfrak F(\tau)$. Also as $\{ \sigma = \tau \} \in \mathfrak F(\sigma) \cap \mathfrak F(\tau)$ we have
$$
 1_{\{\sigma = \tau\}} E[X \mid \mathfrak F(\sigma)] = E[ 1_{\{\sigma = \tau\}}X \mid \mathfrak F(\sigma) ].
$$
But I am unsure how to combine these facts? One way to show equality a.s. is to show that one conditional expectation fulfills the definitional equation of the other, but I just see that this could be applied for sets $F \in \mathfrak F(\sigma) \cap F(\tau)$, but not for the whole of one of those $\sigma$-algebras.","['stochastic-processes', 'conditional-expectation', 'measure-theory', 'stopping-times']"
2028918,Find possible number of triangles with integer sides for a given inradius,"inradius = $5$
possible triangle sides
$(25, 20, 15)$
$(37, 35, 12)
(39, 28, 17)$
... Find formula to find other possible sides of triangles.","['puzzle', 'geometry']"
2028920,Find $\lim \limits_{n \to \infty}{1*4*7*\dots(3n+1) \over 2*5*8* \dots (3n+2)}$,"I am trying to find $$\lim \limits_{n \to \infty}{1*4*7*\dots(3n+1) \over 2*5*8* \dots (3n+2)}$$ My first guess is to look at the reciprocal and isolate factors: $${2 \over 1}{5 \over 4}{8 \over 7} \dots {3n+2 \over 3n+1}= {\left(1+1\right)}\left(1+{1 \over 4}\right)\left(1+{1 \over 7}\right) \dots \left(1+{1 \over 3n+1}\right)$$
Now we take the natural log:
$$\ln{ + \ln\left(1+1\right)} + \ln\left(1+{1 \over 4}\right) +\ln\left(1+{1 \over 7}\right) \dots +\ln\left(1+{1 \over 3n+1}\right)$$
Using $\ln(1+x) \le x$, we get:
$$\ln{ + \ln\left(1+1\right)} + \ln\left(1+{1 \over 4}\right) +\ln\left(1+{1 \over 7}\right) \dots +\ln\left(1+{1 \over 3n+1}\right) \le 1 + {1 \over 4} + {1 \over 7} + \dots + {1 \over 3n+1}$$
Now, I'm stuck. I suppose I might use the fact that the RHS is similar to the harmonic series and show that it converges to some log, but I'm not sure how to do that. Do you have any clues?","['real-analysis', 'infinite-product', 'sequences-and-series', 'limits']"
2028990,Orthogonal matrix and orthonormal columns,"Show that the columns of orthogonal matrix are always orthonormal.
Hint: $A^TA=I$ Can't really get even started, I thought that it has to be orthonormal since the result is $I$ and not only a diagonal matrix with other numbers in them?","['matrices', 'linear-algebra']"
2029003,A nice infinite series: ${\sum\limits_{n=1}^\infty}\frac{1}{n!(n^4+n^2+1)}=\frac{e}{2}-1$ - looking for a more general method,"while clearing out some stuff I found an old proof I wrote for an recreational style problem a while back, here's the sum: ${\sum\limits_{n=1}^\infty}\frac{1}{n!(n^4+n^2+1)}=\frac{e}{2}-1$ I'll show how I got to the limit in a moment, but because my argument was a bit long winded / ad-hoc (first thing that came to mind at the time), just wondering if this kind of inverse polynomial/factorial sum can be attacked using other techniques, is it part of a well known / classical family of these kind of sums / any useful references? Thanks in advance! Proof: \begin{align}
	\displaystyle{\sum\limits_{n=1}^\infty}\frac{1}{n!(n^4+n^2+1)} &= \displaystyle{\sum\limits_{n=1}^\infty}\frac{1}{n!{(n^2+1)^2-n^2}}=
	\displaystyle{\sum\limits_{n=1}^\infty}\frac{1}{n!(n^2+1-n)(n^2+1+n)}\nonumber \\
	&= \displaystyle{\sum\limits_{n=1}^\infty \frac{n}{n \cdot n!(n^2+1-n)(n^2+1+n)} }\nonumber\\
	&= \displaystyle{\frac{1}{2}\sum\limits_{n=1}^\infty\frac{1}{n \cdot n!} \left(\frac{1}{n^2+1-n}-\frac{1}{n^2+1+n}\right)}\end{align}
Now this almost telescopes, the factorial term being the issue, but if we slightly change the groupings of the summands (essentially starting the summation having moved one index along) we get something that we can further simplify by taking the telescoping part out of the parentheses and putting the factorial part in: $
	= \displaystyle{\frac{1}{2}\left[\frac{1}{1\cdot1!}\cdot\frac{1}{1}+\sum\limits_{n=2}^{\infty}\frac{1}{n^2+1-n}\left\{\frac{1}{n \cdot n!}-\frac{1}{(n-1) (n-1)!}\right\}\right]}$ We have taken out the 1/quadratic that was common to successive telescoping summands in the original summation, and taken the difference of the factorials instead since as we will see this simplifies nicely: $	\begin{align}&= \displaystyle{\frac{1}{2}\left[1+\sum\limits_{n=2}^{\infty}\frac{1}{n^2+1-n}\left\{\frac{n-1-n^2}{n\cdot(n-1)\cdot n!}\right\}\right]}\nonumber \\
	&= \displaystyle{\frac{1}{2}\left[1+\sum\limits_{n=2}^{\infty}\frac{1}{n\cdot(n-1)\cdot n!}\right]}\end{align}$ Using the same partial fraction trick we used in (1) to pseudo-telescope and regroup terms to get to (2) helps again: \begin{align} &= \displaystyle{\frac{1}{2}\left[1+\sum\limits_{n=2}^{\infty}\frac{1}{n!}\left(\frac{1}{n-1}-\frac{1}{n}\right)\right]} =\displaystyle{\frac{1}{2}\left[1-\left\{\frac{1}{2!}+\sum\limits_{n=3}^{\infty}\frac{1}{(n-1)}\left(\frac{1}{n!}-\frac{1}{(n-1)!}\right)\right\}\right]}\nonumber \\	&=\displaystyle{\frac{1}{2}\left[1-\left\{\frac{1}{2}-\sum\limits_{n=3}^{\infty}\frac{1}{n!}\right\}\right]}
=\displaystyle{\frac{1}{2}\left[\frac{1}{2}+\left\{\sum\limits_{n=0}^{\infty}\frac{1}{n!}\right\}-2 \frac{1}{2}\right]}\\
	&=\displaystyle{\frac{1}{2}\left\{\sum\limits_{n=0}^{\infty}\frac{1}{n!}\right\}-1=\frac{e}{2}-1}\,\,\,\square
\end{align}",['sequences-and-series']
2029006,Relating evaluations of Functors and polynomials,"I came across the following two constructions recently, which both have in common of relating: on the one hand the evaluation of something resembling a function, and on the other hand a more convoluted construction. My question is: are the two following constructions related? Let $\mathcal C$ be a Category, and $G \in \hat{\mathcal{C}}$ be a presheaf over $\mathcal C$ (that is, a functor from $\mathcal C^{op}$ to $Set$). Let $x$ be in object in $\mathcal C$. Denoting by $Y(x)$ the functor $\mathcal C(\_,x) : \mathcal C^{op} \to Set$, the Yoneda Lemma tells us that the set $\hat{\mathcal C} (Y(x),G)$ is the same as the set $G(x)$. Let $A$ be a ring, and $P \in A[X]$ be a polynomial over $A$. Let $x$ be an element of $A$. Then the image of $P$ in the quotient $A[X]/(X-x)$ is the same as $P(x)$.","['sheaf-theory', 'polynomials', 'algebraic-geometry', 'commutative-algebra', 'category-theory']"
2029009,What is the relation between fractional ideals and divisors on curves?,"I've done courses in Algebraic Number Theory and Algebraic Geometry, where I learned the theory of Dedekind domains (in ANT) and Divisors (in AG). Now, the wikipedia article on divisors says the following: ""The name ""divisor"" goes back to the work of Dedekind and Weber, who showed the relevance of Dedekind domains to the study of algebraic curves. The group of divisors on a curve (the free abelian group on its set of points) is closely related to the group of fractional ideals for a Dedekind domain."" So my question is: What is this relation between fractional ideals and the group of divisors on a curve? I would also appreciate some references that explain this relation in detail. Thank you in advance!","['algebraic-curves', 'algebraic-number-theory', 'algebraic-geometry']"
2029020,"$G$ is finite abelian group, do we have $\{ng : g\in G \} = G$ for $(n, |G|) = 1 $ (coprime)?","$G$ is finite abelian group, do we have $\{ng : g\in G \} = G$ for $(n, |G|) = 1 $ (coprime)? I think it is true, we know $G$ is the direct sum of finitely many cyclic groups $$\mathbb Z/p_1\mathbb Z\oplus \cdots \oplus\mathbb Z/p_m\mathbb Z.$$ 
We will have $m$ generators in the from $(1,0,\cdots,0), (0,1,\cdots,0),$ ... And $(n,0,\cdots,0), (0,n,\cdots,0)...$ will still be the generators since $n$ and each $p_i$ are coprime. Is this correct? And is there a more direct argument?","['abelian-groups', 'abstract-algebra', 'group-theory']"
2029046,Uniform continuity of difference function,"Assume that both $f$ and $f'$ are uniformly continuous real functions. Let $F:\mathbb R^2\to\mathbb R$ be defined by $$F(x_1,x_2)=\begin{cases} 
      \frac{f(x_2)-f(x_1)}{x_2-x_1} & \text{if }x_1\neq x_2 \\
      f'(x_1) & \text{if }x_1=x_2.
\end{cases}$$ Is there a simple proof (i.e. one that proceeds directly from the definitions of uniform continuity and limits) that $F$ is also uniformly continuous? Update: This has been proved by user251257 below, but I am still very interested in a simper proof that does not use the mean value theorem, if that is possible.","['derivatives', 'real-analysis', 'uniform-continuity']"
2029058,Inverting a function given by an integral,"I'm not sure if it's even possible to do this in general, but I'd like to find a function $f^{-1}$ which is the inverse of $$f(t) = \int^t_0\frac{ds}{\sqrt{4s(1-s^2)}}$$","['indefinite-integrals', 'inverse-function', 'functions', 'inverse']"
2029072,Explaining intuitively the notation for derivatives [duplicate],This question already has answers here : Why is the 2nd derivative written as $\frac{\mathrm d^2y}{\mathrm dx^2}$? (6 answers) Closed 5 months ago . I am not clear on the notation we use for the derivative. So let's assume that $y=f(x)=6x-x^2$ so the first derivative is $$\frac{Δy}{\Delta x}=\frac{dy}{dx}=6-2x=F(x)$$ The notation for the second derivative is: $$\frac{dF(x)}{dx}=\frac{d^2y}{dx^2}=-2$$ My question is about the notation of the second derivative. Does the superscript 2 indicate to the power of 2 or just an order? Because I can see that: $$\frac{dF(x)}{dx}=\frac{d}{dx}\frac{dy}{dx}$$ But is that a multiplication really? Because dy is just y2-y1 and what is the plain d then? Update based on comments Why is it not $\frac{d^2y}{d^2x^2}$ if the lower part is multiplication? I can see that the upper part is the differentiation applied twice since it is dF(x)=ddy and seems to indicate just a second (order) differentiation but I am not sure what the denominator indicates. Why is it $\frac{d^2y}{dx^2}$ and not $\frac{d^2y}{d^2x^2}$ ? What does the $x^2$ really mean?,"['derivatives', 'intuition', 'math-history', 'notation', 'calculus']"
