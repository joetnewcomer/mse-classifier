question_id,title,body,tags
3133737,Solution to an ODE 3,"I want to find the solutions to $$x(t)+\frac{1}{10}\int_0^1 e^{t-s} x(s) ds=1.$$ Differential equations are not my field, so I'm not sure where to start.",['ordinary-differential-equations']
3133756,When does a sequence's snake-product converge?,"Following this earlier unanswered question, Does this infinite primes snake-product converge? ( MO version ),
I would like to pose a more general question.
Given any increasing sequence $s_i$ of natural numbers,
form the infinite snake-product by multiplying fractions $\frac{s_i}{s_{i+1}} \cdot \frac{s_{i+3}}{s_{i+2}}$ .
For example, if the sequence $s$ is just the natural numbers, $1,2,3, \ldots$ , the snake-product $\sigma( s )$ is: $$
\sigma = \frac{1}{2}\cdot\frac{4}{3}\cdot\frac{5}{6}\cdot\frac{8}{7}\cdot\frac{9}{10}\cdot\frac{12}{11} \cdot \cdots 
 \approx 0.59907 \;.
$$ The approximate product shown is what I computed up to $i=10^6$ .
A specific question: Q1 . Does the above product (for the sequence $s_i = i$ ) converge, and if so, to what number? Answered by @Peter: Yes, converges to $$
\frac{ \sqrt{2} \, \pi^{3/2} } { \Gamma( \frac{1}{4} )^2 } 
\approx 0.59907 \;.
$$ I would be interested to see other example sequences that appear to, or can be proved to, converge. One might distinguish between strictly increasing sequences and non-decreasing sequences. And the more general title question: Q2 . Under what conditions on the sequence $s_i$ will
  the snake-product converge?","['number-theory', 'sequences-and-series']"
3133762,Show that $\alpha \int_0^{\pi}X^2dx = \int_0^{\pi}(X')^2dx$ holds for $\alpha > 0$,"Consider the equation in the form $X + \alpha X = 0$ , with Dirichlet
boundary conditions at $x = 0$ and $x = Ï€$ . a) Multiply the equation by X and integrate from 0 to Ï€, then
integrate the first term, $XX$ , by parts to obtain the identity $$\alpha \int_0^{\pi}X^2dx=-[X'X]\biggr|_0^{\pi} + \int_0^{\pi} (X')^2dx$$ b) Apply the Boundary Conditions to conclude that either $\alpha >0$ or ( $\alpha = 0$ and X is constant) I have obtained part a) and when applying the boundary conditions $X(0)=0=X(\pi )$ I get the following equation $$\alpha \int_0^{\pi}X^2dx = \int_0^{\pi}(X')^2dx$$ It is clear to me why ( $\alpha = 0$ and X constant) satisfy this equation but I'm having trouble explicitly showing that this equation is also true for $\alpha > 0$ . Could someone walk me through this conclusion? Thanks!","['integration', 'ordinary-differential-equations', 'dirichlet-series']"
3133766,Matrices with $A^2+B^2=2AB$,"Let $A, B \in M_3(\mathbb{C})$ so that $$A^2+B^2=2AB.$$ Prove that $$\det(A+B) ^2=8\det(A^2+B^2).$$ My work: Let $A=X+Y$ , $B=X-Y$ with $X, Y \in M_3(\mathbb{C})$ . The condition rewrites as $-2Y^2=[X,Y]$ . Since $Y$ commutes with $Y^2$ it will also commute with $[X, Y] $ , so according to Jacobson's lemma $[X, Y] $ is nilpotent. I am not sure if this helps, but using my notations the conclusion is equivalent to $\det(X^2)=\det(X^2+Y^2)$ and we also have $\det Y=0$ .","['matrices', 'determinant', 'linear-algebra']"
3133857,Changes in Perron eigenvector given a particular perturbation,"Let $A$ be a matrix with real and strictly positive elements ( $a_{ij}>0$ ). The row or columns of A do not add to one or to the same number. Let $x^A$ be the Perron vector of the matrix, normalized so that $\sum_i x_i^A = 1$ . Let $B$ be a new matrix with real and positive elements formed by perturbing $A$ in the following way: $b_{1j} > a_{1j}$ and $b_{ij} < a_{ij}$ for all $i \geq 2$ . Let $x^B$ be the Perron vector under the same normalization. Is it possible to show that $x_1^B > x_1^A$ and $x_i^B < x_i^A$ for all $i \geq 2$ ? Under what restrictions or conditions (properties of $A$ and $B$ ) would this be satisfied?","['matrices', 'linear-algebra']"
3133869,For which $k$ the following equation has the greatest value: $ k \cdot\binom{99}{k} $?,"After some manipulation I got $$\frac{99!}{(k-1)!\cdot(99-k)!} $$ So I guess I have to find $k$ for which I get the smallest denominator, but I don't know where to go from there.","['binomial-theorem', 'discrete-mathematics']"
3133943,Limit $\lim\limits_{n \rightarrow +\infty} \frac{\sum\limits_{k=1}^{n} \sqrt[k] {k} }{n}= 1$,I would like to prove that: $$\lim\limits_{n \rightarrow +\infty} \frac{\sum\limits_{k=1}^{n} \sqrt[k] {k} }{n}= 1$$ I thought to write $\sqrt[k] {k} = e^{\frac{\ln({k})}{k}}$ but I don't know how to continue.,"['limits', 'radicals']"
3133952,Spivak Calculus on Manifolds - Tangent space on a boundary point of a manifold,"I am an undergraduate student who is studying Spivak's calculus on manifolds. I have several questions in the pages 119 and 120 of the book, which are about the tangent space at a boundary point of a manifold. Our definitions of vector fields, forms, and orientations can
be made for manifolds-with-boundary also. If $M$ is a $k$ -dimensional
manifold-with-boundary and $x\in\partial M$ , then $(\partial M)_x$ is
a $(k-1)$ -dimensional subspace of the $k$ -dimensional vector space $M_x$ . Thus there are exactly two unit vectors in $M_x$ which are
perpendicular to $(\partial M)_x$ ; they can be distinguished as follows
(Figure $5-8$ ). If $f:W\to\mathbf{R}^{n}$ is a coordinate system with $W\subset H^{k}$ and $f(0)=x$ , then only one of these unit vectors is $f_*(v_0)$ for some $v_0$ with $v^{k}<0$ . This unit vector is called
the outward unit normal $n(x)$ ; it is not hard to check that this
definition does not depend on the coordinate system $f$ .
Suppose that $\mu$ is an orientation of a $k$ -dimensional manifold-with-boundary $M$ . If $x\in\partial M$ , choose $v_1,\ldots,v_{k-1}\in(\partial M)_x$ so that $[n(x),v_1,\ldots,v_{k-1}]=\mu _x$ . If it is also true that $[n(x),w_1,\ldots,w_{k-1}]=\mu _x$ , then both $[v_1,\ldots,v_{k-1}]$ and $[w_1,\ldots,w_{k-1}]$ are the same orientation for $(\partial M)_x$ . This
orientation is denoted by $(\partial\mu)_x$ . It is easy to see that the orientations $(\partial\mu)_x$ ,for $x\in\partial M$ , are consistent on $\partial M$ . Thus if $M$ is orientable, $\partial M$ is also orientable, and an orientation $\mu$ for $M$ determines an orientation $\partial\mu$ for $\partial M$ , called the induced
orientation . If we apply these definitions to $\mathbf{H}^{k}$ with the usual
orientation, we find that the induced orientation on $\mathbf{R}^{k-1}=\left\{x\in\mathbf{H}^{k}:x^{k}=0\right\} $ is $(-1)^{k}$ times the usual orientation. The reason for such a choice will become
clear in the next section. I think I understand why there are exactly two unit vectors in $M_x$ which are perpendicular to $(\partial M)_x$ . This is because the dimension of the subspace of $M_x$ that is orthogonal to $(\partial M)_x$ is 1, and in a one dimensional space, there are two unit vectors which have the same size but the different directions (Is this correct?). And Spivak says in the picture that ""..., then only one of these unit vectors is $f_*(v_0)$ for some $v_0$ with $v^k < 0$ . Is it right that ""these unit vectors"" mean the two unit vectors perpendicular to $(\partial M)_x$ and that it is 'only one' because it cannot have two different directions at the same time? Spivak says it is easy to see that given an orientation of a k dimensional manifold with boundary $M$ , the orientations $(\partial \mu)_x$ for $x \in \partial M$ are consistent on $(\partial M)_x$ . My reasoning is that following the Spivak's definition of consistent choices of orientations, consider a chart $f: W \subset H^k \rightarrow \mathbb{R}^n$ and $a, b \in W$ ( $a$ is an interior point and $b$ is an interior point). Because by assumption $M$ is orientable, if $\mu_{f(a)} = [f_*((e_1)a), . . . , f_*((e_k)a)]$ , then $\mu_{f(b)} = [f_*((e_1)b), . . . , f_*((e_{k - 1})b), n(b)]$ where $n(b)$ is an outward unit normal. Then, we can choose $[f_*((e_1)b), . . . , f_*((e_{k - 1})b)]$ as $(\partial \mu)_b$ . In this way, by restricting $f$ to the intersection of its image and $\partial M$ , we can get the same orientation for all $x \in \partial M$ ; thus, if $M$ is orientable, so is $\partial M$ . Please check if my reasoning is okay. At the bottom of the picture, Spivak says the induced orientation on ... is $(-1)^k$ times the usual orientation. I really cannot understand how this happens. In the next page, he says that the vectors $n(x)$ vary continuously on $M$ . Also, conversely, if a continuous family of unit normal vectors $n(x)$ is defined on all of $M$ , we can determine an orientation of $M$ . Could you explain why this is the case? Thank you!","['manifolds-with-boundary', 'smooth-manifolds', 'manifolds', 'general-topology', 'differential-geometry']"
3133954,"Having the sequence $(a_{n})_{n\geq1}$, $a_{n}=\int_{0}^{1} x^{n}(1-x)^{n}dx$, find $\lim\limits_{n{\rightarrow}\infty} \frac{a_{n+1}}{a_{n}}$","Having the sequence $(a_{n})_{n\geq1}$ , $a_{n}=\int_{0}^{1} x^{n}(1-x)^{n}dx$ , find $\lim\limits_{n{\rightarrow}\infty} \frac{a_{n+1}}{a_{n}}$ . Is this limit equal to $\lim\limits_{n{\rightarrow}\infty}(a_{n})^{\frac{1}{2}}$ ? I am unsure if I can apply l'Hospital to this limit, as the integral is a real value. I tried applying integration by parts but I ended nowhere.
Later edit: I am so sorry, it is $(1-x)^{n}$ , not $(1-x^{n})$ , I did this mistake out of tiredness.","['integration', 'limits', 'definite-integrals', 'sequences-and-series']"
3133963,Is the theory of the category of topological spaces computable?,"This question is inspired by this Mathoverflow question . Ignoring size issues, there is a natural way to view a category as a first-order structure in a finite language. In light of this we can ask about the computational complexity of the first-order theory of a given category. Note that a lot of important structure is lost when passing to the first-order level; nevertheless, it still seems interesting to me. Plenty of categories are easily checked to have very complicated first-order theories - for example, assuming the axiom of choice in the background we can identify the finite sets as those for which every self-injection is a surjection, and since Cartesian products and disjoint unions give the usual arithmetic we have that the first-order theory of the category Sets is not computable. Incidentally, choice isn't needed here, but the non-choice argument is a bit more tedious. My question is: Is the first-order theory of the category Top of topological spaces (with morphisms being continuous maps) computable? My suspicion is that the answer is no. An obvious way to prove this would be to show that the finite discrete spaces are first-order characterizable here, since then we could run the same argument as for Sets ; however, I don't see how to do this. (In particular, the notion of ""compact objcet"" of a category is not obviously first-order expressible, so that characterization of finite discrete spaces doesn't seem to help.) There are weaker versions of this question which could be asked: We could focus on the theory of the categorial equivalence class of Top - that is, the set of sentences true in every category which is categorially equivalent to Top . In general categorial equivalence does not imply elementary equivalence (since a two-element category can be categorially equivalent to a one-element category ), so this is a nontrivial weakening. We could also restrict attention to a subcollection of categories - that is, pin down some non-first-order categorial property $P$ of Top , and then ask for a first-order theory which distinguishes the theory of Top (or the theory of the categorial equivalence class of Top ) within this class . For example, Lawvere's ETCS characterizes Sets up to categorial equivalence amongst the locally small complete categories . I am also interested in comments along these lines; however, my main question is specifically about the first-order theory of Top . EDIT: David Roberts brought to my attention Schlomiuk's paper An elementary theory of the category of topological spaces ; among other things, this gives a computable (indeed, finite) theory characterizing Top up to categorial equivalence amongst complete categories, as well as in a more subtle sense (any such category is equivalent to Top $^\mathfrak{S}$ for some model $\mathfrak{S}$ of ETCS).","['general-topology', 'logic', 'category-theory', 'computability']"
3133981,Is it possible for $x$ to appear in the definition of $\delta$ in an $\epsilon-\delta$ proof of limit?,"Logic tells me it is not (it would be circular, since the allowed interval for $x$ is itself defined by $\delta$ ), but I don't know where is the error in my workings. I'll explain... My doubt arised when studying the $\epsilon-\delta$ definition for non-linear polynomial functions, say a quadratic function, when you want to prove that the limit equals the value of the function at any given value of $x$ . This could be the case when checking continuity, for example. As I've seen so far, the normal way of solving the proof for quadratic polynomials consists in converting the epsilon inequality like so: $$|f(x) - L| < \epsilon = -\epsilon < f(x) - L < \epsilon$$ Then equaling the middle part with de lower side of the $\delta$ inequality and finally choosing the minimum of the two possibilities you get, taken into account you have to assure all values of $x$ will have an image inside the $\epsilon$ interval. But! As I've recently realised, the polynomial you get after subtracting the limit to the function when both equal each other, as this nullifies to 0, is always factorable. Plus, one of the two factors you get already corresponds to the lower side of the delta inequality. Here is the formal proof of this. Given: $$\lim_{x \to n} ax^2 + bx + c = an^2 + bn + c$$ The epsilon inequality of the proof would be: $$|ax^2 + bx + c - (an^2 + bn + c)| < \epsilon$$ And working from that: $$|ax^2 + bx + c - an^2 - bn - c| < \epsilon$$ $$|ax^2 + bx - an^2 - bn| < \epsilon$$ $$|a(x^2 - n^2) + b(x - n)| < \epsilon$$ $$|a(x + n)(x - n) + b(x - n)| < \epsilon$$ $$|(x - n)(a(x + n) + b)| < \epsilon$$ $$|x - n||a(x + n) + b| < \epsilon$$ $$|x - n| < \frac{\epsilon}{|a(x + n) + b|}$$ And there it is! You'll always have the lower side of the delta inequality on the left. Thus, you could define $\delta$ as: $$\delta \le \frac{\epsilon}{|a(x + n) + b|}$$ Et voilÃ ! Here you have $x$ defining delta. Now, obviously something weird is going on here... isn't it? I highly suspect my math is flawed at some point, possibly when handling absolute values. Add to that I have no idea how to connect this method with the one that has to choose between two possible values of $\delta$ . Any help with this mess would be much appreciated!","['limits', 'calculus', 'epsilon-delta']"
3134012,"Show $f(x) = 1/x$ is in $L^2\left([1, +\infty)\right)$ but not in $L^1\left([1, +\infty)\right)$.","Proposition $f(x) = 1/x$ is in $L^2\left([1, +\infty)\right)$ but not in $L^1\left([1, +\infty)\right)$ . Discussion So my issue here is that I don't know how to use infinity in Lebesgue integration. It is intuitive (I think) that evaluation of the improper Riemann integrals \begin{align}
\int_1^\infty \left|f(x)\right| &= \int_1^\infty \frac{1}{x} = \lim_{c \to \infty} \ln c = + \infty \\ \\
\int_1^\infty \left|f(x)\right|^2 &= \int_1^\infty \frac{1}{x^2} = 1 - \lim_{c \to \infty} \frac{1}{c} = 1
\end{align} would imply our proposition, but I've only seen $L^p$ -spaces defined in the sense of Lebesgue integrals. So when I get to these steps: \begin{align}
\int_{[1, \infty)} \left|f(x)\right| &= \int_{[1, \infty)} \frac{1}{x} = \cdots \\ \\
\int_{[1, \infty)} \left|f(x)\right|^2 &= \int_{[1, \infty)} \frac{1}{x^2} = \cdots
\end{align} I'm not sure how to proceed. I'm guessing we need an argument for switching between the two types of integration, which I've read up on a little bit, but am not sure how to apply here in the improper case.","['integration', 'improper-integrals', 'normed-spaces', 'real-analysis', 'functional-analysis']"
3134027,Siegfried Bosch's Algebraic Geometry and Commutative Algebra A Good Introduction? [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 5 years ago . Improve this question Since this post was flagged, I have edited the post so that it is less opinion based to fit the guidelines. So I have had quite a good amount of interest in Algebraic Geometry as of late and would like to ask about the following text: Algebraic Geometry and Commutative Algebra by Siegfried Bosch Universitext) I have mostly been exposed to Analysis (as noted with my posting history) so I would like a change of pace that isn't too sudden.
Another thing I would like is for the text to not to have a easily found Solutions Manual. I know I can get tempted if they exist for texts (I'm looking at you Baby Rudin). Quick Questions : How is Bosch's book for a start? Is it a standard text for the subject / does it cover the standard topics needed? What are some pre-requisites for the text? Thanks for reading!","['algebraic-geometry', 'book-recommendation', 'commutative-algebra']"
3134061,Average more accurate than individual guesses,I once saw a documentary where a man enters building with a big jar filled with candies and he would go person to person asking them how much candies they believe were in the jar. The building had little over 200 people. From those 200 none got the number right. Not even close. But then he did something. He added up all the numbers that were given to him and calculated the average. The number was almost spot on! Is this principla actually real and if it is what is it called and how does it work?,['statistics']
3134080,Solving Nonlinear differential equation system,"So, I've been trying to calculate the motion of a Frisbee accounting for drag, lift, and weight, but have reached the nonlinear differential equation system $$
\begin{align}
x'&=\frac{-Dx^3}{m\sqrt{x^2+y^2}}\\
y'&=\frac{Ax}{m}+\frac{Dx^2y}{m\sqrt{x^2+y^2}}-g
\end{align}
$$ where $m,D,A,g$ and constants with initial conditions $y(0)=0$ and $x(0)=x_0$ Any ideas on how to solve it?
Thanks!",['ordinary-differential-equations']
3134092,A peculiar Euler sum,"I would like a hand in the computation of the following Euler sum $$ S=\sum_{m,n\geq 0}\frac{(-1)^{m+n}}{(2m+1)(2n+1)^2(2m+2n+1)} \tag{1}$$ which arises from the computation of $\int_{0}^{1}\frac{\arcsin\sqrt{x}\arcsin\sqrt{1-x}}{\sqrt{x(1-x)}}\,dx=\frac{\pi^3}{24}$ via FL-expansions. By symmetry it equals $$ \frac{G^2}{2}+\frac{1}{2}\int_{0}^{1}\left(\sum_{n\geq 0}\frac{x^{2n}(-1)^n}{(2n+1)^2}\right)^2\,dx \tag{2}$$ with $G$ being Catalan's constant, and we may also recast $S$ as $$ \iiint_{(0,1)^3}\frac{-\log y}{(1+x^2 z^2)(1+y^2 z^2)}\,dx\,dy\,dz. \tag{3}$$ An explicit form for $S$ is equivalent to an explicit form for $$ T= \sum_{n\geq 0}\frac{(-1)^n}{2n+1}\left[\sum_{k>n}\frac{(-1)^k}{2k+1}\right]^2\tag{4} $$ but in any case we have to deal with alternating Euler sums with weight $3$ , and the theory from Flajolet and Salvy might be helpful.","['special-functions', 'euler-sums', 'polylogarithm', 'sequences-and-series', 'summation-by-parts']"
3134105,Space of vector measures equipped with the total variation norm is complete,"Let $\Omega$ be a set $\mathcal A\subseteq2^\Omega$ with $\emptyset\in\mathcal A$ , $E$ be a $\mathbb R$ -Banach space and $\mu:\mathcal A\to E$ be additive. Now, for $A\subseteq\Omega$ , let $$|\mu|(A):=\sup\left\{\sum_{i=1}^k\left\|\mu(A_i)\right\|_E\right\},$$ where the supremum is taken over all $k\in\mathbb N$ and mutually disjoint $A_1,\ldots,A_k\in\mathcal A$ with $\bigcup_{i=1}^kA_i\subseteq A$ . It's easy to see that $$\mu\mapsto|\mu|(\Omega)\tag1$$ is a norm on the vector space of those $\mu$ for which $|\mu|(\Omega)<\infty$ . Are we able to show that this norm is complete? Assuming $(\mu_n)_{n\in\mathbb N}$ is a Cauchy sequence wrt $(1)$ of such $\mu$ . For $\varepsilon>0$ , there is a $N\in\mathbb N$ with $$|\mu_m-\mu_n|(\Omega)<\varepsilon\;\;\;\text{for all }m,n\ge N\tag2.$$ We should clearly have $$\left||\mu_m|(A)-|\mu_n|(A)\right|\le|\mu_m-\mu_n|(A)\le|\mu_m-\mu_n|(\Omega)\tag3.$$ So, $$(|\mu_n|(A))_{n\in\mathbb N}$$ is Cauchy. If $E=\mathbb R$ this might help (a signed measure can be decomposed into a negative and a positive part), but I don't see what we need to do in general. BTW : It would be great if someone knows a textbook reference where it is shown that the space of $E$ -valued vector measures equipped with the total variation norm is complete.","['measure-theory', 'probability-theory', 'bounded-variation']"
3134164,Fractional Derivative and the Fourier Transform,"I've recently came across the notion of a fractional derivative of a function $f$ that is defined as $$\big(D^{\frac{1}{2}} f\big)(x)= \frac{1}{\Gamma(\frac{1}{2})}\int_0^x (x-t)^{-\frac{1}{2}}f(t) dt$$ Now, I typically know of functions of differential operators to be defined in terms of the Fourier Transform. So in my case I'd think of the fractional derivative to be defined as $$\big(D^{\frac{1}{2}}f\big)(x) = \frac{1}{2\pi}\int_{\mathbb{R}}e^{i \langle x, \xi \rangle} \sqrt{\xi} \cdot \hat{f}(\xi) d\xi$$ I imagine these should be the same but how would I go about showing this?","['fourier-analysis', 'fractional-calculus', 'analysis', 'real-analysis']"
3134219,Why is commutativity optional in multiplication for rings?,"More precisely, why is it that all rings are required by the axioms to have commutativity in addition, but are not held to the same axiom regarding multiplication? I know that we have commutative and non-commutative rings depending on whether or not they are commutative in multiplication, but I am wondering why it is that the axioms were defined that way, providing us with this option. I am using this list of axioms, from David Sharpeâ€™s Rings and factorization : Definition 1.3.1. A ring is a non-empty set $R$ which satisfies the following axioms: (1) $R$ has a binary operation denoted by $+$ defined on it; (2) addition is associative, i.e. \begin{align}
a + \left(b+c\right) = \left(a+b\right) + c \text{ for all } a, b, c \in R
\end{align} (so that we can write $a+b+c$ without brackets); (3) addition is commutative, i.e. \begin{align}
a + b = b + a \text{ for all } a, b \in R;
\end{align} (4) there is an element denoted by $0$ in $R$ such that \begin{align}
0 + a = a \text{ for all } a \in R
\end{align} (there is only one such element because, if $0_1$ and $0_2$ are two such, then $0_1 = 0_1 + 0_2 = 0_2$ and they are the same -- we call $0$ the zero element of $R$ ); (5) for every $a \in R$ , there exists an element $-a \in R$ such that \begin{align}
\left(-a\right) + a = 0
\end{align} (there is only one such element for each $a$ , because if $b + a = 0$ and $c + a = 0$ , then \begin{align}
b = 0 + b = \left(c + a\right) + b = c + \left(a + b\right) = c + 0 = c;
\end{align} we call $-a$ the negative of $a$ ); (6) $R$ has a binary operation denoted by multiplication defined on it; (7) multiplication is associative, i.e. \begin{align}
a\left(bc\right) = \left(ab\right)c \text{ for all } a, b, c \in R;
\end{align} (8) multiplication is left and right distributive over addition, i.e. \begin{align}
a\left(b+c\right) = ab + ac,\ \left(a+b\right)c = ac + bc
\text{ for all } a, b, c \in R;
\end{align} (9) there is an element denoted by $1$ in $R$ such that $1 \neq 0$ and \begin{align}
1 \cdot a = a \cdot 1 = a \text{ for all } a \in R
\end{align} (as for the zero element, there is only one such element, and it is called the identity element of $R$ ).","['ring-theory', 'abstract-algebra', 'commutative-algebra']"
3134240,Are these two definitions of limit actually equivalent?,"I've seen the following definition of limits in the given books: Laczkovich/SÃ³s: Real Analysis Let $f$ be defined on an open interval containing $a$ , excluding perhaps $a$ itself. The limit of $f$ at $a$ exists and has value $b$ if for all $\epsilon>0$ , there exists a $\delta >0$ such that $$|f(x)-b|<\epsilon, \text{ if } 0<|x-a|<\delta$$ On the other hand: Zorich: Mathematical Analysis 1 On a function $f:E\mapsto \Bbb{R}$ . $$\forall \epsilon>0\;\; \exists \delta >0 \;\; \forall x\in E \;\; (0<|x-a|<\delta \implies |f(x)-A|<\epsilon)$$ And this sides up with Spivak's definition on his Calculus and many other books I've seen. In the second definition, the inclusion of $\forall x\in E$ seems to tell me something very different. I tried to prove the inexistence of a limit in the Dirichlet function yersterday with the first definition and couldn't but when I read Zorich's definition, it was pretty straightforward so, are both definitions actually equivalent? I guess in the first definition, the authors expect us to understand $\forall x \in E$ by pointing to a figure in the page perhaps.","['limits', 'real-analysis']"
3134267,Proof that the inverse of an analytic function is analytic which uses only real analysis.,"I would like to prove the following result: Let $f:R\to R$ be such that $f(x)=\sum\limits_{k=0}^\infty a_k(x-c)^k$ for all $x$ in some open set $O\subset R$ . Suppose that $f'(c)\ne 0$ . Then there is a function $g:T\to R$ such that $g(f(x))=x$ and $g(x)=\sum\limits_{k=0}^\infty b_k(x-f(c))^k$ , where $T$ is an open set of $R$ containing $f(c)$ . I know how to prove that $f$ is locally invertible but not how to prove that its inverse can be written as a power series on some open set. There are many proofs for the inverse function theorem which use complex analysis but I would like to write a proof which uses only real analysis. I have been writing a book of proofs of many of the important theorems in calculus and I don't want to introduce complex analysis for the proof of one theorem. Thanks, Andrew Murdza.","['power-series', 'inverse-function-theorem', 'real-analysis']"
3134269,"Diffeotopy group, Mapping Class group, Isometry group","There are several closely related concepts on the symmetries or symmetry groups of the space. My apology, but some vague imprecise definitions may be as: Mapping class group (MCG) is an important algebraic invariant of a topological space. Briefly, the mapping class group is a certain discrete group corresponding to the symmetries of the space. Diffeotopy group : The diffeotopy group D(M) of a smooth manifold M is the quotient of the diffeomorphism group Diff(M) by its normal subgroup Diff $_0$ (M) of diffeomorphisms isotopic to the identity. Alternatively, one may think of the diffeotopy group as the group $Ï€_0$ (Diff(M)) of path components of Diff(M), since any continuous path in Diff(M) can be approximated by a smooth one, i.e. an isotopy. Isometry group : the isometry group of a metric space is the set of all bijective isometries (i.e. bijective, distance-preserving maps) from the metric space onto itself, with the function composition as group operation. Its identity element is the identity function. These concepts are important and relevant for exotic $\mathbb{R}^4$ , for example, we can ask: ""Does every isometry group of an exotic $\mathbb{R}^4$ inject into its diffeotopy group ?"" So my question here is aimed for a relation and interpretations between the definitions of Diffeotopy group, Mapping Class group, Isometry group ? For example, it seems that $$
\text{MCG($S^2 \times S^1)= \mathbb Z_2 \oplus \mathbb Z_2$}
$$ $$
\text{extended-MCG($S^2 \times S^1)=\mathbb Z_2 \oplus \mathbb Z_2 \oplus \mathbb Z_2$}
$$ similarly, $$
\text{ diffeotopy group of $(S^2 \times S^1)=\mathbb Z_2 \oplus \mathbb Z_2 \oplus \mathbb Z_2$}
$$ Are there general relations between the above Diffeotopy group, Mapping Class group, Isometry group ?","['mapping-class-group', 'diffeomorphism', 'geometric-topology', 'general-topology', 'differential-topology']"
3134276,Fourier Transform of $r^n e^{-\alpha r}$,"I believe I can solve this problem, but I would like some help deciphering what my professor is asking me to do. He is having us find the Fourier transform of \begin{equation}
r^n e^{-\alpha r}
\end{equation} and says ""find this in terms of $\alpha$ derivatives of the Fourier transform of part (i)"". My result from part (i) is \begin{equation}
f(r) = \dfrac{1}{r}e^{-\alpha r} \Rightarrow \tilde{f}(k) = -\dfrac{4\pi}{(\alpha + ik)^2}
\end{equation} In that problem, he did explicitly ask us to evaluate part (i) in the limit of $\alpha \rightarrow 0$ , which of course is just $\tilde{f}(k) = 4\pi / k^2$ . Any help in understanding what to do would be great. I know how to take the Fourier transforms, but I'm unsure what he is asking us to do with the $\alpha$ derivatives. Thanks!","['integration', 'derivatives', 'fourier-transform']"
3134279,Problem in my Proof (If $\hat{f}=0$ then $f=0$) (pass derivative from integral),"I need idea about my problem but if you have another type proof please give a hint not a whole solution. Problem : Let $f\in L^{2}[-1,1], \; \hat{f}\in L^{2}(\mathbb{R})$ and $Q_{c}\hat{f}=0.$ i.e. $\int_{-c}^{c}f(x)e^{-2\pi ixt}dx=0.$ Prove that $f=0.$ What I have done: the overall idea is to use Plancherel theorem, i.e. $\Vert f\Vert_{2}=\Vert \hat{f}\Vert_{2}.$ I want to prove that $\hat{f}(t)=0$ for all $t.$ For this, I see that $\vert\hat{f}(t)\vert\leq \int_{-1}^{1}\vert f(x)\vert dx<\infty.$ Now, I want to prove that $\hat{f}(t)$ is analytic so that I can use Liouville theorem and proving that $\hat{f}=0.$ For the analyticity, I see that $e^{2\pi ixt}$ is analytic. But I can't use Leibniz integral rule because of my integrand ( $f(x)e^{-2\pi ixt}$ ) in $\hat{f}$ is not necessarily continuous.","['complex-analysis', 'fourier-analysis', 'analysis', 'real-analysis']"
3134293,Pullback of etale cover along itself,"I would like know if there's anything wrong with the following proof. Claim . Let $f : X \rightarrow S$ be a finite etale of degree $n$ . Then there is a noncanonical isomorphism $f^*X \cong \amalg_n X$ . Proof by induction on $n$ . For $n = 1$ , a finite etale morphism of degree 1 must be an isomorphism $S \rightarrow S$ , so we are done. For the induction step, assume the claim for all etale morphisms of degree $n-1 \ge 1$ and let $f : X \rightarrow S$ be finite etale of degree $n$ . The diagonal $X \rightarrow f^* X$ is a clopen immersion that splits off a component isomorphic to $X$ , hence $f^* X \cong X \amalg X'$ where $X' \rightarrow X$ is a finite etale morphism of degree $n-1$ . By induction hypothesis we are done. The above proof is a slight alteration of the proof of Stacks Project Lemma 40.18.3, which claims that there IS a trivializing etale cover for $X$ . I was surprised that I couldn't find a single source that remarks that we can just take the trivializing cover to be $X$ itself. Is there something wrong with my claim?","['algebraic-geometry', 'proof-verification']"
3134316,First-order ODE,Please look at the equation $$ \frac{d x }{dt } = \frac{c}{x} - x + h(t) $$ where $c \geq 0$ is a constant; the initial condition is given at time $t=0$ (say $x = x_0$ at $t=0$ ); and $h(t)$ is a function defined for all $t\geq 0$ . For $c=0$ the solution is $$ x = x_0e^{-t} + \int_0^t e^{-(t-\tau)} h(\tau) d \tau $$ For $h=0$ the solution is $$ x =  \sqrt{c + (x_0^2 - c) e^{-2 t} } $$ What would the solution be for both $c$ and $h$ are different from zero?,"['integration', 'analysis', 'ordinary-differential-equations', 'dynamical-systems']"
3134319,Coefficients of $(1 + x^2 + x^4 + x^7)^{10}$,"I've been asked to find the coefficient of $x^{25}$ in $(1 + x^2 + x^4 + x^7)^{10}$ . While reasoning through the problem, I was thinking that this seems equivalent to solving the system of equations $$
\begin{cases}
0\cdot x_1 + 2\cdot x_2 + 4\cdot x_3 + 7\cdot x_4 = 25 \\
x_1 + x_2 + x_3 + x_4 = 10 \\
x_i \geq 0.
\end{cases}
$$ While at first, this seemed helpful to me, I am not sure where to proceed from here. I have a feeling that either a clever use of generating functions or the multinomial theorem would be helpful, but I can't figure out how to go about it. Thanks in advance for any help.","['combinatorics', 'discrete-mathematics']"
3134358,Can we approximate any eigenvalue of an infinite matrix via eigenvalues of some sequence of submatrices which approximates the matrix?,"Let $T:\ell^2\to\ell^2$ be a compact linear operator. Let $[T]=(a_{i,j})_{i,j=1}^{\infty}$ be the representing infinite matrix of $T$ with respect to the canonical base. Let $T_n$ be the finite rank operator defined by the matrix $(a_{i,j})_{i,j=1}^{n}$ embedded into an infinite matrix. Thus $T_n\to T$ in norm. Can we approximate the eigenvalues of $T$ with eigenvalues of $T_n$ ?","['approximation-theory', 'eigenvalues-eigenvectors', 'infinite-matrices', 'functional-analysis', 'numerical-methods']"
3134371,Is there a geometry meaning of the commutator of a group?,"Conjugation has a geometric meaning: $aba^{-1}$ is the transform $b$ under coordination change $a$ . For example in the symmetry of tetrahedron, let $a$ be the anti-clockwise $1/3$ round rotation around the altitude through vertex $A$ , $b$ be the reflection by the plane through edge $AB$ , the $aba^{-1}$ is the reflection of the plane through edge $AC$ , which is just $b$ after relabeling vertices $A$ $B$$C$ $D$ by $a$ . Is there a similar geometric meaning of the commutator, $aba^{-1}b^{-1}$ ?","['lie-algebras', 'abstract-algebra', 'geometry', 'lie-groups']"
3134380,When a.e. convergence does not imply convergence in probability,"In Varadhan's probability theory text, it is noted that countable additivity is key for showing that convergence a.e. implies convergence in measure. I'm wondering if there is a salient example of a finitely additive ""probability"" measure (i.e. a measure that has all the properties of a probability measure except is merely finitely and not countably additive), such that convergence a.e. no longer implies convergence in probability. Of course, if we don't require that the entire space has finite measure, then we can come up with examples like $f_n = \chi_{n,n+1}$ , but this sort of example doesn't work in a probability space, since of course the tails have to vanish, so in fact this sort of r.v. does in fact converge in probability to zero. I'm more interested in a measure that otherwise acts like a probability measure outside of the countable additivity, in the hope that it will give me further intuition for why countable additivity is so crucial here (and please don't just refer to the proof for ""intuition"" -- I am familiar with the proof, and I understand in theory why countable additivity is important, but I think an illustrative example would make the phenomenon more salient).","['measure-theory', 'convergence-divergence', 'probability-theory', 'probability']"
3134413,Why is it that these two lim inf are the same? generalization of borel cantelli lemma,"Why is it that $\liminf_{n \rightarrow \infty} \frac{E(\eta_n^2)}{E^2(\eta_n)} =1$ is the same as saying that $\liminf_{n\rightarrow\infty} \frac{\sigma^2(\eta_n)}{E^2(\eta_n)} =0$ I am looking at this proof, and I'm confused about this implication, can someone please explain in words what these two limits mean and why they are the same thing.",['measure-theory']
3134418,What does it mean to have a subsequence of a random variable?,"For example, I have a random variable $X$ such that $X_n = \sum_{k=1}^n I_k$ where $I_n = 1$ if $A_n$ occurs and $I_n=0$ otherwise. I am looking at a proof that says that we should be able to find a subsequence $X_{n_k} (X_{n_1} < X_{n_2} < ...) $ such that ... I don't understand what this statement means, can someone please clarify More specifically I want to find a subsequence such that $\sum_{k=1}^\infty \frac{\sigma^2(X_{n_k})}{\epsilon^2E^2(X_{n_k})}<\infty$ What would a subsequence like that look like? This is the proof I am looking at:",['measure-theory']
3134468,"Why is $e^{it}$ a submersion, and what is the relationship between the derivative $\dot h(t)$ and the differential $h_{*,t}$?","My book is An Introduction to Manifolds by Loring W. Tu. In the paragraph below, why exactly is $h: \mathbb R \to S^1, h(t)=(\cos t, \sin t) \cong e^{it}$ a submersion, and what is the relationship between the derivative $\dot h(t) = (-\sin t, \cos t) \cong ie^{it}$ and the differential $h_{*,t}$ ? My argument is that the image of the differential at any $t$ , $h_{*,t}: T_t(\mathbb R) \to T_{e^{it}}(S^1)$ , is a vector subspace of $S^1$ , which has dimension 1, so the image has dimension either $0$ or $1$ , so the differential is either trivial or surjective. I think I can show each $h_{*,t}$ is not trivial directly by computing the image: $$im(h_{*,t}) = h_{*,t}(T_t(\mathbb R)) = \{h_{*,t}(a \frac{d}{dt})\}_{a \in \mathbb R} = \{ah_{*,t}(\frac{d}{dt})\}_{a \in \mathbb R}$$ $$\ne \{\text{the zero element of} \ T_{e^{it}}(S^1) \ \text{which I think corresponds to the point} \ 1+0i \in S^1 \}$$ I cannot seem to identify the precise relationship between the derivative $\dot h(t)$ and the differential $h_{*,t}$ , which I think I would assume in proving the preceding inequality. For a function $f: M \to \mathbb R$ , there is a relationship between $f$ 's a submersion at $p$ and $f$ 's partial derivatives, but now we have $f: \mathbb R \to M$ . Update : My answer considers the inclusion $g = \iota \circ h$ of $h$ into $\mathbb R^2$ . After all, we have in the first place that $h$ is smooth by Theorem 11.15 because $g$ is smooth. For such inclusion $g = \iota \circ h: \mathbb R \to \mathbb R^2$ , $g(t_0) = (\iota \circ h)(t_0) = \iota(h(t_0))=h(t_0), \iota: S^1 \to \mathbb R^2$ , we have by chain rule that for each $t_0 \in \mathbb R$ , $g_{*,t_0}: T_{t_0}\mathbb R \to T_{h(t_0)}\mathbb R^2$ is given by $g_{*,t_0} = \iota_{*,h(t_0)} \circ h_{*,t_0}$ . For each $t_0 \in \mathbb R$ , $\iota_{*,h(t_0)}: T_{h(t_0)}S^1 \to T_{h(t_0)}\mathbb R^2$ is still inclusion so $\iota_{*,h(t_0)} \circ h_{*,t_0} = h_{*,t_0}$ Let $t_0 \in \mathbb R$ . If $h_{*,t_0}$ is not the zero map, then $h_{*,t_0}$ is surjective because the image of $h_{*,t_0}: T_{t_0}\mathbb R \to T_{h(t_0)}S^1$ is a subspace of $T_{h(t_0)}S^1 \cong \mathbb R$ and thus is (isomorphic to) either $\mathbb R$ or $\mathbb R^0 = \{0\}$ . Now, I'll argue that $h_{*,t_0}$ is not the zero map if I can show that $g_{*,t_0}$ is not the zero map since $T_{h(t_0)}S^1$ and $T_{h(t_0)}\mathbb R^2$ share the same zero element since $T_{h(t_0)}S^1$ is a vector subspace of $T_{h(t_0)}\mathbb R^2$ . $g_{*,t_0}$ is not the zero map if the geometric derivative (or velocity vector) $g'(t_0) := g_{*,t_0}[\frac{d}{dt}|_{t_0}]$ is not the zero vector i.e. if $g_{*,t_0}$ maps the basis element of $T_{t_0}\mathbb R$ to a nonzero element of $T_{h(t_0)}\mathbb R^2$ . Finally, $g'(t_0)$ , by Proposition 8.15 , is represented by calculus derivative $\dot g(t) = (-\sin(t),\cos(t))$ through $g'(t_0) = -\sin(t)\frac{\partial}{\partial r^1} + \cos(t) \frac{\partial}{\partial r^2}$ , where $r^1$ and $r^2$ are the standard coordinates on $\mathbb R^2$ such that $\{\frac{\partial}{\partial r^1}, \frac{\partial}{\partial r^2} \}$ forms a basis for $T_{h(t_0)}\mathbb R^2$ . By (6), $g'(t_0)$ is not the zero vector of $T_{h(t_0)}\mathbb R^2$ . Therefore, by (7),(5),(4) $g_{*,t_0}$ is the not the zero map. Therefore, by (3) and (8), neither is $h_{*,t_0}$ i.e. $h'(t_0)$ is the not the zero vector of $T_{h(t_0)}S^1$ .","['proof-verification', 'general-topology', 'derivatives', 'differential-forms', 'differential-geometry']"
3134485,"1st order differential linear equation, question on absolute value","I'm trying to find the general solution to this equation: $$x \frac{dy}{dx}+3(y+x^2)=\frac{\sin(x)}{x} $$ Standard form puts it like this: $$\frac{dy}{dx}+\frac{3}{x}y=\frac{\sin(x)-3x^3}{x^2} $$ To determine the integrating factor I did $e^{\int{3/x}\,dx}$ and got $e^{\ln{\lvert x\rvert}^3}$ . Does this not simplify to $\lvert x\rvert^3$ ? In all the online calculators I've used, they've ignored the absolute value? The problem would be much easier if that was the case but I'm not convinced. I wouldn't know how to integrate the following with the absolute value: $$ \int{\frac{\lvert x\rvert^3}{x^2}\cdot (\sin(x)- 3x^3)\,dx}$$ I'd appreciate any help.","['integration', 'indefinite-integrals', 'absolute-value', 'ordinary-differential-equations']"
3134487,"Evaluate $\int_\gamma \frac{dz}{z-1}$, where $\gamma$ is the unit circle.","We cannot apply the winding number formula here since the curve pass through the point $1$ . How can we evaluate the integral $\int_\gamma \frac{dz}{z-1}$ , where $\gamma$ is the simple unit circle? Can we say $\int_\gamma \frac{dz}{z-1} = \int_\sigma \frac{dz}{z-1}$ , where $\sigma$ is a circle about $0$ with radius $1/2$ ?",['complex-analysis']
3134505,"Given three non-overlapping circles, can we construct (via straightedge and compass) the triangle of minimum perimeter with one vertex on each circle?","G. Polya ""Mathematics and plausible reasoning""
Chapter 9, problem 2: Three circles in a plane, exterior to each other, are given in
  position. Find the triangle with minimum perimeter that has one vertex
  on each circle. From the contents of the chapter it is obvious (using light reflections on three circular mirrors and rubber band methods) that the two sides of the required triangle that meet in a vertex on a given circle include equal angles with the radius. But how can we construct (with the compass and straightedge) these vertices (A,B,C)? UPD Let one of the circles be an infinite radius ( a straight line ): Looks like the same solution... And no idea about construction. So let all of the circles be an infinite radius: And we get Fagnano's problem with clear construction. Hope this will be useful (?)","['geometric-construction', 'recreational-mathematics', 'geometry', 'plane-geometry']"
3134521,"If $V = \text{null}(T-\lambda I) \oplus \text{range}(T-\lambda I)$, then $T$ is diagonalizable?","$V$ is a finite-dimensional complex vector space and $T \in L(V)$ ( $L(V)$ is the set of all linear maps from $V$ to itself), and $\lambda$ is arbitrary in $\mathbb{C}$ . I know $T$ is diagonalizable if it has $\text{dim}(V)$ distinct eigenvalues, or if $V$ has a basis consisting of eigenvectors of $T$ . I was thinking it might also have something to do with eigenspaces (there are a few theorems with diagonalizability and eigenspaces in my text), since those specifically handle $\text{null}(T-\lambda I)$ , but then that would leave out the range. I have the definition of an eigenspace, direct sum, diagonalizability, and some conditions equivalent to diagonalizability, i.e. a matrix is diagonalizable if V consists of eigenvectors of T, there exist 1-dimensional subspaces of V that are invariant under T, where V is the direct sum of these 1D subspaces, V is the direct sum of eigenspaces of T corresponding to each eigenvalue, and the dimension of V is the same as the dimension of the sum of these eigenspaces. Also, the book is ""Linear Algebra Done Right"" by Sheldon Axler. I also have that $T$ is diagonalizable if it contains $dim(V)$ distinct eigenvalues.","['linear-algebra', 'direct-sum', 'linear-transformations']"
3134563,Six real numbers so that product of any five is the sixth one,"One needs to choose six real numbers $x_1,x_2,\cdots,x_6$ such that the product of any five of them is equal to other number. The number of such choices is A) $3$ B) $33$ C) $63$ D) $93$ I believe this is not so hard problem but I got no clue to proceed. The work I did till now. Say the numbers be $a,b,c,d,e, abcde$ . Then, $b\cdot c\cdot d\cdot e\cdot abcde=a$ hence $bcde= +-1$ . Basically I couldn't even find a single occasion where such things occcur except all of these numbers being either $1$ or $-1$ . Are these all cases?",['combinatorics']
3134568,Find the number of group homomorphism between $A_5$ and $S_5$.,"The above question is based on this answer to a similar question. I just want to apply what has been pointed out in that answer to this question. So we are interested in number of homomorphisms $f:A_5 \to S_5$ (if any). 
Now following the ideas given in the linked answer, $A_5$ is simple, so it has to normal subgroups, viz $\{e\}$ and $A_5$ . If $f$ is a homomorphism then ker $f$ is a normal subgroup of $A_5$ . If ker $f$ = $\{e\}$ , then $A_5/\{e\}\sim f(A_5)$ , and so $f(A_5)$ is a subgroup of $S_5$ of order $60$ . If ker $f=A_5$ , then $f$ is trivial. Also we know that ""Any homomorphism from a simple group to any group is either trivial or injective."" Now in the view of above argument what can I conclude? It seems to me that only I can conclude that any homomorphism is injective. But how to calculate how many are there?  Is it a valid approach or do we need a new approach? Thanks.","['group-homomorphism', 'group-theory', 'simple-groups', 'combinatorics']"
3134582,Compute the moment generating function of $Y = X_1X_2 + X_1X_3 + X_2X_3$,"Suppose $X_1, X_2,$ and $X_3$ are independent and $N(0, 1)$ -distributed. Compute the moment generating function of $Y = X_1X_2 + X_1X_3 + X_2X_3$ . I know that any $X_iX_j$ with $i \not =j $ is a joint normal with variables $(x_i,x_j)$ I also know the formula of the moment generating function of a normal
distribution. Furthermore, I know that if $Y_1,â€¦, Y_n$ are independent $N(0,1)$ , that is, $Y = (Y_1,â€¦,Y_n )Â´$ are $N(0,I)$ by definition, the moment generating  function of Y is given by $$e^{\frac{1}{2}\mathbf t' \mathbf t}$$ I thought about using the pdf's and the definition of a moment generating function but it proved to be a really tedious process jacked of multiple integrations. Does anyone know how to easily solve this problem with some relatively simple lines? (Especially using the multivariate normal properties and matrices)","['probability-distributions', 'normal-distribution', 'matrices', 'moment-generating-functions', 'probability']"
3134600,Graded $*$-homomorphisms where are homotopic to $0$,"This is an Exercise 3.15, pg 42. Show that the map between $\Bbb Z/2 \Bbb Z$ - graded $C^*$ -algebras $$C_0(\Bbb R) \rightarrow M_2(\Bbb C)$$ $$ \varphi: f \mapsto \begin{pmatrix}
f(0) & 0 \\ 
0 & f(0)
\end{pmatrix}
$$ is homotopic (through graded $*$ -homomoprhisms) to the $0$ homomoprhisms. Whereas the $*$ -homomorphism $$
\psi:f \mapsto \begin{pmatrix}
f(0) & 0 \\
0 & 0 
\end{pmatrix}
$$ is not null homotopic. By definition in page 41, the even of $M_2(\Bbb C)$ consists of diagonal elemnts, the odd are those of off diagonals. My questions would be: 1. How does one construction this homotopy? 2. How does one show the non-homotopy for second part?","['operator-algebras', 'operator-theory', 'homotopy-theory', 'functional-analysis', 'algebraic-topology']"
3134614,increasing sequence of functions modulo fixed finite error,"Here we consider only functions from $\mathbb N$ to itself.  Say $f <_n g$ if $\{ i : f(i) \geq g(i) \}$ has size at most $n$ . Is it possible to have a fixed integer $k$ , a $<_k$ -increasing (linearly ordered) sequence $\{ f_n : n \in \mathbb N \}$ and a function $g$ such that $g >_k f_n$ for all $n$ ?","['set-theory', 'functions', 'analysis']"
3134638,Names For Polyhedra,"I am trying to enumerate the number of distinct polyhedra that can be formed from a given number of vertices.  So far, i have managed to finish the sets for 4, 5 and 6 vertices and is now a third of the way through the set for 7 vertices. I am proceeding through this exercise without using Schlegel diagrams because i am more concerned with how they look in 3-space.  The method i use is this: starting with the set for a given number of vertices, say, 5, then adding a vertex, then joining that new vertex to other vertices (whenever possible), to obtain the set of polyhedra with 6 vertices.  The task is not easy (for me that is) and i realise that having names that i can refer to for each polyhedron will make the enumeration a bit easier. I am aware that in chemistry, there is a systematic way of naming chemical compounds: the IUPAC Nomenclature system, which, is essence is mathematical in nature because it concerns permutations, graphs, and 3D spatial relationships of the constituent atoms. Is there a similar system of nomenclature for polyhedra?  for example, what are the names of the seven distinct polyhedra with seven vertices?","['polyhedra', 'geometry']"
3134654,Implicit function theorem: The result about equivalence of partial derivatives,"I am trying to understand how one obtains the result of the Implicit Function Theorem which involves the equivalence of the derivatives as stated in the related Wikipedia page ( https://en.wikipedia.org/wiki/Implicit_function_theorem ): Here, $f$ is a continuous differentiable function $f: \mathbb{R}^{n+m} \to \mathbb{R}^{m}$ . At a point $(a,b)$ , we have $f(a,b) = 0 \in \mathbb{R}^{m}$ . Then in a neighborhood $U \in \mathbb{R}^{n}$ around $a$ , we have a $C^1$ function $g: \mathbb{R}^{n} \to \mathbb{R}^{m}$ such that $f(x,g(x))=0$ and $g(a) = b$ in this neighborhood. The above equation of derivatives holds in this neighborhood as well. I tried to replicate the equation above by applying the chain rule straightforwardly to $f(x,g(x))$ . Considering the total derivative of a single component $f_i$ of $f$ with respect to $x_j$ in $U$ , we should have: $$\nabla_{x_j} f_i = \sum_{t=1}^{m}\dfrac{\partial f_t}{\partial g_t}(x,g(x))\dfrac{\partial g_t}{\partial x_j}(x) + \dfrac{\partial f_i}{\partial x_j}(x,g(x))$$ This is simply the sum of all $f_i$ 's components' derivatives with respect to $x_j$ . Generalizing the above to all $f_i$ $(1 \leq i \leq m)$ : $$\left[\nabla_{x_j} f_1, \dots,  \nabla_{x_j} f_m\right]^T_{m \times 1} = [J_{f,y}(x,g(x))]_{m \times m}\left[\dfrac{\partial g_1}{\partial x_j}(x), \dots,  \dfrac{\partial g_m}{\partial x_j}(x)\right]^T_{m \times 1} + \left[\dfrac{\partial f_1}{\partial x_j}(x,g(x)), \dots,  \dfrac{\partial f_m}{\partial x_j}(x,g(x))\right]^T_{m \times 1}$$ Here $J_{f,y}$ is the Jacobian of $f$ with respect to all $g_t$ components. Now, rearrenging I obtain: $$\left[\dfrac{\partial g_1}{\partial x_j}(x), \dots,  \dfrac{\partial g_m}{\partial x_j}(x)\right]^T_{m \times 1} = [J_{f,y}(x,g(x))]_{m \times m}^{-1}\left[\nabla_{x_j} f_1 - \dfrac{\partial f_1}{\partial x_j}(x,g(x)), \dots,  \nabla_{x_j} f_m - \dfrac{\partial f_m}{\partial x_j}(x,g(x))\right]^T_{m \times 1}$$ This is not quite the result shown on the Wikipedia page, as I have a subtraction of the partial derivatives with respect to $x_j$ from the total derivatives. What am I missing here?","['multivariable-calculus', 'calculus', 'implicit-function-theorem', 'real-analysis']"
3134656,The Lie derivative does not determine a well-defined directional derivative,"The errata for ""Riemannian Manifolds: An Introduction to Curvature"" by John Lee has for one of the problems the following correction below. Page 63, problem 4-3(b): Replace the first sentence by â€œShow that there are vector fields $V$ and $W$ on $R^2$ such that $V = W = \partial_1$ along the $x^1$ -axis, but the Lie derivatives $L_V(\partial_2)$ and $L_W(\partial_2)$ are
  not equal on the $x^1$ -axis."" I don't understand how $L_V(\partial_2)$ and $L_W(\partial_2)$ can be different on the $x^1$ -axis. Shouldn't the flow at any point on the $x^1$ -axis be the same for V and W and therefore $L_V(\partial_2)=L_W(\partial_2)$ on the $x^1$ -axis.",['differential-geometry']
3134681,How can I prove that $\ln(e^x)=x$ using the Taylor series of $e^x$ and $\ln(1+x)$?,"I'm stuck to prove that $\ln(e^x)=x$ using the facts that \begin{align}
e^x &= \sum_{k=0}^\infty \frac{x^k}{k!} \quad\text{and}  \\
\ln(1+x) &= \sum_{k=1}^\infty \frac{(-1)^{k+1}}{k}x^k.
\end{align} I tried as follows: $$\ln(e^x)=\ln\left(\sum_{k=0}^\infty \frac{x^k}{k!}\right)=\ln\left(1+\sum_{k=1}^\infty \frac{x^k}{k!}\right)=\sum_{k=1}^\infty \frac{(-1)^{k+1}}{k}\left(\sum_{n=1}^\infty \frac{x^n}{n!}\right)^k$$ but the last sum looks very complicate to simplify. Any ideas?","['sequences-and-series', 'real-analysis']"
3134700,Almost a prime number recurrence relation,"For the number of partitions of n into prime parts $a(n)$ it holds $$a(n)=\frac{1}{n}\sum_{k=1}^n q(k)a(n-k)\tag 1$$ where $q(n)$ the sum of all different prime factors of $n$ . Due to https://oeis.org/A000607 . I have found that $$a(p_n)\approx a(p_{n-2})+a(p_{n-1})\tag 2$$ and conjecture the asymptotic relation. $$\log a(p_n)\sim \log \big(a(p_{n-2})+a(p_{n-1})\big )\tag 3$$ On x-axis the prime numbers $p_n$ are plotted. The blue lines correspond to $a(p_n)$ and the red lines correspond to $a(p_{n-2})+a(p_{n-1})$ . On the oeis site above there is also a formula $$a(n)\sim e^{2\pi\sqrt{n/\log n}\,/\sqrt{3}}\tag 4$$ but I don't know if this really helps? Can this (reformulated) conjecture be proved?","['integer-partitions', 'conjectures', 'analytic-number-theory', 'discrete-mathematics', 'prime-numbers']"
3134735,About the product of two Elliptic integrals,"Let $z,x\in\left(0,1\right)$ . It is possible to prove that $$\int_{0}^{1}\int_{0}^{1}\frac{1}{\sqrt{hy\left(1-h\right)\left(1-y\right)}}\frac{dydh}{\sqrt{\left(1+zhy\right)^{2}-4xzhy}}=\frac{4}{\pi^{2}}K\left(\frac{1-\rho-z}{2}\right)K\left(\frac{1-\rho+z}{2}\right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(1)$$ where $$K\left(a\right)=\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-a\sin^{2}\left(\theta\right)}}=\int_{0}^{1}\frac{du}{2\sqrt{u\left(1-u\right)}\sqrt{1-au}}$$ is the complete elliptic integral of the first kind with the parameter being the elliptic modulus and $$\rho=\sqrt{\left(1+z\right)^{2}-4zx}.$$ It can be done using the generating function of the Legendre polynomials $$\frac{1}{\sqrt{\left(1+z\right)^{2}-4xz}}=\sum_{n\geq0}z^{n}P_{n}\left(2x-1\right)\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(2)$$ and the Brafman's formula $$\sum_{n\geq0}\frac{\left(s\right)_{n}\left(1-s\right)_{n}}{n!^{2}}z^{n}P_{n}\left(2x-1\right)= \,_{2}F_{1}\left(s,1-s;1;\frac{1-\rho-z}{2}\right){}_{2}F_{1}\left(s,1-s;1;\frac{1-\rho+z}{2}\right)$$ with $ s=1/2$ . Question 1 . Is it possible to prove $(1)$ without the use of the generating function $(2)$ but just with a clever manipulation of the double integral? Question 2 . Is it possible to prove $(1)$ using another strategy than mine (not necessarily with a manipulation of the double integral)? I suspect there is some transformation that allows us to ""separate"" the variables, but I'm not able to find it. Thank you.","['integration', 'special-functions', 'real-analysis', 'elliptic-integrals', 'hypergeometric-function']"
3134831,Unexpected proof that $\alpha!$ divides $k!$ if $\alpha_1 + \dots + \alpha_n = k$.,"Let $\alpha = (\alpha_1,\dots, \alpha_n) \in \mathbb{N}_0^n$ be a multiindex with $\alpha_1  + \dots + \alpha_n = k$ . Let $\alpha! = \alpha_1! \dots \alpha_n!$ with the convention that $0! = 1$ . I think I proved the following claim in a somewhat unexpected way, and would like some feedback if it is correct. Claim $\alpha!$ divides $k!$ Proof: We define an action of the symmetric group $S_k$ on the set $M := \{1,\dots,k\}^k$ by $\sigma (n_1,\dots,n_k) := (n_{\sigma(1)},\dots,n_{\sigma(k)})$ . Now consider the element $x = (1, \dots, 1, 2, \dots 2, \dots, n)\in M$ , where each number $i$ appears $\alpha_i$ times. This is in $M$ exactly because of the hypothesis $\alpha_1 + \dots + \alpha_n = k$ . Now the stabilizer subgroup $H$ of $x$ in $S_k$ is the group which acts independently on the blocks of size $\alpha_i$ . But this is isomorphic to $\prod_i S_{\alpha_i}$ , which has order $\alpha!$ . Because the order of any subgroup divides the order of the group we have $\alpha! = \#H \mid \#(S_k) = k!$ . What I like about this is that it is free of any calculations, which would be my first approach to prove this claim.","['symmetric-groups', 'proof-verification', 'combinatorics', 'multinomial-coefficients']"
3134844,Constructing a probability measure on the Hypercube with given moments,"Let $H = [-1, 1]^d$ be the $d$ -dimensional hypercube, and let $\mu \in \text{int} H$ . Under these conditions, I can explicitly construct a tractable probability measure $P$ , supported on on $H$ , which has $\mu$ as its mean. For my purposes, tractability means: (a) $P$ has a density, which can be written down and cheaply evaluated. (b) $P$ can be exactly sampled from, efficiently. This is a relatively simple task, since the problem basically decouples across dimensions; if you can find a probability measure $P_i$ on $[-1,1]$ with mean $\mu_i$ , then setting $P = P_1 (x_1) \cdots P_d (x_d)$ suffices. I've been using the family of random variables given by $P(x) \propto \exp( \langle \theta, x \rangle)$ to do this; the mean of each coordinate is then given by $m_i(\theta) = \coth \theta_i - \theta_i^{-1}$ which is surjective onto $(-1,1)$ and so one can always find a $\theta$ which gives you the mean you want. Now, suppose I want to extend this, and construct a probability measure on $H$ which also has the covariance matrix I want, i.e. let $\Sigma$ be a general $d \times d$ positive semi-definite matrix. Then, I want all the above, plus: \begin{align}
\textbf{Var}_{X \sim P} [X] &= \Sigma
\end{align} This is more or less possible when $\Sigma$ is diagonal, but otherwise I haven't figured out how to do it. Of course, in the general case, I will need $\Sigma$ to be sufficiently small, as I can't have arbitrarily high variance when constrained to a cube, I've considered sampling from a truncated Gaussian, i.e. sampling from $\mathcal{N}(\mu, \Sigma)$ until you land in $H$ . However, this won't generally have a tractable density, due to the normalising constant. It also won't quite have the right moments, though I can basically forgive this; I ultimately don't mind too much if I only have \begin{align}
\mathbf{E}_{X \sim P} [X] &\approx \mu \\
\textbf{Var}_{X \sim P} [X] &\approx \Sigma
\end{align} with some control of the bias. I should perhaps highlight that it's not sufficient for my purposes to approximate $\Sigma$ by a diagonal matrix. So, the concise version of my question is: is there a way of explicitly constructing probability measures on the hypercube which have the mean and covariance matrix which I want it to have? There needn't be a 'canonical' / optimal' answer (whatever that might mean here); something functional will entirely suffice. It's also preferable to have a construction where it's easy to adapt things to sample from a range of different $(\mu, \Sigma)$ .","['covariance', 'probability-distributions', 'moment-problem', 'sampling', 'probability']"
3134849,Matrices commuting with the matrix of ones,"Let $J_n$ be $n \times n$ matrix of ones , so that $J_{ij} = 1$ for all $i,j \in\{1,\ldots,n\}$ . I am interested to find the class of matrices which commute with $J_n$ , i.e. $$M J_n = J_n M.$$ It is not difficult to see that a permutation matrix satisfies this property. By linear superposition, the doubly stochastic matrix also commutes with $J_n$ . Are there any other matrices, apart from doubly stochastic ones, with this property?","['matrices', 'matrix-equations']"
3134869,Example of a preclosure that is not a closure,"For a set $X$ we can define an operator $cl:\mathscr{P}(X)\rightarrow\mathscr{P}(X)$ satisfying for all $A,B\subseteq X$ . $$cl(\emptyset)=\emptyset\tag{1}$$ $$A\subseteq cl(A)\tag{2}$$ $$cl(cl(A))=cl(A)\tag{3}$$ $$cl(A\cup B)=cl(A)\cup cl(B)\tag{4}$$ Kuratowski's theorem says that if such operator is found we can define a topology $\tau$ on X where $cl(A)$ is the $\tau$ -closure of $A$ . My goal was to find operators say $\varphi:\mathscr{P}(X)\rightarrow \mathscr{P}(X)$ s.t. for each (1)-(4) it satisfies the other (3) so for example $\varphi_1:A\mapsto X$ for arbitrary set $X$ satisfies 2,3,4 but not 1, for $\varphi_2:A\mapsto\emptyset$ for arbitrary set $X$ satisfies 1,3,4 but not 2. For 3rd assume $X=\mathbb{N}$ and consider the map $\varphi_4:A\mapsto A\cup\{\min{A}\cdot 2\}$ . Where for example for sets $A=\{2,3\},B=\{3,4\}$ this fails the condition (4). What could be an example of $\varphi_3$ ? Something that satisfies 1,2,4 but not 3? We can assume any set $X$ .","['order-theory', 'general-topology']"
3134938,Why is the algebraic torus an affine variety?,"I am reading Toric Varieties by Cox, Little, and Schenck. I am stuck on the definition an algebraic torus, given in Part 1.1, page 10, which states: The affine variety $(\mathbb{C}^*)^n$ is a group under componentwise multiplication. A torus $T$ is an affine variety isomorphic to $(\mathbb{C}^*)^n$ , where $T$ inherits a group structure from the isomorphism. Now, my understanding of the definition of an affine variety, as per Hartshorne is a subset $A$ of affine $n$ -space over an algebraically closed field $k$ , such that $A = Z(B)$ , i.e. $A$ is the zero-locus of some set $B\subset k[X_1,\dots,X_n]$ . My question is, why is it stated that $(\mathbb{C}^*)^n$ is an affine variety? What set of polynomials is it the zero-locus of? In particular, if $n=1$ , then the only algebraic sets are finite sets, so how can $\mathbb{C}^* = \mathbb{C}\setminus \{0\}$ be an algebraic set if it is infinite? Now, I suspect that affine variety should maybe be just variety as per Hartshorne's definition in Hartshorne page 15 where he defined a variety to be any affine, quasi-affine, projective, or quasi-projective variety. And thus together with morphisms between varieties we have the category of varieties. Also I talked to some other people who know more about algebraic geometry who seems to not understand why there is confusion, since apparently they have a more general and abstract definition of a variety than merely as zero-loci. Whereas I understand that maybe after I've read enough of Hartshorne I may resolve this issue, the prerequisites listed for this particular chapter of Toric Varieties , which is also another book by Cox, called Ideals, Varieties, and Algorithms also only uses this ""elementary"" definition. Therefore I expect this question to be resolvable within the bounds of my current progress of Hartshorne. Would glad to have any help, thanks.","['affine-varieties', 'algebraic-geometry', 'toric-varieties']"
3134946,Conditional Probability - A die is tossed 5 times.,"A die is tossed 5 times. Given that the die falls six at least once, what's the probability it falls six at least twice? Let $A$ be the event the die falls six at least once. $P(A) = 1 - (\frac{5}{6})^{5}$ . Let $B$ be the event the die falls six at least twice. We want to find $$P(B | A) = \frac{P(B \cap A)}{P(A)}$$ How do I find $P(B\cap A)$ ? EDIT: $$\begin{align} P(B) & = P(A \cap B). \\ & =1 -\left [ \binom{5}{0} \left (\frac{1} {6} \right)^{0}\left (\frac{5}{6} \right)^{5} + \binom{5}{1}\left (\frac{1}{6} \right )^{1} \left (\frac{5}{6} \right)^{4} \right ]. \\  & = 1 - 2\times \bigg(\frac{5}{6}\bigg)^{5}. \\ & = 1 - .803755 \cdots \\ & = 0.196244856 \cdots \end{align}$$ is approx 19%.","['conditional-probability', 'dice', 'discrete-mathematics', 'probability']"
3134956,Solve $\int\limits_{0}^{\infty}\frac{\sin(x)}{xe^x} \ dx.$,"I want to solve this using Fourier theory and Plancharels theorem. This theorem states that: Plancharels Theorem: For any $f\in L^2(\mathbb{R}), \ \widehat{f}\in L^2(\mathbb{R}).$ Moreover, $$\langle\widehat{f},\widehat{g}\rangle=2\pi\langle f,g \rangle,$$ and
  thus $$||\widehat{f}||^2_{L^2}=2\pi||f||^2,$$ $\forall \ f,g\in
 L^2(\mathbb{R}).$ The set of functions denoted by $L^2(\mathbb{R})$ is a Hilbert space with scalarprouduct $$\langle f,g \rangle=\int_{\mathbb{R}}f(x)g(x) \ dx,\  \text{if} \ g,f\in\mathbb{R},$$ thus the first line in Plancharels can be rewritten as $$\int_{\mathbb{R}}\hat{f}(x)\hat{g}(x) \ dx = 2\pi\int_{\mathbb{R}}f(x)g(x) \ dx.$$ But unfortunately our integral of concern is not over $\mathbb{R}$ , to ammend this we can extend the integral to $\mathbb{R}$ by writing $$\int_{0}^{\infty}=\frac{\sin(x)}{xe^{x}} \ dx = \frac{1}{2}\int_{\mathbb{R}}\frac{\sin(x)}{xe^{|x|}} \ dx = \frac{1}{2}\int_{\mathbb{R}}\frac{\sin(x)}{x} e^{-|x| \ dx}.$$ This is where I get stuck. In the notes it says the following in order to proceed: Using the Plancharel trick, we can replace those functions (which I assume are $\sin(x)/x$ and $e^{-|x|})$ by the Fourier transforms: $$\frac{1}{2}\int_{\mathbb{R}}\frac{\sin(x)}{x}e^{-|x|} \ dx =\frac{1}{4\pi}\int_{\mathbb{R}}\pi\chi_{(-1,1)}(x)\frac{2}{x^2+1} \ dx = \frac{1}{2}\int_{-1}^{1}\frac{1}{x^2+1} \ dx...$$ Can someone explain what has happened here? I don't see the $\chi-$ function mentioned anywhere in the notes and how to apply it. But reading online it seems to be some sort of indicator function which I'm not familiar with. Any help is greatly appreciated.","['integration', 'fourier-analysis', 'fourier-transform']"
3134973,Zariski projective Topology and quotient Topology,"A projective varieties in $\mathbb{P}^n$ is the common zero locus of some homogeneous polynomials. It is simply to prove that the projective varieties on $\mathbb{P}^n$ are closed set of a Topology $\tau_z$ , that we will define Zariski Topology on $\mathbb{P}^n$ . There is another natural Topology on $\mathbb{P}^n$ that is the quotient Topology $\tau_q$ induced by projection map $\pi : \mathbb{C}^{n+1}/ \{0\} \to \mathbb{P}^n$ where the Topology on $\mathbb{C}^{n+1}/ \{0\}$ is the Zariski Topology of $\mathbb{C}^{n+1}$ induced on the subset $\mathbb{C}^{n+1}/ \{0\}$ . It is correct? I want understand if there is a relationship between $\tau_z$ and $\tau_q$ . I think that they are equal because : $\pi$ is continuos if the Topology fixed on $\mathbb{P}^n$ is the Zariski Topology $\tau_z$ so $\tau_z\subseteq \tau_q$ ; If $X\subseteq \mathbb{P}^n$ is closed on $\tau_q$ then $\pi^{-1}(X)$ is closed on $\mathbb{C}^{n+1}/ \{0\}$ so there exists an ideal $I\subseteq \mathbb{K}[x_0,\dots , x_n]$ such that $\pi^{-1}(X)=Z(I)/ \{0\}$ .
Then $X=\pi(\pi^{-1}(X))=\pi(Z(I)/ \{0\})=Z^{\mathbb{P}^n}(I)$ that is closed on $\tau_z$","['geometry', 'algebraic-geometry', 'abstract-algebra', 'general-topology', 'algebraic-topology']"
3134982,How to show that origin is a saddle point for the following function.,"Let $f:\mathbb{R}^2\to \mathbb{R}$ be defined by $f(x, y)= x^6-2x^2y-x^4y+2y^2$ . Then I need to show that $(0, 0)$ is a saddle point of $f$ . My effort: We have $f_{xx}= 30x^4-4y-12x^2y$ , $f_{xy}= -4x-4x^3$ , $f_{yy}= 4$ . At $(0, 0)$ , we have $$f_{xy}^2-4f_{xx}f_{yy}= 0$$ at $(0, 0)$ . Also $f_{xx}=0$ , means that second order derivative test is inconclusive. So, further test is required. Observe that $f(h, k)-f(0, 0) = h^6-2h^2k-h^4k+2k^2$ . For $h = k$ , we have $$f(h, h)-f(0, 0)= h^6-2h^3-h^5+2h^2$$ As $h$ is near $0$ , $$f(h, h)-f(0, 0)>0.$$ For $h \neq k$ , $$f(h, k)-f(0, 0) = h^6-2h^2k-h^4k+2k^2 = h^2(h^4-2k-h^2k)+2k^2$$ How to proceed further? Am i proceeding in right direction or not?","['maxima-minima', 'multivariable-calculus']"
3134991,"If nine coins are tossed, what is the probability that the number of heads is even?","If nine coins are tossed, what is the probability that the number of heads is even? So there can either be 0 heads, 2 heads, 4 heads, 6 heads, or 8 heads. We have $n = 9$ trials, find the probability of each $k$ for $k = 0, 2, 4, 6, 8$ $n = 9, k = 0$ $$\binom{9}{0}\bigg(\frac{1}{2}\bigg)^0\bigg(\frac{1}{2}\bigg)^{9}$$ $n = 9, k = 2$ $$\binom{9}{2}\bigg(\frac{1}{2}\bigg)^2\bigg(\frac{1}{2}\bigg)^{7}$$ $n = 9, k = 4$ $$\binom{9}{4}\bigg(\frac{1}{2}\bigg)^4\bigg(\frac{1}{2}\bigg)^{5}$$ $n = 9, k = 6$ $$\binom{9}{6}\bigg(\frac{1}{2}\bigg)^6\bigg(\frac{1}{2}\bigg)^{3}$$ $n = 9, k = 8$ $$\binom{9}{8}\bigg(\frac{1}{2}\bigg)^8\bigg(\frac{1}{2}\bigg)^{1}$$ Add all of these up: $$=.64$$ so there's a 64% chance of probability?","['solution-verification', 'discrete-mathematics', 'probability']"
3135015,Is the rectangular function a convolution of $L^1$ functions?,"Do there exist functions $f,g$ in $L^1(\mathbf{R})$ such that the convolution $f \star g$ is (almost everywhere) equal to the indicator function of the interval $[0,1]$ ?","['convolution', 'lp-spaces', 'functional-analysis', 'real-analysis']"
3135088,"Applications of ""finite mathematics"" to physics","Disclaimer: I know that what follows is a biased view on applications, one of the points of the question is to eliminate some of that bias. When I think of applications of maths outside of itself, I have the impression that applications in physics are mostly related to ""continuous/smooth mathematics"" : representation theory of Lie groups, PDEs, functional analysis, different kinds of differential geometry (I don't know if they technically fit into that, but I include, say, symplectic geometry and riemannian geometry in that word), probability theory, and a bunch of other stuff, but all somehow related to $\mathbb{R,C}$ and the differential or topological or measurable structure on these (or related structures); while ""discrete/finite mathematics"" (here I'm almost sure I'm not using the right terminology - what I mean by that is stuff like finite group theory, representation theory of abstract groups, ring theory, linear algebra over finite fields, algebraic geometry, combinatorics, finite probabilities, number theory, graph theory and again a bunch of stuff that somehow fits the intuitive meaning one could put behind ""finite"" or ""discrete"" mathematics) seems to have applications mainly in computer science and related fields. Now this view is probably very biased, and that's because I don't know that many applications of maths/much applied maths. The point of this question is to, if possible, get rid of some of that bias. Since asking ""what are applications of mathematics ?"" would be way too broad, I'll ask something more specific and more related to my personal interests. What, if any, are some applications of ""finite/discrete mathematics"" to physics ? More specifically of ""finite/discrete"" algebra ? (Note that here I use words ""finite/discrete mathematics"" in the sense that I tried to describe vaguely above, not in the common sense, if it is different)","['applications', 'physics', 'soft-question', 'discrete-mathematics']"
3135188,Why is it the metric that allows for the canonical tangent space/ cotangent space identification?,"I have seen the phrase ""The metric allows for a canonical identification of the tangent space with the cotangent space"" all over diff geo resources and questions. I understand the map and why it serves as an identification, but since it works using an inner product, isn't the inner product what allows for the identification? I understand that inner products can induce metrics, but I don't see how the metric comes into play here.",['differential-geometry']
3135237,Calculate in closed form: $\int_{0}^{\pi/2}\arctan\left(\frac{1}{\sqrt{2\sin x}}\right)dx$,"I did the replacement, $u = \sqrt{2\sin x}$ , but I did not succeed. $u = \sqrt{2\sin x}$ . I found, $$
\int_{0}^{\pi/2}\arctan(x)\cot(x)\,dx, \quad \int_{0}^{\pi/2}\arctan(\sin x)\,dx
$$ But the techniques used in these integrals did not help much. Thank you for any help.","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
3135244,Determine whether this map $\phi$ is an isomorphism,"$\langle M_2 (\mathbb{R}), \cdot \rangle$ with $\langle \mathbb{R}, \cdot \rangle$ where $\phi(A)$ is the determinant of matrix $A$ . I don't believe the map is an isomorphism because  I don't think it is injective (one-to-one). I know that if a matrix is not invertible, then the determinant will always be 0 and that the $\det(A) = \det(A^T)$ . However I am having a hard time backing up my answer and explaining what I know to show that the map is not an isomorphism. Here is what I have so far. Let $A$ be a $2 \times 2$ Matrix and let B be the transpose of A. Then $\phi(A) = \det(A) = \det(B) = \phi(B)$ . Is this sufficient enough to show that the map is not injective?","['matrices', 'abstract-algebra']"
3135248,"Is the function $A \mapsto \sum\limits_{j=0}^{\infty} \langle A^j v, A^j v \rangle$ differentiable everywhere?","Suppose $v \in \mathbb R^n$ is a fixed vector. We define a scalar-valued function on $n \times n$ matrices $f: M_n(\mathbb R) \to \mathbb R$ by \begin{align*} A \mapsto \sum\limits_{j=0}^{\infty} \langle A^j v, A^j v \rangle. \end{align*} Let us denote the domain of $f$ by $\text{Dom}(f) = \{A \in M_n(\mathbb R): f(A) < \infty\}$ . It is clear if $\rho(A) < 1$ (spectral radius), then $A \in \text{Dom}(f)$ . If I am not mistaken, $f$ should also be differentiable on $\{A: \rho(A) < 1\}$ . On the other hand, if $\rho(A) \ge 1$ , it is still possible $A \in \text{Dom}(f)$ . For example, if $v$ is chosen to be an eigenvector corresponding to an eigenvalue strictly smaller than $1$ . The question bugs me is: could the function be differentiable on the set $\text{Dom(f)} \setminus \{A:\rho(A) < 1\}$ .","['matrix-analysis', 'linear-algebra', 'real-analysis']"
3135260,"Suppose $a,b\in\mathbb{Z}$. If $a^2(b^2-2b)$ is odd, then $a$ and $b$ are odd.","Suppose $a,b\in\mathbb{Z}$ . If $a^2(b^2-2b)$ is odd, then $a$ and $b$ are odd. Assume not, that is either $a$ is not odd or $b$ is not odd. Wlog, assume $a$ is even. So $a^2$ is even. Let $(b^2-2b)=x$ . Then, $a^2 x=a^2(b^2-2b)$ is even, contradiction. Can you check my proof?","['elementary-set-theory', 'elementary-number-theory', 'proof-writing']"
3135269,Product of two HÃ¶lder continuous function on a bounded interval is also HÃ¶lder continuous.,"Prove that the product of two HÃ¶lder-1/2 continuous functions on a bounded interval is also HÃ¶lder-1/2 continuous. Given a function on a bounded interval, $f:I\rightarrow \mathbb{R}$ , $f$ is HÃ¶lder-1/2 continuous if there exists some $M$ such that $|f(x_1)-f(x_2)|\leq M|x_1-x_2|^{1/2}$ . But I'm having trouble how to link this to the product of two HÃ¶lder continuous functions, $fg:I\rightarrow\mathbb{R}$ .","['holder-spaces', 'ordinary-differential-equations']"
3135303,Exponential growth/decay formula: what happened to the other constant of integration?,"The standard equation for exponential growth and decay starts and is derived like this: $$ {dP\over dt}=kP$$ $$ {dP\over P}=kdt$$ $$ \int{dP\over P}=\int kdt$$ $$ \color{red}{\ln |P|}=kt+C$$ I don't understand the left hand side at this point, isn't $\int{1\over x}dx = \ln |x| +C$ ? Where did the constant of integration from the left integral go?","['integration', 'calculus', 'ordinary-differential-equations']"
3135335,Proving a mapping is a group action,"Let $G$ be a finite group, $S$ be the set of all subsets of $G$ of size $n$ , and for $g \in G$ , $T \in S$ define $g.T=\{gt: t \in T\}$ . My course's notes says that this is a group action of $G$ on $S$ , and ""shows"" it by first stating without proof that $g.T$ is also of size $n$ , hence in $S$ . To me it's not immediately obvious that this is true. For it to be true requires $gt_1 = gt_2 \implies t_1=t_2$ , i.e. two distinct elements of $T$ will always be mapped to distinct values by $g$ . If $g$ maps two distinct elements of $T$ to the same value then the cardinality of $g.T$ will be less than $n$ . Why is it impossible for $g$ to map two distinct values $t_1, t_2$ to the same value?","['group-theory', 'group-actions', 'finite-groups']"
3135351,Probability of rolling fewer than m distinct numbers on n sided die after k rolls,"Question Suppose I have an n-sided die which I roll k times. For any given m $\le$ n, what is the probability that I roll $\le$ m distinct numbers. Follow-up What is the smallest m such that I can be 99% sure there are $\le$ m distinct numbers after k rolls?","['statistics', 'dice', 'probability']"
3135371,"Example of a non-abelian group $(G,.)$ where $a^2b=ba^2\Rightarrow ab=ba $","Give and example of a non-abelian group $(G,.)$ where $a^2b=ba^2\Rightarrow ab=ba$ for all $a,b\in G$ . Can somebody give me some tips, please? Moreover how did you think to get there. I've found that $C(a^2) \subset C(a)=C(a^{|G|+1}) $ Edit: The answer sheet gives the solution the group of matrices of the form $$\begin{pmatrix} 
\hat 1 & a & b \\ 
\hat 0& \hat 1 & c \\ 
\hat 0 & \hat 0 & \hat 1  
\end{pmatrix}\qquad\text{ with }\ a,b,c \in  \Bbb{Z}/3\Bbb{Z}.$$ Then $A^3=I_3$ for all such matrices.
I wanted to know if there are some easier groups to find. It's pretty hard to find matrices.","['contest-math', 'group-theory', 'examples-counterexamples']"
3135399,Calculate the Jordan normal form,"I have the matrix $A=\begin{bmatrix}  -2 & -3 & 6 \\ 1 & 2 & -2\\ -1 & -1 &3 \end{bmatrix}$ and I have to find the transformation matrix and its Jordan normal form. This is what I did so far: Char. polynomial: $p_A=(\lambda-1)^3$ so I have eigenvalue $\lambda=1$ . Then I calculated the kernel: $$\ker(A-I_3)=\ker\begin{pmatrix} -3 & -3 & 6 \\ 1 & 1 & -2 \\ -1 & -1 &2 \end{pmatrix} = \operatorname{span}\left\{\begin{pmatrix} -1\\1\\0\end{pmatrix};\begin{pmatrix} 2\\0\\1\end{pmatrix}\right\}$$ Then I have to calculate a third vector $v_3$ , such that: $(A-I_3)v_3=v_2$ but the system doesn't give me a solution for this vector, am I missing something?","['matrices', 'jordan-normal-form', 'linear-algebra']"
3135407,Recursive formula for Bernoulli numbers from power series,"This is Ch 7, Exercise 62 in Palka's Complex Function Theory . Define $$ f:B(0,2 \pi) \to \mathbb{C},z \mapsto \frac{z}{e^z-1}\text{ if }z \neq 0, \; f(0):=1$$ Let $f$ have Taylor series $$f(z)= \sum_{n=0}^\infty \frac{B_n}{n!}z^n$$ where the $B_n$ are Bernoulli numbers (Here I use Wikipedia's notation, not Palka's). The question is to show $$\sum_{k=1}^{n}\binom{2n+1}{2k}B_{2k}= \frac{2n-1}{2}\tag{1}$$ I am stuck on showing this, but have some initial leads (from earlier in the problem). We have $B_0=f(0)=1$ , and from Palka's hint, $z \mapsto z/2+f(z)$ is even: $$\begin{split}
\frac{z}{2}+f(z)=& \frac{z(e^z-1)+2z}{2(e^z-1)}= \frac{ze^z+z}{2(e^z-1)}\\
\frac{-z}{2}+f(-z)=& \frac{-z}{2}- \frac{ze^z}{1-e^z}= \frac{-z(e^z-1)+2ze^z}{2(e^z-1)}= \frac{ze^z+z}{2(e^z-1)}
\end{split}$$ which reveals $B_1=-1/2$ and $B_n=0$ for $n>2$ odd. I have also tried manipulating $(1)$ : since $B_1$ is the only nonzero odd Bernoulli number: $$\sum_{k=1}^{2n}\binom{2n+1}{k}B_k= \frac{2n-1}{2}+ \binom{2n+1}{1}B_1=-1$$ or starting the sum at $0$ : $$\sum_{k=0}^{2n}\binom{2n+1}{k}B_k=-1+ \binom{2n+1}{0}B_0=0$$ but I didn't get any additional insight on how to show $(1)$ . Any help is greatly appreciated.","['complex-analysis', 'power-series', 'bernoulli-numbers', 'sequences-and-series']"
3135408,Integral involving incomplete beta function,"I have the following integral, $$\int_{0}^1x^{a-1}(1-x)^{b-1}B_x(c,d)dx$$ where $B_x(c,d) = \int_{0}^xt^{c-1}(1-t)^{d-1}dt$ is the incomplete beta function , and $a,b,c,d>0$ . Question : Does this have a closed form? My attempt : First, playing around in Wolfram Alpha makes me think that there may be a (simple?) closed form: example 1 and example 2 . The second example can also be written as $\int x^2(1-x)B_x(12,2)dx = \frac{1}{4}(B_x(16,2) - x^4B_x(12,2))+\frac{1}{3}(x^3B_x(12,2)-B_x(15,2))$ . It seems there is a reduction when $a=c$ , $b=d$ as $$\int_0^1 x^{a-1}(1-x)^{b-1}B_x(a,b)dx=\frac{1}{2}\left(\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\right)^2$$ where $\Gamma(a)$ is the gamma function . However, for the case when $a\neq c$ , $b\neq d$ , things are not quite as clear for me. I found a similar question here but it has not been answered. I also found this but I'm not sure if it is useful.","['integration', 'statistics', 'gamma-function', 'beta-function', 'hypergeometric-function']"
3135426,Getting different answers when using product rule and limit substitution than I do with quotient rule,"I'm trying to differentiate $$y = \frac{x+1}{x-1}.$$ Using quotient rule: http://prntscr.com/mt90yc Using product: http://prntscr.com/mt91dd Using limit definition: http://prntscr.com/mt91ih I get $-2$ for product and limit, but I get $\frac{-2}{x^{2}-2x+1}$ using quotient.","['calculus', 'derivatives']"
3135440,Using a standard pair of dice. What is the probability of roling a $12$ four tosses in a row?,This is throwing me off a bit I believe mainly because the way the question is worded? Would this simply be $4$ out of $36$ ?,"['statistics', 'probability']"
3135534,"Finding $P(B)$ if $A,B$ are independent and given values for $P(A),P(A\cup B)$","I am trying to figure out the math to get to an answer that's given to me right away, which is $1/3$ . The question is asking what the probability of $B$ is with $P(A)=0.4$ and $P(AâˆªB)=0.6$ , with $A,B$ being independent events. I can't seem to come to one-third? Can someone explain it to me like I'm a three year old, since I have been on this for about a few hours? Thanks, and much appreciated! Below is the setup I keep using: $$P(AâˆªB)=P(A)+P(B)-P(Aâˆ©B)$$ which, when substituting in the given values and letting $P(B) = x$ , yields $$0.6 = 0.4 + x - P(A \cap B)$$ But I don't know what $P(A \cap B)$ is, I guess $0.4x$ ?","['statistics', 'probability']"
3135549,Bound for cumulants of bounded random variables,"Let $X$ be a random variable taking values in $[-1,1]$ .  The cumulant generating function is defined as $$
K(t) = \log \mathbb{E} [e^{tX}],
$$ and the cumulants of $X$ are $$
\kappa_n = K^{(n)}(0).
$$ I would like to know the best possible bounds on $\kappa_n$ .   I would be interested in any bound of the form $$
|\kappa_n| \leq C^n
$$ which holds for bounded random variables $X$ .  Does this bound hold? With my limited understanding of cumulants, I would guess that the biggest cumulants would come from the Bernoulli random variable $X$ which takes the values $\pm 1$ equally often.  In this case one has $K(t)=\tanh(t)$ , and the cumulants decay to $0$ as $n\to\infty$ .  I expect that there should be worse examples, but I cannot find them.","['cumulants', 'probability', 'real-analysis']"
3135596,Supremum of arc lengths of graphs of power towers,"Consider the set of all functions of one variable $x\in[0,1]$ that can be constructed from any number of instances of that variable using parentheses and exponentiation only: $$x,\;x^x,\,x^{x^x},\;\left(x^x\right)^x,\;x^{x^{x^x}},\;x^{\left(x^x\right)^x},\;\left(x^x\right)^{x^x},\;\left(\left(x^x\right)^x\right)^x,\; x^{x^{x^{x^x}}},\;x^{x^{\left(x^x\right)^x}},\;...$$ Here are graphs of some functions from this set: Looking at these graphs made me think â€” what is the supremum of arc lengths of these graphs on $[0,1]$ ? Is there a closed form expression for it? Is there an efficient algorithm that can compute it to an arbitrary precision?","['integration', 'arc-length', 'real-analysis', 'power-towers', 'supremum-and-infimum']"
3135603,"Prove that $\lim\limits_{(x,y)\to (0,0)} \frac{\sin(xy)}{\sin(x)\sin(y)}$ exists, using $(\epsilon ,\delta)$-argument","I need to prove that $\lim\limits_{(x,y)\to (0,0)} \frac{\sin(xy)}{\sin(x)\sin(y)}$ exists using the $Ïµ-Î´$ limit definition as $(x,y)â†’(0,0)$ . I know that the limit exist and is equal to $1$ . I worked on it using the squeeze theorem, but we didn't see it in class so I can't use it, the only thing I can use is the $Ïµ-Î´$ definition and I have no idea how to do it.","['limits', 'multivariable-calculus', 'limits-without-lhopital', 'epsilon-delta']"
3135683,Showing the composition of invertible functions $g \circ f$ is invertible with inverse $f^{-1} \circ g^{-1}$,"Let $ f : X \rightarrow Y$ and $g : Y \rightarrow Z$ be invertible functions. Prove that $g \circ f : X \rightarrow Z$ is invertible and that $(g \circ f ) ^{-1} = f^{-1} \circ g^{-1} $ Would this suffice as a proof? $$\begin{align}
(g \circ f) \circ (f^{-1} \circ g^{-1}) &= g \circ ((f \circ f^{-1}) \circ g^{-1})\\
&= g \circ (I_Y \circ g^{-1})\\
&= g \circ g^{-1}\\
&= I_X\\
(f^{-1} \circ g^{-1}) \circ (g \circ f) &= (f^{-1} \circ (g^{-1} \circ g)) \circ f\\
&= (f^{-1} \circ I_Y) \circ f\\
&= f^{-1} \circ f\\
&= I_Z
\end{align}$$","['proof-explanation', 'proof-writing', 'proof-verification', 'discrete-mathematics', 'elementary-set-theory']"
3135684,"Why is using the arc length of a circle, $s$, equivalent to using the angle measure in radians, $\theta$, as the input of the trigonometric functions?","Upon introducing the idea of circular functions my textbook states: We start at the point $(1,0)$ and measure an arc length $s$ along the circle. [...] Let the endpoint of this arc be at the point $(x, y)$ . The circle is the unit circle - it has center at the origin and radius 1 unit (hence the name unit circle ). Recall from algebra that the equation of this circle is: $x^2 + y^2 =1$ The radian measure of $\theta$ is related to the arc length $s$ . For $\theta$ measured in radians, we know that $s=r\theta$ . Here $r=1$ , so $s$ , which is measured in linear units such as inches or centimeters, is equal to $\theta$ , measured in radians. Thus, the trigonometric functions of angle $\theta$ in radians found by choosing a point $(x, y)$ on the unit circle can be rewritten as functions of the arc length $s$ , a real number. When interpreted this way, they are called circular functions. I have several questions regarding this: 1) The text points out that the arc length, $s$ is measured in linear units while the angle measure, $\theta$ , is in radians. Clearly two different units of measurement, so how could the two unit types be used interchangeably as the inputs of the trigonometric functions, where, for example, $\sin(s)= \sin(\theta)$ ? 2) $s=\theta$ is derived from $s=r\theta$ (formula for calculating arc length) and the fact that a unit circle has a radius of $1$ , $r=1$ . However, if the unit circle is not used and the radius is no longer 1, then is the arc length, $s$ , still equal to the angle measure $\theta$ ? How? 3) Why not continue using angle measure in radians as the input for the trig. functions? Why use arc length instead?","['algebra-precalculus', 'trigonometry']"
3135697,Is the set of paths between any two points moving only in units on the plane countable or uncountable?,"Consider 2 arbitrary, fixed points A and B on the plane. Suppose you can move from point A in unit distance at any angle to another point and from this point you can again travel a unit distance at any angle to another point and so on.  Ultimately your goal is to travel from point A to point B along a path of unit-distance line segments without repeating a point during your journey. Is the set of such paths countable or uncountable? I believe it is uncountable and here is my thought process, but I'm not sure of my logic.  Consider the line between the two points equidistant from them; call this line L.  A path of only unit distances can be made between A and any point, P, which lies on L without crossing L(I don't know how to prove this statement but it seems true).  This path can be mirrored on the other side of L to connect B to P.  Thus for any point, P, which lies on L a path can be made from A to B crossing through P halfway through the path.  Since the points on L are uncountable the set of paths between A and B are also uncountable.","['elementary-set-theory', 'geometry']"
3135720,The Zariski closure of a constructible set in Complex Algebraic Geometry,"Let X be an affine variety over $\mathbb{C}$ , and let $Y\subseteq X$ be a constructible set. It is very well-known that the Zariski closure of $Y$ is the same as the closure of $Y$ in the standard Euclidean topology inherited from the inclusion $X\subseteq \mathbb{C}^{n}$ . I would like to know/learn to whom such result is due? I once heard someone say that such result could be due to AndrÃ© Weil or Jean-Pierre Serre. Thanks in advance by any reference.","['complex-geometry', 'algebraic-geometry', 'reference-request']"
3135767,Confused about why the conormal exact sequence is what it is on a scheme,"Consider a composition of morphisms of schemes, $$Z \stackrel{j}{\longrightarrow} X \stackrel{f}{\longrightarrow} Y,$$ where $j: Z \rightarrow X$ is a closed immersion with sheaf of ideals $\mathscr{I}$ . The conormal exact sequence for the sheaves of differentials should read, $$
j^{*}(\mathscr{I} / \mathscr{I}^{2}) \longrightarrow j^{*} \Omega_{X / Y} \longrightarrow \Omega_{Z/Y} \longrightarrow 0.
$$ I am trying to prove this via the corresponding conormal sequence for modules. So suppose the composition of morphisms of schemes above are of affine schemes. Say $X = \text{spec}B$ , $Y = \text{spec}A$ , and $Z = \text{spec}C$ , with $C \simeq B / I$ for some ideal $I$ of $B$ . Then the conormal exact sequence of modules reads, $$
I / I^{2} \longrightarrow \Omega_{B/A} \otimes_{B} C \longrightarrow \Omega_{C/A} \longrightarrow 0,
$$ where this is an exact sequence of $C$ -modules. Now we just have to take the corresponding sequence of quasicoherent $\mathcal{O}_{Z}$ -modules. We are viewing $I/I^{2}$ as a $C$ -module via the isomorphism, $$
I \otimes_{B} C \simeq I \otimes_{B} B / I \simeq I / I^{2}.
$$ But then $I/I^{2}$ is just the $B$ -module $I$ with the change of ring $- \otimes_{B} C$ applied. In that case, the exact sequence of $\mathcal{O}_{Z}$ -modules should just be $$
j^{*}\mathscr{I}  \longrightarrow j^{*} \Omega_{X / Y} \longrightarrow \Omega_{Z/Y} \longrightarrow 0.
$$ So where is my flaw in reasoning? Are $j^{*}\mathscr{I}$ and $j^{*}(\mathscr{I} / \mathscr{I}^{2})$ somehow canonically isomorphic or what?","['coherent-sheaves', 'algebraic-geometry', 'schemes', 'commutative-algebra']"
3135784,Show that $f(x)=\cos(x)$ is Lipschitz continuous function.,I am not sure how to proceed. Should I rewrite $|\cos(x)-\cos(y)|=|2\cos\frac{x+y}{2}\cos\frac{x-y}{2}|$ ?,"['calculus', 'real-analysis']"
3135828,"Can one modify $1 - e^{-\frac{1}{x^2}}$ so that all derivatives at $x=0$ are $1$ and support is $[-1,1]?$","It is well-known that $f(x) = 1 - e^{-\frac{1}{x^2}}$ satisfies $f^{(i)}(0) = 1$ for all natural numbers $i.$ However, its support $\{x\in \mathbb{R}:f(x)\neq 0\} = \mathbb{R}.$ Question: Can one modify $1 - e^{-\frac{1}{x^2}}$ so that all derivatives at $x=0$ are $1$ and support is $[-1,1]?$ I tried $1 - e^{1-\frac{1}{x^2}}$ because it has value $0$ at both $x=-1$ and $x=1.$ However, it is not differentiable at those points.","['derivatives', 'differential-geometry', 'real-analysis']"
3135852,"Prove that $\lim_{x\to 1^{+} } \int_{x}^{x^{3}}\frac{1}{\ln t}\, dt=\ln3$.","Prove that $$\lim_{x\to 1^{+}} \int_{x}^{x^{3}}\frac{1}{\ln t}\, dt=\ln3$$ I have never seen something like this before. I noticed that $\int_{1}^{3} \frac{1}{x}dx=\ln x|_{1}^{3}$ and with the change of variable in the initial integral I obtain, for $x=\ln(t)$ , $\int_{\ln t}^{3\ln t} \frac{1}{\ln(\ln(t))}dt$ and for $t=e^s$ , it's $\int_{s}^{3s} \frac{1}{\ln(s)}ds$ and from here I am unable to obtain the required limit.","['integration', 'limits', 'calculus', 'definite-integrals']"
3135870,"Is it possible to colour the faces of $27$ unit cubes and arrange them to form $3\times3\times3$ cube with all exterior faces red,then white,then blue","Is it possible to colour the faces of $27$ unit cubes so that one can arrange them to form $3\times 3\times 3$ cube with all exterior faces red, then rearrange to form a cube with all exterior faces white, then blue? There have be at least $24$ or $23$ cubes with all three of their faces in one colour... Thereafter trying various possibilities my guess seems to be no,  but I'm not sure and don't know how to prove it.",['combinatorics']
3135884,Proving (without AC) that there is a surjective function from $\mathcal{P}(\omega)$ to $\omega_1$.,"Problem I am working on the following exercise from page 60 of Kunen's Foundations of Mathematics : Prove, without using AC, that one can map $\mathcal{P}(\omega)$ onto $\omega_1$ . In my copy of the text, he provides the following hint: Define $f:\mathcal{P}(\omega\times\omega)\xrightarrow{\text{onto}}\omega_1$ so that $f(R)=\text{type}(R)$ whenever $R$ is a well-order of $\omega$ and $f(R)=|R|$ whenever $R$ is finite. My instructor added to this hint, by saying that $f(R)=0$ (or any other number of your choice) otherwise. Attempt at Solution Ok, so I need to establish that there is a surjective function from $\mathcal{P}(\omega)$ to $\omega_1$ , and the function Kunen provided as a hint is going to help me do this.  With the help of my instructor, I made a rough roadmap for this proof: Ensure $f$ is a well-defined function that does not require the Axiom of Choice. $\checkmark$ Show that $f$ (as given above) is surjective. Establish the fact that $\mathcal{P}(\omega\times\omega)\approx \mathcal{P}(\omega)$ , i.e. there is a bijection $g:\mathcal{P}(\omega)\rightarrow \mathcal{P}(\omega\times\omega)$ . $\checkmark$ Conclude desired result by taking the composition $g\circ f$ , i.e. $g\circ f=h:\mathcal{P}(\omega)\rightarrow \omega_1$ . $\checkmark$ As indicated by the $\checkmark$ 's, I understand all of the steps required of this proof other than showing that $f$ is surjective. I know that $\omega_1$ is the (first) uncountable ordinal containing all countable ordinals.  So to show surjectivity, I need to show that for any ordinal $\alpha\in\omega_1$ we have some $R\in\mathcal{P}(\omega\times\omega)$ such that $f(R)=\alpha$ . My first thought was to think of $\omega_1$ as containing two different types of countable ordinals: countable and finite, and countable and infinite. So if $R$ is a well-order of $\omega$ , then $f(R)=\text{type}(R)=\text{type}(\omega;R)=\alpha$ ,  where $\alpha$ is the unique ordinal such that $(\omega;R)\simeq(\alpha;\in)$ .  I don't think I understand order type very well, but in my head this means that all ordinals $\alpha\geq \omega$ will get ""hit"".  Then the rest of the ordinals $<\omega$ will get hit via $f(R)=|R|$ (or $0$ ?). But this feels wrong... Question Clearly I am struggling with showing this function is surjetive, if the absolute mess of thoughts above wasn't indication enough.  I am wondering if you kind souls would be willing to help me fill in the gaps in my understanding (as it pertains to showing $f$ is surjective), so that I may finally be able to complete this proof. Thank you in advance!","['elementary-set-theory', 'axiom-of-choice', 'ordinals']"
3135902,"Proving that $\|Av\|\geq \lvert \langle u, v\rangle\rvert\cdot \|A\|$ for $\|Au\| = \|A\|$ in a Hilbert space","I've shown that for a matrix $A\in \mathbb{R}^{n\times n}$ and an arbitrary $v\in \mathbb{R}^n$ , we have the inequality $$\|Av\|\geq \lvert \langle u, v\rangle\rvert\cdot \|A\|$$ where $\|\cdot\|$ is the $2$ -norm and $u$ satisfies $\|u\| = 1$ and $\|Au\| = \|A\|$ (the operator norm of $A$ corresponding to the vector $2$ -norm). To do this, I used the singular value decomposition. However, I was wondering if there's a different proof that relies on more general properties of Hilbert spaces (under the assumption that $u$ exists in the more general setting where $\langle \cdot, \cdot\rangle$ and $\|\cdot\|$ are the Hilbert space inner product and norm). My own short proof is below: We first handle the case where $A = \operatorname{diag}(d_1, \ldots, d_n)$ for some $d_1\geq \cdots\geq d_n\geq 0$ . Then, $u = e_1$ , so $$\lvert \langle u, v\rangle\rvert^2\|A\|^2 = d_1^2v_1^2\leq \sum_{i=1}^n d_i^2v_i^2 = \|Av\|^2$$ Now, we consider general $A$ , which has a singular value decomposition $A = U\Sigma V^{\mathrm{T}}$ for $U$ and $V$ orthogonal and $\Sigma$ diagonal in the above form. First, we recall that multiplication by orthogonal matrices preserves the dot product, i.e. $\langle Qx, Qy\rangle = \langle x, y\rangle$ for orthogonal $Q$ . This allows us to write $$\|Av\|^2 = \langle U\Sigma V^{\mathrm{T}}v, U\Sigma V^{\mathrm{T}}v\rangle = \langle \Sigma V^{\mathrm{T}}v, \Sigma V^{\mathrm{T}}v\rangle = \|\Sigma V^{\mathrm{T}}v\|^2$$ for arbitrary $v\in \mathbb{R}$ . Furthermore, as $\|Au\|^2 = \|\Sigma V^{\mathrm{T}}u\|^2$ and $\|V^{\mathrm{T}}u\|^2 = \|u\|^2 = 1$ , we have that $\|A\| = \|\Sigma\|$ and that $y = V^{\mathrm{T}}u$ satisfies $\|\Sigma y\| = \|\Sigma\|$ . Combining our earlier results, $$\|Av\|^2 = \|\Sigma V^{\mathrm{T}}v\|^2\geq \lvert\langle y, V^{\mathrm{T}}v\rangle\rvert^2\|\Sigma\|^2 = \lvert\langle V^{\mathrm{T}}u, V^{\mathrm{T}}v\rangle\rvert^2\|A\|^2 = \lvert\langle u, v\rangle\rvert^2\|A\|^2$$","['hilbert-spaces', 'linear-algebra', 'functional-analysis']"
3135907,"$PSL(2,13)$ has no subgroup of prime index.","I want to show that $PSL(2,13)$ has no subgroup of prime index,where $PSL(2,13) = \frac{SL(2,13)}{\brace-I,I}$ . We have the below fact. ã€ŠIf $G$ be a simple group and $H$ be a subgroup of $G$ such that $|G:H|=n$ , ( $n$ is greater than 1)then $G$ is embeded in $A_n$ .ã€‹ Now I know that $PSL(2,13)$ is simple. Also $|PSL(2,13)|= 2^2Ã—3Ã—7Ã—13.$ If $H$ be a subgroup of $PSL(2,13)$ with prime index $p$ then $p$ can be 2,3,7 or 13. If $p=2$ then $H$ is normal in $PSL(2,13)$ and it is impossible as $PSL(2,13)$ is a simple group. If $p=3$ then according to that fact $PSL(2,13)$ should be embeded in $A_3$ and it is impossible because of the order of the groups. Also if $p=7$ we have the same result as 3. If $p=13$ then $PSL(2,13)$ should be embeded in $A_{13}$ . I don't know how to show that it doesn't happen.","['group-theory', 'simple-groups', 'finite-groups', 'linear-groups']"
3135937,Proof for bijection of a function between positive integers and nonprime positive integers.,"Exercise 4.39 on Concrete Mathematics mentioned a function $S(m)$ : Let $S(m)$ be the smallest positive integer $n$ for which there exists an increasing sequence of integers $$ m = a_1 < a_2 < \cdots < a_t = n$$ such that $a_1a_2...a_t$ is a perfect square. ... We have: $$
\begin{array}{c|cc}
m & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12\\
\hline
S(m) & 1 &6&8&4&10&12&14&15&9&18&22&20
\end{array}$$ This exercise proved that $S(i)â‰ S(j)$ for different $i$ , $j$ and left a remark that ""the sequence $S(1),\ S(2),\ S(3),\ ...$ contains every nonprime positive integer exactly once "" in the appendix ""Answers to Exercises"" but gave no cue or reference to it. I have no idea about it's proof.","['number-theory', 'prime-factorization', 'prime-numbers']"
3135947,General form of parabola using equation of Tangent at Vertex and Axis,I had read in my notes that the equation of parabola can be given by (Equation of axis) $^2$ = (Length of Latus rectum)*(Equation of tangent at vertex) (I don't know the systematic proof. Is there something I am missing in the equation?) Now take look at this very basic equation of a parabola $ y^2=4ax $ Here the equation of axis of parabola is $(y=0)$ and that of tangent at vertex is $(x=0)$ I can also write the equation of axis as $(ny=0)$ and tangent at vertex as $(mx=0)$ (where m and n are constants) And hence using the first equation I can write the equation of parabola as $(ny)^2 = 4a(mx)$ which gives me a completely different parabola. I don't know where I have gone wrong. Please guide me.,"['conic-sections', 'geometry']"
3135950,Central limit theorem for weighted average,"Let $(a_i)_{i\ge1}$ be a bounded positive sequence and $X_i$ be iid random variables with mean $0$ and finite variance. Let $s_n=\frac{\sum_{i=1}^n a_i X_i}{\sqrt{\sum_{i=1}^n a_i}}$ . If $a_i=1$ for all $i$ , then we have the central limit theorem for the limit. Is there anything known for the general case? Do we need to restrict $a_i$ 's more to have a similar limit? PS. I don't use the word converge because I don't want to guess what kind of convergence this may be.","['central-limit-theorem', 'probability-theory', 'random-variables']"
3135982,"Find $n^2+58n$, such that it is a square number","I have the following problem. 
Find all numbers natural n, such that $n^2+58n$ is a square number. My first idea was, $n^2+58n=m^2$ $58n=(m-n)(m+n)$ such that $m-n$ or $m+n$ must be divisible by 29, but this leads to nothing.","['elementary-number-theory', 'square-numbers', 'discrete-mathematics']"
3135987,"Two players throw a die until the sequence $1,2,3$ appears, and the winner is the one who roll $3$. What is the probability the second player wins?","Alameda and Belisario alternate turns throwing a fair die. Alameda plays first and they continue throwing, one at a time, until the sequence $1$ - $2$ - $3$ appears. Whoever throws the $3$ is the winner. What is the probability that Belisario wins? Hmmm - probabilities is not my strong domain!
First let's see what the chances are to get a $1$ - $2$ - $3$ regardless who gets it.
Is it $1$ in $6\cdot 6\cdot 6$ ? Then if this probability is $p$ , my understanding is that the probability for Belisario to win is smaller, but I can't compute it :(","['dice', 'probability']"
3136018,How weakly open imply open?,"When I am seeing the proof of the statement ""Let $C$ be a convex set in a normed linear space $X$ .
Then $C$ is closed if and only if $C$ is weakly closed."" Proof: Suppose that $C$ is weakly closed. Then $C^c$ , the complement of $C$ is weakly open and hence open . Hence $C$ is closed. my problem is in the 2nd line of the proof written in bold line. How weakly open imply open? Thanks.","['functional-analysis', 'analysis']"
3136033,Pulling back $\mathfrak g $-valued 1-forms,"Let $\omega$ be a vector-valued 1-form where the vector space is the Lie algebra $\mathfrak g$ and let $\mathcal P$ be a trivial principal bundle over a manifold $M$ . Thus $$
\omega\in\Omega^1(\mathcal P,\mathfrak g)=\Gamma((\mathcal P \times \mathfrak g)\otimes T^*\mathcal P).\tag1 
$$ I am reading that pulling back $\omega$ via the trivializing section $\sigma:M\to\mathcal P$ gives me a pull-back section $\sigma^*\omega\in\Omega^1(M,\mathfrak g)=\Gamma((M\times\mathfrak g)\otimes T^*M)$ . Namely, $\sigma^*\omega$ is a smooth section of the bundle $(M\times\mathfrak g)\otimes T^*M$ . Since a pull-back section is a section in the pull-back bundle, namely $$
\sigma^*\omega\in\Gamma(\sigma^*((\mathcal P\times\mathfrak g)\otimes T^*\mathcal P)),\tag2
$$ if what I am reading is correct, it follows that $$
\sigma^*((\mathcal P\times\mathfrak g)\otimes T^*\mathcal P)\cong(M\times\mathfrak g)\otimes T^*M.
$$ Moreover, given a fiber bundle $\pi:E\to M$ and a function $f:M'\to M$ , then the pull-back bundle $f^*E$ and the bundle $E$ share same fibers over points $x'\in M'$ and $x=f(x')\in M$ , since it is a special case of a product bundle. Then it follows that $(\mathcal P\times\mathfrak g)\otimes T^*\mathcal P$ must share same fibers with $(M\times\mathfrak g)\otimes T^*M$ $\textbf {Problem}$ : same fibers means $\mathfrak g\otimes T^*P\cong\mathfrak g\otimes T^*M$ , which is of course not true. So what's wrong?","['fiber-bundles', 'differential-forms', 'differential-geometry']"
3136052,Circle problem in which i wanted to find value of $PQ$,"Find a value of $PQ$ : Let the radius of bigger circle be $R$ , and that of the smaller circles be $r_1$ and $r_2$ . Hence we can find value of $R = 10$ .
I don't know how to proceed further to find the Value of $r_1$ and $r_2$ .","['euclidean-geometry', 'geometric-transformation', 'circles', 'geometry']"
3136105,A linear space of degenerate matrices.,"Denote by $M_n(\mathbb{R})$ the set of all $n$ by $n$ real matrices. It's a linear space over $\mathbb{R}$ . Can we find a nontrivial vector subspace in which all the matrices are degenerate? If possible, what is the maximum possible dimension of this subspace? I thought about the set of upper triangular matrices, where all the the numbers at a certain position on the diagonal line of all matrices are always zero. This meets the requirements, but I don't how to deal with the 'maximum' problem. Any hints? Thank you in advance.","['matrices', 'linear-algebra', 'vector-spaces']"
