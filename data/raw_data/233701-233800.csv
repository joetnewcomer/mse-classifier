question_id,title,body,tags
4876185,Simplification infinite series by suppressing sums over $i$ and $j$,"I have the following expression given by $$S_n := \sum_{0 \leq i \leq j \leq n} \frac{1}{i!(n-i)!j!(n-j)!} a^i b^{j-i} c^{n-j}. $$ I wanted to see if I could use the multinomial theorem to simplify it. It is evident that $$\sum_{0 \leq i \leq j \leq n} \frac{1}{i!(n-i)!j!(n-j)!} a^i b^{j-i} c^{n-j} = \sum_{0 \leq i \leq j \leq n} \frac{(j-i)!}{(n-i)!j!} \cdot \frac{1}{i!(j-i)!(n-j)!} a^i b^{j-i} c^{n-j},$$ and, by the multinomial theorem, we have that $$\sum_{0 \leq i \leq j \leq n}  \frac{1}{i!(j-i)!(n-j)!} a^i b^{j-i} c^{n-j} = \frac{1}{n!} (a+b+c)^n.$$ However, I cannot apply it directly because of the coefficients $\frac{(j-i)!}{(n-i)!j!}$ . Does anyone see a way to circumvent this? I spotted an ""almost multinomial type"" expression in $S_n$ , but feel free to utilise other results/techniques besides the multinomial theorem. Ultimately, I'm interested in the sum $$S:= \sum_{n=0}^\infty S_n,$$ which I'd like to write in the form $S = \sum_{n=0}^\infty T_n$ with $T_n$ only depending on $n$ , thus surpassing the intermediate summations over $i$ and $j$ . Any ideas?","['power-series', 'algebra-precalculus', 'combinatorics', 'sequences-and-series']"
4876249,Find the minimum number of conjuncts in the DNF of function $g$,"The Boolean function $g$ of the variables $x_1 , . . . , x_5 , y_1 , . . . , y_5$ , is given by the formula: $$
\bigwedge_{i=1}^{5} (x_i \ \vee \ y_i)
$$ Mission: find the dnf( $g$ ). (the minimum number of conjuncts in the DNF of function $g$ ) Some clarifications: The
smallest number of conjuncts in the DNF representing the function $g$ is denoted by dnf( $g$ ). My thinking: At first, I just expanded this distributivity formula from the task condition and got a disjunction of 32 conjuncts, each conjunct contains a conjunction of 5 boolean variables (some 5 of $x_1, ..., x_5, y_1, ... y_5$ ). In fact, this is the answer to the problem, but I can't prove that there won't be less than 32 conjuncts. I tried to prove the opposite: let's assume that there are fewer than 32 conjuncts, etc., but no contradiction was be deduced by me... Then I tried to consider subtasks when we are given not 10 boolean variables, but for example 4 boolean variables: $(x_1, x_2, y_1, y_2)$ . And then you can see that $3^2=9$ combinations of input data $(x_1, x_2, y_1, y_2)$ will give the value 1 for the function $g$ , the total values of the function $g$ from 4 variables takes $2^4=16$ . Then we can essentially consider a Boolean cube with 16 vertices (vertex = $(x_1,x_2,y_1,y_2)$ ), and dnf( $g$ ) will be equal to the minimum number of faces from the Boolean cube that cover all vertices $(x_1,x_2,y_1,y_2)$ from whose coordinates the function $g$ takes the value 1, and in fact it will be $2^2 = 4$ faces that cover all 9 vertices on which the function takes the value 1. (all this is very easy to get by drawing a 4-dimensional Boolean cube on paper). But I don't quite understand how to improve this idea for 10 variables already, by induction doing this is probably a very dubious idea. Do you have any suggestions on how to solve this problem with this approach?","['boolean', 'boolean-algebra', 'discrete-mathematics']"
4876258,"Find all $f:\Bbb R\to\Bbb R$ st for any $x,y\in\mathbb R$, the multiset $\{(f(xf(y)+1),f(yf(x)-1)\}$ equals the multiset $\{xf(f(y))+1,yf(f(x))-1\}$.","Find all functions $f: \mathbb R \to \mathbb R$ such that for any $x,y \in \mathbb R$ , the multiset $\{(f(xf(y)+1),f(yf(x)-1)\}$ is identical to the multiset $\{xf(f(y))+1,yf(f(x))-1\}$ . Note: The multiset $\{a,b\}$ is identical to the multiset $\{c,d\}$ if and only if $a=c,b=d$ or $a=d,b=c$ . My idea Let's consider the given functional equation $$\{(f(xf(y)+1), f(yf(x)-1)\} = \{xf(f(y))+1, yf(f(x))-1\}$$ From this, we can derive that $$f(xf(y)+1) = xf(f(y))+1$$ and $f(yf(x)-1) = yf(f(x))-1$ for all $x, y \in \mathbb{R}$ . now. We analyze each part: $f(xf(y)+1) = xf(f(y))+1$ $f(yf(x)-1) = yf(f(x))-1$","['contest-math', 'functional-equations', 'analysis', 'real-analysis', 'functions']"
4876263,Inequality $\frac{1}{4}(e^{-4} + e^{-1}) \leq \int_1^2 e^{-x^2}dx \leq \frac{1}{2}(e^{-4}+e^{-1})$,"Please help me to understand how to prove the inequality \begin{equation}
\frac{1}{4}(e^{-4} + e^{-1}) \leq \int_1^2 e^{-x^2}dx \leq \frac{1}{2}(e^{-4}+e^{-1}). 
\end{equation} Using the mean value theorem we can easily show that \begin{equation}
e^{-4} \leq \int_1^2 e^{-x^2}dx \leq e^{-1}.
\end{equation} But I completely don't understand how to obtain main inequality.","['integration', 'calculus', 'definite-integrals']"
4876293,"Prove that if $\{a_{k}\}$ is a sequence of real numbers such that $\sum_{k=1}^{\infty} \frac{|a_{k}|}{k} = \infty$,","Prove that if $\{a_{k}\}$ is a sequence of real numbers such that $$\sum_{k=1}^{\infty} \frac{|a_{k}|}{k} = \infty$$ and $$\sum_{n=1}^{\infty} \left( \sum_{k=2^{n-1}}^{2^n-1} k(a_k - a_{k+1})^2 \right)^{1/2} < \infty,$$ then $$\int_{0}^{\pi} \left| \sum_{k=1}^{\infty} a_k \sin(kx) \right| \,dx = \infty.$$ My idea to prove
Although the condition $$\lim_{k \rightarrow \infty} a_k=0$$ does not appear in the statement of the problem, by the well-known Cantor-Lebesgue theorem, this follows from the fact that the series $$\sum_{k=1}^{\infty} a_k \sin k x$$ is convergent almost everywhere. We note that for the application of this theorem, it would be sufficient if the series (2) were convergent on a set of positive measure. To make reference easier, we list the remaining conditions: $$\sum_{k=1}^{\infty} \frac{\left|a_k\right|}{k}=\infty$$ $$\sum_{n=1}^{\infty}\left(\sum_{k=2^{n-1}}^{2^n-1} k\left|\Delta a_k\right|^2\right)^{1 / 2}<\infty$$ where $$\Delta a_k:=a_k-a_{k+1} \quad(k=1,2, \ldots) .$$ From (4) it follows that the sequence $\left\{a_k\right\}$ has bounded variation, that is, $$\sum_{k=1}^{\infty}\left|\Delta a_k\right|<\infty$$ Really, by the Cauchy inequality, \begin{aligned}
\sum_{k=1}^{\infty}\left|\Delta a_k\right| & =\sum_{n=1}^{\infty} \sum_{k=2^{n-1}}^{2^n-1}\left|\Delta a_k\right| \\
& \leq \sum_{n=1}^{\infty}\left(2^{n-1} \sum_{k=2^{n-1}}^{2^n-1}\left|\Delta a_k\right|^2\right)^{1 / 2} \\
& \leq \sum_{n=1}^{\infty}\left(\sum_{k=2^{n-1}}^{2^n-1} k\left|\Delta a_k\right|^2\right)^{1 / 2}
\end{aligned} Consider the $n$ th partial sum of (2). By Abel's rearrangement, we obtain $$
\sum_{k=1}^n a_k \sin k x = \sum_{k=1}^n \tilde{D}_k(x) \Delta a_k + a_{n+1} \tilde{D}_n(x)
$$ where $\tilde{D}_n(x)$ is the conjugate Dirichlet kernel: \begin{align*}
\tilde{D}_n(x) &:= \sum_{k=1}^n \sin k x \\
&= \frac{\cos \frac{x}{2} - \cos \left(n + \frac{1}{2}\right) x}{2 \sin \frac{x}{2}} \quad (n=1,2, \ldots) .
\end{align*} Introduce the notation $$\bar{D}_n(x):=-\frac{\cos \left(n+\frac{1}{2}\right) x}{2 \sin \frac{x}{2}} \quad(n=0,1, \ldots) .$$ Then $$\tilde{D}_n(x)=\bar{D}_n(x)-\bar{D}_0(x) \quad\left(n=0,1, \ldots ; \tilde{D}_0(x)=0\right),$$","['fourier-series', 'convergence-divergence', 'bounded-variation', 'real-analysis']"
4876388,What’s the difference between an ordered pair and an ordered set?,"I am told relations are (sets of?) ordered pairs. How do these relate to ordered sets? Are ordered sets ordered by ordered pairs (i.e., relations)?","['elementary-set-theory', 'logic']"
4876455,An exact definition of multiplication,"I am looking into repeated operations, and it seems really hard to precisely define multiplication. Of course, for integer $b$ and real number $a$ , we use the grade school definition we all know: $$ab = \underbrace{a + a + a + \cdots + a}_{b\text{ times}}$$ but what about for real numbers $a$ and $b$ ? For exponentiating (for integers: repeated multiplication), we have a precise formula to define it, which is easy to derive: $$a^x = \sum_{n=0}^{\infty} \frac{x^n \left(\ln(a)\right)^n}{n!}$$ which is nice because we only have integer powers in the sum, which we already know how to define: $$x^n = \underbrace{x \times x\times x \times \cdots \times x}_{n\text{ times}}$$ But this just raises the question of how we define $x \times x$ precisely. Is there an analogous formula to this for multiplication? How does the calculator compute multiplication of reals? Note: According to sources, just approximating multiplication for real numbers uses calculus or numerical methods. I cannot grasp why we need these advanced concepts to precisely define this fundamental operation, especially when comparing it to the simple formula for exponentiation. But I still don’t have a formula yet.","['real-numbers', 'definition', 'arithmetic', 'real-analysis']"
4876459,"Irreducible $p(x) \in \mathbb{Q}[X]$ with roots $r, s$ such that $rs = 1$.","If $p(x) \in \mathbb{Q}[X]$ is irreducible and has two roots $r,s$ such that $rs = 1$ , then $p(x)$ is of even degree. I'm not sure how to solve this problem. My initial idea was to consider the field tower $\mathbb{Q}(r, r+s) \supseteq \mathbb{Q}(r+s) \supseteq \mathbb{Q}$ . We know that $\mathbb{Q}(r, r+s) = \mathbb{Q}(r)$ , since $r+s \in \mathbb{Q}(r)$ . Then, $$\operatorname{deg}p(x) = [\mathbb{Q}(r): \mathbb{Q}] = [\mathbb{Q}(r+s)(r): \mathbb{Q}(r+s)][\mathbb{Q}(r+s): \mathbb{Q}].$$ Note that $x^2 - (r+s)x + 1 \in \mathbb{Q}(r+s)[X]$ and has roots $r$ and $s$ . If we could show that this polynomial is irreducible in $\mathbb{Q}(r+s)[X]$ or, equivalently, that $r \notin \mathbb{Q}(r+s)$ , the extension $\mathbb{Q}(r, r+s)/\mathbb{Q}(r+s)$ would have degree $2$ and, therefore, $\operatorname{deg}p(x) = 2k$ for some $k \in \mathbb{Z}_{+}$ . However, I wasn't able to come up with a proof of this. Is this the correct approach?","['irreducible-polynomials', 'roots', 'field-theory', 'abstract-algebra', 'polynomials']"
4876464,How to solve $f(x)=\frac{1}{2} f(3x) + \cos x$?,"Using the Banach fixed-point theorem, I can prove that there exists exactly one continuous function $f:[0,1]→\mathbb{R}$ that satisfies $$f(x) = \frac{1}{2}f(3x)+\cos x.$$ I currently don't know how to solve this $f$ precisely. My attempt is similar to Picard iteration. Define $$Tf = \frac{1}{2}f(3x)+\cos x.$$ Let $f_0 = 2$ , and $f_{n+1} = Tf_n$ . I can calculate that $$f_n = \frac{1}{2^{n-1}}\cos 3^{n-1}x + \cdots + \frac{1}{2}\cos 3x+ \cos x + \frac{1}{2^{n-1}}.$$ But what is the limit of this $f_n$ ? Thank you for any solutions or hints to this problem. Edit: Adding the plots of $f_1,f_3$ and $f_5$ in the interval $x\in[-\pi,\pi]$ , JL","['functional-equations', 'calculus', 'analysis', 'real-analysis']"
4876496,Who named the first fundamental form?,"This is a historic question for which I couldn't find an answer on google or on the math history books I have at hand. The first fundamental form $I$ of a surface $S$ embedded in $\mathbb{R}^3$ is, indeed, a differential form: a choice of bilinear map $I_x\colon T_xS \times T_xS \to \mathbb{R}$ for the tangent space of $x \in S$ that is ""differentiable"" in $x$ in the suitable sense. However, since metric considerations about surfaces goes way earlier than the language of differential forms (say, Gauss' Theorema Egregium dates from 1827, Cartan's paper on differential forms is from 1899), I assume either the first fundamental form didn't have a name, or was called differently, or ""form"" meant something else back then. In the 1902 Princeton translation of Gauss 1827 paper, what we now understand as the first fundamental form is referred to as the ""line element"", as Gauss manipulates the expression $\sqrt{Edp^2 + 2Fdpdq + Gdq^2}$ in the notation therein. Who first named the first fundamental form?","['math-history', 'terminology', 'differential-geometry']"
4876584,Commutator formula between Hessian and Laplacian of a scalar function,"I am looking to derive an identity I found which commutes the Hessian and the Laplacian of a scalar function $f \in C^4(M)$ on a Riemannian manifold $(M,g).$ $$\Delta \nabla_i \nabla_j f = \nabla_i \nabla_j \Delta f + (R_{jp}g_{ik} + R_{ip} g_{jk} - 2R_{kipj}) \nabla_k \nabla_p f + (\nabla_i R_{jp} + \nabla_j R_{pi} - \nabla_p R_{ij}) \nabla^p f.
$$ One can note you have the Laplacian of the Hessian on the LHS of the equality, and the Hessian of the Laplacian on the RHS + some extra terms. This formula It can be found in page 28 of these Lecture Notes: https://www.math.uci.edu/~jviaclov/courses/865_F07.html The problem with understanding the derivation he does there is the notation, as I am more familiar with index notation, so that the Laplacian of a scalar function for example is expressed as $\Delta f = g^{ik}u_{,ik}$ and the Hessian is $u_{,ij}.$ First, $\Delta \nabla_i \nabla_j f$ is the Laplacian of the Hessian, so that in index notation it would be $g^{kl}(f_{,ij})_{,kl},$ whilst he writes $g^{kl}\nabla_k \nabla_l \nabla_i \nabla_j f.$ From this I understand that he is doing $g^{kl}\nabla_k \nabla_l (\nabla_i \nabla_j f)$ and he operates from the left. But then he does $g^{kl}\nabla_k \nabla_l \nabla_i \nabla_j f = g^{kl} \nabla_k (\nabla_l \nabla_i \nabla_j f),$ which would mean $g^{kl}(f_{,lij})_{,k}$ but this does not seem to work well in index notation and I can no longer follow through his steps. The following question helps me but I would need two covariant derivatives instead of one: Commutator of laplacian and covariant derivative of a tensor","['index-notation', 'riemannian-geometry', 'differential-geometry']"
4876600,Theorem about coupling and independence of random variables,"I am reading a book of E. Rio and I found there a theorem ( without a proof ) about coupling. Please see below. Theorem: Let $(\xi_i)_{i \in \mathbb{Z}}$ be a sequence of random variables with values in some Polish space $X$ . Assume that $(\Omega, \mathcal{T}, \mathbb{P})$ is rich enough to contain a random variable $U$ with uniform distribution over $[0, 1]$ , independent of $(\xi_i)_{i \in \mathbb{Z}}$ . Let $\mathcal{F_0} = \sigma(\xi_i: i \le 0)$ and $\mathcal{G}_n = \sigma(\xi_i: i \ge n)$ . Then one can construct a sequence $(\xi^*_i)_{i \in \mathbb{Z}}$ with the same joint distribution as the initial sequence $(\xi_i )_{i \in \mathbb{Z}}$ , independent of $\mathcal{F}_0$ and measurable with respect to the $\sigma$ -field generated by $U$ and $(\xi^*_i)_{i \in \mathbb{Z}}$ , in such a way that, for any positive integer $n$ , $$\mathbb{P}(\xi_k \neq \xi^*_k \text{for some } k \ge n \mid \mathcal{F_0}) = \text{esssup}(|\mathbb{P}(B \mid \mathcal{F_0})−\mathbb{P}(B)| \colon B \in \mathcal{G}_n).$$ Question 1: I am wondering what are the conditions these theorem to hold? Is my understanding correct that the only condition that $\mathcal{X} = (\Omega, \mathcal{T}, \mathbb{P})$ should be reach enough to contain a uniform r.v. independent of $(\xi_i)_{i \in \mathbb{Z}}$ ? Question 2: Could anyone provide some intuition about this condition (i.e., what does it mean, why is it crucial here, why uniform)? I would be grateful for any example where this condition doesn't hold.","['stochastic-processes', 'coupling', 'independence', 'probability-theory']"
4876640,Multidimensional Legendre polynomials?,"Legendre polynomials can be given by several expressions, but perhaps the most compact way to represent them is by Rodrigues' formula as $$P_n(x) = \frac{1}{2^n n!} \frac{d^n}{dx^n} (x^2 - 1)^n.$$ I was wondering if there exists a multidimensional variant of the Legendre polynomials, e.g. an analogous $P_n(x,y,z)$ for instance. Does anyone know anything about this? I can't seem to find anything about it.","['multivariable-calculus', 'calculus', 'polynomials', 'legendre-polynomials']"
4876670,"$f(x,y)= Ax +By +Cxy +D$. if $f(x,y)=\frac 1{xy}$ for $x,y \in \{3,4\}$, then $f(5,5)=?$","Let $f(x,y)= Ax +By +Cxy +D$ . if $f(x,y)=\frac 1{xy}$ for $x,y \in \{3,4\}$ , then what is the value of $f(5,5)$ ? Here in this question I tried substituting $y=x$ thus giving : $f(x,x) = \frac {1}{x^2}$ for $x \in \{3,4\}$ where $f(x,x)$ is a polynomial in $x$ of degree 2. So $x^2f(x,x)$ should be a polynomial of degree 4. $\implies$ $x^2f(x,x)-1 = k(x-3)^2(x-4)^2$ Putting $x=0$ in the eqn gives : $k=\frac {-1}{144}$ Putting $x=5$ in eqn gives : $f(5,5) = \frac 7{180}$ But the answer given is : $f(5,5) = \frac1{36}$ I cant find the mistake so it will be good if someone points out the mistake and provides a correct solution too for the question.","['functions', 'polynomials', 'linear-transformations']"
4876725,Defining a measure on a group from a measure on the equivalence classes,"Let $G$ be a group, and a $H$ a subgroup, and for $a,b \in G$ let $a\sim b$ if $aH = bH$ . Suppose I have a measure $\mu$ on $G/\sim$ , the left cosets of $H$ , and suppose that $G$ is equipped with a sigma algebra $\Sigma_G$ that is closed under right multiplication by $H$ and whose projection is contained in the sigma algebra of $G/\sim$ . How can I define a measure $\nu$ on $G$ that satisfies $$\nu(AH) = \mu(\pi_\sim(A)),\qquad A \in \Sigma_G,$$ where $\pi_\sim : G \to G/\sim$ is the projection map and $AH = \{ ah : a\in A, h\in H \}$ ? In my application, $G = SO(d)$ and $H = \{h\in G : hv=v\}$ for some fixed $v\in S^{d-1}$ . The left cosets of $H$ are isomorphic to $S^{d-1}$ , on which $\mu$ is defined.","['equivalence-relations', 'group-theory', 'measure-theory']"
4876732,How change an non-equivalency relation to equivalency relation?,"I am an engineer, so maybe this question is naive. I study equivalence relations and equivalence classes. An equivalence relation is a binary relation that is reflexive, symmetric, and transitive.  Any number is equal to itself (reflexive). If a = b, then b = a (symmetric). If a = b, and b = c, then a=c (transitive). The relation that does not satisfy these conditions is not an equivalence relation. For example, orthogonality is a non-equivalency relation on a set of lines in $\ R^2$ . Because it is not reflexive and transitive. I want to know: is there any maneuver or algorithm in math that changes a non-equivalency relation to an equivalency relation? For instance, by limitation in space set or adding some conditions.","['elementary-set-theory', 'equivalence-relations']"
4876755,Stochastic average of a differential equation is not the same as average of its solutions,"Assume a static (time-independent) random variable $r$ for which we know its probability distribution $P(r)$ . Consider this to be a Gaussian distribution, such that $\langle r \rangle=0$ and $\langle r^2 \rangle=r_0$ , etc. Consider now the following differential equation: $\frac{d}{dt}X(t)=r A X(t)$ , where $X=(x_1,x_2)^T$ and $A$ is a time-independent $2\times 2$ matrix. I am interested in taking the average of the differential equation over the random variable. If I naively just take the expected value of the previous differential equation, the problem is that I would just get $\frac{d}{dt}X=0$ because $\langle r \rangle=0$ . This is not true, as $X$ is a functional of r, $X(r,t)$ , so $\langle r X(r,t) \rangle\neq0$ . One possible solution would be to formally solve the differential equation as $X(t)=e^{rAt}X(0)$ and then take the average. I would like to avoid this method, as it is not always aplicable. What would be the correct way to proceed if I would like to get an averaged differential equation that properly reproduces the results of averaging the solution? Note: I know about Novikov's theorem which is sometimes used to solve such stochastic differential equations using two-time correlation functions. However, as far as I am aware, it only works if the stochastic variable is dynamical, i.e. r(t) is sampled by the solution of a stochastic differential equation. In my case, my stochastic variable is static, so only the probability distribution is known.","['stochastic-processes', 'average', 'stochastic-differential-equations', 'ordinary-differential-equations']"
4876756,Why do we turn $|\tan^{-1}\theta|$ to $-\tan^{-1}θ$?,"In this question, why did we turn $|\tan^{-1}\theta|$ to $-\tan^{-1}θ$ in the highlighted step? Why is there a negative sign? I first thought that the negative sign was because $\frac\pi2 < \theta < \pi$ , so $\tan\theta$ is negative on this domain. But if $\tan\theta$ is negative, why do we put a (-) sign and make it positive? in a similar question [below], we allowed it to be +ve and that seems correct, but i don't understand why in the first question we need to add (-) sign?","['inverse-trigonometric-functions', 'trigonometry']"
4876772,What are the applications of stochastic differential equations to number theory?,"This semester i'm taking a course about stochastic differential equations. This made me wonder what applications does this topic have to areas like number theory and algebraic geometry, specially arithmetic geometry. Unfortunately i wasn't able to find anything online, all i found was about applications to usual differential equations to number theory in this question.","['ordinary-differential-equations', 'number-theory', 'reference-request', 'stochastic-differential-equations', 'partial-differential-equations']"
4876853,Gap in my proof of $f$ is injective $\implies f^{-1}(f(S))=S$.,"This question is about a gap in a proof I'm writing. I understand intuitively what the gap is - but can't find the correct or standard phrasing. Context The exercise is as follows (edited from ex 3.4.5 Terence Tao Analysis I 4th ed): Let $f : X \to Y$ be a function from one set $X$ to another set $Y$ .
Show that $f^{-1}(f(S))=S$ for every $S \subseteq X$ if and only if $f$ is injective. To do this we need to prove both: $f^{-1}(f(S))=S \implies f$ is injective. $f$ is injective $\implies f^{-1}(f(S))$ . The gap I am asking about is in the proof of the second bullet point only. My Incomplete Proof Let's show $f$ is injective $\implies f^{-1}(f(S))=S$ . To do this we need to show that under the assumption $f$ is injective, both of the following are true: $f^{-1}(f(S)) \subseteq S$ $S \subseteq f^{-1}(f(S))$ Part one: Let's start with the first $f^{-1}(f(S)) \subseteq S$ . By definition of inverse images, $f^{-1}(f(S))$ is the set $\{x \in X: f(x) \in f(S)\}$ . If $x$ is a member of this set then $f(x) \in f(S)$ . ⋮ ⋮ (gap) ⋮ We have shown $x \in f^{-1}(f(S)) \implies x \in S$ . That is, $f^{-1}(f(S)) \subseteq S$ . Part two: Now let's consider $S \subseteq f^{-1}(f(S))$ . By definition of inverse images, $f^{-1}(f(S))$ is the set $\{x \in X: f(x) \in f(S)\}$ . If $x \in S$ , then $f(x) \in f(S)$ , and so $x$ meets the membership conditions of this set. That is, $x \in S \implies x \in f^{-1}(f(S))$ . This is equivalent to $S \subseteq f^{-1}(f(S))$ . Note that we don't need $f$ to be injective for this. Conclusion: By showing both $f^{-1}(f(S)) \subseteq S$ and $S \subseteq f^{-1}(f(S))$ under the assumption $f$ is injective, we have proven $f$ is injective $\implies f^{-1}(f(S))=S$ . $\square$ My Attempt to Complete the Gap By definition of inverse images, $f^{-1}(f(S))$ is the set $\{x \in X: f(x) \in f(S)\}$ . If $x$ is a member of this set then $f(x) \in f(S)$ . For a given $y \in f(S)$ , it must be the case that there is at least one $x \in X$ such that $f(x) = y$ . (why is this? we've not said $f$ is surjective. is it because f(S) is an image not just a co-domain?) Without loss of generality, consider $f(x_1) = y$ and $f(x_2) = y$ , where $x_1 \in S$ and $x_2 \in X$ . Because we have assumed $f$ is injective, by its definition, we have $f(x_1) = f(x_2) \implies x_1 = x_2$ . There are two cases for $x = x_1 = x_2$ . $x \in S$ $x \in X \setminus S$ We know that $x \in S$ because $f(x) \in f(S)$ . (I'm not sure about this, what's the principle or axion or definition?) Thus, $x \notin X \setminus S$ . Thus we have shown $x \in f^{-1}(f(S)) \implies x \in S$ . That is, $f^{-1}(f(S)) \subseteq S$ .","['elementary-set-theory', 'functions', 'solution-verification']"
4876871,Some very basic problems in understanding the definition of algebraic variety,"I just started learning myself some basic algebraic geometry and I have some trouble doing these rather elementary exercises. I am basically misunderstanding something fundamental so it's causing me trouble to progress further. Some definitions: $X\subset k^n$ is an affine (algebraic) variety if $X=\mathbb{V}(I)$ for some ideal $I\subset R,$ where $\begin{aligned}\mathbb{V}(I)=\{a\in k^n\mid f(a)=0\text{ for all }f\in I\}\end{aligned}$ . Exercises : $(1)\:I\subset J\Rightarrow\mathbb{V}(I)\supset\mathbb{V}(J).\:($ “The more equations you impose, the smaller the solution set”.) $(2)\quad\mathbb{V}(I)\cup\mathbb{V}(J)=\mathbb{V}(I\cdot J)=\mathbb{V}(I\cap J).$ (3) $\mathbb{V}(I)\cap\mathbb{V}(J)=\mathbb{V}(I+J).$ (Note: $\langle I\cup J\rangle=I+J.)$ (4) $\mathbb{V}(I),\mathbb{V}(J)$ are disjoint if and only if $I,J$ are relatively prime (i.e. $I+J=\langle1\rangle)$ My thought processes: I don't think I understand why it's $\mathbb{V}(I)\supset\mathbb{V}(J),$ rather than $\mathbb{V}(I)\subset\mathbb{V}(J).$ The union of these two sets represents the set of points where at least one polynomial in either $I$ or $J$ vanishes. I don't see how's it equal to the product of two ideals and the intersection of them. Not sure why's that. If I had to guess, I'd likely say that $\mathbb{V}(I)\cap\mathbb{V}(J)=\mathbb{V}(I\cdot J),$ but that's probably wrong.
3 If $\mathbb{V}(I)$ and $\mathbb{V}(J)$ are disjoint, it means there are no common points between them. Common points between $\mathbb{V}(I)$ and $\mathbb{V}(J)$ correspond to solutions of polynomials in both $I$ and $J$ . If $\mathbb{V}(I)$ and $\mathbb{V}(J)$ are disjoint, there are no common solutions, which implies that $I + J$ contains only constant polynomials (since there are no common solutions, $I + J$ cannot generate non-constant polynomials). Therefore, $I + J = \langle 1 \rangle$ , as any non-constant polynomial would introduce common solutions. Can someone help me understand this better or at least guide me in the right direction? Thanks.","['algebraic-geometry', 'abstract-algebra', 'ideals']"
4876884,Hausdorff property of Grassmannian,"Good evening to everyone. I am new to Manifold Theory, so I am trying the last weeks to study some chapters from the book of John M. Lee 's Introduction to Smooth Manifolds . I was trying to understand this evening the concept of the Grassmannian manifold, which he introduced on pages 23–24 in the book. More specifically, there he was trying to endow the space of $k$ -dimensional subspaces of a $n$ -dimensional space $V$ (obviously $k \leq n$ ) with a smooth structure using a preceding Lemma he had mentioned earlier in the book. I understood most of the part, but I was stuck with the proof of the Hausdorff property of the induced topology (which is the last condition remaining for the application of his Lemma).So, he mentions the following paragraph: Hausdorff condition (v) is easily verified by noting that for any two $k$ -dimensional subspaces $P, P' \leq V$ , it is possible to find a subspace $Q$ of dimension $n-k$ whose intersections with both $P$ and $P'$ are trivial. I do not understand this statement. If we suppose that $\dim V = 5$ and $P = \operatorname{span}\{e_1, e_2, e_3\}$ and $P' = \operatorname{span} \{e_3, e_4, e_5\}$ ,
how do we find a subspace $Q$ of dimension $2$ that has trivial intersections with both $P, P'$ ? Thank you for your help.","['smooth-manifolds', 'grassmannian', 'linear-algebra', 'differential-topology', 'differential-geometry']"
4876996,How to design this two sample exponential distribution test?,"Given independent random samples $(X_1,...,X_m)$ and $(Y_1,...,Y_n)$ , respectively, from the following distributions of $X$ and $Y: X \sim \lambda_1 \exp (-\lambda_1 x) I(x >0)$ , and $Y \sim \lambda_2 \exp (-\lambda_2 y) I(y >0)$ . Consider the problem of testing the null hypothesis $H_0:\lambda_1=\lambda_2 $ against the alternative: $H_1:\lambda_1 \neq \lambda_2 $ . a. Formulate the underlying testing problem as that of testing one of the parameters in a multiparameter exponential family, having expressed the family explicitly in terms of all the parameters and the corresponding statistics. b. Give the formula of the UMPU test at level $\alpha$ , in its conditional form, involving these statistics. c. Describe how this test can be stated equivalently as an unconditional test. What are the ultimate test statistic and the rejection region (in terms of a known distribution)? Some examples of the similar question: Say $X$ and $Y$ are independent. $X \sim Bin(m, p_1), Y\sim Bin(n , p_2)$ .We want to test $$H_0: p_1=p_2 \text{against} H_1: p_1>p_2,$$ Then $(X,Y)\sim f_{\theta_1,\theta_2} (x,y)=e^{\theta_1 T_1 + \theta_2 T_2-A(\theta_1,\theta_2 )}h(x,y)$ with $\theta_1=\log \frac{p_1 (1-p_2)}{p_2 (1-p_1)}, T_1=X, \theta_2=\log \frac{p_2}{1-p_2}, T_2=X+Y$ $$H_0: \theta_1=0 \text{ against } H_1: \theta_1>0.$$ The UMPU test $ \phi_0 (T_1,T_2) = \begin{cases} 1 ,  \text{if} T_1 > c(T_2) \\
      \psi(T_2) ,  \text{if} T_1=c(T_2) \\
0, \text{if} T_1<c(T_2) \end{cases}$ where $c(T_2)$ and $\psi(T_2)$ are such that $E_{\theta_1=0} (\phi_0 (T_1, T_2) | T_2)=\alpha$ . Since the conditional distribution $T_1=X$ given $T_2=X+Y=t_2$ is $$P(X=x | X+Y=t_2)=\frac{{m\choose x}{n \choose t_2-x} }{m+n \choose t_2}$$ $c(t_2)$ and $\psi(t_2)$ are such that $$\sum\limits_{x > c(t_2)}  {m\choose x} {n \choose t_2-x}+ \psi(t_2) {m \choose c(t_2) } {n \choose t_2-c(t_2)}={m+n \choose t_2}\alpha$$","['statistics', 'hypothesis-testing']"
4877016,Exterior derivative for non-alternating forms,"Given a manifold $M$ , a differential $k$ -form $\omega$ assigns to each point $p$ $\epsilon$ $M$ a $k$ -covector $\omega_p$ $\epsilon$ $\bigwedge^k \left(T_p^*M\right)$ , where $\bigwedge^k \left(T_p^*M\right)$ is the space of alternating $k$ -tensors on the tangent space $T_pM$ . If $\omega = \sum f_i dx^i$ denotes a 1-form of $M$ , then the exterior derivative $d:\bigwedge^k \left(T_p^*M\right) \rightarrow \bigwedge^{k+1} \left(T_p^*M\right)$ acts on $\omega$ to give a 2-form: $dw = \sum \frac{\partial f_i}{\partial x^j} dx^j \wedge dx^i$ . I wonder if there is an analogous construction of ""symmetric"" $k$ -forms. By a symmetric $k$ -form I mean a map $\sigma$ that assigns to each point $p$ $\epsilon$ $M$ a $k$ -covector $\sigma_p$ that is a symmetric $k$ -tensor of the tangent space $T_pM$ . If $\sigma = \sum g_i dx^i$ , maybe the exterior derivative of this symmetric 1-form could be $d\sigma = \frac{\partial g_i}{\partial x^j} dx^j \vee dx^i$ , where $dx^j \vee dx^i$ could be analogous to the wedge product with the property $dx^j \vee dx^i = dx^i \vee dx^j$ . Do you have any idea if such a construction exists? Also, could there be an analogous to the de Rham complex?","['manifolds', 'algebraic-topology', 'differential-geometry']"
4877110,Characterization of Determinant using multiplicativity and image of diagonal matrices,"I found a question in the very end of this video, which essentially is the following: Let $n$ be some natural number. Suppose there is a function $f : \mathbb{R}^{n \times n} \rightarrow \mathbb{R}$ such that it satisfies the following 2 properties: $f(A\cdot B) = f(A)\cdot f(B),\ \forall A,B \in \mathbb{R}^{n \times n} $ $f(\text{diag}(a_1,a_2,...,a_n)) = \prod_{i=1}^{n}a_i$ , for any diagonal matrix $\text{diag}(a_1,a_2,...,a_n) \in \mathbb{R}^{n \times n}$ whose diagonal entries (from top-left to bottom-right) are $a_1, ... ,a_n$ . Show that $f(A) = det(A) \text{ (determinant of A)} \; \forall A \in \mathbb{R}^{n \times n}$ , i.e., show that the above two properties characterize the determinant function for real square matrices. My solution : Suppose $A$ is not invertible. Then, we can find invertible matrices $P$ and $Q$ such that $Q^{-1} A P = A' $ is diagonal with ones or zeros on its diagonal. As $A$ is not invertible, $A'$ must have atleast one zero in the diagonal. Hence, $f(Q^{-1})f(A)f(P) = f(A') = 0$ , as $A'$ is diagonal with a zero in the diagonal. But, for any invertible matrix $C,$ $\ 1 = f(I) = f(CC^{-1}) = f(C)f(C^{-1}) \Rightarrow f(C) \neq 0$ . So, $f(Q^{-1})\neq 0$ and $f(P) \neq 0$ . Hence $f(A) = 0 = det(A)$ for any non-invertible $A$ . Next, we know that any invertible matrix $X$ can be written as a product of elementary matrices $X=E_1 ... E_n$ , so if we can show that for an elementary matrix $E$ , $f(E) = det(E)$ , then $f(X) = f(E_1)...f(E_n) = det(E_1)...det(E_n) = det(E_1...E_n) = det(X)$ , and hence we would be done. Hence, it suffices to prove that for an elementary matrix $E$ , $f(E) = det(E)$ . [One useful observation : Suppose $Y$ is a diagonalizable matrix, then $\ \exists$ invertible matrix $M$ such that $D = M^{-1} Y M$ is diagonal. Clearly, $f(D) = det(D) = $ product of diagonal entries, for any diagonal matrix $D$ . So, $det(Y) = det(D) = f(D) = f(M^{-1})f(Y)f(M) = f(Y)f(M^{-1}M) = f(Y)f(I) = f(Y)$ . So, $f(Y) = det(Y)$ for any diagonalizable $Y$ .] If $E$ is an elementary matrix corresponding to multiplication of a row by a non-zero constant $c$ , then $E$ would be diagonal with one diagonal entry $c$ and all other diagonal entries being $1$ , and hence $f(E) = c = det(E)$ . If $E$ is an elementary matrix corresponding to swapping row $i$ and row $j$ ( $i\neq j$ ), then E is symmetric, hence diagonalizable by the spectral theorem for real symmetric matrices. Hence, $f(E) = det(E)$ , by above observation. If $E$ is an elementary matrix corresponding to adding $c$ times the ith row to the jth row ( $i\neq j$ ), then it is the square of the matrix $E'$ which corresponds to adding $c/2$ times the ith row to the jth row. So, $f(E) = f(E')^2$ . By taking $Z$ as the matrix which swaps row i and j (which is its own inverse), it can be shown that $Z^{-1} E Z$ = $E^T$ , so $f(E) = f(E^T)$ . As $EE^T$ is symmetric, by the spectral theorem for real symmetric matrices, it is diagonalizable, hence $f(EE^T) = det(EE^T)\Rightarrow f(E)^2 = det(E)^2 = 1^2 = 1.$ As $E'$ as defined before is an elementary matrix of the same type as $E$ , $f(E')^2 = 1$ as well. So, $f(E) = f(E')^2 = 1 = det(E)$ . This finishes the proof for real square matrices. Note that in the above proof, I never used the fact that the field is the field of real numbers, except for the part where I used the spectral theorem to conclude that a real symmetric matrix is diagonalizable. But this is not true in every field ( Is symmetric matrix over a field F always diagonalizable? ). So my question is as follows : Can we give a proof which works in every field, or can we find a field where the two above mentioned properties do not characterize the determinant? I would be very grateful for any help. Geometric interpretation (along similar lines of the video from which I picked this question) would be highly appreciated. Note : For a field with just 2 elements (0 and 1), like $\mathbb{Z}/2\mathbb{Z}$ , the solution is trivial, because for a non-invertible $A$ , $f(A)=0=det(A)$ , and for invertible $B$ , $f(B)$ and $det(B)$ are each non-zero, so they must each be $=1$ . And hence, we are done in this case.","['matrices', 'determinant', 'linear-algebra']"
4877139,Are two subgroups of prime index with a large intersection necessarily conjugate?,"Assume that $G$ is a finite group, $p$ a prime number, and $H_1,H_2\le G$ such that $[G:H_1]=p=[G:H_2]$ . Assume further that $[G:H_1\cap H_2]<p^2$ . Does it follow that $H_2=gH_1g^{-1}$ for some $g\in G$ ? As an alternative goal: if the main ""guess"" fails, will it still follow that $H_1$ and $H_2$ share the same core in $G$ (in other words: $\bigcap_{g\in G}gH_1g^{-1}=\bigcap_{g\in G}gH_2g^{-1}$ ). Either a proof or a counterexample is welcome! My preliminary thoughts: It is impossible to have $H_1\unlhd G$ , as then we would have $G=H_1H_2$ , and the parallelogram law would imply $[G:H_1]=[H_2:H_1\cap H_2]$ . Obviously the same holds for $H_2$ as well, so the two subgroups are self-normalizing in $G$ . The examples I can think of support this. For example, $H_1$ and $H_2$ can be point stabilizers of (the natural action of) $S_p$ , and as the action of $G$ is transitive, they are always conjugate. The intersection $H_1\cap H_2$ has index $p(p-1)<p^2$ . The exact same thing happens, when $G$ is the holomorph $C_p\rtimes C_{p-1}$ ( $C_{p-1}$ identified with $Aut(C_p)$ ). Another example is the five Sylow $2$ -subgroups $H_1,H_2,\ldots, H_5$ of the dihedral group $D_{20}$ of symmetries of a regular $20$ -gon. The Sylow $2$ s are the groups of symmetries of the five embedded squares (see here for a picture). The four rotations by multiples of 90 degrees are in all the $H_i,i=1,2,3,4,5$ , so the intersections are ""large"". Of course, in that case conjugacy also follows from the fact that $H_1$ and $H_2$ are Sylow $2$ -subgroups. If $X$ stands for the set $G/H_1 \times G/H_2$ with $G$ acting by left multiplication, then the assumption on the size of $H_1\cap H_2$ implies that the action is not transitive. Clearly all the orbits of that action have sizes that are multiples of $p$ (the point stabilizers are of the form $xH_1x^{-1}\cap yH_2y^{-1}$ ). If we could show that the smallest orbit must have size $p$ exactly, we would be done. For the purposes of the alternative goal we would need to conclude that the kernel of this ""product action"" is equal to kernel of the action on either component, but I don't see a way forward. Background: I started thinking about this question when trying to settle this claim . You immediately see that my alternative goal is equivalent to that claim, translated to the language of groups by Galois correspondence. I think I'm missing something simple :-( Searching the site: While I was typing this question, the site engine found this nice thread , where the main question is answered in the affirmative, if we know that $[G:H_1\cap H_2]=p(p-1)$ . The elegant solution is related to my thoughts in the last bullet, but I cannot extend the method to cover this case as well. On the field theory side we have this intriguing old query .","['group-theory', 'finite-groups']"
4877203,"If $(a,b,c)$ are the sides of a triangle, is it true that probability that $a+b > c^{\frac{3}{c}}$ is $\zeta(2)-1$?","Let $(a,b,c)$ be the sides of a triangle inscribed inside a unit circle such that the vertices of the triangle are distributed uniformly on the circumference. The solution of this question unexpectedly showed that the simple triangle inequality $a+b \ge c$ is equivalent to the famous Basel problem . $$
\zeta(2) = 1 + \frac{1}{2^2} + \frac{1}{3^2} + \cdots = \frac{\pi^2}{6}
$$ Motivated by this, I was exploring if there are other relationship interesting relationships between the Riemann zeta function and the triangle inequality and I observed numerically that the probability $$
P\left(a+b \ge c^{\frac{3}{c}}\right) = \zeta(2) - 1
$$ Can this be proved or disproved? Julia Code: step = 10^7
target = step
count = 0
f = 0

while 1 > 0
    count += 1
    angles = (rand(3) .* 2 * π)
    vertices_x = cos.(angles)
    vertices_y = sin.(angles)
    
    push!(vertices_x, vertices_x[1])
    push!(vertices_y, vertices_y[1])
    
    x_diff = diff(vertices_x)
    y_diff = diff(vertices_y)
    side_lengths = sqrt.(x_diff.^2 + y_diff.^2)
    
    a = side_lengths[1]
    c = side_lengths[2]
    b = side_lengths[3]
        
    if (a+b) >= c^(3/c)
        f += 1
    end
    
    if count == target
        println(f,"" "", count,"" "", string(f/count)[1:end-1])
        target += step
    end
end For $3.5 \times 10^9$ trails, this code gives the probability as $0.64492$ which agrees with $\zeta(2)-1$ to four decimal places.","['integration', 'number-theory', 'geometry', 'triangles', 'inequality']"
4877259,Is there a name for shapes that can be cut out on a bandsaw?,"Let $S$ be some subset of $\mathbb{R}^3$ and $\text{Conv}(S)$ be its convex hull. Say $S$ has property $\mathcal{P}$ if each point $x \notin S$ lies on some line contained in $\mathbb{R}^3 \setminus S$ such that there is a ruled surface in $\mathbb{R}^3 \setminus S$ giving a homotopy to a line in $\mathbb{R}^3 \setminus \text{Conv}(S)$ . Intuitively you can think of this as saying that $S$ can be cut out on a bandsaw. Is there a name for this property or a simpler equivalent form? My motivation for this is wanting to define the $\mathcal{P}$ -hull of a subset $S \subset \mathbb{R}^3$ to be the smallest set containing $S$ that has property $\mathcal{P}$ (which is well-defined since the intersection of sets with property $\mathcal{P}$ has property $\mathcal{P}$ ). For example if $S$ is the wireframe of a compound of two tetrahedra as below, its convex hull is a cube, but its $\mathcal{P}$ -hull is a stellated octahedron. (I would also be interested in the stronger property where the ruled surface is replaced with a planar region between the lines. Intuitively the shapes with this property would be those that can be cut out on a bandsaw with only straight cuts.)","['convex-geometry', 'geometry', 'terminology', 'reference-request']"
4877278,Newman's Short Proof of Prime Number Theorem,"I'm going through the paper of D. Zagier on Short Proof of Prime Number Theorem. There it says in V that $\Phi(s)=\int_1^\infty \frac{d\vartheta(x)}{x^s}$ . Can someone please explain in details why that is the case ?
And also how The Analytic Theorem is being applied here to the functions defined as f(t) and g(z) ? MSE question link: Newman's ""Natural proof""(Analytic) of Prime Number Theorem (1980) Edit: I have understood how the Analytic Theorem is being applied here but I am still unable to figure out about that integral. I saw a post mentioning that because $\vartheta(x)$ changes by $log(p)$ for primes only. I got that point but is their a way to formally proof that thing. Any relative reference is appreciated.
Thank you ,","['integration', 'number-theory', 'analytic-number-theory', 'riemann-zeta', 'prime-numbers']"
4877291,Calculating radius given an arc length and the distance between its endpoints,"I wanted a formula that can tell the radius of the circle given the length of an arc of the circle and the distance between the endpoints of the arc. I have derived it to this form: $$d^2 = 2r^2\left(1-\cos{\frac{l}{r}}\right)$$ where $d$ is the distance between the endpoints of the arc, $r$ is the radius of the circle, and $l$ is the arc length. I understand that it cannot be solved for $r$ algebraically for a precise value. Can someone give me a neat and a very good approximation equation for $r$ ?","['trigonometry', 'approximation', 'transcendental-equations']"
4877351,Proving $\frac{s^2}{c}\left[\tan\frac\alpha2+\tan\frac\beta2\right]\left[\tan\frac\alpha2\tan\frac\beta2\right]=(s−c)\cot\frac\gamma2$ in any triangle [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question The question below was a part for my board exam of fbise, the key provided by the board that this statement would be proved through law of cosine, law tangent or law of sine, im confused furthermore I have searched this question over the internet and I haven't found something related to it, so any help would be recommended. In triangle $ABC$ (with usual notations) such that $$ \alpha+\beta+\gamma = \pi $$ then prove that, $$ \frac{s^2}{c}\left[\tan\left(\frac{\alpha}{2}\right)+ \tan \left(\frac{\beta}{2}\right)\right]\left[\tan \left(\frac{\alpha}{2}\right) \tan \left(\frac{\beta}{2}\right)\right] = (s − c) \cot \left(\frac{\gamma}{2}\right)  $$ where $ s=\frac{a+b+c}{2}  $ and $a$ , $b$ and $c$ are the sides of $\triangle$ .
Furthermore, $\alpha $ , $\beta$ and $\gamma$ are the angles of triangles facing each side $a$ , $b$ , and $c$ respectively.",['trigonometry']
4877387,Example implementing the Chain Rule in a textbook by Charles Chapman Pugh,"I am asking for help interpreting an example in a textbook. The author gives two functions from different dimensions of Euclidean space, and he precisely describes the image of arbitrary elements under these functions, but he does not state the arbitrary elements in the domains. Here is the example from Chapter 5. Maybe someone has a different edition of the textbook. Let $f: \mathbb{R}^{2} \to \mathbb{R}^{3}$ and $g: \mathbb{R}^{3} \to \mathbb{R}$ be defined by $f = (x,y,z)$ and $g = w$ where $$w = w(x,y,z) = xy + yz + xz$$ and $$x = x(s,t) = st, \quad y = y(s,t) = s\cos{t} \quad z = z(s,t) = s\sin{t}.$$ a.) Find the matrices that represent the linear transformations $(\mathrm{D}f)_{p}$ and $(\mathrm{D}g)_{q}$ where $p = (s_{\circ}, t_{\circ}) = (0,1)$ and $q = f(p)$ . b.) Use the Chain Rule to calculate the $1 \times 2$ matrix $[\partial{w}/\partial{s}, \partial{w}/\partial{t}]$ that represents $(\mathrm{D}(g\circ{f}))_{p}$ . c.) Substitute the functions $x = x(s,t)$ , $y = y(s,t)$ , and $z = z(s,t)$ directly into $w = w(x,y,z)$ in order to get $[\partial{w}/\partial{s}, \partial{w}/\partial{t}]$ , verifying the answer in Part b.).","['pushforward', 'multivariable-calculus', 'chain-rule', 'real-analysis']"
4877404,Find the center of all circles that touch the $x$-axis and a circle centered at the origin,"Given a circle $C$ of radius $1$ centered at the origin, I want to determine the locus of the centers of all circles that touch $C$ and the $x$ -axis.  This is the red curve in the following Desmos plot , where the blue circle touches $C$ and the $x$ -axis: Let $P=(\sin\alpha,\cos\alpha)$ be the point where the blue circle touches $C$ . Moving $\ell$ units towards the center of $C$ (the origin) gives the point $Q=(1-\ell)(\sin\alpha,\cos\alpha)$ . Now a circle around $Q$ touches the $x$ -axis when the $y$ -coordinate of $Q$ equals $\ell$ , so that $Q$ has the same distance to $C$ and to $y=0$ : $$
Q_y=(1-\ell)\cos\alpha \stackrel.= \ell \tag1
$$ This equation is solved by $$Q_y=\frac{\cos\alpha}{1+\cos\alpha} \tag2$$ It's also easy to compute the $x$ -ccordinate of $Q$ , which yields $Q$ depending on $\alpha$ : $$
Q=Q(\alpha)=\left(\frac{\sin\alpha}{1+\cos\alpha}, \frac{\cos\alpha}{1+\cos\alpha}\right) \tag3
$$ Where I am stuck is to compute $Q_y$ as a function of $Q_x$ , that is find $f$ such that $$
Q_y = f(Q_x) \tag4
$$ By looking at the plot I guessed $$
f(t) = \frac12(1-t^2) \tag 5
$$ and indeed $Q$ satisfies $(5)$ . It's not surprising that $f$ is a curve or order 2. But how to do it without guessing? I have no idea how to find the inverse of $$
\alpha \mapsto \frac{\sin\alpha}{1+\cos\alpha}
$$ Trying to substitute $\alpha = \arcsin z$ gives $$
Q(z) = \left(\frac z{1+\sqrt{1-z^2}} , \frac{\sqrt{1-z^2}}{1+\sqrt{1-z^2}} \right)\tag6
$$ just makes it more complicated...","['locus', 'inverse-function', 'geometry', 'moduli-space']"
4877429,Probability that the defendant is guilty. [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 4 months ago . Improve this question I've just come across this question on Isaac Physics and I have no idea where to begin. The question is as follows: You are serving on a jury in the Crown Court. The defendant has been accused of a serious crime, however the only evidence is that their DNA is a perfect match to the perpetrator's DNA found at the crime scene. The expert in genetic analysis tells you that the chance of a false positive (i.e. an innocent person matching that DNA) is 1 in 3000000. The prosecution lawyer says in their summing up speech that this means that as the defendant matches the DNA, the chance that they are innocent is less than 0.00004%, This means that there is a 99.99996% chance that the person is guilty, and as this is beyond reasonable doubt you and your jury colleagues should decide that the person is guilty. Back in the jury room, the other jurors know that you have mathematical knowledge and ask you for your view on the matter. If you think it relevant, you may also assume that the crime was definitely committed by a British person, and that the population of Britain is 67000000. Based on the information given, what is your best estimate of the probability that the defendant is guilty? Give your answer to two significant figures. As I said, I'm stuck in the mud with this question. I feel something to do with calculating how many innocent people in Britain would match the perpetrator's DNA would be a logical first step, although I'm not sure how I would go about doing this.","['conditional-probability', 'statistics', 'probability']"
4877448,Calculate the limit $\lim_{n \to \infty} \int_1^e x^m e^x (\log x)^n dx$ with limsup and liminf,"I have a question about how to prove the limit using $\limsup$ .
On the other day, I was asked the following problem in the exam: Let $m$ and $n$ be positive integers.
Calculate the following limit, where $m$ is fixed. $$
\lim_{n \to \infty} \int_1^e x^m e^x (\log x)^n dx
$$ To this problem, I answered in the manner shown below: (My solution) Take any $\varepsilon \in (0, 1)$ .
Now $f_n(x)$ denotes the integrand of the integral in question ( $f_n(x)$ depends on $m$ of course, but $m$ is fixed in this problem so no problems may occur even if $m$ is not appear in the notation).
First, split the integral into two pieces, $$
\int_1^e f_n(x) dx
= \int_1^{e - \varepsilon} f_n(x) dx
+ \int_{e - \varepsilon}^e f_n(x) dx,
$$ and call the first term $I_1$ and the second $I_2$ .
Since all of $x^m$ , $e^x$ and $(\log x)^n$ are increasing functions on $[1, e]$ , so is $f_n(x)$ .
Then, $I_1$ is evaluated as below: $$
I_1
\le \int_1^{e - \varepsilon} f_n(e - \varepsilon) dx
= f_n(e - \varepsilon)(e - \varepsilon - 1).
$$ Similarly, $I_2$ is evaluated like $$
I_2
\le \int_{e - \varepsilon}^e f_n(e)
= f_n(e)\varepsilon.
$$ Since $\log(e - \varepsilon) \in (0, 1)$ , I obtained $$
\lim_{n \to \infty} f_n(e - \varepsilon)(e - \varepsilon - 1)
= \lim_{n \to \infty} (e - \varepsilon)^m e^{e - \varepsilon} (\log(e - \varepsilon))^n
= 0
$$ and $$
\lim_{n \to \infty} f_n(e)\varepsilon
= \lim_{n \to \infty} e^m e^e (\log e)^n \varepsilon
= e^{m + e}\varepsilon.
$$ Therefore, I conclude that $$
0 \le \liminf_{n \to \infty} \int_1^e f_n(x) dx
\le \limsup_{n \to \infty} \int_1^e f_n(x)
\le \limsup_{n \to \infty} (I_1 + I_2)
\le \limsup_{n \to \infty} (f_n(e - \varepsilon)(e - \varepsilon - 1) + f_n(e)\varepsilon)
= e^{m + e}\varepsilon.
$$ Since $\varepsilon$ is arbitrary, both $\liminf$ and $\limsup$ of the integral as $n \to \infty$ must be $0$ , which indicates $$
\lim_{n \to \infty} \int_1^e x^m e^x (\log x)^n dx = 0.
$$ Is my proof valid or invalid? If there are either trivial or nontrivial mistakes, please let me know which part I should modify.
Especially, my most worried thing is how to use $\liminf$ and $\limsup$ .","['integration', 'limsup-and-liminf', 'real-analysis', 'solution-verification', 'limits']"
4877463,Question about definition of Sequences in Analysis I by Tao.,"Here's the definition of a sequence as laid out in the text: Let $m$ be an integer. A sequence $(a_n)_{n=m}^\infty$ of rational
numbers is any function from the set $\{n \in \mathbf{Z} : n \geq m\}$ to $\mathbf{Q}$ . I can make sense out of this definition, but I was under the impression that a sequence has an ordering to it. I do not see any order implied on the ""outputs"" in the definition. Am I missing something?","['analysis', 'real-analysis', 'definition', 'functions', 'sequences-and-series']"
4877500,"Find the generating function for $a_n=\sum_{k=0}^{n} 3^{k}(n-k), n\geq0$.",For every integer $n\geq 0$ we define $a_n=\sum_{k=0}^{n} 3^{k}(n-k)$ . Find the generating function of the sequence $(a_n)_{n=0}^\infty$ and write down the answer without series. Attempt: $G(x)=\sum_{n=0}^{\infty} (\sum_{k=0}^{n} 3^{k}(n-k))x^n=\sum_{n=0}^{\infty} (n\sum_{k=0}^{n} 3^{k}-\sum_{k=0}^{n}k3^k)x^n=\sum_{n=0}^{\infty}(\frac{1}{2}n(3^{n+1}-1)-\sum_{k=0}^{n}k3^{k})=...?$ where I took into account that $1+3+...+3^n=\frac{1}{2}(3^{n+1}-1)$ . How to continue? WolframAlpha returns: $\sum_{n=0}^{\infty} (\sum_{k=0}^{n} 3^{k}(n-k))x^n$ = $-\frac{x}{(x-1)^2 (3x-1)}$ . How do I get to this?,"['combinatorics', 'discrete-mathematics', 'generating-functions']"
4877513,Two Limits involving an integral.,"Let $$I_n = \int_{0}^{1}\frac{x^{n-1}-x^{n+p-1}}{(1+x^{n})(1+x^{n+p})} \, \mathrm{d}x,$$ where $p$ is a positive natural number.
Show that: $$\lim_{n \to \infty} n^2 I_n=p\ln{2} \qquad\text{and}\qquad \lim_{n \to \infty} \left({\frac{n^2I_n}{p\ln{2}}}\right)^n=e^{-p} $$ For the first one I substituted $t=x^n$ and we get: $$ n^2I_n=n\int_{0}^{1}\frac{1-x^{\frac{p}{n}}}{(1+x)(1+x^{1+\frac{p}{n}})} \, \mathrm{d}x .$$ Now I argue that $ x^{1+\frac{p}{n}} $ converges uniformly to $x$ on $[0,1]$ so we can substitute it with $x$ and in the limit they will be equal. I do have a rigorous proof for than using the definition for uniform convergence and then making an inequality involving $I_n$ , but its just too long for this post and I am also not sure its correct. So please tell me if such argument is possible. In the end we have: $$ n^2I_n = n\int_{0}^{1}\frac{1-x^{\frac{p}{n}}}{(1+x)^2} \, \mathrm{d}x $$ Now $\frac{1}{(1+x)^2}$ is the diff power series $\sum_{k\ge0} x^k$ , hence after multiplying by $1-x^{\frac{p}{n}}$ and integrating we get: $$ \lim_{n \to \infty}p\sum_{k\ge1} \frac{(-1)^{k+1}}{k+\frac{p}{n}}$$ and by the Weierstrass M-test, using the alternating harmonic series as for $M_n$ , we get that the limit is equal to $p\ln{2}$ If I try the same trick for the second limit (writing the integral as a series) I get a different result, namely $e^{\frac{-p\pi^2}{12\ln{2}}}$ ... It has to do with my shady argument above, that maybe works only for the first limit. Please help :(","['integration', 'limits', 'real-analysis']"
4877526,When is the ratio of largest number and smallest number when the sum and sum of squares is fixed,"Given $n$ positive real numbers $a_1\geq a_2,\cdots\geq a_n>0$ . Assume $\sum_{i=1}^n a_i=b_1, \sum_{i=1}^n a_i^2=b_2$ . I want to find an upper bound on $\frac{a_1}{a_n}$ , and the condition when this upper bound is achieved. My intuition is that the upper bound is achieved either $a_1=a_2=,\cdots=a_{n-1}>a_n$ or $a_1>a_2=a_3=\cdots a_n$ , but I don't know how to prove it. Some ideas: if for any fixed small $a_n$ , the equations $a_1+\cdots+a_{n-1}=b_1-a_n, a_1^2+\cdots+a_{n-1}^2=b_2-a_n^2$ has solutions, then one can let $a_n\to 0$ , and the $\frac{a_1}{a_n}\to\infty$ . To prevent this from happening, we need some conditions on $b_1, b_2$ so that $a_n$ has a positive lower bound. Specifically, one can use the following inequality \begin{align}
\frac{a_1+\cdots+a_{n-1}}{n-1} \leq \sqrt{\frac{a_1^2+\cdots+a_{n-1}^2}{n-1}}
\end{align} We plug in $a_1+\cdots+a_{n-1}=b_1-a_n$ and $a_1^2+\cdots+a_{n-1}^2=b_2-a_n^2$ , after some calculations, we get $na_n^2-2b_1a_n+b_1^2-(n-1)b_2\leq 0$ . Therefore, as long as $b_1^2-(n-1)b_2\leq 0$ , then $a_n$ will have a positive lower bound. Then, my question is when $b_1^2-(n-1)b_2\leq 0$ , what is the maximum value of $\frac{a_1}{a_n}$ and when the maximum is achieved?","['inequality', 'combinatorics', 'analysis']"
4877527,Computing Fourier Transform for $(1 - e^{-iyx})\frac{1}{x^{\alpha}}$,"I have to compute Fourier Transform for the function $(1 - e^{-iyx})\frac{1}{x^{\alpha}}$ where $y > 0, 0.5 < \alpha < 1.5$ . My attempt: I have to compute the following integral $$I = \int_{-\infty}^{\infty} e^{itx}(1 - e^{-iyx})\frac{1}{x^{\alpha}}\, dx$$ I tried to use complex integration to deal with this problem. Firstly, let's assume t < 0, so we can consider contour $K_1$ in lower-half plane consisted of two segments $(-R, -\epsilon)$ and $(\epsilon, R)$ , and two semicircles $C_{\epsilon}, C_R$ with center at $z=0$ and radius $\epsilon, R$ as well. So we have $$I_{K_1} = \int_{K_1} e^{itx}(1 - e^{-iyx})\frac{1}{x^{\alpha}} = 0,$$ as there are no singularities inside $K_1$ . We can prove that $$
\int_{C_R} e^{itx}(1 - e^{-iyx})\frac{1}{x^{\alpha}} \rightarrow 0, R \rightarrow \infty, \qquad \qquad \int_{C_\epsilon} e^{itx}(1 - e^{-iyx})\frac{1}{x^{\alpha}} \rightarrow 0, \epsilon \rightarrow 0,
$$ so we have that $I = 0$ for $t < 0$ . But it seems that we can use the same principe for t > 0, but in upper half plane. However it seems to be completely wrong. I guess it's because of branch cuts of $ \frac{1}{x^{\alpha}}$ , but I don't know how to deal with it. Need your help to finish this problem!","['functions', 'fourier-transform']"
4877545,Equivalence between two integrals,"I would like to prove the equality of the two integrals $$ I_1 = \int_0^1 \frac{\sqrt{1-x^2}}{(1+x)(1+2x)} dx$$ and $$ I_2 = -\int_0^1 \frac{(-x+2)^2-1 -\sqrt{1-x^2}\sqrt{1-{(-x+1)}^2}}{\left(\sqrt{1-x^2}+\sqrt{1-{(-x+1)}^2}\right )(1-x)(3-2x)} dx.$$ Mathematica can solve this and the solution is for both integral $$I_{1/2} = \frac{1}{4} [-\pi - 2 \sqrt{3} \log(2 - \sqrt{3})].$$ What I would like is a simple proof of the equality $I_1 = I_2$ , without having to calculate the solution directly.
Any help would be appreciated. EDIT: We can multiple the integrand of $I_2$ by $\frac{\sqrt{1-x^2}-\sqrt{1-{(-x+1)}^2}}{\sqrt{1-x^2}-\sqrt{1-{(-x+1)}^2}}$ . After a substitution $u = 1-x$ we get $$I_2 = \int_0^1 \frac{\sqrt{1-x^2}}{(1+x)(1+2x)} \frac{(2x-5)(1+x)}{(1-2x)(1-x)}.$$ Almost there but I still can't conclude.","['integration', 'definite-integrals']"
4877572,Simplifying a binomial sum for bridge deals with specific voids,"While trying to get an expression for the number of deals from a generalised bridge deck with nobody being void in any suit I encountered the following subproblem. From a generalised bridge deck with $r$ ranks instead of just $13$ deal four hands of $r$ cards each. How many deals $A(r)$ are there where South is void in $\diamondsuit$ and $\heartsuit$ West is void in $\clubsuit$ and $\heartsuit$ North is void in $\clubsuit$ and $\diamondsuit$ East is unrestricted, though all players may be void in other suits? $A(r)$ is the coefficient of $(wxyz)^r$ in $((w+z)(x+z)(y+z)(w+x+y+z))^r$ where $wxyz$ correspond to $\clubsuit\diamondsuit\heartsuit\spadesuit$ respectively. I solved this as follows: South picks $a$ spades, West picks $b$ spades and North picks $c$ spades in one of $\binom r{a,b,c,r-a-b-c}$ ways East gives $r-a$ clubs, $r-b$ diamonds and $r-c$ hearts to South, West, North respectively in one of $\binom ra\binom rb\binom rc$ ways; the rest of the deal is forced Thus $$A(r)=\sum_{a,b,c}\binom ra\binom rb\binom rc\binom r{a,b,c,r-a-b-c}$$ which after applying Vandermonde's identity to $c$ followed by shuffling binomials becomes the double sum over all $a,b$ $$A(r)=\sum_{a,b}\binom ra^2\binom ab^2\binom{r+b}a$$ But when I computed the terms and compared to the OEIS I found a match with A290575 , with a much simpler formula $$B(r)=\sum_{k=0}^r\binom rk^2\binom{2k}r^2$$ and I cannot prove that these two binomial sums are equal. How can I show that $A=B$ , preferably (but definitely not necessarily) with a combinatorial proof? I know of Zeilberger's algorithm but don't have access to Maple.","['summation', 'combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'card-games']"
4877699,"How to solve the eigenvalue problem $y'' + \lambda y=0$, $y'(-L) = y'(L)=0$","How do I solve the eigenvalue problem $y'' + \lambda y=0$ , $y'(-L) = y'(L)=0$ ? My attempt: Case 1: $\lambda=0$ . If $\lambda=0$ , then the BVP becomes $y''(x)=0$ , $y'(-L)=y'(L)=0$ . The general solution to this differential equation is $y(x)=Ax+B$ . Taking the derivative gives $y'(x)=A$ , and the boundary condition implies that $A=0$ , but $B$ can be arbitrary. This means that $\mu_0=0$ must be an eigenvalue and the function $$y_0(x)=B,\quad B\text{ is a constant}$$ is its corresponding eigenfunction. Case 2: $\lambda<0$ . Let $\lambda = -p^2<0$ . The BVP becomes $y''(x)-p^2y(x)=0$ , $y'(-L)=y'(L)=0$ . The solution to this DE is $y(x)=A\cosh(px)+B\sinh(px)$ . Taking the derivative gives $y'(x)=Ap\sinh(px)+Bp\cosh(px)$ . Plugging in $-L$ and $L$ gives \begin{align*}
    A\cosh(pL) - B\sinh(pL) &= 0\\
    A\cosh(pL) + B\sinh(pL) &= 0
\end{align*} Solving for $A$ and $B$ gives $A=B=0$ , which is a trivial solution. Case 3: $\lambda>0$ . Let $\lambda = p^2 > 0$ . The BVP becomes $y''(x)+p^2y(x)=0$ , $y'(-L)=y'(L)=0$ . The solution to this DE is $y(x)=A\cos(px)+B\sin(px)$ . Taking the derivative gives $y'(x)=-Ap\sin(px) + Bp\cos(px)$ , and applying the boundary condition, \begin{alignat}{3}
    & y'(-a) && = Ap\sin(pL) + Bp\cos(pL) = 0 \quad&& \Longrightarrow\quad A\sin(pL) + B\cos(pL) = 0 \label{eq:X2}\\
    & y'(a) && = -Ap\sin(pL) + Bp\cos(pL) = 0 \quad&& \Longrightarrow\quad A\sin(pL) - B\cos(pL) = 0 \label{eq:Y2}
\end{alignat} Adding the two equations, we obtain $2A\sin(pL)=0$ . This implies that either $A=0$ or $\sin(pL)=0$ . Subtracting the two equations gives $2B\cos(pL)=0$ . This implies that either $B=0$ or $\cos(pL)=0$ . To summarize, we have $$A=0,\;\text{or}\;\sin(pL)=0,\quad\text{and}\quad B=0\;\text{or}\;\cos(pL)=0.$$ Together, they form four cases: $A=B=0$ , $A=\cos(pL)=0$ , and $B=\sin(pL)=0$ , and the final one gives $\cos(pL)=\sin(pL)=0$ , which is not possible. The second, and third case gives $$p_1 = \frac{n\pi}{L}\quad\text{and}\quad p_2 = \frac{(n-\frac{1}{2})\pi}{L},\quad n =1, 2, 3,\ldots,$$ respectively. Therefore, there are only two non-trivial families of solutions in this case: $$\cos\left(\frac{n\pi}{L}x\right)\quad\text{and}\quad\sin\left(\frac{(n-\frac{1}{2})\pi}{L}x\right),\quad n=1, 2, 3,\ldots$$ and thus the corresponding eigenfunctions are $$y_n(x)=a_n\cos\left(\sqrt{\lambda_{n, 1}}x\right)+b_n\sin\left(\sqrt{\lambda_{n, 2}}x\right)$$ where $$\lambda_{n, 1}=\left(\frac{n\pi}{L}\right)^2,\quad \lambda_{n, 2}=\left(\frac{(n-\frac{1}{2})\pi}{L}\right)^2$$ Question: Is my solution correct? Should there be two families of eigenvalues like case 3?","['ordinary-differential-equations', 'partial-differential-equations']"
4877716,Functional equation with certain properties,"Let $a,b \in \Bbb{R}, b>0$ . Find all the differentiable functions $f:\Bbb{R} \rightarrow \Bbb{R}$ , for which $\lim_{x\to -\infty} f(x) $ = a and which verify : $$ f'(x)=b(f(x)-a)^{2},$$ for every $x \in \Bbb{R}$ [Attempt] Suppose there exist $x_{0}$ such that $f(x_{0})\ne a $ and because f is continuous there exist an interval $I$ such that $f(x)\ne a, $ for every $x \in I$ , then: $$ \int  \left(\frac{-1}{f(x)-a}\right)' dx =\int  \frac{f'(x)}{(f(x)-a)^2}dx= \int b \ dx \implies xb+c = \frac{-1}{f(x)-a} $$ so $ \dfrac{-1}{xb+c} +a =f(x)$ but from here I don't know how to get to a contradiction.","['functional-equations', 'functions']"
4877740,Rational functions where they are undefined,"There's something I can't quite wrap my head around, and I am not satisfied with my professor's ""that's just defined that way"" answer. So suppose we have: $$f_1(x) = x - 5$$ $$f_2(x) = \frac{1}{x - 5}$$ $$f_3(x) = 1$$ By simple algebra, we can see that $f4(x) = f_1(x) \cdot f_2(x) = f_3(x)$ What I mean is the rational function we get from dividing $x - 5$ by $x - 5$ is the same as the constant function $y = 1$ Now I like to look at functions graphically to understand them. So we can clearly see that $f_1(5) = 1$ and $f_2(5)$ is not defined. So how come, when we multiply these two functions, we end up with a function (or a curve) that is defined at point $x = 5$ . By logic, on $x = 5$ , we should have $0/0$ , which is undefined. But here it just takes the value $1$ . How come? Visuals to better understand whats bugging me: I would appreciate if your answer addresses this logically (or rather conceptually), not as a plain mathematical proof using algebra. I too, know, that algebraically speaking $(x - 5)/(x - 5)$ is just $1$ , but thinking of functions visually rather than algebraically, this does not look convincing at all.","['calculus', 'functions']"
4877817,does Integrating both sides of an equation in dx will Invalidates the equality?,"I'm struggling to grasp the justification behind integrating both sides of an equation. While I understand that operations can be applied to both sides, maintaining equality, it appears that this principle doesn't apply here. given the equality $x=y$ integrating both sides by dx would give $\frac{x^2}{2} = xy$ but this seems not to be valid since if I start from x=y=5 , I would get $\frac{25}{2}=25 $ that's not true. given for instance $log(a)=log(b)$ if I integrate both sides by $da$ I get: $alog(a)-a=log(b)a$ if $a=b=2$ then $alog(a)-a=-0.61..$ and $log(b)a=1.38$ that are different, what I'm doing wrong here?","['integration', 'logarithms', 'real-analysis', 'calculus', 'derivatives']"
4877840,Reference for a combinatorial identity involving the number of derangements,Let $$c_n=n!\sum\limits_{k=0}^n (-1)^k \frac{1}{k!}$$ be the number of derangements of $n$ elements. The following combinatorial identity is coming up in my research: $$\sum\limits_{j=1}^{n-2}c_{n-j}{n-2\choose j-1}=(n-2)!(n-2)$$ Is there a known reference or proof for this identity? Simulations do support this.,"['summation', 'derangements', 'reference-request', 'binomial-coefficients', 'combinatorics']"
4877851,Probability that the reciprocal triangle inequality $\frac{1}{a} + \frac{1}{b} \ge \frac{1}{c}$ holds,"Let $(a,b,c)$ be the sides of a triangle inscribed inside a unit circle such that the vertices of the triangle are distributed uniformly on the circumference. The regular triangle inequality states that the sum of any two sides is greater than the third side. But what happens if we take the sum of the reciprocal of any two sides? Is it greater than the third side? It turns out that the reciprocal triangle inequality $\frac{1}{a} + \frac{1}{b} \ge \frac{1}{c}$ is not true in general however, experimental data shows an interesting observation that the probability $$
P\left(\frac{1}{a} + \frac{1}{b} \ge \frac{1}{c}\right) = \frac{4}{5}
$$ Can this be proved or disproved? Note that this is equivalent proving or disproving $$
P\left(\frac{1}{\sin x} + \frac{1}{\sin y} \ge \frac{1}{|\sin (x+y)|} \right) = \frac{4}{5}
$$ where $0 \le x,y \le \pi$ . Related question : Probability that the geometric mean of any two sides of a triangle is greater than the third side is $\displaystyle \frac{2}{5}$ .","['integration', 'geometry', 'trigonometry', 'triangles', 'inequality']"
4877880,"Meaning of ""covariant"", ""contravariant"" in operator algebras","I am reading The Novikov conjecture, the group of volume preserving diffeomorphisms and Hilbert-Hadamard spaces , and I came across a usage of the words ""covariant"" and ""contravariant"" that I did not understand. Here is the relevant passage: Generalizing the construction of $C_0(X)$ , we define, for a $C^∗$ -algebra $A$ and a locally compact space $X$ , the $C^∗$ -algebra $C_0(X, A)$ to consist of all continuous functions $f$ from $X$ to $A$ that vanish at infinity, equipped with the pointwise algebraic and $∗$ -operations. This construction is covariant in $A$ with respect to $∗$ -homomorphisms and contravariant in $X$ with respect to proper continuous maps, by
means of composition of maps. I wonder if the categorical meaning of the terms is intended, but as these words are so overloaded, I found it quite difficult to Google. Maybe $C_0(\cdot, A)$ is being considered as a functor from some category of topological spaces to that of $C^*$ -algebras, and similarly for $C_0(X, \cdot)$ ?","['c-star-algebras', 'operator-algebras', 'category-theory', 'functional-analysis', 'terminology']"
4877882,$F(x) = f([x]) \cdot f(\{x\})$ . Find $f$,"Given a function $f: [0,\infty) \rightarrow (0,\infty)$ which admits primitives, and let $F$ be a primitive such that: $F(x) = f([x]) \cdot f(\{x\})$ for all x, and there exists $a>0$ s.t. $f(a)=F([a]) \cdot F(\{a\})$ , where $[x]$ refers to the integer part, and the other to the fractional part. Find every function $f$ that verifies these properties. I have only been able to find that small relations between $F$ and $f$ for intervals of the form $[n,n+1)$ and that $f$ seems to have a recursive formula for natural numbers, yet nothing that would point out a certain clear path..","['continuity', 'derivatives', 'fractional-part', 'real-analysis']"
4877892,Probability that a triangle inscribed in a square comprises at least $\frac{1}{4}$ of the area of the square,"Question: Suppose that points $P_1$ , $P_2$ , and $P_3$ are chosen uniformly at random on the sides of a square $T$ . Compute the probability that $$\frac{[\triangle P_1 P_2 P_3]}{[T]}>\frac{1}{4}$$ where $[X]$ denotes the area of polygon $X$ . Without loss of generality, I assumed the side length of the square to be $1$ . Because the question mentions $\frac{1}{4}$ of the square, I considered splitting the square into quadrants. It is obvious that all three points cannot lie in the same quadrant of the square. Similarly, there cannot be two points in the same quadrant because the area must be less than $\frac{1}{2} \cdot \frac{1}{2} \cdot 1=\frac{1}{4}$ . [EDIT: This is wrong] This means that all three vertices must lie in different quadrants in the square. From here, I considered cases: Case 1: Two of the points are on the same side, which occurs with probability $\frac{9}{16}$ . Case 2: All three points are on different sides, which occurs with probability $\frac{3}{8}$ . From here, my efforts have consisted of just labeling lengths and finding the area in terms of said lengths. However, this has led me with some inequalities that I don't know how to find the probabilities of being true, namely: $x-xz+yz<\frac{1}{2}$ for $0 \leq x,y,z \leq 1$ $(x-y)z<\frac{1}{2}$ fo $0 \leq y<x \leq 1$ and $0 \leq x \leq 1$ If anyone knows how to either find the probability of these inequalities being satisfied (which I think requires multivariable calculus) or a way that circumvents these inequalities, please let me know. Here's my full attempt at the question, though I'm unsure about the accuracy of how I find the probabilities of the three-variable inequalities being satisfied. [NOTE: this method has a few errors in it, a correct version is in the answers below.] Without loss of generality, consider the square with vertices $(0,0), (0,1), (1,0), (1,1)$ . We proceed using casework: All three vertices are on different sides of the square. This occurs with probability $\frac{9}{16}$ . Let the vertices be $(x_1,0)$ , $(0,y_1)$ , and $(x_2,1)$ . Using the determinant form for the area of a triangle, the area is given by $$A=\frac{1}{2} \begin{vmatrix} x_1 & 0 & 1 \\ 0 & y_1 & 1 \\ x_2 & 1 & 1 \end{vmatrix}=\frac{1}{2}x_1-\frac{1}{2}y_1 (x_1-x_2)$$ Assume that $x_1>x_2$ . Then, $$\frac{1}{2}x_1-\frac{1}{2}y_1 (x_1-x_2)>\frac{1}{4} \implies x_1-y_1 (x_1-x_2)>\frac{1}{2}$$ For convenience, replace $x_1$ with $y$ , $x_2$ with $x$ , and $y_1$ with $z$ . We now have the system of inequalities $\begin{cases} y-yz+xz>\frac{1}{2} \\ y>x \\ 0 \leq x,y,z \leq 1 \end{cases}$ . We now consider graphing the inequality $y-yz+xz>\frac{1}{2}$ with $z$ as a constant. The $x$ -intercept is $\frac{1}{2z}$ and the $y$ -intercept is $\frac{1}{2(1-z)}$ . If $0<z \leq \frac{1}{2}$ , then the area of the region is given by $\frac{4z-1}{8z-8}$ . Therefore, the desired probability for this case is $\int^{\frac{1}{2}}_{0} \frac{4z-1}{8z-8} \, dz=\frac{1}{8}(2-\ln 2)$ . If $\frac{1}{2} \leq z<1$ , then the area of the region is given by $\frac{1}{8z}$ . Therefore, the desired probability for this case is $\int^{\frac{1}{2}}_{0} \frac{1}{8z}=\frac{1}{8} \ln 2$ . The overall probability for this case is $$\frac{9}{16} \cdot \frac{1}{2} \cdot \frac{1}{4}=\frac{9}{128}$$ Two vertices lie on the same side of the square and the third vertex lies on the opposite side. This occurs with probability $\frac{1}{8}$ . Let the vertices be $(y,1)$ , $(x,1)$ , and $(z,0)$ where $y>x$ . Then, the area is given by $\frac{1}{2}(y-x)$ , meaning we need $y-x>\frac{1}{2}$ . It is easy to find the probability for this case is $$\frac{1}{8} \cdot \frac{1}{8}=\frac{1}{64}$$ Two vertices lie on the same side of the square and the third vertex lies on an adjacent side. This occurs with probability $\frac{1}{4}$ . Let the vertices of the triangle be $(x,1)$ , $(y,1)$ , and $(1,1-z)$ where $y>x$ . Then, the area is given by $\frac{1}{2}(y-x)(z)$ , meaning we need $(y-x)z>\frac{1}{2}$ . It is easy to see that this is only possible for $\frac{1}{2} \leq z <1$ , in which case the probability is $\frac{2z-1}{8z^2}$ . The probability for this case is thus $$\frac{1}{4} \cdot \int^{1}_{\frac{1}{2}} \frac{2z-1}{8z^2} \, dz=\frac{1}{64} \ln 4-\frac{1}{64}$$ Therefore, the final answer is $$\frac{1}{64} \ln 4-\frac{1}{64}+\frac{1}{64}+\frac{9}{128}=\boxed{\frac{9+4 \ln 2}{128}}$$","['contest-math', 'geometric-probability', 'probability']"
4877902,Are there infinitely many primes that are a highly composite number $\pm 1$?,"I've looked at some highly composite numbers and realized that a lot of them are almost primes, i.e. differ only by $1$ to the next closest prime. Here's a short list (I made) of highly composite numbers and whether they are almost prime: $$ 1 \text{, true $(+1)$} \\
2 \text{, true $(+1)$} \\
4 \text{, true $(\pm 1)$} \\
6 \text{, true $(\pm 1)$} \\
12 \text{, true $(\pm 1)$} \\
24 \text{, true $(-1)$} \\
36 \text{, true $(+1)$} \\
48 \text{, true $(-1)$} \\
60 \text{, true $(\pm 1)$} \\
120 \text{, false} \\
180 \text{, true $(\pm 1)$} \\
240 \text{, true $(\pm 1)$} \\
\dots $$ I've thought about it, and it's actually not so surprising that many highly composite numbers (which I've checked) are almost prime. Since $n \pm 1$ won't share any prime divisors with $n$ , it's likely for $n \pm 1$ to be prime if $n$ has a high number of divisors. But are there infinitely many highly composite numbers that are $\pm 1$ a prime? I don't know how I would prove something like this—I'm not even sure if this conjecture really holds. Also, if there are infinitely many such numbers, do they get rarer? And at what rate?","['number-theory', 'conjectures', 'elementary-number-theory', 'prime-numbers']"
4877923,Number of surjective functions with given property,"Let $n \in \mathbb{N}, n \geq 2$ and $M = \{1, 2, \ldots, n\}$ . Show that there are more than $2^n$ surjective functions $f : \mathcal{P}(M) \rightarrow \{0, 1, \ldots, n\}$ such that $f(A) \leq f(B)$ for all $A \subseteq B \subseteq M$ . ( $\mathcal{P}(M)$ is the power set of $M$ ) My first thought was representing each subset as a bitset of length $n$ , then defining a function for each bitset $A$ , for example $f_A(B) = \text{popcount(A XOR B)}$ , and playing around with other operations such as sum/difference $\pmod{2^n}$ . Then, I would find another unrelated function which satisfies the property (for example, the maximum function or the cardinality function) and state that there would be at least $2^n + 1$ functions. The problem is that the bitwise functions have some edge cases that fail to satisfy the property. Then, I tried to actually compute the number of functions but I don't know how to make sure the property holds while calculating. I suppose the final formula will probably involve ordered Bell numbers due to inclusion being a partial ordering.","['contest-math', 'order-theory', 'combinatorics']"
4877944,How to solve $y(y'(y''(\cdots(y^{(n)}(x))))) = f(x)$ for $y(x)$?,"$$\Large \text{Question}$$ Question Let's say that $y^{\left( \alpha \right)}\left( x \right)$ is the $\alpha$ .th derivative of $y\left( x \right)$ with respect to $x$ ( $x \in \mathbb{R} \vee x \in \mathbb{C}$ and $\alpha \in \mathbb{N}_{0}$ ). How to solve \begin{align*}
y\left( y^{\left( 1 \right)}\left( y^{\left( 2 \right)}\left( y^{\left( 3 \right)}\left( \cdots \left( y^{\left( n \right)}\left( x \right) \right) \right) \right) \right) \right) &= f\left( x \right)\\
\end{align*} for $y\left( x \right)$ ? We could also write this equation as $\left( y \circ y^{\left( 1 \right)} \circ y^{\left( 2 \right)} \circ y^{\left( 3 \right)} \circ \cdots \circ y^{\left( n \right)}\right)\left( x \right) = f\left( x \right)$ or $y_{m + 1}\left( x \right) = \left( y_{m} \circ y^{\left( m + 1 \right)} \right)\left( x \right) \wedge y_{0}\left( x \right) = y\left( x \right) \wedge y_{n}\left( x \right) = f\left( 
 x \right)$ if $m \geq 0$ . I don't necessarily want a closed form solution. I would also be interested in solutions in non-closed form, numerical solutions, series solutions, ..., or even just possible solution methods. Special cases would also be fine. Why this question The question came to my mind in the form of special cases. They seem interesting to me, but in most cases I am unable to find a solution (which made it even more interesting for me). I didn't find anything about it online (the closest thing was questions about functional roots) and online calculators like Mathematica didn't help me either. Hence the question, how do you solve this equation? $$\Large \text{My approach}$$ Special Cases If $f$ is constant Let's say $f\left( x \right) = \text{c}$ where $\text{c}$ is some constant then $y\left( x \right) = \text{c}$ is a solution. Let's say $f\left( x \right) = 0$ then $y\left( x \right) = \sum_{k = 0}^{n - 1}\left[ y_{k} \cdot x^{k} \right]$ (where $y_{k}$ are constant terms) would also be a solution. Or more general: If $y^{\left( n \right)}\left( x \right) = \text{c}$ then $f\left( x \right)$ is also a constant given in terms of $\text{c}$ s. But all this is trivial. If $f\left( x \right) = c \cdot x^{d}$ Let's say $f\left( x \right) = x$ (easy example). Let us assume that a solution of the form $y\left( x \right) = a \cdot x^{b}$ exists (guess), then (using $y^{\left( \alpha \right)}\left( x \right) = a \cdot \frac{\left( b + 1 \right)}{\Gamma\left( b - \alpha + 1 \right)} \cdot x^{b - \alpha}$ ): $n$ $a$ $b$ $0$ $1$ $1$ $1$ $b^{-\frac{b}{1 + b}}$ $\frac{1 \pm \sqrt{5}}{2}$ $2$ $\left( \left( b - 1 \right)^{\left( b - 1 \right) \cdot b} \cdot b^{b + \left( b - 1 \right) \cdot b} \right)^{-\frac{1}{1 + b + \left( b - 1 \right) \cdot b}}$ $2.3247... \vee 0.33764... \pm 0.56228... \cdot i$ $3$ $\left( \left( b - 2 \right)^{\left( b - 2 \right) \cdot \left( b - 1 \right) \cdot b} \cdot \left( b - 1 \right)^{\left( b - 1 \right) \cdot b + \left( b - 2 \right) \cdot \left( b - 1 \right) \cdot b} \cdot b^{b + \left( b - 1 \right) \cdot b + \left( b - 2 \right) \cdot \left( b - 1 \right) \cdot b} \right)^{-\frac{1}{1 + b + \left( b - 1 \right) \cdot b + \left( b - 2 \right) \cdot \left( b - 1 \right) \cdot b}}$ $1.5 \pm 0.4052... \cdot i \vee -0.13224 \vee 3.1322$ $\vdots$ $\vdots$ $\vdots$ $n$ $\left( \prod\limits_{k = 0}^{n - 1}\left[ \left( b - k \right)^{\sum\limits_{g = k + 1}^{n}\left[ \frac{\Gamma\left( b + 1 \right)}{\Gamma\left( b - g + 1 \right)} \right]} \right] \right)^{-\frac{1}{1 + \sum\limits_{g = 1}^{n}\left[ \frac{\Gamma\left( b + 1 \right)}{\Gamma\left( b - g + 1 \right)} \right]}}$ roots of $\prod\limits_{k = 0}^{n}\left[ b - k \right] = 1$ One could probably generalize this for $f\left( x \right) = c \cdot x^{d}$ by saying that $b$ is the solution to $\prod\limits_{k = 0}^{n}\left[ b - k \right] = d$ and $a = \left( c \cdot \prod\limits_{k = 0}^{n - 1}\left[ \left( b - k \right)^{\sum\limits_{g = k + 1}^{n}\left[ \frac{\Gamma\left( b + 1 \right)}{\Gamma\left( b - g + 1 \right)} \right]} \right] \right)^{-\frac{1}{1 + \sum\limits_{g = 1}^{n}\left[ \frac{\Gamma\left( b + 1 \right)}{\Gamma\left( b - g + 1 \right)} \right]}}$ (if the term for a (above) is correct). My thinking behind is that $f\left( x \right)$ is a monomial of degree $d$ and both sides of the equation must have a monomial of same degree. The monomial on the LHS has the degree $\prod\limits_{k = 0}^{n}\left[ b - k \right]$ thus $\prod\limits_{k = 0}^{n}\left[ b - k \right] = d$ . So we just have to find the factor in front of the $x$ (on the LHS) and get $c$ out. So we could set the factor in front of it equal to $c$ (but finding a solution in closed form would almost never be possible), or we set $a = \sqrt[\text{exponent}]{c \cdot \frac{1 }{b\text{-term}}}$ where $b\text{-term}$ is the term involving $b$ in front of the $x$ and $\text{exponent}$ is the exponent $a$ . The root cancels the exponent and $\frac{1}{b\text{-term}}$ cancels the factor in before $x$ (only leaving $c$ ). What remains is $c \cdot x^{d}$ . But this only worked, because of a lucky guess. Reduction Assuming $y^{\left( \alpha \right)}\left( x \right) = y^{\left( \alpha + k \cdot m \right)}\left( x \right)$ where $\left\{ \alpha,\, m \right\} \in \mathbb{N} \wedge k \in \mathbb{N}_{0}$ If $y^{\left( \alpha \right)}\left( x \right) = y^{\left( \alpha + k \cdot m \right)}\left( x \right)$ where $\left\{ \alpha,\, m \right\} \in \mathbb{N} \wedge k \in \mathbb{N}_{0}$ and $g\left( x \right) := \left( y \circ y^{\left( 1 \right)} \circ \cdots \circ y^{\left( m - 1 \right)} \right)\left( x \right)$ we could rewrite the equation to: \begin{align*}
\left( g \circ \cdots \circ g \circ y \circ y^{\left( 1 \right)} \circ \cdots \circ y^{\left( n \right)} \right)\left( x \right) &= f\left( x \right)\\
\left( g^{\left[ k \cdot m \right]} \circ y^{\left( k \cdot m \right)} \circ y^{\left( k \cdot m + 1 \right)} \circ \cdots \circ y^{\left( n \right)} \right)\left( x \right) &= f\left( x \right)\\
\left( g^{\left[ k \cdot m \right]} \circ y \circ y^{\left( 1 \right)} \circ \cdots \circ y^{\left( n - k \cdot m \right)} \right)\left( x \right) &= f\left( x \right)\\
\end{align*} or \begin{align*}
g^{\left[ k \cdot m \right]}\left( y\left( y^{\left( 1 \right)}\left( \cdots\left( y^{\left( n - k \cdot m \right)} \right) \right) \right) \right) &= f\left( x \right)\\
\end{align*} where $g^{\left[ m \right]}$ is the $m$ .th iteration of $g$ . Taking the functional $k \cdot m$ .th-root : \begin{align*}
g\left( y\left( y^{\left( 1 \right)}\left( \cdots\left( y^{\left( n - k \cdot m \right)}\left( x \right) \right) \right) \right) \right) &= f^{\left[ \frac{1}{k \cdot m} \right]}\left( x \right)\\
y\left( y^{\left( 1 \right)}\left( \cdots\left( y^{\left( m \right)}\left( y\left( y^{\left( 1 \right)}\left( \cdots\left( y^{\left( n - k \cdot m \right)}\left( x \right) \right) \right) \right) \right) \right) \right) \right) &= f^{\left[ \frac{1}{k \cdot m} \right]}\left( x \right)\\
\end{align*} Wich may not look simpler but it reduces the order from $n$ to $n - k \cdot m$ or $\text{order} \leq m$ . Of course, $f^{\left[ \frac{1}{k \cdot m} \right]}\left( x \right)$ sometimes has no solution, one solution, multiple solutions, no closed-form solution, closed-form Solutions, ... E.G. if $\frac{n}{k \cdot m} \in \mathbb{N}$ then $y$ would be a Mittag-Leffler function (in some real interval) or / and $f$ would be an iteration of the same Mittag-Leffler function. But that does not solve the problem...","['functional-equations', 'ordinary-differential-equations']"
4878145,Frechet differential and implicit theorem,"Given: $ \Omega \subseteq \mathbb{R}^n $ . $ f: \Omega \rightarrow \mathbb{R}^n $ and $ g: \Omega \rightarrow \mathcal{L}(\mathbb{R}^n, \mathbb{R}^n) $ are two $ C^1 $ -class functions on $ \Omega $ . Define $ F: \Omega \times \Omega \rightarrow \mathbb{R}^n $ as $ F(x, y) = g(x)(f(y)) + f(x) $ . (a) Show that $ F $ is of class $ C^1 $ on $ \Omega \times \Omega $ and find $ DF $ , $ D_1F $ , and $ D_2F $ . Now, let's proceed with the solution: To show that $ F $ is of class $ C^1 $ on $ \Omega \times \Omega $ , we need to demonstrate that ( F ) is continuously differentiable with respect to both variables ( x ) and ( y ). The function $ F(x, y) $ is composed of two parts: $ g(x)(f(y)) $ and $ f(x) $ . Since $ f $ and $ g $ are $ C^1 $ -class functions on $ \Omega $ , their compositions are also $ C^1 $ functions. Therefore, $ g(x)(f(y)) $ and $ f(x) $ are $ C^1 $ functions on $ \Omega \times \Omega $ . Now, let's compute the partial derivatives: $ D_1F = g'(x)f(y) + f'(x) $ $ D_2F = g(x)f'(y) $ I use this: $$
\Omega \rightarrow \mathcal{L}(\mathbb{R}^n, \mathbb{R}^n) \times \Omega \rightarrow \mathbb{R}^n
$$ Where $$ x \mapsto (g(x), y) \mapsto g(x)y $$ Finally, the total derivative $ DF $ can be expressed as the matrix: $$ DF = \begin{pmatrix} D_1F \\ D_2F \end{pmatrix} = \begin{pmatrix} g'(x)f(y) + f'(x) \\ g(x)f'(y) \end{pmatrix} $$ Suppose $ 0 \in \Omega $ and $ f(0) = 0 $ . We want to find conditions on $ f $ and $ g $ for there to exist open neighborhoods $ U $ and $ V $ of $ 0 $ in $ \mathbb{R}^n $ and a function $ \Psi: U \rightarrow V $ of class $ C^1 $ on $ U $ such that $ F(x,y) = 0 $ if and only if $ y = \Psi(x) $ . Additionally,find $ D\Psi(0) $ . For this part, I know I need to examine the matrix $ D_2F $ and check if it's invertible at the point $ (0,0) $ to apply the implicit theorem. For this, I need both the matrix $ g(0) $ and the derivative $ f'(0) $ to be bijective. Can someone confirm this? I need help to find $ D\Psi(0) $ . Thank you.","['banach-spaces', 'gateaux-derivative', 'frechet-derivative', 'partial-derivative', 'derivatives']"
4878210,Almost Sure Convergence of Order Statistics,"Let $F$ be a strictly increasing distribution function. For a given $\tau \in (0,1)$ , suppose there exists $\epsilon_{\tau}$ such that $F(\epsilon_{\tau}) = \tau$ . Considering a set of independent and identically distributed (i.i.d.) random variables $X_{1}, \cdots, X_{n}$ , with their order statistics denoted by $X_{(1)}, \cdots, X_{(n)}$ , and let $k = [n\tau]$ (where $[ \cdot ]$ denotes the floor function). I got the following statement from the post https://stats.stackexchange.com/a/373530/409031 . Specifically, it is stated that $$X_{(k)} \to \epsilon_\tau$$ almost surely as $n$ approaches infinity. Could someone provide a detailed explanation or proof of this form of convergence? I have some basic understanding of order statistics and the concept of almost sure convergence. However, I am struggling to connect these concepts to establish the proof or detailed explanation of this particular scenario. Any insights or step-by-step explanations would be greatly appreciated.","['statistics', 'convergence-divergence', 'probability-theory', 'order-statistics']"
4878212,Why can a compound biconditional statement whose individual statements don't all have the same truth values be true?,"Why can $P_1 ⇔  P_2  ⇔ P_3 ⇔ \ldots ⇔ P_n$ be true when not all the $P$ ’s have the same truth value? For example: If P1 = T P2 = T P3 = F P4 = F would this be true?
T(P1) ⇔  T(P2) ⇔  F(P3) ⇔  F(P4) =
T(P1&2) ⇔ F(P3) ⇔  F(P4) =
F(P1,2,3) ⇔ F(P4) =
True If so how can an evaluation for a series of compound biconditionals with index n and x (individual truth values) be represented as a formula? Potentially statement is true with known x = 0; x = n; ?","['boolean-algebra', 'propositional-calculus', 'logic', 'discrete-mathematics']"
4878297,Show that $\int_{\arccos(1/4)}^{\pi/2}\arccos(\cos x (2\sin^2x+\sqrt{1+4\sin^4x})) \mathrm dx=\frac{\pi^2}{40}$,"There is numerical evidence that $$I=\int_{\arccos(1/4)}^{\pi/2}\arccos\left(\cos x\left(2\sin^2x+\sqrt{1+4\sin^4x}\right)\right)\mathrm dx=\frac{\pi^2}{40}$$ How can this be proved? I was trying to answer another question , and I got it down to this integral. Wolfram does not evaluate the indefinite integral. I tried techniques from a roughly similar integral . Letting $u=\tan \frac{x}{2}$ , I got $$I=\int_{\sqrt{3/5}}^1 \dfrac{2\arccos{\left(\frac{(1-u^2)\left(8u^2+\sqrt{u^8+4u^6+70u^4+4u^2+1}\right)}{(1+u^2)^3}\right)}}{1+u^2}\mathrm du$$ but I don't know what to do with this.","['integration', 'definite-integrals', 'calculus', 'closed-form', 'trigonometry']"
4878299,A stronger version of the Rosen’s subsequence theorem.,"The following question was asked in my combinatorics exam - “Let $n$ be a positive integer. Exhibit an arrangement of integers between $1$ to $n^2$ which has no increasing or decreasing subsequence of length $n + 1$ .” This question has a very similar structure to the Rosen’s subsequence theorem , and I am aware of its proof using the pigeonhole principle, as given in the link above by @Rajdeep. But i am not really sure on how to provide the requisite construction for my question above — the pigeonhole principle fails here as we have $n^2$ numbers and $n^2$ pairs, if we proceed in a similar fashion as done in the proof linked above. So could somebody provide me with some hints or solutions ? I may be completely wrong as well in the using pigeonhole principle for this problem, so different solutions are also welcomed.","['combinatorics', 'discrete-mathematics']"
4878312,Continuous analogue of the discrete simple continued fraction,"Background The classical Riemann integral of a function $f : [a,b] \to \mathbb{R}$ can be defined by setting $$\int_{a}^{b} f(x) \ dx := \lim_{\Delta x \to 0} \sum f(x_{i}) \ \Delta x.  $$ Here, the limit is taken over all partitions of the interval $[a,b]$ whose norms approach zero. We can do something roughly similar with product integrals . They take the limit over a product instead of a sum, and can be interpreted as continuous analogues of discrete products. There are multiple types of product integrals. Type I is often refered to as Volterra's integral . It is defined as follows: \begin{align*}
\prod_{a}^{b} \left(1+f(x) \ dx \right) &:= \lim_{\Delta x \to 0} \left(1 + f(x_{i}) \ \Delta x \right) \newline
&= \exp \left( \int_{a}^{b} f(x) \ dx \right). \tag{1} \label{1}
\end{align*} However, this is not a multiplicative operator. As an alternative, there is also Type II, the geometric integral. It is defined as \begin{align*}
\prod_{a}^{b} f(x)^{dx} &:= \lim_{\Delta x \to 0} \prod f(x_{i})^{\Delta x} \newline 
&= \exp \left( \int_{a}^{b} \ln f(x) \ dx \right). \tag{2} \label{2}
\end{align*} This does amount to a multiplicative operator. A third type, the bigeometric integral, is also an operator with this property. Question I wonder whether something similar can be done with other kinds of expressions ( infinite or finite). In particular, I am curious whether we can obtain a continuous analogue of the discrete simple continued fraction . In the discrete case, it is defined as: $$\underset{i=a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(i)} = \cfrac{1}{f(a) + \cfrac{1}{f(a+1)+\cfrac{1}{\ddots+\cfrac{1}{f(b-1) + \cfrac{1}{f(b)}}}}}$$ In other words, I am looking for a way to complete the following table, by finding a definition of what is described in the bottom right cell: \begin{array}{|c|c|c|}
\hline
& \text{additive} & \text{multiplicative} & \text{simple continued fraction} \\ \hline
\text{discrete} & \sum_{i=a}^{b} f(i) & \prod_{i=a}^{b} f(i) & \underset{i=a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(i)} \\ \hline
\text{continuous} & \int_{a}^{b} f(x) \ dx  & \prod_{a}^{b} f(x)^{dx}  & \underset{a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(x)} \overline{dx} \\ \hline
\end{array} I think we could make a start by setting $$ \underset{a}{\overset{b}{\large{\mathrm K}}} \ \frac{1}{f(x)} \overline{dx} := \lim_{\Delta x \to 0} \large{\mathrm K} \frac{1}{f(x_{i})} \Delta x  .$$ However, I am not sure how this would translate into a formula that is similar to \eqref{1} or \eqref{2}. Is there a way to obtain such a formula (or perhaps multiple, as in the case of the product integral), and if so, what does it look like?","['integration', 'analysis', 'definition', 'products', 'continued-fractions']"
4878315,Expected number of darts thrown in a game,"$A$ and $B$ play a game where they take turns throwing darts at a dart board. The winner is the person to hit first. $A$ hits with probability $a$ and $B$ with probability $b$ . I showed (by computing a sum) that the expected number of darts thrown in one game is $$\frac{\alpha}{a}+\frac{\beta}{b}$$ where $\alpha$ and $\beta$ are the probability $A$ and $B$ win resp. This made me think there should be a solution using the law of total probability (since $\alpha+\beta=1$ ). That would suggest that the expected number of darts thrown given $A$ wins is $\frac{1}{a}$ . This is nonsense, so I dismissed this as a coincidence. However, I ran a simulation for the analogous game for three players $A$ $B$ and $C$ and found that the expected number of darts thrown in the game is $$\frac{\alpha}{a}+\frac{\beta}{b}+\frac{\gamma}{c}$$ Can someone explain whats going on here? I'm expecting someone will be able to arrive at this answer (and presumably the result for $n$ players) in a much more satisfying way than direct computation. EDIT: The result also holds for four players computationally","['conditional-probability', 'algebra-precalculus', 'probability']"
4878396,Solve $2x^2-3\sqrt{2x^2-7x+7}=7x-3$,"Solve $2x^2-3\sqrt{2x^2-7x+7}=7x-3$ $\Rightarrow (2x^2-7x)-3\sqrt{2x^2-7x+7}=-3$ Let $\sqrt{2x^2-7x+7}=y$ so that $2x^2-7x+7=y^2$ , $\Rightarrow (y^2-7)-3y=-3$ or $y^2-3y-4=0$ So $y=4$ or $y=-1$ $\sqrt{2x^2-7x+7}=4$ or $\sqrt{2x^2-7x+7}=-1$ $2x^2-7x+7=16$ or $2x^2-7x+7=1$ $x=\dfrac{2}{9}, -1$ or $x=2, \dfrac{3}{2}$ My question:
If I take $x=2$ and plug it in $2x^2-3\sqrt{2x^2-7x+7}=7x-3$ , the equality does not hold. So is this a valid solution or not?",['algebra-precalculus']
4878410,"What does an ""exotic"" derivation at a point $x_{0}\in \mathbb{R}^{n}$ look like?","for $x_{0}\in \mathbb{R}^{n}$ I denote by $G^{k}(x_{0})\: (1\le k\le \infty )$ the $\mathbb{R}$ -algebra of germs of real-valued $C^{k}$ -functions at $x_{0}$ . Recall that a derivation at $x_{0}$ is a linear form $D:G^{k}(x_{0})\to\mathbb{R}$ which satisfies the product rule $D(f\cdot g)=f(x_{0})\cdot Dg+g(x_{0})\cdot Df$ . I call a differentiable function $c:\mathbb{R\to \mathbb{R}^{n}}$ with $c(0)=x_{0}$ a curve at $x_{0}$ . Recall that each such curve defines  a derivation $D_{c}$ at $x_{0}$ by $D_{c}(f)=\frac{d }{dt}(f\circ c)_{|t=0}$ (where $f$ is any representative of the respective germ), called the derivative in the direction of the tangent to $c$ at $x_{0}$ .The well-known (and not hard to prove) fact that in the case $k=\infty $ every derivation at $x_{0}$ is actually a directional derivative is sometimes used to define the tangent space at a point of a $C^{\infty }$ -manifold. But already as a student I was baffled  to learn that this does not work for $k\lt \infty $ : It has been proved that the vector space of all derivations is infinite-dimensional in this case. Just recently I have studied a rather short proof of this but have not been able to deduce a definite example for such an ""exotic"" derivation which is not a directional derivative. Can anybody present such an example here? And I would like to know: are there derivations $\neq 0$ on $G^{0}(x_{0})$ ?","['manifolds', 'real-analysis']"
4878430,"The vertices of a triangle are three random points on a unit circle. The side lengths are $a,b,c$. Show that $P(ab>c)=\frac12$.","The vertices of a triangle are three uniformly random points on a unit circle. The side lengths are, in random order, $a,b,c$ . Show that $P(ab>c)=\frac12$ . The result is strongly suggested by simulations, and by my attempt shown below. The simplicity of the result suggests that there may be an intuitive explanation. I am hoping for an intuitive explanation, but if that's not possible then any answer is welcome. (Examples of intuitive explanations are here and here .) My attempt Assume that the circle is centred at the origin, and the vertices of the triangle are: $A(\cos(-2Y),\sin(-2Y))$ where $0\le Y\le\pi$ $B(\cos(2X),\sin(2X))$ where $0\le X\le\pi$ $C(1,0)$ Let: $a=BC=2\sin X$ $b=AC=2\sin Y$ $c=AB=\left|2\sin\left(\frac{2\pi-2X-2Y}{2}\right)\right|=|2\sin(X+Y)|$ $P\left[ab>c\right]=P\left[2(\sin X)(\sin Y)>|\sin(X+Y)|\right]$ This probability is the ratio of the area of the shaded region to the area of the square in the graph below. Rotate these regions $45^\circ$ clockwise about the origin and then shrink them by a factor of $\frac{1}{\sqrt2}$ , by letting $X=x-y$ and $Y=x+y$ . Using symmetry, we only need to consider the left half of the blue ""diamond"". Note that in the left half, $0<x<\pi/2$ , so $|\sin(2x)|=\sin(2x)$ . $P\left[2(\sin X)(\sin Y)>|\sin(X+Y)|\right]$ $=P\left[2(\sin (x-y))(\sin (x+y))>\sin(2x)\right]$ $=P\left[(\sin^2x)(\cos^2y)-(\cos^2 x)(\sin^2 y)>(\cos x)(\sin x)\right]$ $=P\left[(\sin^2x)(\cos^2y)-(\cos^2 x)(1-\cos^2 y)>(\cos x)(\sin x)\right]$ $=P\left[\cos^2 y>\cos^2 x+(\cos x)(\sin x)\right]$ $=P\left[ -f(x)<y<f(x)\right]$ where $\color{red}{f(x)=\arccos\left((\cos x)\sqrt{1+\tan x}\right)}$ . Noting that $f\left(\frac{\pi}{4}\right)=0$ , the probability is $$\dfrac{\int_{\pi/4}^{\pi/2}f(x)\mathrm dx}{\frac12\left(\frac{\pi}{2}\right)^2}$$ Numerical evidence suggests that $\int_{\pi/4}^{\pi/2}f(x)\mathrm dx=\frac{\pi^2}{16}$ , but I don't know how to prove this. If that's true, then the probability is indeed $1/2$ . Context This question was inspired by a question , ""If $(a,b,c)$ are the sides of a triangle, what is the probability that $ac>b^2$ ?"" Generalization I found a generalization, which may or may not help in finding an intuitive explanation. For $k\in\mathbb{R^+}$ , we have $$P(ab<kc)=\frac{2}{\pi}\arctan k$$ or equivalently, $$P\left(\frac{ab}{c}<k\right)=\frac{\arctan k}{\frac{\pi}{2}}$$ Proof : Using the set-up in the above ""My attempt"", we have $P[ab\color{red}{>}kc]$ $=P\left[2(\sin X)(\sin Y)>k|\sin(X+Y)|\right]$ $=P\left[2(\sin (x-y))(\sin (x+y))>k\sin(2x)\right]$ $=P\left[(\sin^2x)(\cos^2y)-(\cos^2 x)(\sin^2 y)>k(\cos x)(\sin x)\right]$ $=P\left[(\sin^2x)(\cos^2y)-(\cos^2 x)(1-\cos^2 y)>k(\cos x)(\sin x)\right]$ $=P\left[\cos^2 y>\cos^2 x+k(\cos x)(\sin x)\right]$ $=P\left[ -g(x)<y<g(x)\right]$ where $\color{red}{g(x)=\arccos\left((\cos x)\sqrt{1+k\tan x}\right)}$ . Noting that $g\left(\arctan k\right)=0$ , the probability is $$\dfrac{\int_{\arctan k}^{\pi/2}g(x)\mathrm dx}{\frac12\left(\frac{\pi}{2}\right)^2}$$ To evaluate the integral in the numerator, I copy the method in @Zacky's answer , adjusted for the presence of $k$ . $$I=\int_{\arctan k}^{\pi/2} \arccos\left(\cos x \sqrt{1+k\tan x}\right)dx\overset{\cot x\to x}=\int_0^{1/k} \frac{\arccos \sqrt{\frac{x(k+x)}{1+x^2}}}{1+x^2}dx$$ $$=\int_0^{1/k} \frac{\arctan \sqrt{\frac{1}{x}\frac{1-kx}{k+x}}}{1+x^2} dx\overset{\large \frac{1-kx}{k+x}\to x}=\int_0^{1/k} \frac{\operatorname{arccot} \sqrt{\frac{1}{x}\frac{1-kx}{k+x}}}{1+x^2}dx$$ $$\Rightarrow 2I=\frac{\pi}{2}\int_0^{1/k} \frac{1}{1+x^2}dx\Rightarrow \boxed{I=\frac{\pi}{4}\arctan \frac1k}$$ Above it was utilized that $\, \arccos x =\arctan \left(\frac{\sqrt{1-x^2}}{x}\right)$ and $\arctan x+\operatorname{arccot} x=\frac{\pi}{2}$ . $\therefore P(ab>kc)=\frac{2}{\pi}\arctan\frac1k$ $\therefore P(ab<kc)=1-\frac{2}{\pi}\arctan\frac1k=\frac{2}{\pi}\arctan k$","['integration', 'definite-integrals', 'geometric-probability', 'intuition', 'probability']"
4878507,Is a $\sigma$-locally finite collection of open sets locally countable?,"Problem I encountered this statement on nLab, which says that weakly Lindelöf spaces with a $\sigma$ -locally finite basis are second-countable . The original proof given below the statement is Proof . Let $\mathcal{V}$ be a $\sigma$ -locally finite basis. For each $x\in X$ , there is a neighborhood $N_{x}$ meeting countably many members of $\mathcal{V}$ . If $X$ is weakly Lindelöf, there is a countable $\{N_n\}_{n=1}^\infty$ which covers a dense subset of $X$ . Then $\mathcal{U}=\{V\in\mathcal{V}\mid N_n\cap V\neq\varnothing ~\text{for some}~n\}$ is a countable basis for $X$ . The proof is extremely brief, and I couldn't understand the italicized part, which I refer to as the collection $\mathcal{V}$ being locally countable for the time being. I believe it is not a very common property since I've searched $\pi$ -Base and Wikipedia but couldn't find anything about this locally countable property. The proof simply stated it as if it is an easy corollary. I know that $\mathcal{V}$ being a $\sigma$ -locally finite basis implies that $X$ is first-countable, but this doesn't take me any further. My main question is, how could I prove that $\mathcal{V}$ is locally countable? Would $\mathcal{V}$ being a $\sigma$ -locally finite basis alone suffice? In fact, as I claim later, $\mathcal V$ itself is countable, but it requires the additional assumption that the space $X$ is weakly Lindelöf. And this sounds like circular reasoning to me. I would like to know if the locally countable property holds without the weakly Lindelöf assumption. My thoughts In my opinion, the statement itself is true, as $\mathcal{V}$ itself is countable, which follows easily from the following claim which I believe is true: Claim. Every locally finite family of nonempty open subsets of a weakly Lindelöf space is countable. Proof. (This is an imitation of the proof of theorem 5.1.24 of Engelking's General Topology ) Let $\mathcal{A}$ be a locally finite family of nonempty open subsets of a weakly Lindelöf space $X$ . For every $x\in X$ choose a neighborhood $U_x$ of $x$ that intersects only finitely many members of $\mathcal{A}$ and take a countable subcover $\mathcal{U}$ that covers a dense subset of $X$ . Since every member of $\mathcal{A}$ meets some $U\in \mathcal{U}$ , it follows that $\mathcal{A}$ is countable. I think the situation is a bit strange, since second countability is a very desirable property, but I couldn't find this statement or the locally countable property anywhere else. I checked $\pi$ -base and there are no counterexamples to the statement.","['general-topology', 'second-countable', 'lindelof-spaces', 'real-analysis']"
4878554,Showing that a function is exponential by real analysis.,"Let $f:\left( 0,\infty \right) \rightarrow \mathbb{R} $ be a differentiable function strictly increasing. For every given $a\in\mathbb{R}$ the tangent line of $f$ at the point $(a,f(a))$ cuts the $x$ axis at the point $a-b$ , where $b$ is a constant. Show that $f$ is an exponential function. I can only get that $f(x)=bf'(x)$ for every $x\in(0,\infty)$ . One could think to resolve like in differential equations, but I need to use theorems from real analysis on derivatives. I think I need to use that $f$ is strictly increasing, but I don't know how. I would appreciate any help.","['derivatives', 'real-analysis']"
4878571,"When $X_t$ is conditionally normal distributed and has density $p_t$, how can we compute $\text E\left[\left\|\nabla\ln p_t(X_t)\right\|^2\right]$?","Let $d\in\mathbb N$ and $(X_t)_{t\ge0}$ be an $\mathbb R^d$ -valued process. Assume $$\operatorname P\left[X_t\in\;\cdot\;\mid X_0\right]=\mathcal N(X_0,\Sigma_t)\tag1$$ for some covariance matrix $\Sigma_t$ . Moreover, assume $X_t$ has density $p_t$ with respect to the $d$ -dimensional Lebesgue measure. Let $\varphi_{\Sigma_t}$ denote the density of the $d$ -dimensional normal distribution with mean $0$ and covariance matrix $\Sigma_t$ . By assumption, $$p_t=p_0\ast\varphi_{\Sigma_t}\tag2$$ (is the convolution of $p_0$ and $\varphi_{\Sigma_t}$ ). Now, what I need to do is numerically compute/estimate $$\operatorname E\left[\left\|\nabla\ln p_t(X_t)\right\|^2\right]\tag3.$$ Please assume that we do not know the distribution of $X_0$ . Instead, we only have i.i.d. samples drawn from the distribution of $X_0$ . And given such a sample, I'm able to produce a sample from $X_t$ which is distributed according to $(1)$ . I'm not sure what exactly I need to do. Maybe it's worth noting that $$\nabla\ln p_t=\frac{p_0\ast\nabla\varphi_{\Sigma_t}}{p_0\ast\varphi_{\Sigma_t}}\tag4,$$ but since I don't see that this simplifies further, I'm stuck with that. Remark : For the numerical computation, please note that I do not actually know $\Sigma_t$ . I only know that it exists. So, what I'm doing, is computing $\Sigma_t$ numerically as well.","['measure-theory', 'statistics', 'normal-distribution', 'stochastic-processes', 'probability-theory']"
4878591,What are the properties of the differential operator that justify this?,"Say I am solving the following differential equation: $$y'' - y = x^2$$ It is perfectly permissible for me to proceed as follows: $$D^2y- y = x^2$$ $$y(D^2-1) = x^2$$ $$y = (D^2 - 1)^{-1} \cdot x^2$$ $$y=\sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} D^n\cdot x^2$$ $$y = (-1 - D^2 - D^4...) x^2$$ $$y_p = -x^2 - 2$$ Which specific properties have been used (specifically the second and third step) which justify this? Similarly, why am I allowed to do this? $$\frac{dy}{dx}=2x$$ $$dy=2xdx$$ In general, which properties hold for the differential operator?",['ordinary-differential-equations']
4878604,Question About The Generation of The Borel $\sigma$-Algebra on $\mathbb{R}^d$,"I am new to measure theory. I was asked to prove the following result: Proposition $\quad$ The $\sigma$ -algebra $\mathcal{B}(\mathbb{R}^d)$ of Borel subsets of $\mathbb{R}^d$ is generated by each of the following collections of sets: the collection of all closed subsets of $\mathbb{R}^d$ ; the collection of all closed half-spaces in $\mathbb{R}^d$ that have the form $\{(x_1,\dots,x_d):x_i \leq b\}$ for some index $i$ and some $b$ in $\mathbb{R}$ ; the collection of all rectangles in $\mathbb{R}^d$ that have the form \begin{align*}
    \{(x_1,\dots,x_d):a_i < x_i \leq b_i\ \textit{for}\ i = 1,\dots,d \}.    
\end{align*} where Definition $\quad$ The smallest $\sigma$ -algebra on $X$ that includes $\mathcal{F}$ is called the $\sigma$ -algebra generated by $\mathcal{F}$ . Definition $\quad$ The Borel $\sigma$ -algebra on $\mathbb{R}^d$ is the $\sigma$ -algebra on $\mathbb{R}^d$ generated by the collection of open subsets of $\mathbb{R}^d$ ; it is denoted by $\mathcal{B}(\mathbb{R}^d)$ . The Borel subsets of $\mathbb{R}^d$ are those that belong to $\mathcal{B}(\mathbb{R}^d)$ . Here is my attempt: Proof $\quad$ Let $\mathcal{B}_1$ , $\mathcal{B}_2$ , and $\mathcal{B}_3$ be the $\sigma$ -algebras generated by the collections of sets in parts (1), (2), and (3) of the proposition. We will show that $\mathcal{B}(\mathbb{R}^d) \supseteq \mathcal{B}_1 \supseteq \mathcal{B}_2 \supseteq \mathcal{B}_3$ and then that $\mathcal{B}_3 \supseteq \mathcal{B}(\mathbb{R}^d)$ ; this will establish the proposition. Since $\mathcal{B}(\mathbb{R}^d)$ includes the family of open subsets of $\mathbb{R}^d$ and is closed under complementation, it includes the family of closed subsets of $\mathbb{R}^d$ ; thus it includes the $\sigma$ -algebra generated by the closed subsets of $\mathbb{R}^d$ , namely $\mathcal{B}_1$ . The sets of the form $\{(x_1,\dots,x_d):x_i \leq b\}$ for some index $i$ and some $b$ in $\mathbb{R}$ are closed and so belong to $\mathcal{B}_1$ ; consequently $\mathcal{B}_1 \supseteq \mathcal{B}_2$ . Note that each strip that has the form $\{(x_1,\dots,x_d):a < x_i \leq b\}$ for some $i$ is the difference of two of the half-spaces in part (2) and that each of the rectangles in part (3) is the intersection of $d$ such strips. So, each rectangles in part (3) belongs to $\mathcal{B}_2$ ; thus $\mathcal{B}_2 \supseteq \mathcal{B}_3$ . Finally, note that each strip that has the form $\{(x_1,\dots,x_d):a < x_i < b\}$ for some $i$ is the union of a sequence of sets of the form $\{(x_1,\dots,x_d):p < x_i \leq q\}$ for the same $i$ , that each open rectangle in $\mathbb{R}^d$ is the intersection of $d$ such strips, and that each open set in $\mathbb{R}^d$ is the union of a sequence of open rectangles. Thus, each open subset of $\mathbb{R}^d$ belongs to $\mathcal{B}_3$ , and so $\mathcal{B}_3 \supseteq \mathcal{B}(\mathbb{R}^d)$ . Could someone please help me check if my proof is correct and rigorous? Thanks a lot in advance!","['measure-theory', 'proof-writing', 'real-analysis', 'solution-verification', 'borel-sets']"
4878664,Is the median a measurable function of the probability distribution?,"For $\mu \in \mathcal P(\mathbb R)$ , let $m(\mu)$ be the median of $\mu$ , defined as the smallest of all medians of $\mu$ as follows: $$
m(\mu) = \inf \left\{ x \in \mathbb R \,\middle|\, \mu((-\infty,x]) \ge \frac12 \right\}.
$$ Assume that $\mathcal P(\mathbb R)$ is endowed with the weak convergence topology. Then, $m$ is not continuous as, for exemple, $\mu_n = (\frac12-\frac 1 n) \delta_0 + (\frac12 + \frac 1 n) \delta_1$ weakly converges to $\mu = \frac12(\delta_0+\delta_1)$ , but $m(\mu_n)=1$ for all n while $m(\mu)=0$ . (Counter exemple given in a comment, thanks) Now, endow $\mathcal P(\mathbb R)$ with the Borel sigma algebra associated to the above topology. Is $m$ measurable?","['measure-theory', 'median', 'continuity', 'measurable-functions', 'probability']"
4878665,"In how many ways can numbers $ \in \{1, 2, ..., 3n \}$ be arranged in such way that the sum of every $3$ consecutive numbers is divisible by $3$?","In how many ways can the numbers from the set $ \{1, 2, ..., 3n \}$ be arranged in a sequence such that the sum of every three consecutive numbers is divisible by $3$ ? Solution: All the numbers from the set $ \{1, 2, ..., 3n \}$ can be divided in 3 subsets: numbers that are equal to $0$ mod $3$ numbers that are equal to $1$ mod $3$ numbers that are equal to $2$ mod $3$ The only possible groups of $3$ numbers with sums divisible by $3$ are therefore: element from group $1$ + element from group $1$ + element from group $1$ element from group $2$ + element from group $2$ + element from group $2$ element from group $3$ + element from group $3$ + element from group $3$ element from group $1$ + element from group $2$ + element from group $3$ When I want to make sum of every three consecutive numbers be divisible by $3$ , I can start with choosing the first $3$ numbers of sequence and then moving my 'window' containing 3 numbers by $1$ place to the right. That way, by moving my 'window' I will lose one number on the left and get one new on the right. After such move, for the numbers in the 'window' to maintain divisibility by $3$ I need to add a number that shares the same group with the number that I left outside of my window. By extension, I can conclude that if I need a sequence such that the sum of every three consecutive numbers is divisible by $3$ , I have to structure every another window of 3 numbers the same way as the first one. Therefore my sequences can look like that: | $g1$ + $g1$ + $g1$ | $g1$ + $g1$ + $g1$ | $g1$ + $g1$ + $g1$ | ... | $g2$ + $g2$ + $g2$ | $g2$ + $g2$ + $g2$ | $g2$ + $g2$ + $g2$ | ... | $g3$ + $g3$ + $g3$ | $g3$ + $g3$ + $g3$ | $g3$ + $g3$ + $g3$ | ... | $g1$ + $g2$ + $g3$ | $g1$ + $g2$ + $g3$ | $g1$ + $g2$ + $g3$ | ... Now we can see that first $3$ sequences are impossible, because in my set of numbers I have $n$ elements from group $1$ , $n$ elements from group $2$ , $n$ elements from group $3$ and they all need to be placed in my sequence. Therefore the only scheme that I can use is the last one: | $g1$ + $g2$ + $g3$ | $g1$ + $g2$ + $g3$ | $g1$ + $g2$ + $g3$ | ... Now, I need to permute the places of groups in each window of $3$ numbers: that can be done in $3! = 6$ ways. Then I need to permute numbers in their groups. Therefore, ultimately, I can create $6 \cdot n! \cdot n! \cdot n!$ such sequences. Is that correct?","['permutations', 'combinatorics']"
4878667,Extrema of $\sum_{j=0}^{n-1} \frac{1}{|z-a_j|^2}$ for $z$ on unit circle,"Let $n \in \mathbb{N}$ , $0<r<1$ and $\omega = \exp\left(\frac{2 \pi i}{n}\right)$ .
For $j = 0, 1, \ldots, n-1$ define $a_j = r \omega^j$ , these are the vertices of a regular $n$ -gon inside the circle of radious $r$ . Now for which $z \in \mathbb{C}$ on the unit circle, that is $|z|=1$ , does the following expression $$\sum_{j=0}^{n-1} \frac{1}{|z-a_j|^2}$$ achieve its minimal and maximal value? The answer turns out that the maximal value is achievet for $z_{max} = \exp\left(\frac{2j \pi i}{n}\right) = \omega^j$ , that is for the point lying above the vertices, and minimal for $z_{min}= \exp\left(\frac{(2 j+ 1)\pi i}{n}\right)$ , that is the points above the midpoint between two vertices. Now due to simmetry we can conclude that the points $z_{max}$ and $z_{min}$ will be local extrema and that we can only concentrate on $z$ with argument in $(0,\pi / n)$ . It remains to show that for any given $n$ the above expression is monotone on the interval $(0,\pi / n)$ . But for the life of me I am not able to prove this. I tried calculating the derivative after parametrising, but the calculations are too messy. I tried Lagrange multipliers, in the real and complex setting, but again no luck. I tried some geometric arguments, but again didn't get far. The problem remindes me of potential theory, although I know nothing of the subject. Another neat thing is that the terms are Poisson kernels of the unit disc, so there might be some Harmonic analysis tools one could use. Does anyone have an idea how one can proof this? The problem seems elementary to me, so if it's a known solved problem, I'd love a reference.","['complex-analysis', 'geometry', 'extreme-value-analysis']"
4878726,A question on Lee's Proof of the Global Frobenius Theorem (Lemma 19.22),"I'm afraid this is a stupid question — I'm not a mathematician, so please correct me when I'll be saying something wrong — but I've been stuck at this point for so long that I thought it would be wise to ask for help. Hereafter is an excerpt from Introduction to Smooth Manifold by John Lee. The last line is where I'm lost. Essentially, $D$ is an involutive (smooth) distribution of rank $k$ on a (smooth) manifold $\mathcal M$ of dimension $m$ , $\mathcal N_1$ and $\mathcal N_2$ are integral manifolds for $D$ , with $\emptyset\ne\mathcal N_1 \cap \mathcal N_2$ , and we want to show that $\mathcal N_1 \cap \mathcal N_2$ is open with respect to, say, $\mathcal N_1$ (whose topology may be finer than the subspace topology induced by $\mathcal M$ ). To this end, let $q\in \mathcal N_1 \cap \mathcal N_2$ , and let us consider a chart $(W,\varphi)$ of $\mathcal M$ in $q$ that is flat for $D$ – this means just that $\varphi(W)$ is a cube in $\mathbb R^m$ , and that $\partial/\partial x^1,\dots,\partial/\partial x^k$ is a (local) frame for $D$ over $W$ .
We call $V_i$ ( $i=1,2$ ) the connected component (I guess with respect to the topology of $\mathcal N_i$ ) of $\mathcal N_i \cap W$ containing $q$ . From previous results, we know that our $V_i$ 's are open sets of the same slice $S=\{p\in W \,\vert\, \varphi^{k+1}(p)=c^{k+1},\dots,\varphi^{m}(p)=c^{m}\}$ of $W$ , therefore $V_1\cap V_2$ is open in $S$ with respect to the subspace topology. Here, as far as I understand, Lee is saying: since $V_1\cap V_2$ is open in $S$ , then it is open in $\mathcal N_1$ . What is the line of reasoning one should be following? PS: my sketch argument would be along the following lines. Since $V_1$ is a connected component of $\mathcal N_1\cap W$ , it should be open in $\mathcal N_1$ . Also, for a subset of $V_1$ , being open in $V_1$ , or in $\mathcal N_1$ should be the same. From the fact that $V_1\cap V_2$ is open in $S$ , we have $V_1\cap V_2= G\cap S$ , for some open set $G$ of $\mathcal M$ . Then $V_1\cap V_2= G\cap V_1$ , hence is open in the subspace topology induced by $\mathcal M$ , hence in $V_1$ , hence in $\mathcal N_1$ . But this seems to be too convoluted, I feel like I'm missing some very basic stuff.","['submanifold', 'foliations', 'smooth-manifolds', 'general-topology', 'differential-geometry']"
4878747,Where is the optimal point to “hide” in a shape?,"A puzzle I’ve wondered about but never got around to solving/verifying: In the game Pokemon Let’s Go, the Pokemon Abra instantly teleports away if the player is detected within its line of sight. Let’s say you’re in a square of length $s$ and one Abra randomly spawns in the square, with a random orientation, and has a 60-degree line of sight in front of it. What’s the optimal place in the square to be so that Abra doesn’t teleport away? If Abra has an unlimited length of sight (i.e. can see as far as possible within the square), it seems like any place in the square is equally good? Since given any place you choose to be, no matter where Abra spawns in the square, there is a uniform 5/6 chance that Abra doesn’t detect you due to its orientation? However, if Abra does have a limited length of sight (i.e. can only spot you from a distance $d$ away), my initial intuition would be to hide in the corner. But is there an elegant argument here without e.g. using calculus? And are there any convex polygons where hiding in a corner wouldn’t be the optimal place (if the intuition is correct)? Thanks for reading!","['optimization', 'puzzle', 'geometry']"
4878781,A problem regarding lcm of the first few consecutive natural numbers.,"Let $n$ be a positive integer, and $f(2n+1)$ be equal to $lcm[1,2,\ldots,2n+1]$ . I am supposed to show that $\log(f(2n+1))\ge 2n\log 2$ by using the fact that $e^{f(2n+1)}\int\limits_{0}^{1} x^n(1-x)^n~dx\in\mathbb N$ . Suppose that I accept the fact in faith. One notices that $x(1-x)=1/4-(1/2-x)^2\le 1/4$ whenever $x\in[0,1]$ . Thus $$I:=\int\limits_{0}^{1} x^n(1-x)^n~dx\le \int\limits_{0}^{1}(1/4)^n~dx=2^{-2n},$$ and so $1\le e^{f(2n+1)}I\le e^{f(2n+1)}2^{-2n}$ , i.e., $e^{f(2n+1)}\ge 2^{2n}$ . The rest follows by taking natural logarithm on both sides.
Thus if I can show the fact I will be done. Here is where I am stuck.
By induction, it can be calculated rather easily that $$I=\frac{n\cdot (n-1)\cdots 1}{(2n)(2n-1)\cdots (n+1)}\cdot \frac{1}{2n+1},$$ which according to the fact must be in $e^{-f(2n+1)}\mathbb N.$ Any help is appreciated. Thank you!","['analytic-number-theory', 'number-theory']"
4878783,"Please help me identify any errors in my solution to the following DE: $xf(x)-f'(x)=0$, $f(0)=1$","Context/background: I am self-studying series, first in the context of generating functions and now in the context of functional/differential equations. As such, I like to set myself practise problems, which is what I bring you today. Of course, being self-studying, it can be hard to find anyone to give feedback on my problems and solutions, which is why I bring this one to you today. Thanks for your time. Problem statement: Given the differential equation $xf(x)-f'(x)=0$ and $f(0)=1$ , find $f(x)$ in power series form. Bonus questions: find the closed form of $f(x)$ (if it exists) and the radius of convergence of the power series. Solution: Let $f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+a_3x^3+a_4x^4+...$ Then: $xf(x)=\sum_{n=0}^{\infty}a_nx^{n+1}=a_0x+a_1x^2+a_2x^3+a_3x^4+...$ And: $f'(x)=\sum_{n=0}^{\infty}(n+1)a_{n+1}x^n=a_1+2a_2x+3a_3x^2+4a_4x^3+5a_5x^4+...$ Furthermore: $xf(x)-f'(x)=-a_1+(a_0-2a_2)x+(a_1-3a_3)x^2+(a_2-4a_4)x^3+(a_3-5a_5)x^4+...$ Given that $f(0)=1$ , we must have that $a_0=1$ . Moreover, since the coefficients of $xf(x)-f'(x)$ must all be zero, we have the following (infinite) system of equations: $$-a_1=0\Longrightarrow a_1=0$$ $$a_0-2a_2=0\Longrightarrow a_2=\frac{a_0}{2}$$ $$a_1-3a_3=0\Longrightarrow a_3=0$$ $$a_2-4a_4=0\Longrightarrow a_4=\frac{a_2}{4}=\frac{a_0}{8}$$ $$a_3-5a_5=0\Longrightarrow a_5=0$$ $$a_4-6a_6=0\Longrightarrow a_6=\frac{a_4}{6}=\frac{a_0}{48}$$ The pattern continues, with every odd coefficient equal to $0$ and every odd coefficient equal to $a_0$ divided by the product of every even number up to and including $n$ . As such, it makes sense to define a new sequence of coefficients $c_n$ such that $c_n=a_{2n}$ ; then $c_n=\frac{a_0}{(2n)!!}=\frac{1}{(2n)!!}$ and the power series solution is $f(x)=\sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!!}=1+\frac{x^2}{2}+\frac{x^4}{8}+\frac{x^6}{48}+...$ Radius of convergence Consider $\lim_{n\to\infty}|\frac{c_{n+1}}{c_n}|$ . This is equal to $\lim_{n\to\infty}|\frac{\frac{1}{(2n+2)!!}}{\frac{1}{(2n)!!}}|=\lim_{n\to\infty}|\frac{(2n)!!}{(2n+2)!!}|=\lim_{n\to\infty}|\frac{2n(2n-2)(2n-4)...2}{(2n+2)(2n)(2n-2)(2n-4)...2}|=\lim_{n\to\infty}|\frac{1}{2n+2}|=0$ Since the limit of the ratio of consecutive coefficients is $0$ , the radius of convergence for this series is infinite; i.e. it converges for all $x$ . Closed form solution I'll admit, this one stumped me for a bit. Then I realised that $(2n)!!=2^nn!$ , so we have $\sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!!}=\sum_{n=0}^{\infty}\frac{x^{2n}}{2^nn!}$ . Setting $t=\frac{x^2}{2}$ , we have $\sum_{n=0}^{\infty}\frac{x^{2n}}{2^nn!}=\sum_{n=0}^{\infty}\frac{t^n}{n!}$ . This last series has the very well-known closed form of $e^t$ , so the closed form for our series solution (in $x$ ) is $f(x)=e^{\frac{x^2}{2}}$ . To check said solution: $xf(x)=xe^{\frac{x^2}{2}}$ , and $f'(x)=xe^{\frac{x^2}{2}}$ , so $xf(x)-f'(x)=xe^{\frac{x^2}{2}}-xe^{\frac{x^2}{2}}=0$ . My question: As I mentioned earlier, I'm a self-studier, so it can be hard to find (qualified) feedback. So if there's any you could offer me in regards to my solution — in terms of rigour, flow, leaps of logic, whatever you can think of — I'd really appreciate it. I want to improve!","['power-series', 'calculus', 'closed-form']"
4878814,Probability that the coefficients of a quadratic equation with real roots form a triangle,"Question : What is the probability that the coefficients of a quadratic equation form the sides of triangle given that it has real roots? Assume that the coefficients are uniformly distributed and positive. Note that it is sufficient to assume that the coefficients are uniformly distributed in $(0,1)$ since we can always divide by the largest coefficient to scale all the coefficients to $(0,1)$ . Experimental data : A simulation with $10^{10}$ trails gives the probability as $0.182185$ . This could be a coincidence but this value which agrees with $\displaystyle \frac{\log \pi}{2\pi}$ to five decimal places. Julia code : using Random

step = target = 10^7
count = count_q = qt = 0

while 1 > 0
    count += 1
    random_numbers = rand(3)
    a = random_numbers[1]
    b = random_numbers[2]
    c = random_numbers[3]
    
    if b^2 >= 4*a*c
        count_q += 1
        if (a + b >= c) && (b + c >= a) && (c + a >= b)
            qt += 1
        end
    end
    
    if count_q >= target
        println(count,"" "",count_q/step,"" "",qt,"" "",qt/count_q)
        target += step
    end
end","['integration', 'geometric-probability', 'geometry', 'algebra-precalculus', 'probability']"
4878852,Using integration by parts to solve a differential equation,"I am stuck on the following analysis problem. Suppose that $u\in C([a,b])$ is twice continuously differentiable, $V\in C([a,b])$ , $V(x)\geq 0$ for all $x\in[a,b]$ and $$ -u''(x)+V(x)u(x)=0, \forall x\in[a,b]$$ $$u(a)=u(b)=0$$ Show that $u(x)=0$ for all $x\in[a,b]$ . The hint tells me to use integration by parts to solve this problem, but I can't see where to use it. I tried the following, but it doesn't lead to a meaningful result. Let $v(x)=\int_a^x V(t)dt$ . Then, integrating by parts, we have $$ \int_a^b V(x)u(x)dx = v(b)u(b)-v(a)u(a)-\int_a^bv(x)u'(x)dx = -\int_a^bv(x)u'(x)dx $$ and because $u''(x)=V(x)u(x)$ , $$\int_a^bV(x)u(x)dx = \int_a^bu''(x)dx = u'(b)-u'(a) = -\int_a^bv(x)u'(x)dx $$ I'd appreciate if anyone could tell me where to use the integration by parts in this problem. Thank you!","['integration', 'ordinary-differential-equations', 'real-analysis']"
4878902,Floor of a random variable with bounded density,"Let $X$ be a random variable on $\mathbb R^+$ with bounded density, and let $\lfloor \cdot \rfloor$ denote the floor function. Show that for $\lambda\in \mathbb R^+$ , $$\lim_{\lambda\to\infty} \mathbb P(\lfloor \lambda X\rfloor \mbox{ is even}) = \frac{1}2.$$ The statement makes intuitive sense to me, but I don't know how to show it. I created this simple example to better understand a problem I'm working on where space is partitioned into equivalence classes, (odd / even, in the case of the example) ""uniformly"" spread out over the space, and fine with respect to the extent of the bounded density. If alternative assumptions on $X$ can be used, I would be curious to hear suggestions.","['probability', 'random-variables']"
4878924,Second derivatives of 1/r,"Let $r = \sqrt{x^2 + y^2 + z^2}$ . From the fact that $\nabla^2 r^{-1} = -4\pi \delta^{(3)}(\vec{r})$ , is it correct to say that $$
\frac{\partial^2}{\partial x^2}(r^{-1}) = \frac{3x^2 - r^2}{r^5} - \frac{4\pi}{3} \delta^{(3)}(\vec{r}) \\
\frac{\partial^2}{\partial x \partial y}(r^{-1}) = \frac{3xy}{r^5}
$$ The question is, is it justified to split the Dirac delta evenly in all three directions? This seems like the most straightforward way. $$
\delta = \frac{\delta}{3} + \frac{\delta}{3} + \frac{\delta}{3}
$$ Or maybe there are other ways to distribute, while still respecting symmetry, like $$
\delta = \frac{x^2 \delta}{r^2} + \frac{y^2 \delta}{r^2} + \frac{z^2 \delta}{r^2}
$$","['derivatives', 'dirac-delta']"
4878928,Smoothness of horizontal bundle defined by connection one-form,"Let $G$ denote a Lie group and $\mathfrak{g}$ its Lie algebra. $P$ is a smooth principal bundle. Given a smooth $\mathfrak{g}$ -valued one-form $\omega_p: TP_p \to \mathfrak{g}$ , which fulfils the properties listed below, is there an easy way to see that $p \mapsto ker(\omega_p) =: H_p$ is smooth? For all $p \in P$ and $X \in \mathfrak{g}$ , $\omega_p(\underline{X}_p) = X$ , where $\underline{X}_p$ denotes the fundamental vector field to $X$ . For all $p \in P$ and $g \in G$ , $$\omega_{p \cdot g} \circ (dR_g)_p = Ad_{g^{-1}} \circ \omega_{p}.$$ Here, $R_g$ denotes the right action of $g$ on P and $Ad_g$ the adjoint representation.","['principal-bundles', 'connections', 'lie-groups', 'differential-geometry']"
4878957,"Building a function $f$ such that $\| f - f_n \|_{L^p(B(x,r) \cap \Omega)} \to 0$ as $n \to \infty$ and $f \in L^p_{\text{loc}}(\Omega)$.","Consider an arbitrary open set $\Omega \subset \mathbb R^n$ and an arbitrary element $1 \leqslant p < \infty$ . Moreover, let $(f_n)_{n \in \mathbb N} \subset L^p(B(x,r) \cap \Omega)$ denote a convergent sequence in $L^p(B(x,r) \cap \Omega)$ , for every $x \in \Omega$ and $r > 0$ . In other words, for each $x \in \Omega$ and $r > 0$ there exists a function $f_{x,r} \in L^p(B(x,r) \cap \Omega)$ such that $$ \lim_{n \to \infty}\| f_{x,r} - f_n \|_{L^p(B(x,r) \cap \Omega)} = 0. $$ My goal is, if possible , to build a function $f$ such that $$  \tag{1} f \in L^p_{\text{loc}}(\Omega) \quad \text{ and }\quad  \lim_{n \to \infty}\| f - f_n \|_{L^p(B(x,r) \cap \Omega)} = 0, $$ for every $x \in \Omega$ and $r > 0$ . My attempt. Clearly, to prove the R.H.S condition of $(1)$ , it is sufficient to establish that $$ \lim_{n \to \infty} \| f - f_n \|_{L^p(B(x,r) \cap \Omega)} \leqslant \lim_{n \to \infty} \| f_{x,r} - f_n \|_{L^p(B(x,r) \cap \Omega)}, $$ for every $x \in \Omega$ and $r > 0$ . Furthermore, simple calculations yield $$ \| f - f_n \|_{L^p(B(x,r) \cap \Omega)} \leqslant \| f - f_{x,r} \|_{L^p(B(x,r) \cap \Omega)} + \| f_{x,r}- f_n \|_{L^p(B(x,r) \cap \Omega)}, $$ for every $n \in \mathbb N$ and $x \in \Omega, r > 0$ . This inequality turns out to not provide a viable choice since the functions $f_{x,r}$ are not necessarily equal for different values of $x \in \Omega$ and $r > 0$ . Hence, I am looking for an alternative approach to prove this result. Thanks for any help in advance.","['convergence-divergence', 'functional-analysis', 'examples-counterexamples', 'real-analysis']"
4878974,Find a the Position and Rotation of an Object in 3D Space Given the Pan and Tilt Angle to 3 Other Known Points.,"Goal I want to find the rotation and position of an object in the real world given the pan/tilt or azimuth/elevation angles between the object and 3 or more other points in the room. The rotation of the object should be relative to a fixed room axis. I then want to translate this formula into python code (please feel free to skip this part in the answer, I am mainly trying to figure out the process). If you do want to include code, please do not worry about memory or time performance. Known Information The points A, B, and C in 3D coordinates.
The pan/tilt angles between the light/observer relative to an local up and forward vector. Background Info Using a DMX light (a light with motors built into it), I can find the exact local pan and tilt rotation to point the light at any object in a room by manually adjusting values until the angle is correct. Because these lights use stepper motors the exact angle can be determined to point the light in the correct direction. The origin point in the room is chosen arbitrarily, and the position of the points is measured in the real world using a laser measure and can be accurate to about 10mm. Similar problems While researching this problem I came across a similar question which uses 2D space. My intuition was to break the problem down into 2 2D planes, but I unfortunately am not as good at math as I'd like to believe, and I have a bit of trouble reading mathematical syntax so I was not very successful. Calculate position based on angles between three known points What I Have Tried To clarity, I am not very good at 3D math so I tried to solve the problem by breaking it into 2 different 2D problems I could use that find the pan and tilt angles separately I'll spare you all my messy python code but what I did was the following: Input: 2D room positions [-1, 0], [0, 1], and [1, 0] pan angles -90, 0, and +90 Expected Output: Observer point should be [0, 0] Observer rotation should be 0. Process: Calculate the point of intersection between the bisector of any two positions, and the circle formed by the bisector and those two positions:
Half distance between 2 positions * inverse of slope between 2 positions * cotan(angle/2) Calculate the 2 points intersection between all sets of 2 points and their bisector intersection. Find the common point of intersection between the two sets of intersection points. Comments I think my approach for a point in 2D space is pretty solid, although I do receive NaN points and Inf values from time to time (probably a result of my poor understanding of the math). But I am not really sure how I can scale this up to 3D and how I can adjust the formula to account for the rotation of the object itself. Please feel free to build on my approach or discard it all together. Although I am proud of the attempt I made, I am pretty sure this is not the best way to approach this problem. Diagram I am very sorry about the paint image, I know it is the Comic Sans of diagrams but please bear with me. Concerns Given the use of this formula (creating an accurate digital twin of a lighting fixture), rotation accuracy is very important. Although a minor positional error doesn't make much of a difference over a long distance, and rotational error can vastly offset the final result. The area this is going to be used in is about 100 square meters so the rotation can have some error, but the more accurate the better. Test Data // Positions and rotations use the UnrealEngine coordinate system (Left handed Z Up)
// +Pan rotates the light clockwise, -Pan rotates counterclockwise
// Pan 0 is center
// +Tilt pitches the light downward, -Tilt pitches the light upward
// Tilt 0 is light pointing straight up

// Data I want to find
Light Position = (X=720.081099,Y=-278.892113,Z=905.002818)
Light Nodal Position = (X=720.604299,Y=-279.103379,Z=862.271543)
Light Rotation = (Pitch=-0.288133,Yaw=90.397583,Roll=-179.300499)

// Data I Know
P1 = (X=1655.150801,Y=-102.908631,Z=447.033520)
pan1 = 169.3994∘
tilt1 = -65.80042∘

P2 = (X=1655.150801,Y=-604.529018,Z=746.498843)
pan2 = 199.3925∘
tilt2 = -82.40376∘

P3 = (X=-524.568075,Y=-101.254952,Z=528.269015)
pan3 = 8.697185∘
tilt3 = -76.00046∘ Example of Setup This is not the real venue but instead somewhat similar outdoor venue, please note the upside down lights hanging from the trusses. What I want to find is their location relative to a fixed origin point (lets say the center of the stage on the ground) and the rotation they were installed at. For NDA reasons I cannot show the actual site of the installation, sorry","['euclidean-geometry', 'trigonometry', 'programming']"
4879022,Determine spectrum of a sum $S=T+rG$ of operators on a function space,"I have Markov semigroups $(P_t)_{t\ge0}$ and $(Q_t)_{t\ge0}$ on a measurable space $(E,\mathcal E)$ with pointwise generators $S$ and $T$ , respectively. $S$ and $T$ are related by $$(Sf)(x)=(Tf)(x)+r(x)(Gf)(x)\tag1,$$ where $r:E\to(0,\infty)$ is measurable and $G$ is a bounded linear operator on $F$ ; the space of bounded measurable real-valued functions on $(E,\mathcal E)$ equipped with the supremum norm. I would like to investigate the spectral gap of $(P_t)_{t\ge0}$ defined as $$\sup\left\{\Re(\lambda):\lambda\in\sigma(S)\setminus\{0\}\right\}\tag1.$$ My hope is that it can be related (by a formula) to the spectral gap of $(Q_t)_{t\ge0}$ . The answer to this question clearly depends on whether we are considering $(P_t)_{t\ge0}$ and $(Q_t)_{t\ge0}$ as operator semigroups on $F$ or $L^2(\mu)$ . Here I'm assuming that $\mu$ is a probability measure on $(E,\mathcal E)$ and $(P_t)_{t\ge0}$ is $\mu$ -invariant. $(Q_t)_{t\ge0}$ is not $\mu$ -invariant, but I would assume here that it can be extended to $L^2(\mu)$ nevertheless. I actually don't know how I should approach this. And I'd also like to know if it matters at all whether we consider $F$ or $L^2(\mu)$ . Any help is highly appreciated!","['stochastic-processes', 'markov-process', 'functional-analysis', 'spectral-theory', 'probability-theory']"
4879026,Differential equation with memory,"I am trying to integrate this equation: $$
\frac{dz}{dt} = \alpha \cdot (s(t)-z(t-1))
$$ I came up with this equation to model a ""leaky integrator"" system where s(t) are some samples from the environment and z(t) is a ""decision variable"". Thus this system can continuously decide between 2 alternatives based on the sign of z(t). However, I'm not sure how to integrate the equation and solve for z(t), mainly because of the term z(t-1) which Wolfram Alpha seems to struggle with... Any tips?","['delay-differential-equations', 'ordinary-differential-equations']"
4879077,Maximizing $\sin x \cos x + \cos x$ geometrically,"Using basic calculus, it's easy to see that we have a maximum at $ x = \pi/6$ , but is there a way to prove this geometrically? I can get the $\cos x \sin x$ to appear but I'm stuck here; how should I proceed with maximizing the sum of length of the base + hypotenuse ( $\cos x \sin x + \cos x$ )? Any hints would be appreciated.","['trigonometry', 'geometry']"
4879078,How to efficiently find all solutions of equation $(A + \text{sgn}(x)\text{sgn}(x)^T )x= y x$,"Consider the following equation: $$\left (A + \text{sgn}(x)\text{sgn}(x)^T  \right )x= y \, x  \tag{1} $$ where $A$ is an $n \times n$ symmetric matrix. The variables are $y \in \mathbb R, x \in \mathbb R ^n$ with $\| x \|_2=1$ . Note this can be equivalently written as $$Ax +\|x\|_1 \text{sgn}(x) =y \, x.$$ To avoid trivial or degenerate solutions, we restrict ourselves to the solutions $x \neq 0$ with $\| x \|=1$ ; indeed, any solution $x$ of this equation remains a solution after a linear transformation. Such an equation appears in non-convex optimization, where all of its solutions need to be found so that the problem is fully solved. In fact the original system that needs to be solved is of the form $$\color{blue}{\left [ \left(A + \text{sgn}(x)\text{sgn}(x)^T  \right )x- y \, x \right ] \odot \text{sgn}(x) =0.}, \tag{2} $$ which reduces to the above system if we assume $x_i \neq 0, i=,\dots,n$ . Also, if $A$ is a diagonal matrix, the two systems (1) and (2) become the same. A naïve approach to solve the equation (1) is to fix $\text{sgn}(x)=a$ for any $a \in \{-1,1 \}^n$ , and then solve the system. One can see that if $a$ is considered, $-a$ is not needed to be assessed as both produce the same matrix of $aa^T$ . Hence, $2^{n-1}$ cases should be examined. After fixing $\text{sgn}(x)=a$ , each solution $y$ and $x$ of the resulting system: $$(A + aa^T )x= y x$$ is nothing but an eigenvalue and its normalized eigenvector of matrix $A+aa^T.$ Such a solution is a correct solution for our equation only if $\text{sgn}(x)=a$ or $\text{sgn}(x)=-a.$ When $A=0$ , for each $a$ , $\text{rank}(aa^T)=1$ , and the system has the following $n$ solutions $(x,y)$ : $$(e_1,1),\dots, (e_n,1).$$ I have two related questions: How many solutions can this system have (my guess the number of solutions is of order $O(n)$ though $2^{n-1}$ different possibilities can be obtained by fixing $\text{sgn}(x)$ )? Is there any efficient procedure to find all solutions of the above system (the computational complexity of the naïve approach is of exponential order)?","['systems-of-equations', 'real-analysis', 'matrices', 'calculus', 'linear-algebra']"
4879084,Proof critique - Glueing local isomorphism,"I apologize first for such a frequently asked question. I am not confident about my approach of this problem which is possibly related to glueing local isomorphisms. I am aware of that local isomorphisms do not always glue to global ones - otherwise all locally free sheaf of rank 1 will all be isomorphic. So I guess one needs to have a global morphism first to ensure the compatibility of local information. Problem. Let $\pi:\operatorname{Spec}A\to \operatorname{Spec}B$ be a morphism of affine schemes and $M$ an $A$ -module, hence $\widetilde{M}$ is a quasi-coherent sheaf on $\operatorname{Spec}A$ . Give an isomorphism $\pi_*\widetilde{M}\cong \widetilde{M_B}$ where $M_B$ is simply regarding $M$ as an $B$ -module via the ring map $\varphi:B\to A$ corresponding to the affine scheme morphism. My approach. On the base $\{D(f)\}_{f\in B}$ of $\operatorname{Spec}B$ , we have $$\alpha_{D(f)}:\pi_*\widetilde{M}(D(f))=\widetilde{M}(\pi^{-1}(D(f)))=\widetilde{M}(D(\varphi(f)))\cong M_{\varphi(f)}=(M_B)_f\cong\widetilde{M_B}(D(f))$$ which is compatible with restrictions.
Because both $\pi_*\widetilde{M}$ and $\widetilde{M_B}$ are sheaves on $\operatorname{Spec}B$ , they restrict to sheaves on the base $\{D(f)\}_{f\in B}$ . Hence the $\{\alpha_{D(f)}\}_{f\in B}$ extends to a sheaf morphism (A possible reference is Lemma 6.30.14 https://stacks.math.columbia.edu/tag/009H ) $\alpha:\pi_*\widetilde{M}\to \widetilde{M_B}$ , which is actually an isomorphism since they give isomorphisms on the base. I really find this confusing, which I am not sure if it is a psychological thing. Just in case, I am familiar with extending sheaves on a base using compatible germs. Any help is sincerely appreciated!","['affine-schemes', 'algebraic-geometry', 'schemes', 'sheaf-theory']"
4879164,Problem relating $p$-defect zero characters with their values on a field of characteristic $p$,"Studying Gabriel Navarro's book ""Character Theory and the McKay Conjecture"", I've come across the following problem. First, let's fix some notation: $G$ will be a finite group, $R$ will denote the algebraic integers of $\mathbb{C}$ over $\mathbb{Q}$ , $p$ will be a prime, $M$ will be a maximal ideal of $R$ containing $pR$ and $\mathbb{F}$ will denote the field $R/M$ , where $*:R \to \mathbb{F}$ is the canonical projection. Let $\chi\in \operatorname{Irr}(G)$ and write $\chi^*$ for the function $\chi^*(g) = \chi(g)^*$ . Then: If $\chi_1, ..., \chi_t \in \operatorname{Irr}(G)$ are all distinct and of $p$ -defect zero, then $\chi_1^*, ..., \chi_t^*$ are $\mathbb{F}$ -linearly independent; If $\chi \in \operatorname{Irr}(G)$ has $p$ -defect zero and $x \in G$ is such that $\chi(x)^* \neq 0$ , then $|G|_p = |x^G|_p$ (i.e., the greatest power of $p$ that divides the order of $G$ also divides the size of the conjugacy class of $x$ ); The number of irreducible $p$ -defect zero characters of $G$ is bounded above by the number of conjugacy classes $K$ of $G$ such that $|K|_p = |G|_p$ ; My attempt 1. Suppose $\sum_i \lambda_i \chi_i^* = 0$ . Then we may take $\mu_i \in R$ such that $\mu_i^* = \lambda_i$ . Our goal is $\mu_i \in M$ . Take $\theta = \sum_i \mu_i \chi_i$ . Our hypothesis guarantees that $\theta(G) \subset M$ . We will use 2. and its notation. We have: $$\sum_{i=1}^t \lambda_i \chi_i(1)_{p'}^* \lambda_{\chi_i}(
\hat{K}) = \sum_{i=1}^t \lambda_i \chi_i(1)_{p'}^* \left( \frac{|K|\chi_i(x_K)}{\chi_i(1)} \right)^* = \sum_{i=1}^t \left( \frac{|K|\mu_i \chi_i(x_K)}{p^a} \right)^* = \left( \frac{|K|\theta(x_K)}{p^a} \right)^*$$ where $x_K$ is a representative of the conjugacy class $K$ and $p^a$ is the $p$ -part of $|G|$ . Now, either $\chi_i(x_K)^* = 0$ for all $i$ , or there exists some $i$ such that $\chi_i(x_K)^* \neq 0$ . In this last case, by 2. , the expression above will yield $0$ . All that's left is to show the same for the first case from which it will follow, from the linear independence of the $\lambda_{\chi_i}$ (which can be shown for characters of $p$ -defect zero), that $\lambda_i = 0$ for all $i$ . 2. Let $\lambda_\chi : Z(\mathbb{F}G) \to \mathbb{F}$ be the algebra homomorphism defined by $\lambda_\chi(\hat{K}) = \left( \frac{|K|\chi(x)}{\chi(1)}\right)^*$ , where $K$ is a conjugacy class of $G$ with representative $x$ (that this is well-defined and is a homomorphism was done previously in the book). We can write: $$\left( \frac{|K|\chi(x)}{\chi(1)}\right)^* = \left( \frac{|K|_p |K|_{p'}\chi(x)}{\chi(1)_p \chi(1)_{p'}}\right)^* \in \mathbb{F}$$ i.e., separating out the power of $p$ of the two integers from the rest. Now, by hypothesis, $\left( \frac{\chi(1)_{p'}}{|K|_{p'}\chi(x)}\right)^*$ is a well-defined element of $\mathbb{F}$ . Thus: $$\left( \frac{|K|_p |K|_{p'}\chi(x)}{\chi(1)_p \chi(1)_{p'}}\right)^* \cdot \left( \frac{\chi(1)_{p'}}{|K|_{p'}\chi(x)}\right)^* = \left( \frac{|K|_p}{ \chi(1)_p}\right)^* \in \mathbb{F}$$ This then proves $|K|_p \geq \chi(1)_p$ . As the latter is equal to $|G|_p$ , we get $|K|_p = |G|_p$ , as desired. 3. Here, my guess would be, using 2. , to find an element $x$ of $G$ such that $\chi(x)^* \neq 0$ for one defect zero caracter $\chi$ , but not any other. This, I have no idea how to do. It will probably involve 1. somehow, but I can't see it... Could anyone please provide some hint on how to finish item 1. and how to approach 3. ? Is my intuition for this last item correct and, if so, how to go about applying it? Thanks in advance! PS: I would hazard a guess that this is a particular version of some much more general result for modular characters, but I don't know much of the latter...","['algebraic-integers', 'group-theory', 'finite-groups', 'characters']"
4879235,"Removing an open curve from a regular, simply connected domain's boundary","This is a follow-up to my previous question . I now make my question more precise to avoid the counter-example given there. Let $U \subset \mathbb R^2$ be an open, regular (meaning $U$ is the interior of its closure), bounded and simply connected set. In that situation, $U$ has a connected boundary but it is not necessarily given by a simple closed curve. Let $\gamma : [0,1] \rightarrow \partial U$ be a simple curve such that $\gamma(0) \neq \gamma(1)$ and assume that $\gamma((0,1)) \subset \partial U$ is open in the subspace topology. Is $\partial U \setminus \gamma((0,1))$ connected ? One can note that $K = \partial U \setminus \gamma((0,1))$ is compact and $V = \partial U \setminus \gamma([0,1]) \subset K$ is open. Let $C$ be a connected component of $K$ . According to the lemma in that post , if $C \subset V$ , then $C$ is a connected component of $X = \partial U$ , i.e. $C = \partial U$ which is impossible. Hence $C \nsubseteq V$ and  either $\gamma(0) \in C$ , $\gamma(1) \in C$ or $\{\gamma(0),\gamma(1)\} \subset C$ . In the latter case, $C = K$ and $K$ is connected. Otherwise, $K$ is made of two compact, disjoint, connected components $C_0$ and $C_1$ , $\gamma(i) \in C_i$ , $K = C_0 \cup C_1$ . In particular, $\partial U = C_0 \cup C_1 \cup \gamma((0,1))$ . But is such a boundary possible for a regular simply connected set ? I was thinking of using a separation theorem (such as an hyperplane or Zoretti's theorem, cited in my previous post). Another idea was to use the Riemann Mapping Theorem, i.e. use the biholomorphic map $f : U \rightarrow \{|z| < 1\}$ and try to understand the boundary behaviour (a useful reference is Serge Lang, Complex Analysis, Ch. X.4 : The Riemann Mapping Theorem ). In any case, I think the regularity of $U$ (which is not needed to prove that $\partial U$ is connected) should play a crucial role. Otherwise, it's easy to find a counterexample : take $U = \{z \in \mathbb C : |z| < 1|\} \setminus [-1,0]$ and remove any simple curve $\gamma$ for which $\gamma([0,1]) \subset (-1,0)$ . Thanks in advance.","['connectedness', 'geometry', 'complex-analysis', 'general-topology', 'compactness']"
4879256,A question about the Hardy-Littlewood maximal function,Let $h \in L^1(\mathbb{R}).$ Consider the Hardy-Littlewood maximal function defined as $$h^*(b) = \sup_{t > 0}\frac{1}{2t}\int_{b-t}^{b+t}h.$$ Does it hold that $\{b\in \mathbb{R} : h^*(b) = \infty\}$ is a closed set?,"['measure-theory', 'analysis', 'real-analysis']"
4879272,A domain-covariant notation for functions?,"Note: I'm using the terms ""covariant"" and ""contravariant"" a bit loosely in this question. The standard function notation seems to be naturally codomain-covariant and domain-contravariant. And we have seen examples of this since the very first algebra class. In order to translate a function upwards, we add to the result; in order to translate a function to the right, we subtract from the argument. This generalizes to general compositions of functions. In order to transform the $y$ axis of the graph of some function $y=f(x)$ by some function $g(\cdot)$ , we can just compose it as such: $y_{[y]}=g(f(x))=(g\circ f)(x)$ . However, if we want to transform the $x$ axis of the graph of the function $y=f(x)$ by some function $h(\cdot)$ , we need to compose the argument with the inverse of that function: $y_{[x]}=f(h^{-1}(x))=f\big((I\circ h^{-1})(x)\big)$ . Note that, strictly speaking, the transformation is first and foremost done on the graph rather than the functions, and only then it translates into functional identities through domain contravariance and codomain covariance. This resembles basis transformation and just screams ""Jacobian"" to me, although I don't know where exactly it's hiding and how the covariance/contravariance factors in. The question is, what exactly is it about functional notation in its essence, as a representation of an abstract mathematical object, that makes it naturally act this way? And is it possible to construct a function notation that is covariant with respect to both the domain and the codomain?","['change-of-basis', 'functions', 'transformation']"
4879329,Can a matching be empty and an augmenting path of length 1?,"I'm studying matchings in graph theory. The definition of matching that I have in my textbook states: ""A set M of independent edges in a graph G = (V, E) is called a matching."". Later, the definition of an augmenting path (referring to bipartite graphs with
bipartition { A, B }) is given: ""A path in G which starts in A at an unmatched vertex and then contains, alternately, edges from E $\setminus$ M and from M, is an alternating path with respect to M. An alternating path P that ends in an unmatched vertex of B is called an augmenting path, because we can use it to turn M into a larger matching."". Now my question is: is it possible for an empty matching to exist? In the sense that M = Ø is considered a matching in all respects? And, if so, can a possible augmenting path of M be a single edge between two unmatched nodes?","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4879360,Improved Measure Concentration for small variance random variables.,"Consider the random variable $\delta_1$ with the following probability distribution, where we have: $\mathbb{P}(\delta_1=0) = 1 - 2 \delta$ , $\mathbb{P}(\delta_1=1)= \delta$ , $\mathbb{P}(\delta_1=-1)= \delta$ , where $\delta \ll 1$ is a small number. Let $(\delta_i)_{1 \leq i \leq n}$ by an i.i.d. collection of random variables distributed like $\delta_1$ . I am looking for a good concentration bound for expression $\mathbb{P}(\sum_{i=1}^{n} a_i \delta_i \geq t)$ . A try with Hoeffding's inequality gives that $\mathbb{P}(\sum_{i=1}^{n} a_i \delta_i \geq t) \leq \exp(-t^2/\sum_{i \geq 1} a_i^2)$ . This bound, while exponential does not reflect that the random variables $\delta_i$ are highly concentrated around their mean so we should expect a bit more concentration of measure (Rademacher random variables also give the same bound although they have much higher variance). I tried using a chernoff bound type strategy which seems to yield something like $\mathbb{P}(\sum_{i=1}^{n} a_i \delta_i \geq t) \leq \exp(-(t^2/\sum_{i \geq 1} a_i^2) +n \log \delta)$ . My question is: Is this the best possible bound I can get? I wondering if I can do any better than this. If it makes any difference, in this particular case we have $\delta = n^{- \epsilon}$ for some fixed $\epsilon>0$ . Edit: I would very much like to have something like $\mathbb{P}(\sum_{i=1}^{n} a_i \delta_i \geq t) \leq \exp(-(t^2/\delta^{O(1)} \sum_{i \geq 1} a_i^2)$ , but I am skeptical as to whether this would be possible. In general, is there a way to tell if a concentration bound is 'sharp'? How would one go about proving this?","['statistics', 'probability-limit-theorems', 'probability-theory', 'concentration-of-measure']"
4879386,Implicit details regarding the proof of $\lim_{n \to \infty}\frac{\sqrt[n]{e^1}+\sqrt[n]{e^2}+\cdots\sqrt[n]{e^n}}{n}$,"Problem 9(i) of Spivak's Calculus (Chpt 22) asks for the following limit: $\displaystyle \lim_{n \to \infty}\frac{\sqrt[n]{e^1}+\sqrt[n]{e^2}+\cdots\sqrt[n]{e^n}}{n}$ The terse solution manual's argument reads as follows: $\displaystyle \int_0^1 e^x dx = e-1$ Note: My book has not yet covered infinite series I understand what this argument aims to illustrate, but it seems to me there are several implicit theorems being used here that I do not believe the book has previously addressed (or, at least, not that I am aware of/cannot properly identify). This question is specifically about properly identifying what those theorems are. Firstly, I see that $\frac{\sqrt[n]{e^1}+\sqrt[n]{e^2}+\cdots\sqrt[n]{e^n}}{n}$ is an upper sum representation of an $n$ -subinterval uniform partition $P_n$ on the closed interval $[0,1]$ . As such, given that $e^x$ is integrable on this interval, the $\inf$ of the set of all $U(e^x,P)$ exists (where this notation refers to the upper sum of $e^x$ on all partitions defined on $[0,1]$ ). Next, I know from previous work ( If $S$ is set of lower sums for partitions having $n$-equal subintervals, does $\sup S$ equal the $\sup$ of the set of lower sums for all partitions. ) that the infimum of the set of all uniform partitions of $[0,1]$ is equal to the infimum of the set of all partitions of $[0,1]$ . It seems to me, then, that all I need to do is show that: $\displaystyle \lim_{n \to \infty}\frac{\sqrt[n]{e^1}+\sqrt[n]{e^2}+\cdots\sqrt[n]{e^n}}{n}=\inf\{U(e^x,P_n):\text{$U$ is an upper sum of $e^x$ and $P_n$ is a uniform partition of $[0,1]$}\}$ . If I can prove this, then we are good to go because: \begin{align}e-1=\int_0^1 e^x dx &=\inf\{U(e^x,P):\text{$U$ is an upper sum of $e^x$ and $P$ is a partition of $[0,1]$}\}\\
&=\inf\{U(e^x,P_n):\text{$U$ is an upper sum of $e^x$ and $P_n$ is a uniform partition of $[0,1]$}\}\\
&=\lim_{n \to \infty}\frac{\sqrt[n]{e^1}+\sqrt[n]{e^2}+\cdots\sqrt[n]{e^n}}{n}\end{align} So how exactly do I prove that: $$\displaystyle \lim_{n \to \infty}\frac{\sqrt[n]{e^1}+\sqrt[n]{e^2}+\cdots\sqrt[n]{e^n}}{n}=\inf\{U(e^x,P_n):\text{$U$ is an upper sum of $e^x$ and $P_n$ is a uniform partition of $[0,1]$}\}$$ Through basic partition arguments, I know that: $U(f,P_{a^0}) \geq U(f,P_{a^1}) \geq U(f,P_{a^2}) \geq \cdots\geq U(f, P_{a^m}) \geq\cdots$ for some $a \in \mathbb N$ . but I also know that just because a subsequence is non-increasing does not, in general, mean that the parent sequence is non-increasing. Calculating the upper sums of uniform partitions for the piece-wise function $f=\begin{cases} 0 \quad &\text{ if $x \in [0,1/3]$}\\1 \quad &\text{ if $x \in (1/3,2/3)$}\\0 \quad &\text{ if $x \in [2/3,1]$}\end{cases}$ is a good example of this. So although, for example, the subsequence $U(e^x,P_1),U(e^x,P_2),U(e^x,P_4), \cdots, U(e^x, P_{2^m}), \cdots$ is non-increasing, I am unsure if $U(e^x,P_1),U(e^x,P_2),U(e^x,P_3),U(e^x,P_4),U(e^x,P_5),\cdots,U(e^x,P_m),\cdots$ is non-increasing, as well (if I could confirm this, then we are good to  go). While I could show that: $$\frac{\sqrt[n]{e^1}+\sqrt[n]{e^2}+\cdots\sqrt[n]{e^n}}{n} \geq \frac{\sqrt[n+1]{e^1}+\sqrt[n+1]{e^2}+\cdots\sqrt[n+1]{e^{n+1}}}{n+1} $$ I feel like there must be a more general theorem unrelated to the particular choice of $e^x$ as my function...perhaps if $f$ is continuous then we can say something useful? Edit To flesh out some additional details offered by peek-a-boo , Chapter 13's appendix has the following theorem: Suppose that $f$ is Darboux integrable on $[a,b]$ . Then for every $\varepsilon \gt 0$ there is some $\delta \gt 0$ such that, if $P=\{t_0=a,t_1,\cdots,t_{n-1},t_n=b\}$ is any partition of $[a,b]$ with all lengths $t_i-t_{i-1} \lt \delta$ , then: \begin{align} \left| \sum_{i=1}^nf(x_i)(t_i-t_{i-1})-\int_a^b f(x)dx\right| \lt \varepsilon\end{align} As indicated by peak-a-boo (and others), $\frac{\sqrt[n]{e^1}+\sqrt[n]{e^2}+\cdots\sqrt[n]{e^n}}{n}$ is a Riemann sum of $e^x$ on the closed interval $[0,1]$ where $x_i=\frac{i}{n}$ . By the aforementioned theorem, we know that for an arbitrary $\varepsilon$ : $$\left| \frac{\sqrt[n]{e^1}+\sqrt[n]{e^2}+\cdots+\sqrt[n]{e^n}}{n}-\int_0^1 e^xdx\right| \lt \varepsilon$$ where $\frac{1}{n} \lt \delta$ . However, this theorem also tells us that $$\left| \frac{\sqrt[n+j]{e^1}+\sqrt[n+j]{e^2}+\cdots+\sqrt[n+j]{e^{n+j}}}{n+j}-\int_0^1 e^xdx\right| \lt \varepsilon$$ for any $j \in \mathbb N$ because if $1/n \lt \delta$ , then $1/(n+j) \lt \delta$ . As such, for any $\varepsilon$ , we have found an element ( $N=n-1$ ) such that if $m \gt N: \left| \frac{\sqrt[m]{e^1}+\sqrt[m]{e^2}+\cdots+\sqrt[m]{e^{m}}}{m}-\int_0^1 e^xdx\right| \lt \varepsilon$ , meaning that $\lim_{n \to \infty}\frac{\sqrt[n]{e^1}+\sqrt[n]{e^2}+\cdots+\sqrt[n]{e^n}}{n}=\int_0^1 e^xdx$","['integration', 'proof-explanation', 'calculus', 'sequences-and-series', 'limits']"
4879388,A question on Beukers proof of irrationality of $\zeta(3)$,"I am reading the paper A note on the Irrationality of ζ(2) and ζ(3) by F. Beukers .
In equation $(7)$ , we have $$I_n=\int_{(0,1)^3}\frac{x^n(1-x)^ny^n(1-y)^nw^n(1-w)^n}{(1-(1-xy)w)^{n+1}} dx dy dw \tag{1}$$ Using this above equation, I need to prove that there exists $A_n,B_n\in\mathbb{Z}$ such that $$I_n=\frac{A_n+B_n\zeta(3)}{d_n^3}\tag{2}$$ where $d_n=  \operatorname{lcm}  (1,2,...,n)$ I tried using $n$ -fold partial integration by parts with respect to $w$ in $(1)$ by taking $u=w^n(1-w)^n$ and $v=\frac{x^n(1-x)^ny^n(1-y)^n}{(1-(1-xy)w)^{n+1}}$ , so that $$I_n=\int_{(0,1)^3}\frac{x^n(1-x)^ny^n(1-y)^nP_n(w)}{(1-(1-xy)w)(1-xy)^n} dx dy dw$$ where $P_n(w)=\frac{d^n}{dw^n}(\frac{w^n(1-w)^n}{n!})$ is the shifted Legendre Polynomial. Now $$P_n(w)=\sum_{i=0}^{n}(-1)^i \binom{n}{i}\binom{n+i}{i} w^i$$ $$(1-x)^n=\sum_{j=0}^{n}(-1)^j \binom{n}{j} x^j$$ $$(1-y)^n=\sum_{k=0}^{n}(-1)^k \binom{n}{k} y^k$$ $$\frac{1}{1-(1-xy)w}=\sum_{r=0}^{\infty}(1-xy)^r w^r$$ So $I_n$ becomes $$I_n=\sum_{i,j,k=0}^n(-1)^{i+j+k}\binom{n}{i}\binom{n+i}{i}\binom{n}{j}\binom{n}{k}\sum_{r=0}^{\infty}\int_{(0,1)^3} \frac{x^{n+j} y^{n+k} w^{i+r}(1-xy)^r}{(1-xy)^n}dx dy dw$$ Thank You.","['number-theory', 'analysis', 'complex-analysis', 'analytic-number-theory', 'riemann-zeta']"
4879415,Existence of disjoint paritions of $\mathbb{Z}_m$,"For any integer $m \in \mathbb{N}$ , let $\mathbb{Z}_m$ denote the
integers modulo $m$ . For any two subset $A, B \subset \mathbb{Z}_m$ ,
and $x \in \mathbb{Z}_m$ , denote $s(A,B,x) := \vert \{(a,b) \mid a \in A, b \in B, a+b = x \} \vert$ . For any 2-partition $A,B$ of $\mathbb{Z}_m$ , denote $c(A, B) := \max_{x \in \mathbb{Z}_m} \vert s(A,A,x) + s(B,B,x) - 2 \times s(A,B,x) \vert$ Show that for every odd $m$ , there exists a 2-partition of $\mathbb{Z}_m, (A,B)$ such that $c(A,B) = \mathcal{O} (\sqrt{m \log  m})$ I tried doing the following: Fix a $x \in \mathbb{Z}_m$ and consider the graph $G = (\mathbb{Z}_m, E)$ , where $ab \in E$ if $a+b = x$ . This graph would be a matching on $\mathbb{Z_m} \setminus \{u\}$ where $u$ is some element of $\mathbb{Z}_m$ and a loop on $u$ . Hence, our question basically becomes to find vertex cut of $G$ such that $C(A,\mathbb{Z}_m\setminus A) = \mathcal{O}(\sqrt{m\log m})$ I tried setting up a randomly sampled set $S$ such that $\Pr(x \in S) = p$ ( we will fix $p$ later ) and tried finding value of $c(S, \mathbb{Z}_M \setminus S)$ but I was not getting an appropriate result. Any hints or solution would be appreciated.","['graph-theory', 'probabilistic-method', 'combinatorics', 'probability']"
4879451,Is $\text{BRANCH}(n)$ finite for $n > 2$?,"Is $\text{BRANCH}(n)$ finite for $n > 2$ ? Define $\text{BRANCH}(n)$ as
the maximum length of a string that is composed of at most $n$ unique
characters AND meets the following condition: Define a substring as all letters from positions $i$ to $2i$ (inclusive) (where $i= 1, 2, 3...$ ). Then, a certain substring must
not be allowed to have previous substrings embedded in it, or else the
branch stops. $\text{BRANCH}(1) = 3$ , since the longest string is aaa . The first substring from positions $1$ to $2$ is aa . The theoretical substring from positions $2$ to $4$ would be aaa . However, since aa (substring from $1$ to $2$ ) is embedded inside aaa (substring from $2$ to $4$ ), we cannot make our string aaaa and stop at aaa . $\text{BRANCH}(2) = 11$ , since the longest string is abbbaaaaaaa . At positions $1$ to $2$ , we have substring ab . At positions $2$ to $4$ , we have substring bbb . ab isn't embedded in this, we move on. At positions $3$ to $6$ , we have substring bbaa . None of ab or bbb are embedded in this, we move on. At positions $4$ to $8$ , we have substring baaaa . None of ab , bbb , and bbaa are embedded in this, we move on. At positions $5$ to $10$ , we have substring aaaaaa . None of ab , bbb , bbaa , and baaaa are embedded in this, we move on. However, there's a problem here, since the next sequence from $6$ to $12$ will embed at least one of the previous substrings, so we just add a letter at position $11$ and stop. I can reason that $$\text{BRANCH}(3)\geq1+10\sum_{i=1}^{10}\sum_{j=1}^{10-i}{10 \choose i}{10 - i \choose j}=570021$$ The true value for $\text{BRANCH}(3)$ , if finite, is likely much larger. Moreover, is there any way to prove that $\text{BRANCH}(n)$ is contained by $\text{TREE}(n)$ (a "" $1$ -dimensional"" version of $\text{TREE}(n)$ )?. EDIT: For clarification on ""embedding"" For any substring $S_i$ : [ $i$ , $2i$ ], there is a set of previous substrings { $S_k$ : [ $k$ , $2k$ ]}, where $k = 1,2,..., i-1$ . If any member of this set is a valid substring of $S_i$ (that is, if it's the same as $S_i$ from positions $a$ to $b$ for any $a$ and $b$ where $0 \leq a < b \leq i+1$ ), then that member is ""embedded"" in $S_i$ and the sequence ends at length $2i - 1$ . As an example, consider the string abaaab . The substring $S_3$ from position $3$ to $6$ is aaab . In the set of previous substrings { $S_1$ , $S_2$ } = { ab , baa }, $S_1$ ( ab ) is the same as $S_3$ ( aaab ) from position $3$ to $4$ , so $S_1$ is embedded in $S_3$ .","['graph-theory', 'trees', 'combinatorics', 'big-numbers']"
4879462,"In the definition of outer measure, can we replace ""open intervals"" by ""disjoint open intervals""","The definition of the outer measure of a set $A\subseteq\mathbb{R}$ is as follows: $$
|A| = \inf \left\{ \Sigma_{k=1}^{\infty}\ \mathscr{l}(I_k): I_1, I_2,\dots\text{ are open intervals such that }A\subseteq\bigcup_{k=1}^{\infty} I_k \right\}.
$$ My quick observation is that if two open intervals intersect, we can join them together to create another open interval. So starting from $I_1$ , apply this process for every $I_k$ and we get a countable list of disjoint open intervals $O_1, O_2,\dots$ such that $A\subseteq\bigcup_{k=1}^{\infty} O_k$ and $\Sigma_{k=1}^{\infty}\ \mathscr{l}(I_k)\ge\Sigma_{k=1}^{\infty}\ \mathscr{l}(O_k)$ . Since we're taking the $\inf$ of the length sum, we can replace $I_k$ by $O_k$ . So I wonder why the definition uses ""open intervals"" instead of ""disjoint open intervals"", or if using ""disjoint open interval"", we will get a different set function other than the outer measure?","['measure-theory', 'definition', 'outer-measure', 'real-analysis']"
4879474,Is this method of solving finite nested square roots of 2 via Gray code correct?,"The ""Nested square roots of 2"" section of the Wikipedia entry ""Nested radical"" ( https://en.m.wikipedia.org/wiki/Nested_radical ) describes some properties of finite nested square roots of 2 which have the following general form $\sqrt{2\pm\sqrt{2\pm\sqrt{2\pm\ldots\pm\sqrt{2}}}}$ While it is stated that this expression is related to sine and cosine, it is not clear to me from the information given there or from other related articles on the Internet what the parametric form of the trigonometric expression is that is equal to the radical expression above, nor how to which we can determine the unknown signs when the trigonometric expression is given. However, I had worked on this problem many years ago and I believe that my solution, which I present here, is correct. What I didn't know was that my method was related to the Gray code ( https://en.m.wikipedia.org/wiki/Gray_code ). I found this out when I searched the OEIS for the relevant sequences resulting from my method (even my method of converting an integer to Gray code, which I am listing at the end of this post, is different from the established method as I was unaware of the existence of this code in those days, however it can easily be proved that the new method is just as correct). I present my method briefly, in the form of a mathematical textbook. A proof of these results would be desirable. It is about these two relationships: $2\sin\left(\frac{90^\circ(2m+1)}{2^n}\right)=\sqrt{2_1-\sqrt{2_2\pm\sqrt{2_3\pm \ldots\pm\sqrt{2_n}}}}$ $2\cos\left(\frac{90^\circ(2m+1)}{2^n}\right)=\sqrt{2_1+\sqrt{2_2\pm\sqrt{2_3\pm \ldots\pm\sqrt{2_n}}}}$ where the following restrictions apply: $n=1,2\Rightarrow m=0$ $n\ge2\Rightarrow 0\le m\le 2^{n-2}-1$ If the values of $n, m$ are given, we can determine the values of the string of signs $\pm$ in the nested radicals. That is, we have an signs equation, which we solve as follows: We convert the number m to its Gray code (binary) and then map the digits to the signs $1=-$ and $0=+$ . The resulting string is the requested solution of the signs equation. example with $n=8, m=57$ : $2\sin\left(\frac{90^\circ(2\cdot57+1)}{2^8}\right)=\sqrt{2-\sqrt{2\pm\sqrt{2\pm\sqrt{2\pm\sqrt{2\pm\sqrt{2\pm\sqrt{2\pm\sqrt{2}}}}}}}}$ The Gray code of $57$ is $100101$ , which corresponds to the string $-++-+-$ , so the solution is: $2\sin\left(\frac{90^\circ(2\cdot57+1)}{2^8}\right)=\sqrt{2-\sqrt{2-\sqrt{2+\sqrt{2+\sqrt{2-\sqrt{2+\sqrt{2-\sqrt{2}}}}}}}}$ Determining the values ​​of the parameters $n$ and $m$ of the trigonometric expression when the radical expression is known it is too simple a process to need a separate description here. My method for converting an integer to Gray code (binary) uses successive divisions by powers of $2$ and looks at the parity of the rounded quotient. eg: $29/2=14.5\thicksim15\rightarrow1$ $29/4=7.3\thicksim7\rightarrow1$ $29/8=3.6\thicksim4\rightarrow0$ $29/16=1.8\thicksim2\rightarrow0$ $29/32=0.9\thicksim1\rightarrow1$ The decimal value $29$ has the binary value $10011$ in Gray code.","['nested-radicals', 'trigonometry', 'functions']"
