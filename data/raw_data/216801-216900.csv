question_id,title,body,tags
4411737,Existence of a simple graph following some conditions,"Suppose, $a_1 < a_2 < \cdots < a_k$ are distinct positive integers. I am trying to prove that there exists a simple graph with $(a_k + 1)$ many vertices, whose set of distinct vertex degrees is $a_1, a_2, \cdots , a_k$ . I was trying to use induction on k. I solved using induction when $a_i = i , \forall i$ .
But could not do the general case. Any help will be appreciated. Thanks in advance.","['graph-theory', 'discrete-mathematics']"
4411746,Is the inverse function of an algebraic function algebraic?,"Let $f$ be a real-valued function on some open subset of $\mathbb{R}$ . We say that it is algebraic if there exists a polynomial $P \in \mathbb{R}[x,y]$ such that $P(x,f(x))=0$ identically on the domain of $f$ . Let us assume that $f$ admits an inverse: there is a function $g$ on some open subset of $\mathbb{R}$ such that $f(g(y)) = y$ and $g(f(x))=x$ whenever these equations make sense. Is $g$ also an algebraic function?","['functions', 'abstract-algebra']"
4411779,Show that $\frac{\binom{n}{1}}{1} + \frac{\binom{n}{2}}{2} + \frac{\binom{n}{3}}{3} + \cdots + \frac{\binom{n}{n}}{n} = \sum_{r=1}^{n}\frac{2^r-1}{r}$,"Good day, I was solving this problem: Show that $$\frac{\binom{n}{1}}{1} + \frac{\binom{n}{2}}{2} + \frac{\binom{n}{3}}{3} + \cdots + \frac{\binom{n}{n}}{n} = \sum_{r=1}^{n}\frac{2^r-1}{r}$$ I had already made some progress here , and we can prove using induction $$\int_{0}^{1}{\frac{(1 + x) ^ {n} - 1}{x}}dx = \sum_{r=1}^{n}\frac{2^r-1}{r}$$ But this seems too lengthy. Is there a shorter, more elementary solution? Thanks","['binomial-coefficients', 'combinatorics', 'binomial-theorem', 'sequences-and-series']"
4411781,Rudin's PMA: theorem 10.39,"This theorem is used in the proof of our theorem. Here is our  theorem and its proof : My question is that, how are the representations of $(118)$ and $(121)$ equal of each other? Any help would be appreciated.","['multivariable-calculus', 'calculus', 'analysis', 'real-analysis']"
4411786,Closed rectifiable curve with arbitary winding number.,"This is an exercise from Conway's Functions of One Complex Variable (page 83, exercise 2). Give an example of a closed rectifiable curve $\gamma$ in $\mathbb C$ s.t. for any integer $k$ there is a point $a\notin \{\gamma\}$ with $n(\gamma;a)=k$ . My solution: $$ \gamma_1(t)=\begin{cases}\exp\{\frac{-1+i}{t}\} &t\in(0,1]\\0&t=0\end{cases}$$ is a rectifiable curve since $$\int_0^1|\gamma'(t)|\mathbb d t=\int_0^1\frac{\sqrt2}{t^2}e^{-1/t}\mathrm dt=\int_1^{\infty}{\sqrt2}{}xe^{-x}\mathrm dx=2\sqrt 2e^{-1}$$ If we connect $0$ and $\exp\{{-1+i}\}$ by a segment, then we obtain a desired closed rectifiable curve $\gamma$ . I can see the number of times $\gamma$ orbits about a given point intuitively, but how to compute the winding number rigorously? (Besides, Conway hasn't introduce homotopy there, so I don't know if there are other ways to evaluate the winding number except finding a curve homotopic to the original one?)","['complex-analysis', 'winding-number', 'complex-integration', 'cauchy-integral-formula']"
4411813,Radon-Nikodym derivative of pushforward of Lebesgue measure by differentiable function with respect to Lebesgue measure,"Here is the set up: $f:\mathbb{R}^n\to\mathbb{R}^m$ is a measurable function $\lambda^n$ is the $n$ -dimensional Lebesgue measure $f_*\lambda^n$ is the pushforward of $\lambda^n$ by $f$ $f_*\lambda^n \ll \lambda^m$ What do we know about the Radon-Nikodym derivative $$
\frac{d f_*\lambda^n}{d\lambda^m}
$$ for different types of $f$ ? Notice this Radon-Nikodym derivative exists as long as $f_*\lambda^n$ and $\lambda^m$ are sigma-finite measures on the same space , which is true in this setup. Examples For instance, using the Change of Variables formula and Integration by Substitution when $f$ is a diffeomorphism , the Radon-Nikodym derivative is the absolute determinant Jacobian (see Billingsley ""Probability and Measure"", Theorem 17.2) $$
\frac{d f_*\lambda^n}{d\lambda^m} = |\det J_f|.
$$ I have an intuition, based on this question, (but I have never seen it proven or mentioned anywhere) that when $f$ is simply differentiable , then the Radon-Nikodym derivative is the multidimensional Jacobian defined in Federer's ""Geometric Integration Theorem"" (Lemma 5.1.4) $$
\frac{d f_*\lambda^n}{d\lambda^m} = \begin{cases}
    \mathcal{J}_n f(x) = |\det J_f(x)| && \text{ if } n = m \\
    \mathcal{J}_n f(x) = \sqrt{\det J_f(x)^\top J_f(x)} && \text{ if } n \leq m \\
    \mathcal{J}_m f(x) = \sqrt{\det J_f(x) J_f(x)^\top} && \text{ if } n \geq m
\end{cases}
$$ However I have no idea how to go about proving this.","['measure-theory', 'lebesgue-measure', 'geometric-measure-theory', 'jacobian', 'multivariable-calculus']"
4411837,Identifying hyperelliptic curves as modular curves,"I am reading Poonen's paper The Complete Classification of Rational Preperiodic Points of Quadratic Polynomials over Q: A Refined Conjecture . Several proofs require recognizing certain hyperelliptic curves are actually modular curves, for example, $y^2 = x^6 + 2x^5 + 5x^4 + 10x^3 + 10x^2 + 4x + 1$ $y^2 = x^6 + 2x^5 + x^4 + 2x^3 + 6x^2 + 4x + 1$ Question In general how do we know these are modular curves/map it back to some well known curves? Poonen has a remark The curves were originally recognized as $X_1(13)$ and $X_1(18)$ by computing enough invariants (such as the genus, automorphism group, primes of bad reduction, and Mordell-Weil group of the Jacobian) that the result could be guessed. It seems like some luck is needed here. Since the paper was written almost 30 years ago, is our approach today still the same? Thank you.","['algebraic-geometry', 'computational-number-theory']"
4411842,"Range of $\phi, \theta$ in $\int_0^{\pi/4} \int_0^{\pi/2} \int_0^{2\sin\phi \sin\theta} \rho^3\sin\phi \sin\theta d\rho d\theta d\phi$","The question: A solid bounded by the (y,z)-plane, the (x,y)-plane, the cone $x^2 + y^2 = z^2$ , and the surface $x^2 + y^2 + z^2 - 2y = 0$ . Suppose a density of a chunk of metal of the shape of this solid at the point $(x, y, z)$ is $\sqrt{ x^2 + y^2 + z^2 }$ .
Find the mass of the chunk of metal. So far I have $$\int_0^{\pi/4} \int_0^{\pi/2} \int_0^{2\sin\phi \sin\theta} \rho^3\sin\phi  d\rho d\theta d\phi  $$ but I'm unsure about the range for $\phi$ and $\theta$ ?","['integration', 'multivariable-calculus', 'spherical-coordinates', 'multiple-integral']"
4411883,"Find two sequences $(a_n)$ and $(a_n)$ such that lim$(a_n)$ = lim$(b_n)$ = 0, but lim$(a_n)^{(b_n)}$ = 2022","This is a problem from my real analysis class. I've tried beginning with guesses for the sequences $(a_n)$ and $(b_n)$ to obtain an indeterminate form $0^0$ so that I can take the natural log of $(a_n)^{(b_n)}$ . Then using log properties, I tried to apply L'Hospital's rule and find the limit. I think this is probably the right approach, but so far my guesses have proved inaccurate (i.e. $(a_n)$ = $\frac{1}{n}$ or $\frac{1}{n^2}$ ; have also tried playing around with putting 2022 or ln(2022) in both the numerator and denominator of one of the sequences). I've also tried working backwards, i.e. starting with lim( $(a_n)^{(b_n)}$ ) = 2022 to try to find an $(a_n)$ and $(b_n)$ that work, but neither approach has proved particularly helpful. Hoping for a way to think about this logically to understand the form of the desired sequences.","['limits', 'sequences-and-series', 'real-analysis']"
4411951,Gap between two consecutive order statistics under arbitrary distribution.,"Consider an arbitrary distribution $\mathcal{D}$ supported on $[a,b]$ with density function $\phi(x)\in[\gamma, \Gamma]$ where $\Gamma\geq \gamma>0$ . M i.i.d samples $\{d_i\}_{i=1}^M$ are drawn from $\mathcal{D}$ . Denote $\{d_{k_j}\}_{j=1}^M$ be the ordered sequence: $d_{k_1}\leq d_{k_2}\leq\ldots\leq d_{k_M}$ . I wonder whether we have some gap bound on $\max_{j\in [M]}|d_{k_j}-d_{k_{j-1}}|$ ? Specifically, I wonder what is the bound on $Pr(\max_{j\in [M]}|d_{k_j}-d_{k_{j-1}}|\geq \epsilon)$ . Thanks!","['concentration-of-measure', 'statistics', 'law-of-large-numbers', 'probability']"
4411957,Tietze transformations and the trivial group,"Suppose you have a finite presentation of a group and you want to determine if it yields the trivial group. We know this is unsolvable in general. But say you start from the trivial group and “scramble” it by iteratively using Tietze transformations. Does this mean that once there are sufficiently many interactions introduced (via the Tietze transformations) then unsolvability arises somehow through combinatorial effects? Using randomness on the above application of Tietze transformations is presumably necessary (select a random generator or relation, and / or continue to apply Tietze transformations for a random number of iterations). The randomness is needed as otherwise you would be able to reverse the algorithm? The above is predicated on all presentations of a trivial group can be obtained from starting with a presentation with a singleton set of generators (identity) and singleton relator (identity) and then applying Tietze transformations. Is this valid, or does there exist a trivial group presentation that cannot be obtained this way?","['combinatorial-group-theory', 'group-theory', 'decidability']"
4411967,State-space with second derivative input variable,"I have a second order ODE with derivative terms $$ \alpha_1\frac{d^2i}{dt^2}+\alpha_2\frac{di}{dt}+\alpha_3i=-\beta_1V_r-\beta_2\frac{dV_r}{dt}-\beta_3\frac{d^2V_r}{dt^2} $$ I want to transform this system into state-space, but I'm not sure how to manage the second derivative input term of $V_r$ . I currently have defined $x_1$ and $x_2$ as the following. $$ x_1=I\left(t\right) $$ $$ x_2=\dot{I}\left(t\right)+\frac{\beta_2}{\alpha_1}V_r+\frac{\beta_3}{\alpha_1}{\dot{V}}_r$$ Its derivatives are given as $$ {\dot{x}}_1=\ \dot{I}\left(t\right)=\ x_2-\frac{\beta_2}{\alpha_1}V_r-\frac{\beta_3}{\alpha_1}{\dot{V}}_r$$ $$ {\dot{x}}_2=\ \ddot{I}\left(t\right)+\frac{\beta_2}{\alpha_1}{\dot{V}}_r+\frac{\beta_3}{\alpha_1}{\ddot{V}}_r=\ -\frac{\alpha_3}{\alpha_1}I-\frac{\alpha_2}{\alpha_1}\dot{I}-\frac{\beta_1}{\alpha_1}\ V_r $$ $$ where,  \dot{I} = x_2-\frac{\beta_2}{\alpha_1}V_r-\frac{\beta_3}{\alpha_1}{\dot{V}}_r$$ $$ {\dot{x}}_2=\ -\frac{\alpha_3}{\alpha_1}x_2-\frac{\alpha_2}{\alpha_1}\left(x_2-\frac{\beta_2}{\alpha_1}V_r-\frac{\beta_3}{\alpha_1}{\dot{V}}_r\right)-\frac{\beta_1}{\alpha_1}\ V_r$$ Because I still have a $\dot{V}_r$ term in both $\dot{x}_1$ and $\dot{x}_2$ , I am unable to transform the the system into the state-space representation. Let me know if you have any suggestions.","['control-theory', 'linear-control', 'ordinary-differential-equations', 'dynamical-systems']"
4412055,"If $\pi_t$ is the jump measure of a Lévy process and $B_1,\ldots,B_k$ are disjoint, then $\pi_t(B_1),\pi_t(B_k)$ are independent","Let $E$ be a normed $\mathbb R$ -vector space; $(\Omega,\mathcal A,\operatorname P)$ be a probability space; $(\mathcal F_t)_{t\ge0}$ be a filtration on $(\Omega,\mathcal A)$ ; $(X_t)_{t\ge0}$ be an $E$ -valued càdlàg $(\mathcal F_t)_{t\ge0}$ -Lévy process on $(\Omega,\mathcal A,\operatorname P)$ ; $\tau^B_0:=0$ and $$\tau^B_n:=\inf\underbrace{\left\{t>\tau^B_{n-1}:\Delta X_t\in B\right\}}_{=:\:I^B_n}$$ for $n\in\mathbb N$ and $B\in\mathcal B(E)$ ; $$\pi_t(B):=\sum_{\substack{s\in[0,\:t]\\\Delta X_s\ne0}}1_B\left(\Delta X_s(\omega)\right)=\left|\left\{s\in(0,t]:0\ne\Delta X_s\in B\right\}\right|$$ for $B\in\mathcal B(E)$ and $t\ge0$ . Let $k\in\mathbb N$ and $B_1,\ldots,B_k\in\mathcal B(E)$ be disjoint. How can we show that $\left(\pi_1(B_1),\ldots,\pi_k(B_k)\right)$ is independent? (Maybe we need to assume that $0\not\in\overline{B_i}$ for all $i\in\{1,\ldots,k\}$ .) Since $B_1,\ldots,B_k\in\mathcal B(E)$ are disjoint, it clearly holds $$I^{B_i}_m\cap I^{B_j}_n=\emptyset\tag1$$ for all $i,j\in\{1,\ldots,k\}$ and $m,n\in\mathbb N$ . But how do we need to proceed? I don't know if it is useful, but we may note the following: If $B\in\mathcal B(E)$ with $0\not\in\mathcal B(E)$ , then $\pi_t(B)<\infty$ and $$\pi_t(B)=\sum_{n\in\mathbb N}1_{[0,\:t]}\left(\tau^B_n\right)\tag1$$ (since $\tau^B_n\xrightarrow{n\to\infty}\infty$ , the sum on the right-hand side is finite) for all $t\ge0$ . Moreover, we can show that $\tau^B_1$ is exponentially distributed and if we define $\tilde\Omega:=\left\{\tau^B_1<\infty\right\}$ , $\tilde{\operatorname P}[A]:=\operatorname P\left[A\mid\tilde\Omega\right]$ for $A\in\tilde{\mathcal A}:=\left.\mathcal A\right|_{\tilde\Omega}$ and $\tilde\tau^B_n:=\left.\tau^B_n\right|_{\tilde\Omega}$ for $n\in\mathbb N$ , then $\left(\tilde\tau^B_n-\tilde\tau^B_{n-1}\right)_{n\in\mathbb N}$ is independent and identically distributed on $\left(\tilde\Omega,\tilde{\mathcal A},\tilde{\operatorname P}\right)$ .","['levy-processes', 'measure-theory', 'stochastic-processes', 'stopping-times', 'probability-theory']"
4412084,"If $G,H$ are finite groups, then $G\times G\cong H \times H$ implies $G \cong H$ [duplicate]","This question already has answers here : If $\lvert\operatorname{Hom}(H,G_1)\rvert = \lvert\operatorname{Hom}(H,G_2)\rvert$ for any $H$ then $G_1 \cong G_2$ (3 answers) Closed 1 year ago . Proposition . Let $G,H$ be finite groups (abelian or not). Then the following implication holds: $$G\times G\cong H\times H \Rightarrow G\cong H.$$ In the case of $G,H$ both abelian, one can use the structure theorem (by looking at the invariant factors). Otherwise, I found an argument here , which is a bit unclear to me. I will rephrase it in this post: Lemma . For a finite group $G$ , the map ${\rm FinGr}\rightarrow \mathbb{N}, A\rightarrow |\text{Hom}(A, G)|$ (i.e. $A$ goes to the number of group homomorphisms from $A$ to $G$ ) uniquely determines $G$ , up to isomorphism. Or equivalently, if $G_1$ and $G_2$ are two finite groups, such that for any finite group $A$ we have: $|\text{Hom}(A, G_1)|=|\text{Hom}(A, G_2)|$ , then $G_1\cong G_2$ . Proof proposition : From $G\times G\cong H\times H$ we get $|\text{Hom}(A, G\times G)|=|\text{Hom}(A, H\times H)|$ , for any finite group $A$ . From the universal property of the direct product, we have: $|\text{Hom}(L, K\times K)|=|\text{Hom}(L, K)|^2$ , for any groups $L,K$ . We conclude that: $|\text{Hom}(A, G)|=|\text{Hom}(A, H)|$ for any finite group $A$ . Using the previous lemma, we get the desired conclusion. My question is: how do we prove this lemma? I tried some particular cases of $A$ , cyclic, simple, but it doesn't bring much into light. ( EDIT : My fault, I overlooked an answer to the lemma here , which is not trivial and I still have hard time understanding it). Anyway, if you have a more ellegant/elementary approach to the original proposition, you are more than welcome to share it. Thank you in advance!","['direct-product', 'group-theory', 'group-isomorphism', 'finite-groups']"
4412174,What's the probability of me being obliterated if Thanos snaps his fingers twice?,"Thanos snaps his fingers and half the Earth's population disappear. He snaps again and half of the remaining half disappear. Now, what is the probability of any given person (for example, myself) disappearing after 2 snaps? My simplistic reasoning was that, since after 2 snaps 75% of the population are gone, it means that any one person (e.g. me) has a 75% chance of having been eliminated as well. My friend's asking for a formula, i.e. mathematical proof of this calculation and the best I can come up with is: event (snap): 50% chance of being obliterated event (snap): 50% chance of having survived the first snap and another 50% chance of perishing so 50% x 50% = 25%
Sum of two events = 50% + 25% = 75% Could someone please verify if my logic is sound and what's the correct math behind it. Thanks so much!",['probability']
4412231,Mean value property for harmonic functions,"Consider a bounded harmonic function $u:\mathbb{R}^p \to \mathbb{R}$ (i.e. $u$ is a $C^2$ function such that the Laplacian $\Delta u=0$ ). Prove, without using Liouville's theorem, the following version of the mean value property: $$\forall x \in \mathbb{R}^p,\; u(x)=\frac{1}{2^p}\int\limits_{[-1,1]^p}u(y+x)dy$$ How can we prove it?","['measure-theory', 'lebesgue-integral', 'harmonic-functions', 'real-analysis']"
4412260,Can non-convex optimization problems have closed form solutions?,"In the realm of statistical modelling, creating a statistical model with respect to some data involves optimizing some mathematical function (e.g. Loss Function, OLS Equation, Maximum Likelihood Equation, etc. ) : Determining the final beta regression coefficients involves solving an optimization equation. From what I see: Sometimes, the equation to be optimized has a ""closed form solution"". For example, estimating the coefficients for a Linear Regression model involves optimizing an OLS or an MLE equation - but this optimization equation has a ""closed form solution"". This means that we can directly estimate the regression coefficients and do not require an iterative optimization algorithm. I have noticed that many of these instances where the optimization problem has a ""closed form solution"", the equation (i.e. the mathematical function being optimized) is usually ""Convex"". However, in most cases these optimization equations do not have ""closed form solutions"" (e.g. Neural Networks, Logistic Regression, etc.). In these cases, an iterative optimization algorithm (e.g. Gradient Descent) has to be used to estimate the model parameters (e.g.  weights of a Neural Network). I have also noticed that when this is the case, the optimization equation (i.e. the mathematical function being optimized) is usually ""Non-Convex"". I understand that optimizing some Convex Functions have ""closed form solutions"" while other Convex Functions do not have ""closed form solutions"". But: Can a mathematical problem that is Non-Convex (i.e. a Non-Convex Function) ever have a ""closed form solution""? Or is this never the case? Thank you!","['statistics', 'neural-networks', 'maximum-likelihood', 'optimization', 'probability']"
4412278,Theorem which inspired Dennis Sullivan to switch to maths,"The recent Abel prize winner Dennis Sullivan switched from Chemical Engineering to majoring in Maths after hearing a talk about an illuminating particular theorem. From here : The epiphany for me was watching the professor explaining that any surface topologically like a balloon, and no matter what shape - a banana or the statue of David by Michelangelo could be placed on to a perfectly round sphere so that the stretching or squeezing. required at each and every point is the same in all directions at each such point,” he said. Further the correspondence was unique once the location of three points was specified and these points could be specified arbitrarily… “This was general , deep and absolutely beautiful,” he recalls. What is the exact statement of the theorem, and where can I find a proof?","['general-topology', 'math-history', 'soft-question', 'reference-request']"
4412348,Showing $\int_A f_jg\rightarrow 0$,"Let $f_j\in L^P\cap L^1_{loc}$ and $g\in L^q$ where $p,q$ are conjugates and $||f_j||_p \lt M$ for all $j$ . I have that $\int_S f_j\rightarrow 0$ for all $S\subset{A}$ (measurable) where $A$ is a compact subset of $\mathbb{R}$ and want to show $\int_A f_jg\rightarrow 0$ . Since $A$ is bounded, $g\in L^q\implies g\in L^\infty$ so I want to write down something like $|\int _A f_j g|\le ||g||_{L^\infty} |\int f_j|$ . But this is false. I can only use the tools that I know if I control $\int |f|$ , but I've already come up with counterexamples to show we have no control over that so considering $\int |f_jg|$ is useless.  Another idea was to use Cauchy schwarz but we need not have $\int f_j^2 \rightarrow 0$","['measure-theory', 'functional-analysis', 'analysis', 'real-analysis']"
4412359,A $5\times 5$ square with $+1$ and $-1$,"This is a problem from the Monthly Contest of the Berkeley Math Circle in 2001/2002 . The Problem A 5 ×5 square has been dissected into a unit square grid. One of the unit squares has been filled with “−1” and the rest have been filled with “+1”. In each step, one can choose a square, with side at east 2 and its sides parallel to the sides of the grid, and change all the signs in the square. Find all terh possible intial format(s) such that it is possible to get an all “+1” grid in a finite number of steps. Some comments on the official solution This puzzle has an official solution here , and states that the only possible initial position is the center of the square. Beggining with a $-1$ anywhere else, it is impossible. My feeling is that the answer is true, BUT THE PROOF is wrong. I'll try to explain. First, using $\mod 4$ argument, the solution states that only odd squares movements may affect the total sum of $+1$ and $-1$ . So to reach a solution, we must toggle odd squares an odd number of times; so, as only 3x3 and 5x5 are allowed (and all of them involve the center), the center tile have been toggled an odd number of times. So if we begin with a $+1$ we will end with a $-1$ . The solution says that from this arguments follows the answer. BUT I think it is possible to toggle the center piece also with an even square. So this argument fails. My Question My feeling is that the argument fails, but the answer is true. But I cannot give a proof. Can anyone give a complete answer? Thanks in advance",['puzzle']
4412436,Proof of a theorem about generators of uniformly continuous semigroup,"I´m reading Vrabie and I'm have some problems understanding this proof. We have that $\{S(t); t\geq 0\}$ is uniformly continuous, so $\displaystyle\lim_{t\downarrow 0} S(t)=I$ . Question 1: This implies that $\exists\rho >0$ such that $||\displaystyle\frac{1}{\rho}\displaystyle\int_0^\rho S(t) dt -I||_{\mathcal{L}(X)} <1$ . Because of the limit we have that $\exists\rho >0$ such that $|| S(t)  -I||_{\mathcal{L}(X)} <1$ . Now $$||\displaystyle\int_0^\rho S(t) dt -\displaystyle\int_0^\rho I dt||_{\mathcal{L}(X)}\leq \displaystyle\int_0^\rho ||S(t)  - I|| dt<\displaystyle\int_0^\rho  1 dt$$ this is equivalent to: $$||\displaystyle\int_0^\rho S(t) dt -\rho I ||_{\mathcal{L}(X)}<\rho$$ and just dividing by $\rho$ we have the inequality. I assume that $\displaystyle\int_0^\rho I dt=\rho I$ , but I don't know if it is true. Is it? We notice that the integral here is a Riemann integral of a continuous function $S:[0,\rho]\to\mathcal{L}(X)$ , which is defined by a simple analogy with its scalar counterpart . Question 2: About the sentence in bold, how do we define it? We define it as a operator given by $$x\to \displaystyle\frac{1}{x}\displaystyle\int_0^xS(t) dt$$ A bit lost here. Consecuently, the operator $\displaystyle\frac{1}{\rho}\displaystyle\int_0^\rho S(t) dt$ is invertible and accordingly $\displaystyle\int_0^\rho S(t) dt$ has the same property. Question 3: The phrase in bold just stunned me. We know that $S(t)$ is invertible as it belongs to a uniformly continuous semigroup by how can we deduce that the operator $\displaystyle\frac{1}{\rho}\displaystyle\int_0^\rho S(t) dt$ is invertible? For the following part: Question 4: we can change the letter $\rho$ highlighted in blue by the letter $h$ highlighted in blue, just because of question 2. Is jut notation. Question 5: If we take limit when $h\to 0$ in $$\displaystyle\frac{1}{h}\displaystyle\int_h^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt$$ why do we get $S(\rho)-I$ ?. Lets do a little of work, $$\displaystyle\frac{1}{h}\displaystyle\int_h^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt=\displaystyle\frac{1}{h}\displaystyle\int_0^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt$$ , so it's clear that $$\displaystyle\frac{1}{h}\displaystyle\int_0^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt=S(\rho)\;\text{when}\; h\to 0$$ . But where where does the $I$ come from? It comes from $\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt$ ?","['semigroup-of-operators', 'semigroups', 'functional-analysis', 'analysis']"
4412446,"Showing that if $\sum a_n,a_n\ge 0$ converges, then $\sum_{n=1}^\infty\frac {1}{n^2a_n}$ diverges.","Showing that $\sum_{n=1}^\infty\frac {1}{n^2a_n}$ diverges if $\sum a_n,a_n\ge 0$ converges. Since $\sum a_n$ converges, there exists $N$ such that for any $m\gt N$ , the following holds: $$a_{m+1}+a_{m+2}+\cdots+a_{2m}<\frac 12$$ By AM $\ge$ HM, $$\frac{\sum_{n=m+1}^{2m}a_n}{m}\ge \frac m{\sum_{n=m+1}^{2m}\frac 1{a_n} }\implies \frac 1{m^2}\sum_{n=m+1}^{2m}\frac 1{a_n}\ge \frac 1{\sum_{n=m+1}^{2m}a_n}\gt2. \tag 1$$ Let $S_n:=\sum_{j=1}^n\frac {1}{j^2a_j}$ $$|S_{2m}-S_m|\ge \frac 1{4m^2}\sum_{n=m+1}^{2m}\frac 1{a_n}\gt \frac 12,$$ (by $(1)$ ). It follows that $\lim S_n$ doesn't exist. Is my proof correct? Thanks.","['solution-verification', 'sequences-and-series', 'real-analysis']"
4412476,Testing whether limit exists,"I have the following function: $$f:\mathbb{R^{2}}\to \mathbb{R},\;\;f(x,y):=\frac{x-y}{x^{2}+y^{2}},\;\;(x,y)\neq (0,0).$$ I would like to test whether its limit exists at $(0,0)$ . $$\lim_{x\to 0}\left(\lim_{y\to 0}\frac{x-y}{x^{2}+y^{2}}\right) = \lim_{x\to 0}\left(\frac{x}{x^{2}}\right) = \lim_{x\to 0}\frac{1}{x} = +\infty$$ $$\lim_{y
\to 0}\left(\lim_{x\to 0}\frac{x-y}{x^{2}+y^{2}}\right) = -\lim_{y\to 0}\left(\frac{y}{y^{2}}\right) = -\lim_{y\to 0}\frac{1}{y} = -\infty$$ The divergent nature of the two limits indicates that the limit $$\lim_{(x,y)\to (0,0)}f(x,y)$$ does not exist. It would have been sufficient to show that at least one of the first two limits diverge.","['analysis', 'real-analysis', 'multivariable-calculus', 'solution-verification', 'limits']"
4412538,Rudin partition definition: Why non-strict inequalities?,"In Definition 6.1 in Principles of Mathematical Analysis, Rudin writes: Let $[a, b]$ be a given interval. By a partition $P$ of $[a, b]$ we
mean a finite set of points $x_0, x_1 , \dotsc, x_n$ , where $$a = x_0 \le x_1 \le \dotsb \le x_{n-1} \le x_n = b.$$ Why does he use non-strict inequalities? I don't see how we could have $x_{k-1} = x_k$ if $P$ is a set, in contrast to a multiset. Also, the definition on Wikipedia uses strict inequalities. Is this just Rudin being a little bit sloppy (though I guess, technically, his definition reduces to the other one due to $P$ being a set), or am I actually missing something?","['definition', 'real-analysis']"
4412543,"Sum of ""alternating-like"" series","I tried to prove that the following series converges. $$1 - \frac{1}{2} - \frac{1}{3} + \frac{1}{4}+ \frac{1}{5} - \frac{1}{6} - \frac{1}{7} + \frac{1}{8} +\frac{1}{9} - ...$$ I found a expression for this sum in sigma form by summing pair of numbers with the same sign: $$1 + \sum_{n=1}^{\infty} \frac{8n +1}{16n^{2}+4n} - \frac{8n -3}{16n^{2} - 12n +2} = 1 + \sum_{n=1}^{\infty} \frac{(8n +1)(16n^{2}+4n) - (8n -3)(16n^{2}+4n)}{(16n^{2}+4n)  (16n^{2} - 12n +2)}$$ But I'm getting a very long expression that can't find a way to prove a convergence of it , I feel that there is more elegant way to prove convergences of this series. I will thank for your help.","['calculus', 'convergence-divergence', 'sequences-and-series']"
4412548,"How to prove number of permutations obtained by a deque is C(2(n-1),n-1)?","Given a sequence $\{1,2,\dots,n\}$ ,you can insert the elements one by one via both ends of the deque(double-ended queue).Then you can pop elements out of the two ends until the deque is empty.At last you can obtain a permutation.How to prove that the number of different permutations is ${2(n-1)\choose n-1}$ ? It seems like the number is Type B Catalan number,and we already know that the number of permutations generated by a stack is Type A Catalan number.However I'm not sure if there is any relationship between them.","['catalan-numbers', 'combinatorics']"
4412570,Two definitions of quasi-separated morphisms which may not be equivalent?,"Definition. A topological space $X$ is said to be quasi-separated if the intersection of two quasi-compact open subsets $U,V\subset X$ is quasi-compact. In Definition 11.14 from these notes from a course in Algebraic Geometry taught at Bonn some years ago, they define a ""quasi-separated morphism of schemes"" as one such that the inverse image of any quasi-separated open subset is quasi-separated. On the other hand, on p. 207 of Vakil's Rising Sea , a ""quasi-separated morphism of schemes"" is defined as one such that the pre-image of an affine open subset is quasi-separated. My question is: are these definitions equivalent? Clearly, Bonn's implies Vakil's (as every affine scheme is quasi-separated). But I'm not sure about the converse. Specifically, one can consider the canonical map from the infinite dimensional space with doubled origin to the affine line with doubled origin (induced by $k[x_1]\to k[x_1,x_2,\dots]$ ). The target of this scheme morphism is a quasi-separated scheme for it is Noetherian. But the source is not quasi-separated (see Example 4 here . So this morphism wouldn't be Bonn-quasi-separated). However, I think this morphism is Vakil-quasi-separated? Edit: Grothendieck already gave a positive answer in EGA IV, première partie, Propositions (1.2.6) and (1.2.7) (it's on p. 227, here 's the link to Numdam). See my answer below for another argument.","['algebraic-geometry', 'schemes']"
4412632,Property of unitary matrices,"This article - https://doi.org/10.1016/0370-2693(88)91216-6 - states that for a $3 \times 3$ unitary matrix $A$ , we have $|A_{33}| = |(A_{11}A_{22}-A_{12} A_{21})|$ , where $A_{ij}$ stands for the element in row $i$ and column $j$ of matrix $A$ and |.| stands for the modulus. Why is this true? I tried expanding the square of the right-hand side of the equality but I got a term with $\Re(A_{11}A_{22}A_{12}^*A_{21}^*)$ that I don't think can easily be related to $|A_{33}|$ so this may not be the correct procedure.","['matrices', 'unitary-matrices']"
4412668,A fractional Gronwall lemma,"I have an energy estimate of the form $$\dot{u} + \lambda u^{1+ \delta} \le C (1+t)^{-\sigma},$$ where $u=u(t)$ is positive, $\lambda, \delta > 0$ and one can assume $\sigma > 1$ large. I expect the solution to satisfy a bound of the form $$u \le M\Big(C + u(0)\Big)(1+t)^{-1 / \delta}$$ for some universal $M > 0$ .
The only answer I found online is Proposition 2.2 of New nonlinear inequalities and application to control systems by Ghrissi and Hammami. However, they use a lemma which corresponds to the kind of Gronwall inequality I need but for $\lambda \le 0$ ,
and I did not manage to adapt the proof. Do you have any ideas?","['inequality', 'integral-inequality', 'ordinary-differential-equations', 'partial-differential-equations']"
4412673,How can I use Fubini's Theorem here?,"Exercise: If $F$ is a continuous distribution function on $(\mathbb R, \mathscr B, \mu_{\mathcal L})$ with distribution $\mu_F$ , use Fubini's theorem to show that $\int_{\mathbb R} F(x) \, d\mu_F(x) = \frac{1}{2}$ if $X_1, X_2$ are i.i.d random variables with common distribution $F$ , then $P(\{X_1 \leq X_2 \}) = 1/2$ and $\text E(F(X_1)) = 1/2$ . My Attempt: I don't really understand bs_math's answer so I have been trying to write my own. I just deleted an attempt here that was (I think) completely nonsensical. I am working on another attempt. For example, I don't understand what's going on in line 4 of bs_math's answer.","['measure-theory', 'fubini-tonelli-theorems', 'probability-theory', 'real-analysis']"
4412676,Find a compund Poisson variable with characterist function as centered compound Poisson,"I know that if $(X_j , j \geq 1)$ is a sequence of i.i.d. process with $\sigma$ being the probability distribution of $X_j's$ and $N \sim \text{Poisson}(\lambda)$ independent of the $X_j's$ , then the compund Poisson random variable $$Z = \sum_j^N X_j \sim CP(\lambda, \sigma)$$ is such that its characteristic function is given by: $$\varphi_X(t)= e^{\lambda(\,\phi_X(t)- 1 \,)}, \varphi_X(t) = \int_{\mathbb{R}}e^{itx}d\sigma(x)$$ Moreover, for the centered compound Poisson variable $\tilde{Z} = Z - \mathbb{E}Z = Z - \lambda \mathbb{E}X$ , its ch. f. is given by \begin{equation}\label{eq1}
\varphi_{\tilde{Z}}(t)= e^{\lambda\phi(t)},\quad \phi(t)= \int_{\mathbb{R}}\Big(e^itx - 1 - itx\Big)d \sigma(x)\tag{1}
\end{equation} But I would like to know if there is any $\bar{X}_j$ so that the characteristic function of $$\bar{Z}= \sum_{j=1}^N \bar{X}_j$$ is given, directly (without centering), by equation (\ref{eq1}). I tried putting $\bar{X}_j = X_j - \mathbb{E}X_j$ , but it didn't work. some help!","['poisson-distribution', 'measure-theory', 'probability']"
4412693,Prove that there exists $a\in \mathbb{C}$ such as $B=(1-a)A+aC$,"Let A,B and C be three distinct matrices $A_{2 \times 2},B_{2 \times 2},C_{2 \times 2}$ , with their traces equal to one another $TrA=TrB=TrC$ . Knowing that $AB+BC+CA=BA+CB+AC$ , prove thtat there exists $a\in \mathbb{C}$ such as $B=(1-a)A+aC$ . I tried applying Hamilton Caylay $A^{2}-Tr(A)A+detAI_{2}=O_{2}$ , the characteristic polynomial $det(xI_{2}-A)=x^{2}-Tr(A)x+detA$ and also $det(A+xB)=detA+(Tr(A)Tr(B)-Tr(AB))x+detBx^{2}$ .
Moreover, I attempted to rearange $AB-BA+BC-CB+CA-AC=O_{2}$ and $B-A=a(C-A)$ . I think I am close but I can't find how to prove it. Can you give me some help? Thank you","['matrices', 'determinant']"
4412734,Why do we have a conservation of probability during the transformation?,"I don't know what it is called in the literature but whenever we have a transformation of random variables (x -> y) with the probability distributions of $g(y)\ 
\&\ f(x)$ , we write saying that probability is equal , the following: $$ f(x)dx = g(y)dy$$ Now, y and x are related through a function, let's take it to be $J(x) = y$ . The next step is generally using the delta function for identifying the new probabilities $P(y)$ through $$ P(y) = \int dx\ \delta( y-J(x) ) f(x)  $$ Hence my two-part question: How, starting from this, I can derive the first equation logically. The general derivation treats derivatives as fractions which I don't consider to be correct. In some of the books, it is given directly. So, how does it make sense intuitively, too? Comment Since it is pointed out, the complete expression for P(y) would be $$ g(y)dy = P(y) = \int dx\ \delta( y-J(x) ) f(x)  $$","['statistics', 'probability-distributions', 'probability']"
4412742,Understanding arbitrary currents and currents of integration.,"I've been doing some readings, and there are some notational issues with currents that I do not understand. Let $\Omega\subset \mathbb C^n$ be open. A current $T$ of bidegree $(p,q)$ is an element of the dual space $(D^{(n-p,n-q)}(\Omega))'$ where $D^{(n-p,n-q)}(\Omega)$ is the space of test forms (differential forms of bidegree $(p,q)$ with smooth coefficients of compact support). One can define similary define a current of order zero as an element of $(D_0^{(n-p,n-q)}(\Omega))'$ where now the coefficients of the forms are simply continuous with compact support. Alternatively, we can think of $T$ as a differential form with distributional coefficents $T_{I,J}$ : $$ T=\sum_{I,J}T_{I,J}dz^I\wedge d\bar z^J $$ If $T$ is a current of order $0$ , then each $T_{I,J}$ is a Radon measure (a result of Riesz's Theorem). Now, we can define $T_f$ as the current of integration given by $$ T_f(\phi)=\int_{\Omega}f\wedge \phi$$ where $f$ is a $(p,q)$ form and $\phi$ is an $(n-p,n-q)$ form. In this specific case we can write $$ f=\sum_{I,J}f_{I,J} dz^I\wedge d\bar z^J$$ where $f_{I,J}$ are smooth functions (or continuous functions if $T_f$ is of order $0$ ).
This is where my confusion starts. Some texts I read say that given a current $T$ , there is the duality pairing given by $$ \langle T,\phi\rangle = \int_{\Omega} T\wedge \phi$$ If $T$ is a current like $T_f$ , then I understand this pairing. Otherwise I don't know how to integrate a differential form WITH distributional (or measure) coefficents. If I can write $T$ as a differential forms with smooth or continuous coeffients, then this pairing makes sense. Another example is given a current $T$ of order zero and a compact set $K\subset\Omega$ , we can define $$ ||T||_K=\int_{K}\sum_{I,J}|T_{I,J}|$$ Given that $T_{I,J}$ are measures, I understand the norm above. One can show that if $T\geq 0$ , then there are positive constants $C_1,C_2$ such that $$ C_1||T||_K\leq \int_K T\wedge \beta^p\leq C_2||T||_K$$ where $\beta$ is the Kahler form. I want to try to prove this statement, but I don't know how to interpret the integral in the middle of the inequality above. Can all currents be written an a current of integration?","['integration', 'measure-theory', 'several-complex-variables', 'distribution-theory', 'functional-analysis']"
4412749,How many monotonic functions on sets of naturals are there?,"A function $f\colon P(\mathbb{N}) \to P(\mathbb{N})$ is monotonic iff $x \subseteq y \implies f(x) \subseteq f(y)$ . What is the cardinality of the set $F$ of all such functions? What I have tried: I know the cardinality is either $2^\mathbb{N}$ or $(2^{\mathbb{N}})^{2^\mathbb{N}} =
2^{\mathbb{N}\cdot 2^\mathbb{N}} = 2^{2^\mathbb{N}}$ . I know that if I asked the question about natural numbers and not sets of them, the answer would be the (analogue of the) greater possibility, and if I asked about real numbers, the answer would be the (analogue of the) lesser possibility. (So as far as I know, there is a kind of ""precedent"" for both.) I tried proving it is $2^{\mathbb{N}}$ by finding a bijection between $P(\mathbb{N})$ and $F$ , but I couldn't come up with anything. I tried proving it is $2^{2^\mathbb{N}}$ in two ways. I tried finding a bijection between $P(\mathbb{N}) \to P(\mathbb{N})$ and $F$ . For functions on naturals, this is easy, (and has been asked before ): $$g(f) = n \mapsto \begin{cases}
f(0) & n = 0\\
f(n) + f(n-1) & \text{else}
\end{cases}$$ I tried to generalize this trick by defining $g'(f)$ similarly, in terms of sets containing one fewer element. However, such a definition would not be well-founded, because the set of sets of natural numbers is not well-ordered (take the family $\{ n \mid n > c \}$ for some $c$ ). I also tried a diagonalization proof by contradiction: assume there is a bijection $$g\colon P(\mathbb{N}) \to F\,,$$ then $$x \mapsto P(\mathbb{N}) \setminus g(x)(x)$$ or something similar fails to be a monotonic function not in $g$ 's image, because it fails to be (necessarily) monotonic, and I don't think this can be fixed.","['cardinals', 'functions', 'monotone-functions']"
4412805,"""CLT implies LLN?""","By the central limit theorem we have that for a an iid sequence $X_i$ with mean $\mu$ and variance $\sigma^2$ that, $$\sqrt{n}\frac{\overline{X}_n-\mu}{\sigma}\rightarrow_d N(0,1)$$ as $n\rightarrow\infty$ But this also implies that for sufficiently large $n$ we have approximately, $$  \overline{X}_n \sim N(\mu,\sigma^2/n) $$ And as $n \rightarrow\infty$ we have $\overline{X}_n \sim N(\mu,0)=\mu$ So we have $P(\lim_{n\rightarrow{\infty}}\overline{X}_n=\mu)=1$ . Which would mean that the CLT implies the LLN. This reasoning is probably false since the convergence for the CLT is weak. But still it seems plausible since the limiting distribution will just be a constant.  Can someone point out what is wrong with this reasoning?","['law-of-large-numbers', 'central-limit-theorem', 'probability-theory']"
4412847,Why is the range a larger set than the domain?,"When we have a function $f: \mathbb{R} \to \mathbb{R}$ , I can intuitively picture that and think that for every $x \in \mathbb{R}$ , we can find a $y \in \mathbb{R}$ such that our function $f$ maps $x$ onto $y$ . I'm confused, however, when we have something like: $g: D \to \mathbb{R}$ , where $D$ is the domain of our function such that $D \subset \mathbb{R}$ . How can our function output every element in $\mathbb{R}$ , when our input was specifically less than the whole set of reals?","['real-numbers', 'functions', 'real-analysis']"
4412856,What is 12-fold Stampfli-inflation tiling and where/how can I recognize it in this analysis of dodecagonal 30° twisted bilayer graphene quasicrystal?,"Dodecagonal 30° twisted bilayer graphene is just two graphene honeycomb nets with an exact 30° rotation with respect to the other. If you treat it as a 2D pattern rather than a 3D stacked structure, it would have 12-fold rotational symmetry and apparently some quasicrystal properties, including: the diffraction pattern (and thus the Fourier transform) has clean, crisp 12-fold symmetrical spot pattern, even though there is no translational symmetry the pattern can be tiled The field of tilings is well outside the scope of my understanding of mathematics and so I'd like to make some inroads using a familiar context, and bilayer graphene (and 2D materials in general) is somewhat familiar to me. So I'd like to ask within the context of the paper and figures cited below: Question: What is  12-fold Stampfli-inflation tiling and where/how can I recognize it in this analysis of dodecagonal 30° twisted bilayer graphene? How can I clearly see the ""inflation"" and the ""self-similarity"" at $\sqrt{2 + \sqrt{3}}$ and $2 + \sqrt{3}$ ? I see the drawn boundaries denoting these in the last figures (Fig S7 from the supllement), but is it possible to some how say Here, this is the self-similarity they are talking about. FIGURE 1. A LEED (low energy electron diffraction) pattern and a TEM image of graphene quasicrystal. (A) A LEED pattern of graphene quasicrystal. (B) A Fourier transformed pattern of graphene quasicrystal (see also Fig. S2 in SI). (C)-(D) An atomic structure model of twisted bilayer graphene with a rotational angle of 30°. (E) Atomic structures and TEM images of Stampfli tiles (rhombuses (red), equilateral triangles (green), squares (blue)). (F) A false colored TEM image of graphene quasicrystal mapped with 12-fold Stampfli-inflation tiling. From the supplement: FIGURE S7. TEM images of graphene quasicrystal. The TEM images in (A)-(B) and (D)-(E) are measured after transferring graphene quasicrystal on a TEM grid. A cross-sectional scanning transmission electron microscope (STEM) bright field image in (C) is measured on SiC before transferring graphene quasicrystal. (A) A large-scale TEM image, where its selected area electron diffraction (SAED) pattern is shown in the inset. (B) An enlarged atomic scale TEM image, where the Fourier-transformed image of the TEM image is in the inset. (C) A cross-sectional STEM bright field image of graphene quasicrystal on SiC, where bilayer graphene are clearly observed. (D)-(E) Two different Stampfli-inflation tiling, compared with the TEM image. (D) 12-fold inflation rule with a scaling factor of $\sqrt{2 + \sqrt{3}}$ , (E) Dodecagonal inflation rule with a scaling factor of $2 + \sqrt{3}$ . All of this is from the paper Ahn et al. (2018) Dirac electrons in a dodecagonal graphene quasicrystal Science 361, (6404) pp.782-786 (also in arXiv ). The text refers to figures in the paper's supplemental information or ""SI"" as well. The graphene quasicrystal can be spatially mapped onto a quasicrystal lattice model constructed by dodecagonal compound tessellations (see Figs. 1D-E and Figs. S3D-E in (*supplemental information))) (23-26). Squares, rhombuses, equilateral triangles with different orientations can fill the entire space with a 12-fold rotationally symmetric pattern without translational symmetry. Since the Stampfli tiles have a fractal structure with self-similarity, the same pattern emerges at a larger scale with an irrational scaling factor. For the graphene quasicrystal, the Stampfli tiles have the scaling factor of $\sqrt{2 + \sqrt{3}}$ (Figs. 1D-E and Fig. S3 in SI) 23 . ( note: maybe it's supposed to be S7? ) As shown in the atomic model (Figs. 1C-D), the graphene quasicrystal results in Stampfli tiles such as equilateral triangles and rhombuses. In the false colored TEM image of the graphene quasicrystal transferred from a SiC wafer to a TEM grid (Fig. 1F and Fig. S3D in SI), the Stampfli tiles including squares (blue), rhombuses (red), equilateral triangles (green) with different orientations were obviously observed. The LEED pattern and TEM image clearly support that the twisted bilayer graphene with a rotational angle of 30 ̊ has a quasicrystalline order with 12-fold rotational symmetry. We note that, for TEM experiments, the graphene quasicrystal grown on a SiC wafer should be transferred on other substrates and should be robust to chemical treatments in air. The successful TEM experiments support that graphene quasicrystal can be isolated from a substrate and are chemically and structurally stable at room temperature in air. We expect that the robustness of graphene quasicrystal can lead to further studies on the physical properties and applications. Having established quasicrystalline order in our sample , now we discuss its characteristic electronic structures... 23 P. Stampfli, A dodecagonal quasi-periodic lattice in 2 dimensions. Helv. Phys. Acta 59, 1260-1263 (1986) ( roughly here )","['geometry', 'tiling']"
4412857,Radon-Nikodym Derivative and Push Forward of Measures,"I am wondering how the Radon-Nikodym derivative is affected by push-forwarding with a random variable. Formally, let $(\Omega, \mathcal{F})$ be a measurable space, and $\mathbb{P}$ and $\mathbb{Q}$ be two probability measures on this space such that $\mathbb{Q} \ll \mathbb{P}$ . Radon-Nikodym theorem tells us that there exists a $\mathcal{F}$ -measurable function $f: \Omega \rightarrow [0, \infty)$ denoted by $\frac{d\mathbb{Q}}{d\mathbb{P}}$ , such that $$ \mathbb{Q}(E) = \int_E f(\omega) d\mathbb{P}(\omega) $$ for all $E \in \mathcal{F}$ . If somehow we know the expression for $f(\omega)$ already, and $X: \Omega \rightarrow \mathbb{R}^d$ is a random vector, is there a way to derive the Radon-Nikodym derivative $\frac{d\mathbb{Q}^{X}}{d\mathbb{P}^X}$ for the induced distribution measures $\mathbb{Q}^X$ and $\mathbb{P}^X$ on $(\mathbb{R}^d, \mathcal{B}(\mathbb{R}^d))$ ? My rough guess was $\frac{d\mathbb{Q}^{X}}{d\mathbb{P}^X}(X(\omega)) = f(\omega)$ but am not sure how to prove/disprove it.","['measure-theory', 'probability-theory']"
4412901,How can one find solutions to $f''(x)=f(f(x))$?,"This was a differential equation that I came up with. I have never seen any ODEs which involve composition so I have no idea on how to approach this. One solution appears to be $f(x)=0$ but I can't think of any more. EDIT: In regards to ODEs, I'm aware of separation of variables, integrating factors, solving linear homogeneous ODEs, method of undetermined coefficients, applying Taylor series, and Fourier and Laplace transforms (although I'm not that experienced with transforms). That being said, I have not formally studied an ODE course.","['ordinary-differential-equations', 'function-and-relation-composition']"
4412994,Number of equilateral triangles in triangular grid,"How can we find the number $f(x, y)$ of equilateral triangles that can be made by connecting three points within boundaries $x$ and $y$ ? (Suppose that $x\le y$ ) For example, $f(1, 4) = 15$ because there are $9$ smallest triangles, $3$ triangles that are twice as large, $1$ triangles that are 3 times as large, and $2$ triangle that is turned ( $90$ degrees or $270$ degrees each). example $2$ : $f(3, 6) = 21 + 7 + 3 + 5 + 5 + 3 + 1 + 1 = 46$ I am aware that the difficulty level of this problem is difficult. (I've been thinking about solving this for more than two days.) If it is impossible to find $f(x, y)$ in one formula, I would appreciate it if you could tell me the generalized calculation method. Thank you!","['combinations', 'combinatorics', 'triangles']"
4413055,What if I have different information from the Bayes Theorm diseases question?,"I'm studying Bayes' Rule and came across the diseases problem. And wonder if it also works in other circumstances. Let us say $D$ is the event having the diseases, $T$ is the event testing positive for the diseases and $N$ is the event testing negative .
Usually, we will be given with the probability of $P(D)$ , $P(T|D)$ and $P(N|\overline{D})$ .
So we can figure out $P(D|T) = \frac{P(T|D)P(D)}{P(T|D)P(D)+P(T|\overline{D})P(\overline{D})}$ . But What if I know $P(D)$ , $P(D|T)$ and $P(\overline{D}|N)$ , and trying to figure out $P(T|D)$ . Is that possible?
So far I can only get $$P(T|D) = \frac{P(\overline{D}|T)[P(\overline{D}|T)P(T))+P(D|T)P(T)]}{P(D)} = \frac{P(\overline{D}|T)[P(T|\overline{D})P(\overline{D})+P(T|D)P(D)]}{P(D)}$$ It seems like if I don't know $P(T)$ or $P(T|D)$ itself I can' find the answer. I wonder if there is a way to figure this out knowing the Bayes' Rule. Thank you.","['bayes-theorem', 'probability']"
4413070,Understanding two steps in the proof of Armijo's Convergence Theorem,"I was reading this paper in which Armijo proves his convergence theorem, and I struggle understanding some of the steps in the proof. The following questions regard the first theorem in the paper, which statement begins at the bottom of page 1. The paper begins by defining four conditions that the functions the author is working with must satisfy. Important for our purposes is the third condition: Let $f$ be a real-valued function defined and continuous everywhere on $\mathbb{R}^n$ and bounded below. For a fixed $x_0$ define $S(x_0)=\{x:f(x)\leq f(x_0)\}$ . The function $f$ satisfies condition III at $x_0$ if $f\in C^1$ on $S(x_0)$ and $\nabla f(x)$ is Lipschitz continuous on $S(x_0)$ , i.e., there exists a Lipschitz constant $K>0$ such that $|\nabla f(y)-\nabla f(x)|\leq K|y-x|$ for every pair $x,y\in S(x_0)$ . I do not think understanding condition IV, which is mentinoned in the theorem, is necessary for my post. In any case all conditions can be found in the first page of the linked paper. Convergence Theorem: assume $f$ is a real valued function defined and continuous everywhere on $\mathbb{R}^n$ , bounded below on $\mathbb{R}^n$ , and that conditions III and IV hold at $x_0$ .
If $0<\delta \leq 1/4K$ (here $K$ refers to the Lipschitz constant from condition III), then for any $x\in S(x_0)$ , the set $$(1) \ \ S^*(x,\delta)=\{ x_{\lambda} : x_{\lambda} = x - \lambda \nabla f(x), \lambda > 0, f(x_{\lambda}) - f(x) \leq -\delta |\nabla f(x)|^2\}$$ is a nonempty subset of $S(x_0)$ and any sequence $\{ x_k\}^{\infty}_{k=0}$ such that $x_{k+1}\in S^*(x_k, \delta), k=0,1,2,\ldots$ , converges to the point $x^*$ which minimizes $f$ . Proof. If $x\in S(x_0)$ with $x_{\lambda}=x-\lambda \nabla f(x)$ and $0\leq \lambda \leq 1/K$ , condition III and the mean value theorem imply the inequality $$(2) \ \ \ \ f(x_{\lambda})-f(x)\leq -(\lambda - \lambda^2K)|\nabla f(x)|^2$$ which in turn implies that $x_{\lambda} \in S^*(x,\delta)$ for $$(3) \ \ \ \ \lambda_1\leq\lambda\leq\lambda_2, \ \ \ \lambda_i=\frac{1+(-1)^i\sqrt{1-4\delta K}}{2K}$$ so that $S^*(x,\delta)$ is a nonempty subset of $S(x_0)$ . If $\{ x_k\}^{\infty}_{k=0}$ is any sequence for which $x_{k+1}\in S^*(x_k, \delta)$ , $k=0,1,2,\ldots$ , then $(1)$ implies that sequence $\{ f(x_k)\}^{\infty}_{k=0}$ which is bounded below, is monotone nonincreasing and hence that $|\nabla f(x_k)|\rightarrow 0$ as $k\rightarrow \infty$ . The remainder of the theorem follows from condition IV. My questions: How does Armijo get to the equation $$(2) \ \ \ \ f(x_{\lambda})-f(x)\leq -(\lambda  - \lambda ^2K)|\nabla f(x)|^2 \ \text{?}$$ I suspect the argument goes somewhat like this: by the Mean Value Theorem we have that $$(4) \ \ \ \ |f(x_{\lambda})-f(x)| \leq |\nabla f(x+c(x_{\lambda} - x))||\lambda \nabla f(x)|.$$ Meanwhile condition III gives us $|\nabla f(x_{\lambda})-\nabla f(x)| \leq K|x_{\lambda} - x|$ , or, put differently, $$(5) \ \ \ \ |\nabla f(x_{\lambda})| \leq |\nabla f(x)| + K|\lambda \nabla f(x)|=(1+|\lambda |K)|\nabla f(x)|.$$ Now, if we could assume that $|\nabla f(x+c(x_{\lambda} - x))| \leq |\nabla f(x_{\lambda})|$ , and that $f(x_{\lambda})\leq f(x)$ , then we may combine $(4)$ and $(5)$ to get $$f(x_{\lambda})-f(x) \leq (1+|\lambda |K)|\nabla f(x)||\lambda \nabla f(x)|=(\lambda + \lambda ^2K)|\nabla f(x)|^2$$ which is similar to $(2)$ . Why does the sequence $\{ \nabla f(x_k)\} \rightarrow 0$ as $k \rightarrow \infty$ ?","['proof-explanation', 'multivariable-calculus', 'gradient-descent', 'analysis']"
4413120,$f:P_{n}\rightarrow \mathbb{Z}^{n+1}$ is a bijection,"Let $P_{n}$ denote the set of polynomials of degree less than or equal to $n$ with integer coefficients. I want to show that $f:P_{n} \rightarrow \mathbb{Z}^{n+1}$ given by $f(a_{0}z^{n} + a_{1}z^{n-1} + \dots + a_{n-1}z + a_{n}) = (a_{0},a_{1}, \dots , a_{n})$ is bijective. Attempt: (Injective): We need to show if $f(x)=f(y)$ then $x=y$ . Suppose $f(a_{0}z^{n} + a_{1}z^{n-1} + \dots + a_{n})=f(b_{0}z^{n} + b_{1}z^{n-1} + \dots + b_{n})$ . Then, $(a_{0},a_{1}, \dots , a_{n}) = (b_{0}, b_{1}, \dots , b_{n})$ . Comparing coefficients, injectivity follows. (Surjective): We need to show for all $y \in \mathbb{Z}^{n+1}$ there exists an $x \in P_{n}$ such that $f(x)=y$ . Let $y \in \mathbb{Z}^{n+1}$ . Then $y$ is of the form $(y_{0},y_{1}, \dots , y_{n})$ . Then take the polynomial $y_{0}z^{n} + y_{1}z^{n-1} + \dots + y_{n}$ , applying $f$ to this polynomial we get $ f(y_{0}z^{n} + y_{1}z^{n-1} + \dots + y_{n}) = (y_{0}, y_{1}, \dots , y_{n})$ . Since $f$ is both injective and surjective, $f$ is bijective.","['functions', 'solution-verification', 'polynomials']"
4413185,Proving identity for binomial coefficient using generating funtion,"I am trying to show that $$\sum_{k=0}^n (-1)^k \binom{n}{k} \binom{nx-kx}{n+1} = n x^{n-1} \binom{x}{2}.$$ By using $(1+x)^n(1+x)^n = (1+x)^{2n}$ and $(1+x)^n(1-x)^n = (1-x^2)^n$ and generating functions, I already showed that $$\sum_{k=0}^n \binom{n}{k}^2 = \binom{2n}{n}\quad\text{and}\quad \sum_{k=0}^n (-1)^k \binom{n}{k}^2 = \begin{cases} (-1)^m \binom{2m}{m},&\text{for } n=2m, \\ 0,&\text{else}. \end{cases}$$ So I think there has to be a similiar method to show the first identity, but I am failing at finding it because of having the index in the upper part of $\binom{nx-kx}{n+1}$ . Does someone have an idea, how to get rid of it?","['binomial-coefficients', 'combinatorics', 'binomial-theorem', 'generating-functions']"
4413193,What is a dog saddle?,"I got this question as an assignment. The question is why the graph of the function $f(x,y)=x^3y-xy^3$ is called a dog saddle. I am rather confused as I don't know what our professor is really looking for. Isn't it called a dog saddle because it looks like one? Please help me with this. Thanks in advance.",['multivariable-calculus']
4413234,Why Null Hypothesis for contingency tables is 'independent'?,"For contingency tables I don't understand why the Null Hypothesis is 'independent'. We calculate Sum(Observed^2 / Expected) - N and compare this with the chi-squared distribution table. Let's say: Sum(Observed^2 / Expected) - N = 4 Chi-square value = 5.1 So 4 < 5.1 and we do NOT reject the Null Hypothesis, they are independent. But if 4 < 5.1, then the difference between all Observed and Expected is smaller than our Critical Value, so surely they are correlated and therefore not independent? ADDITIONAL INFORMATION I got confused because I was doing this question: and in the answer: they state the Null Hypothesis is ""no difference"", which read like they are not independent?","['chi-squared', 'statistics', 'correlation', 'hypothesis-testing']"
4413245,Using generating functions to construct or solve differential equations,"I know that $T_n(x)$ is the solution of the differential equation $(1-x^2)y''-xy'+n^2y=0$ , where $$
T_n(x)=\begin{cases} 
T_n(x)=1 & \text{if $n=0$}\\
T_n(x)=x & \text{if $n=1$}\\
T_{n}(x)=2xT_{n-1}(x)- T_{n-2}(x) & \text{if $n\geq 2$}\\
\end{cases}
$$ this can be proved using power series ( https://en.wikipedia.org/wiki/Chebyshev_equation ). I was wondering if there is a way to go ""backwards"", given any recurrence (as a generating function) $f_n$ , can I construct a differential equation such that $f_n$ is a solution of the constructed equation?. For example, we know that $$T_n(x)=\frac{(x+\sqrt{x^2-1})^n+(x-\sqrt{x^2-1})^n} {2},$$ How can I construct $(1-x^2)y''-xy'+n^2y=0$ given that $y(x)=\frac{(x+\sqrt{x^2-1})^n+(x-\sqrt{x^2-1})^n} {2}$ is a solution of that equation?. Thanks in advance.","['chebyshev-polynomials', 'ordinary-differential-equations', 'recurrence-relations', 'intuition', 'constructive-mathematics']"
4413325,The Lovasz-Stein theorem,"Given a family $\mathcal{F}$ of subsets of some finite set $X$ , its cover number of $\mathcal{F}$ , $\text{Cov}(\mathcal{F})$ , is the minimum number of members of $\mathcal{F}$ whose union covers all points of $X$ . Theorem 2.16. If each member of $\mathcal{F}$ has at most $a$ elements, and each point $x\in X$ belongs to at least $v$ of the sets
in $\mathcal{F}$ , then $$\text{Cov}(\mathcal{F})\leq
 \dfrac{|\mathcal{F}|}{v}(1+\ln a).$$ Proof. Let $|X|=N, |\mathcal{F}|=M$ and consider the $N\times M$ $0$ - $1$ matrix $A=(a_{x,i})$ , where $a_{x,i}=1$ iff $x\in X$ belongs to the $i$ -th member of $\mathcal{F}$ . By our assumption, each row of $A$ has at least $v$ ones and each column at most $a$ ones. By double couting, we have that $Nv\leq Ma$ , or equivalently, $$\frac{M}{v}\geq \frac{N}{a} \quad \quad (*).$$ Our goal is to show that $A$ must contain an $N\times K $ submatrix $C$ with no all- $0$ rows and such that $$K\leq (M/v)(1+\ln a).$$ We describe a constructive procedure for producing the desired submatrix $C$ . Let $A_a=A$ and define $A'_a$ to be any maximal set of columns from $A_a$ whose supports are pairwise disjoint and whose columns each have $a$ ones. Let $K_a=|A'_a|$ . Discard from $A_a$ the columns of $A'_a$ and any row with a one in $A'_a$ . We are left with a $k_a\times (M-K_a)$ matrix $A_{a-1}$ , where $k_a=N-aK_a$ . Clearly, the columns of $A_{a-1}$ have at most $a-1$ ones (indeed, otherwise such a column could be added to the previously discarded set, contradicting its maximality). We continue by doing to $A_{a-1}$ what we did to $A_a$ . That is we define $A'_{a-1}$ to be any maximal set of columns from $A_{a-1}$ , whose supports are pairwise disjoint and whose columns each have $a-1$ ones. Let $K_{a-1}=|A'_{a-1}|$ , then discard from $A_{a-1}$ the columns of $A'_{a-1}$ and any row with a one in $A'_{a-1}$ getting a $k_{a-1}\times (M-K_a-K_{a-1})$ matrix $A_{a-2}$ , where $k_{a-1}=N-aK_a-(a-1)K_{a-1}$ . The process will terminate after at most $a$ steps. The union of the columns of the discarded sets form the desired submatrix $C$ with $K=\sum \limits_{i=1}^a K_i$ . The first step of the algorithm gives $k_a=N-aK_a$ , which we rewrite, setting $k_{a+1}=N$ , as $$K_a=\frac{k_{a+1}-k_a}{a}.$$ Analogously, $$K_i=\frac{k_{i+1}-k_i}{i} \  \text{for} \ i=1,\dots,a.$$ Now we derive an upper bound for $k_i$ by counting the number of ones in $A_{i-1}$ in two ways: every row of $A_{i-1}$ contains at least $v$ ones, and every column at most $i-1$ ones, thus $$vk_i\leq (i-1)(M-K_a-\dots-K_{i+1})\leq (i-1)M,$$ or equivalently $$k_i\leq \frac{(i-1)M}{v}.$$ So, $$K=\sum\limits_{i=1}^a K_i=\sum\limits_{i=1}^a \frac{k_{i+1}-k_i}{i}=$$ $$=\frac{k_{a+1}}{a}+\frac{k_{a}}{a(a-1)}+\frac{k_{a-1}}{(a-1)(a-2)}+\dots+\frac{k_{2}}{2\cdot 1}-k_1\leq $$ $$\leq \frac{N}{a}+\frac{M}{v}(\frac{1}{a}+\dots+\frac{1}{2})\leq \frac{N}{a}+\frac{M}{v}\ln a.$$ The last inequality here follows because $1+\frac{1}{2}+\dots+\frac{1}{n}$ is the $n$ -th harmonic number which is known to lie between $\ln n$ and $\ln n+1$ . Together with $(*)$ , this yields $K\leq (M/v)(1+\ln a)$ , as desired. This proof is copied from the book of Stasys Jukna ""Extremal combinatorics"" and I would like to clarify some moments of this proof. My questions: I am a bit confused with the procedure and how it works. In the first step we need to find any maximal set of columns which have pairwise disjoint supports and each of them have $a$ ones. What if there no columns with $a$ ones? Intuitively I can understand why this process terminates after at most $a$ steps but can anyone explain it in a rigorous way, please? How we take the union of the columns of the discarded sets if those columns have different size? If we take $C$ to be the union of the columns of the discarded sets. why any row of $C$ has $1$ among its entries? I would be very thankful if someone can explain those questions!
Thank you so much for your attention!","['matrices', 'combinatorics', 'extremal-combinatorics', 'discrete-mathematics']"
4413330,Folland theorem 6.19 proof doubt,"Consider the following fragment from Folland's real analysis book: They mention theorem 6.14, which is the following theorem: However, to apply theorem 6.14, shouldn't we check that if $h(x)= \int_Y f(x,y)d\nu(y)$ , then $\int_A |h| d\mu<\infty $ for every $A \in \mathcal{M}$ with $\mu(A) < \infty$ ? My question is, in other words, how is theorem 6.14 applied? Note that we may assume that the right hand side of the inequality we want to prove is finite, and this ensures that the supremum in theorem 6.14 is finite. Thanks in advance for any help!","['measure-theory', 'analysis', 'lp-spaces', 'functional-analysis', 'inequality']"
4413341,Why is $\Pr(|X-Y|>t)\le\Pr(|X|>t/2)+\Pr(|Y|>t/2)$?,"I am having trouble proving the inequality in the title of this post. This inequality is commonly used in Statistical Learning Theory, when proving Symmetrization lemmas. When used in such context, it usually takes the following form : $$\mathbb P\left(\left\vert\sum_{i=1}^n\varepsilon_i(f(X_i) - f(X_i'))\right\vert>t\right) \le \mathbb P\left(\left\vert\sum_{i=1}^n\varepsilon_i f(X_i) \right\vert>t/2\right) + \mathbb P\left(\left\vert\sum_{i=1}^n\varepsilon_if(X_i') \right\vert>t/2\right)$$ Where the $\varepsilon_i$ are i.i.d. Rademacher random variables (i.e. take value $\pm 1$ with probability $1/2$ ), $(X_i)$ are i.i.d. random variables and $(X_i')$ are i.i.d. copies of $(X_i)$ and $f$ is some deterministic function. The above inequality can be found, e.g., in the paper A few notes on Statistical Learning Theory by Shahar Mendelson (page 6), and is said to directly follow from the triangle inequality. Although it looks very simple, I can't see why this inequality holds. Here is what I have so far : First, $\mathbb P(|a-b|>t) = 1 - \mathbb P(|a-b|\le t)$ . Furthermore, by the triangle inequality, it is straightforward to see that $\{|a|\le t/2\}\cap\{|b|\le t/2\} \subseteq \{|a-b|\le t\}$ . We thus have the following bound $$\mathbb P(|a-b|>t) \le 1- \mathbb P\left(\{|a|\le t/2\}\cap\{|b|\le t/2\}\right)$$ Now I'd like to have a bound of the form $1 - \mathbb P\left(\{|a|\le t/2\}\cap\{|b|\le t/2\}\right) \le \mathbb P\left(\{|a|> t/2\}\cup\{|b|> t/2\}\right) $ for the RHS of the inequality, from which I would be able to conclude by the union bound, but I can't prove the desired inequality... It seems very elementary, what am I missing here ?","['statistics', 'probability-theory', 'probability', 'inequality']"
4413356,Find solution of ordinary differential equation,"$ \frac{dy}{dx} + 2xy=y $ which satisfies the boundary conditon $y(3)=1$ $$\frac{dy}{dx} + (2x-1)y = 0,$$ then using integrating factor $e^{\int2x-1 dx}$ I then get: $e^{x^2-x}\frac{dy}{dx} + (2x-1)ye^{x^2-x} = 0e^{x^2-x}$ giving: $\frac{d}{dx}{ye^{x^2-x}}=0e^{x^2-x}$ integrate both sides: ${ye^{x^2-x}}=C$ and thus: $y=Ce^{-x^2+x}$ but I'm not sure this is correct as I have been given a different answer.
Any help would be appreciated.",['ordinary-differential-equations']
4413360,Alternative idea to find minimum of $\sin^4x+\cos^4x$ without derivative,My question is about to find minimum value of $f(x)=\sin^4x+\cos^4x$ without derivation. My trial is listed below $$\sin^4x+\cos^4x=\\(\sin^2x+\cos^2x)^2-2\sin^2x\cos^2x=\\1-2(\frac{\sin^2(2x)}{2})^2\\\to min=\frac 12$$ and second try is to use cauchy schwarz inequality $$1.\sin^2x+1.\cos^2x\leq (\sin^4x+\cos^4x)(1^2+1^2)\\(\sin^4x+\cos^4x)\geq \frac12\\min=\frac 12$$ I am thankful if you show other Idea to solve this minimization.,"['inequality', 'calculus', 'cauchy-schwarz-inequality', 'trigonometry', 'algebra-precalculus']"
4413444,Conceptual understanding of Christoffel symbols,"The basis vectors on a manifold are defined as partial derivative operators of any function that can be locally mapped around a point to $\mathbb R^n.$ The Christoffel symbols come about when a vector field on a such a manifold is differentiated, and the product rule calls for differentiating not just the components, but also these basis vectors, since they change from point to point. Is there any sense, therefore, in which the Christoffel symbols can be understood as a second derivative?","['general-relativity', 'differential-geometry']"
4413448,Non Self Adjoint ideal in C(D)?,"I'm currently working on a problem that's asking me to give an example of a non self adjoint ideal of $C(D) = \{f: D \longrightarrow \mathbb{C} \: | \: \text{$f$ is continuous}\}$ with $||\cdot||_{\infty}$ , and $D = \{z \in \mathbb{C} \: |\: |z| \leq 1\}$ . So far I've been trying to mess with holomorphicity, integration, compact supports, and  noninvertible functions. My issue is that I keep yielding either a vector subspace which isn't self adjoint but not an ideal (like the collection of all holomorphic functions), or I get an ideal thats self adjoint (functions with compact support, functions whose integral over $D$ is $0$ , maximal ideals of the form $M_y=\{f \in C(D) \: | \: f(y)=0\}$ , etc).  I know that whatever ideal I choose, it can't be closed ofcourse. I would be very appreciative of a $\textbf{small hint that points me in the right direction, not a full answer}$ !","['c-star-algebras', 'operator-theory', 'complex-analysis', 'abstract-algebra', 'functional-analysis']"
4413501,Decomposing higher order derivatives of composite functions,"I'm new to calculus, and just a little hazy on the skeleton of higher order derivatives when the chain rule is involved. I'm given a table of function and first derivative values, and I need to solve for the second derivative at a specific point. On an abstract level, that's fairly straightforward: (1) $$k(x) = f(g(x))$$ (2) $$\frac{d}{dx}k(x) = \frac{d}{dx}f(g(x))\frac{d}{dx}g(x)$$ (3) $$\frac{d^2}{dx^2}k(x)=\frac{d^2}{dx^2}f(g(x))\left[\frac{d}{dx}g(x)\right]^2+\frac{d}{dx}f(g(x))\frac{d^2}{dx^2}g(x)$$ But I don't know anything about the second derivative, so it's difficult to solve for a specific value of x (I could graph the function, but I'm looking for an exact solution). I'm curious, if I take $\frac{d^2}{dx^2}f(g(x))$ for example, is this the equivalent of $\frac{d}{dx}\left[\frac{d}{dx}f(g(x))\right]$ ? Of course, had I started with this expression, I would apply the chain rule, so I would find myself back at (3). In terms of one of the second-order derivatives in (3), however, since the chain rule was already applied, is it fair to assume $$\frac{d^2}{dx^2}f(g(A))=\frac{d^2}{dx^2}f(B)=\frac{d}{dx}\left[\frac{d}{dx}f(B)\right]=\frac{d}{dx}[C]$$ Edit: The point I'm trying to make here is breaking up the expression into first-order derivatives without applying the chain rule, since g(x) is essentially a constant in this context (to my understanding).","['calculus', 'derivatives', 'chain-rule', 'function-and-relation-composition']"
4413654,Gradient descent inside the expectation-maximization (EM) algorithm,"I am feeling super uncertain about how much I can play around with the EM algorithm. Here is my question: In the EM algorithm, during the M-step, one attempts to find a parameter value, $\theta$ , that maximizes $Q$ . Sometimes the function of $Q$ with respect to that parameter value is continuous, differentiable and concave, but does not have a closed form solution for the stationary points and requires some form of numerical techniques, like gradient descent or Newton-Raphson method, to find the stationary points. Am I correct to conclude that one does not break the convergences of the EM algorithm if you use these numerical methods to optimize $Q$ with respect to some parameter value $\theta$ . In fact, am I correct to conclude that one could use any optimization technique in the E step for all the parameters we wish to optimize for?","['optimization', 'statistics', 'numerical-optimization', 'gradient-descent']"
4413657,Prove that $\int_0^\frac{\pi}{4}\frac{\cos (n-2)x}{\cos^nx}dx=\frac{1}{n-1}2^\frac{n-1}{2}\sin\frac{(n-1)\pi}{4}$,"Prove that $$J_n=\int_0^\frac{\pi}{4}\frac{\cos (n-2)x}{\cos^nx}dx=\frac{1}{n-1}2^\frac{n-1}{2}\sin\frac{(n-1)\pi}{4}$$ For $n=2$ , it is OK. For general $n$ , it seems impossible by integration by parts. Any other method? When I calculate an integral $I_n=\int_0^\frac{\pi}{4}\frac{\cos nx}{\cos^nx}dx$ , we find $I_n/2^{n-1}-I_{n-1}/2^{n-1}=-1/2^{n-1}J_n$ . So we need to find the $J_n$ , as the problem states. The proof of $I_n/2^{n-1}-I_{n-1}/2^{n-1}=-1/2^{n-1}J_n$ is as follows. \begin{align}
I_n/2^{n-1}-I_{n-1}/2^{n-1}
&=\frac{1}{2^{n-1}} \int_0^\frac{\pi}{4}\left(\frac{\cos nx}{\cos^nx}-\frac{2\cos(n-1)x}{\cos^{n-1}x}\right)dx\\
&=\frac{1}{2^{n-1}}\int_0^\frac{\pi}{4}\frac{\cos[(n-1)x+x]-2\cos(n-1)x\cos x}{\cos^nx}dx\\
&=-\frac{1}{2^{n-1}}\int_0^\frac{\pi}{4}\frac{\cos(n-2)x}{\cos^nx}dx
=-1/2^{n-1}J_n
\end{align}","['calculus', 'definite-integrals']"
4413730,"Prove $\delta_{kl} + \frac{1}{3}\sum_{i,j}R_{ikjl}x^ix^j$ defines a metric","Let $g$ be a bilinear form on $V$ over $\Bbb{R}$ , which is defined in terms of basis as : $$\delta_{kl} + \frac{1}{3}\sum_{i,j}R_{ikjl}x^ix^j$$ Where $R_{ikjl}$ is the algebraic curvature tensor, prove it defines a Riemannian metric when $|x|$ is small. (Combined with the fact that Taylor expansion of the metric is $$\begin{equation}
g_{ij} = \delta_{ij} + \frac{1}{3} R_{ikjl} \,x^kx^l + \mathcal{O}(|x|^3) 
\end{equation}
$$ this shows the linear part of the Talor expansion is again a metric which is an approximation of the original metric.） The symmetry of the bilinear form $g$ due to symmetry of the curvature tensor, only needs to prove it's positive definite, that is we need to prove $$R_{ikjl}x^kx^lv^iv^j \ge 0$$ This seems not very obvious for me, I know we must use the symmetry of the curvature tensor, in particular the Jacobi identity for $R_{ikjl}$ .","['riemannian-geometry', 'differential-geometry']"
4413791,"Let $V$ be a bounded open subset of $\mathbb{R}^n.$ Is $B(0,1)$ the union of countably many (pairwise disjoint) rescalings/translates of $V$?","Let $V$ be a bounded open subset of $\mathbb{R}^n,$ and define $B(x,r)$ to be the open ball with centre $x$ and radius $r.$ True or false: $B(0,1)$ is the union of countably many pairwise disjoint rescalings/translates of $V$ . In other words, if $a,\in\mathbb{R}, b,\in\mathbb{R}^n$ then define $V_{a,b} := \{ax+b: x\in V\}.$ The question then is whether or not there exist sequences $(a_n)_n$ and $(b_n)_n$ such that $V_{a_k, b_k} \cap V_{a_j,b_j} = \emptyset\ $ if $ j\neq k $ and $$ \bigcup_{n\in\mathbb{N},\ x\in V} V_{a_n,b_n} = B(0,1).$$ Perhaps the result is false. For example, maybe it cannot be done if we define $V$ as follows: Let $(x_n)_{n\in\mathbb{N}}$ be an enumeration of $\mathbb{Q}^n\cap B(0,1),$ let $y_n=3^{-n},$ and define $V =\bigcup_{n\in\mathbb{N}} B(x_n,y_n).$ The hard part of this question seems to be a geometric one: how can we show that we can entirely fill up $B(0,1)$ for any possible shape that $V$ can be - or alternatively, is there a counter-example?","['general-topology', 'geometry', 'examples-counterexamples', 'real-analysis']"
4413812,Is there any trilateral figure in euclidean geometry that is not a triangle?,"I was going through Euclid's Elements and when i read definition 19 which says: Rectilinear figures are those (figures) contained by straight-lines: trilateral figures being those contained by three straight-lines, quadrilateral by four, and multi-lateral by more than four. and then definition 20 which says: And of the trilateral figures: an equilateral triangle is that having three equal sides, an isosceles (triangle) that having only two equal sides, and a scalene (triangle) that having three unequal sides. I couldn't help but wonder that why did Euclid use the term ""Trilateral Figures"" when he could have simply used the term ""Triangles"" and are there any ""Trilateral Figures"" other than triangles in Euclidean Geometry.","['euclidean-geometry', 'triangles', 'geometry']"
4413817,"Question on ""false"" application of chain rule","Let's consider $K:\mathbb{R}^n\times [0,1]\to\mathbb{R}$ and $f:\mathbb{R}^n\to\mathbb{R}^n$ differentiable with $K(u,t):=\langle f(a+t(u-a)),(u-a)\rangle=\sum\limits_{i=1}^nf_i(a+t(u-a))(u_i-a_i)$ , where $a\in\mathbb{R}^n$ . Now let's take the first partial derivative (with respect to $u_1$ ) and apply the chain and product rule of differentiation: \begin{align*}
&D_1K(u,t)=D_1\left(\sum\limits_{i=1}^nf_i(a+t(u-a))(u_i-a_i)\right)\\
&=\sum\limits_{i=1}^nD_1f_i(a+t(u-a))~\underset{=?}{\underbrace{D_1(a+t(u-a))}}~(u_i-a_i)+f_1(a+t(u-a)).\end{align*} As $(a+t(u-a))$ attains the form of $\begin{pmatrix}a_1+t(u_1-a_1)\\\vdots\\a_n+t(u_n-a_n) \end{pmatrix}$ , I thought that $D_1(a+t(u-a))=D_1\begin{pmatrix}a_1+t(u_1-a_1)\\a_2+t(u_2-a_2)\\ \vdots\\a_n+t(u_n-a_n) \end{pmatrix}=\begin{pmatrix}t\\0\\\vdots\\0 \end{pmatrix}$ but this doesn't match the dimension of the image of $K$ . Where is my mistake?","['calculus', 'derivatives', 'chain-rule', 'real-analysis']"
4413827,Olympiad combinatorics problem (probably related to graph theory?),"Let me state the problem first. The problem is from the 2017 Japanese Mathematical Olympiad. There are $n$ people in the group, where $n$ is greater than or equal to 3. Each day, some people (more than or equal to 3 people) from the group have a social gathering. For each gathering, everyone who participated in the gathering shake hands with each other. After the gathering of $n$ -th day, any two people among total $n$ people shook their hands exactly once throughout all gathering. Show that the number of people attending each gathering is constant. I tried to formulate it in a graph-theoretical way. On the very first day, there are $n$ vertices and no edge. Then we start to add a ""clique"" in this graph. After $n$ such operations, the graph became a complete graph (with no duplicate edges). Now what we want to show is the size of each clique added is always the same. Number of total edges in $K_n$ is ${n \choose 2}$ . As we add each clique with the size $k_i$ , we are adding ${k_i \choose 2}$ edges. So I assume that it can be formulated as: $${n \choose 2} = {k_1 \choose 2} + {k_2 \choose 2} + \cdots + {k_n \choose 2}$$ Then $k_1 = k_2 = \cdots = k_n$ . However I am not sure about the correctness. Both formulation do not give me a hint about how to approach this problem. I am not skilled in both graph theory and combinatorics. Thanks in advance!","['graph-theory', 'combinatorics', 'contest-math']"
4413836,Decomposition of symmetric powers of the standard representation of $SO(n)$,"Let $V$ be the n-dimensional standard-representation of $SO(n)$ .
Since $SO(n)$ preserves a bilinear form on $V$ there is a trivial 1-dimensional subrepresentation in $S^2V$ . So, in general, $S^k V$ seems not to be irreducible. Is there any known decomposition of the $k$ -th symmetric power $S^kV$ into irreducible $SO(n)$ -representations? I am coming from a different area and have little knowledge about general representation theory, but any hint or reference is welcome, thanks.","['representation-theory', 'algebraic-groups', 'linear-algebra', 'lie-groups']"
4413840,Ordering distinct objects into lines and a circle,"It is given that we have $n$ different objects and we want to arrange them in non-empty lines ,after that order these non-empty lines around a circle.How many ways are there in this question ? The given answer is $(2^n-1)!(n-1)!$ .It is given a hint such that use the composition of exponential generating functions. What i thought : Without using the hint i thought that if there are $k$ lines where $k \in \{1,2,3...\}$ , G.F. of these lines is $(x/(1-x)) ,$ so we can say that $$\sum_{k=1}^{\infty}[x^n]\bigg(\frac{x}{1-x}\bigg)^kn!$$ ways there are to arrange these $n$ distinct object.Moreover , i know that if i have $m$ distinct object , i can arrange them around a circle $(m-1)!$ ways.However , icould not employ it in this question because i think that when the number of objects in a line are the same of one another , these lines may be seen as equal and do not allow to use this formula , so i will need Polyas' Enumeration.So , the question will be torturous.. Hence , i want help here... How can i use the hint ,i.e, E.G.F to solve this question and reach the given answer.. Thanks in advanced !! $\mathbf{\text{Addentum:}}$ For $n=3$ ,the answer is $(2^3-1)(3-1)!=14$ according to @Marko Riedel's answer.However ,when i calculate it by brutal force ,i find different answer such that : $1-)$ For only one line : $3! \times (1-1)!=6$ ways $2-)$ For two lines : $3!\times C(1+2-1,1) \times (2-1)!=12$ ways $3-)$ For three lines : $3!\times  (3-1)!=12$ ways Result= $6+12+12=30$ , What am i missing here ? Why is it not equal to $14$ ?","['permutations', 'contest-math', 'combinatorics', 'discrete-mathematics', 'generating-functions']"
4413849,Product of Sum of Digits,"This is a bonus problem my teacher gave us during a competition math class: Is there a positive integer $n$ such that $S(n)S(n + 4)$ = $2022?$ The $S(n)$ means the sum of digits of $n.$ For example, if $n=14,$ then $S(n)=5$ . I've prime factorized $2022=2 \cdot 3 \cdot 337$ so far. And I've tried having $n=337$ and $n=666,$ both of which ended up fruitless. I've realized that there needs to be lots of $9$ s in the middle of the number to carry into $0$ but so far nothing has worked for me. Can I please have some help? Thanks!","['recreational-mathematics', 'discrete-mathematics']"
4413888,"Does there exist a polynomial indicator function over $\mathbb{Z}_p[x_1,\dots,x_{p^3}]$ of degree at most $O(p^2)$?","The Problem Let $p$ be a prime. Does there exist a $p^3$ -variable polynomial $P$ over $\mathbb{Z}_p[x_1,\dots,x_{p^3}]$ such that $P(\boldsymbol{0}) \equiv 0 \ (p)$ $P(\boldsymbol{x}) \equiv 1 \ (p)$ for any $\boldsymbol{x} \ne \boldsymbol{0}$ . $\deg{P} \in O{(p^2)}$ (its degree is at most some $2$ -degree polynomial in $p$ ) Meaning that $P$ is equivalent to the indicator function (or $1-$ the indicator function) over $\mathbb{Z}_p[x_1,\dots,x_{p^3}]$ , and has degree of at most $O(p^2)$ . Fermat's Little Theorem Due to Fermat's little theorem , for any prime $p$ : $$a^{p-1} \equiv \begin{cases}1 \quad \text{ if } a \not\equiv 0 \\ 0 \quad \text{ if } a\equiv 0\end{cases} \quad (p)$$ Trial and error I have tried the following potential candidates, but they all fail in at least one of these aspects: Trial 1: $$P(\boldsymbol{x}) \stackrel{?}{=} \left(\sum_{i=1}^{p^3} x_i^{p-1}\right)^{p-1}$$ $P(\boldsymbol{0}) \equiv 0 \ (p) \quad \checkmark$ But if $\boldsymbol{x} = (\underbrace{1,\dots,1}_p,\underbrace{0,\dots,0}_{p^3-p})$ , then $$P(\boldsymbol{x}) = \left(\sum_{i=1}^{p} 1 + \sum_{i=p+1}^{p^3} 0 \right)^{p-1} = p^{p-1} \equiv 0^{p-1} \equiv 0 \quad (p)$$ So in this case, condition 2 fails to hold. Trial 2: $$P(\boldsymbol{x}) \stackrel{?}{=} 1-\prod_{i=1}^{p^3} (1-x_i)^{p-1}$$ Clearly holds in this case. $\quad \checkmark$ Also holds, $P(\boldsymbol{x}) = 1$ exactly if none of the $x_i$ 's are $0$ , a.k.a., if $\boldsymbol{x} \ne \boldsymbol{0}$ . $\quad \checkmark$ However Condition 3 fails here. The polynomial's degree is $p^3(p-1) = O(p^4)$ . Trial 3: $$P(\boldsymbol{x}) \stackrel{?}{=} \left(\sum_{\begin{matrix}I \subseteq \{1,\dots,p^3\} \\ |I| = kp \\ k \in \{1,\dots,p^2\}\end{matrix}} \left( \sum_{i\in I} x_i^{p-1}  \right)^{p-1}\right)^{p-1} + \left(\sum_{i=1}^{p^3} x_i^{p-1}\right)^{p-1}$$ $P(\boldsymbol{0}) \equiv 0 \ (p) \quad \checkmark$ $P(\boldsymbol{x}) \equiv 1  \ (p)$ also holds here. The right term is the exact same as the entire polynomial in Trial 1; and the left term equals $1$ for exactly the $\boldsymbol{x}^{p-1}$ 's where number of $1$ 's it contains is divisible by $p$ . $\quad \checkmark$ However $\deg{P} = (p-1)^3 \in O(p^3)$ . (But, we're getting closer. The last example was $O(p^4)$ .) Note: $$P(\boldsymbol{x}) \stackrel{?}{=} 1-I(\boldsymbol{x} = 0)$$ where $I$ is the real indicator function. However, $P$ needs to be a polynomial , so I can't just use the ""regular"" indicator function. Context I am trying to prove a theorem using the Chevalley-Warning theorem , which requires certain conditions for the degrees polynomials. The main issue is actually to construct such a polynomial, and once that's done, the theorem would basically ""prove itself"". However, it is possible, that such a polynomial doesn't exist.","['modular-arithmetic', 'finite-fields', 'finite-groups', 'polynomials', 'group-theory']"
4413910,Lower bound on the cardinality of $\bigcup_{i=1}^n A_i$,"Question: Suppose I have $n$ finite sets $A_1,\dots,A_n$ contained in some fixed set $S$ , and I am given non-negative integers $N$ and $N_1,\dots,N_n$ such that each $A_i$ has cardinality $N$ , and each $k$ -tuple intersection has cardinality $\leq N_k$ . Can I use this to construct a good lower bound on the cardinality of $\bigcup_{i=1}^n A_i$ ? Answer by Tim Gowers: I don't know your reason for asking this question, so it's unlikely that what I'm about to write will be
helpful. Nevertheless, there's an easy method I like a lot for
deducing a lower bound just from the knowledge that $N_2$ is small. It
may be contained in what has been said above -- I haven't checked. The idea is to think of each set $A_i$ as a $01$ -valued function on a
set of size $M$ . We then look at the $\ell_ 2$ norm of $\sum_i A_i$ .
The square of the $\ell_ 2$ norm is $\sum_ {i,j}|A_ i \cap A_ j|$ ,
which by assumption is at most $nN + n(n-1)N_2$ . But we also have a
lower bound: the $\ell_ 1$ norm of the sum is $nN$ , from which it
follows that the square of the $\ell_2$ norm is at least $(nN)^2/M$ . Putting these two bits of information together gives a lower bound for $M$ of $nN/(1+(n-1)N_2/N)$ . So, for example, if $N_ 2 = cN$ for some
smallish positive constant $c$ , then the lower bound is roughly $N/c$ . This question has been asked in MathOverflow many years ago and really good answer has been given by professor Tim Gowers but some moments of his answer are unclear. I tried to write it down explicitly but I came across some obstacles so I'd be happy if you can help me to understand that! Suppose that $M=\lvert X\rvert,$ where $X:=\cup_{i=1}^n A_i$ and for each $i=1,\dots,n$ consider $f_i:X\to \{0,1\}$ defined by $$f_i(x) =
\begin{cases}
1, & \text{if }x\in A_i \\
0, & \text{if }x\notin A_i
\end{cases}.$$ We see that $\sum \limits_{x\in X}f_i(x)=|A_i|.$ What is the $\ell_2$ norm of $\sum \limits_{i=1}^n A_i$ ? Can anyone explain how to obtain those bounds please?","['combinatorics', 'extremal-combinatorics', 'discrete-mathematics']"
4413911,Derivative of the smallest eigenvalue,"Suppose $S_0, S$ are known real symmetric matrices and we have the function $f(x) := \lambda_{0}(S_0+x S)$ where $\lambda_0$ is the smallest eigenvalue. By symmetry, the eigenvalues are real and I believe the smallest eigenvalue is a smooth function of $x$ . How can I find the derivative $\frac{df(x)}{dx}$","['derivatives', 'calculus', 'linear-algebra', 'eigenvalues-eigenvectors']"
4414059,"Find the least positive integer $n$ such that $S(n)S(n + 2) = 2022$, where $S(n)$ is the sum of the digits of $n$.","I was doing my math homework and I came across this problem: Find the least positive integer $n$ such that $S(n)S(n + 2) = 2022$ , where $S(n)$ is the sum of the digits of $n$ . I tried letting $S(n)=2022,$ and this is what I got: $S(n)=2022,$ Thus $S(n+2)=1 \Rightarrow n+2=10^k.$ And $n=10^k-2 \Rightarrow S(n)=9k-1 \Rightarrow 9k=2023.$ But then $k$ is not a natural number so I don't know what to do now. Can I have help? Thanks!","['number-theory', 'recreational-mathematics', 'discrete-mathematics']"
4414089,Prove that $\int_{0}^{2\pi}f(x)\cos(kx)dx \geq 0$ for every $k \geq 1$ given that $f$ is convex.,"Given $f: [0, 2\pi] \to \mathbb{R}$ convex function, prove that for every $k\geq1$ \begin{align}
\int_{0}^{2\pi}f(x) \cos (kx)dx \geq 0
\end{align} I am completely stumped. What I have tried to do is return the query for $k=1$ , and for that value of $k$ try to write the integral from $0$ to $2\pi$ as a sum of four integrals from $0$ to $\dfrac{\pi}{2}$ and use the the theorem for first derivative monotony . No luck so far. Any help would be much appreciated. Edit 1: I saw the link here about a similarly asked topic. However, this process gets the general case as I perceive it and I am really supposed to use the method described above. I will try this and come back with a definitive answer. Edit 2: I cannot use the $f''(x) \geq 0$ argument due to the simple fact that I have not been formally taught it as part of the class. Edit 3: Final proof, with thanks to the contributors below. Let's start by setting $A = \frac{\pi}{2}$ and $B = \frac{3\pi}{2}$ . It follows from basic trigonometry that: \begin{align}
&\cos x \geq 0, \ x \in [A,B] \ \text{and}\\
&\cos x \leq 0, \ x \in [0, A] \cap [B, 2\pi].
\end{align} And we also set $L$ to be the line segment such that $L(A) = f(A), \ L(B) = f(B)$ . We will prove a basic property of said line in regards to the convex function $f$ . I can take for granted that (we proved this in class) \begin{align}
L(x) = \dfrac{x-A}{B-A}f(B) + \dfrac{B-x}{B-A}f(A)
\end{align} so for $x \in [A,B]$ there exists $\lambda \in [0,1]$ such that: $x = \lambda A + (1-\lambda) B$ . Taking the aforementioned expression and replacing it on $L(x)$ we get (I omit trivial algebra) \begin{align}
L(x) = (1-\lambda) f(B) + \lambda f(A).
\end{align} Since $f$ is convex, we can write \begin{align}
&f(\lambda A + (1-\lambda) B) \leq \lambda f(A) + (1-\lambda) f(B)\\
\implies &f(x) \leq L(x), \ \forall \ x \in [A,B] \ \text{and} \ \lambda \in [0,1].
\end{align} $\blacksquare$ We assume that there exists $x \in [0,A]: \ f(x) < L(x)$ . Then there exists $A \in [x, B] \ \text{and} \ \lambda \in [0,1]: \ A = \lambda x + (1-\lambda) B$ . Then \begin{align}
&L(A) = \dfrac{A-x}{B-x}f(B) + \dfrac{B-A}{B-x}f(x), \ A \in [x,B]\\
\implies &L(A) = (1-\lambda)L(B)+\lambda L(x)\\
\implies &L(A) = (1-\lambda)f(B) + \lambda L(x).
\end{align} Then assuming that $L(x) > f(x)$ we get \begin{align}
L(A) > (1-\lambda) f(B) + \lambda f(x) \geq f(A), \ \text{assuming convexity}.
\end{align} Because $L(A) = f(A)$ the above inequality becomes $L(A) > f(A)$ which is a contradiction. $\blacksquare$ In the same spirit, for $x \in [B, 2\pi]$ doing the exact same replacements and applying the exact same principles we also get \begin{align}
&f(\lambda B + (1-\lambda) 2 \pi) \leq \lambda f(B) + (1-\lambda) f(2\pi)\\
\implies &f(x) \leq L(x).
\end{align} $\blacksquare$ We then set $g(x) = f(x) - L(x)$ and from (1) and (2) above it is safe to assume that $\cos x$ and $g(x)$ will have the same sign in the whole domain, that is: \begin{align}
&g(x) \geq 0, \ \text{where} \ \cos x \geq 0\\
&g(x) \leq 0, \ \text{where} \ \cos x \geq 0.
\end{align} We have \begin{align}
\int_0^{2\pi} g(x) \cos x dx = \int_0^{\frac{\pi}{2}} g(x) \cos x dx + \int_{\frac{\pi}{2}}^{\frac{3 \pi}{2}} g(x) \cos x dx + \int_{\frac{3\pi}{2}}^{2\pi}g(x) \cos x dx \geq 0,
\end{align} because \begin{align}
&\text{in} \left[0, \frac{\pi}{2} \right], \ \cos x \geq 0 \implies g(x) \geq 0\\
&\text{in} \left[\frac{\pi}{2}, \frac{3 \pi}{2} \right], \ \cos x \leq 0 \implies g(x) \leq 0\\
&\text{in} \left[\frac{3 \pi}{2}, 2 \pi \right], \ \cos x \geq 0 \implies g(x) \geq 0.\\
\end{align} We have that \begin{align}
&\int_0^{2 \pi}\cos x dx = 0 \ \text{trivial}\\
&\int_0^{2 \pi}x \cos x dx = \int_0^{2 \pi}x (\sin x)' dx = \left[ x \sin x \right]_0^{2\pi} - \int_0^{2 \pi} \sin x dx = 0.
\end{align} It then follows that \begin{align}
\int_0^{2 \pi} f(x) \cos x dx = \int_0^{2 \pi} g(x) \cos x dx \geq 0
\end{align} which was previously proven. For the case $k=1$ , the proof is over. For $k>1$ , we have: \begin{align}
\int_0^{2\pi}f(x) \cos (kx) dx = \sum_{i=0}^{k-1} \int_{\frac{2\pi i}{k}}^{\frac{2\pi (i+1)}{k}} f(x) \cos (kx) dx.
\end{align} We perform the change of variable \begin{align}
x = \dfrac{y+2\pi i}{k}\\
\implies \begin{cases}dx = \dfrac{1}{k}dy\\ x = \dfrac{2\pi i}{k} \to y=0\\ x = \dfrac{2\pi (i+1)}{k} \to y = 2\pi \end{cases}.
\end{align} So the above sum becomes \begin{align}
\sum_{i=0}^{k-1} \dfrac{1}{k} \int_{0}^{2\pi}f \left(\dfrac{y+2 \pi i}{k} \right) \cos (y+2\pi i)dy.
\end{align} Because $f$ is convex in $[0, 2\pi]$ there exists $\lambda \in [0,1]$ such that: \begin{align}
	\theta f\left(\frac{y_1 + 2\pi i}{k}\right)
	+ (1 - \theta)f\left(\frac{y_2 + 2\pi i}{k}\right)
	&\ge f\left(\theta\frac{y_1 + 2\pi i}{k}
	+ (1 - \theta) \frac{y_2 + 2\pi i}{k}\right)\\
	&= f\left(\frac{\theta y_1 + (1 - \theta)y_2 + 2\pi i}{k}\right).
\end{align} Having performed the change of variables: \begin{align}
x_1 = \dfrac{y_1 + 2\pi i}{k} \ \text{and} \ x_2 = \dfrac{y_2 + 2\pi i}{k}.
\end{align} So $f \left( \dfrac{y + 2\pi i}{k}\right)$ convex on $[0, 2\pi]$ . Using the result from $k=1$ we have \begin{align}
\int_0^{2\pi} f \left( \dfrac{y + 2\pi i}{k} \right)\cos y dy \geq 0.
\end{align} $\blacksquare$","['convex-analysis', 'functional-analysis', 'real-analysis']"
4414118,Overlap of disk and grid cell,"I have a disk of radius $r$ centered at the origin. Space is divided into square grid cells of size $1$ , with the origin $(0, 0)$ being at the center of one such grid cell. For each grid cell in the plane, I need to know its overlap $f$ with the disk, meaning the fraction (or area) of the cell that is enclosed by the circle of radius $r$ . The figure below illustrates the setup, with non-zero $f$ values written within each such cell. Question What is the general formula for $f$ , given the radius $r$ and some cell specified by its integer coordinates $(i, j)$ ? What I already know I have the following for finding out whether a cell $(i, j)$ is completely outside or contained completely within the disk: $$
\begin{align}
\biggl(|i| - \frac{1}{2}\biggr)^2 + \biggl(|j| - \frac{1}{2}\biggr)^2 &\ge r^2 \Rightarrow f(i, j) = 0 \quad \text{(outside)} \\
\biggl(|i| + \frac{1}{2}\biggr)^2 + \biggl(|j| + \frac{1}{2}\biggr)^2 &\le r^2 \Rightarrow f(i, j) = 1 \quad \text{(inside)} \\
\end{align}
$$ When none of these are true, we have $0 < f < 1$ , for which I'm in need of a formula. I've also realized that the symmetry generally allows me to consider only the positive quadrant, i.e. $i\rightarrow |i|$ , $j\rightarrow |j|$ . The problem still seems hard as the circle cuts the cells in different ways. I think a piecewise function is required. The figure above is generated with the below Python script. Here $f$ is calculated (estimated) using the naive_count() function, which places a large number of points within the cell and checks whether each of them is within the disk or not. import numpy as np
import matplotlib.pyplot as plt

# Configuration
gridsize = 10
r = 2.8

# Plot grid
nyquist = gridsize//2
for i in range(-nyquist + 1, nyquist + 1):
    plt.plot([i - 0.5, i - 0.5], [-nyquist + 0.5, nyquist - 0.5], 'k-')
for j in range(-nyquist + 1, nyquist + 1):
    plt.plot([-nyquist + 0.5, nyquist - 0.5], [j - 0.5, j - 0.5], 'k-')
plt.plot(0, 0, 'k.')

# Plot circle
theta = np.linspace(0, 2*np.pi, 1000)
plt.plot(r*np.cos(theta), r*np.sin(theta), 'C0')

def naive_count(r, i, j):
    N = 100
    count = 0
    for     x in np.linspace(i - 0.5, i + 0.5, N):
        for y in np.linspace(j - 0.5, j + 0.5, N):
            if x**2 + y**2 < r**2:
                count += 1
    return count/N**2

# Fill in each grid cell with a value
for     i in range(-int(np.ceil(r - 0.5)), int(np.ceil(r - 0.5)) + 1):
    for j in range(-int(np.ceil(r - 0.5)), int(np.ceil(r - 0.5)) + 1):
        I = abs(i)
        J = abs(j)
        if (I - 0.5)**2 + (J - 0.5)**2 >= r**2:
            # Completely outside
            continue
        if (I + 0.5)**2 + (J + 0.5)**2 <= r**2:
            # Completely within
            frac = 1
        else:
            # Partly within
            frac = naive_count(r, i, j)
        plt.text(i, j + 0.2, f'{frac:.2f}', ha='center', fontsize=9)
        plt.fill(
            [i + 0.5, i - 0.5, i - 0.5, i + 0.5],
            [j + 0.5, j + 0.5, j - 0.5, j - 0.5],
            color=[1 - frac, 1, 1 - frac], zorder=-1,
        )

# Finalize plot
plt.xlabel(r' $i$ ')
plt.ylabel(r' $j$ ')
plt.axis('image')
plt.xlim(-nyquist + 0.5, nyquist - 0.5)
plt.ylim(-nyquist + 0.5, nyquist - 0.5)
plt.tight_layout()
plt.savefig('grid.png')","['euclidean-geometry', 'circles', 'geometry']"
4414200,"Maximizing $\sum_{i=1}^{10} \cos(3x_i)$, for real $x_i$ such that $\sum_{i=1}^{10} \cos(x_i)=0$","Determine the greatest possible value of $\sum_{i=1}^{10} \cos(3x_i)$ for real $x_1,x_2,\cdots,x_{10}$ , where $\sum_{i=1}^{10} \cos(x_i)=0$ . My approach was to somehow get a equation solely in terms of one of the $\cos x_i$ and then differentiating and getting the maximum according to the range of cos which is $[-1,1]$ . For achieving this, I first inductively proved that $$a^3+b^3 +c^3 +\cdots+n^3= 3\cdot(\text{sum of product of terms taking three at a time})$$ but that just makes the expression to be that we need to maximize the value of $$12\cdot(\text{sum of product of terms taking three at a time})$$ But now how do we get an equation in just one variable?",['trigonometry']
4414207,Jun Shao's Mathematical Statistics - Lemma 2.1,"I am trying to understand Jun Shao's proof on Lemma 2.1 of his Mathematical Statistics book. The statement is that: If a family of population $\mathcal{P}$ is dominated by a $\sigma$ -finite measure, then $\mathcal{P}$ is dominated by a probability measure $Q = \Sigma_{i=1}^{\infty}c_i P_i$ , where $c_i$ 's are non-negative constants with $\Sigma_{i=1}^{\infty}c_i = 1$ and $P_i \in \mathcal{P}$ . Shao only proved the case for finite measure $\nu$ . In his proof, he defined $\mathcal{C}$ to be the class of events $C$ for which there exists $P \in \mathcal{P}_0$ such that $P(C) > 0$ and $dP / d\nu > 0$ a.e. $\nu$ on $C$ . I have two questions: Why does there exist a sequence $\{C_i\} \subset \mathcal{C}$ such that $\nu(C_i) \rightarrow \sup_{C \in \mathcal{C}} \nu(C)$ ? Does it have anything to do with the finiteness of $\nu$ or the assumption that $dP / d\nu > 0$ a.e. $\nu$ on $C$ ? The assumption that $dP / d\nu > 0$ a.e. $\nu$ on $C$ is not very intuitive to me. Why do we need it to complete the proof? May someone explain to me the motivation behind this assumption? Thank you in advance. Here's the original statement and the proof of the lemma:","['measure-theory', 'probability-theory', 'statistics']"
4414224,"If the random variables $(v_k)$ are i.i.d, are the $(v_k-m_k)$ i.i.d. as well?","This is my first time asking, so if my question is lacking, please let me know so I can edit it. $v_{k}$ is iid across $k$ where $k=1,....K$ and I can represent its CDF as $F(v)$ . ( Note : I have very large $K$ for sure ) I am creating a pseudo reservation value such that $\hat{v}_{k} = v_{k} - \max(0, \max_{k^{'}\neq k}(v_{k^{'}}-c_{k^{'}}))$ where $c_{k^{'}}$ is just constant unique to $k^{'}$ . Let's say $\hat{v}_{k}$ follows new CDF called $\hat{F}(\hat{v_{k}})$ . Goal : I want to show $\hat{F}(\hat{v_k})$ is iid across $k$ ( or at least for majority of $k$ ) Attempt is the following.
Assume $v_{1}-c_{1} > v_{2}-c_{2} > v_{3}-c_{c} > ... > v_{K}-c_{K}$ . Then, we have the follwing. $$\hat{v}_{1} = v_{1} - \max(0, v_{2}-c_{2}) \quad k=1$$ $$\hat{v}_{k} = v_{k} - \max(0, v_{1}-c_{1}) \quad \text{for } k = 2,...,K$$ Current Conclusion : (1) $Pr(\hat{v}_{k}\leq x) = Pr(v_{k} \leq x+ \max(0, v_{1}-c_{1}))$ for $k=2,...K$ . Since I can treat $\max(0, v_{1}-c_{1})$ as some exogenous constant term, and $F(v_{k})$ are identically distributed, $\hat{F}(\hat{v_k})$ is identically distributed for at least $k=2,...,K$ . (2) For sets $A_{2},...,A_{K}$ of possible values of $v_{k}$ , $k=2,...,K$ , $$Pr(\hat{v}_{2} \in A_{2} \text{ & } ... \text{ & } \hat{v}_{K} \in A_{K})$$ $$=Pr(v_{k} \in \{ x+\max(0, v_{1}-c_{1}) \quad   x \in A_{k}\}) \text{   for } k= 2,...,K$$ $$ = \Pi_{k=2}^{K} Pr(v_{k} \in \{ x+\max(0, v_{1}-c_{1}) \quad   x \in A_{k}\}))  $$ because $v_{2}, ... v_{K}$ are independent and we have exogenous constant term $$=Pr(\hat{v}_{2} \in A_{2}) ... Pr(\hat{v}_{K} \in A_{K})$$ Thus , can I say that $\hat{F}(\hat{v}_{k})$ is iid for all $k$ except one?","['statistics', 'probability-distributions', 'probability-theory']"
4414245,Does $\lim_{x\to\infty} f(x+1)/f(x) = 1$ imply $\lim_{x\to\infty} f(f(x+1))/f(f(x)) = 1$?,"As mentioned above, does \begin{align}
\lim_{x\to\infty} \frac{f(x+1)}{f(x)} = 1
\end{align} imply \begin{align}
\lim_{x\to\infty} \frac{f(f(x+1))}{f(f(x))} = 1?
\end{align} Here, $f:\mathbb{R}\to \mathbb{R}$ is a function such that (1) $f(x)\neq 0$ for all $x\in\mathbb{R}$ (or for all $x\ge x_0$ for some $x_0$ ); (2) $\lim_{x\to\infty} f(x) =\infty$ ; (3) $f$ is increasing. This is not a homework question, but I just wonder if this statement is true (Indeed, I observe that this holds for $f(x)=\log(x)$ so I have this question). I would appreciate any counterexample/proof/hints. Many thanks!","['limits', 'calculus']"
4414314,Sum of independent continuous and discrete random variables,"I am trying to understand how to create a PDF from the sum of an independent continuous and discrete rv, Z = X + Y . I was wondering if anyone would be willing to provide an example of this. I have seen the equation FZ(z) = ∑ FY(z−x) . PX(x) , but I can't seem to make sense of how the z's and x's are taken as input into FY. Specifically I am working with a Bernoulli and Gaussian, but an example with any continuous and discrete random variables would be appreciated. Thanks.","['statistics', 'random-variables']"
4414347,Can I prove this inequality only by elementary tools?,"Let $\Omega\in\mathbb{R}^2$ be a bounded domain, $\forall a,b,c \in \mathbb{R}$ , I hope to prove that there exists a positive constant $\alpha$ such that: $$
\int_{\Omega} (ax_2+b)^2 + ( -ax_1+c)^2 \geqslant \alpha a^2
$$ where $\alpha$ is independent in $a,b,c$ . I really hope for a directly proof instead of the proof by contradiction.","['inequality', 'integral-inequality', 'functional-analysis', 'analysis']"
4414364,How to establish that an anti-idempotent matrix is singular?,"In linear algebra, idempotent matrices are defined by $$
A^2 = A \tag{1}
$$ for a square matrix $A$ . Obviously, the identity matrix $I$ is an idempotent matrix. It can be also shown that if $M$ is idempotent, then $I - M$ is idempotent by a trivial calculation. $$
(I - M) (I - M) = I - M - M + M^2 = I - M - M + M = I - M
$$ In a similar manner, we can define an anti-idempotent matrix $A$ by the condition $$
A^2 = - A \tag{2}
$$ (A trivial example is the zero matrix.) To find non-trivial examples of an anti-idempotent matrix $A$ , I considered the case of $(2 \times 2)$ matrices: $$
A = \left[ \matrix{ a & b \cr
                    c & d \cr} \right]
$$ If $A$ is anti-idempotent, then it must satisfy: $A^2 = -A$ . This leads to a set of $4$ equations: $$
a^2 + b c = - a, \ a b + b d = - b, \ c a + c d = -c, \ \ b c + d^2 = -d 
$$ A simple manipulation results in the equations $$
b (a + d + 1) = 0, c (a + d + 1) = 0, a^2 + b c = -a, b c + d^2 = -d.
$$ Taking $a = 2$ , we see that $a + d + 1 = 0$ or $d = -3$ . We can choose $b$ and $c$ from $b c = -6$ . One choice is $b = 2, c = -3$ . Thus, an anti-idempotent matrix is: $$
A = \left[ \begin{array}{cc}
            2 & 2 \\
            -3 & -3 \\
        \end{array} \right]
$$ (It is easy to check that $A^2 = -A$ .) It can be easily shown : If $M$ is an anti-idempotent matrix, then $I + M$ is also anti-idempotent. Indeed, $$
(I + M) (I + M) = I + M + M + M^2 = I + M + M - M = I + M.
$$ The examples I considered for anti-idempotent matrices yield singular matrices. I like to know if it is generally true that anti-idempotent matrices are singular matrices. How to establish this result? Your comments are welcome.",['matrices']
4414385,Set of points where stalk is integral domain is open,"I struggled to find a solution for the exercise 4.9 in the second chapter of Liu's book Algebraic Geometry and Arithmetic Curves . The first part is to show the set of points $x\in X$ such that $\mathcal{O}_{X,x}$ is reduced is an open subset when $X$ is a locally Noetherian scheme. This is not a problem because since $X$ is locally Noetherian we can assume to work with an affine Noetherian scheme $\text{Spec}A$ , where $A$ is a Noetherian ring, and $\{\mathfrak{p}\in\text{Spec}A | A_{\mathfrak{p}} \text{is reduced}\}=\{\mathfrak{p}\in\text{Spec}A | (N_A)_{\mathfrak{p}}=0\}=\text{Spec}A-\text{supp}(N_A)=\text{Spec}A-V(Ann(N_A))$ and this is open. The second part is the same thing with integral domains, show the set of points $x\in X$ such that $\mathcal{O}_{X,x}$ is an integral domain is an open subset when $X$ is a locally Noetherian scheme. This is where my problems start. As above we can assume to work with an affine and Noetherian scheme $\text{Spec}A$ with $A$ Noetherian ring. I think I have to find a characterization of the prime ideals where the localization is an integral domain, something like the reduced case. After hours of attemps, my ideas are: if $\frac{a}{s}\frac{b}{t}=0\in A_{\mathfrak{p}}\Leftrightarrow $ exists $u\in A-\mathfrak{p}$ such that $abu=0$ in $A$ and so $a,b$ must be zero-divisors in $A$ . if $a,b\ne0$ in $A$ and $ab=0$ , in $A_{\mathfrak{p}}$ at least one of $a,b$ must be zero in $A_{\mathfrak{p}}$ , otherwise $\frac{a}{1}\frac{b}{1}=0$ , and so $\mathfrak{p}\notin \text{supp}(a)$ or $\mathfrak{p}\notin\text{supp}(b)$ . If we take $D$ the set of the zero-divisors of $A$ without zero, $$ M_{(a,b)}=\text{supp}(a)^c\cup \text{supp}(b)^c=(\text{supp}(a)\cap \text{supp}(b))^c, \quad \forall a,b\in D$$ then each $M_{(a,b)}=\text{Spec}A-V(Ann(a)+Ann(b))$ is open in $\text{Spec} A$ . This is my claim: $U=\bigcap_{a,b\in D}M_{(a,b)}$ is exactly the set of prime ideals such that the localization is an integral domain.
If $\mathfrak{p}$ is such that $A_{\mathfrak{p}}$ is an integral domain, then $\forall a,b\in D$ necessarily $\mathfrak{p}\in(\text{supp}(a)\cap \text{supp}(b))^c=M_{(a,b)}$ . If $\mathfrak{p}$ is such that $A_{\mathfrak{p}}$ is not an integral domain, then there exist $\frac{a}{s},\frac{b}{t}\in A_{\mathfrak{p}}$ such that $\frac{a}{s}\frac{b}{t}=0$ and hence $a,b$ are zero-divisors of $A$ and $\mathfrak{p}\in\text{supp}(a)\cap \text{supp}(b)=M_{(a,b)}^c$ and $\mathfrak{p}\notin U$ . The problem of this construction is that the set $U$ is not open and I can't conclude. Is this the right way to follow? Could someone give me some hints or advices?
Thanks in advance to those who can answer me.","['algebraic-geometry', 'schemes', 'commutative-algebra', 'ideals']"
4414426,"The order statistics of uniform distribution $U_{n,k_n}\rightarrow p$ a.s., when $\frac{K_n}{n} \rightarrow p$","Let $U_1,U_2,...U_n,$ be iid samples from uniform distribution $U(0,1)$ . And the order Statistic: \begin{equation}
U_{n,1}\leq U_{n,2}\leq ...\leq U_{n,n},
\end{equation} Given $p\in(0,1)$ , if $1\leq K_n\leq n$ and $\frac{K_n}{n}\rightarrow p$ . Proof $U_{n,k_n}\rightarrow p$ a.s. My ideas so far: The density function of $U_{n,k_n}$ is \begin{equation}
f_n(x)=\frac{n!}{(k_n-1)!(n-k_n)!}x^{k_n-1}(1-x)^{n-k_n}.
\end{equation} Then I would like to proof: \begin{equation}
\sum_n P(|U_{n,k_n}-p|>\epsilon)<\infty
\end{equation} Then by Borel-Cantelli lemma: \begin{equation}
P(|U_{n,k_n}-p|>\epsilon \quad\text{i.o.})=0
\end{equation} which means: \begin{equation}
U_{n,k_n}\rightarrow p = 0. \quad a.s.
\end{equation} However, the integral of $P(|U_{n,k_n}-p|>\epsilon)<\infty$ is hard to calculate. Do I miss something? Does there exist some easy way to proof this? Thanks in advance for any help!","['probability-limit-theorems', 'uniform-distribution', 'probability-distributions', 'probability-theory', 'probability']"
4414446,"$y = \sin^{-1}x + (\sin^{-1}x)^2$, find the value of $\frac{d^{2r + 1}y}{dx^{2r + 1}}$ at x = 0",Question: If $y = \sin^{-1}x + (\sin^{-1}x)^2$ show that the value of $\frac{d^{2r + 1}y}{dx^{2r + 1}}$ when x = 0 is $\frac{1}{2r}(\frac{(2r)!}{r!})^{2}$ I got $Dy = \frac{1 + 2\sin^{-1}x}{\sqrt{1-x^2}}$ and I know that $D^{2r + 1}y$ is $D^{2r}(Dy)$ but I'm clueless as to how to move further from this point. What am I supposed to do? Please provide a solution if you have idea.,"['calculus', 'derivatives']"
4414477,When is a group von Neumann algebra a factor?,"It is well-known that a von Neumann algebra (on a separable Hilbert space) can be written as a direct integrals of factors, i.e., von Neumann algebras with center $\mathbb C I$ . As such, factors play a central role in the structure theory of von Neumann algebras. A prime example of von Neumann algebras are group von Neumann algebras, constructed as follows: Given a countable discrete group $G$ , let $$
\lambda\colon G\to B(\ell^2(G)),\,\lambda_g\delta_h=\delta_{gh}.
$$ The group von Neumann algebra $L(G)$ is the von Neumann algebra generated by $\lambda(G)$ . Question: When is the group von Neumann algebra a factor?","['von-neumann-algebras', 'group-theory', 'functional-analysis', 'operator-algebras']"
4414486,"Definition of certain Gateaux differentials of the norm in E. R. Lorch: ""A Curvature Study of Convex Bodies in Banach Spaces""","In the paper E. R. Lorch: A Curvature Study of Convex Bodies in Banach Spaces from 1953, the following assumptions and definitions are stated in section II (p. 107-108): Let $(B, \| \cdot \|)$ be a real Banach space and $B^*$ its dual space .
Let $r > 1$ and $G(x) := \frac{1}{r} \| x \|^r$ and $\phi(\alpha, \beta) := G(x + \alpha y + \beta z)$ , where $x, y, z \in B$ are fixed and $\alpha, \beta \in \mathbb{R}$ .
Assuming that $\phi$ is twice continuously differentiable , the derivative of $\phi$ with respect to $\alpha$ at $\alpha = \beta = 0$ will be denoted the $G_y(x)$ . [...]
It is clear that $G_x(x) = r G(x)$ . [...]
The second derivatives of $\phi$ at $\alpha = \beta = 0$ will be denoted by $G_{y, y}(x)$ , $G_{y, z}(x)$ and $G_{z, z}(x)$ . [...] It is clear that $G_{y, z}(x) = G_{z, y}(x)$ . I am trying to understand the functions $G_y$ , $G_{y, y}$ , $G_{y, z}$ and $G_{z, z}$ .
Firstly, we have $G \colon B \to \mathbb{R}$ and $\phi \colon \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ . I would write $$
G_y(x)
= \frac{\partial}{\partial \alpha}\bigg|_{\alpha = \beta = 0} \phi(\alpha, \beta)
= \lim_{\gamma \to 0} \frac{\phi(\gamma, 0) - \phi(0, 0)}{\gamma}
= \lim_{\gamma \to 0} \frac{G(x + \gamma y) - G(x)}{\gamma}
= \text{d}G(x; y),
$$ where the last expression is the Gateaux derivative of $G$ at $x$ in direction $y$ .
This would make sense since we can then verify $G_x(x) = r G(x)$ (which is stated as ""clear"" above): for $x \in B$ we have $$
G_x(x)
= \lim_{\gamma \to 0} \frac{G(x + \gamma x) - G(x)}{\gamma}
= G(x) \lim_{\gamma \to 0} \frac{(1 + \gamma)^r - 1}{\gamma}
= r G(x).
$$ Furthermore, $y \mapsto G_y(x)$ is a linear bounded map $B \to \mathbb R$ , which seems to be what is stated in theorem 1 (""If $x$ is any fixed element in $B$ , then $G_y(x)$ represents a bounded linear functional ""). How do the corresponding expressions for $G_{y, y}(x)$ , $G_{y, z}(x)$ and $G_{z, z}(x)$ look like?
In particular, are the second derivatives both taken with respect to $\alpha$ or are they mixed? Wikipedia gives the following as one definition of the second order Gateaux derivative of $G$ : $$
D^2 G(x)\{y ,z \}
:= \lim_{\gamma \to 0} \frac{\text{d}G(x + \gamma y; z) - \text{d}G(x; z)}{\gamma}
= \frac{\partial^2}{\partial \alpha \partial \beta} \bigg|_{\alpha = \beta= 0} \phi(\alpha, \beta).
$$ Is this expression $G_{y, z}(x)$ ? If so, we would presumably have $$
G_{y, y}(x)
= \frac{\partial^2}{\partial \alpha^2} \phi(\alpha, \beta) \bigg|_{\alpha = \beta = 0}
= \frac{\partial^2}{\partial \alpha^2} G(x + \alpha y + \beta z) \bigg|_{\alpha = \beta = 0}
= \lim_{\gamma \to 0} \frac{\text{d}G(x + \gamma y; y) - \text{d}G(x; y)}{\gamma}
$$ and analogously $$
G_{z, z}(x)
= \frac{\partial^2}{\partial \beta^2} \phi(\alpha, \beta) \bigg|_{\alpha = \beta = 0}
= \frac{\partial^2}{\partial \beta^2} G(x + \alpha y + \beta z) \bigg|_{\alpha = \beta = 0}.
$$ According to Theorem 2 in that paper, we have $G_{y, z}(x) \in B^*$ for $y \in B$ and $G_y(x) \in B^*$ for $x \in B$ .
Do my above guesses about $G_{y, y}$ , ... fulfill those requirements?","['banach-spaces', 'gateaux-derivative', 'normed-spaces', 'dual-spaces', 'derivatives']"
4414526,"Do three distinct functions f, g and h exist such that f'=g, g'=h and h'=f?","First year math student here taking Analysis II. Today we learned about sinh, cosh and tanh and how to take their derivatives. Our teacher, in his magnanimity, wrote on the board after fiddling with sinh and cosh that sinh'=cosh and cosh'=sinh. I noticed the oscillatory nature of this, and after commenting on it, quickly asked the question you see up above. He wasn't able to provide an answer; a friend of mine inquired on it and we ended up learning that functions whose nth derivative is itself have a special name. Yet we are no closer to finding three distinct functions that fit the bill. I would also kindly ask that if you are able to generalize this to the nth derivative please do so!","['derivatives', 'real-analysis']"
4414575,"$\limsup \frac{X_n}{a_n}=1 \Leftrightarrow \limsup \frac{\max\{X_1,...,X_n\}}{a_n}=1$","$\{X_n\}_{n=1}^\infty$ is non-negative random variables, $0\leq a_n \uparrow\infty$ . Then: \begin{equation}
\limsup \frac{X_n}{a_n}=1 \quad a.s.\Leftrightarrow \limsup \frac{\max\{X_1,...,X_n\}}{a_n}=1 \quad a.s.
\end{equation} My ideas so far: Since $X_n$ and $a_n$ are non-negative, we have: \begin{equation}
0\leq \frac{X_n}{a_n}\leq \frac{\max\{X_1,...,X_n\}}{a_n}
\end{equation} So, if $\limsup \frac{\max\{X_1,...,X_n\}}{a_n}=1$ , then $\limsup\frac{X_n}{a_n}\leq 1.$ But I cannot going on to prove $\limsup\frac{X_n}{a_n}\geq 1.$ Thanks in advance for any help!","['sequences-and-series', 'probability-theory', 'probability', 'real-analysis']"
4414621,A Gauss-Markov process is always a solution to a Langevin type SDE?,"Let $(X_t)_{t\in[0,T]}$ be a Gauss-Markov process , that is, a Gaussian process satisfying the Markov property.
On page 230 of the book ""Statistical Orbit Determination"" by Bob Schutz, Byron Tapley, George H. Born the authors claim that all Gauss-Markov processes obey Langevin equations: $$ dX_t=-\beta X_t dt + \sigma dW_t$$ for some $\beta \geq 0, \sigma >0$ . That would mean that all Gaussian-Markov processes are either stationary (and in particular are Ornstein-Ulenbeck processes) or scaled Brownian motion (when $\beta =0$ ). Is this true? If so, how do we prove that all Gaussian-Markov processes are solutions to Langevin equations? I believe it is not true. There must be a Gauss-Markov process that is not stationary (and not scaled Brownian motion). But I cannot produce an example. EDIT: The Brownian bridge is Gauss-Markov and does not obey a Langevin SDE. So clearly, there is something wrong with the claim. If somebody can shed some light on the type of SDE that Gauss-Markov processes satisfy (if this type of SDE exists). To define a Gauss-Markov process, we only need its covariance. If we suppose that the process is never $0$ is probability one on $(0,T)$ , then $Cov(X_s,X_t)=f(\min(s,t)) g(\max(s,t))$ where $\frac f g$ is positive nondecreasing. These functions $f$ and $g$ should play a role in the potential SDE.","['stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4414650,Rudin's PMA: theorem 10.43,"This is the definition which we need for the proof of the theorem : This is the definition of the $\mathscr C''$ - equivalent : There is the theorem: Suppose E is an open set in $R^3$ , $u$ $\in$ $\mathscr C''(E)$ , and $G$ is a vector field in $E$ , of class $C''$ . $(a)$ if $F$ $=$ $\nabla$$u$ , then $\nabla$ $\times$ $F$ $=$ $0$ . $(b)$ if $F$ $=$ $\nabla$ $\times$ $G$ , then $\nabla$ $\cdot$ $F$ $=$ $0$ . Furthermore, if $E$ is $\mathscr C''$ -equivalent to a convex set, then $(a)$ and $(b)$ have converses, in which we assume that $F$ is a vector field in $E$ , of class $\mathscr C'$ : $(a')$ if $\nabla$ $\times$ $F$ $=$ $0$ , then $F$ = $\nabla$$u$ for some $u$ $\in$ $\mathscr C''(E)$ . $(b')$ if $\nabla$ $\cdot$ $F$ $=$ $0$ , then $F$ $=$ $\nabla$ $\times$ $G$ for some vector field $G$ in $E$ , of class $\mathscr C''$ There is the proof  : If we compare the definitions of $\nabla$$u$ , $\nabla$ $\times$ $F$ , and $\nabla$ $\cdot$ $F$ with the differential forms $\lambda_F$ and $\omega_F$ given by ( $124$ ) and ( $125$ ), we obtain the following four statements: $F$ $=$ $\nabla$$u$ if and only if $\lambda_F$ $=$ $du$ .                    ( $\star$ ). $\nabla$ $\times$ $F$ $=$ $0$ if and only if $d\lambda_F$ $=$ $0$ .             ( $\ast$ ) $F$ $=$ $\nabla$ $\times$ $G$ if and only if $\omega_F$ $=$ $d\lambda_G$ .  ( $\oplus$ ) $\nabla$ $\cdot$ $F$ $=$ $0$ if and  only if $d\omega_F$ $=$ $0$ .            ( $\circ$ ) I could n't understand these four statements ( $\star$ ),( $\ast$ ),( $\oplus$ ),( $\circ$ ). Any help would be appreciated.","['vector-fields', 'real-analysis', 'multivariable-calculus', 'calculus', 'convex-analysis']"
4414747,Differential form computation,"I was reading a text on differential geometry and I noticed this:
""Given $\omega=xdy\wedge dz+ydz\wedge dx+zdx\wedge dy\in\Omega^2(S^2)$ with $S^2=\{p\in\mathbb{R}^3:\lvert p\rvert=1\}$ , show that $A^{*}\omega=(\det A)\omega$ for $A\in O(3)$ "". I know some basic computation rules of differential forms but this really stacks me. Any help?","['orthogonal-matrices', 'differential-forms', 'differential-geometry']"
4414750,Can a function be differentiable if the limit does not exist?,"I'm trying to find if the function is differentiable at $x=1$ . Upon solving, the limit does not exist. But when I tried to solve for the limit of $f '(1)$ , I both got $3\over2$ , so $f '(1)$ exists. So, is the function differentiable at $x=1$ ? $$f(x) = \begin{cases}
\ln(x^2+x) & x ≤ 1 \\
3\sqrt x & x > 1 \\
\end{cases}$$ I. $$ f(1) = \ln (1^2 +1) = \ln(2)$$ II. $$\lim_{x\to1^-} f(x) = \lim_{x\to1^-} \ln(x^2+x) = \ln(1^2+1) = \ln(2)$$ $$\lim_{x\to1^+} f(x) = \lim_{x\to1^+} 3 \sqrt x= 3\sqrt1 = 3 $$ $$\lim_{x\to1^-} f(x) ≠ \lim_{x\to1^+} f(x) $$ Thus the $\lim_{x\to1}f(x)$ does not exist. $$f'(x) = \begin{cases}
\frac{2x+1}{x^2+x} & x < 1 \\
\frac{3}{2\sqrt x} & x > 1 \\
\end{cases}$$ a) $$ f'_+(1) = \lim_{x\to1^+}f'(x )=  \lim_{x\to1^+}\frac{3}{2\sqrt x} = \frac{3}{2\sqrt1} = \frac{3}{2}$$ b) $$ f'_-(1) = \lim_{x\to1^-}f'(x)=  \lim_{x\to1^+}\frac{2x+1}{x^2+ x} = \frac{2 + 1}{1+1} = \frac{3}{2}$$","['limits', 'calculus', 'derivatives']"
4414801,Proving that there exists a bijection of A with a proper subset of itself.,"I simply want to know exactly how this particular function is bijective. Since they defined our function $g$ , I would like to assume the set A to be written as $A=\ \left\{a_{\ 1}{,}\ a_{\ 2}{,}\ a_{\ 3}....\right\}$ so therefore from the defined function we can assume that $B=\ \left\{a_{\ 1}\right\}$ , so if we $A-B$ , we are left with $A - B =\ \left\{a_{\ 2}{,}\ a_{\ 3}{,}\ a_{\ 4}....\right\}$ Now if we pick $a_{\ 1}$ from set $B$ and plug it into our function g we should be getting $a_{\ 2}$ since $g\left(a_{\ n}\right)=\ a_{n+1\ }$ for $a_{n\ \ }\in \ B$ at the same time if we choose $\left\{a_{2\ }\right\}\in \ A-B$ into $g$ we would get $a_{\ 2}$ So now $a_{\ 2}$ (in the range of the function) has two pre-image. I fail to understand how this function can be considered a bijection? (my apologies if my question is too elementary, I simply want to understand this theorem)",['elementary-set-theory']
4414849,Probability of spelling IDAHO correctly,"I'm working with a problem stated as follows: A sign reads IDAHO. Two letters are removed and put back at random, each
equally likely to be put upside down as in the correct orientation. What is the
probability that the sign still reads IDAHO? So, we form our events as: $A = \{$ IDAHO is spelt correctly $\}$ $B = \{$ 2 ""symmetric"" characters are chosen $\}$ $C = \{$ 1 ""symmetric"" and 1 ""unsymmetric"" character is chosen $\}$ $D = \{$ 2 ""unsymmetric"" characters are chosen $\}$ Where symmetric means that no matter of orientation, the character looks just like it used to before. Notice, $B,C,D$ form a partition of our sample space. Hence, the the total law of probability gives us: $$P(A) = P(A|B)P(B) + P(A|C)P(C) + P(A|D)P(D) $$ We trivially know that $P(A|B) = 1/2$ , since in choosing two symmetric letters. Furthermore, $P(B) = \frac{\begin{pmatrix}
3 \\2
\end{pmatrix}}{\begin{pmatrix}
5 \\2
\end{pmatrix}} = 3/10$ Then, we have $P(A|C) = 2/4$ , since in choosing 1 symmetric letter and 1 unsymmetric, we have to get the correct position on the unsymmetric letter, whereas the symmetric letter can be switched in orientation without changing the word. Meaning we have 2 out of a total 4 possible scenarios. 4 comes from the fact that we can choose to place the symmetric letter in 2 positions, and orient it in 2 ways, however, these are equivalent. Then, the latter unsymmetric letter has already been given a position, but can be oriented in 2 distinct ways. Furthermore, $P(C) = \frac{\begin{pmatrix}
3 \\2 
\end{pmatrix}\begin{pmatrix}
2 \\1 
\end{pmatrix}}{\begin{pmatrix}
5 \\2
\end{pmatrix}} = 3/5$ Lastly, $P(A|D) = 1/6$ , since we only have one scenario in which it can become the same word, and the total scenarios are given as we choose a position for the first letter (2), then we choose orientation (2), the latter letter has already been given a position and orientation can be choosen (2). Also $P(D) = \frac{\begin{pmatrix}
2 \\
2\end{pmatrix}}{\begin{pmatrix}
5 \\2
\end{pmatrix}} = 1/10$ Finally, we have $P(A) = 1/2 \cdot 3/10 + 1/2 \cdot 3/5 + 1/6 \cdot 1/10 = 7/15$ , which according to the answer sheet is wrong ( $5/16$ ). Does anyone of you guys see the mistake. I've tried to find it and gone through my argumentation but can't find the mistake. Thanks.",['probability']
4414892,Why doesn't my combinatorial solution to simple probability problem work?,"I need to find the probability of NOT getting an ace card on two draws from a deck of 52 cards. My first thought (which I really think is correct) was to get the probability from taking $\frac{48}{52}\frac{47}{51}=\frac{\binom{48}{2}}{\binom{52}{2}}\approx0.85$ . Isn't this correct? Then I though about it in another way. There are $\binom{52}{2}$ ways too choose two cards from 52. To not get an ace, you can to choose two out of 12 values (where the ace is excluded) in $\binom{12}{2}$ ways, and then you can choose $\binom{4}{1}=4$ different cards from each of the chosen values. Then I'm thinking you could calculate the probability by taking $$\frac{\binom{12}{2}\binom{4}{1}^{2}}{\binom{52}{2}}\approx0.80$$ This doesn't give the same as the first method I used. I'm obviously missing something, but can't really figure out what is wrong. I would appreciate some guidance on how I should think differently doing the later method! (I suppose the probability is approx. 0.85 as I got in the first place.)","['combinatorics', 'probability']"
4414963,Continuous bounded martingale of finite variation (ito lemma?),"The question: Suppose that $(M_t) $ $ : t \in [0,\infty)$ is a bounded continuous martingale with finite variation. Prove that $M_t^2
 = M_0^2
 + 2 \int_0^t 
M_s dM_s,$ where the final integral is almost surely well-defined. (Hint: Cite here known results about
one-dimensional functions of bounded variation.) Deduce that $M$ is almost surely constant My attempt Im fairly sure the last part is quite easy by taking the expected value of both sides and knowing that the stochastic integral is a martingale. My struggle lies within the first bit. Can I just use Itô formula using $f(x) = \frac{x^2}{2} $ ? That is to say: $f(M_t) -f(M_0) = \int_0^t f'(M_s)dM_s + \frac{1}{2}\int_0^tf''(M_s)ds$ This then gives me $\frac{M_t^2}{2} - \frac{M_0^2}{2} = \int_0^tM_sdM_s + t$ This is very nearly what I need but I have the annoying $+t$ on the end. Can I get rid of this somehow? I haven't at all used the continuity, boundedness or finite variation so i think I've done something wrong. What is the ""well known result"" the hint suggests I use? EDIT My textbook states: I do not understand how any of the equalities follow or hold? Is it that because the process is of finite variation we can just. use normal calculus? What is the formula being used here please","['martingales', 'brownian-motion', 'probability-theory', 'probability']"
4414971,How to find the derived subgroup of a given group,"Given a group $(G, \cdot)$ is there a way to find its derived subgroup other than calculating it by hand element by element? For instance, if a group is abelian you know that its derived subgroup is $\{1_G \}$ . This is one simple example but is there a similar trick to use to make it easier to calculate $G'$ ? Thanks in advance","['derived-subgroup', 'group-theory', 'abstract-algebra']"
4414994,How can I find the curves whose curvature is $k(s)=\frac{1}{as+b}$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question I'm trying to find the curves whose curvature is $k(s)=\frac{1}{as+b}$ . I guess that they are Logarithmic Spirals. Parametric form of Logarithmic spirals: $x(t)=ae^{(bt)}\cos(t)$ and $y(t)=ae^{(bt)}\sin(t).$ I'm having problems, I have tried to find de angle or to find $T(s)$ and $N(s)$ . We know for Frenet that $T'(s)=k(s)N(s)=\alpha''(s)$ $N'(s)=-k(s)T(s)$ . I don't know how to solve it.","['plane-curves', 'curves', 'curvature', 'differential-geometry']"
4415014,Use the definition of the derivative to differentiate $e^{-1/x^2}$,"Let $f(x)=e^{-1/x^2}$ , $x \not=0$ . Without using the chain rule, find $f'(x)$ . This is an easy problem using the chain rule, however, I am curious to see how one might do it with the definition of the derivative: $$
\lim_{x\to c} \frac{f(x)-f(c)}{x-c}=\frac{e^{-1/x^2}-e^{-1/c^2}}{x-c},
$$ and $$
\lim_{h\to 0} \frac{f(x+h)-f(x)}{h}=
\lim_{h\to 0} \frac{e^{-1/(x+h)^2}-e^{-1/x^2}}{h}.
$$","['limits', 'calculus', 'derivatives']"
4415120,Solving Linear Inhomogeneous System of Differential Equations,"Let $x(t),y\in \mathbb{R}^p,A\in \mathbb{R}^{p\times p}$ , I want to solve the following system \begin{align}
\frac{dx(t)}{dt}&=-A(x(t)-y)
\end{align} with known $x(0)$ . I have 'something' but I suspect that it is wrong. I first take \begin{align}
\frac{dx(t)}{dt}+Ax(t)&=Ay\\
\exp(At)\frac{dx(t)}{dt}+\exp(At)Ax(t)&=\exp(At)Ay\\
\frac{d}{dt}\exp(At)x(t)&=\exp(At)Ay\\
\exp(At)x(t)&=x(0)+\int_0^t \exp(As) ds Ay\\
\exp(At)x(t)&=x(0)+[\exp(At)-I]Ay\\
x(t)&=\exp(-At)[x(0)-Ay]+Ay
\end{align} However I have reason to believe (from looking at some other material) that the answer 'should' be \begin{align}
x(t)&=\exp(-At)[x(0)-y]+y
\end{align} Are either of these answers (either the one I derived or the one I suspect) correct? If the latter is correct, why? If neither are correct, what is the true answer?",['ordinary-differential-equations']
4415150,What is the average of ratio of the length of prime periods of $1/p$?,"For all prime $p \ge 7$ , the decimal representation of $1/p$ repeats. Let $l_p$ be the length of the period or the non-repeating part of the decimal representation of $1/p$ . It can be shown that $l_p \le p-1$ . Thus $\frac{l_p}{p-1}$ is a measure of the length of $l_p$ relative to the magnitude of $p$ . Definition : We define the relative magnitude of the length of the period of a prime $p$ as $\frac{l_p}{p-1}$ . Question 1 : What is the limiting value of $$
c_n = \frac{1}{n}\sum_{k = 1}^n\frac{l_{p_n}}{p_n-1} 
$$ Question 2 : Is it true that $$
\lim_{n \to \infty}\frac{1}{n}\sum_{k = 1}^n\frac{l_{p_n}}{p_n}  
= \lim_{n \to \infty}\frac{\sum_{k = 1}^n l_{p_n}}{\sum_{k = 1}^n p_n}
= \prod_{p} \Big (1 - \frac{p}{p^3-1}\Big ) \approx 0.57596
$$ The plot of $c_n$ vs. $n$ for $n \le 2460000$ and for this value of, $c_n \approx 0.5763$ . Source code p = 7
s1 = s2 = s3 = k = 0
step = target = 10^4

while True:
    k = k + 1
    d = divisors(p-1)
    l = len(d)
    i = 1
    while i < l:
        e = d[i]
        if (10^e)%p == 1:
            s1 = s1 + 1/e.n()
            s2 = s2 + (e/(p-1)).n()
            s3 = s3 + e
            break
        i = i + 1
    
    if k >= target:
        print(k, s1, s2/k, s3)
        target = target + step
    p = next_prime(p)","['elementary-number-theory', 'number-theory', 'limits', 'convergence-divergence', 'prime-numbers']"
4415194,Motivation for blowing-up,"Let $X =\operatorname{Spec}(A)$ and $Z \to X$ is a closed subscheme given by the ideal $I$ . The blowup along $I$ is defined to be $\operatorname{Proj}(A \oplus I \oplus I^2 \oplus\cdots)$ which seems a bit unmotivated to me. I can follow proofs in Hartshorne, but I do not see how one can come up with such a definition. Is there any intuitive approach to it?","['algebraic-geometry', 'projective-geometry', 'commutative-algebra']"
