question_id,title,body,tags
4641683,Diffrentiability in Multivariate calculus [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . This post was edited and submitted for review last year and failed to reopen the post: Original close reason(s) were not resolved Improve this question Let, $f(x, y, z) =x^3+y^3+z^3$ $L$ be a linear map from $\mathbb R^3$ to $\mathbb R$ Satisfying $$\displaystyle\lim_{(x, y, z) \to (0, 0,0)} \frac{f(1+x, 1+y, 1+z) -f(1, 1,1) -L(x, y, z) }{\sqrt{x^2+y^2+z^2}}=0$$ Then find the value of $L(1, 2,4)$ I am unable to understand how to approach. Pls help IIT JAM 2023",['multivariable-calculus']
4641684,"Find all values of $x$ such that $\sin(2x) = \sin(x)$ and $x \in [0, 2\pi]$","The answers are $0, \frac{\pi}{3}, \pi, \frac{5\pi}{3}$ and $2\pi$ .  I found $\frac{\pi}{3}$ and $\frac{5\pi}{3}$ algebraically, I overlooked $0$ and $2\pi$ , but understood once I looked at the answer, but I'm missing how I could have found $\pi$ .  Here's what I did. I noticed that $\sin(2x) = 2\sin(x)\cos(x)$ , so we can multiply both sides by $\frac{1}{\sin(x)}$ and we eventually get $\cos(x) = \frac12$ , which set me looking for the solutions of this last equation, but the $\cos(\pi) = -1$ which is not $\frac12$ , so I would have concluded that $\pi$ does not solve the original problem. I must be losing information with this algebra, but I can't spot what step is wrong.  I believe I'm making a mistake of the sort that we do with square roots.  For instance, if $x^2 = 4$ , I could incorrectly conclude that $x = 2$ and overlook that $x = -2$ also works. So my question is more than how to solve this particular problem.  My question is what should I look out for when I replace one trigonometric equation for another.  (If I'm on the right track here, that is.)  Thank you. Reference . This is problem 8 in Stewart's Calculus 6th edition, tests of algebra D.  (The verification tests so we know what we'll need in reading the book.)","['algebra-precalculus', 'trigonometry']"
4641688,Show that the Maclaurin expansion of exact ODE general solution has the same form as the power series solution,"So, given the ODE $$
y''+2y'+y=0
$$ I have found a power series solution, coefficient recurrence relation and the general solution in terms of elementary functions: $$
\begin{aligned} 
a_{n+2} &= -\frac{2a_{n+1}}{n+2} - \frac{a_n}{(n+1)(n+2)}, \: \: n=0,1,2,... \\
\\
y(x) &= \sum_{n=0}^{\infty}a_nx^n\\
&= a_0(1 - \frac{1}{2}x^2 + \frac{1}{3}x^3 - \frac{1}{8}x^4 +...)+ a_1(x - x^2 + \frac{1}{2}x^3 - \frac{1}{6}x^4 + ...)\\ \\
&= C_1e^{-x} + C_2xe^{-x}\\ \\
&= C_1\sum_{n=0}^{\infty}\frac{(-x)^n}{n!} + C_2x\sum_{n=0}^{\infty}\frac{(-x)^n}{n!}
\end{aligned}
$$ Now, I want to confirm my power series solution by showing that the Maclaurin expansion of the exact general solution has the same form as the power series, and find relations for $C_1$ and $C_2$ in terms of $a_0$ and $a_1$ . I.e. find A, B, C, D such that $$
\begin{align}
C_1 &= Aa_0 + Ba_1\\
C_2 &= Ca_0 + Da_1
\end{align}
$$ I'm really struggling with this.. Equating coefficients feels like it's leading me nowhere and I don't know what else to try. Edit: my original ODE power series solution $$
\begin{align}
y = \sum_{n = 0}^{\infty}a_nx^n, \:\: y' = \sum_{n = 0}^{\infty}na_nx^{n-1}, \:\: y'' = \sum_{n = 0}^{\infty}(n-1)na_nx^{n-2}
\end{align}
$$ So the ODE becomes $$
\begin{align}
&\sum_{n = 0}^{\infty}(n-1)na_nx^{n-2} + 2\sum_{n = 0}^{\infty}na_nx^{n-1} + \sum_{n = 0}^{\infty}a_nx^n = 0 \\
\implies 
& \sum_{n = 2}^{\infty}(n-1)na_nx^{n-2} + 2\sum_{n = 1}^{\infty}na_nx^{n-1} + \sum_{n = 0}^{\infty}a_nx^n = 0\\
\implies
& \sum_{i=0}^{\infty}(i+1)(i+2)a_{i+2}x^{i} + 2\sum_{k=0}^{\infty}(k+1)a_{k+1}x^{k} + \sum_{n = 0}^{\infty}a_nx^n = 0\\
\implies
& \sum_{n=0}^{\infty}[(n+1)(n+2)a_{n+2}+(n+1)2a_{n+1} + a_n]x^n = 0\\
\end{align}
$$ Comparing Coefficients leads to the recurrence relation: $$
\begin{align}
(n+1)(n+2)a_{n+2}+(n+1)2a_{n+1} + a_n &= 0\\
\implies
a_{n+2} &= -\frac{2a_{n+1}}{n+2} - \frac{a_n}{(n+1)(n+2)}, \: \: n=0,1,2,...
\end{align}
$$ Which gives the solution: $$
y = a_0(1 - \frac{1}{2}x^2 + \frac{1}{3}x^3 - \frac{1}{8}x^4 +...)+ a_1(x - x^2 + \frac{1}{2}x^3 - \frac{1}{6}x^4 + ...)
$$ Apparently this is incorrect but I am really struggling to see a mistake after re-doing this","['power-series', 'recurrence-relations', 'taylor-expansion', 'ordinary-differential-equations']"
4641708,Does $\frac{d x}{dy}$ indicate that $x=f(y)$?,"Does the expression $\dfrac{d x}{dy}$ indicate that $x=f(y)$ ? Can we compute it even if $y=g(x)$ ? For example, if $y=g(x)=x^2,$ is $\dfrac{d x}{dy}$ meaningful even though $y=g(x),$ rather than $x=f(y)$ ?","['calculus', 'functions', 'derivatives']"
4641735,Multivariable chain rule to solve a third order derivative,"I came across this problem and I don't know how I should go about solving it: ""Express $\frac{\partial^3f}{\partial x^2\partial y}f(4x^2 + y, x + 1)$ in terms of partial derivatives of the function f"" I'm having a hard time grasping how these multivariable partial derivatives work. Earlier I had $f(x, y) = g(u(x,y), v(x,y))$ For which i had to express $\frac{\partial ^2f}{\partial x\partial y}$ In terms of partial derivatives of g, u and v and I got the following result: $\frac{\partial ^2g}{\partial x\partial u} \frac{\partial ^2u}{\partial x\partial y} + \frac{\partial ^2g}{\partial x\partial v}\frac{\partial ^2v}{\partial x\partial y}$ But I'm very much doubting that this is correct, considering that I dont really understand this topic yet and my Calculus book doesn't seem to help me understand it.","['partial-derivative', 'multivariable-calculus', 'derivatives', 'chain-rule']"
4641776,Maximize $(1-a)(1-c)+(1-b)(1-d)$ over $a^2+b^2=c^2+d^2=1$. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Let $a,b,c,d$ be real numbers such that $a^2+b^2=c^2+d^2=1$ . Find the maximum value of $(1-a)(1-c)+(1-b)(1-d)$ . I tried substituting $a=\sin x, b =\cos x, c = \sin y, d=\cos y$ , then expanded $(1-a)(1-c)+(1-b)(1-d)$ . However this just leads to an ugly expression, and I'm not sure how to proceed","['algebra-precalculus', 'inequality']"
4641838,Is there is a something like function of functions?,"I've seen an answer in which the user defined a function called $D$ where this function takes and input $f$ and gives output $f'$ like an example : if $f(x)=x^2$ then $D(f)=f'=2x$ but is there is a something really like function of functions well acorrding to the function $D$ it takes a function as an input an gives another function as an output but the definition of function that i've studied is that a function defined by it's graph where a function $f=$$\{ (a,f(a)) \}$ which is a set of ordered pairs acorrding to this defintion then there isn't something called function of functions because we can't define a specific graph to it like if we defined another function like $I$ where it takes input a function and gives it's integral out like \begin{gather} I(2x) =x^2 \end{gather} we can't define a graph to this function correct me if I said something wrong","['calculus', 'functions']"
4641851,"Does there exist an infinite sequence of complex numbers $\{a_i\}$, all of whose powers sum to zero?","Does there exist an infinite sequence $a_1, a_2, \dots \in \mathbb{C}$ such that for all integers $k \ge 1$ , we have $$\sum_{i = 1}^{\infty} a_i^k = 0?$$ The statement is true if the $a's$ are absolutely convergent, but this question is about if we relax absolute convergence and only have conditional. If we instead take a finite sequence and sufficiently many powers are zero, then indeed they are all zero, but this method can't work in the infinite case. [If you truncate the series at $a_1\dots a_n$ and only have $n$ equations, there is no guarantee the RHS will be small, and even if there was, there is no guarantee that the coefficients $e_i$ of the polynomial will be small, and even if there was, a polynomial with small coefficients can still have large roots that do not approach zero (eg. $x^n - 1/2^n$ has roots $x = 1/2$ .)] In the style of this excellent video, you could ask if the set of vectors $v_2 = (a_2^1, a_2^2, \dots), v_3 = (a_3^1, a_3^2, \dots)$ , each of which is in $l_2$ , can be linearly combined to produce $v_1 = (-a_1^1, -a_1^2, \dots)$ . Per the video the answer is indeed yes! - but this does not guarantee that the linear combination will be of equal weights $v_1 = 1\cdot v_2 + 1\cdot v_3 + \dots$ . [Actually, an equal weight sum of vectors in $l_2$ is not necessarily even in $l_2$ itself - eg., if $v_i$ is the vector of all zeros except for position $i$ which is 1, then $v$ is a vector of all 1s, which is not in $l_2$ .] To try to use complex analysis, it's pretty clear that for any analytic (on the unit disc?) function $f(z) = \sum_{j = 1}^{\infty} c_j z^j$ for which $f(0) = 0$ , then $\sum_i^{\infty} f(a_i) = 0$ . Or, if you want to remove the $f(0) = 0$ condition, for all analytic functions $g$ , we have $\sum_{i=0}^{\infty}a_i g(a_i) = 0$ . This works for ANY g! Surely at this point it should be obvious that the $a_i$ s must be zero, but alas I can't see it. The same would hold if $a_i$ was replaced with $\overline{a_i}$ , so that $\sum_{i=0}^{\infty}\overline{a_i} g(\overline{a_i}) = 0$ . But I don't think this gets us anywhere, since we already know that $a_i$ converges to zero, so we can't use some trick to show that for some $s$ , then $g(s) = 0$ for all $s$ , contradiction, so all $a_i$ are zero. And we have a fixed set of $a_i$ , so we can't move them around to such an $s$ and obtain that $g(s) = 0$ . And we have to use analytic functions - the Unit Disc Stone Weierstrass theorem says that $f(z)$ can be approximated by a polynomial $p(z, \overline{z})$ , a polynomial in two variables $z, \overline{z}$ . This makes things hard - because we don't have any information on the magnitudes of $a_i$ other than that (WLOG) they are less than 1. Actually, if the $a_i$ s were real numbers (and we eschew the trivial inequality to finish our proof immediately, haha), the  usual Stone Weierstrass would almost give a slick proof, but not quite. If we use the same polynomial $p$ to approximate an arbitrary continuous function $f$ , so that $|p(x)-f(x)| \le \epsilon$ for all $x$ , we'd have the same absolute error bound but an infinite number of terms, so the infinite sum $0 = \sum_{i=0}^{\infty} p(a_i)$ would not be close to $\sum_{i=0}^{\infty} f(a_i)$ as is true in the finite case. If we ignore this issue anyway and assume that $\sum_{i=0}^{\infty} f(a_i)$ is small, then we can construct a neighborhood $[-\epsilon, \epsilon]$ around zero which contains all but a finite nonzero number of the $a_i$ at the beginning of the sequence; then we can construct a continuous function which is zero on this neighborhood but equal to one everywhere else, and we'd obtain a contradiction (the sum would be strictly positive and more than one, vs close to 0), showing that no such interval exists, and thus all the $a_i$ have to be equal, and then it follows that $a_i = 0$ for all $i$ . I wonder if there is a nonzero solution where the $a_i$ are only conditionally convergent but not absolutely. On the other hand, from the vectors idea above, I think a nonzero solution is impossible because we have a uniform summation of the powers of $a_i$ , which would prevent the resulting sum from lying in $l_2$ like $v_1 = (-a_1^1, -a_1^2, \dots)$ does.","['complex-analysis', 'hilbert-spaces', 'functional-analysis', 'sequences-and-series']"
4641882,Complex Derivative: $\frac{d}{dz} (z+\overline{z} -z^3+z\overline{z})$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question How can I compute the following complex derivative $$\frac{d}{dz} (z+\overline{z} -z^3+z\overline{z})$$ I know that $\frac{d}{dz} (z\overline{z})=z$ but not sure about the above. Original problem: $\displaystyle \frac{\partial}{\partial \bar{z}}(z+\bar{z}-z^3+z\bar{z})$ . $\displaystyle \frac{\partial}{\partial \bar{z}}((z+\bar{z}+16z^8-\bar{z})(-z^4+z^7))$","['complex-analysis', 'derivatives', 'partial-derivative', 'complex-numbers']"
4641896,Derivative of Inverse Differential in Proof of Inverse Function Theorem.,"On p. $194$ of Mathematical Analysis by Andrew Browder he defines a map $\psi: \mathbb{R}^n \rightarrow \mathbb{R}^n$ by $$\psi(\mathbf{y})=d\mathbf{f}^{-1}_{\mathbf{p}}(\mathbf{y}-\mathbf{q})$$ as a tool to prove the inverse function theorem. He then claims that $$\psi'(\mathbf{q})=[\mathbf{f}'(\mathbf{p})]^{-1},$$ where $\mathbf{f}(\mathbf{p})=\mathbf{q}$ . Why is this the case? Isn't $d\mathbf{f}^{-1}_{\mathbf{p}}$ a linear map, and therefore its derivative should just be itself? But where is the $\mathbf{q}$ on the right hand side of the equation? Many thanks.","['derivatives', 'inverse-function-theorem', 'real-analysis']"
4641934,Can a cross-product of two $C_1$ gradient fields fail to be a curl?,"Let $u,v : U \subseteq \mathbb{R}^{3} \to \mathbb{R}$ be a pair of functions of class $C^2$ defined on an open set $U$ . Is it possible to always find a field $F$ such that $\nabla u \times \nabla v = \text{curl} (F)$ ? My original intuition was that it wasn't possible to always do this and that the function $G: \mathbb{R}^{3} \setminus \{\vec{0}\} \to \mathbb{R}^{3}$ defined by $$G(x,y,z) := \left(\frac{x}{\left(x^2+y^2+z^2\right)^{3/2}},\frac{y}{\left(x^2+y^2+z^2\right)^{3/2}},\frac{z}{\left(x^2+y^2+z^2 \right)^{3/2}},\right)$$ would somehow help prove this fact.","['multivariable-calculus', 'differential-geometry']"
4641947,Definition of a finite vector-valued Radon measure: isn't this condition vacuous?,"In Diestel & Uhl's ""Vector measures"" one finds the following definition 1 on page 1. Definition 1 (Countably additive vector measure). Let $X$ be a Banach space an $\mathcal F$ a $\sigma$ -algebra on a set $\Omega$ . A function $F \colon \mathcal F \to X$ is a countably additive vector measure if $F\left(\bigcup_{n = 1}^{\infty} E_n\right) = \sum_{n = 1}^{\infty} F(E_n)$ in the norm topology of $X$ for all pairwise disjoint sequences $(E_n)_{n \in \mathbb N} \subset \mathcal F$ with $\bigcup_{n = 1}^{\infty} E_n \in \mathcal F$ . In Yann Brenier & Dimitry Vorotnikov's ""On optimal transport of matrix-valued measures"" , they define $\mathbb P^+$ to be ""the set of $\mathcal P^+$ -valued Radon measures $P$ on $\mathbb R^d$ with finite $\text{tr}$ $(\text{d}P(\mathbb R^d))$ "", where $\mathcal P^+$ is the subspace of  real $d \times d$ symmetric positive semidefinite matrices "". The notation $dP$ is explained in the answer to this question .
They don't define what ""Radon"" means in the context of vector valued measures, but I assume that it means some kind regularity (with respect to the Loewner order on the symmetric matrices ?). My Question If $P \in \mathbb P^+$ , doesn't that mean that $P$ maps from $\Sigma$ into $\mathcal P^+$ , where $\Sigma$ is some $\sigma$ -algebra on $\mathbb R^d$ ?
Because $\text{tr}(A) < \infty$ for all $A \in \mathcal P^+$ and $P(E) \in \mathcal P^+$ for every $E \in \Sigma$ and wouldn't we have in particular $\text{tr}(P(\mathbb R^d)) < \infty$ ? In particular if $d = 1$ , would the one-dimensional Lebesgue measure not belong to $\mathbb P^+$ ? Question 2. Is there some way to find out from the paper what they mean by Radon?
I only found out that in subsection 2.2. of the paper Matrix Measures and Finite Rank Perturbations of Self-adjoint Operators by Constanze Liaw and Sergei Treil , Radon measures (on $\mathbb R$ ) are countably additive set function defined on bounded Borel subsets of $\mathbb R$ , which are bounded on bounded Borel subsets. I have a similar issue with a similar definition in Léonard Monsaingeon & Dmitry Vorotnikov's ""The Schrödinger problem on the non-commutative
Fisher-Rao space"" : they define a TV norm of a $H$ -valued Radon measure on p. 14 (where $H$ are the Hermitian $d \times d$ matrices) and then $\mathbb H$ as the set of $H$ -valued Radon measures with finite TV norm . But a norm on a set should always be finite, so why isn't this condition vacuous as well?","['measure-theory', 'positive-semidefinite', 'borel-measures', 'vector-measure', 'symmetric-matrices']"
4641963,$L^2(\mathbb{R})$ is separable,"I would like to prove that $L^2(\mathbb{R})$ is separable. Using the hint provided in Reed & Simon's book, I was able to show that: the set of simple functions on $[a,b]$ is dense in $L^2([a,b])$ the set of step functions on $[a,b]$ is dense in the set of simple functions on $[a,b]$ . Next, I would like to show that $C([a,b])$ is dense in the set of step functions on $[a,b]$ , so that I can conclude that $C([a,b])$ is dense in $L^2([a,b])$ . Then, using this fact, I would like to show that $L^2(\mathbb{R})$ is separable. This is where I am having some trouble. To prove the denseness of $C([a,b])$ in the set of step functions on $[a,b]$ , note that by definition a step function is defined on a disjoint union of open intervals. Consider two consecutive open intervals, $(a_l, a_r)$ and $(b_l, b_r)$ . We can construct a continuous function that agrees with the simple function on $(a_l, a_r-\epsilon/2) \cup(b_l+\epsilon/2, b_r)$ , and construct a linear function connecting the points at $a_r-\epsilon/2$ and $b_l+\epsilon/2$ . Since the support of this line can be made arbitrarily small we will obtain the result. Based on the above, to prove that $L^2(\mathbb{R})$ is separable it will suffice to prove that $C([a,b])$ has a countable dense subset. One approach I thought of was to use polynomials with rational coefficients. However, this would invoke the Stone-Weierstrass theorem, which does not appear until later in the book so I'm trying to avoid it. I will also have to extend the domain of $C([a,b])$ to $C(\mathbb{R})$ , but I think this can be achieved by taking the countable union $$\bigcup_{n=-\infty}^\infty [n, n+1].$$ Assuming my ideas are correct, I am have some trouble making these arguments rigorous. EDIT: Upon thinking about it further and taking a look at a few other books, I came up with the following. Let $f$ be a step function on $[a,b]$ and $\epsilon > 0$ . By Lusin's theorem there exists a continuous function $g$ that agrees with $f$ except on a set of measure $\epsilon$ . Then $$\|f - g\|_{L_2} = \int_{[a,b]}|f-g|^2d\mu = \int_{\{x \in [a,b]: f(x) \neq g(x)\}} |f-g|^2 d\mu \\\leq \epsilon \sup_{x \in [a,b]} |f|^2 \rightarrow 0$$ I am unsure of the above since Lusin's theorem only holds for functions with compact support, but what if our step function do not have compact support? This might be a problem when extending our domain from $[a,b]$ to all of $\mathbb{R}$ . Also the above doesn't use anything about step functions, so couldn't it have been applied to any $f \in L^2([a,b])$ ? However, if this is correct than I have shown that $C([a,b])$ is dense in $L^2([a,b])$ . By the Stone-Weierstrass theorem, the polynomials are also dense in $L^2([a,b])$ . Countability follows by considering polynomials with rational coefficients (since $\mathbb{Q}$ is dense in $\mathbb{R}$ ). Hence $L^2([a,b])$ is separable. The only thing remaining is to extent this result to $L^2(\mathbb{R})$ . Notice that since $$ \int_{\mathbb{R}} (\cdots) d\mu = \lim_{n\rightarrow \infty} \int_{[-n,n]} (\cdots) d\mu $$ all our previous results hold for $L^2(\mathbb{R})$ and so we are done. Does this last step need to be justified any further?","['measure-theory', 'lp-spaces', 'functional-analysis', 'real-analysis']"
4641994,Injection from binary strings with $i$ bits to $i+1$ bits,"I want to find an injection $F$ from binary strings length $n$ with $i$ bits turned on to $i+1$ bits turned on, with the condition that if $F(S)=S'$ , then $S'$ can be obtained from $S$ by simply turning one additional bit on. Naturally, it is guaranteed that $i<n/2$ , otherwise such an injection cannot exist because there will be more strings with $i$ bits turned on than $i+1$ . Trying some lexicographic ordering injection doesn't work because the maximal/minimal strings will always be mapped to by more than $1$ item.","['matching-theory', 'functions', 'combinatorics', 'bit-strings']"
4642004,Closed form expression for $\psi_{e^{\pi}}^{(3)}(1-i)$,"Let $\psi_q(z)$ be the q-DiGamma function defined for a real variable $\Re(z)>0$ as $$\psi_q(z)=\frac{1}{\Gamma_q(z)}\frac{\partial}{\partial z} (\Gamma_q(z))$$ where $\Gamma_q(z)$ is the q-Gamma function defined as $$\Gamma_q(z)=(1-q)^{1-z}\prod_{n=0}^{\infty}\frac{1-q^{n+1}}{1-q^{n+z}}$$ Question I am looking for a closed form for $$\psi_{e^{\pi}}^{(3)}(1-i)$$ where $i=\sqrt{-1}$ Here is a beautiful answer for calculating $$\psi_{e^{\pi}}^{(3)}(1)$$ Wolfram Alpha gives the expansion at $x=\infty$ : $$\psi_x^{(3)}(1)=\ln^4(x)\left(x^{-1}+9x^{-2}+\dots\right)$$ and these match Oeis A $001158$ with divisor $\sigma_v(n)$ and various theta functions after plugging the sum back in here . Use $\vartheta_v(0,x)=\vartheta_v(x)$ : $$\psi_x^{(3)}(1)=\ln^4(x)\sum_{n=1}^\infty\frac{\sigma_3(n)}{x^n}=\frac{\ln^4(x)}{480}\left(\vartheta_2\left(\frac1{\sqrt x}\right)^8+ \vartheta_3\left(\frac1{\sqrt x}\right)^8+ \vartheta_4\left(\frac1{\sqrt x}\right)^8-2\right)$$ Therefore: $$\psi_{e^\pi}^{(3)}(1)=\frac{\pi^4}{480}\left(\vartheta_2^8\left(e^{-\frac\pi2}\right)+ \vartheta_3^8\left(e^{-\frac\pi2}\right)+ \vartheta_4^8\left(e^{-\frac\pi2}\right)-2\right)$$ Clicking “more digits” here shows a smaller error each time implying the result is true. Now use Dedekind $\eta(z)$ identities for $\vartheta_v\left(e^{-\frac\pi2}\right)$ when $v=2$ , $v=3$ , and $v=4$ $$\psi_{e^\pi}^{(3)}(1)= \frac{\pi^4}{480}\left(\left(2\frac{\eta^2(i)}{\eta\left(\frac i2\right)}\right)^8+\left(\frac{\eta^5\left(\frac i2\right)}{\eta^2\left(i\right)\eta^2\left(\frac i4\right)}\right)^8+\left(\frac{\eta^2\left(\frac i4\right)}{\eta\left(\frac i2\right)}\right)^8-2\right)$$ Using special values in terms of $\Gamma\left(\frac14\right)$ : $\eta\left(\frac i4\right)=2\eta(4i)=\frac{\sqrt[4]{\sqrt2-1} \Gamma\left(\frac14\right)}{2^\frac{13}{16}\pi^\frac34},\eta\left(\frac i2\right)=\frac{\Gamma\left(\frac14\right)}{2^\frac 78\pi^\frac34},\eta(i)=\frac{\Gamma\left(\frac14\right)}{2\pi^\frac34}$ Finally, substitute and have a form in terms of $\Gamma\left(\frac14\right)$ which has no elementary closed form . Therefore: $$\boxed{\psi_{e^\pi}^{(3)}(1)=\frac{11\Gamma\left(\frac14\right)^8}{5120\pi^2}-\frac{\pi^4}{240}}$$ shown here If anyone could please solve this question by hand or mathematica or sage math. I would be highly indebted to you all.","['mathematica', 'number-theory', 'polygamma', 'digamma-function', 'complex-analysis']"
4642025,"Given a sequence $\left(\tfrac{x}{p}\right),\left(\tfrac{x+1}{p}\right),\dots,$ how hard it is to find an $x$ that fits?","Let $p$ be some 1000-bit prime number. Given a sequence $1$ 's and $-1$ 's $$\left(\tfrac{x}{p}\right),\left(\tfrac{x+1}{p}\right),\dots,\left(\tfrac{x+80}{p}\right)$$ how hard it is to find an $x \in \mathbb{Z}_p$ if $p$ is prime? Here $\left(\tfrac{x}{p}\right)$ denotes Legendre symbol, which can be defined as $\left(\tfrac{x}{p}\right) := x^{(p-1)/2} \bmod p$ .","['number-theory', 'quadratic-residues']"
4642027,$n$th derivative of $\sin(\cos(x))$,"$$f_n(x)=\frac{d^n}{dx^n}\sin(\cos(x))$$ Prove there are an infinite number of $n$ such that the global maximum of $f_n(x)$ is an integer. I looked this up online and it seems like this is related to exponential generating functions, but I don't know enough about them. How does this work?","['calculus', 'derivatives']"
4642062,"Does this series ""satisfy"" the M-test? [duplicate]","This question already has answers here : Does $\sum_{n=1}^\infty \frac{\sqrt[n]e-1}{n}$ converge? [duplicate] (3 answers) Closed last year . I'm currently working through a homework problem, but I've been stumped on this one problem for almost a day now. I don't want the problem to be worked out, rather I just want some idea of where to start/if my thought process has been incorrect. The problem is to figure out whether or not the following series satisfies the M-test and converges uniformly on the interval $[0,1]$ . $$\sum_{k=1}^\infty \frac{e^{x/k}-1}{k}$$ I initially tried to see if $$\sum_{k=1}^\infty \frac{e^{1/k}-1}{k}$$ converges, since $$\frac{e^{x/k}-1}{k} \le \frac{e^{1/k}-1}{k}$$ when $x \in [0,1]$ . However, after trying most of the convergence tests (Integral, comparison, ratio, root, etc) I wasn't able to conclude if this bound converges or not. However, I've also been unable to show if $\sum_{k=1}^\infty \frac{e^{x/k}-1}{k}$ diverges, which would mean that it would fail the M-test. Does anybody have any suggestions as to where to start with this problem?","['convergence-divergence', 'sequences-and-series', 'uniform-convergence', 'real-analysis']"
4642075,Is Modus Tollens just an implied Modus Ponens on the Contrapositive?,"In propositional logic, you must explicitly state each transformation even when they are obvious, like resolving double negatives. Insufficient 1 P -> Q   Hypothesis
2 --P      Hypothesis
∴ Q        Modus Ponens 1,2 # <- this implicitly relies on --P == P Sufficient 1 P -> Q  Hypothesis
2 --P     Hypothesis
3 P       Double Negation 2 # <- explicitly resolve the double negation before using it
∴ Q       Modus Ponens 1,3 However, it seems like we might be allowing an implicit step in the case of Modus Tollens. Modus Tollens can be seen as simply Modus Ponens applied on the contrapositive. Standard Modus Tollens 1 P -> Q   Hypothesis
2 -Q       Hypothesis
∴ -P       Modus Tollens 1,2 But is this not implicitly relying on the fact that P -> Q == -Q -> -P in the same way that the double negative example implicitly relied on the fact that --P == P ? We can express the argument purely in terms of Modus Ponens by explicitly stating the contrapositive. 1 P -> Q   Hypothesis
2 -Q -> -P Contrapositive 1
3 -Q       Hypothesis
∴ -P       Modus Ponens 2,3 Is this more explicit? Is Modus Tollens simply a derivation of Modus Ponens and Contrapositive, or do they merely happen to correspond? (identity-vs-value equality distinction). Does Modus Ponens have a more fundamental ontological status, or are they equally derivable in terms of each other?","['propositional-calculus', 'logic', 'discrete-mathematics']"
4642085,Polynomial Long Division for the sequence of a geometric series?,"Why does polynomial long division only work for power series if the leading term of the denominator is smaller than the leading term of the numerator? For instances I see how long division can work for $$ \frac{1}{1-3z}$$ However, Polynomial long divisions will not work for something such as $$ (\frac{1}{2-z})$$ When trying long division for $$ \frac{1}{2-z}$$ , the first couple of terms I got were $$ \frac{1}{2}+\frac{z}{2}+\frac{z^2}{4}+\frac{z^3}{8}... $$ Although by factoring $$ \frac{1}{2-z}$$ to $$ \frac{1}{2}(\frac{1}{1-\frac{z}2})$$ I can replace the z in the standard geometric series with $$ \frac{z}{2}$$ and then multiply each term by 1/2, I see that the correct terms are $$ \frac{1}{2} +\frac{z}{4} + \frac{z^2}{8} + \frac{z^3}{16}... $$ Why does polynomial long division not work in cases where the leading term in the denominator is larger then the leading term in the numerator?","['calculus', 'combinatorics', 'polynomials', 'generating-functions', 'power-series']"
4642095,"If an abelian group contains at most $n$ elements of order divisible by $n$, is it cyclic?","I usually see this question as ""if $G$ has at most $n$ solutions to $x^n=1$ , then $G$ is cyclic"". But here, we have that if $G$ has at most $n$ elements of orders divisible by $n$ , then $G$ is cyclic. I am wondering if the following proof works.
3
Let $G$ be such a group with order $|G| = p_1 ^{\alpha_1} \cdots p_k
^{\alpha_k}$ where each $p_i$ is a distinct prime. Every non-identity
element of any $P_i \in \text{Syl}_{p_i}(G)$ has order divisible by $p_i$ by Lagrange's theorem, but by hypothesis, there are at most $p_i$ such elements, so $|P_i| \le p_i$ . Since $P_i$ is not trivial, this
means $P_i \cong C_{p_i}$ . Furthermore, $P_i$ is normal because $G$ is
abelian. Because this holds for all $i$ , we see that $\alpha_i = 1$ and $$
  G \cong C_{p_1} \times \cdots \times C_{p_k} \cong C_{p_1 p_2 \cdots
    p_k} \cong C_{|G|}.
  $$","['cyclic-groups', 'group-theory', 'abstract-algebra', 'solution-verification', 'abelian-groups']"
4642098,Parametric curve resembling a bean.,"I am looking for a parametric closed curve that roughly resembles a bean. I am looking for something with an explicit parametrization of the form $C(t) = (X(t), Y(t))$ I tried searching online but ""parametric bean"" is not yielding much of use.","['plane-curves', 'parametric', 'geometry', 'parametrization']"
4642139,Question from MIT integration Bee 2023 final: Evaluate $\int^1_0 (\sum^\infty_{n=0}\frac{\left\lfloor 2^nx\right\rfloor}{3^n})^2{\rm d}x$,"I am trying to evaluate the last question from MIT integration Bee 2023 Final. $$\int^1_0 \left (\sum^\infty_{n=0}\frac{\left\lfloor 2^nx\right\rfloor}{3^n} \right )^2{\rm d}x$$ My approach is to divide $(0,1)$ into $1/2^n$ intervals and write the general term of the $y$ -value. E.g. For $x \in (k/2^n, (k+1)/2^n)$ , $$f(x)=\left (\sum^{n-1}_{k=0}\frac{\left\lfloor k/2^k\right\rfloor}{3^{n-k}}\right)^2$$ I know that the final integral is just summing up the areas of all the infinite rectangles but I can't solve it. Please help. Thank you.
(The final answer of this question is $27/32$ . Candidates were allowed to solve it within 4 minutes.)",['integration']
4642157,"What is the value of $L'(1,\chi)$ where $\chi$ is the non-principal Dirichlet character modulo 4?","I was trying to compute the following sum: $$\sum_{n\le x}{\frac{r_2(n)}{n}}$$ where $r_2(n)=\vert\{(a,b)\in\mathbb{Z}^2:a^2+b^2=n\}\vert$ . Using Abel's summation formula with $a_n=r_2(n)$ , $\varphi(t)=\frac{1}{t}$ , rememebering that $R_2(x)=\sum_{n\le x}{r_2(n)}=\pi x+O(\sqrt{x})$ , we get: $$\sum_{n\le x}{\frac{r_2(n)}{n}}=\frac{R_2(x)}{x}+\int_{1}^{x}\frac{R_2(t)}{t^2}dt=$$ $$=\pi+O(x^{-1/2})+\int_{1}^{x}\left(\frac{\pi}{t}+\frac{R_2(t)-\pi t}{t^2}\right)dt=$$ $$=\pi\log(x)+\pi+O(x^{-1/2})+\int_{1}^{+\infty}\frac{R_2(t)-\pi t}{t^2}dt-\int_{x}^{+\infty}{\frac{R_2(t)-\pi t}{t^2}dt}=$$ $$=\pi\log(x)+C+O(x^{-1/2})$$ where we also used the fact that the improper integral of $\frac{R_2(t)-\pi t}{t^2}$ converges since the integrand is $O(x^{-3/2})$ , and the same estimate gives us an errore term for the integral starting from $x$ . Thus, this method provides us with a formula involving some unknown constant $C$ , which in our calculation is given by $\pi$ and the improper integral. However, we can also use the explicit formula for $r_2(n)$ to try and get the exact value of $C$ . It is well know that, if $\chi$ is the non-principal Dirichlet character modulo 4, then $r_2(n)=4\sum_{d\vert n}\chi(d)$ , so we can say that: $$\sum_{n\le x}{\frac{r_2(n)}{n}}=4\sum_{n\le x}\sum_{d\vert n}{\frac{\chi(d)}{n}}$$ If $\frac{n}{d}=m$ , we can exchange the sums and reindex them in terms of $d$ and $m$ as it follows (Dirichlet's hyperbola method): $$=4\sum_{d\le x}\sum_{m\le x/d}{\frac{\chi(d)}{dm}}=4\sum_{d\le x}{\frac{\chi(d)}{d}\sum_{m\le x/d}{\frac{1}{m}}}$$ To be more careful, I decided to break the sum in two parts, when $d\le x^{1/2}$ and when $x^{1/2}<d\le x$ , such that in the first case we can estimate the second sum with the formula for $\sum_{k\le t}{\frac{1}{k}}=\log(t)+\gamma+O(\frac{1}{t})$ because $x/d\ge x^{1/2}$ so we can get a bound for the error. In the second case, we can estimate the sum in other ways to show that it is infinitesimal when $x\rightarrow +\infty$ . Precisely: $$\sum_{d\le x^{1/2}}{\frac{\chi(d)}{d}\sum_{m\le x/d}{\frac{1}{m}}}=\sum_{d\le x^{1/2}}{\frac{\chi(d)}{d}\left(\log(x)-\log(d)+\gamma+O(d/x)\right)}=$$ $$=\log(x)\sum_{d\le x^{1/2}}{\frac{\chi(d)}{d}}-\sum_{d\le x^{1/2}}{\frac{\chi(d)\log(d)}{d}}+\gamma\sum_{d\le x^{1/2}}{\frac{\chi(d)}{d}}+O\left(\frac{1}{x}\sum_{d\le x^{1/2}}{1}\right)$$ We now want to use the fact that $\sum_{d>t}{\frac{\chi(d)}{d}}=O(\frac{1}{t})$ and $\sum_{d>t}{\frac{\chi(d)\log(d)}{d}}=O\left(\frac{\log(t)}{t}\right)$ , together with the convergence of $L(1,\chi)$ and $L'(1,\chi)$ to get: $$\log(x)\left(L(1,\chi)+O(x^{-1/2})\right)+L'(1,\chi)+O\left(\frac{\log(x)}{x^{1/2}}\right)+\gamma\left(L(1,\chi)+O(x^{-1/2})\right)+O\left(\frac{1}{x^{1/2}}\right)=$$ $$=\frac{\pi}{4}\log(x)+\frac{\gamma\pi}{4}+L'(1,\chi)+O\left(\frac{\log(x)}{x^{1/2}}\right)$$ Using the well know fact that $L(1,\chi)=\frac{\pi}{4}$ . For the other sum, notice that when $d$ is even $\chi(d)=0$ , so we only care about odd terms. If we set $d=2k+1$ , we get: $$\sum_{x^{1/2}<d\le x}{\frac{\chi(d)}{d}\sum_{m\le x/d}{\frac{1}{m}}}=\sum_{\frac{x^{1/2}-1}{2}<k\le \frac{x-1}{2}}{\frac{\chi(2k+1)}{2k+1}\sum_{m\le x/(2k+1)}{\frac{1}{m}}}=\sum_{\frac{x^{1/2}-1}{2}<k\le \frac{x-1}{2}}{\frac{(-1)^k}{2k+1}\sum_{m\le x/(2k+1)}{\frac{1}{m}}}$$ If we let $a_k=\frac{1}{2k+1}\sum_{m\le x/(2k+1)}{\frac{1}{m}}$ , then we have $a_k\ge 0$ for all $k$ and the sum of $(-1)^ka_k$ , an alternating sum with $a_k$ strictly decreasing. Thus, we can bound it with the absolute value of the smallest term: $$\sum_{\frac{x^{1/2}-1}{2}<k\le\frac{x-1}{2}}{(-1)^ka_k}=O\left(\frac{1}{x^{1/2}}\sum_{m\le x^{1/2}}{\frac{1}{m}}\right)=O\left(\frac{\log(x)}{x^{1/2}}\right)$$ Combining the result gotten so far, we can say that: $$\sum_{n\le x}{\frac{r_2(n)}{n}}=\pi\log(x)+\pi\gamma+4L'(1,\chi)+O\left(\frac{\log(x)}{x^{1/2}}\right)$$ It now follows that $C=\pi\gamma+4L'(1,\chi)$ , giving us also the following result: $$\int_{1}^{+\infty}{\frac{R_2(t)-\pi t}{t^2}dt}=\pi(\gamma-1)+4L'(1,\chi)$$ Thus the only thing left to compute would be $L'(1,\chi)$ . Does it converge to a nice, well known value?","['analytic-number-theory', 'number-theory', 'dirichlet-character', 'dirichlet-series']"
4642196,"Convergence in probability, To which $P$ are they referring to?","I was reading about convergence in probability and read this: We say that $(X_n)_{n=1}^{\infty}$ converges in probability to $X$ , if
for every $\epsilon>0$ : $$ \lim_{n->\infty} P(|X_n-X|>\epsilon)=0 $$ I'm a little bit confused, to what probability function $P$ are they referring to? The one for the random variable $X$ or the one for $X_n$ Since each random variable can have its own probability function...","['convergence-divergence', 'probability-theory', 'probability']"
4642210,Prove that the limit of $\sqrt{x} e^{\sin(\pi/x)}$ as $x$ approaches $0$ from the right,"I don't know what a correct answer is.  Graphing the function with a computer doesn't make it too clear, but it seems to say that it approaches zero from the right.  There's a good chance this problem might be expecting me to use the Squeeze Theorem, but perhaps it might be the Cauchy-Schwartz inequality.  I'm not sure. An attempt . I first look at what happens to $\pi/x$ as $x$ approaches 0 from the right.  I conclude it goes to infinity.  Then I consider what happens to $\sin(\pi/x)$ and I conclude it oscillates around $-1$ and $1$ , that is, $$-1 \leq \sin(\pi/x) \leq 1.$$ Then I raise $e$ to both inequalities and I get $$e^{-1} \leq \sin(\pi/x) \leq e.$$ Now I multiply both inequalities by $\sqrt{x}$ , getting $$\frac{\sqrt{x}}{e} \leq \sqrt{x} e^{\sin(\pi/x)} \leq e \sqrt{x}$$ and by the Squeeze Theorem I would be able to say that it goes to zero. Related question .  What happens if we let $x$ approach $0$ from the left? It seems that the function would also approach zero, but perhaps my book is avoiding the question of what happens when we consider values of the function outside of the domain --- by only asking what happens when the function is not undefined. Another possible solution . I believe the Cauchy-Schwartz inequality could also be of a solution here, but, even if it is, I wonder what an experienced eye would say the book is expecting me to do.  (I have not yet used the Cauchy-Schwartz inequality in this section of the book yet.) Reference .  This exercise is number $38$ in section $2.3$ of Stewart's Calculus 6th edition translated to the Portuguese language.  The original 6th edition seems to ask us to prove $\sqrt{x} [1 + \sin^2(2\pi/x)] = 0$ as $x$ approaches zero from the right.  This problem from the original version seems to be concerned with the Squeeze Theorem and not the Cauchy-Schwartz inequality.","['limits', 'calculus']"
4642220,"How many random real numbers are needed for sum to exceed $1$, if each number is uniformly distributed from $0$ to constant $\times$ previous number?","Let $u_1$ be a random real number uniformly distributed between $0$ and $1$ . Let $u_k$ be a random real number uniformly distributed between $0$ and $a\times{u_{k-1}}$ , for $k>1$ and fixed positive real number $a$ . What is the expected minimum value of $n$ such that $\sum\limits_{k=1}^n u_k>1$ , in terms of $a$ ? (In other words, on average how many $u$ 's are needed for their sum to exceed $1$ ?) I did simulations using Excel. For small values of $a$ (e.g. $4$ ), sometimes the sequence of $u$ 's gets ""stuck"" around very low values, so that many $u$ 's are required for their sum to exceed $1$ . I suspect the expectation is infinity for low values of $a$ , and possibly for all values of $a$ . This is a variation of a simpler question: On average, how many uniformly random real number between $0$ and $1$ are needed for their sum to exceed $1$ ? The answer to that question is $e$ . I tried to apply similar methods to my question, to no avail.","['expected-value', 'random-variables', 'probability', 'sequences-and-series']"
4642239,How to solve $2^{\sin^2(x)}=\cos(x)$,"$$2^{\sin^2(x)}=\cos(x)$$ How can I find the solution? This is what I did : $$\ln(2^{\sin^2(x)})=\ln(\cos(x))$$ $$\sin^2(x)\ln(2)=\ln(\cos(x))$$ $$(1-\cos^2(x))\ln(2)=\ln(\cos(x))$$ $$\cos^2(x)\ln(2)+\ln(\cos(x))-\ln(2)=0$$ I put : $t=\cos(x)$ $$t^2\ln(2)+\ln(t)-\ln(2)=0$$ If I want to take out the $t$ from the $\ln$ , I will have more complicated equation with $\exp$ function.
I don't know if what I did helps to get the result. Can you help me please?","['trigonometry', 'exponential-function']"
4642245,Evaluate the Integral $\int_{0}^{\infty}\ln\left(1+\pi\frac{2\cosh\left(t\right)+\pi}{t^{2}+\cosh\left(t\right)^{2}}\right)dt$,"That's the essence of the problem: can the (in my opinion rather challenging) integral be solved or simplified: $\int_{0}^{\infty}\ln\left(1+\pi\frac{2\cosh\left(t\right)+\pi}{t^{2}+\cosh\left(t\right)^{2}}\right)dt$ It's got everything logs, cosh, t, and more. I arrived to this expression while looking at the transcendental equation $cos(x)=x$ , hence the similar-looking transcendental equation in the denominator. This was the product of integration by parts so backtracking wasn't helpful, I tried Feynman's trick with a few attempts but nothing simplified well enough. I would be very grateful for any insights, perhaps I have missed a neat solution during my work on this problem.","['integration', 'indefinite-integrals', 'improper-integrals']"
4642311,Using sine rule to prove triangle congruence,"The following problem looks like it should be easy, but I don't know how to prove it rigorously. All I know is the sine rule should be applied somewhere. Let $ABC$ be triangle with angles $\alpha$ , $\beta$ and $\gamma$ and corresponding sides $a,b,c.$ Suppose that $\beta'$ and $\gamma'$ are angles such that $\alpha + \beta' + \gamma' = \pi$ and $\dfrac{b}{\sin{\beta'}} = \dfrac{c}{\sin{\gamma'}}$ . Prove that $\beta' = \beta$ and $\gamma' = \gamma$ . I tried doing a proof by contradiction. Assume without loss of generality that $\beta' > \beta$ . Then $\gamma' < \gamma$ . Now because of the sine rule: $\frac{b}{\sin{\beta}} = \frac{c}{\sin{\gamma}}$ And the statement of the problem tells us that $\frac{b}{\sin{\beta'}} = \frac{c}{\sin{\gamma'}}$ . Thus, $\frac{\sin{\beta}}{\sin{\gamma}} = \frac{\sin{\beta'}}{\sin{\gamma'}}$ . Now since $\beta' > \beta$ , then $\sin{\beta'} > \sin{\beta}$ , and since $\gamma' < \gamma$ , then $\sin{\gamma'} < \sin{\gamma}$ . But if this is true, then $\frac{\sin{\beta}}{\sin{\gamma}} < \frac{\sin{\beta'}}{\sin{\gamma'}}$ , which is a contradiction with what we said before. Therefore, $\beta = \beta'$ and thus $\gamma = \gamma'$ . But the problem with this proof is that $\sin{\beta'}$ is not necessarily greater than $\sin{\beta}$ , if $\beta'$ is greater than $\pi/2$ for example.","['congruences-geometry', 'geometry', 'triangles', 'trigonometry', 'algebra-precalculus']"
4642327,Geometric intuition for expressing $\arcsin(x)$ as an integral?,"For $|x| \leq 1$ we have that: $$\arcsin(x)=\int_0^ x\frac1{\sqrt{1-z^2}}~\mathrm dz$$ Okay... So we are integrating the reciprocal of a function that would parameterize a half of the unit circle, and $\arcsin(x)$ is the inverse of the restriction of the $\sin$ function to a half of the unit circle. If the tangent line to the graph of $f(x)$ at $x=a$ is equal to $m$ then the tangent line to the graph of $f^{-1}(x)$ at $x=f^{-1}(f(a))$ is $\frac{1}{m}$ . I can't help but feel that there is some geometric insight happening here that would make this $\arcsin(x)$ identity a whole lot more transparent...","['integration', 'trigonometry', 'soft-question', 'geometry']"
4642337,Can't find my error in integrating $\sin(1-2x)\cos(1+2x)$,"My goal is to integrate $$I = \int \sin(1-2x)\cos(1+2x)\mathrm dx.$$ First, I let $u = -x$ and $\mathrm du=-\mathrm dx,$ and note that $$I = -\int \sin(1+2u) \cos(1-2u)\mathrm du.$$ Next, I try to integrate $I = \int \sin(1-2x)\cos(1+2x)\mathrm dx$ by parts using $u = \sin(1-2x)$ and $\mathrm dv=\cos(1+2x)\mathrm dx,$ which gives $\mathrm du = -2\cos(1-2x)$ and $v=\frac{\sin(1+2x)}{2}$ . So, $$I = \frac{\sin(1-2x)\sin(1+2x)}{2} - \int\frac{\sin(1+2x)}{2} (-2\cos(1-2x)) \mathrm dx\\= \frac{\sin(1-2x)\sin(1+2x)}{2} + \int\sin(1+2x)\cos(1-2x)\mathrm dx.$$ But $\int\sin(1+2x)\cos(1-2x)\mathrm dx = -I,$ so $$I= \frac{\sin(1-2x)\sin(1+2x)}{2} - I,$$ and so $$I = \frac{\sin(1-2x)\sin(1+2x)}{4} + C.$$ However, symbolab and and wolframalpha give different answers that don't seem to just differ by a constant. I'm having a hard time finding my mistake.","['integration', 'indefinite-integrals', 'calculus', 'reduction-formula']"
4642357,"Subset of non-decreasing functions in $L^1[0,1]$ is compact?","I'm interested in the subset of functions \begin{equation}
A := \{f:[0,1]\rightarrow [0,1] | \text{$f$ is non-decreasing}\} \subset L^1[0,1].
\end{equation} Do we know if $A$ is compact? Since $L^1[0,1]$ is a Banach space, I tried to use the result in one of answers in Examples of compact sets that are infinite dimensional and not bounded , namely: $A$ is compact iff, (1) $A$ is closed and bounded. (2) For each $\varepsilon > 0$ there exists a finite dimensional subset $F \subset L^1[0,1]$ such that $d(f, F) < \varepsilon$ for all $f \in A$ . I think (2) should hold since any non-decreasing functions are Riemann integrable and so I can approximate any $f \in A$ by points in a space of rectangular functions (essentially Riemann sum approximation of the Riemann integral) to any $\varepsilon$ accuracy. For (1) I think it is clear that $A$ is bounded (in a unit ball). But do we know if $A$ is closed? In other words, if we have a sequence $\{f_i\} \in A$ such that $f_i\rightarrow_{L^1} f$ , do we know if $f \in A$ ? I think I can see this if $L^1$ convergence on $A$ implies point-wise convergence, but I'm also not sure if this is true.","['functional-analysis', 'analysis', 'real-analysis']"
4642365,Prove that $\det\begin{pmatrix}A&B\\-B&A\end{pmatrix}$ is a sum of squares of polynomials,"As discussed for example in this question , given any pair of real squared matrices $A,B$ we have the identity $$|\det(A+iB)|^2 = \det\begin{pmatrix}A&B\\ -B&A\end{pmatrix}.$$ In particular, this means that $\det\begin{pmatrix}A&B\\ -B&A\end{pmatrix}$ must be writable as the sum of squares of two polynomials in the elements of $A,B$ .
Namely, it equals the sum of the squares of real and imaginary parts of $\det(A+iB)$ , which are polynomials in the elements of $A$ and $B$ . While this is clear from the above identity using complex numbers, is there a direct way to see that this is true reasoning only on the structure of this matrix, without passing through complex numbers?
On the face of it, it looks like there should be a reasoning similar to what is done to show that the determinant of skew-symmetric matrices can be written as the square of the Pfaffian , but I'm not sure how to make this into a precise argument. This also relates to Calculating the determinant gives $(a^2+b^2+c^2+d^2)^2$? and Prove that $\left|\begin{smallmatrix}a&-b&-c&-d\\b&a&-d&c\\c&d&a&-b\\d&-c&b&a\end{smallmatrix}\right|=(a^2+b^2+c^2+d^2)^2$ , where the structure of the matrices considered in those questions is such that $\det(A+iB)$ is either purely real or purely imaginary.
In fact, using the expressions in those posts as starting point and some guess work, it seems we have, at least when $A,B$ are $2\times2$ , the decomposition $$\det\begin{pmatrix}A&B\\-B&A\end{pmatrix}
= (\det(A)-\det(B))^2 + (a_{11} b_{22} - a_{12} b_{21} + a_{22}b_{11} - a_{21}b_{12})^2.$$ The second term can also probably be cast in a more neat form.","['determinant', 'matrices', 'linear-algebra', 'pfaffian', 'block-matrices']"
4642371,A single-color path in a two-color triangulated square,"Take any triangulation of a square (= a partition into triangles such that any two triangles are either disjoint or intersect in a common face).
Suppose that there are no triangulation vertices on the boundary of the square, except the four corners. Color each vertex either red or blue, with the following constraints: Two opposite corners are red and two opposite corners are blue; In every triangle, at least one corner is red and one corner is blue. Here is an example: In all examples I checked, there is either a red path between the two red corners, or a blue path between the two blue corners. In the example above there is a blue path $D_1-D_4-D_6-D_2$ ; if we change e.g. $D_4$ to red, then there is a red path $C_1-C_3-C_4-D_4-C_8-C_2$ . Does there always exist either a red path between the red corners or a blue path between the blue corners? NOTE: without the condition that there are no triangulation vertices on the boundary, the claim is false: consider the inner square with corners $C_7,D_7,C_8,D_8$ .","['triangulation', 'combinatorics', 'triangles', 'coloring']"
4642429,Solve $\left(3x^2y-xy\right)dx+\left(2x^3y^2+x^3y^4\right)dy=0$,"$\left(3x^2y-xy\right)dx+\left(2x^3y^2+x^3y^4\right)dy=0$ I'm trying to solve this first-order differential equation. I know it's not an exact equation so I'm trying to use the method taught in class to solve it. I get stuck trying to do the integrating factor. Below is what I have: $\frac{\partial \:}{\partial \:x}\left(M\right)=\frac{\partial }{\partial x}\left(3x^2y-xy\right) = 6xy-y$ $\frac{\partial \:}{\partial y}\left(N\right)=\frac{\partial }{\partial y}\left(2x^3y^2+x^3y^4\right)dy = 4x^3y+4x^3y^3$ And since $\frac{\partial \:}{\partial \:y}\left(N\right)\neq\frac{\partial }{\partial \:x}\left(M\right)$ , have that it is not exact. So we apply the formula to get an integrating factor: $\xi =\frac{\left(\:\frac{\partial \:}{\partial \:y}-\frac{\partial }{\partial x}\right)}{N}$ to get a function $\xi(x)$ , or the formula $\xi =\frac{\left(\:\frac{\partial \:}{\partial \:y}-\frac{\partial }{\partial x}\right)}{-M}\:$ to get a function $\xi(y)$ . We use $\xi$ to get an integrating factor $\mu(x)=e^{\int \:\xi(x) dx}$ or $\mu(y)=e^{\int \:\xi(x) dy}$ . Now, when I apply either one of the formulas for $\xi$ , I always get a result dependent on both $x$ and $y$ , so I'm unable to get the integrating factor. Is there supposed to be a simpler way to solve this? I'm using this method because it's what was taught in class, but is there another simple way to solve this that I'm not seeing?","['multivariable-calculus', 'homogeneous-equation', 'ordinary-differential-equations', 'partial-differential-equations']"
4642446,Nigerian Math Olympiad Question,"How many ways can a child who is given the task of laying n bricks lay them out they can do so under the conditions The laying of bricks is constrained to a single vertical plane The bricks must be either placed adjacent to the previously placed brick or directly above it The child constructs the initial layer from left to right No brick can be floating All cases with 6 blocks My attempt: I just thought of this as a case of partitioning n , if I find the number of ways to partition n , I know the structures they make since after partitioning I just arrange the numbers in descending order and that would be it. However I haven't come across any such formula that states the number of partitions a number n can have, I think it may be that my solution is wrong somehow since this was asked in the Nigeran math Olympiad a few years ago (saw it on AOPS) and the answers are almost always a complete formula in these kinds of tests, instead of statements. So is my answer right or wrong?","['contest-math', 'discrete-mathematics']"
4642471,"Issue with a ""proof"" for Maschke's Theorem","I came up with a ""proof"" for the Maschke's Theorem in the representation theory of finite groups that seems to make sense. But it doesn't use the fact that the group being represented is finite. So I suspect there must be something wrong with it. Can someone please help me find where it goes wrong? In the following, the underlying field is assumed to have characteristic $0$ , and all the vector spaces involved are assumed to be finite dimensional. The following form of the theorem is taken from Fulton&Harris' Representation Theory - A First Course (Proposition 1.5). Maschke's Theorem. If $W$ is a subrepresentation of a representation $V$ of a finite group $G$ , then there is a complementary invariant subspace $W'$ of $V$ , so that $V=W\oplus W'$ . Proof. Firstly, since the action of every $g\in G$ is invertible, if a subspace $V'\subset V$ is $g$ -invariant, then $g.V'=V'$ without loss of dimensions. Hence $V'$ is also $g^{-1}$ -invariant and $g^{-1}.V'=V'$ . Suppose for now $W\subset V$ is an arbitrary subspace, not necessarily invariant under the actions of any $g\in G$ . Let $\pi: V\to W$ be the corresponding linear projection, which induces a direct sum decomposition $V=W\oplus \text{Ker}(\pi)$ . Define $p_{g,\pi}:=g^{-1}\pi g$ . Then it is easy to verify $p_{g,\pi}^2=g^{-1}\pi gg^{-1}\pi g=p_{g,\pi}$ and so it is also a linear projection. This would induce an alternative direct sum  decomposition $V= \text{ran}(p_{g,\pi})\oplus \text{Ker}(p_{g,\pi})$ . We further have $g. \text{ran}(p_{g,\pi})\subset W$ . In fact, for every $x\in \text{ran}(p_{g,\pi})$ , we have $x=p_{g,\pi}(x)=g^{-1}\pi gx$ and so $gx=\pi gx\in W$ . In case $W$ is $g$ -invariant, by first paragraph, $W$ is also $g^{-1}$ -invariant. Therefore, since $\pi|_W$ is the identity map, for every $w\in W$ , we have $\text{ran}(p_{g,\pi})\ni p_{g,\pi}(w)=g^{-1}\pi gw=g^{-1}gw=w$ . Hence $W\subset \text{ran}(p_{g,\pi})$ . Then by both previous paragraphs, $g. \text{ran}(p_{g,\pi})\subset W\subset \text{ran}(p_{g,\pi})$ and so $ W= \text{ran}(p_{g,\pi})$ , since $g$ is invertible. This gives $\text{Ker}(p_{g,\pi})=\text{Ker}(\pi)$ and $p_{g,\pi}=\pi$ . That is, the two direct sum decompositions of $V$ become identical. Then, we can check that $g$ and $g^{-1}$ both preserve the two identical kernels. In fact, for every $y\in \text{Ker}(p_{g,\pi})=\text{Ker}(\pi)$ , we have $p_{g,\pi}(g^{-1}y)=g^{-1}\pi gg^{-1}y=g^{-1}\pi y=0$ , and so $g^{-1}y\in \text{Ker}(p_{g,\pi})=\text{Ker}(\pi)$ . By the invertibility of $g^{-1}$ , the two identical kernels are both $g$ -invariant and $g^{-1}$ -invariant. Lastly, if $W$ is $G$ -invariant, then every $g\in G$ induces the same projection $p_{g,\pi}=\pi$ as reasoned above, and the subspace $W':=\text{Ker}(\pi)$ is then also $G$ -invariant and complementary to $W$ such that $V=W\oplus W'$ . ${\rm \square}$ As you can see, nowhere in the ""proof"" above is the fact used that $G$ is a finite group, and that's horrifying to me... UPDATE: According to Eric's answer, the issue is that, once we have $W= \text{ran}(p_{g,\pi})$ , we cannot conclude the two projections are identical because their kernels might still be different. However, we would still have direct sum decomposition $V=W\oplus \text{Ker}(p_{g,\pi})$ . This would be true for every $g\in G$ . But the corresponding kernel would change as $g$ runs through $G$ . This is where the ""averaging technique"" comes into play, and how the finiteness of $G$ plays a key role.","['finite-groups', 'representation-theory', 'fake-proofs', 'linear-algebra', 'group-theory']"
4642505,Why using compound interest formula gives (potentially) wrong answer in this instance,"I was doing some catch up exercise on Khan academy and was given this seemingly simple looking problem Find the compound interest and the total amount after 4 years and 6 months if the interest is compounded annually. Principal = £100,000 Rate of interest 10% percent per annum Calculate total amount and compound interest I calculated it using compound interest formula: $$ 100000(1.1)^{4.5} = 153556.10346 $$ But this turned out to be the wrong answer, the correct answer, as presented by Khan academy was this: khan academy answer 153730.5 I can also arrive at this value by sort of using the compound interest formula for first 4 years, but then calculating interest for the last 6 months manually (0.1/2): $$ 100000(1.1)^{4} = 146410 $$ $$ 146410 + ( 146410 \cdot 0.05 ) = 153730.5 $$ I still feel a bit unsatisfied, and feel I am not really understanding what's going on here and why would calculating the last step manually give a different answer. Can you provide an explanation on why this formula should not apply on this case?","['algebra-precalculus', 'finance']"
4642511,The density of a pushforward probability measure: the reciprocal of the Jacobian determinant?,"$
\def\dee{\mathop{\mathrm{d}\!}}
\def\Jac#1{\mathop{\mathbf{J}_{#1}}}
$ I'm confused about how to use the change of variable formula to describe the density of a pushforward measure.  My question boils down to: which of the two formulae, (1) and (2) below, is the correct one? Let $X$ be a real-valued random variable with density $\rho_X$ wrt the Lebesgue measure and let $Y=f(X)$ be another random variable, where $f$ is a deterministic invertible transformation. What is the probability density function of $Y$ ? [Just to be clear, the connection with the push-forward is: Denote by $P_X$ the law of $X$ . Then the law of $Y$ is $P_Y = P_X \circ f^{-1} = {({P_X})}_\sharp f$ , the push-forward of $P_X$ by $f$ .] For any measurable set $E$ of outcomes for $Y$ , the probability measure of that set is $$
\begin{align}
P_Y(E) = P_X(f^{-1}(E))
&= \int_{f^{-1}(E)} \rho_X(x) \dee x\\
&= \int_{E} \rho_X(f^{-1}(y)) \left|\frac{\dee}{\dee y}f^{-1}(y)\right|\dee y
\end{align}
$$ (using the change of variables formula for integration by substitution ) so the density of $Y$ is $$
\rho_Y(y) = \rho_X(f^{-1}(y)) \left|\frac{\dee}{\dee y}f^{-1}(y)\right|
$$ The derivative in this expression generalizes to the Jacobian determinant in multivariate case (when $f$ is a diffeomorphism, I gather), giving the following formula for the density (see, e.g. this math.SE answer ) $$
\tag{1}
\rho_Y(y) = \rho_X(f^{-1}(y)) \left|\det\Jac{f^{-1}}(y)\right|
$$ However , a few sources (for example, Betancourt's notes see section 4.2, and some implementations, like here )* give a similar expression but with the reciprocal of the Jacobian determinant, as $$
\tag{2}
\rho_Y(y) = \rho_X(f^{-1}(y)) \frac1{\left|\det\Jac{f^{-1}}(y)\right|}
$$ It can't be the case that (1) and (2) both hold in general!  Is (2) just a typo, or have I misunderstood the notation they use? I know that $(\frac{\dee}{\dee x}{f}(x))^{-1}=\frac{\dee}{\dee y}{f^{-1}}(y)$ (by the inverse function theorem). This generalizes to $(\Jac{f}(x))^{-1}=\Jac{f^{-1}}(y)$ . So, I think the correct version of (2) would be ( $2^\star$ ): $$
\tag{2$^\star$}
\rho_Y(y) 
= \rho_X(f^{-1}(y)) \frac1{\left|\det\Jac{f}(x)\right|}\\
= \rho_X(x) \frac1{\left|\det\Jac{f}(x)\right|}
$$ or, in the simpler one-dimensional case, $$
\displaystyle
\rho_Y(y) = \rho_X(f^{-1}(y)) \frac1{\left|\frac{\dee}{\dee x}{f}(x)\right|} \\
= \rho_X(x) \frac1{\left|\frac{\dee}{\dee x}{f}(x)\right|}
$$ But I don't know why you would write it that way (since you want an expression where $y$ is the variable, not $x$ ), instead of the way in $(1)$ . Have I just made a silly mistake reading the notations?  Is $(2)$ somehow correct? *EDIT: I think I may be interpreting Betancourt's notation wrong.  I now think any sources for $(2)$ are either based on a misunderstanding or a typo.","['measure-theory', 'pushforward', 'jacobian', 'change-of-variable', 'density-function']"
4642527,Freiling's question,"In his paper ""Axioms of Symmetry: Throwing Darts at the Real Number Line,"" Freiling suggests two additional axioms that are formally analogous to his axiom of symmetry (which he proves to be equivalent to the negation of CH given ZFC): A-null: For any $f$ from the reals to null sets of reals (relative to the Lebesgue measure), there are $x,y$ such that $x\not\in f(y)$ and $y\not\in f(x)$ A-meagre: For any $f$ from the reals to meagre sets of reals, there are $x,y$ such that $x\not\in f(y)$ and $y\not\in f(x)$ . Freiling says (p.194) he doesn't know whether A-null and A-meagre are jointly formally inconsistent (presumably given ZFC). Does anyone know if this question has already been answered? (I've made no progress on (dis)proving it myself...)","['measure-theory', 'set-theory']"
4642575,"Using Central Limit Theorem, find the probability that the average lifetime?","The lifetime of a certain electronic component is a random variable with the expectation of 5000 hours and a standard deviation of $200$ hours. Using Central Limit Theorem, find the probability that the average lifetime of $100$ components is less than $4650$ hours? Let $X$ be a random variable which represents the lifetime if certain electronic components with mean of $500$ hours & standard deviation $200$ hours. $\mu=5000 $ hours $, \sigma= 200$ hours , n = $100$ . My main question is for the Standard Normal Distribution do I use the formula .A Standard Normal variable, usually denoted by Z, can be obtained from a non-standard
Normal( $μ, σ)$ random variable X by standardizing, that is, subtracting the mean and dividing
by the standard deviation,Using these transformations, any Normal random variable can be obtained from a Standard
Normal variable Z; therefore, we need a table of Standard Normal Distribution only Table. $$Z= \frac{X-\mu}{\sigma}\\ X = μ + σZ.$$ or do I use $$Z = \frac{x̄ -\mu}{\frac{\sigma}{\sqrt{n}}}\\ $$ . Depending on the selected formula the answer changes accordingly..... Any advise?","['statistics', 'central-limit-theorem', 'probability']"
4642590,Evaluating $\int_{-\infty}^{\infty}\frac{\ln\left(\frac{1}{2}+x+x^{2}\right)}{1+x^{2}}dx$,"(Motivation) This is an integral I made up for fun. WolframAlpha doesn't seem to come up with a closed form for it and I'm surprised there doesn't seem to be a duplicate after using Approach0, but I believe it equals $$\pi\ln\left(\frac{5}{2}\right)$$ (Question) Since there seems to be a nice-looking closed form, could there be a cooler or more elegant way of solving this other than my attempt below? If anyone is willing to look over my attempt and provide constructive criticism, I would greatly appreciate it as someone who seeks to expand his skills in complex analysis. (Attempt) Let $f(z) = \displaystyle\frac{\log(1/2+z+z^2)}{1+z^2}$ . Its poles are $z \in \left\{i,-i\right\}$ and its branch points are $\displaystyle z \in \left\{-\frac{1}{2}+\frac{i}{2}, -\frac{1}{2}-\frac{i}{2}\right\}$ . For simplicity's sake, let $\displaystyle z_a = -\frac{1}{2}+\frac{i}{2}$ and $\displaystyle z_b = -\frac{1}{2}-\frac{i}{2}$ . With these, we can rewrite $f(z)$ as $$
\begin{align}
\frac{\log\left(\frac{1}{2}+z+z^{2}\right)}{1+z^{2}} &= \frac{\log\left(\left(z-z_{a}\right)\left(z-z_{b}\right)\right)}{1+z^{2}} \\
&= \frac{1}{1+z^{2}}\left(\log\left(\left|z-z_{a}\right|\cdot\left|z-z_{b}\right|\right)+i\operatorname{arg}\left(\left(z-z_{a}\right)\left(z-z_{b}\right)\right)\right) \\
&= \frac{1}{1+z^{2}}\left(\log\left|z-z_{a}\right|+\log\left|z-z_{b}\right|+i\operatorname{arg}\left(z-z_{a}\right)+i\operatorname{arg}\left(z-z_{b}\right)\right).
\end{align}
$$ But for the equalities to hold, define $\displaystyle \operatorname{arg}(z-z_a) \in \left(\frac{3\pi}{4},\frac{11\pi}{4}\right)$ and $\displaystyle \operatorname{arg}(z-z_b) \in \left(-\frac{11\pi}{4},-\frac{3\pi}{4}\right)$ . Here is a visual of the keyhole contour. By Cauchy's Residue Theorem, we get $$2\pi i \operatorname{Res}(f(z),z=i) = \left(\int_{-R}^{R}+\int_{\Gamma}+\int_{\lambda_1}+\int_{\gamma}+\int_{\lambda_2}\right)f(z)dz.$$ (I forgot to put this in the picture, but $\gamma$ has a small radius $r$ and the gaps between the branch cut and the $\lambda$ s have a small length of $\epsilon$ ). As $R \to \infty$ and $r \to 0$ , it can be proved that $\displaystyle \int_{\Gamma}f(z)dz$ and $\displaystyle \int_{\gamma}f(z)dz$ go to $0$ . For the contour integrals over $\lambda_1$ and $\lambda_2$ , let them approach the branch cut $\Lambda$ . So $$
\begin{align}
& \lim_{R \to \infty}\lim_{\lambda_1, \lambda_2 \to \Lambda}\left(\int_{\lambda_1} + \int_{\lambda_2}\right)f(z)dz \\
=& \lim_{R \to \infty}\lim_{\epsilon \to 0}\int_{Rz_{a}}^{z_{a}}f\left(z+i\epsilon\right)d\left(z+i\epsilon\right) + \lim_{R \to \infty}\lim_{\epsilon \to 0}\int_{z_{a}}^{Rz_{a}}f\left(z-i\epsilon\right)d\left(z-i\epsilon\right) \\
=& \lim_{R \to \infty}\lim_{\epsilon \to 0}\int_{Rz_{a}}^{z_{a}}\frac{d\left(z+i\epsilon\right)}{1+\left(z+i\epsilon\right)^{2}}\left(\log\left|z+i\epsilon-z_{a}\right|+\log\left|z+i\epsilon-z_{b}\right|+i\operatorname{arg}\left(z+i\epsilon-z_{a}\right)+i\operatorname{arg}\left(z+i\epsilon-z_{b}\right)\right) \\
&+ \lim_{R \to \infty}\lim_{\epsilon \to 0}\int_{Rz_{a}}^{z_{a}}\frac{d\left(z-i\epsilon\right)}{1+\left(z-i\epsilon\right)^{2}}\left(\log\left|z-i\epsilon-z_{a}\right|+\log\left|z-i\epsilon-z_{b}\right|+i\operatorname{arg}\left(z-i\epsilon-z_{a}\right)+i\operatorname{arg}\left(z-i\epsilon-z_{b}\right)\right) \\
=& -\int_{z_a}^{i\infty}\frac{dz}{1+z^2}\left(\log\left|z-z_{a}\right|+\log\left|z-z_{b}\right|+\frac{11\pi i}{4}+i\operatorname{arg}\left(z-z_{b}\right)\right) \\
&+ \int_{z_a}^{i\infty}\frac{dz}{1+z^2}\left(\log\left|z-z_{a}\right|+\log\left|z-z_{b}\right|+\frac{3\pi i}{4}+i\operatorname{arg}\left(z-z_{b}\right)\right) \\
=& -2\pi i \int_{z_a}^{i\infty}\frac{dz}{1+z^2} \\
&= i\pi\left(\frac{\pi}{2}+\arctan\left(\frac{1}{2}\right)\right)-\frac{\pi}{2}\ln\left(5\right). \\
\end{align}
$$ Next, we will evaluate the residue at the simple pole $z=i$ like this: $$2\pi i \operatorname{Res}(f(z),z=i) = 2\pi i \lim_{z \to i}\frac{\left(z-i\right)\log\left(\frac{1}{2}+z+z^{2}\right)}{\left(z-i\right)\left(z+i\right)} = \pi\ln\left(\frac{\sqrt{5}}{2}\right)+i\pi\left(\frac{\pi}{2}+\arctan\left(\frac{1}{2}\right)\right).$$ Gathering everything together and taking the appropriate limits, we get $$\pi\ln\left(\frac{\sqrt{5}}{2}\right)+i\pi\left(\frac{\pi}{2}+\arctan\left(\frac{1}{2}\right)\right) = \int_{-\infty}^{\infty}f\left(x\right)dx + 0 + i\pi\left(\frac{\pi}{2}+\arctan\left(\frac{1}{2}\right)\right)-\frac{\pi}{2}\ln\left(5\right) + 0.$$ In conclusion, $$\int_{-\infty}^{\infty}\frac{\ln\left(\frac{1}{2}+x+x^{2}\right)}{1+x^{2}}dx = \pi\ln\left(\frac{5}{2}\right).$$","['integration', 'improper-integrals', 'complex-analysis', 'calculus', 'contour-integration']"
4642622,Can free groups on different cardinals be isomorphic in ZF?,"For the free group on a finite number of generators, $F_n$ , this is simple. It is enough to find a group that can be a quotient of one and not the other, so it is sufficient to find a group that can be generated by $n$ generators and no less. The group $\mathbb{Z}_2^n$ works. Similarly, $\mathbb{Z}_2^{(\aleph_0)}$ cannot be finitely generated, showing the countably generated free group is not isomorphic to a finitely generated free group. With the axiom of choice, for any infinite cardinal $\kappa$ , $|F_{\kappa}|=\kappa$ , so cardinality sorts out all the other infinite free groups, showing $F_{\kappa}=F_{\lambda} \iff \kappa = \lambda$ . However, this is clearly requires the axiom of choice. In ZF, this proves free groups on different well-orderable cardinals are diferent, but not all cardinals. So I would like to know: is it consistent with ZF that free groups on two different cardinals can be isomorphic? And if so, which cardinals work, and what is the proof? My thinking: The proof in ZFC doesn'tn work here, as it is possible for $\mathbb{Z}_2^{(\kappa)} \cong \mathbb{Z}_2^{(\lambda)}$ for $\kappa \ne \lambda$ . Free groups are much like direct sum vector spaces in that they don't need any choice to define them and get a generating set, so there's no obvious reason choice would be necessary for this. It doesn't seem like being given an isomorphism between free groups on different cardinals would allow you to prove anything choicy about those cardinals. But there also isn't any obvious approch to proving they're not isomorphic, and when there's no obvious proof for something simple, it seems to usually be independent. I first thought that the universal property might be enough to distinguish them, but there are other algebraic categories where free objects on different cardinals are isomophic, even for finite cardinals ( Quick proof that free objects on sets of different cardinality are not isomorphic? ). As such, this seems a no-go. It also makes it look very likely that different cardinals can give isomorphic free groups. I then thought that I could leverage the fact that the same vector space can have different sized bases: if I quoutient out the necessary relations to make it commutative and self inverse, $F_\kappa \to \mathbb{Z}_2^{(\kappa)}$ . So this natural quotient can make free groups on different cardinals turn into isomorphic groups. It would be sufficient to prove that this quotient can't make non-isomorphic free groups become isomorphic. But this quotient can clearly make non-isomorphic groups isomorphic in general, and it may well be able to do so with free groups. And I don't know how to prove ths either way. Just knowing about some of the oddities possible in cardinal arithmetic without choice doesn't seem to help either, e.g. I'm pretty sure different cardinals can generate free groups of the same cardinality, but they might not have the same group structure. It seems like the methods I have knowledge of are not suffient.","['cardinals', 'group-theory', 'free-groups', 'axiom-of-choice', 'set-theory']"
4642635,On the elliptic curve $u(u+2186^2)(u+2188^2)=v^2$ and 7th powers?,"I. Elliptic curve Given some constant integer $m$ , a solution to the elliptic curve, $$E:=u\big(u+(m^7−1)^2\big)\big(u+(m^7+1)^2\big)=v^2$$ which is not a torsion point implies a polynomial identity to, $$(a+x)^7+(-a+x)^7+(c-x)^7+(-c-x)^7\\+(b+mx)^7+(-b+mx)^7+(d-mx)^7+(-d-mx)^7=z^7$$ for arbitrary $x$ . Note that the integers $(a,b,c,d,z)$ depend on $u$ and are trivial if $u$ is one of the 5 torsion points of $E$ . But non-torsion points have been found for $\color{blue}{m=2,5}.$ II. Multi-grade For the moment, there are about 30 known primitive integer solutions to, $$(a_1+y)^k+(-a_1+y)^k+(a_2-y)^k+(-a_2-y)^k\\+(a_3+my)^k+(-a_3+my)^k+(a_4-my)^k+(-a_4-my)^k=\color{blue}0$$ simultaneously true for $k=1,3,7$ , a third of which have  either $\color{blue}{m=2,3,5}.$ Hmm. III. Connections Other than being equal sums of 7th powers, the obvious connections between the two Diophantine equations are the LHS obeys, $$x_1+x_2+x_3+x_4 =0 \\ x_5+x_6+x_7+x_8 = 0$$ thus, $$\frac{x_1+x_2}{x_5+x_6} = \frac{x_3+x_4}{x_7+x_8} = \frac1m$$ The $x_i$ are large but a third of the multi-grade solutions have either $\color{blue}{m=2,3,5}$ (a small integer) so it is a bit curious. IV. Question Does it follow then that $E$ for $m=3$ , $$E:=u\big(u+(3^7−1)^2\big)\big(u+(3^7+1)^2\big)=v^2\\ E:=u\big(u+2186^2\big)\big(u+2188^2\big)=v^2$$ also has a non-torsion point?","['number-theory', 'experimental-mathematics', 'elliptic-curves', 'diophantine-equations']"
4642636,Are there theorems in fiber bundle land and differential geometry land that make calculations in electromagnetism easier?,"Some time ago while thinking about life and such, I thought to myself does recasting electromagnetism in bundle theory make certain calculations easier? To be precise are there theorems in differential geometry and fiber bundle theory that will make some calculations in electromagnetism easier? Motivation: A few days ago I realized that a theorem in algebraic topology helped me calculate a homotopy group by calculating a homology. I am still not sure how I could have directly calculated the homotopy group but the theorem made the calculation easier, simpler and doable. This got me thinking maybe I have been ignoring modern mathematics theorems at my own peril. There are some simpler theorems in calculus that have applications in electromagnetism like Stokes's and Gauss's theorem, but I was wondering if some ideas grew out of differential geometry and fiber bundle language that made calculating things significantly easier in electromagnetism. I am thinking of things like the scalar potential, vector potential, electric field, and magnetic field. I am just wondering. I'm inclined to believe that people who go out of their way to define everything in this language are probably using some new and exciting theorems to calculate things. At this stage in my life I am marginally competent with some electromagnetism I guess some U(1) theory as well but I am probably not well versed with broader gauge theories but I am wondering again are there theorems that make calculations easier in this language? I know very very basic fiber language. In fact, probably just the definitions of base space, projection map, fiber, section etc, but I have not had the pleasure of delving deep to see the theorems and such so I don't know.","['fiber-bundles', 'electromagnetism', 'differential-geometry']"
4642652,If $M_x \neq N_y$ then is the differential not exact?,"I have to determine the validity of the following statement: Suppose you have a given differential $M(x,y)dx+N(x,y)dy$ . If $M_x\neq N_y$ then the differential is not exact. My answer is that the statement is false since $M(x,y)$ and $N(x,y)$ have to be continuous with continuous first partials, then the differential is exact if $\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}$ $M_x$ and $N_y$ are kind of irrelevant as far as I can see for determining whether the differential is exact. Am I correct? Also, I was wondering if the continuity part of the theorem plays a role at all in this answer.",['ordinary-differential-equations']
4642728,Mathematical induction proof for integers,"Use mathematical induction to prove that $n! > 4^n$ for $n \geq 9$ . My attempt: Base case: For $n=9$ , we have $9! = 362880$ and $4^9 = 262144$ Since $9! > 4^9$ , the statement is true for $n=9$ Inductive hypothesis: Assume that $k! > 4^k$ for some positive integer $k \geq 9$ Inductive step: We want to show that $(k+1)! > 4^{k+1}$ We start with $(k+1)!$ , which can be written as $(k+1) \cdot k!$ Using the inductive hypothesis, we know that $k! > 4^k$ Substituting this into the expression for $(k+1)!$ , we get $(k+1)! > (k+1) \cdot 4^k$ To complete the proof, we need to show that $(k+1) \cdot 4^k > 4^{k+1}$ Dividing both sides by $4^k$ , we get $k+1 > \frac{4^{k+1}}{4^k}$ Since $k \geq 9$ , we can plug in $k=9$ , through calculating $9+1 > \frac{4^{9+1}}{4^9}$ , and we get $10 > 4$ Since $(k+1)$ is greater than this value, we have $(k+1) \cdot 4^k > 4^{k+1}$ Therefore, we have shown that $(k+1)! > 4^{k+1}$ , which completes the inductive step By the principle of mathematical induction, we have proven that $n! > 4^n$ for all $n \geq 9$","['induction', 'solution-verification', 'discrete-mathematics']"
4642785,Giving examples to counter a false statement,"Provide a counter-example to demonstrate that the following statement is false. $\forall x \in \mathbb{R} \forall y \in \mathbb{R} (\sqrt{x^2y^2} \geq 16) \rightarrow (xy \geq 4)$ My attempt: Statement $P$ : $\forall x \in \mathbb{R} \forall y \in \mathbb{R} (\sqrt{x^2y^2} \geq 16) \rightarrow (xy \geq 4)$ For $x = 16$ and $y = -1$ : $\sqrt{x^2y^2} = \sqrt{16^2 \times (-1)^2} = \sqrt{16^2 \times 1} = 16 \geq 16$ $xy = 16 \times (-1) = -16 < 4$ Therefore, $P$ is false for $x=16$ and $y=-1$ : $P$ is not true for all values of $x$ and $y$ in the set of real numbers. This counter-example shows that the implication in statement $P$ does not hold for all possible values of $x$ and $y$ . Therefore, the statement $P$ is false. Am I correct ? Correction to implement the suggestion from the Answer below: My attempt: Statement $P$ : $\forall x \in \mathbb{R} \forall y \in \mathbb{R} (\sqrt{x^2y^2} \geq 16) \rightarrow (xy \geq 4)$ This counter-example shows that the implication in statement $P$ does not hold for all possible values of $x$ and $y$ . For $x = 16$ and $y = -1$ : $\sqrt{x^2y^2} = \sqrt{16^2 \times (-1)^2} = \sqrt{16^2 \times 1} = 16 \geq 16$ $xy = 16 \times (-1) = -16 < 4$ It is false that every real pair of $x$ and $y$ satisfies $(\sqrt{x^2y^2} \geq 16) \rightarrow (xy \geq 4)$ i.e., $\Big(∀x{\in}\mathbb R\;∀y{\in}\mathbb R \;S(x,y)\Big)$ is false. Therefore, the statement $P$ is false.","['solution-verification', 'logic', 'discrete-mathematics']"
4642786,Proof of midpoint theorem in hyperbolas/conics,"Please help me find a proof for the midpoint theorem in hyperbolas (or conics in general). All midpoints of parallel chords in a hyperbola/conic are located on a common line. Motivation: I study mathematics for teaching and we are doing some basic geometry. We covered hyperbolas on a basic level, i.e. tangents and intersections with hyperbolas. We also discussed the midpoint theorem in hyperbolas, but unfortunately I haven't found a proof that would be comprehensible to high school students. My first idea is to find two midpoints of parallel chords and show that they are located on the same line.","['conic-sections', 'geometry']"
4642809,Sum of distances from point on external bisector,"Given an angle $\angle{APB}$ and a point $P' \neq P$ on its external bisector, prove that $AP + BP < AP' + BP'$ . My first approach was to try Pythagoras, constructing points $A'$ and $B'$ orthogonal projections of $A$ and $B$ respectively onto the external bisector, and since $AA'$ and $BB'$ are perpendicular to the bisector, then triangles $AA'P'$ and $BB'P'$ are right triangles. But I got to a dead end because Pythagoras uses squares of sides and I need to prove a linear inequality. My second idea was to notice that if we define an ellipse with foci $A$ and $B$ and a point $P$ , and another ellipse with foci $A$ and $B$ and a point $P'$ , then it is sufficient to prove that the semiaxis of the latter ellipse is greater than the former, since the equation of an ellipse is $AP + BP = k$ . But I have not been able to continue from there.","['euclidean-geometry', 'algebra-precalculus', 'geometry']"
4642891,"What is the probability that $\sum_{k=1}^\infty u_k>1$, if $u_1=$ random real number in $(0,1)$, and $u_k=$ random real number in $(0,u_{k-1})$?","Let $u_1=$ i.i.d. $\text{Uniform}(0,1)$ -variable. Let $u_k=$ i.i.d. $\text{Uniform}(0,u_{k-1})$ -variable for $k>1$ . What is the probability that $\sum\limits_{k=1}^\infty u_k>1$ ? The expectation of $\sum\limits_{k=1}^\infty u_k$ is $1$ . This can be shown as follows. For $a_i=$ i.i.d. $\text{Uniform}(0,1)$ -variable, we have: $$E\left(\sum\limits_{k=1}^\infty u_k\right)=E\left(\sum\limits_{k=1}^\infty \left(\prod\limits_{i=1}^k a_i\right)\right)=\sum\limits_{k=1}^\infty \left(\prod\limits_{i=1}^k E(a_i)\right)=\sum\limits_{k=1}^\infty \left(\prod\limits_{i=1}^k \left(\frac12\right)\right)=1$$ An Excel simulation suggests that $P\left(\sum\limits_{k=1}^{50} u_k>1\right)$ and $P\left(\sum\limits_{k=1}^{100} u_k>1\right)$ are both roughly $0.44$ . Other than that, I do not know how to approach this problem. Context: This question was inspired by another question involving the sum of random real numbers.","['random-variables', 'probability', 'sequences-and-series']"
4642892,Solve the differential equation: $(x^2-y^2)dx+2xydy=0$.,Given $(x^2-y^2)dx+2xydy=0$ My solution- Divide the differential equation by $dx$ $\Rightarrow x^2-y^2+2xy\frac{dy}{dx}=0$ $\Rightarrow 2xy\frac{dy}{dx}=y^2-x^2$ Divide both sides by $2xy$ $\Rightarrow \frac{dy}{dx}=\frac{1}{2}[\frac{y}{x}-\frac{x}{y}]$ This is a homogenous differential equation . Substitute $y=vx$ $\Rightarrow \frac{dy}{dx}=v+x\frac{dv}{dx}$ $\Rightarrow v+x\frac{dv}{dx}=\frac{1}{2}[v-\frac{1}{v}]$ $\Rightarrow x\frac{dv}{dx}=-\frac{v^2+1}{2v}$ $\Rightarrow -\frac{2v}{v^2+1}dv=\frac{dx}{x}$ Integrating both sides $\Rightarrow -\log|v^2+1|=\log x+\log c$ $\Rightarrow -\log|\frac{y^2}{x^2}+1|=\log xc$ $\Rightarrow -\log|\frac{x^2+y^2}{x^2}|=\log xc$ $\Rightarrow \frac{x^2}{x^2+y^2}= xc$ $\Rightarrow x= c(x^2+y^2)$ $\Rightarrow y=\pm \sqrt{xc-x^2}$ Kindly review my solution and let me know if there are other methods of solving such problems.,"['solution-verification', 'ordinary-differential-equations']"
4642963,"The differential equation $\dot{\mathbf{x}}(t)=M(t)\mathbf{x}(t)$, conditions for a specific type of matrix $M$ to commute at different times","I have some $N$ x $N$ matrices $M_N(t)$ of the general form $$\tag{1}
M_4(t)=\left[\begin{matrix}-f_0 & f_1 & 0 & 0  \\
 0 & -f_1 & f_2 & 0  \\
0 & 0 & -f_2 & f_3  \\
0 & 0 & 0 & -f_3 
\end{matrix}\right] \qquad M_5(t)=\left[\begin{matrix}-f_0 & f_1 & 0 & 0  & 0\\
 0 & -f_1 & f_2 & 0 & 0 \\
0 & 0 & -f_2 & f_3 & 0 \\
0 & 0 & 0 & -f_3 & f_4 \\
0 & 0 & 0 &  0 &-f_4
\end{matrix}\right]
$$ where each $f_n=f_n(t)$ . From the physics of the problem, none of the $f_n$ may vanish for all $t$ , and they are all non-negative. These matrices arise in studying the differential equation $$\tag{2}
\dot{\mathbf{x}}(t)=M(t)\mathbf{x}(t)
$$ where $\mathbf{x}(t)$ is an $N$ -tuple $(x_0(t), \ x_1(t), \ \dots, x_{N-1}(t))^\top$ . I would like to study the conditions on $f_n$ under which the matrices $M$ commute at unequal times , that is $$\tag{3}
\left[ \int\limits_0^t dt' M(t'), \ M(t)  \right]\stackrel{?}{=}0
$$ where $[ \cdot , \cdot]$ is the commutator. This is important to me because under these conditions the solution to (2) is simply $\mathbf{x}(t)=\exp\left( \int\limits_0^t dt' M(t')\right)\mathbf{x}(0)$ while without condition (3) the solution is a time ordered exponential. By playing around with small $N$ cases in a CAS, I think that condition (3) may only be fulfilled only if $$\tag{4}
M(t)=g(t) M_0
$$ where $g(t)$ is a function and $M_0$ is a constant matrix. Obviously (4) is a sufficient condition for (3), but I am uncertain about its necessity. To demonstrate this is a nontrivial claim, note that if we allow any of the $f_n$ to vanish for all $t$ , then $M$ may be partly block diagonalized and (4) is not a necessary condition. Question: Is (4) a necessary condition for (3) to hold with $f_n(t)\neq 0$ , or is there another condition? Related question : given the specific form of $M$ , can the general time ordered solution $\mathbf{x}(t)=\mathcal{T}\exp\left( \int\limits_0^t dt' M(t')\right)\mathbf{x}(0)$ be simplified at all? Here $\mathcal{T}$ denotes the time ordering symbol. Background (not necessary for the question) Equation (2) arises from a master equation governing the number of electrons trapped in a quantum dot. The functions $f_n(t)$ are complicated, but the controlling factor of each is of the form $f_n(t) \approx \exp(e^{\alpha_n t})$","['ordinary-differential-equations', 'matrices', 'matrix-calculus', 'matrix-equations', 'mathematical-physics']"
4642992,Show $2$-degenerate graphs are subgraphs of $2$-degenerate plane bipartite graphs,"I am trying to prove or find a counterexample for the following claim: any planar $2$ -degenerate graph is a subgraph of a $2$ -degenerate plane bipartite graph (a planar graph whose faces are all $4$ -gons). It is known that $2$ -degenerate bipartite graphs can be obtained from $C_4$ by inserting a vertex of degree 2 (and thus two new edges at the same time). My idea was to argue that one could take any $2$ -degenerate planar graph and add edges and vertices in such manner to obtain a plane bipartite graph, but I am completely lacking the argument of how to make sure that 1) this is in fact always possible, and 2) the resulting plane graph is in fact bipartite and 2-degenerate. $2$ -degenerate graphs are graphs such that every subgraph of a $2$ -degenerate graph contains a vertex of degree $2$ .","['graph-theory', 'discrete-geometry', 'discrete-mathematics']"
4643002,Something about the group of translations of vector space.,"Let $\mathrm{V} = \mathrm{V(n, q)}$ be the vector space of dimension n over the field $\mathbb{F}_q$ . $T = \{ \sigma_v: x \mapsto x + v \mid v \in \mathrm{V} \}$ is the group of translations of $\mathrm{V}$ . $\mathrm{AGL(V)}$ is the general affine group of $\mathrm{V}$ . $S_{\mathrm{V}}$ is the symmetric group of $\mathrm{V}$ . I am interested in the set $$ H = \{ g \in S_{\mathrm{V}} \mid T^g < \mathrm{AGL(V)} \},$$ where group $T^g$ is conjugated of $T$ by $g$ . It is clear that $\mathrm{AGL(V)} \subset H$ and for all $\phi, \psi \in \mathrm{AGL(V)}, h \in H$ $$ \psi h \phi \in H.$$ But what else one can deduce about the elements of $H$ ?","['permutations', 'group-theory', 'abstract-algebra']"
4643055,Range of $f(x)=x \sqrt{1-x^2}$,"I have to find the range of $f(x)=x\sqrt{1-x^2}$ on the interval $[-1,1]$ . I have done so by setting $x=\sin\theta$ and thus finding it to be $[-0.5,0.5]$ . Let $x=\sinθ$ . Then, for $x\in[-1,1]$ we get that $θ \in [-\frac{\pi}{2}, \frac{\pi}{2}]$ . Thus, $f(x)$ becomes: $f(\theta)=\sin\theta \sqrt{1-(\sin\theta)^2}= \sin\theta |\cos\theta|$ . Since for $\theta \in [-\frac{\pi}{2}, \frac{\pi}{2}]$ we have that $cos\theta \geq 0$ : $f(θ)=\sinθ\cosθ=\frac{1}{2}\sin2θ$ . Which has maximum value $\frac{1}{2}$ when $\theta = \frac{\pi}{4}$ and minimum value $-\frac{1}{2}$ when $\theta = -\frac{π}{4}$ . When $\theta = \frac{\pi}{4}$ we have $x=\frac{\sqrt{2}}{2}$ and when $\theta = -\frac{\pi}{4}$ we have $x=-\frac{\sqrt{2}}{2}$ , which are the maximum and minimum positions respectively. So, $f(\frac{\sqrt{2}}{2})=\frac{1}{2}$ and $f(-\frac{\sqrt{2}}{2})=-\frac{1}{2}$ . Thus the range of $f(x)$ is $[-\frac{1}{2},\frac{1}{2}]$ . I know that it can be found with derivatives as well. I was wondering how can quadratic theory can be used to find the Range.","['functions', 'quadratics']"
4643100,Proving that a function $f: \mathbb{R}^2 \to \mathbb{R}^2$ is bijection,"I am trying to prove that the function $f: \mathbb{R}^2 \to \mathbb{R}^2$ defined by $f(x,y) = (x+y, x + 2y)$ is bijective. Here is my attempt. Suppose $f(m,n) = f(p,q)$ for $(m,n), (p,q) \in \mathbb{R}^2$ . Then $(p + q, p + 2q) = (m + n, m + 2n)$ , and we have $$
p + q = m + n, \qquad p + 2q = m + 2n. 
$$ But then $$
p + 2q = (p+q) + q = (m+n) + q = (m+n) + n,
$$ and cancelling $m+n$ gives $q = n$ . Substituting into the first equation, we get $m = p$ . Therefore, $(m,n) = (p,q)$ , so $f$ is injective. Now, we prove that $f$ is surjective. Let $(a,b) \in \mathbb{R}^2$ . We will find $u(a,b), v(a,b)$ such that $f(u,v) = (a,b)$ . We have: $$
f(u,v) = (u+v, u + 2v) = (a,b),
$$ so $u + v = a$ and $u + 2v = b$ . In particular, $u = a - v$ , and substituting into the second equation gives $$
b = u + 2v =  (a-v) + 2v = v + a. 
$$ So $b = v + a$ , and hence $v = b - a$ . Then $$
u = a - v = a - (b-a) = 2a - b.
$$ To check that these formulas work, we have: $$
f(2a - b, b - a) = ((2a - b) + (b-a), (2a-b) + 2(b-a)) = (a, b),
$$ so $f$ is surjective, hence bijective. How does this look?","['functions', 'solution-verification']"
4643114,"Prove that $\bigcup \{A,B \} = A \cup B$. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Prove that $\;\bigcup\big\{A,B\big\} = A \cup B$ . I am trying to work on this problem here. But I just do not know where to start.
So I know that the union of set {A,B} is {A,B}. And the union of Set A and B we also get {A,B} for any common elements. How would I show this proof wise",['discrete-mathematics']
4643148,Finding all the minima and maxima within a range [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question I'm not sure how to find all the maximas and minimas where the range is $1≤x≤18$ and the function is: $$20\sin \left(\fracπ6x-\frac {2\pi}3\right)+22$$ I already found the first derivative which is: $$\frac{10\pi}3\cos\left(\frac{\pi}{6}x-\frac{2\pi}{3}\right)=0$$ where $x$ is $7$ and using $f''(x)$ and subbing in my $x$ , I get $-5.48$ which is a maximum.","['maxima-minima', 'derivatives']"
4643164,Is my proof of $\eta(0)=\frac{1}{2}$ correct?,"Is my following proof of $\eta(0)=\frac{1}{2}$ correct? $$\eta(0)=\lim_{s\to 0^{+}}\eta(s)=\lim_{s\to 0^{+}}\sum_{n=1}^\infty \frac{(-1)^{n-1}}{n^s}$$ $$=\lim_{s\to 0^{+}}\left(\lim_{x\to 1^{-}}\sum_{n=1}^\infty \frac{(-x)^{n-1}}{n^s}\right)$$ $$\{\text{change the order of the limits}\}$$ $$=\lim_{x\to 1^{-}}\left(\lim_{s\to 0^{+}}\sum_{n=1}^\infty \frac{(-x)^{n-1}}{n^s}\right)$$ $$=\lim_{x\to 1^{-}}\left(\sum_{n=1}^\infty (-x)^{n-1}\right)$$ $$=\lim_{x\to 1^{-}}\left(\frac{1}{1+x}\right)$$ $$=\frac{1}{2}$$ I let $s\to 0^{+}$ since $\displaystyle\sum_{n=1}^\infty \frac{(-1)^{n-1}}{n^s}$ is defined for $\Re(s)>0$ and I let $x\to 1^{-}$ since $\displaystyle\sum_{n=1}^\infty x^n$ is defined for $|x|<1$ . Thank you,","['limits', 'solution-verification', 'special-functions', 'sequences-and-series']"
4643191,Help computing eigenvalues of matrix $M = (m_{ij})$ where $m_{ij} = r^{|i-j|} - r^{2(n+1) - (i+j)}$,"Let $M$ be the $n \times n$ real symmetric matrix with entries $m_{ij}$ given by $$
m_{ij} =  r^{|i-j|} - r^{2(n+1) - (i+j)}, 
$$ for some $r \in [0, 1]$ . Numerically, it can be verified that this matrix is positive definite, however I struggled to show that this is the case by hand. I am mainly interested in computing the eigenvalues of this matrix. I know that the trace of this matrix has the form $$
n - \frac{1 - r^{2(n+1)}}{1 - r^2},
$$ but so far this is really the most information I could figure out about this matrix. This matrix arises in the study of Markov chains.","['matrices', 'eigenvalues-eigenvectors']"
4643195,How to differentiate this integral with variable limits?,"Source of the question: the exercise 2.18 of Statistical Inference Book by Casella and Berger. Show that if X is a continuous random variable, then $$\min_a E|X-a|=E|X-m|$$ , where m is the median. My attempt: $$E|X-a|=\int_{-\infty}^\infty |x-a|f(x)dx=  \int_{-\infty}^a -(x-a)f(x)dx + \int_a^\infty (x-a)f(x)dx$$ . Then $$\frac{d}{da} E|X-a|=$$ I don't know how to do this differentiation. I need to set it to zero. By using herb steinberg's hint, $$E|X-a|=-\int_{-\infty}^a xf(x)dx + a \int_{-\infty}^af(x)dx+\int_a^\infty xf(x)dx-a\int_a^\infty f(x)dx$$ Then by using Henry's hint, $$\frac{d}{da} E|X-a|= -af(a)+af(a)-af(a)+af(a)$$ . But this is always zero. Somewhere must go wrong.","['integration', 'calculus', 'statistics', 'probability']"
4643214,Probability of Twisting a Phone Cord During a Call,"I invented this problem and am unable to solve it. It is not a homework problem. I make a phone call on a standard handset (with a coiled cord). I start with the
phone on my right ear. With probability p I talk long enough that I transfer the
phone to my left ear, putting a twist in the cord. If I hang up at that point
the cord remains twisted. But at probability p^2 I talk even longer and transfer
the phone back to my right ear, removing the twist. At p^3 I put the twist back,
and at p^4 I remove the twist again, and so on. If the call can be unbounded,
how do I compute the probability P(p) that a call will put a twist in the cord? Here is a diagram of the call, where the (possibly infinite) call is on the
horizontal and the possible hang-ups on the vertical. 1 means a twist, and 0
means no twist. p^1       p^2       p^3       p^4
   0---------1---------0---------1---------0...  Phone call -->
   |         |         |         |
   | 1-p^1   | 1-p^2   | 1-p^3   | 1-p^4         Hangups |
   |         |         |         |                       |
   0         1         0         1                       v

There are scenarios that result in a 1:

Q1 = p^1 * (1 - p^2)             Twist and no untwist
Q3 = p^1 * p^2 * p^3 * (1 - p^4) Twist, untwist, twist again and no untwist
Q5 = p^1 * p^2 * p^3 * p^4 * p^5 * (1 - p^6) Two aborted twists and a twist
... For any given call, there can be at most one Q. But that seems to mean the
exclusive-or of an infinite number of Qs! How can that be done? Or is that the
wrong approach? I'm looking for: How to calculate P(p)?
What is the limit of P(p) as p approaches 1.0? (if it exists)
(Graphs of P(p) and (P(p) - p) might be interesting) Update: When p is 1.0 my Q scenarios all go to 0 because of the last term. It
is impossible to avoid untwisting.",['probability']
4643253,How to calculate probability from transform function?,"In my book I found an interesting question, given the transform function (moment generating function) of random variable $X$ : $$M_X(s)=\frac{1}{2^4}\left (\left (e^s+1\right )^2\left (e^s+3\right )\right ).$$ I want to compute $\mathbb{P}(X>2)$ , which is equal to $1-\mathbb{P}(X\leq 2)$ . I know that I can use $M_X(s)$ to calculate expected value, but how to use it to calculate probability function. Plus the question doesn't say anything if $X$ is continuous or discrete...","['moment-generating-functions', 'probability-distributions', 'probability-theory', 'probability']"
4643255,Example of a function that is continuous at $c$ whose inverse is discontinuous at $f(c)$,"I'd like an example of a function $f:(a,b)\to\mathbb R$ and a point $c\in(a,b)$ such that: $f$ is invertible. $f$ is continuous at $c$ . $f^{-1}$ is discontinuous at $f(c)$ . Motivation: There is a calculus book that states the following. Let $f$ be an invertible function defined on an interval $I$ . If $f$ is differentiable at $c\in I$ and $f'(c)\neq 0$ , then $f^{-1}$ is differentiable at $f(c)$ . In the proof, the continuity of $f^{-1}$ at $f(c)$ is essential. Usually, the said essential fact is an hypothesis (if the domain is not an interval) or it is implied by the hypothesis that $f$ is continuous in a neighborhood of $c$ (if the domain is an interval). But in the said book, both hipothesis are missing and the fact is justified as follows: As $f$ is differentiable at $c$ , $f$ is continuous at $c$ . Therefore, $f^{-1}$ is continuous at $f(c)$ . I suspect continuity at $c$ does not imply continuity of the inverse at $f(c)$ due to the following facts: It seems it is not a common result in analysis books. In the usual proofs that the inverse of a continuos map (on an interval or on a compact set) is continous, in order to prove that the inverse is continuous at a given point, we need the continuity of $f$ in the whole domain. In more recent editions of the said book, the statement was modified (now, it is supposed that $f$ is differentiable in a neighborhood of $c$ , which implies what is needed). However, I do not have a counterexample.","['continuity', 'calculus', 'examples-counterexamples', 'real-analysis']"
4643276,"What is the relationship among $a, b$ and $c$ for $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-\left(ax^2+2bxy+cy^2\right)}dx dy=1$","What relationship must hold between the constants $a, b$ and $c$ to make: $$
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}{\rm e}^{-\left(\, ax^{2}\ +\ 2bxy\ +\ cy^2\,\right)}\phantom{A\,}{\rm d}x\,{\rm d}y = 1
$$ I am absolutely clueless on how to proceed with this question. I found a solution to this question on this website but the initial steps where it uses the transformation with the constraints on $\alpha, \beta, \gamma$ and $\delta$ is not clear to me. Like what was the intuition behind this transformation $?$ . I understand that the end result is somewhat similar to the initial assumptions, but how am I supposed to think of this particular transformation in an exam? I would appreciate any alternate answers/techniques to this question. Explanation(s) to the external linked solution are also welcome. Edit #1: As mentioned by fellow users in the comments, the link is hidden behind a paywall. Here's the crux of what was given as the solution: They used the transformation $$s=\alpha x+\beta y$$ $$t=\gamma x+\delta y$$ where $\left(\alpha\delta-\beta\gamma\right)^2=ac-b^2$ . Then they solved for $x$ and $y$ and proceeded with the Jacobian of the transformation and hence the given integral to obtain the required condition.","['integration', 'jacobian', 'multivariable-calculus', 'calculus', 'multiple-integral']"
4643282,Are turning points and stationary points the same?,"I'm currently doing AS maths and my Pure 1 textbook treats stationary points and turning points as the same thing. Furthermore, my teacher said that stationary and turning points are the same. However, from my understanding, a turning point is where the gradient changes sign and a stationary point is where the derivative is 0. Hence they are slightly different, in the sense that a point of inflexion should not be a turning point. I'm slightly confused. Who is right? Could your provide citations and things to read to prove your answer?","['calculus', 'graphing-functions', 'derivatives']"
4643364,Faà di Bruno's formula for multiple arguments but still with respect to one variable,"Faà di Bruno's formula given (from Wikipedia) by . The following are questions about generalizations that are similar, but not the same: Multivariate Faà di Bruno's formula Extension Faà di Bruno formula Faa di Bruno's formula generalization What I would like an expression for is $$\frac{d^n}{dx^n} f(g_1(x), \ldots, g_k(x))$$ which involves taking the $n$ th order derivative with respect to only $x$ but there are multiple arguments of $f$ .","['multivariable-calculus', 'calculus', 'combinatorics', 'derivatives', 'chain-rule']"
4643387,"Closed and exact forms, and functions $f : \mathbb{R}^2 \setminus \{ 0 \} \to S^1$","I have a simple question on the classic example of a closed form not being exact. It is well-known that the one-form $$
\omega = \frac{x \mathrm{d}y - y \mathrm{d}x}{x^2 + y^2}
$$ is closed on $\mathbb{R}^2 \setminus \{ 0 \}$ (as can be verified by direct calculation), but is not exact. As a result, integrals of $\omega$ along non-homotopic paths can yield different results. Of course, in the back of our heads, we know that $\omega$ is at least locally exact: it is the differential of $\arctan (y / x)$ . Of course, I accept that this function is not well-defined on all of $\mathbb{R}^2 \setminus \{ 0 \}$ ; for example, it is clearly not well-defined at $x = 0$ . This is closely related to the fact that $\arctan(y/x)$ has a branch cut, and cannot be continuously defined on all of $\mathbb{R}^2 \setminus \{ 0 \}$ . However, consider the following approach: instead of considering functions $f : \mathbb{R}^2 \setminus \{ 0 \} \to \mathbb{R}$ , consider instead a function $\varphi : \mathbb{R}^2 \setminus \{ 0 \} \to S^1$ , defined by $$
e^{i \varphi(x,y)} = \frac{x+iy}{\sqrt{x^2 + y^2}}
$$ Here I'm considering $S^1$ as the unit circle in $\mathbb{C}$ , for convenience. Clearly $\varphi(x,y)$ is a smooth $S^1$ -valued function on all of $\mathbb{R}^2 \setminus \{ 0 \}$ . On the other hand, whereas the original target space $\mathbb{R}$ was simply-connected, the new target space $S^1$ is not! As a result, $\varphi$ can ""wind"" by $2\pi n$ around the origin. On the other hand, it's not at all clear that $d\varphi = \omega$ in the way I want it to. Indeed, it's not clear to me if one can even define the exterior derivative on a function $\varphi : M \to S^1$ the same way as for a function $f : M \to \mathbb{R}$ . I suspect the reason that it 'seems' like it might work with $\varphi$ is because, whereas $\varphi$ is $S^1$ -valued, its derivative is $\mathbb{R}$ -valued. If I instead had an $S^2$ -valued function (i.e., a function $\psi : M \to S^2$ ), I'm not sure how I'd generalize at all. So, here is the final question: what is the mathematically precise way to describe the fact that $\omega$ has a well-defined antiderivative $\varphi(x,y)$ on all of $\mathbb{R}^2 / \{ 0 \}$ , despite not being exact, by allowing the target space of $\varphi$ to be $S^1$ instead of $\mathbb{R}$ ? EDIT: Just to be completely clear, in order to address some of the comments: I am considering here $S^1$ to be the set $\mathbb{R} / \sim$ , where $x \sim x + 2\pi n$ for all $n \in \mathbb{Z}$ . Then $\varphi : \mathbb{R}^2 \setminus \{ 0 \} \to S^1$ , and $e^{i \varphi(x,y)}$ lies on the unit circle in $\mathbb{C}$ . As a map between manifolds, $\varphi(x,y)$ is a smooth map; I'm certain that this could be verified by putting down coordinate charts, but it should be plainly obvious by noting that $\varphi$ is just the angular coordinate of the complex variable $z = x+iy$ .","['mathematical-physics', 'differential-forms', 'algebraic-topology', 'differential-geometry']"
4643408,What does disjunct mean?,"I am trying to understand the definition of a disjunctive normal form. I got this definition from this textbook : A propositional formula is in disjunctive normal form if it consists
of a disjunction  of $(1, … ,n)$ disjuncts where each disjunct consists
of a conjunction of $(1, …, m)$ atomic formulas or the negation of an
atomic formula. Example of what is and what not is a disjunctive normal form: Yes $(p∧¬q)∨(¬q∨q)$ No $p∧(p∨q)$ I do not know what a disjunct is, so I searched on Google and found that, according to Google, a disjunct is each of the terms of a disjunctive proposition. So, I got a new question from this definition of disjunct. What do the terms of a disjunctive proposition refer to? Specifically, what is the meaning of terms in this context?","['propositional-calculus', 'logic', 'discrete-mathematics', 'terminology', 'disjunctive-normal-form']"
4643418,"Find the minimum vertex cover for a Bipartite Graph, if we are given the maximum Bipartite Matching [duplicate]","This question already has an answer here : How to find a minumum vertex cover from a maximum matching in a bipartite graph? (1 answer) Closed last year . From Konig's Theorem, the size of Maximum Matching (|M|) and minimum vertex cover is the same. Now we can include both ends of the matching in the vertex cover to find a vertex cover, but its size will be 2|M|. So I considered choosing a matching edge and then checking the degree of both ends. And then include the vertex with the higher degree. Suppose the degree of both ends is the same. We can include either. I tried this with a few examples I could think of it worked. But I am not sure of the algorithm. Is this correct? Also, if not, can anyone provide any counter example.?","['graph-theory', 'bipartite-graphs', 'discrete-mathematics', 'discrete-optimization']"
4643452,Is there something wrong with this multivariable calculus proof?,"There are $2$ parts to the question: Let $f:\mathbb{R} ^{2}\rightarrow \mathbb{R}$ be differentiable in $\mathbb{R} ^{2}$ such that $\nabla f\equiv 0$ for every point $\left( x,y\right) \in \mathbb{R} ^{2}$ .
proof that f is constant. Let $f:\mathbb{R} ^{2}\rightarrow \mathbb{R}$ be a function that satisfies $\left| f\left( x_{1},y_{1}\right) -f\left( x_{2},y_{2}\right) \right| \leq \left( x_{1}-x_{2}\right) ^{2}+\left( y_{1}-y_{2}\right) ^{2}$ for every $\left( x_{1},y_{1}\right) ,\left( x_{1},y_{1}\right) \in \mathbb{R} ^{2}$ .
proof that $f$ is constant Before I go on with the proofs, I would even argue that in the first question, the word ""differentiable"" is superfluous because if we are given that $\nabla f\equiv 0$ , then it's clear that $f$ is differentiable because the partial derivatives are continuous (because they are $0$ everywhere, and what more continuous than the constant function $0$ ?), is this correct? I came up with the following proofs: Given $\nabla f\equiv 0$ this means that $f_{x}\equiv f_{y}\equiv 0$ , if we ""freeze"" $y$ to be $y_{0}$ and let $x$ vary then $f(x,y_{0})$ is constant as a result of lagrange's theorem because $f_{x}\equiv 0$ , if we did something similar on $x$ we get that $f(x_{0},y)$ is constant for every $y$ .
thus for every $\left( x,y\right) \in \mathbb{R} ^{2}$ we get that $f\left( x,y\right) =f\left( 0,y\right) =f\left( 0,0\right) $ (since it is constant if we move parallel to the axis) meaning $f$ is constant. We first prove this for the 1-dimensional case:
meaning if f satisfies $\left| f\left( x\right) -f\left( y\right) \right| \leq \left| x-y\right| ^{2}$ for every $x$ and $y$ then (if $x \neq y$ ) $$\left| \dfrac{f\left( x\right) -f\left( y\right) }{x-y}\right| \leq \left| x-y\right| $$ Therefore $$\left| f^{'}\left( x\right) \right| = \lim _{y\rightarrow x}\left| \dfrac{f\left( x\right) -f\left( y\right) }{x-y}\right| \leq \lim _{y\rightarrow x}\left| x-y\right| = 0$$ meaning $f$ is differentiable and constant. back to the $2$ dimensional case:
if we ""freeze"" $x_{1}$ to be $x_{2}$ to be $x_{0}$ then we get that $$\left| f\left( x_{0},y_{1}\right) -f\left( x_{0},y_{2}\right) \right| \leq \left( y_{1}-y_{2}\right) ^{2}$$ and from the lemma at the begging we proved we conclude that $f_{y}\equiv 0$ the same argument goes for $x$ , and we get that $f_{x}\equiv f_{y}\equiv 0$ ; this means that (because the partial derivatives are continuous because they're $0$ ) $f$ is differentiable in $2$ variables and this coupled with the conclusion from $1$ (the first question) we get that $f$ is constant. These are the proofs I had in mind, but I don't see what's wrong with them. Can someone provide me with insight?","['multivariable-calculus', 'solution-verification', 'derivatives']"
4643485,Derivation of $\sin(15^\circ)$ geometrically,"This is my attempt:- Let us consider a right $\triangle ABC$ such that angle $A$ is $15^\circ$ and $C$ is $75^\circ$ . On the line $AB$ , let us assume a point $D$ such that $\frac{BC}{BD} =\frac{ 1}{\sqrt{3}}$ (Without Loss Of Generality). So $\angle BDC$ becomes $30^\circ$ and $\angle BCD$ becomes $60^\circ$ . Then $\angle DCA$ becomes equal to $\angle BAC$ ,  that is $15^\circ$ ; so $CD = DA$ . $CD$ will be $2$ times $BC$ (angle $BDC = 30^\circ$ ; $\sin 30^\circ$ ). On adding $BD$ and $AD$ we get $$AB = BC(2+\sqrt{3})$$ $$BC^2 + AB^2 = AC^2$$ $$\therefore AC = 2BC\sqrt{2 + \sqrt{3}}$$ $$\sin(15^\circ) = \frac{BC}{AC}$$ $$\sin(15^\circ) = \frac{1}{\sqrt{2 + \sqrt{3}}}$$ Rationalising the denominator 2 times we get:- $$\sin(15^\circ) = \frac{(4-2\sqrt{3})(\sqrt{2+\sqrt{3}})}{4}$$ Further simplifying:- $$\sin(15^\circ) = \frac{(2-\sqrt{3})(\sqrt{2+\sqrt{3}})}{2}$$ $$\sin(15^\circ) = \frac{(\sqrt{2-\sqrt{3}})(\sqrt{2-\sqrt{3}}) (\sqrt{2+\sqrt{3}})}{2}$$ $$\sin(15^\circ) = \frac{\sqrt{2-\sqrt{3}}}{2}$$ Is my answer correct? And is there any other method or way to get the value of $\sin(15^\circ)$ geometrically?","['algebra-precalculus', 'solution-verification', 'trigonometry']"
4643507,Mapping subspaces of dimension $2$ to subspaces of dimension $2$,"Setting. Let $V$ be a vector space over some field $F$ with $\dim(V) \geq 3$ . A map $\phi : V \to V$ is called flat if the image of any subspace of dimension $2$ is again a subspace of dimension $2$ . Question. Is the implication $$f \text{ flat} \implies f \text{ injective}$$ always true? Motivation. In the case of a finite field $F = \mathbb F_q$ , the answer is ""yes"": Assume that $\phi(v) = \phi(w)$ for two distinct vectors $v,w\in V$ . Let $U$ be a subspace of dimension $2$ containing $v$ and $w$ . Then $\#U = q^2$ , but $\#\phi(U) \leq q^2 - 1$ , which is a contradiction. Clarifications. The map $\phi$ is not assumed to be linear! (For linear maps $\phi$ , it is not hard to see that the answer is ""yes"".) The term ""subspace"" means ""vector subspace"" (and not ""affine subspace"").","['functions', 'linear-algebra', 'geometry', 'vector-spaces']"
4643542,How does $e^{2\theta}(2\cos{\theta}-\sin{\theta})=0$ imply $\tan{\theta}=2$?,"The solution to a problem in my textbook seems to make up a trig identity out of nowhere. Here's the question: Find the points on the spiral $r=e^{2\theta}, 0\leq\theta\leq\pi$ , where the tangents are a) perpendicular to the initial line b) parallel to the initial line. Give your answers to three significant figures. The given solution for part a is: $x=r\cos{\theta}=e^{2\theta}\cos{\theta}$ $\frac{dx}{d\theta}=0\implies 0=2e^{2\theta}\cos{\theta}-e^{2\theta}\sin{\theta}$ $0=e^{2\theta}(2\cos{\theta}-\sin{\theta})$ $\implies\tan{\theta}=2$ etc. How did all that turn into a tan? What identity am I missing?","['trigonometry', 'polar-coordinates']"
4643603,Continuous function from $\mathbb{R}^2$ to $\mathbb{S}^2$,"Good afternoon, I am currently going through an introductory topology class. An exercise asked us to prove there is no homeomorphism between $\mathbb{R}^2$ and $\mathbb{S}^2$ (and the reason is no continuous bijection from $\mathbb{S}^2$ to $\mathbb{R}^2$ exists, as $\mathbb{S}^2$ is compact and $\mathbb{R}^2$ is not in the topology induced by $\mathbb{R}^3$ ). My question now is, relaxing the requirement of having a continuous inverse, are there continuous bijections from $\mathbb{R}^2$ to $\mathbb{S}^2$ ?",['general-topology']
4643616,How to add infinite sum of odd/even numbers with factorial? [duplicate],"This question already has answers here : Probability that an integer number having Poisson distribution is even (2 answers) Closed last year . Here's the question I need help:
Let X be a random variable with probability mass function: $$p_{X}(n) = \frac{e^{-1}}{n!}$$ for n = 0,1,2,..., and $p_{X}(n)$ = 0 otherwise. Compute the probability that X is even. I know that the infinite sum of $p_{X}(n)$ adds up to 1, and that the sum can be split into 2 sums where n takes form of $2k$ and $2k + 1$ . But I cannot figure out how to finish. The textbook solution is $1/2 + 1/2e^{-2}$ . Can anyone help? Thanks!","['probability-theory', 'probability', 'sequences-and-series']"
4643631,Ball color expected value,"We have a box containing b black balls and r red balls which we will take out one at a time by placing them on the table in front of us. Before each extraction we write the color of the majority of the balls not yet extracted on a sheet of paper and only in the event of a tie between black and red can we write a random color (black or red). So, once the ball has been extracted, we can establish whether or not its color corresponds to the one written on the sheet. On average, how many times does the color written on the piece of paper coincide with the color of the ball drawn? Driven by curiosity, I simulated this game 1000 times considering a box containing b=26 black balls and r=26 red balls, obtaining an expected value approximately equal to 30 . I was wondering if a closed formula could be written. Thank you! Thanks to Parcly Taxel 's answer I was able to go back to the original problem whose solution is by John Henry Steelman , Indiana University of Pennsylvania, Indiana, PA (valid for $b \ge r$ ): $$
\mathbb{E}(b,r) = b + \sum_{k=0}^{r-1}\binom{b+r}{k}/\binom{b+r}{r}
$$ from which: $$
\mathbb{E}(r,r) = r - \frac{1}{2} + \frac{2^{2r-1}}{\binom{2r}{r}};
\quad
\mathbb{E}(26,26) = \frac{3724430600965475}{123979633237026} \approx 30.040664774713895\,.
$$",['probability']
4643697,Prove that $\frac{d}{dx}\left(3x^2+4\sqrt{x}\right) = 6x+\frac{2}{\sqrt{x}}$,"Prove that $\frac{d}{dx}\left(3x^2+4\sqrt{x}\right) = 6x+\frac{2}{\sqrt{x}}$ Note 2 things here please: I KNOW the differentiation rules, I KNOW that $\frac{d}{dx}\left(3x^2+4\sqrt{x}\right) = 6x+\frac{2}{\sqrt{x}}$ , but I'm having a trouble proving this with a certain method, you see I was training on the basic method -I don't really know what they call it- to get back to the origins of calculus, which is $\frac{d}{dx}f(x) = \lim_{h \to 0}\frac {f(x+h)-f(x)}{h}$ I've done the following: $$\lim_{h \to 0}\frac {3(x+h)^2+4\sqrt{x+h}-3x^2-4\sqrt{x}}{h}$$ which is so far correct, but I'm struggling to transform this to $6x+\frac{2}{\sqrt{x}}$ , I know the problem isn't hard but I wish that someone will be kind and tell me what the next step is.","['limits', 'calculus', 'derivatives']"
4643730,Why is the area of a circle not $2\pi r^2?$ (a fake proof) [duplicate],"This question already has answers here : Trying to understand why circle area is not $2 \pi r^2$ (6 answers) Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved So, I realise that the area of a circle is actually $\pi r^2,$ so basically half of $2\pi r^2,$ however, after trying to prove the area of a circle for myself, I came up with a really convincing and intuitive theorem for why it is 2πr^2 (even though it is not). By the way, I've set the radius to 100/2π so that the circumference would be 100, as I wanted to mark the percentage moved across the circumference for any given angle. There is no reason for this with respects to (my attempt of) this so-called 'proof', it is just for convenience. If you take the circumference of a circle and you flatten it out into a straight line, it would look something like this: From here, I reason that a line segment can be drawn across any , indeed every point along the line equal to the radius of the circle, since said line is equal to the circumference of the circle to begin with, and thus this step just falls out of the definition for how a circle is even defined. Here are a few examples: If you did this for every point across the circumfence and its corresdonding line, you would have filled in the area of the circle on the left, and created a rectangle on the right, whose length (x-axis) is the circumference of the circle and whose height (y-axis) is the radius: It's not obvious what the circumference multiplied by the radius would be, but since we know that the diameter goes into the circumference π times, we can just rewrite the circumference as π•diameter, and in turn rewrite the diameter as 2•radius, so 2πr. Having rewrote the circumference as 2πr, we just multiply this by r to derive 2πr^2. As you can see, this attempt of a proof involves only three steps. It is short, simple, intuitive, and, dare I say, eloquent. Above all else, however, it is also wrong. You can even see this visually, just by eyeballing the image. Now, I need no convincing on what the area of a circle actually is. I've looked up actual proofs online showing why it is πr^2, and I also simply trust the likes of Archimedes, as well as Pythagoras, Newton, the team at NASA, etc. What I need convincing of, however, is that my proof is incorrect. By ""convincing"", I don't mean it in the usual sense, but in quite a literal one. As in, I can consciously accept that my attempted proof is incorrect (again you can even see this visually by comparing the areas of the circle/rectangle), but my heart and soul cannot, because I've managed to construct such a simple, easy to follow, and intuitive proof that ended up being false. It's clear that the mistake made along the way was not a technical one. I mean sure, the mistake is that my derived equation is off by a factor of exactly 2, but there's something very fundamental about the nature of maths itself that I clearly have not grasped, and I have absolutely no starting point to work from in trying to figure out what that is. This is, to me, like trying to understand why two plus two is two instead of four, for I cannot wrap my head around it.","['area', 'circles', 'geometry', 'fake-proofs']"
4643745,Prove $ A^2 = 2A $,"I have the following question in my linear algebra textbook: Let $ A, B \in M_{n \times n}(\mathbb{F}) $ such that $ A(B - I) = A $ and $ B(A - I) = B $ . Prove that $ A^2 = 2A $ . Here is what I have so far: $$ A(B - I) = A \Rightarrow AB - A = A \Rightarrow AB = 2A $$ I understand now that I have to prove $ A^2 = AB $ , but I have no idea how to continue, could you please give me a clue?","['matrices', 'linear-algebra']"
4643764,Generating Functions - Counting how many solutions for an exact $x^n$,"Having trouble with this problem: $$ x_1+x_2+x_3+x_4+x_5+x_6 = 13 $$ $$ x\ne 3 $$ Thanks in advance for any further help. Being able to get to this point: $$\begin{split}
f(x) &= (x^0+x^1+x^2+x^4+x^5+\cdots)^6\\
&= [(-x^3)+{x^0+x^1+x^2+(x^3)+x^4+x^5+\cdots}]^6\\
&= \left[{(-x^3)} + \frac{1}{(1-x)}\right]^6\\
&=\left[ \frac{-x^3(1-x)+1}{(1-x)}\right]^6\\
&=\left[\frac {x^4-x^3+1}{1}\cdot\frac {1}{1-x}\right]^6\\
&=(x^4-x^3+1)^6 \frac {1}{(1-x)^6}\\
\end{split}$$ Now if I long divide $(x^4-x^3+1)$ with $(1-x)$ I get $[(-x^3 + \frac {1}{1-x}) (1-x)]^6 $ thus: $$\begin{split}
f(x)&=\left(\frac {1}{1-x} -x^3\right)^6  \frac {(1-x)^6}{(1-x)^6}\\
&=\left(\frac {1}{1-x} -x^3\right)^6\\
\end{split}$$ That's where I get stuck, honestly.","['algebra-precalculus', 'generating-functions']"
4643798,"Proof that $x>0$ and $r,s\in\mathbb R\implies(x^{r})^{s}=x^{rs}$","I want to  prove the identity $(x^{r})^{s}=x^{rs}$ for real exponents and positive base. My problem essentially boils down to this: I don't know how to prove that for $r_n,s_n\in\mathbb{Q},$ $$\lim_{n\to\infty}{\lim_{k\to\infty}{x^{r_ks_n}}}=\lim_{n\to\infty}{x^{r_ns_n}}.$$ Some of the results that are available for the proof: The other exponent identites $x^rx^s=x^{r+s}$ , and $x^ry^r=(xy)^r$ for real numbers The corresponding identity (and all other exponent equalities and inequalities) for rational exponents That every real number can be expressed as the limit of some increasing (or decreasing if needed) sequence of rational numbers $x^r$ defined as the limit of $x^{r_n},$ where $r_n$ is a sequence of rationals that converges to $r$ All the limit laws, including taking rational exponents inside and out of limits (not real ones of course) Any help would be much appreciated. Please let me know if you need further clarifications.","['real-numbers', 'limits', 'exponentiation', 'real-analysis']"
4643843,Can this rule of inference do replace axiom of extensionality?,"If I replace the below axiom in ZFC (or NBG ) by the below inference rule, there are any consequence in what can be demonstrated? Axiom: If two sets (or classes) have the same elements then their are equal. $$
\forall{ }A\forall{ }B(\forall{ }x((x\in A) \Leftrightarrow (x\in B))\Rightarrow A = B)
$$ Inference Rule: From $\varphi(A)$ and $\forall{ }x((x\in A) \Leftrightarrow (x\in B))$ infers $\varphi(B)$ . $$
\{\varphi(A), \forall{ }x((x\in A) \Leftrightarrow (x\in B))\} \vdash \varphi(B)
$$ Comment This substitution is based on Leibniz Law (but different of this Math.SE question do not quantify about predicates). This substitution have three motives: Play with logic. Remove the equality symbol. Remove the axiom (I like natural deduction, when possible): I imagine that this ""formalism"" have special significance for classes, since a class is defined by the logical property of their elements then one can imagine that the extension (see this Math.SE answer ) is ""not a truth"" but only a ""program"" to perform a proof (see this Math.SE answer ).","['logic', 'alternative-set-theories', 'elementary-set-theory', 'natural-deduction', 'set-theory']"
4643845,Questions about Hausdorff measure.,"I wonder if I can use Hausdorff dimension to conclude that the measure of a graph of a measurable function $$ G_{f}=\left\{ \left(x,f\left(x\right)\right)\thinspace:\thinspace x\in\mathbb{R}^{k}\right\} \subseteq\mathbb{R}^{k+1} $$ is of measure $0$ by the $ m^{k+1}$ measure (Lebesgue measure on $\mathbb{R}^{k+1}$ ). Given such $f$ , define the function $$ \Phi:\mathbb{R}^{k}\to\mathbb{R}^{k+1} $$ . $$ \Phi\left(x\right)=\left(x,f\left(x\right)\right) $$ $\Phi$ is measurable since its composition with projections is measurable, and the graph of $f$ is the image of $\Phi$ . My intuition, is that since $\Phi$ is measurable, its image $\Phi(\mathbb{R}^k)$ cannot have Hausdorff dimension greater than $k$ , and thus the Lebesgue measure on $\mathbb{R}^{k+1}$ , which is just Hasudorff measure of dimension $k+1$ (up to a contant factor), would yield measure 0. How can I justify that indeed the image of $\Phi$ cannot have greater Hausdorff dimension than $k$ ?","['measure-theory', 'real-analysis']"
4643887,"Why $x < \frac1a < y$ implies $\frac1y < a < \frac1x$, not $\frac1x < a < \frac1y$?","Today, I have a rather simple question. EDIT: $$ x, y \in [1,\infty) $$ I have the following inequality: $$ x<\frac{1}{a}<\:y \tag1$$ I want the middle term, which is $\frac{1}{a}$ to be just $ a $ ,
so it should be this: $$ \frac{1}{y}<a<\frac{1}{x} \tag2$$ So, the division from (1) to (2) is clear. But what happened to the signs? Why it is not, instead, this: $$ \frac{1}{x}<a<\frac{1}{y} $$ Thanks for those who invested time reading the question.","['algebra-precalculus', 'arithmetic', 'inequality']"
4643896,If $f:\mathbb{R}\to\mathbb{R}$ is strictly increasing and convex in $\mathbb{R}$ prove $\lim_{x\to\infty} f(x)=\infty$,"The way I did it was using the definition of my highschool book for convexity (which is not a definition and more of a criterion) which says that: If $f$ is continuous on a closed interval $I$ and if $f'$ is strictly increasing in the open interval $I$ then $f$ is convex in all of $I$ so when asked this question in the context of my class I had to answer like so: $f$ convex in $\mathbb{R}$ iff $f'$ strictly increasing in $\mathbb{R}$ and since $f$ is strictly increasing we have $f'(x)\geq0$ $\forall x \in \mathbb{R}$ and $\exists x_0$ such that $f'(x_0)>0$ . Finding the tangent line at $x_0$ gives us: $y=xf'(x_0)-x_0f'(x_0)+f(x_0)$ Since $f$ is convex, $f$ is above its tangent line everywhere except on $x_0$ so we get $$f(x)\geq xf'(x_0)-x_0f'(x_0)+f(x_0)\quad \forall x \in\mathbb{R}$$ Also $$\lim_{x\to\infty}[xf'(x_0)-x_0f'(x_0)+f(x_0)]=\lim_{x\to\infty}xf'(x_0)=\infty $$ Which implies $$\lim_{x\to\infty}f(x)=\infty$$ This approach is also mentioned in this MSE post while also using the Mean Value Theorem which yearns a similar result. What I'm looking for is other more rigorous ways of proving this, using any theorem and any definition from real analysis or anywhere else. Since the actual definition of convexity doesn't require differentiability I'm very interested to see other ways to approach this and since we don't use the $\varepsilon$ - $\delta$ definition of limits I would imagine a rigorous proof would look very different.","['calculus', 'convex-analysis', 'analysis']"
4643908,Problem with understanding how to scale a function,"I'm trying to scale the following function: $$y=\sqrt{1-\frac{x^2}{a+bx}\times c}$$ For example, I want to scale it proportionately by 2. Turns out I need to multiply a and c by 4 (2 squared), but b by just 2. I assume this is something about square root and it also has to do with whether the constant is standing alone or being multiplied by x, but I'm not sure of what's actually going on and how to explain this. Can someone pls help? Edit: for clarification, what I am trying to achieve is to keep the shape of the original function but scale the major axis of the half ellipse (domain 0, 4.2), not just scaling vertically or horizontally. Essentially trying to get the same shape with bigger size. As the screenshot shows, in order to scale it proportionately by 2, I had to multiply b by 2, but a and c had to be multiplied by 4.",['functions']
4643916,"Question on the ""empty function""","Let's say that I have a function $f: A \to B$ . I'd define a function, rigorously, as a set $$
\{(x, f(x)) \mid \forall x \in A, \; \exists ! b \in B \; s.t. b = f(x)\}. 
$$ If $A = \emptyset$ , then $f \subset \emptyset \times A$ , but $\emptyset \times A = \emptyset$ , so that implies that $f = \emptyset$ . That is, $f$ is the ""empty function."" My question is: if I change $B$ , is this the same ""empty function""? I could have $B_1 = \emptyset$ , $B_2 = \mathbb{R}$ , $B_3 = \{1,2,3\}$ , and so forth, and define $f_1: A \to B_1$ , $f_2: A \to B_2$ , and $f_3: A \to B_3$ . Each of these produce the empty set and the empty function, but are they technically different functions since I defined the codomain differently? Is this a case where I would say for all $x \in A$ , $$
f_1 (x) = f_2 (x) = f_3 (x),
$$ i.e., the rules align, vacuously, but $f_1 \neq f_2 \neq f_3$ ?",['functions']
4643937,What are some general strategies to use to get around the issue of non-uniform integrability? Difficulty with exercises in probability theory,"$\newcommand{\pr}{\operatorname{Pr}}\newcommand{\d}{\,\mathrm{d}}\newcommand{\E}{\mathbb{E}}$ I found two nice probability exercises online which at first seemed pretty elementary but I am having a surprising struggle with them: Let $(\Omega;\pr)$ be a probability space. We are given an almost surely vanishing sequence of nonnegative real random variables $(X_n)_{n\in\Bbb N}$ on $\Omega$ satisfying: $$\forall n\in\Bbb N,\,\E(X_n)=n$$ We must prove that $\lim_{n\to\infty}\mathrm{Var}(X_n/n)=\infty$ . And: Let $(\Omega;\pr)$ be a probability space and $p$ a real number greater than two. Given two sequences of random variables $(X_n)_{n\in\Bbb N},(Y_n)_{n\in\Bbb N}$ with both almost surely vanishing and: $$\forall n\in\Bbb N,\,\E(|X_n|^p+|Y_n|^p)\le1$$ We must show that $\E(|X_nY_n|)\to0$ as $n\to\infty$ . I am asking about both because my difficulty with both is essentially the same. I present partial solutions: To the first: For any $N\in\Bbb N$ and $\epsilon>0$ define: $$A_{N,\epsilon}:=\bigcap_{m\ge N}\{X_m\le\epsilon\}$$ Almost sure convergence to zero implies $\lim_{N\to\infty}\pr(A_{N,\epsilon})=1$ for all $\epsilon$ . Moreover, for any $n\ge N$ we know $\{X_n>\epsilon\}\subseteq\Omega\setminus A_{N,\epsilon}$ . Then we can bound, for any fixed $n\ge N$ , $\epsilon$ and $\frac{\epsilon}{n}<\alpha<\beta$ : $$\begin{align}n&=\int_{X_n\le\epsilon}X_n\d\pr+\int_{\epsilon<X_n<n\alpha}X_n\d\pr+\int_{n\alpha\le X_n\le n\beta}X_n\d\pr+\int_{X_n>n\beta}X_n\d\pr\\&\le\epsilon+n\alpha\cdot\pr(\Omega\setminus A_{N,\epsilon})+n\beta\cdot\pr(X_n\ge n\alpha)+\int_{X_n>n\beta}X_n\d\pr\end{align}$$ Hence: $$\E\left(\left(\frac{1}{n}X_n\right)^2\right)\ge\alpha^2\cdot\pr(X_n\ge n\alpha)\ge\frac{\alpha^2}{\beta}-\frac{\epsilon\alpha^2}{n\beta}-\frac{\alpha^3}{n\beta}\cdot\pr(\Omega\setminus A_{N,\epsilon})-\frac{\alpha^2}{n\beta}\int_{X_n>n\beta}X_n\d\pr$$ So if we can make the RHS as large as desired, for some $N$ and all $n\ge N$ and suitable choices of $\alpha,\beta,\epsilon$ , then we are done. So, my only real idea was that, since $\E(X_n)=n$ coupled with the fact that $X_n\to0$ almost surely should imply that $X_n$ should be large with a relatively high probability (otherwise the expectation would be made too small) then I surely have to show $\alpha^2\cdot\pr(X_n\ge n\alpha)$ can be made as large as desired. This was my focus for a while. I found it difficult to find lower bounds of that quantity since I am not well versed in 'tricks' that go beyond simple manipulations + the use of the Tschebyshev inequality. The above was my best attempt. However, it present a serious issue: For any $n$ there will be suitably large $\beta_n$ which can make $\int_{X_n>n\beta_n}X_n\d\pr$ small. BUT , since the sequence $(X_n)_{n\in\Bbb N}$ is not uniformly integrable, I cannot fix a choice of $\beta_n$ . I'll skip the messy inequality chasing, but I can say with confidence that in the above I can make every term as small as desired whilst keeping $\alpha^2/\beta$ large... except for $\frac{\alpha^3}{\beta}\pr(\Omega\setminus A_{N,\epsilon})$ . The trouble is, $\epsilon$ and $N$ have to be fixed first (to make $\pr(\Omega\setminus A_{N,\epsilon})$ suitably small) but for general $n\ge N$ the values $\beta_n$ might be very very large and hence, if I want $\alpha^2/\beta$ to be sufficiently large, the ratio $\alpha^3/\beta$ could blow up. I would like help learning how to get a ""better control"" on the size of things like $\int_{X_n>n\beta}X_n\d\pr$ when as $n$ varies, essentially. On one hand, I'd like $\int X_n$ to be made small, but on the other hand the whole point of this problem is that $\int X_n$ is really quite big - I have failed to balance things. For the other one, the issue is similar: We can show that if $\epsilon>0$ and $\min(|a|,|b|)>\epsilon$ then $|ab|<\epsilon^{2-p}(|a|^p+|b|^p)$ when $p>2$ . Put $Z_n:=|X_n|^p+|Y_n|^p$ for every $n$ . We could start (for all $\epsilon>0$ ): $$\E(|X_nY_n|)\le\epsilon^2+\epsilon^{2-p}\int_{\min(|X_n|,|Y_n|)>\epsilon}Z_n\d\pr$$ And the condition that both sequences vanish almost surely ensures $\pr(\min(|X_n|,|Y_n|)<\epsilon)$ can be made as small as desired for all large $n\ge N$ . We also know $\E(Z_n)<1$ , so we might hope that this gives a way to choose clever values of $\epsilon$ and $N$ (noting that $\epsilon^{2-p}\to\infty$ as $\epsilon\to0^+$ , so care is required) such that $\epsilon^{2-p}\int Z_n\d\pr$ is controlled to be small. If so, then we would be done. But again the trouble is, we don't know whether or not the sequence $(Z_n)_{n\in\Bbb N}$ is uniformly integrable. Without this extra assumption, I don't see how to get the necessary control.","['measure-theory', 'probability-theory', 'real-analysis']"
4643951,How to differentiate the largest eigenvalue of a matrix?,"Currently, I am facing the following problem. Let $t \mapsto H (t)$ be a continuously differentiable, symmetric matrix-valued function. I would like to calculate the following derivative. Is there any general method to calculate it? $$\frac{{\rm d}}{{\rm d} t} \lambda_{\max}(H(t))$$","['eigenvalues-eigenvectors', 'matrices', 'matrix-calculus', 'linear-algebra', 'derivatives']"
4643970,Does the gamblers fallacy not apply to Bayesian probability?,Bayesian probability is an alternative probability theory that uses data from past outcomes to predict future outcomes. Do they have some work-around for the gamblers fallacy or do they just ignore it? Or is Bayesian probability literally just the gamblers fallacy by another name?,"['bayesian', 'probability-theory', 'probability']"
4644004,Understanding Manipulating Inequality,"Today I came across the following inequality in class: $$24-4k<0$$ I tried to manipulate it like this: $$24-(24-4k)<24-0$$ $$4k<24$$ $$k<6$$ This is obviously incorrect. However I can't seem to understand what the problem is.
When solving algebraic equations we can use the method above: $$24-4k=0$$ $$24-(24-4k)=24-0$$ $$4k=24$$ $$k=6$$ So why doesn't it work for manipulating Inequalities.
Furthermore, why doesn't the following work?
Eg. $$24-24-4k<24-0$$ $$-4k<24$$ $$k>-6$$ Please could someone help me. I'm very confused.","['algebra-precalculus', 'linear-algebra', 'inequality']"
4644088,When is the word metric on a finitely generated group right-invariant?,"Let $G$ be a finitely generated group with finite generator $S$ , and define the metric $ d_S : G \times G \to \mathbb{Z} \cap [0, \infty)$ by $$d_S(x, y) = \min \left\{ \ell \geq 0 : \exists s_1, \ldots, s_\ell \in \left( S \cup S^{-1} \right) \setminus \left\{ e \right\} \; \left( x^{-1} y = s_1 \cdots s_\ell \right) \right\} ,$$ i.e. $d_S$ is the word metric associated to $S$ . The metric $d$ will automatically be left-invariant, i.e. $d_S(x, y) = d_S(gx, gy)$ for all $g, x, y \in G$ , since $(g x)^{-1} (g y) = x^{-1} g^{-1} g y = x^{-1} y$ . However, this metric need not always be right-invariant. For an example of the word metric not being right-invariant, consider the example of $G = F_2$ , i.e. the free group on two generators, with (free) generator $S = \{a, b\}$ . Then $d_S(e, b) = 1$ , but $d_S(e a, b a) = d_S \left( e, a^{-1} b a \right) = 3$ , so the ""canonical"" word metric on the free group with two generators is not right-invariant. On the other hand, if $G$ is abelian, then every word metric is right-invariant. Right-invariance is equivalent to the conjugation action $g \cdot x = g^{-1} x g$ of $G$ on itself satisfying $d_S(e, g \cdot x) = d_S(e, x)$ for all $g, x \in G$ , since $d_S (xg , y g) = d\left(e, g^{-1} y g \right)$ . So conjugation needs to preserve word length, and of course the conjugation action for an abelian group is trivial. Alternatively, if $G$ is finite, and $S = G$ , then $d_S$ is the discrete metric (i.e. $d_S(x, y) = \delta(x, y)$ ), and thus is also right-invariant. This trick works for any discrete group if we drop our assumption that $S$ be finite, but I'd rather consider the finite generator case. My question: is there a straightforward characterization of when the word metric is both left- and right-invariant? My guess would be that every word metric is right-invariant if and only if $G$ is abelian, but I can't prove this either. This seems like the kind of problem that should be elementary to solve, but alas, I haven't solved it. Alternatively, is there some sense in which the word metric will be ""almost right-invariant"", where we can place bounds on the extent to which the word metric fails to be right-invariant? Thanks in advance for your help! EDIT 1: Here's a partial answer based on @Mariano Suárez-Álvarez's suggestion, at the end of which are some related follow-up questions. Claim: Let $S$ be a finite generator for $G$ , and set $\tilde{S} = \left(S \cup S^{-1} \right) \setminus \{e\}$ . Let $g \star x = g^{-1} x g$ be the action of $G$ on itself by conjugation (note: this is a right action which I foolishly wrote on the left; thanks @Captain Lama for pointing this out). Then $d_S$ is right-invariant if and only if $s \star t \in \tilde{S}$ for all $s, t \in \tilde{S}$ . Proof: The necessity is obvious, since if there existed $s, t \in \tilde{S}$ for which $s \star t \not \in \tilde{S}$ , then we'd have $d_S(s, t s) = d_S \left( e, s \star t \right) \neq 1 = d_S (e, t)$ . Now we claim it's sufficient. First, we remark that if $\tilde{S}$ is closed under conjugation by itself, then $\tilde{S}$ is closed under conjugation by all elements of $G$ , i.e. $g \cdot s \in \tilde{S}$ for all $s \in \tilde{S}, g \in G$ . This follows by writing an arbitrary element $g$ of $G$ as a product of elements of $\tilde{S}$ and applying an inductive argument. Moreover, we remark that if $\tilde{S}$ is closed under conjugation, then $|x \star y| \leq |y|$ for all $x, y \in G$ , where $|\cdot|$ denotes word length. Let $x = s_1 \cdots s_\ell, y = t_1 \cdots t_m$ , where $s_i, t_j \in \tilde{S}$ and $|y| = m$ . Then \begin{align*}
|x \star y| & = |x \star (t_1 \cdots t_m)| \\
& = |(x \star t_1) \cdots (x \star t_m)| \\
& = \left| t_1' \right| \cdots \left| t_m' \right| \\
& = m & = |y| ,
\end{align*} where $t_j' = x \star t_j \in \tilde{S}$ . Thus conjugation cannot increase word length. We claim, however, that conjugation preserves word length, i.e. that $|x \star y| = |y|$ for all $x, y \in G$ . Assume for contradiction that there exist $x, y \in G$ such that $|x \star y| < |y|$ . Let $y' = x \star y$ . Then $\left| x^{-1} \star y' \right| = |y| > \left|y'\right|$ . This contradicts our earlier claim that $\left|z \star w\right| \leq |w|$ for all $z, w \in G$ . Thus if $\tilde{S}$ is closed under conjugation, then conjugation preserves word length. From here, it follows that if $x, y, g \in G$ , then $$d_S(x g, y g) = \left| (x g)^{-1} (yg) \right| = \left| g^{-1} x^{-1} y g \right| = \left| g \star \left( x^{-1} y \right) \right| = \left| x^{-1} y \right| = d_S(x, y) .$$ This concludes the proof. This criterion encompasses some non-abelian groups. For example, as we mentioned, every finite group can be realized in this way by taking $S = G \setminus \{e\}$ . This proof can be elaborate on to show the word metric is ""almost right-invariant"" in the sense of ""Lie Groups and Error Analysis"" by Schiff and Shnider: Set $$M = \max_{s, t \in \tilde{S}} |s \star t| \in \mathbb{N} .$$ For each $g \in G$ , set $\rho(g) = M^{|g|}$ . Consider $x, y \in G$ , and let $g = s_1 \cdots s_\ell, x^{-1} y = t_1 \cdots t_m$ for $s_i t_j \in \tilde{S}$ , where $\ell = |g|, m = \left| x^{-1} y \right| = d_S(x, y)$ . Then \begin{align*}
d_S(x g, y g) & = \left| g \star \left( x^{-1} y \right) \right| \\
 & = \left| (g \star t_1) \cdots (g \star t_m) \right| \\
 & \leq |g \star t_1| + \cdots + |g \star t_m| \\
 & = \sum_{j = 1}^m |(s_1 \cdots s_\ell) \cdot t_j| .
\end{align*} Applying an inductive argument, we can conclude that $|(s_1 \cdots s_\ell) \cdot t_j| \leq M^\ell$ for all $j$ , so \begin{align*}
d_S(x g, y g) & \leq \sum_{j = 1}^m |(s_1 \cdots s_\ell) \cdot t_j| \\
 & \leq \sum_{j = 1}^m M^\ell \\
 & = M^\ell \cdot m \\
 & = M^{|g|} \left| x^{-1} y \right| & = M^{|g|} d_S(x, y) .
\end{align*} Edit 2: As @Sean Eberhard pointed out, this estimate can be cleaned up significantly to get \begin{align*}
d_S(x g, y g) & = \left| g^{-1} \left( x^{-1} y \right) g \right| \\
 & \leq \left| g^{-1} \right| + d_S(x, y) + |g| \\
 & = 2|g| + d_S(x, y) \\
 & \leq (2|g| + 1) d_S(x, y) ,
\end{align*} so in general $d_S(x g, y g) \leq (2|g| + 1) d_S(x, y)$ . However, this is estimate is in general sharp. Consider the case where $G$ is a free group of rank $\geq 2$ generated freely by $S$ . If $g \in G$ , then we can write $g = s_1 \cdots s_\ell$ for $s_1, \ldots, s_\ell \in S \cup S^{-1}$ , where the word is in reduced form so $|g| = \ell$ . Let $x$ be an element of $S \cup S^{-1}$ such that $x \not \in \left\{ s_1, s_1^{-1} \right\}$ . Then $g^{-1} x g = s_\ell^{-1} \cdots s_1^{-1} x s_1 \cdots s_\ell$ , and this is the reduced form, so $\left| g^{-1} x g \right| = 2|g| + 1$ . Thus $$d_S(g, xg) = 2|g| + 1 = (2|g| + 1) |x| = (2|g| + 1) d_S(e, x) .$$ So if $G$ is a nonabelian free group and $S$ is a free generator for $G$ , then for every $g \in G$ exists $x \in G$ such that $d_S(g, xg) = (2|g| + 1) d(e, x)$ , meaning that $2|g| + 1$ is the optimal Lipschitz constant for $x \mapsto g^{-1} x g$ . Assuming all the above is correct, I think there are at least two questions that are reasonable to ask: What groups admit right-invariant word metrics, or equivalently, which groups admit finite generators closed under conjugation? Clearly every finitely generated abelian group fits this bill, but I've yet to cook up an infinite non-abeliean example For which groups is every word metric right-invariant? Clearly every finitely generated abelian group fits this bill, but I've yet to cook up a non-abelian example, including the finite case.","['geometric-group-theory', 'group-theory']"
4644133,Section of direct-sum and tensor product of line bundles,"Suppose I have a variety $X$ and suppose that I have three line bundles $L_1, L_2$ and $L_3$ over $X$ such that $L_1 \oplus L_2 \oplus (L_2 \otimes L_3)$ has a nonzero global section. Does it imply that we have a nonzero global section of the bundle $L_1 \oplus L_2 \oplus L_3$ ?",['algebraic-geometry']
4644142,Identifying the truth value of quantified statements,"I need help properly identifying the truth values of the following quantified statements: i. Q(0, -3, 1), where Q(x,y,z) := ""The numbers x-y, x+2y, and $z^2+1$ are even."" ii. P(Angela) and P(Anderson), where P(X) := ""The name X contains at least two vowels."" I understand that the first statement is a universal truth statement and I would have to check that the truth value of $(x-y)\wedge (x+2y) \wedge (z^2 +1)$ is true. As for the second statement, I think that I must prove that that the set consisting the letters of the name ""Angela Anderson"" has at least one true value. I think that I'm just having some trouble formatting my answer into the ""proper"" way of proving the truth values. Can anyone help? Thank you!","['quantifiers', 'discrete-mathematics']"
4644183,Question about series as infinite summation,"I have two questions about the series. Grandi's series $1-1+1-1+ \cdots$ is not convergent since its partial summation is not convergent. But someone told me it could be convergent if we rearrange the series.
For example, $1-1+1-1+1-1+...=(1-1)+(1-1)+(1-1)+...=0+0+0+...=0$ . (1) My question is for this new series, we still need to calculate its partial summation by definition and obviously, the partial summation is $S_n=1$ for odds and $S_n=0$ for even, how can we say it is now $lim S_n=0$ . About the definition of a series. Do we still regard series as infinite summation( $\sum_{n=1}^{\infty}a_n=a_1+a_2+...$ ) even have defined it as the limit of partial summation? And we have $\sum_{n=1}^{m-1}a_n+\sum_{n=m}^{\infty}a_n= \sum_{n=1}^{\infty}a_n$ for convergent series? And we allow add in parentheses like in (1)?","['calculus', 'analysis', 'sequences-and-series']"
4644214,"If $S = P^{-1}(0)$ is a surface, show $\int |K|\leq 4\pi C(d)$","Let $P\in \mathbb{R}[X,Y,Z]$ be an irreducible polynomial, such that $S=P^{-1}(0)$ is a non compact regular surface. If $K$ denotes the Gaussian curvature, prove that: $$\int_S |K|\leq 4\pi C(d)$$ Where $C(d)$ is a constant dependent on the degree of P. If it were compact, one could take the maximum value of $|K|$ and use it to conjure an upper bound. Furthermore, if it were compact, Chern-Lashoff's Theorem would use the fact that the normal map is onto in order to gain a lower bound $\int_S |K| \geq 4 \pi$ . However these techniques are all dependent on compactness, which means I really do not know how to estimate this integral.","['surfaces', 'curvature', 'differential-geometry']"
4644230,"What is the probability that $(1+u_1)(1+u_1 u_2)(1+u_1 u_2 u_3)...>e$, where each $u$ is a uniformly random real number in $(0,1)$?","What is the probability that $\prod\limits_{k=1}^\infty \left(1+\prod\limits_{i=1}^k u_i\right)>e$ , where the $u$ 's are i.i.d. $\text{Uniform}(0,1)$ -variables ? The product, $\prod\limits_{k=1}^\infty \left(1+\prod\limits_{i=1}^k u_i\right)$ , has an expectation of $e$ . I am wondering, what is the probability that it is greater than its expectation. Excel simulations suggest that the answer is (simply) $\frac13$ . EDIT: After reading @joriki's comment, I ran more Excel simulations, and now it seems the answer is more like $0.328$ .","['infinite-product', 'probability', 'random-variables']"
