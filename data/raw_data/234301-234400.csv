question_id,title,body,tags
4893694,Shortest path in a graph connecting successive multiples,"Consider an infinite undirected graph whose nodes are the positive integers $\{1, 2, ...\}$ with edges connecting successive multiples of all integers (for all $k,n \geq 1$ there is an edge between $kn$ and $(k+1)n$ ). For example, 40 and 50 are connected by an edge, as they are successive multiples of 10. So we can get from 38 to 49 in three steps by the path: 38 → 40 → 50 → 49. (You can imagine a subway network, where we first ride the ""multiples of 2 line"" for one stop, then transfer to the high-speed ""multiples of 10 line"", and finally change directions for one stop on the ""multiples of 1 line"", which connects all the positive integers but is very slow.) I think this is the shortest path between 38 and 49, so let's write $d(38,49) = 3$ . Is this a ""well-known"" graph? Is there a nice formula or algorithm to efficiently compute $d(x, y)$ ? Here is the subgraph on $\{1, \dots, 24\}$ :","['graph-theory', 'number-theory']"
4893718,Ergodic series converge to the expectation?,"Let $(X_i, Y_i)_{i\in\mathbb{N}}$ be a real-valued stochastic process. We say that $X$ is mean-ergodic, if $$\frac{1}{n}\sum_{i=1}^nX_i\to \mathbb{E}X_1$$ in probability as $n\to\infty$ . Let $S_n:=\{i\in\{1, \dots, n\}: Y_i\in D \}$ where $D$ is some measurable set with $P(Y_i\in D)>0$ . If $(X_i, Y_i)_{i\in\mathbb{N}}$ are mean-ergodic + strictly stationary, does it imply that $$\frac{1}{|S_n|}\sum_{i\in S_n}X_i\to \mathbb{E}[X_1\mid Y_1\in D] $$ in probability as $n\to\infty$ ? I am thinking about the following approach. Let $\tilde{Y}_i := 1[Y_i\in D]$ and let $W_i:=X_i\tilde{Y}_i$ . Now, $$
\frac{1}{|S_n|} \sum_{i\in S_n}X_i =    \bigg( \frac{n}{\sum_{i\leq n}\tilde{Y}_i}\bigg)   \bigg( \frac{1}{n} \sum_{i\leq n}W_i    \bigg) . 
$$ The first part converges to $1/P(Y_1\in D)$ . Does the second part converge to its expectation? As @Michael showed, NO. Hence, function of mean-ergodic series is not a mean-ergodic series. However, if we assume ergodicity in the sense of ''the underlying shift operator is an ergodic transformation'', is it sufficient assumption for the convergence to hold?","['statistics', 'ergodic-theory', 'stationary-processes', 'stochastic-processes', 'dynamical-systems']"
4893752,Transition probability density function for a non-trivial diffusion process.,"Let $\mu$ and $\sigma > 0$ and $\beta_1 \ge 0 $ and $\beta_2 \ge 0$ be real numbers.
Consider a stochastic process $X_t$ that satisfies the following stochastic differential equation: \begin{equation}
d X_t = \mu X_t^{\beta_1} dt + \sigma X_t^{\beta_2} dB_t \tag{1}
\end{equation} where $B_t$ is the Brownian motion. My objective is to derive the transition probability density function $P_x\left( X_t \in dz\right) := P\left( X_t \in (z,z+dz) | X_0 = x \right)$ for the process in question.
Here we will be using a result known from literature (see section 4.11 pages 149-161 in K Ito& H P McKean Jr, ""Diffusion Processes and their Sample Paths"") as below. Theorem: The Laplace transform (with respect to time) of the transition probability density
in question  reads: \begin{equation}
\int\limits_0^\infty e^{-\lambda t} P_x\left( X_t \in dz\right) dt = G_\lambda(z,x) \cdot m(dz) \tag{2}
\end{equation} Here $m(z):= \left( 1/2 \sigma^2(z) S(z) \right)^{-1}$ is the speed measure  with $S(y) := \exp\left(- \int\limits_1^y 2 \mu(\xi)/\sigma^2(\xi) d\xi \right)$ being the scale measure. Then $G_\lambda​(z,x)$ is the Green's function of the operator $\lambda - {\mathfrak G}^{*}$ with ${\mathfrak G}^{*}$ being the infinitesimal generator, i.e. such an operator that its action on a trial function $f$ reads $\left({\mathfrak G}^{*} f\right)(x) = \lim\limits_{\epsilon \rightarrow 0} E \left[ f(X_\epsilon) - f(x) \right]/\epsilon$ subject to $X_0=x$ . Note that the Green's function is easily determined by the eigen-functions and it reads $G_\lambda(z,x) = S(x)/{\mathfrak W}_\lambda(x) \left( G_\lambda(x) F_\lambda(z) 1_{z < x} + F_\lambda(x) G_\lambda(z) 1_{z > x} \right)$ where $F_\lambda(z)$ and $G_\lambda(z)$ is a
strictly increasing and a strictly decreasing eigen-function of the infinitesimal generator to an eigenvalue $\lambda$ and ${\mathfrak W}_\lambda(x) := F_\lambda^{'}(x) G_\lambda(x) - G_\lambda^{'}(x) F_\lambda(x)$ is the Wronskian of the generator in question. What have I managed to achieve?: I have found the result for $(\beta_1,\beta_2) = (1, \beta)$ with $\beta >1$ and I present the derivation below. Here the infinitesimal generator reads ${\mathfrak G}_z := \mu z \cdot d/dz + 1/2 \sigma^2 z^{2 \beta} d^2/ d z^2$ and
its eigenfunctions  (compare my previous question on a similar topic ) read: \begin{eqnarray}
F(z) &=& U\left(\frac{\lambda}{2(-1+\beta) \mu}, 1+ \frac{1}{2(-1+\beta)}, \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2} \right) \tag{3a} \\
G(z) &=& F_{1,1}\left(\frac{\lambda}{2(-1+\beta) \mu}, 1+ \frac{1}{2(-1+\beta)}, \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2} \right) \tag{3b} 
\end{eqnarray} where $U$ is the confluent hypergeometric function. Note that $F^{'}(z) = \frac{\lambda  z^{1-2 \beta }
   U\left(\frac{\lambda }{2 (\beta -1) \mu
   }+1,2+\frac{1}{2 (\beta
   -1)},\frac{z^{2-2 \beta } \mu }{s^2
   (\beta -1)}\right)}{(\beta -1) s^2} >0$ and $G^{'}(z) = -\frac{2 \lambda  z^{1-2 \beta } \,
   _1F_1\left(\frac{\lambda }{2 (\beta -1)
   \mu }+1;2+\frac{1}{2 (\beta
   -1)};\frac{z^{2-2 \beta } \mu }{s^2
   (\beta -1)}\right)}{(2 \beta -1) s^2} < 0$ (note that from the integral representations we see that both $U$ and $F_{1,1}$ are positive --well up to a multiplicative constant in the second case) and as such $F(z)$ and $G(z)$ are strictly increasing and strictly decreasing as it should be. Now the Wronskian  is computed as follows: \begin{eqnarray}
{\mathfrak W}_\lambda(x) &=& {\mathfrak W}_\lambda(1) \cdot \exp\left( -\int\limits_1^x  \frac{2\mu(\xi)}{\sigma^2(\xi)} d\xi \right) \tag{4a} \\
&=& {\mathfrak W}_\lambda(1) \cdot \exp\left( - \frac{2 \mu}{\sigma^2} \frac{x^{2-2\beta} -1}{2-2\beta}\right) \tag{4b} \\
&=&
-\frac{2 (\beta -1) \Gamma
   \left(1+\frac{1}{2 (\beta -1)}\right)
   e^{\frac{\mu }{(\beta -1) \sigma ^2}}
   \left(\frac{\mu }{(\beta -1) \sigma
   ^2}\right)^{\frac{1}{2-2 \beta
   }}}{\Gamma \left(\frac{\lambda }{(2
   \beta -2) \mu }\right)}
\cdot
\exp\left( - \frac{2 \mu}{\sigma^2} \frac{x^{2-2\beta} -1}{2-2\beta}\right) \tag{4c}
\end{eqnarray} The first line above simply follows form the first order differential equation being satisfied by the Wronskian. The second line follows form the fact that $\mu(\xi), \sigma(\xi) = \mu \cdot \xi, \sigma \cdot \xi^\beta$ .
The step to the final line is the most laborious, but not really that mind boggling.
Here one employs the integral representations in order to express the Wronskian at unity as a double integral in which one changes variables to obtain the neat final result. Now we compute the inverse Laplace transform as the Bromwich integral and we use Cauchy theorem (being applied to the usual contour in the negative complex half-plane-- the second & the third quadrants) to evaluate that integral. As seen from $(4c)$ the Laplace transform has infinitely many simple poles at $\lambda_n = (2\beta-2) \mu (-n)$ for $n=0,1,2,\cdots$ . Then the final result reads: \begin{eqnarray}
&&\left. P_x \left( X_t \in dz \right)/dz =
\frac{2 \mu  \left(\frac{\mu }{(\beta -1)
   \sigma ^2}\right)^{\frac{1}{2 (\beta
   -1)}}}{\sigma ^2 \Gamma
   \left(1+\frac{1}{2 (\beta -1)}\right)}
\cdot
z^{-2 \beta } e^{-\frac{\mu  z^{2-2 \beta
   }}{(\beta -1) \sigma ^2}}
\cdot \right.\\
&& \left.
\sum\limits_{n=0}^\infty
\frac{(-1)^n}{n!}
%
\, _1F_1\left(-n;1+\frac{1}{2 (\beta
   -1)};\frac{\mu  (x \vee z)^{2-2 \beta
   }}{(\beta -1) \sigma ^2}\right)
   U\left(-n,1+\frac{1}{2 (\beta
   -1)},\frac{\mu  (x \wedge z)^{2-2 \beta
   }}{(\beta -1) \sigma ^2}\right)
%
\cdot
e^{-2(-1+\beta) \cdot \mu \cdot n \cdot t} \right. \tag{5}
\end{eqnarray} Now we have checked numerically that the result above matches with the solution known from literature (see equations (4) and (5) in V Linetsky, R Mendoza, The Constant Elasticity of Variance Model). Here we go: In[1991]:= (*Verify whether our solution matches that known from \
literature?*)
Clear[mu, s, b, z, lmb, phi, v, W, W1, W0, x];
{mu, s, b} = {1/2, 1/3 + 3/10, 5/2};
{x, z} = RandomReal[{1, 3}, 2, WorkingPrecision -> 50];
t = 3/2;
(*Our solution:*)
res1 = (2  mu ((mu/((-1 + b) s^2))^((1/(2 (-1 + b))))) )/(
   s^2 Gamma[1 + 1/(2 (-1 + b))])
    E^(-((mu z^(2 - 2 b))/((-1 + b) s^2))) z^(-2 b)
    Table[(-1)^n/
     n! HypergeometricU[-n, 1 + 1/(2 (-1 + b)), 
      mu/((-1 + b) s^2) Min[x, z]^(2 - 2 b)] Hypergeometric1F1[-n, 
      1 + 1/(2 (-1 + b)), 
      mu/((-1 + b) s^2) Max[x, z]^(2 - 2 b)] E^(- 
       2 (-1 + b) mu n t), {n, 0, M}];
Total[res1] // N

(*Solution known from literature: Equations (4) & (5) page 2 in V \
Linetsky & R Mendoza The Constant Elasticity of Variance Model.Here \
the solution was obtained,using different techniques,meaning by \
re-scaling the current price and by changing time in the solution of \
a simpler problem,i.e.a problem with the drift parameter being set to \
zero.*)

Clear[tau]; tau[t_] := (Exp[2 mu (b - 1) t] - 1)/(2 mu (b - 1));
res2 = Exp[-mu t] ((Exp[-mu t] z)^(-2 b + 1/2) x^(1/2))/(
   s^2 Abs[b - 1] tau[t])
    BesselI[1/(2 Abs[ b - 1]), (x^(-b + 1) (Exp[-mu t] z)^(-b + 1))/(
    s^2 (b - 1)^2 tau[
      t])] Exp[-((x^(-2 b + 2) + (Exp[-mu t] z)^(-2 b + 2))/(
     2 s^2 (b - 1)^2 tau[t]))];
res2 // N

Out[1996]= 0.0481819

Out[1999]= 0.0481819 Having said all this my question is how do we find the solution in the generic case, i.e. when $\beta_1 \neq 1$ ?
Another obvious question would be to provide a literature reference, if it exists, on that problem solution.","['inverse-laplace', 'ordinary-differential-equations', 'stochastic-processes', 'greens-function', 'hypergeometric-function']"
4893763,Can you remove the poles from $\frac{f'(x)}{f(x) - y}$ with $L^\infty_y L^1_x$ error?,"I believe that the following below holds, which I need for some estimates. I have been struggling to prove it though! Let $f \in C^1(\mathbb{R}; \mathbb{R})$ have finitely many critical points and be generally a rather nice function (for example a real power of a polynomial). I might need more assumptions but I could not nail it down yet. Let $y \in \mathbb{R}$ and $x_1 < \dots < x_n$ be all the solutions to $f(x) = y$ . I claim that there exist $a_j(y), b \in \mathbb{R}$ such that $$
\frac{f'(x)}{f(x) - y} = \frac{b}{1 + |x|} + \sum_{j=1}^n \frac{a_j(y)}{x - x_j} + R(y,x)
$$ where $\sup_{y \in \mathbb{R}, 1 \leq j \leq n} |a_j(y)| < \infty$ (actually trivial) and $$
\sup_{y \in \mathbb{R}} \int_{\mathbb{R}} |R(y, x)| dx < \infty \,.
$$ The idea is simple and I can illustrate what is supposed to happend for the case where $y = 0$ and $\phi(x) = y$ has only one solution $x_1 = 0$ .
Let's say we assume that both $\frac{x f'(x)}{f(x)}$ as well as $\frac{f'(\frac{1}{x})}{f(\frac{1}{x})}$ are differentiable at $0$ with value $0$ . This means we can make the following Taylor expansions: $$
\frac{x f'(x)}{f(x)} = a_1 + a_2 x + R_0(x) x \\
\frac{f'(\frac{1}{x})}{f(\frac{1}{x})} = 0 + b_1 x + b_2 x^2 + R_\infty\left(\frac{1}{x}\right) x^2
$$ which implies $$
\frac{f'(x)}{f(x)} - \frac{a_1}{x} = a_2 + R_0(x) \\
\frac{f'(x)}{f(x)} - \frac{b_1}{x} = \frac{b_2}{x^2} + R_\infty(x) \frac{1}{x^2}
$$ where due to Taylor's theorem the remainder terms $R_0(x)$ and $R_\infty\left(\frac{1}{x}\right)$ are continuous at $0$ and $\infty$ respectively, and hence $R_0 \in C(B_1(0))$ and $R_\infty \in C(\mathbb{R} \setminus B_1(0))$ . Basically, we can understand what happens near the pole and near infinity, and in both regimes cancel out the $\frac{1}{x}$ order. I have of course tried to apply this technique to the general case but struggled with the following issues: I need to estimate my remainder terms uniformly in $y$ , and I have not been able to get a grip on that with the various forms. I always have to deal with the function $\frac{(x - x_j) f'(x)}{f(x) - y}$ . If $f$ is differentiable then the limit $x \rightarrow x_j$ exists, and it is $1$ if $f'(x_j) = 0$ . More generally it seems to be the ""order"" of vanishing of the equation $f(x_j) - y = 0$ . Remark: I have posted another question about this function and I am about to answer it myself. We can understand its derivatives around $y$ , and how exactly they blow up near a critical point. When $x_0$ is near a critical point of $f$ the behavior of the above function becomes bad, related to the fact that two poles are merging into one. I have a feeling that there should be some known theory regarding this. Removing the poles from your function should yield you something continuous, and removing the decay at infinity something integrable. Furthermore, why should this not work uniformly in $y$ ? Lastly I would like to remark that I am trying to not only find the proof but also the lemma here. That is, if you find a counterexample to what I have exactly formulated, it indicates that further assumptions are needed but it might not fully answer my question.","['laurent-series', 'real-analysis', 'complex-analysis', 'taylor-expansion', 'derivatives']"
4893808,General solution of $\cos(3\theta) - \cos(\theta) = 0$,"General solution of $\cos(3\theta) - \cos(\theta) = 0$ Method 1: $$4\cos^{3}(\theta) - 3\cos(\theta) - \cos(\theta) = 0$$ $$4\cos^{3}(\theta) - 4\cos(\theta) = 0$$ $$4\cos(\theta) (\cos^{2} (\theta) - 1) = 0$$ $$\cos(\theta) (\cos^{2} (\theta) - 1) = 0$$ From here,
Either $$\cos(\theta) = 0$$ thus $$\theta = \frac{(2n+1)\pi}{2}$$ Or $$\cos^{2} (\theta) - 1 = 0$$ $$\cos^{2} (\theta) = 1$$ $$\cos^{2} (\theta) = \cos^{2} (0)$$ $$\theta = n\pi$$ Method 2: $$2\sin(2\theta).\sin(\theta) = 0$$ $$\sin(2\theta).\sin(\theta) = 0$$ Either $$\sin(2\theta) = 0$$ $$2\theta = n\pi$$ $$\theta = \frac{n\pi}{2}$$ Or $$\sin(\theta) = 0$$ $$\theta = n\pi$$ Which among the two methods is correct? If both methods are correct, why do we get different answers?",['trigonometry']
4893822,Eigenvalues of likelihood ratio,"Consider the following hypothesis testing problem: Under $H_0$ : $(X,Y) \sim N(0,1)\times N(0,1)$ , i.e. $X$ and $Y$ are independent standard normal. Under $H_1$ : $(X,Y) \sim N\left(\begin{pmatrix}
 0\\0
\end{pmatrix},\begin{pmatrix}
  1& \rho\\
 \rho &1
\end{pmatrix}\right)$ , i.e. they are normal distribution with correlation coefficient $\rho$ . Let $Q$ denote the density of $N(0,1)$ , then the joint density of $(X,Y)$ under $H_0$ is $Q(x)Q(y)$ . Let $P(x,y)$ denote the joint density of $(X,Y)$ under $H_1$ . Then $L(x,y)= \frac{P(x,y)}{Q(x)Q(y)}$ is the likelihood ratio. For functions $f,g:\mathbb{R}\times \mathbb{R}\to \mathbb{R}$ , lets define inner product induced by $Q$ as $<f,g> = \mathbb{E}_{(X,Y)\sim N(0,1)\times N(0,1)} f(X,Y)g(X,Y)$ . Then we know that $L(x,y)$ is diagonalizable in this inner product space and can be written as $$L(x,y) = \sum_{k=0}^{\infty} \lambda_k \phi_k(x)\phi_k(y)$$ where $\lambda_k$ 's are the eigenvalues of $L$ (in this particular example $\lambda_k = \rho^k$ ) and $\phi_k$ 's is a set of orthonomal basis (in this particular example, $\phi_k$ is Hermite polynomial multiplied by constant). My question is if we conduct a transform on the distribution of $X$ and $Y$ , will the eigenvalues of likelihood ratio change or not? To be specific, Let $F$ denote the CDF of $N(0,1)$ . Then $F(X)$ follows $U(0,1)$ (uniform distribution on $(0,1)$ ). After this transformation, we have those two new hypotheses: Under $H_0$ : $(F(X),F(Y)) \sim U(0,1)\times U(0,1)$ . Here we use $\tilde Q(x) \tilde Q(y)$ denote the new joint density. Under $H_1$ : $(F(X),F(Y))$ follows a new distribution where we use $\tilde P(x,y)$ to denote the new density. Then $\tilde L(x,y) =\frac{\tilde P(x,y)}{\tilde Q(x)\tilde Q(y)}$ is the new likelihood ratio after transformation. For functions $f,g : (0,1)\times (0,1)\to \mathbb{R}$ , we define the new inner product as $\langle f,g \rangle = \mathbb{E}_{(X,Y)\sim U(0,1)\times U(0,1)} f(X,Y)g(X,Y)$ . Then $\tilde L$ also has an decomposition in this inner product space, as $$\tilde L(x,y) = \sum_{k=0}^{\infty} \tilde \lambda_k \tilde \phi_k(x)\tilde \phi_k(y)$$ . Here we must have $\phi_k \neq \tilde \phi_k$ becasue they are orthonormal basis on different spaces. But do we have $\lambda_k = \tilde \lambda_k$ ? Sorry about the long description. Let me know if there is any thing needs clarification.","['probability-theory', 'eigenfunctions', 'functional-analysis', 'hypothesis-testing']"
4893848,How to prove this graph doesn't have a Hamiltonian cycle?,"How do I prove this graph doesn't have a Hamiltonian cycle? By looking into the graph, I see that if my starting point is $a$ and then I move to $b$ and then let's suppose that I visit every vertex under $a,b,d$ . When I reach vertex $d$ , if I move to $c$ (which is the last vertex left to visit), then that would leave me with not way to get back to $a$ and complete the cycle. Of course, this reasoning is not good enough to say this graph doesn't have a Hamiltonian cycle. I hope you can help me guys with this. Thanks!","['graph-theory', 'discrete-mathematics']"
4893915,Why does this proof work: $\sum\limits_{n=1}^ \infty \left(\frac{1}{4n-1} - \frac{1}{4n}\right)= \frac{\ln(64)- \pi}{8}$?,$$f(x):= \sum_{n=1}^ \infty \left(\frac{x^{4n-1}}{4n-1} - \frac{x^{4n}}{4n}\right)$$ $$f'(x) = \sum_{n=1}^ \infty ( x^{4n-2}- x^{4n-1})= \frac{x^2}{(1+x)(1+x^2)}$$ $$\int_0 ^1 \frac{x^2}{(1+x)(1+x^2)}= \frac{\ln(64)- \pi}{8} $$ This proof is not correct because $f_N(x) := \sum\limits_{n=1}^ \infty \left(\frac{x^{4n-1}}{4n-1} - \frac{x^{4n}}{4n}\right) $ and $f_N'(x)=\sum\limits_{n=1}^ N( x^{4n-2}- x^{4n-1})= x^{4}\left(\frac{1-x}{x^2}  \right)\frac{x^{4N+4}-1 }{x^4-1}$ doesn't converge uniformly so the the derivative and the sum couldn't be interchanged but the answer is correct. Is it a coincidence ? Is there is other condition to interchange sum and  derivative that don't require uniform convergence? Is there is a way to justify this ? Any other conditions? I am pretty sure I can generate an infinite cases where this interchange work from this example by changing the '4' to any other  integer greater than 1. So why does this work ?,"['summation', 'examples-counterexamples', 'real-analysis', 'uniform-convergence', 'derivatives']"
4893917,"Showing that $x/y$, where $x=x_1/x_0$ and $y=x_2/x_0$, is a local parameter of $x_0x_2^2=x_1(x_1-x_0)(x_1-2x_0)$ at $p=(0:0:1)$","This is question 5 from https://www.dpmms.cam.ac.uk/study/II/AlgebraicGeometry/2023-2024/HW4.pdf . Consider the curve $$ x_0x_2^2 = x_1(x_1-x_0)(x_1-2x_0)$$ over a field of characteristic zero. I need to show $x/y$ is a local parameter at $p=(0:0:1)$ the point at infinity , where $x=x_1/x_0$ and $y=x_2/x_0$ . Hence, find the divisor of $x$ and the divisor of $y$ . Firstly, I don't think this makes sense because $x/y$ isn't even defined at $p$ . So I presume we mean $x_1/x_2$ is a local parameter. I can write To show $x/y$ is a local parameter, I need to show directly that $x/y \in m_p$ but not $m_p^2$ . The proof might go like 'Suppose $x/y$ is in $m_p^2$ '. I don't know what to do next, we have to use the equation of the curve somehow. We know $m_p$ is non-empty since the curve is non-singular. EDIT: I don't understand how $m_p = \langle x/y\rangle$ is even possible since $x_0\in m_p$ is not related to $x_1/x_2$ by a unit. Could an argument go like this: $x/y=0$ iff $x_1=0$ . Putting $x_1=0$ into the equation we have $x_0x_2^2=0$ . So $(1:0:0)$ is a double root and $(0:0:1)$ is a single root. What are the details here?","['algebraic-geometry', 'elliptic-curves', 'projective-space']"
4893962,Hilbert space map with closed image respects closed subspaces?,"Suppose $X$ and $Y$ are Hilbert spaces, and $T:X \rightarrow Y$ is a bounded linear operator with closed image. Is it true that for every closed subspace $M \subseteq X$ , that $T(M)$ will be closed in $Y$ ? I’m struggling to prove this is true, but I’m also failing to find a counter example. The best idea I have had in trying to prove the result is to use the following lemma: Lemma: Let $L: A \rightarrow B$ be a bounded linear map of Hilbert spaces. Then $\text{im}(L) \subseteq B$ is closed if and only if there is a real constant $c > 0$ such that $c ||a|| \leq ||La||$ for all $a \in (\ker L)^\perp$ . Viewing $M$ as a Hilbert space, we can try to prove that the restriction $T \rvert_M: M \rightarrow Y$ has closed image using the lemma. The issue I’m having with this approach is I don’t see any guarantee that the orthogonal complement of $\ker(T_M)$ (inside of of $M$ ) is contained in the orthogonal complement of $T$ (inside of $X$ ). Any help would be appreciated! And if there is a counter example, is there a nice characterization of maps that send closed subspaces to closed subspaces?","['hilbert-spaces', 'linear-algebra', 'functional-analysis']"
4893963,Why does the standard deviation have all the properties it does? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 months ago . Improve this question Currently in a probability class and am having a hard time wrapping my head around the standard deviation. In many ways it feels ""made up"" to me; let me explain: $$\sqrt{a^2 + b^2} \neq a + b$$ So unless it's used relative to other standard deviations, I feel like it wouldn't inherently mean anything. By that I mean, measurements like meters/inches have some grounding in ""reality."" Knowing something has a length of 3 m is useful information even if I don't have any other lengths in meters to compare it to. But when I see that a distribution has a SD of 2, that information only translates to my understanding of how ""spread"" the distribution is when I compare it to other distributions with different SDs. I think, ""Oh distribution 1 is pretty spread out and it has an SD of ___ so this other SD must mean the corresponding distribution 2 is less/more spread out."" Is this just how it is? Are some units of measurement just less intuitive and derive meaning from comparison? To me, it would make more sense to take the mean of the absolute values of all the deviations from the mean, but my professor told me that SD just happens to be more useful. I don't doubt this, and I am aware of the many properties that makes SD more useful, but my question is why? What causes SD to have those properties? For example, what causes most of the points in a normal distribution to land within 2 SD of the mean? It can't be just a coincidence or happenstance; there must be a reason. Any help would be appreciated. Thanks in advance!","['intuition', 'statistics', 'standard-deviation', 'probability']"
4894044,Injectivity of the canonical map to the double dual,"Let $V$ be an arbitrary vector space (possibly infinite-dimensional) and $V^{**}$ be its double dual. Define $$J: V \rightarrow V^{**}\\
\hspace{8mm} x \rightarrow \phi(x),$$ where $\phi(x): V^{*} \rightarrow \mathbb{R}$ with $\phi(x)(f) = f(x)$ . I want to show $J$ is injective. Most discussions I've seen about this, e.g. this one , I kept seeing the same thing being said: ""use the Hahn-Banach Theorem"" . I honestly did not know about his theorem until very recently, and I do not know exactly how to apply it in this situation. I was wondering, why is the existence of a Hamel basis for vector spaces not used instead? If we have $$\phi(x)(f) = \phi(y)(f),\forall f \in V^{*},$$ then, by applying the projections to the equality above, $x$ and $y$ must have the same representation in the given basis, no? We would then be able to conclude that $x=y$ .","['linear-algebra', 'functional-analysis', 'dual-spaces']"
4894052,$1-$Lipschitz function of two variables as limit of separable functions,"Let $X,Y$ be compact metric spaces. Suppose $m: X \times Y \to [0,1]$ is $1-$ Lipschitz, where $X \times Y$ is given the taxicab metric $d((x,y),(x',y'))=d_X(x,x')+d_Y(y,y')$ . Can one write $m$ as a limit of $1-$ Lipschitz functions $m_n$ such that $m_n$ is a convex combination of $1-$ Lipschitz  functions of the form $h \cdot k$ , with $h :X \to [0,1]$ and $k:Y \to [0,1]$ both $1-$ Lipschitz? It is clear that $m$ can be written as a limit of linear combinations of products of Lipschitz functions, by Stone-Weierstrass and the density of Lipschitz functions. It is also clear that the limit of the Lipschitz constants of such a sequence must converge to $1$ . It is clear that such a sequence can be picked so that they are $[0,1]$ valued functions, although this doesn't imply that they a convex combination. I cannot see whether you can argue from that that it is sufficient to restrict to a sequence of $1$ -Lipschitz functions, and whether you can restrict to convex combinations. I would also be interested if its instead possible to restrict to linear combinations $\sum_i \lambda_i h_i \cdot k_i$ with $\sum_i |\lambda_i| \leq 1$ If this result were true, it would in particular allow you to calculate the Kantorovich distance between joint probability measures using a supremum over a smaller class of 1-Lipschitz functions, and this is my motivation.","['metric-spaces', 'lipschitz-functions', 'analysis', 'functional-analysis', 'uniform-convergence']"
4894078,Generic intersection of hyperplanes,"Let $V$ be a finite dimensional real vector space and $V_1,\cdots,V_n$ be subspaces of $V$ . My main question is Prove that $$f:\bigoplus_{i=1}^n V_i\to V^n/\Delta$$ is surjective, where the map is induced by inclusions $V_i\to V$ and $\Delta$ is the diagonal $\Delta= \{ (v,\cdots,v),v\in V \} \in V^n$ , if and only if $V$ have a direct sum decomposition $V=L\oplus L_1\oplus\cdots\oplus L_n$ (the summands are allowed to be $0$ ) so that $$V_i=L\oplus L_1\oplus\cdots L_{i-1}\oplus L_{i+1}\oplus\cdots L_n$$ Some Backgrounds: When $f$ is surjective, we also say $\{V_i\}$ intersect generically, since this condition is equivalent to "" given arbitrary vectors $a_i\in V$ , we can find $v_i\in V_i$ and $v\in V$ , so that $$v_i+a_i=v$$ This means if we move the planes $\{V_i\}$ a little $a_i$ , they still intersect at some point $v$ . More backgrounds(Not needed to solving this linear algebra problem): manifold version In many interesting cases in differential geometry, we need to compactify configuration spaces, and they are done by blowing up submanifolds successively. It turns out that if these submanifolds intersect generically, then the order you perform to blow up these submanifolds doesn't effect the result of compactification ( Dylan Thurston's thesis ). This question is local linearized version.","['abstract-algebra', 'linear-algebra', 'differential-geometry']"
4894088,Implicit differentiation with 3 codependent variables,"I have been going over some exercises on multivariable calculus and have stumbled across the following: Suppose we have some implicit multivariable function of x,y,z: $$F(x,y,z)=0$$ such that $$x=h(y,z), y=g(x,z), z=f(x,y).$$ show that $$ \frac{\partial z}{\partial x}  \frac{\partial x}{\partial y}  \frac{\partial y}{\partial z} = -1$$ Whenever I see the solutions presented to this, they immediately just cite the theorem for implicit differentiation in 3 variables, even though this theorem seems to hinge on the fact that we can, for example, when considering $\frac{dF}{dx}$ , let $\frac{dy}{dx}$ be equal to zero since $y$ is not a function of $x$ , when clearly it is in this specific case. I am slightly confused by this - is this a consequence of us reducing our problem to one where we need only consider $x,z$ since $y$ wholly depends on them, meaning that we need only consider these two variables instead of all three, bringing us back to one with just 2 independent variables?",['multivariable-calculus']
4894098,"Acrobatic Calculus of ""Physicists"" in deriving the Taylor series for $e^x$","While looking for mathematics videos, I stumbled upon this video , dubbed ""S-Tier Acrobatic Calculus that Physicists do"" of which I was pleasantly surprised by what is presented in the video. The video center around the derivation of the Taylor's expansion for $e^x$ , of which we have the general form as $${\displaystyle e^{x}=1+x+{\frac {x^{2}}{2!}}+{\frac {x^{3}}{3!}}+{\frac {x^{4}}{4!}}+\cdots =\sum^{\infty}_{i=0} \frac{x^i}{i!}}$$ Here's how the video proceed to come up with the result (in a pretty carefree tone): Let's say that I forgot the Taylor's expansion of $e^x$ , but I remember that it is the only function whose derivative ${e^{x} }'$ and integral $\int e^x$ are both itself. Let's start with the integration form: $$e^x = \int e^x \: dx$$ as someone who is really good at calculus, I don't care about the lower bound of upper bound. In fact, I don't care about the $dx$ , so let's remove it: $$e^x = \int e^x$$ Much better. Now I can move everything to the left: $$e^{x}-\int e^{x} = 0 $$ which I now have 1 minus integration sign, that is $$\left( 1-\int    \right) e^{x}=0$$ and I am also free to left multiply the inverse: $$e^{x}=\left( 1-\int  \right)^{-1}\cdot 0$$ Now, we notice that $$(1-x)^{-1}=\frac{1}{1-x}=1+x+x^{2}+x^{3}+\dots$$ This looks familiar, doesn't it? ""As someone with a physics background, I can't resist expanding anything that is expendable"", then for our case $$\frac{1}{1-\int   } = 1 + \left( \int \right) + \left(\int\right)^{2}+\left(\int \right)^{3}+\dots$$ This looks very promising. Let's start the calculation: $$e^{x}= \left(1+\int+\left(\int\right)^{2}+\left(\int\right)^{3}+\dots\right)\cdot 0$$ The first term is zero. But the second term? Is it also zero? No, because we don't have $dx$ . That means it is not the normal integral that takes zero as intergrand, it is the operation integral $$\int \left( 0  \right) $$ that applies on 0. So what's the result? It is easier to think from the opposite side. Its derivative is 0, therefore it is a constant. As someone with a physics background, that constant must be 1: $$e^{x}=0 + 1+ \left(+\left(\int\right)^{2}+\left(\int\right)^{3}+\dots\right)\cdot 0$$ Similarly, I am looking for the value whose derivative is one: it's $x$ . Continuing this, for example value whose derivative is $x$ , I found that it is $\frac{x^{2}}{2!}$ . Then this chain of logic continues: $$e^{x}=0+1+x+{\frac {x^{2}}{2!}}+{\frac {x^{3}}{3!}}+{\frac {x^{4}}{4!}}+\cdots$$ which recovers the Taylor's expansion of $e^{x}$ . This seems to me, is between absurdity, but also logically reasonable. But what I found interesting is the remark of the OP: At the end of the video, I want to make some clarifications. With proper definition everything I have done so far is legitimate. Which is quite interesting considering which foundation and definitions would make this possible. I do not have adequate knowledge to be able to figure this out. I also read a person said that This actually has rigorous justifications if you work in an appropriate function space. For example, if you work in $L^{2}$ , the norm of the integral operator is less than one, so the expansion of the operator in the series is justified. In another person's words, he pointed it to operation theory, and the others about functional analysis and holomorphic functional calculus. However, I lack the knowledge to fully clarify these claims or explanation for myself. What is going on for this example, and what is the ""appropriate and proper"" definition and foundation for such derivation?","['calculus', 'analysis']"
4894165,Find the area of the region enclosed by $\frac{\sin x}{\sin y}=\frac{\sin x+\sin y}{\sin(x+y)}$ and the $x$-axis.,"Here is the graph of $\dfrac{\sin x}{\sin y}=\dfrac{\sin x+\sin y}{\sin(x+y)}$ . Find the area of the region enclosed by the curve and the $x$ -axis, from $x=0$ to $x=\pi$ . Where the question came from, and why I think the answer is $\dfrac{\pi^2}{8}$ This question arose when I asked myself: ""A triangle's vertices are three uniformly random points on a circle. The side lengths are, in random order, $a,b,c$ . The triangle inequality tells us that $P(a+b<c)=0$ . But  what is $P\left(a+b<\left(\frac{a}{b}\right)c\right)$ , given that $\frac{a}{b}>1$ ?"" (These kinds of questions often have rational probabilities, for example $P(ab<c^2)=\frac35$ as shown here ; $P(\frac1a+\frac1b<\frac1c)=\frac15$ as shown here ; for a unit circle $P(ab<c)=\frac12$ as shown here .) I assumed that the circle is centred at the origin, and the vertices of the triangle are: $A\space(\cos(-2y),\sin(-2y))$ where $0\le y<\pi$ $B\space(\cos(2x),\sin(2x))$ where $0\le x<\pi$ $C\space(1,0)$ I let: $a=BC=2\sin x$ $b=AC=2\sin y$ $c=AB=|2\sin(x+y)|$ By symmetry, we only need to consider the region where $x+y<\pi$ , so we can drop the modulus signs. $P\left(a+b<\left(\frac{a}{b}\right)c\right)=P\left(\color{red}{\dfrac{\sin x}{\sin y}>\dfrac{\sin x+\sin y}{\sin(x+y)}}\right)$ Then I tried to express $y$ in terms of $x$ , or $x$ in terms of $y$ (so that I could get an integral), but I failed. Wolfram is not helpful . I rotated the graph $45^\circ$ by letting $x\to x-y$ and $y\to x+y$ , which gives $\frac{\tan x}{\tan y}=\frac{\cos x+\cos y}{\cos x-\cos y}$ , but I still couldn't isolate $x$ or $y$ . Wolfram is still not helpful . A simulation of $10^7$ random triangles yielded a proportion of $0.49998$ satisfying $a+b<\frac{ac}{b}$ given that $\frac{a}{b}>1$ , suggesting that the probability is $\frac12$ . If so, then the area of the shaded region should be $\frac12\times\frac{\pi^2}{4}=\color{red}{\frac{\pi^2}{8}}\approx1.2337$ . (Now I am more interested in this area question, than the probability question.) Update 1 Using the Wolfram link in @Masd's answer, I made a desmos graph of $y$ as a function of $x$ . It looks just like the original curve, except it has gaps that I have been unable to bridge. Here is the link to my desmos graph. If someone can bridge the gaps, then we can use desmos to integrate from $x=0$ to $x=\pi$ , and see if the area could be $\frac{\pi^2}{8}$ . Update 2 @Masd's answer now reports a numerical integral of $1.23370055014$ , which matches $\frac{\pi^2}{8}$ to $11$ decimal places. Is there a proof that the area is exactly $\frac{\pi^2}{8}$ ?","['integration', 'definite-integrals', 'geometric-probability', 'triangles', 'probability']"
4894214,Paramterizing the surface on the intersection of $x+z=a$ and interior of $x^2+y^2+z^2=a^2$,"So I am trying to verify Stokes' theorem for $\vec{F}=y\hat{i}+z\hat{j}+x\hat{k}$ where the curve $C$ is on the intersection of $x+z=a$ and $x^2+y^2+z^2=a^2$ . Solving these equations yields the curve $2(x-a/2)^2+y^2=a^2/2$ which I have parametrized as $x=\frac{a}{2}(1+\cos\theta)$ , $y=\frac{a}{\sqrt 2}\sin\theta$ , $z=\frac{a}{2}(1-\cos\theta)$ with $0\le\theta<2\pi$ . The line integral $\int \vec{F}\cdot d\vec{r}$ now evaluates to $-\frac{a^2\pi}{\sqrt{2}}$ . Now I want to take the surface as the intersection of $x+z=a$ and the interior of $x^2+y^2+z^2=a^2$ and prove that the surface integral $\int(\nabla\times \vec{F})\cdot d\vec{s}  $ evaluates to the same value. For this reason, I want the parametric equation of this disk. But what can it be?","['multivariable-calculus', 'stokes-theorem', 'parametrization', 'vector-analysis']"
4894234,Why do we need to prove extension lemmas?,"I'm studying Lee's smooth manifolds, and time and time again, whether it be for functions in $\mathcal{C}^\infty(M)$ where $M$ is a manifold, or for vector fields over $M$ , or for sections of vector bundles, he proves extension lemmas which say that if we can define such an object on a subset of the entire space, then there must exist some other object on the entire space which has a restriction equal to the initial object. Intuitively I don't see why this requires proof, are there any instances where this fails? Does there for example exist an object defined on a subset for which there does not exist an object defined on an entire set with an equal restriction?","['manifolds', 'differential-geometry']"
4894243,Upper and lower bounds on the number of solutions to the equation $\frac{\pi}{4} = \sum_{k=1}^{n} c_{k} \arctan \left(\frac{1}{x_{k}} \right) $,"Background The Norwegian mathematician and astronomer Carl Størmer did important work on the equation $$\frac{\pi}{4} = \sum_{k=1}^{n} c_{k} \arctan \left(\frac{1}{x_{k}}\right), \label{1}\tag{1} $$ where $c_{k} \in \mathbb{Z} \setminus \{0\} $ and $x_{k} \in \mathbb{N}_{> 0}$ for all $k$ . Here, the numbers of the form $\arctan \left(\frac{1}{x_{k}}\right)$ are called the Gregory numbers . For $n=2$ , there is the original formula due to Machin: $$ \frac{\pi}{4} = 4 \arctan\left(\frac{1}{5}\right) - \arctan\left(\frac{1}{239}\right). \tag{2}\label{2} $$ Størmer found three additional solutions to \eqref{1}, and established there are no more than four solutions when $n=2$ . He proceeded by looking for solutions in the case when $n=3$ , and found 103 cases. An example of such a solution is $$\frac{\pi}{4} =  \arctan \left( \frac12 \right) + \arctan \left( \frac15 \right) + \arctan \left( \frac18 \right) .$$ However, he couldn't show there are no more solutions. As Nimbran describes in the following 2010 paper (PDF), two additional solutions were found by J. M. Wrench in 1938, and one more was found by Hwan Chien-lih in 1993. It appears to be an open problem whether these are all solutions. For $n=4$ , Størmer also obtained solutions. For instance, we have $$ \frac{\pi}{4} = 44 \arctan\left(\frac{1}{57}\right) + 7 \arctan\left(\frac{1}{239}\right) - 12 \arctan\left(\frac{1}{682}\right) + 24\arctan\left(\frac{1}{12943}\right) .\tag{3}\label{3}$$ (See p. 5 of the paper by Nimbran.) Again, the total number of solutions appears to be unknown. Let $f(n)$ be the number of solutions to \eqref{1} for $n \geq 1$ . We have: $n$ $1$ $2$ $3$ $4$ $f(n)$ $1$ $4$ $\geq 106 $ ? Questions Are there any upper and lower bounds for $f(n)$ ? Can any results on the asymptotic growth rate of $f(\cdot)$ be established?","['asymptotics', 'reference-request', 'upper-lower-bounds', 'trigonometry', 'open-problem']"
4894273,Question regarding inverse of exponential function,"This question is in the context of the following problem Find the inverse of the function $$f:(-\infty, 1] \rightarrow \Biggr[\frac{1}{2}, \infty\Biggr], \text{ where } f(x) = 2^{x(x-2)}$$ I proceeded for the solution as follows $$y = 2^{x(x-2)}$$ $$x^2 - 2x = \log_2y$$ $$x^2 - 2x - \log_2y = 0$$ $$x = 1 \pm \sqrt{1 + \log_2 y}$$ After this however my and the books solutions from which the question is taken diverged, i went on to create a piecewise function for both cases $x$ negative or positive, however the book did not do so and the answer for this is in the book is $$f^{-1}(x) = 1 - \sqrt{1 + \log_2x}$$ Is their any reason to do so, any help would be appreciated!","['functions', 'exponential-function', 'inverse-function']"
4894280,Multivariate chain rule (again) for differential forms,"I know there has been many questions on multivariate chain rules already, but I read through a few of them and I'm still confused how to correctly do my case, so I will ask again. I'm trying to prove that for an exact $1$ -form $\alpha = \sum_i f_i dx_i$ , the $0$ -form $g(\mathbf{x}) = \sum_j x_j\left(\int_0^1 f_j(t\mathbf{x})dt\right)$ is an antiderivative i.e. $dg = \alpha$ . By definition, it suffices to prove that $\frac{\partial g}{\partial x_i} = f_i$ . So I get \begin{align*}
\frac{\partial g}{\partial x_i}
&= \left(\int_0^1 f_i(t\mathbf{x})dt\right) + \left(\sum_j x_j \left(\int_0^1 \frac{\partial(f_j(t\mathbf{x}))}{\partial x_i} \right)\right) \\
&= \left(\int_0^1 f_i(t\mathbf{x})dt\right) + \left(\sum_j x_j \left(\int_0^1 \color{red}{\sum_k \frac{\partial f_j}{\partial x_k} \cdot \frac{\partial (tx_k)}{\partial x_i} } \right)\right) \\
&= \int_0^1 f_i(t\mathbf{x})dt + \sum_j x_j \int_0^1 \frac{\partial f_j}{\partial x_i}(t\mathbf{x}) \cdot tdt
\end{align*} First question is whether the chain rule application at the red part is correct. The second question is how do I proceed from here? Numerical example I worked through: take $\alpha = x^2y dx + x dy$ . Then, \begin{align*}
\frac{\partial g}{\partial x}
&= \int_0^1 (x^2y)\big|_{x=tx,y=ty} dt + x\int_0^1 \left(\frac{\partial(x^2y)}{\partial x}\right)\big|_{x=tx,y=ty} \cdot tdt + y\int_0^1 \left(\frac{\partial(x^2y)}{\partial y}\right)\big|_{x=tx,y=ty} \cdot tdt \\
&= \int_0^1 t^3x^2y dt + x \int_0^1 (2t^2xy) \cdot tdt + y \int_0^1 (t^2x^2) \cdot tdt
\end{align*} But I don't get much insight on how to simplify this in general. I guess when $f_i$ is polynomial this is easier but about in general..","['multivariable-calculus', 'differential-forms']"
4894365,"Prime powers; are there more powers of 2, or powers of higher primes, smaller than N?","Given the set https://oeis.org/A025475 with the prime powers, excluding the primes themselves. Let's define two subsets of A025475. $A=\{4,8,16,32,\dots\}$ , with all powers of 2 ( $2^k, k>1$ ) smaller than $N$ . And $B=\{1,9,25,27,49,\dots\}$ with all the other powers ( $p^k, k=0$ or $k>1$ , $p$ is a prime larger than 2) smaller than $N$ . I'm interested in de cardinality of $A$ and $B$ . It's easy to see, that there are more elements in $B$ than there are in $A$ , for, let's say $N=1.000$ . In this case $|A| = 8$ and $|B|=18$ . But, is $|B|>|A|$ true for all values $N>1.000$ ? Intuitively not, but I can't prove it","['number-theory', 'prime-numbers']"
4894389,Showing that $ f(x) = 0 $ if the triple integral $\frac{f(x)}{(1+x^2+y^2+z^2)} $ converges,"I'm told the following integral converges, and I'm asked to prove that $ f = 0 $ , when it's a continuous function $ f: \mathbb{R} \to \mathbb{R}$ . $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{f(x)}{(1+x^2+y^2+z^2)}\,dx\,dy\,dz.$ My attempt: Suppose that $ f(x_0) = M > 0$ . Then there's a small enough interval such that $ f(x) > M/2, \forall x \in(x_0 - \delta, x_0 + \delta).$ Then at that interval, $\int_{x_0 - \delta}^{x_0 + \delta}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{f(x)}{(1+x^2+y^2+z^2)}\, dx\,dy\,dz > \int_{x_0 - \delta}^{x_0 +\delta}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{M/2}{(1+x^2+y^2+z^2)}\,dx\,dy\,dz$ . If I can show that the double integral that corresponds to $y, z$ diverges, so does our original integral, and that is a contradiction - so $f = 0$ for all $x$ . I want to change to spherical coordinates, $x = r \cos a \cos b, y = r \cos a \cos b$ , $z = r \sin a$ . Then we have a triple integral, and one of those integrals is of the form $\int_{0}^{\infty}\frac{r^2}{1+ r^2} dr $ , and since its limit at $\infty$ is 1, our triple integral in this interval diverges - but we know that it must converge, so we reach a contradiction. Is this attempt okay? What is not correct and needs refinement?","['integration', 'multivariable-calculus', 'calculus']"
4894436,"What is the ""higher cohomology"" version of the Eudoxus reals?","The ""Eudoxus reals"" are one way to construct $\mathbb{R}$ directly from the integers. A full account is given by Arthan ; here is the short version: A function $f: \mathbb{Z} \to \mathbb{Z}$ is an ""almost homomorphism"" if the function $d_f: \mathbb{Z} \times \mathbb{Z} \to \mathbb{Z}$ defined by $d_f(a,b) = f(a+b) - f(a) - f(b)$ has finite/bounded image. The set of almost homomorphisms forms a group under pointwise addition, and the quotient of this group by the subgroup of functions $f$ with bounded image is isomorphic to the real numbers. This same construction can be applied to functions $f: G \to H$ where $G$ and $H$ are abelian groups. If they are both finitely generated, then Arthan shows that the resulting group is isomorphic to $\operatorname{Hom}_{\text{Ab}}(G, H) \otimes \mathbb{R}$ . (*) There is a ""higher cohomology"" version of this construction. For example, say that a function $f: \mathbb{Z} \times \mathbb{Z} \to \mathbb{Z}$ is an ""almost 2-cocycle"" if the function $d_f: \mathbb{Z}^3 \to \mathbb{Z}$ defined by $d_f(a,b,c) = f(b,c) - f(a+b,c) + f(a,b+c) - f(a,b)$ has finite image. These form a group under addition. The subgroup of ""almost 2-coboundaries"" is the subgroup generated by the true 2-coboundaries (functions of the form $f(a,b) = g(a+b) - g(a) - g(b)$ for some function $g: \mathbb{Z} \to \mathbb{Z}$ ) and by the functions which have finite image. My question is: What is the quotient of the group of almost 2-cocyles by the subgroup of almost 2-coboundaries? (Call this quotient $H^2_{\text{almost}}(\mathbb{Z}, \mathbb{Z})$ .) More generally, what do we get if we repeat this construction for arbitrary abelian groups $G, H$ and consider almost $n$ -cocycles and almost $n$ -coboundaries? In light of (*), I would expect the answer to be some kind of higher Ext group tensored with $\mathbb{R}$ . In particular, since $H^2(\mathbb{Z}, \mathbb{Z}) = 0$ , I would expect that $H^2_{\text{almost}}(\mathbb{Z}, \mathbb{Z}) = 0$ as well. Therefore, a more specific question is: Is the implication $H^2(\mathbb{Z}, \mathbb{Z}) = 0 \Rightarrow H^2_{\text{almost}}(\mathbb{Z}, \mathbb{Z}) = 0$ true, and if so, is there a concrete way to see it?","['real-numbers', 'homological-algebra', 'abstract-algebra', 'group-cohomology', 'homology-cohomology']"
4894465,Distribution of coin tossing - Possible error in solutions?,"I've been studying probability theory through the book ""An Introduction to Probability Theory and Mathematical Statistics"" by V.K. Rohatgi. I've just reached the chapter on random variables, and I either found a mistake in the exercise solutions or I've done one myself. 
The exercise in question asks what the distribution function of a random variable X is. Here, X is the random variable that gives out the number of heads when you throw a fair coin 3 times. What I wrote down on paper is that the distribution gives out $0$ if $x<0$ , $\frac{1}{8}$ if $0\leq x < 1$ , $\frac{1}{2}$ if $1\leq x < 2$ , $\frac{7}{8}$ if $2\leq x < 3$ and $1$ if $x \geq 3$ . The author agrees with me in everything except for the value when $2\leq x < 3$ , which he says is $\frac{5}{8}$ . 
I have no idea why he says it's this value. Can you help me, please?","['probability-theory', 'random-variables']"
4894511,Computing $\int_{0}^{\infty}\frac{x^{1/2} \log x}{x^2 + 1}dx$ and $\int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx$,"I am trying to compute $$\displaystyle\int_{0}^{\infty}\frac{x^{1/2} \log x}{x^2 + 1}dx$$ and $$\displaystyle\int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx$$ via the computation of $$\displaystyle\oint_C \frac{z^{1/2}\log z}{z^2 + 1}dz$$ where $C$ is the keyhole contour with branch cut along the positive real axis. The poles of the function are at $\pm i$ and so by the Residue Theorem I found $$\displaystyle\oint_C \frac{z^{1/2}\log z}{z^2 + 1}dz = \frac{\pi^2}{\sqrt{2}}$$ .The integrals on $C_R, C_{\varepsilon}$ vanish as $R\to\infty, \varepsilon\to 0$ . Writing $L_1$ for the line portion above the real axis, I found $$\displaystyle\int_{L_1} \frac{z^{1/2}\log z}{z^2 + 1}dz = \displaystyle\int_{0}^{\infty}\frac{x^{1/2}\log x}{x^2 + 1}dx$$ and somewhat similarly for $L_2$ , the line portion below the real axis, $$\displaystyle\int_{L_2} \frac{z^{1/2}\log z}{z^2 + 1}dz = -\int_{0}^{\infty}\frac{x^{1/2}\log x}{x^2 + 1}dx - 2\pi i \int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx$$ . However, combining all information together implies $$\displaystyle\int_{0}^{\infty}\frac{x^{1/2} \log x}{x^2 + 1}dx = \frac{\pi^2}{2\sqrt2}$$ and $$\displaystyle\int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx=0$$ . The first of these answers is correct, however I should have found $$\displaystyle\int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx = \frac{\pi}{\sqrt{2}}$$ and am unsure as to where I've gone wrong in my calculations. Any help would be greatly appreciated.","['integration', 'complex-analysis', 'definite-integrals']"
4894523,How to find an acute angle of a right triangle inscribed in a square?,"Working on Daniel J. Velleman. (2017). "" Calculus: A Rigorous First Course "" (p. 66) My question is focused on the purple circle on the image above. The solution given by the author is $\alpha$ ; however, I can't find out how to work out the solution by myself. I have tried thinking in terms of angles and perpendicular lines, but the relationship between the original $\alpha$ and the solution given is not clear to me. How can I approach this ?","['trigonometry', 'angle', 'triangles']"
4894548,Question regarding functional equations involving series,"This question is regarding the following problem. Let $f(x) = x + f(x-1)$ for $\forall x \in R$ . If $f(0) = 1$ , find $f(100)$ I found out the following pattern in the question $$f(1) = 1 + f(0) = 2$$ $$f(2) = 2 + f(1) = 4$$ $$f(3) = 3 + f(2) = 7$$ So the series formed is $1, 2, 4, 7, \ldots$ Now, $$S = 1 + 2 + 4 + 7 + \ldots + f(n-1)$$ $$S = 0 + 1 + 2 + 4 + \ldots + f(n-2) + f(n-1)$$ Subtracting these two we get $$f(n-1) = (1 + 1 + 2 + 3 + 4 + 5 + 6 + \ldots + n \text{ terms }$$ Now, $$f(n-1) = 1 + \frac{n(n+1)}{2}$$ However in my textbook it is given tat $$f(n-1) = \frac{n(n+1)}{2}$$ Is this wrong? or is their a reason for this any help would be highly appreciated!","['functions', 'sequences-and-series']"
4894597,Using the binomial expansion to find derivative formulas,"While differentation the function $f(x) = (1-e^x)^{5}$ , I realize one can use the Binomial Theorem to find formulas for nth derivative: The idea is as follows. Let $f(x) = (1-e^x)^n$ . We have $$ f(x) = \sum_{i=0}^n {n \choose i} (-1)^i e^{ix} $$ So, evidently, the kth derivative is $$ f^{(k)}(x) = \sum i^k (-1)^i {n \choose i} e^{ix} $$ In particular, one has $$ f^{(k)}(0) = \sum i^k (-1)^k {n \choose i} $$ Which seems to be a pretty neat formula. My question is whether anyone has more literature into this identities. In other words, can someone pinpoint me to references that discuss these ideas into more detail. Looking to explore things as $f(x) = (1-g(x))^n$","['calculus', 'combinatorics', 'reference-request']"
4894622,Limit and System of Equation,"if $\;f(x) = ax^2 + bx + c\;$ intercept at y-axis point $(0,1)$ and $\lim_{x\to 1} \frac{f(x)}{x-1} = -4$ , then the value of $\;\frac{b+c}{a}\;$ is ... So, I tried by plug in $x = 0\;$ and $\;y = 0$ . $$\;f(0) = a(0)^2 + b(0) + c\;$$ $$c = 1$$ Then, using derivative to evaluate the limit $$\lim_{x\to 1} \;\frac{f(x)}{x-1} = -4$$ $$\frac{2ax + b}{1} = -4$$ $$2a + b = -4$$ Is $\;2a + b = -4\;$ and $\;c = 1\;$ equation enough to form $\;\frac{b+c}{a}\;$ equation? or we have to get the $a$ and $b$ value too? But, in what way we could get the $a$ and $b$ value? Edit (after @Khosrotash'answer) So, because of $$\lim_{x\to 1} \;\frac{f(x)}{x-1} = -4$$ is actually $\;0/0$ $$\lim_{x\to 1} \;\frac{f(1)}{1-1} = \frac{0}{0}$$ $$f(1) = 0$$ $$a + b + c = 0$$ $$a + b + 1 = 0$$ $$a + b = -1$$ By eliminating the equation, I get $a = -3$ , $b = 2$ , and $c = 1$ . So, the value of $\;\frac{b+c}{a}\;$ is equal to $-1$ . But I'm still confused, how do you know that $\lim_{x→1}\frac{f(x)}{x−1}=−4$ is $0/0$ not $∞/∞$ ? is it because the $x−1$ value is $0$ , so it can be assume that $\lim_{x→1}\frac{f(x)}{x−1}=−4$ is $0/0$ ? –","['limits', 'systems-of-equations', 'solution-verification']"
4894657,If $\lim\limits_{n \to \infty}\frac{1}{n}\sum\limits_{k=1}^n k\ln \left(\frac{n^2+(k-1)^2}{n^2+k^2}\right)$,"If $\lim\limits_{n \to \infty}\frac{1}{n}\sum\limits_{k=1}^n k\ln \left(\frac{n^2+(k-1)^2}{n^2+k^2}\right)$ exists and equal to $l$ , find $l$ . I don't know how to approach this probelm. I thought about sandwich theorem and Integration as limit of sum but failed becuase as $n$ approaches $\infty$ quantity written in logarithm tends to $1$ which make $\ln \left(\frac{n^2+(k-1)^2}{n^2+k^2}\right)=0$ Please Help me solve this problem","['limits', 'calculus', 'definite-integrals']"
4894734,"If R is a finite ring without unity, then every element of R must be zero divisor?","I know two results-
"" $R$ is a finite ring with unity, each element of $R$ is a unit or a zero divisor"" ""If $R$ is a finite ring without zero divisors, $R$ must have a unity"" The commutativity does not require in both case as we can consider both $aR$ and $Ra$ Now, my question is, if $R$ is a finite ring without unity, then every element of $R$ must be zero divisor? I am talking about finite non-trivial rings only. In $2\mathbb{Z}$ , we find no elements as unit, nor a zero divisor This is simply not possible in finite ring. In there is no zero divisor, there comes unity, and therefore every element must be a unit If there is atleast one zero divisor, then we already find one zero divisor. Now the most doubtful question arises when it is a ring with zero divisors, and without unity, can we find a ring where some elements are zero divisors and some are nothing (since, no unity, they can't be unit, they are also not zero divisors as per my statement) Just like in $M_2(2\mathbb{Z})$ , we find zero divisors, and some elements who are not unit nor a zero divisor But failed to find such an example in case of finite ring Every element is becoming zero divisor in this case (left or right side)","['ring-theory', 'abstract-algebra']"
4894759,Is every compact set in the metric topology closed in the minimal topology containing the closed balls as closed sets?,"Let $X$ be a set and $d$ be a metric. I'm interested in the minimal topology $\mathcal{T}$ over $X$ such that every set of the form $\{y\in X:d(y,x)\le r\}$ , with $x\in X$ and $r>0$ , is closed. In other words, I consider the subbase $\{y\in X:d(y,x)>r\}$ , with $x\in X$ and $r>0$ . As the comment pointed out, allowing $r=0$ or not does not make a difference since every $\{x\}$ is the intersection of all closed balls centered at $x$ . An easiest example would be: if $d$ is the discrete metric, then the closed balls are either singletons or the whole $X$ , so $\mathcal{T}$ is the cofinite topology. Another easy example would be a bounded interval equipped with the usual metric, where $\mathcal{T}$ and the metric topology coincide because the complement of an open ball is always a closed ball of the union of two closed balls. Unfortunately, these are the only examples where I could determine explicitly the topology $\mathcal{T}$ . I do not hope for a complete characterization of this topology (I would glad if it does exist!), but I was wondering if this topology has been studied somewhere. My question is: Is every compact set in the metric topology closed in $\mathcal{T}$ ? If not, is at least this result correct for $\mathbb{R}$ equipped with the usual metric, so we are interested in the minimal topology containing every bounded closed interval as a closed set?","['general-topology', 'metric-spaces']"
4894783,Is there a dimensional multiplication operation? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 months ago . Improve this question When expressing numbers with any unit, we know this. We can multiply and divide numbers with different types of units, but we cannot add or compare them. From Terry Tao's 2012 blog post ""A mathematical formalisation of dimensional analysis"" (via terrytao.wordpress.com) (paragraph 6): There is however one important limitation to the ability to manipulate “dimensionful” quantities as if they were numbers: one is not supposed to add, subtract, or compare two physical quantities if they have different dimensions, although it is acceptable to multiply or divide two such quantities. For instance, if ${m}$ is a mass (having the units ${M}$ ) and ${v}$ is a speed (having the units ${LT^{-1}}$ ), then it is physically “legitimate” to form an expression such as ${\frac{1}{2} mv^2}$ , but not an expression such as ${m+v}$ or ${m-v}$ ; in a similar spirit, statements such as ${m=v}$ or ${m\geq v}$ are physically meaningless. While expressing numbers with objects, if we assign units to objects, we can analyze the following operations. As you can see above, we cannot collect 1 apple and 1 pear. Now I want to mention this: When performing operations on integers, considering them as vectors in one dimension and performing operations works perfectly. If we consider integers in one dimension, multiplying two numbers that do not have any units is actually repetitive addition. Or multiplying a number with a unit in a dimension by a number without a unit is also repetitive addition. However, while studying geometry, I noticed something. It is meaningless to multiply numbers with different units in one dimension. Also from Tao's post (paragraph 10): As mentioned before, it is then geometrically natural to multiply two lengths to form an area, by taking a rectangle whose line segments have the stated lengths, and using the area of that rectangle as a product. This geometric picture works well for units such as length and volume that have a spatial geometric interpretation, but it is less clear how to apply it for more general units. For instance, it does not seem geometrically natural (or, for that matter, conceptually helpful) to envision the equation ${E=mc^2}$ as the assertion that the energy ${E}$ is the volume of a rectangular box whose height is the mass ${m}$ and whose length and width is given by the speed of light ${c}$ . We can multiply or divide numbers that have a unit. However, multiplying $4m$ length by $4m$ length in one dimension is meaningless. But when calculating the area of ​​a square, we can multiply the length of $4m$ in two different dimensions $(4m\cdot 4m=16m^{2})$ . My question: We can approach numbers in many different ways. They can act as one-dimensional vectors in relation to objects (for example, apples, pears), or in the number direction. We can even manipulate units like numbers. So, does dimensional multiplication really exist ?","['real-numbers', 'integers', 'linear-algebra', 'geometry']"
4894889,What coordinate substitution should I perform to evaluate this triple integral?,"I am trying to evaluate the following triple integral: \begin{equation}
\int_{-1}^1 \int_{-\sqrt{4-4x^2}}^{\sqrt{4-4x^2}} \int_{\sqrt{4x^2 + z^2}}^2 ye^{4x^2 + y^2 + z^2} \, dy\, dz\, dx
\end{equation} My work so far is as follows. Firstly, I noticed that trying to integrate $e^{4x^2 + y^2 + z^2}$ directly with respect to $x$ , $y$ , or $z$ is probably futile. As a result, I've been trying to change the coordinate system from Cartesian to another coordinate system (probably cylindrical or spherical). For reference, the region of integration is a cone with height along the $y$ -axis: As a result, I was motivated to use cylindrical coordinates . However, I've had difficulties with this, due to the $\sqrt{4x^2 + z^2}$ in the limits of integration not seeming to correspond well to cylindrical coordinates, and also the same issue for the $4x^2 + y^2 + z^2$ exponent in the function. My only other idea was to use spherical coordinates, motivated by the presence of $4x^2 + y^2 + z^2$ (since $x^2 + y^2 + z^2 = \rho$ in spherical coordinates). However, describing the region of integration using spherical coordinates is difficult. Can anyone provide hints/a solution to this problem? I've tried everything I know, but I'm still stuck.","['integration', 'cylindrical-coordinates', 'definite-integrals', 'multivariable-calculus', 'spherical-coordinates']"
4894924,Change of coordinates in projective space,"I am doing exercise I.3.1. of Hartshorne, which asks us to prove that any conic in $\mathbb{P}^2$ is isomorphic to $\mathbb{P}^1$ . After searching for some solutions, I found that almost everyone says that ""after changing coordinates, we may assume that the conic is of the form ...,"" say $x^2+y^2+z^2$ , or $xy-z^2$ , or $axy+byz+cxz$ , etc. I am having trouble understanding what ""projective change of coordinates"" means. Is it just an invertible matrix with coefficients in $K$ (ground field)? Furthermore, how can I get the result stated above, which says that all conics have a canonical form? If I assume $\operatorname{char}K\ne 2$ , then I can probably diagonalize the quadratic form and get something like $x^2+y^2+z^2$ ? What if $\operatorname{char}K=2$ ? I've been thinking about this for a while, and I don't think this is explained well online. Any help would be greatly appreciated. I'm new to projective geometry, and I might not know some of the standard facts or tricks. If you use any non-obvious facts, I would appreciate if you could give a proof or a reference to the proof. Thank you very much!","['algebraic-geometry', 'projective-geometry', 'projective-varieties']"
4894941,$T$ Pokemon trainers catch a Pokemon every day. How many days does it take until two trainers own Pokemons of the same species?,$T$ Pokemon trainers catch $1$ out of $P$ different species of Pokemons every day. Every species has the same chance to be caught. One species can be caught by one trainer multiple times. In mean how many days $D$ does it take until at least $2$ trainers own at least one common species? I'm looking for a approximative formula which can be computed for large $P$ and $T$ with $T\ll P$ where a simulation can't be done anymore. $ \pm 10 \%$ is good enough. Proof is not required. Experiments I did some experiment with $P=1025$ Pokemon (current count) and different trainer $T$ count to calculate the mean day $D$ required (results need to be round up). Furthermore the median days and the total number of caught Pokemon among all trainer and the median times the trainer count. T trainer Mean days $D$ needed Median day $D$ needed Total caught Pokemon Median $D$ times $T$ 2 28.8126 27 57.6251 54 3 16.8072 16 50.4215 48 4 12.0491 11 48.1963 44 5 9.44665 9 47.2332 45 6 7.77382 7 46.6429 42 7 6.63424 6 46.4397 42 8 5.83851 6 46.7081 48 9 5.18945 5 46.705 45 10 4.69666 4 46.9666 40 11 4.29345 4 47.2279 44 12 3.95724 4 47.4869 48 13 3.68294 3 47.8782 39 14 3.44168 3 48.1835 42 15 3.24476 3 48.6714 45 16 3.07407 3 49.1851 48 For $T = 2$ trainers we can approximate the mean days $E(D)$ needed with $$E(D) \approx \frac{\sqrt{P\cdot \pi}}{2}$$ Can we generalize this formula for $T>2$ ? Bonus question 1: If we have a look at the total caught Pokemon we see it has a local minimum. The $T$ needed for this local minimum would also be interesting to know. Bonus question 2: How would the mean days $E(D)$ change if one of those 2 trainer need to be a specific one? E.g the first Is there a well studied problem related to this? Birthday paradox has some similarities.,"['statistics', 'birthday', 'expected-value', 'coupon-collector', 'probability']"
4894964,Finding residue of $f(z) = \frac{1}{z^2-1} \sin\frac{1}{z^2 + z^4}$ at $z = 0$,"Trying to find the residue for $f(z) = \frac{1}{z^2-1}  \sin\frac{1}{z^2 + z^4}$ at $z = 0$ . I am concentrating on the sin term in particular. What is a good strategy to approach this? One thing I tried is to decompose into partial fractions and use a trig identity (along with evenness / oddness of sin and cos) e.g: $$
\sin\frac{1}{z^2 + z^4} 
= \sin(\frac{1}{z^2} - \frac{1}{1 + z^2})
= \sin(\frac{1}{z^2})\cos(\frac{1}{1 + z^2}) - \cos(\frac{1}{z^2})\sin(\frac{1}{1 + z^2})
$$ I could try and develop taylor / laurent series from here but not sure this is the right direction as the expressions are still quite involved. Any hints or directions would be appreciated!","['complex-analysis', 'laurent-series']"
4895008,Factorization and irreducibilty for $x^n-2x^m+1$ trinomials.,"I have encountered a weird phenomenon while trying to solve a problem on Reddit. Here is the phenomenon. Let $a>b \in \mathbb{N}$ and $p_{(a,b)} = x^a - 2x^b + 1$ It seems that if $gcd(a,b,c,d) = 1, (a,b) \neq (c,d)$ and $p_{(a,b)}(x) = p_{(c,d)}(x) = 0$ , where $x \in \mathbb{C}$ , then it should be true that $x = 1$ I cannot prove this fact. I was hoping that the reader would be able to","['irreducible-polynomials', 'number-theory', 'roots', 'algebraic-numbers', 'polynomials']"
4895055,$\lim_{x\to 10}(\sqrt{-(x-10)^2})=0$ or $\lim_{x\to 10}(\sqrt{-(x-10)^2})$ is undefined?,"I am reviewing calculus 1 and I saw this problem. The problem is $\lim_{x\to 10}(\sqrt{-(x-10)^2})$ . The answer guide says the answer is undefined, since the function is not defined on an open interval at x=10 and is only defined on a single point, $x=10$ , at which $\sqrt{-(x-10)^2}=0$ . Well I believe that $\lim_{x\to 10}(\sqrt{-(x-10)^2})$ does in fact $=0$ , contrary to the answer guide. This is because $\sqrt{-(x-10)^2}$ is actually defined for $x\ne10$ , just in the imaginary numbers instead of the real ones. And that the imaginary value of $\sqrt{-(x-10)^2}$ gets closer to zero as x approaches ten. I have shown this using a delta-epsilon proof. $$((\epsilon>0\;\land\;0<\delta\le\epsilon)\implies$$ $$(\lvert{x-10}\rvert<\delta\implies$$ $$\lvert{i(x-10)}\rvert<\delta\implies$$ $$\lvert{\sqrt{-(x-10)^2}}\rvert<\delta\implies$$ $$\lvert{\sqrt{-(x-10)^2}-0}\rvert<\delta\implies$$ $$\lvert{\sqrt{-(x-10)^2}-0}\rvert<\epsilon))\implies$$ $$(\forall\;\epsilon>0\;\exists\;\delta>0 \text{ s.t }\lvert{x-10}\rvert<\delta\implies\lvert{\sqrt{-(x-10)^2}-0}\rvert<\epsilon)\implies$$ $$\lim_{x\to10}(\sqrt{-(x-10)^2}=0$$ The crucial step of this proof is rewriting $\lvert{x-10}\rvert$ as $\lvert{i(x-10)}\rvert$ . As far as I'm aware, the two statements are equivalent because absolute value measures the distance of any complex number to the origin, so in general $\lvert{x}\rvert = \lvert{ix}\rvert$ just as $\lvert{x}\rvert = \lvert{-x}\rvert$ . Where is my mistake?","['limits', 'complex-numbers']"
4895061,How to prove if there is no Hamilton Cycle?,"This picture should be a good example of no Hamilton Cycles: I have checked multiple times and made sure there was no cycle, but I don't know how to prove that there is no cycle. Proving that there is one is easy; just list the path that shows it's real. However, I can't find a proper way of proving there isn't a cycle. How would I go about this?","['graph-theory', 'discrete-mathematics', 'hamiltonian-path']"
4895102,"Solving a Set Theory Problem Involving Intersections, Unions, and Complements","I'm struggling with a particular set theory question and would greatly appreciate some guidance. The problem is as follows: Let $A=\{1,2,5,6,7\}$ , $B=\{0,4,6,7,9\}$ , and $C=\{0,1,2,6,7,9\}$ be subsets of $S=\{0,1,\ldots,9\}$ . I need to determine which subsets $X$ of $S$ satisfy the equation $$ (A \cap X) \cup (B \cap X^c) = C. $$ Here, $X^c$ denotes the complement of $X$ within the universal set $S$ . I understand that $A \cap X$ refers to elements common to both $A$ and $X$ , and $B \cap X^c$ includes elements in $B$ but not in $X$ , with their union needing to equal $C$ . However, I'm having trouble figuring out how to approach finding all possible subsets $X$ that meet this criterion. Could someone please explain a step-by-step method to solve this problem? If there are any specific theorems or concepts that could simplify the process, I'd love to learn about those as well. I'm familiar with basic set theory operations like unions, intersections, and complements but applying them in this context has been challenging for me.","['elementary-set-theory', 'problem-solving']"
4895209,"Does $ \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x+y)^{1000}}\ dx\,dy $ converge?","I want to check if the following integral diverges or converges. $ \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x+y)^{1000}}\ dx\,dy $ I'm not sure if it diverges or converges. I attempted to prove divergence, in two ways: Substituting $ x + y = u, x = v$ we get that the abosolute value of the Jacobian is $1, $ we get that $ -\infty \le  x =v \le \infty $ . However, I'm not sure what are the bounadries on $u$ . How do I find them?
let's say $ a \le u \le b$ . If one of the boundaries of $u$ is infinite, then our integral is now $ \int_{-\infty}^{\infty}\int_{a}^{b}e^{-(u)^{1000}}\ dudv, $ and it diverges. However, again, I'm not sure on the boundaries. Since $e^{-(x+y)^{1000}} \ge 0 $ , we can look at a small enough interval near some $x_0$ , such that $ x-y$ is close to $ 0$ - so in that interval, we can say that $ \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x+y)^{1000}}\ dx\,dy  \ge  \int_{-\infty}^{\infty}\int_{x_0 - \delta}^{x_0 + \delta}\frac{1}{2}\ dx\,dy, $ and diverges. So my questions are:
in the first attempt, how to find the boundaries?
Are my attempts okay? If not, what's wrong with them?","['integration', 'multivariable-calculus']"
4895224,Finding the limiting distribution of $T_{n}/S_{n}$ as n tends to infinity,"Question Let $X_i \sim\left(i . i\right.$ . $d$ .) Bernoulli $\left(\frac{\lambda}{n}\right), n \geq \lambda \geq 0$ . $Y_i \sim\left(i\right.$ i. d.) Poisson $\left(\frac{\lambda}{n}\right),\left\{X_i\right\}$ and $\left\{Y_i\right\}$ are independent.
Let $\sum_{i=1}^{n^2} X_i=T_n$ and $\sum_{i=1}^{n^2} Y_i=S_n$ (say).
Find the limiting distribution of $\frac{T_n}{S_n}$ as $n \rightarrow \infty$ . My attempt: Let $F$ be the cdf of $\frac{T_n}{S_n}$ . Then, $F(\frac{a}{b})$ = $P(T_{n} \leq a) \times P(S_{n} \geq b)$ where $a \in \{0,1,2,...,n^{2}\}$ and $b \in  \{1,2,3,4....\}$ . Now, $P(T_{n}\leq a)$ = $\sum_{i=0}^{a}$ $\binom {n^{2}}{i} ({\frac{\lambda}{n}})^{i}(1-\frac{\lambda}{n})^{n^{2}-i}$ and, $P( S_{n} \geq b)$ = $1- \sum_{k=0}^{b}\frac{(n^{2}\lambda)^{k}}{k!}e^{-n^{2}\lambda}$ . As the distribution of $S_{n}$ came out to be Poisson with parameter $n^{2}\lambda$ . When $n$ tends to infinity, I can see that $\frac{n^{2}}{e^{n^{2}}}$ goes to 0, hence, $P(S_{n}\geq b)$ goes to 1. But I am not able to find the limit of other expression. The question has been discussed here:- Find the limiting distribution of $T_n/S_n$ as $n\rightarrow \infty$ .  There were no answers and I couldn't
benefit from the comment over there.","['probability-distributions', 'probability-theory', 'poisson-distribution', 'bernoulli-distribution']"
4895258,Center of group of order 3773,"What can we say about $|Z(G)|$ if $G$ is of order $3773 = 7^3 * 11$ . Here $|Z(G)|$ means the size of the center. This is an exercise in book about Sylow theorem and I have no idea what Sylow theorem has to do with size of center. My idea is that center should be a normal subgroup, and according to Sylow theorem we know Sylow-11 and Sylow-7 subgroups are both normal subgroup. But I can't prove neither there are only two normal subgroup mentioned above, nor center should be which normal subgroup.","['group-theory', 'sylow-theory', 'finite-groups']"
4895366,Maximum value of $\sin(x)-\sin^3(x)$ without using calculus [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 months ago . Improve this question Can you find the maximum value of $\sin(x)-\sin^3(x)$ without using calculus. I tried AM-GM but could not find anything useful","['inequality', 'trigonometry']"
4895405,The sum of two i.i.d. random variables cannot be uniformly distributed,"Given $X$ and $Y$ iid (independent and identically distributed), show that $Z=X+Y$ cannot be uniformly distributed. The uniform distribution might be on any set, not necessarily an interval. Notice that the density might not exist, so convolution might not work. For the trivial cases of where the pdf (probability density function) exists, we just use the continuity of convolution and it's done. This also shows that in this case any uniform distribution cannot work, whether it's an interval or not. So, I'm guessing for the more general case. The original question is like this. *I'm not asking about this, but (1) might help as a hint, and the original condition might tell you whether my abstraction is correct. $X,Y$ are iid on $[0,\frac12]$ , cdf (cumulative distribution function) is $F(x)$ , $Z=X+Y$ , (1) Show that $\forall \epsilon\in(0,\frac14)$ , $$P(Z\leq\epsilon)\leq F(\epsilon)^2$$ $$P(\frac12-\epsilon\leq Z\leq \frac12+\epsilon)\geq2F(\epsilon)(1-F(\frac12-\epsilon))$$ (2) Prove that $Z$ is not uniformly distributed.","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4895478,Error in graphing the polar equation $r=-1+\cos(\theta)$. I get a different answer than the book for cos(30degrees). Am I wrong?,"I have included a picture of the table that is shown in the answer key. Most of the answers coincide with what I get by using various calculators, but some don't and I don't know if I am making some sort of mistake rounding (I'm rounding to the nearest 1/10th, as is the book, apparently). I don't know if there is a typo in the book or what. These are all in degrees from 0 degrees to 360 degrees. I'll give you an example of my problem. If I enter 30 degrees for theta, I get -.133 and so on, which I round to -.1 but the book says it is .7 as you can see in the table. It goes on for several apparent errors, though when it gets to quadrantal angles, the book seems to have the correct answer and coincides with what I get. I know it's not terribly important, but the mistakes I make help me to learn more, so any help would be appreciated.","['trigonometry', 'polar-coordinates']"
4895484,Is it valid to use L'Hopital's rule inside a power?,"Suppose we have two functions such that f(a)=g(a)=0 and $f'(a)$ and $g'(a)$ are non-zero finite values. We are given to find $lim_{x\to a}\frac{(f(x))^2}{(g(x))^2}$ . So instead of differentiating $(f(x))^2$ , can we differentiate f(x) directly? In other words, Is the above limit equivalent to $(\lim_{x\to a}\frac{f(x)}{g(x)})^2$ And in a more general note, Can we apply this for any n-th power?","['limits', 'calculus']"
4895521,Find value of $2A$ if $A=\frac{3\tan\left(A\right)}{1-\tan\left(A\right)}-1$,"Find value of $2A$ if $A=\frac{3\tan\left(A\right)}{1-\tan\left(A\right)}-1$ My first thought was rewriting the $1$ as $\frac{1-\tan(A)}{1-\tan(A)}$ ,
which implies that $A=\frac{4\tan\left(A\right)-1}{1-\tan\left(A\right)}$ I then noticed that it awfully mirrored the tangent angle addition identity, $\tan\left(\alpha+\beta\right)=\frac{\tan\left(\alpha\right)+\tan\left(\beta\right)}{1-\tan\left(\alpha\right)\tan\left(\beta\right)}$ . However, I am quite unsure on how to progress from here, or if my current progress is even in the correct direction. Any assistance would be greatly appreciated!",['trigonometry']
4895635,Semi-differentiable function vs. restriction of a differentiable function,"I am studying manifolds with boundary, and the definition of a continuous or smooth function on the domain $\mathbb{H}^n \subset \mathbb{R}^n = \{(x_1, \dots, x_n): x_n \geq 0\}$ is a function that is the restriction of a continuous or smooth function defined on a neighborhood of $\mathbb{H}^n$ . Is this definition equivalent to the definition that a function on $\mathbb{H}^n$ is continuous or smooth if we only look at limits of the function or limits of the function's difference quotient at sequences of points that stay in the function's domain? (This is the notion of left- and right- handed limits, or I think semi-differentiability .) Does this equivalence hold for an arbitrary subset of Euclidean space that is not $\mathbb{H}^n$ , possibly with some assumptions? Note: My question seems to be a common one, but I can't find an answer for it. It is unanswered in the comments to the first answer here . I have been searching up ""extensions of continuous functions on an arbitrary subset of Euclidean space,"" and the results do not seem to be what I am asking. Call the definition in paragraph 1 Def 1, and the one in paragraph 2 Def 2. I believe this asks about taking Def 1 and extending to all of Euclidean space, this asks about enlarging the domain for an already open set, and several questions are about converting Def 1 locally into Def 1 globally by a partition of unity. I am asking about assuming Def 2 and ""extending to a neighborhood of the set"" to get Def 1.","['multivariable-calculus', 'differential-topology', 'differential-geometry', 'real-analysis']"
4895637,Simple graph $G$ can be represented as union of $2$ edge-disjoint graphs,"I'm trying to prove the following statement: Prove that any simple graph $G$ can be represented as the union of $2$ edge-disjointed graphs $G_{1}$ and $G_{2}$ , where $G_{1}$ is acyclic and $G_{2}$ such that nodes on $G_{2}$ have an even degree. I know that if $G_{1}$ is acyclic then it's fine because then $G = G_{1}$ . But what if $G$ has cycles? Then I know I can remove cycles from $G$ but I don't know what to do from here on.","['graph-theory', 'discrete-mathematics', 'planar-graphs']"
4895649,Compute $r$ such that $\lim_{x\to 0}\frac{3^{\sqrt{x}}-1}{x^r} \neq 0$?,"I am trying to compute $r$ such that: $$\lim_{x\to 0}\frac{3^{\sqrt{x}}-1}{x^r} \neq 0$$ I tried a few things, mostly multiplying it by some conjugate and using identities like $e^{\log(x)}=x$ but couldn't find out. In the book, the answer seems to be $r=\frac{1}{2}$ but I can't manipulate it in order to get this value. Can you help me? In the book I'm reading (Efimov's Higher Mathematics), this is called ""computing the order of $3^{\sqrt{x}}-1$ relative to $x$ "".","['limits', 'limits-without-lhopital']"
4895697,Finding $A+B+C$ in $\cos^4(x)=A+B\cos(2x)+C\cos(4x)$?,"Given $\cos^4(x)=A+B\cos(2x)+C\cos(4x)$ , Find $A+B+C$ . I'm having trouble trying to find this sum $A+B+C$ , here is my attempt so far: I know that $\cos(4x)=8\cos^4(x)-8\cos^2(x)+1$ and $\cos(2x)=1-2\sin^2(x)$ . Substituting those values into the equation gives a new relation, $$\cos^4(x)=A+B+C-2B\sin^2(x)+(8C-1)\cos^4(x)-8C\cos^2(x)$$ Then I used the identity $\cos^2(x)+\sin^2(x)=1$ , letting $P=A+B+C$ , and $D=\cos^2(x)$ which turns the above equation into a quadratic: $$(8C-1)\cos^4(x)+2(B-4C)\cos^2(x)+P-2B=0$$ From here I assume $D_1$ and $D_2$ as roots, therefore by Vieta's formulas I have arrived at, $$D_{1}+D_{2}-D_{1}D_{2}=\frac{8C-P}{8C-1}$$ I'm not so sure what to do from here, what should I do now?","['algebra-precalculus', 'trigonometry']"
4895716,Questions About Four Definitions of The Upper and Lower Limits of A Sequence,"Related questions have been posted here and here . Background I have seen the following four definitions of the upper and lower limits of a sequence from textbooks and MSE posts: Definition 1 $\quad$ [ Baby Rudin ] $\quad$ The upper limit, $\limsup_{n\to\infty}s_n$ , of a given sequence $\{s_n\}$ is the supreme of the set of numbers $x$ in the extended real number system such that $s_{n_k}\to x$ for some subsequence $\{s_{n_k}\}$ of $\{s_n\}$ . Definition 2 $\quad$ [ Measure Theory by Donald Cohn] $\quad$ The upper limit, $\limsup_{n\to\infty}s_n$ , of a given sequence $\{s_n\}$ is defined by \begin{align*}
\limsup_{n\to\infty}s_n = \inf_{n}\sup_{m\geq n}s_m = \inf_{n}\left\{\sup\{s_n,s_{n+1},\dots\}\right\}.
\end{align*} Definition 3 $\quad$ [MSE] $\quad$ The upper limit, $\limsup_{n\to\infty}s_n$ , of a given sequence $\{s_n\}$ is defined by \begin{align*}
\limsup_{n\to\infty}s_n = \lim_{n\to\infty}\sup_{m\geq n}s_m = \lim_{n\to\infty}\left(\sup\{s_n,s_{n+1},\dots\}\right).
\end{align*} Definition 4 $\quad$ [MSE] $\quad$ A number $t$ in the extended real number system is the upper limit, $\limsup_{n\to\infty}s_n$ , of a given sequence $\{s_n\}$ if \begin{align*}
\text{for all $s<t$, we have $s<s_n$ for infinitely many $n$'s}
\end{align*} and \begin{align*}
\text{for all $s>t$, we have $s<s_n$ for finitely many $n$'s.}
\end{align*} I have some questions about the definitions themselves as well as the equivalence among them. Question 1 I am not sure if I am right about this. I think Definition 3 should have been stated as follows: Definition 3 $\quad$ [Revised] $\quad$ The upper limit, $\limsup_{n\to\infty}s_n$ , of a given sequence $\{s_n\}$ shall be defined as follows: \begin{align*}
\begin{cases}
\limsup_{n\to\infty}s_n &= +\infty &\text{if}\ \ \sup\{s_n,s_{n+1},\dots\}\to+\infty,\\
\limsup_{n\to\infty}s_n &= -\infty &\text{if}\ \ \sup\{s_n,s_{n+1},\dots\}\to-\infty,\ \ \text{and}\\
\limsup_{n\to\infty}s_n &= \lim_{n\to\infty}\sup_{m\geq n}s_m &\text{if}\ \ \text{the limit exists}.
\end{cases}
\end{align*} My concern about this revised definition is that what should we say about the case when the limit of $\sup_{m\geq n}s_m$ does not exist yet neither $\sup\{s_n,s_{n+1},\dots\}\to\pm\infty$ ? Question 2 In one of the link I provided in the beginning, there is a proof of the equivalence between Definition 1 and Definition 3. However, I am not sure if my understanding of that proof is correct, espectially for the proof of $\sup S \leq \limsup_{n\to\infty}s_n$ where the right-hand side of the inequality is in the sense of Definition 3. Moreover, I would like to try to work out all the details. I would really appreciate it if someone could help me check my work! Proof $\quad$ Let $S$ be the set of numbers $x$ in the extended real number system such that $s_{n_k}\to x$ for some subsequence $\{s_{n_k}\}$ of a sequence $\{s_n\}$ . We want to prove that \begin{align*}
\sup S = \limsup_{n\to\infty}s_n,
\end{align*} where the right-hand-side is in the sense of the revised Definition 3. Let $F_n=\{s_n,s_{n+1},\dots\}$ . We first prove that \begin{align*}
\sup S \leq \limsup_{n\to\infty}s_n.
\end{align*} Suppose first that $\sup F_n\to+\infty$ . Then $\limsup_{n\to\infty}s_n=+\infty$ by definition, and we are done. Suppose next that $\sup F_n\to-\infty$ . We want to show that $\sup S = -\infty$ , which is true if and only if every subsequence $\{s_{n_k}\}$ of $\{s_n\}$ such that $s_{n_k}\to x\in S$ satisfies $s_{n_k}\to-\infty$ . Assume to the contrary that there is a subsequence $\{s_{n_k}\}$ such that $s_{n_k}\to\alpha>-\infty$ . For each $n$ , let $n_k$ be the smallest integer bigger than or equal to $n$ , so that $\{s_{n_k},s_{n_{k+1}},\dots\}\subseteq F_n$ . Then $\sup F_n \geq \sup\{s_{n_k},s_{n_{k+1}},\dots\}\geq\alpha$ for each $n$ , contradicting the fact that $\sup F_n\to-\infty$ . Therefore, $\sup S=-\infty$ . Finally, suppose that the limit $\lim_{n\to\infty}(\sup F_n)$ exists. Then for each subsequence $\{s_{n_k}\}$ such that $s_{n_k}\to x\in S$ , construct a sequence $\{u_n\}$ as follows: For index $n$ such that $n=n_k$ for some $n_k$ let $u_n=s_n=s_{n_k}$ , and for other index $n$ let $u_n=u_{n+1}$ . Let $E_n=\{u_n,u_{n+1},\dots\}$ . Then for all $n$ we have $E_n\subseteq F_n$ . Thus for all $n$ we have $\sup E_n\leq\sup F_n$ . Note that $s_{n_k}\not\to+\infty$ . If $s_{n_k}\to x=-\infty$ , then $u_n\to-\infty\leq\lim_{n\to\infty}(\sup F_n)$ . If $\lim_{k\to\infty}s_{n_k}=x\in\mathbb{R}$ , then $\lim_{n\to\infty}u_n=\lim_{k\to\infty}s_{n_k}=x$ . Then $E_n\supseteq E_{n+1}$ and so $\sup E_n\geq\sup E_{n+1}$ for all $n$ . Moreover, $\sup E_n\geq x$ for all $n$ . It follows that $\{\sup E_n\}$ is a bounded decreasing sequence, so $\lim_{n\to\infty}(\sup E_n)$ exists. Since $E_n\subseteq F_n$ for all $n$ , we have $u_n\leq\sup E_n\leq\sup F_n$ for all $n$ , and it follows that \begin{align*}
\limsup_{n\to\infty}s_n = \lim_{n\to\infty}(\sup F_n) \geq \lim_{n\to\infty}(\sup E_n) \geq \lim_{n\to\infty}u_n=\lim_{k\to\infty}s_{n_k}=x\in\mathbb{R}.
\end{align*} Therefore, for any subsequence $\{s_{n_k}\}$ of $\{s_n\}$ such that $s_{n_k}\to x \in S$ we have $x\leq\lim_{n\to\infty}(\sup F_n)$ ; that is, $x\leq\lim_{n\to\infty}(\sup F_n)$ for all $x\in S$ . Hence, $\sup S\leq\lim_{n\to\infty}(\sup F_n)=\limsup_{n\to\infty}s_n$ . This complets the proof of $\sup S \leq \limsup_{n\to\infty}s_n$ . Now we prove the opposite direction \begin{align*}
\sup S\geq\limsup_{n\to\infty}s_n.
\end{align*} If $\sup S=+\infty$ , then there is nothing to prove. If $\sup S=-\infty$ , then every subsequence $\{s_{n_k}\}$ of $\{s_n\}$ such that $s_{n_k}\to x\in S$ satisfies $s_{n_k}\to-\infty$ . We want to show that $\sup F_n\to-\infty$ . If $\sup F_n\to\alpha>-\infty$ , then $\sup F_n\geq\sup F_{n+1}$ for all $n$ implies $\sup F_n\geq\alpha$ for all $n$ . Hence, there must exists a subsequence $\{s_{n_k}\}$ such that $s_{n_k}\to\alpha$ , a contradiction. Therefore, we must have $\sup F_n\to-\infty$ , which means $\limsup_{n\to\infty}s_n=-\infty$ . Now suppose $\sup S\in\mathbb{R}$ . If $\sup F_n\to+\infty$ , we have seen that it will imply the existence of a subsequence $\{s_{n_k}\}$ such that $s_{n_k}\to+\infty$ , contradicting our assumption of $\sup S\in\mathbb{R}$ . If $\sup F_n\to-\infty$ , then there is nothing to prove. So suppose that the limit $\lim_{n\to\infty}(\sup F_n)$ exists. For each $n$ , let $l_n\in\mathbb{N}$ be such that $l_n\geq n$ and \begin{align*}
\sup F_n \geq s_{l_n} \geq \sup F_n - \frac{1}{2^n}.
\end{align*} Then, \begin{align*}
\lim_{n\to\infty}(\sup F_n) = \lim_{n\to\infty}s_{l_n}.
\end{align*} Since $l_n\geq n$ for each $n$ , it follows that $\{l_n\}$ forms a sequence of positive integers such that $l_n\to+\infty$ . There there is an increasing subsequence $\{l_{n_k}\}$ of $\{l_n\}$ . Let $n_k=l_{n_k}$ . Since $\{s_{n_k}\}$ is a subsequence of $\{s_{l_n}\}$ which converges to $\lim_{n\to\infty}(\sup F_n)$ , it follows that $\lim_{k\to\infty}s_{n_k} = \lim_{n\to\infty}(\sup F_n)$ , and so $\lim_{n\to\infty}(\sup F_n)\in S$ . Hence, $\lim_{n\to\infty}(\sup F_n)\leq\sup S$ . This complets the proof of $\sup S \geq \limsup_{n\to\infty}s_n$ . I am not sure if this is all correct and rigorous. For example, when proving $\sup S\geq\limsup_{n\to\infty}s_n$ , I wrote the following without proof: If $\sup F_n\to\alpha>-\infty$ , then $\sup F_n\geq\sup F_{n+1}$ for all $n$ implies $\sup F_n\geq\alpha$ for all $n$ . Hence, there must exists a subsequence $\{s_{n_k}\}$ such that $s_{n_k}\to\alpha$ , a contradiction."" Honestly, I just think this is intuitively ture, but couldn't figure out how to prove this claim. It would be great if someone can help me out. Moreover, any other improvement suggestion would be greatly appreciated! Question 3 This question is about the equivalence of Definition 2 and 3. I was wondering if the equivalence follows from the Monotone Convergence Theorem? Question 4 How would one provide a proof for the equivalence between Definition 4 and one of the other three definitions? Thanks a lot in advance!","['limsup-and-liminf', 'real-analysis', 'sequences-and-series', 'limits', 'supremum-and-infimum']"
4895726,"Is there a name for this set-theoretical definition of natural numbers, or has it been invented?","I'll call it the binary encoding with sets. I think it's nice and trivial, should have been discovered by many genius brains, but i can't find it by searching with efforts. Prior arts are Zermelo's and Von Neumann's definition of ordinals, both starting from $0=\emptyset=\{\}$ , then followed by $s(n):=\{n\}$ or $s(n):=n\cup\{n\}$ correspondingly. The binary encoding just encodes the indices of bits the ""binary form"" of a natural number has. A few examples summarizes it: $0=\{\}$ , $1=\{0\}$ , $2=\{1\}$ , $3=\{0,1\}$ , $4=\{2\}$ , $5=\{0,2\}$ , etc... Here's a longer list since it looks satisfying (using both $\emptyset$ and $\{\}$ for the empty set): 0 {} ∅
1 {{}} {∅}
2 {{{}}} {{∅}}
3 {{}{{}}} {∅{∅}}
4 {{{{}}}} {{{∅}}}
5 {{}{{{}}}} {∅{{∅}}}
6 {{{}}{{{}}}} {{∅}{{∅}}}
7 {{}{{}}{{{}}}} {∅{∅}{{∅}}}
8 {{{}{{}}}} {{∅{∅}}}
9 {{}{{}{{}}}} {∅{∅{∅}}}
10 {{{}}{{}{{}}}} {{∅}{∅{∅}}}
11 {{}{{}}{{}{{}}}} {∅{∅}{∅{∅}}}
12 {{{{}}}{{}{{}}}} {{{∅}}{∅{∅}}}
13 {{}{{{}}}{{}{{}}}} {∅{{∅}}{∅{∅}}}
14 {{{}}{{{}}}{{}{{}}}} {{∅}{{∅}}{∅{∅}}}
15 {{}{{}}{{{}}}{{}{{}}}} {∅{∅}{{∅}}{∅{∅}}}
16 {{{{{}}}}} {{{{∅}}}}
17 {{}{{{{}}}}} {∅{{{∅}}}}
18 {{{}}{{{{}}}}} {{∅}{{{∅}}}}
19 {{}{{}}{{{{}}}}} {∅{∅}{{{∅}}}}
20 {{{{}}}{{{{}}}}} {{{∅}}{{{∅}}}}
21 {{}{{{}}}{{{{}}}}} {∅{{∅}}{{{∅}}}}
22 {{{}}{{{}}}{{{{}}}}} {{∅}{{∅}}{{{∅}}}}
23 {{}{{}}{{{}}}{{{{}}}}} {∅{∅}{{∅}}{{{∅}}}}
24 {{{}{{}}}{{{{}}}}} {{∅{∅}}{{{∅}}}}
25 {{}{{}{{}}}{{{{}}}}} {∅{∅{∅}}{{{∅}}}}
26 {{{}}{{}{{}}}{{{{}}}}} {{∅}{∅{∅}}{{{∅}}}}
27 {{}{{}}{{}{{}}}{{{{}}}}} {∅{∅}{∅{∅}}{{{∅}}}}
28 {{{{}}}{{}{{}}}{{{{}}}}} {{{∅}}{∅{∅}}{{{∅}}}}
29 {{}{{{}}}{{}{{}}}{{{{}}}}} {∅{{∅}}{∅{∅}}{{{∅}}}}
30 {{{}}{{{}}}{{}{{}}}{{{{}}}}} {{∅}{{∅}}{∅{∅}}{{{∅}}}}
31 {{}{{}}{{{}}}{{}{{}}}{{{{}}}}} {∅{∅}{{∅}}{∅{∅}}{{{∅}}}}
32 {{{}{{{}}}}} {{∅{{∅}}}} What's more satisfying is it forms a one-to-one correspondence between ""finite sets of sets"" (we might need a proper description here) to natural numbers. (There's too little room for me to write the complete proof though) The succesor function and comparator will be a bit harder but definitely doable since it's just matter of bit operations. like, $\mathop{addbit}(x,b)=\mathrm{if}(b\notin x): \{b\}\cup x, \mathrm{otherwise}:\mathrm{addbit}(x\setminus b, \mathrm{addbit}(b,0))$ and $s(x)=\mathrm{addbit}(x,0)$ . ps. Some states Zermelo's definition faces problem when dealing with ""limit"" because of ""infinite nesting"" while Von Neumann's doesn't. I'm not sure how to understand it since I think it's kinda obvious that Von Neumann's construct also nests infinitly as it (by definition) contains ""strictly more"" elements (I suspect we need another proper term here). Still, Computer-Science-wise, this scheme nests asymptotically less, say, $O(\log^* n)$ .","['elementary-set-theory', 'binary', 'natural-numbers', 'set-theory']"
4895753,What $3 \times 3$ matrix gives the determinant $a^{2}+b^{2}+c^{2}$?,"The $2D$ rotation matrix is given by a $2\times{2}$ matrix: $$\begin{bmatrix}
a & -b\\
b & a
\end{bmatrix}$$ The determinant of this matrix is $a^{2}+b^{2}$ . Can one construct a $3\times{3}$ matrix for $3D$ rotations where the determinant of the matrix is $a^{2}+b^{2}+c^{2}$ , and the elements of the matrix are $a,b,c\in\mathbb{R}$ . If there is not, is there a $3\times{3}$ or $2\times{2}$ matrix that can be constructed with the same determinant, $a^{2}+b^{2}+c^{2}$ given complex numbers?","['determinant', 'matrices', 'linear-algebra', 'linear-transformations', 'rotations']"
4895798,permutation group on $\mathbb{F}_p$,"Suppose $\mathbb{F}_p$ the field with $p$ elements and $\mathcal{S}(\mathbb{F}_p)$ the permutation group. Define for $a \in \mathbb{F}^\times_p$ and $b \in \mathbb{F}_p$ the permutation $\gamma_{a,b}: \mathbb{F}_p \to \mathbb{F}_p, x \to ax+b$ . The set of all such permutations forms a subgroup for $\mathcal{S}(\mathbb{F}_p)$ we call $AGL(1, \mathbb{F}_p)$ We were asked to show that the set $\{\sigma \in \mathcal{S}(\mathbb{F}_p) \mid \sigma^{-1}\langle\gamma_{1,1}\rangle \sigma = \langle\gamma_{1,1}\rangle\}$ equals $AGL(1, \mathbb{F}_p)$ . I've already showed that $AGL(1, \mathbb{F}_p)\subseteq \{\sigma \in \mathcal{S}(\mathbb{F}_p) \mid \sigma^{-1}\langle\gamma_{1,1}\rangle \sigma = \langle\gamma_{1,1}\rangle\}$ by showing $\langle\gamma_{1,1}\rangle$ is a normal subgroup of $AGL(1, \mathbb{F}_p)$ . It's the other inclusion I am having some trouble with. I was thinking with contraposition but I don't know how to characterize elements that aren't in $AGL(1, \mathbb{F}_p)$ apart from a fairly abstract cykel notation. Any tips?","['permutations', 'group-theory', 'abstract-algebra']"
4895820,Formula for Mirror Numbers,"Hey so I stumbled upon a Problem and I cant really find a good Solution. The Problem is about Mirror Numbers and the Original Question was: Find a Formula to convert a number „ab“, where a and b are the digits of this number( $ab = 15, a = 1, b = 5$ ) to its Mirror number „ba“. My Solution was: $ab - ba = 9\times n$ ; and $n = a-b$ So therefore $ab - 9\times (a-b) = ba$ . I wanted to take this Problem a step further and wanted a Solution for 3 digit number „abc“ -> „cba“ My Solution for this was: $abc - cba = 9\times n$ ; and $n = 10\times (a-c)-(c-a)$ Therefore: $abc - 9\times(10\times(a-c)-(c-a)) = cba$ Now I am struggling with a Solution for 4 digit Numbers.. I already found some Kind of formula but its Not working well. For Number 1234 the formula is $1234 - 9\times(100\times(1-4) - 10\times(2+3-(3-2)) - (4-1))$ But it doesn't work for 4321 or 1235. Does Anyone know good resources on this topic or can someone help me find a Solution?",['algebra-precalculus']
4895826,"Can use one-variable integration to tell whether $R=\{(x,y):\;0\leq y\leq\left|\mathrm{sin}\frac{1}{x}|,\;0<x\ <1\right\}$ is rectifiable(has area)?","$R=\left\{(x,y):\;0\leq y\leq\left|\mathrm{sin}\frac{1}{x}\right|,\;0<x\ <1\right\}$ , to determine whether it's rectifiable, can I use the method of one-variable integration to prove? Like this question, if we see it as a one-variable integration $f(x)=\int_0^1 |\sin(\frac{1}{x})|$ , then it has only one discontinuous point at (0,0), so it's integrable. Can I use this to tell that R is rectifiable? If using the def. of rectifiable(or has area), we need to take partition first and check the area of $\partial D$ has zero area, but it's not so simple to handle the discontinuity at $(0,0)$ . So I hope the question can be converted into the integration on the x,y plane. I guess it's OK, but I don't know how to prove it.","['integration', 'multivariable-calculus', 'area']"
4895881,Contour integral - Cauchy's Residue Theorem,"If $x>1$ show that \begin{equation}\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}\frac{x^s}{s}\textrm{d}s=1,\end{equation} for any $c>0$ . My working: the function $\frac{x^s}{s}$ is meromorphic with only a simple pole at $s=0$ with residue $1$ . So Cauchy's residue theorem says the integral is $2\pi i$ multiplied by the sum of the residues, which rearranges to $\frac{1}{2\pi i}\int\frac{x^s}{s}ds=1$ . The problem that I see with this is that it doesn't depend on the choice of contour which makes me suspicious. Solution given in the book: Consider the integral \begin{equation}\frac{1}{2\pi i}\int_{c-iR}^{c+iR}\frac{x^s}{s}ds\end{equation} with $R>c$ and the contour $\zeta_R$ described by the line segment joining $c-iR$ to $c+iR$ and the semicircle $S_R$ of radius $R$ centred at $c$ and enclosing the origin. Then by the Residue Theorem we have \begin{equation}\frac{1}{2\pi i}\int_{\zeta_R}\frac{x^s}{s}ds=\textrm{Res}_{s=0}\frac{x^s}{s}=1,\end{equation} and so \begin{equation}\frac{1}{2\pi i}\int_{c-iR}^{c+iR}\frac{x^s}{s}ds+\frac{1}{2\pi i}\int_{S_R}\frac{x^s}{s}=1.\end{equation} Now this second integral satisfies \begin{equation}\left|\frac{1}{2\pi i}\int_{S_R}\frac{x^s}{s}ds\right|\ll\frac{x^c}{2\pi R}\int_{\pi/2}^{3\pi/2}x^{R\cos\varphi}d\varphi.\end{equation} By setting $t=-\cos\varphi$ we can evaluate the integral and bound it above by $\frac{x^c}{2R}\to 0$ as $R\to\infty$ . (1) Why this choice of contour? (2) What exactly are they doing? What is the overall strategy behind their solution? I understand they want to consider $c-iR$ to $c+iR$ and take the limit as $R\to\infty$ , but I don't understand their approach","['integration', 'number-theory', 'real-analysis', 'complex-analysis', 'functional-analysis']"
4895890,"In which case can the term ""singularity"" and ""isolated singularity"" be used interchangeably?","I am right now going through complex analysis with the end of goal being able to use Cauchy's residue theorem. I understand that a point $z$ in the complex plane will be termed a ""singularity"" if a function $f(z)$ fails to be analytic at the point. But when will the same $z$ be termed ""isolated singularity"" ? I keep getting confused with this. For instance, consider this example from Section 74 of Churchill & Brown, 8th edition where the author says that the function $f(z) = \frac{z+1}{z^2 +9}$ has an isolated singularity at $z=3i$ . My question is, why is $z=3i$ an isolated singularity? If I have to eventually use residue theorem on a certain function, my understanding says that I would follow these steps: Identify if $f(z)$ has singularities. Check if the singularities are isolated. If the singularities are isolated, what category do they fall under?;removable, pole of order $n$ , a simple pole or essential. If I have to go by the definition of isolated singularity, then I would have to check it's region of convergence or specifically, find the deleted neighbourhood. Would this be a rigorous approach of doing things? feel like I am missing this part in my study. I am referring to Zill & Shanahan and Churchill & Brown to study complex analysis because I am in engineering. Both the literature sources use these terms casually without justifying why a singularity would be termed isolated.",['complex-analysis']
4895891,What is the most efficient way to find the area of a circle given three points on the rim,"let there be three points in $\mathbb{R}^2$ , $(x_1,y_1)$ , $(x_2,y_2)$ , and $(x_3,y_3)$ . I want to find the area of the circle that passes through all three points? I can find the circle by: $$
\begin{cases}
    (x_1 - x_c)^2 + (y_1 - y_c)^2 - r^2\\
    (x_2 - x_c)^2 + (y_2 - y_c)^2 - r^2 \\
    (x_3 - x_c)^2 + (y_3 - y_c)^2 - r^2
  \end{cases}
$$ isolate $y_c$ and $x_c$ as a function of the isolated $y_c$ : $$\begin{cases}
(x_2 - x_c)^2 - (x_1 - x_c)^2 + (y_2 - y_c)^2 - (y_1 - y_c)^2 = 0\\
(x_3 - x_c)^2 - (x_1 - x_c)^2 + (y_3 - y_c)^2 - (y_1 - y_c)^2 = 0 
\end{cases}$$ $$\begin{cases}
x_2^2 - 2x_2x_c + x_c^2 - x_1^2 + 2x_1x_c- x_c^2 + (y_2 - y_c)^2 - (y_1 - y_c)^2 = 0\\
x_3^2 - 2x_3x_c + x_c^2 - x_1^2 + 2x_1x_c- x_c^2 + (y_3 - y_c)^2 - (y_1 - y_c)^2 = 0 
\end{cases}$$ $$\begin{cases}
x_c = \frac{(y_1 - y_c)^2 - (y_2 - y_c)^2 - (x_2^2 - x_1^2)}{2(x_1 - x_2)}\\
x_c = \frac{(y_1 - y_c)^2 - (y_3 - y_c)^2 - (x_3^2 - x_1^2)}{2(x_1 - x_3)}
\end{cases}$$ $$ y_c= \frac{(x_3x_1 + y_2^2)(x_1 - x_3) + (x_1x_2 + y_3^2)(x_2 - x_1) + (x_2x_3 + y_1^2)(x_3 - x_2)}{2[(y_1 - y_3)(x_1 - x_2) - (y_1 - y_2)(x_1 - x_3)]} $$ From here, we can calculate $x_c$ , $y_c$ , and $r^2$ . Then, the area of the circle $S$ is given by $S=\pi r^2$ . I am going to implement this in a program and I am mainly aiming for runtime optimization. What would be the fastest way to calculate the area $S$ from the three points?","['circles', 'geometry']"
4896014,An inequality for Lebesgue measure,"$E\subset [a,b]$ is measurable. $I_k\subset [a,b](k=1,2,\cdots)$ are open intervals satisfying: $m(I_k\cap E)\ge \frac23 |I_k|, k=1,2,\cdots$ Prove: $$m\left(\left(\bigcup_{k=1}^\infty I_k\right)\cap E\right)\ge \frac 13 m\left(\bigcup_{k=1}^\infty I_k\right).$$ I'm not sure how to solve this problem. I tried to find a open cover to estimate the measure, but the inequality is always in a wrong direction. How could I solve this?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4896028,Finding $\liminf n (\sin n)^2$,"I'm solving an exercise from The elements of Real Analysis by Robert G. Bartle, which asks to find the $\limsup$ and $\liminf$ of the sequence given by $a_n = n (\sin n)^2$ . I have already calculated $\limsup a_n$ as follows: Since $\left\lbrace\sin n : n\in\mathbb{N}\right\rbrace$ is dense in $[-1, 1]$ , and $\sin n \neq 1$ for every $n\in\mathbb{N}$ , there are infinite many numbers $m\in\mathbb{N}$ such that $\sin m \geq \frac{9}{10}$ , and therefore, $m(\sin m)^2\geq \frac{81}{100}m$ for infinite many natural numbers. This implies $a_n$ is not bounded above, so $\limsup a_n = +\infty$ . I was trying to apply something similar to find $\liminf a_n$ , but haven't been able to get any conclusion. Can you help me with that, please?","['limits', 'limsup-and-liminf', 'real-analysis']"
4896070,"Integral inequality on $[0,1]$","I found the following question on another forum and unfortunately I don't have any additional information about it: Let $f$ be non-negative and square integrable on $[0,1]$ . Prove that $$\left(\int_0^1 xf(x)\text{d}x\right)\left(\int_0^1 f^2 (x)\text{d}x\right)\geq\frac{4}{9}\left(\int_0^1 f(x)\text{d}x\right)^3.$$ I tried the usual Cauchy inequality tricks but nothing worked. I don't know if the inequality is tight, but I couldn't find an equality case.","['integration', 'inequality', 'integral-inequality']"
4896091,Proving the inequality $\sum_{n=1}^\infty\frac{1}{(n+1)\sqrt{n}} \lt 2$,"The question goes out as follows: Prove the inequality $$\sum_{n=1}^\infty\frac{1}{(n+1)\sqrt{n}} \lt 2$$ The solution is as follows: \begin{align*}
\sum_{n=1}^\infty\frac{1}{(n+1)\sqrt{n}} &= 1 + \sum_{n=2}^\infty\frac{\sqrt{n}-\sqrt{n-1}}{n} \\
&\lt 1 + \sum_{n=2}^\infty\frac{\sqrt{n}-\sqrt{n-1}}{\sqrt{n}\sqrt{n-1}} \\
&= 1 + \sum_{n=2}^\infty\frac{1}{\sqrt{n-1}}-\frac{1}{\sqrt{n}} = 2.
\end{align*} But here's the thing, I didn't understand the first step itself. The other steps were fine, but I didn't get the first step. Can anyone help me out here? Thanks in advance! P.S. It's worth mentioning that $\frac{1}{(n+1)\sqrt{n}} = \frac{\sqrt{n}}{n}-\frac{\sqrt{n}}{n+1}$","['inequality', 'sequences-and-series']"
4896094,Clarification on the Definition of Bounded Sets in Topological Vector Spaces,"In Walter Rudin's ""Functional Analysis"" book, the definition of a bounded set in a topological space is given as follows: A subset $E$ of a topological vector space $X$ is said to be bounded if to every neighbourhood $V$ of $0$ in $X$ corresponds a number $s > 0$ such that $E \subset tV$ for every $t > s$ . I'm having difficulty understanding why the condition ""for every $t > s$ "" is necessary in this definition. It seems to me that requiring $E$ to be contained in $tV$ for some $t > s$ would suffice to characterize boundedness","['general-topology', 'functional-analysis']"
4896103,Finding free subgroup $F_2$ in the free product $\frac{\mathbb{Z}}{5\mathbb{Z}} * \frac{\mathbb{Z}}{6\mathbb{Z}}$,"Is there any free group isomorphic to $F_2$ contained in the free product group $\frac{\mathbb{Z}}{5 \mathbb{Z}}* \frac{\mathbb{Z}}{6 \mathbb{Z}}?$ Let $\frac{\mathbb{Z}}{5\mathbb{Z}}= \langle a \mid a^5=1 \rangle$ and $\frac{\mathbb{Z}}{6\mathbb{Z}}= \langle b \mid b^6=1 \rangle$ . Clearly $ab,ba$ these two elements  are of infinite order in the free product. I want to show that the group generated by $\langle ab, ba\rangle$ is a free subgroup of the free product. How to show that?","['combinatorial-group-theory', 'abstract-algebra', 'free-groups', 'free-product', 'group-theory']"
4896106,Generation of arbitrary integer sequences,"I want to know that if I consider a positive real number $\theta>0$ and the procedure $$b_1=\theta$$ $$b_n = \lfloor b_{n-1} \rfloor(b_{n-1}-\lfloor b_{n-1}\rfloor+1)$$ with $a_n:=\lfloor b_n\rfloor$ , which nondecreasing integer sequences $\{a_n\}$ can be generated this way (i.e. there exists a $\theta$ that works.) For example, for the Fibonacci sequence $\theta = 2.956938891377988\ldots$ works. This problem was inspired by Project Euler Problem 751. I know, for example, that $\{2, 4, 6, 8, \ldots\}$ does not work, since we need $2<\theta\leq 3$ , but then no $\theta$ works to give us exactly 4.","['sequences-and-series', 'real-analysis']"
4896128,Homogeneous notation for differential forms on projective space,"Let $z_0, \dots, z_n$ be homogeneous coordinates for $\Bbb{P}^n$ , say over $\mathbb{C}$ , and let $f$ be a homogeneous polynomial of degree $n + 1$ . I have seen the following notation for differential $n$ -form on projective space (Huybrechts Complex Geometry Exercise 2.4.8): $$\sum_{j = 0}^n (-1)^{j} \frac{z_j}{f} \; dz_0 \wedge \dots \wedge \widehat{dz_j} \wedge \dots \wedge dz_n$$ Intuitively, it makes sense that this should give a well-defined (necessarily meromorphic) differential form on projective space, since the scalar $\Bbb{C}^\times$ -action cancels out. But this does not seem rigorous: the ""correct"" way to understand a global section of a vector bundle is to write it down in local coordinate charts, compatible with the transition functions between charts. I am unsure what the meromorphic differential form I have written above is supposed to be in the standard affine coordinate charts. For example, my intuition would be that $\frac{z_j}{f} dz_0 \wedge \dots \wedge \widehat{dz_j} \wedge \dots \wedge dz_n$ should vanish on all coordinate charts besides $U_j$ , since there would be a $d1 = 0$ term in each such chart, but this cannot be right because then this form would be zero almost everywhere, hence zero. Someone has already asked this about this exact exercise ( How should the notation in this description of a global section of a line bundle in Huybrechts' complex geometry book be interpreted? ), but the answer there does not adequately address the issue I observe and I am not sure it is a completely correct interpretation. It maybe seems that this form is only well-defined as a sum, and each individual term does not work on its own. What does this ""homogeneous differential form"" notation actually mean?","['complex-geometry', 'differential-topology', 'algebraic-geometry']"
4896216,Prove that $(f+g)(x)<t$ ($t\in\mathbb{R}$) holds if and only if there is a rational number $r$ such that $f(x)<r$ and $g(x)<t-r$.,"I got stuck on this question: Prove that $(f+g)(x)<t$ , $t\in\mathbb{R}$ , holds if and only if there is a rational number $r$ such that $f(x)<r$ and $g(x)<t-r$ . I think one direction is trivial: If there is a rational number $r$ such that $f(x)<r$ and $g(x)<t-r$ , then $(f+g)(x) = f(x) + g(x) < r + (t-r) = t$ . However, I am baffled by the other direction. I tried to use proof by contradiction: Suppose that $(f+g)(x) < t$ holds. Assume to the contrary that for all rational number $r$ we have either $f(x)\geq r$ and $g(x)<t-r$ , $f(x)< r$ and $g(x)\geq t-r$ , or $f(x)\geq r$ and $g(x)\geq t-r$ . Apparently, the third scenrio would reach a contradiction. However, I couldn't figure out how the other two cases would lead to a contradiction. I noticed that I haven't used the condition that $r$ is rational, but I can't see how this condition would kick in. Moreover, as a follow-up question, how does that statement imply that \begin{align*}
\{x\in A:(f+g)(x)<t\} = \bigcup_{r\in\mathbb{Q}}\left(\{x\in A:f(x)<r\}\bigcap\{x\in A:g(x)<t-r\}\right),
\end{align*} where $A$ is the domain of $f$ and $g$ ? Could someone please help me out? Thanks a lot in advance! Reference : Measure Theory by Donald Cohn Proposition 2.1.6.","['analysis', 'real-analysis', 'functions', 'inequality', 'rational-numbers']"
4896231,Primes of the form $F(a^{k+1})/F(a^k)$,"Letting $F(n)$ be the $n$ 'th Fibonacci number , for what $a$ and $k\ge 1$ is $F(a^{k+1})/F(a^k)$ prime? I know of just $6$ examples: $$\eqalign{
3 &= F(2^2)/F(2^1)\cr
7 &= F(2^3)/F(2^2)\cr
17 &= F(3^2)/F(3^1)\cr
47 &= F(2^4)/F(2^3)\cr
2207 &= F(2^5)/F(2^4)\cr
97415813466381445596089 &= F(11^2)/F(11^1)\cr}$$ If it exists, any other example is greater than $10^{20000}$ .
It's easy to show that $a$ must be prime. EDIT: A similar question can be asked about Lucas numbers, where we need $a$ to be odd.  The only examples I have found are: $$\eqalign{19 &=L(3^2)/L(3^1)\cr
5779 &=L(3^3)/L(3^2)\cr
599786069 &=L(7^2)/L(7^1)\cr
97420733208491869044199 &=L(11^2)/L(11^1)
}$$","['number-theory', 'fibonacci-numbers']"
4896232,"Which is the smallest integer $c>0$ making $a\cdot (b+c)$ a pairing function for positive integer $a,b$ with $a \le a_{max}$ and $b \le b_{max}$?","The goal is to generate $a_{max} \cdot b_{max}$ different numbers with $$a\cdot (b+c)$$ using an integer offset $c>0$ as small as possible. We know $a \in [1,a_{max}]$ , and $b \in[1,b_{max}]$ I'm looking for a formula computing the smallest suitable $c_{min}$ , even for larger numbers ( $\approx10^{50}$ ). No proof required. An upper bound for $c$ is $c_{max} = a \cdot b$ If there is no formula for the absolute minimum. How much smaller can we make this upper bound? Here are some example values I computed by trying out every possible value $a$ $b$ $c_{min}$ $a$ $b$ $c_{min}$ 2 2 1 2 4 3 3 3 2 3 9 14 4 4 6 4 9 18 5 5 9 10 5 18 8 8 30 10 8 36 10 10 49 5 10 24 20 20 256 5 20 64 32 32 702 16 32 351 47 47 1640 47 113 3960 64 64 3192 47 64 2214 100 100 8190 20 100 1638 113 113 10506 113 47 3536 Bonus question: Minimal $c>0$ for $$a\cdot (c-b)$$","['functions', 'natural-numbers']"
4896239,When can $X_n=O_p(a_n)$ imply $\mathbb E[X_n] = O(a_n)$?,"Let $(X_n)_{n\in\mathbb N}\subset \mathbb R^d$ be a sequence of uniformly bounded random variables which converges almost surely to $0$ , and is stochastically dominated by a sequence of positive numbers $(a_n)\to 0$ , that is, $$\|X_n\|\le B\ \ \forall n,\ \text{ and }\ \forall\varepsilon>0, \exists M,N>0 : \mathbb P\left(\left|{\frac{\|X_n\|}{a_n}}\right|>M\right)<\varepsilon ,\;\forall \;n>N. \tag1$$ (Here $\|\cdot\|$ denotes the usual Euclidean norm, but clearly this doesn't matter.) First, almost sure convergence of $(X_n)$ implies convergence in probability, and since the sequence is uniformly bounded, it is uniformly integrable and therefore converges in $L^1$ to $0$ , that is $\mathbb E[\|X_n\|]\to 0 $ as $n\to\infty$ . My question is : what are some sufficient, non-trivial conditions I can impose on $(X_n)$ so that $\mathbb E[\|X_n\|]$ is dominated by (a function of) $a_n$ ? That is, under what conditions can we have the following $$\limsup_{n\to\infty}\frac{\mathbb E[\|X_n\|]}{a_n} <\infty \tag2 $$ (or in the same spirit, one might replace $(a_n)$ in $(2)$ above by $a_n'\equiv f(a_n)$ for some non-decreasing function $f$ ) ? The discussion in a closely related post suggests to assume that $\frac{\left\|X_n\right\|}{a_n} \mathbf{1}_{\left\{\left\|X_n\right\| > c a_n\right\}}$ is uniformly dominated by an integrable r.v. $X$ and apply reverse Fatou's lemma, but this is quite hard to verify (and doesn't seem to hold for my problem), so I'm hoping that someone might come up with another idea. I believe that the uniform boundedness and almost sure convergence assumptions on $X_n$ should help, but so far I do not see how to exploit them... Thanks in advance for any help !","['asymptotics', 'real-analysis', 'expected-value', 'convergence-divergence', 'probability-theory']"
4896250,Why do we want topologies to be closed under both finite and infinite union but not infinite intersection? [duplicate],"This question already has answers here : Why do we only allow for finite intersections in the definition of a topology? (2 answers) Closed 3 months ago . So I recently read the definition of a topological space and a topology from a book, and according to it, the topology must be closed under finite and infinite union, but it must only be closed under finite intersection. Clearly there is difference between requiring topologies to be closed under finite union and requiring them to be closed under both finite and infinite union, this is because imagine the topology is made up of infinite sets and every set sort of ""adds"" a new element to the infinite union, then there won't be any finite union that is equal to the infinite one, and thus some topologies might be closed under finite union only, but not under infinite union. At first I thought maybe the reason why they definition doesn't explicitly say that it should be closed under infinite intersection was because it wouldn't change anything, as in, every topology that is closed under finite intersection will also be closed under infinite intersection, however I think I found a counterexample. Consider the set $X = \mathbb{N}$ and the topology $\tau = \{\emptyset, \mathbb{N}, (\mathbb{N} \setminus \mathbb{N}_2) \cup \{1\}, (\mathbb{N} \setminus \mathbb{N}_3) \cup \{1\}, (\mathbb{N} \setminus \mathbb{N}_4) \cup \{1\}, ... \}$ where we define $\mathbb{N}_i$ as the set $\{1, 2, 3,..., i\}$ . This is a topology by the definition the book uses since it contains $X$ and $\emptyset$ and it's also closed under finite and infinite union as well as closed under finite intersection, however it is not closed under infinite intersection since it doesn't contain the set $\{1\}$ . So I have two questions, is my reasoning so far correct? and also, if it is, then why do we want the topology I showed before to be a topology and why not make a stronger requirement that it also needs to be closed under infinite intersection?","['elementary-set-theory', 'general-topology']"
4896291,If $F$ attains its local minimum at $x=0$ then $F(x)-F(0)\geq\xi x$,"Let $F:\mathbb{R}\to\mathbb{R}$ be the convex mapping. Prove that
there exists derivatives $F'(0+), F'(0-)$ and if $F$ attains its local minimum at $x=0$ then $$F(x)-F(0)\geq \xi x,\quad\forall x,\xi\in[F'(0-),F'(0+)].$$ I have proved that there exists $F'(0+), F'(0-)$ . I'm stuck at proving if $F$ attains its local minimum at $x=0$ then $F(x)-F(0)\geq \xi x,\quad\forall x,\xi\in[F'(0-),F'(0+)].$ I know that if $F$ attains its local minimum at $x=0$ then $F(x)>F(0)\quad\forall V(0,\epsilon)\setminus\{0\}$ , where $V(0,\epsilon)$ is the neighborhood of $0$ . Could someone help me to deal with the rest of the problem? Thanks in advance!","['multivariable-calculus', 'calculus', 'derivatives', 'real-analysis']"
4896374,Find the values of $b$ for which $f(x)=x^3+bx^2+3x+\sin(x)$ is bijective,"Find the values of $b$ for which $f(x)=x^3+bx^2+3x+\sin(x)$ is bijective. As we know $f(x)$ is surjective, the only task left to prove it bijective is to prove that $f(x)$ is strictly monotonic (which is equivalent to proving it injective because it's already continuous), which means that the derivative shouldn't change sign, and also that the derivative is non-zero except at isolated points. My attempt:
So I tried to prove that derivative is either fully negative and fully positive for all values of $x$ . $$f'(x) = 3x^2+2bx+3+\cos(x)$$ I graphed the derivative on desmos and figured out that $f'(x)$ can't be negative for all values of $x$ but it can be positive for all values of $x$ between an interval of $b$ values, which from the graph comes out to be $[-3.2,3.2]$ Thus the original question can be reduced to for what values of $b$ is $f'(x)$ positive for all $x$ values, i.e. the graph lies above the $x$ -axis. I got the answer from the graph but I can't solve the question to the answer. I tried just randomly with discriminant but it is pointless as the derivative of the function is not a polynomial obviously. I might have used the words postive and negative a bit more vaguely (including points at which it is 0) because as the derivative is zero only at distinct points, it is of no issue.","['calculus', 'functions', 'derivatives', 'analysis']"
4896388,"How to prove an adapted Feynman Kac Formula for $v_t + \frac{1}2 \sigma ^2 (t,y) v_{yy} + b(t,y) v_y - \delta(t,y) v + h(y) = 0$ using SDE techniques?","I am considering a generalisation of the Feynman-Kac (FK) Theorem. The traditional FK Theorem states the following: Fix a filtered probability space $(\Omega, \mathscr{F}, (\mathscr{F}_t), \mathbb{P}^*)$ supporting a one dimensional standard Brownian Motion $W^*$ . If a process $Y$ satisfies the SDE $$dY_t = b(t,Y_t)dt + \sigma (t, Y_t) dW_t^*$$ Then suppose $w(t,y) \in C^{1,2}$ and solves the PDE: $$w_t + \frac{1}{2} \sigma ^2 (t,y) w_{yy} + b(t,y) w_y - \delta(t,y) w = 0$$ with the terminal condition: $$w(T,y) = f(y)$$ Then under suitable assumptions (omitted), the function $w$ has the expression: $$ w(t,Y_t) = \mathbb{E}^{\mathbb{P}^*}[ \exp ( - \int _t^T \delta(s,Y_s) ds) f(Y_T) | \mathscr{F}_t]$$ Having encountered this theorem, I have come across a generalisation of the above where we now consider the function $v \in C^{1,2}$ such that $v$ solves the PDE: $$v_t + \frac{1}{2} \sigma ^2 (t,y) v_{yy} + b(t,y) v_y - \delta(t,y) v \space \bf{ + \space h(y)} = 0$$ with the terminal condition: $$v(T,y) = f(y)$$ This has the solution (again under omitted conditions) which can be shown fairly straightforwardly from a ""PDE perspective"": $$ v(t,Y_t) = \mathbb{E}^{\mathbb{P}^*}[ \int _t^T \exp ( - \int _t^s \delta(u,Y_u) du) h(Y_s)ds + \exp ( \int _t^T \delta(s,Y_s)ds) f(Y_T) \space | \space \mathscr{F}_t]$$ However, from an ""SDE perspective"" I am struggling to prove that this is the solution. Here is my partial solution: By Ito's Formula, we have: $$dv(t,Y_t) = (v_t + b(t,Y_t)v_y + \frac{1}{2} \sigma ^2 (t,Y_t)v_{yy}) dt + \sigma (t,Y_t) v_y dW_t^*$$ And then by substituting in from the PDE in that $v$ solves, we see that this simplifies to: $$ dv(t,Y_t) = (\delta (t, Y_t) v(t,Y_t) - h(y) )dt + \sigma(t,Y_t) v_y dW_t^*$$ Now by integrating we get that: $$v(T,Y_T) - v(t,Y_t) = \int _t^T  (\delta (s, Y_s) v(s,Y_s) - h(Y_s) )ds + \int _t^T \sigma(s,Y_s) v_y dW_s^*$$ And by rearranging for $v(t,Y_t)$ and taking conditional expectations, we get that: $$ v(t,Y_t) = \mathbb{E}^{\mathbb{P}^*}[f(Y_t) - \int _t^T  (\delta (s, Y_s) v(s,Y_s) - h(Y_s) )ds - \int _t^T \sigma(s,Y_s) v_y dW_s^* | F_t ]$$ And finally, since the conditional expectation of a stochastic integral with respect to a Brownian Motion (under assumed regularity conditions) is $0$ , we get: $$v(t,Y_t) = \mathbb{E}^{\mathbb{P}^*}[f(Y_t) - \int _t^T  (\delta (s, Y_s) v(s,Y_s) - h(Y_s) )ds | F_t ]$$ This is, of course, not a satisfactory solution. For one, it does not match the desired form. And secondly, and more importantly, the right-hand side contains $v(s,Y_s)$ inside the integral and so this is not an explicit solution. I would be grateful for any assistance in progressing from this point. Note: as mentioned above, I am aware that this can be solved using PDE methods, however, I am specifically interested in continuing from this particular approach; so other answers on this site that solve this using those methods do not answer my question.","['stochastic-differential-equations', 'partial-differential-equations', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4896400,Rigorous Mathematical foundations of Machine Learning / Deep Learning / Neural Networks,"I am an Engineering Graduate (with a strong background in Probability/Measure Theory, Linear Algebra and Calculus) wanting to dig deep into Deep Learning and Neural Networks, and I'm looking for mathematically thorough and rigurous (almost pedantic) material and books to go through the material, but I don't seem to find something of my liking among all the myriad of hype-train books out there considering how popular Machine Learning has become in the last years. Even the rather non-hands-on books about it seem to be too hands-on to me. I have both Pattern Recognition and Machine Learning by Bishop , which I started working through (about 150 Pages), as well as Deep Learning by Goodfellow (which I only skimmed through), but I am not happy with either. They both seem to try to cover too much material, with little rigorousness and detail, getting lost in specificalities. I am only speaking out of intuition here, but I feel like due to the assumption that the average reader lacks real depth in the areas of Probability, Linear Algebra and Calculus (which is clear by the sloppiness of the use of them), I feel like there's the need for too much explanation. What I'm looking for is a more concise, but more mathematically precise book on the fundamentals of Deep Learning / Machine Learning that assumes a strong and formal knowledge of Linear Algebra, Probability Theory (Kolmogorov, Probability Spaces, etc.), and Calculus where I can draw my own (mathematical) conclusions and without too much specific gibberish (e.g. I don't need 5 examples, graphs and intuitive explanations on the difference between bias and variance, I just need a rigorous definition of both, and a few important Theorems around them which I can work with). My reasoning here is that the field is so complex already and expanding in an exponential manner on so many directions, and understanding concepts like regularization and optimization seems so daunting, because of the lack of mathematical abstractions behind it, so it seems that every application needs its own interpretation. As experience has showed me, this happens when there's a lack of fundamental mathematical knowledge that can help extrapolate into other examples without the explicit need to understand the details of it. An analogous example is say with systems of differential equations: You might need to understand a few examples (e.g. Romeo and Juliet/Love affairs https://ai.stanford.edu/~rajatr/articles/SS_love_dEq.pdf ) but then it suffices to be able to understand those systems as general mathematical constructs and how to solve them/approach them without having to understand intuitively every single example a priori. Thank you","['statistics', 'neural-networks', 'machine-learning', 'linear-algebra', 'probability-theory']"
4896434,Number of trials required to break even with a certain probability,"I made a problem myself while reading an old econometrics book, but having a hard time solving it. A single trial of a game is as follows. 1. You pay L dollars to start the game. 2. Toss a coin until you get heads. (The probability for heads and tails are equal.) 3. You earn $2^m$ dollars, where $m$ is the number of tails before heads. $I(n)$ is the income you get from $n$ trials. $I(n)=\sum_{i=1}^n (G_i - L)$ , where $G_i$ is the money you earn from the $i$ -th game. If $L=10000$ , what is the minimum value of $n$ for $P[I(n)\ge 0]\ge 1/2$ ? The expected value of income from a single trial, $$\begin{align}E[I(1)]&=\frac12(2^0-L)+\frac1{2^2}(2^1-L)+\frac1{2^3}(2^2-L)+ \ldots\\&=-L\left( \frac12+\frac1{2^2}+\frac1{2^3} + \ldots\right)+\frac12+\frac12+ \ldots\\&=\infty\end{align}$$ However, $P[I(1)\ge0]=1/2^{15}+1/2^{16}+1/2^{17}+\ldots=1/2^{14}$ Intuitionally, since a single trial has an expectation of infinity, you can expect $$\lim_{n\to\infty}P[I(n)\ge0]=1$$ I ran a simulation, and got the expected result. The number of samples of $I(n)$ was $10000$ . (L = 10)
n      , P[I(n) >= 0]
      1, 0.0646
      2, 0.0787
      4, 0.0867
      8, 0.0984
     16, 0.1058
     32, 0.1206
     64, 0.1327
    128, 0.1350
    256, 0.1426
    512, 0.1583
   1024, 0.1781
   2048, 0.1962
   4096, 0.2266
   8192, 0.2529
  16384, 0.2858
  32768, 0.3344
  65536, 0.3909
 131072, 0.4659
 262144, 0.5602
 524288, 0.6633
1048576, 0.7953

(L = 20)
n      , P[I(n) >= 0]
      1, 0.0300
      2, 0.0354
      4, 0.0374
      8, 0.0378
     16, 0.0399
     32, 0.0438
     64, 0.0481
    128, 0.0537
    256, 0.0565
    512, 0.0566
   1024, 0.0611
   2048, 0.0646
   4096, 0.0650
   8192, 0.0632
  16384, 0.0654
  32768, 0.0647
  65536, 0.0690
 131072, 0.0755
 262144, 0.0728
 524288, 0.0754
1048576, 0.0773 You can see $P$ converges very slowly with only $L=20$ , and we need to calculate $P$ for $L=10000$ . It doesn't seem possible to get the result by simulation within reasonable time. It seems best to get a simple algebraic solution for $f(n)=P[I(n)\ge0]$ , but is it possible? What are possible approaches to solve this problem?","['gambling', 'statistics', 'probability']"
4896440,Uniform Integrability and Proving martingale of Poisson product process,"So I have been stuck on the following question that I stumbled across in a textbook. It is trying to show the following product process is a martingale. $$M_n = n! \prod_{k=1}^{n} X_k \ \text{where } X_{k} \sim \text{Poi}(1/k) \text{ and } X_k \text{ are independent} $$ I know I have to prove adaptedness, integrability and the martingale equality but I am struggling to show the integrability as I have the working out below that I cannot seem to bound above to prove $ \mathbb{E}|M_n| < \infty$ My working is as below, $$
\mathbb{E}|M_n| = \mathbb{E}\left| n! \prod_{k=1}^n X_k \right| = n! \cdot  \mathbb{E} \prod_{k=1}^{n} X_k = n! \prod_{k=1}^{n} \mathbb{E} X_k = n!  \cdot (\mathbb{E} X_k)^n =\dfrac{n!}{k^n} < \  ? $$ Any help would be greatly appreciated. However a follow up to this is also I am unsure how to determine whether M is a uniformly integrable martingale or not. I have tried using the Doob-Kolmogorov inequality but it breaks down when I have attempted to show it holds for L-1 norm. Below is my attempt at trying to show that $M_n$ converges almost surely to a limit and thus is UI. use the Doob-Kolmogorov Inequality for  martingales but for $p=1$ , $$ \mathbb{P}\left( \max_{k \leq n} | M_{k} - 0 | > b  \right) \leq \dfrac{\mathbb{E}|M_n|}{b} \quad \text{ for } b > 0.
$$ However we know that $\mathbb{E}|M_n| = \frac{n!}{k^n}$ therefore taking the limit of the above inequality and take $n \to \infty$ , $$\lim_{n \to \infty} \mathbb{P} \left( \max_{k \leq n} | M_{k} - 0 | > b \right) \leq \lim_{n \to \infty} \dfrac{\mathbb{E}|M_n|}{b} = \lim_{n \to \infty} \dfrac{n!}{k^n b} = 0 $$ However, I dont think it works in this case as the limit is not 0. Any help on this would be great too!","['martingales', 'statistics', 'probability', 'real-analysis']"
4896457,"At every step, add $k$ previous terms, then remove all zeros. Then $k=3$ leads to a $300,056,874$-cycle.","Recently, I've been interested with the sequence of the form: $$a_n =\text{Zr}(\sum_{j=1}^k a_{n-j}),\ \ n≥k$$ $$a_n=1,\ \ 0≤n<k$$ Where $\text{Zr}(x)$ is just $x$ with $0$ removed in its digits (i.e. zero remover function). For $k=2$ , the sequence enters a cycle that has a period of $912$ . For $k=3$ , the sequence enters a very long cycle that have period of $300,056,874$ (I used the Tortoise and Hare algorithm to find this cycle. The naive algorithm would be very memory intensive), and here's the beginning of that cycle: $$2847, 26331, 5851,... $$ Only the first 3 terms of the cycle is needed to reproduce the entire cycle. This cycle starts at $n=208,666,297$ . For $k=4$ , it is unknown if it enters a cycle (feel free to try it, but i have ran the Tortoise and Hare program for $10,000,000,000$ iterations and no cycle have been found so far. ), but we expect it to cycle as the expected growth factor of the number of digits is $0.9$ . I have two questions: Let $C_k$ be the size of the cycle that the sequence will enter, given $k$ . Can anyone provide a way to calculate $C_k$ (without having to do the computationally expensive task of actually going through the sequence until it cycles) or at least come up with a good approximation for $C_k$ ? Can anybody prove this sequence to cycle for all values of $k$ ?","['recreational-mathematics', 'recurrence-relations', 'discrete-mathematics']"
4896483,Expected value of fastest career hat-trick?,"This is a sports variation of line segment (rope, stick...) divided into $n+1$ subsegments by $n$ uniformly distributed points. Assuming goals are uniformly distributed among player's career, what's the expected value of fastest hat-trick (3 goals) as measured by time between goals? (Real hat-tricks count only if they happen within single match, of course, but let's ignore this detail, it adds too much complexity to the problem). Here is the solution for fastest brace (2 goals): According to this answer expected value of shortest subsegment length: $$
\mathbb{E}[shortest\_period\_without\_goals] = \frac{career\_matches\ {\small(or\ career\_minutes)}}{(career\_goals+1)^2}
$$ However, a brace can't be first or last subsegment, because there are no goals at the career beginning and end, so out of $n(n-1)$ permutations of $2$ shortest subsegments indices: The shortest subsegment is not first or last: $(n-2)(n-1)$ permutations; The shortest subsegment is first or last, but 2 nd shortest  is not: $2(n-2)$ permutations; The shortest and 2 nd shortest subsegments are first and last: $2$ permutations. After simplifying we finally get: $$
\mathbb{E}[fastest\_brace\_time] = \frac{career\_matches\ {\small(or\ career\_minutes)}}{career\_goals^2-1 }
$$ But what about fastest hat-trick ? I mean, the line segment $\overline{A_0A_{n+1}}$ of length $1$ is divided to $n+1$ parts by $n$ uniformly distributed points: $\{{A_m|1\leq m \leq n, A_0\leq A_m\leq A_{m+1}\leq A_{n+1}}\}$ .
Then we take $n-2$ subsegments (to avoid $A_0$ and $A_{n+1}$ which are not goals) with exactly $1$ point inside $\{{\overline{A_{m}A_{m+2}}|1\leq m \leq n-2}\}$ and choose the shortest. What will be an expected value of its length? Update : I wrote a small program that uses Monte Carlo methods to get some practical results. Probably they can help someone to find an analytic solution or/and verify it. Here are the expected values of $\{{\overline{A_{m}A_{m+2}}|0\leq m \leq n-1}\}$ lengths, sorted from shortest to longest: # Points $\mathbb{E}[1^{st}]$ $\mathbb{E}[2^{nd}]$ $\mathbb{E}[3^{rd}]$ $\mathbb{E}[4^{th}]$ $\mathbb{E}[5^{th}]$ $\mathbb{E}[6^{th}]$ $\mathbb{E}[7^{th}]$ 2 $\frac{3}{6}$ $\frac{5}{6}$ 3 $\frac{9}{32}$ $\frac{16}{32}$ $\frac{23}{32}$ 4 $\frac{34}{180}$ $\frac{55}{180}$ $\frac{85}{180}$ $\frac{114}{180}$ 5 $\frac{530}{3888}$ $\frac{850}{3888}$ $\frac{1185}{3888}$ $\frac{1708}{3888}$ $\frac{2207}{3888}$ 6 $\frac{1890}{18144}$ $\frac{2977}{18144}$ $\frac{4121}{18144}$ $\frac{5355}{18144}$ $\frac{7408}{18144}$ $\frac{9352}{18144}$ 7 $\frac{16937}{204800}$ $\frac{3309}{25600}$ $\frac{967759}{5529600}$ $\frac{19571}{86400}$ $\frac{1564889}{5529600}$ $\frac{52621}{138240}$ $\frac{11621}{24576}$ And here are the probabilistic estimation of expected values for shortest and longest segments (not all digits are verified): # Points $10$ $20$ $30$ $40$ $50$ $100$ $200$ $300$ $500$ $1000$ $\mathbb{E}[shortest]$ $\frac{1}{20.766}$ $\frac{1}{60.382}$ $\frac{1}{113.16}$ $\frac{1}{176.71}$ $\frac{1}{249.59}$ $\frac{1}{727.46}$ $\frac{1}{2107.8}$ $\frac{1}{3917.2}$ $\frac{1}{8529}$ $\frac{1}{24428}$ $\mathbb{E}[longest]$ $\small0.38163$ $\small0.23944$ $\small0.17803$ $\small0.1431$ $\small0.1203$ $\small0.0689$ $\small0.0387$ $\small0.0274$ $\small0.0176$ $\small0.0096$ It seems like $lim_{n\to\infty} \mathbb{E}[shortest]$ is close to $\mathcal{O}(1/n\sqrt{n})$ , and it makes sense: like in birthday problem , for most subsegment permutations at least 1 pair of $\mathcal{O}(\sqrt{n})$ shortest subsegments will be adjacent and the expected length of $\sqrt{n}^{th}$ shortest subsegment is also $\mathcal{O}(1/n\sqrt{n})$ . Note : those numbers can be used to answer the initial question only if they are multiplied by ${career\_matches\ {\small(or\ career\_minutes)}\ between\_first\_and\_last\_goals}$ .","['expected-value', 'uniform-distribution', 'probability']"
4896508,Why $E$ shold be open in Baby Rudin theorem 9.19,"I didn't understand why $E$ should be open or what part of the proof it is used Theorem 5.19 is If $f$ is a continuous function from $[a,b]$ to $R^k$ and $f$ is differentiable in $(a,b)$ then $\left|\frac{f(b)-f(a)}{b-a}\right|\leq |f'(c)|\,\,\text{for some $c\in(a,b)$}.$","['elementary-set-theory', 'proof-explanation', 'real-analysis']"
4896553,"A false ""proof"" that record setting events are dependent","Let $\{X_i\}$ be a sequence of i.i.d continuous RV.  Call $i$ record-setting if $$X_i > \max_{1 \leq j < i} X_j.$$ It is well - established on math.SE and elsewhere that the events "" $X_n$ is record-setting"" and "" $X_{n+1}$ is record-setting"" are independent.  Below is a ""proof"" that they are indeed dependent.  Where's the error? Let $$M_{n} := \max\limits_{1 \leq j \leq n} X_j.$$ The event "" $n$ and $n+1$ are both record-setting"" occurs if and only if: $X_n > M_{n-1}$ $X_{n+1} > M_{n-1}$ $X_{n+1} > X_n$ . By symmetry, event #1 has probability $1/n$ .  Since the $X$ are i.i.d, event #2 has probability $1/n$ as well, and is independent of event #1.  Finally, given events #1 and #2, the conditional probability of event #3 is $1/2$ .  Therefore the probability that both $n$ and $n+1$ are record-setters is $$P(A\cap B) = \frac 1 {2n^2} \neq \frac 1 {n(n+1)} = P(A)P(B)$$ and so the events are dependent.","['conditional-probability', 'fake-proofs', 'independence', 'probability']"
4896597,Approximation using polynomials with integer coefficients,"The classic Weierstrass Theorem states that the set of polynomials are dense in $C[0,1]$ equipped with $|| \cdot ||_{\infty}$ . Bernstein's proof of Weierstrass Theorem gives an explicit form of polynomials that approximate a given function $f \in C[0,1]$ . $$B_n^f(x) := \sum_{k=0}^n f(\frac kn) \binom nk x^k (1-x)^{n-k}.$$ If we want to approximate a function by using polynomials of integer coefficients, we must have $f(0)$ and $f(1)$ are both integers. Otherwise, $||f - p||_\infty \ge \varepsilon >0$ for all polynomials $p$ with integer coefficients. My question is if the converse is true, that is, if $f(0)$ and $f(1)$ are both integers, then can we approximate $f$ by a sequence $\{p_n\}_{n \in \mathbb{N}}$ of polynomials with integer coefficients, $||p_n - f||_\infty \to 0$ as $n \to \infty$ . This can be reduced to the case that $f(0) = f(1) = 0$ .
Assuming the statement is true for all $f \in C[0,1]$ with $f(0) = f(1) = 0$ .
Given $g \in C[0,1]$ with $g(0) = a, g(1) = b$ both integers, we can approximate $f(x) = g(x) - (b-a)x - a$ by a sequence $\{p_n\}$ with integer coefficients by assumption, since $f(0) = g(0) - a = 0$ and $f(1) = g(1) - b = 0$ .
Then $g(x)$ can be approximated by $p_n +(b-a)x + a$ .","['polynomials', 'analysis', 'real-analysis']"
4896725,Neighbourhoods via sequences,"I'm studying general topology and a question has come to my mind. By definition, a sequence of points in a topological space converges to a point if its values are eventually in every neighbourhood of the point. Rephrasing, the neighbourhoods of a point satisfy the condition that the values of every sequence converging to the point are eventually in them. I wonder if that is a characterising property of the neighbourhoods, viz. given a point and a subset of a topological space, if every sequence converging to the point eventually belongs to the subset, is it true that the subset is a neighbourhood of the point? If not (this feels having something to do with countability properties), I'd like to get to know about sufficient conditions on the spaces under which this happens and how to prove it. Disclaimer For a subset, to be a neighbourhood of a point I mean containing an open subset which contains the point.","['general-topology', 'geometry', 'analysis']"
4896781,Does a Poincaré inequality for a Markov process $X_t$ with invariant measure $μ$ infer a convergence rate of $\frac1t\int_0^tf(X_s){\rm d}s$ to $μf$?,"Let $(X_t)_{t\ge0}$ be a time-homogeneous shift-ergodic Markov process with transition semigroup $(\kappa_t)_{t\ge0}$ and invariant measure $\mu$ . One implication of a Poincaré inequality is a $L^2$ -contractivity $$\operatorname{Var}_\mu\left[\kappa_tf\right]\le e^{-2\lambda t}\operatorname{Var}_\mu[f]\;\;\;\text{for all }t\ge0\tag1$$ for all $f\in L^2(\mu)$ for some $\lambda>0$ . I see that we get an exponential convergence rate of $\kappa_tf$ to $\mu f$ in $L^2(\mu)$ as $t\to\infty$ for all $f\in L^2(\mu)$ . But can we also derive a convergence rate for the ergodic average $$A_tf:=\frac1t\int_0^tf(X_s)\:{\rm d}s\;\;\;\text{for }t>0$$ to $\mu f$ as $t\to\infty$ for $f\in L^2(\mu)$ ? Intuitively, this should be possible. For this, let $\operatorname P_x$ denote the measure wrt which $(X_t)_{t\ge0}$ is started at $x$ . I guess the corresponding convergence would then hold in $L^2(\operatorname P_\mu)$ , where $\operatorname P_\mu:=\int\mu({\rm d}x)\operatorname P_x$ . But then I would also wonder of what practical use this convergence rate would be (since considerations wrt $\operatorname P_\mu$ mean that the process is started at $\mu$ , which is clearly unrealistic in practice).","['monte-carlo', 'stochastic-analysis', 'stochastic-processes', 'markov-process', 'probability-theory']"
4896790,What is the $\inf$ and $\sup$ of the area of quadrilateral given its sides length?,"Now asked on MO here . Given the length of the sides of a quadrilateral $a,b,c,d$ the area of the quadrilateral is less than or equal to $\frac{(a+b+c+d)^2}{16}$ i.e it is an upper bound of the area of  any quadrilateral  with side length $a,b,c,d$ . A question that came to my mind is if $\frac{(a+b+c+d)^2}{16}$ is an upper bound Then the least upper bound exits so what is the infimum  and supremum of the area of quadrilateral (0 is an oblivious lower bound so the infimum   exists too)? I made a silly mistake thinking that $\frac{(a+b+c+d)^2}{16}$ is the supremum but it turns out I was wrong. If $a=b, \ c=d$ ""a kite"" or if $a=c, \   d=b$ ""a parallelogram"" then it is easy to prove the infimum  is $0$ and the supremum  is just $a^2$ when it is a rhombus. The infimum  depends on $a,b,c,d$ in this examples the side lengths are $19,13,7,4$ and the minimum area is around $9.5$ so it is not $0$ . Conjecture: If $a<b<c<d$ and the area of the quadrilateral  is $S$ then $\inf S $ is the area of the triangle $a,b,d-c$ which is $$\sqrt{\frac{a+b+d-c}{2}\cdot\frac{a-b+d-c}{2}\cdot\frac{-a+b+d-c}{2}\cdot\frac{a+b-d+c}{2}}$$ This also works when $a=b=c=d, \ \ \ a=b, \ c=d \ \ \  ,a=c, \ d=b $ . I couldn't rigorously proof that claim but I am pretty sure it is correct. In the previous example when side lengths are $19,13,7,4$ the naximum area is around $73.5$ but this time I have no Idea what kind of shape is that. How to determine  the infimum and the supremum  of the area given $a,b,c,d$ .","['euclidean-geometry', 'conjectures', 'area', 'geometry', 'supremum-and-infimum']"
4896807,"Analysis of an expression involving a function on $\mathbb R^n$. Related to limits, supremums and translations.","Let $n \in \mathbb N, \, 0 < \lambda < n$ and $1 \leqslant p < \infty$ , consider the usual Lebesgue measure on $\mathbb R^n$ and define the function $f \colon \mathbb R^n \to \mathbb R$ by $$ f(x) := |x|^{\frac{\lambda - n}{p}}, $$ for every $x \in \mathbb R^n \setminus \{0\}$ ( $f$ is defined arbitrarily at $x = 0$ ). Question. Prove that $$ \lim_{\xi \to 0} \, \, \sup_{x \in \mathbb R^n, r > 0} r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy \neq 0 $$ My attempt. Clearly, to prove the result it suffices to find a constant $c > 0$ such that $$ \sup_{x \in \mathbb R^n, \, r > 0} r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy \geqslant c, $$ for every $\xi \in \mathbb R^n \setminus \{0\}$ . This is an important observation because it allows us to work with any values of $x$ and $r$ that we find most adequate. I have tried the following: $$ \sup_{x \in \mathbb R^n, \, r > 0}r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy \geqslant \int_{B(0,1)} |f(y-\xi) - f(y)|^p \, dy, $$ where I have simply used the definition of supremum and took $x = 0$ and $r=1$ . Furthermore, applying a simple inequality, it follows that $$ \int_{B(0,1)} |f(y-\xi) - f(y)|^p \, dy \geqslant 2^{-p} \int_{B(0,1)} |f(y-\xi)|^p \, dy - \int_{B(0,1)} |f(y)|^p \, dy. $$ Now let us compute the integrals of the right-hand side. For the latter one, using the theory of integration of radial functions, we have that $$ \int_{B(0,1)} |f(y)|^p \, dy = \int_{B(0,1)} |y|^{\frac{\lambda - n}{p}} \, dy = \int_0^1 t^{\lambda -1} \, dt = \frac{1}{\lambda}. $$ Now, I am having a hard time computing the first integral. Does anyone have an idea on how to proceed? Note that the final lower bound should be strictly positive! Thanks for any help in advance.","['lebesgue-measure', 'lebesgue-integral', 'real-analysis', 'functional-analysis', 'limits']"
4896852,Proof $[c]^{\omega}=\{A\subseteq c : |A|=\omega\}=c$,"I would like to calculate the cardinality of the set $[c]^{\omega}=\{A\subseteq c : |A|=\omega\}$ , where $c$ is the cardinality of the continuum (cardinality of the Reals) and $\omega$ is the cardinality of the set donates numerous natural ones. I know the right answer is that $|[c]^{\omega}|=c$ .
I know how to prove it only for the case $[A]^n=\{B\subseteq A: |B|=n\}$ . But I don't know how I could prove it. Could anyone help? Could you help me prove that $|[c]^{\omega}|=c$ ?
thanks for the help!!!","['elementary-set-theory', 'set-theory']"
4896871,$|f(x)-f(y)| < |g(x) - g(y)|$ when $|f'| < |g'|$,"Let $f, g: \mathbb R \rightarrow \mathbb R$ be differentiable. Suppose that $|f'(x)| < |g'(x)|$ for all $x \in \mathbb R$ . The goal is to prove that $|f(x) - f(y)| < |g(x) - g(y)|$ for $x, y \in \mathbb R$ with $x \ne y$ . By the Darboux theorem, $g'$ must be either positive or negative everywhere. With extra assumptions that $f, g$ are absolutely continuous on every compact interval (e.g. holds when $f', g'$ are bounded on every compact interval), the desired result follows by the fundamental theorem of Calculus for Lebesgue integrals. Here are my questions: (1) Is the statement true without the additional assumptions? If so, what would be a proof? (2) In case it is true, is there a ""short"" proof of the statement without using any measure theory? I would appreciate any hint or reference.","['derivatives', 'real-analysis']"
4896872,Find the Laurent Series of $f(z) = \frac{1}{(z-6)(z+4)}$,"Can someone help me find the Laurent Series centered at $z=6$ for $f(z) = \frac{1}{(z-6)(z+4)}?$ I was going to start by rewriting this as $f(w) = \frac{1}{w(w+10)}$ so that I can find the Laurent Series centered at $w=0$ and then let $w=z-6$ in the end. However, I am not sure how to proceed from here.","['complex-analysis', 'laurent-series']"
4896925,What geometry is preserved by the translation maps on elliptic curves?,"Let $E$ be an elliptic curve (over some field).  For any $P \in E$ , there is a translation map $T_P: E \to E$ given by $Q \mapsto P+Q$ .  This map is rational (i.e. the coordinates of $T_P(Q)$ are rational functions of the coordinates of $Q$ ) and has rational inverse $T_{-P}$ .  So $T_P$ is a birational map. To what extent does the birationality of the translation maps tell us that the geometry of the curve at one point is the same as its geometry at any other point of the curve?  In particular, I am confused by the following: the intersection multiplicity of $E$ with the tangent at the point $\mathcal O$ at infinity is 3.  So why is it that the intersection multiplicity of the curve with the tangent at $P = T_P (\mathcal O)$ typically 2, not 3? (My reason for expecting that some geometry be preserved is seeing the analogous setup in Lie groups, where the translation maps are diffeomorphisms).","['algebraic-curves', 'elliptic-curves', 'plane-curves', 'algebraic-geometry', 'lie-groups']"
4896946,A problem using Chain Rule theorem,"Let $F(u,v)$ be of class $C^1(\mathbb{R}^2)$ and $z=z(x,y)$ be the
function defined by the equation $$F\left(x+\dfrac{z}{y},y+\dfrac{z}{x}\right)=0.$$ Prove that the
function $z(x,y)$ satisfies the following equation $$x\dfrac{\partial
 z}{\partial x}+y\dfrac{\partial z}{\partial y}=z-xy.$$ My attempt: Let $u=x+\dfrac{z}{y}, v=y+\dfrac{z}{x}$ . Then $F(u,v)=0$ Using the Chain rule, I have $$\dfrac{\partial F}{\partial u}.\dfrac{\partial u}{\partial x}+\dfrac{\partial F}{\partial v}.\dfrac{\partial v}{\partial x}=0 \quad (1)$$ Since $\dfrac{\partial u}{\partial x}=1+\dfrac{1}{y}.\dfrac{\partial z}{\partial x}, \dfrac{\partial v}{\partial x}=\dfrac{\partial z}{\partial x}.\dfrac{1}{x}-\dfrac{z}{x^2}$ , $(1)$ becomes $$ \dfrac{\partial F}{\partial u}\dfrac{\partial z}{\partial x}\dfrac{1}{y}+\dfrac{\partial F}{\partial v}\dfrac{\partial z}{\partial x}\dfrac{1}{x}+\dfrac{\partial F}{\partial u}+\dfrac{\partial F}{\partial v}\dfrac{-z}{x^2}=0$$ Similarly $$\dfrac{\partial F}{\partial u}\dfrac{\partial z}{\partial y}\dfrac{1}{y}+\dfrac{\partial F}{\partial v}\dfrac{\partial z}{\partial y}\dfrac{1}{x}+\dfrac{\partial F}{\partial v}+\dfrac{\partial F}{\partial u}\dfrac{-z}{y^2}=0 $$ So we have the following equations: $$\begin{cases} \dfrac{\partial F}{\partial u}\left(\dfrac{\partial z}{\partial y}\dfrac{1}{y}-\dfrac{z}{y^2}\right)=-\dfrac{\partial F}{\partial v}\left(\dfrac{\partial z}{\partial y}\dfrac{1}{x}+1\right) \quad (2)\\\\ \dfrac{\partial F}{\partial u}\left(\dfrac{\partial z}{\partial x}\dfrac{1}{y}+1\right)=-\dfrac{\partial F}{\partial v}\left(\dfrac{\partial z}{\partial x}\dfrac{1}{x}-\dfrac{z}{x^2}\right)\quad (3).\end{cases}$$ If we divide (2) with (3) side by side, we have the equality we wanna proof. But the problem is what if one of the terms is $0$ ? How can I deal with this case? Could someone help me or have other way to crack this problem? Thanks in advance!","['partial-derivative', 'multivariable-calculus', 'calculus', 'real-analysis']"
4897010,Show that $\frac{\text{quadratic}}{\text{quadratic}}$ with no common factors is many-to-one,"Let $${f(x)=\frac{ax^2+bx+c}{dx^2+ex+f}}$$ hence, $${f'(x)= \frac{(2ax+b)(dx^2+ex+f)-(2dx+e)(ax^2+bx+c)}{(dx^2+ex+f)^2}}.$$ If $f$ is not a continuously decreasing or increasing function then it is many-to-one. Therefore,on simplifying sign of $f'(x)$ , this depends on the sign of expression $${(ae-db)x^2 +2(af-dc)x+(bf-ec)}.$$ Hence, if ${(af-dc) ^2-(bf-ec) (ae-db) <0 }$ and ${ae \neq db}$ then $f$ is one-to-one. So we want to show that these two conditions must not be satisfied simultaneously, but I don't know how to show that, or is there another simpler way to prove this. My book says that a quadratic by quadratic function with no common factors is always a many-to-one function.","['calculus', 'functions', 'quadratics']"
4897082,Solving the summation $ S = \sum_{r=1}^{25} \frac{1}{z^{18r} + z^{9r} + 1} $,"Question: I am trying to solve the following summation problem involving complex numbers: Given a complex number $z$ satisfying $z^{26} = 1$ , and the summation: $$
S = \sum_{r=1}^{25} \frac{1}{z^{18r} + z^{9r} + 1}
$$ I've attempted to manipulate the terms inside the sum by expressing $z$ as $e^{i\frac{2\pi k}{26}}$ , where $k$ is an integer, but nothing fruitful came out. However, I'm unable to proceed further towards finding a solution. Could someone please provide guidance on how to approach this problem? Any insights or alternative methods for solving the summation would be greatly appreciated. Thank you! Answer given is $17$","['complex-analysis', 'summation', 'telescopic-series', 'complex-numbers']"
4897106,"How to calculate $\int(yy'' + (y')^2)\,dx$?","Context : I have the following second-order differential equation: $$yy'' + (y')^2 = x$$ I noticed that $$yy'' + (y')^2 = (yy')'_x \,\,\,\, (1)$$ So I integrate both sides with respect of $x$ : $$\int(yy'' + (y')^2)\,dx = \int{x\,dx}$$ I already know the answer for the left integral: $\int(yy'' + (y')^2)\,dx = yy' + C_1$ . But what if I didn't notice $(1)$ ? How would I solve the integral? I'm struggling for some reason..","['integration', 'derivatives', 'ordinary-differential-equations']"
