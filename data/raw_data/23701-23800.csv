question_id,title,body,tags
206890,"""The Egg:"" Bizarre behavior of the roots of a family of polynomials.","In this MO post , I ran into the following family of polynomials: $$f_n(x)=\sum_{m=0}^{n}\prod_{k=0}^{m-1}\frac{x^n-x^k}{x^m-x^k}.$$ In the context of the post, $x$ was a prime number, and $f_n(x)$ counted the number of subspaces of an $n$ -dimensional vector space over $GF(x)$ (which I was using to determine the number of subgroups of an elementary abelian group $E_{x^n}$ ). Anyway, while I was investigating asymptotic behavior of $f_n(x)$ in Mathematica, I got sidetracked and (just for fun) looked at the set of complex roots when I set $f_n(x)=0$ .  For $n=24$ , the plot looked like this: (The real and imaginary axes are from $-1$ to $1$ .) Surprised by the unusual symmetry of the solutions, I made the same plot for a few more values of $n$ .  Note the clearly defined ""tails"" (on the left when even, top and bottom when odd) and ""cusps"" (both sides). You can see that after approximately $n=60$ , the ""circle"" of solutions starts to expand into a band of solutions with a defined outline.  To fully absorb the weirdness of this, I animated the solutions from $n=2$ to $n=112$ . The following is the result: Pretty weird, right!?  Anyhow, here are my questions: First, has anybody ever seen anything at all like this before? What's up with those ""tails?""  They seem to occur only on even $n$ , and they are surely distinguishable from the rest of the solutions. Look how the ""enclosed"" solutions rotate as $n$ increases.  Why does this happen? [Explained in edits.] Anybody have any idea what happens to the solution set as $n\rightarrow \infty$ ? Thanks to @WillSawin, we now know that all the roots are contained in an annulus that converges to the unit circle , which is fantastic.  So, the final step in understanding the limit of the solution sets is figuring out what happens on the unit circle.  We can see from the animation that there are many gaps, particularly around certain roots of unity; however, they do appear to be closing. The natural question is, which points on the unit circle ""are roots in the limit""? In other words, what are the accumulation points of $\{z\left|z\right|^{-1}:z\in\mathbb{C}\text{ and }f_n(z)=0\}$ ? Is the set of accumulation points dense?  @NoahSnyder's heuristic of considering these as a random family of polynomials suggests it should be- at least, almost surely. These are polynomials in $\mathbb{Z}[x]$ .  Can anybody think of a way to rewrite the formula (perhaps recursively?) for the simplified polynomial, with no denominator?  If so, we could use the new formula to prove the series converges to a function on the unit disc, as well as cut computation time in half. [See edits for progress.] Does anybody know a numerical method specifically for finding roots of high degree polynomials?  Or any other way to efficiently compute solution sets for high $n$ ? [Thanks @Hooked!] Thanks everyone.  This may not turn out to be particularly mathematically profound, but it sure is neat . EDIT : Thanks to suggestions in the comments, I cranked up the working precision to maximum and recalculated the animation.  As Hurkyl and mercio suspected, the rotation was indeed a software artifact, and in fact evidently so was the thickening of the solution set.  The new animation looks like this: So, that solves one mystery: the rotation and inflation were caused by tiny roundoff errors in the computation.  With the image clearer, however, I see the behavior of the cusps more clearly.  Is there an explanation for the gradual accumulation of ""cusps"" around the roots of unity?  (Especially 1.) EDIT : Here is an animation $Arg(f_n)$ up to $n=30$ .  I think we can see from this that $f_n$ should converge to some function on the unit disk as $n\rightarrow \infty$ .  I'd love to include higher $n$ , but this was already rather computationally exhausting. Now, I've been tinkering and I may be onto something with respect to point $5$ (i.e. seeking a better formula for $f_n(x)$ ).  The folowing claims aren't proven yet, but I've checked each up to $n=100$ , and they seem inductively consistent.  Here denote $\displaystyle f_n(x)=\sum_{m}a_{n,m}x^m$ , so that $a_{n,m}\in \mathbb{Z}$ are the coefficients in the simplified expansion of $f_n(x)$ . First, I found $\text{deg}(f_n)=\text{deg}(f_{n-1})+\lfloor \frac{n}{2} \rfloor$ .  The solution to this recurrence relation is $$\text{deg}(f_n)=\frac{1}{2}\left({\left\lceil\frac{1-n}{2}\right\rceil}^2 -\left\lceil\frac{1-n}{2}\right\rceil+{\left\lfloor \frac{n}{2} \right\rfloor}^2 + \left\lfloor \frac{n}{2} \right\rfloor\right)=\left\lceil\frac{n^2}{4}\right\rceil.$$ If $f_n(x)$ has $r$ more coefficients than $f_{n-1}(x)$ , the leading $r$ coefficients are the same as the leading $r$ coefficients of $f_{n-2}(x)$ , pairwise. When $n>m$ , $a_{n,m}=a_{n-1,m}+\rho(m)$ , where $\rho(m)$ is the number of integer partitions of $m$ .  (This comes from observation, but I bet an actual proof could follow from some of the formulas here .)  For $n\leq m$ the $\rho(m)$ formula first fails at $n=m=6$ , and not before for some reason.  There is probably a simple correction term I'm not seeing - and whatever that term is, I bet it's what's causing those cusps. Anyhow, with this, we can make almost make a recursive relation for $a_{n,m}$ , $$a_{n,m}= \left\{
     \begin{array}{ll}
       a_{n-2,m+\left\lceil\frac{n-2}{2}\right\rceil^2-\left\lceil\frac{n}{2}\right\rceil^2} & : \text{deg}(f_{n-1}) < m \leq \text{deg}(f_n)\\
       a_{n-1,m}+\rho(m) & : m \leq \text{deg}(f_{n-1})  \text{ and } n > m \\
       ? & : m \leq \text{deg}(f_{n-1})  \text{ and } n \leq m
     \end{array}
   \right.
$$ but I can't figure out the last part yet. EDIT :
Someone pointed out to me that if we write $\lim_{n\rightarrow\infty}f_n(x)=\sum_{m=0}^\infty b_{m} x^m$ , then it appears that $f_n(x)=\sum_{m=0}^n b_m x^m + O(x^{n+1})$ .  The $b_m$ there seem to me to be relatively well approximated by the $\rho(m)$ formula, considering the correction term only applies for a finite number of recursions. So, if we have the coefficients up to an order of $O(x^{n+1})$ , we can at least prove the polynomials converge on the open unit disk, which the $Arg$ animation suggests is true.  (To be precise, it looks like $f_{2n}$ and $f_{2n+1}$ may have different limit functions, but I suspect the coefficients of both sequences will come from the same recursive formula.)  With this in mind, I put a bounty up for the correction term, since from that all the behavior will probably be explained. EDIT : The limit function proposed by Gottfriend and Aleks has the formal expression $$\lim_{n\rightarrow \infty}f_n(x)=1+\prod_{m=1}^\infty \frac{1}{1-x^m}.$$ I made an $Arg$ plot of $1+\prod_{m=1}^r \frac{1}{1-x^m}$ for up to $r=24$ to see if I could figure out what that ought to ultimately end up looking like, and came up with this: Purely based off the plots, it seems not entirely unlikely that $f_n(x)$ is going to the same place this is, at least inside the unit disc.  Now the question is, how do we determine the solution set at the limit?  I speculate that the unit circle may become a dense combination of zeroes and singularities, with fractal-like concentric ""circles of singularity"" around the roots of unity...  :)","['algebraic-geometry', 'abstract-algebra', 'numerical-methods', 'recreational-mathematics', 'complex-analysis']"
206896,Why begin with distributions and then move to tempered ones?,"After reading several books on distribution theory, I got a strange feeling. Why do they all begin with the theory of distributions and then move on to tempered distributions? Why can't we just start with Schwartz class and tempered distributions since they are the ones we use most often? I think the theory of Schwartz class and tempered distributions can pretty well live on their own, and this theory would be much easier to develop since schwartz class is metrizable and everything works well with tempered distributions. So why do we begin with test functions and distributions, not schwartz class and tempered distributions? Thanks!","['education', 'functional-analysis', 'distribution-theory']"
206898,"Is ""$n$ is divisible by $4$ if and only if $n^2$ is even"" a True Statement?","I'm working on some high school geometry homework, and I'm having some trouble with a problem about proofs and counterexamples. The question posses the statement $n$ is divisible by $4$ if and only if $n^2$ is even and asks if that is a true statement (and to provide a counter example if it is not). My understanding of the statement is that ""a prerequisite of divisibility by $4$ is that a number is even when squared."" Since the square root of an even number is also even (even $\cdot$ even = even), and the definition of an even number is even divisibility by $2$, the statement can be reduced to ""a prerequisite to divisibility by $4$ is divisibility by $2$"", which is clearly true. However, I'm concerned that my understanding of the statement is fundamentally flawed. Is the statement true or false, and why?",['geometry']
206931,How to solve this equation with another way?,"I have the  equation $$\left(\dfrac{x+1}{x-2}\right)^2 + \left(\dfrac{x+1}{x-3}\right) = 12\left(\dfrac{x-2}{x-3}\right)^2$$
I want to solve this equation in the set of all real numbers. First way. Put $t = \dfrac{x-2}{x-3}$, and then $x = \dfrac{3t-2}{t - 1}$
Substitute $t$ onto  the given equation, we have
\begin{equation*}
12t^4-4t^3-13t^2+24t-9 = 0.
\end{equation*}
This equation has two roots $t = -\dfrac{3}{2}$ and $t=\dfrac{1}{2}$.
And then, the given equation has roots $x= \dfrac{13}{5}$ and $x=1$. Please solve for me the given equation with another way. Thank you very much. I have just found another way.  The given equation equavalent to 
\begin{equation*}
\left(\dfrac{x+1}{x-2}\right)^2 + \left(\dfrac{x+1}{x-2} \right)\cdot\left(\dfrac{x-2}{x-3} \right) = 12\left(\dfrac{x-2}{x-3}\right)^2.
\end{equation*}
Put $a = \dfrac{x+1}{x-2}$ and $b = \dfrac{x-2}{x-3}$, we get
\begin{equation*}
a^2 + ab - 12b^2 = 0.
\end{equation*}
Solve this equation, we get $a = -4b$ and $a = 3b$. With $a = -4b$, we have 
\begin{equation*}
\dfrac{x+1}{x-2} = -4 \dfrac{x-2}{x-3}.
\end{equation*}
This equation has two roots $x= \dfrac{13}{5}$ and $x=1$. With $a = 3b$, we have 
\begin{equation*}
\dfrac{x+1}{x-2} =3 \dfrac{x-2}{x-3}.
\end{equation*}
has no real solution.",['algebra-precalculus']
206932,Probability of winning a best of 3 out of 5 game,"Two people play a series of independent games. Person 1's probability of winning any game is 0.6, which leaves 0.4 for Person 2. If they play a best of 5 tournament (3/5), find the following. (1)Probability that person 1 wins the tournament in 3 games. (2) Probability that the tournament lasts exactly 3 games. (3) probability that the tournament lasts exactly 4 games. (4) Probability that Person 1 wins the tournament. (5) Probability that it lasts only 3 games if won by person 1. (6) Probability that person 1 won the tournament if it lasted exactly 3 games. So, I'm pretty lost on how to even start with these problems. I was thinking that for (1) it would be (3/5)*(3/5) -- Person1's probability of winning times the best of 5 wins. Is that even on the right track? And how would you start the others?",['probability']
206934,Resources for Proof practice,"I am a Math fan and a self-learner. I try to look at least once a day at a Linear Algebra or Calculus problem to keep myself in shape and to learn. I also like Analysis, Abstract Algebra and Discrete Math, but I feel I need to and would like to get proficient at proofs. I have much harder time finding good online sources of solved problems and step-by-step guides for practicing proofs (induction, contradiction). I would ideally like to have a list of solved proofs that progress in difficulty. Something like this but in the area of analysis and abstract algebra: http://archives.math.utk.edu/visual.calculus/0/domain.1/index.html http://archives.math.utk.edu/visual.calculus/ Please suggest any resources and thank you in advance.","['proof-writing', 'abstract-algebra']"
206941,basis for the solution space,"Find the basis for the solution space of the system and describe all solutions: $3x_1 - x_2 + x_4 = 0$ $x_1 + x_2 + x_3 + x_4 = 0$ I row reduce the matrix: $\begin{pmatrix}
1&1&1&1\\
0&-4&-3&-2
\end{pmatrix}$ And from here I do not know what to do.","['matrices', 'linear-algebra']"
206967,history of the double root solution of $ay''+by'+cy=0$,"Motivation: It is a well-known fact that $ay''+by'+cy=0$ has solutions which are found from substituting the ansatz $y=e^{\lambda t}$ into the DEqn. It turns out that we replace the calculus problem $ay''+by'+cy=0$ with the algebra problem of solving the characteristic equation $a\lambda^2+b\lambda+c=0$. When the solution is a conjugate pair of complex numbers or distinct pair of real numbers the solutions arise from $e^{\lambda t}$. On the other hand, when the solution is real and repeated then the ansatz solution $y=e^{\lambda t}$ only covers half of the general solution. Suppose that $a\lambda^2+b\lambda+c=0$ has double root solution $\lambda = r$ then we form the general solution of $ay''+by'+cy=0$ as
$$ y(t) = c_1e^{rt}+c_2te^{rt}. $$
The inclusion of the $t$ in the solution is surprising to many students. I think many have asked ""where'd the $t$ come from?"". Of course, we could just as well ask ""where the $e^{\lambda t}$ come from?"". I know of several ways to derive the $t$. In particular: $y''=0$ integrates twice to $y=c_1+tc_2$ and $e^{0t}=1$ so this is an example of the double root. A simple change of coordinates allows this derivation to be extended to an arbitrary double-root. reduction of order to a system of ODEs in normal form. We'll obtain a $2 \times 2$ matrix which is not diagonalizable. However, the matrix exponential gives a solution and the generalized e-vector piece generates the $t$ in the second solution. you can use the second linearly independent solution formula from the theory of ODEs. This formula is found by making a reduction of order based on the fact $y=e^{rt}$ is a solution. After a bit the problem reduces to a linear ODE which integrates to give a lovely formula with nested integrals. This formula also will derive the $t$ in the double root solution. Laplace transforms. We can transform the given ODE in $t$ to obtain an algebra equation with $(s-r)^2Y$ which gives $\frac{F(s)}{(s-r)^2}$ and upon inverse transform the appearance of the $(s-r)^2$ in the denominator gives us the $te^{rt}$ solution Inverse operators. By writing the given ODE as $(D-r)^2[y]=0$ we can integrate in a certain way and again derive the $te^{rt}$ solution. Series solution techniques. added 10/6: start with the distinct root solution $y=c_1e^{\lambda_1 t}+c_2e^{\lambda_2t}$ and consider the limit $\lambda_1 \rightarrow \lambda_2$ to derive the second solution. These are the methods which seem fairly obvious in view of the introductory course (up to notation, several of these are the same method).  My question is this: Question: What is the history of the solution $y=te^{rt}$? Who studied the problem $ay''+by'+cy=0$ and found this solution? I'm also interested in the particular sub-histories of the other methods I mention above. Thanks in advance for any insights!","['ordinary-differential-equations', 'math-history']"
206980,Intuition behind independence and conditional probability,"I have a good intuition that $A$ is independent of $B$ if $P(A \vert B) = P(A)$ , and I see how you can easily derive from this that it must hold that $P(A,B) = P(A)P(B)$ . But the first statement is not normally taken as a definition; instead the second is. What is the intuition, or even derivation behind defining $A$ and $B$ as independent iff $P(A, B) = P(A)(B)$ ? The kind of explanation I am looking for would be one similar to that given by Jaynes for the definition of conditional probability in the first chapter of Probability: The Logic of Science, or even a Kolmogorov axiomatic explanation would help.","['motivation', 'probability', 'definition']"
207029,$A = B^2$ for which matrix $A$?,"Is it true that for any $A\in M(n,\mathbb{C})$there exist a $B\in M(n,\mathbb{C})$ such that $A = B^2$? I think this is not true (but I don't know nay example), and then is it possible to characterize such $A$?","['radicals', 'matrices', 'linear-algebra']"
207040,Pre-image of a measurable set A is always measurable?,"Let $f$ be a continuous function on a set $E$.  Is it always true that $f^{-1}(A)$ is always measurable if $A$ is measurable? I say no.  We know that $\bar{\psi(x)}:=\frac{\phi(x)+x}{2}$ where $\phi$ is the Cantor (or Cantor-Lebesgue) function and $\bar{\psi}:[0,1] \rightarrow [0,1]$.  We know this function maps a measurable subset of $C$ (the Cantor set) into a non-measurable set $W$. A few observations about $\bar{\psi(x)}$.  It is strictly increasing and continuous, so it has a continuous inverse $\bar{\psi(x)}^{-1}$.  Thus $\bar{\psi(x)}^{-1}(W)=c \subset C$; that is, $\bar{\psi(x)}^{-1}(W)$ is non-measurable, but c is. Is this correct?","['measure-theory', 'real-analysis']"
207073,Definite integral over a simplex,"Let $T^d:=\{(x_1,..,x_d):x_i \geq 0, \sum_{i=1}^{d}x_i \leq 1\}$ be the standard simplex in $\mathbb{R}^d$. Compute the integral $$\int_{T^d} x_1^{\nu_1-1}x_2^{\nu_2-1}...x_d^{\nu_d-1}(1-x_1-...-x_d)^{\nu_0-1}$$ where $\nu_i>0$. Remark: I know the answer is $$\frac{\prod_{i=0}^{d}\Gamma(\nu_i)}{\Gamma(\sum_{i=0}^{d}\nu_i)}.$$ I evaluated for the case $d=2$ by using the transformation $(p-1)\iiint\limits_{T^{3}} x^{m-1}y^{n-1}z^{p-2} \mathrm{d}z\mathrm{d}y\mathrm{d}x= 
\iint\limits_{T^{2}} x^{m-1}y^{n-1}(1-x-y)^{p-1}\mathrm{d}y\mathrm{d}x$ and the substitutions $ \left\{\begin{matrix}x=u^2& &\\y=v^2& &\\z=w^2& &\end{matrix}\right.$and    $ \left\{\begin{matrix}u=r\sin\varphi\cos\theta& &\\v=r\sin\varphi\sin\theta& &\\w=r\cos\varphi& &\end{matrix}\right.,$ but this method is complex for computing the general case.","['definite-integrals', 'calculus', 'integration', 'real-analysis']"
207076,Convex combination,"Assume that $I$ is a countable set, and we have $u_i\in \mathbb{R}^n$ for $i\in I$. Suppose that $v=\sum_{i\in I} a_i u_i$ and $\sum_{i\in I}a_i=1$ and $a_i\geq 0$. Can one show that there exists a finite $J\subseteq I$ and $b_j$ for $j\in J$ such that 
$\sum_{j\in J}b_j=1$, $b_j\geq 0$, and $v=\sum_{j\in J} b_j u_j$? Is this some well-known result?","['geometry', 'convex-analysis']"
207084,Lipschitz functions carry $F_\sigma$ to $F_\sigma$.,"Let $f:[a,b] \rightarrow \mathbb{R}$ be a Lipschitz function.  I want to show that it carries $F_\sigma$ sets to $F_\sigma$ sets. I'm not sure how to demonstrate this.  Specifically I'm not sure what property of continuity or Lipschitz would preserve the $F_\sigma$ property.  I do know that this is true: $f(\bigcup_{i} A_i)=\bigcup_{i}f(A_i)$.","['measure-theory', 'real-analysis']"
207118,Prove continuity on a function at every irrational point and discontinuity at every rational point.,"Consider the function: $f(x)= \begin{cases} 
1/n \quad &\text{if $x= m/n$ in simplest form} \\
0 \quad &\text{if $x \in \mathbb{R}\setminus\mathbb{Q}$} 
\end{cases} $ Prove that the function is continuous at every irrational point and also that the function is not continuous at every rational point.  Also, we can say that the function is continuous at some point $k$ if $\displaystyle\lim_{x \to k} f(x)=f(k)$. I was thinking of doing an epsilon delta proof backwards using the fact that $\mathbb{Q}$ is dense in $\mathbb{R}$ for rational points and irrational points.  Any ways on how to expand on this are welcome.",['real-analysis']
207120,Disjoint Union of Subsets and Direct Sum of Subspaces (Clarify Explanation),"Can someone clarify for me the side note found in Axler's Linear Algebra Done Right, that is: 
Direct sums of subspaces are analogous to disjoint union of subsets. I am not sure what exactly is a disjoint union of subsets (having checked online definitions) and thus its relation to direct sums. Thanks.",['linear-algebra']
207133,"Properties of sum of real symmetric, positive semi-definite matrices",I have two correlation matrices A and B. They are: Real symmetric (with ones on the diagonal) Positive semi-definite (eigenvalues are $\ge 0$) I want to try to prove that the average of these two matrices $C={1\over2}A + {1\over2}B$ still has the same properties. It was a few years since my linear algebra courses and would like some help along the way here. I am assuming it's possible as I have tested it numerically and for my 5000 randomly generated correlation matrices the property holds (at least within machine precision). Grateful for any kind of help!,"['numerical-linear-algebra', 'linear-algebra']"
207172,"Bijection from $(0,1]$ to $[0, \infty)^2$","Define a bijection from $(0,1]$ to $[0, \infty)^2$ Route to follow, A-) First define a bijection from $(0,1]$ to $(0,1]^2$ B-) Since there is a bijection from $(0,1]$ to $[0, \infty)$, namely $f(x) = (1/x) -1$, there is a bijection from $(0,1]^2$ to $[0, \infty)^2$ B says, if $f:A \rightarrow B$ is a bijection then there is a bijection $h:A^2 \rightarrow B^2$ Can anyone define me a function that satisfies A , and a function h for proof of B . Rigor at elementary - intermediate analysis level will be appericiated. Note: If possible I wonder the validity of infinite decimal approach for defining a function for part A.","['cardinals', 'functions']"
207174,Positive definite of infinite sum of matrices,Let $A$ be a matrix and let $C$ be a positive definite symmetric matrix. Then $B = \sum_{r=0}^ \infty (A^T)^rCA^r$ is symmetric and positive definite if $\|A\|<1.$ Any hints or proof? Thanks!,"['matrices', 'linear-algebra']"
207184,Similarity involving Miquel's Theorem,"Let $\Delta ABC$ be a triangle. If we place points $D,\ E,\ F$ arbitrarily on the sides $\overline{AB},\ \overline{BC}$ and $\overline{CA}$ respectively, then the circumcircles of the triangles $\Delta ADF,\ \Delta BDE$ and $\Delta CEF$ will all pass through a common center via Miquel's theorem . Let $M$ be this common point, the Miquel point. If we locate the centers of the circumcircles and name them $P,\ Q,\ R$ respectively, then the resulting triangle $\Delta PQR$ will be similar to triangle $\Delta ABC$. I want to prove that the Miquel point $M$ is the center of the spiral similarity which carries $\Delta PQR$ to $\Delta ABC$ If somebody could provide a proof or a reference to a proof that would be greatly appreciated. Thank you. If someone could just provide a proof for the regular similarity between the two triangles, that would also be appreciated. I have looked around online but could not find a proof.","['geometry', 'triangles', 'euclidean-geometry']"
207185,Tautology Proof without truth table,How would I go about proving this without a truth table? $[(p \lor q) \implies r ] \implies [ \neg r \implies (\neg p \land \neg q)]$,"['propositional-calculus', 'discrete-mathematics']"
207208,Cutting a $n$-dimensional cubic cake,"Given a cubic cake, defined as $\{(x,y,z)|0\leq x,y,z\leq 1\}$. We cut it by the planes $p_1\leftrightarrow x=y$ $p_2\leftrightarrow y=z$ $p_3\leftrightarrow x=z$. How many pieces will we have after cutting? And the 4-dimensional case:
$\{(x,y,z,u)|0\leq x,y,z,u\leq 1\}$ How many pieces will we have after having cut by the spaces $x=y$, $x=z$, $x=u$, $y=z$, $y=u$, $z=u$? And the $n$-dimensional case...","['dimension-theory-analysis', 'combinatorics']"
207217,Power series of a function,"I am wondering if there are any functions $f(x)$ such that it cannot be expressed as a power series of $x$? This might turn out to be a silly question, but I can't think of one at the moment. Thanks!",['analysis']
207250,Axioms of closed sets,Let $S$ be a topological space. It means that collection of open subsets of $S$ satisfying the following axioms: $\varnothing$ and $S$ are open any union of open sets is open any finite intersection of open sets is open Besides we may consider collection of closed sets of $S$. Question: can you give me an axioms of closed sets? Thanks a lot!,['general-topology']
207258,"Looking for simple function: Passes through 0, sqrt like but never reaches 100","It's in the title. I am looking for simple function that passes through 0, square root like, never reaches 100, but comes closer and closer to it. I'm sure this is very basic, nothing fancy. But I don't know it now. Thanks a lot!","['functions', 'analysis']"
207325,Elementary Glueing example of affine spaces (Ex. 2.3.6 in Hartshorne),"Let $k$ be a field, $X_1=X_2=\mathbb{A}^1_k=\operatorname{Spec}(k[x])$, $P$ the point corresponding to the maximal ideal $(x)$, $U_1=U_2=\mathbb{A}^1_k-\left\{P\right\}$ and $\phi:U_1 \rightarrow U_2$ the identity map. What do we mean by the phrase ""let $X$ be obtained by glueing $X_1,X_2$ along $U_1,U_2$ via $\phi$""? (Example 2.3.6 page 76 in Hartshorne)",['algebraic-geometry']
207335,Prove $\sup(f+g) \le \sup f + \sup g$,"Suppose $D$ is a nonempty bounded subset of reals. Let $f:D \to \mathbb R$ and $g:D \to \mathbb R$ . Define $(f+g)(x)=f(x)+g(x)$ . Prove $\sup(f+g)(D) \le \sup f(D) + \sup g(D)$ (also prove that $\sup (f+g)$ exists). I understand why this is the case, just not how to prove it. Left side is pretty much $\sup  (f(x)+g(x))$ and right side is $\sup (f(x) + g(y))$ for $x,\,y \in D$ . Basically $f+g$ has to use the same variable and $f(D)+g(D)$ use different ones. But I don't know how to go about proving this. The second part of the question is to find a specific example where strict inequality holds. Let $D=\{a,b\}$ and $f: a \to 1,\, b\to 0, \,g: a \to 0,\, b\to 1$ . $\sup f(D) = 1,\, \sup g(D) = 1,\, \sup f(D) + \sup g(D) = 2.$ $\sup (f+g)(D) = 1$ (if we choose a, $f+g = 1+0,\, b,\, f+g=0+1$ ).",['real-analysis']
207356,Mandelbrot set's border in parametric form,"I've post this question just because I'm curious, Mandelbrot set is defined as: $ z_{n+1} = z^2_n + c $, if $n \rightarrow \infty $ and it doesn't diverge we get the border.
This border is unlimited and it's direction isn't definite, so I think it's a problematic. My question is if we can write the border in a parametric way like this:
$
\left
\lbrace
\begin{array}{l}
x = x(t) \\
y = y(t)
\end{array} 
\right.
$
, if we need some extra variabile to approximate the system or it's impossible to write it in this form. ($z \in \mathbb{C}$ so we can decompose it in: $z = x + yi$)","['algebra-precalculus', 'fractals']"
207359,Probability of Gambler's Ruin with Unequal Gain/Loss,"I've spent some time reading other questions about Gambler's Ruin, but couldn't find the answer I was looking for. In most questions, it is assumed the Gambler wins \$1 or loses \$1. I'm curious how one could approach the problem with a payoff or loss that is not equal. For example, what is the probability of ruin on a gambler who starts with \$1000.00 who wins \$41.00 with probability 0.6 and loses \$43.00 with probability 0.4? Is it possible to generalize such a solution? Cheers,
Josh","['stochastic-processes', 'probability']"
207372,Challenge question for normed linear spaces.,"I have come across the following challenging problem in my analysis course: Let $K$ be a compact convex set in a normed linear space. Suppose that 
$$\sup_{x,y\in K}\{||x-y||\}=\delta>0.$$ Show there exists $x_0\in K$ such that 
$$\sup_{y\in K}\{||x_0-y||\}<\delta$$ We were given the following hint: since $K$ is compact, choose $a,b\in K$ such that $||a-b||=\delta$. Let $K_0$ be a maximal subset of $K$ containing $a$ and $b$ such that $||x-y||$ is either $0$ or $\delta$ whenever $x,y\in K_0$. Now, I've made this progress so far: The set $K_0$ as described above is a discrete subset of a compact set; thus, it must be finite. My strategy so far has thus been: suppose, towards a contradiction that I can't find such an $x_0$. Then, since $K$ is compact, this means that for any $x\in K$, there is some $y\in K$ such that $||x-y||=\delta$. Using this, and convexity, I am then trying to show that I can add another point to $K_0$. I've experimented with just assuming for a bit that $K_0$ only has two elements and playing around with what this give me using the triangle inequality, but I haven't had much luck. If anyone has some ideas on how to tackle this, it'd be much appreciated.","['geometry', 'metric-spaces', 'functional-analysis']"
207392,Why is the total derivative of a diffeomorphism invertible?,"I'm trying to brush up on some differential geometry, but there's a subtle point I don't understand. Suppose $h$ is a diffeomorphism. Then the lecture notes here suggest that it's derivative $df_x$ is an invertible linear map. Why precisely does the invertibility of $df_x$ follow from that of $f$? Apologies if this is a trivial question - I'm a little out of practise with total derivatives!","['differential-geometry', 'inverse', 'derivatives', 'real-analysis']"
207395,Limit of a continuous function,"Suppose that $f$ is a continuous and real function on $[0,\infty]$. How can we show that if $\lim_{n\rightarrow\infty}(f(na))=0$ for all $a>0$ then $\lim_{x\rightarrow+\infty} f(x)=0$?","['calculus', 'limits']"
207397,Real jordan form to complex jordan form then compute P matrix.,"I have the matrix 
$$A = 
\begin{bmatrix}
5 &  0 & 1 &  0 & 0 & -6 \\
3 & -1 & 3 &  1 & 0 & -6 \\
6 & -6 & 5 &  0 & 1 & -6 \\
7 & -7 & 4 & -2 & 4 & -7 \\
6 & -6 & 6 & -6 & 5 & -6 \\
2 &  1 & 0 &  0 & 0 &  0
\end{bmatrix}$$ This can be brought in the following Jordan form, i.e. $A = TJT^{-1}$. $$J = 
\begin{bmatrix}
2-3j & 1 & 0 & 0 & 0 & 0 \\
0 & 2-3j & 1 & 0 & 0 & 0 \\
0 & 0 & 2-3j & 0 & 0 & 0 \\
0 & 0 & 0 & 2+3j & 1 & 0 \\
0 & 0 & 0 & 0 & 2+3j & 1 \\
0 & 0 & 0 & 0 & 0 & 2+3j 
\end{bmatrix}$$ $$T = 
\begin{bmatrix}
2j & 2j & 1+j & -2j & -2j & 1-j \\
1+j & 2j & j & 1-j & -2j & -j \\
0 & 2j & 2j & 0 & -2j & -2j \\
0 & 1+j & 2j & 0 & 1-j & -2j \\
0 & 0 & 2j & 0 & 0 & -2j \\
-1+j & -1+j & j & -1-j & -1-j & -j
\end{bmatrix} 
$$ Now I have to bring A into its real Jordan form. This is easy:
$$J^{R} = 
\begin{bmatrix}
2 & 3 & 1 & 0 & 0 & 0 \\
-3 & 2 & 0 & 1 & 0 & 0 \\
0 & 0 & 2 & 3 & 1 & 0 \\
0 & 0 & -3 & 2 & 0 & 1 \\
0 & 0 & 0 & 0 & 2 & 3 \\
0 & 0 & 0 & 0 & -3 & 2  
\end{bmatrix}
$$ Now I have to compute $V$ such that $VJ^RV^{-1} = A$. My question now is how do I compute this $V$? For real valued jordan forms this is easy. I just have to compute the eigenvectors of A, $\{T_i\}$ and then $T = \begin{bmatrix} T_1 | T_2 | \ldots | T_n\end{bmatrix}$.",['matrices']
207399,How to prove that monos are injective?,"Let $A$ and $B$ be non-empty sets, and let $f\,:\,A\rightarrow B$ be a function. $ \color{darkred}{\bf Theorem}$: The function $f$ is injective if and only if $f\circ g=f\circ h$ implies $g=h$ for all functions $g,h:\,Y\rightarrow A$ for all sets Y.  ($f\,:\,A\, \rightarrowtail \,B$, $f$ is a monomorphism) I want to prove this ${\bf {\it theorem}}$, but I get stuck. $\color{darkred}{\bf proof\,\,}$: $\Rightarrow$) Assume that $f$ is injective. Let $g,h:\,Y\rightarrow A$ be functions such that $f\circ g(y)=f\circ h(y),\,\,\,\,\,\,y\in Y$ it follows that $f(g(y))=f(h(y))$, and $f$ is injective, therefore $g(y)=h(y)$ for every $y\in Y$. $\Leftarrow$) and here I get stuck, can’t figure out how to prove this. Can someone help me with this proof?","['category-theory', 'elementary-set-theory', 'function-and-relation-composition']"
207405,Image of smooth manifold is a submanifold,"It's know that if $M$ is a compact, smooth manifold of dimension $n$ and the map $f: M \to \mathbb{R^m}$ is injective, smooth, $n \le m$ and $Jf(a)$, the Jacobian, has rank $n$ for every $a \in M$, then $f(M)$ is a submanifold of $\mathbb{R^m}$.  I'm trying to think of a counterexample to this statement if we suppose $M$ is not compact, but haven't gotten anywhere.  Can anyone offer a hint?","['differential-topology', 'analysis']"
207406,Irreducible Components of the Prime Spectrum of a Quotient Ring and Primary Decomposition,"Recently I encountered a problem (the first exercise from chapter four of Atiyah & McDonald's Introduction to Commutative Algebra ) stating that if $\mathfrak{a}$ is a decomposable ideal of $A$ (a commutative ring with unity), then the prime spectrum of $A/ \mathfrak{a}$ has finitely many irreducible components. This follows easily from the recognition that the maximal irreducible subspaces of $\textrm{ Spec } (A / \mathfrak{a})$ are precisely the ""zero loci"" of the minimal prime ideals of $A /\mathfrak{a}$. I'm curious about the converse - the proof isn't easily reversed since the notion of minimal ideals of $\mathfrak{a} $ doesn't make sense before we know what we are trying to prove. My intuition says that it is false based on the general premise that images are badly-behaved (and the fact that it isn't part of the exercise.) However, I've had some difficulty constructing a counterexample, so that the main purpose of this post is to ask for a reasonable procedure or heuristic for doing so (or, of course, proof that my intuition is false.) If it helps, if $\textrm{Spec }(A / \mathfrak{a})$ is irreducible then the nilradical $\mathcal{R}_{A /\mathfrak{a}}$ is prime so that $r(\mathfrak{a}) = \rho^{-1} ( \mathcal{R}_{A / \mathfrak{a}} ) = \displaystyle\cap_{i=1}^n \rho^{-1} (p_i),$ where $p_i$ are the minimal prime ideals of $A /\mathfrak{a}$ and $\rho $ is the associated projection, is also prime. Thanks!","['commutative-algebra', 'ring-theory', 'algebraic-geometry', 'abstract-algebra']"
207415,Second countable topological space,"I think I solved a problem from Lee's book and I just like to see if it has any mistake, Problem Let $X$ be a topological space and let $\mathcal{U}$ be an open cover of $X$. Show that if $\mathcal{U}$ is countable and each $U$ $\in$ $\mathcal{U}$ is second countable, then $X$ is second countable. Proof: For each $U$ $\in$ $\mathcal{U}$, let $\mathcal{B}_{U}$ be a countable basis for $U$. We have that the union $\mathcal{B}$ of all such bases is countable and it's a basis for X. Indeed, each of them is open in X and, let $A$ be an arbitraty open set of $X$, we have that $A$ can be written as the union of all the intersections $A$$\cap$$U$, for all $U$ $\in$ $\mathcal{U}$, each such intersection being written as a union of elements of $\mathcal{B}_{U}$. Then $\mathcal{B}$ is a countable basis for $X$.",['general-topology']
207418,How to understand this combinatorially: $\sum^{2k}_{i=0} \binom{4k}{2i} (-1)^{i}=2^{2k}(-1)^{k}$,"The TAs in my department are stuck in assisting an undergraduate with the following problem: $$\sum^{2k}_{i=0} C^{4k}_{2i}(-1)^{i}=2^{2k}(-1)^{k}.$$ We tried to solve this via induction (obviously failed), via various combinatorial identities, via generating functions, etc. Aside from the fact that nothing works, we also do not know how to solve this nicely by using some combinatorial interpretation given this is some undergraudate's HW. I venture to ask in here for I want to see how it can be properly understood.","['binomial-coefficients', 'combinatorics']"
207420,"$dy/dx + 3y = 8$, $y(0) = 0$ : Initial value problem","$$\begin{cases} \frac{dy}{dx} + 3y = 8, \\ y(0) = 0. \end{cases}$$ So, I have been getting an answer of $3$ by integrating and getting $\ln(8-3y) = x$ and solving. But my book says the answer must be expressed as a function of $x$. I do not know what to do.",['ordinary-differential-equations']
207435,Isometry between $L_\infty$ and $\ell_\infty$,"It is known that there exist some isomorphism between $L_\infty$ and $\ell_\infty$, which is not explicit at all. Could someone tell me whether there exist an isometric isomorphism between $L_\infty$ and $\ell_\infty$?","['lp-spaces', 'functional-analysis', 'banach-spaces']"
207455,Has the $\Gamma$-like function $f_p(n) = 1^{\ln(1)^p} \cdot 2^{\ln(2)^p} \cdot \ldots \cdot n^{\ln(n)^p} $ been discussed anywhere?,"In an older fiddling with the gamma-function (expanding on the idea of sums of consecutive like-powers of logarithms, similarly as the bernoulli-polynomials for the sums of like powers of consecutive integers) I hadn't looked at the assumed approximations for the family of p -parametrized gamma-relatives (where p is nonnegative integer)
$$ \begin{align} f_p(n) & =\exp \left(\sum_{k=0}^n \ln(1+k)^p \right) \\ & = 1^{\ln(1)^{p-1}}\cdot 2^{\ln(2)^{p-1}} \cdots n^{\ln(n)^{p-1}} \end{align}$$ 
where $p \gt 1$       . I just looked at that treatize and would like to improve it with some knowlegde about the functions $f_p$ where $p \gt 1$ (for $p=1$ this is the factorial function). Q: Has someone seen one of these functions being discussed elsewhere? Here is some context: an older question at MO , an older question at MSE , the original text discussing this idea initially posted at the tetrationforum a very q&d or, a bit better written in ""uncompleting the gamma"" , from page 13","['gamma-function', 'special-functions', 'reference-request', 'number-theory']"
207458,Estimating the number of observations from a set of samples,I repeatedly measure a value $S_n$ which is the sum of a set of $n$ hidden inputs.  The goal is to identify the number of hidden inputs. All of the hidden inputs are driven by an experimenter controlled stimulus.  The stimulus is quantal (consists of all or none events) but multiple events can occur in each input.  For each stimulus the probability density function of the input's activation level can be estimated.  The random result of this activation is then scaled by a variable input specific weight $w_n$ before being summed into $S_n$. If I use the central limit theorem then I can find $\sigma {\sqrt n}$ by a least squares curve fit of the sample's cumulative distribution to the standard normal cdf.  But I can't then solve for $n$ since the unknown set of $w_n$ makes $\sigma$ unknown. My current thought is to use the Berry-Esseen theorem to try and get another handle on $n$ based on the convergence rate to normality.  But I'm unsure how to proceed with the supremum in that function and would prefer to avoid the inequality (confidence intervals would be perfect). How would you proceed?,"['convergence-divergence', 'parameter-estimation', 'statistics', 'sampling', 'probability']"
207484,Derivative of $\frac{1}{\sqrt{x+5}}$,"I'm trying to find the derivative of $\dfrac{1}{\sqrt{x+5}}$
  using $\displaystyle \lim_{h\to 0} \frac {f(x+h)-f(x)}{h}$ So, $$\begin{align*}
\lim_{h\to 0} \frac{\dfrac{1}{\sqrt{x+h+5}}-\dfrac{1}{\sqrt{x+5}}}{h} &= \frac{\dfrac{\sqrt{x+5}-\sqrt{x+h+5}}{(\sqrt{x+h+5})(\sqrt{x+5})}}{h}\\\\
&= \frac{\dfrac{x+5-x-h-5}{(\sqrt{x+h+5})(\sqrt{x+5})}}{\dfrac{h}{\sqrt{x+5}}+\dfrac{h}{\sqrt{x+h+5}}}
\end{align*}$$ I do not know if this is correct or not.. please help. I'm stuck.","['calculus', 'derivatives']"
207487,Are there oscillating functions that don't reduce to trigonometric functions?,"I am wondering whether there exists a class of oscillating functions that are distinct from trigonometric functions.  The only oscillating functions I could think of are of the $e^{ix}$ and $(-1)^x$ varieties, but these are easily expressed as trigonometric functions (or sums thereof).  I'm looking for functions that cannot be expressed as finite sums of trigonometric functions (but functions that are not themselves finite sums either). I'm not sure how best to phrase this, but I'm looking for non-trivial answers to this.  Its trivial to find a function that has the same value at two distinct x-values and tile it infinitely.  I'm looking for something that can be expressed in a simpler way than an infinite piece-wise function.",['algebra-precalculus']
207521,How to prove an Inequality,"I'm beginner with proofs and I got the follow exercise: Prove the inequality $$(a + b)\Bigl(\frac{1}{a} + \frac{4}{b}\Bigr) \ge 9$$ when $a > 0$ and $b > 0$. Determine when the equality occurs. I'm lost, could you guys give me a tip from where to start, or maybe show a good resource for beginners in proofs ? Thanks in advance.","['inequality', 'algebra-precalculus']"
207551,Splicing together Short Exact Sequences,"Given an exact sequence of vector spaces $$\cdots\longrightarrow V_{i-1}\longrightarrow V_{i}\longrightarrow V_{i+1}\longrightarrow\cdots$$ I want to show that it is the same as having a collection of short ones such that $$0\longrightarrow K_i \longrightarrow V_i \longrightarrow K_{i+1}\longrightarrow0$$ So to start I want to show exactness at an arbitrary $V_i$, so I space them suggestively: $$\begin{array}{c}
0&\rightarrow &K_{i-1}&\rightarrow &V_{i-1}&\rightarrow &K_{i}&\rightarrow&0\\
&&&&0&\rightarrow&K_i&\rightarrow &V_i&\rightarrow&K_{i+1}&\rightarrow&0\\
&&&&&&&&0&\rightarrow&K_{i+1}&\rightarrow&V_{i+1}&\rightarrow&K_{i+2}&\rightarrow&0
\end{array}$$ I drop inclusions down among the corresponding $K_i$'s, and then compose until I get a function from $V_{i-1}$ to $V_i$ and one from $V_i$ to $V_{i+1}$.  I check that the image of the first composite mess is the kernel of the second composite mess, which indeed it is. Question: Am I done?  Is showing exactness at one such $V_i$ enough?  The question now prompts me to worry about the case were the orginal sequence isn't infinite in both directions...I'm not sure how that case is different?","['vector-spaces', 'linear-algebra', 'abstract-algebra']"
207559,Continuous Functions and Cauchy Sequences,"We know that if a function $f: A \mapsto \mathbb{R}$, $A \subseteq \mathbb{R}$, is uniformly continuous on $A$ then, if $(x_n)$ is a Cauchy sequence in $A$, then $(f(x_n))$ is also a Cauchy sequence. I would like an example of continuous function $g: A \mapsto \mathbb{R}$ such that for a Cauchy sequence $(x_n)$ in $A$, it is not true that $f(x_n)$ is a Cauchy sequence. Thanks for your help.","['sequences-and-series', 'examples-counterexamples', 'cauchy-sequences', 'continuity', 'real-analysis']"
207570,Infinite DeMorgan laws,"Let $X$ be a set and $\{Y_\alpha\}$ is infinite system of some subsets of $X$.
Is it true that:
$$\bigcup_\alpha(X\setminus Y_\alpha)=X\setminus\bigcap_\alpha Y_\alpha,$$
$$\bigcap_\alpha(X\setminus Y_\alpha)=X\setminus\bigcup_\alpha Y_\alpha.$$
(infinite DeMorgan laws) Thanks a lot!",['elementary-set-theory']
207583,Is this question erroneous?  (Stationary points),"Using the second partial derivative test, I have found (-1,1) to be a saddle point but this option is not available in the MCQ. Have I made a mistake? The person who set the question insists that (-1,1) is a minimum, not saddle, because: ""We are talking about the local situation at the 2 points. Since the y
coordinate of the 2 points are the same at y=1, that means we pass a
vertical plane through the 3-D surface, so it becomes a 2-D curve in
terms of x. So using 2nd derivative test (as stated explicitly in the
question), it leads to local minimum. ""","['optimization', 'multivariable-calculus']"
207593,Embedded Submanifold,"This is a question from Lee : Introduction to Smooth manifolds.  p.201 For each $a \in \mathbb{R}$, let $M_a$ be the subset of $\mathbb{R}^2$ defined by 
$$M_a = \{(x,y) : y^2 = x(x-1)(x-a)\}$$ For which values of $a$ is $M_a$ an embedded submanifold of $\mathbb{R}^2$?  For which values can $M_a$ be given a topology and smooth structure making it into a immersed manifold? I'm not sure how to get started.  I know some things, but none of it seems to apply to this problem.  For example, we know that that the graph of a smooth map $f:U \subset \mathbb{R}^{n} \rightarrow \mathbb{R}^{k}$ is an embedded submanifold of $\mathbb{R}^{n+k}$.  So maybe we can write this as the graph of a smooth function?  Even if that were the case, that only gives sufficiency, and this is asking for necessary and sufficient conditions.",['geometry']
207601,Joint densities and conditional densities of sums of i.i.d. normally distributed random variables,"Let $X_1,X_2,…$ be independent with the common normal density $\eta$, and $S_k= X_1+⋯+X_k$. If $m <n$ find the joint density of $(S_m,S_n)$ and the conditional density for $S_m$ given that $S_n=t$. Solution attempt Let the independent random variables have $X_1,X_2,…$  with the common normal density $\eta$ have parameters $(\mu,\sigma^2)$. Then both $S_m$ and  $S_n$ have normal densities with parameters$(m\mu,m\sigma^2)$ and $(n\mu,n\sigma^2)$ respectively and there densities are $f_{S_m}$ and $f_{S_n}$. We also observe that $S_m$ and $S_{n-m}$ are two independent random variables with density $f_{S_m}$and $f_{S_{n-m}}$ and the density of their sum $S_n$ is $f_{S_n}$. The pairs $(S_m,S_n )$ and $(S_m,S_{n-m})$ are related by linear transformation $S_m= S_m,S_{n-m}=S_n-S_m$ with determinant 1. So the joint density of $(S_m,S_n)$ is given by $f_{S_m}(x) f_{S_{n-m}}(s-x)$ $f_{S_m} (x) f_{S_{n-m}} (s-x)=\dfrac{1}{\sqrt {2\pi m \sigma ^{2}}}e^{-\dfrac {\left( x-m \mu\right) ^{2}} {2m \sigma ^{2} }}\dfrac{1}{\sqrt {2\pi (n-m) \sigma ^{2}}}e^{-\dfrac {\left( s-x-(n-m)\mu\right) ^{2}} {2(n-m) \sigma ^{2} }}$ $f_{S_m} (x) f_{S_{n-m}} (s-x)=\dfrac{1}{2\pi\sigma ^{2}\sqrt { m(n-m) }}e^{\dfrac {-(n-m)\left( x-m \mu\right) ^{2}-m\left( s-x-(n-m)\mu\right) ^{2}} {2m(n-m) \sigma ^{2} }}$ I can 't seem to be able to put this into the standard bi-variate normal distribution form. This appears like a bivariate normal density which tallies with the answer provided by the author but he also states a bivariate normal density with variances $m, n$ and covariance $\sqrt {\dfrac {m} {n}}$.","['probability-theory', 'normal-distribution', 'probability-distributions', 'conditional-expectation']"
207604,floor number sum,"For $a$ and $n$ , there is formula to calculate $$a + 2a + 3a + \cdots + na = \frac{n(n+1)}{2} a.$$ Is there formula: $$\lfloor a\rfloor + \lfloor 2a\rfloor + \lfloor 3a\rfloor + \cdots + \lfloor na\rfloor $$",['sequences-and-series']
207609,The measurability of convex sets,"How to prove the measurability of convex sets in $R^n$ ? I have seen a proof, but too long and not very intuitive. If you have seen any, please post it here.","['convex-analysis', 'measure-theory', 'real-analysis']"
207635,Help identifying math symbols: big crosshair and caret,"$$
\left[\begin{array}{lllll}
\psi_{i}(0) & \psi_{i}(1) & \psi_{i}(2) & \ldots & \psi_{i}\left(2^{L}-1\right)
\end{array}\right]^{\top}
$$ $I$ also showed in chapter $? ?$ that $\Psi$ was orthogonal; hence $\Psi$ represents orthogonal basis. It was also noted that w was unique. Recall the definition of $\psi_{i}:$ $$ \psi_{i}(x)=Y\left(\bigoplus_{k=0}^{L-1}(x[k] \wedge i[k])\right) \quad x, i \in \mathcal{B}^{L}$$ Can anyone identify the names of the crosshair and ^ symbols? 90% sure the ^ means bitwise AND from searching wikipedia, but I haven't found the crosshair symbol anywhere else.","['computer-science', 'algebra-precalculus']"
207636,Can an underdetermined system have a unique solution?,"In my case, I am calling an underdetermined system as a system of linear equations where there are fewer equations than variables (unknowns). My textbook says the answer is false, however the internet says otherwise. On the outside, it makes sense that an underdetermined system can't have a unique solution, but I am leaning towards trusting the internet on this one. Can someone explain how a unique solution exists? A proof  might be helpful too, although hopefully not too complex.","['matrices', 'linear-algebra']"
207638,Proof of the definition corollary of curl,"$$(\nabla \times \mathbf{F}) \cdot \mathbf{\hat{n}} \ \overset{\underset{\mathrm{def}}{}}{=} \lim_{A \to 0}\left( \frac{1}{|A|}\oint_{C} \mathbf{F} \cdot d\mathbf{r}\right)$$ This is how the curl operator is usually defined, and I want to know the proof that the left hand and the right hand are equivalent.",['multivariable-calculus']
207660,Value of Logarithm of negative number,Why the logarithmic value of negative number can't be define? Is there any special reason?,"['logarithms', 'linear-algebra']"
207685,How to find the minimal axis-parallel ellipse enclosing a set of points.,"I have a set $X$ of points in $\mathbb{R}^2$ and I'm trying to find the smallest encompassing ellipse which main axes are parallel to the coordinate system's (to put it differently, its both centres share one coordinate). I need the gravitational centre and the vertical and horizontal radius. Now, I managed to do this for a horizontally aligned rectangle, but that isn't much help (although it's a first approximation, as I can easily draw a rectangle around $X$). I also found some formulas on the internet, but they seem to be wrong as I keep getting $0$ for the radius. Can this be done algebraically?","['geometry', 'conic-sections']"
207687,probability - 2 cards with same rank,"From a deck of 52 cards,What's the probability that he gets a combination of 2 cards with same rank. Eg: 3♥ 3♠","['probability-theory', 'probability-distributions', 'probability']"
207707,Finding all real zeros of the polynomial?,"Okay, so I need to find all real zeros in this polynomial... $$f(x) = 2x^3 + x^2 - 13x + 6$$ I know that the first step is to find the factors of 6 and 2, then see which when multiplied by the other coefficients have them add up to equal zero, but none of the factors I tried came out to zero. Is there an easier way to go about doing this???","['algebra-precalculus', 'polynomials']"
207723,"My proof of ""the set of diagonalizable matrices is Zariski-dense in $M_n(\mathbb F)$"".","The following is my proof of the assertion that the set of diagonalizable matrices  is Zariski-dense in $M_n(\mathbb F)$. Is this right ? Let $\mathbb F$ be an infinite field (not necessarily algebraically closed)and $M_n(\mathbb F)$ the set of all $n \times n$ matrices with entries in $\mathbb F$. We denote by $D_n(\mathbb F)$ the set of $n \times n$ diagonalizable matrices with entries in $\mathbb F$. For each $A \in M_n(\mathbb F)$, we denote by $d(A)$ the discriminant of the characteristic polynomial of $A$. Since $d(A)$ is a polynomial in the entries of $A$ with coefficients in $\mathbb F$,
the set $U :=  \{ X \in M_n(\mathbb F) : d(X) \not = 0 \}$ is Zariski-open.
(Here, we are identifying $M_n(\mathbb F)$ with ${\mathbb A}^{n^2}$.) It follows from the fact that ${\mathbb A}^{n^2}$ is irreducible that $U$ is Zariski-dense in ${\mathbb A}^{n^2}$. As $U$ is contained in $D_n(\mathbb F)$, $D_n(\mathbb F)$ is also Zariski-dense in ${\mathbb A}^{n^2}$. Thanks in advance.","['general-topology', 'linear-algebra']"
207761,Finding Partial Derivatives,"For the equation below, of Van der Waal form:
  $$\left(P+\frac{n^{2}a}{V^{2}}\right)(V-nb)=nRT$$ Determine the partial derivatives; $\Bigl(\frac{\partial V}{\partial T}\Bigr)_{P,n}    \text{and }  \Bigl(\frac{\partial P}{\partial V}\Bigr)_{T,n}$ Where $a,b, n, R$ are constants. This is what I've done so far for one of the partial derivatives. I dont know what else to do from here, sorry.
$$P=\frac{nRT}{V-nb}-\frac{n^{2}a}{V^{2}}$$ $$\frac{\partial P}{\partial V}=-\frac{nRT}{(V-nb)^2}+\frac{2n^{2}a}{V^{3}}$$","['calculus', 'derivatives']"
207763,Criterion for constancy of an entire function.,"in Conway's Complex Analysis textbook he writes that if $f$ is entire and $|f|\leq 1+|z|^{1/2}$ then f is constant. Obviosuly from Liouville's theorem I only need to show that $f$ is always bounded. For $|z|\leq 1$ it's obvious that $|f|\leq 2$, but how about $|z|>1$? Thanks in advance.",['complex-analysis']
207766,Noncompact sequentially compact space,"Have you an example of a noncompact sequentially compact space, without using ordinal?","['general-topology', 'compactness', 'examples-counterexamples']"
207767,"""Agreement Domain"" of Function Families","Given a family of functions $\{f_i : X \to Y\}_{i \in I}$ between topological spaces $X$ and $Y$, I define an operation $\bigcap\limits^{\scriptscriptstyle\text{dom}}$ on the family such that $\bigcap\limits^{\scriptscriptstyle\text{dom}}_{i \in I}f_i = \{x \in X\ |\ f_i(x) = f_j(x) \:\:\forall i, j \in I\}$. That is, the operation outputs the set of all points of $X$ on which every function in the family agrees on. The notation is motivated because the operation simply takes the intersection of the graphs of  each $f_i$, which is also the graph of some function $f$, then outputs the domain of $f$. Question: Given that every $f_i$ is continuous, what restrictions must be placed on $X$, $Y$ or the family of functions itself such that $\bigcap\limits^{\scriptscriptstyle\text{dom}}_{i \in I}f_i$ is closed? For instance, perhaps compactness of $X$ would be required, or $I$ would need to be a finite index set.","['general-topology', 'elementary-set-theory']"
207774,"Relation between $\operatorname{Proj} \, k[x_0,\cdots,x_n]$ and $\mathbb{P}^n$","In Hartshorne's ""Algebraic Geometry"" p. 77, Example 2.5.1, it is mentioned that if ""$k$ is an algebraically closed field, then the subspace of closed points of $\operatorname{Proj} \, k[x_0,\cdots,x_n]$ is naturally homeomorphic to the projective $n$-space $\mathbb{P}^n$. He refers to Ex. 2.14d, however I don't see the connection. Any insights? Thanks. P.S. Ex. 2.14(d) seems to me a little bit obscure at this point, this is why i am not reproducing it. Any argument relating $\operatorname{Proj} \, k[x_0,\cdots,x_n]$ and $\mathbb{P}^n$ is very welcome.",['algebraic-geometry']
207776,Finding eigenvalues of matrix of matrix.,"Let A be 4 X 4 matrix with eigenvalues -5, -2, 1, 4. Which of the following option is an eigenvalue of $\begin{bmatrix}A & I\\I & A\end{bmatrix}$, where I is 4 X 4 identity matrix? options: (A) -5  (B) -7 (C) 2 (D) 1","['matrices', 'eigenvalues-eigenvectors']"
207790,What is the role of Topology in mathematics?,What is the role of Topology in Mathematics? Is it like Logic that you need in every areas of math?,"['general-topology', 'philosophy', 'intuition']"
207791,$4 \times 4$ matrix and its inverse. Is my method ok?,"I have been struggling to derive inverse matrix of a $4 \times 4$  Lorenz matrix $\Lambda$. 
$$
\Lambda =
\begin{bmatrix}
\gamma&0&0&-\beta \gamma \\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
-\beta \gamma & 0 & 0 & \gamma 
\end{bmatrix}
$$ My professor says that inverse is: $$
\Lambda^{-1} =
\begin{bmatrix}
\gamma&0&0&\beta \gamma \\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
\beta \gamma & 0 & 0 & \gamma 
\end{bmatrix}
$$ Soo i tried to derive inverse matrix $\Lambda^{-1}$ using adjugate formula: $$\Lambda^{-1} = \frac{1}{|\Lambda|} \textrm{adj}(\Lambda)$$ I did it quite metodically by first calculating the determinant of $\Lambda$, then matrix of minors, matrix of cofactors, 
adjugate matrix and in the end using the above formula to find the inverse ( here is my whole derivation ). Long story short, at the end I end up with this: $$
\Lambda^{-1} = \frac{1}{|\Lambda|} \textrm{adj}(\Lambda) = \frac{1}{\gamma^2 (1 - \beta^2)}
\begin{bmatrix}
\gamma & 0 & 0 &\beta \gamma\\
0 & \gamma^2(1-\beta^2) & 0 & 0\\
0 & 0 & \gamma^2(1-\beta^2) & 0\\
\beta \gamma & 0 & 0 & \gamma\\
\end{bmatrix}
$$ The result is not what my professor says i should get. In my adjugate matrix parts with $\gamma$ and $\beta \gamma$ seem wrong. Is it possible my professor wrote down wrong inverse matrix?
Could anyone point me to the right direction? 
I am kind of lost here, but i am sure i have done a lot of work and am near the solution.",['matrices']
207799,Determine sum of exponential,I am struggling to find an answer of the following series $$\sum_{i=1}^n \frac{1}{1+\exp(a_i+b_ix)}$$ Any suggestion?,['sequences-and-series']
207802,Proving the limit - multiple variable differentiation,"I'm working through an advanced calculus book and want to be certain I understand the idea behind proving limits. This is not homework, I'm just a statistician looking to learn more about mathematics. The exercise I'm concerned with proving is as follows: $$\begin{aligned}
\lim_{(x,y)→(0,0)} 
\frac{x^3y}{x^2 + y^4} \\\
\end{aligned}$$ My understanding is that I can choose a value to substitute in for y that allows for some easy cancellation that proves the limit equals 0.  For instance: $$\begin{aligned}
x= y^2 \ ;  
\frac{(y^2)^3y}{(y^2)^2 + y^4} \\\
\end{aligned}$$ From here, we have: $$\begin{aligned}
\frac{y^7}{y^4(1 + 1)} \\\
\end{aligned}$$ Then as y→0 this simplifies to: $$\begin{aligned}
\frac{0^3}{2} = 0 \\\
\end{aligned}$$ Is this how the limit could/would be proved?","['multivariable-calculus', 'proof-writing', 'limits']"
207807,Example of a non-commutative monoid,"Is there an explicit example of a non-commutative monoid $M$ such that for all elements $m,n \in M$ and $p \in \mathbb{N}$ we have $(m \cdot n)^p=m^p \cdot n^p$? It suffices to construct a semigroup $H$ with an absorbing element $0$ such that $a^2=0$ for all $a$, because then $M := H \cup \{e\}$ will do the job. This sounds easy at first glance, but I cannot find an example.","['abstract-algebra', 'monoid']"
207817,Why are locally compact groups Weil complete?,"Why are locally compact groups Weil complete? Note: A topological group $G$ is Weil complete if every left Cauchy net in $G$ is convergent. Thank you, and sorry if I have bad writing.","['general-topology', 'topological-groups', 'uniform-spaces', 'locally-compact-groups']"
207831,Twist on log of sine and cosine integral $\int_{0}^{\frac{\pi}{2}}x\ln(\sin x)\ln(\cos x)dx$,"I ran across this integral and have not been able to evaluate it. $$\displaystyle \int_{0}^{\frac{\pi}{2}}x\ln(\sin(x))\ln(\cos(x))dx=\frac{{\pi}^{2}\ln^{2}(2)}{8}-\frac{{\pi}^{4}}{192}$$ I had some ideas.  Perhaps some how arrive at $\displaystyle\frac{1}{2}\sum_{k=0}^{\infty}\frac{1}{(2k+1)^{4}}=\frac{{\pi}^{4}}{192}$ . and $\displaystyle \ln(2)\int_{0}^{\frac{\pi}{2}}x\ln(2)dx=\frac{{\pi}^{2}\ln^{2}(2)}{8}$ by using the identity $\displaystyle\sum_{k=1}^{\infty}\frac{x\cos(2kx)}{k}=-x\ln(\sin(x))-x\ln(2)$ and/or $\displaystyle \ln(\cos(x))=-\ln(2)-\sum_{k=1}^{\infty}\frac{(-1)^{k}\cos(2kx)}{k}$ I have used the first one to evaluate $\displaystyle\int_{0}^{\frac{\pi}{2}}x\ln(\sin(x))dx$ ,  so I thought perhaps it could be used in some manner here. I see some familiar things in the solution, but how to get there?. Does anyone have any clever ideas?. Thanks.","['trigonometric-integrals', 'integration']"
207836,"principal ""pseudo eigenvector"" of a real symmetric positive-semidefinite matrix","Let $A$ be a real symmetric positive-semidefinite matrix and suppose that $c>0$ is a sufficiently small number. I wonder if it is possible to solve the non-convex optimization 
$$\arg\max_u\ u^\mathrm{T}Au\\ \mathrm{subject\ to\ }\left\Vert u\right\Vert_2\leq 1\\ \quad\quad\quad\quad\left\Vert u\right\Vert_1\leq c,$$ efficiently. For solving the optimization, I couldn't get farther than writing KKT conditions which do not help much in specifying the multipliers. Given that without the $\ell_1$-norm constraint (i.e. $c\to\infty$), the problem reduces to finding the principal eigenvector of $A$ that can be solved efficiently (e.g., using power iteration method), we can think of the solution to the optimization above as ""psuedo eigenvector"".","['optimization', 'numerical-linear-algebra', 'linear-algebra', 'numerical-methods']"
207840,An Example where $gHg^{-1} \ne g^{-1}Hg$,"I was trying to think of an example where $gHg^{-1} \ne g^{-1}Hg$.  I couldn't think of one, but I am curious if the following reasoning demonstrates that, at the very least, such an example must exist: Let $H$ be a non-normal subgroup of $G$ and let $g \in G - H$.  Then it is possible that $gHg^{-1} \ne g^{-1}Hg$ since if we supposed otherwise we would have $$\begin{align}
gHg^{-1}   & = g^{-1}Hg \\
g^2Hg^{-1} & = Hg\\
g^2        & = e\\
g          & = e
\end{align}$$ which is impossible since we assumed $g \in G - H$.",['group-theory']
207861,Expected value of applying the sigmoid function to a normal distribution,"Short version: I would like to calculate the expected value if you apply the sigmoid function $\frac{1}{1+e^{-x}}$ to a normal distribution with expected value $\mu$ and standard deviation $\sigma$ . If I'm correct this corresponds to the following integral: $$\int_{-\infty}^\infty \frac{1}{1+e^{-x}} \frac{1}{\sigma\sqrt{2\pi}}\ e^{ -\frac{(x-\mu)^2}{2\sigma^2} } dx$$ However, I can't solve this integral. I've tried manually, with Maple and with Wolfram|Alpha, but didn't get anywhere. Some background info (why I want to do this): Sigmoid functions are used in artificial neural networks as an activation function, mapping a value of $(-\infty,\infty)$ to $(0,1)$ . Often this value is used directly in further calculations but sometimes (e.g. in RBM's ) it's first stochastically rounded to a 0 or a 1, with the probabililty of a 1 being that value. The stochasticity helps the learning, but is sometimes not desired when you finally use the network. Just using the normal non-stochastic methods on a network that you trained stochastically doesn't work though. It changes the expected result, because (in short): $$\operatorname{E}[S(X)] \neq S(\operatorname{E}[X])$$ for most X. However, if you approximate X as a normal distribution and could somehow calculate this expected value, you could eliminate most of the bias. That's what I'm trying to do.","['statistics', 'probability', 'integration']"
207865,The eigenvalues of a block matrix of the form $\pmatrix{B&C\\0&D}$,"It is known that all $B$, $C$ and $D$ are $3 \times 3$ matrices. And the eigenvalues of $B$ are $1, 2, 3$; $C$ are $4, 5, 6$; and $D$ are $7, 8, 9$. What are the eigenvalues of the $6 \times 6$ matrix 
$$\begin{pmatrix}
B & C\\0 & D
\end{pmatrix}$$
where $0$ is the $3 \times 3$ matrix whose entries are all $0$. 
From my intuition, I think the eigenvalues of the new $6 \times 6$ matrix are the eigenvalues of $B$ and $D$. But how can I show that?",['linear-algebra']
207877,Building a graph from pairwise distances,"I am not familiar with graphs. However, I am curious about a question on graphs. Given a finite set equipped with a metric, is there any studying on the following problem? Problem: given the metric $d$ for vertexes on $V$, build the undirected graph with the minimum number of edges such that, for each pair of vertices $(v_1, v_2)\in V$, the shortest path from $v_1$ to $v_2$ equals $d(v_1, v_2)$. Is this graph unique? This kind of graph building would be very important to model depedency of agents on economical dynamic systems.","['graph-theory', 'analysis', 'combinatorics']"
207884,"Understanding Proof that Interval $[0,1]$ is Compact.","On page 44-45 of the notes from http://www.msc.uky.edu/droyster/courses/fall99/math4181/classnotes/notes5.pdf , the author writes in the proof that the  interval $[0,1]$ is compact that supposing the interval isn't compact, that this implies either $[0,1/2]$ or $[1/2,1]$ isn't compact. How do we know that both $[0,1/2]$ and $[1/2,1]$ are not compact?","['general-topology', 'compactness', 'real-analysis']"
207902,Under what conditions can I interchange the order of limits for a function of two variable?,"Suppose I have $f:\mathbb{R}^2 \to \mathbb{R}$.   What conditions do I need to say that $$\lim_{x \to a} \lim_{y \to b} f(x,y) = \lim_{y \to b} \lim_{x \to a} f(x,y)$$ ? What about in a more general case, by taking $X,Y$ and $Z$ topological (Hausdorff) spaces and $f$ from $X \times Y$ to $Z$ ? Thank you","['general-topology', 'real-analysis', 'limits']"
207910,Prove convergence of the sequence $(z_1+z_2+\cdots + z_n)/n$ of Cesaro means [duplicate],"This question already has answers here : On Cesàro convergence: If $x_n \to x$ then $z_n = \frac{x_1 + \dots +x_n}{n} \to x$ (3 answers) Closed 6 years ago . Prove that if $\lim_{n \to \infty}z_{n}=A$ then:
 $$\lim_{n \to \infty}\frac{z_{1}+z_{2}+\cdots + z_{n}}{n}=A$$
I was thinking spliting it in: $$(z_{1}+z_{2}+\cdots+z_{N-1})+(z_{N}+z_{N+1}+\cdots+z_{n})$$
where $N$ is value of $n$ for which $|A-z_{n}|<\epsilon$
then taking the limit of this sum devided by $n$ , and noting that the second sum is as close as you wish to $nA$ while the first is as close as you wish to $0$. Not sure if this helps....","['sequences-and-series', 'calculus', 'real-analysis', 'cesaro-summable', 'limits']"
207911,"I need to disprove an alternate definition of an ordered pair. Why is $\langle a,b\rangle = \{a,\{b\}\}$ incorrect?","So we know that the an ordered pair $(a,b) = (c,d)$ if and only if $a = c$ and $b = d$.  And we know the Kuratowski definition of an ordered pair is: $(a,b) = \{\{a\},\{a,b\}\}$ http://en.wikipedia.org/wiki/Ordered_pair#Kuratowski_definition The proof for the Kuratowski definition is in the wikipedia link. Now, why is this alternate definition, $(a,b) = \{a,\{b\}\}$ incorrect? I'm trying to follow the proof for $(a,b) = (c,d)$ iff $a = c$ and $b = d$ as given in the wikipedia link, only for this alternate definition for an ordered pair, in order to search for a contradiction.  But I don't think I'm dong it right. I started with... $(a,b) = (c,d)$ Then $\{a,\{b\}\} = \{c,\{d\}\}$ based on the alternate definition Now... Suppose $a \neq b$ $\{a,\{b\}\} = \{c,\{d\}\}$ But since it's an ordered pair, either of the following can be true? $a = c$ and $\{b\} = \{d\}$ ? OR $a = \{d\}$ and $\{b\} = c$ ? Yeah I have no idea where to reallly go from here.   Is that a contradiction in itself? I can't tell. Thank you for the help. Edit: Alright, I have developed a counter example based mostly off of Asaf Karagila answer (Thanks Asaf!).  Essentially what I needed to do was prove that, by this definition, a != c or b != d, even when (a,b) = (c,d). So using what Asaf told me, I set a = {x} and b = y. Which by the incorrect definition gives...
(a,b) = {{x},{y}} Then I set c = {y} and d = x, which gives
(c,d) = {{y},{x}} which is equivalent to {{x},{y}} So, (a,b) = (c,d) even though a != c and b != d, which is a contradiction. I cleared this method with my professor. Thanks for the help everyone!","['discrete-mathematics', 'elementary-set-theory', 'definition']"
207920,"Find the interior, accumulation points, closure, and boundary of the set","I need to find the interior, accumulation points, closure, and boundary of the set $$
A = \left\{ \frac1n + \frac1k \in \mathbb{R} \mid n,k \in \mathbb{N} \right\}
$$
  and use the information to determine whether the set is bounded, closed, or compact. So far, I have that the interior is empty, but not sure how to prove it. My thoughts are to fix $n$ and then the accumulation points would be $\left\{ \frac 1n \mid n \in \mathbb{N} \right\}$. But I'm not sure if that is correct. Then, I believe that the boundary is $[0,2]$. Can someone confirm that? Any help would be appreciated.",['real-analysis']
207933,The topological boundary of the closure of a subset is contained in the boundary of the set,"Let $X$ be a topological space and $N$ a subset of $X$. I want to show that $\partial \bar N\subset \partial N$. I know that since $\bar N$ is closed then $\partial \bar N\subset \bar N$. By definition $\bar N=N \cup \partial N$, now if $x\in \partial\bar N$ then $x\in \bar N$ but why $x$ must lie precisely in $\partial N\subset \bar N$?",['general-topology']
207934,Conformal parametrization of an ellipse,"I am looking to a formula for the conformal map from the unit disc in the interior of an ellipse centered in $0$ and with semiaxes $a,b>0$. I know that depends on elliptic function, but I didn't find any references. Thanks in advance for any formula or reference.","['reference-request', 'complex-analysis']"
207955,Prove that $\operatorname{Spec}\sqrt2$ contains infinitely many powers of $2$.,$\newcommand{\spec}{\operatorname{Spec}}\spec\sqrt2=\{\lfloor k\sqrt2\rfloor: k \ge 0\}$. I have no idea of how I can prove the statement in the question. Prove that $\spec\sqrt2$ contains infinitely many powers of $2$.,['discrete-mathematics']
207970,A point in subset $A $of metric space is either limit point or isolated point.,"Let $A$ be a subset of a metric space. $A'$ be the set of limit points of $A$ and $A^i$ be the set of isolated points of $A$. Show $A \subset A' \cup A^i$. The picture on my mind is that I draw a disk plus a point outside of the disk, call it a set $A$, if the point falls on the disk, it is automatically a limit point of $A$. If the point is the point that lies outside of the disk, it is of course an isolated point. But I got trouble formalizing it in a rigorous way. Any hint? EDIT: Definition added: Limit Point: $A \subset X$, $X$ a metric space. $b \in X$ is a limit point if every neighborhood (topology sense) of $b$ contains a point of $A$ different from $b$. Isolated point: $b$ is an isolated point of $A$ if there exists a neighborhood around $b$ that contains no point from $A$.","['general-topology', 'metric-spaces']"
207983,Undergraduate Research,"I am currently writing a couple of undergraduate papers about primes and irrational numbers, and my advisor keeps saying that I need to motivate the topics and include a discussion at the end. Can anyone explain how to motivate Mersenne primes and/or a new irrationality criterion. How do mathematicians motivate theorems about numbers such as Mersenne primes? Apparently, I need to explain why my results are useful, but is it not obvious? Why else do we prove theorems?",['number-theory']
207990,Vector Spaces and AC,"I know that the proof that every vector space has a basis uses the Axiom of Choice, or Zorn's Lemma. If we consider an axiom system without the Axiom of Choice, are there vector spaces that provably have no basis?","['vector-spaces', 'hamel-basis', 'linear-algebra', 'axiom-of-choice']"
207991,Eigenvalues of a rectangular matrix,I've read that the singular values of a matrix are equal to the $$\sigma=\sqrt{\lambda_{K}}$$ where $\lambda$ are the eigenvalue but I'm assuming this only applies to square matrices. How could I determine the eigenvalues of a non-square matrix. Pardon my ignorance.,['linear-algebra']
208001,Are there any decompositions of a symmetric matrix that allow for the inversion of any submatrix?,"I am given a $J \times J$ symmetric matrix $\Sigma$. For a subset of $\{1, ..., J\}$,  $v$, let $\Sigma_{v}$ denote the associated square submatrix. I am in need of an efficient scheme for inverting $\Sigma_v$ for potentially any subset $v$. Essentially, I will have to invert many different submatricies $\Sigma_v$, but I won't know which ones I need to invert in advance of running some program; I would rather invest in a good matrix decomposition at the outset, if one exists (or otherwise get whatever information necessary, if not a decomposition). I've messed around with the eigen decomposition a little bit, but wasn't able to coerce the inverse of a submatrix out of it. UPDATE: Apparently the term for the type of submatricies I want to invert is ""principal submatrix"". I'm wondering if I can't make some progress on this via the Cholesky decomposition. Suppose I'm content to calculate $\Sigma_{jj} ^ {-1}$ for any $j \in \{1, 2, ..., J\}$, where $\Sigma_{jj}$ denotes the submatrix obtained by deleting rows/columns greater than $j$. Write $\Sigma = LL^T$ and let $Q = L^{-1}$. Write 
$$
L = \begin{pmatrix}L_1 & \mathbf 0 \\ B & L_2\end{pmatrix}, 
Q = \begin{pmatrix}Q_1 & \mathbf 0 \\ C & Q_2\end{pmatrix} 
$$
where $L_1$ and $Q_1$ and $j \times j$. It follows that $\Sigma_{jj} = L_1 L_1^T$ and $Q_1 = L_1 ^{-1}$ so that $\Sigma_{jj} ^{-1} = Q_1^T Q_1$. So, once I have the Cholesky decomposition I have the inverses of the leading principal submatricies. This doesn't solve the problem as stated since I may need to deal with other principal submatricies, but it should be a useful partial solution.","['matrices', 'numerical-linear-algebra', 'block-matrices']"
208002,"Finding the $n$-th derivative of $f(x) =e^x \sin x$, solving the recurrence relation","I am trying to find a closed solution for the nth derivative of the function: $f(x) = e^x \sin x$ So far I have been able to obtain the derivative as: $f^{(n)}(x) = e^x S_n \sin x + e^x C_n \cos x$ The sequences S and C are defined as below: $S_n = S_{n-1} - C_{n-1}$ $C_n = S_{n-1} + C_{n-1}$ $S_0 = 1$ , $C_0 = 0$ I have been able to further simplify this by combining the two equations and obtaining: $C_n = 2S_{n-2}$ $S_n = S_{n-1} - 2 S_{n-3}$ However, I have no idea what to do now. Can anyone help me find the closed form solution?","['recurrence-relations', 'sequences-and-series', 'derivatives']"
208009,Defining the derivative without limits,"These days, the standard way to present differential calculus is by introducing the Cauchy-Weierstrass definition of the limit. One then defines the derivative as a limit, proves results like the Leibniz and chain rules, and uses this machinery to differentiate some simple functions such as polynomials. The purpose of my question is to see what creative alternatives people can describe to this approach. The nature of the question is that there is not going to be a single best answer. I have several methods that I've collected which I'll put in as answers to my own question. It's not reasonable to expect answers to include an entire introductory textbook treatment of differentiation, nor would anyone want to read answers that were that lengthy. A sketch is fine. Lack of rigor is fine. Well known notation and terminology can be assumed. It would be nice to develop things to the point where one can differentiate a polynomial, since that would help to illustrate how your method works and demonstrate that it's usable. For this purpose, it suffices to prove that if $n>0$ is an integer, the derivative of $x^n$ equals $0$ at $0$ and equals $n$ at $1$; the result at other nonzero values of $x$ follows by scaling. Doing this for $n=2$ is fine if the generalization to $n>2$ is obvious.","['calculus', 'definition', 'education', 'derivatives', 'limits']"
208022,Meaning of functions that vanish at a point,"I realize this may be a very thick question, but I have been wondering for some time. Sometimes I am asked to prove or read proofs involving ""functions that vanish at a point"" or ""every point"" or something along these lines. The problem is I do not know what it means for a function to vanish at a point. It sounds like it means the function goes to 0 as a sequence of points arrives at the point, but if the function is continuous, doesn't that just mean the function is equivalently 0 at the point? I am basically confused what ""vanish"" means.","['functions', 'convergence-divergence', 'real-analysis']"
208026,Tail bounds for square of sub-exponential random variable,"Let $X$ be a sub-exponential random variable as defined in section 5.2.4 of Roman Vershynin's notes available here: http://www-personal.umich.edu/~romanv/papers/non-asymptotic-rmt-plain.pdf . In that case, there exists exponential tail bounds for $X-\mathbb{E}X$. But I need exponential tail bounds for $X^2-\mathbb{E}X^2$. Any ideas or pointers to relevant literature will be appreciated.",['probability-theory']
208027,Probability of one event given the probability of two other events,"Let $A$, $B$, and $C$ be events. Suppose $P(A) \ge .9$, $P(B) \ge .8$, and $P(A \cap B \cap C)=0$. Show that $P(C) \le .3$. Now, I tried using the inclusion-exclusion principle to solve this, but I'm getting nowhere. Perhaps that is the correct way of starting, but I'm looking at it the wrong way? It's been a little white since I've worked with this, so I'm not sure I'm on the right track. Also, is it correct that $P(A \cap B \cap C)=0$ means that the events $A$, $B$, and $C$ are disjoint (but not necessarily $A \cap C$, $A \cap B$, and $B \cap C$)? Any hints would be appreciated. Thanks.",['probability']
208049,Find $\frac{d}{dx}(\cos x)$,"Find $\dfrac{d}{dx}(\cos x)$ I know the answer is $-\sin x$ only by process of elimination. I can find solution graphically but I need to know algebraically. Here is my proof so far. $\begin{align*} \dfrac{d}{dx}\cos x=\lim_{h\to 0}\dfrac{\cos (x+h)-\cos x}{h} &=\lim_{h\to 0}\dfrac{\cos x\cos h-\sin x\sin h-\cos x}{h}
\end{align*}$ And that's where I end up and I have no clue where to go from here. Can someone please give me the next step but not the complete answer.","['trigonometry', 'calculus', 'derivatives']"
208066,fields are characterized by the property of having exactly 2 ideals [duplicate],"This question already has answers here : Closed 11 years ago . Possible Duplicate: A ring is a field iff the only ideals are $(0)$ and $(1)$ Michael Artin's Algebra in the introduction of maximal fields, there was a sentence stated that fields are characterized by the property of having exactly 2 ideals. what does mean? can someone elaborate on this ? I don't actually remember there are such proofs.","['ring-theory', 'ideals', 'abstract-algebra', 'field-theory']"
208075,Extracting an (almost) independent large subset from a pairwise independent set of Bernoulli variables,"Let $n>1$, and let $X_1,X_2, \ldots ,X_n$ be non-constant random variables with values in $\lbrace 0,1 \rbrace$. Let us say that a subset of variables $X_{i_1},X_{i_2}, \ldots,X_{i_d}$ is complete if the vector $\overrightarrow{X}=(X_{i_1},\ldots,X_{i_d})$ satisfies $P(\overrightarrow{X}=\overrightarrow{c})>0$ for any $\overrightarrow{c}\in \lbrace 0,1 \rbrace^d$. Prove or find a counterexample :  if $X_1,X_2, \ldots ,X_n$ are pairwise independent Bernoulli variables, then we may extract a complete subset of cardinality at least $t+1$, where $t$ is the largest integer satisfying $2^{t} \leq n$. This is true for $n=3$ (and hence also true for $n$ between $3$ and $7$), as is shown  in the main answer to that MathOverflow question . (That other MathOverflow question is also related, and provides several links) If true, this result is sharp, as can be seen by the classical example of taking all arbitrary sums modulo 2 of an initial set of fully independent $t+1$ Bernoulli variables. This produces a set of pairwise independent $2^{t+1}-1$ variables, and where the maximal cardinality of a complete subset is $t+1$. Update 10/10/2012 : By induction, it would suffice to show the following : if $X_1, \ldots ,X_t$ is a fully independent set of $t$ Bernoulli variables and $X$ is another Bernoulli variable, such that the pair $(X_i,X)$ is independent for each $i$, then there are coefficients $\varepsilon_0,\varepsilon_1, \ldots ,\varepsilon_t$ in $\lbrace 0,1 \rbrace$ such that, if we put $$
H=\Bigg\lbrace (x_1,\ldots,x_t,x) \in \lbrace 0,1 \rbrace ^{t+1} \Bigg| x=\varepsilon_0+\sum_{k=1}^{t}\varepsilon_kx_k \ {\sf mod} \ 2\Bigg\rbrace,  \ \overrightarrow{X}=(X_{1},\ldots,X_{t},X)
$$
then $P(\overrightarrow{X}=h)>0$ for any $h\in H$.","['probability', 'combinatorics']"
