question_id,title,body,tags
3407480,Strictly increasing bounded function of class $C^1$,"Let $f:\mathbb{R} \to \mathbb{R}$ be a strictly increasing bounded function of class $C^1$ . Prove that there exists a sequence $\{x_n\}_n$ of real numbers such that $x_n\to\infty$ and $\lim_{n \to \infty} f'(x_n)=0$ . Then construct a strictly increasing bounded function $f:\mathbb{R} \to \mathbb{R}$ of class $C^1$ such that the $\lim_{x \to \infty} f' (x)$ does not exist. I know that if we assume $a_n$ increases ( $a_{n+1}\ge a_n$ ), then either it is bounded, or not. If yes, then it converges to the $\sup a_n$ , else it goes to $+\infty$ . Since it is bounded here it goes to a limit and so eventually it approaches the limit and derivative approaches 0. But I dont really know how to prove it rigorously and construct such a function such that limit DNE.","['real-numbers', 'calculus', 'functions', 'real-analysis']"
3407489,empty set confusion,$\neg\left (\neg{\left (A\setminus A  \right )}\setminus A  \right )$ $A\setminus A $ is simply empty set and $\neg$ of that is again empty set. Empty set $\setminus$ A is empty set or? But every empty set is included in every set? I am confused when it comes to this..,['elementary-set-theory']
3407592,Prove $f(x) \in f(A) \implies x \in A$ if $f$ is injective and $b \in B \implies f^{-1}(b) \in f^{-1}(B)$ if $f$ is surjective,"Let $f: X \to Y$ a function. Let $A \subseteq X, \ B \subseteq Y.$ Prove $f(x) \in f(A) \implies x \in A$ if $f$ is injective and $b \in B \implies f^{-1}(b) \in f^{-1}(B)$ if $f$ is surjective. Proof: Suppose $x_1 \in A, \ x_2 \in X - A$ s.t. $x_1 \ne x_2$ . By definition of $f$ we have $f(x_1), f(x_2) \in f(A)$ and since $f$ is injective $f(x_1) \ne f(x_2).$ This shows(?) every element in $f(A)$ has a preimage in $A$ ? Suppose $f$ is surjective. Then for any $b \in B$ , there's $x \in X$ . Since $f^{-1}(B) \subseteq X$ , every element in $B$ must have a preimage in $f^{-1}(B).$ Does the proof above make sense? If not, what can I change to fix it? Is there a better way? Thanks.","['proof-verification', 'functions', 'discrete-mathematics', 'elementary-set-theory', 'algebra-precalculus']"
3407635,Is evaluating the limit inside brackets then applying power valid?,"I am attempting to find the limit of the sequence $$
\frac{(2n+3)^n}{(3n+1)^n}
.$$ I started by making this into one fraction with a single power $$
\left(\frac{2n+3}{3n+1}\right)^n
$$ after which I calculated that the fraction as $n \to \infty =\frac{2}{3}$ , and this to the power $\left(\frac{2}{3}\right)^n =0$ . However, I feel as though what I did wasn’t valid. Was it incorrect? And if so, why, and what should I have done? (note: we are not yet supposed to use L’Hopitals or logarithms)","['limits-without-lhopital', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
3407648,Evolution of laplacian under Ricci flow,"I'm trying to understand this lemma on ""Hamilton's Ricci Flow"", by Ben Chow. but I can't see how he got to the equality outlined in red (the author mentions the $\cdot$ stands for dot product or standard multiplication).  I've done some work already but didn't quite get what I want. Now, it's easy to show that: $$
\frac{\partial}{\partial t} g^{i j}=-g^{i k} g^{j \ell} h_{k \ell}
$$ where I'm assuming that $$
\frac{\partial}{\partial t} g_{i j}=h_{i j}
$$ where $h$ is some symmetric $2$ -tensor. Now, if $f$ is an arbitrary smooth function on $M$ , the product rule gives us: $$
\begin{aligned} \frac{\partial}{\partial t}(\Delta f) &=\frac{\partial}{\partial t}\left[g^{i j}\left(\partial_{i} \partial_{j}-\Gamma_{i j}^{k} \partial_{k}\right) f\right] \\ &=\left(\frac{\partial}{\partial t} g^{i j}\right) \nabla_{i} \nabla_{j} f-g^{i j}\left(\frac{\partial}{\partial t} \Gamma_{i j}^{k}\right) \nabla_{k} f+\Delta\left(\frac{\partial}{\partial t} f\right) \end{aligned}
$$ and we could replace $\frac{\partial}{\partial t} g^{i j}$ with the expression found earlier, but I can't see how that'd give us the part outlined in red. Comparing both expressions, I guess my doubt is how to show the following equality: $$
-\frac{\partial}{\partial t} g_{i j} \cdot \nabla_{i} \nabla_{j} = \left(\frac{\partial}{\partial t} g^{i j}\right) \nabla_{i} \nabla_{j} +\Delta\left(\frac{\partial}{\partial t} \right)
$$ which is where I really got stuck. I'm grateful for any help.","['riemannian-geometry', 'ricci-flow', 'laplacian', 'manifolds', 'differential-geometry']"
3407671,Prove that there exists $B$ such that $\mu(f(B))>0$.,"Let $\mu$ be the Lebesgue measure. Suppose $f:\mathbb{R}\rightarrow\mathbb{R}$ is measurable. $f(B)$ is measurable for every Borel set $B\subset \mathbb{R}$ , and $\{y:f^{-1}(y) \text { is infinite}\}$ has measure $0$ . Suppose there is some $A\subset\mathbb{R}$ such that $\mu(A)=0$ and $\mu (f(A))>0$ . Prove there is a closed set $F$ such that $\mu(F)=0$ and $\mu(f(F))>0$ . My try: $\mu(f(A))>0$ implies that there exists a subset $C$ of $A$ such that $f(C)$ is not measurable. I try to find a closed set $B$ such that $C\subset B$ and $\mu(B)=0$ . Then we can get $\mu(f(B))>0$ . However, I don’t know how to find such $B$ .","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3407679,Arrange all numbers from 1 to n such that no 3 of them are in Arithmetic Progression,"Is there any possible arrangement of numbers all from $1$ to $n$ such that in the resultant array of numbers, no subsequence of length $3$ is in Arithmetic Progression.
For example, in $1,3,2,4,5$ , there is a subsequence of length $3$ that is in AP, that is, $1,3,5.$ But in $1,5,3,2,4$ there is no subsequence of length $3.$ I am trying to find a possible arrangement for larger values of $n,$ but unable to do so.","['arithmetic-progressions', 'combinatorics']"
3407718,Group with simple relations,"I'm working on the following algebra problem: Prove that if elements $a$ and $b$ of some group satisfy the relations 1) $a^5 = b^3 = 1$ and 2) $b^{-1}ab = a^2$ then $a = 1$ . Here is an example of what I've tried: If $a^5 = 1$ , then using relation 2) , multiplying on the right by $a^3$ on both sides gives * $b^{-1}aba^3 = 1$ . If $b^3 = 1$ , then using relation 2), multiplying on the left by $b^4$ on both sides gives ** $ab = b^4a^2$ . Then, I tried substituting ** into *. Unfortunately, this just gets me relation 1) back again. It seems that my methods to exploit the relations always end up in simply recovering relation 1), rather than showing $a = 1$ . Is there anything more clever I can try here? Thanks!","['group-theory', 'abstract-algebra']"
3407722,How do I prove if the following functions are differentiable at the given value?,I have been stumped on the attached question for a while. What would I have to do in order to prove if the functions are differentiable at the given value? I'm only stuck on how to start the problem.,"['limits-without-lhopital', 'limits', 'calculus', 'derivatives']"
3407753,Questions about Approximating a Function at Infinity,"I was playing around today and happened across a nice method for approximating functions at infinity. The first example of a function I found that my method worked for was $$f(x) = \frac{1}{1+x^2}$$ The method is outlined as follows. Define an auxiliary function $g$ as $$g(x) := f\left(\frac{1}{x}\right)$$ Since $g$ is analytic everywhere except possibly at the point $0$ , we can express $g$ as $$g(x) = \sum_{n=0}^\infty\frac{g^{(n)}(x_0)}{n!}(x-x_0)^n$$ for any point $x_0 \neq 0$ . Now define $f_a$ as $$f_a(x) := \lim_{x_0 \to 0}\sum_{n=0}^\infty\frac{g^{(n)}(x_0)}{n!}\left(\frac{1}{x}-x_0\right)^n$$ I truncated the series definition of $f_a$ at the third term and took $x_0 = 0.0001$ . Then I plotted the resulting graph (Dotted Blue) and compared it to the graph of $f$ (Solid Black) as shown below. So we can see that "" $f_a$ approximates $f$ at infinity"", in some sense. I suspect that this method is related to asymptotic expansions, but I am having trouble formalizing the ideas presented here. My two questions are: Formally, what is the process that I am getting at here? For what class of functions can we expect this method to work? Being analytic is a pretty obvious requirement, but the method also seemed to fail for functions like $\dfrac{\sin(x)}{1+x^2}$ . Moreover, if anyone has some online references that they could point me to for more information, that would be appreciated as well.","['approximation', 'asymptotics', 'calculus', 'functions', 'taylor-expansion']"
3407813,Group which can be written as a union of proper normal subgroups,"Let $G$ be a group and $\{N_j\}_{j \in J} $ be a family of proper normal subgroups of $G$ such that $G=\cup_{j \in J} N_j$ and $N_i \cap N_j =\{e\}$ for every $i\ne j \in J$ . Then how to prove that $G$ is abelian ? I can show that $ab=ba$ whenever $a\in N_i, b \in N_j$ for some $i\ne j$ . So if I can only show each $N_i$ is abelian, we're done. Unfortunately I'm unable to show that. Please help","['group-theory', 'normal-subgroups', 'abelian-groups']"
3407825,Chance of winning a raffle with a special rule,"A person is hosting a raffle event. There are 1000 participants in the raffle.
The raffle draw will produce one winner. The Special Rule The host is also 1 of 1000 participants, but he announces he will not claim the prize, so, if the host wins the raffle, a re-draw will happen and he will be removed from the second draw which makes the total number of participant to 999. Question What is the probability of me winning the raffle? Note This might be a really silly question, but i cant seem to come up with an answer :). My attempts: Answer 1. Chance is $1/1000 + 1/1000 * 1/999$ = (chance of me winning first round + chance of host winning * chance of me winning second round) Answer 2. Chance is $1/999$ because logically speaking, there are 999 people who can win, chance is just 1/999. Edit : just did a calculation to actually calculate above 2 answers, they have the same result :)",['probability']
3407917,General formula for evaluating integrals of the form $\displaystyle\int_0^\infty x^ae^{-bx}dx$,Find a general formula for evaluating integrals of the form $\displaystyle\int_0^\infty x^a e^{-bx}dx.$ I've been pondering about this question for some time. Obviously integration by parts is way too inefficient. I know there's recursion involved and maybe there's a way to find a general formula through intuition.,"['integration', 'real-analysis']"
3407995,"Why is a topology easily described by its subbasis, but a $\sigma$-algebra cannot be expressed by its generating set?","Let $X$ be a set and $ \mathscr{A} \subseteq 2^X $ a collection of subsets. Describing the topology $\tau(\mathscr{A})$ generated by $\mathscr{A}$ is easy: any open set in $\tau(\mathscr{A})$ is just an arbitrary union of finite intersections of elements in $\mathscr{A}$ . However, the $\sigma$ -algebra $\sigma(\mathscr{A})$ generated by $\mathscr{A}$ is a completely different story. One can never hope to express $\sigma(\mathscr{A})$ explicitly in terms of $\mathscr{A}$ , even if $\mathscr{A}$ is an algebra of sets. In fact, let's say $\mathscr{A}$ is an algebra of sets. Let $\kappa(\mathscr{A})$ be the collection of sets obtained from taking complements, countable unions and countable intersections of members of $\mathscr{A}$ countably many times . Then $\kappa(\mathscr{A})$ is still a very small subcollection compared to $\sigma(\mathscr{A})$ , nowhere approaching all of $\sigma(\mathscr{A})$ . The collection $\sigma(\mathscr{A})$ is simply too huge to have any explicit expression. Why is it so much harder to describe a $\sigma$ -algebra than a topology, even though the definitions of both look simple and similar?","['set-theory', 'general-topology', 'measure-theory', 'real-analysis']"
3408082,"If $f[A]$ and $A$ intersect for (most) $A \subset \mathbb{R}$, then how far is $f$ from the identity?","$\textbf{Definition}$ : We say $f:\mathbb{R} \to \mathbb{R}$ is intersecting if for every nonempty $A \subset \mathbb{R}$ , $f[A] \cap A \neq \varnothing$ . There is only one intersecting function: the identity. The reason for this is that $f[\{x\}] \cap \{x\} \neq \varnothing$ forces $f(x)=x$ . If we impose restrictions on the subsets $A$ we consider (say, for instance, we rule out singletons), must $f$ still be the identity? Let's try this. We can first introduce some definitions. $\textbf{Definition:}$ For any cardinal $\ell$ , we say $f:\mathbb{R} \to \mathbb{R}$ is $\ell$ - intersecting if for every nonempty $A \subset \mathbb{R}$ with $|A| \geq \ell$ , $f[A] \cap A \neq \varnothing$ . $\textbf{Definition:}$ For any $f:\mathbb{R} \to \mathbb{R}$ , its deviation is the cardinality of the set $\{x \in \mathbb{R} : f(x) \neq x\}$ . I'm going to write down some results for the $2$ -intersecting case. Suppose $f$ has the property that for every $A \subset \mathbb{R}$ with $|A| \geq 2$ that $f[A] \cap A \neq \varnothing$ . There indeed exists a non-identity example here, one with deviation $3$ , in fact. Let $f(x) = x$ for $x \in \mathbb{R} \setminus \{1,2,3\}$ and let $f(1)=2, f(2)=3, f(3)=1$ . Is there a $2$ -intersecting $f$ with deviation $\geq 4$ ? No. The argument for this is combinatorial. Suppose an $f$ with this property existed, and let $A=\{x_1, x_2, x_3, x_4\}$ be a set of four elements with $f(x_i) \neq x_i$ for each $x_i \in A$ . We first note that $f$ restricted to $A$ defines a function $f_{A}:A \to A$ . To justify this claim, we can argue by contradiction. Suppose for some $x_i \in A$ that $f(x_i) = y \notin A$ . Let $x_j, x_k$ be distinct elements in $A$ , also both distinct from $x_i$ . Then since $f[\{x_i, x_j\}] \cap \{x_i, x_j\} \neq \varnothing$ , and $f[\{x_i, x_k\}] \cap \{x_i, x_k\} \neq \varnothing$ , it's easy to see this implies $f(x_k) = f(x_j) = x_i$ , which implies $f[\{x_k, x_j\}] \cap \{x_k, x_j\} = \varnothing$ , contradiction. We also note the restricted function $f_{A}$ is injective. To see this, suppose, say, $f(x_i) = x_{m}$ and $f(x_j) = x_{m}$ , and each of $i,j, m$ are distinct. Then $f[\{x_i, x_j\}] \cap \{x_i, x_j\} = \varnothing$ , contradiction. Since $A$ is finite, this implies it's bijective too. Hence $f_{A}$ corresponds to a permutation in $S_4$ without fixed points. It cannot have two cycles, since if $x_j, x_i$ are in distinct two-cycles, then $f[\{x_i, x_j\}] \cap \{x_i, x_j\} = \varnothing$ . Hence it's cyclic. But if it's cyclic, then $f[\{x_{i_2}, x_{i_4}\}] \cap \{x_{i_2}, x_{i_4}\} = \varnothing$ , where $x_{i_2}, x_{i_4}$ are respectively the second and fourth elements of the cycle. This establishes the contradiction. What eludes me is the general case. The combinatorics seem to get harder if $\ell \geq 3$ . $\textbf{Problem 1:}$ The finite case. Let $\ell$ be a finite cardinal (i.e., a positive integer). Then is it true that any $\ell$ -intersecting map $f:\mathbb{R} \to \mathbb{R}$ has finite deviation? If this is true (which I suspect), is there a closed form for the largest possible deviation in terms of $\ell$ ? $\textbf{Problem 2:}$ The infinite case. What is the maximum deviation of an $\aleph_{0}$ -intersecting map $f:\mathbb{R} \to \mathbb{R}$ ? What is the maximum deviation of a $\mathfrak{c}$ -intersecting map $f:\mathbb{R} \to \mathbb{R}$ ? In particular, are there such functions with infinite deviation? You will notice that we don't really use any of the analytic or algebraic structure of $\mathbb{R}$ here, so really these notions can be generalized to functions $f:X \to Y$ for arbitrary sets $X,Y$ . We could, however, ask different kinds of questions similar to the ones described above which make use of the structure of $\mathbb{R}$ . Instead of considering subsets $A$ with sufficiently large cardinality, we could alternatively consider subsets $A$ which are (nondegenerate) intervals, as has been suggested in the comments, or possibly subsets which are nonempty open sets. In a broad sense, we're interested in finding ""highly non-identity"" functions $f$ satisfying $f[A] \cap A \neq \varnothing$ for $A$ in some 'large' collection of subsets of $\mathbb{R}$ . If you have the solution to a different problem, but one which is similar in the broad sense described above, you're free to share it.","['elementary-set-theory', 'combinatorics', 'real-analysis']"
3408083,Passengers probability problem,"There is a train with $m$ wagons and $n$ $(n\geq m)$ passengers. Calculate the probability where for every wagon there is at least one passenger to enter. Let $A$ be ""there is a passenger on every wagon"", $A_k$ be ""there is a(t least one) passenger on the $k$ -th wagon"". If we want $A$ to be realized, every $A_k$ have to realize: $A = \bigcap_{k=1}^{m} A_k.$ De Morgan's laws: $\overline{A}= \bigcup_{k=1}^{m} \overline{A_k}$ , where $\overline{A_k}$ is ""there are no passengers on the $k$ -th wagon"". After this step I couldn't follow my probability teacher so if anyone could explain it to me, I would be very very grateful: $P(\overline{A_i}) =\frac{(m-1)^n}{m^n}$ , $P(\overline{A_i}\overline{A_j}) = \frac{(m-2)^n}{m^n}$ etc. After this we could conclude that in every wagon can be $n$ passengers, but I don't think it's the case. What does it actually mean?","['probability-theory', 'probability']"
3408117,In Pollard's p-1 how is M calculated?,"In the Wikipedia article the following equation is given for calculating $M$ : ${\displaystyle M=\prod _{{\text{primes}}~q\leq B}q^{\lfloor \log _{q}{B}\rfloor }}$ for some bound $B$ Variations I have seen include $M=r!$ where $r$ starts at a low number and increases to $B$ . where the LCM is computed of all the numbers between 2 to $B$ . ( example ) So basically the point is to make $M$ a highly composite number. Is this correct? Are all these methods equally valid or do some produce different results? In practice it would seem factional is the easiest to compute, though would result in largest number. Small aside question: in the next step when the GCD is calculated, can you take $a^M \bmod{n}$ or $a^M-1 \bmod{n}$ ? Does it make a difference?","['factorial', 'number-theory', 'gcd-and-lcm', 'discrete-mathematics', 'algorithms']"
3408132,"Prove that $\Phi(y, u, v) := \int_{u}^{v} f(x, y) dx$ is a $C^1$ function.(differentiation under the integral sign)","I am reading ""A Course in Analysis vol.3"" by Kazuo Matsuzaka. The author wrote the following fact without a proof. I think it is easy to show that $D_2 \phi(y, u, v) = f(v, y)$ and $D_3 \phi(y, u, v) = -f(u, y)$ are continuous. But I cannot show that $D_1 \phi(y, u, v) = \int_{u}^{v} D_2 f(x, y) dx$ is continuous. Please tell me the proof. Let $f(x, y)$ be a continuous function from $S = [a, b] \times I \subset \mathbb{R}^2$ to $\mathbb{R}$ , where $I$ is an interval in $\mathbb{R}$ . Suppose that $D_2 f(x, y)$ exists in $S$ and is continuous in $S$ . Let $\Phi(y, u, v) := \int_{u}^{v} f(x, y) dx$ be a function from $I \times [a, b] \times [a, b]$ to $\mathbb{R}$ . Then, $\Phi(y, u, v)$ is a $C^1$ function.","['partial-derivative', 'multivariable-calculus']"
3408142,Convolution of a probability measure with itself is the same measure,"Say you have a probability measure $\mu$ on $\mathbb{R}$ . Suppose $\mu*\mu = \mu$ . Then prove that $\mu = \delta_0$ i.e $\mu$ is the Dirac measure at zero. I know this can be proved using uniqueness characteristic functions. However, I was wondering if there is a way to prove this purely using Measure Theoretic Probability. I've tried to prove that $\mu(\{0\}) = 1$ since this should be sufficient but I've not been able to. Despite that, I can't help but feel there is some trick or technique which I've missed.","['measure-theory', 'probability-theory']"
3408198,Sum related to discrete Fourier Transform,"I am interested in calculation of the following sum: \begin{equation}
\sum_{n=0}^{N-1}\frac 1 {1-a\cos(2\pi n/N)}
\end{equation} where $0<a<1$ . 
I Tried to pass to the exponential notation for the cosine in order to get a geomtric sum, but I could not get to any useful result. Is it possible to solve it analytically? If so, how?","['fourier-transform', 'sequences-and-series', 'fourier-series', 'trigonometry', 'geometric-series']"
3408239,Part 2: Does the arithmetic mean of sides right triangles to the mean of their hypotenuse converge?,"A primitive Pythagorean triplet is a triplet $a^2 + b^2 = c^2$ be where $a,b,c$ have no common factors and is generated by $a = r^2 - s^2, b = 2rs, c = r^2 + s^2$ where $r > s \ge 1, \gcd(r,s) = 1$ and exactly one of the two numbers $r$ and $s$ is even. Clearly as $r$ increases, the number of primitive triplets formed for a given $r$ increases since the number of $s$ satisfying the above conditions increases. Claim: Let $c_1,c_2,\ldots$ be the hypotenuse and $b_1,b_2,\ldots $ be the corresponding longer of the two orthogonal sides formed for Pythagorean triangles for all $r \le x$ then as $x \to \infty$ , $$\frac{b_1 + b_2 + b_3 + \cdots}{c_1 + c_2 + c_3 + \cdots} = \sqrt{2} - \frac{1}{2}$$ Can this claim be proved or disproved? The difference between this question and the related question : Part 1: Does the arithmetic mean of sides right triangles to the mean of their hypotenuse converge? is that here the triangles are in sequenced in ascending order of $r$ and $s$ where as in the related question, they are sequenced in ascending order of the hypotenuse and depending on the choice of sequencing, the limiting value differs.","['number-theory', 'geometry', 'real-analysis', 'sequences-and-series', 'limits']"
3408245,The Schur-Zassenhaus Theorem and properties of an associated matrix,"Let $A$ be an abelian normal subgroup of a group $G$ and let { $g_i| 1\le i\le m$ }, be a set of left coset representatives of $A$ in $G$ . Then, for each choice of $i$ and $j$ , we have $$g_ig_j=g_ra_{ij}$$ for some $r$ and $a_{ij}\in A$ . In the case that $m$ and $|A|$ are coprime, properties of the matrix $(a_{ij})$ and how it is transformed under a change of representatives provide a reasonably elegant way of writing out a proof that $A$ has a complement in $G$ (the Schur-Zassenhaus Theorem). However, my question is - have properties of these matrices been studied in general i.e. when $m$ and $|A|$ are not necessarily coprime? The following example shows that there might be properties worth investigating (if this work has not already been done). For $(a_{ij})$ defined as above, the product of all the elements of $(a_{ij})$ is in $Z(G)$ . Proof For any choice of $i,j,k$ let $g_ig_j\in g_rA$ , $g_jg_k\in g_sA$ , $g_ig_jg_k\in g_tA$ . Then $$(g_ig_j)g_k= g_ra_{ij}g_k=g_rg_k(a_{ij})^{g_k}=g_ta_{rk}(a_{ij})^{g_k}$$ and also $$g_i(g_jg_k)= g_ig_sa_{jk}=g_ta_{is}a_{jk}.$$ Let $z$ denote the product of all the elements of $(a_{ij})$ and let $a(k)$ denote the product of all the elements in the $k$ th column of $(a_{ij})$ . Equating the two expressions for $g_ig_jg_k$ gives $$a_{rk}(a_{ij})^{g_k}=a_{is}a_{jk}$$ and therefore $$\prod_{i,j}a_{rk}(a_{ij})^{g_k}=\prod_{i,j}a_{is}a_{jk}.$$ As $i$ and $j$ run over all values from $1$ to $m$ , $r$ runs over all values $m$ times and $s$ runs over all values once (independently of $i$ ). Therefore $$a(k)^mz^{g_k}=za(k)^m\implies z^{g_k}=z.$$ Then $<A,g_1, ..., g_m> \subseteq C(z)$ and so $z\in Z(G)$ .","['group-theory', 'finite-groups']"
3408246,Probable error on N-ball recurrence relation on Wikipedia,"The relation given below, given on Wikipedia https://en.wikipedia.org/wiki/N-sphere#Recurrences in recurrence section,  establishes the relation between the volume of (n+1) ball and surface area of n-ball. $$S_{n}R^{n}=\frac{dV_{n+1}R^{n+1}}{dR}={(n+1)V_{n+1}R^{n}}$$ Where, $S_n$ and $V_n$ are surface area and volumes of unit balls in n dimesions.
For $n=2$ , $S_{n} = 2\pi$ , $V_{n+1} = \frac{4 \pi}{3}$ i.e surface area of a circle and volume of a sphere respectively. Therefore the relation becomes, $$2\pi R^{2} = 4\pi R^2 = 4\pi R^2$$ I think the relation will hold if we say, $$S_{n+1}R^{n}=\frac{dV_{n+1}R^{n+1}}{dR}={(n+1)V_{n+1}R^{n}}$$ can anyone check and confirm my understanding?","['data-analysis', 'geometry', 'dimension-theory-analysis']"
3408249,Compute $\lim_{x\rightarrow 0 }\biggr ( \dfrac{1}{x}\ln (x!)\biggr )$,"I want to compute the following limit $$\lim_{x\rightarrow 0 }\biggr ( \dfrac{1}{x}\ln (x!)\biggr )$$ Since factorial is only defined for integers, we must use the gamma function. $$\lim_{x\rightarrow 0} \dfrac{\ln (\Gamma (x+1))}{x} = \lim_{x\rightarrow 0}\dfrac{\dfrac{d}{dx}(\ln(\Gamma(x+1))}{\dfrac{d}{dx}(x)} = \lim_{x\rightarrow 0} \dfrac{\Gamma'(x+1)}{\Gamma(x+1)} = \psi(1)$$ Where $\psi$ is the digamma function. $$\psi(x+1) = -\gamma +\int^{1}_{0}\dfrac{1-t^{x}}{1-t}dt$$ What we want is $$\psi(1) = -\gamma +\int^{1}_{0}\dfrac{1-t}{1-t}dt = -\gamma$$ where $\gamma $ is Euler-Mascheroni constant. Is there a way to compute this limit without using digamma function?","['gamma-function', 'limits', 'digamma-function']"
3408266,Tossing a ring over pegs: Snaring exactly one peg?,"Let there be a point peg at every $\mathbb{Z}^2$ lattice point. Let a ring be a radius $r$ circle. Q1 . Which value of $r$ maximizes the chance that a
randomly placed ring will enclose exactly one peg? Small $r$ may capture no pegs; large $r$ may capture
more than one peg. I know this is elementary, but I am not seeing an easy
route to calculate $r$ . My real question is this generalization: Q2 . Which value of $r$ maximizes the chance that a
randomly placed $(d{-}1)$ sphere in $\mathbb{R}^d$ will enclose exactly one lattice
point of $\mathbb{Z}^d$ ? Questions inspired by ""ring toss"": (Image from gameplanent .) Added . Following @GussB's suggestion, I compute $r=0.541$ :","['geometry', 'probability', 'plane-geometry']"
3408310,Sequence with promising sum.,"A sequence is defined as $a_{k+1}=a_k(a_k+1)$ and $a_1=\frac12$ for $k>0$ . What is the smallest integer greater than $$\frac{1}{a_1+1}+\frac{1}{a_2+1}+\frac{1}{a_3+1}\cdots+\frac{1}{a_{100}+1}?$$ Using the definition, $a_k+1=\frac{a_{k+1}}{a_k}$ . So our sum is $$\frac{a_1}{a_2}+\frac{a_2}{a_3}+\frac{a_3}{a_4}\cdots \frac{a_{100}}{a_{101}}$$ This looks promising, but I´m not sure how to proceed since I cannot find the explicit formula.",['algebra-precalculus']
3408328,Product of sines to sum,"I stumbled across the following identity in a system I'm considering: \begin{align}
\prod_{j=1}^N \sin a_j
\end{align} which I need to rewrite as a sum (or otherwise) of sines or other expressions. $a_j$ in my context must be treated as completely general real variables, but with distinct numerical values. (At a certain point I am required to take a sum over $p$ when $a_j\mapsto pb_j+c_j$ . If the expressions that arise remain simple that would be a tremendous plus. It is fine if we get products of rational functions, as these can be simplified using Heaviside's decomposition.) So I know of the double angle formula, which yes, in principle allows for rewriting this expression as a sum: \begin{align}
\prod_{j=1}^N \sin a_j &= \frac1{2^{N-1}} \sum_{\sigma_2,\cdots,\sigma_N=\pm} \left[\prod_{j=2}^N \sigma_j\right]\sin\left[a_1 + \sum_{j=2}^N \sigma_j a_j + \frac{\pi(1-N)}2\right]
\end{align} however it is not clear what the meaning is (in the analytical sense) of the resulting expressions. In my system $N$ will get very large, and it is not clear how to proceed with the expressions that follow (what is an infinite-fold signed sum of sines?! No idea either). What I was considering is using Euler's reflection. Intuitively this should lead to an identity containing a Barnes $G$ function. I would like to consider expressions of the form \begin{align}
\prod_1^N \Gamma(a_j)
\end{align} but I have not found general identities relating this to Barnes $G$ , and it is also not clear how to sum Barnes $G$ functions given my context above. I'd here like to discuss alternatives options and hope Math.SE can offer insight that I could not find. EDIT: a potential option would be to proceed with the expression and use the following result of the binomial theorem in reverse: \begin{align}
\prod_{j=1}^N f_j &= \frac1{2^{N-1}N!}\sum_{\sigma_{2,\dots,N} = \pm} \left(\prod_{j=2}^N \sigma_j \right) \left(f_1+\sum_{i=2}^N \sigma_j f_j\right)^N
\end{align} This would require us to find a function $f$ that satisfies it. EDIT 2: Possibly useful: Using the above identity, one could for example perform a subset of summations like so: \begin{align}
\sum_{p=-n}^n \prod_{j=1}^N \sin (pb_j+c_j) &= \frac1{2^{N-1}} \sum_{\sigma_2,\cdots,\sigma_N=\pm} \left[\prod_{j=2}^N \sigma_j\right] \sum_{p=-n}^n\sin\left[pb_j+c_j + p\sum_{j=2}^N \sigma_j b_j + \sum_{j=2}^N c_j + \frac{\pi(1-N)}2\right]\\
&= \frac1{i2^{N}} \sum_{\sigma_2,\cdots,\sigma_N=\pm} \left[\prod_{j=2}^N \sigma_j\right] \left[\exp\left[ic_j + i\sum_{j=2}^N c_j + \frac{i\pi(1-N)}2 \right] \left(\sum_{p=-n}^n \exp\left[ipb_j + ip\sum_{j=2}^N \sigma_j b_j\right]\right) - \exp\left[-ic_j - i\sum_{j=2}^N c_j + \frac{i\pi(N-1)}2 \right] \left(\sum_{p=-n}^n \exp\left[-ipb_j - ip\sum_{j=2}^N \sigma_j b_j\right]\right)\right]
\end{align} then (heuristically) using symmetricity in dummy index $p$ to perform the summation, then perhaps using Cauchy products (in reverse) in the finite case to extract dirichlet kernels from the summation.","['gamma-function', 'trigonometry', 'angle', 'products']"
3408341,Number theory problem on divisors! [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question For any positive integer $n$ , let $d(n)$ denote the number of positive divisors of $n$ (including $1$ and itself). Determine all positive integers $k$ such that $$\frac{d\left(n^2\right)}{d(n)} = k$$ for some $n$ . Please help me solve this number theory problem. I have tried a lot and found two values of k, $1$ and $3$ , but I don't know if there are some others.",['number-theory']
3408361,"Prove or disprove: $\lim_{n\to\infty}(a_n n\ln n)=0$, where $a_n>0$, $a_{n+2}-a_{n+1}\geq a_{n+1}-a_n$, and $\sum_{k=1}^{n}a_n$ is bounded","Suppose, for all $n \in \mathbb{N^+}$ , it holds that (1) $a_n>0$ ; (2) $a_{n+2}-a_{n+1}\geq a_{n+1}-a_n$ ; (3) $\sum_{k=1}^{n}a_n$ is bounded. Prove or disprove $$\lim\limits_{n \to \infty} (a_n\cdot n\cdot\ln n )=0$$ First, since (1) and (3), we can obtain $\sum_{n=1}^{\infty}a_n$ is convergent,hence $\lim\limits_{n \to \infty}{a_n}=0.$ Now,notice that $\sum_{k=1}^{n-1}(a_{k+1}-a_{k})=a_n-a_1$ , and let $n \to \infty$ , we can obtain $\sum_{n=1}^{\infty}(a_{n+1}-a_n)=-a_1$ .But how to go on with these? Maybe we may consider $$\ln n \sim \sum_{k=1}^{n}\frac{1}{k}, n\to \infty.$$","['limits', 'calculus', 'sequences-and-series']"
3408433,What is wrong with my proof of Vitali convergence theorem?,"Below statement is in The Elements of Integration and Lebesgue Measure, BARTLE, 76p. Vitali Convergence Theorem. Let $\left\{ f_{n} \right\}$ ba a sequence in $\mathcal{L}_{p} ( X , \Sigma , \mu)$ and $1 \in [1, \infty)$ . Then the following three conditions are necessary and sufficient for the $\mathcal{L}_{p}$ convergence of $\left\{ f_{n} \right\}$ to $f$ : (i) $\left\{ f_{n} \right\}$ converges to $f$ in measure (ii) For each $\varepsilon > 0$ there is a set $E_{\varepsilon} \in \Sigma$ with $\mu(E_{\varepsilon}) < \infty$ such that if $F \in \Sigma$ and $F \cap E_{\varepsilon} = \emptyset$ , then $$\int_{F} |f_{n}|^{p} d \mu < \varepsilon^{p} \qquad \forall n \in \mathbb{N}$$ (iii) For each $\varepsilon > 0$ there is a $\delta(\varepsilon) > 0$ , such that if $E \in \Sigma$ and $\mu(E) < \delta(\varepsilon)$ , then $$\int_{E} |f_{n}|^{p} d \mu < \varepsilon^{p} \qquad \forall n \in \mathbb{N}$$ Textbook says that the fact that $\mathcal{L}_{p}$ convergence of the $\left\{ f_{n} \right\}$ implies (ii) and (iii) is not difficult and is left to reader. I seconded it in first glance, but i can't prove it at all. These are proved nowhere, just left 'easy' or 'trivial'. I wasted so much time to prove these. Please tell me what is wrong. Proof $(\Rightarrow)$ (ii). It's sufficient to prove that a case $F = E_{\varepsilon}^{c}$ . Take $E_{\varepsilon}:=\left\{ x \in X : | f_{n} (x) | \ge \varepsilon \right\}$ then $$\int_{E_{\varepsilon}^{c}} |f_{n}|^{p} d \mu < \int_{E_{\varepsilon}^{c}} \varepsilon^{p} d \mu = \varepsilon^{p} \mu (E_{\varepsilon}^{c}) $$ Proof $(\Rightarrow)$ (iii). For each $\varepsilon>0$ , take $E:=\left\{x \in X : |f_{n}(x)|^{p} < \varepsilon^{p-1} \right\}$ and $\delta(\varepsilon) := \varepsilon$ . If $E \in \Sigma$ and $\mu(E) < \delta(\varepsilon)$ , then $$\int_{E} |f_{n}|^{p} d \mu < \int_{E} \varepsilon^{p-1} d \mu = \varepsilon^{p-1} \mu(E) < \varepsilon^{p-1} \varepsilon = \varepsilon^{p} $$ for all $n \in \mathbb{N}$ . Questions (ii) We can't normalize just by putting $E_{\varepsilon} = \left\{ x \in X : | f_{n} (x) | \ge \varepsilon / \mu (E_{\varepsilon}^{c})^{1/p} \right\}$ , right? And there is no guarantee for $\mu (E_{\varepsilon}) < \infty$ . I approached $$\mu (E_{\varepsilon}) = \int_{E_{\varepsilon}} d \mu  = \int_{E_{\varepsilon} \cap ( 1 \le |f_{n}|) } d \mu + \int_{E_{\varepsilon} \cap ( 1 > |f_{n}|) } d \mu$$ then the first term can be finite since $f_{n} \in \mathcal{L}_{p}$ but second term  is not. I gave up. (iii) I didn't mention any hypothesis. What i missed? I have no idea how use the condition convergence in $\mathcal{L}_{p}$ .","['measure-theory', 'convergence-divergence', 'real-analysis']"
3408462,Does $G\cong H\times K$ imply $H\unlhd G?$,"I want to prove the following exercise. If a group $G$ is the direct product of subgroups $H,K$ , then $K$ is isomorphic to $G/H$ . To prove this, I think I need first to show $H$ is normal in $G$ . I can show that there is a normal subgroup $J$ in $G$ that is isomorphic to $H.$ But I don’t know how to show $H$ is a normal subgroup in $G.$ I’m not sure but my guessing is that $gHg^{-1} = J$ for some $g \in G$ , so that $H = J.$ But . . . maybe there’s a counterexample. Also, if I show $H$ is normal somehow, I still don’t know how the conclusion of the exercise follows from it. When $G = H\times K$ means the internal direct product of its normal subgroups, I can solve the exercise. If not, I know that, by using $\pi_k : H\times K \to K$ (the canonical projection), we can show $H\times K/\ker(\pi_k)$ is isomorphic to $K.$ But how can i show that $G/H$ is isomorphic to $H\times K/\ker(\pi_k)$ ? Can somebody help? Thank you!","['direct-product', 'group-theory', 'normal-subgroups']"
3408523,Counting ways to arrange $5$ different balls into $3$ different boxes so that no box remains empty. I get $150$; official answer is $720$.,"In how many ways can $5$ different balls be arranged into $3$ different boxes so that no box remains empty? My Approach: Acc to me, since there are $2$ ways to arrange the balls ie, $(1,1,3)$ and $(1,2,2)$ the total number of methods would be $$ \frac{C^{5}_1\cdot C^4_1\cdot  C^3_3 \cdot 3!}{2!} +\frac{C^{5}_1\cdot C^4_2\cdot  C^2_2 \cdot 3!}{2!} = 150$$ By arranging the balls and then multiply by $3!$ as amount of ways $3$ different boxes can be arranged and dividing by $2!$ because one value is same. Correct Answer: However, the correct answer is given as $$\frac{5! \cdot 3!}{2!}+ \frac{5! \cdot 3!}{2!}$$ What is wrong with my approach and why does the answer vary? Edit: After reading all the answers, I also think that the question is not stated clearly and the order within the box must be important. It was taken from Skills In Mathematics algebra by Dr. SK Goyal (Chapter Permutation and Combination) if someone wants to look further upon the question.",['combinatorics']
3408586,Are functions defined in terms of ordered pairs or are ordered pairs defined in terms of functions?,"From https://www.mimuw.edu.pl/~tarlecki/teaching/ct/papers/chap1.pdf , an excerpt from an educational book about universal algebra, category theory etc, from the section that talks about many-sorted sets: Exercise 1.1.3. Extend the above definitions of union, intersection, product and disjoint union to operations on $I$ -indexed families of $S$ -sorted sets, for an arbitrary index set $I$ . For example, the definition for product is $\left(\prod{\left\langle X_i\right\rangle_{i\in I}}\right)_s=\left\lbrace {f:I\rightarrow\bigcup_{i\in I}{\left(X_i\right)_s}|{f\left(i\right)\in\left(X_i\right)_s}}\text{ for all }i\in I\right\rbrace$ for each $s\in S$ . Weird. If I understand correctly, a cartesian product is defined here as a set of tuples (as expected), BUT a tuple is defined as a function that assigns a value to the index. Problem is that, if I remember correctly, I've always been told something different: A function is defined as a set of ordered pairs (argument, value) such that no two pairs assign two different values two any given argument. Since an ordered pair is a special case of a tuple... Do we have here a situation when an ordered pair is defined as a function while a function is defined as an ordered pair? I suppose there may not be cyclic definitions in mathematics? Thus how are these two beasts (functions, tuples) typically defined?","['elementary-set-theory', 'foundations', 'functions', 'products']"
3408619,Conjugacy classes of a quotient group,"Let $G$ be a finite group and let $N$ be a normal subgroup of $G$ . Let $c(H)$ denote the number of conjugacy classes of a finite group $H$ . (a) Is it true that $c(G)\leq c(N)c(G/N)$ ? (b) When does equality hold? (c) Is the ratio $c(N)c(G/N)/c(G)$ always an integer? This question can also be phrased in terms of a short exact sequence of finite groups $$1\longrightarrow N\longrightarrow G\longrightarrow K\longrightarrow1.$$ If this short exact sequence is isomorphic to $1\to N\to N\times K\to K\to1$ then it can be shown that $c(G)=c(N)c(K)$ . However, even if the short exact sequence splits (meaning that the short exact sequence is isomorphic to $1\to N\to N\rtimes K\to K\to1$ ), it might not be the case that $c(G)=c(N)c(K)$ . For instance, $N$ and $K$ might both be abelian while $G$ is nonabelian.","['inequality', 'finite-groups', 'quotient-group', 'normal-subgroups', 'group-theory']"
3408643,$u$ harmonic and $u^3$ harmonic then $u$ constant,"Let $u(x,y)$ be a harmonic function defined in a connected open subset of $\Bbb R^2$ . Does $u^3(x,y)=(u(x,y))^3$ harmonic implies $u$ is constant? It is easily shown as true, when I replace $3$ with $2$ , so I am wondering that this holds for all $n>2$ .","['complex-analysis', 'harmonic-functions']"
3408691,Weyl equidistribution criterion.,"I'm currently  reading the chapter 4 of the book ""Ergodic Theory
with a view towards Number Theory"" by Manfred Einsiedler and Thomas Ward. More precisely  I'm reading section 4.4 which deals with equidistribution. Then there is Weyl theorem: Theorem: If $a_k$ is a irrational number the sequence $\{p(n)\}_{n\in \mathbb{N}}$ , where $p(n)=a_k n^k+\cdots a_1 n+a_0$ , is equidistributed in $S^1$ , i.e, for any continuous function $f:S¹\to \mathbb{R}$ $$
\dfrac{1}{n}\sum_{n=0}^{\infty} f(p(n))\longrightarrow \int_{S^1} f(x)\; dLeb(x)
$$ as $n\to \infty$ . The proof is based  on the unique ergodicity of the skew product $$
T(x_1,\ldots, x_k)=(x_1+\alpha, x_2+x_1, \ldots, x_k+x_{k-1})
$$ on the torus $\mathbb{T}^k$ . Then the autors shows that, $$
T(x_1,\ldots, x_k)=
\left(\begin{array}{c}
n\alpha+x_1
&\\
\binom{n}{2}\alpha+nx_1+x_2
&\\
\vdots
&\\
\binom{n}{k}\alpha+ \binom{n}{k-1}x_1+\cdots+nx_{k-1}+x_k
\end{array}\right)
$$ So far no problem. My problem: the autors claims that by putting $\alpha=k!a_k$ we can choose $x_1, \ldots, x_k$ such that $$
p(n)=\binom{n}{k}\alpha+ \binom{n}{k-1}x_1+\cdots+nx_{k-1}+x_k.
$$ Therefore that is my difficult, how to choose those suitable $x_1, \ldots, x_k$ ?","['number-theory', 'ergodic-theory', 'elementary-number-theory']"
3408712,Prove that sums of powers are equal,"All numbers between $1$ and $10^{20}$ ( $10^{20}$ not included) are divided into two sets: one with numbers, whose sum of digits is odd and the other with remaining numbers(those, whose sum of digits is even). Prove that sums of 10th power of numbers in both sets are equal. I've tried to construct pairs from both sets with equal sums but nothing came out of it. Do you have any idea?","['number-theory', 'summation', 'combinatorics', 'big-numbers']"
3408715,$B_{16}$ Meaning on Math Clock,"In my office, there's a clock that replaces the usual numbers on an analog clock with equivalent mathematical expressions. For instance, in place of the number "" $10$ ,"" the clock has $\log_2(1024)$ . Most of these expressions are simple to figure out, like in this case. However, in place of "" $11$ ,"" the clock has written $B_{16}$ . I might be missing something super obvious, but I don't know what this means. Does anyone know what $B_{16}$ refers to? Presumably it is the $16$ th number in some famous sequence, probably named after someone with last name starting with ""B,"" and is equal to $11$ , but it isn't Bernoulli numbers (where $B_{16}\approx -7$ ) nor Bell numbers (some massive quantity). If you Google ""mathematical clock,"" this exact clock is one of the first that shows up, in case someone wants to see it directly.","['reference-request', 'sequences-and-series']"
3408738,Name of analytic function identity,"Doing partial integration on $\int 1\times f(x)dx$ , one gets $$\int f(x)dx=xf(x)-\int xf'(x)dx+C$$ $$=xf(x)-\frac{x^2}2 f'(x)+\int \frac{x^2}2 f''(x)+C$$ $$=...=C+x\sum_{n=0}^\infty \frac{(-x)^n}{(n+1)!} f^{(n)}(x)$$ Replacing $f(x)$ by $f'(x)$ and bringing the right hand side to the left, this becomes $$\sum_{n=0}^\infty \frac{(-x)^n}{n!}f^{(n)}(x)=C$$ My questions: A) Is this correct? B) This identity was not hard to show and looks pretty nice, so I'm sure it has been found by other people, but I could not find any reference to it. Has it appeared anywhere before? If yes, what is its name?","['integration', 'calculus', 'derivatives', 'sequences-and-series']"
3408792,Permutation without repetition/ Solution result doesn't match!,"this is a study question given to us by the professor, but i am a bit confused regarding the result i get, it doesn't match with the professor's one! Is there anything that i am doing wrong and missing here, if so please correct me.
Thank You. With Respect Umer Selmani the question:
How many numbers larger than 56100 can formed with digits from (0,1,5,6,7), repetitions not allowed? the solution given to us: $2 * 4! + 1 * 1 * 3! + 1 * 1 * 2 * 2!$ the solution i got: $2* 4! + 1 * 1 * 3! + 1 * 1 * 1 + 2!$","['permutations', 'discrete-mathematics']"
3408840,Projection of a symmetric matrix onto the positive semidefinite (PSD) cone under the nuclear norm,"Question: Given a symmetric matrix, $S$ , what is the solution to the optimization problem $$ \arg\min_{P \in \mathcal{S}_{\ge 0}} \| S - P \|_N $$ where $\| \cdot \|_N$ denotes the nuclear norm, i.e. the sum of the singular values, or equivalently in the case of a symmetric matrix, the sum of the absolute value of the eigenvalues? In other words, given a symmetric matrix $S$ , what is the closest matrix $P \succeq 0$ in the positive semidefinite cone (denoted $\mathcal{S}_{\ge 0}$ ) with respect to the nuclear norm ? Guess: I would guess that the answer is the same as for the spectral norm and the Frobenius norm, provided that the nuclear norm is unitarily invariant (is it? I don't know, and I can't find a reference). Namely the answer in those cases is apparently: $$ V \Lambda^+ V^\top $$ where $V \Lambda V^\top$ is the eigenvalue decomposition of $S$ , and $V^+$ is the diagonal matrix that is the same as $\Lambda$ except that any negative entries have been replaced with zero. The following argument is obviously wrong, but imagine we would have that for any symmetric matrix $R = U K U^\top$ , after applying a rotation to get $\tilde{R} = V K V^\top$ , we would still have that (in fantasy land) $||S - R||_N = ||S - \tilde{R}||_N$ , then we could assume without loss of generality that $U = V = I$ , and then so calculating the nuclear norm of the difference would reduce to finding the nuclear norm $|| \Lambda - K ||_N$ . But this is easy, since it's just the sum of the absolute values of the diagonals, $\sum_i| \lambda_i - \kappa_i|$ , and then the only way to minimize this (is the following claim actually true?), while ensuring that all of the $\kappa_i \ge 0$ , is to take $\kappa_i = \max\{ \lambda_i , 0 \}$ , i.e. $K = \Lambda^+$ . Of course the claim about taking $V=U=I$ without loss of generality is obviously wrong, since then there would be no unique minimizer/no unique solution to the optimization problem, but the PSD cone is convex and the nuclear norm is a convex function. However, it would seem to suffice to be able to show that $$ || V \Lambda V^\top - V K V^\top||_N < || V \Lambda V^\top - U K U^\top||_N$$ for any $U$ orthogonal and $U \not=V$ . So I guess two claims of questionable merit: Claim 1: The minimizer of $$ \min_{i, \kappa_i \ge 0} \sum_{i} | \lambda_i - \kappa_i | $$ is $\kappa_i = \max \{0, \lambda_i\}$ for all $i$ . Claim 2: $$ || V \Lambda V^\top - V K V^\top||_N < || V \Lambda V^\top - U K U^\top||_N$$ for any $U$ orthogonal and $U \not=V$ . (And any diagonal matrix $K$ .) The following questions are all related but don't answer my question, because they either involve projection onto a set different than the PSD cone, or projection with respect to a distance different from that induced by the nuclear norm (or w.r.t. an unspecified distance). So this question does not seem to be a duplicate of any of these. Orthogonal projection on nuclear-norm ball How to project a symmetric matrix onto the cone of positive semidefinite (PSD) matrices? Find the matrix projection of a symmetric matrix onto the set of symmetric positive semi definite (PSD) matrices Matrix projection onto positive semidefinite cone with respect to the spectral norm Projection of a matrix onto the spectral norm ball Projection of a real symmetric matrix to the cone of positive semidefinite (PSD) matrices ($ \mathcal{S}_{+} $)","['positive-semidefinite', 'nuclear-norm', 'convex-optimization', 'matrices', 'linear-algebra']"
3408866,Question on the proof of the spectral mapping theorem for polynomials,"Theorem 13.9 in these lecture notes is the polynomial spectral mapping theorem: Theorem 13.9 For a polynomial $p$ we have $\sigma(p(T)) = p(\sigma(T))$ . My questions: First direction: Why would $q(T) (p(T) - p(\lambda) I)$ be an inverse for $T - \lambda I$ ? Update: @RobertIsrael said that instead $(p(T) - p(\lambda) I)^{-1} q(T)$ should be $(p(T) - p(\lambda) I)^{-1}$ be?
According to the lines in the proof above, we have $$
(p(T) - p(\lambda) I)^{-1}
= ((T - \lambda I) q(T))^{-1}
\overset{?!}{=} q(T)^{-1} (T - \lambda I)^{-1},
$$ but as $\lambda \in \sigma(T)$ by assumption, $(T - \lambda I)^{-1}$ doesn't exist, right? For the second direction: Why does $x_i \in \sigma(T)$ imply that $p(x_i) - \mu = 0$ ? I thought that $x_i \in \sigma(T)$ only implies that $T - x_i I$ is not boundedly invertible.
Since $\mu \in \sigma(p(T))$ we have that $p(T) - \mu I = c \prod_{k = 1}^{n} (T - x_i I)$ is not boundedly invertible but how does that help? In both statements there seem to be a notion of ""a product of operators is not invertible iff all factors aren't invertible"", but how can that be true? Consider i.e. shift operators on a sequence spaces such that $ST = I$ , i.e. the left and right shift operators, then the above mentioned notion doesn't hold, right?","['proof-explanation', 'spectral-theory', 'functional-analysis']"
3408908,Algebra homomorphisms from real valued smooth functions to $\mathbb{R}$,"Let $M$ be a Hausdorff, paracompact, finite-dimensional real manifold. How can I show that any algebra homomorphism from $C^{\infty}_{\mathbb{R}}(M)$ to $\mathbb{R}$ is an evaluation $\varphi_p$ i.e. $f\mapsto f(p)$ for some $p\in M.$ I can easily see that the converse of this is true, but have no idea how start with the given statement.","['algebraic-geometry', 'smooth-manifolds', 'commutative-algebra', 'differential-geometry']"
3408916,What is the name of this function property?,"How would you name a following property of the function $T$ ? $$ \forall a,b\;(a \subset b \implies T(a) \subset T(b)) $$","['functions', 'terminology']"
3408976,"Show that $\sum_{i=1}^n \frac{1}{|x-p_i|}\le 8n\sum_{i=1}^n \frac{1}{2i-1}$ for some $x,0\le x \le 1$","Let $0\le p_i \le 1$ for $i = 1,2,\dots n.$ Show that $\displaystyle\sum_{i=1}^n \dfrac{1}{|x-p_i|}\le 8n\displaystyle\sum_{i=1}^n \dfrac{1}{2i-1}$ for some $x,0\le x \le 1.$ First, it seems reasonable to convert the RHS to a closed form. I know $\dfrac{\pi}{4}=\displaystyle\sum_{i=1}^\infty (-1)^{i-1}\dfrac{1}{2i-1},$ but I don't think that will be useful for this problem. Also, I don't think the LHS can be simplified using a telescoping series. I think I should pick several candidates $x_j$ and then take $\displaystyle\sum_{j=1}^n \dfrac{1}{|x_j-p_j|}.$ I should probably also consider different intervals of $[0,1].$ Any help would be appreciated.","['calculus', 'sequences-and-series']"
3408994,"Riemann and Lebesgue Integrals of continuous functions on $[a,b]$","I'm trying to show that $\forall f:[a,b]\to\mathbb R$ continuous then $ \int_{[a,b]}fd\lambda=\int_a^bf(x)dx$ where $\lambda$ is Lebesgue measure. I've shown Riemann and Lebesgue Intergrals of bounded functions on a closed and bounded interval are same but I couldn't show it for every continuous functions.( $f$ is not non-negative, it is only continuous) My definitions : $$\int_{[a,b]}fd\lambda = \int_{[a,b]}f^+d\lambda-\int_{[a,b]}f^-d\lambda  $$ where $f^+=\max\{f,0\}$ and $f^-=-\min\{f,0\}$ and $f=f^++f^-$ is measurable. $$\int fd\lambda = \sup\left\{\int \varphi d\lambda : \varphi \; \;\textrm{is simple and} \;\; \varphi \leq f\right\} $$ where $f$ is non-negative and measurable. $$\int_a^b f(x)dx = \sup\left\{\int_a^b \varphi(x) dx : \varphi \; \;\textrm{is step function and} \;\; \varphi \leq f\right\}=\inf\left\{\int^b_a \psi(x) dx : \psi \; \;\textrm{is step function and} \;\; \psi \geq f\right\} $$ (Riemann integral definition) I have known that since $f^+$ is a non-negative function its Lebesgue integral on $[a,b]$ is : $\int_{[a,b]}f^+d\lambda=\int f^+.\chi_{[a,b]}d\lambda = \sup\left\{\int \varphi d\lambda : \varphi \; \;\textrm{is simple and} \;\; \varphi \leq f^+.\chi_{[a,b]}\right\}$ where $\chi_{[a,b]}$ is characteristic function of $[a,b]$ . In addition, I have shown that Riemann and Lebesgue integrals of step functions are equal on $[a,b]$ . How can I show $\int_{[a,b]}f^+d\lambda-\int_{[a,b]}f^-d\lambda\leq \int_a^b f(x)dx$ and $\int_{[a,b]}f^+d\lambda-\int_{[a,b]}f^-d\lambda\geq \int_a^b f(x)dx$ for every continuous functions? I appreciate for any help","['riemann-integration', 'measure-theory', 'lebesgue-integral', 'real-analysis']"
3409027,Two sample hypothesis test with unknown standard deviation problem help,"The problem: The International Air Transport Association surveys business travelers to develop quality ratings for transatlantic gateway airports. The maximum possible rating is 10. Suppose a simple random sample of 50 business travelers is selected and each traveler is asked to provide a rating for the Miami International Airport, and another simple random sample of 50 traveling agents that qualified at the Los Angeles airport. The exercise provided the data but I will just give you the summary of each sample: Miami (1): Size: $n_1=50$ Mean: $\overline{x}_1=6.34$ Standard deviation: $s_1=2.1629$ Los Ángeles (2): Size: $n_2=50$ Mean: $\overline{x}_1=6.72$ Standard deviation: $s_1=2.3737$ There are two questions: With α = 0.025, perform hypotheses test to determine that the two airports are highly competitive. Using the information of the first problem, perform a hypothesis test, with α = 0.025, to determine if there is a significant difference between the two airports. This is what I did. $$H_0: μ_1=μ_2$$ $$H_1: μ_1≠μ_2$$ For the critical value: Freedom degrees: $n_1+n_2-2=50+50-2=98$ Since the  α = 0.025 and I believe this is two-tailed $\frac{α}{2}=0.0125$ But this number does not appear in the table of t. So I used a website to give me the critical value. Critical Value: $t=±2.276$ Since there is unknown the population standard deviation or variance, I looked for the pooled sample variance. This is the formula I used: $$s_p^2=\frac{(n_1-1)s_1^2+(n_1-1)s_2^2}{n_1+n_2-2}$$ $$s_p^2=\frac{(50-1)(2.1629)^2+(50-1)(2.3737)^2}{50+50-2}=5.1563$$ $t$ -statistic: $$t=\frac{\overline{x}_1-\overline{x}_2}{\sqrt{\frac{s_p^2}{n_1}+\frac{s_p^2}{n_2}}}$$ $$t=\frac{6.42-6.72}{\sqrt{5.1563(\frac{1}{50}+\frac{1}{50})}}=-0.6609$$ The null hypothesis is not rejected because -0.6609 falls in the region between -2.276 and 2.276. The data does not show that there is a significant difference between the means of the quality ratings for the airports of Miami and Los Angeles. Am I right? If I am, this answers the second question but, what I have to do with hypothesis test to determine that the two airports are highly competitive? Thanks in advance for any help.","['statistical-inference', 'statistics', 'hypothesis-testing']"
3409050,Can a nonzero matrix be PSD and have zero trace?,"Does there exist an $n \times n$ matrix $A$ which satisfies the following? $A \succeq 0$ ( $A$ is positive semidefinite) $\sum_i A_{ii} = 0$ (trace of $A$ is zero) $A \neq 0$ ( $A$ is not the zero matrix) I know that there exist no such matrices which are symmetric, but I am not sure how to prove that no matrices satisfy the above for the case of non-symmetric matrices, or to otherwise find a counterexample.","['matrices', 'trace', 'positive-semidefinite', 'examples-counterexamples']"
3409051,Bijections between disjoint sets,"Problem:Let $X,X'$ be disjoint sets and $Y,Y'$ be likewise disjoint. If $X\sim$ $Y$ and $X'\sim Y'$ then $X\cup X' \sim Y\cup Y'$ (for every sets $Z$ and $Z'$ ,the notation $Z \sim Z'$ means that there is a bijection from $Z$ to $Z'$ ). My proof: Let $f: X\rightarrow Y$ and $g: X'\rightarrow Y'$ be the respective bijections. Define $h: X\cup X' \rightarrow Y\cup Y'$ by $h(x)=$ $ \begin{cases} 
      f(x) & x\in X \\
      g(x) & x\in X' \\  
   \end{cases}$ . h is well defined: Let $a=b$ . Then either they are both in $X$ or both in $X'$ , in either case, $f,g$ are well defined hence $h(a)=h(b)$ . Injective:
  Suppose $a\neq b$ .
  Case (1): wlog suppose $a\in X$ and $b\in X'$ . Then $f(a)\in Y$ and $g(b) \in Y'$ , as $Y\cap Y' = \varnothing$ it follows that $f(a)\neq g(b)$ . Case (2): wlog suppose $a,b \in X$ such that $a\neq b$ . As $f$ is bijective, $f(a)\neq f(b)$ $\implies$ $h(a)\neq h(b)$ . Surjective:
  If $y\in Y\cup Y'$ then it is either in $Y$ or $Y'$ , not both. As $f,g$ are surjective we can find an $x\in X'$ such that $f(x) = y$ . Similarly for $Y'$ . Can I have feedback, please?","['proof-writing', 'proof-verification', 'functions', 'discrete-mathematics', 'elementary-set-theory']"
3409068,Find the infimum and supremum of $\{x\in Q \mid x² < 9\}$,"I have to find (and prove) the infimum and supremum of the following set: $M_1:=\{x\in\mathbb{Q} \mid x^2 < 9\}$ On first glance, I would say: $\inf M_1=-3 $ $\sup M_1=3$ Now I have to prove that these really are the infimum and supremum of the set, and that's the point where I'm having problems. According to the definition of $\inf$ and $\sup$ , this means, that $-3$ is the biggest lower bound and 3 is the lowest upper bound: $\forall x\in\mathbb(M_1): -3 \leq x \leq 3$ We can see, that -3 and 3 are not elements of M1, which means: $\forall x\in\mathbb(M_1):-3<x<3$ But how can I show that -3 and 3 are the $\textbf{biggest / smallest}$ bound? I mean, for example, what if there is a number bigger than -3 that acts like a lower bound to the set? Obviously there isn't a bigger lower bound, but how can I mathematically show it? Do you guys have any advice? Thanks in advance, and sorry for my English :D","['limits', 'calculus', 'supremum-and-infimum', 'analysis']"
3409088,How does the Hopf map generate $\pi_3(S^2)$?,"I have been studying the Hopf fibration which is an example of a map from $S^3$ to $S^2$ . It is a member of $\pi_3(S^2)$ and shows that this group is non-trivial. It can be shown using a long exact sequence applied to the Hopf fibration that $\pi_3(S^2) \cong \mathbb{Z}$ . However, the set of notes I am using to learn this material ( here ) then claims that the Hopf map is a generator of this group. Furthermore, the Hopf invariant, which is a sort of linking number of the preimages of distinct points can be used to define this isomorphism with $(\mathbb{Z},+)$ . I fail to see how the hopf map generates $\pi_3(S^2)$ . Specifically, I would like to know how you can compose the hopf map to generate maps with higher hopf invariant that make up this integer-like group structure. It would be really helpful to explicitly see this in action (i.e. pointers on how to writ this out using elementary algebra). Note: I have a Chemistry background so my understanding of Mathematics is quite basic though I am willing to learn.","['hopf-fibration', 'general-topology', 'higher-homotopy-groups', 'algebraic-topology']"
3409110,Prove that the pullback $F^* g$ of a Riemannian metric $g$ is a Riemannian metric iff $F$ is a smooth immersion,"So I'm trying to prove that $F^*g$ , where $F^*$ is the pullback of $F: M \rightarrow N$ , and $g$ is a Riemannian metric on $N$ is a Riemannian metric on $F$ is and only if $F$ is a smooth immersion. Among trying to prove this exercise (Page 329 Lee Introduction to smooth manifolds second edition) I realized that my understanding of differentials and pullbacks is a bit flawed. So, my understanding of $dF_p: T_pM \rightarrow T_{F(p)}N$ is that given $v \in T_pM$ and $f \in C^\infty(N)$ we have that $dF_p(v)(f)=v(f \circ F)$ . In otherwords, it seems that the way $dF$ pushes forward the vectors $v \in T_pM$ is by composing it with the function that is defined on $N$ . However, with a Riemannian metric, the vectors that we use as arguements arn't really seen as differential opertors, we simply have two vectors that we plug into the metric to get a real number. Okay, so let me try to define $F^*g$ . Let $v_1,v_2 \in T_pM$ . Then $F^*g(v_1,v_2)=g(dF_p(v_1),dF_p(v_2))$ . I guess this is natural, since $dF_p$ is a linear map, so $(dF_p(v_1),dF_p(v_2))$ is a vector in $T_{F(p)}N$ ... But I guess i'm used to understanding them by viewing $dF_p(v_i)$ as a differential operator... Anyway, I hope I'm not talking in circles. If anyone can lend me any insight what-so-ever into this situation or with the exercise I'm trying to work through (the first sentence of this post) I'd greatly appreciate it. Thanks!","['pullback', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
3409136,Evaluate $\lim\limits_{n\to\infty} \left(1+\frac{1}{n}\right)^{n^2}\cdot\left(1+\frac{1}{n+1}\right)^{-(n+1)^2}$ using the definition of $e$,"Evaluate $\lim\limits_{n\to\infty} \left(1+\frac{1}{n}\right)^{n^2}\cdot\left(1+\frac{1}{n+1}\right)^{-(n+1)^2}.$ I know that $e^x=\lim\limits_{n\to\infty}\left(1+\dfrac{x}{n}\right)^n,$ so I need to somehow convert the limits to this form. I also noticed that $\left(1+\frac{1}{n+1}\right)=\left(\frac{n+2}{n+1}\right)=\left(\frac{n(n+2)}{(n+1)^2}\right)\cdot\left(1+\frac{1}{n}\right).$ Thus, the limit can be rewritten as $$\lim\limits_{n\to\infty}\dfrac{\left(1+\frac{1}{n}\right)^{n^2}}{\left(1+\frac{1}{n}\right)^{(n+1)^2}}\cdot\left(\dfrac{(n+1)^2}{n(n+2)}\right)^{(n+1)^2}\\
=\lim\limits_{n\to\infty}\left(1+\dfrac{1}{n}\right)^{-2n}\cdot\left(1+\dfrac{1}{n}\right)^{-1}\cdot\left(1+\dfrac{1}{n^2+2n}\right)^{n^2+2n}\cdot\left(1+\dfrac{1}{n^2+2n}\right)$$ $$=\dfrac{1}{e^2}\cdot(1)\cdot e\cdot(1)=\dfrac{1}{e}$$","['limits', 'calculus']"
3409156,"If an element in ring of integers is square modulo every prime, then it is a square in ring of integers.","I know that if an integer is square modulo every integer prime, then it is an integer square. I was investigating if same holds in any algebraic number field 'L = $\mathbb{Q}$ [ a $_{1}$ , ... , a $_{k}$ ] i.e. if $\alpha$ is in ring of integers 'O' and $\alpha$ is square modulo every prime 'p' in ring of integers, then $\alpha$ is actually a square in ring of integers. Does this hold at least when k = 1 and extension is quadratic ? I took a course in algebraic-number-theory but hardly remember any of it. Could someone please help me?","['number-theory', 'algebraic-number-theory']"
3409178,Cardinality of initial algebras and final coalgebras,"I have a hunch that the following is true: Conjecture. Let $\kappa$ denote a regular cardinal. Let $F$ denote an endofunctor of $\mathbf{Set}$ such that the category of all $\kappa$ -small sets $\mathbf{Set}_{<\kappa}$ is closed under $F$ . Then $F$ has an initial fixed point of cardinality at most $\kappa$ and terminal fixed point of cardinality at most $2^\kappa$ . My reasoning is roughly that by the usual transfinite composition argument, we should be able to witness the initial fixed point of $F$ as a quotient of a coproduct of at most $\kappa$ -many objects of size strictly less than $\kappa$ , which should have cardinality at most $\kappa$ . Similarly, we should be able to witness the terminal fixed point of $F$ as a subset of a product of at most $\kappa$ -many objects of size strictly less than $\kappa$ , which should have cardinality at most $2^\kappa$ . However, I'm having trouble with the details of the argument; in particular, how do we use the fact that $\mathbf{Set}_{<\kappa}$ is closed under $F$ to deduce that we need at most $\kappa$ -many objects to be added or multiplied together? I included an assumption that $\kappa$ is a regular cardinal in order to help with this, but I'm having trouble using this ""helper assumption"".","['elementary-set-theory', 'category-theory']"
3409189,Does there exists a continuous injective map $f:\mathbb{S^2}\longrightarrow RP^2\times S^1$?,"Does there a continuous injective map $f:\mathbb{S^2}\longrightarrow RP^2\times S^1$ of the form $f(x)=(p(x),g(x))$ where $p(x):\mathbb{S^2}\longrightarrow RP^2$ is universal covering and for some $g(x):\mathbb{S^2}\longrightarrow S^1?$ Here $RP^2$ denotes a real projective plane.","['general-topology', 'differential-geometry', 'algebraic-topology', 'real-analysis']"
3409199,Using automorphic forms to classical results of (algebraic) number theory?,"Recently I have been reading Automorphic Representations and $L$ -Functions for the General Linear Group: Volume I by D. Goldfeld and J. Hundley. On page 51 there is a remark by Ivan Fesenko: So, the most fundamental theorems of classical algebraic number theory follow as easy and fast corollaries of the adelic computation of the zeta function. Before this sentence, it is shown how one can obtain the classical functional equation for the Riemann zeta function by computing the adelic integral of certain test functions using the adelic Poisson summation formula. The remark says that it is possible to do this over any number field $K$ , and the corresponding functional equation implies, e.g., Dirichlet's unit theorem follows easily from this. Where can I find proofs of these? How does one prove classical results in number theory (e.g. Dirichle's unit theorem) by automorphic forms? Any books or articles? Thanks in advance!","['number-theory', 'automorphic-forms', 'algebraic-number-theory', 'reference-request']"
3409202,"In a club with 99 people, everyone knows at least 67 people. Prove there's a group of 4 people where everyone knows each other","I tried solving this with graph theory.
So there are 99 points (each point is a person) and if we draw a line between two points, it will mean those two people knows each other (Since knowing someone is mutual). So, there are at least $$\frac{99*67}{2}$$ lines and we need to prove there's at least a group of four points where every point is connected with the other three. Directly, i don't know how to prove it, since there are $\frac{99*98*97*96}{6}$ possible groups of four points, but i don't find anything with this. So i also tried to see how many groups of three points where all three are connected, because, if another point is connected to these, it will be the group of 4 we're looking for. For this, if we select one point $A$ , it will connect with at least 67 points, then, we select one $B$ of those 67. It will connect with at least 66 besides $A$ . If we want to have the less posible amount of triangles, then we have to connect $B$ with the points that aren't connected with $A$ , and these are 31, but $B$ will have to connect with at least other 36, so there are at least 36 triangles. Now, this only used $A$ and $B$ so there are way more triangles but i don't know how to proceed. Any suggestions or ideas?","['graph-theory', 'recreational-mathematics', 'combinatorics', 'discrete-mathematics']"
3409213,Continuity of $\int_{0}^{1} \frac{f(t)^2}{\sqrt{t}}dt$ wrt Lp norm,"Let $F:C^{0}[0,1]\rightarrow \mathbb{R}$ , $F(f)=\displaystyle\int_{0}^{1} \frac{f(t)^2}{\sqrt{t}}dt$ How to prove that $F$ is continuous wrt the metric induced by $L^{p}$ norm for $2<p\leq \infty$ UPDATE: The statement doesn't hold true for $2<p\leq 4$ . Thanks MaoWao and Sangchul Lee!",['functional-analysis']
3409223,"Prove that for Bernoulli distribution, the normal-based CI is shorter than the one given by Hoeffding's inequality","This is a result claimed in Example 6.17 of All of Statistics . Basically, we need to prove that: $$
z_{\alpha/2} \sqrt{\frac{\hat p_n (1- \hat p_n)}{n}} < \sqrt{\frac 1 {2n} \log(\frac 2 \alpha)}
$$ We can use the inequality $p(1-p) \le \frac 1 4$ and make the substitution that $t=\alpha/2$ . We only need to prove that $$
\sqrt{-2\log t} > z_t
$$ The approach I used requires quite a lot of work: Since the CDF of the standard normal distribution $\Phi$ is monotonic, it suffices to prove that (apply $\Phi$ on both sides): $$
\int_0^{\sqrt{-2\log t}} \frac 1 {2\pi}\exp(-\frac{x^2}2)\,\mathrm dx > \frac 1 2 -t
$$ Then let $g(t)=\text{LHS}\, - \, \text{RHS}$ and take its derivative. We find that g(t) first increase then decrease. Thus we only need to verify the values it takes at boundary points. After some analysis, we can show that this is always positive. Is there a better way to do this? I've tried the Mill's inequality as well to get bounds of the tail of the normal distribution, but it only works in an interval and you still need to justify the inequality on the interval leftover.","['distribution-tails', 'statistics', 'probability', 'inequality']"
3409265,Find all matrices $A\in \mathbb{R}^{2\times2}$ such that $A^2=\bf{0}$,"Attempt: Let's consider $A = 
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}$ . $$\begin{align}
A^2 &= 
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\cdot
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix} \\&=
\begin{bmatrix}
a\cdot a+b\cdot c & a\cdot b + b\cdot d \\
c\cdot a + d\cdot c & c\cdot b + d\cdot d 
\end{bmatrix}\\ &= \bf{0}
\end{align}$$ This gives us the system of equations: $$\begin{align}
a\cdot a+b\cdot c &= 0 \tag{1}\\
a\cdot b + b\cdot d  = b\cdot(a+d)&= 0 \tag{2}\\
c\cdot a + d\cdot c = c\cdot(a+d)&= 0 \tag{3}\\
c\cdot b + d\cdot d &= 0 \tag{4}
\end{align}$$ Now from equation $(2)$ and $(3)$ , we have eight cases: $b = 0$ $c = 0$ $a+d = 0$ and 5 combinations of (1,2,3) which I won't bother listing. Case 1 ( $b=0$ ): $b=0$ implies $a = 0$ in equation $(1)$ and $d = 0$ in equation $(4)$ . This means that if $A = \begin{bmatrix}0&0\\c&0\end{bmatrix}$ then $A^2=\bf{0}$ . Case 2 ( $c=0$ ): From symmetry with $b$ , $c=0 \implies A=\begin{bmatrix}0&b\\0&0\end{bmatrix}$ . So, we only consider cases where $b\neq0$ and $c\neq 0 $ which leaves us only with case 3 ( $a+d=0$ ). Case 3 ( $a+d=0$ ): In equation (1), $a+d=0 \implies a\cdot d - b\cdot c = 0$ . So $A=\begin{bmatrix}a&b\\c&-a\end{bmatrix}$ is not invertible, $A^2 = \bf{0}$ . In summary, if $A$ has one of the following forms: $$\begin{bmatrix}0&b\\0&0\end{bmatrix},
\begin{bmatrix}0&0\\c&0\end{bmatrix},
\begin{bmatrix}a&b\\c&-a\end{bmatrix} \text{ (and not invertible) }$$ then $A^2=\bf{0}$ . Questions: Is this a correct proof? What is the standard proof? ""Strange question"": How can I know if there was only the 8 cases? As in, how do I know that only these 8 cases are relevant to $A^2 = \bf{0}$ ?","['proof-verification', 'matrices', 'nilpotence', 'linear-algebra', 'matrix-equations']"
3409304,This proof of the uncountability of real numbers is valid?,"Some textbooks write down a kind of poof of the uncountability of real numbers, applying the nested interval theorem. Its outline looks as follows: Consider proving by contradiction. Assume the real number set $\mathbb{R}$ is countable, then we can list all of the numbers like this $$\mathbb{R}=\{x_1,x_2,\cdots,x_n,\cdots\}.$$ First, take a closed interval $[a_1,b_1]$ such that $x_1 \notin [a_1,b_1]$ , which is always possible. Now, divide $[a_1,b_1]$ into three equal parts, namely $\left[a_1,\frac{2a_1+b_1}{3}\right],\left[\frac{2a_1+b_1}{3},\frac{a_1+2b_1}{3}\right]$ and $\left[\frac{a_1+2b_1}{3},b_1\right],$ there exists at least one of which not containing $x_2$ . Denote it as $[a_2,b_2]$ . Then, divide $[a_2,b_2]$ again. Similarily, among the new three, there exists at least one  not containing $x_3$ , which is denoted as $[a_3,b_3]$ . Repeat the procedure again and again and infinitely many times. We obtain nested intervals $\{[a_n,b_n]\}$ satisfying that $$x_n \not\in [a_n,b_n],n=1,2,3\cdots.$$ As per the nested interval theorem, there exists an unique real number $\xi$ belonging to all the intervals $[a_n,b_n]$ . In another word, $\xi \neq x_n(n=1,2,3,\cdots)$ , which contradicts. The proof above is valid?","['elementary-set-theory', 'calculus', 'proof-verification']"
3409342,Prove that $A\cup (X \setminus A) = X$,"Let $X$ be a set containing $A$ . Proof: $y\in A \cup (X \setminus  A) \Rightarrow y\in A$ or $y \in (X \setminus A)$ If $y \in A$ , Then $y \in X$ because $A \subset X$ . If $y \in (X \setminus  A)$ , Then $y \in X$ and $y \notin A$ . So $y \in X$ . Therefore $y \in A \cup (A \setminus X) \Rightarrow y \in X$ . Now I've proved that every element of $A \cup (A \setminus X)$ is also an element of $X$ .
But how to prove vice versa??",['elementary-set-theory']
3409363,"Let $A$ be a matrix. If $A^{n-1} \neq \bf{0}$ but $A^{n} = \bf{0}$, what does it say about the matrix $A$?","Since the question doesn't say find all matrices $A$ , I will only produce one example for both: a. $A = \begin{bmatrix}0 & 1\\0&0\end{bmatrix}$ b. $A = \begin{bmatrix}0 & 0 &0\\1&0 & 0\\0&1&0\end{bmatrix}$ from Find a matrix so that $A^2$ not equal to 0 but $A^3$ is [Strang P78 2.4.23] Just based off of these two, I would conjecture that Matrices $A$ satisfying $A^{n-1} \neq \bf{0}$ but $A^{n} = 0$ must be such that they are of dimension $n$ However, there is a matrix $A \in \mathbb{R}^{3\times3}$ that satisfies $A\neq \bf{0}$ but $A^2= \bf{0}$ , namely $$\begin{bmatrix}0&0&0\\1&0&0\\0&0&0\end{bmatrix}$$ The next thing I could think of was: Matrices $A$ satisfying $A^{n-1} \neq \bf{0}$ but $A^{n} = 0$ must be such that they have a dimension of at least $n$ But I don't know if that's it or not... Question(s): What was I meant to conjecture?","['matrices', 'proof-writing', 'linear-algebra']"
3409373,Integrating $\int_{1}^{\infty}\int_{1}^{\infty}(x+y)^2e^{-(x+y)}dydx$,"How to evaluate the following double integral $$\int_{1}^{\infty}\int_{1}^{\infty}(x+y)^2e^{-(x+y)}dydx\,?$$ Is there an easier way to evaluate this than to brute force it through integration by parts many many times? I don't think polar coordinate transformation would help for this but I could be mistaken.","['integration', 'multivariable-calculus']"
3409450,Formula for length of the diagonal of a parallelepiped,"Let $a,b,c$ and $\alpha, \beta, \gamma $ are sides and angles ( $\alpha$ is the angle between the sides $b$ and $c$ and so on)  of a parallelepiped. By using the vector algebra it is easу to prove the formula for  the length of the diagonal $d$ of this  parallelepiped $$
d=\sqrt{a^2+b^2+c^2+2ab\cos \gamma+2ac\cos \beta+2bc \cos \alpha}
$$ Question. How to prove the formula without vectors? It is clear that we have to use two times the cosine theoren but what is the angle between one side and the diagonal of parallelogram formed by two other sides?",['geometry']
3409502,$\lim_{x \to 0} \frac{x^{11}-3x^2+\sin x}{e^x - \cos x}$ Is this solution correct?,Just want to double check whether all my operations are legal. The result seems to be correct but I want to make sure I didn't make any mistakes. $\require{cancel}$ $$\lim_{x \to 0} \frac{x^{11}-3x^2+\sin x}{e^x - \cos x}=$$ $$=\lim_{x \to 0} \frac{x^{11}-3x^2}{e^x - \cos x}+\frac{\sin x}{e^x - \cos x}=$$ $$=\lim_{x \to 0} \frac{x^{11}-3x^2}{e^x - \cos x}+\frac{x}{e^x - \cos x}\cdot\cancelto{1}{\frac{\sin x}{x}}=$$ $$=\lim_{x \to 0} \frac{x^{11}-3x^2+x}{e^x - 1 + 1 - \cos x}=$$ $$=\lim_{x \to 0} (\frac {1-\cos x}{x^{11} - 3x^2 + x} + \frac {e^x-1}{x^{11}-3x^2+x})^{-1}=$$ $$=\lim_{x \to 0} (\frac {1-\cos x}{x^2\cancelto{-\infty}{(x^9-3-\frac {1}{x})}} + \frac {e^x-1}{x\cancelto{1}{(x^{10}-3x+1)}})^{-1}=$$ $$ =\lim_{x \to 0} (\cancelto{\frac{1}{2}}{\frac{1-\cos x}{x^2}}\cdot\cancelto{0}{\frac{1}{-\infty}} + \cancelto{1}{\frac {e^x-1}{x}})^{-1}=$$ $$=\lim_{x \to 0} (\frac {1}{2} \cdot0+ 1)^{-1}=$$ $$=(0+1)^{-1} = 1$$ Is this correct?,"['limits', 'calculus']"
3409518,Solve this trigonometric equation. $\frac{1}{\sqrt2}(\sin(\theta)+\cos(\theta))=\frac{1}{\sqrt2}$,"I tried solving this equation as follows where $0\leq\theta\leq2\pi$ : $$\frac{1}{\sqrt2}(\sin(\theta)+\cos(\theta))=\frac{1}{\sqrt2}$$ Divide both sides by $\frac{1}{\sqrt2}$ . $$\sin(\theta)+\cos(\theta)=1$$ Divide both sides by $\cos(\theta)$ . $$\tan(\theta)+1=\sec(\theta)$$ square both sides: $$(\tan(\theta)+1)^2=\sec^2(\theta)$$ $$\tan^2(\theta)+2\tan(\theta)+1=\sec^2(\theta)$$ Use the identity $\sec^2(\theta)=\tan^2(\theta)+1$ : $$\tan^2(\theta)+2\tan(\theta)+1=\tan^2(\theta)+1$$ $\therefore$ $$2\tan(\theta)=0$$ $\therefore$ $$\tan(\theta)=0$$ $\therefore$ $$\theta=0,\pi,2\pi$$ I know that 0 and $2\pi$ are correct but that $\pi$ is wrong. I also know that the other correct answer is $\frac{\pi}{2}$ . Where did I go wrong?",['trigonometry']
3409535,Curious about definition of well-ordered set,"We know that the definition of a well-ordered set guarantees the existence of least element in every non-empty subset of a set. This isn't a creative or good question, but I am just wondering why it using the existence of least element but not the greatest element or even both, just partially the least element. Anyone shed a bit light would be really appreciated.","['elementary-set-theory', 'discrete-mathematics']"
3409553,How to prove $(\phi-1)(\phi-2)...(\phi-p) = \sqrt{5} + p\left(\frac{1}{2}+A\sqrt{5}\right) \bmod p^2$?,"We consider the solution of $x^2=x+1$ and denote them as $\phi=\frac{1}{2}(1-\sqrt{5}),\bar\phi=\frac{1}{2}(1+\sqrt{5})$ .
Suppose $\phi \not\in \mathbb{F}_p$ . In other words, $\sqrt{5} \not \in \mathbb{F}_p\Leftrightarrow p = \pm 2 \bmod 5$ . Arbitrary element of $\mathbb{F}_p(\phi)$ , $a$ and $b$ satisfies $ab=0\Leftrightarrow a=0 \lor b=0$ . From Fermat's little theorem and factor theorem, $$(x-1)(x-2)...(x-p) = x^p -x \bmod p .$$ Then, put $x=\phi$ . Since Frobenius map, $x \mapsto x^p$ , transfer $\phi$ to conjugate of itself, $\bar\phi$ , \begin{align}
&(\phi-1)(\phi-2)...(\phi-p) = \bar\phi -\phi \bmod p \\
\Leftrightarrow &(\phi-1)(\phi-2)...(\phi-p) = \sqrt{5}\bmod p
\end{align} Then, what about $(\phi-1)(\phi-2)...(\phi-p) \bmod p^2$ ?
Surprisingly, this can be expressed as $$(\phi-1)(\phi-2)...(\phi-p) = \sqrt{5} + p\left(\frac{1}{2}+A\sqrt{5}\right) \bmod p^2$$ empirically (I verify this rule by a code here ideone . $a$ and $b$ of the output corresponds to $a=\frac{1}{2}p$ and $b=1+pA$ ). I cannot find the rule for $A$ , but everytime the coeffieicnt of $p$ of $\mathbb{F}_p$ is $\frac{1}{2}$ at $\bmod p^2$ . How to prove this rule? UPDATE: \begin{align}
&(\phi-0)(\phi-1)...(\phi-(p-1)) = \sqrt{5} + p\left(-2+A\sqrt{5}\right) \bmod p^2 \\
&(\phi-1)(\phi-2)...(\phi-(p+0)) = \sqrt{5} + p\left(2^{-1}+A\sqrt{5}\right) \bmod p^2 \\
&(\phi-2)(\phi-3)...(\phi-(p+1)) = \sqrt{5} + p\left(-2^{-1}+A\sqrt{5}\right) \bmod p^2 \\
\end{align}","['fibonacci-numbers', 'number-theory', 'finite-fields', 'quadratic-residues', 'stirling-numbers']"
3409562,"A rational map is not a map,...Why???","Def of rational map Let X and Y be varieties. A rational map $\phi:X \to Y$ is an equivalence class of pairs $(U,\phi_U)$ where $U$ is a nonempty open subset of X, $\phi_U$ is a mophism of U to Y and where $(U,\phi_U)$ and $(V,\phi_V)$ are equivalent if $\phi_U$ and $\phi_V$ agree on $U \cap V$ . Question: Why a rational map is not in general a map of the set $X$ to $Y$ . Ref: Robin Hartshorne p24 I think the reason why a rational map is not a map as sets is that it dose not agree along the closed curve. image",['algebraic-geometry']
3409613,Problems with solving a 2 variable limit,"There is this exercise I'm trying to solve but can't seem to get it, it states the following: Study the continuity in the origin (depending of $\alpha \in \mathbb{R}$ ) of $$f_\alpha(x,y) = \frac{x^6y^3}{(x^8+y^6)^\alpha}$$ when $(x,y) \neq (0,0)$ and $$f_\alpha(x,y) = 0$$ when $(x,y) = (0,0)$ I have tried various methods, passing to polar coordinates and so on but can't really seem to get anywhere, I know the only possible candidate for the limit is $0$ and thats the result I got several times, so I'm guessing it is continous at $0$ (maybe I'm wrong) and I would have to prove this using the definition but don't know how to do it. Any hints are appreciated and sorry for my broken English.","['multivariable-calculus', 'calculus', 'real-analysis']"
3409650,The Calculation of an improper integral,"For the integral $$\int_{0}^{\infty} \frac{x \ln x}{(x^2+1)^2} dx $$ I want to verify from the convergence then to calculate the integral! For the convergence, simply we can say $$ \frac{x \ln x}{(x^2+1)^2} \sim \frac{1 }{x^3 \ln^{-1} x}$$ then the integral converge because $\alpha=3 > 1$ . Is this true? To calculate the integral, using the integration by parts where $u = \ln x$ and $dv = \frac{x \ln x}{(x^2+1)^2} dx$ . So, $$\int_{0}^{\infty} \frac{x \ln x}{(x^2+1)^2} dx =  \frac{- \ln x}{2(x^2+1)}- \frac{1}{4x^2} +\frac{\ln |x|}{2} ~\Big|_{0}^{\infty}  $$ and this undefined while it should converge to $0$ ! what I missed? I found an error in the calculation so the integral = So, $$\int_{0}^{\infty} \frac{x \ln x}{(x^2+1)^2} dx =  \frac{- \ln x}{2(x^2+1)}+ \frac{1}{8} \Big( \ln x^2 - \ln (x^2+1) \Big) ~\Big|_{0}^{\infty}  $$ and it's still undefined!","['integration', 'calculus', 'improper-integrals']"
3409761,Confusion with contraposition of an implication with quantifiers,"I'm learning about functions and more specific, surjections and injections. However I'm a little bit confused regarding the use of quantifiers and ""contraposition"". Example: A function $ f: X \rightarrow Y$ is injective if $ \forall x_1, x_2 \in X: x_1 \not = x_2 \implies f(x_1) \not = f(x_2) $ , My book says this is equivalent to the contrapostion, that is: $ \forall x_1, x_2 \in X: f(x_1)  = f(x_2) \implies x_1 = x_2 $ . So in this case the quantifiers don't change. However if we look at this example, which is an excercise my professor wrote out for us. The original implication is: $$\forall B \in P(Y): \forall C \in P(Y): f^{-1}(B) \subset f^{-1}(C) \implies B \subset C \implies f $$ is surjective And our professor said that the contraposition of this implication is: $f$ is not surjective $$ \implies  \exists B \in P(Y): \exists C \in P(Y): \neg \left[ f^{-1}(B) \subset f^{-1}(C) \implies B \subset C \right] $$ So in this case the quantifiers do change. I can't seem to find the reason why sometimes they change and other times they don't. If anyone could clarify, I would highly appreciate it!","['quantifiers', 'proof-writing', 'logic', 'analysis']"
3409830,Try to give a description of $Q/ \sim$.,"Let $Q$ be the following subset of $\mathbb Z^2$ : $$Q = \{ (a,b) \in \mathbb Z^2 \mid b \neq 0 \}$$ . Define the relation $\sim$ in $Q$ by $$(a,b) \sim (c,d) \iff ad=bc$$ (i) Prove that $\sim$ is an equivalence relation in $Q$ . I have done that. (ii) Indicate the equivalence class $[\left( 2,3\right)]$ . I have also done that.  Please see below: $$[\left( 2,3\right)] = \{ (c,d) \in \mathbb Z^2 \mid (2,3) \sim (c,d) \} \\ $$ $\iff$ $$[\left( 2,3\right)] = \{ (c,d) \in \mathbb Z^2 \mid (2d=3c \} \\ $$ (iii) Indicate $[(a,b)]$ . I have also done that.  See below $$[(a,b)]=\{ (c,d) \in \mathbb Z^2 \mid ad=bc \}$$ (iv) Try to give a description  of $Q/ \sim$ . This is my main problem. I'm not sure what I'm even asked to do. I thought the description was already presented in (iii). Maybe I should describe it geometrically ?","['equivalence-relations', 'discrete-mathematics']"
3409831,Prove that $\sqrt[n]{n} < (1 + \frac{1}{\sqrt{n}})^2$ for all n in the naturals.,"I need to prove that $\sqrt[n]{n} < (1 + \frac{1}{\sqrt{n}})^2$ for all n in the naturals.
I started by using Bernoulli's inequality: $(1+\frac{2}{\sqrt{n}}) < (1 + \frac{1}{\sqrt{n}})^2$ I can say that: $(1+\frac{2}{\sqrt{n}}) = (1+\frac{2\sqrt{n}}{n})$ I can also subtract the one and divide by 2 on the left side without changing the inequality (because it makes it even smaller): $(\frac{\sqrt{n}}{n}) < (1 + \frac{1}{\sqrt{n}})^2$ But now I am stuck...","['inequality', 'radicals', 'analysis', 'real-analysis']"
3409912,Solve equation $1+\sin^2\theta=3\sin\theta\cos\theta \text { (given } \tan71 ^{\circ}34^{\prime}=3) $,Solve equation $1+\sin^2\theta=3\sin\theta\cos\theta \text { (given } \tan71 ^{\circ}34^{\prime}=3)  $ My attempt is as follows:- $$1-2\sin\theta\cos\theta+\sin^2\theta-\sin\theta\cos\theta=0$$ $$\left(\sin\theta-\cos\theta\right)^2+\sin\theta\left(\sin\theta-\cos\theta\right)=0$$ $$\left(\sin\theta-\cos\theta\right)(2\sin\theta-\cos\theta)=0$$ $$\tan\theta=1 \text { or } \tan\theta=\frac{1}{2}$$ $$\theta=n\pi+\frac{\pi}{4}$$ For $\tan\theta=\dfrac{1}{2} \text { how to make use of given condition } \tan71 ^{\circ}34^{\prime}=3$,['trigonometry']
3409978,Proof of Chu–Vandermonde identity,"I am working through Probability Theory, the logic of Science by E.T. Jaynes and came across the following equation (6.16): $$S \equiv \sum_{R=0}^N {R \choose r} {N - R \choose n - r} = {N + 1 \choose n + 1}$$ I have been trying to find a proof for this and came across the Chu–Vandermonde identity on wikipedia (eq. 9), which can take the form: $$
\sum_{m=0}^n {m \choose j} {n - m \choose k - j} = {n + 1 \choose k + 1}
$$ It looks like this is the identity that I need, but I have not been able to find a proof of this particular form and was hoping someone may be able to point me to it, or help me walk through it. Thanks!","['binomial-coefficients', 'combinatorics', 'probability-theory', 'binomial-theorem']"
3410023,"Is ""conditional independence"" of $\sigma$-algebras implied by ""set-wise conditional independence"" of $\sigma$-algebras?","Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, let $\mathcal{G}_1,\mathcal{G}_2,\mathcal{H}$ be sub- $\sigma$ -algebras of $\mathcal{F}$ , and suppose that for all $G_1 \in \mathcal{G}_1$ , $G_2 \in \mathcal{G}_2$ and $H \in \mathcal{H}$ with $\mathbb{P}(H)>0$ , $G_1$ and $G_2$ are conditionally independent given $H$ . Does it follow that $$ \mathbb{P}(G_1 \cap G_2 | \mathcal{H}) \overset{\mathbb{P}\textrm{-a.s.}}{=} \mathbb{P}(G_1|\mathcal{H})\mathbb{P}(G_2|\mathcal{H}) $$ for all $G_1 \in \mathcal{G}_1$ and $G_2 \in \mathcal{G}_2$ ? More extenstive discussion of the problem, and my attempts so far, now follows. Fix a probability space $(\Omega,\mathcal{F},\mathbb{P})$ and sub- $\sigma$ -algebras $\mathcal{G}_1,\mathcal{G}_2,\mathcal{H}$ of $\mathcal{F}$ . Definition of conditional independence: We say that $\mathcal{G}_1$ and $\mathcal{G}_2$ are conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ if the following equivalent statements [1, Sec. 3.2, p131] hold: for all $G_1 \in \mathcal{G}_1$ and $G_2 \in \mathcal{G}_2$ , $\ \mathbb{P}(G_1 \cap G_2 | \mathcal{H}) \overset{\mathbb{P}\textrm{-a.s.}}{=} \mathbb{P}(G_1|\mathcal{H})\mathbb{P}(G_2|\mathcal{H}) \,$ ; for all $G_1 \in \mathcal{G}_1$ , there is an $\mathcal{H}$ -measurable version of $\mathbb{P}(G_1|\sigma(\mathcal{G}_2 \cup \mathcal{H}))$ ; for all $G_2 \in \mathcal{G}_2$ , there is an $\mathcal{H}$ -measurable version of $\mathbb{P}(G_2|\sigma(\mathcal{G}_1 \cup \mathcal{H}))$ . [1] M. M. Rao, Probability Theory with Applications , Academic Press, Inc., 1984. The heuristic interpretation is: ""If I have the knowledge of $\mathcal{H}$ , the whole of $\mathcal{H}$ and nothing but $\mathcal{H}$ , then from this standpoint $\mathcal{G}_1$ and $\mathcal{G}_2$ are independent."" Note that conditional independence of $\mathcal{G}_1$ and $\mathcal{G}_2$ given $\mathcal{H}$ neither implies nor is implied by independence of $\mathcal{G}_1$ and $\mathcal{G}_2$ . Definition of ""set-wise conditional independence"": I will say that $\mathcal{G}_1$ and $\mathcal{G}_2$ are set-wise conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ if $$ \hspace{23mm} \mathbb{P}(G_1 \cap H)\mathbb{P}(G_2 \cap H) \ = \ \mathbb{P}(G_1 \cap G_2 \cap H)\mathbb{P}(H) \hspace{20mm} (\ast) $$ for all $G_1 \in \mathcal{G}_1$ , $G_2 \in \mathcal{G}_2$ and $H \in \mathcal{H}$ . For any $E \in \mathcal{F}$ with $\mathbb{P}(E)>0$ , define the probability measure $\mathbb{P}_E$ by $\mathbb{P}_E(A)=\frac{\mathbb{P}(A \cap E)}{\mathbb{P}(E)}$ . Then the formula $(\ast)$ is equivalent to the statement, ""if $\mathbb{P}(H)>0$ then $G_1$ and $G_2$ are $\mathbb{P}_H$ -independent"". Note that if $\mathcal{G}_1$ and $\mathcal{G}_2$ are set-wise conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ , then in particular $\mathcal{G}_1$ and $\mathcal{G}_2$ are $\mathbb{P}$ -independent. The heuristic interpretation of set-wise conditional independence is meant to be: "" $\mathcal{G}_1$ and $\mathcal{G}_2$ are independent, and no amount of knowledge from $\mathcal{H}$ will change this"". Remark. If $\mathcal{G}_1$ and $\mathcal{G}_2$ are set-wise conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ , then for any $H_0 \in \mathcal{H}$ with $\mathbb{P}(H_0)>0$ , $\mathcal{G}_1$ and $\mathcal{G}_2$ are set-wise conditionally $\mathbb{P}_{H_0}$ -independent given $\mathcal{H}$ . THE QUESTION. Given what the intuition behind set-wise conditional independence is meant to be, a natural question is: Are the following statements equivalent? $\mathcal{G}_1$ and $\mathcal{G}_2$ are set-wise conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ . For every sub- $\sigma$ -algebra $\tilde{\mathcal{H}}$ of $\mathcal{H}$ , $\mathcal{G}_1$ and $\mathcal{G}_2$ are conditionally $\mathbb{P}$ -independent given $\tilde{\mathcal{H}}$ . This question easily simplifies to: Does set-wise conditional independence given $\mathcal{H}$ imply conditional independence given $\mathcal{H}$ ? [To see this: If set-wise conditional independence given $\mathcal{H}$ is equivalent to conditional independence under every sub- $\sigma$ -algebra of $\mathcal{H}$ , then in particular set-wise conditional independence given $\mathcal{H}$ implies conditional independence given $\mathcal{H}$ . Conversely, suppose set-wise conditional independence implies conditional independence. In the one direction, obviously set-wise conditional independence given $\mathcal{H}$ implies set-wise conditional independence -- and hence conditional independence -- given every sub- $\sigma$ -algebra of $\mathcal{H}$ . In the other direction, set-wise conditional independence given $\mathcal{H}$ is verified set-wise as a consequence of conditional independence given $\sigma(\{H\})$ for each $H \in \mathcal{H}$ .] Intuitively, I think the answer ought to be yes . And yet, I'm having trouble proving it. (One special case where the answer should clearly be yes is when $\mathcal{H}$ has a generator consisting of countably many mutually disjoint sets; but I am interested in the general case.) A further characterisation of the question: Let us say that $\mathcal{G}_1$ and $\mathcal{G}_2$ have uncorrelated conditional probabilities under $\mathbb{P}$ given $\mathcal{H}$ if for all $G_1 \in \mathcal{G}_1$ and $G_2 \in \mathcal{G}_2$ the random variables $\mathbb{P}(G_1|\mathcal{H})$ and $\mathbb{P}(G_2|\mathcal{H})$ are uncorrelated random variables over $(\Omega,\mathcal{F},\mathbb{P})$ . Then yet another equivalent formulation of the question (as a general question about general probability spaces and sub- $\sigma$ -algebras) turns out to be the following: Does set-wise conditional independence given $\mathcal{H}$ imply uncorrelated conditional probabilities given $\mathcal{H}$ ? To see the equivalence, we give the following two lemmas: Lemma 1. If $\mathcal{G}_1$ and $\mathcal{G}_2$ are conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ , then the following statements are equivalent: $\mathcal{G}_1$ and $\mathcal{G}_2$ are $\mathbb{P}$ -independent; $\mathcal{G}_1$ and $\mathcal{G}_2$ have uncorrelated conditional probabilities under $\mathbb{P}$ given $\mathcal{H}$ . Lemma 2. If $\mathcal{G}_1$ and $\mathcal{G}_2$ are set-wise conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ , then the following statements are equivalent: $\mathcal{G}_1$ and $\mathcal{G}_2$ are conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ ; for every $H \in \mathcal{H}$ with $\mathbb{P}(H)>0$ , $\mathcal{G}_1$ and $\mathcal{G}_2$ have uncorrelated conditional probabilities under $\mathbb{P}_H$ given $\mathcal{H}$ . If set-wise conditional independence implies conditional independence, then by Lemma 1, set-wise conditional independence also implies uncorrelated conditional probabilities. And if set-wise conditional independence implies uncorrelated conditional probabilities, then applying this to $\mathbb{P}_H$ for all $H \in \mathcal{H}$ with $\mathbb{P}(H)>0$ (which we can do by virtue of the Remark further above), we have by Lemma 2 that set-wise conditional independence implies conditional independence. Proof of Lemma 1. Assume $\mathcal{G}_1$ and $\mathcal{G}_2$ are conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ . For any $G_1 \in \mathcal{G}_1$ and $G_2 \in \mathcal{G}_2$ , we have $$ \mathbb{P}(G_1)\mathbb{P}(G_2) \ = \ \mathbb{E}_\mathbb{P}[\mathbb{P}(G_1|\mathcal{H})]\mathbb{E}_\mathbb{P}[\mathbb{P}(G_2|\mathcal{H})] $$ and $$ \mathbb{P}(G_1 \cap G_2) \ = \ \mathbb{E}_\mathbb{P}[\mathbb{P}(G_1|\mathcal{H})\mathbb{P}(G_2|\mathcal{H})]. $$ Proof of Lemma 2. Assume $\mathcal{G}_1$ and $\mathcal{G}_2$ are set-wise conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ . Clearly the following statements are equivalent: $\mathcal{G}_1$ and $\mathcal{G}_2$ are conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ ; for every $H \in \mathcal{H}$ with $\mathbb{P}(H)>0$ , $$ \frac{1}{\mathbb{P}(H)}\int_H \mathbb{P}(G_1 \cap G_2|\mathcal{H}) \, d\mathbb{P} \ = \ \frac{1}{\mathbb{P}(H)}\int_H \mathbb{P}(G_1|\mathcal{H})\mathbb{P}(G_2|\mathcal{H}) \, d\mathbb{P}. $$ Now in this last equality, $$ \mathrm{RHS} \ = \ \mathbb{E}_{\mathbb{P}_H\!}[\mathbb{P}_H(G_1|\mathcal{H})\mathbb{P}_H(G_2|\mathcal{H})] $$ and using set-wise conditional independence, $$ \mathrm{LHS} \ = \ \mathbb{P}_H(G_1 \cap G_2) \ = \ \mathbb{P}_H(G_1)\mathbb{P}_H(G_2) \ = \ \mathbb{E}_{\mathbb{P}_H\!}[\mathbb{P}_H(G_1|\mathcal{H})]\,\mathbb{E}_{\mathbb{P}_H\!}[\mathbb{P}_H(G_2|\mathcal{H})]. $$ Further thoughts towards solving the problem (and more generally towards understanding set-wise conditional independence). 1. One question we could ask is whether, in general, conditional probabilities of independent events are uncorrelated, or perhaps even independent, random variables. The answer is no . (The independence question has actually already been addressed in Math.SE, here .) Take the probability space $\{1,2\} \times \{1,2\}$ with the uniform distribution. The events $A:=\{\textrm{1st coordinate is }1\}$ and $B:=\{\textrm{2nd coordinate is }1\}$ are obviously independent; but letting $C=\{(1,1)\}$ , we have \begin{align*}
\mathrm{Prob}(A|C) = \mathrm{Prob}(B|C) &= 1 \\
\mathrm{Prob}(A|C^c) = \mathrm{Prob}(B|C^c) &= \tfrac{1}{3}.
\end{align*} The events $\{\mathrm{Prob}(A|\sigma(\{C\}))=1\}$ and $\{\mathrm{Prob}(B|\sigma(\{C\}))=1\}$ are certainly not independent events, but are in fact the same non-trivial event $C$ itself. Moreover, in this example, the random variables $\mathrm{Prob}(A|\sigma(\{C\}))$ and $\mathrm{Prob}(B|\sigma(\{C\}))$ are strictly positively correlated: \begin{align*}
\mathrm{Exp}[\mathrm{Prob}(A|\sigma(\{C\})).\mathrm{Prob}(B|\sigma(\{C\}))] \ &> \ \mathrm{Prob}(C).\mathrm{Prob}(A|C).\mathrm{Prob}(B|C) \\
&= \ \tfrac{1}{4} \ = \ \mathrm{Prob}(A).\mathrm{Prob}(B).
\end{align*} 2. We have the following: Lemma 3. Let $G_1,G_2 \in \mathcal{F}$ be $\mathbb{P}$ -independent events. Then the set $\Lambda$ of all $H \in \mathcal{F}$ that are $\mathbb{P}$ -independent of $G_1$ and fulfil $(\ast)$ forms a $\lambda$ -system on $\Omega$ . This immediately follows from the following: Lemma 4. For any $G_1,G_2,H_1,H_2 \in \mathcal{F}$ with $H_1 \subset H_2$ , if at least one of $G_1$ and $G_2$ is both $\mathbb{P}$ -independent of $H_1$ and $\mathbb{P}$ -independent of $H_2$ , and $(\ast)$ is fulfilled with $H:=H_1$ and with $H:=H_2$ then $(\ast)$ is fulfilled with $H:=H_2 \setminus H_1$ . Proof. First note that for any $G_1,G_2,H \in \mathcal{F}$ with $G_1$ and $H$ being $\mathbb{P}$ -independent, $(\ast)$ is equivalent to the statement that $G_1$ and $G_2 \cap H$ are $\mathbb{P}$ -independent. So now assume the conditions of the Lemma, with $H_1$ and $H_2$ each being $\mathbb{P}$ -independent of $G_1$ , from which it follows that $H_2 \setminus H_1$ is $\mathbb{P}$ -independent of $G_1$ . We know that $G_2 \cap H_1$ and $G_2 \cap H_2$ are each $\mathbb{P}$ -independent of $G_1$ , from which it follows that $G_2 \cap (H_2 \setminus H_1)$ is $\mathbb{P}$ -independent of $G_1$ . Hence the result. 3. Let us now consider a relatively basic case of set-wise conditional independence: Proposition 5. Let $\mathcal{G}_1,\mathcal{G}_2,\mathcal{G}_3$ be mutually $\mathbb{P}$ -independent sub- $\sigma$ -algebras of $\mathcal{F}$ . For any $\,G_1,\tilde{G}_1 \in \mathcal{G}_1$ , $\ G_2,\tilde{G}_2 \in \mathcal{G}_2\,$ and $\,\tilde{G}_3 \in \mathcal{G}_3$ , $(\ast)$ is fulfilled with $H:=\tilde{G}_1 \cap \tilde{G}_2 \cap \tilde{G}_3$ . Let $\mathcal{H}$ be either $\sigma(\mathcal{G}_1 \cup \mathcal{G}_3)$ or $\sigma(\mathcal{G}_2 \cup \mathcal{G}_3)$ . Then $\mathcal{G}_1$ and $\mathcal{G}_2$ are set-wise conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ , and are also conditionally $\mathbb{P}$ -independent given any sub- $\sigma$ -algebra $\tilde{\mathcal{H}}$ of $\mathcal{H}$ . Proof. The first part is a straightforward verification: both sides of the equation $(\ast)$ simplify to $$ \mathbb{P}(G_1 \cap \tilde{G}_1)\mathbb{P}(G_2 \cap \tilde{G}_2)\mathbb{P}(\tilde{G}_1)\mathbb{P}(\tilde{G}_2)\mathbb{P}(\tilde{G}_3)^2. $$ Now let $\mathcal{H}=\sigma(\mathcal{G}_2 \cup \mathcal{G}_3)$ . The set-wise conditional independence follows from the first part of the proposition by Lemma 3 (using the Dynkin $\pi$ - $\lambda$ theorem). As for the conditional independence given sub- $\sigma$ -algebra $\tilde{\mathcal{H}}$ of $\mathcal{H}$ : for any $G_1 \in \mathcal{G}_1$ and $G_2 \in \mathcal{G}_2$ , since $G_1$ is independent of $\sigma(\{G_2\} \cup \tilde{\mathcal{H}})$ we have $$ \mathbb{P}(G_1 \cap G_2|\tilde{\mathcal{H}}) \ = \ \mathbb{P}(G_1)\mathbb{P}(G_2|\tilde{\mathcal{H}}) \ = \ \mathbb{P}(G_1|\tilde{\mathcal{H}})\mathbb{P}(G_2|\tilde{\mathcal{H}}). $$ Definition. I will say that $\mathcal{G}_1$ and $\mathcal{G}_2$ are trivially set-wise conditionally $\mathbb{P}$ -independent given $\mathcal{H}$ if $\mathcal{G}_1$ and $\mathcal{G}_2$ are $\mathbb{P}$ -independent and there exists a sub- $\sigma$ -algebra $\mathcal{G}_3$ of $\mathcal{F}$ that is $\mathbb{P}$ -independent of $\sigma(\mathcal{G}_1 \cup \mathcal{G}_2)$ , such that $\mathcal{H}$ is a sub- $\sigma$ -algebra of either $\sigma(\mathcal{G}_1 \cup \mathcal{G}_3)$ or $\sigma(\mathcal{G}_2 \cup \mathcal{G}_3)$ . So Proposition 5 gives an affirmative answer to my question in the case of ""trivial"" set-wise conditional independence; but I don't know if it's possible to have ""non-trivial"" cases of set-wise conditional independence.","['conditional-expectation', 'probability-theory']"
3410107,Proving the sum of two differentiable functions is also differentiable,"I have to prove that If $f$ and $g$ are differentiable at some point $c$ , i.e. $f'(c)$ and $g'(c)$ exist, then $(f+g)(x)$ is also differentiable, i.e. $(f+g)'(x)$ exists. In all the answers given everywhere, people have shown just that $(f+g)'(x)=f'(x)+g'(x)$ . But that doesn't really show that the function is differentiable in a rigorous way (Does it?). I mean that shouldn't we use the $\epsilon-\delta$ to show actually that the limit exists? I tried to show that given \begin{gather}
0 < \lvert x-c \rvert < \delta_1 \implies
\Biggl\lvert\frac{f(x)-f(c)}{x-c} - f'(c)\Biggr\rvert < \epsilon_1 \\
0 < \lvert x-c \rvert < \delta_2 \implies
\Biggl\lvert\frac{g(x)-g(c)}{x-c} - g'(c)\Biggr\rvert < \epsilon_2
\end{gather} we somehow need to show that if we set $$ \delta = \min\{\delta_1, \delta_2\} $$ then $$ \forall \lvert x - \delta \rvert < 0, \qquad
\Biggl\lvert\frac{(f+g)(x)-(f+g)(c)}{x-c}-L\Biggr\rvert < \epsilon$$ But I can't proceed to prove this.","['calculus', 'derivatives', 'real-analysis']"
3410112,Extinction probability in a population with competition,"Suppose we have a colony of bacteria. At the end of each day, each bacterium produces an exact copy of itself with probability $p$ and then dies with probability $q$ . However, $q$ is not constant, but a function of $N$ , the total number of bacteria: $$q=p\bigg(1-\frac{1}{N}\bigg)$$ So in larger populations of bacteria, each bacterium is more likely to die (because of competition, say). To clarify, $N$ counts the number of bacteria before new ones were born. For instance, if there are $2$ bacteria on one day and they both reproduce to form $4$ bacteria, both of them still have exactly $p/2$ chance of dying (not $3p/4$ ). And the babies that have just been born cannot die immediately. Let $P_N$ be the probability that a bacteria colony consisting of $N$ bacteria initially eventually goes extinct. Can we find an asymptotic formula for $P_N$ ? I suspect that we will have $$P_N\sim \alpha^N$$ for some $\alpha$ , but I don’t know how to calculate this constant. I did manage to figure out that if we keep $q$ constant, then the probability of eventual extinction starting with $N$ bacteria is exactly equal to $$\bigg(1-\frac{p-q}{p(1-q)}\bigg)^N$$ for $p>q$ , and equal to $1$ for $p\le q$ . But that problem was much easier because “newborn” bacteria were independent from their parents, whereas in this problem the chance of each bacterium’s survival is dependent on the overall population size. So, really my question is: what is the value of $$\lim_{N\to\infty}P_N^{1/N}=\space ?$$","['limits', 'asymptotics', 'probability', 'stochastic-processes']"
3410150,If $f(x) = (x-a)^3(x-b)^3$ then what is the nature of the roots of $f^{\prime\prime}(x) = f^\prime (x)$?,"If we try solving it by finding $f''(x)$ then it is very long and difficult to do, so my teacher suggested a way of doing it, he said find nature of all the roots of $f(x) =f'(x)$ , and on finding nature of the roots we got them to be real(but not all distinct) and then he said as all the roots of $f(x) = f'(x)$ are real so all the roots of $f'(x)= f''(x)$ are real and distinct. I did not understand how to prove that if all the roots of $f(x) = f'(x)$ are real so all the roots of $f'(x)= f''(x)$ are real and distinct.Can anyone please help me to prove this? Is this  statement (all roots of $f(x) = f'(x)$ are real so all roots of $f'(x)= f''(x)$ are real and distinct) true only for this question or is it true in general for all function $f(x)$ whose all roots of $f(x) = 0$ are real? If instead of $f(x) = (x-a)^3(x-b)^3$ we had $f(x) = (x-a)^4(x-b)^4$ then would we say that as all roots of $f(x) = f'(x)$ are real so all the roots of $f'(x)= f''(x)$ are real and distinct or rather we would say that as  all roots of $f(x) = f'(x)$ are real so all roots of $f'(x)= f''(x)$ are real. If anyone has any other way of solving this question $f(x) = (x-a)^3(x-b)^3$ then what is the nature of the roots of $f''(x) = f'(x)$ please share it.",['functions']
3410200,$c$ and $c_0$ are not isometrically isomorphic,"I'm learning that the two Banach Spaces, $c$ and $c_0$ are not Isometrically Isomorphic. Where $c$ = The set of all converging sequences. $c_0$ = The set of sequences converging to zero. Seems like the reason behind this is that the closed unit ball in two spaces have a different behavior. That is in $c$ they have got ""extreme points"" and in $c_0$ they do not. I'm a bit confused about this word ""extreme point"". First I thought that they were the boundary points but they cannot be because for sequences like $a=(1,0,0,0,0,...)$ in both $c_0$ and $c$ , $||a||=\{\sup|a_n|: n\geq1\}=1$ . So it is a boundary point of the closed balls in both spaces. Could some one help me out explaining about the extreme points in the given closed unit balls and how to determine whether they have got such points or not","['banach-spaces', 'functional-analysis']"
3410265,"Finding the critical points of $f(x,y)=(y-x^2)(y-2x^2)$","Finding the critical points of $f(x,y)=(y-x^2)(y-2x^2)$ I know that $(a,b)$ is a critical point $\iff \nabla f(a,b)=(0,0)$ So $\nabla f(x,y)= (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})$ $\frac{\partial f}{\partial x}[(y-x^2)(y-2x^2)]=8x^3-6xy$ $\frac{\partial f}{\partial y}[(y-x^2)(y-2x^2)]=-3x^2+2y$ $\nabla f(x,y)=(8x^3-6xy, -3x^2+2y)=(0,0) \iff (x,y)=(0,0)$ So finding the Hessian matrix at $(0,0)$ $\frac{\partial ^2f}{\partial x^2}[(y-x^2)(y-2x^2)]=\frac{\partial
   f}{\partial x}[8x^3-6xy]=24x^2-6y$ $\frac{\partial ^2f}{\partial y^2}[(y-x^2)(y-2x^2)]=\frac{\partial f}{\partial y}[-3x^2+2y]=2$ $\frac{\partial ^2f}{\partial x \partial y}[(y-x^2)(y-2x^2)]=\frac{\partial
   f}{\partial x}[8x^3-6xy]=-6x$ $H(0,0)=\begin{pmatrix}
0 & 0 \\
0 & 2
\end{pmatrix}$ Finding the eigenvalues for $H(0,0)$ : $\det(H(0,0)-\lambda I)= \begin{vmatrix}
-\lambda & 0 \\
0 & 2-\lambda
\end{vmatrix} = \lambda^2-2\lambda$ $\lambda^2-2\lambda=0 \iff \begin{cases} 
      \lambda_1 = 0 \\
      \lambda_2 = 2 
   \end{cases}$ So I can't really tell if $f(x,y)$ has any relative maximum nor minimum or a saddle point, and I don't know how to continue... Edit: I know that this is a duplicate post , but I don't understand the answer, and the question is 4 years old.","['partial-derivative', 'multivariable-calculus', 'calculus', 'hessian-matrix']"
3410279,Differentiability implies bounded variation?,"Does a differentiable function on $[a,b]$ has bounded variation ? I recall that differentiable on $[a,b]$ means differentiable on $(a,b)$ and $\lim_{x\to a^+}f(x)$ and $\lim_{x\to b^-}f(x)$ exist. I know that if $f'$ is integrable on $[a,b]$ , then it works (since we can majorate $V_{[a,b]}(f)$ by $\int_a^b|f'|$ ). But if $f'$ is not integrable ? Can we find a differentiable function on $[a,b]$ that has no bounded variation ?","['bounded-variation', 'derivatives', 'real-analysis']"
3410300,$fg$ integrable for all $f\in L^1$ iff $g\in L^\infty.$,"I have been trying to solve this problem for a week and couldn't come up with a solution. Let $g $ be a Lebesgue measurable function defined on $\mathbb R$ .
 Prove that the following are equivalent: a) The function $g$ is bounded (i.e., $g\in L^\infty(\mathbb R)$ . b) For every function $f \in L^1(\mathbb R)$ we have $f(x) g(x)\in L^1(\mathbb R)$ That a) $\Rightarrow$ b) is quite straightforward, if $g\in L^\infty(\mathbb R)$ then there exists a constant $C = inf \{ c:\mu(\{x:|g(x)|\geq c\})=0\} $ , therefore, for every function $f \in L^1(\mathbb R)$ we have $$\int| f(x) g(x)|d\mu \leq \int |f(x)|Cd\mu < \infty $$ Which means $f(x) g(x)\in L^1(\mathbb R)$ . For the converse, I thought about negating and proving by contradiction. This is, if $g\notin L^\infty(\mathbb R)$ , then there exists a function $f \in L^1(\mathbb R)$ , but $f(x) g(x)\notin L^1(\mathbb R)$ So far, my idea is that we know that for every constant $k$ , $\mu(\{x:|g(x)|\geq k\}) > 0 $ , because if it is $0$ then we already have a contradiction, as that means $g\in L^\infty(\mathbb R)$ . I thought about creating characteristic functions using those decreasing sets, as for each $k$ we are able to obtain a set. Moreover, if $k$ increases then the size of the set decreases. But I am unable to move forward from here. Any hints or solutions will be appreciated.","['measure-theory', 'lp-spaces', 'functional-analysis', 'real-analysis']"
3410349,Does Schmidt decomposition exist on infinite dimensional Hilbert spaces?,"A pure quantum state in a bipartite system, which is an operator $$\rho = \langle\psi \,,\, \cdot \,\rangle \, \psi \in \mathcal{L}(H_1 \otimes H_2)$$ for some $\psi \in H_1 \otimes H_2$ , is factorizable (i.e. not entangled) iff the reduced density matrices $\rho_s$ are such that $\rho_s^2 = \rho_s$ , where $\rho_s$ is the partial trace of $\rho$ over $H_s$ for $s= 1$ or $2$ . To prove this result for finite dimensional Hilbert spaces $H_1$ and $H_2$ , we use Schmidt decomposition in those spaces. And the decomposition exists because of the SVD theorem. I want to know if it is also true for separable Hilbert spaces in general, where a lot of quantum mechanics happen. I believe it is, but haven't found a proof. So, is SVD theorem valid for ""infinite matrices""? Or, if it is not, is there another way to prove there exists a Schmidt decomposition (a series) in this case?","['quantum-mechanics', 'infinite-matrices', 'functional-analysis']"
3410364,Complete Butcher Array,"I've been given a partially complete Butcher Array, but I seem to be missing some other condition to find out the rest of the values. This is the Butcher Array I've been given: Using the conditions about row sums and consistency I get: $ c_1 = a_{11} + a_{12} => a_{12} = -1/4$ ; $ c_2 = a_{21} + a_{22} => c_2 - a_{21} = 5/12$ ; $b_1 + b_2 = 1 => b_1 = 1/4$ So I have to be missing some condition for $c_2$ and $a_{21}$ .","['runge-kutta-methods', 'numerical-methods', 'ordinary-differential-equations']"
3410414,Let $(G_k)$ a sequence of open and dense subsets. Show that $\bigcap_{k\geqslant 0}G_k$ is also open and dense,"The exercise says Let $(G_k)$ a sequence of open and dense subsets in a complete metric space. Show that $\bigcap_{k\geqslant 0}G_k$ is also open and dense I have shown that $G:=\bigcap_{k\geqslant 0}G_k$ is dense, however I think that I have a counterexample for $G$ to be also open. If it would be true then it must also be true that $G^\complement =\bigcup_{k\geqslant 0}G_k^\complement $ would be closed and with empty interior. However setting $G_k^\complement :=\{1/k\}$ we have that $G^\complement=\{1/k:k\in \Bbb N_{> 0} \} $ have empty interior in $\Bbb R$ but it is not closed because zero is a limit point of $G^\complement $ . Im wrong or the exercise is wrong?","['baire-category', 'metric-spaces', 'analysis']"
3410423,Proving that any continuously differentiable functions preserve Jordan measurability of bounded sets,"Let $f:\mathbb{R}^n\to\mathbb{R}^n$ be continuously differentiable and $U\subset \mathbb{R}^n$ be bounded and Jordan measurable. Prove that $f(U)$ is also Jordan measurable. I know that  any bounded set is Jordan measurable iff its boundary is of measure $0$ and any continuously differentiable function maps sets of measure $0$ to sets of measure $0$ . Then I wanted to show that the boundary of $f(U)$ is contained in  the image of the boundary of $U$ , i.e. $\partial f(U)\subset f(\partial U)$ . But in this problem, $f$ is only continuously differentiable. I got stuck in carrying out this idea. Any hints? Thanks in advance! Added Put $M=\{x:\det Jf (x)=0\}$ . I realized that it suffices to prove that $f(M)$ is of measure $0$ , which looks like a conclusion of sard theorem. But in this case $f$ is only $C^1$ . Then I tried the following For $x\in M$ , it's known that $$\lim_{r\to 0}\frac{m(f(B(x,r)))}{m(B(x,r))}=0$$ where $m$ denotes the Lebesgue measure. Given $\varepsilon$ . Then for each $x\in M$ , we can choose $r_x$ such that $\frac{m(f(B(x,r)))}{m(B(x,r))} \le \varepsilon$ for all $r\le r_x$ . There exists an open cover $\bigcup_{x\in M} B(x,r_x)$ . Considering that $M$ is compact, we know that there exists a subcover $\bigcup_{1}^N B(x_i,r_{x_i})$ . Then we have $$m(f(M))\le \varepsilon \sum_1^N m( B(x_i,r_{x_i}))$$ But how can we show that $\varepsilon \sum_1^N m( B(x_i,r_{x_i}))$ can be arbitrarily small? Can we find a subcover such that $\sum_1^N m( B(x_i,r_{x_i}))\le C$ where $C$ is a constant  independent of $\varepsilon$ ? Any hints? Thanks in advance!","['measure-theory', 'real-analysis']"
3410452,Area of the shaded region of a infinitely circumscribed set of polygons.,"The problem itself. Recently, a friend of mine has sent me this image, and asked me if I could figure out the area of the white region in this circle; initially, I thought it was simply a infinite summation of a n-gon area, but I found myself stuck, and as such, I wanted some help. The puzzle is: We have a infinite set of circumscribed polygons, each one in a increasingly larger circle; a infinite amount of them form this larger circle. What is the area of all of the polygons? My attemp boiled down to, first, describing the problem: $$(\ \sum^{\infty}_{?}W_a +\sum^{\infty}_{?}C_a\ )=\pi r_1^2$$ This is just a statement that the infinite sum of the white shades and the circles give off the area of a circle, which is just $\pi r^2$ ; my next step was delineating this to a more specific sum. I decided that the white shades should be the series of a circumscribed n-gon: $$ \sum^{\infty}_{3}\frac{1}{2}nr_n^2 sin(\frac{2 \pi}{n})$$ So, to find the actual area would just be: $$\sum^{\infty}_{3}\frac{1}{2}nr_n^2 sin(\frac{2 \pi}{n})=\pi r_1^2- ( \sum^{\infty}_{n} \pi r_n^2)$$ However, I realized that the actual radii would be changing in as well, in the form of a infinite series: $$\sum^{\infty}_0 r_n+y$$ After that, I couldn't figure it out anymore. 
Does anyone know how to deal with something like this?","['puzzle', 'geometry', 'sequences-and-series']"
3410458,Which is the proper taylor series?,"My friend and I are trying to tackle the following problem: Write the Taylor series around a = 1 for $xe^{x}$ I approached the problem the following way: We know that Taylor series for $e^x$ around 1 are: $f(x) = e^{x} = \Sigma \frac{e^{1}}{n!}(x-1)^{n}$ so if we do g(x) = $xe^{x}$ , $g(x) = x(f(x))$ and $ g(x) = x(\Sigma \frac{e}{n!}(x-1)^{n}) = \Sigma x\frac{e}{n!}(x-1)^{n}$ from n = 0 to infinty Which, according to WolframAlpha, leads back to $xe^{x}$ . Now, my friend did it the following way: 
he found f(x) of $xe^{x}$ , and then f'(x)= $xe^{x}+e^{x}$ , and then f''(x) = $xe^{x} + e^{x} + e^{x}$ , and so forth (each iteration added another $e^{x}$ ). He came up with the following expression: $\Sigma (n+1)\frac{e(x-1)^{n}}{n!}$ from n=0 to infinity. I put both equations on a graph and they both seem to approximate $xe^{x}$ fairly well around a=1. So my question is, who has the most acceptable Taylor Series? Is the other answer wrong?","['power-series', 'taylor-expansion', 'sequences-and-series']"
3410519,Why is the lognormal not a stable distribution?,"Everyone makes a distinction between what are called stable distributions and infinitely divisible distributions.  Stable distributions act as an 'attractor' of sorts: the sum of a large number of iid random variables will, by the central limit theorem (CLT), converge to a normal distribution independent of the original distribution. Infinitely divisible distributions are a bit weaker, it seems. But suppose that we are not adding iid RVs, but multiplying them (and for the sake of simplicity, let's assume the RVs in question have only positive support): $$Y_n=\Pi_0^n X_i$$ If we take the logarithm, then $$logY_n=\Sigma_0^nlogX_i$$ Now, assuming the original $X_i$ are iid, it seems by the CLT that the sum on the left also is the sum of iid RVs; it, too, must converge (using the CLT again) to a normal distribution as n becomes large.This seems to imply that multiplying (strictly positive) RVs must converge, in the limit, to a lognormal distribution since the ""underlying distribution"" must converge to a normal distribution - and,when we ""go back"" by exponentiating, it seems we should get: $$e^{Y_n}$$ which should converge to a lognormal distribution as $n\rightarrow\infty$ , since $Y_n$ is converging to a normal distribution. I am sure there is a mathematical wormhole here, but I can't spot it.","['probability-distributions', 'probability-theory']"
3410544,Counterexample of $\text{Supp}(M)=\mathbb{V}(\text{Ann}(M))$ [duplicate],"This question already has an answer here : The support of a non finitely generated module (1 answer) Closed 4 years ago . This is an exercise from Atiyah-MacDonald: Show that when $M$ is finitely generated $A$ -module, we have $\text{Supp}(M)=\mathbb{V}(\text{Ann}(M))$ . Can somebody give counterexamples when $M$ is not finitely generated?","['algebraic-geometry', 'commutative-algebra']"
3410551,"Exhibiting a one-one function from $\left[ 0, 1 \right)$ to $P \left( \mathbb{N} \right)$.","I was trying to prove that $P \left( \mathbb{N} \right)$ is bijective with the interval $\left[ 0, 1 \right)$ . I wish to employ Schoder-Berstein theorem which states that if there there are two one-to-one functions between two sets, then those sets are bijective. To obtain the first one-to-one function, I formed $f: P \left( \mathbb{N} \right) \rightarrow \left[ 0, 1 \right)$ as $$f \left( S \right) = \begin{cases} 0, & S = \emptyset \\ 0.a_1a_2a_3\cdots, & S \neq \emptyset \end{cases}$$ where $a_n = \begin{cases} 1, & n \in S \\ 0, & n \notin S \end{cases}$ . Clearly, this is a one-to-one function, as commented upon in Bijection from $\mathcal{P}(\mathbb{N})$ to $(0,1)$ . However, I find it difficult to create the other one-to-one function. I tried to form $g: \left[ 0, 1 \right) \rightarrow P \left( \mathbb{N} \right)$ as $$g \left( x \right) = \begin{cases} \emptyset, & x = 0 \\ \left\lbrace a_1, a_2, \cdots \right\rbrace, & x = 0.a_1a_2\cdots \end{cases}$$ However, clearly, this is not one-to-one since $0.1$ and $0.11$ are mapped to the same set $\left\lbrace 1 \right\rbrace$ . Also, it is not well-defined since $0.01$ is not mapped anywhere. Any hints for constructing this one-to-one function will be appreciated. Note: I would like to use the decimal expansion only for the construction, and not base-two expansion.",['elementary-set-theory']
3410561,Solve $\lim_{n\to\infty} \sum_{r=0}^n \frac{\binom{n}{r}}{(n^r)(r+3)}$ [duplicate],This question already has answers here : Evalute $ \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{\binom{n}{k}}{n^k(k+3)} $ (5 answers) Closed 4 years ago . $\lim_{n\to\infty} \sum_{r=0}^n \frac{\binom{n}{r}}{(n^r)(r+3)}$ I have no idea how do I solve this problem. I do know I have to somehow convert this in a function of $\frac{r}{n}$ cause this looks to me as Riemann integral problem. Any help would be appreciated.,"['integration', 'riemann-sum', 'definite-integrals', 'sequences-and-series']"
3410597,Introducing a mathematical structure.,"Most of the times, introducing a new mathematical structure is done in the following path. Start with a set/collection, name it as $X$ . It is possible that $X$ already have a structure with it, namely the structure of topological  space/manifold/vector space etc. Define a structure on $X$ , denote it by $\mathcal{A}$ . So, a structure is a pair $(X,\mathcal{A})$ . Define what does it mean to say a substructure of $(X,\mathcal{A})$ . Giving names to well-behaved substructures. Define what are maps between two structures, say $(X,\mathcal{A})$ and $(Y,\mathcal{B})$ . Giving names to well-behaved maps between two structures. Define what does it mean to say two structures $(X,\mathcal{A})$ and $(Y,\mathcal{B})$ are ""equivalent"". Construct new structures from old structures. This step 6 differs drastically from one structure to another structure. Some ways to produce new structures from old structures are Quotients. Pullbacks. Products (direct). Sums (direct). Limits (Injective/Projective). ... ... For example, defining the notion of structure of a group on a set, we follow the same procedure. Defining a group, defining what does it mean to say a group morphism, what it means to say a subgroup (a normal subgroup), quotients of subgroup, (direct) sum, (direct) product of groups, (Injective/Projective) limit of (a collection) of groups and so on. In this post, I want to collect this procedure for most of the structures introduced in undergraduate or beginning graduate courses in Mathematics.","['abstract-algebra', 'big-list', 'category-theory', 'differential-geometry']"
3410633,"Suppose $B \subseteq A$ and $ R = \{(X,Y) \in \mathscr P(A) \times \mathscr P(A) \mid X\Delta Y\subseteq B\}$ Prove $R$ is an equivalence relation.","Suppose $B \subseteq A$ and define a relation $R$ on $\mathscr P(A)$ as follows: $$ R = \{(X,Y) \in \mathscr P(A) \times \mathscr P(A)
 \mid X\Delta Y \subseteq B \}$$ Where $\Delta$ stands for symmetric difference. Prove that $R$ is an equivalence relation on $\mathscr P(A)$ My attempt: We need to show that $R$ is: Reflexive Symmetric Transitive 1. Take arbitrary $X \in \mathscr P(A)$ , because $X \Delta X  = \emptyset$ , we have $X \Delta X \subseteq B $ , and thus $(X,X) \in R$ , which means that set is reflexive. 2. Take $(X,Y) \in R$ . Since $X \Delta Y = Y \Delta X$ , we have $(Y,X) \in R$ . Hence set is symmetric. 3. Take $(X,Y), (Y,Z) \in R$ Need to show that $X \Delta Z \subseteq B$ . Take arbitrary $a \in X \Delta Z$ . It implies that either $a \in X$ and $a \notin Z$ or $a \notin X$ and $a \in Z$ Consider first case. Suppose $a \in Y$ . Then $a \in Y \Delta Z$ and $a \in B$ . Suppose $a \notin Y$ . Then $a \in X \Delta Y$ and hence $a \in B$ . Consider second case. Suppose $a \in Y$ . Then $a \in X \Delta Y$ and hence $a \in B$ . Suppose $a \notin Y$ . Then $a \in Y \Delta Z$ and then $a \in B$ . Therefore, provided that $(X,Y), (Y,Z) \in R$ , we have $X \Delta Z \subseteq B$ and $(X,Z) \in \mathscr P(A)$ , Hence $R$ is transitive. We've shown that $R$ is reflexive, symmetric and transitive, and thus we can conclude that $R$ is equivalence relation on $A$ . $\Box$ Is it correct?","['elementary-set-theory', 'equivalence-relations', 'proof-verification']"
3410644,Proving $A \subset B \implies A \cup B = B$,"I have to prove this: $A \subset B \implies A \cup B = B$ One direction: $x\in A \subset B$ $\implies x \in A \land x \in B$ $\implies x \in A \cup B$ $\implies x \in (A \cup B = B)$ Other direction: $x \in (A \cup B = B)$ $\implies x \in A \land x \in B$ (I used $\land$ here instead of $\lor$ because it equals B, so it has to be in both sets.) $\implies x \in A \subset B$ Is my proof correct or are there some steps missing.
If not I hope you could show me the right direction.","['elementary-set-theory', 'proof-verification']"
3410691,Prove that for every $X \in \mathscr P(A)$ there is exactly one $Y \in [X]_R$ such that $Y \cap B = \emptyset$,"Suppose $B \subseteq A$ and $ R = \{(X,Y) \in \mathscr P(A) \times
 \mathscr P(A) \mid (X\Delta Y)\subseteq B\}$ Prove that for every $X \in \mathscr P(A)$ there is exactly one $Y \in [X]_R$ such that $Y \cap B = \emptyset$ My attempt: Since $R$ is an equivalence relation , I use the notation: $[X]_R = \{Y \in \mathscr P(A) \mid (Y,X) \in R \}$ Suppose there are $T,Q \in [X]_R$ , such that $T \cap B = \emptyset$ and $Q \cap B = \emptyset$ . Need to prove that $T = Q$ . Take $a \in T$ . Note that $a \notin B$ . We have $(T,X) \in R$ . Since $T \Delta X \subseteq B$ , it follows that $a \in X$ . Since $Q \in [X]_R$ , it follows that $(Q,X) \in R$ . Since $a \in X$ and $a \notin B$ , it follows that $a \in Q$ . Arbitrary element was considered, hence $T \subseteq Q$ . I will omit proof of $ Q \subseteq T$ , because it is almost identical. $\Box$ Is it correct?","['proof-explanation', 'equivalence-relations', 'proof-verification', 'relations', 'elementary-set-theory']"
