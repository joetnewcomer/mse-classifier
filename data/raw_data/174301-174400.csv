question_id,title,body,tags
3111858,Simplifying the Derivative of a European Call Option,"I know the title suggests finance, but I'm stuck on the mathematics of this. I need to take the following derivative: $$
-\frac{\delta C(X)}{\delta X}=-\frac{\delta}{\delta X}
\Big[Se^{-dT}N(d_1)-Xe^{-rT}N(d_2)\Big]
$$ where S, d, and T are constants, $$
d_1 = d_2 + \sigma \sqrt{T} = \frac{ln(\frac{S}{X})+(r-d+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}}
$$ $$
d_2 = \frac{ln(\frac{S}{X})+(r-d-\frac{\sigma^2}{2})T}{\sigma \sqrt{T}}
$$ and $\sigma$ is a function of X such that $\sigma = \sigma(X)$ . Further, I need to show this derivative ultimately equals $$
-\frac{\delta C(X)}{\delta X} = e^{-rT}N(d_2) - Xe^{-rT}N'(d_2) \sqrt{T} \sigma'(X)
$$ I have currently done the following $$
\begin{align}
-\frac{\delta C(X)}{\delta X} &=-\frac{\delta}{\delta X}
\Big[Se^{-dT}N(d_1)-Xe^{-rT}N(d_2)\Big] \\
&=-\frac{\delta}{\delta X}
\Big[Se^{-dT}N(d_1)\Big]+\frac{\delta}{\delta X}\Big[Xe^{-rT}N(d_2)\Big] \\
&= -Se^{-dT}\frac{\delta}{\delta X}
\Big[N(d_1)\Big]+e^{-rT}\frac{\delta}{\delta X}\Big[XN(d_2)\Big] \\
&= -Se^{-dT}\frac{\delta N(d_1)}{\delta d_1}\frac{\delta d_1}{\delta X} + Xe^{-rT}\frac{\delta N(d_2)}{\delta d_2}\frac{\delta d_2}{\delta X} + e^{-rT}N(d_2)
\end{align}
$$ Finding the partial derivatives $$
\frac{\delta N(d_1)}{\delta d_1} = N'(d_1)
$$ $$
\frac{\delta N(d_2)}{\delta d_2} = N'(d_2)
$$ $$
\frac{\delta d_1}{\delta X} = \frac{\delta d_2}{\delta X} + \frac{\delta}{\delta X}\Big[\sigma(X) \sqrt{T} \Big] = \frac{\delta d_2}{\delta X} + \sigma'(X) \sqrt{T}
$$ $$
\begin{align}
\frac{\delta d_2}{\delta X} &= \frac{\delta}{\delta X} \Bigg[ \frac{ln(\frac{S}{X})+(r-d-\frac{\sigma(X)^2}{2})T}{\sigma(X) \sqrt{T}} \Bigg] \\
&= \frac{\delta}{\delta X}\Bigg[\frac{ln(\frac{S}{X})}{\sigma(X) \sqrt{T}}\Bigg] + \frac{\delta}{\delta X}\Bigg[\frac{(r-d)T}{\sigma(X)\sqrt{T}}\Bigg] + \frac{\delta}{\delta X}\Bigg[\frac{\frac{\sigma(X)^2T}{2}}{\sigma(X)\sqrt{T}}\Bigg] \\
&= \frac{ \frac{\sigma(X)}{X}-\sigma'(X)ln(\frac{S}{X})}{\sigma(X)^2 \sqrt{T}} + (r-d)\sqrt{T} \frac{\sigma'(X)}{\sigma(X)^2} + \frac{\sigma'(X) \sqrt{T}}{2}
\end{align}
$$ Applying these partial derivatives to the derivation $$
\begin{align}
-\frac{\delta C(X)}{\delta X} &= -Se^{dT}N'(d_1)\Bigg[ \frac{\delta d_2}{\delta X} + \sigma'(X) \sqrt{T} \Bigg] + Xe^{-rT}N'(d_2) \frac{\delta d_2}{\delta x} + e^{-rT}N(d_2) \\
&= -Se^{-dT}\Bigg[N(d_1)\frac{\delta d_2}{\delta X} + \sigma'(X) \sqrt{T} \Bigg] + Xe^{-rT}N'(d_2)\frac{\delta d_2}{\delta X} + e^{-rT}N(d_2)
\end{align}
$$ From here it appears that I need the first term to zero out and $\frac{\delta d_2}{\delta X} = \sqrt{T} \sigma'(X)$ , but after working with the formulae for a while I've gotten no closer to an answer.","['partial-derivative', 'derivatives', 'normal-distribution', 'finance']"
3111864,Construct Triangle given bisectors and circumcircle,"Suppose we have three concurrent lines $g,h,k$ in the Euclidean plane which meet at a point $P\in g\cap h\cap k.$ Moreover, let $K$ be some circle with center $P$ and some radius $r>0$ . I would like to construct, with ruler and compass, all triangles with circumcircle $K$ and such that $g,h,k$ become the perpendicular bisectors of the sides of the triangle. My ideas so far: I proved that the composition of reflections $s_g\circ s_h\circ s_k$ is again a reflection in some line through $P$ (where $s_g$ denotes the reflection in the line $g$ etc.). If we permute the order of the three reflections ( $s_g,s_h, s_k$ ), we get again a reflection but in a different line through $P$ . Using this observation, I came up with the following idea for a construction. Suppose, we reflect some point $A\in K\cap g$ successively at $k$ and then at $h$ . We obtain some point $A'$ (This would correspond to the point $A'=s_h\circ s_k\circ s_g(A)$ , i.e. A' is the reflection of $A$ at some line $d$ through $P$ ). When we construct the bisector of $A A'$ , we get the line of reflection $d$ of $s_h\circ s_k\circ s_g$ . Let $Q\in d\cap K$ . If we reflect the point $Q$ successively at $g,k,h$ , we obtain a triangle, which meets all criteria. If we do the same with all other points, we get in total two triangles which meet all criteria. But I do not know how to prove that there are no further such triangles. Moreover, it seems that my construction is somehow lengthy. Is it possible to construct the triangles more elegant? I am very grateful for your help! Best wishes!","['euclidean-geometry', 'triangles', 'geometry', 'geometric-construction']"
3111879,Find the particular solution of the differential equation,"$\sqrt{x} + \sqrt{y}y' =0$ , with $y(1) = 9$ . I get: $\sqrt{y}dy = -\sqrt{x}dx \Rightarrow \frac{2}{3}y^{\frac{3}{2}} = -\frac{2}{3}x^{\frac{3}{2}}+C$ . What should I do from here?",['ordinary-differential-equations']
3111988,Is a closed embedding of CW-complexes a cofibration?,"It is a standard fact that the inclusion of a sub-CW-complex into a CW-complex is a cofibration, it follows from the fact that the inclusions $S^k\to D^{k+1}$ are, and that they are preserved by pushouts. My question is about a general closed embedding of a CW-complex into another one, say $f:Y\to X$ ; but it's not necessarily cellular, and even if it were, it doesn't necessarily witness $Y$ as a sub-CW-complex of $X$ . Is it still necessarily a cofibration ? If it helps/changes the answer, we may assume that $Y$ or both $X,Y$ are finite dimensional, or even finite (though if the answer is ""yes"" for one of these cases with more hypotheses and ""no"" with fewer hypothese, I would still be interested in counterexamples for fewer hypotheses)","['cofibrations', 'general-topology', 'cw-complexes', 'algebraic-topology']"
3112006,What vector x will maximize the norm of $\|Ax\|_2 / \|x\|_2$ (norm 2),"I know for norm one vector $x$ should be a basis vector. Where one is in the column of matrix $A$ . and for infinity norm $x$ should have elements $-1$ for negative values of the maximum row and $+1$ for positive values of matrix $A$ . Now I am trying to figure out how can I maximize it for norm 2. I think it is related to the eigenvalues of (AT.A), however, I don't know where to start.","['matrices', 'numerical-methods', 'normed-spaces']"
3112020,The inverse of a Dirichlet product is the Dirichlet product of the inverses of each function,"Let $f,g$ be arithmetic functions. According to Wikipedia , $(f*g)^{-1} = f^{-1} * g^{-1}$ if $f(1) \neq 0$ and $g(1) \neq 0$ . However, it is not clear to me why this is true, and the statement does not provide a reference text for a proof. Would anyone be able to provide a link to one and/or prove it?","['complex-analysis', 'number-theory', 'analytic-number-theory']"
3112104,Proving $\mathcal{M}\left(\sin(x)\right)(s) = \Gamma(s)\sin\left(\frac{\pi}{2}s \right)$ using Real Analysis,"recently I've been investigating Mellin Transforms and this morning solved for case of $\sin(x)$ using Ramunajan's Master Theorem. I was curious if there were any Real based methods to evaluate this integral as in searching around online all proofs seem to rely on Contour Integration. My approach: \begin{equation}
\mathcal{M}\left(\sin(x)\right)(s) = \int_0^\infty x^{s - 1}\sin(x)\:dx = \Gamma(s)\sin\left(\frac{\pi}{2}s \right)
\end{equation} By definiton: \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \int_0^\infty x^{s - 1}\sin(x)\:dx = \int_0^\infty x^{s - 1}\left[\sum_{m = 0}^{\infty} (-1)^m \frac{x^{2m + 1}}{(2m + 1)!} \right]\:dx \\&= \int_0^\infty x^{s} \sum_{m = 0}^{\infty} (-1)^m \frac{\left(x^2\right)^m}{(2m + 1)!}\:dx
\end{align} Here we make the substitution $u = x^2$ : \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \int_0^\infty \left(\sqrt{u}\right)^{s} \sum_{m = 0}^{\infty} (-1)^m \frac{\left(u\right)^m}{(2m + 1)!}\frac{du}{2\sqrt{u}} = \frac{1}{2}\int_0^\infty u^{\frac{s - 1}{2}}\sum_{m = 0}^{\infty} (-1)^m \frac{u^m}{(2m + 1)!}\:du  \\
&= \frac{1}{2}\int_0^\infty u^{\frac{s + 1}{2} - 1}\sum_{m = 0}^{\infty} \frac{(-u)^m}{(2m + 1)!}\:du = \frac{1}{2}\mathcal{M}\left(g(u)\right)\left(\frac{s + 1}{2}\right)
\end{align} Where \begin{equation}
g(u) = \sum_{m = 0}^{\infty} \frac{(-u)^m}{(2m + 1)!} = \sum_{m = 0}^{\infty} \frac{m!}{(2m + 1)!}\frac{(-u)^m}{m!} = \sum_{m = 0}^{\infty} \frac{\Gamma(m + 1)}{\Gamma(2m + 2)}\frac{(-u)^m}{m!}
\end{equation} We observe that this form enables the use of Ramanujan's Master Theorem. Thus: \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \frac{1}{2}\mathcal{M}\left(g(u)\right)\left(\frac{s + 1}{2}\right) = \frac{1}{2} \Gamma\left( \frac{s + 1}{2}\right) \cdot \frac{\Gamma\left(- \frac{s + 1}{2} + 1\right)}{\Gamma\left(2\cdot -\frac{s + 1}{2} + 2\right)} \\
&= \frac{1}{2} \frac{\Gamma\left( \frac{s + 1}{2}\right)\Gamma\left(1 - \frac{s + 1}{2}\right)}{\Gamma(1 - s)}
\end{align} Employing Euler's Reflection Formula on the terms in the numerator we arrive at: \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \frac{1}{2} \frac{\Gamma\left( \frac{s + 1}{2}\right)\Gamma\left(1 - \frac{s + 1}{2}\right)}{\Gamma(1 - s)} = \frac{1}{2} \cdot \frac{1}{\Gamma(1 - s)}\cdot \frac{\pi}{\sin\left(\pi  \cdot \frac{s + 1}{2}\right)} = \frac{\pi}{2} \frac{1}{\Gamma\left(1 - s\right)\sin\left(\frac{\pi}{2} (s + 1)\right)}
\end{align} Now Euler's Reflection Formula: \begin{equation}
 \Gamma(s)\Gamma(1 - s) = \frac{\pi}{\sin(\pi s)} \rightarrow \Gamma(1 - s) =  \frac{\pi}{\Gamma(s)\sin(\pi s)}
\end{equation} Returning to our integral and observing that $\sin\left(\frac{\pi}{2} (s + 1)\right)= \cos\left(\frac{\pi}{2}s\right)$ we arrive at: \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \frac{\pi}{2} \frac{1}{\Gamma\left(1 - s\right)\sin\left(\frac{\pi}{2} (s + 1)\right)} = \frac{\pi}{2} \cdot \frac{1}{\frac{\pi}{\Gamma(s)\sin(\pi s)} \cdot \cos\left(\frac{\pi}{2}s\right)} = \frac{\Gamma(s)\sin(\pi s)}{2 \cos\left(\frac{\pi}{2} s\right)}
\end{align} We now use the double angle formula $\sin\left(\pi s\right) = 2\sin\left(\frac{\pi}{2} s\right)\cos\left(\frac{\pi}{2} s\right)$ to yield: \begin{align}
\mathcal{M}\left(\sin(x)\right)(s) &= \frac{\Gamma(s)\sin\left(\pi s\right)}{2 \cos\left(\frac{\pi}{2} s\right)} = \frac{\Gamma(s)\cdot  2\sin\left(\frac{\pi}{2} s\right)\cos\left(\frac{\pi}{2} s\right)}{2 \cos\left(\frac{\pi}{2} s\right)} = \Gamma(s)\sin\left(\frac{\pi}{2} s\right)
\end{align} As required.","['integration', 'definite-integrals', 'real-analysis', 'gamma-function', 'mellin-transform']"
3112139,Is the sum of series $\sum_{n=0}^{\infty} \lfloor n\pi \rfloor x^n$ a rational function?,I was reading this paper by L.J. Mordell (original paper by Morris Newman). I am trying to apply first theorem from the paper with $f(x)=x$ and $k=\pi$ . So the series $\displaystyle \sum_{n=0}^{\infty} \lfloor n\pi \rfloor x^n$ must not converge to a rational function for $|x|<1$ . But it seems that the series converges to a rational function as the series can be written as $3x+6x^2+9x^3+...=\frac{3x}{(x-1)^2}$ after we evaluate $\lfloor n\pi \rfloor$ for each $n$ ? Am I doing something wrong with the evaluation of $\lfloor n\pi \rfloor$ ? EDIT: David found my mistake. Now the question is can we find the sum of this series or show that the sum is not a rational function without Mordell's result?,"['power-series', 'rational-functions', 'analysis', 'real-analysis']"
3112157,Let $x=1+\frac{1}{2+\frac{1}{1+\frac{1}{2+\frac{1}{...}}}}$; then the value of $(2x-1)^2$ equals...,Let $$x=1+\frac{1}{2+\frac{1}{1+\frac{1}{2+\frac{1}{...}}}};$$ then the value of $(2x-1)^2$ equals... I don't how to start this question. Please help.,"['real-numbers', 'algebra-precalculus', 'continued-fractions']"
3112175,Can someone explain Dirac Distribution?,"I have been reading book on Deep Learning and in the chapter of probability and information theory I found this topic ""Dirac Distribution and Empirical Distribution"". It says: In some cases, we wish to specify that all the mass in a probability distribution clusters around a single point. I don't understand what this means? And also how this is related to machine learning, what is the significance of it? I can understand a bit of mathematics of this but unable to picturize the concept, if someone can help me in this(in a more simpler way) it will be a real help. PS: If you can provide some other study material regarding this it will be really helpful. Thanks","['probability-distributions', 'probability-theory', 'dirac-delta']"
3112210,Permute columns by pre-multiplying and rows by post-multiplying?,"I was looking at Gilbert Strang's lectures on Linear Algebra and noticed that in lecture 2, Elimination with Matrices, around the 40nth minute he mentions that you can use the permutation matrix, $$P=
\begin{bmatrix}
    0       & 1 \\
    1       & 0 
\end{bmatrix}
$$ so that $AP$ is a permutation of $A$ 's columns and $PA$ is a permutation of $A$ 's rows. I was wondering if there exists a matrix $P'$ such that $P'A$ is a permutation of $A$ 's columns and $AP'$ is a permutation of $A$ 's rows. How to prove there is no $P'$ such that $P'A=AP$ and $AP'=PA$ for all $n\times n$ matrices? For which matrices $A$ there is such a $P'$ ? Tried the $2\times 2$ case $L$ pre-multiplies $A$ and permutes its columns, $$
\begin{bmatrix}
x_1       & x_2 \\
x_3       & x_4 
\end{bmatrix}
\begin{bmatrix}
a       & b \\
c       & d 
\end{bmatrix}=\begin{bmatrix}
b       & a \\
d       & c 
\end{bmatrix}	\Rightarrow
\begin{bmatrix}
x_1       & x_2 \\
x_3       & x_4 
\end{bmatrix}=\frac{1}{ad-bc}\begin{bmatrix}
bd-ac       & a^2-b^2 \\
d^2-c^2       & ac-bd
\end{bmatrix}=L
$$ $R$ post-multiplies $A$ and permutes its rows, $$
\begin{bmatrix}
a       & b \\
c       & d 
\end{bmatrix}
\begin{bmatrix}
x_1       & x_2 \\
x_3       & x_4 
\end{bmatrix}=
\begin{bmatrix}
c       & d \\
a       & b 
\end{bmatrix}	\Rightarrow
\begin{bmatrix}
x_1       & x_2 \\
x_3       & x_4 
\end{bmatrix}=\frac{1}{ad-bc}\begin{bmatrix}
cd-ab       & d^2-b^2 \\
a^2-c^2       & ab-cd
\end{bmatrix}=R
$$ $L\neq R$ in the general case $det(A)\neq 0$ for $R$ and $L$ to exist if $det(A)\neq 0$ and $a=d=0$ , then $P'=L=R$ Additional notes I feel I have to clarify further. I was looking for a matrix $P'$ that behaves just like $P$ but from the opposite side, meaning $P'$ permutes columns when pre-multiplying $A$ and rows when post-multiplying $A$ .","['permutations', 'linear-algebra', 'combinatorics']"
3112222,Show that if $G$ is a finite group and $H_i$ are subgroups of $G$ with $[G:H_i]=2$ then $[G:\cap H_i]=$ some power of $2$,"Show that if $G$ is a finite group and $H_i$ are subgroups of $G$ with $[G:H_i]=2$ then $[G:\cap H_i]=$ some power of $2$ . My try : Let the number of subgroups of $G$ be $H_1,H_2,\ldots ,H_m$ Its clear that each $H_i$ is a normal subgroup of $G$ and every $H_i$ has exactly two left/right cosets. Let the left cosets of $H_1$ in $G$ be $H_1,g_1H_1$ ,that of $H_i$ in $G$ be $H_i,g_iH_i$ and so on. Let $H=\cap H_i$ Now we know that $[G:H\cap K]\le [G:H][G:K]$ for any two subgroups $H,K$ of $G$ . Thus we have $[G:H]\le 2^m$ Now I need to show only that $[G:H]\ge 2^m$ Now I understand that since $g_i\notin H_i\implies g_i\notin H$ hence we have at least $m$ cosets of $H$ in $G$ given by $g_1H,g_2H,\ldots g_mH$ But I need to find at least $2^m$ How can I do it? Please give some hints","['group-theory', 'abstract-algebra']"
3112254,How to solve ode of the form $ a_n(y')^n + a_{n-1}(y')^{n-1} + \cdots + a_1 y' + a_0 =0 $,"$
a_n(y')^n + a_{n-1}(y')^{n-1} + \cdots + a_1 y'+a_0 =0
$ I am also unclear on how to describe this as it is not nth order. The polynomial being in the derivative is not something that I think I have seen before.",['ordinary-differential-equations']
3112262,"How to prove $\{G_i\to F\}$ is open covering only if $\forall$ field $K$, $F(Spec K)=\cup_iG_i(Spec(K))$?","This is an exercise in Eisenbud, Harris, Geometry of Schemes VI-11 as this part is skipped in Mumford Algebraic Geometry II. I think I figured out a way to do it but I am not totally sure. $\{G_i\to F\}$ is a collection of open subfunctors with $F:Schemes\to Set$ where open subfunctors means for all $h_R=Hom(-,Spec(R)),\phi\in Nat(h_R,F)$ , $G_i\times_\phi h_R$ is a subfunctor of $h_R$ where pullback is defined on affine objects. Now $\{G_i\to F\}$ is called covering if for any scheme $X$ with $h_X=Hom(-,X)$ and any $\phi\in Nat(h_X,F)$ , $G_i\times_Fh_X$ is representable as $h_{U_i}$ with $U_i$ covering $X$ . Show that $\{G_i\to F\}$ is open covering iff $F(Spec(K))=\cup G_i(Spec(K))$ for all field $K$ . Forward direction is trivial by applying all functors to $Spec(K)$ . The fiber product has either 1 element or none by embedding into $Hom(Spec(K), Spec(K))=Hom(K,K)=\{1_K\}$ . It follows equality of $F(Spec(K))=\cup G_i(Spec(K))$ . I am kind of having trouble with reverse direction. If $F$ is representable as a scheme, then it boils down to prove the statement for affine schemes where $G_i$ will be identified as hom functor of open subschemes of affine scheme. Use all residue fields to detect the missing points of covering. Then I can see it indeed forms a covering. $\textbf{Q:}$ How do I prove the converse statement? I am also kind of having trobule to grasp the main point of the converse statement. What is the geometric meaning?","['algebraic-geometry', 'abstract-algebra', 'category-theory']"
3112285,Let $G$ be a group and let $S$ be a subset such that if $a\in S$ then $a^{-1}\not \in S$.,"Let $G$ be a group and let $S$ be a subset of $n$ distinct elements of $G$ with the property that $a\in S$ implies $a^{-1}\not\in S$ . Consider the $n^2$ products (not necessarily distinct) of the form $ab$ , where $a\in S$ and $b\in S$ . Prove that at most $n(n-1)/2$ of these products belong to $S$ . I have been thinking about this problem for two days, I have done nothing about the proof, but just exploring some examples. I consider the group $G=\langle\mathbb Z_{11}, \cdot\rangle$ , and let $$S=\{2,3,5,7\}$$ which clearly satisfy the property. Then I tested it, in $16$ of the numbers, exactly $6$ of them are in $S$ . Any suggestion for how to prove it?",['group-theory']
3112287,Understanding the textbook: Expressing a triple integral in a different order,"I was going over my textbook and have trouble understanding this process: Why does the innermost integral have the bounds $\sqrt{y}$ and $1$ ? The shape on the $xz$ plane looks the same as the one on the $xy$ plane so I figured that you could replace $\sqrt{y}$ with $\sqrt{z}$ in the bounds. I did a quick calculation on my computer using $f(x,y,z)=xyz$ to check and the results were $0.0125$ and $0.02916667$ respectively. What am I missing here?","['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'multiple-integral']"
3112296,Show that $AB=CD $.,"Let $ABCD $ a tetrahedron s.t. $ \angle ABD=\angle BDC $ and $ \angle BAC=\angle ACD$ . Show that $AB=CD $ . I construct $DD_1, CC_1 \perp AB $ , $C_1, D1\in AB $ and $AA_1, BB_1\perp CD $ , $B_1, A_1\in CD $ . By congruences of triangles we have $BD_1=DB_1$ and $AC_1=CA_1$ . If $AB\perp CD$ then it is easy because $C_1=D_1$ and $A_1=B_1$ . Now I am stuck.","['euclidean-geometry', 'geometry', '3d']"
3112302,Proving IID Central Limit Theorem using Lindeberg Conditions.,"The goal is to prove the IID Central Limit Theorem through Lindeberg's Condition. Suppose that $X_1,X_2,\ldots\displaystyle\sim\text{i.i.d.}$ with $E[X_i]=\mu$ and $Var[X_i]=\sigma^2<\infty$ . Let $Y_i=X_i-\mu$ and $s_n^2=\sum_{i=1}^{n}Var[Y_i]=n\sigma^2$ . Prove that $Z_n:=\frac{\sum_{k=1}^{n}(X_k-\mu)}{s_n}\rightarrow$ N $(0,1)$ in distribution. Lindeberg's condition is as follows: If the following holds: $\displaystyle\lim_{n\rightarrow\infty}\frac{1}{s_n^2}\sum_{i=1}^{n}E\big[Y_i^2\cdot\mathbb{I}_{( \ |Y_i|\geq\epsilon\cdot s_n \ )}\big]=0$ for all $\epsilon>0$ Then $Z_n\rightarrow$ N $(0,1)$ in distribution. So going right to lindbergs condition: $\displaystyle\lim_{n\rightarrow\infty}\frac{1}{n\sigma^2}\sum_{i=1}^{n}E\big[Y_i^2\cdot\mathbb{I}_{( \ |Y_i|\geq\epsilon\cdot \sigma\sqrt{n} \ )}\big]=\displaystyle\lim_{n\rightarrow\infty}\frac{1}{\sigma^2}E\big[Y_i^2\cdot\mathbb{I}_{( \ |Y_i|\geq\epsilon\cdot \sigma\sqrt{n} \ )}\big]$ Now at this point I do know I can use Lebesgue Dominated Convergence theorem due to the following: $ |Y_i^2\cdot\mathbb{I}_{( \ |Y_i|\geq\epsilon\cdot \sigma\sqrt{n} \ )}|=Y_i^2\cdot\mathbb{I}_{( \ |Y_i|\geq\epsilon\cdot \sigma\sqrt{n} \ )}\leq Y_i^2$ and $Y_i^2$ is integrable as $E[Y_i^2]=\sigma^2<\infty$ This means: $\displaystyle\lim_{n\rightarrow\infty}\frac{1}{\sigma^2}E\big[Y_i^2\cdot\mathbb{I}_{( \ |Y_i|\geq\epsilon\cdot \sigma\sqrt{n} \ )}\big]=\frac{1}{\sigma^2}E\big[\displaystyle\lim_{n\rightarrow\infty}Y_i^2\cdot\mathbb{I}_{( \ |Y_i|\geq\epsilon\cdot \sigma\sqrt{n} \ )}\big]$ Now I do not understand how this above is zero, and that's where I am stuck. I do know that by Markov's inequality: $P(|Y_i|\geq\epsilon\sigma\sqrt{n})\leq\frac{E\big[|Y_i|\big]}{\epsilon\sigma\sqrt{n}}\rightarrow0$ as $n\rightarrow\infty$ as $E[|Y_i|]<\infty$ , and so the probability of this event becomes zero. Any help with understand this final step would be much appreciated!","['central-limit-theorem', 'lebesgue-integral', 'probability-theory', 'real-analysis']"
3112316,"Is it true that $\int_0^1 \big(K(k^{1/2})\big)^2\,dk = \frac{7}2\zeta(3)$?","Define the complete elliptic integral of the first kind as, $$K(k) = \tfrac{\pi}{2}\,_2F_1\left(\tfrac12,\tfrac12,1,\,k^2\right)$$ Part I. From the link above, we find some of the evaluations below, $$\begin{aligned}
\int_0^1 K(k^{1/1})\,dk &= 2C\\ 
\int_0^1 K(k^{1/2})\,dk &= 2\\ 
\int_0^1 K(k^{1/3})\,dk &= \frac34(2C+1) \\ 
\int_0^1 K(k^{1/4})\,dk &= \frac{20}9 \\ 
\int_0^1 K(k^{1/5})\,dk &= \frac5{64}(18C+13)
\end{aligned}$$ and so on (?) where $C$ is Catalan's constant . Part II. On a hunch, I decided to check 2nd powers. It turns out that, $$\begin{aligned}
\int_0^1 \big(K(k^{1/2})\big)^2\,dk &= \frac{7}2\zeta(3)\\ 
\int_0^1 \big(K(k^{1/4})\big)^2\,dk &= \frac{7}2\zeta(3)+1\\ 
\int_0^1 \big(K(k^{1/6})\big)^2\,dk &= \frac{231}{64}\zeta(3)+\frac{51}{32}\\ 
 \int_0^1 \big(K(k^{1/8})\big)^2\,dk &= \frac{238}{64}\zeta(3)+\frac{881}{432}\\ 
\end{aligned}$$ and so on (?) where $\zeta(3)$ is Apery's constant . Question: Does the pattern of Part I (involving Catalan's constant) and Part II (involving Apery's constant) really go on forever? What is the closed-form?","['integration', 'elliptic-functions', 'closed-form', 'zeta-functions', 'hypergeometric-function']"
3112329,Looking for where I went wrong: Finding the volume of a solid that lies within both a cylinder and sphere,"I'm currently working on this question: Find the volume of the solid that lies within both the cylinder $x^2+y^2=1$ and the sphere $x^2+y^2+z^2=4$ . I decided to use polar coordinates so that the cylinder equation becomes $r^2=1$ and the sphere becomes $r^2+z^2=4$ . Solving for $z$ , I get the inequality $-\sqrt{4-r^2}\leq z\leq \sqrt{4-r^2}$ . Since I know what $r^2$ is, I plug that in to get the inequality where $z$ is between $-\sqrt{3}$ and $\sqrt{3}$ . Combining that to make a triple integral, I get: $$\int_0^{2\pi}\int_0^1\int_{-\sqrt{3}}^\sqrt{3}r\,\mathrm{d}z\,\mathrm{d}r\,\mathrm{d}\theta$$ However, Slader has a different answer where they didn't plug in $\sqrt{3}$ into the bounds. Why does plugging in the value for $r^2$ make the calculation wrong? Isn't $r^2$ always $1$ ?","['integration', 'volume', 'multivariable-calculus', 'calculus', 'multiple-integral']"
3112349,Computing volume of Riemannian manifolds and its $n$-sheeted covering,"I have a question- It is given that $f: M \to N$ is an $n$ -sheeted covering map and a local isometry then I have to show that volume $(M) = n$ volume $(N)$ , where $M$ and $N$ are Riemannian manifolds. Let $p$ be an element in $M$ , and let $(U,(x_1,...x_n))$ be a coordinate chart around $p$ . Now $f(p) \in N$ , let $V$ be an open set in $N$ and $(V,(y_1,...y_n))$ be a coordinate chart around $f(p)$ in $N$ . Now, looking at the expression of the volume of a region $R $ in manifold, $$ vol(R) = \int_{R} \sqrt{\det(g_{ij})} dx_{1}...dx_{n}    $$ it seems like we have to use local isometry and covering space definition at some point to express $g_{ij}$ , Riemannian-metric of $M$ in terms of Riemannian-metric $h_{ij}$ of $N$ ....I've been trying to play around with the definitions, but unsuccessfully! Can someone give a hint on where to proceed from here?","['volume', 'riemannian-geometry', 'covering-spaces', 'isometry', 'differential-geometry']"
3112430,Center is a normal subgroup of G,"This is a problem from Herstein's Topics in Algebra. I have already shown the above result using the definition of normal subgroup. But now I want to prove it by constructing a homomorphism such that kernel is center of the group G. How can I construct such homomorphism? I was thinking of going like this. Given a $g$ in $G$ construct $E_g(x)=g x g^{-1}$ So given each element we have a transformation. Set of transformations like this form a group with inverse given by $E_{g^{-1}}$ . Kernel consist of all those elements for which $E_g$ is identity. In other words, $E_g(x)=x$ or $g x g^{-1}=x$ or $gx=xg $ for all $x$ . That is $g$ commutes with everything in $G$ . Am I going into the right direction. Edit: I became interested in proving the result through homomorphism approach as problem is in section 2.7 which is titled homomorphisms. Herstein must be expecting us to take this route.","['group-theory', 'abstract-algebra', 'group-isomorphism']"
3112452,Does there exist some sort of classification of finite verbally simple groups?,"Let’s call a group verbally simple if it does not have any non-trivial verbal subgroup. Does there exist some sort of classification of finite verbally simple groups? $G^n$ , with $G$ being a finite simple group, is always verbally simple as it has no nontrivial characteristic subgroups and all verbal subgroups are characteristic. However those may be not the only examples... If $G$ is verbally simple it is either abelian or perfect, as commutator subgroup is verbal. If $G$ is abelian, then it is $C_p^n$ for some prime $p$ as for any abelian group $A$ $V_{x^q}(A)$ is a nontrivial proper subgroup for any $q$ that is a nontrivial proper divisor of $exp(A)$ . However, I do not know, how to deal with the case, when $G$ is perfect.","['characteristic-subgroups', 'finite-groups', 'verbal-subgroups', 'abstract-algebra', 'group-theory']"
3112459,Apex angle of a triangle as a random variable,"I am not an expert in Probability Theory and I apologize if I make some mistakes in ""translating"" the following problem into the language of random variables. Any help also to improve the formulation of the problem is very much welcome. Problem. Given two angles $\alpha, \beta \in (0,2\pi)$ , consider the isosceles triangle with the apex angle $\theta = \alpha - \beta$ (or $\pi -(\alpha -\beta)$ if $\alpha - \beta > \pi$ ) and leg lengths 1. By well known elementary geometry, (twice) the area (without sign) of the triangle is given by $\vert \sin \theta \vert$ . Now consider a probability measure $\mu$ and suppose $\alpha, \beta$ are independent random variables w.r.t. to this probability measure. Q. Which is the probability measure $\mu$ which maximizes the mean area of the triangle, i.e. $$
\int_0^{2\pi}\int_0^{2\pi} | \sin(\alpha - \beta)| d\mu(\alpha) d \mu(\beta)?
$$ Thanks to @JeanMarie's comments below, we can get rid of an angle (by sticking one of the sides of the triangle to the x-axis) so the question above can be equivalently formulated as follows: Q. Which is the probability measure $\mu$ which maximizes the mean area of the triangle, i.e. $$
\int_0^{2\pi} | \sin(\alpha)| d\mu(\alpha)?
$$ For instance, if $\mu$ is the normalized Lebesgue measure on $(0,2\pi)$ the area of the triangle is $$
\frac{1}{4\pi}\int_0^{2\pi} \int_0^{2\pi} \vert \sin (\alpha-\beta) \vert d\alpha d \beta = \frac{1}{2}. 
$$ Is this the maximum value?","['triangles', 'probability-distributions', 'probability-theory', 'probability']"
3112476,"If $\tan{\frac{x}{2}}=\csc x - \sin x$, then find the value of $\tan^2{\frac{x}{2}}$.","If $\tan{\frac{x}{2}}=\csc x - \sin x$ , then find the value of $\tan^2{\frac{x}{2}}$ . HINT: The answer is $-2\pm \sqrt5$ . What I have tried so far: $$\tan{\frac{x}{2}} = \frac{1}{\sin x}-\sin x$$ $$\tan{\frac{x}{2}} = \frac{1-\sin^2 x}{\sin x}$$ $$\tan{\frac{x}{2}} = \frac{\cos^2 x}{\sin x}$$ I don't know how to solve this problem. Pls help. Thank you :)",['trigonometry']
3112504,Need help with the proof of Ricci tensor of Einstein and anti self dual manifold.,"Let M be an anti-self dual and Einstein 4-manifold with scalar curvature s. Then the Ricci tensor $c_Z$ of the twistor space  is given by $$c_Z (E,E) = (s/4 - t(s/12)^2) \|X\|^2 + (1+ (ts/12)^2)\|V\|^2,$$ where $X= \pi_\ast E, V= \mathcal{V}E$ and $i_X : \Lambda^2TM \to TM$ is the interior product. Proof The ricci tensor of the twistor space is given by $$c_Z(E,E) = c_M (X,X) + t  Trace (A \to (\nabla_A R)(\sigma \times V, X)) + (t^2/4)\| \mathcal{R}(\sigma \times V)\|^2 -(t/2) \| i_X \circ \mathcal{R}_+\|^2 + (t/2) \| i_X \circ \mathcal{R}(\sigma)\|^2 + \|V\|^2.$$ Since the manifold M is Hermitian, anti self dual and Einstein, $$\mathcal{J}V = \sigma \times VE,$$ $$\mathcal{R}= (s/6) Id + \mathcal{W}_-, \mathcal{R}_+ = (s/6)Id,$$ and $$g((\nabla_Y R)(W,X),Y)=g((\nabla_Y R)(X \wedge Y),W)=0, $$ for $X,Y \in \mathcal{X}$ and $W \in \mathcal{V}.$ Thus, $$c_Z(E,E) = \sum_{i=1}^4 g(R(X,E_i)X,E_i) -(t/2) \| i_X \circ \mathcal{R}_+\|^2 + (t/2) \| i_X \circ \mathcal{R}(\sigma)\|^2 +(1+ (ts/12)^2)\|V\|^2,$$ where $\{E_1,E_2,E_3,E_4\}$ are orthonormal basis of TM. Now I'm stuck how to proceed further.Also, $\| i_X \circ \mathcal{R}_+\|^2 - \| i_X \circ \mathcal{R}(\sigma)\|^2 = \sum_{i=1}^4 (g(R(U)X,E_i))^2 + (g(R(\sigma \times U)X,E_i))^2.$",['differential-geometry']
3112513,The explicit expression of $\frac{δF}{δP}$,"I'm writing simulation code of ferroelectric domain, and there is a math problem that I can't solve. The expression of $F$ is $$
F = \frac{|\vec{k} \cdot \vec{P}(\vec{k})|^2}{k^2}.
$$ $\vec{k}$ is a wave-vector in Fourier space, i.e. $\vec{k}=(k_x,k_y)$ , and $\vec{P}(\vec{k}) = (P_x,P_y)$ is the Fourier transform of Polarization in real space. What's the explicit expression of $$
\frac{δF}{δP_x}?
$$ More details are available here .","['variational-analysis', 'derivatives']"
3112589,Application of Schwarz lemma and Liouville's theorem,"I recently came across the following problem Let $h : \Bbb C → \Bbb C$ be an analytic function such that $h(0) = 0, h(1/2) = 5$ , and $|h(z)| < 10$ for $|z| < 1$ . Then pick out the correct statement(s): the set $\{z : |h(z)| = 5\}$ is unbounded by the Maximum Principle the set $\{z : |h'(z)| = 5\}$ is a circle of strictly positive radius $h(1) = 10$ regardless of what $h'$ is, $h'' \equiv 0$ . My try Consider $f(z)=\frac{h(z)}{10}$ and $f$ satisfies the hypothesis of Schwarz lemma, so $$\vert f(z) \vert \leq \vert z \vert$$ and so $$\vert h(z) \vert \leq 10 \vert z \vert ^1$$ so by extended Lioville's theorem, $h$ is a polynomial of degree atmost $1$ . So $h$ is either constant or linear. But given hypothesis implies $h$ is not constant and $h$ is exactly $10z$ . That is, $$(\forall z \in \Bbb C): h(z)\equiv 10z$$ So the set in the first bullet is a circle $\vert z \vert =1/2$ ,which is bounded and so it is false The second set is $\varnothing$ and so the second  one is false Third and fourth are true. Since $h$ is linear , its second derivative vanishes! Is my reasoning correct ? Any thoughts? Edit: I agree that my reasoning in wrong as Mr. Kavi Rama Murthy says in the comment. So this $h(z)=10z$ helps to eliminate the first two options. But how to conclude the last two. Any help ?",['complex-analysis']
3112600,The probability that a Binomial Distribution deviates from its mean by one standard deviation,"Let $X$ be a random variable that follows the Binomial Distribution $\text{BIN}(n,p)$ , where $n$ is a positive integer while $p\in(0,1)$ . Its mean is $np$ , and standard deviation is $\sqrt{np(1-p)}$ . Chebyshev's inequality yields that $$\Pr\left(|X-np| > \sqrt{np(1-p)}\right) \le 1,$$ which is trivial. Hoeffding’s inequality seems not helpful to improve the bound if applied in a direct way. Questions: When $p=1/2$ , is it possible to prove for all $n\ge 1$ that $$\Pr\left(|X-np| > \sqrt{np(1-p)}\right) \le \frac{1}{2}?$$ What can we say for a general $p\in(0,1)$ ? Remarks: I found a positive answer to Question 1 (presented below; essentially the same as @Mau314's comment). However, it is not completely satisfactory because we have to verify the inequality for small n (at most 25) numerically. I am still looking forward to an answer that is completely analytical . I am teaching basic probability theory and these questions occur to me when I think about the Central Limit Theorem. When $n\rightarrow \infty$ , asymptotically we have $$\Pr\left(|X-np| > \sqrt{np(1-p)}\right) \sim \Pr(|Y| > 1) < \frac{1}{2},$$ where $Y$ is a random variable that follows the Standard Normal Distribution. Hence I raise the questions by curiousity. Note that my main interest is the non-asymptotic behaviour of the probability because the asymptotic case is characterized by the CLT. One may attack the problem by directly estimate the cumulative distribution function of Binomial Distributions. To this end, bounds for Binomial Coefficients are likely necessary. Results of such kind can be found in, e.g., [Das] , [Stanica] , [Spencer, Chapter 5] , and Wikipedia . Note that non-asymptotic estimations are needed. Thanks.","['statistics', 'probability-distributions', 'real-analysis', 'probability-theory', 'probability']"
3112616,"If $f:I\to \mathbb{R}$ is convex and interval $I$ is bounded, prove that $f$ is bounded below.","Let $I$ be a bounded interval and $f:I\to \mathbb{R}$ be a convex function. Prove that $f$ is bounded below in $I.$ Attempt. Let $a,~b\in I$ , by convexity of $f$ on $[a,b]:$ $$f(x)\leq g(x):=f(a)+\frac{f(b)-f(a)}{b-a}(x-a)$$ for all $x\in [a,b]$ . So it is enough to prove that: $f(x)\geq g(x)$ for $x\in I,~x<a$ or $x>b$ , $f$ attains a minimum value $m$ on $[a,b]$ , Thanks for the help.","['convex-analysis', 'analysis', 'real-analysis']"
3112634,Derivative of Kronecker product of vector with itself,"I'm struggling with the following problem. Suppose $\pmb{x}$ and $\pmb{y}$ are vectors of the same length and $\pmb{y}$ is not a function of $\pmb{x}$ . What is the following derivative? $$
\frac{\partial}{\partial \pmb{x}} (\pmb{y} - \pmb{x}) \otimes (\pmb{y} - \pmb{x})
$$ My thought was to write use $\pmb{z} = \pmb{y} - \pmb{x}$ and $\pmb{f} = \pmb{z} \otimes \pmb{z}$ and derive first: \begin{align}
d\pmb{f} &= ((d\pmb{z}) \otimes \pmb{z}) + (\pmb{z} \otimes (d\pmb{z})) \\
&= (\pmb{I} \otimes \pmb{z})d\pmb{z} + (\pmb{z} \otimes \pmb{I})d\pmb{z} \\
&= ((\pmb{I} \otimes \pmb{z}) + (\pmb{z} \otimes \pmb{I}))d\pmb{z} \\
\frac{\partial \pmb{f}}{\partial \pmb{z}} &= (\pmb{I} \otimes \pmb{z}) + (\pmb{z} \otimes \pmb{I})
\end{align} and then obtain by chain rule: $$
\frac{\partial}{\partial \pmb{x}} (\pmb{y} - \pmb{x}) \otimes (\pmb{y} - \pmb{x}) = -\left( (\pmb{I} \otimes (\pmb{y} - \pmb{x})) + ((\pmb{y} - \pmb{x}) \otimes \pmb{I}) \right)
$$ Which seems sensble. However, this is part of a Hessian I am deriving, and it's corresponding transpose element I derived to be: $$
-2\left(\pmb{I} \otimes (\pmb{y} - \pmb{x})\right)
$$ Which is very similar but not the same. Am I missing something obvious?","['vectors', 'matrices', 'matrix-calculus', 'kronecker-product', 'derivatives']"
3112651,Integral $\int{\frac{1}{\sqrt{2x^2+x+1}}}dx$,"I am trying to solve this integral but I can not figure what I do wrong. $$I=\int{\frac{1}{\sqrt{(2x^2+x+1)}}}dx$$ Here's how I go about it: I think that maybe it can be solved following the $$\int{\frac{1}{\sqrt{\color{red}{x}^2+\color{blue}{a}^2}}}dx=\ln\left(x+\sqrt{x^2+a^2}\right)$$ I turn the denominator into a sum of 2 products: $$2x^2+x+1=\left(x\sqrt{2}+\frac{1}{2\sqrt{2}}\right)^2+\left(\frac{\sqrt{7}}{2\sqrt{2}}\right)^2$$ and "" $\color{red}{x}$ "" from the formula would be "" $\left(x\sqrt{2}+\frac{1}{2\sqrt{2}}\right)$ "" while "" $\color{blue}a$ "" would be "" $\left(\frac{\sqrt{7}}{2\sqrt{2}}\right)$ also "" $x^2+a^2$ "" is the denominator "" $2x^2+x+1$ "". When I plug in these numbers I get the following result: $$I=\ln\left(\left(x\sqrt{2}+\frac{1}{2\sqrt{2}}\right)+{\sqrt{2x^2+x+1}}\right)$$ I sometimes check my results using an online integral calculator and for this one it shows a different result: $$\frac{\ln\left(\sqrt{\frac{(4x+1)^2}{7}+1}+\frac{4x+1}{\sqrt{7}}\right)}{\sqrt{2}}$$ I am sorry if the formatting is not quite right, It's the best I can do and it took me about an hour aswell. $\ddot \frown$","['integration', 'indefinite-integrals', 'proof-verification']"
3112675,Prove that any two open intervals are equinumerous.,"This is Lay's exercise $8.4.b$ . Prove that any two open intervals are
  equinumerous. Is my proof correct? And even if it is how can I make it better? Is there a better alternative? Consider two open intervals $(0,1)$ and $(m,n)$ for any $m,n\in\Bbb R$ such that there exists a function $f$ such that $f:(0,1)\longrightarrow (m,n)$ . Let $f(x)=(n-m)x+m$ . Then we can see that $f$ is a bijection between $(0,1)$ and $(m,n)$ . Now consider another open interval $(p,q)$ such that there exists a bijective function $g$ such that $g:(0,1)\rightarrow (p,q)$ . From the bijective functions $f$ and $g$ we get the bijective function $h$ such that $h=g\circ f^{-1}:(m,n)\longrightarrow (p,q)$ . Since $(m,n)$ and $(p,q)$ were arbitrary we can conclude that any two open intervals are equinumerous. I think my proof is correct but since the statement seems to be a general one how could it be proved so simply? Thank you in advance","['elementary-set-theory', 'proof-writing', 'proof-verification', 'real-analysis']"
3112682,Weak-$*$ topology on algebraic dual,"I was looking at Izzo, Alexander J. , A functional analysis proof of the existence of Haar measure on locally compact Abelian groups , Proc. Am. Math. Soc. 115, No. 2, 581-583 (1992). ZBL0777.28006 . which proves existence of the Haar-measure for locally compact abelian groups using the Markov-Kakutani theorem. What I find strange is that the Haar measure is constructed as an element of the dual of $C_c(X)$ . But for noncompact $X$ (such as $X$ being the real numbers $\Bbb R$ ) this must be an unbounded functional (as the Lebesgue-measure on $\Bbb R$ is not finite). It seems like the author has no problem with this, and (without mentioning it further) goes on to define a weak-* topology for this case and even uses Banach-Alaoglu. I have not seen this being done this way before, am I misunderstanding something or can one define a weak-* topology on the algebraic dual of a TVS without any problems?","['harmonic-analysis', 'haar-measure', 'functional-analysis']"
3112685,Tangent vector field to a smooth curve over a smooth manifold,"I am teaching myself some elementary differential geometry and am stuck on the concept of the tangent vector field of a smooth curve. I have searched the web for an hour or so but cannot find anything pertaining specifically to my issue here. For a smooth curve $\gamma:(0,1)\rightarrow M$ , where $M$ is a manifold with a connection $\nabla$ , we can define the notion of autoparallel transport. We say the curve is autoparallely transported if $\nabla_{\dot{\gamma}}\dot{\gamma} = 0$ on the curve, where $\dot{\gamma}$ is the tangent vector field to $\gamma$ . My question is, how do we define this guy in terms of an element in the section of the tangent bundle? This is my initial guess, sort of with an abuse of notation: \begin{equation} \dot{\gamma}(\bullet) := \dot{\gamma}(\gamma(\bullet)) = \dot{\gamma}\circ \gamma : (0,1)\to TM ,\end{equation} which I think would make the latter $\dot{\gamma}\in\Gamma(TM)$ , i.e. a vector field sort of sending $M\lvert_{\gamma}\to TM$ . But something feels off, is this enough to be able to continue?","['vectors', 'vector-spaces', 'riemannian-geometry', 'differential-geometry']"
3112690,Is the function $\frac{x \cos x}{1+x^2}$ improperly integrable function?,"I was wondering if the function $\frac{x \cos x}{1+x^2}$ is improperly integrable on $[0,\infty)$ , i.e., $\int_0^\infty \frac{x \cos x}{1+x^2}dx$ exists in the sense of improper integration. More precisely, $\displaystyle \lim_{R \to \infty} \int_0^R \frac{x \cos x}{1+x^2}dx$ converges in $\mathbb{R}.$ Clearly, the Cauchy principal value $\int_{-\infty}^{\infty} \frac{x \cos x}{1+x^2}dx$ is $0,$ since $\frac{x \cos x}{1+x^2}$ is an odd function. By similar argument as in the answer of Per Manne [ Convergence/absolute convergence of $\int_0^\infty \frac{\cos x}{1+x}dx$ (Baby Rudin P6.9) , I can conclude $\displaystyle \lim_{n \to \infty} \int_{\frac{\pi}{2}}^{(n+\frac{1}{2})\pi}\frac{x \cos x}{1+x^2}dx$ converges. Here $n \in \mathbb{N}$ is crucial for the proof, since the alternating series test was used in the proof. But, I was wondering if it implies the convergence of $\displaystyle \lim_{R \to \infty} \int_0^R \frac{x \cos x}{1+x^2}dx$ . Please give me any comment for my question. Thanks in advance!","['complex-analysis', 'calculus']"
3112718,Finding derivative of $\sqrt[3]{\sin(2x)}$ using only definition of derivative,"First post here, so hello everyone. Here's the problem: Find the first derivative of: $$\sqrt[3]{\sin(2x)}$$ But, you can only use the difference quotient... 
(i.e. the limit of $\frac{f(x+h)-f(x)}{h}$ as $h \rightarrow 0$ ) This is a particular problem given by my prof. It's not for credit or even an assignment, she gave it as an interestingly difficult problem with some twist and turns. I've worked on it for several hours now and would like to see how others approach solving it as I'm wrapped around an axel after using double-angle ID and clearing the cubes in the numerator. Any advice is appreciated. Thanks.","['calculus', 'derivatives', 'trigonometry', 'real-analysis']"
3112755,When does Riemann-Lebesgue lemma hold in general?,"Let's say for simplicity that I'm on the torus $\mathbb{T}=\mathbb{R}/\mathbb{Z}$ . In this setting, Riemann-Lebesgue lemma could be stated as: for any $f\in L^1(\mathbb{T})$ , $$ \lim_{\vert k\vert\to\infty}\int_\mathbb{T} f(x)e^{ikx}\, dx =0$$ which can be interpreted as: for any $f\in L^1(\mathbb{T})$ , $\widehat{f}\in c_0$ , where $\widehat{\cdot}$ denotes the discrete Fourier transform and $c_0$ the space of infinitesimal sequences. If I considered a Radon measure $\mu$ on $\mathbb{T}$ instead of $f$ , then it's easy to check that $\widehat{\mu}\in l^\infty$ and Riemann-Lebesgue lemma fails, take $\mu=\delta_x$ . The case $d\mu = f\,dx$ with $f\in L^1$ corresponds to measures which are absolutely continuous w.r.t. the Lebesgue measure. However the same technique of proof for $f\in L^1$ , i.e. showing the statement for smooth functions and then exploiting a density argument, shows that Riemann-Lebesgue lemma holds for any $\mu$ belonging to the closure of smooth functions in the total variation norm $\Vert\cdot\Vert_{TV}$ . So my question is: what is this space? Is it just the space of all absolutely continuous measures or it contains other objects (like the derivative of the Cantor function)? Does it have a proper characterization?","['fourier-series', 'measure-theory', 'fourier-analysis', 'bounded-variation']"
3112760,$ \mu(|f|\geq \alpha) = \frac{1}{\alpha} ||f||_1$,"I'm having difficulty with this problem here: Let $f\in L^1(X,\mathcal{M},\mu)$ with $||f||_1 \neq 0$ . Prove that there exists a unique $\alpha$ so that $ \mu(\lbrace|f|\geq \alpha\rbrace) = \frac{1}{\alpha} ||f||_1$ . My first thought was to use the Hardy LittleWood Maximal Theorem which says that there exists $C$ so that $\mu(\lbrace Hf(x) >\alpha\rbrace) \leq \frac{C}{\alpha} ||f||_1$ Unfortunately this theorem does not take me very far in this approach. Any help is appreciated. Edit: It seems the question was asking to show that there is at most one such $\alpha$","['integration', 'measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
3112770,Upper bound on expected norm of subgaussian random matrix,"Let $A \in \cal{M}_{n \times m}(\Bbb{R})$ be a random matrix with IID subgaussian entries with variance proxy $\sigma^2$ .  Show that $E[||A||_{op}] \le c \sigma \sqrt{m+n}$ for a constant $c$ to be determined, where the operator norm of $A$ is induced from $\ell^2$ vector norm (i.e. given by $||A||_{op}=\sup\limits_{x \in \Bbb{R}^m} ||Ax||_2/||x||_2$ ). My try: First, observe that in the given definition of operator norm, the ratio remains the same upon multiplication of $x$ by a nonzero scalar $k$ , so the supremum can be taken within the unit sphere $||u||_2=1$ . $$||A||_{op} = \sup\limits_{||u||_2=1} ||Au||_2$$ Let $a_{ij}$ be the $(i,j)$ -th entry of $A$ for all $i = 1,\dots,n$ and $j = 1,\dots,m$ .  Observe that each row of $A$ is a subgaussian vector with variance proxy $\sigma^2$ since the entries of $A$ are IID subgaussian.  By definition of subgaussian vector, each entry of $Au$ is a subgaussian variable with variance proxy $\sigma^2$ since $$\sum_{j=1}^m a_{ij} u_j = (a_{i1},\dots,a_{im})\,u \text{ and } ||u||_2 = 1.$$ Therefore, $||Au||_2^2$ is the sum of square of $n$ subgaussian variables. $$\forall i = 1,\dots,n, E\left[\left(\sum_{j=1}^m a_{ij} u_j\right)^2\right] \le 4\sigma^2 \tag{moment condition}$$ Summing the above inequality on $i$ , we get $E[||Au||_2^2] \le 4n\sigma^2$ .  Finally, I applied Jensen's inequality to get $$E[||Au||_2]\le \sqrt{E[||Au||_2^2]}\le 2\sqrt n\sigma.$$ The RHS of the above inequality is independent of $u$ , so $$\sup\limits_{||u||_2=1}E[||Au||_2]\le 2\sqrt n\sigma.$$ However, I think I am in the wrong direction because there's no $m$ in the final inequality, and the $\sup$ should be inside the expectation. Thanks for reading. Source of the question: Exercise 2.2.7 in my lecture notes .","['expected-value', 'random-matrices', 'normed-spaces', 'probability-theory']"
3112873,Generalization of the fundamental theorem of duality,"The ""fundamental theorem of duality"" states: If $X$ is a real linear space and $f, f_1,...,f_n$ are linear functionals on $X$ , then $f$ lies in the span of $f_1,...,f_n$ (i.e. $f = \sum_{i=1}^n \lambda_i f_i$ , where $\lambda_i \in \mathbb R$ ) if and only if $\bigcap_{i=1}^n ker \ f_i \subseteq ker \ f$ (where $ker \ g = \{x \in X: g(x)=0\}$ ). My question is: Can someone please direct me to a reference that generalizes this result to arbitrary collections of linear functionals? In other words, if $I$ is an arbitrary index set, I am wondering how to characterize $$\bigcap_{i \in I} ker \ f_i \subseteq ker \ f.$$ Intuitively, I suspect that the equivalent statement is something like: there exists a finitely additive signed measure $\mu$ on $2^I$ such that for all $x \in X$ $$ f (x) = \int f_i (x) \ \mu(di).$$ But this will be tricky as it involves finitely additive integration of potentially unbounded functions. (Clearly, though, this condition is sufficient for the condition about kernels to hold. The difficulty is in showing that it is also necessary.) Anyway, surely generalizations of the fundamental theorem of duality have been explored, and any references or pointers would be appreciated.","['reference-request', 'real-analysis', 'linear-algebra', 'functional-analysis', 'duality-theorems']"
3112875,"when $\sqrt{y}$ and $\sqrt{x}$ are defined, is $\sqrt{y}$ = $\sqrt{x}$ a function?","when $\sqrt{y}$ and $\sqrt{x}$ are defined, is $\sqrt{y}$ = $\sqrt{x}$ a function? for (x,y) in the reals. I think I'm thinking to hard about what the graph will look like",['functions']
3112881,Chebyshev function: variational formulation,"The Wikipedia article on the Chebyshev function $\psi(x)$ states that $\psi(e^t)$ minimizes the functional $$J[f] = \int_0^\infty \dfrac{f(s)\zeta'(s+c)}{\zeta(s+c)(s+c)}ds - \int_0^\infty \int_0^\infty e^{-st}f(s)f(t)ds dt,$$ so that $f(t) = \psi(e^t)e^{-ct}$ for $c>0$ . Wikipedia gives no source for this. Can someone please point me to a text proving this (and possibly more information on the connection between analytic number theory and the calculus of variations)?","['calculus-of-variations', 'number-theory', 'reference-request', 'analytic-number-theory', 'chebyshev-function']"
3112888,How many roots does an exponential polynomial have?,"Let $s$ be a complex variable and consider two polynomials with real coefficients: $$A(s) = s^n + a_{n-1}s^{n-1}+\ldots+a_1s+a_0,$$ $$B(s) = s^m + b_{m-1}s^{m-1}+\ldots+b_1s+b_0,$$ where $n \ge m$ . Let $k$ be a real constant. I am looking for roots of the function $$A(s)+e^{sk}B(s)=0.$$ Obviously, when $k=0$ I have just a polynomial of degree $n$ and I have $n$ roots. Then I have two questions: Is it true that this function always has $n$ roots for any fixed $k$ ? (Negative, see UPD2). Do these roots depend continuously on $k$ ? UPD: We also assume that $n\ge 1$ . UPD2: Let us take $A(s)=s$ and $B(s)=1$ . Then we have $$s+e^{ks} = 0.$$ For $k=0$ the only roots is $s_1=-1$ . However, for $k=-1$ we have $$se^{s}=-1$$ and we have multiple solutions. So the answer to the first question is negative. However, I do not know if the roots are continuous with respect to $k$ .","['complex-analysis', 'exponential-function', 'polynomials', 'roots']"
3112898,Closed form expression for the series $\sum_{k=0}^\infty \frac{k^j}{k!}$ in terms of $j$,"For my research I need to compute ""approximate"" moment generating function (we approximate exponential with the first $D$ elements of its Taylor series) of the number of fixed points of a permutation which involves the following sum: \begin{equation}\sum_{k=0}^\infty \frac{k^j}{k!} \end{equation} in terms of $j$ . I am pretty sure this grows exponential in $j$ but I am interested in finding the exact exponent. With some elementary calculations I get the following recurrence relation \begin{equation}\sum_{k=0}^\infty \frac{k^j}{k!}  = \sum_{m=0}^{j-1}{j-1\choose m} \sum_{k=0}^\infty \frac{k^{j-1-m}}{k!}  \end{equation} but I couldn't proceed further.","['combinatorics', 'sequences-and-series']"
3112916,"How to visualize the partial derivation of $f(x,y,z)$ with respect to only one of the axes?","given a function $f(x,y)$ , we can easily visualize the partial derivation of $f(x,y)$ with respect to $x$ or $y$ . 1. The output of the function $f(x,y)$ is in the z direction. Just like the output of $f(x)$ is in the y direction. 2. $\frac{\partial }{\partial x }f(x,y)$ can be visualized by thinking that $y$ is constant and $x$ is changing. And with the change of $x$ the output is also changing. (we can think it as the change in height of the graph or simply the change in z direction). then slight change in $z$ direction divided by slight change in $x$ direction is $\frac{\partial }{\partial x }f(x,y)$ . But what about $f(x,y,z)$ ? 1. If I continue to follow the previous examples about visualizing the output on a different dimension (like in $f(x,y)$ the output was in z direction) then I need to think of a fourth dimension. Which in my level, is not possible instantly, and also I don't think it's necessary to solve my problem. 2. Now to think about the partial derivation process, $\frac{\partial }{\partial z }f(x,y,z)$ means $x$ and $y$ both are to be thought as constants. Now with the change of z the output changes. But as I cannot even visualize the change of outputs as mentioned in (no 1), I cannot think of any curve forming like it did in case of $f(x,y)$ (If we pick a certainly value of $y$ , say $y = 1$ , and move to $x = 0$ (say) to $x = 7$ (say) and pinpoint the outputs in the $z$ direction, then adding all those points will give us a curve, which can be thought of a simple two dimensional curve and the rate of change of this very curve is what we call the partial derivative of $f(x,y)$ with respect to x.) Now my question is simply, how do I visualize taking the partial derivative of functions that include all of the $ x, y, z $ variables?",['multivariable-calculus']
3112918,How to find the direction cosines and direction angles for a given line,"Given the line $5x – 12y = 60$ What are the direction angles and direction cosines? From my notes, the direction cosines as the $x, y$ -components of the unit vector having the same slope as the line. Slope vector is $\langle 12, 5 \rangle$ and converting to a unit vector: $\bigl\langle \frac{12}{13}, \frac{5}{13} \bigr\rangle$ .
Using $\arccos$ with these two values, I get $22.6^\circ$ and $67.4^\circ$ respectively. Since the line has a positive slope, I can picture a positive acute angle that the line would make with the $x$ -axis, so $22.6^\circ$ makes sense. But looking at the angle made with the y axis, it should be obtuse so it makes sense to subtract $67.4$ from $180$ , yielding $180^\circ - 67.4^\circ = 112.6^\circ$ . This aligns with the correct answer provided. However, the answer sheet gave the direction $\cos$ for $y$ as $-\frac{5}{13}$ which would get the $112.6^\circ$ answer automatically. But how do I know when to include a negative, when the slope is positive?","['algebra-precalculus', 'trigonometry']"
3112932,What is a neat way to solve $\nabla\mathbf{u}+\nabla\mathbf{u}^T=\mathbf{\mathbf{C}}$?,"Let $\mathbf{u}:\mathbb{R}^3\to\mathbb{R}^3$ be a smooth enough vector field that satisfies the following equation $$\nabla\mathbf{u}+\nabla\mathbf{u}^T=\mathbf{C},\tag{1}$$ where $\nabla\mathbf{u}$ is gradient of $\mathbf{u}$ and $^T$ denotes transpose of a second order tensor and $\mathbf{C}$ is a constant symmetric second order tensor. What is a neat way to solve $(1)$ ? I mean I am not really inclined to write down the components and solve each scalar equation. I want a basis-free approach. The final answer is $$\mathbf{u}(\mathbf{x})=\mathbf{c}_1+\mathbf{c}_2\times\mathbf{x}+\mathbf{C}\cdot\mathbf{x}\tag{2}$$ where $\mathbf{c}_1$ and $\mathbf{c}_2$ are two constant vectors, $\cdot$ is simple contraction (or generalized dot product) and $\times$ denotes the usual cross product.","['vector-fields', 'tensors', 'multivariable-calculus', 'vector-analysis', 'partial-differential-equations']"
3112957,Balls and Boxes,"Three urns contain marbles. Each urn is large enough to hold all the marbles . The only operation allowed is to move marbles from an urn to another urn, such that the number of marbles in the receiving urn is doubled. Prove that it is possible, regardless of the initial configuration, to obtain a configuration where one urn is empty. This is an exercice from a french old book. I’ve been trying to solve it since 3 years without any issue.","['number-theory', 'arithmetic', 'abstract-algebra']"
3112964,Trouble understanding topological groups.,"I understand that a topological group is a group $G$ endowed with a topology $\tau$ on $G$ such that addition and inverse are continuous on $\tau$ . Now, the definition of continuity is that for all $U\in\tau$ , $f^{-1}(U)\in\tau$ . But in this case $+^{-1}(U)$ is a subset of $G\times G$ because the domain of $+$ is $G\times G$ , and we endowed $G$ with a topology, not $G\times G$ . So, where am I messing up?","['general-topology', 'abstract-algebra', 'definition', 'topological-groups']"
3112970,Is the proof of Pythagorean theorem using dot (inner) product circular?,"$x,y$ are perpendicular if and only if $x\cdot y=0$ . Now, $||x+y||^2=(x+y)\cdot (x+y)=(x\cdot x)+(x\cdot y)+(y\cdot x)+(y\cdot y)$ . The middle two terms are zero if and only if $x,y$ are perpendicular. So, $||x+y||^2=(x\cdot x)+(y\cdot y)=||x||^2+||y||^2$ if and only if $x,y$ are perpendicular.  ( I copied this ) I think this argument is circular because the property $x\cdot y=0 $ implies $x$ and $y$ are perpendicular comes from the Pythagorean theorem. Oh, it just came to mind that the property could be derived from the law of cosines. The law of cosines can be proved without the Pythagorean theorem, right, so the proof isn't circular? Another question : If the property comes from the Pythagorean theorem or cosine law, then how does the dot product give a condition for orthogonality for higher dimensions? Edit :  The following quote by Poincare hepled me regarding the question: Mathematics is the art of giving the same name to different things.","['proof-verification', 'linear-algebra', 'geometry', 'logic']"
3112975,Improper Integrals in Analysis,"Let $f:[0,\infty) \rightarrow \mathbb{R}$ be a continuous function and let $g(x)=\frac{1}{x}\int_1^x f(t)dt$ ; $x>0$ . Assume that $\lim_{x\rightarrow \infty} g(x)=B$ exists. Let $0 < a < b$ be two fixed numbers. Show $$\lim_{T\rightarrow \infty}\int_{Ta}^{Tb}\frac{f(x)}{x}dx=B\ln\left(\frac{b}{a}\right)$$ Here's my partial solution: $g(x)=\frac{1}{x}\left(F(x)-F(1)\right) \Rightarrow f(x)=g(x)+xg'(x)$ , Therefore: \begin{align*}
\lim_{T\rightarrow \infty}\int_{Ta}^{Tb}\frac{f(x)}{x}dx&=\lim_{T\rightarrow \infty}\int_{Ta}^{Tb}\frac{g(x)}{x}dx+\int_{Ta}^{Tb}g'(x)dx\\
&=\lim_{T\rightarrow\infty}\ln(Tb)g(Tb)-\ln(Ta)g(Ta)-\int_{Ta}^{Tb}g'(x)\ln(x)+\int_{Ta}^{Tb}g'(x)dx\\
&=\lim_{T\rightarrow \infty}B\ln\left(\frac{Tb}{Ta}\right)-\int_{Ta}^{Tb}g'(x)\ln(x)+\int_{Ta}^{Tb}g'(x)dx\\
&=B\ln\left(\frac{b}{a}\right)+\dots
\end{align*} I can see how $\lim_{T\rightarrow\infty}\int_{Ta}^{Tb}g'(x)dx=\lim_{T\rightarrow \infty}g(Tb)-g(Ta)=0$ but I can't see to make the $-\int_{Ta}^{Tb}g'(x)\ln(x)$ go to zero. Any help on that?","['calculus', 'improper-integrals', 'analysis', 'real-analysis']"
3112981,"Is $Y_n=f(X_n)$ a Markov chain, when $X_n$ is?","Let $X_n$ be an independent Markov chain which has values (states) $X_n={0,1,2}$ with its transition matrix. $$\\p=
 \begin{pmatrix}
  0 & \frac{1}{2} & \frac{1}{2} \\
  \frac{1}{2} & \frac{1}{2} & 0 \\
  1 & 0 & 0
 \end{pmatrix}
$$ Let $Y_n=f(X_n)$ $f(0)=0$ $f(1)=1$ $f(2)=1$ is $Y_n$ a Markov chain? My intuition and solution: It is not a Markov chain. $P(Y_n=1|Y_{n-1}=1,Y_{n-2}=0)=\frac{1}{4}$ $P(Y_n=1|Y_{n-1}=1)=\frac{1}{2}*p(1)$ , where p(1) is a probability that we are at the state 1, given we are either at the state 1 or 2. Surely probability that we are at the state 1 isn't equal to half, which is obvious if we look at the transition matrix, but how to calculate this probability? My guess: Lets calculate stationary probabilities, which are $(\frac{2}{5} ,\frac{2}{5} ,\frac{1}{5})$ , so we are on average two times more often in the state one than in state 2. which would mean that $p(1)$ , I am looking for is equal to $\frac{2}{3}$ ? EDIT: I found a way to avoid looking for this probability: it is enough to calculate the probability $P(Y_n=1|Y_{n-1}=1,Y_{n-2}=1,Y_{n-3}=0)$ , but still I would like to know if I had been right before I found this way.","['conditional-probability', 'markov-chains', 'probability-theory', 'probability']"
3112999,Determine a valid substitution for a differential equation,"I am trying to determine whether any of the $v(x)$ substitutions that I'm given are are possible to make the equation first order linear in terms of $v$ . $$y' = \frac{y}{x^2} + x^3y^3$$ The given possible substitutions are $$v(x)=x^3y^3
\\v(x)=y^2
\\v(x)=y^{-2}
\\v(x)=y/x$$ I don't know how to infer the answer by looking at it. So I went with a brute force approach. Solving for y in the first three yields $y = \frac{v^{1/3}}{x}$ , $y=\sqrt{v}$ , and $y=v^{-1/2}$ respectively. I went with the logic that this would not satisfy the linearity of the problem since $v$ in each case is nonlinear. Is the last one also false because it would make the equation $$vx = y
\\v'x + v = y'
\\v'x + v = \frac{v}{x} + x^7v^3$$ Thus keeping the equation in a nonlinear form. This is just my thinking on the matter. I want to make sure it is correct.","['substitution', 'ordinary-differential-equations']"
3113007,Find the maximum value of $\square OXPY$,"Problem: There are moving point $X$ and $Y$ lie on the $x$ and $y$ axes, respectively. For moving point $P$ , $PX=3$ and $PY=4$ . Find the maximum area of $\square OXPY$ .( $O$ is origin). My solution: $W.L.O.G$ , $X=(a,0),Y=(0,b)$ , $XY \leq 7$ ,then $0 \leq a^2+b^2 \leq 7^2$ . 
  By $A.M.$ , $$a^2+b^2 \geq 2 |ab|$$ When $\theta=\angle YPX$ we get $$a^2+b^2=16+9-2\cdot3\cdot4 \cos(\theta)$$ Area of $\square OXPY$ is ${1 \over 2}ab +\triangle XPY$ .
    So, $$\frac{1}{2}\cdot3\cdot4 \sin(\theta) +   {25-2\cdot3\cdot4 \cos(\theta) \over 4} \geq A(\theta)$$ . Maximum value is $$A(\theta) = 6\sin(\theta)-6\cos(\theta)+\frac{25}{4} ={25 \over 4}+6\sqrt2\sin(\theta-\frac{\pi}{4}) \leq {25 \over 4} + 6\sqrt{2}$$ , where $\theta=\frac{3}{4}\pi$ My intuition told me that $\triangle XPY$ is maximum $\triangle XPY$ is minimum But both of them is $\frac{49}{4}$ and smaller than my answer. Is my solution right? If it is right, why is my intuition wrong? And I would like to share if you know a different way of solution.","['area', 'geometry']"
3113050,Cans on a shelf,"6 cans are placed on a shelf in a circular arrangement. After tidying and cleaning the kitchen, the cans are placed again in new, random positions around a circle. What is the probability that none of the cans are in their initial positions or their adjacent? Isn't it $(\frac{3}{6})^6$ ?",['probability']
3113091,A poker hand contains five cards. Find the probability that a poker hand can be....,"a) Four of a kind (Contains four cards of equal face value) So for this one, we want four cards that have the same face value, different suit. And the last card can be any remaining card. There are 13 ranks, (A, 2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K). We have $\binom{13}{1}$ ways to pick 1 rank out of 13. For each of these ranks, we want to pick 4 cards that are all the same rank (and it is forcefully implied that these 4 cards will differ in suits). So $\binom{13}{1}\binom{4}{4}$ , and for each of these ways, we have $\binom{48}{1}$ way to pick 1 card out of the remaining deck of 48 cards, because we picked 4 cards already. Thus, $$\frac{\binom{13}{1}\binom{4}{4}\binom{48}{1}}{\binom{52}{5}}$$ is the total number of ways. b) Full House (Three cards of equal face value, and two others of equal face value). So i.e: 3, 3, 3, 2, 2 would be a full house where the three 3's and two 2's are distinguishable (different suit). So there are 13 ranks again, $\binom{13}{1}$ ways to pick 1 rank out of 13 total. For each of these ways, we want to pick three cards of the same rank. So $\binom{13}{1}\binom{4}{3}$ . Now we want two more cards that are of equal face value that differ from the other three cards picked earlier, so there are 12 ranks left, and $\binom{12}{1}$ ways to pick 1 rank out of 12 remaining. For each of these ways, we have $\binom{4}{2}$ ways to pick 2 cards out of the 4 suits belonging to the same rank, thus $\binom{12}{1}\binom{4}{2}$ . The total number of ways is: $$\frac{\binom{13}{1}\binom{4}{3}\binom{12}{1}\binom{4}{2}}{\binom{52}{5}}$$ c) Three of a kind. (Three cards of equal face value, and two cards with face values that differ from each other and the other three). 13 ranks, we want to pick 3 cards of the same rank. $\binom{13}{1}\binom{4}{3}$ . Now we want two cards that have different face values from the three picked, and each other. Now here is where I'm a bit confused, the ranks MUST be different, but the suits can be the same. So proceeding, $\binom{12}{1} \binom{4}{1}$ to pick 1 card of a different face value, but could be same suit, and then $\binom{11}{1}\binom{4}{1}$ to pick another card with a different face value, but could be a different suit. Multiply these altogether and divide by the denominator, and we have our total ways. Is my work correct?","['discrete-mathematics', 'combinatorics', 'probability']"
3113155,Convergence of $\sum\limits_{n=0}^{\infty} (1-|a_n|)$,"So I must prove that if $(a_n)$ is a sequence of points in $\mathbb{C}$ with $0< |a_n| < 1 \; \forall n \in \mathbb{N}$ and verifying that $|b| \leq \prod\limits_{n=1}^{\infty} |a_n|$ with $0<|b| < 1$ , then the series $$\sum\limits_{n=0}^{\infty} \left(1-|a_n|\right)$$ converges. My closest attempt: Naming the partial sums as $S_n = \sum\limits_{k=1}^n \left(1-|a_n|\right)$ using the general Bernouilli inequality for $-\frac{|a_k|}{n}$ I get that $$\prod\limits_{k=1}^n \left(1-\frac{|a_n|}{n}\right) \geq 1 - \sum\limits_{k=1}^n \frac{|a_n|}{n} $$ Multiplying by $n$ in each side give us $$n \prod\limits_{k=1}^n \left(1-\frac{|a_n|}{n}\right) \geq n - \sum\limits_{k=1}^n |a_n| = \sum\limits_{k=1}^n (1-|a_n|)=S_n$$ Therefore we get that $S_n \leq n \prod\limits_{k=1}^n \left(1-\frac{|a_n|}{n}\right)$ Taking $\log$ both sides give us $$\log{S_n} \leq \log{(n)} + \sum\limits_{k=1}^n \log{\left(1-\frac{|a_n|}{n}\right)}$$ Now using that $\log{(1-x)} \leq -x$ we get $$\log{S_n} \leq \log(n) - \sum\limits_{k=1}^n \frac{|a_n|}{n}$$ And now I can't continue, the problem is I need to use $|b| \leq \prod\limits_{n=1}^{\infty} |a_n|$ somewhere but I couldn't manipulate the series to use that. Any hint on how can I show convergence?","['real-analysis', 'complex-analysis', 'sequences-and-series', 'infinite-product', 'convergence-divergence']"
3113176,Evaluating an indefinite integral $\int \frac{dx}{\sqrt{x^3+2x+3}}$,"Evaluate the following integral \begin{equation}
J = \int \frac{dx}{\sqrt{x^3+2x+3}}
\end{equation} I do not find suitable substitution to compute the above indefinite integral. Since $x^3+2x+3=(x+1)(x^2-x+3)$ , substituting $z=\sqrt{x+1}$ , we have $$J= 2\int \frac{dz}{\sqrt{z^4-3z^2+5}}.$$ I think this is not a good substitution. Any help is appreciated.","['integration', 'indefinite-integrals']"
3113187,Combinatorial proof of $\sum_{k=1}^n k^2 =\binom{n+1}{3} + \binom{n+2}{3}$,"What reason or hint would there be that $$\sum_{k=1}^n k^2 =\binom{n+1}{3} + \binom{n+2}{3}$$ Every combinatoric proof I have seen, seemed quite intuitive with the equation already giving hints to how to prove it. This statement above however does not seem logical. Although algebraically it does work out.
My question specific: What does the left side mean? How would you interpret it combinatorially?","['combinatorics', 'discrete-mathematics']"
3113234,Easy Integration by Parts in Spherical Coordinates,"I am trying to use the integration by parts formula in spherical coordinates $$\int_\Omega \frac{\partial u}{\partial x_i} v d \Omega = \int_\Gamma u v \nu_i d\Gamma-\int_\Omega u \frac{\partial v}{\partial x_i} d \Omega, $$ with $\nu$ a unit vector normal to the boundary $\Gamma$ , but I must be misunderstanding something. Toy problem of integrating over unit ball: $$\int_0^{2 \pi} \int_0^\pi \int_0^1 r \cdot 1 (r^2 \sin \theta dr d\theta d\phi)=\pi. $$ Try integration by parts for the above with $\frac{\partial u}{\partial r}=1$ and $v=r$ . The unit normal to the disk is $\nu=\frac{\partial}{\partial r}$ so $\nu_i=1$ . At the boundary, $r=1$ so I obtain $$2\int_0^{2 \pi} \int_0^\pi \int_0^1 r (r^2 \sin \theta dr d\theta d\phi)=\int_{S^2} dS=4\pi.$$ This does not match what the answer should be! Where did I go wrong in trying to use the formula?","['multivariable-calculus', 'vector-analysis']"
3113251,Extending absolute values on local fields - what is the 'correct' normalization and the relation to the global theory?,"I'm having a tough time figuring out the 'correct' normalization for extending absolute values of local fields. I'm also trying to piece together how this interacts with the global theory, so below is essentially a discussion of my thoughts and a few questions at the end. Am I thinking about this the right way? If $K/\mathbb Q$ is finite then every nonzero ideal $\mathfrak a\subseteq \mathcal O_K$ has a unique factorization $$
\mathfrak a=\prod_{\mathfrak p: \,\text{prime}}\mathfrak p^{e_\mathfrak p(\mathfrak a)}.
$$ The function $v_\mathfrak p:\mathcal O_K\rightarrow \mathbb Z\cup \infty$ defined by $v_\mathfrak p(x)={e_\mathfrak p(x\mathcal O_K)}$ defines a discrete valuation on $\mathcal O_K$ . We can extend this to a valuation on $K$ by noting that all elements of $K$ can be represented by a quotient of algebraic integers. Fixing a prime $p$ and a prime ideal $\mathfrak p\mid p$ , for any $0<a<1$ we have an induced nonarchimedean absolute value on $K$ , given by $|x|_\mathfrak p=a^{v_\mathfrak p(x)}$ . Completing $K$ with respect to this absolute value results in a finite extension $K_\mathfrak p$ of $\mathbb Q_p$ . If $e$ and $f$ are the ramification and inertia degrees of $p$ , by Neukirch Theorem 4.8, I also understand that $K_\mathfrak p$ has a unique absolute value extending the $p$ -adic absolute value $|\cdot |_p$ on $\mathbb Q_p$ , given by $$
|x|_\mathfrak p=|N_{K_\mathfrak p/\mathbb Q_p}(x)|_p^{\frac{1}{ef}}
$$ since $[K_\mathfrak p:\mathbb Q_p]=ef$ . This forces us to fix a value for $a$ above: since we want the absolute value on $K_\mathfrak p$ to extend $|\cdot |_p$ , we'd like to have $|p |_\mathfrak p=p^{-1}$ . Therefore, we need $
a^{v_\mathfrak p(p)}=|N_{K_\mathfrak p/\mathbb Q_p}(p)|^{\frac{1}{ef}},
$ which, since $N_{K_\mathfrak p/\mathbb Q_p}(p)=p^{ef}$ (do I need to assume the extension is Galois here?) and $v_\mathfrak p(p)=e$ , implies that $a=p^{-1/e}$ . Therefore, we normalize so that $$
|x|_\mathfrak p=p^{-v_\mathfrak p(x)/e}
$$ is the absolute value extending that of $\mathbb Q_p$ . All this said, Serre mentions in page 27 of Local Fields, that for a locally compact field (which is the case for $K_\mathfrak p$ ), there is a ""canonical way to choose the number $a$ [defined the same as I have done above]: one takes $a=q^{-1}$ , where $q$ is the number of elements in the reside field"". In our case, I think that $q=|\mathcal O_{K_\mathfrak p}/\mathfrak p|=p^f$ ? If so, then it seems like Serre would have us take $a=p^{-f}$ ... What is the advantage of doing this? Am I making some horrible mistake here? Is there a better way to think about (normalized) valuations on extensions? Is there a way to 'get around' computing the field norm whenever we'd like to take the absolute value of an an element? I realize I've bunched a few questions together here, and I'd be happy to write them separately if someone with more experience on this site would recommend it. That said, partial answers or even just comments would be welcome.","['algebraic-number-theory', 'number-theory', 'p-adic-number-theory', 'local-field', 'valuation-theory']"
3113325,Find equation that minimizes the expected loss - probability,"For fun, I am working through Probability Via Expectation by Peter Whittle. I am currently stuck on one of his exercises. The statement:
A steel billet is trimmed to a length $x$ and then rolled. After rolling, its length becomes $y = \beta x + \epsilon$ , where $\epsilon$ is a r.v. expressing the variability of the rolling. It is then trimmed again to a final length $z$ . If $y$ is greater than $z$ then there is a loss proportional to the excess: $a(y - z)$ . If $y$ is less than $z$ , then the billet must be remelted, and there is a flat loss $b$ . The original trim-length $x$ must now be chosen so as to minimize the expected loss. Show that if $\epsilon$ has the probability density $f(\epsilon)$ , then the equation determining the optimal value of $x$ is $$b f(z - \beta x) = a \int_{z - \beta x} ^ \infty f(\epsilon) d\epsilon$$ Attempt:
Since $y \geq z \iff \beta x + \epsilon \geq z \iff \epsilon \geq z - \beta x$ , we can write the loss function as $$g_x(\epsilon) = \begin{cases}
    a(y-z) \quad & \epsilon \geq z - \beta x \\
    b \quad & \epsilon < z - \beta x
    \end{cases}$$ We want to minimize the expected loss, so we want to minimize $$ \begin{align}E[g_x(\epsilon)] =& a(y - z)P(\epsilon \geq z - \beta x) + b P(\epsilon < z - \beta x) \\
=& a(\beta x + \epsilon - z)P(\epsilon \geq z - \beta x) + b P(\epsilon < z - \beta x) \end{align}$$ From here, I took the derivative with respect to $x$ and set equal to $0$ , but this does not result in the correct equation. Instead, I get $$a P(\epsilon \geq z - \beta x ) - a(\beta x + \epsilon - z) f(z - \beta x) - b f(z - \beta x ) = 0$$ which is a result of using the fundamental theorem of calculus and the product rule. I feel like I'm missing something obvious but I can't figure it out. I'm using $\frac{\partial}{\partial x} P(X \leq g(x) ) = f(g(x))g'(x)$ , which I think is right, but I'm a little rusty.","['expected-value', 'self-learning', 'probability-theory', 'probability']"
3113330,What separation axiom is necessary for existence of neighborhood which closure is a subset of another given neighborhood,"I'm looking for the weakest separation axiom, which gives the following property: Let $A$ be neighborhood of the point $x$ . Then there exists another neighborhood $B$ of $x$ , such $\overline{B}\subset A$ . I thought that $T_{3}$ would suffice, but I was able to get only closed set $C$ such $$x\in C\subset A $$ I could take $B=Int(C)$ , but then I do not see that $B$ will be neighborhood of $x$ , i.e. $x\in B$ . If no separation axiom per se, then in what space this property would hold? The more abstract, the better.","['general-topology', 'separation-axioms']"
3113339,Product rules in Combinatorics: Why do we multiply and not add or divide?,"Simply speaking, my question is as such: why do we use multiplication? For instance, let's suppose I have two elements in set $\{A,B\}$ and want to know how many possible ways there are illustrating them as words with 3 letters, I know this is $2*2*2=8$ , $\{AAA,AAB,ABB,...,BBB\}$ but could someone explain why this works?",['combinatorics']
3113363,Have similar theories like knot theory been developed in higher dimensions?,"Well, my question is kind of basic but I hope it would be taken seriously by the community. Also, I'm very new to this topic and I want to study knot theory in future. Knot theory is the study of embedding $S^{1}$ in $\mathbb{R}^3$ . Right? So, it seems reasonable to consider embedding $S^n$ in $\mathbb{R}^m$ for appropriate $(n,m) \in \mathbb{N}\times\mathbb{N}$ . Are there any well-developed theories for these embeddings? If yes, is there a name for these theories? Are there any references to learn about them? Also, I would be happy to know about some self-contained and good references to learn about knot theory and these higher order theories.","['knot-theory', 'soft-question', 'geometry', 'reference-request']"
3113366,Show that $\cot{142\frac{1}{2}^\circ} = \sqrt2 + \sqrt3 - 2 - \sqrt6$. [duplicate],"This question already has an answer here : Finding the value of cot 142.5° (1 answer) Closed 5 years ago . Show that $\cot{142\frac{1}{2} ^\circ} = \sqrt2 + \sqrt3 - 2 - \sqrt6$ . What I have tried: Let $\theta = 142\frac{1}{2}^\circ \text{ and }  2\theta = 285^\circ$ . $$\cos 285^\circ = \cos 75^\circ$$ $$\cos 75^\circ = \frac{\sqrt3 - 1}{2\sqrt2}$$ $$\cot \theta = \sqrt{\frac{1 + \cos 2\theta}{1 - \cos 2\theta}}$$ From here, pls help me proceed further. Thank you :)",['trigonometry']
3113420,Proof of Bloch's theorem for functions of one complex variable,"I would like to understand how the Schwarz's lemma gives a bound for $|f'(z) - f'(a)|$ in the following theorem,  which is a theorem of Conway's book: $\textbf{1.4 Bloch's Theorem.}$ Let $f$ be an analytic function on a region containing the closure of the disk $D = \{ z \ ; \ |z| < 1 \}$ and satisfying $f(0) = 0$ and $f'(0) = 1$ , then there is a disk $S \subset D$ on which $f$ is one-one and such that $f(S)$ contains a disk of radius $\frac{1}{72}$ . $\textit{Proof.}$ Let $K(r) = \max \{ |f'(z)| \ ; \ |z| = r \}$ and let $h(r) = (1 - r) K(r)$ . It is easy to see that $h: [0,1] \longrightarrow \mathbb{R}$ is continuous, $h(0) = 1$ , $h(1) = 0$ . Let $r_0 = \sup \{ r ; h(r) = 1 \}$ ; then $h(r_0) = 1$ , $r_0 < 1$ and $h(r) < 1$ , if $r > r_0$ . Let $a$ be chosen with $|a| = r_0$ and $|f'(a)| = K(r_0)$ , then $$ \textbf{(1.5)} \ |f'(a)| = (1 - r_0)^{-1}.$$ Now if $|z - a| = \frac{1}{2} (1 - r_0) = \rho_0$ , $|z| < \frac{1}{2} (1 + r_0)$ ; since $r_0 < \frac{1}{2} (1 + r_0)$ , the definition of $r_0$ gives $\textbf{(1.6)} |f'(z)| \leq K(\frac{1}{2} (1 + r_0)) = h(\frac{1}{2} (1 + r_0)) [ 1 - \frac{1}{2} (1 + r_0) ]^{-1} < [ 1 - \frac{1}{2} (1 + r_0) ]^{-1} = \frac{1}{\rho_0}$ for $|z - a| < \rho_0$ . Combining $(1.5)$ and $(1.6)$ gives $$|f'(z) - f'(a)| \leq |f'(z)| + |f'(a)| < \frac{3}{2} \rho_0.$$ According to Schwarz's Lemma , this implies that $$|f'(z) - f'(a)| < \frac{3 |z - a|}{2 \rho_0^2}$$ for $z \in B(a;\rho_0)$ . Hence if $z \in S = B(a; \frac{\rho_0}{3})$ , $$|f'(z) - f'(a)| < \frac{1}{2\rho_0} = |f'(a)|$$ By lemma $1.4$ , $f$ is one-one on $S$ . It remains to show that $f(S)$ contains a disk of radius $\frac{1}{72}$ . For this define $g: B(0;\frac{\rho_0}{3}) \longrightarrow \mathbb{C}$ by $g(z) = f(z+a) - f(a)$ , then $g(0) = 0$ , $|g'(0)| = |f'(a)| = (2 \rho_0)^{-1}$ . If $z \in B(0; \frac{\rho_0}{3})$ , then the line segment $\gamma = [a,a+z]$ lies in $S \subset B(a;\rho_0)$ . So by (1.6) $$|g(z)| = |\int_{\gamma} f'(w) dw | \leq \frac{1}{\rho_0} |z| < \frac{1}{3}.$$ Applying lemma $1.2$ gives that $$g(B(0;\frac{\rho_0}{3}) \supset B(0, \sigma)$$ where $$\sigma = \frac{\left( \frac{\rho_0}{3} \right)^2 \left( \frac{1}{2\rho_0} \right)^2}{6 \left( \frac{1}{3} \right)} = \frac{1}{72}$$ If this is translated into a statement about $f$ , it yields that $$f(S) \supset B \left( f(a); \frac{1}{72} \right). \square$$","['complex-analysis', 'proof-explanation']"
3113422,Sum of 5 square roots equals another square root. What is the minimum possible value of the summed square root?,"In the equation $\sqrt{a}+\sqrt{b}+\sqrt{c}+\sqrt{d}+\sqrt{e}=\sqrt{f}$ , each variable is a distinct positive integer. What is the least possible value for $f?$ Out of purely trial and error, I have come to the solution of a = 1, b= 4, c= 9, d = 16, e = 25 which sums to 15 thus $\sqrt{f} = 15$ and $f = 225$ . I suspect the final answer will turn out to be $f = 225 = 15^2$ , but I have not yet managed to come up with a rigorous proof that no smaller value of $f$ is possible. The main source of inspiration I've been trying work from so far is to consider a simpler variation of this problem where you only have two square roots on the left hand side, $$\sqrt{a}+\sqrt{b} = \sqrt{f}.$$ In this variation of the problem, we can square both sides to find that $$a+b+2\sqrt{ab} = f.$$ Therefore, $$2\sqrt{ab} = f-a-b,$$ consequently, $$4ab = (f-a-b)^2.$$ Since $a,b,f$ are integers and the left hand side ( $4ab$ ) is an integer that is divisible by $2$ , we see that the product $(f-a-b)^2 = (f-a-b)\cdot (f-a-b)$ is divisible by $2$ . A product of integers is only divisible by a prime $p$ if and only if at least one of the factors is divisible by $p$ . (This is a simple consequence of the Fundamental Theorem of Arithmetic, which is just the official name for the fact that prime factorizations of integers are unique.) Therefore, the integer factor $f-a-b$ itself must be divisible by $2$ . In other words, we have deduced that $k = \frac{f-a-b}{2}$ must be an integer. Thus we see that $$ab = k^2$$ is the square of an integer. Therefore, as a consequence of the Fundamental Theorem of Arithmetic (which is just the official name for the fact that prime factorizations of integers are unique), it must be the case that there exist integers $\alpha, \beta,\gamma$ such that $a = \alpha^2 \gamma$ and $b = \beta^2 \gamma,$ so \begin{align*}f &= a+b+2\sqrt{ab}\\
&= \alpha^2\gamma + \beta^2\gamma+2\sqrt{\alpha^2\beta^2\gamma^2}\\
&= (\alpha^2+\beta^2+2\alpha\beta)\gamma\\
&= (\alpha^2+\beta^2)\gamma\end{align*} Thus, in this simpler version of the problem where there is only a sum of two square roots, clearly the smallest possible value of $f$ comes from taking $\gamma = 1, \alpha = 1, \beta = 2$ which gives the solution $$\sqrt{1}+\sqrt{4} = \sqrt{9}.$$ I've been looking for ways to then bootstrap this argument (or something like it) up to something that can be applied to solving the original problem with a sum of five square roots. So far, I haven't found the right path forward. I have also been looking for ways to exploit some routine square root inequalities. In particular, for any positive numbers $a_1, a_2,\dots,a_n$ , it is the case that $$\sqrt{a_1+a_2+\dots+a_n} < \sqrt{a_1}+\sqrt{a_2}+\dots+\sqrt{a_n} \le \sqrt{n} \sqrt{a_1+a_2+\dots+a_n}.$$ This also has not yet led to progress. I am consequently stuck here with no ideas how to continue. Your help is appreciated! Also, can you also help me on this ( $N$'s base-5 and base-6 representations, treated as base-10, yield sum $S$. For which $N$ are $S$'s rightmost two digits the same as $2N$'s? ) problem too? Thanks! Max0815","['radicals', 'linear-algebra']"
3113437,show that the number $n = 11$ is not a sum of $4$'s and $5$'s,"I don't know how to get started with this question. $11 = 4(x) + 5(y)$ What I've tried is : $x = 0, y = 2$ $11 = 4(0) + 5(2)$ $11 = 10$ $11$ is not equal to $10$ . Wouldn't that be my base case? What more could I do to solve this question? If i needed to apply induction to this question, how would I do it? Any help is greatly appreciated.",['discrete-mathematics']
3113495,Understanding why a limit proof using another limit works,"Sorry for the title, hopefully I can explain it better. I think the title is about as good as I could get in terms of description. I have a problem: Let $x_n \ge 0$ for all $ N \in \mathbb{N}$ If $(x_n) \to x$ , show that $(\sqrt{x_n}) \to \sqrt(x)$ Assume that we have already proved the limit going to zero. My proof was as follows: Our goal is to find an $N$ that satisfies the inequality: $|\sqrt{x_n|} - \sqrt{x}| \lt \epsilon$ with epsilon being arbitrary. So: $|\sqrt{x_n|} - \sqrt{x}| \lt \epsilon$ $|\sqrt{x_n|}| \lt \epsilon + \sqrt{x}$ $|\sqrt{x_n|}|^2 \lt (\epsilon + \sqrt{x})^2$ $|x_n| \lt (\epsilon + \sqrt{x})^2$ $|x_n| \lt \epsilon^2 + 2 \epsilon \sqrt{x} + x$ $|x_n - x| \lt \epsilon^2 + 2 \epsilon \sqrt{x}$ Since we already know $|x_n - x|$ can be made arbitrarily small we are ready to proceed. Then allow $\epsilon > 0$ to be arbitrary and choose an $N \in \mathbb{N}$ satisfying: $|x_n - x| \lt \epsilon^2 + 2 \epsilon \sqrt{x}$ For $n \ge N$ we find after some algebra (to save typing the above backwards) $|\sqrt{x_n|} - \sqrt{x}| \lt \epsilon$ Which shows that given the limit we can choose an $N$ for any given $\epsilon$ and find that all $n \ge N$ will be inside the $\epsilon$ -neighborhood of $\sqrt{x}$ . Where I am confused is how I am using the given limit $(x_n) \to x$ . I am sort of following a template here from the author. Adding this extra limit has confused me. How does reducing the inequality $|\sqrt{x_n|} - \sqrt{x}| \lt \epsilon$ to $|x_n - x| \lt \epsilon^2 + 2 \epsilon \sqrt{x}$ and then knowing ""we can make it arbitrarily small"" help us prove the given limit? What is the intuition?","['limits', 'epsilon-delta', 'real-analysis']"
3113496,How to handle little $o$ in the central limit theorem,"I am having some trouble understanding a couple of lines in the proof of the central limit theorem using characteristic functions: https://en.wikipedia.org/wiki/Central_limit_theorem#Proof_of_classical_CLT I'm not sure how $(1-\frac{t^2}{2n} + o(\frac{t^2}{n}))^n \to e^\frac{-t^2}{2}$ as $n \to \infty$ . Originally I proved (by induction) a lemma that for complex numbers $w_k$ and $z_k$ if $|w_k|\leq 1$ , $|z_k|\leq 1$ then $$|\prod_k{w_k}-\prod_k {z_k}| \le \sum_k{|z_k-w_k|}$$ I tried to use this to show that $(1-\frac{t^2}{2n} + o(\frac{t^2}{n}))^n - (1-\frac{t^2}{2n})^n \to 0$ as $n\to \infty$ . But I got stuck. Instead,  I've taken the logarithm of the characteristic function (see the link) and then $$n\ln[1-\frac{-t^2}{2n} + o(\frac{t^2}{n})] \stackrel{?}{=} n[\frac{-t^2}{2n} + o(\frac{t^2}{n})] = \frac{-t^2}{2} + o(1) \to \frac{-t^2}{2}$$ However, I'm not sure how to get the first equality, indicated by $?$ Any help would be greatly appreciated. Thanks in advance.","['calculus', 'central-limit-theorem', 'probability-theory', 'asymptotics']"
3113510,Prove that each integer n ≥ 12 is a sum of 4's and 5's using strong induction,"So I've been given the following problem:
Prove that each integer n ≥ 12 is a sum of 4's and 5's 
What I have so far:
(Basis): n ≥ 12 
  Therefore, 12 ≤ 4(x) + 5(y) x = 3 | y = 0 12 ≤  4(3) + 5(0) 12 ≤ 12 = Correct However, what I don't understand is how would I use the x & y variable in induction step. Where exactly would I place these? and how would I go on to solve this?",['discrete-mathematics']
3113528,Evaluating integrals in the paper Auto-Encoding Variational Bayes,"This is the first time that I'm asking on this site so apologies in advance if it's not quite the usual standard. I'm going thorough the paper Auto-Encoding Variational Bayes https://arxiv.org/abs/1312.6114 and in page 10 appendix B there is a simple enough looking equation that I don't fully understand how they got the results they have. $$\int q_{\phi}(\mathbf{z})log\,p(\mathbf{z})d\mathbf{z} = ... $$ when I expand $log\,p(\mathbf{z})$ I get $$log\,p(\mathbf{z}) = log\, \mathcal{N}(\mathbf{z}; \mathbf{0}, \mathbf{I}) = 
 log\, det(2\pi I)^{-1/2}\,e^{-1/2 \,\mathbf{z}'I\mathbf{z}} = \frac{-J}{2}\, log \,2\pi \, -\frac{1}{2}\left\lVert \mathbf{z} \right\rVert ^{2}$$ where $J$ is the dimension of $\mathbf{z}$ . From there it is easy to see that the term $\int q_{\phi}(\mathbf{z})\frac{-J}{2}\, log \,2\pi\, d\mathbf{z}$ evaluates to $\frac{-J}{2}\, log \,2\pi$ given that the integral over a density function evaluates to 1 which is in agreement with the paper. However I'm not sure how to proceed with the second part $\int q_{\phi}(\mathbf{z})\frac{-J}{2}\, \left\lVert \mathbf{z} \right\rVert ^{2} d\mathbf{z}$ . I'll really appreciate if anyone that knows could please explain what is it that I'm misunderstanding.
Thanks.","['integration', 'probability']"
3113559,"A checkboard and sums problem, prove that there is a column or row such that the absolute value of sum doesn't exceed $2018^2/2$","Now we have a $2018\times 2018$ checkboard. Each space is filled with one integer with absolute value not bigger than $2018$ . Suppose the sum of all the numbers is $0$ , prove that there is a column or a row such that the sum of $2018$ integers in that column or row is with absolute value not bigger than $2018^2/2$ . My attempt: I let the sum of each row be $R_i$ with $i=1,2,\cdots,2018$ , and let $C_j$ the same for column sum. Then I have $$\begin{cases}R_1+\cdots+R_{2018}=0\\C_1+\cdots+C_{2018}=0\end{cases}$$ Since each number does not have absolute value bigger than $2018$ , I conclude that $$|R_i|\leq 2018^2,\qquad |C_j|\leq 2018^2.$$ But the problem want me to collapse to half of the bound. Any suggestion?","['combinatorics', 'discrete-mathematics']"
3113642,In any triangle is $\sin A+\sin B+\sin C=\frac{3\sqrt3}{2}$ always,"Well, I came with an interesting proof. But I just want to verify it $$\begin{array}{l}
{\text { Applied at function } y=\sin x,} \\
 {\text { We have, } \sin \left(\frac{A+B+C}{3}\right) \geq \frac{\sin A+\sin B+\sin C}{3}}
\end{array}$$ From here we will get $$\sin A+\sin B+\sin C\leq \frac{3\sqrt3}{2}.$$ $$\begin{array}{l}
{\text { Also by A.M. } \geq \text { G.M. in an acute angled triangle }} \\ 
{\frac{\sin A+\sin B+\sin C}{3} \geq \sqrt[3]{\sin A \sin B \sin C}} \\ 
{\Rightarrow \sin A+\sin B+\sin C \geq 3(\sqrt[3]{\sin A \sin B \sin C})} \\ 
{\Rightarrow \sin A+\sin B+\sin C \geq 3\left(\frac{\sqrt{3}}{2}\right)=\frac{3 \sqrt{3}}{2}>2}
\end{array}$$ and from this I get $$\sin A+\sin B+\sin C\geq \frac{3\sqrt3}{2}.$$ Now the equation to be satisfied,
only equality condition should hold. So in an acute angled triangle $$\sin A+\sin B+\sin C=\frac{3\sqrt3}{2}.$$ Is there any fallacy in this convention.","['inequality', 'a.m.-g.m.-inequality', 'calculus', 'functions', 'trigonometry']"
3113643,"Intuition behind a counterexample to $|A+A|\leq |A-A|$, where $A$ is a finite set","Define $$A+A=\{a+b:a,b \in A\}, A-A = \{a-b:a,b \in A\}$$ Then prove or disprove the following $$|A+A|\leq |A-A|$$ Intuitively, it should be true, as $$a+b=b+a$$ $$a-b \neq b-a$$ However, I accidentally discovered the following counterexample by Conway proposed in 1969 $$A=\{1,2,3,5,8,9,13,15,16\}$$ where $$A+A=\{2,3,...,32\}-\{27\}, |A+A|=30$$ $$A-A=\{-15,-14,...,15\}-\{\pm 9\}, |A-A|=29$$ Incredible! How did he come up with this? (Of course, I can do it using a computer program...)","['intuition', 'combinatorics', 'examples-counterexamples']"
3113665,Translation from Dutch [closed],"Closed. This question is off-topic . It is not currently accepting answers. Closed 5 years ago . This question is not about mathematics, within the scope defined in the help center . Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Improve this question Google Translator cannot help me with this translation. May you translate it?","['translation-request', 'analysis']"
3113682,Poisson point process in 2D with reflecting boundaries,"Consider a point process $\{(X_n,T_n)\}$ on a plane $[0,1/\lambda]\times\mathbf R^+$ , generated from a Poisson point process $\{T_n\}$ with rate $\lambda$ on $\mathbf R^+$ (i.e. $(T_n-T_{n-1})$ is iid exponentially distributed $\sim\lambda\exp(-\lambda T)$ ) in the following way: $X_0=T_0=0$ and the boundaries at $0$ and $1/\lambda$ are reflecting. The $X_n$ step direction is unchanged until hitting the boundary (or one can consider unbiased random walk alternatively), The step size is given by $X_n = T_n-T_{n-1}$ , or $X_n$ is uniformly distributed on $[0,T_n-T_{n-1}]$ , or $X_n$ is exponentially distributed with mean $T_n-T_{n-1}$ Q: Is this 2D point process Poissonian in each of the three cases? If so, is it homogeneous?","['stochastic-processes', 'probability-theory', 'poisson-process']"
3113707,Sum of square of binomial coeffcient with positive and negative terms,Finding $\displaystyle \binom{2n}{1}^2-2\binom{2n}{2}^2+3\binom{2n}{3}^2-\cdots \cdots -2n\binom{2n}{2n}^2.$ What I've tried: $$(1-x)^{2n}=\binom{2n}{0}-\binom{2n}{1}x+\binom{2n}{2}x^2+\cdots \cdots +\binom{2n}{2n}x^{2n}$$ $$-2n(1-x)^{2n-1}=-\binom{2n}{1}+2\binom{2n}{2}x-3\binom{2n}{3}x^2+\cdots +n\binom{2n}{2n}x^{2n-1}$$ Sum notation: $$\sum_{k=0}^{2n} (-1)^{k-1}k\binom{2n}{k}^2$$,['sequences-and-series']
3113711,Why does any function get thinner as $x$ is multiplied by a constant?,"Example: $$\cos(x)$$ $$\cos(8x)$$ ""Thinner"" might not be the correct term. But I just want to know why does changing $x$ to $8x$ make it look like that?","['constants', 'trigonometry', 'functions']"
3113749,Functional analysis on manifolds,"The basic object of functional analysis is the topological vector space, so vector spaces with some topology, we can add additional structure by introducing metrics etc, but the underlying object is a linear space anyway. I was wondering if there's any field of math that still studies functionals, but defined on manifolds instead of vector spaces.","['soft-question', 'functional-analysis', 'differential-geometry']"
3113769,Prove that integral of infinite sum is in $L^1$,"The function $g$ is a composition of functions as follows ( $\chi_A$ is the characteristic function of the set $A$ ): $$f(x) = \frac{1}{\sqrt{x}} \chi_{[0,1]}(x)$$ For an enumeration of the rationals $\{r_i\}$ : $$g(x) = \sum_{n=1}^\infty 2^{-n}f(x-r_n)$$ Prove that $g \in L^1(m)$ where $m$ is the Lebesgue measure for $m$ almost everywhere on $x$ . Prove that $g$ is unbounded on every open interval $I \in [0,1]$ and that $\forall I, \forall N \in \mathbb{N} \Rightarrow m(\{x\in I : g(x)>N\}) > 0$ Putting everything in place, we have $$\int \sum_{n=1}^\infty 2^{-n}\frac{\chi_{[0,1](x-r_n)}}{\sqrt{x-r_n}} dm$$ Analyzing the parts of this function, we see that $2^{-n} \geq 0; \chi \geq 0$ I don't know how to show that with the denominator this is a function in $L^+$ . I know that for being in $L^1$ I am interested in $|f|$ but I am still unsure if I should and how to proceed about this. If I could show that, then we can swap the sum and the integral and get $$\sum_{n=1}^\infty \int 2^{-n}\frac{\chi_{[0,1](x-r_n)}}{\sqrt{x-r_n}} dm$$ where $$\int 2^{-n}\frac{\chi_{[0,1](x-r_n)}}{\sqrt{x-r_n}} dm \leq \int 2^{-n}\frac{1}{\sqrt{x-r_n}} dm \leq \int \frac{1}{\sqrt{x-r_n}}dm$$ Do I proceed with some sort of limit theorem or keep trying to prove that this is less than some other function?","['integration', 'measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
3113780,Conditional Probability Problem about identical twins,"Assume that $\frac{1}{3}$ of all twins are identical twins. You learn that Miranda is expecting twins, but you have no other information. a) Find the probability that Miranda will have two girls. b) You learn that Miranda gave birth to two girls. What is the probability that the girls are identical twins? For a), we have, letting $A$ be the event that Miranda will have two girls, and assuming that the probability a child is a girl at birth is $\frac{1}{2},$ we have $P(A)=\frac{1}{2}\cdot \frac{1}{2}=\frac{1}{4}.$ For b), letting $B$ be the event that the girls are identical twins, then we find that $P(A\cap B)=P(B|A)\cdot P(A)=\frac{1}{3}\cdot \frac{1}{4}=\frac{1}{12}.$ Is the above reasoning correct? I think that I have assumed that $\frac{1}{3}$ of twins which are girls are identical twins. Would that be incorrect, since it is also stated to explain any assumptions one makes? Thank you for your time and appreciate the feedback.","['proof-verification', 'probability']"
3113834,"Does $\triangle ABC$ exist such that $\triangle ABC \sim \triangle DEF$, with $D, E, F$ being the incentre, centroid, orthocentre of $\triangle ABC$?","Question: Does $\triangle ABC$ exist such that $\triangle ABC \sim \triangle DEF$ , with $D, E, F$ being the incentre, centroid, orthocentre of $\triangle ABC$ , resp.? For such a triangle to exist, it must be obtuse. Besides that, I have no idea how to prove or disprove it. For the case of $D, E, F$ being the orthocentre, centroid, circumcentre, it's impossible as they lie on the same line (Euler's line). That's the motivation of the problem. I have a feeling that brute force methods are needed (coordinate geometry). But I hate such an ugly approach. Any idea?","['euclidean-geometry', 'triangles', 'geometry']"
3113847,Is there any non zero matrix whose adjoint is a zero matrix,"Just wanted to know whether their exits a non zero matrix whose adjoint is a zero matrix. And if so what would be inverse of a matrix whose adjoint as well as determinant is zero, as $$A^{-1}=\dfrac{1}{ |A|} adj(A)$$","['matrices', 'determinant']"
3113885,What did I do wrong while trying to solve this integral?,"So the question is $$\int x^3\ln(x+1)\,dx$$ and I did it this way: $$= {1\over 4}\ln(x+1)x^4 - {1\over 4}\int {x^4\over x+1}\,dx$$ $$= {1\over 4}\ln(x+1)x^4 - {1\over 4}\int {x^4-1+1\over x+1}\,dx$$ $$= {1\over 4}\ln(x+1)x^4 - {1\over 4}\int {(x^2 -1)(x^2+1)\over x+1}\,dx - {1\over 4}\int{1\over x+1}\,dx$$ $$= {1\over 4}\ln(x+1)x^4 - {1\over 4}\int (x -1)(x^2+1)\,dx - {1\over 4}\int{1\over x+1}\,dx$$ $$= {1\over 4}\ln(x+1)x^4 - {1\over 4}\int x^3\,dx - {1\over 4}\int x\,dx + {1\over 4}\int x^2\,dx +{1\over 4}\int dx - {1\over 4}\int {1\over x+1}\,dx$$ $$= {1\over 4}\ln(x+1)x^4 - {x^4\over 16} - {x^2\over 8} + {x^3\over 12} + {x\over 4} - {1\over 4}\ln(x+1) + c$$ I looked the solution up on another website, and although it had a similar answer, when I graphed both of the equations I found that they were not equal and a small constant of: $${25\over 48}$$ was the difference between the two equations. Is this small constant lost in c? If so is my solution still right? If not, why? Sorry if I missed something obvious :P",['integration']
3113894,A formula for tan(2x),"Help with solving... Suppose that $\tan^2x=\tan(x-a)·\tan(x-b)$ , show that $$\tan(2x)=\frac{2\sin(a)·\sin(b)}{\sin(a+b)}$$ As far as I know, so far the $\tan2x$ can be converted to $\frac{2\tan x}{1-\tan^2x}$ using the double angle formula and the $\tan 2x$ can be further be substituted to the following given item ( $\tan(x-a·)\tan(x-b)$ ) however the problem now is that there are no ways to simplify this to my knowledge...",['trigonometry']
3113932,Fix $0\leq\delta\leq1.$ Bob rolls a die repeatedly in the hopes of rolling a six.,"Fix a parameter $0\leq\delta\leq1.$ Bob rolls a die repeatedly in the hopes of rolling a six. However, after each failure to roll a six he gives up with probability $1-\delta$ and decides to try again with probability $\delta$ . What is the probability that Bob will never roll a six? Let $A$ denote the event that Bob does not roll a six, and let $B$ be the event that he gives up after a failure. Then $P(A\cap B)=1-\delta$ and $P(A\cap B^{C})=\delta.$ Now after the first roll, the probability that Bob did not roll a six is $5/6$ . I am having difficulty with understanding how the parameter $\delta$ comes into the calculation of the probability that Bob does not get a six given that he failed and tried again. Could you please provide a hint, no solutions please, just a hint on how to start thinking about this kind of problem. Thank you for time, I appreciate any feedback.","['dice', 'probability']"
3113938,Show an identity involving a sum of arctangents of algebraic expressions [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question Show that $$
2\tan^{-1}\frac{\sqrt{x^2+a^2} - x + b}{\sqrt{a^2-b^2}} + \tan^{-1}\frac{x\sqrt{a^2-b^2}}{b\sqrt{x^2+a^2} + a^2} + \tan^{-1}\frac{\sqrt{a^2-b^2}}{b} = n\pi .
$$ I tried using $$ x= a \tan \theta ,\; b= a \sin\phi,$$ but then calculations are not working out; that is,
I am not able to further simplify.",['trigonometry']
3114030,Soft question: why define smooth manifolds intrinsically?,"A possibly naive question, but one I've been grappling with since starting Riemannian geometry. I have done some searching and I cannot find this question asked, yet it feels like a very natural one so I do apologise if I've missed it. Why do we introduce the notion of a smooth manifold as intrinsic (locally Euclidean) when we can investigate Riemannian manifolds extrinsically as embedded in some higher dimensional Euclidean space (by virtue of the embedding theorems)? My reason for asking this question is not because I find this intrinsic approach unsatisfying, on the contrary my reason for asking is rather because I want to convey my fascination for its beauty to my friends and yet I very much struggle to give a cause-and-effect practical answer as to why we need, say, atlases and charts to tackle questions like curvature of a hyper-surface. I am wanting to say something along the lines of ""well we cannot study the surface properly unless we exist on it as a kind of citizen of the surface without reference to anything else"", but as of right now I cannot justify this statement at all (and maybe I shouldn't even be able to!).","['general-topology', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
3114035,Stuck at proving whether the sequence is convergent or not,"I have been trying to determine whether the following sequence is convergent or not. This is what I got: Exercise 1 : Find the $\min,\max,\sup,\inf, \liminf,\limsup$ and determine whether the sequence is convergent or not: $X_n=\sin\frac{n\pi}{3}-4\cos\frac{n\pi}{3}$ I wrote down a few cases: $X_1 = \sin\frac{\pi}{3}-4\cos\frac{\pi}{3}= \frac{\sqrt3-4}{2} $ $X_2 = \sin\frac{2\pi}{3}-4\cos\frac{2\pi}{3}= \frac{\sqrt3+4}{2} $ $X_3 = \sin\frac{3\pi}{3}-4\cos\frac{3\pi}{3}=4 $ $X_4 = \sin\frac{4\pi}{3}-4\cos\frac{4\pi}{3}= \frac{4-\sqrt3}{2} $ $X_5 = \sin\frac{5\pi}{3}-4\cos\frac{5\pi}{3}= \frac{4-\sqrt3}{2} $ $X_6 = \sin\frac{6\pi}{3}-4\cos\frac{6\pi}{3}= -4 $ So as found above, $\min = -4$ , $\max = 4$ , $\inf = -4$ , $\sup = 4$ , $\liminf = -4$ , $\limsup = 4$ . Let's check whether its convergent or not:
The sequence is bounded as stated above so lets check if its decreasing or increasing. $X_n \geq X_{n+1}$ $\sin\frac{n\pi}{3}-4\cos\frac{n\pi}{3} \geq\sin\frac{(n+1)\pi}{3}-4\cos\frac{(n+1)\pi}{3}$ $\sin\frac{n\pi}{3} - \sin\frac{(n+1)\pi}{3} \geq  -4\cos\frac{(n+1)\pi}{3} + 4\cos\frac{n\pi}{3}$ I used trigonometrical identity for $\sin\alpha+\sin\beta$ and $\cos\alpha-\cos\beta$ : $-\cos\frac{\pi(2n+1)}{6} \geq 4\sin\frac{\pi(2n+1)}{6}$ What should I do next? I am stuck here.
Thanks, and sorry if I made mistakes.","['limsup-and-liminf', 'real-analysis', 'trigonometry', 'sequences-and-series', 'convergence-divergence']"
3114092,"In $\triangle ABC$, $AD$ $\perp$ $BC$ and $GE$ is the extended line of $DG$ where $G$ is centroid. Prove that $GD$ = $\frac{EG}{2}$","Let $ABC$ be a triangle and in $\triangle ABC$ , $AD$ $\perp$ $BC$ and three median lines intersect at point $G$ where $G$ is the centroid of $\triangle ABC$ . The extension of $DG$ intersects the circumcircle of $\triangle ABC$ at point $E$ . Prove that $$GD = \frac{EG}{2}$$ I found this as an isolated problem. My attempt: Nothing speciality I discovered from the diagram. I only connected segment $AE$ and drew $GI$ , where $GI$ $\perp$ $AD$ . From the above diagra, $G$ is the centroid. So, $\frac{AG}{GF}$ = $\frac{1}{2}$ . And then from right angled triangle $\triangle AGI$ and $\triangle ADF$ , We get $AI:ID$ = $1:2$ (as $\triangle AGI$ $\sim$ $\triangle ADF$ ). Right then, if $\triangle ADE$ can be showed as a right angled triangle ( $\angle EAD$ = 90 $^\circ$ ) and $\triangle ADE$ $\sim$ $\triangle GID$ , we can also likewise show that $\frac{DG}{GE}$ = $\frac{1}{2}$ . But my reverse effort went into vain. I can't anyhow show that $\angle EAD$ = 90 $^\circ$ . So, how to solve for that case? SOURCE: BANGLADESH MATH OLYMPIAD Can it be solved by vector? Thanks in advance.","['contest-math', 'euclidean-geometry', 'homothety', 'geometry']"
3114107,Find the power of the quotient set of this relation,"In the set $\mathbb Z^{\mathbb N}$ of all infinite sequences with integers words we define the relation of equivalence: $f\equiv g \Leftrightarrow((f(2n)=g(2n))  \wedge(f(n)\cdot g(n)>0 \vee f(n)=0=g(n)))$ for $f,g \in \mathbb Z^{\mathbb N}$ . Find the power of the quotient set of this relation. My try: Let $|A|$ - the power of the quotient set of this relation. I know that $A \subseteq Z^{\mathbb N} \Rightarrow |A| \le |\mathbb Z^{\mathbb N}|=continuum$ . If I show $|A| \ge continuum$ , from Schröder–Bernstein theorem  I will know that $|A|=continuum$ . I can find a function $X \rightarrow A$ which is $1-1$ and $|X|=continuum$ to show that $|A| \ge continuum$ but it is difficult for me and I have other idea but I don't know if this is correct: Let $B=\left\{f: \mathbb N  \rightarrow \left\{ -1,1\right\}: f(2n)=g(2n) \right\}$ . $B\subseteq A$ so $|A|\ge|B|=continuum$ Can you check the correctness of my idea?",['elementary-set-theory']
3114116,"$\Sigma$-equivalence between $(y,z,1)$ and $(y+\mathcal{O}(2),z+\mathcal{O}(2),1)$","Consider the sets $\mathfrak{X}(\mathbb{R}^3) = \{X: \mathbb{R}^3 \to \mathbb{R}^3; X \mbox{ is smooth}\}$ and $\Sigma = \{0\}\times\mathbb{R}^2$ . Let $X, Y$ be vector fields in $\mathfrak{X}(\mathbb{\mathbb{R}}^3)$ , such that $$X(x,y,z) = (y,z,1) $$ and $$Y(x,y,z) = (y+ \mathcal{O}_1(2),z+\mathcal{O}_2(2),1), $$ where $\mathcal{O}_i(3):\mathbb{R}^3 \to \mathbb{R}$ is a smooth function satisfying $$\exists \ r_i>0, \exists \ K_i>0; \ |\mathcal{O}_i(2)(x,y,z)|\leq K_i\|(x,y,z)\|^2,\ \forall\  (x,y,z) \in B_{r_i}(0,0,0). $$ Definition: Let $Z_1,Z_2$ $\in$ $\mathfrak{X}{(\mathbb{R}^3)}$ , we say that $Z_1,Z_2$ are $\Sigma$ -equivalent at the origin if there exists a homeomorphism $h:U_0\to V_0$ , where $U_0$ and $V_0$ are neighborhoods of the origin such that $h(U_0\cap \Sigma) = V_0 \cap \Sigma$ $h$ maps the orbits of $Z_1$ into orbits of $Z_2$ preserving the orientation of the orbits. My question: Question: Are $X, Y$ (as defined at the beginning of the text) $\Sigma$ -equivalent at the origin? Just for the records, these orbits are in respect of the ODEs $$(\dot{x},\dot{y},\dot{z})= X(x,y,z) $$ and $$(\dot{x},\dot{y},\dot{z})= Y(x,y,z). $$ Moreover, we can assume that $$Y(x,0,z) = (0,z+ \mathcal{O(2)}, 1)\  \text{and} \   Y(x,y,0) = (y+\mathcal{O}(2),0,1).$$ It seems true, however, I don't have many ideas of how construct such $h$ , can anyone help me?","['ordinary-differential-equations', 'dynamical-systems']"
3114122,Determinant of a particular matrix.,"What is the best way to find determinant of the following matrix? $$A=\left(\begin{matrix}
1&ax&a^2+x^2\\1&ay&a^2+y^2\\ 1&az&a^2+z^2
\end{matrix}\right)$$ I thought it looks like a Vandermonde matrix, but not exactly. I can't use $|A+B|=|A|+|B|$ to form a Vandermonde matrix. Please suggest. Thanks.","['matrices', 'determinant', 'linear-algebra']"
3114127,Finding the closed form for $x_n^2 = -x_{n-1}^2+6x_{n-2}^2+n$,"I'm trying to find a closed form for the recurrence relation $x_n^2 = -x_{n-1}^2+6x_{n-2}^2+n$ , with $x_1 = \frac{1}{4}, x_2=\frac{\sqrt{13}}{4}$ and $x_i \in \mathbb R^+$ . 
My attempt was to let $z_n=x_n^2$ and then transform the recurrence relation to a 2nd order non-homogeneous linear recurrence relation. Substitution gives: $$z_n = -z_{n-1}+6z_{n-2}$$ . Now the above recurrence relation has complementary function $$z'_n = L2^n +K(-3)^n$$ for $L,K$ arbitrary constants. 
I also found that the new recurrence relation has a particular solution given by $$z''_n = \frac{11-n}{4}$$ . Thus the general solution is $$z_n = L2^n +K(-3)^n + \frac{11-n}{4}$$ By substitution I then found $L=\frac{-7}{8}, K = \frac{11}{48}$ . So the unique solution for our recurrence relation is: $$z_n = \frac{-7}{8}2^n +\frac{11}{48}(-3)^n + \frac{11-n}{4}$$ Now, for $n=3$ I obtain a negative value of $z_n$ indicating that my solution is invalid. I've checked my solution numerous times and I'm pretty sure my numbers are right so I'm wondering if my method here is wrong.","['recurrence-relations', 'closed-form', 'discrete-mathematics']"
3114128,About the tangent cone and tangent space of an affine variety,"Let $X\subset \mathbb{A}^n$ be an affine variety then $X= Z(I)$ is the zero locus of the ideal $I$ . In general the tangent cone at $0$ is define as $TC= Z(I^{in})$ where $I^{in}$ is the initial ideal and $TS= Z(I^{lin})$ is the tangent space.
If $I=(f)$ then it's easy compute tangent space and cone in fact $I^{in}=(f^{in})$ and $I^{lin}= (f^{lin})$ . My question is : if the Ideal $I$ is not principal there is a general way to compute the initial ideal and the linearization. Because for $TS$ in general I can use the definition $TS= \sum_i \frac{\partial f_j}{\partial x_i}f(p)(x_i-p)=0$ but for the tangent cone? If the ideal I is homogeneus $I^{in}=I$ ? And if it is homogeneus of degree >1 $I^{lin}=0$ ? Thanks for the clarifications!","['affine-varieties', 'algebraic-geometry', 'tangent-spaces']"
3114144,"Understanding the difference between two theorems related to $|G\,:\,K|=|H\,:\,K|\cdot|G\,:\,H|$. [duplicate]","This question already has answers here : Finite Index of Subgroup of Subgroup (3 answers) Closed 2 years ago . I came across with the following two theorems: First theorem: Let $G$ be a finite group so $K\leq H\leq G$ . Then $|G\,:\,K|=|H\,:\,K|\cdot|G\,:\,H|$ . Second theorem: Let $G$ be a group so $K\leq H\leq G$ when both $H$ and $K$ have finite index. Then $|G\,:\,K|=|H\,:\,K|\cdot|G\,:\,H|$ . How the second theorem is different then the first one? If $G$ is finite then $H,K$ are finite and they have finite indexes. Is the second theorem is ""sub-theorem"" of the first one? I proved the first one with Lagrange: $$ \begin{cases}
|G|=|H|\cdot|G\,:\,H|\\
|H|=|K|\cdot|H\,:\,K|\\
|G|=|K|\cdot|G\,:\,K|
\end{cases}\Rightarrow|G\,:\,K|=|H\,:\,K|\cdot|G\,:\,H| $$ But as I understand I can't use the Lagrange on the second one. So how to prove the second one? related thread .","['group-theory', 'abstract-algebra']"
3114162,"If integration is a continuous analog of summation (Addition), what is the continuous analog of multiplication (Product)?","One definition of integration over a continuous interval [a,b] into n subintervals with equal width $\Delta x$ , and from each interval choose a point $x_i^*$ .  Then the definite integral of $f(x)$ from a to b is $$\int_{b}^{b}f(x)dx = \lim_{n\to\infty}\sum f(x_i^*) \Delta x$$ . What happens if the summation ( $\sum$ ) is replaced with a product ( $\prod$ )?  Is there a name for this type of infinite product?","['integration', 'summation', 'products']"
