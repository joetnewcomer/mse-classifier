question_id,title,body,tags
3989755,Find the indicial equation of $x^{3}y''+(\cos 2x-1)y'+2xy=0$,Find the indicial equation of $x^{3}y''+(\cos 2x-1)y'+2xy=0$ (where $y''$ stands for the second derivative) The reference answer is $m(m-1)-2m+2=0$ I really don't know how to ... please help. thanks in advance!!,['ordinary-differential-equations']
3989816,Hard integral - does it have a closed form?,"$$ \int_{-1}^0 \sqrt{ x + \ln  \left( \frac{1}{x+1} \right ) }  ~~~dx$$ All I could do is this: $$ f(x) = \sqrt{ x + \ln  \left( \frac{1}{x+1} \right ) }  = \sqrt{ x - \ln  \left( {x+1} \right ) }$$ Then: $$ u = x+ 1 \\ x = u - 1 \\ \Rightarrow du = dx $$ $$\sqrt{ u-1 - \ln  \left(u \right ) } ~~ du$$ But this is a deadend, because I don't know how to continue from this type of function.. Any help would be appreciated!","['integration', 'calculus']"
3989961,Integral of $1/x^2$ without power rule,"I was wondering if it was possible to evaluate the following integral without using the power rule for negative exponents \begin{equation*}
  \int \frac{1}{x^2} \; dx
\end{equation*} When using integration by parts, you end up with the same integral in the rhs so it seems out of luck \begin{equation*}
  \int \frac{1}{x^2} \; dx = \frac{\ln x}{x} + \int \frac{\ln x}{x^2} \; dx
\end{equation*} This question is inspired by this blackpenredpen's video Using integration by parts with $u = \frac{1}{x^2}$ and $v = x$ is NOT accepted. If you write \begin{equation*}
  \int \frac{1}{x^2} \; dx = \frac{1}{x} + 2 \int \frac{1}{x^2} \; dx
\end{equation*} you are still implicitly using the power rule to compute the derivative of $\frac{1}{x^2}$ so it is not correct per the rules. The same rules apply for $u$ -substitution which implicitly use the power rule. See the following with $u = \frac{1}{x}$ such that \begin{equation*}
    \int \frac{1}{\left(\frac{1}{x}\right)^2} \left(\frac{1}{x}\right)' \; dx = \int \frac{1}{u^2} \; du
\end{equation*} and here the power rule is also considered used to compute the derivative of $\frac{1}{x}$ although it can be subject to discussion (geometric proof, limit definition of derivative, etc...)","['integration', 'calculus', 'recreational-mathematics']"
3989971,"The arcsine is ""not really a function""","My math teacher said that later on in life, if you pursue math, you would see that the arcsine function isn't actually a function. I do not see how it's not a function.","['trigonometry', 'functions']"
3990014,problem involving centroid and perpendiculars,"in triangle $ABC$ , $G$ is the centroid and $L$ is an arbitary line through the centroid. $K,I,J$ are the foot of the perpendiculars from $A, B, C$ i need to prove that $KA+JC=BI$ how to use the midpoints of the sides and is there any way to use the fact that centroid cut the median in 2:1 ratio","['euclidean-geometry', 'geometry', 'plane-geometry']"
3990020,Christoffel symbols $\Gamma$ as an $\text{End}(E)$-valued one-form implies it is a tensor field?!,"I'm currently reading John Baez's Gauge Fields, Knots and Gravity for fun, and I stumbled across the following when talking about connections in a vector bundle. So, let $\pi:E\to M$ be a smooth vector bundle, and $\nabla$ a connection on $E$ . If we take a vector field $X$ on $M$ and a section $s$ of $E$ , and express them locally over some open $U\subset M$ as $X=\sum X^i\frac{\partial}{\partial x^i}$ and $s=\sum s^je_j$ (for some local sections $e_j$ of $E$ ), then we get \begin{align}
\nabla_Xs &= \left(X^i\frac{\partial s^k}{\partial x^i} + \Gamma^k_{ij}X^is^j\right)e_k
\end{align} (in the book he uses the notation $A_{\mu j}^i$ instead of $\Gamma$ 's and calls them the vector potential for reasons I haven't fully understood yet, but that's besides the point). Then, he goes on to talk about how to interpret $\Gamma$ more geometrically. He says the term $\Gamma^k_{ij}X^is^je_k$ is a local section of $E$ , and is $C^{\infty}(U)$ -linear in $X^i$ and $s^j$ , which means it should be interpreted as an $\text{End}(E)$ -valued 1-form over $U$ , that is, a section of \begin{align}
\text{End}(E|_U)\otimes T^*U.
\end{align} Then, later on he says this is not just a local description, but in fact global, so $\Gamma$ is a section of $\text{End}(E)\otimes T^*M$ . Here's where I'm getting confused. If we take $E=TM$ to be the tangent bundle, then it means $\Gamma$ is a section of $\text{End}(TM)\otimes T^*M\cong TM\otimes T^*M\otimes T^*M$ . But doesn't this say that $\Gamma$ is a $(1,2)$ -tensor field over the base manifold $M$ , which goes against everything that is emphasized in an introductory Riemannian geometry course? I must obviously be misinterpreting what is written in the text, or have made a mistake unwinding the various ""abstract"" definitions. Any help in clarifying this is appreciated.","['connections', 'vector-bundles', 'riemannian-geometry', 'differential-geometry']"
3990082,sum of covariant derivatives implies constant Ricci curvature,"Let $R_{ij}$ denote the Ricci tensor . If $R_{ij,k}+R_{jk,i}+R_{ki,j}=0$ , prove that the scalar curvature $R$ is constant . A straightforward calculation gives $$R_{ij,k}=\frac{\partial R_{ij}}{\partial x^k}-\Gamma^l_{ik}R_{lj}-\Gamma^l_{jk}R_{il} \\ R_{jk,i}=\frac{\partial R_{jk}}{\partial x^i}-\Gamma^l_{ji}R_{lk}-\Gamma^l_{ki}R_{jl} \\ R_{ki,j}=\frac{\partial R_{ki}}{\partial x^j}-\Gamma^l_{kj}R_{li}-\Gamma^l_{ij}R_{kl}$$ Since $R_{ij}$ is symmetric $$R_{ij,k}+R_{jk,i}+R_{ki,j}=0 \\ \implies \frac{\partial R_{ij}}{\partial x^k}+\frac{\partial R_{jk}}{\partial x^i}+\frac{\partial R_{ki}}{\partial x^j}=2\Gamma^l_{ik}R_{lj}+2\Gamma^l_{jk}R_{il}+2\Gamma^l_{ij}R_{kl}$$ I know that $R=g^{ij}R_{ij}$ , but multiplying above expression with any one of $g^{lj},g^{il},g^{kl}$ won't help . How to approach this one ? Any help is appreciated .","['curvature', 'tensors', 'riemannian-geometry', 'differential-geometry']"
3990086,Using generalized logistic curve to create a mathematical model from data.,"The first row is time and the second row is height of a plant. We need to use generalized logistic curve to model the behavior of the plant. The equation of the logistic curve is : $$N = \frac{N_*}{1+(N_*/N_0-1)e^{-a_0t}}$$ where $N_*$ is the maximum height of the plant or in other words, the supremum of the logistic function. $a_0$ is how fast the function increases. My question is the following: I know how to determine $N_*$ , we just make up a number that is above $251$ and less than $251 + (251 - 247) $ . But how to exactly determine $a_0$ . I am quite lost here. How I would do it is to calculate every $a_0$ for every time and height, with equation $a_0 = \frac{1}{t}\ln(\frac{N}{N_0})$ . In this equation $N$ would be the difference between heights, so let's say between $18$ and $33$ the $N$ for time equals $1$ would be $33 - 18$ . Is the process here of creating a table of $\sum_{k = 1}^{10}(10 - i)$ column elements of differences, so that we take multiple $N_0$ until we run out of data from the primary table in the picture (so until $N_0 = 9$ ). Then we can use a formula for $a_0 = \frac{\sum a_{0l}*t_l}{\sum t_l}$ where $l$ is the number of elements. Is my procedure for creating a logistic out of the data provided correct?","['statistics', 'functions', 'mathematical-modeling', 'logistic-regression']"
3990110,Hölder inequality and interpolation,"I was seeking the other day for intuition about the Hölder inequality and more precisely the geometric intuition behind the relation $\frac{1}{p} + \frac{1}{q} = 1$ . I found this interpretation of the Hölder inequality. It says that we are trying to find an estimate of : $$ \int f^a g^b$$ with the knowledge of $\int f^p$ and $\int g^q$ . For $(a, b) = (1,1)$ , we need the relationship $\frac{1}{p} + \frac{1}{q} = 1$ so that the points : $(p, 0), (1,1), (0, q)$ lie on the same line. We can know use interpolation to have the Hölder inequality which give an estimate at the point $(1, 1)$ . But I feel like an argument is missing here. Why the estimate is using the geometric mean of the  function $f$ and $g$ ?  Why we don't have something like (using arithmetic mean) : $$
\int \mid fg \mid \leq \frac{1}{p} \int f^p + \frac{1}{q}\int g^q \;\;?
$$ Moreover the article is saying we can generalize this approach to approximate $\int f^ag^b$ but in this case we need three points such that $(a,b)$ is in the convex hull of these three points. Why for $(1, 1)$ two points are needed while for general $(a, b)$ we need three points ?","['integration', 'interpolation', 'real-analysis', 'intuition', 'inequality']"
3990113,Prove a graph is cyclic,"I encountered the following problem: Let $G(V, E)$ be a finite undirected graph with $2n$ vertices, in which every connected component has an even number of vertices. Given that exactly $n$ of the vertices of $G$ have a degree at least $3$ , prove that $G$ is cyclic. My solution to the problem goes as follows: Let $C$ denote the set of all connected components $G_i(V_i, E_i)$ of $G$ and $|C| = k$ . Let's assume that $G$ is acyclic. That means that every $G_i$ is a tree. Then, by adding exactly $k-1$ edges to $G$ , we can connect all connected components in such a way (root of one to another one's leaf, ordering them in a sequence that way) that we obtain a tree $G'(V', E')$ , where $V'= V$ and $|E'| = |E| + k - 1$ . But since $G'$ is a tree, that means that $$|E'| = |V'| - 1 = |V| - 1 = 2n - 1$$ By the handshake lemma we also know that $$2|E'| = \sum_{v\in V'}\deg v \ge \underbrace{3n}_{\substack{\text{atleast $n$}\\ \text{with degree $\ge$ 3}}} + \underbrace{n}_{\substack{\text{G' is connected}\\\text{$\Rightarrow$ the remaining $n$ have $\deg\ge 1$}}}$$ Thus we obtain that $$2(2n-1) \ge 4n \Leftrightarrow -1 \ge 0$$ which is a contradiction, hence $G$ can't be acyclic and contains a cycle. Now, what bothered me about that is that I never used the given fact, that every $G_i$ had an even number of vertices, so I felt like I was missing something huge or my solution is flawed in a fundamnetal way... and I really banged my head against the wall hard enough, but was nonetheless unable to figure out what was wrong. So my question would be: am I missing something or is the mentioned statement in the problem simply redundant? And if it is redundand, how would one go about proving it? I really got stuck somewhere about there. Many thanks in advance!","['graph-theory', 'graph-connectivity', 'discrete-mathematics']"
3990116,Proving $abcd+3\geq a+b+c+d$,"If $a,b,c,d$ are non negatives and $a^2+b^2+c^2+d^2=3$ prove that $$abcd+3\ge a+b+c+d$$ The inequality is not as simple as it looks.The interesting part is that the equality occurs when $a=0,b=c=d=1$ upto permutation .(I don't know if there are further equality cases.) I tried rewriting the inequality as $$a^2+a(bcd-1)+b^2+c^2+d^2-b-c-d\ge 0$$ As its a quadratic in $a$ it suffices to show  the discriminant $$\Delta_a={(bcd-1)}^2-4(b^2+c^2+d^2-b-c-d)\le 0$$ which unfortunately is wrong (when $a=\sqrt{3},b=c=d=0$ ) P.S ;I am not aware of using Lagranges multipliers Its from here Update :An answer has been posted in the link above (AOPS) similar to Dr Mathva's answer","['contest-math', 'algebra-precalculus', 'sum-of-squares-method', 'inequality']"
3990185,"Why $[\mathbb{Q}(\sqrt[n]{p}, \sqrt[m]{q}):\mathbb{Q}] = nm$?","As title says, I want to prove that $\mathbb{Q}(\sqrt[n]{p}, \sqrt[m]{q})$ is degree $nm$ extension of $\mathbb{Q}$ when $p \neq q$ are distinct primes. By Eisenstein's criterion, $x^{n} - p$ is irreducible over $\mathbb{Q}$ and so $[\mathbb{Q}(\sqrt[n]{p}):\mathbb{Q}] = n$ . However, I wonder if it is trivial that $x^{m} - q$ is irreducible over $\mathbb{Q}(\sqrt[n]{p})$ , assuming the general form of Eisenstein's criterion over integral domain (see Wikipedia for the statement). I'm not sure if the polynomial satisfies assumptions of the Eisenstein's criterion. Also, I tried some $p$ -adic ( $q$ -adic will be more appropriate) strategy. If we assume that $\mathbb{Q}(\sqrt[n]{p})$ can be embedded in $\mathbb{Q}_{q}$ , then the irreducibility of $x^{m} - q$ over $\mathbb{Q}(\sqrt[n]{p})$ follows from it over $\mathbb{Q}_{q}$ , which is a result of Newton polygon. However, the assumption fails in general, and when we enlarge $\mathbb{Q}_q$ to its finite extension that contains $\sqrt[n]{p}$ , the situation gets worse. Another direction (which I believe that shouldn't be easy) is to prove that the polynomial $$
f(x) = \prod_{0\leq i < n, 0 \leq j < m} (x - \sqrt[n]{p}\zeta_{n}^{i} - \sqrt[m]{q}\zeta_{m}^{j})
$$ is irreducible over $\mathbb{Q}$ . Then the claim follows from the another proposition $$\mathbb{Q}(\sqrt[n]{p} + \sqrt[m]{q}) = \mathbb{Q}(\sqrt[n]{p}, \sqrt[m]{q})$$ (this is not trivial, but I have a proof by the light use of Galois theory.) According to this , this proposition implies our main claim when $n, m$ are coprime. Can we use Eisenstein's criterion in this case? If not, is there a simple way to prove this? Maybe related: Why $\sqrt[3]{3}\not\in \mathbb{Q}(\sqrt[3]{2})$?","['galois-theory', 'number-theory', 'algebraic-number-theory', 'prime-numbers']"
3990191,"When will matrix multiplication become just ""concatenation""?","Saw below entertaining matrix multiplication examples. Obviously they are just coincidences. But I am curious when does below hold? \begin{equation}
\begin{pmatrix}
a_1 & b_1 \\
c_1 & d_1 
\end{pmatrix} \times \begin{pmatrix}
a_2 & b_2 \\
c_2 & d_2 
\end{pmatrix} = \begin{pmatrix}
\overline{a_1 a_2} & \overline{b_1 b_2} \\
\overline{c_1 c_2} & \overline{d_1 d_2}
\end{pmatrix}
\end{equation} here $\overline{a_1 a_2}$ means gluing the integers together, not multiplications.. assuming we are working on integer matrices.","['matrices', 'number-theory', 'linear-algebra']"
3990234,Random signs inequality,"Let $X_1, X_2, \dots, X_n, \dots$ be Rademacher random variables (random signs), i.e. with the distribution $$X_i \sim  \left\{
  \begin{array}{@{}ll@{}}
    \ \ \ 1 & \text{with probability} \ \ \ \frac{1}{2},\\
    -1 & \text{with probability} \ \ \ \frac{1}{2}.
  \end{array}\right.$$ Now, let us fix $y\in\ell^2$ - meaning that $y=(y_i)_{i=1}^{\infty}$ and $\sum_{i=1}^{\infty}y_i^2<\infty$ . How to prove that $$\Bigg(\mathbb{E}\Bigg|\sum_{i=1}^{\infty} y_iX_i\Bigg|^{p}\Bigg)^{\frac{1}{p}} \ \le \ c\cdot\sqrt{p}\cdot\Bigg(\sum_{i=1}^{\infty}y_i^2\Bigg)^{\frac{1}{2}}?$$ Here $p>1$ , and $c$ is a universal constant. I have found it in a paper being called a ""standard inequality"", but I am stuck on how to obtain
this $\sqrt{p}$ factor. I will be glad for any help or insight.","['lp-spaces', 'normed-spaces', 'functional-analysis', 'probability']"
3990245,Showing $e^{-x^2}$ small when $x$ is large just from series definition,"This is a sort of open ended question. Suppose $$F(x):=\sum_{L=0}^{\infty} (-1)^L\frac{x^{2L}}{L!}.$$ One can show that the series is absolutely convergent for every fixed $x$ , by noting that $x^{2L} \le (L-2)!$ for all large enough $L$ . So, $F(x)$ is well defined. I want to show $F(x)$ is very small for $x$ large enough. I DON'T want to use the fact that $F(x)$ is actually $e^{-x^2}$ . Actually, in my research, I am dealing with a similar problem where I have an alternating series quite similar to exponential (but no closed form), where I need to show its small when $x$ is large. So, I was trying with this toy problem.","['real-analysis', 'taylor-expansion', 'sequences-and-series', 'power-series', 'exponential-function']"
3990332,Proving the Leibniz Integral Rule,"I've been wondering if there is a fairly simple proof or derivation for the following (called the Leibniz Rule): $$ \frac{d}{dt} \int_{a(t)}^{b(t)}f(x,t)dx = \int_{a(t)}^{b(t)}\frac{\partial{f}}{\partial{t}}dx+ f(b(t),t)\frac{d}{dt}b(t)-f(a(t),t)\frac{d}{dt}a(t)$$ What I tried doing is integrating by parts the leftmost integral, and then applying the time derivative to what it's equal to. However, it seems the other side is very far from what the result above. A small hint would suffice as I like trying to prove things on my own.","['integration', 'multivariable-calculus', 'calculus', 'leibniz-integral-rule']"
3990363,Minimum sum of digits for a polynomial,"Given $n$ a natural number (greater than $0$ ) and $p(n)$ a polynomial, $p(n) = 2n^2 + 43n + 502$ . Defining the arithmetic function $s(k) : \mathbb{N}^*  \to \mathbb{N}^*$ , the sum of the digits of the number $k$ . Find $\min s(p(n))$ . (Problem proposed for JMBO practice) Approach 1. $s(a + b) \leq s(a) + s(b)$ , therefore $s(p(n)) \leq s(n(2n + 43)) + 7$ . But $s(ab) \leq s(a)s(b)$ , therefore $s(n(2n + 43)) \leq s(n)s(2n + 43)$ . Applying the sum property, then $s(2n + 43) \leq s(2n) + 7$ . By product property, $s(2n) \leq 2s(n)$ . To sum up, $s(p(n)) \leq s(n)[2s(n) + 7] + 7 = 2s(n)^2 + 7s(n) + 7$ , from which I'm stuck. Approach 2. Let's prove $\min s(p(n)) = 7$ . Let's prove it's impossible to have $s(p(n)) \in \{1, 2, 3, 4, 5, 6\}$ . As $p(n) \not\equiv 0 \mod 3$ , $s(n) \not\in \{3, 6\}$ . Also, if $s(p(n)) = 1$ , $p(n) = 10^q$ , $q \in \mathbb{N}$ . But $p(n) \not\equiv 0 \mod 10$ , so it's impossible. For the last numbers, I haven't found any approach. Thanks!","['summation', 'modular-arithmetic', 'number-theory', 'elementary-number-theory', 'functions']"
3990386,Prove that $\lim\limits_{x\to+\infty}f'(x)=\lim\limits_{x\to+\infty}f''(x)=0$,"Given $f \in C^3(\mathbb{R}).$ Suppose that $\lim\limits_{x\to+\infty}f(x)=a \in \mathbb{R}$ and $\lim\limits_{x\to+\infty}f'''(x)=0$ . Prove that $\lim\limits_{x\to+\infty}f'(x)=\lim\limits_{x\to+\infty}f''(x)=0$ . My solution: Since $\lim\limits_{x\to+\infty}f(x)=a$ , there exists $M>0$ that for all $x, x' \in \mathbb{R}$ , if $x,x'>M$ then $f(x), f(x') \in \left(a-\varepsilon^2/2, a+\varepsilon^2/2\right)$ . This yields $$\vert f(x)-f(x') \vert < \varepsilon^2.$$ Therefore $\frac{\vert f(x)-f(x') \vert}{\varepsilon} < \varepsilon$ . So, for all $x>M$ , $x'=x+\varepsilon$ we have $$0\le\frac{\vert f(x+\varepsilon)-f(x) \vert}{\varepsilon} < \varepsilon.$$ Let $\varepsilon \to 0$ we have $$0 \le f'(x) \le 0 \text{ or } f'(x)=0.$$ That means $\lim\limits_{x\to+\infty}f'(x)=0$ . Doing similarly as above we have $$\lim\limits_{x\to+\infty}f''(x)=0.$$ My question: $\lim\limits_{x\to+\infty}f'''(x)=0$ is really necessary or not (because I don't use it in my proof)?","['limits', 'calculus', 'derivatives']"
3990399,How we can know the ramification ideals geometrically?,"Let $L/\mathbb{Q}$ be a finite Galois extension of degree n, let $\mathcal{O}_{L}$ be the ring of integers of $L$ ,
By Dedekind lemma we have that $\mathfrak{p}=\mathfrak{b}_{1}^{e}...\mathbb{b}_{g}^{e}$ where $ \mathfrak{b}_{i} \in \mathcal{O}_{L}$ and $\mathfrak{p}$ $\in $$\mathbb{Z}$ Since this extension is Galois we have $gfe=[L:\mathbb{Q}]=n$ where $e$ is the ramification index and $f$ is the inertia degree which is given by $f_{i}=[\mathcal{O}_{L}/\mathfrak{b} :\mathbb{Z}/\mathfrak{p}]$ we know that $\mathfrak{p}$ is ramified if $e >1 $ ,
I am asking is there a geometric way to know whether a prime ideal is ramified or not? if it exists can you apply it on this picture (give any example) Also, I want to know what is the geometric interpretation of the inertia degree and the ramification index ?","['algebraic-geometry', 'maximal-and-prime-ideals', 'ramification']"
3990422,Why does a right-tailed distribution have a positive third moment?,"So a right-tailed distribution (or a left-leaning one) has positive skewness, which means a positive third moment. I was trying to convince myself why this is always the case. I am considering a compactly supported discrete distribution for simplicity. Now a typical right-tailed distribution will have its mode and most of the frequency concentrated towards the left, dragging the mean to a bit left of where it would have been for a uniform distribution. So that there are more positive terms in the sum $$\sum_i(x_i-\bar x)^3f_i$$ than negative terms. But the median is to the left of $\bar x$ , which means there is more frequency associated with the negative terms than the positive ones. Why is  it always the case then, that this sum ends up positive? I am guessing that the third powers of the higher magnitude terms end up on top but I am not very convinced.","['statistics', 'probability']"
3990477,Any proof (in English) that Generalised Peano Continua are continuous images of the real line?,"By generalised Peano continuum I mean a metric, connected, locally connected, locally compact space. I found a paper called A Hahn-Mazurkiewicz Theorem for generalized Peano continua , which says that this result was proved by Mazurkiewicz in his seminal paper Sur les lignes de Jordan . However it is a more than 40 pages long document in French and I don´t understand french, is there a more recent book in English that deals with this topic? I haven´t found any.","['general-topology', 'continuum-theory']"
3990576,"Integrability of an ""almost"" CR-structure defined thanks to an almost contact metric structure.","Let $(M,\eta,\xi,\varphi)$ be an almost contact metric manifold , that is: $M$ is a smooth manifold $\eta$ is a contact 1-form on $M$ , i.e $\eta$ is a 1-form and $\mathrm{d}\eta|_{\ker \eta}$ is non-degenerate $\xi$ is the Reeb vector field of $\eta$ , that is $\xi \in \ker \mathrm{d}\eta$ and $\eta(\xi) \equiv 1$ $\varphi$ is a section of $End(TM)$ with $\varphi^2 = -\mathrm{id} + \eta\otimes\xi$ and $\varphi (\xi) = 0$ One can think of this manifold as an ""almost"" (I don't know if this is the right terminology) CR manifold : let $H = \ker \eta$ be the contact distribution and $J = \varphi|_{H}$ , which is a section of $End(H)$ . Then $J$ is a complex structure on the contact distribution $H$ . For $(M,H,J)$ to be a CR manifold, $J$ needs to be integrable on $H$ , that is, in the complexified bundle $(H^{\mathbb{C}},J)$ with the usual holomorphic/anti-holomorphic decomposition $H^{\mathbb{C}} = H^{1,0} \oplus H^{0,1}$ , one needs to have $[H^{1,0},H^{1,0}] \subset H^{1,0}$ . I remember having read a paper where it was stated that such an ""almost"" CR structure is integrable and leads to a CR manifold if and only if the Nijenhuis tensor of $\varphi$ , defined by $$
N(X,Y) = -[\varphi X,\varphi Y]+ \varphi[\varphi X,Y] + \varphi[X,\varphi Y] -  \varphi^2[X,Y]
$$ satisfies a certain condition. I think this condition was something like $N$ taking values proportional to $\xi$ . The problem is I cannot find this paper anymore, and some long research on the Internet and on arXiv had been unproductive. Moreover, I remember that, in this paper, this was only stated and wasn't proved. My question is the following: does anybody have a reference about this fact? Edit Here are some more lines to be more precise about my question.
Regarding only $J$ , it is clear that $(M,H,J)$ is integrable if and only if the Nijenhuis tensor of $J$ indentically vanishes. My question is more about how to translate this condition to a condition on the full tensor $\varphi$ and not just its restriction? A - possibly abusive - parallel can be done with almost Kaehler geometry: given an almost Kaehler manifold $(M,g,J)$ , there are three equivalent conditions that assure the integrability of the structure, that is $(M,g,J)$ is a Kaehler manifold. It may happen that some condition is more relevant in some context than the others: in a Riemannian setting, it is easier to show some parallel condition than some symplectic ones, etc . Here, I try to find a condition on $\varphi$ without applying any restriction on it; moreover, I am almost sure I have already read such a condition!","['contact-geometry', 'differential-geometry']"
3990580,Charecterization of Topologies via Galois Connections,"Let $X$ and $Y$ be two sets and let $F: \mathcal{P}(X) \to \mathcal{P}(Y)$ and $G: \mathcal{P}(Y) \to \mathcal{P}(X)$ be two set functions that satisfy $$F(A) \subseteq B \iff A \subseteq G(B). \tag{1}$$ That is, the pair $F$ and $G$ constitute a Galois connection. It is well known that $c = G\circ F$ is a(n abstract) closure operator and $i = F \circ G$ is an (abstract) interior operator. I.e., $c$ (resp $i$ ) satisfies: $A \subseteq c(A)$ (resp., $i(A) \subseteq A$ ) for all $A \subseteq X$ . $c(c(A)) = c(A)$ (resp., $i(i(A)) = i(A)$ ), for all $A \subseteq X$ . $A \subseteq B$ implies $c(A) \subseteq c(B)$ (resp., $i(A) \subseteq i(B)$ ), for all $A, B \subseteq X$ . A topological closure operator also satisfies $c(A \cup B) = c(A) \cup c(B)$ for all $A, B \subseteq X$ . A topological interior operator also satisfies $i(A \cap B) = i(A) \cap i(B)$ for all $A, B \subseteq X$ . Question : are there intelligible conditions on $F$ and $G$ such that $c$ is a topological closure operator and $i$ and topological interior operator? Is there a characterization of topology via Galois connections? A bit more motivation: if $Z$ is a topological space, and $X$ is the set of closed sets and $Y$ the set of open sets, then $F: A \mapsto int(A)$ (the interior of $A$ ) and $G: B \mapsto cl(B)$ (the closure), then $F$ and $G$ satisfy (1). Then $i$ sets $A$ to $int(cl(A))$ , the regular interior of $A$ . Now, the intersection of regular open sets is regular open, so it would seem (naively) that $F \circ G$ is a topological interior operator, despite regular opens sets not being closed under unions.","['order-theory', 'general-topology', 'galois-connections']"
3990618,find angle x in isosceles triangle,"Considering the attached image, is that possible to compute for $\angle x$ in the isosceles triangle below, with no further known values? $\\$ I have found $\angle B$ and $\angle C$ are $50^o$ . So $\angle B$ is divided by $\overline{BP}$ to $10^o$ and $40^o$ . But I can't find how the $\angle C$ is divided by $\overline{CP}$ . Also its clear that $\overline{BP}$ is perpendicular to leg $\overline{AC}$ .","['triangles', 'geometry']"
3990632,What are sets made up of in topology?,"I am looking for a very basic answer, I am in 10th grade and studying it for a project. I don't understand what the sets are referring to. Is it nodes? Edges? Axioms? Vertices? Or does it just depend?",['general-topology']
3990643,Matrices which determinant is obvious.,"At least in my current Linear Algebra course, exercises concerning the determinant of a matrix of dimension $n\geq 5$ more often than not require one to notice some property of the matrix so that its determinant becomes obvious and one needs not compute it in the usual way. I'm trying to find a simple way to realize that the determinant of $$\begin{pmatrix} 0 & -1 & -1 & -1 & -1
\\
1 & 0 & -1 & -1 & -1
\\
1  & 1 & a & -1 & -1
\\
1 & 1 & 1 & 0 & -1
\\
1 & 1 & 1 & 1 & 0\end{pmatrix}$$ is equal to $a$ , and I would appreciate anyone who points me in the right direction. Also, since these are the types of questions that might go to my (as well as other students') final, I'm open to users giving other examples of matrices which determinants are obvious without the need to compute them, here are a few: $$\det \begin{pmatrix} 
-4 & 1 & 1 & 1 & 1
\\
1 & 1 & 1 & -4 & 1
\\
1  & 1 & -4 & 1 & 1
\\
1 & -4 & 1 & 1 & 1
\\
1 & 1 & 1 & 1 & -4\end{pmatrix}
=
\det\begin{pmatrix} 
-4 & 1 & 1 & 1 & 1
\\
1 & 1 & 1 & -4 & 1
\\
0  & 0 & 0 & 0 & 0
\\
1 & -4 & 1 & 1 & 1
\\
1 & 1 & 1 & 1 & -4
\end{pmatrix}=0
$$ by adding rows $1,2,4,5$ to row $3$ . $$\det \begin{pmatrix} 
1 & 2 & 3 & 4 & 5
\\
2 & 3 & 4 & 5 & 6
\\
3  & 4 & 5 & 6 & 7
\\
4 & 5 & 6 & 7 & 8
\\
5 & 6 & 7 & 8 & 9\end{pmatrix}
=\det \begin{pmatrix} 
1 & 2 & 3 & 4 & 5
\\
1 & 1 & 1 & 1 & 1
\\
2  & 2 & 2 & 2 & 2
\\
4 & 5 & 6 & 7 & 8
\\
5 & 6 & 7 & 8 & 9\end{pmatrix}=0$$ by subtracting row $1$ from row $2$ & $3$ . $$\det \begin{pmatrix} 
0 & -a & -b & -d & -g
\\
a & 0 & -c & -e & -h
\\
b  & c & 0 & -f & -i
\\
d & e & f & 0 & -j
\\
g & h & i & j & 0\end{pmatrix}=0$$ since $(-1)^5\det (A)=\det(A^T)=\det(A)$ . All of these can be generalized to other dimensions, with the exception of the last one for which the dimension of the matrix must be odd.","['determinant', 'big-list', 'matrices', 'linear-algebra', 'linear-transformations']"
3990721,Inequality for determinant of sum of two orthogonal matrices,"My goal is to show that : $$\forall A,B\in O_n(\mathbb{R}), |{\rm det}(A+B)| \le 2^n.$$ How can I access the determinant of the sum of two matrices ? I don’t know many ways to establish inequalities in Algebra, except with Bessel’s inequality and Cauchy Schwarz inequality, but they don’t seem to be very helpful here.","['orthogonal-matrices', 'inequality', 'determinant', 'linear-algebra']"
3990786,Combinatorial/ Binomial Identity,I am finding difficulty in showing the following identity: $$\sum_{k=1}^n {n-1\choose k-1} {2n-1\choose k}^{-1} = \frac{2}{n+1}.$$ Mainly because of the inverse part.,"['binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
3990790,Curve of number of attempts to gain a result with probability p,"Recently, I've been reinvigorated to get a better intuition for probability and statistics by its several simultaneously vexing and intriguing paradoxes. One of which I've discovered through my interest in programming. I have a program that will generate a random number between 0 and 1 over and over again until it is greater than some number N, counting the number of times it does this. It then keeps a list of each possible number of tries it could have taken and counts them up. In essence, it's repeating random chance 1-N until it gets the result and telling me how many times the program was able to get it in X attempts. Now, originally I was expecting a bell curve around the number 1/p where p would be 1-N, or just the probability. However, I was completely surprised to find out that the most common result was only 1, and it trails off in probability from there! The average number of attempts is still 100, I suppose, but it's simply baffling to me that the graph comes out this way. For an example of why it seems paradoxical to me, take the case of N = 0.99, so P = 0.01 or 1 in 100. It will be more likely that you have a result before fifty attempts than it will be that you have a result between 75 to 125 attempts. I don't know if anyone here is able to shed light on the topic, as it may just come down to poor intuition, but I felt like it was striking enough to post a question regarding it. Below I will post a graph of my results from 10,000,000 trials where X is the number of attempts before getting the result, and Y is how many times it took exactly that number of attempts.","['intuition', 'statistics', 'probability', 'curves']"
3990904,Have there already been sucessful attempts at using induction proof methods for the rationals?,"I developed a proof method which builds on natural induction for propositions regarding closed intervals of $\mathbb{Q}$ . The description takes very little effort and it boils down to the use of two inductions, I've just sent it to Acta Mathematica and I'd like to know if there have already been other methods that allow for rational induction.
I already knew about transfinite induction before doing this, the main appeal of the paper is in the construction itself.","['elementary-set-theory', 'proof-writing', 'logic']"
3990952,Known bounds on $\int_{-\infty}^{\infty}|f(x)-f(x-a)|dx$?,"Suppose that $f(x)$ is continuous on $\mathbb{R}$ such that $|f(x)|\leq M$ . Are there any know result for estimating the quantity $$
I(a)=\int_{-\infty}^\infty|f(x)-f(x-a)|dx.
$$ I have the following results: (i) If $f$ is strictly increasing (equivalently $f(-x)$ strictly decreasing), $$
\begin{align}
I(a)&=\int_{f(\mathbb{R})} \left[f^{-1}(y)+a-f^{-1}(y)\right]dy \\
&\leq\int_{[-M,M]} a\,dy \\
&= 2Ma
\end{align}
$$ (ii) If $f\geq0$ , $f$ strictly increasing on $(-\infty,0)$ and strictly decreasing on $(0,\infty)$ . By the intermediate value theorem, there exist $c\in(0,a)$ such that $f(c)=f(c-a)$ , then $$
\begin{align}
I(a)&=\int_{-\infty}^c \left[f(x)-f(x-a)\right]dx+\int_c^\infty\left[f(x-a)-f(x)\right]dx \\
&=\int_{-\infty}^cf(x)dx-\int_{-\infty}^{c-a}f(x)dx+\int_{c-a}^\infty f(x)dx-\int_c^\infty f(x)dx \\
&=2\int_{c-a}^c f(x)dx \\
&\leq 2Ma
\end{align} 
$$ Is there a generalization of such bound? My guess would be that for any bounded continuous function that has finitely many extrema, $$
I(a)\leq 2aMn,
$$ where $n$ is the number of extrema. Any insight would be greatly appreciated.","['integration', 'analysis', 'real-analysis']"
3990957,The nature of variables within proofs,"I am confused about how best to view certain kinds of variables that show up in the course of an elementary proof. For example, to prove that the square of every odd integer is odd, I would write: Let $m \in \mathbf{Z}$ be arbitrary. Suppose $m$ is odd. Choose $k \in \mathbf{Z}$ such that $m = 2k + 1$ . Then $m^2 = (2k + 1)^2 = 2(2k^2 + 2k) + 1$ , so since $2k^2 + 2k \in \mathbf{Z}$ , it follows that $m^2$ is odd. The symbol $m$ is introduced as part of the technique of universal generalization; Velleman [1] describes this step as ""introducing a new variable [ $m$ ] ... to stand for an arbitrary object."" The symbol $k$ is introduced as part of the technique of existential instantiation; [1] describes this as ""introducing a new variable [ $k$ ] to stand for an object for which [the predicate $m = 2k + 1$ ] is true."" In what sense are $m$ and $k$ variables? They do not seem to be bound variables, as they are in the scope of no quantifier. They also can't be free variables, because if they were, then, for example, the sentence being proved true after the introduction of arbitrary integer $m$ -- namely that if $m$ is odd then $m^2$ is odd -- would be a predicate and not even have a truth value; I am also obviously not free to substitute values into $m$ and $k$ , if for no other reason than $m$ and $k$ are dependent. Stoll [2] says a variable is free if it is not bound, so there is no third option. Is it more correct to think of the phrases ""let $m \in \mathbf{Z}$ be arbitrary"" and ""choose $k \in \mathbf{Z}$ such that ..."" as introducing new constants rather than variables, as their values are fixed despite being unspecified ( $m$ ) or unknown ( $k$ )? More generally, the idea of a symbol that stands for a particular but arbitrary element of some set seems incompatible with the typical description of a variable as a placeholder as seen in, e.g., Epp [3]. [1] Velleman, D. (2006). How to Prove It: A Structured Approach (2nd ed.). Cambridge: Cambridge University Press. [2] Stoll, Robert Roth (1963). Set Theory and Logic. W.H. Freeman. [3] Epp, Susanna S. Variables in Mathematics Education. https://condor.depaul.edu/~sepp/VariablesInMathEd.pdf","['proof-writing', 'predicate-logic', 'discrete-mathematics']"
3990966,Solving a PDE by method of characteristics,"I solved the following: \begin{align}
\frac{dy}{y}&=\frac{dy'}{2y'+wy}\\
\ln y + \ln C &= \frac{1}{2} \ln \left(2y'+wy\right)\\
2\ln y + \ln C &= \ln \left(2y'+wy\right)\\
C &= \frac{2y'}{y^2} + \frac{w}{y}\\
\end{align} I isolated the constant since the purpose of solving is to find the differential invariant. However the answer given is: $$
\frac{y'}{y^2}+\frac{w}{y}
$$ where am I going awry?","['calculus', 'ordinary-differential-equations']"
3991052,Picture of l'Hopital's rule at infinity,"I know that when L'Hopital's rule is taken for the $\frac{0}{0}$ case, we get that the functions start looking at lines, like in this figure from Stewart's Calculus: But what is the visual when the limit is the $\frac{\infty}{\infty}$ case? The only thing that comes to mind that is easily visualized is a rational function's asymptotes (and that comes from dividing numerator and denominator by the largest exponent in the denominator)","['limits', 'calculus', 'real-analysis']"
3991080,Question about the stereographic projection. How to find the arc of great circle of the stereographic sphere S.,"Let $z$ , $w$ be complex numbers and let $P(z)$ , $P(w)$ be the corresponding
points on the sphere $S$ , associated to $z$ , $w$ via the stereographic projection.
Find the length of the arc of the great circle lying on $S$ and joining $P(z)$ to $P(w)$ . I started to consider the arc of the center should be the greatest length. But I'm so confused.","['complex-analysis', 'geometry']"
3991165,ways to walk on a hexagon to return to same point with n steps,"So I have an hexagon and I start at point 1 and I can only move right or left, I need to find the formula to reach the same point within n steps,
Assuming I have an odd number of steps allowed, I can't ever return to the point. But assuming I have even steps, I'm not sure how do I even think about it, I divided by two because I can go from 1>>2 or from 1>>6 so it's easier to just look at it as if i started with 1 step right. This means that if the sequence is $ a_n $ and I took a step right now i have $ a_{n-1}$ steps to come back.
how do I go on from here?","['combinatorics', 'discrete-mathematics', 'recursion']"
3991179,Optimization with strictly increasing continuous function with zero derivative (a.e.),"Suppose we have a strictly increasing, continuous scalar variable function $f(x)$ such that $f'(x) = 0 \text{ a.e.}$ The problem is $$\underset{x \in [a, b]}{\max}\ f(x) - cx$$ for some positive $c$ . Since we have a sum of two continuous functions, it follows from the Weirstrass theorem that there exists a solution. If solution is interior, we cannot find it from the first order condition since $f'(x) = 0 \text{ a.e}$ . My question is: are there ways to characterize the interior solution analytically in general (without specifying $f(x)$ )? I am also interested in a more general setting, where instead of $-cx$ there could be any continuous function. If you have references to papers which study this, would be very grateful if you share.","['optimization', 'derivatives', 'real-analysis']"
3991193,"Given isosceles triangles $\triangle ABC$ and $\triangle DBF$ (all spherical chords), identify the chord $\overline {DF}$ so that $|AD| = |DF| = |FC|$","tl;dr : As shown in the image below, find the chord $\overline {DF}$ so that $|\overline {AD}| = |\overline {DF}|$ , and have the answer be in the form of the ratio between $|\overline {AC}|$ and $|\overline {DF}|$ . Ideally, I'm actually looking for a slightly different ratio (which is explained below), but this is the basic problem I believe that needs to solved. Only the isosceles triangle $\triangle ABC$ may be used as input  (the image is slightly misleading, as they all should be straight lines and $\overline DF$ should be slightly higher than $\triangle ABC$ and not intersecting it). The actual problem I'm trying to solve is to subdivide a dodecahedron into smaller faces (known as a chamfered dodecahedron), and approximate a sphere using that polyhedron. This is done in 3 steps: Scale down each face from its center by some scale factor (what I'm trying to solve for) so that the hexagons constructed in the next step are planar (they will not be equiangular, however) Construct hexagons in between the newly subdivided faces (goldberg polyhedrons will always have 12 pentagons, no matter how subdivided they are) Scale the new face from the origin so that each vertex has the same magnitude of the original face (this is done by normalizing each vertex, then scaling by a specified radius - this will make the hexagons non-planar) My problem is that I'm stuck on how to identify how to scale factor in the first step, as there doesn't seem to be a linear relationship between the scale factor needed and the number of subdivisions that are performed. This is shown in the two images below, where I'm adding the new faces over the original dodecahedron (radius of 1 in my case). I manually calculated approximately $0.40706$ (or $40.706\%$ ) for the first subdivision, which allowed the construction of the [green] hexagon to have satisfyingly equivalent sides. However, when I tried scaling the new pentagons again with the same scale factor (shown with the yellow pentagons), the hexagons in-between were not equilateral... I suspect that the scale factor will change the more I subdivide these faces - indicating that there must be a non-linear relationship between side length and the length of the connecting chord. To attempt to solve my problem, I reconstructed the issue to what I hope is a solvable math problem, shown in the images below (although lines $\overline {AB}$ and $\overline {BC}$ should be chords, not arcs - thanks to @Blue in the comments). However, I frankly have no idea where to start... I feel like solving this problem is beyond my knowledge. I then tried condensing this to a simpler case with an isosceles triangle in 2D space, but I encountered a similar block as well. I know that I need ""scale"" as the output (specifically when scale was used in the first step), and my known values are the pentagon/hexagon side lengths, its position in 3D space, and the magnitude of each of the points. I would be very grateful for your guidance on this issue! Edit: This is the result I'm trying to achieve, using the following manually approximated scale factors (note that the hexagons are not planar): Subdivision | Pentagons | Hexagons 
------------|-----------|---------
     0      |     -     |    -   
     1      |  0.40706  |    -   
     2      |  0.44950  |  0.4874 
     3      |    ...    |   ...","['spheres', 'polyhedra', '3d', 'geometry', 'triangles']"
3991196,finding the limit $\lim_{x \to 0}[ \lim_{ y \to 0}x\sin\frac{1}{y}]$,"limit $$\lim_{x \to 0}[ \lim_{ y \to 0}f(x,y)]$$ Given : $$f(x,y) =
\left\{
	\begin{array}{ll}
		x\sin\frac{1}{y}  & \mbox{if } y \ne 0 \\
		0 & \mbox{if } y = 0
	\end{array}
\right.$$ this would mean since: $y\neq0$ we need to find : $$ \lim_{x\to 0}[\lim_{y\to0}x\sin\frac{1}{y}]$$ My approach was let the limit be L then it's clear that : $$ \lim_{x\to 0} -x \geq L \geq \lim_{x\to 0} x$$ So the limit must be 0. But it's given to be not defined.","['multivariable-calculus', 'calculus']"
3991201,Probability to be infected in series of meetings,"I have difficulty interpreting a simple probabilistic calculation and would be happy to help. We are in the midst of the Covid-19 epidemic and I am trying to assess someone's probability to be infected in a time window. For simplicity, I guess the probability of getting infected during a single session depends on its duration alone. In math terms: The probability to be infected in a given meeting is $P(t) = t\cdot\Theta$ , where $t$ is meeting duration and $\Theta$ is only a calibration parameter of the virus (and keep the probability under 1). Hence, the probability to be infected in a series of meetings is the complementary event to be not-infected in any meeting: $P(t_1...t_N) =  1-\prod_{k=1}^N(1-t_k\cdot\Theta)\tag{*}$ Does this inference sound reasonable? It seems to me that this is the material of a basic course in probability. Surprisingly and incomprehensibly to me, I saw in an article a calculation that actually reaches the following result for the same question (and the same assumptions): $P(t_1...t_N) = 1- \exp(-\Theta\sum_{k=1}^{N}t_{k})\tag{**}$ I do not understand what the relationship is between $(*)$ and $(**)$ and how in the calculation of a complementary event they arrived at the equation $(**)$ ? Thanks a lot!","['exponential-function', 'problem-solving', 'probability']"
3991322,Term-by-term antidifferentiation and power series representation of antiderivative,"I have solved the following exercise and I would like to know If I have made any mistakes (NOTE: I can't use integrals, since they are defined in a later chapter): Assume $f(x)=\sum_{n=0}^{\infty} a_n x^n$ converges on $(-R,R)$ . (a) $F(x)=\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1}$ is defined on $(-R,R)$ and satisfies $F'(x)=f(x)$ . (b) Antiderivatives are not unique. If $g$ is an arbitrary function satisfying $g'(x)=f(x)$ on $(-R,R)$ find a power series representation for $g$ . My solution: (a) Let $x\in (-R,R)$ : then the series $\sum_{n=0}^{\infty} a_n x^{n+1}$ is convergent in $(-R,R)$ since $\sum_{n=0}^{\infty} a_n x^n$ is convergent there by hypothesis and since $(\frac{1}{n+1})_{n=0}^{\infty}$ is a non-negative and decreasing sequence by Abel's Test we have that $\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1}$ is convergent in $(-R,R)$ too so $F(x)$ is defined on $(-R,R)$ . Since by Theorem 6.5.2 * $\sum_{n=0}^{\infty} a_n x^n$ converges uniformly on every interval $[-c,c]\subseteq (-R,R)$ , $0\in [-c,c]$ and $F(0)=\sum_{n=0}^{\infty}\frac{a_n}{n+1}0^{n+1}=0$ is convergent, by Term-by-Term Differentiability Theorem we have $F'(x)=\sum_{n=0}^{\infty} F'_n(x)=\sum_{n=0}^{\infty} a_n x^n=f(x)$ on each $[-c,c]\subseteq (-R,R)$ . (b) $g(x)=C +\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1}$ ( $C$ costant) since $g'(x)=0+(\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1})'\stackrel{\text{(a)}}{=}\sum_{n=0}^{\infty}a_n x^n=f(x)$ , as desired. *Theorem 6.5.2 If a power series converges absolutely at $x_0$ then it converges uniformly on the interval $[-|x_0|,|x_0|]$","['real-analysis', 'solution-verification', 'sequences-and-series', 'power-series', 'derivatives']"
3991338,Is there a nonrigid space X homeomorphic to X/Aut(X)?,"Is there a topological space $X$ satisfying Conditions (a) and (b) below? (a) $X$ has at least one nontrivial automorphism, (b) $X$ is homeomorphic to the quotient $X/\!\operatorname{Aut}(X)$ of $X$ by the action of its automorphism group.","['automorphism-group', 'examples-counterexamples', 'quotient-spaces', 'general-topology', 'group-actions']"
3991400,The group of additive sequences of integers,"Let $G$ be the additive group of the sequences $(a_n)_{n\ge 1} \subset \mathbb{Z}$ and let $f:G \to \mathbb{Z}$ be a group homomorphism. We denote by $e_i$ the sequence $(0,0,...,0,1,0,0...)$ (we just have a $1$ on the $i$ -th position, everything else is zero). Show that the set $\{i\ge 1 | f(e_i) \ne 0\}$ is finite. I thought that I should consider the set of sequences of finite support i.e. the subgroup generated by the $e_i$ s. Let's denote it by $H$ . We have that $f(H)$ is a subgroup of $\mathbb{Z}$ , so $f(H)=m\mathbb{Z}$ for some integer $m$ . We also have that $f(G)=n\mathbb{Z}$ for some positive integer $n$ . But I don't know how to proceed any further. I think that we should aim for a contradiction. I can only think about something having to do with divisibility, but I don't know.","['group-homomorphism', 'group-theory', 'abstract-algebra']"
3991406,Sequence of non-constant integer polynomials whose limit is constant,"Does there exist a sequence of non-constant polynomials $\big \{p_{n} \big \}_{n \geq 1}$ , each $p_n$ in $\mathbb{Z}[X]$ , such that $$ \lim_{n \to \infty} p_n =c $$ pointwise on $[0,1]$ , where $c$ is a constant? We can find such polynomials easily on the open interval $(0,1)$ , such as $p_n =x^n$ , however the closed interval does not seem as simple. The answer seems to be no, but I wouldn't know where to start.","['sequence-of-function', 'analysis', 'real-analysis', 'polynomials', 'limits']"
3991429,A so-called total-variation inequality,"Given real polynomial $P\left ( x \right )$ so that $P\left ( 0 \right )= P\left ( 1 \right )= 0, \int\limits_{0}^{1}\left | {P}'\left ( x \right ) \right |{\rm d}x= 1.$ For $x\in\left ( 0, 1 \right )$ prove or disprove that $\left | P\left ( x \right ) \right |< \frac{1}{2}.$ I read on Twitter a fleet that proves the following inequality with strictly decreasing $f\left ( x \right )$ for all $x\in \left [ 0, 1 \right ]$ by Cauchy-Schwarz $$\int_{0}^{1}f\left ( x \right ){\rm d}x+ \left ( \int_{0}^{1}xf\left ( x \right ) \right )^{2}+ 1> 0$$ I'm trying to find a strictly decreasing function from the Original Problem.. of course there exists no such $P\left ( x \right ), {P}'\left ( x \right ).$ Then replace it for $f\left ( x \right ).$","['integration', 'cauchy-schwarz-inequality', 'polynomials', 'integral-inequality']"
3991707,How were these much simpler derivatives derived (vs. my longer/messier ones)? Efficient/correct use of chain vs. product rule?,"I was trying to reverse engineer an equation from an article I was reading. The basic expressions I was working from were: $ε = \frac{1}{2}w_x^2\tag{1}$ $b_1ε_tw_{xx} + b_2ε_{tt}w_{xx} + b_1ε_{xt}w_x + b_2ε_{xtt}w_x\tag{2}$ $(2)=2b_1w_xw_{xx}w_{xt} + 2b_2w_xw_{xx}w_{xtt} + b_1w_x^2w_{xxt} + b_2w_x^2w_{xxtt}\tag{3}$ I found that I was able to get their equation (3) but only by using derivations of the $ε$ terms using a specific method I'm not sure of. Solely using the product rule applying all derivatives in a stepwise fashion we get the following set of equations we will call ""Set A"": [SET A] All derivatives via stepwise product rule: $ε = \frac{1}{2}w_x^2$ $ε_x = w_xw_{xx}$ $ε_t = w_xw_{xt}$ $ε_{xt} = w_{xt}w_{xx}+w_xw_{xxt}$ $ε_{tt} = w_{xt}^2 +  w_xw_{xtt}$ $ε_{xtt} = w_{xtt}w_{xx} + 2w_{xt}w_{xxt} + w_xw_{xxtt}$ If you substitute ""Set A"" into (2) you get: $2b_1w_xw_{xx}w_{xt} + b_2w_{xt}^2w_{xx} + 2b_2w_xw_{xx}w_{xtt} + b_1w_x^2w_{xxt} + 2b_2w_xw_{xt}w_{xxt} + b_2 w_x^2w_{xxtt}\tag{4}$ (4) at least superficially does not seem the same as (3). If however, we assume that you can apply a double derivative of the same nature (ie. $y_{tt}$ or $y_{xx}$ ) in one step via the product rule so that $ε_{tt} = (\frac{1}{2}w_xw_x)_{tt} = w_xw_{xtt}$ , we get ""Set B"" of definitions: [SET B] Apply dual derivatives in one step: $ε = \frac{1}{2}w_x^2$ $ε_x = w_xw_{xx}$ $ε_t = w_xw_{xt}$ $ε_{xt} = w_xw_{xxt} + w_{xt}w_{xx}$ $ε_{tt} = w_xw_{xtt}$ //DIFFERENT FROM SET A $ε_{xtt} = w_{xx}w_{xtt}+w_xw_{xxtt}$ //DIFFERENT FROM SET A If you substitute ""Set B"" into (2) you get (3) exactly, so these seem to be the derivatives they are using. However this creates the impossible equality: $ε_{tt} = w_xw_{xtt} = w_{xt}^2 +  w_xw_{xtt}\tag{5}$ Is ""Set B"" valid? Is it a manifestation of the chain rule perhaps? ie. $y_{tt} = (y_t)_t$ ? What about ""Set A""? I presume there is no way they can both be true. I wonder if the authors could have made a mistake doing this. It's perhaps possible I made some other error, but I think it would be incredibly coincidental that using ""Set B"" derives their terms identically. Everything else in our equations line up except this part. So I do presume ""Set B"" is what they did. I don't really understand the chain rule. I'm not that knowledgeable about derivatives. Which is mathematically valid: ""Set A"" or ""Set B""? Any clarification is appreciated. Thanks.","['partial-derivative', 'derivatives']"
3991725,Change of variables during differentiation,If we have a function: $\psi(x)$ and $x=\rho\gamma$ where $\rho$ is a constant Then is the following statement correct: $1)~~$ $\psi(x) =\psi(\rho \gamma)=\rho \psi(\rho)$ What is the equation for : $\frac{d\psi(\gamma)}{d\gamma}$ and $\frac{d^2\psi(\gamma)}{d\gamma^2}$ Below is my attempt at finding it: This question is the context of findings the solution of simple harmonic oscillator I am having trouble understanding how $9.19$ is derived from $9.18$ .,"['quantum-mechanics', 'change-of-variable', 'derivatives']"
3991819,"Prove that $g(x)=(x^2−1)^x$ is increasing on $(1,+\infty)$","Let $A = \mathbb{R}\setminus[−1,1]$ . Let $g : A\to\mathbb{R}$ be defined by $g(x)=(x^2-1)^x$ for all $x\in A$ . Prove that $g(x)=(x^2-1)^x$ is increasing on $(1,\infty)$ I have currently attempted to prove this by showing $g'(x)\geq 0 $ for all $ x\in A $ which gives $g'(x)= (x^2-1)^{x-1}(2x^2+(x^2-1)ln(x^2-1))$ which should be greater than or equal to $0$ however I am unsure how to show this or whether I have gone about this the right way. Any Help will be grateful.","['calculus', 'derivatives', 'real-analysis']"
3991884,deriving Stokes' theorem from substituting this identity,"As per requested: deriving Stokes' theorem from substituting this identity Making substitution with $ ∇ \times(\phi a) $ where $\phi$ is a scalar field, and $a$ is a vector field. Given that the identity is equal to: $ ∇ \times(\phi a) = \phi (∇ \times a) + (∇\phi)\times a$ then derive stokes theorem: $\int_{C}\phi dr = \iint_{S}n \times (∇\phi )dS$ for a surface $S$ with unit normal $n$ that is bounded by the closed curve $C$ , where the direction of $n$ obeys the usual convention. What I've tried: following from the hint $$\nabla \times (\phi \mathbf{a}) \cdot \mathbf{n} = (\nabla \phi \times \mathbf{a})\cdot \mathbf{n} = (\mathbf{n} \times \nabla \phi) \cdot \mathbf{a}$$ substituting this into the theorem above gives, $\int_{C} \phi \cdot \mathbf{a} dr=\iint_{S} (\mathbf{\hat{n}}\times\nabla\phi)\cdot\mathbf{a}dS$ Then taking out the constant a $\mathbf{a}\cdot(\int_{C}\phi dr)=\mathbf{a}\cdot(\iint_{S}\mathbf{\hat{n}}\times(\nabla\phi)dS) $ since a is arbitrary we deduce that: $\int_{C}\phi dr = \iint_{S}n \times (∇\phi )dS$","['multivariable-calculus', 'stokes-theorem', 'vector-analysis']"
3991900,"Continuous and differentiable function $f(x)$ in $(x_1, x_2)$.","Consider the continuous and differentiable function $f(x)$ in $[x_1, x_2]$ . Let $f'(x)$ be its derivative and $f'(x_2) = 0$ Show that there exist a number $c >0$ and a $x_3 \in (x_1, x_2)$ such that $f'(x_3) = c(f(x_3)-f(x_1))$ . Apologies but I haven't touched calculus for more than 20 years. One of the few things I remember is that when the 1st derivative is zero, we have a local minimum or maximum. (I am not a student or anything).","['calculus', 'functions']"
3991950,Deadly disease and probability to die,"NHS records show that of patients suffering from a certain disease, 75% die of it.
In a specific hospital, there are 100 patients. What is the probability that 25 to 65 will die? Clearly this is an example of use of binomial distribution. I don't know, however, how to deal with the range 25 to 65. If the question was for exactly 25 patients to die, it would be $P = \binom {100}{25}0.75^{25}.0.25^{75}$ but I don't know how to take into account the range. Thank you.","['combinatorics', 'probability']"
3991965,finding the limit of the integral of an unknown function,"I have a question from an exam written 18 years ago, and I can't solve it (I'm solving old questions to prepare for my own exam). The Question The question goes like so: let  g be a function, which has a derivative defined $\forall x\in [-1,1]$ , and $g(0)=0,g'(0)=1$ .
Does the limit $$\lim_{x\rightarrow0}\frac{\sin\left(\int\limits _{x^{3}}^{x^{2}}\left(\int\limits _{0}^{t}g\left(s^{2}\right)ds\right)dt\right)}{x^{8}}$$ converge? What have I tried? I thought about using L'Hôpital's rule, and F.T.C. so I wrote: $$\left(\int\limits _{x^{3}}^{x^{2}}\left(\int\limits _{0}^{t}g\left(s^{2}\right)ds\right)dt\right)'=2x\int\limits _{0}^{x^{2}}g\left(s^{2}\right)ds-3x^{2}\int\limits _{0}^{x^{3}}g\left(s^{2}\right)ds$$ but that didn't help much. I can even show that the limit is of the form $""\frac{0}{0}""$ to use the rule. I tested this on $g(x)=x$ and it seems like this limit does converge (to 0 if I'm not mistaken) I also thought Taylor was somehow related, but I really doubt it. I am also unsure of where the information we have about $g(0),g'(0)$ comes in. BTW this is what I got after applying L'Hôpital once $$\lim_{x\rightarrow0}\frac{\left(2\int\limits _{0}^{x^{2}}g\left(s^{2}\right)ds-3x\int\limits _{0}^{x^{3}}g\left(s^{2}\right)ds\right)\cos\left(\int\limits _{x^{3}}^{x^{2}}\left(\int\limits _{0}^{t}g\left(s^{2}\right)ds\right)dt\right)}{8x^{6}}$$","['integration', 'limits', 'calculus', 'derivatives']"
3991998,Does this type of real functions have a name ? What are their characteristics?,"Let $n > 0$ be an integer. Let $f:\mathbb R^+ \times \mathbb R^+ \rightarrow \mathbb R_0$ be a symmetric function such that the $(n+1) \times k$ matrix \begin{equation}
\mathbb M=
\begin{pmatrix}
f(s_{k-n+1},s_1) & \ldots & f(s_{k-n+1},s_k)\\
\vdots &  & \vdots \\
f(s_{k+1},s_1) & \ldots & f(s_{k+1},s_k)\\
\end{pmatrix}
\end{equation} has at most $n$ linearly independent rows for any $k>n$ and real sequence $s_1<s_2<\ldots<s_{k+1}$ . What does that say about $f$ ? Do such function have a name ? What are their characteristics/properties ? Is there an easier ""definition"" that would be equivalent, without matrices for example? Or a stronger ""easier"" property that would imply the above property ?","['analysis', 'real-analysis', 'matrices', 'linear-algebra', 'functional-analysis']"
3992000,Hints for two proofs,"Please do not give me the answers. I only want hints to approach these two proof problems I'm struggling with: There exists some differentiable function $f(x)$ such that $f'(x)\ne f(x)$ but $f''(x)=f(x)$ There exists some $x\in \mathbb R$ such that $x^2\in \mathbb \{R -\mathbb Q\}$ while $x^4\in \mathbb R$ For the first one I started out defining $f'$ as a limit but I couldn't come up with any way to utilize this for my proof. For the second one I've tried doing it by cases, considering $x<0, x=0, x>0$ . But I didn't get anywhere doing this.","['algebra-precalculus', 'proof-writing', 'logic', 'discrete-mathematics']"
3992042,Solutions of $x'' + 2x' + x = 0$ such that $x(1) = 0$,"I am trying to solve this question from an older exam that my teacher uploaded. The marks given for answering this question were very low, so I am thinking there is an easy way to solve it.
The question asks to find the solutions of $x'' + 2x' + x = 0$ such that $x(1) = 0$ .
My approach is using the change $y=x'$ , and therefore getting: $$
x' = y
$$ $$
y' = -x -2y
$$ Which is a linear system. What I would normally do here is finding the eigenvalues (in this case there is only one eigenvalue, -1 with eigenvector (1,-1)), and solve the corresponding system using the Jordan matrix and the Jordan basis, but I was thinkin whether there was a faster approach, maybe playing with the fact that $x(1)=0$ , or something else. Thanks",['ordinary-differential-equations']
3992056,What is wrong with my 'proof' of $i=1$?,"So I recently learnt about $i$ and I can't wrap my head around the concept of $i^2=-1$ or that $\sqrt{-1}$ can even exist. Today I was thinking about $i$ again and thought of a ""proof"" that $i=1$ . $$i^4=1 \text{ and } 1^4=1 \text{ so } i^4=1^4$$ $$i^4=1^4 \to \sqrt[4]{i^4}=\sqrt[4]{1^4} \to i=1$$ But if $i=1$ , then $i^2 \neq -1$ . So I think I must have messed up something in the proof. Can someone point out where this went wrong? I know you can $\text{""prove"" }1=2$ by accidentally dividing by $0$ and I suspect something similar is happening. For anyone else having trouble with complex numbers @mrsamy commented this link and I found it quite helpful: https://www.math.toronto.edu/mathnet/answers/imaginary.html","['algebra-precalculus', 'fake-proofs', 'complex-numbers', 'arithmetic']"
3992094,Can every vector field be written as a Lie bracket of two other vector fields?,"Let $M$ be a $n$ -dimensional smooth manifold, and $X$ a (globally defined) smooth vector field on $M$ . Is it always possible to write $X$ as a Lie bracket $[X_1,X_2]$ of two other globally defined smooth vector fields? Clearly this is possible locally: Choosing a coordinate patch $O$ diffeomorphic to $[0,1]^n$ , we can write $X = \sum_{j=1}^nX^j \frac{\partial}{\partial x^j}$ and solve $[\frac{\partial}{\partial x^1}, Y] = \sum_{j=1}^n (\frac{\partial Y^j}{\partial x^1}) \frac{\partial}{\partial x^j} = \sum_{j=1}^n X^j \frac{\partial}{\partial x^j}$ to obtain $X$ as a Lie bracket $[\frac{\partial}{\partial x^1}, Y]$ . But, globally, a nowhere vanishing vector field to play the role of $\frac{\partial}{\partial x^1}$ in the previous calculation need not exist, and even if it exists, the resulting ODE for the coefficients of $Y$ might run into topological trouble. To give another simple example, the question has a positive answer for the unit circle $\mathbb{S}^1$ as well: Every vector field $X = f(\theta)\frac{\partial}{\partial \theta}$ can be written as a Lie bracket by the following trick: Let $c = \int_0^{2\pi} f(\theta)d\theta$ , and $\gamma(\theta) = \frac{2\pi}{c} \int_0^\theta f(\theta)d\theta$ . Then $\sin\circ \gamma$ and $\cos\circ \gamma$ are smooth functions on $\mathbb{S}^1$ , and $$\left[\frac{c}{2\pi}\sin(\gamma(\theta))\frac{\partial}{\partial \theta}, \cos(\gamma(\theta))\frac{\partial}{\partial \theta}\right] = \frac{c}{2\pi}\left(\sin(\gamma(\theta))^2+\cos(\gamma(\theta))^2\right) \gamma'(\theta) \frac{\partial}{\partial \theta} =  f(\theta)\frac{\partial}{\partial \theta}.$$ It is not possible to write $\frac{\partial}{\partial \theta}$ as $[\frac{\partial}{\partial \theta},X]$ for any vector field $X$ , on the other hand.","['vector-fields', 'differential-geometry']"
3992122,Is the union axiom really needed to prove existence of intersections?,"I once asked a while ago how to prove that the intersection $\bigcap S$ of any non-empty set $S$ exists, using the ZFC axioms. The answer involved the union axiom. However, I don't think it is necessary. Since the intersection of $S$ is a subset of an element $s$ of $S$ , I think all that is needed is the axiom schema of separation. Is this true? Or is the union axiom essential?",['elementary-set-theory']
3992138,Solving for $x$: $a \sin(bx + c) = \sin(x)$,"Ok, I scoured the internet for more than a few months for this one (whenever I had the time). But just because of this question I've created my first Stack Exchange account. I have thrown this at all Computer Algebra System software I was accessible to. (not Maple or Mathematica though) I also tried complex transformation, no luck there (not too much experience there) I know this could be easily numerically solved because it is guaranteed that there is exactly one solution for every interval when: $a \sin(bx + c) \le 1$ . In my case, the general solution is crucial because I'm applying this to an optimization where I can't afford to have an additional dimension to be optimized. This problem can be visualized as the intersection of two sine waves. A simplified version of the problem (when a = 1) could be easily solved with the sum-to-product identity: $\sin(bx + c) = \sin(x)$ $0 = \sin(bx + c) - \sin(x)$ $0 = 2 \cos(\frac{x(b + 1) + c}{2})\sin(\frac{x(b - 1) + c}{2})$ $0 = \cos(\frac{x(b + 1) + c}{2})$ and $0 = \sin(\frac{x(b - 1) + c}{2})$ etc... But for the time being, the only valid assumption in my case is that b is rational",['trigonometry']
3992233,Matrix equation $X+AX-XA^2=0$ has only zero solution,"Let $A$ be a $n\times n$ complex matrix, with even numbers as eigenvalues. Show that the matrix equation $X+AX-XA^2=0$ has only zero solution. My attempt is just consider an eigenvector $v$ of $A$ , then $Xv+AXv-4m^2Xv=0.$ Then what to do? Or just $v'(X+AX-XA^2)v=0$ . But $A$ is not symmetric, how to proceed?","['matrices', 'matrix-equations', 'linear-algebra', 'eigenvalues-eigenvectors']"
3992267,Integral inside a limit?,"Problem gives me a review of FTC2, saying $\frac{d}{dx}(\int_{a}^{x}f(t)\,dt) = f(x)$ then tells me: For $f(t)$ continuous, calculate the limit. $$\lim_{h\to 0}1/h\int_{h}^{2h}f(t)\,dt$$ I started by putting it in the format of $\lim_{h\to 0}\frac{F(2h)-F(h)}{h}$ , but now I am kind of stuck. I'm not sure what to do next. Thank you for any help in advance!","['limits', 'calculus', 'definite-integrals']"
3992305,How to understand orbit of size $1$ in this case,"I am a self-studying beginner in group theory, so please bear with this question which could have some simple answers. Given a $p$ -group $G$ for some prime $p$ , let $H$ be a subgroup of $G$ . Let $X$ be the set of all conjugates of $H$ . Now, $H$ acts on $X$ by conjugation. I read that there are at least $p$ orbits of size $1$ in $X$ . One example of an orbit with size $1$ is $\{H\} \in X$ . This example follows since $aHa^{-1}=H$ for any $a \in H$ since $H$ is a subgroup, and we have $\text{Orb}(H)=H$ . But I read that since $p$ is prime, that there are at least $p-1$ other orbits of size $1$ . So there should be another orbit $gHg^{-1} \neq H$ of size $1$ in $X$ . What I don't understand is how $gHg^{-1}$ could be of size $1$ under the action of $H$ . Shouldn't this mean that $\text{Orb}(gHg^{-1})=\{agHg^{-1}a^{-1} | a \in H\}$ and $\text{Orb}(gHg^{-1})$ may not necessarily be equal to $gHg^{-1}$ . However, it should have size $1$ , which means that $\text{Orb}(gHg^{-1})$ should in fact be equal to $gHg^{-1}$ . For reference, this result came from Rotman's Theorem 4.6, where no extra conditions were imposed on $H$ and $G$ except that $H$ is a subgroup of the $p$ -group $G$ ... What am I missing here?","['group-theory', 'abstract-algebra']"
3992322,Monty Hall Problem with Multiple Players?,"I understand the common Monty Hall Problem and why switching provides a 2/3 chance of winning, but I'm having trouble wrapping my head around how the probabilities work when multiple players are involved, as their probabilities seem to be contradictory. Let's say we have two players and four doors.
Each door has a 1/4 probability of hiding a prize.
Let's say contestant 1 chooses door A and contestant 2 chooses door B.
Door D is then revealed to be empty, leaving A, B, and C.
From contestant 1's perspective the odds are 1/4 for door A, 3/8 for door B, and 3/8 for door C.
But from contestant 2's perspective the odds are 3/8 for door A, 1/4 for door B, and 3/8 for door C. What are the actual odds for each door hiding the prize, does it change depending on if they know each other's choices, and does it differ from the normal example of the problem? If so why, and if not, why not?","['monty-hall', 'probability']"
3992426,Three equilateral triangles,"Can you provide a proof for the following claim: Claim. Given an arbitrary equilateral triangles $\triangle ABC$ and $\triangle BDE$ with common vertex $B$ . The points $H$ and $I$ divide line segments $CE$ and $AD$ respectively in the ratio $2 : 1$ . Let $G_1$ be the centroid of triangle $\triangle ABC$ . Then triangle $\triangle G_1IH$ is an equilateral triangle as well. GeoGebra applet that demonstrates this claim can be found here . So far I have managed to prove that $|AE|=|CD|$ and that line segments $AE$ and $CD$ intersect each other at angle of $60^{\circ}$ , but I dont know if this can be of any use. Also, can we apply the fundamental theorem of similarity in some way?","['euclidean-geometry', 'triangles', 'geometry']"
3992433,What is the expectation of $B_s^2B_t^2$ where $B_t$ is a standard Brownian motion?,"I am tring to find the variance of the integral $I=\int_0^T B_t^2 dt$ . I have found that $\mathbb{E}[I] = \int_0^T \mathbb{E}[B_t^2] dt = \frac{T^2}{2}$ . Since $\mathbb{E}[I^2] = \mathbb{E}[\int_0^T \int_0^T B_t^2 B_s^2 \ dt\ ds] = \int_0^T \int_0^T \mathbb{E}[B_t^2 B_s^2] \ dt \ ds$ , I want to know the value of $\mathbb{E}\left[B_t^2 B_s^2\right]$ .","['expected-value', 'stochastic-processes', 'brownian-motion', 'probability']"
3992467,a question on $\epsilon$ chain sequences,"I want to prove that if $X$ is a compact metric space and $f:X\to X$ is a homeomorphism which has a unique chain class $C$ then $X=C$ . Chain recurrence: Suppose $X$ is a compact metric space and $ f : X \to X$ is a homeomorphism.
By an $\epsilon$ -chain from $x$ to $y$ in $X$ we mean a finite sequence $x_0 ,x_1 ... , x_k$ in $X$ with $x_0=x$ and $x_k=y$ such that : \begin{align}
d(f(x_n) , x_{n+1})< \epsilon \; , \; n=0 ,..., k-1
\end{align} a point $x \in X$ is said to be chain recurrent of $f$ , if for any $\epsilon>0$ there is an $\epsilon$ -chain from $x$ to $x$ . Chain class: The set of chain recurrent points of $f$ is called the chain recurrent set of $f$ , denoted $CR(f)$ . Two points $x,y \in CR(f)$ are called chain equivalent, written $x \sim y$ if for any $\epsilon >0$ there is an $\epsilon$ -chain from $x$ to $y$ and an $\epsilon$ -chain from $y$ to $x$ . This is equivalence relation on $CR(f)$ . Each equivalence class is called a chain class. My attempt: By contradiction suppose $X \neq C$ we know that always $C \subseteq X$ so we have to prove that $X \subseteq C$ so by a contradiction we mean $X \not\subseteq C$ so there exists a point $x \in X$ such that it is not in $C$ this means there exists an $\epsilon>0$ such that there is no $\epsilon$ -chain between $x$ and the $m$ if we take $C=[m]$ , I think we should find another chain class of $f$ to reach a contradiction. Could you please give me a hint if my solution is right or if not give me a solution? My guess:
We have the following properties that I think they are useful: 1- If $f$ is connected and $CR(f)=X$ then $X$ is a chain class. 2- Any chain class is indecomposable, It means It can not be decomposed into a disjoint union of two non-empty compact invariant sets.","['general-topology', 'analysis', 'dynamical-systems']"
3992475,Range of $\frac{\cos\theta_1+\cdots+\cos\theta_{10}}{\sin\theta_1+\cdots+\sin\theta_{10}}$ given $\sin^2\theta_1+\cdots+\sin^2\theta_{10}=1$,"Given that for $ \theta_i \in \left[0, \dfrac{\pi}{2}\right]$ , where $1 \le i \le 10$ , $\sin^2\theta_1+\sin^2\theta_2+\cdots+\sin^2\theta_{10}=1$ , find the minimum and maximum value of $$\dfrac{\cos \theta_1+\cos\theta_2+\cdots+\cos\theta_{10}}{\sin\theta_1+\sin\theta_2+\cdots+\sin\theta_{10}}$$ I tried using Cauchy Schwarz inequality which just gives $\displaystyle \sum_{i=1}^{10} \sin\theta_i \le \sqrt{10}$ and $\displaystyle \sum_{i=1}^{10} \cos \theta_i \le \sqrt{90}$ because $\displaystyle \sum_{i=1}^{10}\cos^2\theta_i =9$ but I don't think it would be helpful here. Maybe some inequality can be applied by writing it as $$\dfrac{\displaystyle \sum_{i=1}^{10} \sqrt{1-\sin^2\theta_i}}{\displaystyle \sum_{i=1}^{10} \sin \theta_i}$$ Any hints would be appreciated!","['inequality', 'a.m.-g.m.-inequality', 'cauchy-schwarz-inequality', 'trigonometry', 'algebra-precalculus']"
3992504,"Prove that $f\left ( x \right )- x^{2021}$ always has at least one root $x_{0}\in\left ( 0, 1 \right )$","Given positive continuous function $f\left ( x \right )$ on the interval $\left [ 0, 1 \right ]$ so that $\int_{0}^{1}f\left ( x \right ){\rm d}x< \frac{1}{2022}.$ Prove that $f\left ( x \right )- x^{2021}$ always has at least one root $x_{0}\in\left ( 0, 1 \right ).$ Suppose that there no exists $x_{0}\in \left ( 0, 1 \right )$ as the statement, let $g\left ( x \right )= f\left ( x \right )- x^{2021}.$ Because $f\left ( x \right )$ is a continuous function on $\left [ 0, 1 \right ]$ so $g\left ( x \right )$ too, then $g\left ( x \right )= kF\left ( x \right )$ with $F\left ( x \right )> 0, k= constant.$ On the other hand $g\left ( 0 \right )= f\left ( 0 \right )> 0\Rightarrow k> 0\Rightarrow g\left ( x \right )> 0,$ let $G\left ( x \right )= \int_{0}^{1}g\left ( x \right ){\rm d}x.$ We obtain ${G}'\left ( x \right )> 0,$ it reminds me the following inequality with strictly decreasing $f\left ( x \right )$ for all $x\in \left [ 0, 1 \right ]$ by Cauchy-Schwarz $\int\limits_{0}^{1}f\left ( x \right ){\rm d}x+ (\int\limits_{0}^{1}xf\left ( x \right ))^{2}> -1.$ How should I try next ? I need to the help, thank you.","['integration', 'roots', 'cauchy-schwarz-inequality', 'functions', 'boundary-value-problem']"
3992536,ODE - Floquet theorem recommendations,Can someone please refer me to a good textbook or some papers about the Floquet Theorem (English)? Thanks,['ordinary-differential-equations']
3992557,How can I construct a variation problem whose solution is $f^{(3)}=0$?,"For example, for the variation problem $$
\min_{f\in H([a,b])} \int_a^b f''^2(x) \mathrm{d} x
$$ Based on Euler-Lagrange, we can obtain $$
f^{(4)}=0
$$ Now I need to find a variation problem whose solution is $$
f^{(3)}=0
$$ However, it seems really hard to construct such a variation problem. Something like $\min \int f' f'' \mathrm{d} x$ and $\min \int f f^{(3)} \mathrm{d} x$ cannot work. Could anyone please help me with that?","['calculus-of-variations', 'euler-lagrange-equation', 'spline', 'functional-analysis', 'optimization']"
3992565,Computing time of flight in a medium with a varying index of refraction,"The index of refraction , $n$ , for some medium, is the speed of light in a vacuum divided by the speed of light in the medium ( $n = \frac{c}{v}$ ). Consider some light source , $p_s$ (at coordinates $[x_p,y_p,z_p]$ ), and some light receiver , $p_r$ (at coordinates $[x_r,y_r,z_r]$ ). Neglecting the effects of refraction, we might calculate the time of flight for a light signal, moving in a straight line from $p_s$ to $p_r$ , as: $T = \frac{1}{c} \cdot \sqrt{(x_p - x_r)^2 + (y_p - y_r)^2 + (z_p - z_r)^2 }$ For a constant index of refraction, we simply replace $c$ by $\frac{c}{n}$ . How, however, might we calculate the time of flight for a light signal traveling in a ""straight line"" when the index of refraction varies as a function of some coordinate (say, depth ( $z$ ))? In this case, of course, light doesn't actually travel in a straight line, in the common sense. What I have so far... Note: This could be very wrong. It's just where I've gotten so far. Using the principle of least time (see), we have that: $T = \int dt$ We have that $v = \frac{ds}{dt}$ . Thus, $T = \int \frac{ds}{v}$ Suppose $n$ is a function of $z$ . Then, $n = n(z)$ . Using $v = \frac{c}{n(z)}$ , we have that: $T = \int \frac{n(z)}{c} ds$ Now, by Pythagoras $ds^s = dx^2 + dy^2 + dz^2$ . So, $ds = \sqrt{dx + dy +dz}$ . Then, $T = \int \frac{n(z)}{c} \sqrt{dx + dy + dz}$ What I want, in the end, is to have a formula such that I can plug in $[x_p,y_p,z_p]$ , $[x_r,y_r,z_r]$ , $n(z)$ , $c$ , and get $T$ . EDIT I've found a reference, here. Unfortunately, I don't quite follow everything. I recreate the argument here. I use the above example, where $n$ is a function of depth, $z$ . We simplify this to a problem in 2-dimensions. We know that the index of refraction, $n$ , at a point $z,x$ is equal to $c/v$ , where $v$ is the velocity of light at that point. We parameterize the path by equations $x = x(u)$ and $z = z(u)$ . The ""optical path length"" from a point $A$ to a point $B$ is then given by: $$L = \int_A^B n\;ds = \int_A^B n\;\sqrt{z^2 + x^2}\;du$$ I don't quite follow what is being done at this point. I know that we want to minimize $L$ ... I'm not sure, however, how this step achieves this. To make this integral an extremum, let $f$ denote the integrand function: $f(z,x,\dot{z},\dot{x}) = n(z,x)\sqrt{z^2+x^2}$ Then the Euler equations are: $$\frac{\partial n}{\partial z} = \frac{d}{du}\Bigg(\frac{\partial f}{\partial \dot{z}}\Bigg)$$ $$\frac{\partial n}{\partial x} = \frac{d}{du}\Bigg(\frac{\partial f}{\partial \dot{x}}\Bigg)$$ Which gives: $\frac{\partial n}{\partial z} \sqrt{\dot{z}^2 + \dot{x}^2} = \frac{d}{du} \Bigg[\ \frac{n\dot{z}}{\sqrt{\dot{z}^2 + \dot{x}^2}}\Bigg]$ $\frac{\partial n}{\partial x} \sqrt{\dot{z}^2 + \dot{x}^2} = \frac{d}{du} \Bigg[\ \frac{n\dot{x}}{\sqrt{\dot{z}^2 + \dot{x}^2}}\Bigg]$ If we define parameter $u$ as spatial path length $s$ , then $z^2 + x^2 = 1$ , and the above equations reduce to: $$\frac{\partial n}{\partial z} = \frac{d}{ds}\Bigg(n \frac{dz}{ds}\Bigg)$$ $$\frac{\partial n}{\partial x} = \frac{d}{ds}\Bigg(n \frac{dx}{ds}\Bigg)$$ I could be looking at this wrong, however this is my interperetation... So, since (in my example) $n$ is a function of $z$ , it occurs to me that we can drop the partial derivatives, and have: $$\frac{dn}{dz} = \frac{d}{ds}\Bigg(n \frac{dz}{ds}\Bigg)$$ Then: $$L = \frac{d}{ds} = \frac{\frac{dn}{dz}}{\Bigg(n \frac{dz}{ds}\Bigg)}$$ Suppose $n(z) = e^z$ , for example... $$L = \frac{e^z}{\Bigg(e^z \frac{dz}{ds}\Bigg)}$$ $\frac{dz}{ds}$ should be a differential equation, yes? Perhaps I'm being slow about this, but it isn't immediately clear to me how I'd then use starting and ending values for $x$ , $y$ , and $z$ to obtain an $L$ . As I recall, the derivative can't simply be ""flipped"" (i.e. this isn't $= \frac{ds}{dz}$ )... but maybe I'm wrong.","['integration', 'calculus-of-variations', 'calculus', 'physics', 'mathematical-physics']"
3992573,coordinate form of an affine transformation,"I am having trouble with this seemingly simple question: Write the standard coordinate form of an affine transformation in $\mathbb{A}^{2}(\mathbb{R})$ that maps
the point (1, −2) to the point (0, 10), and the lines $10x_{1} − 4x_{2} = 1$ and $3x_{1} − 3x_{2} = −7$ to the lines $x_{1} − 2x_{2} = −3$ and $x_{1} − x_{2} = 6$ , respectively. Now as I understand for the point transformation $\begin{pmatrix}0\\10\\0\end{pmatrix}=\begin{pmatrix}a_{11} & a_{12} & b_{1}\\a_{21} & a_{22} & b_{2}\\0 & 0 & 1\end{pmatrix}\begin{pmatrix}1\\-2\\0\end{pmatrix}$ and I need to find this matrix of transformation, this matrix equation gives me two equations with 6 unknowns. So I guess 4 more equations will be coming from the transformations of the two lines. Now here is where I am having trouble, I don't know how to get those equations. Any kind of hints would be really helpful. Thanks in advance.","['euclidean-geometry', 'linear-algebra', 'geometry', 'affine-geometry']"
3992575,Planar graph with $|V|\ge3$ has the average degree of vertices smaller than $6$,"$G$ is a planar graph with $|V|\geq 3$ . How to show that $G$ has an average degree of vertices smaller than 6? Would it be enough to note that if $G$ has less then 6 nodes the average cannot be over 5 and the existence of connected components further decreases the $\frac{\sum_{v\in V}deg(v)}{|V|}$ , so w.l.g. it can be proved for a connected graph (component) and extended to all connected components if needed? Proof: If $G$ has no cycles it is a tree. $|E|=|V|-1$ and that is smaller than $|V|\leq |V|+2|V|-6 = |3|V-6$ so $$|E|\leq |3|V-6$$ If $G$ has cycles: we note that each face is bounded by 3 edges and the sum of the degrees of the faces has to be at least three times number of all faces: $$\sum_{f\in F}deg(f)\geq3|F|$$ Furthermore, each edge bounds at most two faces so $$\sum_{f\in F}deg(f)\leq2|E|$$ We would have $3|F|\leq2|E|$ . Using Euler's formula we get $$3|F|=3|E|-3|V|-6$$ after multiplying it by scalar and combining both we get: $2|E|\geq 3|E|-3|V|-6$ and $$|E|\leq 3|V|-6$$ Let's suppose that the degree of each node in $G$ is at least 6 (contraposition). $$\frac{\sum_{v\in V}deg(v)}{|V|} \geq 6 \\  2|E|\geq 6|V| \\ |E|\geq 3|V|$$ But we have just shown that $|E|\leq 3|V|-6$ , so $2|E|\leq 6|V|-12$ . This two cannot be true at the same time for graph/connected component with $|V|\geq 6$ .
If $G$ isn’t connected, the result still holds, because we can apply our proof to each connected component individually.","['graph-theory', 'solution-verification', 'discrete-mathematics', 'planar-graphs']"
3992608,Closed form of a complicated series,"Consider the series $$\sum_{m=0}^{\infty}|G_{m+1}|\sum_{k=0}^{m}(-1)^{k}\binom{m+1}{k+1}f(s,k+1+\delta)$$ Where $|G_{m+1}|$ are the absolute Gregory's coefficients, $0<\delta<1$ and : $$f(s,x)=\frac{x^{-s}}{2\pi i}\left[x^{\frac{2\pi i}{\log j}}\Phi\left(x^{\frac{2\pi i}{\log j}},1,1-\frac{\log j}{2\pi i}s\right)-x^{-\frac{2\pi i}{\log j}}\Phi\left(x^{-\frac{2\pi i}{\log j}},1,1+\frac{\log j}{2\pi i}s\right)\right]$$ where $\Phi(\cdot,\cdot)$ is the Lerch transcendent, $s\in \mathbb{C}$ , and $j\in \mathbb{Z^{+}}$ . I'm seeking a closed form for this series. I tried expanding $f(s,x)$ as : $$f(s,x)=\frac{x^{-s}}{s\log j}+x^{-s}\sum_{l\in \mathbb{Z}}\frac{x^{\frac{2\pi i l}{\log j}}}{2\pi i l-s\log j}\;\;\;\;\;(A)$$ and i know how to evaluate: $$\sum_{m=0}^{\infty}|G_{m+1}|\sum_{k=0}^{m}(-1)^{k}\binom{m+1}{k+1}(k+1+\delta)^{z}\;\;\;\;z\in \mathbb{C}$$ but the resulting series (A) is only conditionally converging, and i can't reverse the order of the summation. Any help is highly appreciated. EDIT : we have that : $$\sum_{k=0}^{m}(-1)^{k}\binom{m+1}{k+1}e^{-kx}=(1-e^{x})(1-e^{-x})^{m}+e^{x}$$ Using the generating function of Gregory coefficients : $$\frac{y}{\log(1+y)}=1+\sum_{m=1}^{\infty}G_{m}y^{m}\;\;\;\;\;|y|<1$$ we have : $$\sum_{m=0}^{\infty}|G_{m+1}|(1-e^{-x})^{m}=\frac{1}{1-e^{-x}}-\frac{1}{x}$$ Thus : $$\sum_{m=0}^{\infty}|G_{m+1}|\sum_{k=0}^{m}(-1)^{k}\binom{m+1}{k+1}e^{-kx}=\frac{e^{x}-1}{x}$$ Where we used : $$\sum_{m=0}^{\infty}|G_{m+1}|=1$$ Now we have : $$(1+\delta+k)^{-z}=\frac{1}{\Gamma(z)}\int_{0}^{\infty}e^{-(1+\delta+k)x}x^{z-1}dx\;\;\;\;\Re(z)>0$$ Thus : $$\sum_{m=0}^{\infty}|G_{m+1}|\sum_{k=0}^{m}(-1)^{k}\binom{m+1}{k+1}(1+\delta+k)^{-z}=\frac{1}{\Gamma(z)}\int_{0}^{\infty}(e^{-\delta x}-e^{-(1+\delta)x})x^{z-2}dx$$ $$=\frac{1}{\Gamma(z)}\left(\delta^{1-z}\Gamma(z-1)-(1+\delta)^{1-z}\Gamma(z-1)\right)=\frac{1}{z-1}\left(\delta^{1-z}-(1+\delta)^{1-z}\right)$$ But it can be easily verified that the series above converges everywhere. Thus : $$\sum_{m=0}^{\infty}|G_{m+1}|\sum_{k=0}^{m}(-1)^{k}\binom{m+1}{k+1}(1+\delta+k)^{z}=\frac{1}{z+1}\left[(1+\delta)^{1+z}-\delta^{1+z}\right]\;\;\;z\in \mathbb{C}$$","['complex-analysis', 'special-functions', 'sequences-and-series']"
3992613,"How can a discrete stochastic integral be used to show that $(X_n)$ submartingale, $S\le T\implies E(X_S)\le E(X_T)$","I am just learning about martingales and discrete stochastic integrals. For completeness, we define a discrete stochastic integral as $$Y_n=\sum_{k=1}^nC_k(X_k-X_{k-1})=(C\bullet X)_n$$ for $(X_n)$ adapted and $C_n$ previsible in this context. An exercise from this section says we can use a discrete stochastic integral to show that if $(X_n)$ is submartingale and $S, T$ are a.s. bounded stopping times with $S\le T$ then $E(X_S)\le E(X_T)$ . Because of the boundedness of the stopping times, I thought this looked more like an optional stopping problem, but the exercise clearly states that this can be shown using a suitable discrete stochastic integral. Does someone know how the stochastic integral relates to this problem? EDIT: The only result I already have for the discrete stochastic integral is
if $C_n\ge 0$ then $(C\bullet X)_n$ is a submartingale as well. Perhaps I need to define a discrete stochastic integral and then apply optional stopping to that?","['stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
3992654,Fourier Transform of $\frac{1}{1+|x|^\alpha}$,"I'm trying to figure out either of these two things: $\int_w \frac{e^{iwt}}{1+|w|^\alpha}$ or $\sum_{n \in Z} \frac{e^{int}}{1+|n|^\alpha}$ , for $0<\alpha<1$ I think I know roughly what the asymptotics would look like for small t (UV): one can do a inverse fourier transform of $|w|^{-\alpha}$ and the answer would look like $\propto |t|^{\alpha-1}$ . Any help would be appreciated, thank you!","['complex-analysis', 'calculus', 'fourier-analysis', 'analysis']"
3992663,Some calculus Prove/Disprove question,"I have 2 questions that I would like to make sure I did the first one right, and get some tips for the next one :) Prove or disprove: Let $f: \mathbb{R} \to \mathbb{R}$ be a differentiable function. If the equation $f'(x)=0$ has exactly one solution, then the equation $f(x)=0$ has at least two solutions. I think this one is false, because I can take $f(x)=x^2$ . It is clear that the function is differentiable, and $f'(x)=2x$ , so the equation $f'(x)=0$ has only one solution as needed, when $x=0$ . And the equation $f(x)=0$ has only one solution, when $x=0$ , which is in contradiction to the fact that $f(x)=0$ has at least two solutions. -- Let $f:[a,b) \to \mathbb{R}$ be a differentiable function such that $\lim_{x \to b^-}f(x)$ does not exist in the extended sence.
Then  there exists an $x_{0} \in [a,b)$ for which $f'(x_{0})=0$ . So I have tried to think for functions that I know which for their limits does not exist, like $\cos (\frac{1}{x})$ , and started playing with them, but no matter what I did, $f'$ always had a point $x_{0}$ which for $f'(x_{0})=0$ . So I think that this statement is true, but if you guys can please enlighten me, I will very appreciate that! Thanks a lot!","['calculus', 'functions', 'solution-verification', 'limits', 'derivatives']"
3992667,Find the derivative of $f(x)=x^{x^{\dots}{^{x}}}$.,"Find the derivative of $f(x)$ : $$f(x)=x^{x^{\dots}{^{x}}}$$ Let $n$ be the number of overall $x's$ in $f(x)$ . So for $n=1$ , $f(x)=x$ . I then tried to determine a pattern by solving for the derivative from $n=1$ to $n=5$ . Here's what I got: \begin{align}
n = 2 \Longrightarrow f(x) &= x^x \\
f'(x) &= \frac{d}{dx}\left(e^{x\ln \left(x\right)}\right) \\
&= e^{x\ln \left(x\right)}\frac{d}{dx}\left(x\ln \left(x\right)\right) \\
&= e^{x\ln \left(x\right)}\left(\ln \left(x\right)+1\right) \\
&= x^x\left(\ln \left(x\right)+1\right)
\end{align} \begin{align}
n = 3 \Longrightarrow f(x) &= x^{x^{x}} \\
f'(x) &= \frac{d}{dx}\left(e^{x^x\ln \left(x\right)}\right) \\
&= e^{x^x\ln \left(x\right)}\frac{d}{dx}\left(x^x\ln \left(x\right)\right) \\
&= e^{x^x\ln \left(x\right)}\left(x^x\ln \left(x\right)\left(\ln \left(x\right)+1\right)+x^{x-1}\right) \\
&= x^{x^x}\left(x^x\ln \left(x\right)\left(\ln \left(x\right)+1\right)+x^{x-1}\right)
\end{align} \begin{align}
n = 5 \Longrightarrow f(x) &= x^{x^{x^{x^{x}}}} \\
f'(x) &= ... \\
&= x^{x^{x^{x^x}}}\left(x^{x^{x^x}}\ln \left(x\right)\left(x^{x^x}\ln \left(x\right)\left(x^x\ln \left(x\right)\left(\ln \left(x\right)+1\right)+x^{x-1}\right)+x^{x^x-1}\right)+x^{x^{x^x}-1}\right)
\end{align} However, I am not sure if I see a pattern here that can help solve the question.","['real-analysis', 'complex-analysis', 'calculus', 'algebra-precalculus', 'derivatives']"
3992691,Transformation of the metric tensor under an active transformation,"What is the general transformation law of the components of the metric tensor $g_{\alpha\beta}$ on a parametric surface $x(u,v) = (x_1(u,v), x_2(u,v), x_3(u,v))$ under an active transformation $x_{new}(u,v) =(f_1(x_{1},x_{2},x_{3}), f_2(x_{1},x_{2},x_{3}), f_3(x_{1},x_{2},x_{3}))$ ? The functions $f_1, f_2, f_3$ might be different. Note that we are not changing the curvilinear coordinates--we are changing the parametric surface itself. Denoting $x'_{i} = f_{i}(\textbf{x})$ and using the chain rule, we can find the components of the new metric tensor using the equation: \begin{equation}
g'_{\alpha\beta} = \sum_{i=1}^{3} \frac{\partial x'_{i}}{\partial u^{\alpha}} \frac{\partial x'_{i}}{\partial u^{\beta}}.
\end{equation} Using the chain rule and some algebra, it's easy to rewrite the equation above in terms of the Jacobian matrix $\textbf{J}$ with components $J_{ij} = \frac{\partial f_{i}}{\partial x_{j}}$ : \begin{equation}
      g'_{ij} = \sum_{k=1}^{3} J_{kl} J_{km} \left(\textbf{e}_{i}\right)^{l} \left(\textbf{e}_{j}\right)^{m}
\end{equation} where we use the summation convention over indices $l$ and $m$ . It seems as if the transformation of the components of the metric cannot be written as a matrix equation. However, from GR, we can treat an active transformation as a passive transformation as well. Can this problem be solved and can one get a nice transformation law? I do not know much differential geometry so I am struggling to find an answer. Would greatly appreciate anyone's help!","['general-relativity', 'transformation', 'differential-geometry']"
3992724,"Simplify $\sum_{i=1}^n\frac{a}{\theta-x_i}$ where $\{x_i\}\in(0,\theta)$ and $a\in(-1,0]$","I have this problem where I need to solve $-\frac{n(1+a)}{\theta}+\sum_{i=1}^n\frac{a}{\theta-x_i}=0 $ for $\theta$ (the solution is allowed to be in terms of the mean of $x_1,..., x_n$ ), provided  that $\{x_i\}\in(0,\theta)$ and $a\in(-1,0]$ . I really have no idea how to simplify the summation. Maybe I'm just missing something simple?","['statistics', 'sequences-and-series', 'summation', 'real-analysis']"
3992751,Why is it not obvious that the Zermelo hierarchy is a hierarchy?,"In Sets, Logic and Categories , Cameron writes First, [we show that the Zermelo hierarchy] really is a hierarchy: the sets get larger as we progress. (This is not obvious; each set is the power set of its predecessor, and most sets $X$ don't satisfy $X\subseteq\mathcal{P}X$ .) I'm stuck on the parenthetical sentence. What does he mean by ""each set is the power set of its predecessor""? (Is this referring to ordinals, or to sets in general?) And what does he mean by ""most sets $X$ don't satisfy $X\subseteq\mathcal{P}X$ ""? (Most sets = most ordinals? Certainly this holds for finite sets, no? But not ordinals?)","['elementary-set-theory', 'ordinals', 'set-theory']"
3992881,use Schwarz inequality to get $\int_x^{+\infty }f(t) dt \leq \frac{1}{1+x^2}$,"$f(x)\ge 0,\forall x\in \Bbb R$ . If $$\int_{-\infty}^{+\infty}f(x) dx =1, \int_{-\infty}^{+\infty}xf(x) dx =0,\int_{-\infty}^{+\infty}x^2f(x) dx =1. $$ Show that , for every $x\gt0$ , $$\int_x^{+\infty }f(t) dt \leq \frac{1}{1+x^2}$$ I have a method: proof by contradiction If $a\gt0$ such that $\int_a^{+\infty }f(t) dt \gt \frac{1}{1+a^2}$ , then $$ \int_{a}^{+\infty}xf(x) dx \gt \dfrac{a}{1+a^2},\int_{a}^{+\infty}x^2f(x) dx \gt\frac{a^2}{1+a^2}. $$ so $$\int_{-\infty}^a f(x) dx \lt -\dfrac{a^2}{1+a^2}, \int_{-\infty}^a xf(x) dx \lt -\dfrac{a}{1+a^2},\int_{-\infty}^a x^2f(x) dx \lt\frac{1}{1+a^2}. $$ $$ \int_{-\infty}^a (ax+1)^2f(x) dx \lt a^2 \frac{1}{1+a^2}-2a\frac{a}{1+a^2}+\frac{a^2}{1+a^2}=0. $$ I heard that the inequality is related to probability theory，and it can't be improved, how to cite a function to get this?  Are there any other methods, use Schwarz inequality ?","['calculus', 'probability-theory', 'inequality', 'real-analysis']"
3992964,Evaluating the double integral $\int_0^\infty d a \int_0^\infty d b\ \frac{ \sin(x a) \sin( y b ) }{a+b}$.,"Consider the double-integral $$
F(x,y) := \int_0^\infty d a \int_0^\infty d b\ \frac{ \sin(x a) \sin( y b ) }{a+b} \ .
$$ By playing around in Mathematica I have come to believe that the above evaluates to $$
F(x,y) \stackrel{?}{=} \frac{\pi}{2(x+y)} \ .
$$ How would one prove this result? EDIT: I've been able to evaluate the $a$ -integral, showing that the above is equal to $$
F(x,y) = \int_0^\infty db\ \sin(by) \bigg[ \text{Ci}(bx) \sin(bx) + \frac{1}{2} \cos(bx) \big( \pi - \text{Si}(bx) \big) \bigg] \ ,
$$ where $\text{Ci}$ and $\text{Si}$ are the cosine integral and sine integral functions, respectively. From here I cannot make progress. There is a symmetry in the order of integration ( ie. the above would look the same if I did the $b$ -integral first). It is curious that the function seems to depend on $x+y$ : I don't see how this dependence falls out of the definition of $F$ .","['integration', 'multiple-integral', 'definite-integrals']"
3993000,"The distribution of areas of a random triangle on the sphere - what are the second, third, etc. moments?","Suppose that we choose three points independently and uniformly at random on the surface of a unit sphere as the vertices of a triangle, and consider the area of this triangle. Call this random variable $X$ . The area of such a triangle is the sum of its angles minus $\pi$ , so by linearity of expectation the expected value of $X$ is just $3q-\pi$ , where $q$ is the expected value of one of the angles. But by symmetry we can fix one point to be at the north pole and the other to lie on the Prime Meridian, from which it is obvious that the angle distribution is uniform on $[0,\pi]$ . Thus $\mathbb{E}[X]=\pi/2$ , or one-eighth of the sphere's area. However, because the angles are not independent (they cannot sum to less than $\pi$ , for instance), we cannot use this sort of logic to easily infer the values of the second and third moments of this distribution. After gathering some numerical data, it appears that $\mathbb{E}[X^2]=\frac{\pi^2}2$ , but I am not sure how to prove this. (Note that this is equivalent to the statement that the standard deviation of $X$ is $\pi/2$ , which may be easier to show?) I also have $\mathbb{E}[X^3]\approx 20.36$ , or that the third moment of $X$ is approximately $4.86$ (either can be inferred from the other, given lower-order moments - they should differ by $\pi^3/2$ ). I haven't found any particularly nice formulas that match either of these values, though I'm not sure about the last digit in either of these estimates. In general, what is $\mathbb{E}[X^n]$ or $\mathbb{E}[(X-\frac{\pi}2)^n]$ ? If any of the values are open, has it been discussed in the literature? Is there a nice geometric argument for $\mathbb{E}[X^2]$ ? Edit: Here is a histogram of the area distribution from $1000000$ samples. Interestingly, it seems not to decay to $0$ at the upper bound of $2\pi$ .","['spherical-trigonometry', 'geometric-probability', 'geometry', 'spherical-geometry', 'random-variables']"
3993012,Connecting the two definitions of $e$,"There are two different ways I've been taught to understand the meaning of $e$ . $e$ can be defined as the total growth from continuously compounding interest in a single period. To make the growth rate the same as the dollar amount, consider I start with $\$1$ . Then, $a_n$ represents the amount I have after compounding $n$ times. \begin{align}
a_n&=1\cdot\left(1+\frac{1}{n}\right)^n\\[5pt]
e=a_{\max}&=\lim_{n \to \infty}\left(1+\frac{1}{n}\right)^n
\end{align} As we increase the number of times we compound the interest, the final amount (in the case that we start with $\$1$ ), approaches $e$ . All exponential functions increase at a rate proportional to their current value. For example, consider the derivative of $2^x$ : \begin{align}
\frac{d\left(2^x\right)}{dx}&=\lim_{h \to 0}\frac{2^{x+h}-2^x}{h}\\[5pt]
&=\lim_{h \to 0}\frac{2^x2^h-2^x}{h}\\[5pt]
&=\lim_{h \to 0}\frac{2^x\left(2^h-1\right)}{h}\\[5pt]
&=2^x\lim_{h \to 0}\frac{\left(2^h-1\right)}{h}
\end{align} From this we can see that the rate at which $2^x$ changes is proportional to its current value, with a proportionality constant of $$\lim_{h \to 0}\frac{\left(2^h-1\right)}{h}.$$ For $2^x$ , this constant happens to be $\ln(2)$ (why is this the case?). $e$ can be defined as the base where this proportionality constant is $1$ ; therefore, the rate $e^x$ grows at is exactly its current value. My question is this: how can the same constant have two (apparently) separate definitions? Is there a way of looking at $e$ that will combine these two definitions, and make it obvious as to why they are true? Also, why is the proportionality constant of any exponential functions growth rate the natural log of its base? Is there a way to understand why this is the case intuitively?","['calculus', 'exponential-function', 'logarithms']"
3993019,Finding Specific Generators of $\left(\mathbb{Z} / p \mathbb{Z}\right)^{\times}$,"Let $p$ be an odd prime. Does there exist an element $y$ of the group $\left(\mathbb{Z} / p \mathbb{Z}\right)^{\times}$ such that both $y$ and $1-y$ are generators of $\left(\mathbb{Z} / p \mathbb{Z}\right)^{\times}$ ? Clearly, this is true when $2$ is a generator of $\left(\mathbb{Z} / p \mathbb{Z}\right)^{\times}$ as $y = 2^{-1} = 1-y$ does the trick. But I'm not sure how to prove that there is such an element when $2$ is not a generator of $\left(\mathbb{Z} / p \mathbb{Z}\right)^{\times}$ .","['field-theory', 'elementary-number-theory', 'group-theory']"
3993037,Computing the gradient of Cross Entropy Loss,"The categorical cross entropy loss is expressed as: $$L(y,t) = -\sum_{k=1}^{K}t_k\ln{y_k}$$ where $t$ is a one-hot encoded vector. $y_k$ is the softmax function defined as: $$y_k = \frac{e^{z_k}}{\sum_{j=1}^{K}e^{z_j}}$$ I want to compute the gradient, $\nabla_z$ , of the loss function with respect to the input of the output node. What I know: I understand how to compute the partial derivative of L with respect to a selected node (say, $z_k$ ). This yields the following expression: $$\frac{\partial L}{\partial z_k} = y_k - t_k$$ But I am not sure how to generalize this to the entire vector, $z$ . In essence, I know how to compute $\frac{\partial L}{\partial z_k}$ when $k = j$ and $k \neq j$ , but I don't know how to calculate the gradient, $\nabla_z$ .",['derivatives']
3993154,Why the cauchy is t distribution with 1 degree of freedom,"Cauchy: Place a spinner at (0,1) in the plane. Spin it in such a way
that all angles are equally likely (uniformly distributed). The number
that the spinner points to on the x axis follows the Cauchy
distribution. This is the same as the T distribution with 1 degree of
freedom. I don't understand this paragraph. I know the Student's t distribution is defined as follows: let $Z \sim N(0,1)$ and $V \sim \chi^2(v)$ . If Z and V are independent, then the distribution of $$T=\frac{Z}{\sqrt{V/v}}$$ has the Student's t distribution with v degrees of freedom. I am wondering if the df is 1, then $$T=\frac{Z}{ \sqrt{\chi^2(1)}}=\frac{Z}{Z}=1$$ Please point me where I am wrong.","['statistics', 'probability-theory']"
3993156,"If $A$ is an infinite set and $b \notin A$, is the equivalence $A \sim A \bigcup \{b\}$ provable without the axiom of choice (i.e. in ZF)?","A set $A$ is said to be infinite if there is a surjection from $A$ to $\mathbb N_0$ . Let $A$ be infinite, and denote by $b$ some set that is not included in $A$ . (Such a $b$ exists by the specification axiom). To prove the equivalence $$A \sim A \bigcup \{b\} \tag{
1}$$ in elementary set theory, one usually uses the fact that $A$ has a countably infinite subset (whose universal existance depends on some kind of choice). Q: But can the equivalence in $(1)$ be proven in some other way, without the axiom of choice? More specifically, in the context of ZF without regularity? I assume it cannot be done but I am not sure... I am still quite the novice when it comes to axiom of choice considerations, having begun only yesterday. If if it cannot be done, should I expect the impossibility to have an elementary proof? (Again, I think not but am unsure).","['elementary-set-theory', 'axiom-of-choice']"
3993177,"Which book is better for learning calculus Stewart, Larson or Thomas","I want to master calculus in every possible way, I'm working in my bases like algebra and trigonometry (Precalculus) since I haven't had a good start in calculus, I want to read books like Calculus by Spivak, Calculus by Apostol and Courant books from Calculus and analysis. I want to know which books of calculus those 3 authors (Stewart, Larsom, Thomas) could help me to make a good aproach to calculus, if they are any substantial differences, if you think they are others best books please tell me","['book-recommendation', 'real-analysis', 'multivariable-calculus', 'calculus', 'algebra-precalculus']"
3993195,Line Integral - Why are these two integrals the same?,"I am starting to study calculus III and I came across the following situation Given the following form $$ydx - xdy$$ Why the integral along the semicircle $$P(t) = cos(t)\vec{i} + sin(t)\vec{j},\:0 \leq t \leq \pi$$ It is the same when using the following parameterization $$y = \sqrt{1-x^2}, -1 \leq x \leq 1$$ Since the parameterizations are reversed? When computing the first integral, I got $ - \pi $ . $$\int_c ydx - xdy$$ $$x = cos(t) \Rightarrow \frac{dx}{dt} = -sin(t)$$ $$y = sin(t) \Rightarrow \frac{dt}{dt} = cos(t)$$ $$\int_0^\pi ( sin(t)(-sin(t)) - cos(t)cos(t)) dt = -\pi$$ How do I compute the second one that has an inverted sense of integration than the first one? Thanks in advance!","['differential-forms', 'multivariable-calculus', 'line-integrals', 'vector-analysis']"
3993198,Prove that three specific lines in a triangle are concurrent,"In a triangle ABC, through circumcenter O we draw three lines parallel to the triangle sides. Call the intersection of lines with the triangle as D,E,F,G,H,K as on the image. Connect the neighboring of these which do not lie on the same side, as on the image. Prove that perpendicular bisectors on these new segments (KD, EG, FH) are concurrent. I found four properties but cannot progress further. (check image for reference) A'B'C' has circumcircle also centered at O with radius 1/2 of the (ABC) one. Thus homothety from O with factor 2 into larger circle. B'C' C'A' A'B' are also parallel with respective sides of the triangle and at half the distance between the respective side and it's parallel that is passing through O. If you extend KD, EG and HF they form a new triangle A1, B1, C1. Surprisingly these A1, B1, C1 are collinear with CC'O AA'O and BB'O. (but don't know how to prove it) Call the intersection that needs to be proven as I. Then A1B'IC' are concyclic. Same for B1 and C1.","['contest-math', 'triangles', 'geometry']"
3993258,Showing weak convergence of a sequence in $L^p(R)$,"I have a sequence of functions $f_k$ in $L_p(R)$ , with $1<p<\infty$ and I'd like to show that it weakly converges to $0$ .
This is the sequence, where $k\in N$ $$f_k = 1_{[k,k+1]}$$ What I've tried:
If $f_k$ converges to $0$ , then we should have $$\lim_{k\to \infty}\int_R(f_k-0)\phi dx =0$$ where $\phi \in L_{p'}(R)$ (the dual, or here, $L_q(R)$ .
Putting in the function, one gets $$\lim_{k\to \infty}\int_k^{k+1}1.\phi dx$$ Now I need to show this goes to $0$ for all functions $\phi \in L_q(R)$$, but that isn't necessarily true right?","['lp-spaces', 'functional-analysis', 'weak-convergence']"
3993292,"Why is this integral diverging? $\int\limits^{\infty}_{-\infty} \,\frac{x}{x^2+1} dx$","$$\int\limits^{\infty}_{-\infty} \,\frac{x}{x^2+1} dx$$ I can easily prove that this integral is diverging by taking the limit over the following proper integral, $$\lim_{R_1,R_2\to \infty} \int\limits^{R_2}_{-R_1} \,\frac{x}{x^2+1} dx$$ Mathematically, this makes sense to me, but intuitively I am not able to absorb this. If we observe the function $y=\frac{x}{x^2+1}$ , it is clearly an odd function. And since, integrals return signed areas, an integral to an odd function having limits that are the negatives of each other, should evaluate to zero. As clearly seen by the following plot. Can someone please clarify this? Thank you.","['integration', 'calculus', 'definite-integrals', 'convergence-divergence']"
3993323,Number of pairs of subsets that have no elements in common,"A set $M$ consists of $n$ elements. Determine the number of pairs of subsets of $M$ which have no elements in common ( don't forget to account for the empty set ). If we choose a subset of one element, then there are $(n-1)+1$ corresponding different subsets of the same size, hence the total is ${n\choose1}+\frac12{{n-1}\choose 1}{n\choose1}$ when we shuffle through each subset and repeat the same operation. Then, if we choose one with two elements, then there are $1+{{n-2}\choose1}+{{n-2}\choose2}$ corresponding different subsets of the same size and of size = 2-1, which gives us a total of ${n\choose2}+{{n-2}\choose1}{{n}\choose2}+\frac12{{n-2}\choose2}{n\choose2}$ . With three, we get for the one selection ${n\choose3}+{{n-3}\choose1}{n\choose3}+{{n-3}\choose2}{n\choose3}+\frac12{{n-3}\choose3}{n\choose3}$ for the same size, size-1 and size-2. If I am to do this for a subset with $k$ elements, then the total is $s_k={n\choose k}+\frac12{{n-k}\choose k}{n\choose k}+\sum_{i=1}^{k-1}{{n-k}\choose i}{n\choose k}$ . The big total is when I shuffle through all possible values of $k$ , so $\sum_{k=1}^{n-1}s_k$ . Is there any flaw in my reasoning? Thank you for your time!",['combinatorics']
3993356,"Understanding the difference between ""different iid random variables"" and ""different instance of same random variable""","In the derivation of unbiased sample variance , it is considered that $X_i$ are iid random variables while $X_i$ actually represents a sample from a population. So my question is that shouldn't we consider $X_i$ to be an instance of same random variable whose probability distribution is probability of selecting $X_i$ ? For example, consider I have population of people with different heights and I select a sample of people from this population. This sampling of people can be thought of a repetitive procedure of sampling a value from a random variable i.e uniform r.v. Isn't it so? Also I have generally seen it that when we consider samples of a stochastic process, we model each sample as i.i.d instead of repetitive sampling (taking multiple values) from the stochastic process. For example in the definition of strict sense stochastic process we say that the joint distribution of different samples of $X_t$ will be same whether we sample it at any time. Here my question will be rephrased as how can we have joint distribution of constant numbers? Like if I have a series of dice-face-numbers then there is no meaning of considering joint probability distribution of these numbers?","['conditional-probability', 'independence', 'probability-theory', 'probability', 'random-variables']"
3993372,Show that $ \text{Tr}(XYZ) + \text{Tr}(YXZ)+ \text{Tr}(X)\text{Tr}(Y)\text{Tr}(Z) = ... $,"Let $X, Y, Z$ be $2 \times 2$ matrices.  Show that these two matrix combinations are equal: $ \text{Tr}(XYZ) + \text{Tr}(YXZ)+ \text{Tr}(X)\text{Tr}(Y)\text{Tr}(Z)   $ $ \text{Tr}(X) \, \text{Tr}(YZ) + \text{Tr}(YX)\,\text{Tr}(Z)+ \text{Tr}(Z)\,\text{Tr}(XY) $ There's lots of identities for matrix trace , here's the only other one that I know, that we can switch the order of the matrix: $$ \text{Tr}(XYZ) = \text{Tr}(YZX) $$ I might specifically need to have $X,Y,Z \in \text{SL}_2(\mathbb{C})$ .  The right side looks symmetric under the cycle permutation of $(XYZ)$ (basically a triangle ) while the left side does not.","['matrices', 'linear-algebra', 'invariant-theory']"
3993380,Defining a real number,"No answers, please, hints only.
I want to express every nonzero $x\in \mathbb R$ as a product of two numbers that are not rational. My attempt is $x=(a_1+b_1i)(a_2+b_2i)$ where $a_1, b_2\in \mathbb R$ Am I correct or do I need a hint?","['real-numbers', 'elementary-number-theory', 'irrational-numbers', 'discrete-mathematics']"
3993411,For $0<p<1$ showing $\Big(\int_\Omega |f|^pd\mu\Big)^{1/p}\leq \frac{1}{R_0} + \Big(\int_\Omega |f_{R_0}|^p\Big)^{1/p} $,"In this problem Limit of $L^p$ norm when $p\to0$ , the writer states that for $0<p<1$ we have that $$\Big(\int_\Omega |f|^pd\mu\Big)^{1/p}\leq \frac{1}{R_0} + \Big(\int_\Omega |f_{R_0}|^p\Big)^{1/p} $$ where $\mu$ is a positive measure such that $\mu(\Omega)=1$ and $f_{R_0}=|f|1_A$ where $A={\{x: |f(x)|\geq \frac{1}{R_0}\}}$ . I see that $$\int_\Omega |f|^pd\mu = \int_A |f|^pd\mu + \int_{A^c}|f|^pd\mu \leq \int_\Omega |f_{R_0}|^pd\mu + \frac{1}{R_0^p}$$ But I know that $a^{1/p} + b^{1/p}\leq (a+b)^{1/p}$ where $a,b\geq0$ . So I am not sure how to distribute the the $1/p$ power.","['integration', 'measure-theory', 'real-analysis', 'lp-spaces', 'inequality']"
3993429,"Given dihedral angles, find a set of edges","In the paper Space Vectors Forming Rational Angles a special set of tetrahedra is mentioned. ""The remaining three are in the R-orbit of the tetrahedron with dihedral angles ( $π/7, 3π/7, π/3, π/3, 4π/7, 4π/7$ )."" What is a set of edge lengths or vertices for this tetrahedron? I've written a function that converts edges to angles , but I need the reverse. I solved it in a very messy way: https://community.wolfram.com/groups/-/m/t/2169279 . An elegant solution would still be nice.","['polyhedra', 'geometry', 'tiling', 'computational-geometry']"
