question_id,title,body,tags
1626958,$3^x-2^y=17$ where $x$ and $y$ are both positive integers,"First, I was wondering what would the solution(s) be to the equation $3^x-2^y=17$ such that $x$ and $y$ are both positive integers. I couldn't find any small possibilities. Is there a proof that such a solution exists? How about the general case $3^x-2^y=z$? Is there exactly one x and y positive integer solution for every positive integer z (not divisible by 2 or 3 that is)? Thanks in advance!","['algebra-precalculus', 'diophantine-equations', 'elementary-number-theory']"
1626959,In how many ways can a $31$ member management be selected from $40$ men and $40$ women so that there is a majority of women?,"Here's the question: In an organization there are $80$ people, $40$ men and $40$ women.
  In how many ways can we choose, from those $80$ people, a $31$ member management so that there is a majority of women? My answer was to first choose $16$ women (to assure the majority) out of the $40$ meaning $\binom{40}{16}$,
and then choose the rest $15$ from the rest of all people meaning $\binom{64}{15}$. So the final answer is ${40 \choose 16} \cdot{64 \choose 15}$ but that isn't the right answer so I really don't know..
help?
Thank you. Note $\binom{n}{k} = \frac{n!}{k!(n-k)!}$, the binomial coefficient.","['combinatorics', 'discrete-mathematics']"
1626964,What is the standard deviation of this random variable? (I want to check my calculation),"Consider the random variable (orientation angle) $0\le\theta\le 2\pi$ with the following PDF where $\theta_0$ is the mean orientation angle: $(n\in\mathbb Z , n\ge 0)$ $$p(\theta,\theta_0,n)=\frac{\cos^{2n}(\theta-\theta_0)}{\int_0^{2\pi}\cos^{2n}(\theta-\theta_0) \, d\theta}$$
What is the variance of the random variable $\theta$ in terms of $n$ I have taken these steps: First of all we should calculate $A_n=\int_0^{2\pi}\cos^{2n}(\theta-\theta_0)\,d\theta$ From the Integration by substitution method assuming that $x=\theta-\theta_0$ we have: $$A_n=\int_{-\theta_0}^{-\theta_0+2\pi}\cos^{2n}x\,dx$$ From page $30$ in Elementary Functions we know:
$$\cos^{2n}x=\frac{1}{2^{2n}}\binom{2n}{n}+\frac{1}{2^{2n-1}}\sum_{k=0}^{n-1}\binom{2n}{k}\cos 2(n-k)x$$
Then
$$A_n=\int_{-\theta_0}^{-\theta_0+2\pi}\frac{1}{2^{2n}}\binom{2n}{n}\,dx+\frac{1}{2^{2n-1}}\sum_{k=0}^{n-1}\binom{2n}{k}\int_{-\theta_0}^{-\theta_0+2\pi}\cos 2(n-k)x\,dx$$
About the second integral in the above expression, we know that the period of the function $\cos 2(n-k)x$ is $\frac{\pi}{n-k}$ So the length of the interval $[-\theta_0,-\theta_0+2\pi]$ is $2(n-k)$ times the period which is an integer coefficient of the period, that means the answer of this integral will be $0$ So $$A_n=\frac{\pi}{2^{2n-1}}\binom{2n}{n}$$ The PDF mentioned at the begining will be converted to: $$p(\theta,\theta_0,n)=\frac{\cos^{2n}(\theta-\theta_0)}{A_n}$$
So the variance will be:
$$\sigma^2(\theta)=\frac{1}{A_n}\int_0^{2\pi}(\theta-\theta_0)^2\cos^{2n}(\theta-\theta_0)\,d\theta$$
Again from the Integration by substitution method and the aforementioned relation we have: $$\begin{align}
\int_0^{2\pi}(\theta-\theta_0)^2\cos^{2n}(\theta-\theta_0)\,d\theta
&=\int_{-\theta_0}^{-\theta_0+2\pi}x^2\cos^{2n}x\,dx\\
&=\int_{-\theta_0}^{-\theta_0+2\pi}x^2(\frac{1}{2^{2n}}\binom{2n}{n}+\frac{1}{2^{2n-1}}\sum_{k=0}^{n-1}\binom{2n}{k}\cos 2(n-k)x)\,dx\\
&=\frac{\binom{2n}{n}}{2^{2n}}\underbrace{\int_{-\theta_0}^{-\theta_0+2\pi}x^2\,dx}_1\\
&+\frac{1}{2^{2n-1}}\sum_{k=0}^{n-1}\binom{2n}{k}\underbrace{\int_{-\theta_0}^{-\theta_0+2\pi}x^2\cos 2(n-k)x\,dx}_2
\end{align}$$
About the first integral in the above equation, we have $$(1):\int_{-\theta_0}^{-\theta_0+2\pi}x^2\,dx=\left[\frac{x^3}{3}\right]_{-\theta_0}^{-\theta_0+2\pi}=\frac{8\pi^3-12\pi^2\theta_0+6\pi\theta_0^2}{3}$$ And about the second integral: $$(2):\int_{-\theta_0}^{-\theta_0+2\pi}x^2\cos 2(n-k)x\,dx$$
From integration by parts method: $$
\begin{array}{c|c}
\text{Derivation}&\text{Integration}\\
\hline
{} \oplus x^2 & \cos 2(n-k)x  \\
{} \ominus 2x & \frac{1}{2(n-k)}\sin 2(n-k)x \\
{} \oplus 2 & \frac{-1}{2^2(n-k)^2}\cos 2(n-k)x \\
{} \ominus 0 & \frac{-1}{2^3(n-k)^3}\sin 2(n-k)x
\end{array}
$$
$$\begin{align}
(2):\int_{-\theta_0}^{-\theta_0+2\pi}x^2\cos 2(n-k)x\,dx &=\left[\frac{x^2}{2(n-k)}\sin 2(n-k)x\right]_{-\theta_0}^{-\theta_0+2\pi} \tag 1\\
&+\left[\frac{x}{2(n-k)^2}\cos 2(n-k)x\right]_{-\theta_0}^{-\theta_0+2\pi} \tag 2 \\
&-\left[\frac{1}{2^2(n-k)^3}\sin 2(n-k)x\right]_{-\theta_0}^{-\theta_0+2\pi} \tag 3
\end{align}$$
For $(1)$ regarding the facts $\sin (2n\pi+\theta)=\sin\theta$ and $\sin(-\theta)=-\sin\theta$ after some simplifications, we have $$(1)=\frac{2\pi(\theta_0-\pi)}{n-k}\sin 2(n-k)\theta_0$$
For $(2)$ regarding the facts $\cos (2n\pi+\theta)=\cos\theta$ and $\cos(-\theta)=\cos\theta$ after some simplifications, we have $$(2)=\frac{\pi}{(n-k)^2}\cos 2(n-k)\theta_0$$ Also after some simplifications, we conclude that $(3)=0$ So:$$\int_{-\theta_0}^{-\theta_0+2\pi}x^2\cos 2(n-k)x\,dx=\frac{2\pi(\theta_0-\pi)}{n-k}\sin 2(n-k)\theta_0+\frac{\pi}{(n-k)^2}\cos 2(n-k)\theta_0$$
Then
$$\begin{align}
&\int_0^{2\pi}(\theta-\theta_0)^2\cos^{2n}(\theta-\theta_0)\,d\theta=\frac{\binom{2n}{n}}{2^{2n}}\frac{8\pi^3-12 \pi^2 \theta_0 + 6\pi\theta_0^2}{3}+{}\\
&\frac{1}{2^{2n-1}}\sum_{k=0}^{n-1}\binom{2n}{k}\left(\frac{2\pi(\theta_0-\pi)}{n-k}\sin 2(n-k)\theta_0+\frac{\pi}{(n-k)^2}\cos 2(n-k)\theta_0\right)
\end{align}$$ And the variance will be: $$\frac{2^{2n-1}}{\binom{2n}{n}\pi}\left(\begin{align}
&\frac{\binom{2n}{n}}{2^{2n}}\frac{8\pi^3-12\pi^2\theta_0+6\pi\theta_0^2}{3}+{}\\
&\frac{1}{2^{2n-1}}\sum_{k=0}^{n-1}\binom{2n}{k}\left(\frac{2\pi(\theta_0-\pi)}{n-k}\sin 2(n-k)\theta_0+\frac{\pi}{(n-k)^2}\cos 2(n-k)\theta_0\right)
\end{align}\right)$$ With the final form: $$\sigma^2(\theta)=\frac{4\pi^2-6\pi\theta_0+3\theta_0^2}{3}+\sum_{k=0}^{n-1}\frac{n!n!}{k!(2n-k)!}\left(\frac{2(\theta_0-\pi)}{n-k}\sin 2(n-k)\theta_0+\frac{1}{(n-k)^2}\cos 2(n-k)\theta_0\right)$$
So it seems that the variance is dependent to $\theta_0$(mean orientation angle) and even if we assume $\theta_0=0$
$$\sigma(\theta)=\sqrt{\frac{4\pi^2}{3}+\sum_{k=0}^{n-1}\frac{n!n!}{k!(2n-k)!}.\frac{1}{(n-k)^2}}$$ But in the papers that I'm studying: A General Characterization for Polarimetric Scattering From Vegetation Canopies Adaptive Model-Based Decomposition of Polarimetric SAR Covariance Matrices It has been pointed out that: Also the limit of the expression that I get finally is not $0$ an $n\to +\infty$ which is not consistent with our hypothesis I'm really confused","['trigonometry', 'calculus', 'integration', 'probability', 'sequences-and-series']"
1626975,Cardinality of the set of functions $f: A \to B$ where where $|A|=\aleph_0$ and $|B|=2^{\aleph_0}$,"Let $X$ be the set of all functions $f: A \to B$ where $|A|=\aleph_0$ and $|B|=2^{\aleph_0}$. Using some cardinal arithmetic, one can show that $|X|=2^{\aleph_0}$. However, I wanted to construct a bijection $g: P(\mathbb{N}) \to X$ explicitly. Here is what I came up with: Letting $C^\mathbb{N}$ denote the set of sequences on $C$, I constructed a bijection $P(\mathbb{N}) \to (0,1) \to (0,1)^\mathbb{N} \to B^\mathbb{N} \to X$. For $(0,1) \to (0,1)^\mathbb{N}$, I represent each $x_n$ by its binary expansion $0.x_{n1}x_{n2}\dots$ and consider $x \in (0,1)$ given by $0.x_{11}x_{12}x_{21}x_{13}x_{22}x_{31}\dots$. I would like to simplify this, though, if possible. I'm wondering if it is possible to construct a bijection directly, without having to go through the reals. In other words, I would like to avoid the step $(0,1) \to (0,1)^\mathbb{N}$ outlined above, or else encode it somehow in a direct bijection $P(\mathbb{N}) \to B^\mathbb{N}$. Does anyone have any suggestions? Thanks!","['cardinals', 'elementary-set-theory']"
1626983,Is there a choice homomorphism?,"Let $\pi : \mathbb{R} \to \mathbb{R}/ \mathbb{Q}$ be the canonical projection. With the axiom of choice we ""know"" that there are choice functions $\alpha : \mathbb{R}/ \mathbb{Q} \to \mathbb{R}$ with $\pi \circ \alpha = id_{\mathbb{R}/ \mathbb{Q}}$ (i.e. with $\alpha(x) \in x$ for all $x \in \mathbb{R}/ \mathbb{Q}$). Are there such choice functions which are group homomorphisms (with respect to addition)?","['real-numbers', 'rational-numbers', 'group-theory', 'axiom-of-choice']"
1627002,Contrapositive - Convergence of a sequence,"I know that the convergence of a sequence $\{x_n\}_{n \geq0}$ in $\mathbb{R}$ is defined as for all $ \epsilon > 0$, there exists $N \in \mathbb{N}$ such that $|x_n-x|< \epsilon$ for all $n \geq N$. What is the contrapositive of this definitiion. My thought : There exist $ \epsilon > 0$ such that for all $N \in \mathbb{N}$ $|x_n-x| \geq \epsilon$ for all $n \geq N$.","['sequences-and-series', 'convergence-divergence']"
1627069,Can we assign a value to the sum of the reciprocals of the natural numbers?,"I know the sum of the reciprocals of the natural numbers diverges to infinity, but I want to know what value can be assigned to it. $$\sum_{n=1}^{\infty}\frac1n=\frac11+\frac12+\frac13+\frac14+\dots=L$$ As a few examples of what kind of answer I want, here are a few similar problems: $$\sum_{n=1}^{\infty}n=1+2+3+4+\dots=-1/12$$ $$\sum_{n=1}^{\infty}(-1)^{n+1}n=1-2+3-4+\dots=1/4$$ $$\sum_{n=0}^{\infty}(-1)^n=1-1+1-1+\dots=1/2$$ As you can see, I want to assign a value to the divergent series of the reciprocals of the natural numbers.","['summation', 'divergent-series', 'sequences-and-series']"
1627158,What does $\bigotimes$ and $X^*$ mean?,"Can someone explain / link me to a linear algebra worked problem where I can see how these work. I've searched and given their statistics and matrix specialty uses, can't find any ready examples.",['matrices']
1627168,No subsequence converges when sequence has limit point in topological space,"In general topological space, suppose $x_0$ is a limit point of a sequence $\{x_n\}$, is it possible that there is no subsequence that converges to it？",['general-topology']
1627174,$G$ equivariant quasicoherent sheaves on $X$ as compatible $G$ actions on the total spaces?,"Let $G$ be an algebraic group, and $X$ a scheme on which $G$ acts: i.e the $S$ points of $G \times X \to X$ is a group for each affine $S$. Let $F$ be a quasicoherent sheaf on $X$. There is a notion of a $G$ equivariant sheaf here: I am wondering if it is equivalent to the following definition (which in my eyes is simper, being more geometric): Let $T$ be the total space of $F$, by which I mean relative $\operatorname{Spec_X}$ of the symmetric algebra on $F$. $\pi : T \to X$ is the natural affine map. (Maybe there is some subtlety in the extent to which this is related to $F$. I think $F$ is still the degree 1 piece of  $\pi_* O_T$.) Let $F$ is a $G$ equivariant sheaf on $X$ if the action of $G$ on $X$ extends to an action of $G$ on $T$. In symbols: There is some $G \times T \to T$ which is a group-scheme action. Moreover, this action commutes with $\pi : T \to X$, so that the natural diagram involving the actions and $G \times T \to G \times X$ and $T \to X$ commutes. Is this definition different for some obvious reason that I am not aware of? Assume $X$ is affine. Let $O(Y)$ denote the total sections of the structure sheaf of some scheme $Y$. Then the $O(G)$ comodule structure on $F(X)$ should be seen by dualizing the equation $G \times T \to T$. In particular, we have $O(T) \to O(G) \otimes O(T)$, and I expect that this can be restricted to degree 1 to reconstruct the comultiplication $F(X) \to F(X) \otimes O(G)$. So for instance, from this definition it is easy to feel how the category of $G$ equvariant quasicoherent sheaves on $X$ equivalent to the category of $H$ comodules which are $A$ modules, such that the multiplication $A \otimes M \to M$ is a map of comodules (is there a snappy name for this, better than (H-co,A)-modules?)","['sheaf-theory', 'equivariant-maps', 'algebraic-geometry', 'representation-theory', 'hopf-algebras']"
1627179,Recurrence $x_{k+1} = \sum_{i=1}^m \theta_i x_{k+1-i} + r_{k+1}$ solution,"Let $x_k$ be the solution of the recurrence equation 
$$x_{k+1} = \sum_{i=1}^m \theta_i x_{k+1-i} + r_{k+1}$$
where $(r_k)$ is a general sequence.
I'm trying to find a explicit solution 
for $(x_k)$ of the form $$x_n = \sum_{k=1}^n b_{k,n} \cdot r_k$$ Let ${\bf y}_k=  (x_k,x_{k-1},\dots,x_{k-m+1})^T$ where $k \geq m-1$. Then $${\bf y}_{k+1} = A {\bf y}_k + {\bf z}_{k+1}$$
Where $A$ is a matrix consisting on $A_{1,{\bf :}} = (\theta_1, \dots, \theta_m) $
and $A_{i,{\bf :}} = Id_{(i-1),{\bf :}}$ for $i \geq 2$ ($Id$ is de identity matrix of size $m$). And $z_k  = (r_k,0,\dots,0)^T$. Then is easy to show that $${\bf y}_n = \sum_{k=m}^n A^{n-k} {\bf z_k} + A^{n-(m-1)}{\bf y}_{m-1}$$
for $n \geq m$. then by the definition of ${\bf y}$ and ${\bf z}$ we get that
$$x_n = \sum_{k=m}^n a^{(n-k)}_{1,1} r_k + (A^{n-(m-1)}{\bf y_{m-1}})_1$$
for $n \geq m$. So I almost got the representation I wanted ($b_{k,n} = a_{1,1}^{(n-k)}$ for $k \geq m$) but I can't get $b_{k,n}$ for $k \leq m-1$. Any help will be appreciated Thanks","['recurrence-relations', 'recursion', 'discrete-mathematics']"
1627218,Understanding of Proof of Wilson's Theorem,"In my algebraic structures textbook there is a proof for the theorem if $p$ is a prime then $(p-1)!\equiv -1\pmod p\ $. Proof: Since p is prime, each element $1,2,3...(p-1)$ in $\Bbb Z_p $ has an inverse, hence the pairs of inverses will cancel out leaving only the self-inverse elements 1 and -1 and thus $(p-1)! = 1(-1) = -1 $ in  $\Bbb Z_p $. My question is how is that possible when the elements of $(p-1)! $ are all positive? I must be confusing something...",['number-theory']
1627293,"How to Show that the Only Subspaces of $R^2$ are the zero subspace, $R^2$ itself, and the lines through the origin","I'm having trouble with a question from an introductory Linear Algebra book. It goes: ""Show that the Only Subspaces of $R^2$ are the zero subspace, $R^2$ itself, and the lines through the origin.""
I'm thinking the easiest way to do this is to show that if $W$ is a subspace of $R^2$ containing $2$ different lines through the origin then $W$ is all of $R^2$. Is this a good way to go about it? and how could I show this?","['linear-algebra', 'vector-spaces']"
1627307,"At least $P(m, n - 1) = {{m!}\over{(m - n+1)!}}$ surjective functions from $[m]$ to $[n]$?","How do I see that there are at least$$P(m, n - 1) = {{m!}\over{(m - n+1)!}}$$surjective functions from $[m]$ to $[n]$?","['functions', 'algebra-precalculus', 'combinatorics', 'contest-math', 'discrete-mathematics']"
1627311,Showing $\sum_{k = 2}^n \binom{k}{2} \binom{n}{k} = \binom{n}{2} 2^n$ without induction.,"How do I prove the identity$$\sum_{k = 2}^n \binom{k}{2} \binom{n}{k} = \binom{n}{2} 2^{n-2}$$combinatorially, i.e. counting the cardinality of the same set in two different ways? I know how to do it by induction, but I am at a loss on how to do it combinatorially. Thanks in advance.","['algebra-precalculus', 'combinatorics', 'discrete-mathematics']"
1627332,Postive-semidefiniteness of matrix with entries $1/(a_i+a_j)$,"Let $a_1, \ldots, a_n$ be a set of positive numbers. Define a matrix $M_{ij} = \frac{1}{a_i+a_j}$. I'm trying to prove that $M$ is positive-semidefinite. The hint says to use the fact that $\int_{0}^{\infty} e^{-sx}\; dx = \frac{1}{s}$ if $s > 0$. However I don't know how this hint is useful. I've tried choosing an arbitrary vector $x$ and substituting $x^{\intercal}Mx = \sum_{i}\sum_{j} \frac{x_ix_j}{a_i+a_j}$ into $s$ and using properties of exponents to simplify the equation into something that is clearly positive, but without any luck. The denominator $\frac{1}{a_i+a_j}$ is simply too difficult to work with. At this point I think I'm just missing some trick that I don't know. Any help would be appreciated.","['positive-definite', 'linear-algebra']"
1627340,Limit of the floor function of $\frac{x}{\sin(x)}$,Alright this looks like a very simple problem at the first go. I need to find $$\lim_{x\rightarrow0^+} { \left\lfloor{\frac{x}{\sin (x)}}\right\rfloor}$$ So since I know the inner function's graph beforehand I know the answer will be 1. But now here's my problem.When I'm trying to find which function $x$ or $\sin(x)$ is greater when $x \rightarrow 0^+$ I take a function $g(x)=x-\sin(x)$ and find its derivative.So $g'(x)=1-\cos(x)$.Now when I find $g'(x \rightarrow 0^+)$ I get $0$ ! So I'm not being able to prove that $g(x)$ is increasing when $x$ is slightly greater than $0$.And thus I can't prove that $x>sin(x)$ when $x$ is slighty greater than $0$ and neither can I prove $\frac{x}{\sin (x)}>1$.Where am I going wrong?,"['ceiling-and-floor-functions', 'trigonometry', 'calculus', 'limits']"
1627361,Proving trig identity using De Moivre's Theorem,Question: Prove $$\cos(3x) = \cos^3(x) - 3\cos(x)\sin^2(x) $$ by using De'Moivres Theorem So far (learning complex numbers at the moment) that De Moivre's theorem states that if $z$ $=$ $r\text{cis}(\theta)$ then $z^n = r^n\text{cis}(n\theta)$ so with this question I was thinking if $$ z = \cos(3\theta) + i\sin(3\theta) $$ then $$ z = (\cos(\theta) + i\sin(\theta))^3 $$ and then expanding and comparing the real part? Is that the right way to go for this question?,"['algebra-precalculus', 'trigonometry', 'complex-numbers']"
1627376,Trigonometric Expression for $1 + \cos \alpha + \cos 2\alpha + \cdots + \cos n \alpha$ using complex numbers,"This question is not a duplicate because I am asked here to use the fact that  $1 + \cos \alpha + \cos 2 \alpha + \cdots + \cos n \alpha = Re (1 + z + z^{2} + \cdots + z^{n})$, where the question this is suspected of being a duplicate of does not use this I am supposed to use the fact that $1 + \cos \alpha + \cos 2 \alpha + \cdots + \cos n \alpha = Re (1 + z + z^{2} + \cdots + z^{n})$, where $z = \cos \alpha + i \sin \alpha$, ro find a trigonometric expression for $1 + \cos \alpha + \cos 2 \alpha + \cdots + \cos n \alpha$. Now, when $z \neq 1$, we have the geometric sum $\displaystyle 1 + z + z^{2} + \cdots + z^{n} = \frac{1-z^{n+1}}{1-z}$. (When $z = 1$, $1 + z + z^{2} + \cdots + z^{n} = n + 1$, so $Re(1 + z + z^{2} + \cdots + z^{n}) = Re(n+1) = n+1$, finished.) So, in the case where $z \neq 1$, $\displaystyle 1 + \cos \alpha + \cos 2 \alpha + \cdots + \cos n \alpha = Re(1 + z + z^{2} + \cdots + z^{n}) = Re\left(\frac{1-z^{n+1}}{1-z} \right) = Re \left(\frac{1-(\cos \alpha + i \sin \alpha)^{n+1}}{1-(\cos \alpha + i \sin \alpha)} \right) = Re \left[ \left(\frac{1-(\cos \alpha + i \sin \alpha)^{n+1}}{(1-\cos \alpha) - i \sin \alpha} \right)\cdot \frac{(1-\cos \alpha) + i \sin \alpha}{(1 - \cos \alpha)+ i \sin \alpha}\right] = Re \left( \frac{(1-\cos \alpha)+i\sin \alpha - ( 1- \cos \alpha)(\cos \alpha + i \sin \alpha)^{n+1}-i \sin \alpha (\cos \alpha + i \sin \alpha)^{n+1}}{(1-\cos \alpha)^{2} + \sin^{2} \alpha}\right)$. Then, to make a long story short, after an application of De Moivre's Theorem to turn the $(\cos \alpha + i \sin \alpha)^{n+1}$'s into $(\cos(n \alpha + \alpha)+ i \sin (n \alpha + \alpha))$'s, usage of the sine and cosine of sum identities, collecting of like terms and cancelling some things, we obtain $\displaystyle Re \left(\frac{1-(\cos \alpha + i \sin \alpha)^{n+1}}{1-(\cos \alpha + i \sin \alpha)} \right) = Re \left(\frac{(1-\cos \alpha + \cos (n\alpha)) + i(\sin \alpha - \cos (n\alpha)\sin \alpha + \sin (n \alpha))}{2-2\cos \alpha} \right) = \frac{1-\cos \alpha + \cos (n \alpha)}{2-2\cos \alpha}$. At least I think it does. Which brings me to my question: is this correct? If not, where did I go wrong and how do I fix it?","['complex-analysis', 'trigonometry', 'complex-numbers']"
1627388,Is there an upper bound on the growth rate of analytic functions?,"This problem comes from a solution tactic used in Is there a rational surjection $\Bbb N\to\Bbb Q$? , where I discovered that there is an analytic function $f(z)$ that takes the values $f(n)=a_n$ for all $n\in\Bbb N$, as long as $a_n$ has at most polynomial growth; here I am interested in seeing how far I can relax the ""polynomial growth"" constraint. Let us call an analytic function a kernel if it satisfies $k(0)=1$ and $k(n)=0$ for all $0\ne n\in\Bbb Z$. The main kernel used in the above question/answer was the function ${\rm sincz}(z)=\frac\pi z\sin(\frac z\pi)$, which has growth rate $O(z^{-1})$ (in the positive and negative real direction). Then ${\rm sincz}^m(z)$ is also a kernel, with growth rate $O(z^{-m})$, and any kernel yields an analytic function via $f(z)=\sum_{n\in\Bbb N}a_nk(n)$, which works for all sequences whose growth rate is no more than $O(\frac1{k(n)n^2})$ (or substitute some other summable series for $n^{-2}$). But if $k$ is a kernel and $g$ is any analytic function with $g(0)=1$, then $g(z)k(z)$ is also a kernel, which allows for much faster-decaying kernels, such as $e^{-z^2}{\rm sincz}(z)$. In fact, given any eventually monotonic analytic function $g(z)$ with $g(0)=1$, the function $\frac{{\rm sincz}^2(z)}{g(z^2)}$ is a kernel with growth rate $O(\frac1{g(z^2)z^2})$, which can create analytic functions for any sequence of growth rate less than $O(g(n^2))\supseteq O(g(n))$. So the problem is reduced to the question in the title: Is there any upper bound on the growth rate of analytic functions? That is, is there a definable sequence $a_n$ which grows so fast that it eventually outpaces any analytic function $f(z)$ sampled at the natural numbers? The examples given still fall far short of such fast-growing functions as the Ackermann function or Graham's sequence, but it is not obvious to me that there are not similar techniques for producing extremely fast-growing analytic functions.","['number-theory', 'combinatorics', 'complex-analysis', 'analyticity']"
1627405,The partial derivative of a characteristic function (exercise).,"Assume that you have a probability space $(\Omega, \mathcal{F},P)$ and a random varaible $X: (\Omega, \mathcal{F})\rightarrow(\mathbb{R}^d,\mathcal{B}(\mathbb{R}^d))$. Define the characteristic function $\phi_X: \mathbb{R}^d \rightarrow \mathbb{C}$, by $\phi_X(u)=\int_\Omega e^{i<u,X(\omega)>}dP$. I want to prove this theorem: If $X=(X_1,\ldots,X_d)$ and $\mathbb{E}(|X_j^n|)<\infty$ for some
  $1\le j\le d$  and $n \in \mathbb{N}$ then: $\mathbb{E}(X_j^n)=i^{-n}\frac{\partial^n}{\partial u_j^n}\phi_X(u) 
 |_{u=0}$ I tried two things, but I get problems with both approaches: approach 1: It is not hard to show this ""naively"", if you just differentiate like you learned in calculus, and go under the integral sign. then you will get the disered result. But there are two problems with this approach: Why is it partial differentiable at 0, and so why can we differantiate at all? Why can we differentiate under the integral sign? approach 2(dominated convergence theorem): Here I will try to solve both problems directly, but I still get problems: I start with n=1 for simplicity: Let $h_n$ be any sequence of real numbers converging to 0, but which is never zero, let $u_n=(0,0,0,h_n,0,0)$, where $h_n$ is in position j. Then if the partial derivative exits at 0, we must have that $\frac{\int_\Omega e^{i<u_n,X>}dP-\int_\Omega e^{i<0,X>}dP}{h_n}=\int_\Omega\frac{e^{i<u_n,X>}-1}{h_n}dP$ converges. Now the problem is reduced to taking a limit outside the integral, to inside the integral, if we can move the limit inside the integral, we see that we then get our derivative inside the integral. but why can we move the limit inside the integral? If we denote: $g_n=\frac{e^{i<u_n,X>}-1}{h_n}$, the problem will be solved if all $|g_n|$  will be bounded by an integrable function?(the dominated convergence theorem). But can you see a dominating function here? Do you guys have any tips or help?","['real-analysis', 'characteristic-functions', 'probability-theory', 'probability', 'measure-theory']"
1627410,Proving Trig Identities (Complex Numbers),"Question: Prove that if $z = \cos (\theta) + i\sin(\theta)$, then $$ z^n +  {1\over z^n} = 2\cos(n\theta) $$ Hence prove that $\cos^6(\theta)$ $=$ $\frac 1{32}$$(\cos(6\theta)$ + $6\cos(4\theta)$ + $15\cos(2\theta)$ + $10$) I learnt to prove the first part in another post linked here . The second part is where I am confused because there is a 'hence' so I thought of taking 2 approaches: either $$ z^6 + \frac 1{z^6} $$ or $$ z^6 $$ and equating real parts. I will start with my first approach $$ z^6 + \frac 1{z^6} $$ $$ (\cos(x)+i\sin(x))^6 + \frac 1{(\cos(x)+i\sin(x))^6} $$ $$ (\cos(x)+i\sin(x))^6 + {(\cos(x)+i\sin(x))^{-6}} $$ $$ \cos(6x) + i\sin(6x) + \cos(-6x)+ i \sin(-6x) $$ $$ 2\cos(6x) $$ Which is no where near what I am suppose to prove.. So with my second approach (expanding and equation real parts) $$ z^6 $$ $$ (\cos(x) + i \sin(x))^6 $$ Using pascals $$ \cos^6(x) + i*6\cos^5(x)\sin(x) + i^2*15\cos^4(x)\sin^2(x) + i^3*20\cos^3(x)\sin(x) + i^4*15\cos^2(x)\sin^4(x) + i^5*6\cos(x)\sin^5(x) + i^6 * \sin^6(x) $$ Simplifying $$ \cos^6(x) - 15\cos^4(x)\sin^2(x)+ 15\cos^2(x)\sin^4(x) - \sin^6(x) +i(6\cos^5(x)\sin(x)-20\cos^3(x)\sin(x) + 6cos(x)\sin^5(x)$$ Now considering only real $$ \cos^6(x) - 15\cos^4(x)\sin^2(x)+ 15\cos^2(x)\sin^4(x) - \sin^6(x) $$ At this point I'm confused , am I on the right approach?","['trigonometry', 'complex-numbers']"
1627445,Prove with use of derivative [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How to prove this inequality using derivative ? For each  $x>4$ , $$\displaystyle \sqrt[3]{x} - \sqrt[3]{4} < \sqrt[3]{x-4} $$","['derivatives', 'inequality']"
1627456,Can Laplace solve every lineair differential equation?,"I'm learning about laplace tranform method to solve lineair differential equations but i'm wondering if laplace transformations can be used to solve every linear differential equations there is.
Or are there some limitations? I know that for the operator method the equation has to be from the form: $e^at$ / $sin(bt)$ / $cos(bt)$ and Polynomial.","['ordinary-differential-equations', 'laplace-transform']"
1627466,When An Infinite Product Topology Is Hausdorff?,"Is the following true: Suppose $(X_1, \tau_1)$ is Hausdorff while $(X_2, \tau_2)\space ...$ are not. $x, y \in X_1 \space\space U,V\in\tau_1\space x \in U , y \in V \space\space V \cap U = \emptyset $
since $X_1$ is Hausdorff. Lets try to test whether is $\prod(X_i, \tau_i) = (X, \tau)$ Hausdorff. let $x, y \in X$ then $ x = (x_1, x_2, ....),  y = (y_1, y_2, ....)$  let $ A=U \times X_2 \times ... $ and  $ B=V \times X_2 \times ... $ where $U$ and $V$ are two neiborhoods of $x_1$ and $y_1$ shuch that  $V \cap U = \emptyset $. Then $A \cap B = \emptyset $ since $ A \cap B = U\cap V \times X_2 \times ...$ So $(X, \tau)$ is Hausdorff since $x, y$ are arbitrary points and $A$ and $B$ are open.
Where am I wrong ?","['general-topology', 'infinite-product']"
1627497,"The Functional Inequality $f(x) \ge x+1$, $f(x)f(y)\le f(x+y)$","Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a continuous function that satisfies the following conditons. $$f(x)f(y)\le f(x+y)$$
$$f(x)\ge x+1$$ What is $f(x)$? It is not to difficult to find that $f(0)=1$. If $f(x)$ is differentiable, we can further these results so that $f'(0)=1$, and $f(x)=f'(x)$. However, I was not able to go any further than this. I believe that $f(x)=e^x$, but cannot prove it. Any help would be appreciated.","['functional-inequalities', 'calculus']"
1627525,Recommend book Taylor expansions,"I've taken up self-study of math and i start using the book called : Mathematical Analysis I Authors: Canuto, Claudio, Tabacco, Anita I would like to start from zero  to understand taylor expansion and i'm looking about  some textbook with homework problems with step-by-step math answers if possible and cover specialy those subjects: Local comparison of functions. Taylor expansions and applications I would appreciate any book recommendations. Thanks in advance.","['self-learning', 'big-list', 'book-recommendation', 'soft-question', 'analysis']"
1627539,"Prove that one of the elements can't be in the interval $(0,1)$","Let be $a,b,c\in\mathbb{R}$ so that the sum of two of them is never equal to $1$. Prove that atleast on of $\frac{ab}{a+b-1},\frac{bc}{b+c-1},\frac{ca}{c+a-1}$ can't be in the interval $(0,1)$. I aproached it with contradiction, but can't get sth that is not true.","['algebra-precalculus', 'inequality', 'fractions']"
1627558,$\nabla^2(\|\boldsymbol{x}-\boldsymbol{x}_0\|^{-1})=-4\pi\delta(\boldsymbol{x}-\boldsymbol{x}_0)$ with distributions defined on Schwartz space,"I know, from a recent enlightening answers received here , that, if we define the distribution represented by Dirac's $\delta$ on the space $K$ of test functions of class $C^\infty$ whose support is contained in a compact subset of $\mathbb{R}^3$, then$$\nabla^2\left(\frac{1}{\|\boldsymbol{x}-\boldsymbol{x}_0\|}\right)=-4\pi\delta(\boldsymbol{x}-\boldsymbol{x}_0)$$where the Laplacian obviously is to be intended in the sense of the derivatives of distributions. More explicitly, that means that $$\forall\varphi\in K\quad \int_{\mathbb{R}^3}f\,\nabla^2\varphi\,d\mu=-4\pi\varphi(\boldsymbol{x}_0)=:-4\pi\int_{\mathbb{R}^3}\delta(\boldsymbol{x}-\boldsymbol{x}_0)\varphi(\boldsymbol{x})$$where $f:\boldsymbol{x}\mapsto\|\boldsymbol{x}-\boldsymbol{x}_0\|^{-1}$ and the first integral is intended as a Lebesgue integral. I suspect that the identity may well also hold with $\varphi$ as a more generical function belonging to the Schwartz space, but I cannot generalise this excellent proof , to whose author I am immensely grateful, for the $\varphi\in K$. If the identity really holds with the distribution defined on the Schwartz space, how can it be proved? I $\infty$-ly thank any answerer.","['functional-analysis', 'dirac-delta', 'distribution-theory', 'laplacian']"
1627566,nested quantifiers (exactly one questions),"Express this statement using quantifiers, without using the uniqueness
quantifier.""There is exactly one student in this class who has taken exactly one mathematics class at this school"" 
T (x, y):means that student x has taken class y and the domain is all
students in this class The correct answer says : ∃x∀z((∃y∀w(T (z, w) ↔ w = y)) ↔ z = x) My answer is: ∃x∃y,(T(x,y)∧ (∀m,∀n, T(m,n)∧((x=m)∧(y=n)))) What's wrong with my answer?","['quantifiers', 'predicate-logic', 'logic', 'discrete-mathematics']"
1627573,How to indicate the space of sesquilinear forms?,"Is there a canonical way to indicate the space of sesquilinear forms? For bilinear forms I can use the (0,2) Tensor space, but since sesquilinear forms are not linear in one variable I don't know how to indicate the space where they live. Does anybody have an idea?
Thank you","['riemannian-geometry', 'tensors', 'functional-analysis', 'differential-geometry', 'linear-algebra']"
1627598,Applications of Dominated/Monotone convergence theorem,"Consider a measure $\mu$ on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ where $\mathcal{B}(\mathbb{R})$ is the Borel $\sigma$-algebra on $\mathbb{R}$. Consider the function $f: [0,\infty)\rightarrow \{1,0\}$, $f(u)=1_{[u=0]}$ where $$
1_{[u=0]}=
\begin{cases}
1 \text{ if $u=0$}\\
0 \text{ otherwise}
\end{cases}
$$ Consider the function $g: [0,\infty)\rightarrow [0,\infty)$, $g(u)=u$. Statement : (a) it is possible to construct a sequence of measurable continuous functions $\{f_m(\cdot)\}_m$ such that $\lim_{m \rightarrow \infty}f_m(u)=f(u)$ $\forall u$, and $1\geq f_m(u)\geq f_{m+1}(u)$ $\forall u,m$ (hence, we can apply dominated convergence theorem). (b) it is possible to construct a sequence of measurable continuous functions $\{g_m(\cdot)\}_m$ such that $\lim_{m \rightarrow \infty}g_m(u)=g(u) \forall u$ and $0\leq g_m(u)\leq g_{m+1}(u)$ $\forall u,m$ (hence, we can apply monotone convergence theorem). (from van der Vaart ""Asymptotic Statistics"" proof Lemma 6.4 p.89) . Question : on which result this statement is based? My attempt : I know the following two results but I'm not sure whether they fit for the statement above (1) Consider $h: \mathbb{R}\rightarrow [0,\infty)$. Construct the partition of $[0,\infty)$ in $2^{2m}+1$ intervals of length $2^{-m}$. Let $I_{m.k}$ be the $k$-th interval. Define $h_m(u)=\sum_{k=1}^{2^{2m}+1}\frac{k-1}{2^m}1_{[f^{-1}(I_{m,k})]}$. Then $\lim_{m \rightarrow \infty}h_m(u)=h(u)$ $\forall u$. (2) Consider an open set $G \in \mathcal{B}(\mathbb{R})$. Then, there exists a sequence of functions $h_m(\cdot)$ such that $0\leq h_m(u)\leq h_{m+1}(u)$ $\forall m,u$, $\lim_{m \rightarrow \infty}h_m(u)=1_{[u \in G]}$ $\forall u$ (implying that $h_m(u) \leq 1$ $\forall m,u$).","['measure-theory', 'lebesgue-integral', 'sequences-and-series', 'limits']"
1627600,If $U$ is unitary operator then spectrum $\sigma(U)$ is contained inside the unit circle,"In a Hilbert space, let $U$ be a continuous operator which it unitary. Prove $\sigma (U)\subseteq \Bbb{S}^1$ . It is important for me to know how I am doing, and I didn't come by a clear explanation so it would be appreciated to have your correcting and guiding. Attempt: $U^{*}U=UU^{*}=I$ and therefore $U^{-1}=U$ . I have $$
\| U x \|^2
= \langle U x, U x \rangle
= \langle U^* U x, x \rangle
= \langle x, x \rangle
= \|x\|^2
$$ and the same goes for $U^*$ and therefore $\| U \| = \| U^{-1} \| = 1$ . Looking at $U-\lambda$ I can take $$
-\lambda U(U^{-1}-{1\over \lambda}I)
=U-\lambda I.
$$ Clearly, for $\lambda \ne 0$ , $-\lambda U$ is defined and is a linear operator. $(U^{-1}-{1\over \lambda}I)$ is invertible if $|\lambda|\ge 1$ and therefore $\sigma (A^{-1})=\sigma (A)\subseteq \{|z|\le1\}$ . But from the RHS, $U-\lambda I$ is invertible if $|\lambda|\le 1$ and therefore $\sigma(A)\subseteq \Bbb{S}^1$ . I have noticed my claims tend to have holes in them many times which I cannot spot. If it is substantially true, how can I make sure it is completely formal?","['solution-verification', 'linear-algebra', 'hilbert-spaces', 'vector-spaces']"
1627613,If $[x+0.19] +[x+0.20] +[x+0.21] +\cdots [x+0.91] =546$ find the value of $[100x]$..,Problem : If $[x+0.19] +[x+0.20] +[x+0.21] +\cdots [x+0.91] =546$ find the  value of $[100x]$ where [.] represents the greatest integer function less than equal to x. My approach : $x +1.19 = x + \frac{19}{100} = \frac{100x+19}{100}$ Similarly other terms Not getting any clue further please suggest will be of great help.,"['algebra-precalculus', 'functions', 'ceiling-and-floor-functions']"
1627618,Natural bijections between Dyck paths,"A dyck path with $2n$ steps is a lattice path in $\mathbb{Z}^2$ starting at the origin $(0,0)$ and going to $(2n,0)$ using the steps $(1,1)$ and $(1,-1)$ without going below the x-axis. What are some natural bijections between the set of such dyck path with $2n$ steps? For example of course the identity is such a bijection but also the map sending a given dyck path to the same dyck path but viewing $(2n,0)$ as the origin is a bijection. What are some other examples of natural(or easy to write down) bijections? Note that dyck path correspond to ballot sequences, which are sequences of length $2n$ consisting of 1(corresponding to (1,1)) and -1(corresponding to (1,-1)) with the property that in this sequence the partial sums are never negative. So maybe its sometimes easier to write down such bijections explicitly using ballot sequences. Note that the number of dyck paths is the Catalan number.",['combinatorics']
1627627,An elementary introduction to Puiseux series?,"While studying Analytic combinatorics of Flajolet and Sedgewick (to be more specific, the coefficient asymptotics of algebraic functions), I have come across the concept of Newton-Puiseux expansions. Flajolet and Sedgewick explain these series only in a sketchy way, so I have been looking for some other sources in order to gain better understanding. However, most explanations I have found rely on quite sophisticated mathematics, such as modern algebraic geometry or Riemann surfaces. I do not doubt this is the most elegant way to approach the topic. But on the other hand, none of these have been known in the era of Newton. For this reason, I guess there should be some more elementary approach to the topic. This would be preferable for me, as my mathematical background is quite modest. So my question is: can you recommend me a reference for a more-or-less elementary introduction to Newton-Puiseux series? Thanks a lot.","['complex-analysis', 'reference-request', 'power-series']"
1627652,How could I calculate $\lim_{t\rightarrow 0}\frac{x(t)}{t^{\sqrt{3}}}$ for the following,"If $x(t)$ satisfy $t^2x''+tx'+(t^2-3)x=0$ then what is the limit $$ \lim_{t\rightarrow 0}\frac{x(t)}{t^{\sqrt{3}}}$$ It is very important to me, hint or full help, please. I know that the solution to the differential equation is a Bessel function so its solution are determinate by $$\{ J_\sqrt{3}(t)\ ;\ J_{-\sqrt{3}}(t)\}\ \ \ \ (because \ \alpha\not\in\mathbb{Z})$$
but I have big problem when I work with $J$ bessel. I need help please.","['bessel-functions', 'ordinary-differential-equations', 'calculus', 'limits']"
1627662,Prove that $f:\mathbb{R}^n\to\mathbb{R}$ is continuous,"Let $f:\mathbb{R}^n\to\mathbb{R}$ such that for every continuous curve, $\gamma:[0,1]\to\mathbb{R}^n$: $f\circ\gamma$ is continuous. Prove that $f$ is continuous. So I know we shall prove it by a contradiction. Let's assume that $f$ isn't continuous at $x_0$. Then, there's a sequence, $\{x_k\}$ converging to $x_0$ such that $\lim_{k\to\infty} f(x_k) \ne f(x_0)$. Now, I need to have some curve in order to get a contradiction. I'd be glad to get help with that. Thanks.","['multivariable-calculus', 'general-topology', 'curves', 'calculus']"
1627684,Exercise 32 from chapter 4 (“Hilbert Spaces: An Introduction”) of Stein & Shakarchi's “Real Analysis” 2,"I have a question about the following problem in Stein and Shakarchi's book. Consider the operator $T:L^2([0,1]) \to L^2([0,1])$ defined by $$T(f)(t) = tf(t).$$
  (a) Prove that $T$ is a bounded linear operator with $T = T^*$, but that $T$ is not compact.
  (b) However, show that $T$ has no eigenvectors. First of all, I don't really get how this operator works. Could you guys give me insight into this operator and guide me through these problems? Thanks!","['functional-analysis', 'real-analysis']"
1627757,Residue fields of schemes of finite type (over $\mathbb{Z}$),"Suppose $X$ is a scheme of finite type over $\mathbb Z$ . I want to prove that: (1) The residue fields of closed points of $X$ are finite; (2) For a given $q=p^n$ with $p$ prime, there is only a finite number of closed points of $X$ whose residue field is $\mathbb F_q$ . For (1), I see that one has to use some form of Nullstellensatz. First, we suppose $X=\operatorname{Spec}(A)$ affine, and $A=\mathbb{Z}[X_1,...,X_n]/I$ . We need to show that if $m$ a maximal ideal of $A$ , then $A/m$ is finite. Note $p=\mathbb Z\cap m$ . If $p$ is maximal, we get that $A/m$ is of finite type over $\mathbb F_p$ thus a finite extension by Nullstellensatz and we are done. But $p$ could be the $(0)$ ideal in which case I neither conclude nor get a contradiction. For (2), we can again argue locally, so we need to show that $A$ contains only a finite number of maximal ideals $m$ with $A/m$ of fixed cardinality $q=p^n$ . I get an impression that to count these ideals its the same as to count the number of automorphisms of $\mathbb{F}_{q}$ which fix $\mathbb F_p$ , but the corrspondence is not bijective, so we only get an upper bound which is enough. Is this idea correct?","['finite-fields', 'algebraic-geometry', 'schemes', 'affine-schemes', 'commutative-algebra']"
1627763,Relationship between row space and orthogonal component of kernel of complex vector space,"When we consider the real vector space, row space is equal to orthogonal complement of the null space (kernel). This fact can be proved as follows. Let's consider linear map
$A : \mathbb{R}^n \rightarrow \mathbb{R}^m$. Row space of the matrix is
$\mathrm{Row}(A) := \mathrm{span}(\{\mathbf{r}_i\}_{i=1,2,...,m})$.
Where, $\{\mathbf{r}_i\}_{i=1,2,...,n}$ is row vector of matrix A. On the other hand, the kernel of the matrix is 
$\mathrm{Kernel}(A) := \{ \mathbf{v}\in\mathbb{R}^n | A\mathbf{v}=0 \}
= \{ \mathbf{v}\in\mathbb{R}^n | \mathbf{r}_i\cdot\mathbf{v}=0 \mathrm{\ for\ all\ i }\}$ Then, orthogonal subspace of the kernel is nothing but row space. However, when we consider complex vector space $A : \mathbb{C}^n \rightarrow \mathbb{C}^m$, inner product of row vector is $\mathbf{r}_i^{*}\cdot\mathbf{v}$.
So, in this case, the condition $\mathbf{r}_i^{*}\cdot\mathbf{v}=0$ is different from the definition of the kernel. Is there some relationship between row space and orthogonal space of the kernel of complex vector space? Ref: https://en.wikipedia.org/wiki/Row_and_column_spaces#Relation_to_the_null_space","['matrices', 'linear-algebra']"
1627787,Find the limit of $\lim_n a_n$,"Let , $\{a_n\}$ be a sequence of real numbers such that $$\lim_n \left|a_n+3\left(\frac{n-2}{n}\right)^n\right|^{1/n}=\frac{3}{5}.$$Compute $\displaystyle \lim_na_n$. I have no idea about this..Please help.","['real-analysis', 'limits']"
1627823,Minimum of a function that is an expectation,"Let $X$ and $Y$ be two random variables. Define a function $f$ by 
  $$f(t)=E[(X+tY)^2]$$
  Find the value $t$ that minimizes $f(t)$ in terms of $E(X^2),E(Y^2),$ and 
  $E(XY)$. Evaluate the function at this value $t$ and show that 
  $[E(XY)]^2\leq E(X^2)E(Y^2)$? So, I expanded the expectation $f(t)=E[X^2+2tXY+t^2Y^2]=E[X^2]+2tE[XY]+t^2E[Y^2]$ $f'(t)=2E[XY]+2tE[Y^2]=0 \Rightarrow t=-\dfrac{E[XY]}{E[Y^2]}$ $f''(t)=2>0$ and thus the $t$ value is at a minimum. Evaluating $f$ at this $t$ gives
\begin{align*}
f\left(-\dfrac{E[XY]}{E[Y^2]}\right)&=E[X^2]+2\left(
-\dfrac{E[XY]}{E[Y^2]}\right)E[XY]+\left(-\dfrac{E[XY]}{E[Y^2]}\right)^2E[Y^2]\\
&=E[X^2]-\dfrac{E[XY]^2}{E[Y^2]}
\end{align*}
I think I'm close, but I'm stuck here.","['probability-theory', 'probability']"
1627830,Calculate the limit of $\lim _{x\to 0}\left(\frac{1-\frac{\left(1+x\right)^{\frac{1}{x}}}{e}}{x}\right)$,"I have to calcuate this limit: $$\lim _{x\to 0}\left(\frac{1-\frac{\left(1+x\right)^{\frac{1}{x}}}{e}}{x}\right)$$ I have no idea how to start, maybe taylor series? Thanks.","['taylor-expansion', 'limits']"
1627863,Limits with complex numbers,"$$\lim_{z\to 0} \frac{z^*}{z}$$ The way I see it is that it's asking what happens when $z$ approaches $0$. However, I can't just say undefined because $z$ is actually $z=x+iy$. So if I take the complex conjugate of the bottom I remove the $i$ from the denominator and can get it into standard form. The problem then is what does it mean to say that $z$ goes to $0$ when I have $x, y, i$?","['complex-numbers', 'limits']"
1627873,Who knows further prime factors of $3^{3^3}+4^{4^4}=3^{27}+4^{256}\ $?,"The partial prime factorization of $$3^{3^3}+4^{4^4}=3^{27}+4^{256}$$ is $$43\times 691\times C150$$ , where C150 = 451243830308033423066470063548138731446820106370692739801553577347348434357807928408503829911718891653149525735754923799473425194761436743056617460491 is a composite $150$ -digit number without a small prime factor. It is very likely that it has no prime factor with less than $30$ digits because the number passes at least $1000$ ECM- $250$ K-curves and at least $1200$ ECM- $1$ M-curves. Can I take advantage of the special form of the number and accelerate the search of further prime factors ? Does anyone know further prime factors of $3^{27}+4^{256}$ ?","['number-theory', 'prime-factorization']"
1627894,Number of points on a line in a finite projective plane,"I've been reading some proofs regarding finite projective planes of order n, and often they start out by assuming that each line contains n+1 points. Is this a fact that follows from the axioms for finite projective planes? Or does this relate to the order of the finite field being projected? (for example in the Fano plane, does the fact that each line contains 3 points follow from the fact that there are 3 elements {0,1,2} in Z_2?) I haven't been able to find a clear explanation of this (clearly I do not yet have a solid understanding of how such planes are constructed). If someone could enlighten me that would be excellent.","['projective-space', 'projective-geometry', 'algebraic-geometry', 'geometry']"
1627911,Line bundle with a nowhere vanishing global section is trivial.,"Let $k$ be a field and $X$ be a projective variety over $k$. I think it should be true that if $L$ is a line bundle on $X$ such that exists $s \in \Gamma(X,L)$ with $s_x \neq 0$ for all $x \in X$, then $L$ is isomorphic to $\mathcal{O}_X$. At least this is what analytic intuition would tell me. I am looking for a good proof or reference of this fact.",['algebraic-geometry']
1627916,Asymptotic relation for the following series?,"Questions Is the asymptotic relationship correct? How do I determine $c_1$ and $\kappa$? As, $|s| \to 0$ $$ \sum_{r=1}^\infty s^r \ln(r) \sim c_1 \sqrt{s} + (\kappa - 1 + \frac{\ln(2 \pi)}{2} )s$$ Is it possible to find $A_r$ using this relationship (for any of the r's)? Where, $$ \sum_{r=1}^\infty A_r(s) \ln(p_r) \sim c_1 \sqrt{s} + (\kappa - 1 + \frac{\ln(2 \pi)}{2} )s$$ Background Recently I found a technique as in a previous question: Number-theoretic asymptotic looks false but is true? This led me to another series with interesting properties: $$ K(s) = s \ln 1 + s^2 \ln 2 + s^3 \ln 3 +  \dots  =  \sum_{r=1}^\infty s^r \ln(r) $$ $$ \implies K(s) =  A_1(s) \ln 2 + A_2(s) \ln(3) + A_3(s) \ln 5 + \dots = \sum_{r=1}^\infty A_r(s) \ln(p_r) $$ Where $p_r$ is the r'th prime Now consider the following: $$ K(s) =  \sum_{r=1}^\infty s^r \ln(r) $$
$$ \implies \sum_{r=0}^\infty s^r K(s) = s \ln 1! + s^2 \ln 2! + s^3 \ln 3! + \dots $$
$$ \implies \frac{ K(s)}{1-s} = \sum_{r=1}^\infty s^r \ln r! $$ Using Stirling approximation: $$ \implies \frac{ K(s)}{1-s} = \sum_{r=1}^\infty s^r (r \ln(r) - r +\frac{1}{2}\ln(2 \pi) + \frac{1}{2} \ln r + \sum_{k=1}^\infty a_k/r^k)  $$ where $a_n$ is the coefficients of the $1/r$ error terms $$ \implies \frac{ K(s)}{1-s} = s \frac{dK}{ds} - \frac{s}{(1-s)^2} + \frac{\ln(2 \pi) s }{2(1-s)} + \frac{K}{2} + a_1 \text{Li}_1(s)  + a_2 \text{Li}_2(s)\dots $$ where $\text{Li}_r(s)$ is the $r$'th poly-logarithmic function.
Taking $|s| \to 0 $ and using the asymptotic relation of $\text{Li}_r(s) \sim s$ $$ \implies \frac{ K(s)}{2} \sim s \frac{dK}{ds} - s + \frac{\ln(2 \pi) s }{2} +  \kappa s $$ whre $\kappa= \sum a_r$ and now solving the differential equation: $$ K(s) \sim c_1 \sqrt{s} + (\kappa - 1 + \frac{\ln(2 \pi)}{2} )s$$","['prime-numbers', 'asymptotics', 'proof-verification', 'number-theory', 'ordinary-differential-equations']"
1627942,"Limits: ""does not exist"" vs ""cannot be evaluated""","Assuming we have a limit which doesn't exist, i.e.
$$\lim_{x\rightarrow x_0}{f(x)} \not{\exists}$$ Is the above wording and notation mathematically equivalent to saying ""The limit cannot be evaluated""? Of course, a limit that doesn't exist can't be evaluated. But can there be a limit that exists and yet cannot be evaluated?","['notation', 'limits']"
1627949,Bounding series with integrals,"Let $f(r)$ be a positive, continuous real function on $[N, \infty)$.
Is it always true that
$$
\int^{\infty}_{N+1} f(r) dr \leq \sum\limits_{i=N}^{\infty} f(i) \leq \int_{N-1}^{\infty} f(r) dr?
$$
What would I need in order it to be true? Note: I see how to prove it when f(r) is strictly increasing or strictly decreasing, but I am looking for for some bounds that holds for a general increasing function.","['real-analysis', 'integration', 'definite-integrals', 'sequences-and-series']"
1627973,Expectation of a function of a normally distributed random variable,"Consider that I have to produce this result: $$E[u(W_0+r(\theta))] = u(W_0)+\theta-\frac 12\rho\sigma^2$$ From this: $$
E[u(W_0+r(\theta))]
    = \int_{-\infty}^\infty u(w_0+r) \frac{1}{\sigma \sqrt{2\pi}}
                            \exp\left(-\frac{(r-\theta)^2}{2 \sigma^2}\right) dr
$$ And: $$\int_{-\infty}^\infty
          \frac{1}{\sigma \sqrt{2\pi}}
                            \exp\left(-\frac{(r-\theta)^2}{2 \sigma^2}\right) dr = 1 $$ Also:
$u(w) = -\exp(-\rho w) $ And:
$r(\theta) \sim N(\theta, \sigma^2)$ I have reached the point where I removed the constants, and realize that I have both a positive exponent and negative exponent, and I believe no way to combine them. Help is much appreciated. Thanks.","['statistics', 'normal-distribution', 'calculus', 'probability-distributions']"
1628000,Group conjecture,"Conjecture : Given a finite group $G$ and a subset $A\subset G$. Then
  $\{A,A^2,A^3,\dots\}$ is a group iff  $\forall n\in \mathbb N: |A^n|=|A^{n+1}|$. Given that the composition between the subsets $A,B\subset G$ is 
$A\cdot B=\{g\in G|\exists a\in A\exists b\in B:g=a\cdot b\}$. Example: suppose that $N\subset G$ is a normal subgroup, then the cosets 
$\{Ng,Ng^2,...\}$ have the same cardinality and constitutes a group, while for random sets the cardinality seems to grow when multiplying: { 3412 2143 4321 1234 } { 2143 1234 } nswap pnormal . -1  ok
{ 3412 2143 4321 1234 } { 2143 1234 } pquotient set. {{3412,4321},{2143,1234}} ok
{ 4321 3412 } go  ok
gen. {4321,3412} ok
gen. {2143,1234} ok
gen. {4321,3412} ok
ndrop  ok
{ 2431 2341 } go  ok
gen. {2431,2341} ok
gen. {4132,3142,4312,3412} ok
gen. {1234,1243,3214,4213,1324,1423,3124,4123} ok
gen. {2413,2314,3421,4321,1423,1324,2341,2431,2143,2134,3241,4231,1243,1234} ok
gen. {4123,3124,4321,3421,2143,2134,4132,3142,4213,3214,4231,3241,3412,4312,1432,1342,2413,2314,2431,2341} ok
gen. {4231,3241,1234,1243,3214,4213,1432,1342,1324,1423,2134,2143,2314,2413,4123,3124,4321,3421,4132,3142,4312,3412} ok
gen. {3412,4312,2314,2413,2341,2431,2143,2134,4321,3421,3241,4231,1342,1432,3142,4132,1234,1243,3214,4213,1324,1423,3124,4123} ok
gen. {4123,3124,3142,4132,3412,4312,1432,1342,3214,4213,2413,2314,3421,4321,1423,1324,2341,2431,2143,2134,3241,4231,1243,1234} ok
ndrop  ok","['finite-groups', 'computational-algebra', 'conjectures', 'group-theory']"
1628015,Proving the set of all finite or countable unions of intervals is not a Sigma Algebra,"I would like to extend on a question I asked here Consider a set $J$ of all (open, closed, half-open, singleton, empty) intervals on $[0,1]$ Now consider further a set $B$ which is the set of all finite or countable unions of elements of $J$. According to the text I'm reading, $B$ is not a $\sigma$-algebra. I suspect that it is because it is not closed under countable intersections, however I can't understand why. Surely any countable intersection is simply an interval or the empty set? Can't come up with any sort of contradiction.",['probability-theory']
1628033,Riemann Integrability of Step Function,"Call $f: [a,b] \to \mathbb{R}$ a step function if there exist a partition $P=\{x_0, \ldots, x_n \}$ of $[a,b]$ such that $f$ is constant on the interval $[x_i, x_{i+1})$. I'm having some issues proving that f is Riemann integrable. If $f$ was defined such that it was constant on the closed interval $[x_i, x_{i+1}]$ then it would be trivial to define a partition such that the supremum and infinum coincide, and the upper and lower sums would cancel. But since we are working with a half-open interval, I'm having issues defining the partitions that will allow me to prove integrability. All proofs I've seen online rely on theorems we haven't proven in class, so I'm assuming I must be missing something simple.","['real-analysis', 'integration', 'riemann-integration', 'riemann-sum']"
1628106,Proof of the Axiom of Choice,"This exercise is from Bloch's book and can be found here . Bloch introduces equivalent variations of the axiom of choice where the one that will be proven is stated in terms of functions: AC1 Let $I$ be a nonempty set, and let $\{A_i\}_{i \in I}$ be a family of nonempty sets indexed by I. Then there is a function $f: I \rightarrow \bigcup_{i \in I} A_i$ such that $f(i) \in A_i$ for all $i \in I$. As you can read from the text from the above link, the author suggests that AC1 can be deduced from AC2 below AC2 Let $I$ be a nonempty set, and let  $\{A_i\}_{i \in I}$ be a family of nonempty, pairwise disjoint sets indexed by I. Then there is a function $f: I \rightarrow \bigcup_{i \in I} A_i$ such that $f(i) \in A_i$ for all $i \in I$. Since the second definition implies the first, it is sufficient to prove the second version of AC, which will imply the first version. We will prove that if every surjective function has a right inverse, then AC2 is true, from which it follows that AC1 is true. Here is my attempt of a proof. Suppose that every surjective function has a right inverse, and let $f: A \rightarrow B$ be such a surjective function with a right inverse $g: B \rightarrow A$ where $A$ and $B$ are nonempty sets. Since $f$ is surjective and $g$ is a right inverse of $f$ it follows that $g$ is injective (this fact can be proven by contradiction). Since $g$ is a function every $b \in B$ is defined, so that the image $g(\{b\}) \neq \varnothing$. Further, the injectivity of $g$ implies that $g(\{b_1\}) \cap g(\{b_2\}) = \varnothing$ for arbitrary $b_1, b_2 \in B$ such that $b_1 \neq b_2$. Observe also that $g(\{b\})$ contains only one element for each $b$. Finally,  it is clear that $A = \bigcup_{b \in B} g(\{b\})$ since $g$ is injective,$f$ is defined on $A$, and $g$ is defined on $B$. If we now consider the family $\{g(\{b\})\}_{b \in B}$, it is clear that this collection contains nonempty, pairwise dsjoint sets. Because $A = \bigcup_{b \in B} g(\{b\})$, we have $g:B \rightarrow \bigcup_{b \in B} g(\{b\})$, which is the desired choice function in the second definition above. Therefore, the AC of the first defintiion follows. $\blacksquare$ Any criticisms or advice on the above proof is most welcome. I am unsure about my reason for the set equality $A = \bigcup_{b \in B} g(\{b\})$, but it can be proven formally in the traditional manner in demonstrating that the two sets are subsets of each other, but I wanted to keep my reasons for their equality concise, so I extracted the main arguments from such a proof and put them in this proof for the AC. Thanks!","['axiom-of-choice', 'elementary-set-theory', 'proof-verification']"
1628121,A kind of converse to the Jordan curve theorem,"The Jordan curve theorem in $\mathbb{R}^2$ says that if $S$ is a closed curve in $\mathbb{R}^2$. Then $S$ splits $\mathbb{R}^2$ into exactly two connected components $A$ and $B$. I was thinking about a kind of converse to this problem which is as follows. Let $S$ be a closed and bounded set in $\mathbb{R}^2$ and let $x,y\in\mathbb{R}^2 \setminus S$. Define $$A:=\{a\in\mathbb{R}^2 \setminus S: a\text{ and } x \text{ are path connected in } \mathbb{R}^2\setminus S\}$$ $$B:=\{b\in\mathbb{R}^2\setminus S: b\text{ and } y\text{ are path connected in }\mathbb{R}^2 \setminus S\}$$ If $A\cap B=\emptyset$ does there exists a connected $T\subset S$  such that $\mathbb{R}^2 \setminus T$ is split into exactly two connected components one which contains $A$ and the other that contains $B$. Any help or references on this would be much appreciated Note: Ive made an edit to reflect the comments using the topologists sine curve. I believe that now this will not be a counter-example because the 'quasi-circle"" is obviously connected. Also I apologize for not being more specific originally.","['general-topology', 'connectedness']"
1628154,"Is $\frac{1}{xy}$ convex for $x,y>0$","Is the function $$f(x,y) = \frac{1}{xy}$$ convex for $x,y>0$. I computed the hessian but it is very complicated and I do not know how to show it is positive semi definite.","['multivariable-calculus', 'real-analysis']"
1628155,The integral $\int\frac{{\rm d}x}{(x+3)^{\frac87}(x-2)^{\frac 67}}$,"I need to solve the integral
$$\int\frac{{\rm d}x}{(x+3)^{\frac87}(x-2)^{\frac 67}}$$ I can't seem to be able to set it in a suitable form; and I know it has to do something with the fact that $\frac87+\frac67=2$ which here would be a negative integer; but I still can't integrate it. The question is from a book and Answer was ""None of these"" So I don't know the answer; 
Thank you!
The other options are:
$$\frac{7}{5}\left(\frac{x+3}{x-2}\right)^{\frac{1}{7}}$$ 
$$\frac{7}{5}\left(\frac{x-2}{x+3}\right)^{\frac{-1}{7}}$$
$$\frac{5}{7}\left(\frac{x-2}{x+3}\right)^{\frac{1}{7}}$$ + constant of integration","['algebra-precalculus', 'integration', 'calculus']"
1628175,Calculating this limit without l'Hospital/Taylor/derivatives,Let's consider $$a_n = n^2 \log \left(\cos \frac 1n\right)$$ It's easy to calculate $$\lim_{n\to\infty} a_n$$ by using l'Hospital/Taylor. But how to do it without using anything that involves derivatives? (Pure sequences!),"['limits-without-lhopital', 'calculus', 'limits']"
1628192,What does echelon mean?,"When you solve a system of linear equations, you write down the augmented matrix and reduce this to reduced row echelon form. What is the meaning of the word echelon?","['matrices', 'linear-algebra', 'terminology']"
1628205,Huber loss vs l1 loss,"From a robust statistics perspective are there any advantages of the Huber loss vs. L1 loss (apart from differentiability at the origin) ? Specifically, if I don't care about gradients (for e.g. when using tree based methods), does Huber loss offer any other advantages vis-a-vis robustness ? Moreover, are there any guidelines for choosing the value of the change point between the linear and quadratic pieces of the Huber loss ?
Thanks.","['statistics', 'robust-statistics']"
1628223,Prove that identity matrix is the only idempotent $n x n$ matrix that is invertible.,"I do get that let's say 
$A=(A*A)$ $A^{-1} * A = A^{-1} *(A*A)$ $(A^{-1}*A)=(A^{-1}*A)A$ Then $I=I*A$, therefore $I=A$ Are those correct? Is there another or proper way to prove this?",['linear-algebra']
1628253,Subspace of a separable space is separable,"Let $(X,d)$ be a separable space and $Y \subset X$. Show that $(Y,d)$ is also separable. My approach is as follows: Let $(X,d)$ be a separable space and $Y \subset X$. Since $X$ is separable, by definition there exists a countable dense subset in $X$, call it $K$. We want to show that $K \subset Y$. Is this a valid approach to take to the proof? P.S. This is an analysis course, not strictly a topology course.","['real-analysis', 'metric-spaces']"
1628286,$\mu \ll m$ finite Borel implies $x \mapsto \mu(A + x)$ is continuous,"Why is it true that if $\mu$ is a finite Borel measure on $\mathbf{R}$ which is absolutely continuous with respect to Lebesgue measure $m$, then $x \mapsto \mu(A + x)$ is continuous for any fixed Borel set $A$? I want to use the $\epsilon-\delta$ characterization of absolute continuity, since $\mu$ is finite.  This amounts to showing that $m(A_x \Delta A_y) \ll 1$ when $|x-y| \ll 1$, where $A_x = A + x$ and $\Delta$ is the symmetric difference. Why is this true? I didn't find this post helpful: $\mu$ is a finite Borel measure on $\Bbb R$, absolutely continuous w.r.t. to the Lebesgue measure $m$. Prove that $x \mapsto \mu(A+x)$ is continuous. Thanks.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1628297,Using Set Builder Notation on a set that jumps in intervals?,"I'm new to the world of Discrete Mathematics. I have been reviewing a little on Set Builder Notation and have looked over the following site thoroughly: http://www.mathsisfun.com/sets/set-builder-notation.html What I can't seem to understand though, is how I would use set builder notation on the following examples taken from my textbook: $\{1,4,9,16,25,...,121\}$ $\{(a,1),(a,2),(a,3),(b,1),(b,2),(b,3),(c,1),(c,2),(c,3)\}$ In the first one, I am looking at all integers, so I believe I would use $\mathbb{Z}$, and I am going from $1$ to $121$ at an increasing rate of $(3,5,7,9,...)$. However, I'm not sure how to write it out completely. Also, on the second one, I am completely at a loss. Can someone break this down for me?","['notation', 'discrete-mathematics']"
1628325,"Prove convergence in measure (i.e., in probability) ""distributes"" over addition and respects nonnegativity.","Suppose $X_{n}$, $Y_{n}$, and $Z_{n}$ are random variables, with $Z_{n} \geq 0$ a.s. and $X_{n} \xrightarrow{p} X$, $Y_{n} \xrightarrow{p} Y$, and $Z_{n} \xrightarrow{p} Z$. Prove the following statements: $X_{n} + Y_{n} \xrightarrow{p} X + Y$ $Z \geq 0$ a.s. For 1, I noticed that $$\begin{split} \{ |X_{n} + Y_{n} - (X + Y)| > \epsilon \} &\subseteq \{ |X_{n} - X| + |Y_{n} - Y| > \epsilon \} \\ &= \bigcup \limits_{ q \in \Bbb Q} \{ |X_{n} - X| > q \} \cup \{ |Y_{n} - Y| \geq \epsilon - q \}.\end{split}$$ I'm not sure if this is the right approach to take.  Any help is appreciated. For 2, we consider for each $m \in \Bbb N$, $A_{m}= \{ Z < -1/m\}$.  I want to show $\{Z < 0\} = \bigcup \limits_{m = 1}^{\infty} A_{m}$ has measure $0$, and I'll do that by showing $A_{m}$ has measure $0$ for each $m$. Ok, well if $\omega \in A_{m}$, then $Z(\omega) < -1/m$, but we know for almost all $\omega$, $Z_{n} \geq 0$, so $|Z - Z_{n}| > 1/m$ for almost all $\omega \in A_{m}$, and this is true for every $n$.  By convergence in probability, $P\{| Z - Z_{n}| > 1/m\} \to 0$ as $n \to \infty$. But if $A_{m}$ has measure $\delta>0$, then for each $n$, $P\{|Z_{n} - Z| > 1/m\} > \delta$ (since $\{Z < -1/m\} \subseteq \{|Z_{n} - Z| > 1/m\}$), which contradicts convergence in probability.  Thus, $A_{m}$ has measure $0$, so $\{Z \geq 0\}$ has measure $1$. Is the above argument correct?","['real-analysis', 'probability-theory', 'proof-verification', 'convergence-divergence', 'analysis']"
1628330,Multivariable chain rule exercise,"$F:\mathbb{R}^2\rightarrow\mathbb{R}$ is a $C^2$ function with $F_x(1,1)=F_{yy}(1,1)=1$ and $F_y(1,1)=F_{xx}(1,1)=0$ and $g:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $g(r,\theta)=F(r\cos\theta,r\sin\theta)$. I'm asked to find the value of $g_{r\theta}(\sqrt2,\pi/4)$. This what I understand about the chain rule and what I've done so far. I can see $g$ as the composition of $F$ and $M=(r\cos\theta,r\sin\theta)$. $M$ is differentiable at $(\sqrt2,\pi/4)$ and $F$ is differentiable at $M(\sqrt2,\pi/4)=(1,1)$ because its partial derivatives exist and are continuous. The chain rule tells me that (*) $\frac{\partial g}{\partial r}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial r}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial r}=\frac{\partial f}{\partial x}\cos\theta+\frac{\partial f}{\partial y}\sin\theta$. Then I have to take the derivative of that with respect to $\theta$, and I don't know how to do that. EDIT: This is a silly question, but I'd also like to know what are the points of evaluation in (*) and/or the chain rule thesis in general. My textbook says it's understood, but I'm not too good at this. English is not my first language, so feel free to correct any mistake.",['multivariable-calculus']
1628404,"Prove that there exists some $c\in(-3,3)$ such that$ \ \ g(c) \cdot g''(c)<0$.","$f(x)$ is a differentiable function and $g(x)$ is a double differentiable function such that $|f(x)|\leqslant 1$ and $f'(x)=g(x)$ . If $$f(0)^2+g(0)^2=9$$ then prove that there exists some $c\in(-3,3)$ such that $ \ \ g(c) \cdot g''(c)<0$ . Attempt: Let us define a function $h(x) = g(x) g~'(x)$ . Then $$h'(x) =  g(x)g''(x) + \left( g'(x) \right)^2 \tag 1$$ If we prove that for some $c \in (-3,3), h~'(c) < 0,$ then $$g(c)g''(c) <0 \tag 2$$ Also, $$\left|f(0)\right| < 1 \implies f'(0) \in (-3,-2\sqrt 2 ) \cup (2\sqrt 2,3) $$ Could someone please advise me how do I move forward from here. Thank you very much for your help in this regard.",['calculus']
1628462,Proof for $e^{\frac{1}{n+1}}-1-\frac{1}{n}\leq0$,"I'm looking for a proof of $e^{\frac{1}{n+1}}-1-\frac{1}{n}\leq0$, optionally $\ln(n)+\frac{1}{n+1}\leq \ln(n+1)$","['real-analysis', 'inequality', 'analysis']"
1628464,What is Representation Theory?,"I'm beginning a course that uses representation theory, but I do not really understand what that is about. In the text I am following, I have the following definition: A representation of the Lie group $G$ on the vector space $V$ is a continuous mapping $\cdot \colon G \times V \to V$ such that for each $g \in G$, the translation $T_{g} \colon V \to V$ given by $T_g(v) = g \cdot v$, $v \in V$, is a linear map; $T_{e} = \mathrm{Id}$ where $e$ is the identity element of $G$; $T_{gh} = T_{g} T_{h}$ for $g, h \in G$. We call the pair $(V,\cdot)$ a real (resp. complex) representation and $V$ the representation space. What is the motivation behind this sort of definition? From my google searches I have seen different definitions, but I still don't really know why what I am reading is defined that way. Why a Lie group and not a regular group? etc.","['representation-theory', 'group-theory', 'motivation']"
1628489,"Proving that $\{(x,y):x^2-2x+y^2=0\}\cup \{(x,0):x\in [2,3]\}$ is connected","Let $E=\{(x,y):x^2-2x+y^2=0\}\cup \{(x,0):x\in [2,3]\}$. It appears to me from the sketch that this set is connected. However, I have no idea how to prove this, since the set is not convex, and we probably cannot use the line connecting any two points of the set argument to show the set is connected. I have almost no experience proving things like that, would appreciate some help.","['general-topology', 'real-analysis', 'analysis']"
1628501,"What is the derivative of $\int_{-10}^{-3} e^{\tan(t)} \,dt$ with respect to x?","We were learning about the Fundamental Theorem of Calculus today in my high school and the above integral came up as an example of an integral with a ""constant"" value. At first I accepted that the derivative of it was zero, but then I realized that since derivatives are defined for radian versions of the trigonometric functions, $e^{\tan(t)}$ does not satisfy the continuity requirement for it to be guaranteed to have an convergent integral on the interval $[-10, -3]$. I had my suspicions that the integral did not exist as a real number, so I checked it with the NINT tool on my graphing calculator and got $\infty$. I also cross-validated the result with WolframAlpha and reaffirmed that the integral does not converge to any real number. Yet a peer pointed out that the calculator also evaluated the derivative of $\infty$ as $0$, and WolframAlpha confirms that as well. My Question Is the derivative of $\infty$ truly $0$? It seems rediculous to me to treat it as a constant since it's only a concept, but has someone defined a use or reason for it to be that way?","['derivatives', 'integration', 'infinity', 'calculus']"
1628529,Derive the bias and MSE of the estimator $\hat{\beta}$,"Let $Y_1, Y_2, \ldots, Y_n$ denote a random sample of size $n$ from a population whose density is given by 
$$
f(y)=
\begin{cases} 
      3\beta^3 y^{-4} & \beta \leq y \\
      0 & \text{elsewhere}
   \end{cases}
$$
where $\beta > 0$ is unknown. (This is a Pareto distribution) Consider the estimator $\hat{\beta} = \min(Y_1, Y_2, \ldots, Y_n)$. a. Derive the bias of the estimator $\hat{\beta}$. b. Derive MSE($\hat{\beta}$). For part a Since this is an estimator that is dealing with random variables, and I only need the minimum, then I can represent this as ordered statistics. Namely, $\hat{\beta} = Y_{(1)}$. Using the formula for ordered statistics, the pdf is:
$$
g_{(1)}(Y_{(1)}) = n(1-F(y))^{n-1}f(y)
$$ My first question arises here, I am not sure how I am supposed to find $F(y)$ (and I am assuming that $f(y)$ is the given distribution above). Checking against the solution, I know that the pdf is supposed to be:
$$
g_{(1)}(Y_{(1)}) = 3n\beta^{3n}y^{(-3n-1)}
$$ After solving getting the pdf of $Y_{(1)}$, I can then use it in the bias formula. $$
Bias(\hat{\beta}) = E[\hat{\beta}] - \beta \\
Bias(\hat{\beta}) = E[3n\beta^{3n}y^{(-3n-1)}] - \beta
$$ My second question arises here. I'm not really sure how I am supposed to find the expectation. Namely, I don't know how I am supposed to treat the $y$ variable. I think that the $\beta$ variable is supposed to be treated the same way as a random variable, and that $n$ is supposed to be treated as a constant. After solving for the expectation, I should be able to get the Bias. For part b $$
MSE(\hat{\beta}) = Bias(\hat{\beta})^2 + Var(\hat{\beta})
$$
The bias part of this formula should follow from part a. This would leave just the variance to calculate. I would expect to use the variance formula:
$$
Var(\hat{\beta}) = E[\hat{\beta}^2] - E[\hat{\beta}]^2
$$
And I would also expect that both expectations in this formula could be calculated using the same methods in part a. Is this correct?",['statistics']
1628550,How many metrics are there on a set up to topological equivalence?,I want to find the number of topologically nonequivalent metrics on a set. I think if the cardinal of set is finite then we have one metric that is the discrete metric  and every metric on this set is equivalent with the discrete metric. Now what is the number of nonequivalent metrics on an infinite  set?,"['general-topology', 'metric-spaces']"
1628561,Prove $a+b \leq 1$ with two disjoint squares in a larger square.,"Two disjoint squares are located inside a square of side $1$. If the lengths of the sides of the two squares are $a$ and $b$, prove $a+b \leq 1$. I thought about setting up a coordinate axes with the leftmost vertex of the square at the origin. Then I will have to show that the sum of the side lengths is less than or equal to $1$ while the two squares being disjoint and still inside the square. This is where I get stuck.","['inequality', 'geometry']"
1628578,Maximal Ideals in Ring of Continuous Functions,"Dummit and Foote, 7.4.33(a) : Let $R$ be the ring of all continuous functions $[0,1] \to \mathbb{R}$ and let $M_c$ be the kernel of evaluation at $c \in [0,1]$, i.e. all $f$ such that $f(c) = 0$. Show that if $M$ is a maximal ideal in $R$ then $M = M_c$ for some $c$. I am aware of proofs that use the compactness of $[0,1]$; however I have been told that a simpler proof exists using only the isomorphism theorems for rings along with basic facts about ideals. I have tried on my own to prove it this way but still have no success; is it possible? (My attempts go something like: since $M$ is maximal, the quotient ring $R/M$ is a field. Then it would be nice to use the fact that $R/M$ has no zero divisors, but I can't quite make it work. Alternatively, consider $M \cap M_c$.)","['continuity', 'abstract-algebra', 'ring-theory', 'maximal-and-prime-ideals']"
1628584,Inverse tangent of a complex variable,Show that $$\tan^{-1}(z) = \frac{i}{2}\ln\left(\frac{i + z}{i - z}\right)$$ I tried this approach: $$\tan(w) = z$$ $$\tan(w) = \frac{\sin(w)}{\cos(w)}$$ $$\tan(w) = \frac{\frac{e^{iw}-e^{-iw}}{2i}}{\frac{e^{iw}+e^{-iw}}{2}}$$ let $$ u = e^{iw}$$ $$\tan(w) = \frac{u - u^{-1}}{i(u + u^{-1})}$$ But I don't see a way from there,"['trigonometry', 'complex-numbers']"
1628634,"prove there is a unique set $X$ such that every set $Y$, $Y∪X = Y$","For this proof. It seems obvious that $X=∅$ such that for every set $Y$,  $Y∪X = Y$ since the $Y∪X$ is just Y. How should I go about this? Let there be sets $X,Z$ Since $Y∪X = Y$ then, {$x| x ∈ Y ∨ x ∈ X$} = {$x| x ∈ Y$} Since $Y∪Z = Y$ then, {$x| x ∈ Y ∨ x ∈ Z$} = {$x| x ∈ Y$} Then equate the two {$x| x ∈ Y ∨ x ∈ Z$} = 
{$x| x ∈ Y ∨ x ∈ X$} Therefore conlude Z = X and prove that X is a unique set.","['elementary-set-theory', 'proof-verification']"
1628650,Expectation of $\bar X^2$,If all enumerated $X$s are observations from a population with a population $\mu$ and variance $\sigma^2$: Why is this true? $$\mathbb E\!\left(\bar X ^2\right) = \frac{\sigma^2}{n}+\mu^2$$ Source.,['statistics']
1628657,Dimensions of a rectangle containing a rotated rectangle,"Given sides a , b , and an arbitrary rotation Θ (0 - 360 degrees) around the centerpoint of the rectangle, how would I calculate sides A and B of a containing rectangle?","['rectangles', 'trigonometry', 'geometry']"
1628682,Plane of intersection of two spheres,"What is the plane of intersection of spheres $$x^2+y^2+z^2+2x+2y+2z+2=0$$ and $$x^2+y^2+z^2+x+y+z-\frac{1}{4}=0$$ I am not sure of how to do this, i just subtracted the two equations and i got a plane equation $$4x+4y+4z=-9$$ But i think its not correct since when two spheres meet we get a curved surface or solid. please correct me","['analytic-geometry', 'spheres', '3d', 'geometry']"
1628696,"How many integer solutions are there to the equation $x_1 + x_2 + x_3 + x_4 = 12$ with restrictions on $x_1,x_2,x_3,x_4$? [duplicate]","This question already has answers here : Counting bounded integer solutions to $\sum_ia_ix_i\leqq n$ (5 answers) Closed 1 year ago . How many integer solutions are there to the equation $x_1 + x_2 + x_3 + x_4 = 12$ with $x_i > 0$ for each $i \in \{1, 2, 3, 4\}$?  How many solutions with $x_1 > 1$, $x_2 > 1$, $x_3 > 3$, $x_4 \geq 0$? So for the first part I did this: We set $x_i = y_i + 1$ for $i = 1, 2, 3$ and obtain $y_1 + y_2 + y_3 = 12 − 3 = 9$. So, there is one-to-one correspondence between the positive integer solutions of $x_1 + x_2 + x_3 = 12$ and the non-negative integer solutions of $y_1 + y_2 + y_3 = 9$. Hence, both equation have the same number of solutions. Since $y_1 + y_2 + y_3 = 9$ has ${9+3-1 \choose 3-1} = {11 \choose 2} = 55$ integer solutions. Is this correct? For the second part I do not understand how to figure out how many solutions with $x_1 > 1$, $x_2 > 1$, $x_3 > 3$, $x_4 \geq 0$?","['combinatorics', 'discrete-mathematics']"
1628728,Exercise 27 from chapter 4 (“Hilbert Spaces: An Introduction”) of Stein & Shakarchi's “Real Analysis” 2,"I'm having trouble with the following problem: Prove that the operator $$Tf(x) = \frac{1}{\pi} \int_0^\infty \frac{f(y)}{x+y} dy$$ is bounded on $L^2(0,\infty)$ with norm $||T|| \leq 1$. I have no idea where to start for this problem. 
I guess I can write $Tf(x) = \frac{1}{\pi} \int_0^\infty K(x,y)f(y) dy$  with $K(x,y) = \frac{1}{x+y}$ but I don't see how I can proceed from here. Can anyone help me out? Thanks!","['functional-analysis', 'real-analysis']"
1628741,Is this quadratic pointing up or down? How do I know?,"The equation is $-2x^2 + 4x + 30 = 0$. 
I simplified it to $-2(x^2 - 2x - 15)$. To know if it points up, I need to look at $ax^2$, and if $a > 0$ it is up and if $a$ is $< 0$ it is down. However, which version do I look at? In the original version, it would be pointing down. In the simplified version, it would be pointing up","['algebra-precalculus', 'quadratics', 'roots', 'graphing-functions']"
1628759,What are the Laws of Rational Exponents?,"On Math SE, I've seen several questions which relate to the following. By abusing the laws of exponents for rational exponents, one can come up with any number of apparent paradoxes, in which a number seems to be shown as equal to its opposite (negative). Possibly the most concise example: $-1 = (-1)^1 = (-1)^\frac{2}{2} = (-1)^{2 \cdot \frac{1}{2}} = ((-1)^2)^\frac{1}{2} = (1)^\frac{1}{2} = \sqrt{1} = 1$ Of the seven equalities in this statement, I'm embarrassed to say that I'm not totally sure which one is incorrect. Restricting the discussion to real numbers and rational exponents, we can look at some college algebra/precalculus books and find definitions like the following (here, Ratti & McWaters, Precalculus: a right triangle approach , section P.6): The thing that looks the most suspect in my example above is the 4th equality, $(-1)^{2 \cdot \frac{1}{2}} = ((-1)^2)^\frac{1}{2}$ , which seems to violate the spirit of Ratti's definition of rational exponents (""no common factors"")... but technically, that translation from rational exponent to radical expression was not used at this point. Rather, we're still only manipulating rational exponents, which seems fully compliant with Ratti's 2nd property: $(a^r)^s = a^{rs}$ , where indeed ""all of the expressions used are defined"". The rational-exponent-to-radical-expression switch (via the rational exponent definition) doesn't actually happen until the 6th equality, $(1)^\frac{1}{2} = \sqrt{1}$ , and that seems to undeniably be a true statement. So I'm a bit stumped at exactly where the falsehood lies. We can find effectively identical definitions in other books. For example, in Sullivan's College Algebra , his definition is (sec. R.8): ""If $a$ is a real number and $m$ and $n$ are integers containing no common factors, with $n \ge 2$ , then: $a^\frac{m}{n} = \sqrt[n]{a^m} = (\sqrt[n]{a})^m$ , provided that $\sqrt[n]{a}$ exists""; and he briefly states that ""the Laws of Exponents hold for rational exponents"", but all examples are restricted to positive variables only. OpenStax College Algebra does the same (sec. 1.3): ""In these cases, the exponent must be a fraction in lowest terms... All of the properties of exponents that we learned for integer exponents also hold for rational exponents."" So what exactly are the restrictions on the Laws of Exponents in the real-number context, with rational exponents? As one example, is there a reason missing from the texts above why $(-1)^{2 \cdot \frac{1}{2}} = ((-1)^2)^\frac{1}{2}$ is a false statement, or is it one of the other equalities that fails? Edit: Some literature that discusses this issue: Goel, Sudhir K., and Michael S. Robillard. ""The Equation: $-2 = (-8)^\frac{1}{3} = (-8)^\frac{2}{6} = [(-8)^2]^\frac{1}{6} = 2$ ."" Educational Studies in Mathematics 33.3 (1997): 319-320. Tirosh, Dina, and Ruhama Even. ""To define or not to define: The case of $(-8)^\frac{1}{3}$ ."" Educational Studies in Mathematics 33.3 (1997): 321-330. Choi, Younggi, and Jonghoon Do. ""Equality Involved in 0.999... and $(-8)^\frac{1}{3}$ "" For the Learning of Mathematics 25.3 (2005): 13-36. Woo, Jeongho, and Jaehoon Yim. ""Revisiting 0.999... and $(-8)^\frac{1}{3}$ in School Mathematics from the Perspective of the Algebraic Permanence Principle."" For the Learning of Mathematics 28.2 (2008): 11-16. Gómez, Bernardo, and Carmen Buhlea. ""The ambiguity of the sign √."" Proceedings of the Sixth Congress of the European Society for Research in Mathematics Education. 2009. Gómez, Bernardo. ""Historical conflicts and subtleties with the √ sign in textbooks."" 6th European Summer University on the History and Epistemology in Mathematics Education . HPM: Vienna University of Technology, Vienna, Austria (2010).","['fake-proofs', 'algebra-precalculus', 'rational-numbers', 'paradoxes', 'exponentiation']"
1628822,"Evaluate $\sum_{n=1}^{\infty }\int_{0}^{\frac{1}{n}}\frac{\sqrt{x}}{1+x^{2}}\,\mathrm{d}x$","I have some trouble in evaluating this series
$$\sum_{n=1}^{\infty }\int_{0}^{\frac{1}{n}}\frac{\sqrt{x}}{1+x^{2}}\mathrm{d}x$$
I tried to calculate the integral first, but after that I found the series become so complicated.
Besides, I found maybe the series equals to 
$$\sum_{k=0}^{\infty }\frac{\left ( -1 \right )^{k}\zeta \left ( 2k+\dfrac{3}{2} \right )}{2k+\dfrac{3}{2}}$$
this is definitely a monster to me. So I want to know is there a good way to solve this integral-series.","['zeta-functions', 'calculus', 'integration', 'sequences-and-series', 'analysis']"
1628835,Show that the set of points where a real-valued continuous sequence of functions converges is $F_{\sigma\delta}$,"By $F_{\sigma\delta}$ , I mean that the set can be expressed as a countable intersection of $F_\sigma$ sets. Let this sequence of functions be $f_n$ , and the set of points where $f_n$ converges be $C$ . Since $f_n$ must be Cauchy, I can define $C$ as $$C = \{x \in \mathbb{R}\:|\: \forall\epsilon\in\mathbb{N} \; \exists N\in\mathbb{N} \::\:|f_m(x) - f_n(x)| < \frac{1}{\epsilon} \: \forall m,n > N\}$$ . Looking at this set, I'm trying to prove $F_{\sigma\delta}$ by firstly trying to find some countable intersection. I try to rewrite $C$ as $$C = \bigcap\limits_{\epsilon \in \mathbb{N}}\{x \in \mathbb{R}\:|\: \exists N\in\mathbb{N} \::\:|f_m(x) - f_n(x)| < \frac{1}{\epsilon} \: \forall m,n > N\}$$ . Now I need to express each set being intersected as a countable union of closed sets to prove that it is $F_\sigma$ . Then the countable intersection of these must be $F_{\sigma\delta}$ . I tried to pick $N$ to describe this union, and rewrote $C$ as $$C = \bigcap\limits_{\epsilon \in \mathbb{N}}\bigg[\;\bigcup\limits_{N \in \mathbb{N}}\{x \in \mathbb{R}\:|\:|f_m(x) - f_n(x)| < \frac{1}{\epsilon} \: \forall m,n > N\}\;\bigg]$$ . But this option confuses me because $N$ depends on $\epsilon$ , so I don't know whether it would make sense to let both vary freely like that. Am I going in the wrong direction?",['real-analysis']
1628840,How do I solve $\int4\cos^2(x) dx$? [duplicate],"This question already has answers here : Evaluating $\int P(\sin x, \cos x) \text{d}x$ (3 answers) Closed 8 years ago . I have the basic idea of how to work out the integral of a trig function, but am having trouble in applying the concept. Would really appreciate it if someone could help me. Thanks!","['indefinite-integrals', 'integration', 'trigonometry', 'calculus']"
1628879,Bound on variance of bounded random variable,"For a bounded random variable $X \in [a,b]$, we know $\operatorname{Var}(X) \le (b-a)^2/4$, see for example this answer . I am trying to give an alternate proof using symmetrization. If $Y$ is an independent copy of $X$, we can rewrite the variance as $\frac{1}{2} \mathbb{E}(X-Y)^2$, where the expectation is over $X$ and $Y$. Another formulation is $\mathbb{E}[(X-Y)^2\mathbf{1}[Y \ge X]]$. However, bounding $(X-Y)^2$ by $(b-a)^2$ gives the looser upper bound $(b-a)^2/2$. Is there a way to tighten this while still using this symmetrization idea, or can this approach not be used to get the optimal $(b-a)^2/4$ bound?","['expectation', 'probability', 'random-variables']"
1628951,A particle moves along the x-axis find t when acceleration of the particle equals 0,"A particle moves along the x-axis, its position at time t is given by $x(t)= \frac{3t}{6+8t^2}$, $t≥0$, where t is measured in seconds and x is in meters. Find time at which acceleration equals 0. I got the answer 0.5 and 0 but apparently i got the answer wrong. I know I need to use derivative twice. Can someone please help me Calculation
$velocity = \frac {-3(4t^2-3)}{2(4t^2+3)^2}$ $Acceleration = 36x(16x^4+8x^2-3)$ answer 0.5 and 0 was found through wolfram",['derivatives']
1628976,No non-trivial clopen sets in $\mathbb{R}$? How to give a direct proof? [duplicate],"This question already has answers here : If a nonempty set of real numbers is open and closed, is it $\mathbb{R}$? Why/Why not? (10 answers) Closed 5 years ago . How to give a direct proof of the following result? Let $A$ be a subset of $\mathbb{R}$ such that $A$ is both open and closed. Then $A$ is either empty or all of $\mathbb{R}$? My work: If $A$ is not empty, let $u \in A$. Then there is some open interval $(a,b)$ such that $$u \in (a,b) \subset A.$$ Now if $\mathbb{R} - A$ is not empty either, then let $v \in \mathbb{R} - A$. Then there is some open interval $(c,d)$ such that 
$$v \in (c,d) \subset \mathbb{R} - A.$$ Suppose that $u < v$. Since $$\emptyset \subset (a,b) \cap (c,d) \subset A \cap (\mathbb{R} - A) = \emptyset,$$
we must have $$(a,b) \cap (c,d) = \emptyset.$$
So we can conlclude that $$b \leq c.$$
But $b \in A$ and $c \in \mathbb{R} - A$. So we must have $$b < c.$$ What next? Can anybody here please help complete the proof from here on? An edit based on a comment by Marc Paul: Let us assume that the set $A$ is a non-trivial clopen set in $\mathbb{R}$. Let us define a function $f \colon \mathbb{R} \to \mathbb{R}$ as follows: 
$$f(x) \colon= \begin{cases} 1 \ \mbox{ for } \ x \in A; \\ 0 \ \mbox{ for } \ x \in \mathbb{R} - A. \end{cases}$$ Let $V$ be an open set in the range space $\mathbb{R}$. We show that the inverse image set $f^{-1}(V)$ is open in the domain space $\mathbb{R}$. The following cases arise: If $0, 1 \in V$, then $f^{-1}(V) = \mathbb{R}$. If $0 \in V$ but $1 \not\in V$, then $f^{-1}(V) = \mathbb{R} - A$. If $1 \in V$ but $0 \not\in V$, then $f^{-1}(V) = A$. And, if $0 \not\in V$ and $1 \not\in V$, then $f^{-1}(V) = \emptyset$. Thus, $f^{-1}(V)$ is open in the domain space $\mathbb{R}$. Hence the function $f$ is continuous. What next? How does this lead to our desired conclusion? [Yet another edit, again based on valuable comments from Marc Paul: ] So if both $A$ and $\mathbb{R} - A$ were non-empty, then let's suppose $a \in A$ and $b \in \mathbb{R} - A$, and we can assume without any loss of generality that $a < b$. Then $f(a) = 1$ and $f(b) = 0$. So by the intermediate value theorem there is a real number $c \in (a,b)$ such that $f(c) = 1/2$, which is a contradiction because the image set of $f$ does not contain $1/2$.","['general-topology', 'analysis']"
1629055,Normal Space of an orbit at a point,"Let $X$ be a curve of genus $g$. If $d>n(2g-1)$,then for any vector bundle $E$ of rank $n$ and degree $d$ over $X$ has $H^1(X,E)=0$ and $E$ is generated by its global sections. For such a vector bundle $dim(H^0(X,E)=d+n(1-g)$. Consider $R(2,p):=$ set of all holomorphic maps from $h:X\rightarrow G(2,p)$ such that $h^{*}(T):=E(h)$ has degree $d$ with determinant $\mathcal{O}_X$ where $T$ is the tautological bundle over $G(2,p)$ The map on sections $\mathbb{C}^p\rightarrow H^{0}(M,E(h))$ induced from the quotient bundle map $\mathbb{C}^p\times X\rightarrow E(h)$ is an isomorphism. $R(2,d)$ is a non-singular quasi-projective variety. $Sl(p)$ acts naturally on $R(2,d)$. The geometric invariant theory quotient $R(2,d)//Sl(p)$ can be naturally identified with $M(2,d)$. Where, $M(2,d):=\frac{\text{set of all vector bundles of rank 2 and degree d with determinant} \mathcal{O}_X}{\sim}$ $\sim$ is the equivalence defined by $E_1\sim E_2$ if and only if $gr(E_1)\sim gr(E_2)$ $M(2,d)$ has a decomposition $M(2,d)=\mathbb{Z}^{2g}\sqcup(\mathbb{k}-\mathbb{Z}^{2g})\sqcup (\text{points corresponding to stable vector bundles})$, where $\mathbb{Z}^{2g}$ represents all non-stable semistable vector bundles which is equivalent some $L\oplus L$ and $\mathbb{k}-\mathbb{Z}^{2g}$ represents all non-stable semistable points which are equivalent to $L\oplus L^{-1}$ such that $L\ncong L^{-1}$ Consider a point $l=[L\oplus L^{-1}]\in \mathbb{Z}^{2g}\subset M(2,d)$. Let $W_l$ be the etale slice (by Luna's theorem) of the unique closed orbit in $R(2,d)^{ss}$ over $l$. By deformation theory, the normal space of the orbit at a point $l$ which represents $[L+L^{-1}]$, is $N_l=H^{1}(End_0(L\oplus L))\cong H^1(\mathcal{O})\otimes sl(2)$, where the subscript $0$ denotes the trace free-part. Can someone please explain the last line? Why is the normal space to the orbit over a point is $H^{1}(End_0(L\oplus L^{-1}))$?","['moduli-space', 'deformation-theory', 'vector-bundles', 'algebraic-geometry']"
1629071,Gradient in terms of first fundamental form,"In Do Carmo's Differential Geometry of Curves and Surfaces , I'm having a quite hard time trying to solve Excersise 14 on pages 101-102. He defines the gradient of a differentiable function $f:S\to \Bbb{R}$ as a differentiable map $\text{grad}f:S\to\Bbb{R}^3$ which assigns to each point $p\in S$ a vector $f(p)\in T_p(S)\subset \Bbb{R}^3$ such that $$\qquad \langle \text{grad}f(p),v\rangle_p=df_p(v)\qquad\text{for all }v\in T_p(S)$$ The question is to express $\text{grad}f$ of $\phi(U)$ in terms of the coefficients $E,F,G$ of the first fundamental form in a parametrization $\phi:U\subset\Bbb{R}^2\to S$ . The solution is $$\qquad \text{grad}f=\frac{f_uG-f_vF}{EG-F^2}\phi_u\,+\,\frac{f_vE-f_uF}{EG-F^2}\phi_v$$ I'm not sure how to start. Sure that if $gradf$ sends points into vectors of the tangent plane, it must be a linear combination of $\phi_u$ and $\phi_v$ , but I don't know how to use the information that $\langle \text{grad}f(p),v\rangle_p=df_p(v)$ $\,$ for all $v\in T_p(S)$ to get the desired result. The $\dfrac{1}{EG-F^2}$ makes me think about the inverse matrix of the first fundamental form, but again, I don't see how can I relate one thing to the other. Any hints that can point me in the right direction would be appreciate. Thanks in advance!","['differential-geometry', 'surfaces']"
1629095,Let $x$ be a real number. Prove the existence of a unique integer $a$ such that $a \leq x < a+1$,"Let $x\in \mathbb{R}$ , Using the Well-Ordering Property of $\mathbb{N}$ and the Archimedean Property of $\mathbb{R}$, show that there exist a unique $a \in  \mathbb{Z}$ such that $a \leq x < a+1$ My approach so far:
Suppose $x$ greater than some Integer $a$. $x\geq a$ By Archimedean Property of $\mathbb{R}$, there exist $n_{x} \in \mathbb{N},  x<n_{x}$ Combining those 2 inequality I have $a\leq x < n_{x}$. But I do not know how to proceed from here, any help or insights is deeply appreciated. Thank you for reading my post.","['real-analysis', 'real-numbers', 'integers', 'elementary-number-theory']"
1629155,Finding the $n$-th derivative of $f(x)=e^{x}\sin(x)$,"I am trying to find the general form for the $n$-th derivative of $f(x)=e^{x}\sin(x)$. I have calculated the derivatives up to $5$, but I am having trouble coming up with a general rule. Here is my work so far: $$\begin{align}
f^{(1)}(x)&=e^{x}\cos(x) + e^{x} \sin(x)\\
f^{(2)}(x)&= 2e^{x}\cos(x)= e^{x}(\sin(x)+ \cos(x)) + e^{x}(\cos(x)- \sin(x))\\
f^{(3)}(x)&=2e^{x}\cos(x) -2e^{x} \sin(x)\\
f^{(4)}(x)&=-4e^{x}\sin(x)\\
f^{(5)}(x)&=-4e^{x}\cos(x) -4e^{x} \sin(x)\\
\end{align}$$ I am having trouble spotting the pattern to derive the formula for the nth derivative of this function. Any help would be greatly appreciated.","['derivatives', 'calculus']"
1629163,How to find derivative of $\sin\sqrt{x}$ using difference quotient?,"The definition of derivative of a function $f(x)$ is $$\lim_{h\to0} \frac{f(x+h)-f(x)}{h}$$ Using this definition, the derivative of $\sin\sqrt{x}$ will be: $$\lim_{h\to0} \frac{\sin\sqrt{x+h}-\sin\sqrt{x}}{h}$$ $$\lim_{h\to 0} \frac{\cos\left(\frac{\sqrt{x+h}+\sqrt{x}}{2}\right)\sin\left(\frac{\sqrt{x+h}-\sqrt{x}}{2}\right)}{h}$$ Now i got stuck. How to find the limit or simplify this expression? I get intuition that we have to use $$\lim_{x\to0}\frac{\sin x}{x} = 1$$ but that too is leading no where. I am unable to remove h from denominator. NOTE I know the derivative of $\sin\sqrt{x}$ is $\frac{\cos\sqrt{x}}{2\sqrt{x}}$ using chain rule, but this exercise was given to us for practice using division quotient.","['derivatives', 'limits-without-lhopital', 'calculus', 'limits']"
1629167,"What do these symbols mean: $\bigcap$, $\bigcup$, $\bigwedge$, $\bigvee$?","I know that some of these symbols are used in set theory like $A \cup B$, but that's not what I'm talking about. I have seen those symbols used in a way similar to $\Sigma$ summation and $\Pi$ product. I hope this makes sense, but I'm just wondering what it means.","['notation', 'elementary-set-theory']"
1629213,What is the fastest way to find the range of functions having modulus: $f(x) = |x+3| - |x+1| - |x-1| + |x-3|$,While solving problems I saw a question in which I was supposed to find the range of a function $$f(x) = |x+3| - |x+1| - |x-1| + |x-3|$$ I know the way in which I can take different cases of $x$ larger than $3$ and then $x$ between $3$ and $1$ and so on. But it gets a bit long as there are 5 cases and the competitive exam for which I am getting my brain ready for wants me to solve a question in average 1.5 minutes. I want to know if there exists a faster way to find its range.,"['algebra-precalculus', 'absolute-value', 'functions']"
1629219,Finding the $n$-th derivative of $f(x)=\log\left(\frac{1+x}{1-x}\right)$,"I am trying to find the general form for the $n$-th derivative of $f(x)=\log\left(\frac{1+x}{1-x}\right)$. I have rewritten the original formula as: $\log(1+x)-\log(1-x)$ for my calculations. I have calculated the derivatives up to $5$: $$\begin{align}
f^{(1)}(x)&=\frac{1}{x+1} - \frac{1}{x-1}\\
f^{(2)}(x)&= \frac{1}{(x-1)^{2}} - \frac{1}{(x+1)^{2}}\\
f^{(3)}(x)&=\frac{2}{(x+1)^{3}} - \frac{2}{(x-1)^{3}}\\
f^{(4)}(x)&=\frac{6}{(x-1)^{4}} - \frac{6}{(x+1)^{4}}\\
f^{(5)}(x)&=\frac{24}{(x+1)^{5}} - \frac{24}{(x-1)^{5}}\\
\end{align}$$ I have noticed that the exponent in the denominator corresponds to $n$, and the positions of $x+1$ and $x-1$ switch each time. I have also noticed that in the 3rd and 4th derivative, the numerator times the exponent in the denominator equals the numerator for the next derivative. I understand that the pattern is probably quite obvious, but I am just having a little issues piecing this all together to derive the general formula. Any help would be much appreicated, thank you.","['derivatives', 'functions']"
