question_id,title,body,tags
4476991,Limit and conditional expectation commute in a uniformly integrable sequence,"I am thinking of the next proposition: Proposition. Let $(\Omega, \mathcal{F}, P)$ a probability space, and $\{X_n\}_{n=1,\cdots}$ a uniformly integrable r.v. sequence s.t. $X_n \rightarrow X ~ \text{a.s.}$ , where $X$ is an $L^1$ r.v. Then for any $\mathcal{G}$ : a sub- $\sigma$ -algebra of $\mathcal{F}$ , we have \begin{equation*}
\lim_{n\rightarrow \infty} \mathbb{E}[X_n | \mathcal{G}] = \mathbb{E}[X | \mathcal{G}] \quad \text{a.s.}
\end{equation*} My quesiont is: Is this proposition true? I consider it indeed is true, for the following reason. For any $A\in \mathcal{G}$ and $R > 0$ , $\{X_n 1_A\}$ is a u.i. sequence, which is shown by \begin{equation*}
\sup_n \mathbb{E}[ |X_n 1_A|, |X_n 1_A| > R ] \leq \sup_n \mathbb{E}[ |X_n |, |X_n| > R ]
\end{equation*} with the right hand side going to $0$ with $R \rightarrow \infty$ (we denote $\mathbb{E}[X,A] = \mathbb{E}[X1_A]$ ). Thus by the commutability of conditional expectation and limit in a u.i. sequence we have \begin{equation*}
\mathbb{E}[X1_A] = \mathbb{E}[ \lim_{n \rightarrow \infty}X_n 1_A] = \lim_{n \rightarrow \infty} \mathbb{E}[X_n 1_A]. \qquad (1)
\end{equation*} On the other hand, \begin{align*}
& \mathbb{E}[ \lim_{n \rightarrow \infty} \mathbb{E}[X_n | \mathcal{G}], A] = \mathbb{E}[ ( \lim_{n \rightarrow \infty} \mathbb{E}[X_n | \mathcal{G}]) 1_A]  = \mathbb{E}[ \lim_{n \rightarrow \infty} (\mathbb{E}[X_n | \mathcal{G}] 1_A)] \\
=~& \mathbb{E}[ \lim_{n \rightarrow \infty} (\mathbb{E}[X_n 1_A | \mathcal{G}])]\\
=~& \lim_{n \rightarrow \infty} \mathbb{E}[ \mathbb{E}[X_n 1_A | \mathcal{G}]]\\
=~& \lim_{n \rightarrow \infty} \mathbb{E}[X_n 1_A]. \qquad (2)
\end{align*} Here to swap the expectation and the limit I used the fact that the sequence $\{ \mathbb{E}[Y_n | \mathcal{G}] \}_{n=1,\cdots}$ with a u.i. r.v. sequence $\{ Y_n \}_{n=1,\cdots}$ is u.i., which I presume is true. Therefore by (1) and (2), the above proposition holds. A previous post ( https://mathoverflow.net/questions/124589/uniformly-integrable-sequence-such-that-a-s-limit-and-conditional-expectation-d ) claims otherwise, which I guess is wrong: I think one can't compose a u.i. sequence $\{Z_n\} = \{X_n Y_n\}$ that satisfies the conditions written in the link above.","['conditional-expectation', 'uniform-integrability', 'probability-theory', 'probability']"
4477003,How to find a statistical function from this model?,"We note that the set of parameters $\quad \theta = (q_{k},\Sigma^{(k)} )_{k \in [K]} $ where $\Sigma^{(k)}$ is a SDP Matrix (symmetric definite matrix) We have also $\quad \mathbb{P}[z_{u}=k]=q_{k}\quad$ with $\quad q_{k}\geq0 \quad$ and $\sum_{k \in [K]} q_{k}=1 \quad and \quad z_{u}$ is random variable (type of $u\in [P]$ ) wich has values in [K] as $u \in [P],\quad q_{k}*P \quad$ is the number of u such that $z_{u}=k \quad$ and with $\quad \mathbb{P}_{\theta} (rep. \mathbb{E}_{\theta})\quad $ the conditional probability for the parameters given these information below : $(N_{uj}|(z_{u}=k,(\alpha_{kj})_{kj}) \quad \sim \quad Poisson(exp(\alpha_{kj}))$ $(N_{uj}|(z_{u}=k,(\alpha_{kj})_{kj}))_{j\in [J]} \quad $ are independant and K vectors (in dimension J) and $\quad (\alpha_{kj}:j\in [J])_k\in[K]\quad$ are independant and for all $k \in[K] \quad (\alpha_{kj})_{j \in [J]}\quad \sim \quad\mathcal{N}(0,\Sigma^{(k)})$ $\mathbb{P}[z_{u}=k]=q_{k}\quad$ $N_{uj} > 0 \quad \forall u \in [P] \quad and \quad \forall j \in [J]$ we are willing to find the following function $R_{\theta}((c_{j})_{j=1}^{J})=\frac{1}{P} \sum_{u_{0} \in[P]} \mathbb{P}_{\theta}(\sum_{j \in [J]} N_{u_{0}j}>0 | \sum_{u \in [P]}N_{uj}=c_{j}\quad \forall j \in [J])$ I tried to calculate it but the result does not depend on $\theta$ , can somoene tell me how can I calculate this with given informations","['statistics', 'conditional-probability', 'probability-distributions', 'education', 'probability']"
4477045,Details of the self-map on $BO_\infty$ which exchanges tangential and stable-normal structures?,"According to Remark 2.14 in these notes ,
there is a self-map $s : BO_\infty \rightarrow BO_\infty$ which ""exchanges the stable normal and tangential structures"". As I understand it, this means for $X$ a smooth manifold, with tangent bundle classifying
map $\tau : X \rightarrow BO_{\mathrm{dim}X} \hookrightarrow BO_\infty$ ,
and with stable normal bundle classifying map $\nu : X \rightarrow BO_\infty$ ,
then $\tau = s \circ \nu$ and $\nu = s\circ \tau$ (only up to homotopy?). (Here the stable normal bundle is the normal bundle to $X$ w.r.t. a smooth embedding of $X$ in some $S^m$ , for $m$ large enough that all embeddings of $X$ in $S^m$ are isotopic, modulo stable equivalence; the homotopy class of the resulting $X \rightarrow BO_\infty$ should be independent of the choice of $m$ and the embedding. As described in the notes linked above,
such an $m$ depending only on $\mathrm{dim}X$ can be chosen following a theorem of Whitney.) Would anyone be able to suggest a hint on how to find such a map $s$ ?
Or, could anyone suggest a source that explains the details of this construction?","['classifying-spaces', 'principal-bundles', 'vector-bundles', 'algebraic-topology', 'differential-geometry']"
4477098,how do you calculate a derivative with respect to another derivative,I just learned some basics principles in Lagrangian mechanics and theres this equation $$\frac{d}{dt}\left(\frac{dL}{d(\frac{dx}{dt})}\right)=\frac{dl}{dx}$$ where $l$ is kinetic energy-potential energy and therefore is a function of $x$ and $t$ . What I don't get is how you solve and simplify the $\frac{d}{d(\frac{dx}{dt})}$ part because  it has a derivative at the bottom instead of a variable. When doing some expamples for the lagrangian equation I came across some examples that said that $\frac{d{x}}{d(\frac{dx}{dt})}=0$ . Shouldn't that equal to $0$ only if $x$ is independant of $\frac{dx}{dt}$ ? I feel like I'm missing something significant here... so any help would be much appreciated. Thanks in advance,"['partial-derivative', 'derivatives', 'euler-lagrange-equation', 'partial-differential-equations']"
4477108,How can we conclude about the probability of two events?,"Suppose there are two events $E_1$ and $E_2$ such that if both $E_1$ and $E_2$ holds then another event $E_3$ that will occur. How do I derive that $P(E_{1} \cap E_{2}) \leq P(E_{3})$ ? If $E_{1} \implies E_{2}$ is also true, how can we derive $P(E_{1}) \leq P(E_{2})$ ? Could anybody explain it with proper proof and example?","['elementary-set-theory', 'probability']"
4477126,$\mathbb{S}^n$ is a topological $n$-manifold.,"In Lee's ""Introduction to Smooth Manifolds"" he provides the following example showing that $\mathbb{S}^n$ is a topological $n$ -manifold. For each integer $n \geq 0$ , the unit $n$ -sphere $\mathbb{S}^n$ is Hausdorff and second-countable because it is a topological subspace of $\mathbb{R}^{n+1}$ . To show that it is locally Euclidean, for each index $i = 1, \ldots, n+1$ let $U_i^+$ denote the subset of $\mathbb{R}^{n+1}$ where the $i$ th coordinate is positive: $$U_i^+ = \big\{(x^1, \ldots, x^{n+1}) \in \mathbb{R}^{n+1}: x^i > 0\big\}.$$ Similarly, $U_i^-$ is the set where $x^i < 0$ . Let $f: \mathbb{B}^n \rightarrow \mathbb{R}$ be the continuous function $$f(u) = \sqrt{1-|u|^2}.$$ Then for each $i = 1, \ldots, n+1$ , it is easy to check that $U_i^+ \cap \mathbb{S}^n$ is the graph of the function $$x^i = f(x^1, \ldots, \hat{x}^i, \ldots, x^{n+1}),$$ where the hat indicates that $x^i$ is omitted. Similarly, $U_i^- \cap \mathbb{S}^n$ is the graph of the function $$x^i = -f(x^1, \ldots, \hat{x}^i, \ldots, x^{n+1}).$$ Thus, each subset $U_i^\pm \cap \mathbb{S}^n$ is locally Euclidean of dimension $n$ , and the maps $\varphi_i^\pm: U_i^\pm \cap \mathbb{S}^n \rightarrow \mathbb{B}^n$ given by $$\varphi_i^\pm(x^1, \ldots, x^{n+1}) = (x^1, \ldots, \hat{x}^i, \ldots, x^{n+1})$$ are graph coordinates for $\mathbb{S}^n$ . Since each point of $\mathbb{S}^n$ is in the domain of at least one of these $2n+2$ charts, $\mathbb{S}^n$ is a topolgoical $n$ -manifold. My question is perhaps more on notation, but how is $U_i^+ \cap \mathbb{S}^n$ (likewise for $U_i^- \cap \mathbb{S}^n$ ) a graph using the given definition? As I understand it, we are omitting $x^i$ when passing the point in $\mathbb{R}^{n+1}$ into $f$ , is the result of the function the new value for $x^i$ ? However, this seems incorrect as $\varphi$ doesn't seem to involve $f$ at all, so how does it follow that the maps are indeed graph coordinates (or more generally, coordinate maps)?","['manifolds', 'smooth-manifolds', 'differential-geometry']"
4477179,Prove that a bounded nondecreasing function is differentiable Lebesgue-almost everywhere,"Let $g:\mathbb R\to\mathbb R$ be bounded, nondecreasing and right-continuous. We know that there is a unique finite measure $\mu_g$ on $\mathcal B(\mathbb R)$ with $$\mu_g((a,b])=g(b)-g(a)\;\;\;\text{for all }a\le b\tag1.$$ Moreover, by Lebesgue`s decomposition theroem, $$\mu_g=\mu_g^a+\mu_g^s\tag2$$ for some finite measures $\mu_g^a\ll\lambda$ and $\mu_g^s\perp\lambda$ , wher $\lambda$ denotes the Lebesgue measure on $\mathcal B(\mathbb R)$ . Furthermore $^1$ , $\mathbb R\setminus\mathcal D(\mu_g^a)$ is a $\lambda$ -null set and $$\frac{{\rm d}\mu_g^a}{{\rm d}\lambda}={\rm D}\mu_gÃ¢\tag5$$ and $\mathbb R\setminus\mathcal D(\mu_g^s)$ is a $\lambda$ -null set and $$\mathcal D(\mu_g^s)=0\;\;\;\lambda^{\otimes d}\text{-almost everywhere}\tag6.$$ How do we conclude that $g$ is differentiable $\lambda$ -almost everywhere? From the definition we see that $${\rm D}\mu(x):=\lim_{r\to0+}\frac{\mu_g^a((x-r,x+r))}{\lambda((x-r,x+r))}\;\;\;\text{for all }x\in\mathcal D(\mu_gÃ¢)\tag7.$$ But this looks more like this could be more useful to show symmetric differentiability , which is not enough to show differentiability. $^1$ If $d\in\mathbb N$ and $\mu$ is a locally finite measure on $\mathcal B(\mathbb R)^{\otimes d}$ , let $$\mathcal D(\mu):=\left\{x\in\mathbb R^d:\limsup_{r\to0+}\frac{\mu(B_r(x))}{\lambda^{\otimes d}(B_r(x))}<\infty\right\}$$ and $${\rm D}\mu(x):=\lim_{r\to0+}\frac{\mu(B_r(x))}{\lambda^{\otimes d}(B_r(x))}\;\;\;\text{for }x\in\mathcal D(\mu).$$ We can show that if $\mu\ll\lambda^{\otimes d}$ , then $\mathbb R^d\setminus\mathcal D(\mu)$ is a $\lambda^{\otimes d}$ -null set and $$\frac{{\rm d}\mu}{{\rm d}\lambda^{\otimes d}}={\rm D}\mu\tag3$$ (where ${\rm D}\mu$ is arbitrary extended to $\mathbb R^d$ ). if $\mu\perp\lambda^{\otimes d}$ , then $\mathbb R^d\setminus\mathcal D(\mu)$ is a $\lambda^{\otimes d}$ -null set and $$\mathcal D(\mu)=0\;\;\;\lambda^{\otimes d}\text{-almost everywhere}\tag4.$$","['measure-theory', 'lebesgue-measure', 'radon-nikodym', 'real-analysis']"
4477196,Relation between second derivatives,"In attempting to find a relation between second derivatives of $\frac{dx}{dy}$ and $\frac{dy}{dx}$ , I used the relation: $\frac{dx}{dy} \times \frac{dy}{dx}$ = $1$ Then I differentiated both sides with respect to $x$ giving, using product rule and chain rule: $\left(\frac{dy}{dx} \times \left(\frac{d^2x}{dy^2}\times\frac{dy}{dx}\right)\right)$ + $\left(\frac{dx}{dy} \times \frac{d^2y}{dx^2}\right)$ = $0$ Thus $(\frac{dy}{dx})^2 \times \frac{d^2x}{dy^2} =  $ - $\left(\frac{dx}{dy} \times \frac{d^2y}{dx^2}\right)$ Which gives: - $\left(\frac{d^2y}{dx^2}\right)$ = $(\frac{dy}{dx})^3 \times \frac{d^2x}{dy^2}$ But apparently the answer is: - $\left(\frac{d^2y}{dx^2}\right)$ = $(\frac{dx}{dy})^3 \times \frac{d^2x}{dy^2}$ Am I wrong in applying the chain rule there? Or is there another mistake?","['calculus', 'derivatives']"
4477214,"An example of a matrix in $\mathrm{SL}(4, \mathbb{Z})$ with the following properties","I am looking for a matrix $M\in \mathrm{SL}(4, \mathbb{Z})$ , with all eigenvalues equal to $1$ , and with the following properties: Write $M=\begin{bmatrix}
A_1&A_2\\
A_3&A_4 
\end{bmatrix}, 
$ where the $A_i$ are $2$ by $2$ sumbatrix of $M$ . Let $d_i$ be the dot product of two rows of $A_i$ , i.e. if $A_i = \begin{bmatrix}
a&b\\
c&d 
\end{bmatrix}$ , then $d_i = ac +bd$ . Let $a_i = \mathrm{det}(A_i) - d_i$ . For example if $A_1 =\begin{bmatrix}
1&2\\
3&4
\end{bmatrix}$ ,  then $d_1 = 11$ and $a_1 = - 2 - 11 = -13$ . Consider the matrix $A =  \begin{bmatrix}
a_1&a_2\\
a_3&a_4 
\end{bmatrix}$ ,  I would like to find $M$ such that $A$ is in $\mathrm{GL}(2, \mathbb{Z})$ , and has one eigenvalue with absolute value not equal to $1$ . The matrices I have tried so far : since $M$ needs to have eigenvalues all equal to $1$ , I tried the matrices consisting of just Jordan blocks, unfortunately, they didn't work. I also tried the matrix in the form $M=\begin{bmatrix}
A_1&A_2\\
0&A_4 
\end{bmatrix}, 
$ where $A_1$ and $A_4$ have all eigenvalues all equal to $1$ , but in this case $A$ will have all eigenvalues with absolute values $1$ . I am a bit stuck as I can't think of other $4$ by $4$ matrices that have all eigenvalues equal to $1$ . Any idea to construct such matrices will be really appreciated.","['eigenvalues-eigenvectors', 'matrices', 'abstract-algebra', 'linear-algebra', 'group-theory']"
4477220,Intersection of decreasing sequence of sets,"In an exam, my teacher wrote this problem , I proved it in a similar way to the answer in that post , but he pointed out to me that a step in the proof is wrong. What my teacher says Let $A_1 \supset A_2 \supset A_3 \supset \dots$ sequence of decreasing sets We can construct the following sequence of disjoint sets $B_1 = A_1 - A_2$ $B_2 = A_2 - A_3$ $B_3 = A_3 - A_4$ $\dots$ let $A = \cap_{k \in \mathbb{N}} A_k$ , then $A$ and $\cup_{k = N}^{\infty} B_k$ do not form a partition of $A_N$ , are not disjoint, in fact $A$ is contained in the union $\cup_{k = N}^{\infty} B_k$ . But I think he is wrong, and I have this proof to prove that step. My proof $A_1 - \cap_{k \in \mathbb{N}} A_k  = A_1 \cap (\cap_{k \in \mathbb{N}} A_k)^c = A_1 \cap (\cup_{k \in \mathbb{N}} A_k^c)$ The intersection is distributed over the union, then $A_1 \cap (\cup_{k \in \mathbb{N}} A_k^c) = \cup_{k \in \mathbb{N}} (A_1 \cap A_k^c) = $ $= \emptyset \cup (A_1 - A_2) \cup (A_1 - A_3) \cup (A_1 - A_4) \cup ... = $ since the sequence is decreasing, finally $= \emptyset \cup (A_1 - A_2) \cup (A_1 - A_3) \cup (A_1 - A_4) \cup ... = $ $= \emptyset \cup (A_1 - A_2) \cup (A_2 - A_3) \cup (A_3 - A_4) \cup ... =$ $= \cup_{k \in \mathbb{N}} B_k$ This is $A_1 - \cap_{k \in \mathbb{N}} A_k  = \cup_{k \in \mathbb{N}} B_k$ . The case $A_N - \cap_{k \in \mathbb{N}} A_k  = \cup_{k = N}^{\infty} B_k$ is analogous. Is my proof correct?",['elementary-set-theory']
4477235,How to calculate the statistical uncertainty in a particle physics simulation?,"I have a Monte Carlo code which simulates ions in a Tokamak . Most of the particles remain trapped forever. However, some of the particles escape. I can use my code to predict the fraction of particles that escape versus those that stay trapped. The code models collisions using a stochastic collision operator. Therefore, there is a statistical uncertainty with my results. Do you know any techniques to calculate this statistical uncertainty? One idea I had is to use the Bootstrapping technique . I'll quickly explain my plan now: Suppose we have $N$ trapped particles and $n<N$ that escape. We denote the particles that are trapped with $t_i$ (for $i\in\{1,2,...,N\}$ ) and the ones that escape with $e_i$ (for $i\in\{1,2,...,n\}$ ). Hence, the estimate for the fraction of markers that escape is $n/(n+N)$ . To calculate the uncertainty, we make a list which looks like the following: $$\{t_1,t_2,...,t_N,e_1,e_2,...,e_n\}.$$ We resample this list (with replacement) $m$ times (e.g. $m=100$ ) to produce lists which may look something like this: $$\underbrace{\{t_{36}, e_{12}, t_{444}, t_{321},...\}}_{n+N\ \mathrm{elements}}.$$ Then calculate the fraction of markers which escape for each of the $m$ cases and take away $n/(n+N)$ to produce a list of values with length $m$ . Finally, take, e.g. a 95% quantile of this to get a 95% confidence interval. Can you see any problems with this method? Is there a more straightforward method I could be using? I know barely any statistics, so any advice is appreciated.","['statistics', 'monte-carlo', 'computational-mathematics', 'physics', 'mathematical-physics']"
4477287,"If $\sum_{k=0}^{n-1}\frac{x^k}{k!}=\sum_{k=n}^{\infty}\frac{x^k}{k!}$ for large $n$, approximate $x$ in terms of $n$.","Split the Maclaurin series for $e^x$ into two sub-series - the first $n$ terms, and the remainder - then equate the two sub-series. What is a good approximation for $x$ , for large $n$ ? Since $e^x=\sum_{k=0}^{\infty}\frac{x^k}{k!}$ , we have $\sum_{k=0}^{n-1}\frac{x^k}{k!}=\frac{1}{2}e^x$ . Experimenting with desmos, it seems that $x\approx n-\frac{1}{3}$ for even or odd $n$ , and $x\approx -0.28n-0.4$ for even $n$ . Other than that, I do not know how to approach this question. (Context: I was trying to answer this question , but I think the question in this post is interesting by itself.)","['approximation', 'real-analysis', 'taylor-expansion', 'limits', 'exponential-function']"
4477305,Verification of an inequality involving $|x|^\alpha$,"Let $u:\mathbb{R}^n\to \mathbb{R}$ , $\alpha>0$ and $w(x)=|x|^\alpha.$ Then this paper I am reading has the following claim, $$\nabla^2\log w(\nabla u,\nabla u)+\frac{(\nabla \log w\cdot \nabla u)^2}{\alpha}\leq 0.$$ I am trying to check if this inequality indeed holds. I first computed $$\frac{(\nabla \log w\cdot \nabla u)^2}{\alpha}=\frac{\alpha}{|x|^4}(x\cdot \nabla u)^2$$ since $$\nabla\log w = \frac{\alpha}{|x|^2} x.$$ I am guessing that (the authors did not define this in the paper) $$\nabla^2\log w(\nabla u,\nabla u)=\sum_{i,j}D_{ij}\log wD_iuD_ju$$ where $D_i u$ is the partial derivative wrt to $x_i$ of $u.$ This gives me $$\nabla^2\log w(\nabla u,\nabla u)=\frac{\alpha |\nabla u|^2}{|x|^2}-\frac{2\alpha (x\cdot \nabla u)^2}{|x|^4}$$ since $$D_{ij}\log w = \frac{\alpha}{|x|^2}\delta_{ij}-\frac{2\alpha}{|x|^4}x_i x_j.$$ Therefore, $$\nabla^2\log w(\nabla u,\nabla u)+\frac{(\nabla \log w\cdot \nabla u)^2}{\alpha}=\frac{\alpha |\nabla u|^2}{|x|^2}-\frac{\alpha (x\cdot \nabla u)^2}{|x|^4}\\
=\frac{\alpha |\nabla u|^2}{|x|^2}\left(1-\cos^2(\theta_x)\right)\geq 0,$$ where $\theta_x$ is the angle between $x$ and the gradient of $u$ , however, this contradicts the claim in the paper. Where have I made a mistake?","['multivariable-calculus', 'partial-differential-equations', 'inequality', 'real-analysis']"
4477342,How to evaluate double integrals of a surface over a specific region?,"I found this exercise while exercising for the exam: Let $T$ $\subset$ $R^2$ be the triangle with these vertices $(0,0), (2,0), (0,1)$ and let $\Omega$ be the surface defined like this: $\Omega$ = { $(x,y,z) \in R^3 : z^2 - x^2 - y^2 = 0, z > 0, (x,y) \in T$ } Evaluate $\iint_{\Omega}x^2 y dS $ I'm having a hard time solving it because it confounds me... I can't seem to ""visualize"" the situation. What's exactly the role of the triangular region, where am I going to have to use that region when solving the integral? Could you help me visualize the problem in some way?","['integration', 'multivariable-calculus', 'surface-integrals']"
4477387,Does the compact-open topology make Top a (symmetric) closed category?,"Does the category Top of topological spaces, equipped with the compact-open topology on hom-sets,
form a closed category ? (Perhaps even a symmetric closed category? This notion is mentioned in the above linked nLab page and defined in this article .) Note: it is known that Top cannot form a Cartesian closed category, even when restricted to the exponentiable spaces;
cf. this nLab page . Edit: In trying to verify the axioms ""directly"",
I am getting stuck trying to show the function $$
L^X_{Y,Z} : [Y,Z] \xrightarrow{f \mapsto f_*} [[X,Y],[X,Z]]
$$ is continuous (or show it need not be). Would anyone have any suggestions on how to approach this step?","['general-topology', 'category-theory', 'algebraic-topology']"
4477389,Evaluate $\int_{-\infty}^\infty \frac{1}{(x^2 + D^2)^2}dx$.,"$$\int_{-\infty}^\infty \frac{1}{(x^2 + D^2)^2}dx$$ Edit : $D> 0$ . My work: Let $x = D\tan \theta$ $$\int_{-\infty}^\infty \frac{1}{(x^2 + D^2)^2}dx=\int_{-\infty}^\infty \frac{1}{(D^2\sec^2 \theta)^2}dx$$ $$=\int_{-\infty}^\infty \frac{1}{(D^2\sec^2 \theta)^2}D\sec^2\theta d\theta
= \frac{1}{D^3}\int_{-\infty}^\infty \cos^2 \theta d\theta$$ $$=\frac{1}{D^3}[\frac{\theta}{2} + \frac{\sin {2\theta}}{4} + C]_{-\infty}^\infty$$ Put $\theta = \arctan{\frac{x}{D}}$ ; $$=\frac{1}{D^3}[\frac{\arctan{\frac{x}{D}}}{2} + \frac{\sin {(2\arctan{\frac{x}{D})}}}{4} + C]_{-\infty}^\infty$$ We get the integral as $\frac{\pi}{2D^3}$ . Since $\lim_{x \to +\infty} \arctan(x) = \frac{\pi}{2}$ and $\lim_{x \to -\infty} \arctan(x) = \frac{-\pi}{2}$ I feel something isn't right here. Can anyone point the mistake please. Thank you very much.","['integration', 'physics', 'calculus', 'definite-integrals']"
4477394,Strength of Krein-Milman vs Dependent Choice,"I am wondering about the relationship between the Krein-Milman theorem (KM) and some other weak forms of the axiom of choice (AC). I currently basically know the following: KM + BPI (Boolean prime ideal theorem) implies AC. (Bell, Fremlin, A geometric form of the axiom of choice ) I also know the following: DC (dependent choice) + BPI does not imply AC (consequently DC does not imply KM). (Pincus, Adding Dependent Choice to the Prime Ideal Theorem ) I have two closely related questions about these axioms, but have been unable to find an answer. These are Does KM imply DC? Is KM consistent with AD (axiom of determinacy)? If we replace ''KM'' with ''DC'' in the above questions, the answer is always ""yes"" (for question 1, this is of course trivial). So my underlying motivation is to figure out how similar KM and DC are.","['axiom-of-choice', 'functional-analysis', 'set-theory']"
4477418,Determine if the following autonomous differential equation has periodic solutions,"Let $\begin{cases}
x'=-y\\
y'=x+y^{2}
\end{cases}$ be an autonomous system. Determine whether or not is has periodic solutions. Hint: Consider the differential equations of the orbit: $\frac{dy}{dx}=-\frac{x}{y}-y$ I just started learning about about autonomous systems, and I have no idea how to approach this. I assume I need to use Poincare-Bendixon but I don't see how to use the hint (or even understand it). Bendixon's criteria doesn't help either. Any help would be much appreciated.","['dynamical-systems', 'analysis', 'ordinary-differential-equations', 'real-analysis']"
4477475,Example of non-continuous composition for the compact-open topology in non-LCH case?,"For topological spaces $X,Y$ let $[X,Y]$ denote the space of continuous maps $X\rightarrow Y$ , equipped with the compact-open topology. According to the Wikipedia page for the compact-open topology,
when $Y$ is locally-compact Hausdorff,
then the composition map $$
[Y,Z] \times [X,Y] \xrightarrow{ (g,f) \mapsto g \circ f } [X,Z]
$$ is continuous. (Here the LHS has the product topology.) I was wondering, does anyone have a counter-example showing the above map need not be continuous when $Y$ is not locally-compact Hausdorff?","['general-topology', 'algebraic-topology', 'compactness']"
4477643,Filling a box with cubes sized as powers of 2 with as few boxes as possible,"Say I have a box with integer dimensions and I need to fill it with cubes with side lengths of power of 2, how can I fully fill the box with as few cubes as possible? So say i have a $5 \times 5 \times 9$ box, how do I fill the space with as few cubes as possible (options would in this case include cubes with side length: 1,2,4). I'm looking for a general explanation, not just the answer to this example","['discrete-optimization', 'geometry', 'tiling']"
4477665,Find the limit $\lim _{x\to 0}\frac{\cos x-1+\frac{x}{2}\cdot \sin x}{\ln ^4(x+1)}$,"I need to find $\displaystyle \lim _{x\to 0}\frac{\cos x-1+\frac{x}{2}\cdot \sin x}{\ln ^4\left(x+1\right)}$ .
I tried using the following: \begin{align*}
\ln(1+x)&\approx x,\\
\sin(x)&\approx x-\frac{x^3}{2},\\
\cos(x)&\approx 1-\frac{x^2}{2!}+\frac{x^4}{4!},
\end{align*} I managed to get $\displaystyle \lim _{x\to 0}\frac{\cos x-1+\dfrac{x}{2}\cdot \sin x}{\ln ^4\left(x+1\right)}=\lim _{x\to 0}\frac{\left(\dfrac{\left(-\frac{x^{2}}{2!}+\frac{x^{4}}{4!}\right)}{x^{4}}+\dfrac{x^{2}-\frac{x^{4}}{3!}}{2x^{4}}\right)}{\left(\dfrac{\ln\left(1+x\right)-x}{x}+1\right)^{4}}$ but I can't see how to use  that $\displaystyle \lim _{x\to 0}\frac{\ln\left(1+x\right)-x}{x}=0$ in here. Am I missing something? I didn't learn the little- $\mathcal{O}$ notation yet. Thanks","['limits', 'calculus']"
4477666,Does this property of algebra morphisms (related to idempotents) have a name?,"Let $F$ be a field. I am in the category of finite-dimensional $F$ -algebras. Let $f:A \rightarrow B$ a homomorphism of two of those. The property of $f$ which came up as useful in something I consider is: $$(*) \text{ For every idempotent } e \in A, \dfrac{\dim_F(A\cdot e)}{\dim_F(A)} = \dfrac{\dim_F(B \cdot f(e))}{\dim_F(B)}.$$ (For example, the diagonal embedding $diag: F \rightarrow F \times F$ has this property, but funnily the map $diag \times id: F \times F \rightarrow F \times F \times F$ does not.) I could call an $f$ satisfying $(*)$ ""consistent"" or something, but there's far too many of those words already. So I was hoping somebody who knows more about algebras and algebraic geometry can tell me, ""ah, that's just an unusual way to say that $f$ is (locally etale / proper / flat and finitely presented / some other cool words)"". Question : Is property $(*)$ equivalent to some well-known property or combination of properties of algebra morphisms? At least maybe in the case that one or both of $A, B$ are commutative (some algebro-geometric term)? Or is it at least, say, stronger than (well-known condition C1) but weaker than (well-known condition C1)? If it helps a lot, I am also fine with assuming $f$ to be injective.","['idempotents', 'algebraic-geometry', 'ring-theory', 'abstract-algebra', 'algebras']"
4477667,Finding the volume using Washers,"Problem: Find the volume generated when the region bounded by the given curves and line is revolved about
the x-axis. $$ y = 3x - x^2$$ $$ y = 3x $$ Answer: Let $V$ be the volume we are trying to find. The first step is to find the points where $3x - x^2$ and $y = 3x$ intersect. \begin{align*}
3x - x^2 &= x \\
-x^2 &= 2x \\
x = 0 &\text{ or } x = 2 \\
V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\
\dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\
\end{align*} Now we have two integrals to evaluate. \begin{align*}
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^2 + 9 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^3}{3} + 9x \Big|_0^2 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(8)}{3} + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 16 + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{42}{5} \\
\end{align*} For the second integral we have: \begin{align*}
\int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\
\dfrac{V}{\pi} &= \dfrac{42}{5} - \dfrac{8}{3} = \dfrac{ 3(42) - 5(8)}{15} \\
\dfrac{V}{\pi} &= \dfrac{86 }{15 } \\
V &= \dfrac{86\pi}{15}
\end{align*} However, the book gets: $ \dfrac{ 56 \pi}{15} $ . Where did I go wrong? Based upon a comment from John Douma, I realized that I copied the question incorrectly. Here is the revised question with my solution which still has the wrong answer. Problem: Find the volume generated when the region bounded by the given curses and line is revolved about
the x-axis. $$ y = 3x - x^2 $$ $$ y = x $$ Answer: Let $V$ be the volume we are trying to find. The first step is to find the points where $3x - x^2$ and $y = 3x$ intersect. \begin{align*}
3x - x^2 &= x \\
-x^2 &= -2x \\
x = 0 &\text{ or } x = 2 \\
V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\
\dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\
\end{align*} Now we have two integrals to evaluate. \begin{align*}
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^2 + 9 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^3}{3} + 9x \Big|_0^2 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(8)}{3} + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 16 + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{42}{5} \\
\end{align*} For the second integral we have: \begin{align*}
\int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\
\dfrac{V}{\pi} &= \dfrac{42}{5} - \dfrac{8}{3} = \dfrac{ 3(42) - 5(8)}{15} \\
\dfrac{V}{\pi} &= \dfrac{86 }{15 } \\
V &= \dfrac{86\pi}{15}
\end{align*} However, the book gets: $ \dfrac{ 56 \pi}{15} $ . Where did I go wrong? Here is an updated answer based upon the comments from DougM. Answer: Let $V$ be the volume we are trying to find. The first step is to find the points where $3x - x^2$ and $y = x$ intersect. \begin{align*}
3x - x^2 &= x \\
-x^2 &= -2x \\
x = 0 &\text{ or } x = 2 \\
V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\
\dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\
\end{align*} Now we have two integrals to evaluate. \begin{align*}
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^3 + 9 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^4}{4} + 9x \Big|_0^2 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(16)}{4} + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 24 + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{2}{5} \\
\end{align*} For the second integral we have: \begin{align*}
\int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\
\dfrac{V}{\pi} &= \dfrac{2}{5} - \dfrac{8}{3} = \dfrac{ 6 - 24}{15} \\
\dfrac{V}{\pi} &= -\dfrac{18 }{15 } \\
V &= -\dfrac{18\pi}{15}
\end{align*} This answer is obviously wrong. The book gets: $ \dfrac{ 56 \pi}{15} $ . Where did I go wrong? Here is an updated answer based upon the comment from N. F. Taussig. I now have a correct solution. Answer: Let $V$ be the volume we are trying to find. The first step is to find the points where $3x - x^2$ and $y = x$ intersect. \begin{align*}
3x - x^2 &= x \\
-x^2 &= 2x \\
x = 0 &\text{ or } x = 2 \\
V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\
\dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\
\end{align*} Now we have two integrals to evaluate. \begin{align*}
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^3 + 9x^2 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^4}{4} + \dfrac{9x^3}{3} \Big|_0^2 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(16)}{4} + \dfrac{9(8)}{3} \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 24 + 24 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} \\
\end{align*} For the second integral we have: \begin{align*}
\int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\
\dfrac{V}{\pi} &= \dfrac{32}{5} - \dfrac{8}{3} = \dfrac{ 96 - 40}{15} \\
\dfrac{V}{\pi} &= \dfrac{56 }{15 } \\
V &= \dfrac{56\pi}{15}
\end{align*} This answer matches that given in the book.","['integration', 'calculus', 'solid-of-revolution']"
4477720,Is there an accurate way to estimate the product $\prod_{n=1}^{1009}\frac{2n+1}{2n}$?,"I don't have a background in mathematics, I just was doing this for fun...so if there are any flaws in any logic below please feel free to correct the logic. Anyways, there was a question posed in this video ( https://www.youtube.com/watch?v=jZPFIQ8kIUs ) asking if $(1+\frac{1}{2})(1+\frac{1}{4})(1+\frac{1}{6})...(1+\frac{1}{1009}) > 50$ or not. The algebra behind why the product is less than $50$ is clear, but I was curious if there was a way to estimate or accurately determine what this number actually is or might be. My attempt to find that out goes as follows: $$(1+\frac{1}{2})(1+\frac{1}{4})(1+\frac{1}{6})...(1+\frac{1}{2018}) = \frac{3}{2}\cdot\frac{5}{4}\cdot\frac{7}{6}\cdots\frac{2019}{2018} = \prod_{n=1}^{1009}\frac{2n+1}{2n}.$$ Doing a little bit of research I read that you can write a product $\prod a_i$ as: $$e^{\sum\ln(a_i)},$$ which is essentially a Riemann sum (I believe) and thus allows one to write: $$e^{\int \ln(a_i)}.$$ If the reason why $e^{\sum \ln(a_i)} = e^{\int \ln(a_i)}$ isn't because the summation isn't a Riemann sum, I would be curious to read a brief explanation in a comment below as to the reasoning for why they're equal. Back to the calculation... \begin{align}
\prod_{n=1}^{1009}\frac{2n+1}{2n} = \prod_{n=1}^{1009}{(2n+1)}\cdot\prod_{n=1}^{1009}{\frac{1}{2n}} = e^{\int_1^n{\ln(2n+1)}}\cdot e^{\int_1^n{\ln(\frac{1}{2n})}} \quad (1)
\end{align} After a bit more research I figured out I could use Stirling approximation to determine the values of $\ln\big(\frac{1}{2n}\big)$ and $\ln(2n+1)$ . From the Stirling approximation Wikipedia page I learned that $\displaystyle \sum_{j=1}^n{\ln(j)} \approx \int_1^n {\ln(x) dx} = n\cdot \ln(n) - n + 1$ , so: $$\sum_{j=1}^n{\ln(\frac{1}{2n})} = -2\cdot \sum_{j=1}^n{\ln(n)} = -2\cdot \int_1^n {\ln(x) dx} = -2\cdot (n\cdot \ln(n) - n + 1)\\and\\\sum_{j=1}^n{\ln({2n+1})} = \sum_{j=2}^{n+1}{\ln(2n)} = 2\cdot \sum_{j=2}^{n+1}{\ln(n)} = 2\cdot \int_2^{n+1} {\ln(x) dx} = 2\cdot (n\cdot \ln(n) - n + 1)$$ So from $(1)$ we now have, $$e^{2\cdot \int_2^{n+1} {\ln(x) dx}}\cdot e^{-2\cdot \int_1^n {\ln(x) dx}} = \frac{e^{2\cdot \int_2^{n+1} {\ln(x) dx}}}{e^{2\cdot \int_1^n {\ln(x) dx}}}$$ and with $n = 1009$ , we have $$\frac{e^{\int_2^{1010} {\ln(x) dx}}}{e^{\int_1^{1009} {\ln(x) dx}}} \approx \frac{e^{5977.88266...}}{e^{5970.96545...}} = e^{6.91721} = 1009.49955 > 50$$ But we know that the product we're trying to estimate is less than $50$ . In order to get a value for $x$ such that $e^{x}<50$ , then $x < 3.912$ . From the Stirling approximation, we found $x$ to be $6.91721$ . However, upon further reading about Stirling approximation of $\ln(x)$ can be off by $0.008$ . For the given value of $5977.88266...$ this means the real value can fall within $5977.88266... \pm 107$ , which means the difference between $5977.88266...$ and $5970.96545...$ are within margin of error. Despite them being so close to each other, to get the proper estimate, their difference needs to be less than $3.912$ . However I don't know how to move forward in being able to find an estimation, unless the logic behind my estimation is flawed...in which case the solution would be found as per finding out the error. But if my logic was sound and this discrepancy between $e^{6.91721}$ and $e^{x<3.912}$ is just the result of approximation errors in the Stirling approximation, then my question becomes: Is there a way to better accurately determine or approximate this answer?","['calculus', 'approximation', 'analysis']"
4477783,Reference for topological invariants written in integrals,"When studying physics, I came up with various integrals that only takes integer values due to topological reasons. Winding number $S^1 \to S^1$ is the most elementary example, which moreover provides an isomorphism $\pi_1(S^1) \to \mathbb Z$ . More generally, homotopy class of a map from $S^d$ or $T^d$ to a Lie group $G$ is often represented by an integral, as I saw in various physics literature (one example is Homotopy and quantization in condensed matter physics , written by mathematical physicists). I want a systematic theory on this matter. Already in this site, there are at least two questions related to my question, which focuses on a specific choice of $X$ and $Y$ when classifying maps $X\to Y$ : Why is the winding number of a matrix an integer? Cartan 3-form on a Lie group G What I want to address is more general, since I feel that there is a more general theory of obtaining homotopy invariant as an integral. Let $X = S^d$ or $T^d$ , and let $G$ be a ""nice"" Lie group (say, compact). What is the homotopy class $[X, G]$ ? Can we associate an invariant written in terms of an integral over $X$ that takes only integer values? If such invariant exists, when does this invariant completely characterizes the homotopy class? In particular, in the link Why is the winding number of a matrix an integer? , a map $f:S^1âU(N)$ is classified by the degree of the map $\det\circ f:S^1\to S^1$ . Then, does this invariant completely characterizes $[S^1,U(N)]$ ? Any answers or references on this question is welcomed. I have a basic knowledge of algebraic topology and differential geometry. Note added : After several comments, I can make my question more concrete. According to a comment: If $f:XâG$ is smooth and $\omega$ is a $k$ -form representing an
integer cohomology class on $G$ , then $f^â\omega$ will be a $k$ -form
on $X$ representing an integer cohomology class. In particular,
integrating it against an integer homology class will give an integer
invariant of $f$ . My question following the comment is: How to find a $k$ -form representing an integer cohomology class? What is the ""integer homology class""? Is it correct to say that $\int_N f^*\omega \in \mathbb Z$ whenever $N$ is a $k$ -dimensional submanifold of $M$ ?","['homotopy-theory', 'algebraic-topology', 'differential-geometry']"
4477813,What is this relationship between trigonometric and hyperbolic function?,"In the following, I don't understand how they put $\tan{\phi} = \sinh{\frac{\psi}{\sqrt{2}}}$ . Is there a relationship?","['multivariable-calculus', 'calculus', 'complex-numbers']"
4477852,Combinatorics: n people moving between three queues.,"We have 3 queues; at first n people are in first queue, numbered from 1 to n. One of the following three things happens every minute : first person from first queue moves behind the last person in second queue . first person from second queue moves behind the last person in third queue . first person from first queue moves behind the last person in third queue . The second queue has a capacity of ""k"" people . A) if n=100 and k=1 what is the number of different ways that the given n people can stand in the third queue ? B) if n=11 and k=4 what is the number of different ways that the given n people can stand in the third queue ? I solved part A and answer is $2^{99}$ . can you help me with part B  ?","['permutations', 'combinatorics']"
4477885,Surreal numbers ordering and false inequality $1\le0$,"Given numeric forms $x =\{ X_L \,|\, X_R\}$ and $y =\{ Y_L \,|\, Y_R\}$ of two surreal numbers we say that $x \le y$ if and only if There is no $x_L \in X_L$ such that $y \le x_L$ and There is no $y_R \in Y_R$ such that $y_R \le x$ . According to this definition it seems to me that the comparison $$
\{ \,| \,2 \, \} \le \{ \, -1 \, | \, \},
$$ which is a short for $\{\varnothing\,| \,\{2\} \} \le \{ \{-1\} \, | \, \varnothing \}$ , should be automatically true, since $X_L = \varnothing$ and $Y_R = \varnothing$ .
However, as far as I understand the surreal numbers, the expressions $ \{ \,| \,2 \, \}$ and $ \{ \, -1 \, | \, \}$ are valid numeric forms that represent numbers $1$ and $0$ respectively. Combining these two notions we arrive at $$
1 \le 0,
$$ which seems to be absurd, since one expect surreal numbers to be compatible with the natural ordering. Obviously, I am overlooking something. I would be very grateful if someone could point out a mistake.","['elementary-set-theory', 'order-theory', 'surreal-numbers']"
4477887,Difference between seemingly identical statements,"Someone asked the following question, but deleted it for some reason. I think it's a nice subtle point to make. In propositional calculus, $$  ((P\Rightarrow Q)\lor (P\Rightarrow R))\iff P\Rightarrow (Q\lor R). $$ However, for set inclusion it is false that $$ A\subseteq B \ \mathrm{or}\ A\subseteq C \iff A\subseteq B\cup C.$$ $â¹$ is true, while counterexamples for $â¸$ are easy to come up with. Why is only $â¹,$ but not $â¸,$ true? My intuition immediately says that this is because the set inclusion statement is quantified as $$ \forall x (x\in A \Rightarrow x\in B) \ \lor \forall x(x\in A \Rightarrow x\in C) \Longrightarrow \forall x(x\in A \Rightarrow x\in B\cup C)$$ but there is no $\Longleftarrow$ . Is there some elegant way to better articulate this point?","['elementary-set-theory', 'propositional-calculus', 'predicate-logic', 'logic']"
4477898,13 boys and 2 girls are to be placed next to each other. What is the probability that there are exactly 4 boys between the 2 girls?,"$13$ boys and $2$ girls are to be placed next to each. What is the probability that there are exactly $4$ boys between the $2$ girls? Arrangements possible: $GBBBBGBBBBBBBBB$ $BGBBBBGBBBBBBBB$ $BBGBBBBGBBBBBBB$ $BBBGBBBBGBBBBBB$ $BBBBGBBBBGBBBBB$ $BBBBBGBBBBGBBBB$ $BBBBBBGBBBBGBBB$ $BBBBBBBBGBBBBGB$ $BBBBBBBBBGBBBBG$ So the number of ways the arrangement can happen is $C(13,9) = 715$ I need help beyond this to frame the solution.","['combinatorics', 'probability']"
4477922,How to integrate $\int_0^{2\pi} \ln \vert \cos \pi e^{i\theta}\vert d \theta$ using complex analysis?,"I'm trying to find the value $\int_0^{2\pi} \ln | \cos \pi e^{i\theta}\vert d \theta$ for the entire function $\cos z$ (Here the $\ln$ is natural logarithm ) I put the $z = e^{i\theta}$ , then we have $dz= iz d\theta$ . Therefore, $$\int_0^{2\pi} \ln \vert \cos \pi e^{i\theta}\vert d \theta = \int_{\vert z \vert =1} \frac{\ln\vert \cos \pi z\vert}{iz} dz$$ Therefore isolated singularities are $z=0, \frac{1}{2}$ and $\frac{-1}{2}$ . say $g(z) = \frac{\ln\vert \cos \pi z\vert}{iz}$ For the $z=0$ case, $\operatorname{res}(g,0) = 0$ [simple pole]. But I'm stuck for solving the other cases $\frac{1}{2}$ and $\frac{-1}{2}$ .
How can I integrate that?","['integration', 'complex-analysis']"
4477973,Expectancy of 10 mutually independent random variables and conditional probability,"I've been given the following question, which has 2 parts: Consider 10 (mutually) independent experiments (each is a random variable) numbered 1 through 10. The probability that the i-th experiment will be over within an hour from the moment it began is $\frac{i}{10}, 1\leq i\leq 10$ 1st part: (Assuming all the experiments are started at the same time,) what is the
expected number of experiments that will be over within an hour? The solution just says 1 (without further explanation) but I disagree. At least one (the 10th experiment) will be over within an hour with probability 1, and the rest also have non-zero probability to end within the same time, so the expectancy has to be at least greater than 1. My way of solving this was to define each experiment as a random variable $x_i$ such that $$
x_i=
\begin{cases}
\displaystyle 1,  & \text{w.p. $\frac{i}{10}$ } \\
0, & \text{w.p. $\frac{10-i}{10}$ }
\end{cases}
$$ each with expectancy $E[x_i]=\frac{i}{10}$ ,
and then define $$Y=\sum_{i=1}^{10} x_i$$ Thus, the expectancy would be $$
E[Y]=E[\sum_{i=1}^{10} x_i]=\sum_{i=1}^{10}E[x_i]=\sum_{i=1}^{10}\frac{i}{10}=5.5
$$ By linearity of expectation. To verify my solution, I tried calculating $P(y)$ for each $y, 0\leq y\leq 10$ and then finding out the expectancy using its definition for discrete random variables. I've got the values for $y=0, 1, 9, 10$ but the rest were too difficult to generalize. The 2nd part is as follows (it's unrelated to the 1st one, just to the main question): What's the probability that the 5th experiment was over (within an hour), given a randomly selected experiment is over? It wasn't quite clear in the question, but I'm assuming that they meant all the experiments had started, and after an hour we picked one randomly and it was over, what's the probability that it's the 5th one? I calculated the probability for a randomly selected experiment to be over by multipling the probability for each experiment to be over with the probability for choosing it (using the law of total probability). Each experiment is equally likely to be choosen, so $$
P(B)=\sum_{i=1}^{10}\frac{i}{10}\frac{1}{10}=0.55
$$ Then, if $A_5$ is the event that the 5th experiment was over, $$
P(A_5 | B)=\frac{P(B|A_5)P(A_5)}{P(B)}=\frac{1\cdot \frac{1}{10}}{0.55}=0.182
$$ But in the solution they defined $A_5$ as the event that the 5th experiment was choosen (so in general $P(A_i)=\frac{1}{10}$ for every $i$ ) and said $$
P(A_5 | B)=\frac{P(B|A_5)P(A_5)}{P(B)}=\frac{\frac{5}{10}\cdot \frac{1}{10}}{0.55}=0.091
$$ But this doesn't make much sense to me. Did I interrupt the question differently than I should have? Edit: Added the second part which wasn't included when I posted the question at first.","['statistics', 'conditional-probability', 'expected-value', 'probability', 'random-variables']"
4477987,"Let $G$ be a group and $A,B\leq G$ abelian subgroups such that $AB=G$. Show $A\cap B\leq Z(G)$","I am working on an exercise that states: Let $G$ be a group and $A,B\leq G$ abelian subgroups such that $AB=G$ . Show $A\cap B\leq Z(G)$ . My attempt so far: $Z(G)=\{z \in G \mid \forall g \in G, zg = gz\}$ Because $A,B\leq G$ are abelian subgroups $A\cap B\leq G$ is an abelian subgroup. Can anyone give me a hint?","['group-theory', 'abstract-algebra', 'abelian-groups']"
4477989,Examples of abstract objects (like sheaf cohomology) with computational perspectives (Äech cohomology),"The derived functor perspective on sheaf cohomology is useful for developing theory, but when we calculate it, we use Äech cohomology.
Likewise, we can see group cohomology as a right-derived functor, but for calculations, we use more concrete objects coming from projective modules.
Both of these examples come from the fact that we can calculate derived functors using acyclic objects. What are some other examples of this principle? I'd be interested either in different derived functors or something completely different.","['homological-algebra', 'algebraic-geometry', 'abstract-algebra', 'soft-question']"
4478049,Evaluate $\int_0^1 \frac{\log^3 x}{1+x}\arcsin^2\left(\frac{\sqrt{x}}{2}\right)\mathrm{d}x.$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm looking for evaluation of the integral $$\int_0^1 \frac{\log^3 x}{1+x}\arcsin^2\left(\dfrac{\sqrt{x}}{2}\right)\mathrm{d}x.$$ I've tried some trivial substitutions and series expansions so far but it didn't get me anything satisfactory. I'm wondering if there's a closed form for this integral in terms of classical special functions and constants? Any help would be highly appreciated. Thanks!","['integration', 'definite-integrals', 'special-functions', 'complex-analysis', 'sequences-and-series']"
4478060,Prove that the Jacobian is constant,"Suppose that $u=u(x,y)$ and $v=v(x,y)$ have continuous second partial derivatives. If for each $f$ , we have $$\frac{\partial^2f}{\partial u^2}+\frac{\partial^2f}{\partial v^2}=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2f}{\partial y^2},$$ prove that the Jacobian $\begin{pmatrix}\frac{\partial u}{\partial x}&\frac{\partial u}{\partial y}\\\frac{\partial v}{\partial x}&\frac{\partial v}{\partial y}\end{pmatrix}$ is constant. By using the chain rule, $$\frac{\partial^2f}{\partial x^2}=\left(\frac{\partial u}{\partial x}\right)^2\frac{\partial^2f}{\partial u^2}+2\frac{\partial u}{\partial x}\frac{\partial v}{\partial x}+\left(\frac{\partial v}{\partial x}\right)^2\frac{\partial^2f}{\partial v^2}\\
\frac{\partial^2f}{\partial y^2}=\left(\frac{\partial u}{\partial y}\right)^2\frac{\partial^2f}{\partial u^2}+2\frac{\partial u}{\partial y}\frac{\partial v}{\partial y}+\left(\frac{\partial v}{\partial y}\right)^2\frac{\partial^2f}{\partial v^2}$$ and we see that the Jacobian is orthogonal. But how to prove that it is constant?","['multivariable-calculus', 'calculus', 'jacobian']"
4478093,Number of $f \colon \mathbb{N} \to \mathbb{N}$ such that $f^a(x) = x+b$,"This is a generalized version of the IMO problem: Prove that the function $f \colon \mathbb{N} \to \mathbb{N}$ such that $f(f(x)) = x+k$ is nonexistent if $k$ is odd. Now here is my problem: For natural numbers $a,b$ , let $$\mathcal{F} = \{f \colon \mathbb{N} \to \mathbb{N}\mid \forall x \in \mathbb{N},\: f^a(x) = x+b \}$$ (a) Find a neccesary and sufficient for $\mathcal{F} \neq \varnothing$ . (b) Find $|\mathcal{F}|$ . (in terms of $a$ and $b$ ) I was able to give proof for (a), by mimicking the solution of the above IMO problem. I think that $\mathcal{F}$ is nonempty iff $a\mid b$ . Let $S_i = f^{i}(\mathbb{N}) \setminus  f^{i+1}(\mathbb{N})$ , where $S_0 = \mathbb{N} \setminus f(\mathbb{N})$ . Proposition 1. $f^{j}(\mathbb{N}) \subseteq f^{i}(\mathbb{N})$ if $i \leq j$ . Proof. Since $f^{j}(\mathbb{N}) = f^{i}(f^{j-i}(\mathbb{N}))$ and $f^{j-i}(\mathbb{N}) \subseteq \mathbb{N}$ , it is pretty clear. Proposition 2. $S_i \cap S_j = \varnothing$ if $i \neq j$ . Proof. WLOG $i<j$ . Since $S_{i} = f^{i}(\mathbb{N}) \setminus  f^{i+1}(\mathbb{N})$ , so $S_i$ does not contain the elements of $f^{i+1}(\mathbb{N})$ . However, $S_j = f^{j}(\mathbb{N})\setminus  f^{j+1}(\mathbb{N})$ , so $S_j$ is a subset of $f^{j}(\mathbb{N})$ , and thus a subset of $f^{i+1}(\mathbb{N})$ , by proposition 1. So they are disjoint. Proposition 3. $|S_i| = |S_j|$ for all $i,j$ . Proof. It suffices to show that $|S_i| = |S_{i+1}|$ . I claim that $f$ is actually a bijection between them. (upon restriction of domain to $S_i$ ) We know that $f$ is an injection since $f^a$ is an injection. So surjectivity is a problem. Let $x \in f^{i+1}(\mathbb{N}) \setminus  f^{i+2}(\mathbb{N})$ , then there exists $y \in f^{i}(\mathbb{N})$ such that $f(y) = x$ , by definition. But if $y \in  f^{i+1}(\mathbb{N})$ , then $f(y) = x \in f^{i+2}(\mathbb{N})$ , contradiction. Therefore $y \in f^{i}(\mathbb{N}) \setminus  f^{i+1}(\mathbb{N}) = S_i$ , proving surjectivity. Let's say $|S_i| = n$ for all $i$ . Proposition 4. $\mathcal{F}$ is nonempty iff $a|b$ . Proof. If part is obvious(give $f(x) = x + b/a$ ). So let's go for only if part. Consider the set $$ \bigcup_{i = 0}^{a-1} S_i = [\mathbb{N} \setminus f(\mathbb{N})] \cup \cdots \cup [f^{a-1}(\mathbb{N}) \setminus  f^{a}(\mathbb{N})] = \mathbb{N} \setminus f^{a}(\mathbb{N})$$ And note that $\mathbb{N} \setminus f^{a}(\mathbb{N}) = \{1,2,\cdots, b\}$ by the condition of problem. However since $S_i$ 's are all disjoint, so $\left| \bigcup_{i = 0}^{a-1} S_i \right| = an = b $ . Thus $\mathcal{F} = \varnothing $ if $a \nmid b$ , completing the proof. However, I have no idea about (b). Hint for this problem suggests finding a bijection from $\mathcal{F}$ to some easy set, such as a subset of $S_b$ (symmetric group). But what is the set? How can I evaluate the size of $|\mathcal{F}|$ ? Thank you for any help, hint, or solution.","['elementary-set-theory', 'combinatorics', 'discrete-mathematics', 'contest-math']"
4478098,Proof help for the existence of a function in Velleman 3rd Chapter 8 section 2,"I'm quite stuck on problem 16 from above chapter in Velleman's book. Prove there is a function $f: \Bbb Z^+ \rightarrow \Bbb Z^+$ such that for all positive integers $a$ , $b$ , and $c$ there exists a positive integer $n$ such that $f(an+b) = c$ . To prove this I need to come up with said function $f$ . I know that for specific $a$ and $b$ , $an+b$ describes a kind of discrete linear function on the positive integers, the range of which is infinite. Also, every infinite subset $A$ of $\Bbb Z^+$ is denumerable, so $A \sim \Bbb Z^+$ . But this is where I fail to come up with a function $f$ general enough to work for every possible $a$ and $b$ . When definining $f$ , I don't know anything about $a$ , $b$ , or $c$ . After coming up with $f$ the only ""value"" I can tweak is $n$ . To proceed I would greatly appreciate some hints.","['elementary-set-theory', 'proof-explanation', 'functions', 'integers']"
4478112,Solving $ y' = x+y $ with Euler's method,"I was going over Euler's method for solving DE and I had an idea: Could we use it to get an exact solution to a DE by considering an infinitesimal step size? This is the main idea:  if the approximations get better the smaller the step size, what if we took the limit as the step size $ h $ go to zero (consider an infinitesimal step size), in theory that should make the approximations exact. This is how I implemented that idea: Given a first order linear ODE $ dy/dx=F(x,y) $ with initial conditions $ (x_0, y_0) $ and step size $ h $ . Euler's Method: $ x_{n+1} = x_n + h , \ \ \ \    y_{n+1} = y_n +F(x_n, y_n)h  $ Consider the DE $ dy/dx=x+y, \ \ \ y(0)=1 $ and consider an arbitrary step size $ h $ . To approximate the value of the function at an arbitrary $ x $ value we would need the following iterations $ x_1=x_o+h $ $ x_2 = x_1+h  = (x_o+h)+h = x_o+2h $ $ x_3 = x_2+h = (x_o+2h)+h = x_o+3h $ $ \vdots $ $ x_n = x $ (Iterate until we reach our desired $ x $ value). If we take a look at the pattern that emerges for each successive iteration, we can see that $ x_n = x_0+nh $ . Solving the equation above for $ n $ we get.. $ x_n=x $ $ x_0+nh=x \implies n = \cfrac{x-x_0}{h} $ Here, $ n $ is the number of iterations needed to reach a desired $ x $ value with an initial condition $ x_0 $ and step size $ h $ . Here we can see why having a step size that is infinitesimally small would result in an infinite number of iterations (as $ h \to 0 $ , $ n \to \infty $ ). Now let us consider the iterations for $ y $ . $ y_{n+1}=y_n+F(x_n,y_n)h $ $ y_{n+1}=y_n+(x_n+y_n)h $ (Plugging in our $ x_n $ from above, we get...) $ y_{n+1}=y_n+((x_0+nh)+y_n)h $ We can now solve this recurrence relation with initial conditions $ y(x_0)=y_0 $ $ y(n+1)=y(n)+(x_0+nh+y(n))h, \ \ \ \ y(x_0)=y_0 $ $ y(n)=(x_0h + x_0 + y_0 + 1)(h+1)^{n-x_0}-x_0 -hn -1 $ (Plugging in our initial conditions $ (x_0, y_0) = (0, 1) $ , we get...) $ y(n)=2(h+1)^n -hn-1 $ This is a function in terms of $ n $ and our solution has to be in terms of $ x $ . At this step we plug in our $ n $ from above and rewrite this in terms of $ x $ . $ n = \cfrac{x-x_0}{h} = \cfrac{x}{h} $ $ y(x) \approx 2(h+1)^{x/h}-h \left( \cfrac{x}{h}\right)-1 $ $ y(x) \approx 2(h+1)^{x/h}-x-1 $ And finally we take the limit as the step size $ h $ go to zero. $ \displaystyle y(x) = \lim_{h\to 0} \left[ 2(h+1)^{x/h}-x-1 \right] $ $ \displaystyle y(x)=\lim_{h\to 0}\left[ 2(h+1)^{x/h} \right]-x-1 $ $ \displaystyle y(x)=2\lim_{h\to 0}\left[(h+1)^{x/h} \right]-x-1 $ $ y(x) = 2e^x-x-1 $ It can be verified that $ \displaystyle \lim_{h\to 0}(h+1)^{x/h}=e^x $ And there we have it, we have just solved $ dy/dx = x+y, \ \ y(0)=1 $ exactly using Euler's Method by considering an infinitesimally small step size $ h $ . Has this been done before? Or does this method have a special name? I tried researching 'Euler's method with an infinitesimal step size' but I found nothing. (I've tried it on a few other ODE and came to an exact solution.)","['calculus', 'eulers-method', 'ordinary-differential-equations', 'numerical-methods']"
4478118,Probability of being close to a cycle permutation is $o(1)$,"I'm trying to prove that in some sense, cycle (maybe circular is a better wording?) permutations are ""sparse"" in the set of all permutations $S_n$ . Let's assume permutations distribute uniformly out of the $n!$ combinations. It is clear that the probability of getting a cycle permutation is $\frac{1}{n}$ . My question is what happens when we ""dilate"" the set according the the Hamming distance $$d(\sigma, \pi) = \frac{1}{n}\left |\left \{ i: \sigma(i)\ne \pi(i)\right \}\right |$$ Then denote the event of being $\epsilon$ -close to a cycle, i.e. $d(\sigma, \text{set of all cycles})<\epsilon$ , for instance for $\epsilon = \frac{1}{4}$ . How can I show this occurs in probability $o(1)$ if this is even true? I tried to upper-bound the probability by considering in how many ways I can degrade any $\frac{n}{4}$ indices of a given cycle, while still remaining a permutation, and got the bound $\frac{(n-1)!\binom{n}{n/4}(\frac{n}{4})!}{n!}$ which is way too loose. I thought of maybe representing the number of changes to get a cycle as a sum of indicator random variables somehow, and bound the probability to deviate from the expectation.","['permutations', 'combinatorics', 'probability']"
4478161,Why am I getting extraneous solutions solving $\frac{1+\sqrt{1-x}}{x-\sqrt{1-x^{2}}}=2 x$?,"I came across a beautiful problem: Solve $$\frac{1+\sqrt{1-x}}{x-\sqrt{1-x^{2}}}=2 x$$ My approach: Obviously $x=0$ is not a solution.
Now we have: $$1+\sqrt{1-x}=2x^2-2x\sqrt{1-x^2} \tag1$$ Squaring the above equation both sides we get $$2(1+\sqrt{1-x})-x=4x^4+4x^2(1-x^2)-8x^3\sqrt{1-x^2} \tag2$$ From $(1)$ we get $$2(2x^2-2x\sqrt{1-x^2})-x=4x^2-8x^3\sqrt{1-x^2} \tag3$$ $\implies$ $$(8x^3-4x)\sqrt{1-x^2}=x \tag4$$ So we have: $$(8x^2-4)\sqrt{1-x^2}=1 \tag5$$ Letting $\sqrt{1-x^2}=p, p>0$ We get $$8p^3-4p+1=0 \tag6$$ which gives $p = 0.5, \frac{\sqrt{5}-1}{4}$ So using $x=\pm \sqrt{1-p^2}$ we get four values of $x$ as: $$x=\frac{\pm \sqrt{3}}{2}, \frac{\pm \sqrt{10+2 \sqrt{5}}}{4} \tag7$$ Out of which only $$x=\frac{-\sqrt{3}}{2}, \frac{-\sqrt{10+2 \sqrt{5}}}{4}$$ Will satisfy. But at which step the positive extraneous roots boiled down and how to get rid them in this approach?","['contest-math', 'algebra-precalculus', 'irrational-numbers', 'polynomials']"
4478242,"Why is ""isometric"" not part of ""isomorphism""?","Suppose we have two normed/metric spaces $X$ and $Y$ and suppose $X$ and $Y$ are isometrically isomorphic meaning there exists an isomorphism $T: X \rightarrow Y$ which is also an isometry. This is the ""nicest"" map possible between these spaces. However, if one should think of an isomorphism in general as something that ""preserves structure"", why is isometry property not just contained in the definition of an isomorphism? Wouldn't one expect that distances is part of the structure of a set (with a distance function)? In Rudin's ""Functional Analysis"", he also writes ""isometric isomorphism"" when appropriate. So from I can tell this in the general notion? Can anyone help shed some light on this?","['isometry', 'functional-analysis', 'terminology']"
4478306,Missing Values in a Dataset for Multiple Regression,"I've seen countless projects and tutorials on Kaggle where authors use calculated mean values to replace missing values in a column. However, I am now reading the Multiple Regression. A Primer. book by Paul D. Allison in which the author writes this: [...] Even worse than pairwise deletion is replacement with means. In this method, the missing values are replaced with the mean value of the variable for those individuals without the missing data. So, I am curious is replacement with means a legitimate way to deal with missing values or are there more common and preferrable ways to do it?","['linear-regression', 'statistics']"
4478329,Two ways of defining connection coefficients,"I am relatively new to general relativity, and I am puzzled about the relationship between two ways of introducing connection coefficients on a differentiable manifold. One way (e.g. in S. Carroll's ""Spacetime & Geometry"") defines the connection coefficients, $\Gamma^\nu_{\mu\lambda}$ , as arising from a linear correction added to the partial derivative of a vector field, $\partial_\mu V^{\nu}$ , to make the whole thing --- $\nabla_\mu V^\nu = \partial_\mu V^{\nu} + \Gamma^\nu_{\mu\lambda}V^\lambda$ --- transform like a tensor. To achieve this, the $\Gamma^\nu_{\mu\lambda}$ have to satisfy a certain transformation law, but otherwise there are no constraints. So it seems this leaves us with a lot of freedom in choosing the connection coefficients: Fixing a coordinate system, I can choose any combination of $4\cdot 4\cdot 4 = 64$ smooth functions to define my connection coefficients. The other way introduces connection coefficients (as far as I understand) as arising from the change in the basis vectors when traveling between tangent spaces (as seen from the perspective of the basis of the original tangent space): \begin{align}
&\partial_\mu \left(V^{\nu}\cdot \mathbf{e}_{(\nu)}\right)\\
=& \left(\partial_\mu V^\nu\right)\cdot \mathbf{e}_{(\nu)} + V^\lambda \cdot \left(\partial_\mu \mathbf{e}_{(\lambda)}\right)\\
=& \left(\partial_\mu V^\nu\right)\cdot \mathbf{e}_{(\nu)} + V^\lambda \cdot \left(\Gamma^\nu_{\mu\lambda}\mathbf{e}_{(\nu)}\right)\\
=&\left(\partial_\mu V^\nu + V^\lambda \Gamma^\nu_{\mu\lambda}\right)\cdot \mathbf{e}_{(\nu)} 
\end{align} where in the third step we have defined $\Gamma^\nu_{\mu\lambda}\mathbf{e}_{(\nu)} := \partial_\mu \mathbf{e}_{(\lambda)}$ . I'm puzzled because this second way of defining the connection coefficients doesn't seem to come with the same freedom in choosing the coefficients: By fixing a chart, we already fix a basis of the tangent space at each point within the chart's domain, and so it seems we automatically fix the connection coefficients as expressed in that chart. Whereas previously we could choose the connection coefficients by freely choosing $64$ different functions, now there seems to be a unique connection (at least within that part of the chart's domain which doesn't overlap with any other chart's domain). What am I getting wrong here?","['connections', 'general-relativity', 'differential-geometry']"
4478353,Non-simple group with only two quotients (up to isomorphism),"In Paul Cohn's Universal Algebra book, p. 61, he writes ""[...]Every $\Omega$ -algebra $A$ has itself and the trivial algebra as homomorphic images. If it has no others and is non-trivial it is said to be simple . By [the first isomorphism theorem] an $\Omega$ -algebra $A$ is simple if and only if has precisely two congruences, namely $A^2$ and $\Delta$ [the diagonal]."" Here, as usual, we identify algebras up to isomorphism. I am pretty sure this is wrong. At least, the argument does not follow, as a quotient of a strucutre $A$ by a non-trivial congruence may be still isomorphic to $A$ (e.g. the multiplicative circle $\mathbb{S}^1\subseteq\mathbb{C}\setminus\left\{0\right\}$ modulo $\left\{\pm 1\right\}$ , as a structure in the signature of groups; or any infinite set modulo a relation that identifies two points, in the empty signature.) So I was trying to find an example of a non-simple structure $A$ (in the sense that it has nontrivial congruences) for which all non-trivial homomorphic images are isomorphic to $A$ . I'd suppose this is possible even for groups, but could not find a concrete example Question : Is there a non-simple group $G$ whose homomorphic images are all isomorphic to either $G$ or to the singleton group $\{1\}$ ? I thought a bit of $G=\mathbb{Q}/\mathbb{Z}$ but got nowhere.","['group-theory', 'abstract-algebra', 'examples-counterexamples']"
4478367,"In a group $G$ acting on a set $X$, can $X$ have indistinguishable elements?","I'm coming back to Group Theory after 20+ years out of it, and thus my vocabulary is a bit rusty. I'll try to express my problem the best I can. Consider a group G acting on a set X. What I want is to let some elements of X to be indistinguishable. My guess is that it should be some quotient, but I don't know how to exactly define it. For example, I have the group $G$ generated by $a=(1,2,3,4)$ and $b=(2,7,6,5)$ . That's two circles intersecting at point 2 that can only rotate. I can express any element of $G$ as a word of $a$ and $b$ . My problem is: Now imagine points 1, 3 and 4 being of color blue. I want to express an element of $G$ as a word of $a$ and $b$ but I don't care where 1, 3 and 4 move as long as they fall on a blue position again. That is: elements 1, 3, and 4 are indistinguishable as far as color matters. How can I define that? Am I talking about stabilizers? Isotropy? Some kind of quotient? Where can I read further to find what I am looking for?","['group-theory', 'group-actions']"
4478417,$e^{Ax}=\cosh(x)I_{n}+\sinh(x)A$,"Let $A\in\mathbb{M}_{n}$ such that $A^2=I_n$ . Show that for every $x\in\mathbb{R}$ , $$e^{Ax}=\cosh(x)I_{n}+\sinh(x)A$$ . Attempt: $$\begin{align*}
        e^{xA}&=\begin{pmatrix}
        \sum_{i=0}^{\infty}\frac{(xa_{11})^{i}}{i!} & \cdots  & \sum_{i=0}^{\infty}\frac{(xa_{1n})^{i}}{i!} \\
        \vdots  & \ddots  & \vdots  \\
        \sum_{i=0}^{\infty}\frac{(xa_{n1})^{i}}{i!} & \cdots  & \sum_{i=0}^{\infty}\frac{(xa_{nn})^{i}}{i!} \\
        \end{pmatrix}\\
        &=\begin{pmatrix} \sum_{k=0}^{\infty}\frac{(a_{11}x)^{2k}}{(2k)!}+\sum_{k=0}^{\infty}\frac{(a_{11}x)^{2k+1}}{(2k+1)!} & \cdots  & \sum_{k=0}^{\infty}\frac{(a_{1n}x)^{2k}}{(2k)!}+\sum_{k=0}^{\infty}\frac{(a_{1n}x)^{2k+1}}{(2k+1)!} \\ \vdots  & \ddots  & \vdots  \\ \sum_{k=0}^{\infty}\frac{(a_{n1}x)^{2k+1}}{(2k+1)!}+\sum_{k=0}^{\infty}\frac{(a_{n1}x)^{2k+1}}{(2k+1)!} & \cdots  & \sum_{k=0}^{\infty}\frac{(a_{nn}x)^{2k+1}}{(2k+1)!}+\sum_{k=0}^{\infty}\frac{(a_{nn}x)^{2k+1}}{(2k+1)!} \\ \end{pmatrix}\\
        &=\begin{pmatrix} \cosh(x)a_{11}^{2k} & \cdots  & \cosh(x)a_{1n}^{2k} \\ \vdots  & \ddots  & \vdots  \\ \cosh(x)a_{n1}^{2k} & \cdots  & \cosh(x)a_{nn}^{2k} \\ \end{pmatrix}+\begin{pmatrix} \sinh(x)a_{11}^{2k}a_{11} & \cdots  & \sinh(x)a_{1n}^{2k}a_{1n} \\ \vdots  & \ddots  & \vdots  \\ \sinh(x)a_{n1}^{2k}a_{n1} & \cdots  & \sinh(x)a_{nn}^{2k}a_{nn} \\ \end{pmatrix}\\
        &=\cos(x)A^{2k}+\sinh(x)A^{2k}A\\
        &=\cosh(x)I_{n}+\sinh(x)A.
    \end{align*}$$ Is this correct?","['matrices', 'solution-verification', 'ordinary-differential-equations']"
4478424,Statement in an Erdos paper about primitive sets,"I was reading a paper from Erdos where he proved the following: let $(a_n)$ be a sequence such that if $a_n$ divides $a_m$ then $m=n$ , and let $p_n$ be the greatest prime factor of $a_n$ then $\sum_{k=1}^\infty \frac{1}{a_n \log(a_n)}$ converges, and there's something about the proof that I don't get. The thing I don't understand is that in the proof he shows that $\sum_{k=1}^\infty \frac{1}{a_n}\prod_{p\leq p_k}(1-\frac{1}{p})\leq 1$ (this I understand), but he goes on to say that this implies that $\sum_{k=1}^\infty \frac{1}{a_n \log(a_n)}$ converges because there exists some $c$ such that: $$ \prod_{p\leq p_n} \left(1-\frac{1}{p}\right) > \frac{c}{\log p_n}\geq \frac{c}{\log a_n}$$ Why does this $c$ exist? The second inequality is trivial but I don't see where the first one comes from.","['number-theory', 'prime-numbers']"
4478430,Sub-blocks of matrix quadratic equations,"Suppose the following matrix quadratic equation has at least one real solution for $X$ : $$
\begin{bmatrix} 
	A & a \\
	0 & 1\\
\end{bmatrix} X^2 +
\begin{bmatrix} 
	B & b \\
	0 & 0\\
\end{bmatrix} X +
\begin{bmatrix} 
	C & c \\
	0 & 0\\
\end{bmatrix} = 0,
$$ where $A$ , $B$ and $C$ are real square matrices, $a$ , $b$ and $c$ are real vectors, and $X$ is a square matrix the same size as $\begin{bmatrix} 
	A & a \\
	0 & 1\\
\end{bmatrix}$ . Now consider the following matrix quadratic equation for $Y$ : $$A Y^2 + B Y + C = 0,$$ where $Y$ is a square matrix the same size as $A$ . Must this have a real solution for $Y$ ? If not, must this at least hold generically over all $A,B,C,a,b,c$ for which the first equation has a real solution for $X$ ? Note: In the case in which $A,B,C,a,b,c$ are all scalars, the solutions to the original equation are as follows: Either: $X_{1,1}$ is a solution to $A X_{1,1}^2+B X_{1,1}+C=0$ , $X_{2,1}=X_{2,2}=0$ , $X_{1,2}=-\frac{c}{AX_{1,1}+B}$ . OR $X_{1,1}=-\frac{C c}{B c-b C}$ , $X_{1,2}=-\frac{c^2}{B c-b C}$ , $X_{2,1}=-\frac{C^2}{B c-b C}$ , $X_{2,2}=\frac{C c}{B c-b C}$ . In the former case, my claim holds. In the latter case, there is no guarantee that it does (it seems).","['matrices', 'linear-algebra', 'matrix-equations', 'quadratics', 'quadratic-forms']"
4478459,Find angle $x$ in the figure below,"For reference: If $AB = BC$ and $AC = BD$ find x(Answer: $30^o$ ) I found the possible Angles, I drew some auxiliary lines ( $DG \parallel AC, NH \perp AC$ ) but I still couldn't find the relationship for the solution $\alpha+x+84-\beta +\theta = 180 \implies x = 96-\alpha+\beta-\theta\\
\beta +24-\alpha = 180-138 \implies \beta-\alpha = 18\\
x =114 - \theta $","['euclidean-geometry', 'geometry', 'plane-geometry']"
4478473,"Why the phrase ""we can assume without loss of generality""?","I am studying the proof of the following theorem, but I don't understand the phrase ""we can assume without loss of generality"" Theorem: Let $X$ be a compact space and $Y$ a $KC$ space such that $Y^{\star}=Y\cup \{ \infty \}$ be the one-point compactification of $Y$ . Then a set $C\subset X\times Y$ is compactly closed in $X\times Y$ if and only if $C\cup (X\times \{ \infty \})$ is compactly closed in $X\times Y^{\star}$ . Definitions: $X$ is a $KC$ space if every compact $K\subset X$ is closed. Let $C\subset X$ . Then $C$ is compactly closed if for all closed compact $K\subset X$ , if $C\cap K$ is compact. Proof of the Theorem Suppose $C$ is compactly closed in $X\times Y$ . Let $\Phi=\{V_\alpha \}_{\alpha \in I}$ be an open cover of $C\cup (X\times \{\infty \})$ . Since $X\times \{\infty \}$ is compact we can assume without loss of generality that it exists $\alpha_0 \in I$ such that $V_{\alpha_0}$ has the form $V_{\alpha_0}=X\times W$ , where $W$ is an open set of $Y^\star$ with $\infty \in W$ . By the definition of the topology of $Y^\star$ , we have that $W = Y^\star -K$ with $K$ compact and closed subset of $Y$ . Note that $Y^\star-W = K$ and therefore $Y^\star-W$ is compact and closed in Y. Note that $X\times (Y^\star -W)$ is compact and closed in $X\times Y$ . Since $C$ is compactly closed in $X\times Y$ , we have $H=C\cap (X\times (Y^\star -W) )$ is compact and therefore exists $\{V_{\alpha_1}, V_{\alpha_2}, \ldots , V_{\alpha_n} \} \subset \Phi$ such that $H\subset \bigcup_{i=1}^n V_{\alpha_i}$ . Then $C\cup (X\times \{\infty \})\subset \bigcup_{i=1}^n V_{\alpha_i}$ Now suppose $C\cup (X\times \{\infty \})$ is compact in $X\times Y^\star$ . Let $K$ be a compact closed subset of $X\times Y$ . Consider the projection function $\rho_2: X\times Y \rightarrow Y$ . Then we have $\rho_2(K)$ is compact in $Y$ . Since $Y$ is a $KC$ space, we have $\rho_2(K)$ is closed in $Y$ and then $\rho_2(K)$ is closed in $Y^\star$ . Finally note that $C\cup K=[C\cup(X\times \{\infty \})]\cap (X\times \rho_2(K))\cap K$ is compact and therefore $C$ is compactly closed en $X\times Y$ . If you have any idea why the phrase ""we can assume without loss of generality"" is valid please let me know","['proof-explanation', 'general-topology']"
4478500,Assume $A$ is open convex and $f$ convex continuous. Then $f$ is $L$-lipschitz on $A$ if and only if $\partial f(A) \subset L B_{X^*}$,"This thread is meant to record a question that I feel interesting during my self-study. I'm very happy to receive your suggestion and comments. See: SE blog: Answer own Question and MSE meta: Answer own Question . Let $A$ be a subset of a normed space $X$ and $f:A \to \mathbb R$ . The subdifferential of $f$ at $a \in A$ is the set $$
\partial f(a)=\left\{x^* \in X^* \mid f(x) - f(a) \ge \langle x^*, x-a \rangle \text { for each } x \in A\right\}.
$$ The elements of $\partial f(a)$ are called subgradients of $f$ at $a$ . The multivalued mapping $\partial f: A \to \mathcal P(X^*), x \mapsto \partial f(x)$ , is called the subdifferential of $f$ . The image $\partial f(E)$ of a set $E \subset A$ is the set $$
\partial f(E)=\bigcup_{x \in A} \partial f(x) .
$$ Theorem: Assume $A$ is open convex and $f$ convex continuous. Then $f$ is $L$ -lipschitz on $A$ if and only if $\partial f(A) \subset L B_{X^*}$ .","['convex-analysis', 'normed-spaces', 'derivatives', 'functional-analysis']"
4478519,Is it possible to maximize $\frac{3t^2}{t^3+4}$ (where $t>0$) without taking derivative?,"To find the maximum of $f(t)=\dfrac{3t^2}{t^3+4}$ (for $t>0$ ) we can simply equate the derivative with zero, $$f'(t)=0\Rightarrow 6t(t^3+4)-3t^2(3t^2)=0\Rightarrow -3t^4+24t=0\Rightarrow t=2$$ And $f_{max}=f(2)=1$ . I'm wondering is it possible to find the maximum without taking derivative? I'm eager to see other methods to maximize the function.","['algebra-precalculus', 'inequality']"
4478525,prove or disprove that:$a^2+b^2+c^2\leq \frac{27}{4}$,"I tried to solve the below problem, I spend more than 5h just for prove it but without any result , this is the best attempt I did ,just I need to show that $a^2+b^2+c^2\leq \frac{27}{4}$ if $a,b,c>0$ and $2abc+3(ab+ac+bc)=27$ . prove that; $16(a^2+b^2+c^2)+8abc\geq 135$ My attempt: $2abc+3(ab+ac+bc)=27$$\Leftrightarrow$$10abc+15(ab+ac+bc)=135$ so $135\leq 16(a^2+b^2+c^2)+8abc$$\Leftrightarrow$ $10abc+15(ab+ac+bc)\leq 16(a^2+b^2+c^2)+8abc$$\Leftrightarrow$$2abc+15(ab+ac+bc)\leq 16(a^2+b^2+c^2)$ . We know this; $15(ab+bc+ac)\leq 15\sqrt{a^2+b^2+c^2}\sqrt{a^2+b^2+c^2}=15(a^2+b^2+c^2)$ (*)(schwartz inequality). and $2abc\leq 2\sqrt{(\frac{(ab+ac+bc)}{3})^3}\leq 2\sqrt{\frac{(a^2+b^2+c^2)^3}{27}}$ . Let's show that : $2\sqrt{\frac{(a^2+b^2+c^2)^3}{27}}\leq a^2+b^2+c^2$ $2\sqrt{\frac{(a^2+b^2+c^2)^3}{27}}\leq a^2+b^2+c^2$$\Leftrightarrow$$ \frac{(a^2+b^2+c^2)^3}{27}\leq \frac{(a^2+b^2+c^2)^2}{4}$$\Leftrightarrow$$ \frac{4(a^2+b^2+c^2)^3}{4.27}-\frac{27(a^2+b^2+c^2)^2}{4.27}\leq 0$ (**) If (**) is true then we will have $ 2abc\leq a^2+b^2+c^2$ and that's what we need for complet the proof . Put $ f(x)=4x^3-27x^2$ ,the only positive solution of this function is $\frac{27}{4}$ ,and after the variation tableau of this function We can see that $f(x)\leq 0$ for $ x\leq \frac{27}{4}$ . then $ \frac{4(a^2+b^2+c^2)^3}{4.27}-\frac{27(a^2+b^2+c^2)^2}{4.27}\leq 0$ ,so $ 2abc\leq a^2+b^2+c^2$ $\color{blue}{\textrm{note that $ :a^2+b^2+c^2\leq \frac{27}{4}$}}$ So finally; after (*) and (**) we can say that : $ 2abc+15(ab+ac+bc)\leq 16(a^2+b^2+c^2)$ ,and this complet the proof . the problem is that:i can't show why  this $\color{blue}{\textrm{ $ :a^2+b^2+c^2\leq \frac{27}{4}$}}$ is true So my question is that :can you prove or disprove this: $\color{blue}{\textrm{ $ :a^2+b^2+c^2\leq \frac{27}{4}$}}$ ?. Note that:( i dont want any answers for the general problem,just i need to develop my attempt if that is possible)","['algebra-precalculus', 'inequality']"
4478530,Characterize subdifferential of a convex function by directional derivative,"This thread is meant to record a question that I feel interesting during my self-study. I'm very happy to receive your suggestion and comments. See: SE blog: Answer own Question and MSE meta: Answer own Question . Let $A$ be a subset of a normed space $X$ and $f:A \to \mathbb R$ . Let $a \in \operatorname{int} A$ . For $v \in X$ , the right directional derivative $f_{+}^{\prime}(a)[v]$ , the left directional derivative $f_{-}^{\prime}(a)[v]$ , and the ( bilateral ) directional derivative $f^{\prime}(a)[v]$ are defined by: $$
\begin{aligned}
f_{+}^{\prime}(a)[v] &= \lim _{t \to 0^+} \frac{f(a+t v)-f(a)}{t} \\
f_{-}^{\prime}(a)[v] &= \lim _{t \to 0^-} \frac{f(a+t v)-f(a)}{t} \\
f^{\prime}(a)[v] &= \lim _{t \to 0} \frac{f(a+t v)-f(a)}{t}.
\end{aligned}
$$ We say that $f$ is GÃ¢teaux differentiable at $a$ if $f^{\prime}(a) \in X^{*}$ . The subdifferential of $f$ at $a \in A$ is the set $$
\partial f(a)=\left\{x^* \in X^* \mid f(x) - f(a) \ge \langle x^*, x-a \rangle \text { for each } x \in A\right\}.
$$ The elements of $\partial f(a)$ are called subgradients of $f$ at $a$ . Theorem: Assume $A$ is open convex and $f$ convex. For $a\in A$ and $x^* \in X^*$ , the following assertions are equivalent: (i) $x^* \in \partial f(a)$ ; (ii) $x^*(v) \leq f_{+}^{\prime}(a)[v]$ for each $v \in X$ ; (iii) $f_-^{\prime}(a)[v] \leq x^*(v) \leq f_{+}^{\prime}(a)[v]$ for each $v \in X$ . As a corollary, we obtain that $\partial f(a)$ is fully determined by the values of $f$ in any neighborhood of $a$ .","['convex-analysis', 'normed-spaces', 'derivatives', 'functional-analysis']"
4478534,"If $\int f=\int g$ and $\int_0^t f \geq \int_0^t g$ for all $tâ¥0$, then is $\int_0^t g^{-1} \geq \int_0^t f^{-1}$ for all $t\geq 0$?","Suppose $f,g$ are continuous, integrable, decreasing, nonnegative-real-valued functions, each defined on some interval of nonnegative real numbers with left endpoint $0$ , and satisfying $\inf f = \inf g = 0$ . (The intervals can be open, half-open, or closed, and may be infinite.) The assumptions imply that $f$ and $g$ have similarly continuous, integrable, decreasing inverses (I mean inverse with respect to function composition) defined on similar intervals. Then suppose $$\int f = \int g,$$ where the integration is over their domains. Finally, suppose that for all $t$ for which both functions are defined, we have $$ \int_0^t f \geq \int_0^t g.$$ It seems to me that in this situation it follows that $$ \int_0^t g^{-1} \geq \int_0^t f^{-1} $$ for all $t$ for which both $f^{-1}$ and $g^{-1}$ are defined, as well. But I think this for an essentially hand-wavy reason: ""Since the functions are decreasing and trapped in the first quadrant, then if mass is moved downard, it has to go to the right."" Question: Is the claim correct, and if so, how is it properly proven?",['real-analysis']
4478545,"Find the maximum of $\frac{abc}{(4a+1)(9a+b)(4b+c)(9c+1)}$,where$a,b,c>0$ [duplicate]","This question already has answers here : Find the maximum value of $\frac{xyz}{(1+5x)(4x+3y)(5y+6z)(z+18)}$ (4 answers) Closed 1 year ago . $a,b,c>0$ , find the maximum of : $$\frac{abc}{(4a+1)(9a+b)(4b+c)(9c+1)}$$ I try to find the minimum of $\frac{(4a+1)(9a+b)(4b+c)(9c+1)}{abc}=\frac{4a+1}{\sqrt{a}}\cdot\frac{9a+b}{\sqrt{ab}}\cdot\frac{4b+c}{\sqrt{bc}}\cdot\frac{9c+1}{\sqrt{c}}
=\left(4\sqrt{a}+\frac{1}{\sqrt{a}}\right)\left(9\sqrt{\frac{a}{b}}+\sqrt{\frac{b}{a}}\right)\left(4\sqrt{\frac{b}{c}}+\sqrt{\frac{c}{b}}\right)\left(9\sqrt{c}+\frac{1}{\sqrt{c}}\right) \geq 4 \times 6\times 4\times 6$ when I try to use AM-GM inequality, I find I can't take equality in the above $4$ parentheses.so I only find a upper bound of the expression.: $\frac{abc}{(4a+1)(9a+b)(4b+c)(9c+1)} < \frac{1}{576}$","['algebra-precalculus', 'analysis', 'inequality']"
4478547,Why is the set of real numbers non-enumerable?,"If, for every real number I can think of, I can always find a new natural number to pair with it, then why are the reals non-enumerable ? If the definition of enumerable is that I can pair a natural number with it, and there are infinite natural numbers, then I can always find a bigger natural number to pair with whatever real number I can think of, regardless of the number of digits, and regardless of whether or not I found them through diagonalization.",['elementary-set-theory']
4478563,"Is everything ok with this notation? $A=\{x\mid 2x,x\in\Bbb{N}\}$","I couldn't understand this notation. Is everything ok with this notation? I would write it like this: $$A=\left\{n: n=2k, k\in\mathbb Z_{>0}\right\}$$ or $$A=\left\{n\mid n=2k, k\in\mathbb Z_{>0}\right\}$$ or $$A=\left\{n: \text{n is an even natural number}\right\}$$ or $$A=\left\{n\mid \text{n is an even natural number}\right\}$$","['elementary-set-theory', 'notation']"
4478566,"Let $Z$ be a standard normal random variable, prove the following","Let $Z$ be a standard normal random variable, prove: $P(Z > z) \leq \frac{e^{-\frac{z^2}{2}}}{2}$ How do I approach this question? Do I assume the moment generating function (in Chernoff Bounds) is $e^{-\frac{z^2}{2}}$ ? Does it even have anything to do with moment generating function or I can just simply use something like Markov's inequality or Chebyshev's inequality to solve this?",['statistics']
4478592,"How to show $A^2e_i=-\frac{1}{2} R_A(\cdot, \cdot)e_i$ in Topping's Lectures on the Ricci flow","When I read the 9.5 of Topping's Lectures on the Ricci flow, I have some problem. Assume $W$ is a vector bundle over manifold $(M,g)$ , and $A$ is connection on $W$ . $\{e_1,...,e_l\}$ is a frame of $W_p$ , and extend it to a local frame for $W$ by radial parallel translation using the connection $A$ , then we have $Ae_i=0 $ , and $$
A^2e_i=-\frac{1}{2} R_A(\cdot, \cdot)e_i   \tag{1}
$$ where $R_A$ is the curvature of the connection $A$ .   I don't know how to get $(1)$ . In my view, for any $v_a,v_b\in T_pM$ $$
R_A(v_a,v_b)e_k= A_a A_b e_k - A_b A_a e_k
$$ where $A_ae_k=A_{v_a}e_k$ .  How it imply $(1)$ ? PS(2022-7-4): I use the notation of chapter 4 of   S.S. Chern and W.H. Chen's ãå¾®åå ä½è®²ä¹ã. Assume the connection matrix of $W$ is $$
\omega=
\begin{pmatrix}
\omega_1^1 ,...\omega_1^l
\\
~~\\
\omega_1^l ,...\omega_l^l
\end{pmatrix}
$$ where $\omega_a^b=\Gamma_{ai}^b dx^i$ . And the curvature matrix is $\Omega= d\omega -\omega\wedge \omega$ , namely $$
\Omega=
\begin{pmatrix}
d\Gamma_{ai}^b \wedge dx^i  - \omega_a^r\wedge \omega_r^b
\end{pmatrix}
$$ Therefore, the curvature operator is $$
R_A(\cdot, \cdot) e_a
=
\begin{pmatrix}
d\Gamma_{ai}^b \wedge dx^i  - \omega_a^r\wedge \omega_r^b
\end{pmatrix} \otimes e_b
$$ where $e_a=\frac{\partial}{\partial x_a}$ is tangent vector. I use the definition $$
e_1\wedge e_2 = e_1 \otimes e_2 - e_2 \otimes e_1
$$ Then, I have $$
R_A(\cdot, \cdot) e_a =  
d\Gamma_{ai}^r\otimes dx^i \otimes e_r
- dx^i \otimes d\Gamma_{ai}^r\otimes e_r  \\
-\Gamma_{ai}^b \Gamma_{bj}^r dx^i\otimes dx^j\otimes e_r 
+\Gamma_{bi}^r \Gamma_{aj}^b dx^i\otimes dx^j\otimes e_r
\\
=\left(
\frac{\partial \Gamma_{aj}^r}{\partial x_i}
-\frac{\partial \Gamma_{ai}^r}{\partial x_j}
-\Gamma_{ai}^b\Gamma_{bj}^r
+\Gamma_{bi}^r\Gamma_{aj}^b
\right) dx^i\otimes dx^j \otimes e_r  
\tag{2}
$$ On the other hand, first, I have $$
A e_a = \Gamma_{ai}^b dx^i\otimes e_b
$$ Then $$
A^2 e_a = 
A(
\Gamma_{ai}^b dx^i \otimes e_b
)\\
=
d\Gamma_{ai}^b \otimes dx^i \otimes e_b 
-
\Gamma_{ai}^b  \Gamma_{cd}^i dx^c \otimes dx^d\otimes e_b 
+
\Gamma_{ai}^b \Gamma_{bk}^r dx^i \otimes dx^k \otimes e_r
\\
=\left(
\frac{\partial \Gamma_{aj}^r}{\partial x_i}
-\Gamma_{ak}^r\Gamma_{ij}^k
+\Gamma_{ai}^b\Gamma_{bj}^r
\right)dx^i\otimes dx^j \otimes e_r  
\tag{3}
$$ Then, I think I should use an analogue of Bianchi to get $(1)$ from $(2)$ and $(3)$ . But now, I still don't know how to use a suitable Bianchi. Besides, of course, if assume  local trivialization, (2) and (3) will become much simple. PS(2022-7-5): If I use the geodesic normal coordinate at $p$ , then (2) and (3) become $$
R_A(\cdot, \cdot) e_a
=
\begin{pmatrix}
d\Gamma_{ai}^b \wedge dx^i  - \omega_a^r\wedge \omega_r^b
\end{pmatrix} \otimes e_b
$$ where $e_a=\frac{\partial}{\partial x_a}$ is tangent vector. I use the definition $$
e_1\wedge e_2 = e_1 \otimes e_2 - e_2 \otimes e_1
$$ Then, I have $$
R_A(\cdot, \cdot) e_a 
=  
\left(
\frac{\partial \Gamma_{aj}^r}{\partial x_i}
-\frac{\partial \Gamma_{ai}^r}{\partial x_j}
\right) dx^i\otimes dx^j \otimes e_r  
\tag{4}
$$ $$
A^2 e_a 
=
\frac{\partial \Gamma_{aj}^r}{\partial x_i}
dx^i\otimes dx^j \otimes e_r  
\tag{5}
$$ The power series expansion of the metric in the coordinate is $$
h_{ij}(x) = \delta_{ij} -\frac{1}{3} R_{ikjl}(p) x^k x^l  + O(|x|^3)
\tag{6}
$$ where $x$ is in the geodesic normal coordinate of $p$ , $h$ is the metric of $W$ , and $R$ is cuvature of connection $A$ . But I want to show (1) at $p$ , seemly, it has nothing to the point $x$ . PS(2022-7-10): In fact, the next calculation is finished  in two or three days ago. Just since feeling boring, I  stopped. First,  since $Ah=0$ , we have $$
\Gamma_{ij}^m = \frac{1}{2}h^{lm} (\partial_i h_{jl}+ \partial_j h_{li} - \partial_l h_{ij})
$$ Therefore, in the geodesic normal coordinate of $p$ , we have $$
\partial_k \Gamma_{ij}^m(p)
= 
\frac{1}{2}h^{lm} (\partial_k\partial_i h_{jl}+ \partial_k\partial_j h_{li} - \partial_k\partial_l h_{ij})|_{x=p}
$$ Using (6) to calculate $\partial_k\partial_i h_{jl} (p)$ and $h^{lm} =\delta^{lm}$ ,we have $$
\partial_k \Gamma_{ij}^m(p) = -\frac{1}{3} (R_{mijk}(p) +R_{mjik}(p))
\tag{7}
$$ From (4),(5), we have $$
R_A(\cdot, \cdot) e_a =R_{raij} dx^i \otimes dx^j \otimes e_r
\tag{8}
$$ $$
A^2e_a = \frac{1}{3}(R_{raij} - R_{rjai}) dx^i\otimes dx^j\otimes e_r
\tag{9}
$$ Obviously, they are not  sustaining the (1).  Besides, I am sure (7) is right, since before I calculated it , I found it in Wiki . The possible error is the miscalculate of (8) and (9). But, in fact, I have done 5 or 6 times, and not  sustaining the (1). Therefore, I feel boring...","['ricci-flow', 'riemannian-geometry', 'differential-geometry']"
4478593,"Let $X_{1}, X_{2}, X_{3},...$ be a sequence of independent and identically distributed random variables, prove the following","Let $X_{1}, X_{2}, X_{3},...$ be a sequence of independent and identically distributed random variables with common (finite) mean $\mu$ . Prove that there exists and event A such that $P(A) = 1$ and for all $w \in \Omega$ , the quantity $\lim_{n \to \infty}(X_{1}(w)X_{2}(w)...X_{n}(w))^\frac{1}{n}$ converges and determine this limit. I get the part where these random variables are identically distributed, so the expectation of the sequence is also $\mu$ . But how do I proceed further? Will central limit theorem help here?",['statistics']
4478597,Euler's totient function and primes,"I have found a formula for Euler's totient function but I have no proof. I need help to demonstrate the formula if possible. $\phi$ denotes the Euler's totient function, $a$ denotes a natural number $>1$ and $n$ denotes a natural number multiple of $4$ . If the remainder of the division of $\phi\left(a^n-2\right)+1$ by $n$ is equal to $n-1$ then $\phi(a^n-2)+1$ is always a prime number. Example with $a=119$ and $n=20$ The remainder of the division of $\phi\left(119^{20}-2\right)+1$ by 20 is equal to 19 then $\phi\left(119^{20}-2\right)+1$ is prime. Thanks for your answers.",['number-theory']
4478606,"Balls and urns with constraints: no runs of adjacent balls, urns contain only 1 ball.","Here's the question:
Given some number of identical balls $1 < n\leq \frac{u}{2}$ , how many ways can you distribute them into a line of urns $u$ such that no two adjacent urns contain balls, and each urn may contain only 1 ball. Alternatively: given the same constraints on $n$ , what is the probability that the urns will contain a run of at least 2 adjacent balls (the real problem I'm trying to solve). My attempt thus far: The way to compute the ways we can order the balls is a combination $u\choose{n}$ , since the balls are indistinguishable. The first part of the problem may be an easier (or harder formulation) but I tried to tackle it with the understanding that I could use the following setup: $$\frac{{u\choose{n}} - part1}{u\choose n}$$ Where part1 is the solution to the first question. So, this reads: the total number of ways to arrange the balls - all the ways the balls are completely separated from each other, divided by all the configurations, to give me the probability I wanted as the solution for part 2. I think I'm on the right track here for solving part1, but I can't confirm it's correct. I think it's just ${u-n+1}\choose{n}$ . My (shaky) justification is that to guarantee there are no runs of adjacent balls, I must position the balls in a limited set of spaces, and this limit seemed right, though I'm unsure how to demonstrate it. I've done some empirical tests which seem to confirm it, and it looks like it works even for edge cases of only 1 ball (returns 0% probability), but I could seriously use a cross-check here. All in all, my final function is the following: $$\frac{{u\choose{n}} - {u-n+1\choose{n}} }{u\choose n}$$ And here's some Julia code that I used to ""verify"" it: #Recursively checks a list for consecutive 1s, returns a list of all instances.
has_consec(x)= 
length(x) > 1 ? vcat(x[1] == x[2] && x[1]==1, has_consec(x[2:end])) : return []

#filters a list of all the unique permutations of my list 
#(the combinations function is funky in julia for some reason) 
#by making sure there are no consecutive 1s, gets the length of the final list.

emp_test(n,y)=length(
filter(x->!any(has_consec(x)),
unique(
collect(
permutations(
vcat(repeat([1],y),repeat([0],n-y)
)))))) ```",['combinatorics']
4478615,Can quasi-concave and monotone functions level curves that are not path-connected?,"For $X = \mathbb{R}^2$ , does there exist a quasi-concave and monotone function $f : X \to \mathbb{R}$ that has a level curve which is not path-connected? Secondly, will every level curve necessarily intersect with the $x$ -axis and the $y$ -axis for the domain $X = \mathbb{R^2_{+}}$ ( $0 \in \mathbb{R_{+}}$ )? If $\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} > \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} \implies f(x_1, x_2) > f(y_1, y_2)$ for all comparable $(x_1, x_2) \in X$ and $(y_1, y_2) \in X$ , then we call $f$ (weakly) monotone. Context : While doing Economics, I was wondering if convex and monotone preferences (assuming completeness and transitivity) can have indifference curves that are not path-connected. A more precise version of the question will be: Given $X = \mathbb{R}^2$ and a preference relation $\succeq$ that is complete, transitive, convex and monotone, does there exist an indifference curve which is not path-connected? Definitions: Convexity: $\forall x,y \in X: x \succeq y \implies \lambda x + (1-\lambda) y \succeq y \ \forall \lambda \in [0,1]$ . Monotonicity: $\forall x, y \in X: x \geq y \implies x \succeq y$ . [Here, if $x = (x_1, x_2)$ and $y = (y_1, y_2)$ , then $x \geq y$ means $(x_1 \geq x_2) \land (y_1 \geq y_2)$ .] Indifference curve (IC): Suppose $x \in X$ . Then the set of elements that are indifferent to $x$ is given by $\text{IC}(x) := \{y \in X : y \succeq x \land x \succeq y \}$ . If $y \in \text{IC}(x)$ , we say $y \sim x$ where $\sim$ denotes indifference. Completeness and transitivity is defined similar to any other binary operator. This is a more precise version of the problem as there may not (necessarily) exist a function $f$ that describes $\succeq$ . But since this is MSE and I did not want to burden with definitions, I decided to assume the existence of $f$ .","['economics', 'general-topology', 'binary-operations', 'connectedness']"
4478619,Convex function on Riemannian manifold,"When I read the 9.5 of Topping's Lectures on the Ricci flow, I have some problem. Assume $W$ is a vector bundle over manifold $(M,g)$ , and $A$ is connection on $W$ . $\{e_1,...,e_l\}$ is a frame of $W_p$ , and extend it to a local frame for $W$ by radial parallel translation using the connection $A$ ,  and $E\in \Gamma(W)$ . $\Psi :W\rightarrow \mathbb R$ is parallel function, namely if $\omega_1\in W$ can be parallel translated (using the connection A) into $\omega_2\in W$ , then $\Psi(\omega_1)=\Psi(\omega_2)$ . Denote the restriction of $\Psi$ to the fibre $W_p$ as $\Psi_p$ , then there is $$
\nabla d (\Psi\circ E)(p)= Hess(\Psi_p)(E(p)) (AE(p), AE(p))   \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ d\Psi_p(E(p))(A^2E(p)+\frac{1}{2}R_A(\cdot,\cdot)E(p))  
\tag{9.5.5}
$$ If assume $\Psi_p$ weakly convex, after taking the trace of $(9.5.5)$ , how to get $$
\Delta_M(\Psi\circ E)(p)\ge d\Psi_p(E(p))(\Delta_A E(p))  ~~~?  \tag{9.5.6}
$$ What I try: First, the weak convex means $$
\text{Hess}(\Psi_p)(E(p)) (AE(p), AE(p))  \ge 0   \tag{1}
$$ but I don't know how to deal $$
\text{tr } d\Psi_p(E(p))R_A(\cdot,\cdot)E(p))   \ge 0   \tag{2}
$$ I guess, the convex on manifold is not same with linear space. Maybe the weak convex on manifold mean $(1)$ and $(2)$ . But I can't find the definition of convex on Riemannian manifold. PS(2022-6-25):  After some thinking, I think it is irrelevant with convex on manifold, since $W_p$ is vector space.  Besides, by maltreat the Weitzenbock formula, I give a rough process : Weitzenbock formula: assuming $\omega=\omega_i dx^i$ and $\overline\omega$ is the dual vector of $\omega$ , then $$
\Delta \omega (Y) = tr \nabla^2 \omega(Y) - R(\overline\omega,Y)
$$ Roughly, (or similarly), I have $$
(\Delta_A E)(\omega) = (tr A^2 E)\omega - g^{ij} R(e_i,\omega)E(e_j) 
$$ By Bianchi, I have $$
R(e_i, \omega) E(e_j) + R(\omega, E)e_i(e_j) + R(E,e_i)\omega(e_j)=0
$$ Therefore,  (treat $R(e_i, \omega) E(e_j)$ as $\langle R(e_i, \omega) E, e_j \rangle $ ) $$
g^{ij}R(e_i,\omega) E(e_j) = -\frac{1}{2}g^{ij} R(e_i, e_j)E(\omega)
$$ At last, I get $$
\Delta_A E = tr A^2 E+ \frac{1}{2} tr R(\cdot, \cdot)E
$$ So, the $(2)$ is redundant. And since it is ordinary convex, $(1)$ must be right.  Therefore, we can get $(9.5.6)$ . But the process  is very rough. Who can rigorous it? Thanks.","['ricci-flow', 'riemannian-geometry', 'differential-geometry']"
4478644,Find all function satisfying $f(f(n))+f(n)=2n+3k$,"Find all functions $f:\mathbb{N_{0}} \rightarrow \mathbb{N_{0}}$ satisfying the equation $f(f(n))+f(n)=2n+3k,$ for all & $n\in \mathbb{N_{0}}$, where $k$ is a fixed natural number. A friend of mine suggested me to use recurrence relation. While it is unfortunate that I have problems understanding recurrence relation. Please refer information to understand difference equation, homogeneous equation & auxillary equation. Thanks in advance!
Kindly don't down vote for not showing my attempt. I am seriously not able to understand the question and the process suggested to use.","['functional-equations', 'functions', 'recurrence-relations']"
4478659,Closed form for $\sum_{k=1}^\infty\frac{H_k^{(m)}}{k^n}$,"Let's define $$\sigma(m,n)=\sum_{k=1}^\infty\frac{H_k^{(m)}}{k^n}$$ where $H_k^{(m)}=\sum_{n=1}^{k}\frac{1}{n^m}$ is the k-th generalized harmonic number of order $m$ . In mathworld site Eq (20), I found $$\sigma(m\text{ even},n\text{ odd})=\frac12\left[\binom{m+n}{m}+1\right]\zeta(m+n)+\zeta(m)\zeta(n)$$ $$-\sum_{j=1}^{m+n}\left[\binom{2j-2}{m-1}+\binom{2j-2}{n-1}\right]\zeta(2j-1)\zeta(m+n-2j+1)\label{1}\tag{1}$$ and $$\sigma(m\text{ odd},n\text{ even})=-\frac12\left[\binom{m+n}{m}+1\right]\zeta(m+n)$$ $$+\sum_{j=1}^{m+n}\left[\binom{2j-2}{m-1}+\binom{2j-2}{n-1}\right]\zeta(2j-1)\zeta(m+n-2j+1)\label{2}\tag{2}$$ I know that \eqref{2} follows from \eqref{1} by using the well-known identity $$\sum_{k=1}^\infty \frac{H_k^{(m)}}{k^n}+\sum_{k=1}^\infty \frac{H_k^{(n)}}{k^m}=\zeta(m)\zeta(n)+\zeta(m+n)$$ The proof of \eqref{1} may be found here but I am looking for different ones if possible. Thanks Update (Nov 2 2023) In this preprint , we proved the following generalizations for integers $p$ and $q$ : \begin{multline*}
i)\sum_{n=1}^\infty\frac{H_n^{(p)}}{n^q}=\frac12\zeta(p+q)-(-1)^q\sum_{k=0}^{\lfloor{\frac{q-1}{2}}\rfloor}\binom{p+q-2k-1}{p-1}\zeta(2k)\zeta(p+q-2k)\\
-(-1)^q\sum_{k=0}^{\lfloor{\frac{p}{2}}\rfloor}\binom{p+q-2k-1}{q-1}\zeta(2k)\zeta(p+q-2k);
\end{multline*} \begin{multline*}
 ii)\sum_{n=1}^\infty\frac{(-1)^nH_n^{(p)}}{n^q}=-\frac12\eta(p+q)+(-1)^q\sum_{k=0}^{\lfloor{\frac{q-1}{2}}\rfloor}\binom{p+q-2k-1}{p-1}\eta(2k)\zeta(p+q-2k)\\
-(-1)^q\sum_{k=0}^{\lfloor{\frac{p}{2}}\rfloor}\binom{p+q-2k-1}{q-1}\eta(2k)\eta(p+q-2k);
\end{multline*} \begin{multline*}
iii)\sum_{n=1}^\infty\frac{\overline{H}_n^{(p)}}{n^q}=\frac12\eta(p+q)+\frac12(1+(-1)^{q})\zeta(q)\eta(p)\\
+(-1)^q\sum_{k=0}^{\lfloor{\frac{q}{2}}\rfloor}\binom{p+q-2k-1}{p-1}\eta(2k)\eta(p+q-2k)\\
-(-1)^q\sum_{k=0}^{\lfloor{\frac{p}{2}}\rfloor}\binom{p+q-2k-1}{q-1}\eta(2k)\zeta(p+q-2k);
\end{multline*} \begin{multline*}iv)\sum_{n=1}^\infty\frac{(-1)^n \overline{H}_n^{(p)}}{n^q}=-\frac12\zeta(p+q)-\frac12(1+(-1)^q)\eta(q)\eta(p)\\
-(-1)^q\sum_{k=0}^{\lfloor{\frac{q}{2}}\rfloor}\binom{p+q-2k-1}{p-1}\zeta(2k)\eta(p+q-2k)\\
-(-1)^q\sum_{k=0}^{\lfloor{\frac{p}{2}}\rfloor}\binom{p+q-2k-1}{q-1}\zeta(2k)\eta(p+q-2k);
\end{multline*} \begin{multline*}
v)\sum_{n=1}^\infty\frac{H_n^{(p)}}{(2n+1)^q}=-(1+(-1)^q)2^{p-1}\lambda(q)\eta(p)\\
-(-1)^q2^{p}\sum_{k=0}^{\lfloor{\frac{q-1}{2}}\rfloor}\binom{p+q-2k-1}{p-1}\lambda(2k)\lambda(p+q-2k)\\
-(-1)^q2^{p}\sum_{k=0}^{\lfloor{\frac{p}{2}}\rfloor}\binom{p+q-2k-1}{q-1}2^{-2k}\zeta(2k)\lambda(p+q-2k);
\end{multline*} \begin{multline*}
vi)\sum_{n=1}^\infty\frac{\overline{H}_n^{(p)}}{(2n+1)^q}=\frac12(1+(-1)^q)\lambda(q)\eta(p)\\
+(-1)^q2^{p-1}\sum_{k=0}^{\lfloor{\frac{q-1}{2}}\rfloor}\binom{p+q-2k-2}{p-1}\frac{|E_{2k}|}{(2k)!}\left(\frac{\pi}{2}\right)^{2k+1}\beta(p+q-2k-1)\\
-(-1)^q2^{p}\sum_{k=0}^{\lfloor{\frac{p}{2}}\rfloor}\binom{p+q-2k-1}{q-1}2^{-2k}\eta(2k)\lambda(p+q-2k);
\end{multline*} \begin{multline*}
vii)\sum_{n=1}^\infty \frac{(-1)^{n}H_n^{(p)}}{(2n+1)^q}=-(1-(-1)^q)2^{p-2}\frac{|E_{q-1}|}{(q-1)!}\left(\frac{\pi}{2}\right)^{q}\eta(p)\\
+(-1)^q2^{p-1}\sum_{k=0}^{\lfloor{\frac{q-2}{2}}\rfloor}\binom{p+q-2k-2}{p-1}\frac{|E_{2k}|}{(2k)!}\left(\frac{\pi}{2}\right)^{2k+1}\lambda(p+q-2k-1)\\
-(-1)^q2^{p}\sum_{k=0}^{\lfloor{\frac{p}{2}}\rfloor}\binom{p+q-2k-1}{q-1}2^{-2k}\eta(2k)\beta(p+q-2k);
\end{multline*} \begin{multline*}
viii)\sum_{n=1}^\infty\frac{(-1)^n\overline{H}_n^{(p)}}{(2n+1)^q}=\frac{1-(-1)^q}{4(q-1)!}|E_{q-1}|\left(\frac{\pi}{2}\right)^q\eta(p)\\
-(-1)^q2^{p}\sum_{k=0}^{\lfloor{\frac{q}{2}}\rfloor}\binom{p+q-2k-1}{p-1}\lambda(2k)\beta(p+q-2k)\\
-(-1)^q2^{p}\sum_{k=0}^{\lfloor{\frac{p}{2}}\rfloor}\binom{p+q-2k-1}{q-1}2^{-2k}\zeta(2k)\beta(p+q-2k),
\end{multline*} where $\zeta(s)$ is the Riemann zeta function, $\eta(s)=(1-2^{1-s})\zeta(s)$ is the Dirichlet eta function, $\lambda(s)=(1-2^{-s})\zeta(s)$ is the Dirichlet lambda function, $\beta$ is the Dirichlet beta function, and $E$ is the Euler number. Note that $p\ge1$ and $q\ge2$ in the non-alternating sums, and $p,q\ge1$ in the alternating sums. We also have the sum weight $(p+q)$ is odd in the first six generalizations and even in the last two generalizations.","['reference-request', 'alternative-proof', 'harmonic-numbers', 'sequences-and-series', 'riemann-zeta']"
4478664,"What is the difference between Projective Geometric, Clifford Algebra, Grassman Algebra, Geometric Algebra, Quaternion Algebra and Exterior Algebra?","Since a few years now, Special Interest Group on Computer Graphics have been shilling this new type of algebra that they advertise fixes all the problem with Linear Algebra like no Gimbal locks, error free transformations, co-ordinate free representation of primitives and robust collision detection as well as trivially generalizing to higher dimensions, among other things. All of these seems too good to be true and it feels like I am being sold on some mathematical cult. Nevertheless, this got me interested and everytime I try to read the literature, I get all confused by these algebras: Projective Geometric Algebra PGA, Clifford Algebra, Grassman Algebra, Geometric Algebra, Exterior Algebra, Quaternion Algebra I maybe wrong but I observe: Quaternion algebra feels like the odd one out (but everyone mentions it). All other algebras support the wedge product. Some algebra merge dot and wedge into one operations and some don't. Some algebra is fixed only to three (or four homogeneous) dimensions. Clifford seems like a superset of all except Quaternion. Exterior and Grassman seems to be the same thing but it doesn't merge dot and wedge together. Geometric Algebra does seem merge them together and PGA is like Geometric but in 3D. For those who know, please tell what is the difference between all these algebras and a little history and chronology (if that is too much to ask, a link to appropriate resource will be appreciated) and which algebra should I pick to study (and optionally suggest a good book) if my interest is mostly computer graphics and I want to overcome the limitations that vanilla Linear Algebra has. I don't want a deeper understanding of Spinors or Minkowski's space time or condensing Maxwell 4 EM equations into one.","['geometric-algebras', 'clifford-algebras', 'linear-algebra', 'exterior-algebra']"
4478672,Independence of diagnostic tests,"A rare disease afflicts only $1\%$ of the population. Fred gets tested for it with two diagnostic tests. The sensitivity and specificity of the second test are both $0.95$ (so, its overall accuracy is $0.95$ ), and knowing that Fred has the disease makes the second test independent of the first. Let $D$ be the event that Fred has disease, $T_1$ be the event that his first result is positive and $T_2$ be the event that his second result is positive. I would like to know if I am interpreting each of the following correctly: $P(T_1 \cap T_2 | D)$ : Probability that the results are positive in both the tests given Fred has the disease. $P(T_1 \cap T_2)$ : Probability that the results are positive in both the tests taking into account both the cases - when Fred has the disease and when not. Does this mean that $P(T_1 \cap T_2) = P(T_1 \cap T_2 | D) P(D) + P(T_1 \cap T_2 | D^c) P(D^c)$ ? Is it true that $P(T_1 \cap T_2 | D) = P(T_1 | D) P(T_2 | D)$ ? Is it true that $P(T_1 \cap T_2) = P(T_1)P(T_2)$ ? My computations show that $(3)$ holds while $(4)$ does not, but I can't intuit why; an explanation using (conditional) independence would be helpful.","['conditional-probability', 'independence', 'probability']"
4478674,Evaluate the indefinite integral $\int\frac{dx}{(x^2+1)\sqrt{x^2+1}}$ without trigonometric substitution.,"In order to find $$
\int\frac{dx}{(x^2+1)\sqrt{x^2+1}}
$$ we set $t=\arctan x$ . Then $x=\tan t$ and $dt=\frac{dx}{x^2+1}$ , so $$
\int\frac{dx}{(x^2+1)\sqrt{x^2+1}}=\int\frac{dt}{\sqrt{\frac{1}{\cos^2t}}}=\int\cos tdt\\
=\sin t+C=\sin(\arctan x)+C
$$ Now, since $$
\sin(\arctan x)=\sqrt{\frac{\tan^2(\arctan x)}{\tan^2(\arctan x)+1}}
$$ the answer is $\frac{x}{\sqrt{x^2+1}}+C$ . My Question : Is there another way to find this integral without using trigonometry?","['integration', 'indefinite-integrals', 'calculus']"
4478779,Extracting coefficients of a series counting directed animals of the lattice,"For a fixed integer $k$ , consider the function : $$f(z)= \frac{1}{2} \left(\sqrt{\frac{1+z}{1-3z}}-1 \right) \left(\frac{1-z-\sqrt{(1+z)(1-3z)}}{2z} \right)^k$$ I'd like to extract the $n$ -th coefficient ; $$a_n = [z^n] (f)$$ of $f(z)= \sum_{n \geq 0} a_n z^n$ , more precisely I am interested in an equivalent for $a_n$ , not the exact formula; I presume the asymptotic will take the form: $$a_n \sim \alpha_k \cdot 3^n, \qquad \text{ as } n \to \infty$$ where $\alpha_k$ depends on $k$ only. Context: $a_n$ counts the number of directed animals of the lattice $\mathbb Z^2$ with $n$ vertices and  a ""compact source"" of size $k$ , see Flajolet and Sedgewick, Analytic combinatorics, Example 1.18.","['complex-analysis', 'combinatorics']"
4478791,Nonnegative random variable whose characteristic function is differentiable at 0 has first moment,"Iâm trying to prove the following claim: Let $X$ be a nonnegative random variable, and let $\phi(t) = \mathbb E\left[e^{itX}\right]$ be its characteristic function. Suppose $\phi$ is differentiable at $t=0$ . Then $\mathbb E[X] < \infty$ . This is a component of an exercise in my probability theory textbook (Achim Klenke, âProbability Theory: A Comprehensive Courseâ, Exercise 15.4.4(iii)). This question has also been asked here , and thereâs an answer that supposedly gives a counterexample, but Iâm not convinced: the counterexample is a symmetric random variable instead of nonnegative, and itâs not clear to me how the characteristic function of a a symmetric random variable relates to the corresponding nonnegative random variable (i.e. its absolute value). What Iâve tried: Klenke has a proof of the existence of $2n^\textrm{th}$ moments when $\phi^{(2n)}(0)$ exists for some $n \geq 1$ , but Iâve been having trouble adapting the proof to the first derivative case. I tried considering a symmetric random variable $Y$ for which $Y^2 = X$ (and thus $\mathbb E[Y^2] = \mathbb E[X]$ ). If I were able to prove $\phi_Yââ(0)$ exists, then Iâd be done. But letting $f(x) = x^2$ , and noting that $f_* \mathbb P_Y = \mathbb P_X$ , I ended up computing: $$
\phi_Y(t) = \int_{\mathbb R} e^{ity} \mathbb P_Y[dy] = \mathbb P[Y=0] + 2\int_{(0,\infty)} \cos\left(t\sqrt x\right) \mathbb P_X[dx]
$$ and itâs not at all obvious to me that this map should be differentiable (let alone twice differentiable). Any suggestions/places to find the answer?","['characteristic-functions', 'probability-theory', 'probability']"
4478825,"Ways to show $\int_0^{\infty}\frac{\sin^2(\pi x)}{x^2}\Big\lvert x-\Big\lfloor x +\frac12 \Big\rfloor \Big\rvert \, \mathrm{d}x = \frac{\pi^2}{8}$?","Whilst reading about Lobachevsky's integral formula I tried constructing some interesting integrals which could be evaluated with said formula. One result I found was $$\int\limits_{0}^{\infty} \frac{\sin^2(\pi x)}{x^2}\left\lvert x - \left\lfloor x + \frac12 \right\rfloor \right\rvert \, \mathrm{d}x = \frac{\pi^2}{8} $$ which under substitution $ u = \pi x$ can  be evaluated with Lobachevsky's formula since $\int_0^\frac\pi2 \Big\lvert \frac{x}{\pi} - \Big\lfloor \frac{x}{\pi} + \frac12 \Big\rfloor \Big\rvert \, \mathrm{d}x =\int_0^\frac\pi2 \frac{x}{\pi}\, \mathrm{d}x =\frac{\pi}{8}$ . However, once I had shown this result I remembered that $\frac{\pi^2}{8}$ has the very nice series representation $$
\sum_{n\ge0} \frac{1}{(2n+1)^2} =  \frac{\pi^2}{8} \tag{1}
$$ So my question is Is there a way to evaluate this integral using $(1)$ ? Or is it just a coincidence that the values match?","['integration', 'definite-integrals', 'alternative-proof', 'calculus', 'sequences-and-series']"
4478826,"Find the integer solution: $a+b+c=3d$, $\: a^{2} + b^{2} + c^{2}= 4d^{2}-2d+1$","Find  the integer solutions: $$a+b+c=3d$$ $$a^{2} + b^{2} + c^{2}= 4d^{2}-2d+1$$ Attempt: Notice that $a=b=c=d=1$ is a solution. Other facts: Notice that $a^{2} + b^{2} + c^{2} > 0$ , so $4d^{2}-2d+1>0$ . Notice that $4d^{2}-2d+1$ has negative discriminant: $D = -12$ , and so it is always one sign, which is positive in this case. So we cannot narrow $d$ -solution this way. Next, since $(a+b+c)^{2} = a^{2} + b^{2} + c^{2} + 2(ab+ac+bc)$ , we get $$ 9d^{2} - 2(ab+ac+bc) = 4d^{2}-2d+1  $$ $$ 5d^{2} + 2d -1 = 2(ab+ac+bc) $$ $5d^{2} + 2d - 1 = 0$ has solutions: $$ d_{1,2} = \frac{-2 \pm \sqrt{24}}{10} = \frac{-1 \pm \sqrt{6}}{5}$$ and when $d > (-1 + \sqrt{6})/5$ , OR, $d < (-1 - \sqrt{6})/5$ we have $5d^{2}+2d-1 > 0$ . And when $d$ is between the 2 roots we have $5d^{2}+2d-1<0$ . Then also nocitce that $(a+b+c)d= 3d^{2}$ , and so $$a^{2}+b^{2}+c^{2}-(a+b+c)d = (d-1)^{2}$$ Now if $d \ne 1$ we must have $$ a^{2}+b^{2}+c^{2}-(a+b+c)d > 0$$ Another fact: $$d = \frac{a+b+c}{3} \ge (abc)^{1/3} $$ by AM-GM. Also notice $d < \max(a,b,c)$ , because $$a^{2}+b^{2}+c^{2} =d^{2} + d^{2} + d^{2} + (d-1)^{2}$$","['contest-math', 'systems-of-equations', 'nonlinear-system', 'algebra-precalculus', 'quadratic-forms']"
4478841,Existence of probability density on $S^1$ with first circular moment on $S^1$,"I am searching for a function $f$ on the unit circle $S^1$ with the following properties. $f \geq 0$ almost everywhere. $\int_{S^1} f = 1$ . $\int_{S^1} e^{i \theta} f (\theta) d \theta = 1$ . In other words, I would like a probability density $f$ on the unit circle whose first circular moment also lies on the unit circle. Using a little Fourier series, the conditions 2 and 3 above are equivalent to saying $f$ has the form $$ f(\theta) = \frac{1}{2 \pi} \left( 1 + 2 \cos{\theta} \right) + \sum_{n \in \mathbb{Z} \backslash \{ \pm 1 , 0 \}} a_n e^{in\theta} \tag{*} $$ for some coefficients $a_n \in \mathbb{C}$ . The question then becomes whether it is possible to choose the $a_n$ so that $f$ is non-negative. My intuition tells me, if such a function exists, then it would be rather pathological. Question: Is some able to demonstrate the existence/non-existence of such a function? If such a function exists and is ""nice"" (e.g. smooth), is a closed-form possible? Context: I've been studying some directional statistics and I've noticed that many of the wrapped distributions have the property that their circular moments do not lie on the unit circle. For example, the wrapped normal distribution has circular moments $\langle e^{in\theta} \rangle = e^{in\mu - n^2 \sigma^2 / 2}$ , where $\mu$ and $\sigma$ are the mean and standard deviation of the unwrapped distribution. Clearly, this lies on $S^1$ only if $\sigma = 0$ , in which case the wrapped normal becomes a Dirac comb distribution. Part of me feels like this behavior is reasonable: depending on where the distribution is concentrated, it will ""pull"" the unit vector $e^{in\theta}$ toward the concentration center (in this case $\mu$ ), but not ""all the way"". For a uniform distribution, e.g. $\sigma \rightarrow + \infty$ , the circular moments all lie at the origin (i.e., they all get ""pulled"" equally in all directions). I started to wonder if this is always the case for probability densities on the unit circle. Or could I, at the very least, make the first moment lie on the unit circle?","['probability-distributions', 'real-analysis', 'fourier-series', 'probability-theory', 'probability']"
4478857,"$E \subseteq \mathbb{R}^n$ is compact $\iff f(E)$ is compact, then f is continuous","Let $f:R^n \to R^m$ such that for every $E \subseteq \mathbb{R}^n, E$ is compact $\iff f(E)$ is compact, then f is continuous. I saw this proof on MSE: ""Suppose $f$ is not continuous at $x$ . Then there exists $r>0$ and $x_n \to x$ such that $|f(x_n)-f(x)| \geq r$ for all $n$ . Now, Since the graph is compact, $(x_n,f(x_n))$ has a subsequence $(x_{n_k}, f(x_{n_k}))$ converging to some point $(u,f(u))$ on the graph. This implies $x_n\to u$ so we must have $u=x$ and $f(x_{n_k}) \to f(u)=f(x)$ . But $|f(x_{n_k})-f(x)| \geq r$ for all $k$ and we have arrived at a contradiction."" But does this proof hold for my claim? This proof does not use the iff assumption, just one direction rather: $E$ is compact $\implies f(E)$ is compact. However, we can find a counterexample for the following claim: For every $E \subseteq \mathbb{R}^n,$ the set $f(E) \subseteq \mathbb{R}^m$ is also compact (this time not iff) then f is continuous. Indeed, consider the function $f: \mathbb{R}\rightarrow \mathbb{R}$ defined by $f(x)=1$ if $x \in \mathbb{Q}$ and $f(x)=0$ otherwise. Then $f$ is nowhere continuous, but the image of each compact subset $K$ of $\mathbb{R}$ is compact as finite sets are compact. I am a bit confused. Any help would be greatly appreciated.","['metric-spaces', 'continuity', 'calculus', 'general-topology', 'compactness']"
4478892,"Calculus: Difference between functions and ""equations"" from a theoretical perspective","I've been studying Multivariable Calculus for a while; but I still don't quite know the difference between $f(x,y) = x^2 + y^2$ and $x^2 + y^2 = 9$ .
I know that the former graphs a paraboloid, while the latter a cylinder. But what's the difference (or 'theoretical difference' if you will) between having a function and having an equation . Do we assume that the function maps to z-axis because of convention? Doesn't the equation map it's point to the z-axis as well? I sometimes feel like this fact was skipped or even 'ignored' by the teachers. Thanks in advance","['notation', 'multivariable-calculus', 'calculus', 'functions', 'terminology']"
4478931,Solving the ODE $(x^2 y-1)y'-(xy^2-1)=0$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I need to solve $(x^2 y-1)y'-(x y^2-1)=0$ . Not sure how to approach this ODE, would love some help regarding solving it or any useful resources.","['calculus', 'ordinary-differential-equations']"
4478997,Formal group of Abelian Varieties,"I am reading Barry Mazur and Tom Weston's note Euler Systems and Arithmetic Geometry ([here][1] is the link). I have a question about the following fact in section 2.2 page 87: Let $K$ be a local field with the ring of integers $\mathcal O$ with the maximal ideal $\mathfrak m$ and the residue field $k$ , and let $A$ be an abelian variety over $K$ . Then the kernel $A_1$ of the reduction map $A(\mathcal O)\to A(k)$ can be identified with the $\mathfrak m-$ valued points of some formal group. Here, I guess this formal group is defined by the completion of $\mathcal A$ (the Neron model of $A$ ) along $\mathfrak m$ at the identity element, similarly to the elliptic curve case (you can see more detail about this definition [here][2], Andrei Jorza's thesis, section 2.6.1). Is it true? When $A$ is an elliptic curve, this result is proved in Joseph Silverman's book, The arithmetic of elliptic curves, Chapter VII. Proposition 2.2. However, it seems that this proof can not handle the general case because abelian varieties can not be described by explicit equations. Can anyone provide me some hints with any reference to this fact? Edit: It is Theorem C.2.6 in Diophantine Geometry, M. Hindry and J. Silverman. The construction of formal groups and the proof of the theorem are quite similar to the elliptic curve case.
[1]: https://people.math.umass.edu/~weston/cn/mazur.pdf [2]: https://wstein.org/projects/jorza.pdf","['number-theory', 'formal-groups', 'elliptic-curves']"
4479000,Only commutators solve this functional equation,"I need to prove that the only linear transformations $f\colon  M_{n\times n}\rightarrow M_{n\times n}$ that solve the functional equation in $\mathcal{L}(M_{n\times n})$ $$f(XY)=f(X)Y+Xf(Y)$$ are the commutators. That is, I have to show that for every linear solution $f$ there exists a matrix $A$ such that $f(X)=[A,X]$ for all $X$ . Bearing in mind that every commutator satisfies the equation, I have tried to solve this studying the sets $V_A=\{X\in M_{n \times n}\colon f(X)=[A,X]\}$ , where $A\in \ker f$ , which happen to be subalgebras that are stable under $f$ and include inverses. Specifically, I have been looking for a maximality argument that leads me to conclude that some of these $V_A$ must be all of $M_{n\times n}$ . So far, I have not been able to exploit the finite dimension of $M_{n\times n}$ . This problem appears as an exercise at the end of an introductory chapter on matrices and finite dimensional vector spaces in Katsumi Nomizu's Fundamentals of Linear Algebra , so little else beyond the rank-nullity theorem is assumed.","['functional-equations', 'linear-algebra']"
4479099,Show that a random variable uniformly distributed over the unit square is independent,"Let $X=(U,V)$ be a random variable which is uniformly distributed on $[0,1]^2$ . I want to show that $U$ is independent of $V$ . Let $A,B\in B([0,1])$ where $B([0,1])$ denotes the Borel Sigma algebra. We have, $$\begin{aligned}P(U\in A, V\in B) &= P((U,V)\in A\times B) \\&=1_{[0,1]^2}(A\times B) \\ &=1_{[0,1]}(A) \cdot 1_{[0,1]}(B) \\ &=P(U\in A)P(V\in B) \end{aligned}$$ Is my solution correct? What could I have done better?","['solution-verification', 'probability-theory', 'probability']"
4479131,Maximum amount of iterations to get to an empty list from repeatedly taking the smallest complement of a list,"I was trying to solve Iterative Smallest Complement on the Code Golf website, and thought of an interesting question. A basic run down of what the smallest complement of a list is: ... B is a complement of A if: B has all of the integers between the minimum and maximum of A which are missing from A , and B has none of the integers present in A So basically, you make the list $[\min(A)\ldots\max(A)]\backslash A$ , where $\backslash$ represents set difference. Now given a list of non-repeating numbers $A$ , repeatedly find the smallest complement until you obtain the empty set, and record how many iterations it took. Iterations include the initial list $A$ and the empty set: $\{\}$ . For example, given $A=[1,3,4,5,8]$ , the iterations are as follows: $[1,3,4,5,8]$ $[2,6,7]$ $[3,4,5]$ $[]$ So, $4$ iterations when $A=[1,3,4,5,8]$ . Now I was wondering: For any given list $L$ which includes no repeating numbers, what is the highest amount of iterations possible? I'm looking for some kind of expression in terms of $L$ (like its length, or maximum value, etc.). I'm also wondering if there is some kind of algorithm which can generate a list with the most amount of iterations.","['elementary-set-theory', 'recreational-mathematics']"
4479133,Is the derived category of coherent sheaves idempotent closed?,"I am wondering whether the bounded derived category $D^b Coh(X)$ is idempotent complete, i.e. whether every idempotent morphism splits. This is true for $Perf(X)$ since perfect complexes are the compact objects in the category of quasi-coherent sheaves and compactness is preserved under taking retracts. For example, if X is smooth, then $D^bCoh$ and $Perf$ agree and so $D^b Coh$ is also idempotent closed. Question : Are there any general conditions on a scheme X that imply that $D^b Coh$ is also idempotent closed?","['derived-categories', 'coherent-sheaves', 'algebraic-geometry', 'category-theory']"
4479185,How do we know that mathematics is independent of the definition of the cartesian product?,"In my first analysis lecture, I learnt what might be the most common way to rigorously deal with functions: Firstly, ordered pairs were defined and then a function was defined as a triple $(X,Y,G)$ with $G\subset X\times Y$ . Since there are different ""good"" ways to define ordered pairs $^1$ , I am wondering: How do we know that the results we derive do not depend on the definition of $X\times Y$ ? $^1$ For example, see this question and links therein.","['functions', 'logic', 'set-theory']"
4479227,Find an upper bound for a series,"How can I show that the following is true? $$\sum_{k=1}^{\infty}{\frac{1}{k\sqrt{k}}} < 3$$ I just need this series to be between 1 and 3 so that I can conclude some result. I have found the appropriate lower bound series $$\sum_{k=1}^{\infty}{\frac{1}{k^2}} =\frac{\pi^2}{6}\leq\sum_{k=1}^{\infty}{\frac{1}{k\sqrt{k}}},$$ but am struggling to find an upper one. Could somebody lend me a hand, please?","['upper-lower-bounds', 'sequences-and-series']"
4479290,$F$ such that $F(A_{0}B_{0}+A_{1}B_{1})=F(A_{0})+F(B_{0})+F(A_{1})+F(B_{1})$?,We know that the only way of turning a multiplication into addition is by a logarithmic function; so if $$C=A_{0}B_{0}$$ then we know that: $$\log(C)=\log(A_{0})+\log(B_{0})$$ but can we extend this idea? . In the case of: $$C=A_{0}B_{0}+A_{1}B_{1}$$ can we prove or disprove the existing of a function F(x) such as: $$F(C)=F(A_{0})+F(B_{0})+F(A_{1})+F(B_{1})$$,"['functional-equations', 'functions', 'logarithms']"
4479299,"Distribution of areas in regular $n$-gon with diagonals, as $n\to\infty$","Consider a regular $n$ -gon with all diagonals drawn. Here is an example with $n=10$ . What is the distribution of the areas of the regions, as $n\to\infty$ ? That is, if we write down each region's area, how are those numbers distributed? What does the density function look like? The number of regions is approximately $\dfrac{1}{24}n^4$ for large $n$ . Here is an exact formula for the number of regions. Context: I've been thinking about regular $n$ -gons with connected vertices, and this question naturally arose. UPDATE I found a nice geogebra applet that allows you to play with various regular polygons with all diagonals drawn. I would not be surprised if the answer to my question is currently unknown, because the following related question, which seems simpler than my question, is an open question : If a regular n -gon has all its diagonal drawn, does there exist a bound on the maximum number of sides of the sub-polygons thus formed (excluding the central sub-polygon when n is odd)? UPDATE2 This question has inspired another question: Regular polygon of radius $1$ with diagonals: mysterious ring of radius $1/e$ ?","['area', 'probability-distributions', 'geometry', 'polygons', 'limits']"
4479303,"Given a group number and an potentially infinite number of groups, how would you calculate the predicted total number of groups?","Sorry for the confusing title, Iâll elaborate with an example. You have been assigned group #4, and you are curious how many groups in total there are. Assuming groups are numbered sequentially starting at 1 and group assignment is random, you can assume there are at least 4 groups. So far, I know that the most probable number of groups is your group number. You are more likely to be in group 4 if there are 4 groups, than if there were 1000. There is some kind of decay model where the larger the guess, the less probable it is, but I canât figure it out. Whatever the probability model is, the area under it should equal 1.","['word-problem', 'statistics', 'probability-distributions', 'probability']"
4479311,Difficulty with verifying Stokes' theorem for a given domain and a specific differential $(n-1)$-form,"Let $f:\Bbb R^{n-1}\to\Bbb R$ be an everywhere positive $C^\infty$ function and let $U$ be a bounded open subset of $\Bbb R^{n-1}.$ Verify directly that Stokes' theorem is true if $D$ is the domain defined by $$0<x_n<f(x_1,\ldots,x_{n-1}),\quad (x_1,\ldots,x_{n-1})\in U$$ and $\mu$ an $(n-1)$ -form $$\varphi(x_1,\ldots,x_n)dx_1\wedge\ldots\wedge dx_{n-1},$$ where $\varphi\in C^\infty(\Bbb R^n;\Bbb R).$ Source: Differential Forms, page $132$ My attempt: First, $\begin{aligned}d\mu(x_1,\ldots,x_n)&=d\varphi(x_1,\ldots,x_n) dx_1\wedge\ldots\wedge dx_{n-1}\\&=\partial_n\varphi(x_1,\ldots,x_n)dx_n\wedge dx_1\wedge\ldots\wedge dx_{n-1}.\end{aligned}$ I need to verify that $$\int_Dd\mu=\int_{\partial D}\mu.$$ Since $\partial D$ is of Jordan-measure $0,$ I integrated over the closure $\overline D$ and applied Fubini's theorem: $$\begin{aligned}\int_Dd\mu&=\int_{\overline D}d\mu\\&=\int_{\overline U}\int_0^{f(x_1,\ldots,x_{n-1})}\partial_n\varphi(x_1\ldots,x_n)dx_ndx_1\ldots dx_{n-1}\\&=\int_{\overline U}(\varphi(x_1,\ldots,x_{n-1},f(x_1,\ldots,x_{n-1}))-\varphi(x_1,\ldots,x_{n-1},0))dx_1\ldots dx_{n-1}.\boldsymbol{(1)}\end{aligned}$$ On the other hand, (I believe) $$\begin{aligned}&\color{white}=\partial D\\&=\partial\{(x,y)\in\Bbb R^{n-1}\times\Bbb R\mid x\in U, 0<y<f(x)\}\\&=\overline U\times f(\overline U)\cup\overline U\times\{0\}\cup V,\end{aligned}$$ where $$V=\partial U\times\{y\in\Bbb R\mid 0\le y\le f(x),x\in\partial U\},$$ but, I think we should be able to parametrize $V\subset\Bbb R^n$ in some coordinates with $n-1$ conditions so that one of the differentials $dx_1,\ldots,dx_{n-1}$ turns into $0$ when pulled-back in the integral of the differential $(n-1)$ -form $\mu,$ as in the proof of Stokes' theorem for the unit cube, where one coordinate on each edge is constant. However, I got a bit confused. Question: How should I proceed? If $\Phi_{\overline U\times\{0\}}$ and $\Phi_{\overline U\times f(\overline U)}$ are parametrizations of $\overline U\times\{0\}$ and $\overline U\times f(\overline U)$ respectively s. t. the normal to $\overline U\times\{0\}$ has a negative $x_n$ component, and the normal to $\overline U\times f(\overline U)$ has a positive $x_n$ component, I'm going to get $\boldsymbol{(1)}.$","['integration', 'multivariable-calculus', 'stokes-theorem', 'differential-forms']"
4479360,Weighted count of Egyptian fraction representations,"This question emerged during an activity I ran for some middle school students a couple weeks ago; basically, it's about a way to ""count"" - with an appropriate kind of weight - the Egyptian fraction representations of a given rational. Let $[\mathbb{N}]^{\mathit{fin}}$ denote the set of finite sets of natural numbers. Given a positive rational $q$ , let $\mathsf{E}(q)=\{X\in[\mathbb{N}]^{\mathit{fin}}: q=\sum_{x\in X}{1\over x}\}$ be the set of Egyptian fraction representations (without duplication!) of $q$ . For example, $\mathsf{E}(1)$ contains both $\{1\}$ and $\{2,3,6\}$ , as well as infinitely many other finite sets. I'm interested in the function $$\mathscr{E}: q\mapsto \sum_{X\in \mathsf{E}(q)} \left( \prod_{x\in X}{1\over x}\right).$$ Intuitively, I'm trying to count the Egyptian fraction representations of $q$ ; of course this is infinite regardless of what $q$ is (since ${1\over n}={1\over n+1}+{1\over n^2+n}$ ), but I'm tweaking things to pay more attention to ""short"" representations with ""small"" denominators. In general, $\mathscr{E}$ 's behavior seems rather complicated. I have an outline of an argument that $\mathscr{E}(q)$ is indeed finite for all $q$ , but I'm not entirely certain it's correct; even if it is correct, anything more than that is unclear to me, including its behavior on specific values. So my two questions are: Question 1 : What is $\mathscr{E}(1)$ ? Of course we can trivially work out some lower bounds, e.g. $\mathscr{E}(1)>{37\over 36}$ , but I don't see a way to approach a good exact understanding. Question 2 : Is $\mathscr{E}(q)$ in fact finite for all $q\in\mathbb{Q}_{>0}$ ? In general, I'm interested in any information about $\mathscr{E}$ , or indeed other ways to ""count"" Egyptian fraction representations. Most of the research I can find is about determining individual Egyptian fraction representations which are ""optimal"" in some sense (e.g. minimal length), which isn't exactly what I'm looking for. (I can't even find any information about the function sending $(q,n)$ to the number of Egyptian fraction representations of $q$ of length $n$ .)","['number-theory', 'recreational-mathematics', 'egyptian-fractions', 'sequences-and-series']"
4479363,Complex Analysis to solve this integral? $\int_0^{\pi/2} \frac{\ln(\sin(x))}{\sqrt{1 + \cos^2(x)}}\text{d}x$,"Complex Analysis time! I need some help in figuring out how to proceed to calculate this integral: $$\int_0^{\pi/2} \frac{\ln(\sin(x))}{\sqrt{1 + \cos^2(x)}}\text{d}x$$ I tried to apply what I have been studying in complex analysis, that is stepping into the complex plane. So $$\sin(x) \to z - 1/z$$ $$\cos(x) \to z + 1/z$$ Obtaining $$\int_{|z| = 1} \frac{\ln(z^2-1) - \ln(z)}{\sqrt{z^4 + 3z^2 + 1}} \frac{\text{d}z}{i}$$ I found out the poles, $$z_k = \pm \sqrt{\frac{-3 \pm \sqrt{5}}{2}}$$ But now I am confused: how to deal with the logarithms? Also, what when I have both imaginary and real poles? I am a rookie in complex analysis so please be patient...","['integration', 'cauchy-integral-formula', 'complex-analysis', 'residue-calculus', 'complex-integration']"
4479430,Is there a handy integrable test function with a polynomial decay and a compactly supported Fourier transform?,"From what I am aware of, let the function $\varphi$ be $$\varphi(x)=\frac{\cos(x)}{\pi^2-4x^2}.$$ One can easily show that $\varphi\in L^1$ and its tail has a polynomial decay $\varphi\sim x^{-2}$ . Its Fourier transform is $$\hat\varphi(s)=c\cos(\pi s/2),\quad s\in(-1,1)$$ with a proper constant $c$ depending on the convention of Fourier transform. Also it is easily seen that $\hat\varphi$ has a compact support. My question is, is there a generalisation of this test function such that $\varphi_k\in L^1$ , $\hat\varphi_k$ has a compact support, and $\varphi_k$ decays like $x^{-k}$ for a fixed $k$ ? The previous one is like $\varphi_2$ .","['fourier-analysis', 'harmonic-analysis', 'analysis', 'real-analysis', 'functional-analysis']"
4479450,Probability Question: Flipping coins to determine who pays for lunch at work (elimination game),"I am trying to figure out how to determine the probability of someone ""winning this game"" (having to pay for everyone's lunch that day. The game is played by everyone flipping a coin. Whoever is in the minority flip has to then continue flipping. i.e. if 6 people flip heads and 4 flip tails, the 4 who flipped tails have to then flip again until 1 person is left. When there are 2 people whoever hits heads first wins (a tie would be a reflip). How do I calculate the probability I would be the last person remaining? I can then figure out the cost of lunch and my EV based on it to see if it is worth flipping that day. Sorry if this doesn't make sense (software engineer on a trading desk; hence these work shenanigans haha). Feel free to ask any additional questions if I did not provide enough information.","['statistics', 'conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
4479472,Notation of functions that take other functions as arguments,"With the usual function notation, one denotes functions as $f: A \to B$ , where $A$ and $B$ are sets (see here ), e.g. $f: \mathbb{R} \to \mathbb{R}$ for $f(x) = x^2$ . My question is: What would the notation be if a function $f$ takes another function $g: C \to D$ , where $C$ and $D$ are again sets, as its argument? Would it be $\require{enclose}
     \enclose{horizontalstrike}{f: D \to B}$ ? Or $\require{enclose}
     \enclose{horizontalstrike}{f: C \to B}$ ? Or something else? I am leaning towards the second option because set $\require{enclose}
     \enclose{horizontalstrike}D$ should match set $\require{enclose}
     \enclose{horizontalstrike}A$ as the output of function $\require{enclose}
     \enclose{horizontalstrike}g$ is the input to $\require{enclose}
     \enclose{horizontalstrike}f$ . I am not sure whether this is always the case and whether I am missing something, however. Edit: Matthew Towers is absolutely right, what I was wrongfully thinking of was composition of functions. The question, however, is targeted at a function that takes another function as input and I do not know what the notation would look like in that case.","['notation', 'functions']"
4479481,Hartshorne problem III.10.5,"Here is a problem I thought I solved but now I think it can't be right. The problem is as follows: Let $X$ be a scheme and $\mathcal{F}$ a coherent sheaf such that every $x\in X$ has an Ã©tale neighborhood $f:U\to X$ such that $f^*\mathcal{F}$ is free, then $\mathcal{F}$ is locally free on $X$ . Here is my attempt: Let $x\in X$ be any point, $f:U\to X$ an Ã©tale neighborhood, $y\in U$ and $f(y)=x$ . In particular $f$ is flat so $\mathcal{O}_x\to \mathcal{O}_y$ is flat. Since it is a local homomorphism of rings it is in fact faithfully flat (Matsumura, corollary to 4A). Now $\mathcal{F}_x\otimes_{\mathcal{O}_x}\mathcal{O}_y=(f^*\mathcal{F})_y$ is free by assumption. It then follows that $\mathcal{F}_x$ is free $\mathcal{O}_x$ module (Matsumura 4E). This approach is not really good because I need $X$ noetherian to conclude that $\mathcal{F}$ is locally free. Also, I'm not sure if the argument is even correct because only flatness of $U\to X$ is being used. If anyone can point out an error in my solution and/or give me a hint on how to attack this problem I would be grateful!","['algebraic-geometry', 'flatness', 'coherent-sheaves', 'sheaf-theory']"
4479485,How to calculate the bounding box of any Reuleaux triangle?,"How to calculate the bounding box of any Reuleaux triangle ? The Reuleaux triangle are given in the following form: [
    [
        (-13.705965094283357, -8.320529222222632),
        27.771461198696837,
        1.2608311697667869,
        61.260831169766824
    ],
    [
        (14.058772226517263, -7.70944934392086),
        27.771461198696837,
        121.2608311697668,
        181.2608311697668
    ],
    [
        (-0.3528071322338966, 16.029978566143498),
        27.771461198696837,
        241.26083116976682,
        301.2608311697668
    ]
] The above is a Python list of three sub-lists, each list describes an arc, the first element is a tuple , it is a coordinate of the arc's center, the second element is the radius of the arc, the third and fourth elements are the starting and ending degrees of the arc. The centers of the arcs are vertices of an equilateral triangle, all arcs have the same radius and span 60 degrees. (The center of the Reauleaux triangles are all at the origin) Basically, I want to remove the extra blank spaces in pictures like this by limiting the axes: To do that, I need to calculate the bounding box of any given Reuleaux triangle, but I don't know how to do that, and again, Google searching proved futile. I only know a very specific case, if the Reuleaux triangle is directly upwards (I don't know how to describe it in any natural language), like this: Then the left-most point is the left vertex, the right-most point is the right vertex, the top is the other vertex, and the lowest point is on the lowest arc halfway between the vertices, and the bounding box is a square. I know how to calculate the bounding box in this specific case, but I won't show the calculations here, for fear of over-cluttering the post. So how to calculate the bounding box of any Reuleaux triangle given the parameters above? I am trying to find the four coordinates of the square that is tangent to the Reuleaux triangle. For example, if the lowest side is parallel to the x axis, and the equilateral triangle has radius $r$ , then the three coordinates of the vertices are: $\begin{aligned}
(0&, r) \\
(- \frac{\sqrt{3} r} {2}&, - \frac{r} {2}) \\
(\frac{\sqrt{3} r} {2}&, - \frac{r} {2})
\end{aligned}$ And the lowest point is: $(0, r - \sqrt{3} r)$ Then the coordinates of the vertices of the bounding square is (counter-clockwise): $\begin{aligned}
(- \frac{\sqrt{3} r} {2}&, r - \sqrt{3} r) \\
(\frac{\sqrt{3} r} {2}&, r - \sqrt{3} r) \\
(\frac{\sqrt{3} r} {2}&, r) \\
(- \frac{\sqrt{3} r} {2}&, r)
\end{aligned}$ I already worked all these out before I have written the post, and that's how I made the second picture. I am asking, given the coordinates of the three vertices of a ROTATED Reuleaux triangle, how to calculate the four coordinates of the vertices of the bounding square of the triangle? The accepted method indeed does work.",['geometry']
4479495,Is $0$ the root of the equation $\frac{x^2}{ x}= 0$?,I want to understand if I understand the concept of equation correctly. For this I want to know if $0$ is the root of the equation $\frac{x^2}{x} = 0$ . If we simplify the equation then we can get equation $x = 0$ and then we have solution $0$ . But if we replace $x$ by $0$ in the original equation then we get expression $\frac{0^2}{0} = 0$ which doesn't make sense. I'm interested in your opinion.,['algebra-precalculus']
4479592,Prove spectral convergence $\lim\limits_{n\to\infty} \sigma(P_n H P_n) = \sigma(H)$,"Suppose we want to numerically solve the following Schrodinger equation on a bounded open set $\Omega \subset \mathbb R^n$ : $$Hu = (-\Delta + V(x)) u = \lambda u, \qquad \left.u\right\vert_{\partial \Omega} = 0,\qquad V \in L^2(\Omega,\mathbb R), \not \equiv 0$$ I will discretize the Hamiltonian as a finite-dimensional operator $H_N$ acting on an orthonormal basis $(\phi_j)_{j=1}^N$ (where $\phi_j$ are Dirichlet eigenfunctions of $-\Delta$ on $\Omega$ with eigenvalues $\mu_j$ ). This is called the expansion method . Now I have: $$H_{N_{nm}} = \langle \phi_n, H \phi_m\rangle = \mu_n\delta_{nm} + \langle \phi_n, V \phi_m\rangle$$ Alternatively, we can write this as $$H_N = P_N H ,\qquad P_N = \text{projection onto }\operatorname{span}(\phi_j)_{j=1}^N$$ Note that both $H$ and $H_N$ are self-adjoint. Furthermore, by convention we can define $H_N$ as an operator acting in $H^1_0(\Omega)$ by defining: $$H_N := P_N H P_N$$ Now, referencing to Mathematical Methods in Quantum Mechanics by G. Teschl, I am able to prove $H_N$ converges to $H$ in the strong resolvent sense, which in turn gives us: $$\lim_{N\to \infty} \sigma(H_N) \supseteq \sigma(H)$$ I am, however, a bit confused why it's so hard to show either $H_N \to H$ in the norm resolvent sense (which would give us equality in the spectral limit above), or go from the above spectral containment then show it is indeed an equality (perhaps by contradiction). Do I have reason to believe that: $$\lim_{N\to\infty} \sigma(H_N) = \sigma(H)$$ isn't necessarily true? If so, could I put additional requirements on $V$ in order for this to be true? I suppose the proof might be similar to the analog for a finite-difference method, but I couldn't find any literature on that either.","['spectral-theory', 'functional-analysis', 'partial-differential-equations']"
4479624,Categorical Koszul duality,"Consider a (small idempotent closed) dg categories C over K. I am interested in the weak dual $C^{\vee}=Fun^{ex}(C, Perf  K)$ and when the functor $C \rightarrow (C^{\vee})^{\vee}$ is an quasi-equivalence, which is a kind of categorical Koszul duality. This functor is an equivalence if C is a dualizable object in the symmetric monoidal category of (small idempotent closed) dg categories C over K, which is precisely when C is smooth and proper. It is also an equivalence if $C= D^b Coh(X)$ for a proper scheme $X$ by https://arxiv.org/abs/1312.7164 . Question: What are some other examples? Is there a general criterion for when this reflexitivity is true? References would be appreciated as well. I am aware of https://arxiv.org/abs/1409.5934 but this paper seems to study only the class of underived categories, where the situation is quite different as the authors point out.","['derived-categories', 'algebraic-geometry', 'duality-theorems', 'category-theory']"
