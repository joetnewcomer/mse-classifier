question_id,title,body,tags
4863216,An 8th Grade Geometry Problem [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question $△ABC$ is an isosceles right-angled triangle with $∠A = 90^\circ$ . $DE\parallel BC$ . Square $DFGH$ is constructed, with $F$ lying on $AC$ and $G$ on $BC$ . Prove that $∠EDF=∠EGF$ . I attempted to prove it using some equivalent methods, such as demonstrating the concyclicity of points DEFG or the perpendicularity of EG to BC, etc. However, I couldn't find a method to prove it, or I kept falling into the fallacy of circular reasoning. Appreciate any hints or assistance in solving the problem.",['geometry']
4863342,Intuition behind the component formula of the scalar product,"I have learned the component formula for the scalar product for quite a while, but really, it doesn't seem intuitive at all. I have 2 questions that confuse me while imagining what the scalar product really is, which are: 1- Why is the scalar product defined as $\vec{u} . \vec{v} = \sum u_{i}v_{i}$ (It doesn't make sense at all). 2- Does the concept of angles exist in dimensions higher than 3? In other words, can we always say that $\vec{u} . \vec{v} = ||\vec{u}|| \times ||\vec{v}|| \times \cos \theta$ ?","['inner-products', 'linear-algebra']"
4863343,Beurling's Theorem and Multipliers of $H^2(\mathbb{D})$,"I am currently studying a paper by Harold S. Shapiro on a generalized Beurling theorem on reproducing kernel hilbert spaces and I cannot seem to understand a certain detail. (Link: https://www.ams.org/journals/tran/1964-110-03/S0002-9947-1964-0159006-5/S0002-9947-1964-0159006-5.pdf ) In the introduction Shapiro states the condition of a subspace $S$ of $H^2(\mathbb{D})$ being invariant as "" $f\in S$ implies $\phi f \in S$ for all bounded analytic $\phi$ "". This differs from the usual definition I find in literature, which is "" $f\in S$ implies $z f\in S$ "". He argues that the two formulations are equivalent since polynomials span the bounded analytic functions in the topology of bounded pointwise convergence. Neither do I understand why the formulations are equivalent nor his reasoning. I know that the multiplier class for $H^2$ is exactly $$H^\infty=\{\phi:\mathbb{D}\rightarrow \mathbb{C}\mid \phi \text{ analytic and bounded}\}$$ and that polynomials are not dense in $H^\infty$ as $H^\infty$ -functions can have essential singularities on the unit circle, which is why I would think that the condition of invariance stated in the paper is stronger than the usual one. I have to admit that my knowledge about topology is a little lackluster, which is why I may not grasp Shapiros argument. I would be very thankful if somebody could either help me figure out why the formulations are equivalent or refer me to some literature regarding Shapiros argument. Thank you in advance","['complex-analysis', 'functional-analysis']"
4863364,Example of a point that is not the limit of any sequence in a connected topological space,"Question: Let $X$ be a connected space with a topology not necessarily sequential. What is an example where a point in $X$ is not the limit of any not eventually constant sequence? Motivation. Suppose $X$ is a topological vector space over $\mathbb{R}$ or $\mathbb{C}$ . If we define a sequentially separated set $S$ of $X$ as such that, for every $x\in S$ , $x$ lies outside the sequential closure of subspace $Y_x:=\text{span}(S\setminus \{x\})$ . I'm trying to use the usual Zorn's lemma argument claiming there always exists a maximal such set. But it seems, if $X$ has a not-so-nice topology, there might be points which cannot be approximated by any not eventually constant sequence, and that can derail the reasoning. I came to think about this issue when trying to understand uncountable Schauder basis. Thanks.","['general-topology', 'topological-vector-spaces', 'schauder-basis']"
4863399,How to visualize matrix functions?,"I’m a middle school (6th grade) student who is self-learning these topics , including linear algebra, and this is another question I have (teacher didn’t know again): I recently came across exponentiating matrices here , which we basically define by plugging in a matrix $A$ into the series; $\exp(A) = e^A = \sum_{n = 0}^{\infty} \frac{1}{n!}A^n$ . This is when I learned about matrix functions . Matrix functions take in matrices as inputs and output matrices: $f: A_x \rightarrow A_y$ . The function $\exp(A)$ is probably the most noteworthy example of a matrix function (do correct me if I’m wrong). One of the beauties of functions is their visualizations; graphs, vector fields, linear transformations, and so on. Now, I think that a valid question to ask is How do we visualize matrix functions? It would require, for the bare minimum of $2 \times 2$ matrices; 8 dimensions…? However, we already have two main methods for visualizing $4$ -dimensional functions; transformations, and the one I prefer, vector fields. Is there something analogous to transformations or vector fields to visualize them? I would also (really) like to know if matrix functions are linear, that is, for some matrix function $M$ , if $$M(A + B) = M(A) + M(B)$$ $$\text{and}$$ $$M(kA) = kM(A)\text{.}$$ I have already tested it for one simple function; $f(A) = 2A$ , and the result I got is that $M(A + B)$ is indeed equal to $M(A) + M(B)$ and that $M(kA)$ is equal to $kM(A)$ , but maybe this is a coincidence as I selected a simple function; $M$ is just a scaling function in the first place. Related in some sense : Does the exponential of a function converge? What can we do with it?","['matrices', 'functions']"
4863427,Evaluate $\int_{0}^{1} \int_{0}^{1} \int_{0}^{1} \frac{1}{\sqrt{3-x^2-y^2-z^2} }\text{d}x \text{d}y\text{d}z$,"How to evaluate $$
I=\int_{0}^{1} \int_{0}^{1} \int_{0}^{1} 
\frac{1}{\sqrt{3-x^2-y^2-z^2} }\text{d}x
\text{d}y\text{d}z?
$$ Some simple calculation shows that $$
I=\frac{\sqrt{2} -1}{4}\pi+\frac{\pi^2}{4} 
-\frac{3\pi}{4}\arctan\left ( \sqrt{2}  \right )+\frac12\int_{0}^{1}\arctan\left ( \frac{1}{\sqrt{x} }  \right ) 
\left ( \arcsin\left ( \frac{1}{\sqrt{2-x} }  \right ) 
-\arctan\left ( \sqrt{1-x}  \right ) \right ) \text{d}x
$$ which is out of my capability.","['integration', 'multivariable-calculus', 'multiple-integral', 'closed-form']"
4863450,Trace inequality: $\mathrm{Tr}(|\rho^{1-t}x\rho^t|)\leq \mathrm{Tr}(|\rho^{1-t}y\rho^t|)$,"I've been researching some operator spaces and have stumbled upon the following problem. Suppose $x,y\in\mathcal{B}(\mathcal{H})$ (for $\mathcal{H}$ separable) such that $0\leq x\leq y\leq 1$ . Further let $t\in[0,1]$ and $\rho$ be a density matrix (i.e. $\rho\geq 0$ , $\mathrm{Tr}(\rho)=1$ ). Is it true that $$
\mathrm{Tr}\big(\vert \rho^{1-t}x\rho^t\vert\big)\leq \mathrm{Tr}\big(\vert \rho^{1-t}y\rho^t\vert\big) \leq 1?
$$ Obviously this holds for $t=\frac{1}{2}$ , but I'm not really sure how to approach it for other values. I've tried playing around with the polar decomposition but haven't gotten anywhere. I'm convinced that the inequality is actually true at least in the case of $2\times 2$ matrices as I've run a randomized computer search and have not found any counterexamples, but even there trying to prove it directly by writing out all the matrices is very ugly. EDIT: I've verified the inequality numerically on matrices of sizes up to $5\times 5$ with $10\,000$ random samples per size (hopefully the code is correct, generating random unitaries is a bit tricky). It seems my code was indeed incorrect.","['operator-algebras', 'trace', 'matrices', 'linear-algebra', 'inequality']"
4863457,"Prove that for any function $f:\mathbb R\to\mathbb R$ there exist real numbers $x,y$, with $x\neq y$, such that $|f(x)-f(y)|\leq 1$.","I need help with a 9th grade functions exercise: Prove that for any function $f:\mathbb R\to\mathbb R$ there exist real numbers $x,y$ , with $x\neq y$ , such that $|f(x)-f(y)|\leq 1$ . I tried assuming that $|f(x) - f(y) | > 1$ so that I could say that each $f(z)$ , where $z$ is an integer, is from an interval $(a, a+1]$ where a is also an integer. So $f: \mathbb Z \to \mathbb R$ would just be from the interval $(-\infty, \infty)$ . But if n is not an integer, $f(n)$ would still be from $\mathbb R$ , meaning $|f(z) - f(n)| \leq 1$ , a contradiction. However, I don’t really know how to write this so it’s easily understandable. Sorry if I haven’t explained myself well, but this is how we do it where I’m from.","['real-numbers', 'functions']"
4863460,How is the tensor exponential map defined over a generic Banach space?,"Given some vector $x = (x_1,...,x_d)$ in $\mathbb{R}^d$ we may embed it in the tensor algebra $T(\mathbb{R}):= \prod_{n=0}^\infty (\mathbb{R}^d)^{\otimes n}$ via the tensor exponential map $\exp_\otimes$ . Specifically, we have that $$\exp_\otimes(x) := \sum_{n=0}^\infty \frac{x^{\otimes n}}{n!}:= \sum_{n=0}^\infty \frac{1}{n!} \sum_{i_1,...,i_n=1}^d x_{i_1}...x_{i_n} e_{i_1} \otimes ... \otimes e_{i_n},$$ where $\{e_1,...,e_d\}$ denotes the canonical basis of $\mathbb{R}^d$ . Now, if we consider a generic Banach space $V$ (possibly infinite dimensional), how is $\exp_\otimes $ defined? This is a fairly common map in the context of Rough Path Theory, but I haven't found any reference where $\exp_\otimes$ is properly defined. Thanks in advance for any insight. Also, a reference would suffice.","['rough-path-theory', 'reference-request', 'abstract-algebra', 'tensor-products', 'functional-analysis']"
4863467,Affine morphisms are stable under base change,"I'm over all a little confused about base change in general, and also a little confused about what it means to be stable under base change. Let me highlight my confusion in the context of affine morphisms. So, let $f:X\rightarrow Y$ be an affine morphism, and $g:Z\rightarrow Y$ be another morphism between schemes $X,Y,$ and $Z$ . Now if I understand correctly, the base change of $X$ is the fibre product $X\times_Y Z$ , and it is called a base change because we now have a morphism $X\times_YZ\rightarrow Z$ which is literally just the projection $\pi_Z$ from the fibre product to $Z$ . So if $f:X\rightarrow Y$ is an affine morphism, and then for affine morphisms to be stable under base change means that the projection map $\pi_Z:X\times_YZ\rightarrow Z$ is also an affine morphism. (I think this is the correct way of thinking about this, if not please let me know) To do this, we need only show that there exists an open cover affines $U_i$ in $Z$ such that $\pi^{-1}(U_i)$ .  So, let $z\in Z$ , then there is an affine open neighborhood $V$ of $z$ which maps into an affine open neighborhood $U\subset Y$ . By definition $f^{-1}(U)$ is affine, and so $\pi_X^{-1}(f^{-1}(U))=f^{-1}(U)\times_U Z$ . Is there some reason that this should actually be $f^{-1}(U)\times_UV$ ? And if so, how do we then conclude that $\pi_Z^{-1}(V)$ is equal to $f^{-1}(U)\times_UV$ ? Since these are the only affines we have to work with I assume that this should be the form I am looking for, I just can't quite see how to get there.","['algebraic-geometry', 'schemes', 'fibre-product']"
4863564,"How to integrate $\int_{0}^{1} \int_{0}^{1} \tanh^{-1}\left(\frac{x}{y} + \frac{y}{x}\right) \,dx\,dy$","how to integrate $$\int_{0}^{1} \int_{0}^{1} \tanh^{-1}\left(\frac{x}{y} + \frac{y}{x}\right) \,dx\,dy$$ My attempt $$\int_{0}^{1} \int_{0}^{1} \tanh^{-1}\left(\frac{x}{y} + \frac{y}{x}\right) \,dx\,dy = \int_{0}^{1} \int_{0}^{1} \tanh^{-1}\left(\frac{x^2 + y^2}{xy}\right) \,dx\,dy$$ $$\int_{0}^{1} \int_{0}^{1} \frac{1}{2} \ln\left[\frac{1 + \frac{x^2 + y^2}{xy}}{1 - \frac{x^2 + y^2}{xy}}\right] \,dx\,dy$$ $$\int_{0}^{1} \frac{1}{2} \int_{0}^{1} \ln\left[\frac{xy + x^2 + y^2}{xy - (x^2 + y^2)}\right] \,dx\,dy$$ $$\frac{1}{2} \int_{0}^{1} \int_{0}^{1} \ln\left[\frac{x^2 + yx + y^2}{-(x^2 - yx + y^2)}\right] \,dx\,dy$$ $$\frac{1}{2} \int_{0}^{1} \int_{0}^{1} \{\ln(x^2 + yx + y^2) \,dx\,dy - \ln(-1)\left[(x^2 - yx + y^2)\right]\}$$ $$\frac{1}{2} \int_{0}^{1} \int_{0}^{1} \ln(x^2 + yx + y^2) - \ln(-1) - \ln(x^2 - yx + y^2) \,dx \,dy$$ $$\frac{1}{2} \int_{0}^{1} \int_{0}^{1} \left[\ln(x^2 + yx + y^2) - \ln(x^2 - yx + y^2) - \ln(e^{i\pi})\right] \,dx\,dy$$ $$\frac{1}{2} \int_{0}^{1} \left[\int_{0}^{1} [\ln(x^2 + yx + y^2) - \ln(x^2 - yx + y^2)] \,dx - i\pi \int_{0}^{1} 1 \,dx\right] \,dy$$ $$\frac{1}{2} \int_{0}^{1} \left[\int_{0}^{1} \ln(x^2 + yx + y^2) \,dx - \int_{0}^{1} \ln(x^2 - yx + y^2) \,dx - i\pi \big[x\big]_{0}^{1}\right] \,dy$$ $$\frac{1}{2} \int_{0}^{1} \left[ \ln(1 + y + y^2) - \int_{0}^{1} \frac{2x^2 + yx}{x^2 + yx + y^2} \,dx  - \ln(1 - y + y^2) + \int_{0}^{1} \frac{2x^2 - yx}{x^2 - yx + y^2} \,dx - ix \right]\,dy$$","['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'closed-form']"
4863593,Can we derive a norm and an inner product from a metric?,"Given an inner product on a vector space, I can always define a norm and a metric (and a topology using that metric). Is the converse true? That is, given a metric on a vector space, can I define an inner product with it? And what about a metric space that is not a vector space?","['inner-products', 'normed-spaces', 'metric-spaces', 'vector-spaces', 'real-analysis']"
4863598,Relation between square root of matrix representation of Riemannian metric and Christoffel symbols,"Let $(M,g)$ be a Riemannian manifold of dimension $n$ and let $g_{ij}$ be the matrix of the components of $g$ in coordinates. Since $g_{ij}$ is symmetric and positive-definite, it can diagonalized by $g_{ij} = PDP^{T}$ , with $D = \text{diag}(\lambda_1, \ldots, \lambda_n)$ diagonal and $P$ orthogonal. For $s \in \mathbb{R}$ , define the matrix power $g_{ij}^s := PD^sP^T$ , where $D^s = \text{diag}(\lambda_1^s, \ldots, \lambda_n^s)$ . Is there a link between the matrix $g_{ij}^{-1/2}$ and the Christoffel symbols $\Gamma^i_{jk}$ ? For instance, denoting the matrix by $G := g_{ij}$ , if I consider the derivative: \begin{equation}
   \frac{\partial G^{-1/2}}{\partial x^k} = -\frac{1}{2} G^{-1}\frac{\partial G}{\partial x^k}G^{-1/2},
\end{equation} several ""ingredients"" of the Christoffel symbols appear. I know that the following holds when the Christoffel are contracted: \begin{equation}
   \Gamma^i_{ij} = \frac{1}{2}g_{ik}^{-1}\frac{\partial g_{ik}}{\partial x^j},
\end{equation} but I'd be interested to know if a relation involving $g_{ij}^{-1/2}$ exists before contraction. On another note, I derived the expression of the derivative of $G^{-1/2}$ using the usual approach when taking derivatives of matrices (i.e., taking the derivative of the identity $G^{-1/2}GG^{-1/2} = I$ ), but from I read in this paper https://epubs.siam.org/doi/10.1137/S089547989528274X (Theorem 3.5 with $\alpha = -1/2$ ), there might be additional terms... I try to interpret the following ""symmetry"" relation on the matrix components of a metric: \begin{equation}
g_{ti}^{-1/2}\frac{\partial g_{kj}^{-1/2}}{\partial x^t} = g_{tj}^{-1/2}\frac{\partial g_{ki}^{-1/2}}{\partial x^t}
\end{equation} Could this be a relation between Christoffel symbols? For context, I am trying to reinterpret some approaches in numerical geometry in engineering using differential geometry, and I need to reverse-engineer some results to try and make sense of them on manifolds.
For the first question, I consider an isometry $F:U \to M$ between $(U \subseteq \mathbb{R}^2, \bar{g})$ and $(M,g)$ , where $\bar{g}$ is the Euclidean metric. If $y^j$ are coordinates on $U$ , it is my understanding that in coordinates, the components of the differential map $dF$ satisfy: \begin{equation}
\frac{\partial F^i}{\partial y^j} = g_{ik}^{-1/2}O_{kj}
\end{equation} for some orthogonal matrix $O$ , since $F$ pulls back the metric $g$ to the Euclidean metric, that is: \begin{equation}
\bar{g}_{ij} = \delta_{ij} = \left(\frac{\partial F^k}{\partial y^i}\right)^T g_{kl} \frac{\partial F^l}{\partial y^j},
\end{equation} which is satisfied if the Jacobian matrix has the form written above. Thus, the square root matrix of 1. originates from this. The symmetry relation of 2. is obtained as follows: I consider a map $F:U \to M$ that is not necessarily an isometry or even smooth, and I suppose that I can write its Jacobian matrix in coordinates as: \begin{equation}
\frac{\partial F^i}{\partial y^j} = g_{ik}^{-1/2}R_{kj},
\end{equation} with $R$ a rotation matrix. Then I consider what kind of constraints there should be on the components of the metric to have at least a $C^2$ map. The symmetry relation is obtained by imposing that the second derivatives of $F^i$ are equal, to ensure $C^2$ continuity. Please feel free to correct any nonsense in what I said, I am learning DG on my own using Lee's Introduction to smooth and Riemannian manifolds.
Thanks for your help!","['riemannian-geometry', 'differential-geometry']"
4863638,Stacking blocks on the right or above,"Imaging if I have n sequares where the first one is fixed at the origin. The subsequent blocks may be stacked either on the right on top of the previous block. If one configuration can rotate into another, they are counted as the same configuration. If one can just be mirrored into another, but not rotated, then they are counted as two configs. E.g.
O = start, R = right, U = up OURR and ORRU is the same, but OUUR and ORRU are different. How many different chains can n blocks form? I was thinking, if there are n blocks there are $2^{n-1}$ chains in total. Each chain
rotating 180 degrees gives a repetition so I need to divide by 2 giving $2^{n-2}$ distinct chains. However, this is incorrect. Imaging 4 blocks, the configurations are ORRR, ORRU, ORUR, OURU, OUUR, which gives 5, so that breaks my relation immediately. How should I do the counting here?",['combinatorics']
4863648,floor(x/y)=y graph,"I was experimenting with some equations on Desmos and stumbled upon the floor function. I tried using it in different equations, but nothing caught my attention until I tried y = floor(x/y) ( Screenshot ). This graph looks like a step function with an increasing step of 0.5, which is confusing to me. I asked MathGPT and tried to figure it out myself, but I still don't understand it. Can you please explain why this is happening?","['ceiling-and-floor-functions', 'functions', 'graphing-functions']"
4863723,"Finding $x$ and $y$ with the information that ""On exactly half of the days,No more than one student was absent"". [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question Henry recorded the number of students present everyday for a total of $40$ days. Also there are a total of $29$ students. Here is the frequency table: Also it is given that,on exactly half of the days, No more than one student was absent. Find the values of $x,y$ . My try: Obviously $1+2+x+10+y+12=40 \Rightarrow x+y=15$ . But I am unable to frame other equation. I found cumulative frequencies as: $1,3,x+3,x+13,28,40.$ But not sure how to proceed?","['cumulative-distribution-functions', 'statistics', 'probability']"
4863744,Likelihood an employee gets a failing evaluation by chance,"Suppose I have a group of $n$ employees. Each one is expected to perform a certain task, and the result of this task is either Pass or Fail. As a part of their reviews, I have to tally up their totals, and each employee should have > 80% passing results. But suppose there is an unknown (but constant) chance that the task fails completely out of the employee's control. Based on the number of passing and failing results of each employee, how can I assess the likelihood that the outcome was just by chance, and not because a certain employee is underperforming? For example, I have 12 employees, each with 40 attempts. One has a 100% passing rate, while the lowest is an 80% passing rate.","['statistics', 'probability']"
4863751,Sqrt LASSO vs LASSO,"In the paper Square Root Lasso: Pivotal Recovery of Sparse Signals via Conic Programming they talk about Sqrt-LASSO which is simply just trying to minimize $\|Ax-b\|_2 + \lambda\|x\|_1$ rather than the regular LASSO $\|Ax-b\|_2^2 + \lambda\|x\|_1$ . Can anyone point out the theoretical differences between the two in terms of whether one is more robust to outliers, do we still have sparsity, etc? What about in practice, do these implementations have much of a difference?","['statistics', 'regression', 'regression-analysis', 'linear-regression', 'optimization']"
4863790,Other approaches to $\int_{0}^{1} \frac{K\left ( x \right ) }{\sqrt{3-x} } \text{d}x$,"Let $K(x) = \int_0^{\pi/2}\frac{1}{\sqrt{1-x^2 \sin^2 \theta}}d\theta$ be the complete elliptic integral of first kind. It could be shown that $$
\int_{0}^{1} \frac{K\left ( x \right ) }{\sqrt{3-x} } \text{d}x=\frac{\Gamma\left ( \frac{1}{24}  \right ) 
\Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{96\pi\sqrt{3}}
$$ thanks to the known results here . With its simple appearance, I wonder whether   a brief approach exists so that we can understand them better. Appreciate your creative efforts. Applying the well-known quadratic transformation formula $$
K\left ( \frac{1-x}{1+x}  \right ) 
=\frac{1+x}{2} K^\prime(x),K^\prime(x):=K\left ( \sqrt{1-x^2}  \right )
$$ we get $$
\int_{0}^{1} \frac{K(x)}{\sqrt{3-x} }\text{d}x
=\frac{1}{\sqrt{2} } \int_{0}^{1} \frac{K^\prime(x)}{\sqrt{\left ( 1+x\right )\left ( 1+2x \right )  } }
\text{d} x.
$$ If using the expansion $$
\frac{1}{\sqrt{\left ( 1+x\right )\left ( 1+2x \right )  } }
=\sum_{n\ge0} a_n x^n
$$ and $$ \int_{0}^{1}x^n K^\prime(x)\text{d}x
=\frac\pi4\frac{\Gamma\left ( \frac{n+1}{2}  \right )^2 }{
\Gamma\left ( \frac{n+2}{2}  \right )^2},$$ we could obtain a series representation for the integral. This works more on the case mentioned by @MiracleInvoker, because the series could split into two hypergeometric series, both of which are normally evaluated.","['integration', 'alternative-proof', 'elliptic-integrals', 'real-analysis']"
4863794,Find all $x \in \mathbb{R}$ such that $1+\sqrt{1-\frac{1}{x}}+\sqrt{x-\frac{1}{x}}=x^2$.,"This problem was posed by a friend of mine. He's got it from a social media post. We don't know how to solve it, but there are similar types of questions in this website, whose techniques I've tried to apply but to no avail. I'm trying to find some sort of manipulation that allows us to find the real roots of the equation: \begin{equation*}1+\sqrt{1-\frac{1}{x}}+\sqrt{x-\frac{1}{x}}=x^2\end{equation*} \begin{align*}\text{Where } x \in \mathbb{R} \end{align*} I've already tried many things, but this is what seems cleaner: \begin{align*}
a=\sqrt{x-\frac{1}{x}},& \quad b=\sqrt{1-\frac{1}{x}}\\
\implies a+b=x^2-1,&\quad  a-b=\frac{1}{x+1}\\
\to a^2+b^2=\frac{x^2+x-2}{x}, &\quad a^2-b^2=x-1
 \end{align*} But I couldn't get much further. This looks like a method where I could stay organized and try to spot something with more ease, but I wasn't able to do so. I'm not interested in employing numerical methods. I know $1$ is a trivial solution and, through the usage of software, that $\varphi$ is also a solution. Thanks in advance.","['algebra-precalculus', 'factoring', 'substitution']"
4863800,Cayley Table Sudoku,"Given an $n\times n$ grid partially filled with the numbers $0,\ldots, n-1$ , we can play a Sudoku-like game by trying to fill in the rest of the grid so that the end result is the Cayley table for a group (edit: where position $i,j$ in the table would represent the product of $i $ and $j$ in the group structure). I am curious about the following value: let $g(n)$ represent the smallest natural number such that if $g(n)$ squares in an $n\times n$ grid are filled, then the configuration always can be extended to a solution in at most $1$ way. Another way to think of this question is as follows: How similar can two distinct $n\times n$ Cayley tables be? $g(n)$ is the smallest natural number such that if two $n\times n$ Cayley tables agree for $g(n)$ entries, then they are identical. What does $g(n)$ look like? I'm most interested in the asymptotic behaviour (as I imagine $g(n)$ may fluctuate wildly as $n$ increases due to the fluctuation in the number of groups of order $n$ as $n$ increases). One particularly interesting question: can $g(n)\over n^2$ be bounded above by a constant (say, 0.99)? In other words, if two Cayley tables agree on $99\%$ of entries, must they be identical? And if so, how much lower than $0.99$ can we bring this constant?","['group-theory', 'abstract-algebra', 'cayley-table', 'information-theory']"
4863812,Why does the limit $\lim _{a \rightarrow \infty} \frac{\int_0^a \sin ^4 x d x}{a}$ not exist?,"I have some confusion regarding the existence of the follow limit: $$\lim _{a \rightarrow \infty} \frac{\int_0^a \sin ^4 x d x}{a}$$ According to the answer key of the material I found this question in (from a local study institute), this limit does not exist.
I also verified this using Desmos, and it is true that as $a$ gets large the function does not appear to take a constant value. However this is how I solved it: Let $$a=n\frac{\pi}{2}+k , 0<k<\frac{\pi}{2},n\in \mathbb{N}$$ Then we can rewrite the limit as $$\lim_{n \rightarrow \infty}\frac{\int_{0}^{n\frac{\pi}{2}+k}\sin^4x\ dx}{n\frac{\pi}{2}+k}$$ $$\lim_{n \rightarrow \infty}\frac{\int_{0}^{n\frac{\pi}{2}}\sin^4x\ dx + \int_\frac{n\pi}{2}^{n\frac{\pi}{2}+k}\sin^4x\ dx}{n\frac{\pi}{2}+k}$$ The $k$ in the denonimator can be neglected with respect to $n\frac{\pi}{2}$ , and the first integral evaluates to $\frac{3n\pi}{16}$ , so we get $$\lim_{n \rightarrow \infty}\frac{\frac{3n\pi}{16}+\int_\frac{n\pi}{2}^{n\frac{\pi}{2}+k}\sin^4x\ dx }{\frac{n\pi}{2}}$$ The second integral can be neglected in comparison the first  so finally we get $$\lim_{n \rightarrow \infty}\frac{\frac{3n\pi}{16}}{\frac{n\pi}{2}}=\frac{3}{8}$$ And yet, this limit $\it{apparently}$ does not exist. So, can someone tell me what's wrong with my method and $\it{why}$ does the limit not exist?","['limits', 'calculus', 'trigonometry']"
4863836,What Is the Probability The Second Kid Is a Boy? [duplicate],"This question already has answers here : In a family with two children, what are the chances, if one of the children is a girl, that both children are girls? (21 answers) Closed 4 months ago . Okay, so I was asked this question in an interview on a machine learning expert position. To be honest, the question itself (and the hint by the interviewer) seemed quite ill-phrased, which probably is the reason I ended up failing the interview, and he thought I must be super dumb. Here is the original question. You know your colleague has two kids, and also know one of them is a
boy. What is the probability that the other one is a boy too? I was a bit puzzled, then he gave me a hint, by asking me to use Bayes' theorem, which I knew from high school $$\mathbb{P}(A\cap B)=\mathbb{P}(A|B)*\mathbb{P}(B)$$ I could see that given one kid is a boy corresponds to event $B$ , but could not really figure out the other quantities. To confuse matters, he gave me hints like when you see people with two kids, most of the times it is a boy and a girl, right? I could not argue with him, obviously, but I cannot reach any such conclusion based on my personal observation either. I tried to tell things like to calculate it we need empirical data like survey of all couples having two kids in the city/country etc. absent other information, the second child has the same probability of being a boy as the percentage of males in the country, assuming each kid's gender is independent But seems she had some assumption about the scenario (that meant the problem can be solved purely mathematically) that I failed to clarify. Upon further thought, there may be some biological concepts on how chromosomes interact to decide the gender of the second kid (and whether it is biased one way or another), but that is hardly fair to expect from an ML engineer. Is that where the answer lies? But the reason for this post is not to complain, but I am giving the context, just to ask what exactly am I missing in the question assuming it is meant to be a probability (and not biology) question.","['statistics', 'bayesian', 'probability-theory', 'probability']"
4863842,"Evaluation of $\int_{0}^{\infty} \, \frac{x^\alpha \log^n{(x^\beta)}}{k^m+x^m}\, dx$","How to evaluate the following integral/ $$\int_{0}^{\infty} \, \frac{x^\alpha \log^n{(x^\beta)}}{k^m+x^m}\, dx $$ , $\alpha, \beta, m, n$ are real and $k$ is a positive integer. For the case where $\beta=1$ and $m=2$ , we can consider the following integral: $$
\begin{aligned}
I(\alpha, k) & =\int_0^{\infty} \frac{x^\alpha}{k^2+x^2} d x \\
& =\frac{k^{\alpha-1}}{2} \int_0^{\infty} \frac{t^{(\alpha-1) / 2}}{1+t} d t \\
& =\frac{k^{\alpha-1}}{2} \mathrm{~B}\left(\frac{1+\alpha}{2}, \frac{1-\alpha}{2}\right) \\
& =\frac{k^{\alpha-1}}{2} \frac{\pi}{\cos \left(\frac{\pi}{2} a\right)}
\end{aligned}
$$ Differentiating $I(\alpha, k)$ with respect to $\alpha$ yields: $$
\begin{align*}
  \int_{0}^{\infty} \, \frac{x^\alpha \log^n{x}}{k^2+x^2}\, dx &= \frac{\partial^{n} }{\partial a^n} \left(\frac{k^{\alpha-1}}{2}\, \frac{\pi}{\cos{\displaystyle \left(\frac{\pi}{2}\alpha\right)}}\right)
\end{align*}
$$ (based on this answer). Are there any other methods to evaluate the general integral?
Thank you!","['integration', 'improper-integrals', 'definite-integrals', 'logarithms', 'derivatives']"
4863909,A matrix manipulation from Berezin's paper,"I am reading a paper by Berezin, entitled General Concept of Quantization . He writes: and then: This ought to be some clever manipulation of matrices and yet I am not able to show how (1.4) follows from (1.1) if $\omega$ is given to be invertible. Any leads?","['matrices', 'matrix-calculus', 'partial-derivative']"
4863958,How to evaluate this sum $\sum_{n=1}^{\infty} \frac{(-1)^n}{(n^2 + 3n + 1)(n^2 - 3n + 1)}$,"How to evaluate this sum $$\sum_{n=1}^{\infty} \frac{(-1)^n}{(n^2 + 3n + 1)(n^2 - 3n + 1)}$$ My attempt $$\sum_{n=1}^{\infty} \frac{(-1)^n}{(n^2 + 3n + 1)(n^2 - 3n + 1)}$$ $$= \sum_{n=1}^{\infty} \frac{(-1)^{n - 1 + 1}}{(n^2 + 2 \cdot \frac{3}{2}n + 1)(n^2 - 2 \cdot \frac{3}{2}n + 1)}$$ $$
= \sum_{n=1}^{\infty} \frac{(-1)^{n - 1} \cdot (-1)}{(n^2 + 2 \cdot \frac{3}{2}n + \frac{9}{4} - \frac{5}{4})(n^2 - 2 \cdot \frac{3}{2}n + \frac{9}{4} - \frac{5}{4})}
$$ $$= - \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{(n^2 + 2 \cdot \frac{3}{2}n + \left(\frac{3}{2}\right)^2 - \left(\frac{\sqrt{5}}{2}\right)^2)(n^2 - 2 \cdot \frac{3}{2}n + \left(\frac{3}{2}\right)^2 - \left(\frac{\sqrt{5}}{2}\right)^2)}$$ $$= - \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{[(n + \frac{3}{2})^2 - (\frac{\sqrt{5}}{2})^2][(n - \frac{3}{2})^2 - (\frac{\sqrt{5}}{2})^2]}$$ $$ = -\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{(n + \frac{3}{2} - \frac{\sqrt{5}}{2})(n + \frac{3}{2} + \frac{\sqrt{5}}{2})(n - \frac{3}{2} - \frac{\sqrt{5}}{2})(n - \frac{3}{2} + \frac{\sqrt{5}}{2})}$$ $$ = - \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{[n + \frac{3 - \sqrt{5}}{2}][n + \frac{3 + \sqrt{5}}{2}][n - \frac{3 + \sqrt{5}}{2}][n - \frac{3 - \sqrt{5}}{2}]}$$ $$= - \sum_{n=1}^{\infty} (-1)^{n-1} \cdot \frac{1}{[n + \frac{3 + \sqrt{5}}{2}][n - \frac{3 + \sqrt{5}}{2}]} \cdot \frac{1}{[n + \frac{3 - \sqrt{5}}{2}][n - \frac{3 - \sqrt{5}}{2}]}$$ $$= - \sum_{n=1}^{\infty} (-1)^{n-1} \cdot \frac{(3 + \sqrt{5} + n - n)}{(3 + \sqrt{5})[n + \frac{3 + \sqrt{5}}{2}][n - \frac{3 + \sqrt{5}}{2}]} \cdot \frac{(3 - \sqrt{5} + n - n)}{(3 - \sqrt{5})[n + \frac{3 - \sqrt{5}}{2}][n - \frac{3 - \sqrt{5}}{2}]}$$ $$= = -\frac{1}{(3+\sqrt{5})(3-\sqrt{5})} \sum_{n=1}^{\infty} (-1)^{n-1} \cdot \frac{\left(\frac{3 + \sqrt{5}}{2} + \frac{3 - \sqrt{5}}{2} + n - n\right)}{\left[n + \frac{3 + \sqrt{5}}{2}\right]\left[n - \frac{3 + \sqrt{5}}{2}\right]} \cdot \frac{\left(\frac{3 - \sqrt{5}}{2} + \frac{3 + \sqrt{5}}{2} + n - n\right)}{\left[n + \frac{3 - \sqrt{5}}{2}\right]\left[n - \frac{3 - \sqrt{5}}{2}\right]}$$ $$= -\frac{1}{3^2 - (\sqrt{5})^2} \sum_{n=1}^{\infty} (-1)^{n-1} \cdot \frac{\left(n + \frac{3 + \sqrt{5}}{2} - n + \frac{3 + \sqrt{5}}{2}\right)}{\left[n + \frac{3 + \sqrt{5}}{2}\right]\left[n - \frac{3 + \sqrt{5}}{2}\right]} \cdot \frac{n + \frac{3 - \sqrt{5}}{2} - n + \frac{3 - \sqrt{5}}{2}}{\left[n + \frac{3 - \sqrt{5}}{2}\right]\left[n - \frac{3 - \sqrt{5}}{2}\right]}$$ $$- \frac{1}{4} \sum_{n=1}^{\infty} (-1)^{n-1} \left[\frac{1}{n - \frac{3 + \sqrt{5}}{2}} - \frac{1}{n + \frac{3 + \sqrt{5}}{2}}\right]\left[\frac{1}{n - \frac{3 - \sqrt{5}}{2}} - \frac{1}{n + \frac{3 - \sqrt{5}}{2}}\right]$$","['calculus', 'closed-form', 'summation', 'sequences-and-series']"
4863986,Understanding $\mathbb{C}P^2$,"I am trying to understand $\mathbb{C}P^2$ . Since I understand the Hopf fibration quite well, I like the following construction: Attach a $\mathbb{D}^2$ (2-cell) to a point $\mathbb{D}^0$ (0-cell) to get $S^2$ (thanks to @Leo Mosher for suggesting the more sensible order of attaching) Now attach $\mathbb{D}^4$ to $S^2$ by gluing the boundary $\partial \mathbb{D}^4=S^3$ to $S^2$ using the projection map P of the Hopf bundle $S^1 \hookrightarrow S^3 \rightarrow S^2$ I picture the result as a $S^2$ bundle over $S^2$ which I know is $\textbf{wrong}$ *. Here is my reasoning. Where did I go wrong? As $S^3 = \partial \mathbb{D}^4$ is a $S^1$ bundle over $S^2$ , the ""interior"" $\mathbb{D}^4 $ is a $\mathbb{D}^2$ bundle over $S^2$ (by just ""filling"" every $S^1$ ) Gluing the boundary $\partial \mathbb{D}^4 = S^3$ to $S^2$ via the projection of the Hopf fibraton amounts to gluing the boundary of every fiber (ie $\partial \mathbb{D}^2 = S^1$ ) together, giving us $S^2$ at every point. I think that 2. probably does not work, ie that the boundaries of the smoothly over $S^2$ varying $\mathbb{D}^2$ 's (making up $\mathbb{D}^4$ ) cannot be glued together to give smoothly varying $S^2$ 's at every point, but that at one point this has to break.
That would mean that $\mathbb{C}P^2 - \{\text{point}\}$ is a $S^2$ bundle. Can someone help me clarify, where my reasoning is wrong? I would also gladly appreciate any other insight into $\mathbb{C}P^2$ . Thank you! \ * There are only two $S^2$ bundles over $S^2$ : One being the trivial one, and the other one being a twisted one. Can someone maybe also confirm or reject my image of this twisted $S^2$ bundle: Imagine $SO(3)$ as $S^1$ bundle over $S^2$ (which is the unit tangent bundle of $S^2$ denoted $T_1S^2$ ). The interior is obtained by filling those $S^1$ 's giving us a $\mathbb{D}^2$ bundle over $S^2$ Now glue together the boundaries of every fiber $\partial \mathbb{D}^2 = S^1$ to obtain a sphere a $S^2$ at every point. This gives a pretty ""simple"" visual of the twisted $S^2$ bundle over $S^2$ . $\textbf{EDIT:}$ I imagine the breaking via stereographic projection of $S^3\subset \mathbb{R}^4$ to $\mathbb{R}^3$ :
The projection gives tori filling $\mathbb{R}^3$ . Every fiber $S^1$ becomes a Villarceau circle. One can move these circles to the points on $S^2$ they will end up under the Hopf map. The circle that ends up at the north pole is degenerated to a line (which is of course just an artifact of the projection). One cannot picture this fiber as an acual circle, without loosing the picture of the $S^1$ 's varying smoothly. Now I can imagine filling all of these $S^1$ 's to become $\mathbb{D}^2$ 's including the degenerated one, which (if this is possible at all) has remain a degenerated line in order to vary smoothly (necessary not sufficient though). If I further glue the boundaries of these $\mathbb{D}^2$ 's to get $S^2$ 's at every point, I can (because $\mathbb{C}P^2$ is not a bundle) get smoothly varying $S^2$ 's except for the degenerate $\mathbb{D}^2$ at the north pole somehow...","['differential-topology', '4-manifolds', 'low-dimensional-topology', 'algebraic-topology', 'differential-geometry']"
4863999,Is it possible that it exists a $C>0$ that verify for all continuous function on $[a;b]$; $\|f\|_{\infty} \leq C \|f\|_2 $? [duplicate],"This question already has an answer here : Are all the norms of $L^p$ space equivalent? (1 answer) Closed 4 months ago . Question: When $f \in C^2[a;b]$ we define $ \|f\|_2=\sqrt{\int_{[a;b]}|f|^{2}}$ and when $f$ is bounded on $[a;b]$ we define $\|f\|_{\infty} = \sup_{x \in [a;b]} |f(x)| $ . Is it possible that it exists a $C>0$ that verify for all continuous function on $[a;b]$ ; $||f||_{\infty} \leq C \|f\|_2 $ ? Answer: 1- Let choose the set of functions $ f_k(x)=\frac{(x-a)^k}{(b-a)^k} , k \geq 1$ . Hence $ \|f_k(x)\|_2 ^2  = \int_a^b \frac{(x-a)^{2k}}{(b-a)^{2k}} dx = \frac{1}{(b-a)^{2k}} [\frac{(x-a)^{2k+1}}{2k+1}]_a^b = \frac{b-a}{2k+1} $ 2- When $k \to \infty$ we have that for every $C$ we can choose: $ C \|f_k(x)\|_2 \to 0 $ . 3- On an other side we have that $ \|f_k(x)\|_{\infty} = f(b)=1 $ 4- Thus no matter which $C$ we can choose $ \exists K $ s.t. $ \forall k > K $ we will have $ \|f\|_{\infty} = 1 > C \|f\|_2 $ For exemple you can choose: $K = \frac{b-a-1}{2}$ Is this correct? Thank you.","['integration', 'calculus', 'normed-spaces', 'analysis']"
4864012,Proving the convergence of a series with very little information,"Let $m \in \mathbb{N}$ be a fixed natural number and $ (a_n)_{n \geq 1}$ be a sequence of positive real numbers such that $\forall n \geq 1\colon a_{n+1} \leq a_n - a_{mn}$ . Prove that the series $\sum_{n=1}^{\infty}n^\alpha a_n$ converges $\forall \alpha \in \mathbb{R}_+$ . My first idea was to show that $a_n$ is decreasing. From the hypothesis, we have that $ 0 < a_{mn} \leq a_n-a_{n+1}$ . So we can sum this equality up from $n=s$ to $n=t$ , so we get $\sum_{n=s}^t a_{nm} \leq a_s-a_{t+1}$ . However I do not think that it helps that much.","['analysis', 'real-analysis', 'calculus', 'sequences-and-series', 'problem-solving']"
4864023,"Is there an ""elementary irrational number"" without a certain digit in its decimal presentation?","In this question, I define an ""elementary irrational number"" as an irrational number which is built up of a finite combination of integers, field operations (addition, multiplication, division, and root extractions -- the elementary operations) and exponential and trigonometric functions and their inverses under repeated compositions, as if the definition of elementary function (I made up the name since I couldn't find it on the Internet). The numbers $\pi=\arccos(-1)$ , $\mathrm e=\exp(1)$ and $\phi=\frac{1+\sqrt5}2$ are ""elementary irrational numbers"", while $1$ , $\frac32$ and $1.2121121112\dots$ are not. It's easy to find either a rational number or an irrational one without a certain digit in its decimal presentation, for example, $1.2121121112\dots$ doesn't contain digit $0$ . However, among ""elementary irrational numbers"", it seems hard to find such number, despite proving there is no such number is not easy either. I can come up with some possible ways to solve the problem: Most constructed numbers aren't ""elementary"", while there does exists possibility to find one. To disprove the existence of such number, we may prove through all the operations to build up the elementary functions that if its arguments are ""elementary number(s)"" which is/are either rational or contains every kind of digit, then the result satisfies the same property, but it seems to be quite difficult to prove it. Or maybe the problem is connected to some existed open problems in mathematics (normal numbers might be an example), then we may be convinced with the difficulty of the problem. I'd appreciate it if you could give me any advice on the problem.","['elementary-functions', 'irrational-numbers', 'real-analysis']"
4864043,Information-theoretic Inequality,"If we have two discrete RVs, X, and Y. How can we show: $$\sum_{x,y} p(x|y)p(y|x) \geq 1.$$ The question goes further with finding a sufficient and necessary condition for equality. My attempt:
For equality, assuming that X, Y are independent will enable us to sum over each variable PMF and get exactly one. However, I am stuck with showing how the inequality holds in general, and I appreciate any hints and tips.","['conditional-probability', 'probability-distributions', 'information-theory', 'inequality', 'probability']"
4864050,Can a discontinuous function be increasing or decreasing,"How would we decide increasing or decreasing function if the function is not differentiable? Can a discontinuous function be increasing or decreasing?
And can it be monotonic?","['calculus', 'functions', 'derivatives', 'monotone-functions']"
4864067,Reduced groupoid $C^*$-algebra,"I was reading Ozawa's book and in Chapter 5 they discuss $C^* $ -algebras of locally compact Hausforff étale groupoids. I have a question regarding the reduced $C^* $ -algebra construction. It is first defined by considering the $* $ -algebra $C_c(G)$ and endow it with a $C_0(G^{(0)})$ -valued inner product defined for $f,g \in C_c(G)$ and $x \in G^{(0)}$ by $$
\langle f,g \rangle(x) = \sum\limits_{\gamma \in G_x}\overline{f(\gamma)}g(\gamma).
$$ Thus we obtain the Hilbert module $L^2(G)$ completing $C_c(G)$ for this inner product. Now we define the left regular representation $\lambda: C_c(G) \xrightarrow{} B(L^2(G))$ by $$
\lambda(f)g = f* g, \quad \text{ for } f,g \in C_c(G),
$$ and endow $C_c(G)$ with the norm $\|f\|_\lambda := \|\lambda(f)\|.$ Completing for this norm, we obtain the reduced groupoid $C^* $ -algebra, denoted by $C_\lambda^* (G)$ . However, there is a footnote saying that we can consider a Hilbert space instead of module by letting $\mu$ be a regular Borel measure with full support on $G^{(0)}$ and defining an inner product on $C_c(G)$ by $$
\langle f,g \rangle_\mu := \int_{G^{(0)}} \langle f,g \rangle(x) d\mu,
$$ obtaining the Hilber space $L^2(G,\mu)$ through completion. We can then consider the same regular representation on $B(L^2(G,\mu))$ and complete $C_c(G)$ for the norm $\|f\|_{\lambda,\mu} := \|\lambda(f) \|_{B(L^2(G,\mu))}$ and lets denote this by $C_{\lambda,\mu}^* (G)$ . My question is how do these norms relate. The first one is $$
\|f\|_\lambda^2 = \|\lambda(f)\|_{B(L^2(G))}^2 = \sup\limits_{\|g\|_{L^2(G)}=1} \|f * g \|_{L^2(G)}^2 = \sup\limits_{\|g\|_{L^2(G)}=1} \| \langle f * g, f * g \rangle \|_\infty.
$$ while the second, $$ \| f \|_{\lambda,\mu}^2 = \| \lambda(f) \|_{B(L^2(G,\mu))}^2 = \sup\limits_{\| g \|_{L^2(G,\mu)} = 1} \| f * g \|_{L^2(G,\mu)}^2 = \sup\limits_{\| g \|_{L^2(G,\mu)}=1} \int_{G^{(0)}} \langle f * g, f * g \rangle(x) d\mu . 
$$ We can see that in case 1 the norm is obtained by taking taking the infinity norm of $\langle f* g,f* g \rangle$ but on the second we take the $L^1$ norm of the same function. Because of this I'm having troubles believing these form the same $C^* $ -algebra, did I misunderstand something?","['c-star-algebras', 'groupoids', 'functional-analysis', 'operator-algebras']"
4864077,Question regarding set notation,"In an academic research paper in computer science,  I have a formula using the set builder notation in order to define a set $A$ , as follows: $$A = \{ y(b_i)=a_i | b_i \in B \}.$$ In the particular example, I want to convey in a concise way that each element $a_i \in A$ results from the transformation given by the function $y(b_i)$ for all $b_i \in B$ . Is that an appropriate and correct formulation, especially the part on the left-hand side (LHS) of the set builder? That is, in set theory, is it allowed/correct to use an equality on the LHS of the set builder in the same way as above?","['notation', 'set-theory', 'discrete-mathematics']"
4864078,Question about properties of Picard-Lindeloef existence theorem,"I have a few questions about solutions that arise from differential equations where Picard-Lindeloef can be applied: In the problem $y'=f(t,y)=-y^2$ , solutions have the form $\frac{1}{x-c}$ and always have a discontinuity point, what is causing this? Is it that $f(t,y)$ has a zero for some value of $y$ ? Will this always happen? If a structure funciton $f$ is locally Lipschitz on an entire interval, do the unique solutions always exist on that entire interval? I don't mean globally Lipschitz, I mean locally Lipschitz on an interval such as $[0,1)$ , for example $f(t,y)=\frac{1}{y-1}$ What about $y'=\arctan(y)$ , this is globally Lipschitz but also has a zero, do the solutions have a discontinuity point? (I don't think this is analytically solvable, but I'm guessing it can still be analyzed using numerical methods)",['ordinary-differential-equations']
4864130,Can you explain how to project this vector onto the complex plane to someone who hasn't learned Differential Geometry or Topology?,"Background I am trying to understand the paper ""On $C^2$ -smooth Surfaces of Constant Width"" by Brendan Guilfoyle and Wilhelm Klingenberg . However, despite my efforts to decipher the paper, I can't follow everything it is describing. This paper is written around complex differential geometry and topology, and appears either to play fast and loose with notation or to rely on conventions I haven't encountered before. I have no prior experience with differential geometry or topology in general, and the (seemingly) flexible use of notation makes it difficult to follow without intimate familiarity with the conventions of the fields in question. I have been unable to find other resources on the same topic that are easier to parse, or lessons on how to understand the concepts in the paper that aren't either behind a paywall or also way over my head. What I Understand So Far In Section 2.1: The Space of Oriented Lines , the authors say Let $𝕃$ be the set of oriented lines, or rays, in $𝔼^3$ . Such a line $γ$ is uniquely determined by its unit direction vector $\vec{U}$ and the vector $\vec{V}$ joining the origin to the point on the line that lies closest to the origin. That is, $$
γ = \{ \vec{V} + r \; \vec{U} ∈ 𝔼^3 \; | \; r ∈ ℝ \}
$$ where r is an affine parameter along the line. By parallel translation, we move $\vec{U}$ to the origin and $\vec{V}$ to the head of $\vec{U}$ . Thus, we obtain a vector that is tangent to the unit 2-dimensional sphere in $𝔼^3$ . The mapping is one-to-one and so it identifies the space of oriented lines with the tangent bundle of the 2-sphere $TS^2$ (see Figure 1). $$
𝕃 = \{ (\vec{U}, \vec{V}) ∈ 𝔼^3 × 𝔼^3 \; | \quad |\vec{U}| = 1 \quad \vec{U} ⋅ \vec{V} = 0\}
$$ $$
\text{Fɪɢᴜʀᴇ 1.}
$$ So far, so good. This all makes sense. In Section 2.2: Coordinates on 𝕃 , the authors introduce $ξ$ as ""the local complex coordinate on the unit 2-sphere in $𝔼^3$ obtained by stereographic projection from the south pole"" in terms of standard spherical polar angles $(θ, φ)$ with the definition $ξ = \tan\left(\frac{θ}{2}\right) e^{i φ}$ . That all checks out. The next thing they say is We convert from coordinates $(ξ, \bar{ξ})$ back to $(θ, φ)$ using $$
\cos(θ) = \frac{1 - ξ \bar{ξ}}{1 + ξ \bar{ξ}},\quad \sin(θ) = \frac{2 \sqrt{ξ \bar{ξ}}}{1 + ξ \bar{ξ}},\quad \cos(φ) = \frac{ξ + \bar{ξ}}{2 \sqrt{ξ \bar{ξ}}},\quad \sin(φ) = \frac{ξ - \bar{ξ}}{2 i \sqrt{ξ \bar{ξ}}}
$$ I know the conversion from unit-spherical coordinates $(θ, φ)$ to Cartesian coordinates is $(\sin(θ) \cos(φ), \sin(θ) \sin(φ), \cos(θ))$ , and these equations all check out. Where I Get Confused Continuing farther into Section 2.2 is where I run into trouble, as the authors add another complex variable $η$ without defining it in a way I understand: This can be extended to complex coordinates $(ξ, η)$ on 𝕃 minus the tangent space over the south pole, as follows. First note that a tangent vector $\vec{X}$ to the 2-sphere can always be expressed as a linear combination of the tangent vectors generated by $θ$ and $φ$ : $$
\vec{X} = X^θ \frac{∂}{∂ θ} + X^φ \frac{∂}{∂ φ}.
$$ In our complex formalism, we have the natural complex tangent vector $$
\frac{∂}{∂ ξ} = \cos\left(\frac{θ}{2}\right)^2 \left(\frac{∂}{∂ θ} - \frac{i}{2 \cos\left(\frac{θ}{2}\right) \sin\left(\frac{θ}{2}\right)} \frac{∂}{∂ φ}\right) e^{-i φ},
$$ and any real tangent vector can be written as $$
\vec{X} = η \frac{∂}{∂ ξ} + \bar{η} \frac{∂}{∂ \bar{ξ}}.
$$ for a complex number $η$ . We identify the real tangent vector $\vec{X}$ on the 2-sphere (and hence the ray in $𝔼^3$ ) with the two complex numbers $(ξ, η)$ . Loosely speaking, $ξ$ determines the direction of the ray, and $η$ determines its perpendicular distance vector to the origin — complex representations of the vectors $\vec{U}$ and $\vec{V}$ . I get that $\vec{U}$ is being projected from $𝔼^3$ into the complex plane as $ξ$ , since $\vec{U}$ is a unit vector with its base on the origin and its head on the unit sphere at position $(θ, φ)$ . However, I don't understand how $\vec{V}$ gets converted to $η$ . $\vec{V}$ is not necessarily of unit length, so it can't be as simple as $\vec{U}$ 's projection. There seem to be a number of different ways to project $\vec{V}$ into the complex plane, and I'm not sure which is correct. Can you explain the conversion between $\vec{V}$ and $η$ to someone whose formal math education hasn't gone past Calc 1? Specifically, based on the next sections of the paper, I need to be able to convert from coordinates $(ξ, η)$ to the vectors $\vec{U}$ and $\vec{V}$ . I completely understand the definition for $\vec{U}(ξ)$ , so I just need to know how to derive a definition for $\vec{V}(η)$ or $\vec{V}(ξ, η)$ .","['linear-algebra', 'differential-geometry']"
4864151,"Under $ad-bc=1$, is every element of a finite field of the form $a^2+b^2+c^2+d^2$?","The Question: Let $x\in\Bbb F_q$ , where $\Bbb F_q$ is the field of $q=p^r$ elements, $p$ prime, $r\in\Bbb N$ . Can we write $$x=a^2+b^2+c^2+d^2\tag{1}$$ for $a,b,c,d\in\Bbb F_q$ such that $ad-bc=1$ ? Thoughts: Relevant theorems include: Lagrange's Four Squares Theorem: Each natural number is the sum of four squares. The number of squares in $\Bbb F_q$ is $q$ if $q$ is even, and $\frac{q+1}{2}$ if $q$ is odd. Each element of a finite field is the sum of two squares. I have tested via GAP that it holds for prime powers up to $q=101$ . Motivation: I was playing around with traces of elements of $\operatorname{SL}_2(\Bbb F_q)$ . I have intuitive, hard-to-articulate reasons to suspect the answer to my question is ""yes"". It would help my research to know an answer; however, field theory is not my forte. I will, of course, credit whoever answers first (if this is a new problem).","['field-theory', 'finite-fields', 'abstract-algebra']"
4864152,How to evaluate $\int\left(\frac{\sin(x)}{2\sin(x)- x(1+\cos(x))}\right)^2dx$?,"I saw this problem: $$\int\left(\frac{\sin(x)}{2\sin(x)- x(1+\cos(x))}\right)^2dx$$ I tried to solve this problem and I found a strange and unsatisfactory solution using differential equations. Let $$I:=\int\left(\frac{\sin(x)}{2\sin(x)- x(1+\cos(x))}\right)^2dx$$ and let $x=2t$ . Then $$I=\frac{1}{2}\int\left(\frac{\sin(t)\cos(t)}{\sin(t)\cos(t)- t\cos^2(t)}\right)^2dt=\frac{1}{2}\int\left(\frac{\sin(t)}{\sin(t)- t\cos(t)}\right)^2dt$$ Since $\frac{d}{dt}\frac{v}{u} = \frac{uv'-vu'}{u^2}$ and the integral is in the form $\frac{uv'-vu'}{u^2}$ , such that $u =\sin(t)- t\cos(t)$ and $u' =t\sin(t)$ , we need to find a function $v(t)$ such that $v'(t)(\sin(t)- t\cos(t)) -v(t)(t \sin(t)) =\sin^2(t)$ or $$-t(  \sin(t)v(t) +\cos(t) v'(t) ) + v'(t) \sin(t) =\sin^2(t)$$ Since $\sin^2(t)$ is not a multiple of $t$ , we can assume that $\sin(t)v(t) +\cos(t) v'(t)=0$ , i.e., $v(t) =- \cos(t)$ , and then $v'(t) \sin(t)=\sin^2(t)$ will solve this differential equation. So $$I=\frac{-\cos(t)}{2(\sin(t)- t\cos(t))}+C = \frac{-\cos(\frac{x}{2})}{2\sin(\frac{x}{2})- x\cos(\frac{x}{2})}+C$$ Although I found this strange solution to this integral, I want to see how to solve it using other methods.","['integration', 'indefinite-integrals', 'calculus', 'alternative-proof']"
4864153,"How to evaluate $\int_{0}^{\frac{\pi}{2}} \frac{\cos(x)}{(1 + \sqrt{\sin(2x)})^n} \,dx$","How to evaluate $$\int_{0}^{\frac{\pi}{2}} \frac{\cos(x)}{(1 + \sqrt{\sin(2x)})^n} \,dx$$ My attempt The transformation of $x \rightarrow \frac{\pi}{2}-x$ yields $$ \int_{0}^{\frac{\pi}{2}} \frac{\cos(x)}{(1 + \sqrt{\sin(2x)})^n} \,dx = \int_{0}^{\frac{\pi}{2}} \frac{\sin(x)}{(1 + \sqrt{\sin(2x)})^n} \,dx$$ $$
\int_{0}^{\frac{\pi}{2}} \frac{\cos(x)}{(1 + \sqrt{\sin(2x)})^n} \,dx = \frac{1}{2} \int_{0}^{\frac{\pi}{2}} \frac{\cos(x) + \sin(x)}{(1 + \sqrt{\sin(2x)})^n} \,dx = \frac{1}{2} \int_{0}^{\frac{\pi}{2}} \frac{\sqrt{(\cos(x) + \sin(x))^2}}{(1 + \sqrt{\sin(2x)})^n} \,dx
$$ $$= \frac{1}{2} \int_{0}^{\pi/2} \frac{\sqrt{1 + \sin(2x)}}{\left(1 + \sqrt{\sin(2x)}\right)^n} \,dx = \int_{0}^{\frac{\pi}{4}} \frac{\sqrt{1 + \sin(2x)}}{\left(1 + \sqrt{\sin(2x)}\right)^n} \,dx = \frac{1}{2} \int_{0}^{\frac{\pi}{2}} \frac{\sqrt{1 + \sin(x)}}{\left(1 + \sqrt{\sin(x)}\right)^n} \,dx$$","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'closed-form']"
4864162,What exactly is the orbit-stabilizer theorem?,"Obviously, being a professional group theorist, I know what the orbit-stabilizer theorem is. Or at least I thought I did. I thought that the orbit-stabilizer theorem was that if $G$ is a finite group acting on a set $X$ , with stabilizer $H$ of a point $x$ , then $|G|$ is the product of $|H|$ and the length of the $G$ -orbit of $x$ . But now I see another version of the orbit-stabilizer theorem, which is that if $G$ acts transitively on a set $X$ , and $H$ is the point stabilizer, then the action is equivalent to the action on the cosets of $H$ . Applying Lagrange's theorem yields the statement that I think is the orbit-stabilizer theorem. I have no name for this statement about transitive actions being equivalent to coset actions. So my question is: how widespread is this second version? Wikipedia calls the second one the orbit-stabilizer theorem, but a sample of lecture notes that Google served up all used the first version. This is important because, in a second course on group theory for example, you might want to use the orbit-stabilizer theorem, and you probably won't go and check the previous lecturer's notes to find out what it is they chose. Named theorems should mean the same to all people, lest we fall down some Wittgensteinian rabbit hole.","['permutations', 'finite-groups', 'orbit-stabilizer', 'group-theory', 'soft-question']"
4864186,Circumscribed (irregular) pentagon,"You're given the consecutive lengths of the sides of an irregular pentagon, and you want to circumscribe this pentagon (whose interior angles are unknown yet) about a circle of unknown radius. Is there a known procedure to do this ? The image below is one that I created in Microsoft Excel using VBA code that implements a Newton-Raphson iteration in the the five interior angles of the pentagon, using consecutive side lengths of $3, 6, 6, 3, 2 $ .","['geometry', 'polygons']"
4864215,"On the asymptotic behavior of a sequence[Bulgaria MO 2024 Regional Round, 12,2]","Let $N$ be a positive integer. The sequence $x_1, x_2, \ldots$ of non-negative reals is defined by $$x_n^2=\sum_{i=1}^{n-1} \sqrt{x_ix_{n-i}}$$ for all positive integers $n>N$ . We aim to prove that : $$x_{n}=\frac{\pi}{8}n+ o(n)$$ For this let $y_{n}:= \frac{x_{n}}{n}$ . We have that : $$y_{n}^{2}= \frac{1}{n} \sum_{i=1}^{n-1} \sqrt{y_{i}\cdot y_{n-i}} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}}~~(1)$$ We can notice from here that due to Rieman sum : $$\frac{1}{n} \sum_{i=1}^{n-1} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}} ~~\to_{n \to +\infty} \int_{0}^{1} \sqrt{x\cdot (1-x)} = \frac{\pi}{8}$$ Now we are left to prove that: $(y_{n})$ converges Taking the limit in $(1)$ lead to : $$\lim~~ y_{n} = \lim ~~\frac{1}{n} \sum_{i=1}^{n-1} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}}=\frac{\pi}{8} $$ For this let $u_{n}:= \frac{1}{n} \sum_{i=1}^{n-1} \sqrt{\frac{i}{n} \cdot \frac{n-i}{n}}$ and $l=\lim u_{n}$ . Consider $r_{n}=\frac{y_{n}}{l}$ we aim to prove that $\lim ~~r_{n}=1$ . For this we have from $(1)$ : $$u_{n}\cdot \min\{y_{k} : k< n\} \leq y_{n}^{2}  \leq u_{n} \cdot \max\{y_{k}: k < n\}$$ But here I can find ways to conclude. Any help or reference if such exercice is standard would be welcome. Thanks.","['limits', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4864308,Definition of flatness for a connection on a vector bundle,"Let $M$ be a Riemannian manifold, $E \to M$ a vector bundle and $\nabla : \Gamma(E) \to \Gamma(T^*M \otimes E)$ a connection on $E$ . I'm a bit confused on the definition of flatness. The word is used a bit vaguely here and there. Lee calls a Riemannian manifold flat if and only if the curvature tensor $R $ vanishes identically. Some authors call the connection flat if $R$ vanishes identically. Is this just a poor choice of words or am I misunderstanding something here? To be clear, I'm trying to understand the definition of a flat connection, but I can't find a good resource anywhere. Someone also defined flat connection by $\nabla^2 = 0$ , but this is different than $R = 0$ right? If $F$ is a $(k,l)$ -tensor field, then $\nabla^2F$ is a $(k,l+2)$ -tensor field and I don't see how $\nabla^2_{X,Y}F= \nabla_X(\nabla_YF) - \nabla_{\nabla_XY}F$ is related to the curvature. Any clarification for this would be appreciated.","['riemannian-geometry', 'differential-geometry']"
4864327,8 people seated around a circular table. Each person has a connection value with each other person. How do you arrange the table to maximize?,"8 people total.  Seated around a circular table.  Hibachi dinner?  You can only talk to the 2 people seated directly next to you. Each person has a connection value with each other person.  eg : #1 and #3  are friendly and have a ""conversation value"" of 7.  #1 and #2 are not friends, and have a ""conversation value"" of 3. Picture a star diagram to represent all connections between all people.  How do you represent this data?  Matrix?    How do you arrange the table to maximize the seating arrangement to result in the highest total connection value? How do I actually solve an example scenario with actual sample values? How do I store this data? How do I solve the maximization?","['matrices', 'optimization', 'discrete-optimization']"
4864346,"Value of $E_{A,y}[\cos(A^T y,A^{-1} y)]$ for Gaussian $A,x,y=Ax$","For $d\times d$ matrix $A$ , $x\in \mathbb{R}^d$ with IID standard normal entries and $y=Ax$ , I'm interested in the the following quantity where $\cos(u,v)$ refers to cosine similarity : $$E_{A,x}[\cos(A^T y,A^{-1} y)]$$ It appears to be $\frac{1}{\sqrt{2}}$ , in simulations, can this be proven? Motivation: This justifies the use of transpose+line search to approximately solve linear equations randn[dims__] := RandomVariate[NormalDistribution[], {dims}];
mat = randn[1000, 1000];
{d2, d1} = Dimensions[mat];
pinv[mat_] := Inverse[mat];

bs = 10000;
vecsIn = randn[bs, d1];
vecsOut = (mat . vecsIn\[Transpose])\[Transpose];
meanAlign[vec1_, vec2_] := 
  Median@MapThread[(#1 . #2/(Norm[#1] Norm[#2])) &, {vec1, vec2}];
Print[""average cosine similarity: "", 
 meanAlign[(mat\[Transpose] . vecsOut\[Transpose])\[Transpose], 
  vecsIn]]","['random-matrices', 'probability-theory', 'probability', 'random-variables']"
4864348,"When does $\mathbb{E}[|X-Y|]\leq\mathbb{E}[X]$ hold for $X,Y\geq 0$ i.i.d.?","Let $X,Y\geq 0$ be i.i.d. random variables with a continuous distribution with PDF $f$ and CDF $F$ . My question is: Under what conditions does the following inequality hold, \begin{align}
\mathbb{E}[|X-Y|] \leq \mathbb{E}[X].
\end{align} If $X,Y\sim Exp(\lambda)$ , then the above identity can easily be shown to hold with equality. Is it the case that if the distribution is more heavy-tailed than the exponential distribution, then the inequality does not hold? Formally, this would correspond to the hazard rate $f(x)/(1-F(x))$ being nondecreasing (for the exponential distribution the hazard rate is constant).","['probability-distributions', 'stochastic-processes', 'probability-theory', 'probability', 'random-variables']"
4864377,Why is the Brownian Motion related to the Normal Distribution?,"I am trying to learn more about the Inverse Gaussian Distribution and its applications (e.g. First Passage Time). First, define a Brownian Motion with Drift : If $X(t) = \mu t + W(t)$ represents a Brownian motion with drift $\mu$ , where $W(t)$ is a standard Brownian motion (i.e. $W(t) \sim N(0,t)$ ). Here, the initial position of this Brownian Motion is 0, i.e. $W(t=0)=0$ . The drift term is important because without drift, the Inverse Gaussian Distribution is not defined. Next, define First Passage Time : The first passage time $T$ is the time it takes for the process $X(t)$ to reach a certain level $a > 0$ for the first time. Finally, define the Inverse Gaussian Distribution : The first passage time $T$ follows an Inverse Gaussian distribution with parameters $\mu = \frac{a}{\mu}$ and $\lambda = a^2$ , i.e., $T \sim IG(\frac{a}{\mu}, a^2)$ : $$
f(x;\mu,\lambda) = \left(\frac{\lambda}{2\pi x^3}\right)^{1/2} e^{ -\frac{\lambda(x-\mu)^2}{2\mu^2 x} }
$$ where: $x > 0$ is the variable $\mu > 0$ is the mean $\lambda > 0$ is the shape parameter. Note: It's clear to see that if the drift term $\mu$ is 0, we would end up with division by zero when trying to define the mean of the Inverse Gaussian distribution, which makes it undefined:  When $\mu = 0$ , the term $\frac{\lambda(x-\mu)^2}{2\mu^2 x}$ in the exponent of the distribution becomes $\frac{\lambda x^2}{2 \cdot 0^2 \cdot x}$ , which is undefined because we're dividing by zero. Here is where I get confused: I can not understand the relationship between the Brownian Motion with Drift and the Inverse Gaussian Distribution. That is, (given some initial conditions) is it possible to mathematically manipulate a Brownian Motion $X(t)$ and mathematically demonstrate that the time at which $X(t)$ will reach a certain point is indeed given by the Inverse Gaussian Distribution? For example, suppose $T$ is the first passage time of a Brownian motion with drift to reach a certain level $a > 0$ . $T$ follows an Inverse Gaussian distribution with parameters $\mu = \frac{a}{\mu}$ and $\lambda = a^2$ , i.e., $T \sim IG(\frac{a}{\mu}, a^2)$ . The probability that $T$ falls in the interval $[t_1, t_2]$ can be written as: $$
P(t_1 \leq T \leq t_2) = \int_{t_1}^{t_2} f(t;\mu,\lambda) dt
$$ where $f(t;\mu,\lambda)$ is the probability density function of the Inverse Gaussian distribution: $$
f(t;\mu,\lambda) = \left(\frac{\lambda}{2\pi t^3}\right)^{1/2} e^{ -\frac{\lambda(t-\mu)^2}{2\mu^2 t} }
$$ The expected value of $T$ is equal to the mean parameter of the Inverse Gaussian distribution, which is $\mu = \frac{a}{\mu}$ . This represents the average amount of time needed for the Brownian motion with drift to reach the level $a$ . But I am still confused: Why is the distribution of the first passage time an Inverse Gaussian distribution in the first place? I get it that a Brownian Motion is related to a Gaussian Distribution via the Wiener Process $W(t)$ ... but why is the distribution of the first passage time an Inverse Gaussian distribution? Thanks! PS: I tried to write the Inverse Gaussian Distribution for a Brownian Motion that does not start at 0. If we define a Brownian Motion with drift such that the position at time =0 is given by $x_0$ : $$X(t) = x_0 + \mu t + W(t)$$ Then the Inverse Gaussian Distribution for this modified Brownian Motion becomes: $$T' \sim IG\left(\frac{a - x_0}{\mu}, (a - x_0)^2\right)$$ $$
f(t;\mu',\lambda') = \left(\frac{\lambda'}{2\pi t^3}\right)^{1/2} e^{ -\frac{\lambda'(t-\mu')^2}{2\mu'^2 t} }
$$ where $\mu' = \frac{a - x_0}{\mu}$ and $\lambda' = (a - x_0)^2$ .","['stopping-times', 'brownian-motion', 'probability-theory']"
4864456,Trace Class Operators On Manifolds With Boundary,"Let $X$ be an $n$ -dimensional manifold with nonempty boundary $\partial X$ and $n\geq 2$ . Proposition 4.1 of this paper by Schrohe states that it is ""not very difficult"" to show: Proposition : A bounded operator on $L^2(X)$ with range in $H^{n+1}(X)$ is trace class. Tragically, I cannot figure out a proof. I've tried generalizing the approach outlined in chapter 8 of Roe's Elliptic Operators, Topology and Asymptotic Methods for the boundaryless case, I've thought about Mercer's Theorem but the setting seems quite different, and looked around in Hörmander's PDEs book III and I don't think it's in there. A hint, proof outline or reference would be greatly appreciated, thank you!","['reference-request', 'operator-theory', 'functional-analysis', 'differential-geometry']"
4864457,I need an example of a function such that the derivative minus x is the same as the original function [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question I'm curious if there is a function F(x) whose derivative f(x) is the same as F(x) + x. I tried to make one myself by adding things to e^x with no success. the function was mentioned in Question 21 of the academic assessment conducted in Korea in March 2021. It didn't mention the specific function, and the question was solvable without it. But I'm curious what that function would be. The test was for high schoolers, so I think the function will not wander outside the context of Korean High School Math curriculum.(The only Transcendental function taught in Korea's high school math curriculum is logarithmic function, trigonometric functions, and exponential function)","['integration', 'calculus', 'ordinary-differential-equations']"
4864497,"Exercise 2.14 in Villani's ""Topics in Optimal Transportation""","Exercise 2.14 in Villani's ""Topics in Optimal Transportation"" asks us to show the following: Let $Q=[0,1]^{n-1}$ , and $\mu$ is uniform on $Q\times \{0\}$ , $\nu$ uniform on $Q\times \{1\} \cup Q\times \{-1\}$ , with the cost function $c(x,y)=|x-y|^2$ . Then the only solution to the Kantorovich problem is $\pi=(\delta_{x_1=1}+\delta_{x_1=-1})/2$ . Additionally, the Monge problem has no solution. To make it a bit simpler, I take $n=3$ .
My questions are the following: I am having trouble even understanding the way that $\pi$ is written. Intuitively, one just needs to split the mass at each point of $Q\times \{0\}$ and transfer half of it to $Q\times \{1\}$ and the other half to $Q\times \{-1\}$ , or, visually, stack three unit squares on top of each other with a unit distance between them, then $\mu$ 's mass is concentrated on the middle one, and each point just splits half the mass perpendicularly to both the upper and the lower unit square ( the union of which is the support of $\nu$ ). But I do not see how $\pi=(\delta_{x_1=1}+\delta_{x_1=-1})/2$ is supposed to mean what I just described. Why $x_1$ ? I simply cannot understand what he means here. I am assuming we are to use Knott-Smith to show that the this is the unique solution to the Kantorovich problem, but I have no clue where to start, though this could also be related to the fact that I cannot understand how $\pi$ is represented here. Any ideas and hints would be great! Thank you very much in advance! Kind regards!","['measure-theory', 'optimal-transport']"
4864520,Different definitions of a sheaf,"In SHEAVES, COSHEAVES AND APPLICATIONS , the author introduced a different definition for sheaves. Namely, Definition : X is a topological space and $\mathcal{D}$ is a category, e.g. $\mathbf{Sets}$ , $\mathbf{Vect}$ , e.t.c.. Suppose $\mathcal{F}:\mathbf{Open}(X)^{op}\rightarrow \mathcal{D}$ is a presheaf, $\mathcal{U}=\{U_i\}$ is an open cover of a open set $U\subset X$ , we say that $\mathcal{F}$ is a sheaf on $U$ if the unique map from $\mathcal{F}(U)$ to the limit of $\mathcal{F}\circ \iota^{op}_\mathcal{U}$ is an isomorphism. Here $\iota_\mathcal{U}$ is the canonical functor from nerve $\mathcal{N}(\mathcal{U})$ to $\mathbf{Open}(X)$ . In section 2.2, the author left an exercise to prove this definition is equivalent to the sheaf axioms of locality and gluing (see Sheaf (mathematics) ). He implies that we should utilize a structure theorem in category theory: limits can be expressed by products and equalizers (Theorem 2.2.1). However, I haven’t made any progress following this idea. The author said that “ Limit or colimit over the nerve of a cover can be determined after considering only the elements of the cover and their pairwise intersections. Do this by observing that the limit or colimit over the 1-skeleton of the nerve defines a cone or cocone over the whole nerve and employing universal properties.” To start with, is it possible to prove $\prod \mathcal{F}(U_i)\cong \prod \mathcal{F}(U_I)$ , where $I$ corresponds to all objects in $\mathcal{N}(\mathcal{U})$ ?","['algebraic-geometry', 'category-theory', 'algebraic-topology', 'sheaf-theory']"
4864564,Combinations of indistinguishable marbles,"Let's consider this problem: A bag contains 5 black marbles and 6 white ones. Marbles of the same color are indistinguishable from each other. If I draw two marbles, what is the probability they have different colors? This problem can be trivial if we reason like this: $P(first = black, second=white) = P(first = black)*P(second=white|first = black) = \frac{5}{11} * \frac{6}{10} = \frac{3}{11}$ $P(first = white, second=black) = P(first = white)*P(second=black|first = white) = \frac{6}{11} * \frac{5}{10} = \frac{3}{11}$ Hence summing the two $P(\text{both black and white}) = \frac{6}{11}$ . If we frame the problem using combinations we can get the same result using this: $P(\text{both black and white}) = \frac{\binom{6}{1}\binom{5}{1}}{\binom{11}{2}} = \frac{6}{11}$ This is like the number of combinations of choosing 1 marble from the white, 1 marble from the blacks and divide then by the number of combinations of choosing 2 marbles from the 11. Is it a coincidence or is it correct this way of solving it?
I cannot figure out how I should be allowed to use $\binom{11}{2}$ at denominator since to me there are at most 3 different ways to get a group of two marbles from the 11 namely (BB, BW, WW) and not 55 like $\binom{11}{2}$ states. Please clarify this.","['combinations', 'binomial-coefficients', 'combinatorics', 'probability']"
4864595,"For $f:\Bbb P^1\to\Bbb P^1$ by $[x:y]\mapsto [x^2:y^2]$, show $f_*\mathcal{O}_{\Bbb P^1}\cong \mathcal{O}\oplus\mathcal{O}(-1)$","I am currently preparing for an exam in algebraic geometry and came across the following exercise: Let $k$ be an algebraically closed field. Let $f\colon\mathbb{P}_k^1\to \mathbb{P}_k^1$ , $[x,y]\to [x^2,y^2]$ . Show that $f_*(\mathcal{O}_{\mathbb{P}_k^1})$ is a vector bundle $\mathcal{E}$ on $\mathbb{P}_k^1$ , which has rank 2. Show that $\mathcal{E}\cong \mathbb{P}_k^1\oplus\mathbb{P}_k^1(-1)$ Somebody showed me a proof that used that $f_*(\mathcal{O}_{\mathbb{P}_k^1})$ is the universal scheme over $\mathbb{P}_k^1$ containig a square root and some properties about $\text{Spec}$ underlined. But I didn't really understand that and feel that is seems unecessarily complicated. I would like to give a more concrete proof showing first that $f_*(\mathcal{O}_{\mathbb{P}_k^1})(U)$ and $f_*(\mathcal{O}_{\mathbb{P}_k^1})(V)$ are both $\mathcal{O}_{\mathbb{P}_k^1}$ -modules of rank 2 where $U\cup V= \mathbb{P}_k^1$ is the standard cover and then gluing those along transition maps (which I think should work and would prove 1. and 2.). However I seem to be confused on what the module $\mathcal{O}_{\mathbb{P}_k^1}$ -module strucure on $f_*(\mathcal{O}_{\mathbb{P}_k^1})$ actually is since I somehow can't work it out. I would be thankful if somebody could show me/teach me how such a concrete proof might go since I am lost in defintions!",['algebraic-geometry']
4864597,"Gaussian fit for dice rolls has peculiar constants, looking for their origin","For a programming project I want to describe huge quantities of dice rolls using a gaussian. Hence I set out to find a relation between: $size$ = the number of sides a single die has, $amount$ = the number of same sided dice rolled at once, and the gaussian that describes the probability of rolling a specific total sum $x$ with all of these dice. Dice in this case mean some abstract object that on 'rolling' give a random integer number between 1 and $size$ (inclusive), just like normal dice would. Let the gaussian be: $$ P(x) = A * e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$ Where usually $A = \frac{1}{\sigma \sqrt{2 \pi}}$ is used as normalisation constant while $\mu = amount * \frac{size + 1}{2}$ is straightforward. Now, using several parameters for $size$ and $amount$ I fitted simulated distributions to the gaussian using $A$ and $\sigma$ as fitting parameters. I found the following holds true in very good approximation when $size$ and $amount$ are both bigger than like 4: $$\sigma = a * size * \sqrt{amount}$$ $$A = \frac{b}{size * \sqrt{amount}}$$ with the two constants: $$a \approx 0.28915937$$ $$b \approx 137445.476$$ Now I am asking myself (and in extension you) if the constants $a$ and $b$ that appear here can somehow be explained. I would be surprised if these are just some *shrug* numbers that randomly appear in a system as simple as same-sided dice and would be much more satisfied if they could be explained and maybe consist of well-known numbers like $\pi$ and $e$ . Please help me satisfy my curiosity here! I am aware that I am using a continuous probability distribution function to describe discrete events but I am pretty sure this does not invalidate the question. Also I did not study mathematics, so please assume that I only have surface knowledge regarding black magic aka mathematics. edit: I should add that the distribution I fitted is a histogram of $x$ over 100k simulated rolls. Therefore the integral over the whole distribution is not 1 and $b$ is large for this reason. I am mainly interested in the $\sigma$ .","['statistics', 'probability-distributions']"
4864623,Finding the ideal B-spline through data points using Euler-Lagrange: is it just too hard to do?,"I am not even sure I have a question anymore (I will just give up)... in the past month or so I have been researching cubic Bézier curves. The idea was to find a fit through data points, using piecewise Bézier curves and the Euler-Lagrange equation to minimize an ""energy"" proportional with the square of the length of the (total) curve (elastic energy used to stretch) plus the integral of the square of the curvature $\kappa$ of the curve (energy required to bend the curve). Edit: it took me three whole days to type this ""question"". Please don't get mad for wasting your time if you choose to read it. Here is what I have done so far with respect to the length of the curve. Let $$\begin{equation}\begin{aligned}\vec{P}(t) &= (1-t)^3 \vec{P_0} + 3(1-t)^2t \vec{C_0} + 3(1-t)t^2 \vec{C_1} + t^3 \vec{P_1} \\
&= \vec{P_0} + 3(\vec{C_0} - \vec{P_0})t + 3(\vec{C_1} - 2\vec{C_0} + \vec{P_0})t² + (\vec{P_1} - 3\vec{C_1} + 3\vec{C_0} - \vec{P_0})t³\end{aligned} \end{equation}\tag{1}$$ be a cubic Bézier curve defined by its begin and end points $\vec{P_0}$ and $\vec{P_1}$ , and control points $\vec{C_0}$ and $\vec{C_1}$ . Instead of $\{\vec{P_0}, \vec{C_0}, \vec{C_1}, \vec{P_1}\}$ to define the curve, lets use $\{\vec{B}, \vec{V}, \vec{A}, \vec{J}\}$ where $$\begin{align}\vec{B}&=\vec{P_0} \\
\vec{V}&=3(\vec{C_0} - \vec{P_0}) \\
\vec{A}&=6((\vec{C_1} - \vec{P_0}) - 2(\vec{C_0} - \vec{P_0})) \\
\vec{J}&=6((\vec{P_1} - \vec{P_0}) - 3(\vec{C_1} - \vec{C_0}))\end{align} \tag{2}$$ This is not random, you will thank me later; the idea behind it is that the coefficients other than the constant term represent the velocity, acceleration and jerk (if $t$ was ""time"") with respect to $t$ , which only depends on the shape of the curve, not its position. Therefore those coefficients should be expressed in terms of differences between any two of $\{\vec{P_0}, \vec{C_0}, \vec{C_1}, \vec{P_1}\}$ . Substituting this into $(1)$ gives, $$\vec{P}(t) = \vec{B} + \vec{V}t + \frac{1}{2}\vec{A}t^2 + \frac{1}{6}\vec{J}t^3 \tag{3}$$ Then the chord length of the curve between $\vec{P_0}$ and $\vec{P_1}$ is given by $$chord\_length = \int_0^1 \lVert \vec{P'}(t) \rVert \,dt \tag{4}$$ where $\vec{P'}(t)$ is the derivative of $\vec{P}(t)$ with respect to $t$ , also called the velocity : $$\vec{P'}(t) = \vec{V} + \vec{A}t + \frac{1}{2}\vec{J}t^2 \tag{5}$$ In order to calculate the norm $\lVert \vec{P'}(t) \rVert$ we can take the square root of the dot product with itself: $$\lVert \vec{P'}(t) \rVert = \sqrt{\vec{P'}(t) \cdot \vec{P'}(t)} \tag{6}$$ Working out this dot product we get a fourth degree polynomial in $t$ : $$\vec{P'}(t) \cdot \vec{P'}(t) = c_4 t^4 + c_3 t^3 + c_2 t^2 + c_1 t + c_0 \tag{7}$$ where $$\begin{aligned}c_0 &= \lVert \vec{V} \rVert^2 \\
c_1 &= 2 (\vec{V} \cdot \vec{A}) \\
c_2 &= \vec{V} \cdot \vec{J} + \lVert \vec{A} \rVert^2 \\
c_3 &= \vec{A} \cdot \vec{J} \\
c_4 &= \frac{1}{4}\lVert \vec{J} \rVert^2\end{aligned} \tag{8}$$ Note how all of this holds for vectors of dimension two or more . We're not talking about $x$ and $y$ or whatever. We might also be interested in a polynomial in $u$ after a substitution $t = u + s$ , where we'll integrate from $u=-s$ to $u=-s+1$ . After substitution we end up with: $$\begin{align}chord\_length &= \int_{-s}^{-s+1} \sqrt{c_4' u^4 + c_3' u^3 + c_2' u^2 + c_1' u + c_0'} \space \,du \\
&= \sqrt{c_0'} \int_{-s}^{-s+1} \sqrt{\frac{c_4'}{c_0'}u^4 + \frac{c_3'}{c_0'}u^3 + \frac{c_2'}{c_0'} u^2 + \frac{c_1'}{c_0'} + 1}\space\space \,du \end{align} \tag{9}$$ where $$\begin{aligned}c_0' &= c_0 + c_1 s + c_2 s^2 + c_3 s^3 + c_4 s^4 \\
c_1' &= c_1 + 2 c_2 s + 3 c_3 s^2 + 4 c_4 s^3 \\
c_2' &= c_2 + 3 c_3 s + 6 c_4 s^2 \\
c_3' &= c_3 + 4 c_4 s \\
c_4' &= c_4\end{aligned} \tag{10}$$ Let $$\begin{aligned}d_1 &= \frac{c_1'}{c_0'} \\
d_2 &= \frac{c_2'}{c_0'} \\
d_3 &= \frac{c_3'}{c_0'} \\
d_4 &= \frac{c_4'}{c_0'}\end{aligned} \tag{11}$$ So that the problem of finding the chord length becomes integration of $\sqrt{1 + d_1 u + d_2 u^2 + d_3 u^3 + d_4 u^4}$ . Since there is no known algebraic expression for that, we can proceed by finding the Taylor series of this square root around $u=0$ ( $t=s$ ). Lets write $$\sqrt{1 + d_1 u + d_2 u^2 + d_3 u^3 + d_4 u^4} = 1 + \sum_{n=1}^{\infty} \frac{1}{n}g_n u^n \tag{12}$$ then the following recursive relationship for the $g_n$ can be found (with three days of hard work): $$\begin{aligned}
g_1 &= +\frac{1}{2}d_1 \\
g_2 &= -\frac{1}{2}d_1 g_1 + \frac{1}{1}d_2 \\
g_3 &= -\frac{3}{4}d_1 g_2 - \frac{0}{1}d_2 g_1 + \frac{3}{2}d_3 \\
g_4 &= -\frac{5}{6}d_1 g_3 - \frac{1}{2}d_2 g_2 + \frac{1}{2}d_3 g_1 + \frac{2}{1}d_4 \\
g_5 &= -\frac{7}{8}d_1 g_4 - \frac{2}{3}d_2 g_3 - \frac{1}{4}d_3 g_2 + \frac{1}{1}d_4 g_1 \\ 
&\space\space\vdots \notag \\
g_n &= -\frac{2n-3}{2n-2}d_1 g_{n-1} - \frac{n-3}{n-2}d_2 g_{n-2} - \frac{2n-9}{2n-6}d_3 g_{n-3} - \frac{n-6}{n-4}d_4 g_{n-4}\end{aligned} \tag{13}$$ This Taylor series should converge if $u$ is close enough to zero. In fact, it turns out that for tame curves where, say the deviation from a straight line is less than a third of the distance between $\vec{P_0}$ and $\vec{P_1}$ , one can calculate the chord length with a single integral taking $s=1/2$ and integrating from $-1/2$ to $+1/2$ . Otherwise we might have to do it piecewise and add up a few chord length pieces. For example one could pick six pieces where $s$ is one of $\{\frac{1}{12}, \frac{3}{12}, \frac{5}{12}, \frac{7}{12}, \frac{9}{12}, \frac{11}{12} \}$ and then integrate (six times) from $-\frac{1}{12}$ to $+\frac{1}{12}$ . Thus, in general we are interested in obtaining the integral between $-\delta$ and $+\delta$ where $\delta$ is some fixed, positive real less than or equal $1/2$ . Combining equation $(9)$ through $(13)$ we can write, $$chord\_length\_piece = \sqrt{c_0'} \int_{-\delta}^{\delta}\left(1+\sum_{n=1}^{\infty}\frac{1}{n}g_n u^n\right)\,du \tag{14}$$ Assuming $\delta$ is small enough that the sum converges, we can swap the sum and integration and get $$chord\_length\_piece = 2\delta\sqrt{c_0'} \left( 1 + \sum_{even\ n\geq 2}^{\infty} \frac{\delta^n}{n(n+1)}g_n \right) \tag{15}$$ Note that only the $g_n$ for even $n$ are left because the odd ones cancel. For example $\int_{-\delta}^{\delta} g_1 u\,du = \left[\frac{1}{2}g_1u^2\right]_{-\delta}^{\delta} = \frac{1}{2}g_1\delta^2 - \frac{1}{2}g_1(-\delta)^2 = 0$ . The other terms are doubled, hence the leading 2. I also moved one factor of $\delta$ to the front, making all exponents of $\delta$ equal to the index of $g_n$ again. For a straight line, the sum is zero and $\vec{c_0'} = \lVert \vec{V_s} \rVert^2$ the square of the velocity at $t=s$ . The total chord length then can be found by summing $(15)$ for all values of $s$ involved. Let $\delta = \frac{1}{2N}$ for some positive integer $N$ , the number of pieces we will cut the curve into. Then we need $N$ values of $s$ : $$s = (2k + 1)\delta \qquad\text{ for } k = \mathbin{{0}{...}{N}{-}{1}}$$ and $$\begin{aligned}chord\_length &= 2 \delta \sum_{k=0}^{N-1} \sqrt{c_0'^{(s)}} \left( 1 + \sum_{even\ n\geq 2}^{\infty} \frac{\delta^n}{n(n+1)}g_n^{(s)} \right) \\
&= 2 \delta \sum_{k=0}^{N-1} \sqrt{c_0'^{(s)}}
 + 2 \delta \sum_{even\ n\geq 2}^{\infty} \frac{\delta^n}{n(n+1)} \sum_{k=0}^{N-1} g_n^{(s)} \end{aligned} \tag{16}$$ Recall that the $g_n$ are a function of the $d_i$ which in turn are a function of the $c_i'$ which are polynomials in $s$ (hence the notation $g_n^{(s)})$ with coefficients $c_i$ , where $s = (2k + 1)\delta$ . Since $\delta$ is a known, fixed constant, the second term of equation $(16)$ is a multivariate polynomial in the $c_i$ . The first term contains the $\sqrt{c_0'^{(s)}}$ , which will carry over to a lot of other terms after squaring the length; but is still straightforward to differentiate. Note that $c_0'$ is the square of the velocity at $t=s$ , so that if we make $N$ arbitrary large (causing $\delta$ to go to zero) the first sum turns into the original integral (equation $(4)$ ). At this point I feel like giving up though: assuming I can come up with a differentiable equation for the bending energy part, I am starting to think this is going to be a dead-end. In order to calculate the ideal curve I'd have to minimize the energy of the whole curve, all Bézier curves combined, where one curve influences the next only due to the demand that the curve must have $G_1$ continuity. Am I right to give up, or does anyone have an idea that can be done in a reasonable amount of time?","['multivariable-calculus', 'bezier-curve', 'arc-length', 'euler-lagrange-equation']"
4864661,High School Combinatorics - I do not agree with the provided explanation,"The proposed problem was the following (translated into English by me): ""A school has $16$ students interested in participating in a team competition. First, they must split themselves into $8$ pairs. Then, each of those pairs must choose (among themselves) someone to be their leader. Finally, two teams of $8$ students are formed by choosing $4$ pairs to compose the first team, which already locks the opposing team. Considering two teams to be equal if, and only if, they are composed of the same pairs (with pairs being equal if they are composed of the same students, with the same leader), in how many distinct ways can these $16$ students divide themselves into two teams of eight?"" Solution Provided (translated by me): First, compute the total amount of distinct pairs that can exist. This is clearly given by $C_{16,2} = \frac{16!}{14!2!} = 15\cdot8$ . Since each pair has two choices of leader, we also multiply by two, to obtain $16\cdot15 = 240$ Now, multiply the amount of distinct pairs by the total number of ways of choosing $4$ out of the $8$ formed pairs to obtain the final answer: $C_{8,4} \cdot 240 = \frac{8!}{4!4!} \cdot 240 = 70 \cdot 240 = 16800$ My issues: I don't believe that using $C_{16,2}$ makes any sense, as (in my mind) after you choose the first pair, you cannot choose any other pair that contains either of the students in the first one you chose (and so on). I also don't believe this number correctly represents the total amount of ways of forming 8 pairs under these conditions, which is the number that would make sense to use in place of $C_{16,2}$ (in my opinion). My attempt: We begin by computing the number of ways of constructing $8$ distinct pairs out of $16$ students. For the first pair, we have $16 \choose 2$ possible ways. For the second pair, $14\choose 2$ . And so on until the final pair which is $2 \choose 2$ . So the total number of ways of assembling 8 pairs out of 16 students seems to be given by: $\prod_{n=1}^{8}{2n \choose 2}$ Which works out quite nicely to $\frac{16!}{2^{8}}$ But this is not quite the number I am looking for yet. Since I don't actually care about the order of the pairs, I have to divide by $8!$ , since when multiplying them out like I did above, I consider choosing some pair $A$ before some pair $B$ different then choosing first $B$ and then $A$ , when it clearly is not. So, the final number works out to: $\frac{16!}{8!2^{8}}$ . Like it was done in the official solution, I must now multiply by $2^{8}$ , since each individual has two choices of leaders. This gives me the total number of ways of forming $8$ distinct pairs with leaders which is $\frac{16!}{8!}$ The conclusion is now the same as in the official solution: We now multiply by the amount of different ways of choosing $4$ out of the $8$ pairs. This gives: $\frac{16!}{8!}\cdot C_{8,4} = \frac{16!}{8!}\cdot\frac{8!}{4!4!} = \frac{16!}{4!4!}$ And finally, we must divide by two since choosing, for example, pairs $A,B,C,D$ (out of pairs $A,B,C,D,E,F,G$ ) or choosing $D,E,F,G$ makes no difference on the resulting two teams. So my final answer turns out to be $\frac{16!}{4!\cdot 4!\cdot 2}$ , which is not even close to the number obtained in the official one. The question: Which one (if any) is right, and why is the other one (or both) wrong?","['solution-verification', 'combinatorics']"
4864669,"Estimating $\sqrt{n}\approx a+\frac{n-a^2}{b^2-a^2}$, where $a^2$ and $b^2$ are the perfect squares nearest non-square $n$. Is it already known?","A long time ago, I developed a technique for estimating irrational square roots.  I accepted it for the longest time as accurate as I noticed it produced consistently accurate results, though the calculation was always an underestimate. Only recently have I developed the personal tools to prove this technique works for larger and larger irrational square roots.  That is, I was able to prove that my technique converges towards the square root function as we estimate irrational roots towards infinity. Now, I am curious, is this technique already known?  If so, does it have a name? My technique: Let $n$ be a positive integer such that $\sqrt{n}$ is not an integer. Find the nearest integers to $n$ that are perfect squares.  Call them $a^2$ and $b^2$ . Then my estimate will be: $$\sqrt{n} = a+ \frac{n-a^2}{b^2-a^2}$$ For example: Estimate $\sqrt{160452}$ $$\begin{align}
\sqrt{160452} &\approx 400 + \frac{160452 - 400^2}{401^2 - 400^2} \\[4pt]
\sqrt{160452} &\approx 400.56460153139 \\[4pt]
\frac{160452 - 400^2}{401^2 - 400^2} &\approx 400.56429463171
\end{align}$$","['limits', 'convergence-divergence', 'real-analysis']"
4864688,Area of Shaded Region,"I am using the Pearson Edexcel A-Level maths books for my A-Level studies and stumbled across a question I do not understand (Exercise 3G Question 8d). The last part of the question confused me since it asked me to calculate the area of the shaded region. I checked the solution bank after attempting the question and found that it was using the area of two triangles and taking away the distance between them, despite the smaller triangle not being a right-angled triangle and no angles being provided, hence my confusion. Below is the question in full, a diagram of what I have mentioned above along with the coordinates of each vertices. Question 8.) a.) On a coordinate grid, shade the region which satisfies the following inequalities $y \lt x + 4$ , $y + 5x + 3 \ge 0$ , $y \ge 1$ and $x \lt 2$ b.) Work out the coordinates of the vertices of the shaded region c.) Which of the vertices lie within the region identified by the inequalities? d.) Work out the area of the shaded region. Diagram Diagram of the problem Coordinates of Verticies $$A = (-7/6, 17/6),
B = (2, 6),
C = (-2/5, -1),
D = (2, -1)$$ My Confusion I understand to do this finding the distance between the points B and D is 7, and D and C is also 7, therefore the area of the large right-angled triangle which can be seen in the diagram is $49\over2$ . The part which has stumped me is how the area of the region would be found since as mentioned before there is no angle given and the other triangle enclosed within the area calculated is not a right-angled triangle. However, the solution bank claims the following is the solution. Solution bank answer for question 8d I assume I am missing something but I cannot work out what.
Thank you in advance for your help!","['inequality', 'area', 'linear-algebra', 'geometry']"
4864720,Solve $\int_0^\infty\frac{\ln(2e^x-1)}{e^x-1}dx$,"In one of the final problems of the MIT integration bee for this year, $$I=\int_0^\infty\frac{\ln(2e^x-1)}{e^x-1}dx$$ was one of the given problems. My try was to let $u=e^x-1$ to get $$I=\int_0^\infty\frac{\ln(2u+1)}{u(u+1)}du=\int_0^\infty\frac{\ln(x+1)}{x(x+2)}dx$$ I don't know whether I would be right but I had a feeling this was a dead end. Turning the original integral into a geometric series doesn't seem promising either. How should I solve this? Note: These problems are solved in 5 minutes so please come up with a solution that can be done in such a time limit.","['integration', 'calculus', 'contest-math']"
4864752,"how to evaluate $\int_{0}^{1} \int_{0}^{1} \frac{\ln(1 - xy) \cdot \text{Li}_{4}(1 - x)}{x(1 - x)(1 - xy)} \,dy\,dx$","how to evaluate $$\int_{0}^{1} \int_{0}^{1} \frac{\ln(1 - xy) \cdot \text{Li}_{4}(1 - x)}{x(1 - x)(1 - xy)} \,dy\,dx$$ My attempt $$ \Omega =\int_{0}^{1} \int_{0}^{1} \frac{\ln(1 - xy) \cdot \text{Li}_{4}(1 - x)}{x(1 - x)(1 - xy)} \,dy\,dx$$ $$
* \Omega = \int_{0}^{1} \frac{\text{Li}_{4}(1 - x)}{x(1 - x)} \,dx \cdot \int_{0}^{1} \frac{\ln(1 - xy)}{1 - xy} \,dy
$$ $$
= \int_{0}^{1} \frac{\text{Li}_{4}(1 - x)}{x(1 - x)} \,dx \cdot \left[-\frac{1}{x} \int_{0}^{1} \frac{\ln(1 - xy)}{1 - xy} \,d(1 - xy)\right]
$$ $$
= \int_{0}^{1} \frac{\text{Li}_{4}(1 - x)}{x(1 - x)} \,dx \cdot \left[-\frac{1}{2x} \ln^2(1-xy)\bigg|_{0}^{1}\right]
$$ $$= - \frac{1}{2} \int_{0}^{1} \frac{\ln^2(1 - x) \cdot \text{Li}_{4}(1 - x)}{x^2(1 - x)} \,dx$$","['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'closed-form']"
4864798,Show that $z$ is a root of unity,"I want to show that if $z\in\mathbb{C}$ such that $|z|=1$ and $1+z^{k_1}+z^{k_2}=0$ for integers $k_1<k_2$ , than $z$ is a root of unity. Here is my approach:
Suppose $z=\cos(\theta)+i\sin(\theta)$ . Then we have $$1+\cos(k_1\theta)+\cos(k_2\theta)+i(\sin(k_1\theta)+\sin(k_2\theta))=0.$$ This would require $$\sin(k_1\theta)+\sin(k_2\theta)=2\sin\left(\frac{(k_1+k_2)\theta}{2}\right)\cos\left(\frac{(k_2-k_1)\theta}{2}\right)=0.$$ This would mean either $$\frac{(k_1+k_2)\theta}{2}=n\pi$$ or $$\frac{(k_2-k_1)\theta}{2}=\frac{(2n+1)\pi}{2}$$ so either $$\theta=\frac{2n\pi}{k_1+k_2}, \frac{(2n+1)\pi}{k_2-k_1}$$ Now note that $$1+\cos(k_1\theta)+\cos(k_2\theta)=1+2\cos\left(\frac{(k_1+k_2)\theta}{2}\right)\cos\left(\frac{(k_2-k_1)\theta}{2}\right)$$ But now I do not know how to continue, because I would need the real part to be $0$ . Any hints on how to continue? My guess is that it would be a $k_1+k_2$ -root of unity. Also, is there an algebraic way/trick of doing this without resorting to using polar forms and trig identities? Thank you.","['complex-analysis', 'complex-numbers']"
4864799,Solving $\int_1^3\frac{1+\frac{1+...}{x+...}}{x+\frac{1+...}{x+...}}dx$,"In the Regular season of the MIT Integration bee, the following integral was given $$I=\int_1^3\frac{1+\frac{1+...}{x+...}}{x+\frac{1+...}{x+...}}dx$$ If the integrand is called $f(x)$ then I am guessing that $f(x)=\frac{1+f(x)}{x+f(x)}$ implying $f(x)=\frac{1-x+\sqrt{(x-1)^2+4}}2$ . So we now have $$I=\int_2^4\sqrt{x^2+4}dx-1=\sqrt2-1+\log(\sqrt2+1)$$ Is there a way to arrive at this, particularly without even finding $f(x)$ ?","['integration', 'calculus', 'contest-math']"
4864802,Volume of a great icosahedron,"This is the image of a Great Icosahedron that I obtained starting from the coordinates of the vertices, as $A$ , $B$ $C$ , etc.. Now I want to calculate the volume of the solid. In internet (as in this page of Wolfram ) I can find a formula for this volume, $$
V=\frac{l^3}{4}(25+9\sqrt{5})
$$ but I can't understand how it is obtained. Some one can explain how it come from.","['polyhedra', 'geometry']"
4864838,Finding the perimeter of an equilateral triangle given height,"Given an equilateral triangle with vertices $A,B,C$ , find the perimeter of the triangle if the height of this triangle is $8\sqrt{6}$ . I do not have a visual image for this problem, because I cannot see. However, this shape is not complex, and I think it is really not needed here. When I first attempted this problem, I imagined an altitude $\overline{DE}$ being drawn from one of the vertices to the base forming a right angle. I also think that the height is equal to the altitude. This creates two right triangles and I now can apply trigonometric ratio to solve for the hypotenuse. I think $\sin$ will be a good choice, since I want to get the hypotenuse: $$\sin {A} = \frac{8\sqrt{6}}{h}$$ Since this is an equalatteral triangle, all of its angle is $60^\circ$ . Therefore, I can safely assume that $A$ is also $60^\circ$ . By chance, I also know that $\frac{\sqrt{3}}{2}$ is equal to $\sin{60}$ . Substituting the values into the equation, I get $$\frac{\sqrt{3}}{2} = \frac{8\sqrt{6}}{h}$$ Solving this, I found that the length of the hypotenuse is of $\triangle ADE$ is $16\sqrt{2}$ . Since the question asks for the perimeter, the answer is $48\sqrt{2}$ . But my teacher does not agree with my solution. She insists that the answer is $24\sqrt{2}$ . I really do not understand why I am wrong. Can anyone help me with this question?","['algebra-precalculus', 'trigonometry']"
4864859,Proving $e^{-\mu}\left(\left(\frac{e}{1+\delta}\right)^{(1+\delta)\mu}+\left(\frac{e}{1-\delta}\right)^{(1-\delta)\mu}\right) \le 2e^{-C\mu\delta^2}$ [duplicate],"This question already has an answer here : Solving exercise 2.3.5 in Vershynin's HDP book with the best choice of c (1 answer) Closed 4 months ago . I'm trying to show that $$e^{-\mu}\left(\left(\frac{e}{1+\delta}\right)^{(1+\delta)\mu} + \left(\frac{e}{1-\delta}\right)^{(1-\delta)\mu}\right) \le 2e^{-C\mu\delta^2},$$ for an absolute constant $C > 0$ , where $\delta \in (0,1]$ and $\mu > 0$ . Using the idea in this answer , I wrote $$\log(1+\delta) = 0 + \delta \frac{1}{1+0} - \frac{\delta^2}{(1+\delta')^2},$$ for some $\delta' \in (0, \delta).$ This gives $(1+\delta')^2 \le 4$ , i.e., $- \frac{\delta^2}{(1+\delta')^2} \le -\frac{\delta^2}{4}$ . Plugging back in, we have the upper bound $$\log(1+\delta) = \delta - \frac{\delta^2}{(1+\delta')^2} \le \delta - \frac{\delta^2}{4}.$$ This doesn't immediately seem helpful, since we are looking for a bound of the form $$\log(1+\delta) \ge \frac{\delta + C\delta^2}{1+ \delta},$$ i.e., a lower bound. Could I get a hint? Thanks!","['proof-explanation', 'analysis']"
4864894,Technique for generating Lie point symmetries,"Consider I believe that there is something wrong with this text. In particular, how is $$\Delta=0 \quad \Longrightarrow \quad V(\Delta)=0$$ completely non trivial by linearity of operators? Moreover, I do not understand how this is used to find lie point symmetries. More context For context, the text also mentions so I assume that we plug such $V$ in $$ V(\Delta)=0$$ and then try to solve for the coefficients. But again, I do not understand why all coefficient dont simply work. Question: Why is $$\Delta=0 \quad \Longrightarrow \quad V(\Delta)=0$$ not true for any $V$ ? Moreover, can someone give me a simple example of how this technique is used to generate lie point symmetries?","['integrable-systems', 'lie-algebras', 'ordinary-differential-equations', 'partial-differential-equations']"
4864900,"Limit of a trigonometric integral: $\lim\limits_{x \to \infty} \int_{0}^{\pi} \frac{\sin (t)}{1+\cos^{2}(xt)} \, \mathrm dt$","For all $x \in \mathbb{R}$, let 
$$
{\rm f}\left(x\right)
=\int_{0}^{\pi}\frac{\sin\left(t\right)}{1 + \cos^{2}\left(xt\right)}\,{\rm d}t
$$
Compute the limit when $x\rightarrow +\infty$. My attempt : I tried the substitution $u=\sin(t)$ (and $u=\cos^2(xt)$) but it seems worse:
$$
\int _{0}^{1}\!{\frac {u}{ \left( 1+ \left( \cos \left( x\arcsin
 \left( u \right)  \right)  \right) ^{2} \right) \sqrt {1-{u}^{2}}}}{du}
$$
I tried to use a subsequence $(x_n)$ which is tends to $+\infty$ and use the dominated convergence theorem but it didn't work either. Sorry if my attempt doesn't make much sense. Thank you in advance.","['integration', 'limits', 'calculus', 'real-analysis']"
4864902,Looking for a counterexample when dropping one of the constraints (Linear algebra),"I want to find a counterexample for the following ""Theorem"":
Let $V \neq 0$ be a finite dimensional $K$ - vector space and $L \subset \mathfrak{gl}(V)$ a linear subspace. If all $x \in L$ are nilpotent as maps $V \rightarrow V$ then there is a $v \in V, v \neq 0$ such that $\forall x \in L: x(v) = 0$ . When $L$ is a Lie subalgebra of $\mathfrak{gl}(V)$ this statement is supposedly true, according to my lecture notes, but not true if it's just a linear subspace of $\mathfrak{gl}(V)$ . I couldn't come up with a counterexample though, so that's why I am asking for help. Certainly counterexamples can't be abelian, as then they'd automatically be Lie subalgebras. So they also, in particular, can not be one dimensional subspaces. I tried to construct something with two nilpotent matrices where their anti-commutator vanishes so that any power of their linear combinations consists of only powers of themselves again, but so far to no avail. There was always a vector in V mapped to 0 by all elements of $L$ Thank you in advance :)","['nilpotence', 'linear-algebra', 'lie-algebras']"
4864910,Probability of winning a circular ball game,"Context: You and 4 other people are sitting in a circle. You are given a ball to start the game. Every second of this game, the person with the ball has three choices they can make. They can either pass the ball to the left, pass the ball to the right, or keep the ball (all with equal probability). This game goes on till someone keeps the ball. What is the probability that you are the person to end the game and keep the ball? I am refreshing my knowledge of probability, and I am stuck on this question. For reference, it is the 'pass the Quantguide. Working: Consider the following diagram for clarity. Let's define random variable $W(S_i)$ to be the event that player $S_i$ keeps the ball, and end the games. Well, by symmetry we have that $\mathbb{P}(W(S_1)) = \mathbb{P}(W(S_2))$ , and similarly $\mathbb{P}(W(S_3)) = \mathbb{P}(W(S_4))$ . For simplicity, let use denote $W(S_1)$ and $W(S_2)$ by $N$ (neighbour), and $W(S_3), W(S_4)$ by $NN$ (next neighbour). So, we wish to find $\mathbb{P}(W(S_0))$ (presumably by also finding $\mathbb{P}(N)$ and $\mathbb{P}(NN)$ ). To proceed, we may condition on the action of $S_0$ . Thus, $$
\mathbb{P}(W(S_0)) = \frac{1}{3} + \frac{1}{3} \mathbb{P}(W(S_0) | \text{left} ) + \frac{1}{3} \mathbb{P}(W(S_0) | \text{right}) = \frac{1}{3} + \frac{2}{3} \mathbb{P}(N)
$$ Now, I struggle to proceed. I think that I should try to find an expression for $\mathbb{P}(N)$ in terms of $\mathbb{P}(W(S_0))$ and $\mathbb{P}(NN)$ , and similarly an expression for $\mathbb{P}(NN)$ in terms of $\mathbb{P}(W(S_0))$ and $\mathbb{P}(N)$ , however I am unsure of how I can do this without ending up with the exact same system of equations.",['probability']
4864927,Is the metric topology determined by its convergent sequences?,"I am aware that in a first countable space (and thus any metric space) is completely determined by its convergent sequences and their limits, i.e. , If $\tau_1$ and $\tau_2$ are two first countable topologies on a set $X$ such that $x_i\to c$ in $\tau_1$ iff $x_i\to c$ in $\tau_2$ , then $\tau_1 = \tau_2$ . However, this raises the following question: If two metrics on a space have the same convergent sequences, will they have the same limits as well (and thus the same topology)?","['general-topology', 'metric-spaces', 'sequences-and-series']"
4864951,L'Hopital's rule with dual numbers,"Background: For the dual numbers , we extend the reals with an additional unit vector $\epsilon$ subject to the constraint that $\epsilon^2 = 0$ . We can write dual numbers as $x_0 + x_1 \epsilon$ for $x_0,x_1 \in \mathbb{R}$ . We have a rule for multiplication, $$
(x_0 + x_1 \epsilon)(y_0 + y_1 \epsilon) = x_0 y_0 + x_1 y_0 \epsilon + x_0 y_1 \epsilon + x_1 y_1 \epsilon^2 = x_0 y_0 + (x_1 y_0 + x_0 y_1)\epsilon,
$$ as well as division, $$
\frac{x_0 + x_1 \epsilon}{y_0 + y_1 \epsilon} = \frac{x_0 + x_1 \epsilon}{y_0 + y_1 \epsilon} \left(\frac{y_0 - y_1 \epsilon}{y_0 - y_1 \epsilon}\right) = \frac{x_0 y_0 + (x_1 y_0 - x_0 y_1)\epsilon}{y_0^2}.
$$ It's clear that if $y_0=0$ , division is undefined for all values of $y_1$ . That is, numbers of the form $x_1 \epsilon$ are zero divisors. Furthermore, we have the exact Taylor series $$
f(x_0 + x_1\epsilon) = f(x_0) + x_1 f'(x_0) \epsilon.
$$ My question is: Is L'Hopital's rule $$\lim_{x\rightarrow x_0} \frac{f(x_0)}{g(x_0)} = \lim_{x\rightarrow x_0} \frac{f'(x_0)}{g'(x_0)}$$ when $f(x_0)=g(x_0)=0$ just the addition of the rule $$
\frac{x_1 \epsilon}{y_1 \epsilon} = \frac{x_1}{y_1}?
$$ That is, we can write that, if $f(x_0)=g(x_0)=0$ , then L'Hopital's rule can be implemented by making the simplification $$
\frac{f(x_0 + x_1 \epsilon)}{g(x_0 + x_1 \epsilon)} = \frac{x_1 f'(x_0) \epsilon}{x_1 g'(x_0) \epsilon} = \frac{f'(x_0)}{g'(x_0)}?
$$ Basically, is L'Hopital's rule equivalent to saying we can ""cancel"" the unit $\epsilon$ when the number is pure imaginary? What are the conditions necessary for making this ""cancellation""? The case where the numerator and denominator are different variables seem to make this less than trivial: if $f(x_0) = g(y_0) = 0$ , then the simplistic version of the rule would say that you should assign the value $$
\frac{f(x_0 + x_1 \epsilon)}{g(y_0 + y_1 \epsilon)} = \frac{x_1 f'(x_0) \epsilon}{y_1 g'(y_0) \epsilon} = \frac{x_1 f'(x_0)}{y_1 g'(y_0)}
$$ which does give a well-defined procedure for defining the value of the ratio, but it doesn't necessarily correspond to a limit in $x$ and $y$ (since it's not obvious what order the limits on $x$ and $y$ should be taken). It does, however, correspond to the limit along the path $\big(x(t),y(t)\big) = \big(x_0 + x_1 t, y_0 + y_1 t\big)$ such that $$
\lim_{t\rightarrow 0} \frac{f\big(x(t)\big)}{g\big(y(t)\big)} = \lim_{t\rightarrow 0}\frac{f'\big(x(t)\big)x'(t)}{g'\big(y(t)\big)y'(t)} = \frac{x_1 f'(x_0)}{y_1 g'(y_0)}.
$$ In the context of the dual numbers, is there a ""good"" reason for choosing this path? It does seem to have the advantage that it reduces to L'Hopital's rule for $y=x$ .","['analysis', 'abstract-algebra', 'dual-numbers', 'limits', 'exterior-algebra']"
4864953,Why Doesn't the St Petersburg Paradox Happen All the Time?,"I am learning about the St Petersburg Paradox https://en.wikipedia.org/wiki/St._Petersburg_paradox - here is my attempt to summarize it: A fair coin is tossed at each stage. The initial stake begins at 2 dollars and is doubled every time tails appears. The first time heads appears, the game ends and the player wins whatever is the current stake As we can see, this game will have an expected reward of infinite dollars: $$E(X) = \sum_{i=1}^{\infty} x_i \cdot p_i$$ $$E  = \sum_{n=1}^{\infty} \frac{1}{2^n} \cdot 2^n = \frac{1}{2} \cdot 2 + \frac{1}{4} \cdot 4 + \frac{1}{8} \cdot 8 + \frac{1}{16} \cdot 16 + ...  = 1 + 1 + 1 + 1 + ...  = \sum_{n=1}^{\infty} 1  =  \infty$$ The paradox is that even though the game has an infinite reward, in real life simulations, the game usually ends up with a finite reward. Although seemingly counterintuitive, this does seem logical. Even more, we can write computer simulations to see that large number of games will have finite rewards. My question is about applying the insights of the St Petersburg Paradox to the first passage/first hitting times of Brownian Motions . For example, consider the generic Brownian Motion: $$Y_t = y_0 + \mu t + \sigma W_t$$ $Y_t$ is the value of the process at time $t$ . $y_0$ is the initial value of the process at time $t=0$ . $\mu$ is the drift rate, representing the deterministic trend. $\sigma$ is the diffusion coefficient, scaling the Brownian motion $W_t$ . $W_t$ is a standard Brownian motion. Now, consider the following situations (all with 1 dimensional continuous Brownian Motions): Situation 1: An unconstrained Brownian Motion without Drift starting at $y=1$ at time $t=0$ : When will it be expected to first reach the point $y=5$ ? Situation 2: A Brownian Motion without Drift starting at $y=1$ at time $t=0$ : If the stopping condition is when it reaches the point $y=5$ or $y=-5$ for the first time - when will it be expected to stop? Situation 3: A Brownian Motion without Drift starting at $y=1$ at time $t=0$ : If it can only go between the points $y=5$ or $y=-5$ ,  when will it be expected to first reach the point $y=3$ ? (i.e. if it reaches $y=-5$ , it ""bounces back"" - however I am not sure how to mathematically encapsulate this ""bouncing back"" behavior) Situation 4: How will the first passage times for Situations 1,2,3 change if we use Brownian Motions with Drift? (i.e. I know that the Inverse Gaussian Distribution is used here) Situation 5: How will the first passage times for Situations 1,2,3,4 change if the Brownian Motion can only take discrete values? (i.e. a random walk) Naively, using the logic from the St Petersburg Paradox, I could argue that the expected stopping times for the above 5 questions will all be infinite. That is, technically, there are very small probabilities that all these Brownian Motions can get stuck in an infinite loop of going back and forth, and never reach their stopping conditions. Thus, each of these infinitely long paths are weighted with infinitely small probabilities - and since there are an infinite number of these paths : the expected value would be infinite. Is the St Petersburg Paradox fundamentally at odds with the concept of First Passage/Hitting Times? Yet this is clearly not the case. I can repeatedly simulate any of the above situations and see that all of them have finite stopping times (even though some of them might be long). However, now it seems to me that in theory, the more simulations you do, the probability of encountering a very long simulation increases, thus the average stopping time would statistically increase as the number of simulations increase? Can someone please help me understand how to mathematically analyze the probability distributions and expected first passage/hitting times of the above situations? Why will some of them be finite and some of them be infinite? In situations where there is an infinite answer - is it wrong to just simulate the situation many times and take the expectation of the empirical distribution of these simulations as the average hitting time? It seems to me that by virtue of the St Petersburg Paradox, all hitting time distributions should be infinite - yet this is clearly not the case? Note: Why is the Brownian Motion related to the Normal Distribution?","['paradoxes', 'brownian-motion', 'probability']"
4864965,Minimize the area of $n$ intersecting circles,"Let $0 < c < 2/(n-1)$ . For $p_1,...,p_n \in \mathbb{R}^2$ , consider the property $$
|p_{k+1}-p_k| = c, \  k=1,...,n-1 \tag 1
$$ where $| \cdot |$ is the Euclidean distance. Denote $C_k$ for the unit circle centered at $p_k, \ k=1,...,n$ . $\textbf{Question}$ : For what geometric configuration of $p_1,...,p_n \in \mathbb{R}^2$ satisfying $(1)$ is the area of $C_1 \cap \cdots \cap C_n$ minimized?  (note $c$ is fixed) Comment: I've been trying to prove the solution is a straight line, as it appears to lead to a solution for the following: if you are placed at a random position in a circle, which path minimizes the average time it takes for you to escape the circle? (something I really want to solve) $\textbf{Update} \ (2/19)$ : I solved the alternate question, that the area of $C_1 \cup \cdots \cup C_n$ is maximized when $p_k$ are in line. The proof is below. Write $\mu(A)$ for the area of a set $A$ . Write $B_1 = C_1 \cup \cdots \cup C_{n-1}$ and $B_2 = C_2 \cup \cdots \cup C_n$ . Then \begin{align} 
\mu(C_1 \cup \cdots \cup C_n) &= \mu(B_1 \cup B_2) \\ 
                           &= \mu(B_1) + \mu(B_2) - \mu(B_1 \cap B_2) \\ 
                           &= \mu(B_1)+\mu(B_2) - \mu\big((C_1 \cap C_n) \cup (C_2 \cup \cdots \cup C_{n-1})\big) \\ 
&\leq \mu(B_1) + \mu(B_2) - \mu(C_2 \cup \cdots \cup C_{n-1})   \tag 2 \\ 
&= \mu(B_1 \backslash (C_2 \cup \cdots \cup C_{n-1})) + \mu (B_2) \\ 
&= \mu(C_1 \backslash (C_2 \cup \cdots \cup C_{n-1})) + \mu(B_2) \\ 
&\leq \mu(C_1 \backslash C_2) + \mu(B_2) \tag 3
\end{align} We will now prove by induction $\mu(C_1 \cup \cdots \cup C_n)$ is maximized when $p_k$ are in a sequential line. For case $n=3$ , notice $\mu(B_1)$ and $\mu(B_2)$ only depend on $c$ , therefore are constant. Let $p_1, p_2, p_3$ be in a line. One can prove with law of consines that $C_1 \cap C_3 \subset C_2$ . So equality holds at $(2)$ , which shows the result. For case $n > 3$ , assume case $n-1$ holds. Let $p_1,...,p_n$ be in a line. By the induction hypothesis, $\mu(B_2)$ is maximized. $\mu(C_1 \backslash C_2)$ only depends on $c$ , so is constant. So $(3)$ is maximized. Also, by law of cosines one can show $C_1 \cap C_n \subset C_2 \cup \cdots \cup C_{n-1}$ and $C_1 \backslash C_2 = C_1 \backslash (C_2 \cup \cdots \cup C_n)$ . So equality holds at $(2)$ and $(3)$ . This proves case $n$ .",['geometry']
4864970,On why solutions to $a^4+b^4+c^4+d^4 = (a+b+c+d)^4$ also come in pairs,"Jacobi and Madden found infinitely many primitive solutions to, $$a^4+b^4+c^4+d^4 = (a+b+c+d)^4$$ using an elliptic curve. We will use a different approach that, like the method for, $$a^4+b^4+c^4 = d^4$$ discussed in this post , also yields pairs of solutions. I. The system Define, $$x^4+y^4+z^4+1 = (x+y+z+1)^4\tag1$$ $$\frac{x^2+x+1}{(x+y+1)(x+z+1)}=u\tag2$$ $$\frac{y^2+y+1}{(y+z+1)(y+x+1)}=v\tag3$$ $$\frac{z^2+z+1}{(z+x+1)(z+y+1)}=w\tag4$$ The variable $w$ is dependent via a rather complicated expression on $(u,v)$ so $(4)$ is redundant. Use the first three equations to solve for the three unknowns $(x,y,z)$ . After some algebra, we find they are roots of quadratics hence yields pairs of solutions. The discriminant of the quadratic is, $$D^2 = -3(2 - u + u^2)^2 + 6(2 + u + 3u^2 - 3u^3 + u^4)v - 3(5 - 6u + 6u^2 - 2u^3 + u^4)v^2 + 6(1 - u)(1 - 2u - u^2)v^3 - 3(1 - u)^2v^4$$ II. Table of known u If there is rational $(u,v)$ such that $D$ is also rational, then the quartic in $v$ is birationally equivalent to an elliptic curve. Furthermore, if $(u,v)$ is a solution, then $\big(\frac{u+1}{u-1},v\big)$ is also a solution and which slightly complicates things. As of 2015, there are only eight $u$ ( Update : Now twelve as of 2024) of small height (numerator and denominator < $1000$ ) that are known, and not counting its partner $u'=\frac{u+1}{u-1}$ . Namely, \begin{array}{|c|c|c|c|c|}
\hline
\text{#} & \color{red}u & u'=\frac{u+1}{u-1} & \color{red}v & \text{Discoverer}\\ 
\hline
1 & \dfrac{511}{450} & \dfrac{961}{61}  & \dfrac{1651}{126} & \text{Brudno}\\
\hline
2 & \dfrac{193}{18} & \dfrac{211}{175} & \dfrac{619}{450} & \text{Wroblewski}\\
\hline
3 & \dfrac{31}{6} & \dfrac{37}{25} & \dfrac{6619}{5550} & \text{Rouse} \\
\hline
4 & \dfrac{211}{150} & \dfrac{361}{61} & \dfrac{2041}{150} & \text{Tomita}\\
\hline
5 & \dfrac{157}{150} & \dfrac{307}{7} & \dfrac{8467}{150} & \text{Rouse}\\
\hline
6 & \dfrac{181}{150} & \dfrac{331}{31} & \dfrac{277567}{31675} & \text{Tomita}\\
\hline
7 & \dfrac{373}{150} & \dfrac{523}{223} & \dfrac{9785779}{952879} & \text{Tomita}\\
\hline
8 & \dfrac{121}{96} & \dfrac{217}{25} & \dfrac{6250987}{506400} & \text{Tomita}\\
\hline
\end{array} III. Example From the table, choose #4 and let $u=\dfrac{211}{150}$ and $v_1 =\dfrac{2041}{150}$ where $u$ was also employed by Tomita here using a different method. Solving the system, $$x^4+y^4+z^4+1 = (x+y+z+1)^4$$ $$\frac{x^2+x+1}{(x+y+1)(x+z+1)}=\frac{211}{150}$$ $$\frac{y^2+y+1}{(y+z+1)(y+x+1)}=\frac{2041}{150}$$ yields the pair of solutions, $$ \left(- \frac{1984340}{107110}\right)^4 + \left( \frac{1022230}{107110}\right)^4 + \left( - \frac{1229559}{107110}\right)^4 + 1 = (x+y+z+1)^4$$ $$ \left(-\frac{3597130}{1953890}\right)^4 +  \left(- \frac{561760}{1953890}\right)^4 +  \left(- \frac{1493309}{1953890}\right)^4 + 1 = (x+y+z+1)^4$$ where $(x,y,z)$ are the first three addends. These are the 3rd and 5th smallest known solutions, also found by Tomita in the link above. From the initial $v_1$ , one can find infinitely many other $v_k$ . IV. Question So given, $$D^2 = -3(2 - u + u^2)^2 + 6(2 + u + 3u^2 - 3u^3 + u^4)v - 3(5 - 6u + 6u^2 - 2u^3 + u^4)v^2 + 6(1 - u)(1 - 2u - u^2)v^3 - 3(1 - u)^2v^4$$ can you find a $u$ of small height not in the table above?","['number-theory', 'elliptic-curves', 'diophantine-equations']"
4865023,Why $\lim_{h→0+}\frac{ f (x + 2h) − f (x + h)}{h}$ is not a derivative?,"Let $f :\mathbb{R}→\mathbb{R}$ be a continuous function with the property that $\lim_{h→0+}\frac{ f (x + 2h) − f (x + h)}
{h} = 0$ , for all $x ∈ \mathbb R$ .
Prove that $f$ is constant I’m not asking for a solution to this problem, I’m interested in why what is written cannot be considered a definition of a derivative. We can take $t=x+h$ and then $\lim_{h→0+}\frac{ f (t+h) − f (t)} {h} = 0$ and this is the definition of derivative. Or is the problem in $\lim_{h→0+}$ ,but $f$ is a continuous function, which means $\lim_{h→0+}f(x+h)=\lim_{h→0-}f(x+h)$ I will be glad if you can help me understand this problem.","['derivatives', 'real-analysis']"
4865035,How many nilpotent matrices are there in $M_n(\mathbb R)$ up to similarity?,"I am trying to count all nilpotent matrices in $M_n(\mathbb R)$ up to similarity. I did the same exercise for idempotent matrices and it was quite simple. I realised that rank of an idempotent matrix is a non-negative integer and two idempotent matrices are similar iff they have the same rank. So I got the answer $n+1$ . However, it doesn't seem to be simple for nilpotent matrices. Things which are clear to me: If two nilpotent matrices are similar, they must have the same order of nilpotence. There is exactly one class of similarity for nilpotent matrices of order $n$ . The question I would like to ask: How many nilpotent matrices of order $k$ are there up to similarity? The answer is $1$ if $k=n$ . The answer is $1$ again if $k=1$ . (The null matrix is the only matrix which has nilpotence of order $1$ and it is its own similarity class.) What about other values of $k$ ? By looking at Jordan normal form, here's what I found about $n=2$ and $n=3$ . How to generalize? For $n=2$ , order of nilpotence vs number of matrices up to similarity. $\begin{array}{l|l} k& \text{#}\\ \hline 1&1\\ 2& 1\end{array}\tag*{}$ For $n=3$ , $\begin{array}{l|l} k& \text{#}\\ \hline 1&1\\ 2& 1\\ 3&1\end{array}\tag*{}$ The answer is $1$ for $k=2$ because there is only one unique way to create blocks along the diagonal so that one of the blocks is nilpotent of order $2$ . The arrangement if $2+1$ . $\begin{pmatrix}\color{red}{0}&\color{red}1&0\\ \color{red}0&\color{red}0&0\\0&0&\color{blue}0\end{pmatrix}\tag*{}$ Note that each block should be nilpotent in itself and hence, the choice of zero and non-zero entries. For $n=4$ , $\quad k=2$ : From $2+2$ and $2+1+1$ , I get one matrix each. If I consider $3+1$ , I would overcount. $\displaystyle \begin{pmatrix}
0 & 1 &  & \\
0 & 0 &  & \\
 &  & 0 & 1\\
 &  & 0 & 0
\end{pmatrix} \ \begin{pmatrix}
0 & 1 &  & \\
0 & 0 &  & \\
 &  & 0 & \\
 &  &   & 0
\end{pmatrix}\tag*{}$ $\quad k=3$ : From $3+1$ , I get one matrix. $\displaystyle \begin{pmatrix}
0 & 1 & 0 & \\
0 & 0 & 1 & \\
0 & 0 & 0 & \\
 &  &  & 0
\end{pmatrix}\tag*{}$ Thus, $\begin{array}{l|l} k& \text{#}\\ \hline 1&1\\ 2& 2\\ 3&1\\ 4 & 1\end{array}\tag*{}$ I am not sure how to generalize. It seems to me that there should be a recursive formula.","['matrices', 'linear-algebra', 'combinatorics']"
4865039,Another Sophomore's Dream: $\int_{-\infty}^{\infty}\binom{n}{x}dx=\sum_{i=0}^{n} \binom{n}{i}$,"I found an identity $$\int_{-\infty}^{\infty}\binom{n}{x}dx=\sum_{i=0}^{n} \binom{n}{i}$$ where LHS can be calculated by the Reflection relation and Dirichlet integral . The result is $2^n$ , which is apparently equal to RHS. This identity is in the form ""integration=summation"", which is similar to the ""Sophomore's dream"" , $\int_0^1 x^{-x}dx = \sum_{n=1}^\infty n^{-n}$ . Is this another coincidence? If not, what is the reason behind it to make it true.","['integration', 'calculus', 'binomial-coefficients', 'binomial-theorem']"
4865126,Example 3.5 Silverman's Elliptic Curves: problem with a point in the projective space,"I am reading Silverman's The Arithmetic of Elliptic Curves . I have a question regarding Example 3.5, which is the following: I have doubts about the equality $[-Y^2,Y(X-Z))]=[-Y,X-Z]$ . Two points $[x_0,\ldots, x_n]$ , $[x_0',\ldots, x_n']$ in the projective space are the same if we can write $x_i=\lambda x_i'$ for some $\lambda \in K^\times$ for every $i=0\ldots, n$ . So my question here is: what happens if $Y=0$ ? I don't think we can safely state that $[-Y^2,Y(X-Z))]=[-Y,X-Z]$ . Actually, if $Y=0$ , we have that $[-Y^2,Y(X-Z))]=[0,0]$ so this is not even a well-defined point. What worries me is that this reasoning he does is precisely meant to solve a problem that arises precisely when $Y=0$ , which is the only time when I don't see his reasoning working.","['algebraic-geometry', 'elliptic-curves', 'projective-space']"
4865182,$\varphi^G$=$\psi^G$ implies that subgroups are conjugate,"Let $G$ be a finite group and let $\varphi,\psi$ be linear characters of $H$ and $K$ respectively, where $H,K$ are subgroups of $G$ . I'm trying to prove that if the $G$ -induced characters of $\varphi,\psi$ are equal (this is, $\varphi^G=\psi^G$ ) then $H$ and $K$ are conjugate in $G$ . Yet I don't know if the result is true either. If it was true, it would conclude another proof that I'm attempting. If $\varphi^G=\psi^G$ , evaluating the function at $1$ we obtain that $|G:H|=|G:K|$ and hence $|H|=|K|$ . This makes me think that the result may be true. I have also proved that $\psi_{H^s\cap K}=\varphi^s_{H^s\cap K}$ for some $s\in G$ . Mackey's decomposition formula states that if $S$ is a set of double coset representatives so that $G=\cup_{s\in S}HsK$ is a disjoint union then $$(\varphi^G)_K=\sum_{s\in S}(\varphi^s_{H^s\cap K})^K.$$ Now, by Frobenius reciprocity it holds $0\not=[\varphi^G,\psi^G]=[(\varphi^G)_K,\psi]=\sum_{s\in S}[(\varphi^s_{H^s\cap K})^K,\psi]=\sum_{s\in S}[\varphi^s_{H^s\cap K},\psi_{H^s\cap K}]$ . Thus, there exists $s\in S$ such that $0\not=[\varphi^s_{H^s\cap K},\psi_{H^s\cap K}]$ and hence $\psi_{H^s\cap K}=\varphi^s_{H^s\cap K}$ (as both characters are linear and therefore irreducible). However, I don't know neither this equality of characters may be relevant, as $H^s\cap K$ may be trivial. Any hint or suggestion for how to proceed is appreciated.","['characters', 'representation-theory', 'finite-groups', 'abstract-algebra', 'group-theory']"
4865184,"How to evaluate $\int_0^1 \dfrac{\operatorname{Li}_2\left(\frac{x}{4}\right)}{4-x}\,\log\left(\dfrac{1+\sqrt{1-x}}{1-\sqrt{1-x}}\right)\,dx$","crossposted: https://mathoverflow.net/q/464839 How to evaluate $$\int_0^1 \dfrac{\operatorname{Li}_2\left(\frac{x}{4}\right)}{4-x}\,\log\left(\dfrac{1+\sqrt{1-x}}{1-\sqrt{1-x}}\right)\,dx=\dfrac{\pi^4}{1944}.$$ I am going to have to look back through my papers to find how it was evaluated. It's been a while and I forget without doing it all over again. Unless, someone wants to jump on it before I get back...please feel free. I used a sub ( $t=x/4$ ), parts, and the identity $\sum_{n=1}^{\infty}H_{n}^{(2)}x^{n}=\frac{Li_{2}(x)}{1-x}$ Maybe break it up using: $$H_{n+1}^{(2)}-\frac{1}{(n+1)^{2}}=H_{n}^{(2)}$$ and/or $$\sum_{n=1}^{\infty}\frac{x^{2n}}{(n+1)(2n+1)\binom{2n}{n}}=\frac{4(\sin^{-1}(\frac{x}{2}))^{2}}{x^{2}}$$ I get it now. This is related to the identity: $$
\left( \sin^{-1}(z)\right)^4=\frac{3}{2}\sum_{k=1}^\infty\frac{H_{k-1}^{(2)}(2z)^{2k}}{k^2 \binom{2k}{k}} \quad |z|<1
$$","['integration', 'calculus', 'definite-integrals', 'closed-form']"
4865202,Motivation for the definition of curvature of a plane curve,"I am seeking a motivation for the definition of the curvature of a plane curve. How did people come with the idea of the definition of the curvature? Below I am more specific. The fundamental theorem for plane curves states the following. Giving two curves $\alpha$ and $\beta$ there exists a rigid motion $M:\mathbb R^2\to \mathbb R^2$ such that $\beta(t)=M(\alpha(t))$ if, and only if, the curves $\alpha$ and $\beta$ have the same curvature function. Assume for a moment that you do not know about the curvature and that you would like to classify plane curves up to rigid motions. How could one come up with the idea of a geometric invariant that is enough to distinguish curves?","['plane-curves', 'curvature', 'differential-geometry']"
4865237,Calculate the differentiate of $ Q: \mathbb{R}^n \rightarrow \mathbb{R}$ defined by $Q(x)=B(x;x)$,"I have a question on the differentiable of a function and I want to be sure that my understanding of this concept is correct. Question: Calculate the differentiate of $ Q: \mathbb{R}^n \rightarrow \mathbb{R}$ defined by $Q(x)=B(x;x)$ where $B: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R} $ is bilinear. Answer: 1- It is given that $B$ is a bilinear mapping thus $B(x;x)$ is continuous and $ \forall x \in \mathbb{R}^n, \exists c > 0  $ s.t. $ |B(x;x) - B(0;0)| \leq c \| x -0 \| $ . And as $B(0;0)=0$ because $B$ is linear mapping the last inequality can be re write $ |B(x;x)| \leq c \| x\| $ 2- Now according to the definition; for any given point $x \in \mathbb{R}^n$ the differential in this point will be the linear mapping $L(h)$ verifying that $ \forall  h  \in \mathbb{R}^n $ , $Q(x+h)$ can be writen as follow: $ Q(x+h)= Q(x) + L(h) + O(h)$ with $O(h)$ verifying $ \lim_{h \to 0} || \frac{O(h)}{h}|| = 0 $ Rem: $h$ can not appear in the exepression $Q(x)$ but $x$ can appear in the expression of $L(h)$ and $O(h)$ . More over $h$ can appear in the expression of $L(h)$ only as a power of $1$ that means $h$ but not $h^2 , h^3,...$ 3- First by definition we can write $Q(x+h) = B(x+h;x+h)$ . After as $B(.;.)$ is a bilinear mapping (given) we can keep on and continue to write $ Q(x+h) = B(x+h;x+h) = B(x;x+h) + B(h;x+h)=B(x;x)+B(x;h)+ B(h;x+h) = B(x;x)+B(x;h)+ B(h;x) + B(h;h) $ Rem: We used the following property of bilinear mapping: $\phi(a+v;b)= \phi(a;b) + \phi(v;b) $ 4- First lets note that according to ""1-"" we have that if we take $O(h) = B(h;h)$ that $ \lim_{h \to 0} || \frac{O(h)}{h}|| = 0 $ . Now $L(h) = B(x;h)+ B(h;x) = D[Q(x)] h  $ is the differentiate of $Q(x)$ as it is a the sum of two linear mapping in $h$ by assumption. Is it correct? I have mostly a question on my part ""2-"" concerning my understanding of the differentiate definition. Thank you.","['differential', 'differential-topology', 'solution-verification', 'derivatives']"
4865249,Proof that $\cos(x) = \cos\left(\frac{\pi}{2}\right)\implies x = \frac{\pi}{2} + k\pi$,"I know that $$\cos(x) = \cos\left(\frac{\pi}{2}\right)$$ yields to $$(1)\qquad \qquad \qquad x = \frac{\pi}2 + 2k\pi$$ or $$(2)\qquad \qquad \qquad x = -\frac{\pi}2 + 2k\pi$$ which (as I found out in a textbook) is the same as $$x = \frac{\pi}{2} + k\pi$$ My question is how (1) and (2) imply the last result, I would like an algebraic proof or demonstration. $\\$ I know it's obvious, but I couldn't find out a rigorous demonstration by myself to why it's the same thing.","['trigonometry', 'proof-writing']"
4865325,Must the orientation of a parameterized curve $\gamma:I\subseteq\mathbb{R}\to \mathbb{R}^n$ be in the direction of the increasing parameter?,"In the context of evaluating the line integral of a vector field $\vec{F}:\vec{x}\in\mathbb{R}^n\mapsto \vec{F}(\vec{x})\in\mathbb{R}^n$ along a regular parameterized curve $\gamma:t\in{I}\subseteq\mathbb{R}\mapsto \vec{x}(t)\in\mathbb{R}^n$ , we define $$\int_{\gamma}\vec{F}\cdot d\vec{x} = \int_{I}\vec{F} \left(\vec{x}(t)\right)\cdot \frac{d\vec{x}}{dt}(t)dt$$ I have learned that this definition ensures its invariance under an orientation-preserving diffeomorphism $\tau:t\in I \mapsto \tau(t)\in I'$ since the chain rule provides that $$\frac{d\vec{x}}{d\tau} (t(\tau)) = \frac{d\vec{x}}{dt} (t(\tau)) \frac{dt}{d\tau} (\tau)$$ and the change in measure introduces a factor $\lvert{\frac{dt}{d\tau}(\tau)}\rvert$ such that $$\int_{\gamma} \vec{F} \cdot d\vec{x} = \int_{I'}\vec{F}(\vec{x}(t(\tau))) \cdot \left[\frac{\frac{d\vec{x}}{d\tau}(t(\tau))} {\frac{dt}{d\tau}(\tau)}\right] \lvert \frac{dt}{d\tau}(\tau)\rvert d\tau = \int_{I'}\vec{F} \left(\vec{x}(t(\tau))\right)\cdot \frac{d\vec{x}}{d\tau}(t(\tau))d\tau$$ This seems to crucially rely on $\frac{dt}{d\tau}(\tau)$ being positive on $I'$ or equivalently $\frac{d\tau}{dt}(t)$ being positive on $I$ , which I took to be the definition of $\tau(t)$ being an orientation-preserving reparameterization of the regular curve $\gamma$ . Since $\frac{d\vec{x}}{d\tau} (t(\tau)) = \frac{d\vec{x}}{dt} (t(\tau)) \frac{dt}{d\tau} (\tau)$ , I have noted that this is equivalent to the tangent vectors being parallel at any given point along the curve. My confusion then comes from what is meant by orientation. It seems that the orientation of $\gamma$ could equally be defined as being along the direction that the parameter decreases (antiparallel to the tangent vector) instead of the direction that the parameter increases (parallel to the tangent vector). Both are compatible with the notion of orientation-preserving reparameterizations. Is orientation independent of parameterization? For simple curves, does the notation $\int_{\gamma}$ only refer to integrating over the image $\{\gamma(t)\vert t\in I\}$ or the family of parameterizations of the image $\{\gamma(t)\vert t\in I\}$ with a particular orientation? If so, it is more correct to write $$\int_{\gamma}\vec{F}\cdot d\vec{x} = \pm \int_{I}\vec{F} \left(\vec{x}(t)\right)\cdot \frac{d\vec{x}}{dt}(t)dt$$ where $\pm$ is chosen such that the orientation of $\gamma$ is in the direction of increasing $\pm t$ ? For example, suppose I wanted to integrate some vector field $\vec{F}:\vec{x}\in\mathbb{R}^3\mapsto \vec{F}(\vec{x})\in\mathbb{R}^3$ along the straight line segment connecting $(1,1,1)$ to the origin, starting from $(1,1,1)$ and ending at $(0,0,0)$ . If we define $\gamma:t\in[0,1]\mapsto (t,t,t)\in \mathbb{R}^3$ , would this line integral still be denoted $\int_{\gamma}\vec{F}(\vec{x}) \cdot d\vec{x}$ or would it be necessary to write $-\int_{\gamma}\vec{F}(\vec{x}) \cdot d\vec{x}$ or even $\int_{-\gamma}\vec{F}(\vec{x}) \cdot d\vec{x}$ , where $-\gamma:t\in[0,1]\mapsto (1-t,1-t,1-t)\in \mathbb{R}^3$ ?","['curves', 'multivariable-calculus', 'line-integrals']"
4865373,Can a function be defined as the union of two other functions?,"So I read from various sources that a function can be defined as a binary relation. Then is it valid to say, for example, $f = \{ (1, 2), (2, 3) \}$ ? And suppose I have another function $g = \{ (4, 5) \}$ . Does it then make sense to write $(f \cup g)(2) = 3$ ?","['notation', 'functions', 'relations']"
4865381,"Using only the universal property, prove that $X$ is dense in its Cauchy completion","Long ago, I learned about the Cauchy completion of metric spaces via the usual explicit construction of quotienting the set of Cauchy sequences. For a metric space $X$ , let $\hat X$ denote this Cauchy completion of $X$ . We then have a canonical isometric embedding $\iota\colon X\to\hat X$ ( $x\mapsto(x, x, x, \ldots)$ ). It's easily seen that $\iota(X)$ is dense in $\hat X$ , and using this, one can show that the following universal property characterizes $\hat X$ up to bi-isometries: Any isometric embedding of $X$ into a complete metric space factors uniquely through $\iota$ . Now, my question is this: Using this as the definition of a Cauchy completion, and avoiding the Cauchy-sequence-construction , can one show that $X$ is dense in $\hat X$ ?","['complete-spaces', 'cauchy-sequences', 'metric-spaces', 'universal-property', 'general-topology']"
4865415,"If $A \subseteq B$, then $A\Delta B=B\backslash A$","So I'm working on my discrete math homework, and I came to this one:
If $A \subseteq B$ , then $A\Delta B=B\backslash A$ I was getting it split up into cases to prove it, then I noticed that it was $B\backslash A$ , and that didn't make sense to me. If it was $A\backslash A$ that would make sense to me because then both $A\Delta B$ and $A\backslash A$ would equal $\emptyset$ , but I don't see any way that $B\backslash A$ could equal $A\Delta B$ .","['elementary-set-theory', 'discrete-mathematics']"
4865481,Combinatoric and subsets,"I recently encountered a question Let $n \in \mathbb{Z} ^+$ , Let $U$ be a set containing $n$ elements. Let $\mathcal{S} \subseteq P(U)$ . Let $m \in \mathbb{Z}^+, m \leq n$ . Prove that $$
|\mathcal{S}| > \sum_{i=0}^{m - 1} \binom{n}{i} \Rightarrow \exists S' \in P(U). (|S'|=m) \wedge \left(\{S \cap S':S \in \mathcal{S}\} = P(S')\right)
$$ Currently, I am inducting on $n$ and $m$ . I have proved the cases when $n = 1$ or $m = 1$ . For the induction step, I have three cases. The first case is when none of the sets in $\mathcal{S}$ contains the $n$ th element of $U$ . The second case is when exactly half of the sets in $\mathcal{S}$ contain the $n$ th element of $U$ . The last case is when the previous two cases are false. I can prove the first two cases. However I am strugling with the third case. I have tried using the induction hypothesis on $n - 1, m - 1$ and $n, m - 1$ , but I didn't get anywhere. My idea is to use a non-constructive probabalistic method, but that turned real messy real fast, due to each event being dependent on the others. So I guess I am missing something obvious here? Maybe there is an observation that can make things more elegant? Or should I keep going down that probabalistic method path?","['elementary-set-theory', 'probabilistic-method', 'combinatorics']"
4865501,Taking the Limit of Stars and Bars,"I'm current working on the following question from Chapter 1 of Theory of Probability and Random Processes by Koralov and Sinai: For integers $n$ and $r$ , find the number of solutions of the equation $$x_1 + \ldots + x_r = n,$$ where $x_i \geq 0$ are integers. Assuming the uniform distribution on the space of the solutions, find $P(x_1=a)$ and its limit as $r\to \infty$ , $n\to\infty$ , $n/r\to\rho > 0$ . So, the first part is just a simple stars and bars and we get $\binom{n+r-1}{r-1}$ . However, it's the second part, taking the limit, that I'm struggling with. Essentially, I'm trying to evaluate the following limit $$\lim_{n\to\infty\,r\to\infty\\n/r\to\rho} \binom{n-a+r-2}{r-2}\big/\binom{n+r-1}{r-1}.$$ Intuitively, I expect the limit to converge to some Poisson Distribution parameterized by $\rho$ and evaluated at $a$ . I've attempted this by applying Stirling's Approximation, but have no idea how to proceed: \begin{align}P(x_1=a)&=\binom{n-a+r-2}{r-2}\big/\binom{n+r-1}{r-1} \\ &= \frac{(n-a+r-2)!(r-1)!n!}{(r-2)!(n-a)!(n+r-1)!} \\ 
&= \frac{(r-1)n!(n-a+r-2)!}{(n+r-1)!(n-a)!} \\
&\approx \frac{(r-1)\sqrt{2\pi n}(n/e)^n\sqrt{2\pi (n-a+r-2)}\left(\frac{n-a+r-2}{e}\right)^{n-a+r-2}}{\sqrt{2\pi(n+r-1)}\left(\frac{n+r-1}{e}\right)^{n+r-1}\sqrt{2\pi (n-a)}\left(\frac{n-a}{e}\right)^{n-a}} \\
&\,\,\vdots \\
&= (r-1)n^n (e^{r-1})(n-a+r-2)^{n-a+r-2}(e^{2-r})\left(\frac{1}{n-a}\right)^{n-a}\left(\frac{1}{n+r-1}\right)^{n+r-1} \\
&\,\,\vdots \\
&= (r-1)e\left(\frac{n}{n+r}\right)^a \\
P(x_1=a)&= (r-1)e\left(\frac{n/r}{n/r+1}\right)^a.
\end{align} Then, taking the limit of the last expression we get $(r-1)e(\rho/(\rho+1))^a$ , which then goes to infinity. But this result makes no sense...","['limits', 'solution-verification', 'combinatorics', 'probability']"
4865510,Is there a perfect group in which not every element is a commutator?,"Is there a perfect group in which not every element is a commutator? By a well-known fact,  it must have order at least $96.$ By Ore's conjecture (now a theorem), it must be infinite or non-simple.","['conjectures', 'group-theory', 'abstract-algebra']"
4865540,Find an asymptotic solution of a ODE system,"Consider the following ODE system: $\frac{dx}{dt}=x^2+y^2-y\\ \frac{dy}{dt}=-2xy-x$ I want to prove that there exists a solution $(x(t),y(t))$ such that $\lim_{t\to +\infty}(x(t),y(t))=\lim_{t\to -\infty}(x(t),y(t))=(0,0)$ and $(x(t),y(t))\neq (0,0)$ for any $t$ . I can prove that when $F(x,y)=6x^2y+3x^2+2y^3-3y^2$ , we have $\frac{d}{dt}F(x(t),y(t))=0$ . So if there exists a solution satisfying the condition,  it should be on the curve of $6x^2y+3x^2+2y^3-3y^2=0$ . Then I have no idea about what I can do next. Should I try to find a special solution?","['calculus', 'ordinary-differential-equations', 'dynamical-systems']"
4865555,"Number of Salem–Spencer subsets of $\{1,2,3,\dots ,n\}$","I was wondering about sets that do not contain any $3$ -term AP, and came to know that the official name of such a set is Salem–Spencer set . I was considering the question of counting the number of Salem–Spencer subsets of $\{1,2,3,\dots ,n\}$ for a given $n$ . This is listed in OEIS A051013 . However, neither on OEIS, nor on anywhere else could I find any literature dealing with the specific question at hand. So, my question is whether anyone knows of any formula (recursive or otherwise) for the number of Salem–Spencer subsets of $\{1,2,3,\dots ,n\}$ for a given $n$ . I tried my hand at a recursion technique and tried to use the fact that such a set either ends with $n$ or it doesn't. So, we have $$T(n)=T(n-1) + S(n)$$ where $T(n)$ is the number of Salem–Spencer subsets of $\{1,2,3,\dots ,n\}$ and $S(n)$ is the number of Salem–Spencer subsets of $\{1,2,3,\dots ,n\}$ containing $n$ . These $S(n)$ 's are listed in OEIS A334893 (with the specific cases listed in OEIS A334892 ) but there's no formula for these to be seen. Even if an exact result is not available, I would like to know whether it is possible to get any asymptotic results or even any useful bounds $^{*}$ . Even references which may be helpful are welcome. $[^*]$ A set that avoids a $3$ -AP is also a set that avoids three consecutive numbers, which is known to be the Tribonacci numbers. So, the term useful bounds is to be read as bounds better than this. Also posted on MathOverflow","['number-theory', 'elementary-number-theory', 'arithmetic-progressions', 'combinatorics', 'pattern-recognition']"
4865581,"Triple-Transitivity/""Specify three know all"" property of exotic transitive $S_5\subset S_6$","Let the exotic transitive subgroup $S_5\subset S_6$ act on $\{1,2,\dots,6\}$ . For $1\leq i,j\leq 6$ , define subsets: $$X_{ji}:=\{\sigma\in S_5\,\mid \sigma(j)=i\}.$$ Does the following properties hold (note 3 implies the other two): Exotic $S_5$ is doubly-transitive in the sense that for all distinct $1\leq j_1,j_2\leq 6$ and (necessarily) distinct $1\leq i_1,i_2\leq 6$ there exists $\sigma\in S_5$ such that $$\sigma(j_1)=i_1\text{ and }\sigma(j_2)=i_2.$$ That is, we have $X_{j_1i_1}\cap X_{j_2i_2}\neq \emptyset$ . Exotic $S_5$ is also triply -transitive in the sense that for all distinct $1\leq j_1,j_2,j_3\leq 6$ and (necessarily) distinct $1\leq i_1,i_2,i_3\leq 6$ there exists $\sigma\in S_5$ such that: $$\sigma(j_m)=i_m\qquad (1\leq m\leq 3).$$ That is, we have $X_{j_1i_1}\cap X_{j_2i_2}\cap X_{j_3i_3}\neq \emptyset$ . Once we know three values of any $\sigma\in S_5$ , we in fact know all six. That is, for suitably distinct $1\leq j_1,j_2,j_3\leq 6$ and distinct $1\leq i_1,i_2,i_3\leq 6$ , the intersection $X_{j_1i_1}\cap X_{j_2i_2}\cap X_{j_3i_3}$ is a singleton. I am led to this question by the following observations. $X_{11}$ is a stabiliser subgroup and the $X_{1j}$ are cosets. Also $\sqcup_{j=1}^6 X_{1j}$ is a partition of $S_5$ and so the cardinality of each must be $|S_5|/6=20$ . Now consider the intersections $X_{11}\cap X_{2j}$ . We know that $X_{11}\cap X_{21}$ is empty and therefore we have five such sets $X_{11}\cap X_{2j}$ . We have the sum of the $|X_{11}\cap X_{2j}|$ is twenty and if each had equal cardinality there four would be five lots of four. Say $|X_{11}\cap X_{22}|=4$ . Now, looking at $|X_{11}\cap X_{22}\cap X_{3k}|$ , there are four of them, and if each of them has equal cardinality then they have to be singletons, with e.g. $X_{11}\cap X_{22}\cap X_{33}=\{e\}$ . I presume this could be sorted out with some GAP code.","['permutations', 'finite-groups', 'symmetric-groups', 'group-theory', 'group-actions']"
4865615,$nt$ derivative of $\displaystyle \frac{1}{A-\cos x}$,"Assuming $A>1$ , I'm triying to obtain the $nt-$ derivative of $\displaystyle f(x)=\frac{1}{A-\cos x}$ at $x=0$ , that is, $f^{(n}(0)$ . First, $$f(x)=\frac{1}{A-\cos x}=\frac{1/A}{1-\frac{\cos x}{A}}=\frac{1}{A}\sum_{k=0}^{\infty}\frac{1}{A^k}\cos^k x$$ and then $$f^{(n}(0)=\sum_{k=0}^{\infty}\frac{1}{A^{k+1}}\,\left.\frac{d^n(\cos^k x)}{d\,x^n}\right|_{x=0}.$$ We have taht odd derivatives of $f(x)$ vanishes at $x=0$ and the even derivatives see $$\left.\frac{d^{2n}(\cos^k x)}{d\,x^{2n}}\right|_{x=0}=\frac{1}{2^k}\sum_{j=0}^k\binom{k}{j}(-1)^{n}(k-2j)^{2n}$$ Thus $$\boxed{f^{(2n}(0)=\frac{(-1)^{n}}{A}\sum_{k=0}^{\infty}\,\frac{1}{(2\,A)^k}\sum_{j=0}^k\binom{k}{j}(k-2j)^{2n}}$$ I wonder if there is some way to simplify this last expression or another procedure to obtain a simplest formula.","['derivatives', 'taylor-expansion', 'sequences-and-series']"
4865620,Can Darboux Theorem be stronger by making the derivative at c continuous?,"Darboux Theorem: If $f$ is differentiable on $[a,b]$ and $\eta$ between $f'(a)$ and $f'(b)$ , then exists $c\in[a,b]$ such that $f'(c)=\eta$ . The common counterexample about $f'$ at $c$ not necessarily continuous is $f(x)=x^2\sin\frac{1}{x}(\mathbb{R}-\{0\}); 0(0)$ , $\eta=0$ and $f'(0)=0$ where $f'$ is not continuous. Yet we have a lot of zeroes around the point $0$ and all of $f'$ there are continuous. It's easy to prove there are these discontinuities of $f'$ are convergence points (Edit: This proof is wrong.), but the points around could also be discontinuities. I want to state that we can find this $c$ such that $f'$ is continuous at $c$ , which is a stronger proposition. Is this statement true? If so, how to prove it, and can we extend it to ""continuous on its neighborhood""? If not, do we have a counterexample?","['continuity', 'derivatives', 'analysis', 'real-analysis']"
4865623,"Is there a theory of ""quadratic"" Hamiltonian evolutions on Poisson manifolds?","I am dealing with a PDE which can be written in the form $$\frac{d}{dt} f(t) = \{a, f(t)\} + \{\{b, f(t)\}, f(t)\}$$ A Hamiltonian equation on a Poisson manifold has the following form: $$\frac{d}{dt} f(t) = \{a, f(t)\}$$ My equation seems to be a sort of ""quadratic"" Hamiltonian evolution. I hope that there exists some theory (existence, uniqueness, etc.) of such equations on Poisson manifolds which leverages the present algebraic structure. I suppose even more generally one could study equations of the form $$\frac{d}{dt} f(t) = \sum_{k=0}^n \{ \dots \{a_k, \underset{k}{\underbrace{f\}, \dots, f\}}}$$ . If one says that the Hamiltonian evolution is the simplest possible ODE one could study on a Poisson manifold, then clearly these equations come right after that in terms of simplicity and naturality.","['ordinary-differential-equations', 'smooth-manifolds', 'hamilton-equations', 'partial-differential-equations', 'poisson-geometry']"
4865638,"Determining limit of multivariable function $ \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)-\sin(2y)}{\cos(x)\cos(y)}$","Evaluate $ \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)-\sin(2y)}{\cos(x)\cos(y)}$ So far my steps have included: $ = \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)-2\sin(y)\cos(y)}{\cos(x)\cos(y)}$ using the double angle identity $ = \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{\cos(y)(1-2\sin(y))}{\cos(x)\cos(y)} = \lim\limits_{(x,y)\rightarrow(\pi,\frac{\pi}{2})} \frac{1-2\sin(y)}{\cos(x)}$ , which allows us to take the limit at the point since the point exists within the domain, so: $ = \frac{1-2\sin(\frac{\pi}{2})}{\cos(\pi)} = \frac{-1}{-1} = 1$ There is no solution included in my textbook so I was hoping someone might verify. I was also wondering if this solution was sufficient or if there were further steps to evaluate the limit, since other questions require taking different paths to confirm the limit existed from every direction. Would this be required here? Thank you in advance!","['multivariable-calculus', 'limits', 'calculus', 'epsilon-delta']"
4865640,Evaluate limit of this trig function,"I'm going through the first openstax calculus book and i'm struggling to understand something.
Let me process this limit as I think it goes and then ask the question about it. Evalulate $$\require{cancel}\lim_{\Theta \to 0}\frac{1-\cos \Theta}{\sin \Theta}$$ \begin{align}
\lim_{\Theta \to 0}\frac{1-\cos \Theta}{\sin \Theta} & = \lim_{\Theta \to 0}\frac{1 -\cos \Theta}{\sin \Theta}\cdot\frac{1 + \cos \Theta}{1 + \cos \Theta} \\
& = \lim_{\Theta \to 0} \frac{1 - \cos^2 \Theta}{\sin \Theta(1+ \cos \Theta)} \\
& = \lim_{\Theta \to 0} \frac{\sin^2 \Theta}{\sin \Theta(1 + \cos \Theta)} \\
& = \lim_{\Theta \to 0}\frac{\sin \Theta}{\sin \Theta}\cdot\frac{\sin \Theta}{1 + \cos \Theta} \\
& = \frac{0}{0}\cdot \frac{0}{2} \\
& = undef
\end{align} I thought this would have been right, however the book says the answer should be $0$ . All I can think of is if it goes like this, picking up on third-last line. \begin{align}
\lim_{\Theta \to 0} \frac{\sin^2 \Theta}{\sin \Theta(1 + \cos \Theta)} 
& = \lim_{\Theta \to 0}\frac{\cancel{\sin \Theta}}{\cancel{\sin \Theta}}\cdot\frac{\sin \Theta}{1 + \cos \Theta} \\
& = \frac{0}{2} \\
& = 0
\end{align} But I thought that's not right because for example, $$f(x) = \frac{\cancel{(x+3)}}{\cancel{(x+3)}(x-1)}$$ Still has a discontinuity at $x = -3$ So the cancelling out doesn't remove the discontinuity and I thougt this would be so with the limit above also. Or is there some other way they came to $0$ that I'm not seeing? Thanks.",['limits']
4865775,Terminology for Complex Algebraic Geometry with Complex Conjugation,"Semialgebraic geometry is essentially real algebraic geometry but with the defining polynomial relations allowed to be inequalities rather than just equalities. This doesn't make sense over $\mathbb{C}$ because it isn't an ordered field, but we do have something not present over $\mathbb{R}$ : a nontrivial automorphism, namely complex conjugation. So, instead of replacing polynomial equations with polynomial inequalities we can instead replace those polynomials in $z_i$ with polynomial in $z_i$ and $\bar{z_i}$ For example, we could have the equation $z = \bar{z}$ , which defines the real line. Is there a name for this theory, the study of graphs of polynomials in complex variables and their conjugates? Note, I am not looking for Hodge theory, which starts with a complex variety and then looks at differential forms in conjugate variables -- I want to start with a generalised variety that uses conjugate variables in its definition, as in the above examples of the real line.","['hodge-theory', 'semialgebraic-geometry', 'several-complex-variables', 'algebraic-geometry', 'terminology']"
4865780,$x^{x^{x^{x^{x^{...}}}}} = 2$. Why is $-\sqrt{2}$ not a solution?,"I just watched a video by blackpenredpen where he solved this equation. Here are the steps he took (The process transitions from one line to another. He didn't explicitly use an implies or iff sign, so I'll leave it as such): $x^{x^{x^{x^{x^{...}}}}} = 2$ $x^2 = 2$ $\left[ 
\begin{array}{l}
x = \sqrt{2} \\
x = -\sqrt{2} 
\end{array} 
\right.
$ At this point, he just crossed out the $-\sqrt{2}$ option. Could someone please explain why $-\sqrt{2}$ is not a solution? Thank you!","['real-analysis', 'calculus', 'power-towers', 'limits', 'radicals']"
