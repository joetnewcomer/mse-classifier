question_id,title,body,tags
3646431,How well-studied is origami field theory?,"It's well known that angle trisection cannot be done with straightedge and compass alone, as Theorem 1. If $z \in \mathbb C$ is constructible with straightedge and compass from $\mathbb Q$ , then $$\mathbb Q (z) : \mathbb Q = 2^n.$$ But the minimal polynomial of $\cos 20 ^{\circ}$ is $8 x ^ { 3 } - 6 x - 1$ , so $$\mathbb Q (\cos 20 ^{\circ}) : \mathbb Q = 3,$$ That proves we cannot trisect $ 60 ^{\circ}$ . However, it's doable with origami, as Huzita Axiom 6 - Computing the Origami Trisection of an Angle shows. My question is: Exactly what field extensions can be obtained by considering origami constructible number? Is this as well-studied as straightedge and compass, i.e. do we have a similar theorem as Theorem 1 ?","['field-theory', 'geometric-construction', 'origami', 'reference-request']"
3646527,"$\{b_n\}$ is a complex sequence for which $\sum_n a_nb_n$ converges whenever $\{a_n\}\in \ell^p$, then $\{b_n\}\in \ell^{p'}$","Let $p$ be such that $1<p<\infty$ and $p'$ be its conjugate exponent.  Show that if $\{b_n\}$ is a complex sequence for which $\sum_n a_nb_n$ converges whenever $\{a_n\}\in \ell^p$ , then $\{b_n\}\in \ell^{p'}$ . So we must be careful here because we do not know if the map $\{a_n\}\mapsto \sum_n a_nb_n$ is a bounded linear functional. So I Instead, look at the partial sums operators $\{a_n\}\mapsto \sum_{n=1}^N a_nb_n$ which I KNOW to be bounded linear maps. But I'm trying to figure out how to compute their norm, because if I do that then maybe I can apply the Uniform Boundedness Principle. Can somebody help me out here? Thanks","['complex-analysis', 'functional-analysis', 'analysis', 'real-analysis']"
3646612,Almost sure convergence of subsequence,If we have for a sequence of identically independent variables with $\limsup \{|X_n|>n\}$ happens almost surely. Can we conclude that $\limsup \{n^{-1}|\sum X_j|>1\}$ happens a.s.? One of the problem I was working on seem to need this to conclude but I don't see why this is true. Any help?,"['sequence-of-function', 'probability-theory', 'probability', 'real-analysis']"
3646658,Derivative function at the point,"Can anyone help me out with the question below? Let $ f: \Bbb{R}→ \Bbb{R} $ be a function such that $ \vert f (x) −f (p)\vert\le \vert x-p \vert^{3\over 2}$ , for all $ x, p \in \Bbb{R}$ . Show
that $ f$ is derivable, calculating its derivative at each point.","['calculus', 'derivatives']"
3646679,Integral inequality with Cauchy-Schwarz-inequality,"Let $f:[a,b] \to \mathbb{R}$ be a differentiable function with $f(a)=0$ and so that $f'$ is continuous. Show that $$\int_a^b |{f(x)f'(x)}|\:\mathrm{d}x \leq \frac{b-a}{2} \int_a^b f'(x)^2\: \mathrm{d}x.$$ I received a hint for examining the function $G(x):=\int_a^x |f'(t)|\:\mathrm{d}t$ but haven't really gotten anywhere with it. Any tips would be greatly appreciated","['integration', 'cauchy-schwarz-inequality', 'calculus', 'functions', 'derivatives']"
3646706,Prove that $(a_1-a_2)+(a_2-a_3)+....$ converges iff ${a_n}$ converges,"Let $b_n=a_n-a_{n+1}$ . We first assume that ${a_n}$ converges so , $\lim(b_n)=\lim(a_n)-\lim(a_{n+1})$ , hence $\lim(b_n)=0$ . Now let $s_m$ and $s_n$ be the consecutive partial sums of ${b_n}$ . So $|s_n-s_m| = |a_{n+1}-a_{m}|=|a_{n+1}-L+L-a_m|< \epsilon$ for $N\ge M \ge M(\epsilon)$ .So the series is cauchy convergent . Now let us assume that the series is convergent so the sequence formed by the partial sums are convergent hence $s_M=a_1-a_{M+1}$ hence $-\lim(s_M) +a_1 = \lim(a_{M+1})$ .So $A=-S+a_1$ . Can someone go through my attempt and point out my mistake instead of suggesting another answer.","['limits', 'sequences-and-series', 'real-analysis']"
3646725,Identity relating coefficients of degrees $0$ and $1$ from characteristic polynomials,"Let $A$ be a square matrix each of whose columns has sum $1$ . Let $B$ be the matrix obtained by replacing the lowest row in $A-I$ with a row of ones. Can anybody show (or find a counterexample) that $-\det(B)$ is exactly the degree $1$ coefficient in the characteristic polynomial $\det(A-I-tI)$ ? With a computer I have checked this for $n=2,3,4,5$ . Even though the hypothesis does not ask that the entries of $A$ be non-negative, the problem has a stochastic flavour, so I added a [stochastic-matrices] tag.","['matrices', 'stochastic-matrices', 'linear-algebra', 'commutative-algebra']"
3646766,Prove that no 5 digit EXTREME PRIMES exist.,"I have been trying to come up with my own math problems recently and this is one of my first. It introduces the idea of an extreme prime. I hope that an extreme prime isn't already a thing, because I just used the name to describe a special number. I have a solution to the problem, but I'd like to see smarter solutions and get some feedback on the problem so I can make better ones in the future. An extreme prime is a number such that every number within the number is prime, expect one-digit numbers, and the number itself is prime. Examples are below for clarity, as I'm bad at explaining. Examples: $617$ is a prime. Also, $61$ is a prime and $17$ is a prime. Therefore $617$ is an extreme prime. Note $6$ is composite: the digits need not be prime. $1373$ is prime. Also, $13$ is prime, $37$ is prime, $73$ is prime, $137$ is prime, $373$ is prime. Therefore $1317$ is an extreme prime. Fun fact: $373$ is also the only $3$ digits extreme prime where the digits are prime, so I guess it must be ultra-prime. The question is to prove that no $5$ digit extreme prime exists. I'm looking forward to some feedback and some ways I can word what an extreme prime is, hope it is fun to solve. Some other facts I noticed when checking my proof with python (which I do not have a proof for):
you may like to try to prove them. A $3$ digit extreme prime cannot contain a $2,8$ or $5$ . A $4$ digit extreme prime cannot contain a $2, 8, 5$ or $4$ . A $4$ digit extreme prime never starts with $7$ . Quite a few super primes (primes that occupy prime numbered positions in the sequence of all prime numbers) are extreme primes. Can you find them all and create the prime-st number set of all time!","['contest-math', 'number-theory', 'elementary-number-theory', 'recreational-mathematics', 'prime-numbers']"
3646779,A subinvariant random variable is already invariant,"Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space, $\tau:\Omega\to\Omega$ be a measurable map on $(\Omega,\mathcal A)$ with $\operatorname P\circ\:\tau^{-1}=\operatorname P$ and $X:\Omega\to\overline{\mathbb R}$ be $\mathcal A$ -measurable with $$X\circ\tau\le X\;\;\;\operatorname P\text{-almost surely}\tag1.$$ I would like to conclude $$X\circ\tau=X\;\;\;\operatorname P\text{-almost surely}\tag2.$$ I assume that this is somehow almost trivial, but I can't figure out how we need to approach it. Maybe by showing that $\{X\circ\tau\ge X\}$ has probability $1$ or by showing that $\{X\circ\tau<X\}$ is a null set?","['measure-theory', 'probability-theory', 'dynamical-systems']"
3646826,"If $a ≡ b$ (mod $n$), prove that $a^5 ≡ b^5$ (mod $n$) [duplicate]","This question already has answers here : Proving $\, a\equiv \bar a\Rightarrow a^b\equiv \bar a^b\, \pmod{\!n},\, $ e.g. $\,\bar a = a\bmod n$ [Congruence Power Rule] (4 answers) Closed 4 years ago . If $a ≡ b$ (mod $n$ ), prove that $a^5 ≡ b^5$ (mod $n$ ). I know there is some way of solving this using binomial expansion but it has me stumped.","['elementary-number-theory', 'modular-arithmetic', 'discrete-mathematics']"
3646840,how to prove $(x+1)^{(x-1)} \leq x^x$? [duplicate],"This question already has answers here : When $x$ is a real number and $x>1$, why is $x^x>(x+1)^{x-1}$? (6 answers) Closed last month . I need to prove the statement: $$\forall x \ge 1, \quad \log_{2}(x!) - \frac{1}{2}x\log_2(x) \geq 0$$ I tried with a proof by induction (if there is an easier way, let me know!), so I tested the basis case $x = 1$ , which holds, and by induction hypothesis, assumed that $\log_{2}(x!) - \frac{1}{2}x\log_2(x) \geq 0$ holds. Now I want to come to the conclusion that $$\log_{2}((x+1)!) - \frac{1}{2}(x+1)\log_2(x+1) \geq 0$$ After some manipulations, I find that $$\log_{2}((x+1)!) - \frac{1}{2}(x+1)\log_2(x+1) = \log_{2}(x!) - \frac{1}{2}\left[x\log_2(x+1)-\log_2(x+1)\right]$$ So if $$x\log_2(x+1)-\log_2(x+1) \leq x\log_2(x)$$ then $$\log_{2}((x+1)!) - \frac{1}{2}(x+1)\log_2(x+1) \geq 0$$ Again, after some manipulations, I come to this inequality $(x+1)^{(x-1)} \leq x^x$ . I know it is true, but I can't figure out how to prove it.","['algebra-precalculus', 'functions', 'exponential-function']"
3646856,Using differential equations to determine the number of rolls on a roll of toilet paper,"Puzzle: A role of toilet paper has $180$ sheets on it. The outside is covered with exactly two sheets. The inside around the cardboard cylinder is covered by exactly one. Question of the puzzle: how many layers of toilet paper are on the roll of toilet paper? The ""Given"" Solution: One way to solve this is by saying that the average round is covered by $1.5$ sheets, so therefore the answer is $120$ (I have no source for whether this is actually correct) I Tried: I tried to solve it with a differential equation, but ultimately failed:  Let $S$ be the number of toilet paper sheets on the roll, and $n$ the number of rotations. I think the number of sheets per rotation depends linearly on the number of rotations at a given point, because with every rotation the toilet role becomes more thick, so: $$\frac{\mathrm{d}S}{\mathrm{d}n}=kn.$$ This equation is separable, so $$dS=(kn)\,\mathrm{d}n.$$ Integrate to get $$S=\frac{1}{2}kn^2+C.$$ Now we need to find the values of constants $k$ and $C$ : we know that when $S=1$ then $n=1$ and also when $S=180$ then $\frac{dS}{dn}=2$ . But now I am stuck. My Question: What would be the correct way of solving this problem using differential equations ? This question is also linked to this question: Toilet paper puzzle (question 2)","['calculus', 'puzzle', 'ordinary-differential-equations']"
3647014,"Under what conditions can $L^p(X,\sigma,\mu)$ be an inner product space for $p\neq 2$?","My measure theory book* claims that $L^p(X,\sigma,\mu)$ is an inner product space only for $p=2$ , where it is implied that the inner product induces the $p$ -norm on $L^p$ . I have seen proofs of this statement for specific $L^p$ spaces, like $\ell^p(\mathbb N)$ ( here ) and $L^p[0,1]$ ( here ). However, I think this claim cannot be true in general, because of the following counterexample. Let $(X,\sigma)$ be any measurable space and let $\mu_0$ denote the trivial measure $\mu(A)=0$ for all $A\in\sigma$ . Then $L^p(X,\sigma,\mu)$ is the Banach space $\{0\}$ , which is trivially an inner product space. Are there any other examples where $L^p(X,\sigma,\mu)$ is an inner product space (where the inner product induces the $p$ -norm) with $p \neq 2$ ? If so, what are the strictest conditions on $(X,\sigma,\mu)$ under which $L^p(X,\sigma,\mu)$ is an inner product space only for $p=2$ ? *René Schilling, Measures, Integrals, and Martingales (2nd Edition), p. 326.","['measure-theory', 'inner-products', 'normed-spaces', 'lebesgue-integral', 'lp-spaces']"
3647038,Are integrals in Riemann-Lebesgue theorem Riemann or Lebesgue,"Riemann-Lebesgue theorem says that if $f$ is Lebesgue-integrable on $\mathbb R$ that \begin{equation}
\lim_{n\to+\infty}\int_{-\infty}^{+\infty}f(x)\cos(nx)\,dx=0.
\end{equation} Are integrals $\int_{-\infty}^{+\infty}f(x)\cos(nx)\,dx$ Riemann improper integrals or Lebesgue integrals? If they are Riemann, why do they exist?","['riemann-integration', 'measure-theory', 'lebesgue-integral']"
3647084,The Galois group of polynomial $p(x)\in\mathbb{K}[x]$ is cyclic and is generated by $q(x)\in\mathbb{K}[x]$.,"My question here is motivated by this question and that question .  The subject of particular interest is when $n=3$ and $\mathbb{K}=\mathbb{Q}$ . Let $\mathbb{K}$ be a field with the algebraic closure $\bar{\mathbb{K}}$ .  For a given integer $n\geq 2$ , suppose that $$p(x)=x^n+a_1x^{n-1}+a_2x^{n-2}+\ldots+a_{n-1}x+a_n$$ is an irreducible polynomial in $\mathbb{K}[x]$ such that the Galois group $G:=\text{Gal}(p/\mathbb{K})$ of $p(x)$ over $\mathbb{K}$ is isomorphic to the cyclic group $C_n$ of order $n$ .  We say that a polynomial $$q(x)=b_1x^{n-1}+b_2x^{n-2}+\ldots+b_{n-1}x+b_n$$ in $\mathbb{K}[x]$ augments $p(x)$ if $q$ generates $G$ .  That is, we can order the roots of $p(x)$ as $r_1,r_2,\ldots,r_n\in\bar{\mathbb{K}}$ in such a way that $$r_j=q(r_{j-1})$$ for $j=1,2,\ldots,n$ (where $r_0:=r_n$ ).   Furthermore, we also say that $q(x)\in\mathbb{K}[x]$ of degree at most $n-1$ is an $n$ -augmentation polynomial if $q(x)$ augments some monic irreducible polynomial $p(x)\in\mathbb{K}[x]$ of degree $n$ such that $\text{Gal}(p/\mathbb{K})\cong C_n$ . What are $n$ -augmentation polynomials $q(x)\in\mathbb{K}[x]$ for a given positive integer $n$ ? A trivial example is when $n=2$ .  Let $a_1$ be an element of $\mathbb{K}$ such that $$p(x)=x^2+a_1x+a_2$$ is irreducible for some $a_1\in\mathbb{K}$ .  Then, $$q(x)=-x-a_1$$ is a $2$ -augmentation polynomial.  These are all possible $2$ -augmentation polynomials. Let now $n>2$ . For a given $m\in\mathbb{Z}_{\geq 0}$ , let $f^{[m]}(x)$ denote the $m$ -time iteration of $f(x)\in\mathbb{K}[x]$ .  A necessary condition is that $\tilde{q}_n(x):=\prod\limits_{d\mid n}\big(q^{[d]}(x)-x\big)^{\mu\left(\frac{n}{d}\right)}$ has an irreducible factor of degree $n$ (here, $\mu$ is the Möbius function ).  I conjecture that this is also a sufficient condition, and any irreducible factor $p(x)$ of $\tilde{q}_n(x)$ such that $p(x)$ is monic and has degree $n$ is augmented by $q(x)$ .  Is my conjecture (restated below) true? Conjecture. A polynomial $q(x)\in\Bbb{K}[x]$ of degree less than $n$ is an $n$ -augmentation polynomial if and only if $$\tilde{q}_n(x):=\prod\limits_{d\mid n}\big(q^{[d]}(x)-x\big)^{\mu\left(\frac{n}{d}\right)}\in\mathbb{K}[x]$$ has an irreducible factor of degree $n$ .  Furthermore, $q(x)$ augments every monic irreducible factor $p(x)$ of degree $n$ of $\tilde{q}_n(x)$ . Closing Remark. If $G:=\text{Gal}(p/\mathbb{K})$ is cyclic, then it is generated by some $\gamma\in\text{Aut}_\mathbb{K}(\bar{\mathbb{K}})$ .  Fix a root $r_1\in\bar{\mathbb{K}}$ of $p(x)$ .  Because $G$ is cyclic (in particular, $|G|=n=\big[\mathbb{K}(r_1):\mathbb{K}\big]$ ), $p(x)$ splits into linear factors over $\mathbb{K}(r_1)$ .  Thus, $r_2:=\gamma(r_1)$ is in $\mathbb{K}(r_1)$ .  Therefore, there exists a polynomial $q(x)\in\mathbb{K}[x]$ of degree less than $n$ such that $$r_2=q(r_1)\,.$$ Define $r_j:=\gamma^{[j-1]}(r_1)$ for $j=1,2,\ldots,n$ (and $r_0:=r_n$ as before).  Then, the equality above ensures that $r_j=q(r_{j-1})$ for $j=1,2,\ldots,n$ . Conversely, suppose that there exists $q(x)\in\mathbb{K}[x]$ such that the roots $r_1,r_2,\ldots,r_n\in\bar{\mathbb{K}}$ of $p(x)\in\mathbb{K}[x]$ satisfy $r_j=q(r_{j-1})$ for $j=1,2,\ldots,n$ .  Then, it follows that $\text{Gal}(p/\mathbb{K})$ is generated by $q$ , whence it is a cyclic group.  Therefore, the condition that $p(x)$ has a cyclic Galois group is both necessary and sufficient for this question. However, that $p(x)$ splits into linear factors over $\mathbb{K}(r_1)$ for some root $r_1\in\bar{\mathbb{K}}$ is not sufficient for the setting of this question.  For example, when $\mathbb{K}=\mathbb{Q}$ and $p(x)=x^4-10x^2+1$ , then the roots of $p$ are $\pm\sqrt{2}\pm\sqrt{3}$ , and $p(x)$ splits into linear factors over $\mathbb{Q}(r_i)$ for any root $r_i$ of $p(x)$ .  For any choice of indexes of the roots $r_i$ of $p(x)$ , you can write $r_2=q(r_1)$ for some $q(x)\in\mathbb{K}[x]$ , you cannot expect that $r_3=q(r_2)$ , $r_4=q(r_3)$ , and $r_1=q(r_4)$ .  This is because $\text{Gal}(p/\mathbb{K})$ is not cyclic.  It is isomorphic to $C_2\times C_2$ .  (That is, if you have $r_2=q(r_1)$ , then you will have $r_1=q(r_2)$ .)  Anyway, the condition that $p(x)$ with roots $r_1,r_2,\ldots,r_n\in\bar{\mathbb{K}}$ splits into linear factors over $\mathbb{K}(r_1)$ is necessary and sufficient for the existence of polynomials $q_j(x)\in\mathbb{K}[x]$ such that $r_j=q_j(r_1)$ for all $j=1,2,\ldots,n$ , which is equivalent to the condition that $\big|\text{Gal}(p/\mathbb{K})\big|=n$ .","['irreducible-polynomials', 'roots', 'galois-theory', 'abstract-algebra', 'polynomials']"
3647108,What is this integrating-factor approach to homogeneous ODE's good for?: Better for some equations? Illustrates something worthwhile?,"We have an ODE of the form $$M(x,y)\,dx + N(x,y)\,dy = 0$$ in which both $M$ and $N$ are homogeneous functions of the same degree. The standard method for handling such an equation (assuming it's not separable or linear, in which case(s) we have a choice of ""standard methods"") is to use the substitution $\{y=ux, dy=udx+xdu\}$ , obtaining a separable equation. If the resulting integral in 'u' fails to be nice (the one on the 'x' side is always nice), we can try swapping the roles of $x$ and $y$ for an alternative substitution. This is all very standard, AFAIK. Now, from an exercise in a book (Boyce & DiPrima), I see that any homogeneous equation may be made exact via the integrating factor $$\mu(x,y)=\frac{1}{xM(x,y)+yN(x,y)}.$$ This is great, it illustrates that integrating factors can exist that aren't constant in either variable, and doing it this way is an interesting exercise, as is proving that the given $\mu$ actually works. However... is there any example of a homogeneous DE where this is a better method than just using the normal substitutions? I haven't encountered an example that makes me think: ""This is the nice way to do it!"" Can anyone display a nice example, that plays to the strengths of this technique? Does this method perhaps exist more as a curiosity than as a practical tool? Does the resulting exact equation give us access to some kind of insight we would not have encountered the other way? Is the interesting part simply the fact that any homogeneous equation can be made exact?","['integrating-factor', 'homogeneous-equation', 'ordinary-differential-equations']"
3647159,An impressive combinatorial identity,"I'm trying to show that, $\forall n\in\mathbb{N},\forall p\in\mathbb{N^*},$ $$\sum_{k=1}^{n+1}\left(\frac{4^k}{2k-1}{2k\choose k}\frac{\displaystyle{n+k-1\choose n-k+1}{2(n+p-k)\choose n+p-k}}{\displaystyle{n+p\choose k}}\right)=\frac{8}{p}{2(p-1)\choose p-1}\frac{\displaystyle{2(2n+p)\choose2n+p}}{\displaystyle{2n+2p\choose2n+p}}$$ I have checked numerically, I'm confident it's true. But I have absolutely no idea as of how to even start to prove this. Any suggestion ?","['summation', 'combinatorics', 'discrete-mathematics', 'real-analysis']"
3647267,Prove nontrivial topological properties,"Let $C$ be a compact set in $\Bbb{R}^n$ with the following property: for any $\varepsilon> 0$ there exists a  finite number $M(=M(\varepsilon))$ of open balls $B_i$ with radius $r_i$ , $i\in\{1,\ldots,M\}$ s. t. $\bigcup\limits_{i=1}^{M}B_i \supset C $ and $\sum\limits_{i=1}^{M} r_i \leqslant \varepsilon $ Prove the following : (1) for $n \geqslant  2 \; \Bbb{R}^n \setminus C$ is connected (2) for $n \geqslant 3 \;\Bbb{R}^n\setminus C$ is simply connected   (I mean by that it has trivial fundamental group) The only thing I was able to try were proof by contradiction or trying to understand intuitively how this space is made  but I did not conclude . 
In particular , I tried this: for $\varepsilon > 0$ call $S_{\varepsilon }$ the finite cover of $C$ made by a finite number of balls whose sum of radii is less or equal than $\varepsilon$ ,say it $\{B_{i}^{\varepsilon}\}_{i=1}^{M(\varepsilon)},$ which exists by hypotesis.
 I noticed that, since $C$ is closed , if a point $x$ does not belong to $C$ , then $L=\operatorname{dist}(x,C)  >  0$ . Choosing a cover of $C$ of the form $S_{\varepsilon } $ for $\varepsilon < L$ and assuming that every element of $S_{\varepsilon }$ intersects nontrivially $C$ , I can argue that $x \notin B_i^\varepsilon $ for any $i$ .
Otherwise, there would be a point $y$ in $C$ s. t. $\operatorname{dist}(x,y)< L$ which is not possible. Define $A_{\varepsilon}= \bigcup\limits _{i=1}^{M(\varepsilon)} B_i^\varepsilon$ .From what said, if I consider a small enough $\varepsilon$ ,then $x \notin A_{\varepsilon }$ . Choosing $\varepsilon = \frac{1}{i}$ for $i \in \Bbb{N}$ from what  said  I could argue that $\mathbb{R}^n \setminus C = \bigcup\limits_{i=1}^{\infty}(\Bbb{R}^n\setminus A_{\frac{1}{i}})$ Manipulating a bit  this equality (not straightforward ) ,if, for example I may prove that $\exists j \geqslant 1 $ s. t. $\mathbb{R}^n\setminus A_{\frac{1}{k}}$ is  connected for any $k \geqslant j$ , I may easily get point (1). Anyway I do not see how to prove it. I do not even see clearly if my previous observations are correct. REMARK: Someone suggested this set is discrete , I think that this not set is necessarily discrete: If I take $C=\{(\frac{1}{n},0) \cup {(0,0)}\}$ , $n \in \mathbb{N}$ ,and choose $\epsilon > 0$ I can find a cover of $C$ according to the hypotesis: Choose a ball centered in $(0,0)$ of radius less than $\frac{\epsilon}{2}$ . You are left to cover a finite number $M$ of points. For any point,choose a open ball centered in itself with radius less than $\frac{\epsilon}{2M}$ . Formally you can do it because a point has a zero lebesgue measure and you can cover it with a ball of any radius you want. This cover respects the hypotesis. Since I found in a standard topology exercise set ,its solution should not require big  machinery, so I would really appreciate a solution of this kind. In particular, it is a exercise from an admission test to a fourth year of universiy, so I assume  it is possible to use standard general topology(from a university course), fundamental groups, universal convering and homology.Please help. Thanks in advance.","['general-topology', 'algebraic-topology', 'connectedness']"
3647290,Rotman's Introduction to the Theory of Groups Exercise 1.27,"My try for (i) $G = \{a_1,\dots,a_n\}$ . If $n$ is even, an odd number of $a_j \ne e$ must have its inverse among the $a_j\ne 0$ , then at least one $a_j\ne 0$ must be its own inverse, which contradicts the hypothesis. Thus $n$ must be odd. Thus each $a_j\ne 0$ is multiplied by its inverse in $a_1*a_2*\cdots*a_n$ so it equals $e*e = e$ . But I think something must be wrong with this proof because in (ii), using the hint, we would have that $\mathbb{Z}_p-\{0\} = \{1,\dots, p-1\}$ is a group satisfying the hypothesis in (i) with $n = p-1$ even for most $p$ . Also, this won't lead to the answer because we would have $[(p-1)!] = [1]$ which leads to $(p-1)!\equiv_p 1$ and $1$ isn't equivalent to $-1\mod{p}$ for all $p$ prime. What I'm doing wrong? Thanks in advance!","['elementary-number-theory', 'group-theory', 'abstract-algebra', 'finite-groups']"
3647315,What is the formal definition of the singular support of a distribution?,"The definition I have is that：For a distribution $u \in \mathcal{D}'(U)$ where $U$ is an open subset of $\mathbb R^n$ , a point $x$ is in the singular support of $u$ if $u$ is not smooth on an open set containing $x$ . This definition does not make sense to me. How should one interpret the smoothness of $u$ on an open open subset of $U$ ? The only thing that I can think of is that $u$ is smooth if $u$ can be given by integration against a smooth function, but I am not sure whether this is the right definition or not. For instance take $f \in C^{\infty}_{c}(\mathbb R^n)$ , $F \in \mathcal{D}'(\mathbb R^n)$ , how is $fF$ a compactly supported smooth function which is supposed to be only a distribution by definition?","['distribution-theory', 'functional-analysis', 'real-analysis']"
3647375,Number of non-zero entries in $A$ and $AA^T$,"Let $A$ be a matrix with each entry $0$ or $1$ such that each row contains at most $r$ $1$ 's and each row of $AA^T$ contains at most $z$ non-zero entries. Can we somehow bound from above the number of $1$ 's on each column of $A$ in terms of $r$ and $z$ only? I get lost in the use of the matrix multiplication calculations.
(The bound need not be tight but should be expressible as a simple function of $r$ and $z$ .) Any help appreciated!","['linear-algebra', 'combinatorics']"
3647458,"Suppose that for two $n \times n$ matrices $A,B$ we have $AB = A + B$. Prove that $\text{rank}(A^2) + \text{rank} (B^2) \leq 2 \text{rank} (AB).$","Suppose that for two $n \times n$ matrices $A,B$ we have $AB = A + B$ . Prove that $\text{rank}(A^2) + \text{rank} (B^2) \leq 2 \text{rank} (AB).$ This reminds me of Sylvester's Rank Inequality theorem, but I'm not sure if that's really helpful here. I haven't really made significant progress on this beyond writing out a few matrix multiplication. Would appreciate any help at all! Thank you.","['matrices', 'inequality', 'linear-algebra', 'matrix-rank']"
3647519,the limit of absolute value is the absolute value of the limit,"Let A $\subset$ $\Bbb R$ , let $f$ : A $\to$ $\Bbb R$ and let $c$ $\in$ $\Bbb R$ a acumulation point of A. If $\lim_{x\to c} f$ exist and if $|f|$ is the function defined for $x$ $\in$ A, by $|f|(x) = |f(x)|$ , then prove that $\lim_{x\to c} |f| =$ $\ $ | $ lim_{x\to c} f $ | $ $ I have the idea to prove it, with the triangular inequality, but... it's not clear to me. Edit: This is the proof Let $L$ = $\lim_{x\to c} f$ , by definition of limit, for all $\epsilon > 0$ , exist $\delta >0$ such that $0<|x-c|<\delta $ then $|f(x)-L|<\epsilon$ , now, by triangular inequality $||f(x)|-|L||<|f(x)-L|$ , so, when $|x-c|<\delta $ , $||f(x)|-|L||<|f(x)-L|<\epsilon$ , so , $\lim_{x\to c} |f| = |L| $ and $\lim_{x\to c} |f| =$ $\ $ | $ lim_{x\to c} f $ | $ $","['limits', 'analysis', 'real-analysis']"
3647528,Find Mobius transformations $\varphi$ satisfying $\sum (1-|\varphi_n(z)|)<\infty$ in the unit disc.,"Suppose that $\varphi$ is a Mobius transformation which maps the unit disc onto itself. Let $\varphi_n(z)=\varphi(\varphi_{n-1}(z))$ , where $n=1,2,\ldots$ and $\varphi_0(z)=z$ . Find all $\varphi$ satisfying the condition $\sum (1-|\varphi_n(z)|)<\infty$ in the unit disc if $\varphi$ has a unique fixed point on the unit circle. My thought: Suppose $\varphi(z)=\frac{az+b}{cz+d}$ with $ad-bc=1$ . From the hypotheses, we know that $c \neq 0$ , $a+d=\pm 2$ and the fixed point will be $\frac{\pm 1-d}{c}$ . Then I don't know how to use these to determine $\varphi$ . Any help?","['complex-analysis', 'mobius-transformation']"
3647540,Is the proof of $(A\cap B) \subseteq A$ correct?,I need to proof that: $(A\cap B) \subseteq A$ is true. My attempt was: $$(A\cap B) \subseteq A = \{\forall x: (x\in (A\cap B)) \to (x \in A)\}$$ $$= \{\forall x: ((x\in A) \land (x\in B)) \to (x\in A)\}$$ Using the tautology $p \land q \to p$ the statement is always true. Is this correct or there are some observations to improve?,"['elementary-set-theory', 'solution-verification']"
3647549,Find the limit of a random variable,"I'd like to find distribution of the almost sure limit $X_\infty$ of $X_{t+1}=
 \begin{cases}  
      1-p+pX_t & \text{w/prob: } X_t \\
      pX_t & \text{w/prob: } 1-X_t
   \end{cases}$ where $X_t$ is a random variable (and also a martingale), and $p\in [0,1]$ . Firstly, to show it converges a.s., i need to show $P(\lim \sup X_n = \lim \inf X_n)=1.$ (Which i guess makes sense intuitively, as sup $X_n$ is evaluated when $X_n=1$ and inf $X_n$ when $X_n=0$ , right?) Then i'm not sure how to  take the limit for any $\omega \in \Omega$ , i.e. $\underset{n \to \infty}{\lim} X_n(\omega)$ .. i guess looking at probabilites since it is discrete and applying Borel Cantelli or something?","['limits', 'measure-theory', 'probability-theory', 'real-analysis']"
3647593,Solving the iterated equation $f^{\circ n}(x)=f(x)^k$,"On my spare time, I'm trying to solve equations of the form $$f^{\circ n}(x)=f(x)^k,\quad n,k\in \mathbb{Z}$$ where $f^{\circ n}(x)=f\circ f\circ\dots\circ f$ , $n$ times. I know $f(x)=x^{\sqrt[n-1]{k}}$ is a solution, but I cannot prove if there's a more general solution. How did I get the solution? I assumed that the equation had a solution of the form $x^t$ , then I solved for $t$ in : $x^{tk}=f(x)^k = f^{\circ n}(x) = x^{t^n}$ For $n=2$ and any $k$ , that is $f\circ f(x) = f(x)^k$ , I am able to prove that the only solution is $f(x)=x^k$ , without using my general solution. My problem lies when $n>2$ or $n<0$ (inverse functions). For instance, for $f\circ f\circ f(x)=f(x)$ , I know the solution $f(x)=x^{\pm1}$ works (from my general solution), but I can't prove how to get it without using my general solution, and I can't prove its uniqueness. Same goes for $n=k=-1$ , in other words $f^{-1}(x) = \frac{1}{f(x)}$ . I know $f(x)=x^{\pm i}$ is a solution (from my general solution), I can't get there without my general solution and I can't prove the uniqueness. I tried taking the derivative and solving this way (it worked for $n=2$ ), but got nowhere.","['iterated-function-system', 'ordinary-differential-equations', 'function-and-relation-composition']"
3647643,A homogeneous polynomial which is identically equal to 0,"I totally believe that there should be a proof to the following statement: Let $A,B$ and $C$ be three homogeneous polynomials in $\mathbb{C}[x,y,z]$ of the same degree $d>1$ . And assume that $$
xA+yB+zC \equiv 0 \quad (\text{identically equal to } 0)
$$ then all of $A,B$ and $C$ have to have common zero(s) which are not $(x,y,z)=(0,0,0)$ . But I realize that it is not obvious to me. Is there a way to prove the above statement? Or in fact this is not true? Any comment is welcome.","['algebraic-geometry', 'polynomials', 'commutative-algebra']"
3647666,Number of $2 \times 2$ matrices over the finite field $\mathbb{F}_q$ whose minimal polynomial is divisible by $X-1$.,"I want to calculate the number of $2 \times 2$ matrices over the finite field $\mathbb{F}_q$ whose minimal polynomial is divisible by $X-1$ . The characteristic polynomial must be $(X-1)(X-a)$ for some $a \in \mathbb{F}_q$ . If $a \neq 1$ then each of them is similar to $\text{diag}(1,a)$ . but how do I count the number of matrices in this case ? Another case if $a=1$ , then either the matrix is $2 \times 2 $ identity or similar to its JCF which is upper triangular with all nonzero entry $1$ . But again how do I count total number of matrices in each cases except the identity situation ? I need help.","['finite-fields', 'vector-spaces', 'matrices', 'linear-algebra', 'linear-transformations']"
3647678,On algebraic minimal surfaces,"In the theory of minimal surfaces, we could construct various types of minimal surface based on the way we choose a pair of holomorphic & meromorphic fucntion. To be more specific, let $f(\zeta)$ be a holomorphic function on an open set $U$ in the complex plane, not identically zero while $g(\zeta)$ be a meromorphic function on the same domain such that if $\zeta_0$ is a pole of order $m \geq 1$ of $g$ then it is a zero of order $\geq 2m$ of $f$ then we have a parametrization of a minimal surface $$\sigma(u,v) = \mathfrak{Re} \int \varphi(\zeta)d\zeta, \zeta = u + iv$$ in which $\sigma$ is a surface patch and $\varphi = \frac{1}{2}(f(1-g^2),if(1+g^2),2fg)$ . For example when $(f,g) = (2,\zeta)$ we have the classical Enneper surface $$\sigma(u,v) = (u - \frac{u^3}{3} + uv^2, -v + \frac{v^3}{3} - u^2v, u^2 - v^2)$$ In this wikipedia , I know that the classical Enneper surface is algebraic which means that it can be defined as the zero locus of a polynomial of degree $9$ . Naturally, I can generalize this example by choosing $(f,g)=(2,\zeta^n)$ which gives us $$\sigma(u,v) = (x(u,v),y(u,v),z(u,v)$$ in which $$x(u,v) =  u - \frac{1}{2n+1}\sum_{k=0}^n \binom{2n+1}{2k+1}u^{2k+1}(iv)^{2(n-k)}$$ $$y(u,v) =  -v - \frac{1}{2n+1}\sum_{k=0}^{n}\binom{2n+1}{2k}u^{2k}i^{2(n+1)-2k}v^{2n+1-2k}$$ $$z(u,v) =   2\sum_{2 \mid k-n-1}\binom{n+1}{k}u^k i^{n+1-k}v^{n+1-k} \ (0 \leq k \leq n+1)$$ and hope these general Enneper surfaces are algebraic. A quick search convinces me that this class of Enneper surfaces are algebraic so I ask for books providing a criterion for determining whether a given minimal surface is algebraic or not, at least in case the pair $(f,g)$ are both polynomials in $\zeta$ . Moreover, if it is algebraic then is there an algorithm to derive its defining polynomial-equation? Other questions, let $\mathcal{S}$ be a algebraic minimal surface (obtained by a pair $(f,g)$ as above) we define $\mathrm{deg}(\mathcal{S})$ to be the least degree of a polynomial (in three variables) defining $\mathcal{S}$ . Is there a lower bound for $\mathrm{deg}$ ? For a given $n \in \mathbb{N}$ , how can we know whether there exists a surface $\mathcal{S}$ with $\mathrm{deg}(\mathcal{S})=n$ ?","['algebraic-geometry', 'minimal-surfaces', 'reference-request', 'differential-geometry']"
3647743,"Find the polynomial $P(x)$ s.t. for any $a,b\in\Bbb C$ s.t. $a^2+b^2=ab$, $P(a+b)=6\big(P(a)+P(b)\big)+15a^2b^2(a+b)$.","Suppose that $P(x)$ is a polynomial such that $$P(a+b)=6\big(P(a)+P(b)\big)+15a^2b^2(a+b)$$ for any complex numbers $a$ and $b$ satisfying $a^2+b^2=ab$ , then find the polynomial $P(x)$ . I don't know how to even approach this one, it seems very complex. Your help is appreciated!","['contest-math', 'functional-equations', 'functions', 'polynomials', 'complex-numbers']"
3647744,Properties of the remainders from division into primes,"This is a question that has bothered me for almost 6 years now on and off, and I still don't really know enough to tackle it. To phrase it somewhat formally: Let $P$ be the series of prime numbers such that $P(i)$ is the i-th prime number. Let $N$ be a positive integer with a value greater than $2$ . Let $X$ be the series of numbers such that $X(0) = 0$ and $X(i) = (X(i - 1) + P(i)) \pmod N$ Does there exist an $N$ for which the set of elements $X(0),\ldots,X(N-1)$ contain every number from $0$ to $N-1$ exactly once? Can we prove it one way or another? I've made computer simulations and let them run overnight and the results seem to suggest that no, there is no such N, but the graphs I got out of doing this are fairly interestingly shaped. For instance, this is a scatter plot where the $x$ -axis is $N$ and the $y$ -axis is the percent of numbers in $X(0), \ldots, X(N-1)$ that were reached before a duplicate. And if we zoom in we can see some definite ""structure"" to the proportions, which is also interesting. And on the long tail there seems to be a definite ""range"", with some outliers I don't know how to explain why the graph is shaped like it is, or what the structure in it means, or why it seems like a ""thick"" logarithmic curve. The shape would seem to imply. Also interestingly, the minimum proportion seems to approach ~0.0001592, which I have no clue the significance of.","['modular-arithmetic', 'prime-numbers', 'sequences-and-series']"
3647759,Determinant of matrix with constant lines apart diagonal,"I would like to compute the determinant of a matrix with the following structure: \begin{equation}
\begin{pmatrix}
D_1 & l_1 & l_1 &\cdots & l_1 \\
l_2 & D_2 & l_2 &\cdots & l_2 \\
l_3 & \cdots & D_3  &\cdots & l_3 \\
l_4 & \cdots & l_4 & D_4  & l_4 \\
l_5 & \cdots & \cdots & l_5 & D_5  \\
\end{pmatrix}
\end{equation} That is, it is constant on each line apart from the diagonal. $l_i, D_i \in \mathbb R^+$ . Is there a way to make use of such symmetric structure to simplify the calculation of the determinant?","['matrices', 'determinant', 'linear-algebra']"
3647781,Is there any way to compute $f(f(...f(x))$ where $f(x)=x^2+x+1$?,"I am simply curious if starting with $f(x)=x^2+x+1$ you can compute $f(...f(f(x)))$ where $f$ appears $n$ times. I think this can be done by induction, but I tried computing $f(f(x))$ , $f(f(f(x))$ and they don't look alike so I could't establish the induction hypothesis. Please help! Thank you! This kind of exercise is pretty easy when $g(x)=ax+b$ since you can set the induction easily. But I am in the position of needing to find the roots of the polynomial $f(f(...f((x)))+constant$ and it drives me crazy since I have no ideas.",['functions']
3647818,How to find quartiles using histogram?,"Can you suggest how can I find first and third quartiles and median after building histogram from raw data? I can sort the data and find the values, but still how it can be done using the chart...",['statistics']
3647840,"How to construct six points $ABCDEF$ on a plane so that the distance between any two of them is an integer, and no three are collinear?","How to construct six points $ABCDEF$ on a plane so that the distance between any two of them is an integer, and no three are collinear? I tried with some right angled triangles  with pythagorean triples and you get 3 points. and i am stuck with 3 points","['number-theory', 'geometry-of-numbers', 'pythagorean-triples', 'geometry', 'plane-geometry']"
3647893,Do smooth and $L^1$ functions vanish at infinity?,"Sometimes when proving some estimates on a smooth solution of a PDE over the whole space domain, one integrates over space terms like $\partial_x u$ , and those terms vanish for some reason. Since we are working on the whole space and no boundary conditions are prescribed, my guess was that the reason was that $u \in L^1 \cap C^1$ , hence $\lim_{|x| \to +\infty} u(x) = 0$ and then $\int_{-\infty}^{+\infty} \partial_x u \, dx = \left[u(x)\right]_{-\infty}^{+\infty} = 0$ basically. However, I failed to prove such a statement and don't even know if it is true. I know that there are $L^1$ functions that do not vanish at infinity, and that aren't even bounded, but I couldn't think of any function that would be smooth as well, let's say $C^1$ . I thought that if $u$ is $C^1$ then you can't have peaks as stiff as you want, so you can't imagine peaks which mass would tend to zero ... but I failed to make this rigorous. Does anybody have a clue on that?","['integration', 'lebesgue-integral', 'derivatives', 'partial-differential-equations']"
3647967,Limit of resolvent in terms of limit of semigroup,"Let $(T_t)_{t\geq 0}$ be a $C_0$ -semigroup on a Banach space $X$ with generator $A$ such that the spectral bound $s(A)=0.$ Suppose there exists an operator $P$ on $X$ such that $$T(t) \stackrel{t\to \infty}{\to} P \text{ strongly }.$$ Then I was able to show that $$\lim_{\lambda\to 0}\lambda R(\lambda,A)f \text{ exists for each } f\in X \qquad\qquad (1).$$ I was told the converse is true if the semigroup is holomorphic. However, I'm wondering what can be said about the limit in $(1).$ I know that if $0$ is an isolated spectral value and a pole, then the limit in $(1)$ is the spectral projection associated to $0.$ What happens when it is not an isolated spectral value? Is the limit in $(1)$ equal to $P?$ Edit: Let $\lambda>0.$ Since $T(t) \to P,$ therefore $\mathrm{Im}\, P=\ker A$ and $\overline{\mathrm{Im}\, A}\subseteq \ker P.$ The first implies that $$\lambda R(\lambda,A)P=P$$ and the second implies that $$\lim_{\lambda \to 0}\lambda R(\lambda,A)=0 \text{ on a closed subspace of } \ker P.$$ Can we now conclude by the existence of limit that, infact $$\lim_{\lambda \to 0}\lambda R(\lambda,A)=0 \text{ on } \ker P$$ and hence $$\lim_{\lambda \to 0}\lambda R(\lambda,A)=P?$$","['semigroup-of-operators', 'functional-analysis']"
3647975,"When does a matrix have an ""invariant quadratic form""?","Yesterday I computed that the matrix $$
A = \begin{pmatrix} 2&1\\1&1\end{pmatrix}$$ satisfies $q(m,n) = q \left((m,n)A\right)$ for the quadratic form $$q(m,n) = m^2 - mn - n^2.$$ E.g., $-1 = q(1,1) = q(3,2) = q(8,5) =\ \ldots\ $ which is quite satisfying. On the other hand, the matrix $$B = \begin{pmatrix} 1&1\\1&0 \end{pmatrix}$$ fixes no such quadratic form, although it does preserve $(m,n)\mapsto (q(m,n))^2$ (since $B^2 = A$ this is maybe unsurprising). My question. Is it known when a square matrix with integer entries preserves a non-trivial quadratic form? Moreover, when does such a quadratic form have integer coefficients? It seems easy to verify for individual examples, but is there a general theory?","['matrices', 'integer-lattices', 'quadratic-forms']"
3648024,"If $AB\parallel DC$, $BC\parallel AD$, and $AC\parallel DQ$, find $\Bbb X$ in terms of the areas $\Bbb A$, $\Bbb B$, and $\Bbb C$.","If $AB\parallel DC$ , $BC\parallel AD$ , and $AC\parallel DQ$ , find $\Bbb X$ in terms of the areas $\Bbb A$ , $\Bbb B$ , and $\Bbb C$ . Please, I wrote a lot of relations, but I just need to prove that $\overline{AC}\cap\overline{BQ}=P\implies BP=PQ$ .  If I prove this, I will get $\Bbb X=\Bbb A+2\Bbb B+\Bbb C.$ For context, if $BP=PQ$ , then $\triangle BPC$ and $\triangle CPQ$ have equal area.  Let $\overline{BP}\cap\overline{CD}=R$ .  Then, we can show that $\triangle APR$ and $\triangle BRC$ have equal area.  If $S=\overline{AQ}\cap\overline{CD}$ , then we can see that $\mathbb{X}-\mathbb{A}-\mathbb{B}$ is the area of $\triangle ARS$ .  It is not difficult to show that the area of $\triangle ARS$ is $\mathbb{B}+\mathbb{C}$ .","['euclidean-geometry', 'area', 'geometry', 'polygons', 'plane-geometry']"
3648046,Let $f_{k+1}(x)=f_{k}(\cos x)$ and $f_{1}(x)=\cos x$ then $\lim_{k\to\infty}f_{k}(x)=0.73905\cdots$ [duplicate],"This question already has answers here : Prove that $f(x) = \cos(x)$ has a unique fixed point. (2 answers) Computing the fixed point for $\cos x$ (1 answer) Closed 4 years ago . Let $f_{k+1}(x)=f_{k}(\cos x)$ and $f_{1}(x)=\cos x$ then $\lim_{k\to\infty}f_{k}(x)=0.73905\cdots$ I was just piddling around with the calculator one day. I don't know what happened but I just happened to take the cosine of a single number (in radians) repeatedly. It converged to a single value $0.739085133\dots$ It converged to this same thing for every number I tried. Like for example, the cosine of the cosine of the cosine of the cosine $\dots$ of any arbitrary value is equal to that. Please tell me if I have made a new observation, or if it's just a false alarm.","['trigonometry', 'recreational-mathematics', 'sequences-and-series']"
3648075,Computational check whether $G\cong \tilde G$,"Unless I was mistaken somewhere in the proof, the following claim should hold: Claim . Let $G, \tilde G$ be groups. Then, $G\cong\tilde G$ if and only if there are a set $S$ and two bijections, $f\colon S\to G$ and $\tilde f\colon S\to \tilde G$ , such that: $$\tilde f^{-1}(\tilde f(s)\tilde f(t))=f^{-1}(f(s)f(t)), \space\space\forall s,t \in S\tag 1$$ (Operation symbol is omitted in both groups.) As a first test of $(1)$ , I'd like to show by brute force that, if $G=\mathbb{Z}_4$ , $\tilde G=\mathbb{Z}_2\times \mathbb{Z}_2$ and $S=\{1,2,3,4\}$ , then: $$\forall \alpha,\beta =1,\dots,24, \exists i,j \in S \mid f_\alpha^{-1}(f_\alpha(i)+f_\alpha(j)) \ne \tilde f_\beta^{-1}(\tilde f_\beta(i)+\tilde f_\beta(j)) \tag 2$$ where $\{f_\alpha, \alpha=1,\dots,24\}=\operatorname{Sym}(S,G)$ and $\{\tilde f_\alpha, \alpha=1,\dots,24\}=\operatorname{Sym}(S,\tilde G)$ , $\operatorname{Sym}(X,Y)$ being the set of bijections from $X$ to $Y$ . Is there any available fast resource to computationally check $(2)$ ?","['computational-mathematics', 'group-theory']"
3648186,Derivative of $g(t) = Df(x + t(y-x))(y - x)$?,$f \in C^2(\mathbb{R}^2)$ and $x$ represents the column vector while $x^T$ represents horizontal vector. I was told that $g'(t) = (y - x)^T D^2f(x + t(y - x))(y - x)$ . Why is that?,"['multivariable-calculus', 'derivatives', 'analysis']"
3648197,"If $\dim \ker T=4, \dim \ker T^3=9, \dim \ker T^4=11$. Then, Find $\dim \ker T^2$","Let $T: \mathbb C^{11} \rightarrow  \mathbb C^{11} $ be a linear transformation, such that $\dim \ker T=4, \dim \ker T^3=9, \dim \ker T^4=11$ . Then, Find $\dim \ker T^2.$ Attempt: We have : $\ker T \subset \ker T^2 \subset \ker T^3 \subset \ker T^4$ where $T$ is clearly nilpotent with index $4$ $\implies \dim \ker T <  \dim \ker T^2 <  \dim \ker T^3 <  \dim \ker T^4$ Thus, $\dim \ker T^2 $ can assume values from $\{5,6,7,8 \}$ . How do I move forward from here?","['linear-algebra', 'linear-transformations']"
3648226,How to prove $\lim_{n\to\infty}\frac{1}{\Gamma(n)}\int_{n}^{\infty}t^{n-1}e^{-t}dt = 1/2$?,"How to prove $$
\lim_{n\to\infty}\frac{1}{\Gamma(n)}\int_{n}^{\infty}t^{n-1}e^{-t}dt = 1/2?
$$ This is confirmed numerically. It is also stated in Limit involving incomplete gamma function that $\lim_{n\to\infty}\gamma(n,n)/\Gamma(n,n) = 1$ as a fact. So I believe the above statement is true. But I am still searching for a proof.","['integration', 'improper-integrals', 'gamma-function', 'calculus', 'limits']"
3648234,Behavior of the Fourier Transform at infinity,"I have a problem in the proof of the following result: Given $f \in L^1(\mathbb{R}^n)$ , we have that $$|\hat{f}(\xi)| \rightarrow 0, \;\;\; as |\xi| \rightarrow \infty.$$ This result is known as the Riemann-Lebesgue Lemma (Proposition 2.2.17 from Grafakos's book Classical Fourier Analysis, third edition). In the proof of this proposition, one considers the function $$g := \prod_{j=1}^n \chi_{[a_j,b_j]},$$ that I suppose is the characteristic function of the cube $\prod_{j=1}^n[a_j,b_j] \subset \mathbb{R}^n$ , and whose Fourier transform is $$\hat{g}(\xi) = \prod_{j=1}^n \frac{e^{-2\pi i \xi_ja_j} - e^{-2\pi i \xi_jb_j} }{2\pi i \xi_j},$$ in the meaning that if some $\xi_j = 0$ , the correspondent  factor is equal $b_j-a_j$ .  Now, if $\xi =(\xi_1, ..., \xi_n) \neq 0$ , choose $j_0$ such that $|\xi_{j_0}| \geq |\xi|/\sqrt{n}$ . So that $$\left| \prod_{j=1}^n  \frac{e^{-2\pi i \xi_ja_j} - e^{-2\pi i \xi_jb_j} }{2\pi i \xi_j} \right| \leq 
\frac{2\sqrt{n}}{2\pi|\xi|}\sup_{1\leq j_0\leq n}\prod_{j\neq j_0}(b_j-a_j).$$ This inequality is what I'm not being able to prove. Once it's proved, I have the desired result.","['fourier-analysis', 'fourier-transform', 'analysis', 'real-analysis']"
3648239,"How does one prove $\ln \zeta (s)=\sum _{{n=2}}^{\infty}{\frac{\Lambda (n)}{\log(n)}}\,{\frac{1}{n^{s}}}$","Introduction The Von Mangoldt function is defined as follows: $$\Lambda (n)={\begin{cases}\log p&{\text{if }}n=p^{k}{\text{ for some prime }}p{\text{ and integer }}k\geq 1,\\0&{\text{otherwise.}}\end{cases}}$$ On the Wikipedia page over the Von Mangoldt function the identity below is listed: $$\ln\zeta(s)=\sum _{{n=2}}^{\infty}{\frac{\Lambda (n)}{\ln(n)}}{\frac{1}{n^{s}}}\qquad {\text{Re}}(s)>1$$ Question How do you prove this? I'm not familiar with the Von Mangoldt function so pardon me if the identity has a completely trivial derivation, but I couldn't find it anywhere so I had to ask here.","['number-theory', 'sequences-and-series']"
3648272,number of parameters of $SO(3)$ group,"The $SO(3)$ rotation group is defined by: $A\cdot A^T=\mathbb{1}$ and $\det{A}=1$ . The group is supposed to have 3 free parameters, as suggested by Euler's angles. However, I am doing something wrong with counting of the parameters because I cannot arrive at that number. $3\times3$ matrix has $9$ free parameters. The requirement $A\cdot A^T=\mathbb{1}$ produces 6 equations for those parameters: $$
\vec{a_1}\cdot\vec{a_1}=1\\
\vec{a_1}\cdot\vec{a_2}=0\\
\vec{a_1}\cdot\vec{a_3}=0\\
\vec{a_2}\cdot\vec{a_2}=1\\
\vec{a_2}\cdot\vec{a_3}=0\\
\vec{a_3}\cdot\vec{a_3}=1\\
$$ ( $\vec{a_i}$ being the columns of $A$ ) and reduces the number of free parameters to 3. But with this requirement $\det{A}=\pm 1$ and so $\det{A}=1$ presents another restricting euqation, and so i get 2 free parameters. If I do a similar counting for the $SU(2)$ group, i have 8 free parameters of $2\times2$ complex matrix, the requirement $A\cdot A^\dagger=\mathbb{1}$ provides 4 independent equations and the determinant provides 2 more conditions (complex and imaginary parts) which again reduces to only 2 free parameters. Where am i doing the mistake in counting the parameters and equations?","['finite-groups', 'matrices', 'linear-algebra', 'group-theory', 'lie-groups']"
3648276,"Game on the unit segment, $n$ players (points).","The $n$ player 1-dimensional unit game The $n\ge 2$ players take turns placing their own point on the unit segment $[0,1]$ . (Hence, 1-dimensional .) The closest distance they can place their point to someone elses placed point is some small $\varepsilon\gt 0$ . A player owns the part of the segment closest to their own point. The goal is to own the largest part of the segment. That is, say $a,b$ are closest points to your own point $x$ . Then: If your point ends up between two points $x\in(a,b)$ then your score is $s_x=\frac12(b-a)$ . If your point ends up in $x\in[0,a)$ , then your score is $s_x=\frac12 (x+a)$ . Symmetrically, if your point $x\in(b,1]$ , then your score is $s_x=1-\frac12 (x+b)$ . Which player(s) have the winning strategy? (Have highest chance of highest score, then maximize that score.) WLOG (Symmetry) first player plays in $x_1\in[0,\frac12]$ . I've observed so far that the optimal play (and winniners) should be: $(n=2)-$ Is a win for the $1$ st player. Player plays $x_1=\frac12$ . $(n=3)-$ Is a win for either $1$ st or $2$ nd player (coin flip). Players play $x_1=\frac14+\frac12\varepsilon$ and $x_2=1-x_1$ , then $x_3\in(x_1,x_2)$ uniformly randomly (we assume if a player is presented with equally good choices, he picks uniformly randomly), which results in $0.5$ chance (a coin flip) for the one of the $x_1,x_2$ to win (have highest score). More details are below. Do correct me if I missed anything about $(n=3)$ there. Can we find strategies and winners for $n\ge 4$ ? $2$ -player case is a win for $1$ st player For $n=2$ , it is not hard to see that the first player has a winning strategy by playing $x_1=\frac12$ . The most the second player can score then is by playing $x_2=\frac12-\varepsilon$ . This nets them $s_2=\frac12 (\frac12-\varepsilon+\frac12)=\frac12-\frac12\varepsilon$ score, while the first player now has $s_1=\frac12+\frac12\varepsilon$ . $3$ -player case is a win for either $1$ st or $2$ nd player with $0.5$ probability For $n=3$ , this strategy does not work for the first player anymore. The third player would play $x_3=\frac12+\varepsilon$ . (Mirror the second player.) The second and third player now share the win by both having $\frac12-\frac12\varepsilon$ score. The first player is now in $x_1\in(x_2,x_3)$ and their score is reduced to a minimal $\varepsilon$ . So WLOG if in $n=3$ the winning strategy exists for the first player, they will play in $x_1\in[0,\frac12)$ . Now the question is, what would be the optimal response of the second player to this? The first player plays $x_1=\frac12-\delta$ . Assuming $\delta$ is small enough, the second player can play $x_2=\frac12+\delta+\varepsilon$ to win the game. Notice that this play makes $[0,x_1)$ more favorable for the third player, than the $(x_2,1]$ , meaning the third player would leech off the first player. That is, by small $\delta$ we are assuming that the score $s_3$ of $x_3$ if they would play in $x_3\in(x_1,x_2)$ is smaller than if they would play optimally in $[0,x_1)$ . In other words we are assuming $\delta \lt \frac14-\frac12\varepsilon$ here: $$s_3((x_1,x_2))\lt s_3([0,x_1)) \iff \delta+\frac12\varepsilon\lt \frac12-\delta-\frac12\varepsilon \iff \delta \lt \frac14 -\frac12 \varepsilon$$ This forces $x_3$ to play in $[0,x_1)$ , namely $x_3=\frac12-\delta-\varepsilon\implies s_3=\frac12-\delta-\frac12\varepsilon$ . The score of the second player will be $s_2=\frac12-\frac12\varepsilon$ , and the first player will have $s_1=\delta+\varepsilon$ . We have $s_1\lt s_3 \lt s_2$ . This now gives a winning strategy for second player if $x_1\in(\frac14+\frac12\varepsilon,\frac12]$ . So WLOG if in $n=3$ the winning strategy exists for the first player, they will play in $x_1\in[0,\frac14+\frac12\varepsilon]$ . If $x_1=\frac14+\frac12\varepsilon$ and $x_2=(1-x_1)+\varepsilon$ still, then $x_3$ would play uniformly randomly in $x_1\in(x_1,x_2)$ for $s_3=\frac14+\varepsilon$ . This will result in $x_1$ winning with slightly larger probability than $0.5$ (depending on the size of $\varepsilon$ ), and other times $x_2$ will win. Can $x_2$ now change its strategy to improve its odds? If it increases the $\varepsilon$ increment, is has worse odds. (Depending on the size of the $\varepsilon$ .) If it removes it, both players $x_1,x_2$ have exactly $0.5$ chance of winning. ( $x_2$ mirrors $x_1$ ) If it decreases the $\varepsilon$ increment, is now always loses because $x_3\in(x_2,1]$ is forced. That is, for $x_1=\frac14+\frac12\varepsilon$ it seems the optimal strategy for second player is $x_2=1-x_1$ , to mirror it. This forces the $x_3$ to play uniformly randomly in $(x_1,x_2)$ , giving both $x_1,x_2$ equal chances of winning (probability $0.5$ ). The question now remains, can first player do better than $0.5$ if he picks $x\in[0,\frac14+\frac12\varepsilon)$ ? It seems that if $x_1$ plays in this interval (is closer to $0$ ), this allows the $x_2$ to decrease the mentioned increment (come closer to the $\frac12$ ) without making $(x_2,1]$ more valuable than the $(x_1,x_2)$ for the $x_3$ to play in. Hence $x_2$ has higher probability than $0.5$ of winning now. From all of these cases for $x_1$ , it follows that the optimal play would be $x_1=\frac14+\frac12\varepsilon$ , then $x_2=1-x_1$ and finally $x_3\in(x_1,x_2)$ does a coin flip to decide if it wants to steal more from $x_1$ or $x_2$ , giving the other one the win. $n\in\mathbb N$ players case Is it possible to solve this for general $n$ ? I'm wondering if there is a way to find conclusions about the game in general, other than tackling individual $n$ cases.","['game-theory', 'recreational-mathematics', 'geometry']"
3648334,What does it mean that a distribution is integrable?,"I am studying geometric control theory, and I am focusing on the Frobenius theorem . I have seen that it gives sufficient and necessary conditions for integrability of a distribution, but I am having troubles understanding well the concept. The Frobenius theorem states that a distribution is integrable if and only if it is involutive. I have clear the concept of involutivity, but what I have not clear is the concept of integrability. From the notes of my professor, I have that a distribution $\Delta (x)$ of rank $k$ is integrable if there exist the functions $\lambda_1(x) .... \lambda_{k-n}(x) $ such that: $\frac{\partial \lambda }{\partial x}\Delta (x)=0$ but what does it mean? I cannot understand clearly the concept of integrability of a distribution. Moreover, I don't understand the meaning of the fact that the product of the derivative of $\lambda$ with respect to $x$ with  the distribution gives zero. I know that this implies orthogonality, but what does it mean that these two are orthogonal? Can somebody please help me?","['dynamical-systems', 'control-theory', 'differential-geometry']"
3648343,Why does fundamental theorem of calculus not work for this integral $\int_0^{2\pi}\frac{dx}{(3+\cos x)(2+\cos x)}$?,"$$\int\frac{dx}{(3+\cos x)(2+\cos x)}=  \frac{2\arctan(\frac{\tan(\frac x2)}{\sqrt3})}{\sqrt3} - \frac{\arctan(\frac{\tan(\frac x2)}{\sqrt2})}{\sqrt2} + C $$ This is the antiderivative . By the FTC : $$\int_a^b f(x) = F(b) - F(a)$$ where $F(x)$ is a primitve function. $$\left. \int_0^{2\pi}\frac{dx}{(3+\cos x)(2+\cos x)}= \frac{2\arctan(\frac{\tan(\frac x2)}{\sqrt3})}{\sqrt3} - \frac{\arctan(\frac{\tan(\frac x2)}{\sqrt2})}{\sqrt2} \right|_0^{2\pi}=0$$ $\frac{dx}{(3+\cos x)(2+\cos x)}$ is positive on $[0,2\pi]$ hence the result above is wrong. Correct result is: $$\int_0^{2\pi}\frac{dx}{(3+\cos x)(2+\cos x)}=\Bigl(\frac2{\sqrt3}-\frac1{\sqrt2}\Bigr) \pi$$ Why am I not getting the correct result ?","['integration', 'calculus']"
3648415,how to find directional derivative,"I am trying to find the directional derivative of the following problem $F(x,y,z) = 4x^2+ 3y−3xz+ 2z^2$ at the point $(2,1,2)$ in the direction $i−k$ ; I worked out the derivatives of $F(x,y,z)$ as $f_x = 8x -3z$ $f_y = 3$ $f_z = -3x+4z$ But I don't know to do next; can someone explain how to find the directional derivative here please? Thank you","['multivariable-calculus', 'algebra-precalculus']"
3648427,Limits of integration on double integrals,"I was given this problem: Find an integral equal to the volume of the solid bounded by $z=4-2y,z=0,x=y^4,x=1$ and evaluate. I understand how to evaluate once my double integral is set up, but I do not know how to find my limits of integration. I am assuming that my function will be $z=4-2y$ and that using this I should be able to find my limits of integration. I can say that $0=4-2y$ which means that $y=2$ . I can then plug that into $x=y^4$ and get $1\leq x\leq 16$ which may be correct, but I still am missing the limits of integration for y. Am I thinking about this problem correctly? How can I go about solving this?","['integration', 'multivariable-calculus', 'multiple-integral', 'definite-integrals']"
3648433,Binomial in Statistics,"I was asked a question;
A student was late for college 0.25 of the time, what is the probability he is late 4 days in one college week. My answer was this: L = Late, N = Not Late L = 0.25, N = 0.75 if its a 7 day week P(L+N)7 = L7 + (7C1)L6 N + (7C2)L5N2+(7C3)L4N3+(7C4)L3N4+… P(4L, 3N) = (7C3)(0.25)4(0.75)3 = 0.057 If it’s a 5 day week P(L+N)5 = L5+ (5C1)L4 N + (5C2)L3N2+(5C3)L2N3+(5C4)LN4+… P(4L1N) = (5C1)(0.25)4(0.75) = 0.0146 Have I got this ok?",['statistics']
3648443,Between two Real Numbers exists Rational Number,"I can't understand the following proof I found in Rudin, ""Principles of mathematical analysis"". This is the statement: if $x\in\mathbb{R}, y\in\mathbb{R}$ and $x<y$ , then there exists $p\in\mathbb{Q}$ such that $x<p<y$ . Here is the proof I can't understand. proof. 1) Since $x<y$ , we have $y-x>0$ (ok, I got it) 2) Archimedean principle furnishes a positive integer $n$ such that $n(y-x)>1$ (ok) 3) Apply archimedean principle again to obtain positive integers $m_1,m_2$ such that $m_1>nx$ and $m_2>-nx$ (ok) 4) Then $-m_2<nx<m_1$ (ok) 5) Hence there is an integer $m$ (with $-m_2\leq m\leq m_1$ ) such that $m-1\leq nx< m$ Here is where I have some problems. Why do I need integers $-m_2$ and $m_1$ ? Couldn't just say: every real number lies between an integer and its successor? Moreover, even knowing the existence of integers $m_1, m_2$ , how do I deduce the fact that $m$ exists with that property? 6) If we combine these inequalities, we obtain $nx<m\leq 1*nx<ny$ (ok) 7) Since $n>0$ , it follows that $x<\frac{m}{n}<y$ (ok) 8) Take $p=\frac{m}{n}$ . (ok)","['proof-explanation', 'analysis']"
3648486,Necessary and sufficient conditions for a right triangle,"Show that if for $\triangle ABC$ the equalities $h_c^2=a_1b_1$ and $b^2=b_1c$ are true where $h_c$ is the height, $AC=b, AB=c$ and $a_1$ and $b_1$ are the the projections of $BC$ and $AC$ on $AB$ , then the triangle is right angled. I want to demonstrate that together the conditions are sufficient without using the Law of Cosines or the Pythagorean theorem. The relations in a right triangle that we have studied and can use in the problem: $h_c^2=a_1b_1, a^2=ca_1$ and $b^2=cb_1$ where $BC=a, AC=b, AB=c$ and $a_1$ and $b_1$ are the projections of $AC$ and $BC$ on $AB$ . Counterexample: $\triangle ABC$ $(AC<BC)$ is right triangle and then $h^2=a_1b_1$ . Let $M$ lie on $BH$ and $HM=AH=b_1$ . For $\triangle MBC$ $h^2=a_1b_1$ but it is NOT A RIGHT TRIANGLE.",['geometry']
3648511,How to prove there is infinite prime numbers of form $5n+3$ without Dirichlet theorem?,Is there a nice elementary way to prove there is infinite prime numbers of form $5n+3$ (also for $5n+2$ ) with $n\in \mathbb{N}$ ? I know how to do it for primes of form $pn+1$ for any prime $p\geq 3$ but not in this case.,"['number-theory', 'arithmetic-progressions', 'prime-numbers']"
3648522,Does every finite topological space map to a family of pairwise disjoint subsets of the reals under the usual topology with closure preserved?,"For a simple example, suppose $X=\{1,2,3\}$ under the partition topology $\mathcal{T}=\{\varnothing,\{1\},\{2,3\},X\}.$ The map $\mu$ taking $1$ to $\{1\}$ , $2$ to $[2,3]\cap\mathbb{Q}$ , and $3$ to $[2,3]\setminus\mathbb{Q}$ clearly satisfies $cl(\{x\})\mapsto cl(\mu(x))$ for each $x\in X,$ and since topological closure distributes over finite unions, $cl(A)\mapsto cl(\mu(A))$ for each $A\subseteq X.$ For further examples involving only connected finite spaces, see the alternative version of essentially this same question that I asked here a few years ago. The present version is much simpler and lifts the connectedness restriction on $X.$ Are there any theorems in general topology that ensure the existence of the map $\mu$ for every finite topological space $X?$ The assertion looks neither provable nor disprovable to me.",['general-topology']
3648581,Intuition behind orientation of a surface,"Let $S\subseteq \mathbb{R}^3$ be a smooth surface. Suppose $\phi: U\rightarrow S$ is a (local) parametrization for $S$ , where $U \subseteq \mathbb{R}^2$ is an open set. Then $\phi(U)$ has a standard orientation, that is, given $p\in\phi(U)$ ( $p=\phi(x_0)$ , $x_0 \in U$ ) the orientation of $T_pS$ is defined to be the orientation given by the basis $\{d\phi(x_0)(e_1),d\phi(x_0)(e_2)\}$ ( $\{e_1,e_2\}$ being the standard basis of $\mathbb{R}^2$ ). I would like to understand how this definition implies that if $p, q \in S$ are close, than the orientations for $T_pS$ and $T_qS$ are ""similar"", without referring to normal vectors. I guess the problem is how to formalise this ""similarity"". We're trying to extend the concept of orientation from vector spaces to a piece of $S$ . However is $p\neq q \in S$ then $T_pS$ and $T_q S$ are different vector spaces, so it doesn't make much sense to compare the orientations of two respective basis. One idea I had is to ""slide"" $T_qS$ to $T_pS$ along the surface $S$ to be able to compare two respective basis. This argument will only work when $\phi(U)$ is connected, i.e. only when $U$ is connected (as $\phi$ is a homeomorphism onto $\phi(u)$ ), but this is not a big issue because if $\phi(U)$ is not connected than it doesn't make much sense (to me) to talk about a ""global"" orientation of disconnected pieces. So I can suppose $U$ and $\phi(U)$ are connected. Since we're working in euclidean spaces then it means that $U$ and $\phi(U)$ are path connected, so there exists a path $\gamma$ in $U$ that lifts to a path from $p$ to $q$ in $S$ . However this path need not be smooth, so it might not behave well for this purposes. So I cannot come up with a canonical way of ""sliding"", hence I'm stuck here, provided this is a good idea. Do you know how to go on from here or an alternative explanation?","['manifolds', 'surfaces', 'smooth-manifolds', 'differential-geometry']"
3648584,Relation between Pascal's Triangle and Euler's Number,"My friends and myself were discussing Pascal's Triangle, specifically the following property of it. First, consider the Pascal's Triangle - $$1\\ 1\ 1\\ 1\ 2\ 1\\ 1\ 3\ 3\ 1\\ 1\ 4\ 6\ 4\ 1\\ 1\ 5\ 10\ 10\ 5\ 1\\ 1\ 6\ 15\ 20\ 15\ 6\ 1\\ ..................\\ .....................$$ Now, one interesting observation that can be easily spotted is that initial rows of the Pascal's Triangle follow the form $11^n$ and then a few of the following rows follow $101^n$ and so on. Now, if we deform the above argument as the following: The zeroth row is of form $(1.1)^0$ The first row is of the form $(1.1)^1$ Similarly the second row as $(1.1)^2$ the following one as $(1.1)^3$ and then the fourth row as $(1.1)^4$ Now for the immediate next row if we follow the pattern shown above we get overflow due to digits being carried forward and hence we represent the fifth row as $(1.01)^5$ yielding $1.0510100501$ . Now again we keep on proceeding with the introduction of more zeroes after the decimal point for preventing overflow and preserving the form of the Pascal's Triangle. Consider the following idea, the $n^\text{th}$ row can be represented as following $(1.\overbrace{000.....0}^{n}1)^n$ . Now I grossly miscalculated this, previously. As $n \to \infty$ this value becomes equal to $1$ . Due to the fatal error above, as pointed out in the answers, I rephrase the initial question as: Is there any way to make this series converge to e with the adding of zeroes suitably as needed? Also, later on studying the Pascal's Triangle further I found another interesting relation. Consider, $f(n)$ to be the product of all digits in the $n^\text{th}$ row of the Pascal's Triangle. Also, consider the beginning to be the zeroth row. Then, with algebraic manipulation we obtain: $$\frac{f(n)}{f(n-1)} = \frac{n^{(n-1)}}{(n-1)!}$$ from which we can further deduce that $\frac{f(n+1) \times f(n-1)}{f(n)^2}$ converges to $e$ as $n \to \infty$ i.e., $$\lim_{n \to \infty} \frac{f(n+1) \times f(n-1)}{f(n)^2} = e$$ Since, I have updated the problem statement, I am not very sure if the following questions hold . can we connect the initial observation with above observation? is there some correlation between the number $e$ and the product of its digits? (Well this seems quite silly now :)) Update: Thanks for pointing out the mistake.","['number-theory', 'binomial-coefficients', 'combinatorics', 'eulers-number-e']"
3648588,"Implementing QR Algorithm from Golub & Van Loan's ""Matrix Computations"" - notation and operation assistance","I am implementing the Francis QR Algorithm from Golub & Van Loan's ""Matrix Computations"" (see algorithms below). I am unsure about the meaning of the following step: ""Find the largest nonnegative $q$ and the smallest non-negative $p$ such that..."" EDIT 4/30/2020: I have made some progress, the notation being used is ""Block Matrix"" notation, however I have noticed more difficulties.
To simplify the question I am removing components that are no longer relevant. Francis Step requires a Matrix of minimum size $3\times 3$ . Does this mean the algorithm is only viable for matrices of minimum size $5\times 5$ ( $p=q=1$ )? Are there workarounds for $3\times3$ & $4\times4$ matrices? When partitioning the block matrix, I have not been able to find dimensions that satisfy the given conditions. For example, using the following $8\times8$ matrix $A$ (larger size so it's a bit easier to demonstrate), 1  2  3  4  5  6  7  8
9 10 11 12 13 14 15 16
0 18 19 20 21 22 23 24
0  0 27 28 29 30 31 32
0  0  0 36 37 38 39 40
0  0  0  0 45 46 47 48
0  0  0  0  0 54 55 56
0  0  0  0  0  0 63 64 There are a number of ways to partition it (while maintaining an $H_{22}$ of at least size $3\times3$ ), for example: $p = 1 ;\; q = 4$ 1   2 3 4      5 6 7 8     

 9   10 11 12   13 14 15 16 
 0   18 19 20   21 22 23 24 
 0    0 27 28   29 30 31 32 

 0   0 0 36     37 38 39 40 
 0   0 0  0     45 46 47 48 
 0   0 0  0      0 54 55 56 
 0   0 0  0      0  0 63 64 $p = 2 ;\; q = 3$ 1  2    3  4  5    6  7  8 
 9 10   11 12 13   14 15 16 

 0 18   19 20 21   22 23 24 
 0  0   27 28 29   30 31 32 
 0  0    0 36 37   38 39 40 

 0 0    0 0 45     46 47 48 
 0 0    0 0  0     54 55 56 
 0 0    0 0  0      0 63 64 $p = 3 ;\; q = 2$ 1  2  3    4  5  6    7  8 
 9 10 11   12 13 14   15 16 
 0 18 19   20 21 22   23 24 

 0 0 27    28 29 30   31 32 
 0 0  0    36 37 38   39 40 
 0 0  0     0 45 46   47 48 

 0 0 0     0 0 54     55 56 
 0 0 0     0 0  0     63 64 $p = 4 ;\; q = 1$ 1  2  3  4    5  6  7    8 
 9 10 11 12   13 14 15   16 
 0 18 19 20   21 22 23   24 
 0  0 27 28   29 30 31   32 

 0 0 0 36     37 38 39   40 
 0 0 0  0     45 46 47   48 
 0 0 0  0      0 54 55   56 

 0 0 0 0      0 0 63     64 $p = 1 ;\; q = 3$ 1   2 3 4 5       6 7 8    

 9   10 11 12 13   14 15 16 
 0   18 19 20 21   22 23 24 
 0    0 27 28 29   30 31 32 
 0    0  0 36 37   38 39 40 

 0   0 0 0 45      46 47 48 
 0   0 0 0  0      54 55 56 
 0   0 0 0  0       0 63 64 and on and on. Essentially, I have not been able to find combinations that result in a zero matrix in the $H_{12}$ spot. Every time it shifts it picks up a new subdiagonal non-zero scalar. What am I doing wrong?","['eigenvalues-eigenvectors', 'computational-mathematics', 'linear-algebra', 'algorithms', 'matrix-decomposition']"
3648653,Comparing definitions of mutually exclusive: disjoint events or zero probability of intersection?,"It is not clear to me what is/should be the standard definition of ""mutually exclusive"" in probability, as there seem to be two definitions in the literature. In the top response in this thread , mephistolotl wrote Two events are mutually exclusive if the probability of them both occurring is zero, that is if $\text{Pr}(A\cap B)=0$ . The user also said that this is the definition in some but not all texts. On the other hand, the Wikipedia article on mutual exclusivity says that Formally said, the intersection of each two of them is empty (the null event): $A\cap B= \varnothing$ . Does one definition dominate the other in mathematics, and if so, which one? If not, what are the merits and disadvantages of each? Of course, we could just give a different name to each of the two, but given the prevalence of the term ""mutually exclusive,"" I am interesting in knowing the best meaning to assign to it. If it helps, I am mainly interested in discrete probability at the moment, but it would be nice if the definition extended to general probability. In discrete probability, if we know that none of the elements have zero probability, then the two definitions are equivalent.","['discrete-mathematics', 'combinatorics', 'probability']"
3648683,Example of $L_1$ and $L_p$ function whose convolution is not in $L_1$,"Given functions $f \in L_p(\mathbb{R})$ and $g \in L_1(\mathbb{R})$ , one can show that the convolution $f*g$ is well defined and Young's convolution inequality tells us that $\|f*g\|_{p} \leq \|f\|_{p}\|g\|_{1}$ , thus showing that $f*g \in L^{p}$ . What I want to know is whether it's true that $f*g \in L_1(\mathbb{R})$ for all such $f,g$ . On the outset, it seems unlikely that it's true, but I'm having a difficult time finding a counter example. I have tried using some properties of Fourier tranforms and their relations with convolutions to show the existence of $f$ and $g$ with no success. So my question is, is it true that $f*g \in L_{1}$ , if it is not, then how does one go about coming with a counterexample? More generally is $f*g \in L_{r}(\mathbb{R})$ for any $r \neq p$ ?","['measure-theory', 'lp-spaces', 'fourier-analysis', 'convolution']"
3648726,Finding the integral $\int\frac{e^{x}}{e^{2x}+1}dx$,"This question has been puzzling me for a bit and I'd like an explanation for what I'm doing wrong as my answer doesn't coincide with the correct one. Let's say we're asked to find: $$\int \frac{e^{x}}{e^{2x}+1}\mathrm{d}x$$ The way I chose to solve this was factor out an $e^{2x}$ from the denominator and work my way from there. So what I get is: $$\int \frac{e^{x}}{e^{2x}(1+1/e^{2x})}dx$$ This could be rewritten as: $$\int \frac{1}{e^{x}(1+(1/e^{x})^2)}dx$$ If we let: $$u = \frac{1}{e^x}$$ $$du = -e^{-x} dx$$ $$dx = -e^xdu$$ So, now we're at: $$\int \frac{1}{e^{x}(1+u^2)}(-e^x)du$$ Cancelling out the $e^x$ and removing the minus sign outside of the integral gives us: $$-\int \frac{1}{1+u^2}du$$ This leaves us with: $$-\int \frac{1}{1+u^2}du = -\tan^{-1}(\frac{1}{e^x})+c$$ However, I know the answer is wrong because the correct one is $\tan^{-1}(e^x) + c$ . Can someone please tell me where I screwed up? Many thanks in advance!","['integration', 'indefinite-integrals', 'calculus']"
3648785,Fourier series of a polynomial.,"I am looking for the Fourier series of a monomial restricted to the inteval $(0,2\pi)$ . Let $n\in\mathbb{N}$ and $$\forall x\in (0, 2\pi), \ f(x)=x^n.$$ By definition, the Fourier coefficients are $$c_k = \frac{1}{2\pi}\int_0^{2\pi} x^n e^{-ikx} dx,$$ and we know that $$f(x) =_{\text{a.e.}} \sum_{k\in\mathbb{Z}} c_k e^{ikx}.$$ This can be written in terms of gamma incomplete function, but there might be a closed form for this particular definite integral. What is the exact value of $c_k$ , the Fourier coefficient of the monomial $x^n$ ?","['integration', 'fourier-series', 'gamma-function']"
3648803,Young inequality: Generalization on $\mathbb{T}$ space.,"I'm interested in resolving this question that I find but on the $\mathbb{T}$ space.  ( Show that for any $f\in L^1$ and $g \in L^p(\mathbb R)$, $\lVert f ∗ g\rVert_p \leqslant \lVert f\rVert_1\lVert g\rVert_p$. ) Now my question is, is it possible? I mean I understand quite well the explanation that I found there but, if I take an $f\in L^1(\mathbb{T})$ and a $g\in L^p(\mathbb{T})$ is possible to prove, in the same way, that $f*g\in L^p$ for $1\le p\le \infty$ and that $||f*g||_{p}\le||f||_{1}||g||_{p}$ ? And if $p=\infty$ ? Is possible to prove that $||f*g||_{\infty}\le ||f||_{1}||g||_{\infty}$ if $f*g\in\mathbb{C(\mathbb{T})}$ ? Thanks you very much!","['measure-theory', 'convolution', 'young-inequality', 'complex-analysis', 'lp-spaces']"
3648861,Beginner question : logic and set theory,"First post on here, and apologies for its very basic content (I don't even understand most of the questions on here) but I'm wondering if anyone can help. I'm trying to teach myself maths at the ripe old age of 47, and wanted to start quite basic, with a book called ""Advanced precalculus"" by Daniel Kim. I liked the proposed approach, which is more theoretical and proof based than any of the other precalculus stuff I've come across (which I have gone through before, but it never seems to stick as the books never explain why stuff works). I've stumbled across something that's got be stumped though. One of the problems right at the start asks the reader to evaluate the truth of falsity of $A \subseteq B \cup C \to A \subseteq B \lor A \subseteq C$ I looked at this and doodled a Venn diagram and concluded that it had to be false, since I can find an example of a set A that is a subset of the union of B and C but is not a subset of either (e.g. A={4,5,6}; B={3,4,5}; C={5,6,7}). But the book says that the statement is true, and gives the following proof. $A \subseteq B \cup C \to \forall x \in U, x \in A \to x \in B \cup C$ $A \subseteq B \cup C \to \forall x \in U, x \in A \to x \in A \to (x \in B \lor x \in C)$ $A \subseteq B \cup C \to \forall x \in U, x \in A \to \sim(x \in A) \lor (x \in B \lor x \in C)$ $A \subseteq B \cup C \to \forall x \in U, x \in A \to \sim(x \in A) \lor \sim(x \in A) \lor (x \in B \lor x \in C)$ $A \subseteq B \cup C \to \forall x \in U, x \in A \to (\sim(x \in A) \lor x \in B) \lor (\sim(x \in A) \lor x \in C)$ $A \subseteq B \cup C \to \forall x \in U, (x \in A \to x \in B) \lor (x \in A \to x \in C)$ $\therefore A \subseteq B \cup C \to A \subseteq B \lor A \subseteq C$ It seems to me that the existence of a counterexample (I'm doubting myself so badly I'm wondering if it even is a counterexample now) must mean there is something wrong with the proof, but I'm struggling to see what it is. I'm guessing intuitively that something goes awry around line 4 or 5 in the author's use of the Idempotent and Associative Laws. I'm thinking along the lines that for every x in U, x can be (not in A but in B), or (not in A but in C), but that does not imply that x is always in B or always in C (it can be in either) and so A does not have to be a subset of B or C.","['elementary-set-theory', 'logic']"
3648864,Injective morphisms of locally free sheaves and fiberwise injectivity of vector bundles,"Let's fix a smooth integral algebraic variety $X$ over $\mathbb C$ . If $\mathscr E$ is a locally free sheaf on $X$ , then at each closed point $x\in X$ we have a complex vector space $E_x:=\mathscr E_x\otimes\mathbb C$ and $E:=\bigsqcup_x E_x$ can be endowed with a structure of smooth vector bundle. The transition between locally free sheaves and vector bundles is functorial and it is actually an equivalence of categories. Now assume that $\varphi:\mathscr E\to\mathscr H$ is an injective morphism between locally free sheaves. It means that for any $x\in X$ we have an injective morphisms of $\mathscr O_{X,x}$ -modules $\varphi_x:\mathscr E_x\to\mathscr H_x$ . It is well known that the induced map at the level of vector spaces (fibres of vector bundles) $\Phi_x:E_x\to H_x$ is in general not injective . This because the operation $-\otimes\mathbb C$ doesn't preserve left exactness. Finally the question: Let's keep the above hypotheses and notation. Is it true or false that there is always a Zariski open subset $U\subset X$ such that $\{\Phi_x\}_{x\in U}$ are injective (as homomorphism of vector spaces)? Are we able to describe such a subset? Is it characterized by some ""nice"" property?","['complex-geometry', 'vector-bundles', 'algebraic-geometry', 'sheaf-theory', 'differential-geometry']"
3648899,Topology of open and closed sets,"Since any open set A is basically the interior of the set A itself, and the boundary of a set is basically the end points of the set being that it could be in the set A or the complement of A, then is A-BdA = IntA and is A-IntA= BdA","['general-topology', 'real-analysis']"
3648902,Example of a linear onto map which is not open,"We know that every linear open map between normed spaces is onto. This fact actually motivates the Open Mapping theorem which gives extra assumptions for converse to hold true. But I am unable to construct counterexample for the first fact, i.e. I am looking for a linear map between normed spaces which is onto but not open. Any hint for such map. Thanks.",['functional-analysis']
3648928,"Using the Strong Law of Large Numbers to find a constant, c.","Let $X_1, X_2...$ be independent and identically distributed with mean 4 and variance 20. Set $S_n = X_1 + X_2 + ... + X_n$ and $V_n = X_1^2 + X_2^2 + … + X_n^2$ . Use the Strong Law of Large Numbers to find a constant $c$ such that $P\left(\lim_{n \to \infty} \frac{S_n}{V_n} = c\right) = 1$ . I've tried to find the expected value of $\frac{S_n}{V_n}$ as I believe that should be the answer, which I keep computing, incorrectly, as $\frac{1}{104}$ . What should I be doing here instead?","['law-of-large-numbers', 'probability-theory', 'probability']"
3648964,"Prove f(x) =0 for all x belonging to [0,1]","Suppose f : [0, 1] → R is differentiable and f (0) = 0. Suppose $|f′(x)| ≤ |f(x)|^2$ , $∀x ∈ [0, 1]$ . Prove that $f(x) = 0, ∀x ∈ [0,1]$ . Since f is differentiable, then $f$ is continuous, then if such a function is continuous, then for any given $\varepsilon > 0$ and $x \in [a, b]$ , there exists a $\delta > 0$ such that $|x-y| < \delta \implies |f(x) - f(y)|< \varepsilon$ ...I am not too sure how to continue from here.","['proof-explanation', 'proof-writing', 'real-analysis', 'continuity', 'derivatives']"
3648982,Monotonicity of a fraction combined with series (related to probability distributions),"Let $(p_n)_{n \geq 0}$ be a probability distribution on $\mathbb{N}_0$ with finite expectation, thus $\sum_{n = 0}^\infty p_n = 1$ and $\sum_{n=0}^\infty n \, p_n < \infty$ . I want to show that for all $0 \leq s \leq t < 1$ $$ \frac{\sum_{n=1}^{\infty} (1-s^n)\,  n \, p_n}{\sum_{n=1}^{\infty} (1-s^n)\, p_n} \leq \frac{\sum_{n=1}^{\infty} (1-t^n)\,  n \, p_n}{\sum_{n=1}^{\infty} (1-t^n)\, p_n}. $$ It is actually a little bit surprising to me that this is true as I thought that the $n$ in the numerator increases the weight of the bigger choice of $t$ compared to the denominator, but I plotted the function for some choices of $(p_n)$ and found that it is indeed increasing. Any ideas for formal proofs or arguments are appreciated.","['inequality', 'monotone-functions', 'probability', 'sequences-and-series']"
3649085,"True or False: a real function can be drawn without lifting pen from paper $\iff$ it is a) continuous, b) bounded and c) has finite arc length?","Title in the question. I suspect the answer is yes, but love being proven wrong. I've always wanted to know when ""a function can be drawn without taking pen off of paper"", but the necessary properties of such functions are not obvious because continuity on it's own doesn't do the trick. The missing pieces to this puzzle has bugged me for some time, but now I think I might be getting close to closure, and not the topological kind- excuse the pun. Drawing by lifting pencil from paper can still beget continuous function. So yes, $$ f(x) =
\begin{cases}
x \sin \frac 1x, & \text{if }x\neq 0 \\
0, & \text{if }x=0
\end{cases}
$$ has infinite arc length in any interval $[a,b]$ with $a\leq0$ and $b>0$ , so you can't draw this with pen and paper. But with for example $$ f(x) =
\begin{cases}
x^2 \sin \frac 1x, & \text{if }x\neq 0 \\
0, & \text{if }x=0
\end{cases}
$$ I believe the arc length is finite in any interval $[a,b]$ with $a\leq0$ and $b>0$ , so this I think you would be able to draw with pen and paper. Setting aside the fact that paper isn't ""smooth"" because it is made of atoms, and other irrelevant stuff, is my proposition in the title true, or are there really weird continuous, bounded functions with finite arc length that I'm not aware of? Also, are there ""better"" ways to characterise functions ""which can be drawn without taking pen off paper""? Edit: Actually, now that I think about it, ""boundedness"" is redundant, since finite arc length $\implies$ bounded.","['continuity', 'functions', 'graphing-functions', 'real-analysis']"
3649121,Computing the $L^p$ norm of an integral operator,"Let $T : L^{p}(\mathbb{R}_{>0}) \to L^{p}(\mathbb{R}_{>0})$ be given by: $$
(Tf)(x) = \int_0^\infty \frac{f(y)}{x+y} \mathrm{d}y
$$ I would like to show that this is a bounded operator for $ p \in (1,\infty)$ . I was given a hint for the question, namely to make a linear change of variables $y = xu$ .  Thus, to compute $\|T_f\|_p$ , we calculate: $$
\|Tf\|
_p = \left(\int_0^\infty\left(\int_0^\infty \frac{f(y)}{x + y} \mathrm{d}y\right)^p\mathrm{d}x\right)^{1/p} = \left(\int_0^\infty\left(\int_0^\infty \frac{f(xu)}{1 + u} \mathrm{d}u\right)^p\mathrm{d}x\right)^{1/p}
$$ From here, I am stuck. The natural thing to me seems to apply Holder's inequality to the inner integral, and compute: $$
\left(\int_0^\infty\left(\int_0^\infty \frac{f(xu)}{1 + u} \mathrm{d}u\right)^p\mathrm{d}x\right)^{1/p} \leq \left(\int_0^\infty \|f(x\cdot)\|_{L^p}^p \left\|\frac{1}{1+ \cdot }\right\|_{L^q}\mathrm{d}x\right)^{1/p} \leq \left\|\frac{1}{1+ . }\right\|_{L^q}^{1/p} \left(\int_0^{\infty}\|f(x \cdot)\|_{L^p}\mathrm{d}x\right)^{1/p}
$$ But the integral on the right hand integral does not seem to converge. Does anyone have any ideas? Another question I have is, how is the operator above related to the Laplace transform: $$
(\mathcal{L}f)(s) := \int_0^\infty e^{-sx}f(x) dx
$$ ? Many thanks.","['harmonic-analysis', 'laplace-transform', 'real-analysis', 'lp-spaces', 'functional-analysis']"
3649257,"If every finite subfamily of $\mathcal{F}$ has a transversal of size $n$, does $\mathcal{F}$ have the following intersection property?","Let's fix a family of sets $\mathcal{F}$ . A transversal of $\mathcal{F}$ is a set $T$ that intersects every set in $\mathcal{F}$ . Suppose that there exists $n\in\mathbb{N}$ such that every finite subfamily $\mathcal{F'}\subseteq \mathcal{F}$ has a transversal of size $n$ . This is for example the conclusion of the Alon Kleitman Matousek $(p,q)$ -theorem . Note that in particular it follows that any such $\mathcal{F'}$ partitions into $n$ subfamilies, each with the finite intersection property. Does it follows that $\mathcal{F}$ partitions into $n$ subfamilies, each having the finite intersection property? This seems to be provable using model theoretic compactness. Namely if $\cup\mathcal{F}$ is a structure where all the sets in $\mathcal{F}$ are definable we consider an $n$ -type describing a transversal of $\mathcal{F}$ of size $n$ . Since it is finitely satisfiable there is one such transversal in an elementary extension and the result easily follows. This approach seems like overkill, is there an easier way to prove this? Perhaps through some combinatoric argument or argument using choice?","['filters', 'model-theory', 'combinatorics', 'set-theory']"
3649434,"Evaluate $\frac{q}{1+q^2}+\frac{q^2}{1+q^4}+\frac{q^3}{1+q^6}$, where $q^7=1$ and $q\neq 1$.","Let $q$ be a complex number such that $q^7=1$ and $q\neq 1$ . Evaluate $$\frac{q}{1+q^2}+\frac{q^2}{1+q^4}+\frac{q^3}{1+q^6}.$$ The given answer is $\frac{3}{2}$ or $-2$ . But my answer is $\pm 2$ . At first, I tried to evaluate it directly. And the LHS equals to \begin{align}
\frac{q}{1+q^2}+\frac{q^2}{1+q^4}+\frac{q^3}{1+q^6} 
 & = \frac{q}{1+q^2}+\frac{q^2}{1+q^4}\cdot\frac{q^3}{q^3}+\frac{q^3}{1+q^6}\cdot\frac{q}{q} \\
 & = \frac{q}{1+q^2}+\frac{q^5}{1+q^3}+\frac{q^4}{1+q} \\
 & = q\cdot\frac{(1+q)(1+q^3)+q^4(1+q)(1+q^2)+q^3(1+q^2)(1+q^3)}{(1+q)(1+q^2)(1+q^3)} \\ 
 & = q\cdot\frac{1+q+q^3+q^4+q^4+q^5+q^6+1+q^3+q^5+q^6+q}{(1+q)(1+q^2)(1+q^3)} \\
 & = \frac{-2q^3}{(1+q)(1+q^2)(1+q^3)} \\ 
\end{align} And $$(x-q)(x-q^2)(x-q^3)(x-q^4)(x-q^5)(x-q^6)=x^6+x^5+x^4+x^3+x^2+x+1$$ Let $x=-1$ I get that $$(1+q)(1+q^2)(1+q^3)(1+q^4)(1+q^5)(1+q^6)=1$$ and $$(1+q)(1+q^2)(1+q^3)\cdot q^4(q^3+1)\cdot q^5(q^2+1)\cdot q^6(q+1)=1$$ therefore $$\left[(1+q)(1+q^2)(1+q^3)\right]^2=\frac{1}{q^{15}}=\frac{1}{q}$$ hence $$\left[\frac{-2q^3}{(1+q)(1+q^2)(1+q^3)}\right]^2=\frac{q}{1}\cdot 4q^6=4$$ $$\frac{-2q^3}{(1+q)(1+q^2)(1+q^3)}=\pm 2$$ And I try for a solution as a polar-form method $.\\$ Suppose $q=\cos\frac{2j\pi}{7}+i\sin\frac{2j\pi}{7}$ $$\frac{q^k}{1+q^{2k}}=\frac{\cos\frac{2jk\pi}{7}+i\sin\frac{2jk\pi}{7}}{2\cos\frac{2jk\pi}{7}\left(\cos\frac{2jk\pi}{7}+i\sin\frac{2jk\pi}{7}\right)}=\frac{1}{2\cos\frac{2jk\pi}{7}}$$ Am I going to the right direction? How I finish it? And please help to figure out what's wrong with my calculation at the first part. I appreciate for your help.","['summation', 'trigonometry', 'roots-of-unity', 'algebra-precalculus', 'complex-numbers']"
3649435,Show that $\lim_{n \to \infty}\frac {a_n} {n}=1 \implies \lim_{n \to \infty }\sup_{0\le k \le n} \frac {|a_k - k|} {n }=0$,"As the title suggests, I would like to derive the following implication. For a sequence ${a_n } $ of real numbers, show that $$\lim_{n \to \infty}\frac {a_n} {n}=1 \implies \lim_{n \to \infty }\sup_{0\le k \le n} \frac {|a_k - k|} {n }=0$$ Thanks in advance!","['calculus', 'sequences-and-series', 'real-analysis']"
3649460,Finding the maximum value of $\int_0^1 f^3(x)dx$,Find the maximum value of $\int_0^1 f^3(x)dx$ given that $-1 \le f(x) \le 1$ and $\int_0^1 f(x)dx = 0$ I could not find a way to solve this problem. I tried to use the cauchy-schwarz inequality but could not proceed further $$\int_0^1 f(x) \cdot f^2(x) dx \le \sqrt{\left(\int_0^1f^4(x)dx\right) \left( \int_0^1 f^2(x) dx\right)}$$ Any hints/solutions are appreciated.,"['integration', 'definite-integrals', 'calculus', 'integral-inequality', 'inequality']"
3649474,Why does $\tan(30^{\large\circ})=\frac{\tan(10^{\large\circ})\tan(50^{\large\circ})}{\tan(20^{\large\circ})}$?,"This problem is based on this Facebook post . One can find the value of $x$ in this diagram by noticing that $\angle CBD=50^{\large\circ}$ , and therefore, $$
\frac{\tan\left(10^{\large\circ}\right)}{\tan\left(20^{\large\circ}\right)}
=\frac{ED}{CD}=\frac{\tan(x)}{\tan\left(50^{\large\circ}\right)}\tag1
$$ Solving equation $(1)$ gives $$
\tan(x)=\frac{\tan\left(10^{\large\circ}\right)\tan\left(50^{\large\circ}\right)}{\tan\left(20^{\large\circ}\right)}\tag2
$$ Numerically computing the arctangent of the quantity in $(2)$ gives $x=30^{\large\circ}$ . This surprised me; I had expected some odd angle, but apparently, this turns out to be a nice angle. My question is: why does $\tan\left(30^{\large\circ}\right)=\frac{\tan\left(10^{\large\circ}\right)\tan\left(50^{\large\circ}\right)}{\tan\left(20^{\large\circ}\right)}$ ?",['trigonometry']
3649533,About triviality of a path in Residue Theorem,"Reading the Residue theorem in the following form : Theorem : Let $D \subseteq \mathbb{C}$ open, $f: D - S \longmapsto \mathbb{C}$ holomorphic, $S$ closed and discret in $D$ . Let $R \subseteq D$ compact with $C^{1}$ boundary. $R \cap S = \left\lbrace z_{1},\cdots, z_{k}\right\rbrace$ is finite and $\partial R \cap S = \varnothing$ Then we have $\int_{\partial R}f(z)dz = 2\pi i \sum\limits_{1 \leq i \leq k} Res(f,z_{i})$ . I've stumble across the following problem : (Adding a picture to clarity) Let $\beta = \gamma \ast l_{k} \ast \bar{\alpha_{k}} \ast \bar{l_{k}} \ast l_{k-1} \ast \cdots \ast l_{1} \ast \alpha_{1} \ast \bar{l_{1}}$ Where for example $\bar{\alpha_{k}}$ denote the inverse path of $\alpha_{k}$ , i.e $\alpha_{k}(1-t)$ , and each $\alpha_{i}$ denotes a little circumference around $z_{i}$ traveled counterclockwise. What I don't get is why $\beta$ is homotopically trivial in $R - \left\lbrace z_{1},\cdots, z_{k}\right\rbrace$ . I have the same problem with the proof of Laurent expansion of homolorphic function on Annulus . I think the reasoning are the same, which should be that $\beta$ runs through a disk, so it's hotopically trivial in $R - \left\lbrace z_{1},\cdots, z_{k}\right\rbrace$ . There is a topological way to see this fact ? I would like to avoid proofs that require any stronger  characterization. The assert to prove is very clear from the picture I attached, but it doesn't seem trivial or obvious to me prooving it.","['differential-geometry', 'curves', 'residue-calculus', 'general-topology', 'compactness']"
3649605,Any section of a smooth morphism is regular,"Let $f:X\to Y$ be a smooth morphism of relative dimension $n$ of separated schemes which are of finite type over $\text{Spec}(k)$ , where $k$ is any field. Suppose that $i: Y\to X$ is a section of $f$ , i.e. $f\circ i = \text{id}_Y$ . I want to prove that in this case, $i$ is a regular closed embedding of codimension $n$ . By this I mean that $i$ is a closed embedding, and every point $y\in Y$ has an affine open neighborhood $\text{Spec}(A)$ in $X$ such that if $Y$ corresponds to an ideal $I\subset A$ , we have that $I$ can be generated by a regular sequence of length $n$ . This statement is in Fultons 'Intersection Theory', Appendix B.7.3. It is also in the Stacks Project, Tag 067R. However, I do not completely understand the given proofs. So far I can prove the following: $i$ is a closed immersion. $i$ is regular if and only if for every closed point of $Y$ , say corresponding to a maximal ideal $\mathfrak{m}$ in an affine open $\text{Spec}(A)$ , the image $I_{\mathfrak{m}}$ of $I$ in the local ring $A_{\mathfrak{m}}$ can be generated by a regular sequence. For every closed point as above, the inclusion $\text{Spec}(A/\mathfrak{m})\to X$ is a regular embedding. So the embedding is regular on the fibers. But how to conclude from this that $i$ is a regular embedding? Any help would be really appreciated.","['algebraic-geometry', 'intersection-theory']"
3649633,Prove $\frac1{x^4} < \frac1{x^3} - \frac1{(x+1)^3}$,"Prove that for $x \ge 2$ , $$\frac1{x^4} < \frac1{x^3} - \frac1{(x+1)^3}.$$ What I have so far is: $${1\over x^3} - {1\over (x+1)^3} = {(x+1)^3-x^3\over x^3(x+1)^3} = {3x^2+3x+1\over x^3(x+1)^3} > {(x+1)^2\over x^3(x+1)^3} = {1\over x^3(x+1)}.$$ As seen, this won't lead to the correct expression, so could anyone give me any hints on how should I approach the question algebraically? I've thought of using the graph of ${1\over x^4}$ and using the area-under the graph but I was thinking if there is a trick to solving it via algebraic means.","['algebra-precalculus', 'inequality']"
3649785,Do intersections commute with direct sum?,"This is just a basic linear algebra question without that much context to it.  I'm wondering if the following identity holds for vector spaces: $$ (A \oplus  B)  \cap C  = (A \oplus 0 )\cap C  + (0 \oplus B)\cap C. $$ My intuition tells me it's always true, but I could be wrong.","['elementary-set-theory', 'direct-sum', 'linear-algebra', 'vector-spaces']"
3649825,Find the conditional expectation $E(X|aX+bY)$ $X$ and $Y$ are correlated normal random variables.,"let $(X,Y)$ is a bivariate normal random variable with $\mathbb  E(X)=\mu_1$ , $\text{ Var}(X)=\sigma_1^2$ , $\mathbb E(Y)=\mu_2$ , $\text{ Var}(Y)=\sigma_2^2$ and $\rho(X,Y)=\rho$ ( $\rho$ is the correlation function ).
I want to find Find the conditional expectation $\mathbb E(X\mid aX+bY)$ and  know the following calculations is correct or no?
The question is: Is the calculation  correct? The importance of this question is all of following question are special case of this so we can answer it easily use the results of this question. $\color{red}{1)}\mathbb E( cX + dY | aX + bY = u), \text{i.i.d case}$ here , $\color{red}{2)}\mathbb E(X \, |  \, 3X + 4Y],\text{i.i.d case}$ here. $\color{red}{3)}\mathbb{E}(X-Y \mid 2X+Y),\text{correlated case}$ here $\color{red}{4)}\mathbb{E}(X\mid X+Y),\text{i.i.d case}$ here. $\color{red}{5)}\mathbb{E}(3X+Y|X-Y=1),\text{correlated case}$ here $\color{red}{6)}\mathbb{E}(X\mid 2X + Y),\text{i.i.d case}$ here. $\color{red}{7)}\mathbb E(2X+Y|X-Y=1),\text{i.i.d case}$ here. I want to find $d$ such that $\text{cou}(X-dY, aX+bY)=0$ and by Correlations_and_independence conclude $X-dY$ and $aX+bY$ are independent. $$0=\text{cou}(X-dY, aX+bY)=a\text{Var}(X)-db \text{Var}(Y)+(b-ad) \text{cou}(X,Y)=a\sigma_1^2-db\sigma_2^2 +(b-ad) \rho \sigma_1 \sigma_2=(a\sigma_1^2 +b \rho \sigma_1 \sigma_2)
-d(b\sigma_2^2+a \rho \sigma_1 \sigma_2)$$ so $d=\frac{a\sigma_1^2 +b \rho \sigma_1 \sigma_2}{b\sigma_2^2+a \rho \sigma_1 \sigma_2}$ So I want to use following properties \begin{eqnarray}
\left\{
\begin{array}{c}
\mathbb E(aX+bY\mid aX+bY) =aX+bY      \\
\mathbb E(X-dY \mid aX+bY) = \mathbb E(X-dY)=\mu_1 -d \mu_2       
\end{array}
\right.
\end{eqnarray} and find $E(X|aX+bY)$ . \begin{eqnarray}
\left\{
\begin{array}{c}
a\mathbb E(X \mid aX+bY)+b\mathbb E(Y\mid aX+bY) =aX+bY      \\
\mathbb E(X \mid aX+bY)-d \mathbb E(Y \mid aX+bY)=\mu_1 -d \mu_2         
\end{array}
\right.
\end{eqnarray} \begin{eqnarray}
\left\{
\begin{array}{c}
\mathbb E(Y \mid aX+bY) &=&\frac{(aX+bY)-a(\mu_1 -d \mu_2)}{b+ad}      \\
\mathbb E(X \mid aX+bY) &=&\frac{(aX+bY)+\frac{b}{d}(\mu_1 -d \mu_2)}{a+\frac{b}{d}}   
=  \frac{d(aX+bY)+b(\mu_1 -d \mu_2)}{b+ad}    
\end{array}
\right.
\end{eqnarray} $$\mathbb E(X\mid aX+bY)=\frac{a\sigma_1^2+b \rho \sigma_1 \sigma_2}{
a^2 \sigma_1^2+b^2 \sigma_2^2 +2ab\sigma_1 \sigma_2}(aX+bY)+\frac{b^2 \sigma_2^2 +2ab\sigma_1 \sigma_2}{
a^2 \sigma_1^2+b^2 \sigma_2^2 +2ab\sigma_1 \sigma_2}(\mu_1 -\frac{a\sigma_1^2 +b \rho \sigma_1 \sigma_2}{b\sigma_2^2+a \rho \sigma_1 \sigma_2} \mu_2)$$ Thanks in advance for any help you are able to provide","['conditional-expectation', 'probability-theory']"
3649873,Convolution must be a bounded bilinear operator if it is well-defined,"In this answer I claimed the following. Claim . Suppose that the convolution $f\ast g$ belongs to $L^1(\mathbb R)$ for all $f\in L^p(\mathbb R)$ and all $g\in L^1(\mathbb R)$ . Then there is a constant $C>0$ independent on $f$ and $g$ such that $$\tag{1}\lVert f\ast g\rVert_1\le C\lVert f \rVert_p \lVert g \rVert_1.$$ This is an empty statement, as it is not true that $f\ast g\in L^1$ for all $f\in L^p, g\in L^1$ ; see this answer , for example. And indeed, the conclusion (1) is also false and it can be easily disproved by the scaling argument. The idea of my linked answer is to prove by contradiction that $f\ast g$ may fail to be in $L^1$ , using that (1) cannot hold. But then I realized that I cannot easily prove the Claim above. Question . Can you prove the Claim ? I had carelessly thought that this Claim followed from a straightforward adaptation of the classic application of the uniform boundedness principle given, for example, in this answer . There, we prove that if $g$ is a measurable function such that $fg\in L^1$ for all $f\in L^p$ , then there is a $C>0$ such that $$\left\lvert \int fg\ \right\rvert \le C\lVert f\rVert_p.$$ This follows from the uniform boundedness principle and from dominated convergence. But I don't see how to apply the same reasoning to the problem at hand.","['banach-spaces', 'functional-analysis', 'real-analysis']"
3649881,Is there a general method to compute the area/volume enclosed by an implicit curve/surface?,"If I have an implicit function $f_2(x,y) = C$ of a closed curve or an implicit function $f_3(x,y,z) = C$ of a closed surface, is there a general manner in which I can compute the area or volume enclosed by the curve or surface, respectively? Or is this dependent on the choice of functions?","['surfaces', 'curves', 'implicit-function', 'multivariable-calculus', 'differential-geometry']"
3649911,Strong convergence with comparable speed implies uniform convergence,"Let $X$ be a Banach space and $(T_n)_n$ a sequence of bounded operators on $X$ . Suppose there is a sequence $(q_n)_n$ in $\mathbb R$ converging to $0$ such that for each $x \in X$ there is $M_x > 0$ such that $\lVert T_n x \rVert \leq M_x q_n$ . I want to show that this implies $T_n \to 0$ uniformly. I thought Baire's theorem should do the trick: Let $\varepsilon > 0$ and consider the sets $A_k := \{x \in X : \forall n \geq k: \lVert T_n x \rVert \leq \varepsilon \}$ . Then it is clear that the $A_k$ 's are closed, $A_k \subseteq A_{k + 1}$ and $X = \bigcup_{k \in \mathbb N} A_k$ . Hence, there is $K \in \mathbb N$ such that $A_K$ has non-empty interior. In particular, there is $x_0 \in X$ and $r_0 > 0$ such that $B(x_0, r_0) \subseteq A_K$ . Now for $x \in B[0, 1]$ and $0 < r < r_0$ I estimate as follows: $$\lVert T_n x \rVert \leq \frac 1 r (\lVert T_n x_0 \rVert + \lVert T_n (rx + x_0) \rVert) \leq \frac{2\varepsilon}{r} \overset{r \to r_0} \longrightarrow \frac{2\varepsilon}{r_0}$$ and hence $\lVert T_n \rVert \leq \frac{2\varepsilon}{r_0}$ for all $n \geq K$ . But $r_0$ depends on $\varepsilon$ so this does not give me the desired conclusion. I also tried other set families $(A_k)_k$ but I seem to struggle to find the right one. I think my main problem is that my $A_k$ 's do not make use of the fact that $\lVert T_n x \rVert \leq M_x \varepsilon$ for big $n$ .","['metric-spaces', 'functional-analysis', 'real-analysis']"
3649953,Calculate surface integral with force,"Let $S=\{(x,y,z)\in \mathbb{R} : x^2+y^2+z^2 = 11\}$ and $F(x,y,z)=(2\sqrt{11}x,y^2,z^2)$ .
Calculate $$\int\int_SF(x,y,z)\;dS$$ I've calculated S parameterized, whose expression is: $$S(x,y) = (x,y,\sqrt{11-x^2-y^2})$$ Hence, $$r_x=\frac{\partial S}{\partial x}=(1,0,\frac{-x}{\sqrt{11-x^2-y^2}})$$ $$r_y=\frac{\partial S}{\partial y}=(0,1,\frac{-y}{\sqrt{11-x^2-y^2}})$$ $$||r_x \times r_y|| = \frac{\sqrt{11}}{\sqrt{11-x^2-y^2}}$$ So $dS = \frac{\sqrt{11}}{\sqrt{11-x^2-y^2}}dxdy$ and then $$\int\int_SF(x,y,z)\;dS = \int\int_SF(x,y,z)\;\frac{\sqrt{11}}{\sqrt{11-x^2-y^2}}dxdy$$ But then I don't know how to continue with this procedure. Another idea I have is to establish the following relationship: $$\int\int_SF\;dS= \int\int\int_V \nabla F\;dV$$ Any hint? Thanks in advance.","['multivariable-calculus', 'vector-analysis']"
3649992,A subsequence of a Markov chain is a Markov chain,"How to show it?
In other words, I want to show that if $  \{X_n\}$ is a Markov chain then for every $ n\in\mathbb{N} $ and $ 1\le i_1<\dots<i_k\le n $ such that $P(X_{i_1}=x_1,\dots ,X_{i_k}=x_k)>0 $ it holds that $ P(X_{n+1}=x |X_{i_1}=x_1,\dots ,X_{i_k}=x_k)=P(X_{n+1}=x |X_{i_k}=x_k) $ I have tried to prove it by induction on $n$ and by using the formula $ P(X_{n+1}=x|A)= \sum_{a} P(X_n=a|A)P(X_{n+1}=x|A,X_n=a)$ where $A=\{X_{i_1}=x_1,\dots ,X_{i_k}=x_k\}$ , but the expression $P(X_{n+1}=x|A,X_n=a)$ isn't any better.
Another attempt was to ""fill the holes"": let $\{j_1,\dots,j_{m}\}=\{1,\dots,n\}\setminus\{i_1,\dots i_k\}$ so $\\ P(X_{n+1}=x|A)=\sum_{b_1,\dots , b_m}P(X_{n+1}=x|A,X_{j_1}=b_1, \dots ,X_{j_m}=b_m)P(X_{j_1}=b_1, \dots ,X_{j_m}=b_m|A) $ but again I didn't know how to deal with the second expression.","['markov-chains', 'probability-theory', 'probability']"
3650011,The image of a disc under a diffeomorphism.,"This question is a simpler version of this . Suppose that you have a diffeomorphism $f:\mathbb{R}^2\to \mathbb{R}^2$ acting on the unit disc $D=\{(x,y):x^2+y^2\leq 1\}$ . Let $D_xf$ denote the total derivative of $f$ at point $x$ . Assume also that the biggest eigenvalue of $D_xf$ at $x$ , let us call it $\lambda_x$ , satisfies $|\lambda_x|>M$ , for all $x\in D$ and some $M>0$ . Take now $S$ to be a square with center $f(0)$ and side length $2$ . Question: Is it true that the image of the unit disc under $f$ will intersect $S$ assuming $M$ big enough? In other words is it true that $f(D)\cap S\not=\emptyset$ for big enough $M$ ? The idea is that $f$ stretches locally the disc in one direction by a lot but of course not at the same direction at every point. So  the answer to my question might be no but it seems very hard to believe. Does anyone have any ideas?","['multivariable-calculus', 'dynamical-systems', 'eigenvalues-eigenvectors', 'differential-geometry']"
3650143,Formal power series are a euclidean ring,"Denote by $F[[T]]$ the ring of formal power series over a field $F$ (i.e expressions of the form $\sum_{n=0}^{\infty}a_nT^n$ , $a_i \in F$ ). I need to show that this is a euclidean ring with respect to the norm $\mathrm{ord}(\sum a_iT^i) = \min\{n \mid a_n \neq 0\}$ . This just feels too trivial and I think I am missing something obvious, so I would like to get a feedback on my proof (if it is even correct). Take $f,g$ in the ring. If $\mathrm{ord}(g) < \mathrm{ord}(f)$ we get $g=f \cdot 0 + g$ , so we can divide $g$ by $f$ and get a remainder. Now suppose $\mathrm{ord}(g)=n>\mathrm{ord}(f)=k$ . I claim $f\mid g$ and therefore $g=fq+0$ for some $q$ . First, if $f=T^k$ then obviously $f\mid g$ . So take $q_1$ such that $g=T^kq_1$ . Now since $\mathrm{ord}(f) = k$ we can write $f=T^kf_1$ for some $f_1$ that is invertible (since the coefficient of $1=T^0$ in it is nonzero). Taking $q = f_1^{-1}q_1$ we get $g=fq$ , so $f\mid g$ and we are done. Is this true? Am I missing something? This is suspicious for me because it seems like I proved that whenever $\mathrm{ord}(g) > \mathrm{ord}(f)$ we have $f\mid g$ which seems like a much stronger result than what I was asked to prove.","['field-theory', 'ring-theory', 'abstract-algebra', 'formal-power-series', 'power-series']"
3650153,Prove: $\sin (\alpha - \beta) = \sin \alpha \cos \beta - \sin \beta \cos \alpha$,"This is an exercise in Gelfand's Trigonometry, It is not that difficult but I am doing something wrong that is preventing me from proving the identity. We need to use the following diagram to prove it: My attempt: $$
\begin{eqnarray*} 
\sin (\alpha - \beta) = \frac{CD}{AC} \\
= \frac{PQ}{AC} \\
= \frac{BQ - BP}{AC} \\
= \frac{BQ}{AC} - \frac{BP}{AC} \\ 
\end{eqnarray*}
$$ Now in the following step we should use an intermediary to make this equal to the required identity, but for the first fraction I can't find anything rather than $AB$ } $$
= \frac{BQ}{AB} \cdot \frac{AB}{AC} \\
$$ My problem here is I don't see how $\frac{AB}{AC}$ would simplify to $\cos \beta$ to me this seems like $\sec \beta$ How could this be fixed?","['trigonometry', 'solution-verification']"
3650175,How to calculate the angles from angle-side couples in a series of triangles (with an algebraic solution)?,"In this image, the red and black labels are known, and the aim is to calculate one of the blue angles or sides (the rest will follow). There is no standard trigonometric solution, as we know only one angle and one side for each triangle. However, I believe there should be an algebraic solution for the common variables we have for different angles. For example, $$\frac{\sin b}{2A} = \frac{\sin c}{C} = \frac{\sin d}{D} $$ $$\frac{\sin b1}{A} = \frac{\sin c}{B} = \frac{\sin a}{D} $$ $$\frac{\sin b2}{A} = \frac{\sin e}{C} = \frac{\sin d}{B} $$ When knowing only $2A$ and $b$ for the main triangle, there are unlimited solutions. However, the given $a$ restricts to one single solution. This is why I believe this problem is solvable.","['trigonometry', 'linear-algebra', 'geometry']"
3650200,"""Gaps"" or ""holes"" in rational number system","In Rudin's Principles of Mathematical Analysis 1.1, he first shows that there is no rational number $p$ with $p^2=2$ . Then he creates two sets: $A$ is the set of all positive rationals $p$ such that $p^2<2$ , and $B$ consists of all positive rationals $p$ such that $p^2>2$ . He shows that $A$ contains no largest number and $B$ contains no smallest. And then in 1.2, Rudin remarks that what he has done above is to show that the rational number system has certain gaps. His remarks confused me. My questions are: If he had shown that no rational number $p$ with $p^2=2$ , this already gave the conclusion that rational number system has ""gaps"" or ""holes"". Why did he need to set up the second argument about the two sets $A$ and $B$ ? How does the second argument that "" $A$ contains no largest number and $B$ contains no smallest"" showed gaps in rational number system? My intuition does not work here. Or it is nothing to do with intuition?","['rational-numbers', 'analysis', 'real-analysis']"
3650214,"Questions regarding $\ln(x) = \sum_{n=1}^{\infty}\frac{(-1)^{n}}{n}(\zeta(n,x)-\zeta(n))$. Have I found something ""new""?","Introduction TL;DR
I was messing around with the Taylor series for $\ln(x)$ when I ended up with the formula \begin{align} \ln(x) &= \sum_{n=1}^{\infty}\frac{(-1)^{n}}{n}(\zeta(n,x)-\zeta(n)) \\\\ & =\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}H_{x-1}^{(n)}\end{align} (Here $\zeta(n,x)$ is Hurwit's Zeta function and $H_{x-1}^{(n)}$ is the $(x-1)$ -th Harmonic number of order $n$ (Generalized Harmonic numbers)) I claim that this formula works for all $x > 0$ (only $x\in\mathbb{R}$ for now). My questions are at the bottom of the post. Here are some numerical examples (using WolframAlpha): $\ln(2)$ $\ln(3)$ $\ln(0.5)$ $\ln(69)$ $\ln(1000)$ Derivation My derivation of the formula bases on the taylor series for $\ln(x+1)$ shown below $${\displaystyle \ln(1+x)=\sum _{n=1}^{\infty}{\frac{(-1)^{n-1}}{n}}x^{n}}$$ which is valid for $|x|\leq1$ . We can clearly see that we could get a infinite series for $\ln(2)$ by plugging in $1$ . But how would we get a series for $\ln(3)$ ? Well, one could plug in $\frac{1}{2}$ to get that $${\displaystyle \ln(1+\frac{1}{2})=\sum _{n=1}^{\infty}{\frac{(-1)^{n-1}}{n2^n}}}$$ By adding the inside of the natural logarithm on the LHS, and then using basic logarithm properties we get: $${\displaystyle \ln(3)=\ln(2) + \sum _{n=1}^{\infty}{\frac{(-1)^{n-1}}{n2^n}}}$$ Then, using the infinite series from earlier for $\ln(2)$ we get \begin{align} \ln(3) & =\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n} + \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n2^n} \\\\ & = \sum_{n=1}^{\infty}\frac{(2^n+1)(-1)^{n+1}}{n2^n}\end{align} Do you get the point? Now, in general, plugging in $\frac{1}{x}$ , we would get: \begin{align} \ln(x+1) & = \ln(x) + \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{nx^n}\end{align} Now what's $\ln(x)$ ? Well, one could do the exact same thing (the process I described above) for first $x$ , then $x-1$ , then $x-2$ and so on, all the way until $1$ since $\ln(1) = 0$ . So doing this we get: \begin{align} \ln(x+1) & = \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n} + \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n2^n} \cdots + \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{nx^n} \\\\ & = \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n} + \frac{(-1)^{n+1}}{n2^n} \cdots + \frac{(-1)^{n+1}}{nx^n} \\\\ & = \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}\left(1+\frac{1}{2^n}+\frac{1}{3^n}\cdots+\frac{1}{x^n}\right) \\\\ & = \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}\sum_{k=1}^x \frac{1}{k^n}\\\\ & =\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}H_{x}^{(n)} \\\\  &= \sum_{n=1}^{\infty}\frac{(-1)^{n}}{n}(\zeta(n,x+1)-\zeta(n))\end{align} Then plugging in $x-1$ we get: $$\boxed{\ln(x) = \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}H_{x-1}^{(n)} = \sum_{n=1}^{\infty}\frac{(-1)^{n}}{n}(\zeta(n,x)-\zeta(n))}$$ Questions First of all, is my derivation correct? (I believe so, since I have tested the formula numerically a lot now, and it has worked) The title is a bit misleading; me finding something new about something elementary as natural logarithms is pretty much impossible, but I couldn't find this series listed anywhere, so if anyone recognizes this series please link some reference? Does this series work for all $x>0$ and $x\in\mathbb{R}$ ? Maybe even complex numbers? Does this series converge quickly? Can something else be said about the series? (Cool things to note, possible simplifications... whatever)","['riemann-zeta', 'harmonic-numbers', 'logarithms', 'sequences-and-series']"
3650223,Why Is $\ln 23+\cfrac{1}{\color{red}{163}+\cfrac{1}{1+\cfrac{1}{\color{red}{41}}}}\approx\pi$,"I know from reading that the Heegner number 163 yields the prime generating or Euler Lucky Number 41 . Now apparently $\ln23<\pi$ and this can be shown without calculators. I noticed that $$ 
\pi-\ln23= \cfrac{1}{\color{red}{163}
          + \cfrac{1}{1
          + \cfrac{1}{\color{red}{41} + \cfrac{1}{2 + \cdots}}}}
$$ Question: Are there any ""good"" mathematical reasons why the largest Heegner and largest Euler Lucky number occur within the first three (-four ?)
  terms  of the expansion ? Or is it purely coincidence ? Indeed the finite c.f. $$\cfrac{1}{\color{red}{163}
          + \cfrac{1}{1
          + \cfrac{1}{\color{red}{41}}}}:=\frac{42}{6887}\approx 0.00609843\ldots.$$ In turn this yields the crude approximation $$\ln 23+\frac{42}{6887}\approx\pi;$$ which I believe gives the first 8 digits of $\pi$ correctly.","['number-theory', 'continued-fractions', 'pi', 'approximation']"
3650233,What properties must $f$ have if $f(x)=f(\sin(\pi x)+x)\iff x\in\Bbb{Z}$?,"This is a follow-up from my previous question . I now know that the statement: $$f(x)=f(\sin(\pi x)+x)\iff x\in\Bbb{Z}$$ is not true for all $f$ . For example, $f$ can be $x$ to any constant power or any constant to the $x$ th power but it cannot be the gamma function $\Gamma(x)$ or $\sin(x)$ or $x^x$ . According to the answer I received, it is important to note whether or not $f$ is injective. However, $f(x)=x^2$ is not injective, yet it satisfies the statement. If being injective is only a sufficient condition as opposed to a necessary condition, what exactly do we know about the class of functions that makes this statement true? Thanks in advance!","['functions', 'real-analysis']"
3650255,Derivative of the Determinant of the Jacobian Matrix,"Let $f:\mathbb{R}^n\to \mathbb{R}^n$ be a smooth vector field, with flow $\phi_t$ taking values in $\mathbb{R}^d$ i.e $\partial_t \phi_t= f(\phi_t)$ , and $\phi_0=id$ . Let $J_t(x) := \det \Big(D_x(\phi_t(x))\Big)$ be the determinant of the Jacobian matrix $D_x(\phi_t(x))$ . Is it really obvious that  : $$ \partial_t \det \Big(D_x(\phi_t(x))\Big)=\text{div}\Big(f(\phi_t(x)) \Big)J_t(x) ?$$ To prove this does one have to write out the entire formula for the detemrinant of an $n\times n$ matrix?","['divergence-operator', 'real-analysis', 'jacobian', 'multivariable-calculus', 'calculus']"
3650275,Proving two binomial identities,"I would like to show that \begin{align}
&\sum_{j=n-k}^n\binom nj(1-x)^{n-j-1}x^{j-1}(j-nx)\\
&\qquad=\binom n{n-k}(n-k)(1-x)^kx^{n-k-1}\sum_{k=0}^{n-1}\frac{(-1)^k}n\binom{n-1}k\binom n{n-k}(n-k)(1-x)^kx^{n-k-1}\\
&\qquad=(-1)^{n-1}\sum_{k=0}^{n-1}\binom{n-1}k\binom{n+k-1}k(-x)^k
\end{align} I feel I exhausted all identities/properties of binomials without success. Mathematica says it is true, but how to show it?","['summation', 'binomial-coefficients', 'combinatorics']"
