question_id,title,body,tags
4595141,$\ast$-homomorphism and two-sided ideals of unitarization $C^\ast$-algebra,"We work with the unitization of $C^\ast$ -algebra. I.e. $C^\ast$ -algebra $\mathcal{A}$ with norm $\|\cdot\|$ . Let $\tilde{\mathcal{A}}=\mathcal{A}\oplus \mathbb{C}$ as a vector space. We endow it with multiplication and involution, $$(a,\lambda)\cdot (b,\mu):=(ab+\lambda b+\mu a,\lambda \mu)$$ and $$(a,\lambda)^\ast:=(a^\ast,\bar{\lambda})$$ I managed to prove that this is a $\ast$ -algebra. Now I have some questions because I want to understand this example. We denote $\omega:\mathcal{A}\rightarrow\tilde{\mathcal{A}}$ by the map $\omega(a):=(a,0)$ . I want to show that this is a $\ast$ -homomorphism, i.e. it is linear and so on. The example does not show anything regarding $\ast$ -homomorphism, but my attempt: Let $a,b\in\mathcal{A}$ then $\omega(a+b)=(a+b,0)=(a,0)+(b,0)=\omega(a)+\omega(b)$ . Is it true? I readed about two side ideals in my book. Is it true that $\omega(\mathcal{A})$ is a two-sided ideal in $\tilde{\mathcal{A}}$ ? If so, how come? Definition: A (two-sided) ideal $J$ in a Banach algebra $\mathscr{A}$ is a subspace of $A$ with the property that $A \in \mathscr{A}$ and $S \in J$ implies $AS \in J$ and $SA \in J$ . Is this definition the right one to use? If so, then I will try to do the part "" $AS \in J$ "" if anyone can give an example of the other part $SA \in J$ .","['operator-theory', 'operator-algebras', 'functional-analysis', 'ideals']"
4595200,Find the value of $\int_{0}^{\sqrt{3}}\int_{0}^{\tan^{-1}y}\sqrt{xy}\ dxdy$,How can I find the value of $I = \int_{0}^{\sqrt{3}}\int_{0}^{\tan^{-1}y}\sqrt{xy}\ dxdy$ ? I tried changing the limits and got the new integral as $I = \int_{0}^{\pi/3}\int_{\tan(x)}^{\sqrt{3}}\sqrt{xy}\ dydx$ And then tried evaluating it to get $I = \frac{2}{3} \int_{0}^{\frac{\pi}{3}}\sqrt{x}(3^\frac{3}{4} - \tan^\frac{3}{2}x)\ dx$ but I do not know how to proceed further to get the result Wolfram alpha gives the same answer on all cases so I don't think my steps are wrong. Or is this an XY problem at this point?,"['integration', 'multivariable-calculus', 'definite-integrals']"
4595208,Limits of Sequences,"I'm having trouble calculating this limit directly : $$\lim_{n\to\infty}\frac{(2n＋1)(2n＋3)\cdots(4n＋1)}{(2n)(2n＋2)\cdots(4n)}$$ It can be calculated using the inventory method and the result is: $$\lim_{n\to\infty}\frac{(2n＋1)(2n＋3)\cdots(4n＋1)}{(2n)(2n＋2)\cdots(4n)} ＝\sqrt {2}$$ But the limit method is not smart, I need help doing this in a direct way. This is the first time I am asking a question on this site. I apologize if it is not a good question.",['limits']
4595225,How do the $l^p$ topologies on sequences relate to the product topology?,"This is a soft (because not fully specified) question. Let (as usual) $\mathbb{R}^\mathbb{N}$ be the set of real sequences $(x_n)_{n\in \mathbb{N}}$ . This is the cartesian product of $\omega$ -many copies of $\mathbb{R}$ , so if we endow $\mathbb{R}$ with its usual topology, then $\mathbb{R}^\mathbb{N}$ can be given the product topology. (If $\mathbb{R}^\mathbb{N}$ is interpreted as the space of maps $\mathbb{N}\rightarrow \mathbb{R}$ via $(x_n)\mapsto (n\mapsto x_n)$ , then endowing it with the compact-open topology [assuming the discrete topology on $\mathbb{N}$ ] does the same thing, it seems to me.) Meanwhile, for $1\leq p \leq \infty$ , let (as usual) $l^p$ be the subset of $\mathbb{R}^\mathbb{N}$ consisting of $p$ th-power-summable sequences, i.e., those satisfying $$ \sum_{n\in\mathbb{N}} |x_n|^p < \infty.$$ (For $p=\infty$ the condition is instead that $(x_n)$ is bounded.) Usually $l^p$ is viewed as a Banach space with the $l^p$ -norm $$ \|(x_n)\|_p := \left(\sum_{n\in\mathbb{N}} |x_n|^p\right)^{1/p}.$$ (For $p=\infty$ , the norm is instead $\sup_n |x_n|$ .) My question is this: What is known about the relationship between the topology on $l^p$ coming from the $l^p$ norm, vs. the subspace topology induced by the product topology on $\mathbb{R}^\mathbb{N}$ ? For example: It seems to me that both topologies will induce the same topology on a finite-dimensional subspace of $l^p$ ; but this is not saying much. It seems to me that the open unit ball with respect to the $l^p$ norm is not an open set of the subspace topology on $l^p\subset \mathbb{R}^\mathbb{N}$ , because any open set in $\mathbb{R}^\mathbb{N}$ contains sequences with coordinates that are $>1$ , and some of them are in $l^p$ . (Am I thinking about this right?) So, is it the case that the $l^p$ norm topology is strictly finer than the subspace topology coming from $\mathbb{R}^\mathbb{N}$ ? Or are there open sets in the latter that are not open in the former? If so, what's an example? Context: This section can be ignored, it adds no mathematical content to the question. I include it only for readers who are curious why I am asking this. The above question was stimulated by the following meandering and undisciplined, but to me provocative, train of thought: The closed unit ball in a Hilbert space with a countably infinite Hilbert space basis such as $l^2$ is famously not compact; so undoubtedly the ""unit cube"" is also not compact. But Tychonoff's theorem says that the product of an $\mathbb{N}$ -indexed collection of unit intervals is compact. Huh. Oh wait. If I take the unit cube in $l^2$ to be sequences $(x_n)\in l^2$ such that each $x_i$ is in $[0,1]$ , then the cartesian product of $\mathbb{N}$ -many $[0,1]$ 's is actually strictly bigger than this! In particular, $(1,1,\dots)$ is not in $l^2$ . (In fact, only the finitely supported $0,1$ -sequences are in $l^p$ for any $p<\infty$ .) So there is a proper subset of $[0,1]^\mathbb{N}$ that is in $l^2$ ! I wonder what this subset ""looks like"". Meanwhile, the entirety of $[0,1]^\mathbb{N}$ is in $l^\infty$ ! More precisely, there is a natural bijection of $[0,1]^\mathbb{N}$ (the compact space with the product topology) with the subset of $l^\infty$ satisfying $0\leq x_n\leq 1$ for all $n\in\mathbb{N}$ . Is it a homeomorphism?","['general-topology', 'soft-question', 'functional-analysis']"
4595241,"Proving $\,\sin^272^\circ - \sin^260^\circ = \frac18\left(\sqrt5 - 1\right)$","I'm fairly new into proving trigonometric equations, but I believe I'm getting better at it day after day. Yet, I have stumbled upon one such which left me at an impasse. The problem is presented as follows: $$\sin^272^\circ - \sin^260^\circ = \dfrac{\sqrt5 - 1}8$$ The goal is to prove the truth of the so-presented equation. I first complicated the right-hand side of the equation to $\dfrac{\sqrt5-1}4\cdot\dfrac12$ to then simplify it into $\sin18^\circ\cdot\sin30^\circ$ (knowing, of course, that $\dfrac{\sqrt5-1}4$ is equal to $\sin18^\circ$ , and that $\dfrac12$ is equal to $\sin30^\circ$ ). I then proceeded to work the proof from the left-hand side, now that both sides of the equation are termed in trigonometric ratios. The left-hand side is easily noticed to be a difference of squares, hence being apt for complication as $(\sin72^\circ - \sin60^\circ)(\sin72^\circ + \sin60^\circ)$ . It is only a coincidence to notice that the expressions can be further simplified knowing the sum/difference to product rule (i.e. that $\sin A + \sin B = 2\sin\dfrac{A + B}2\cos\dfrac{A - B}2$ , and also that $\sin A - \sin B =2\cos\dfrac{A + B}2\sin\dfrac{A - B}2$ ). Executing these simplifications, we get the next result: $2\sin66^\circ\cdot\cos66^\circ\cdot2\sin6^\circ\cdot\cos6^\circ$ . It is evident to see how the double angle rules could be of use here (that $2\sin\theta\cos\theta =\sin2\theta$ ). The penultimate result with this rule applied is thus, so far: $\sin132^\circ\cdot\sin12^\circ$ . I can only go as far as to simply right the ratios in terms of angles less that $45$ degrees, which is: $\cos42^\circ\cdot\sin12^\circ$ . For the most of it, I am stuck; and any rules I apply to simplify either expression gets me to a result previously acquired in the run. How am I to finish proving that $\cos42^\circ\cdot\sin12^\circ$ is equal to $\sin18^\circ\cdot\sin30^\circ$ , or equivalently $\sin18^\circ\cdot\cos60^\circ$ . This problem can be found in S.L Loney's book on plane trigonometry, a really helpful book for getting a good picture on trigonometry. Thank you in advance.","['trigonometry', 'solution-verification', 'proof-writing']"
4595261,Doubt in finding volume and setting up the limits?,"I am currently confused in how to find volume enclosed between two surfaces $z_1=f_1(x,y),z_2(x,y)=f_2(x,y)$ After going through some online resources, I thought the general way of doing this is: $\iint_{D} (z_2-z_1) dxdy$ where it will be $(z_2-z_1)$ or $(z_1-z_2)$ depending on the surfaces. $D=\{(x,y)\in \mathbb{R}^2:  f_1(x,y)=f_2(x,y)$ (eliminating $z$ from the two equation) $\}$ However I got stuck while solving the problem . Find the volume between the surfaces $x+y+2z=2,2x+y+z=4$ in the first octent. I could figure out that it would be of the form $$\iint _D \left\{(4-(2x+y))-\frac{(2-(x+y))}{2}\right\}\,dxdy$$ However how do I figure out $D$ ? Should I take the projection of the two surfaces on the $xy$ plane and then find out the limits as we did in double integration? Or should I equate the two surfaces and then find out the limit?","['multivariable-calculus', 'multiple-integral', 'volume']"
4595302,Proving that a non-homogeneous wave equation preserves energy $E(t)\equiv\int_{B}(u_t^2+c^2|\nabla u|^2)dx$,"The following problem is an old qual. prep problem and I am not sure whether the problem contains all necessary information. The problem is verbatim the following: Let $B$ be an open and bounded subset of $\mathbb{R}^n$ , $c > 0$ and $r$ a smooth enough function. Let $u$ be a solution to equation $(\frac{\partial^2}{\partial t^2} - c^2\Delta)u = r$ in $S\times (0, T)$ . Show that the energy $E(t)\equiv \frac{1}{2}\int_B\left(\left(\frac{\partial u}{\partial t}\right)^2 + c^2\left|\nabla u\right|^2\right)dx$ is preserved by $u$ . You may assume $u$ to be smooth enough. The equation in question is a homogeneous/non-homogeneous wave equation. Since no other information is provided about $r$ it is best to assume that $r\not\equiv 0$ . Below I am denoting an $n-1$ dimensional surface measure by $dS(x)$ . By Lebesgue's dominated convergence theorem we can differentiate $E$ w.r.t. $t$ to obtain that $$E'(t) = \frac{1}{2}\int_B 2 \frac{\partial^2u}{\partial t}\cdot \frac{\partial u}{\partial t} + 2c^2\nabla u\cdot \nabla \frac{\partial}{\partial t}udx$$ and then apply Green's first identity to simplify $E'(t)$ to $$E'(t) = \int_B \frac{\partial^2u}{\partial t}\cdot \frac{\partial u}{\partial t}dx + c^2\left(-\int_B\frac{\partial u}{\partial t}\cdot \Delta udx + \int_{\partial B}\frac{\partial u}{\partial n}\cdot \frac{\partial u}{\partial t}dS(x)\right)\Longleftrightarrow$$ $$E'(t) = \int_B \frac{\partial^2u}{\partial t}\cdot \frac{\partial u}{\partial t} - c^2\frac{\partial }{\partial t}\cdot \Delta udx + c^2\int_{\partial B}\frac{\partial u}{\partial n}\cdot \frac{\partial u}{\partial t}dS(x)\Longleftrightarrow$$ $$E'(t) = \int_B \frac{\partial u}{\partial t}\left(\frac{\partial^2 u}{\partial t} - c^2\Delta u\right)dx + c^2\int_{\partial B}\frac{\partial u}{\partial n}\cdot \frac{\partial u}{\partial t}dS(x)\Longleftrightarrow$$ $$E'(t) = \int_B \frac{\partial u}{\partial t}\cdot rdx + c^2\int_{\partial B}\frac{\partial u}{\partial n}\cdot \frac{\partial u}{\partial t}dS(x)$$ I am not sure how to move beyond this point. We want $\int_B \frac{\partial u}{\partial t}\cdot rdx = -c^2\int_{\partial B}\frac{\partial u}{\partial n}\cdot \frac{\partial u}{\partial t}dS(x)$ but I can't recall any useful identity/lemma to verify this. If we assume that $u$ vanishes on the boundary of $S$ then we would also need to have $r \equiv 0$ . I suppose that the integral $ I \equiv \int_B \frac{\partial u}{\partial t}rdx$ can vanish w/o $r\equiv 0$ , but considering that this problem is supposedly on the easier side of things, I suspect that there is no need to divide $B$ in any way in order to analyze how $I = 0$ , but rather that either there is some information missing or that I am missing some important tool(s).","['measure-theory', 'initial-value-problems', 'real-analysis', 'wave-equation', 'partial-differential-equations']"
4595316,How to standardize a random variable up to the fourth moment?,"Suppose $X$ is a random variable such that $\operatorname*{E}(X^n) < \infty$ for any $n \in \mathbb{Z}$ . We can standardize $X$ up to the second moment by letting $Y = \frac{X - \operatorname*{E}(X)}{\sqrt{\operatorname*{E}\left(X^2\right) - \operatorname*{E}(X)^2}}$ such that $\operatorname*{E}(Y) = 0$ and $\operatorname*{E}(Y^2) = 1$ . How do we construct a random variable $Z$ from $X$ such that it satisfies the following properties? The right-hand side can be any constant. I picked 0's and 1's simply because they look nice. \begin{equation}
\begin{cases}
\operatorname*{E}(Z) &= 0 \\
\operatorname*{E}(Z^2) &= 1 \\
\operatorname*{E}(Z^3) &= 0 \\
\operatorname*{E}(Z^4) &= 1 \\
\end{cases}
\end{equation} I would imagine we need some higher-order operations like square root or maybe logarithm, but I cannot wrap my head around it.","['probability-distributions', 'expected-value', 'moment-generating-functions', 'probability-theory', 'random-variables']"
4595336,Derivative of sign function with two variables in the argument,"It is known that the derivative of the sign function $\mathbf{sgn}$ , defined as $$
\mathrm{sgn}(x) = \frac{x}{|x|}
$$ is given as $$
\frac{\mathrm d}{\mathrm dx}\mathrm{sgn}(x) = 2\delta (x)
$$ where $\delta(x)$ represents the Dirac delta function. Suppose now I have a sign function with 2 variables $x$ , $y$ in its argument. Namely: $\mathrm{sgn}(x-y)$ . How can I compute $$
\frac{\mathrm d}{\mathrm dy}\mathrm{sgn}(x-y)\qquad?
$$","['derivatives', 'absolute-value', 'dirac-delta']"
4595337,Resources to study climate models and the stability of solutions to differential equations,"I am currently taking a course on the mathematical models of climate change, and we are studying from a book called ""Mathematics & Climate"" by Kaper & Engler, and these two papers: UMAPclimate.pdf WalshStommel.pdf While these two papers are not super hard to read, they are a bit advanced for my level. Same goes for the book as well. I took an introductory course on differential equations, but it was all about how to solve them. We didn't really talk about the solutions themselves. And a big chunk of this paper is about stability of the solutions -- or at least that's what my professor is focusing on. We covered stuff like Lipschitz Condition, asymptotic stability, Lyapunov stability, and the Adomian decomposition method -- all of which seem completely foreign to me and a bit advanced. Is there a textbook or a course somewhere that explains at least some of these topics in a more beginner-friendly way and/or in a bit more detail? Any help would be greatly appreciated!","['book-recommendation', 'ordinary-differential-equations', 'reference-request']"
4595345,"Does an equivalence relation on a group play well with the group operation, provided that the equivalence class of the identity is a normal subgroup?","Given an equivalence relation $\sim$ on a group $G$ , such that $$
a \sim a' \ \text{ and } \ b \sim b' \ \Longrightarrow \ ab \sim a'b' \ ,
$$ the equivalence class $[e_G]$ of the identity is a normal subgroup of $G$ . Moreover, $a \sim b$ if and only if $ab^{-1} \in [e_G]$ . Furthermore, this way we can define an equivalence relation $\sim_H$ , which ""plays nicely"" with the group operation, for any normal subgroup $H$ . I am curious whether the converse statement is true: if an equivalence relation on a group $G$ is such that $[e_G]$ is a normal of subgroup of $G$ , then $$
a \sim a' \ \text{ and } \ b \sim b' \ \Longrightarrow \ ab \sim a'b' \ ?
$$ If not, provide a counterexample. I have spent considerable amount of time thinking about the statement and I think it is false.","['equivalence-relations', 'group-theory', 'abstract-algebra', 'normal-subgroups']"
4595347,Primes of the form $x^2 + n y^2$ using theta function,"It is well known that if $p$ is a prime, then $p$ can be written in the form $x^2 + n y^2$ under certain congruences conditions. For example, \begin{align}
p = x^2+y^2 &\Leftrightarrow p = 2 \text{ or } p = 1 \text{ }(\mathrm{mod}\text{ }4) \\
p = x^2+2y^2 &\Leftrightarrow p = 2 \text{ or } p = 1,3 \text{ }(\mathrm{mod}\text{ }8) \\
p = x^2+3y^2 &\Leftrightarrow p = 3 \text{ or } p = 1 \text{ }(\mathrm{mod}\text{ }3)
\end{align} At the same time, one can constructs a generating function for the number of solutions of the equation $x^2 + n y^2 = m$ , using the theta function. If one define, \begin{equation}
\phi(q) = \sum_{m=-\infty}^\infty q^{m^2}
\end{equation} Then, we have the following generating function, \begin{equation}
\phi(q)\phi(q^n) = \sum_{m=0}^\infty q^m \text{ nb_solutions}(x^2 + n y^2 = m) 
\end{equation} Question : I do wonder whether it is possible to demonstrate the congruences above using an analytical method through the generating function written above, and the identities of the theta function. This would for example imply verifying that the coefficient is zero for primes satisfying a certain congruence, and verifying it is not for others. Note : Being a physics student, I would be really pleased by a solution which only involves symbolic manipulations, and identites of the theta function. I do not understand high mathematical theories concerning modular forms, elliptic functions or whatever. I have come to understand such things only by playing with it alone, and finding some information on the internet.","['number-theory', 'theta-functions', 'generating-functions']"
4595367,Borel-Cantelli lemma variation,"Consider $(X_i)$ independent events such that $\sum_{i = 1}^\infty P(X_i) = \infty$ Let $S_n = \sum_{i = 1}^n \mathbb{1}_{X_i}$ , then $\frac{S_n}{\mathbb{E}S_n} \to 1$ almost surely. Do you have any ideas how to prove this? I've been thinkint about applying law of large numbers or usual Borel-Cantelli lemma, but I could not come up with anything.",['probability']
4595378,Question on meromorphic functions,"I'm trying to do this optative exercise from my complex analysis course: Check if series $$\sum_{k=-\infty}^{\infty}\left(\frac{1}{z-\pi k}\right)^2$$ converges for all $z\in \mathbb{C}$ to a meromorphic function $f_1(z)$ , so that if we consider the function $$f(z)=\frac{1}{\sin^2 z}$$ then $g(z)=f(z)-f_1(z)$ is an entire function. This exercise is way harder than the previous ones I've done and don't know how to approach it. I don't know if I should find $f_1(z)$ first or there's a way to prove the result without finding it. Thanks for the time and suggestions.","['complex-analysis', 'complex-numbers', 'sequences-and-series']"
4595442,How to prove identity $\sum_{k=0}^{n-1}\tan^2\left(\theta+{k\pi\over n}\right)=n^2\cot^2\left({n\pi\over2}+n\theta\right)+n(n-1)$?,"Looking at Jolley, Summation of Series, formula 445: $\sum_{k=0}^{n-1}\tan^2\left(\theta+{k\pi\over n}\right)=n^2\cot^2\left({n\pi\over2}+n\theta\right)+n(n-1)$ How can one prove this? Considering $\left(  \sum^{n-1}_{j=0}Z_j\right)^2 = \sum^{n-1}_{j=0} Z_j^2 +\sum^{n-1}_{j=0}\sum^{n-1}_{i\neq j} Z_jZ_i$ from here I thought one could take the log differential of the well known sine product: $\begin{align}
\prod_{k=0}^{n-1}\sin\left(\theta + \frac{k\pi}{n}\right)
&= 2^{1-n} \sin(n\theta)
\end{align}$ to get $n \cot(n\theta) = \sum_{k=0}^{n-1}\cot\left(\theta+\frac{k\pi}{n}\right)$ or $-n \cot(\frac{n\pi}{2}+n\theta) = \sum_{k=0}^{n-1}\tan\left(\theta+\frac{k\pi}{n}\right)$ then square to get $n^2 \cot(\frac{n\pi}{2}+n\theta)^2 = (\sum_{k=0}^{n-1}\tan\left(\theta+\frac{k\pi}{n})\right)^2$ but I cannot see how to show the $\sum _{j=0}^{n-1} \sum _{k=0}^{n-1} \tan \left(\theta +\frac{\pi  j}{n}\right) \tan \left(\theta +\frac{\pi  k}{n}\right) [j\neq k]$ necessary for the $n(n-1)$ part of the identity. Perhaps somebody could show a more successful method.","['summation-method', 'summation', 'trigonometric-series', 'contour-integration', 'trigonometry']"
4595443,Different type of function composition,"When I first heard about function composition in school, I also expected the graphs of the composed function to 'behave' like both simple functions that are composed. Suppose we have f(x) = $x^2$ and $g(x) = \cos(x)$ The graph of $y = f(g(x))$ looks like in the first image: But my instinct in school was to think that the graph looks like the second image: Red curve represents f(x) = $x^2$ and the blue curve represents the result of the special composition $y = f(g(x))$ . Think of this special type of composition as molding/transposing the X-axis on the graph/curve of the outer function in the composition and then draw the graph of the inner function on previously modified coordinates plane. In the example above think of the parabola $x^2$ as the X-axis and then draw $ \cos(x)$ around this new X-axis (which is no longer a straight line). My questions are: Does this type of function composition have a name? Has it been studied? Are there any papers I can read on it? If yes, how can I search about them? I know that in this type of composition, the result might not be a function that can be written in explicit form $y=f(x)$ , but when can it be? As a final note, here is $\cos(x)$ composed with $\frac{1}{6}\cos(6x)$ . It looks nice, like the beginning of a fractal series. Red curve represents $g(x) = \cos(x)$ and the blue curve represents the result of the special composition.","['calculus', 'functions', 'graphing-functions', 'function-and-relation-composition']"
4595444,Linear map of unitization of a non-unital $C^\ast$-algebra,"I recently discussed unitization of a non-unital $C^\ast$ -algebra. I proved some facts and so on (which I afterwards found on the internet). Anyway let's consider the following $C^\ast$ -algebra. That is, the $C^\ast$ -algebra $\mathcal{A}$ with norm $\|\cdot\|$ . Let $\tilde{\mathcal{A}}=\mathcal{A}\oplus \mathbb{C}$ as a vector space. We endow it with multiplication and involution, $$(a,\lambda)\cdot (b,\mu):=(ab+\lambda b+\mu a,\lambda \mu)$$ and $$(a,\lambda)^\ast:=(a^\ast,\bar{\lambda})$$ As mentioned I did some proofs of this and discussed with fellow members of this site. However I can see people discuss norm of this beast, let me clarify this. We consider $\omega:\mathcal{A}\rightarrow\tilde{\mathcal{A}}$ by the map $\omega(a):=(a,0)$ . This is indeed a two sided ideal $\omega(\mathcal{A})$ and of course also a $\ast$ -homomorphism. Now we will suppress the inclusion $\omega$ and think of $\mathcal{A}$ as an ideal in $\mathcal{A}$ . I claim: Take $x\in\tilde{\mathcal{A}}$ then we consider the linear map $\tilde{L}_x:\tilde{\mathcal{A}}\rightarrow \tilde{\mathcal{A}}$ by $y\mapsto xy$ restricts to a map, $L_x:\mathcal{A}\rightarrow \mathcal{A}$ which is bounded with $\|L_x\|_\infty \leq \|x\|_1$ . I want to show the above claim. Here we know that the one-norm is given by $x=(a,\lambda)\in \tilde{\mathcal{A}}$ by $\|x\|_1=\|a\|+|\lambda|$ and of course $\|\cdot\|_\infty$ denote the operator norm. But how can we construct such a proof of this claim? I've tried to read more about this kind of unitization and norms on the following link however I don't think they do the claim that I give. http://www.math.nagoya-u.ac.jp/~richard/teaching/s2020/Terasawa.pdf So, how would one do it?","['c-star-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras']"
4595447,Multiplicative strictly increasing sequences,"Suppose $1=x_1<x_2<x_3<\cdots$ is a strictly increasing sequence such that $x_{nm}=x_n x_m$ for all $n,m$ . Is it true that there has to exist some $c>0$ such that $x_n=n^c$ ?","['analysis', 'sequences-and-series']"
4595571,How are there two generating functions for the Fibonacci sequence?,"I've come across two generating functions for the Fibonacci sequence, $$
  F(z) = \frac{1}{1-z-z^2}
  \quad\text{and}\quad
  F(z) = \frac{z}{1-z-z^2} \,.
$$ I've seen both of their proofs and both of them seem legible, but I'm still unable two understand how both the equations give same function.","['generating-functions', 'fibonacci-numbers', 'combinatorics', 'sequences-and-series']"
4595588,Calculate $\mathbf{A}^+\mathbf{y}$ indirectly without accessing $\mathbf{A}$,"The Original Question (informal): I have a black-box linear system $f:\mathbb{R}^N\rightarrow\mathbb{R}^M~(0\ll M\ll N)$ . It is guaranteed that there exists a full-rank matrix $\mathbf{A}\in\mathbb{R}^{M\times N}$ satisfying $\forall \mathbf{x}\in\mathbb{R}^N,f(\mathbf{x})\equiv\mathbf{Ax}$ . Here I call $\mathbf{A}$ the transformation matrix of $f$ . Now given a vector $\mathbf{y}\in\mathbb{R}^M$ , I want to calculate $\mathbf{z}=\mathbf{A}^+\mathbf{y}\in\mathbb{R}^N$ , where $\mathbf{A}^+\in\mathbb{R}^{N\times M}$ , satisfying $\mathbf{AA}^+=\mathbf{I}_M$ in some cases, is the Moore–Penrose inverse of $\mathbf{A}$ ( Reference 1 ). There are some supplementary information: I may know that there is a way to get the explicit form of $\mathbf{A}$ from a given fixed $f$ ( Reference 2 ). However, since $M$ and $N$ are both very large numbers (about $10^6$ or even $10^9$ ), I would not get $\mathbf{A}$ , explicitly calculate $\mathbf{A}^+$ , and finally calculate $\mathbf{z=A}^+\mathbf{y}$ . (The computational complexity of direct calculation is too high for me.) I may want an indirect way of obtaining $\mathbf{z=A}^+\mathbf{y}$ . The inner structure of linear $f$ is very complicated. Actually, $f$ is a black-box. But in my system, I can conveniently calculate $f(\mathbf{r})$ for any given $\mathbf{r}\in\mathbb{R}^N$ . In other words, the forward pass of $f$ is fast and efficient. I may not need $\mathbf{A}$ , $\mathbf{A}^+$ or some operators about them. I may only want $\mathbf{z=A}^+\mathbf{y}$ , which is known to be unique when $\mathbf{y}$ and $f$ are given and fixed. There are no prior knowledge about $f$ , $\mathbf{A}$ and $\mathbf{y}$ . In other words, their inner values or elements can be random numbers, like some noise. So, how to get $\mathbf{z=A}^+\mathbf{y}$ efficiently? Some of My Efforts: I was trying to search an $\mathbf{x}$ satisfying $\mathbf{Ax=y}$ . To be concrete, I use a tool dubbed PyTorch (a deep learning framework on Python) to optimize a randomly initialized $\mathbf{x}$ with a loss function $\mathcal{L}=\lVert f(\mathbf{x})-\mathbf{y} \rVert _2^2$ . And $f$ is a neural network in my own implementation. When $\mathcal{L}$ hits $0$ , I stop my optimization program and get the estimation. However, since $0\ll M\ll N$ , there may exist $\mathbf{x}_1$ and $\mathbf{x}_2$ satisfying $\mathbf{x}_1\ne \mathbf{x}_2$ and $\mathbf{Ax}_1=\mathbf{Ax}_2=\mathbf{y}$ . Therefore, I think this method could not exactly find the unique $\mathbf{z=A}^+\mathbf{y}$ that I want. Does there exists an efficient way to achieve this? There may be two statements (but in fact, only one of them is true): The answer is ""Yes"". There exists a way to efficiently calculate $\mathbf{z=A}^+\mathbf{y}$ from given fixed $f$ and $\mathbf{y}$ , without accessing the explicit forms of $\mathbf{A}$ or $\mathbf{A}^+$ . In other words, there are some properties of $\mathbf{z=A}^+\mathbf{y}$ can be utilized. But I have still not found them. The answer is ""No"". To get $\mathbf{z=A}^+\mathbf{y}$ , I should try to get the explicit form of $\mathbf{A}^+$ and then calculate $\mathbf{A}^+\mathbf{y}$ . There are no efficient and indirect ways. After a long struggle, I still have no idea about this problem. Now I may tend to believe that the above second statement is true. Any solutions, suggestions and discussions about this problem would be appealing for me. I am still searching, trying and thinking ...","['matrices', 'pseudoinverse', 'linear-algebra', 'optimization', 'matrix-equations']"
4595695,How to approach & algebraically solve a GMAT work rate problem involving related rates?,"I have a question based on the GMAT exam regarding work rate problems. In particular, problems that involve multiple entities having different work rates, but those work rates are defined relative to each other. My goal is to find a general algebraic solution to solve problems like this. Here's an example question: Machine A is twice as fast as Machine B in completing a task. Together they can complete the task in 10 hours. How long will it take each of the machines to complete the task separately? To solve general work rate problems, most GMAT books use the formula below like so: $\frac{1}{A} + \frac{1}{B} = \frac{1}{t}$ where A and B are the work rates of two separate machines required to complete a task. This is simple enough when we have a question like $\text{A takes 2 hours to complete a task. B takes 3 hours to complete a task. How many hours does it take for A and B completed to task together?}$ The solution is simply: $\frac{1}{2} + \frac{1}{3} = \frac{1}{t}$ Solving for t gives $\frac{6}{5}$ hours. However, I'm probably missing something when it comes to related rates like the first question above. My approach is as below: Define A in terms of B. Then solve using the formula provided above. The problem I face is translating (1) to (2). If Machine A is twice as fast as Machine B, then it should be: $\text{A = 2B}$ . Substituting this into the work rate formula yields: $\frac{1}{2B} + \frac{1}{B} = \frac{1}{10}$ . Solving for B gives 15 hours. Taking this result into $\text{A} = 2B$ gives 30 hours, which is the inverse result I'm looking for. Instead, the solution should be $\text{A = 15 hours, and B = 30 hours.}$ Pretty sure it's something subtle, but would appreciate some greater clarity in approaching problems like these.","['word-problem', 'algebra-precalculus']"
4595698,"Understanding The Fundamental Theorem of Calculus, Part 1","First Part of the Fundamental Theorem of Calculus says that "" the derivative of a definite integral with respect to its upper limit is the integrand evaluated at the upper limit. ""
So it means that the antiderivative of integrand evaluated at the upper limit is it's integral? Can you please explain for me how it works? So it says that for $$g(x) = \int_a^x f(t) dt$$ $$\frac{d}{dx}g(x) = f(x)$$ So does it mean that we can find an integral from a to x, by finding antiderivative of f(x)? If so where does a go?","['integration', 'calculus', 'derivatives']"
4595707,Curves where holomorphic function is real valued through a critical point.,"Let $f:\Omega\to \mathbb{C}$ be a holomorphic function, where $\Omega$ is an open set containing $\alpha\in \mathbb{R}$ , and suppose that $f\big|_{\Omega \hspace{0.5mm} \cap \hspace{0.5mm}\mathbb{R}}$ is real valued (so that $f^{(k)}(\alpha)$ are real valued for all $k$ ). Suppose that $f'(\alpha)=0$ and that $f''(\alpha)\ne 0$ . For $r>0$ such that $\{z\in \mathbb{C}:|z-\alpha|\le r\}\subset \Omega$ , define $g=g_{r}:[0,2\pi)\to \mathbb{C}$ by $$g(\theta)=f(\alpha+r\text{e}^{\theta i}).$$ I am trying to prove that there exists a $r_0>0$ , such that for all $0<r\le r_0$ , there are exactly four distinct values $\theta_1, \theta_2, \theta_3, \theta_4\in [0,2\pi)$ such that $g(\theta_i)\in \mathbb{R}$ . What I amble to do, and where my approach falls short. If we use a power series expansion about $\alpha$ , we have $$f(\alpha+r\text{e}^{\theta i})=f(\alpha)+\frac{1}{2}r^2\text{e}^{2\theta i}f''(\alpha)+R(r,\theta)$$ with $R(r,\theta)=O(r^3)$ uniformly in $\theta$ as $r\to 0^{+}$ . If $C>0$ is such that $|R(r,\theta)|\le Cr^3$ for $r$ sufficiently small, then by geometric considerations, the argument of $\frac{1}{2}r^2\text{e}^{2\theta i}f''(\alpha)+R(r,\theta)$ cannot differ from $2\theta=\text{arg}(\frac{1}{2}r^2\text{e}^{2\theta i}f''(\alpha))$ by more than $\theta^{\ast}$ , as shown in the image below: Here $P=\frac{1}{2}r^2\text{e}^{2\theta i}f''(\alpha)$ , $|P-Q|=Cr^3$ and $\angle P Q 0$ is a right angle. Thus, $\theta^{\ast}=\arcsin(\frac{2Cr}{f''(\alpha)})=O(r)$ as $r\to 0^{+}$ . This allows me to write $$f(\alpha+r\text{e}^{\theta i})-f(\alpha)=(\frac{1}{2}r^2f''(\alpha)+O(r^3))\text{e}^{(2\theta +O(r))i}.$$ For sufficiently small $r$ , this allows me to conclude that there are at least four values of $\theta$ such that $g(\theta)$ is real valued, given by $$\theta_1=0, \hspace{4mm} \theta_2\approx \frac{\pi}{2}, \hspace{4mm} \theta_3=\pi, \hspace{4mm} \theta_3\approx \frac{3\pi}{2}.$$ The problem is that I am unable to show that there are exactly 4 zeros when $r$ is sufficiently small. I have tried bounding the number of zeros of $$h(z)=\frac{1}{2i}(f(\alpha+r\text{e}^{zi})-f(\alpha+r\text{e}^{-zi}))$$ on the real line using different techinques, but the bounds I get are not much better than 16 zeros, which is not good enough (you may assume that $f$ satisfies $f(\overline{z})=\overline{f(z)}$ ). I also had the idea to consider the stable and unstable manifolds of the dymaical system $x'(t)=f'(x(t))$ near the critical point $x=\alpha$ , but this did not lead anywhere. Besides the values $\theta=0$ and $\theta=\pi$ , where $f$ is definitely real valued, I may compute for $\xi=u+iv$ that $$\text{Im}f(\alpha+\xi)=f''(\alpha)uv+\frac{1}{6}f^{3}(\alpha)(3u^2v-v^3)+\frac{1}{6}f^{(4)}(\alpha)(u^3v-uv^3)+O(|\xi|^5).$$ If I set the left hand size equal to zero and ignore the big $O$ -term, then I get an approximation for the nontrivial curve through $\alpha$ for which $f(z)$ is real valued: $$v=\pm \sqrt{\frac{6f''(\alpha)+3f^{(3)}(\alpha)u^2+f^{(4)}(\alpha)u^3}{f^{(3)}(\alpha)+f^{(4)}(\alpha) u}}$$ (assuming $f^{(3)}(\alpha)$ and $f^{(4)}(\alpha)$ are not both zero). There may be a very simple way to prove the statement, but I am unable to see it.
Any help would be greatly appreciated.",['complex-analysis']
4595714,Area of a right triangle with its angle bisector and hypotenuse,"A right triangle $ABC$ is given with right angle at $C$ . If $AB=a$ and the angle bisector of $A$ is $AL=l$ , find the area of the triangle $ABC$ . The angles of the triangle $ABL$ are $\dfrac{\alpha}{2}, 90-\alpha$ and $90+\dfrac{\alpha}{2}$ , where $\alpha$ is the angle $BAC$ . The law of sines gives $$\dfrac{\sin\left(90+\dfrac{\alpha}{2}\right)}{\sin(90-\alpha)}=\dfrac{a}{l}$$ which is equivalent to $$\dfrac{\cos\dfrac{\alpha}{2}}{\cos\alpha}=\dfrac{a}{l}$$ If we use that $\cos\alpha=2\cos^2\dfrac{\alpha}{2}-1$ , the last equality becomes an equation for $\cos\dfrac{\alpha}{2}=x>0$ . We would have $$\cos\dfrac{\alpha}{2}=\dfrac{a}{l}\left(2\cos^2\dfrac{\alpha}{2}-1\right) \\ \dfrac{2a}{l} x^2-x-\dfrac{a}{l}=0 \\ 2ax^2-lx-a=0$$ The positive solution is $x=\cos\dfrac{\alpha}{2}=\dfrac{l+\sqrt{l^2+8a^2}}{4a}$ . By definition $\cos\dfrac{\alpha}{2}$ is $\dfrac{AC}{AL}$ , so for $AC=b$ I got $$b=\dfrac{l^2+l\sqrt{l^2+8a^2}}{4a}$$ From here I don't see anything else except the Pythagorean theorem for $BC$ but it's pretty messy. I wish I could see something better. I tried letting Wolfram Alpha do the simplification, which probably avoids errors, but it's still ugly: (l (√(8 a^2 + l^2) + l) √(a^2 - (l^2 (√(8 a^2 + l^2) + l)^2)/(16 a^2)))/(8 a) I have tried to find a better way, but I keep coming back to expressions involving $b$ and the Pythagorean Theorem. For instance, the area of $\triangle ABC$ can be expressed as $\dfrac{c(a+b)}{2}$ where $c = |CL|$ ; but this doesn't simplify any more neatly.","['euclidean-geometry', 'geometry', 'plane-geometry']"
4595748,A Jacobian criterion for the tangent space and the tangent cone to be equal,"For a lecture that I am preparing as part of an introductory course on algebraic varieties, I am trying to give an elementary approach to non-singular points of affine algebraic sets. I work over an infinite field $k$ of characteristic $0$ , not necessarily algebraically closed, and I would like to show that, if $Z\subset k^n$ is an algebraic set whose ideal $$\mathcal{I}(Z):= \lbrace P \in k\left[x_1,\ldots ,x_n\right]\ |\ \forall x\in Z, P(x)=0\rbrace$$ is generated by a family of polynomials $(f_1,\ldots,f_m)$ such that the Jacobian matrix $$J_x = \begin{pmatrix} \frac{\partial f_1}{\partial x_1}(x) & \ldots & \frac{\partial f_1}{\partial x_n}(x) \\\ \vdots & & \vdots \\\ \frac{\partial f_m}{\partial x_1}(x) & \ldots & \frac{\partial f_m}{\partial x_n}(x)  \end{pmatrix},$$ has rank $m$ , then the tangent cone at $x$ is equal to the Zariski tangent space at $x$ (of course, this can only happen if $m\leq n$ ). The Zariski tangent space $T_xZ$ is (almost by definition) the kernel of $J_x$ , meaning that $$T_xZ = \bigcap_{i=1}^m\ker f_i'(x)\ ,$$ while the tangent cone $C_x(Z)$ is defined to be the common zero locus of all initial terms of polynomials in $\mathcal{I}(Z)$ : for all $f\in \mathcal{I}(Z)$ , denote by $f^\ast_x$ the first non-zero term in the Taylor expansion of $f$ at $x$ ( $f^\ast_x$ is therefore a symmetric $r$ -linear form on $k^n$ ) and define $$C_x(Z) = \lbrace h\in k^n\ |\ f^\ast_x(h,\ldots,h)=0\rbrace.$$ Since $f'_i(x)$ is the initial term of $f_i$ for all $i\in\lbrace 1,\ldots,m\rbrace$ , we always have $C_x(Z)\subset T_xZ$ . So the goal is to use the regularity assumption on the Jacobian matrix to prove the converse inclusion. There is a proof of this in Corollary 10.14-(b) of Andreas Gathmann's 2021 lecture notes (p.82) but it uses the notion of dimension of an algebraic variety, which I do not yet have in the course: https://www.mathematik.uni-kl.de/~gathmann/class/alggeom-2021/alggeom-2021.pdf Note that, for all $f\in\mathcal{I}(Z)$ , we can write $f=\sum_{i=1}^m a_i f_i$ for some polynomials $a_i\in k[x_1,\ldots,x_n]$ . Since the linear forms $(f_1'(x),\ldots,f_m'(x))$ are linearly independent, the linear term of $f$ at $x$ is $0$ if and only if $a_i(x)=0$ for all $i \in\lbrace 1,\ldots,m \rbrace$ . Can we use this to say something about the initial term of $f$ at $x$ ? Is this initial term necessarily $0$ on $(h,\ldots,h)$ if $h\in\ker J_x$ ? Notes Maybe using the Taylor expansion in place of changing the coordinates in order to be able to assume that $x=0$ makes things more complicated: if we do change coordinates and assume that $x=0$ , then we can just think of the Taylor expansion as the decomposition of $f$ into homogeneous components and the initial term is just the non-zero homogeneous component of lowest degree. Is it easier to say something about the initial term of $\sum_{i=1}^m a_if_i$ if we think of it this way? An argument along these lines would also have the advantage of being valid in positive characteristic, I believe. In case I am missing something by not supposing $k$ algebraically closed, this assumption can be added (it is not quite what I would like at this basic stage of the course, but it is OK).","['algebraic-geometry', 'education']"
4595782,How to generate trees with 11 and 12 vertices (with diagrams)?,"I looked at this thread , but I couldn't seem to understand how to use geng and nauty to generate trees and get the diagrams. I need to generate trees with 11 and 12 vertices and possibly with more vertices, and I would need the diagrams because I need to mark the color of each node. Does anyone know a software where I can generate trees with n vertices (where n > 10) with a diagram? If diagrams are not possible, does anyone have a suggestion as to how I could efficiently mark each node in the tree without a diagram? This is part of my independent study. Thank you in advance.","['graph-theory', 'trees', 'discrete-mathematics']"
4595795,$X$ uncorrelated with any function of $Y$ implies $X$ and $Y$ independent.,"This question is purely out of curiosity and mainly to question my intuitions about independence of random variables. Q: Take two non trivial random variables, with non disjoint support (see edit below) $X,Y \in \mathbb{L}_2(\Omega, \mathcal{F}, \mathbb{P})$ , so that projections and covariance formulas are well defined. If $X$ is uncorrelated with any function $g$ of $Y$ , i.e. $\operatorname{Corr}(X,g(Y)) = 0, \: \forall \: g$ measurable, this implies $X$ and $Y$ are independent. Is the above statement true? I could not find any post on mathstack on this. One way I tried to prove the above is by proving the following: Assume that $X$ and $Y$ are dependent, then there exists a function $f$ such that the correlation between $X$ and $f(Y)$ is nonzero. Reason why $\mathbb{L}_2$ is important: This is also the reason why we have to take the random variables in $\mathbb{L}_2$ , otherwise one could find counterexamples to the second statement by taking $X$ with undefined variance or expectation and show that the covariance can never be nonzero, as it is not well defined. Thoughts: Any ideas or references? Maybe something additional must be assumed about the functions $g$ ? Maybe instead of this, one should assume that the correlation is zero with any random variable $Z$ which is $X$ -measurable? Thank you very much for your help and time. Reason why non disjoint support is important: EDIT. Here I post a counterexample that contradicts the second statement , if we do not assume that the random variables have non disjoint support, i.e.: Assume that $X$ and $Y$ are dependent, then there exists a function $f$ such that the correlation between $X$ and $f(Y)$ is nonzero. Take $([0,1], \mathcal{B}([0,1]), \lambda)$ , where $\lambda$ is the Lebesgue measure. The key idea is that if they have disjoint support we can find a counterexample. Take: $$ X(x) = \left(x - \frac{1}{2} \right) \mathbb{1}_{[0,1/2]}(x)$$ and: $$ Y(x) = \left(x - \frac{3}{2} \right) \mathbb{1}_{[1/2,1]}(x)$$ Take any function $f$ , then $f(Y(x)) = f(0)$ constant for any $x \in [0,1/2]$ thus: $$ X(x)f(Y(x)) = f(0)X(x) \mathbb{1}_{[0,1/2]}(x)$$ which, as $\int X d\lambda = 0$ , implies: $$\int X f(Y) d \lambda = 0$$ for any $f$ . This implies they are uncorrelated and it provides a counterexample.","['statistics', 'analysis', 'probability-theory', 'probability', 'random-variables']"
4595847,Inequality needed in molecular dynamics,"$\newcommand{\f}[2]{\frac{#1}{#2}}$ $\newcommand{\nor}[2]{ \left| \! \left| #1 \right| \! \right|_{#2} }$ $\newcommand{\ab}[1]{\left|#1\right|}$ $\newcommand{\R}{\mathbb{R}}$ $\newcommand{\pa}[1]{\left( #1 \right)}$ $\def\1{1}$ Consider the norm \begin{align*}
	\nor{V}{L^{\f{3}{2}}(\R^3) + L^\infty(\R^3)} := {\underset{\substack{V_{3/2} \in L^{\f{3}{2}  }(\R^3) \\ V_{\infty} \in L^\infty(\R^3) \\ V = V_{3/2} + V_\infty}}{\text{inf}}\quad} \pa{\nor{V_{3/2}}{L^{\f{3}{2}}} + \nor{V_\infty}{L^\infty(\R^3)}}
\end{align*} which defines a Banach space. It enables to treat local singularities since $\ab{\cdot}^{-p} \in L^{\f{3}{2}} + L^\infty$ for any $p < 2$ , by decomposing $\ab{\cdot}^{-p} = \ab{\cdot}^{-p} \1_{\ab{\cdot} \le c}  + \ab{\cdot}^{-p}\1_{\ab{\cdot} \ge c}$ (the first function is in $L^{\f{3}{2}}$ and the second in $L^{\infty}$ ) but $\ab{\cdot}^{-p} \notin L^{\f{3}{2}}$ . But $\ab{\cdot}^{-2} \notin L^{\f{3}{2}} + L^\infty$ . My question is, do we have \begin{align*}
	\nor{ \ab{r-\cdot}^{-1} - \ab{\cdot}^{-1}  }{L^{\f{3}{2}} + L^\infty} \le c \ab{r} 
\end{align*} for any $r \in \R^3$ , where $c$ does not depend on $r$ ? Of course it's when $r$ is small that there is an issue. At first sight, it is natural to do \begin{align}\label{eqq}
	\ab{\ab{r-x}^{-1} - \ab{x}^{-1}} &= \ab{\ab{r-x} - \ab{x}}\ab{r-x}^{-1}\ab{x}^{-1} \nonumber \\
						 & \le \ab{r} \ab{r-x}^{-1}\ab{x}^{-1}
\end{align} but then $\nor{\ab{r-\cdot}^{-1}\ab{\cdot}^{-1}}{L^{\f{3}{2}} + L^\infty}$ is not bounded in $r$ when $r$ is small, since \begin{align*}
\nor{\ab{r-\cdot}^{-1}\ab{\cdot}^{-1}}{L^{\f{3}{2}} + L^\infty} \rightarrow +\infty
\end{align*} when $\ab{r} \rightarrow 0$ . But for any $\varepsilon > 0$ small, \begin{align*}
\ab{r}^\varepsilon \nor{\ab{r-\cdot}^{-1}\ab{\cdot}^{-1}}{L^{\f{3}{2}} + L^\infty} \rightarrow 0
\end{align*} So is there an approach, which is finer and which yields the result ? Or what is the lowest singularity, that we cannot remove ?","['inequality', 'functional-analysis', 'lipschitz-functions']"
4595848,Does $f(t) \leq - \int_0^t f(s) ds \implies f \leq 0$?,"I don't know if $f(t) \leq - \int_0^t f(s) ds \implies  f \leq 0$ is true or not (f is a continuous function with $f(0) = 0$ ). The range of $t$ is $[0,\infty[$ I can show that it implies that $\int_0^t f(s) ds \leq 0$ (by multiplying by $e^{t}$ and integrating) but there are function f that are not negative everywhere such that $\int_0^t f(s) ds \leq 0$ (really negative next to $0$ and then positive but small). For the context, I am trying to get information about $g \geq 0$ when I know that $g(t) + \int_0^t g(t) \leq t$ . I would like to obtain a kind of gronwall bound, with a ""small"" contant (not exponential) as: $g(t)\leq C t$ or better something with not depend on $t$ .","['ordinary-differential-equations', 'real-analysis']"
4595908,"Understanding The Fundamental Theorem of Calculus, Part 2","I'm trying to understand the difference between 1st and 2nd parts of The Fundamental Theorem of Calculus.
Let's start from the definitions: First part says that if $f$ is continuous on $[a,b]$ , then the function $g$ defined by $$g(x) = \int_a^x f(t) dt, \ a <= x <=b $$ is continuous on $[a,b]$ and differentiable on $(a,b)$ , and $$\frac{d}{dx}g(x) = f(x)$$ Second part says that: If $f$ is continuous on $[a,b]$ , then $$\int_a^b f(x) dx = F(b) - F(a)$$ where $F$ is any antiderivative of $f$ , that is, a function $F$ such that $$\frac{d}{dx}F(x) = f(x)$$ So the part I don't understand is why in the first equation we have integral of the function expressed as a SINGLE antiderivative, when in the second equation we have a difference of antiderivatives? What is the connection between $F(x)$ and $g(x)$ ?","['integration', 'calculus', 'derivatives']"
4595910,Japanese Temple Geometry Problem: Radii of inner circles inside quarter arcs,I was able to get the equation for the radius of larger circle but couldn't think for the smaller one. Source: wu riddles,"['sangaku', 'circles', 'geometry']"
4595914,mass of a body with a density function [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Let $V$ be the body confined by the following quadratic surfaces: $$
x^{2} + y^{2} = 12z,\quad x^{2} = 3z,\quad z = 3
$$ Compute the mass of $V$ with the density function $\rho = 1$ . My problem is I can't figure the integration boundaries $\ldots$ I know that substituting to spherical or cylindrical coordinates won't help because it says so in the question.","['multivariable-calculus', 'density-function']"
4595930,Equivalent definitions for conformal map,"I've encountered several definitions for conformal maps, and I was wondering whether they are equivalent or not. My goal is understanding how the concepts of conformality and holomorphy relate to each other. One of the definitions says that conformal maps are the ones which preserve angles at a certain point (both orientation and size). Thus, for a map to be conformal in an open set U, it must be conformal at each point in U. On the other hand, a conformal map can be defined as a holomorphic function with non-vanishing derivative at all points in some open set U. Under the second definition, it is easy to prove that conformality and holomorphy are equivalent. Under the first definition, it is clear that holomorphy still implies conformality, but it isn't as obvious that conformality implies holomorphy. Could anyone shed some light on this topic? Thanks in advance.","['complex-analysis', 'conformal-geometry']"
4596029,"If $f(x,y)$ is concave such that $f_1 < 0, f_2 > 0$, how are the level curves supposed to look like?","Suppose $f(x,y)$ is a concave function such that $\frac{\partial f}{\partial x} < 0$ and $\frac{\partial f}{\partial y} > 0$ . How are the level curves supposed to look like? Can I get an example of such a concave function? Just to be clear, this isn't a homework question. I am learning multivariable calculus and I am stuck with convex and concave functions, especially visualizing the various types of such functions. My attempt: To answer (1) , I have drawn two possible images below. The curves are essentially $L_c = \{(x,y) : f(x,y) = c\}$ or the level sets of $f$ . In each figure, the arrow represents the direction in which the level curves are attaining higher values (of $c$ ). $\hskip2in$ We know that $tf(x) + (1-t)f(y) \leq f(tx + (1-t)y)$ $(t \in (0,1))$ for a concave function. If we pick two points $p$ and $q$ on a level curve, then it's easy to notice that $f(x(r), y(r)) > \max\{f(x(p), y(p)), f(x(q), y(q))\}$ . This is only true for the first image and not the second image. Is there a better way than this, just for visualization? $\hskip2in$ I don't have an answer to (2).","['multivariable-calculus', 'convex-analysis']"
4596090,Ratio of largest to smallest distance in a set of six points is $\ge \sqrt 3$?,"Here is problem A1 of 25th Putnam 1964. Let $A_1, A_2, A_3, A_4, A_5, A_6$ be distinct points in the plane. Let $D$ be the longest distance between any pair, and $d$ the shortest distance. Show that $D/d ≥ \sqrt3$ . The following solution is taken from this page out of John Scholes's collection of math problems and solutions. Let ABC be a triangle with sides $a = BC ≥ b = CA ≥ c = AB$ . Suppose that angle $A ≥ 2π/3$ . It follows immediately from the cosine formula we have $a^2 = b^2 + c^2 - 2bc \cos A ≥ b^2 + c^2 + bc ≥ 3c^2$ , and hence that $a/c ≥ \sqrt 3$ . So it is sufficient to find a triangle with an angle of at least $2π/3$ . If the 6 points form a convex hexagon, then the six angles of the hexagon sum to 4π so at least one is at least 2π/3. If not then one point is inside the convex hull of the others. Draw diagonals to triangulate the hull. Then the inside point must lie inside (or on) one of the triangles. But if P lies inside (or on) the triangle ABC, then at least one of the angles APB, BPC, CPA is at least 2π/3. I don't understand how this proof shows that $D/d \ge \sqrt 3$ .  This proof seems to just show that a triangle with angle greater than $2\pi/3$ exists in the triangulation of the hull of the set.  How does this show that $D/d \ge \sqrt 3$ ?  The two points $\{d_1, d_2\}$ with distance $d$ might not have any point in common with points $\{D_1, D_2\}$ that have distance $D$ . What am I missing here?","['contest-math', 'convex-hulls', 'triangulation', 'geometry', 'trigonometry']"
4596119,"For independent $X_i \sim Exp(\lambda_i)$, why are $\min\{X_1, ..., X_n\}$ and $[\max\{X_1, ..., X_n\} - \min\{X_1, ..., X_n\}]$ independent?","As someone who is trying to pick probability and statistics back up after not using it for the last 4 years, I'd like to ask for some help with a question I encountered on a MIT OCW midterm test (see below for links). I have been given independent exponential random variables $X \sim Exp(\lambda_1 = 1)$ , $Y \sim Exp(\lambda_2 = 2)$ , $Z \sim Exp(\lambda_3 = 3)$ with $M = \max\{X, Y, Z\}$ and $L = \min\{X, Y, Z\}$ , and told that $L$ and $M - L$ are independent. The only line referring to this in the partial solutions (see link below) states that the ""Idea is to argue first that $\min\{X,Y,Z\}$ and $\max\{X,Y,Z\}-\min\{X,Y,Z\}$ are independent"" but I would like to know how someone would argue or show this. I've considered an approach where I try to show the joint distribution is the product of the marginal distributions but I'm not confident my working is correct. The promptness of the solution description makes me think there must be a different way to approach or think about this problem and having looked around, I've noticed there are a few threads on the distribution of $M-L$ if you have 2 i.i.d. exponential RVs but not for multiple exponential RVs with different $\lambda_i$ . My thoughts going into this: If RVs are independent, then their joint distribution is the product of their marginal distributions: $f(x_1,...,x_n) = \Pi_{i=1}^{n}f_i(x_i)$ (and similarly with CDFs). Additionally, covariance = 0. If I can show any of these, then I can verify the statement. Exponential RVs can be interpreted as ""time elapsed before an event occurs"" and have the memoryless property: $P(T > t_1 + t_2 | T > t_1) = P(T > t_2)$ . Is it worth considering the sum of these elapsed times as ways to represent $M$ and $L$ ? $L\sim Exp(\lambda = \lambda_1 + \lambda_2 + \lambda_3)$ but $M$ is not an Exponential RV. I would also have to consider the different possible orders of $X,Y,Z$ when summing the times up to be equivalent to $M$ . You can derive a joint distribution from a conditional and marginal distribution, $f(x, y) = f_{X|Y}(x)f_Y(y)$ , and I can derive $f_{M-L}(a) = \int f_M(m)f_L(m - a)dm$ . Information calculated: $F_M(a) = P(\max\{X,Y,Z\} \leq a) = (1- e^{-\lambda_1 a})(1- e^{-\lambda_2 a})(1- e^{-\lambda_3 a})$ $f_M(m) = \frac{d}{da}F_M(a)$ $F_L(a) = 1 - P(\min\{X,Y,Z\} \geq a) = 1 - e^{-(\lambda_1 + \lambda_2 + \lambda_3)a}$ $f_L(l) = \lambda e^{-\lambda l}$ where $\lambda = \lambda_1 + \lambda_2 + \lambda_3$ What I attempted: I considered trying to verify independence by showing $f(x_1,...,x_n) = \Pi_{i=1}^{n}f_i(x_i)$ . I think I can compute $f_{M-L}(a) = \int f_M(m)f_L(m-a) dm$ or $F_{M-L}(a)$ . I'm not given the joint distribution but I think I could try to aim for it using conditional probability relation: $$f_{M-L|L=l}(a) = \frac{f_{M-L,L}(a, l)}{f_L(l)}$$ If I consider $F_{M-L}(a) = P(M-L \leq a)$ , then I also have that $$F_{M-L|L}(a) = P(M-L \leq a | L = l)$$ Where I think $$P(M-L \leq a | L = l) = P(M \leq a + L | L = l)$$ $$\int_0^a f_{M-L|L}(r)dr = \int_0^{a+l} f_{M | L} (m) dm$$ And if I let $m = \frac{a+l}{a}r$ : $$\int_0^{a} f_{M-L|L}(r)dr = \int_0^{a} f_{M | L} \bigg(\frac{a+l}{a}r\bigg) \frac{a+l}{a}dr$$ $$\implies f_{M-L|L}(r) = f_{M | L} \bigg(\frac{a+l}{a}r\bigg) \frac{a+l}{a} = \frac{f_{M-L,L}(r,l)}{f_L(l)}$$ And so if $f_{M | L} \bigg(\frac{a+l}{a}r\bigg) \frac{a+l}{a} = f_{M-L}(r)$ then $M-L$ and $L$ are independent. I'm uncertain about this because now I have introduced some arbitrary constant $a$ and I'm not certain how to proceed regarding calculating $f_{M|L}(m)$ since I haven't shown if $M$ and $L$ are independent of each other so I don't want to say $f_{M, L}(m,l) = f_M(m)f_L(l)\}$ . I'd be grateful for any insight into how to approach determining if $M-L$ and $L$ are independent as well as any pointers as to whether my approach is correct and if I have any errors or misconceptions. Thank you very much for your help! MIT OCW course followed: https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/ Query based off question 5(e) from: https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/c7f788511ef3812ac7b3d60af57032ae_MIT18_440S14_prctcmdtrm2.pdf Provided partial solution: https://ocw.mit.edu/courses/18-440-probability-and-random-variables-spring-2014/128cda4cdf29684293f78b56d0a08627_MIT18_440S14_prctcmdtrm2sl.pdf","['independence', 'exponential-distribution', 'order-statistics', 'probability-theory', 'probability']"
4596172,"Confusion on Vakil's universal property of pullback, Vakil 16.3.A","I'm having some trouble wrapping up the proof for 16.3.A in Vakil's FOAG, which basically asks to show that the tensor product construction of $\pi^{\ast} \mathscr{G}$ (for $\pi : X \to Y$ , $\mathscr{G}$ quasicoherent on $Y$ ) satisfies the following universal property:
for any $\mathscr{O}_{X}$ module $\mathscr{F}$ ( not necessearily quasicoherent ) there is a (functorial) bijection: $$
\operatorname{Hom}_{\mathscr{O}_X}(\pi^{\ast} \mathscr{G}, \mathscr{F})
$$ $$
\updownarrow
$$ $$
\operatorname{Hom}_{ \mathscr{O}_Y }( \mathscr{G} , \pi {\ast} \mathscr{F} )
$$ In fact question 16.3.A simplifies this by assuming that $Y = \operatorname{Spec} B$ is affine. If I fix an affine in the preimage, say $\pi (\operatorname{Spec} A) \subset \operatorname{Spec} B$ , general intuition from the prior sections (and the questions hint) tell me to use the adjointness of $(\cdot \otimes_B A, \cdot_B )$ on the level of modules / rings. By Exercise 2.5.C in Vakil's text (or Theorem 13.3.2, whichever you prefer), we may work on the level of a distinguished affine base in order to work our way down to this case of adjointness. Let $ \mathscr{G} = \widetilde{N} $ so that $ \pi^\ast \mathscr{G} \vert_{\operatorname{Spec} A} = \widetilde{ N \otimes_B A} $ and fix some $\mathscr{O}_X$ -linear $\phi : \pi^\ast \mathscr{G} \to \mathscr{F}$ ; we should have on each distinguished open affine $D(f) \subset \operatorname{Spec} A$ the data of an $A_f$ -linear map $$
\phi(D(f)) : N \otimes_B A_f \to \mathscr{F}(D(f))
$$ My thought was to use adjointness of $\cdot \otimes_B A_f$ here, except in this case the functor $\cdot_B$ arises from the composition of ring maps $B \to A \to A_f$ . Then we would get a unique map $ \psi : N \to \mathscr{F}(D(f))_B $ ; however, the issue is that this doesnt really carry local data well: if we have our corresponding morphism of local rings sends some $g \in B$ to $f \in A$ , I can imagine we would somehow want adjointness to give a map $$
\psi(D(g)) : N_g \to \mathscr{F}( \pi^{-1}(D(g)) )
$$ which would then glue (using Exercise 2.5.C / Theorem 13.3.2 again) to get a morphism of sheaves. In fact, it's not entirely clear to me how to play with $\mathscr{F}$ here since it is not quasicoherent; exercise 16.2.A only tells us how to associate $\pi_\ast \widetilde{M}$ and $\widetilde{M}_B$ , but nothing from §16.2 tells us what to do in the case of more general $\mathscr{O}_X$ -modules. Any help would be appreciated. (Also for any admin I cant seem to get that functorial bijection above to render correctly into one MathJax cell — the last term is meant to read $\pi_\ast \mathscr{F}$ but I suspect certain underscores are being rendered as markdown).",['algebraic-geometry']
4596177,Full version of Marc Nieper-Wißkirchen's lecture notes,"I recently have discoverd by this answer Marc Nieper-Wißkirchen's unpublished lecture notes which is with respect to algebraic geometry and written in view of category theory. I think I would like to read this lecture, and so I have two questions: Is there a full version of this lecture notes? In this lecture notes there are no after from chapter of cohomology. Is there a English version? If no, I will read it as I translate it, but I don't understand German at all. Thanks in advance!","['algebraic-geometry', 'schemes', 'category-theory', 'reference-request']"
4596200,How should I interpret these integrals from Griffiths 'Intro to Electrodynamics'?,"The book defines the electric field at a point $P$ a distance $r$ due to a point charge $q$ as : $$ E = \frac{1}{4\pi \epsilon _0} \frac{q}{r^2}$$ it then tells us that the electric field at a point $P$ due to $n$ point charges $q_i$ is  : $$ E = \sum^n_{i=1} \frac{1}{4\pi \epsilon _0} \frac{q}{r_i^2}  $$ where $r_i$ is the distance between $q_i$ and $P$ . As in the electric field follows the superposition principle. It then tells us that when we have instead of discrete point charges, have a continuous charge (either a line charge, a surface charge, or volume charge) then the electric fields given for the respective situations are given as : $$ E = \frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\lambda(r')}{r^2}dl' $$ $$ E = \frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\sigma(r')}{r^2}da' $$ $$ E = \frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\sigma(r')}{r^2}d \tau' $$ Where for a line charge : $dq= \lambda dl' $ , for a surface charge : $dq = \sigma da'$ for a volume charge : $dq = \rho d\tau'$ Here's where my question comes in, I am having a great trouble understanding how to 'interpret' these integrals. Firstly, I tried viewing these integrals mathematically. For example for the surface charge integral (eq 2.) I tried to express it as the Electric field at a point $P$ due to a charged rectangle in cartesian coordinates of length $a$ and width $b$ : $$ E = \frac{1}{4\pi \epsilon _0} \int_{area}^{}  \frac{\sigma(r')}{r^2}da' = \frac{1}{4\pi \epsilon _0} \int_{area}^{}  \frac{\sigma(r')}{r^2}dx'dy' = \frac{1}{4\pi \epsilon _0}(\int_{0}^{a} (\int_{0}^{b}  \frac{\sigma(r')}{(\sqrt{(x_1 - x)^2 + (y_1 - y)^2)}^2}dx')dy') $$ Where $r$ is the distance of each point in the rectangle to the point $P = (x_1,y_1)$ from the arbitrary points to integrate over $(x,y)$ ,  (assuming the charged rectangle and $P$ are on the same plane here. I made a rough paint to illustrate I tried to interpret the last expression on the RHS as the volume above a rectangle of length $a$ and width $b$ , with a height of function $\frac{1}{4\pi \epsilon _0} \frac{\sigma(r')}{r^2}$ . Where the 'height' of each point above this rectangle, is the magnitude of the contribution for a point charge located at that point. Kind of like the height above each point is $dE$ at $P$ , and adding them all together gives us $E$ at $P$ . I made another rough paint . But we're talking about 'surface charges' not point charges. So this interpretation shouldn't hold for a line charge, because it uses a different variable for some reason ( $\lambda$ instead of $\sigma$ ). But I don't see why you can't similarly just interpret equation 1. as the area above a line with height $\frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\lambda(r')}{r^2}dl'$ , similar to how one interprets definite integrals in Integral Calculus. Which leads to my second question Why are we using different terms for the integration variable? At first I thought it was so you get the correct units but I'm not so sure how to justify that intuition. How does the integral 'know' I'm talking about areas when I say $da'$ and lines or volumes when I say $dl'$ and $d\tau '$ ? The easiest way I've found to think of all this is to think of the function under the integral 'as a whole'. As in to think of equation 2. As taking an infinitesmal area of the charged rectangle (surface element $da'$ ) and multiplying it by the charge density ( $\sigma(r')$ ), which would give you the total charge on the surface element ( $dq$ ). Then picking a point on this infinitesmal surface area and measuring the distance from this point to $P$ (this distance would be $r$ ). Then multiplying this entire expression by $\frac{1}{4 \pi \epsilon _0}$ . Which gives the expression $$ dE = \frac{1}{4\pi \epsilon _0} \frac{\sigma(r')}{r^2}da' = \frac{1}{4\pi \epsilon _0} \frac{dq}{r^2}$$ as the electric field due to a singular surface element a distance $r$ away from a point $P$ . And then when you take the integral of this above expression, since integrals represent infinite sums, the above integrated would represent adding up the contribution of all these surface elements, to give the total electric field of the entire surface. And this makes a lot of sense to me, however it is not actually how integration works to my knowledge.
What I've described is multiplying the area of a bunch of squares ( $da$ ) by a factor dependent on this squares position and charge density ( $\frac{1}{4 \pi \epsilon _0} \frac{\sigma(r')}{r^2})$ . Which gives an associated $dE$ with each square, and adding them (integrating) to get an $E$ associated with the whole rectangle. To my knowledge integrating the above expression over a rectangle for example, would be to integrate the contribution of a line on this rectangle, with its height determined as $\frac{1}{4\pi \epsilon _0} \frac{dq}{r^2}$ , then sweeping it along the rectangle. My calculus professor made a demo of what I mean here . Sorry if this question is extremely long or difficult to follow. Part of my question is me not really understanding what I'm missing about how to interpret these integrals. I suspect I also have some fundamental flaw in my understanding of integrals that I'm missing. Any answers appreciated.","['integration', 'physics', 'multivariable-calculus']"
4596264,Propositional Equivalence-Discrete Mathematics,"$$ p ∧ ((¬p ∨ q) ∧ (¬q ∨ p))$$ is it a contradiction?
I have to prove that the statement is a contradiction.But my answer is $p$ . I couldn't find out   my mistake.","['equivalence-relations', 'propositional-calculus', 'discrete-mathematics']"
4596319,Compatibility of flow and inverse flow of a non-autonomous ODE,"Suppose we are given a function $f:\mathbb{R}^d\times [0,T]\to\mathbb{R}^d$ , s.t. the ODE \begin{align}
\dot{y}(t)&= f(y(t),t), \quad t\in[0,T]\newline
      y(0)&= x
\end{align} is well-posed, for any $x\in\mathbb{R}^d$ (e.g. $f$ is $\mathcal{C}^1$ with bounded derivative). Then we can define the flow of this ODE to be the map $[0,T]\times\mathbb{R}^d\ni(t,x)\mapsto \phi_t(x)\in\mathbb{R}^d$ . Moreover we can define the ODE reversed in time: \begin{align}
\dot{z}(s)&= -f(z(s),T-s), \quad s\in[0,T]\newline
      z(0)&= \xi
\end{align} for any $y\in\mathbb{R}^d$ and a corresponding reverse flow $[0,T]\times\mathbb{R}^d\ni(s,x)\mapsto \psi_s(\xi)\in\mathbb{R}^d$ . I can manage to prove the identity \begin{equation}
\psi_s(\phi_t(x)) = \phi_{t-s}(x)
\end{equation} for any $0\leq s\leq t \leq T$ . Under what circumstances does $\phi_t(\psi_s(x)) = \phi_{t-s}(x)$ also hold? I am having trouble proving this (I tried showing, that both sides satisfy the same ODE, so I can conclude with uniqueness, but there I run into well-definedness issues). What I can prove however is \begin{equation}
\phi_t(\psi_s(x)) = \psi_{s-t}(x)
\end{equation} for any $0\leq t\leq s \leq T$ . Heuristically speaking it shouldn't matter, if I move backwards first and then forwards or vice versa. I read other posts on this website about it, and it seems to have something to do with whether the initial ODE is autonomous or not, but I just can't seem to wrap my head around it. Can someone help me?","['ordinary-differential-equations', 'real-analysis']"
4596326,Show $\prod\limits_{k=1}^{+\infty}\coth\left({\frac{\pi k}{2}}\right)=\left({\frac{2}{\pi}}\right)^{\frac{1}{4}}\Gamma\left({\frac{3}{4}}\right)$ [duplicate],"This question already has answers here : series involving $\log \left(\tanh\frac{\pi k}{2} \right)$ (2 answers) Closed 1 year ago . Prove that $$\prod_{k=1}^{+\infty}\coth\left({\frac{\pi k}{2}}\right)=\left({\frac{2}{\pi}}\right)^{\frac{1}{4}}\Gamma\left({\frac{3}{4}}\right).$$ The best I was able to get: $$\ln(P)=\sum_{k=1}^{\infty}{\frac{1}{k-1/2}}\cdot{\frac{1}{e^{2(k-1/2)\pi}-1}},$$ where $P$ denotes the product. I'm not sure what to do next. Maybe it makes sense to try $${\frac{1}{a}}=\int_{0}^{1}t^{a-1}dt=\int_{0}^{\infty}e^{-a t}dt$$ or something else.","['real-analysis', 'gamma-function', 'calculus', 'sequences-and-series', 'infinite-product']"
4596347,Solve $\lim_{x\to0^+} \frac{(1+\sin x)^{\frac1x} - \exp(-\frac x2)}{(\tan x )^\alpha}$,Is this limit solved correctly? $$\lim_{x\to0^+} \frac{(1+\sin x)^{\frac1x} - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\ln(1+\sin x)\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\ln(1+x+o(x^2)\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\left(\left[x+o(x^2)\right]-\frac12\left[x+o(x^2)\right]^2 + o(x^2)\right)\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\left[x-\frac{x^2}{2} + o(x^2)\right]\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(1-\frac{x}{2} + o(x)\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp(1)\cdot\exp\left(-\frac{x}{2}\right)\cdot\exp(o(x)) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(-\frac{x}{2}\right)\cdot\left(e\cdot\exp(o(x)) - 1\right)}{x^\alpha} = \frac{1\cdot(e\cdot1 -1)}{\lim_{x\to0^+}x^\alpha} = \begin{cases}\frac{e-1}{0^+} = +\infty && \ \text{if   } \alpha >0\\\frac{e-1}{1} = e-1 &&\ \text{if } \alpha =0\\\frac{e-1}{+\infty} = 0 &&\ \text{if } \alpha <0\end{cases}$$ EDIT: A first-order approximation is enough: $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\ln(1+x+o(x^2)\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(\frac1x\left[x+o(x^2)+o(x)\right]\right) - \exp(-\frac x2)}{(\tan x )^\alpha}$$ $$\lim_{x\to0^+} \frac{\exp\left(1+o(1)\right) - \exp(-\frac x2)}{(\tan x )^\alpha} = \frac{e-1}{\lim_{x\to0^+}x^\alpha} = \begin{cases}\frac{e-1}{0^+} = +\infty && \ \text{if   } \alpha >0\\\frac{e-1}{1} = e-1 &&\ \text{if } \alpha =0\\\frac{e-1}{+\infty} = 0 &&\ \text{if } \alpha <0\end{cases}$$,"['limits', 'taylor-expansion']"
4596368,Understanding how a ring $R$ can be viewed as a ring of functions on $\operatorname{Spec} R$ using polynomial rings as an example,"I've read that a ring $R$ is often viewed as a ring of functions on its spectrum $\operatorname{Spec} R$ in the following way: $f\in R$ is the function in question, $x\in \operatorname{Spec}R$ an element it's supposed to map. Then $f(x)$ is the element of $R/x$ such that the quotient map $R\to R/x$ maps $f\mapsto f(x)$ . I would like to understand this using an example where it's ""obvious"" how the elements of our ring should act as functions: Polynomial rings. But I found some obstructions which I'd like to see clarified. First off, I think it's very natural to consider polynomials in $R[X]$ as functions on $R$ by way of the evaluation homomorphism. So I will always come back to this idea. So let's look at a few examples: The complex numbers: $\mathbb C[X]$ ""should"" contain functions acting on $\mathbb C$ . So $\operatorname{Spec}\mathbb C[X]$ should contain $\mathbb C$ . And in a way it does: The prime ideals of $\mathbb C[X]$ are $(0),\mathbb C[X]$ and $(X-z)$ , where $z\in\mathbb C$ . And the ideals $(X-z)$ correspond to the elements of $\mathbb C$ . Also, $\mathbb C/(X-z)\cong\mathbb C$ via the homomorphism evaluating at $z$ , and in this way, $f\in\mathbb C[X]$ is actually sent to $f(z)$ by the quotient map $\mathbb C[X]\to\mathbb C[X]/(X-z)$ , so it all checks out! However, what do the remaining elements of the spectrum, $(0)$ and $\mathbb C[X]$ , do here? What do they correspond to, and how can I think of a polynomial as acting on them naturally? The reals: Polynomials in $\mathbb R[X]$ should naively be functions on $\mathbb R$ . However, the spectrum now contains more elements to act on. The prime ideals are now the trivial ideals, $(X-r), r\in\mathbb R$ , and $((X-z)(X-\bar z)),z\in\mathbb C\backslash\mathbb R$ . The polynomials can still be thought of as being maps on $\mathbb R$ , where $\mathbb R$ can be included in the spectrum by identifying $r$ with the ideal $(X-r)$ . And the additional elements can be thought of as complex numbers to act on, since $\mathbb R[X]/((X-z)(X-\bar z))\cong\mathbb R[z]=\mathbb C$ via the homomorphism evaluating at $z$ , and the abstract construction from the beginning actually maps $z$ to $f(z)$ . However, there is no way to distinguish between $z$ and $\bar z$ here. So the spectrum doesn't contain all the complex elements on which a polynomial could act, only up to complex conjugation. Why not include the whole ring? Also, the ""superfluous"" elements are still there.","['algebraic-geometry', 'maximal-and-prime-ideals', 'commutative-algebra']"
4596387,"Proving $\prod_{j=1}^n(1-\prod_{i=1}^m\sin^2x_{ij})+\prod_{i=1}^m(1-\prod_{j=1}^n\cos^2x_{ij})\geq1$, for real numbers $x_{ij}$","I have been struggling to solve the following problem which seems to be some kind of generalized trigonometric Pythagorean identity: Let $x_{ij}$ $(1 \leq i \leq m$ , $1\leq j \leq n)$ be real numbers, then prove that: $$\prod_{j=1}^{n}\left(1-\prod_{i=1}^{m}\sin^2(x_{ij})\right) + \prod_{i=1}^{m}\left(1-\prod_{j=1}^{n}\cos^2(x_{ij})\right) \geq 1$$ I have tried the following induction: Prove for $n=1$ and any $m$ and then do an induction on $n$ . The base case is immediate using $1-\cos^2(x_{ij}) = \sin^2(x_{ij})$ . However I have not managed to pull off the inductive step. If anyone has a contribution it would be much apreciated.","['products', 'trigonometry', 'inequality']"
4596465,"Does every positive, decreasing, real sequence whose series converges have a corresponding convex sequence greater than it whose series converges?","Definition: A real sequence $\ (x_n)_n\ $ is convex if $\ x_n - x_{n+1} \geq x_{n+1} - x_{n+2}\quad \forall\ n\in\mathbb{N}. $ Suppose $\ (x_n)_n\ $ is a positive, decreasing sequence of real numbers such that $\ \displaystyle\sum x_n\ $ converges. Proposition: There exists a convex sequence $\ (y_n)_n,\ $ such that: $\ y_n\geq x_n\quad \forall\ n\in\mathbb{N},\ $ and $\ \displaystyle \sum_n y_n\ $ converges. I suspect there is some counter-example, and I think my best attempt to find one is that maybe there is an increasing subsequence $\ (A_n)_n\ $ of $\ \mathbb{N}\ $ such that $\ x_n:= \frac{1}{A_k}\ $ for all $\ n\ $ with $\ A_{k-1} < n \leq A_k,\ $ and then maybe the convexity of $\ (y_n)_n\ $ sort of forces $\ y_n \approx \frac{1}{n}\ ?$ But I'm not sure if this is true or how to prove this. Edit: I have spent more time thinking about this problem without further progress towards a solution. Therefore I have added a bounty of $+50$ .","['examples-counterexamples', 'real-analysis', 'sequences-and-series', 'recreational-mathematics', 'problem-solving']"
4596498,The determinant of a certain square matrix.,"Let $n > 1$ be an odd number. Let $A$ be an $n \times n$ matrix defined as follows \begin{equation}
\label{wams} a_{i, j} = \begin{cases} 1, & \text{for}\ i - j  \equiv \pm 2 \pmod n\\ 2, & \text{for}\ i = j\\ 0, & \text{otherwise}.\end{cases}
\end{equation} Calculate the determinant of matrix $A$ . Could someone please give me a hint for this question? I am completely green. I have tried at my best level, and still am not able to come up with a solution.","['matrices', 'toeplitz-matrices', 'determinant', 'linear-algebra']"
4596516,Prove $P(A \cup B) \leq P(A) + P(B)$,"The axioms of probability are: $$\text{(i) }0\leq P(A) \leq1 \text{ for each event $A$}\\ \text{(ii) }P(\Omega)=1\text{ and }P(\varnothing)=0\\
\text{(iii) If $A$ and $B$ are disjoint, then }P(A\cup B)=P(A)+P(B)
\\
\text{(iv) If }A_1, A_2, A_3,... \text{is a sequence of pairwise disjoint events then}\\
P\left( \bigcup_{i=1}^{\infty}A_i\right) = \sum_{i=1}^{\infty} P(A_i)$$ I'm supposed to use the fact that $$A \cup B =(A \cap B^c)+(A\cap B)+(A^c\cap B).$$ Thus, we have $$P(A \cup B) =P(A \cap B^c)+P(A\cap B)+P(A^c\cap B)$$ Not too sure where to go from here.","['elementary-set-theory', 'probability']"
4596522,Envelope of x-t graph in Damped harmonic oscillations,"In our lecture, we learnt that the $x-t$ graph of an underdamped harmonic oscillator is basically a sinusoidal curve with a fixed frequency, bounded by an envelope which is an exponentially decaying curve, like this: Now, we learned to solve the damped ODE $\ddot x+2\beta \dot x+\omega_0^2x=0$ to get the general non-trivial solution $x(t)=e^{-\beta t}\bigg(c_1\exp(i\omega t)+c_2\exp(-i\omega t)\bigg)$ where $\omega=\sqrt{\omega_0^2-\beta^2}$ . The envelope is supposed to be $y=e^{-\beta t}$ . The problem is : I want to prove that all the extremas of $y=\exp(-ax)\sin(bx)$ lie on the envelope $y=\exp(-ax)$ and its mirror image about the x-axis. So, first, for getting the extremas, $\dot y=0$ which gives us $$\exp(-ax)\bigg(-a\sin bx+b\cos bx\bigg)=0$$ or $$\tan bx =\dfrac ba$$ . Now suppose that $x=x_0$ satisfies the last condition. Then how do we show that the point $\bigg(x_0, \exp(-ax_0)\sin(bx_0) \bigg)$ must lie on the curve $y=\pm \exp(-ax)$ , or $y^2-\exp(-2ax)=0$ ?",['ordinary-differential-equations']
4596545,Finding the original ODE using a solution,"Does there exist a linear-homogeneous ODE with constant coefficients so that $$y(x) = x \cos^2(x)-\sin(x)$$ is its solution? If it does exist, how can I know what the original ODE is
or how can I say there's no such ODE? I've tried changing $y(x)$ to $$y(x) =x \, (\cos(2x)+1)-\sin(x) \\
y(x) = x \, \cos(2x)+x-\sin(x).$$ Now I've figured few things: that if $y(x) =x \, \cos(2x)+x-\sin(x)$ is a solution
then $y(x) = x + \cos(2x) - \sin(x)$ is also a solution because the solution consist of $\cos$ and $\sin$ then the roots of the ODE must consist of a root with the form $k=a+bi$ and $k=a-bi$ . Other than that I'm pretty stuck with how I should move forward from here. Edit:
Correct answer after confirming with WolfarmAlpha",['ordinary-differential-equations']
4596579,Convergence of a confidence interval for the variance,"It's known that a confidence interval for the variance with $1-\alpha$ confidence is as it follows $$\sigma^2\in\Biggl(\frac{(n-1)S_{n}^{2}}{\chi^{2}_{n-1,\alpha/2}},\frac{(n-1)S_{n}^{2}}{\chi^{2}_{n-1,1-\alpha/2}}\Bigg)=(A_{n},B_{n}). $$ I'm asked to compute $$\lim_{n \to \infty}P\Bigl(\sigma^2\in(A_{n},B_{n})\Bigr),$$ taking into account that $X_{1},\dots,X_{n},\dots,$ are i.i.d. (but no necessarily normal) and $\mathbb{E}(X_{1}^{4})<\infty$ . I know that $\sqrt{n} (S_n^2 - \sigma^2) \rightarrow_{d} \text{N}(0, \sigma^4 (\kappa - 1)),$ where $\kappa=\mu_{4}/\sigma^{4}$ and $\mu_4 = E(X_i -\mu)^4$ , but I have no clue in how to continue. Could you give me some ideas?","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4596586,Solving a nonlinear ODE,"$$
 y''-i(\sin(x)y)'-i\omega y-\lvert y \rvert^2y'=0 , \quad y\rvert_{x=0}=0 \quad y'\rvert_{x=0}=0
$$ Hello, im looking for advice on how to solve this equation, im intrested in knowing what possible values of $\omega$ could be. Ive tried to get a solution by using the WKBJ method by assuming that $y$ is \begin{equation}
y=\psi(x)\exp(iS(z))
\end{equation} And then subbing this solution into the above equation and then assuming that both $\psi$ and $S$ are real then having seperating that equation into real and imaginary parts but then I have no idea what to do with the nonlinear $y$ term. Is there any methods or ways anyone could suggest on how to approach this problem? I also tried to find a solution to this equation by assuming that $y$ is real and splitting the problem into real and imaginary part. The imaginary part of that equation is separable and gives a solution but how then do i use the real part? Thank you. Update: So i tried another way solving this equation i haven't tried implementing the boundary conditions yet but does this seem like a reasonable approach? $$
 y''-i(\sin(x)y)'-i\omega y-\lvert y \rvert^2y'=0 , \quad y\rvert_{x=0}=0 \quad y'\rvert_{x=0}=0
$$ I then split this equation in to real and imaginary parts: $$
\textrm{real: } y'' - \lvert y\rvert^2 y' = 0\\
\textrm{Imaginary: } \omega y - (sin(x)y)' = 0 
$$ The imaginary part can be solved to get $$
y' = \dfrac{\omega - cos(x)}{sin(x)}y \\
$$ which directly integrates to $$
\lvert y \rvert = \exp\left(\int\dfrac{\omega - cos(x)}{sinx(x)} \right)
$$ I can rearrange the real part and integrate it to find that $$
\lvert y' \rvert = \exp\left(\int \lvert y\rvert^2 dx\right)
$$ This integral $$
\int \lvert y\rvert^2 dx = \dfrac{2sin(x)}{\omega - cos(x)}\exp\left(2\int\dfrac{\omega - cos(x)}{sin(x)}dx\right)
$$ combining all these together i get that $$
\lvert y \rvert = \left| \dfrac{sinx(x)}{\omega - cos(x)}\right|\exp\left(-\dfrac{2sin(x)}{\omega - cos(x)}\exp\left(2\int \dfrac{\omega-cos(x)}{sin(x)} dx\right)\right) 
$$ The solution i found looks really messy so i cant help but feeling like ive done some bad maths somehow does this seem reasonable? Thanks to Gribouillis for the suggestion, this equation originally had a $dy/dt$ term but im looking for a steady state solution, numerical solutions found from timestepping were oscilatory so thats where the $\omega$ was coming from, i was hoping there was some sort of expression i could get from looking at the steady state. Thanks again to anyone who can help.","['mathematical-physics', 'ordinary-differential-equations', 'eigenfunctions']"
4596589,How do we solve this rather simple ODE (Loewner equation with driving function $\sqrt t$)?,"Remember the following result for the Loewner equation : If $\lambda:[0,\infty)\to\mathbb R$ is continuous, then for all $z\in\mathbb C\setminus\{\lambda(0)\}$ there is a uniqe $\zeta(z)\in(0,\infty]$ and a unique continuous $g(\;\cdot\;,z):[0,\zeta(t))\to\mathbb C$ with $$g(t,z)\ne\lambda(t)\tag1$$ and $$g(t,z)=z+\int_0^t\frac2{g(s,z)-\lambda(s)}\:{\rm d}s\tag2$$ for all $t\in[0,\zeta(z))$ . Now assume $$\lambda(t)=2\sqrt{\kappa t}\;\;\;\text{for all }t\ge0$$ for some $\kappa\ge0$ . How can we determine $g$ here and how can we determine $\zeta(z)$ ? Let $y_\pm:=\sqrt\kappa\pm\sqrt{\kappa+4}$ and $$H(w):=\frac{2y_+\ln(w-y_-)-2y_-\ln(w-y_+)}{y_+-y_-}.$$ Using this, I was able to find the relation $$H\left(\frac{g(t,z)}{\sqrt t}\right)=2\ln\frac z{\sqrt t}\tag3,$$ but how do we solve this for $g(t,z)$ ? EDIT : From $(3)$ , I was only able to obtain $$\gamma(t):=g_t^{-1}(\lambda(t))=c\sqrt t,$$ where $$c:=\exp\left(\frac12H(2\sqrt\kappa)\right).$$ If we cannot find an explicit expression for $g$ , can we at least determien $\zeta(z)$ ? My guess is that $\zeta(z)=\infty$ for all $z\in\mathbb C\setminus\{\lambda(0)\}$ , but how do we prove this?","['mathematical-physics', 'conformal-geometry', 'ordinary-differential-equations', 'partial-differential-equations']"
4596656,True or False: $f(x)$ is differentiable at $x=0$ and $\lim_{x \to 0}\frac{f(x)}{x}=3 \Rightarrow \; f(0)=0 $ and $f'(0)=3$?,Prove or contradict: $f(x)$ is differentiable at $x=0$ and $\lim_{x \to 0}\frac{f(x)}{x}=3 \Rightarrow \; f(0)=0 $ and $f'(0)=3$ My answer: True and below the proof: If $f(x)$ is differentiable at $0$ it means $f'(0)$ exists and verifies: $f'(0)=\lim_{x \to 0}\frac{f(x)-f(0)}{x}=\lim_{x \to 0}\frac{f(x)}{x}-\frac{f(0)}{x}$ . As by assumption it is given that $\lim_{x \to 0}\frac{f(x)}{x}$ exists and equal to $3$ and because $f(x)$ is differentiable at $x=0$ hence $f(x)$ is continuous at $x=0$ and so $f(0)$ exits it cames by limit arithmetic that: $f'(0)=\lim_{x \to 0}\frac{f(x)-f(0)}{x}=\lim_{x \to 0}\frac{f(x)}{x}-\frac{f(0)}{x}=\lim_{x \to 0}\frac{f(x)}{x}-\lim_{x \to 0}\frac{f(0)}{x}=3-\lim_{x \to 0}\frac{f(0)}{x}$ Now if $f(0) \neq 0 \Rightarrow \lim_{x \to 0}\frac{f(0)}{x} = +- \infty$ and so in such case $f'(0)$ will not exist. That's why $f(0)=0$ and if $f(0)=0$ it cames naturally that $f'(0)=3$ . Q.E.D Is it correct?,"['limits', 'calculus', 'solution-verification', 'derivatives']"
4596733,3blue1brown and the visual argument that a vector is fundamentally different from a matrix,"After watching the wonderful playlist of Linear Algebra on the YouTube channel 3blue1brown , I realized that vectors and matrices are, fundamentally, different concepts: Vectors are numerical entities in a $n$ -dimensional space. Matrices are how theses numbers are (linearly) transformed. I am restricting it to a Linear Algebra. Of course, for multilinear algebra, matrices (and tensors) are, in fact, numerical entities in the same way that a vector is. Regardless, for Linear Algebra, this argument seems to have a much deeper interpretation that what is commonly said about vectors and matrices : A vector is just a one-column matrix I am not saying that such answer is wrong, once denoting a vector as a column is merely a convention. But doesn't it seem that this argument leaves out the main purpose of using matrices in linear algebra, which is apply linear transformations on vectors though matrix product,i.e., $\mathbf{Ax}$ ?","['matrices', 'linear-algebra', 'vectors', 'linear-transformations']"
4596742,Is there a name for this topology?,"First off, I want to  point out a popular topology for the real line. We know it commonly as the Michael line. It is given by $\mathfrak{T}_M =\{U \cup I: U$ is open under usual topology, $I \subset \mathbb{R} \setminus \mathbb{Q} \}$ . But is there a name for the topology given by $\mathfrak{T} =\{U \cup Q: U$ is open under usual topology, $Q \subset  \mathbb{Q} \}$ ?","['general-topology', 'soft-question']"
4596750,Use Monte Carlo algorithm to approximately compute E(X).,"I'm okay with (a), but I got stuck in trying to solve (b) My idea was to change the integration $$ \int_0^1 \int_{0}^1 x g(x,y) \ dxdy $$ to be the expectation for a function of $X, Y$ Similarly, I want to do the same thing for $$ \int_0^1 \int_{0}^1  g(x,y) \ dxdy $$ also be a random variable relating to $X, Y$ . But I don't know how to do this. I don't know how to let the integration be the expectation of ""something"". To be more specific, for the one-variable integration $$ \int_{0}^\infty e^{-5x-14x^2} \ dx$$ I can manipulate this integration to be $$ \int_{0}^\infty \dfrac{1}{5}e^{-14x^2} \cdot 5e^{-5x} \ dx=\dfrac{1}{5}E(e^{-14X^2})$$ where $X\sim Exponential(5)$ . Now the original integration is changed to be the expectation of a random variable $e^{-14X^2}=h(X)$ which is a function of random variable $X$ with the distribution of $X$ being Exponential(5). I don't know how to do the same thing here(for this double integration), or maybe, this question doesn't need to use Monte Carlo for double integration. Any help on this? Thanks.","['integration', 'monte-carlo', 'probability-theory']"
4596764,Finding the laurent expansion of $\frac{z}{(z-1)(z-2)}$,"I want to  find the Laurent series for $\frac{z}{(z-1)(z-2)}$ in the region $1 < |z| < 2$ . This implies that $\frac{1}{|z|} < 1$ , so noticing that $(z-1) = z(1 - \frac 1 z)$ I can rewrite the desired function as $$\frac{z}{z(1- \frac 1 z)(z-2)} = \frac{1}{z-2} \cdot \frac{1}{1 - \frac 1 z}.$$ Now using the definition of the geometric series I rewrite it as $$\sum_{k=0}^{\infty} \frac{1}{z^k(z-2)}$$ Is this the right Laurent series, and if not, where did I go wrong? Please note I am trying to understand where I made a mistake, not simply finding any solution . I've read a similar question at Finding the Laurent series of $f(z)=1/((z-1)(z-2))$ and one answer uses the fact that $\frac{|z|}2 < 1$ , but I am not sure if my method is also valid.","['complex-analysis', 'laurent-series', 'sequences-and-series']"
4596783,How do we know that a surface integral is the same as an iterated double integral?,"I'm going through my professor's calculus notes for integration of a function $f(x,y)$ over a rectangle in $\mathbb{R}^2$ . It defines partitions of a rectangle $R$ into subrectangles, and then says that the integral of $f(x,y)$ is the supremum of the lower sum (or infimum of upper sum) defined here : So in essence the integral of $f(x,y)$ over $R$ : $$\int^{}_{R} f\,dV$$ is the sum of the volume of these infinitesimal rectangles, he posts another picture to illustrate : Here's my question : I know that the volume underneath the surface described by $f(x,y)$ is computed as an iterated double integral : $\int_{c}^{d} \left(\int_{a}^{b} f(x,y) \,dx\right)\,dy$ ,     (where the rectangle $R$ in $\mathbb{R}^2$ is described as $R = [a,b] \times [c,d]$ ). However from my understanding, what the iterated double integral does is calculate the area underneath an ' $x$ -slice' as a function of $y$ ( as $F(y) = \int_{a}^{b} f(x,y) \,dx$ ) , and then sums up all these areas as $\int_{c}^{d} F(y) \,dy$ . Professor provides animation here for reference . This is not the same as measuring the volume underneath every infinitesmal subrectangle, and adding them up. If we were talking about the volume of a cheese, the above partition integral definition $\int_{R} f\,dV$ amounts to slicing the cheese into a bunch of rectangles along $x$ and $y$ , then measuring the volume of each and adding them.
What an iterated double integral does as I've described it, would be to simply slice the cheese into a bunch of slices along $x$ only, measure the volume of those, and add them together. My question is how do I rigorously know these operations are equivalent? In short, why is the following true? : $$\int_{R} f\,dV = \int_{c}^{d} \left(\int_{a}^{b} f(x,y) \, dx\right) \, dy $$ And why is $dV = dxdy$ ?","['integration', 'multivariable-calculus']"
4596805,To prove $f$ is Injective where $f$ satisfies $f\left(\frac{f(x)}{x}+y\right)=1+f(y)$,"Consider the function $f:\mathbb{R^+} \to \mathbb{R^+}$ such that $f\left(\frac{f(x)}{x}+y\right)=1+f(y)$ . Find all such functions. My try: Plug $x=y=1$ and assume $f(1)=k$ , we get $$f(k+1)=k+1$$ Now assuming $f$ is injective, we have $$\begin{aligned}
f\left(\frac{f(x)}{x}+y\right) & =f\left(\frac{f(y)}{y}+y\right)=1+f(y) \\
\Rightarrow \quad \frac{f(x)}{x} & =\frac{f(y)}{y} \\
\Rightarrow \frac{f(1)}{1}=\frac{f(k+1)}{k+1} \Rightarrow k(k+1)=k+1 \Rightarrow k=1
\end{aligned}$$ Thus $f(x)=x$ . But what is left is to prove $f$ is Injective a Priori.","['functional-equations', 'algebra-precalculus', 'functions']"
4596818,I need help solving this differential delay equation (inverse Laplace transform problem),"Let us consider a differential delay equation (DDE) with $a,b\in\mathbb{R}$ : $$
\frac{d}{dt}y(t)=ay(t)+bH(t-1)y(t-1),~0\le t<\infty,
$$ where $H(t)=\int_{-\infty}^t\delta(t')dt'$ is the Heaviside step function. Let $\hat{y}(s)=\int_0^{\infty}e^{-st}y(t)dt$ be the Laplace transform of $y(t)$ . i). Find $\hat{y}(s)$ and, using the inverse Laplace transform, solve the given DDE. Here is what I have tried so far : $$
\begin{aligned}
\mathcal{L}\left(\frac{d}{dt}y(t)\right)(s)&=\mathcal{L}\left(ay(t)+bH(t-1)y(t-1)\right)(s)\\
s\hat{y}(s)-y(0^{-})&=a\hat{y}(s)+be^{-s}\hat{y}(s),
\end{aligned}
$$ therefore $$
\begin{aligned}
s\hat{y}(s)-a\hat{y}(s)-be^{-s}\hat{y}(s)&=y(0^{-})\\
\hat{y}(s)\left(s-a-be^{-s}\right)&=y(0^{-})\\
\hat{y}(s)&=\frac{y(0^{-})}{s-a-be^{-s}}\\
y(t)&=\mathcal{L}^{-1}\left(\hat{y}(s)\right)=\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}e^{st}\hat{y}(s)ds\\
&=\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}\frac{y(0^{-})e^{st}}{s-a-be^{-s}}ds\\
&=\frac{y(0^{-})}{2\pi i}\int_{c-i\infty}^{c+i\infty}\frac{e^{st}}{s-a-be^{-s}}ds
\end{aligned}
$$ I need help: I don't know how to get the exact value of the inverse Laplace transform of $\hat{y}(s)$","['inverse-laplace', 'ordinary-differential-equations', 'laplace-transform', 'complex-analysis', 'partial-differential-equations']"
4596824,Trying to understand the Kramer-Kronig relations with example $f(t) =\left(1-t^2\right)^4\cdot\theta(1-t^2)$,"Trying to understand the Kramer-Kronig relations with example $f(t) =\left(1-t^2\right)^4\cdot\theta(1-t^2)$ Introduction Let think in the function: $$f(t) =\left(1-t^2\right)^4\cdot\theta(1-t^2)\equiv \left(\frac{1-t^2+|1-t^2|}{2}\right)^4\tag{Eq. 1}\label{Eq. 1}$$ which is a real-valued square-integrable even function, with $\theta(t)$ the unitary step function . At can be seen in Wolfram-Alpha , the Fourier Transform is given by: $$\begin{array}{r c l}
F(w)  & = & \int\limits_{-\infty}^{\infty} f(t)\,e^{-iwt}\,dt \\
& = & \int\limits_{-1}^{1} (1-t^2)^4\,e^{-iwt}\,dt \\ & = & \displaystyle{\frac{768}{w^9}\cdot \Big(5\ w\ (2\ w^2-21)\cos(w)+(w^4-45\ w^2+105) \sin(w)\Big)}  \tag{Eq. 2}\label{Eq. 2} \\
\end{array}$$ which is reviewed in detail in this question since shows an horrid spike when plotted, but it end to be an artifact due current software, since it is continuous at the origin as is show in this answer by @CalvinKhor. As expected, since $f(t)$ is real-valued and an even function, its Fourier transform $F(w)$ is also real-valued and even function (see Wikipedia Table line $111$ ). Main part In the Wikipedia page for the Kramers-Kronig relations is stated that if a function $x(t)$ have an initial time $t_0$ such as $x(t) = 0\ \forall\ t<t_0$ , and without loss of generality using the assumption that $t_0=0$ , then if the Fourier Transform $X(w) \in \mathbb{C}$ could be split as $X(w) = U(w)+iV(w)$ with both $U(w),\ V(w) \in \mathbb{R}$ real-valued, then it is possible to find under some mild-assumptions that: $$\begin{array}{r c l}
U(w) & = & \frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\
V(w) & = & -\frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}\ d\xi\tag{Eq. 3}\label{Eq. 3}
\end{array}$$ which is traditionally stated as the property a causal function should fulfill. And here is where I got into a problem: thinking that $f(t)$ it is a real-valued causal function, in principle it should be fulfilling \eqref{Eq. 3}, but since its Fourier transform its real valued, the split $F(w) = U(w)+iV(w)$ will lead: $$\begin{array}{r c l}
U(w) & = & \frac{768}{w^9}\cdot \Big(5\ w\ (2\ w^2-21)\cos(w)+(w^4-45\ w^2+105) \sin(w)\Big) \\
V(w) & = & 0 \tag{Eq. 4}\label{Eq. 4}
\end{array}$$ And I don't know how it could be possible then to fulfill with these values the relations of \eqref{Eq. 3}: $$\begin{array}{r c l}
U(w) & = & \frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{\textbf{0}}{\xi-w}\ d\xi \overset{?!}{\equiv} \frac{768}{w^9}\cdot \Big(5\ w\ (2\ w^2-21)\cos(w)+(w^4-45\ w^2+105) \sin(w)\Big)\\
V(w) & = & -\frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{\frac{768}{\xi^9}\cdot \Big(5\ \xi\ (2\ \xi^2-21)\cos(\xi)+(\xi^4-45\ \xi^2+105) \sin(\xi)\Big)}{\xi-w}\ d\xi \overset{?!}{\equiv} 0 \tag{Eq. 5}\label{Eq. 5}
\end{array}$$ and even it could be possible of having a zero-valued integral, in \eqref{Eq. 5} showing a function as result of integrating a zero makes no sense for me , so surely I have something mistakenly understood, or otherwise there some assumptions of the Kramer-Kronig relation that $f(t)$ is not fulfilling, and I don't know which it is (at least I checked that $F(w)$ decrease much master than $1/|w|$ , other assumptions I don't really understand them so I cannot check them). Hope you could explain with detail and sources what is happening here. Added later Reading again the Wikipedia page for the Kramers-Kronig relations I think the problem could be in the requirement ""Suppose this function is analytic in the closed upper half-plane of $w$ "", which could be requiring than the Fourier Transform must have a non-zero imaginary part $V(w) \neq 0$ from which I think I have understood from the page for analytic signal . But this makes the problem even worst: with the following procedure I would try to make appear the Kramers-Kronig relations under a few assumptions and using the real/imaginary split in general form, and I think without loss of generality it means that by replacing the terms by zero on the imaginary part of the spectrum it should still be holding as true, but as it could be seen is not what end by happening, and I don't know why is that so: Let be $x(t)$ a square-integrable real-valued continuous function with Fourier Transform $X(w)$ such as $X(w)=U(w)+iV(w)$ with $U(w)$ and $V(w)$ real-valued functions in the angular frequency variable $w$ ( $X(w)\in\mathbb{C};\,U(w),V(w) \in\,\mathbb{R}$ ). Since $x(t)$ is real-valued then $X(-w) = \overline{X(w)}$ his complex conjugate. Let also $x(t)$ be a causal function, so, there exist an initial time $t_0$ where $x(t)=0,\ \forall t<t_0$ . For simplicity, but without loss of generality, I will assume that $t_0=0$ . With this, it will be equivalent to represent $x(t)$ as $x(t) = x(t)\cdot\theta(t)$ with $\theta(t)$ the unitary step function . This assumption have a serious consequence: since $x(t)$ is not extended from $t \to \pm \infty$ , then it cannot be band-limited, as is stated in Wikipedia , so the domain on the frequencies must go from $w\in \ (-\infty,\ \infty)$ . Also having an initial time can be described through the Kramer-Kronig relation , which are used as a characteristic a causal function must fulfill, obtained by applying the Fourier Transform the last equation: $$\begin{array}{r c l}
X(w) & = & \mathbb{F}\left\{x(t)\cdot\theta(t) \right\}(w) \\
& = & \frac{1}{2\pi} X(w)\circledast\mathbb{F}\left\{\theta(t) \right\}(w) \\
& = & \frac{1}{2\pi} X(w)\circledast\left[\pi\delta(w) - \mathit{P\!V}\frac{i}{w} \right](w)\\
& = & \frac{X(0)}{2}-\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{iX(\xi)}{\xi-w}\ d\xi \tag{Eq. 6}\label{Eq. 6}
\end{array}$$ where $\circledast$ is the Convolution operator, and $\mathit{P\!V}$ means the Principal Value which if there exist a singularity at point "" $\epsilon$ "", it can be calculated as $\mathit{P\!V}\int\limits_{-\infty}^{\infty}f(x)\ dx =\lim\limits_{c\rightarrow 0^+}\left[\int\limits_{-\infty}^{\epsilon-c}f(x)\ dx +\int\limits_{\epsilon+c}^{\infty}f(x)\ dx \right]$ . By splitting $X(w) = U(w)+iV(w)$ and using the property for real-valued functions one gets that: $$\begin{array}{r c l}
X(-w)  & =  & \overline{X(w)}\\
\Rightarrow U(-w) + iV(-w) & = & U(w)-iV(w) \\
\Rightarrow U(-w) & = & U(w)\,\,\text{even function}\\
V(-w) & = & -V(w)\,\,\text{odd function}\\
\Rightarrow V(-0) = -V(0) & \Rightarrow & V(0) = 0
\end{array}$$ This split into \eqref{Eq. 6} leads to: $$\begin{array}{r c l}
U(w)+iV(w) & = & \frac{U(0)}{2}+\frac{i\require{cancel}\cancel{V(0)}}{2}-\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{iU(\xi)}{\xi-w}\ d\xi +\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\
\text{pairing real and imaginary terms}\,\Rightarrow U(w) & = & \frac{U(0)}{2} +\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\
V(w) & = & -\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}\ d\xi\tag{Eq. 7}\label{Eq. 7}
\end{array}$$ Which are ""almost"" the relation shown in Wikipedia page for the Kramer-Kronig relation (show on \eqref{Eq. 8}): there is a constant $U(0)$ and a $1/2$ term don't fit: I don't know exactly why is that, and this is not the formal way to find the relations, but since it don't change why I want to ask, and I found more intuitive this procedure, please keep it mind. If you know how to get the following: $$\begin{array}{r c l}
U(w) & = & \frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\
V(w) & = & -\frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}\ d\xi\tag{Eq. 8}\label{Eq. 8}
\end{array}$$ from \eqref{Eq. 7} I will love to know how to do it: I think is related with the fact that the Fourier Transform just go from $t=0\to\infty$ as it kind of fit with the definition of the analytic signal , but to be honest I don't know in the formal definitions how they make appear the DC component $U(0)$ from an even function $V(w)$ . Also, as you can see from \eqref{Eq. 7} by replacing just $V(w)=0$ the equalities do not hold true, which means that the Kramer-Kronig relations, the only tool for causality analysis I know from signal analysis, don't hold for any squared-integrable real-valued causal continuous function $x(t)$ , which don't seen right to me since physical signals in time should fulfill it (as the particular example of the question $f(t)$ ). As can you see I am quite lost, so please elaborate about why this is isn't working . 2nd Added later The full conditions shown in Wikipedia for the Kramer-Kronig relation says: ""Suppose this function is analytic in the closed upper half-plane of $w$ and vanishes faster than $1/|w|$ as $|w|\to \infty$ . Slightly weaker conditions are also possible."" The closed upper half-plane is defined in Wikipedia as ""the union of the upper half-plane $\mathcal{H}\equiv \{x+iy\,|\,y>0;\,x,\ y\in\mathbb{R}\}$ and the real axis. It is the closure of the upper half-plane."" So, since $f(t)$ lives in the real axis as also do $F(w)$ , which I believe it is indeed analytic in $w$ , and also if I plot the Fourier Transform of Eq. 2 jointly with the function $1/|w|$ it shows indeed a decay faster than the $1/|w|$ , so in principle I think its fulfilling the conditions, as can be seen here So $f(t)$ should be fulfilling the conditions of the Kramers-Kronig relations, so I don't know why the previous equations aren't fulfilled. Hope you explain in detail if I have misunderstood some of the conditions.","['fourier-analysis', 'fourier-transform', 'real-analysis', 'complex-analysis', 'signal-processing']"
4596864,Prove/Disprove $f(z)=0$ if and only if $z=0$ for a power series $f$ about the centre $0$ having radius of convergence $2$,"Q. Let $f(z)$ be a power-series (with complex coefficients) centred at $0 \in \mathbb{C}$ and with a radius of convergence $2$ . Suppose that $f(0)=0$ . Choose the correct statement(s) from below: (A) $f^{-1}(0)=\{0\}$ (B) If $f$ is a non-constant function on $\{z \in \Bbb C~:~|z|<2\}$ , then $f^{-1}(0)=\{0\}$ ; (C) If $f$ is a non-constant function, then for all $\zeta \in \Bbb C$ with sufficiently small $|\zeta|$ , the equation $f(z)=\zeta$ has a solution; (D) $\int_\gamma f^{(n)}(z) d z=0$ for every $n \geq 1$ , where $\gamma$ is a unit circle centred at $0$ , oriented clockwise, and $f^{(n)}$ is the $n^{\text{th}}$ derivative of $f(z)$ . How can I confirm $f(z)=0$ if and only if $z=0$ , in the geometric series $\sum_{n=1}^\infty 2^{-n}z^n$ , I got first option is true, but how to generalize. Option D says all the coefficients of the terms is zero, I feel it is impossible.","['complex-analysis', 'real-analysis']"
4596884,Existence of solution for an ODE,"This is a homework question but no hint is given. I think it is related to comparison principle of the ordinary differential equation.
Let f be a $C^{1}$ -function in $\mathbf{R}^{2}$ such that $|f(x,y)|\leq 1+|y|$ for all $(x,y)\in\mathbf{R}^{2}$ Show that the initial value problem $$y'=f(x,y), x(0)=0$$ has a solution y(x) for all $t\in(-\infty,\infty)$ My try is
Let $E(x)=\frac{1}{2}|y(x)|^{2}$ ,then $|E'(x)|=|y(x)\dot y'(x)|\leq C |y|(1+|y|)\leq C_{0}(1+|E(x)|)$ Then $E(x)\leq (E(0)+1)exp(C_{0} x)-1$ and E(x) never blow up thus y is globally defined.
I am not sure whether this is the standard answer because comparison principle or Gronwall's inequality is not taught in my course but this is the only way I can figure it out.
Are there any other methods?  Any help is appreciated!","['analysis', 'ordinary-differential-equations']"
4596896,"For every non-closed set, there is a continuous function that has no continuous extension [duplicate]","This question already has an answer here : If every continuous function on a set can be extended to a continuous function on $\mathbb{R}$ then the set is closed. (1 answer) Closed 1 year ago . I have a question. We know that ""if $A \subset \mathbb{R}$ is a closed set, then every continuous function $f : A \to \mathbb{R}$ has a continuous extension from $\mathbb{R}$ to $\mathbb{R}$ "". But I now I have to prove ""if $A \subset \mathbb{R}$ is a non-closed set, then there is a continuous function $f : A \to \mathbb{R}$ which has no continuous extension from $\mathbb{R}$ to $\mathbb{R}$ "". This means when we omit the assumption about closedness of $A$ , the theorem does not hold. But how can we prove for every non-closed set $A$ ? I find a similar question that we can prove by the contrapositive: If every continuous function on a set can be extended to a continuous function on $\mathbb{R}$ then the set is closed. But my question is ""Is there any way to prove directly?"" Please give me some hints. Any help is highly appreciated.","['continuity', 'general-topology', 'analysis']"
4596910,Different Ways To Calculate Conditional Probabilities,"I thought of the following math problem: Suppose there is a basketball coach that wants to test the following hypothesis: The coach believes that once a player successfully scores a few baskets - the player is then more likely to score more baskets. Suppose the basketball coach then carries out an experiment - the coach asks different players to shoot baskets and records the results. As an example, the data might look something like this (I simulated this using the R programming language): Player                                                               Baskets
1 Player 1                      Miss Miss Miss Miss Hit  Hit  Hit  Miss Miss Hit
2 Player 2                                               Hit  Miss Miss Hit  Hit
3 Player 3            Miss Miss Hit  Hit  Hit  Miss Hit  Miss Hit  Miss Hit  Hit
4 Player 4 Hit  Hit  Hit  Miss Miss Hit  Miss Miss Miss Hit  Hit  Miss Hit  Miss
5 Player 5                               Hit  Hit  Miss Hit  Miss Miss Hit  Miss Based on this format of data - I thought of the 3 following methods to answer the coach's question: Method 1: For each individual player, count the number of times (in general, over all shots) that ""Miss"" follows ""Miss"", ""Miss"" follows ""Hit"", ""Hit"" follows ""Hit"" and ""Hit"" follows ""Hit"". Repeat this for all players, and then you can construct a 4-State Markov Chain with Conditional Probabilities Method 2: For each individual player, ignore everything except the last two shots. Then, count the number of at ""Miss"" follows ""Miss"", ""Miss"" follows ""Hit"", ""Hit"" follows ""Hit"" and ""Hit"" follows ""Hit"". Repeat this for all players, and then you can construct a 4-State Markov Chain with Conditional Probabilities only taking into consideration the last shot. Method 3: For each individual player, assign a value of ""1"" when a basket is ""Hit"" and a value of ""0"" when a basket is missed"". Then calculate the average for each player (e.g. Hit, Hit, Miss, Hit, Hit = 1+1+0+1+1 / 5 = 0.8) but ignore the last basket. The data should now be in the following format: Player Average_of_All_Baskets_Excluding_Last Last_Basket
1 Player 1                                 0.330         Hit
2 Player 2                                 0.500         Hit
3 Player 3                                 0.545         Hit
4 Player 4                                 0.500        Miss
5 Player 5                                 0.570        Miss Based on this approach, a Regression Model (e.g. Logistic Regression)  can be fit that models the probability of making the next basket, based on the scoring average of the player up until that point. We can also add other variables such as the ""result of the second last basket"" or the ""average of the second last and the third last baskets"" that can try to capture and benefit the model with more recent information. As a result, this model will try to estimate the conditional probability of making the next basket based on the history of the player. However, I do not know if using ""lagged values of the response variable"" will violate the assumptions of the Regression Model. Thus, my question is - are all 3 methods valid approaches to estimating the conditional probabilities and testing the hypothesis of the coach? Are some of these methodologies more ""valid"" than others (e.g. perhaps some contain inherent biases and flaws)? Thanks!","['statistics', 'probability']"
4596954,Similarities between the Bessel $J_0(x)$ function and the sinc functions,"The Bessel functions are particular functions defined as the solution of the differential equation: $$x^2 \frac{d^2y}{dx^2} + x\frac{dy}{dx} + (x^2-\alpha^2)y = 0$$ for an arbitrary complex number $\alpha$ (I'll consider it real for what follows). There are different kinds of Bessel functions, depending on the value of $\alpha$ : I only consider here the Bessel function of the first kind , which is $J_0(x)$ . It really looks like to some kind of sinc function to me, or something similar. Hence I try to approximate the Bessel function (which has a tough expression) with ""nice"" trigonometrical functions. Here is a graph comparing Bessel function of the first kind $J_0(x)$ to $A\frac{\sin(bx+\varphi)}{cx+\phi}$ (which is some kind of ""generalised"" sinc function since $bx + \varphi = cx + \phi = \pi x$ in a classical one) with $A = 1.1091$ , $b = c= 1$ and $\varphi = \phi = 0.78$ : Isn't the ressemblance between those two functions stunning ? Is it just a coincidence or is there a specific reason to it ? Do you think that I could find the exact Bessel function by changing the parameters $a, b, c, \varphi$ and $\phi$ ? EDIT : the result of the proposition of @Gary (see comments below):","['functions', 'sequences-and-series', 'power-series', 'trigonometry', 'bessel-functions']"
4596977,Trouble with simple notation in proving linearity of an operator.,"Consider the operator $T\colon \mathcal C[0,1]\to \mathcal C[0,1]$ defined by $$ Tf(x) = \int_0^x f(t) \, dt, \quad \forall f \in \mathcal C[0,1].$$ To prove its linearity, consider two scalars $\alpha,\beta \in \mathbb K$ and two functions $f,g \in \mathcal C[0,1]$ , as arbitrary as possible. Then, \begin{equation*}
    T(\alpha f+\beta g)\color{red}{(x)} = \int_0^x (\alpha f + \beta g)(t)\, dt = \int_0^x \alpha f(t) + \beta g(t) \, dt = \alpha \int_0^x f(t) \, dt + \beta \int_0^x g(t) \, dt = \alpha Tf\color{red}{(x)} + \beta Tg\color{red}{(x)}.
\end{equation*} My question. Does my notation in $\color{red}{red}$ makes sense? Somehow, in my head it would make more sense if I just wrote $T(\alpha f + \beta g)$ but, at the same time, according to the definition it doesn't make that much sense. I get even more confused when it comes to the final conclusion: We just proved that $$ T(\alpha f + \beta g) = \alpha Tf + \beta Tg, \quad \forall \alpha,\beta \in \mathbb K, \, \forall f,g \in \mathcal C[0,1].$$ Here, I didn't use the notation in $\color{red}{red}$ since it makes zero sense to me. Thanks for any help in advance.","['notation', 'functions', 'functional-analysis']"
4597015,A convex decomposition of probability measures,"I recently came across a problem when I read M.Hochman's paper . He said before Lemma 3.3 that 'it follows from standard measure theory that for every $\epsilon>0$ , there is a $\delta>0$ such that if $||\mu-\nu||<\delta$ then there are probability measures $\tau,\mu',\nu'$ such that $\mu=(1-\epsilon)\tau+\epsilon\mu'$ and $\nu=(1-\epsilon)\tau+\epsilon\nu'$ '. Here we just consider the Borel probability measures on $\mathbb{R}^d$ , and denote $||\mu-\nu||=\sup_{A\in\mathcal{B}(\mathbb{R}^d)}|\mu(A)-\nu(A)|$ the total variation. I do not know how to prove this assertion and cannot find any material that contains a proof. I believe the key point is to find a probability measure $\tau$ satisfying $$\mu(A)-(1-\epsilon)\tau(A)\geq0,\quad\nu(A)-(1-\epsilon)\tau(A)\geq0\quad\forall A\in\mathcal{B}(\mathbb{R}^d).$$ But I do not know how to start. Would you  give me some hints or materials? Thank you very much!","['measure-theory', 'probability-theory']"
4597051,Banach space of a unitization non-unital $C^\ast$-algebra and norms as well,"I've discussed a problem with good mathematicians here. We consider again a non-unital $C^\ast$ -algebra. Let's consider the following $C^\ast$ -algebra. That is, the $C^\ast$ -algebra $\mathcal{A}$ with norm $\|\cdot\|$ . Let $\tilde{\mathcal{A}}=\mathcal{A}\oplus \mathbb{C}$ as a vector space. We endow it with multiplication and involution, $$(a,\lambda)\cdot (b,\mu):=(ab+\lambda b+\mu a,\lambda \mu)$$ and $$(a,\lambda)^\ast:=(a^\ast,\bar{\lambda})$$ I already did some proofs including proof of homomorphism and norm etc. so we will take that for granted. Now take $x\in\tilde{\mathcal{A}}$ then we consider the linear map $\tilde{L}_x:\tilde{\mathcal{A}}\rightarrow \tilde{\mathcal{A}}$ by $y\mapsto xy$ restricts to a map, $L_x:\mathcal{A}\rightarrow \mathcal{A}$ which is bounded with $\|L_x\|_\infty \leq \|x\|_1$ . This is already proven. Notice that we know the one-norm is given by $x=(a,\lambda)\in \tilde{\mathcal{A}}$ by $\|x\|_1=\|a\|+|\lambda|$ and of course $\|\cdot\|_\infty$ denote the operator norm. Let's consider the following Banach space, call it $\mathscr{X}:=\mathcal{A}\oplus \mathbb{C}$ with the following norm. $$\|(a,\lambda)\|_{\max}=\max\{\|a\|,|\lambda|\}.$$ Set $x=(a,\lambda)$ and let $x\in \tilde{\mathcal{A}}$ , then I define the following: $p(x):\mathscr{X}\rightarrow \mathscr{X}$ by using a matrix given below, i.e. $$p(x)=\begin{pmatrix}
L_x & 0 \\
0 & \lambda \\
\end{pmatrix}$$ My questions: Can we verify that $p(x)\in\mathbb{B}(\mathscr{X})$ and further show $\|p(x)\|_\infty=\max\{\|L_x\|_\infty,|\lambda|\}$ ? Is $p$ a unital, injective homomorphism of algebras? Here $\mathbb{\mathscr{X}}$ is bounded operators on a Banach space $\mathscr{X}$ . For my second question I know the idea of homomorphism but now injective homomorphism ? Any suggestion would be helpful for me! If there are more conditions to show homomorphism, I would like to see how one of them can be done then I will do the other conditions. Further let's define a new norm on $\tilde{\mathcal{A}}$ by setting $\|(a,\lambda)\|_\sim=\|p(a,\lambda)\|_\infty$ . My third and last question. Is it possible to show the norm $\|\cdot \|_\infty$ restricts to the original $C^\ast$ -norm on $\mathcal{A}\subset\tilde{\mathcal{A}}$ ? I know that there is a lot of questions and I hope someone could help me to understand this way more better than I already do. I hope someone can give me an detailed answers/sketches so I can understand this and fill details. Thanks in advance. If I get a great answer I will mark it accepted.","['c-star-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras']"
4597085,"In the given triangle with two intersecting cevians, find the area of the shaded quadrilateral.","As title suggests, the question is to find the area of the shaded quadrilateral inside a triangle formed via two intersecting cevians, given the area of $3$ smaller triangles with areas $3$ , $6$ and $9$ respectively. I discovered this puzzle online posted on a language-learning app, and I found it quite interesting. The asker claimed that it was a grade 9 problem. I did attempt it in several ways, but a lot of the attempts did not lead anywhere. I will post my successful attempt as an answer below, please do let me know if it is correct, if something can be improved or if the answer may be wrong (the asker did not reveal the answer). And please share your own attempts too!","['euclidean-geometry', 'geometry', 'solution-verification', 'triangles', 'trigonometry']"
4597093,"Is this just a coincidence? Expectation of product of areas in unit circle equals $\pi^2/6$, the answer to the Basel problem.","Draw line segments from the centre of a unit circle to two uniformly random points on the circle, forming two regions of area $A_1$ and $A_2$ . It is easy to show that the expectation of the product of the areas $E(A_1A_2)=\frac{1}{\pi}\int_0^\pi\frac{1}{2}x\left(\pi-\frac{1}{2}x\right)dx=\frac{\pi^2}{6}$ , which happens to be the answer to the Basel problem , $\sum\limits_{n=1}^\infty \frac{1}{n^2}$ . Is this just a coincidence? Or can we solve the Basel problem by showing that $\sum\limits_{n=1}^\infty \frac{1}{n^2}=E(A_1A_2)$ , and only then calculating $E(A_1A_2)=\frac{\pi^2}{6}$ ? The reason I think it might not be a coincidence, is that $E(A_1A_2)$ and $\sum\limits_{n=1}^\infty \frac{1}{n^2}$ are both simple, non-arbitrary constructions. It's not like we're choosing special constants just to make two things equal.","['area', 'circles', 'expected-value', 'products', 'sequences-and-series']"
4597100,Is $\varphi (t)$ the characteristic function of some random variable?,"whether the following $$\varphi (t) = -\cos^{4} t+2\cos^{2}t$$ is a characteristic function (short for ch.f.) of some random variable $X$ ? Soppose $\varphi (t)$ is a ch.f. of a random variable $X$ , then $$\frac{\mathrm{d} }{\mathrm{d} x}\varphi (t)|_{t=0}=i\mathbb{E}X=0;\quad\frac{\mathrm{d^2} }{\mathrm{d} x^2}\varphi (t)|_{t=0}=i^2\mathbb{E}X^2=0.$$ Thus $X$ has some symmetric distribution and $\int _{\mathbf{R}}X^2(\omega)\mathbb{P}(d\omega)=0.$ $$\int _{\mathbf{R}}X^2(\omega)\mathbb{P}(d\omega)=0\Longrightarrow \mathbb{P}(X=0)=1\Longrightarrow \text{The ch.f of }X:\phi_{X}(t)=1\ne\varphi (t).$$ I am not sure the above solution is right. I need some help to verifty it.","['self-learning', 'probability-theory', 'characteristic-functions']"
4597173,One of f and g has to be injective for g ◦ f to be injective. Is this a counter-example,"In this example that I have created, f and g are both surjective and total (total is a requirement for both f and g in this question) and the composite is bijective meaning it is injective. Is this a counter-example? Context: Im studying cs first year in uk and one of the modules is discrete maths","['graph-theory', 'discrete-mathematics']"
4597284,Subgroup containing normalizers of its $p$-subgroups,"This is exercise $1.D.5$ of Isaacs' ""Finite Group Theory"". It goes: Let $G$ be a finite group and let $H$ be a subgroup of $G$ . Suppose $N_G(P) \subset H$ for all $p$ -subgroups $1 \neq P$ of $H$ . Then, $H$ is a Frobenius complement in $G$ . For the relevant definition, a Frobenius complement is a proper subgroup $H$ such that $H \cap H^g = 1$ for all $g \in G \setminus H$ . The problem above comes with a hint, namely: HINT : Show that $H \cap H^g$ satisfies the hypothesis for $g \in G$ . Then, suppose it is not-trivial, take a non-trivial Sylow subgroup $Q \in \operatorname{Syl}_p(H \cap H^g)$ and prove $Q, Q^{g^{-1}}$ are conjugate in $H$ I have been able to prove the first part of the hint, but I can't get the second part. Here's what I have thus far: Let $Q$ be as in the hint. In particular, $Q\subset H$ and $Q^{g^{-1}} \subset H \cap H^{g^{-1}}$ . By a former problem, we can extend $Q \subset P$ , where $P \in \operatorname{Syl}_p(H)$ is such that $P \cap (H \cap H^g) = Q$ . The same way, we can define a $P'$ with $P' \cap (H^{g^{-1}} \cap H) = Q^{g^{-1}}$ . By the Second Sylow Theorem, there exists $h \in H$ such that $P^h = P'$ . Therefore, $P^h \cap (H \cap H^{gh}) = Q^h \implies Q^h = P'\cap (H \cap H^{gh})$ I don't know how to continue. I hoped the $h$ given above (by the SST) would be the one to work, but the two expressions didn't come out equal... How should I proceed? Thanks in advance!","['group-theory', 'sylow-theory', 'finite-groups', 'frobenius-groups']"
4597308,Proof of Bonnet's Recursion Formula for Legendre Functions of the Second Kind?,"I'm doing some self-study on Legendre's Equation.  I have seen and understand the proof of Bonnet's Recursion Formula for the Legendre Polynomials, $P_n(x)$ . $$(n+1)P_{n+1}(x) = (1+2n)xP_n(x) - nP_{n-1}(x)$$ Additionally, I have read that this formula also applies to Legendre functions of the second kind $Q_n(x)$ , however I have not found any source that does more than simply state this fact.  My question is: how does one prove that Bonnet's Recursion Formula extends to Legendre functions of the second kind? For context, the proof I am familiar with for Bonnet's Recursion Formula relies on the generating function for the Legendre Polynomials $$ \frac{1}{\sqrt{1-2xt+t^2}} = \sum_{k=0}^{\infty} P_n(x){t^n} $$ where you differentiate with respect to $t$ and then collect coefficients of ${t^n}$ .","['legendre-functions', 'special-functions', 'ordinary-differential-equations', 'legendre-polynomials']"
4597314,"$\lim\limits_{n\to\infty}(\frac{\text{number of 1s in the binary numeral of }b^n}{n}) = \log_4(\text{OddPart}(b))$, for any positive integer b?","Notation : For any positive integer $x$ , let $\operatorname{Ones}(x)$ denote the Hamming weight of $x$ (i.e., the number of $1$ s in the binary numeral for $x$ ), and let $\operatorname{OddPart}(x)$ denote the odd part of $x$ (i.e. the largest odd divisor of $x$ ). While computing some Hamming weights, I was surprised to see the following behavior: It seems that for any given positive integer $b$ , $\operatorname{Ones}(b^{(10^k)})$ has the same leading decimal digits for all sufficiently large $k$ . Furthermore (with help of OEIS), these leading digits are discovered to be those of $\log_4\operatorname{OddPart}(b)$ in every case except when $b$ is a power of two (in which case there is of course only one $1$ in the numeral of $b$ ). Examples : It's only necessary to look at odd $b$ because $\operatorname{Ones}(b^n) = \operatorname{Ones}(\operatorname{(OddPart}(b))^n),$ since multiplying by a power of two just appends $0$ s to the binary numeral. b     Ones(b^10^9)   Ones(b^10^9)/10^9 ≈ log_4(b)
----------------------------------------------------
   1               1         0.0000          0.0000
   3       792490796         0.7925          0.7925
   5      1160951533         1.1610          1.1610
   7      1403656246         1.4037          1.4037
   9      1584925047         1.5849          1.5850
  11      1729722034         1.7297          1.7297
  13      1850206247         1.8502          1.8502
  15      1953457439         1.9535          1.9534
 123      3471255304         3.4713          3.4713
4567      6078486804         6.0785          6.0785 Numerically, the same behavior is found to occur for ${\operatorname{Ones}(b^n)\over n}$ with sufficiently large $n$ not necessarily a power of ten, leading to the following conjecture: For any positive integer $b$ , $$\lim_\limits{n\to\infty}{\operatorname{Ones}(b^n)\over n}= \log_4 \operatorname{OddPart}(b).$$ Question : Can someone suggest how to prove this, and/or provide a source in the literature? (There is a slightly related question here .) SageMath code for the above table: for b in [1 .. 15, step=2] + [123, 4567]:
    if is_odd(b):
        ones = (b**10**9).popcount()
        print(f""{b:4} {ones:12} {float(ones/10**9):7.4f} {float(log(b,4)):7.4f}"") Update (12/18/2022): The ""random digits"" heuristic described in the comment by @HagenvonEitzen lends plausibility to the following more general conjectures, which do indeed appear to be borne out numerically (using SageMath): For positive integers $p, x$ , define $N_p(x)\overset{def}{=}$ the number of nonzero digits in the base- $p$ numeral for $x$ , $S_p(x)\overset{def}{=}$ the sum of all the digits in the base- $p$ numeral for $x$ . $R_p(x)\overset{def}{=}$ the result of dividing $x$ by the largest power of $p$ that divides $x.$ $Z_p(x)\overset{def}{=}\min\left({e_1^\prime\over e_1},\ldots,{e_k^\prime\over e_k}\right),$ where in standard form the prime factorization of $p$ is $p=\prod_{i=1}^k p_i^{e_i}$ , and $e_i^\prime$ is the exponent of $p_i$ in the prime factorization of $x$ . Claim :
If $p, b$ are positive integers, then $$\lim_\limits{n\to\infty}{N_p(b^n)\over n}= {p-1\over p}\big(\log_p(R_p(b))-Z_p(R_p(b))\big).$$ $$\lim_\limits{n\to\infty}{S_p(b^n)\over n}= {p-1\over 2}\big(\log_p(R_p(b))-Z_p(R_p(b))\big).$$ The reasoning is as follows, where we let $r=R_p(b)$ : $r$ is just the result of removing all trailing $0$ s (if any) from the base- $p$ numeral of $b$ , which affects neither the number of nonzero digits nor the sum of all the digits; hence, $N_p(b^n)=N_p(r^n)$ and $S_p(b^n)=S_p(r^n)$ . Now some of the base- $p$ digits of $r^n$ may also be trailing $0$ s. This number (say $z\ge 0$ ) is the largest $m$ such that $p^m$ divides $r^n$ , which is seen to be the largest $m$ such that $m e_i\le n e_i^\prime$ for all $i\in 1..k,$ where the $e_i$ are the exponents in the prime factorization of $p=\prod_{i=1}^\infty p_i^{e_i}$ and $e_i^\prime$ is the exponent of $p_i$ in the prime factorization of $r$ .  Thus $z$ is the largest $m$ such that $m\le n Z_p(r);$ hence, $z\sim n Z_p(r)$ as $n\to\infty$ . Example: $p=2^3 5^7, (e_1,e_2)=(3,7);r=2^1 3^3 5^2,(e_1^\prime,e_2^\prime)=(1,2);Z_p(r)=\min(1/3,2/7)=2/7.$ Hence, (number of base- $p$ digits of $b^n$ that are not trailing $0$ s ) $\sim n \log_p(r)- n Z_p(r).$ If the digits that are not trailing zeros tend to occur with approximately equal frequencies , then we obtain the above results (1) and (2): $$N_p(r^n)\sim n\ \big(\log_p(r)- Z_p(r)\big)\ {p-1\over p}$$ and $$S_p(r^n)\sim n\ \big(\log_p(r)- Z_p(r)\big){0+1+2+...+(p-1)\over p}=n\ \big(\log_p(r)- Z_p(r)\big)\ {p-1\over 2}.$$ Note that $Z_p(r)=0$ iff some prime divisor of $p$ is not a divisor of $r$ .","['elementary-number-theory', 'binary', 'combinatorics', 'reference-request']"
4597349,How to solve $x^2y''+xy'+x^2ky = 0$,"I am working on a $2D$ steady state heat equation (Laplacian). I did Separation of Variables and am evaluating $3$ cases ( $k>0, k<0, k=0$ ). For the last scenario, I am not sure how to solve this ODE that developed after Separation of Variables. It is almost a Bessel equation of order zero, but not quite because of that pesky $k$ . Could anyone give me a hint about how to solve this ODE? $$r^2R''+rR'+r^2kR = 0$$ Edit to add some additional information:
I started with: $${∂^2T\over∂r^2} +{1\over r}{∂T\over∂r} + {∂^2T\over∂z^2}=0$$ Tried Separation of Variables as follows: $T(r,z) = R(r)Z(z)$ Which resulted in these two ODEs: $${1\over R}{∂^2T\over∂r^2} +{1\over r}{∂T\over∂r} = k$$ $$-{1\over Z}{∂^2Z\over∂z^2} = k$$ The problem is a cylinder with height 1 and radius 1, where the temperature is 1 on the top, and 0 on the curved wall and the bottom. Thus $T(r,0)=0$ , $ T(r,1)=1$ , $T(1,z)=0$ . I am struggling to understand which case is appropriate for this problem. I believe $k=0$ results in a singularity at $r=0$ due to a natural logarithm term, so that case does not seem correct.",['ordinary-differential-equations']
4597357,How to evaluate $\sum_{k=1}^n\frac{(2k)!!}{(2k-1)!!}z^k$,"I want to find a closed form for the following: $$J_k=\int_0^1\frac{x^k}{x+1}\operatorname{arsinh}^2(\sqrt{x}/2)dx$$ Using the power series for these 2 functions we get $$=\int_0^1x^k\left(\sum_{n=0}^\infty(-1)^nx^n\right)\left(\frac{1}{2}\sum_{m=0}^\infty\frac{(-1)^{m}(2x)^{2m+2}}{(m+1)^2{{2m+2}\choose{m+1}}}\right)$$ Using the Cauchy Product we get: $$=\frac{1}{2}\int_0^1x^k\sum_{m=0}^\infty\sum_{n=0}^m(-1)^{m-n}x^{m-n}\frac{(-1)^n4^{n+1}x^{2n+2}}{(n+1)^2{2n+2\choose n+2}}dx$$ I simplified and switched the order of the sums and the integral. Evaluating that integral we get $$=\frac{1}{2}\sum_{m=0}^\infty(-1)^{m}\sum_{n=0}^m\frac{4^{n+1}}{(n+1)^2(m+n+k+3){2n+1\choose n+1}}$$ I wanted to get rid of the $(m+n+k+3)$ , so I set up an integral inside the inner sum $$=\frac{1}{2}\sum_{m=0}^\infty(-1)^{m}4^{-m-k-2}\sum_{n=0}^m\frac{4^{m+n+k+3}}{(n+1)^2(m+n+k+3){2n+2\choose n+1}}$$ $$=\frac{1}{2}\sum_{m=0}^\infty(-1)^{m}4^{-m-k-2}\sum_{n=0}^m\frac{1}{(n+1)^2{2n+2\choose n+1}}\int_0^4x^{m+n+k+2}dx$$ Then I took the integral to the outside $$=\frac{1}{2}\int_0^4\sum_{m=0}^\infty(-1)^{m}4^{-m-k-2}\sum_{n=0}^m\frac{x^{m+n+k+2}}{(n+1)^2{2n+2\choose n+2}}dx$$ My best idea after this was to make another integral $$=\frac{1}{2}\int_0^4\sum_{m=0}^\infty(-1)^{m}4^{-m-k-2}x^{m+k+1}\sum_{n=0}^m\frac{x^{n+1}}{(n+1)(n+1){2n+2\choose n+2}}dx$$ $$=\frac{1}{2}\int_0^4\sum_{m=0}^\infty(-1)^{m}4^{-m-k-2}x^{m+k+1}\sum_{n=0}^m\frac{1}{(n+1){2n+2\choose n+2}}\int_0^xy^ndy\space dx$$ $$=\frac{1}{2}\int_0^4\int_0^x\sum_{m=0}^\infty(-1)^{m}4^{-m-k-2}x^{m+k+1}\sum_{n=0}^m\frac{y^n}{(n+1){2n+2\choose n+2}}dy\space dx$$ My approach from here was to make another integral $$=\frac{1}{2}\int_0^4\int_0^x\sum_{m=0}^\infty(-1)^{m}4^{-m-k-2}\frac{x^{m+k+1}}{y}\sum_{n=0}^m\frac{1}{{2n+2\choose n+2}}\int_0^yz^n dz\space dy\space dx$$ $$=\frac{1}{2}\int_0^4\int_0^x\int_0^y\sum_{m=0}^\infty(-1)^{m}4^{-m-k-2}\frac{x^{m+k+1}}{y}\sum_{n=0}^m\frac{z^n}{{2n+2\choose n+2}} dz\space dy\space dx$$ Reindex $$=\frac{1}{2}\int_0^4\int_0^x\int_0^y\sum_{m=0}^\infty(-1)^{m}4^{-m-k-2}\frac{x^{m+k+1}}{yz}\sum_{n=1}^{m+1}\frac{z^n}{{2n\choose n}} dz\space dy\space dx$$ Using $${2n\choose n}=4^n\frac{(2n-1)!!}{(2n)!!}$$ We get $$\sum_{n=1}^{m+1}\frac{z^n}{{2n\choose n}}=\sum_{n=1}^{m+1}\frac{(z/4)^n(2n)!!}{{(2n-1)!!}}$$ We can reduce the problem to finding $$\sum_{k=1}^n\frac{(2k)!!}{(2k-1)!!}z^k$$ I am not sure how to go from here. I don't know any integral representation for $\frac{(2k)!!}{(2k-1)!!}$ , and I don't have too much experience in sums","['integration', 'summation', 'definite-integrals', 'binomial-coefficients', 'sequences-and-series']"
4597358,"Function for ""active users"" over time, given a constant ""new users per day"", and an exponential decay of new users who keep the app after ""x"" days","Forgive me if this is basic, it has been a while since I have used maths like this. Given: $install\ rate= 1\ new\ user\ per\ day$ $retention\ rate = 0.85^x$ The ""retention rate"" is percentage of new users who keep the app after ""x"" days. In my example, about 30% of the users keep the app after 7 days: $0.85^7 = 0.32$ and basically no one keeps using it after 30 days. It decays towards zero because this app isn't used long term. How would I create a function for active users over time, and will this function approach a positive finite limit? How would I calculate this steady-state value? And a follow up question: My ""retention rate"" model isn't very good, I tried to fit my data to an exponential equation, but I only had one parameter to play with and $0.85^x$ is the best I could do. The actual data looks like exponential decay going towards zero, but 30% of users keeps the app after 1 day, 10% after 7 days, 3% after 30 days, and close to 0% after 90 days. I'm guessing I need a polynomial model to fit this data with, but I do not know how to do this. What is an equation that fits this data, and how does the answer to the first question change when using this new model?",['functions']
4597398,Help with the study of the function $f(x) = \frac{-2}{5x-\ln\vert x \vert}$,"I'm having problems in understanding few things about this function, also because some of my calculations do not match the plot. $$f(x) = \dfrac{-2}{5x-\ln\vert x \vert}$$ Here is what I did. First of all, the domain is $\Omega: x\in (-\infty, x^*)\cup(x^*, 0)\cup(0, +\infty)$ where $x^*$ is the point that makes $5x - \ln\vert x \vert = 0$ . I understood through a very easy analysis that it's negative. The domain is a union of open sets to there is no garancy we have max or min but we go on the same. I calculated these behaviours: $$\lim_{x\to \pm\infty} f(x) = 0^{\mp}$$ $$\lim_{x\to 0^{\pm}} f(x) = 0$$ $$\lim_{x\to x^{*+}} f(x) = -\infty$$ $$\lim_{x\to x^{*-}} f(x) = +\infty$$ So thanks to this I sketched the plot a bit, understanding that there is a vertical asymptote at $x^*$ and a horizontal asymptote at zero.
At $x = 0$ the function is continuos (removable singularity) but not differentiable (it might be a cusp point), for $$f'(x) = \dfrac{2}{(5x-\ln\vert x\vert)^2}\left(5 - \dfrac{1}{\vert x \vert}\right)$$ Where we see the non differentiability at $x^*$ and at $0$ . Also $f(x)$ is not continuous at $x^*$ . Troubles start now, where I have to study the monotonicity of the function. Splitting it into positive and negative $x$ I have $$f'(x) = \begin{cases}
 \dfrac{2}{(5x-\ln(x))^2}\left(5 - \dfrac{1}{x}\right) \qquad & x > 0 \\
\dfrac{2}{(5x-\ln(-x))^2}\left(5 + \dfrac{1}{x}\right) \qquad & x < 0 
\end{cases}
$$ When studying $f'(x) > 0$ I recognize the left term is always positive, but the term in the round brackets in the right changes. I get for $x > 0$ $$f'(x) > 0 \rightarrow x > 1/5 $$ And the function is decreasing for $0 < x < 1/5$ . This matches the plot. But for $x < 0$ I get $$f'(x) > 0 \rightarrow x\in(-\infty, -1/5)$$ And it's decreasing for $-1/5 < x < 0$ The problems here are: -- The function is actually increasing for $x\in (-\infty, x^*)$ and for $x\in(x^*, 0)$ -- There is no decreasing part for $x < 0$ -- Also the limit for $f(x)$ at zero seems like to not be zero... Hence here I got stuck. Can someone please help? Thank you!","['monotone-functions', 'analysis', 'continuity', 'functions', 'limits']"
4597401,Calculating a line integral using Green's theorem in a region with a singularity,"Problem :
Calculate the line integral $$
\int_{A}\frac{y\,dx-(x+1)\,dy}{x^2+y^2+2x+1}
$$ where $A$ is the line $|x|+|y|=4$ , travelling clockwise and making one rotation. Answer : $-2\pi$ Solution : The denominator $x^2+y^2+2x+1$ can be rewritten as $(x+1)^2+y^2$ , which shows that there is a singularity at $(-1, 0)$ . Now, if $$
\left\{\begin{aligned}
M &= \frac{y}{(x+1)^2+y^2} \\[2pt] 
N &= \frac{-(x+1)}{(x+1)^2+y^2} 
\end{aligned}\right.
$$ then $$
\frac{\partial N}{\partial x} = \frac{\partial M}{\partial y}
$$ Thus, Green's theorem yields: $$
\tag{0}\int_{A-B} M\,dx + N\,dy 
= \iint_{D} \frac{\partial N}{\partial x} 
- \frac{\partial M}{\partial y}\,dx\,dy = 0
$$ $B$ can be parameterized with $$
\left\{\begin{aligned}
x &= -1 + \cos t \\
y &= \sin t,
\end{aligned}\right. 
\qquad 0 \leq t \leq 2\pi
$$ which finally yields \begin{align}
\int_{A} M\,dx + N\,dy 
&= \int_{B} M\,dx + N\,dy \\
&= \int_{0}^{2\pi} -\frac{\sin^2 t}{1} - \frac{\cos^2 t}{1}\,dt \\
&= \int_{0}^{2\pi} - 1 \,dt \\[2pt] 
&= -2\pi  
\end{align} I don't understand how to interpret the "" $A - B$ "" on the left-hand side of equation $(0)$ . It seems to me that it does not represent a simple closed curve, as shown in the image above. How can Green's theorem then still be used? Also, why is $B$ a unit circle around the singularity? Why not a circle of radius $2$ , or a square? Why is $B$ necessary at all? I can't figure out why $B$ appears as part of the solution.","['integration', 'greens-theorem', 'multivariable-calculus', 'vector-analysis', 'differential-geometry']"
4597447,Doubts on the solution of a differential equation,"I've encountered myself with a differential equation which I'm not sure that I've solved correctly. The situation is the following: I have a set of parametric equations, which I'll call $\vec{r}$ \begin{array}{ll}
x(\theta) &= R \cdot \cos(\theta) \cr
y(\theta) &= k \cdot \theta \cr
z(\theta) &= R \cdot \sin(\theta)
\end{array} $x$ , $y$ and $z$ represent position in all three axes. As I'm going to need them later, I'll calculate the first and second derivative of $\vec{r}$ \begin{array}{ll}
x\prime(\theta) &= - R \cdot \sin(\theta) \cr
y\prime(\theta) &= k \cr
z\prime(\theta) &= R \cdot \cos(\theta)
\end{array} \begin{array}{ll}
x\prime\prime(\theta) &= - R \cdot \cos(\theta) \cr
y\prime\prime(\theta) &= 0 \cr
z\prime\prime(\theta) &= - R \cdot \sin(\theta)
\end{array} The resulting directional vector would be written as follows: $\vec{u} = (- R \cdot \sin(\theta), k, R \cdot \cos(\theta))$ . Using the Pythagorean theorem, we can calculate the modulus of this vector: $\|\vec{u}\| = \sqrt{R^2 \cdot \sin^2(\theta) + k^2 + R^2 \cdot \cos^2(\theta)}$ . Combining these, we obtain the unitary vector: $$\hat{u} = \frac{\vec{u}}{\|\vec{u}\|} = \frac{(- R \cdot \sin(\theta), k, R \cdot \cos(\theta))}{\sqrt{R^2 + k^2}}$$ Therefore, using the vector for the gravitational acceleration $\vec{v} = (0, 0, g)$ and knowing that $\|\vec{a}_T\| = \|\vec{v}\| \cdot \cos(\beta) = \|(\vec{v} \cdot \hat{u})\|$ , we can obtain the modulus of the total acceleration: $$\|\vec{a}_T\| = \frac{\sqrt{(0^2 + 0^2 + g^2 \cdot R^2 \cdot \cos^2(\theta))}}{\sqrt{R^2 + k^2}} = \frac{g \cdot R \cdot \cos(\theta)}{\sqrt{R^2 + k^2}}$$ The total acceleration will then result in multiplying this modulus we have just obtained by the unit vector which points towards the direction of the acceleration: $$\vec{a} = \|\vec{a_T}\| \cdot \hat{u} = \frac{g \cdot R \cdot \cos(\theta)}{\sqrt{R^2 + k^2}} \cdot \frac{(- R \cdot \sin(\theta), k, R \cdot \cos(\theta))}{\sqrt{R^2 + k^2}}$$ $$\vec{a} = \frac{(- g \cdot R^2 \cdot \cos(\theta) \cdot \sin(\theta), g \cdot k \cdot R \cdot \cos(\theta), g \cdot R^2 \cdot \cos^2(\theta))}{R^2 + k^2}$$ Writing it in parameterized form (I used the chain rule on the left side): \begin{array}{rl}
x: \ddot{\theta}(t) \cdot x\prime(\theta) + \dot{\theta}(t) \cdot x\prime\prime(\theta) &= \displaystyle \frac{- g \cdot \cos(\theta) \cdot \sin(\theta)}{1 + k^2} \cr
y: \ddot{\theta}(t) \cdot y\prime(\theta) + \dot{\theta}(t) \cdot y\prime\prime(\theta) &= \displaystyle \frac{g \cdot k \cdot \cos(\theta) \cdot \sin(\theta)}{1 + k^2} \cr
z: \ddot{\theta}(t) \cdot z\prime(\theta) + \dot{\theta}(t) \cdot z\prime\prime(\theta) &= \displaystyle \frac{g \cdot \cos^2(\theta)}{1 + k^2}
\end{array} Isolating $\ddot{\theta}(t)$ and subtituting with the formulas above, I get the following differential equations which I know how to solve numerically by using Euler's method: \begin{array}{rl}
x: \ddot{\theta}(t) &= \displaystyle \frac{- g \cdot \cos(\theta)}{R \cdot (1 + k^2)} + \dot{\theta}^2(t) \cdot \tan(\theta) \cr
y: \ddot{\theta}(t) &= \displaystyle \frac{g \cdot k \cdot \cos(\theta)}{1 + k^2} \cr
z: \ddot{\theta}(t) &= \displaystyle \frac{g \cdot \cos(\theta)}{R \cdot (1 + k^2)} - \dot{\theta}^2(t) \cdot \tan^{-1}(\theta)
\end{array} My question is: shouldn't the three last formulas be the same or equivalent to each other? Edit: for anyone wondering, the final objective of this is to make an animation using Python. Here's the final code (edited out to only include the math): import math

# VARIABLES
g = - 9.8
R = 10
k = 2
ddtheta = 0

theta0 = math.radians(90)
dtheta0 = 1
x0 = R * math.cos(theta0)
y0 = k * theta0
z0 = R * math.sin(theta0)

x1 = 0
y1 = 0
z1 = 0
theta1 = 0
dtheta1 = 0

object.position[0] = x0
object.position[1] = y0
object.position[2] = z0

# ANIMATION
for i in range(1, 200):
    t = 0.00003 # Small step size
    costheta0 = math.cos(theta0)
    sintheta0 = math.sin(theta0)
    
    # EULER'S METHOD FOR THETA
    ddtheta = (g * R * costheta0)/(R**2 + k**2)
    dtheta1 = ddtheta * t + dtheta0
    theta1 = dtheta0 * t + theta0
    
    # THE THREE AXES
    x1 = R * costheta0
    y1 = k * theta0
    z1 = R * sintheta0
        
    # RESET VALUES FOR NEXT ITERATION
    x0 = x1
    y0 = y1
    z0 = z1
    theta0 = theta1
    dtheta0 = dtheta1
    
    object.position[0] = x1 # X-axis
    object.position[1] = y1 # Y-axis
    object.position[2] = z1 # Z-axis EDIT : The issue is solved... See Lutz's answer below... But now, I tried applying the same method to another curve: the clothoid, but it hasn't worked. The clothoid has the following formulas for $\vec{q}$ : $$\begin{array}{rl}
x(\theta) &= \displaystyle \int^\theta_0 \cos(t^2)\ dt \cr
y(\theta) &= \displaystyle \int^\theta_0 \sin(t^2)\ dt
\end{array}$$ The derivatives are: $$\begin{array}{rl}
x'(\theta) &= \cos(\theta^2) \cr
y'(\theta) &= \sin(\theta^2)
\end{array}$$ $$\begin{array}{rl}
x''(\theta) &= -2 \cdot \theta \cdot \sin(\theta^2) \cr
y''(\theta) &= 2 \cdot \theta \cos(\theta^2)
\end{array}$$ Knowing that the chain rule is $\vec{a} = \vec{q}'' \cdot \dot{\theta}^2 + \vec{q}' \cdot \ddot{\theta}$ and that $\vec{q}'$ and $\vec{q}''$ are orthogonal ( $\cos\alpha = 0$ ): $$\require{cancel}\begin{array}{c}
\vec{q}''(\theta) \cdot \vec{q}'(\theta) = \|\vec{q}''\| \cdot \|\vec{q}'\| \cdot \cancelto{0}{\cos 90} = 0 \cr \cr
\vec{a} \cdot \vec{q}' = \underset{0}{\underbrace{\vec{q}''(\theta) \cdot \vec{q}'(\theta) \cdot \dot{\theta}^2}} + \vec{q}'(\theta)^2 \cdot \ddot{\theta}(t) \cr \cr
\dfrac{\vec{F}}{m} \cdot \vec{q}'(\theta) = (0, g) \cdot (\cos(\theta^2), \sin(\theta^2)) = g \cdot \sin(\theta^2)
\end{array}$$ By combining the expressions above (and knowing that the denominator $\sin^2(\theta^2) + \cos^2(\theta^2) = 1$ ): $$\require{cancel}\begin{array}{c}
\cancelto{1}{\|\vec{q}\|(\theta)^2} \cdot \ddot{\theta}(t) = g \cdot \sin(\theta^2)
\end{array}$$ Why can't I apply the same method as explained by @Lutz Lehmann?? Here's the code for this equation: include math
# VARIABLES
fps = 30
g = - 9.8
ddtheta = 0

theta0 = 0
dtheta0 = 5
x0 = 0
y0 = 0
vx0 = math.cos(theta0**2)
vy0 = math.sin(theta0**2)

x1 = 0
y1 = 0
vx1 = 0
vy1 = 0
theta1 = 0
dtheta1 = 0

object.position[0] = x0
object.position[2] = y0

# ANIMATION
for i in range(1, 2000):
    t = 0.00003 # Small step size
    ddtheta = g * math.sin(theta0**2)
    dtheta1 = ddtheta * t + dtheta0
    theta1 = dtheta0 * t + theta0
        
    vx1 = math.cos(theta0**2)
    x1 = vx0 * t + x0
    vy1 = math.sin(theta0**2)
    y1 = vy0 * t + y0
    
    x0 = x1
    y0 = y1
    vx0 = vx1
    vy0 = vy1
    theta0 = theta1
    dtheta0 = dtheta1
    
    object.position[0] = x1 # X-axis
    object.position[2] = y1 # Y-axis ```","['ordinary-differential-equations', 'multivariable-calculus', 'calculus', 'numerical-methods', 'eulers-method']"
4597473,"Does there exist a group $G$, such that $G\otimes H \cong G$ for all finite groups, $H$?","A friend and I, whom only have elementary knowledge of group theory, were playing around some of the natural transformations between group theory and other categories. We probably shouldn't be playing around with as high-level questions as these without properly establishing our understanding beforehand, but it is in the sake of recreation. An obvious fact is that the trivial group behaves as a zero-object, in that $$\{e\}\otimes G\cong G$$ for all groups, $G$ (where $\otimes$ is the direct product). The next idea was if there were something akin to an additive identity. This may have a trivial answer, but it evades me at the moment, Does there exist a group $G$ (possibly infinite), such that $G\otimes H \cong G$ for all finite groups, $H$ ? Originally, I suspected that such a $G$ probably doesn't exist, on behalf of the fact that it would have to behave like the empty-set, which is not a group. However, akin to the fact that $\infty$ in the extended real numbers also behaves like an additive identity, I wonder if there exists an infinite group which behaves as an absorbing identity to finite groups (I'm not sure if the finitude of $H$ is neccesary, assuming it exists, but it seems like a intuitive restriction). If not, I suppose this idea could be salvaged into, Does there exist a group $G$ , such that $G * H \cong H$ for all finite groups, $H$ ? where $*$ is the free product, considering that it is indeed the coproduct in $\text{Grp}$ . The trivial group, might trivially have the property that $\{e\} * H \cong H$ , but my understanding of what a free product is, is very limited. Any and all insight would be greatly appreciated, despite the possibility that this is simply a silly idea, and I am a little out of my pay-grade.","['direct-product', 'group-theory', 'group-isomorphism', 'infinite-groups']"
4597499,Solution to Kepler ODE,"I'm working on Problem 8.20 from Teschl's ODEs book which asks to solve the ODE $$\frac{1}{r^2} \frac{dr}{d\varphi} = \sqrt{\frac{2 (H - \frac{1}{r})}{J_0^2} - \frac{1}{r^2}}$$ (where $H, J_0$ are constant with respect to $\varphi$ ). The hint is to substitute $\rho = r^{-1}$ and I did that and got $$\frac{d\rho}{d\varphi} = - \sqrt{\frac{2 (H - \rho)}{J_0^2} - \rho^2}$$ This is separable, but I'm stuck trying to integrate the reciprocal of the righthand side (I tried a trigonometric substitution and it didn't seem to help). The claimed answer is $$r (\varphi) = \frac{J_0^2}{1 - \sqrt{1 + H J_0^2} \cos (\varphi - \varphi_0)}$$","['calculus', 'ordinary-differential-equations']"
4597554,Generating Functions and Equivalent Sums,"Source : Somewhere in the DLMF the below was quoted without proof or reference (I forgot the exact location sorry) Prove that for $|q|<1$ , $$\sum\limits_{m=1}^\infty \frac{q^{m(m+1)/2}}{(1-q)(1-q^2)\cdots (1-q^m)}=\sum\limits_{m=1}^\infty q^m(1+q)\cdots(1+q^{m-1}).$$ Attempt : It is easy to see, denoting $S(n,m)$ as Stirling numbers of the second kind, that $$\sum\limits_{n=0}^\infty S(n,m)q^n=\frac{1}{(1-q)\cdots(1-q^m)},$$ so $$\sum\limits_{m=1}^\infty \frac{q^{m(m+1)/2}}{(1-q)(1-q^2)\cdots (1-q^m)}=\sum\limits_{m=1}^\infty \sum\limits_{n=0}^\infty S(n,m)q^nq^{m(m+1)/2}.$$ Moreover, we have $$\prod\limits_{i=1}^\infty (1+q^i) = \prod\limits_{i=1}^\infty \frac{1-q^{2i}}{1-q^i}=\prod\limits_{\text{odd } i} \frac{1}{1-q^i}=\sum\limits_{m=0}^\infty \sum\limits_{n=0}^\infty S(n,m)q^n$$ which maybe helps us manipulate the RHS but I got stumped here. Question : Any help for proving the identity would be very much appreciated! Bonus points if it is somewhat in the neighborhood of my very imcomplete thought process.","['combinatorics', 'generating-functions', 'sequences-and-series', 'stirling-numbers', 'bessel-functions']"
4597582,Does the distribution of the maximum increase when adding independent Gaussian processes?,"Let $x(t)$ and $y(t)$ be independent, mean-zero Gaussian processes, indexed over some general metric space $T$ . Is it true that $\Pr(\sup_{t \in T} |x(t) + y(t)| > z) \ge \Pr(\sup_{t \in T} |x(t)| > z)$ for all $z > 0$ ?","['empirical-processes', 'brownian-bridge', 'brownian-motion', 'probability-theory']"
4597603,Examples of ODEs that are toy models for complicated phenomena in PDEs,"Let me elaborate on what I mean by the title of this question. In studying (specifically non-linear) PDEs, one finds that each term in the equations contributes a certain amount to the behavior of the solution and the resulting behavior of the said solution depends on the tension between these effects. Such effects are for example non-linearity, transport, diffusion, dispersion, and dissipation. I am interested in example where these phenomena are manifested in simpler ODEs, and these examples would then be considered toy models. One example would be a tension between linear dissipation (the first term) and non-linear growth (the second term) manifested in this ODE: $$
\dot{y}(t)=-y+y^2, y(0)=y_0\neq 0, 1.
$$ Let $y_0>0$ . This is well manifested in the solution, which takes the form $$y(t)=\left(1+e^t\left(\frac{1-y_0}{y_0}\right)\right)^{-1}$$ is globally defined for $t\ge 0$ if $y_0<1$ (the linear term dominates), and in fact $y(t)\to 0$ as $t\to\infty$ . However if $y_0>1$ , then the solution blows up at $t=-\log(1-1/y_0)$ , originating from the fact that the non-linear term is dominating. I am looking for similar examples where the solution to an ODE qualitatively represents the tension between effects seen in more complicated PDEs.","['partial-differential-equations', 'ordinary-differential-equations', 'real-analysis']"
4597604,Is this probability always $1$?,"Let $X$ be a random variable, uniformly distributed in the interval $(0,1)$ . Compute $P(X(1-X) < \frac14)$ . Attempt: $$P(X(1-X)<\tfrac14)=P(X-X^2<\tfrac14)=P(X^2-X+\tfrac14>0)=P((X-\tfrac12)^2>0)$$ Isn't this always $1$ though? I guess it's a trick question? (Because the rest of the problems basically steer you towards working with the density functions)","['statistics', 'solution-verification', 'probability']"
4597634,How do I convert this boolean expression to its DNF form?,"I am having trouble converting this to its Disjunctive Normal Form. (A ∨ ¬C ∨ D)∧ (B ∨ C ∨ ¬D) ∧(¬A ∨ B ∨ C)∧ (¬A ∨ B ∨ ¬D) The 3 variables that exist in the brackets, is confusing me. How do the steps look like when I am trying to expand 3 variables with another 3. Any help would be greatly appreciated. Kind regards","['discrete-optimization', 'boolean-algebra', 'discrete-mathematics']"
4597664,Does maximum likelihood estimation maximize probability really?,"In maximum likelihood estimation (for continuous rv) we would like to maximize some parameter of our density function such that at some fixed observations our density function is bigger or equal for same density function bur for all other parameters. I would like to say that if we maximize the likelihood function, the probability that our observations occur is maximized as well. But, I can’t really produce a proof of this (I can’t convince myself that around each sample point I can maximize its probability by maximizing the density function). If someone could explain this I would appreciate it","['statistics', 'approximation', 'parameter-estimation', 'probability-theory', 'probability']"
4597804,Expression of a metric tensor with constant curvature on surface,"Consider the plane $\mathbb{R}^2$ together with a metric $\mathsf{g}$ . Choosing suitable local coordinates $(x_1,x_2)$ , $\mathsf{g}$ takes the form : $$
\mathsf{g}=r(x_1,x_2)\left((dx_1)^2+\epsilon (dx_2)^2\right),\quad r>0\quad\textrm{and}\quad\epsilon = \pm1.
$$ Question : Assume that $\mathsf{g}$ has constant curvature $K$ , is there a closed form expression for $r(x,y)$ ? In particular in [1] (p. 331)  it is claimed $\mathsf{g}$ is isometrically equivalent to $$
\mathsf{g} =\frac{1}{\left(1+\frac{K}{4}\left((y_1)^2+\epsilon(y_2)^2\right)\right)^2}\left((dy_1)^2+\epsilon (dy_2)^2\right)
$$ Is this true ? and if so can somebody provides a reference or a proof of that result ? My work : When trying to solve the question myself, I'm stuck with the following non-linear PDE (Liouville equation) $$
\frac{\partial ^2r}{\partial (x_1)^2}r + \epsilon\frac{\partial ^2r}{\partial (x_2)^2}r
+\left(\frac{\partial r}{\partial x_1}\right)^2  - \epsilon\left(\frac{\partial r}{\partial x_2}\right)^2
= - K
$$ [1] George R. Wilkens. “Centro-Affine Geometry in the Plane and Feedback Invariants of Two-State Scalar
Control Systems”. Proceedings of Symposia in Pure Mathematics. Ed. by G. Ferreyra et al. Vol. 64. Amer-
ican Mathematical Society, 1998. doi: 10.1090/pspum/064/1654544.","['partial-differential-equations', 'curvature', 'differential-geometry']"
4597823,Euler equation of a function in a function.,"Let's assume you have some function $F = e^ab(c(t))$ , where $a \in \mathbb{R}$ and $b(c(t))$ is a function, also $c(t)$ is a function. To give the Euler equation of $c(t)$ we have to use: $$\frac{\partial F}{\partial c} - \frac{dF}{dt}(\frac{\partial F}{\partial \dot{c}}) = 0$$ The first element is equal to: $$ \frac{\partial F}{\partial c} = e^a\dot{b}(c(t))\dot{c}(t)$$ Now the question is what is the derivative of F w.r.t. $\dot{c}$ , I think it is $0$ can anyone confirm this?","['solution-verification', 'derivatives', 'ordinary-differential-equations', 'calculus-of-variations']"
4597839,How to show that $\int_0^\pi \arcsin{\left(\frac{\sin{x}}{\sqrt{5/4-\cos{x}}}\right)}dx=\frac{\pi^2}{4}$?,"I am trying to show that $$\int_0^\pi \arcsin{\left(\frac{\sin{x}}{\sqrt{5/4-\cos{x}}}\right)}dx=\frac{\pi^2}{4}$$ Context: I was working on another question (""Attempt $2$ "") and miscopied an integral, so I was trying to evaluate the integral in my question here. Anyway, now I'm intrigued by this integral, because my computer strongly suggests that it has a closed form, $\frac{\pi^2}{4}$ . My attempt: I have tried substituting $u=\cos{x}$ or $u=\frac{\sin{x}}{\sqrt{5/4-\cos{x}}}$ , and the half-angle tangent substitution , but they do not seem to work. (Ideally, there is an elementary solution, but any solution would be appreciated.)","['calculus', 'closed-form', 'definite-integrals', 'trigonometry']"
4597898,Family of transformations can I find a density function?,"Let's consider the family of transformations given by $$g_a(Y)=\begin{cases}
\frac{e^{aY}-1}{a} & \text{ for } a\neq 0 \\
Y & \text{ for } a=0 
\end{cases}$$ for $Y\in\mathbb{R}$ . Analogous to the estimation of the Box-Cox parameter $\lambda$ , the parameter $a\in\mathbb{R}$ can be estimated using a profile likelihood approach. Let $Y_1,...,Y_n\in\mathbb{R}$ be independent
responses together with corresponding predictors $\mathbf{x}_1,...,\mathbf{x}_n\in\mathbb{R}^n$ . We assume for $a$ that there exist $\textbf{b}\in\mathbb{R}^p$ and $\sigma^2>0$ such that $g_a(Y_i)\sim N(\textbf{x}_i^T \textbf{b},\sigma^2)$ for $i=1,...,n$ . Here is my questions: Can we derive such a density function $f_Y$ of the untransformed observations $Y_i$ . If so, would it be $$f_{Y_i}(\textbf{x})=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{1}{2}\cdot \frac{(\textbf{x}-\textbf{x}_i^T\textbf{b})^2}{\sigma^2}}$$ How would we find the log-likelihood function $\ell(a,b,\sigma^2;y_1,...,y_n)$ ? I know that $\sum_{i=1}^n \log(f_{Y_i}(y_i))$ . Thanks in advance.","['statistics', 'transformation', 'normal-distribution', 'log-likelihood']"
4597929,"In any triangle $\triangle ABC$, show that $4R\sin(\frac{A}{2})\sin(\frac{B}{2})\sin(\frac{C}{2})=r$","This is a problem problem I found in a JEE examination prep textbook, it was a ""starred"" question which I believe implies that it is more challenging than usual. It goes as follows: In any triangle $\triangle ABC$ , show that $$4R\sin\left(\frac{A}{2}\right)\sin\left(\frac{B}{2}\right)\sin\left(\frac{C}{2}\right)=r$$ Hint: $$2R^2\sin\left(A\right)\sin\left(B\right)\sin\left(C\right)=\Delta$$ Here is my attempt at it. I want to know if this is correct and if there any better alternative approaches to achieve the same result, please do share them! We know that: $$\Delta=rs$$ Using the given hint: $$2R^2\sin\left(A\right)\sin\left(B\right)\sin\left(C\right)=rs$$ $$16R^2\sin\left(\frac{A}{2}\right)\cos\left(\frac{A}{2}\right)\sin\left(\frac{B}{2}\right)\cos\left(\frac{B}{2}\right)\sin\left(\frac{C}{2}\right)\cos\left(\frac{C}{2}\right)=r\left(\frac{a+b+c}{2}\right)$$ $$16R\sin\left(\frac{A}{2}\right)\cos\left(\frac{A}{2}\right)\sin\left(\frac{B}{2}\right)\cos\left(\frac{B}{2}\right)\sin\left(\frac{C}{2}\right)\cos\left(\frac{C}{2}\right)=r\left(\sin\left(A\right)+\sin\left(B\right)+\sin\left(C\right)\right)$$ Now, focusing on the equation of the right hand side for a bit, we know that: $$A+B+C=\pi$$ $$\frac{A+B}{2}=\frac{\pi}{2}-\frac{C}{2}$$ $$\sin\left(A\right)+\sin\left(B\right)+\sin\left(C\right)=2\sin\left(\frac{A+B}{2}\right)\cos\left(\frac{A-B}{2}\right)+2\sin\left(\frac{C}{2}\right)\cos\left(\frac{C}{2}\right)$$ $$2\cos\left(\frac{C}{2}\right)\left(\cos\left(\frac{A-B}{2}\right)+\cos\left(\frac{A+B}{2}\right)\right)$$ $$4\cos\left(\frac{A}{2}\right)\cos\left(\frac{B}{2}\right)\cos\left(\frac{C}{2}\right)$$ Now substituting this back into the original problem: $$16R\sin\left(\frac{A}{2}\right)\cos\left(\frac{A}{2}\right)\sin\left(\frac{B}{2}\right)\cos\left(\frac{B}{2}\right)\sin\left(\frac{C}{2}\right)\cos\left(\frac{C}{2}\right)=\left(4r\right)\left[\cos\left(\frac{A}{2}\right)\cos\left(\frac{B}{2}\right)\cos\left(\frac{C}{2}\right)\right]$$ And that gives us: $$4R\sin\left(\frac{A}{2}\right)\sin\left(\frac{B}{2}\right)\sin\left(\frac{C}{2}\right)=r$$","['euclidean-geometry', 'geometry', 'solution-verification', 'trigonometry', 'algebra-precalculus']"
4598013,Maximum error of cylinder volume with percentages,"This question about maximum error is giving me a hard time, you can see what I tried to do to solve it just below. Q: The height and radius of a circular cylinder are measured with possible errors of 2% and 4%, respectively. Aproximate the maximum error percetange in calculating the cylinder volume. I've tried using dV here, pluggin in the values 0,02 and 0,04 and then $$ P = \frac{dV}{V} $$ to get to the result but it's wrong. I don't know what else to try as this particular section is new to me. Help would be greatly appreciated. Edit: the correct answer is 10%","['partial-derivative', 'multivariable-calculus', 'calculus']"
4598083,How to find $\lim_{n\to\infty}\Big(\sum_{k=1}^n\sqrt{\frac{1}{k}-\frac{1}{n}}-\frac{\pi}{2}\sqrt n\Big)$?,"I'm looking for the asymptotic of the series $\displaystyle S(n)=\frac{1}{2}\sum_{k=1}^n\int_k^n\frac{dx}{x\sqrt{x(n-x)}}\,$ at $\,n\to\infty$ The first term can be found, for example, via switching to Riemann sums, changing the order of integration, and is equal to $\frac{\pi}{2}$ . To find the next term I performed integration and got $$S(n)=\frac{1}{n}\sum_{k=1}^{n-1}\sqrt{\frac{n}{k}-1}=\frac{1}{\sqrt n}\sum_{k=1}^{n-1}\sqrt{\frac{1}{k}-\frac{1}{n}}=\frac{1}{\sqrt n}\sum_{k=1}^{n-1}f(k)\tag{1}$$ To get the flavour of the next asymptotic term I used the Euler-Maclaurin formula $$\sum_{k=1}^{n-1}f(k)\sim\int_1^{n-1}f(k)dk+\frac{1}{2}\big(f(1)+f(n-1)\big)+\frac{1}{12}\big(f'(n-1)-f'(1)\big)+ ...\tag{2}$$ At $n\to\infty$ the first term (the integral) gives $\frac{\pi}{2}\sqrt n-2$ ; other terms in (2) give non-zero values at $k=1$ . Evaluating a couple of such terms, I got $$\sum_{k=1}^{n-1}f(k)\sim\frac{\pi}{2}\sqrt n-2+\frac{1}{2}+\frac{1}{24}=\frac{\pi}{2}\sqrt n-1.4583$$ The numeric evaluation at WolframAlpha for $n=1000$ gives $$ \sum_{k=1}^{n-1}f(k)-\frac{\pi}{2}\sqrt n\,\bigg|_{n=1000}=-1.46046$$ All this strongly resembles $\displaystyle \zeta\Big(\frac{1}{2}\Big)=-1.46035...\,\,$ ; at $\,n=10000\,$ we get even better agreement. Questions: How can we prove that $\,\,\displaystyle \lim_{n\to\infty}\Big(\sum_{k=1}^n\sqrt{\frac{1}{k}-\frac{1}{n}}-\frac{\pi}{2}\sqrt n\Big)=\zeta\Big(\frac{1}{2}\Big)\,\,$ ? Can we get next asymptotic terms (at least, several of them) analytically ?","['asymptotics', 'real-analysis', 'sequences-and-series', 'limits', 'zeta-functions']"
4598112,Using the Identity Theorem to prove $\sin^2 z+ \cos^2 z =1$,"I have a question. Using Identity theorem, we can give proof of the standard trigonometric identities; in particular, $\sin^2 z+ \cos^2 z =1$ . My approach: Let $f(z) = \sin^2 z+ \cos^2 z -1$ , so $f$ is an entire function. We know that $\sin^2 x+ \cos^2 x -1 =0$ for every real number. Let $x_0$ be a real number. Then any neighborhood of $x_0$ contains a point other that $x_0$ for which $\sin^2 x+ \cos^2 x -1 =0$ . So, $x_0$ is a limit point of zero set, $Z(f)$ in $\mathbb{C}$ . So, by the identity theorem $f=0$ throughout $\mathbb{C}$ . So $\sin^2 z+ \cos^2 z =1$ . Am I correct?","['complex-analysis', 'solution-verification']"
4598153,"Why is $E[X|X+Y] = E[Y |X+Y]$ if X,Y are i.i.d random variables","In proof of the fact that $E[X|X+Y] = \frac{X+Y}{2}$ when $X,Y$ are independent, identically distributed random variables, one uses the observation that $E[X|X+Y] = E[Y|X+Y]$ but I don't see why this is the case. Is there an easy proof of this statement?","['lebesgue-integral', 'conditional-expectation', 'probability-theory', 'probability', 'random-variables']"
