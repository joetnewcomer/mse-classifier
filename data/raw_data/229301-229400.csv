question_id,title,body,tags
4749466,Geometric intuition behind Noetherian rings?,"In Algebraic Geometry, there is this nice interpretation for the localization of rings and for quotient rings. Let $A$ be a ring. Then, localizations of $A$ at different elements correspond to the distinguished open subsets of Spec $(A)$ , and quotients of $A$ over different ideals correspond to closed subschems of Spec $(A)$ . Thus in broad terms, it is helpful to think of localization as opens, and of quotients as closed. This interpretation helped me understand why many properties of rings work well with localization but do not with quotients (e.g. being a UFD). This makes sense to me because we usually understand the elements in $A$ as functions on Spec $(A)$ , and functions behave better on open sets than on closed sets. However, I have trouble understanding Noetherian rings with this intuition, as this property works well both with localizations and quotients. Is there any geometric intuition behind why Noetherian rings work well with both open and closed sets?","['algebraic-geometry', 'commutative-algebra', 'noetherian']"
4749474,"Let $a+b+c=3,$ then prove$\sqrt[3]{\frac{a+b}{5ab+4}}+\sqrt[3]{\frac{c+b}{5cb+4}}+\sqrt[3]{\frac{a+c}{5ac+4}}\ge \sqrt[3]{6}$","Problem If $a,b,c\ge 0: a+b+c=3,$ then $$\sqrt[3]{\frac{a+b}{5ab+4}}+\sqrt[3]{\frac{c+b}{5cb+4}}+\sqrt[3]{\frac{a+c}{5ac+4}}\ge \sqrt[3]{6}.\tag{1}$$ I came up with when trying to prove $$\frac{a+b}{5ab+4}+\frac{c+b}{5cb+4}+\frac{a+c}{5ac+4}\ge \frac{2}{3},$$ which is not hard by $uvw$ method.
I'm looking for a nice proof $(1).$ It might be useful if the solver give motivate path. Thank you for helping.","['uvw', 'inequality', 'multivariable-calculus', 'symmetric-polynomials', 'algebra-precalculus']"
4749481,Relationship between gradient and position vector,"When we take a derivative of the function f(x) with respect to x we find out how much an infinitely small change in x will change f(x) . When we take the gradient of a multivariable function say f(x,y,z) we define it as $$ \nabla.f(x,y,z) = \frac{\partial f(x,y,z)}{\partial x} \hat{i} + \frac{\partial f(x,y,z)}{\partial y} \hat{j} + \frac{\partial f(x,y,z)}{\partial z} \hat{k}
$$ and if the position vector is given by $ \vec{r}(x,y,z) = x\hat{i} + y\hat{j} + z\hat{k} $ Then is it safe to correlate that the gradient is the directional derivative of the function with respect to the position vector $\vec{r}$ If I am wrong can you explain in detail why ?","['multivariable-calculus', 'vectors', 'vector-analysis']"
4749483,Adjoint functors in analysis on manifolds,"I am doing some work on the topic ""Adjunction"" and my current interest is to give some examples. I've wanted to provide some examples connected to some basic concepts from analysis on manifolds if such exists. What I thought maybe would work is the following: Tangent space functor and cotangent space functor Tensor product functor and multilinear form functor Pushforward and pullback of differential form I am not sure if any of these are indeed adjoint functors, so if they are I would appreciate any help in proving such a thing.","['manifolds', 'smooth-manifolds', 'category-theory', 'differential-geometry']"
4749503,Group Structure of the $2\times2$ Matrix Sphere,"I define the $2\times2$ matrix sphere as the quotient set $$\boxed{\mathbb{S}_{2}:=\{(X,Y)\in M_2(\mathbb{R})^2:X^2+Y^2=I\}/\sim}$$ where $M_2(\mathbb{R})^2=\mathbb{R}^{2\times2}\times\mathbb{R}^{2\times2}$ is the set of pairs of $2\times2$ real matrices and $\sim$ is the equivalence relation $$(A,B)\sim(C,D)\iff\exists P\in GL_2(\mathbb{R}):(C,D)=(PAP^{-1},PBP^{-1})$$ The motivation comes from generalizing the $1$ -sphere $S^1=\{(x,y)\in\mathbb{R}^2:x^2+y^2=1\}$ to $2\times2$ matrices identifying those pairs that are essentially the same since every matrix solution $(A,B)$ of $X^2+Y^2=I$ leads to an infinite amount of solutions as $$\forall P\in GL_2(\mathbb{R}):(PAP^{-1})^2+(PBP^{-1})^2 = P^2(A^2+B^2)P^{-2}=P^2IP^{-2}=I$$ Q: My question then is, analogous to the group structure of $(S^1,\cdot)$ considering it as a subgroup of $\mathbb{C}^\times$ , is there any natural operation $\odot:\mathbb{S}_{2}\times\mathbb{S}_2\to\mathbb{S}_2$ that $(\mathbb{S}_2,\odot)$ is a group? The thing with $\mathbb{S}_2$ is that it is not abelian. So, if we considered the operation $$\odot:[M_2(\mathbb{R})^2/\sim]\times[M_2(\mathbb{R})^2/\sim]\to M_2(\mathbb{R})^2/\sim$$ $$(A,B)\odot(C,D):=(AC-BD,AD+BC)$$ which is an intuitive generalization of the complex product, $\mathbb{S}_2$ is not closed under $\odot$ so $(\mathbb{S}_2,\odot)$ is not a group. In fact, if we define the norm $N:M_2(\mathbb{R})^2\to M_2(\mathbb{R})$ as $N(A,B)=A^2+B^2$ , then $\forall (A,B),(C,D)\in\mathbb{S}_2$ $$N((A,B)\odot(C,D))=N(A,B)N(C,D)\iff ACBD+BDAC=ADBC+BCAD$$ So $N$ is only multiplicative given that condition. I don't know if this helps but it is easy to see that the orthogonal group $\mathcal{O}_2=\{Q\in\mathbb{R}^{2\times2}:Q^TQ=I\}$ acts on $\mathbb{S}_2$ as $$\begin{pmatrix}\alpha& \beta\\ \gamma& \delta\end{pmatrix}(A,B)=(\alpha A+\beta B,\gamma A+\delta B)$$","['matrices', 'group-theory', 'matrix-equations']"
4749508,"Simplifying $3S_1 + 2S_2 + 2S_3$, where $S_1=2\sum_{k=0}^n16^k\tan^4{2^kx}$, $S_2=4\sum_{k=0}^n16^k\tan^2{2^kx}$, $S_3=\sum_{k=0}^n16^k$","If $$S_1=2\sum_{k=0}^n 16^k \tan^4 {2^k x}
$$ $$S_2=4\sum_{k=0}^n 16^k \tan^2 {2^k x}
$$ $$S_3= \sum_{k=0}^n 16^k
$$ Find $(3S_1 + 2S_2 + 2S_3)$ as a function of $x$ and $n.$ In the expression asked it rearranges to $$2[(\tan^2x+1)(3\tan^2x+1)+(\tan^22x+1)(3\tan^22x+1)\cdots]$$ Now this is not simplifying further, I tried in converting $\tan$ to $\sec$ but no approach still found. Maybe there is some special series devoted to summation of $\tan^2$ and $\tan^4$ about which I’m not aware.
I also thought differentiating twice the expression $$\log(\cos (x)\cos(2x)\cos(2^2x)\cdots\cos(2^nx))=\log\left(\frac{\sin(2^{n+1}x)}{2^{n+1}\sin(x)}\right)$$ so series of $\tan^2$ can be created by $\tan^2\theta+1=\sec^2\theta$ but the constant terms are coming wrong and series of $\tan^4$ is still far away.","['trigonometric-series', 'trigonometry', 'summation', 'sequences-and-series']"
4749546,Deriving the Oversampling formula,"I am familiar with the Shanon Sampling theorem, which states that: Let $f \in L_1(\mathbb{R})$ and $supp(\mathcal{F}f) \subseteq [-B,B]$ ,then $f(x)=\sum_{n \in \mathbb{Z}} f(\frac{n}{2B}) sinc(2B(x-\frac{n}{2B}))$ in the sense that the RHS converges to $f$ in $L_2(\mathbb{R})$ . This states that a function can be recoverd from the values $(f(\frac{k}{2B}))_{k \in \mathbb{Z}}$ . Now I heard that one can ""oversample"" to get better convergence.
I got a hint on how to do the prove:
Use a function $g$ , such that $\mathcal{F}g(\xi)=1$ on $[-B,B]$ and $\mathcal{F}g(\xi)=0$ on $|\xi| > B'$ My approach: Now assume $supp(\mathcal{F}f) \subseteq [-B,B]$ , and consider some $B'$ >B. One can write $f(x)=\int_{-B}^{B} \mathcal{F}f(\xi) e^{2 \pi i \xi x}d\xi$ Now if I choose a function $g$ , such that $g(\xi)=1 for \xi \in [-B,B]$ and $g(\xi)=0 for |\xi|>B'$ the above equals to $=\int_{-B'}^{B'} \mathcal{F}f(\xi) e^{2 \pi i \xi x}g(\xi)dx $ If I now consider the Fourier series of f on $(-B',B')$ I further get $= \int_{-B'}^{B'}\sum_{n \in\mathbb{Z}} \frac{1}{2B'} f(-\frac{n}{2B'}) e^{\frac{2 \pi i n \xi}{2B'}} g(\xi) e^{2 \pi i x \xi} d \xi=$ $\sum_{n \in\mathbb{Z}} f(-\frac{n}{2B'}) \frac{1}{2B'}\mathcal{F}^{-1}(g)(x+\frac{n}{2B'})$ To sum it up, one expands $\mathcal{F}f$ as a Fourier series, recover Fourier coefficients and then do the inverse Fourier transform. Now it seems to come down to calculating $g$ .
For that I also got a hint: Assume $g$ has the form $g=\frac{1}{2a} \chi_{[-a,a]} \ast ... \ast \frac{1}{2a} \chi_{[-a,a]} \ast \chi_{[-B-ka,B+ka]}$ , where $\chi_{[c,d]}$ denotes the indicator function on some intervall $[c,d]$ That's where I am stuck. Alternatively if someone has another prove this would be fine too.","['sampling-theory', 'fourier-analysis', 'analysis']"
4749587,Prove $\mathcal T∘\mathcal R=\mathcal T∘\mathcal S$ for any $\mathcal T$ iff $\mathcal R∘\mathcal T=\mathcal S∘\mathcal T$ for any $\mathcal T$.,"I am trying to understand if for two binary relations $\mathcal R$ and $\mathcal S$ the following statements are equivalent. $\mathcal R=\mathcal S$ $\mathcal R\circ\mathcal T=\mathcal S\circ\mathcal T$ for any (possible) relation $\mathcal T$ $\mathcal R[X]=\mathcal S[X]$ for any (possible) $X$ $\mathcal R^{-1}=\mathcal S^{-1}$ $\mathcal T\circ\mathcal R=\mathcal T\circ\mathcal S$ for any (possible) relation $\mathcal T$ So if $1$ holds then trivially $2$ holds. Now if $2$ holds then for any $X$ we can consider the identity relation $\operatorname{id}_X$ so that the equality $$
\tag{2.1}\label{2.1}\mathcal R\circ\operatorname{id}_X=\mathcal S\circ\operatorname{id}_X
$$ holds. So by \eqref{2.1} we argue that the equality $$
\mathcal R[X]=\mathcal R\big[\operatorname{id}_X[X]\big]=(\mathcal R\circ\operatorname{id}_X)[X]=(\mathcal S\circ\operatorname{id}_X)[X]=\mathcal S\big[\operatorname{id}_X[X]\big]=\mathcal S[X]
$$ holds. Now if $(y,x)$ is in $\mathcal R^{-1}$ then $(x,y)$ is in $\mathcal R$ and so $y$ is in $\mathcal R[x]$ ; however, if by hypothesis the equality $$
\mathcal R[x]=\mathcal S[x]
$$ holds then $y$ is in $\mathcal S[x]$ and thus $(x,y)$ is in $\mathcal S$ : we conclude that $(y,x)$ is in $\mathcal S^{-1}$ which is in particular not empty. Moreover by analogous arguments we can see that if $\mathcal S^{-1}$ is not empty then even $\mathcal R^{-1}$ is not empty and in particular it contains $\mathcal S^{-1}$ . So by extensionality we conclude that $\mathcal R^{-1}$ and $\mathcal S^{-1}$ are equals. Now if $\mathcal R^{-1}$ and $\mathcal S^{-1}$ are equals then the equality $$
\mathcal R^{-1}\circ\mathcal T^{-1}=\mathcal S^{-1}\circ\mathcal T^{-1}
$$ holds so that even the equality $$
\mathcal T\circ\mathcal R=\big((\mathcal T\circ\mathcal R)^{-1}\big)^{-1}=(\mathcal R^{-1}\circ\mathcal T^{-1})^{-1}=(\mathcal S^{-1}\circ\mathcal T^{-1})^{-1}=\big((\mathcal T\circ\mathcal S)^{-1}\big)^{-1}=\mathcal T\circ\mathcal S
$$ holds. Finally if 5. holds then the equality $$
\mathcal T^{-1}\circ\mathcal R=\mathcal T^{-1}\circ\mathcal S
$$ holds so that as above it is not complicate to conclude that the equality $$
\mathcal S^{-1}\circ\mathcal T=\mathcal R^{-1}\circ\mathcal T
$$ holds. So as in 2. we conclude that $$
\mathcal R^{-1}[X]=\mathcal S^{-1}[X]
$$ for any set $X$ . Finally as in $3$ we conclude that $$
\mathcal R=\mathcal S
$$ So first of all I ask if actually statements 1-5 are equivalent and so I ask if I well proved it. Specifically I ask to prove it more using a more clear argument since it seems to me it would be better to prove $1\to 5\to 2\to 3\to 4\to 1$ but unfortunately I am not able to prove $5\to 2$ . So couls someone help me, please?","['relations', 'examples-counterexamples', 'alternative-proof', 'solution-verification', 'elementary-set-theory']"
4749609,Variables as place-holders and the set builder notation,"In logic, variables don't have a meaning by themselves. They simply hold the place in an expression where the names of objects could be present. Adopting this placeholder viewpoint, I'm having trouble interpreting the set builder notation. Consider, for instance, the set of all natural numbers less than 10, denoted in the set builder notation as $$\{x:x<10\}$$ A common interpretation of this notation is The set of all x such that x is less than 10 This interpretation IMO doesn't go well with the placeholder viewpoint of variables, as when we replace ' x ' by say '2', we get the expression The set of all 2 such that 2 is less than 10 which doesn't make sense. So my question is how should I interpret $\{x:x<10\}$ from a placeholder viewpoint of variables? I can understand only the $x<10$ part from the placeholder viewpoint of variables as it is clearly an open sentence that gives a statement upon replacement of 'x' by appropriate constants. As an extension of this question, a similar issue arises when the domain of 'x' is specified in the set builder notation as: $$\{x \in N :x<10\}$$ This is often interpreted as The set of all x in N such that x is less than 10 Adopting the placeholder viewpoint, replacing 'x' by say '3' yields, The set of all 3 in N such that 3 is less than 10 Again only the '3 is less than 10' part of the expression makes sense.","['elementary-set-theory', 'predicate-logic', 'logic']"
4749615,Solution of a function: Why is it the point where it crosses zero?,"This is my first question here ever (not sure how basic this might be): Ive been studying set theory on my own (I have a couple of books and a lot of youtube videos). I understand that a function is ""something"" that relates the elements from one set to another. Therefore, a function will ""link"" an $x_1$ to a given $f(x_1)$ . I know there are certain functions that don't allow certain $x_n$ 's to be inputted and/or some $f(x_n)$ 's that will not be outputted. I got curious regarding the graphic representation of a function. Let's say we have the function $f(x)=x^2-1$ which has the following graphic representation: (I want to highlight where it crosses the x-axis, given it is a second order equation, it crosses it in two points. If it were $f(x)=x^2+1$ I know that the solution is in the complex realm.) I've always been taught that the ""solution"" of a function is where it crosses the $x$ -axis. In this case the function will have those $2$ solutions. My question/confusion is: What does the ""solution"" of a function mean, like...why is the relationship with zero important? Because if I think of it from the set theory approach, there is no ""solution"" (not sure about this though, I'm literally sharing my mind right now) it is just a relationship. I saw something regarding that ""...it is the  of the function"" I've googled that term but didn't quite found something that was clear enough for me. Why is the ""solution"" of a function where the $f(x)$ equals zero? Thanks for reading, responses and patience :)","['elementary-set-theory', 'functions']"
4749631,proving angle formed by simson line equals another angle,"P is a point on the circumcircle of $\triangle{ABC}$ and $O$ is its
circumcentre. Prove that $\angle{APO} =$ the angle between the Simson
line of $P$ and $BC$ . Here's a diagram I drew on geogebra of the problem, where $NLM$ is the Simson line. I'm trying to prove the red angles are equal (the 22.8 is arbritrary, just for me to mark the angles.) I've tried creating an isoceles triangle by constructing $OA$ , but I'm really not sure how to angle chase this.  I think I might need to use subtended angles with cyclic quadrilaterals, or maybe some constructions? I'm not able to connect the dots though. Could someone lead to the right direction?","['euclidean-geometry', 'triangles', 'angle', 'geometry']"
4749667,A singular average integral,"Let $f(\theta, \epsilon)$ be continuous on the unit circle for $\epsilon>0$ , $f(\theta, \epsilon)>0$ for $\epsilon>0$ , and $f(\theta, 0)=(\theta -\theta^*)^2h(\theta)$ with $h(\theta)>0$ for all $\theta \in [0,2\pi]$ . Is the following true? $$\lim _{\epsilon \rightarrow 0}\big(\int_0^{2\pi}\frac{\sin \theta}{f(\theta, \epsilon)}d\theta\big)\big(\int_0^{2\pi}\frac{1}{f(\theta, \epsilon) } d \theta\big)^{-1}=\sin (\theta^*).$$ Intuitively, the limit should give us the weighted average, and if we have a singularity the average should be $\sin \theta^*$ . I am not sure how to prove this.","['analysis', 'real-analysis', 'multivariable-calculus', 'calculus', 'functional-analysis']"
4749685,Evaluate $\lim_{n\to\infty}\frac{12^n\cdot(n!)^5}{(n^2+n)^{2n}\cdot(n+1)^n}$,Evaluate the following limit: $$\lim_{n\to\infty}\frac{12^n\cdot(n!)^5}{(n^2+n)^{2n}\cdot(n+1)^n}$$ The factorial function is creating a problem for me here. I can manage all other terms (by clubbing them together) but what to do with the factorial function. I also thought of taking logarithm on both sides after assuming the limit as $y$ but that didn't help me much. Any help is greatly appreciated.,"['limits', 'calculus', 'factorial', 'real-analysis']"
4749702,Proving that $\frac{\sin(t\theta)}{\sin(\theta)}\leq t$ for $t > 0$,"Let $t > 0$ . I am trying to show that then $\frac{\sin(t\theta)}{\sin(\theta)}\leq t$ . I am under the assumption that this shouldn't be too hard, which leads me to believe that I must be missing something obvious as I am stuck with the proof. One observation is that you can show that $\lim_{\theta\to 0}\frac{\sin(t\theta)}{\sin(\theta)} = t$ with L'Hopital's rule. So it would suffice to show that for no $\theta\in\mathbb{R}$ , $\frac{\sin(t\theta)}{\sin(\theta)} > t$ .","['trigonometry', 'real-analysis']"
4749793,"Calculating $\mathbb E[X|X^2+Y^2]$ with $X,Y\overset{iid}{\sim}\mathcal N(0,1)$","How can one calculate the following conditional expectancy : $\mathbb E[X\mid X^2+Y^2]$ where $X,Y\overset{iid}{\sim}\mathcal N(0,1)$ $?$ What I tried was to write that $\mathbb E[X\mid X^2+Y^2]=h(X^2+Y^2)$ where $\forall s\ge0$ , $f(s)=\mathbb E[X\mid X^2+Y^2=s]$ . This quantity is calculable as : $$h(s)=\int_\mathbb R xf_{X\mid X^2+Y^2=s}(x)dx$$ where the conditional density $f_{X\mid X^2+Y^2=s}$ verifies $\forall x\in\mathbb R$ , $f_{X\mid X^2+Y^2=s}(x)=\dfrac{f_{(X,X^2+Y^2)}(x,s)}{f_{X^2+Y^2}(s)}$ . It is known that in this situation we have $f_{X^2+Y^2}(s)=\frac{1}{2}e^{-\frac s2}\mathbb 1_{s>0}$ ( $X^2+Y^2\sim\chi^2(k))$ and by taking $\varphi:\mathbb R^2\rightarrow\mathbb R$ to be continuous and bounded, we have : \begin{eqnarray*}
\mathbb E[\varphi(X,X^2+Y^2)]&=&\int_{\mathbb R^2}\varphi(x,x^2+y^2)\frac{1}{2\pi}e^\frac{-(x^2+y^2)}{2}dxdy\\
&=&\int_{\mathbb R}\int_{s^2}^\infty\varphi(s,t)\frac{1}{2\pi}e^{-\frac t2}\frac{dsdt}{2\sqrt{t-s^2}}\quad\text{by letting $(s,t)=(x,x^2+y^2)$}
\end{eqnarray*} hence $f_{(X,X^2+Y^2)}(x,s)=\dfrac{1}{4\pi\sqrt{s-x^2}}e^{-\frac s2}\mathbb 1_{s>x^2}$ which yields $f_{X\mid X^2+Y^2=s}(x)=\dfrac{1}{2\pi\sqrt{s-x^2}}\mathbb 1_{s>x^2}$ and finally : $$h(s)=\int_{-\sqrt s}^{\sqrt s}\frac{x}{2\pi\sqrt{s-x^2}}dx=0\quad\text{by imparity}$$ and therefore $\mathbb E[X\mid X^2+Y^2]=0$ . This result does intuitively make sense (having information on $X^2+Y^2$ yields no information about the sign of $X$ , and therefore since $X$ is symmetric about $0$ , we could expect the conditional expectancy to be $0$ ). Therefore, one could be tempted to ask the following question : is the result still true if we only assume $X$ and $Y$ to be independent ant and symmetric about $0$ ? The problem is that with the method presented here, it isn't possible to answer this question. So is there a more general method of calculating this conditional expectancy that would allow one to give an answer to the above question?","['conditional-probability', 'conditional-expectation', 'probability-theory', 'probability']"
4749800,Can $f([x])$ be continuous?,"There is a problem I encountered which has two functions $f(x)$ and $g(x)$ such that $g(f(x))=x$ and $f(\lfloor g(x) \rfloor)=x, \quad \forall x\geq 0$ . I am not giving any other details here, because what I want to know is how it is possible that $f(\lfloor x \rfloor)=x$ ? All I know by graphical transformation of $f(x)\to f(\lfloor x \rfloor)$ that the graph obtained is piecewise, not continuous. So how is this case possible?
One situation I could think of is $f(x)=x, x \in \mathbf Z$ . But here, the domain is $\mathbf R$ . I would like to know other simple cases when it could be possible, so that it won't be hard for me to imagine such a function. Edit Here is the complete definition of function: $$f(x)=\begin{cases}
\sqrt{1-x^2},& -1\leq x < 0\\
x^2+1,& 0\leq x < 1\\
\frac{(x-1)^2)}{4} +2, & x\geq 1
\end{cases}
$$ $g(x)$ is defined such that $g(f(x))=x, x \geq -1$ and $f(\lfloor(g(x)\rfloor)= x, x\geq0$","['ceiling-and-floor-functions', 'functions', 'graphing-functions']"
4749853,Proving that the projection map from Cartesian product is onto,"I am trying to prove the following theorem. Let $(X_i)_{i \in I}$ be a collection of nonempty sets. Using the Axiom of Choice, prove that for any $i_0 \in I$ , the projection map $$
\pi_{i_0}: \prod\limits_{i \in I} X_i \to X_{i_0}, \; (x_i)_{i \in I} \mapsto x_{i_0}. 
$$ If $I = \emptyset$ , the statement seems vacuously true, so I'm going to proceed under the assumption that $I$ is nonempty. I'm having some difficulty figuring out exactly where I can use choice. Fix $i_0 \in I$ , and let $y \in X_{i_0}$ . As $\prod\limits_{i \in I} X_i$ consists of all tuples $(x_i)_{i \in I}$ where $x_i \in X_i$ for each $i \in I$ , there exists some tuple consisting of $y \in X_{i_0}$ . The image of this tuple under $\pi_{i_0}$ is then $y$ , so $\pi_{i_0}$ is surjective. I feel as though this cannot be right. The problem makes clear I should be using the axiom of choice somewhere, which tells me that the product set is nonempty. Even if it is nonempty, however, that doesn't guarantee a tuple whose $i_{0}th$ position is $y$ . I'd appreciate some clarification on this. I'll update this post as I continue to work on this problem.","['elementary-set-theory', 'proof-explanation']"
4749880,Is there a mistake in a GRE preparation book?,"Preface: I am preparing for the GRE Math subject test, and one of the books that I use is this one by Charles Rambo. I have currently been solving through the “Linear Algebra #1” section of the book, and there is one problem that I think has an incorrect answer in the solutions section. Let me state the problem as it was drawn in the book: Let $V$ be the set of all possible polynomials $p(x)$ with coefficients in $\mathbb{R}$ . And let the linear operators $T$ and $S$ be defined over $V$ as follows: $$
T(p(x)) = xp(x)
\\
S(p(x)) = \frac{d}{dx}p(x) = p’(x) 
$$ We denote $ST$ and $TS$ as the expected compositions $S\circ T(p(x))$ and $T \circ S(p(x))$ respectively. Which of the following is true? $ST = 0$ $ST = T$ $ST = TS$ $ST - TS$ is the identity map of $V$ $ST + TS$ is the identity map of $V$ My attempt: In my opinion, the correct option is (4), since: $ST = \frac{d}{dx}\left[xp(x)\right] = x’p(x) + xp’(x) = p(x)+xp’(x)$ $TS = x \cdot \frac{d}{dx}p(x) = xp’(x)$ $F(p(x)) = (ST-TS)(p(x))$ is a transformation defined over $V$ , then $F(p(x)) = (ST - TS)(p(x)) = p(x)+xp’(x)-xp’(x) = p(x)$ , hence $F : V \mapsto V$ , in other terms $F$ forms an identity map over $V$ . Book’s solution: In fact, the only thing written in the book about this problem (page 84 in the pdf) was that the answer is (5), and that one had to use the product rule to obtain the correct answer (no explicit solution given). And so I wonder whether there is any fallacy in my logic above, or it is the book, which is mistaken. Any help will be greatly appreciated. Thank you in advance!","['gre-exam', 'solution-verification', 'linear-algebra', 'linear-transformations']"
4749900,Motivation for Connections over smooth manifolds,"I was reading about connections on Wikipedia and I found this section about ""Motivation"": My question is the following one. If I look at the vector bundle $E$ like a manifold, then locally I have charts and I can think of $X: M \rightarrow E$ like a smooth map between open subsets of $\mathbb{R}^n$ and $\mathbb{R}^m$ for some $n$ and $m$ . Hence, I should naturally have a definition of derivatives. Am I wrong? What is the point of that argument? Thanks in advance","['euclidean-geometry', 'connections', 'vector-bundles', 'derivatives', 'differential-geometry']"
4749905,Spectrum of $A + s N$ constant in $s$ when?,"Let $A$ , $N$ matrices in $M_n(\mathbb{C})$ such that the spectrum ( with multiplicities) of the matrix $A + s N$ does not depend on $s$ . I am trying to show that $A$ , $N$ can be simultaneously reduced to an upper triangular form, with $N$ strictly upper triangular. Notes: The hypothesis is equivalent to "" the characteristic polynomial of $A + s N$ does not depend on $s$ "". It is easy to see that condition is equivalent to : for every $1\le m \le n$ the trace $\operatorname{tr} ( A + s N)^m$ is constant in $s$ .  This implies right away that $\operatorname{tr} N^m = 0$ for all $1\le m \le N$ , so $N$ is nilpotent. It would be enough to show that $A$ and $N$ have a common eigenvector.  One could try to prove this: if $\det (A + s N) = 0$ for all $s$ , then $A$ , $N$ have a common zero eigenvector. That would in fact prove the result, so it might be a viable route.  (In fact not viable !, see below) Any feedback would be appreciated! $\bf{Added:}$ I forgot that $\det(A+s N) = 0$ for all $s$ does not imply that the kernels of $A$ , $N$ intersect non-trivially. There may be the issue that the images of $A$ , $N$ do not generate the whole space. $\bf{Added:}$ The assertion is wrong, as showed by the counterexample of @just a user. With Mathematica, I found some counterexample in size $3 \times 3$ as follows: take $A$ a Jordan cell of size $3$ ( the $\lambda$ will not matter).  Now find all those $N$ satisfying $$\operatorname{tr}(N, A N, N^2, A^2 N, A N^2, N^3) = 0$$ Clearly any such $N$ is nilpotent. However, it may happen that $A$ and $N$ do not share an eigenvector. A generic solution of such $N$ is $$N = \begin{pmatrix} q^2 r & q^2 s & r^3\\ - q^3 & - 2 q^2 r & q( - 3 r^2 + q s)\\0 & q^3 & q^2 r \end{pmatrix}$$ with unique eingenvector ( up to proportionality) $$v = ( r^2 - q s, q r, - q^2)$$ However, $v$ is not an eigenvector of $A$ . Hence, $A$ , $N$ cannot be simultaneously brought to an upper triangular form. $\bf{Added:}$ With the above $A$ ( for $\lambda= 0$ ) $N$ , we have the kernels of $A$ do not intersect, the ranges of $A$ together span the full space, yet $a A + s N$ is nilpotent for all $a$ , $s$ .","['matrices', 'abstract-algebra', 'linear-algebra']"
4749941,How separated is a submetrizable space?,"A space is submetrizable if its topology contains a coarser metrizable topology. Each such space is Hausdorff: given two points $x,y$ , the sets $B_{|x-y|/2}(x),B_{|x-y|/2}(y)$ are open in the space. How far can this be strengthened? To $T_3$ ? To functionally Hausdorff ? To $T_{2.5}$ ?","['general-topology', 'metric-spaces', 'separation-axioms']"
4750001,What is the asymptotic density and Lebesgue density of two sets which partition the reals into subsets of positive measure?,"Suppose we partition the reals into two sets $A$ and $B$ that are dense (with positive Lebesgue measure) in every non-empty sub-interval $(a,b)$ of $\mathbb{R}$ , where Lebesgue measure $\lambda$ restricts outer measure $\lambda^{*}$ to sets measurable in the Caratheodory sense . Does there exist an example (similar to this one ) where both: $$\lim\limits_{t\to\infty}\lambda(A\cap [-t,t])/(2t)$$ and $$\lim\limits_{t\to\infty}\lambda(B\cap [-t,t])/(2t)$$ are greater than zero but neither equal $1/2$ ? A bounty is also offered for this question here .","['measure-theory', 'analysis', 'real-analysis']"
4750024,How to prove this integral function is analytic?,"Given $$G(z) = \int_{1+i}^{z}\text{sin} (\theta^2) d\theta$$ Prove that $G(z)$ is an analytic function of $z$ . I read that integration preserves analyticity. But why is that true when $z$ is in the integral bounds?
What if the function is something different instead of $\text{sin}(\theta^2)$ , say $f(z)$ which is only analytic in a certain region $R$ and I integrate along a path that goes outside $R$ ?","['complex-analysis', 'analyticity']"
4750032,Is any $\sigma$-compact space a union of two countable unions of pairwise disjoint compact sets?,"Let $X$ be a $\sigma$ -compact topological space, meaning that $X = \bigcup_{i\in\mathbb{N}} K_i$ for a sequence of compact subsets $(K_i)$ . Does this already imply that $X$ can be written as a union $$\tag{1} X \,=\, A\cup B \qquad\text{with} \qquad A \,=\, \bigcup_j K^{a}_j \quad\text{and}\quad B \,=\, \bigcup_k K^{b}_k$$ where each $(K_\nu^{\gamma})_\nu$ [ $\gamma\in\{a, b\}$ ] is a sequence of pairwise disjoint compact sets?","['general-topology', 'compactness']"
4750065,Is there a Measure-Theoretic Proof of this new Result from Categorical Probability?,"Recently, I stumbled across a new paper in categorical probability . Interestingly, they prove a result which may be formulated in purely measure-theoretic terms about which they note that ""As far as we know, this strengthening is new,
and in particular no measure-theoretic proof exists to date."" This makes me wonder about whether one can prove this in a measure-theoretic way, without the tools from categorical probability they developed. The point of this post is to state the result and explain the definitions from a viewpoint of measure theory. My question then is: Can we prove this Theorem with purely measure-theoretic tools, meaning without the tools of categorical probability used in the paper? Main Theorem Every idempotent Markov kernel between standard Borel spaces is balanced and splits. For further context, this Theorem is a stronger version of the more well-known Theorem due to Blackwell, which the authors formulate as follows: For a reference, see ""Idempotent Markoff Chains"" by Blackwell . Definitions Standard Borel Space: We say that $(X,\Sigma)$ is a standard Borel space if $X$ is metrizable with metric $d$ such that $\Sigma$ is the Borel- $\sigma$ -algebra generated by the topology of $(X,d)$ and $(X,d)$ is separable and complete. Markov Kernel: Let $(X,\mathcal A)$ and $(Y,\mathcal B)$ be measurable spaces. A Markov Kernel from $(X,\mathcal A)$ to $(Y,\mathcal B)$ is a map $\kappa:X\times\mathcal B\rightarrow [0,1]$ such that For every $B\in\mathcal B$ the map $x\mapsto \kappa(x,B)$ is $\mathcal A$ -measurable. For every $x\in X$ the map $B\mapsto \kappa(x,B)$ is a probability measure on $(Y,\mathcal B)$ . Example: If we consider Markov Chains on a discrete space, say $S:=\{1,2\}$ , usually described by a transition matrix $$e:=\begin{pmatrix}\frac{1}{4} & \frac{3}{4} \\ \frac{2}{5} & \frac{3}{5} \end{pmatrix}$$ then our spaces are $X=Y=S$ with their $\sigma$ -algebras being the power set of $S$ . The associated Markov Kernel $\kappa$ then is given by $$\begin{align*}\kappa(1,\{1\})&= \frac{1}{4}, \hspace{1cm}\kappa(1,\{2\})=\frac{3}{4} \\ \kappa(2,\{1\})&=\frac{2}{5}, \hspace{1cm}\kappa(2,\{2\})=\frac{3}{5}\end{align*}$$ where one may interpret $\kappa(x,B)$ as $\mathbb P[X_1\in B|X_0=x]$ if $(X_n)_{n\in\mathbb N}$ is a Markov Chain with transition matrix $e$ . Composition of Markov Kernels Let $(X,\mathcal A), (Y,\mathcal B), (Z,\mathcal C)$ be three measurable spaces and let $\kappa$ be a Markov Kernel from $X$ to $Y$ , let $\lambda$ be a Markov Kernel from $Y$ to $Z$ . We define the composition of the two kernels $\lambda\circ \kappa$ by $$(\lambda\circ \kappa)(x,dz):=\int_Y \lambda(y,dz)\kappa(x,dy)$$ Example: If we consider again the case of a discrete state space, we may represent Markov Kernels as stochastic matrices. Composition of two Kernels then equates to the product of the corresponding matrices. We may now view Markov Kernels as morphisms. Idempotence: We say an morphism $e:X\rightarrow X$ is idempotent if it satisfies $e\circ e = e$ . Example: Examples of  idempotent transition kernels on the state space $S:=\{1,2\}$ would be $$e_1=\begin{pmatrix}1 & 0  \\ 0 & 1\end{pmatrix},\hspace{1cm}e_2=\begin{pmatrix}\frac12 & \frac12  \\ \frac12 & \frac12\end{pmatrix}$$ Splitting: Given an idempotent $e:X\rightarrow X$ , we say that $e$ splits if there exists a $T$ and morphisms $$\iota:T\rightarrow X, \hspace{1cm} \pi:X\rightarrow T$$ such that $$\pi\circ \iota =\text{id}_T, \hspace{1cm} \iota\circ \pi=e$$ Example: (Example 4.1.4.)
Consider the discrete state space $S:=\{1,2\}$ and define a Markov Chain on $S$ by the transition kernel $$e=\begin{pmatrix}\frac12 & \frac12 \\ \frac12 & \frac12\end{pmatrix}$$ Then for $$\iota:=\begin{pmatrix}\frac12 \\ \frac12 \end{pmatrix}, \hspace{1cm} \pi:=\begin{pmatrix} 1 & 1\end{pmatrix}$$ we see that $\pi\circ\iota =\text{id}$ as well as $\iota\circ \pi=e$ . (Here composition is considered as matrix-multiplication). Balanced: We say that $e$ is balanced if for every invariant distribution $\pi$ of $e$ , meaning $e\pi=\pi$ , we have that a Markov Chain with transition kernel $e$ and initial (stationary) distribution $\pi$ is reversible. (I am not entirely sure that I understood this definition correctly. Please take a look at Definition 4.1.1 and Proposition 4.1.10 in the paper and correct me if you feel like I am stating the wrong definition here)","['markov-process', 'category-theory', 'markov-chains', 'probability']"
4750082,Effects of perturbation on level set.,"Consider the function $f:\mathbb{R}^n\rightarrow \mathbb{R}$ , defined as $f:x\mapsto \|x\|^2_2$ . It's evident that the pre-image $f^{-1}(1)$ becomes $(n-1)$ -sphere $S^{n-1}$ . Consider a smooth $\epsilon$ -perturbation $g$ of $f$ in the uniform sense: $$\|f-g\|_{\operatorname{sup}}<\epsilon$$ I want to prove that $g^{-1}(1)$ can not be embedded in $\mathbb{R}^{n-1}$ if $\epsilon$ is a sufficiently small value. If $g^{-1}(1)$ is a manifold, it is clear because a compact $(n-1)$ -manifold can not be embedded in $\mathbb{R}^{n-1}$ . However, I am not sure that $g^{-1}(1)$ is a manifold because $1$ is not necessarily the regular value of $g$ . But it should be something very similar to $S^{n-1}$ because it divides the entire Euclidean space $\mathbb{R}^n$ into (at least) two components, $g^{-1}([0,1))\supset B_{1-\epsilon}(0) $ and $g^{-1}((1,\infty))\supset \mathbb{R}^n - B_{1+\epsilon}(0) $ . Any assistance you can offer would be greatly appreciated.","['geometric-topology', 'general-topology', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
4750129,Book that Develops the Theory of Tangent Space whilst Defining Tangent Vectors as Equivalence Classes of Curves,"Currently reading Lee's Introduction to Smooth Manifolds . At the end of chapter $3$ he mentions that tangent vectors may be defined in terms of equivalence classes of curves, but by that time he has developed the theory of the tangent space using Derivations, and thus does not delve further. Is there a book that develops the theory of tangent spaces whilst definiting tangent vectors as equivalence classes of curves?","['tangent-spaces', 'reference-request', 'smooth-manifolds', 'definition', 'differential-geometry']"
4750160,If $\cap_{n=1}^\infty A_n \subset B$ does it imply $A_m \subset B$ for $m$ large enough? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question Let $A_i$ be a sequence of non-empty monotonic decreasing open and bounded subsets $A_1\supset A_2\supset \cdots$ of $\mathbb{R}^n$ with a non-empty intersection.
If the intersection is compactly embedded in an open subset $B$ of $\mathbb{R}^n$ , i.e. $$\cap_{i=1}^\infty A_i \subset \subset B$$ does it imply $A_m \subset B$ for $m$ large enough?",['elementary-set-theory']
4750188,Differential Equation with $f'(t)=\cos(f(t))$ prove that $-2 < f(t) < 2$,I cannot solve the following exercise: Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be infinetly often differentiable with $f(0)=0$ and $f'(t)=\cos(f(t))$ . Prove that $-2 < f(t) < 2 $ for any $t \in \mathbb{R}$ . I saw that $|f'(t) \leq 1|$ and tried applying mean value theorem for $0$ and $t$ but still couldn't find a solution. I also tried to integrate but still could only show that $f(t) \leq t$ .,"['ordinary-differential-equations', 'real-analysis']"
4750202,Are vectors in one dimension scalar?,"A physics textbook that I use mentions that ""If a physical quantity has magnitude as well as direction but doesn't add up according to the triangle rule, it will not be called a vector quantity"". The paragraph further explains that electric current is not a vector quantity because there is no meaning of triangle law there. While I understand why electric current is a scalar quantity, I am unable to comprehend how definite the rule is. For all I think, in the case of a right angled triangle, the rule is just calculating the hypotenuse (resultant vector). In dimensions two and above, it would make complete sense. However, in one dimension  there isn't any angle involved and each point depends on only one real number. The two directions: left and right can be dealt with two sets of real numbers, i.e., negative and positive. I want to know why vectors in one dimension can't be written as scalars. Why they must be written in the form of $x\hat{\imath}+0\hat {\jmath}+0\hat k$ when they can be mentioned simply as the scalar number on the $x$ axis.","['linear-algebra', 'vectors']"
4750268,Pushforward under Segre Embedding,"This question originated from an answer to this post. Take $\mathcal{O}(1,0)$ on $\mathbf{P^1}\times \mathbf{P}^1$ and consider its push-forward $F$ on the quadric $\subset \mathbf{P}^3$ under the Segre embedding. Then there should be a resolution $0\rightarrow \mathcal{O}(-1)^2 \rightarrow \mathcal{O}^2\rightarrow F \rightarrow 0$ . There are powerful tools like the Tate resolution that allow us to get to this sequence, but I think this should be possible with more modest methods? Glad for any help. I thought about $\mathcal{O}(-1)^2 \xrightarrow{\begin{pmatrix} x &y \\
z & w\end{pmatrix}} \mathcal{O}^2$ which has I think a co-kernel where $\det=xw-yz$ is zero which is the equation of the Segre quadric...?","['algebraic-geometry', 'sheaf-theory']"
4750360,Exchangeability of an interacting particle SDE system,"Let $(W^1,\ldots,W^N)$ be $N$ independent Brownian motions, $\sigma > 0$ and $k$ a Lipschitz function. Consider the following system of SDE's \begin{equation}
\mathrm{d}X_t^{i} = -\frac{1}{N} \sum\limits_{j=1}^N k(X_t^{i}-X_t^{j}) \mathrm{d}t + \sigma \mathrm{d}W_t^{i}, \quad \quad i=1,\ldots, N,
\end{equation} which is subject to i.i.d. initial conditions. The system is well-defined and has a strong solution by the classic theory. Now, in every paper on interacting particle systems the author's claim that this system is exchangable. This seems reasonable. Unfortunately, I have never seen a complete proof of this statement and I am struggling with the proof myself. If you have an references or could provide a proof of the exchangability I would appreciate it! I think you need to use the pathwise uniqueness + Yamada-Watanabe Theorem, which provides a $\mathbb{R}^N$ -valued function of the form $h(X_0,W_{.})$ ,( $X_0,W$ are the vectors of the whole system).","['stochastic-differential-equations', 'systems-of-equations', 'probability-theory', 'probability']"
4750402,"When is ""do-almost-nothing"" a good idea in CHOMP?","Now asked at MO : The proof by strategy-stealing that CHOMP on a rectangular board is a first-player win involves player 1 taking the top-right square on their first move. Of course given the proof-by-contradiction nature of the argument, this does not mean that the top-right square is in general a good opening move, and there are plenty of rectangular boards on which it is in fact a losing move for the first player. Note: different versions of CHOMP place the ""poison square"" at different locations. For simplicity, I'm sticking with the convention from the above-linked notes. I'd like to know how often it is in fact a good move. (Note: by a strategy stealing argument, if it's winning in a given board it is in fact the only winning move in that board. I missed this at first.) There are a couple ways to phrase this problem: Question 1 : For fixed $n$ , let $c_n$ be the upper asymptotic density of the set of $m$ such that moving in the top-right square is the winning move for the first player on the $m$ -by- $n$ board. What is $c_3$ ? (It's easy to see that $c_1=0$ and $c_2=1$ , so this is the first interesting case. Of course the higher $c_i$ s are also interesting, but I worry that they are intractable.) Question 2 : Is it the case that, for every $k$ , there are $m,n>k$ such that the top-right square is the winning move for the first player in the $m$ -by- $n$ board? (Is this even true for $k=2$ ?)","['recreational-mathematics', 'combinatorics', 'combinatorial-game-theory']"
4750403,Why is $\frac{25}{2} \left(-2 + \pi -\tan^{-1}\frac{44}{117}\right)$ equivalent to $\frac{75}{2}\sin^{-1}\frac{4}{5}-25$?,"I was following along a MindYourDecisions problem on YouTube I flipped the image, split the problem in two sections, then feeling lazy I used Wolfram Alpha to do the calculus: left circle
x^2 + (y-5)^2 = 5^2
y = 5 - sqrt(25 - x^2)

bottom circle
(x-5)^2 + y^2 = 5^2
y = sqrt(10 x - x^2)

big circle
x^2 + (y-10)^2 = 10^2
y = 10 - sqrt(100 - x^2)

Part 1: Left area
integrate left - big from 0 to 5
integrate (5 - sqrt(25 - x^2)) - (10 - sqrt(100 - x^2)) from 0 to 5
(25 (6 (-2 + Sqrt[3]) + Pi))/12

Part 2: Intersection
bottom = big
sqrt(10 x - x^2) = 10 - sqrt(100 - x^2)
x = 0, x = 8

Part 3: Right area
bottom - big
integrate bottom - big from 5 to 8
integrate (sqrt(10 x - x^2)) - (10 - sqrt(100 - x^2)) from 5 to 8
(25 (5 Pi - 6 (Sqrt[3] + ArcTan[44/117])))/12

Part 1 + Part 3
(25 (6 (-2 + Sqrt[3]) + Pi))/12 + (25 (5 Pi - 6 (Sqrt[3] + ArcTan[44/117])))/12 This simplified to: 25/2 (-2 + π - tan^(-1)(44/117)) https://www.wolframalpha.com/input?i=25%2F2+%28-2+%2B+%CF%80+-+arctan%2844%2F117%29%29 Now, I checked the answer in the video: 37.5*arcsin(0.8)-25 https://www.wolframalpha.com/input?i=37.5*arcsin%280.8%29-25 That surprised me, I assumed I did something wrong. But numerically, they seem to be identical: 9.7735706750604587160692173595910801521402790714590197858199 How are these solutions connected? It doesn't ""feel"" right that 44/117 and 0.8 are connected.","['calculus', 'definite-integrals', 'trigonometry']"
4750431,Does every closed curve admit a line that intersects it at only one point?,"Have a very basic question on closed plane curves, from this Wikipedia math reference desk conversation , that I can't seem to come up with any results for. In particular: If $C : [0, 1] \rightarrow \mathbb{R}^{2}$ is a closed curve (not necessarily simple), then does there necessarily exist a line $L$ such that $L$ intersects the image of $C$ at exactly one point? If the question is rephrased to two intersections, then since simplicity is not required, space-filling curves serve as effective counterexamples. And, if the question is rephrased to two intersections plus simplicity, then it appears that the boundary of the dragon curve (or some dragon-like curve) serves as a counterexample.","['plane-curves', 'geometry', 'plane-geometry']"
4750436,Why does the exponent for time increase when we integrate acceleration to get velocity?,I'm trying to build an intuition for the relationship of integrals to the equations of motion. I was using this site to get an overall intuition for the relationship between integrals and derivatives. It seems clear how a cube changes in that the derivative is like adding 3 areas so that it grows by $3x^2$ . And the reverse is just integrating the rate to get the cube $x^3 + c$ . Now my confusion is when we consider movement. Acceleration has units in $m/s^2$ but velocity is not cubed (at least in my understanding). So why does the integral of acceleration with respect to time produce values with cubes? I feel like I'm missing something obvious here.,"['integration', 'self-learning', 'calculus', 'derivatives']"
4750465,A method to solve a system of nonlinear differential equations,"Is the system of ODE $$x'(t)=y$$ $$y'(t)=\frac{x^2}{2}$$ solvable through analytic means? I might be missing something, since by the chain rule the second equation gives the second order ODE $y''=xy$ , which doesn't seem to help.",['ordinary-differential-equations']
4750466,Understanding a Specific Instance of Leibniz's Law in Discrete Mathematics,"I'm currently studying discrete mathematics and working on exercises related to Leibniz's law: $$\frac{X=Y}{E[z:=X]=E[z:=Y]}$$ I've come across a particular problem that I'm struggling to understand: Below, are a number of instantiations of Leibniz, with parts missing. Fill in the missing parts and write down what the expression $E$ is. $$\frac{7=y+1}{7\cdot{x}+7\cdot{y}=?}$$ Here's what I've tried so far: Step 1. $$E=x\cdot z+y\cdot z$$ Step 2. $$\frac{7=y+1}{E[z:=7]=E[z:=y+1]}$$ Step 3. $$\frac{7=y+1}{7\cdot x+7\cdot y=x\cdot (y+1)+y\cdot (y+1)}$$ $${\mathrm{Answer:}}\ x\cdot (y+1)+y\cdot (y+1)$$ But according to the textbook, there should be three answers. Could someone please explain the logic behind this problem and how to approach it using Leibniz's law? Are there any underlying principles or theorems that I should be aware of to solve this problem? My textbook is ""A Logical Approach to Discrete Mathematics"" . Any insights, explanations, or references to relevant material would be greatly appreciated.","['logic', 'discrete-mathematics']"
4750521,Why does the Mandelbrot fractal appear when plotting $\underbrace{x\cos(x\cos( \cdots x\cos}_9(x))))$?,"while plotting the function $x\cos(x\cos(x\cos(x\cos(x\cos(x\cos(x\cos(x\cos(x\cos(x)))))))))$ using matplotlib in python I found the mandelbrot fractal. What is the reason that the mandelbrot fractal appears in the process and what literature should I read to learn more about this. Maybe the mandelbrot fractal appears in more places where you iterate a function over and over again.
here is the code import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import cmath

# Defining the function to map a complex function
def map_complex_function(complex_func, real_range, imag_range, color_scheme=(""magnitude"", ""argument"", ""(10x)/(10x+1)"")):
    # Creating the domain
    real_values, imag_values = np.meshgrid(np.linspace(real_range[0], real_range[1], 1000),
                                           np.linspace(imag_range[0], imag_range[1], 1000))
    z_values = real_values + 1j * imag_values

    # Applying the complex function to the domain
    w_values = complex_func(z_values)

    # Extracting the magnitude and argument
    magnitude = np.abs(w_values)
    argument = np.angle(w_values) / (2 * np.pi) % 1

    # Determining brightness based on the provided color scheme
    if color_scheme[2] == ""(10x)/(10x+1)"":
        brightness = (10 * magnitude) / (10 * magnitude + 1)
    else:
        brightness = magnitude / np.max(magnitude)

    # Creating HSV values
    saturation = np.ones_like(argument)
    hsv_values = np.stack([argument, saturation, brightness], axis=-1)

    # Converting HSV to RGB
    rgb_values = mcolors.hsv_to_rgb(hsv_values)

    # Plotting the image
    plt.imshow(rgb_values, origin='lower', extent=[real_range[0], real_range[1], imag_range[0], imag_range[1]])
    plt.xlabel('Real part')
    plt.ylabel('Imaginary part')
    plt.title(f'Complex map for {complex_func.__name__}')
    plt.colorbar(label='Magnitude (custom scale)')
    plt.show()

def f(z):
    return z*np.cos(z*np.cos(z*np.cos(z*np.cos(z*np.cos(z*np.cos(z*np.cos(z*np.cos(z*np.cos(z)))))))))
    
# Creating the plots
real_range = (0, 6)
imag_range = (-4, 4)
map_complex_function(f, real_range, imag_range, color_scheme=(""magnitude"", ""argument"", ""(10x)/(10x+1)"")) ```","['complex-analysis', 'chaos-theory', 'fractals']"
4750571,Expected Value of a DnD/Baldur's Gate Feat,"The Baldur's gate feat Savage Attacker lets you reroll an attack, and choose the better of the two attacks. I am attempting to calculate the expected benefit of this feat. Assume, for the sake of simplicity, that a melee attack has a 1d10 damage, which means it follows a discrete uniform distribution with integer values from 1 to 10 (equiprobable). $$A_1\text{ follows Discrete }U(1,10)$$ If we have savage attack, we take two rolls, and then the better of the two. How do I calculate the expected value of the benefit: $$E[|A_1-A_2|] $$ Update 1: I found this video by Stand-Up Math which calculates the actual probabilities (and not just the expected value)","['expected-value', 'statistics', 'probability-distributions', 'uniform-distribution']"
4750578,Asymptotic of specific type of recurrence using Riemann integral approximation,"So I've seen this technique used before, but can't find where I saw it nor do I remember how to finish. The general idea was let's say you have a recurrence of the form $$a_n=a_{n-1}+\frac{f(n)}{g(a_{n-1})}$$ Where $g$ is an increasing and positive function and $f$ is positive (it's usually something simple like the identity or constant). You want to compute an asymptotic for $a_n$ . The idea is that you can rewrite it as $$g(a_{n-1})(a_n-a_{n-1})=f(n)$$ And then sum from $n=1$ to $N$ to get $$\sum_{n=1}^N g(a_{n-1})(a_n-a_{n-1})=\sum_{n=1}^N f(n)$$ Now the LHS looks like a riemann sum, and since $g$ is increasing, we have that it is bounded above by $\int_{a_0}^{a_N} g(x)\, dx$ . If $\frac{d}{dx} G(x)=g(x)$ , we can say that $$G(a_N)-G(a_0)\geq \sum_{n=1}^N f(n)$$ $$G(a_N)\geq G(a_0)+\sum_{n=1}^N f(n)$$ $$a_N\geq G^{-1}\left(G(a_0)+\sum_{n=1}^N f(n)\right)$$ And this gives a lower bound. But to make any conclusions about the asymptotics we would also need an upper bound and I am stuck there. We can't really use the same approach for the upperbound because we can't make it a right riemann sum. I'm honestly just curious if anyone has a source for this type of approach or solutions that solve this type of problem this way as I can't remember where I initially saw this approach. But also if anyone knows how to solve it, that would of course also be appreciated","['recurrence-relations', 'asymptotics', 'real-analysis']"
4750596,Sum of all 4 digit numbers formed using 6 numbers,"Find the sum of all four digit numbers (without repetition of digits) formed using the digits $1,2,3,4,5,6$ I have tried this problem and my answer is $\frac{5!}{2!}\times (1+2+3+4+5+6)\times (10^3+10^2+10+1)=1399860$ Am i right?","['solution-verification', 'combinatorics']"
4750625,About the Integral $\int\arcsin\left(\sin^{2}x\right)dx$,"$$\int\arcsin\left(\sin^{2}x\right)dx$$ I am not able to find a closed form elementary solution for this, though I have no reason to believe it exists. But trying out the Definite Integral as follows: $$I=\int_{0}^{\frac{\pi}{2}}\arcsin\left(\sin^{2}x\right)dx$$ Using the Series Expansion for $\arcsin(x)$ : $$\arcsin(x)=\sum_{n=0}^{\infty}\frac{\left(2n\right)!}{2^{2n}\left(n!\right)^{2}}\ \frac{x^{2n+1}}{2n+1}$$ $$\arcsin\left(\sin^{2}x\right)=\sum_{n=0}^{\infty}\frac{\left(2n\right)!}{2^{2n}\left(n!\right)^{2}}\ \frac{\left(\sin x\right)^{4n+2}}{2n+1}$$ $$I=\sum_{n=0}^{\infty}\frac{\left(2n\right)!}{2^{2n}\left(n!\right)^{2}\left(2n+1\right)}\ \int_{0}^{\frac{\pi}{2}}\left(\sin x\right)^{4n+2}dx$$ We are aware of the closed form of the Integral: $$\int_{0}^{\frac{\pi}{2}}\left(\sin x\right)^{a}dx=\frac{\sqrt \pi}{2}\frac{\Gamma\left(\frac{a+1}{2}\right)}{\Gamma\left(\frac{a}{2}+1\right)}$$ $$I=\frac{\sqrt \pi}{2}\sum_{n=0}^{\infty}\frac{\left(2n\right)!}{2^{2n}\left(n!\right)^{2}\left(2n+1\right)}\ \frac{\Gamma\left(\frac{4n+3}{2}\right)}{(2n+1)!}$$ $$I=\frac{\sqrt{\pi}}{2}\sum_{n=0}^{\infty}\frac{\Gamma\left(\frac{4n+3}{2}\right)}{2^{2n}\left(n!\right)^{2}\left(2n+1\right)^{2}}\ $$ Wolfram Gives a Closed Form for the Summation in terms of Hypergeometric Function as follows: $$\sum_{n=0}^{\infty}\frac{\Gamma\left(\frac{4n+3}{2}\right)}{2^{2n}\left(n!\right)^{2}\left(2n+1\right)^{2}}\ =\frac{\sqrt{\pi}}{2}\,_{4}F_{3}\left(\frac{1}{2},\frac{1}{2},\frac{3}{4},\frac{5}{4};1,\frac{3}{2},\frac{3}{2};1\right)$$ Hence, $$I=\frac{\pi}{4}\,_{4}F_{3}\left(\frac{1}{2},\frac{1}{2},\frac{3}{4},\frac{5}{4};1,\frac{3}{2},\frac{3}{2};1\right)$$ I have seen some cases where these hypergeometric functions end up in terms of closed form expressions. Well my question is: Whether the Indefinite Integral has a solution. Whether the Definite Integral has a Closed Form Solution. Whether the Hypergeometric Series has a Closed Form, which will lead us directly to the Definite Integral.","['integration', 'special-functions', 'calculus', 'closed-form', 'hypergeometric-function']"
4750656,Can we compute $\int_1^{\infty} \frac{\sin ^2(\ln x)}{x^2 \ln ^2 x} d x$ without Feynman’s trick?,"When I came across the integral $$I=\int_1^{\infty} \frac{\sin ^2(\ln x)}{x^2 \ln ^2 x} d x,$$ I immediately thought of the Feynman’s trick after the substitution $x\mapsto \frac{1}{x}$ . $$
I=\int_0^1 \frac{\sin ^2(\ln x)}{\ln ^2 x} d x
$$ Considering the parameterised integral $$
I(a)=\int_0^1 \frac{\sin ^2(a \ln x)}{\ln ^2 x} d x
$$ Differentiating $I(a)$ w.r.t. $a$ twice, we get $$
I^{\prime}(a)=\int_0^1 \frac{\sin (2a \ln x)}{\ln x}dx
$$ and $$
I^{\prime \prime}(a) =2 \int_0^1 \cos (2 a \ln x) d x =\frac{2}{4 a^2+1}
$$ Integrating back yields $$
I ^{\prime}(a)=I^{\prime}(a)-I^{\prime}(0)= \int_0^a \frac{2}{4 t^2+1} d t =\tan ^{-1}(2 a)
$$ Further integration gives $$
\boxed{\int_0^1 \frac{\sin ^2(a \ln x)}{\ln ^2 x} d x =\int_0^a \tan ^{-1}(2 t) d t=a \tan ^{-1}(2 a)-\frac{1}{4} \ln \left(4 a^2+1\right)}
$$ Now we can conclude that $$\int_1^{\infty} \frac{\sin ^2(\ln x)}{x^2 \ln ^2 x} d x =I(1)= 
\tan ^{-1} 2-\frac{\ln 5}{4}
$$ My question: Can we compute $$\int_1^{\infty} \frac{\sin ^2(\ln x)}{x^2 \ln ^2 x} d x$$ without Feynman’s trick? Your comments and alternative methods are highly appreciated.","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'trigonometric-integrals']"
4750684,Solution verification on a Borel Cantelli Exercise,"I have a question regarding the following exercise that I just solved. Let $(X_n)$ be any sequence of random variables, show that there exists a sequence of positive constants $(l_n)$ such that one has $$
P(\lim_{n \to \infty} \frac{X_n}{l_n} = 0).
$$ Solution : Getting started, there exists a sequence of $(l_n)$ such that for every $n \in \mathbb N$ one has $P(|l_n X_n| \geq 1/n) \leq 1/n^2$ . Indeed applying the Markov inequality one has $$
P(|l_n X_n| \geq 1/n) \leq l_n E[|X_n|] n \leq 1/n^2,
$$ if one chooses $l_n \leq \frac{1}{E[X_n] n^3}$ . Now to finish the proof it sufficies to show that for every $\epsilon > 0$ one has $$
P(\limsup_{n \to \infty} \frac{|X_n|}{l_n} \geq \epsilon) = 0.
$$ We are going to apply a Borel-Cantelli argument, for this observe that for every $\epsilon > 0$ there exists some $n \in \mathbb N$ such that one has $\epsilon > 1/n$ , applying this one observation in combination with the prior estimate yields that $$
\sum_{n \in \mathbb N} P(\frac{X_n}{l_n} \geq \epsilon) \leq \sum_{n \in \mathbb N} P(\frac{X_n}{l_n} \geq 1/n) \leq \sum_{n \in \mathbb N} 1/n^2 < \infty,
$$ Hence by the first Borel Cantelli lemma one has that $$
P(\limsup_{n \to \infty} |X_n| /l_n \geq \epsilon) = 0,
$$ and this sufficies. Question : Is this reasoning, especially the choice of $(l_n)$ correct? What if one takes $X_n = + \infty$ for all $n \in \mathbb N$ as this should be allowed for random variables.","['solution-verification', 'probability-theory', 'probability']"
4750688,Express corners in a triangle from two angles and angle and polar coordinates of circumcenter,"I have some lecture notes where the following is claimed: Suppose that three points $0, p_1, p_2$ constitute a triangle $T$ in $\mathbb{R}^2$ . Suppose the angle between $p_1$ and $p_2$ is $\theta$ and $0$ and $p_2$ is $\varphi$ and that the center of the circumball around $T$ has center $(r, \tau)$ written in polar coordinates. Then we may express the points $p_1,p_2$ in cartesian coordinates as $$
p_1 = -2r \big(\sin(\varphi) \cos(\tau+\varphi), \sin(\varphi) \sin(\tau+\varphi) \big)
$$ and $$
p_2 = -2r \big(\sin(\theta + \varphi) \cos(\theta+\tau+\varphi), \sin(\theta+\varphi) \sin(\theta+\tau+\varphi) \big)
$$ I have attached a drawing of the setup here (for me it suffices to look at the case of acute triangles): To begin with we just assume $r=1$ and scale everything up after (corresponding to multiplication with $2r$ ). My attempts so far have been trying to using Thales theorem to create some right angles in order to have expression with sine and cosine appear. However I can't for the life of me make sense of what "" $\varphi + \tau$ "" actually represent and how to connect it to the first and second coordinates of the points. Update: I think the formula in the notes might be wrong. Consider the case of $\theta=\varphi=\pi/3$ , $\tau=0$ and $r=1$ . Then $p_1$ lies in the first quadrant and should have positive coordinates, yet the formula yields $p_1=(-\sqrt{2}/3,-4/9)$ . As a separate attempt to come up with a parametrization of $p_1$ myself, I have drawn the following: Appealing to the law of sines, we would then have $$
p_1 = 2r\sin(\varphi)(\cos(\pi/2-\alpha+\tau),\sin(\pi/2-\alpha+\tau)).
$$ However I need $\alpha$ to be an expression involving only $\theta, \phi, r, \tau$ . If one loads up Geogebra to play around it actually looks like $\alpha = \varphi$ , at least in the case of an acute triangle. However I cannot see why this is true but this would certainly solve the problem if it was. I have tried to look at so many angle identities around triangles and parallel lines as I could find. Can anyone help me solve this?","['trigonometry', 'geometry', 'plane-geometry']"
4750736,"Alternative proof of uniqueness in Problem 3.6.6 of Velleman's ""How to Prove it""","I am going through Vellemen's How to prove it in my free time.
Currently I am working on problem 6 in section 3.6:
Prove that there is a unique $A ∈ P(U)$ such that for every $B ∈ P(U), A \cup B =B$ . I already guessed that the unique set sought is the empty set and there is a solution in the back of the book. But to prove uniqueness I thought whether there might be another way. So here's the outline. Suppose $A$ is not the empty set. Then there must be at least one non-trivial element in $A$ , call it $x$ Since the property has to hold for all $B$ if we can find  counter example then we can show that $A$ has to be empty set. So suppose $B$ is a set which doesn't contain $x$ . Then $A\cup B\neq B$ . Therefore $A$ has to be the empty set. Is this a valid proof for uniqueness?","['elementary-set-theory', 'alternative-proof']"
4750737,Local extremum of multivariable functions,"Consider $C^\infty$ functions. For functions of one variable, we know that in general, at $x=c$ if the lowest order nonzero derivative is an even order say $2k$ , then $f(c)$ is an extremum, depending on the sign of $f^{(2k)}(c)$ . For example, if $f'(c)=f''(c)=f'''(c)=0$ and $f''''(c)>0$ , then it is a minimum; if $f'(c)=f''(c)=0$ , $f'''(c)\neq 0$ , then it is not a local extremum. Now for multivariable functions. We know the theory of Hessian matrix and eigenvalues. What if some of the second derivatives are zero? For example, consider $f(x,y)$ at $(a,b)$ : $f_x=f_y=f_{xx}=f_{xy}=0$ . Apparently if $f_{xxx}\neq 0$ the function cannot have a local extremum there, as the trend along the $x$ -axis is monotone. Suppose $f_{xxxx},f_{yy}$ do not vanish. Should we examine the eigenvalues of \begin{equation}
H=\begin{bmatrix} f_{xxxx} & f_{xxy}\\ f_{xxy} & f_{yy}\end{bmatrix}?
\end{equation} What is the general theory about it? Any reference on this? thanks!",['multivariable-calculus']
4750797,Understanding $\frac{\text{d}u}{\text{d}t}$ vs. $\frac{\text{D}u}{\text{D}t}$,"I'm working on an unassessed course problem. The setup might be unnecessary for my question, but I give it in case it helps. An incompressible fluid of constant density $\rho$ has velocity $\boldsymbol{u}(x,t)$ . For fluid occupying a closed volume $V$ bounded by a surface $S$ , with outward normal unit vector $\boldsymbol{n}$ , deduce that $$\frac{\text{d}}{\text{d}t}\int_V\rho u_i\text{ d}V=\int_S\sigma_{ij}n_j\text{ d}S+\int_V\rho F_i\text{ d}V,$$ where $\sigma_{ij}$ is the stress tensor, and $\boldsymbol{F}$ is the body force per unit mass. Use Gauss' divergence theorem with $f_j=\sigma_{ij}a_i$ , where $\boldsymbol{a}$ is an arbitrary constant vector, to deduce that $$\rho\frac{\text{D}u_i}{\text{D}t}=\frac{\partial\sigma_{ij}}{\partial x_j}+\rho F_i,$$ throughout the fluid. The solution booklet contains the line, $\rho\text{ d}V$ is the mass of a fluid element, and does not change as the element moves around, so $$\frac{\text{d}}{\text{d}t}\int_V\rho u_i\text{ d}V=\int_V\left(\frac{\text{D}u_i}{\text{D}t}\right)\rho\text{ d}V.$$ I don't follow. I would interpret the verbal statement to mean $$\frac{\text{D}}{\text{D}t}(\rho\text{ d}V)=0,$$ and I could follow the symbolic step $$\frac{\text{d}}{\text{d}t}\int_V\rho u_i\text{ d}V=\int_V\left(\frac{\text{d}u_i}{\text{d}t}\right)\rho\text{ d}V$$ but I don't get how the verbal statement implies the symbolic step given. Doesn't this require $$(\boldsymbol{u}\cdot\nabla)\boldsymbol{u}=\boldsymbol{0}?$$ and how should we know that's true? A related question I asked a few months ago yielded a link to a more substantial answer than I can take in without giving significant time to background which is outside the scope of my module.","['physics', 'multivariable-calculus', 'fluid-dynamics']"
4750871,Eigenvectors of any arbitrary matrix is same as its adjoint.,"Recently I came across Normal matrices and their properties, one of which states that their eigenvectors are the same as their adjoint and are orthogonal. I've gone across some proofs and I understand it but when I tried to prove the same using the inner product, It somehow states that the above is true for any arbitrary matrix. Can someone possibly help me point out where I'm going wrong? Let's say A is a matrix and B is its adjoint. If x is an eigenvector of A with eigenvalue k, then, < x | A | x > = < x | k x > = k < x | x > also < x | A | x > = < Bx | x > hence < B x | x > = k < x | x > =  < kx | x > So, x is also an eigenvector of adjoint of A.","['matrices', 'inner-products', 'adjoint-operators', 'eigenvalues-eigenvectors']"
4750920,Stuck on (simple looking!) discontinuous double integral with gaussian weight and double pole,"The question is what is the analytic answer to the limit of the difference of integrals $(I_{+\epsilon}-I_{-\epsilon})|_{\epsilon \rightarrow 0} =\int_{0}^{\infty}dx \int_{0}^{\infty}dy\left(\frac{e^{-\frac12 x^2-\frac12(y+i \epsilon)^2}}{(x-i (y+i \epsilon))^2}-\frac{e^{-\frac12 x^2-\frac12(y-i \epsilon)^2}}{(x-i (y-i \epsilon))^2}\right)|_{\epsilon \rightarrow 0}$ . This comes from considering the integral $I_0= \int_{0}^{\infty}dx \int_{0}^{\infty}dy\frac{e^{-\frac12 x^2-\frac12y^2}}{(x-i y)^2}$ . By itself this is not a convergent integral, and playing around with it, it seemed to me that the divergence is logarithmic. However, by shifting the y variable above and below the real axis there, one can find a slight alteration which should be convergent (I believe), given by the expression at the top of this question. I'm struggling to get a clean working calculation for this, but the most reasonable ones seemed to give essentially $\pi$ . Any idea if this is indeed the correct result and how to obtain it cleanly?","['integration', 'definite-integrals', 'contour-integration', 'branch-cuts', 'limits']"
4750980,What is the probability that N points randomly selected on a circle all lie within a fraction f of the circumference?,"How do I prove that the answer to this question:
If N objects are placed randomly in a ring, what is the probability that they all lie together along an arc of length that is a fraction, f < 1/2, of the circumference or shorter? is: $Nf^{N-1}$ . This problem comes up in astronomy in the guise of what the chances are that these N planets in the sky are above the horizon at some point in a day.  It can also come up in coverage problems and length of gaps of coverage from independent Earth-observing satellites.","['statistics', 'probability']"
4750985,Choose a parameter so that $\sup_f\operatorname E_\mu\left[\left|\int_0^\tau f(X_t)\:{\rm d}t-\tau\pi f\right|^2\right]$ is minimized,"Let $(E,\mathcal E,\lambda)$ be a $\sigma$ -finite measure space; $\mathcal E_b:=\left\{f:E\to\mathbb R\mid f\text{ is bounded and }\mathcal E\text{-measurable}\right\}$ be equipped with the supremum norm; $(\kappa_t)_{t\ge0}$ be a Markov semigroup on $(E,\mathcal E)$ ; $A$ denote the generator $^1$ of $(\kappa_t)_{t\ge0}$ and $A^\ast$ denote the bounded linear operator on $\mathcal L^1(\lambda)$ with $$\int(Af)g\:{\rm d}\lambda=\int fA^\ast g\:{\rm d}\lambda\tag1$$ for all $f\in\mathcal E_b$ and $g\in\mathcal L^1(\lambda)$ ; $(\Omega,\mathcal A)$ be a measurable space; $(\mathcal F_t)_{t\ge0}$ be a filtration on $(\Omega,\mathcal A)$ ; $(X_t)_{t\ge0}$ be an $(E,\mathcal E)$ -valued $(\mathcal F_t)_{t\ge0}$ -progressive process on $(\Omega,\mathcal A)$ ; $\operatorname P_x$ be a probability measure on $(\Omega,\mathcal A)$ with $$\operatorname E_x\left[f(X_{s+t})\mid\mathcal F_s\right]=(\kappa_tf)(X_s)\tag1$$ for all $f\in\mathcal E_b$ and $s,t\ge0$ for $x\in E$ ; $\mu$ be a probability measure on $(E,\mathcal E)$ with density $u:E\to[0,\infty)$ ; $\pi$ be a probability measure on $(E,\mathcal E)$ with density $p:E\to(0,\infty)$ and $$c:=\frac{c_0u+A^\ast p}p$$ for some $c_0>0$ ; $$A_t:=\int_0^tc(X_s)\:{\rm d}s$$ and $$M_t:=e^{-A_t}$$ for $t\ge0$ ; $\xi$ be a real-valued random variable on $(\Omega,\mathcal A)$ exponentially distributed with respect to $\operatorname P_x$ for all $x\in E$ and $$\tau:=\inf\left\{t\ge0:A_t\ge\xi\right\}.$$ Question : Let $$Z(f):=\int_0^\tau f(X_t)\:{\rm d}t$$ for $f\in\mathcal E_b$ . How should we choose $c_0$ such that $$\sup_{\substack{f\in\mathcal E_b\\f\ge0}}\operatorname E_\mu\left[\left|Z(f)-\tau\pi f\right|^2\right]\tag2$$ is minimized? We can show that $$\frac{\rm d}{{\rm d}t}\operatorname E_\pi\left[f(X_t);t<\tau\right]=-c_0\operatorname E_\mu\left[f(X_t);t<\tau\right]\tag3$$ for all $t\ge0$ and hence $$\operatorname E_\mu\left[Z(f)\right]=\int_0^\infty\operatorname E_\mu\left[f(X_t);t<\tau\right]\:{\rm d}t=\frac{\pi f}{c_0}\tag4$$ for all $f\in\mathcal E_b$ . In particular, $$\operatorname E_\mu\left[\tau\right]=\frac1{c_0}\tag5.$$ Idea : Maybe we can also obtain an expression for $Z(f)^2$ and obtain a bound for $(2)$ that way? Possible simplifications : If the problem is too hard in general, I'm willing to assume one or all of the following simplifications: $\pi$ is $(\kappa_t)_{t\ge0}$ -invariant and hence $$A^\ast p=0\tag6.$$ $$\kappa_t=e^{t A}\;\;\;\text{for all }t\ge0$$ and $$A=\alpha\left(\kappa-\operatorname{id}_{\mathcal E_b}\right)\tag7$$ for some Markov kernel $\kappa$ on $(E,\mathcal E)$ and $\alpha\in\mathcal E_b$ with $\alpha\ge0$ and $\pi$ is $\kappa$ -invariant. In the setting of (2.), we may also ask how to choose $c_0$ and $\alpha$ (possibly constant) together such that $(2)$ is minimized. $^1$ $(\kappa_t)_{t\ge0}$ is considered as a contraction semigroup on $\mathcal E_b$ .","['measure-theory', 'stochastic-processes', 'markov-process', 'functional-analysis', 'probability-theory']"
4751011,Relationship between scalar and vector valued automorphic forms,"Let $F$ be a number field with signature $(r_1,r_2)$ . There are two definitions of automorphic forms on $GL_{2,F}$ that I am aware of. Let $K=O_2(\mathbb R)^{r_1}\times U_2(\mathbb C)^{r_2}$ be a maximal compact subgroup of $GL_2(F)$ . Roughly speaking, the two definitions are Definition 1 (scalar valued): An automorphic form is a smooth function $f:GL_2(\mathbb A_F)\to \mathbb C$ that is left $GL_2(F)$ -invariant, right $K$ finite, ... Denote the space of all such functions by $\mathcal A$ . Definition 2 (vector valued): Fix an irreducible representation $(\rho,V)$ of $K$ . An automorphic form (of weight $V$ ) is a smooth function $f:GL_2(\mathbb A_F)\to V$ that is left $GL_2(F)$ -invariant, $f(gk)=\rho(k)^{-1}f(g)$ for all $g\in GL_2(\mathbb A_F),k\in K,$ ... Denote the space of all such functions by $\mathcal A(\rho)$ . May I ask what is the relationship between these two notions? For instance, will a statement like $\mathcal A\cong \oplus_{\rho} \mathcal A(\rho)\otimes_{\mathbb C} V^\vee$ , where $\rho$ runs over the isomorphism classes of finite dimensional irreducible representations of $K$ , holds? Also, given $f\in \mathcal A(\rho)$ , how should one define the automorphic representation (which should be an irreducible subquotient of some space of scalar valued functions) associated to it?","['number-theory', 'automorphic-forms', 'representation-theory', 'adeles', 'modular-forms']"
4751035,Why do we stick to single valued functions?,"The definition of a function of real variables given in my book was that a function is a rule which assigns to each element in the domain exactly one element of the range. I am new to what a mathematical structure is, but from what I have gathered so far, if I agreed to accept a definition, then I cannot invoke any rules other than those which relate the definition. Was this structure (definition) imposed to obey the same mathematical structure of algebra (definitions and rules of equality, addition, and multiplication)? For instance, how would we define $\sqrt{4} + 4$ as $+2+4$ or $-2+4$ ? From this point of view a multi-valued mapping violates the above definition and cannot be regarded as a function, since
a multi-valued mapping assigns to a single element in a set two or more distinct elements of another set. For example, let $y=\sqrt{x}$ . To find its inverse, we break down the curve into a union of single-valued pieces at the points of vertical tangents, which occurs at the origin and consider one branch at a time where the function is bijective. Why do we do this? Graphically, $\sqrt{4}= \pm 2 $ . What is the problem with plotting multiple ordered pairs from the mappings to sketch the curve of $\sqrt{x}$ , where $ f:4 \rightarrow \pm 2 $ or $ (4,-2) $ and $ (4,2) $ and forming the inverse by reflecting the graph about y=x. To clarify in case the readers of this question are confused if it seems I am asking different unrelated questions, what I am trying to say in short is: did we state the existence conditions (injectivity and surjectivity) to take advantage of the mathematical structure and can we use some of the mathematical structure if we violate the definition? Namely graphically, we can plot multivalued functions but can we perform arithmetic without it being ambiguous?",['functions']
4751147,Can a Markov chain be used to compute probabilities of sets of draws?,"Problem statement Consider the following scenario: We have an extremely large bag of glass marbles and metallic balls. We do not know how many items are in the bag, but we do know the percentage of the total for each type of metal ball: Color Percentage Gold 1% Silver 2% Copper 5% We are drawing a single ball at a time, replacing it, and mixing up the bag. It is not possible to distinguish what we are drawing by feel, so what we draw is random. Our goal is to draw (see) each metal at least once, and we want to examine the probability of having completed the set over the first thousand draws. Just to give us some specific values to compute, let's say we'll look at the probabilities at 50, 100, 250, 500, and 1000 draws. Solution, according to my understanding I believe we can approach this problem using a Markov chain. We can model the problem by having each state be a subset of metals that we have encountered, and the transition matrix will be the probability of drawing any unseen metal. While there is no interesting long term steady state (The probability of each state will asymptotically approach 0 except for the completed set which will approach 1.), the chain will at the very least help organize the computations in a much simpler way than trying to perform them ad hoc. To get our states, we can take the power set of the metals, and we can use some abbreviations to shorten things: Seen Abbreviation {} $ \emptyset $ {Gold} G {Silver} S {Copper} C {Gold,Silver} GS {Gold,Copper} GC {Silver,Copper} SC {Gold,Silver,Copper} GSC Before we lay out our transition matrix, let's compute the probability of drawing a glass marble. This value will be useful for constructing the transition matrix: $$ 1 - (0.01 + 0.02 + 0.05) = 0.92 $$ For our transition matrix, we will use the $ X_n = P X_{n+1} $ convention, meaning each column will represent a current state and each row will represent the next state. So our transition matrix $ P $ , with some labels to make interpreting it easier, looks like this: $$
\newcommand\pad[1]{\rlap{#1}\phantom{0000}}\begin{matrix}
From: & \begin{array}{*8c} \pad{\emptyset} & \pad{\text{G}} & \pad{\text{S}} & \pad{\text{C}} & \pad{\text{GS}} & \pad{\text{GC}} & \pad{\text{SC}} & \pad{\text{GSC}} \end{array} &   \\
  &   & To: \\
  & \begin{bmatrix} \pad{0.92} & \pad{0} & \pad{0} & \pad{0} & \pad{0} & \pad{0} & \pad{0} & \pad{0} \\ \pad{0.01} & \pad{0.93} & \pad{0} & \pad{0} & \pad{0} & \pad{0} & \pad{0} & \pad{0} \\ \pad{0.02} & \pad{0} & \pad{0.94} & \pad{0} & \pad{0} & \pad{0} & \pad{0} & \pad{0} \\ \pad{0.05} & \pad{0} & \pad{0} & \pad{0.97} & \pad{0} & \pad{0} & \pad{0} & \pad{0} \\ \pad{0} & \pad{0.02} & \pad{0.01} & \pad{0} & \pad{0.95} & \pad{0} & \pad{0} & \pad{0} \\ \pad{0} & \pad{0.05} & \pad{0} & \pad{0.01} & \pad{0} & \pad{0.98} & \pad{0} & \pad{0} \\ \pad{0} & \pad{0} & \pad{0.05} & \pad{0.02} & \pad{0} & \pad{0} & \pad{0.99} & \pad{0} \\ \pad{0} & \pad{0} & \pad{0} & \pad{0} & \pad{0.05} & \pad{0.02} & \pad{0.01} & \pad{1} \\ \end{bmatrix} & \begin{array}{*8wc100} \pad{\emptyset} \\ \pad{\text{G}} \\ \pad{\text{S}} \\ \pad{\text{C}} \\ \pad{\text{GS}} \\ \pad{\text{GC}} \\ \pad{\text{SC}} \\ \pad{\text{GSC}} \end{array}
\end{matrix} \\
$$ The initial state $ X_0 $ is simple: we start with the empty set 100% of the time. So it's just an 8 element matrix where the first element is 1 and the remaining elements are 0. With these two matrices in hand, computing the probability of having drawn all metals is just a matter of matrix multiplication and extracting the value of interest. $$
\begin{align}
X_{50} &= P^{50} X_{0} \\
X_{100} &= P^{100 - 50} X_{50} = P^{50} X_{50} \\
X_{250} &= P^{250 - 100} X_{100} = P^{150} X_{100} \\
X_{500} &= P^{500 - 250} X_{250} = P^{250} X_{250} \\
X_{1000} &= P^{1000 - 500} X_{500} = P^{500} X_{500} \\
\end{align}
$$ And we find that the probabilities for completion are: Draws Probability 50 0.22836397 = 22.8364% 100 0.54550092 = 54.5501% 250 0.91302709 = 91.3027% 500 0.99338874 = 99.3389% 1000 0.99995683 = 99.9957% Is this analysis correct? Motivation If anyone is wondering, this is not a homework problem. I'm planning to apply the technique to analyzing gacha game probabilities, and I'd like to be certain that the approach I'm using actually works. If this approach is correct, I am interested in generalizing to more complex situations. For example, I want to examine the effect of having two bags to choose from (where you decide which bag to draw from based on the seen set you already have). I may also be interested in the probabilities of the various subsets, as well. But before I get into those, I wanted to be certain that the approach is valid. Additional properties? Given the context, are there any well known properties that a Markov chain exhibits that might be of interest in this sort of problem space? I can go read about them on my own, but it would help if I know what to look for. Extra details I chose to distinguish the items of interest as metals rather than colors because not having to distinguish between colors of interest and colors not of interest made writing the problem simpler. I've omitted the actual computations of the final probabilities because I used Python and numpy to perform them rather than do them by hand. If you'd like to perform computations yourself, here's the transition matrix in code form (with the assumption you've done the conventional import numpy as np ): np.array([[0.92, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
          [0.01, 0.93, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
          [0.02, 0.  , 0.94, 0.  , 0.  , 0.  , 0.  , 0.  ],
          [0.05, 0.  , 0.  , 0.97, 0.  , 0.  , 0.  , 0.  ],
          [0.  , 0.02, 0.01, 0.  , 0.95, 0.  , 0.  , 0.  ],
          [0.  , 0.05, 0.  , 0.01, 0.  , 0.98, 0.  , 0.  ],
          [0.  , 0.  , 0.05, 0.02, 0.  , 0.  , 0.99, 0.  ],
          [0.  , 0.  , 0.  , 0.  , 0.05, 0.02, 0.01, 1.  ]])","['markov-chains', 'probability']"
4751150,Intuition behind the spectral theorem in infinite dimensions,"I would like to verify my intuition behind the spectral theorem in infinite dimensions. For the moment I am putting bounded/unbounded and domain issues aside. In finite dimensions the spectral theorem says that for any self-adjoint/Hermitian matrix $A$ , $$A = U^{-1}\Lambda U $$ where $\Lambda$ is a diagonal matrix consisting of the eigenvalues of $A$ and $U$ is a change of basis matrix. The interpretation here is that $U$ maps to a basis that is a direct sum of eigenspaces. In each eigenspace the the action of $A$ is multiplication by the appropriate eigenvalue. Thus we may write $$A = \sum_i^n \lambda_i P_i \tag{1}$$ where $P_i$ is the projection onto the eigenspace corresponding to $\lambda_i$ . Moving to infinite dimensions, let $A$ be a self-adjoint operator on some Hilbert space $H$ . Unlike the finite dimensional case, in general the spectrum of $A$ : Is no longer countable. Contains elements that are not eigenvalues (i.e. the continuous part of the spectrum). To address the first point, the sum in (1) is now made into an integral: $$A = \int_{\sigma(A)} \lambda dP_\lambda. \tag{2}$$ For points in the spectrum of $A$ that are eigenvalues, the integral (2) is to be interpreted similarly to (1). That is, we project onto the eigenspace spanned by $\lambda$ and the action of $A$ on this subspace is to multiply by $\lambda$ . Making sense of (2) for points in the continuous spectrum addresses the second point above. The operator $P_\lambda$ for $\lambda$ in the continuous spectrum is the zero projector. To account for the contribution of the continuous spectrum we make sense of (2) in a limiting sense similar to how the Lebesgue integral is constructed. That is, we consider Borel sets $\Omega$ containing an approximate eigenvalue and interpret $P_\Omega$ as projecting onto an approximate eigenspace where the action of $A$ is approximately multiplication by $\lambda$ . Its contribution to (2) is obtained as $\Omega \rightarrow \{\lambda\}$ in some sense, again similar to how we use simple functions to construct the Lebesgue integral and then take a limit. I know the above is a bit handwav and can be made more precise using distribution functions $P_\lambda = P((-\infty, \lambda])$ to define a Stieltjes integral, but my goal is to build an intuition for now. Is my intuition of the spectral theorem and the integral (2) correct?","['operator-theory', 'spectral-theory', 'functional-analysis']"
4751263,CDF of a convergent positive series,"Let $Y_0, Y_1, \ldots$ be an i.i.d. random sequence such that $$ \mathbb{P}(Y_k = 0) \;=\; 1 - \mathbb{P}(Y_k = 1) \;=\; p \qquad \text{for each $k\ge 0$}. $$ I am interested in the following random sequences $$ U_k = \prod_{i=0}^{k-1}\gamma^{Y_i} \theta^{1 - Y_i}, $$ $$ Q_n = \sum_{k=1}^n U_k, $$ where $0<\theta<1<\gamma$ . I have proved that $Q_\infty = \sum_{k=1}^\infty U_k$ converges a.s. when $p> \log \gamma/\log (\theta^{-1}\gamma)$ . That means under this case, $Q_\infty$ is a random variable and it makes sense to consider its CDF (cumulative distribution function). Let us use $F_n$ and $F_\infty$ to denote the CDFs of $Q_n$ and $Q_\infty$ , respectively. I also proved that actually $F_n \downarrow F_\infty$ when $n\to\infty$ . Now I am curious about what is exactly $F_\infty$ . Can we have a beautiful explicit formula for it? Or can we have a tight lower bound of it? I also use MATLAB to draw the pictures of $F_n$ as follows (I set $\gamma=2$ , $\theta=0.5$ , and $p=0.51$ ), but I can only have $n$ around 20 since the time of computing this $F_n$ is $\mathcal{O}(2^n)$ . Any comments are welcome.","['cumulative-distribution-functions', 'limits', 'pointwise-convergence', 'probability']"
4751286,Varying Coefficient Helmholtz Equation IVP,"I want to find an odd solution to: $$u''(x) = V(x) u(x)$$ for $V(x)$ an even function. I propose the following IVP: $$\partial_t \begin{pmatrix} u \\\ v \end{pmatrix} = \begin{pmatrix} 0 & 1 \\\ V(t) & 0 \end{pmatrix} \begin{pmatrix} u_0 \\\ v_0 \end{pmatrix}$$ with $$\begin{pmatrix} u_0 \\\ v_0 \end{pmatrix} = \begin{pmatrix} 0 \\\ 1 \end{pmatrix}$$ Now we have: $$\partial_t U = A(t) U(t)$$ And this can be solved as so: $$e^{-\int_0^t A(s) ds} \partial_t U = -\left(\partial_t e^{-\int_0^t A(s) ds} \right) U$$ Giving us $$\partial_t\left( e^{-\int_0^t A(s)  ds} U(t) \right) = 0$$ Hence, $$e^{-\int_0^t A(s) ds} U(t) = U(0)$$ And we have the matrix $$e^{-\int_0^t A(s) ds}$$ is always invertible, giving us: $$U(t) = e^{\int_0^t A(s) ds} U(0)$$ which can be solved by exponentiating the matrix: $$M(t) = \begin{pmatrix} 0 & t \\\ \int_0^t V(s) ds & 0 \end{pmatrix}$$ using traditional methods. If $V(t)$ is constant, this gives us the expected result. Should this approach work in general? What would the alternative approach for solving this for general $V$ be? I tried comparing results to numerical solutions, but they didn't seem to match.",['ordinary-differential-equations']
4751311,How can we calculate the variance of this stopping time?,"Let $(E,\mathcal E,\lambda)$ be a $\sigma$ -finite measure space; $(\kappa_t)_{t\ge0}$ be a Markov semigroup on $(E,\mathcal E)$ with generator $^1$ $A$ ; $\mu,\pi$ be probability measures on $(E,\mathcal E)$ with densities $u,p$ with respect to $\lambda$ with $p>0$ and $$c:=\frac{A^\ast p+c_0u}p$$ for some $c_0>0$ ; $\mathcal E_b:=\left\{f:E\to\mathbb R\mid f\text{ is bounded and }\mathcal E\text{-measurable}\right\}$ be equipped with the supremum norm; $(\Omega,\mathcal A)$ be a measurable space; $(X_t)_{t\ge0}$ be an $(E,\mathcal E)$ -valued progressive process on $(\Omega,\mathcal A)$ ; $\operatorname P_x$ be a probability measure on $(\Omega,\mathcal A)$ with $$\operatorname E_x\left[f(X_{s+t})\mid\mathcal F^X_s\right]=(\kappa_tf)(X_s)\tag1$$ for all $f\in\mathcal E_b$ and $s,t\ge0$ for $x\in E$ ; $$A_t:=\int_0^tc(X_s)\:{\rm d}s$$ and $$M_t:=e^{-A_t}$$ for $t\ge0$ ; $\xi$ be a real-valued random variable on $(\Omega,\mathcal A)$ exponentially distributed with respect to $\operatorname P_x$ for all $x\in E$ and $$\tau:=\inf\left\{t\ge0:A_t\ge\xi\right\}.$$ Question 1 : How can we compute $$\operatorname E_\mu[\tau^2]=\int\mu({\rm d}x)\operatorname E_x[\tau^2]?\tag2$$ Noting that $$\frac{\rm d}{{\rm d}t}\operatorname P_\pi\left[t<\tau\right]=-c_0\operatorname P_\mu\left[t<\tau\right]\tag3$$ for all $t\ge0$ , we easily see that $$\operatorname E_\mu[\tau]=\frac1{c_0}\tag4.$$ In order to compute $(2)$ , we should note that $$\operatorname E_\mu\left[\left|\int_0^\tau f(X_t)\:{\rm d}t\right|^2\right]=2\int_0^\infty\int_0^t\operatorname E_\mu\left[f(X_s)f(X_t);t<\tau\right]\:{\rm d}s\:{\rm d}t\tag5$$ for all $f\in\mathcal E_b$ . Applying this to $f=1$ and using $(3)$ should yield \begin{equation}\begin{split}\operatorname E_\mu[\tau^2]&=2\int_0^\infty t\operatorname P_\mu\left[t<\tau\right]\:{\rm d}t\\&=2\left[\lim_{t\to\infty}t\operatorname P_\mu\left[t<\tau\right]-\int_0^\infty\operatorname P_\mu\left[t<\tau\right]\:{\rm d}t\right],\end{split}\tag6\end{equation} where $\lim_{t\to\infty}\operatorname P_\mu\left[t<\tau\right]=0$ . Question 2 : Assuming this is correct so far, can we simplify $(6)$ further? Question 3 : In the same way as $(4)$ is derived from $(3)$ , we can derive $$\operatorname E_\mu\left[\int_0^\tau f(X_t)\:{\rm d}t\right]=\frac{\pi f}{c_0}\tag7$$ from $$\frac{\rm d}{{\rm d}t}\operatorname E_\pi\left[f(X_t);t<\tau\right]=-c_0\operatorname E_\mu\left[f(X_t);t<\tau\right]\tag8.$$ But what, similar to $(6)$ , can we do to derive an expression for $$\operatorname E_\mu\left[\left|\int_0^\tau f(X_t)\:{\rm d}t\right|^2\right]?\tag8$$ $^1$ $(\kappa_t)_{t\ge0}$ is considered as a contraction semigroup on $\mathcal E_b$ . $^2$ $A^\ast$ denotes the adjoint of $A$ with respect to $\lambda$ ; i.e. $$\int fA^\ast g\:{\rm d}\lambda=\int gAf\:{\rm d}\lambda\tag9$$ for all $f\in\mathcal D(A)$ and $g\in\mathcal D(A^\ast)$ .","['measure-theory', 'stochastic-processes', 'markov-process', 'semigroup-of-operators', 'probability-theory']"
4751353,Geometry question: equidistant center point of 45-45-90 triangle,"Okay, I've been struggling on this for a couple days now. Given a 45-45-90 triangle with legs of length n , extend three perpendicular rays, one from each segment, such that they intersect within the body of the right triangle, and such that each is of equivalent length. What is that length? It's empirically possible, obviously, but I can't seem to solve for x to save my life. The intersection point represents the physical rotation point of the overall triangle, such that every face can rest flat against the X axis and that intersection point will travel only laterally, never vertically. The closest guess I've arrived at is n -(( n √2)/2), but I'll be damned if I know WHY (or even if that's actually correct); it just seems to fit a series of test inputs to an approximate degree, though I cannot prove it right. I cannot seem to find a critical length whatever approach I try. (Forgive the crudeness of the diagram; I did that on my phone)","['triangles', 'geometry', 'rotations']"
4751390,Why does probability of $n$ consecutive success equal the probability when the last is a failure given that the last $n - 1$ have at least one failure,"I have been the question: Independent trials, each resulting in a success with probability $p$ or a failure with probability $q = 1 - p$ , are performed. We are interested in computing the probability that a run of $n$ consecutive successes occurs before a run of $m$ consecutive failures. This is the given solution: Let $E$ be the event that a run of $n$ consecutive successes occurs before a run of $m$ consecutive failures. To obtain $P(E)$ , we start by conditioning on the outcome of the first trial. That is, letting $H$ denote the event that the first trial results in success, we obtain: $$P(E) = pP(E|H) + qP(E|H^c)$$ Now, given that the first trial was successful, one way we can get a run of $n$ successes before a run of $m$ failures would be to have the next $n - 1$ trials all result in successes. So, let us condition whether or not that occurs. That is, letting $F$ be the even that trials $2$ through $n$ all are successes, we obtain $$P(E|H) = P(E|FH)P(F|H) + P(E|F^cH)P(F^c|H)$$ On the one hand, clearly, $P(E|FH) = 1$ ; on the other hand, if the event $F^cH$ occurs, then the first trial would result in success, but there would be a failure some time during the next $n - 1$ trials. However, when this failure occurs, it would wipe out all of the previous success, and the situation would be exactly as if we started out with a failure. Hence, $$P(E|F^cH) = P(E|H^c)$$ Now I understand why the failure wipes out the entire work, but I don't get why the last equation holds. In particular, $P(E|F^cH)$ means the probability of success given that the last $n - 1$ trials have at least one failure. I don't see how that can reduce to just given the information that the last trial was a failure. For example, if the sequence ends with $...FSSS$ , we also have the information that the last $3$ are successes. Can someone please explain this? I know there is a solution here but I don't get it. Thanks","['conditional-probability', 'discrete-mathematics', 'probability']"
4751399,Simple objects in Quasi-coherent sheaves are isomorphic to structure sheaves of a closed point,"Need to prove that simple objects in Quasi-coherent sheaves are isomorphic to structure sheaves of a closed point. ""Simple object"" means that there is no non-trivial subobject. I encountered this problem in Vakil's FOAG(April 1, 2023), 6.5.L. I have proved that if the simple sheaf is a simple module on every affine open set, we have the result. The approach is to notice that, under this assumption, the simple qcoh sheaf is supported at a closed point of the scheme X on every affine open set, then we can construct a skyscraper subsheaf. But I don't know how to continue. My attempt to prove the ""fact"" ( the simple qcoh sheaf is a simple module on every affine open set ) is as the following. Suppose we have M on Spec A , M is an A -module, and since M is not simple, we have a submodule of M isomorphic to A/p , where p is a maximal ideal of A , but not necessarily a closed point. I'm thinking of constructing a skyscraper at p with A/p . But since the closure of p is not necessarily inside Spec A , I can't do the zero extension and get an injection from the skyscraper into the quasi-coherent sheaf. PS: the scheme X may not have any closed point. I think maybe there is a more subtle way of proving using the sheaf morphisms which I have not learned systematically.","['algebraic-geometry', 'quasicoherent-sheaves']"
4751472,Proving the identity $\sum_{k=1}^n {k^3} = \big(\sum_{k=1}^n k\big)^2$ without induction,"I recently proved that $$\sum_{k=1}^n k^3 = \left(\sum_{k=1}^n k \right)^2$$ using mathematical induction.  I'm interested if there's an intuitive explanation, or even a combinatorial interpretation of this property. I would also like to see any other proofs.","['algebra-precalculus', 'visualization', 'summation', 'sequences-and-series']"
4751481,Ant walking on a cube riddle generalization to higher dimensions,"There's a pretty common riddle floating around which goes something like this: An ant starts at one corner of a unit cube. They wish to reach the opposite corner. If they can traverse along any face, but cannot fly, what is the length of the shortest path they can take? Pretty obviously the answer to this is $\sqrt{5}$ (from turning it into a net). But I was wondering if there was some generalization of this riddle for other shapes? It seems like the math gets quite messy if there are asymmetries, so for this question, I'd like to assume that every side length is 1. For any n-simplex, I'm fairly sure that the length is 1. Though it doesn't really make sense to define an ""opposite"", any vertex is 1 away from another, so it also doesn't really matter. I guess I can define opposition as any pair of vertices which have the longest traversal length. But I'm having trouble with generalizing it for other polytopes. An octahedron would have $\sqrt{3}$ , an icosahedron would have $\sqrt{7}$ , a dodecahedron would have $\sqrt{\frac{7 \sqrt{5} + 17}{2}}$ , and a tesseract would have $2 \sqrt{2}$ (the diagonals of 2 squares). But I'm not really sure how to compute this value for things like 120-cells, 600-cells, n-cubes, and n-orthoplexes. I'm also interested in generalizing it so that traversal can also be higher dimensional. For an ant with 3-dimensional traversal, they would be able to traverse the cube in $\sqrt{3}$ and the octahedron in $\sqrt{2}$ (as these are the diameters of the circumscribed spheres). For a tesseract, they would be able to traverse along every diagonal within a cell, rather than just the faces, though I haven't figured out the length yet. I would appreciate any help on this generalization, even some Mathematica code would be valuable to me. Here are the nets I've used: Cube: Octahedron: Dodecahedron: Icosahedron:","['polytopes', 'geometry']"
4751513,Possible new formula for OEIS A191522,"The following formula (proved here ): $$\sum_{k=\lfloor \frac{n+1}{2} \rfloor}^{n}{k{k-1 \choose \lfloor \frac{n+1}{2}\rfloor - 1}} = \Big\lceil \frac{n}{2} \Big\rceil{n+1 \choose \lfloor \frac{n}{2} \rfloor}$$ counts the sum of the maximum elements of each subset of $[n]=\{1,\ldots,n\}$ with size $\lfloor (n+1)/2 \rfloor$ . For example for $n=3$ there are three subsets $\{1,2\}, \{1,3\}, \{2,3\}$ and the sum of maximum values is $2+3+3=8$ . It seems to be equal to $a(n+2)$ where $a$ is OEIS A191522 , ""Number of valleys in all left factors of Dyck paths of length n. A valley is a (1,-1)-step followed by a (1,1)-step"". For example $a(4)=3$ because the total number of valleys in $UDUD, UDUU, UUDD, UUDU, UUUD, UUUU$ is $1+1+0+1+0+0=3$ , where $U=(1,1)$ , $D=(1,-1)$ . OEIS does not report the above formula. Is there a way to relate it directly to the above count of valleys of Dyck paths of length $n+2$ ? Or is at least possible to prove that it coincides with the other proposed formulas?","['summation', 'binomial-coefficients', 'combinatorics', 'oeis']"
4751544,Calculating the volume of $(x-z)^2+(y-z)^2 \le \sin^2(z) $,I am trying to calculate the volume of $(x-z)^2+(y-z)^2 \le \sin^2(z) $ and $0 \le z \le \pi $ . I am having trouble parameterising the equation and setting the bounds for the parameters. I think the $-z$ in the equation is just rescaling the object so it does not affect the volume. I have set the volume integral as $$ \pi \int_{0}^{\pi}\sin^2(z) dz$$ and found the volume as $\frac{\pi^2}{2} $ . Is this the correct answer or is my calculation wrong? Thank you very much!,"['cylindrical-coordinates', 'multivariable-calculus', 'volume']"
4751559,How can i evaluate the product? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question I have to evaluate the following product: $$\prod_{n=3}^{\infty} \frac{(n^3+3n)^2}{n^6-64} $$ But I couldn't develop anything. I thought about breaking the product into smaller parts with some factoring, but I couldn't calculate those products.","['limits', 'calculus', 'infinity', 'products']"
4751572,How to find unique least number of moduli when performing arithmetic addition of two integers using Chinese Remainder theorem?,"Here's my problem.
I have to find the sum of 123456 and 987456 using chinese remainder theorem with least moduli.
We know sum = 123456+987456=1,110,912.
Let me assume that we have to use n number of moduli m1,m2,...mn(pairwise relatively prime to represent the integers as n tuple). Eg: If I assume m1=99,m2=98,m3=97,m4=95 then
(123456)=(3,74,72,51).
Similarly for 987456 and the sum.
I also know that sum < m1 m2 m3*m4.
And I get system of linear congruences. Now my question is basically how to find the m1...mn which are pairwise and also the smallest combination possible (unique) so that it is just greater than the sum. Eg: I could have also taken m1=81,m2=83,m3=85,m4=89 that satisfies the above condition.
Surely there are other combinations as well. Is there any theorem where can I find smallest number of moduli such that I don't have to do hit and trial to search for moduli as it is very time consuming? Here's one example of such problem in the book Discrete maths by Rosen. Using chinese remainder theorem to do large integer arithmetic In the book , it basically tells us to do with moduli 99,98,97,95. However it gives no idea about how to find smallest combination of moduli we can take.","['elementary-number-theory', 'chinese-remainder-theorem', 'discrete-mathematics']"
4751590,What was the difficulty in enumerative geometry problems before physics?,"I have read the book 'Enumerative Geometry and String Theory' by Katz, and it left me with some questions. It is outlined in the text how ideas from String theory and TQFT has enriched enumerative geometry. Examples of problems that couldn't be solved, but were solved using these methods are: 'Number' of rational curves of degree $d$ in the quintic threefold Number of degree $d$ curves passing through $3d-1$ points in $\mathbb{P}^2$ . Personally, the fact that the second problem with $d\geq 5$ couldn't be solved without the help of physics was mind-boggling, especially when the answer is as small as 87,304. The classical methods outlined in the text seem pretty strong, and I want to know what exactly was the difficulty was to solving these problems. One point that Katz mentions is that the moduli space of stable maps was motivated by physics. But historically Deligne and Mumford compactified similar moduli spaces, and why couldn't people use this moduli space to solve the problems? Please note that I am not fully aware of some 'subtleties' that might be present in enumerative geometry, since I know only very crudely how enumerative problems are solved. If you could identify the hard point inside(or outside!) the follwing steps of solving enumerative problems that would be nice. Find a moduli space compactifying the geometric objects considered. Compute the cohomology ring of the moduli space. Identify what needs to be integrated over the moduli spade. Find the excess intersection, and show that the answer is actually enumerative.","['moduli-space', 'string-theory', 'mirror-symmetry', 'algebraic-geometry', 'mathematical-physics']"
4751619,category theory for algebraic geometry,"I don't know if this question has been asked before,this year i take a course in algebraic geometry in the first 5 paragraph of chapter 2 of hartshorne. Our professor doesn't use  a lot the language of category theory, but he advised us to  learn it, for example in the course he asked us to show that kernel commute with stalk in classical way a by proving that a map is well defined and isomorphism, but i see that this result is particular case of limit commute with kernel. another example is proving stalk of inverse image using adjoint. so my question is : -what important part that should be learned. -some mathematicians treat isomorphism like equality for example some define exact sequence such that kernel equal image but some define exact sequence as complex and canonical morphism between kernel and image is isomorphism,what your advice about that. And thanks","['algebraic-geometry', 'category-theory']"
4751723,PDE: Is the solution unique?,"Consider the partial differential equation: $$ t\frac{\partial^2}{\partial t^2} \sum_{n=1}^k \Phi_n(x,t)=-x\frac{\partial}{\partial x}\sum_{n=1}^k \Phi_n(x,t)+\sum_{n=2}^{k-1}a(n)\Phi_n(x,t) $$ where $a(n)$ is some prescribed sequence of natural numbers. For now let $a(n)$ be the oblong numbers . I'm wondering how much power the sequence $a(n)$ has on constraining the solution space. For example, if you specify an $a(n),$ does this imply the solution is unique, if it exists? Fix $a(n)$ to be the oblong numbers. Then using an exponential ansatz, I found a solution to the equation: $$ \Phi_n(x,t)=e^{\frac{nt}{\log x}} $$ Are there other solutions, or is this unique? Edit: Letting $k \to \infty$ we obtain: $$ t\frac{\partial^2}{\partial t^2}\Psi(x,t)=-x\frac{\partial}{\partial x}\Psi(x,t)+h(x,t)$$ where $$\Psi(x,t)=-\frac{e^{\frac{t}{\log x}}}{e^{\frac{t}{\log x}}-1}=\sum_{n=1}^\infty\Phi_n(x,t)$$ and $$h(x,t)= -\frac{2e^{\frac{t}{\log x}}}{(e^{\frac{t}{\log x}}-1)^3}=\sum_{n=2}^{k-1}a(n)\Phi_n(x,t) $$","['ordinary-differential-equations', 'analysis', 'linear-pde', 'partial-differential-equations', 'sequences-and-series']"
4751733,Probability of dart hitting a point given that it hits a point in a finite set of points,"I understand the probability of the dart hitting the center point is 0, and I think this is used as an example of how a probability of 0 doesn't mean something is impossible. Now image we define 3 arbitrary points on the dart board, say, the center point C, and the two points between the center and the top/bottom edges called T and B respectively. Also let's suppose the player throws the dart in such a way that it's equally likely to hit any point on the board. What's the probability of the dart having landed on C given that we know it already landed on one of the 3 points? Intuitively the answer seems to be 1/3. But working out the math would give an undefined result if I attempt to solve it like P(C|T∪C∪B) = P(C)/P(T∪C∪B) = 0/0. Is there a way to properly solve this? Or is the only way to just assign equal probabilities to all points and ignore the fact that the probability of hitting any one point is zero? EDIT: I think I made some progress thanks to your comments, but I hope someone smarter than myself can comment on my attempted solution. I think the zero probability comes from dividing 1/∞, which afaik is by itself not well defined. The full rigorous expression would be $\lim_{x \to \infty} \frac{1}{x}$ Intuitively $x$ is the number of points in the board, so the probabilities of the dart landing on each point is $1/x$ and the probability of landing on any of the 3 points is $3/$ x, so the full probability now becomes: $\lim_{x \to \infty} \frac{1/x}{3/x}$ Which if I'm not mistaken is just 1/3. EDIT 2: I get the impression that most comments and answers here suggest that it's impossible to calculate these probabilities unless we come up with some ad hoc definitions. I just want to clarify that the dart board is a metaphor for a random point in a circle which, in my above example, has a uniform distribution. Since the example is so trivial it provides little motivation to actually solve it, so here is another example that is a little less trivial based on @Vincent's comment. Imagine a random real generator R that generates a real number from -1 to +1 that has a probability density function D. Also, imagine that I wrap R into another function F that returns the absolute value of the number produced by R like F = ABS(R()) . So, let's say we run F and it outputs $n$ . What's the probability that the number generated by R was actually $n$ (as opposed to $-n$ ), given that we know the density function D? If I'm not mistaken, the probability is just $\frac{D(n)}{D(-n)+D(n)}$ , which I can't prove but intuitively seems right. Applying the same logic to the original dart problem would again give 1/3 without having to deal with divisions by zero (at least not explicitly) and without the need for any ad hoc definition.","['conditional-probability', 'probability', 'infinitesimals']"
4751742,A phone number has seven digits and cannot begin with a 0. How many phone numbers contain the sequence 123?,I was going though a course on edx.org and there the instructor counted $5$ different instances like $1 2 3$ _ _ _ _ $ = 10^4$ _ $1 2 3 $ _ _ _ $= 9\times10^3$ _ _ $1 2 3 $ _ _ $= 9\times10^3$ _ _ _ $1 2 3$ _ $ = 9\times10^3 - 10$ _ _ _ _ $1 2 3 $ $= 9\times10^3 - 10 - 9$ And add all of them. I understood what the instructor did here. But what she did was manually count the exceptions. Would I also do it manually in case the number of digits are $10 $ or $15 $ or may be $20 ?$ I think it would be very tedious. There should some standard solution rather than manually revisiting what the exceptions are. What is the standard solution and also explain please?,"['combinatorics-on-words', 'combinatorics', 'discrete-mathematics']"
4751765,Frobenius action on the Picard group,"I have been concerned for the better part of today with the following problem: let $X/\mathbb{F}_q$ be a geometrically connected smooth projective curve. The trinity of Frobenii on $X_{\overline{\mathbb{F}_q}}$ consists of The absolute Frobenius $F$ The relative Frobenius $F_r=F_{X/\mathbb{F}_q}\times\text{id}$ The geometric Frobenius $F_g=\text{id}\times\text{Frob}^{-1}$ It is known that $F$ acts trivially on all étale cohomology groups of $X_{\overline{\mathbb{F}_q}}$ ; in particular, it acts trivially on $\text{Pic}(X_{\overline{\mathbb{F}_q}})$ . Moreover, $F=F_r\circ F_g=F_g\circ F_r$ , so the pullback $F_r^*:\text{Pic}(X_{\overline{\mathbb{F}_q}})\rightarrow\text{Pic}(X_{\overline{\mathbb{F}_q}})$ should be an isomorphism. This bugs me because I thought that $F_r$ was a degree $q$ map between curves! This would entail that no degree $1$ divisor is in the image of $F_r^*$ . Surely there is some mistake in my argument, but I haven't managed to spot it for myself :(","['algebraic-curves', 'algebraic-geometry', 'etale-cohomology', 'positive-characteristic']"
4751792,Convergence of the integration of three functions,"I'm considering a convergence problem of the integration of three functions say $f_{k},g_{k},h_{k}$ . What I have is on a bounded smooth domain $\Omega$ , $f_{k}\to f$ strongly in $L^{2}$ , $g_{k}\rightharpoonup g$ weakly in $L^{2}$ and $h_{k}\to h$ a.e. in $\Omega$ and $|h_{k}|,|h|\leq C$ for all $x\in\Omega$ . Besides, I also have $\lVert f_{k}\rVert_{2},\lVert g_{k}\rVert_{2}\leq C$ . Then my question is whether \begin{align}
\int_{\Omega}f_{k}g_{k}h_{k}dx\to\int_{\Omega}fghdx.
\end{align} My try is \begin{align*}
&|\int_{\Omega}f_{k}g_{k}h_{k}-\int_{\Omega}fghdx|\\
\leq&|\int_{\Omega}f(g_{k}-g)hdx|+|\int_{\Omega}(f_{k}-f)g_{k}h_{k}dx|+|\int_{\Omega}fg_{k}(h_{k}-h)dx|
\end{align*} The first term goes to zero by weak convergence, the second by strong convergence and the last by dominated convergence theorem. Does my thought make sense?","['weak-convergence', 'sobolev-spaces', 'analysis', 'real-analysis']"
4751796,"Different sidewalks that can be laid using n $1 \times 1$ square white, blue and red tiles so that no two red tiles lie next to each other?","How many different sidewalks of length $n \geq 1$ can be laid using n $1 \times 1$ square tiles, given an unlimited supply of such tiles in white, blue and red colors so that no two red tiles lie next to each other? We have that: $f_1 = 3$ , we can choose white, blue or red tile; $f_2 = 8$ , we can choose every combination from set: WW, BB, WB, BW, RB, BR, WR, RW (only RR can't be chosen); $f_3 = 22$ , we can choose every combinatorion ( $3 \cdot 3 \cdot 3 = 27$ ) besides RRR $(1)$ , RR[not R] $(2)$ , [not R]RR $(2)$ , therefore there is: $27 - 1 - 2 - 2 = 22$ ; $f_4 = 64$ , we can choose every combinatorion ( $3 \cdot 3 \cdot 3 \cdot 3 = 81$ ) besides RRRR $(1)$ , RR[not R][every possible] $(6)$ , [not R][every possible]RR $(6)$ , [not R]RR[not R] $(4)$ therefore there is: $81 - 1 - 6 - 6 - 4 = 60$ ; Then I came up with an recursive formula: $f_n = 2f_{n-1} + 2f_{n-2}$ . We take all the sequences without R on the last place - we can have W or B on the last place, therefore we have $2f_{n-1}$ . Then we take all the sequences with R on the last place, those can't have R on the next-to-last place, therefore there is $2f_{n-2}$ of them. The formula works for $f_3$ and $f_4$ so I assume it's correct. So I have: $f_1 = 3$ , $f_2 = 8$ , $f_n = 2f_{n-1} + 2f_{n-2}$ . I calculate: $f_n = 2f_{n-1} + 2f_{n-2} \iff f_n - 2f_{n-1} - 2f_{n-2} = 0$ $x^2 - 2x - 2 = 0 \implies \Delta = 4 - 4 \cdot 1 \cdot (-2) = 12 \implies \sqrt{\Delta} = 2\sqrt{3}$ $x_1 = \frac{2 + 2\sqrt{3}}{2} = 1 + \sqrt{3} \ $ and $ \ x_2 = \frac{2 - 2\sqrt{3}}{2} = 1 -\sqrt{3}$ $f_n = a(1 + \sqrt{3})^n + b(1 - \sqrt{3})^n$ so: $a(1 + \sqrt{3}) + b(1 - \sqrt{3}) = 3 \iff a + \sqrt{3}a + b - \sqrt{3}b = 3$ $a(1 + \sqrt{3})^2 + b(1 - \sqrt{3})^2 = 8 \iff a(1 + 2\sqrt{3} + 3) + b(1 - 2\sqrt{3} + 3) = 8 \iff 2a + \sqrt{3}a + 2b - \sqrt{3}b = 4$ By subtracting first equation from second one we get: $a + b = 1 \iff a = 1 - b$ $1 - b + \sqrt{3}(1 - b) + b - \sqrt{3}b = 3 \iff b = \frac{1}{2} - \frac{\sqrt{3}}{3}$ $a = 1 - \frac{1}{2} + \frac{\sqrt{3}}{3} = \frac{1}{2} + \frac{\sqrt{3}}{3}$ So: $f_n = (\frac{1}{2} + \frac{\sqrt{3}}{3})(1 + \sqrt{3})^n + (\frac{1}{2} - \frac{\sqrt{3}}{3})(1 - \sqrt{3})^n$ Is that correct?","['recurrence-relations', 'recursion', 'combinatorics', 'discrete-mathematics', 'tiling']"
4751811,How to solve: $\int{\frac{r}{\sqrt{a r^2 + b r + c}}}dr$,"I've been trying to solve the following integral: $$\int{\frac{r}{\sqrt{a r^2 + b r + c}}}dr$$ But I don´t get the substitution trick that I have to perform. I've tried to: $$
r^2 = x
$$ $$
2r dr = dx
$$ $$\frac{1}{2}\int{\frac{dx}{\sqrt{a x + b \sqrt{x} + c}}}$$ But I also do not know how to solve this. I've also tried to do decomposition in simple fractions, but this won´t work because of the root. I would apreciate any hint on what variable change I should perform.","['integration', 'indefinite-integrals', 'calculus']"
4751863,Maximise $\left( \sum_{i=1}^{n} p_i \cdot i \right) - \left( \max_{j=1}^{n} p_j \cdot j \right)$ with $p$ permutation of size $n$,"I'm trying to maximise the following value: $\left( \sum_{i=1}^{n} p_i \cdot i \right) - \left( \max_{j=1}^{n} p_j \cdot j \right)$ where $p$ is an array consisting of $n$ distinct integers from $1$ to $n$ in arbitrary order (a permutation of $\{1, 2, \dots, n\}$ ). I have to find an optimal permutation $p$ . A brute force approach in Python can allow us detect a pattern: from __future__ import annotations  # noqa
from itertools import permutations  # noqa

if __name__ == ""__main__"":
    for n in range(1, 12):
        elements = range(1, n+1)
        perms = permutations(elements, n)
        best: int = 0
        best_perm: list[int] | None = None
        for perm in perms:
            perm: list[int] = list(perm)
            s = 0
            max = 0
            for i in range(0, n):
                s += (i+1) * perm[i]
                if (i+1) * perm[i] > max:
                    max = (i+1) * perm[i]
            s -= max
            if s >= best:
                best = s
                best_perm = perm
        print(f""{n=}, {best_perm=}, {best=}"") Outputs: n=1, best_perm=[1], best=0
n=2, best_perm=[2, 1], best=2
n=3, best_perm=[1, 3, 2], best=7
n=4, best_perm=[1, 4, 3, 2], best=17
n=5, best_perm=[1, 2, 5, 4, 3], best=35
n=6, best_perm=[1, 2, 3, 6, 5, 4], best=62
n=7, best_perm=[1, 2, 3, 7, 6, 5, 4], best=100
n=8, best_perm=[1, 2, 3, 4, 8, 7, 6, 5], best=152
n=9, best_perm=[1, 2, 3, 4, 5, 9, 8, 7, 6], best=219
n=10, best_perm=[1, 2, 3, 4, 5, 6, 10, 9, 8, 7], best=303
n=11, best_perm=[1, 2, 3, 4, 5, 6, 7, 11, 10, 9, 8], best=406 We can notice that the numbers in the permutation ""go up"", then ""go down starting from $n$ "". It makes sense (small numbers in the beginning, and we can't put the largest number at the end since there is the $\max_{j=1}^{n} p_j \cdot j$ term subtracted in the function we are trying to maximise). Could it potentially fail for large values of $n$ ? However, I have thoroughly tested all computationally-feasible cases and have not discovered a counterexample. I'm struggling to find a proof. This post is adapted from the Problem - C competition from codeforces that ended on the 12th of August 2023. EDIT: I'm looking for a proof of the pattern , but there might be a proof to find an permutation that maximises the value we are trying to maximise, which would be even more satisfying.","['permutations', 'contest-math', 'proof-writing', 'combinatorics', 'discrete-optimization']"
4751895,Why are infinite-dimensional vector spaces usually equipped with additional structure?,"In a first course in linear algebra, it is common for instructors to mostly restrict their attention to finite-dimensional vector spaces. These vector spaces are usually not assumed to be equipped with any additional structure, such as an inner product, norm, or a topology. On the other hand, it seems that when infinite-dimensional vector spaces are encountered in later courses, it is much more common to equip with them additional structure. Why is this? A partial answer might be that infinite-dimensional vector spaces are often studied in functional analysis, where extra structure is needed to properly define analytical concepts such as infinite series. However, I would have expected that ""pure"" infinite-dimensional vector spaces have a use in some area of mathematics. I have now also asked this on MathOverflow .","['soft-question', 'linear-algebra', 'functional-analysis', 'vector-spaces']"
4751946,Compute the induced homomorphism on deRham cohomology,"Consider the smooth map on the torus to itself $f: S^1 \times S^1 \rightarrow S^1 \times S^1$ defined by $f(z_1, z_2) = (z_1^2z_2, z_1^{-1}z_2)$ (here we identify $S^1$ as the unit circle on the complex plane). Compute the homomorphism $f^*: H^2_{dR}(S^1 \times S^1) \rightarrow H^2_{dR}(S^1 \times S^1)$ . Hint: Compute the degree of $f$ . My attempt: So I want to first compute the degree of $f$ . There are three ways I know to compute the degree: (i) compute $f_*:H_2(S^1 \times S^1) \rightarrow H_2(S^1 \times S^1)$ (taking $[x] \mapsto \deg(f) [x]$ ); or (ii) compute $\int_{S^1 \times S^1} f^* \omega = \deg(f) \cdot \int_{S^1 \times S^1} \omega$ ; or (iii) find a regular value $y$ of $f$ and compute $\mathrm{deg}(f) = \sum\limits_{x \in f^{-1}(y)} \mathrm{sgn}(x)$ . It seems like (ii) is the easiest so I work out the following computation: $$\int_{S^1 \times S^1} f^*(dz_1 \wedge dz_2) = \int_{S^1 \times S^1} d(z_1^2 z_2) \wedge d(z_1^{-1} z_2) = 3 \int_{S^1 \times S^1} z_2 dz_1 \wedge dz_2$$ It seems like the $2$ -form I chose led to an inconclusive result because the integrand has a factor of $z_2$ . Would I have to explicitly compute $\int_{S^1 \times S^1} z_2 dz_1 \wedge dz_2$ to retrieve the degree of $f$ ? (Maybe I should've done (iii) instead) Also, thinking ahead, after I compute the degree of $f$ , how does that tell me what the induced homomorphism on deRham cohomology is? I am guessing that it would be multiplication by $\frac{1}{\mathrm{deg}(f)}$ as it is the ""inverse"" to the homology case written in (i).","['smooth-functions', 'de-rham-cohomology', 'differential-forms', 'differential-geometry']"
4751955,"Meaning of $\frac{dx}{f(x,y)}=\frac{dy}{g(x,y)}$","Consider the rotation group $SO(2)$ which has infinitesimal generator $v = -y\partial_x +x\partial _y$ . The corresponding characteristic
system is $$\frac{dx}{-y}=\frac{dy}{x}.$$ This first order ordinary
differential equation is easily solved; the solutions are $x^2+y^2=c$ for $c$ an arbitrary constant. I don't really understand the form of the ODE. Does it really mean $\frac{dy}{dx}=-\frac{y}{x}$ ? and that the solutions are $y = \pm\sqrt{c-x^2}$ for any $c$ ? Why write it into this weird form?","['ordinary-differential-equations', 'differential-geometry']"
4752016,Solving ODE Derived from Weird Problem: $\sqrt{k^2-{f'(t)}^2}f(t)-\sqrt{t^2-{f(t)}^2}f'(t)=t\sqrt{k^2-1}$,"$$\large \text{Problem}$$ I'll give the question first and then the motivation.
Is there a closed form solution to the differential equation $$\sqrt{k^2-{f'(t)}^2}f(t)-\sqrt{t^2-{f(t)}^2}f'(t)=t\sqrt{k^2-1}$$ with initial value $$f{\left(\frac{1}{\sqrt{k^2-1}}\right)}=\frac{1}{\sqrt{k^2-1}}?$$ What about $$\sqrt{k^2-{g'(t)}^2}(1-g(t))+\sqrt{t^2-{(1-g(t))}^2}g'(t)=t\sqrt{k^2-1}$$ with initial value $$g{\left(\frac{1}{\sqrt{k^2-1}}\right)}=1?$$ Alternatively, can we give the solution to one as a function of the solution to the other? These are increasingly tricky approaches to solving the problem given in the motivation. $$\large \text{Motivation}$$ I have been playing in my spare time with what happens when we have two ""blobs"" in $\mathbb{R}^2$ that radially expand from every point in the blob at a constant speed, starting as two points, but stop expanding anywhere they come into contact. More formally, we have sets $A_t$ and $B_t$ indexed by time $t\geq0$ such that $A_0=(0,0)$ and $B_0(1,0)$ . For all $x\in\mathbb{R}^2$ and times $s>t$ , if there exists a path from $A_t$ to $x$ of length $k(s-t)$ that does not intersect $B_r$ for any $r<s$ , the $x\in A_s$ . For all $x\in\mathbb{R}^2$ and times $s>t$ , if there exists a path from $A_t$ to $x$ of length $s-t$ that does not intersect $B_r$ for any $r<s$ , the $x\in A_s$ . The difference being the appearance of $k$ , the speed of $A_t$ 's expansion relative to that of $B_t$ . We suppose that $k>1$ , without loss of generality. For all $x\in A_s$ , there must be a path $\gamma(t)$ to $x$ such that $\gamma(t)$ 's length changes at a rate of at most $k$ , and for all $t<s$ , $\gamma(t)\in A_t\setminus B_t$ . Similarly for $B_s$ , but we change the rate of growth from $k$ to $1$ . For a point $x\in\mathbb{R}^2$ , if there exists $t$ such that $x\in A_t\cap B_t$ , then for all times $s$ , if $x\in A_s$ , then $x\in B_s$ . These conditions uniquely define $A_t$ and $B_t$ . I am interested in the final shapes of $A=\bigcup_{t\geq0}A_t$ , and $B=\bigcup_{t\geq0}B_t$ . We can show that $B$ is a closed set, and $A$ is the closure of its complement, so we can obviously just focus on $B$ . I know that $B$ is convex and bounded, and its boundary can be split into three curves. The first is easy. We consider the $x$ such that the distance from $x$ to $(0,0)$ is $k$ times the distance from $x$ to $(1,0)$ . If there is no issue with the straight line path between $x$ and $(0,0)$ intersecting with $B_t$ , or the straight line path between $x$ and $(1,0)$ intersecting with $A_t$ , then the paths clearly belong to $A$ and $B$ respectively, and they meet at these boundary points. The set of $x$ satisfying this distance equation is a certain circle, with $(1,0)$ on the inside and $(0,0)$ on the outside. Things are okay until the path from $(0,0)$ becomes tangent to the circle, at which point it's no longer obvious that it avoids $B_t$ , and indeed, it does not. There are two points on the circle which meet these tangent paths. These are the beginnings of the other two curves, which by symmetry, will obviously be mirror images in the line $y=0$ . So we might as well focus on the top. We can show that what needs to happen is that as we trace along the curve at speed $k$ , we just meet up with the expansion of $A_t$ . Now, we can show without much difficulty that $B_t$ is only ever going to expand via straight lines from $(1,0)$ . These lines are going to be growing radially from the points on the boundary of a circle of radius $t$ centred at $(1,0)$ which have not already been obstructed by the growth of $A_t$ . To get a little heuristic, locally, at a point $c(t)$ on the curve, we're going to be meeting an expanding wall of growth at unit speed, going up in parallel lines. What kind of motion is going to beat the wall? Well, we'll be moving at speed $k$ , so we need to pick an angle such that the movement in the direction of the wall is at unit speed. This is actually already satisfied by the derivative of the first curve at the starting point of $c$ . The curves will stop when they run into each other somewhere along $y=0$ . $$\text{To summarise}$$ the magnitude of $c$ 's derivative is $k$ . $c$ is always on a point of the circle of radius $t$ centred at $(1,0)$ . $c$ 's derivative forms a constant angle with the tangent line to this circle, which we can calculate from the initial value. We can also calculate the initial value of $t$ , where $A_t$ and $B_t$ no longer grow in lines colliding with each other, and the curve starts. Writing out the coordinates of $c$ and its derivative, and putting all this into the language of inner products, we find that we can solve for the $x$ -coordinates from the $y$ -coordinates and $t$ and vice versa. We then put the equations together to get the $y$ coordinate given by the function $f(t)$ above. $g(t)$ gives the $x$ coordinate. $$\text{EDIT}$$ It's been pointed out to me that the properties defining my differential equation are satisfied by logarithmic spirals (by the differential equation, this means that logarithmic spirals with given initial conditions uniquely satisfy them). Logarithmic spirals are however given in polar coordinates, so I suppose my original question becomes one of expressing a part of a logarithmic spiral as a closed-form Cartesian function. But I can't find this, so I suspect that there is none. Anyway, as far as characterising the set goes I'm satisfied. It's more natural to shift everything to the left by $1$ . Then the boundary is given by these curves: The part of circle with centre $\left(\frac{1}{k^2-1},0\right)$ and radius $\frac{k}{k^2-1}$ to the left of the $y$ -axis, The polar curve $r=\frac{1}{\sqrt{k^2-1}}e^\frac{2\theta-3\pi}{2\sqrt{k^2-1}}$ from $\theta=\frac{3\pi}{2}$ to $2\pi$ , and The reflection of that curve in the $x$ -axis. Here is an obligatory picture of boundaries for a family of $B$ 's for different $k$ 's, which is certainly what I wanted out of this.","['plane-curves', 'euclidean-geometry', 'ordinary-differential-equations']"
4752040,Characterizing smooth functions in polar coordinates,"This is a question about representing smooth functions in polar coordinates. I'll write $\mathbb{C}$ for the complex plane with coordinates denoted interchangeably by $z=x+iy=(x,y)$ . Meanwhile, I'll write $\mathbb{R}^2$ for the $(r,\theta)$ -plane. I'll give a bit of a lengthy preamble, but you are welcome to scroll down for the question. I titled it in bold. OK, let's suppose $f:\mathbb{C}\to\mathbb{C}$ is a $C^\infty$ smooth function. Correspondingly, we get a $C^\infty$ smooth function $g:\mathbb{R}^2 \to \mathbb{C}$ defined by $g(r,\theta)=f(re^{i\theta})$ . Let's ask ourselves the question ""what kind of smooth functions $g$ can arise this way?"". Clearly it's not all of them. For one thing, $g(0,\theta)=f(0)$ for all $\theta$ . Another thing: $g$ is $2\pi$ -periodic in the $\theta$ variable [ Correction: actually it must satisfy the stronger symmetry condition $g(r,\theta)=g(-r,\theta+\pi)$ ]. But is that the whole story? No it is not. We also have to account for infinitesimal behaviour. For example, it is quite easy to check that $$ g_r(0,\theta) =  f_x(0) \cos \theta +  f_y(0) \sin \theta,$$ $$g_{rr}(0,\theta) = f_{xx}(0)\cos^2 \theta  + 2f_{xy}(0)\cos\theta \sin\theta +f_{yy}(0) \sin^2 \theta $$ and so on,  using subscript shorthand for partial derivatives. OK, so it seems that the $r$ partial derivatives of $g$ are forced to have very particular forms along the $\theta$ -axis. What is really going on here? How can we formalize this and understand better what exactly the restriction on $g$ is? Here is my attempt to make sense of things. We have a usual Taylor expansion for $f$ at the origin: $$ f(x,y) \sim \sum_{m,n \geq 0} a_{m,n} x^m y^n.$$ That is not too helpful for understanding what's going on with $g$ .  However, I believe it should also make sense (and be equivalent) to talk about the Taylor expansion of $f$ in the variables $z=x+iy$ and $\overline z = x-iy$ : $$ f(x,y) \sim \sum_{m,n \geq 0} b_{m,n} z^m \overline z^n.$$ I guess that, into this expansion, we can directly substitute $z=re^{i\theta}$ , $\overline z=re^{-i\theta}$ to obtain: $$g(r,\theta) \sim \sum_{m,n \geq 0} b_{m,n} r^{m+n} e^{i(m-n)\theta}$$ where this should be thought of as a kind of asymptotic expansion along the $\theta$ -axis,  as $r \to 0$ . We could also rewrite this as: $$g(r,\theta) \sim \sum_{n \geq 0}  p_n(e^{i\theta}) r^n$$ where $p_n$ is a polynomial of the form $p_n(u) = \sum_{m=0}^n c_{m,n} u^{n-2m}$ . The above is all to sketch a proof of the following claim. Claim: If $f(x+iy)$ smooth function $\mathbb{C} \to \mathbb{C}$ , then  the function $g(r,\theta)=f(re^{i\theta})$ has the property $\frac{\partial^n}{\partial r^n} g(0,\theta) = p_n(e^{i\theta})$ where $p_n(u) = \sum_{m=0}^n c_{m,n} u^{n-2m}$ where all the $c_{m,n}\in\mathbb{C}$ . OK, so now we have a pretty good idea of what the restriction should be and the question is: Question: Does the converse hold? If $g(r,\theta)$ is a smooth function with the property in the claim above that is also $2\pi$ -periodic in $\theta$ (and constant on the $\theta$ -axis), does it follow that $g(r,\theta)=f(re^{i\theta})$ for some smooth function $f:\mathbb{C}\to\mathbb{C}$ ? Assuming the answer is ""yes"", I would be happy to be pointed to a reference. I'm not overly concerned with seeing a proof, just wanted some reinforcement that this is correct. I can somewhat imagine how this might go even. Given such a function $g$ , I suppose we can subtract off a function $h(re^{i\theta})$ with the same expansion as $g$ along the $\theta$ axis using Borel's theorem. Then we have a $g$ with an identically vanishing expansion and the problem becomes to show under that assumption that we can push it forward to a smooth function while keeping it smooth which sounds pretty believable to me. Also welcome would be a nice rephrasing of what I've written in better language! I feel like I should be trying to package this more in terms of harmonic analysis or representation theory, but I'll leave it in the above form for now. Thanks for reading this long rambling question! Added 13/Aug/2023 Two things to add: Firstly, it should be pointed out that I missed a necessary condition above. Given $f(x,y)$ smooth, the corresponding $g(r,\theta) =f(re^{i \theta})$ is clearly not simply $2\pi$ -periodic in $\theta$ , but actually satisfies the stronger symmetry condition $g(-r,\theta+\pi) = g(r,\theta)$ of which $2\pi$ -periodicity in $\theta$ is a consequence. Secondly, here is a bit more information about pushing forward a given smooth function $g(r,\theta)$ that satisfies the symmetry condition $g(-r,\theta+\pi) = g(r,\theta)$ and vanishes to infinite order along $r=0$ . Let me write $T(r,\theta)=re^{i\theta}$ for the polar coordinates transformation, which is a local diffeomorphism away from $r=0$ .  Note that $\frac{\partial}{\partial x}$ and $\frac{\partial}{\partial_y}$ pull back under $T$ to mercifully nice operators vector fields. Specifically: \begin{align*}
T^*(\frac{\partial}{\partial x}) = \cos \theta \frac{\partial}{\partial r} - \frac{\sin \theta}{r} \frac{\partial}{\partial \theta} &&
T^*(\frac{\partial}{\partial y}) = \sin \theta \frac{\partial}{\partial r} + \frac{\cos \theta}{r} \frac{\partial}{\partial \theta}
\end{align*} From the symmetry condition alone, it follows that there is a unique function $f$ such that $g=f \circ T$ . The problem is smoothness of $f$ . Firstly, clearly $f$ is $C^\infty$ away the origin (where $T$ is a local diffeomorphism). It is also clear that $f$ vanishes to infinite order at the origin, in the sense that it vanishes faster than any power of $r$ . This already implies that $f$ is differentiable at $0$ with vanishing Jacobian. On the other hand, from the above pullback formulas, it can be seen that the first order partials of $f$ are the pushforwards of functions in the same class as $g$ (vanishing to infinite order on $r=0$ and with the same symmetry condition). So, by induction, all the partial derivatives of $f$ exist and vanish at the origin.","['complex-analysis', 'polar-coordinates', 'smooth-functions', 'taylor-expansion']"
4752060,"Evaluating $\int_{0}^{1}\frac{\arctan(x)}{x^2+x+1}\,dx$","As in the title I was evaluating the integral, $$I:=\int_{0}^{1}\frac{\arctan(x)}{x^2+x+1}\,dx$$ But numerically speaking, I seemed to have gotten an incorrect answer. What have I done wrong? Here is my work. Let $$x\longrightarrow{\frac{1-x}{1+x}}$$ $$I=2\int_{0}^{1}\frac{\frac{\pi}{4}-\arctan(x)}{3+x^2}\,dx$$ $I.B.P$ $$I=\frac{\pi^2}{12}\left(1-\frac{1}{\sqrt{3}}\right)+\frac{1}{\sqrt{3}}\int_{0}^{1}\frac{\arctan\left(\frac{x}{\sqrt{3}}\right)}{1+x^2}\,dx$$ Let $$x\longrightarrow{\tan(x)}$$ $$I=\frac{\pi^2}{12}\left(1-\frac{1}{\sqrt{3}}\right)+\frac{1}{\sqrt{3}}J$$ $$J:=\int_{0}^{\frac{\pi}{4}}\arctan\left(\frac{\tan(x)}{\sqrt{3}}\right)\,dx$$ To evaluate $J$ define: $$J(t):=\int_{0}^{\frac{\pi}{4}}\arctan\left(t\tan(x)\right)\,dx$$ We can see that: $$J(0)=0,J\left(\frac{1}{\sqrt{3}}\right)=J$$ Now it follows that from the $F.T.C.$ $$J=\int_{0}^{\frac{1}{\sqrt{3}}}J’(t)\,dt$$ And so: $$J’(t)=\int_{0}^{\frac{\pi}{4}}\frac{\tan(x)}{t^2\tan^2(x)+1}\,dx$$ Multiply the numerator and denominator by $\sec(x)$ then let: $$\sec(x)\longrightarrow{x}$$ $$J’(t)=\int_{1}^{\sqrt{2}}\frac{1}{x(t^2x^2+1-t^2)}\,dx$$ $$=\frac{1}{1-t^2}\left[\int_{1}^{\sqrt{2}}\frac{1}{x}\,dx-\int_{1}^{\sqrt{2}}\frac{t^2x}{t^2x^2+1-t^2}\,dx\right]$$ $$J’(t)=\frac{1}{1-t^2}\frac{1}{2}\left[\log(2)-\log(1+t^2)\right]$$ And so $$J=\frac{1}{2}\int_{0}^{\frac{1}{\sqrt{3}}}\frac{1}{1-t^2}\left[\log(2)-\log(1+t^2)\right]\,dt$$ Let $$t=\frac{1-x}{1+x}$$ $$J=\frac{1}{4}\int_{2-\sqrt{3}}^{1}\frac{\log\left(\frac{(1+x)^2}{1+x^2}\right)}{x}\,dx$$ $$=\frac{1}{4}\left[2\int_{2-\sqrt{3}}^{1}\frac{\log(1+x)}{x}\,dx-\int_{2-\sqrt{3}}^{1}\frac{\log(x+i)}{x}\,dx-\int_{2-\sqrt{3}}^{1}\frac{\log(x-i)}{x}\,dx\right]$$ Now using $$\int\frac{\log(x+a)}{x}\,dx=\log(a)\log(x)-Li_{2}\left(-\frac{x}{a}\right)$$ $$=-\frac{1}{4}\left[2Li_{2}(-x)+Li_{2}(ix)+Li_{2}(-ix)\right]_{2-\sqrt{3}}^{1}$$ Using: $$Li_{2}(z)+Li_{2}(-z)=\frac{1}{2}Li_{2}(z^2)$$ We have $$=-\frac{1}{4}\left[2Li_{2}(-x)+\frac{1}{2}Li_{2}(-x^2)\right]_{2-\sqrt{3}}^{1}$$ And so $$J=\frac{5\pi^2}{96}+\frac{1}{4}\left[2Li_{2}(\sqrt{3}-2)+\frac{1}{2}Li_{2}(4\sqrt{3}-7)\right]$$ And so we have that $$I=\frac{\pi^2}{12}-\frac{7\pi^2}{96\sqrt{3}}+\frac{1}{4\sqrt{3}}\left[2Li_{2}(\sqrt{3}-3)+\frac{1}{2}Li_{2}(4\sqrt{3}-7)\right]$$ Where did I how wrong? Any help is appreciated.","['integration', 'solution-verification', 'definite-integrals', 'real-analysis']"
4752066,Doubt about the power of $x$ of the function to find that $f(x) = 3$ is an even function,"Using the definition of even functions, $f(-x) = f(x) = 3$ , hence it is an even function. Also, it is being mentioned that if $f(x)$ is an even power of $x$ , then it is an even function of $x$ . Same for an odd function. We know that the equation of a horizontal line in slope-intercept form is $y = mx + b$ where $m$ is the slope and $b$ is the $y$ -intercept. That means, the equation of the horizontal line $y = 3$ can be written as, $y = 0x^1 + 3$ where $m = 0$ and b = 3 . Therefore, $y = f(x) = 0x^1 + 3 = 0x^1 + 3x^0$ Now, what is the power of the function above, $0$ or $1$ ? I want to apply the even power method to determine whether $f(x) = 3$ is an even function or let any function having multiple terms with $x^n$ to determine whether it is an even function (or an odd function let's say) just by using the power method.","['even-and-odd-functions', 'functions']"
4752079,Proving that there are infinitely many points on the unit circle such that the distance between any two of them is a rational number.,Having trouble solving this Math contest problem. Prove that there are infinitely many points on the unit circle such that the distance between any two of them is a rational number. It's obvious to see that there are infinitely many points on a unit circle such that the distance between two points on a unit circle is rational and it's also obvious that there are infinitely many rational distances between two points. But how in the world can you construct an infinite set of points such that the distance between any two points is rational?,"['contest-math', 'number-theory', 'measure-theory', 'trigonometry']"
4752131,Intuition behind connection 1-forms and Ehresmann connections,"I am learning about mathematical gauge theory and so far I have been able to develop an intuition behind all the objects I've read about such as principal bundles, associated bundles, and vertical/horizontal tangent spaces. However I have recently come across Ehresmann connections and connection 1-forms and I have been having trouble seeing the motivation behind them. I understand the technical definition for both but I cannot picture what idea they're trying to capture. Given that there is a one-to-one correspondence between connection 1-forms and Ehresmann connections I assume they both represent the same idea, but I am missing what that idea is. This lack of intuition has made it hard for me to understand further concepts such as curvature 2-forms and the motivation behind those since they're defined in terms of connection 1-forms. The only motivation I have for connection 1-forms is that they somehow ""pick out"" the the vertical component of a tangent vector in the principal tangent bundle. In a lecture given by Frederic Schuller ( https://www.youtube.com/watch?v=jFvyeufXyW0, ), he says around 16:30 that the idea of a connection is to make a choice of how to ""connect"" the individual points of ""neighboring"" fibers in a principal bundle. and around 18:10 gives an analogy that if we were to ""walk"" from one point $x$ to another point $y$ in the base manifold the connection tells us how to associate to a point in the fiber at $x$ a point in the fiber at $y$ . I don't see how the definition of a connection 1-form corresponds to this picture. Can anyone give an intuitive explanation on what motivates connection 1-forms and/or Ehresmann connections? But I am not sure why this is important or of any interest geometrically.","['principal-bundles', 'connections', 'fiber-bundles', 'gauge-theory', 'differential-geometry']"
4752189,Find the density of $\cos(X)$ when $X$ is an exponential.,"I want to find the density of $\cos(X)$ where $X$ is an exponential with density given by $$
f_{X}(x) = re^{-rx}\mathbb{1}_{[0,+\infty}(x),\quad r>0
$$ My attempt is the following :
First we notice, since the law of $X$ is a density measure (of density $f_{X}$ ) with respect to the Lebesgue measure and using a well known measure-theoretic result on the pushforward measure, that : $$
\mathbb{E}[\cos(X)] = \int_{\mathbb{R}}\cos(x)\,d\mathbb{P}_{X} = \int_{\mathbb{R}}\cos(x)f_{X}(x)\,d\lambda = \int_{0}^{+\infty}\cos(x)re^{-rx}\,dx
$$ Now the idea is to make the change of variable $y = \cos(x)$ however we are integrating with respect to $x$ . So for the moment we start to work on interval of the form $[k\pi, (k+1)\pi[$ for $k\in\mathbb{N}$ where the cosine function is monotonic in order to consider his inverse function $x = \arccos(y)$ . Now by the standard change of variable formula we end up with something like this if there is no mistake: $$
\int_{}^{}\left\lvert\frac{-1}{\sqrt{1-y^2}}\right\rvert re^{-r\cos^{-1}(y)}dy
$$ No if we consider the domain of integration, I think I should use the fact that $\cos(k\pi) = (-1)^{k}$ and $\cos(k\pi + \pi) = (-1)^{k+1}$ but this gives me for $k\in\mathbb{N}$ something between $-1$ and $1$ and I don't see how to handle this. I think this problem is a consequence of the fact that I have not been able to make a bridge between the integral over $[0, +\infty[$ and $[k\pi, (k+1)\pi]$ for $k\in\mathbb{N}$ . To this latter problem, my first idea was to use dominated convergence on the sequence defined by $$
f_n(x) = \sum_{k=0}^{n}\cos(x)re^{-rx}\mathbb{1}_{[k\pi, (k+1)\pi[}(x) 
$$ But the dominating hypothesis is not verified from what I have tried. Thus, I would like to have your help on this issue by providing some hints, for example.. or just telling me what is wrong in my attempt, please. Thank you a lot!","['integration', 'probability-distributions', 'solution-verification', 'probability-theory', 'density-function']"
4752246,"Let $f: A\rightarrow B, g: B\rightarrow C, and h: C\rightarrow D$. If $f$ and $g$ are both injective, then $g\circ f$ is injective.","Proof:
Assume that $f$ and $g$ are injective. Let $a_1,a_2\in A$ and $b_1=f(a_1), b_2=f(a_2)$ .
Suppose that $g(f(a_1))=g(f(a_2))\Rightarrow g(b_1)=g(b_2)\Rightarrow b_1=b_2$ ( $g$ is injective) $\Rightarrow f(a_1)=f(a_2)\Rightarrow a_1=a_2$ ( $f$ is injective). Therefore, $g\circ f$ is injective. $\blacksquare$","['functions', 'solution-verification']"
4752254,Prove that $a^2+b^2+c^2 \geq \frac{9abc}{a+b+c} + (\frac{a^2+b^2}{a+b}-c)^2$,"Let: $a,b,c\geq0$ . Prove that: $$a^2+b^2+c^2 \geq \frac{9abc}{a+b+c} + (\frac{a^2+b^2}{a+b}-c)^2$$ I tried to put $LHS-RHS$ but it is too complicated, also I tried Schur but still get nothing. Here is my solution (not in English): Solution 2 (from the picture): We have the following identity \begin{align*}
	&a^2 + b^2 + c^2 - \frac{9abc}{a + b + c} - \left(\frac{a^2 + b^2}{a + b} - c\right)^2\\[6pt]
	={}& \frac{(a - b)^2(ab + bc + ca)}{(a + b)^2} + \frac{a(b-c)^2 + b(c - a)^2 + c(a - b)^2}{a + b + c}.
\end{align*} The desired inequality follows.","['algebra-precalculus', 'inequality']"
4752300,Does derivative assigns diffrential?,"So there are 3 main definitions of derivation in 3 different contexts. Calculus of one variable real functions. Say we have an everywhere differentiable function $f: \mathbb{R} \to \mathbb{R}$ .
Then its derivative is function $f': \mathbb{R} \to \mathbb{R}$ . Its differential is linear map $df: \mathbb{R} \to \mathbb{R}$ . Functional analysis. Say we have a Banach spaces $E$ and $F$ , and a function $f: E \to F$ .
Then for each point $x \in X$ we can define a Frechet derivative $f'(x)$ which is a continuous linear map $E \to F$ .
So we have an assignment $x \mapsto B(E, F)$ . Manifolds. Say we have manifolds $M$ and $N$ and a morphism $f: M \to N$ .
Then for each point $x \in M$ we can assign a differential $df_x$ which is a linear map of tangent spaces $df_x: T_x M \to T_{f(x)} N$ . Is it true that all of these definitions coincide in the context of
calculus? To be more precise: So is it true that a derivative is in fact an assignment of
differential to each point? Where differential is morphism of tangent spaces? I use term assignment to emphasize that tangent spaces may be different for each point. It would be beautiful if there is a functor of some sort, but I can't figure it out. Here are my thoughts: There is a distinct notion of derivative and differential in the
context of calculus, but no differential of function of Banach
spaces, nor derivative of morphism of manifolds. Frechet derivative can be identified with usual derivative because $B(\mathbb{R}, F) \cong F$ (isometric isomorphism). Differential of morphisms coincide with usual differential because $T_x \mathbb{R} \cong \mathbb{R}$ . So $T_x E \cong E$ for Banach spaces? (Some kind of synthetic-algebraic definition needed here.) Is there a notion of differential forms on Banach spaces, e.g. when doing integrals? Is there some geometric structure on tangent spaces? I mean from algebraic view they all have same dimension $T_x M \cong T_y M \cong \mathbb{R}^m$ , so we may define a function $f': M \to L(\mathbb{R}^m, \mathbb{R}^n)$ ? (Here $\dim M = m, \dim N = n$ .) Bonus: what about TVS?","['frechet-derivative', 'calculus', 'functional-analysis', 'differential-forms', 'differential-geometry']"
4752339,What is the probability that two random subsets of a superset have no intersection?,"Let $$ S={1,2,...,n} $$ From the power set, $ P(S) $ , of $S$ , two subsets $A$ , $B$ are chosen at random. If each subset is equally likely to be chosen, then, the probability that the sets $ A $ and $ B $ have no elements in common is.... Here are some of my tries: Probability that two particular sets will be chosen is $ 2^{-2n} $ .  If from one set $\phi$ is chosen, no intersection, is guaranteed, if a Singelton is chosen from first set then if $ \phi $ is chosen from second set, then no intersection is
guaranteed if some set with cardinality = 1 is chosen from second set then no intersection guaranteed for all $n$ sets except for 1 which is the same as first set. if some set with cardinality = 2 is chosen from second set then for no intersection guaranteed for all $^{n}C _2 $ sets except for $n-1$ which contain the singleton in first set. if some set with cardinality = 3 is chosen from second set then for no intersection guaranteed for all $^ {n}C _2 $ sets except for???? I can't think further.  I can't figure out a general term for $n$ cardinality. So far, I have figured. $ P(A\bigcap B = \phi) =  2^{-2n} [^{n}C_0 * 1 + ^{n}C_1(^{n}C_0 * 1 + ^{n}C_1 -1 + ^{n}C_2 -(n-1) ..... ) + ^{n}C_2(....) .......^{n}C_n(....) ] $ I have tried finding complement of the question (finding probability that they have at least something in common) I have tried using de Morgans law, $ P(A\bigcap B = \phi) =  P((A^c \bigcup B^c)^c) = \phi) $ but I can't figure it out. I created a python program to find probability however computer cant calculate probabilities for $n >= 25$ and the result does not seem to have converged. At $ n = 25 $ , probability is around $0.001$ . I have looked at other similar questions however they don't deal with subsets of superset but  some simple subset of simple sets. The probability that two randomly selected subsets of the set $\{1 , 2, 3, 4 , 5\}$ have exactly two elements in their intersection , is","['measure-theory', 'probability']"
4752398,How is the infinite summation under limit that approaches zero converted to an improper integral?,"I am trying to understand the conversion of Fourier series to the Fourier integral. The book I am referring to is ""Advanced Engineering Mathematics"" by Erwin Kreyszig. The proof contains the following style of conversion: $
\lim_{\Delta \alpha\rightarrow 0} \sum_{n=1}^{\infty} F(\alpha_n)\Delta \alpha \rightarrow \int_{0}^{\infty}F(\alpha)d\alpha 
$ How is that summation under zero-approaching limit converted to an improper integral ? What's the essential piece I am missing here to understand it?","['limits', 'fourier-transform']"
4752408,Solutions of the differential equation of the form: $y=y'-\frac{x^{-1/2}}{n}$,"I was experimenting with the expansion of ""e"" and discovered this expansion: Let $y= \frac{x^{1/2}}{Γ(3/2)}+\frac{x^{3/2}}{Γ(5/2)}+\frac{x^{5/2}}{Γ(7/2)}+...$ This function upon differentiation gives: $y'=\frac{x^{-1/2}}{Γ(1/2)}+\frac{x^{1/2}}{Γ(3/2)}+\frac{x^{3/2}}{Γ(5/2)}+...$ Which is essentially y plus the first term ( $\frac{x^{-1/2}}{Γ(1/2)}$ ) from y' Therefore $y=y'-\frac{x^{-1/2}}{\sqrt{\pi}}$ However I cannot reduce this expression further and have seem to hit a roadblock. Is the solution for this equation even derivable?","['calculus', 'taylor-expansion', 'ordinary-differential-equations']"
4752412,Elements of $\mathbb{Z}_6 / \mathbb{Z}_2$,"Studying some concepts of group theory and after having read and trying to assimilate the concept of quotient group, I was wondering what structure would have the group $\mathbb{Z}_6 / \mathbb{Z}_2$ . My reasoning was the following: Using Lagrange's Theorem, we know that $|\mathbb{Z}_6 / \mathbb{Z}_2| = |\mathbb{Z}_6 : \mathbb{Z}_2| = \frac{|\mathbb{Z}_6|}{|\mathbb{Z}_2|} = \frac{6}{2} = 3$ . Therefore, there are three equivalence classes, but taking into account that we are working with $\mathbb{Z}_2$ , I think that all elements would be reduced modulo 2, and therefore I cannot see where the three equivalence classes appear. Am I using Lagrange's Theorem in an improper way? I would be grateful if someone could explain me.","['group-theory', 'quotient-group', 'finite-groups']"
4752434,Maximal non-decreasing subsequence,"Let $n\in \mathbb{N}$ . A sequence $(y_i)$ with $0\le i\le 2^n-1$ is called beautiful of rank $n$ if for all $0\le i\le 2^n-1$ , $0\le y_i\le i$ . For a beautiful sequence $(y_i)$ of rank $n$ let $A((y_i))$ be the size of the maximal nondecreasing subsequence of $(y_i)$ . Find $min  \ A(y_i)$ over all beautiful sequences of rank $n$ . It can be shown by a straightforward example that this number cannot exceed $n+1$ as is shown in the attached picture which is easily generalizable by induction. We take $y_{2^n-1}=0, y_{2^n-2}=1, \cdots, y_{2^{n-1}}=2^{n-1}-1$ , and redoing this.  I think there always should be a non-decreasing subsequence of that size but I couldn't prove it.","['combinatorics', 'extremal-combinatorics']"
4752466,Diameter of Euclidean Sphere using $||x||_p$,"Question: Given a real number $p$ , $p \in [0, \infty] $ , What is the diameter of an Euclidean Sphere with respect to $||x||_p$ ? Attempt: What quickly jumped out was the answer would be the max value of $||x||_p$ minus the minimum value of $||x||_p$ , with $x_1^2+ x_2^2...x_n^2=1$ , but I couldn't do it. I tried reducing this to the case of 2d, but failed. What is the solution?","['metric-spaces', 'optimization', 'geometry', 'real-analysis']"
4752496,Is there a name for this probabilistic paradox?,"Let $X\sim Exp(1)$ and $Y\sim Exp(\lambda)$ , independent. Then, \begin{align}
f_{X|Y=mX}(x) = \frac{f_{X,Y}(x,mx) }{\int f_{X,Y}(x,mx) \:dx }=\frac{f_X(x)f_Y(mx) }{\int f_X(x)f_Y(mx) \:dx } = \frac{e^{-(1+\lambda m)x}}{\int e^{-(1+\lambda m)x} dx} = (1+m\lambda)e^{-(1+\lambda m)x} 
\end{align} So $X|_{Y=mX} \sim Exp(1+m\lambda)$ . That means $E[X|Y=mX]=\frac{1}{1+ \lambda m} < 1$ whenever $m>0$ . This makes sense mathematically. $X|_{Y=0}\sim Exp(1)$ , $f_{X,Y}(0,0)$ is the same for all $m$ , but $f_{X,Y}(x,mx)<f_{X,Y}(x,m'x)$ if $m'<m$ . The practical implication seems weird, though. Your friend gets to your house via one Poisson bus and one Poisson train. You expect someone to wait 1 minute for a train. But they tell you they waited $m$ times as long for the bus than they did for the train, and now you gotta revise your expectation about the train down? Edit: I think the reason for the (seeming) violation of the Tower property is that I incorrectly defined $f_{X|Y=mX}$ . It should instead be $$
f_{X|Y=mX}(x) = \frac{xf_{X,Y}(x,mx) }{\int x f_{X,Y}(x,mx) \:dx} = \frac{xe^{-(1+\lambda m)x}}{\int xe^{-(1+\lambda m)x} dx}
$$ Think about it like the flag of Seychelles: the width of the ""ray"" is twice as large if you go twice as far out. (Aside: this is indeed Borel's paradox) This means that \begin{align}
E[X|Y=mX]= \frac{\int x^2e^{-(1+\lambda m)x} dx}{\int xe^{-(1+\lambda m)x} dx} = \frac{2}{1+\lambda m}
\end{align} The distribution of slopes is the distribution of $M:=Y/X$ \begin{align}
f_{M}(m) &= \int_0^\infty f_Y(y) f_{1/X}(m/y) y^{-1}\:dy\\
    &= \int_0^\infty \lambda e^{-\lambda y}\left(\frac{y}{m}\right)^2 e^{-y/m} y^{-1} \:dy\\
    &= \frac{\lambda}{m^2} \int_0^\infty y e^{-y(\lambda + \frac{1}{m}) } \:dy \\
    &= \frac{\lambda}{m^2}\frac{1}{(\frac{1}{m} + \lambda)^2}\\
    &= \frac{\lambda}{ (1+\lambda m)^2}
\end{align} The Law of Iterated Expectaions holds for this definition of the conditional density: \begin{align}
\int E[X|Y/X = m] \: dP(M\leq m) = \int_0^\infty \frac{2\lambda}{ (1+\lambda m)^3} \:dm = -\frac{1}{(1+\lambda m)^2}\bigg|_0^\infty = 1 = EX
\end{align}","['probability-distributions', 'paradoxes', 'probability-theory', 'probability', 'terminology']"
4752562,Vertical bundle and tangent bundle of the fiber,"For a fiber bundle $\pi: E\rightarrow B$ with typical fiber a manifold $F$ , we define the vertical subbundle $V$ by the exact sequence of verctor bundles over $E$ : $$0\rightarrow V\rightarrow TE \rightarrow \pi^*TB\rightarrow0.$$ In the case when $F$ has a Lie group structure and $E$ is a principal bundle, we know that $V$ is the trivial bundle with fiber $T_eF$ , the Lie algebra of $F$ . In general, can we say anything about $V$ and $TF$ ?","['fiber-bundles', 'vector-bundles', 'manifolds', 'differential-topology', 'differential-geometry']"
