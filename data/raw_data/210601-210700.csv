question_id,title,body,tags
4233689,Logic behind two consecutive heads/tails,"We play a game with a fair coin where we toss $N$ amount of times before we get a consecutive head or consecutive tails. What is the expected number of rolls ( $N$ )? My solution differed from that of the textbooks - which argued the expectation is $$ \sum_{2}^{\infty} \left(\frac{1}{2}\right)^{k-1},$$ however my logic was $$ \frac{1}{2}(2) + \frac{1}{2}(x+1) = x $$ where $\frac{1}{2}(2)$ is the the choice where you roll two consecutive numbers, and the $\frac{1}{2}(x+1)$ is the choice where you roll a non-consecutive, meaning you would have to roll again.
Both answers gave $3$ ; however I am concerned if my logic is incorrect for my solution or if I found an alternate solution.","['probability-theory', 'discrete-mathematics']"
4233703,Find the missing angles in the picture containing squares and circles,"In the following figure, $ABCD$ is a square. $\stackrel{\frown} {AB}$ and $\stackrel{\frown}{AC}$ are semicircles and quarter circles respectively. The blue circle touches $\stackrel{\frown} {AB}$ at $P$ , $\stackrel{\frown} {AC}$ at $Q$ and line $BC$ . $MN$ is a line such that $MN\parallel PQ$ and touches the blue circle at $R$ . If $\angle MPR=\alpha$ and $\angle NQR=\beta$ , find $\alpha+\beta$ . Here is my progress in solving the problem: I could just find the radius of the blue circle. Let $OG=r$ and $AB=2$ . So, $BE=1$ , $BO=2-r$ , $OE=1+r$ , $EH=1-r$ . Then, $BG^2=HO^2=4-4r$ and $4-4r+(1-r)^2=(1+r)^2$ . This gives $r=\frac 12$ . I can't proceed from here. Also while creating the figures, I found the answer: $\alpha=45^\circ$ and $\beta=30^\circ$ So, I need a solution to the problem. All solutions are welcome, but as I'm always interested in synthetic solutions I'll most likely accept an answer with synthetic solution.","['contest-math', 'euclidean-geometry', 'circles', 'geometry']"
4233707,Rigorously Proving Countable Additivity of Discrete Measure,"I'm trying to show, in full rigor, that a measure on a discrete probability space is necessarily countably additive. That is, with countable $\Omega$ and $p: \sum_{x\in \Omega} p(x) = 1$ and $p(x) \ge 0$ for all $x$ , we define: $P(A) = \sum_{x\in A} p(x)$ Now my hand-wavy solution is, with the $A_i$ disjoint: $P(\bigcup A_i) =\sum_{x\in \bigcup A_i} p(x) = \sum_i \sum_{x\in A_i} p(x) = \sum_i P(A_i) $ But it feels like the middle equality is doing some work that I can't necessarily justify. Specifically, if the collection of $i$ is countably infinite, and each $A_i$ is countably infinite.","['measure-theory', 'probability']"
4233762,Do polynomials have a limit ratio of 1 over fixed intervals?,"If $p$ is an increasing polynomial function, and $c>0$ is a constant, do we always have: $$ \underset{x \rightarrow \infty}{\lim}  \dfrac{p(xc+c)}{p(xc)} = 1?$$ I think if I am not missing anything, as the coefficient and degree of the leading terms will be equal in the numerator and denominator, by L'HÃ´spital's rule the limit should be equal to $1$ . I am wondering if there is an easier proof for this fact, or anything that I am missing.","['alternative-proof', 'limits', 'convergence-divergence', 'polynomials']"
4233777,Existence of Complex Frames on a Complex Vector Bundle,"$E \rightarrow M $ be a complex vector bundle (of real rank $2r$ ) with almost complex structure $J:E\rightarrow E \space\space\space(J^2 =-1)$ on it. $U\subset M$ be a trivial neighbourhood. Does there exist a complex frame for $E \rightarrow M$ over U? By a complex frame, I mean a set of $r$ (real) linearly independent $E_U$ -sections $\{X_1, ... , X_r\}$ , such that $\{X_1, ... , X_r, JX_1, ... , JX_r\}$ forms a complete $E_U$ -frame. (By ""linearly independent"", I mean they are linearly independent in the fibre above every point $x\in U$ ). It seems like such frames do exist as I recall seeing such frames in definition of complex connection and curvature matrices on complex vector bundles, which then lead to construction of Chern forms and classes. I need such a frame to prove that existence of a $J$ map for $E \rightarrow M $ implies a reduction of structure group of E to $GL(r,C)$ . I am stuck with this, even though I can prove the converse by explicitly constructing a complex frame using the reduction. Edit: I am using the following definition for 'complex vector bundle': A real even ranked vector bundle with a J-map. Sorry for the confusion.","['complex-geometry', 'vector-bundles', 'algebraic-geometry', 'almost-complex', 'differential-geometry']"
4233814,"Why is our definition of ""smooth"" function on a totally disconnected space the ""right"" definition?","My main interest in this comes from starting to learn about representations of totally disconnected, locally compact topological groups $G$ , and their representations. I came across the definition of ""smooth"" functions $\phi:G\to \mathbb{C}$ as being those which are locally constant, and smooth representations $G\to\text{End}(V)$ being those for which $\text{Stab}_G(v)$ is open for all $v\in V$ . These definitions lead me to two questions, which I suspect do not have technically precise answers, but for which I'm hoping there exists some good heuristic motivations. Why is the definition of ""smooth"" for functions $\phi:G\to\mathbb{C}$ the ""right"" definition?  That is, why is this considered to be the notion of functions on totally disconnected spaces that is analogous to the classic notion of smooth functions $f:\mathbb{R}\to\mathbb{R}$ ? in the same manner, why is the definition of smooth representations as above, the ""right"" analogue of smooth representations of Lie groups? These don't seem totally alien to me given the nature of the topology of totally disconnected spaces, but I'm hoping someone can spin a narrative more convincing than just saying the definitions involved are very ""pointy"". Also, I suspect that the answer to 2) will follow more or less directly from any reasonable answer to 1). I'm happy to simply be directed to a source that has already laid this out.","['p-adic-number-theory', 'smooth-functions', 'representation-theory', 'functional-analysis', 'langlands-program']"
4233833,"Why do the vertices of $f(x) = ax^2 + bx + c$, when fixing $a$ and $c$ but varying $b$, lie on $g(x) = -ax^2 + c$?","As a bit of background context, I'm teaching a calculus course this semester, and for a learning activity, the students had to examine various functions and how certain modifications affected them. In one case, they were examining the function $f(x) = 3x^2 + bx - 2$ , for various values of $b$ : what did those changes in $b$ do to the shape of the parabola, and why? I got curious, and noticed that as we varied $b$ , the parabola would move in a parabolic way with respect to the vertex. That is, if we increased $b$ when it was positive, it would move right and down; if $b$ was decreased and negative, it would move left and down. (The vertical changes were sharper for larger $b$ of course.) So naturally, I wanted to graph the various vertices of the function. I couldn't figure out a proper way to do this in Desmos, but I managed something feasible: Create a list $L$ of test values from $-100$ to $100$ with small intervals in-between. These $L$ will function as our $b$ values. The axis of symmetry of $f$ then is at $x = -b/6$ for each $b \in L$ . Of course, then, the vertex is at $(-b/6,f(-b/6))$ for each $b \in L$ . (You can play with a demo of it here if you wish.) Graphing a bunch of these points in black and $\color{red}{f(x)}$ in red gives this: Curiously, the set of vertices (for variable $b$ ) lie on a parabola that is precisely $f$ (when $b=0$ ) flipped vertically; that is, the vertices lie on $g(x) = -3x^2 - 2$ . (And in general, if we vary $b$ in $f(x) = ax^2 + bx + c$ , then the vertices lie on $g(x) = -ax^2 + c$ .) I've been wondering about this for a few days and a student even brought it up to me, and I can't think of a reason why this should be. Maybe it's incredibly simple, but can someone perhaps enlighten me? Thanks!","['algebra-precalculus', 'graphing-functions', 'quadratics']"
4233879,Disk glued to a Torus. Classifying the resulting spaces - How/Where to begin?,"My algebraic topology professor gave me an exercise and I feel very lost.
The exercise is the next: Describe the homotopy type of the different spaces that can be obtained from gluing a disk to a torus. Obs: The gluing is only at the disk's border. On a first try, I was thinking of gluing the border of the disc to a single side of the torus (>), then to two sides (maybe continuous sides (>,>>) or inverting the orientation of the first gluing(>,<)), etc. After a week, I gave up and asked my prof for hints. He said that could be useful to think in $\mathbb{R}^2/\mathbb{Z}^2$ and he also said that at some point I would be working on an algebra problem instead of a topology problem.
To do the classification of the spaces, he also mentioned that
Van Kampen would be necessary. Any ideas/suggestions about this?","['general-topology', 'algebraic-topology']"
4233904,Existence of a set $A\subset\mathbb{R}$ such that $|G\setminus A|=\infty$ for every open set $G\supset A$ [duplicate],"This question already has an answer here : There exists a set $A\subset\mathbb{R}$ with $|G-A|=\infty$ for each open set containing $A$. (1 answer) Closed 2 years ago . I am trying to prove the following statement and I would like to have an hint about how to complete it, thanks: ""Prove that there exists a set $A\subset\mathbb{R}$ such that $|G\setminus A|=\infty$ for every open set $G$ that contains $A$ ."" What I have tried: We know that if $A\subset\mathbb{R}$ is Lebesgue measurable iff for each $\varepsilon>0$ , there exists an open set $G\supset A$ such that $|G\setminus A|<\varepsilon$ so the set $A$ we are looking for must be non Lebesgue measurable. Now, the only non Lebesgue measurable set I know of is the Vitali set $V$ which is a subset of $[-1,1]$ so although it must have a positive and finite outer measure $|V|$ does not have the property required. I have thought about $\bigcup_{k=1}^{\infty}(r_k+V)$ (where the $r_k$ s are an enumeration of the rationals) but this has infinite outer measure. I have also thought about the set $\tilde V$ which contain one element from each of the sets $\{V+r:r\in\mathbb{Q}\}$ but one could find an open set $G$ containing it made up of $(-2,2)$ and then of intervals $I_k=(a_k-\frac{\varepsilon}{2^k},a_k+\frac{\varepsilon}{2^k})$ which has a total outer measure of $4$ so it doesn't work. Thus I am guessing this set needs to contain an uncountable number of elements (to prevent $G$ from having finite measure) and containing ever bigger terms. Is my intuition correct? I would appreciate any hint about how to find/build this set. Thanks $|A|$ denotes the outer measure of the set $A$","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4233942,Imaginary order differential equations,"I would like to find the solution of the imaginary order differential equation $y^{(2i)}+y^{(i)}+y=0$ I started with the Fourier transform differintegral as it seemed more suitable than the Riemann-Liouville fractional derivative,  as I saw on this post:
( Imaginary-Order Derivative ): $$f^{(s)}(x)=\dfrac{1}{2\pi}\displaystyle\int_{-\infty}^{+\infty} e^{- i \omega x}(-i \omega)^s \displaystyle\int_{-\infty}^{+\infty}f(t)e^{i\omega t}dt \, d\omega.$$ Then I applied it to the equation,  including $y(x)=y^{(0)}(x)$ : $$f^{(s)}(x)=\dfrac{1}{2\pi} \displaystyle\int_{-\infty}^{\infty} \displaystyle\int_{-\infty}^{\infty}e^{i\omega(t-x)}y(t)(-i\omega)^{s}dtd\omega,$$ and got to $$\dfrac{1}{2\pi} \displaystyle\int_{-\infty}^{\infty} \displaystyle\int_{-\infty}^{\infty}e^{i\omega(t-x)}y(t)(-i\omega)^{2i}dtd\omega + \dfrac{1}{2\pi} \displaystyle\int_{-\infty}^{\infty} \displaystyle\int_{-\infty}^{\infty}e^{i\omega(t-x)}y(t)(-i\omega)^{i}dtd\omega +\dfrac{1}{2\pi} \displaystyle\int_{-\infty}^{\infty} \displaystyle\int_{-\infty}^{\infty}e^{i\omega(t-x)}y(t)(-i\omega)^{0}dtd\omega$$ $$\dfrac{1}{2\pi} \displaystyle\int_{-\infty}^{\infty} \displaystyle\int_{-\infty}^{\infty}e^{i\omega(t-x)}y(t)((-i\omega)^{2i}+(-i\omega)^{i}+(-i\omega)^{0})dtd\omega=0.$$ taking the derivative of both sides doesn't help to get any solutions besides $y=0$ . Any help would be appreciated.","['complex-analysis', 'fractional-calculus']"
4233948,Proving symmetric difference is associative,"I'm trying to prove that for $A,B,C \subset X$ , we have $$
(A \Delta B) \Delta C = A \Delta (B \Delta C). 
$$ I tried to prove it by brute force by taking an argument in the left-hand side, but I couldn't do it in a way that every step is reversible. There has to be a ""clever"" way to do this that I'm just not thinking of, but I'm not able to gain any kind of graphical intuition by plotting venn diagrams to see exactly how to do that. EDIT: My attempt at brute force: \begin{align*}
x \in (A \Delta B) \Delta C & \iff x \in ((A \Delta B) - C) \cup (C - (A \Delta B)) \\
& \iff x \in (((A - B) \cup (B -  A)) - C) \cup (C - ((A - B) \cup (B - A)) 
\end{align*}","['elementary-set-theory', 'proof-explanation']"
4233965,Image of a polyhedral set under a linear map,"I say that a subset $P$ of $\mathbb{R}^n$ is a polyhedral set iff there exists some positive integer $m$ , a matrix $A\in Mat(m\times n,\mathbb{R})$ and a column vector $b\in\mathbb{R}^m$ such that $P=\{x\in\mathbb{R}^n\mid Ax\le b\}$ . When $x$ and $y$ are two vectors in $\mathbb{R}^n$ I write $x\le y$ to say that $x_i\le y_i$ for all $i=1,\dots, m$ . Now let $T\in Mat(m\times n,\mathbb{R})$ e let $f:\mathbb{R}^n\to \mathbb{R}^m$ be the linear transformation defined by $f(x)=Tx$ for all $x\in\mathbb{R}^n$ .
My problem is to prove that if P is a polyhedral set, then also the image of $P$ under f, i.e. $f(P)=\{Tx\mid x\in\mathbb{R}^n\text{ and }Ax\le b\}$ , is a polyhedral set . The idea is to show that there exists a $p\times m$ matrix $M$ , for some positive integer $p$ , and a column vector $\beta$ such that $f(P)=\{y\in\mathbb{R}^m\mid My\le\beta\}$ . But I don't how to show this latter thing. Can anyone help me, please?","['geometry', 'analysis', 'linear-algebra', 'linear-transformations', 'convex-analysis']"
4234007,Is there a set-based semantics for the untyped lambda calculus?,"Is there a set-based semantics for the untyped lambda calculus? As an example, here's a simple set-based semantics for the simply typed lambda calculus (henceforth STLC). It is extremely naive and amounts to little more than an observation that a function from $A$ to $B$ can be represented as a set of pairs. This treatment here sweeps some details under the rug regarding management of free variables in the interest of brevity. For every ground type $A, B, C \cdots$ , we have a sets $A, B, C \cdots$ . For each primitive constant and function, we send the symbol to its interpretation. For example if $f$ is a primitive function from $A$ to $B$ , the interpretation of $f$ will be a subset of $A \times B$ that is left-total. Let $\varphi$ be a well-formed formula with free variables $x_1, \cdots, x_n$ of type $A_1, \cdots, A_n$ . Let the interpretation of $\varphi$ be a function that sends $v_1, \cdots, v_n$ to the interpretation of $\varphi(v_1, \cdots, v_n)$ . Let the interpretation of $\lambda x : A \mathop. \varphi$ be a function that sends each $a$ in $A$ to the interpretation of $\varphi(a)$ . Let the intepretation of $a(b)$ be the unique value $u$ such that $(b^*, u) \in a^*$ where $a^*$ is the interpretation of $a$ and $b^*$ is the interpretation of $b$ . Attempting to do the same trick for the untyped lambda calculus doesn't work because ZFC is well-founded. For example, if $X$ is the interpretation of $(\lambda x : x(x))$ , then $X(X)$ would have to equal to $X$ .","['elementary-set-theory', 'lambda-calculus']"
4234016,Convolution of two $L^1$ functions,"Let $f,g\in L^1(\mathbb{R};\mathbb{R}).$ Define the convolution $f*g:\mathbb{R} \rightarrow \mathbb{R},$ by $(f*g)(x)=\int\limits_{y\in \mathbb{R}}f(x-y)g(y)dy.$ Note that Fubini's theorem and translation invariance of Lebesgue measure implies that \begin{eqnarray}
\int\limits_{R}\int\limits_{R} |f(x-y)g(y)|dydx = ||f||_{L^1(\mathbb{R})}||g||_{L^1(\mathbb{R})}.
\end{eqnarray} Now, again invoking Fubini's theorem we get, $(f*g)(x)=\int\limits_{y\in \mathbb{R}}f(x-y)g(y)dy < \infty$ for a.e. $x\in \mathbb{R}.$ Furthermore, $f*g \in L^1(\mathbb{R}).$ Question: Do we have $|(f*g)(x)|< \infty $ for all $x\in \mathbb{R}?$ If yes, how to prove it, if not, what are the  counter examples?","['convolution', 'measure-theory', 'functional-analysis', 'analysis']"
4234033,Every finite group with more than $2$ elements has a non-trivial automorphism.,"$\newcommand{\Aut}{\operatorname{Aut}}$ Theorem: Given a finite group $G$ of order $n\gt 2$ ,  it follows that $\left|\Aut G \right| \gt 1$ . Proof:
I tried it like this: Suppose on the contrary that $\left|\Aut G\right|\le1$ . It follows that $\left|\Aut G\right|=1$ as identity permutation $i:G\to G$ defined as $i(x)=x$ , always belongs to $\Aut G$ . I know the result that: $G/Z(G)\sim I(G)$ , where $I(G)$ is the set of all inner automorphisms of $G$ ; and that $I(G)\le \Aut G$ . It follows that $G=Z(G)\implies G$ is abelian. It can be shown here that $T:G\to G$ defined by $T(x)=x^{-1}$ is an automorphism. But as per the assumption, it should follow that $T(x)=x$ for all $x\in G$ , that is, $x=x^{-1}$ for all $x\in G\implies$ every non-identity element of $G$ is of order $2.\implies G$ is of even order. Above shows that if $G$ (finite and of order greater than $2$ ) is a non-abelian group or a group of odd order, then the theorem is proved by contradiction. So all it remains is to prove the result when $G$ is abelian and every non-identity element is of order $2$ . $G=\{1,a_1,a_2,\dots,a_{2k-1}\}$ , where $k\ge 2$ and $2k=n$ . I tried to create a non-trivial automorphism as follows: Let $T:G\to G$ be defined as $T(x)=\begin{cases} x  \text{ if $x\notin \{a_1,a_2\}$}\\a_2 \text{ if $x=a_1$}\\a_1 \text{ if $x=a_2$}\end{cases}$ $T$ is clearly a bijection. I tried to prove it homomorphism but got stuck and I feel that it's not a homomorphism (as it's not yet clear where $xy$ will be mapped by $T$ when $x$ and $y$ both are neither $a_1$ nor $a_2$ ). How can I create a not trivial automorphism of $G$ ?  Any suggestions are welcome. Thanks. I have seen this question being asked before ( here ) but the answers use arguments on finite field which I don't currently know much about.  Therefore, I tried to construct an explicit non-trivial automorphism of $G$ .","['abelian-groups', 'group-theory', 'finite-groups']"
4234074,Probability a ball will stay in urn forever,"Suppose that there is an urn with infinite capacity. In the first day, we put one ball in the urn and remove it. In the second day, we put 2 balls and remove one randomly. in the $k^{th}$ -day, we put $k$ balls and remove one randomly. What is the probability that, eventually, we will put a ball and never remove it. Well, first, it's not hard to see that before removing a ball, in the $k^{th}$ day there are: $$\frac{k(k+1)}{2}-(k-1)=\frac{k(k-1)}{2}+1$$ Now, let's focus on some ball from the $N^{th}$ day, and for every $n\geq N$ , let $A_n$ is the event a ball that was inserted in the $N^{th}$ day will be removed by, at most, the $n^{th}$ day. So $A_N\subseteq A_{N+1}\subseteq\cdots$ and the event that a ball from the $N^{th}$ will stay in the urn forever is: $$B_N=\bigcap_{n=N}^\infty A_n^C$$ also, we get that: $$P(A_{n+1}|A_n^C)=\frac{1}{\frac{n(n+1)}{2}+1}$$ Therefore, $\sum_{n=N}^\infty P(A_{n+1}|A_n^C)<\infty$ Those are just ideas, I don't really know how to proceed  from here...","['borel-cantelli-lemmas', 'probability-theory']"
4234080,Subgaussianity of the Mixture of Gaussian Random Variables,"Is a mixture distribution of gaussian subgaussian? We know that if $X$ follows a normal distribution $\mathcal{N} (0, \sigma^2)$ , then it is automatically $\sigma$ -subgaussian by definition. Suppose we have a mixture of normal random variables $X_1, X_2$ . Then does the mixture of these two also have subgaussianity? Suppose the mixture has distribution $f_x(x)$ , then: $$f_X(x) = \alpha f_1(x) + (1-\alpha)f_2(x),$$ where $\alpha$ is the probability assigned to a component. Subgaussianity: Suppose random variable $X$ follows the inequality below: $$\mathbf{\mathbb{E}[e^{\lambda X}]\leq \exp(\lambda^2 \sigma^2)},$$ then we say that $X$ is $\boldsymbol{\sigma}$ -subgaussian.","['statistics', 'gaussian', 'probability']"
4234094,Find fourth side of quadrilateral given three sides and two angles,"To completely determine a quadrilateral, you have to have five independent pieces of information, of sides and angles. If you have five data (all outer sides and a diagonal), finding the angles is easy with the cosine rule. Similarly with four sides and an angle, the cosine rule is enough to solve for the other angles. With three sides and two angles, simple application of the cosine rule is enough to find the other sides and angles. Except in one case. If the two angles are not in between any of the three sides, then there is no immediate way to use the cosine rule. How would the final side be solved for in this case? Example (apologies for bad quality):","['quadrilateral', 'trigonometry']"
4234103,A problem about polar coordinate of $\mathbb{R}^2-{0}$ in Bott-Tu's book.,"In the Chapter 6 of Bott-Tu's Differential Forms in Algebraic Geometry (P71, P72), they use the polar coordinates of $\mathbb{R}^2-{0}$ to define the global Angular form: Now suppose the rank of $E$ is $2$ , and ${U_{\alpha}}$ is a coordinate open cover of
M  that trivializes $E$ . Since $E$ has a Riemannian structure, over each ${U_{\alpha}}$ we can  choose an orthonormal frame. This defines on $E^O|_{U_{\alpha}}$ polar
coordinates $r_{\alpha}$ and $\theta _{\alpha}$ ; if $\pi^*x_1,...\pi^*x_n$ are coordinates on ${U_{\alpha}}$ , then $\pi^*x_1,...\pi^*x_n,r_{\alpha},\theta _{\alpha}$ are coordinates  on $E^O|_{U_{\alpha}}$ . Here $E^O|_{U_{\alpha}}$ is the complement of the zero section in $E$ , which is a vector bundle. Since the angel function $\theta_{\alpha}$ is not continuous on $\mathbb{R}^2-0$ , I wonder why $\pi^*x_1,...\pi^*x_n,r_{\alpha},\theta _{\alpha}$ are coordinates  on $E^O|_{U_{\alpha}}$ . Do they mean that for $E^O|_{U_{\alpha}}$ , we discompose it by four charts: ${U_{\alpha}}\times\mathbb{R}\times\mathbb{R}^+,{U_{\alpha}}\times\mathbb{R}^-\times\mathbb{R},{U_{\alpha}}\times\mathbb{R}\times\mathbb{R}^-,{U_{\alpha}}\times\mathbb{R}^+\times\mathbb{R}$ , , and on these charts, $\theta_{\alpha}$ is smooth? If we do this, I wonder if the Euler class could be defined by Bott-Tu's way, I am not sure if I am right. Maybe this problem is too strange, It has a simplified versionï¼
How to understand the angel function $\theta$ in $\mathbb{R}^2-{0}$ , and 1-form $d\theta$ , since $\theta$ is not continuous in $\mathbb{R}^2-{0}$ .","['algebraic-topology', 'differential-geometry']"
4234160,"Let $G$ be a finite group, $H$, $K$ be normal subgroups and $P$ a sylow p-subgroup . Then $PH \cap PK=P\left( H\cap K\right)$.","I was studying the group theory book by John S. Rose and this is the problem 257, which is driving me nuts. It looks so easy, but i can't solve it. Let $G$ be a finite group, $H$ , $K$ be normal subgroups and $P$ a sylow p-subgroup . Then $PH \cap PK=P\left( H\cap K\right)$ . Of course, one incluision is trivial: if $x\in P\left( H\cap K\right)$ there exists $p\in P$ and $y\in H\cap K$ such that $x=py$ , but $py\in PH$ and $py\in PK$ , so $x\in PH \cap PK$ . But i don't know how to proceed with the other inclusion. This is my work so far: Since we are working with a sylow subgroup, the only information we have is related with the order of the group, so maybe we can prove that both sides have the same cardinality and we will be done. Let $\lvert G \rvert= p^nr$ with $p\nmid r$ , $\lvert H \rvert= p^hs$ with $p\nmid s$ and $\lvert K \rvert= p^kl$ with $p\nmid l$ . By definition of $P$ we have that $\lvert P \rvert= p^n$ and since $H$ and $K$ are normal subgroups, $H\cap P$ and $K\cap P$ are sylow p-subgroups of $H$ and $K$ respectively, so $\lvert H\cap P \rvert= p^h$ and $\lvert K\cap P \rvert= p^k$ . Now, I don't know what to do, I tried to work with the formula for the order of the product of subgroups and with the index, but I got no results. I'll appreciate your help, many thanks.","['group-theory', 'normal-subgroups', 'sylow-theory']"
4234175,"Find the total number of ordered pairs $(m,n)$ such that $m^2n=20^{20}$ where $m,n$ are positive integers.","Find the total number of ordered pairs $(m,n)$ such that $m^2n=20^{20}$ where $m,n$ are positive integers. My Approach: Simplifying $$m^2n=20^{20}=(2^2 \times 5)^{20}=2^{40} \times 5^{20}$$ Hence one way is $n=5^{20}, m=2^{20}$ or vice-versa or $m=2^{10},n=10^{20}$ and $m=5^{10},n=4^{20}$ . These are the only ways I could think of initially but there are several combinations possible for values of $m,n$ . If anyone can help spot a pattern or what are the possible values of $m,n$ , I 'll try again to proceed further. I believe that there are many integers possible but I am not able to apply the correct logic. Thank You.",['combinatorics']
4234180,How do i see how all the generators get mapped during automorphism in GAP,"I'm using GAP for the first time.
I used AllAutomorphism command to see all automorphisms of SymmetricGroup of order $3$ . gap> AllAutomorphisms(SymmetricGroup(3));

     [ ^(), ^(2,3), ^(1,3), ^(1,3,2), ^(1,2,3), ^(1,2) ] I want to understand what the output means?
I would also like to see how generators(or group elements) map during each automorphism map.","['gap', 'group-theory']"
4234228,"$\mathbb C^2_{0} = \{(a,b)\in X: a\ne 0, b\ne 0\}$ is an open path-connected subset of $\mathbb C^2$","If $X$ is an open path-connected subset of $\mathbb C^2$ , prove that $$\mathbb C^2_{0} = \{(a,b)\in X: a\ne 0, b\ne 0\}$$ is also an open path-connected subset of $\mathbb C^2$ . $\mathbb C^2_{0}$ is open . $\mathbb C^2_{0}$ is open in the subspace topology on $X$ , from which the claim follows. $\mathbb C^2_{0}$ is path-connected . Take $x,y\in \mathbb C^2_{0}$ (of course, $x$ and $y$ are tuples, and I am lazy). I need to find a continuous function $f: [0,1]\to \mathbb C^2_{0}$ such that $f(0) = x$ and $f(1) = y$ . Since $X$ is path-connected, there exists $g: [0,1]\to  X$ with $g(0) = x$ and $g(1) = y$ . My sense is that we must modify $g$ in some way (restrict to $\mathbb C^2_{0}$ , or something) to construct $f$ , but I am stuck here. Possible Generalization: I wonder if the following generalization is true, for $n\in \mathbb N$ : If $X$ is an open path-connected subset of $\mathbb C^n$ , is $$\mathbb C^n_{0} = \{(a_1,a_2,\ldots,a_n)\in X: a_i \ne 0, 1\le i\le n\}$$ also an open path-connected subset of $\mathbb C^n$ ? I was able to prove path-connectedness for a very special case, i.e. when $n=1$ and $X\subset \mathbb C$ is convex. I doubt if a similar idea would work for the more general cases ( $X$ may not be convex, $n=2$ or greater, etc.) Take $n=1$ , and let $X$ be a convex subset of $\mathbb C$ . If $0\notin X$ , then $C_0^1 = X$ , so there is nothing to do. Assume $0\in X$ . Then, $C_0^1 = X\setminus \{0\}$ . Consider $x,y\in C_0^1$ , and the straight line path in $X$ given by $f:[0,1]\to X$ such that $f(t) = (1-t)x + ty$ for $t\in [0,1]$ . If $f(t)\ne 0$ for all $t\in [0,1]$ , then $f:[0,1]\to C_0^1$ is the required path in $C_0^1$ . If $f(t) = 0$ for some $t\in [0,1]$ (there is at most one such $t$ ), then consider $z\notin f([0,1])$ . Define $g_1:[0,1]\to C_0^1$ by $g_1(t) = (1-t)x + tz$ and $g_2:[0,1]\to C_0^1$ as $g_2(t) = (1-t)z + ty$ . Concatenate $g_1$ and $g_2$ to get $g:[0,1]\to C_0^1$ . $$g(t) = \begin{cases}g_1(2t) & t\in [0,1/2]\\ g_2(2t-1) & t\in [1/2,1] \end{cases}$$ $g$ is the required path between $x$ and $y$ in $C_0^1$ . Thanks!","['complex-analysis', 'general-topology', 'path-connected']"
4234274,The projection of an unit cube by uniformly random orthogonal pair of unit vectors becomes close to a disc as dimension becomes large.,"Let $U, V \in \mathbb{R}^n$ are uniformly random orthogonal unit vectors, i.e. $||U||_2=||V||_2=1$ w.p. $1$ , $U^{T}V=0$ w.p. $1$ and $(OU,OV)$ is distributed same as $(U,V)$ for all orthogonal $n \times n$ matrices $O$ . Consider the subset of the plane $$ \mathcal{P}_n := \left\{ (z^{T}U, z^{T}V) : z \in [-1,1]^n \right\}.$$ I am asked to show that for large $n$ , $\mathcal{P}_n$ is close to a disc in the sense that $R_n/r_n$ converges in probability to $1$ , where $$ R_n := \sup \left\{||z||_2 : z \in \mathcal{P}_n \right\}, \; r_n := \inf \left\{||z||_2 : z \in \mathbb{R}^2 \setminus \mathcal{P}_n \right\}.$$ It is clear that $R_n \geq r_n$ . But I am not sure how to approach the other side. I hope convexity of the set $\mathcal{P}_n$ will be crucial and may be the representation of $(U,V)$ by Gaussian vectors might be helpful. Any suggestion is welcome.","['geometric-probability', 'probability-limit-theorems', 'probability-theory', 'probability']"
4234276,Linear algebra book for early phd student,"I understand this is a frequently asked question, but I'm posing it again, since I am not sure about the extent to which other people who have asked this wanted the same thing as I do. I'm an early phd student in an engineering discipline. I am looking for a book on LA aimed at advanced ugrads/early grads to help me cover my gaps. I've taken an introductory course in LA whose exposition focused more on linear systems etc, as well as an abstract algebra course focusing on group theory. Now I want an LA book focusing more on the vector space aspects of it instead of matrix theory. The particular topics of focus are linear spaces and transformations , eigenvalues and eigendecompositions , inner product spaces, matrix norms and quadratic forms , as well as the geometric interpretation of these notions. Despite not being a math student, I'm not looking for an applied book (like Strang). My area of research requires an excellent command of the aforementioned topics, so I want a theoretically sound, proof-based exposition that doesn't delve too much into functional analysis/operator theory. Also, challenging problems are definitely a plus.","['linear-algebra', 'reference-request']"
4234277,Is there a function $f$ so that $f(x-\exp(y))$ is linear in $y$,"I wonder if there is any continuous function $f$ so that $f(x-\exp(y))$ is linear in $y$ . I exclude cases where $f$ is a constant or depends on $x$ or $y$ . For instance, I do not allow $f(c)=\ln(x-c)$ .","['functions', 'transformation']"
4234283,Applications of matrix differentiation,"I know that ordinary differentiation has many real world applications, from quantum physics to economics, but I cannot think of any real world applications of matrix differentiation. So, do any real world application exist?","['applications', 'matrices', 'calculus', 'matrix-calculus', 'derivatives']"
4234285,Is this quotient topology compact?,"Let $X$ be punctured Euclidean 3D space (or what's the term for this?) $$
X = \mathbb{R}^3 \setminus \{(0, 0, 0)\}
$$ with the Euclidean topology and let  â¼ be the relation defined as follows: $$
(x_1,y_1,z_1) â¼ (x_2,y_2,z_2) \iff \exists \: \mathrm{n} â \mathbb{Z} \: \mathrm{such} \: \mathrm{that} \:(x_2, y_2, z_2) = (2^nx_1, 3^ny_1, 6^{ân}z_1).
$$ Let Y be the set Y = X/~. Is Y compact? I cannot figure out a way to proceed and I am not even sure of the answer.
Before this, I have already proven that Y is connected, not Hausdorff and that the projection map is open and I donât know if any of this might help in the proof.
I know that X is not compact but that doesnât mean anything I think.
The only way I could think is by using the definition of compact space (each of its open covers has a finite subcover) but I donât have a clear image of the topology on Y.
Maybe a homeomorphism to a particular space?
Maybe finding finite subcovers of of each open and saturared cover of X?","['quotient-spaces', 'general-topology', 'compactness']"
4234296,derivations of the ring of germs of $C^{\infty}$ functions,"Let $\mathcal{O}_{\mathbb{R},0}$ be the ring of germs of $C^{\infty}$ funcitons on the real line. A derivation of $\mathcal{O}_{\mathbb{R},0}$ is a $\mathbb{R}$ -linear map $\partial:\mathcal{O}_{\mathbb{R},0}\to\mathcal{O}_{\mathbb{R},0}$ that satisfies the Leibniz rule $\partial fg=f\partial g+g\partial f$ for all $f,g\in\mathcal{O}_{\mathbb{R},0}.$ The set $Der_{\mathbb{R}}(\mathcal{O}_{\mathbb{R},0},\mathcal{O}_{\mathbb{R},0})$ of all derivations is a (left) $\mathcal{O}_{\mathbb{R},0}$ -module in the obvious way. Question: Let $x$ be a coordinate on the real line and let $\frac{d}{dx}$ be a derivation $f\mapsto\frac{df}{dx}.$ Is it true that $Der_{\mathbb{R}}(\mathcal{O}_{\mathbb{R},0},\mathcal{O}_{\mathbb{R},0})=\mathcal{O}_{\mathbb{R},0}\frac{d}{dx}$ as an $\mathcal{O}_{\mathbb{R},0}$ -module? Motivation: If $\mathcal{O_{\mathbb{C},0}}$ is the ring of germs of complex analytic functions and $z$ is a complex coordinate then $Der_{\mathbb{C}}(\mathcal{O}_{\mathbb{C},0},\mathcal{O}_{\mathbb{C},0})=\mathcal{O}_{\mathbb{C},0}\frac{d}{dz}.$ Unfortunately the proof of this fact is based on the observation that if $\mathfrak{m}\subset\mathcal{O}_{\mathbb{C},0}$ is the unique maximal ideal then $\bigcap_{n\geq0}\mathfrak{m}^{n}=(0).$ This is not true in case of $\mathcal{O}_{\mathbb{R},0}$ . EDIT: Following the comments I changed the notation from $\mathcal{O}$ to $\mathcal{O}_{\mathbb{R},0}.$ By ""the ring of germs of $C^{\infty}$ funcitons on the real line"" I meant the ring of germs at $x=0$ of $C^{\infty}$ functions defined on open neighbourhoods of the origin of the real line.","['local-rings', 'differential-algebra', 'differential-geometry']"
4234315,How to interpret $\cos(n\pi)^2$,"I am stuck in an elementary problem which somehow makes me confused. If it is written $$\cos(n\pi)^2$$ What does this mean? Is it $(\cos{(n\pi)}) \cdot (\cos{(n\pi)})$ , or $\cos{(n^2\pi^2)}$ ??? Because if it is $(\cos{(n\pi)}) \cdot (\cos{(n\pi)})$ , I usually write it as $\cos^2{(n\pi)}$ . How to write it internationally? I am confused, please help. And how to internationally write the other one? Thank you in advance",['trigonometry']
4234366,What is the value of segment GH in the square below?,"For reference: If ABCD and EFGH are squares, HI = 2 and GN= $\sqrt5$ , calculate EH. My progress: If point I were given as the midpoint of $AD$ the exercise would be quite easy $AE=4=AI, EH = \sqrt{(4^2+2^2)} = \sqrt20 = 2\sqrt5=GH$ so I think the idea is to demonstrate that it is the midpoint. I drew some auxiliary lines...",['geometry']
4234383,Does $\sum_{n=4}^\infty \left(\frac{1}{\log(\log(n))}\right)^{\log(n)}$ converge?,"I have this series that I can't understand how to find it's character. $$\tag{1} \sum_{n=4}^\infty \left(\frac{1}{\log(\log(n))}\right)^{\log(n)} $$ To exercise, I solved a similiar series to get the ground how to solve the previous one. $$\tag{2} \sum_{n=2}^\infty \left(\frac{1}{\log(n)}\right)^{\log(n)} $$ This is my solution: $ a_n $ is a series made by positive terms, so it can't be unsolvable (diverges positively or converges). $$ \log(n)^{\log(n)} = (e^{\log(n)})^{\log(\log(n))} = n^{\log(\log(n))} \ge n^{\log(3)} $$ $ \forall n >e^3$ , $\log(3)>3$ , $\log(\log(n))>\log(3)>1$ . So we have that $$ \frac{1}{\log(n)^{\log(n)}} \le \frac{1}{n^{\log(3)}} $$ $\forall n> e^3 $ . And by the critery of asymptotic comparison by the fact that $$\tag{3} \sum \frac{1}{n^{\log(3)}}$$ is the generalized harmonic series with $p=\log(3)>1 $ so it converge. $(3)$ converges so then $(2)$ converges. I don't know how to use this to solve $(1)$ , I can't find any inequality to get it right.","['convergence-divergence', 'sequences-and-series', 'summation', 'real-analysis']"
4234397,Derivatives of Complex Functions,"For single variable function, it is considered to be differentiable at a point when left derivative equal to right derivative. But in the case of complex function we need to have derivative that approach our point of interest in all direction to be equal in order to be differentiable. My question here is that a complex function have two real variables as multi variable function but why multi variable function donât have that âapproaching in all directionâ problem when we define its derivative?","['complex-analysis', 'derivatives']"
4234452,Prove that there is no function $f$ such that $\int_{0}^{\pi} |f(x)-\sin x|^2dx \le \frac34$ and $\int_{0}^{\pi} |f(x)-\cos x|^2 dx \le \frac34$,"Prove that there does not exist a continuous function $f:[0,\pi] \to \mathbb{R}$ such that $$\int_{0}^{\pi} |f(x)-\sin x|^2 \mathrm{d}x \le \frac34  \\ \text{and} \\ \int_{0}^{\pi} |f(x)-\cos x|^2 \mathrm{d}x \le \frac34$$ I thought of some inequalities like cauchy-schwarz and AM-GM , but it didn't help. Also, since there is no condition on $f(x)$ , it could be positive or negative, so that makes it a bit challenging. Thank you for helping!","['integration', 'definite-integrals', 'calculus', 'cauchy-schwarz-inequality', 'inequality']"
4234475,"Definition and properties of $W^{-1,k}_{loc}(\Omega)$ space","I have a basic understanding of the Sobolev spaces. Let $\Omega \subset \mathbb{R}^n$ and $k=1,2,3... .$ I understand that $W^{-1,k}(\Omega)$ is defined as the dual of $W_0^{1,k'}(\Omega)$ where $k'$ is the conjugate exponent of $k.$ In other words, $W^{-1,k}(\Omega)$ is a normed linear space which consists of all the functionals $f :W_0^{1,k'}(\Omega) \rightarrow \mathbb{R}$ equipped with operator norm defined by $||f||_{W^{-1,k}(\Omega)}= \sup\limits_{||\phi||_{W^{1,k'}(\Omega)}=1}|f(\phi)|.$ I recently came across the space $W^{-1,k}_{loc}(\Omega).$ I would like to understand the followings: 1. Is there a norm (or atleast a metric) on $W^{1,k}_{loc}(\Omega)$ ? 2. What is the definition of $W^{-1,k}_{loc}(\Omega)$ ? How to interpret these spaces? (In view of (1) is it the dual of $W^{1,k}_{loc}(\Omega)$ under the norm/metric ) 3. What do we mean by compact sets and bounded sets in $W^{-1,k}_{loc}(\Omega)$ ?","['analysis', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations', 'compactness']"
4234535,"Do there exist integers $a_1,a_2,\ldots,a_{n-1},a_n$ such that $a_i+a_{i+1}+a_{i+3}=1$?","I am curious about the existence of a sequence of integers $a_1,a_2,\ldots,a_{n-1},a_n$ for some $n\ge1$ with the property that for all positive integers $i$ , $$a_i+a_{i+1}+a_{i+3}=1$$ where we compute indices modulo $n$ as necessary. My interest lies in the more general case of any finite weighted sum of indices relative to $i$ ; the case above is simply the first which seems nontrivial. (The motivation for this comes in thinking about ""tiling"" $\mathbb{Z}$ with finite weighted tiles, where we want to lay down an integer number of copies in each position: the existence of such a series of $a_i$ amounts to the existence of a periodic tiling.) It is easy to see that $n$ must be a multiple of three, by adding all congruences together; I can also rule out individual cases of $n=3$ or $n=6$ , but I don't see how to get all $n$ in general. It would suffice to show that the vectors $(1,1,0,1,0,\ldots,0)$ and its cyclic rotations are linearly independent over $\mathbb{R}$ , but I'm not sure how to do this either. As a remark, this is not true if we replace $(i,i+1,i+3)$ with $(i,i+1,i+5)$ , so the argument must hinge on the specific offsets somehow.","['elementary-number-theory', 'linear-algebra']"
4234536,How was the closed form of this alternating sum of squares calculated?,"I am reading through this answer at socratic.org . The question is to find the closed form of the sum $$1^{2}-2^{2}+3^{2}-4^{2}+5^{2}-6^{2}+\ldots.$$ I understand that, if the terms were added, the sum would be $$
\sum_{n=1}^{N} n^{2}=1^{2}+2^{2}+\ldots+N^{2}.
$$ The person goes on to say, if the series were not alternating, the sum would be $$
S=\frac{N(N+1)}{2}
$$ But is that correct? I thought the sum of the first $N$ squares would be $$
\frac{N(N+1)(2N+1)}{6}.
$$ Lastly, I understand moving the $-1$ constant out of the summation as such $$
=-\sum_{n=1}^{N}(-1)^{n} n^{2}
$$ But I am completely missing how the final closed form was calculated $$
S_{N}=-\frac{(-1)^{N} N(N+1)}{2}
$$","['summation', 'proof-explanation', 'square-numbers', 'closed-form', 'sequences-and-series']"
4234559,Sawtooth-like polygonal chains in $\mathbb R^2$ must self-intersect.,"We consider closed polygonal chains in the 2-dimensional plane with an even number of sides, say $2n$ , numbered as $A_1B_1A_2B_2\dots A_nB_nE$ , where $E = A_1$ . We require additionally, that each $B_i$ lies right below $A_i$ , like with sawtooth. Edit: Since right below seems ambiguous, let's state this line explicitly as that the $y$ -coordinate of each $B_i$ be strictly less than that of $A_i$ . Is it true that any such polygonal chain must self-intersect? In the 3-dimensional case, there is the obvious counterexample of traversing the vertices of any prism in the obvious order. I have however yet to find a proof or counterexample in the 2-dimensional case.","['combinatorial-geometry', 'combinatorics', 'geometry']"
4234638,How to determine a possible solution when an equation has multiple unknowns,"Suppose we have an equation $$15 = 5i + 3j + 2k$$ where $i, j, k$ are non-negative integers. It is easy to find some values that make this equation true: $$i=3, j=0, k=0$$ $$i=0, j=5, k=0$$ $$i=1, j=2, k=2$$ $$...$$ But if we change the equation to be something quite a bit more complex, it becomes much harder to determine if a solution exists. For example, consider the equation $$2038 = 17i + 8j + 7k$$ again where $i, j, k$ are non-negative integers. Is there a way to determine if a solution exists other than by trial and error?","['linear-algebra', 'linear-diophantine-equations']"
4234640,Finding a minimal set of equations that determine a variable.,"I have a system of $m$ linear equations on $n$ variables, which I'm representing as $Ax=b$ , with $A$ an $m\times n$ matrix representing the equations and $b$ an $\mathbb R^m$ vector representing the constants of the equations. I'm given that there exists a solution $s\in\mathbb R^n$ (i.e., $As=b$ ) and that $m<<n$ . My goal is the following: Find which variables are determined (i.e., for which there are no solutions with values different than the one in $s$ ). For each variable in 1., find a minimal subsystem of equations which determines that variable. By minimal, I mean the following: the subsystem determines the variable as in 1., and no proper subset of the subsystem does this. I want to find any such minimal subsystem: which one I pick is irrelevant to me. Progress : I've solved 1. by computing the nullspace of $A$ , and looking for those variables for which none of the basis vectors of the null-space have a non-zero coefficient. I've attempted to solve 2. by computing the pseudo-inverse of $A$ and looking at which coefficients in the row corresponding to the chosen variable are non-zero. This approach doesn't work, though, since the pseudo-inverse $P$ minimizes the Frobenius norm among all matrices $X$ such that $X\cdot b=s$ . This norm is an $L_2$ norm, but what we really want is an $L_0$ norm, since we want to minimize the number of non-zero elements in the matrix. Of course, finding the matrix minimizing the $L_0$ norm is an NP problem, so that's not a valid approach for me. Are there any other approaches I'm missing? Perhaps some heuristic methods? I'm struggling to find anything on this subject.","['pseudoinverse', 'systems-of-equations', 'linear-algebra']"
4234651,Hard vs Soft accuracy: Bounds in specific probability problem,"Let $\mathbf{X}=(X_1,\cdots,X_m)$ a random vector which takes values in $(0,1)^m$ such that $\sum_{i=1}^mX_i=1$ (a.s.) and let $Y$ a random variable which takes values in $\{1,\cdots,m\}$ . Let $s=\mathbb{E}[X_Y]$ and $h=\mathbb{P}\left(Y\in\mathop{\arg\max}_i X_i\right)$ , I need bounds that relate these magnitudes in the general case. MOTIVATION: In Machine Learning context, $s$ and $h$ are the soft  and hard accuracy. Usually we take the hard one, so we assume that $h>s$ . I want to prove it. The tightest bound that I found is: $$s=\int_0^1\mathbf{P}(X_Y>t)dt=\int_0^\frac{1}{2}\mathbf{P}(X_Y>t)dt+\int_\frac{1}{2}^1\mathbf{P}(X_Y>t)dt\leq\frac{1}{2}+\frac{1}{2}\mathbf{P}(X_Y>1/2)\leq \frac{1+h}{2}$$ where I use that if some value is greater than $1/2$ , it is the maximum. Q1) Is there another upper bound of $s$ tighter than this? (suppose $h$ and $s$ are values close to $1$ ). Q2) I would like $s\leq h$ . Can you think of a counterexample? Q3) Can you think extra assumptions to ensure $s\leq h$ ? Thanks","['machine-learning', 'probability-theory', 'probability', 'upper-lower-bounds']"
4234658,Proof of: There exists a function which dominates all computable ones,"Let $C \subset \mathbb{N} \times \mathbb{N} $ be the set of all (turing-)computable functions. Then: $$\exists f \in \mathbb{N}^\mathbb{N}: C \subset o(f)$$ I am looking for a proof and the name of a potential statement implying this statement. Personally, I feel this is very counter-intuitive... This statement must be true. If you say that a problem $X \in P$ , you usually say that $$ \exists M \in  
\mathcal P. \forall x \in X: \text{M accepts x with step counting function f} \in \mathcal{O(px^n)}$$ Now you can proof that such a problem is decidable by defining a Turing machines which halts after it has done at max $f(n)$ steps. You can now show that this holds true for any function with a computable step counting function.(the complexity class might change, of course) This is however does not hold true for an uncomputable step counting function $g$ .
You could try to contradict, saying that for g there is a bigger $f$ , which is computable, such that $g \in \mathcal{O(f(n))}$ . But this is known to be false.","['functions', 'turing-machines', 'computer-science']"
4234692,A pathological example of a distribution.,"Everyone who studied distributions at some point have seen how locally all distribution are basically formal derivatives of continuous functions. The question is: does there exists a bounded function $f\in L^\infty(\mathbb R^d,\mathbb R^d)$ such that its distributional derivative is not a measure? The easiest formulation of the question would be to find such a function in $L^\infty (\mathbb R)$ . Since the topology on the distribuitions is the pointwise topology when looking at bounded functions we can just look at characteristic functions. Given a measurable set $A$ we know that $\chi_A$ can be approximated by above with open sets $O_n$ and from below with compact sets $K_n$ , thus creating two converging sequences $$
T_{\chi_{O_n}}\to T_{\chi_{A}}\leftarrow T_{\chi_{K_n}}.
$$ We get $$
\partial T_{\chi_A}(\varphi)=- T_{\chi_A}(\varphi')=\lim -T_{\chi_{O_n}}(\varphi')=\sum_k (\varphi(a^n_k)-\varphi(b^n_k)).
$$ The inequality $|\partial T_{\chi_A}(\varphi)|\leq\|\varphi'\|_\infty\mathcal{L}^1(A)$ is trivial but if the number of intervals in $O_n$ is a finite number $M_n\in\mathbb N$ for any $n\in\mathbb N$ then we get $$
|\partial T_{\chi_A}(\varphi)|\leq 2 M_n \|\varphi\|_\infty+\varepsilon,
$$ thus pointing out that if $M_n\leq M$ uniformly then we would get a measurable set with a measure as a derivative. Is such a condition necessary? Of course not, because Cantor's functions is an example of a bounded function whose derivative is a measure supported on an uncountable set. Does this mean that considering the function $$
f=\sum_n(-1)^{n+1}\frac{n}{2}\chi_{[-\frac{1}{n},\frac{1}{n}]}
$$ is enough? It is bounded because the alternated harmonic series converges and its derivative is $$
\int_{\mathbb R}\varphi' f\mathcal L=\sum_n(-1)^{n+1}\frac{\varphi(1/n)-\varphi(-1/n)}{2/n}.
$$ Are there more trivial examples? Is there a mistake and this function in reality has a measure as derivative? Is there an example of a continuous function that doesn't have a measure as a derivative? What are some multidimensional examples?","['derivatives', 'distribution-theory']"
4234700,What is the shortest proof that there is an irrational number?,"This is going to be hard to make precise, because it depends on details of the proof theory, but here I'm considering a foundation of ZFC just after constructing the real numbers as a complete ordered field, and the rational numbers as a subset of the reals. From such a basis, what is the shortest proof of that there exists an irrational number? Here are some other statements from which the result follows easily: The rational numbers are not complete. The rational numbers are countable and the real numbers are not. There is a real number $x$ such that $x^2=2$ , and no rational number can have this property. If there are methods other than the countable/uncountable proof or $\sqrt 2\not\in\Bbb Q$ that you think can be done with a shorter proof from the axioms, I would also be interested to hear. Remember not to assume too much here. All of the proofs mentioned here are hiding some significant complexity: proving that the real numbers are uncountable requires constructing a cantor-set-like family of infinite series; proving that $\sqrt 2$ exists requires the construction of the square root function via the babylonian method or similar. I'm curious if there is some clever manipulation of the complete ordered field axioms that directly leads to the existence of an irrational number without the detours.","['elementary-set-theory', 'elementary-number-theory', 'alternative-proof', 'real-analysis']"
4234728,Find $\frac{|AE|}{|EB|}$ in the following figure containing square and circular arc,"$ABCD$ One frame and $E\in[AB]$ .Â The point where $[DE]$ cuts the arc of the circle with center $B$ and radius $[AB]$ is $F$ , if $|FE|=|FC|$ is $\frac{|AE|}{|EB|} =?$ To make the question easier, I accepted one side of the square as a unit. Then I tried to decipher it by typing the coordinates. $A(0,0),B(1,0),C(1,1),D(0,1),E(m,0), d_{DE}:y=\frac{-x}{m}+1,\ \ \ \bigcirc : (x-1)^2+y^2=1$ Wrote. Now $d_{DE}\cap\bigcirc = \{F\}$ I used to reach the coordinates of point $F$ . But it's kind of hard from now on. $|FE|=|FC|$ We can use it to get results but there's a wall in front of me: $m^6-4m^5+5m^4-8m^3+8m^2-8m+4=0$ Can you help me solve the equation or a solution with basic geometry?","['trigonometry', 'circles', 'geometry']"
4234748,A simple looking yet tricky Second Order Nonlinear Differential Equation.,"I have been struggling with the following second order differential equation for quite a while, $$y''(t)=\frac{(y'(t))^2}{y(t)}-\frac{(y'(0))^2}{y'(0)\times t+c}$$ Where $y'(0)$ is a constant of units $\frac{1}{[T]}$ and c is dimensionless. It looks fairly simple and I can bet there is a simple trick to be used to solve it but I just cannot see it. Any help would be greatly appreciated. If necessary, the initial conditions known are, $y(0)=1$ and $y'(0)$ is approximately $8.7e43$ . My first reflex was to write $y''(t)$ as $\frac{dy'(t)}{dt}$ since I could potentially just integrate $\int\frac{(y'(0))^2}{y'(0)\times t+c}dt$ , which is easy. However it didn't work. Simiralry, I have tried separating $y''(t)$ into $\frac{dy'(t)}{dy(t)}\times y'(t)$ . This however didn't work since isolating $y'(t)dy'$ was out of my range of skills. Finally, I just tried to integrate both sides with respects to $t$ , however, I lack information to compute $\int\frac{(y'(t)^2)}{y(t)}dt$ when using integration by parts.","['calculus', 'derivatives', 'ordinary-differential-equations']"
4234774,How to write 1 tuple as sets?,"A 2-tuple is $(a, b) = \{\{a\}, \{a, b\}\}$ What about a 1 tuple? Is it $(a) = \{\{a\}\}$ ? Then if $a = b$ : $(a, b) = (a, a) = \{\{a\}, \{a, a\}\} = \{\{a\}, \{a\}\} = \{\{a\}\}$ So $(a, a) = (a)$ ?",['elementary-set-theory']
4234778,Show the function is concave function but not strictly concave,"For an optimisation problem, it is required to prove: is the utility function $$f(\lambda X)=â\frac{1}{\alpha}\log\left(\frac{1}{n}
\sum_{i=1}^n \exp\{â\alpha \lambda ğ^{(ğ)}\}\right),$$ strictly concave (strictly convex-up) function  of the $\lambda$ on an open set? Here $0\leq \alpha \leq 1$ , $n$ is a sample size, $ğ^{(ğ)}$ 's are random samples from leaner space $X$ . The notations are borrowed from (Miyahara, 2010) , $g_X(\lambda):=f(\lambda X)$ is a scaled value measure and $\lambda$ is a scale parameter. From first principles: A twice differentiable function of one variable is convex-up (strictly convex-up) on an interval if and only if its second derivative is non-negative (negative) there. In my case I should test: $f''_{\lambda \lambda}(\lambda X)<0$ . My solution is: $$f'(\lambda X)=â\frac{1}{\alpha}\left(\log\left(\frac{1}{n}(\exp\{â\alpha \lambda ğ^{(1)}\}+\exp\{â\alpha \lambda ğ^{(2)}\}+\ldots + \exp\{â\alpha \lambda ğ^{(n)}\})\right)\right)'=\\
=â\frac{n}{\alpha}\cdot \frac{\left(\exp\{â\alpha \lambda ğ^{(1)}\}+\exp\{â\alpha \lambda ğ^{(2)}\}+\ldots + \exp\{â\alpha \lambda ğ^{(n)}\}\right)'}{\exp\{â\alpha \lambda ğ^{(1)}\}+\exp\{â\alpha \lambda ğ^{(2)}\}+\ldots + \exp\{â\alpha \lambda ğ^{(n)}\}}=\\
=â\frac{n}{\alpha}\cdot \frac{{\sum_{i=1}^n (\exp\{â\alpha \lambda ğ^{(i)}\}})'}{\sum_{i=1}^n\exp\{â\alpha \lambda ğ^{(i)}\}}=\\
=â\frac{n}{\alpha}\cdot \frac{-\alpha{\sum_{i=1}^n X^{(i)}\exp\{â\alpha \lambda ğ^{(i)}\}}}{\sum_{i=1}^n\exp\{â\alpha \lambda ğ^{(i)}\}}=\\
=n\cdot\frac{{\sum_{i=1}^n X^{(i)}\exp\{â\alpha \lambda ğ^{(i)}\}}}{\sum_{i=1}^n\exp\{â\alpha \lambda ğ^{(i)}\}}.$$ $$f''(\lambda X) = \left(n \cdot \frac{{\sum_{i=1}^n X^{(i)}\exp\{â\alpha \lambda ğ^{(i)}\}}}{\sum_{i=1}^n\exp\{â\alpha \lambda ğ^{(i)}\}}\right)'= \left(\frac{u(\lambda)}{v(\lambda)}\right)'=\\
=n \cdot\frac{({\sum_{i=1}^n X^{(i)}\exp\{â\alpha \lambda ğ^{(i)}\}})'\cdot \sum_{i=1}^n\exp\{â\alpha \lambda ğ^{(i)}\} - \sum_{i=1}^n X^{(i)}\exp\{â\alpha \lambda ğ^{(i)}\}\cdot(\sum_{i=1}^n\exp\{â\alpha \lambda ğ^{(i)}\})'}{(\sum_{i=1}^n\exp\{â\alpha \lambda ğ^{(i)}\})^2}=\\
=n \cdot \frac{{\sum_{i=1}^n (X^{(i)}\exp\{â\alpha \lambda ğ^{(i)}\}})'\cdot \exp\{â\alpha \lambda ğ^{(i)}\} - \sum_{i=1}^n X^{(i)}\exp\{â\alpha \lambda ğ^{(i)}\}\cdot(\exp\{â\alpha \lambda ğ^{(i)}\})'}{(\sum_{i=1}^n\exp\{â\alpha \lambda ğ^{(i)}\})^2}=\\
=n \cdot \frac{{\sum_{i=1}^n \exp\{â\alpha \lambda ğ^{(i)}\}((X^{(i)}\exp\{â\alpha \lambda ğ^{(i)}\}})'  -  X^{(i)}\cdot(\exp\{â\alpha \lambda ğ^{(i)}\})')}{(\sum_{i=1}^n\exp\{â\alpha \lambda ğ^{(i)}\})^2}=\\
=n \cdot \frac{{\sum_{i=1}^n \exp\{â\alpha \lambda ğ^{(i)}\}((X^{(i)}\exp\{â\alpha \lambda ğ^{(i)}\}})'  -  X^{(i)}\cdot(\exp\{â\alpha \lambda ğ^{(i)}\})')}{(\sum_{i=1}^n\exp\{â\alpha \lambda ğ^{(i)}\})^2}=\\
=n \cdot \frac{{\sum_{i=1}^n X^{(i)} \cdot \exp\{â\alpha \lambda ğ^{(i)}\}\cdot  (\color{red}{(\exp\{â\alpha \lambda X^{(i)}\})'-(\exp\{â\alpha \lambda X^{(i)}\})'})}}{(\sum_{i=1}^n\exp\{â\alpha \lambda ğ^{(i)}\})^2}=0.
$$ I have found that $f''_{\lambda \lambda}(\lambda X)=0$ because in the numerator we have $((\exp\{â\alpha \lambda ğ^{(i)}\})'  -  (\exp\{â\alpha \lambda ğ^{(i)}\})')=0$ and therefore $f(\lambda X)$ is not strictly concave , it is the concave function only. Question. I need the solution verification or critical comments. My be this Q&A is usefull for verification. Literature Miyahara Y. (2010). âRisk-Sensitive Value Measure Method for Projects Evaluation,â J. Option and Strategy, 2, 185-204.","['convex-optimization', 'calculus', 'solution-verification', 'derivatives']"
4234781,How to find the limit of $\lim_{x \to 0} \frac{x \tan^{-1} x}{1-\cos x}$,How to find the limit of the following function? $$\lim_{x \to 0} \frac{x \tan^{-1} x}{1-\cos x}$$ What I tried is as follows. $$\tan^{-1} x = y \implies \tan y = x $$ $$\frac{x \tan^{-1} x}{1-\cos x}=\frac{\tan y \cdot y}{1-\cos(\tan y) }$$ But it didn't work. Please Help me.,"['limits', 'algebra-precalculus']"
4234825,Inner product of polynomials defined using determinant.,"The following question was on my qualifying exam. I'd like to understand this question, but I've never seen anything like this, so I don't even know what key words to research or where to look for more information. I'm not necessarily looking for an answer to the following question. Rather, in years past, exams have routinely featured a question that explores the connection between determinants and polynomials, such as the following question. So any sources with more information about this topic, or any hints about the following problem, would be much appreciated! Question. Consider a continuous function $f: \mathbb{R} \rightarrow (0, \infty)$ such that $\int_{-\infty}^\infty |x|^j f(x) \ dx < \infty$ for all $j \geq 0$ . Denote by $\mathcal{P}_m$ the vector space of all real-valued polynomials of degree at most $m$ , equipped with the inner product $$\langle g, h \rangle = \int_{-\infty}^\infty g(x)h(x)f(x) \ dx.$$ Let $m_j = \int_{-\infty}^\infty x^j f(x) \ dx \ (j = 0, 1, 2, \dots)$ and consider the polynomials $p_0(x) = m_0$ and \begin{align}
p_n(x) = \det \left( \begin{matrix} 
m_0 & m_1 & m_2 & \dots & m_n \\
m_1 & m_2 & m_3 & \dots & m_{n+1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
m_{n-1} & m_n & m_{n+1} & \dots & m_{2n-1} \\
1 & x & x^2 & \dots & x^n 
\end{matrix} \right) \ \ \ \ n = 1, 2, \dots
\end{align} Prove that $p_0, \dots, p_m$ are orthogonal in $\mathcal{P}_m$ . Observations. I have been able to ascertain the following intuition: Any (nonzero) polynomial cannot be integrated from $-\infty$ to $\infty$ , so for polynomials $g, h \in \mathcal{P}_m$ , their product will certainly fail to have an integral. Therefore, the function $f$ is introduced. The function $f$ must have tails that go to zero fast enough so that not only can $f$ can be integrated across all of $\mathbb{R}$ , but also $f$ modulates the end behavior of the term $x^j$ so that it can be integrated. Then, the product of $f$ and $g$ will be a polynomial, the integral of a polynomial can be split across the terms, the coefficients pulled out of the integrals, and the result will be of the form $\sum_{j=1}^{2m} a_j \int_{-\infty}^\infty x^j f(x) \ dx = \sum_{j=1}^{2m} a_j m_j$ , where $a_j$ is some constant and $m_j$ is defined as per the problem. Lastly, I have checked a couple of lower order polynomials and confirmed that their inner product is zero. The computations have felt a little like the kind of index-tracking exercises that I've done during my limited exposure to multilinear algebra. So I am thinking that maybe with more powerful tools for tracking indices, I can prove the desired result. Or maybe it is an induction on $m$ . Or maybe it has something to do with somehow forcing a determinant to have linearly dependent rows. If you have read this far, bless your heart!","['inner-products', 'orthogonal-polynomials', 'determinant', 'linear-algebra']"
4234837,Showing that $\lim_{x\to\infty}x^{2}e^{-x^{8}\sin^{2}(x)}$ does not exist.,"The following function turns up in quantum mechanics as an example of an element of $\mathscr{L}^{2}(\mathbb{R})$ which does not decay to zero at $\pm\infty$ : $$
f(x)
=
x^{2}e^{-x^{8}\sin^{2}(x)}
$$ Intuitively the reason is that $f(x)$ oscillates with a shorter period the larger $x$ becomes and this plays havoc with convergence. I want to show explicitly that this limit does not exist, but it has been a long time since elementary calculus and I haven't been able to get very far. Any help would be appreciated.","['limits', 'calculus']"
4234840,"Find UMVUE of $\tau=(\lambda-\mu)e^{-(\lambda+\mu)}$ from samples $X_1, \dots, X_n\sim \rm{Pois}(\lambda)$ and $Y_1,\dots, Y_n\sim \rm{Pois}(\mu)$.","Problem Statement We have two independent Poisson samples $X_1, \ldots, X_n$ with means $\lambda$ and $Y_1, \ldots, Y_n$ with means $\mu$ . We would like to estimate $\tau=(\lambda-\mu)e^{-(\lambda+\mu)}$ . (a) Find a function of $X_1$ and $Y_1$ that is an ubiased estimator of $\tau$ . (b) Find the UMVUE of $\tau$ . (c) Calculate the asymptotic variance of this estimator when $\lambda=\mu$ . Context I am studying some old exams, and I came across this problem. I think I have the correct unbiased estimator, but my UMVUE is definitely wrong since it is a function of unknown parameters. I am not sure if I made a mistake with my complete sufficient statistic, or I did the conditional expectation wrong. Attempted Solution (a) Find some function $g(X_1,Y_1)$ such that $\mathbb{E}(g)=\tau$ . $$
\tau = \lambda e^{-(\lambda+\mu)}-\mu e^{-(\lambda+\mu)}=\mathbb{P}(X_1=1, Y_1=0)-\mathbb{P}(X_1=0, Y_1=1)=\mathbb{E}\big[I_{X_1}(1)I_{Y_1}(0)-I_{X_1}(0)I_{Y_1}(1)\big],
$$ therefore, $U=I_{X_1}(1)I_{Y_1}(0)-I_{X_1}(0)I_{Y_1}(1)$ is unbiased for $\tau$ . (b) The Lehman-Scheffe theorem tells us that, given a complete sufficient statistic $T$ and an unbiased estimate $U$ , the UMVUE is $E(U|T)$ . Let $S_j=\sum_{i=j}^nX_i+Y_i$ . From previous results we know that $S_j\sim Pois((n+1-j)(\lambda+\mu))$ , where Poisson is in the exponential family of distributions and $S_1$ is a complete sufficient statistic. Therefore, \begin{equation*}
    \begin{split}
        E(U|S_1=t)=& P(X_1=1,Y_1=0|S_1=t)-P(X_1=0, Y_1=1|S_1=t)\\
        =& \frac{P(X_1=1,Y_1=0, S_2=t-1)-P(X_1=0,Y_1=1, S_2=t-1)}{P(S_1=t)}\\
        \overset{ind.}{=}& \frac{P(X_1=1)P(Y_1=0)P(S_2=t-1)-P(X_1=0)P(Y_1=1)P(S_2=t-1)}{P(S_1=t)}\\
        =&\bigg(\frac{n-1}{n}\bigg)^t\frac{t}{(n-1)(\lambda+\mu)}(\lambda-\mu),
    \end{split}
\end{equation*} is the UMVUE for $\tau$ .","['statistics', 'poisson-distribution', 'probability-distributions', 'parameter-estimation', 'conditional-expectation']"
4234915,Expectation of random variable with two vectors,"I am reading this paper ""Continuous Diffusion Analysis"" and in page 7 there are a next formula $\mathbb{E}_{x,y}[d_S(x,y)]=\dfrac{1}{4}\sum_{i=0}^{n-1}\int^{1}_{-1}\int^{1}_{-1}|sgn(x_i)-sgn(y_i)|dx_idy_i$ . I am trying to obtain this formula using the formula $\mathbb{E}[z]=\int_{\mathbb{R}}zf(z)dz$ presented in ""Absolutely continuous case"" of wikipedia , but I do not understand how the authors of ""Continuous Diffusion Analysis"" reach that formula. For now, considering $z=d_S{(x,y)}$ follows the uniform distribution, I think that the probability density function of $d_S{(x,y)}$ is $\dfrac{1}{2n}$ . Then according to wikipedia $\mathbb{E}[z]=\int_{\mathbb{R}}zf(z)dz=\mathbb{E}[z]=\int_{\mathbb{R}}\dfrac{1}{2n}d_S(x,y)dz$ . Could you help to obtain that formula with an explanation, please? For example, how they go from $\mathbb{R}$ to $-1, 1$ limits?",['statistics']
4234963,Which of the following properties must be true.,"Consider a matrix $A = (a_{ij})_{nÃn}$ with integer entries such that $a_{ij} = 0$ for $i>j$ and $a_{ii} = 1$ for $i = 1,2...,n$ . Which of the following properties must be true ? $A^{-1}$ exists and it has integer entries. $A^{-1}$ exists and it has some entries that are not integers. $A^{-1}$ is a polynomial function of $A$ with integer coefficients. $A^{-1}$ is not a power of $A$ unless $A$ is the identity matrix. My Attempt: Here we see that $|A|=1$ and $A^{-1} = \frac{adj(A)}{|A|} = adj(A)$ . So option 1 is true and option 2 is false.
Also $ch_A(x) = (x-1)^n$ and we know that every square matrix satisfy its characteristic polynomial. So $(A-I)^n = O \implies n_{C_0}A^n(-I)^0+n_{C_1}A^{n-1}(-I)^1+n_{C_2}A^{n-2}(-I)^2+n_{C_3}A^{n-3}(-I)^3+...+n_{C_{n-1}}A(-I)^{n-1}+n_{C_n}A^0(-I)^n = 0$ There will be two cases.
Case 1: $n = 2m , m \in \Bbb N$ . Therefore $n_{C_0}A^n-n_{C_1}A^{n-1}+n_{C_2}A^{n-2}-n_{C_3}A^{n-3}+...-n_{C_{n-1}}A+n_{C_n}I = 0$ Now multiplying by $A^{-1}$ on both sides, we get $n_{C_0}A^{n-1}-n_{C_1}A^{n-2}+n_{C_2}A^{n-3}-n_{C_3}A^{n-4}+...-n_{C_{n-1}}I+n_{C_n}A^{-1} = 0 \implies n_{C_0}A^{n-1}+n_{C_1}A^{n-2}-n_{C_2}A^{n-3}+n_{C_3}A^{n-4}+...+n_{C_{n-1}}I = A^{-1}$ Case 2: $n = 2m+1 , m \in \Bbb N$ . Therefore $n_{C_0}A^{n-1}+n_{C_1}A^{n-2}-n_{C_2}A^{n-3}+n_{C_3}A^{n-4}+...-n_{C_{n-1}}I = A^{-1}$ We know that $n_{C_r} \in \Bbb Z$ . Hence option 3 is correct.
If I take $A = I$ then above cases can be written as
Case 1: $n = 2m , m \in \Bbb N$ . $n_{C_0}I^{n-1}+n_{C_1}I^{n-2}-n_{C_2}I^{n-3}+n_{C_3}I^{n-4}+...+n_{C_{n-1}}I = A^{-1}$ . So $I(n_{C_0}+n_{C_1}-n_{C_2}+n_{C_3}+...+n_{C_{n-1}}) = A^{-1} \implies kI = A^{-1}$ , where $k =(n_{C_0}+n_{C_1}-n_{C_2}+n_{C_3}+...+n_{C_{n-1}} \in \Bbb Z$ We see clearly that $A^{-1}$ is powers of $A$ and $A$ is not powers of $A$ when $A = I$ . So option 4 is true. Please correct me if I'm wrong anywhere. Thanks.","['matrices', 'solution-verification', 'inverse']"
4234976,Logic of proofs involving variables and âfor allâ statements,"It is taught that in order to prove ""for all"" statements such as $$P(G): \mathrm{For \ all \ groups} \ G \ \mathrm{it \ holds \ that} \ \{(g,g+g) \in G \times G \ | \ g \in G\} \ \mathrm{is \ a \ set}$$ one assumes that $G$ is a group and then proves the statement $P(G)$ . I wonder about the logic behind this and ""what is logically allowed"", which is never explained in introductory courses. Q $1$ : What does ""Let $G$ be a group"" even mean? A $1$ : As far as I can tell, it means that one assumes that all properties of the object of interest, in this case a group, are true and one calls the object of interest $G$ . Explicitly this would mean that it is true that $G$ is a set equipped with an addition/multiplication (whatever definition you are using) satisfying the group laws. Note that as far as I can tell one does not know which elements are contained in $G$ , only that $G$ is a set satisfying some extra properties. Q $2$ : Why is the above a set? Since I use the naive set definition, meaning that any unordered collection of objects are sets, it is true that the collection of all $(g,g+g)$ is a set, which is exactly $\{(g,2g) \in G \times G \ | \ g \in G\}$ . Q $3$ : Why does this prove the statement? Or more generally, why does this method suffice to prove a for all statement in general? I think there would be two ways to view this: $(1)$ One proved the statement is correct using nothing but the properties of a group which any group obviously satisfies. Thus the proof is ""a path"" one could follow for any explicit group to show that the statement is indeed true. $(2)$ Assume there exists a group which does not satisfy a statement proven this way. Then one could go through the process of the proof with this object and comes to a contradiction.","['elementary-set-theory', 'proof-writing', 'abstract-algebra', 'logic']"
4234984,Infinite natural numbers?,"Only using the successor function $\nu$ and the other axioms, how do we guarantee that the ""next"" generated number is different from all the ""previous"" numbers (I am using quotations because we havent defined order yet)? Concrete example: Prove that 3 is different from all ""previous"" numbers. $$0\neq \nu(\nu(\nu(0)))$$ This is true by axoiom 4. Now we prove that $1\neq3$ $$\nu(0) \neq \nu(\nu(\nu(0)))$$ by injection axiom of $\nu$ we get $$ 0 \neq \nu(\nu(0))$$ And by axoiom 4, this is a true statement. Now we could prove $2\neq3$ but i think my point is clear. We just ""peel"" off $\nu$ until we arrive at some form of $0 \neq \nu(*)$ . I don't see how one would prove this generally, that every ""next"" generated number is different from all the  ""previous"" ones without really defining order. But in order to define order we need to guarantee that every ""next"" generated number is different from the previous ones hence we are looping around. I still gave it a go and came up with some kind of ""proof"" $$ 0 \neq \nu(m) \ , \ \forall m \in \mathbb{N}$$ This is guaranteed by axiom 4. And now by applying the $\nu$ ""as often as you want"" to both sides we allways obtain that both sides are not equal due to injection.","['elementary-set-theory', 'logic', 'natural-numbers', 'analysis']"
4235011,Embedding of wedge sum into product space [duplicate],"This question already has an answer here : $X\vee Y\cong (X\times\{y_0\})\cup (\{x_0\}\times Y)$ (1 answer) Closed 2 years ago . Define the wedge sum of two pointed spaces $X$ and $Y$ as $$ X\vee Y := X\sqcup Y / \sim $$ where $\sim$ is the equivalence relation which identifies the base points of $X$ and $Y$ .
I have read in some lecture notes that the canonical map $$X\vee Y \to X\times Y $$ is a homoemorphism onto its image $X\times \{y_0\} \cup \{x_0\} \times Y$ . However, I do not see why the inverse map $$ X\times \{y_0\} \cup \{x_0\} \times Y \to X\vee Y $$ is continuous. It is given by $$ (x,y_0) \mapsto [x], \quad (x_0,y) \mapsto [y], $$ so it restricts to continuous maps on $X\times \{y_0\}$ and $\{x_0\} \times Y$ . If $X$ and $Y$ are $T_1$ -spaces, then these subspaces would be closed and the map would be continuous on all of $X\times \{y_0\} \cup \{x_0\} \times Y$ . But how do I proceed if $X$ or $Y$ is not a $T_1$ -space? Many thanks in advance!","['general-topology', 'algebraic-topology']"
4235024,A Gambling Problem (statistics),"I was watching a twitch stream the other day and the streamer was giving away his money based on a dice game.  The game works as follows: First, he rolls a 5 sided die to determine your initial starting money (from 1 to 5 dollars).  From then on, you can either take the total money, or roll again.  If you roll again, the value of the die will be added to your total money, unless you roll a 1 where you will lose it all. He created a second game where you roll an 8 sided die with similar rules to the first game, except you lose all your money if you roll a 1 or an 8. People in chat said that it was worse than the first game (since now you have a 75% chance to lose all your money instead of 80%), but there seems to be something intuitively wrong about it I can't explain. I'm not a statistician, but if someone could point me towards some sort of way to figure out which game is better for the player it'd be much appreciated!","['gambling', 'statistics']"
4235113,Simplify $\sum_{m=1}^{6} \frac{1}{(\sin k)(\sin k - \cos k)}$ where $k = \theta + \frac{m \pi}{4}$,"Good Day, I was trying to solve the below problem: Simplify $$\sum_{m=1}^{6} \frac{1}{(\sin k)(\sin k - \cos k)}$$ where $k = \theta + \frac{m \pi}{4} \text{and } 0 < \theta < \frac{\pi}{2}.
$ I was thinking of decomposing the fraction somehow and getting a telescoping sum or something, but was unable to do so. I am absolutely clueless and there is nothing that I know that simplifies or progresses on the problem. The only thing that I think works is $$\frac{1}{(\sin k)(\sin k - \cos k)} = \frac{1}{\sin^2k-\sin k \cos k} = \frac{2}{1 - \cos 2k-\sin2k}$$ but again I've no idea how to proceed. Any help would be appreciated. Thanks.","['algebra-precalculus', 'trigonometry', 'sequences-and-series']"
4235115,Infinite series identity via integrating doubly connected domain,"I am having some trouble obtaining the correct answer to the following problem.  I would appreciate it if someone could point out where I went wrong in my computations. This problem occurs as exercise 1020 in A Collection of Problems On Complex Analysis by  L. I. VolkovyskiÄ­, et. al. Problem: Prove that $$\sum_{n=1}^{\infty}\frac{n}{(n^2-3)\sqrt{4n^2-3}} = \int_{0}^{\sqrt{3}/2}\frac{x\cot(\pi x)}{(3-x^2)\sqrt{3-4x^2}}\mathrm{d}x+\frac{1}{6}\cot(\pi(2-\sqrt{3}))$$ Hint:  Use the integral \begin{equation}
    \int_C\frac{z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\mathrm{d}z
\end{equation} over the contour $C$ bounding the doubly connected domain given in figure where the small circular arcs are centered around $z=\pm\sqrt{3}/2$ and the large circular arc is of radius $n/2$ centered at the origin.  (The book doesn't actually specify the orientation of the contours.) Attempt at a Solution: First of all, I don't believe the book's suggested radius of the outer circle is correct if you take $n$ to be an arbitrary integer, as $\cot(\pi z)$ is not bounded on $\vert{z}\vert=k$ for $k\in\mathbb{Z}$ .  I'm also not sure why they write $\cot(\pi(2-\sqrt{3}))$ instead of $-\cot(\pi\sqrt{3})$ . The answer I obtain is close but off by a factor of $\pi$ for the last term and $2\pi$ for the integral term.  In particular, I get \begin{equation}
 \sum_{k=1}^{\infty}\frac{k}{(k^2-3)\sqrt{4k^2-3}}  = 2\pi \int_{0}^{\sqrt{3}/2}\frac{t\cot(\pi t)}{(3-t^2)\sqrt{3-4t^2}}\mathrm{d}t - \frac{\pi}{6}\cot(\pi\sqrt{3})
\end{equation} The factor of pi arises because the residues of $\cot(\pi z)$ at the integers is $1/\pi$ while the factor of two on the integral comes from the fact that integrating over the dog bone contour results in two integrals from $-\sqrt{3}/2$ to $\sqrt{3}/2$ , one each from the bottom and top segments, and these simplify to a single integral over $0$ to $\sqrt{3}/2$ multiplied by 4, due to the integrand being an even function. I would love to know where I went wrong! Proof: Diverging from the hint slightly, we consider the integral \begin{equation}
    \int_C\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\mathrm{d}z
\end{equation} over the contour bounding the doubly connected domain.  To integrate over the doubly connected domain we will use the following fact. Proposition: If $D$ is a doubly connected domain with boundary curves $C_1$ and $C_2$ oriented in the same direction and if f is analytic in a
domain containing $D$ and its boundary, then \begin{equation}
    \int_{C_1}f(z)\mathrm{d}z = \int_{C_2}f(z)\mathrm{d}z
\end{equation} Before beginning with integration, we need to select a branch of $\sqrt{4z^2-3}$ which is analytic in the complex plane slit from $-\sqrt{3}/2$ to $\sqrt{3}/2$ .  To do this we may factor the polynomial in the square root and choose a branch for each of the square roots of linear factors.  We define the branch of $\sqrt{2z-\sqrt{3}}$ to be such that $-\pi<\arg{(2z-\sqrt{3})}\leq \pi$ and the branch of $\sqrt{2z+\sqrt{3}}$ to be such that $-\pi<\arg{(2z+\sqrt{3})}\leq \pi$ . Recall that the complex logarithm $\log(z)$ is defined as \begin{equation}
    \log(z)=\log(\vert{z}\vert)+i\arg(z)
\end{equation} Thus the complex square root function is given by \begin{equation}
    \sqrt{z}=e^{\frac{1}{2}(\log(\vert{z}\vert)+i\arg(z))}=\sqrt{\vert{z}\vert}e^{i\frac{1}{2}\arg(z)}
\end{equation} This shows that \begin{equation}
    \sqrt{4z^2-3} = \sqrt{\vert{4z^2-3}\vert}e^{i\frac{1}{2}(\arg{(2z-\sqrt{3})}+\arg{(2z+\sqrt{3})})}
\end{equation} where $-2\pi < \arg{(2z-\sqrt{3})}+\arg{(2z+\sqrt{3})} \leq 2\pi$ .  We would like to show that $\sqrt{4z^2-3}$ is continuous across the branch cut below $z=-\sqrt{3}/2$ .  To do this let $z = -t$ where $t>\sqrt{3}/2$ and consider the points $-t+i\varepsilon$ and $-t-i\varepsilon$ above and below the real axis for $\varepsilon$ real and tending to $0$ .  For the point above we see that \begin{align}
    \lim_{\varepsilon\to 0}\sqrt{4(-t+i\varepsilon)^2-3} &= \lim_{\varepsilon\to 0}\sqrt{\vert{4(-t+i\varepsilon)^2-3}\vert}e^{i\frac{1}{2}(\arg{(2(-t+i\varepsilon)-\sqrt{3})}+\arg{(2(-t+i\varepsilon)+\sqrt{3})})} \\
    &= \sqrt{4t^2-3}e^{i\frac{1}{2}(\pi+\pi)} \\
    &= -\sqrt{4t^2-3}
\end{align} For the point below we have \begin{align}
    \lim_{\varepsilon\to 0}\sqrt{4(-t-i\varepsilon)^2-3} &= \lim_{\varepsilon\to 0}\sqrt{\vert{4(-t-i\varepsilon)^2-3}\vert}e^{i\frac{1}{2}(\arg{(2(-t-i\varepsilon)-\sqrt{3})}+\arg{(2(-t-i\varepsilon)+\sqrt{3})})} \\
    &= \sqrt{4t^2-3}e^{i\frac{1}{2}(-\pi-\pi)} \\
    &= -\sqrt{4t^2-3}
\end{align} Thus $\sqrt{4z^2-3}$ is in fact continuous across the branch cut.  To show that $1/\sqrt{4z^2-3}$ is holomorphic in $D$ we integrate along any circle in $D$ , splitting it in half along the real line.  By continuity the integrals along the segments on the branch cut annihilate each other, and since the integrals along the half circles are 0 the integral along the whole circle vanishes as well.  Thus, by Morera's Theorem $1/\sqrt{4z^2-3}$ is holomorphic in $D$ . Let us denote the circular contour of radius $N+1/2$ as $C_N$ and denote the interior dog bone contour by $D$ .  The the above proposition shows that \begin{equation}
    \int_{C_N}f(z)\mathrm{d}z = \int_{D}f(z)\mathrm{d}z
\end{equation} if we traverse each contour in the counterclockwise direction.  Beginning with the circular contour, by parameterizing we see that \begin{equation}
    \int_{C_N}\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\mathrm{d}z = \pi i(N+1/2)^2\int_{0}^{2\pi}\frac{e^{2it}\cot(\pi (N+1/2)e^{it})}{((N+1/2)^2e^{2it}-3)\sqrt{4(N+1/2)^2e^{2it}-3}}\mathrm{d}t
\end{equation} Taking the absolute value of both sides we have \begin{align}
    \bigg\vert{\int_{C_N}\frac{\pi z\cot(z)}{(z^2-3)\sqrt{4z^2-3}}\mathrm{d}z}\bigg\vert &\leq \pi (n+1/2)^2\bigg\vert{\int_{0}^{2\pi}\frac{e^{2it}\cot(\pi (N+1/2)e^{it})}{((N+1/2)^2e^{2it}-3)\sqrt{4(N+1/2)^2e^{2it}-3}}\mathrm{d}t}\bigg\vert \\
    &\leq \pi(n+1/2)^2\coth^2(\pi/2)\int_{0}^{2\pi}\bigg\vert{\frac{1}{((N+1/2)^2e^{2it}-3)\sqrt{4(N+1/2)^2e^{2it}-3}}}\bigg\vert\mathrm{d}t \\
    &\leq  \frac{\pi^2\coth^2(\pi/2)}{n+1/2}
\end{align} Thus \begin{equation}
    \lim_{N\to\infty}\int_{C_N}\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\mathrm{d}z = 0
\end{equation} On the other hand the contour encloses the poles at $z=\pm \sqrt{3}$ as well as the poles of cotangent at $z\in \mathbb{Z}$ except for $z = 0$ .  Therefore the residue theorem shows that \begin{equation}
    \frac{1}{2\pi i}\int_{C_N}\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\mathrm{d}z = \sum_{k=-N}^{N}\underset{{z=k}}{\mathrm{res}}\left(\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\right) + \underset{{z=\pm \sqrt{3}}}{\mathrm{res}}\left(\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\right)
\end{equation} With this in mind we first evaluate the residues at $\pm \sqrt{3}$ , finding that \begin{align}
    \underset{{z=\sqrt{3}}}{\mathrm{res}}\left(\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\right) &= \lim_{z\to\sqrt{3}}(z-\sqrt{3})\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}} \\
    &= \lim_{z\to\sqrt{3}}\frac{\pi z\cot(\pi z)}{(z+\sqrt{3})\sqrt{\vert{4z^2-3}\vert}e^{i\frac{1}{2}(\arg{(2z-\sqrt{3})}+\arg{(2z+\sqrt{3})})}} \\
    &= \frac{\pi}{6}\frac{\cot(\pi\sqrt{3})}{e^{i\frac{1}{2}(\arg{(\sqrt{3})}+\arg{(3\sqrt{3})})}} \\
    &= \frac{\pi}{6}\frac{\cot(\pi\sqrt{3})}{e^{i\frac{1}{2}(0+0)}} \\
    &= \frac{\pi}{6}\cot(\pi\sqrt{3})
\end{align} Likewise the residue at $z=-\sqrt{3}$ is given by \begin{align}
    \underset{{z=-\sqrt{3}}}{\mathrm{res}}\left(\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\right) &= \lim_{z\to-\sqrt{3}}(z+\sqrt{3})\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}} \\
    &= \lim_{z\to-\sqrt{3}}\frac{\pi z\cot(\pi z)}{(z-\sqrt{3})\sqrt{\vert{4z^2-3}\vert}e^{i\frac{1}{2}(\arg{(2z-\sqrt{3})}+\arg{(2z+\sqrt{3})})}} \\
    &= \frac{\pi}{6}\frac{\cot(-\pi\sqrt{3})}{e^{i\frac{1}{2}(\arg{(-3\sqrt{3})}+\arg{(-\sqrt{3})})}} \\
    &= -\frac{\pi}{6}\frac{\cot(\pi\sqrt{3})}{e^{i\frac{1}{2}(\pi+\pi)}} \\
    &= \frac{\pi}{6}\cot(\pi\sqrt{3})
\end{align} Similarly evaluating the residues at $z = \pm k$ for $k \in \mathbb{Z}_{>0}$ we find that \begin{equation}
    \underset{{z=k}}{\mathrm{res}}\left(\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\right) = \frac{k}{(k^2-3)\sqrt{4k^2-3}}
\end{equation} and \begin{equation}
    \underset{{z=-k}}{\mathrm{res}}\left(\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\right) = \frac{k}{(k^2-3)\sqrt{4k^2-3}}
\end{equation} Therefore, we find that \begin{equation}
    \frac{1}{2\pi i}\int_{C_N}\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\mathrm{d}z = 2\sum_{k=1}^{N}\frac{k}{(k^2-3)\sqrt{4k^2-3}} + \frac{\pi}{3}\cot(\pi\sqrt{3})
\end{equation} Taking the limit as $N\to \infty$ and using Cauchy's Theorem for multiply connected domains gives \begin{equation}
 2\sum_{k=1}^{\infty}\frac{k}{(k^2-3)\sqrt{4k^2-3}} + \frac{\pi}{3}\cot(\pi\sqrt{3}) = \int_{D}\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\mathrm{d}z
\end{equation} Next we break up the dog bone contour into four parts \begin{equation}
    \int_{D}=\int_{C_{\varepsilon}^{-}}+\int_{\Gamma_{\varepsilon}^{-}}+\int_{\Gamma_{\varepsilon}^{+}}+\int_{C_{\varepsilon}^{+}}
\end{equation} Where $C_{\varepsilon}^{-}$ and $C_{\varepsilon}^{+}$ denotes the circular arcs of radius $\varepsilon$ traversed in the counter clockwise direction and centered at $-\sqrt{3}/2$ and $\sqrt{3}/2$ respectively, and $\Gamma_{\varepsilon}^{-}$ and $\Gamma_{\varepsilon}^{+}$ are the lines a distance of $\varepsilon$ below and above the real line respectively. It is not difficult to show that the integrals along the small circular arcs $C_{\varepsilon}^{-}$ and $C_{\varepsilon}^{+}$ vanish as $\varepsilon \to 0$ .  On the other hand the integrals along the lines above and below the real axis can be evaluated as \begin{align}
    \lim_{\varepsilon \to 0}\int_{\Gamma_{\varepsilon}^{+}}\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\mathrm{d}z &=  \lim_{\varepsilon \to 0}\int_{\sqrt{3}/2}^{-\sqrt{3}/2}\frac{\pi (t+i\varepsilon)\cot(\pi (t+i\varepsilon))}{((t+i\varepsilon)^2-3)\sqrt{4(t+i\varepsilon)^2-3}}\mathrm{d}t \\
    &= -2i\int_{0}^{\sqrt{3}/2}\frac{\pi t\cot(\pi t)}{(t^2-3)\sqrt{4t^2-3}}\mathrm{d}t
\end{align} and \begin{align}
    \lim_{\varepsilon \to 0}\int_{\Gamma_{\varepsilon}^{-}}\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\mathrm{d}z &=  \lim_{\varepsilon \to 0}\int_{-\sqrt{3}/2}^{\sqrt{3}/2}\frac{\pi (t-i\varepsilon)\cot(\pi (t-i\varepsilon))}{((t-i\varepsilon)^2-3)\sqrt{4(t-i\varepsilon)^2-3}}\mathrm{d}t \\
    &= -2i\int_{0}^{\sqrt{3}/2}\frac{\pi t\cot(\pi t)}{(t^2-3)\sqrt{4t^2-3}}\mathrm{d}t
\end{align} Where we have used the fact that for $-\sqrt{3}/2<x<\sqrt{3}/2$ we have on the line above the real axis \begin{align}
     \lim_{\varepsilon \to 0}\sqrt{\vert{4(-x+i\varepsilon)^2-3}\vert}e^{i\frac{1}{2}(\arg{(2(-x+i\varepsilon)-\sqrt{3})}+\arg{(2(-x+i\varepsilon)+\sqrt{3})})} 
    &= \sqrt{4x^2-3}e^{i\frac{1}{2}(\pi+0)} \\
    &= i\sqrt{4x^2-3}
\end{align} Likewise below the real axis we find that \begin{align}
     \lim_{\varepsilon \to 0}\sqrt{\vert{4(-x-i\varepsilon)^2-3}\vert}e^{i\frac{1}{2}(\arg{(2(-x-i\varepsilon)-\sqrt{3})}+\arg{(2(-x-i\varepsilon)+\sqrt{3})})} 
    &= \sqrt{4x^2-3}e^{i\frac{1}{2}(-\pi+0)} \\
    &= -i\sqrt{4x^2-3}
\end{align} Putting this all together the integral over the contour $D$ becomes \begin{align}
    \int_{D}\frac{\pi z\cot(\pi z)}{(z^2-3)\sqrt{4z^2-3}}\mathrm{d}z = -4i\int_{0}^{\sqrt{3}/2}\frac{\pi t\cot(\pi t)}{(t^2-3)\sqrt{4t^2-3}}\mathrm{d}t
\end{align} giving in the end \begin{equation}
 \sum_{k=1}^{\infty}\frac{k}{(k^2-3)\sqrt{4k^2-3}}  = 2\int_{0}^{\sqrt{3}/2}\frac{\pi t\cot(\pi t)}{(3-t^2)\sqrt{3-4t^2}}\mathrm{d}t - \frac{\pi}{6}\cot(\pi\sqrt{3})
\end{equation}","['complex-analysis', 'complex-integration', 'sequences-and-series']"
4235124,Getting the most accurate bezier curve that plots a sine wave,"I would like to draw a bezier curve that looks the most like a sine wave that has a single wave length of 1000 pixel and an amplitude of 1, which is 159.15 pixels high (wave length / 2ğ). Here are the parameters of my bezier curve: p1 = starting point p2 = ending point h1 = outer handle linked to p1 h2 = inner handle linked to p2 Since I have set the wave length at 1000 pixels to begin with, I know my p1 and p2: p1 = (0,0) p2 = (1000,0) Now, I need to find the coordinates of h1 and h2. But I don't know how to get there by calculation. By trial and error I get approximately this: h1 = (477,550) h2 = (523,-550) This is not perfect as no bezier would represent perfectly a sine curve but pretty close (see picture below). My question is: knowing my starting parameters (wave length in pixels, amplitude in pixels, p1 and p2 coordinates), how can I find mathematically h1 and h2? It seems that a bezier function with 4 coordinates (p1,h1,p2,h2) uses a cubic equation that should look like this: $$B(t) = (1-t)^3 . p1 + 3(1-t)^2.t.h1 + 3(1-t).t^2.h2+t^3.p2$$ I don't know what to do with this though since the wave length and the amplitude need to be accounted for so that h1 and h2 can be found. That said, I only need to find h1 since h2 will always have as its x value the wave length minus the x value of h1 ; and its y value will be the negative of the y value of h1 . Anyway, here is the visual result I came up with by trial and errors: The white graph is the Desmos Graph accurate rendering of a sin(x). Once cropped in Photoshop to make it a single wave length and edited to make it 1000 pixels wide, I used bezier (the black graph) in After Effects to replicate it. In pink you can see the values of h1 and h2 I came up with by trials and errors. Thanks for your help.","['bezier-curve', 'calculus', 'functions', 'optimization', 'trigonometry']"
4235127,How to compute $\int_{0}^{\infty}\frac{\sqrt x\log(x)}{x^2+16}\;dx$ using Residue theorem?,"I considered $\gamma$ the following curve: From there, I know that \begin{equation}
\int_{\gamma}\frac{\sqrt z\log(z)}{z^2+16}\;dz = 2\pi i\left(\operatorname{Res}\left(f(z),4i\right)+\operatorname{Res}\left(f(z),-4i\right)\right) = \frac{2\pi i}8e^{\frac{\pi i}4}(1-i)(\log(16)+\pi) = \frac{\sqrt2}4\pi i(\log(16)+\pi)
\end{equation} \begin{equation}
\operatorname{Res}(f,4i) = \lim_{z\to 4i}(z-4i)f(z) = \lim_{z\to 4i}\frac{\sqrt z\log(z)}{x+4i} = \frac{2e^{\frac{\pi i}{4}}(\log(16)+\frac\pi2 i)}{8i} = \frac18e^{\frac{\pi i}{4}}(-i\log(16)+\pi)
\end{equation} \begin{equation}
\operatorname{Res}(f,-4i) = \lim_{z\to -4i}(z+4i)f(z) = \lim_{z\to -4i}\frac{\sqrt z\log(z)}{x-4i} = \frac{-2e^{\frac{3\pi i}{4}}(\log(16)-\frac\pi2 i)}{-8i} = \frac18e^{\frac{\pi i}{4}}(\log(16)-\pi i)
\end{equation} We also have that \begin{equation}
\int_{\gamma}f(z)\;dz = \int_{\gamma_1}f(z)\;dz + \int_{\gamma_2}f(z)\;dz - \int_{\gamma_3}f(z)\;dz - \int_{\gamma_4}f(z)\;dz 
\end{equation} It is easy to check that $\int_{\gamma_2}f(z)\;dz\to 0$ when $R\to\infty$ and $\int_{\gamma_4}f(z)\;dz\to 0$ when $\varepsilon\to0^+$ . Then, the only thing we need to do is the following: \begin{equation}
\int_{\gamma_1}f(z)\;dz =  \int_0^{\sqrt{R^2-\varepsilon^2}}f(x+i\varepsilon)\;dx\to \int_0^{\infty}f(x)\;dx\text{ when $R\to\infty$ and $\varepsilon\to0^+$}
\end{equation} Now, with $\gamma_3$ we get that \begin{equation}
\int_{\gamma_3}f(z)\;dz =  \int_0^{\sqrt{R^2-\varepsilon^2}}f(x-i\varepsilon)\;dx\to -\int_0^{\infty}\frac{\sqrt x(\log(x) + 2\pi i)}{x^2+16}\;dx\text{ when $R\to\infty$ and $\varepsilon\to0^+$}
\end{equation} Therefore: \begin{equation}
\frac{\sqrt2}4\pi i(\log(16)+\pi) = 2\int_0^{\infty}\frac{\sqrt x\log(x)}{x^2+16}\;dx + \int_0^{\infty}\frac{2\pi i\sqrt x}{x^2+16}\;dx\;\;\;\;(1)
\end{equation} To not complicate this much, I computed $\int_0^{\infty}\frac{2\pi i\sqrt x}{x^2+16}\;dx$ in Mathematica and got $\frac{i\pi^2}{\sqrt2}$ . Substracting it in (1) we get \begin{equation}
\int_0^{\infty}\frac{\sqrt x\log(x)}{x^2+16}\;dx = i\pi\frac{\log(16)-\pi}{4\sqrt2}\ne \pi\frac{\log(16)+\pi}{4\sqrt2}
\end{equation} which is the result that Mathematica gives me. Could anyone please check where my calculations are wrong?","['complex-analysis', 'contour-integration', 'residue-calculus']"
4235131,On the integral $\int_0^\infty{\frac{\sin(\lambda x)\mathrm{d}x}{e^{2\pi x}-1}}$,"In the book Asymptotics and Special Functions by Frank Olver, an integral formula of Legendre reads $$\int_0^\infty{\frac{\sin(\lambda x)\mathrm{d}x}{e^{2\pi x}-1}}=\frac{1}{2(e^\lambda-1)}-\frac{1}{2\lambda}+\frac{1}{4}\text{ for }\lambda>0$$ Which is then used to prove an already-well-known integral
(by repeatedly differentiating with respect to $\lambda$ and setting $\lambda=0$ in the formula above): $$\int_0^\infty{\frac{x^{2s-1}\mathrm{d}x}{e^{2\pi x}-1}}=(-1)^{s-1}\frac{B_{2s}}{4s}\text{ for }s>1$$ However, the author introduced, that the first formula can be obtained by straightforwardly integrating the function $\frac{e^{-i\lambda{z}}}{e^{2\pi{z}}-1}$ , in which $\lambda>0$ , around a rectangle with vertices at $0,K,K+i,i$ , and indentations at $0$ and $i$ . Then let $K\to\infty$ and the indentations shrink to zero. I literally don't see how this works, if simply applied residue theorem, and how are the integrals combined. Could someone please show me more details about the process producing the first formula through such integration? Very appreciative. Ps. I'm not a native English speaker, so I got confused with what he intrinsically meant by the term ""indentations"", the contour wasn't drawn so there's not an illustration ;) EDIT. HUUUGE gratitude to Mark Viola, I finally figured out the method. The core is to only consider the imaginary part of the integral, which won't diverge. (Added) Here's a modified version of my first approach following Mark's answer: (Equivalent) Choose $0<\delta<\frac{1}{2}$ $$I=\int_\delta^K{\frac{e^{-i\lambda{x}}\mathrm{d}x}{e^{2\pi x}-1}}+\int_K^\delta{\frac{e^{-i\lambda(x+i)}\mathrm{d}x}{e^{2\pi(x+i)}-1}}+\int_K^{K+i}{\frac{e^{-i\lambda{x}}\mathrm{d}x}{e^{2\pi{x}}-1}}+e^\lambda\int_0^{\frac{3}{2}\pi}{\frac{e^{-i\lambda\delta{e^{i\theta}}}i\delta{e^{i\theta}}\mathrm{d}\theta}{e^{2\pi\delta{e^{i\theta}}}-1}}+\int_{\frac{\pi}{2}}^{2\pi}{\frac{e^{-i\lambda\delta{e^{i\theta}}}i\delta{e^{i\theta}}\mathrm{d}\theta}{e^{2\pi\delta{e^{i\theta}}}-1}}+\int_{\delta{i}}^{i-\delta{i}}{\frac{e^{-i\lambda x}\mathrm{d}x}{e^{2\pi x}-1}}=I_1+I_2+I_3+I_4+I_5+I_6$$ $I=i(e^\lambda+1)$ by residue theorem. Taking the limit, $$\Im(I_1+I_2)=\int_0^\infty{\Im(\frac{e^{-i\lambda{x}}}{e^{2\pi x}-1})\mathrm{d}x}(1-e^\lambda)=(e^\lambda-1)\int_0^\infty{\frac{\sin(\lambda x)\mathrm{d}x}{e^{2\pi x}-1}}$$ $$\left|I_3\right|\le\max_{0\le{x}\le1}{\left|\frac{e^{-i\lambda(ix+K)}}{e^{2\pi(ix+K)}-1}\right|}\to 0$$ $$\Im(I_4)=e^{\lambda}\int_0^{\frac{3}{2}\pi}{\Im(\frac{ie^\lambda}{2\pi}+O(\delta))\mathrm{d}\theta}=\frac{3e^\lambda}{4}$$ $$\Im(I_5)=\int_{\frac{\pi}{2}}^{2\pi}{\Im(\frac{ie^\lambda}{2\pi}+O(\delta))\mathrm{d}\theta}=\frac{3}{4}$$ $$\Im(I_6)=-\int_\delta^{1-\delta}{\Im(i\frac{e^{\lambda x}}{e^{2\pi ix}-1})\mathrm{d}x}=-\int_\delta^{1-\delta}{-\frac{e^{\lambda x}}{2}\mathrm{d}x}=_{\delta\to0}\frac{e^\lambda-1}{2\lambda}$$ Combine the results and we get the result.","['complex-analysis', 'contour-integration']"
4235137,Addition of Permutations,"Sorry, if the title doesn't provide any clarity, but I didn't really know how to call it. Anyways, I've been studying quantum field theory from Blundell's book and during the derivation of the formula for second quantizing an operator I'm getting a little bit confused about a step he makes. I'll walk you through: We first write down an expression of an $N$ -particle state $$\left|\psi_1\cdots\psi_N\right>=\frac{1}{\sqrt{N!}}\sum_P\xi^P\prod_{i=1}^N\left|\psi_{P(i)}\right>$$ where the sum is taken over all $N!$ permutations of the single particle states $\left|\psi_i\right>$ . We take $\xi=+1$ for bosons while $\xi=-1$ is for fermions. Now, the inner product of two such states (either both boson states or fermion states) is: $$\left<\chi_1\cdots\chi_N|\psi_1\cdots\psi_N\right>=\frac1{N!}\sum_P\sum_Q\xi^{P+Q}\prod_{i=1}\left<\chi_{Q(i)}|\psi_{P(i)}\right>\label{before}\qquad (1)$$ Then comes the confusing part. The author says that we can rewrite the above expression using $P'=P+Q$ which spans all the permutations $N!$ times and hence: $$\left<\chi_1\cdots\chi_N|\psi_1\cdots\psi_N\right>=\sum_{P'}\xi^{P'}\prod_{i=1}^N\left<\chi_i|\psi_{P'(i)}\right>\qquad (2)$$ Writting down examples for $N=2$ and $N=3$ I can see what he means and then the generalisation seems kind of intuitive. So for $N=2$ one would write down the 2-particle state as: $$\left|\psi_1\psi_2\right>=\frac1{\sqrt{2!}}(\xi^{\sigma_0}\left|\psi_1\right>\left|\psi_2\right>+\xi^{\sigma_1}\left|\psi_2\right>\left|\psi_1\right>)$$ Where $\sigma_0=i,\sigma_1\in S_2$ (the elements of the symmetry group). Then the inner product with another state $\left|\chi_1\chi_2\right>$ would be: $$ \frac12(\xi^{\sigma_0+\sigma_0}\left<\chi_1|\psi_2\right>\left<\chi_2|\psi_1\right> +
+\xi^{\sigma_0+\sigma_1}\left<\chi_1|\psi_1\right>\left<\chi_2|\psi_2\right>
+\xi^{\sigma_1+\sigma_0}\left<\chi_1|\psi_1\right>\left<\chi_2|\psi_2\right>
+\xi^{\sigma_1+\sigma_1}\left<\chi_1|\psi_2\right>\left<\chi_2|\psi_1\right>)$$ $$ =\frac122(\xi^0\left<\chi_1|\psi_2\right>\left<\chi_2|\psi_1\right>+\xi^{1}\left<\chi_1|\psi_1\right>\left<\chi_2|\psi_2\right>) $$ So, as it seems to me, for $N=2$ : $$\sigma_0+\sigma_0=\sigma_1+\sigma_1=\sigma_1$$ $$\sigma_0+\sigma_1=\sigma_1+\sigma_0=\sigma_0$$ However, the author from the example of $N=3$ seems to have defined in general for $\sigma_n,\sigma_m\in S_3$ $$\sigma_n+\sigma_m=\sigma_{(n+m)mod(3!)}$$ Well, now to make my confusion a bit more precise, Why is there a disagreement with my definition of addition and the authors one. Is it maybe because he might consider $\sigma_1$ for example a different permutation in the symmetry group? Does the generalisation come from noticing the common theme in specific examples or is there a more formal and elegant way of transitioning from $(1)$ to $(2)$ ? The question is also posted on PSE ( link ).","['permutations', 'finite-groups', 'quantum-field-theory', 'quantum-mechanics', 'group-theory']"
4235139,"Introductory, college-level math books based on _modern concepts_?","Has there ever been a more-or-less successful attempt to write a truly modern math book, say, on analysis or geometry, one rooted in the more abstract mathematics of the last century, that is introductory in nature, i.e. that does not assume that a person has already been through a four-semester calculus sequence? Maybe something that anchors or connects it to some mainly modern subject, like abstract algebra or topology? Or that ties it more to foundational concepts, as in systems of relationships between concepts and fields, rather than, e.g., set theory. (I describe these, somewhat unconventionally, as foundational, because I think they are closer to the heart of of the practice of working mathematicians. Somewhat in the flavor of catagory theory, maybe?) I have been feeling a need to review my college calculus, and it occurred to me that I might like to learn something new at the same time, maybe something that would give me a deeper appreciation of the subject (though Apostel was pretty deep). But since my original motive was that I have forgotten much of this over the last 35 years or so, Iâd rather not focus on a book that assumes that I have all of it readily at hand. Also, Iâve been working my way through Pinterâs A Book of Abstract Agebra 2nd ed., and itâs loads of fun. It is really nice to find that I can still do proofs, after not doing any for thirty years or more.","['self-learning', 'foundations', 'learning', 'complex-analysis', 'abstract-algebra']"
4235143,Lottery fairness problem [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm wondering if this scheme is fair or not. I'm not an expert in math so I have no clue. When lottery room starts the winning number is set by random integer between 0-999. Every player enters the lottery also given random integer between 0-999, there is no duplicate. After lottery ends the payer closest to the winning number is the winner. For example: Winning number is 500
Player 1: assigned random number 123
Player 2: assigned random number 789
Player 3: assigned random number 456
So the winner is Player 3. Is there any problem with this, can I call it fair? In my opinion it seems fair because everything is random, but then again there is something that disapproves its fairness. Thank you.","['random', 'statistics', 'lotteries', 'probability']"
4235156,$x^m-1 \nmid f(x)$ in $\mathbb{Z}/p\mathbb{Z}[x]$ where $f(x)=(x+1)((x+1)^{2m}+(x+1)^{m}+1)$,Problem: Suppose that $p$ is a prime. Suppose that there is $m \in \mathbb{N}$ such that $p=1+3m$ . Define: $$f(x)=(x+1)((x+1)^{2m}+(x+1)^{m}+1) \in \mathbb{Z}/p\mathbb{Z}[x]$$ I would like to prove that if $p \neq 7$ and $p \neq 13$ then $x^m-1 \nmid f(x)$ in $\mathbb{Z}/p\mathbb{Z}[x]$ . Attempt: I observed that $f(x-1)(x^m-1)=x^p-x$ . Then I observed that $x^p-x$ has all its roots in $\mathbb{Z}/p\mathbb{Z}$ and that its roots are all distinct. I would like to prove that for $m \geq 6$ then $x^m-1$ and $((x-1)^m-1)$ share at least a root. This is sufficient because then we would have that $x^p-x$ has at least one multiple roots (I mean that it is not simple).,"['number-theory', 'finite-fields', 'abstract-algebra', 'sylow-theory', 'polynomials']"
4235161,Wrong answer in Thomas Calculus 14th Edition textbook,"There is this question on derivatives to which the answer is given as $\frac{43}{75}$ rad/sec in the answers section. This answer appears to be wrong. My Solution $$
\theta+\tan^{-1}\left(\frac{6}{4-x}\right)+\tan^{-1}\left(\frac{3}{x}\right)=\pi
$$ which gets reduced to $$
\theta=\pi-\tan^{-1}\left(\frac{3x+12}{4x-x^2-18}\right)
$$ Therefore, $$
\frac{d\theta}{dt}=\left(\frac{3x^2+24x-102}{(4x-x^2+18)^2+(3x+12)^2}\right)\frac{dx}{dt}
$$ Given $x=4$ and $\frac{dx}{dt}=2\text{ cm/sec}$ , $$
\frac{d\theta}{dt}=-\frac{7}{75}\text{rad/sec}
$$ I've got the graph of $\theta$ as a function of $x$ here , which also indicates my answer is correct. Or am I? Kindly help.",['derivatives']
4235171,"What is the general formula of the sum $\sum_{k=0}^{n}(-1)^{k} \binom{n}{k}\binom{k/2}{m}$ for $m,n\in\mathbb{N}$?","The classical Euler's gamma function $\Gamma(z)$ can be defined by \begin{equation}
\Gamma(z)=\lim_{n\to\infty}\frac{n!n^z}{\prod_{k=0}^n(z+k)}, \quad z\in\mathbb{C}\setminus\{0,-1,-2,\dotsc\}.
\end{equation} The extended binomial coefficient $\binom{z}{w}$ for $z,w\in\mathbb{C}$ is defined by \begin{equation}
\binom{z}{w}=
\begin{cases}
\dfrac{\Gamma(z+1)}{\Gamma(w+1)\Gamma(z-w+1)}, & z\not\in\mathbb{N}_-,\quad w,z-w\not\in\mathbb{N}_-;\\
0, & z\not\in\mathbb{N}_-,\quad w\in\mathbb{N}_- \text{ or } z-w\in\mathbb{N}_-;\\
\dfrac{\langle z\rangle_w}{w!},& z\in\mathbb{N}_-, \quad w\in\mathbb{N}_0;\\
\dfrac{\langle z\rangle_{z-w}}{(z-w)!}, & z,w\in\mathbb{N}_-, \quad z-w\in\mathbb{N}_0;\\
0, & z,w\in\mathbb{N}_-, \quad z-w\in\mathbb{N}_-;\\
\infty, & z\in\mathbb{N}_-, \quad w\not\in\mathbb{Z},
\end{cases}
\end{equation} where \begin{align}
\mathbb{Z}&=\{0,\pm1,\pm2,\dotsc\}, & \mathbb{N}&=\{1,2,\dotsc\},\\
\mathbb{N}_0&=\{0,1,2,\dotsc\}, & \mathbb{N}_-&=\{-1,-2,\dotsc\}
\end{align} and \begin{align}
\langle\alpha\rangle_n&=\prod_{k=0}^{n-1}(\alpha-k)\\
&=
\begin{cases}
\alpha(\alpha-1)\dotsm(\alpha-n+1), & n\ge1\\
1, & n=0
\end{cases}
\end{align} is called the falling factorial. My question is: what are the general formulas of the finite sums $$
\sum_{k=0}^{n}(\pm1)^{k} \binom{n}{k}\binom{k/2}{m}
$$ for $n\ge0$ and any suitable number $m$ ? When $m\ne0$ and $n\ge0$ , it is known that the identity $$
\sum_{k=0}^{n}\binom{n}{k}\binom{k/2}{m}=\frac{n}{m}\binom{n-m-1}{m-1}2^{n-2m}
$$ is valid. When $2m\ge n+1\ge1$ , it is known that the identity \begin{align}
\sum_{k=0}^{n}(-1)^{k} \binom{n}{k}\binom{k/2}{m}
&=(-1)^m\biggl[\binom{2m-n-1}{m-1}-\binom{2m-n-1}{m}\biggr]2^{n-2m}\\
&=(-1)^m\frac{n}{m}\binom{2m-n-1}{m-1}2^{n-2m}
\end{align} is valid. These two answers are slightly modifications in form of Items (3.163) and (3.164) on pages 91--92 in the monograph: R. Sprugnoli, Riordan Array Proofs of Identities in Gouldâs Book , University of Florence, Italy, 2006. (Has this monograph been formally published somewhere?) The identity \begin{equation}
\sum_{k=0}^{r}(-1)^k\binom{r}{k}\binom{k/2}{q}
=\frac{(-1)^q}{2^{2q-r}}\frac{r}{2q-r}\binom{2q-r}{q}, \quad 0\le r\le q
\end{equation} at Finite summation with binomial coefficients, $\sum (-1)^k\binom{r}{k} \binom{k/2}{q}$ has a more strict restriction $0\le r\le q$ than the above restriction $2m\ge n+1\ge1$ . Finally and essentially speaking, my question is: if without the restriction $2m\ge n+1\ge1$ , what is the general formula of the sum $\sum_{k=0}^{n}(-1)^{k} \binom{n}{k}\binom{k/2}{m}$ for $m\in\mathbb{N}$ and $n\ge0$ ? even for $m\in\mathbb{C}$ and $n\ge0$ ?","['summation', 'polygamma', 'gamma-function', 'combinatorial-number-theory', 'combinatorics']"
4235176,The category of finite abelian groups has no (nonzero) projective objects.,"This is a proof verification post about the following statement: The category of finite abelian groups has no nonzero projective objects. Similar questions: About finitely generated abelian groups and about (all) finite groups . $\newcommand{\Z}{\mathbb{Z}}$ First we make the following observations for objects in any abelian category: Observation 1. Let $A = B \oplus C$ . If $A$ is projective, then so is $B$ . (The proof is essentially as the proof that a direct summand of a free module is projective.) Observation 2. Suppose $P$ is projective. Then, any exact sequence of the form $$A \to P \to 0$$ splits. (Use projectivity to lift the map via $P \xrightarrow{\operatorname{id}_{P}} P$ .) Now, by the Structure Theorem for Finite Groups, it suffices to show that $\mathbb{Z}/p^{n}$ is not projective for any prime $p$ and $n \ge 1$ . To this end, consider the exact sequence \begin{equation*} 
	\mathbb{Z}/p^{n + 1} \xrightarrow{p} \mathbb{Z}/p^{n} \to 0.
\end{equation*} Since $\mathbb{Z}/p^{n + 1}$ cannot be written as a direct sum of nontrivial cyclic groups, it follows that the above sequence does not split. In turn, it follows that $\mathbb{Z}/p^{n}$ is not projective. Question: Is the above correct?","['finite-groups', 'category-theory', 'group-theory', 'solution-verification', 'abelian-groups']"
4235182,Finding a closed form for $\sum_{k=0}^{n}k^5$ using generating functions [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Question: Find a closed form for $\sum_{k=0}^{n}k^5$ using generating functions. $Solution.$ We define $f(x)=\sum_{n=0}^{\infty}\sum_{k=0}^{n}k^5x^n=\sum_{n=0}^{\infty}a_nx^n$ . Therefore, $$\begin{array}{l}
a_{n} =\{1,1+32,1+32+243,1+32+243+1024,\dotsc \}\\
\\
b_{n} =\{1,32,243,1024,3125,7776,16807,32768,\dotsc \}\\
\\
c_{n} =\{1,31,211,781,2101,4651,9031,15961,\dotsc \}\\
\\
d_{n} =\{1,30,180,570,1320,2550,4380,6930,\dotsc \}\\
\\
e_{n} =\{1,29,150,390,750,1230,1830,2550,\dotsc \}\\
\\
f_{n} =\{1,28,121,240,360,480,600,720,\dotsc \}\\
\\
g_{n} =\{1,27,93,119,120,120,120,120,\dotsc \}\\
\\
h_{n} =\{1,26,66,26,1,0,0,0,\dotsc \}\\
\\
\Longrightarrow h_{n} =1+26x +66x^{2} +26x^{3} +x^{4}\\
\\
\Longrightarrow a_{n} =\frac{1+26x +66x^{2} +26x^{3} +x^{4}}{( 1-x)^{7}}\\
\\
\Longrightarrow f( x) =\frac{1+26x +66x^{2} +26x^{3} +x^{4}}{( 1-x)^{7}}\\
\\
=\frac{1}{( 1-x)^{7}} +\frac{26x}{( 1-x)^{7}} +\frac{66x^{2}}{( 1-x)^{7}} +\frac{26x^{3}}{( 1-x)^{7}} +\frac{x^{4}}{( 1-x)^{7}}\\
\\
\\
=\sum _{n=0}^{\infty }\binom{n+7-1}{7-1} x^{n} +26\sum _{n=0}^{\infty }\binom{n+7-1}{7-1} x^{n+1} +66\sum _{n=0}^{\infty }\binom{n+7-1}{7-1} x^{n+2}\\
+26\sum _{n=0}^{\infty }\binom{n+7-1}{7-1} x^{n+3} +\sum _{n=0}^{\infty }\binom{n+7-1}{7-1} x^{n+4}\\
\\
=\sum _{n=0}^{\infty }\binom{n+6}{6} x^{n} +26\sum _{n=0}^{\infty }\binom{n+6}{6} x^{n+1} +66\sum _{n=0}^{\infty }\binom{n+6}{6} x^{n+2}\\
+26\sum _{n=0}^{\infty }\binom{n+6}{6} x^{n+3} +\sum _{n=0}^{\infty }\binom{n+6}{6} x^{n+4}
\end{array}$$ By substitution, $$ \begin{array}{l}
=\sum _{n=0}^{\infty }\binom{n+6}{6} x^{n} +26\sum _{n=0}^{\infty }\binom{n-1+6}{6} x^{n} +66\sum _{n=0}^{\infty }\binom{n-2+6}{6} x^{n}\\
+26\sum _{n=0}^{\infty }\binom{n-3+6}{6} x^{n} +\sum _{n=0}^{\infty }\binom{n-4+6}{6} x^{n}\\
\\
=\sum _{n=0}^{\infty }\binom{n+6}{6} x^{n} +26\sum _{n=0}^{\infty }\binom{n+5}{6} x^{n} +66\sum _{n=0}^{\infty }\binom{n+4}{6} x^{n}\\
+26\sum _{n=0}^{\infty }\binom{n+3}{6} x^{n} +\sum _{n=0}^{\infty }\binom{n+2}{6} x^{n}\\
\\
=\sum _{n=0}^{\infty }\left[\binom{n+6}{6} +26\binom{n+5}{6} +66\binom{n+4}{6} +26\binom{n+3}{6} +\binom{n+2}{6}\right] x^{n}\\
\\
\Longrightarrow a_{n} =\binom{n+6}{6} +26\binom{n+5}{6} +66\binom{n+4}{6} +26\binom{n+3}{6} +\binom{n+2}{6}
\end{array}$$ Now, I have checked over the Wolfram site, and it is incorrect. I don't know where I was wrong.","['discrete-mathematics', 'generating-functions']"
4235197,Is a stopping time still a stopping time when the time set of the filtration is extended?,"Let $I\subseteq\overline{\mathbb R}$ and $(\mathcal F_t)_{t\in I}$ be a filtration on a measurable space $(\Omega,\mathcal A)$ . Remember that $\tau:\Omega\to I\cup\{\sup I\}$ is called $(\mathcal F_t)_{t\in I}$ -stopping time if $$\forall t\in I:\{\tau\le t\}\in\mathcal F_t\tag1.$$ Now it's natural (think about $I=\mathbb N_0$ and $J=[0,\infty)$ , for example) ask whether we can show the following: Let $J\subseteq\overline{\mathbb R}$ with $I\cup\{\sup I\}\subseteq J\cup\{\sup J\}$ and $(\mathcal G_t)_{t\in I}$ be a filtration on $(\Omega,\mathcal A)$ with $$\forall t\in I:\mathcal F_t=\mathcal G_t\tag2.$$ Assuming $\tau$ is an $(\mathcal F_t)_{t\in I}$ -stopping time, is it an $(\mathcal G_t)_{t\in I}$ -stopping time as well? Let $t\in J$ . We weed to show $\{\tau\le t\}\in\mathcal G_t$ . If $t\in I$ , then $\{\tau\le t\}\in\mathcal F_t=\mathcal G_t$ . If $t\ge\sup I$ , then $\{\tau\le t\}=\Omega\in\mathcal G_t$ . If $t<\inf I$ , then $\{\tau\le t\}=\emptyset\in\mathcal G_t$ . Now, I struggle to show $\{\tau\le t\}\in\mathcal G_t$ in the remaining cases. Clearly, it will boil down to argue that there must be a $s\in I$ with $s<t$ and $$\{\tau\le t\}=\{\tau\le s\}\in\mathcal F_s=\mathcal G_s\subseteq\mathcal G_t\tag3.$$ But can we really find such an $s$ ? (If we can but only under some additional assumption, please feel free to add this assumption.)","['measure-theory', 'stopping-times', 'probability-theory']"
4235246,What is the size of this angle?,"The diagram shows two circles, centres $A$ and $B$ , each of radius $10 \text{cm}$ . The point $B$ lies on the circumference of the circle with centre $A$ . The two circles intersect at the points $C$ and $D$ . The point $E$ lies on the circumference of the circle centre $B$ such that $ABE$ is a diameter. Write down, in terms of $\pi$ , angle $CBE$ . Note: when trying to reproduce the diagram, I forgot to put a line segment between $A$ and $C$ . I tried to solve this question but got the answer wrong: $\triangle ABC \text{ is an equilateral triangle since all of its sides are equal in length}$ \begin{align}
\angle ABC &= \frac{360^\circ}{3} \\
&= \frac{2\pi}{3} \\
\\
\angle CBE &= 180^\circ - \angle ABC \\
&= \pi - \frac{2\pi}{3} \\
&= \frac{\pi}{3} \text{rad}
\end{align} My answer, $\frac{\pi}{3} \text{rad}$ , was incorrect. The correct answer was $\frac{2\pi}{3}$ . How and why?","['euclidean-geometry', 'geometry']"
4235331,Which finite groups have their minimal permutation degree equal to their order?,"I define the degree of a permutation representation of a group (or group acting on a set) as the number of letters in that representation, and the minimal degree of a group $G$ to be the minimum number of letters that the group can act on; i.e., $G$ injects into $S_n$ but not into $S_{n-1}$ . I define a group $G$ to be Cayley if its minimal degree is the same as the order of the group, so that the Cayley representation is an example of this minimal degree. So which groups are Cayley? So far I have found that cyclic groups of prime power order are Cayley, and the Klein 4-group is Cayley. The quaternion group $Q_8$ is Cayley because it has too many elements of order $4$ (six) to be injected into $S_7$ , which has only four order- $4$ elements. The direct product of a group $G$ of order $>2$ and a group $H$ of order $>1$ is not Cayley. I wonder if there are any other Cayley groups. In particular I wonder if the generalized quaternion group $Q_{16}$ is Cayley.","['permutations', 'finite-groups', 'symmetric-groups', 'group-theory', 'quaternions']"
4235332,Is $\log(\aleph_0)$ undefined? [duplicate],"This question already has answers here : Is the logarithm of $\aleph_0$ infinite? (4 answers) Closed 7 months ago . NOTE: In the context of this question, the base of $\log(x)$ is $10$ . I was researching cardinal arithmetic when I found out about logarithms of infinite cardinal numbers. Assuming the axiom of choice and given a infinite cardinal $Îº$ and a finite cardinal $Î¼$ greater than 1, there may or may not exist a cardinal $Î»$ which satisfies $Î¼^Îº=Î»$ . If $Î»$ does exist, then it is infinite and less than $Îº$ . Since $\aleph_0$ is the smallest infinite cardinal number, $Î»$ cannot exist, so does that mean $\log(\aleph_0)$ is undefined?","['elementary-set-theory', 'cardinals']"
4235369,If $A$ is Lebesgue measurable then there exist $F_1\subset F_2\subset\dots$ closed in $A$ such that $|A\setminus\bigcup_{n=1}^{\infty}F_n|=0$,"I have proved the following statement and I would like to know if it is correct and/or/if it could be improved somehow, thanks. ""If $A\subset\mathbb{R}$ is Lebesgue measurable, then there exists an increasing sequence $F_1\subset F_2\subset\dots$ of closed sets in $A$ such that $|A\setminus\bigcup_{n=1}^{\infty}F_n|=0$ "" My proof: Let $A$ be a Lebesgue measurable set: then we know that for every $\varepsilon>0$ there exists a closed set $C\subset A$ such that $|A\setminus C|<\varepsilon$ , so, for every $n\geq 1$ , we have that there exists $C_n\subset A$ closed such that $|A\setminus C_n|<\frac{1}{n}$ . Consider now, for $n\geq 1$ , the sets $F_n$ defined by $F_n:=\bigcup_{k=1}^{n}C_k$ : these form an increasing sequence of sets ( $F_{n+1}=\bigcup_{k=1}^{n+1}C_k=\bigcup_{k=1}^{n}C_k \cup C_{n+1}=F_n\cup C_{n+1}\supset F_n$ ) which are also closed (being the union of finitely many closed sets). Now, since $A\setminus\bigcup_{n=1}^{\infty}F_n\subset A\setminus F_k\subset A\setminus C_k$ for every $k\geq 1$ we have as a consequence that $|A\setminus\bigcup_{n=1}^{\infty}F_n|\leq |A\setminus F_k|\leq |A\setminus C_k|<\frac{1}{k}$ for every $k\geq 1$ so $|A\setminus\bigcup_{n=1}^{\infty}F_n|=0$ , and this concludes the proof. $\square$","['measure-theory', 'solution-verification', 'lebesgue-measure', 'real-analysis']"
4235387,How to tell if polygons are interlocked?,"Consider an arrangement of nonoverlapping concave polygons. For instance here are two different arrangements. Any single polygon is ""interlinked"" in the sense that it can't be moved apart if the others remain stationary. However, the arrangement on the left can be separated, but the one on the right can't; it is ""interlocked"". Is there a way to calculate whether an ""interlinked"" arrangement is ""interlocked"" or not? Are there existing computer algorithms to determine this? How about for polyhedra in 3D? (See for example ""coordinated motion puzzle"".)","['euclidean-geometry', 'geometry']"
4235401,What is the measure of the $\measuredangle EAD$ where $E$ is outside the square $ABCD$?,"For reference: Let the squares $ABCD$ and $FGDE$ such that E, G and C are collinear, $GE = GC$ , Calculate the measure of the $\measuredangle EAD$ where $E$ is outside the square $ABCD$ .(answer: 18.5 $^\circ$ ) My progress.. I was able to draw the figure but I don't know there is some restriction...in geogebra the solution matches","['euclidean-geometry', 'angle', 'geometry', 'plane-geometry']"
4235432,time-dependent inflection points,"For a function $u:\mathbb{R}\times [0,\infty)\rightarrow\mathbb{R}$ , let $\bar{x}$ be an inflection point of $u_0(x):=u(x,0)$ to the right of its maximum. Further, let the inflection point evolve with time $t\mapsto\bar{x}(t)$ such that $\bar{x}(0)=\bar{x}$ . Surely we then have $(u_0)_{xx}(\bar{x}(0))=u_{xx}(\bar{x}(0),0)=0$ , but why is it true that $u_{xx}(\bar{x}(t),t)=0$ for all $t\geq 0$ ? I'm assuming this follows since $(\bar{x}(t),t)$ is also an inflection point of $u$ , but I can't see why this is true. For reference, I'm trying to understand the proof of Lemma 6 from this paper: https://arxiv.org/pdf/1707.09000.pdf . It is in this proof that I encountered the claim above. I'm pretty confused about the entire idea of a time-dependent inflection point, and in particular, about going from an inflection point of a function of a single variable, i.e. an inflection point of $u_0(x)$ , to an inflection point of the two-variable function $u$ . I would really appreciate a clear, pedagogical answer if possible.. Thanks in advance.","['multivariable-calculus', 'partial-differential-equations']"
4235442,Deriving a formula for the amount of time you should study for multiple tests,"Just a bit of procrastination here but at the bus stop I was wondering how should I study before multiple tests given the percentage of the grade each one was worth, the days until each test and the amount of hours you want to study that day. Here's my attempt I have come to the conclusion that it would be an inverse relationship between the percentage of time you should allocate (S) and the time until the test (t). Multiplied this by the grade percentage the test was worth (G). $$S\propto \frac{G}{t}$$ Given that you have multiple tests and 8 hours (S=8) a day to study I sum the above relationship to and equate the hours to yield the following: (k= 8/sum) $$8=k \sum^n_{i=1} S_i$$ $$8=k \sum^n_{i=1} \frac{G_i}{t_i}$$ For instance let's say that I have 6 tests: English( $G_1=0.25$ , $t=58$ days) Maths Methods( $G_2=0.5$ , $t=60$ days), Specialist Maths( $G_3=0.5$ , $t=65$ days), Chemistry( $G_4=0.5$ , $t=72$ days), Modern History( $G_5=0.25$ , $t=74$ days), Physics( $G_6=0.5$ , $t=76$ days). Today I have 8 hours to study (more like 7 after I'm done with this), using my formula I can calculate how long I should study for each test today ( $S_i$ ). $$8=k \sum^n_{i=1} \frac{G_i}{t_i}= k\Big(\frac{G_1}{D_1}+\frac{G_2}{D_2}+\frac{G_3}{D_3}+\frac{G_4}{t_4}+\frac{G_5}{t_5}+\frac{G_6}{t_6}\Big)$$ $$=k\Big(\frac{0.25}{58}+\frac{0.5}{60}+\frac{0.5}{65}+\frac{0.5}{72}+\frac{0.25}{74}+\frac{0.5}{76}\Big)$$ $k\approx 214.84$ Therefore, English $S= 0.93$ hours Maths Methods $S= 1.79$ hours Specialist Maths $S= 1.65$ hours Chemistry $S= 1.49$ hours Modern History $S= 0.73$ hours Physics $S= 1.41$ hours Now given that the time approaches the day before the first test (English) I want to just study the content of the first test so I am ready for the test. So let $t_1=1$ $$8=k\Big(\frac{0.25}{1}+\frac{0.5}{3}+\frac{0.5}{8}+\frac{0.5}{15}+\frac{0.25}{17}+\frac{0.5}{19}\Big)$$ $k\approx 14.45 $ Therefore English $S= 3.61$ hours Maths Methods $S= 2.4$ hours Specialist Maths $S=0.90 $ hours Chemistry $S= 0.48$ hours Modern History $S= 0.21$ hours Physics $S= 0.38$ hours This is where my model somewhat fails as I want most of the day to be studying the most urgent subject. If the test dates were more 'spaced out' or English had a 50% of the marks test then I would spend most of the day before the test studying the subject. Can any of you 'test anxious' mathematicians suggest any improvements of this crude model (I am a high school student)? I was thinking about throwing in some differential equations ( $\frac{\partial S}{\partial t_1} )$ but I'm not smart enough for that yet. Thank you Edit: I have more of a think about it and have come up with the following conditions. Ignoring $S\propto \frac G t$ $$1=k \sum^n_{i=1} S_i $$ $$0=\sum^n_{i=1} \frac{dS_i}{dt} $$ $$ S_i(t_i=1)=1 $$ $$ S_i(t_i<1)=0$$ $t$ is the days until the exam. I have come up with the following model for 3 different tests with test dates t=10, t=20, t=30 https://www.desmos.com/calculator/jrrawaofki This application can be extended to all tasks you pursue which have a deadline. This isn't a serious model and there is plenty of space to play with. How could I extend and improve this model using differential equations? This is the second bounty","['ordinary-differential-equations', 'calculus', 'partial-differential-equations', 'recreational-mathematics', 'derivatives']"
4235443,Boundary in the sense of currents VS measure theoretic boundary in arbitrary codimension,"Let $M^m$ be an oriented Lipschitz submanifold with boundary of $\mathbb{R}^n$ . One can ask whether the boundary of $M$ in the sense of currents coincides with the topological boundary. A celebrated result by Federer about finite perimeter sets implies that the answer is positive if $m=n$ , while a paper of Harrison proves it in general codimension (Theorem 3.iii, https://arxiv.org/abs/math/9310231 ). I am asking whether there exist a proof of the general codimension case that avoids the use of $(\lambda,n)$ -sets introduced by Harrison and directly generalize the methods of Federer? More specifically, I want to know if there is a way to define a notion of measure theoretic boundary in arbitrary codimension that allows one to prove something along the lines of ""if the measure theoretic boundary of a countably rectificable set has finite Hausdorff measure, then the measure theoretic boundary and the boundary in the sense of currents coincide""?","['stokes-theorem', 'geometric-measure-theory', 'differential-forms', 'differential-geometry']"
4235454,Arc Radius Calculation from 2 points,"I've seen other questions on here and tried to follow them, but I was hoping somebody could help me understand where I'm going wrong in my solution, and point me in the right direction. I've got the points shown in the above picture: $A$ and $B$ , which are known - thus the distance between them (labeled $\overline{\rm AB}$ ).  Also known/given is the arc length, $\overset{\mmlToken{mo}{â}}{AB\,}$ .  I am trying to find R ( $\overline{\rm AC}$ or $\overline{\rm BC}$ ) and/or $\theta$ . (C is unknown.) $$\overset{\mmlToken{mo}{â}}{AB\,} = \theta R,\space \text{thus}\space R = \frac{\overset{\mmlToken{mo}{â}}{AB\,}}{\theta} $$ $$2R \sin (\frac{\theta}{2}) = \overline{\rm AB}, \space \text{so}\space R = \frac{\overline{\rm AB}}{2\sin (\frac{\theta}{2})}$$ Using these, we can find: $$\frac{\overset{\mmlToken{mo}{â}}{AB\,}}{\theta} = \frac{\overline{\rm AB}}{2\sin (\frac{\theta}{2})}$$ With some rearranging, $$\sin (\frac{\theta}{2}) = \frac{\overline{\rm AB}}{\overset{\mmlToken{mo}{â}}{AB\,}}\frac{\theta}{2}$$ From here, we can do some variable reassignment:  I'll say $t = \frac{\theta}{2}$ , and $k = \frac{\overline{\rm AB}}{\overset{\mmlToken{mo}{â}}{AB\,}}$ . This gives us $\sin (t) = kt$ I see no way for $k$ to be $>1$ (and $k=1$ only if the radius is infinite and theta is $0$ ), but I suppose theta could be basically any angle - I'm solving for positive, and for my use case I expect it will always fall in the $0-\pi$ range, but I guess it doesn't have to. I feel fairly confident on the geometry side of things; that makes sense to me.  I haven't done much by way of approximation, so I don't know where to go from here.  I understand from my reading of other questions that this is a ""transcendental"" equation, which apparently means something like ""doesn't have a closed-form algebraic solution""?  Thus my question: have I screwed anything up?  If so, what?  If not, where do I go from here? I tried doing something like $\frac{\sin (t)}{t} = k$ , but I still don't know how to computationally approximate this - I'm embedding this in an algorithm I'm using on a website for solving a specific class of geometry problems, so I'd like it to be as accurate as possible - an arbitrary number of decimal places would be great, but I'll settle for like $4$ . Thank you!","['arc-length', 'approximation', 'circles', 'geometry', 'trigonometry']"
4235459,"Proving that $\lim_{s \to 0^-} \int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} x^{s} \, dx = \int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} \, dx $","In an attempt to show that $$\int_0^\infty \frac{\arcsin(\sin x)}{x} \, \mathrm dx =2G,$$ where $G$ is Catalan's constant,  I first evaluated the Mellin transform $$I(s)=\int_0^\infty \frac{\arcsin(\sin x)}{x}  x^s\, \mathrm dx, \quad -1 < s< 0,$$ by using the fact that $\arcsin(\sin x)$ can be represented by the Fourier series $$\arcsin(\sin x) = \frac{4}{\pi} \sum_{n=0}^\infty \frac{(-1)^n \sin \left[(2n+1)x \right]}{(2n+1)^2}. $$ Replacing $\arcsin(\sin x)$ with this representation and then switching the order of summation and integration, which is justified by Fubini's theorem, I got $$ I(s) = \frac{4}{\pi} \beta(2+s) \Gamma(s) \sin \left(\frac{\pi s}{2} \right),$$ where $\beta(s)$ is the Dirichlet beta function . Now I want to argue that $$\lim_{s \to 0^-} I(s) = \int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} \, \mathrm dx  $$ so that I can conclude that $$\begin{align} \int_0^\infty \frac{\arcsin(\sin x)}{x} \, \mathrm dx &= \frac{4}{\pi} \lim_{s \to 0^-} \beta(2+s) \Gamma(s) \sin \left(\frac{\pi s}{2} \right) \\ &= \frac{4}{\pi} \beta(2) \lim_{s \to 0^-} \left( \frac{1}{s} + \mathcal{O}(1) \right) \sin \left(\frac{\pi s}{2} \right) \\ &= 2 G. \end{align} $$ The issue is that $\int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} \, \mathrm dx$ doesn't converge absolutely, so we can't appeal immediately to the dominated convergence theorem. In the case of the Laplace transform,  it turns out that $$\lim_{s \to 0^+} \int_0^\infty \frac{f(x)}{x} e^{-sx} \, \mathrm dx = \int_0^\infty \frac{f(x)}{x} \, \mathrm dx$$ if $\int_0^\infty \frac{f(x)}{x} \, \mathrm dx$ exists as an improper Riemann integral.  This has to do with the fact that the improper integrals $\int_0^\infty \frac{f(x)}{x} e^{-sx} \, \mathrm dx$ converge uniformly on $[0, \infty)$ . (See here and here .) I don't know if the Mellin transform has a similar property, but I imagine it does since the two transforms are related.","['integration', 'real-analysis', 'fourier-series', 'limits', 'mellin-transform']"
4235496,Need help in understanding how to Calculate Estimation and Variance,"I have this problem at hand and have no idea on how to approach it. Any leads/solution would be appreciated. You roll a fair $6$ -sided die, and flip a coin n times where n is the number on the die. If $H$ refers to the number of heads observed after n coin flips, find $E[H|n]$ and $\operatorname{Var}(H|n)$ for $n$ in set ${1,2,3,4,5,6}$ Find $E[H]$ and $\operatorname{Var}(H|n)$","['expected-value', 'statistics', 'variance', 'probability']"
4235529,Infinite nested radicals from Putnam exam 1953,"Prove that the following sequence is convergent and find the limit: $$\sqrt{7}, \sqrt{7-\sqrt{7}}, \sqrt{7-\sqrt{7+\sqrt{7}}}, ... $$ with $x_{n+2}=\sqrt{7-\sqrt{7+x_{n}}}$ . Notice that $$2=\sqrt{7-3}=\sqrt{7-\sqrt{7+2}}=\sqrt{7-\sqrt{7+\sqrt{7-\sqrt{7+2}}}} $$ $$=\sqrt{7-\sqrt{7+\sqrt{7-\sqrt{7+\sqrt{7-\sqrt{7+...}}}}}}$$ Q: Is this proof valid? If not why? Since this is a contest problem and the trick is well known (but not at that time i guess) I already know the rigorous solution, but this is not my Q.","['contest-math', 'algebra-precalculus', 'nested-radicals', 'sequences-and-series']"
4235552,Non-iterative deterministic function to map an input to a random output in range without repeating,"Apologies if this has been answered before or is impossible, but: Is there a state-independent, non-iterative function that, given an (integer) input (n) and (integer) minimum (min) and maximum (max) values, uniquely maps n to a pseudorandom (integer) output between min and max ? By non-iterative, I mean it wouldn't have to iterate through every number in the set from min to max in order to map n . For example (I do not know LaTeX, sorry): The function is defined as f(n, min, max) f(1, 1, 5) = 5
f(2, 1, 5) = 2
f(3, 1, 5) = 4
f(4, 1, 5) = 1
and f(5, 1, 5) = 3 And similarly for other min and max values.","['random-functions', 'random', 'functions']"
4235566,"Evaluate $\prod_{p \in \mathbb{P}} \left( 1 + \frac{3p^2}{(p^2 - 1)^2} \right)$ for prime numbers $p$, where $p = 14k \pm 1$ or $p = 18k \pm 1$","Evaluate : $$\prod_{p \in \mathbb{P}} \left( 1 + \frac{3p^2}{(p^2 - 1)^2} \right)$$ for prime numbers $p$ such that $$p = 14k \pm 1$$ or $$p = 18k \pm 1.$$ (These are two different questions.) I managed to calculate it for all primes $p$ : $$\prod_{p \in \mathbb{P}} \left( 1 + \frac{3p^2}{(p^2 - 1)^2} \right) = \prod_{p \in \mathbb{P}} \frac{p^4 + p^2 + 1}{(p^2 - 1)^2} = \prod_{p \in \mathbb{P}} \frac{(p^2 - p + 1)(p^2 + p + 1)}{(p^2 - 1)^2}$$ $$= \prod_{p \in \mathbb{P}} \frac{(p^3 - 1)(p^3 + 1)}{(p^2 - 1)^3} = \prod_{p \in \mathbb{P}} \frac{1 - p^{-6}}{(1 - p^{-2})^3} = \frac{\left\{ \zeta(2) \right\}^3}{\zeta(6)}$$ $$= \frac{35}{8}.$$ However, I have no clue about $p = 14k \pm 1$ and $p = 18k \pm 1;$ Can anyone help?",['number-theory']
4235586,Absolute irreducibility of a polynomial over a finite field,"Let $k$ be a positive integer, $\mathbb{F}_q$ be the finite field with $q$ elements. Consider the polynomial $f(X, Y) = 1 + X^k + Y^k + X^{2k} + Y^{2k}$ over $\mathbb{F}_q$ . How to verify whether it is absolutely irreducible (irreducible over $\overline{\mathbb{F}_q}$ ) or not? The context is the following:
I am considering existence of rational points over finite fields. In that direction I wanted to use Andre Weil's result as in ""Numbers of solutions of equations in finite fields"". In order to use Weil's results, I need to first prove absolute irreducibility of the polynomial.","['algebraic-number-theory', 'irreducible-polynomials', 'number-theory', 'algebraic-geometry', 'arithmetic-geometry']"
4235608,Why is $\frac{|1 - e^{2 \pi i \alpha N}|}{|1 - e^{2 \pi i \alpha}|} \leq \frac{\sin(\pi \alpha N)}{\sin(\pi \alpha)} \leq \frac{1}{||\alpha||}$?,"I came across this chain of inequalities in notes I am reading. $|\sum_{1 \leq n \leq N}e^{2 \pi i \alpha n}| \leq \frac{|1 - e^{2 \pi i \alpha N}|}{|1 - e^{2 \pi i \alpha}|} \leq \frac{\sin(\pi \alpha N)}{\sin(\pi \alpha)} \leq \frac{1}{||\alpha||}$ . Here $\alpha$ is a nonzero real number and $||\alpha ||$ is the distance from $\alpha$ to the nearest integer. The first inequality I was able to verify since it follows from the formula for the sum of a finite geometric series. I am not sure how to verify the 2nd and 3rd inequalities. I tried exploiting that the numerator and denominator of the 2nd term is a difference of two squares and using the relation $e^{i\theta} = \cos \theta + i\sin \theta$ but I didn't have success. No idea why the 3rd one is true either. A similar question which has been answered considers the final inequality, but I did not understand a few steps in the solution. I post it here for reference: $\left|{\sin(\pi \alpha N)}/{\sin(\pi \alpha)}\right| \leq {1}/{2 \| \alpha \|}$","['exponential-sum', 'analysis']"
4235613,Show that this curve is regular,"Given that $\beta$ is parametrised by arc length (with strictly positive curvature), show that the curve $\alpha (s) = \beta (s) - s \beta{'}(s)$ is a regular curve. To show that it's regular I have to show that $|| \alpha'(s) || > 0$ for all $s$ . To try to do this I found the derivative of $\alpha$ to be $$\alpha'(s) = \beta'(s) - \beta'(s) - s\beta''(s) = -s\beta''(s) =  -s\kappa(s)N(s)$$ where $\kappa$ and $N$ are the curvature and normal vector of $\beta$ . My problem is now where to go from here. It is never stated whether or not the interval being worked with contains $0$ or not. If it does, clearly $\alpha'(0) = 0$ so that $\alpha$ is not regular. Is my work up until this point correct, or is the question just wrong? The question was given to me by another student of the same course, so it may have been written down incorrectly.","['curves', 'geometry', 'differential-geometry']"
4235634,Determining $A = \{n\in\Bbb N : n\ge2$ and there exists a branch of $\sqrt[n]{f}$ in $D \}$,"Consider $D = \Bbb C\setminus(\{e^{i\theta} : -\frac\pi2\le\theta\le\frac\pi2\}\cup(-\infty,0])$ and $f(z) = z(z^2+1)$ . We know that $f$ can't be zero for any $z\in D$ . If we consider now $A = \{n\in\Bbb N : n\ge2$ and there exists a branch of $\sqrt[n]{f}$ in $D \}$ , what is $A$ exactly? I tried the following: Take $\Gamma$ any closed curve (the only ones that interest us are the ones that go around $\{e^{i\theta} : -\frac\pi2\le\theta\le\frac\pi2\}$ ) \begin{equation}
\frac1{2\pi i}\int_{\Gamma}\frac{f'(z)}{f(z)}\;dz = \frac1{2\pi i}\int_{\Gamma}\left(\frac1{z}+\frac1{z-i}+\frac1{z+i}\right)\;dz = n(\Gamma,0) + n(\Gamma,i) + n(\Gamma,-i) = 2n(\Gamma,i)
\end{equation} With this, the only possibility is that $A\subset2\Bbb N$ . Rewrite $f(z) = \frac{z(z-i)}{(z+i)}(z+i)^2 = g(z)h(z)$ , with $g(z) = \frac{z(z-i)}{(z+i)}$ and $h(z) = (z+i)^2$ . By the same procedure as before, we know that $\frac1{2\pi i}\int_{\Gamma}\frac{g'(z)}{g(z)}\;dz = 0$ , so there exists a branch for $\log(g)$ , which means that there exists a branch for $\sqrt[n]g$ for any $n\ge2$ . The branch for $\sqrt h$ is just $z+i$ , so we know that there exists at least a branch of $\sqrt f$ , so $\{2\}\subset A \subset 2\Bbb N$ . Can we say that $A$ is either $\{2\}$ of $2\Bbb N$ ? If we instead consider $D' = \Bbb C\setminus(\{e^{i\theta} : -\frac\pi2\le\theta\le\frac\pi2\}\cup[-1,2])$ , for that same $f$ we would get $\frac1{2\pi i}\int_{\Gamma}\frac{f'(z)}{f(z)}\;dz = 3n(\Gamma,0)$ , and by the same reasoning as before, $\{3\}\subset A'\subset3\Bbb N$ . Could the same result for $A$ be said for $A'$ (but with $3$ 's instead of $2$ 's now)?","['complex-analysis', 'branch-cuts']"
4235681,Axiom 3. Comprehension Scheme,"The following is from Kenneth Kunen's Book - Set Theory, An Introduction to Independence Proofs. The Comprehension Axiom is intended to formalize the construction of sets of the form $\{x:P(x)\}$ where $P(x)$ denotes some property of $x.$ Since the notion of property is made rigorous via formulas, it is tempting to set forth as axioms statements of the form $$ \exists y \forall x (x\in y \iff \phi)$$ where $\phi$ is a formula. Unfortunately, such a scheme is inconsistent by the famous Russell paradox : If $\phi$ is $x\in x,$ then this axiom gives us a $y$ such that $$ \forall x(x\in y \iff x\notin x),$$ whence $y\in y \iff y\notin y.$ Fortunately in mathematical applications it is sufficient to be able to use a property $P(x)$ to define a subset of a given set, so we Postulate Comprehension as follows. Axiom 3. Comprehension Scheme. For each formula $\phi$ without $y$ free, the universal closure of the following is an axiom $$\exists y \forall x(x\in y \iff x\in z \,\wedge \phi).$$ My Question : Why is it that Axiom $3$ can prevent Russel Paradox in comparison with the first proposed definition? Can we not argue that if $\phi$ is $x\notin x $ in Axiom $3$ and come up with Russel Paradox again? Why is the introduction of $z$ in Axiom $3$ helpful?","['elementary-set-theory', 'logic', 'axioms']"
4235732,Is this a correct formula for sin(a b)?,"First of all, recall the complex definitions of the two trigonometric functions: $$ \sin(x) = \frac{e^{ix}-e^{-ix}}{2i}, \space \cos(x) = \frac{e^{ix}+e^{-ix}}{2}. $$ Now, the formula for $\sin(2x)$ can be derived from these two. But so can it be extended by multipying each new result by $ 2\cos(2^{b}x) $ : $$ 2\sin x \cos x = \sin(2x), \space 4\sin x \cos x \cos(2x) = \sin(4x), \space ...
 $$ and so on. Therefore $$ 2^{b}\sin x \prod_{n=0}^{b-1}\cos(2^{n}x) = \sin(2^{b}x). $$ However, it might be possible - and this is the step that could be a fallacy - to rewrite the product so as to generalize the outcome to some real numbers. $$ 2^{b}\sin\ x \prod_{n=0}^{\infty}\frac{\cos(2^{n}x)}{\cos(2^{n}2^{b}x)} = \sin(2^{b}x) =>   \prod_{n=0}^{\infty}\frac{\cos(2^{n}x)}{\cos(2^{n}bx)} = \frac{\sin(bx)}{b\sin x}.$$ So far, is this true?
Pick x = 1 and notice this is false for b = $\frac{\pi}{4}$ . However, for x = 1 and b = $\pi$ or x = 1 and b = 1 the product goes to appropriate values. Can I now deduce from this the formula for the reciprocal of the product? $$ \prod_{n=0}^{\infty}\frac{\cos(2^{n}bx)}{\cos(2^{n}x)} = \frac{b\sin x}{\sin(bx)} $$ Instead of starting the derivation with sinx, I could have started it with $\frac{1}{\sin x}$ and then continued multiplying the denominator. Hence the invalidity of the formula for $\frac{\pi}{2^{n}}, \space n > 1 \space and \space n \in \mathbb{N}, \space$ cannot be explained by the first product's singularities, right? Is the idea anyhow correct, at least for some numbers?","['infinite-product', 'trigonometry']"
4235752,Ideal sheaf is quasi-coherent if and only if it is locally generated by sections,"I would like to prove the ""if"" part in the following equivalence: Given a scheme $X$ and an ideal sheaf $\mathcal{I}$ of $O_X$ , then $\mathcal{I}$ is quasi-coherent if and only if $\mathcal{I}$ is locally generated by sections. By locally generated by sections I mean that $X$ has a cover by open subsets $U_i,\ i\in I$ such that $\mathcal{I}_{|U_i}$ is generated by global sections (i.e. there exists a family of sections $(s_{\alpha})_{\alpha\in A}$ such that for all $x\in U_i,\ \mathcal{I}_x$ is generated as an $O_{X,x}-$ module by $s_{\alpha_x},\ \alpha\in A$ ) and that for all $i\in I$ . Let $U_i,\ i\in I$ be an open cover of $X$ such that $\mathcal{I}_{|U_i}$ is generated by global sections for all $i\in I.$ We can suppose that each one of the open subsets $U_i$ is affine, say $U_i\cong \operatorname{Spec}A_i$ . We want to prove that $\mathcal{I}_{|U_i}\cong \widetilde{I_i}$ where $I_i=\mathcal{I}(U_i).$ Let $D(f),\ f\in A_i$ be the basis of distinguished open subsets of $\operatorname{Spec}A_i$ . The restriction morphism $I_i=\mathcal{I}(U_i)\rightarrow \mathcal{I}(D(f))$ induces a unique morphism $\varphi(D(f)):\widetilde{I_i}(D(f))=I_{i_f}\rightarrow \mathcal{I}(D(f))$ defined by $\frac{s}{f^r}\mapsto \frac{s_{|D(f)}}{f^r}\ \forall s\in I_i.$ These morphisms $\varphi(D(f))$ define a morphism $\varphi:\widetilde{I_i}\rightarrow \mathcal{I}_{|U_i}$ . The corresponding morphisms on stalks are $\varphi_p:I_{i_p}\rightarrow \mathcal{I}_p,\ \frac{t}{s}\mapsto \frac{t_p}{s}\ \forall (t,s)\in I_i\times(A_i\setminus p)$ and they are surjective since $\mathcal{I}_{|U_i}$ is generated by global sections. I'm not sure about the injectivity though, if $\varphi(D(f))(\frac{s}{f^r})=0$ then $f^ms_{|D(f)}=0$ for some $m\in\mathbb{N}$ . $s_{D(f)}=s$ in $A_{i_f}$ as $\mathcal{I}$ is an ideal of $O_X$ so $s=0$ in $A_{i_f}.$ Is that right ? Thank you for your help!","['algebraic-geometry', 'schemes', 'sheaf-theory']"
4235768,Staver's identity relating $\sum_{k=1}^{n}\binom{2k}{k}\frac{1}{k}$ and $\sum_{k=1}^{n}\left(k\binom{n}{k}\right)^{-2}$,"I am looking for a proof of the following identity, very relevant for evaluating certain binomial sums $\pmod{p}$ : $$ \sum_{k=1}^{n}\binom{2k}{k}\frac{1}{k} = \frac{2n+1}{3}\binom{2n}{n}\sum_{k=1}^{n}\frac{1}{k^2\binom{n}{k}^2} \tag{1}$$ It is due to Tor B. Staver, Om summasjon av potenser av binomialkoeffisienten, Norsk Mat.
Tidsskrift 29 (1947) , but I did not manage to get access to the original article. I have attempted a few things, so far unsuccessfully: A) Exploit $$ \sum_{k=0}^{n}\frac{\lambda^k}{\binom{n}{k}}=(n+1)\sum_{r=0}^{n}\frac{\lambda^{n+1}+\lambda^{n-r}}{(r+1)(1+\lambda)^{n-r+1}}\tag{A}$$ which holds for any $\lambda > -1$ , as shown by Bala Sury through Euler's Beta function. B) Exploit a partial fraction decomposition of the reciprocal squared binomial coefficient, namely $$ \frac{1}{k^2\binom{n}{k}^2} = \sum_{r=0}^{k-1}\frac{\binom{k-1}{r}^2}{(n-r)^2}+2\sum_{r=0}^{k-1}\frac{\binom{k-1}{r}^2}{(n-r)}(H_{k-1-r}-H_{r-1}) \tag{B}$$ C) Exploit the Beta function and symmetry in order to get $$ \sum_{k=1}^{n}\frac{1}{k^2\binom{n}{k}^2}=\frac{1}{4^n n!^2}\iint_{(-1,1)^2}\frac{(1+x+y+xy)^{n+1}-(1+xy)^{n+1}}{x+y}\,dx\,dy \tag{C}$$ D) Apply Wilf's snake oil method, proving that the LHS and the RHS of (1) have the same OGF. Quite clearly $$ \sum_{k=1}^{n}\binom{2k}{k}\frac{1}{k} = 2[z^n]\frac{\log(2)-\log(1+\sqrt{1-4z})}{1-z}.\tag{D}$$ E) I am also able to show through the Beta function that $$\begin{eqnarray*} \sum_{r=0}^{n}\frac{1}{\binom{n}{r}^2}&=&2\frac{(n+1)^2}{(n+2)}\sum_{r=0}^{n}\frac{1}{(n-r+1)\binom{n+r+2}{r}}\\&=&2\frac{(n+1)^2}{(n+2)}[z^{n+1}]\left(-\log(1-z)\cdot\phantom{}_2 F_1\left(1,1;n+3;x\right)\right)\\&=&
2(n+1)^2 [z^{2n+3}](1-z)^{n+1}\left(\log(1-z)-\log^2(1-z)\right)\tag{E}\end{eqnarray*}$$","['summation', 'binomial-coefficients', 'combinatorics', 'special-functions']"
4235783,The rotational symmetry groups of the $5$-cell and the icosahedron are isomorphic. Is there a geometric proof of this fact?,"The rotational symmetry group of the $n$ -simplex (not permitting reflections) is always the alternating group $A_{n+1}$ . When $n=4$ , this coincides with the rotational symmetry group of the icosahedron and dodecahedron, which is also $A_5$ .  (I believe this is the only ""exceptional isomorphism"" between symmetry groups of regular polytopes in any dimension, i.e. one besides the isomorphism between dual polytopes.) Obviously one can prove this fact by showing that each group happens to be the alternating group on $5$ elements, but I am curious whether there is a geometric approach to exhibiting the isomorphism - some natural bijection between rotations of the $4$ -simplex and rotations of the icosahedron which shows they exhibit the same multiplicative structure, without determining what that structure is.","['polytopes', 'polyhedra', 'geometry', 'group-theory', 'symmetry']"
4235795,Continuous surjective function from closed interval to itself that fix only the endpoints,"I am trying to solve the following question Does there exist a continuous function from $[a,b]$ to $[a,b]$ which is onto and $a$ and $b$ are the only two fixed points? The function $f(x)= x$ has $a$ and $b$ as fixed points but it also has other fixed points. When $a=0$ and $b=1$ the function $f(x) = x^2$ works. So I tried the function $f(x) = (x-a)(x-b) + x$ but in general I found that it does not work. The function $f(x) = \log\left( \frac{e^b-e^a}{b-a}x+ \frac{be^a - ae^b}{b-a}\right)$ works for  several examples that I have checked but I am not able to prove in general. Given points $a<b$ ,  I can imagine a function inside the rectangle of vertices at $(a,a), (a,b), (b,a), (b,b)$ such that the curve joins $(a,a)$ and $(b,b)$ and does not pass through any points of the form $(x,x)$ but I am not able to construct explicitly.  Hints are appreciated.","['fixed-points', 'examples-counterexamples', 'real-analysis', 'continuity', 'functions']"
4235807,Proof that the monster group is simple.,"I was reviewing an abstract algebra book, and I get the comment that ""the monster group"" (the set of matrix of size $196833 \times 196833$ ) has no normal subgroups,I wanted to know if anyone knows any proof of this fact or bibliography of the subject, greetings! PD: The book is ""Abstract Algebra , T. Judson""","['simple-groups', 'group-theory', 'abstract-algebra']"
4235824,Identity $\arctan{\frac{1-\beta}{2\sqrt{\beta}}}=\arcsin{\frac{1-\beta}{1+\beta}}$,"In the book, Control Systems Engineering - frequency design, the author used the equality $$\phi_{max}=\arctan{\frac{1-\beta}{2\sqrt{\beta}}}=\arcsin{\frac{1-\beta}{1+\beta}}$$ Is this some famous identity?
Am I seriously missing out since I've never seen this formula before. Edit:
Using the formula on the comments: $$\sin{\arctan{\theta}}=\frac{x}{\sqrt{1+x^2}}$$ Let $\theta= \frac{1-\beta}{2\sqrt{\beta}}$ $$\arctan{\theta}=\arcsin{\frac{\theta}{\sqrt{1+\theta^2}}}
$$ $$\arctan{\theta}=\arcsin{\frac{1-\beta^2}{\beta^2+2\beta+1}}
$$ Thus $$\arctan{\frac{1-\beta}{2\sqrt{\beta}}}=\arcsin{\frac{1-\beta}{1+\beta}}
$$",['trigonometry']
4235831,Proving an inequality for operators.,"Let $\mathbb P_n$ be the space of all $n \times n$ self-adjoint positive definite matrices. Consider the function $\varphi: \mathbb P_n \longrightarrow \mathbb R$ defined by $$\varphi (A) = -\text {tr}\ (A \log A).$$ Show that for all $t \in (0,1)$ $$\varphi ((1 - t) A + t B) \leq (1 - t) \varphi (A) + t \varphi (B) - \eta (t,1-t)$$ where $\eta (t,1 - t) = t \log (t) + (1 - t) \log (1 - t).$ I know that $\varphi$ is operator concave. But I don't have any idea as to how to bound $\varphi$ from above. Could anyone please give me some hint? Thanks a bunch!","['self-adjoint-operators', 'functional-analysis', 'matrix-analysis', 'convex-analysis', 'positive-definite']"
