question_id,title,body,tags
3390668,Can $4\cdot n!-4n+1$ be a perfect square when $n>4$?,"When $n=4,4\cdot n!-4n+1=4\times24-4\times4+1=9^2.$ I wonder if $4\cdot n!-4n+1$ can be a perfect square when $n>4$ ? I searched $n$ from $5$ to $10000$ but no qualified number was found. I know that: (1) $n$ is even, since $-4n+1 \equiv1 \mod 8$ ; (2) if $p|4n-1$ and $p\leq n/2$ then $p^2|4n-1$ , here $p$ is prime; (3) if $p \not|4n-1$ and $p\leq n$ then $\left (\frac{-4n+1}{p}\right) = 1$ , here $p$ is prime, $\left (\frac{a}{p}\right)$ is Jacobi Symbol.","['number-theory', 'diophantine-equations']"
3390700,What's the Point That Rudin is Trying to Make After Theorem 3.29?,"After proving Theorem 3.29 in page 63 of Rudin's ""Principles of Mathematical Analysis"" , Rudin says that ""this procedure may evidently be continued"", what procedure is he talking about exactly? Moreover, I don't really understand the point he is trying to make after that before going to define the number $e$ . It would really be helpful for me if someone could explain more in detail and intuitively what he is trying to say.","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
3390722,"Are there two dependent, but uncorrelated random variables $X,Y\sim \mathcal {N}(0,1)$ such that their sum $X+Y$ is normal, i.e. $\mathcal {N}(0,2)$?","Context : The question comes from another question I saw regarding the characterization of Brownian motion : does it hold that for all $t>s\ge 0$ $$\operatorname{Cov}(B_s, B_t-B_s) = 0\ \ \ \ \Longrightarrow\ \  B_s \perp \!\!\! \perp B_t-B_s \text{ (independent})$$ provided that $B_0 = 0$ and $B_t-B_s \sim \mathcal N(0,t-s)$ ? The asker did not assume that $(B_t)_{t\ge 0}$ is a Gaussian process. So I thought the answer is ""no"" because in general mere $\operatorname{Cov}(B_s,B_t-B_s) = 0$ does not imply $B_s \perp \!\!\!\perp B_t-B_s$ unless $(B_s,B_t-B_s)$ is jointly normal. (Some counterexamples can be found here in Wikipedia.) In the linked page, we can find various pairs of dependent normal random variables $X,Y \sim \mathcal{N}(0,1)$ with $\operatorname{Cov}(X,Y) = 0$ . Also we can generate many other examples using similar ideas and techniques. However, I have failed to find an example where $X+Y$ is also normally distributed, which must hold in the setting of the original question, i.e. $$
\underbrace{B_1}_{ =X \sim \mathcal N(0,1)} + \underbrace{B_2-B_1}_{ =Y\sim \mathcal N(0,1)} = \underbrace{B_2}_{=X+Y\sim \mathcal N(0,2)}.
$$ So my question is: are there uncorrelated dependent r.v.'s $X,Y \sim \mathcal {N}(0,1)$ such that their sum $X+Y$ is also distributed normally, i.e. $\mathcal N(0,2)$ ? If there are, can we construct an example explicitly? My thought : Essentially the given condition is providing information about the moments of all orders $$
E[X^n],\ \, E[Y^n],\ \  E[(X+Y)^n]\qquad \forall n\ge 1.
$$ However, we cannot pin down, for instance, the values of $E[X^2Y]$ or $E[XY^2]$ using this information only. This suggests that the given condition does not determine the distribution of $(X,Y)$ uniquely. So my guess is that the answer is affirmative. I've also tried an abstract approach to find a characteristic function (Fourier transform or a positive-definite function equivalently) $\hat \mu(s,t) = \int_{\mathbb R^2} e^{i(sx+ty)}d\mu(x,y)$ satisfying $d\mu(x,y) \neq (2\pi)^{-1}e^{-(x^2+y^2)/2}dxdy$ $$
\hat\mu (t,0) = \hat\mu(0,t) = e^{-t^2/2},\quad \hat\mu(t,t) = e^{-t^2}\qquad\forall t\in\mathbb R,
$$ but was in vain.","['measure-theory', 'probability-distributions', 'probability-theory', 'probability']"
3390723,Is there any way to calculate harmonic or geometric mean having probability density function?,"I have probability density of function of some data (it's triangular.) How can I calculate harmonic or geometric mean of the data? I know for calculating arithmetic mean of a variable like $K$ , I have to calculate $\int_{0}^\infty K.P(K)dK$ but I don't have any ideas for other types of averaging methods (Harmonic and geometric).","['statistics', 'harmonic-functions', 'probability-distributions', 'probability', 'density-function']"
3390736,Is this a new characteristic function for the primes?,"I recently found the following function, that returns $1$ if its argument is a prime number, and $0$ otherwise. Let $\chi_{\mathbb{P}}: \mathbb{Z}\rightarrow\{0,1\}$ for $n>4$ be defined as follows: $$
\chi_{\mathbb{P}}(n)=\frac{(-1)^{\frac{2}{n}\Gamma(n)}-1}{(-1)^{-\frac{2}{n}}-1}.
$$ Then $\chi_{\mathbb{P}}$ is the characteristic function of the primes. Note that here $\Gamma$ denotes the usual gamma function. The first few values, starting from $n=5$ are the following: 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, ... For convenience I include the relevant Mathematica code here: Table[((-1)^(2 Gamma[n]/n) - 1)/((-1)^(-2/n) - 1), {n, 5, 20}] Is this already known? I have not found references to it in literature.","['number-theory', 'characteristic-functions', 'prime-numbers', 'reference-request']"
3390788,How could outcomes be equally likely?,"Hi I've just started learning probability but I have some questions.
When sloving a problem like ""If 3 balls are “randomly drawn” from a bowl containing 6 white and 5 black balls, what is the probability that one of the balls is white and the other two black?"" , they assume outcomes in the sample space are equally likely. However when considering picking 2 balls from 2 white and 1000000 black balls, how could (w,w) (w,b) be (b,b) (unordered) considered equally likely? 
In my opinion (b,b) are much more likely than (w,w) because there are way much black balls than white balls. 
Of course their numbers are considered when solving the probability, I still wonder   is it valid that (w,w) (w,b) be (b,b) be assumed equally likely. Sorry for my bad english because i'm not a native speaker. Please excuse me. Thank you. -edited In a textbook, it says that ""When the experiment consists of a random selection of k items from a set of n items, we have the flexibility of either letting the outcome of the experiment be the ordered selection of the k items or letting it be the unordered set of items selected. In the former case we would assume that each new selection is equally likely to be any of the so far unselected items of the set, and in the latter case we would assume that all (n k) possible subsets of k items are equally likely to be the set selected ."" I'm curious about emphasied part.","['combinations', 'probability']"
3390796,"If $\alpha$, $\beta$, $\gamma$ and $\delta$ be the roots of the polynomial $x^4+px^3+qx^2+rx+s$, prove the following.","If $\alpha$ , $\beta$ , $\gamma$ and $\delta$ be the roots of the polynomial $x^4+px^3+qx^2+rx+s$ , show that $$(1+\alpha^2)(1+\beta^2)(1+\gamma^2)(1+\delta^2)=(1-q+s)^2+(p-r)^2.$$ I have tried putting \begin{align}P(1)&=(\alpha-1)(\beta-1)(\gamma-1)(\delta-1)=1+p+q+r+s\\P(-1)&=(\alpha+1)(\beta+1)(\gamma+1)(\delta+1)=1-p+q-r+s.\end{align} Then \begin{align}P(1)P(-1)&=(\alpha^2-1)(\beta^2-1)(\gamma^2-1)(\delta^2-1)\\&=(1+p+q+r+s)(1-p+q-r+s)=(1+q+s)^2-(p+r)^2\end{align} Somehow it does not match the statement given.","['algebra-precalculus', 'roots', 'polynomials']"
3390798,Find coefficient of $x^ky^m$ in $(x+y-1)^n$,"Find coefficient of $x^ky^m$ in $(x+y-1)^n$ I have issue when in polynomial we have $-1$ , or another constant. I will explain,
if we have to find coefficient $x^ky^m$ in $(x+y)^n$ then it is easy, according to multinomial theorem. It is $\binom{n}{k,m}$ .
if we take another example without constant in polynomial, for example to find coefficient of $x^2y^3z^4$ in $(x+y+z)^9$ , it is again, according to multinomial theorem $\binom{9}{2,3,4}$ . However, this did not works when we have constant in polynomial, I don't understand how to make this correct here: $\binom{n}{k,m}$ in $(x+y-1)$ and not $(x+y)$ .","['combinatorics', 'discrete-mathematics', 'multinomial-coefficients']"
3390870,"Given a Fibonacci number , find the next Fibonacci number","The Fibonacci sequence is $0, 1, 1, 2, 3, 5, 8, 13, 21, 34,\ldots$ , where each term after the first two is the sum of the two previous terms. Can we find the next Fibonacci number if we are given any Fibonacci number? For example, if $n = 8$ then the answer should be $13$ because $13$ is the next Fibonacci number after $8$ .","['fibonacci-numbers', 'combinatorics', 'discrete-mathematics', 'sequences-and-series']"
3390876,Series for $\log 3$,"I have the following series: $$\sum_{k=0}^{\infty} \left(\frac{1}{3k+1}+\frac{1}{3k+2}-\frac{2}{3k+3}\right)$$ Wolfram says this is just $\log 3$ . I have been trying to figure out how this works purely through series manipulation (without integrals etc.). I've tried splitting it up into several series but nothing seems to fit nicely because the pattern is 3-period. The series I know for $\log$ which tried first was: $$\log(1+x)=\sum_{k=1}^{\infty} \frac{(-1)^{k+1}}{k}x^k$$ perhaps with $x=-\frac{2}{3}$ , but this introduces powers which don't seem natural to derive from the original expression. Any help would be great.","['logarithms', 'sequences-and-series']"
3390903,What is Gian-Carlo-Rota really saying about DE courses? (For students like me),"I am a bit confused (and entertained) after reading TEN LESSONS I WISH I HAD LEARNED BEFORE I STARTED TEACHING
DIFFERENTIAL EQUATIONS by
Gian Carlo Rota For example, he writes The most preposterous items
  are found at the beginning, when the text (any text) will list a number of disconnected tricks that
  are passed off as useful, such as exact equations, integrating factors, homogeneous differential
  equations, and similarly preposterous techniques. Since it is rare – to put it gently – to find a
  differential equation of this kind ever occurring in engineering practice, the exercises provided
  along with these topics are of limited scope: as a matter of fact, the same sets of exercises have
  been coming down the pike with little change since Euler He goes on to mock many other so called ""tricks"" that he describes as jokes. As a student, these tricks are all I have. Can someone explain what he means. Is it that, in the real world (whatever that is), the equations are never suitable for these bags of tricks, and therefore we should be learning about numerical methods or analyzing phase portraits, or high performance computing? I must admit, I am struggling in general to understand how people create differential equations out in the wild. How does one transition from bags of tricks to understanding DE's out in the wild so to speak?",['ordinary-differential-equations']
3390937,"When the product of a multi-digit integer and its mirror is a palindrome, can the original number have digits greater than $2$?","I am reposting a question I posted on r/mathematics . It was suggested I ask it here. My son was jotting down some multiplications for school and asked me if there were many numbers that, when multiplied by their mirror image, resulted in a palindromic number (e.g. 221 x 122 = 26962). I made a quick Python script to test this and found the results rather surprising. For 3-digit numbers, there are 11 results. For 4-digit number, there are 23. The number of positive results doubles approximately with each addition of a digit, reaching 642 results with 9-digit numbers and 1118 results with 10-digit numbers. As you can see from the table below, the ratio of 2 seems to decrease with every iteration after the 6th. This is the longest number we could test because calculating time increases exponentially by a factor of approximately 10, reaching 3 hours for the last example. What I find interesting is that in all of the above results, with no exception, the factors are invariably composed of zeros, ones and twos. There's never anything else. For example: 2100110011 x 1100110012 = 2310352049402530132. I asked a mathematician friend — not remotely involved with number theory or arithmetic – and he said it might be related to ""carry digits"" messing things up. It's true that for 1-digit numbers, excluding the trivial zero, there are only 3 possible examples (1, 2 and 3) before the symmetry breaks (4 x 4 is 16 which isn't palindromic). But when multiplying huge 10-digit numbers you get tons of ""carry digits"" as can clearly be seen from the results: these can include any digit as seen in the example above. It does seem to have some impact, though. For a test for n digits, all the multiplication results have the exact same number of digits, which is always 2n-1. e.g. 4-digit numbers always give 7-digit results. I am sure there must be a deep reason for never seeing digits above 2 in the factors, but for the life of me I can't understand what it is. Like I wrote I've only tested this up to ten digits, so my conclusion could be wrong. Any insights are welcome. I'm not a mathematician, so please forgive me if this seems trivial to you. I hope the table below is clear. Thanks a lot. digits  digits  number       ratio       calc
in      in      of           with        time
factors results palindromes  previous
1       1       3       
2       3       4            1,333          0,001
3       5       11           2,750          0,001
4       7       23           2,091          0,011
5       9       46           2,000          0,110
6       11      93           2,022          1,081
7       13      185          1,989         10,973
8       15      353          1,908        108,295
9       17      642          1,819       1132,420
10      19      1118         1,741      11227,896 And BTW the script is below in case someone cares. I'm not a programmer either, so I wouldn't know how to multithread or otherwise optimize this, but it's a bit besides the point I think — the pattern here *does* seem to confirm itself, although of course it's no proof. def mirror(length):
    print('Working...')
    count = 0
    start = time.time()
    for i in range(1, pow(10,length)):
        a = str(i).zfill(length)
        b = a[::-1]
        result = str(int(a) * int(b))
        if (result == result[::-1]):
            print(a, b, result)
            count += 1
    end = time.time()
    print(f'-----------\nSize : {length}\nTotal  : {count}\nTime  : {round(end-start, 3)}')
    
mirror(6)","['number-theory', 'arithmetic', 'programming', 'palindrome']"
3391066,finding limit for a two variable function,"How can I calculate $$ \lim_{(x,y)\to (1,-1)}  \frac{x+y}{1+xy}$$ I want to show that this function has a maximum value of 1. I found the maxima are at (±1,±1) and for $ (1,1)$ and $(-1,-1)$ , $f(x,y)= 1$ . I am stuck on the rest two. $$$$ Edit: I will explain a little more about the situation, in special relativity, velocity addition has the form $$ \overline{v} = \frac{w+V}{1+\frac{wV}{c^2}} $$ where velocities have usual meanings. So, when a frame is moving away from you with a speed of $c$ and light flashes towards you in the opposite direction, you should get $$ \overline{v} = -c $$ because the speed of light is same in all the frame of references.  How can we justify that from this equation? Essentially I have to prove that $ \overline{v}$ never exceeds $c$","['limits', 'maxima-minima', 'functions']"
3391098,Discrete Probability Lottery Ticket Question,"I was wondering if someone could help with this type of question: Suppose that you buy a lottery ticket containing k distinct numbers from among{1,2,...,n}where 1≤k ≤n. To determine the winning ticket, k balls are randomly drawn without replacement from a bin containing n balls numbered 1,2,...,n. What is the probability that at least one of the numbers on your lottery ticket is among those drawn from the bin? The way that I approached this problem is by saying that: P(having at least 1 number) = 1 - the probability of not having any numbers chosen But I cannot seem to be able to figure out exactly the probability of not having a single number chosen. I know for example lets say k=3 and n=10, then probability first number isnt chosen is 7/10, then for the second number to not be chosen I think it is 6/9, and then 5/8 and so on. Then I would just multiply these numbers. I'm not sure if there is an easy way to write this, or an even easier way to approach this question so any help would be much appreciated!","['contest-math', 'discrete-mathematics', 'probability']"
3391222,Can large numbers be expressed as sums of large primes?,"A well known result states that every integer $n \geq 7$ is the sum of distinct primes (we allow sums consisting just of one summand). Another way of writing this is that each $n \geq 7$ can be written as $n=\sum_{k \geq 1} a_kp_k$ where each $a_k \in \{0,1\}$ and $p_k$ is the $k$ th prime. I'm wondering if the following result is known: For every $M \in \mathbb{N}$ there exists an $N_m \in \mathbb{N}$ such that every integer $n>N_m$ is the sum of distinct primes larger than $M$ . This theorem is suggested in this paper, but not explicitly stated. For instance, they prove the special case that every integer $>57$ is the sum of distinct primes $ \geq 13$ . However, the paper proves an important lemma which may be helpful in proving the result. I would be surprised if this problem admits a quick and easy answer, so this is more of a reference request than anything else. Although if you have a solution, you're free to share it! $\textbf{Edit}$ : The paper is behind a paywall (I initially overlooked this since I have university access by default) . The lemma is the following, quoted verbatim. Note sums of the form $\sum_{i=j}^{k} a_i$ for $j>k$ are interpreted as $0$ . $\textbf{Lemma}$ Let $\{S_i\}_{i=1}^{\infty}$ be a sequence of positive integers, and assume there exist fixed integers $r \geq 0$ and $K_0 \geq 0$ such that $(i)$ $S_{n+1} \leq S_{r+1} + \sum_{i=r+1}^{n} S_i$ for all $n \geq r$ and $(ii)$ each integer $N$ in the range $K_0 \leq N < K_0 + S_{r+1}$ can be expressed as $\sum_{i=1}^{r} a_iS_i$ for $a_i \in \{0,1\}$ . Then any integer $N$ satisfying $K_0 \leq N < K_0 + S_{r+1} + \sum_{i=r+1}^{n} S_i$ for some $n \geq r$ is expressible in the form $N=\sum_{i=1}^{n} a_iS_i$ for $a_i \in \{0,1\}$ . $\textbf{Sublemma}$ The following ""sub-lemma"" is used to verify the first hypothesis $(i)$ in the above lemma. Namely, if a sequence $\{S_i\}_{i=1}^{\infty}$ of positive integers satisfies $S_{n+1} \leq 2S_n$ for all $n \geq r+1$ , then the hypothesis $(i)$ of the above lemma holds for all $n \geq r$ . $\textbf{An illustration of the lemma's power:}$ Every $n>57$ is the sum of distinct primes each of which is at least $13$ . $\textbf{Proof:}$ Let $S_i$ be defined by $S_1 = 13, S_2 = 17, S_3 = 19, ...$ i.e., the sequence of primes $ \geq 13$ . We claim that this sequence satisfies the hypotheses for lemma $1$ with $K_0=58, r=10$ . To verify the first hypothesis $(i)$ , this is an easy application of the sublemma and Bertrand's postulate. We now verify the second hypothesis. Note that $S_{r+1} + K_0 = 111$ . We must verify that each integer $n \in [58, 110]$ is the sum distinct elements in the set $\{S_1, S_2, \cdots, S_{10}\}$ ; in other words, each $58 \leq n \leq 110$ is $n=\sum_{i=1}^{10} a_iS_i$ for $a_i \in \{0,1\}$ . This is trivial, and can be done by hand or computer. Hence, the conclusion to lemma $1$ applies to our sequence $S_i$ . Since $S_i \to \infty$ , it follows that given any $N \geq 58$ , there exists a large $\ell$ such that $N < K_0 + S_{r+1} + \sum_{i=r+1}^{\ell} S_i$ , and hence we have that $N = \sum_{i=1}^{\ell} a_iS_i$ for $a_i \in \{0,1\}$ , as desired. The reason this approach may not be ""scalable"" to the general problem lies in the fact that we require computational verification for a certain range.","['number-theory', 'reference-request']"
3391225,What's the standard term for a (connected?) plane set whose intersection with any horizontal line is either empty or an interval?,"What is the term for a (connected?) set $S$ of the plane $\mathbb{R}^2$ such that the intersection of $S$ with every horizontal line $\ell_{b}: y=b$ is either empty, or an interval of the line $\ell_b$ ?","['geometry', 'terminology']"
3391251,"Find $f'$ for the function $f(x,y) = \int_{a}^{x + y}g$","Find $f'$ for the function $f(x,y) = \int_{a}^{x + y}g$ , where $g:\mathbb{R} \to \mathbb{R}$ is continuous. My question has to do more with the concept than solving this specific expression. How do I interpret the composition? This exercise appears in my class with regards to applying the chain rule.  I remember that it has to do with the fundamental theorem of calculus. Mechanically I know that the answer will be: $$f(x,y) = \int_{a}^{x + y}g(z) dz \Rightarrow f'(x,y) = g(x+y) \cdot [1,1]$$ but I'm not seeing how the composition of the functions comes about. Is it an expression of the sort: $$ F(g(z)) =  \int_{a}^{x+y}g(z),\  \text{where}\ F = \int_{a}^{x + y}\phi$$ As you can see, all the symbols and variables are confusing me a bit. Could somebody give me a bit of an explanation? Edit: Possible Interpretation One of the things I have been working on understanding is performing the differentiation process in matrix/ vector notation. With that being said this expression is a composition of the functions: $$F(u) =\int_{a}^{u}g(t)dt\ \text{where} \ F:\mathbb{R} \to \mathbb{R}\ \text{and} \\ \ u(x,y) = x + y, \ \text{where}\ u: \mathbb{R}^{2} \to \mathbb{R} \\ \Rightarrow (F \circ u)(x,y) = F(u(x,y))$$ So when the question asks for $f'$ , what is being asked for is $$F'(u) = DF(u(x,y)) \cdot Du(x,y) \\  DF(u(x,y)) = [D_{1}F] = [g(u)] \\ Du(x,y) = [D_{x}u(x,y), D_{y}u(x,y)] = [1,1] \\ \therefore \ DF(u(x,y)) \cdot Du(x,y) = g(u) \cdot [1,1] \\ = [g(u), g(u)] \ \text{since} \ u = x + y \\ = [g(x+y), g(x+y)] $$ From this what I have to be aware of is that the composition involves the integrand and one of the limits of integration. The function $g(t)$ is just a part of the integrand function.","['calculus', 'derivatives', 'real-analysis']"
3391271,How many pairs of students (with one working alone) are possible if there are $21$ students in a class?,"My solution was to just add up binomials where you choose $2$ people from a smaller and smaller pool of people (since with every choice $2$ people become unavailable): $$\sum_{k=1}^{10} {{2k+1}\choose2}$$ was my answer (it evaluates to $825$ ). But according to my textbook the correct answer is $21! / (2^{10} * 10!)$ , which is far from $825$ ... not only do I have no idea why, but I can't see what's wrong with my answer. Just for reference, the question in full is: There are $21$ students in a biology class. The students must pair up to work as lab partners, but, of course, one student will be left over to work alone. In how many ways can the students be paired up?"" Any help is appreciated.","['binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
3391328,Find all $f:\mathbb{N} \to \mathbb{N}$ such that $f(n) + f\big(f(n)\big) = 6n$ for every $n\in\mathbb{N}$.,"Find all functions $f:\mathbb{N} \to \mathbb{N}$ such that $$f(n) + f\big(f(n)\big) = 6n$$ for every $n\in\mathbb{N}$ . I don't know any formal way to solve this. I just tried to use some functions of the form $f(n) =\alpha n$ and found that $f(n) = 2n$ is the answer because $2n + 2(2n) = 6n$ I don't know how to correctly solve this type of questions and just used simple try-error method, So any help on solving this in mathematical way is appreciated. I am not also sure about the tags I selected. It looks like a number theory problem to me that is also related to recurrence relations. Sorry if the tag is inappropriate.","['contest-math', 'functional-equations', 'elementary-number-theory', 'recurrence-relations', 'functions']"
3391335,Getting contradiction for $\left[\begin{array}{rrr|r}1&1&1&3\\2&-1&1&0\\-3&5&7&7\end{array}\right]$,"I'm trying to solve the following matrix and I'm getting a contradiction yet I know there is a solution as I've graphed it and the book has the same answer.  What am I doing wrong? $$\left[\begin{array}{rrr|r}1&1&1&3\\2&-1&1&0\\-3&5&7&7\end{array}\right]$$ Now the book gets (1,2,0) as an answer.  And here is my attempt. Replace R2 with -2R1+R2 and I also replace R3 with 3R1+R3: $$\left[\begin{array}{rrr|r}1&1&1&3\\0&3&1&6\\0&8&10&16\end{array}\right]$$ Then I multiply R2 by 1/3 as to get a 1 in the leading term: $$\left[\begin{array}{rrr|r}1&1&1&3\\0&1&1/3&2\\0&8&10&16\end{array}\right]$$ Now I want to eliminate the leading term of R3 so I replace it with -8R2+R3 and get: $$\left[\begin{array}{rrr|r}1&1&1&3\\0&1&1/3&2\\0&0&22/3&0\end{array}\right]$$ And so 22/3 = 0 is a contradiction.  What the heck am I doing wrong?","['matrices', 'linear-algebra']"
3391345,Problem of values in a $3\times3$ magic square,"We give the following integers : $$\left[\begin{matrix}
-2 & x_2 & x_1 \\
17 & -8 & 9 \\
3 & x_3 & x_4
\end{matrix}\right]$$ The magic constant is equal to $18$ then I found two possibilities for the four unknown integers. The first weird thing is the fact that $-8$ is at the center of the square... What value do I have to change to get only one solution? Thanks in advance!","['matrices', 'magic-square', 'arithmetic', 'integers']"
3391366,Find $f'(x)$ given that $f(x) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)d\theta$,"Find $f'(x)$ given that $f(x) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)\,d\theta$ , where $|x| < 1$ Recently I asked a question that involved the theory in dealing with a question like this: Find $f'$ for the function $f(x,y) = \int_{a}^{x + y}g$ Using what I got from that question I went about solving this expression: $$\text{Define} \ F(u) = \int_{0}^{\frac{\pi}{2}}g(\theta)\,d\theta,\ \text{where}\ g(\theta) = \log(1 - x^{2}\cos^{2}\theta)\ \text{where}\ F:\mathbb{R} \to \mathbb{R} \\ \text{Define}\  u(x) = \frac{\pi}{2}, \ \text{where}\ u:\mathbb{R} \to \mathbb{R}$$ Using the ideas covered previously: $$(F \circ u)(x) = F(u(x)) = \int_{0}^{\frac{\pi}{2}}\log(1 - x^{2}\cos^{2}\theta)\,d\theta $$ THerefore: $$D(F \circ u)(x) =  DF(u(x)) \cdot Du(x) \\
\text{where}\ DF(u(x)) = \log(1-x^{2}\cos^{2}\theta) \\ \text{and}\ Du(x) = 0 \\ \therefore DF(u(x)) \cdot Du(x) = 0 $$ And I know/feel this is not right. I feel I'm supposed to treat the things within the log function in some way, but according to how I was looking at them in my previous question they are all dummy variables with no importance. Where am I going wrong in my examination of the expression ?","['integration', 'calculus', 'derivatives', 'real-analysis']"
3391388,Coefficients of the power series solution for $f'(x)^2+f(x)^4=1$,"I am attempting to calculate the power series coefficients of the solution to the differential equation $$f'(x)^2+f(x)^4=1\qquad f(0)=0.\tag{1}$$ I am trying to do so, because $f(x)$ is the inverse function of $$g(x)=\int_0^x\frac{dt}{\sqrt{1-t^4}},$$ i.e. $f\circ g(x)=g\circ f(x)=x$ .
I already know that $f$ can be represented in terms of the Jacobi elliptic function $\mathrm{sn}$ as $$f(x)=\mathrm{sn}(x|-1),$$ and I was hoping to understand a little more about elliptic functions like this through the lens of power series. I have made a little progress, I think. First off, let $f'(x)^2=\sum_{n\ge0}a_nx^n$ and $f(x)^4=\sum_{n\ge0}b_nx^n$ . Then $$\sum_{n\ge0}(a_n+b_n)x^n=1$$ so $a_0+b_0=1$ and $a_n=-b_n$ for $n>0$ . Then let $f(x)=\sum_{n\ge0}f_nx^n$ . Our aim is to find $f_n$ , or at least evaluate the first few $f_n$ . from $(1)$ , it's pretty easy to see that $f_0=0$ and $f_1=1$ . Then we see, from the Cauchy product, $$f'(x)^2=\sum_{n\ge0}x^n\sum_{k=0}^{n}(k+1)(n-k+1)f_{k+1}f_{n-k+1}$$ and $$f(x)^4=\sum_{n\ge0}x^n\sum_{k=0}^{n}q_kq_{n-k}$$ where $$q_n=\sum_{r=0}^{n}f_rf_{n-r}.$$ Thus for $n>0$ , $$\sum_{k=0}^{n}\left(q_kq_{n-k}+(k+1)(n-k+1)f_{k+1}f_{n-k+1}\right)=0.$$ Is there an easier way to calculate these coefficients?","['ordinary-differential-equations', 'elliptic-functions', 'sequences-and-series', 'power-series', 'elliptic-integrals']"
3391393,"How many walks are there from $(0,0)$ to $(N, r)$ on $\mathbb Z^2$ along diagonals?","Suppose we are looking at all paths from $(0,0)$ to $(N,r)$ using only the steps $(1,\pm 1)$ while always staying between the horizontals $y=r-1$ (except for the last step) and $y=-l$ . Visualization : Roughly speaking, I am looking at all paths consisting only of unit diagonals up/down between the two blue points while always staying between the two horizontal lines. The green line indicates the last step, which is the only possible last step since we have to stay below the red line in all prior steps. My question. How many such paths are there for any given $N,l$ and $r$ ? Remarks. This question popped up while I was looking at a Bernoulli path based on this MSE question . If $F(N, l,r)$ denotes the number of such paths, then $F$ satisfies $$F(N,l,r)=\begin{cases}1, &\text{ if } \min(N,l)\geq0 \land N=r
\\ 0, &\text{ if } \min(N,l,r)<0\lor (N\geq 1 \land r\le 0)\lor r >N\\
F(N-1, l-1,r-1)+F(N-1,l+1,r+1), &\text{ otherwise}
\end{cases}.
$$ Is there any good way to get a simplified expression out of this? If $l\geq \frac{N-d}2$ , then the triangular sequence $$\begin{matrix}
F(1,l,1) \\
F(2,l,1) & F(2,l,2)  \\
F(3,l,1) & F(3,l,2) & F(3,l,3) \\
\dots & \dots & \dots & \ddots
\end{matrix}$$ is simply the Catalan triangle with $0$ s .","['combinatorial-geometry', 'combinatorics']"
3391399,Computing Lesbegue Integral Using the definition of super level set,"Use the definition of the Lebesgue integral in terms of level sets to compute: $$\int_{\mathbb{R^2}} e^{-\mid x\mid^2} dx$$ Also, I am required to verify my answer using polar coordinates. My Approach Defining a super level set for $f(x)$ $$s_{f}(t):=\{x \in \mathbb{R^2}:f(x) \gt t\}$$ After solving I got these cases for $s_{f}(t)$ $$s_{f}(t)=\begin{cases}
\mathbb{R^2}, t \leq 0\\
0 \lt \mid x \mid \lt \sqrt{ln\mid 1/t \mid}, 0 \lt t \leq 1\\
\phi,t \gt 1
\end{cases}$$ My question: How to proceed forward and evaluate the Lebesgue Integral? I know that the Lebesgue Integral formula that is : $$\int_{\mathbb{R^2}} f d\mu = \int_{0}^{\infty} \mu(s_f(t)) dt$$ $\mu$ is the measure.","['measure-theory', 'lebesgue-measure']"
3391419,reducing $\cos{x} + 2\sin{x}$ to $R\cos{x - a}$ finds strange phase-shift,"So, I'm analysing the function $\cos{x} + 2\sin{x} (*)$ and determining whether it is convertible to the single function $R\cos{x - a}$ for $a \in (0, \frac{\pi}{2}); R > 0$ ; so I begin by observing the maximum/minimum of $(*)$ and determining its max./min. at $\pm\sqrt{5}$ , so then I apparently have that $R = \sqrt{5}$ as the radius/magnitude of the corresponding function. Next, I seek to find the phase-shift difference between my $\sqrt{5}\cos{x}$ and $\cos{x} + 2\sin{x}$ , so, I take two reference points at their relative roots, i.e. $\cos{x} + 2\sin{x} = 0 \iff x = -\arctan{\frac{1}{2}} + n\pi, n \in \mathbb{Z}$ , and respectively, $\sqrt{5}\cos{x} = 0 \iff x = \frac{\pi}{2} + n\pi, n \in \mathbb{Z}$ . So I figure that if I take the x's matching with the roots of the functions in the same period, then I should have the phase-shift and my function mutation should be done. I take $n = 1$ for both functions, so I have $x_{\sqrt{5}\cos{x}} = \pi + \frac{\pi}{2}$ and $x_{\cos{x} + 2\sin{x}} = \pi - \arctan{\frac{1}{2}}$ . Then I take their absolute difference, and theoretically this should be the shift. So, $|x_{\sqrt{5}\cos{x}} - x_{\cos{x} + 2\sin{x}}| = |[\pi + \frac{\pi}{2}] - [\pi - \arctan{\frac{1}{2}}]| = |\frac{\pi}{2} + \arctan{\frac{1}{2}}| \approx 2.034$ .
So immediately this evaluation seems incorrect; 2.034 radians is well over half a period, and according to plots of these functions the difference does not seem this drastic. Observe the plot for $y = \sqrt{5}\cos{x - (\frac{\pi}{2} + \arctan{\frac{1}{2}})}$ and $y = \cos{x} + 2\sin{x}$ Evidently incorrect, so I then plot $x = -\arctan{\frac{1}{2}}$ and $y = \cos{x} + 2\sin{x}$ to actually determine where this x-value was being matched: For some reason, the $-\arctan{\frac{1}{2}}$ was in a completely separate period relative to $\sqrt{5}\cos{x}$ at $n=1$ . And here my question(s) arises: why is this root observed in a separate period? How can I find where the start of these periods are? Is there a better method to finding the difference between these trigonometric functions besides at the roots?","['trigonometry', 'analysis']"
3391424,"Solve the initial value problem $x'=\frac{1+x}{1-t^2},x(0)=1$","We know $x(t)=-1$ is a solution. Setting that one aside, we have $$\frac{dx}{1+x}=\frac{dt}{1-t^2} \iff \\ \ \\  \ln|x+1|= -\frac{1}{2}\ln\bigg|\frac{t-1}{t+1}\bigg|+C$$ But $C=\ln 2$ if $x(0)=1$ . Hence, $$\ln|x+1|=\ln\bigg( \sqrt{\frac{4|t+1|}{|t-1|}} \bigg)$$ The log's value has discontinuities and singularities, so I'm not sure whether the next inferential step is warranted: $$|x+1|=\sqrt{\frac{4|t+1|}{|t-1|}}$$ And even then, how do we move on from there? We could stipulate $x(t)+1\geq 0$ : $$x(t)=\begin{cases}  2\sqrt{\frac{t+1}{t-1}}-1 & t>1 \\ 
??????? & t=1 \\ 2\sqrt{\frac{t+1}{1-t}}-1 & -1\leq t<1 \\ 2\sqrt{\frac{-t-1}{1-t}}-1 & t<-1  \end{cases}$$ Is this what's expected? And what about $|x+1|\leq0$ ? I'm confused because the textbook says that the solution of an equation like $x'=x^2,x(0)=1$ , which is $x=1/(1-t)$ , is only defined for $(-\infty,0)$ . But in theory $x=1/(1-t)$ is defined everywhere except at $t=1$ . Do we have to choose the maximal interval such that it contains the value of the restriction AND the solution to the differential equation is continuous, etc. there?",['ordinary-differential-equations']
3391456,Geometric and Visual explanation of Symplectic viewpoint of classical mechanics,"I recently learnt that one can express all of classical mechanics on a symplectic manifold . Is it possible to provide an entirely geometric example as to why this is the case? The geometric example should show: A choice of manifold $M$ , closed 2-form $\omega$ , function $H: M \rightarrow \mathbb R$ How these uniqely determine a vector field $V_H$ , whose flow lines ""are the dyanmics"" --- and some way to see that this choice is unique for a fixed $\omega$ and $H$ . What mechanical system corresponds to this choice of $(M, H, \omega)?$ Is there a natural classical system that corresponds to the example? Why $\omega$ must be a closed form (ie, what fails if it is not). What is the geometry behind this condition? While algebraically, one can work through the details, I do not see why this entire recipe works. Some sort of picture / visualization / geometry to the whole thing would be greatly appreciated.","['symplectic-geometry', 'classical-mechanics', 'smooth-manifolds', 'hamilton-equations', 'differential-geometry']"
3391494,Notation of a function raised to a power [duplicate],This question already has an answer here : Conventions for function notation (1 answer) Closed 4 years ago . $f(x)^2$ is sometimes written as $f^2(x)$ . Why is it not written as $f(x)^2$ ? Is it just a matter of taste or convention?,"['notation', 'exponentiation', 'functions']"
3391551,"Let A, B and C be three sets. If A ∈ B and B ⊂ C, is it true that A ⊂ C?. If not, give an example.","Let $A, B$ and $C$ be three sets. If $A ∈ B$ and $B ⊂ C$ , is it true that $A ⊂ C?$ If not, give an example. This question is from my textbook. And the answer is: No. Let $A = \{1\}, B = \{\{1\}, 2\}$ and $C = \{\{1\}, 2, 3\}.$ Here $A ∈ B$ as $A = {1}$ and $B ⊂ C.$ But $A ⊄ C$ as $1 ∈ A$ and $1 ∉ C.$ Note that an element of a set can never be a subset of itself. I am having trouble in in understanding ""But $A ⊄ C$ as $1 ∈ A$ and $1 ∉ C.""$ How could $1 ∉ C?$ Clearly $C$ contains $B$ and $A$ is an element of $B$ and $A$ have $1$ so $C$ must have $1$ .
I would be very grateful If you answer this.",['elementary-set-theory']
3391566,Periodic Solutions for the system,"I'm having lots of troubles with proving that systems do not have periodic solutions. For example, PROBLEM: Prove that the system: $\dot x = A(t)x$ has no periodic solutions but the trivial , where $A(t) = \left [ \begin{matrix}
    0 & 2+sin(t) \\
    4+cos^{2}(t) & 0 \\
   \end{matrix} \right ]$ My naive attempt has to try to calculate the solutions explicitly but I went no where. My other idea is find some criterion for perturbed systems (i.e $\dot x = (A+B(t))x$ ) which allows me to prove periodicity but I couldn't find any. Any help would be incredibly appreciated, thanks guys <3","['calculus', 'ordinary-differential-equations']"
3391593,The polynomial $x^{2k}+1+(x+1)^{2k}$ is not divisible by $x^2+x+1$. Find the value of $k \in \mathbb N$. [duplicate],"This question already has answers here : For which $k\in \Bbb N$ does $x^2+x+1$ divide $ (x + 1)^{2k}+x^{2k} + 1$? (3 answers) Closed 2 years ago . The polynomial $x^{2k}+1+(x+1)^{2k}$ is not divisible by $x^2+x+1$ . Find the value of $k\in \mathbb{N}$ . I tried finding out the roots of $x^2+x+1$ which were $\dfrac{-1±\sqrt{3}i}{2}$ but in vain. I got no result other than making the polynomial more complicated. Here's what I got : $$\left(\frac{-1±\sqrt{3}i}{2}\right)^{2k}+1+\left(\frac{-1±\sqrt{3}i}{2}+1\right)^{2k}.$$ Now, I don't know what to do next. Any help would be appreciated.","['algebra-precalculus', 'quadratics', 'roots', 'polynomials']"
3391609,Does the symmetricity hold for paired data drawn from symmetric sample data without replacement?,"If the distribution of $X_1 ,\dots, X_n$ are symmetric about a common median $\theta$ , which means for every $X_i$ , $$\Pr(X_i \leq \theta-x) = \Pr(X_i \geq \theta +x),$$ where $i = 1, 2,\dots, n$ . May I get the equation $$\Pr(W+Y \leq 2*(\theta - x)) = \Pr(W+Y \geq 2*(\theta + x))$$ if $W$ and $Y$ are drawn from $X_1$ , ..., $X_n$ without replacemnt? Any suggestion will help! Thank you in advance :)","['statistics', 'median', 'probability-theory', 'probability']"
3391612,Number of 6-character passwords containing at least one number - why is my answer wrong?,"Here's the question: How many six-character passwords can be built with lowercase letters and numbers, given that at least one of its characters is a number? Here's my answer: $$10\dbinom{6}{1}36^5$$ $10$ for the number possible choices for digits $\binom{6}{1}$ where one location in the $6$ -character string is being chosen for the digit to be placed $36^5$ for remaining character combinations ( $26$ letters + $10$ digits) Here's the actual answer: $$36^6 - 26^6$$ $36^6$ for all strings of length $6$ minus $26^6$ for the number of strictly alphabetical passwords I get why the solution works, but I can't see where my solution went wrong; it's quite a bit bigger than the answer. Can someone explain why my answer is wrong?","['permutations', 'combinatorics', 'discrete-mathematics']"
3391646,How often do tetrations appear in this sequence of prime factorizations?,"Define the fully factored form (FFF) of an integer $n\ge 2$ to be the result of the following procedure: Write the prime factorization of $n$ in standard form $\prod_{i=1}^{k} p_i^{n_i},$ where $p_1 < p_2 < ... < p_k$ are primes and the $n_i$ are positive integers, then do the same for each exponent $n_i,$ and continue doing this until only prime numbers or $1$ s appear in the expression . Finally, remove all $1$ s. E.g., $\text{FFF}(2^{18}\cdot 3^{64})=2^{2\cdot 3^2}\cdot 3^{2^{2\cdot 3}}$ . Now consider the following infinite sequence of FFFs, in which each successive FFF is obtained from the previous FFF by adding $1$ to every prime number occurring in it: $$\begin{align}&2\\
&3\\
&4=2^2\\
&3^3\\
&4^4=2^{2^3}\\
&3^{3^4}=3^{3^{2^2}}\\
&4^{4^{3^3}}=2^{2^{5\cdot 11}}\\
&3^{3^{6\cdot 12}}=3^{3^{2^3\cdot 3^2}}\\
&4^{4^{3^4\cdot 4^3}}= 2^{2^{10369}}\\
&3^{3^{10370}}=3^{3^{2\cdot 5\cdot 17\cdot 61}}\\
&4^{4^{3\cdot 6\cdot 18\cdot 62}}=2^{2^{40177}}\\
&3^{3^{40178}}=3^{3^{2\cdot 20089}}\\
&4^{4^{3\cdot 20090}}=2^{2^{149\cdot 809}}\\
&3^{3^{150\cdot 810}}=3^{3^{2^2\cdot 3^5\cdot 5^3}}\\
&4^{4^{3^3\cdot 4^6\cdot 6^4}}=2^{2^{5\cdot 227\cdot 252559}}\\
&3^{3^{6\cdot 228\cdot 252560}}=3^{3^{2^7\cdot 3^2\cdot 5\cdot 7\cdot 11\cdot 19\cdot 41}}\\
&\cdots
\end{align}$$ Clearly the sequence must alternate between powers of $2$ and powers of $3$ , and we notice that each of the first four terms is a tetration (i.e., it's an exponential tower of $2$ s only or $3$ s only). Q : How to settle the question of whether a tetration appears in this sequence after $3^3$ ? Are there no more at all? Or do they occur infinitely often? NB : If any more tetrations do occur, they must be towers of $3$ s only, because no step of form $3^{3^x}\to 4^{4^y}=2^{2^{2y+1}}$ can result in a tower of $2$ s only, as $2y+1$ is odd. However, in the steps of form $2^{2^x}\to 3^{3^y}$ , is there some number-theoretic argument against $y$ occasionally (though rarely) being a tower of $3$ s only? NB : Following are the exponents $x$ in the first $12$ terms of form $3^{3^x}$ , suggesting that these exponents are always even , thus preventing any more tetrations (but how to prove this?): 2^2
2^3 * 3^2
2 * 5 * 17 * 61
2 * 20089
2^2 * 3^5 * 5^3
2^7 * 3^2 * 5 * 7 * 11 * 19 * 41
2^8 * 3^3 * 5 * 13 * 41 * 97 * 251
2^19 * 3^7 * 7^2 * 31 * 619 * 683 * 1489 * 1709 * 15420877 * 758647158424919209
2^5 * 3^2 * 5 * 7 * 11^2 * 59 * 6247 * 128703053 * 1678759855446808823464308071758681001
2^2 * 5^3 * 23 * 79 * 25541 * 197316931971016973669 * 137517302030623161523706659273
2^4 * 7 * 1117 * 1657 * 605660678957687521638971 * 741743082447608620588796259899
2^4 * 3^4 * 5 * 7 * 31 * 136859 * 811553 * 1101593 * 14558849 * 132381127 * 40222263624231587 * 7742819802333668771 Summarizing: If we let $f(x)$ be the result of incrementing all the primes in the full factorization of $x$ , then the original sequence has $3^{3^x}\to 2^{2^{2f(x)+1}}\to 3^{3^{f(2f(x)+1)}}\to\cdots $ . Since $2f(x)+1$ is always odd, $2^{2^{2f(x)+1}}$ can never be a tower of $2$ s only, and if $f(2f(x)+1)$ is always even , then $3^{3^{f(2f(x)+1)}}$ can never be a tower of $3$ s only, thus precluding any more tetrations at all.","['number-theory', 'recreational-mathematics', 'prime-factorization', 'prime-numbers']"
3391666,Find a such that $ax^{17}+bx^{16}+1$ is divisible by $x^2-x-1$.,Find $a$ such that $ax^{17}+bx^{16}+1$ is divisible by $x^2-x-1$ . I tried taking the roots of the polynomial which are $\frac{1±\sqrt{5}}{2}$ And I got the equation $a(\frac{1±\sqrt{5}}{2})^{17}+b(\frac{1±\sqrt{5}}{2})^{16}+1=0$ Now I don't know what to do next. Any help would be appreciated.,"['roots', 'common-root', 'polynomials', 'algebra-precalculus', 'quadratics']"
3391691,When to include the constant of integration when finding the integrating factor,"I'm currently studying ODE's with Advanced Engineering Mathematics 10e (Kreyszig) and had a question regarding the constant of integration when finding the integrating factor. I'm currently solving an exercise problem in the section where they explain using the integrating factor to solve nonhomogeneous linear ODEs. More specifically: Solve: $$y' - 2y - x = 0$$ My approach is as follows: Since $y' - 2y = x$ , we can first find the integrating factor by: $$
Fy' - 2Fy = xF$$ where $-2F = F'$ . From here it follows that: $$
\begin{align}
\frac{F'}{F} & = -2 \\
\left( \ln(F) \right)' & = -2 \\
\ln(F) & = -2x + C \\
F & = e^{-2x + C}
\end{align}
$$ Plugging this back into the equation above: $$
\begin{align}
e^{-2x + C}y' -2e^{-2x + C}y & = xe^{-2x + C} \\
\left( e^{-2x + C}y \right)' & = xe^{-2x + C} \\
e^{-2x + C} y & = -\frac{x}{2}e^{-2x + C_1}+C_2
\end{align}
$$ And this is where I get stuck. The reason why I'm confused is because I'm not sure how to deal with the constants of integration that I've introduced into the entire process after multiple integration operations. Is my overall approach correct? And if so, how might I go about dealing with the constants of integration? Thanks.","['integration', 'ordinary-differential-equations']"
3391722,Giving bounds to the principal solution matrix,"I've been trying to solve a problem from ODE and Dynamical System of G.Teschl and I got stuck in some linear algebra problem(linear algbra more than ODE I guess). Problem For any matrix A (so it can be constant or depending on $t$ ), the matrix $Re(A) = \frac{1}{2}(A + A^{*})$ is symmetric and hence has only real eigenvalues. Let $\alpha_{0}$ be its largest eigenvalue. Let $A(t)$ be given and and define $\alpha_{0}(t)$ as above, prove that $||\Pi(t,t_{0}) || \leq e^{\int_{t_{0}}^{t} \alpha_{0}(s)dx}$ for $t>t_{0}.$ My Problem I know I can consider $\frac{d}{dt}|x(t)^{2}| = 2 <x(t),\dot x(t)> = 2<x(t),Ax(t)>$ . After this, I cant find the way to relate $Re(A)$ with $A$ in order to bound my inner product(this is why I think my problem is with linear algebra more than ODE). After this I know I can use the fact that $<x,Re(A)x> \leq \alpha_{0}|x^{2}(t)|$ and then apply some gronewall's inequality. Thanks so much for your answers!","['calculus', 'linear-algebra', 'ordinary-differential-equations']"
3391808,Doubt about proof in Tube Lemma,"So i have been studying topology and when proving that the finite product of compact spaces is going to be compact we have to use the tube Lemma, and we have to prove it. I have a question about the proof : Well we start by covering $ x \times Y $ with basis elements and then since $Y$ is compact and $x \times Y$ is homeomorphic to $Y$ we can find a finite subcolection. My question why do i need this finite sub collection??  If every basis elements $ U \times V$ is in $N$ their infinite union is still gonna be in $N$ , there has to be something that i am missing , i guess what im really asking is why the tube lemma doesn't work if $Y$ isnt compact, so any help is appreciated , Thanks.",['general-topology']
3391813,How to understand that $\mathcal F$ is a $\sigma$-field containing all intervals?,"Since I cannot understand the definition of Borel $\sigma$ -field, which is defined as $$\mathcal B=\bigcap\{\mathcal F:\mathcal F \text{ is a }\sigma\text{ -field containing all intevals}\}.$$ We say that $\mathcal B(\mathbb{R} )$ is the smallest $\sigma$ -field generated by all intervals and we call the elements of $\mathcal B$ Borel sets. $\mathbb R$ is a set containing all real numbers. $\mathcal F$ is the $\sigma$ -field on $\mathbb R$ , then $\mathcal F \text{ is a proper }\text{collection of subsets of }\mathbb R$ satisfying 3 conditions of the definition of the $\sigma$ -field. But how can we say that $\mathcal F$ contains all the intervals? Some examples? 
Thank you!",['measure-theory']
3391814,Prove that if $\eta = \log(\frac{\phi}{1-\phi})$ then $\phi = \frac{1}{1+e^{-\eta}}$,"In the pdf supervised learning from Andrew Ng, p. 23, it is said that if $$\eta = \log(\frac{\phi}{1-\phi})$$ then $$
\phi = \frac{1}{1+e^{-\eta}}
$$ I can we prove it step by step? I tried backward but got stuck \begin{align}
\frac{1}{1+e^{-\eta}}&=\phi\\
\implies 1&=\phi(1+e^{-\eta})\\
&= \phi + \frac{\phi}{e^{\eta}}\\
&= \frac{e^{\eta}\phi + \phi}{e^{\eta}}\\
&= \phi\frac{e^{\eta} + 1}{e^{\eta}}
\end{align} Edit (with help of posted answers) \begin{align}
1&= \phi\frac{e^{\eta} + 1}{e^{\eta}}\\
\implies \frac{1}{\phi} &= \frac{e^{\eta} + 1}{e^{\eta}}\\
\implies \phi &= \frac{e^{\eta}}{e^{\eta} + 1}\\
 &= \frac{e^{\eta}}{e^{\eta} + 1}\frac{\frac{1}{e^{\eta}}}{\frac{1}{e^{\eta}}}\\
&=\frac{1}{1+ e^{-\eta}}
\end{align}","['algebra-precalculus', 'exponential-function']"
3391830,(History) Books About Geometry of Curves and Surfaces,"Are there any books regarding how curvature, torsion, etc., did born? When these math notions were used for the first time?","['surfaces', 'curves', 'book-recommendation', 'reference-request', 'differential-geometry']"
3391848,"All positive continuous functions $g$ on $[0,\infty)$ s.t. $\frac{1}{2}\int_0^x [g(t)]^2 dt = \frac{1}{x}\left(\int_0^x g(t)dt\right)^2$.","Repeating the title, the question is as followed: Find all positive continuous functions $g$ on $[0,\infty)$ such that $\frac{1}{2}\int_0^x [g(t)]^2\text{ d}t = \frac{1}{x}\left(\int_0^x g(t)\text{ d}t\right)^2$ for $x > 0$ . Clearly $g(t) = 0$ is a solution, but it is not positive so it is not what the question is asking for. One can check that $g(t)$ can never be a positive constant function. I don't really know how to proceed with this question. I conjectured that $g(0) = 0$ necessarily, which implies that no such positive function exists but to no avail. Any help is appreciated.","['functions', 'real-analysis']"
3391884,Maximum distance between samples with equal value in a sequence of i.i.d. discrete samples,"Let $\left(X_i\right)_{i=1}^n$ be a sequence of i.i.d. samples with discrete outcome space $S=\{s_1,...,s_k\}, k<\infty$ , $s_i \in \mathbb{R}$ with respective probabilities $p_1,...,p_k$ .  Define the maximum distance between two samples with equal value as \begin{equation}
D_n = \max_{i=1,...,n}\{\min_{j>i}\{|i-j|:X_i=X_j\}\},
\end{equation} where we simply take $n-i$ if $X_i$ is the last sample in the sequence with its value. I want to show if it holds that \begin{equation}
\mathbb{P}(D_n \le \varepsilon n) \xrightarrow{n\to\infty}1, \forall \varepsilon >0.
\end{equation} My idea of why this holds is that as the number of samples till we see a sample with value $s_i$ is Geometrically distributed with parameter $p_i$ . As all the $X_i$ are i.i.d. it follows that for $n$ large enough that the number of samples in the sequence with value $s_i$ is $p_i n$ . We know that, given that we have $p_i n$ samples with value $s_i$ , the maximum of a sequence of Geometrically distributed samples converges a.s. to $\frac{\log(p_i n)}{\log(1/(1-p_i)}$ . If the maximum for every value was independent of the maximum of the other values it would follow that \begin{equation}
D_n \xrightarrow{a.s.}\max\{\frac{\log(p_i n)}{\log(1/(1-p_i)},i=1,...k\}.
\end{equation} As the order of growth for $D_n$ is $\log(n)$ it follows that for $n$ large enough that $D_n < \varepsilon n, \forall \varepsilon > 0$ , such that \begin{equation}
\mathbb{P}(D_n \le \varepsilon n) \xrightarrow{n\to\infty}1, \forall \varepsilon >0.
\end{equation} The problem is that the maximum of the values are dependent of each other. I am having real trouble finding how to approach this problem and can't find anything on the internet. Any advice how to solve this or approach this will be greatly appreciated as this would complete the proof of a theorem I am working on.","['probability-theory', 'probability', 'sequences-and-series']"
3392033,Prove $R$ is transitive $\iff R \circ R \subseteq R $,"Proposition. $R$ is transitive $\iff R \circ R \subseteq R $ My attempt: $(\rightarrow)$ Suppose $R \subseteq A \times A$ and $R$ is transitive. Consider arbitrary $(x,y) \in R \circ R$ . Implies that there is some $a \in A$ , such that $(x,a) \in R$ and $(a,y) \in R$ . By definition of transitivity, we know that $(x,y) \in R$ . Arbitrary element was considered, hence $R \circ R \subseteq R$ . $(\leftarrow)$ By contrapositive. Suppose $R$ is not transitive. It follows that there are at least two elements, $(x,y)$ and $(y,p)$ , such that both are in $R$ . Then $(x,p) \in R \circ R$ . But since $R$ is not transitive, $(x,p) \notin R$ . Hence $R \circ R \not \subseteq R$ . Is it correct?","['elementary-set-theory', 'proof-verification']"
3392043,"Proof for a bound $\epsilon > m$ such that $A^TXA \preceq \epsilon X$, where $X \succ 0$.","I'm interested in the following problem: Let $A,X$ be matrices in $\mathbb{R}^{n\times n}$ with $X$ symmetric and positive definite ( $X \succ 0$ ). Proof a lower bound $0 < m < \epsilon$ so that $$ A^TXA \preceq \epsilon X.$$ As illustrated in an related question $m = \lambda_\max(A)^2$ is not sufficient even for $A \succeq 0$ . I came up with the following bound based on inequalities for the trace of matrix product : If $ \epsilon > \lambda_\max(AA^T)\frac{\lambda_\max(X)}{\lambda_\min(X)},$ then $ A^TXA \preceq \epsilon X.$ The proof is given below. Questions: First, I'm happy if anyone could point out flaws in the proof below. Are there better bounds for $\epsilon$ such that the matrix inequality holds? Is there a simpler proof of the given solution? Proof: Let $\rho(\cdot)$ denote the spectral radius of a matrix and let $||\cdot||_F$ be the Frobenius norm. Then \begin{align}
A^TXA \preceq \epsilon X &\iff \rho (A^TXA (\epsilon X)^{-1}) < 1 \\
&\iff \underset{k\rightarrow \infty}{\lim} ||(A^TXA (\epsilon X)^{-1})^k||_F = 0.
\end{align} We use the following inequality : Let $A,B$ be matrices in $\mathbb{R}^{n\times n}$ with $B \succeq 0$ . Then $$ \text{trace}(AB) \le \lambda_\max \left(\frac{A+A^T}{2}\right) \text{trace}(B) $$ We have \begin{align}
||(A^TXA (\epsilon X)^{-1})^k||_F &= \sqrt{ \text{trace} \left( (A^TXA (\epsilon X)^{-1})^k \right)^T \left( (A^TXA (\epsilon X)^{-1})^k \right)} \\
&= \frac{1}{\epsilon^k} \sqrt{ \text{trace} \left( X^{-1} A^TXA \ldots X^{-1} A^TXAA^T X A X^{-1} \ldots A^T X A X^{-1} \right)} \\
&= \frac{1}{\epsilon^k} \sqrt{ \text{trace} \left( X^{-2} \underbrace{A^TXA \ldots X^{-1} A^TXAA^T X A X^{-1} \ldots A^T X A}_{\text{positive semidefinte by definition}} \right)} \\
&\le \frac{1}{\epsilon^k} \sqrt{ \lambda_\min(X)^2 \text{trace} \left( AA^TXA \ldots X^{-1} A^TXAA^T X A X^{-1} \ldots A^T X \right)} \\
& \ \ \vdots \\
&\le \left(\frac{1}{\epsilon} \lambda_\max(AA^T) \frac{\lambda_\max(X)}{\lambda_\min(X)} \right)^k n
\end{align} The first equality applies the definition of the Frobenius norm. In the second equality we use that $X$ is symmetric and so is $X^{-1}$ . In the third equality we apply the cyclic property of the trace and observe that the right part is positive semidefinite per construction. The first inequality applies the trace inequality, uses the fact the eigenvalues of the square of a symmetric matrix are the eigenvalues of that matrix squared and that the eigenvalues of the inverse of a positive definite matrix are the reciprocals of the eigenvalues of that matrix. For the last inequality we apply the proceeding approach successively for $X^{-2}, AA^T, X^2$ , $A^TA$ and use that the eigenvalues of $AA^T$ and $A^TA$ are equal. Hence if $\frac{1}{\epsilon} \lambda_\max(AA^T) \frac{\lambda_\max(X)}{\lambda_\min(X)} < 1$ , then $\underset{k\rightarrow \infty}{\lim} ||(A^TXA (\epsilon X)^{-1})^k||_F = 0$ .","['proof-verification', 'matrices', 'spectral-radius', 'alternative-proof', 'linear-algebra']"
3392122,Decay of tail of a function - relation to integrability,"Possibly a basic question, but I could not find anything online. Suppose we have a bounded metric space $(X,d)$ with a Borel probability measure $\mu$ . If we have an integrable function $f:X\to \mathbb{N}$ , can we relate the decay of $\mu(f>n)$ to the integrability of $f$ ? For example, if $\mu(f>n)=O(n^{-\alpha})$ for some $\alpha>0$ , can we say $f\in L^2(X)$ (or even $L^p$ for $p>2$ )? (I would imagine certain conditions would be required on $\alpha$ here). Similarly, if $\mu(f>n)=O(e^{-cn})$ for some $c>0$ or $\mu(f>n)=O(e^{-cn^b})$ for $c>0$ , $0<b<1$ , can we say that $f\in L^p$ for all $p>1$ ? (I believe the terms above are polynomial decay, exponential decay, and stretched exponential decay - correct me if I am mistaken).","['measure-theory', 'ergodic-theory', 'asymptotics', 'lp-spaces', 'mixing']"
3392238,Covariant derivative on the base space,"The basic definition of a covariant derivative for a Lie algebra valued n-form $\alpha \in \Omega^n(P)\otimes T_eG$ with $P$ a principle bundle with base space a manifold $M$ , and $T_eG$ the Lie algebra of the fiber $G$ is $$
D\alpha(x_1, ..., x_{n + 1}) = d\alpha(x_1^H, ..., x_{n + 1}^H)
$$ Where $x = (x_1, ..., x_{n+1}) \in T_qP$ and $x^H \in H_qP$ , being this the horizontal tangent space at $q \in P$ such that $T_qP = V_qP\oplus H_qP$ and $V_qP \subset T_qG$ the vertical tangent space. Also, $d$ is the exterior derivative map $d: \Omega^n \rightarrow \Omega^{n + 1}$ In Nakahara's ""Geometry, Topology and Physics"" the equation (10.47) stands that for any Lie algebra valued n-form defined on $M$ , $\eta$ , you can write $$
D\eta = d\eta + [A, \eta] = d\eta + A\wedge\eta - \eta\wedge A
\tag1$$ Where $A = S_i^*\omega$ with $S_i: U_i\subset M \rightarrow P$ and $\omega$ is the connection 1-form. I tried to prove it aplying $S_i^*$ to $D\alpha$ since $S_i^*\alpha$ is a Lie algebra valued n-form defined on $M$ , after defining $D$ in a more suitable way: $$
D\alpha = d\alpha + \sum_{i = 1}^n(-1)^{i + 1}\omega(x_i)\wedge \alpha(x_1, ..., \hat{x}_i, ..., x_n)
\tag2$$ Where the hat over $x_i$ means that that vector doesn't appear on $\alpha$ I think this is the right way to define $D$ due to the answer in Covariant derivative: QFT vs. Math But if you apply $S_i^*$ to Eq. (2) and call $\eta = S_i^*\alpha$ you get $$
D\eta = d\eta + \sum_{i = 1}^n(-1)^{i + 1}(S_i^*\omega(x_i))\wedge (S_i^*\alpha(x_1, ..., \hat{x}_i, ..., x_n)) = \\ d\eta + \sum_{i = 1}^n(-1)^{i + 1}A(x_i)\wedge \eta(x_1, ..., \hat{x}_i, ..., x_n)
$$ Therefore, Eq. (2) is equal to Eq. (1) only in the case $\eta \in \Omega^1(M)\otimes T_eG$ , i.e. just for $n = 1$ . Nevertheless, Nakahara ensures that is true for any n. Is Eq. (2) wrong or maybe my interpretation of commutator in Eq. (1)? How can you write in terms of a connection 1-form the covariant derivative for any Lie algebra valued n-form such that when defining on $M$ (i.e., after using $S_i^*$ ) leads you to Eq. (1)?","['lie-algebras', 'principal-bundles', 'fiber-bundles', 'lie-groups', 'differential-geometry']"
3392318,Compatible atlas induced the same topology.,"Let $M$ a set and $\mathcal{A}=\{(U_\alpha,\varphi_\alpha)\}$ an atlas we said that $A\subseteq M$ is open iif $\varphi_\alpha(A\cap U_\alpha)$ is open in $\mathbb{R}^n$ for all chart $(U_\alpha,\varphi_\alpha).$ We denote with $\tau_\mathcal{A}$ the topology definited by $\mathcal{A}$ . Now let $\mathcal{B}=\{(V_\alpha,\psi_\alpha)\}$ another atlas compatible with $\mathcal{A}$ , that is $$\psi_\beta\circ \varphi^{-1}_\alpha\colon\varphi_{\alpha}(U_\alpha\cap V_\beta)\to\psi_\beta(U_\alpha\cap V_\beta),$$ is a diffeomorphism and $\varphi_{\alpha}(U_\alpha\cap V_\beta)$ , $\psi_\beta(U_\alpha\cap V_\beta)$ are open sets of $\mathbb{R}^n.$ Denote with $\tau_\mathcal{B}$ the topology definited by $\mathcal{B}$ . I must prove that $\tau_\mathcal{A}=\tau_\mathcal{B}$ . Now, if $A\in\tau_\mathcal{A}$ , then $\varphi_\alpha(A\cap U_\alpha)$ is open, therefore $$\varphi_\alpha(A\cap U_\alpha)\cap \varphi_\alpha(U_\alpha\cap V_\beta)=\varphi_\alpha(U_\alpha\cap V_\beta\cap A)$$ is open. Now $$\psi_\beta(A\cap U_\alpha\cap V_\beta)=(\psi_\beta\circ\varphi^{-1})(\varphi_\alpha(A\cap U_\alpha\cap V_\beta))$$ is open since $\psi_\beta\circ\varphi_\alpha^{-1}$ is a diffeomorphism. Question. I don't know how he shows at this point that $\psi_\beta(A\cap V_\beta)$ is open for all chart in $\mathcal{B}$ .
  Same hints? Thanks!","['manifolds', 'general-topology', 'differential-geometry']"
3392331,Proof: minimum volume of a notch cut at equal angle of cutting surfaces with horizontal plane,A notch is cut in a cylindrical vertical tree trunk. The edge of the cut reaches the axis of the cylinder and the cut is between two half-circle planes. Each half-circle is bounded by a horizontal line passing through the axis of the cylinder. The angle between the two half-circle planes is θ. Prove that the volume of the notch is minimized (for given tree and θ) by taking the half-circle planes at equal angles with the horizontal plane. I'm having trouble even visualizing the problem. I think it would be useful to find the total volume of the notch in terms of the angles that the bounding planes form with the horizontal plane and then differentiate to find when the minimum is achieved. Can someone provide a solution?,['trigonometry']
3392368,"Proving geometrically that, for vectors $u$ and $v$ in $\Bbb{R}^2$, $\lim_{x\rightarrow 0}\frac{|u+xv|-|u|}{x}$ exists and is finite","Let $u,v$ be two vectors in $\mathbb{R}^2$ (e.g. $u=(1,2), v=(-1,1)$ ). Consider the function $$x\mapsto \frac{|u+xv|-|u|}{x}$$ which is defined for $x>0$ . I'm trying to prove in a purely geometric manner that $$\lim_{x\rightarrow 0}\frac{|u+xv|-|u|}{x}$$ exists and is finite. Analytically this is easy (prove that it is a monotonic, bounded function, and hence all one-sided limits exist and are finite), but geometrically (=without resorting to numbers) I was not able to do it. The basic problem is that I cannot plot this function by purely geometric constructions ; from such a plot the existence and finiteness would automatically follow . Can you do it? Philosophical remarks: I'm having a hard time accepting that it's possible to prove this with an analytic argument that lacks any geometric insight - and that it is not possible to also provide a geometric proof. This is because it's usually possible to have both. An example from calculus where both approaches are possible, is analyzing differentiability of a function at a point $x_0$ : The analytic proof there actually has a strong geometric content, since one relies on the geometric plot of the function to see what goes wrong: if it has a kink at $x_0$ or goes to infinity or oscillates, then this information is directly represented and used in the analytic proof. Thus, here the two approach are intimately connected, whereas in my example above they seem to be divorced. This is how far I got in my construction (using Geogebra): Plot the vectors $u$ and $v$ : Then you can trace out, with the green vector arrows, the map $$x\mapsto u+xv$$ (of course this is the line spanned by $v$ , translated via $u$ , but let's pretend we didn't knew this already from experience): Now: Then we obtain a plot of $$x \mapsto \frac{|u+xv|-|u|}{x},$$ and now of course it is easy to ""see"" that the limit exists. Note: Of course one could have obtained this plot in Geogebra, by simply starting to measure the various vector lengths and then dividing that number by the relevant $x$ and marking that spot on the coordinate system (which is probably how Geogebra does it internally). But I would have preferred obtaining this picture by simply doing geometric constructions (e.g. by using those that are available in Geogebra). Perhaps no such constructions are possible and going about this problem simply by using numbers (which is what the analytical approach in the end boils down to) is all there is. But then I would like a proof (or at least a justification) for why that is the case. I feel like I will nee to offer bounty for this questions.","['limits', 'calculus', 'graphing-functions', 'geometry']"
3392386,"Determine all functions $f:\mathbb{Z}\to\mathbb{Z}$ such that $f\big(f(n)\big)=-(q-p)\,f(n)+pq\,n$ for all $n\in\mathbb{Z}$.","Let $p$ and $q$ be integers.  Let $S$ be a subset of $\mathbb{Z}$ , and $f:S\to S$ .  Consider the functional equation of the form $$f\big(f(n)\big)=-(q-p)\,f(n)+pq\,n\text{ for each }n\in S\,.\tag{*}$$ If $p$ and $q$ satisfy $0<p<q$ , and $S=\mathbb{Z}_{\geq 0}$ or $S=\mathbb{Z}_{>0}$ , then the only solution is known to be $$f(n)=pn\text{ for all }n\in S\,.$$ See here and here for references. The case of my particular interest is when $S=\mathbb{Z}$ .  We know that there are at least two solutions: $$f(n)=pn\text{ for all }n\in\mathbb{Z}$$ and $$f(n)=-qn\text{ for all }n\in\mathbb{Z}\,.$$ Are there other solutions when $S=\mathbb{Z}$ ? What happens if $p<q$ does not hold (but they are still positive integers)?  What can happen if we simply allow $p$ and $q$ to be any integer?  How would these changes affect the cases $S=\mathbb{Z}_{\geq 0}$ , $S=\mathbb{Z}_{>0}$ , and $S=\mathbb{Z}$ ?  (For example, when $p=1$ and $q=-1$ , then there can be other solutions such as $f(n)=n+1$ for all $n\in S$ .) If you feel particularly enthusiastic today, then you can also consider the case where $p$ and $q$ are nonintegral, not necessarily real, algebraic integers such that $q-p$ and $pq$ are both integers.  In this version of the problem (except for a few pairs $(p,q)$ ), I do not expect a solution in any of the cases $S=\mathbb{Z}_{\geq 0}$ , $S=\mathbb{Z}_{>0}$ , and $S=\mathbb{Z}$ . The trivial case $p=q=0$ is completely solved.  Other known trivial cases are $p=q=1$ and $p=q=\sqrt{-1}$ .  However, I do not know other results even when $p=q$ . Here is a nontrivial example for a nonintegral pair $(p,q)$ . When $S=\mathbb{Z}_{\geq0}$ or $S=\mathbb{Z}_{>0}$ , there exists a strictly increasing function $f:S\to S$ such that $$f\big(f(n)\big)=3n\text{ for all }n\in S\,.$$ (This is an example when $p=q=\sqrt{3}$ .) This may be (or may not be) helpful.  Here, $f^0:=\text{id}_S$ and $$f^k:=f\circ f^{k-1}$$ for $k\in\mathbb{Z}_{\geq 1}$ .  If $p+q\neq 0$ , then $$f^k(n)=p^k\,\left(\frac{qn+f(n)}{p+q}\right)+(-q)^k\,\left(\frac{pn-f(n)}{p+q}\right)$$ for all $n\in S$ and $k\in\mathbb{Z}_{\geq 0}$ .  On the other hand, if $q=-p$ and $p\neq 0$ , $$f^{k}(n)=p^k\,n+k\,p^{k-1}\,\big(f(n)-pn\big)$$ for all $n\in S$ and $k\in\mathbb{Z}_{\geq 0}$ .","['functional-equations', 'integers', 'recurrence-relations', 'recursion', 'functions']"
3392435,Proof of the fact that the decreasing limit of continuous increasing functions is right continuous.,"As the title suggests, I'm looking for a proof of the following statement: Let $(f_n)_{n \in \mathbb{N}}$ be a sequence of continuous and increasing functions. If $f_n(x) \searrow f(x)$ , as $n \rightarrow +\infty$ , then $f(x)$ is right continuous. My attempt to prove it: Suppose that $f(x)$ is not right continuous; i.e., $\lim_{\delta \rightarrow 0^+} f(x + \delta) \neq f(x)$ . Then, $\exists \epsilon > 0$ , such that, $\forall \delta > 0$ , $|f(x + \delta) - f(x)| > \epsilon$ . It follows that: $$
\begin{align}
\epsilon & < |f(x + \delta) - f(x) + f_n(x) - f_n(x) + f_n(x + \delta) - f_n(x + \delta)| \leq \\ 
&\leq |f(x + \delta) - f_n(x + \delta)| + |f_n(x) - f(x)| + |f_n(x + \delta) - f_n(x)|.
\end{align}
$$ The first two terms can be controlled by choosing a sufficiently large $n$ s.t. $|f(x + \delta) - f_n(x + \delta)|$ and $|f_n(x) - f(x)|$ are $< \frac{\epsilon}{3}$ , while the third term can be controlled by choosing a $\delta$ s.t. $|f_n(x + \delta) - f_n(x)| = \frac{\epsilon}{3}$ . Implying that $\epsilon < \frac{3 \epsilon}{3} = \epsilon$ , which is an absurdity, completing the proof. Since I'm not very convinced if I made something wrong, my question is: Is this a valid proof? If it is not, what should I do in order to correctly prove the described statement? EDIT: as pointed out by @TheoreticalEconomist, it is necessary an inequality (instead of a "" $=$ "", as it was written before) for controlling the first two terms. But even though, I'm not sure if the presented proof is correct: the fact that $f_n$ decreases to $f$ , as $n \rightarrow +\infty$ , is being used at the ""two terms"" part; however, the third term is still a problem $-$ somehow it should be justified by the $(f_n)$ characteristics. EDIT 2: I think it is correct because: 1) chosen $n = \max(n_0, n_1)$ , s.t. $|f(x + \delta) - f_n(x + \delta)|$ and $|f_n(x) - f(x)|$ are $< \frac{\epsilon}{3}$ , it is possible to choose a convenient $\delta$ s.t. $|f_n(x + \delta) - f_n(x)| = \frac{\epsilon}{3}$ (notice that, before that point, $\delta$ was not chose yet); and 2) the hyphothesis of $(f_n)$ being a sequence of increasing function (implying that $f$ is a increasing function as well) is indeed needed $-$ recall that $|f(x + \delta) - f(x)| > \epsilon$ has to be satisfied (otherwise, it would not be true). Which, if I'm not wrong, eliminates all potential problems. FINAL EDIT: for a proper proof, please refer to the Tom's answer.","['analysis', 'real-analysis']"
3392477,"Example 2.2.9 in Herstein's TOPICS IN ALGEBRA, 2nd ed: The order of the group of all $2\times 2$ invertible matrices of integers modulo $p$","My question is as follows: Let $p$ be a given prime number (i.e. a natural number greater than $1$ whose only positive divisors are $1$ and itself). Let $G$ be the group of all the $2 \times 2$ matrices $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ , where $a, b, c, d \in \{ 0, \ldots, p-1 \}$ and $ad - bc \not\equiv 0 \mod p$ , under multiplication modulo $p$ . What is the order of $G$ ? My Attempt: Let us first put $p \colon= 2$ . In this case, $G$ consists of all the matrices $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ , where $a, b, c, d \in \{ 0, 1 \}$ and $ad - bc \not\equiv 0 \mod 2$ . Case 1. First, we consider the case where at least one of the entries of the general element $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ of $G$ is $0$ ,  more precisely is congruent to $0$ modulo $2$ . In this case, neither of the rows and neither of the columns of $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ can be zero modulo $2$ . Accordingly, we have the following four sub-cases: Sub-Case 1.1. If $a = 0$ , then $b$ and $c$ cannot be $0$ , although $d$ can; this gives us two possibilities, namely $\left[ \begin{matrix}  0 & 1 \\ 1 & 0 \end{matrix} \right]$ and $\left[ \begin{matrix}  0 & 1 \\ 1 & 1 \end{matrix} \right]$ . Sub-Case 1.2. If $b = 0$ , then $a$ and $d$ cannot be $0$ , although $c$ can; this gives us two possibilities, namely $\left[ \begin{matrix}  1 & 0 \\ 0 & 1 \end{matrix} \right]$ and $\left[ \begin{matrix}  1 & 0 \\ 1 & 1 \end{matrix} \right]$ . Sub-Case 1.3. If $c = 0$ , then $a$ and $d$ cannot be $0$ , although $b$ can; this gives us two possibilities, namely $\left[ \begin{matrix}  1 & 0 \\ 0 & 1 \end{matrix} \right]$ and $\left[ \begin{matrix}  1 & 1 \\ 0 & 1 \end{matrix} \right]$ , but only one of these two is a new possibility, for the other one (i.e. the first) has already occured in Sub-Case 1.2. So we have only one more possibility, namely $\left[ \begin{matrix}  1 & 1 \\ 0 & 1 \end{matrix} \right]$ . Sub-Case 1.4. Finally, if $d = 0$ , then $b$ and $c$ cannot be $0$ , although $a$ can; this gives us two possibilities, namely $\left[ \begin{matrix}  0 & 1 \\ 1 & 0 \end{matrix} \right]$ and $\left[ \begin{matrix}  1 & 1 \\ 1 & 0 \end{matrix} \right]$ , but only one of these two is a new possibility, for the first matrix has already occured in Sub-Case 1.1 above. So we have one more possibility, namely $\left[ \begin{matrix}  1 & 1 \\ 1 & 0 \end{matrix} \right]$ . Thus in the case where at least one of the entries of $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ is $0$ , we have the following six elements of $G$ : $\left[ \begin{matrix}  0 & 1 \\ 1 & 0 \end{matrix} \right]$ , $\left[ \begin{matrix}  0 & 1 \\ 1 & 1 \end{matrix} \right]$ , $\left[ \begin{matrix}  1 & 0 \\ 0 & 1 \end{matrix} \right]$ , $\left[ \begin{matrix}  1 & 0 \\ 1 & 1 \end{matrix} \right]$ , $\left[ \begin{matrix}  1 & 1 \\ 0 & 1 \end{matrix} \right]$ , and $\left[ \begin{matrix}  1 & 1 \\ 1 & 0 \end{matrix} \right]$ . Case 2. Next, we consider the case when none of the entries of $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ is $0$ . In this case, we are only left with the matrix $\left[ \begin{matrix}  1 & 1 \\ 1 & 1 \end{matrix} \right]$ . However, for this matrix we have $1 \cdot 1 - 1 \cdot 1 = 0$ . So we obtain no new elements of $G$ in this case. Therefore for $p = 2$ , the order of $G$ is $6$ . Am I right? Next, we put $p \colon= 3$ . In this case, $G$ consists of all the matrices $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ , where $a, b, c, d \in \{ 0, 1, 2 \}$ and $ad - bc \not\equiv 0 \mod 3$ . Case 1. First, we consider the case where at least one of the entries of the general element $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ of $G$ is $0$ ,  more precisely is congruent to $0$ modulo $3$ . In this case, neither of the rows and neither of the columns of $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ can be zero modulo $3$ . Accordingly, we have the following four sub-cases: Sub-Case 1.1. If $a = 0$ , then $b$ and $c$ cannot be $0$ , although $d$ can; this gives us $2 \times 2 \times 3 = 12$ possibilities. Sub-Case 1.2. If $d = 0$ , then $b$ and $c$ cannot be $0$ , although $a$ can; however for $a=0$ four possibilities from Sub-Case 1.1 will occur again. Thus we have $2 \times 2 \times 2 = 8$ new possibilities. Sub-Case 1.3. If $b = 0$ , then $a$ and $d$ cannot be $0$ , although $c$ can; this gives us $2 \times 2 \times 3 = 12$ possibilities. Sub-Case 1.4. Finally, if $c = 0$ , then $a$ and $d$ cannot be $0$ , although $b$ can; this gives us $2 \times 2 \times 3 = 12$ possibilities in all, but for $b = 0$ the four possibilities of Sub-Case 1.3 will occur again. Thus we get only $2 \times 2 \times 2 = 8$ new possibilities. Thus in the case where at least one of the entries of $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ is $0$ , we have $12 + 8 + 12 + 8 = 40$ possibilities. Case 2. Next, we consider the case when none of the entries of $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ is $0$ modulo $3$ . In this case we have $2 \times 2 \times 2 \times 2 = 16$ possibilities at most. However, in this case, neither of the two columns of $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ can be a multiple modulo $3$ of the other column, and neither of the two rows of this matrix can be a multiple modulo $3$ of the other row. Thus the following eight matrices cannot occur in $G$ : $$ \left[ \begin{matrix} 1 & 1 \\ 1 & 1 \end{matrix} \right], \left[ \begin{matrix} 1 & 1 \\ 2 & 2 \end{matrix} \right], \left[ \begin{matrix} 1 & 2 \\ 1 & 2 \end{matrix} \right], \left[ \begin{matrix} 1 & 2 \\ 2 & 1 \end{matrix} \right], \left[ \begin{matrix} 2 & 1 \\ 2 & 1 \end{matrix} \right], \left[ \begin{matrix} 2 & 1 \\ 1 & 2 \end{matrix} \right], \left[ \begin{matrix} 2 & 2 \\ 1 & 1 \end{matrix} \right], \left[ \begin{matrix} 2 & 2 \\ 2 & 2 \end{matrix} \right].  $$ We are thus left with only $16 - 8 = 8$ possibilities in the case where none of the entries of $\left[ \begin{matrix} a & b \\ c & d \end{matrix} \right]$ is $0$ modulo $3$ . Therefore for $p=3$ , the order of $G$ is $40 + 8 = 48$ . Am I right? Now we consider a general $p$ . In this case, $G$ consists of all the matrices $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ , where $a, b, c, d \in \{ 0, 1, \ldots, p-1 \}$ and $ad - bc \not\equiv 0 \mod p$ . Case 1. First, we consider the case where at least one of the entries of the general element $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ of $G$ is $0$ ,  more precisely is congruent to $0$ modulo $p$ . In this case, neither of the rows and neither of the columns of $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ can be zero modulo $p$ . Accordingly, we have the following four sub-cases: Sub-Case 1.1. If $a = 0$ , then $b$ and $c$ cannot be $0$ , although $d$ can; this gives us $$ (p-1) (p-1)p = p(p-1)^2 $$ possibilities. Sub-Case 1.2. If $d = 0$ , then $b$ and $c$ cannot be $0$ , although $a$ can, giving us $(p-1)(p-1)p = p(p-1)^2$ possibilities; however for $a=0$ the $(p-1)(p-1) = (p-1)^2$ possibilities from Sub-Case 1.1 will occur again. Thus we only have $$ p(p-1)^2 - (p-1)^2 =  (p-1)^3 $$ new possibilities. Sub-Case 1.3. If $b = 0$ , then $a$ and $d$ cannot be $0$ , although $c$ can; this gives us $$ (p-1)(p-1)p = p(p-1)^2 $$ possibilities. Sub-Case 1.4. Finally, if $c = 0$ , then $a$ and $d$ cannot be $0$ , although $b$ can; this gives us $(p-1)(p-1)p = p(p-1)^2$ possibilities in all, but for $b = 0$ the $(p-1)(p-1) = (p-1)^2$ possibilities of Sub-Case 1.3 will occur again. Thus we get only $p(p-1)^2-(p-1)^2 = (p-1)^3$ new possibilities. Thus in the case where at least one of the entries of $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ is $0$ modulo $p$ , we have $$ p(p-1)^2+(p-1)^3+p(p-1)^2+(p-1)^3 = 2(2p-1)(p-1)^2 $$ possibilities. Case 2. Next, we consider the case when none of the entries of $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ is $0$ modulo $p$ . Now if $x \in \{ 1, \ldots, p-1 \}$ , then $\mbox{gcd}(x, p) = 1$ and so there are integers $x^\prime$ and $p^\prime$ such that $xx^\prime + pp^\prime = 1$ , by Lemma 1.3.1 in Herstein, and hence $xx^\prime -1 = pp^\prime$ , which implies that $p$ divides $xx^\prime - 1$ , or in other words, we have $$ xx^\prime \equiv 1 \mod p.  \tag{A} $$ Now if we divide $x^\prime$ by $p$ , we get integers $q^\prime$ and $r^\prime$ such that $$x^\prime = pq^\prime + r^\prime, $$ where $r^\prime \in \{ 0, 1, \ldots, p-1 \}$ , from which it follows that $$ x^\prime \equiv r^\prime \mod p, $$ and therefore we obtain $$ xx^\prime \equiv xr^\prime \mod p, \tag{B} $$ by Lemma 1.3.3 in Herstein. Thus from (A) and (B) and by Result 1. in Lemma 1.3.3 in Herstein, we have shown that, for each element $x \in \{ 1, \ldots, p-1 \}$ , there exists an element $y \colon= r^\prime$ in $\{ 1, \ldots, p-1 \}$ such that $$ xy \equiv 1 \mod p, $$ or in other words, $$ xy = 1 \ \mbox{ modulo } p. \tag{1} $$ Let us denote this element $y$ by $x^{-1}$ . Alternatively, since $p$ is a prime, the set $\{ 0, 1, \ldots, p-1 \}$ is a field under the operations of addition and multiplication modulo $p$ , which implies that, for each element $x$ of this set, there exists a unique element $x^{-1}$ in the set, such that $$ xx^{-1} = x^{-1}x = 1 \ \mbox{ modulo $p$}. \tag{1} $$ Now since each of the entries of the general element $\left[ \begin{matrix} a & b \\ c & d \end{matrix} \right]$ of $G$ is to be from the set $\{ 1, \ldots, p-1 \}$ , therefore we have $(p-1)(p-1)(p-1)(p-1) = (p-1)^4$ possibilities at most. However, once we have chosen $a$ , $b$ , and $c$ arbitrarily from the set $\{ 1, \ldots, p-1 \}$ , for which there are $(p-1)^3$ possibilities, we cannot choose $d$ from this set in such a way that $ad = bc$ modulo $p$ . Thus, corresponding to each one of the $(p-1)^3$ possible choices for the ordered triple $(a, b, c)$ , the entry $d$ can only be chosen from the set $$ \{ 1, \ldots, p-1 \} - \{ \ a^{-1}bc \}. \qquad \mbox{ [ Refer to (1) above. ] } $$ Thus there are only $p-2$ choices for $d$ corresponding to each one of the $(p-1)^3$ choices for the ordered triple $(a, b, c)$ . Therefore in the case when none of the entries of $\left[ \begin{matrix}  a & b \\ c & d \end{matrix} \right]$ is $0$ modulo $p$ we have $(p-2)(p-1)^3$ possibilities. Hence the order of $G$ is $$ 
\begin{align}
2(2p-1)(p-1)^2 + (p-2)(p-1)^3 &= \big[ 2(2p-1) + (p-2)(p-1) \big] (p-1)^2 \\
&= \big[ (4p-2) + (p^2 - 3p + 2) \big] (p-1)^2 \\
&= (p^2 + p)(p-1)^2 \\
&= p(p+1)(p-1)^2. 
\end{align}
$$ For $p = 2$ , we note that $$ p(p+1)(p-1)^2 = 2(2+1)(2-1)^2= 2(3)(1)^2 = 6,$$ whereas for $p=3$ , we have $$ p(p+1)(p-1)^2 = 3(3+1)(3-1)^2= 3(4)(2)^2 = 48.$$ Both these calculations agree with our earlier results. Is my answer correct for a general prime $p$ ? If so, then is each and every detail of my solution accurate enough too? Is my approach the same as that required by Herstein? Or, are there problems of accuracy, clarity, or rigor with my attempt?","['proof-verification', 'finite-groups', 'abstract-algebra', 'combinatorics', 'group-theory']"
3392504,Solve the differential equation $\frac{dy}{dx}=\frac{y-4x}{x-y}$.,Solve the differential equation $\frac{dy}{dx}=\frac{y-4x}{x-y}$ . I used the substitution $y(x)=v(x)x$ and got $v+xv'=\frac{v-4}{1-v}$ . But I am not sure what do I do after this? What method do I need?,['ordinary-differential-equations']
3392562,What is the expected number of random small circles it takes to cover a large circle? [duplicate],"This question already has answers here : Rain droplets falling on a table (4 answers) Closed 2 years ago . I have a large circle of radius $B$ . I choose a random point inside the large circle, and draw a small circle of radius $A$ around it ( $A<B$ ). What are the chances that, after drawing $n$ such small circles, the union of their area covers the large circle (i.e., includes every point in the large circle)? What is the expected value of the number of small circles I must draw to cover the large circle? My progress so far: If I pick any point in the large circle, the chance that it has not been covered by $n$ small circles is: 1 minus the chance that it has been covered = 1 minus the chance that the origin of a small circle is within $A$ of this point, after $n$ tries = 1 minus (the small circle of radius $A$ around this point divided by the total area of the large circle), after $n$ tries = $\left(1 - \frac{\pi A^2}{\pi B^2}\right)^n$ = $\left(\frac{B^2 - A^2}{B^2}\right)^n$ So, the chance that any point has been covered is $1 - \left(\frac{B^2 - A^2}{B^2}\right)^n$ . How do I use this info to know whether all point have been covered after $n$ small circles? And find the expected value of the number of small circles it takes? Also, even this oversimplifies it: If the point I pick in the large circle is within $A$ away from the edge, a circle of radius $A$ around it isn't completely contained in $B$ , so the ratio of ""hits"" is actually less than $\frac{A^2}{B^2}$ . I assume we can calculate the amount of overlap for the ""average"" small circle, which is 100% for most of them but less than that (is it $\frac{3}{4}$ ?) for $\frac{A}{B}$ of them.","['area', 'circles', 'probability']"
3392591,Proving a possible corollary of the monotone convergence theorem,"Let $\{f_n\}$ be a sequence of nonnegative measurable functions on $E$ that converges point wise on $E$ to $f$ . Suppose $f_n \leq f$ for each $n$ . Show that: $$\lim_{n \rightarrow \infty} \int_E f_n = \int_E f.$$ So, I've already proved the monotone convergence theorem, the assumption of which is the same as what I'm trying to prove except for in the monotone convergence theorem the sequences of functions is increasing. I feel like there must be some clever trick to use the convergence theorem to prove this one... I've been thinking about it for awhile but I fear I'm stuck in tunnel vision. Insights appreciated!! Thanks!","['measure-theory', 'convergence-divergence', 'real-analysis']"
3392689,Some kind of perturbed system of ODEs,"Problem $\dot x = Ax + h(t)$ , where $h(t)\in \mathbb{R}^{n}$ is continuous and bounded over $\mathbb{R}$ and $A\in M_{n\times n}$ a constant matrix with all its eigenvalues having negative real part . I have to prove that there's only one bounded solution over $(-\infty, \infty)$ and in case that $|h(t)| \to 0$ when $t \to \infty$ , then all solutions tends to $0$ . My thoughts Since A is constant with all its eigenvalues having negative real part then we can ensure that $ | \Pi_{A}(t,s) | \leq Ce^{-\alpha(t-s)}$ , i.e all solutions for the homogeneous system $\dot x=Ax$ are bounded. Now, since $\dot x= Ax+h(t)$ is an inhomogeneous system, we can find the solutions by using Duhamel's formula, $x(t)=| \Pi_{A}(t,s) |x_{0} + \int | \Pi_{A}(t,r)|h(r)dr$ and after this im lost, I mean, I could bound it by $|x(t)| \leq Ce^{-\alpha(t-s)}x_{0}+\int Ce^{-\alpha(t-r)}C_{1}dr$ but I can't see where to be able to prove uniqueness of a bounded solutions. Im pretty sure that this must be some kind of theorem but I couldn't be able to find it. So, any help would be really appreciated. Thanks so much for your help.","['ordinary-differential-equations', 'real-analysis', 'stability-in-odes', 'calculus', 'linear-algebra']"
3392742,Problem regarding the Wronskian formula,"I'm studying from G.Teschl ODE & Dynamical system and I got stucked at problem 3.20, which is: Find a formula for the Wronskian $W(x,y)=\dot xy-\dot yx$ of the autonomous system $\ddot x + c_{1}\dot x + c_{0}x=0$ . I know I can find the characteristic polynomial $\lambda^{2}+c_{1}\lambda+c_{0}=0$ where the can be calculated and that way try to find explicits solutions. But I don't think that's what is wanted where. Thanks so much for your answers!.","['calculus', 'ordinary-differential-equations']"
3392748,"A bounded, self-adjoint, positive operator $T$ induces a positive semidefinite quadratic form $\langle Tx,x\rangle$.","Let $H$ be a Hilbert space and $T\in \mathcal B(H)$ be a bounded, self-adjoint linear operator that is positive in the sense that $\sigma(T) \subset [0,\infty)$ . Is there an elementary method of proving that $T$ induces a positive semidefinite quadratic form, i.e. $$
\langle Tx,x\rangle \ge 0
$$ for all $x\in H$ ? The proof of this statement (and its converse) can be found in this post . However, while the converse can be proved by an elementary mean, the proof of the statement that I want relies on the spectral theorem for self-adjoint operators. I want to know if there is a more rudimentary way to do it (i.e. without using these high-tech theorems).","['hilbert-spaces', 'operator-theory', 'spectral-theory', 'functional-analysis']"
3392780,Show $ F^{\prime}\left(t_{0}\right)=D f_{p+t_{0} v}(v)=D_{v} f\left(p+t_{0} v\right) $,"Suppose $f: \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$ is differentiable and given $p, v \in \mathbb{R}^{m}$ define $F(t)=f(p+t v)$ for $t \in \mathbb{R}$ . Show that for
all $t_{0} \in \mathbb{R}$ we have $$
F^{\prime}\left(t_{0}\right)=D f_{p+t_{0} v}(v)=D_{v} f\left(p+t_{0} v\right)
$$ My attempt. $F^{\prime}\left(t_{0}\right)=f'(p+t_0v)$ , $D_{v} f\left(p+t_{0} v\right)=\lim_{t\to 0} \dfrac {f(p+t_0v+tu)-f(p+t_0v)} {t},$ What is the $D f_{p+t_{0} v}(v)$ ? And how can I show that equality? $$
F^{\prime}\left(t_{0}\right)=D f_{p+t_{0} v}(v)=D_{v} f\left(p+t_{0} v\right)
$$ May you help? Thanks...","['multivariable-calculus', 'real-analysis']"
3392850,A Proposed Converse to A Theorem on Products of Limits,"Suppose $(x_n)$ is bounded and $\lim_{n\rightarrow \infty} y_n = 0. $ It is well known that $\lim_{n\rightarrow \infty}(x_ny_n) = 0.$ My question is the following: if for all sequences $(y_n)$ for which $\lim_{n\rightarrow \infty} y_n = 0$ we have $\lim_{n\rightarrow \infty}(x_ny_n) = 0,$ is $(x_n)$ necessarily bounded?","['indeterminate-forms', 'limits', 'cauchy-sequences', 'sequences-and-series']"
3392889,"Why is $h_n(1,x,\ldots,x^{m}) = h_m(1,x,\ldots,x^{n})$?","Let $h_n$ denote the degree $n$ symmetric polynomial corresponding to the homogeneous basis. That is, in, e.g., $m$ variables $$
h_n(x_1,\ldots,x_m) = \sum_{1\le i_1\le \cdots \le i_n \le m} \prod_{j\le n} x_{i_j}
$$ Why is the following identity in $\mathbb{Q}[x]$ true? $$
h_n(1,x,\ldots,x^m) = h_m(1,x,\ldots,x^n)
$$ I am interested in this because its equivalent to Hermite reciprocity of representations of $\mathfrak{sl}_2$ : $\text{Sym}^n(\text{Sym}^m(V)) \cong \text{Sym}^m(\text{Sym}^n(V))$ for $V$ the standard representation.","['representation-theory', 'combinatorics', 'symmetric-polynomials']"
3393006,Is SO(n) a deformation retract of SO(n+1) for $n\ge 3$?,"I know that $SO(n)$ shares many topological invariants with $SO(n+1)$ for $n\ge 3$ (such as first fundamental group, first singular cohomology class over $\mathbb{Z}$ , etc.). I also know that there is an inclusion $i: SO(n) \hookrightarrow SO(n+1)$ taking $A \mapsto \begin{bmatrix} A & 0 \\ 0 & 1 \\ \end{bmatrix}$ . Is $SO(n)$ a deformation retract of $SO(n+1)$ ? Edit: Perhaps this can be deduced from $SO(n+1)/SO(n)\cong \mathbb{S}^n$ altough I do not know how to do that.","['general-topology', 'topological-groups', 'lie-groups', 'algebraic-topology']"
3393040,Given a function $f$ infinitely differentiable at a point $c$ does there exist a neighborhood of $c$ in which $f$ is infinitely differentiable?,"Suppose $f$ is a real valued function defined on a subset of the reals and $f$ is infinitely differentiable at $c$ .Then is it possible that there does not exist any neighborhood of $c$ in which $f$ is infinitely differentiable.Clearly $c$ cannot be an isolated point of any of the domains of $f^{(k)}$ .But I cannot proceed further.
Alternatively can we say that the set $S(f) :=\{c : f$ is infinitely differentiable at $c\}$ is an open set?","['metric-spaces', 'real-analysis', 'continuity', 'general-topology', 'derivatives']"
3393041,Problem : What are our ages now?,"Problem : My age was twice your age , when I was the same age as yours now. And when you are the same age as mine now ,  the sum of our ages is $63$ . What are our ages now? I see this problems in my teacher and solution is $(28,21)$ But I don't know how I solve it ! Please if any one have ideas drop here to see Thanks!",['algebra-precalculus']
3393056,What are the steps for solving sec(arctan(-8))?,"In the image below you can see how I started out solving the problem. I knew that arctan(-8) would give me an angle measure, so I drew out where tan(x)= -8 would be on the coordinate plane. From there I thought that I could just find sec(x), but then I realized that I do not know the values of y or x. Am I approaching this problem the wrong way? Is there something I'm missing? Is there another way to solve it?","['algebra-precalculus', 'trigonometry']"
3393081,"Does there exist a set X such that for any set Y, there exists a surjective function f : X → Y?","I was thinking about this question and I have my own thoughts on this, please do let me know if my reasoning is correct. The question asks if I can pick an $X$ such that this condition is fulfilled for all $Y$ . If the function $f$ is to be surjective, it must mean that every element must be an output of $f$ . So if $X$ was countably infinite, this would not be possible, since I can pick $Y$ an uncountably infinite set. So $X$ would have to be uncountably infinite set, but then this set $X$ would not work if $Y$ was the null set, because a function only exists from $f:X \rightarrow \emptyset $ only if $X$ is the null set. So this statement is false. Is my reasoning correct?","['functions', 'proof-verification', 'discrete-mathematics']"
3393140,Whats the name of this surface $a^2+b^2+c^2+2abc-1=0$?,"Motivation:
Let $a = \cos(\alpha), b = \cos(\beta),c = \cos(\gamma)$ where $\alpha,\beta,\gamma$ are the angles in a triangle.
Then by $$\alpha+\beta+\gamma = \pi$$ and using $\cos(x+y) = \cos(x)\cos(y)-\sin(x)\sin(y)$ , $\sin(\operatorname{acos}(x) = \sqrt{1-x^2}$ we find that the $a,b,c$ are points on the surface: $$a^2+b^2+c^2+2abc-1=0$$ What is known about this surface, or what is it called? a plot of the surface with Wolfram Alpha Edit:
With the help of @GEdgar I think I found the name of this surface by looking at the plots of both surfaces. (Cayley's nodal cubic surface). Now the question remains how to show algebraically that they are the
  same? Edit :
Since every three point metric space can be isometrically emdedded in $\mathbb{R}^2$ , we can build the possibly to a line degenerated triangle from these three points: Using the law of cosines to define angles, given distances, we find that the quantities: $$a:=\frac{d(y,z)^2+d(y,x)^2-d(x,z)^2}{2d(y,z)d(x,y)}, $$ $$b:=\frac{d(y,z)^2+d(z,x)^2-d(x,y)^2}{2d(y,z)d(z,x)},$$ $$c:=\frac{d(x,z)^2+d(y,x)^2-d(y,z)^2}{2d(x,z)d(x,y)}$$ satisfy by what was given above the following equation: $$a^2+b^2+c^2+2abc-1=0$$ hence are points on the Cayley's nodal cubic surface.
For instance for the metric on natural numbers $$d(x,y) = \sqrt{\sigma(x)+\sigma(y)-2\sigma(\gcd(x,y))}$$ and for three pairwise distinct primes $p,q,r$ we get the following nice formula: $$\pi = \operatorname{acos}(\frac{r}{\sqrt{(p+r)(q+r)}})+\operatorname{acos}(\frac{q}{\sqrt{(p+q)(q+r)}})+\operatorname{acos}(\frac{p}{\sqrt{(p+r)(p+q)}})$$ Setting $$a = \frac{r}{\sqrt{(p+r)(q+r)}}, b= \frac{q}{\sqrt{(p+q)(q+r)}}, c=\frac{p}{\sqrt{(p+r)(p+q)}}$$ we see that those are points on the Cayley nodal cubic surface.","['cubics', 'algebraic-geometry', 'surfaces']"
3393191,Probability: Optimal strategy in die game,"You are given a single die. When you roll the die, you have two choices: End the game here, in which case the number you just rolled is your final score. Continue the game and roll once again, after which you will face these same two choices. You can roll the die a maximum of 4 times. After the fourth roll, you will not have a choice and will have to accept whatever you rolled as your final score. There are two questions to answer here: What is the optimal strategy to maximize your final score? What is the expected value of the final score? I was asked this question in an interview and struggled to work it out. I ended up getting the job anyway, but am still interested to figure out the approach to this question. When I was unable to answer this question, my interviewer said that he didn't expect me to answer this question as it needs some intuition from Markov chains, which I had no prior exposure to.","['markov-chains', 'probability']"
3393202,"2D LOTUS: joint PDF on unit square $\{ (x, y) : x, y \in [0, 1] \}$","My textbook, Introduction to Probability by Blitzstein and Hwang, presents the following example: Example 7.2.2 (Expected distance between two Uniforms). Let $X$ and $Y$ be i.i.d. Unif $(0, 1)$ r.v.s. Find $E(|X - Y|)$ . Solution : Since the joint PDF is 1 on the unit square $\{ (x, y) : x, y \in [0, 1] \}$ , 2D LOTUS gives $$\begin{align} E(|X - Y|) &= \int_0^1 \int_0^1 |x - y| \ dx dy \\ &= \int_0^1 \int_y^1 (x - y) \ dxdy + \int_0^1 \int_0^y (y - x) \ dxdy \\ &= 2 \int_0^1 \int_y^1 (x - y) \ dxdy = 1/3 \end{align}$$ First we broke up the integral into two parts so we could eliminate the absolute value; then we used symmetry. The textbook defines the 2D LOTUS as follows: Theorem 7.2.1 (2D LOTUS). Let $g$ be a function from $\mathbb{R}^2$ to $\mathbb{R}$ . If $X$ and $Y$ are discrete, then $$E(g(X, Y)) = \sum_x \sum_y g(x, y) P(X = x, Y = y).$$ If $X$ and $Y$ are continuous with joint PDF $f_{X, Y}$ , then $$E(g(X, Y)) = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x, y) f_{X, Y}(x, y) \ dx dy.$$ LOTUS means Law of the Unconscious Statistician . My multiple integral knowledge is rusty, so I would appreciate it if people could please take the time to explain what's going on for each step here: $$\begin{align} E(|X - Y|) &= \int_0^1 \int_0^1 |x - y| \ dx dy \\ &= \int_0^1 \int_y^1 (x - y) \ dxdy + \int_0^1 \int_0^y (y - x) \ dxdy \\ &= 2 \int_0^1 \int_y^1 (x - y) \ dxdy = 1/3 \end{align}$$ Thank you. EDIT: The PDF of a continuous uniform random variable is $$f(x) = {\begin{cases}{\frac {1}{b-a}}&\mathrm {for} \ a\leq x\leq b,\\[8pt]0&\mathrm {for} \ x<a\ \mathrm {or} \ x>b\end{cases}}$$ Since $X$ and $Y$ are independent, my understanding is that the product of their individual PDFs is equal to the joint PDF: $$p_{X, Y}(x, y) = p_X(x) p_Y(y) = (1)(1) = 1$$","['multivariable-calculus', 'probability-distributions', 'uniform-distribution']"
3393300,Matrix derivative of $\hat{G}\ln \hat{G}$ where $\hat{G}$ is a diagonalizable matrix?,"Let $\hat{G}$ be a $n\times n$ matrix that is diagonalizable $\hat{D}=P^{-1}\hat{G} P$ . What is the matrix-by-matrix derivative: $$
\frac{\partial }{\partial \hat{G}} \hat{G}\ln \hat{G}
$$ My intuition would be to proceed as follows: $$
\begin{align}
\frac{\partial }{\partial \hat{G}} \hat{G}\ln \hat{G}&=\left( \frac{\partial }{\partial \hat{G}} \hat{G} \right)\ln \hat{G}+\hat{G}\frac{\partial }{\partial \hat{G}} \ln \hat{G}\\
&=\ln \hat{G}+\hat{G}\frac{\partial }{\partial \hat{G}} P^{-1} \ln \hat{D} P\\
&=\ln \hat{G}+ \left[ \hat{G} \left( \frac{\partial }{\partial \hat{G}} P^{-1} \right) \ln \hat{D} P \right] + \left[\hat{G} P^{-1} \left( \frac{\partial }{\partial \hat{G}}   \ln \hat{D} \right) P \right]+ \left[\hat{G} P^{-1} \ln \hat{D} \frac{\partial }{\partial \hat{G}}   P \right]
\end{align}
$$ What can I simplyfy from there? Can $\left( \frac{\partial }{\partial \hat{G}} P^{-1} \right)$ or $\left( \frac{\partial }{\partial \hat{G}} P \right)$ be made equal to $0$ ? What about the middle term $P^{-1} \left( \frac{\partial }{\partial \hat{G}}   \ln \hat{D} \right) P$ ? $$
P^{-1} \left( \frac{\partial }{\partial \hat{G}}   \ln \hat{D} \right) P=P^{-1} \left( \frac{1}{\hat{D}} \frac{\partial \hat{D}}{\partial \hat{G}} \right) P
$$ I imagine that because $\hat{G}$ is diagonalizable, eventually $$
\frac{\partial }{\partial \hat{G}} \hat{G}\ln \hat{G} \to \ln \hat{G} +1
$$ but I cannot seem to get there.","['matrices', 'derivatives']"
3393361,Faithful group actions for singleton group or for empty set,"Let $G$ be a group, possibly a singleton. Let $M$ be a set, possibly empty. Let $\mu: M \times G \to M$ be a right group action. Let $1_G$ be the identity of $G$ . I understand Wikipedia 's second definition of faithful as follows: $\mu$ is faithful if for each $g \in G$ such that $g \ne 1_G$ , there exists $x \in M$ such that $\mu(x,g) \ne x$ . $\tag{1}$ Later Wikipedia says if $\mu$ is free and $M$ is a non-empty set , then $\mu$ is faithful. Question: Given the idea that we can have $M$ as an empty set, I would like to clarify: Are these correct? If $G$ is a singleton, then every action $\mu$ is faithful, whether or not $M$ is empty. If $G$ is not a singleton, but $M$ is empty, then every action $\mu$ is not faithful.","['logic', 'abstract-algebra', 'elementary-set-theory', 'group-theory', 'group-actions']"
3393406,Another gcd problem,"My friend gave me yet another challenge. Show that $\sum_{a=1}^{n}{\gcd(n,a)}\leq 2n^{3/2}.$ I have no idea where to start. This is known as Pillai's arithmetic function, and I put this inequality in OEIS, and it seems to hold, but I don't know how to construct a proof for this.","['elementary-number-theory', 'algebra-precalculus']"
3393443,"For $a \in \mathbb R$, find polynomials $P$ such that $(x^2-ax+18)P(x)-(x^2+3x)P(x-3)=0$","Find all polynomials $P(x)$ with real coefficients such that:- $(x^2-ax+18)P(x)-(x^2+3x)P(x-3)=0$ This is $a$ creating some big problems for me. I don't know what to do. I am not able to figure out anything because of that $a$ . The best I can figure out is that I will find the roots of $P(x)$ because $a$ is not computable as there is no way of finding out the zeros of $P(x-3)$ . If I would have been able to find the roots of $(x^2-ax+18)$ then I would have been able to figure out what to do. If there would have been no $a$ I would have found of the roots of $P(x)$ like for example $\alpha, \beta$ then I would have written out $P(x)$ in the form of $(x-\alpha)(x-\beta)Q(x)$ for $Q(x)$ being any polynomial. Then I would have tried to calculate the answer. Any help would be appreciated","['algebra-precalculus', 'polynomials']"
3393450,Showing that $\ln \frac{1+e^x}{1+e^{-x}} = x$,"Is the below a valid approach? $$\frac{1+e^x}{1+e^{-x}} = \frac{1+e^x}{1+e^{-x}} \times \frac{e^{-x}}{e^{-x}}$$ We know that $\frac{e^{-x}}{1+e^{-x}} = \frac{1}{e^x+1}$ , so $$\frac{1+e^x}{1+e^{-x}} \times \frac{e^{-x}}{e^{-x}} = \frac{1+e^x}{1+e^x} \times \frac{1}{e^{-x}}.$$ So, $\ln \frac{1+e^x}{1+e^{-x}} = \ln \frac{1}{e^{-x}} = - \ln e^{-x} = x$ .","['algebra-precalculus', 'exponential-function', 'logarithms']"
3393455,Properties of Tensor field pullbacks,"Page 270, Lee's Introduction to Smooth Manifolds, 1st edition, Proposition 11.9: Suppose $F: M \rightarrow N$ and $G:N \rightarrow P$ are smooth maps, $\sigma \in \mathcal{T}^k(N)$ , $\tau \in \mathcal(T)^l(N)$ , and $f \in C^\infty(N)$ . (Here $\mathcal{T}^k(N)$ is the vector space of covariant k-tensor fields over $N$ , so $\sigma: N \rightarrow T^k(N)$ is a smooth section from $N$ into the k-covariant tensor bundle of $N$ ). The first property stated is bit confusing, and the details are left as an exercise. It states that: $F^*(f \sigma) = ((fF) F^* \sigma)$ . The next property says that $F^*(\sigma \otimes \tau) = F^*(\sigma) \otimes F^*(\tau)$ , and that by identifying $f \otimes \sigma$ with $f \sigma$ , that the first property is a special case of the second (this identification makes sense as we can regard $f$ as a 0-form). Anyway, This was confusing me. I know that $F^*(\sigma)(X_1,....X_k)= \sigma(F_*X_1,....F_*X_k)$ And I feel like $F^*(f \sigma) = ((fF) F^* \sigma)$ , doesn't make sense, as $F^* \sigma$ already sends vectors in $M$ to $\mathbb{R}$ , so what exactly is $fF$ doing? I would appreciate any insight what so ever. Thank you!!","['smooth-manifolds', 'tensor-products', 'pullback', 'manifolds', 'differential-geometry']"
3393464,"If volume of a sphere increases by 72.8%, what is change of its surface area?",If the volume of a sphere is increased by 72.8% what would be the change in surface area? I'm trying to solve the problem using application of derivatives. I noticed that on differentiating the formula for volume of a sphere we directly end up with that of the surface area. How to approach this link?,['algebra-precalculus']
3393578,Cubic polynomial smoothly connecting two circles,"Given two circles with radii $R_L$ and $R_R$ and centers at $(-(R_L+a),\,0)$ and $(R_R+a,\,0)$ , respectively, find a cubic polynomial $p(x)=b+cx^2+dx^3$ that smoothly connects the two circles. $b$ is a parameter so $p(0)=b$ and the linear term of the polynomial is omitted because we want $\frac{\mathrm{d}p}{\mathrm{d}x}\Big|_{x=0}=0$ . My attempt at a solution. Let $\mathrm{C}_{L,R}$ the equations for the upper half of the $L,R$ circles. I formulate two equations relating $\mathrm{C}_{L,R}$ and $p$ , and two equations relating $\frac{\mathrm{d}}{\mathrm{d}x}\mathrm{C}_{L,R}$ and $\frac{\mathrm{d}p}{\mathrm{d}x}$ . Let $x_{L,R}$ be the points where $p(x)$ and $\mathrm{C}_{L,R}(x)$ intersect, then: $$
\mathrm{C}_L(x_L)-p(x_L) = 0
$$ $$
\mathrm{C}_R(x_R)-p(x_R) = 0
$$ $$
\frac{\mathrm{d}}{\mathrm{d}x}\mathrm{C}_L(x_L) - \frac{\mathrm{d}p}{\mathrm{d}x}(x_L) = 0
$$ $$
\frac{\mathrm{d}}{\mathrm{d}x}\mathrm{C}_R(x_R) - \frac{\mathrm{d}p}{\mathrm{d}x}(x_R) = 0
$$ so I have a system of four nonlinear equations with four unknowns $(x_L,\,x_R,\,c,\,d)$ . I coded a simple Newton's method for the system and it works well for some combinations of parameters $(a,\,b,\,R_L,\,R_R)$ when the initial guess is close enough, especially when $|R_L-R_R|$ is not too large and I use a constant damping for the Newton iterations. I can find initial guesses that I think are good via a graphical interface I coded. However, as $|R_L-R_R|$ gets larger the solver fails spectacularly to converge even with very close initial guesses and very small damping. (I should add that I'm actually taking the square of the equations to avoid square roots of negative numbers during the Newton iterations). My question is threefold: a) what other method or modification can I use to make the solver more stable? b) this problem seems to me like it should be solved somewhere, do you know a reference? c) more generally, is there a reason this should fail as horribly as it does when $|R_L-R_R|>>1$ ?","['numerical-methods', 'geometry']"
3393610,Group in which every nonidentity element is of order 2,"I'm working on the following problem in Algebra: Let $G$ be a group in which every nonidentity element is of order $2$ . Show that every subgroup $H$ of $G$ has the property that $G/H$ is isomorphic to a subgroup of $G$ . Here's my progress so far: First, I've shown that any group $G$ such that every nonidentity element is of order $2$ is abelian. That part is easy. Then, this means that every subgroup $H$ of $G$ is then normal, as every subgroup of an abelian group is normal ( $\forall$ x $\in$ G & $\forall$ $h \in H$ , $xhx^{-1} = xx^{-1}h = h \in H$ ). Now, we recall that if $\phi:G \longrightarrow H$ is a group homomorphism, then $G/\ker(\phi) \cong \phi(G)$ , where $\ker(\phi)$ is normal in $G$ by the First Isomorphism Theorem. Since every subgroup $H$ of $G$ is normal, & every normal subgroup is the kernel of a group homomorphism $\phi: G \longrightarrow G/H$ , $G/H \cong \phi(G)$ . It's left to show that $\phi(G)$ , the image of $\phi$ , is isomorphic to a subgroup of $G$ , where $\phi:G \longrightarrow G/H$ is a homomorphism for a normal subgroup $H$ of $G$ . This is the last piece of the proof that I'm stuck on. Is my logic up to this point sound? If so, how can I show this last piece of the proof? Thanks!","['normal-subgroups', 'group-theory', 'abstract-algebra', 'group-isomorphism']"
3393631,"Find $n \in N$, for which $a+b+c+d = n \sqrt{(abcd)}$ has a solution $(a, b, c, d)$ in natural numbers.","By pure logical observations, I got that $n=1,2,4$ satisfies but I couldn't get the proper theoretical proof. Any help is appreciated.","['number-theory', 'vieta-jumping']"
3393635,How to prove $\lim\limits_{n \to \infty}\sin 2^n \neq 0$?,"We can prove $\{\sin n\}$ diverge. We can also prove $\{\sin n^2\}$ diverge. But can we prove $\{\sin 2^n\}$ diverge? I tried the same methods like the former two, but failed. Can anyone help?","['limits', 'calculus']"
3393640,How to prove that the singular value of product of two orthonormal matrix is related to the principal angles between their columns space?,"Assume that $A$ , $B$ $\in R^{p\times d}$ both have orthonormal columns, then the vector of $d$ principal angles between their column spaces is give by $(\cos^{-1}\sigma_1,\cos^{-1}\sigma_2, \dots, \cos^{-1}\sigma_d)^T$ , where $\sigma_1 \ge \dots \ge \sigma_d$ . For the definition of principal angles, it's the copy from wiki. Let $V$ be an inner product space. Given two subspaces $\mathcal{U},\mathcal{W}$ with $\dim(\mathcal{U})=k\leq \dim(\mathcal{W}):=\ell$ , there exists then a sequence of $k$ angles $ 0 \le \theta_1 \le \theta_2 \le \cdots \le \theta_k \le \pi/2$ called the principal angles, the first one defined as $\theta_1:=\min \left\{ \arccos \left( \left. \frac{ |\langle u,w\rangle| }{\|u\| \|w\|}\right) \,\right|\, u\in \mathcal{U}, w\in \mathcal{W}\right\}=\angle(u_1,w_1),$ where $\langle \cdot , \cdot \rangle $ is the inner product and $\|\cdot\|$ the induced norm. The vectors $u_1$ and $w_1$ are the corresponding ''principal vectors.'' The other principal angles and vectors are then defined recursively via $\theta_i:=\min \left\{ \left. \arccos \left( \frac{ |\langle u,w\rangle| }{\|u\| \|w\|}\right) \,\right|\, u\in \mathcal{U},~w\in \mathcal{W},~u\perp u_j,~w \perp w_j \quad \forall j\in \{1,\ldots,i-1\} \right\}.$ The question is that how can I prove that $\sigma_1 \ge \dots \ge \sigma_d$ are actually the singular values of $B^TA$ ?","['svd', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'matrix-decomposition']"
3393691,"Why do we need $F \subset E\;$ for $f(E)\,\backslash\, f(F) \subset f(E \;\backslash\ F)$ to hold?","Theorem: If $f: A\to B$ is a function and $E$ and $F$ are subsets of $A$ , then $f(E)\,\backslash\, f(F) \subset f(E \;\backslash\ F)\,$ if $F \subset E$ . I proved this theorem and my proof is exactly the same as the one in the book. But the proof doesn't use the fact that $F \subset E$ and I don't understand why it is needed.","['elementary-set-theory', 'functions']"
3393770,Simply the expression $n+(n+1)+(n+2)+(n+3)+...+(2n)$,"For starters, I don't believe the geometric series sum formula can be applied? Unless I'm misunderstanding geometric series. Anyways, I thought of splitting up each $2$ -number element. i.e. consider the first element in each pair of brackets: for every pair of brackets this number is $n$ . The sum goes from $n$ to $2n$ , so there are $n$ number of $n$ . Adding these up gives us $n^2$ . Then the second element in each pair of brackets (note that the second element in the first pair of brackets is $0$ and in the last it's $n$ , since $2n=n+n$ ). We have a finite series $0+1+2+3+...+ n$ , whose sum is $n(n+1)/2$ . So $n+(n+1)+(n+2)+(n+3)+...+(2n)$ can be simplified to $n^2 +n(n+1)/2$ . Is that correct? My textbook says the answer is $(n+1)n +$ $n+1 \choose 2$ , which I don't understand and doesn't seem to match what I have...","['discrete-mathematics', 'sequences-and-series']"
3393808,Degrees of smooth equivalence relations,"Smooth equivalence relations (see the introduction of this paper ) are, when viewed up to Borel reducibility, rather boring: a smooth equivalence relation is determined up to Borel reducibility by how many classes it has, and that in turn is always finite, countable, or continuum. However, the situation seems more interesting when we restrict attention to continuous reducibility. For example, unless I'm missing something it's not hard to show that there is no smooth equivalence relation which is maximal among smooth equivalence relations with respect to continuous reducibility. I'd like to know more about the structure of smooth equivalence relations with respect to continuous reducibility; what is a good source on this topic? (I'm happy to restrict to equivalence relations on Baire space if that would help, but in general I'm interested in arbitrary Polish spaces.)","['descriptive-set-theory', 'logic', 'reference-request', 'general-topology', 'set-theory']"
3393823,The symmetric Difference [closed],Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Is $A\triangle B := A\cup B-A\cap B $ a suitable definition for the symmetric difference of two sets $A$ and $B?$ Or is $A\triangle B := (A-B) \cup (B-A) $ only allowed?,['elementary-set-theory']
3393844,Compute $\sum_{n=1}^{\infty} \frac{ H_{n/2}}{(2n+1)^3}$,"How to prove that $$S=\displaystyle \sum_{n=1}^{\infty} \frac{ H_{n/2}}{(2n+1)^3} \quad=\quad \frac{\pi^2G}{4}-\frac{21\zeta(3)\ln(2)}{8}+\frac{\pi^4}{64}+\frac{\Psi^{(3)}(\frac{1}{4})}{512}- \frac{\Psi^{(3)}(\frac{3}{4})}
{512}$$ This problem was proposed by @Ahmad Bow but unfortunately it was closed as off-topic and you can find it here . Any way, I tried hard on this one but no success yet. here is what I did: Using the identity $$H_{n/2}=H_n-n\int_0^1 x^{n-1}\ln(1+x)\ dx, \quad x\mapsto x^2$$ $$H_{n/2}=H_n-2n\int_0^1 x^{2n-1}\ln(1+x^2)\ dx$$ We can write $$S=\sum_{n=0}^\infty\frac{H_n}{(2n+1)^3}-\int_0^1\frac{\ln(1+x^2)}{x}\sum_{n=0}^\infty \frac{2nx^{2n}}{(2n+1)^3}\ dx$$ where \begin{align}
\sum_{n=0}^\infty \frac{2nx^{2n}}{(2n+1)^3}&=\frac1x\sum_{n=0}^\infty \frac{x^{2n+1}}{(2n+1)^2}-\frac1x\sum_{n=0}^\infty \frac{x^{2n+1}}{(2n+1)^3}\\
&=\frac1{2x}\sum_{n=0}^\infty \frac{x^{n+1}}{(n+1)^2}(1+(-1)^n-\frac1{2x}\sum_{n=0}^\infty \frac{x^{n+1}}{(n+1)^3}(1+(-1)^n\\
&=\frac1{2x}\sum_{n=1}^\infty \frac{x^{n}}{n^2}(1-(-1)^n-\frac1{2x}\sum_{n=1}^\infty \frac{x^{n}}{n^3}(1-(-1)^n\\
&=\frac1{2x}\left(\operatorname{Li}_2(x)-\operatorname{Li}_2(-x)-\operatorname{Li}_3(x)+\operatorname{Li}_3(-x)\right)
\end{align} Therefore $$S=\sum_{n=0}^\infty\frac{H_n}{(2n+1)^3}-\frac12\int_0^1\frac{\ln(1+x^2)}{x^2}\left(\operatorname{Li}_2(x)-\operatorname{Li}_2(-x)-\operatorname{Li}_3(x)+\operatorname{Li}_3(-x)\right)\ dx$$ The sum can be done using the following identity $$ \sum_{n=1}^{\infty} \frac{H_{n}}{ (n+a)^{2}}= \left(\gamma + \psi(a) \right) \psi_{1}(a) - \frac{\psi_{2}(a)}{2} \, , \quad a >0.$$ Differentiate both sides with respect to $a$ then set $a=1/2$ we get $$\sum_{n=0}^\infty\frac{H_n}{(2n+1)^3}=\frac{45}{32}\zeta(4)-\frac74\ln2\zeta(3)$$ and the question here is how to calculate the the remaining integral or a different way to tackle the sum $S$ ? Thanks","['integration', 'harmonic-numbers', 'calculus', 'closed-form', 'sequences-and-series']"
3393903,Floor and ceiling Functions in Inequalities,"Let a and b be real numbers with a < b. how do I Use the floor and/or ceiling functions to express the number of integers n that satisfy a ≤ n ≤ b? Since we know that x ≤ n if and only if ⌈x⌉ ≤n ( n integer ). And we know that x ≤ n if and only if n ≤ ⌊x⌋ ( n integer ) ⌈a⌉ ≤ n ≤ ⌊b⌋ , but i am not sure where to do from here?","['inequality', 'functions', 'discrete-mathematics']"
3394024,Compute $\lim_{n \to \infty}(\frac{a_n+b_n}{2})^n$,"I am trying to solve the following problem: Compute $\lim_{n \to \infty}(\frac{a_n+b_n}{2})^n$ when $\lim_{n \to \infty} a_n^n=a>0$ and $\lim_{n \to \infty} b_n^n=b>0$ such that $a_n,b_n>0 \ \forall \ n \ \in \mathbb{N}$ . I tried to use the Sandwich Theorem to come up with an answer, but my upper bound was not tight: $\max(a_n,b_n)\ge(\frac{a_n+b_n}{2}) \ge \sqrt{a_nb_n}$ On passing to the limits I got the following: $\max(a,b)\ge \lim_{n \to \infty}(\frac{a_n+b_n}{2}) \ge \sqrt{ab}$ But this doesn't help me at all. How could I actually compute the limit?","['limits', 'calculus', 'sequences-and-series']"
3394039,Is any immersed manifold also embedded?,"Let $M$ be a smooth manifold. To establish my criteria let me define immersed and embedded submanifolds: A subset $N\subset M$ is an immersed submanifold when $N$ is itself a manifold and $\iota:N\rightarrow M$ is an (injective) immersion. In the same way, a subset $N\subset M$ is an embedded submanifold if the inclusion is an embedding. By theorem 5.8 of John M. Lee's book Introduction to smooth manifolds , any subset $N$ of a smooth manifold $M$ such that each point $p\in N$ is contained in the domain of a chart $(U,\varphi)$ of $M$ verifying $$
\varphi(U\cap N)= V \times \{ c \},
$$ for $V\subset\mathbb R^k$ open and $c\in \mathbb R^{n-k}$ constant, is a topological submanifold of $M$ and it admits a smooth structure making it into a $k$ -dimensional embedded submanifold. Conversely, any embedded submanifold exhibits such a property. On the other hand, Proposition 5.22 of the same book states that if $N\subset M$ is an immersed submanifold then, for each point $p\in N$ there is an open neighbourhood (with respect to $N$ ) $U'$ of $p$ such that $U$ is an embedded submanifold. However, if $U'$ is an embedded submanifold, by the previous result, there exists a chart $(U,\varphi)$ of $M$ such that $p\in U$ and $$
\varphi(U'\cap U)=V \times \{ c\} ,
$$ for $V$ and $c$ as before. On the other hand, $U'=N\cap U''$ , for some open neighbourhood (with respect to $M$ ) $U''$ of $p$ . Then, if we set $\tilde U=U\cap U''$ and $\tilde\varphi = \varphi|_{\tilde U}$ , we have found a chart of $M$ with $p\in \tilde U$ and such that $$
\tilde\varphi(N\cap \tilde U)= V\times \{c \} .
$$ Then, again using theorem 5.8, we deduced that $N$ is an embedded submanifold. Maybe, warned by Lee's remark after proposition 5.22, the embedded structure of $N$ may or may not agree with the immersed structure. What I am saying is that for a subset $N$ being an immersed submanifold is such a nice property that the same set can be endowed with an embedded structure too. Question. Am I right? Does any immersed submanifold admit a (possibly different) structure of embedded submanifold? If not, what is wrong with my previous reasoning? Remark. Notice that being an immersed submanifold is not a trivial condition, i.e. not every subset can be endowed with such a strcture. For instance, the boundary of a square in $\mathbb R^2$ (Problem 5-9 of Lee's book). Then, not every set can be realised as an embedded submanifold.","['submanifold', 'smooth-manifolds', 'differential-geometry']"
3394045,Adding and Subtracting Vector Subspaces,"Let $U$ and $W$ be two vector subspaces of some other arbitrary vector space. We define $U - W = \left\{ \vec{u} - \vec{w} \, \big| \, \vec{u} \in U, \vec{w} \in W \right\}$ . We similarly define $U + W = \left\{ \vec{u} + \vec{w} \, \big| \, \vec{u} \in U, \vec{w} \in W \right\}$ . My question is: Why is $U - U = \left\{ 0 \right\}$ false ? From my understanding, subtracting something from itself always results in zero, no? Similarly, why is $U - W = U + W$ true ? Thanks for the clarification!","['linear-algebra', 'vector-spaces']"
3394052,A good graduate complex analysis textbook,"I'm about to teach a graduate level one-semester complex analysis course.  The audience will be very good advanced undergraduate students and first year graduate students. I don't want to use Lang or Ahlfors. I was considering ""Complex Analysis"" by Elias Stein and Rami Shakarchi or possibly ""Complex Analysis"" by Eberhard Freitag and Rolf Busam.  Does anyone have any experience teaching courses with those books, particularly for a one semester course with the audience I mentioned above? Thanks,
Alan","['complex-analysis', 'book-recommendation', 'reference-request']"
3394180,"Given the second derivatives of y and x wrt. a third variable, what is the second derivative of y wrt. x?","I understand that if ${dy\over dt}$ and ${dx\over dt}$ are known, then ${dy\over dx}={{dy\over dt}\over {dx\over dt}}$ based on this logic: y' wrt x in terms of y' wrt t and x' wrt t I'm wondering how this relates to second derivatives: If ${d^2y\over dt^2}$ and ${d^2x\over dt^2}$ are known, what is ${d^2y\over dx^2}$ ? I tried using the same method as the one above, but got stuck at this step: y'' wrt. x in terms of y'' wrt. t and x'' wrt. t My guess is that the above expression is the same as ${{d^2y\over dt^2}\over {d^2x\over dt^2}}$ . If this is the case, is the result for third derivatives ${d^3y\over dx^3}={{d^3y\over dt^3}\over {d^3x\over dt^3}}$ , and so on? I tried googling it but couldn't find an answer.","['calculus', 'derivatives']"
3394224,How would you define f(7) to make this function continuous?,This is an AP question: How would you define $f(7)$ in order to make $f$ continuous at $7$ ? $$f(x) = \frac{x^2 - 2x - 3}{x - 7}$$ A long division turns $f$ to $$f(x) = (x + 5) + \frac{32}{x - 7}.$$ Obviously $f$ tends to infinity at $x = 7$ .  I don't see how I could define $f(7)$ to make it continuous.  Is the question problematic?  Any idea?,"['continuity', 'calculus', 'functions']"
3394288,Can two different functions have the same graph?,"I know that, Identical Functions (Equal Functions) are those functions which have the same domain and give the same output for every input value. These functions have the same graph. For example, The functions $f(x)=x^3/x$ and $g(x)=x^4/x^2$ have the same domain (set of non-zero real numbers), and give the same output for every input value. These are identical functions (equal functions) and generate the same graph. On the other hand, the function $h(x)=x^2$ is not identical to the functions $f$ and $g$ , as the domain of $h$ (set of real numbers) is different from that of $f$ and $g$ (set of non-zero real numbers). The only difference in the graphs of $h$ and $f$ (or $g$ ) is at the point $x=0$ . Now coming to my doubt, Can two different functions have the same graph? Or in other words, if the graphs of two different functions are exactly same, can we conclude that the two functions are identical (equal) ? If not please give some examples where two different functions generate the same graph.","['functions', 'graphing-functions']"
3394307,Definition of differentiability using sequence notations,"Let $f$ be differentiable at $c$ and let $\{a_n\}$ and $\{b_n\}$ be sequences such that $a_n<c<b_n$ , and $a_n$ and $b_n$ converges to $c.$ Prove that $$\lim_{n\to\infty}\frac{f(a_n)-f(b_n)}{a_n-b_n}=f'(c).$$ What I have so far is that \begin{align}
& \frac{f(a_n)-f(b_n)}{a_n-b_n}-f'(c) \\[8pt]
= {} & \left[\frac{f(b_n)-f(c)}{b_n-c}-f'(c)\right]\cdot\frac{b_n-c}{b_n-a_n} \\[8pt]
& {} + \left[\frac{f(c)-f(a_n)}{a_n-c}-f'(c)\right]\cdot\frac{c-a_n}{b_n-a_n}
\end{align} All is left to prove is that $\frac{b_n-c}{b_n-a_n}$ and $\frac{c-a_n}{b_n-a_n}$ is bounded. Then, I am planning to use the fact that $f$ is differentiable at c to show that the brackets goes to 0 and thus the whole expression goes to 0. Thus, the equation. But I am having hard time formally proving that $\frac{b_n-c}{b_n-a_n}$ and $\frac{c-a_n}{b_n-a_n}$ is bounded. Can you guys help me?","['derivatives', 'sequences-and-series', 'real-analysis']"
3394315,Intuition for the derivative of the exponential function,"1. THE PROBLEM Take the definition of the derivative: $$\frac{d}{dx}f(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$$ Using this definition to calculate the derivative of $e^x$ is not the most trivial thing to do, as one ends up with: $$\frac{d}{dx}e^x=e^x\lim_{h\to 0}\frac{e^h-1}{h}$$ We can finish this off by a change of variables $n=\frac1h$ . $$e^x\lim_{h\to 0}\frac{e^h-1}{h}=e^x\lim_{n\to\infty}n(e^{1/n}-1)=e^x\cdot\ln e=e^x$$ Note: the second to last equality holds because of a limit definition of the natural logarithm: $\ln x=\lim_{n\to\infty}n(x^{1/n}-1)$ . As we see, calculating the derivative of the exponential function is not easy with the usual limit definition of the derivative. It requires calculating a limit that is not obvious without knowing a special limit definition of $\ln x$ . One can wonder then, are there easier ways of proving that $\frac{d}{dx}e^x=e^x$ ? Indeed, there are easier ways to prove this. But all of the proofs I have ever seen either assume a taylor series or limit definition of the exponential function, or somehow use the derivative of $\ln x$ which itself has similar calculation problems. Finally, the proofs lack deep motivated intuition, and are raw algebraic manipulations for the most part. They prove things well, but they don't explain things well. Question: is there a way to find the derivative of the exponential function intuitively? 2. POSSIBLE SOLUTIONS I didn't ask this question without giving it a little thought. Path A I figured that one solution to this problem might be by intuituvely explaining how in the world $\ln x$ is equal to $\lim_{n\to\infty}n(x^{1/n}-1)$ . Euler observed, quite unrigorously, that if $\epsilon$ is an arbitrarily small number, then: $$\ln(1+\epsilon)=\epsilon$$ Similarly, if we let $n$ be an arbitrarily large number, we can observe that: $$x^{1/n}-1=\epsilon$$ Plugging this observation into the first one, we have: $$\ln(x^{1/n})=x^{1/n}-1$$ $$\frac1n\ln x=x^{1/n}-1$$ $$\ln x=n(x^{1/n}-1)$$ Thus: $$\ln x=\lim_{n\to\infty}n(x^{1/n}-1)$$ This would almost work as a solution, except for the fact that here we make observations that work for logarithms of all bases. The observation $\log_b(1+\epsilon)=\epsilon$ is valid for all bases $b$ . The second observation we made doesn't even relate specifically to logarithms. Thus, the ""intuition"" in this case assumes that the limit can be equal to a logarithm of any base. This is obviously false; computations evidently show that this limit holds only for $b=e$ . And it is not evident at all why it has to be $e$ and nothing else. This solution will be complete if it can be shown why base $e$ and none other work. Path B Another solution to this problem would be noting that the exponential function grows proportionally to its size. The problem with this intuition is that it is not at all evident why would this function follow such a behavior. The mystery is, how does one start with simple algebraic properties of exponents, which are trivially defined by multiplication, and arrive the conclusion that this function follows its unique growth behavior. It might help to note that exponentiation turns an arithmetic sequence into a geometric sequence. Id est , if: $$\alpha_n=a+\sum^n_1 d$$ $$\gamma_n=b\prod^n_1 r$$ Then: $$e^{\alpha_n}=e^{a+\sum^n_1 d}=e^a\prod^n_1 e^d=b\prod^n_1 r=\gamma_n$$ If there is a way to start with basic algebraic facts about exponents and end up (intuitively) with the fact that exponential growth is proportional to its size, we could then justify the fact that $e^x$ is the solution of $y'=y$ , $y(0)=1$ . From there, we could automatically say that the derivative of the natural exponential is itself. Caveat: While solving this ODE, there is still a problem because we need to compute the integral of $\frac1x$ . It turns out that we can intuitively solve this task. We can begin by splitting the area under the curve into n rectangles of equal area $A$ , situated between corresponding x coordinates: $\{x_0, x_1, ..., x_n\}$ . We will then note that: $$A=y_0(x_1-x_0)=y_1(x_2-x_1)$$ $$\frac{x_1-x_0}{x_0}=\frac{x_2-x_1}{x_1}$$ $$\frac{x_1}{x_0}-1=\frac{x_2}{x_1}-1$$ $$\frac{x_1}{x_0}=\frac{x_2}{x_1}$$ This will generalize to $\frac{x_n}{x_{n-1}}=\frac{x_{n+1}}{x_n}$ . What this means is that, if rectangles are the same area , if we increase the x coordinates geometrically (because the ratio between next and current x coordinate is constant), we increase the area arithmetically. This is precisely what logarithms do, they turn geometric sequences into arithmetic sequences (opposite of the exponentials). Thus, the integral of $\frac1x$ will be some kind of logarithm. The missing bit here, again, is...why is it base e , and not some another base? Other paths Those two paths are most likely not the only approaches. 3. MOTIVATION At this point, I overstressed the word ""intuition"", and I just wanted to explain myself. I just really love to explore things that are proven symbolically, in a natural way. I might be considered weird for trying to do that so deeply for such a simple derivative, but oh well. Thank you in advance for any good insights into this problem.","['calculus', 'derivatives', 'exponential-function', 'intuition']"
3394321,"If for every $h\in\mathbb{R}^n$ $d^k f ( x ) ( h )^k = 0 $, then $d^k f ( x ) = 0?$","Let $f:\mathbb{R}^n \rightarrow \mathbb{R}$ be a infinitely differentiable real function . Let ${d}^k f ( \mathbf{x} ) $ denotes the $k-$ th diferential of the function $f$ at $\mathbf{x}$ . Is it true that if ${d}^k f ( \mathbf{x} )(\mathbf{h})^{k}=0$ for all $\mathbf{h}\in\mathbb{R}^n,$ then ${d}^k f ( \mathbf{x} ) = \mathbf{0}$ ? For which values of $k$ this is true?","['calculus', 'derivatives']"
3394344,Does $\sqrt{3-\sqrt{3}} \in \mathbb{Q}(\sqrt{3+\sqrt{3}})$?,"Does $\sqrt{3-\sqrt{3}} \in \mathbb{Q}(\sqrt{3+\sqrt{3}})$ ? Let $L=\mathbb{Q}(\sqrt{3+\sqrt{3}})$ .
I know that $\frac{1}{\sqrt{3+\sqrt{3}}} = \frac{\sqrt{3-\sqrt{3}}}{\sqrt6}$ . So I just need to know whether $\sqrt6 \in L$ . Since $\sqrt 3 = (\sqrt{3+\sqrt{3}})^2 - 3$ , I only need to know if $\sqrt 2 \in L$ . I would guess it is not, but how to show it? If I suppose that $\sqrt 2 \in L$ and aim for a contradiction: It is clear that $\mathbb{Q}(\sqrt 3) \subset L$ , and if $\sqrt 2 \in L$ , then $\mathbb Q(\sqrt 2) \subset L$ as well. Then $K = \mathbb Q (\sqrt 2, \sqrt 3) \subset L$ . Since both $K$ and $L$ are degree $4$ over $\mathbb Q$ , this would imply they are equal. But then I'm back at square one.","['field-theory', 'galois-theory', 'abstract-algebra']"
