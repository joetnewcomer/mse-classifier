question_id,title,body,tags
130123,Complex analysis: Coefficients of Laurent series,"I have some past exam questions that I am confused with http://i39.tinypic.com/vuwxl.png sorry, can't embed images yet I'm not sure how to approach this, I'm completely lost and just attempted to solve a few: a) it says $f(z)$ has a pole of order 5, so $ f(z) = \frac{g(z)}{z^5}, g(z)\neq0 $ so then I guess the condition is $a_{4} = \frac{g^{(4)}(0)}{4!}$? c) $f(\frac{1}{z}) = \frac{g(\frac{1}{z})}{z^5} => f(z) = z^5g(z)$ so the coefficients are $a_{n} = \frac{1}{2\pi i} \oint_\gamma z^5g(z) dz$? d) $\frac{1}{f(z)} = \frac{g(z)}{z^5} => f(z) = \frac{z^5}{g(z)}$ so, $a_{n} = \frac{1}{2\pi i} \oint_\gamma \frac{z^5}{g(z)} dz$ g) $a_{-1} = \frac{1}{2\pi i} \oint_ \gamma f(z) dz = \frac{1}{2\pi i} = Res(f; c)*I(\gamma; c) = -Res(f; c)$ h) $\frac{a_{n}}{16} = 4^{n}a_{n} => 0 = a_{n}(4^{n} - 4^{-2}) => a_{n} = 0$ or $n = -2$ for e) and f), I'm not sure what the relevance of the essential singularity is Well, I think you can see I'm clearly lost, would appreciate if you could help me out.","['sequences-and-series', 'complex-analysis']"
130142,Banach space in functional analysis,Prove that a closed subspace of a Banach space is also a Banach space. Show that the linear space of all polynomials in one variable is not a Banach space in any norm.,"['functional-analysis', 'banach-spaces']"
130159,What is the distribution of this random series?,"Let $\xi_n$ be iid and uniformly distributed on the three numbers $\{-1,0,1\}$.  Set $$X = \sum_{n=1}^\infty \frac{\xi_n}{2^n}.$$
It is clear that the sum converges (surely) and the limit has $-1 \le X \le 1$.. What is the distribution of $X$? Does it have a name?  Can we find an explicit formula?  What else can we say about it (for instance, is it absolutely continuous)? We can immediately see that $X$ is symmetric (i.e. $X \overset{d}{=} -X$).  Also, if $\xi$ is uniformly distributed on $\{-1,0,1\}$ and independent of $X$, we have $X \overset{d}{=} \frac{1}{2}(X+\xi)$.  It follows that for the cdf $F(x) = \mathbb{P}(X \le x)$, we have
$$F(x) = \frac{1}{3}(F(2x+1) + F(2x) + F(2x-1)). \quad (*)$$ The cdf of $\sum_{n=1}^{12} \frac{\xi_n}{2^n}$ looks like this: It looks something like $\frac{1}{2}(1+\sin(\frac{\pi}{2}x))$ but that doesn't quite work (it doesn't satisfy (*)). I'd be interested if anything is known about this.  Thanks!","['probability-theory', 'probability-distributions', 'probability']"
130161,Computing knot/link groups,"The knot group of a knot $K$ is the fundamental group of $\mathbb R^3 \smallsetminus K$; that is, the set of possibly self-crossing closed paths (starting and ending at any single point in space) which you can take through real-space which are not equivalent to each other if you forbid passing through the locus of the knot; and where you can compose the paths in the natural way. For instance, the knot group of the un-knot is just $\mathbb Z$, corresponding to the number of times that a path winds about a cycle in space. We can consider similar questions for links (""knots"" having more than one closed loop) as for knots of a single component: given a link, we can consider the group of closed paths in space which avoids crossing either of the two components. I have a friend who has a tattoo of a knot (more precisely, a link of two components) in the shape of a maple leaf; we were curious what properties it had. A presentation of the link is given below. How would one obtain a presentation of the link group? Does the link group of this particular link correspond to any relatively nice group?","['group-theory', 'knot-theory']"
130167,Improper integral of $\frac{x}{e^x-1}$,"This integral came up in an exercise on the estimation of the specific heat of a 1-D solid and is probably a standard integral, possibly one that can be solved by contour integration: \begin{equation}
\int_0^{+\infty} \frac{x}{e^x-1} dx
\end{equation} I have some rudimentary knowledge of contour integrals, but I can't come up with a proper path, also because of the many singularities along the imaginary axis. Any suggestion?","['improper-integrals', 'calculus', 'integration']"
130168,Multiple linear regression with interaction,"I'm doing a multiple linear regression with interacting variables. I'll give you an example: $y$=value, $x_1$=material, $x_2$=weight, $x_3$=color $x_1$ and $x_2$ are interacting variables but $x_3$ is not. Right now I'm using something like:
$$
y = a_0 + a_1x_1 + a_2x_2 + a_3x_3 + a_{12}x_1x_2 + u
$$
I'm pretty new to regression analysis so I wonder if there is any way to convert this formula to something like 
$$
y = a_0 + a_1x_1 + a_2x_2 + a_3x_3 + u
$$
so I can see how much effect $x_1$ and $x_2$ have simply by looking at $a_1$ and $a_2$? What I want to do is to just be able to look at the equation and understand how much 1 kg of extra weight adds in value without needing to calculate y. Splitting up the interaction term $a_{12}$ and distributing the effect over $a_1$ and $a_2$ if you guys understand what I mean. Maybe it's not possible or maybe there is a better regression method that is more suited for this, I don't know. I'd love to get some pointers from you guys. Thanks.","['statistics', 'regression']"
130179,"Calculating $f(0), f'(0), f''(0)$ for a function $f$ satisfying $\lim_{x \to 0} (1 + x + f(x)/x)^{1/x} = e^3$.","I'm trying to do the following problem and could use some help(from Apostol, Calculus, Volume I , 7.11 Ex. 33 p. 291): A funtion $f$ has a continuous third derivative everywhere and
  satisfies the relation $$ \lim_{x \to 0} \left(1 + x + \dfrac{f(x)}{x}\right)^{1/x} = e^3.$$ Compute $f(0), f'(0), f''(0),$ and $\lim_{x \to 0} \left(1 + \frac{f(x)}{x}\right)^{1/x}$. [ Hint: If $\lim_{x \to 0} g(x) = A$, then $g(x) = A + o(1)$ as $x \to 0$.] The book gives the following as answers: $f(0) = 0, f'(0) = 0, f''(0) = 4$ and the limit $= e^2$. I cannot seem to make any forward progress on this.  It is the last Exercise in a section of exercises on taking limits by using polynomial expansions of functions.  (This set of Exercises is immediately before the section on L'Hopital's rule... I don't know if that is applicable here, but a solution without it is appreciated since Apostol intends this to be done without it.) My initial attempts involve writing: $$\begin{align*}
& \lim_{x \to 0} \left(1 + x + \dfrac{f(x)}{x}\right)^{1/x} &= e^3.\\
\implies & \lim_{x \to 0} \left(e^{(1/x)\log(1 + x + f(x)/x)}\right) &= e^3.\\
\implies & \lim_{x \to 0} \left(e^{-\frac{\log x}{x} + \frac{1}{x} \log (x + x^2 + f(x))}\right) &= e^3.
\end{align*}$$ I wanted to do this to attempt to get to a point that I could write a polynomial expansion of $\log$ at $0$, but I can't seem to make any progress on that front.  Maybe there is a better way to simplify things? Thanks for any help.  Full solutions or hints are equally welcome.","['calculus', 'limits']"
130180,Estimation for modulus of characteristic function,"It is well-known that if $\xi$ is an absolutely continuous random variable with characteristic function $\phi(t)$, then for each $\epsilon>0$ one has $\sup\limits_{|t|>\epsilon}|\phi(t)|<1$ (sometimes it is called Cramer's theorem). However, if we can say something about existence of moments of the r.v. then this result can be improved. For instance, if $\mathsf E \xi=0,\, \mathsf E \xi^2<\infty$, then one can conclude(if I do not mistake) that exists such $K>0$ that for all $s \in (0,1)$ $\sup_{|t|>s}|\phi(t)|<e^{-Ks^2}$.
The questions are the following: is the statement above true? How one can prove it(I tried to do something with Taylor's expansion and the estimation like $|e^{is}-1-is|\leq |s|^2/2$, but I did not manage to prove it)? Can there be generalization for r.v. that possesses higher moments? Thanks","['probability-theory', 'inequality', 'probability']"
130182,Is it equation true - limit,"Is it true that :
$\lim_{ x\to\infty }  \left( 1+\frac{f \left( x \right) }{x} \right) ^x = \exp \left(  \lim_{ x\to\infty } f \left( x \right)   \right)$ ? Assumption is that limit of $\lim_{ x\to\infty } f(x)$ exists.","['limits', 'analysis']"
130192,Method for estimating the $n^{th}$ derivative?,"When using numerical analysis, I often find that I am required to estimate a derivative (e.g. when using Newton Iteration for finding roots).  To estimate the first derivative of a function $f(x)$ at a point $x_0$ (assuming that $f(x)$ is continuous at $x_0$), one can use the slightly-modified (to avoid bias to one side) first principles formula for derivatives, shown below. For small $h$: $$f'(x_0)\approx\frac{f(x+h)-f(x-h)}{2h}\tag{1}$$ Using this method, we can estimate $f^{(n)}(x)$ recursively with, for sufficiently small $h$: $$f^{(n)}(x_0)\approx\frac{f^{(n-1)}(x+h)-f^{(n-1)}(x-h)}{2h}\tag{2}$$ The problem I have with $(2)$ is that each recursion produces a loss of accuracy that builds up.  As well, to estimate $f^{(n)}(x_0)$, the function $f(x)$ is required to be computed $2^n$ times. Is $(2)$ the best method for approximating the $n^{th}$ derivative of $f(x_0)$ numerically or are there more efficient methods?","['approximation', 'estimation', 'derivatives', 'numerical-methods']"
130193,Isometries of $\mathbb{S}^n$,How to prove so elementary (elementary = without using the concept of geodesic) that an isometry of $\mathbb{S}^n$ is a restriction on $\mathbb{S}^n$   of  an isometry of $\mathbb{R}^{n+1}$  ? EDIT: You will isometry  with respect metric is induced by $\mathbb{R}^{n+1}.$,"['riemannian-geometry', 'reference-request', 'differential-geometry']"
130194,How can I find power series of $f(x)$?,"$$f(x)=\dfrac{1}{1+\dfrac{x}{1+\dfrac{x^2}{1+\dfrac{x^3}{1+\dfrac{x^4}{\ddots}}}}}$$ How can a power series be found given the continued fraction $f(x)$? I'm trying to find $f(x) =1+a_1x+\dfrac{a_2x^2}{2!}+\dfrac{a_3x^3}{3!}+\dfrac{a_4x^4}{4!}+\cdots$ I tried some ways to define $f(x)$ but I could not find the general patern. It goes to complex patern after $n=3$ in my approach. I think that I need another approach to problem. $$\begin{align}
&f_1(x)=\frac{1}{1+x}=1-x+x^2-x^3+\ldots\\
&f_2(x)=\frac{1}{1+\frac{x}{1+x^2}}=\frac{1+x^2}{1+x+x^2}\\
&f_3(x)=\frac{1}{1+\frac{x}{1+\frac{x^2}{1+x^3}}}=\frac{1+x^2+x^3}{1+x+x^2+x^3+x^4}\\
&\lim_{n\to \infty} {f_n(x)}=f(x)
\end{align}$$ Could you please give me hand on how to find the patern of this function?","['power-series', 'continued-fractions', 'functions']"
130206,An enlightening proof of a specific combinatorial identity,"Concerns about the arithmetic genus of projective hypersurfaces led me to make the following combinatorial conjecture:
$${d-1\choose n+1} =\sum_{i=0}^{n+1} (-1)^{n+i+1} {d\choose i}$$
for $d \geq 1$, $n \geq 0$. If I made no mistakes in my code, I was also able to find some reasonably strong numerical evidence that this is, in fact, an identity. Unfortunately, my skill at combinatorics is sufficiently poor that while I might be able to prove the statement with a fair amount of effort, I doubt I could find an enlightening proof. Can someone supply an enlightening proof of the statement above? Obviously, a counterexample would also suffice, but I doubt there is one. Additional note: I did not see a reasonable way to search for this specific identity, so it is quite possible this is an exact duplicate of another question, in which case my question should be closed.","['binomial-coefficients', 'combinatorics']"
130207,Finding Null Space Basis over a Finite Field,"I have more a systems background, but I have a math-y type question so I figured I'd give it a shot here...This is more of an implementation question, I don't need to prove anything at the moment. I'm trying to find the basis of the null space over $\mathbb{F}_2^N$ for a given basis quickly and I was hoping someone here would know how. For example if I have the following basis in $\mathbb{F}_2^{16}$: $$ 
\left( 
\begin{array}{cccccccccccccccc}
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\
0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 \\
0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\
\end{array} 
\right)
$$ How would I find the null space basis for this matrix? If I put my basis into reduced row echelon form, I could find it easily, but for my particular problem I cannot do that. I know there are exhaustive search methods, but the matrices I'm dealing with can be quite large, which make those impractical. BEGIN EDIT @Neil de Beaudrap, It has to do with the fact that I am actually splitting up the vector space and using part of it for another purpose. If I change this matrix with elementary row operations and put it into reduced-row-echelon form it messes things up.... I am unfamiliar with column operations, could you explain in a bit more detail what you are talking about? Thanks! END EDIT","['matrices', 'linear-algebra', 'coding-theory']"
130243,Please check my answer to $\sum\limits_{i=1}^n \frac{\sin{(ix)}}{i} < 2\sqrt{\pi}$,"$$\sum_{i=1}^n \frac{\sin{(ix)}}{i} < 2\sqrt{\pi}$$ I have this answer, please let me know if there is a more beautiful proof. My answer: at first, we prove two inequalities: If $x\in (x,\pi)$ then $\sin x \leq x$ When $x\in(0,\frac{\pi}{2})$, $\sin x \geq \frac{2x}{\pi}$ 1) first, let $y = \sin x -x $ $y^{\prime} = \cos x -1 \leq 0$ so $\sin x - x \leq \sin 0 -0 = 0$ which can be rewritten as $\sin x \leq x$ 2) Let $y=\sin x - \frac{2x}{\pi}$ thus $y^{\prime} = \cos x - \frac{2}{\pi}$ because $x\in (0, \frac{\pi}{2})$ so y at first decreases and then increases on the boundary of $x \in (0,\frac{\pi}{2})$ so $ \sin x - \frac{2}{\pi}\leq \max \{{\sin 0 - \frac{2}{\pi}0, \sin (\frac{\pi}{2} - \frac{2}{\pi}\frac{\pi}{2}) \}}$ so $\sin x \leq \frac{2x}{\pi}$ Then select $M\in N$ $\frac{\sin (mx)}{m} + \frac{\sin ((m+1)x)}{m+1} + \ldots + \frac{\sin ((m+n)x)}{m+n} \leq  \frac{\sin (mx)}{m} + \frac{\sin ((m+1)x)}{m} + \ldots + \frac{\sin ((m+n)x)}{m} $ => $\frac{1}{2M} \times \frac{\sin ((m-\frac{1}{2})x) - \sin ((n+\frac{1}{2})x)}{\sin \frac{x}{2}} < \frac{1}{m \times \sin \frac{x}{2}} \times \sin x + \frac{\sin 2x}{2} + \ldots + \frac{\sin ((m-1)x)}{m-1} < x + \frac{2x}{2} + \ldots + \frac{(m-1)x}{m-1} $ so just need to prove that $(m-1)x + \frac{1}{m \times \sin \frac{x}{2}} \leq 2\sqrt{\pi}$ select M which satisfies $ \frac{\sqrt{\pi}}{x} \leq m < \frac{\sqrt{\pi}}{x} + 1 $ so $ (m-1)x < [ \frac{\sqrt{\pi}}{x} \times x = \sqrt{\pi} ] $ thus $\frac {1}{m \times \sin(\frac{x}{2})}\leq[ \frac{1}{\sqrt{\pi}}\times \frac{2}{\frac{ \sin (0.5x)}{0.5x}} = \frac{1}{\sqrt{\pi}} \times \frac{2}{\frac{\sin 0.5x}{0.5x}} ]$ because $x\in (0, \pi)$ thus $\frac{x}{2} \in (0, \frac{\pi}{2})$ $ (m-1)x + \frac{1}{m \times \sin(0.5x)} \leq 2\sqrt{\pi} $ thanks, for viewing and commenting. ps. I'm still learning latex and mathematics, so my answer isn't pretty to read, nor is the latex I wrote.","['inequality', 'trigonometry', 'fourier-series', 'calculus']"
130287,Set of subsets notation. [duplicate],This question already has answers here : What's the meaning of a set to the power of another set? (2 answers) Closed 10 years ago . Why is it that we denote the set of all subsets of $A$ by $2^A$? Is there any historical or logical cause that motivated this notation?,"['notation', 'elementary-set-theory']"
130297,Generator of Normal Group,"Let $G$ be a group and $S,T \subset G$ are subsets of $G$  such that $\langle T\rangle =G$ and $\langle S \rangle=N$. Does the following hold: $N$ is normal in $G$ iff for all $t\in T$ we have $tSt^{-1} \subseteq N$ Here $\langle X \rangle$ denotes the subgroup generated by $X$ (the smallest subgroup of $G$ containing $X$) I only have the answer for $G$ is finite group. Edit: Sorry, now the question is right.",['group-theory']
130300,Compute floor sum,"Write as a single sum: Given $\{a_n\}_n$, $a_i \in \mathbb{Q}$, $0 \lt a_i \le a_{i+1}$ $\sum_{i=1}^{n} \sum_{j=i+1}^{n} \lfloor a_j - a_i \rfloor$ I am not sure if this is possible. I know that if there is no floor, then the sum can be written as: $\sum_{i=1}^{n} (2i-n-1) a_i$","['sequences-and-series', 'number-theory', 'combinatorics']"
130301,A question on proof writing and set builder notation.,"I'm proving that if $A , B\subset S$ then $$A\subset B \Leftrightarrow A\cup B=B$$
$$A\subset B \Leftrightarrow A\cap B=A$$ I go as follows: Suppose it is true that $A\subset B$. This implies that if $x\in A$ then it is also true that $x \in B$, this is to say $$\tag{1} x\in  A\Rightarrow x \in B$$ This means that, using the set builder notation $$B=\{ x:x\in B\}=\{x:x\in B \vee  x\in A\}=A\cup B$$ Now suppose $A\cup B=B$ is true. This means that $$A\cup B=B=\{ x:x\in B\}=\{x:x\in B \vee x\in A\}$$ But then this means that if $x\in A$ then it is also true that $x\in B$, which means that $A\subset B$. (The proof has been now corrected.)",['elementary-set-theory']
130303,Proving a certain binomial identity with three parameters,"I would like to prove the following identity: $$\sum_{m\geq 0} (-1)^{i-m}{m+k \choose m} {i-1 \choose m-1}{m+k+1 \choose j}  = \sum_{m\geq 0} {m+k \choose k}{k+1 \choose i-m}{k+1 \choose j-m}$$ for fixed $i,j \geq 1$, $k\geq 0$.  If it helps, I have a combinatorial interpretation of the RHS:  it counts the number of arrangements of $i$ $a$'s, $j$ $b$'s, and $k$ $c$'s so that the substrings ""aa"", ""bb"" cannot occur (there is no restriction on the c's.)  This I can show, although I have not found a direct combinatorial proof.  If it's helpful I can give my proof for the right-hand side; I derived it from the so-called Carlitz-Scoville-Vaughan theorem which I found out about recently on Math Overflow. It is related to this question; I have a proof for it, contingent on proving this binomial identity.  I ""stumbled"" across this through some very optimistic guessing, but I'm not sure how to prove it.  I thought of using WZ theory, but as far as I know that only applies to identities with one parameter, not more general ones - but I'd be very happy if an algorithmic verification was possible. Other possibilities:  Interpret the left-hand side as an inclusion-exclusion argument, or a determinantal formula.  Or, find a recurrence that both sides satisfy.","['binomial-coefficients', 'combinatorics']"
130310,Equidistribution of roots of prime cyclotomic polynomials to prime moduli,"Here is a relevant - and longstanding, I'm told - conjecture. Let $f \in \mathbb{Z}[x]$ be irreducible and of degree > 1. Set $E_p = \{x/p \: | \: 0 \leq x < p, f(x) \equiv 0 \: (p) \}$ = { normalised least positive residues of zeros of $f$ in $\mathbb{F_p}$} and $E = \bigcup_{p} E_p \subset [0,1]$ Conjecture: $E$ is equidistributed mod 1 I understand that this is very difficult problem. (The only progress that I'm aware of is that the quadratic case was proven by Duke, Friedlander, and Iwaniec in 1995 - and that if one assumes the Bouniakowsky Conjecture then it follows that $E$ is dense in the unit interval.) A while back, I tried to see how far I could get in proving the above for the prime cyclotomic polynomials (i.e. the polynomials $\phi_{\ell}(x) = \frac{x^{\ell}-1}{x-1} = x^{\ell - 1} + \cdots + x + 1$ for prime $\ell$). I figured that in this case, there would be more to grab onto and work with as the $E_p$ can be concretely described. The mod $p$ roots of $\phi_{\ell}$ are the primitive $\ell$-th roots of unity in $\mathbb{F}_p$. One can show that these roots of unity exist if and only if $p \equiv 1 \: (\ell)$ - and in that case we have $|E_p| = \ell - 1$. Let $S_p = \displaystyle\sum_{\zeta/p \in E_p} e(\zeta/p)$, where $e(x) = e^{2 \pi i x}$, and let $\pi_{\ell}(x) = |\{p \leq x \: | \: p \equiv 1 \: (\ell) \}|$. We can also write 
$S_p = \displaystyle\sum_{k = 1}^{\ell - 1} e(\alpha^{k \cdot \frac{p-1}{\ell}}/p)$, where $\mathbb{F}_p^{\times} = <\alpha>$ Then, by Weyl's equidistribution theorem, the conjecture in our case is equivalent to the equality $0 = \displaystyle\lim_{x \to \infty} \frac{1}{\pi_{\ell}(x)}\displaystyle\sum_{p \equiv 1 (\ell), p \leq x}S_p$ Does anyone have any ideas/suggestions as to how one could proceed? Would you know of any tools that might be efficient in dealing with the limiting average of the $S_p$'s? Any input would greatly be appreciated. Addendum: There seems to be relatively little cancellation occurring 'within' each $S_p$. That is, the second-most trivial estimate of our sum, $\displaystyle\sum_{p \equiv 1 (\ell), p \leq x}|S_p|$, is not (seemingly) $o(\pi_{\ell}(x))$.","['exponential-sum', 'equidistribution', 'analytic-number-theory', 'number-theory']"
130314,Encyclopedia of Groups,"I was wondering whether or not there was an online encyclopedia of groups--finite or infinite. If there isn't, would you suppose that such a thing would be useful?","['online-resources', 'reference-works', 'group-theory', 'soft-question']"
130322,Connected Reinhardt Domain which is not complete,"Can i have an example of connected Reinhardt domain in $C^n$ which contains zero.  But it is not complete. Complete means: For $w= (w_1,..w_n)\in D$, if $z$ is such that $|z_j|\leq |w_j$ for all $j$ implies $z\in D$.","['complex-geometry', 'several-complex-variables', 'complex-analysis']"
130325,Overlapping Events,I have an event that will start exactly X times within a single 24 hour period. Each event lasts Y milliseconds. The start time of an individual event is uniform randomly distributed throughout the 24 hour period.  The start time of each event is independent of the others. What is the probability that at any point during the 24 hour period Z (or more) events are currently occurring and overlapping each other?,"['statistics', 'probability']"
130330,Find a set of $n$ points such that any triangulation gives a vertex of degree $n-1$.,"Find a set of $n$ points such that any triangulation gives a vertex of degree $n-1$, i.e. the vertex is incident to $n-1$ edges. It is easy to do if we place $n-1$ points on a horizontal line and the remaining point above the line. My question is: are there more general set of points that can do? Especially points that are in general position: no 3 points on same line, no 4 points on same circle, etc.",['geometry']
130341,Integral from ball to sphere,"Well I am working something, which deals with the following problem: For example, I want to compute an integral $\int_{B(0,R)}f(x)dx$, where $B(0,R)=\{x\in\mathbb R^n:\;|x|\leq R\}$ and $S(0,R)=\{|x|=R\}$. Now we have the following formula $$
\int_{B(0,R)}f(x)dx=\int_0^Rdr\int_{S(0,r)}f(y)dS(y)
$$ My question is: I assume only that $f(x)$ is a measurable respect to Lebesgue measure and non-negative. Does the above formula hold? How do understand exactly the integral $\int_{S(0,r)}f(y)dS(y)$? Could I consider $\int_{S(0,r)}f(y)dS(y)$ as the Lebesgue integral of a Lebesgue measurable function $f$ on a Lebesgue measurable $S(0,r)$?","['measure-theory', 'integration']"
130352,Last non Zero digit of a Factorial,"I ran into a cool trick for last non zero digit of a factorial . This is actually a recurrent relation which states that: If $D(N)$ denotes the last non zero digit of factorial, then $$D(N)=4D\left(\left\lfloor{\frac N5}\right\rfloor\right)\cdot D(\mbox{Units digit of $N$})  \qquad \mbox{(If tens digit of $N$ is odd)}$$
$$D(N)=6D\left(\left\lfloor{\frac N5}\right\rfloor\right)\cdot D(\mbox{Units digit of $N$})  \qquad \mbox{(If tens digit of $N$ is even)}$$ Where $\left\lfloor\cdots\right\rfloor$ is greatest integer function. I was wondering, if anybody could explain why this works?","['factorial', 'elementary-number-theory', 'number-theory']"
130356,"Show that $x \leq f(x) \leq 2x, \forall x\geq0$","Prove: $$x \leq f(x) \leq 2x, \forall x\geq0$$ conditions: $f$ is differentiable $f(0) = 0$ $1 \leq f'(x) \le 2, \forall x\ge0$ I've tried to do it by limit defn but couldn't seem to get to the right solution: $$ 1 \le  \lim_{x \to c} \frac{f(x)-f(c)}{x-c} \le 2$$ how do i manipulate them in such a way that I get $$x \leq f(x) \leq 2x, \forall x\geq0 $$ I've also noticed that $f(x)$ is an increasing function as $f'(x) > 0$. Is this information of any use?",['real-analysis']
130357,Arc Length of $x^{\frac{2}{3}}+y^{\frac23}=1$,"How do I find the arch length of $$x^{\frac{2}{3}}+y^{\frac23}=1$$ The hint given was ""4x the arc in first quadrant"" I think I am supposed to use the formula: $$L=\int^b_a \sqrt{1+(f'(x))^2} dx$$ I tried plotting the equation in a graphing utility like https://www.desmos.com/calculator , which results in an error. So I tried expanding the term : $$y=\sqrt{(1-x^{\frac{2}{3}})^3} = \sqrt{1-2x^{\frac{2}{3}}-x^2}$$ Then how do I proceed? Complete the square? Doesn't look likes its in an appropriate form?",['integration']
130374,How to show this random variable equals its conditional expectation?,"Suppose $(\Omega,\mathcal F,\mathbb{P})$ is a probability space and that $\mathcal G$ is a sub-sigma-algebra of $\mathcal F$. If $X$ is an integrable, non-negative random variable with the same distribution as $\mathbb{E}[X|\mathcal G]$, how does one show that $X=\mathbb{E}[X|\mathcal G]$ a.s.? Thank you. (I can do the case where $X$ has finite variance, but the method doesn't seem to extend to this...) Edit I've just found a hint for this question (but remain stuck): Show that $f(X)=f(\mathbb{E}[X|\mathcal{G}])$, almost surely, where $f(x)$ equals $1$ if $x>0$, $0$ if $x=0$ and $-1$ otherwise.",['probability-theory']
130409,Slowing down divergence 2,"Let $f(x)$ and $g(x)$ be positive nondecreasing functions such that
$
\sum_{n>1} \frac1{f(n)} \text{ and } \sum_{n>1} \frac1{g(n)}
$ diverges. (Why) must the series $$\sum_{n>1} \frac1{g(n)+f(n)}$$ diverge?","['convergence-divergence', 'sequences-and-series']"
130417,Covariant derivative,"In my book the author makes the remark: If $X,Y$ are smooth vector fields, and $\nabla$ is a connection, then $\nabla_X Y(p)$  depends on the Value of $X(p)$ and the value of $Y$ along a curve, tangent to $X(p)$. When I got it right, then we can consider a curve $c:I\rightarrow M$ with $c(0)=p$ and $c'(0)=X_p$. I was wondering, why this is true. When I consider a coordinate representation around the point p, i.e. $X=\sum x^i\cdot \partial_i$ and $Y=\sum y^i \cdot \partial_i$, then we can calculate that $\nabla_X Y(p)$ depends on: $x^i(p)$, $y^i(p)$ and $X_p(y^i)$. This again is only depending on $X_p$ and the values of $y^i=Y(x^i)$ in a arbitrary small neighborhood of $p$.  But I cannot see any curve ... I hope you can help me! Regards",['differential-geometry']
130442,Evaluating Limit Question $\lim\limits_{n\to \infty}\ \frac{1+\sqrt[2]{2}+\sqrt[3]{3}+\cdots+\sqrt[n]{n}}{n}=1$?,Does $\displaystyle\lim_{n\to \infty}\ \frac{1+\sqrt[2]{2}+\sqrt[3]{3}+\cdots+\sqrt[n]{n}}{n}=1$? Thanks a lot for your time and help.,"['radicals', 'calculus', 'real-analysis', 'limits']"
130444,Riemann zeta function and uniform convergence,"A question in a past paper says prove that this series converges pointwise but not uniformly $$\xi(x):= \sum_{n=1}^\infty \frac{1}{n^x} .$$ But I thought that it did converge uniformly to some function $\xi(x):(1,\infty) \to \mathbb{R}$. Here's why; if you work on $(1+\delta,\infty)$ for $\delta >0$, then clearly we have $$\left\|\frac{1}{n^x}\right\|_\infty = \frac{1}{n^{1+\delta}}$$ and clearly $\sum \frac{1}{n^{1+\delta}}$ converges so by the Weierstrass M-test $\sum_{n=1}^\infty \frac{1}{n^x}$ converges uniformly on $(1+\delta,\infty)$ so letting $\delta \to 0$ gives $\sum_{n=1}^\infty \frac{1}{n^x}$ converging uniformly on $(1,\infty)$?","['riemann-zeta', 'real-analysis', 'analysis']"
130449,Orbits of adjacency matrices under conjugation by permutation matrices.,"(Disclaimer: I am new here, so be patient with my mistakes, but I welcome corrections, advice or comments.) I am interested in if anyone knows of ways of characterizing the orbits of an adjacency matrix of a multi-graph when it is conjugated by a permutation matrix. To clarify: The matrices in question are symmetric, with zero diagonal (sometimes called hollow symmetric), and all entries non-negative integers. Conjugation by a permutation matrix will give another adjacency matrix with all the same properties. From a certain perspective this can be visualized as re-drawing the graph that gave rise to the adjacency matrix (in a manner that is isomorphic in the graph-theory sense). I want to know if, given two adjacency matrices with the same dimensions, is it possible to determine if they lie in the same orbit (without re-constructing the necessary permutation matrices). However, I am interested in any results that are related to this question, since I believe the answer is not known (correct me if I am wrong here).","['matrices', 'graph-theory', 'group-actions', 'abstract-algebra']"
130451,"Verify for $f(x,y)$, homogeneous of degree $n$: $xf_x+yf_y=nf$","$F(x,y)$ is homogenous of degree n if $f(tx,ty)=t^nf(x,y)$. Verify that $xf_x(x,y)+yf_y(x,y)=nf(x,y)$ $x^2f_{xx}(x,y)+2xyf_{xy}+y^2f_{yy}(x,y)=n(n-1)f(x,y)$ Looks like I need enlightenment again... Hopefully, its not another embarrassingly simple thing I missed out in another question What I tried: $f_x(x,y)=t^nf_x(x,y)$ $f_y(x,y)=t^nf_y(x,y)$ $xt^nf_x(x,y) + yt^nf_y(x,y) = t^n(xf_x(x,y) + yf_y(x,y))$ Doesn't look like I am doing the right thing?","['ordinary-differential-equations', 'integration']"
130457,continuous and strictly increasing implies differentiable,"I am not sure if this is true, but intuitively it seems that if a function is strictly increasing and it is also continuous...it is differentiable. It may be because there are no bumps like in the absolute value.",['analysis']
130471,Convergence/divergence of $\sum\frac{a_n}{1+na_n}$ when $a_n\geq0$ and $\sum a_n$ diverges,"A question from Rudin (Principles) Chapter 3: Let $a_n\geq0$ and $\sum a_n$ diverges. What can be said about convergence/divergence of $\sum\frac{a_n}{1+na_n}$? This one is being recalcitrant. Given that $x>y$ implies $\frac{x}{1+nx}>\frac{y}{1+ny}$ and when $a_n=1/n\log n$ the sum in question diverges, it seems plausible that in general the sum will always diverge, but I can't get a proof out. If it does diverge, it does so pretty slowly as
$$\frac{a_n}{1+na_n}=\frac{1}{n}-\frac{1}{n+n^2a_n}\leq\frac{1}{n}.$$","['sequences-and-series', 'real-analysis']"
130472,Residue integral: $\int_{- \infty}^{+ \infty} \frac{e^{ax}}{1+e^x} dx$ with $0 \lt a \lt 1$.,"I'm self studying complex analysis. I've encountered the following integral: $$\int_{- \infty}^{+ \infty} \frac{e^{ax}}{1+e^x} dx \text{ with } a \in \mathbb{R},\  0 \lt a \lt 1. $$ I've done the substitution $e^x = y$. What kind of contour can I use in this case ?","['definite-integrals', 'residue-calculus', 'complex-analysis']"
130493,Cohomology of quasi-coherent sheaves with respect to pushforward question (Exercise in Hartshorne),"I am confused about something in Exercise 4.1 of Chapter 3 of Hartshorne.  It asks us to prove : Let $f : X \rightarrow Y$ be an affine morphism of Noetherian, separated schemes.  Show that for any quasi-coherent sheaf $\mathcal{F}$ on $X$, there are natural isomorphisms $$H^i(X, \mathcal{F}) \cong H^i(Y, f_* \mathcal{F}) \ \ \forall i \geq 0$$ I seem to have a proof without assuming that $f$ is affine or that the schemes are separated (but I need that $X$ is Noetherian).  So I know my proof must be bogus, but I can't figure out why. Proof : Exercise 3.6 of Chapter 3 of Hartshorne says that we can compute cohomology of a quasi-coherent sheaf as the derived functors of $\Gamma(X, \cdot)$, considered a a functor from the category of quasi-coherent sheaves to abelian groups.  So let $\mathcal{F} \rightarrow \mathcal{I}^{\cdot}$ be an injective resolution of $\mathcal{F}$ in the category of quasi-coherent sheaves.  Then $f_* \mathcal{F} \rightarrow f_* \mathcal{I^{\cdot}}$ is a flasque resolution of quasi-coherent sheaves of $f_* \mathcal{F}$  (pushforward of quasi-coherent is quasi-coherent since $X$ is Noetherian).  Moreover, we clearly have $\Gamma(X, \mathcal{I}^{\cdot}) = \Gamma(Y, f_* \mathcal{I}^{\cdot})$.  Therefore, we get the same cohomology groups (this is the same method of proof of Lemma 2.10, Chapter 3, of Hartshorne). What did I do wrong? Sincerely, David",['algebraic-geometry']
130497,Proving that: $\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$,"Let $a$ and $b$ be positive reals. Show that
$$\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$$","['radicals', 'real-numbers', 'real-analysis', 'means', 'limits']"
130502,Affine schemes are quasi-separated,"Exercise 6.1.G of Ravi Vakil's notes asks to prove that all affine schemes are quasi-separated, where quasi-separated schemes are defined as schemes where the intersection of any two quasi-compact open subsets is quasi-compact, or equivalently the ""intersection of any two affine open subsets is a finite union of affine open subsets."" Can someone give a hint or solution?","['algebraic-geometry', 'schemes']"
130503,"Bounded Linear functional on $\mathcal{L}_{2}[a,b]$","Lets say that $f:[a,b] \rightarrow \mathbb{R}$ is a measurable function such that 
$H: \mathcal{L}_{2}[a,b] \rightarrow \mathbb{R}$ defined as $H(g) = \int_{a}^{b}fg$ is finite for all $g \in \mathcal{L}_{2}[a,b]$ I was wondering if $H$ is a bounded linear functional on $\mathcal{L}_{2}[a,b]$",['functional-analysis']
130511,Noetherian module implies finite direct sum of indecomposables?,"Noetherian module implies finite direct sum of indecomposables? Let R be a ring and let M be a Noetherian R-module. If M is indecomposable we are done. Otherwise, M is a direct sum of two proper and non-trivial submodules. If M were also Artinian, I could use induction on the (finite) length of M and prove the result in the title. I don't know how to proceed in the general case.
Thanks in advance for any ideas!","['modules', 'abstract-algebra']"
130534,"Given an arbitrary number of points, how do you find an equidistant center?","Given an arbitrary set of points on a Cartesian coordinate plane, is there a generalized formula to find the closest point that is equidistant from all the given points? My first guess was finding the centroid, which is fairly easy to calculate, but the centroid of a polygon isn't equidistant from all its vertices.","['geometry', 'graph-theory', 'coordinate-systems']"
130547,"Getting the sequence $\{1, 0, -1, 0, 1, 0, -1, 0, \ldots\}$ without trig?","What's the simplest way to write a function that outputs the sequence: {1, 0, -1, 0, 1, 0, -1, 0, ...} ... without using any trig functions? I was able to come up with a very complex sequence involving -1 to some complicated formula, but I was hoping there is a more simple solution. $n$ should start at 0 and go to infinity. Update: All the solutions you guys provided are great! I wasn't aware there were so many of them. I should have mentioned that I prefer a solution which doesn't use recursion; imaginary numbers; matrices; functions with if statements; or functions such as ceil , floor , or mod . I'm looking for something using basic algebra: addition/subtraction, multiplication/division, exponents, etc. However, I will accept anything since I didn't  include this clause originally. This is what I came up with: $$a_n=\frac{\left(-1\right)^n+1}{2}\cdot \left(-1\right)^{\left(\frac{n}{2}-\frac{\left(-1\right)^{n+1}+1}{4}\right)}$$ Is there a less complicated way (i.e. fewer terms) to get this same sequence?","['sequences-and-series', 'pattern-recognition']"
130551,Do Conformal Maps of Open Sets of the Complex Plane Always Take Boundary to Boundary?,"Do Conformal Maps of Open Sets of the Complex Plane Always Take Boundary to Boundary? For instance I'm trying to create a conformal map which takes the slit open unit disk in the complex plane to the open unit disk, and takes the boundary, that is, the set $(-1,0]\cup e^{i\theta}$ for $0\leq\theta<2\pi$ to the set $e^{i\phi}$ for $0\leq\phi<2\pi$. I currently have a conformal map which takes the slit open unit disk to the open unit disk, can I be confident that it takes boundary to boundary?",['complex-analysis']
130554,"Gaussian Kernels, Why are they full rank?","I'd like to ask why a Gaussian Kernel's Gram Matrix is of full rank. Lots of texts and articles always write about assuming this is the case, and refer me to an unavailable research article online, but I haven't been able to find a single source that sheds light on why this is the case.","['linear-algebra', 'reference-request']"
130555,Conditional probability distribution with Gaussian noise,"If I have a relationship as follows: $$Y = a X + G(0,\sigma^2),\text{ so }y = a X + \text{some Gaussian noise}.$$ The conditional probability distribution of $y$ given $x$, i.e. $P(y|x)$, is equal to a Gaussian with mean $= a X$ and variance $= \sigma^2$. I intuitively understand this as the expected value for $y$ should be $a X$ and this will vary due to the noise with the same variance of the noise. Is there a formal proof for this? Thanks,
Aly","['statistics', 'normal-distribution', 'machine-learning']"
130561,Prove or Disprove that $\left|\frac{e^{2i\theta} -2e^{i\theta} - 1}{e^{2i\theta} + 2e^{i\theta} -1}\right| = 1$,"Prove or disprove that $$\left|\frac{e^{2i\theta} -2e^{i\theta} - 1}{e^{2i\theta} + 2e^{i\theta} -1}\right| = 1$$ This is a step in an attempt to solve a much larger problem, thus I'm fairly sure it's true but not absolutely sure.  It looks like it should be simple but it's resisted all my attempts so far.",['complex-analysis']
130562,Probability - Coin Toss - Find Formula,"The problem statement, all variables and given/known data: Suppose a fair coin is tossed $n$ times. Find simple formulae in terms of $n$ and $k$ for a) $P(k-1 \mbox{ heads} \mid k-1 \mbox{ or } k \mbox{ heads})$ b) $P(k \mbox{ heads} \mid k-1 \mbox{ or } k \mbox{ heads})$ Relevant equations: $P(k \mbox{ heads in } n \mbox{ fair tosses})=\binom{n}{k}2^{-n}\quad (0\leq k\leq n)$ The attempt at a solution: I'm stuck on the conditional probability. I've dabbled with it a little bit but I'm confused what $k-1$ intersect $k$ is. This is for review and not homework. The answer to a) is $k/(n+1)$ . I tried $P(k-1 \mbox{ heads} \mid k \mbox{ heads})=P(k-1 \cap K)/P(K \mbox{ heads})=P(K-1)/P(K).$ I also was thinking about $$P(A\mid A,B)=P(A\cap (A\cup B))/P(A\cup B)=P(A\cup (A\cap B))/P(A\cup B)=P(A)/(P(A)+P(B)-P(AB))$$","['statistics', 'probability']"
130591,Checkerboard coloring problem,Consider a $n\times n$ checkerboard. Each cell can be colored white or black. $n$ is even. How many configurations are there so that each row and each column have an odd number of white cells?,"['matrices', 'combinatorics']"
130597,what is the best way to win: every 1000 submission will win,"I have a question about probability. The game is like this: Every $1000$th submission will win, but the players don't know how many
  submissions were made before. Is it better for a player to throw all of his $100$ credits in one time or is it better to throw one then wait then submit the next... and so on?",['probability']
130599,Boolean algebras of projections,"Suppose $X$ is a Banach space with an unconditional basis $(e_n)$. Then, one may easily define a Boolean algebra of projections in $\mathcal{L}(E)$ which is isomorphic to the power-set of $\mathbb{N}$ (by a Boolean algebra of projections I understand a family of bounded idempotents on a Banach space which is a Boolean algebra under operations $P\wedge Q = PQ$ and $P\vee Q = P+Q-PQ$, zero-element equal to zero operator and unit equal to the identity on $X$). What are sufficient conditions for a Banach space to have a complete Boolean algebra of projections? Is there a separable Banach space $X$ without unconditional basis with some Boolean algebra of projections isomorphic to the power-set of $\mathbb{N}$?","['boolean-algebra', 'banach-algebras', 'functional-analysis', 'banach-spaces']"
130613,Computing $ \sum\limits_{i=1}^{\infty}\sum\limits_{j=1}^{\infty} \frac{(-1)^{i+j}}{i+j}$,"I would like to compute: $$ \sum_{i=1}^{\infty} \sum_{j=1}^{\infty} \frac{(-1)^{i+j}}{i+j}$$ I wanted to use Fubini's theorem for double series but $$ \frac{(-1)^{i+j}}{i+j}_{(i,j)\in\mathbb{N*^2}}$$ is not a summable family for $$ \forall i>0$$ $$ \sum_{j=1}^{\infty} \frac{1}{i+j}=\infty$$ What am I supposed to do?","['sequences-and-series', 'limits']"
130618,Masters in Actuarial Science,"I am applying to a grad school for the Masters in Actuarial Science. Now i am getting cold feet. I do love math, i was always good in math  (not excellent or a genius). Did all adv. calculus classes in College, financial, economics. I have degrees in Computer programming (AAS) and Information System Management (BS) in 2002.
Worked in the financial world for a quite a while.  I was always interested in getting a degree in that field, even if  i don't become an actuary. My problem: I think i forgot everything about maths, it has been over 10 yrs.I don't even think i might get admitted in the first place with this background. do you really have to be super excellent in maths or statistic to even think about embarking in that journey?","['statistics', 'education', 'probability', 'actuarial-science']"
130619,Idempotence and the Rao–Blackwell theorem,"Original question: In the Wikipedia article on the Rao–Blackwell theorem , we read: In case the sufficient statistic is also a complete statistic, i.e., one which ""admits no unbiased estimator of zero"", the Rao–Blackwell process is idempotent. and indeed this is commonly heard and quite easy to prove (just apply definitions). Suppose the hypothesis of completeness were dropped.  What would be an example in which idempotence would not hold? Later edit for the sake of self-containment: Consider a parametrized family of probability distributions of a random variable $X$, with parameter $\theta$.  In typically seen examples $X$ is a sequence $X_1,\ldots,X_n$ of independent identically distributed random variables and $\theta$ is a tuple of one or two scalars.  $\theta$ may be $(\mu,\sigma)$, the expectation and variance of a normal distribution, or may be $p$, the expected value of a Bernoulli-distributed random variable (thus e.g. the proportion of the population who have blue eyes), or may be the shape and scale parameters of a gamma distribution, etc. A statistic is an ""observable"" random variable, i.e. its value depends on $(X,\theta)$ only through $X$.  E.g. heights of 21-year-old men may be normally distributed with expectation $\mu$ and standard deviation $\sigma$; we observe the heights of $20$ such men, randomly chosen.  The average of the $20$ heights is observable; the amount by which that average exceeds the population average $\mu$ is unobservable.  Often unobservable things are those that depend on the values of parameters represented by Greek letters. A sufficient statistic is a statistic $T(X)$ such that the conditional distribution of $X$ given $T(X)$ does not depend on $\theta$. For example: If $X_1,\ldots,X_n\sim \operatorname{i.i.d.} N(\mu,\sigma^2)$, it can be shown that the conditional distribution of $X_1,\ldots,X_n$ given the pair $(X_1+\cdots+X_n,X_1^2+\cdots+X_n^2)$ does not depend on $(\mu,\sigma^2)$.  Thus that pair of random variables is a sufficient statistic for that family of probability distributions. If $X_1,\ldots,X_n\sim\operatorname{i.i.d.} \operatorname{Uniform}(0,\theta)$, then the conditional distribution of $X_1,\ldots,X_n$ given $\max\{X_1,\ldots,X_n\}$ does not depend on $\theta$.  Thus $\max\{X_1,\ldots,X_n\}$ is a sufficient statistic for that family of distributions. If $X_1,\ldots,X_n\sim\operatorname{i.i.d.}\operatorname{Gamma}(\alpha,\beta)$ (i.e. with density proportional to $x\mapsto x^{\alpha-1}e^{-x/\beta}$ for $x > 0$), then the conditional distribution of $X_1,\ldots,X_n$ given $(X_1+\cdots+X_n,\  X_1\cdots X_n)$ does not depend on $(\alpha,\beta)$.  Thus the pair whose components are the sum and the product is a sufficient statistic for that family of distributions. If $X$ has density proportional to $x\mapsto e^{-|x-\theta|}$ for $x\in\mathbb{R}$, and the parameter space is given by $\theta\in\{1, -1\}$, then $$\begin{cases} 1 & \text{if }X\ge 1 \\  -1 & \text{if }X\le -1 \\  X & \text{if }-1&ltX&lt1 \end{cases}$$ is a sufficient statistic. Intuitively, a sufficient statistic constains all information in the data that is relevant to estimating the value of $\theta$, if the model, which is the parametrized family of disetributions, is right.  But a sufficient statistic contains none of the information in the data that might indicate that the model is wrong (unless what's wrong is the support of the distribution; e.g. you get a negative number when you thought the Gamma family was right). A complete statistic is a statistic $S(X)$ (i.e. and observable random variable) that admits no unbiased estimator of $0$.  An unbiased estimator of $0$ is a function $f(S(X))$, where $f$ is not allowed to depend on $\theta$ whose expected value remains equal to $0$ regardless of the value of $\theta$.  Thus in the first example above, $(X_1,X_2)$ is a statistic, but is not complete, since the expectation of $X_1-X_2$ is $0$ regardless of the values of $\mu$ and $\sigma^2$.  The sum $X_1+\cdots+X_n$ is a complete statistic in that case. An unbiased estimator of $g(\theta)$ is a statistic (an observable random variable) whose expected value remains equal to $g(\theta)$ regardless of the value of $\theta$.  (The emphasis on ""observable"" is a sort of warning against a newbie mistake that says a biased estimator can be changed to an unbiased estimator by subtracting the bias from it.  That doesn't work since the bias is unobservable in all cases where one would need to seek an estimator.  The resulting random variable is not a statistic; it cannot be used as an estimator.) The Rao–Blackwell theorem says that if $h(X)$ is an unbiased estimator of $\theta$ and $T(X)$ is a sufficient statistic for $\theta$ (i.e. for the family of distributions parametrized by $\theta$) then $j(X)=\mathbb{E}(h(X) \mid T(X))$ is an unbiased estimator that is no worse, in the sense of mean-square-error or any of a variety of similar function, than is $h(X)$.  In typical applications, $j$ is far better than $h$, and that's why Rao–Blackwell is used. For example: suppose one wants a good unbiased estimator of the probability that no phone call comes into the switchboard in the next two minutes, when their arrival is a Poisson process with a rate of $\lambda\text{ per minute}$.  The parameter $\lambda$ is unobservable.  One wants to estimate $e^{-2\lambda}$.  The data consist of numbers of calls that arrived in each of $10$ disjoint two-minute intervals in the past $20$ minutes.  A crude unbiased estimator is $1$ if no call arrived in the first two minutes and $0$ if one or more arrived. Exercise: Find the conditional expected value of that crude estimator given the data.  You should get $\left((9/10)^\text{(number of calls in the past 20 minutes)}\right)$.  That is the improved unbiased estimator. You start with a crude estimator $h(X)$ and find a better estimator $j(X)$.  What happens if you apply the process again, starting with $j(X)$? If the sufficient statistic happens to be complete, you just get $j(X)$ again. That's idempotence. The proof of idempotence is just definition-chasing.","['statistics', 'probability-theory']"
130621,Evaluating integral using Riemann sums,"It is given that: $$\sin\frac{\pi }{n} \sin\frac{2\pi }{n}\cdots\sin\frac{(n-1)\pi }{n}=\frac{n}{2^{n-1}}$$
It is asked to use the above identity to evaluate the following improper integral:
$$\int_0^\pi  \log(\sin x) \, dx$$ I used the definition of the integral in terms of Riemann sums:
$$\begin{align*}\int_0^\pi \log(\sin x) \, dx
&=\lim_{n\rightarrow \infty }\frac{\pi }{n}\left[\sum_{k=1}^{k=n-1}\log\left(\sin\left(\frac{k\pi }{n}\right)\right)\right]\\
&=\lim_{n\rightarrow \infty }\frac{\pi }{n}\left[\log\left(\sin\frac{\pi }{n}\sin\frac{2\pi }{n}\cdots\sin\frac{(n-1)\pi }{n}\right)\right]\\
&=\lim_{n\rightarrow \infty }\frac{\pi }{n}\Big(\log n-(n-1)\log 2\Big)\\
&=-\pi \log 2
\end{align*}$$ However, this integral is improper, so $\log(\sin(\pi ))=\log(0)=-\infty $. I am kind of cheating in my solution, because the Riemann sum above should be:  $$\sum_{k=1}^{k=n-1}\log\left(\sin\left(\frac{k\pi }{n}\right)\right)+\frac{\pi }{n}\log\left(\sin\left(\frac{n\pi }{n}\right)\right)\;,$$ but I have no idea how to deal with the last term of the sum since $\sin\left(\frac{n\pi }{n}\right)=\sin(\pi)=0 $. Can anyone show me how to deal with this?
Also, if someone knows how to prove the first identity: $$\sin\frac{\pi }{n}\sin\frac{2\pi }{n}\cdots\sin\frac{(n-1)\pi }{n}=\frac{n}{2^{n-1}}$$
 please write down your proof below? Thanks","['calculus', 'real-analysis', 'analysis']"
130628,Irreducible representations of a tensor product,"Let $A, B$ be finitely generated (noncommutative) algebras over a field $k$ (say, algebraically closed). Can we get all irreducible representations of $A \otimes_k B$ from tensoring representations of $A$ with representations of $B$? I'm especially interested in the case where $A, B$ are the enveloping algebras of finite-dimensional Lie algebras.","['tensor-products', 'representation-theory', 'abstract-algebra']"
130639,All partial derivatives are 0.,"I know that for a function $f$ all of its partial derivatives are $0$. Thus, $\frac{\partial f_i}{\partial x_j} = 0$ for any $i = 1, \dots, m$ and any $j = 1, \dots, n$. Is there any easy way to prove that $f$ is constant? The results seems obvious but I'm having a hard time expressing it in words explicitly why it's true.","['calculus', 'derivatives', 'real-analysis']"
130650,Prove: Every Subspace is the Kernel of a Linear Map between Vector Spaces,"I'm trying to show that, given two finite-dimensional vector spaces $V,W$, and any subspace $V'$ of $V$, that there is a linear map $T:V\to W$, whose kernel is precisely $V'$, given the condition that  $\dim V-\dim(\ker T)<\dim W$. I would like to know if the same is true for infinite-dimensional spaces. Because of Rank-Nullity, we have restrictions on the respective dimensions; we need $$\dim W =\dim V-\dim(\ker T), \qquad\mbox{ (I think) }.$$ This is my work: let $\dim V=m $, $\dim W=r$; $r=m-n  $, for $\dim(\ker T)=n$. We start by taking a basis $$B_V':=\{v'_1,\ldots,v'_n\},$$ 
and extend $B_V'$ into a basis $B_V:=\{v'_1,\ldots,v'_n,v'_{n+1},\ldots,v'_m\}$ for $V$. Let $B_W:=\{w_1,w_2,\ldots,w_r\}$. Now, we define $T$:
$$T(B_V'):=0,$$ 
i.e., $T$ is zero for every vector in $B_V'$, and $T$ is linear. By linearity, $T$ is zero on $V'$. Now: This is the part that seems harder: how to define $T$ outside of $V'$, so that $T(w) \neq0$ for $w \in V\setminus V'$. My idea is: i)We set up a bijection between the basis vectors in $B_V\setminus B_V'$, and the basis vectors in $B_W$, say: $$T(v'_{n+1})=w_1,$$
$$T(v'_{n+2})=w_2,$$
$$\vdots$$
$$T(v'_m)=w_r,$$ and extend $T$ linearly. ii) Since a bijection between basis vectors extended linearly gives rise to a Vector Space isomorphism, the kernel of $T|_{V\setminus V'}\rightarrow W$ is an isomorphism, so that its kernel is $0$. Does this work? Can we extend it to the infinite-dimensional case? Thanks.",['linear-algebra']
130660,Poincare inequality?,"Let $\Omega$ be a smooth bounded open subset of $\mathbb{R}^n$.  Does there exist $A = A(\Omega)$ with the property that for any $f \in C^\infty(\bar{\Omega})$ with $f = 0$ on $\partial \Omega$, $\int_\Omega |f(x)|\,dx \leq A\int_\Omega |\nabla f(x)|\,dx$?  I think that this is known as some version of ``Poincare's inequality''.","['multivariable-calculus', 'sobolev-spaces']"
130665,"Normal distribution -Why $\Phi(a,b)=\Phi(b)-\Phi(a)$?","I am reading ""Probability"" by Pitman and in the section that talks about normal distribution the book sais that $\Phi(a,b)=\Phi(b)-\Phi(a)$. Is this a definition or a theorem ? if it's a theorem why is it true ?",['probability-theory']
130666,Complex Analysis Solution to the Basel Problem ($\sum_{k=1}^\infty \frac{1}{k^2}$) [duplicate],"This question already has answers here : Different ways to prove $\sum_{k=1}^\infty \frac{1}{k^2}=\frac{\pi^2}{6}$ (the Basel problem) (54 answers) Closed 7 years ago . Most of us are aware of the famous ""Basel Problem"": $$\sum_{k=1}^\infty \frac{1}{k^2} = \frac{\pi^2}{6}$$ I remember reading an elegant proof for this using complex numbers to help find the value of the sum.  I tried finding it again to no avail.  Does anyone know a complex number proof for the solution of the Basel Problem?","['pi', 'complex-numbers', 'sequences-and-series', 'complex-analysis']"
130668,computation of the sum,"I am having trouble to compute the following sum:
$$
\sum_{k=0}^n(n-2k)^p \frac{{n \choose k}{2m-n \choose m-k}}{{2m \choose m}}
$$
Here $p\geq 2$. To simplify the question, we can even assume that $n/2-C\sqrt n\leq k \leq n/2+C\sqrt n$.
Any help or sources will be very helpful. Thank you.","['statistics', 'approximation', 'binomial-coefficients', 'combinatorics']"
130744,"Conflicting definitions of ""continuity"" of ordinal-valued functions on the ordinals","I've encountered the following definition in Kunen, Levy, and other places: A function $\mathbf{F}:\mathbf{ON}\to\mathbf{ON}$ is continuous iff for every limit ordinal $\lambda$ , we have $\mathbf{F}(\lambda)=\sup\{\mathbf{F}(\alpha):\alpha<\lambda\}$ . I will say such $\mathbf{F}$ are ordinally continuous . If we consider $\mathbf{ON}$ in the order topology, this definition of continuous coincides with topological continuity for non-decreasing $\mathbf{F}:\mathbf{ON}\to\mathbf{ON}$ . If we remove that requirement, though, there are functions that are ordinally continuous, but not topologically continuous, and vice versa. For example, defining $$\mathbf{F}(\xi)=\begin{cases}0 & \text{if }\xi=2\!\cdot\! n\!+\!1\text{ for some }n<\omega\\\xi & \text{otherwise}\end{cases}\qquad\text{and}\qquad\mathbf{G}(\xi)=\begin{cases}\omega+1 & \text{if }\xi=0\\\omega & \text{if }0<\xi<\omega\\\xi & \text{otherwise,}\end{cases}$$ then $\mathbf{F}$ is ordinally but not topologically continuous, and $\mathbf{G}$ is topologically but not ordinally continuous. Before I proceed to ask my question, let me clarify one thing. When I speak of a ""topology"" on $\mathbf{ON}$ , I'm not speaking of something that formally exists in ZF(C)--as such a creature would be a class of (sometimes proper) classes. Instead, we'll describe ""topologies"" on $\mathbf{ON}$ indirectly as follows. We'll say that a class $\mathbf{B}$ of sets of ordinals is a basis class iff $$\forall U\!,V\!\!\in\!\mathbf{B}\;\forall\alpha\!\in\! U\cap V\;\exists W\!\!\in\!\mathbf{B}\;(\alpha\in W\subseteq U\cap V).$$ Given a basis class $\mathbf{B}$ , we'll say that a subclass $\mathbf{M}$ of $\mathbf{ON}$ is ""( $\mathbf{B}$ -)open"" iff one of the following holds: (i) $\mathbf{M}=\mathbf{ON}$ (ii) $\forall\alpha\!\in\!\mathbf{M}\;\exists U\!\in\!\mathbf{B}\;(\alpha\!\in\!U\!\subset\!\mathbf{M}).$ For further discussion of why I chose these particular definitions of basis class and openness, see this post . Question : Is there a way to ""topologize"" $\mathbf{ON}$ such that the ordinally continuous functions $\mathbf{ON}\to\mathbf{ON}$ are precisely the topologically continuous functions $\mathbf{ON}\to\mathbf{ON}$ ? If so, what's an example? If not, how can one show that there is no way? Remark : When considering $\mathbf{ON}$ in the order topology, limit ordinals and limit points are identical. It would, of course, be ideal to find a topology in which this still held and where topologically continuous and ordinally continuous functions are the same, but I would still be interested in any topology satisfying only the latter. Current Goals : (A) I'd like to generalize Brian M. Scott's result from below to other limit ordinals. In other words, assuming that $\mathbf{ON}$ has been ""topologized"" in such a way that ""ordinally continuous"" and ""topologically continuous"" are identical, I'd like to determine for which limit ordinals $\lambda$ we can conclude that $[0,\lambda]$ is contained in all open classes containing $\{\lambda\}$ . (Brian showed that this property holds for $\lambda=\omega$ . Does this hold for all limit ordinals $\lambda$ ? Only when $\lambda$ is an aleph? Only when $\lambda$ is a regular aleph? Only when $\lambda=\omega$ ? Only when [fill in the blank appropriately]? (B) I'd like to find a counterexample similar to $\mathbf{B}_1$ (from my answer below) satisfying the condition that limit points and limit ordinals are identical. If you can help me with (A) or (B), but not yet answer my overarching question, let me know, and I'll make a new question for you to answer. (Heck, I'll even give you a portion of the bounty offered on this question, if it's a good answer.)","['general-topology', 'set-theory', 'ordinals', 'definition']"
130754,How to prove (or find) that a given group representation is irreducible?,"Given a group representation, how can I definitely know whether it is irreducible or not? In principle I should check for non-trivial invariant subspaces, or find, if any, block-triangular similar matrix, but that sounds computationally difficult.",['group-theory']
130758,Dividing a square into equal-area rectangles,"How many ways are there to tile an $n\times n$ square with exactly $n$ rectangles, each of which has integer sides and area $n$? The sequence $C(n)$ begins 1, 2, 2, 9, 2, 46, 2, 250, 37. Clearly $C(p) = 2$ for prime $p$.  The value $C(8) = 250$ was provided to me by Sjoerd Visscher, but I cannot vouch for it personally, not having seen the details of his enumeration. OEIS was no help.","['geometry', 'tiling', 'discrete-geometry', 'recreational-mathematics', 'combinatorics']"
130773,Find bound for sum of square roots,"Let $a_1,...,a_n$ be real numbers, such that $a_1+...+a_n=A$. What can we say about $\sqrt{a_1}+...+\sqrt{a_n}$? I would like to bound from above thus sum in terms of $A$.","['approximation', 'calculus', 'number-theory', 'combinatorics']"
130788,Isn't there a bijection between real numbers and natural numbers?,"I'm not a math guy and probably this is a stupid question. However, I was browsing Wikipedia out of curiosity and I could not understand this one statement : [...] not all infinite sets have the same cardinality. For example, Georg Cantor (who introduced this branch of mathematics) demonstrated that the real numbers cannot be put into one-to-one correspondence with the natural numbers (non-negative integers), and therefore that the set of real numbers has a greater cardinality than the set of natural numbers. Now, shortly before this a formula was used to map even integers to odd integers: n → 2 n + 1 Granted we have infinite natural and real numbers, we can devise a formula to map them like so: 0  →  0
1  →  1
2  → -1
3  →  2
4  → -2
5  →  3
6  → -3
7  →  4
8  → -4
9  →  5
10 → -5 As I'm a math analphabet, this is javascript: ​function realToNatural(x) {
    if(x == 0)
        return 0;
    if(x < 0)
        return x * -2;
    else
        return x * 2 - 1;
}

function naturalToReal(x) {
    if(x == 0)
        return 0;
    if(x % 2 == 0)
        return x / 2 * -1;
    else
        return (x + 1) / 2;
} Now, I'm sure there is a hole in my argument, but what is it? A couple of additional thoughts: The article mentions the cardinality of the set of odd integers being equal to the one of even integers, and as well equal to the cardinality of all integers, so my confusion is: if this applies to odd and even numbers (being both a ""full"" infinity instead of ""half"" infinity) versus the set of both, so it would to natural numbers versus real ones. Also it states that I just need a function in the form of f : S → ℕ to make a set countable.","['infinity', 'elementary-set-theory', 'combinatorics']"
130814,"$3^k$ not congruent to $-1 \pmod {2^e}, e > 2$.","$3^k \not\equiv -1 \pmod {2^e}$ for $e > 2, k > 0$. Is this true? I have tried to prove it by expanding $(1 + 2)^k$. [Notation: $(n; m) := n! / (m! (n - m)!)$] E.g., for $e = 3$ I get: $(1+2)^k + 1 = 2 + (k; 1) 2 + (k; 2) 2^2 + (k; e) 2^e + ...$ So, here it's enough to prove that $2^3$ does not divide $2 + (k; 1) 2 + (k; 2) 2^2$. The validity for general e seems very hard to prove.","['elementary-number-theory', 'discrete-mathematics']"
130817,How do we prove $\cos(\pi/5) - \cos(2\pi/5) = 0.5$ ?.,"How do we prove $\cos(\pi/5) - \cos(2\pi/5) = 0.5$ without using a calculator. Related question: how do we prove that $\cos(\pi/5)\cos(2\pi/5) = 0.25$, also without using a calculator",['trigonometry']
130823,Is $1+x+\frac{x^2}2+\dots+\frac{x^n}{n!}$ irreducible?,"The polynomial $f(x)=1+x+\frac{x^2}2+\dots+\frac{x^n}{n!}$ often appears in algebra textbooks as an illustration for using derivative to test for multiple roots. Recently, I stumbled upon Example 2.1.6 in Prasolov's book Polynomials (Springer, 2004), where it is shown that this polynomial is irreducible using Eisenstein's criterion and Bertrand's postulate . However, I do not think the argument is correct. Below you can find the argument presented in the book -- I do not see how Eisenstein is applicable here, since we do not know $p\mid n$. And if we are using Eisenstein's criterion directly to the polynomial $n!f(x)$, this is one of the coefficients that would have to be divisible by $p$. (However, the argument works at least if $n$ is prime.) So my main question is about the irreducibility of the original polynomial, but I also wonder whether Prasolov's proof can be corrected somehow. To summarize: Is the polynomial $f(x)=1+x+\frac{x^2}2+\dots+\frac{x^n}{n!}$ irreducible over $\mathbb Q$? Is the Prasolov's proof correct or can it be easily corrected? (Did I miss something there?) Here is the (whole) Example 2.1.6 from Prasolov's book. The same example is given in прасолов: многочлены(Prasolov: Mnogochleny; 2001,MCCME). Example 2.1.6. For any positive integer $n$, the polynomial
$$f(x)=1+x+\frac{x^2}2+\dots+\frac{x^n}{n!}$$
is irreducible. Proof: We have to prove that the polynomial 
$$n!f(x)=x^n+nx^{n-1}+n(n-1)x^{n-2}+\dots+n!$$
is irreducible over $\mathbb Z$. To this end, it suffices to find the prime $p$ such that $n!$
is divisible by $p$ but is not divisible by $p^2$, i.e., $p \le n < 2p$. Let $n = 2m$ or $n = 2m + 1$. Bertrand's postulate states that
there exists a prime p such that $m < p \le 2m$. For $n = 2m$, the inequalities $p \le n < 2p$ are obvious. For $n = 2m + 1$, we
obtain the inequalities $p \le n-1$ and $n-1 < 2p$. But in this case the number
$n-1$ is even, and hence the inequality $n-1 < 2p$ implies $n < 2p$. It is also
clear that $p \le n - 1 < n$. $\hspace{20pt}\square$","['abstract-algebra', 'polynomials']"
130850,Free group and universal property,"I'm trying to understand universal properties. An example is the definition of a free group (as I understand it so far): Revised definition: A free group $F_S$ over a set $S$ is a pair $(g,F_S)$ that satisfies the (universal) property that if $G$ is a group and $f: S \to G$ is a map then there exists a unique homomorphism $\varphi : F_S \to G$ such that $\varphi \circ g = f$. (What I had written before: 
If $S$ is a set and $G$ is a group and $f: S \to G$ is an arbitrary map then the free group over $S$ is the pair $(g,F_S)$ that satisfies (the universal property) that there exists a unique homomorphism $\varphi : F_S \to G$ such that $ \varphi \circ g = f$.) Is the map $g: S \to F_S$ required to be the inclusion or can it be an arbitrary map?","['free-groups', 'group-theory', 'abstract-algebra', 'definition']"
130855,"Summation of a binomial style sum, equal to gamma function?","I am trying to prove the following for a very, very long time: $$\sum_{k=0}^j \frac{ (-1)^k}{k! (j-k)!} \frac{1}{2k+1} = \frac{1}{2} \frac{\sqrt{\pi}}{\Gamma(3/2 + j)}$$ or, equivalently $$\sum_{k=0}^j \frac{ (-1)^k}{k! (j-k)!} \frac{1}{2k+1} = \frac{(j+1)! 4^{j+1}}{2\cdot (2(j+1))!}$$ I would be extremely happy if somebody could help me with this!","['factorial', 'gamma-function', 'number-theory']"
130864,Showing that $\sum \frac{\log n}{n^x}$ converges for $x>1$,"I'm trying to show that $\sum \frac{\log n}{n^x}$ converges for $x>1$ by the ratio test. Here's what I've got so far $$\frac{a_{n+1}}{a_n} = \frac{\log (n+1) n^x}{(n+1)^x \log n}$$
$$=\left(\frac{n}{n+1}\right)^x \frac{\log (n+1)}{\log n}$$ but I can't see how to manipulate the $\frac{\log (n+1)}{\log n}$ term to make this congerge to a limit less than 1, can anyone help?",['analysis']
130866,Adjoint action smooth (in Tapp's book)?,"I'm reading ""Matrix Groups for Undergraduates"" by Tapp with a student.  A ""matrix group"" means a subgroup $G$ of $GL_n(\mathbb R)$ which is (relatively) closed-- so if $(A_n)\subseteq G$ with $A_n\rightarrow A$, and $A$ is invertible, then $A\in G$. In the book, Manifolds are treated in an adhoc way.  Given $X\subseteq\mathbb R^m$ a map $f:X\rightarrow \mathbb R^n$ is ""smooth"" if for each $x\in X$ there is an open set $U\subseteq\mathbb R^m$ containing $x$, and a smooth map $g:U\rightarrow\mathbb R^m$ which agrees with $f$ on $U\cap X$.
Then a manifold is defined in the obvious way. We then have the adjoint action of a lie group $G$ on its lie algebra $\mathfrak g$.  If $\mathfrak g$ is $d$ dimensional, then by taking a basis $A_1,\cdots,A_d$ of $\mathfrak g$, we can regard the adjoint action as a homomorphism $Ad:G\rightarrow GL_d(\mathbb R)$.  So for each $g\in G$ there is a matrix $(X_{ij}(g))$ with
$$ g A_j g^{-1} =\sum_i X_{ij}(g) A_i. $$ How do we show that $Ad$ is smooth? If we follow the definition from the book then we'd need to show that $G\rightarrow \mathbb R^{d\times d}; g \mapsto (X_{ij}(g))$ is smooth.  So for each $g\in G$ I need an open set $U$ in $GL_n(\mathbb R)\subseteq\mathbb R^{n\times n}$ and a smooth function $f:U\rightarrow\mathbb R^{d\times d}$ such that $f(g) = (X_{ij}(g))$ for $g\in G\cap U$.  This seems intractable...? (If one has more Manifold theory, then this becomes sort of obvious, as $Ad$ is just the derivative of the conjugation action, which is smooth, and the derivative of a smooth map is smooth.  But I want to stick to what the book has told us...) Edit: Maybe I can actually argue as follows.  The map $(A,B)\mapsto \operatorname{Tr}(AB)$ is an inner-product on $\mathbb M_n(\mathbb R)$; so I can find $B_1,\cdots,B_n\in\mathbb M_n(\mathbb R)$ with $\operatorname{Tr}(A_iB_j)=\delta_{i,j}$.  Thus
$$ X_{ij}(g) = \operatorname{Tr}(g A_j g^{-1} B_i). $$
Thus I could take as my map
$$ f(h) = \big( \operatorname{Tr}(h A_j h^{-1} B_i) \big)_{i,j}. $$
This is now a composition of matrix inverse and multiplication, and trace, all smooth maps, hence $f$ is smooth. Does this seem reasonable?  Is this the easiest approach?","['lie-groups', 'differential-geometry']"
130870,Continuity and differentiability of function series,"Examine the continuity and differentiability of functions: a) $\displaystyle f(x)=\sum_{n=1}^{+\infty}\frac{\sin(nx)}{n^3}$ b) $\displaystyle f(x)=\sum_{n=1}^{+\infty}\arctan\left(\frac{x}{n^2} \right)$ in the case of differentiability explore the sign of $f '(0)$ . So, we are dealing with function series I think. I tried a):
Let $f_n(x)=\frac{\sin(nx)}{n^3}$ . With Weierstrass M-test $|f_n|\le \frac{1}{n^3}$ so the series $\sum_{n=1}^{+\infty}f_n$ converges  uniformly. The same applies for series of derivatives that is: $\sum_{n=1}^{+\infty}\frac{\cos(nx)}{n^2}$ so function $f$ is differentiable and so it is continuous. $f'(0)>0$ Is this argumentation ok? I don't know how to approach b).","['continuity', 'derivatives', 'real-analysis']"
130881,Taylor expansion for matrices,"Is it possible to define a Taylor expansion for matrices ? Can I use functional derivative ? More precisely I have to calculate something like :
$\ln(A+B)$ using a Taylor expansion, where $A$ and $B$ are hermitian matrices which depend also on $x\in \mathbb{R}^3$. My idea is to use functional derivative but I don't know if the result is correct!","['operator-algebras', 'matrices', 'operator-theory', 'taylor-expansion']"
130892,"Group question: only one element $x$ with order $n>1$, then $x\in Z(G)$","I just tried to solve this question on a exam: Let $G$ be a group with only one element $x$ of order $n$, $n$ natural. Show that $x\in Z(G)$. (I'm using multiplicative notation) $Z(G)$ is the subset of G such that every element of $Z(G)$ commutes with every element of G (called the Center of G). I tried to do it like this: Suppose that x does not commute with at least one $g\in G$. Then $xg\neq gx\rightarrow x\neq gxg^{-1}$. Then, $x^n\neq (g x g^{-1})^n=g x g^{-1}g x g^{-1}...g x g^{-1}=xx...x=gx^ng^{-1}=1$ Notice that $g x g^{-1}$ cannot have an order $n < m$ because its order depends exclusively of the order of $x$, which is $n$. That means that there are two distinct elements in $G$ with order $n$, which is a contradiction. Can someone tell me if it was done correctly?","['group-theory', 'abstract-algebra']"
130899,Multiple differentiable functions,"Check for what $n\in\mathbb{N}\cup \left\{+\infty\right\}$ $f\in C^n(\mathbb{R})$ (that is $f$ is $n$ times differentiable, I wasn't sure that this designation is common), if: a) $f(x)=|x|^m, \ m\in\mathbb{R}$ b) $f(x)= \begin{cases} e^{-1/x}, \ x>0 \\ 0, \ x\le 0 \end{cases} $ I wasn't given in school any convenient theorem to check such things but I really want to learn it. How can I check it?","['derivatives', 'real-analysis']"
130909,Where's my mistake?,"This is partially an electrical engineering problem, but the actual issue apparently lays in my maths. I have the equation $\frac{V_T}{I_0} \cdot e^{-V_D\div V_T} = 100$ $V_T$ an $I_0$ are given and I am solving for $V_D$. These are my steps: Divide both sides by $\frac{V_T}{I_0}$: 
$$e^{-V_D\div V_T} = 100 \div \frac{V_T}{I_0}$$
Simplify: 
$$e^{-V_D\div V_T} = \frac{100\cdot I_0}{V_T}$$
Find the natural log of both sides: 
$$-V_D\div V_T = \ln(\frac{100\cdot I_0}{V_T})$$
Multiply by $V_T$: 
$$-V_D = V_T \cdot \ln(\frac{100\cdot I_0}{V_T})$$
Multiply by $-1$:
$$V_D = -V_T \cdot \ln(\frac{100\cdot I_0}{V_T})$$
Now if I substitute in the numbers; $V_T \approx 0.026$, $I_0 \approx 8 \cdot 10^{-14}$
$$V_D = -0.026 \cdot \ln(\frac{8 \cdot 10^{-12}}{0.026})$$
Simplify a little more:
$$ V_D = \frac{26}{1000} \cdot -\ln(\frac{4}{13} \cdot 10^{-9})$$
and you finally end up with $V_D \approx 0.569449942$ There is an extra step to the problem as well: the question calls not for $V_D$, but for $V_I$ which is a source voltage and that can basically be determined by solving this:
$$V_I \cdot \frac{1}{40} = V_D$$
I.e. $V_I = 40V_D$; which makes $V_I \approx 22.77799768$. However, this is off by quite a bit (the answer is apparently $1.5742888791$). Official Solution: We find $V_D$ to be $\approx$ 0.57. (Woo! I did get that portion right.) Since we know that $\frac{V_D}{i_D}$ is 100, we find $i_D$ to be 0.00026. Background: $\frac{V_D}{i_D}$ is the resistance that I was originally solving. $V_D$ was the voltage across the element, and $i_D$ was the current through the element. However, either I'm making some stupid mistake but if $i_D = \frac{V_D}{100}$ then how did they get 0.00026?
Completing the rest of the solution's method (quite convoluted in comparison to mine, checked mine though another method; $V_I$ does in fact equal $40V_D$), with the correct value, 0.0057, I arrived at exactly the same final value as before. Would it be fair to say that it is likely that my logic is correct?",['algebra-precalculus']
130913,Algebraic maps from products of affine varieties,"I have a question which might be fairly elementary, but I could not find an answer in the literature yet. Any pointers are very welcome :) Let $X$, $Y$ and $Z$ be affine algebraic varieties. I have a map 
$f:X\times Y \rightarrow Z$ and I know that for fixed $x_0\in X$, the map 
$Y\rightarrow Z, y \mapsto f(x_0,y)$ is a morphism. Also, for fixed
$y_0\in Y$, the map $X\rightarrow Z, x \mapsto f(x,y_0)$ is a morphism. I regard 
$X\times Y$ as the product variety of $X$ and $Y$. Is $f$ itself a morphism then? Can I prove this by stating a few known results from algebraic geometry? (Pointers to literature would be very helpful). Or do I have to prove it ""by hand""? Or is the statement even false? Thank you!","['commutative-algebra', 'algebraic-geometry']"
130928,"Is there a quantitative definition of ""maneuverability"" in an arbitrary space?","I apologize beforehand if this question is too philosophical or ill-defined.  Hopefully, someone can provide some insight as to whether this concept exists in mathematics or I'm exploring a dead-end. I am interested in whether a quantifiable measure of ""maneuverability"" in an arbitrary space exists.  The normal Euclidean spaces provide a good way to illustrate my question. An object in a zero dimensional space has zero maneuverability.  An object (say a point) can't move outside of its universe (the identical point).  This would seem to be the lower limit of maneuverability i.e. zero. A one dimensional space seems to have more maneuverability.  For example, imagine a infinte line universe with objects like points and/or line segments placed randomly on the line.  The points and line segments can now move along the line until they bump into one of their neighbors.  Interestingly, it is impossible for the point or line segment to ""jump"" over their neighbor.  Hence they are confined to a predetermined subset of the one dimensional space. A two dimension space provides even more maneuverability.  Say a square and a triangle exist in this space.  Now the square and triangle can move in two degrees of freedom.  Importantly, now the square for example can potentially completely encircle its neighbor the triangle.  Arbitrarily, the square should be able to explore any part of the two dimensional space. I struggle to formulate whether three dimensional space inherently has more maneuverability than two dimensional space.  Naively, it seems to have more maneuverability than 2D but still the best that can be accomplished is that an object can fully explore the 3D space (same as the 2D space).  So a key question is does the 3D space have quantifiably more maneuverability than the 2D space? I realize that to some extent the maneuverability is impacted by how ""full"" you make the space with objects; however, I am interested in the inherent maneuverability of the space concept rather than an actual configuration specific definition.  The key assumption is that solid objects exist i.e. they can't be passed through by another object which in the 1D case prevents an object from arbitrarily exploring its full space. Does my need to define maneuverability have any mathematical meaning?  If so, can it be quantified?  If so, how to apply to arbitrary mathematical spaces (say starting with non-Euclidean spaces?)",['geometry']
130930,Improving my understanding of Cantor's Diagonal Argument,"I studied Cantor's Diagonal Argument in school years ago and it's always bothered me (as I'm sure it does many others).  In my head I have two counter-arguments to Cantor's Diagonal Argument.  I'm not a mathy person, so obviously, these must have explanations that I have not yet grasped. My first issue is that Cantor's Diagonal Argument ( as wonderfully explained by Arturo Magidin ) can be viewed in a slightly different light, which appears to unveil a flaw in the argument.  If we assume that s_f is an image of f at some index n , then it does not make sense to define s_f as $s_f=(s_1,s_2,s_3,…,s_n,…)$  where
$\begin{equation} s_k = \begin{cases}0 & \mathrm{if\ } f(n)_n = 1\\1 & \mathrm{if\ } f(n)_n = 0\end{cases}\end{equation}$ since then the $n$th element of $s_f$ would be defined as the opposite of itself .  Since Cantor's Diagonal Argument uses this definition that wouldn't make sense if s_f has an index, then s_f must not have an index, and from there it seems obvious that they would reach the conclusion that s_f is not an image of f .  Isn't that begging the question? My second issue is more complicated, and less articulate, but basically that when I attempt to put numbers into Cantor's Diagonal Argument, I could demonstrate that the ""missing"" element was the within a constant distance from the last element in the ""series"", which means all of the infinite other numbers must be before it, which means no matter how long you count, you'd never reach it.  For example, if one puts these in the most obvious order of ""counting"" 0000..., 1000..., 0100...., 1100..., 0010... then the element to be found is obviously the element where all $s_k = 1$, which would be the ""last"" element in that ordering.  But that also seems to apply to the counting numbers, which also seems to violate Cantor's Arguments.",['elementary-set-theory']
130944,Can characters occur in automorphic representation,Let $\pi$ be an irreducible cuspidal automorphic representation of $GL(2)$ over a global field with factorisation $\pi = \otimes \pi_v$. Then at most finitely many $\pi_v$ are not spherical. Questions: Can it happen that $\pi_v(g) = \chi_v(\det g)$ for a unitary character of $F_v^\times$? Can it happen infinitely often? Is the Steinberg spherical? Can it occur as $\pi_v$? Can it appear infinitely often?,"['automorphic-forms', 'number-theory']"
130948,"""Converse"" to composition of measurable functions is measurable","Here is a restatement of a problem in a textbook I encountered. I'm
well beyond the age of doing homework and this is purely for
self-study. Exercise : Let $f : (X,\Sigma_1) \to (Y, \Sigma_2)$ and
      $h:(X,\Sigma_1) \to (\mathbb R, \mathcal B)$ be
      measurable maps where in the latter case $\mathcal B$ denotes the
      Borel $\sigma$-algebra over $\mathbb R$. Let $\Sigma_f =
    \sigma(f)$. Show that $h$ is $\Sigma_f$-measurable if and only if there exists $g : (Y,\Sigma_2) \to
    (\mathbb R, \mathcal B)$ such that $h(x) = g(f(x))$ for all $x \in
    X$. One direction of the proof is easy. Suppose such a $g$ exists. Then,
for all $B \in \mathcal B$, $h^{-1}(B) = f^{-1}( g^{-1}(B) )$ and so
$h^{-1} \in \Sigma_f$. There seem to be some holes in the opposite direction which I can't
quite fill. For all $z \in \mathbb R$, I defined $$
A_z = \{x: h(x) = z\}. 
$$ Then $A_z \in \Sigma_1$ since the singletons $\{z\}$ are
Borel-measurable. Also, for $z \neq z'$, it is true that $A_z \cap
A_{z'} = \emptyset$. Now, if $h$ is $\Sigma_f$-measurable, then $A_z =
f^{-1}(B_z)$ for some $B_z \in \Sigma_2$. But then, for $z \neq z'$,
we have that $B_z \cap B_{z'} = \emptyset$ as well, so the $\{B_z\}_{z
\in \mathbb R}$ sets partitions $Y$ modulo the portion not in the image
of $f$. Now, set $g(y) = z$ on $B_z$ and set $g(y) = 0$ on $y \in N_0 := Y
\setminus \cup_{z \in \mathbb R} B_z$. It seems reasonable to claim that $N_0$ is a measurable set by considering that $N_0 = Y \setminus \cup_n
C_n$ where $f^{-1}(C_n) = h^{-1}((-\infty,n))$ and $C_n \in \Sigma_2$
by assumption. But, this only seems to show that we can construct a well-defined $g$. It doesn't seem to prove that it is measurable ! To get
measurability we need to show something in addition to this, like $\{y: g(y) \leq z\} \in
\Sigma_2$ for all $z \in \mathbb R$. For $z < 0$ it seems we should be able to get a correspondence between
$\{y: g(y) \leq z\}$ and $C_z$ where $C_z \in \Sigma_2$ satisfies $f^{-1}(C_z) = h^{-1}((-\infty,z))$. For $z \geq 0$, I think it would be
something like $\{y : g(y) \leq z\} = C_z \cup N_0$, I think. I can't quite seem to make the argument go through. Questions: Is this on the right track? If so, how do we finish it off? (It seems a little ""too constructive"" for a typical measure-theoretic argument.) Is there some other more clever or direct argument? If so, what is it?","['probability-theory', 'measure-theory', 'elementary-set-theory', 'real-analysis']"
130950,Free groups: unique up to unique isomorphism,A free group $F_S$ over a set $S$ is unique up to unique isomorphism. After reading this answer here I think this means that if $F^\prime_S$ is another free group over $S$ then there exists exactly one isomorphism $\varphi : F_S \to F_S^\prime$ such that $\varphi \circ i = i^\prime$ where $i: S \to F_S$ and $i^\prime : S \to F^\prime_S$ are the inclusions. Why is uniqueness of the isomorphism interesting at all? Wouldn't it be good enough to know that all free groups over $S$ are isomorphic to define free groups?,"['category-theory', 'abstract-algebra']"
130966,Singular measures - approximate characteristic function,"One can decompose a $\sigma$-finite measure $\mu$ on $\mathbb{R}$ in three parts: $\mu_{ac}$: absolutely continuous $\mu_{sc}$: singular continuous $\mu_{pp}$: pure point A common example for a singular continuous probability measure is Cantor's function as cdf. Such a cdf is continuous. I have two question: (1) Do singular cont. probability measures come up? E.g. as law of pure jump L\'evy processes, for which there are criteria to guarantee a density. What about semimartingales? (2) The characteristic function can be defined in two ways $\int e^{itx}dF(x)$ or $\int e^{itx}d\mu_{sc}(x)$. Though there is no density function, can $\int e^{itx}dF(x)$ be approximated by a sequence of, say a continuous $\mathcal{L}^1$ functions, such that $\int e^{itx}dF(x) = \lim_{n\rightarrow\infty}\int e^{itx}f_n(x)dx$? I can't find anything on that, so maybe singular measures are quite opaque objects in probability.","['probability-theory', 'singular-measures', 'fourier-analysis', 'probability']"
130967,How to find generating function of cumulative sum?,"Given the recurrence,
$$
a_n =
\begin{cases}
1, & n = 0 \\
a_{n-1} + 2a_{n-2} + 3a_{n-3} + \ldots + (n - 1)a_1 + na_0, & n \geq 1
\end{cases}
$$
My attempt was, rewrite $a_n$ as:
$$a_n = \displaystyle{\sum_{i=0}^na_i}(n - i) \Leftrightarrow \displaystyle{\sum_{n=0}^{\infty}a_nz^n} = \displaystyle{\sum_{n=0}^{\infty}\bigg(\displaystyle{\sum_{i=0}^na_i(n - i)\bigg)}}z^n + 1$$
then I tried to compare it with one of my generating function formulas:
$$\dfrac{G(z)}{1 -z} = \displaystyle{\sum_{n=0}^{\infty}\bigg(\displaystyle{\sum_{i=0}^na_i\bigg)}}z^n$$
They're very similar except for the coefficient $n - i$ and the $1$. 
The answer given in the note was:
$$a_n = \dfrac{-1}{\sqrt{5}}\bigg(\dfrac{2}{3 + \sqrt{5}}\bigg)^n + \dfrac{1}{\sqrt{5}}\bigg(\dfrac{2}{3 - \sqrt{5}}\bigg)^n$$ 
but I had no idea how did they get to that expression? 
I also tried to consider 
$$G(z) - zG(z) - 2z^2G(z) - 3z^3G(z) - 4z^4G(z) - \ldots = 1$$ 
but this approach leads to:
$$G(z) = \dfrac{1}{1 - z - 2z^2 - 3z^3 - \ldots }$$ which is impossible to factor out the denominator. I guess there must be a clever trick to pull out the $n - i$ term but somehow I couldn't see it. I wonder could anyone give me a hint how to solve this problem? Thanks.","['generating-functions', 'number-theory']"
130970,Function with the same derivative are equal up to a constant,"If functions $f$ and $g$ are continuous on $[a,b]$ differentiable on $(a,b)$, and $f'(x) = g'(x)$ on $(a,b)$, then there exists a real number $K$ such that $f(x) = g(x) + K$ for all $x\in [a,b]$.","['functions', 'real-analysis']"
130976,Weak convergence of probability measure,"I am working on a problem. Show that for each probability measure $\mu$, there exists probability measure $\mu_n$ with finite support such that $\mu_n$ converges weakly to $\mu$. I am thinking about the empirical measure, which has the distribution function $F_n(t)=1/n\sum 1(X_i\le t)$. So from LLN, we have $F_n(t)\rightarrow F(t)$ for each fixed $t$ a.s. But the convergence here is almost surely. So does this still means $F_n$ converges to $F$ weakly?","['probability-theory', 'measure-theory', 'probability']"
130987,Evaluate $\int \frac{\sec^{2}{x}}{(\sec{x} + \tan{x})^{5/2}} \ \mathrm dx$,I am unable to solve the following integral: $$\int \frac{\sec^{2}{x}}{(\sec{x} + \tan{x})^{5/2}} \ \mathrm dx.$$ Tried putting $t=\tan{x}$ so that numerator has $\sec^{2}{x}$ but that doesn't help.,"['calculus', 'integration', 'indefinite-integrals']"
130990,"Consequence of metrizability proof - disregard, the question is an error","In Marian Fabian et al's Functional Analysis and Infinite-Dimensional Geometry , Proposition 3.22 states/proves that if $X$ is a separable Banach space, then the (closed) unit ball, $B_{X^{*}}$ of $X^{*}$, endowed with the weak-$*$ topology of $X^{*}$ is metrizable. They do this by defining a metric $d$ on $X^{*}$ using a dense subset of the unit sphere of $X$, and then showing the identity map $I:(B_{X^{*}},weak-*)\to (B_{X^{*}}, d)$ is a homeomorphism. The importance in restricting to the unit balls comes from the use of Alaoglu's theorem which states that $B_{X^{*}}$ is compact in the weak-$*$ topology.  This avoids the problem of having to show that $I^{-1}$ is continuous, as $(B_{X^{*}},weak-*)$ is compact, and $(B_{X^{*}}, d)$ is Hausdorff. Note:  the comment below is incorrect and was a complete misread of the statement.  Please disregard the question. After the proof is completed, it is concluded as a remark that the spaces $(X^{*},weak-*)$ and $(X^{*},d)$ are in fact homeomorphic and thus the weak-$*$ topology of $X^{*}$ is metrizable. I have a problem proving this Corollary.  Continuity on the unit ball can easily be extended to the whole space since scalar multiplication (and thus normalization) is a homeomorphism in a topological vector space, but as far as I know we have no such guarantee in a metric space.  Can anyone suggest how I can see the truth behind this consequence?","['normed-spaces', 'topological-vector-spaces', 'metric-spaces', 'functional-analysis']"
130996,What geometrical obstructions to $M$ being flat do elements which map to 0 in $M \otimes I$ represent?,"I'm trying to get geometric intuition for the notion of a flat module over a ring, and am running into some problems with my intuition. I am comfortable with flat modules and tensor products from the commutative side, so when I ask what an object ""is"", I mean how can I translate the commutative algebra into geometric reasoning. Consider $M=\mathbb{C}[x,y,z]/(xz-y)$ as an $R=\mathbb{C}[x,y]$ module. As was explained very well by many users here: Why isn't $\mathbb{C}[x,y,z]/(xz-y)$ a flat $\mathbb{C}[x,y]$-module , the reason that $M$ is not flat as an $R$-module is because if we consider the ideal $I=(x,y)$, then $M \otimes I$ contains the non-zero element $1 \otimes y-z \otimes x$, which is 0 if considered as an element of $M$ (it happens to have x-torsion). We also know that the geometric reason for the failure of flatness is that a whole line is mapped to $(0,0)$ when we map the variety corresponding to $M$ to the plane (the variety corresponding to $R$). My question is what geometric ""object"" exactly is $1 \otimes y-z \otimes x$, which maps to $0$ when we consider multiplication but happens to be non-zero when we consider some other random bilinear form, and how does this object correspond to a non-constant dimensional fiber if we consider it geometrically? If the object is in fact just something we can apply bilinear forms to, then I would like to know if the general bilinear forms on $M \times I$ are possibly ""geometrical"" in some sense (i.e. products of nth derivatives of functions, which could correspond to nth order behavior around a specific point). On a related note, I'm not sure what the ""functions"" in $M \otimes \kappa(P)$ actually correspond to (here $\kappa$ mean residue class field); I would really appreciate if anyone could clear this up (specifically, what exactly do these functions ""act on"", and where do they ""live""). Thank you, Rofler Edit: I thought understanding $M \otimes \kappa(P)$ would shed some light on $M \otimes P$, but actually the former is actually easy to understand, since none of the elements in the tensor products are non-elementary, and we've really just inverted a few functions, and set a few others to be $0$. $M \otimes P$ on the other hand...",['algebraic-geometry']
131004,Proof that $\sum\limits_{k=1}^\infty\frac{a_1a_2\cdots a_{k-1}}{(x+a_1)\cdots(x+a_k)}=\frac{1}{x}$ regarding $\zeta(3)$ and Apéry's proof,"I recently printed a paper that asks to prove the ""amazing"" claim that for all $a_1,a_2,\dots$ $$\sum_{k=1}^\infty\frac{a_1a_2\cdots a_{k-1}}{(x+a_1)\cdots(x+a_k)}=\frac{1}{x}$$ and thus (probably) that $$\zeta(3)=\frac{5}{2}\sum_{n=1}^\infty {2n\choose n}^{-1}\frac{(-1)^{n-1}}{n^3}$$ Since the paper gives no information on $a_n$, should it be possible to prove that the relation holds for any ""context-reasonable"" $a_1$? For example, letting $a_n=1$ gives $$\sum_{k=1}^\infty\frac{1}{(x+1)^k}=\frac{1}{x}$$ which is true. The article is ""A Proof that Euler Missed..."" An Informal Report - Alfred van der Poorten.","['special-functions', 'power-series', 'sequences-and-series', 'riemann-zeta']"
131005,"If $T\alpha=c\alpha$, then there is a non-zero linear functional $f$ on $V$ such that $T^{t}f=cf$","I am self-studying Hoffman and Kunze's book Linear Algebra . This is exercise $4$ from page $115$. It is in the section of The transpose of a Linear Transformation . Let $V$ be a finite-dimensional vector space over the field
  $\mathbb{F}$ and let $T$ be a linear operator on $V$. Let $c$  be a
  scalar and suppose there is a non-zero vector $\alpha$ in $V$ such
  that $T\alpha=c\alpha.$ Prove that there is a non-zero linear
  functional $f$ on $V$ such that $T^{t}f=cf.$ ($T^{t}$ is the transpose of $T$.) I tried to solve this question by induction on $\dim V$. I was able to show the base case, that is, when $\dim V=1$, but I got stuck in the  inductive step. If $\alpha $ is a non-zero vector, then we can find a base $\mathcal{B}=\{\alpha,\alpha_{1},\ldots\alpha_{m}\}$ of $V$. We can write $V=W_{1}\oplus W_{2}$, where $W_{1}=\langle \alpha \rangle$ and $W_{2}=\langle \alpha_{1},\ldots,\alpha_{m}\rangle.$ I can not show that $T(W_{2})\subset W_{2}.$ Anyway, $\alpha \notin W_{2}.$ EDIT: If $T$ is a linear transformation from $V$ to $W$, then $T^{t}$ is the linear transformation from $W^{\star}$ into $V^{\star}$ such that $$(T^{t}f)(\alpha)=g(T\alpha)$$ for every $f\in W^{\star}$ and $\alpha \in V$.",['linear-algebra']
131013,Matrix exponential convergence,Help me please to prove that matrix exponential which defined as: $e^{A}=\sum\limits_{k=0}^{\infty }\frac{A^{k}}{k!}$ converges for all matrix $A$ Thanks beforehand.,"['matrices', 'sequences-and-series', 'calculus']"
131014,Approach to Analytically Solving Nonlinear Differential,"This first order nonlinear differential was posted on a science forum and I am very interested in it.  I would like to know what are the most appropriate steps taken to test for the possibility of an analytical solution, as well as what might be the appropriate steps to solve it analytically. $\dot{x} = - \alpha {x}^{1/2} \arctan{(kx)}$ for $x>0$ and $t \in [0,T]$, where $T < \infty$ and $k > 0$ I'm also kind of curious where it might have come from, which I hadn't asked the original poster and so I do not know!  I'm not looking for a lesson, or a complete solution, just an outline if possible.  The only book I have on differentials does not cover nonlinear and so I'm not even sure if this is a particular case of some subset of study.",['ordinary-differential-equations']
131021,Constrained optimization problems resulting in equal variable assignments,"Suppose you trying to find the extreme value of some function $f(x_1,x_2,\ldots,x_n)$ over the set $\{x_i\}_n$ that is constrained by an unweighted sum as follows $\sum_n x_i=S$.  Let's assume that $f(\cdot)$ is  differentiable and convex. Problems like these commonly arise in engineering and other disciplines, and one usually solves them using the method of Lagrange multipliers by constructing a Lagrangian multiplier $\mathcal{L}(\lambda,x_1,x_2,\ldots,x_n)=f(x_1,x_2,\ldots,x_n)+\lambda(\sum_n x_i-S)$ and finding the stationary point by solving a system of $n+1$ equations $\frac{\partial \mathcal{L}}{\partial x_i}=0~\mbox{for}~i=1,2,\ldots,n; \frac{\partial \mathcal{L}}{\partial \lambda}=0$. I am interested in $f(\cdot)$'s for which the result is equal $x_i$'s: $x_1=x_2=\ldots=x_n$ (under unweighted sum constraint).  Is there a way to ""test"" $f(\cdot)$ (or its derivates) to infer that the variable assignments must be equal at the stationary point? Seems to me that some kind of symmetry property is required in $f(\cdot)$ for that to occur, but I can't quite formulate it.","['optimization', 'multivariable-calculus']"
