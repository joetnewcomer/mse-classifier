question_id,title,body,tags
421116,How to evaluate the trigonometric integral $\int \frac{1}{\cos x+\tan x }dx$,$$\int \dfrac{1}{\cos x+\tan x }dx$$ This can be converted to $$\int \dfrac{\cos x}{\sin x+\cos^2x}dx$$ But from here I get stuck. Using t substitution will get you into a mess. Are there any tricks  which can split the fraction into simpler forms?,"['trigonometry', 'calculus', 'integration', 'indefinite-integrals']"
421134,Two definitions of a bounded set in topological vector spaces,"Let $X$ be a topological vector space. A subset $E$ is bounded if to
every open set $V$ containing $0$ in $X$ there corresponds a number $s>0$ such that $E\subseteq tV$ for every $t > s$ . Would the content of this
definition be altered if it were required merely that to every
open set $V$ containing $0$ there corresponds some $t>0$ such that $E\subseteq tV$ ?","['topological-vector-spaces', 'functional-analysis']"
421138,What is the solution if $a=1$,"We have the equation $$x_n = a^nx_0+b\,\left(\dfrac{1-a^n}{1-a}\right).$$ I had to find the solution if $a=1$ and $a=-1$ For $a=1$ we must divide by $0$ which is of course impossible, but I wonder; does that render $a=1$ 'solutionless' or does that term just disappear, leaving $x_n = x_0$ for $a=1$? Am I correct to say that for $a=-1$ we have the solution $x_n = - x_0 + B$ for uneven $n$ and $x_0$ for even $n$?",['algebra-precalculus']
421142,Ergodic action of a group,"What does it mean and how is it defined if the action of a group is meant to be ergodic?
Thank you for your replies!","['ergodic-theory', 'measure-theory', 'lie-groups', 'group-actions']"
421161,How to integrate a binomial expression without expanding it before?,"Let $(3-x^2)^3$ be a binomial expression. What is the integral of such expression? First I tried integration by substitution, because there is a composition of two functions. But$\displaystyle\frac{d}{dx}(3-x^2)=2x$ and I learned that this method only works if the integrand has the derivative of the inner function, multiplied by a constant. Then I used what I learned about power series: $$(3-x^2)^3=\binom{3}{0}3^3+\binom{3}{1}3^2(-x^2)+\binom{3}{2}3(-x^2)^2+\binom{3}{3}(-x^2)^3$$ And so, \begin{align}
(3-x^2)^3&=27-27x^2+9x^4-x^6\\
\int(3-x^2)^3\,\mathrm dx&=\int(27-27x^2+9x^4-x^6) dx
\end{align} Finally: $$\displaystyle\int(3-x^2)^3dx=27x-9x^3+\frac{9}{5}x^5-\frac{1}{7}x^7+C$$ But imagine that the power is $10$, or maybe $20$? There is any way to integrate this kind of expression without expand it? Thanks.","['calculus', 'integration', 'indefinite-integrals']"
421179,Rationalizing expressions,"In my precalc book, I have the following problem: Calculate $a+b+c$ if $a,b,c\in\mathbb{Q}$ and 
$$\sqrt[3]{\sqrt[3]{2}-1}=\sqrt[3]{a}+\sqrt[3]{b}+\sqrt[3]{c}$$ I think that the RHS can stay untouched, while operating the LHS, but I can't find a way to factor $\sqrt[3]{2}-1$ as the third power of something. Any help is greatly appreciated. With the help of Olegg,  i got the solution
$$\sqrt[3]{\sqrt[3]{2}-1}=\sqrt[3]{\frac{(\sqrt[3]{2}-1)(\sqrt[3]{4}+\sqrt[3]{2}+1)}{\sqrt[3]{4}+\sqrt[3]{2}+1}}$$
$$\sqrt[3]{\frac{1}{\sqrt[3]{4}+\sqrt[3]{2}+1}}$$
$$\sqrt[3]{\frac{1}{(\sqrt[3]{\frac{1}{3}}+\sqrt[3]{\frac{2}{3}})^3}}$$
$$\frac{1}{\sqrt[3]{\frac{1}{3}}+\sqrt[3]{\frac{2}{3}}}$$
$${\sqrt[3]{\frac{1}{9}}-\sqrt[3]{\frac{2}{9}}+\sqrt[3]{\frac{4}{9}}}$$
$$a+b+c=\frac{1}{3}$$",['algebra-precalculus']
421211,"$A$ tosses a fair coin $n+x$ times, $B$ tosses a fair coin $n$ times","This is an extension of the $n+1$ vs n problem here: Probability of $5$ fair coin flips having strictly more heads than $4$ fair coin flips So a common way to think about this is to say that after $n$ tries, both players have the same expected number of heads. So player $A$ has a $50\%$ chance of winning because he gets a head with $50\%$ chance on his last try. But if he gets $x$ more tries than player $B$, does that still stand? He wins with probability $1 - \frac{1}{2^x}$? I did the calculations for a few $n$'s and $x$'s and it doesn't seem to be the case. What's the answer here? Thanks!",['probability']
421215,Definition of a monoid: clarification needed,"I'm only in high school, so excuse my lack of familiarity with most of these terms! A monoid is defined as ""an algebraic structure with a single associative binary operation and identity element."" A binary operation, to my understanding, is something like addition, subtraction, multiplication, division i.e. it involves 2 members of a set, a single operation, and the resulting third member within that set. And an identity element is a special type of element of a set, with respect to a binary operation on that set, which leaves other elements unchanged when combined with things. Examples are ""$0$"" as an additive identity and ""$1$"" as a multiplicative identity. How does this definition correspond to a category with a single object? What are some examples of monoids?","['category-theory', 'monoid', 'abstract-algebra', 'definition']"
421220,Proving a function is continuous on all irrational numbers,"Let $\langle r_n\rangle$ be an enumeration of the set $\mathbb Q$ of rational numbers such that $r_n \neq r_m\,$ if $\,n\neq m.$ $$\text{Define}\; f: \mathbb R \to \mathbb R\;\text{by}\;\displaystyle f(x) = \sum_{r_n \leq x} 1/2^n,\;x\in \mathbb R.$$
  Prove that $f$ is continuous at each point of $\mathbb Q^c$ and discontinuous at each point of $\mathbb Q$. I find this question very challenging and have no idea even how to start off with the proof. Please suggest a proof or any hint.","['sequences-and-series', 'real-analysis', 'analysis']"
421225,"Terminological question on ""action factors through""",What does it mean that the action of a group on some space factors through the action of another one?,"['terminology', 'group-theory']"
421229,dimension of space of polynomials,"Let $\mathcal P_k^n$ be the space of all polynomials of degree $\leq k$ in $n$ variables. Prove $\dim\mathcal P_k^n = {n+k\choose k}$. I tried showing this by taking $n\in\mathbb N$ an arbitrary number and using induction for $k$ to show that the formula holds, but couldn't work out the inductive step. How do I work this out?","['vector-spaces', 'polynomials', 'combinatorics']"
421238,Evaulate $\lim_{x\to\infty}\frac{3x^2-36x+12}{5x^2+113x-2}$,"Question:  Find the limit, $$\lim_{x\to\infty}\frac{3x^2-36x+12}{5x^2+113x-2}$$ The limit should be $\frac{3}{5}$ since when $x$ approaches infinity since $\frac{3*\infty^2}{5*\infty^2}$ and infinity squared cancels out.  Am I correct?","['calculus', 'limits']"
421249,Integrating $\exp (\exp (x))$,How am I able to integrate $e^{e^x}$? $$\int e^{e^x}dx$$ Am I suppose to use $u$ substitution?  But what should I let $x$ be?  And what should $dx$ be? Thanks for the help!,"['calculus', 'integration']"
421262,"Find $\frac{d x}{d y_0}$ of $x'=xy+t^2,y'=\frac{-y^2}{2}$ for $x_0=3$ and $y_0=2$","We've got:
$x' = xy +t^2$, $y' = \frac{-y^2}{2}$, $x(1)=x_0$,$y(1)=y_0$.
My task is to find $\frac{d x}{d y_0}$ for $x_0 = 3$ and $y_0 = 2$. I computed $y$ for $y_0=2$ and so we have $y(t) = \frac{2}{t}$, hence $x' = x \cdot \frac{t}{2} + t^2$.
But how to find $\frac{d x}{d y_0}$? I know how to find $\frac{d x}{d x_0}$, having a certain theorem, but it's not the question... Can anybody help?",['ordinary-differential-equations']
421263,Help with understanding the evaluation of a real integral using complex analysis,"This morning I decided to look up some complex analysis, and I came across this Wikipedia section , where the following integral is evaluated: $$\int_0^3 \frac{x^{3/4}(3-x)^{1/4}}{5-x}dx$$ There are a couple things I do not quite understand and I was wondering whether someone could clear them up for me. For convenience I shall copy and paste the sections that are troubling me. Here is the drawing alongside: We will construct $f(z)$ so that it has a branch cut on $[0, 3]$ , shown in red in the diagram. To do this, we choose two branches of the logarithm, setting $z^{\frac{3}{4}} = \exp \left (\frac{3}{4}\log(z) \right )$ where $-\pi \le \arg(z) < \pi $ and $(3-z)^{\frac{1}{4}} = \exp \left (\frac{1}{4} \log(3-z) \right )$ where $0 \le \arg(3-z) < 2\pi$ I don't understand the branch choice. Looking at the contour, I would have written the opposite: $0\leq \arg z<2\pi$ and $-\pi\leq \arg (3-z)<\pi$ . Why is this flawed thinking? Sorry if that's a very silly question. Let $z=r$ (in the limit, i.e. as the two green circles shrink to radius zero), where $0 ≤ r ≤ 3$ . Along the upper segment, we find that $f(z)$ has the value: $$r^{\frac{3}{4}} \exp(\tfrac{3}{4}0 \pi i) (3-r)^{\frac{1}{4}} \exp(\tfrac{1}{4}2 \pi i) = i \, r^{\frac{3}{4}} (3-r)^{\frac{1}{4}}$$ and along the lower segment, $$r^{\frac{3}{4}} \exp(\tfrac{3}{4}0 \pi i) (3-r)^{\frac{1}{4}} \exp(\tfrac{1}{4}0 \pi i) = r^{\frac{3}{4}} (3-r)^{\frac{1}{4}}$$ It follows that the integral of $\dfrac{f(z)}{5-z}$ along the upper segment is $-iI$ in the limit, and along the lower segment, $I$ . I do not understand why it is $-iI$ along the upper segment rather than $iI$ . $$(1-i) I = -2\pi i \left( \mathrm{Res}\left( \frac{f(z)}{5-z},5\right) + \mathrm{Res}\left( \frac{f(z)}{5-z} ,\infty\right)\right)$$ This is where I realised that I must be completely misunderstanding the idea. From what I understood, neither the singularity at $2$ nor that at $\infty$ are in the contour, so I do not see how they come into play and how the residue theorem is being used. I realise this is the whole point of all the previous working, but I don't understand what is going on. Also, if instead the poles were not on the real line - for instance, if we were considering: $$\int_0^3 \frac{f(x)}{1+x^4}\,dx$$ How would one adapt the residue calculations? Would we consider all the poles? A big thank you to anyone who answers.","['definite-integrals', 'integration', 'complex-analysis', 'contour-integration']"
421274,Efficient way to compute $\sum_{i=1}^n \varphi(i) $,"Given some upper bound $n$ is there an efficient way to calculate the following: $$\sum_{i=1}^n \varphi(i) $$ I am aware that: $$\sum_{i=1}^n \varphi(i) = \frac 12 \left( 1+\sum_{i=1}^n \mu(i) \left \lfloor \frac ni \right\rfloor ^2 \right) $$ Where: $\varphi(x) $ is Euler's Totient $\mu(x) $ is the Möbius function I'm wondering if there is a way to reduce the problem to simpler computations because my upper bound on will be very large, ie: $n \approx 10^{11} $. Neither $\varphi(x) $, nor $\mu(x) $, are efficient to compute for a large bound of $n$ Naive algorithms will take an unacceptably long time to compute (days) or I would need would need a prohibitively expensive amount of RAM to store look-up tables.","['prime-numbers', 'totient-function', 'summation', 'number-theory']"
421284,Minimizing Appreciating Quantities vs. Maximizing Depreciating Quantities,"Suppose you have a set $S = \{r_1, ..., r_n :\, r_k \in (1, \infty)\, \forall \,k \in \{1,...,n\}\}$. Find a bijective mapping $f: \{0,...,n-1\}\rightarrow \{1,...,n\}$ that minimizes
\begin{align*}
\sum_{k=0}^{n-1}r_{f(k)}^{k}
\end{align*} Solution : Let $f$ be a mapping that satisfies $k< j \, \rightarrow \, r_{f(k)} \ge r_{f(j)}$. In other words, $f$ sorts $S$ in non-increasing order. To prove optimality, consider a bijective mapping $g$ defined on the same domain and range, and suppose $g$ minimizes the sum. Further, suppose $g$ differs from $f$. Therefore, there must be some inversion: an index $i$ such that $r_{g(i)} < r_{g(i+1)}$. We will show that swapping these two images decreases the total sum, contradicting the optimality of $g$. Notice that if we only swap these two, then the remaining terms of the sum are unchanged. To show that the swap decreases the total sum, we must verify
\begin{align*}
r_{g(i+1)}^{i}+r_{g(i)}^{i+1} &< r_{g(i)}^{i}+r_{g(i+1)}^{i+1}\\
r_{g(i)}^{i+1} -r_{g(i)}^{i}&< r_{g(i+1)}^{i+1}-r_{g(i+1)}^{i}&\iff \\
r_{g(i)}^i(r_{g(i)} - 1)&<r_{g(i+1)}^i(r_{g(i+1)}-1)&\iff 
\end{align*}
But this final inequality holds because each rate exceeds one and by assumption $r_{g(i)} < r_{g(i+1)}$. This contradicts the optimality of $g$, hence any mapping that deviates from sorting the rates in descending order is not optimal. Now suppose you have a set $S = \{r_1, ..., r_n :\, r_k \in (0, 1)\, \forall \,k \in \{1,...,n\}\}$. Find a bijective mapping $f: \{0,...,n-1\}\rightarrow \{1,...,n\}$ that maxmizes
\begin{align*}
\sum_{k=0}^{n-1}r_{f(k)}^{k}
\end{align*} Note : It is interesting to point out that the naïve attempt of sorting $S$ in ascending order fails to maximize the sum in certain cases. Take for example the set
\begin{align*}
\biggr\{\frac{1}{10}, \frac{1}{2}, \frac{4}{7}, \frac{3}{5}, \frac{2}{3}\biggr\}&\\
\biggr(\frac{1}{10}\biggr)^0 + \biggr(\frac{1}{2}\biggr)^1 + \biggr(\frac{4}{7}\biggr)^2 + \biggr(\frac{3}{5}\biggr)^3 + \biggr(\frac{2}{3}\biggr)^4&\approx 2.240061476\\
\biggr(\frac{1}{10}\biggr)^0 + \biggr(\frac{4}{7}\biggr)^1 + \biggr(\frac{2}{3}\biggr)^2 + \biggr(\frac{3}{5}\biggr)^3 + \biggr(\frac{1}{2}\biggr)^4&\approx 2.294373016
\end{align*} You can verify that this second sum is optimal in Mathematica, using the following code S = {1/10, 1/2, 4/7, 3/5, 2/3}; p = Range[0, 4]; perm = Permutations[S]; totals = Total[#^p] & /@ perm; maxPos = First[Flatten[Position[totals, Max[totals]]]] perm[[maxPos]] So my question is: can you think of an efficient rule to maximize the second sum? Currently my best solution is to literally enumerate all $n!$ possible ways to order the terms. Also, why is this problem so much harder? I feel it has something to do with the unboundness of the first problem. A colleague of mine suggested that the solution space of first problem is convex while the second is not, though I do not have a good grasp on why that is. (I know the definition of convexity, but what part of the problem formulation makes the solution space second problem not convex?) Any help / input would be greatly appreciated.","['optimization', 'summation', 'number-theory', 'combinatorics']"
421295,Relations between $\|x+a\|$ and $\|x-a\|$ in a normed linear space.,"1) Can it happen that $\|x+a\|=\|x-a\|=\|x\|+\|a\|$ when $a\ne0$? 2) How large can $\min(\|x+a\|,\|x-a\|)/\|x\|$ be when $\|x\|\ge \|a\|$? (For a inner-product space, the answers are no and $\sqrt{2}$ I think.)","['normed-spaces', 'inner-products', 'functional-analysis']"
421300,Roots of $z^{2n} + \alpha z^{2n -1} + \beta ^2$,"I've been looking at a problem available here . The problem is: Let $n$ be a natural number, and $\alpha$, $\beta$ nonzero reals. Show that the number of roots of $p(z) = z^{2n} + \alpha z^{2n -1} + \beta ^2$ in the right half plane is
\begin{cases}
n&&\text{ if } n \text{ is even} \\
n \pm 1 &&\text{ if } n \text{ is odd} 
\end{cases} I have a solution which uses the Argument Principle. The contour consists of two pieces: the line segment on the imaginary axis from $Ri$ to $-Ri$, and the semicircle in the right half-plane which connects $-Ri$ to $Ri$. Evaluating each piece separately and passing to the limit as $R \to \infty$ gives the answer (the $\pm 1$ for the odd case appears because of the branch cut for log$()$ on the negative real axis). My question is this: Is there another way to do this which does not involve using the Argument Principle? I thought maybe you could use a conformal map from the open unit disk to the right half-plane in order to rewrite the problem in terms of zeros on the unit disk (and then use Rouche's Theorem), but I didn't succeed in getting it to work. Thanks again!","['residue-calculus', 'roots', 'complex-analysis', 'polynomials']"
421306,Maximum length of sequence of non-coprimes of $N$ - least upper bound for Jacobsthal's function,"I am looking at the length of the longest sequences of adjacent integers that are not coprime to $N$ for very large $N$. Let $F_N$ be the set of integers less than $N$ which are not coprime with $N$:
$$
F_N = \{x : x \in \mathbb{N}, x \lt N , \gcd(x,N)=1\} 
$$
Let $g(N)$ be the length of the longest sequence of adjacent integers in $F_N$. IE something like:
$$
g(N) = \max (i\in \mathbb{N}) : \exists a\in F_n : \forall b : a \le b \lt a+i, b\in F_N 
$$
What is the least upper bound on $g(N)$, i.e. the maximum length of a sequence of adjacent numbers in $F_N$? Some direct calculation suggests that $g(N)$ steps up when $N$ is primorial, so if $P_i$ is the primorial sequence, then:
$$
\forall N \lt P_i, g(N) \lt g(P_i)
$$
I have no proof of this, but it suggests $g(P_i)$ may be useful in finding a least upper bound. There are $N - \phi (N)$ non-coprimes of $N$ less than $N$ (where $\phi$ represents Euler's totient function, labelled 'phi' in the table below). The assumption that these are evenly spread giving $$g(N) \approx \frac N {\phi (N)}$$ is clearly wrong. Better estimates (for the primorial values of $N$ only) seem to be: $$g(N) \approx 2 \log(N)$$ and $g(N) \approx$ twice the highest prime factor of $N$. However, these are untested outside the data below, and again I have no proof. Below is a table of $g(N)$ and $\phi(N)$ of the primorials: N (Primorial)   Highest prime   phi (N)    g(N) 
 2               2               0          1    
 6               3               1          3    
 30              5               7          5    
 210             7               47         9    
 2310            11              479        13   
 30030           13              5759       21   
 510510          17              92159      25   
 9699690         19              1658879    33   
 300690390       21              49766399   39 Edit: As Gerry Myerson kindly points out in the comments, $g(N)$ is similar to Jacobsthal's function , $j(N)$ series here where $j(N) = g(N)+1$. Several online sources point to a proof by H. Iwaniec in 'On the problem of Jacobsthal, Demonstratio Math. 11, 225–231, (1978)' (original not available online) that: $$j(N) \ll \log \log (N)$$
Whilst this might be true in asymptotic terms, it's trivially not true for the primorials above, and the sequence appears closer to $\log (N)$ than $\log \log(N)$. So the question remains open.","['prime-numbers', 'analytic-number-theory', 'elementary-number-theory', 'number-theory']"
421309,Do proper dense subgroups of the real numbers have uncountable index,"Just what it says on the tin.  Let $G$ be a dense subgroup of $\mathbb{R}$; assume that $G \neq \mathbb{R}$.  I know that the index of $G$ in $\mathbb{R}$ has to be infinite (since any subgroup of $\mathbb{C}$ of finite index in $\mathbb{C}$ has to have index 1 or 2); does it have to be uncountable, though?  All the examples I can readily come up with (e.g. $\mathbb{Q}$, $\mathbb{Z}[\sqrt{2}]$, ...) have uncountable index in $\mathbb{R}$. Thanks!","['abstract-algebra', 'group-theory', 'real-analysis', 'axiom-of-choice']"
421313,Why must the determinant of a matrix with with integer entries be an integer?,"Why must the determinant of a matrix with integer entries be an integer? Note: I know what a determinant of a matrix is, not sure how to explain this question. Is that because if the matrix is made with integers the determinant has to be an integer as well?","['matrices', 'determinant']"
421343,Is this a spectral decomposition/embedding/isometry?,"Given a symmetric p.s.d matrix G, we know that a gram matrix/inner-product representation, X exists where $G=X^TX$ and $X=U\lambda^{1/2}$ via the eigen decomposition of $G$. Now if I take the same matrix $G$ and find a matrix $V$ that minimizes $\sum_{i,j}G_{i,j}\langle v_i.,v_j.\rangle$ w.r.t $V$ where $V^T$ is constrained to be have $V^TDV=I$, where $D$ is a diagonal matrix to avoid the trivial zero matrix solution while minimizing this. Also, lets assume that $V$ is non-singular. Given that all entries are real, then: Basic questions: q1) Is $V$ obtained via an isometric(inner-product) preserving transformation? q2) Is $V$ a spectral decomposition? q3) How is $V$ different from $X$? q4) Any examples of where such $V$ can be useful? q5) Is this some sort of an 'Embedding' from a metric spaces/topology point of view where some relations in the p.s.d matrix $G$ are preserved, via inner-products? i.e, if $G_{ij}$ is large, the minimization makes sure that the inner product $\langle v_i,v_j \rangle$ is smaller in comparison to smaller $G_{ij}'s$ where the inner-product can be larger. You may assume positive semi-definiteness of $G$, if these questions make more sense in that case. Advanced question: Now if I said $v_i$ is instead the result of a vector-valued function $f_i(.)$ in a Reproducing Kernel Hilbert Space (RKHS) where $\sum_{i,j}G_{i,j}\langle v_i.,v_j.\rangle$ is minimized again under orthonormal constraints on $V$ and when the inner-product is the RKHS Norm, what is special or useful about this transformation? You may also assume that we use a regularizer under the RKHS norm on $f's$ as well, if you were worried about some sort of lack of smoothening or trivial solutions. Any where that this is applied? What are its properties from  a functional analysis or linear algebraic or spectral analysis or isometric transformations or Hilbert space point of view? What is happening here !? is what am looking for and the mentioned subjects are the only areas that i reasonably can attempt to understand or have at least studied till now.","['fourier-analysis', 'general-topology', 'linear-algebra', 'hilbert-spaces', 'functional-analysis']"
421344,How to define the natural map on the second page of a spectral sequence?,"I'm learning about spectral sequences in Ravi Vakil's notes , and can't quite figure out how to define the map ($d_2$) on the bottom of page 59 (he describes it as a worthwhile exercise). It should be a morphism of the cohomology complexes on page $E_1$ of the sequence. I tried using short exact sequences to get a connecting homomorphism, but I haven't been able to do it.","['spectral-sequences', 'homological-algebra', 'algebraic-geometry']"
421353,Are primes randomly distributed?,"There is a famous citation that says ""It is evident that the primes are randomly distributed but, unfortunately, we don't know what 'random' means."" R. C. Vaughan (February 1990) I have this very clear but rather broad question that might be answered by different opinions and view points. However, my question is really not targeting an intuitive or philosophical answer, and I beg you for view points with a strength of mathematical foundation. are primes randomly distributed? so then what is 'random' in this context? A posterior I A possible hint comes perhaps from the theory of complex dynamical systems. It can be difficult to tell from data whether a physical or other observed process is random or chaotic, because in practice no time series consists of pure 'signal.' There will always be some form of corrupting noise, even if it is present as round-off or truncation error. Thus any real time series, even if mostly deterministic, will contain some randomness. All methods for distinguishing deterministic and stochastic processes rely on the fact that a deterministic system always evolves in the same way from a given starting point. (ref 1 , 2 , 3 , and "" Distinguishing random from chaotic data "") - complying to latter, remind that every prime $p$ can be trivially identified by a sieving that applies prior primes $q<p$ so it is possible to determine that somehow the system evolves in the same way from a given starting point . Of course to take into account that time must be substituted by a walking index as well. A posterior II Thank you for all of many the comprehensive answers and discussions. This is a quite classic question on MSE and meanwhile we moved much forward. You are right that primes are not random as per above question. Indeed we could show that they are in their sequence some type of ""deterministic chaos"". We don't need the Riemann function for this purpose. The primes sequence is a so called ""ordered iterative sequence"". Meanwhile this has been further elaborated by this source: ""The Secret Harmony of Primes"" (ISBN 978-9176370001) http://a.co/iIHQqR8 Some of you correctly referred to sieving. It is crucial however that we regard sieving procedures as a subset of ""interference"" (incl. frequencies and amplitudes). We can iteratively apply interference rules in order to gain from the first prime 2, the next ordered sequence. This can be iterative continued in an ""ordered"" way and within exact boundaries of p-squares (for 100% certainty). Indeed, in order to construct an ordered sequence of primes you just need to begin with 2. The Riemann approach is charming but would raise difficulties since we don't have yet a proof of the hypothesis that connects the order of the non-trivial zeros with the primes. So if you apply Riemann, as some colleagues here suggest, we would need to say at any time in the begin of your argumentation something like ""provided the Riemann hypothesis would be true..."". Having in mind that the very unique rule that primes follow, is that in an interference scheme all odd prime frequencies dance on the base frequency of 2 (ordered iterative sequence), one may even give it a thought to something of a parallel in the Riemann transformed world, that all non-trivial zeros dance on 1/2. But latter remains not more than a tempting trivial speculation yet.","['prime-numbers', 'probability', 'random', 'number-theory']"
421385,Stabilize Variance for Statistics (Transformation),"Problem: When $Y (> 0)$ has mean and variance equal to $\mu$ and
$\mu/n$ respectively, it is shown in the textbook that the appropriate transformation of Y to stabilize variance is the square root transformation.
Suppose that Y has mean equal to  respectively, it is shown on pages 76-77 of our textbook that the mu and variance equal to $\frac{\mu^4}{n}$ where $n$ is ""large"".  Find the appropriate transformation $Z = f(Y)$ of $Y$ that makes the variance of $Z$ equal to 1. My feedback: I know I am to use the equation $Var(f(Y)) \approx [f'E(Y))]^2 \times Var(Y)$. This is equivalent to $1 \approx [f'(\mu)]^2 \times (\frac{\mu^4}{n} )$. I am struggling to understand how to apply this derivative to solve this equation.","['statistics', 'transformation', 'calculus']"
421387,How to find a solution(s) when given two equations of two variables?,"This is something that I have been trying to comprehend from an algebraic point of view. Take the equations $$3x^2 - 12y = 0$$ $$24y^2-12x = 0 $$
The first equation implies that $x =\pm 2\sqrt{y}$ and the second one that $x =2y^2$. From here, there are many different paths that can be taken to find a solution(s). One that I understand would be to set $2\sqrt{y}=2y^2$ and $-2\sqrt{y}=2y^2$ and this is a valid move since $x=x$. Then we can solve for values of $y$. My first question arises from seeing the way that this problem is solved in my book. They take the equation $x =2y^2$ and substitute this value of $x$ into the first equation so that $$3(2y^2)^2 - 12y =0$$ and then they proceed to solve for $y$ and the results ends up being correct. I understand that, in this case, making such substitution is the same as setting $2\sqrt{y}=2y^2$ and $-2\sqrt{y}=2y^2$ because we were able to obtain explicit descriptions of $x$ in the first place. But, if it is not possible to obtain explicit descriptions for the variables in one of the two equations but possible in the other, will making a similar substitution work? if yes, why? The resulting values of $y$ that satisfy both equations end up being $y=0$ and $y=1$ and the corresponding values of $x$ turn out to be $x=0$ and $x=2$. At this point,the author of the book claims that the final solution to this system of equations are these values of $y$ with their corresponding values of $x$. Is it not necessary to make the other substitution of $x =\pm 2\sqrt{y}$ into the second equation in an attempt to find more pairs of values of $x$ and $y$ that may satisfy both of the given equations? I understand that by graphing this equations (which can be easily roughly sketched) one can see that there are only two points of intersection and thus we know we do not have to do the other substitution. So, is this why the author did not include the other substitution? i.e. is this just a special case were it is obvious that we only have two solutions so we do not have to make the other substitution in order to look for more solutions? And, lastly, when given two equations of two variables such as the above or e.g.$$2x+y=0$$ $$2y+x+1=0$$ How can one be sure to find all the solutions to the system? Is there a way to tell?","['multivariable-calculus', 'algebra-precalculus']"
421388,Differentiation chain rule,"If $f(tx,ty,tz) = t^nf(x,y,z)$ then in my lecture notes it says differentiating this equation with respect to $t$ gives: $$x\frac{df}{dx} + y\frac{df}{dy} + z\frac{df}{dz} = nt^{n-1}f(x,y,z)$$ But why isn't it: $$x\frac{df}{d(tx)} + y\frac{df}{d(ty)} + z\frac{df}{d(tz)} = nt^{n-1}f(x,y,z)\ ?$$",['multivariable-calculus']
421393,"Explicit form of the homeomorphism between $C[0,1]$ and $C[0,1]\setminus 0$","How to construct the homeomorphism between $C[0,1]$ and $C[0,1]\setminus\{\theta\}$
in the explicit form? Here, as usual, $C[0,1]$ is the Banach space of continuous functions $f:[0,1]\to\mathbb{R}$ with the norm
$$
\|f\|=\max\limits_{x\in[0,1]}|f(x)|;
$$
and $\theta(x)\equiv 0$ $\forall x\in[0,1]$.",['functional-analysis']
421398,Formal completion as a functor,"Let $X$ be a scheme with closed subscheme $Z$. There is a natural way to think of $X$ as a functor from schemes to sets, $$X : S \mapsto X(S) = \mathrm{Mor}(S,X).$$ It seems there will be a similar way to understand the completion $\hat X$ of $X$ at $Z$ as a functor, but I am not sure how we do this. Please tell me if you know.",['algebraic-geometry']
421416,"Difference between ""Differentiability"" and ""Differentiation"" .","This is a question of which i have not got a clear visualization still , so thought of asking for your views on it. In the textbook, there are two separate chapters... one is ""Differentiability"" and the other is ""Differentiation"" ... My question is simple : What is the difference between the two? Why i asked this is because when we see the proofs of differentiation of any basic functions (be it $\cos x$, $\sin x$, $e^x$, $\tan x$, etc.) ,they are proved by the method "" differentiation from first principles "" using the basic definition of Differentiability at a point. I tried to read the answers on this forum for similar questions like this previously asked, but the answers to them were  of higher understanding level which i have not been enlightened yet. Is there any simple explanation available for this..If anyone can help me understand the difference between the two in simple words, kindly do so... and thanks a lot for all your help.","['calculus', 'functions']"
421432,Every element of $U + V + W$ can be expressed uniquely in the form $u + v + w$,"Suppose that $U$, $V$ and $W$ are subspaces of some given vector space. With the obvious definition of $U + V + W$, show that every element of $U + V + W$ can be expressed uniquely in the form $u + v + w$, where $u ∈ U$, $v ∈ V$ and $w ∈ W$, if and only if $$\dim(U + V + W) = \dim(U) + \dim(V) + \dim(W)$$ 1) Say every element of $U + V + W$ can be expressed uniquely in the form $u + v + w$. Let $u_1,...,u_n$, $v_1,...,v_m$, $w_1,...,w_q$ be respective bases. Then every element in $U + V + W$ can be expressed uniquely as $$(\alpha_1u_1+...+\alpha_nu_n)+(\beta_1v_1+...+\beta_mv_m)+(\gamma_1w_1+...+\gamma_qw_q)$$ showing that $$\dim(U + V + W) = \dim(U) + \dim(V) + \dim(W)$$ 2) How do I prove the converse? Hints are appreciated (also verification, that what I did, is not useless :) ). Thanks!","['vector-spaces', 'linear-algebra']"
421438,Evaluate complicated sum $\sum \sum {n \choose k}{n-k \choose l}(j-i-1)^{n-k-l}.$,"Evaluate following sum: $$\sum_{1\leqslant i< j \leqslant m}\sum_{\substack{1\leqslant k,l \leqslant n\\ k+l\leqslant n}} {n \choose k}{n-k \choose l}(j-i-1)^{n-k-l}.$$ Hint: use combinatorial interpretation. I tried to solve it with the hint. We have a set of $n$ people, we choose first subset of $k$ of them, then we choose second subset of $l$ people from the rest. Those who weren't chosen neither to first nor to second group we divide into $j-i-1$ groups... I don't know what to do next? Thanks in advance.","['summation', 'discrete-mathematics', 'combinatorics']"
421449,A case of the central limit theorem,"I want to show that $$\frac{\sum_{k=1}^N X_k}{\sqrt{\sum_{k=1}^N X_k^2}} \overset{N\to\infty}{\to} \mathcal{N}(0,1)\text{ in distribution,}$$ where $X_1,X_2,\ldots$ is a sequence of iid random variables with $\mathbb{E}(X_1)=0$ and $\mathbb{E}(X_1^2) = s < \infty$. Now I know about the CLT, i. e. $\frac{\sum_{k=1}^N X_k}{\sqrt{sn}}\to\mathcal{N}(0,1)$, and about the proof with characteristic functions, but calculating the CF of this thing seems a bit cumbersome and I think I am missing a more elegant approach. I would appreciate a hint. TIA","['convergence-divergence', 'weak-convergence', 'probability-limit-theorems', 'probability-theory', 'normal-distribution']"
421476,Change of variables in $k$-algebras,"Suppose $k$ is an algebraically closed field, and let $I$ be a proper ideal of $k[x_1, \dots, x_n]$.  Does there exist an ideal $J \subseteq (x_1, \dots, x_n)$ such that $k[x_1, \dots, x_n]/I \cong k[x_1, \dots, x_n]/J$ as $k$-algebras?","['commutative-algebra', 'algebraic-geometry', 'abstract-algebra']"
421507,Probability (usage of recursion),"In an hour, a bacterium dies with probability $p$ or else splits into two. What is the probability that a single bacterium produces a population that will never become extinct?","['probability', 'recursion', 'limits']"
421521,"Is there a deep reason why $(3, 4, 5)$ is pythagorean? [closed]","It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened, visit the help center . Closed 11 years ago . The triple $(3, 4, 5)$ is a pythagorean triple - it satisfies $a^2 + b^2 = c^2$ and, equivalently, its components are the lengths of the sides of a right triangle in the Euclidean plane. But of course, the first thing anybody notices is that the triple $(3, 4, 5)$ also happens to be an arithmetical succession of small numbers. Is there a deep reason why choosing these three successive numbers just so happens to yield a pythagorean triple? To anyone who feels the question is silly: consider $3^3+4^3+5^3$.","['geometry', 'euclidean-geometry', 'soft-question', 'number-theory']"
421530,analysis problem proof with derivative,"$f:[a,b] \to \mathbb R$ is a continuous function and $0 < a < b$ and $f$ is differentiable in $(a,b)$ and $\dfrac{f(a)}{a} = \dfrac{f(b)}{b}$. Prove that there exists $x \in (a,b)$ so that $xf'(x) = f(x)$.","['real-analysis', 'analysis']"
421545,When a smooth curve is an immersion (John Lee's Smooth manifold book p 156) and Example 7.3,"In John Lee's Intro to Smooth Manifold book (2003 Springer) , I need some help with an example of an immersion. On page 156 Example 7.1 c), If $\gamma(t): J \to M$ is a smooth curve ...then $\gamma$ is an immersion if and only if $\gamma'(t)\neq 0 $ for all $t \in J$. Can someone provide a general explanation for what this $\gamma'(t)\neq 0 $ means?  The curve does not ""stop""? How is the $\gamma'(t)\neq 0 $ condition related to the fact that a map $F$ is an immersion if its pushforward $F_*$ is injective?  Is  $\gamma'(t)$ suppose to be injective (one to one)?  Would this mean $\gamma'(t)$ have to be different for each $t$ ?  Seems like if $\gamma'(t)=a$ a constant, then it will not be one to one? In example 7.3 on p157, how is it that: $$| e^{-2\pi i c n_2}(e^{2\pi ic n_1}-e^{2\pi ic n_2})|=|e^{2\pi ic n_1}-e^{2\pi ic n_2}| $$ I can't figure out what happened to $e^{-2\pi i c n_2}$, am I missing something obvious?",['differential-geometry']
421553,Evaluating $\lim_{x\to0}\frac{x+\sin(x)}{x^2-\sin(x)}$,"I did the math, and my calculations were: $$\lim_{x\to0}\frac{x+\sin(x)}{x^2-\sin(x)}= \lim_{x\to0}\frac{x}{x^2-\sin(x)}+\frac{\sin(x)}{x^2-\sin(x)}$$ But I can not get out of it. I would like do it without using derivation or L'Hôpital's rule .","['calculus', 'limits']"
421563,"Does $(\{0,1\},*)$ form a group?","I am reading my first book on abstract algebra. I am not enrolled in a class on the subject. Given $S = \{0,1\}$. Is $(S,\cdot)$ a group? $S$ is closed under multiplication. $$0\cdot1=0,\,1\cdot0=0,\,0\cdot0=0,\,1\cdot1=1.$$ 
$S$ has an identity, $1$, I think. $$0\cdot1=0,\,1\cdot1=1.$$ I don't believe $S$ satisfies $a\cdot a^{-1}=\operatorname{id}.$ However, zero is excluded when stating that $\mathbb R$ satisfies $a\cdot a^{-1} = \operatorname{id}$ under multiplication. $S$ would be a group if zero is excluded. $1\cdot1=1.$ So is $S$ a group or not?","['group-theory', 'abstract-algebra']"
421574,How to check if a set of vectors is a basis,"OK, I am having a real problem with this and I am desperate. I have a set of vectors $\{(1,0,-1), (2,5,1), (0,-4,3)\}$ . How do I check is this is a basis for $\mathbb{R}^3?$ My text says a basis $B$ for a vector space $V$ is a linearly independent subset of $V$ that generates $V$ . OK then. I need to see if these vectors are linearly independent, yes? If that is so, then for these to be linearly independent the following must be true: $a_1v_1 + a_2v_2 + ... + a_nv_n \neq 0$ for any scalars $a_i$ Is this the case or not? If it is, then I just have to see if $a_1(1,0.-1)+ a_2(2,5,1)+ a_3(0,-4,3) = 0$ or $a_1 + 2a_2 + 0a_3 = 0$ $0a_1 + 5a_2 - 4a_3 = 0$ $-a_1 + a_2  + 3a_3 = 0$ has a solution. Adding these equations up I get $8a_2 - a_3 = 0$ or $a_3 = 8a_2$ so $5a_2 - 32a_2 = 0$ which gets me $a_2 = 0$ and that implies $a_1 = 0$ and $a_3=0$ as well. So they are all linearly dependent and thus they are not a basis for $\mathbb{R}^3$ . Something tells me that this is wrong. But I am having a hell of a time figuring this stuff out. Please someone help, and I ask: pretend I am the dumbest student you ever met.","['vector-spaces', 'linear-algebra']"
421580,Is there a rational number between any two irrationals?,"Suppose $i_1$ and $i_2$ are distinct irrational numbers with $i_1 < i_2$.  Is it necessarily the case that there is a rational number $r$ in the interval $[i_1, i_2]$?  How would you construct such a rational number? [I posted this only so that the useful answers at https://math.stackexchange.com/questions/414036/rationals-and-irrationals-on-the-real-number-line/414048#414048 could be merged here before that question was deleted.]","['irrational-numbers', 'real-analysis', 'rational-numbers']"
421604,$A$ is some fixed matrix. Let $U(B)=AB-BA$. If $A$ is diagonalizable then so is $U$?,"This is from Hoffman and Kunze 6.4.13. I am studying for an exam and trying to solve some problems in Hoffman and Kunze. Here is the question.
Let $V$ be the space of $n\times n$ matrices over a field $F$. Let $A$ be a fixed matrix in $V$. Let $T$ and $U$ be linear operators on $V$ defined by $T(B)=AB$ and $U(B)=AB-BA$. a) (True or False) If $A$ is diagonalizable then $T$ is diagonalizable.
This is true. I can show that both A and T have the same minimal polynomial. So if 
$A$ is diagonalizable then the minimal polynomial of $T$ should be a product of distinct linear factors. Proving that $T$ is diagonalizable. but its the next question I am having trouble with. b)(True or false) If $A$ is diagonalizable then $U$ is diagonlizable. 
I am thinking this is false. But I can't think of of a counter example. May be I am wrong. Can anybody help?. Thanks for all your help.","['matrices', 'linear-algebra', 'diagonalization']"
421627,"Definition of smoothness ""up to boundary""","Let $U\subseteq \mathbb{R}^n$ be an open set and let $f\in\mathcal{C}^k(U)$ for some positive integer $k$. Are the following definitions of $\mathcal{C}^k$ regularity ""up to boundary"" equivalent? (1) There exists an open set $V$ containing $U$ and a $\mathcal{C}^k$ extension of $f$ to $V$. (2) $D^{\alpha}f$ is uniformly continuous on every bounded subset of $U$ for each $|\alpha|\leq k$. I have always interpreted the class $\mathcal{C}^k\left(\overline{U}\right)$ using definition (1) but Evans-PDE uses definition  (2) in its appendix (at least in the edition I am working from). I can easily show that (1)$\Rightarrow$(2) but it would be nice to know if Evans definition is more general than mine.","['multivariable-calculus', 'partial-differential-equations', 'differential-geometry']"
421638,Proving $(A\le B)\vee (B\le A)$ for sets $A$ and $B$,"For any pair of sets $A$ and $B$, we can define $A\le B$ iff there exists injection $f\colon A\rightarrow B$. I am trying prove that 
$$(A\le B)\vee (B\le A).$$ I have tried assuming  $\neg (A\le B)$, then proving $B\le A$ by constructing the required injection, but I haven't been able to make any progress. Any hints, etc. would be appreciated. EDIT Assuming  $\neg (A\le B)$, can you prove there exists a surjection $f: A\rightarrow B$? Then it would be easy, by applying AC, to construct an injection $g: B\rightarrow A$",['elementary-set-theory']
421645,Random walk type problem with time increments,Imagine you have $\$50$ and every $2$ minutes you either gain or lose $33$ cents. How would you model the evolution of the hypothetical bankroll for the next hour? My approach based on what i've read about about random walks: $\sigma=.33\cdot(\frac{60}{2})^.5$ $\mu=50.$ Plug these into a normal distribution to find the probability of having $x$ amount of money after an hour. Some sources say if $\frac{1}{2}$ then $\sigma$ would have to be divided by $2$. But others just use the square root.,"['statistics', 'random-walk', 'probability']"
421646,Understanding Newman's proof of the prime number theorem,"I am trying to understand D.J. Newman's proof of the prime number theorem, as presented by D. Zagier . I am not too familiar with analysis, and so there are some things I don't understand. In (III), what does Zagier mean by ""$\theta(x)$ changes by $O(\log x)$ if $x$ changes by $O(1)$?"" I know the definition of Big O notation, but not what this grammatical construction means. In (V), I am having trouble parsing the integral
$$ \int_1^\infty \frac{d \theta(x)}{x^s}. $$
What does the notation $d \theta(x)$ mean? Is $\theta(x)$ the variable of integration in some sense? If so, how? (This may be a naive question.) In the lower bound for $\theta(x)$ (on p. 707), in the bounds for the first summation -- $x^{1-\epsilon} \leq p \leq xx$ -- is $xx$ a typo, supposed to be $x$, or is it correct? If it's correct, why is the first inequality true? I don't understand why the lower bound for $\theta(x)$,
$$ \theta(x) \geq (1-\epsilon) \log x (\pi(x) + O(x^{1-\epsilon})), $$
is good enough. As $\epsilon \rightarrow 0$, $O(x^{1-\epsilon})$ tends to $O(x)$, and based on my interpretation of the previous summations, I expect this error term to be negative. Since $\pi(x) \leq x$, I basically don't understand why this is saying more than that $\theta(x) \geq 0$. Thank you!","['number-theory', 'asymptotics', 'analysis', 'prime-numbers', 'complex-analysis']"
421712,How do we know we need the axiom of choice for some theorem?,"I have been working through Munkres Topology book and in an exercise he says that there was a theorem he proved in a previous section that relied on the axiom of choice and the task is to find it. I am fairly certain I found it (and judging by the the research for this question I did find a theorem that requires choice) and the theorem is: $\bf{Theorem \; 1}:$ A countable union of countable sets is countable. Anyways, I have been using the theorem quite a bit in exercises and I have been thinking about the theorem and the proof and deciding whether or not some variant of the theorem needs choice. I realized though that I don't know that choice is needed, I just know that the proof I know of is a proof that uses choice. This leads to my question(s): How can we tell a theorem needs the axiom of choice (or some weaker version of choice) to be proven? I suspect that there might be no good specific answer that would apply to all theorems that seem to need choice, so is there some general strategy that would work for theorems like $\bf{Theorem \; 1}$ or similar things that could pop up in Munkres? It is worth pointing out that here that Arturo Magidin mentions that without choice it has been shown that it is consistent with ZF that $\bf{Theorem \; 1}$ is false. In my mind, I am thinking that a strategy would be to show that without choice the theorem doesn't need to be true. If that strategy is particularly effective is there some ""canonical"" (I can't quite think of a better word for what I am talking about) way to do this? With my little experience with AC I guess a good heuristic would be if the theorem deals with infinite amount of arbitrary sets then AC would be needed, but I am sure that there are some theorems that look like AC would be needed, and is the obvious way to go about proving them, but there is actually some clever way around it. I guess this is where my questions are coming from. Also, I am interested in learning more on similar things so references, books or articles, would be appreciated. Would Set Theory by Kunen be a good book for learning to answer questions like this?","['elementary-set-theory', 'reference-request', 'axiom-of-choice']"
421735,metric and measure on the projective space,"Let $RP^n$ be the $n$-dim real projective space. I have the following four questions. What is the so called standard metric on $RP^n$? More generally, consider a metric space $M$ with an equivalent relation ~. Then there is a natural way to define a (pseudo)metric on the quotient space $M$\~. See Why are quotient metric spaces defined this way? for details. It seems to me that if we regard $RP^n$ as the quotient from $R^n$\{0}, then the induced pseudo metric is not a metric, however, if we regard $RP^n$ as the quotient from $S^n$, then the induced pseudo metric is a metric. Am I correct? Is there a contradiction? What is the standard measure on $RP^n$? How to in general define a measure on the quotient of a measure space? (similar to question 2) Thank you for reading and thinking for my long questions. I put them in a logical order, however, I'm mostly interested in the third question. If you can't help me with all the questions, please try Q3 first.","['general-topology', 'measure-theory', 'metric-spaces', 'geometric-measure-theory']"
421741,What is exactly the meaning of being isomorphic?,"I know that the concept of being isomorphic depends on the category we are working in. So specifically when we are building a theory, like when we define the natural numbers, or the real numbers, or geometry, I often hear that people say that such a structure is complete in the sense that any other set that satisfy their properties is isomorphic. What is the real meaning and implications of being isomorphic? At least to me it is not very clear. Let's see for example the axioms of geometry. To begin with, there are different systems of axioms, all of them trying to define that object called geometry. So if we are talking about the same object, I suppose the theories generated should be isomorphic each other. Does this make sense? Again what is the real implication of being isomorphic? According to the definition there should be a bijection between the sets, which means that it cannot be larger sets (with a higher cardinality, or even different cardinality) that hold the propierties of the given structure and also the operations defined on each structure should be mantained in the bijection. Is this all? Please, if I'm saying bullshit forgive me I just want to really understand this concept. I will really appreciate any comment. Edit: In my example I meant Euclidian Geometry.","['category-theory', 'model-theory', 'abstract-algebra']"
421750,A problem on continuity of a function on irrationals for $f(x) = \sum_{r_n \leq x} 1/n^2$ [duplicate],"This question already has an answer here : Real Analysis Question: continuity of $f(x) = \sum_{q_n<x} 1/n^2$ (1 answer) Closed 10 years ago . Let $\langle r_n\rangle$ be an enumeration of the set $\mathbb Q$ of rational numbers such that $r_n \neq r_m\,$ if $\,n\neq m.$ $$\text{Define}\; f: \mathbb R \to \mathbb R\;\text{by}\;\displaystyle f(x) = \sum_{r_n \leq x} 1/n^{2},\;x\in \mathbb R.$$
  Prove that $f$ is continuous at each point of $\mathbb Q^c$ and discontinuous at each point of $\mathbb Q$. I find it difficult to prove especially the continuity on irrationals, I proved the discontinuity on a rational number in the following way is it correct? Let $ c \in \Bbb Q $ then $ c=r_n $ for some $n \in \Bbb N  $ and Let $ \epsilon_0 = 1/n^{2} $ Let $\delta > 0 $ be arbitrary and let $ x \in (c-\delta. c+\delta)$ such that $x<c $ Then $|f(x)-f(c)|>1/n^{2}=\epsilon_0 $ How do prove that it is continuous on irrationals?","['sequences-and-series', 'real-analysis']"
421751,"Rationale behind MLE of $f_{\theta}(x) = \frac{1}{\theta} I_{\{1, \dots,\theta\}}(x)$","Our probability density for $\theta \in \{1,\dots,\theta_0\}$ is $$f_{\theta}(x) = \frac{1}{\theta} I_{\{1, \dots,\theta\}}(x)$$ Let $X_{(n)}$ be the largest order statistic. Acooding to the solution, the likelihood function is $$\frac{1}{\theta^n} I_{\{X_{(n)}, \dots,\theta_0\}}(x)$$ I don't get why. Also, $X_{(n)}$ is supposed to be the MLE of $\theta$. Why?","['statistics', 'maximum-likelihood', 'uniform-distribution', 'probability-distributions']"
421769,Permutations without any cycle of length $k$,"Let $k\in\mathbb{N}$ be a fixed natural number and $f_k(n)$ denotes the number of all permutations of $\left\{1,..,n\right\}$ that does not contain any cycle of length $k$. Find as simple as possible ( but not simpler ;) ) formula for $f_k(n)$ and count $\lim_{n\to \infty} \frac{f_k(n)}{n!}$ I was learning for the test and encountered this problem. It's very unusual I think, I completely don't know how to approach. Can anybody help?",['combinatorics']
421813,Projection matrix onto null space,I have a matrix $H$ and I want to find the projection matrix onto null space. How can I do this?,"['matrices', 'linear-algebra', 'matlab']"
421868,Equivalent forms of the recursion theorem,"I have found the next two definitions on the theorem of recursion: Definition 1: For any set $A$, any $a\in A$ and any function $g:A\times \mathbb{N} \longrightarrow A$, there exists a unique funtion $f: \mathbb{N}\longrightarrow A$ such that $1) f(0)=a$ $2) f(n+1)= g(f(n), n)$ Definition 2: For any set $A$, anty $a\in A$ and any function $g:A\longrightarrow A $, there exists a unique funtion $f: \mathbb{N}\longrightarrow A$ such that $1) f(0)=a$ $2) f(n+1)=g(f(n))$ These are my questions: a) My first impression is that the the second definition is not very appropriate for examples in which we wanted to have the function $g$ explicitly. For example in defining the factorial function ($n!$) we can say that $g$ is defined as $g(x,n)=x(n+1)$ in such a way that using definition 1 we get $f(n+1)=g(f(n),n)=f(n)(n+1)$. If we use the second definition I don't know if we can give the function $g$ explicitly (???? I'd like to know). b) Intuitivelly I see that both formulas are equivalent (They have to, I guess, because they are given as alternative definitions for the recursion theorem). My attempt to prove that $1\longrightarrow 2$ is by defining $g^{*}:A\times \mathbb{N}\longrightarrow A$ such that $g^{*}(x,n)=g(n)$, supossing $g$ is given. Then we have $f(n+1)=g^{*}(x,n)=g(f(n))$. I have not idea to prove $2\longrightarrow 1$ (I probably would do the same, namly define $g^{*}(x)=g(x,n)$, supposing $g$ is given. But I'm not sure because $g(x,n)$ might have different values depending on $n$).",['elementary-set-theory']
421886,Ordered set partitions,"Let $a_n$ be a number of ordered partitions of the set $\left\{1,\ldots,n\right\}$, which means that order of elements in block is not relevant, but order of blocks does matter. (so $a_n = \sum_k\left\{n\atop k\right\}k!$, if I'm not mistaken). Prove recurrence relation:
  $$a_0=1; \ a_n=\sum_{i=0}^{n-1}{n\choose i} a_i$$ I don't know if I'm tired or it is just difficult. I tried with combinatorial interpretation but nothing came to my mind, so I tried with formula $a_n = \sum_k\left\{n\atop k\right\}k!$ but his also led me to nowhere. Can anybody help?",['combinatorics']
421892,How to calc arc sine without a calculator?,How can I find the arc sine of a sine without using a calculator? Thank you.,"['trigonometry', 'functions']"
421897,How to calc the square root of a number without calculator? [duplicate],This question already has answers here : How to manually calculate cube roots (5 answers) Closed 11 years ago . How can I find the square root of a number without using a calculator?,"['algebra-precalculus', 'functions']"
421903,Ideals (one-sided ideals) of $n×n$ upper triangular matrices,"Is there any characterization of ideals (one-sided ideals) of $n\times n$ upper triangular matrices? I have just seen in monthly journal about $2 \times 2$ matrices in the below article
Left and Right Ideals in the Ring of $2 \times 2$ Matrices
Henryk Minc
The American Mathematical Monthly
Vol. 71, No. 1 (Jan., 1964), pp. 72-75","['matrices', 'ideals', 'linear-algebra']"
421905,Equilibrium point linear differential equation,"as far as I know, there is a comfortable criterium to find out whether the y=0 solution of $y'=Ay$ is stable if the maximum real part of the eigenvalues of A is negative and unstable if it is positive. now I was wondering whether this is also true if we can find a different solution $y=c$ where c is not necessarily 0. or is this really only applicable to the situation $y=0$?","['ordinary-differential-equations', 'calculus', 'real-analysis']"
421915,Need to find function by the data,"I have found interesting sequence, but I can't find its function. Here are the input and output data: 0   30
1   26.7
2   24
3   21.8
4   20
5   18.5
6   17.1
7   16
8   15
9   13.3
10  12
11  10.9
12  10
13  9.2
14  8.6
15  8
16  7.5
17  6.7
18  6
19  5.5
20  5
21  4.6
22  4.3
23  4
24  3.7
25  3.3
26  3
27  2.7
28  2.5
29  2.3
30  2.1
31  2 Does anybody know what is behind this sequence? Please help.","['graphing-functions', 'data-analysis', 'functions']"
421928,"Hartshorne exercise 1.6.4 : Is it true that $\mathcal{O}_{P,X} \cong \mathcal{O}_{\varphi(P),\Bbb{P}^1}$?","Let us work over a fixed algebraically closed field $k$ and consider a non-singular projective curve $X$ and $\varphi : X \to \Bbb{P}^1$  a non-constant morphism. My question is: For $P \in X$, do we have an isomorphism
    $$\mathcal{O}_{P,X} \cong \mathcal{O}_{\varphi(P),\Bbb{P}^1}?$$ The reason I ask this question is because I want to prove that $\varphi$ is surjective. I believe I have almost done this, and this is the last part in the proof that I basically need. Now I have determined that $\varphi$ is actually a dominant morphism (by topological considerations and using that the cardinality of $X$ is necessarily infinite). So actually I already know that $$\varphi_P^\ast : \mathcal{O}_{\varphi(P),\Bbb{P}^1} \to \mathcal{O}_{P,X}$$ is injective. How can I prove that it has to be surjective? Do I know that $\mathcal{O}_{P,X}$ is finitely generated (as a module) over the image of $\mathcal{O}_{\varphi(P),\Bbb{P}^1}$?",['algebraic-geometry']
421943,Jacobian determinant and orientation,"So in Jacobian determinant, it is often said that it gives information about whether Jacobian matrix changes orientation, but I cannot get what orientation exactly in this context.","['multivariable-calculus', 'linear-algebra']"
421946,"Number of sequences with n digits, even number of 1's","ASKED: Let $c_n$ be the number of sequences with $n$ digits from $\{1,2,3,4\} $ with an even number of $1's$. Determine $c_n$ for $n \geq 0$. GIVEN RESULT: $c_{n+1} = 3 \cdot c_n + 1 \cdot (4^n-c_n) \Rightarrow c_{n+1} = 2 \cdot c_n + 4^n$
and  from this $c_n$ can be solved. MY RESULT My problem with this question is that I don't get how to come to the result: $c_{n+1} = 3 \cdot c_n + 1 \cdot (4^n-c_n) $ My current reasoning is as follows: $c_{n+1}$ will have one more slot than $c_n$. Since $c_1 = 3$, this means that there are $3 \cdot c_n$ more possibilities for $c_{n+1}$. Also, because $n$ can be either even or uneven, we need to incorporate the number of possibilities that now 'become available' if $n+1$ is an even number and $n$ was not. This is done by adding $(4^n-c_n)$ to the equation such that the final (intermediate step) equation becomes: $c_{n+1} = 3 \cdot c_n + 1 \cdot (4^n-c_n) $. Writing this out gives $c_{n+1} = 2 \cdot c_n + 4^n$. Note that I came up with this reasoning after looking at the answer. Now this question is supposed to be linked to combinatorics so I think there is a more structural and mathematic approach to this question (instead of reasoning). I am curious to what this approach might be. So what I'm looking for is either a very structural approach to solving these kind of questions or a combinatorial approach to this question. Thanks in advance. I will be actively commenting if needed.","['recurrence-relations', 'combinatorics']"
421958,A subset of a field that is a subfield,"It can be verified that the following assertion is true: a subset $S$ of a field $F$ is a subfield if $S$ contains the additive and multiplicative identities 0 and 1, if $S$ is closed under addition, multiplication, additive inverses, and $S-\{0\}$ is closed under multiplicative inverses.  An exercise asks to show that the condition $0,1 \in S$ can be replaced by the condition that ''$S$ contains at least two elements''.  The hint given is ''Consider $ax=a$.'' Suppose $S$ contains at least two distinct elements, say $a,b$.  By the hypotheses, $-a \in S$, so $a+(-a) =0 \in S$.  At least one of $a,b$ is nonzero, say $a \ne 0$. Then its inverse $a^{-1} \in S$ and so the product $a a^{-1}=1 \in S$.  Thus, $0,1 \in S$.  This solves the exercise.  My question is whether there's another solution that uses the hint of considering $ax=a$.","['commutative-algebra', 'abstract-algebra', 'field-theory']"
421959,How prove this $\frac{\sin{(A-B)}\sin{(A-C)}}{\sin{2A}}+\frac{\sin{(B-C)}\sin{(B-A)}}{\sin{2B}}+\frac{\sin{(C-A)}\sin{(C-B)}}{\sin{2C}}\ge 0$,"let $0<A,B,C<\dfrac{\pi}{2}$,and $A+B+C=\pi$,prove that $$\dfrac{\sin{(A-B)}\sin{(A-C)}}{\sin{2A}}+\dfrac{\sin{(B-C)}\sin{(B-A)}}{\sin{2B}}+\dfrac{\sin{(C-A)}\sin{(C-B)}}{\sin{2C}}\ge 0$$ my idea
$$\sin{2B}\sin{2C}\sin{(A-B)}\sin{(A-C)}+\sin{2A}\sin{2C}\sin{(B-C)}\sin{(B-A)}+\sin{2A}\sin{2B}\sin{(C-A)}\sin{(C-B)}\ge 0$$ But following I can't, and I find sometime,I have find the Similar quetions: http://www.artofproblemsolving.com/Forum/viewtopic.php?p=128597&sid=55397cc8fe6b896712e5495b124dd247#p128597","['trigonometry', 'inequality']"
421995,If $x+{1\over x} = r $ then what is $x^3+{1\over x^3}$?,"If $$x+{1\over x} = r $$ then what is $$x^3+{1\over x^3}$$ Options: $(a) 3,$ $(b) 3r,$ $(c)r,$ $(d) 0$",['algebra-precalculus']
422004,Solve $g(g(x))=f(x)$,"If $f(x)$ is a continuous and monotonically increasing function on an interval $(0,∞)$ and $f(x)>0$ for every $x>0$, then does there always exist a continuous and monotonically increasing function $g(x)$ on $(0,∞)$ so that for every $x\in (0,∞),g(x)\in (0,∞)$ and $g(g(x))=f(x)$? If $f(x)=c~x^k~(c>0,k>0),$and $g(x)=r~x^{\sqrt{k}},$where $c=r^{\sqrt{k}+1},$then $g(g(x))=f(x)~(x>0).$ But how to solve it in general conditions?Thanks in advance!","['functions', 'real-analysis']"
422006,Generating function with Stirling's numbers of the second kind,"It's very easy to prove that: $$\sum_k \left\{k\atop n\right\}z^k=\frac{z^n}{(1-z)(1-2z)...(1-nz)}$$ But this generating function looks very pretty, so my question is: does this identity have some simple combinatorial interpretation?","['stirling-numbers', 'generating-functions', 'combinatorics']"
422008,mathematical analysis books for self study [duplicate],"This question already has answers here : Good First Course in real analysis book for self study (7 answers) Closed 11 years ago . I am looking for mathematical analysis books whose explanation is polite. If they has many familiar example, I will be happier. I am familiar with set theory, group theory, elementary theory, but am not familiar with analysis at all.","['reference-request', 'analysis']"
422012,"Neyman-Pearson, solving for $k$","I've got a one major problem using the Neyman-Pearson lemma. We're testing hypotheses $H_0: \theta \le \theta_0$ vs. $H_1: \theta > \theta_0$. Our $$f(x,\theta) = \frac{1}{\theta}e^{-\frac{x}{\theta}}$$ We use the Neyman-Pearson lemma and get the following expression for the critical value of the test: $$\phi(X) = \mathbf{1}_{\{f(x,\theta_1) > k f(x,\theta_0)\}}(x) + \gamma \mathbf{1}_{\{f(x,\theta_1) = k f(x,\theta_0)\}}(x)$$ Now I can get some expression in terms of $\frac{1}{n} \sum_{i=1}^n X_i > g(k)$ for the test, but I need a value for $k$ from the expression for $\phi(X)$. How would I compute $$\mathbb{E}[\phi(X) |H_0] =\alpha$$ so that I can solve for $k$?","['statistics', 'probability']"
422018,The difference between analysis and real analysis,"I do not know the difference between analysis and real analysis. When I study analysis, is it alright if I use the books on real analysis?","['real-analysis', 'analysis']"
422040,minimum of this function,"Define $f(x,y)  =  (a - bx - by)e^{-(x^2+c y^2)}$, where $c > 1$ and $b>a$ then define $g(x,y)  =  f(x,y)$ if $x<y$ and $g(x,y)=f(y,x)$ if $x>=y$. Look at the function $g(x,y)$. Obviously $g(x,y)$ is symmetric about $x=y$ and not differentiable at $x=y$, but I don't know why the minimum of $g(x,y)$ also occurs at $x=y$?","['optimization', 'multivariable-calculus']"
422068,"Proof of ""triangles are similar iff corresponding angles are equal""","I'm looking for a basic proof of the basic plane Euclidean geometry theorem:  Two triangles are similar if and only if their corresponding (interior) angles are equal. (The theorem can be also worded in terms of only two of the interior angles.) I'm having great difficulty finding an ""elementary"" proof of this theorem.  (I have searched both online and in my local university library.) By ""elementary"" I mean a proof that uses only ""the standard axioms"" of plane Euclidean geometry (as taught in basic high-school geometry).  (In particular, I'm specifically ruling out proofs that are based on analytic geometry.) I have been able to find some proofs online, but they all depend on the theorem that ""two transversals are divided into proportional segments by a system of parallel lines.""  Unfortunately, every proof that I can find of this particular theorem happens to be based on the subject line's theorem. Can someone point me to a reference where this theorem is proved in full? EDIT: When I write ""two triangles $ABC$ and $DEF$ are similar"" I mean that the ratios of the lengths of their (corresponding) sides are equal, i.e., $$\overline{AB}:\overline{BC}:\overline{CA} = \overline{DE}:\overline{EF}:\overline{FD}$$",['geometry']
422071,How to show that the Hessian matrix of $G$ is positive definite?,"Let $\{g_i:X\subset\Bbb R\to\Bbb R;\;i=1,\ldots,m\}$ be a linearly independent set of real functions. Given $n$ points $(x_1,y_1),\ldots,(x_n,y_n)\in X,$ consider the following function $$G(\beta_1,\ldots,\beta_n)=\sum_{k=1}^n\left(\sum_{l=1}^m\left[ \beta_lg_l(x_k)-y_k\right]\right)^2$$ I need to prove that $G$ attains a local minimum. For this, I need to show that the Hessian matrix of $G$ is positive definite, but I'm not able to do. I'm trying to show it, but I'm able to prove that it's just positive semidefinite (for this, I'm doing things like this and this ). Can someone help me show that the Hessian matrix of $G$ is positive definite? Notice that $$\frac{\partial G}{\partial \beta_i}=\sum_{k=1}^n\left(2g_i(x_k)\sum_{l=1}^m\left[ \beta_lg_l(x_k)-y_k\right]\right)$$ and $$\frac{\partial^2 G}{\partial \beta_j\beta_i}=2\sum_{k=1}^ng_i(x_k)g_j(x_k)$$ Hence, the Hessian matrix of $G$ is $$H=2
\begin{bmatrix}
\sum_{k=1}^ng_1(x_k)^2 & \sum_{k=1}^ng_1(x_k)g_2(x_k) & \cdots & \sum_{k=1}^ng_1(x_k)g_m(x_k)\\ 
\sum_{k=1}^ng_2(x_k)g_1(x_k) & \sum_{k=1}^ng_2(x_k)^2 & \cdots & \sum_{k=1}^ng_2(x_k)g_m(x_k)\\ 
 &  & \vdots & \\ 
\sum_{k=1}^ng_m(x_k)g_1(x_k) & \sum_{k=1}^ng_m(x_k)g_2(x_k) & \cdots & \sum_{k=1}^ng_m(x_k)^2
\end{bmatrix}$$ Thanks.","['optimization', 'multivariable-calculus', 'numerical-linear-algebra', 'numerical-methods']"
422087,Solve $\tanα+2\tan2α+4\tan4α+8\tan8α+16\tanα=\cotα$ for $\alpha$,My knowledge of trigonometry are still insufficient to resolve this problem. Any help would be greatly appreciated. Solving for $\alpha$: $$\tanα+2\tan2α+4\tan4α+8\tan8α+16\tanα=\cotα$$,"['trigonometry', 'algebra-precalculus']"
422100,What's the difference between $\frac{dy}{dx}$ and $dy$?,"Ok, so I was doing a substitution problem and I realized that $dy = u\ dx + x\ du$ and not $\frac{dy}{dx} = u\ dx + x\ du$ and I was wondering what the difference was between those two. My first guess would be that $\frac{dy}{dx}$ means differential of $y$ with respect to $x$, and $dy$ would be differential of $y$, but I don't know what that implies exactly. Can you provide examples so I can wrap my head around that concept? If $\frac{dy}{dx}$ = integral of $3x$, what would $y$ be?",['ordinary-differential-equations']
422120,Known bounds for the number of groups of a given order.,"The number of nonisomorphic groups of order $n$ is usually called $\nu(n)$.  I found a very good survey about the values. $\nu(n)$ is completely known absolutely up to $n=2047$, and for many other values of $n$ too (for squarefree n, there is a formula). In general, however, $\nu$ is very hard to calculate. So, I would like to at least have easy to calculate lower and upper bounds for $\nu$. Are such bounds known?","['groups-enumeration', 'asymptotics', 'finite-groups', 'p-groups', 'group-theory']"
422135,How to calculate $ \int_{0}^{\infty} \frac{ x^2 \log(x) }{1 + x^4} $?,I would like to calculate $$\int_{0}^{\infty} \frac{ x^2 \log(x) }{1 + x^4}$$ by means of the Residue Theorem. This is what I tried so far: We can define a path $\alpha$ that consists of half a half-circle part ($\alpha_r$) and a path connecting the first and last point of that half circle (with radius $r$) so that we have $$ \int_{-r}^{r} f(x) dx + \int_{\alpha_r} f(z) dz = \int_{\alpha} f(z) dz = 2 \pi i \sum_{v = 1}^{k} \text{Res}(f;a_v) $$ where $a_v$ are zeros of the function $\frac{x^2 \log(x) }{1+x^4}$. If we know $$\lim_{r \to \infty} \int_{\alpha_r} f(z) dz = 0 \tag{*} $$ then we know that $$\lim_{r \to \infty} \int_{-r}^{r} f(x) dx = \int_{-\infty}^{\infty} f(x) dx = 2 \pi i \sum_{v=1}^{k} \text{Res}(f;a_v) $$ and it becomes 'easy'. Q : How do we know (*) is true?,"['residue-calculus', 'improper-integrals', 'integration', 'complex-analysis']"
422139,Conditions on a measure,"Suppose we have a measure $\mu$ on $\mathbb R_+$ such that $\forall s>0$ $t^s\in L^1(\mathrm d\mu(t))$, but functions $\mathbf{1}_{t>0} $ and $\mathbf{1}_{t\in (0,1)}$ are not necessarily in $L^1(\mathrm d\mu(t))$. I'd like to impose some conditions on $\mu$ so the function $$f:p\to \frac{\int_0^\infty t^p \mathrm d\mu(t)}{\int_0^\infty t^{p-1}\mathrm d\mu(t) },\quad p>1 $$
is monotone on $(1,\infty)$. For example, if $\mathrm d\mu(t)=e^{-t}\mathrm d t $, then $f(p)=p$. Another example is $\mathrm d\mu(t)= \mathbf{1}_{t\in (0,1)}\mathrm d t $, $f(p)=p/(p+1)$ (monotone, too). Somehow this reminds me the Riesz–Thorin theorem , but I don't see how I can apply it here. Are there any results on this (or similar) subject? What are possible ways to approach this problem?","['improper-integrals', 'measure-theory', 'lebesgue-integral']"
422142,when is the ideal $I(\overline{V})$ of the projectivization of an affine variety generated by the homogenization of the generator of $I(V)$,"Let $k$ be a field and $V \subset \mathbb{A}^n(k)$  an affine variety defined by polynomials $f_1,\cdots, f_k$. Under which conditions the ideal $I(\overline{V})$ of the projective completion $\overline{V}\subset \mathbb{P}^n(k)$  of $V$ is genereted by the homogenizations of $f_1,\cdots, f_k$ ? What are the examples where it is not true?",['algebraic-geometry']
422162,counting another problem,"I am trying to do my homework and it seems really hard. i would like to get checked here and make sure that im on the right track. can anyone help me?? Question: A group of hundred students want to create a committee of twelve which will then select a chairman for the committee. a) In how many ways can this be accomplished? b) what if they decide to have two members serve as co-chairs?? Answer a) $P(100,12)= \dfrac{100!}{12!(100-12)!}$; Answer b) $P(100,12)+ P(12,1)+ P(12,2)$. since the order matters and repetition is not allowed, i selected 12 from 100 students. did i do it right??","['discrete-mathematics', 'number-theory']"
422205,What is the intuition of conjugacy classes?,"How can I fully understand what are conjugacy classes are in groups? I know the definition, that $a$ and $b$ are conjugate if $gag^{-1}=b$ for some $g\in G$. But what is the intuition? Using a multiplication table (all the possible multiplications), how can I understand what a conjugacy class is? For example, on Wikipedia you can see this table: How can I relate the idea of a conjugacy class with this table?","['intuition', 'group-theory', 'abstract-algebra']"
422218,Does setting derivative to zero suffice always for minimization of convex functions?,"I have this convex function in $X$, given by $Trace(AX^TBX)$ where $A$, $B$ are p.s.d and all entries are real. Now if I had a linear function $l(X)$ that prevents a trivial zero-matrix solution for $X$ in the minimization of  $Trace(AX^TBX)$ w.r.t $X$. Now would setting the first-derivative(gradient) of $Trace(AX^TBX) + \lambda l(X)$ w.r.t $X$ as zero and solving for $X$ 
suffice to get the optimal solution for $X$ given the convexity, or are there more conditions/directional derivatives and so forth to consider? For your convenience, the gradient is $(B \otimes A^T + B^T \otimes A)vecX + \lambda \nabla_Xl(X)$ and the gradient, $\nabla_Xl(X)$ does not contain $X$ in it, but has other terms. $\lambda$ is the Lagrange multiplier for enforcing the linear constraint.","['optimization', 'multivariable-calculus', 'convex-analysis', 'calculus-of-variations', 'derivatives']"
422225,I don't understand this proof of the AM-GM inequality?,"The proof uses this lemma which I understand: $\mathbf {Lemma}$: Suppose $x$ and $y$ are positive real numbers such that $x>y$. If we decrease $x$ and increase $y$ by some positive quantity $E$ such that $x-E \ge y+E$, then $(x-E)(y+E) \gt xy$ . $\;$Hence, by subtracting $E$ from $x$ and adding it to $y$, we leave the average of the two numbers unchanged while increasing their product. $\mathbf {Proof}:$ Suppose $a_{1}, a_{2}, a_{3}... a_{n}$ are positive real numbers with average $A$ and product $P$. If all $a_{i}$ are equal, then both the geometric mean and the arithmetic mean are equal to $A$. Let $a_{j}$ be one number closest to $A$ without being equal to $A$. Without loss of generality, let $a_{j} \lt A$ . Since the average of the numbers is $A$, there is at least one member of the set greater than $A$. Let $a_{k}$ be the greatest of these numbers. Clearly we must have $a_{k}-A \gt A-a_{j}$ since $a_j$ is closer to $A$ than any other $a_i$ not equal to $A$. We now use our lemma. Replace $a_j$ with $A$ and $a_k$ with $a_k-(A-a_j)$ . Note that  $a_k-(A-a_j) \ge a_j +(A-a_j)$ , so we can apply our lemma with $(A-a_j)$ as our $E$ . By our lemma, the average of the numbers in the new set is the same, but the product is now higher. If we continue this process, we make one of the members of the set equal to $A$ with each application of the process. Hence, in some finite number of steps, we will make all the numbers equal to $A$. Thus, we prove that of all the sets of positive numbers with average $A$, the set with maximum product has all the elements equal to $A$. There are three things I don't understand about this proof: $1)$ I don't understand why they don't loose generality when they say to let $a_j$ be the number closest to $A$ and let $a_j \lt A$. It certainly is possible for this not to be the case, for example the set ${2, 10, 10, 10}$. The average is $8$, but the number closest to $A$ is greater than $A$, so I don't see how the proof can apply to this set. $2)$ I don't see how this process makes makes the elements of the set equal to $A$. If you want $a_j+E$ and $a_k-E$ to be equal to $A$, then $a_j$ and $a_k$ have to be equidistant from $A$. $3)$ if you do bring one pair of terms at a time equal to $A$, then that means you must have an equal number of therms below and above $A$. Any help is appreciated, thanks!","['inequality', 'convex-analysis', 'algebra-precalculus']"
422233,How to find a minimal polynomial (field theory),"I was asked to find a minimal polynomial of $$\alpha = \frac{3\sqrt{5} - 2\sqrt{7} + \sqrt{35}}{1 - \sqrt{5} + \sqrt{7}}$$ over Q . I'm not able to find it without the help of WolframAlpha, which says that the minimal polynomial of $\alpha$ is $$19x^4 - 156x^3 - 280x^2 + 2312x + 3596.$$ (Truely it is - $\alpha$ is a root of the above polynomial and the above polynomial is also irreducible over Q .) Can anyone help me with this? Thank you!",['abstract-algebra']
422238,Application of Stone-Weierstrass with a non-unital algebra,"Let $X$ be a locally compact Hausdorff space. We say that a function $f\colon X \to \mathbb{R}$ vanishes at $\infty$ if for each $\epsilon >0$ there exists a compact $K_\epsilon \subset X$ such that   $|f(x)|< \epsilon, \forall x\in X-K_\epsilon$. We denote by $C_0(X)$ the set of all such functions and we endow it with the usual operations of addition and multiplication of functions, and multiplication of functions by scalars. (i) Show that one has a well-defined norm $\left|\left|\cdot\right|\right|:C_0(X) \to \mathbb{R}, f\to 
||f||:=\sup\left\{ {|f(x)|:x \in X}\right\}$ and together with this norm, $C_0(X)$ becomes a Banach space. (ii) Assume that $\mathbb{A}\subset C_0(X)$ has the following properties: $\mathbb{A}$ is a non-unital subalgebra of $C_0(X)$. $\mathbb{A}$ is strongly point-separating: it is point separating and, $\forall x \in X$, there exists $f\in \mathbb{A}$ such that $f(x)\neq0$. Show that $\mathbb{A}$ is dense in $C_0(X)$. (i) was easy, but I'm completely stumped on (ii). Since Stone Weierstrass requires an unital subalgebra in a nontrivial way I don't know how to proceed. Due the way the question is phrased and the conditions given, it is very suggestive of using one-point compactification and then either producing a unit (which doesn't work as far as I can see) or proceeding in an entirely different way. I'd very much appreciate a hint. Please note that this is not homework. EDIT: I've found the solution. Proof. $X$ is a locally compact Hausdorff space and so can be one-point compactified. We do so and obtain $X^*$. We imbed $A$ into $C(X^*)$, using the fact that any element $a$ of $A$ vanishes at the extra point $\infty$. Send that  $a$ to the $b\in C(X^*)$ that is is equal to $a$ for all point in $X$ and zero at $\infty$. This defines $A^*$, a subalgebra of $C(X^*)$. Similarly we can imbed $C_0(X)$ into $C_0(X^*)$. This obviously respects the norm. Moreover we define $B^*:=$Span$(A^*)\cup \left\{{1}\right\}$. $B^*$ is a point separating unital subalgebra of $C(X^*)$, because $A$ is strongly point separating. We now apply Stone-Weierstrass and the fact that $X^*$ is compact Hausdorff to find that $B^*$ is dense in $C(X^*)$. Observe that the closure of $A^*$ lies in $C(X^*)\cap $Cl$(B^*)$, because $C(X^*)\cap $Cl$(B^*)$ is closed and and contains $A^*$. Let $\gamma \in C_0(X^*)=C(X^*)\cap $Cl$(B^*)$. $\gamma$ lies in $C_0(X^*)$, so $\gamma$ is zero at $\infty$, and it is a limit point of $B$. Note however that every sequence to $\gamma$ lies in $A^*$ after a certain index. So the $\gamma$ is in the closure of $A^*$. As noted earlier, bijection between $A$ and $A^*$ respects the norm. It follows that $A$ lies dense in $C_0(X)$.","['general-topology', 'banach-algebras', 'functional-analysis', 'real-analysis']"
422247,Is projection of a closed set $F\subseteq X\times Y$ always closed?,"If we have closed subset $F$ of product $X \times Y$ (product topology) does it mean that $p_1(F)$ (projection on first coordinate) is closed in $X$ and $p_2(F)$ in $Y$ are closed? If not, why not (some counterexample or explanation please =)","['general-topology', 'closed-map', 'examples-counterexamples', 'product-space']"
422249,Graphing $\sin(|x|)$?,"I'm confused on how the graph is in both quadrant II and III. If $|x|$ is evaluated first wouldn't all the answers be positive, so that when the range of $|x|$ is plugged into $\sin$ wouldn't the range of $\sin$ be all positive too?","['trigonometry', 'graphing-functions']"
422250,Showing how the roots of this complex polynomial are different.,"I want to show that the complex polynomial $p(z) = z^5 + 6z - 1$ has four different roots in the annulus $\{z \in \mathbb{C} : \frac{3}{2} < |z| < 2 \}$. I used Rouché's theorem to proof that $p(z)$ has exactly four roots in the annulus, but I don't see why they should be different. Any ideas?
Thanks.","['roots', 'complex-analysis']"
422264,Constructing a functions with Gelfand Naimark,"If $X$ and $Y$ are compact Hausdorff spaces, show that for any algebra homomorphism
$$
F:C(Y) \to C(X)
$$
there exists a continuous function $f:X\to Y$ such that 
$$
F(\phi)=\phi \circ f, \forall \phi \in C(Y)
$$ The spaces are compact Hausdorff, so presumably one should use the Gelfand-Naimark theorem and construct a continuous function from the spectra of $C(X)$ and $C(Y)$. 
To be honest I'm pretty confused. I don't know exactly what I should try to do; using characters and spectra instead of points in spaces doesn't seem to get me closer to the answer. Any help would be appreciated. Please note that this is not homework. EDIT: I've found the solution. Proof. Let $A=C(X)$ and $B=C(Y)$ be two commutative algebras over $\mathbb{R}$. If $X$ and $Y$ are compact Hausdorff spaces, show that for any algebra homomorphism
$$
F:B \to A
$$
there exists a continuous function $f:X\to Y$ such that 
$$
F(\phi)=\phi \circ f, \forall \phi \in C(Y)
$$
 We first define all the relevant maps.
Let $X_A$ and $Y_B$ be the spectra of $A$ and $B$ respectively. Let $\sigma: Y_B \to Y$ and $\tau: X_A \to X$ be the homeomorphisms induced by the Gelfand Naimark theorem, since $X$ and $Y$ are compact Hausdorff. Let $\lambda: A \to \mathbb{R}$ and $\chi: B\to \mathbb{R}$ be two characters in $X_A$ and $Y_B$ respectively. Notice that $\lambda \circ F$ is a function from $B \to \mathbb{R}$ that lies in $Y_B$ since $F$ respects the $\mathbb{R}$-linearity and multiplicative identities of the algebra structure on $\mathbb{B}$ and $\lambda$ is a character and therefore respects the same identities by definition. So there exists an injective function $h: X_A \to Y_B$. We prove that $h$ is continuous. Apply Exercise 4: we have that $ f_a \circ h = f_a(h(\lambda))= f_a(\lambda \circ F)=\lambda(F(a))= g_{F(a)}$ with $g_a: X_A \to \mathbb{R}$ and $g_a(\lambda)=\lambda(a)$ and $f_a: Y_B \to \mathbb{R}$ and $f_a(\chi)=\chi(a)$. Define $f$ as follows: $f(x):= (\sigma \circ h \circ \tau)(x)$. The composition of continuous function is continuous so f is continuous. We now prove the identity $\phi \circ f=F(\phi)$.
\begin{align*}
(\phi \circ f)(x)&=
(\phi \circ \tau \circ h \circ \sigma)(x) \\
&=
(\phi \circ \tau\circ h) (\chi_x)\\
&=
 (\phi \circ \tau \circ \chi_x \circ F)\\
&=
(\phi \circ \tau \circ \chi_y)\\
&=\phi(y)\\
&=
\chi_y(\phi)\\
&=(\chi_x \circ F)(\phi)\\
&=F(\phi)(x)
\end{align*}","['general-topology', 'banach-algebras', 'functional-analysis']"
422270,Property of modules via exact sequences,"Suppose $A\neq 0$ is a commutative ring with $1$. Let $L, M, N$ be $A$-modules such that the sequence
$$0\longrightarrow L\overset{\alpha}{\longrightarrow} M\overset{\beta}{\longrightarrow} N\longrightarrow 0$$
is exact. Furthermore suppose $P$ is some property of an $A$-module (e.g. $P$ = Noetherian, Artinian, finitely generated, etc.) Now, I will say $P$ is ""middle-property"" if the following is true:
$$
M \textrm{ satisfies property } P \Longleftrightarrow L \textrm{ and } N \textrm{ satisfies property }P.
$$ Using this terminology, in ""Undergraduate Commutative Algebra"" by Miles Reid, it is proved that (in page 53) the property $P$ = Noetherian is ""middle-property"". I have the following questions 1) Is there standard name for what I called ""middle-property""? 2) What are some other examples of ""middle-property""? I have remarked
  above that being Noetherian is ""middle-property"". Are being Artinian, free,
  finitely-generated, flat, projective, injective, etc. are also
  ""middle-property""? I realize that I have put awful lot of questions out there. Answering any one of them is greatly appreciated :) Basically, I would like to have a list of important  ""middle-properties"". Thanks. Edit. I just realized the following: If $A$ is a Noetherian ring, then property $P$ = ""finitely-generated"" is also ""middle property"". Indeed, being Noetherian and finitely-generated are equivalent for modules over Noetherian rings (See Corollary 3.5 part (ii) in ""Undergraduate Commutative Algebra"" by Miles Reid, page 53). So answers that illustrate ""middle-property"" for particular class of rings are also welcome.","['modules', 'commutative-algebra', 'homological-algebra', 'abstract-algebra']"
422283,"Does there exist rational $a,b,c$, such that $\sqrt[3]{1}+\sqrt[3]{2}+\sqrt[3]{4}=\sqrt[3]{a}+\sqrt[3]{b}+\sqrt[3]{c}$","Let $w = \sqrt[3]{1}+\sqrt[3]{2}+\sqrt[3]{4}$. How to prove that there are no triples $(a,b,c)$, such that $a,b,c \in \mathbb{Q}$; $a \leqslant b \leqslant c$; $(a,b,c)\ne (1,2,4)$; $w = \sqrt[3]{a}+\sqrt[3]{b}+\sqrt[3]{c}$. Or maybe there exists one?","['algebra-precalculus', 'abstract-algebra', 'number-theory']"
422296,"The form of maximal ideal in the real polynomial ring $\mathbb R[x,y]$","Every maximal ideal of the real polynomial ring $\mathbb R[x,y]$ is of the form $(x-a, y-b)$ for some $a,b \in \mathbb R$. True or false? Any suggestions?",['abstract-algebra']
422304,Dimension of the space of algebraic Riemann curvature tensors,"Given $n\in \mathbb N$, consider the vector space $\mathbb R^{n^4}$ whose elements I will denote by $(R_{abcd})$ with indices $a,b,c,d \in \{1, \dots, n\}$. This vector space is $n^4$-dimensional. The space of algebraic Riemann curvature tensors is the subspace $V$ of $\mathbb R^4$ consisting of all $(R_{abcd})$ satisfying the following symmetries: $R_{abcd} = - R_{bacd}$ $R_{abcd} = - R_{abdc}$ $R_{abcd} + R_{cabd} + R_{bcad} = 0$ How would one go about calculating $\dim V$? Some thoughts: The three symmetries above imply another symmetry. Namely $R_{abcd} = R_{cdab}$. Combining this with the first two symmetries, it follows that $R_{abcd}$ can be viewed as a symmetric matrix of antisymmetric matrices. Since the space of antisymmetric matrices is $\binom n2$-dimensional, it follows that the first two symmetries together with the new one will cut out a space of dimension $$\frac{\binom n2 \left(\binom n2 +1\right)}{2}.$$
We still need to take into account the third symmetry. It is not quite clear to me which of the resulting additional equations are independent of each other and the old ones. But I believe there should certainly be some dependencies... I noticed that if I only take into account those equations corresponding to values $a<b<c<d$ (of which there are $\binom n4$) and if I assume that these are all independent of each other, then I obtain that 
$$\dim V = \frac{\binom n2 \left(\binom n2 +1\right)}{2} - \binom n4 = \frac{n^2(n^2-1)}{12}.$$
(modulo miscalculation). This seems to be the right answer. However, I'm neither sure whether these equations indeed are independent nor am I sure whether they form a maximal set of independent equations. I would appreciate your input! Thanks. =)","['linear-algebra', 'representation-theory', 'differential-geometry', 'combinatorics']"
422309,Finding a generating function of a series,"So say if you have a sequence defined as, for $a\in\mathbb{Z}$,
$$ c_n = \binom{a}{0} \binom{a}{n} - \binom{a}{1} \binom{a}{n-1} + \cdots+ (-1)^n \binom{a}{n} \binom{a}{0} = \sum_{i=0}^n (-1)^i \binom{a}{i} \binom{a}{n-i}$$
How would you find the generating function?
It's easy to see that for $n = 2k+1$,  $c_n= 0$, and for $n = 2k$ we can make something like this:
$$ 2 \sum_{i=0}^k \binom{a}{i} \binom{a}{2k-i}  +  (-1)^{k+1} \binom{a}{k+1} $$
If $k > a$ we can use Voldemort's identity and turn the sum into $2 \binom{2a}{2k}$
That makes it more specific though so possibly I'm looking at it too narrowly","['discrete-mathematics', 'combinatorics']"
422310,Does $\frac{\mathrm d}{\mathrm dx} \ln(x)=\frac{\mathrm d}{\mathrm dx} \ln|x|$?,"For some time, I've seen different solutions for the same problems. Let $f$ be any continuous function, differenciable on its domain such that, $$\int \frac{\mathrm d f}{f}=\ln(f)$$ but some authors say, $$\int \frac{\mathrm d f}{f}=\ln|f|$$ I know that, $$\displaystyle \frac{\mathrm d}{\mathrm dx}\ln |x|=\frac{\mathrm d}{\mathrm dx}\ln \sqrt(x^2)=$$
$$\displaystyle \frac{\mathrm d}{\mathrm dx}\ln (x^2)^{\frac{1}{2}}=\frac{\mathrm d}{\mathrm dx}\frac{1}{2}\ln (x^2)=$$
$$\frac{\mathrm d}{\mathrm dx}\ln (x)$$ And so I concluded that:
$$\frac{\mathrm d}{\mathrm dx} \ln|x|=\frac{\mathrm d}{\mathrm dx} \ln(x)$$ Can I generalizate that, $$\int \frac{\mathrm d f}{f}=\ln(f)= \ln|f|$$","['indefinite-integrals', 'integration', 'derivatives']"
422341,Matrix representation of field automorphism,"Let $K$ be the degree $n$ field extension of the field $k$, and let $\alpha_1,\dots,\alpha_n\in K$ be a basis of $K$ over $k$ (as a vector space).
I read somewhere that the following matrix
$$
M=
\begin{pmatrix}
\alpha_1 & \sigma(\alpha_1) & \dots & \sigma^{n-1}(\alpha_1)\\
\vdots & \vdots &\ddots & \vdots\\
\alpha_n & \sigma(\alpha_n) &\dots & \sigma^{n-1}(\alpha_n)\\
\end{pmatrix}
$$
represents the automorphism $\sigma\in \mathrm{Gal}(K/k)$ in the basis $\alpha_1,\dots,\alpha_n$. However I don't know how to show it. Can someone help me? Thanks! Edit : As pointed out by Berci the statement that $M$ represents $\sigma$ is probably incorrect. In the notes I read this statement is used to show that $M$ is nonsingular. So here is the new question: Question : Is there an alternative way of proving the nonsingularity of $M$ (or disproving it by finding a counter-example)? P.S. I notice that in the context of algebraic number theory, the discriminant of an algebraic number field $K$ has a very similar form, being $(\det(M))^2$. So perhaps there is a connetion?","['field-theory', 'number-theory']"
422347,"How to prove that the set $\{\sin(x),\sin(2x),...,\sin(mx)\}$ is linearly independent? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Could you help me to show that the functions $\sin(x),\sin(2x),...,\sin(mx)\in V$ are linearly independent, where $V$ is the space of real functions? Thanks.",['linear-algebra']
