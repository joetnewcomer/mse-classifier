question_id,title,body,tags
3670034,Prove this function is injective [duplicate],"This question already has an answer here : $f(z)=\frac{z}{1+|z|} $ Is 1-1? (1 answer) Closed 4 years ago . Suppose $f:\mathbb{C}\to \mathbb{C}$ be a function defined by $f(z)=\frac{z}{1+|z|}$ .Show that $f$ is one-one. How can I show this?From normal straight forward definition it became very complex.There is also no concept about monotonicity in $\mathbb{C}$ .So,what is the procedure?","['complex-analysis', 'calculus', 'functions']"
3670065,"Every harmonic function is real analytic, Evans p.31-32","In L.Evans book 'Partial Differential Equations' theorem $10$ page $31$ proves that every harmonic function is real analytic. I am stuck in a certain point of the proof. He mentions: To verify this, let us compute for each $N$ the remainder term $$
\begin{align}
R_N(x) &= u(x)-  \sum_{k=0}^{N-1} { \sum_{|a|=k}{\dfrac{D^a u(x_0)}{a!}}(x-x_0)^a} \\&= \sum_{|a|=N} \dfrac{D^a u(x_0+t(x-x_0))}{a!} (x-x_0)^a   
\end{align}\tag{*}$$ for some $t \in [0,1]$ , $t$ depending on $x$ . We establish this formula by writing out the first $N$ terms and the error in the Taylor expansion about $0$ for the function of one variable $$g(t):=u(x_0+t(x-x_0))\quad \text{at $t=1$.}$$ Question: Can someone provide a detailed proof of the second equality above in $(*)$ ? My approach: As the hint suggests I computed using the chain rule that $$g^{(k)}(t)=\sum_{|a|=k} D^a u(x_0+t(x-x_0)) (x-x_0)^a $$ and so writing $$g(t)= \sum_{k=0}^{\infty} \frac{g^{(k)}(0)}{k!}t^k  $$ one gets by plugging in $t=1$ that $$u(x)= \sum_{k=0}^{\infty} { \sum_{|a|=k}{\dfrac{D^a u(x_0)}{k!}}(x-x_0)^a} $$ Then, I split the sum from $k=0$ to $N-1$ and from $N$ to $\infty$ but I don't know how to continue.","['taylor-expansion', 'analysis', 'partial-differential-equations']"
3670103,Is there any name for this property of quasigroups: $x \times (y \times (z \times t)) = (x \times y) \times (t \times z)?$,"Is there a name for the property $x \times (y \times (z \times t)) = (x \times y) \times (t \times z)$ ? Some basic facts about it I was able to figure out: It is shared by all four basic arithmetic operations (ie. addition, multiplication, substraction and division). In quasigroups, it implies an existence of right-identity element $e$ . This element satisfies the property $e \times (x \times y) = y \times x$ . If $e \times x = x$ holds for any $x$ then $\times$ is an abelian group operation. If $x \times x = e$ holds for any $x$ then $\times$ is an inverse group operation. The operation $x \cdot y := x \times (e \times y)$ forms a group.","['quasigroups', 'definition', 'binary-operations', 'group-theory', 'terminology']"
3670129,"$(\frac{1}{3},1)\cup(\frac{1}{5},\frac{1}{3})\cup(\frac{1}{7},\frac{1}{5})\cup\ldots$ is not a finite union of open intervals","Let $X=(\frac{1}{3},1)\cup(\frac{1}{5},\frac{1}{3})\cup(\frac{1}{7},\frac{1}{5})\cup\ldots \subseteq \mathbb{R}$ I have tried to show that $X$ is not a finite union of open intervals. Does the following argument make sense? Suppose, for contradiction, that $$X=(a_1,b_1)\cup\ldots\cup(a_n,b_n)$$ where we assume that $a_1\leq a_2 \leq \ldots \leq a_n.$ If $a_1<0,$ then there is a negative number in $X$ - which is a contradiction. If $a_1=0,$ then $X$ contains $\frac{1}{2m+1}$ for sufficiently large $m \in \mathbb{N}$ - which is a contradiction. If $a_1>0,$ then $X$ does not contain $\frac{1}{2m}$ for sufficiently large $m \in \mathbb{N}$ - which is a contradiction.","['general-topology', 'real-analysis']"
3670244,How to write a diffeomorphism between an open set of the real projective plane $P^2(\Bbb{R})$ and the Möbius strip,"I am trying to solve the following exercise, from do Carmo's Riemannian Geometry: Show that the real projective plane $P^2(\Bbb{R})$ is not orientable. Hint : Show that if the manifold $M$ is orientable, then every open set of $M$ is an orientable manifold. Observe that $P^2(\Bbb{R})$ contains an open set diffeomorphic to the Möbius strip, which is not orientable. I am trying to follow the hint, and managed to prove the first part. Now, I have the intuition that if we take a neighborhood of the equator in the sphere $S^2$ , this amounts to an open set in $P^2(\Bbb{R})$ . On the other hand, it also amounts to the Möbius band, which I think of, following the book, as the quotient manifold $C/G$ , where $C$ is a right cylinder and $G = \{ Id, A\}$ , where $A$ is the antipodal map. In other words, I am thinking of a map $$
U \subset P^2(\Bbb{R}) \rightarrow V \subset S^2 \rightarrow C \xrightarrow{\pi} C/G \\
[p]  \mapsto p \mapsto q \mapsto[q]
$$ where $V = \{p \in S^2 \ : \ |p_3| < \varepsilon\}$ is a neighborhood of the equator and $q$ can be obtained from $p$ by $$
p = (p_1, p_2, p_3) \mapsto \left(\frac{p_1}{\sqrt{p_1^2 + p_2^2}}, \frac{p_2}{\sqrt{p_1^2 + p_2^2}}, p_3 \right) = q
$$ However, I am not able to write this map explicitly, nor show that it is a differomorphism. I am also havin difficulty determining the domain $U \subset P^2(\Bbb{R})$ . I will apreciate any hints or solutions. Thanks in advance and kind regards.","['manifolds', 'general-topology', 'projective-space', 'differential-geometry']"
3670314,"$(a, b) = (c, d)$ iff $a=c$ and $b=d$","I want to use the definitions of equality of sets and equality of ordered pairs to prove this statement. Here's what I've come up with so far: Assume $(a, b) = (c, d)$ . Then $\{\{a\}, \{a, b\} = \{c\}, \{c, d\}\}$ since $(a, b) = \{\{a\}, \{a, b\}\}$ and $(c, d) = \{\{c\}, \{c, d\}\}$ . Since $\{a\}$ and $\{c\}$ are the only two singleton sets between the two sets respectively, $\implies \{a\} = \{c\}$ . Similarly, since $\{a, b\}$ and $\{c, d\}$ are the only non-songleton sets between the two sets respectively and $a=c$ , $\implies b=d \space\blacksquare$ I imagine this is a bit of stretch of a proof, but any help/feedback is much appreciated!",['elementary-set-theory']
3670324,Poisson distribution - find value for λ given a known probability,"I am trying to solve the following: The number of particles decaying per second in a radioactive material with a high half-life (e.g. of several millennia) is (in very good approximation) Poisson-distributed. a) On average, 20.00 particles per second decay from a radioactive sample. What is the probability that 20 particles decay in one second? b) An average of 20.00 particles per second decay from a radioactive sample. What is the probability that at least 10 particles decay in one second? c) From a radioactive sample, it is known that with a probability of 1%, at most 3 particles per second decay. How many particles in this period decay on average in one second? Give your result to three decimal places. My Solution: a) $$p(x=20)=\frac{20^{20}}{20!}e^{-20} = 0.089$$ b) $$p(x \geq 10)= 1- p(x \leq 9) \\
           = 1- (\sum\limits_{x=0}^{9} \frac{20^{x}}{x!}e^{-20}) \\
           = 1- 0.005   \\
           = 0.995
$$ c) $$p(x \leq 3)=  \sum\limits_{x=0}^{3} \frac{\lambda ^{x}}{x!}e^{-\lambda} =0.01 \\
          0.01 = (\frac{\lambda^0}{0!} +\frac{\lambda^1}{1!}+ \frac{\lambda^2}{2!} +\frac{\lambda^3}{3!}) e^{-\lambda}   \\
           = (1 +\lambda+ \frac{\lambda^2}{2} +\frac{\lambda^3}{6}) e^{-\lambda}   \\
= \frac{(\lambda^3 +3\lambda^2 + 6\lambda+ 6)} {6e^{\lambda}}   \\
$$ Using an online calculator, I found out that $\lambda \approx 10.0451$ , but I'm not entirely sure whether that's the right answer. My question is: If my calculations are correct so far: How to calculate $\lambda$ ?","['poisson-distribution', 'statistics', 'probability-distributions', 'probability']"
3670354,Let $f:X\rightarrow\textbf{R}$ be continuous and $X$ compact. Then $f$ is bounded and $f$ attains its maximum and its minimum at some points in $X$,"Let $(X,d)$ be a compact metric space, and let $f:X\rightarrow\textbf{R}$ be a continuous function. Then $f$ is bounded. Furthermore, $f$ attains its maximum at some point $x_{max}\in X$ and also attains its minimum at some point $x_{min}\in X$ . MY ATTEMPT Since $f$ is continuous, it maps compact sets onto compact sets. Once compact sets are closed and bounded, we conclude that $f(X)$ is closed and $f(X)\subseteq [-L,L]\subset\textbf{R}$ . Given that $f(X)$ is bounded, it admits a supremum $M = \sup f(X)$ and an infimum $m = \inf f(X)$ . But both $m$ and $M$ are adherent points of $f(X)$ . Thus $f(X)\ni m$ and $f(X)\ni M$ . In other words, $m = f(x_{min})$ for some $x_{min}\in X$ and $M = f(x_{max})$ for some $x_{max}\in X$ , as previously mentioned. Any comments or contributions to my solution?","['general-topology', 'solution-verification', 'metric-spaces', 'real-analysis']"
3670384,Why is the sequential criterion true in metric spaces? When does it fail in general?,"I’ve read just the basics of some introductory analysis books and sometimes they show that we can characterize things like limits, continuity, compactness, etc. in terms of sequences. I’ve heard that these sequential criteria hold for general metric spaces, but that in topology for example one encounters situations where sequences aren’t quite sufficient, or where it’s better to consider some other object. My questions are: Is there some intuition for why the sequential criterion holds in things like Euclidean space or general metric spaces, but not in some other spaces? Does it simply have to do with the fact that we have a metric, and if so, why does the metric “induce” such sequential criteria (versus without a metric we may not)? Is the notion of distance/metric captured in some way by sequences because approaching some value is equivalent to a sequence approaching that value? Are there any ways by which we can determine whether a general space possesses these sequential criteria? They seem quite useful.","['metric-spaces', 'analysis', 'real-analysis', 'sequences-and-series', 'general-topology']"
3670409,Does $ak \equiv bk \textrm{ mod }m \implies a \equiv b \textrm{ mod } m$? [duplicate],"This question already has answers here : $\ ac≡bc\pmod{\! m}\!\iff\! a≡b\pmod {\!m/d},\ d = \gcd(c,m)\ $ [Congruence Cancellation & Division Rule] (6 answers) Closed 4 years ago . I'm wondering the following: I know that $a \equiv b \textrm{ mod } m \implies ac \equiv bc \textrm{ mod } m$ for $a,b,c,m \in \mathbb{Z}$ and $m \neq 0$ . I'm wondering that if we know that $\gcd(c,m) = 1$ and that $ac \equiv bc \textrm{ mod } m$ , is $a \equiv b \textrm{ mod } m$ ? If it is true, how would we prove it?","['modular-arithmetic', 'discrete-mathematics']"
3670435,"Generalisation of $ \sum \frac {1}{k}-\ln n=\gamma$ to $0 \lt\alpha \lt1 , \sum \frac{1}{k^\alpha}-f(n)= \beta$","looking at Find the value of : $\lim\limits_{n\rightarrow\infty}\left({2\sqrt n}-\sum\limits_{k=1}^n\frac{1}{\sqrt k}\right)$ and knowing that for $$\alpha=1 ,\lim_{ n \to \infty}  \sum_{k=1}^n \frac {1}{k^\alpha}-\ln n=\gamma$$ Makes one wonder if there are other results for $0 \lt \alpha \lt 1 $ $$\lim_{ n \to \infty}  \sum_{k=1}^n \frac {1}{k^\alpha}-f(n)=\beta$$ where $f,\beta$ are determined by the value of $\alpha$ is there a topic that relates to some results similar to above?","['limits', 'divergent-series', 'asymptotics', 'sequences-and-series']"
3670440,Does taking the dot product of two column vectors involve converting one of the vectors into row vectors first?,"If you have two vectors living in subspace $V$ and you want to take dot product, it seems that you cannot technically do this operation because if you write both vectors in matrix form, they would both be column vectors living in the same subspace. In order to take the dot product, you would need to convert one of the vectors into a row vector which lives in a completely different dual subspace $V^*$ and then take the dot product of this dual space vector with the column vector. Is all of this true?","['inner-products', 'linear-algebra', 'vector-spaces']"
3670441,Can convolution be expressed as a differential equation?,"The integral equation for (causal) convolution is given by $$y(t) = \int_{-\infty}^{t} K(t - \tau) x(\tau) d\tau$$ Can one write an equivalent differential equation for general well-behaved kernel $K$ ? Since convolution with an exponential kernel is a solution to a linear ODE with with time-dependent input (e.g. see 2nd example here ), the transformation is possible for some kernels. If it turns out that there are well-behaved kernels for which the transformation is not possible, it would be great to address the question on determining the set of kernels for which the transformation is possible. EDIT : I have found a related post , where respondents argue that this operation is indeed impossible for general kernels. I will comment further when I have fully understood if the outlined arguments apply to my case, where causality is enforced by the limits of integration.","['calculus', 'convolution', 'ordinary-differential-equations']"
3670484,Poissonization use for sample size,"(Cross-post from Stats Stack Exchange) I'm interested in using the Poissonization trick to solve the following problem, which I made up: Suppose I have a categorical random variable $X$ taking values $1$ , $2$ , and $3$ , with probability distribution $(\pi_1, \pi_2, \pi_3)$ .
How many samples from $X$ should I take before I have at least 5
samples from one of the categories, with probability $.9$ ? Or at least 5 samples from every category? If $Y_1, Y_2, Y_3$ are independent Poisson random variables with rates $\pi_1, \pi_2, \pi_3$ , then the Poissonization trick tells us that $(Y_1, Y_2, Y_3 \mid \sum Y_i = k)$ has distribution $\sim \operatorname{Mult}(k; \pi_1, \pi_2, \pi_3)$ on $\mathbb{N}^3$ . If $A\subset \mathbb{N}^3$ is the set $\{x_1, x_2, x_3 \leq 4\}$ (I'm interested in the complement of $A$ ), then $$\mathbb{P}\{(Y_1, Y_2, Y_3)\in A\}$$ $$ = \sum \mathbb{P}\{(Y_1, Y_2, Y_3)\in A \mid \sum Y_i =k \} \cdot \mathbb{P}(\sum Y_i = k)$$ And (this is where I get a bit shaky) I should expand in a sort of fake variable $\lambda$ , representing the rate of $Y_1 + Y_2 + Y_3$ (though I know that rate is actually one), and then $\mathbb{P}\{(Y_1, Y_2, Y_3)\in A\}$ is $$e^{\pi_1 \lambda}(1 + \frac{(\pi_1\lambda)^1}{1!} + \dotsb + \frac{(\pi_1\lambda)^4}{4!})\, \cdot \, $$ $$e^{\pi_2 \lambda}(1 + \frac{(\pi_2\lambda)^1}{1!} + \dotsb + \frac{(\pi_2\lambda)^4}{4!})\, \cdot \,  $$ $$e^{\pi_3 \lambda}(1 + \frac{(\pi_3\lambda)^1}{1!} + \dotsb + \frac{(\pi_3\lambda)^4}{4!})\, \cdot \,  $$ And now I should multiply this by $e^{-\lambda}$ , expand in $\lambda$ somehow, and look for the coefficient on $\frac{\lambda^k}{k!}$ ? Is that right?","['poisson-distribution', 'statistics', 'approximation', 'probability']"
3670562,"Prove $\lim_{(x,y)\to (0,0)}\frac{xy\sin(y)}{x^2+y^2}$ using delta-epsilon argument","I am trying to prove this limit using delta epsilon: $$\lim_{(x,y)\to (0,0)}\frac{xy\sin(y)}{x^2+y^2}$$ I know how the individual components relate to delta etc but I can't put it together. Please help. (Alternatively, is it valid to use polar coordinates?)","['limits', 'multivariable-calculus', 'epsilon-delta']"
3670595,why do we call the constant of integration a constant when really its an unknown variable??,"when I started learning integration and my teacher called ""C"" a constant I questioned her and asked why we call it a constant when the value is never constant, hoping someone can shed some light on this?","['integration', 'philosophy', 'derivatives']"
3670600,Measure of complexity of a function,"Generally, when we try to write a function to fit to a group of points (curve fitting), it is important to find a balance between the accuracy and complexity of the function. The following picture shows different examples of this. If the function is excessively simple at the cost of accuracy, it will underfit (such as with the line). On the other hand, if the function is excessively complex, it will overfit (such as with the curve on the right). I am looking for a formal and widely accepted definition for the complexity of the function. One definition I was thinking about was how much the slope varies. Letting the average slope of the function in an interval be $\bar{m}$ , then I can write the complexity as $$\int_a^b (f'(x)-\bar{m})^2 dx$$ where $f(x)$ is the function, $a$ is the lower endpoint, and $b$ is the upper endpoint. Expanding the square, I get $$\int_a^b (f'(x)^2 - 2\bar{m}f'(x) + \bar{m}^2)dx = \int_a^bf'(x)^2dx - 2\bar{m}\int_a^bf'(x)dx + \bar{m}^2\int_a^bdx$$ Using the Fundamental Theorem of Calculus on the second integral, I get $$\int_a^bf'(x)^2dx - 2\bar{m}(f(b)-f(a)) + \bar{m}^2(b-a)$$ Because $\bar{m}$ is $$\frac{f(b)-f(a)}{b-a}$$ I can simplify the above equation to get $$\int_a^bf'(x)^2dx - \frac{\left(f(b)-f(a)\right)^2}{b-a}$$ The higher this value is, the more ""complex"" and varying the function is. If this value is $0$ , then $f(x)$ is a line from $a$ to $b$ . Are there other measures of the complexity of a function? If so, what are the corresponding equations for them? Edit: I am not looking for a complexity function dependent on the points. For example, $f(x) = x$ should have the same complexity no matter what points are on the graph. Edit 2: A problem with the above definition of complexity is that $c^x$ would have a very high complexity (when I believe it should have a lower value). Specifically, from $a$ to $b$ , the complexity is $$ \ln(c) \frac{c^{2b}-c^{2a}}{2} - \frac{(c^b-c^a)^2}{b-a}$$ which increases exponentially with respect to $b$ .","['integration', 'approximation-theory']"
3670653,Finding $\lim\limits_{n→∞}n\cos x\cos(\cos x)\cdots\underbrace{\cos(\cos(\cdots(\cos x)))}_{n\text{ times of }\cos}$,"Find $$\lim_{n→∞}n\cos x\cos(\cos x)\cdots\underbrace{\cos(\cos(\cdots(\cos x)))}_{n \text{ times of } \cos}.$$ I approximated cos(cosx) to cos x, but i don't think it is the proper approach. I got answer as 0 on the approximation. It is clear that it is a 0/0 form, but how can the l's Hopital rule be applied? I tried using the sandwich theorem but I am unable to reach the answer. I plotted the graph on desmos. But I got the resultant graph covering the entire area. please help me reach the proper answer. Thanks in advanced to all.","['limits', 'limits-without-lhopital', 'real-analysis']"
3670714,Does every group have an object of symmetry?,"I'm aware of Cayley's theorem, which says that every group is isomorphic to some subgroup of a symmetric group. But it's not clear to me whether symmetric groups themselves (apart from their name) capture the notion of geometric symmetry that ""objects of symmetry"" have (and by geometric symmetry I mean the type of symmetry expressed when we talk about the rotations and flips of a square ( $D_4$ ), or the symmetries of a cube ( $S_4$ )) Some stackexchange posts answer the question, but I can't tell if the first one is talking about symmetry (as in symmetric group) or symmetry (as in symmetry of a square), and the second answer is a bit too technical for me.... Furthermore, group explorer doesn't have objects of Symmetry for $Q_4$ and $Z_2 \times Z_4$ . Is that for lack of imagination, an incomplete database, or because there is no object of symmetry for these groups (and the many others there)? Thanks","['group-theory', 'abstract-algebra', 'geometry', 'symmetry']"
3670732,Leibniz rule for covariant derivative,"I've been learning about the covariant derivative and I have some doubts. This answer suggests that $\nabla_{\mathbf{u}} T = \nabla T (\mathbf{u})$ , where $T$ is a tensor. The tensor $\nabla T$ appears to be acting on the vector $\mathbf{u}$ in the same way a covector acts on a vector to give a scalar. The answer then proceeds to derive the identity $\nabla^2_{\mathbf{u}, \mathbf{v}} = \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} \mathbf{w} - \nabla_{\nabla_{\mathbf{u}} \mathbf{v}} \mathbf{w}$ , where $\mathbf{u}$ , $\mathbf{v}$ and $\mathbf{w}$ are vectors. According to my interpretation, $$\nabla_{\mathbf{u}} \nabla_{\mathbf{v}} \mathbf{w} = \nabla_{\mathbf{u}} (\nabla \mathbf{w} (\mathbf{v})) \\ =  \underbrace{(\nabla_{\mathbf{u}} (\nabla \mathbf{w}))}_{\text{a (1,1) tensor}} (\mathbf{v}) + \nabla \mathbf{w} (\nabla_{\mathbf{u}} \mathbf{v}) \\ = \underbrace{\nabla \nabla \mathbf{w}}_{\text{a (1,2) tensor}}(\mathbf{u}, \mathbf{v}) + \nabla_{\nabla_{\mathbf{u}} \mathbf{v}} \mathbf{w} \\ = \nabla^2_{\mathbf{u}, \mathbf{v}} + \nabla_{\nabla_{\mathbf{u}} \mathbf{v}} \mathbf{w} \\ \therefore \nabla^2_{\mathbf{u}, \mathbf{v}} = \nabla_{\mathbf{u}} \nabla_{\mathbf{v}} \mathbf{w} - \nabla_{\nabla_{\mathbf{u}} \mathbf{v}} \mathbf{w}.$$ My confusion arises here. Let $T$ and $S$ be tensors. The above derivation make use of some version of the Leibniz rule that appears to be of the form $\nabla_{\mathbf{u}}(T(S)) = (\nabla_{\mathbf{u}} T)(S) + T(\nabla_{\mathbf{u}} S)$ . Is my interpretation correct? Yet according to this answer , the rule $\nabla (T\otimes S) = \nabla T \otimes S + T\otimes \nabla S$ doesn't exist, but when you add a direction $\mathbf{u}$ $\nabla_{\mathbf{u}} (S\otimes T) = \nabla_\mathbf{u} S \otimes T + S \otimes \nabla_\mathbf{u} T$ , it suddenly becomes true. Why? I'm quite confused by these various versions of the Leibniz rule and the ""total covariant derivative"" $\nabla$ versus the covariant derivative $\nabla_{\mathbf{u}}$ . I appreciate if someone could clear it up for me a little.","['tensors', 'differential-geometry']"
3670740,"Find the minimum value of $\frac ab+\frac {b}{a+b+1}+\frac {b+1}{a}$ when $a,b>0$","I was trying to solve this question and I observed that if I use partial derivative then it would be calculating. Moreover, I observed that $\frac ab\times\frac {b}{a+b+1}\times\frac {b+1}{a}=\frac {b+1}{a+b+1}$ . So I thought to use AM-GM inequality and got $3(\frac {b+1}{a+b+1})^\frac13 \leq \frac ab+\frac {b}{a+b+1}+\frac {b+1}{a}$ . Now the equality holds if $\frac ab=\frac {b}{a+b+1}=\frac {b+1}{a}$ .
From here I also got these relations $\frac {b+1}{a+b+1}=\frac a{a+b}$ and $\frac ab=\frac {b}{a+b+1}=\frac {b+1}{a}=\frac {a+b+1}{a+b}$ . I am hoping that I will get a finite expression of $(\frac {b+1}{a+b+1})^\frac13 $ and then we are done as the particular values of $a,b$ are not asked. Help me from here. Am I in the correct way? Do you have any other suggestions?","['contest-math', 'multivariable-calculus', 'calculus', 'optimization', 'inequality']"
3670801,Studying the neighborhoods of a point,"Problem: Let be $X=\{(x,y,z) \in \mathbb{C}^3: z^2=xy \}$ . Prove that cannot exists a neighborhood of $(0,0,0)$ in $X$ which is homeomorphic to $D \times D$ where $D$ denote the unitary open disc of $\mathbb{C}$ . My attempt: The suggestion of the exercise is to first prove that the map $$p : \mathbb{C}^2 \to X$$ such that $p(s,t)=(s^2,t^2,ts)$ induces a covering $p: \mathbb{C}^2-\{0\} \to X-\{0\}$ . I prove it but I cannot manage to use it to conclude.","['complex-analysis', 'general-topology', 'algebraic-topology']"
3670896,"examples of unbiased, biased, high variance, low variance estimator","I have just learnt variance and bias in machine learning and statistics. I still don't understand examples of function that estimates distribution with high bias/variance, or low bias/variance. If function overfitts distribution that means that it has a high variance, but according to MSE loss formula it shouldn't be so, because of my logic: if it fits every data point then MSE loss is zero, hence bias and variance are all zeroes, that contradicts my knowledge. Please help me to answer this question, and also give me examples of estimator of distribution with high/low bias/variance.","['machine-learning', 'statistics', 'variance']"
3670908,Cartesian product of embeddings is embedding,"Let $M_1,M_2,N_1$ and $N_2$ be differentiable manifolds and $f_1:M_1\to N_1$ , $f:M_2\to N_2$ two embeddings which in my literature is defined as an injective, proper immersion and a proper function is a function for which the preimage of every compact set is compact. Now I want to show that $f_1\times f_2:M_1\times M_2\to N_1\times N_2$ is also an embedding. My ideas: Well, the injectivity is no problem. To show that it is an immersion I want to calculate $d_{(x_1,x_2)}(f_1\times f_2)$ which is a priori a function from $T_{(x_1,x_2)}M_1\times M_2=T_{x_1}M_1\times T_{x_2}M_2$ to $T_{(f_1(x_1),f_2(x_2))}N_1\times N_2=T_{f_1(x_1)}N_1\times T_{f_2(x_2)}N_2$ . Now I am not sure if $d_{(x_1,x_2)}(f_1\times f_2)=(d_{x_1}f_1,d_{x_2}f_2)$ but if it was, it would be easy to show that it is an immersion.
Furhtermore, to show that $f_1\times f_2$ is a proper function, I remebered from my topology class that every second-countable Hausdorff-space is compact if and only if it is sequentially compact and manifolds are second-countable Hausdorff-spaces. Then I took a compact set $K\subset N_1\times N_2$ and choose a sequence $(x_n^1,x_n^2)_{n\in\mathbb{N}}\subset f^{-1}(K)$ . I considered taking the image under $f$ of this sequence from which I can choose a convergent subsequence because $K$ is sequentially compact. Then I thought about going back to $(x_n^1,x_n^2)_{n\in\mathbb{N}}$ itself because choosing a subsequence of the image is also choosing a subsequence of the original sequence, but I am not sure if the subsequence, say $(x_{n_k}^1,x_{n_k}^2)_{k\in\mathbb{N}}$ , then has to converge in $f^{-1}(K)$ . Can someone please help me?","['differential-topology', 'smooth-manifolds', 'differential-geometry']"
3670913,Prove $\lim\limits_{x \to +\infty}\int_0^{\pi} xe^{-x\sin t}{\rm d}t=2$.,"Someone gives a proof as follows： Above all, notice that \begin{align*} I(x):&=\int_0^{\pi} x{\rm  e}^{-x\sin t}{\rm d}t=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm  d}t+\overbrace{\int_{\frac{\pi}{2}}^{\pi} x{\rm e}^{-x\sin t}{\rm  d}t}^{t~ \mapsto ~t+\frac{\pi}{2}}\\ &=\int_0^{\frac{\pi}{2}} x{\rm  e}^{-x\sin t}{\rm d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin  \left(t+\frac{\pi}{2}\right)}{\rm d}t\\ &=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\cos t}{\rm  d}t\\ &=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm  d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\cos\left(\frac{\pi}{2}-  t\right)}{\rm d}t\\ &=\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm
d}t+\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm d}t\\
&=2\int_0^{\frac{\pi}{2}} x{\rm e}^{-x\sin t}{\rm d}t. \end{align*} Consider making a substitution that $\theta:=tx.$ Then $x=\theta/t,
 {\rm d}\theta=x{\rm d}t.$ Thus $$I(x):=2\int_0^{\frac{\pi x}{2}}
 \exp\left(-x\sin\frac{\theta}{x}\right){\rm d}\theta.$$ Since $\theta/x \in[0,\pi/2],$ and $f(x):=\dfrac{\sin x}{x}$ decreases over $(0,\pi/2]$ , hence $ \sin \dfrac{\theta}{x}\ge \dfrac{2\theta}{\pi x}.$ Therefore $ -x\sin \dfrac{\theta}{x}\le -\dfrac{2\theta}{\pi },$ and
   further we obtain $$\left|
\exp\left(-x\sin\frac{\theta}{x}\right)\right|=
 \exp\left(-x\sin\frac{\theta}{x}\right)\le \rm
 e^{-\frac{2\theta}{\pi}},$$ the right hand side of which is integrable
   over $ [0,+\infty) .$ Moreover $$\lim_{x \to
 +\infty}\exp\left(-x\sin\frac{\theta}{x}\right)=\exp\lim_{x \to +\infty}\left(\frac{\sin \frac{\theta}{x}}{\frac{\theta}{x}}\cdot -\theta\right)=e^{-\theta}.$$ Now, we can exchange the orders of the limit and the integral by Lebesgue dominated convergence theorem and
   obtain $$\lim_{x \to +\infty}I(x)=2\int_0^{+\infty}{\rm
 e}^{-\theta}{\rm d}\theta=2.$$ Is this correct? I don't know Lebesgue dominated convergence theorem well. Is there another proof more elementary?","['integration', 'limits', 'calculus', 'real-analysis']"
3670996,About the limit $\mathop {\lim }\limits_{n \to \infty } \frac{1}{n \cdot \cos (n)} $,"How do you prove that the limit $ \lim\limits_{n \to \infty } \frac{1}{n \cdot \cos (n)} $ does not exist? I tried using the fact that $ \{ \, \cos n \mid n \in \mathbb{N} \, \} $ is dense in $[0,1]$ , but all I get from this is that there is a subsequence $(y_n)$ of $(x_n)$ , $x_n=\frac{1}{n \cdot \cos (n)}$ such that $\lim\limits_{n \to \infty} y_n = 0$ . (Because we can choose $(y_n)$ such that $\lim\limits_{ n \to \infty} \cos (y_n) = 1 $ )",['limits']
3671053,Isomorphism of affine algebras implies isomorphism of polynomial rings over related associated graded rings ( Exercise of E.Kunz ),"I'd like some help solving Exercise 13 chapter V of the book ""Introduction to commutative algebra and algebraic geometry"" by E. Kunz. Let an affine algebra $\mathcal{A}$ over a field $K$ have two representations $$\mathcal{A} =K\left[X_1,\ldots, X_n\right]/ I=K\left[Y_1,\ldots, Y_m\right]/ J $$ as homomorphic images of the polynomial rings $R:= K\left[X_1,\ldots, X_n\right]$ and $S:= K\left[Y_1,\ldots, Y_m\right]$ . Prove that
 there is an isomorphism of graded $\mathcal{A}$ -algebras $$\mathrm{g}r_I(R)\left[Y_1,\ldots,Y_m\right]\cong \mathrm {gr}_J(S)\left[X_1,\ldots,X_n\right],$$ where $X_i$ and $Y_j$ are all of degree one. Hint: It is suffices to show that $$ \mathrm{gr}_{J'}(S')=\mathrm{gr}_I(R)[Y_1,\ldots,Y_m], $$ for $S'=k[X_1,\ldots\, X_n,Y_1,\ldots, Y_m]$ and $J'=IS'+(Y_1,\ldots,Y_m)S'$ . He mentioned above as a hint. Actually, I can prove the hint, but I can not see how this hint results the claim. Any guidance would be appreciated.","['algebraic-geometry', 'commutative-algebra']"
3671084,Is it possible to find an expression for $\frac{d^n}{dx^n}e^{-x^2}$?,I am trying to find a general form to derivatives of the function $e^{-x^2}$ . I tried to do it by finding a pattern for the first derivatives but with no success. Any tip is welcome.,"['calculus', 'derivatives']"
3671099,"Prove that, if languages $R$ and $S$ are boring, then so are $R\cup S$ and $R\cdot S$","This problem is adapted from ""Mathematics for Computer Science"" (Lehman, Leighton, Meyers, 2018). Can anyone verify my solution attempt? Problem A word is a finite sequence of $0$ 's and $1$ 's, and a language is a set of words. Let's say a language $S$ is $0$ - finite when it includes only a finite number of words whose bits are all $0$ 's, that is, when $S\cap 0^*$ is a finite set of words. A language $S$ is boring when either $S$ or $\overline{S}$ is $0$ -finite. (a) Explain why $\left\{00\right\}^*$ is not boring. (b) Verify that, if $R$ and $S$ are boring, then so is $R\cup S$ . (c) Verify that, if $R$ and $S$ are boring, then so is $R\cdot S$ ( Note : $R\cdot S$ is the language consisting of all the words that can be obtained by concatenating a word from $R$ with a word from $S$ ). Hint : By cases: Whether $R$ and $S$ are both $0$ -finite, whether $R$ or $R$ contains no all- $0$ words at all (including the empty word $\lambda$ ), and whether neither of these cases hold. Solution attempt (a) Let $S=\left\{00\right\}^*$ . To show $S$ is not boring, I will show that both $S$ and $\overline{S}$ are not $0$ -finite. $S$ is not $0$ -finite. Proof . By contradiction. If $S$ is $0$ -finite, then there is a longest member in $S$ whose bits are all $0$ s. Call this member $s$ . However, $s' = s00$ is longer than $s$ and $s' \in S$ , which is a contradiction. $\overline{S}$ is not $0$ -finite. Proof . By counter-example. $\overline{S}=\overline{\left\{00\right\}^*}$ is the language of all words that aren't formed by an even number of $0$ 's. For example, the set $R$ of all words formed by an odd number of $0$ 's (that is, $R = \left\{ w\text{ | }w = 0^{2n+1}\right\}$ where $n \geq 0$ ) is a subset of $\overline{S}$ . Since $R$ has infinitely many members, this means that $\overline{S}$ contains an infinite number of all- $0$ words, and therefore $\overline{S}$ is not $0$ -finite. (b) Proof by cases: $R$ and $S$ are both $0$ -finite: The number of all- $0$ words in $R\cup S$ is: (number of all- $0$ words in $R$ ) + (number of all- $0$ words in $S$ ) - the number of all- $0$ words in $R\cap S$ . So, $R\cup S$ is $0$ -finite, and therefore boring. $R$ is $0$ -finite and $\overline{S}$ is $0$ -finite: $\overline{R\cup S} = \overline{R}\cap \overline{S}$ is $0$ -finite, because $\overline{S}$ is $0$ -finite. So, $R\cup S$ is boring. $\overline{R}$ is $0$ -finite and $\overline{S}$ is $0$ -finite: $\overline{R}\cap \overline{S}$ is $0$ -finite, because $\overline{R}$ is $0$ -finite and $\overline{S}$ is $0$ -finite. So, $R\cup S$ is boring. (c) I got stuck in this case. Here is a partial attempt: $R$ and $S$ are both $0$ -finite: The all- $0$ words in $R\cdot S$ are formed by the concatenation of all- $0$ from $R$ with all- $0$ from $S$ . Since the number of all- $0$ words from R and S are finite, the number of all- $0$ words in $R\cdot S$ must also be finite. So, $R\cdot S$ is $0$ -finite, and therefore boring. Either $R$ or $S$ contains no all- $0$ words (including the empty word $\lambda$ ): $R\cdot S$ consists of words $r\cdot s$ where $r\in R$ and $s\in S$ . Without loss of generality, assume $R$ contains no all- $0$ words. Since $r$ is not an all- $0$ word, then $r\cdot s$ can't be an all- $0$ word. So, $R\cdot S$ contains a finite number (zero) of all- $0$ words, and is therefore boring. Neither of the cases above: I got stuck here. Any hints?","['formal-languages', 'discrete-mathematics', 'regular-language']"
3671179,"show that limit $\lim_{n\to+\infty}f_{n}(x)=x+3$,if $f_{n+1}(x)=\sqrt{6(1+x)+f_{n}(x^2)}$","let $x$ is give postive real number,if $f_{0}(x)=0,0<x\le\dfrac{1}{2}$ , and such $$f_{n+1}(x)=\sqrt{6(1+x)+f_{n}(x^2)}$$ show that $$\lim_{n\to+\infty}f_{n}(x)=x+3$$ This problem is from AMM 11967(2017),this solution",['limits']
3671255,Weak convergence of $\Bigl(\sum\limits_{k=n}^\infty e_k\Bigr)_n$,"Let $e_n:=δ_{kn}$ , for $k\in\mathbb N$ . Given the sequence $(a_n):=\sum\limits_{k=n}^∞e_k\subset\ell^\infty$ , i.e. $$((1,1,\cdots),(0,1,1,\cdots),(0,0,1,1,\cdots),\cdots).$$ I want to know if $(a_n)$ converges weakly to zero. Assume $(a_n)$ does not converges weakly, then I could use Hahn-Banach to find a linear functional $\varphi \in (\ell^\infty)^*$ with $$
\lim_{n \to \infty} \varphi((a_n)) \neq 0 \; ,
$$ but if $(a_n)$ converges weakly to zero, then I have no idea to show this, since I lack an usefull characterization of $(\ell^\infty)^*$ .","['lp-spaces', 'functional-analysis', 'weak-convergence']"
3671294,Find value of $p$ to make the series $\sum\limits_{n=1}^\infty\left(\dfrac1{n^p}\sum\limits_{k=1}^nk^{3/2}\right)$ converge,"Find value of $p$ that makes $\sum\limits_{n=1}^\infty\left(\dfrac1{n^p}\sum\limits_{k=1}^nk^{3/2}\right)$ converge. I am using comparaison test: $0<k^{\frac{3}{2}}<k^2$ , $0<\sum{k^{\frac{3}{2}}}<\sum k^2$ . Since $\sum k^2=\dfrac{1}{6}n(n+1)(2n+1)$ , then, $$\sum _ { n = 1 } ^ { \infty } ( \frac { 1 } { n ^ { p } } \sum _ { k = 1 } ^ { n } k ^ { 3 / 2 } )<\sum _ {n=1} ^{\infty} \frac{1}{n^{p-3}}$$ By p-series $$\sum _ {n=1} ^{\infty} \frac{1}{n^{p-3}}$$ Converges when $$p>4$$ Have other test and how about my method right or wrong ?","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
3671399,"Probability that $Y > 3X$ where $X,Y$ are $N(0,1)$ random variables","$X$ and $Y$ are i.i.d. $N(0,1)$ random  variables.   You  are  given  that $Y >0$ .   What is the probability that $Y>3X$ ? Solution. The key is that $N(0,1)^2$ is cyclically symmetric.  When plotting the distributions,the pdf will be cyclically symmetric about the origin.  Then one can perform a geometric probabiltiy calculation to obtain the answer. How is $N(0,1)$ cyclically symmetric? Im not sure what this means in this case. How does $Y>0$ change this question? When you dont have this you can just consider adding multiple of the normal distribution (ie considering $Y-3X$ )","['probability-distributions', 'geometry', 'probability', 'normal-distribution']"
3671582,Spherical Harmonics expansion,"In the contex of $L^2$ space, it is usually stated that any square-integrable function can be expanded as a linear combination of Spherical Harmonics: $$
f(\theta,\varphi)=\sum_{\ell=0}^\infty \sum_{m=-\ell}^\ell f_\ell^m \, Y_\ell^m(\theta,\varphi)\tag 2
$$ where $Y_\ell^m( \theta , \varphi )$ are the Laplace spherical harmonics. The context here is important because this equality holds only in the sense of the $L^2$ -norm. This expansion holds in the sense of mean-square convergence — convergence in [[Lp space|L 2 ]] of the sphere — which is to say that $$\lim_{N\to\infty} \int_0^{2\pi}\int_0^\pi \left|f(\theta,\varphi)-\sum_{\ell=0}^N \sum_{m=- \ell}^\ell f_\ell^m Y_\ell^m(\theta,\varphi)\right|^2\sin\theta\, d\theta \,d\varphi = 0.$$ So in general this limit is NOT pointwise? So I can't say that the value at a point of the function equals the value of the expansion at the same point? If so, why it's usually stated out of the context of the structure of Hilbert space, that a bounded function or a square integrable function on the unit sphere can be expanded with Spherical Harmonics if it's not pointwise? I mean in some context outside the Hilbert space, where I am not interested in their square integral. Furthermore, if it's not pointwise, but only in the norm, am I allowed to sum term by term two different functions, with two different expansions, like in the quantum scattering problem?","['functional-analysis', 'spherical-harmonics', 'sequences-and-series']"
3671613,Second-Order In(exact) ODEs,"The second total derivative of $F(x,\ y(x))$ is $F_y y'' + F_{yy}(y')^2 + 2F_{xy}y' + F_{xx}$ .  Thus by analogy to first-order exact ODEs, if one notices a second-order ODE where this pattern equals some expression containing no $y$ 's, i.e., $\frac{1}{y}y'' - \frac{1}{y^2}(y')^2 - \frac{1}{x^2} = 0$ , the LHS can be condensed, in this case turning the ODE into $(ln|xy|)'' = 0$ , and both sides doubly integrated, in this case $ln|xy| = Ax + B \implies y = \frac{Be^{Ax}}{x}$ .  Of course, I had to reverse engineer this example from a chosen $F(x,\ y(x))$ ; otherwise it would have been likely prohibitively hard to spot. 1)  I assume a second-order ODE is exact if and only if the LHS can be condensed into $F(x,\ y(x))$ and integrated as shown above.  Is there an analogy here to the 2D-curl test for exactness? 2)  Is there a method here for finding integrating factors to make inexact equations exact, at least in certain cases? 3)  Is there a way to convert such ODEs into differential forms (analogous to how one ""sort of"" multiplies the first-order ODE through by $dx$ , even though that's not really what's going on but in practice it basically is)? 4)  There aren't many good resources on second-order exact ODEs, but the SE questions I found and this video spoken in a language I don't know never seem to involve $(y')^2$ .  Is this because it is only practical to consider the subset of cases where $F_{yy} = 0$ , as opposed to my contrived example where $F_{xy} = 0$ but $F_{yy} \neq 0$ , or are these second-order exact ODEs a different animal?  If the latter, are they related to my notion of second-order exact ODEs in any way?","['integration', 'ordinary-differential-equations', 'curl', 'integrating-factor', 'partial-derivative']"
3671629,If $f'(t)≥\sqrt{f(t)}$ then prove that $\sqrt{f(x)}≥\sqrt{f(1)}+\frac{1}2(x-1)$,QUESTION: Suppose $f$ is a function such that $f(x)>0$ and $f'(x)$ is continuous for every real number $x$ . If $f'(t)≥\sqrt{f(t)}$ then prove that $\sqrt{f(x)}≥\sqrt{f(1)}+\frac{1}2(x-1)$ . MY APPROACH: We observe that graph lies entirely above the $x$ -axis. And I had assumed that $f(x)=kx^{2n}$ $(k>0)$ . That satisfies the given conditions.. but that's not a general proof. How may I prove it rigorously? Any help is much appreciated. Thank you so much..,"['calculus', 'functions', 'graphing-functions', 'proof-writing']"
3671672,Singular value decomposition in the language of operator theory,"Let $H_i$ be a $\mathbb R$ -Hilbert space, $A\in\mathfrak L(H_1,H_2)$ be compact, $|A|:=\sqrt{A^\ast A}$ and $\sigma\in\mathbb R$ . How would we describe the singular value decomposition of $A$ in the language of operator theory? (Assuming $\dim H_i\in\mathbb N$ , if necessary.) To fix terminology, say that $\sigma>0$ is a singular value of $A$ if $\sigma$ is an eigenvalue of $|A|$ , i.e. $\mathcal N(\sigma-|A|)\ne\{0\}$ . This definition is equivalent to claim that there are $x_i\in H_i$ with $\left\|x_i\right\|_{H_i}=1$ and $$Ax_1\sigma x_2\text{ and }A^\ast x_2=\sigma x_1\tag1.$$ By the Courant-Rayleigh minimax principle , we may enumerate the singular values of $A$ in nonincreasing order. So, let $\sigma_i(A)$ denote the $i$ th largest singular value of $A$ for $i\in\mathbb N$ . (If there are only $k$ different singular values, $\sigma_i(A)=0$ for all $i>k$ .) Now we may mimic some parts of the spectral theorem for compact self-adjoint operators: Let \begin{align}E_i&:=\mathcal N(\sigma_i(A)-|A|),\\d_i&:=\dim E_i\end{align} and $\left(e^{(i)}_1,\ldots,e^{(i)}_{d_i}\right)$ be an orthonormal basis of $E_i$ for $i\in\mathbb N$ and \begin{align}(\sigma_i)_{i\in\mathbb N}:=(\underbrace{\sigma_1(A),\ldots,\sigma_1(A)}_{=:\:d_1\text{ times}},\underbrace{\sigma_2(A),\ldots,\sigma_2(A)}_{=:\:d_2\text{ times}},\ldots),\\(e_i)_{i\in\mathbb N}:=\left(e^{(1)}_1,\ldots,e^{(1)}_{d_1},e^{(2)}_1,\ldots,e^{(2)}_{d_2},\ldots\right).\end{align} Then $(e_i)_{i\in\mathbb N}$ is an orthonormal basis of $\mathcal N(A)^\perp$ (since $\mathcal N(A)=\mathcal N(|A|)$ ) and $$|A|x_1=\sum_{i\in\mathbb N}\sigma_i\langle x_1,e_i\rangle_{H_1}e_i\tag2.$$ How do we need to proceed? And how is this related to the polar decomposition $^1$ of $A$ ? $^1$ There is an unique partial isometry $U$ from $H_1$ to $H_2$ with $\mathcal N(U)=\mathcal N(A)$ and $A=U|A|$ .","['svd', 'eigenvalues-eigenvectors', 'operator-theory', 'functional-analysis', 'spectral-theory']"
3671705,Are there unsolved indeterminate limits?,"I find the question itself is hard to put precisely. I apologize in advance. A simple version could be: Let $\mathcal{F}$ be the set of functions obtained via elementary binary operations (sum, product, power, end their inverses) and composition, including polynomials and sinusoidals ( $cos(x)$ and $sin(x)$ ). For example $f(x)=e^{e^x}+cos(x)$ . Basically the functions one encounters in basic calculus. The question is the following: Are there $f,g\in \mathcal{F}$ such that: $$\lim_{x\to \infty} f(x)=\infty,$$ $$\lim_{x\to \infty} g(x)=\infty,$$ but such that the limit $$\lim_{x\to \infty}\dfrac{f(x)}{g(x)}$$ is unsolved? What I mean by this is that no one in the mathematical community knows if the limit exists, or even if existence is guarranteed, no one knows if the limit is finite on infinit.","['indeterminate-forms', 'limits', 'calculus']"
3671714,"""Type"" of a permutation, set function, partition","Suppose I take the permutations on $n$ letters, $\mathcal{S}_n$ , and consider two functions from it $$f:\mathcal{S}_n \xrightarrow{\substack{\text{count} \\ \text{number} \\ \text{of cycles}}} \{1, \dotsc, n\}$$ $$g:\mathcal{S}_n \xrightarrow{\substack{\text{get} \\ \text{type}}} \substack{\text{Cycle types} \\ \text{ of }\mathcal{S}_n} \cong \substack{\text{partitions}\\ \text{ of } n}$$ Given $i \in \{1, 2, \dotsc, \#S\}$ , the fiber of $f$ over $i$ will have cardinality $\displaystyle {n \brack i}$ . As for $g$ , given a cycle type, represented by the formal monomial $1^{m_1}2^{m_2}\dotsb n^{m_n}$ the fiber of $g$ over that element will have cardinality $$\frac{n!}{1^{m_1}m_1! \dotsb n^{m_n}m_n!} \tag{1}$$ Now suppose I switch gears and compare partitions of sets to partitions of integers. Given a set $S$ , there is a surjection $$\bar{f}: \substack{\text{Partitions} \\ \text{ of }S} \xrightarrow{\substack{\text{count} \\ \text{number} \\ \text{of parts}}} \{1,\dotsc, \#S\}$$ $$\bar{g}: \substack{\text{Partitions} \\ \text{ of }S} \xrightarrow{\substack{\text{forget} \\ \text{elements}}} \substack{\text{Partitions} \\ \text{ of }\#S}$$ Given $i \in \{1, 2, \dotsc, \#S\}$ , the fiber of $\bar{f}$ over $i$ will have cardinality $\displaystyle {n \brace i}$ . As for $\bar{g}$ , given a partition of the integer $\#S$ , represented by $1^{n_1}2^{n_2}\dotsb \#S^{n_{\# S}}$ the fiber of $\bar{g}$ over that element will have cardinality $$\frac{\#S!}{\prod\limits_{k=1}^{\# S} k!^{n_k} n_k!} \tag{2}$$ Now suppose I switch gears again and talk about set functions from $\{1, \dotsc, k\} \to \{1, \dotsc, n\}$ . Denote the sets of these by $F_{k,n}$ . Consider the two maps $$\hat{f}: F_{k,n} \xrightarrow{\substack{\text{quotient by} \\  \text{left action of } \\ S_k}} \substack{\text{Multisets} \\ \text{of }\{1, \dotsc, n\} \\ \text{of size }k}$$ $$\hat{g}: F_{k,n} \xrightarrow{\substack{\text{get} \\ \text{""type""}}} \substack{\text{(Weak) } \\ n-\text{compositions} \\ \text{ of }k} \cong \substack{\text{(Strong) } \\ n-\text{compositions} \\ \text{ of }n + k}$$ Given a multiset of $\{1, \dotsc, n\}$ of size $k$ , written as $1^{m_1}\dotsb n^{m_n}$ , the number of elements in the fiber of $\hat{f}$ over that multiset is the multinomial coefficient $\displaystyle {k \choose m_1, \dotsc, m_n}$ . As for $\hat{g}$ , given a weak $k$ -composition of $n$ , the number of elements in the fiber of $\hat{g}$ over that element is $$?? \tag{3}$$ Question: It is mostly (2) that I am interested in. Does it have a name? (This came up when I was studying the Faa di Bruno formula, in particular related to moments and cumulants of a probability distribution.) I am also particularly interested in the relation to ""types"" in statistics (which are like the multi-sets you can get when you draw from a categorical distribution). Also, is there some unifying framework for these things? Why are the fibers of $f, \bar{f}, \hat{f}$ such well-named and familiar combinatorial objects, but the fibers of $g, \bar{g}, \hat{g}$ aren't (in my limited experience)?","['formal-power-series', 'combinatorics']"
3671809,Proving $f^{-1}(G\cup H)=f^{-1}(G) \cup f(H)$ and $f^{-1}(G\cap H)= f^{-1}(G) \cap f^{-1}(H)$,"""Introduction to Real Analysis"" (Robert G. Bartle) Chapter 1, Question 13: Show that if $f:A\to B$ and $G,H$ are subsets of $B$ , then $f^{-1}(G\cup H)=f^{-1}(G) \cup f(H)$ and $f^{-1}(G\cap H)= f^{-1}(G) \cap f^{-1}(H)$ EDIT: Despite @ArtudoMagdin's comments, I still cannot form a complete, correct solution. I require a full answer. I crossed out the solution verification tag. Attempt: Proving $f^{-1}(G\cup H)=f^{-1}(G)\cup f^{-1}(H)$ We know $\require{enclose} \enclose{horizontalstrike}{f(x)\in G}$ and $\require{enclose} \enclose{horizontalstrike}{G\subseteq G\cup H}$ so $\require{enclose} \enclose{horizontalstrike}{f(x)\in G\cup H}$ meaning $\require{enclose} \enclose{horizontalstrike}{f^{-1}(G)\subseteq f^{-1}(G\cup H)}$ and $\require{enclose} \enclose{horizontalstrike}{f(x)\in H}$ and $\require{enclose} \enclose{horizontalstrike}{H \subseteq G\cup H}$ so $\require{enclose} \enclose{horizontalstrike}{f(x)\in G\cup H}$ meaning $\require{enclose} \enclose{horizontalstrike}{f^{-1}(H)\subseteq f^{-1}(G\cup H)}$ . Since $\require{enclose} \enclose{horizontalstrike}{f(x)}$ is independetly in $\require{enclose} \enclose{horizontalstrike}{G}$ or $\require{enclose} \enclose{horizontalstrike}{H}$ , $\require{enclose} \enclose{horizontalstrike}{f^{-1}(G)\cup f^{-1}(H) = f^{-1}(G\cup H)}$ Edit: Here is my new attempt: Second Edit: I made additional changes by the request of @ArturoMagidin If $x\in f^{-1}(G)$ then $f(x) \in G \subseteq G\cup H$ , hence $f^{-1}(G)\subseteq f^{-1}(G\cup H)$ If $x\in f^{-1}(H)$ then $f(x)\in H\subseteq G \cup H$ , hence $f^{-1}(H)\subseteq f^{-1}(G\cup H)$ If $x\in f^{-1}(G)\cup f^{-1}(H)$ , it follows 1. and 2. that $f(x)\in G \cup H$ which also means that $x\in f^{-1}(G\cup H)$ . It follows from 3. that $f^{-1}(G)\cup f^{-1}(H)\subseteq f^{-1}(G\cup H)$ If $x\in f^{-1}(G\cup H)$ then $f(x)\in G\cup H$ , hence $f(x)\in G$ or $f(x)\in H$ which means $x\in f^{-1}(G)$ or $x\in f^{-1}(H)$ . Hence, $f^{-1}(G\cup H)=f^{-1}(G)\cup f^{-1}(H)$ Is my proof correct, it seems this part is right? Proving $f^{-1}(G\cap H)\subseteq f(G) \cap f(H)$ We know $\require{enclose} \enclose{horizontalstrike}{f(x)\in G}$ and $\require{enclose} \enclose{horizontalstrike}{G\supseteq G\cap H}$ so if $\require{enclose} \enclose{horizontalstrike}{f(x)\in G\cap H}$ meaning $\require{enclose} \enclose{horizontalstrike}{f^{-1}(G)\supseteq f^{-1}(G\cap H)}$ and $\require{enclose} \enclose{horizontalstrike}{f(x)\in H}$ and $\require{enclose} \enclose{horizontalstrike}{H \supseteq G\cup H}$ so $\require{enclose} \enclose{horizontalstrike}{f(x)\in G\cup H}$ meaning $\require{enclose} \enclose{horizontalstrike}{f^{-1}(H)\subseteq f^{-1}(G\cup H)}$ . I am not sure how to proceed from here. Is my approach correct? If so, how would this imply that $\require{enclose} \enclose{horizontalstrike}{f^{-1}(G\cap H)=f^{-1}(G)\cap f^{-1}(H)}$ Edit: Here is my new attempt for 2. Second Edit: I made additional changes by the request of @ArturoMagidin Third Edit: I shortened my steps per @ArturoMagidin's comments. I figure I could have done the same for the first proof. If $x\in f^{-1}(G)\cap f^{-1}(H)$ then $f(x)\in G$ and $f(x)\in H$ therefore $f(x)\in G\cup H$ so $f^{-1}(G\cap H)\subseteq f^{-1}(G)\cap f^{-1}(H)$ If $x\in f^{-1}(G\cap H)$ then $f(x)\in G\cap H$ , hence $f(x)\in G$ and $f(x)\in H$ which means $x\in f^{-1}(G)$ and $x\in f^{-1}(H)$ . Hence, $f^{-1}(G\cap H) \supseteq f^{-1}(G)\cap f^{-1}(H)$ From 1. and 2. we state that $f^{-1}(G\cap H)\subseteq f^{-1}(G)\cap f^{-1}(H)$ I am not satisfied with step 5.? How do we show $\require{enclose} \enclose{horizontalstrike}{f^{-1}(G\cap H)\supseteq f^{-1}(G)\cap f^{-1}(H)}$ ? Is this correct? EDIT: According to @ArtudoMagdin, it is still wrong. What is the full solution for this proof?","['elementary-set-theory', 'proof-explanation', 'real-analysis']"
3671867,How to solve this system of ODEs,"Let $f:[0,1]\to\mathbb{C}$ be a given ""nice"" function and let $a,b:[0,1]\to\mathbb{R}$ be two unknown functions to be solved for (with some initial conditions $a(0),a'(0),b(0),b'(0))$ . How can one solve the following equation: $$ f(x) = \exp\left(-\mathrm{i}\int_0^x\cos(a(y))b'(y)\mathrm{d}y\right)\left(a'(x)+\mathrm{i}\sin(a(x))b'(x)\right)\qquad(x\in[0,1])\,? \tag{E}$$ I'm a bit stumped by the mis-match of the $\sin$ and $\cos$ . For example, had the equation been $$ f(x) = \exp\left(-\mathrm{i}\int_0^x\cos(a(y))b'(y)\mathrm{d}y\right)\left(a'(x)-\mathrm{i}\cos(a(x))b'(x)\right)\qquad(x\in[0,1]) $$ we could have re-written the RHS as $$ \left(a \exp\left(-\mathrm{i}\int_0^\cdot\cos(a)b'\right)\right)'  $$ so that $$ \int_0^x f = a(x) \exp\left(-\mathrm{i}\int_0^x\cos(a(y))b'(y)\mathrm{d}y\right) $$ and we could have found $a,b$ by looking at the absolute value or respectively the phase of the LHS. However, due to the mismatch between the cosine and sine in (E) I am not sure how to proceed, though I hope it does not mean that there isn't a closed form solution.","['analysis', 'ordinary-differential-equations']"
3672019,Why does Fourier Transform use a negative exponent in its formula?,"I realize this question has been asked before but I don't understand the explanations. For example I have read this ( https://en.wikipedia.org/wiki/Negative_frequency ) as well as numerous other answers. I understand e^-jwt is simply a point going clockwise in the complex domain. What I don't understand is why you multiply a signal x(t) with a clockwise complex number. What would be the result of let's say Fourier Transform using the positive exponent? I guess one theory I had is they define shifted impulses using delta[t-to] and the Fourier transfform of that is going to be e^-jwto. We think of things shifted after time 0 rather than before, so is this why we have a negative exponent? If I'm wrong, please provide clear sequential steps.","['complex-analysis', 'signal-processing', 'fourier-analysis', 'fourier-transform']"
3672087,Why is it nice to be able to “add in quadrature” [$f(x+y)^2 = f(x)^2 + f(y)^2$]?,"If $x, y$ are two mathematical objects/variables, and $f$ is some function, then by “adding in quadrature” I mean that $(f(x+y))^2 = (f(x))^2 + (f(y))^2$ . For example, we have the Pythagorean Theorem for inner product spaces: for orthogonal vectors $u, v$ , we have $$\lVert {u + v} \rVert^2 = \lVert {u} \rVert^2 + \lVert {v} \rVert^2.$$ For two independent random variables $X, Y$ , the variance of the sum is the sum of variances: $$\sigma_{X+Y}^2 = \sigma_{X}^2 + \sigma_{Y}^2.$$ Also, I believe that when errors are uncorrelated, we can add uncertainties in quadrature. My question is: In general, when might we have $$(f(x+y))^2 = (f(x))^2 + (f(y))^2?$$ Why is this important? In all the examples above, there was some notion of independence: orthogonality, independent variables, and uncorrelatedness all have some interpretation as to how things are “independent”. My attempt at answering it might be: “If the square of $f(x+y)$ can be written only in terms of the original function values $f(x)$ and $f(y)$ , this indicates some independence among the variables.” But, why might it be related to squaring? Another thing I found interesting is that, if instead of squaring we just considered powers of 1, then we get $f(x + y) = f(x) + f(y)$ , which is just additivity. Is there something about squaring that is sort of like a “second-order” condition? (Whereas additivity might be the “first-order” condition?) Perhaps analogous to something like the first-derivative test and second-derivative test? Is it an interesting question to ask more generally whether $(f(x+y))^n = (f(x))^n + (f(y))^n$ , or some other similar question?","['statistics', 'real-analysis', 'functions', 'linear-algebra', 'physics']"
3672102,How to solve $\frac{dy}{dx} = \frac{1}{\sqrt{2^2 - x^2}}$?,"I've just started with differential equations and in the textbook I was given two, with one of which I have trouble. The task was to solve them with software, but I considered it'd be better to solve by hand. But since it's just the start of the topic in the textbook, not much has been provided about how to do it. The equation and the initial value problem are: $$
\frac{dy}{dx} = \frac{1}{\sqrt{2^2 - x^2}} \\\
y(0) = 2
$$ I looked into two tables of integrals (so it's doublechecked) and found this: $$
\int{\frac{1}{\sqrt{a^2 - x^2}}}{dx} = \frac{1}{\sin{\frac{x}{a}}} + C
$$ But the problem is that y is not defined at 0 (if the tables are correct). Since y(0) = 2 might be a typo in the textbook, I tried to check whether the solution would work for some other point. I took x = 1 and C = 0 and tried to plot y and t (the tangent) in software: $$
y(x) = \frac{1}{\sin{\frac{x}{a}}} \\\
y'(1) = \frac{1}{\sqrt{3}} \\\
t(x) = \frac{x - x_0}{\sqrt{3}} + t(x_0) = \frac{x - 1}{\sqrt{3}} + \frac{1}{\sin{\frac{1}{2}}}
$$ As you can see, t (the yellow curve) is definitely not tangent to y(x). But it seems normal, and indeed, when I changed the yellow to $t(x) = -\sqrt{3}(x - 1) + \frac{1}{\sin{\frac{1}{2}}}$ it became tangent at x = 1. So could anyone, please, explain, what's going on here? Did I use the integral tables correctly? How come the function under the integral sign (which is a derivative) gives not a tangent but normal to the right hand side of the integral solution? Thank you.","['ordinary-differential-equations', 'tangent-line', 'calculus', 'indefinite-integrals', 'derivatives']"
3672133,Is this like the Birthday Problem? Poisson Halloween Party,"Suppose that there are n guests at a Halloween party, and that each is wearing one of 200 possible costumes available at local store, uniformly at random and independently of all other guests. Using Poisson approximation and the value $\sqrt{\log{2}}$ = $0.83$ , show that only about n = 17 guests are needed to ensure that some pair of guests are wearing the same costume with probabiliy at least 50%. The hint I received for this question was: ""This approximation is quite accurate. It can be shown that, when $n = 17$ , the true probability of a match is $1 − (200)_{17}/200^{17}$ = 50.3%."" I interpreted this question as one similar to the ""Birthday Problem"" often discussed in probability theory courses. But I'm not sure how to handle/incorporate the Poisson approximation and 'uniformly at random' aspects of this problem. It would be great to hear any insight as to how I could possibly structure this problem—thank you!","['birthday', 'statistics', 'approximation', 'probability']"
3672139,"How do I show that if $f$ and $g$ are entire and $|f|\ge |g|$, then there is some $\beta$ such that $f(z) = \beta g(z)$ for all $z$? [duplicate]","This question already has answers here : Suppose $f$ and $g$ are entire functions, and $|f(z)| \leq |g(z)|$ for all $z \in \mathbb{C}$, Prove that $f(z)=cg(z)$. (4 answers) Closed 4 years ago . I was wondering if you could help me with a question: Suppose that $ f $ and $ g $ are entire functions, and that $ |f(z)| \leq |g(z)| ,\forall z \in C $ . Prove that there $ \exists \beta \in C $ such that $f(z) = \beta g(z), \forall z âˆˆ C$ . I tried to show $f(z)/g(z) $ was constant by Liouville theorem however we don't know if $ f(z)/g(z)$ is entire as $g(z)$ might be equal to $0$ . So I couldn't use the fact that it is entire and bounded to use Liouville theorem. Do you have an idea? 
thank you in advance",['complex-analysis']
3672157,"Copeland-Erdős constant: does the cumulative even digit count ever reach or overtake the cumulative odd digit count, and if so, at what prime?","Arthur Herbert Copeland and Paul Erdős proved in 1946 that the Copeland-Erdős constant is a normal number . Since all prime numbers other than 2 are odd, all prime numbers other than 2 end in an odd digit, so one might expect skew of the digit distribution toward odds, since each prime number other than 2 is guaranteed at least 1 odd digit, while there is no such at-least-1-digit guarantee for even digits. So for the constant to be normal, it must be that as the prime numbers go toward infinity, prime numbers become so long digit-wise that the oddness of the last digit becomes negligible. Now, looking at the first few digits (0.235711131719232931374143...), it is obvious that odd digits far outnumber even digits within the early digits. But since the constant is normal, the evens must ""catch up"" eventually: either... (a) ...the evens asymptomatically approach from below a 50% distribution of all digits, or... (b) ...(what seems to me much more likely) which parity of digits is ahead changes infinitely often, though it might take a long time and a very large prime for the evens to first catch up (reminiscent of the very large Skewes's numbers and related numbers wherein π(x) finally catches up to li(x) for the first time), or... (c) ...(what seems to me to be unlikely) a combination of the above two cases so that after a finite number of switches of the lead, one parity stays ahead forever while the other stays asymptomatically close. Does anyone know if there is a proof of which of the three cases is true? If, as I suspect, case (b) is true, what is the smallest prime at which the cumulative even digit count catches up to the odds? Numerical Results Let r(n) be the proportion of even digits after the nth prime. So, since the constant starts out 0.2 3 5 7 11 13 ..., the first few values of r(n) are r(1) = 100%, r(2) = 50%, r(3) = 33.333...%, r(4) = 0.25%, r(5) = 16.666...%, r(6) = 12.5%. Below, when I refer to the ""maximum value"" of r(n), I am disregarding the trivial r(1) and r(2) values. I wrote a script to calculate r(n) up to $n = 7.5 \times 10^7$ (75 million). For reference as to roughly how large these primes are, the 75,000,000th prime is 1,505,776,939. For n ≥ 3, r(n) initially falls before starting to rise, before finally tying r(3) = 1/3 at r(380), with r(381) = 444 / (444 + 883) ≈ 33.45% being the first value of r(n) to exceed r(3). Beyond r(381), r(n) oscillates (obviously), but on average, it rises much more than it falls and initially grows rapidly on average — but as the primes get larger and larger, its average rate of growth falls. r(n) first hits 34% at r(389), hits 35% at r(416), hits 36% at r(654), hits 37% at r(1,106), hits 38% at r(3,097), hits 39% at r(6,861), hits 40% at r(24,613), hits 41% at r(55,426), hits 42% at r(210,117), hits 43% at r(1,790,106), and hits 44% at r(25,609,981). Anyway, as of the 75th million prime 1,505,776,939, the highest value of r(n) thus far is 44.2537565841856...% at the 46,450,161st prime, 909,090,109. I still do not know if r(n) does ever hit 50%.","['number-theory', 'irrational-numbers', 'analytic-number-theory', 'limits', 'prime-numbers']"
3672164,Evaluating $\int_0^\pi\frac{e^{-\sin x}}{e^{-\sin x}+e\sin x}dx$,"I think I thought of this integral a little over a year ago and I just haven't been able to do it. I really want to and everytime I sit down with it I learn a little bit more (or atleast I think I do) and then I get really stumped and give up. The integral in question is $$I=\int_{0}^{\pi}\frac{e^{-\sin(x)}}{e^{-\sin(x)}+e\sin(x)}dx$$ Which can be rewritten into $$I=2\int_{0}^{\pi/2}\frac{e^{-\sin(x)}}{e^{-\sin(x)}+e\sin(x)}dx$$ by making note of the fact that the function enclosed in the integral is even. One other thing to note is that if we consider the $esin(x)$ term in the denominator and write a ""new"" integral with this term replacing the $e^{-\sin(x)}$ term in the numerator we get the following $$J=\int_{0}^{\pi}\frac{e\sin(x)}{e^{-\sin(x)}+e\sin(x)}dx.$$ Taking the sum of $I$ and $J$ we, of course, get the answer $\pi$ , meaning that a solution to either one would give a solution to the other (hopefully this helps in some way). I have a decent background and level of understanding with many of the more popular ""advanced"" integration techniques and special functions, so don't be afraid to write a really scary looking solution. Finally, I genuinely don't think there is a nice closed-form solution for this integral, but I don't know how to show that and am really hoping there is a closed-form. Best of luck with your solutions.","['integration', 'trigonometry', 'definite-integrals', 'special-functions']"
3672181,Why does the tensor product appear in the codomain of a Lie Algebra valued one-form?,"I'm watching a lecture series on differential geometry and a Lie-algebra valued one-form $A$ on a principle $G-$ bundle $(P, \pi, M)$ is written to belong to the space $$
 A \in \Omega^1(M) \otimes T_e G
$$ Where $\Omega^1(M)$ is the space of one-forms over $M$ and $T_e G$ is the lie algebra of the lie group $G$ . Can someone explain why the tensor product shows up here?",['differential-geometry']
3672191,"'Amount' of nowhere-differentiable functions in $C([0,1])$?","A well-known consequence of the Baire Category Theorem that the set of nowhere-differentiable continuous functions is dense in $C([0,1])$ . This is often cited as 'almost all continuous functions are nowhere differentiable' (see here ), but to me this seems like a strange way of stating the fact, akin to saying that 'almost all real numbers are rational' just because the rationals are dense in the reals. By the Weierstrass Approximation Theorem, the set of polynomials is also dense in $C([0,1])$ , so is it correct to say that almost all continuous functions are polynomials? This statement seems to contradict the original statement about nowhere-differentiable functions. I was wondering if there was some way to remedy my confusion using a measure on $C([0,1])$ , since the Lebesgue measure on $[0,1]$ clarifies what it means to say 'almost all' in the context of real numbers; that a property holds for all real numbers outside a set of measure $0$ . In this context, certainly not 'almost all' real numbers are rational, almost all real numbers would be irrational. If this is the right way of thinking about 'sizes' of subsets of $C([0,1])$ , what would be the 'right' measure to use? If not a measure, is there another way to formalize the idea of 'almost all' in the context of $C([0,1])$ ?","['measure-theory', 'baire-category', 'real-analysis']"
3672238,Is the following ratio of gamma functions increasing: $\frac{\Gamma(2n - \frac{1.25506n}{\ln n})}{\Gamma(n)^2}$?,"For $n > 1$ , is the following ratio of gamma functions increasing: $\dfrac{\Gamma(2n - \frac{1.25506n}{\ln n})}{\Gamma(n)^2}$ I suspect that it is at some point where $n > 1$ . I would like figure out if the derivative is increasing or not and if increasing, from what point? I had hoped that this series ψ would be sufficient with: $$\frac{d}{dx}(\ln\Gamma(x)) = \frac{\psi(x)}{dx} = -\gamma + \sum_{k=0}^\infty(\frac{1}{k+1} - \frac{1}{k + x})$$ So, my goal would be to show that the following is increasing for $n \ge 1$ : $$\ln\Gamma(2n - \dfrac{1.25506n}{\ln n}) - 2\ln\Gamma(n)$$ This got me to: $$\frac{d}{dx}\left(\ln\Gamma(2n - \dfrac{1.25506n}{\ln n}) - 2\ln\Gamma(n)\right) = \frac{\psi(2n - \frac{1.25506n}{\ln n})}{2 - \frac{1.25506}{\ln n} + \frac{1.25506}{\ln^2 n}} - 2\psi(n)$$ When I tried to apply the last part, I was at a loss. How would I complete the argument to determine whether there exists a real $n > 0$ where the function is strictly increasing? Edit 1: I had a thought.  Does the following logic work? An easier problem is: $$\frac{d}{dx}(\ln\Gamma(2n) - 2\ln\Gamma(n)) = \frac{\psi(2n)}{2} - 2\psi(n) = \sum\limits_{k=0}^{\infty}\left(\frac{1}{n} - \frac{1}{2n}\right) > 0$$ If I change this to some real constant $c < 1$ : $$\frac{d}{dx}(\ln\Gamma(n(2-c)) - 2\ln\Gamma(n)) = \frac{\psi(n(2-c))}{2-c} - 2\psi(n) = \sum\limits_{k=0}^{\infty}\left(\frac{1}{n} - \frac{1}{n(2-c)}\right) > 0$$ Would it now be sufficient to complete the argument by showing that for $n \ge 4$ : $$\frac{1.25506}{\ln n} < 1$$ and showing that: $$\frac{d}{dx}\left(\frac{1.25506}{\ln n}\right) = -\frac{1.25506}{n\ln^2(n)}$$ which is decreasing at $n\ge 4$ . Is this enough to establish the conclusion? Edit 2: To be clear, it should be: $$\frac{\Gamma(2n - \frac{1.25506n}{\ln n})}{[\Gamma(n)]^2}$$","['gamma-function', 'inequality', 'derivatives', 'digamma-function']"
3672252,there are infinitely many $n$ such that $p(n)<p(n+1)<p(n+2)<p(n+3).$,"Let $p(n)$ denote the largest prime factor of $n$ . Prove that there are infinitely many $n$ such that $$p(n)<p(n+1)<p(n+2)<p(n+3)$$ if the three consecutive numbers,I can prove it,also can see three , erdos ,but for four (or more) consecutive number maybe is old？and How do to？",['number-theory']
3672257,Determining if a function is differentiable,"$$f(x)= \sum^{\infty}_{n=1}\frac{1}{(n+x)^2}$$ for $x \in [0, \infty)$ This above is a function which (with help) I have proved to be continuous on $[0, \infty)$ I now want to prove that $f$ is differentiable however I am struggling to see how. I know that since $f$ is continuous, if is integrable which also applies to $f'$ ? Furthermore the solution to this question posted ( Uniform convergence of $f'$ on an interval implies locally uniform convergence of $f$ ) is clearly a related answer as I know that proving differentiability involves the uniform convergence of a sequence of functions. Can anyone explain to me how the example I linked might be applied to my particular problem? Many thanks!","['continuity', 'uniform-convergence', 'analysis', 'real-analysis']"
3672259,A certain Gaussian integral,"Can somebody evaluate the following Gaussian integral? $$
I(t,\sigma) := \int_{-\infty}^\infty \frac{dx e^{-x^2/(2\sigma^2)}}{\sqrt{2\pi \sigma^2}}
\frac{\sin{\left(2 t\sqrt{1+x^2} \right )}}{\sqrt{1+x^2}} \tag{1}
$$ Where of course $\sigma>0$ (and $t\in\mathbb{R}$ ). Expanding the $\sin$ I could not resum the series. However I have some hope (perhaps misplaced) that the integral can be expressed in terms of elementary functions. 
In fact an analogous integral (that arises in the same problem) turns out to be surprisingly simple: \begin{align}
G(t,\sigma) &:=  \int_{-\infty}^\infty \frac{dx e^{-x^2/(2\sigma^2)}}{\sqrt{2\pi \sigma^2}} \tag{2}
\frac{1+x^2\cos{\left(2 t\sqrt{1+x^2} \right )}}{1+x^2}\\
&\simeq \exp\left( -2 \sigma^2 \sin(t )^2 \right)
\end{align} Added My memory failed me in a more complicated way. It turns out that the above equation is not an exact evaluation of the integral $G$ but instead an excellent approximation (based on some physical theory). To my excuse here is a plot of $G(t,\sigma=0.2)$ (dots) versus the approximation (continuous line) At this point I have little hope that $I$ (or $G$ ) can be still evaluated analytically but I would love to be proved wrong. Thanks again!","['integration', 'definite-integrals', 'gaussian-integral']"
3672348,Find an isometry from a plane to another in $R^3$,"Question If P is the plane through $(1/2, -1, 0)$ orthogonal to $(0, 1, 0)$ find an isometry F = TC such that F(P) is the plane through $(1, -2, 1)$ orthogonal to $(1, 0, -1)$ . This is an exercise in O'neill's Elementary Differential Geometry. I found the given plane P: y = -1 and F(P): x-z= $0$ But that's all I did. How should I approach this problem?",['differential-geometry']
3672439,How continuous are functions that map dense sets to dense sets?,"Let $f:[0,1] \to [0,1]$ be a function with the property that for every dense $D \subset [0,1]$ , $f(D)$ is dense in $f([0,1])$ . We can note that $f$ need not be continuous. For instance, consider $f(x)=\left|\sin \ \left(\frac{1}{x-1/2}\right)\right|$ with $f(1/2)=0$ . Is there a nowhere continuous example? If not, how large must the continuity set of such functions be?","['continuity', 'real-analysis']"
3672450,Understanding the definitions of vector and scalar,"So I am preparing now to start studying Lagrangian and Hamiltonian mechanics with Marion's book on classical dynamics. It is the first time I encounter the formal definition of vector and scalar, and I found it hard to understand. First of all, the definition of vector: correct me if I am wrong, but what I have understood is that, if a set of quantities, $A_1, A_2, A_3$ for three-dimensions, transforms as a point under a rotation transformation, then we call $\vec{A}=(A_1, A_2, A_3)$ a vector. In summary, a vector's components transform as a point under a coordinate rotation. The definition I don't quite understand is the definition of a scalar. It is said that a scalar is a quantity that remains invariant under a coordinate rotation. How can one understand this definition for, for example, temperature? How can a scalar be expressed in terms of the coordinate we are in? I would appreciate help in understanding these concepts, thanks in advance!","['coordinate-systems', 'group-theory', 'definition', 'vectors']"
3672534,Direct proof that closed 1-form on $\mathbb{R}^2$ is exact,"Let $\omega = a_1\cdot dx_1 + a_2 \cdot dx_2 \in \Omega^1 \mathbb{R}^2$ . Show that if $d\omega = 0$ , then $$f(x) = f(x_1,x_2) := x_1 \cdot \int_0^1 a_1(tx)dt + x_2 \cdot \int_0^1 a_2(tx)dt$$ $x \in \mathbb{R}^2$ defines a function $f \in \Omega^0 \mathbb{R}^2 = C^{\infty} (\mathbb{R}^2,\mathbb{R})$ with $df = \omega$ . I tried to derive this f, but I have some problems with dealing with the map h. We defined the derivative of f like this:
For a chart (U,h,V) around p with coordinates $(x_1,..,x_n)$ in V we get: $$df(p) = \sum_{i=1}^n \frac{\partial (f\circ h^{-1})}{\partial x_i} (h(p)) \cdot dx_i(p)$$ I did this so far:
Using this definition we get for our f: $$df(x) = \frac{(f\circ h^{-1})}{\partial x_1} (h(x)) \cdot dx_1(p) + \frac{(f\circ h^{-1})}{\partial x_2} (h(x)) \cdot dx_2(x)$$ Using the chain rule we get $$= (\frac{\partial f}{\partial x_1}\circ h^{-1}) (h(x)) \cdot \frac{\partial h^{-1}}{\partial x_1}(h(x)) \cdot dx_1(x) + (\frac{\partial f}{\partial x_2}\circ h^{-1}) (h(x)) \cdot \frac{\partial h^{-1}}{\partial x_2}(h(x)) \cdot dx_2(x)$$ $$= (\frac{\partial f}{\partial x_1} (x)) \cdot \frac{\partial h^{-1}}{\partial x_1}(h(x)) \cdot dx_1(x) + (\frac{\partial f}{\partial x_2}(x) \cdot \frac{\partial h^{-1}}{\partial x_2}(h(x)) \cdot dx_2(x)$$ $$= ((\int_0^1 a_1(tx)dt) \cdot \frac{\partial h^{-1}}{\partial x_1}(h(x)) \cdot dx_1(x) + ((\int_0^1 a_2(tx)d) \cdot \frac{\partial h^{-1}}{\partial x_2}(h(x)) \cdot dx_2(x)$$ but I am a bit lost at this point, since I would have to use the indefinite integral of the $a_i$ and I don't see how I could end up with $\omega$ in the end. Also, I am confused by the $\frac{\partial h^{-1}}{\partial x_i}(h(x))$ . How could I do anything with them? It is just some map I don't know anything particular about.","['manifolds', 'differential-forms', 'differential-geometry']"
3672619,If $\lim_{r \to 1} \frac{1}{2\pi}\int_0^{2\pi} \left| \log \left| f(re^{it}) \right| \right| dt = 0$ then $\left| f(z) \right| \leq 1$,"My aim is to prove that Blaschke products are the only holomorphic funcions that verify the property $$
\lim_{r \to 1} \frac{1}{2\pi} \int_0^{2\pi} \left| \log \left| f(re^{it}) \right| \right| dt = 0.
$$ For this, I need to show that if $f$ is a holomorphic function defined in the open unit disc $\mathbb{D}$ such that $$
\lim_{r \to 1} \frac{1}{2\pi} \int_0^{2\pi} \left| \log \left| f(re^{it}) \right| \right| dt = 0
$$ then $\left| f(z) \right| \leq 1$ for all $z \in \mathbb{D}$ . I know that $z \in \mathbb{D} \mapsto \log \left| f(z) \right| $ is a subharmonic function and hence $$
t \in (0,1) \mapsto \frac{1}{2\pi} \int_0^{2\pi}  \log \left| f(re^{it}) \right| dt
$$ is an increasing function. How can I conclude?","['complex-analysis', 'blaschke-products', 'harmonic-functions']"
3672632,What is the most efficient way to find the inverse of large matrix?,"Let $A$ be a large square $(n+1) \times (n+1)$ invertible matrix, where $n \approx 1000$ . $$A = \begin{bmatrix}
-1 & 0 & 0 &\cdots & 0 & a_0\\
1 & -1 & 0 &\cdots & 0 & a_1\\
0 & 1 & -1 &\cdots & 0 & a_2\\
\vdots & \vdots & \vdots &\ddots & \vdots & \vdots\\
0 & 0 & 0 &\cdots & -1 & a_{n-1}\\
0 & 0 & 0 &\cdots & 1 & a_n-1\\
\end{bmatrix}$$ What is the most efficient way to find its inverse or solve its linear equation? Matrix $A$ is the result of a subtraction of a matrix with the identity matrix. I try to solve this to find the result of the series of a matrix and apparently Gaussian elimination method was not efficient enough.","['systems-of-equations', 'matrices', 'linear-algebra', 'inverse', 'numerical-linear-algebra']"
3672666,"Does $f$ have a critical point if $f(x, y) \to +\infty$ on all horizontal lines and $f(x, y) \to -\infty$ on all vertical lines?","Given a function $f:\Bbb R^2 \to \Bbb R, f \in C^\infty$ with the property that $$\lim_{x\to+\infty}f(x,y_0) = \lim_{x\to-\infty}f(x,y_0) = +\infty \qquad \forall y_0\in \Bbb R, \\[2ex]
\lim_{y\to+\infty}f(x_0,y) = \lim_{y\to-\infty}f(x_0,y) = -\infty \qquad \forall x_0\in \Bbb R.$$ Determine whether $f(x,y)$ necessarily has at least one critic point. My attempt: I suppose that such a function could be something just like: $(x^{2n}-y^{2m})$ ; anyway what I mean is that both $f(x,y_0)$ and $f(x_0,y)$ have to assume eventually the shape of a sort of ""parabola"". Because of $f\in C^{\infty}$ then both $f(x,y_0)$ and $f(x_0,y)$ are continuous, thus: 1. $f(x,y_0)=g(x),$ has a global minimum and it means: $\forall y_0 \in \Bbb R$ there is at least one $x^*$ such that $f_x(x^*,y_0)=0;$ 2. $f(x_0,y)=h(y),$ has a global maximum and it means: $\forall x_0 \in \Bbb R$ there is at least one $y^*$ such that $f_y(x_0,y^*)=0.$ If my attempt is correct until now, the last thing I need to do is observe that there is at least a couple $(x^*,y^*)$ such that $f_x(x^*,y^*)=0$ and $f_y(x^*,y^*)=0$ . This last step is the one I stuck in. Is there someone who can handle this? (or can propose another path to follow)","['examples-counterexamples', 'real-analysis', 'maxima-minima', 'multivariable-calculus', 'calculus']"
3672690,"Integral $ \int_E \frac{1}{x^ay^b}dxdy \qquad E={x>0,y>0,xy \geq 1}$","I'm asked to compute: $$ \int_E \frac{1}{x^ay^b}dxdy \qquad E={x>0,y>0,xy \geq} 1$$ but I'm really finding difficulties in doing it as the domain is unlimited. I can pretty much solve this kind of integrals when the domain is limited and there is a problem in one or more than one points, but how can I compute this in an efficient way?
I am also a little bit concerned about polar coordinates: I think that it could be a good way to do it, but people here, on other questions, told me that I should avoid doing it to prevent other issues. 
Can you please help me understanding how to proceed in problems like this one?
Thanks in advance. EDIT We notice that we can express the domain as $E=\{x>0, y>\frac{1}{x}\}$ so we have: $$\int_E \frac{1}{x^ay^b}dxdy= \int_0^{+\infty}\left(\int_{\frac{1}{x}}^{+\infty}\frac{1}{x^ay^b}dy\right)dx=\int_0^{+\infty}\frac{1}{x^a}\left[ \frac{1}{(1-b)y^{b-1}} \right]_{\frac{1}{x}}^{\infty}dx$$ Let's suppose $b>1$ (otherwise the integral diverges): $$\frac{-1}{b-1}\int_0^{+\infty} \frac{1}{x^a}x^{b-1}dx=\frac{-1}{b-1}\int_0^{+\infty} \frac{1}{x^{a-b+1}}dx$$ But now we have something that diverges for all values of $a,b$ as it has both problems in $0$ and at $\infty$ . Is this correct?","['integration', 'improper-integrals', 'calculus', 'multivariable-calculus', 'limits']"
3672716,Find the rank of $T^2$,"Question: Let $\mathbb{C}^{11}$ is a vector space over $\mathbb{C}$ and $T:\mathbb{C}^{11}\to \mathbb{C}^{11}$ is a linear transformation. If dimension of Kernel $T=4$ , dimension of Kernel $T^3=9$ and dimension of Kernel $T^4=11$ . Then the dimension of Kernel $T^2=$ ............ Since $T$ is a linear operator, $T^2, T^3,T^4$ will also be linear operators and there will be matrices associated with these linear operators, say $[T]$ represents the matrix related to the linear operator $T$ . By rank-nullity theorem, we get $rank(T)+nullity(T)=dim(\mathbb{C}^{11})=11$ . So, rank $(T)=7$ and similarly,  we  can get rank $(T^3)=2$ and rank $(T^4)=0$ . Therefore, $T$ is nilpotent. Again by rank-nullity theorem, $nullity(T^2)=dim(\mathbb{C}^{11})-rank(T^2)=11-rank(T^2)$ . Now the main problem is reduced to find the rank of $T^2$ . We know that $T$ is nilpotent. Now, let $B_{11 \times 11}=T^2$ and $B^2=T^4=0$ , then the rank of $B$ can be found using this fact Matrix algebra: If $A^2=0$, Proof rank(A) $\le \frac{n}{2}$ . We get, rank $(T^2)\leq \frac{11}{2}$ . In this way we can tell possibilities of the dimension of Kernel of $T^2$ . Can we only find the possibilities not the exact rank of $T^2$ with the given data?","['matrices', 'matrix-rank', 'linear-algebra', 'linear-transformations']"
3672720,Extreme points of $f(t)=h(t)\cos(t)+g(t)\sin(t)$,"Let $f:[0,12]\to R$ , $f(x)=3x\cos(x)+2x^3\sin(x)$ and I have to find $\min(f(x))$ and $\max(f(x))$ . I have tried the derivative  and some other things like observing the minimum is between $3\pi$ and $\left(3+\frac12\right)\pi$ but cant find the exact value. I have tried to see if any formula for $a\cos(t)+b\sin(t)$ where $a$ , $b$ are fixed constants may work here but it's not the case (I tried Cauchy–Bunyakovsky–Schwarz inequality and the so called R-method) PS: first post here so excuse me if i break the rules in any way.","['calculus', 'trigonometry']"
3672722,Sign of $df_x$ is locally constant,"This question is about the book Topology from the Differentiable Viewpoint of Milnor. Let $M$ and $N$ be oriented $n$ -manifolds without boundary, and assume $M$ is compact and $N$ is connected. Let $x\in M$ be a regular point of $f$ , so that $df_X:TM_x\to TN_{f(x)}$ is a vector space isomorphism. Define the sign of $df_x$ to be $+1$ or $-1$ according as $df_x$ preserves or reverses orientation. How can we show that the sign of $df_x$ is locally constant function of $x$ ? Since $M$ is oriented, $x$ has a neighborhood $U$ and a diffeomorphism $h$ of $U$ onto an open subset $V$ of $\Bbb R^n$ which is orientation preserving, in the sense that for each $y\in U$ the isomorphism $dh_y$ carries the specified orientation of $TM_y$ to the standard orientation of $\Bbb R^n=TV_{h(y)}$ . I think I should use this fact, but I can't see how does this imply that the sign of $df_x$ is constant in a neighborhood of $x$ .","['smooth-functions', 'smooth-manifolds', 'orientation', 'differential-topology', 'differential-geometry']"
3672746,Find global median from medians of subgroups,"Supposte I have a list of numbers $\{x_1, \ldots, x_N\}$ , not necessarily ordered, and I divide it in subsets $\{ \{x_1,\ldots,x_{d_1}\}, \{x_{d_1+1},\ldots,x_{d_2}\}, \ldots \}$ , where $d_n$ is the number of elements in each subset, then the mean over the entire set $\langle x\rangle$ is the weighted mean of the mean in each subset: $$
\langle x \rangle = \sum_j \langle x_j\rangle \frac{d_j}{N}
$$ where $d_j$ is the number of elements in each subset and the sum is over the subsets. If I want to compute the median of the entire set and I know the median in each subset, does an analogous formula exists?","['descriptive-statistics', 'statistics', 'median']"
3672766,"For random variables $X,Y\in L^1$, if $E[X|\sigma(Y)]=Y$ and $E[Y|\sigma(X)]=X$, prove that $X=Y$ a.s.","For random variables $X,Y\in L^1$ , if $E[X|\sigma(Y)]=Y$ and $E[Y|\sigma(X)]=X$ , prove that $X=Y$ a.s. My attempt: Note that $E[X-Y|\sigma(Y)]=E[X|\sigma(Y)]-E[Y|\sigma(Y)]=Y-Y=0$ . Similarly, $E[X-Y|\sigma(X)]=0$ . By 1, if I could prove the following lemma, then $E[X-Y|\sigma(X,Y)]=X-Y=0$ as desired. Lemma (not necessarily true). If $E[Z|\mathcal A]=E[Z|\mathcal B]=0$ where $\mathcal A$ and $\mathcal B$ are two $\sigma$ -fields, then $E[Z|\sigma(\mathcal A,\mathcal B)]=0$ . Questions: Is this ""lemma"" correct? Is there a counterexample? If not, how should I solve the original question?","['conditional-expectation', 'probability-theory']"
3672859,Determining the period of $\sin 2x +\sin\frac{x}{2}$ without using the LCM of periods,"I tried to calculate period of function described as: $$y=\sin 2x +\sin\frac{x}{2}$$ but without using LCM of periods. From definition of periodic function we have: $$\begin{align}
0 &= \sin(2x+2T)+\sin\left(\frac{x+t}{2}\right)-\sin2x-\sin\left(\frac{x}{2}\right) \\[4pt]
&=2\sin\left(\frac{5x+5T}{4}\right)\cos\left(\frac{x+T}{4}\right)-2\sin\left(\frac{5x}{4}\right)\cos\left(\frac{3x}{4}\right)
\end{align}$$ I cannot do further more.","['periodic-functions', 'trigonometry', 'functions', 'analysis']"
3672866,What is correct ranking for Spearman Correlation?,"In order to calculate Spearman Correlation Coefficient, the data should be ranked. However, many people do this in different way. Some sort them like an increasing sequence (i.e the smallest number has rank 1 and the greatest has rank $n$ ), others do this in an opposite way, they give the highest rank to the smallest number and rank 1 to the greatest. Can you suggest what is the most appropriate way to do that?","['statistics', 'correlation']"
3672932,Matrices with $M\binom ab\not<\binom 11$,"Let $Q:=\{(x,y)\colon\max\{x,y\}<1\}$ and $Q_0:=\{(x,y)\colon\max\{x,y\}\le1\}$ . Also, let $\Gamma:=\mathbb N^2$ . Is there any comprehensible description of the set of all real square matrices $M$ of order $2$ such that $M\binom{1}{0}\in Q_0$ , $M\binom{0}{1}\in Q_0$ , and $M\Gamma$ is disjoint from $Q$ , with the possible exception of the vector $M\binom{1}{1}$ which can be in $Q$ ? As an example, $M$ has this property given that each of the two column sums of $M$ is $2/3$ at least. Another example: $$ M=\begin{pmatrix} 1 & -1 \\ -1/2 & 1 \end{pmatrix}. $$ In contrast, if all elements of $M$ are non-positive, then $M$ does not have the property in question.","['matrices', 'linear-algebra', 'combinatorics', 'linear-transformations', 'inequality']"
3672961,Expressing the sign pattern $+--++--+\cdots$ in a series [duplicate],"This question already has answers here : How do you create an alternating series with the sign being the same twice in a row? (7 answers) Closed 4 years ago . I had to find the Taylor series for the function $f(x)=\cos(x)$ centred at $a=\frac{\pi}{4}$ . I found the pattern but the only part I'm missing is the sign. Since the series is centred at $\frac{\pi}{4}$ , no value of $f^{\{n\}}(a)$ is equal to zero, and the pattern is $+, -, -, +, +, -, -, +$ . I have checked the answer that my teacher put in the document, but he just wrote $$f(x) = \sum_{n=0}^{\infty}{\text{sign} \frac{\sqrt{2}}{2(n!)}}\left(x-\frac{\pi}{4}\right)^n$$ here $\text{sign} =+--++--++--+ \cdots$ which I find rather disappointing... Is there a mathematical way to insert the sign pattern into the series, similarly to $(-1)^n$ for a normal alternating series?","['power-series', 'trigonometry', 'taylor-expansion', 'sequences-and-series']"
3672966,How to get the characteristic polynomial of this matrix?,"Consider a $n\times n$ matrix: $$
M_n = \begin{pmatrix}
a_1 & 1 & 0 & 0 & 0 & \cdots & 1 \\
1 & a_2 & 1 & 0 & 0 & \cdots & 0 \\
0 & 1 & a_3 & 1 & 0 & \cdots & 0 \\
\vdots & \vdots& \vdots& \vdots & \vdots& \vdots & \vdots \\
0 & \cdots & \cdots & 0 & 1 & a_{n-1} & 1 \\
1 & \cdots & \cdots & \cdots & \cdots & 1 & a_n
\end{pmatrix}
$$ where $a_k=2\cos(k\phi)+2\mathrm{i}\gamma\sin(k\phi)$ , with $\phi=2\pi/n$ and $0<\gamma<1$ . $~n\ge5$ , and can be assumed to be prime numbers if necessary. How to get its characteristic polynomial $P_n(x)=\det(M_n-xI)$ ? $x^n$ , $x^{n-2}$ and $x^0$ terms are easy to get, can you get other terms?","['matrices', 'trace', 'determinant', 'characteristic-polynomial']"
3673020,Example of a non compactly generated complete locally convex topological vector space over $ \mathbb{R}$,"I am looking for an example of a non compactly generated complete locally convex topological vector space over $\mathbb{R}$ . Being familiar with the fact that every complete locally convex topological vector space over $\mathbb{R}$ is a cofiltered limit of banachspaces (see https://www.math.utah.edu/~taylor/LCS.pdf Prop. 2.5) I tried to use the fact that the category of compactly generated spaces (with continuous maps as morphisms) is complete. This seems to fail because then it would be necessary that a cofiltered limit of banach spaces (in the category of topological vector spaces) is also a cofiltered limit of the underlying compactly generated spaces (in the category of compactly generated spaces). 
Therefore (and also because of this question https://mathoverflow.net/questions/52734/on-locally-convex-and-compactly-generated-topological-vector-spaces ) I am assuming that there exists such an example but for me it is already hard enough to find a non compactly generated space besides the example on the wikipedia site. Also I need to clarify that in this context complete means that every cauchy net converges uniquely implying that the space is hausdorff.","['topological-vector-spaces', 'examples-counterexamples', 'category-theory', 'functional-analysis', 'general-topology']"
3673107,"$\mathbf{A}^T = p(\mathbf{A})$, prove that $\mathbf{A}$ is invertible","Let $\mathbf{A}$ be a square matrix defined over a field $\mathbb{R}$ . It is known that $\mathbf{A}^\text{T} = p(\mathbf{A})$ , where $p(\mathbf{A})$ is a polynomial with a constant coefficient $a_0 \neq 0$ . Prove that $\mathbf{A}$ is invertible. Is it true that for every operator $\mathbb{\phi}: \mathbb{R}^n\rightarrow\mathbb{R}^n$ there exists some polynomial $p(x)$ and the basis for which the matrix $\phi$ satisfies the condition of $\mathbf{A}^\text{T} = p(\mathbf{A})$ ? Solution I. By the definition of $\mathbf{A}$ : $$\mathbf{A}^t = P(\mathbf{A}) = a_n \cdot \mathbf{A}^n + \cdots + a_1 \cdot \mathbf{A} + a_0 \cdot \mathbf{I}$$ Consider $$\mathbf{A}^t \cdot \mathbf{A} = a_n \cdot \mathbf{A}^n \cdot \mathbf{A} + \cdots + a_1 \cdot \mathbf{A} \cdot \mathbf{A} + a_0 \cdot \mathbf{I} \cdot \mathbf{A} \\ = a_n \cdot \mathbf{A}^{n + 1} + \cdots + a_1 \cdot \mathbf{A}^2 + a_0 \cdot \mathbf{A}$$ $$\mathbf{A} \cdot \mathbf{A}^t = a_n \cdot \mathbf{A} \cdot \mathbf{A}^n + \cdots + a_1 \cdot \mathbf{A} \cdot \mathbf{A} + a_0 \cdot \mathbf{A} \cdot I \\ = a_n \cdot \mathbf{A}^{n + 1} + \cdots + a_1 \cdot \mathbf{A}^2 + a_0 \cdot \mathbf{A}$$ hence, $\mathbf{A}^t \cdot \mathbf{A} = \mathbf{A} \cdot \mathbf{A}^t$ . Therefore, the matrix $\mathbf{A}$ is normal, which assumes $\text{ker}(\mathbf{A}) = \text{ker}(\mathbf{A^t})$ . Assume that $\mathbf{A}$ is not invertible, then its kernel is non-trivial. Take non-zero $v$ from $\text{ker}(\mathbf{A})$ : $$\mathbf{A}^t \cdot v = a_n \cdot \mathbf{A}^n \cdot v + \cdots + a_1 \cdot \mathbf{A} \cdot v + a_0 \cdot \mathbf{I} \cdot v \\ \mathbf{A}^t \cdot v = 0 + \cdots + 0 + a_0 \cdot v = a_0 \cdot v$$ As per definition, $a_0 \neq 0$ , so $a_0 \cdot v \neq 0$ , and it follows that $\mathbf{A}^t \cdot v \neq 0$ . However, the condition $\text{ker}(\mathbf{A})=\text{ker}(\mathbf{A^t})$ implies that $\mathbf{A}^t \cdot v = 0$ . We have a contradiction, hence $\mathbf{A}$ is invertible. II. It was proven that $\mathbf{A}$ is normal if its transpose can be represented by a polynomial. But normality is independent of the basis chosen, so the second statement would mean that every operator is normal, which is not the case. Any mistakes, improvements?","['matrices', 'solution-verification', 'linear-algebra']"
3673172,Find minimum value of $\sum \frac{\sqrt{a}}{\sqrt{b}+\sqrt{c}-\sqrt{a}}$,"If $a,b,c$ are sides of triangle Find Minimum value of $$S=\sum \frac{\sqrt{a}}{\sqrt{b}+\sqrt{c}-\sqrt{a}}$$ My Try: Let $$P=\sqrt{a}+\sqrt{b}+\sqrt{c}$$ we have $$S=\sum \frac{1}{\frac{\sqrt{b}}{\sqrt{a}}+\frac{\sqrt{c}}{\sqrt{a}}-1}$$ $$S=\sum \frac{1}{\frac{P}{\sqrt{a}}-2}$$ Let $x=\frac{P}{\sqrt{a}}$, $y=\frac{P}{\sqrt{b}}$,$z=\frac{P}{\sqrt{c}}$ Then we have $$\frac{1}{x}+\frac{1}{y}+\frac{1}{z}=1$$ By $AM \ge HM$ $$\frac{x+y+z}{3} \ge \frac{3}{\frac{1}{x}+\frac{1}{y}+\frac{1}{z}}$$ Hence $$x+y+z \ge 9$$ Any way to proceed further?","['jensen-inequality', 'rearrangement-inequality', 'cauchy-schwarz-inequality', 'geometric-inequalities', 'inequality']"
3673227,Does $\lim_{N\rightarrow \infty} \sum_{n = 1}^{N} \frac{1}{(N+1) \ln (N+1) - n \ln n } = 1$?,"Question: Find the limit \begin{equation}
A = \lim_{N\rightarrow \infty} \sum_{n = 1}^{N} \frac{1}{(N+1) \ln (N+1) - n \ln n }
\end{equation} The series originated from the asymptotic analysis in this question . I can show that it converges. Numerical evaluation in Mathematica suggests that it is close to $1$ . I am just curious, is it possible to prove one of the following? $A > 1$ $A = 1$ $A < 1$ Perhaps one can think about the integral \begin{equation}
\int_0^1 \frac{1}{( 1 + \frac{1}{N} ) \ln (N+1) - x \ln (x N) } dx 
\end{equation} Update 1 : Let $A_N = \lim_{N\rightarrow \infty} \sum_{n = 1}^{N} \frac{1}{(N+1) \ln (N+1) - n \ln n }$ , numerical evaluations up to $10,000$ shows $A_N < 1$ We can also plot the difference $1 - A_N$ as a function of $N$ . To check the estimation $1- A_N \sim 0.3 / \ln N $ by one of the comments, we plot $( 1- A_N) \ln N$ update 2 Following Crostul's answer, I did an exercise to prove that the integral is also equal to $1$ in the limit $N \rightarrow \infty$ Relation between $A_N$ and the integral: \begin{equation}
  A_N = \sum_{n=1}^{N} \frac{1}{N} \frac{1}{ (1 + \frac{1}{N}) \ln ( N+ 1 ) - \frac{n}{N} \ln( \frac{n}{N} N ) }
\end{equation} so to find $A_{N\rightarrow \infty}$ , we may look at the limit \begin{equation}
\lim_{N \rightarrow  \infty} \int_0^1 \frac{1}{( 1 + \frac{1}{N} ) \ln (N+1) - x \ln (x N) } dx 
\end{equation} We can simplify the denominator \begin{equation}
( 1 + \frac{1}{N}  ) \ln ( N+1 ) - x \ln ( xN ) = \ln N ( 1 + \frac{b_N}{N} -x  - \frac{x \ln x}{\ln N} )
\end{equation} where \begin{equation}
b_N = 1 +  ( N + 1 )\frac{\ln (1 + \frac{1}{N})}{\ln N  } = 1 + \frac{1}{\ln N} + o(\frac{1}{N} ) 
\end{equation} So we study \begin{equation}
 I_N = \frac{1}{\ln N} \int_0^1 \frac{1}{ 1 + \frac{b_N}{N} - x - \frac{x \ln x}{\ln N} } dx 
\end{equation} Now use Crostul's relaxing trick Define $f(x) = x + \frac{x \ln x}{ \ln N}$ , the denominator is $f(1 + \frac{1}{N} ) - f(x)$ . On one hand we have \begin{equation}
f(1 + \frac{1}{N} ) - f(x) \ge  f(1 + \frac{1}{N} ) - x \ge 1 + \frac{1}{N}  - x 
\end{equation} where the second inequality holds for large enough $N$ . On the other hand, by mean value theorem \begin{equation}
  \frac{f( 1 + \frac{1}{N} ) - f(x) }{1 + \frac{1}{N} - x} = f'( y) \le f'( 1 + \frac{1}{N} ) = 1 + \frac{1 + \ln (1 + \frac{1}{N})}{ \ln N }
\end{equation} Hence \begin{equation}
 \frac{1}{1 + \frac{1 + \ln (1 + \frac{1}{N})}{ \ln N }} \int_0^1 \frac{1}{1 + \frac{1}{N } - x } dx  \le     I_N  \ln N \le \int_0^1 \frac{1}{1 + \frac{1}{N } - x } dx 
\end{equation} Both integrals on the left and right are $\ln N$ . Therefore \begin{equation}
 \frac{1}{1 + \frac{1 + \ln (1 + \frac{1}{N})}{ \ln N }}  \le I_N \le 1 
\end{equation} Taking the limit $N\rightarrow$ , we have $\lim_{N\rightarrow \infty } I_N = 1$ .","['limits', 'definite-integrals', 'sequences-and-series']"
3673257,How to get a linear map with specified kernel and image,"This is more of an abstract question. Let $U, W$ be subspaces of a vector space $V$ . I’m thinking about how one might get these subspaces to “interact” or be “paired up” in some way. The natural thought I had was to find a linear map $T$ from $V$ to itself so that $\ker T = U$ and $\text{im} \, T = W$ . Then the rank-nullity theorem implies that we need the dimensions of $U$ and $W$ to add up to $\dim V$ . So this is a constraint. One reason I was considering such a map is that, by the First Isomorphism Theorem, $V/U \cong W$ , so we can think of $T$ as partitioning $V$ into identical copies/pieces of $U$ , and each of these pieces is uniquely associated with a vector in $W$ . So in a sense $U$ is sent into every point of $W$ . How might we find such a linear map? Does it matter whether or not $U, W$ intersect at a point other than zero, i.e. does it matter whether $U + W$ is a direct sum?","['abstract-algebra', 'linear-algebra']"
3673285,"Principal generators of prime ideals is $\mathbb{Q}[\sqrt{m}]$ for m=-1, -2, -3","I'm reading Marcus ""Number fields"" book and at a certain point (page 52) in the chapter about prime decomposition he writes We now consider in detail the way in which primes p $\in \mathbb{Z}$ split in quadratic fields. Let $R=A \cap \mathbb{Q}[\sqrt{m}]$ , m squarefree. Recall that R has integral basis $\{1,
\sqrt{m}\}$ and
  discriminant 4m when $m\equiv 2\; or\; 3\; (mod\; 4)$ , and integral basis $\{1,\frac{1+\sqrt{m}}{2}\}$ and
  discriminant m when $m\equiv 1\; (mod\; 4)$ . Let p be a prime in $\mathbb{Z}$ . Theorem 21 shows that there are just three possibilities: $$ pR=\begin{cases}
P^2&\Leftarrow f(P|p)=1\\
P&\Leftarrow f(P|p)=2\\
P_1P_2 &\Leftarrow f(P_1|p)=f(P_2|p)=1.
\end{cases}$$ Theorem 25 With notation as above, we have: If p | m, then $$ pR=(p,\sqrt
{m})^2.$$ If m is odd, then $$ 2R= \begin{cases} (2,1+\sqrt
{m})^2&\text{if $m\equiv 3\pmod4$}\\
\left(2,\frac{1+\sqrt{m}}{2}\right)\left(2,\frac{1-\sqrt{m}}{2}\right) & 
\text{if $m\equiv 1\pmod8$}\\
\text{prime if $m\equiv 5\pmod8$.}
\end{cases}$$ If p is odd, $p\not| m$ then $$ pR=\begin{cases} (p,n+\sqrt{m})(p,n-\sqrt{m})\; \text{if $m\equiv n^2 \pmod p$}\\
\text{prime if $m$ is not a square mod $p$}
\end{cases}$$ where in all relevant cases the factors are distinct. Proof. I'll skip this. The prime ideals involved in these factorizations do not look like principal ideals,
  but we know in certain cases they must be principal: for example when m = −1, -2
  or −3 (exercises 7 and 14, chapter 1). Can you describe principal generators for the
  various prime ideals in these two cases? Now my problem is that I don't understand what it means in the last question, how do I find the principal ideals, and whose prime ideals is he referring to. Any help in understanding both the question and how to solve it would be welcomed.","['number-theory', 'algebraic-number-theory', 'prime-factorization', 'ideals']"
3673292,complex analysis topology problem,"$\underline{\textbf{Given:}}$ (1) $\;f\,$ is a continuous function. (2) $\;f \,: \mathbb{C} \to \mathbb{C}.$ (3) $\;|f(z)| \to \infty\;$ as $\;|z| \to \infty.$ (4) $\;f(\mathbb{C})\;$ is an open set. $\underline{\textbf{To Prove:}}$ (5) $\;f(\mathbb{C}) = \mathbb{C}.$ $\underline{\textbf{My Request:}}$ I provide context and analysis below.  I welcome any hint, guide, or proof of
the problem.  It will help me if you keep your responses basic (i.e. consistent
with the context). $\underline{\textbf{Context:}}$ I am recreationally self-studying ""An Introduction to Complex Function Theory"" : 
Bruce Palka : 1991.  My query represents the very last exercise from chapter 2
of this book (i.e. exercise 5.37). Chapter 1 of this book focuses on the complex number system and chapter 2 
focuses on the rudiments of plane topology.  Within chapter 2 are definitions,
theorems and (end-of-chapter) exercises that focus on A. disks, open/closed sets, boundary points, sequences, convergence, accumulation points B. continuity, limits C. connected/disconnected sets, domains, components of open sets D. bounded sets and sequences, Cauchy sequences, compact sets, uniform continuity. A set is connected $\;\Leftrightarrow\;$ the set is not disconnected. A set $\,S\,$ is disconnected $\;\Leftrightarrow\; \exists \;$ disjoint open sets $\;U, \,V \;\ni$ $(S \bigcap U) \neq \phi \neq (S \bigcap V)\;$ and $\;S \;\subseteq \;(U \bigcup V).$ $b\,$ is a boundary point of a set $\,S\; \equiv \;$ $\forall \;r > 0, \Delta(0,r)\;$ contains at least one element from $\,S\,$ and one element that is not in $\,S.$ I interpret premise (3) above as: $\forall \;\epsilon > 0 \;\exists \;r > 0 \;\ni \;|z| \geq r \;\Rightarrow \;|f(z)| > \epsilon.$ Please let me know if you disagree with my interpretation. I completed all prior exercises in chapter 2.  The problem (exercise 5.37) provides
a hint: Assume $\;G = f(\mathbb{C}) \neq \mathbb{C}\;$ and use exercise 5.25 from chapter
2 to get a contradiction. Exercise 5.25 : If $\;G, \,D \;$ are domains (i.e. open, connected sets) $\;\ni$ $G \subseteq D\;$ and $\;G \neq D,\;$ then $\;\partial G \;\bigcap D \;\neq \;\phi$ (i.e. $\,D\,$ contains at least one of the boundary points of $\,G$ ). $\underline{\textbf{My Analysis:}}$ First of all, let $\;h(z) = |f(z)|.$ From the theorems in chapter 2, I know that $\;h(z)$ is a continuous function, and that $\;h(\mathbb{C})\;$ is therefore an unbounded, open, and connected set. This means that $\;\forall \;r > 0, \;\exists \;z \,\in \,\mathbb{C} \;\ni
\;|f(z)| = r.$ Second of all, following the hint, and assuming that $\;G \neq \mathbb{C},\;$ $G$ has a boundary point $\,b \;\ni \;b \,\not\in \,G.$ Let $\;r \equiv |b|,\;$ and $\;K(0,r) \equiv \{ \,z \,: \;|z| = r \,\}.$ Let $\;K_1 \equiv G \bigcap K(0,r)\;, K_2 \equiv [K(0,r) \sim K_1].$ From previous exercises in chapter 2, I know that both $\;K(0,r)\;$ and $\;\overline{G}\;$ are connected sets. However, I do not (as of yet) know whether any of $\;\partial G, \;K_1, \;$ or $\;K_2 \;$ are connected
 sets. Further, even if I did know this, I don't see how it would help. Since $\,b\,$ is a boundary point of $\,G,\,$ both $\,G\,$ and $\,(\mathbb{C} \sim G)\,$ contain elements arbitrarily close to $\,b.$ I don't see how this helps either. $\underline{\textbf{Addendum-1 Reaction to Qiyu Wen's answer:}}$ I followed almost all of the answer, but I have some questions. In response to your paragraph:
""I just want to mention a gap in your interpretation: ... so 0 is what we try to capture."" Q1: What does $\;\mathbb{C^*}\;$ refer to? Q2: By ""gap in your interpretation,"" are you referring to my presumption that $\forall \;\epsilon > 0 \;\exists \;r > 0 \;\ni \;|z| \geq r \;\Rightarrow \;|f(z)| > \epsilon$ or are you referring to my analysis that concluded that $\;\forall \;r > 0, \;\exists \;z \,\in \,\mathbb{C} \;\ni \;|f(z)| = r?$ If the former, then how (else) does one interpret $\;|f(z)| \to \infty\;$ as $\;|z| \to \infty?$ If the latter, I know that since $\,f(z)$ is continuous, so is $\,h(z) = |f(z)|.$ Further, I know that since $\,\mathbb{C}\,$ is a connected set, so is $\,h(\mathbb{C}).$ I also know, from premise (3), that $\,h(\mathbb{C})\,$ is unbounded. However, I now realize that I do not immediately know that $\,{0} \,\in \,h(\mathbb{C}).$ Is this what you are referring to? Q3: In your statement ""Since $\,f(C)\,$ is open, there exists an open disk cantered at $\,w$ "", do you intend ...cantered at $\,w_0$ ? Q4: In the paragraph that begins: ""Suppose, for contradiction, that $\,w_0 \neq 0$ ..."" I agree that both $\;f(\mathbb{C})\;$ and $\;h(\mathbb{C})\;$ are open sets, since both $\,f\,$ and $\,h = |f|\,$ are continuous. I need to be clear about your reasoning in this paragraph. Are you saying that since $\;h(\mathbb{C})\;$ is an open set, $\exists \;\delta > 0 \;\ni \;\Delta(|w_0|, \delta) \,\subseteq\,h(\mathbb{C})?$ Q5: At the end of your first paragraph, you say ""...Note that $\,w_0 < M.$ "" Why?  I agree that $\,|f|\,$ has a minimum at some point $\,z_0\,$ on the closed disk $\,D,\,$ but I don't understand why $\,|f(z_0)|\,$ must be $\,< M.$ If I'm right that this is unclear, I think that the analysis is immediately remedied by simply taking any random $\,z_1 \,\in \,\mathbb{C}\,$ and then choosing $\,M > |f(z_1)|.$ What do you think? Q6: If I'm not mistaken, you ignored the hint given by the book. Is there a relation between your approach and the book's hint? $\underline{\textbf{Addendum-2 Reaction to Qiyu Wen's response to my questions:}}$ Way cool! ""...I'm working with $\,f(C),\,$ which I know is open, not $\,h(C).$ "" ""You pick a non-zero complex number, then in any ball about it"" ""you can find another complex number closer to 0..."" It's a critical point. When I first saw your approach, I was blind to the above idea. When you emphasized it, it sunk in.  Thanks ""...I didn't realize there's a hint. See my edited answer."" Again, thanks.  The hint-approach , which is probably the intended answer, is
something that I might eventually have stumbled into.  Re your initial approach,
there's no way that I would have thought of that on my own.","['complex-analysis', 'general-topology']"
3673301,Tricky multivariable limit [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Calculate the limit without using L'Hopital's rule: $\lim_{(x,y) \rightarrow (0,0)} \sqrt{(x^2+y^2)} ~\log|y|$","['limits', 'multivariable-calculus', 'limits-without-lhopital']"
3673335,Fast computation of $\det(A^\top A)$ with $A$ lower triangular,"Let $A \in \mathbb{R}^{m \times n}$ , where $m \geq n$ , be a lower triangular matrix, i.e. $A_{ij} = 0$ whenever $j > i$ . I am interested in computing $\det(A^\top A)$ as efficiently as possible, and would rather avoid having to compute the product $A^\top A$ . In the case where $m=n$ , the problem simplifies as: $$\det(A^\top A) = \det(A^\top) \det(A) = \det(A)^2 = \prod_{i=1}^n A_{ii}^2$$ Is there a similar simplification (or any other) for the case where $m > n$ ? Thanks!","['numerical-linear-algebra', 'linear-algebra']"
3673382,How to solve $x^{\prime\prime\prime} + 2x^{\prime\prime} - x = e^{-t}\cos(2t)$ using the operator method?,"I did $(D^3 + 2D - 1)x = e^{-t}\cos(2t) \Rightarrow x = \frac{1}{D^3 + 2D - 1}e^{-t}\cos(2t) \Rightarrow x= \frac{e^{-t}}{(D-1)^3 + 2(D-1) - 1}\cos(2t) \Rightarrow$ $x =\frac{e^{-t}}{D^3-3D^2+3D-1+2D-2-1}\cos(2t) = \frac{e^{-t}}{D^3-3D^2+5D-4}\cos(2t)$ , but I'm having trouble with $D^3-3D^2+5D-4$ . Am I doing something wrong?",['ordinary-differential-equations']
3673393,contour integration of $\int_0^\infty \frac{\ln(x)}{x^2-1}dx$,"I asked for a problem for few days ago, regarding integration of $$I=\int_0^\infty \frac{\ln(x)}{x^2-1}dx$$ I know that I could do the substitution $x=it$ and do the integral where the denominater is $x^2+1$ and so on... But I tried anyway to create a contour and compute my integral. Down below my contour $C$ can be seen. Let $f(z)=\ln(z)/(z^2+1)$ . I compute my residue and obtain 0. I.e. $\oint_C f(z)dz=0$ . The following are also giving me zero, i.e. $$\begin{align} 
\int_{\gamma_R}f(z)=0\\
\int_{\gamma_r}f(z)=0
\end{align}$$ by the estimation lemma. So now my integral is, $$0=e^{\pi/3i}\int_r^R \frac{\ln|z|+\pi/3i}{z^2-1}dz+e^{2\pi/3i}\int_r^R \frac{\ln|z|+2\pi/3i}{z^2-1}dz$$ I rewrite the above to $$0=(e^{\pi/3i}+e^{2\pi/3i})\int_r^R \frac{\ln(x)}{z^2-1}dz+(\frac{\pi}{3}ie^{\pi i/3}+\frac{2\pi}{3}ie^{2\pi i/3})\int_r^R \frac{1}{z^2+1}dz$$ And calculating the above gives me, $$\int_r^R \frac{\ln(x)}{z^2-1}dz=\frac{\pi^2}{4}+\frac{i\sqrt{3}\pi^2}{36}$$ But if I compute the integral $I$ in Maple, I obtain $I=\frac{\pi^2}{4}$ . My questions: Are my contour plot correct? Are my integrals correct, i.e. arc and residue? If yes to 1. and 2. Can I just drop the imaginary part and just use the real part?","['complex-analysis', 'contour-integration']"
3673395,Event Schedule 12 Teams / 4 Activities / Play Each Activity 3X / Play Each Team Once,Anyone know how to schedule an event where you have 12 teams and 4 different activities. The different activities all have the ability to host two of the same activities simultaneous. The different activities will have each team plays each event 3 times and plays each team once. I added a picture of what I am working with but as you get towards the end I can't solve it. Thanks,"['permutations', 'statistics', 'combinatorics', 'combinations']"
3673426,Why can't we define traces and determinants for non-square matrices?,"I'm reading a book that claims that determinants and traces are only defined for square matrices, but doesn’t really explain why. 
From a calculation standpoint, this seems correct because I wouldn’t really know how to calculate the determinant of something like a column vector (with more than one row). However, the only justification I can think of is that using the meaning of a determinant, we’re trying to calculate the scaling factor of our unit cube, hypercube, whatever, while missing some “dimension” of it. I don’t really know if that makes sense, but it’s like if I were to ask you to find the volume of a box using only the dimensions of one of its faces. Is this a reasonable explanation, or is there a better reason why we can’t calculate the determinant of a non-square matrix ? And as for the trace, does that really just come down to the definition of the trace, or is there something more “meaningful” ?","['matrices', 'linear-algebra']"
3673474,Laplace functional for Poisson Process: $E[e^{-\sum_{n=1}^{\infty}f(W_n)}]= e^{-\lambda\int_0^{\infty}(1-e^{-f(t)})dt}$,Let $W_n$ be the $n$ waiting time of a Poisson process. Proof that $$E[e^{-\sum_{n=1}^{\infty}f(W_n)}]= e^{-\lambda\int_0^{\infty}(1-e^{-f(t)})dt}$$ for a measurable postive function $f$ I really don´t know where to start. The first thing that I tried is: $$E[e^{-\sum_{n=1}^{\infty}f(W_n)}]=E[\prod_{n=1}^{\infty}e^{-f(W_n)}]$$ but I don´t know if the last expression is equivalent to $$\prod_{n=1}^{\infty}E[e^{-f(W_n)}]$$ I would really appreciate any hint or suggestion.,"['expected-value', 'stochastic-processes', 'poisson-process', 'probability-theory', 'probability']"
3673505,"When is $C_{\mathbb{C}}(\Omega)$ dense in $L_{\mathbb{C}}^{2}(\Omega,\mu)$ with respect to the $L^{2}$-norm?","Suppose that $\Omega$ is a topological space and $\mu$ a positive Borel measure on $\Omega$ . What assumptions on $\Omega$ and $\mu$ do we need in order to conclude that $C_{\mathbb{C}}(\Omega)$ (= space of continuous functions from $\Omega$ to $\mathbb{C}$ ) is $L^{2}$ -norm dense in $L_{\mathbb{C}}^{2}(\Omega,\mu)$ ? For example, does $\Omega$ need to be second countable, or does $\mu$ need to be regular? What is the most general setting? Most references I encounter only consider the case where $\mu$ is the Lebesgue measure on some Euclidian space.","['measure-theory', 'hilbert-spaces', 'lp-spaces', 'functional-analysis', 'general-topology']"
3673510,How to show that a prime ideal of height 2 can’t necessarily be generated by 2 elements? (Hartshorne exercise I.1.11) [duplicate],"This question already has answers here : Height and minimal number of generators of an ideal (4 answers) Closed 4 years ago . In Hartshorne section 1.1 he gives a problem (ex 1.11) which says that, Let $Y \subset \mathbb A^3$ be the curve given parametrically by $x=t^3, y=t^4, z=t^5$ . Show that $I(Y)\subset k[x,y,z]=A$ is a prime ideal of height 2 which cannot be generated by two elements. Firstly, my question is that how can I check whether $Y$ is Zariski closed or not. Is it always the case that, a subset given by $x=f(t), y=g(t), z=h(t); f,g,h \in k[t]$ is always Zariski closed ? My attempt was to prove that $A/I(Y) \cong k[t^3,t^4,t^5]$ to show $I(Y)$ is prime. And then I wanted to show the map $ \mathbb A^1 \rightarrow \mathbb A^3$ sending $t \rightarrow (t^3,t^4,t^5)$ is a topological embedding. I am stuck in proving that the map is continuous. After proving this I could have said $Y$ (having proved that it is Zariski closed) is a variety of dimension 1, hence $I(Y)$ has height 2. I don’t know how to determine the least number of generators. Any help from basic algebraic results would be helpful. I am a beginner in algebraic geometry so I don’t know many things, please keep it as basic as possible.","['affine-varieties', 'algebraic-geometry', 'commutative-algebra']"
3673511,looking for 8 digit numbers with 4 digits being used twice,I'm looking for $8$ digit numbers with $4$ digits being used twice. for example : $11223344$ and $12123434$ and $11002233$ Its not allowed to use one digit like: $11223345$ for $4$ digit numbers with $2$ digits being used twice I have computed $243$ numbers. I need to find out how many 8-digit numbers satisfy afore-mentioned conditon,['analysis']
3673518,Value of $\lim\limits_{n\rightarrow\infty}\prod\limits_{k=3}^n\left(1-\tan^4\frac{\pi}{2^k}\right)=\frac{\pi^3}{m}$,Find the value of m for the following $$\lim\limits_{n\rightarrow\infty}\prod_{k=3}^n\left(1-\tan^4\frac{\pi}{2^k}\right)=\frac{\pi^3}{m}$$,"['trigonometry', 'summation']"
3673543,Compatibility conditions for Maurer-Cartan forms on a homogeneous space,"I am reading Maurer-Cartan forms on a homogeneous space and am  unable to show that $\theta_V=Ad(h_{UV}^{-1})\theta_U+(h_{UV})^*\omega_H$ . Notation : We are considering $G \to G/H$ as a homogeneous H-space, where $G$ is a Lie group, and $H$ is a closed subgroup. $\omega $ is the Maurer-cartan form. For intersecting open sets $U$ and $V$ , we have sections $s_U : U \to G$ and $s_V : V \to G/H$ . Further $\theta_U=s_U^*\omega$ and $\theta_V=s_V^*\omega$ ; on the overlap $U \cap V$ , we have $h_{UV}=s_V \circ s_U^{-1}$ . Here is my attempt :  Let $X\in T_x (G/H)$ . We need to show that $\theta_V(X)=Ad(h_{UV}^{-1})\theta_U(X)+(h_{UV})^*\omega_H(X)$ . Let $c(t)$ be a curve starting at $x \in G/H$ i.e. $c(0)=x$ with tangent vector $c'(0)=X$ . Then $\theta_V(X)= (s_V^*(\omega))(X)=\omega((s_V)_*(X))=\omega((h_{UV}\circ s_U)_*(X))=\omega(\frac{d}{dt}|_{t=0}(h_{UV}(c(t))\cdot s_U(c(t)))$ . But am unable to further simplify this to obtain R.H.S. Kindly help. Thanks a lot !",['differential-geometry']
3673597,Example of a compact topological space $M$ such that $\mathcal M_1(M)$ is not compact.,"It is well known that if $(M,\tau)$ is a compact Hausdorff topological space then (by Riesz–Markov–Kakutani representation theorem + Banach–Alaoglu theorem ) we have that the space $$\mathcal M_1(M) :=\left\{\mu;\ \mu\ \text{is a }\tau\text{-Borel probability measure on }M\right\} $$ is compact in the weak $^*$ topology, $\textit{i.e}.$ the topology generated by the basis of neighborhoods $$V(\mu;f_1,\ldots,f_n;\varepsilon):=\left\{\lambda\in\mathcal M_1(M);\ \left|\int f_i\  \mathrm{d}\mu - \int f_i\  \mathrm{d}\lambda\right|<\varepsilon, \ \forall \ i\in\{1,\ldots,n\}\right\}, $$ where $f_1,\ldots,f_n \in C^0_b(M)=\{g: M\to\mathbb R; \ g \text{ is a continuous bounded function}\}$ . My question: Is there an example of a compact topological space $M$ , such that $\mathcal M_1(M)$ is not compact in the weak ${^*}$ topology? I have searched online but I was not able to find any reference for this problem. Moreover, I have tried to construct a counter-example but I have failed miserably. Can anyone help me?","['measure-theory', 'ergodic-theory', 'functional-analysis', 'probability']"
3673599,If $C_1$ has a radius of $10cm$ then show that the sum of the areas of all these circles is $\frac{25π}{3\sqrt2-4}cm^2$.,"QUESTION: Let { $C_n$ } be an infinite sequence of circles lying in the positive quadrant of the $x$ - $y$ plane with strictly decreasing radii and satisfying the following conditions. Each $C_n$ touches both $x$ and $y$ axis. Further for all $n≥1$ the circle $C_{n+1}$ touches the circle $C_n$ externally. If $C_1$ has a radius of $10cm$ then show that the sum of the areas of all these circles is $\frac{25π}{3\sqrt2-4}cm^2$ . MY APPROACH: Since circles have decreasing radii it's quite clear that the sum of areas will be finite. Area of $C_1=100π$ . The I tried to find out the area of the second circle. Let radius of $C_2=r$ . Then, $2(10-r)^2=(10+r)^2$ , since distance between centres=sum of their radii. Solving the above I find that $r=30-20√2$ . Now I thought I would get some relation in areas from which I can convert this sum into an infinite GP. But I cannot solve it.. Any help is much appreciated.
Thank you.","['coordinate-systems', 'circles', 'geometry', 'sequences-and-series']"
3673658,Why is the pullback (between affine varieties) of a quasi coherent sheaf quasi coherent?,"Let $\phi:A\to B$ be a ring homomorphism inducing $f :\operatorname{Spec}(B) \to \operatorname{Spec}(A)$ on spectra. Let $M$ be an $A$ -module and $\widetilde{M}$ be the corresponding quasi coherent sheaf on $\operatorname{Spec}(A)$ . I define the pullback of an $\mathcal{O}_A$ -module $\mathcal{F}$ to be $$f^{*} \mathcal{F} = f^{-1}\mathcal{F} \otimes_{f^{-1}\mathcal{O}_A} \mathcal{O}_B$$ I want to show that $f^*\widetilde{M} \simeq \widetilde{M\otimes_A B}$ . In Hartshorne it is said that this follows directly from definitions, however, the definition of the pullback involves taking limits, sheafifying, taking the tensor product and sheafifying again, so opening all of that doesn't seem so simple. I know that the stalks of these two sheaves at $q\triangleleft B$ are equal to $M_{\phi^{-1}q} \otimes_{A_{\phi^{-1}q}} B_q$ , since all of the relevant operations act nicely on stalks, so I am missing a map between the two sheaves that would induce identity on the stalks. So I want to define an $f^{-1}\mathcal{O}_A$ - bilinear map $f^{-1}\widetilde{M}\times\mathcal{O}_B \to \widetilde{M\otimes_A B}$ , that it suffices to define on distinguished $D(g)\subset \operatorname{Spec}(B)$ , but I don't see how to express $(f^{-1}\widetilde{M})(D(g))$ in a reasonable way, and in any case, I think that there should be a very simple proof because Hartshorne says this follows directly from definitions. I just don't have a good intuition on what is going on here.","['quasicoherent-sheaves', 'algebraic-geometry', 'sheaf-theory', 'pullback', 'schemes']"
3673735,"Let $f:[a,b] \rightarrow [a,b]$ be a continuous function. Define $p_0 = p \in [a,b]$ and $p_{n+1} = f(p_n)$ prove that","Let $f:[a,b] \rightarrow [a,b]$ be a continuous function. Define $p_0 = p \in [a,b]$ and $p_{n+1} = f(p_n)$ and let the set $K_p := \{p_n, n\geq 0\}$ be a closed set. Prove that $K_p$ is finite. My attempt: Either all elements of $K_p$ are isolated which would mean we could build a monotone sequence $a_n$ with all elements of $K_p$ . $a_n$ is clearly bounded as it lies in $[a,b]$ so it must converge to a limit $L$ . Because $K_p$ is closed $L \in K_p$ but a monotone sequence can only converge to a member of itself if $a_n = L, \forall n \geq n_0$ which would mean $K_p$ is finite. Or $K_p$ has at least one acumulation point $x_0$ . So we can take a sequence of $K_p$ converging to $x_0$ this would mean $\lim_{n \rightarrow \infty} a_n = x_0$ but because $f$ is continuous $\lim_{n \rightarrow \infty} f(a_n) = f(x_0) \iff \lim_{n \rightarrow \infty} a_{""n+1""} = f(x_0) = x_0$ .
So all acumulation points of $K_p$ are fixed points but I can't finish the proof (that this would mean $K_p$ is finite). Actually this last paragraph got a little messy.","['continuity', 'general-topology', 'functions', 'real-analysis']"
3673778,Spectral norm of projected matrix,"Let $M_{n,m}$ be the set of real matrices of $n\times m$ , and let $T:M_{n,m}\to M_{n,m}$ be a orthogonal projection operator, i.e., $T$ is such that for any $A,B\in M_{n,m}$ $$T(A+B)=T(A)+T(B),$$ $$T(T(A))=T(A),$$ $$\langle T(A),B\rangle = \langle A,T(B)\rangle.$$ where $\langle A,B\rangle=tr(A^{\top}B)$ . For $A\in M_{n,m}$ let $\|A\|$ be its spectral norm and $\|A\|_F$ its Frobenius norm. I want to prove that $$
\|T(A)\|\leq \|A\|.
$$ I've been able to prove that $\|T(A)\|_F\leq \|A\|_F$ which is immediate since $\langle T(A),(I-T)(A)\rangle=0$ which implies that $\|A\|_F=\|T(A)+(I-T)(A)\|_F=\|T(A)\|_F+\|(I-T)(A)\|_F$ . For the spectral norm maybe I can use that $\|A\|=\sup_{\|x\|_2=1}\|Ax\|_2$ but I can't prove that $x^{\top}T(A)^{\top}(I-T)(A)x=0$ . Any help will be appreciated. Edit : If it helps, the form of $T$ is $T(A)=A-P_1AP_2$ , where $P_1$ and $P_2$ are some $n\times n$ and $m\times m$ projection matrices. This question is motivated by the paper https://arxiv.org/pdf/1011.6256.pdf , proof of Theorem 1, page 9, inequality after equation (2.15).","['normed-spaces', 'orthogonality', 'matrices', 'linear-algebra', 'matrix-analysis']"
3673803,Prove there is such a triple,"There are $3n$ colored numbers from $1$ to $3n$ such that each color is represented exactly by $n$ numbers. Prove there is 3-colored triple $a,b,c$ such as $$a+b=c$$ I started by assuming various values for minimum each of color, but didnt come to any conclusions(","['contest-math', 'combinatorics']"
3673823,You flip a coin four times. First flip is heads. What are the chances that at least three of the four flips will be heads?,"Wondering if my work is correct. 2^4 = 16 Successful attempts to get at least three heads: HHHH, HHHT, THHH, HTHH, HHTH = 5 5/16 is my answer.  I just wanted to make sure this was correct.  Thank you.","['discrete-mathematics', 'solution-verification', 'combinatorics', 'probability']"
3673828,Pushforward and Pullback Applied to Composition of Maps,"I'm working through Frankel's ""The Geometry of Physics"" this summer, and I'm stuck on a problem concerning the pushforward and pullback operations.  The problem is stated as follows: Let $F:M^n \rightarrow W^r$ and $G:W^r \rightarrow V^s$ be smooth maps. Let x, y, and z be local coordinates near $p \epsilon M$ . $F(p) \epsilon W$ and $G(F(p)) \epsilon V$ , respectively.  We may consider the composite map $G \circ F: M \rightarrow V$ . (i) Show, by using bases $\frac{\partial}{\partial x}$ , $\frac{\partial}{\partial y}$ $\frac{\partial}{\partial z}$ , that $$(G \circ F)_* = G_* \circ F_*$$ (ii) Show, by using bases $dx$ , $dy$ , $dz$ , that $$(G \circ F)^* = G^* \circ F^*$$ So far I've started with the fact that: $$(G \circ F)_* = (G_* \circ F) \cdot F_*$$ by the chain rule, but I'm not sure how to proceed.  I feel like if I can get help with the first one the second one shouldn't be much of an issue. Thank you.","['pullback', 'physics', 'pushforward', 'differential-geometry']"
3673845,Evaluating $\lim_{n\rightarrow\infty} (\frac{(1+\frac{1}{n})^n}{e})^n$,"We know that $\lim_{n\rightarrow\infty}(1+\frac{1}{n})^n=e$ and so I thought the approach to evaluating the limit in the question would be to just use this fact and substitute it into the numerator. This approach would tell us the above limit evaluates to $1$ . However, that does not seem to be the correct limiting value. In fact, it evaluates to $\frac {1}{\sqrt{e}}$ . Why is this so?","['limits', 'real-analysis']"
