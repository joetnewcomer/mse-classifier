question_id,title,body,tags
1725493,Factorials and the Mod function,"I was just playing with the factorial and the modulo function. I just observed this interesting property. I was using a calculator $$13!\equiv 13\times 12\pmod{169}\\ 17!\equiv 17\times 16\pmod{289}$$ It is easily verifiable that this works for $2,3,5,7,11$ also. I conjecture that for any prime $p$,
$$p!\equiv (p)\times (p-1)\pmod{p^2}$$ How does one go about proving it? and by the way is this well known or anything?","['number-theory', 'factorial']"
1725508,Primes of form $a^2 + 24b^2$,"For a prime number $p \neq 2$, $3$, is it necessarily the case the prime number can be written in the form $a^2 + 24b^2$ if and only if $p \equiv 1 \text{ mod }24$? I think this has to be true based upon the facts that $73 = 7^2 + 24(1^2)$, $241 = 5^2 + 24(3^2)$.","['number-theory', 'abstract-algebra', 'prime-numbers', 'algebraic-number-theory']"
1725527,"If $K$ is the algebraic closure of $F$, is $K$ also algebraically closed?","This comes from a question about the fundamental theorem of algebra. Is the algebraic closure of $\mathbb{C}$ implied by the fact that $\mathbb{C}$ is the algebraic closure of $\mathbb{R}$? More generally, if a field $K$ is the algebraic closure of a field $F$, is $K$ algebraically closed? Why or why not? Wikipedia says that $K$ must be closed, but is that a result of definition, or is there a more complex reason?","['abstract-algebra', 'field-theory']"
1725549,A conjecture about traces of projections,"Let $M_n$ denote the space of all $n\times n$ complex matrices. Define $\tau:M_n\rightarrow \mathbb{C}$ by $$\tau(X)=\frac{1}{n}\sum_{i=1}^n x_{ii},$$ where of course $X=[x_{ij}]\in M_n$. Recall that a matrix $P\in M_n$ is called an ""orthogonal projection"" if $P=P^*=P^2$. Let $A, B, C$ be orthogonal projections in $M_n$ and define two quantities - $$x=\frac{1}{3}\tau(A+B+C)$$ and $$y=\frac{1}{3}\tau(AB+BC+CA).$$ It is easy to see that for any orthogonal projections $A,B,C\in M_n$ the value of $x$ lies in $[0,1]$. I want to investigate the case when $x\in [\frac{1}{3},\frac{1}{2}]$. My goal is to compute the infimum of $y$ under this constrain on $x$. My observation is that the minimum value of $y$ is $\frac{3x-1}{4}$ when $x\in [\frac{1}{3},\frac{1}{2}]$ and for all $n\in \mathbb{N}$. However I am unable to prove this and I wondered if people here may be able to see why this might hold, or provide a counterexample to this conjecture.","['matrices', 'trace', 'conjectures', 'operator-theory']"
1725574,Prove the inequality $\frac{\sin(x)}{x}<1$,"Is there a good way to show that $\frac{\sin(x)}{x}$ is bounded above by $1$? We can see visually that $\frac{\sin(x)}{x}$ is bounded above by $1$ because the tallest hump is at the origin and $\lim_{x \to 0} \frac{\sin(x)}{x}=1$. But is there a way to prove this rigorously? Preferably without expanding $\sin(x)$ into a Taylor series, unless Taylor series is the only way.","['real-analysis', 'inequality', 'trigonometry']"
1725593,Which one is the variable? (Derivatives),"I'm very confuzzled as to where the variable is here. I don't know where to differentiate. Here's the question, differentiate: $$y = \frac{\sin(\theta)}{2} + \frac{c}{\theta}$$ Do I solve for $f(c)$ or $f(\theta)$? I tried solving it for $f(c)$ treating theta as a constant, but I'm unsure if that's correct.","['derivatives', 'calculus']"
1725601,Finding the integer solution that makes $\lvert x-y \rvert$ the greatest?,"I was attempting to solve this problem. Let $x,y$ be non-negative integers which satisfy the equation. $$2^{x} +2^{y} = x^{2} +y^{2}$$ Find the maximum possible value for $\lvert x-y \rvert$? At first I tried constructing a function of two variables and finding critical points but the critical points ended up having non-integer values. Then I tried manipulating the equation to get $x^2 +y^2$ to look like $\lvert x-y \rvert$ but that didn't help make it easier to find integer solutions. I ended up trying to guess solutions to the equation and the best I could come up with was $(3,0)$ which gives $\lvert x-y \rvert$ a value of $3$. Only thing is that I'm not sure if this is the highest that you can get the value of $\lvert x-y \rvert$ to be. I was wondering is there any way to solve this problem without guessing and prove that the solution you get is what maximizes $\lvert x-y \rvert$?","['algebra-precalculus', 'optimization']"
1725612,Example of GCD in UFD that can't be expressed as linear combination,"I proved that any two elements in PID have GCD and it can be expressed as linear combination of those two elements.
I know that even in case of UFD GCD exists but it may not be expressed as linear combination. Can anyone give me an example for which GCD is not expressed as linear combination?","['unique-factorization-domains', 'gcd-and-lcm', 'number-theory', 'ring-theory', 'integral-domain']"
1725619,Is $\cot(x)=\frac{1}{\tan(x)}$?,"I just found in some place say that $\cot(x)=\frac{1}{\tan(x)}$. 
If you think about it as a function they are definitely not same. but if you think about as a relation between angle in a right angle triangle and its side they will be equal. 
so the question is its right to say they are equal or not( at least pedagogically)? Update:-
Even wolfram alpha and Desmos say they are not equal ( have different domain).","['algebra-precalculus', 'trigonometry']"
1725661,Morita equivalence between $\mathbb{C}[G]$ and $\mathbb{C}[H]$?,"What we can say about two groups G and H when their group rings, $\mathbb{C}[G]$ and $\mathbb{C}[H]$, are Morita equivelent?","['group-theory', 'group-rings']"
1725718,Labeled balls and labeled boxes at least X in a box,"I am having a problem to solve all the exercises that involve the ""at least $1$ ball need to be in box #2"" kind of problems. For example: there are $8$ numbered cells and we drop $10$ numbered balls into them (each cell can contain unlimited number of balls). What is the probability that cell number $1$ will contain $0$ balls and cell number $2$ will contain at least $1$ ball? What I did is to calculate the probability of cell number $1$ to be empty $(0.263)$ and the probability of cell number $2$ NOT containing $0$ balls $(1 - 0.263)$ and then I multiplied them both getting a result of $0.19$ which is wrong. I searched google and found this https://www.uni-due.de/~hn213me/mt/w13/isedm/KOBallsBoxes.pdf case 1.1.2 but the problem is that we didn't learn about Stirling numbers and we never gonna learn about them as far as I know (seems way too advance for this course). How do I solve these kind of exercises? Thanks in advance","['combinatorics', 'probability']"
1725730,What is a left-invariant Vector field? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved Improve this question Is left invariant vector field on  $\mathbb{G}$,is same as a vector field which is invariant under group transformation.","['algebraic-geometry', 'geometry']"
1725737,Counter-intuitive intersection property,"Consider the following sequence of sets $A_k $ ,Where $A_k  = \{ n \in \mathbb Z \mid n \geqslant k \}$. Now let us consider $\bigcap_{k=1}^{\infty} A_k$. In a book that I was going through it says that this expression is equal to $\varnothing $. I tried to prove it on my own and I could find an easy argument which I suspect has flaws and I am interested in finding it out. My reason for suspicion is a piece of information, that I already had that, this is in a way related to something called as Caratheodory's lemma which has a different proof. Anyways the little, making -it-look -like-obvious type of proof goes as follow: Let us assume that the expression  $\bigcap_{k=1}^{\infty} A_k$ equals to some set  $\ B $ .As we can see every set in the sequence contains its successor . So the expression $\bigcap_{k=1}^{\infty} A_k$ must be a set of the format $A_m $  for some $m \in \mathbb Z$, otherwise if it is a set which is a collection of integers then for every integer we can find a set which cannot have it . Now at the same time if is equal to $\ A_m$ then the set $ A_t $, where $ t $, for any $t >\ m $ cannot contain it. As such the only thing all the sets can have in common is $\varnothing $. What are the flaws in this argument, what is a standard proof for it? And if there aren't any flaws in this argument then It gives me a feeling that this points to some connections to the axioms of of set theory, like in defining the null set to be present  in all sets,which can be powerful enough to lead to other such implications. It maybe just a feeling but I would also like to know if this is also true.",['elementary-set-theory']
1725750,The expression of the sum of infinite gaussian functions,"Let $f(x|\mu,\sigma^2)$ be the gaussian function (normal distribution): $$f(x|\mu,\sigma^2)=\frac{1}{\sigma\sqrt{2\pi}}e^{ -\frac{(x-\mu)^2}{2\sigma^2} }$$ We know its integral over $\mathbb{R}$ is 1. Now we divide the interval $[0,1]$ into $n$ subintervals, each with an equal length of $1/n$. For each subinterval $[\frac{i-1}{n},\frac{i}{n}], (i=1,2,\cdots,n)$, there is a function $g_i(x)$: $$g_i(x)=\frac{1}{n}f(x|\frac{i-0.5}{n},\sigma^2)$$ Namely, a rescaled gaussian function whose mean is the center of the subinterval and integral over $\mathbb{R}$ is $1/n$. Add them together: $$G(x) = \sum_{i=1}^n g_i(x)$$ Problem: What's the expression of $G(x)$ when $n\rightarrow \infty$ ? I simulated the result in Matlab by setting $n=10, \sigma=0.08$: ($g_i(x)$ in blue and $G(x)$ in red) By the way, I plan to use $G(x)$ in image processing, so an analytical expression that can be computed directly may be more useful than a mathematical strict yet confusing solution (such as a series). Common special functions like Bessel function are also OK. And necessary approximation will also be acceptable. Thank you in advance.","['special-functions', 'statistics', 'calculus']"
1725754,What is the new probability density function by generating a random number by taking the reciprocal of a uniformly random number between 0 and 1?,"I have a random number generator which can generate a random number between $0$ and $1$. I attempt to generate a random number between 1 and infinity, by using that random number generator, but taking the reciprocal of that result. Is the new generator uniform? Certainly not. Then what is the probability density function of the new generator?","['probability', 'calculus', 'probability-distributions']"
1725761,How many quadratic extension are there on a field?,"Given a field $F$, how many non-isomorphic quzdratic extension of $F$ are there ? I don't know if there is a general answer, for instance there is only one for $F=\mathbf{R}$, viz. $\mathbf{C}$, and no one for $F=\mathbf{C}$, for it is  algebraically closed. There may be a more precise answer for quadratic extension of number fields. For $F=\mathbf{Q}$, there are only two, every real extension being isomorphic and of the form $\mathbf{Q}(\sqrt{d})$, and every complex extension being a $\mathbf{Q}(\sqrt{-d})$, with $d$ a positive integer. What about $p$-adic fields ? If $F$ is a finite extension of degree $d$ of $\mathbf{Q}_p$, how many quadratic extension $E/F$ are there ? Krasner seems to have given an explicit formula for coutning such extensions of degree $n$, but I hope there is an easier way to reach the answer for $n=2$. If $x$ is in $E \backslash F$, $E = F(x)$. We can suppose $x = \sqrt{a}$ for an $a \in F$, what can be seen as in the real or rational case by factorizing the minimal polynomial (of degree 2) of $x$. We can also suppose it squarefree, for each square factor do not change the extension over $F$. For $\pi$ a uniformizer, every element can be written $x = \pi^r u$ with $u \in O_F$ an integer of $F$, hence quadratic extensions are of the form $E = F(\sqrt{u})$ or $E = F(\sqrt{\pi u})$, with $u$ squarefree unit. Hence quadratic extensions are parametrized by squarefree units, that is $O_F/O_F^2$, that is the kernel of $x \mapsto x^2$, and hence of cardinal 1 or 2 ? ($x^2=1$ only having one or two solutions in a field) Am I right ? Any clue or idea would be welcome ;) Best regards,","['number-theory', 'abstract-algebra', 'extension-field', 'field-theory']"
1725783,Question regarding proof of property related to the stopped sigma-algebra.,"I have a proof of a property regarding the stopped $\sigma$-algebra, where one part I do not understand, I'll highlight what I do not get, can you please help me? We have a probability space $(\Omega,\mathcal{F},P)$, with a filtration $\{\mathcal{F}_t\}$. We have a random variable T, which is a stopping time, i.e. $\{\omega: T(\omega)\le t\}\in \mathcal{F}_t\ \  \forall t$. The stopped sigma-algebra is defined as $\mathcal{F}_t = \{F\in \mathcal{F}: F\cap\{\omega: T(\omega)\le t\}\in \mathcal{F}_t \ \forall t\}$. The theorem proved is this: Let $T$ be a finite stopping time. Then $\mathcal{F}_t$ is the
  smallest $\sigma$-algebra containing all càdlàg processes sampled at
  T. That is: $\mathcal{F}_T = \sigma(X_T: \text{X adapted and càdlàg}).$ The proof given is this: Let $\mathcal{G} = \sigma(X_T: \text{X adapted and càdlàg})$. Let $A
\in \mathcal{F}_T$. Then $X_t=1_A(\omega)\cdot 1_{\{T(\omega)\le t\}}$
  is a càdlàg process, and $X_T(\omega)=1_A(\omega)$. Hence $A \in
\mathcal{G}$, and $\mathcal{F}_t \subset \mathcal{G}$. Next let $X$ be an adapted càdlàg process. We need to show that $X_T$
  is $\mathcal{F}_T$ measurable. Consider $X(s,\omega)$ as a function
  from $[0,\infty)\times\Omega$ into $\mathbb{R}$. Construct $\phi: \{T
\le t\}\rightarrow[0,\infty)\times\Omega$  by
  $\phi(\omega)=(T(\omega),\omega))$. Then since X is adapted and
  càdlàg, we have that $X_T = X \circ \phi$ is a measurable mapping from
  $(\{T\le t\},\mathcal{F}_t\cap\{T \le t\})$ into
  $(B,\mathcal{B}(\mathbb{R})).$ Therefore $\{\omega: X(T(\omega),\omega)\in B\}\cap \{T\le t\}$ is in $\mathcal{F}_t$, and this implies that $X_T$ is
  $\mathcal{F}_T$-measurable. I have two questions regarding this proof which I do not understand. It is assumed ha T is a finite stopping time, but where in the proof is that actually used? I can't see where he uses it? This is my main question, and it is in regard to the second part which I have highlighted. Why does it follow that since X is an adapted càdlàg-process, then $X\circ \phi$ is measurable wih regards to the given spaces$ (\{T\le t\},\mathcal{F}_t\cap\{T \le t\})$ and $(B,\mathcal{B}(\mathbb{R}))$. Is this easy to see, or do we have to do some work to show this? From what I see, in order to finish this part of the proof and show that $X_T$ is $\mathcal{F}_T$-measurable, we have to show two things. First that $X_T^{-1}(B)\in \mathcal{F}$ and for all t, $X_T^{-1}(B)\cap\{T\le t\}\in \mathcal{F}_t$for all t, and all Borel-sets. Is this showed in the proof?","['stochastic-processes', 'probability-theory', 'measure-theory', 'stopping-times']"
1725786,Clearly explaining what the chromatic number of a particular graph is,"Given the graph below: I need to find the chromatic number and explain clearly why this is the case. I know that the chromatic number has to be at least 3 because the chromatic number of a pentagon-shaped graph is 3 (which in a sense is the ""base"" of this graph). Simply by trying I can tell that the graph needs to have one more colour, thus the chromatic number is 4. What I'm stuck on is how do I explain clearly/prove that it is in fact 4. There are so many different ways of colouring it so do I really need to explain all of them? How do I do that in the clearest/most systematic way (if that really is the way to go about this)?","['graph-theory', 'discrete-mathematics']"
1725788,"How many ways to choose $a<b<c<d<e$ from the set $\{1,2,3,\dotsc,100\}$ such that $100<a+b+c+d+e<145$?","I would appreciate if somebody could help me with the following problem: In how many ways can I choose five numbers $a,b,c,d,e$ satisfying $a<b<c<d<e$ from the set $\{1,2,3,\dotsc,100\}$ such that $$100<a+b+c+d+e<145 \quad ?$$ One approach seems to be to first compute, for each $k$ , the number of $5$ -tuples $(a,b,c,d,e)$ satisfying $a,b,c,d,e\in \{1,2,3,\dotsc,100\}$ and $a<b<c<d<e$ and $a+b+c+d+e=k$ . But this doesn't look simple either.","['generating-functions', 'combinatorics', 'integer-partitions']"
1725790,Calculate third point of triangle from two points and angles,"I've got two points(p1 and p2) and two angles(angle1 and angle2), I can calculate the third angle, but how do I calculate the coordinates of point p? Not just the distances from the points, but coordinates. I'm trying to use this to do texture mapping on triangles. Here is an image of my idea p1 = (2, 0)
p2 = (6, 4)

angle1 is angle next to p1,  
angle2 is angle next to p2.",['trigonometry']
1725791,How local is the information of a derivative?,"I have read it a thousand times: ""you only need local information to compute derivatives."" To be more precise: when you take a derivative, in say point $a$, what you are essentially doing is taking a limit, so you only need to look at the open region $ (a-\delta,a+\delta) $. Taylor's theorem seems to contradict this: from the derivatives in just one point, you can reconstruct the whole function within its radius of convergence (which can be infinity). For example, consider the function:
$f: \mathbb{R} \rightarrow \mathbb{R}:x\mapsto \left\{
     \begin{array}{lr}
       x+3\pi/2:& x \leq-3\pi/2 \\
       \cos(x): & -3\pi/2\leq x \leq3\pi/2\\
       x+3\pi/2& : x\leq-3\pi/2
     \end{array}
   \right.\\$ Wolfram Alpha tells me that $D^{100}f(0)=\cos(0)$... This should give us more than enough information to get a Taylor expansion that converges beyond the point where $f$ is the $\cos$ function ($R=\infty$ for $\cos$ so eventually we have to get there) ... Let me put it this way: Look at the limiting case. All you need to have for a Taylor expansion that converges over all the reals is all the derivatives in 0. This would give you the exact same Taylor expansion as you'd get for the cosine function, while the function from which we took the derivatives is clearly not the cosine function over all the reals. So my question is: Is Wolfram Alpha wrong? If it is right, why does this seem to violate Taylors theorem? If it's wrong, is that because the local region of the domain you need to compute the nth derivative grows with n? Edit 1 :
en.m.wikipedia.org/wiki/Taylor%27s_theorem. The most basic version of Taylors theorem for one variable does not mention analyticity, and it's easy to prove that the ""remainder"" goes to zero as you take more and more derivatives, so that f(x) is determined at any x by the derivatives of f in 0.","['real-analysis', 'taylor-expansion']"
1725800,Find maximum $2\sin 5x-3\cos x$.,Is it possible to find the maximum of $2\sin 5x-3\cos x $ without using calculus nor numerical methods? I suspect there is a way to play around with trig identities until the expression is only in terms of either $\cos$ or $\sin$. I have done some trials but have only managed to make the expression messier.,['trigonometry']
1725828,Sized Biased Picking Distribution,"I am having trouble understanding the following proof on sized biased picking. We have the following situation: Let $ X_1, \cdots , X_n $ be i.i.d. and positive, and $S_i = X_1 + \cdots + X_i$ for $ 1 \leq i \leq n $. The values of $S_i/S_n$ are used to partition the interval $[0,1]$, each sub-interval has size $Y_i = X_i/S_n$. Suppose $U$ is an independent uniform r.v. on $(0,1)$, and let $\hat{Y}$ denote the length of the sub-interval containing $U$. We aim to calculate the distribution of $\hat{Y}$. The claimed result is $ \mathbb{P}(\hat{Y} \in dy) = n y \, \mathbb{P}(Y \in dy) $, where the notation means $\mathbb{P}(\hat{Y} \in A) = \int_A n y \mu(y) \, dy$ with $\mu$ the law of $Y$ (I think perhaps they mean the density function $f_Y(y)$). The proof given is 
\begin{align*}
\mathbb{P}(\hat{Y} \in dy) &  = \sum_{i=1}^n \mathbb{P}\left(\hat{Y} \in dy, \frac{S_{i-1}}{s_n} \leq U < \frac{S_i}{S_n}\right) \\
& = \sum_{i=1}^n \mathbb{P}\left(\frac{X_i}{S_n} \in dy, \frac{S_{i-1}}{S_n} \leq U < \frac{S_i}{S_n}\right) \\
&  =  \sum_{i=1}^n \mathbb{E}\left[\frac{X_i}{S_n} \, 1\left(\frac{X_i}{S_n} \in dy\right)\right] \tag{$\ast $} \\
& = \sum_{i=1}^n y \, \mathbb{P}\left(\frac{X_i}{X_n} \in dy \right) \tag{$\ast  \ast $} \\
& =ny \, \mathbb{P}(Y \in dy) \\
\end{align*} I do not understand the equalities $(\ast)$ and $(\ast \ast)$, and do not fully understand the notation given in the proof.","['probability-theory', 'probability', 'measure-theory', 'probability-distributions']"
1725852,Another chain of six circles,"I found a conjecture: A chain of six circles associated with six points on a circle (in Mobius plane) . This is a generalization of the last my previous question in Three chains of six circles . (Noting that, in this configuration: $P'_1P_1P_4P'_4$ don't lie on a circle) I am looking for a proof of the conjecture as following: Conjecture: In the Möbius plane , let a chain of six circles $(C_1)$ , $(C_2)$ , $(C_3)$ , $(C_4)$ , $(C_5)$ , $(C_6)$ , such that two neighbors circles $(C_i)$ meets $(C_{i+1})$ at two points $P_i, P'_i$ where $i=1, 2, 3, 4, 5, 6$ ; Such that six points $P_1, P_2, P_3, P_4, P_5, P_6$ lie on a circle. Let $A_1$ be a point on the circle $(C_6)$ , let the circle $(A_1P_1P_2)$ meets the circle $(C_2)$ at $A_2$ , Let the circle $(A_2P'_3P'_4)$ meets the circle $(C_4)$ at $A_3$ , Let the circle $(A_3P_5P_6)$ meets the circle $(C_6)$ at $A_4$ , Let the circle $(A_4P'_1P'_2)$ meets the circle $(C_2)$ at $A_5$ , Let the circle $(A_5P_3P_4)$ meets the circle $(C_4)$ at $A_6$ . Then show that: Four points $A_6, P_5, P_6, A_1$ lie on a circle. Six points $A_1, A_2, A_3, A_4, A_5, A_6$ lie on a circle","['plane-geometry', 'euclidean-geometry', 'geometry']"
1725911,Minimize the probability of getting the same sum of two rolls.,"If you can design two dice that not necessarily fair and not necessarily weighted in the same manner, then roll twice. How can you minimize the probability of getting the same sum of the two rolls? My thought is since the probability of rolling the same sum of two fair dice is $\dfrac{6^2+2\times(1^2+2^2+3^2+4^2+5^2)}{36^2}$. Our goal is to minimize the numerator by changing the weight of each side of the dice. If we make one die with side $1$ weighted more and the other die with side $6$ weighted more, then we may reduce the probability of getting the same sum. Am I right? I have no idea what's next. Can someone help me please? Thanks.",['probability']
1725944,Estimate the bound of the sum of the roots of $1/x+\ln x=a$ where $a>1$,"If $a>1$ then  $\frac{1}{x}+\ln x=a$ has two distinct roots($x_1$ and $x_2$, Assume $x_1<x_2$). Show that $$x_1+x_2+1<3\exp(a-1)$$ First I tried to estimate the place of the roots separately. I have got that $x_1\leq \frac{1}{a}$ and $\exp(a-1)<x_2<\exp(a)$. Then I tried to think about $x_1 + x_2$ as a whole. Because $1/x_1+\ln x_1=a$ and $1/x_2+\ln x_2=a$. I tried to express $x_1+x_2$ as a function of $a$, But I failed. I have no idea to solve this problem. Please help me  :)","['inequality', 'analysis']"
1725979,$g\circ f$ is Surjective and $g$ is Injective then Prove that $f$ is surjective,"Let $f : A \to B$ and $g : B \to C$ be functions such that $g\circ f: A \to C$ is a surjection and $g$ is an injection , Then prove that $f$ is a surjection. Since $g$ is a function $\forall y \in B$ we have $$g(y)=z \tag{1}$$ for some $z \in C$ Since $g\circ f$ is a surjection $\forall z \in C \exists x \in A$ such that $$g\circ f(x)=z$$ that is $$g(f(x))=z \tag{2}$$ from $(1)$ and $(2)$ we have $$g(y)=g(f(x))$$ and since $g$ is an injection we get $$f(x)=y$$ so $f$ is surjective. Is my proof correct?","['algebra-precalculus', 'functions', 'proof-verification']"
1726012,How to solve for $\theta$ in $130\cos(\theta) - 122\sin(\theta)=10$?,"The original question was solve for $\theta$ in $65\cos(2\theta)-56\sin(2\theta)-55=0$. I reduced it to $\cos(\theta)((130\cos(\theta)-122\sin(\theta))-10=0$, therefore we have a $\theta=0$ but I don't know how to solve the above. Please could anyone help me solve it?","['angle', 'trigonometry', 'education']"
1726101,How to solve the antiderivative of $x\cos\left(x^3\right)$,"What is the way to solve: $$\int x\cos\left(x^3\right)\space\text{d}x$$ Thanks, I've no idea how to start","['derivatives', 'integration']"
1726137,integrate $\int_{0}^{1}\int_{3y}^{3} e^{x^2}dxdy$,$$\int_{0}^{1}\int_{3y}^{3}e^{x^2}dxdy$$ I understand I need to change the integration limits but just changing the order does not help to solve the integral. $\int_{3y}^{3}\int_{0}^{1}e^{x^2}dydx=\int_{3y}^{3}[ye^{x^2}]_{0}^{1}dx=\int_{3y}^{3}e^{x^2}dx$ How should I approach it?,"['multivariable-calculus', 'integration']"
1726144,How are second order nonlinear ordinary differential equations solved?,"I conceived the following second order nonlinear ordinary differential equation: $$\frac{d^2y(x)}{dx^2}=\frac{k}{(y(x))^2}$$ I can tell it's nonlinear because of the $\frac{k}{(y(x))^2}$ term and second order because of the second order derivative. Also, I did some research and concluded that it is of the type ""missing x "". In this category we use the relation, according to SOS math :  $$\frac{d^2y(x)}{dx^2}=v\frac{dv}{dy} (1)$$  by making v equal to $$\frac{dy(x)}{dx} (2)$$ and then, using the chain rule, simplify to obtain equation (1). 
Despite all that, I can't seem to find a logical solution to my ODE. I would appreciate any help or clue! Thanks!","['ordinary-differential-equations', 'nonlinear-system']"
1726174,Interior of cartesian product is cartesian product of interiors,"I have to prove that: $$\operatorname{Int}(A\times B) = \operatorname{Int}(A)\times \operatorname{Int}(B)$$ Where $A\subset M$ and $B\subset N$ , both $M$ and $N$ metric spaces. The problem is that the exercise does not specify the metric, so I need to try to prove it using a generic metric. If $x\in \operatorname{Int}(A\times B)$ , then an open ball can be centered at $(x,y)$ such that it is contained in $A\times B$ . Therefore, this open ball, $B\big((x,y),r\big)\subset A\times B$ . It means that $d\big((x,y),a\big)<r$ for any $a\in A\times B$ for some $r>0$ . I guess that if I can take from here that $d(x,a)<r$ and $d(y,a)<r$ I can prove thhat $\operatorname{Int}(A\times B)\subset \operatorname{Int}(A)\times \operatorname{Int}(B)$ , but I still have to prove the other way around. Any ideas?","['general-topology', 'metric-spaces']"
1726185,"Show, that $c$ and $c_0$ is a Banach space","Let $c=\lbrace x=\lbrace x_n\rbrace ,n\in \mathbb{N}: \exists  \, \text{lim}_{n\to \infty}x_n\rbrace$ and $c_0=\lbrace x=\lbrace x_n\rbrace ,n\in \mathbb{N}: \text{lim}_{n\to \infty}x_n=0\rbrace$. 
I want to show that $c$ anc $c_0$ are Banach (I have to show that both are complete metric spaces). Is it enough to show that $\text{lim}_{n\to \infty}x_n$ belongs to space? These are my thoughts: Let: $$x^{(n)}=\left( x^{(n)}_j\right)_{j=1}^\infty =(x^{(n)}_1,x^{(n)}_2,\dots )$$ be a Cauchy sequence in $c$. Lets define the $\infty$-norm $||\cdot||_\infty$:
$$||(x_j)_{j\geqslant 0}||_\infty = \text{sup}_{j\geqslant 0} |x_k|$$ Let $\varepsilon >0$, there exists $n_0\in \mathbb{N}$ such that:
$$||x_j^{(n)}-x_k^{(m)}||<\varepsilon,\,\,\,\forall n,m >n_0$$
$$\text{sup}_{j\geqslant 0} |x_j^{(n)}-x_j^{(m)}|<\varepsilon,\,\,\,\forall n,m >n_0$$
$$|x_j^{(n)}-x_j^{(m)}|<\varepsilon,\,\,\,\forall n,m >n_0, \,\,\,\forall j\geqslant 0$$ Now I have to prove that $$\text{lim}_{n\to\infty} x^{(n)}= x$$
and $$x\in c.$$ Unfortunately I don't know how to show that $x\in c$.","['functional-analysis', 'banach-spaces']"
1726198,"integrate $\int_0^1 \int_{\sqrt{x}}^1 e^{\frac{y^3}{3}} \, dy \, dx$","$$\int_0^1 \int_{\sqrt{x}}^{1}e^{\frac{y^3}{3}} \, dy \, dx$$ So I understand I need to change the integration limits by looking at the domain. I have sketched it But how do I know if I need to take the area under the curve or above it? (in the drawing the area filled with vercital or horizontal lines)","['multivariable-calculus', 'integration']"
1726223,What's the probability to win (or lose) this solitaire? [duplicate],"This question already has answers here : Combinatorial card game [duplicate] (3 answers) Closed 8 years ago . Me and my friends used to play a ""solitaire"" and always asked ourselves which are the odds to win, or lose. I studied Maths and many of them did as well, but nobody could find a good answer to this question. The solitaire goes as follows: get a regular deck of 52 cards and start flipping cards one at a time. When you flip the first card, you say ""one"": if the card is actually a ""one"", that is, an ace, then you lose the game. If not, you move on flipping another card and saying ""two"": as above, you lose if the flipped card is actually a two. You do the same for the number $3$ and then you switch back to one, that is, you say ""one"" by flipping the fourth card (supposing you haven't lost the game yet). My questions are: What are the the odds of going through the whole deck of cards without saying the number of the card you are flipping, i.e. the odds of winning this game? Does the number of cards in the deck make a difference? For example, would it be more or less likely to win if I had a deck of $48$ cards, from which I took out the queens? Does the numbers you say make a difference? For example, would it been more or less likely to win if I said ""one, two, three, four , one, two, ..."" while flipping the cards, instead of ""one, two, three, one, ...""? The only information I got is that it is extremely hard to win this game. Anyway, it is not impossible (so far, I have seen me or my friends win about $5$ times).
My attempts to directly calculate probabilities, using combinatorics techniques, failed utterly. Thanks!","['combinatorics', 'statistics', 'probability', 'recreational-mathematics']"
1726231,How to design a matrix with multiple chosen eigenvalues?,"I want to ""design"" (build) a matrix that have multiple known eigenvalues. For these apriori chosen eigenvalues, I want to know the corresponding eigenvectors, too. The point is that I want to start from the eigenvalues, chose the eigenvectors appropriately and then compute the matrix. All values must be integers. I need this for a cryptographic application. Namely, I'm looking for something similar to Chinese remainder theorem , but done with eigenvalues and eigenvectors.
To make an idea, in paper Fully Homomorphic SIMD Operations the authors propose an improvement upon previous schemes. They choose a specific cyclotomic polynomial which factorizes in n polynomials and so they can encrypt n message at a time. I want the same but for the approximate eigenvector problem .","['matrices', 'eigenvalues-eigenvectors', 'cyclotomic-polynomials']"
1726241,Taking inverse Fourier transform of $\frac{\sin^2(\pi s)}{(\pi s)^2}$ [duplicate],"This question already has answers here : Evaluating the integral $\int_{-\infty}^\infty \frac{\sin^2(x)}{x^2}e^{i t x} dx$ (5 answers) Closed 8 years ago . How do I show that
$$\int_{-\infty}^\infty \frac{\sin^2(\pi s)}{(\pi s)^2} e^{2\pi isx} \, ds = \begin{cases} 1+x & \text{if }-1 \le x \le 0 \\ 1-x & \text{if }0 \le x \le 1 \\ 0 & \text{otherwise} \end{cases}$$ I know that $\sin^2(\pi s)=\frac{1-\cos(2\pi s)}{2}=\frac{1-(e^{2\pi i s}-e^{-2\pi i s}))/2}{2}$, so
$$\int_{-\infty}^\infty \frac{\sin^2(\pi s)}{(\pi s)^2} e^{2\pi isx} \, ds=2\int_0^\infty \frac{2e^{2\pi isx}-(e^{2\pi is(1+x)}+e^{2\pi i s(-1+x)})}{4\pi^2s^2} \, ds$$ I am also allowed to use the known identity
$$\int_{-\infty}^\infty \frac{1-\cos(a \pi x)}{(\pi x)^2} \, dx = |a|$$
for some real number $a$.","['integration', 'fourier-analysis']"
1726246,"For any real number $x$, if $ x^3+2x+33\neq 0$, then $x+3 \neq0$",How to solve this type of sum using indirect proof. Appreciate if anyone can explain it step by step.,"['discrete-mathematics', 'elementary-number-theory']"
1726266,Why does this limit approach $e$? [duplicate],"This question already has answers here : Why isn't $\lim \limits_{x\to\infty}\left(1+\frac{1}{x}\right)^{x}$ equal to $1$? (10 answers) Closed 8 years ago . Why does
 $$(1+1/k)^{k}\rightarrow e\hspace{0.5cm}\text{as}\hspace{0.5cm}k\rightarrow\infty$$ why does it not approach $$(1+1/k)^{k}\rightarrow (1+0)^k=1\hspace{0.5cm}\text{as}\hspace{0.5cm}k\rightarrow\infty$$","['calculus', 'limits']"
1726287,Integral of a weak derivative,"While reading chapter 6 of John Hunter's notes ( https://www.math.ucdavis.edu/~hunter/pdes/pde_notes.pdf ) I got stuck on some steps.
I think they are all based on a similar idea as the following. Let $u \in L^2(0,T; H_0^1(U))$ and it's weak time derivative $u_t \in L^2(0,T; H^{-1}(U))$ does there hold (for $U$ a bounded set of $\mathbb{R}^N$)
$$ u(t) = u(0) + \int_0^t u_t(s) ds \: \: \: ?$$
To write this in a meaningfull manner $u(0)$ has to be defined uniquely, therefore is $u$ continuous? My questions are 1) Does there hold $$ \int_0^t \frac{d}{dt}||u||_{L^{2}(U)}^2(s) ds = ||u||_{L^{2}(U)}^2(t)-||u||_{L^{2}(U)}^2(0) \: \: \: ?$$
again $||u||_{L^{2}(U)}^2(0)$ has to be well-defined. 2) In proposition 6.5 the author gives a prove for the existence of solution of the differential equation
$$ \vec{c_n}_t + A(t) \vec{c_n} = \vec{f}(t), \: \: \: \vec{c_n}(0)= \vec{g}  $$
Where $A \in L^{\infty}(0,T; \mathbb{R}^{N \times N}), \: \: \: \vec{f} \in L^2(0,T; \mathbb{R}^N) $ And then he writes the 'equivalent' integral equation
$$ \vec{c_n}(t) = \vec{g}- \int_0^t A(s) \vec{c_n}(s) ds + \int_0^t \vec{f}(s)ds $$
Why is this equivalent to the original problem?
I know this holds in the classical sence, but here we are considering weak derivatives. I don't see how we get from the weak equation to the integral form and backwards. 3) My last question involves a Gronwall inequality in the context of weak derivatives. If for a.e. $t \in [0,T]: \frac{d}{dt}||u(t)||_{L^2(U)} \leq ||u(t)||_{L^2(U)}$ can we bound $u(t)$? I searched a lot about this but I couldn't find any answer.
The most problems will be solved if $u$ was absolutely continous. But since $u_t(t) \in H^{-1}(U)$ and not per se in $H_0^1(U))$ so we can't use the Sobolev theorem directly? The literature I found didn't consider the case for $u_t(t) \in H^{-1}(U)$. EDIT: In the notes of John Hunter and in the book of Evans they use $${u_n}_t(u_n) = \int_{U} {u_n}_t u_n dx $$
Where $u_n$ are approximate solutions. So if the action of ${u_n}_t$ is just the inner product of $L^2(\Omega)$ why not take ${u_n}_t(t)$ to be in $H_0^1(\Omega)$? Can't it be bounded in $H_0^1(\Omega)$-norm?","['bochner-spaces', 'partial-differential-equations', 'functional-analysis', 'weak-derivatives', 'sobolev-spaces']"
1726307,Confusion for Proof of GCD Theorem?,"I'm having some trouble understanding a part of a theorem that states that the gcd between two integers exists and is unique. First it state the Euclidean Algorithm for positive integers, that for positive integers $a$ and $b$,
$$
\begin{align}
a & = bq_1 + r_1 \\
b & = r_1q_2 + r_2 \\
r_1 & = r_2q_3 + r_3
\end{align}
$$
As long as the remainders aren't zero And so on and so forth. Then it states the fact that 
$b > r_1 > r_2 > r_3 >\cdots>0$ Then it says, using induction, that 
$$0 \le r_i \le b - i.$$ Where did that $b - i$ come from? What does it mean? How is this inequality true?","['number-theory', 'euclidean-algorithm', 'proof-explanation', 'elementary-number-theory']"
1726329,"How many fixed points are there for $f:[0,4]\to [1,3]$","Let , $f:[0,4]\to [1,3]$ be a differentiable function such that $f'(x)\not=1$ for all $x\in [0,4]$. Then which is correct ? (A) $f$ has at most one fixed point. (B) $f$ has unique fixed point. (C) $f$ has more than one fixed point. Here, $f:[0,4]\to [1,3]\subset [0,4]$ is continuous and $[0,4]$ is compact convex. So by Brouwer's fixed point theorem $f$ has a fixed point. But I am unable to use the condition $f'(x)\not=1$ and how I can conclude that how many fixed points are there for $f$ ?","['general-topology', 'fixed-point-theorems', 'real-analysis', 'analysis']"
1726391,Why does the graph of $\frac{1-cos(3x)}{3x}$ look so weird at small values,"So I was just doing some homework, and I happened to graph this $\frac{1-cos(3x)}{3x}$ For some unknown reason I decided to zoom into to ridiculous levels and I saw a really really weird behavior of the graph. Sorta like a cartoony signal wave or something. Is this actually what this graph would look like in a perfect world or is this just a display of the inaccuracies of computer trigonometry?  How would you go about proving what the graph actually looked like considering such small number are used?",['trigonometry']
1726452,"Showing that $x^n=\sum_{k=1}^{n}{n\brace k}(x)(x-1)\ldots (x-k+1)$ holds for all numbers, not just positive integers","I just finished proving that this statement holds for all positive integers $r$ (through a combinatorial argument) $$r^n=\sum_{k=1}^{n}{n\brace k}(r)(r-1)\ldots (r-k+1)$$ (where the curly braces indicate the Stirling set numbers) However, I'm being asked to show that this holds for all numbers $x$. That is $$x^n=\sum_{k=1}^{n}{n\brace k}(x)(x-1)\ldots (x-k+1)$$ The hint we received was that we should note that the second equation has at most $n$ solutions. However, I'm not sure how that will help me here. Any help would be greatly appreciated.","['stirling-numbers', 'combinatorics']"
1726511,"Unnecessary assumption in exercise (from Spivak, Calculus on Manifolds)","I have a question on the following exercise (which is taken from Spivak, Calculus on Manifolds , page 105). If $\omega$ is a $1$-form $f dx$ on $[0,1]$ with $f(0) = f(1)$, show that there is a unique number $\lambda$ such that $\omega - \lambda dx = dg$ for some function $g$ with $g(0) = g(1)$. Why the assumption $f(0) = f(1)$ in the exercise, I guess it is superfluous; as I guess I have a solution without using it. So why the assumption $f(0) = f(1)$? My solution : Set $\lambda := \int_0^1 f$, then with $g(x) := \int_0^x (f - \lambda)$ everything works out as it should, and integrating $f - \lambda = g' \Leftrightarrow \omega - \lambda dx = dg$ on $[0,1]$ gives uniqueness of $\lambda$.","['differential-forms', 'calculus', 'integration', 'differential-geometry', 'analysis']"
1726525,Projection theorem for conditional probability,"$\def\cov{\mathop{\mathrm{cov}}}\def\var{\mathop{\mathrm{var}}}$My professor uses something that he calls the ""projection theorem"", to get rid of the condition in conditional probabilities (expectation and variance). I have not found anything about it on the internet, so I am wondering where it comes from, and if it is right. Here is the so-called ""projection theorem"": $$E[\tilde{x}\mid \tilde{y} = y] = E[\tilde{x}] + \frac{\cov(\tilde{x},\tilde{y})}{\var(\tilde{y})}\times(\tilde{y}-E(\tilde{y})),$$ and $$\var[\tilde{x}\mid \tilde{y}] = \var(\tilde{x})-\frac{\cov^2(\tilde{x},\tilde{y})}{\var(\tilde{y})}.$$ Are these formulas correct?","['conditional-expectation', 'probability', 'variance', 'covariance']"
1726557,If the sum to 4 terms of a geometric progression is 15 and the sum to infinity is 16 find the possible values of the common ratio.,I can't find a way to get an answer for this. I have tried using the formula for the sum to infinity and dividing it by the sum to 4 terms but i can't get it to work.,['sequences-and-series']
1726562,Is the result true when the valuation is trivial and $\dim(X)=n$?,"Here I proved the following result: Proposition: Let $(K,|\cdot|)$ be a valued field with non-trivial valuation and let $X$ be a vector space over $(K,|\cdot|)$. Two norms $p_1,p_2$ on $X$ are equivalent (i.e., they induce the same topology) iff there are constants $c_1$ and $c_2$ such that $c_1p_1\leq p_2\leq c_2p_1$ And here Eric Wofsey showed, with a counterexample, that the proposition is false when the valuation is trivial (i.e. $|x|=1$, for each $x\in K^*$)
 and the space $X$ is infinite-dimensional. Question: Is the proposition true when the valuation is trivial and $X$ is finite-dimensional?","['functional-analysis', 'normed-spaces', 'general-topology', 'valuation-theory']"
1726578,Understanding a proof of Schröder-Bernstein theorem,"Abbott's intro analysis text gives a guided exercise to work through the Schröder-Bernstein Theorem. There are two key (probably related) parts I do not understand. Theorem: Let there be an injective function $f: X \rightarrow Y$ and another injective function $g: Y \rightarrow X$. Then there exists a bijection $h: X \rightarrow Y$. Proof: Idea is to partition $X$ and $Y$ into $X = A \cup A'$ and $Y = B \cup B'$, with $A \cap A' = \emptyset$ and $B \cap B' = \emptyset$, such that $f$ maps $A$ onto $B$ and $g$ maps $B'$ onto $A'$. So set $A_1 = X \setminus g(Y)$ and inductively define a sequence of sets by letting $A_{n+1} = g(f(A_n))$. The exercise had me prove that $\{A_n : n \in \mathbb{N}\}$ is a pairwise disjoint collection of subsets of $X$, while $\{f(A_n) : n \in \mathbb{N} \}$ is a similar collection in $Y$. Why does pairwise disjointness matter? To conclude, let $A = \cup_{n=1}^\infty A_n$ and $B = \cup_{n=1}^\infty f(A_n)$. I showed that $f$ maps $A$ onto $B$ (trivial). Let $A' = X \setminus A$ and $B' = Y \setminus B$. Why does $g$ map $B'$ onto $A'$? This is such a key question that I feel bad admitting I need a hint.","['elementary-set-theory', 'functions']"
1726593,Intersections of a Set of Intervals,"Given a finite collection $\mathcal{B}$ of closed intervals $[i,j]$ such that among any three intervals $B_1 \in \mathcal{B}, B_2 \in \mathcal{B}, $ and $B_3 \in \mathcal{B}$, $\exists$ some $k \in \mathbb{R}$ such that $k$ is in at least two of $B_1, B_2, B_3$, show that $\exists$ $c_1$ and $c_2$ such that $c_1 \in B_i$ or $c_2 \in B_i$ $\forall B_i \in \mathcal{B}$. By the way, $\forall$ means for all, $\exists$ means there exists, and $\in$ means in. Thanks!",['elementary-set-theory']
1726602,Intersection of Three Unique Paths in a Tree,"For a tree $T$, define $P[u, v]$ to be the subgraph that is the path between $u$ and $v$. Prove that for 3 distinct vertices $u$, $v$, and $w$ in a tree, that $P[u, v] \cap P[u, w] \cap P[v, w]$ consists of a single vertex. I know that two vertices in a tree can only be connected by one unique path and trees are acyclic. I believe one of the three points has to be a shared node between all three paths, giving you that vertex. However, I don't really know how to go about proving that or if I'm right in my assumption.","['graph-theory', 'discrete-mathematics']"
1726603,"On the meaning of formal sums of $k$-cubes, i.e. $k$-chains (in integration on manifolds)","A singular $k$-cube in $A \subseteq \mathbb R^n$ is a continuous function $c : [0,1]^k \to A$. A singular $0$-cube in $A$ is then a function $f : \{0\}\to A$, what amounts to the same thing, a point in $A$. A formal sum of singular $k$-cubes in $A$ multiplied by integers is an $k$-chain, i.e. sums like
$$
 3c_1 + 2c_2 - 4c_3
$$
for singular $k$-cubes $c_1, c_2, c_3$. These could be defined more formally as ""polynomials"" in the $k$-cubes on $A$, i.e. functions from the $k$-cubes to $\mathbb Z$ for which only finitely many function values are non-zero. By the way these definitions are taken from Spivak, Calculus on Manifolds , page 97 (the more formal one is from an exercise therein). So now my question. As formal sums they do not admit any interpretation behind being merely formal objects with prescribed rules to manipulate them. Now for an $n$-cube (and also an $n$-chain by linear extension) its boundary is defined as such a formal sum of $n-1$-cubes got from this cube by fixing one variable. 
Then relations like $\partial^2 = 0$, or extending the integral over $n$-chains lineary from $n$-cubes, all work out fine, and ultimately lead to Stokes theorem. But when I look at the definition, and try to give it some meaning, or picture to have, such a formal sum seems to correspond to an union of sets (i.e. the parts of the boundary) enriched by some notion of orientation (by the sign in the sum) and ""speed of transversal"" (by its magnitude) (Remark: Actually this seems more of a multiset, as different $n$-cubes could share parts as sets). So, it might be possible to interpret these sums pointwise, i.e. for example $2c_1 - c_2$ not as a formal sum, but as the pointwise addition of functions, i.e. $2\cdot c_1(x) - c_2(x)$ for all $x \in [0,1]^k$ if $c_1, c_2$ are singular $k$-cubes. Of course, I mean this addition if we interpret every element of $\mathbb R^n$ as a vector (i.e. a position vector w.r.t. the origin), as a priori points could not be added. Comparing with the picture (I appended one from the above mentioned book) this seems to work fine. So why not use this more concrete definition, but use abstract formal sums? (or is my interpretation to naive, and fails somewhere?)","['manifolds', 'differential-forms', 'integration', 'differential-geometry']"
1726666,Deriving the q-Gaussian PDF,"Ok it may sound a bit too simple but I am quite confused here. While studying generalized entropic forms, in my case that of $S_q$ or in another words the Tsallis Entropy , I reach a point where I have to derive the maximal distribution that corresponds to $S_q$. In order for that to be done, one has to impose some constraints and follow the Lagrange parameters method. In this particular case the constraints required would be: \begin{align}
& \int_{0}^{\infty}p(x)dx=1 \quad \text{(Normalization Constraint)} \\
& \langle x_q \rangle=\int_{0}^{\infty}xP(x)dx=X_q \quad \text{(q-mean value)}
\end{align}
where $P(x)$ is called the Escort Distribution and is defined as:
\begin{align}
P(x)=\frac{[p(x)]^q}{\int_{0}^{\infty}[p(k)]^qdk}
\end{align}
Now we define the quantity:
\begin{equation}
Φ(x;p;q)=\frac{1-\int_{0}^{\infty}[p(x)]^qdx}{q-1}-α \int_{0}^{\infty}p(x)dx-\beta_q \frac{\int_{0}^{\infty}x[p(x)]^qdx}{\int_{0}^{\infty}[p(x)]^qdx}
\end{equation}
and demand that $\partial{Φ}/\partial{p}=0$. By solving that, one ends up with the pdf:
\begin{equation}
p_{opt}(x)=\frac{e_q^{-\beta_q(x-X_q)}}{\int_{0}^{\infty} e_q^{-\beta_q (x'-X_q)}dx'}
\end{equation} 
where $e_q^x$ are the q-expodentials. This $p_{opt}(x)$ is also known as q-Gaussian pdf. My problem in deriving the pdf, is that I am not able to see how to calculate the quantity: \begin{equation}
\frac{\partial{}}{\partial{p}}\left( \frac{\int_{0}^{\infty}x[p(x)]^qdx}{\int_{0}^{\infty}[p(x)]^qdx}\right)
\end{equation} Perhaps treating it like a function of the form $h(x)=f(x)/g(x)$? Am I making a mistake thinking of it in this way? Because I am not able to reach the final formula of the pdf. Also, I am not able to find any paper where the derivation of $p_{opt}$ is worked out. I would really appreciate your help.
Thank you!","['partial-derivative', 'chain-rule', 'statistical-mechanics', 'statistics', 'entropy']"
1726710,Is there a sequence with an uncountable number of accumulation points?,Let $(x_{n})_{n \geq 1}$ sequence in $\mathbb{R}$. Is there a sequence with an uncountable number of accumulation points? Thank you!,"['general-topology', 'real-analysis', 'sequences-and-series']"
1726722,"Immersion of $M^n$ into $\mathbb{R}^n$, is $M^n$ orientable? Compact? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Say we have an immersion of $M^n$ into $\mathbb{R}^n$ (same dimension). I have two questions. Is $M^n$ orientable? Is $M^n$ compact? Thanks in advance!","['differential-topology', 'manifolds', 'algebraic-topology', 'general-topology', 'differential-geometry']"
1726761,"Intuitive understanding of the ""Multiplication Rule""?","I apologize in advance that this question has a long set-up. In the set up I am presenting how I currently understand the material, and the actual question is if my understanding is correct and applicable to more types of questions. I am referring to the rule $$P(A \text{ and } B)=P(A) \cdot P(B|A)$$ Or just $$P(A \text{ and } B)=P(A) \cdot P(B)$$ If the events are independent. I will be using absolute value bars as notation for the size of a set (i.e. $|\{z_1, z_2, ...z_n\}|=n$) I feel like I understand this rule for the most part when we have $2$ independent events. For example, rolling an even number on a standard dice and flipping ""tails"" with a coin. The way I understand this is as follows: $$ \dfrac{ \text{Desired outcomes}}{\text{Total Outcomes}} = \dfrac{|\{2T, 4T, 6T\}|}{|\{1H, 1T, 2H, 2T,...,6H, 6T\}|} =   \dfrac{|\{2, 4, 6 \}|}{|\{1, 2, 3, 4, 5, 6\}|} \cdot \dfrac{ |\{T\}|}{|\{H, T\}|}$$ When it comes to two dependent events, it mostly makes sense but I don't think I've completely internalized it. For example, if the question asks: ""Find the probability of drawing a king, and then a queen from a deck of cards without replacement."" I have a little bit of trouble making sense of this one in the context of one deck, so I just pretend that we have two decks, one with $52$ cards and the other with $51$ cards (with the king of diamonds missing), and I reduce the problem to drawing a king and a queen in sequence from the two decks. I think I can make sense of it the same way I made sense of the dice and the coin: $$ \dfrac{ \text{Desired outcomes}}{\text{Total Outcomes}} = \dfrac{|\{K_{clubs}Q_{clubs}, K_{clubs}Q_{spades},...,K_{diamonds}Q_{diamonds}\}|}{|\{Ace_{clubs}Ace_{clubs}, Ace_{clubs}Ace_{spades},..., K_{diamonds}K_{hearts} \}|} =   \dfrac{|\{K_{clubs}, K_{spades}, K_{hearts}, K_{diamond} \}|}{|\{Ace_{clubs}, Ace_{spades},...K_{diamonds}\}|} \cdot \dfrac{ |\{ Q_{clubs}, ... Q_{diamond}  \}|}{| \{Ace_{clubs}, Ace_{spades},...K_{hearts} \}|}$$ However, what really gives me trouble is when we have just one event. For example ""15,000 U.S. medical school seniors applied to residency programs in 2009. Of those, 93% were matched with residency positions. 82% of those seniors matched with residency positions were matched with one of their top $3$ choices. Find the probability that a randomly selected student was matched with a residency position and that it was one of their top $3$ choices."" This problem is easy to solve from an  ""algebraic"": $$\dfrac{.82 \cdot(\text{n. of students who got a residency position})}{\text{total number of students}}=\dfrac{.82(.93 \cdot 15,000)}{15,000} $$ However, when I tried to tackle this one like I did the card problem, everything collapsed. Without loss of generality, we let students $s_1$ to $s_{13950}$ be the students who got residency positions, and let students $s_1$ to $s_{11439}$ be the students who got matched with one of their top $3$ choices. If I try to multiply like I did with the cards problem, the results just don't make sense to me (even though I know it gives me the right answer): $$\dfrac{|\{s_1,...,s_{13950}\}|}{|\{s_1,...,s_{15000}\}|} \cdot \dfrac{|\{s_1,...,s_{11439}\}|}{|\{ s_1,...s_{13950} \}|} = \dfrac{|\{s_1s_1, s_1s_2,...,s_{13950}s_{11439} \}|}{|\{s_1s_1,...,s_{15000}s_{13950}\}|}$$ My questions are: Is my understanding fine for independent events? Can it be applied to dependent events (like with the card problem)? Can it be applied to problems containing only $1$ choice (like the last problem)?","['intuition', 'statistics', 'probability']"
1726764,How to derive $\sum j^2$ from telescoping property,"The book Real Analysis via Sequences and Series has a method of proving that $$\sum_{j=1}^n j = \frac{n(n+1)}{2}$$ that I've never seen before.  The way they do it is by starting with $\sum (2j+1)$, using the fact that $2j+1 = (j+1)^2-j^2$, and then using the telescoping property. I find this method very aesthetically pleasing, but I have two questions about this: $(1)$ what was the motivation for starting with $2j+1$?  Why would that have come into the authors' minds as a way of deriving a formula for $\sum j$?  And, a related question: $(2)$, after that derivation the authors state that the formula for $\sum_{j=1}^n j^2$ can be found in a similar way.  I haven't been able to figure out which telescoping series I should equate this to.  How can the formula for this summation be found similarly?","['algebra-precalculus', 'telescopic-series', 'summation']"
1726786,Understanding Derivation of Euler Lagrange,"I am trying to understand the derivation of the Euler-Lagrange equation. I drew a graph below. So, according to the graph, $$ \int_{t_1}^{t_2} L(x+\delta{x},\dot{x}+\delta\dot{x}\,t)  dt - \int_{t_1}^{t_2} L(x,\dot{x},t) dt = \delta{S}$$ That is, we take the difference between the integral of the true path and the integral of the variated path, where S is the action defined by $\int_{t_1}^{t_2} L(x,\dot{x},t) dt$ and $\delta{S}$ is the variation in action (in the graph it is the distance between the blue line and the red line at any t value). Where do I proceed from there? I don't understand this line in the derivation: $$ \int_{t_1}^{t_2} L(x,\dot{x},t) dt + \frac{d{L}}{d{x}}\delta{x} + \frac{d{L}}{d{\dot{x}}}\delta{\dot{x}}+O((\delta{x})^2)- L(x,\dot{x},t) dt = \delta{S}$$ (Source: http://wiki.math.toronto.edu/TorontoMathWiki/index.php/Euler-Lagrange_Equation ) Namely, what is the function O? Where are the partial derivatives coming from? The only thing I know to do is to combine the two integrals because the times ( $t_1$ and $t_2$ ) and the integration variables (dt) are the same. How do I get all the mess to the left of $L(x,\dot{x},t)$ in the equation above?","['multivariable-calculus', 'classical-mechanics', 'integration', 'euler-lagrange-equation']"
1726806,Limit of the composition of two functions with f not necessarily being continuous.,"Let $g$ be a continuous function at a point $a \in \mathbb{R}$, and let $$\lim_{x\rightarrow g(a)} f(x) = L$$ Show that $$\lim_{x\rightarrow a} (f\circ g)(x) = \lim_{x\rightarrow g(a)} f(x)$$. How do I prove this in the case that $f$ is not continuous. My current proof is incorrect since it assumes $f$ is continuous. Let $\lim_{u\rightarrow g(a)} f(u) = L$. Also, since $g$ is continuous, $\lim_{x\rightarrow a} g(x) = g(a)$. If $\lim_{x\rightarrow a} (f\circ g)(x) = L$, then for all $\epsilon > 0$ there exists a $\delta > 0$ such that $0<|x - a|< \delta$ implies that $|(f\circ g)(x) - L | < \epsilon$.\ Now we will use the continuity of $g$ to prove the desired result. Since $\lim_{x\rightarrow a} g(x) = g(a)$, for all $\delta_1 > 0$ there exists a $\delta > 0$ such that  $0<|x - a|< \delta$ implies that $|g(x) - g(a) | < \delta_1$.\ Since $\lim_{u\rightarrow g(a)} f(u) = L$, for all $\epsilon > 0$ there exists a $\delta_1 > 0$ such that  $0<|u - g(a)|< \delta_1$ implies that $|f(u) - L | < \epsilon $.\ Now if we let $u = g(x)$, then we achieve the desired result since a $\delta$ and a $\delta_1$ can be chosen such that, $0 < |x - a| < \delta$ implies that $0 < |g(x) - g(a)|< \delta_1$, which finally implies that $|(f\circ g)(x) - L | < \epsilon$.\ Therefore 
$$
\lim_{x\rightarrow a} (f\circ g)(x) = \lim_{x \rightarrow g(a)} f(x).
$$","['continuity', 'real-analysis', 'calculus', 'limits']"
1726844,Embedding of $2$-torus minus one point into $\mathbb{R}^2$ as an open set?,Let $M^2$ be the $2$-torus minus one point. Is there an embedding of $M^2$ into $\mathbb{R}^2$ as an open set?,"['multivariable-calculus', 'differential-topology', 'differential-geometry', 'manifolds']"
1726859,Two ODE planar systems that are orthogonal to each other,"Given two planar systems X'=F(X) and X'=G(X) (so F and G are both $C^1$). Assume the dot product of F(X) and G(X) is always zero on $R^2$. Now if F has a closed orbit,  prove that G has a zero. My attempt: The first thing pops up in my mind is a circle (the closed orbit of F). On this circle, G(X) is perpendicular to F(X), and then there must be an equilibrium point inside the circle. However I cannot generalize my idea. P.S.: this should be an easy exercise after learning the Poincare-Bendixson theorem.","['ordinary-differential-equations', 'dynamical-systems']"
1726869,Does it necessarily follow that the integral curves of $k^a$ are null geodesics?,"Let $f$ be a function on a spacetime $(M, g_{ab})$ whose gradient, $k_a = \nabla_a f$, ie everywhere null, i.e., $k_ak^a = 0$ throughout $M$. Does it necessarily follow that the integral curves of $k^a$ are null geodesics?","['riemannian-geometry', 'general-relativity', 'mathematical-physics', 'manifolds', 'differential-geometry']"
1726896,Are physical/material/dimensional/temporal explanations of Banach-Tarski necessarily irrelevant?,"I recently re-reviewed some of my undergrad analysis text and read the sketch of the proof of Banach-Tarski presented on Wikipedia, starting with a proof that the free group with two generating elements can be cut and merged into two copies of itself by first accepting a partition of the group into members starting with each element/inverse, divying those by element, prepending one side by its inverse element $aS(a^-1) \bigcup S(a) $, and noticing that $aS(a^-1)$ is going to be a sequence that starts with anything but $a$, so you end up by symmetry with the same thing on the $b$ side, and you get $2$ copies of the free group of two generating elements. I have never felt the unease that others attribute to the paradox, despite having been aware of it at a rather vapid but conceptual level from the age of 14 or 15. I am curious to know whether my instincts are just wrong (but have made me feel okay) or whether there is any merit to them. My first instinct when younger was to attribute the change in volume to a change in how ""tightly packed"" the points of the interior were to each other, assuming multiple configurations were possible and meaningful under the assumptions of the theorem. As I got older, I started to think about it as an analogous to how you can fold and wrinkle paper into a still-planar manifold, and considered whether it could have less surface area. I discovered that at least one case is obvious - if you end up gluing edge to edge, you lose an edge worth of area. So there was nothing intrinsic about surface area for this piece of paper, why would volume work differently? Once I reached analysis, I had convinced myself of a different physical theory - that somehow you could ""pull a string out of the ball"" of a single point radius, and that your choice of reconstruction would be a matter of how wasteful you were in spooling the core of the new ball. Since points had no dimensionality, I thought, it should somehow be possible to thread the core for arbitrarily long string, and yet not fill a continuum of points through any ray in 3D from the origin. Today, I have to say I am no smarter than I was when I was younger, and in fact I don't feel comfortable that any of these pseudo explanations are relevant to the interplay between the paradox and the notion of embedding some space with measure. So I would appreciate to know: Which if any of my past conceptualizations was a reasonable thought model for the paradox? Whether any physical explanation is likely to be a reasonable model Whether any material, temporal, or otherwise constructive in a layman's sense, notion might be useful Any recommended articles, areas of study, problems, related to these questions","['real-analysis', 'measure-theory', 'paradoxes']"
1726897,"Is $T(M)$ necessarily equivalent to the normal bundle of $M$ in $\textbf{R}^{2n}$, if $M$ is totally real?","Consider real submanifolds, $M^n \subset \textbf{C}^n$. ($\textbf{C}^n \equiv (\textbf{R}^{2n}, J)$, where $J(x, y) = (-y, x)$). $M^n$ is totally real if $J(T_pM) \cap T_pM = 0$, for all $p \in M$. Is $T(M)$ necessarily equivalent to the normal bundle of $M$ in $\textbf{R}^{2n}$, if $M$ is totally real?","['manifolds', 'differential-geometry', 'differential-topology', 'complex-geometry']"
1726905,Understanding what is the order of a meromorphic function,"I am reading a book on Complex Curves by Miranda. The book defined meromorphic function:
A function on an open set W of a Riemann surface X is said to be meromorphic at $p \in W$ if there exists a chart $\phi: U \rightarrow V$ where $p \in U$ such that $f \circ \phi^{-1}$ is either holomorphic or has removable singularity at $\phi(p)$ or a pole at $\phi(p)$. Then we defined the order of a meromorphic function f at p to be the minimal power of the Laurent series ie $ord_p(f)=\min\{n: c_n\}$ for the $\sum c_n(z-z_0)^n$.
So it was noted that this is independent of the chart. I am confused on what this order is. For example if we work with the Riemann Sphere and we have a $ord_p(f)=3$ what does this mean for f? I guess from what I have read, the $ord_p(f)$ is the order of the pole if it negative value. Is this what I should understand from the order function?","['abstract-algebra', 'complex-analysis', 'algebraic-geometry']"
1726912,Is $\mathbb{C}^n$ a C*-algebra?,"Hi I am new in learning C*-algebra. I know that under the usual $\mathbb{C}^n$-norm, $\mathbb{C}^n$ is a Banach Space. However, what will be an intuitive multiplication on $\mathbb{C}^n$ so that the usual $\mathbb{C}^n$-norm will be sub-multiplicative? In fact, I should prove that the usual $\mathbb{C}^n$-norm is a C* norm too. I was trying pointwise multiplication but I was stuck in the calculations. Edited(my calculations):
I used for $u=(u_1,u_2,...,u_n),v=(v_1,v_2,...,v_n)\in \mathbb{C}^n$, $uv= (u_1v_1,u_2v_2,...,u_nv_n)$. So I am trying to proceed in proving $\|uv\|_n \le \|u\|_n \|v\|_n$. In particular I am stuck in proving that $\|uv\|_n=\|(u_1v_1,u_2v_2,...,u_nv_n)\|_n \le \|u\|_n \|v\|_n$. What I mean is I don't know how to ""split"" $(u_1v_1,u_2v_2,...,u_nv_n)$ into relations of $(u_1,u_2,...,u_n)$ and $(v_1,v_2,...,v_n)$. Another edit: By the answers below, $\mathbb{C}^n$ will be not a C*-algebra under the usual $\mathbb{C}^n$-norm.","['abstract-algebra', 'operator-algebras']"
1726939,Is this intuition for the semidirect product of groups correct?,"My abstract algebra class introduced me to direct products, not semidirect products. I became interested in semidirect products when confronted with the following homework problem: Define the quaternion group $Q_8 = \{1, i, j, i \mid i^2 = j^2 = k^2 = ijk = -1\}$. Show that the set $\mathbb{H} = \{a + bi + cj + dk \mid a, b, c, d \in \mathbb{R}\}$ is a group under multiplication. Rather than checking all the properties, I wanted to use my intuition that $\mathbb{H}$ was a group because it ""combined"" $Q_8$ and $\mathbb{R}^4$, which were both groups. That is, any element of $\mathbb{H}$ can be uniquely specified by choosing an element of $Q_8$ and an element of $\mathbb{R}^4$. So I wrote down $\mathbb{H}$ is a group because it is isomorphic to $Q_8 \times \mathbb{R}^4$ which I now see is clearly wrong. Now, the Internet tells me (I think) that what I was actually thinking about was the semidirect product, not the direct product. So that's my first question: Is $\mathbb{H}$ a semidirect product of $Q_8$ and $\mathbb{R}^4$? Which acts on which (which belongs on the left side of $\rtimes$?) My second question: I'm forming the intuition that, given normal subgroup $N$ and subgroup $H$, one forms $G$ by: Defining an element of $g$ as a unique combination of an element of $N$ and an element of $H$. Giving rules for how multiplication between different elements $h_1n_1$ and $h_2n_2$ will work. If you choose: $$(h_1n_1)(h_2n_2) = (h_1h_2)(n_1n_2)$$ then it's just a direct product, but you might choose anything, such as: $$(h_1n_1)(h_2n_2) = (h_1h_2^{-1})(n_1n_2)$$ How inaccurate is this vague idea? In particular, is the above a valid choice for a way to multiply elements of $N \rtimes H$? What restrictions are there on these choices?","['finite-groups', 'semidirect-product', 'group-theory']"
1726960,Is the matrix norm of a matrix equal to the maximum of the norms of its Jordan block?,"Let $J$ be a Jordan block matrix with blocks $J_1,\cdots,J_n$. I came up with some examples of $J$ and noticed that $\|J\|=\max_{i=1,\cdots,n}\|J_i\|$. Does this result always hold? The norm I use here is the induced 2-norm . (I used Matlab for my examples. The norm would be the one that Matlab uses when I type norm(A).)","['matrices', 'normed-spaces', 'jordan-normal-form']"
1726968,Immersions are open maps if dimensions are equal,"Let $M,N$ be manifolds with $\dim M = \dim N$. If $f:M\to N$ is an immersion then $f$ is open. I thought that I have solved it, but then I thought there could be a mistake: Let $p\in M$. As $f$ is an immersion, $df_p$ is injective. Hence it is an isomorphism because $\dim M= \dim N$. By the Inverse Function Theorem (for manifolds), $f$ is a local diffeomorphism at $p$. Let $A\subseteq M$ be an open set. For every $p\in A$, let $U_p\subseteq A$ be an open set such that $f\restriction_{U_p}:U_p\to f(U_p)$ is a diffeomorphism. I thought that as $f\restriction_{U_p}$ is a diffeomorphism, it is a homemorphism, hence $f(U_p)$ is open and $f(A)$ is union of open sets. But then I remembered that $f(U_p)$ is only open in $f(U_p)$... which we already knew. I mean, $f\restriction_{U_p}$ is open as a function $U_p\to f(U_p)$, where $f(U_p)$ has the subspace topology, so that doesn't mean $f(U_p)$ is open in $N$. Right?","['differential-geometry', 'differential-topology']"
1726998,"Does every connected metric space , with more than one point , contains a path connected subset with more than one point ?","Does every connected metric space , with more than one point , contains a path connected subset with more than one point ? Is there any additional condition imposing which on the mother space will guarantee the existence of such non-trivial path connected subset ?  I know that every connected metric space , with more than one point , contains a proper connected subset with more than one point ; but I cannot make any headway if we want the subset to be path connected . Please help . Thanks in advance","['general-topology', 'metric-spaces', 'soft-question', 'connectedness']"
1727070,Why there isn't any integer solutions in non-zero integers for $z^3 = 3(x^3 +y^3+2xyz)$?,"Consider the following Diophantine equation $$z^3 = 3(x^3 +y^3+2xyz)$$ Is there any elementary proof for the non-solubility in non-zero integers for this  Diophantine equation, where the absolute value of $x, y$ and $z$ are pairwise coprime integers? I have proved the impossibility of solution of this Diophantine equation in non-zero integers, but that took quite a few pages and I consider this too long. I hope for a much more elementary and shorter proof for this puzzle. Hint: It should be noted that is also true for the non-availability of the similar form (replacing coefficient 3 by 1) as the following: $$z^3=x^3+y^3+2xyz,$$ but this case is very simple to prove, where $x, y\;\&\; z$ are nonzero integers.","['number-theory', 'diophantine-equations', 'elementary-number-theory']"
1727079,what is the probability space of gaussian random variable,"In the most text book of advance probability theory, they always start from a probability space $(\Omega, F, P)$, and introduce the corresponding measure theory, then use measurable function to define a random variable. However, the probability space part is always vague. When we define a new random variable, we always directly define it by its density function or mass function. However, we cannot see the connection between probability space and random variable anymore. My question is that can we always define a random variable from a probability space? For example, I want to define a Bernoulli distributed random variable. We can start from a prob. space that $\Omega=\{H,T\}$ and $P(H)=p, P(T)=1-p$, then define the measurable function between prob.space and $(R,B)$ as $X(T)=1$ and $X(H)=0$. Or a prob. space that $\Omega=[0,1]$, and $P$ is the Lebesgue measure, and $F$ the collection of Lebesgue measurable sets. Then define $X(\omega)=1$ if $\omega<0.5$ and so on. Exponential distributed random variable is still OK, because the uniform distributed random variable can be defined by a identity function on the prob.space mentioned above, and we can apply the inverse function of the CDF of exponential distribution to define a exponential distributed random variable. However, if we go further to define Gaussian random variable. Can we do similar procedure as above to define it? What can I image is that we can invoke CLT to define a Gaussian from a $\Omega =\{H,T\} $ or $\Omega=[0,1]$. But it is not very clear. Also, if we continue the thinking, how to construct a complicate stochastic process?","['probability', 'measure-theory']"
1727088,How do I show that $\sum_{k = 0}^n \binom nk^2 = \binom {2n}n$? [duplicate],"This question already has answers here : Combinatorial proof of summation of $\sum\limits_{k = 0}^n {n \choose k}^2= {2n \choose n}$ (9 answers) Closed 8 years ago . $$\sum_{k = 0}^n \binom nk^2 = \binom {2n}n$$ I know how to ""prove"" it by interpretation (using the definition of binomial coefficients), but how do I actually prove it?","['combinatorics', 'summation', 'binomial-coefficients']"
1727106,"Some doubts in the proof that $[0,1]$ is uncountable","There is this proof about the fact that the set $[0,1]$ is uncountable. I found the first few reasoning in the proof does not really make sense but I'm not sure whether I am correct. I will write the entire proof for reference: Suppose $a_1, a_2, \ldots$ was a complete list of real numbers in $[0,1]$. Assume $a_1\neq0$. Form a subsequence as follows: $b_1=a_1$, $b_2=a_{k(2)}$, $k(2)$ is the least integer $m>1$ such that $a_m<a_1$, $b_3=a_{k(3)}$, $k(3)$ is the least integer $m>k(2)$ such that $a_m>a_{k(2)}$, $b_4=a_{k(4)}$, $k(4)$ is the least integer $m>k(3)$ such that $a_m<a_{k(3)}$, and so on. Then $b_{2n}$ is an increasing sequence, $b_{2n}\to B=a_N$. Note $a_{2n}<B<a_{2l+1}$ $\forall n,l$. Suppose $k(n)<N<k(n+1)$. Then we would have chosen $b_{n+1}=aN=B$. Contradiction. What I think is wrong is the claim that $b_{2n}$ is an increasing sequence. A counterexample that I can think of is: $b_1=89.5, b_2=76.43, b_3=2937, b_4=2.3$. Obviously, $b_4<b_2$. Is my counterexample wrong? Could anyone please point my mistake? Thanks.","['proof-explanation', 'elementary-set-theory', 'proof-verification']"
1727140,Finding the Cartesian equation of an ellipse (Midpoints),"Question: The normal to the ellipse $ \frac{x^2}{25} + \frac{y^2}{9} = 1$ at a point $Q$ meets the coordinate axes at A and B respectively. As $Q$ varies, the locus of the midpoint of $AB$ is another ellipse. Find the Cartesian equation of this ellipse. What I have done Let $x = 5 \cos(\theta)$ and $y=3\sin(\theta)$ then $$ \frac{dx}{d\theta} = -5 \sin(\theta) , \frac{dy}{d\theta}=3cos(\theta)$$ $$ \frac{dy}{dx} = \frac{dy}{d\theta} *\frac{d\theta}{dx} $$ $$ \frac{dy}{dx} = \frac{-3\cos(\theta)}{5 \sin(\theta)}$$ $$ m_{normal} \cdot m_{tgt} = -1 $$ $$ m_{normal} = \frac{5\sin(\theta)}{3\cos(\theta)}$$ For equation of normal $$ y-y_1 = m(x-x_1) $$ $$ y - 3\sin(\theta)=  \frac{5\sin(\theta)}{3\cos(\theta)}(x-5\cos(\theta))$$ $$ y - 3\sin(\theta) = \frac{5\sin(\theta)}{3\cos(\theta)}x - \frac{25}{3}\sin(\theta)$$ $$ y= \frac{5\sin(\theta)}{3\cos(\theta)}x - \frac{16}{3} \sin(\theta) $$ At $$  y=0, x= \frac{16 \cos(\theta)}{5} $$ and $$ x = 0 , y = - \frac{16}{3} \sin(\theta)$$ So we have 2 points which lie on the new ellipse Hence the midpoint is given by $$\frac{1}{2} (X_1+X_2) $$ so $$ \frac{1}{2}(0 + \frac{16 \cos(\theta)}{5}) $$ Midpoint is $$ \frac{8\cos(\theta)}{5} $$ Now I am stuck , how should I continue?","['conic-sections', 'trigonometry']"
1727144,Probability that $5$ or $6$ are rolled $k$ times after $12$ rolls,"Suppose you roll a fair dice $12$ times in a row. What is the probability of the event ""exactly $k$ of the rolls are a $5$ or a $6$"" ? I'm just asking for some verification of my counting. Let $X$ be the random variable that counts the number of $5$ and $6$ rolled. $$\begin{align}\displaystyle P(X=k)&=P\left((X=k)\bigcap \left(\bigcup_{i=0}^k \text{5 rolled $i$ times}\right)\right)\\&=\sum_{i=0}^kp\left((\text{5 rolled $i$ times})\cap (\text{6 rolled $k-i$ times}) \right)\\&=\sum_{i=0}^k \frac{\binom{12}{i}\binom{12-i}{k-i}}{6^{12}}=\frac1{6^{12}}\binom{12}{k}2^k \end{align}$$ Is that right ? I think I'm supposed to find some Binomial distribution, so I must be wrong.","['combinatorics', 'probability']"
1727158,Using squeeze thorem find $ \lim_{n \to \infty}{\frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n}}$ [duplicate],"This question already has answers here : To show for following sequence $\lim_{n \to \infty} a_n = 0$ where $a_n$ = $1.3.5 ... (2n-1)\over 2.4.6...(2n)$ (5 answers) Closed 8 years ago . It is already solved here at Math.stackexchange, but we haven't learned Stirling's approximation (at our school), so can it be solved using only squeeze theorem?
$$\lim_{n \to \infty}{\frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n}}$$ My attempt, Let $y_n = \frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n}$, we see that $ \frac{1 \cdot 3 \cdot 5 \cdot ...\cdot (2n-1)}{2\cdot 4 \cdot 6\cdot ...\cdot 2n} > \frac{1 \cdot 2 \cdot 2 \cdot ... \cdot 2}{2\cdot 4 \cdot 6 \cdot ...\cdot 2n} = \frac{1}{1 \cdot 2 \cdots .. \cdot (n-1)\cdot2n} = x_n$ $$\lim_{n \to \infty}{x_n} = 0 $$
Now I just need to find a sequnce $z_n>y_n$, so, $\lim_{n \to \infty} z_n = 0$.","['real-analysis', 'sequences-and-series']"
1727200,Compute weight of a point on a 3D triangle,"Let's say I have a 3D triangle $ABC$ with $x$, a random point on it, I know the coordinates of each one of the points. Each of $A$, $B$ and $C$ have a ""weight"" which is a decimal value between 0 and 1 (actually the color value of the point on a black and white gradient). What I want to do is to find the precise weight of $x$, knowing that the weight interpolation between the points is linear. What formula should I use?","['3d', 'geometry']"
1727226,Several different positive integers lie strictly between two successive squares. Prove that their pairwise products are also different.,"Several different positive integers lie strictly between two successive squares. Prove that their pairwise products are also different. Let the numbers be $n$ and $n+1$. So, their squares are $n^2$ and $n^2 + 2n+1$. I haven't found an idea on how to solve this. Could you suggest some ideas or give hints on how to solve this? Thanks.","['number-theory', 'elementary-number-theory']"
1727231,Does $ \int_a^b |f(x) - f_1(x)| = 0$ imply $ \int_a^b |f(x) - f_1(x)|^2 = 0$?,"Context:I'm trying to solve this problem: Suppose $f, f_1, g, g_1$ all Riemann integrable complex valued functions on $[a, b]$ such that $f \sim f_1$ and $g \sim g_1$ . Prove $\langle f, g \rangle = \langle f_1, g_1 \rangle$ . In this problem, $f \sim f_1$ means $ \int_a^b |f(x) - f_1(x)| = 0$ , $\langle f, g \rangle$ is the Hermitian scalar product $ \int_a^b f(x) \overline{g(x)}$ , and $\|f\| = \sqrt{\langle f, f \rangle}$ . I tried saying that \begin{align}\langle f, g \rangle &= \langle f - f_1, g \rangle + \langle f_1, g \rangle\\[0.2cm]&= \langle f - f_1, g \rangle + \overline{\langle g, f_1 \rangle}\\[0.2cm]&= \langle f - f_1, g \rangle + \overline{\langle g - g_1, f_1 \rangle} + \overline{\langle g_1, f_1 \rangle}\\[0.2cm]&= \langle f - f_1, g \rangle + \langle f_1, g - g_1 \rangle + \langle f_1, g_1 \rangle\end{align} And I wanted to say that the first two terms were $0$ by the Cauchy Schwarz Inequality, by saying $$|\langle f - f_1, g \rangle| \leq \|f-f_1\| \cdot \|g\|$$ and that $\|f - f_1\|$ is $0$ , so the left side is too. So that's why I'm wondering if there's a way to show that $$\int_a^b |f(x) - f_1(x)| = 0 \implies \int_a^b |f(x) - f_1(x)|^2 = 0$$","['normed-spaces', 'integration', 'fourier-analysis']"
1727237,Lebesgue differentiation theorem in weighted Lebesgue spaces,"It is well known that $$\lim\limits_{r\rightarrow 0}\dfrac{\|f\chi_{B(x,r)}\|_{L_{p}(\mathbb{R}^{n})}}{\|\chi_{B(x,r)}\|_{L_{p}(\mathbb{R}^{n})}}=|f(x)|$$ for almost all $x\in\mathbb{R}^{n}$ and $1\leq p< \infty$. Here $\chi_{B(x,r)}$ denotes the characteristic function of the open ball $B(x,r)$. I wonder whether there is an analogue of this property in weighted Lebesgue spaces, that is, $$\lim\limits_{r\rightarrow0}\dfrac{\|f\chi_{B(x,r)}\|_{L_{p}^{w}(\mathbb{R}^{n})}}{\|\chi_{B(x,r)}\|_{L_{p}^{w}(\mathbb{R}^{n})}}=|f(x)| $$ for almost all $x\in\mathbb{R}^{n}$ ? More generally, is there a version of Lebesgue differentiation theorem for general measures?","['functional-analysis', 'real-analysis', 'measure-theory']"
1727265,Eigenvalues of $3 \times 3$ block matrix,"What are the eigenvalues of the following block matrix? $$\begin{bmatrix}
A & I_n & I_n \\ 
I_n & I_n & O_n \\
I_n & O_n & I_n 
\end{bmatrix}$$ Here, $A$ is any square matrix of order $n$ whose eigenvalues are denoted by $\lambda_1, \lambda_2, \dots, \lambda_n$ , $I_n$ is an identity matrix of order $n$ and $O_n$ is a zero matrix of order $n$ .","['matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
1727281,Evaluate the sum with special function [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Evaluate the following sum: $$\sum \limits_{n=1}^{\infty}\dfrac{2^{<n>}+2^{-<n>}}{2^n}$$ where $<n>$ the nearest integer to $\sqrt{n}$.","['number-theory', 'sequences-and-series']"
1727339,Jordan form exercise,"What am I doing wrong? I've been learning how to put matrices into Jordan canonical form and it was going fine until I encountered this $4 \times 4$ matrix: $A=\begin{bmatrix} 
2 & 2 & 0 & -1 \\ 
0 & 0 & 0 & 1 \\
1 & 5 & 2 & -1 \\
0 & -4 & 0 & 4 \\
\end{bmatrix}
$ Which has as only eigenvalue $\lambda_1=\lambda_2=\lambda_3=\lambda_4=2$ with 2 corresponding eigenvectors, which I will for now call $v_1$ and $v_2$: $v_1 = \pmatrix{0\\0\\1\\0}, v_2=\pmatrix{-3 \\ 1 \\ 0 \\ 2}
$ 2 eigenvectors means 2 Jordan blocks so I have 2 possibilities: $J= \pmatrix{2 & 1 & 0 & 0 \\ 
0 & 2 & 1 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 2} $ or $ J= \pmatrix{2 & 1 & 0 & 0 \\
0 & 2 & 0 & 0 \\
0 & 0 & 2 & 1 \\
0 & 0 & 0 & 2}
$ I consider the first possibility. This gives me the relations: $Ax_1=2x_1 \\
Ax_2=x_1+2x_2 \\
Ax_3=2x_3+x_2 \\
Ax_4=2x_4 \\
$ where $x_1$ and $x_4$ should be $v_1$ and $v_2$. From the second relation $(A-2I)x_2=x_1$ I see $\pmatrix{0 & 2 & 0 & -1 \\
0 & -2 & 0 & 1 \\
1 & 5 & 0 & -1 \\
0 & -4 & 0 & 2} \pmatrix{a \\ b \\ c \\ d} =\pmatrix{0 \\ 0 \\ 1 \\ 0} $ ( $v_2= \pmatrix{ -3 \\ 1 \\ 0 \\ 2}
$ will give an inconsistent system) Now I get that $x_2 = \pmatrix{-2 \\ 1 \\ 0 \\ 2}
$ From the third relation $(A-2I)x_3=x_2$: $\pmatrix{0 & 2 & 0 & -1 \\
0 & -2 & 0 & 1 \\
1 & 5 & 0 & -1 \\
0 & -4 & 0 & 2} \pmatrix{e \\ f \\ g \\ h} =\pmatrix{-2 \\ 1 \\ 0 \\ 2} $ But this system is inconsistent as well! No matter which vectors I try in which places, when I try to generalize eigenvectors I seem to always end up with some inconsistency. Is there something staring me in the face that I am overlooking? Or am I doing it completely wrong (even though this method worked fine for me before)? Sorry for the lengthiness and thank you in advance.","['matrices', 'jordan-normal-form', 'linear-algebra']"
1727361,Is there an example of a non von Neumann algebra with this property?,"What is  an example  of a  $C^{*}$  subalgebra  $A$ of  $B(H)$ such that $A$ contains the  identity $I_{H}$ and satisfies the following properties: 1) For every $T\in A$, The  orthogonal projection $\pi_{T}$ on the  closure of $Range(T)$ belongs to $A$. but 2)  $A$ is  not  a  Von  Neumann  Algebra. The  question is  motivated by the fact that every  Von Neumann  algebra satisfies (1).","['c-star-algebras', 'operator-theory', 'functional-analysis', 'von-neumann-algebras', 'operator-algebras']"
1727376,Find formula for $\frac{1}{\sqrt 1}+ \frac{1}{\sqrt 2}+\cdots+\frac{1}{\sqrt n}$,"I have the series: $$\frac{1}{\sqrt 1}+ \frac{1}{\sqrt 2}+\cdots+\frac{1}{\sqrt n}$$ I find hard to generalize into one formula, any explanation would be helpful.","['radicals', 'summation', 'sequences-and-series']"
1727401,Why do we take the domain of $f(x)/g(x)$ as $\mathbb{R} - \{0\}$ rather than $\mathbb{R}$ when $f(x) = x$ and $g(x) = 1/x$?,"Let $f(x)=x$ and $g(x)=\frac{1}{x}$, Domain$(f)=\mathbb{R}$ and Domain$(g)= \mathbb{R}-\{0\}$. We have to find the domain of $\frac{f(x)}{g(x)}$. When we solve this expression, as the $x$ of $g(x)$ would go to the numerator, we would get the final term as $x^2$. As $x^2$ is defined for all $\mathbb{R}$, the domain should be $\mathbb{R}$ for $\frac{f(x)}{g(x)}$. Then why do we take the domain as $\mathbb{R}-\{0\}$ instead of $\mathbb{R}$?",['functions']
1727571,Geometric interpretation of the determinant of a complex matrix,"A complex $n$-dimensional vector space $V$ can be thought of as a real $2n$-dimensional vector space equipped with a map $J:V \to V$ with $J^2 = -I$. Complex-linear maps are then linear maps $V \to V$ which commute with $J$. One can think of $J$ as an infinitesimal rotation, so that $\exp(tJ)$ gives a family of rotations of this space, and $\mathbb R$-linear maps $V \to V$ are complex-linear if they respect this family. From this point of view, or some other geometric point of view, is there a nice interpretation of the complex determinant $\det_{\mathbb C} L$ of a complex-linear map $L: V \to V$? Or, almost the same question, is there a geometric interpretation of the unique (up to scaling by complex numbers) antisymmetric complex-multilinear $n$-form $\operatorname{vol}_{\mathbb C}: V \times V \times ... \times V \to \mathbb C$? The norm is fairly easy to interpret. $| \det_{\mathbb C} L |^2 = |\det_{\mathbb R} L|$. One way to see this is to look at the diagonalization of $L$ over $\mathbb C$. This also gives you a way to interpret the argument, as the total amount of rotation in all the invariant subspaces of $L$. Is there a geometric interpretation of $\det_{\mathbb C} L$, not just its norm, which does not require one to diagonalize the matrix first? Even the special case when $L$ is unitary is of interest.","['complex-geometry', 'linear-algebra', 'complex-numbers', 'multilinear-algebra']"
1727574,Prove a probability inequality,"Let $X,Y\sim\phi(x)$ be i.i.d. ($\phi:\Bbb R\to\Bbb R_{\ge 0}$ denotes the PDF), show that 
  $$P(|X+Y|\le1)\le2P(|X-Y|\le1).$$ My thoughts are standard, 
$$P(|X+Y|\le1)=\int_\Bbb R \rm dx\phi(x)\int_{-1-x}^{1-x}\rm dy\phi(y)=\int_\Bbb R \rm dx\phi(x)(\Phi(1-x)-\Phi(-1-x)),$$
in which $\Phi$ denotes CDF. And
$$P(|X-Y|\le1)=\int_\Bbb R \rm dx\phi(x)\int_{x-1}^{x+1}\rm dy\phi(y)=\int_\Bbb R \rm dx\phi(x)(\Phi(x-1)-\Phi(x+1)).$$
Then naturally,
$$2P(|X-Y|\le1)-P(|X+Y|\le1)=\int_\Bbb R\rm dx\phi(x)(2(\Phi(x+1)-\Phi(x-1))-\Phi(1-x)+\Phi(-1-x)). $$
Then I don't know what to do. Am I even on the right track? (I slightly doubt it because my professor labelled this problem as ""hard"" so I think it's kinda resistant to standard approaches. ) By the way, $2$ is said to be optimal here.","['real-analysis', 'calculus', 'probability-theory']"
1727590,AB and BA have identical nonsingular Jordan blocks,If A and B are square matrices of the same size I know how to show that AB and BA have the same eigenvalues and characteristic polynomials. But I want to show that they have identical nonsingular Jordan Blocks. I am not really sure how to proceed. Maybe some argument about the dimension of the kernel of AB and BA?,"['matrices', 'jordan-normal-form', 'linear-algebra']"
1727647,"Identity in general relativity, not sure if true or not","Let $(M, g_{ab})$ be a spacetime and define a new metric, $\tilde{g}_{ab}$, on $M$ by $\tilde{g}_{ab} = \Omega^2 g_{ab}$, where $\Omega$ is a smooth, positive function. Let $\nabla_a$ denote the derivative operator associated with $g_{ab}$ and let $\tilde{\nabla}_a$ denote the derivative operator associated with $\tilde{g}_{ab}$. Let $v^a$ be an arbitrary smooth vector field on $M$. Do we necessarily have the following identity:$$\tilde{\nabla}_a v^b = \nabla_a v^b = {\delta^b}_a v^c \nabla_c \text{ln}\,\Omega - v^b \nabla_a \text{ln}\,\Omega - g_{ac} v^c g^{bd} \nabla_d \text{ln}\,\Omega?$$","['riemannian-geometry', 'general-relativity', 'physics', 'mathematical-physics', 'differential-geometry']"
1727653,Number of Ways to Fill a Matrix with symbols subject to Weird contsraint.,"Got this problem from a friend and would like some pointers if anyone has any. Fill an $n \times n$ matrix with the symbols $\{a,b,c,d\}$ subject to the following: Only the symbol $a$ appears to the left or above an $a$ Only the symbol $b$ appears to the right or above a $b$ Only the symbol $c$ appears to the left or below a $c$ Only the symbol $d$ appears to the right or below a $d$ Here are some examples of a valid fillings:
$$\left( \begin{matrix} a & a & b  & b \\ a & a & b & b \\ c & c & c & d \\  c & c & c & d  \end{matrix}\right) $$ $$\left( \begin{matrix} a & a & a  & a & a & b  & b \\ a & a & a  & a & a & b  & b \\ a & a & a & d & d & d & d \\  a & a & a & d & d & d & d\\a & a & d & d & d & d & d\\c & c & d & d & d & d & d\\  c & c & d & d & d & d & d  \end{matrix} \right) $$ What I have done: We'll it is clear to me that the $a$'s (for example) make up a sort of Young Diagram just by the property above. But there is more to the problem than that. If your YT for $a$ is just a square, then you might have a reversed (right justified) YT available to place for the $b$'s. Details like this seem to make this a tricky problem for me. I am intrigued by this problem and I just thought I'd share it here in case there is some easy way to do this counting problem that I've never heard of. Thanks,",['combinatorics']
1727699,"Is it true or false that $\lvert P([0, 1]) \setminus P((0, 1)) \rvert > \lvert P(\mathbb{N}) \rvert$","Is it true or false that $\lvert P([0, 1]) \setminus P((0, 1)) \rvert > \lvert P(\mathbb{N}) \rvert$ ? I need a hint because I don't see a way to solve this",['elementary-set-theory']
1727723,How useful are non-square matrices in maths or sciences?,"I know that a matrix will be either square or rectangular matrix. I know that square matrices are used to solve a system of linear equations. But what's the use of rectangular matrices, why do we study them? Are they used somewhere in Math or science? Please answer my questions.","['matrices', 'soft-question']"
1727735,Upper bound for $\sum_{n=1}^xn^{k-1}$,"From some Calculus and guess-work, I found that $$k\sum_{n=1}^xn^{k-1}<(x+\frac12)^k\tag1$$ In fact, I found that it was very, very, close. And, from even more guesswork, $$\lfloor(x+\frac12)^k\rfloor-1\le k\sum_{n=1}^xn^{k-1}\le\lfloor(x+\frac12)^k\rfloor\tag2$$ I want to know if what I have is actually correct, because if so, then this is a great bound to the summation and is off by $\pm1$, which I find the summation for large $k$ has $\lfloor(x+\frac12)^k\rfloor-1=k\sum_{n=1}^xn^{k-1}$, but I have no way of proving this. EDIT $k\in\mathbb{R}$ It appears that $(2)$ becomes false for decently large $k,x$, as Slade has shown.  It fails at $k=3,x=8$. However, I would like to know if $(1)$ is still provable and whether or not it works. Also, for what values of $k$ does $(2)$ start to fail?  I can see from my calculator that it starts to fail around $k=2.4$, which I managed to input $x=1000$, but to see if a number like $k=2.3$ fails would require my calculator to manage some very large numbers... and that's not quite a proof. I can also see that it doesn't fail for $k=2$ because of Faulhaber's formula, but I'm not quite sure about $k=2.3$ or something.","['algebra-precalculus', 'summation', 'calculus']"
1727742,No duplicate: Is there a set $X$ such that $X=X^X$?,"Given two sets $A$ and $B$ we define $B^A$ to be the set of all functions from $A$ to $B$ . Question. Is there a set $X$ with $X=X^X$ ? One may ask the question: Why the hell do I have to ask this question twice? Yes, I already asked this question on math.stackexchange and I got an answer. The argument for proving that there is not a set $X$ with $X=X^X$ goes as follows: Recall that a function $X\to X$ is really a subset $f\subseteq X\times X$ satisfying the requirement $$\forall a\in X.\ \exists! b\in X.\ (a, b)\in X.$$ Also, note that a pair $(a, b)$ is defined as the set $\{\{a\}, \{a, b\}\}$ . Now, suppose there is an $X$ with $X=X^X$ . Since $\emptyset$ is obviously not the same as the set $\emptyset^{\emptyset}=\{\mathrm{id}_\emptyset\}$ and for $X$ with $|X| \geq 2$ we have $|X^X| \geq |2^X|> |X|$ and therefore $X\not = X^X$ , we can infer that $|X|=1$ . Let $X=\{\bullet\}$ . This unique element $\bullet\in X$ has to be equal to the function $\mathrm{id}_X$ because we supposed $X$ to be the same as $X^X = \{\mathrm{id}_X\}$ . According to our definitions $$\mathrm{id}_X=\{(\bullet, \bullet)\}=\{\{\{\bullet\}, \{\bullet, \bullet\}\}\}=\{\{\{\bullet\}\}\}.$$ But we have already said that $\mathrm{id}_X=\bullet$ . Hence $\bullet=\{\{\{\bullet\}\}\}$ which is forbidden by the axiom of regularity . Conclusion: If one codes functions as special subsets of the cartesian product and pairs as special sets, my question has an easy solution with the help of the axiom of regularity. But in my opinion, these encodings are unnatural and do not convey the true nature of functions and pairs. That is why I wonder how to answer my question above when one regards functions and pairs as non-sets , that is to say as objects that are not sets. Remark. I was not able to edit my old question because I asked it with a quest account and I got automatically logged out. By the way: In his book ""Analysis 1"", Terry Tao proposed a natural axiom system of set theory with non-sets and in particular he comments: ""Strictly speaking, functions are not sets, and sets
are not functions"" My question also can be viewed as asking if the problem whether there is a set $X$ with $X=X^X$ is decidable in the axiom system proposed by Terry Tao.",['elementary-set-theory']
1727777,Multivariable Limit,"I am trying to  find the following limit:
$$\lim_{(x,y) \to (0,0)} \frac{\ln(e+3x^2+3y^2+x^2y^3+x^3y^2)-\sqrt{1+2x^2+2y^2+x^4+y^4}}{\ln(1+x^2+y^2+x^4+y^4)}$$ I solved the limit along $x=0$ , $y=0$ and $x=y$ and I got $\frac{3}{e} -1$ for all of them. I then used wolfarm to check whether the limit exists or not , it exist and equals to the value mentioned above, but I'm not able to find a way to show that along all the paths to $(0,0)$ the limit equals to $\frac{3}{e} -1$","['multivariable-calculus', 'calculus', 'limits']"
1727808,3D Analogue of a Catenary,"When a cable is supported at its ends and droops due to its own weight, the resulting curve is called a catenary . However, is there a three-dimensional analogue of this shape? For example, let's say I take the unit square $([0,0,0], [1,0,0], [0,1,0], [1,1,0])$, support it at these points, and fill it with a uniform material. It will droop due to its own weight. I have a few questions about the resulting surface: Do the edges turn into local catenaries? Is there a parametric function that describes this shape? Is there a name for this shape? Any information would be much appreciated!","['surfaces', 'geometry']"
1727828,Use the definition of a limit to prove that $\lim_{y \to 0} y^3 = 0$.,"Attempt: The limit $\lim_{y \to 0} y^3 = 0$ exists if: $$\forall\ \epsilon >0 \ \exists\ \delta >0 \ \forall y \ |y-0|  < \delta \Longrightarrow |y^3 - 0|< \epsilon.$$ Now, I came up with the idea to use $\delta=\sqrt[3]{\epsilon}$. Would this be allowed and would it work?","['proof-verification', 'limits']"
1727834,Sheaf cohomology via resolutions vs. derived categories,"So I know that when introducing sheaf cohomology, there are two main approaches via derived categories, and a perhaps more ""down to earth"" method of resolving by acyclic, fine, soft, sheaves.  I'm curious exactly how these two approaches are related and what the benefits are of one over the other. For a nice topological space $X$, in the category $\rm{Sh}(X)$ of sheaves on $X$, we can search for certain objects in the category like fine, acyclic, soft, sheaves and find resolutions $0 \to \mathcal{F} \to \mathcal{C}^{0} \to \mathcal{C}^{1} \to \ldots$ of a sheaf $\mathcal{F}$.  The sheaf cohomology is then given by $H^{q}(X, \mathcal{F}) \simeq H^{q}(X, \mathcal{C}^{*}(X))$. To my very naive eye, in the derived categories approach, we can think of these resolutions as quasi-isomorphisms $\mathcal{F} \simeq [\mathcal{C}^{0} \to \mathcal{C}^{1} \to \ldots]$ in the derived category $\mathcal{D}\rm{Sh}(X)$ of sheaves on $X$.  Of course, they give rise to isomorphic cohomology. What I'm unsure about, is one of these methods superior to the other?  Is the method using resolutions merely a ""shadow"" of something much more powerful and general in derived categories?  In my naiveté, it seems like in both methods you have to get your hands dirty and search for possibly elusive objects in some category: acyclic objects in $\rm{Sh}(X)$, or quasi-isomorphisms in $\mathcal{D}\rm{Sh}(X)$.","['algebraic-geometry', 'sheaf-cohomology', 'homological-algebra', 'category-theory', 'abelian-categories']"
