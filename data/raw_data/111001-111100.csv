question_id,title,body,tags
1599205,Why is there only one term on the RHS of this chain rule with partial derivatives?,"I know that if $u=u(s,t)$ and $s=s(x,y)$ and $t=t(x,y)$ then the chain rule is $$\begin{align}\color{blue}{\fbox{$\frac{\partial u}{\partial x}=\frac{\partial u}{\partial s}\times \frac{\partial s}{\partial x}+\frac{\partial u}{\partial t}\times \frac{\partial t}{\partial x}$}}\color{#F80}{\tag{A}}\end{align}$$ A short extract from my book tells me that: If $u=(x^2+2y)^2 + 4$ and $p=x^2 + 2y$ then $u=p^2 + 4$ therefore $$\frac{\partial u}{\partial x}=\frac{\partial u}{\partial p}\times \frac{\partial p}{\partial x}\tag{1}$$ as $u=u(x,y)$ and $p=p(x,y)$ The book mentions no origin of equation $(1)$ and unlike $\color{#F80}{\rm{(A)}}$ is has only one term on the RHS; So I would like to know how it was formed. Is $(1)$ simply equivalent to $\color{#F80}{\rm{(A)}}$ but with the last term missing? Or is there more to it than that? Many thanks, BLAZE.","['derivatives', 'partial-derivative', 'chain-rule', 'calculus']"
1599247,"In Borel-Cantelli lemma, what is the limit superior?","In a proof of the Borel-Cantelli lemma in the stochastic process textbook, the author used the following. $$\limsup_{n\to\infty}A_n=\bigcap_{n\ge1}\bigcup_{k\ge n} A_k$$ Can someone explain why lim sup is intersection and union? Thank you","['probability-theory', 'limsup-and-liminf', 'borel-cantelli-lemmas']"
1599262,How is substituting $-y$ for $y$ with $y \neq 0$ legal?,"I'm currently going through Spivak's Calculus, and I got across the following problem: Prove that $x^3+y^3=(x+y)(x^2-xy+y^2)$. To prove it, the author wants to use another problem that was proved earlier, i.e $x^3-y^3 = (x-y)(x^2+xy+y^2)$ The solution says to simply replace $y$ with $-y$ in the above equation. But $y$ can't be equal to $-y$ as long as $y \neq 0$, so how is this a legal operation?",['algebra-precalculus']
1599265,cumulant of infinite sum of random variables,"Could you help me the following question? Let $X_i$ are identical independent random variables. Putting $Z:=\sum_{i=1}^{\infty}X_i$. Which conditions do we have
$$k_n(Z)=\sum_{i=1}^{\infty}k_n(X_i),$$
where $k_n(Z)$ is the $n$-th cumulant of $Z$. How to prove this identity or any book mentions to this result?
Thank you very much!","['stochastic-analysis', 'statistics', 'probability']"
1599288,Area of the shadow of a regular polygon inscribed in a sphere.,Consider the situation given below: Let a regular polygon be inscribed in a sphere such that its circumcentre is at a distance $r$ from the centre of the sphere of radius $R$. A point source of light is kept at the centre of the sphere. How can we calculate the area of the shadow made on the surface of the sphere. I tried to use the relation: $ \Omega = \frac{S}{R^2} $ But of course that is the case when a circle would be inscribed. So can I somehow relate it for any general polygon?,"['area', 'geometry']"
1599294,Prove a fact about a mapping of complete poset to itself,"Let M be a poset such that any its totally ordered subset has an upper bound: $\forall U \subseteq M$, $U$ is totally ordered $\exists m \in M: \forall u \in U \ \ u \leq m$. And let $f: M \to M$ be a mapping such that $\forall x \in  M \ \ x \leq f(x) $ 
Prove that $\exists x_0 \in M: f(x_0)=x_0$ Looks a little scary for me, not sure how to approach the problem.","['order-theory', 'elementary-set-theory']"
1599320,"The initial value problem $y'=\sqrt {y}, \:\: y(0)=\alpha$.",The initial value problem $$\begin{cases}y'=\sqrt {y}\\y(0)=\alpha\end{cases}$$ has $A.$ At least two solutions if $\alpha =0$ $B.$ No solution if $\alpha > 0$ $C.$ At least one solution if $\alpha > 0$ $D.$ A unique solution if $\alpha = 0$ ATTEMPT I solved the differential equation as $y(x) = \frac{x^2}{4} + \alpha +x\sqrt {\alpha}$ How do i choose correct options?,['ordinary-differential-equations']
1599335,upper bound for $ \mathbb{E}[X] \cdot \mathbb{E}[1/X] $ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question This is part of problem 2.6.46 in Shiryaev's 'problems in probability'.
Let $X$ be a random variable which takes values in the interval $[a,b]$ for some $0<a<b<\infty$. Prove that $$ \mathbb{E}[X]\cdot\mathbb{E}\!\left[\dfrac{1}{X}\right]\leq \dfrac{(a+b)^2}{4ab}. $$ Edit I study alone from the books of Shiryaev: 'probability' and 'problems in probability'. After reading section 2.6 I know what are Random Variables, Lebesgue Integral, Expectation. I also know the inequalities of Chebyshev, Cauchy-Bunyakovskii, Jensen, Lyapunov, Holder and Minkowski, but I don't see how any of these help me with this question. I might be missing something very simple, but for now I have no idea where to start.",['probability-theory']
1599336,How do I find a meagre dense subset of $\mathbb{R}^2$?,"How do I find $M \subset \mathbb{R}^2$ which is meagre and dense? If I understand the definitions correctly, I need  $$M=\bigcup_{k \in \mathbb{N}} A_k, \quad {\overline M}=\mathbb{R}^2.$$ Trying $M=\mathbb{Q}^2$ seems to work, $A_k$ being a singleton set is rare in $\mathbb{R}^2$ for each $k$. Is this correct?","['real-analysis', 'analysis', 'proof-verification']"
1599363,$\frac{2^{2n}}{2n} \le {2n \choose n}$?,"Show that $\frac{2^{2n}}{2n} \le {2n \choose n}$, $n\in\mathbb{N}$. What I can see easily is that $\frac{2^{2n}}{2n + 1} \le {2n \choose n}$, since
$(1+1)^{2n} = \sum_{m=0}^{2n}{2n \choose m} = 2^{2n}$. This sum contains $2n+1$ terms with the largest being ${2n \choose n}$. So ${2n \choose n}$ is larger than $\frac{2^{2n}}{2n + 1}$. However I have found the statement $\frac{2^{2n}}{2n} \le {2n \choose n}$, which is not obvious to me.","['combinatorics', 'real-analysis']"
1599371,Swapping partial derivative and curl operator,"The third Maxwell equation states that
$$\nabla \times \mathbf E = -\frac{\partial\mathbf B}{\partial t}.$$ Then I have in my notes:
$$\nabla \times (\nabla \times \mathbf E) = -\nabla \times \frac{\partial\mathbf B}{\partial t} \color{red}{=} -\frac{\partial}{\partial t}\nabla \times \mathbf B.$$ I don't understand why it's possible to swap the two operators. What are the conditions that have to be satisfied in order to do so?","['multivariable-calculus', 'vector-analysis']"
1599393,Line integral of a vector field,"$$\int_{\gamma}ydx+zdy+xdz$$ 
given that $\gamma$ is the intersection of $x+y=2$ and $x^2+y^2+z^2=2(x+y)$ and its projection in the $xz$ plane is taken clockwise. In my solution, I solved the non-linear system of equations and I found that $x^2+y^2+z^2 = 4$. Given the projection in the $xz$ plane is taken clockwise, the parametrization is: $\gamma(t) = (2\sin t, 2 - 2\sin t,2\cos t)$, $0<t<2\pi$. But when I evaluate this integral, I keep getting wrong results: $\int_{0}^{2\pi}[(2-2\cos t)2\cos t+2\cos t(-2\cos t)+2\sin t(-2\sin t)]dt$, I've only susbstituted $x,y,z$ and $dx,dy,dz$; which leads me to $-8\pi$, but the answer on my textbook is $-2\pi \sqrt{2}$. Is there any conceptual mistake in my solution?","['multivariable-calculus', 'integration', 'line-integrals']"
1599447,Asymptotic of an integral involving sine and erf,"In a recent paper with two colleagues http://arxiv.org/pdf/1408.5720.pdf we derived a certain integral ruling the distribution of eigenvalues spacings for a model of random matrices. The integral reads
$$
J_M(s)=\int_0^\infty dt\ \frac{\sin(s t)}{t^{M-1}}[\mathrm{erf}(t)]^M\ ,
$$
where $\mathrm{erf}(x)=(2/\sqrt{\pi})\int_0^x\ e^{-y^2}dy$ is the Error function. $M$ is a large positive parameter. I am somehow struggling to find the asymptotic behavior of this integral as $M\to +\infty$. A saddle-point approach seemed a natural candidate, but somehow the oscillating part is getting in the way (unless I am making a silly mistake). Any help would be much appreciated.","['asymptotics', 'integration', 'definite-integrals']"
1599465,Simultaneous diagonalization of two symmetric matrices vs. diagonalization of one nonsymmetric matrix,"In physics, when considering the motion of a system with $N$ degrees of freedom described by vector $x$, the linearized equations of motion take the form $$M \ddot{x} = - K x.$$ Here, $M$ is a symmetric (in most cases diagonal), positive definite matrix, and $K$ is a symmetric, (in general) indefinite matrix. Using the standard ansatz $x(t) \propto e^{i\omega t}$, we have $\ddot{x} = -\omega^2 x$, which, in turn, leads to the eigenvalue equation $$\omega^2 M x = K x.$$ Usually, this problem is solved by simultaneously diagonalizing both $M$ and $K$. However, given that $M$ is positive definite, wouldn't it make more sense to write the eigenvalue problem as $$M^{-1} K x = \omega^2 x$$ and solve it the usual way? Wouldn't the solutions (eigenvalues and eigenvectors) necessarily be the same?","['matrices', 'diagonalization', 'eigenvalues-eigenvectors']"
1599486,On the number of Goldbach partitions,"http://members.chello.nl/k.ijntema/partitions.html?text1=8&area1=You+entered%3A+6%0D%0ANumber+of+Goldbach+partitions+%3D+1%0D%0A%0D%0AGoldbach+partitions%3A%0D%0A3+%2B+3+++%0D%0AEnd Here you can find the number of Goldbach partitions of an even number. $r(190)=8$, $r(50)= 5$ and so on. Now $r(4)=r(6)=r(8)=r(12)=1$. Here is my question. Is there any $n>6$ which yields $r(2n)=1$. I tried all $n$ values for $n<96$ and didn't found any. And is there any research on this function's behaviour?","['number-theory', 'prime-gaps', 'goldbachs-conjecture', 'prime-numbers']"
1599493,How does Variance become an Autocorrelation Function?,"""For a Gaussian stochastic process $X=\{X(t)|-\infty<t<\infty\}$ with mean function $\mu(t)=0$ for all $t$, its autocorrelation function is 
$$
E(X(t)\cdot X(s))=R(h)=\max(0,1-|h|), h=t-s.
$$
Compute the characteristic function of 
$$
Y=X(t)-X(t-0.5).""
$$
Since I know it is Normal distributed (Gaussian), I only have to find the mean and variance of $Y$ and then find the corresponding characteristic function. I have found the mean by
$$
E(Y)=E(X(t)-X(t-0.5))=E(X(t))-E(X(t-0.5))=\mu(t)-\mu(t-0.5)=0-0=0.
$$ 
However I am stuck on computing the variance, and only get that:
$$
Var(Y)=Var(X(t)-X(t-0.5))=Var(X(t))+Var(X(t-0.5))-2Cov(X(t),X(t-0.5)).
$$
I know that 
$$
Var(X(t))=E(X(t)^2)-E(X(t))^2=E[X(t)\cdot X(t)]-\mu(t)^2=E[X(t)\cdot X(t)]-0^2
$$
But the answer states that
$$
Var(Y)=R(0)+R(0)-2R(0.5)=1+1-2\cdot 0.5=1
$$
How do I get there and does this imply that I am supposed to see that $t=s$ (so that $E(X(t)^2)=R(h)$)?
Any help is much appreciated. Edit: Also, how do I see that $h=t-s=0$ and $h=0.5$ in $R(0)$ and $R(0.5)$ respectively?","['stochastic-processes', 'variance', 'probability', 'correlation']"
1599508,Hypersurfaces meet everything of dimension at least 1 in projective space,"The following exercise is taken from ravi vakil's notes on algebraic geometry. Suppose $X$ is a closed subset of $\mathbb{P}^n_k$ of dimension at least $1$, and $H$ is a nonempty hypersurface in $\mathbb{P}^n_k$. Show that $H\cap X \ne \emptyset$. The clue suggests to consider the cone over $X$. I'm stuck on this and I realized that i'm at this point again where i'm not sure how a neat formal proof of this should look like. Thoughts: Does a hypersurface in projective space mean $H=V_+(f)$, the homogeneous primes not containing $f$? If $X \hookrightarrow Proj(S_{\bullet})$ is a closed embedding then it corresponds (before taking $Proj$(-)) to a surjection of graded rings $S_{\bullet} \to R_{\bullet}=S_{\bullet}/I_+(X)$ where $I_+(X)$ is the set of all homogeneous elements vanishing on $X$. The cone $C(X)$ over a $X$ is then obtained by taking the $Spec(-)$ of this morphism $C(X) \hookrightarrow Spec(S_{\bullet})$. Is that right? How can this help me prove the theorem above? I got the feeling so far that there's a very elegant way to describe all hypersurfaces in terms of vanishing of global sections of line bundles. This would help me enourmously since it would enable me to carry my geometric intuition to this setting. In this context the statement would look like: A global section of a non-trivial line bundle on projective space must have a zero on all zariski closed subsets. This is the same as saying that a nontrivial line bundle on projective space restricts to a nontrivial line bundle on all closed subspaces. And here I have a cohomology problem that feels pretty specific and managable, This all feels much less ambiguous to me than ""take the cone over $X$"". Clarifying this would help me a lot.","['schemes', 'projective-geometry', 'algebraic-geometry']"
1599546,Product of two abelian subgroups,"A theorem of Ito says that if $G=AB$ where $A,B$ are abelian subgroups of $G$ then $G'$ is abelian. It was an exercise in a book, to prove, without using above fact , that If $G$ is finite group and $G=AB$ where $A,B$ are abelian subgroups then $G$ is solvable. Can you give a hint for this?","['finite-groups', 'abstract-algebra', 'group-theory']"
1599597,Why are Grothendieck's and Hartshorne's definitions of quasi-coherence equivalent?,"Hartshorne's Algebraic Geometry defines an $\mathcal O_X$-module $\mathscr F$ to be quasi-coherent if there is an open affine cover $(U_i=\operatorname{Spec} A_i)_{i\in\mathcal I}$ of $X$ such that each $\mathscr F\rvert_{U_i}$ is isomorphic to the sheaf of modules $\widetilde{M_i}$ associated to some $A_i$-module $M_i$.
It is later proved that this is equivalent to the statement that for any open affine $U=\operatorname{Spec} A\subseteq X$, there exists an $A$-module $M$ with $\mathscr F\rvert_U\cong\widetilde M$. On the other hand, Grothendieck's Éléments de Géométrie Algébrique defines $\mathscr F$ to be quasi-coherent if every point $x\in X$ admits an open neighborhood $U\subseteq X$ such that $\mathscr F\rvert_U$ is isomorphic to the cokernel of a morphism of free $\mathcal O_U$-modules. Now I wonder how those definitions are equal. I tried proving it by myself, but both directions seem quite nontrivial to me... Any hints?","['quasicoherent-sheaves', 'modules', 'algebraic-geometry', 'definition']"
1599654,Measurability of the map $x\mapsto \delta_x$,"Let $X$ be a real-valued random variable defined on a measurable space $(\Omega, \mathscr{F})$. Let also $(\mathcal{P}(\Omega), \mathscr{P})$ be the set of probability measure on $\Omega$; here the $\sigma$-algebra $\mathscr{P}$ is the collection of sets generated by $\{\lambda \in \mathcal{P}(\Omega): \lambda(A) \in B\}$ where $A \in \mathscr{F}$ and $B$ is a Borel set of $\mathbf{R}$. How can we prove that the map $x\mapsto \delta_x$ is measurable, where $\delta_x$ is the Dirac on $x$?","['real-analysis', 'measure-theory']"
1599670,Theorems on substitution in indefinite integrals,"I can't understand some facts on the substitutions in indefinite integrals. On my textbook is reported only the ""standard"" case (integral of a composed function and the derivative of the inner function): Considered two intervals $I$ and $J$ let .$f: I\rightarrow \mathbb{R}$ be a function that has an antiderivative $F(x)$ on $I$ . $\phi : J\rightarrow I$  be a function differentiable on $J$. Then the function $f(\phi(x))\phi’(x)$ is integrable and we have $\int f(\phi(x))\phi’(x)dx=F(\phi(x))+c$ The theorem is clear, but, reading other books, I saw that there are at least two other cases of substitutions in which there are different (and more) conditions to impose, since it is necessary to use the inverse of the function $\phi$. I report the theorem(s) about these two cases Considered two intervals $I$ and $J$, let .$f: I\rightarrow \mathbb{R}$ be a function continuous $I$ .$\phi : J\rightarrow I$ be a function differentiable with continuous derivative on $J$ with $\phi’(x)\ne 0 \forall x \in J$ Some of the versions of the theorem now add the following condition: .$f(J)=I$ (i.e. $\phi$ surjective) Then $\int f(x) dx= \int f(\phi(t))\phi’(t)dt$ (using substitution $x=\phi(t)$, from which, since $\phi$ is invertible, we get back the $x$ with $t=\phi^{-1}(x)$) And $\int f(\phi(x)) dx= \int f(t)[\phi^{-1}(t)]’dt$ (using substitution $t=\phi(x)$, from which, since $\phi$ is invertible, we get the $dx$ to substitute in the integral) The condition of $f$ continuous implies (Fundamental Theorem of Calculus) that it has an antiderivative and $\phi’(x)\ne 0$ in $J$ means that $\phi$ is injective (which means invertible). Firstly I can't understand why it is required for $\phi(x)$ to have continuous derivative , instead of just being continuous (as in the first theorem, taken from my textbook). I see that in the same theorems regarding definite integrals the same condition ($\phi \in C^{1}$)is imposed, but I think that in that case it is strictly necessary to have the function that we want to integrate continuous, so that we can apply the fundamental theorem of calculus. But is it necessary to impose it also for indefinite integrals, or maybe just in the second and in the third case I listed? Secondly is it strictly necessary for $\phi(x)$ to be surjective besides being injective (so invertible)?. In other terms is it necessary that $\phi$ is a bijection between  $I$ and $J$ or is it enough for it to be invertible (i.e. $\phi^{-1}$ exists)? Thanks a lot in advice","['derivatives', 'integration', 'calculus']"
1599745,Prove that every solution of an ODE system converges to some point,"Suppose $p(t)>2$ and is continuous for all $t\in\Bbb R$, 
  $$x'=2y,\\ y'=-2x-p(t)y^3,$$ prove that for each solution $(x(t),y(t))$ there exists a point $(x^*,0)$ to which it converges. I guess the most probable approach is Lyapunov stability theory , which is within the range of my ODE course. But this is a non-autonomous system (with time-varying input $p(t)$), and all I have learned from class is criteria for stability for autonomous systems. It seems kinda obvious that $x^*$ is relevant to $p(t)$, but I have trouble  even proving the existence of a singular point $(x^*,0)$ for this system, let alone dealing with it analytically. The case $p(t)\equiv\text{const}$ is easy, with $(0,0)$ apparently being the only singular point which is also apparently globally Lyapunov asymptotically stable on $\Bbb R^2$, and hence globally attracting (meaning all solutions converge to this singular point.) But for a time-varying input it is entirely different. Maybe I'll need a non-autonomous version of criterion for Lyapunov stability? But since this is beyond my course's level I think it there must be some more elementary alternatives. I'd be very grateful if anybody can provide me with one. (Of course if using non-autonomous versions can't be helped I would also be glad to learn about such tricks as long as they are effective.) EDIT Terribly sorry. I made a mistake. $(x^*,0)$ may be dependent upon the solution (or the initial condition).","['stability-in-odes', 'ordinary-differential-equations', 'dynamical-systems']"
1599827,How can there be no largest number in an interval set?,"The question I have is: Given the interval $[1,3)$, explain why there is no largest number in this set. I don't understand how that interval set does not have a largest number. I know what this means and when you draw it. I watched my lecture and read through all the notes and everything, but being an online class I'm taking, it makes it a bit more difficult for me.","['algebra-precalculus', 'notation']"
1599843,Pseudorandom Number Generator Using Uniform Random Variable,"I am working out of Mathematical Statistics and Data Analysis by John Rice and ran into the following interesting problem I'm having trouble figuring out. Ch 2 (#65) How could random variables with the following density function be generated from a uniform random number generator? $$f(x) = \frac{1 + \alpha x}{2}, \quad -1 \leq x \leq 1,\quad -1 \leq \alpha \leq 1$$ So I believe I'm suppose to use the following fact to solve the problem Proposition D Let U be uniform on [0, 1], and let X = $F^{-1}$ ( U ). Then the cdf of X is F . Proof $$P(X \leq x) = P(F^{-1}(U) \leq x) = P(U \leq F(x)) = F(x)$$ That is, we can use uniform random variables to generate other random variables that will have cdf F So my goal should then be to find a cdf and it's inverse then give as input to the inverse the uniform random variable. I've included my attempt. Given $f(x) = \frac{1 + \alpha x}{2}$ $$F(X) = \int_{-1}^{x} \frac{1 + \alpha t}{2} dt \; = \; \frac{x}{2} + \frac{\alpha x}{4} + \frac{1}{2} - \frac{\alpha}{4}$$ $$4 \cdot F(X) - 2 + \alpha = 2x + \alpha x$$ $$F^{-1}(X) = \frac{4X - 2 + \alpha}{2 + \alpha}$$ So our random variable is, for example, T where $$T = F^{-1}(U) = \frac{4U - 2 + \alpha}{2 + \alpha}$$ The answer in the back of the book is $$X = [-1 + 2 \sqrt{1/4 - \alpha(1/2 - \alpha / 4 - U)}]/ \alpha$$ I'm not really sure where I went wrong. Any help?","['random', 'probability-distributions', 'statistics', 'probability', 'uniform-distribution']"
1599846,Hessian matrix for convexity of multidimensional function,"To prove that a one dimensional differentiable function $f(x)$ is convex, it is quite obvious to see why we would check whether or not its second derivative is $>0$ or $<0.$ What is the intuition behind the claim that, if the Hessian $H$ of a multidimensional differentiable function $f(x_1,...,x_n)$ is positive semi-definite, it must be convex$?$ Is there a way I interpret this requirement, $y'H(x)y \ge 0\, \forall y$, geometrically$?$ (as in the case of one dimensional $f(x)$) How to interpret vector $y$ here$?$","['hessian-matrix', 'matrices', 'convex-analysis', 'calculus', 'positive-semidefinite']"
1599873,"Evaluate $\int{\mathrm{tr}}\left( {AB(I-xB)^{-1}}\right) {\,dx}$ for $A$ and $B$ square real matrices","Let $A$ and $B$ be two real $n\times n$ matrices, and let $C(x)=B(I-xB)^{-1}$,
where $I$ is the identity matrix of order $n$, for any real scalar $x$ such that $I-xB$ is invertible. Denote by $\mathrm{tr}$ the trace operator. Is it possible to get a closed form solution for $$\int{\mathrm{tr}}\left(  {AC(x)}\right)   {\,dx},$$ at least when $B$ is diagonalizable? What I've done so far: Assuming $B$ is diagonalizable, $C$ is too, and hence
it admits the spectral decomposition $$
C(x)=\sum_{\lambda\in\mathrm{Sp}(B)}\frac{\lambda}{1-x\lambda}Q_{\lambda},
$$ where $\mathrm{Sp}(B)$ denotes the set of distinct eigenvalues of $B$, and
$Q_{\lambda}$ is the projector onto $\mathrm{null}(B-\lambda I)$ along
$\mathrm{col}(B-\lambda I)$ ($\mathrm{null}$ and $\mathrm{col}$ stand for the null and the column spaces). Hence $$
\int{\mathrm{tr}}\left(  {AC(x)}\right)  {\,dx}=\sum_{\lambda\in
\mathrm{Sp}(B)}\mathrm{tr}(AQ_{\lambda})\int\frac{\lambda}{1-x\lambda}\,dx
$$ If all the eigenvalues of $B$ are real we obtain $$
\int{\mathrm{tr}}\left(  {AC(x)}\right)  {\,dx}=\sum_{\lambda\in
\mathrm{Sp}(B)}\mathrm{tr}(AQ_{\lambda})\int\frac{\lambda}{1-x\lambda
}\,dx=\sum_{\lambda\in\mathrm{Sp}(B)}\mathrm{ln}(\left\vert 1-x\lambda
\right\vert )\mathrm{tr}(AQ_{\lambda})
$$ But what about the case in which not all eigenvalues of $B$ are real? Do we
get any simplification from the fact that the eigenvalues of real matrices
come in complex conjugate pairs, and the eigenvectors (and hence the
projectors $Q_{\lambda}$) associated to complex conjugate eigenvalues are
complex conjugates? Can we still use the formula in the last display above for $\int{\mathrm{tr}}\left(  {AC(x)}\right)  {\,dx}$?","['eigenvalues-eigenvectors', 'matrices', 'integration', 'trace', 'linear-algebra']"
1599878,How to find the Haar measure on the group of affine transformations on $\mathbb{R}^n$?,"Let
$$\operatorname{Aff}_n(\mathbb{R}) := \left\{\begin{pmatrix} A & v\\ 0 & 1 \end{pmatrix}: \, A \in \operatorname{GL}_n(\mathbb{R}), v \in \mathbb{R}^n\right\}$$
be the group of affine transformations on $\mathbb{R}^n$. How can I find an explicit formula for a Haar measure on this group? Up to now, I've been given Haar measures and was (mostly) able to verify they were indeed Haar measures, but I have no idea where I would start to construct one. We also skipped the proof of existence of a Haar measure since apparently it is not very enlightening.","['topological-groups', 'measure-theory']"
1599890,Count the permutations which are products of exactly two disjoint cycles.,"Let $a_n$ be the number of those permutation $\sigma $ on $\{1,2,...,n\}$  such that $\sigma $ is a product of exactly two disjoint cycles. Then find $a_4$ and $a_5$. Calculating $a_4$: Possible cases which can happen are $(12)(34),(13)(24),(14)(23)$, any cycle of the form $(123)$ or $(12)$ i.e. two-cycles and three cycles thus we have in total $3+\frac{1}{3}4P_3+\frac{1}{2}4P_2=3+8+6=17$ but the correct answer is given to be either $11$ or $14$. Where am I wrong? Please help.","['abstract-algebra', 'permutations', 'combinatorics', 'group-theory', 'symmetric-groups']"
1599892,Importance of diagonal (topology),"On p.101 of Munkres Topology, Excersise 13 states that a space $X$ is Hausdorff if and only if the diagonal $\Delta=\{(x,x):x\in X\}$ is closed on $X\times X$. To me, it seems a nice characterization of Hausdorff spaces. However, it doesn't seem to be the most useful tool to prove that a concrete space $X$ is Hausdorff. So my questions are the following: Is this characterization really useful on working with Hausdorff spaces? Is the diagonal set useful in some particular branch of mathematics? Thank you in advance.","['general-topology', 'soft-question']"
1599921,Countable-infinity-to-one function,"Are there continuous functions $f:I\to I$ such that $f^{-1}(\{x\})$ is countably infinite for every $x$? Here, $I=[0,1]$. The question "" Infinity-to-one function "" answers is similar but without the condition that it be countable. (The range was $S^2$, not $I$, but the accepted answer also worked for $I$.) I doubt one exists, since I haven't been able to come up with one, but I'm not sure. There's probably some topological reason why this is impossible.","['continuity', 'general-topology', 'infinity', 'functions']"
1599946,Why is my Monty Hall answer wrong using Bayes Rule?,"The Monty Hall problem is described this way: Suppose you're on a game show, and you're given the choice of three
  doors: Behind one door is a car; behind the others, goats. You pick a
  door, say No. 1, and the host, who knows what's behind the doors,
  opens another door, say No. 3, which has a goat. He then says to you,
  ""Do you want to pick door No. 2?"" Is it to your advantage to switch
  your choice? I am interested in finding the probability of winning when you switch. I already know it's $2/3$ but I want to show it with Bayes Rule. I tried this: $A$ = car behind door $1$ $B$ = goat is behind door $3$ $$P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{1 \cdot 1/3}{1-1/3} = \frac{1}{2}$$ $P(B|A)$ = the probability that a goat is behind door $3$ given that the car is behind door $1$. This is equal to $1$ because if we know where the car is, then any other door must have a goat. $P(A)$ = the probability of the car being behind door $1$. Assuming any door is equally likely to contain a car before we open any doors, this is $1/3$. $P(B)$ = the probability of a goat behind behind door $3$. This is equal to $1$ minus the probability that the car is behind door $3$, so $1-1/3$. Where is my mistake?","['bayes-theorem', 'number-theory', 'bayesian', 'monty-hall', 'probability']"
1599966,Derivative of multivariate normal distribution wrt mean and covariance,"I want to differentiate this wrt $\mu$ and $\Sigma$ : $${1\over \sqrt{(2\pi)^k |\Sigma |}} e^{-0.5 (x-\mu)^T \Sigma^{-1} (x-\mu)} $$ I'm following the matrix cookbook here and also this answer . The solution given in the answer (2nd link), doesn't match with what I read in the cookbook. For example, for this term, if I follow rule 81 from the linked cookbook, I get a different answer (differentiating wrt $\mu$) : $(x-\mu)^T \Sigma^{-1} (x-\mu)$ According to the cookbook, the answer should be : $-(\Sigma^{-1} + \Sigma^{-T}) (x-\mu)$ . Or, am I missing something here? Also, how do I differentiate $(x-\mu)^T \Sigma^{-1} (x-\mu)$ with respect to $\Sigma$ ?","['matrices', 'calculus', 'derivatives']"
1599976,How can I integrate this? (for calculate value of L-function ),"I want to calculate the definite integral:
$$
\int_{0}^{1} \frac{x+x^{3}+x^{7}+x^{9}-x^{11}-x^{13}-x^{17}-x^{19}}{x(1-x^{20})}dx.
$$ Indeed, I already know that $\int_{0}^{1} \frac{x+x^{3}+x^{7}+x^{9}-x^{11}-x^{13}-x^{17}-x^{19}}{x(1-x^{20})}dx=L(\chi,1)=\frac{\pi}{\sqrt{5}}$ where $\chi$ is the Dirichlet character for $\mathbb{Z}[\sqrt{-5}]$. I have some trouble with this actual calculation. My calculation is as following: \begin{eqnarray}
&&\int_{0}^{1} \frac{x+x^{3}+x^{7}+x^{9}-x^{11}-x^{13}-x^{17}-x^{19}}{x(1-x^{20})}dx \cr
&=& \int_{0}^{1} \frac{1+x^{6}}{1-x^{2}+x^{4}-x^{6}+x^{8}} dx\cr
&=& \int_{0}^{1} \frac{x^{2}(1+x^{2})(\frac{1}{x^{2}}-1+x^{2})}{x^{4}(\frac{1}{x^{4}}-\frac{1}{x^{2}}+1-x^{2}+x^{4})}dx
\end{eqnarray}
Substitute $x-\frac{1}{x}=t$. Then, $(1+\frac{1}{x^{2}})dx= dt$. So The integral is
$$
\int_{-\infty}^{0} \frac{t^{2}+1}{t^{4}+3t^{2}+1}dt
$$ However, it has different value with the value which is calculated by the site http://www.emathhelp.net/calculators/calculus-2/definite-integral-calculator/?f=%28t%5E%7B2%7D%2B1%29%2F%28t%5E%7B4%7D%2B3t%5E%7B2%7D%2B1%29&var=&a=-inf&b=0&steps=on So I've just given up to try more. Is there any fine solution?","['number-theory', 'algebraic-number-theory']"
1599981,Are real antisymmetric matrices orthogonally similar to their transposes?,"Let $A$ be a real antisymmetric matrix. Is it true that $A$ must be orthogonally similar to its transpose (i.e to $-A$)? Note: It's known that every matrix is similar to its transpose .
It's also known, that in general, not every real matrix is orthogonally similar to its transpose . However, in the example given in the reference above, $A$ is not antisymmetric.","['matrices', 'linear-algebra']"
1600029,"CLT question, the density function of the horizontal deviation from shot arrow to center is given....","The horizontal deviation from shot arrow to the center of the target is given as : 
$$ \varphi (x)= \begin{cases} 1- |x| , x\in (-1,1) \\ 0 , \text{ otherwise. } \end{cases}$$
If the horizontal deviation from shot arrow to center is within the set $$(-\frac{1}{2}, \frac{1}{2})\cup (0,\frac{2}{3})$$ the shooter gets one point, otherwise he loses one point. What is the probability that in $100$ shots the shooter claimed less than $55$ points? The assignment is done like $S_n=\sum{X_i}$ where $$X_i =\begin{cases} 1, \text{(success, meaning the deviation is within the set)} \\ -1, \text{the deviation is within the set $(\frac{2}{3},1)$} \end{cases}$$ Then using the CLT.. I am having trouble finding the probabilities of $X_i = 1,-1$.","['probability-theory', 'central-limit-theorem', 'probability', 'random-variables']"
1600039,"Time series analysis, moving-average model, ARMA model","I'm reading documents about time series analysis, including Autoregressive–moving-average model , Moving average model , Wold's theorem , etc. The notation MA(q) refers to the moving average model of order q: $$X_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \cdots + \theta_q \varepsilon_{t-q} \qquad (*)$$ where μ is the mean of the series, the $θ_1$, ..., $θ_q$ are the parameters of the model and the $ε_t$, $ε_{t−1}$,..., $ε_{t−q}$ are white noise error terms. 1) I would understand if we would want to express $X_t$ as a linear combination of past values $X_{t-1}$, $X_{t-2}$, $X_{t-3}$, etc. i.e. : $$X_t = a_1 X_{t-1} + a_2 X_{t-2} + ... + a_q X_{t-q} + \epsilon_t$$ and if we would want to find optimal values for $(a_i)_{i \leq q}$. This would make sense to me (by the way, does this approach exist, what's its name?). 2) But here I don't understand why we try to express $X_t$ in terms of past values of something which is totally uncontrolled and has nothing to do with $(X_t)$ : $\epsilon_t$, i.e. some random white noise! Why do we do this in the moving average model ? (By the way, how are exactly defined the $\epsilon_t$ ?)","['time-series', 'average', 'real-analysis', 'statistics']"
1600054,Why does the minimum value of $x^x$ equal $1/e$?,"The graph of $y=x^x$ looks like this: As we can see, the graph has a minimum value at a turning point. According to WolframAlpha, this point is at $x=1/e$. I know that $e$ is the number for exponential growth and $\frac{d}{dx}e^x=e^x$, but these ideas seem unrelated to the fact that the mimum value of $x^x$ is $1/e$. Is this just pure coincidence, or could someone provide an intuitive explanation (i.e. more than just a proof) of why this is?","['exponential-function', 'calculus', 'exponentiation']"
1600088,Prove that exists a positive integer $n$ such that first $2016$ digits of $n^{2016}$ are all 1.,"Prove that exists a positive integer $n$ such that first $2016$ digits of $n^{2016}$ are all 1. Tip: see the decimal representation of $\sqrt[2016]{\dfrac{1}{9}}$. I just made this:
$$
\sqrt[2016]{\dfrac{1}{9}} = \sqrt[2016]{0,111\dots} = k
$$
$$
k^{2016} = 0,111\dots
$$
$$
(10k)^{2016} = \underbrace{111\dots111}_{2016 {\ \ }times},111\dots = \underbrace{111\dots111}_{2016{\ \ }times} + \dfrac{1}{9}
$$
$$
10^{p}(10k)^{2016} - \dfrac{10^{p}}{9} = \underbrace{111\dots111}_{2016{\ \ }times}.10^{p}
$$
And thats all. Help me, please, i can't continue from this point.",['number-theory']
1600092,"How to calculate $\sum_{n \in P}\frac{1}{n^2}, P=\{n \in \mathbb{N}: \exists (a,b) \in\ \mathbb{N^+} \times \mathbb{N^+} \mbox{ with } a^2+b^2=n^2\}$","How I can evaluate
$$\sum_{n \in P}\frac{1}{n^2} \quad \quad P=\{n \in \mathbb{N^+}: \exists (a,b) \in\ \mathbb{N^+} \times \mathbb{N^+} \mbox{ with } a^2+b^2=n^2\}$$
It's clearly convergent. I thought about seeing the sum as a sum of complex numbers using $(a+ib)(a-ib)=a^2+b^2$.","['complex-analysis', 'sequences-and-series', 'convergence-divergence', 'pythagorean-triples']"
1600101,Prove that $\dfrac{1}{2}\sqrt{4\sin^2(36^{\circ})-1} = \cos(72^{\circ})$,"Prove that $\dfrac{1}{2}\sqrt{4\sin^2(36^{\circ})-1} = \cos(72^{\circ})$ This question seemed pretty simple so I first started out by turning the left-hand side into terms of $\cos(x)$. We have $\dfrac{1}{2}\sqrt{4(1-\cos^2(36^{\circ}))-1} = \dfrac{1}{2}\sqrt{3-4\cos^2(36^{\circ})}$. Then I would use $\cos(72^{\circ}) = 2\cos^2(36^{\circ})-1$. Thus, $\dfrac{1}{2}\sqrt{3-4\cos^2(36^{\circ})} = \dfrac{1}{2}\sqrt{3-2(\cos(72^{\circ})+1)} = \dfrac{1}{2}\sqrt{1-2\cos(72^{\circ})}$. I get stuck here.",['trigonometry']
1600113,Derivative of $x\cdot|x|$ on $x=0$?,"$$f(x) = x |x|$$ Wolfram Alpha says is : $$f'(x) = \frac{2x^2}{|x|}$$ and thus $f'(0)$ is indeterminate, while an HP48 says that: $$f'(x) = |x| + x \operatorname{sgn} x,$$ which would yield $f'(0) = 0$. If I say that: $$f(x) = -x^2$$ for $x<0$, 0 for $x=0$ and: $$f(x) = x^2$$ for $x>0$, I kinda think that $f'(0)$ is 0; all three parts' derivatives converge to $0$ on $x=0$, and thus I think that Wolfram is wrong, but I don't really dare say that! (but I don't see any spiky bits on $f(x)$, although it seems clear to me that $f''(0)$ is indeterminate). (excuse my poor TeX) edit: according to the answer below, it seems that Wolfram Alpha is wrong- I've already sent them some feedback, but can anyone elaborate on that?","['derivatives', 'absolute-value', 'calculus']"
1600114,Interesting shapes using probability and discrete view of a problem,"Suppose we have a circle of radius $r$, we show the distance between a point and the center of the circle by $d$. We then choose each point inside the circle with probability $\frac{d}{r}$ , and turn it black (note that $\frac{d}{r}<1$). With these rules we get shapes like this: (With help of Java) The shapes made were pretty interesting to me. I decided to add a few lines of code to my program to keep count of the drawn points, and then divide them by the area of the circle, to see what percentage of the circle is full. All I got for multiple number of tests was a number getting close to $2/3$ as the circle got bigger. Problem A : Almost what percentage of the circle is black? I found an answer as following: As all the points with distance $l$ from the center lie on a circle of radius $l$ and the same center, to find the ""probable"" number of black points, we shall consider all circles with radius $l$ ($0<l<r$). Consider one of these circles with radius $l$. the circumference of that circle is $2\pi l$ and by how we choose black points, almost $\frac{l}{r}*2\pi l=\frac{2\pi l^2}{r}$ of that circle is black. Taking the sum of all these circles means integrating the circumferences of these circles from $l=0$ to $r$. So: $$\int_0^r{\frac{2\pi l^2}{r}dl}=\frac{2\pi r^2}{3}$$ and so the percentage of all black points shall be $$\frac{\frac{2\pi r^2}{3}}{\pi r^2}=\frac{2}{3}$$. The other question that came up in my mind is: Problem B : For given number $x\geq 1$, what is the probability $P$ that $1/x$ of the circle is black? I think the answer is $P=0$ for any given $x$. We can say if there exists an area $A>0$ full of black points, there exists an infinite number of points so that their distance to the center of the circle is equal, and the probability of all of them being black is $0$ (because there is an infinite number of them, $d$ and $r$ are constant, $\frac{d}{r}<1$ and $\lim_{n\to\infty}{(\frac{d}{r})}^n=0$). It gets more interesting with a discrete view of the problem. Suppose we have a grid full of $1*1$ squares with the size $(2r+1)*(2r+1)$.We define the reference square with length $2l+1$ (we call $l$ it's radius ) as the set of squares ""around"" the reference square with length $2l-1$, and the reference square with length $1$ is the set of $8$ squares around the central square. We define the distance $d$ of a square from the central square by the radius of it's reference square. Now we are ready to propose a problem similar to problem A: Problem C : Suppose each square with distance $d$ to the central square turns black with a probability $\frac{d}{r}$. Prove that Almost $2/3$ of the squares turn black, as $r\to \infty$ We begin our proof by proving a reference square of radius $l$ contains exactly $8l$ squares. This is easy because there are $(2l+1)^2-(2l-1)^2=8l$ squares in a reference square with radius $l$. Now, each square in the reference square is black with probability $\frac{l}{r}$, so almost $\frac{l}{r}*8l=\frac{8l^2}{r}$ of the reference square is black (Similar to the proof of Problem A). By summing up all the reference squares with radius $l=1$ to $r$ we get: $$\sum_{l=1}^{r} \frac{8l^2}{r} = \frac{8}{r}*\frac{(r)(r+1)(2r+1)}{6}=\frac{4(r+1)(2r+1)}{3}$$ and so the percentage of the whole square that is black (as $r$ tends to infinity) is:$$\lim_{r\to\infty} \frac{\frac{4(r+1)(2r+1)}{3}}{(2r+1)^2}=\frac{4(r+1)}{3(2r+1)}=\frac{2}{3}$$ as expected. Some problems, though, still remain. First , I would appreciate the verification of my solutions to problems A, B and C. The program I wrote in java only fills out a finite number of ""pixels"", does this represent Problem A or Problem C? would there be a difference if we draw the circle for infinite number of ""points""? Second ,The result of Problem B seems a little strange because as $X$ tends to $\frac{3}{2}$, $P$ should go to $1$ because $\frac{2}{3}$ of the circle is black (as proved in Problem A).  Then Why do we get $P=0$ for all values of $X$? How can we explain this? Third, What is the connection between the discrete view of the problem and the main problem? Can someone generalize the proof of Problem C to prove Problem A? I think one can easily generalize Problem C to ""circles"" full of tiny squares and prove the fact similarly, but going to an infinite number of points instead of ""pixels"" (which are equivalent to tiny squares in problem C) is still another matter. I would appreciate any help.","['definite-integrals', 'probability', 'recreational-mathematics', 'differential-geometry', 'discrete-mathematics']"
1600120,What is $\sum_ {i=1}^{10}2$?,"What is $\displaystyle\sum\limits_{i=1}^{10}2$? Is it  $$\sum\limits_{i=1}^{10}2  = 200$$ or $$\sum\limits_{i=1}^{10}2 = 2$$ or $$\sum\limits_{i=1}^{10}2= 0$$ or none of the above? I feel a little ashamed to even ask such a basic question, but I have looked over the definition of $\sum$'s and I can't wrap my head around it. The question comes from pg 40 of Apostle's Calculus which I am independently studying.","['algebra-precalculus', 'summation']"
1600124,Has this equation appeared before?,"I want to know if the following equation has appeared in mathematical literature before, or if it has any important significance. $$\sqrt{\frac{a+b+x}{c}}+\sqrt{\frac{b+c+x}{a}}+\sqrt{\frac{c+a+x}{b}}=\sqrt{\frac{a+b+c}{x}},$$
where $a,b,c$ are any three fixed positive real and $x$ is the unknown variable.","['algebra-precalculus', 'reference-request']"
1600125,proof that a system has at least one limit cycle,"The problem is: Assuming that the parameters $a, b$ are real numbers and that $ab \neq0$, by transforming the system using polar coordinates , prove that
the system
$$x'=y+x(1-a^2x^2-b^2y^2)$$
$$y'=-x+y(1-a^2x^2-b^2y^2)$$
has at least one limit cycle in the phase plane. My proof is: taking $x=r\cos\theta$ and $y=r\sin\theta$, we have $x^2+y^2=r^2$, i.e. $rr'=xx'+yy'$, so:
$$r'=r(1-a^2r^2\cos^2\theta-b^2r^2\sin^2\theta)$$
$$\theta'=-1.$$
So, $\theta = \theta(0)-t$ and $$1-a^2r^2\cos^2\theta-b^2r^2\sin^2\theta=0 \implies \frac{x^2}{b^2}+\frac{y^2}{a^2}=\frac{1}{(ab)^2}.$$
That means the system has a periodic solution which is ellipse. My question is how to prove that it is limit cycle? or the system has another solution which is limit cycle? Any hints are welcome!
Thank you!",['ordinary-differential-equations']
1600131,Intersection of subsets representing flipping a (countably) infinite number of coins,"Consider flipping a coin $n$ times. Define the sample space as
$$
\Omega = \{(r_1,r_2,r_3,\dots); r_i = 0 \text{ or }1\}
$$
Define subsets of the sample space as
$$
A_{a_1a_2\dots a_n} = \{(r_1,r_2,\dots )\in \Omega; r_i =a_i \text{ for } 1\leq i \leq n\}
$$
where $r_i$ is $0$ if the $i$th coin flip is tails and $1$ if it is heads. Define a set $\mathcal{J}$ by
$$
\mathcal{J} = \{ A_{a_1a_2\dots a_n}; n\in \mathbb{N}, a_1,a_2,\dots ,a_n \in \{ 0,1\}\} \cup \{\emptyset , \Omega\}
$$ I want to show that $\mathcal{J}$ is a semi-algebra, and am currently thinking about whether it is closed under finite intersections. My question is: If $B = A_{a_1a_2\dots a_m}$, $C = A_{a_1a_2\dots a_n}, m<n$, then $B\cap C$ = C, correct? Because I believe $C\subset B$ (since $C$ restricts more values of $r_i$ than $B$ does).","['abstract-algebra', 'probability-theory', 'measure-theory', 'elementary-set-theory']"
1600136,"What is the span of $(1, 1, 1), (0, -1, 1), (0, 0, -1) \in \mathbb R^3$?","What is the span of $(1, 1, 1), (0, -1, 1), (0, 0, -1) \in \mathbb R^3$? Supposing we haven't covered linear in/dependence, can we solve the problem as done below? The span is a set of all systems: $$
\left\{ 
\begin{array}{c}
a+b\cdot 0 + c\cdot 0 \\ 
a-b + c \cdot 0 \\ 
a+b -c
\end{array}
\right. 
$$ where $a, b, c \in \mathbb R$. Suppose $(x, y, z) \in \mathbb R^3$ and $$
\left\{ 
\begin{array}{c}
a+b\cdot 0 + c\cdot 0 = x \\ 
a-b + c \cdot 0 = y\\ 
a+b -c = z
\end{array}
\right. 
$$ Then $b = x – y$ and $c = x + x – y – z = 2x -y – z$. So, $x(1, 1, 1) + (x – y)(0, -1, 1) + (2x -y – z)(0, 0, -1)$ $= (x, x, x) + (0, y – x, x – y) + (0, 0, -2x + y + z)$ $= (x, y,  z)$ Thus the given set of vectors spans $\mathbb R^3$.",['linear-algebra']
1600153,Why are there more Irrationals than Rationals given the density of $Q$ in $R$?,"I'm reading ""Understanding Analysis"" by Abbott, and I'm confused about the density of $Q$ in $R$ and how that ties to the cardinality of rational vs irrational numbers. First, on page 20, Theorem 1.4.3 ""Density of $Q$ in $R$"" Abbot states: For every two real numbers a and b
  with a < b, there exists a rational number r satisfying a < r < b. For which he provides a proof. Later, on page 22, in the section titled ""Countable and Uncountable Sets"" he states: Mentally, there is a temptation to think of $Q$ and $I$ as being intricately mixed together in equal proportions, but this turns out not to be the case...the irrational numbers far outnumber the rational numbers in making
  up the real line. My question is: how are these two statements not in direct contradiction? Given any closed interval of irrational numbers of cardinality $X$, $A$, shouldn't be the case that we would have corresponding set of $X-1$ rational numbers, $B$, where each rational in $B$ falls ""between"" two other irrationals in $A$? If this is not the case, how do we have so many more irrationals than rationals while still satisfying our theorem that between every two reals there is a rational number? I know there are other questions similar to this, but I haven't found an answer that explains this very well, and none that address this (perceived) contradiction.","['real-analysis', 'irrational-numbers']"
1600241,Semi-colon in set notation,"In a math text, what does something like
$$
\{ (1,2,3,\dots, n); n\in \mathbb{N}\}
$$
mean? More specifically, would it be $\{(1,2,3,\dots, n)\}$ for a specific $n \in \mathbb{N}$ , or would it be $\{ (1), (1,2), (1,2,3) , \dots \}$. That is, would it apply to all $n\in \mathbb{N}$ ? I ask because I guess I am confused on the use of $;$ in set notation versus $:$. Semi-colon is such that, so if the set was $\{ (1,2,3,\dots, n); n\in \mathbb{N}\}$ I believe it would apply for all $n\in\mathbb{N}$. Semicolon is for a specific $n$ perhaps though? I can't find much reference to this, although the wiki says the semicolon serves to add an additional rule. That is, the semicolon (or comma) is like an ""and"". Where I have encountered this notation? Rosenthal's introduction to probability theory.","['probability-theory', 'notation', 'elementary-set-theory']"
1600242,Is a Radon measure determined by its action on $C_b(X;\mathbb{R})$?,"Let $\mu$ and $\nu$ be  finite Radon measures on $\left(X,\mathscr{B}(X)\right)$, where $X$ is a topological space (not necessarily locally compact) and $\mathscr{B}(X)$ the Borel sigma field. Then if
$$\int_X f(x)\,\mu(\mathrm{d}x)=\int_X f(x)\,\nu(\mathrm{d}x)$$
for all bounded continuous functions $f\in C_\mathrm{b}\left(X;\mathbb{R}\right)$, must we have $\mu=\nu$? I strongly suspect that this is the case, but cannot seem to prove it. Any hints/partial answers would be much appreciated! EDIT: Suppose further that $C_\mathrm{b}\left(X;\mathbb{R}\right)$ is an algebra which separates points.",['measure-theory']
1600270,"Bijective continuous function between $(0 ,\infty)$ and $\mathbb{R}$.","I've been working on some problems in topology, and it is odd to start
working again on sets, so I've needed some help. I want to build a
bijective continuous function $f$ such that its inverse is continuous
from $(0,\infty)$ and $(-\infty,\infty).$ I have a feeling that this
problem is similar to the Hilbert's Hotel problem, but I am having
issues figuring out the trick to make this surjective. I understand that
I need to negate the negatives, but how to differentiate the mapping of
the negatives and the positives to $\mathbb{R}^+$ seems peculiar to me.",['general-topology']
1600283,Analytic function with a non-essential singularity at $\infty$ reduces to a polynomial.,"Show that a function $f(z)$, which is analytic in the whole plane and has a non-essential singularity at $\infty$ reduces to a polynomial. This question has been asked previously and I have gone through a solution. Following is a line of reasoning I came up with independently, but I need to know if it is sound enough. $f(z)$ has no poles in the complex plane. Then $g(z) = f(\frac{1}{z})$ has a non-essential singularity at $z=0$ and none other. Suppose the algebraic order of this singularity of $g(z)$ is $h$. Following cases are possible. $h=0$. Then $g(0)\ne 0$ and $g(z)$ is analytic at $0$. Then $[f(z)]_{z\rightarrow \infty}$ is finite. Since $f(z)$ is analytic, from Liouville's Theorem it follows that $f(z)$ is a constant. $h\lt 0$. Then $g(z)$ has a zero of order $h$ and the same conclusion is possible as in 1. $h>0$. In this case $g(z)$ has a pole of order $h$ at $0$. By Taylor's Theorem $$z^hg(z)=B_h+B_{h-1}z+\cdots + B_1z^{h-1} + \phi(z)z^h$$where $\phi(z)$ is analytic at $0$. For $z\ne0$, it may be now written as $$g(z)=\frac{B_h}{z^h}+\cdots+\frac{B_1}{z}+\phi(z).$$Since the only singular part of $g(z)$ is in the RHS of above apart from $\phi(z)$, it must be true that $\phi(z)$ has no singularities in the extended plane. Thus $\phi(z)$ is a constant. By $f(z) = g(\frac{1}{z})$, $f(z)$ is a polynomial. Thanks.","['complex-analysis', 'proof-verification']"
1600289,"Prove that there exists bipartite graph with this degree sequence: $(3,3,3,3,3,5,6,6,6,6,6,6,6,6)$","How do I prove that there exists (or does not exist) bipartite graph with this degree sequence: $(3,3,3,3,3,5,6,6,6,6,6,6,6,6)$ ? The sum of the degree sequence is even and simplfied it looks like this: $(1,1,1,1,1,1)$ => it's a degree sequence of a graph. The simplified degree sequence might suggest that it could be bipartite graph, but I have no idea how to prove it.","['graph-theory', 'discrete-mathematics']"
1600294,An elementary verification of the equivalence between two expressions for $e^x$,"I would appreciate some constructive comments on the following argument for
\begin{equation*}
\sum_{n=0}^{\infty} \frac{x^{n}}{n!}
= \lim_{n\to\infty} \left(1 + \frac{x}{n}\right)^{n} .
\end{equation*}
I understand that there are several different arguments for it.  I like that this does not involve the natural logarithm function.  I looked in many elementary textbooks on real analysis for such an argument.  I only found one, but it was in the special case $x = 1$, and it was flawed.  The only analysis techniques used are the convergence of the sequence defined by
\begin{equation*}
\left(1 + \frac{x}{n}\right)^{n} ,
\end{equation*}
and the absolute convergence of
\begin{equation*}
\sum_{n=0}^{\infty} \frac{x^{n}}{n!} .
\end{equation*}
Here it is. Demonstration According to the Binomial Theorem, for every positive integer $n$,
\begin{align*}
&\sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n} \\
&\qquad = \sum_{i=0}^{n} \frac{x^{i}}{i!} - \sum_{i=0}^{n} \binom{n}{i} \frac{x^{i}}{n^{i}} \\
&\qquad = \sum_{i=0}^{n} \left[\frac{x^{i}}{i!} - \binom{n}{i} \frac{x^{i}}{n^{i}} \right] \\
&\qquad = \sum_{i=0}^{n} \left[\frac{1}{i!} - \frac{1}{n^{i}} \binom{n}{i} \right] x^{i} \\
&\qquad = \sum_{i=2}^{n} \left[\frac{1}{i!} - \frac{1}{n^{i}} \binom{n}{i} \right] x^{i} .
\end{align*}
For each integer $2 \leq i \leq n$,
\begin{align*}
\frac{1}{n^{i}} \binom{n}{i} &= \frac{1}{n^{i}} \cdot \frac{n!}{i!(n-i)!} \\
&= \frac{1}{n^{i}} \cdot \frac{n(n - 1)(n - 2) \cdots (n - i + 1)}{i!} \\
&= \frac{1}{i!} \cdot \frac{n(n - 1) (n - 2) \cdots (n - (i - 1))}{n^{i}} \\
&= \frac{1}{i!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) .
\end{align*}
So,
\begin{equation*}
\sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n}
= \sum_{i=2}^{n} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{x^{i}}{i!}
\end{equation*}
According to the Triangle Inequality, for each pair of positive integers $2 \leq k < n$,
\begin{align*}
&\left\vert \sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n} \right\vert \\
&\qquad \leq \left\vert \sum_{i=2}^{k} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{x^{i}}{i!} \right\vert \\
&\qquad\qquad + \left\vert \sum_{i=k+1}^{n} \frac{x^{i}}{i!} \right\vert
+ \left\vert \sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} x^{i} \right\vert \\
&\qquad \leq \sum_{i=2}^{k} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{\vert x \vert^{i}}{i!} \\
&\qquad\qquad + \sum_{i=k+1}^{n} \frac{\vert x \vert^{i}}{i!}
+ \sum_{i=k+1}^{n} \frac{1}{n^{i}}\binom{n}{i} \vert x \vert^{i} .
\end{align*}
\begin{align*}
&\sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i} \\
&\qquad = \sum_{i=k+1}^{n} \frac{1}{i!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right)
\vert x \vert^{i} \\
&\qquad = \frac{1}{(k+1)!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k}{n}\right) \vert x \vert^{k+1} \\
&\qquad\qquad \!\begin{aligned}[t]
&+ \frac{1}{(k+2)!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k+1}{n}\right) \vert x \vert^{k+2} \\
&+ \frac{1}{(k+3)!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k+2}{n}\right) \vert x \vert^{k+3} \\
&+\ldots
+ \frac{1}{n!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{n-1}{n}\right) \vert x \vert^{n} .
\end{aligned} \\
&\qquad = \frac{1}{k!} \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{k}{n}\right) \vert x \vert^{k} \\
&\qquad\qquad \!\begin{aligned}[t]
&\biggl[\frac{1}{k+1} \left(1 - \frac{k}{n}\right) \vert x \vert \\
&\hphantom{\biggl[\vphantom{\frac{1}{k+1}}}+ \frac{1}{(k+1)(k+2)} \left(1 - \frac{k}{n}\right) \left(1 - \frac{k+1}{n}\right) \vert x \vert^{2} \\
&\hphantom{\biggl[\vphantom{\frac{1}{k+1}}}+ \frac{1}{(k+1)(k+2)(k+3)}  \left(1 - \frac{k}{n}\right) \left(1 - \frac{k+1}{n}\right) \left(1 - \frac{k+2}{n}\right) \vert x \vert^{3} \\
&\hphantom{\biggl[\vphantom{\frac{1}{k+1}}}+ \ldots \\
&\hphantom{\biggl[\vphantom{\frac{1}{k+1}}}
+ \frac{1}{(k+1)(k+2) \cdots n} \left(1 - \frac{k}{n}\right) \left(1 - \frac{k+1}{n}\right) \cdots \left(1 - \frac{n-1}{n}\right)
\vert x \vert^{n-k}
\biggr]
\end{aligned} \\
&\qquad < \frac{\vert x \vert^{k}}{k!} \left[
\frac{\vert x \vert}{k+1} + \left(\frac{\vert x \vert}{k+1}\right)^{2}
+ \ldots + \left(\frac{\vert x \vert}{k+1}\right)^{n-k}
\right] . \\
\text{So, if $k \geq \vert x \vert$,} \\
&\sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i} \\
&\qquad < \frac{\vert x \vert^{k}}{k!} \sum_{i=1}^{\infty} \left(\frac{\vert x \vert}{k+1} \right)^{i} \\
&\qquad = \frac{\vert x \vert^{k}}{k!} \cdot \frac{\dfrac{\vert x \vert}{k + 1}}{1 - \dfrac{\vert x \vert}{k+1}} \\
&\qquad = \frac{\vert x \vert^{k}}{k!} \cdot \frac{\vert x \vert}{k + 1 - \vert x \vert} , \\
\text{and if $k \geq 2\vert x \vert$,} \\
&\sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i}
< \frac{\vert x \vert^{k}}{k!} .
\end{align*} By the absolute convergence of
\begin{equation*}
\sum_{n=0}^{\infty} \frac{x^{n}}{n!} ,
\end{equation*}
for every $\epsilon > 0$, there is a big enough positive integer $K$ such that for every integer $k \geq K$,
\begin{equation*}
\sum_{i=k}^{\infty} \frac{\vert x \vert ^{i}}{i!}
< \frac{\epsilon}{3} ,
\end{equation*}
and so,
\begin{equation*}
\frac{\vert x \vert^{k}}{k!}
< \sum_{i=k}^{\infty} \frac{\vert x \vert ^{i}}{i!}
< \frac{\epsilon}{3}
\qquad \text{and} \qquad
\sum_{i=k+1}^{\infty} \frac{\vert x \vert ^{i}}{i!}
< \sum_{i=k}^{\infty} \frac{\vert x \vert ^{i}}{i!}
< \frac{\epsilon}{3} .
\end{equation*}
So, if $k \geq \max\{2\vert x \vert, \, K\}$, and if $n > k$,
\begin{equation*}
\sum_{i=k+1}^{n} \frac{1}{n^{i}} \binom{n}{i} \vert x \vert^{i}
< \frac{\vert x \vert^{k}}{k!}
< \frac{\epsilon}{3} .
\end{equation*}
Likewise, since for each integer $2 \leq i \leq k$,
\begin{equation*}
\lim_{n\to\infty} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] = 0 ,
\end{equation*}
there is a big enough positive integer $N$ such that for every integer $n \geq N$,
\begin{equation*}
\sum_{i=2}^{k} \left[ 1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right]
< \frac{\epsilon}{3 \cdot \max\{\vert x \vert^{k}, \, 1\}} ,
\end{equation*}
and so,
\begin{equation*}
\sum_{i=2}^{k} \left[1 - \left(1 - \frac{1}{n}\right) \left(1 - \frac{2}{n}\right) \cdots \left(1 - \frac{i - 1}{n}\right) \right] \frac{\vert x \vert^{i}}{i!}
< \frac{\epsilon}{3} .
\end{equation*}
Consequently, for any positive integers $k \geq \max\{2\vert x \vert, \, K\}$ and $n > \max\{k, \, N\}$,
\begin{equation*}
\left\vert \sum_{i=0}^{n} \frac{x^{i}}{i!} - \left(1 + \frac{x}{n}\right)^{n} \right\vert < \epsilon .
\end{equation*}
Equivalently,
\begin{equation*}
\sum_{i=0}^{\infty} \frac{x^{i}}{i!} = \lim_{n\to\infty} \left(1 + \frac{x}{n}\right)^{n} .
\end{equation*}","['real-analysis', 'proof-verification', 'limits']"
1600308,Reflected rays /lines bouncing in a circle? [duplicate],"This question already has answers here : Reflection inside spherical mirror (5 answers) Closed 2 years ago . Consider the following situation. You are standing in a room that is perfectly circular with mirrors for walls.  You shine a light, a single ray of light, in a random direction.  Will the light ever return to its original position (the single point where the light originated from)?  If so, will it return to its position an infinite amount of times or a definite amount of times?  Will it ever return to its original position in the original direction? I thought of this little teaser when reading about a problem concerning rays in a circle and wondered about this question. As for my attempts, this is well beyond my skill.","['recreational-mathematics', 'geometry']"
1600340,Proving finite additivity for this semi-algebra (infinite coin flips),"Background copied and pasted from another one of my questions: Background: Consider flipping a coin $n$ times. Define the sample space as
$$
\Omega = \{(r_1,r_2,r_3,\dots); r_i = 0 \text{ or }1\}
$$
Define subsets of the sample space as
$$
A_{a_1a_2\dots a_n} = \{(r_1,r_2,\dots )\in \Omega; r_i =a_i \text{ for } 1\leq i \leq n\}
$$
where $r_i$ is $0$ if the $i$th coin flip is tails and $1$ if it is heads. Define a set $\mathcal{J}$ by
$$
\mathcal{J} = \{ A_{a_1a_2\dots a_n}; n\in \mathbb{N}, a_1,a_2,\dots ,a_n \in \{ 0,1\}\} \cup \{\emptyset , \Omega\}
$$ Let $P(A_{a_1a_2\dots a_n}) = 1/2^n$ for each set $A_{a_1a_2\dots a_n}$ Problem: I want to show that the above $\mathcal{J}$ and $P$ satisfy the following property for finite collections $\{D_n\}$: $$
P(\cup_n D_n)= \sum_n P(D_n) \text{ for } D_1,D_2,\dots \in \mathcal{J} \text{ disjoint with }\cup_nD_n \in \mathcal{J}
$$ Hint: The hint I am given is: For a finite collection $\{D_n\}\in\mathcal{J}$, there is a $k\in \mathbb{N}$ such that the results of only coins $1$ through $k$ are specified by any $D_n$. Partition $\Omega$ into the corresponding $2^k$ subsets. I have submitted what I think a solution, yet I would be very happy if someone can provide me with a cleaner/shorter solution (I believe one exists).
Thanks. Edit: Note: I changed my interpretation of $k$ from what I thought initially. Edit: I have deleted my work since I think I found a solution and now simply want a nicer one. I think the cleaner look makes it more likely that I will get one.","['alternative-proof', 'probability-theory', 'measure-theory']"
1600353,Closed Subset of Connected Space with Boundary a Single Point is Connected,"Old qual question here. Let $X$ be connected and $C\subset X$ be closed such that the boundary of $C$ is a point. Show that $C$ is connected. Attempt: For contradiction, suppose $U,V$ are nonempty disjoint sets open in $C$ such that $C=U\cup V$. Call $y=\partial C$ and say $y\in U$. Then $V\subset\operatorname{int}C$, which is open, so $V$ is open in $X$. Since $C$ is closed in $X$, $C^C$ is open in $X$. This is where I'm stuck. I think that $U\cup C^C$ has to be open in $X$, then $X=(U\cup C^C)\cup V$ would not be connected, a contradiction. But I'm stuck here.","['general-topology', 'connectedness']"
1600414,Finding the sum of the infinite series whose general term is not easy to visualize: $\frac16+\frac5{6\cdot12}+\frac{5\cdot8}{6\cdot12\cdot18}+\cdots$ [duplicate],"This question already has answers here : Sum $\frac{1}{6} + \frac{5}{6\cdot 12} + \frac{5\cdot 8}{6\cdot 12\cdot 18} + \frac{5\cdot 8\cdot 11}{6\cdot 12\cdot 18\cdot 24}+\ldots$ [closed] (4 answers) Closed 6 years ago . I am to find out the sum of infinite series:-
$$\frac{1}{6}+\frac{5}{6\cdot12}+\frac{5\cdot8}{6\cdot12\cdot18}+\frac{5\cdot8\cdot11}{6\cdot12\cdot18\cdot24}+...............$$
I can not figure out the general term of this series. It is looking like a power series as follows:-
$$\frac{1}{6}+\frac{5}{6^2\cdot2!}+\frac{5\cdot8}{6^3\cdot3!}+\frac{5\cdot8\cdot11}{6^4\cdot4!}+.....$$
So how to solve it and is there any easy way to find out the general term of such type of series?","['power-series', 'real-analysis', 'sequences-and-series']"
1600432,Number of $k$-dimensional subspaces in $V$ [duplicate],This question already has an answer here : How to count number of bases and subspaces of a given dimension in a vector space over a finite field? [duplicate] (1 answer) Closed 8 years ago . I know my following question is somewhat similar to this one but still I need help . How many $k$-dimensional subspaces of a $n$-dimensional vector space $V$  over the finite field $F$ with $q$ elements are there?,"['combinatorics', 'linear-algebra', 'vector-spaces']"
1600443,Can $G$ have a Sylow -5 subgroups and Sylow -3 subgroups [CSIR-NET-DEC-2015],"Let $G$ be a simple group of order $60$. Then $G$ has six Sylow -5 subgroups. $G$ has four Sylow -3 subgroups. $G$ has a cyclic subgroup of order 6. $G$ has a unique element of order $2$. $60=2^2.3.5$ No. of Sylow -5 subgroups =$1+5k$  divides 12.So $1+5k=1,6\implies n_5=1,6\implies n_5=6$ as $G$ is a simple group. Consider $n_3=1+3k$ divides $20\implies 1+3k=1,4,10\implies 1+3k=4,10$. If $n_3=4$ then we have $8 $ elements of order $3$ and $A_5$ has 20 elements of order $3$ which is a contradiction.Hence $n_3=10$. Since  $A_5$ has no  element  of order $6$.So 3 is false. $A_5$ has many elements of order $2$ viz. $(12)(34),(13)(24),,$. Hence $1$ is correct only .Please can someone check whether I am correct /not?","['abstract-algebra', 'group-theory']"
1600452,"What is the probability that a family with $3$ children has at least one girl, given that at least one is a boy$?$","$(1)$ What is the probability that that a family with $3$ children
has at least one girl, given that at least one is a boy? $(2)$ And what's the probability that they have at least one boy AND one girl? $(3)$ And what's the probability that they have at least one boy OR one girl? All possible combinations: BBB 
  BBG
  BGB
  BGG
  GBB
  GBG
  GGB
  GGG Question $(1)$: Exclude the 'GGG' option. $\implies$ $P$(at least $1$ boy) $= 7/8$ The new sample space is $7$ of which $6$ have at least $1$ girl $\implies$ $P$(at least $1$ girl $\mid$ at least $1$ boy) $= 6/7$ Question $(2)$: By looking at the combinations I can quickly say that there are $6$ out of $8$ that satisfy (at least $1$ girl AND at least $1$ boy). $\implies 6/8 = 3/4$. 
However, considering larger data sets, this becomes impossible. According to: $P(A\ \text{and}\ B) = P(A)*P(B|A) = \frac{7}{8} \times  \frac{6}{7} = \frac{6}{8} = \frac{3}{4} $ 
I get the same result. So, why does multiplying  $7/8 \times  6/7$ provide the correct result? I know that the denominators $7\times 8$ produce a new sample space (permutations), and the $7\times 6$ new desirable outcomes (permutations). But these are permutations of $(x,y)$ ($2$ children) not $(x,y,z)$ ($3$ children). But I cannot explain why it works, since the original sample space does not consider permutations. Or am I wrong? Many thanks for assistance in explaining why this works. $(3)$ What is the difference between $(2)$ and $(3)?$ Are they not the same?","['combinatorics', 'probability']"
1600458,Does $\|f\|_K = \sup_{z \in K} |f(z)|$ define a norm on $C(\mathbb{C})$ and $H(\mathbb{C})$ for compact $K$?,Let $C(\mathbb{C})$ denote the vector space of continuous complex valued functions on $\mathbb{C}$ and $H(\mathbb{C})$ denote vector space of entire functions. For any function $f$ in $C(\mathbb{C})$ and $H(\mathbb{C})$ and for any compact subset $K$ define $\|f\|_K = \sup_{z\in K} |f(z)|$. Is $\|\cdot\|_K$ a norm on $C(\Bbb C)$ for every compact $K\subset \Bbb C$? Is $\|\cdot\|_K$ a norm on $H(\Bbb C)$ for every compact $K\subset \Bbb C$? I can't figure out how should I proceed with these problems. It will be great if someone could figure out what to do here.,"['functional-analysis', 'complex-analysis', 'normed-spaces']"
1600472,The greatest integer less than or equal to the number $R=(8+3\sqrt{7})^{20}$,"Given $$R=(8+3\sqrt{7})^{20}, $$ if $\lfloor R \rfloor$ is Greatest integer less than or equal to $R$ , then which of the following option(s) is/are true? $\lfloor R \rfloor$ is an even number $\lfloor R \rfloor$ is an odd number $R-\lfloor R \rfloor=1-\frac{1}{R}$ $R(R-\lfloor R \rfloor-1)=-1$ My try:  I wrote $R$ as $$R=8^{20}\left(1+\sqrt{\frac{63}{64}}\right)^{20} \approx8^{20}\left(1+\sqrt{0.98}\right)^{20} \approx8^{20}\left(1.989\right)^{20} .$$ Now, $8^{20}\left(1.989\right)^{20}$ is slightly less than $8^{20} \times 2^{20}=2^{80}$ , $$\lfloor 2^{80}\rfloor=2^{80}$$ hence $$\lfloor R \rfloor=2^{80}-1,$$ so option $2$ is correct. How does one figure out whether options $3$ and $4$ are correct or wrong?","['algebra-precalculus', 'binomial-coefficients']"
1600486,"Is there an equivalent concept of a ""variety"" for SAT?","Couldn't find anything via google - I was wondering what work is out there looking at SAT problems from the perspective akin to an algebraic variety, e.g. a set of variables $X_1=$true, $X_2=$false, .., $X_k=$true, etc define a set of propositions which are all satisfed by that particular set of values in much the same way that a variety defines a set of polynomials that all happen to vanish at the same points. Thanks.","['computational-complexity', 'algebraic-geometry']"
1600551,Is this a valid way to think about sheafification?,"This all feels like it should be valid, but I just wanted to get more experienced eyes on it in case I've made a mistake. Take a presheaf $\mathscr{F}$ on a topological space $X$. In order to be a sheaf, for any set of compatible functions $f_i \in \mathscr{F}(U_i)$, there needs to exist a unique gluing. So the sheafification $\mathscr{F}^+$ of $\mathscr{F}$ can be constructed by doing the ""least amount of work"" to make this happen. Intuitively, I feel like this should mean two things happen: First, suppose more than one gluing exist. If $f$ and $g$ are both gluings of $\{f_i\}$, we have no natural way to decide on which to keep. The easiest thing to do is to equate all gluings, and require that $f=g$ in $\mathscr{F}^+$. Second, if a gluing for $\{f_i\}$ does not exist, one is freely adjoined. This new section will not be equal to any old sections in $\mathscr{F}$. We can identify this new section with the collection $\{f_i\}$, perhaps even calling it by the name $[f_i]$. Doing this, we need to have the understanding that it's very likely another compatible family $\{g_j\}$ will exist which glues together to form the same section. If this is the case, we require $[f_i]=[g_j]$. But since no section existed previously, you need a rule to tell when two such compatible families of functions should glue to the same section. In general, they will be defined on different covers, and you need to take a common refinement $\{V_i\}$ of these covers. By pushing each $f_i$ and each $g_j$ through the restriction maps into the appropriate open sets in this refinement, you can compare them directly for equality. And if all the right $f_i$'s and $g_j$'s are equal, then $[f_i] = [g_j]$.","['sheaf-theory', 'algebraic-geometry']"
1600591,LHôpitals rule - can I use second and third derivatives after first one doesnt help?,"After taking the first derivative, I still had 0 in the denominator. I then took the second and third derivatives and got the right answer. But in class, our teacher only talked about using first derivative. So is it okay? Help!","['calculus', 'limits']"
1600595,$p$-adic valuation of harmonic numbers,"For an integer $m$ let $\nu_p(m)$ be its $p$-valuation i.e.  the greatest non-negative integer such that $p^{\nu_p(m)}$ divides $m$. Let now 
$H_n=1+\dfrac{1}{2}+ \cdots+ \dfrac{1}{n}$. If $H_n=\dfrac{a_n}{b_n}$  then $\nu_p(H_n)=\nu_p(a_n)-\nu_p(b_n).$ It is known that if $2^k \leq n < 2^{k+1}$  then $\nu_2(H_n)=-k.$ Question For what $n$ and arbitrary $p$  we may be sure that $\nu_p(H_n)<0?$ Any estimates and reference, please.","['number-theory', 'reference-request', 'p-adic-number-theory']"
1600610,Does convergence in probability w.r.t. a topology make sense?,"Let $(S,d)$ be a separable metric space. A sequence of random variables $X_n$ is said to converge in probability to $a \in S$ if and only if for all $\varepsilon > 0$ $$P(d(X_n,a) > \varepsilon) \to 0,$$
as $n \to \infty$. Would this make sense in a topological space? Let $S$ be a topological space instead and, instead of the condition above. require that for any open $U \ni a$ $$P(X_n \in U) \to 1.$$ Does this type of concept exist? Would it make any sense? What I have in mind is a probabilistic version of convergence of operators on Hilbert space w.r.t. the weak operator topology.",['probability-theory']
1600626,Geometric progression in an inequality,"Problem: Show that if $a>0$ and $n>3$ is an integer then $$\frac{1+a+a^2 \cdots +a^n}{a^2+a^3+ \cdots a^{n-2}} \geq \frac{n+1}{n-3}$$ I am unable to prove the above the inequality. I used the geometric progression summation formula to reduce it to proving $\frac{a^{n+1}-1}{a^2(a^{n-3}-1)} \geq \frac{n+1}{n-3}$. Also writing it as $$\frac{1+a+a^2 \cdots +a^n}{n+1} \geq \frac{a^2+a^3+ \cdots a^{n-2}}{n-3}$$
 seems to suggest that some results on mean-inequalities can be used but I can't figure out what that is.","['algebra-precalculus', 'inequality', 'arithmetic-progressions', 'geometric-progressions']"
1600641,"How to solve this integral $\int _0^{\infty} e^{-x^3+2x^2+1}\,\mathrm{d}x$","My classmate asked me about this integral:$$\int_0^{\infty} e^{-x^3+2x^2+1}\,\mathrm{d}x$$
but I have no idea how to do it. What's the closed form of it? I guess it may be related to the Airy function.","['airy-functions', 'improper-integrals', 'integration', 'definite-integrals', 'sequences-and-series']"
1600648,"Second order ODE, first derivative missing","I have the following second order equation, where the first derivative is missing, and I am asked to find its general solution:
$$6x^{2}yy''=3x(3y^{2}+2)+2(3y^{2}+2)^3$$
I don't know how to solve it. I have tried with a $u(x)=3y^{2}+2$ substitution but it doesn't seem useful... Is there any method for this kind of equation whithout $y'$?",['ordinary-differential-equations']
1600667,Which of the following sets form a group under mutliplication modulo $14$?,"Which of the following sets form a group under mutliplication modulo 14? $\{1,3,5\}$ $\{1,3,5,7\}$ $\{1,7,13\}$ $\{1,9,11,13\}$ I figured that only $\{1,3,5\}$ forms a group. But my answers say that that one is also wrong because 1) $3x3=9$ so $9$ should be in there as well and 2) $5\times5=11 \mod 14$ and $5$ is in there but $11$ is not. Could someone explain this reasoning to me please?",['group-theory']
1600673,"Example of a Borel measure, which is not Borel-regular","I have asked a question to find four types of outer measures here , and I could find three of the four examples. We call an outer measure $\mu: \mathcal P(\mathbb R^n) \to [0, \infty]$ Borel , if every Borel set $B \subset \mathbb R^n$ is $\mu$-measurable. We say, that an outer measure $\mu: \mathcal P(\mathbb R^n) \to [0, \infty]$ is Borel-regular , if $\mu$ is Borel and for any subset $A \subset \mathbb R^n$ there is a Borel set $B \supset A$, such that $\mu(A) = \mu(B)$. I would like to give an example of a Borel measure, which is not Borel-regular. Can you help me?","['geometric-measure-theory', 'real-analysis', 'examples-counterexamples', 'measure-theory']"
1600687,"$0<\beta < \alpha \leq 1$, unit ball of Hölder space $C^{0,\alpha}[0,1]$ compact in $C^{0,\beta}[0,1]$?","So this is a very basic question on Hölder spaces. Let $0 < \beta < \alpha \leq 1$.
  Prove that the unit ball of $C^{0,\alpha}[0,1]$ is compact in $C^{0,\beta}[0,1]$. For reference: $\| f \|_\alpha = \| f \|_\infty + \sup_{x \neq y} \frac{\left| f(x)-f(y) \right|}{\left| x-y\right|^\alpha}$ is the Hölder norm and $C^{0,\alpha}[0,1]=\{ f:[0,1]\rightarrow \mathbb{R} \mid \| f \|_\alpha < \infty \}$ the space of Hölder-continuous functions. Idea would be to use the Arzela-Ascoli theorem, but I seem to get confused. Obviously the unit ball of $C^{0,\alpha}[0,1]$ bounded with respect to $\| \cdot \|_\infty$ and uniformly equicontinuous.
The Arzela-Ascoli theorem then yields, that the unit ball of $C^{0,\alpha}[0,1]$ has compact closure with respect to $\| \cdot \|_\infty$.
But $\| \cdot \|_\gamma$ has a coarser topology than $\| \cdot \|_\infty$.
So if the unit ball is precompact with respect to $\| \cdot \|_\infty$ it certainly needs to be precompact with respect to $\| \cdot \|_\alpha$ and $\| \cdot \|_\beta$.
However the closed unit ball in a infinite dimensional Banach space cannot be compact, so the above conclusion must be false. Where did I go wrong?
And equally important, how can I solve this exercise?","['functional-analysis', 'holder-spaces', 'compactness', 'analysis']"
1600710,Why is this theorem also a proof that matrix multiplication is associative?,"The author remarks that this theorem, which is basically all about what happens if we compose linear transformations, also gives a proof that matrix multiplication is associative: Let $V$, $W$, and $Z$ be finite-dimensional vector spaces over the field $F$; let $T$ be a linear transformation from $V$ into $W$ and $U$ a linear transformation from $W$ into $Z$. If $\mathfrak{B}$, $\mathfrak{B^{'}}$, and $\mathfrak{B^{''}}$ are ordered basis for the spaces $V$, $W$, $Z$, respectively, if $A$ is the matrix of $T$ relative to the pair $\mathfrak{B}$, $\mathfrak{B^{'}}$, and $B$ is the matrix of $U$ relative to the pair $\mathfrak{B^{'}}$, $\mathfrak{B^{''}}$, then the matrix of the composition $UT$ relative to the pair $\mathfrak{B}$, $\mathfrak{B^{''}}$ is the product matrix $C=BA$. However, I see no reason why that's true...",['linear-algebra']
1600724,Ring of Invariants of symmetric group,"The symmetric group $S_n$ acts on $\mathbb C^n$ by permuting the coordinates. In this case the ring of invariants is generated by elementary symmetric polynomials in n-variables. Now consider the regular representation of $S_n$, the basis of the vector space is indexed by the elements of $S_n$. Then what are the generators for the ring of invariants ? I guess the elementary symmetric polynomial in $n!$ variables generate the ring but I am not sure.","['abstract-algebra', 'invariant-theory', 'group-theory', 'commutative-algebra']"
1600731,Find the number of real roots of the derivative of $f(x)=(x-1)(x-2)(x-3)(x-4)(x-5)$ [duplicate],"This question already has answers here : How to find root of derivative of any polynomial/equation? (4 answers) Closed 8 years ago . Find out the number of real roots of equation $f'(x) = 0$, where 
  $$f(x)=(x-1)(x-2)(x-3)(x-4)(x-5)$$ How can I differentiate this function without expanding it to the polynomial form. Am I underestimating some theory of equation concept associated with it? (I know the product rule approach and solving by simplifying but I want to know is there any other way to solve it)","['derivatives', 'roots', 'polynomials', 'calculus']"
1600747,Integrating secans over the imaginary axis using the residue theorem,"I am trying to integrate $\sec(z)$ over the whole imaginary axis using the residue theorem. i.e., I want to calculate the integral $$\int_{\Gamma} \frac{dz}{\cos{z}}$$ where $\Gamma$ is the (open) contour that moves along the straight line $x=0$ from $y=-\infty$ to $y=\infty$. 
$1/\cos(z)$ has infintely many simple poles at either side of the contour. The residues at these poles are
$$\text{Res}_{z=-\frac{\pi}{2}+ 2n\pi}\left(\frac{1}{\cos(x)}\right)=1$$
and
$$\text{Res}_{z=\frac{\pi}{2}+ 2n\pi}\left(\frac{1}{\cos(x)}\right)=1$$
for $n\in \mathbb{Z}$.
My question doesn't really concern the calculation, I just want to find the appropriate contour(s). My initial idea was to use two contours so that the infinite poles on either side of $x=0$ would somehow cancel out, but I haven't gotten very far.","['complex-analysis', 'contour-integration', 'residue-calculus']"
1600773,How to draw complex function graphics,I'm trying to understand why Wolfram plots $y=\sqrt{x^2-4}$ in this way: I did understand why the blue line is drawn in this way (the real part). What I didn't understand is why the imaginary part is drawn like this. Note also that a complex function is in the form $f:\mathbb R^2\to \mathbb R^2$ and shouldn't be drawn in a plane.,['complex-analysis']
1600800,Simplex method - identity matrix,"I want to solve the following linear programming problem: $$\min (5y_1-10y_2+7y_3-3y_4) \\ y_1+y_2+7y_3+2y_4=3 \\ -2y_1-y_2+3y_3+3y_4=2 \\ 2y_1+2y_2+8y_3+y_4=4 \\ y_i \geq 0, i \in \{ 1, \dots, 4 \}$$ $\begin{bmatrix}
1 & 1 & 7 & 2 & | & 3\\ 
-2 & -1 & 3 & 3 & | & 2\\ 
2 & 2 & 8 & 1 & | & 4
\end{bmatrix} \to \begin{bmatrix}
1 & 1 & 7 & 2 & | & 3\\ 
0 & 1 & 17 & 7 & | & 8\\ 
0 & 0 & 6 & 3 & | & 2
\end{bmatrix}$ So the problem is written equivalently as follows: $$-\max (-5y_1+10y_2-7y_3+3y_4) \\ y_1+y_2+7y_3+2y_4=3 \\ y_2+17y_3+7y_4=8 \\ 6y_3+3y_4=2 \\ y_i \geq 0, i \in \{ 1, \dots, 4 \}$$ But we want the $3 \times 3$ identity matrix to appear at the matrix that represents the linear programming problem, right? So we solve the following problem, right? $$-\max (-5y_1+10y_2-7y_3+3y_4) \\ y_1+y_2+7y_3+2y_4=3 \\ y_2+17y_3+7y_4+y_5=8 \\ 6y_3+3y_4+y_6=2 \\ y_i \geq 0, i \in \{ 1, \dots,6 \}$$ Then: $\begin{matrix}
B & c_B & b & P_1 & P_2 & P_3 & P_4 & P_5 & P_5 & \theta & \\ 
P_1 & -5 & 3 & 1 & 1 & 7 & 2 & 0 & 0 & \frac{3}{7} &L_1 \\ 
P_5 & 0 & 8 & 0 & 1 & 17 & 7 & 1 & 0 & \frac{8}{17} & L_2\\ 
P_6 & 0 & 2 & 0 & 0 & 6 & 3 & 0 &1  & \frac{1}{3} &L_3 \\ 
 & z & 0 & -5 & 10 & -7 & 3 & 0 & 0 &  & L_4
\end{matrix}$ $|-7|> |-5|$ so $P_3$ gets in the basis and $P_6$ gets out of the basis. Then we get the following tableau: $\begin{matrix}
B & c_B & b & P_1 & P_2 & P_3 & P_4 & P_5 & P_5 & \theta & \\ 
P_1 & -5 & \frac{2}{3} & 1 & 1 & 0 & -\frac{3}{2} & 0 & -\frac{7}{6} &  &L_1'=L_1-7L_3' \\ 
P_5 & 0 & \frac{7}{3} & 0 & 1 & 0 & -\frac{3}{2} & 1 & -\frac{17}{6} &  & L_2'=L_2-17L_3'\\ 
P_3 & -7 & \frac{1}{3} & 0 & 0 & 1 & \frac{1}{2} & 0 &\frac{1}{6}  & &L_3'=\frac{L_3}{6} \\ 
 & z & \frac{7}{3} & -5 & 10 & 0 & \frac{13}{2} & 0 & \frac76 &  & L_4'=L_4+7L_3'
\end{matrix}$ Have I maybe done something wrong? Because from the last tableau we get that $P_1$ gets out of the basis and $P_1$ gets in the basis... EDIT : We could introduce the artificial variables as follows: $$- \max (-5x_1+10x_2-7x_3+3x_4) \\ x_1+x_2+7x_3+2x_4+x_5=3 \\ -2x_1-x_2+3x_3+3x_4+x_6=2 \\ 2x_1+2x_2+8x_3+x_4+x_7=4 \\ x_i \geq 0, i=1,2, \dots, 7$$ At the first phase, we solve the linear programming problem $\min (x_5+x_6+x_7)$ under the new restrictions and at the second phase the initial problem, for which we will have found from the first phase a basic feasible solution. The problem of the first phase can be written as follows: $$ -\max (-x_5-x_6-x_7) \\ x_1+x_2+7x_3+2x_4+x_5=3 \\ -2x_1-x_2+3x_3+3x_4+x_6=2 \\ 2x_1+2x_2+8x_3+x_4+x_7=4 \\ x_i \geq 0, i=1,2, \dots, 7$$ Then I thought the simplex tableau would be the following: $\begin{matrix}
B & c_B & b & P_1 & P_2 & P_3 & P_4 & P_5 & P_6 & P_7 & \theta & \\ 
P_5 & -1 & 3 & 1 & 1 & 7 & 2 & 1 & 0 & 0 &  & L_1\\ 
P_6 & -1 & 2 & -2 & -1 & 3 & 3 & 0 & 1 & 0 &  & L_2\\ 
P_7 & -1 & 4 & 2 & 2 & 8 & 1 & 0 & 0 & 1 &  & L_3\\ 
 & z & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 &  & L_4
\end{matrix}$ But I found that it is this one: How do we find the $z_k-c_k$- values? EDIT 2 : I get the following tableaus: $\begin{matrix}
B & c_B & b & P_1 & P_2 & P_3 & P_4 & P_5 & P_6 & P_7 & \theta & \\ 
P_1 & -1 & 3 & 1& 1&7  & 2 & 1 & 0 & 0 & \frac{3}{7} & L_1 \\ 
P_5 & -1 & 2 & -2 & -1 & 3 & 3 & 0& 1 & 0 &\frac{2}{3} &L_2\\ 
P_6 & -1 & 4 & 2 & 2 & 8 & 1 & 0 & 0 & 1 &\frac12 &L_3\\ 
 & z & -9& -1 & -2 & -18 & -6& 0& 0 & 0 & &
\end{matrix}$ $$$$ $
\begin{matrix}
B & c_B & b & P_1 & P_2 & P_3 & P_4 & P_5 & P_6 & P_7 & \theta & \\ 
P_3 & 0 & \frac{3}{7} & \frac{1}{7}& \frac{1}{7}&1  & \frac{2}{7}& \frac{1}{7} & 0 & 0 & \frac{3}{2} & L_1'=\frac{L_1}{7} \\ 
P_6 & -1 & \frac{11}{7} & \frac{-17}{7} & -\frac{10}{7} & 0 & \frac{15}{7} & -\frac{3}{7}& 1 & 0 &\frac{11}{15} &L_2'=L_2-3L_1'\\ 
P_7 & -1 & \frac{4}{7} & \frac{6}{7} & \frac{6}{7} & 0 & -\frac{9}{7} & -\frac{8}{7} & 0 & 1 &- &L_3'=L_3-8L_1'\\ 
 & z & \frac{-9}{7}& \frac{11}{7} & \frac{4}{7} & 0 & \frac{-6}{7}& \frac{18}{17}& 0 & 0 & & L_4'=L_4+18 L_1'
\end{matrix}$ $
\begin{matrix}
B & c_B & b & P_1 & P_2 & P_3 & P_4 & P_5 & P_6 & P_7 & \theta & \\ 
P_3 & 0 & \frac{23}{105} & \frac{7}{15}& \frac{1}{3}&1  & 0& \frac{1}{5} & -\frac{2}{15} & 0 &  & L_1''=L_1'-\frac{2}{7}L_2'' \\ 
P_4 & 0 & \frac{11}{15} & \frac{-17}{15} & -\frac{2}{3} & 0 & 1 & -\frac{1}{5}& \frac{7}{15} & 0 & &L_2''=\frac{7}{15} L_2'\\ 
P_7 & -1 & \frac{17}{5} &  -\frac{123}{35} & 0 & 0 & 0 & -\frac{7}{15} & \frac{9}{15} & 1 & &L_3''=L_3'+\frac{9}{7}L_2''\\ 
 & z & \frac{3}{5}& \frac{3}{5} & 0 & 0 & 0& \frac{12}{5}& \frac{6}{15} & 0 & & L_4''=L_4'+\frac{6}{7} L_2''
\end{matrix}$ $$$$ But according to my textbook it should be as follows: Have I done something wrong at the calculations?","['simplex', 'optimization', 'linear-programming', 'discrete-mathematics']"
1600830,"What is meant by a ""hyperbolic periodic orbit""?","At the end of the Wikipedia article on ""hyperbolic sets"" ( https://en.wikipedia.org/wiki/Hyperbolic_set ) there is a reference to a periodic orbit being ""hyperbolic"", i.e. a periodic orbit of a diffeomorphism $f: M \to M$ with period $n$ is hyperbolic if and only if $Df^n$ at any point of the orbit has no eigenvalue with absolute value 1. I am confused by how this definition extends to flows as it seems to be related instead to periodic points. In particular, given a periodic orbit $x(t) = x(t+T)$ of a  (non-)autonomous $C^1$-vector field, what does it mean for $x(t)$ to be ""hyperbolic""? An analogy with fixed points would be helpful.","['ordinary-differential-equations', 'differential-geometry']"
1600846,Verify the following identities,I want to verify the following identities: $${\sin^3\alpha-\cos^3\alpha\over \sin\alpha -\cos\alpha} = 1 + \sin\alpha \cos\alpha$$ I feel like I need to work on the first member – the second one looks fine. I can't really figure out how to transform the first one. Any hints?,['trigonometry']
1600860,Is every linear operator which is $SO(n)$-invariant necessarily isotropic?,"Let $M_n$ be the vector space of $n \times n$ real matrices. We say a linear operator $\alpha:M_n \to M_n$ is hemitropic* if: $(*) \, \, \alpha(S^TXS)=S^T\alpha(X)S \, , \, \forall S \in SO(n)$ and isotropic , if the above formula holds $\forall S \in O(n)$ My question: Is every hemitropic operator necessarily isotropic? Does the answer change if we assume $\alpha$ is injective? In odd dimensions, the answer is yes, since for any $Q \in O(n) \setminus SO(n)$, $\det(-Q)=1$ so $-Q \in SO(n)$ and by (*): $\alpha(Q^TXQ)=\alpha\big((-Q)^TX(-Q)\big)=(-Q)^T\alpha(X)(-Q)=Q^T\alpha(X)Q$ I suspect the answer in even dimensions is negative, but so far I didn't find a way to construct a hemitropic non-isotropic operator. (Though I guess this can be done even in dimension $2$). Update: For dimension $2$ I have constructed (see answer below) a hemitropic non-isotropic operator. However, my construction relied on the fact that all $2$-dimensional rotation commute. This is not the case for higher dimensions. (See here ). This leaves open the question for even dimensions above two. The terminology ""hemitropic"" comes from Elasticity theory.","['rotations', 'matrices', 'symmetry', 'linear-transformations', 'linear-algebra']"
1600883,Variables as Elements of Sets,"Suppose we have a set $A = \{1, 2, 6\}$. Let's also say we have a variable $x$. If you were asked if $x \in A$ is true, without knowing the value of $x$, how would you respond? Would the answer be false? Is there insufficient information so far? Further, suppose we have another set $B = \{x, y, 7\}$. Is this notation even valid? Can we say that the variable $y \in B$ without knowing the value of $y$? One last example: if we have a set $C =\{x, y, z\}$, to determine if $a \notin C$, would we need to know all of the following: $a \neq x$, $a \neq y$, $a \neq z$?",['elementary-set-theory']
1600934,Random process involving CDF and PDF of standard normal.,"Im studying old exams and found this one: Let 
$ \Phi(x)=\int_{-\infty}^{x} \frac{1} { \sqrt{2\pi} } e^{-y^2 /2} dy $
and $ \phi(x)=\Phi^\prime(x)=\frac{1} { \sqrt{2\pi} } e^{-x^2 /2} $
be the standard normal (zero - mean and unit variance) cummulative probability distribution function and the standard normal probability density function, respectively. Find a random process $(X(t), \, t \in \mathbb{R}) $ such that the following calculation is valid:
\begin{align}P\left(X(1) ≤ 0, X(2) ≤ 0\right) &= P\left(X(1) ≤ 0, X(2)-\frac{1} {\sqrt{2}}X(1)≤-\frac{1} {\sqrt{2}}X(1)\right)\\&=\int_{-\infty}^{0} P\left(X(2)-\frac{1} {\sqrt{2}}X(1)≤-\frac{1} {\sqrt{2}}x\right)\phi(x) dx\\&=\int_{-\infty}^{0} \Phi(-x)\phi(x) dx =\int_{0}^{\infty} \Phi(x)\phi(x) dx =\Bigg[\frac{\Phi(x)^2} {2}\Bigg]^{\infty}_{0}=\frac{3} {8}\end{align} How should I be thinking about solving a task like this?","['stochastic-processes', 'probability-theory', 'probability-distributions']"
1600942,Proof based problem related to non-trivial solution of a linear equation system,"If the system of linear equations $$a(y+z)-x=0$$ $$b(z+x)-y=0$$$$c(x+y)-z=0$$
  has a non-trivial solution $(a,b,c \neq -1)$,then show that $$\frac{1}{1+a}+\frac{1}{1+b}+\frac{1}{1+c}=2$$ MY ATTEMPT: I took this determinant and put it to $0$. $\begin{vmatrix} -1 & a & a \\ b & -1 & b \\ c & c & -1 \end{vmatrix}=0$ I'm getting $2abc+ab+bc+ca-1=0$ but that is not what is asked for!
Someone help me out !","['algebra-precalculus', 'determinant']"
1600953,Example of a topological vector space which is not locally convex,"I'm currently studying Functional Analysis and the professor gave an example for a TVS (which we have defined to be a vector-space $X$ in which addition $X \times X \rightarrow X, (x, y) \mapsto x + y$ and scalar-multiplication $\mathbf{R} \times X \rightarrow X, (\lambda, x) \mapsto \lambda x$ are continuous), which is not locally convex. The example was the following: Let $L^0([0, 1])$ denote the set of measurable functions $f : [0, 1] \rightarrow \mathbf{R}$ modulo equivalence almost-everywhere for some measure $\mu$. We make this a metric-space by defining: $$d(f, g) = \int \frac{\vert f - g \vert}{1 + \vert f - g \vert} d \mu$$ and the then claimed that this is a TVS with the topology induced by $d$. I wanted to check this, and addition is not too big an issue, but I got stuck on scalar-multiplication. I'd appreciate some help on this. He went on explaining that convergence in $d$ of a sequence $(f_n)_{n \in \mathbf{N}}$ is convergence in measure. The exercise he gave us then (and which would be my question) was: Any non-empty open convex set $A$ in $L^0([0, 1])$ is equal to the whole space. I have very little idea on how to do this. My idea would have been to pick an element $g \notin A$ and then choosing a sequence $(g_n)_{n \in \mathbf{N}} \subset X - A$ converging to $g$. This may give me some contradiction, but I really do not see how to use the convexity of $A$. Thanks for any help!","['functional-analysis', 'locally-convex-spaces', 'topological-vector-spaces']"
1600958,Prove that the determinant is $0$ by expressing as a product,"I need to prove that the determinant $$\begin{vmatrix} my+nz & mq-nr &
 mb+nc \\ kz-mx & kr-mp & kb-ma \\ nx+ky & np+kq & na+kb
 \end{vmatrix}=0$$ In my book it is given as hint that the determinant can be expressed as a product of two other determinants whose value will evaluate to $0$.But I'm not being able to express the given determinant as a product of two other determinants.How should I do it?Please guide me through the procedure.","['matrices', 'determinant']"
1600963,Poisson distribution transformation method,"Random independent variables  $x_1, x_2 \sim \ \ \text{poisson(}\lambda )$ $y=x_1+x_2$ $z=x_1-x_2$ The possible density function $f(y,z)=?$ by using Inverse transformation method. Note that I can solve this question by moment generating function But it's difficult to apply inverse transformation method. First of all, $x_1=(y+z)/2 $  and $ x_2=(y-z)/2$ And $$f(x_1, x_2)= \frac{e^{-2\lambda}\lambda^{(x_1+x_2)}}{x_1! x_2!}$$ $$f(y,z)= \sum_{x_2} \sum_{x_1}f(x_1,x_2)=\sum\sum  \frac{e^{-2\lambda}\lambda^{y}}{\left(\frac{y+z}{2}\right)!
                                \left(\frac{y-z}{ 2}\right)!}$$ In this point I'm stacking. Thank you for helping.","['probability-theory', 'poisson-distribution', 'probability-distributions']"
1600994,Is the Zariski topology the same as the cofinite topology?,"Let $R$ be a commutative ring, $spec(R)$ be the set of all prime ideals on $R$. For any ideal $I$ on $R$, we define the $V_I$ to be the set of all prime ideals containing $I$. We define the Zariski topology on $spec(R)$ via the closed sets $\{V_I:I\textrm{ is an ideal of }R\}$. I am still wrapping my mind around this topology. Can someone tell me if it is a cofinite topology, i.e. the open sets are complements of finite sets, or not?","['abstract-algebra', 'maximal-and-prime-ideals', 'ring-theory', 'commutative-algebra', 'ideals']"
1601020,Find sum of series $\sum_{n=1}^{\infty}\frac{1}{n(4n^2-1)}$,"I need help with finding sum of this:
$$
\sum_{n=1}^{\infty}\frac{1}{n(4n^2-1)}
$$
First, I tried to telescope it in some way, but it seems to be dead end. The only other idea I have is that this might have something to do with logarithm, but really I don't know how to proceed. Any hint would be greatly appreciated.",['sequences-and-series']
1601045,"If $f\in\mathcal{H}(\mathbb{C})$ not constant, then $f(\mathbb{C})$ is dense in $\mathbb{C}$.","By reductio ad absurdum suppose that $f(\mathbb{C})$ is not dense in $\mathbb{C}$, then there exists $w_0\in\mathbb{C}$ and $\varepsilon_0>0$ such that $|f(\zeta)-w_0|\geq \varepsilon_0$ for all $\zeta\in\mathbb{C}$; this implies:
$$|\frac{1}{f-w_0}|\leq \frac{1}{\varepsilon_0} \text{ for all } \zeta\in \mathbb{C}$$ As $f\in\mathcal{H}(\mathbb{C})$ we have that: $$\frac{1}{f-w_0}\in\mathcal{H}(\mathbb{C})$$
So by Liouville's theorem $f$ is constant.","['complex-analysis', 'proof-verification']"
1601051,How many arbitrary constants to put in these integrals?,"How many different constants is it right to use in these cases of integration? $1) \int \frac{1}{x} dx= \begin{cases} \log(x)+c_1, x>0\\\log(-x)+c_2 x<0 \end{cases}$ Since the domain it's not an interval, so the constants can be different, right? $2)\int \mid x \mid dx=\begin{cases} \frac{x^2}{2}+c_1, x\geq0 \\ -\frac{x^2}{2}+c_2 x<0 \end{cases}=\begin{cases} \frac{x^2}{2}+c_1, x\geq0 \\ -\frac{x^2}{2}+c_1 x<0 \end{cases}$ In this case I must impose $c_1=c_2$ right? Otherwise the function is not even continuous in $0$. Are these correct? Thanks for your help","['integration', 'calculus']"
1601059,Is a series of successive derivatives known/useful?,"So, while trying to find something else, it looks like I've found, for many $f(x)$: $$f(x) + f'(x) + f''(x) + f^{(3)}(x) + \dots + f^{(n)}(x)$$ Assuming that there is an easy way to find this sum above, is there any use for it?  I will elaborate a bit.  I mean that I believe I have found a method that finds the sum of all of the derivatives above, and is much faster than calculating each derivative.  In fact, it seems that calculating the sum above for most functions isn't much harder than calculating $f^{(n)}(x)$, and it also should give a ""closed form"" of elementary expressions for most $f(x)$. I have one example that comes to mind: a ""closed form"" for a partial sum of $e^x$, as in this question .  If my ideas work, we would have the closed form that this question asks for. So I'm wondering, is there anything else that this method is useful for? IMPORTANT NOTE I'm assuming that we have use of the ""fractional calculus"", which gives us the ability to calculate $f^{(n)}$ reasonably well and efficiently, using ""differintegrals"".  This may make the sum above fairly trivial.  I'm sorry if I misled anyone.","['derivatives', 'intuition', 'calculus']"
1601065,The sum of subspaces is the smallest subspace containing all the summands,"In Axler's Linear Algebra Done Right the theorem given is Suppose $U_1,\ldots, U_m$ are subspaces of $V$. Then $U_1+\cdots+ U_m$ is the smallest subspace of $V$ containing $U_1,\ldots, U_m.$ I can see that the sum will be a subspace of $V$. What I don't understand is the following paragraph: ""Clearly $U_1,\ldots, U_m$ are all contained in $U_1+\cdots+ U_m$ (to see this consider sums $u_1 + \cdots+u_m$ where all except one of the $u$s is $0$). Conversely every subspace of V containing $U_1,\ldots, U_m$ must contain $U_1+\cdots+ U_m$ (because subspaces must contain all finite sums of their elements). Thus, $U_1+\cdots+ U_m$ is the smallest subspace of $V$ containing $U_1,\cdots, U_m.""$ This question was addressed here: Misunderstanding in the proof that the sum of subspaces is the smallest containing subspace. but the answer wasn't clear enough for me. I can see how the first part is true, that  $U_1,\ldots, U_m$ are all contained in $U_1+\cdots+ U_m$ and the second part, $U_1,\ldots, U_m$ must contain $U_1+\cdots+ U_m$ but I don't understand how together they prove the theorem. Can someone please take the trouble of giving an intuitive explanation, or a geometric one if it is possible? Thanks a lot.","['linear-algebra', 'proof-explanation']"
1601067,How to differentiate both sides with an independent variable if one doesn't have a formula?,"I have an equation similar to the following: $$\frac{a}{b} = c$$ Now I want to differentiate both sides with respect to an independent variable: $$\frac{\mathrm{d} }{\mathrm{d} x} (\frac{a}{b}) = \frac{\mathrm{d} }{\mathrm{d} x}(c)$$ I have the data that represents $a/b$ and $c$ and how it changes with $x$ but I do not have a formula. Now if I try to differentiate like that I would get $0$. Right? Because the derivative of a constant is equals to zero. Is there a way to calculate the derivatives when you do not have an explicit formula? Note: If this question sounds stupid, I am sorry because I am very new to calculus.","['derivatives', 'calculus']"
1601099,Proving that there only finitely many minimal prime ideals of any ideal in Noetherian commutative ring,"Currently, I'm trying to solve a problem from a textbook: Let $R$ be a commutative Noetherian ring with identity, and let $I \subset R$ be a proper ideal of $R$ . Then we know that set of prime ideals of $R$ containing $I$ has minimal elements by inclusion (I decided to call this set $\mathrm{Min}(I)$ in sequel). Prove that $\mathrm{Min}(I)$ is finite. There is also a hint: Define $\mathcal{F}$ as set of all ideals $I$ of $R$ such that $ \vert \mathrm{Min}(I) \vert = \infty$ . Assume that $\mathcal{F} \neq \emptyset$ . Then it must have a maximal element $I$ . Find ideals $J_1,J_2$ such that they all strictly include $I$ , such that $J_1J_2 \subset I$ and deduce a contradiction. So I went along this hint: $I$ can't be a prime, as a prime is the only minimal prime over itself. It means that $\exists a,b \not \in I: ab \in I$ . As $R$ is Noetherian there is a finite list of elements that generates $I = (r_1, \dots, r_n)$ . Then  it's possible to set $J_1 = (r_1, \dots,r_n,a)$ , $J_2 =(r_1, \dots, r_n, b )$ with all required properties. As $I$ is maximal in $\mathcal{F}$ the sets $\mathrm{Min}(J_1)$ and $\mathrm{Min}(J_2)$ must be finite. I am failing to find a desired contradiction and will be grateful for any help.","['abstract-algebra', 'ring-theory', 'ideals', 'commutative-algebra']"
1601103,Displacement of a point on a circle,There is a mark on a wheel of radius $30$ cm. The mark is in contact with a horizontal plane. The wheel rotates to a distance of $10\pi \approx 31.4$ cm. $1.$ What is the angle that the old position of the mark makes with the new position? $2.$ What is the distance between the new position of the mark with that of the ground? [point B can be anywhere on the circle],"['circles', 'trigonometry', 'geometry']"
1601116,Atoms in measurable spaces,"In this post, by ""separable measurable space"" I mean the sigma algebra can be generated by a 
countable collection of sets. An algebra is a collection of sets that contains the empty set 
and is closed under complement and finite intersection. Definition Let $(\Omega, \mathcal F)$ be a measurable space (not necessarily separable).
 For any $\omega \in \Omega$, let 
$$\mathcal F_\omega=\{ B\in \mathcal F:\omega\in B \}, A(\omega)=\bigcap_{B\in \mathcal F_\omega} B.$$ $A(\omega) $ is called the $\mathcal F$-atom containing $\omega$. $A(\omega) $ is not necessarily in $\mathcal F$. $A(\omega)$ is not the smallest measurable set containing $\omega$(which may be different from other definitions you have seen before). Here is my question: Let $\mathcal C$ be an algebra generating $\mathcal F$. For any $\omega \in \Omega$, 
    let $\mathcal C_\omega=\{ C\in \mathcal C:\omega\in C \}$, then do we have
    $$A(\omega)=\bigcap_{C\in \mathcal C_\omega} C.$$
    for general measurable spaces $(\Omega, \mathcal F)$? If it fails for a nonseparable measurable space, does it still hold for a separable 
    measurable space $(\Omega, \mathcal F)$? I have tried monotone class theorems for this problem but I find it hard to construct 
a ""good"" collection of sets. Every comment, solution or counterexample will be appreciated!","['probability-theory', 'measure-theory']"
1601118,Spivak Calculus on Manifolds - Theorem 4-10,"Part (4) of Theorem 4-10 in Spivak's Calculus on Manifolds says the following: If $\omega$ is a $k$-form on $\mathbb{R}^m$ and $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is differentiable, then $f^{*}(d \omega) = d(f^{*}\omega)$. In the Proof, Spivak says that it is clear if $\omega$ is a $0$-form. I tried expanding both sides using the definitions, but I'm not getting the desired result even after a lot of effort. I suppose I'm only missing something straightforward. Could anyone please help me out? Thanks in advance. Edit: Some of what I attempted is as follows: \begin{align}
&\ f^{*} d\omega (p) (v_p)\\
=&\ f^{*} \bigl(d\omega (f(p))\bigr)(v_p)\\
=&\ d\omega \bigl(f(p)\bigr) (f_{*}v_p)\\
=&\ d\omega \bigl(f(p)\bigr) \bigl(Df(p)(v)\bigr)_{f(p)}\\
=&\ D\omega (f(p))(Df(p)(v))
\end{align} I also tried writing $d\omega$ as $\sum_{i=1}^n \omega_i dx^i$, so that
\begin{align}
f^{*} d\omega &= f^{*}\left(\sum_{i=1}^n \omega_i dx^i\right)\\
&= \sum_{i=i}^n f^{*}(\omega_i dx^i)\\
&= \sum_{i=1}^n \omega_i \circ f \cdot f^* (dx^i)\\
&= \sum_{i=1}^n \omega_i \circ f \cdot \sum_{j=1}^n D_j f^i \cdot dx^j
\end{align} Then, $d(f^*\omega)(p)(v_p) = D(f^*\omega)(p)(v)$, but I don't know how to connect this with the last line. As I understand, a $0$-form is just a function from $\mathbb{R}^n$ to $\mathbb{R}$. The operator $d$ takes a $k$-form and converts it into a $k+1$-form.","['multivariable-calculus', 'differential-forms']"
