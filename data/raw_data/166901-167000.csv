question_id,title,body,tags
2910605,Spivak Calculus Chapter 5 Limits Problem 22?,"Hmm, 4th edition, did I find another error in this book? (This turns out to be misunderstanding one word that makes a huge difference, edited) It's Chapter 5 Question 22 about limits. The question and it's answer exactly is: Question: Consider a function $f$ with the following property: if $g$ is any function for which $\lim_{x\to 0}g(x)$ does not exist, then $\lim_{x\to 0}[f(x)+g(x)]$ also does not exist. Prove that this happens if and only if $\lim_{x\to 0}f(x)$ does exist. Hint: This is actually very easy: the assumption that $\lim_{x\to 0}f(x)$ does not exist leads to an immediate contradiction if you consider the right $g$. Answer from Answer book: If $\lim_{x\to 0}f(x)$ does exist, then it is clear that $\lim_{x\to 0}[f(x)+g(x)]$ does not exist whenever $\lim_{x\to 0}g(x)$ does not exist [this was Problem 8(b) and (c)]. On the other hand, if $\lim_{x\to 0}f(x)$ does not exist, choose $g=-f$; then $\lim_{x\to 0}g(x)$ does not exist, but $\lim_{x\to 0}[f(x)+g(x)]$ does exist. I think the question is wrong or a typo(on ""if and only if"")? if $f(x)=1/x$ and $g(x)=1/x+1$, then $\lim_{x\to 0}[f(x)+g(x)]$ does not exist. And the answer from ""On the other hand"" then on is point less, because randomly choose $g=-f$ only proves something can be true/false, but not must be true/false.",['limits']
2910607,Where does this series converges $1+\frac{1+2}{2!}+\frac{1+2+3}{3!}+\cdots$,"Consider the following series, $$1+\frac{1+2}{2!}+\frac{1+2+3}{3!}+\cdots$$ My Efforts The series can be written in compact for as, $$\sum_{n=1}^{\infty}{n(n+1)\over 2n!}$$ $$={1\over 2}(\sum_{n=1}^{\infty}{n^2\over n!}+\sum_{n=1}^{\infty}{n \over n!})$$
$$={1\over 2}(\sum_{n=1}^{\infty}{n^2\over n!}+\sum_{n=1}^{\infty}{1 \over (n-1)!})$$ Now 
$$\sum_{n=1}^{\infty}{1 \over (n-1)!}=1+1+{1\over 2!}+{1\over 3!}+\cdots=e$$
First summand can be simplified as, 
$$\sum_{n=1}^{\infty}{n \over (n-1)!}$$ But where does it converge? Any hints? Edit: Thank you for all the quick replies. I now realize it was so easy. I just had to use $n=n-1+1$.","['calculus', 'sequences-and-series']"
2910608,Doubts Regarding Proof Of Cauchy Integral Formula,"I was trying to understand the proof of Cauchy Integral formula from the book ""Complex Analysis"" By Stein and Shakarchi. I understand most steps of that proof, but I have one doubt: I understand that due to Cauchy Integral Theorem, the integral of the domain bounded by the curve $C$ is 0, and this irrespectively of the value around the $|\zeta|=\epsilon$ part. However in book they state that in the limit $\delta \to0$ the integral of $f(\zeta)/{(\zeta -z)}$ equals the one around the exterior circle part of $C$ plus the integral around the circle $|\zeta|=\epsilon$, and I do not get this part . Any Help will be appreciated","['complex-analysis', 'holomorphic-functions', 'cauchy-integral-formula', 'analysis']"
2910682,How to compute Lower Central Series by hand for this simple example,"Let $G=\langle x,y,z\mid z^2=1\rangle\cong \mathbb{Z}*\mathbb{Z}*\mathbb{Z}/2$. I am interested to compute by hand (or by any other means) the quotient groups $\gamma_n/\gamma_{n+1}$ where $\gamma_n$ is the $n$th term of the lower central series. That is $\gamma_1=G$, $\gamma_2=[G,G]$, $\gamma_3=[\gamma_2,G]$, etc. For $\gamma_1/\gamma_2$, it is essentially the abelianization of $G$, hence it is $\mathbb{Z}\oplus\mathbb{Z}\oplus\mathbb{Z}/2$. I am facing some difficulties computing $\gamma_2/\gamma_3$. I computed that $\gamma_2=[G,G]=\langle x^{-1}y^{-1}xy,x^{-1}z^{-1}xz,y^{-1}z^{-1}yz\mid z^2=1\rangle$. (Update: This is probably wrong.) However things start to get complicated with $\gamma_3=[\gamma_2,G]$. Is there an ""easy"" way to find $\gamma_2/\gamma_3$ or is brute-force the way to go? Thanks. Update: I think my expression for $\gamma_2$ may be wrong, there should be way more generators than just the 3 commutators $x^{-1}y^{-1}xy,x^{-1}z^{-1}xz,y^{-1}z^{-1}yz$, in fact $\gamma_2$ may not even be finitely generated?","['group-theory', 'abstract-algebra', 'free-product']"
2910691,"$Y\subseteq X$ iff $X\cup Y^c=\Omega$, and $X\cap Y=\emptyset$ iff $X^c\cup Y^c=\Omega$","Let $X$ and $Y$ be subsets of the universe $\Omega$. Prove the following: 1) $Y\subseteq X$ iff $X\cup Y^c=\Omega$ 2) $X\cap Y=\emptyset$ iff $X^c\cup Y^c=\Omega$ Here $^c$ denotes the complement The statements do logically makes sense, however I'm having trouble proving it formally.",['discrete-mathematics']
2910695,Obtaining equation of Tangent(s) to A Curve at origin by equating the lowest degree terms to zero. [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question In a rational algebraic expression in 2 variable having no constant term, equation of the tangent at origin can be formed by equating the lowest degree term to be zero. Ex. For $ x^4+y^4+2xy^2-2y=0 $ the equation of tangent at origin is $ 2y=0 $ or $ y=0 $ Is there an Analytical Method to prove this or is this pure observation?","['calculus', 'implicit-differentiation', 'derivatives', 'tangent-line']"
2910750,"knowing: $ \tan x=2-\sqrt{3}$ , obtain: $ \cos 2x$","Knowing: $$ \tan x=2-\sqrt{3} $$ Obtain: $$\cos2x$$ I tried converting $\tan x$ into it's sinus and cosine form and trying to square both sides to try to get the form of: $$\cos^2x-\sin^2x$$ But I can't really get to this form without having extra expressions of sine or cosine, any ideas how to start this properly? Taken out of one of the entry tests to Maths in TAU. Solution: $$\cos2x=\cos^2x-\sin^2x=\frac{\cos^2x-\sin^2x}{\sin^2x+\cos^2x}:\frac{\cos^2x}{\cos^2x}=\frac{1-\tan^2x}{1+\tan^2x}$$ $$ \frac{1-\tan^2x}{1+\tan^2x}=\frac{-6+4\sqrt{3}}{8-4\sqrt{3}}=\frac{-3+2\sqrt{3}}{4-2\sqrt{3}} $$","['trigonometry', 'calculus', 'algebra-precalculus']"
2910795,How to prove $\int_0^\infty e^{-x}\ln^n \frac{1}{x}dx<n!$?,"Once I met a problem about limits: $$\lim_{n\rightarrow\infty}\int_0^\infty e^{-x}\frac{\ln^n \frac{1}{x}}{n!} dx=1$$ After I proved it, I used Mathematica and discovered that the sequence $\int_0^\infty e^{-x}\frac{\ln^n\frac{1}{x}}{n!} dx$ seems to be monotone increasing to its supremum $1$. I guess so, but I’m confused about how to prove. My question is: How to prove that $$\int_0^\infty e^{-x}\ln^n \frac{1}{x}dx<n!,\quad \forall n\ge 1$$ and the monotonity of $$\int_0^\infty e^{-x}\frac{\ln^n\frac{1}{x}}{n!} dx$$ Additional: I have already proved that $$\lim_{n\rightarrow\infty} \frac{2^{n+1}}{n!}\left(n!-\int_0^\infty e^{-x}\ln^n \frac{1}{x}dx\right)=\lim_{n\rightarrow\infty} \frac{2^{n+1}}{n!}\left(\int_0^1 \left(1-e^{-x}\right)\ln^n\frac{1}{x} dx-\int_1^\infty e^{-x}\ln^n\frac{1}{x} dx\right)=1$$ by using the inequality $x-\frac{x^2}{2}\le 1-e^{-x}\le x$ and $\ln^n x<n! x\left(x\ge 1\right)$. It can indicate that $1-\int_0^\infty e^{-x}\frac{\ln^n \frac{1}{x}}{n!} dx$ is EVENTUALLY  (which means for sufficient large $n$) positive and monotone decreasing at $O\left(2^{-n}\right)$.","['improper-integrals', 'analysis', 'real-analysis', 'complex-analysis', 'limits']"
2910803,"What is a jump function, and which measure does it induce?","I'm facing now for the first time the topic in the title, and found myself having hard times to figure out what a jump function is. If you look for anything on google you'll always find stuff about C++ or assembly jumps which is of course not what I'm looking for. Here's what I have in my notes: Suppose being assigned $n$ real numbers $t_1,t_2,\dots,t_n$ where $\forall i, \;\;t_i\in\mathbb{R}$, and the same amount of corresponding jump values $h_1,h_2,\dots,h_n$. Then we define the jump function as $F(t):\mathbb{R}\to\mathbb{R}$ given by $F(t) = \sum\limits_{j\;|\;t_j \leq t} h_j$ and this is a probability measure if $F(t)$ has $1$ as horizontal asymptote. This is quoted as example in my notes for a Lebesgue-Stieltjes measure, but still I'm not getting how is this related, and moreover I'm not getting how the plot of a jump function should look like (maybe like steps? It resembles a cumulative distribution function for a discrete random variable).","['measure-theory', 'stieltjes-integral', 'lebesgue-integral', 'probability-theory']"
2910835,Doubt about the RADEMACHER'S THEOREM demonstration.,"I was recently reading an article called An Elementary Proof of Rademacher's Theorem. In the text below $U$ is a ball and $ f: U \to \mathbb{R}$ is a lipschitz function. Let $S$ the set of $x\in U$ for which $\partial_v f(x)$ does not exist. My question. How can we prove that the continuity of $ f $ implies that the set $ S $ is measurable? I'm trying to look at $ S $ as the following set.
$$
S=
\left\{ 
x\in U 
\left|
\begin{matrix} 
\forall T:\mathbb{R}^n\to \mathbb{R} \mbox{ linear }\exists \epsilon>0 
\\ \mbox{ such that }
\forall \delta>0 
\\
|t|<\delta \mbox{ and } \frac{|f(x+tv)-f(x)-Tv|}{|v|}>\epsilon
\end{matrix}
\right.
\right\}
$$
Then, prove that such a set is measurable. But something tells me that $ S $ can not be described in this way. So the question seems to be this. How to describe $ S $ properly and then prove that it is a measurable set? Update. Examining the text in the image better, I realized that I can express the set $ S $ in the form below. I believe it's the correct expression for $ S $. Recall that $|v|=1$. $S=\{x\in U: \partial_{v}f(x) \mbox{ does not exist }\}$ $S=\left\{x\in U: \lim_{t\to 0} \frac{f(x+tv)-f(x)}{t}\mbox{ does not exist }\right\}$ $S=\left\{x\in U\left| \begin{matrix}
\mbox{ logical negation of} \\
(\exists \partial_vf(x)\in\mathbb{R}) (\forall \epsilon>0)(\exists \delta >0)
\\
0<|t|<\delta\implies\left|\frac{f(x+tv)-f(x)}{t}-\partial_vf(x)\right|<\epsilon
\end{matrix}\right.\right\}$ $S=\left\{x\in U\left| \begin{matrix}
(\forall \partial_vf(x)\in\mathbb{R}) (\exists \epsilon>0)(\forall \delta >0)
\\
0<|t|<\delta\mbox{ and } \left|\frac{f(x+tv)-f(x)}{t}-\partial_vf(x)\right|\geq \epsilon
\end{matrix}\right.\right\}$ $S=\displaystyle\bigcap_{L\in\mathbb{R}}
\displaystyle\bigcap_{\delta>0} \big( S_{L,\delta}^+\cup S_{L,\delta}^+) 
$ for 
$
S_{L,\delta}^+=\left\{x\in U\left| 
\frac{f(x+tv)-f(x)}{t}\leq L+ \epsilon,
0<|t|<\delta
\right.\right\}
$ 
and 
$
S_{L,\delta}^-=\left\{x\in U\left| 
\frac{f(x+tv)-f(x)}{t}\geq L - \epsilon,
0<|t|<\delta
\right.\right\}
$","['partial-derivative', 'measure-theory', 'lebesgue-measure', 'real-analysis']"
2910845,"If $\sin^8(x)+\cos^8(x)=48/128$, then find the value of $x$?","If $$\sin^8(x)+\cos^8(x)=48/128,$$ then find the value of $x$ ? I tried this by De Moivre's theorem: $$(\cos\theta+i\sin\theta)^n=\cos(n\theta)+i\sin(\theta)=e^{in\theta}$$ But could not proceed further please help.","['trigonometry', 'complex-numbers']"
2910857,Bredon's Cone Construction,"Bredon has a slick proof based on the cone construction,  that if $X$ is a contractible topological space, then $H_i(X)=0$ for $i\neq 0$. My question has to do with a small detail: Let $F:X\times I\to X$ be a homotopy from $1_X$ to a constant function $x_0\in X,\ $ set $t'=1-t_0,$ and for each singular simplex $\sigma:\Delta_{n-1}\to X$ define $D\sigma:\Delta_n\to X$ by $D\sigma(t_0e_0+\cdots +t_ne_n)=F\left ( \sigma \left ( \frac{t_1}{t'}e_0+\cdots \frac{t_n}{t'}e_{n-1} \right ),t_0 \right ),\ $ It is then straightforward to prove that $\partial D+D\partial=1-\epsilon,$ as desired. Now, $D\sigma,$ as it stands, is not defined at $t_0=1$, so I guess that Bredon is assuming without comment that $D\sigma(e_0)=x_0.$ My question is: how do we prove continuity of $F$ at $e_0?$ It is clear that $\left \{ \frac{t_1}{t'}e_0+\cdots \frac{t_n}{t'}e_{n-1} \right \}$ is bounded by $1$ as $t_i$ ranges over $0\leq t_i\le 1$ with $\Sigma t_i=1.$ Then, if $z_n\to e_0\in \Delta_n,\ $ this induces a sequence $\lambda_n\in I$ such that $\lambda_n\to 1$ and a sequence $w_n\in \Delta_{n-1}.$ Now, using compactness, we can find an $x\in X$ and subsequences $w_{n_k},\lambda_{n_k}$ such that $\sigma (w_{n_k})\to x$ and $\lambda_{n_k}\to 1.$ Continuity of $F$ now implies that $F(\sigma (w_{n_k}),\lambda_{n_k})\to F(x,1)=x_0.$ Of course, this is not enough, since I was forced to pass to a subsequence. So, my questions are: is the case $t_0=1$ so trivial that Bredon left it out of his definition? How do we prove continuity of $D\sigma$ on all of $\Delta_n?$","['homology-cohomology', 'algebraic-topology', 'analysis', 'real-analysis']"
2910885,characteristic function of a convolution of measures,"Take the probability measures $\mu,\nu$ on $\mathbb{R}$ and denote $\varphi_{\mu}$ (the same for $\nu$) its characteristic function. Why holds
$$\varphi_{\mu *\nu}(t)=\varphi_{\mu}(t)\cdot\varphi_{\nu}(t)$$
where $\mu*\nu$ denotes the convolution of $\mu$ and $\nu$?","['convolution', 'characteristic-functions', 'functional-analysis', 'probability']"
2910895,Prove that $\alpha:=\sqrt{\pi}+\sqrt 2 \in \Bbb{C} $ is trancendental over $\Bbb{Q}$.,"We want to prove that $\alpha:=\sqrt{\pi}+\sqrt 2 \in \Bbb{C} $ is
  trancendental over $\Bbb{Q}$ . Attempt. We use proof by contradiction and so assume that $\alpha \in \Bbb{C}$ is algebraic over $\Bbb{Q}$ . Then we have that $$\left[\Bbb{Q}\left(\sqrt{\pi}+\sqrt 2\right):\Bbb{Q}\right]<\infty.$$ It is true that $[K(\sqrt \pi ):K]=\infty$ , where $K$ is a field and the proof is the same with this post. So, one can think to use the following relations: $\sqrt \pi,\sqrt2 \in \Bbb{Q}(\sqrt \pi, \sqrt 2) \implies \sqrt \pi+\sqrt2 \ \in \Bbb{Q}(\sqrt \pi, \sqrt 2) \implies \Bbb{Q}(\sqrt \pi+ \sqrt 2)\subseteq \Bbb{Q}(\sqrt \pi, \sqrt 2) $ $\Bbb{Q} \leq \Bbb{Q}(\sqrt2)\leq  \Bbb{Q}(\sqrt \pi+ \sqrt 2)\leq \Bbb{Q}(\sqrt \pi, \sqrt 2)$ and one logical thought is to apply the Tower Law, but I m not sure if this helps. Any ideas please? Thank you.","['field-theory', 'abstract-algebra', 'pi', 'transcendental-numbers', 'extension-field']"
2910924,"Solve $x,y$ given $1+x^2+2x\sin(\arccos y)=0$","I have to solve $x,y$ given- $$1+x^2+2x\sin(\arccos y)=0$$ My attempt: $$1+x^2+2x\sin(\arcsin \sqrt{1-y^2})=0$$ (Is this step valid? Can I convert arccos to arcsin like this?) $$\implies1+x^2+2x\sqrt{1-y^2}=0$$ Now,I can't procced further. What to do next?","['algebra-precalculus', 'inverse', 'trigonometry']"
2911001,A interesting question on Skew-symmetric matrix...finding the determinant.,"Let $a_1,a_2,\cdots ,a_{2n}$ be complex numbers. We construct a $2n \times 2n$ matrix, say $A$ which is skew symmetric and entries are from complex numbers. 
  $A=(\alpha_{ij})$, where $\alpha_{ij}=a_ia_j$ for $i<j$. To find the determinant of the matrix $A$. Since $A$ is a even order skew-symmetric matrix, we have determinant of $A$ a perfect square. My Intuition: $\det A = a_1^2 \times a_2^2 \times \cdots \times a_{2n}^2$. I was trying to see what happens when $n=2$, i.e. we have $4 \times 4$ matrix $A$. Thus we have complex numbers $a_1,a_2,a_3 \  \text{and} \ a_4$ and \
$A=
\begin{bmatrix}
      0 & a_1a_2 & a_1a_3 & a_1a_4 \\
    -a_1a_2 & 0 & a_2a_3 & a_2a_4 \\
    -a_1a_3 & -a_2a_3 & 0 & a_3a_4\\
     -a_1a_4 & -a_2a_4 & -a_3a_4 & 0
\end{bmatrix}
$ We can see 
$
A=
\left[
\begin{array}{c|c}
D_1 & B \\
\hline
-B^T & D_2
\end{array}
\right]
$, where $D_1 = \begin{bmatrix}
0 & a_1a_2\\
-a_1a_2 & 0 
\end{bmatrix}
$, $D_2 = \begin{bmatrix}
0 & a_3a_4\\
-a_3a_4 & 0 
\end{bmatrix}
$ and $
B = \begin{bmatrix}
a_1a_3 & a_1a_4\\
a_2a_3 & a_2a_4
\end{bmatrix}
$ Also I have noted that $\det B =0$. Can we use the result of determinant of block matrices? Can someone shed some light how to do the problem? Let
$
D = \text{diag}(a_1,a_2,\cdots,a_{2n})$ , and $C = (c_{ij})$, where $C$ is a skew symmetric matrix with $c_{ij} = 1$ when $i<j$. Then one can easily see that $A=DCD$. So we are let to prove that $\det C =1$.","['determinant', 'matrices', 'linear-algebra', 'symmetric-matrices', 'matrix-decomposition']"
2911007,variation of an action functional,"I would like to confirm my answer since it is in direct contradiction from my book on physics.  So suppose we have an action functional
$$S(\phi) = \int_M \phi(\Delta + m^2)\phi dV,$$
where $M$ is a manifold and $\phi$ is in $C_c^\infty(U)$, the space of compactly supported smooth functions in an open set $U$. So I get the variation
$$S(\phi+t\psi) = S(\phi) + t(\int_M \psi(\Delta + m^2)\phi dV + \int_M \phi(\Delta + m^2)\psi dV) + t^2 S(\psi),$$
hence getting the functional derivative
$$\int_M \psi(\Delta + m^2)\phi dV + \int_M \phi(\Delta + m^2)\psi dV.$$ But then the book claims that the Euler Lagrange equation is $(\Delta + m^2)\phi=0$.  Is there some condition on $\psi$ that I don't know about or did I misunderstand some of the notations involved?","['calculus-of-variations', 'differential-geometry']"
2911049,"Gradient and Hessian of $x x^T$ w.r.t. $x$, where $x \in \mathbb{R}^{n \times 1}$,?","Question: Can we find the gradient and Hessian of $x x^T$ w.r.t. $x$, where $x \in \mathbb{R}^{n \times 1}$ ? EDIT: 
If we can, may I know how to compute that? Thank you.","['multivariable-calculus', 'matrix-calculus', 'linear-algebra', 'hessian-matrix', 'derivatives']"
2911073,Show $\| X \| = \sqrt{X^* A X}$ is a norm,"I'm trying to show that, given a positive definite matrix $A\in \mathcal{M_n}(\mathbb{C})$, the function $$\begin{array}{cccl}
\lVert \cdot \rVert: &\mathbb{C}^n &\longrightarrow &\mathbb{R}
\\\ &X &\longmapsto & \lVert X \rVert = \sqrt{X^* A X}
\end{array}$$ is a norm. I've proved all properties except $\| X+Y \| \leq \| X \|+\| Y \| \ \forall \ X,Y \in  \mathbb{C}^n$. $$\| X+Y \|= \sqrt{(X+Y)^* A (X+Y)}=\sqrt{(X^*+Y^*) A (X+Y)}=\sqrt{X^*AX+X^*AY+Y^*AX+Y^*AY}$$ $$\| X\| +\|Y \|= \sqrt{X^* A X} + \sqrt{Y^* A Y}$$ I don't know how to arrive to the inequality from here.","['matrices', 'normed-spaces', 'linear-algebra']"
2911077,Why Frey's curve cannot be constructed?,"My question is related to the fact that Frey's curve: $$y^{2} = x(x-a^{p})(x+b^{p})$$ Could not be constructed if Fermat's last theorem holds true. I mean because Fermat's last theorem implies that there is no solution for $a$, $b$, $c$, and prime number $p > 2$ such that: $$a^{p}+b^{p}=c^{p}$$ As a result the Frey's curve which is semi-stable elliptic curve could be not constructed because according to Ribet's theorem this curve is not modular but Taniyama-Shimura-Weil conjecture implies that each elliptic curve should be modular. The thing I can't understand that is when we say Frey's curve could not be constructed in what sense it could not be constructed? I mean the equation $y^{2} = x(x-a^{p})(x+b^{p})$ contains only $a^{p}$ and $b^{p}$ but it's not completely symmetric to contain all $a$, $b$, $c$ in a symmetric way. I appreciate if someone could explain this to me.","['number-theory', 'modular-forms', 'algebraic-geometry', 'elliptic-curves']"
2911085,"Show that if $A ⊆ B$, then $C - B ⊆ C - A$","Show that if $A ⊆ B$,
$C - B ⊆ C - A$ I've tried doing this by introducing an element $x$ that is in both $A$ and $B$ and an element $y$ that is in $B$ and $C$ but I can't seem to find a proof which makes sense.","['elementary-set-theory', 'logic']"
2911108,Calculate $ \biggr\lfloor \frac{1}{4^{\frac{1}{3}}} + \frac{1}{5^{\frac{1}{3}}} + ... + \frac{1}{1000000^{\frac{1}{3}}} \biggr\rfloor$,"Calculate $ \biggr\lfloor  \frac{1}{4^{\frac{1}{3}}}  +   \frac{1}{5^{\frac{1}{3}}}  +   \frac{1}{6^{\frac{1}{3}}}  + ... +   \frac{1}{1000000^{\frac{1}{3}}} \biggr\rfloor$ I am just clueless. I just have some random thoughts. I can find the sum of this series and then put in the value $1000000$ and then find the floor of that. To do this, I would have to telescope this series which seems impossible to me. But is there any other way to directly find the floor without finding the general sum? I am very new to calculus, so please provide hints and answers that dio not involve calculus.","['ceiling-and-floor-functions', 'summation', 'elementary-number-theory', 'sequences-and-series', 'algebra-precalculus']"
2911121,Groups of order $360$ have a subgroup of order $10$,"I want to prove that groups of order $360$ must have a subgroup of order $10$ . By Sylow's theorem, the number of Sylow $5$ -subgroups $n_5 \equiv 1 \pmod 5$ and $n_5\mid 360$ . There are three solutions: $1, 6, 36$ (let me know if I missed any). If $n_5=1$ , then the only one is normal, making the product with an element of order $2$ we get a subgroup of order $10$ . If $n_5=36$ , then pick any Sylow $5$ -subgroup, $[G:N_{G}(P)]=36$ . It follows that $N_G(P)$ is a subgroup of order $10$ . But how to deal with the case when $n_5=6$ ?","['group-theory', 'abstract-algebra', 'finite-groups']"
2911147,"$G=\langle a,b \mid baba^{-1}=1\rangle$ Show that $\langle a \rangle$ is infinite","Let $G=\langle a,b \mid  baba^{-1}=1\rangle$. Show that the subgroup generated by $a$ is infinite. My attempt Suppose $\langle a\rangle$ is finite so $a^k = 1$ for some $k \in \mathbb{Z}$. So I would like to prove that the relation $a^k=1$ is not a consequence of the original relation $baba^{-1}=1$. I also have been able to prove that every element $g$ in $G$ can be written as $g= a^n b^m$ for $n,m \in  \mathbb{Z} $ Any hint will be appreciated.","['combinatorial-group-theory', 'group-presentation', 'group-theory', 'free-groups']"
2911148,How to find out the number of solutions of a given proposition.,"so I'm rather new to Discrete Math and I've been trying to tackle propositional logic. I have the following question: How many situations are there for any given proposition? This question really confuses me. First of all, by situations I guess they mean outputs, and I guess the conclusion can only ever be one at a time? Or are situations rather a combination of truth values? Any input is appreciated.","['propositional-calculus', 'logic', 'discrete-mathematics']"
2911177,Solutions to $\frac{Q\partial P}{x\partial x}-\frac{P\partial Q}{x\partial x}-\frac{Q\partial P}{y\partial y}+\frac{P\partial Q}{y\partial y}=0$,"We have $$
F(x,y)= \frac{Q}{x} \frac{\partial P}{\partial x}
- \frac{P}{x} \frac{\partial Q}{\partial x}  
- \frac{Q}{y} \frac{\partial P}{\partial y} 
+ \frac{P}{y} \frac{\partial Q}{\partial y} 
$$ where $$P = \sum_{i=1}^{N}(a_i x + b_i y)^2 $$
$$Q = \sum_{i=1}^{N}(c_{i} x + d_{i} y)^2$$ and $a_i, b_i, c_{i}, d_{i}$ are constant parameters defined for $1\leq i,j \leq N$. We want to find the solutions of equation $F(x,y)=0$. For a simpler case, where $P$ is defined as above and $Q=1$, by using a change of variable $m=\frac{y}{x}$, we get a quadratic equation for $m$ that could be simply solved and give the two solutions for $m$. I wonder if using same change of variable, one could obtain the solutions for the more general case (with $Q$ of the form given above or similar form). Some background: $x,y$ are coordinates of a point in a two dimensional space and define a line that passes through $(x,y)$ and the origin $(0,0)$. $P$ and $Q$ are derived from projection of some other points on this line. Using $m=y/x$ makes sense since the projections only depend on the slope of the line and not the actual values of $(x,y)$. Our goal is to find the line (defined by its slope $m$) that satisfies $F(x,y)=0$. EDIT: This problem is the special case for a more general problem to find the mapping from an $N$ dimensional space to an $M$ dimensional space that would minimize sum of distances of pairs of similar points ($P$) normalized to sum of distances of pairs of dissimilar points ($Q$). The solution given here solves it for mapping from 2D to 1D. I also posted a new question for the case mapping from 3D to 1D here which I expect to be solvable in a similar way.","['ordinary-differential-equations', 'multivariable-calculus', 'calculus', 'algebraic-geometry', 'differential-geometry']"
2911187,What is this curve?,"Lines (same angle space between) radiating outward from a point and intersecting a line: This is the density distribution of the points on the line: I used a Python script to calculate this. The angular interval is 0.01 degrees. X = tan(deg) rounded to the nearest tenth, so many points will have the same x. Y is the number of points for its X. You can see on the plot that ~5,700 points were between -0.05 and 0.05 on the line. What is this graph curve called? What's the function equation?","['graphing-functions', 'geometry']"
2911205,How many malicious bingo cards are there?,"Consider a flexible form of bingo, where each square contains a condition and you mark off whether or not the condition applies to you. The number of bingos you obtain ostensibly measures the extent to which you satisfy the theme of the bingo card. I, a soulless automaton, have inferred that these are commonly shared as images on social media as ways to bond over shared preferences. Suppose, in my purely hypothetical robotic misanthropy, I wished to construct a standard bingo card (5x5 with a free space in the center) on which no bingos (vertical, horizontal, or diagonal) were possible. To do this, on each possible bingo line, place a pair of squares which cannot be simultaneously satisfied, such as ""tall"" and ""short"". Twelve possible bingos mandates twelve such pairs of squares, and luckily enough there are 24 usable squares in a standard bingo card. Here is an example of one such malicious bingo card, using the letters A through L to denote the pairs: AABCD
EEBCD
FG_HH
FIJKJ
IGLLK How many malicious bingo cards are there? Is there a human-understandable way to count them, or should this enumeration be left to a computer like myself? On a secondary note, are there well-known combinatorial objects they are in a natural bijection with? Searching the vast indices in my hard disks, I can only come up with ""maximum matchings in a particular 24-vertex graph such that each maximal clique contains exactly one matched edge""—I do not expect to find something this convoluted in any of your human mathematical literature.","['matching-theory', 'combinatorics']"
2911215,Fundamental group of $M$ has no subgroup of index $2\Rightarrow M$ is orientable,"Let $M$ be a connected smooth manifold such that, for every $p\in M$, the fundamental group $\pi_1(M,p)$ has no subgroup of index $2$. Prove that $M$ is orientable. Here's what I know: there is a smooth, orientable manifold $\widetilde{M}$ and a covering map $\pi:\widetilde{M}\to M$ whose fibers have cardinality $2$, and such that $M$ is orientable $\Leftrightarrow\widetilde{M}$ is disconnected. Supposing by contradiction that $M$ is not orientable, $\widetilde{M}$ must be connected. I suppose the ""subgroup of index $2$"" part has to do with the fact that the fibers have cardinality $2$, and probably $\widetilde{M}$ being connected also plays a role. I don't know much about covering spaces except for basic definitions, so I'm pretty much stuck.","['fundamental-groups', 'orientation', 'smooth-manifolds', 'differential-geometry']"
2911223,Why a countable set of numbers can characterize a function in Hilbert space?,"I'm a new graduate student major in physics; the only mathematical course I was formally trained is Real Analysis, so my following statement of my question may seem a little unprofessional, and I hope anyone who's willing to help could use as little math terminology as possible, I would really appreciate that. In my study of Quantum Mechanics, Hilbert space is defined as a collection of functions in a interval $I$ (bounded or maybe unbounded) which are square integrable on $I$, together with a definition of inner product. With out much proof, our professor claim that any functions in Hilbert space could be expanded as a linear combination of a set of complete orthogonal functions $\{u_n\}$ (since that's exactly how completeness is defined), with coefficient $a_n=\langle u_n \mid f\rangle$. My question is, if we are to describe the function $f$ in terms of the set of coefficients $(a_n)$, then it seems that the information we need is countable (forgive me for I couldn't give a proper definition of ""information""); while if we try to describe it in our original way, on $x$-axis, the information we need seems turn into uncountably infinite. So how to explain such a contradiction? Or maybe it's just that the ""information"" of later is also countable, but why?","['hilbert-spaces', 'functional-analysis']"
2911240,Sum formulas for Pontrjagin square and Postnikov square,"Inspire by this , I wonder Pontrjagin square : There is a geometric interpretation of $\mathfrak{P}_2$, due to Morita. Assume $q=2k$, so that the Pontrjagin square is a map 
$$\mathfrak{P}_2 \colon H^{2k}(X, \mathbb{Z}_2) \longrightarrow \mathbb{Z}_4.$$ Set $$Z \equiv \sum_{t \in H^{2k}(X, \mathbb{Z}_2)} e^{2 \pi i \mathfrak{P}_2(t)}.$$ Then $$\textrm{Arg}(Z)= \textrm{Arg}( \sum_{t \in H^{2k}(X, \mathbb{Z}_2)} e^{2 \pi i \mathfrak{P}_2(t)})= \frac{\sigma(M)}{8} \in \mathbb{Q}/\mathbb{Z}, \tag{(a)}$$ where $\sigma(X)$ denotes the signature of $M$. For a proof, see Gauss Sums in Algebra and Topology , by Laurence R. Taylor. Postnikov square : Question: Are there similar statements for Postnikov square? (like eqn.(a)) Postnikov square is a certain cohomology operation from a first cohomology group $H^1$ to a third cohomology group $H^3$, introduced by Postnikov (1949). Eilenberg (1952) described a generalization taking classes in $H^t$ to $H^{2t+1}$. Refs: Postnikov, M. M. (1949), ""The classification of continuous mappings of a three-dimensional polyhedron into a simply connected polyhedron of arbitrary dimension"", Doklady Akademii Nauk SSSR (N.S.) (in Russian), 64: 461–462 Eilenberg, Samuel (1952), ""Homotopy groups and algebraic homology theories"", Proceedings of the International Congress of Mathematicians, Cambridge, Massachusetts, 1950, vol. 2, Providence, R.I.: Amer. Math. Soc., pp. 350–353, MR 0045388","['algebraic-topology', 'geometric-topology', 'differential-topology', 'characteristic-classes', 'differential-geometry']"
2911253,Show that $\frac{3^n}{n!}$ converges to $0$,"I was wondering if this proof is correct. $$\left|\frac{3^n}{n!} - 0\right| = \frac{3^n}{n!} \lt \frac{3^n}{2^n} \lt \varepsilon$$ So then
$$\frac{2^n}{3^n} \gt \frac{1}{\varepsilon}$$
$$\left(\frac{2}{3}\right)^n \gt \frac{1}{\varepsilon}$$
$$\log\left(\frac{2^n}{3^n}\right) \gt \log\left(\frac{1}{\varepsilon}\right)$$
$$n\log\left(\frac{2}{3}\right) \gt \log\left(\frac{1}{\varepsilon}\right)$$
$$n \gt \log\left(\frac{1}{\varepsilon}\right)/\log\left(\frac23\right)$$ So then any $N \gt \log(\frac1\varepsilon)\log(\frac23)$ yields the result we want, so that for all $n \gt N, \left|\frac{3^n}{n!} - 0\right| \lt ε$ Thanks in advance!","['proof-verification', 'real-analysis']"
2911315,Number of solutions for a given logical equation,"I came across the following question while studying logic and cannot find a solution for it anywhere. I am studying by myself and think I just don't know exactly the right terms to search for it online (I'm not sure it is called a logical equation so excuse the title of this question in case it isn't): Given the proposition $P$, it's logical value is defined as $[P] = 0$, in case $P$ is false, and $[P] = 1$, in case $P$ is true. Consider the following open sentences defined in the set of integers: $ P_i(x): x \le 5$ $ P_{ii}(x): x \ge 3$ $ P_{iii}(x): $ x is odd $ P_{iv}(x): x \ge 6$ How many solutions does the following equation have? $ x = [P_i(x)] + 2 \cdot[P_{ii}(x)]+3\cdot[P_{iii}(x)]+4\cdot[P_{iv}(x)]$ I've made this jsfiddle and from there, I can count the number of solutions through a loop. In this case I've looped from 0 to 1000 and it yields 2 solutions. Though I can clearly reason it wouldn't be possible for a very large number to work here, since these are all sums of multiplications of 0s or 1s, I am having a hard time articulating exactly why. How would you go about finding the largest number possible, in this very specific case? So you wouldn't have to loop through values of X too far off from it?","['elementary-number-theory', 'logic', 'combinatorics', 'problem-solving']"
2911318,Presentation for a Finite Etale Cover of an (Affine) Elliptic Curve,"I will write the question first, then try and explain myself more clearly after: Question: How can one find a presentation for a finite etale cover of an affine piece of an elliptic curve? If $E/\mathbb{C}$ is an elliptic curve, then it is homeomorphic to a torus. For this reason we know $\pi_{1}(E) \cong \mathbb{Z}\times \mathbb{Z}$. If we puncture $E/\mathbb{C}$, then we obtain an affine curve, let us denote it $X$. Moreover, $X$ can be thought of as the elliptic curve minus the point at infinity. Since this is affine, we can give a presentation for it, say, 
$$X:= \text{Spec}(\mathbb{C}[x,y]/\langle y^{2} - f(x) \rangle)$$
for $f(x) = x(x-1)(x-2)$. We know $\pi_{1}^{et}(X) \cong \hat{\pi_{1}(X(\mathbb{C})^{an})}$ and $X(\mathbb{C})^{an}$ is homeomorphic to a punctured torus. The fundamental group of a punctured torus is the free group on two generators $F_{2}$. The profinite completion of which is non-trivial. So, there are algebraic covers of such an affine curve. How can we get our hands on them? Is there a presentation which in some sense is in terms of $f(x)$? Does anyone have a reference for a discussion on such a construction? Thanks in advance :)","['algebraic-geometry', 'reference-request']"
2911319,Show $\mathbb{P}(S_n \geq a+b) \leq \mathbb{P}(S_n \geq a) \mathbb{P}(S_n \geq b)$ for sum of independent Bernoulli random variables,"Let $X_1,...,X_n$ be random variables independent and identically distributed, where $X_i \sim \textrm{Bernoulli}(p)$ . Define $$\displaystyle S_n = \sum_{i=1}^n X_i$$ Show that $P(S_{n} \geq a+b) \leq P(S_n \geq a)P(S_n \geq b)$ , where $a + b \leq n.$ Since $X_1,\cdots,X_n$ be random variables i.i.d. and $X_i \sim \textrm{Bernoulli}(p)$ , we have that $S_n \sim \textrm{Binomial}(n,p)$ . Thus, $$ P(S_n \geq a)P(S_n \geq b) =  \sum_{i=a}^n \dbinom{n}{i}p^i(1-p)^{n-i}\sum_{j=b}^n \binom{n}{j}p^j(1-p)^{n-j}$$ $$\begin{align*} P(S_n \geq a)P(S_n \geq b) &=  \sum_{i=a}^n \sum_{j=b}^n \binom{n}{i}p^i(1-p)^{n-i} \binom{n}{j}p^j(1-p)^{n-j}\\ &= \sum_{i=a}^n \sum_{j=b}^n \binom{n}{i}\binom{n}{j}p^{i+j}(1-p)^{2n-i-j} \end{align*}$$ Any hint to continue?","['probability-distributions', 'binomial-distribution', 'inequality', 'probability-theory', 'probability']"
2911357,"Is a positive, monotone and sub-additive function concave?","Consider a function $f : [0, 1] \to \mathbb{R}^+$ such that $f(0) = 0$ and $f(x) \leq f(y)$ for all $ x \leq y$ (i.e $f$ is monotone). Additionally, I also restrict $f$ to be a sub-additive function i.e $f(x+y) \leq f(x) + f(y)$. I am trying to prove that such a function need not be concave. I believe that such a function does exist but cannot find a counter example. Is there a function that satisfies my requirements? Ideally, I would like to find a function which is also continuous and differentiable but any function not satisfying these conditions would also be fine. I believe (no formal reason) that continuous and differentiable does force the function to become concave.","['continuity', 'functions', 'convex-analysis']"
2911358,Maps on compact riemannian manifolds that differ by a small amount are homotopic?,"Consider a compact Riemannian manifold $(M,g)$. Does there exist a small enough $\varepsilon$ such that for any maps $f$ and $g$ from $M$ to $M$, if $\sup_{x \in M} d(f(x), g(x)) < \varepsilon$, then $f$ and $g$ must necessarily be homotopic to each other. In other words, given a compact Riemannian manifold $M$, is there a distance small enough such that functions from $M$ to $M$ that are within that small distance are actually the same in homotopy? If anyone has any idea, or possibly a reference where this sort of question might have a (partial) answer, I'd be quite grateful.","['homotopy-theory', 'riemannian-geometry', 'differential-geometry']"
2911361,Is this excercise doable without a computer? Triangular Number with n divisors,"The sequence of triangle numbers is generated by adding the natural
  numbers. So the 7th triangle number would be 1 + 2 + 3 + 4 + 5 + 6 + 7
  = 28. The first ten terms would be: 1, 3, 6, 10, 15, 21, 28, 36, 45, 55, ... Let us list the factors of the first seven triangle numbers: 1: 1
 3: 1,3
 6: 1,2,3,6
10: 1,2,5,10
15: 1,3,5,15
21: 1,3,7,21
28: 1,2,4,7,14,28 We can see that 28 is the first triangle number to have over five
  divisors. What is the value of the first triangle number to have over five
  hundred divisors? from https://projecteuler.net/problem=12 $n$th triangular number=$\frac{n(n+1)}{2}$ (little Gauss) Algorithm to find number of divisors: from https://www.geeksforgeeks.org/total-number-divisors-given-number/ # Python3 program for finding  
# number of divisor 

# program for finding  
# no. of divisors 
def divCount(n): 

    # sieve method for 
    # prime calculation 
    hh = [1] * (n + 1); 

    p=2; 
    while((p * p) < n): 
        if (hh[p] == 1): 
            for i in range((p * 2),n,p): 
                hh[i] = 0; 
        p+=1; 

    # Traversing through  
    # all prime numbers 
    total = 1; 
    for p in range(2,n+1): 
        if (hh[p] == 1): 

            # calculate number of divisor 
            # with formula total div =  
            # (p1+1) * (p2+1) *.....* (pn+1) 
            # where n = (a1^p1)*(a2^p2)....  
            # *(an^pn) ai being prime divisor 
            # for n and pi are their respective  
            # power in factorization 
            count = 0; 
            if (n % p == 0): 
                while (n % p == 0): 
                    n = int(n / p); 
                    count+=1; 
                total *=(count + 1); 

    return total; 


# Driver Code 
n = 24; 
print(divCount(n)); 

# This code is contributed by mits Is the task doable without a computer, in reasonable time? If so, what's the ""magic"" trick?","['number-theory', 'algorithms']"
2911415,Show $\int_0^{2\pi} \exp\left(\frac{7+5 \cos x}{10+6\cos x}\right) \cos \left( \frac{\sin x}{10+6 \cos x} \right) dx = 2\pi e^{2/3}$ Real Methods,"Taken from the post: The Integral that Stumped Feynman? I want to know if the integral: $$\int_0^{2\pi} \exp\left(\frac{7+5 \cos x}{10+6\cos x}\right) \cos \left( \frac{\sin x}{10+6 \cos x} \right) dx = 2\pi e^{2/3}$$ can be evaluated using strictly real methods. I've tried series of $e^x$ and $\cos x$ but to no avail. I tried differentiating under the integral, but nothing seemed to come out of it. Is there any wizardry that can conjure up this answer without complex analysis.","['integration', 'calculus']"
2911467,How can I solve $y'+\sin y'=x$?,I have no idea how to start with this. I tried taking the derivative and making use of $\cos{y'}^2+\sin{y'}^2=1$ but it seems to get even more confusing. Of course we can replace $y'$ with $f$ since $y$ does not appear but that's as far as I can get.,['ordinary-differential-equations']
2911500,Ratio between farthest and second farthest distance,"$n\geq 3$ points lie in three-dimensional space. What is the largest $c(n)$ such that there always exists a point for which the ratio between the distance to the farthest point from it and the distance to the second farthest point from it is at most $c(n)$? In the case where $n=3$ and all three points are on a line, we can suppose that the distance between the first and second point is $a$, and between the second and third point is $b$, where $a\leq b$. The ratios from each of the three points are $\frac{a+b}{a},\frac{b}{a},\frac{a+b}{b}$. Among these ratios, $\frac{a+b}{a}$ is the highest, so we may ignore it. The remaining two ratios are equalized when $\frac{b}{a}=\frac{\sqrt{5}+1}{2}$.","['golden-ratio', 'combinatorial-geometry', 'geometry']"
2911517,An equality with factorials [duplicate],"This question already has answers here : How to simplify the summation kk! without using induction? (6 answers) Closed 5 years ago . Prove that
$$2\times 2! + 3\times 3!+4\times 4! +....+n\times n!=(n+1)!-2$$
I know that it can be proved by mathematical induction, but I want to prove it without using the mathematical induction.
I tied the equation $$C^n_0 + C^n_1 + C^n_2 +...+C^n_n=2^n$$
But I did not get any thing useful.",['combinatorics']
2911524,How do you calculate this integral $\int_{0}^{2\pi}\ {e^{\sin t}}dt $,Evaluate the  Integral: $$\int_{0}^{2\pi}\ {e^{\sin t}}\ dt $$ I've found results for this which involve an infinite series which becomes increasingly complicated and other series that involve more integrals. Ideally the solution should be a well defined constant.,"['integration', 'calculus', 'definite-integrals']"
2911613,"Why are these called ""meromorphic"" differential forms?","On p.30 of Silverman's The Arithmetic of Elliptic curves, the notion of a meromorphic differential form on a curve $C$ is defined as follows: Definition. Let $C$ be a curve. The space of (meromorphic) differential forms on $C$, denoted by $\Omega_C$ , is the $\overline{K}(C)$-vector space generated by symbols of the form $dx$ for $x \in \overline{K}(C)$, subject to the usual relations: (i) $d(x + y) = dx + dy$ for all $x, y \in \overline{K}(C)$. (ii) $d(xy) = x dy + y dx$ for all $x, y ∈ \overline{K}(C)$. (iii) $da = 0$ for all $a \in \overline{K}$ However as far as I can tell, meromorphic functions in their full generality don't really appear as coefficients here. For example, we're probably not going to have $e^x dx$ in there. If so, then why is this terminology used? Shouldn't they be called rational differential forms?","['complex-analysis', 'algebraic-geometry', 'algebraic-curves', 'meromorphic-functions']"
2911618,Proof that every codimension 1 subvariety of $\mathbb{P}^n$ is $V((f))$,"I've been told that Theorem. Every codimension 1 subvariety of $\mathbb{P}^n$ is $V((f))$, where $f$ is some prime homogeneous polynomial. I'm under the impression that this is true over any algebraically closed field. However, I'm unable to locate a proof. Does anyone know where a proof of this theorem can be located? A version of this for affine space is here , but a bit light on the details. Also, in this context, does $V((f))$ mean $V$ of the principal ideal generated by $f$, or is the double bracketing indicative of some kind of a power series construction?","['algebraic-geometry', 'projective-space']"
2911638,Is $p_1p_2...p_k - 1$ ever a perfect power?,"Is $p_1p_2...p_k - 1$ ever a perfect power, where $p_1 < p_2 < ... < p_k$ are the smallest primes? The case for squares is simple, but I am struggling to generalise to all powers.","['number-theory', 'prime-numbers']"
2911663,"A certain step in a proof of König's theorem from Hrbacek-Jech ""Introduction to Set Theory""","I'm trying to understand the proof of König's theorem in set theory using Hrbacek's and Jech's book ""Introduction to Set Theory"". In the end, the proof сomes down to a certain result or lemma, which I'm having a trouble with understanding. Let $(Y_i)_{i \in I}$ be indexed family. Then there can be no indexed family $(Z_i)_{i \in I}$ so that $(\forall i,j \in I)(i \neq j \Longrightarrow Z_i\cap Z_j = \varnothing),$ $(\forall i \in I)(|Z_i| < |Y_i|),$ $\bigcup_{i \in I} Z_i = \prod_{i \in I} Y_i$ . Hrbacek and Jech proceed the following way. They defined a new family $(A_i)_{i \in I}$ so that $A_i$ is the subset of $Y_i$ of all elements $y$ for which there is a function $f\colon I\to\bigcup_{i \in I} Y_i$ in $Z_i$ so that $y = f(i)$ . Then they claim that for any $i \in I$ , $A_i \subsetneq Y_i$ since $|A_i| \leq |Z_i| < |Y_i|$ . This is where is stumble: I don't understand why we have $|A_i| \leq |Z_i|$ for each $i \in I$ .",['elementary-set-theory']
2911674,Finding $\max |A|$ with $a_{ij}=\pm 1$,"$A$ is a matrix sized $n\times n$, all elements in $A$ are $\pm1$. Find $\max |A|$. My Attempt Denote $f(n)=\max |A_{n\times n}|$. $f(1)=1$. $f(2)=2$ is also obviously. If $n\ge2$, $|A|$ must be even. For $n=3$, $f(3)\ge4$ because $\left|\begin{array} r1&1&1\\1&-1&1\\1&-1&-1
\end{array}\right|=4$. Also, $|A|=A_{11} A_{22} A_{33}+A_{12} A_{23} A_{31}+A_{13} A_{21} A_{32}-A_{13} A_{22} A_{31}-A_{11} A_{23} A_{32}-A_{12} A_{21} A_{33}$ can not be $6$ since $A_{11} A_{22} A_{33}A_{12} A_{23} A_{31}A_{13} A_{21} A_{32}A_{13} A_{22} A_{31}A_{11} A_{23} A_{32}A_{12} A_{21} A_{33}$ must be $1$.
 $1\ne 1\cdot1\cdot1\cdot(-1)\cdot(-1)\cdot(-1)$. Hence we have $f(3)\ne 6$. $f(3)=4$. For $n\ge4$, I have no idea where to start with $f(n)$. A trivial bound is $0<f(n)\le n!$. EDIT Related question: Maximum value of Determinant of $3 \times 3$ Matrix with entries $\pm 1$ It is not duplicated since I am discussing $n\times n$ determinants, not $3\times3$.","['determinant', 'linear-algebra']"
2911688,Finding value of $\lim_{n\rightarrow \infty}\frac{1+\frac{1}{2}+\frac{1}{3}+ \cdots +\frac{1}{n^3}}{\ln(n)}$,"Find the value of $$\lim_{n\rightarrow \infty}\frac{1+\frac{1}{2}+\frac{1}{3}+\cdots  +\frac{1}{n^3}}{\ln(n)}$$ My Try: Using Stolz-Cesaro, Let $\displaystyle a_{n} = 1+\frac{1}{2}+\frac{1}{3}+\cdots \cdots +\frac{1}{n}$ and $b_{n} = \ln(n)$ So $\displaystyle \frac{a_{n+1}-a_{n}}{b_{n+1}-b_{n}} = \lim_{n\rightarrow \infty}\frac{1}{{(n+1)^3}}\cdot \frac{1}{\ln\bigg(1+\frac{1}{n}\bigg)} = 0$ Please explain if what I have done above is right.","['limits', 'sequences-and-series', 'real-analysis']"
2911706,How to 'randomize' a given discrete probability distibution?,"Given a discrete probability distribution, $\boldsymbol{\lambda} = (\lambda_1,\lambda_2, ..., \lambda_n)$, how can we construct a random distribution,
$\boldsymbol{\Lambda} = (\Lambda_1, \Lambda_2, ..., \Lambda_n)$ such that $\mathbb{E}(\boldsymbol{\Lambda}) = \boldsymbol{\lambda}$? The goal is to generate random variations of the initial distribution, $\boldsymbol{\lambda}$, with the above property. If it is possible to do this and have some degree of control over the variance/spread of each $\lambda_i$, then even better! I suspect that there are many ways to achieve this, so any approaches/answers are appreciated. What I've tried: My initial thought was to let each $\Lambda_i \sim Beta(\alpha_i, \beta_i)$, with $\alpha_i$, $\beta_i$ chosen so that $\mathbb{E}(\Lambda_i) = \lambda_i$. By choosing larger $\alpha_i$, and $\beta_i$ values, we can also make the distributions tighter around the mean. However, this has the issue that, in general, $$
\sum_{i=1}^n \Lambda_i \ne 1
$$ We may work around this by normalizing $\boldsymbol{\Lambda}$ after evaluating each $\Lambda_i$ (by dividing through by sum). Unfortunately I think I am right in saying that, in general, this means that $\mathbb{E}(\boldsymbol{\Lambda}) = \boldsymbol{\lambda}$ no longer holds. Similarly, we could 'normalize' $\boldsymbol{\Lambda}$ by setting $$
\Lambda_n = 1 - \sum_{i=1}^{n-1} \Lambda_i
$$ This preserves $\mathbb{E}(\boldsymbol{\Lambda}) = \boldsymbol{\lambda}$ (at the expense of the distribution of $\Lambda_n$ now being difficult to determine exactly, though we can approximate it ). The problem with this method is that $\Lambda_n$ could now be negative!","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
2911717,Can the adjugate matrix of $A$ be expressed as a polynomial of $A$?,"We know that if $A$ is invertible, then $A^{-1}$ can be expressed as a polynomial of $A$ , that is to say, there exists a polynomial $f(x)$ such that $$
A^{-1} = f(A)
$$ Of course in this case, $\operatorname{adj}(A)$ (the adjugate matrix of $A$ ) can also be expressed as a polynomial of $A$ . I wonder if $A$ is not invertible, can $\operatorname{adj}(A)$ be expressed as a polynomial of $A$ ？ Hope for your comments.","['matrices', 'matrix-equations', 'linear-algebra']"
2911757,"Sums of powers of elements in a subset and polynomial with $\{-1,0,1\}$ coefficients","Recently, I came across the following problem: It is known that $k$ and $n$ are positive integers, and that $k+1 \le  \sqrt{\frac{n+1}{\ln(n+1)}}.$ Prove that we can find a polynomial $P$ of degree $n$, whose coefficients belong to $\{-1,0,1\},$ such that $(x-1)^k$ divides $P$ ( Some background: This was proposed to me by a colleague, who stated it was from an Ukrainian IMO team selection test. I have taken a look around, but never succeeded in the task of finding an actual solution. Also, the colleague who proposed it to me did not know himself how to solve it either.) At first, I thought a simple construction would suffice: namely, one relatively 'naive' idea is to consider products of the form $$ (x^{\alpha_1} - 1)(x^{\alpha_2} -1) \cdots (x^{\alpha_k} -1),$$ where $\alpha_1 + \alpha_2 + \cdots + \alpha_k = n.$ Indeed, this works and yields a non-trivial result through an algorithmic method, but only when $k \lesssim \ln(n).$ One can improve $\ln(n)$ slightly in the construction - which is not at all sophisticated -, but not so that one can reach the $n^{1/2}/\ln(n)^{1/2}$ type of upper bound, which is asked. Therefore, I guess the 'hint' here is that we should not be looking for a 'constructive' proof, but rather for an 'existence' one. Therefore, with that in mind, we write our desired polynomial as $$P(x) = \sum_{a \in A} x^a - \sum_{b \in B} x^b.$$ Then, $(x-1)^k$ dividing $P$ means that $P(1) = P'(1) = \cdots = P^{(k-1)}(1) = 0.$ But this implies, after some manipulations, that $$ \sum_{a \in A} 1 = \sum_{b \in B} 1, $$
$$ \sum_{a \in A} a = \sum_{b \in B} b, $$
$$ \sum_{a \in A} a^2 = \sum_{b \in B} b^2,$$ 
$$ \vdots $$ 
$$ \sum_{a \in A} a^{k-1} = \sum_{b \in B} b^{k-1}.$$ Let then $s_k(A) = \sum_{a \in A} a^k,$ where $A \subset \{0,1,...,n\}.$ The problem can therefore be restated as follows: There are two subsets $A,B \subset \{0,1,...,n\}$ such that $|A| = |B|$ and $s_j(A) = s_j(B), \,j=1,...,k-1.$ Notice that $A$ and $B$ need not necessarily be disjoint, as any possible overlap is 'cancelled' in the definition of $P.$ In order to solve this problem, I have been trying to use continuity arguments, like 'keep the sum unchanged by changing two elements, and the difference between the squares decreases' or something of this kind. This might indeed be helpful, but I cannot see how to take it on further than that, that is, how to use this construction for $k > 2.$","['contest-math', 'combinatorics', 'polynomials']"
2911764,Lipschitz and Uniqueness of an IVP,"Consider the IVP $$\frac{dx}{dt}=1+x^2, \ \ \ x(0)=1.$$ I am trying to show that $f(x)=1+x^2$ is Lipschitz and hence the above IVP has a unique solution. My attempt: I considered $f(x)=1+x^2$. Now if $f$ is Lipschitz, then $\exists L\in\mathbb{R}, \ \text{such that} \ \forall x,y\in\mathbb{R}$, $$|f(x)-f(y)|\leq L|x-y|.$$
Now, \begin{align}
L&\geq \frac{|1+x^2-1-y^2|}{|x-y|} \\
&=\frac{|(x-y)(x+y)|}{|x-y|} \\
&=|x+y|
\end{align} I'm a bit unsure of how to proceed. How can I use the initial value conditions to further my answer?","['proof-verification', 'lipschitz-functions', 'ordinary-differential-equations']"
2911787,"Is the word ""empty"" in set theory different from the word ""empty"" in ordinary language? [duplicate]","This question already has answers here : How is an empty set truly ""empty""? (6 answers) Closed 5 years ago . I skimmed over this question Is the empty set a subset of itself? , and I'm currently under the impression that it's a widespread belief that the empty set contains itself. But a contradiction seems to arise: if the empty set contains itself, then it's NOT empty. After all, if we call a set non-empty just because it contains the empty set, then why should we treat the empty set itself differently? Then it occurred to me that maybe mathematicians define ""empty"" differently in set theory. Maybe by ""empty set"" they mean a ""set that contains only itself"", instead of a ""set that contains absolutely nothing"". Is my surmise correct?",['elementary-set-theory']
2911863,Substituting $y^2$ in a differential equation,"I have the IVP: $$2yy'+5 = y^2 + 5x; y(0)=6.$$ In order to solve this, I have attempted to make the substitution $u = y^2 \implies \sqrt{u} = y$. However, I am not sure how to solve for $u'$, so I don't know how to proceed in the problem. I know that $\frac{du}{dx} = \frac{d(y^2)}{dx}$ but I'm not really sure what that means or how to deal with it.","['derivatives', 'ordinary-differential-equations']"
2911883,Finding domain of $ f(x) ^ {g(x)} $?,"My coaching module says ""for domain of  $ k(x)= f(x) ^ {g(x)} $ conventionally the conditions are $f(x)>0$ and $g(x)$ must be real."" I get why $g(x)$ must be real, but why must $f(x)>0$ ? If for instance $f(x) = -1$,why wouldn't $k(x)$ be real? Please help me in understanding this.",['functions']
2911898,Showing continuity of $xy$ at all points,"My friend asked me to show continuity of $f(x,y) = xy$ at all points in $\Bbb R^2$. I started We need to show $|xy-ab| \lt \epsilon$ whenever $d((a,b), (x,y)) < \delta$.
So we have $|x-a| < \delta$ and $|y-b| < \delta$ then
$$|xy-ab | = |xy-ab-bx+bx-ay+ay-xy+xy-ab| \\
= |x(y-b) + y(x-a) -(x-a)(y-b)| \\
 < |x(y-b)| + |y(x-a)|+|(x-a)(y-b)|$$ Now we have $|x-a| < \delta$ or $a-\delta<x<a+\delta$ or $|x| < \max (|a+\delta|, |a-\delta|)$ and similarly $|y| < \max(|b-\delta|, |b+\delta|)$ So we obtain: $$|xy-ab| < \delta (\max (|a+\delta|, |a-\delta))+\max(|b-\delta|, |b+\delta|)) \delta ^2 < \epsilon$$ Now how to conclude from here?? Do we take four cases? Thanks a lot! Edit As Holo said, i mistakenly didnt write modulus $|a-\delta|$, now I have edited.","['limits', 'multivariable-calculus', 'epsilon-delta']"
2911903,"Solution verification post : Problem $12.1$, Mathematical Analysis, Apostol","As a continuation of this post , I'm starting to post my solutions (or attempts) to the exercise problems of Chapter 12 (Multivariable Differential Calculus), Mathematical Analysis by Apostol (suggested by Masacroso ). Since I'm essentially self-studying, I'd really appreciate if anyone checks the solutions and let me know if there is any gap in my arguments or if there exists any cleverer solutions. Thank you. Problem $12.1.$ Let $S$ be an open subset of $\mathbb{R}^n$, and let $f:S \to \mathbb{R}$ be a real-valued function with finite partial derivative $D_1f,\dots,D_nf$ on $S$. If $f$ has a local maximum or a local minimum at a point $c$ in $S$, prove that $D_kf\left(c\right)=0 \,\,\forall \,k$. Solution. Suppose that $f$ has a local maximum at $c \in S$. This implies that $\exists \delta>0$ such that
$$f(c) \geq f(x) \text{ for all } x \in B_{\delta}(c)=\left\{y \in S : \left\Vert y-c \right\Vert < \delta \right\} \tag1$$
Let $k \in \{1,\dots,n\}$. Now,
$$D_kf(c)=\lim_{h \to 0} \frac{f(c+he_k)-f(c)}{h} \tag2$$
It is given that $(2)$ exists finitely, then
$$D_kf(c)=\lim_{h \to 0;\,h \in (0,\delta)} \frac{f(c+he_k)-f(c)}{h} \leq 0 \tag3$$
On the other hand,
$$D_kf(c)=\lim_{h \to 0;\,h \in (-\delta,0)} \frac{f(c+he_k)-f(c)}{h} \geq 0 \tag4$$
$(3)$ and $(4)$ $\Rightarrow$ $D_kf(c)=0$. This holds for every $k \in \{1,\dots,n\}$ and hence we are done.","['proof-verification', 'analysis', 'real-analysis', 'multivariable-calculus', 'derivatives']"
2911908,"Prob. 5, Sec. 26, in Munkres' TOPOLOGY, 2nd ed: Any pair of disjoint compact subspaces of a Hausdorff is separated by disjoint open sets","Here is Prob. 5, Sec. 26, in the book Topology by James R. Munkres, 2nd edition: Let $A$ and $B$ be disjoint compact subspaces of the Hausdorff space $X$. Show that there exist disjoint open sets $U$ and $V$ containing $A$ and $B$, respectively. First of all, here is Lemma 26.1: Let $Y$ be a subspace of (a topological space) X. Then $Y$ is compact (relative to the subspace topology that $Y$ inherits from $X$) if and only if every covering of $Y$ by sets open in $X$ contains a finite subcollection covering $Y$. And, here is Lemma 26.4: If $Y$ is a compact subspace of the Hausdorff space $X$ and $x_0$ is not in $Y$, then there exist disjoint open sets $U$ and $V$ of $X$ containing $x_0$ and $Y$, respectively. I think I am clear on the proof of either of Lemmas 26.1 and 26.4. So I'll be using these in my proof of Prob. 5, Sec. 26, which is as follows: Since $A$ and $B$ are disjoint, therefore for each point $a \in A$, there exist disjoint open sets $U_a$ and $V_a$ of $X$ containing $a$ and $B$, respectively, by Lemma 26.4. As the collection $$ \left\{ \ U_a \colon \ a \in A \ \right\}$$ is a covering of $A$ by sets open in $X$, so by Lemma 26.1 there is a finite subcollection of this collection that also covers $A$; let $U_{a_1}, \ldots, U_{a_n}$ be this finite subcollection. Now let us put 
  $$ U \colon= \bigcup_{i=1}^n U_{a_i} \qquad \mbox{ and } \qquad V \colon= \bigcap_{i=1}^n V_{a_i}. \tag{1} $$
  Here $V_{a_1}, \ldots, V_{a_n}$ are the open sets that correspond to the sets $U_{a_1}, \ldots, U_{a_n}$, respectively, as in the first paragraph of this proof. Then both the sets $U$ and $V$ as defined in (1) here are open sets of $X$; moreover the set $U$ contains $A$ by our choice of the sets that $U$ is composed of. As each set $V_a$ in the first paragraph contains $B$, so does each of the sets $V_{a_i}$ in (1) above; therefore $B$ is contained in $V$. Finally, if $u \in U$, then $u \in U_{a_k}$ for some $k = 1, \ldots, n$, and thus this point $u$ would not be in the corresponding set $V_{a_k}$, and hence $u$ would not be in the set $V$ in (1) above. Conversely, if $v \in V$, then $v$ is in each of the sets $V_{a_1}, \ldots, V_{a_n}$, and therefore $v$ is in none of the sets $U_{a_1}, \ldots, U_{a_n}$, which implies that $v$ is not in $U$. Thus the sets $U$ and $V$ are disjoint. Is this proof correct? If so, then is it clear in each and every one of its steps? If not, then where are the issues?","['general-topology', 'proof-verification', 'compactness']"
2911921,"How does one evaluate $I(\alpha,\beta,\gamma)= \int_{0}^{\infty} \cos\left(\frac{x(x^2-\alpha^2)}{x^2-\beta^2}\right)\frac{1}{x^2+\gamma^2} \, dx.$","I have been trying to evaluate this integral for a while now,
$$I(\alpha,\beta,\gamma)= \int_{0}^{\infty} \cos\left(\frac{x(x^2-\alpha^2)}{x^2-\beta^2}\right)\frac{1}{x^2+\gamma^2} \, dx.$$
I first came across this integral on Quora ( https://www.quora.com/What-improper-integrals-are-hard-to-solve ). Someone stated that this integral was submitted as a problem to the Gazette of the Royal Mathematics Society of Spain and was (at the time) still open, so the complete solution would not be published there. (have not been able to find the publication). Although no derivation, a closed form expression was given:
$$I(\alpha,\beta,\gamma)=\frac{\pi}{2\gamma}e^{-\frac{\gamma(\alpha^2+\gamma^2)}{\beta^2+\gamma^2}}.$$
One might be wondering, why not just respond to the post on Quora? That I did. However, the thread seems to inactive. I have tried several methods including the one employed by Mark Viola in this post ( Computing $\int_{-\infty}^{\infty} \frac{\cos x}{x^{2} + a^{2}}dx$ using residue calculus ). Trying to work backwards from the closed form expression, by treating the term $ \frac{\gamma(\alpha^2+\gamma^2)}{\beta^2+\gamma^2}$ in exponent as one variable. However, I have not been able to succeed. Hopefully, you can help me solve this integral/ give suggestions/ different methods. edit: I found that when $\alpha=\beta$, the integral becomes
$I= \int_{0}^{\infty} \cos(x)\frac{1}{x^2+\gamma^2}dx$ , which evaluates to $I=\frac{\pi}{2\gamma}e^{-\gamma}$. This is comparable to the desired result. The argument in the exponent is different however.","['integration', 'complex-analysis', 'real-analysis']"
2911922,A Problem About Arithmetic And Geometric Sequences,"The problem I'm trying to solve is this: The first three terms of a geometric sequence are also the first, eleventh and sixteenth terms of an arithmetic sequence. The terms of the geometric sequence are all different. The sum to infinity of the geometric sequence is 18. Find the common ration of the geometric sequence, and the common difference of the arithmetic sequence. What I've done so far is to write the following equations, where u is the first term, r is the common ratio, and d is the difference: $ur=u+10d$ $ur^2=u+15d$ $u/(1-r)=18$ But I don't know what to do from there. Any suggestions?","['algebra-precalculus', 'sequences-and-series']"
2911942,Dualizing sheaf for local complete intersection,"I am studying section III.7 (The Serre Duality Theorem) of Hartshorne's Algebraic Geometry and have some issues with the proof of Theorem 7.11. Let $X$ be a closed subscheme of $P = \mathbb{P}^N_k$ which is a local complete intersection of codimension $r$. Let $\mathscr{I}$ be the ideal sheaf of $X$. Then $\omega_X^{\circ} \cong \omega_P \otimes \bigwedge^r (\mathscr{I}/\mathscr{I}^2)^\vee$. In particular, $\omega_X^{\circ}$ is an invertible sheaf on $X$. Say $j \colon X \hookrightarrow P$ is a closed immersion, defining the ideal sheaf 
    \begin{align*}
 			\mathscr{I} = \ker(j^{\flat} \colon \mathscr{O}_P \twoheadrightarrow j_* \mathscr{O}_X).
 	\end{align*}
    I guess what we want to show is $j_* \omega^{\circ}_X \cong \omega_P \otimes \bigwedge^r (\mathscr{I}/\mathscr{I}^2)^\vee$. What I already know is that
        \begin{align*}
			\omega^{\circ}_X \cong j^* \mathscr{E}xt^r_P(j_* \mathscr{O}_X, \omega_P),	
		\end{align*}
    so that
        \begin{align*}
				j_*\omega^{\circ}_X \cong j_*j^* \mathscr{E}xt^r_P(j_* \mathscr{O}_X, \omega_P) \cong \mathscr{E}xt^r_P(j_* \mathscr{O}_X , \omega_P).
		\end{align*}
Hence we have to compute $\mathscr{E}xt_P^r(j_* \mathscr{O}_X , \omega_P)$. Now this is where Hartshorne's proof (and with it my problems) begins. Since I don't understand the way how Hartshorne gets the required isomorphism I just start writing down things I know (or think to know) - maybe, with someone's help, it turns out that this gives a rise to a way of proving the theorem in a way I understand. The aim is to cover $P$ by open affines $U_i \cong \text{Spec}(A_i)$, such that $\mathscr{I}_{|U_i}$ is generated by $r$ local sections $f_{i,1}, \dots , f_{i,r} \in \Gamma(U_i, \mathscr{O}_P)$ (this is possible since $X$ is a local complete intersection in $P$) and then to construct isomorphisms
        \begin{align*}
	 		\mathscr{E}xt_P(j_* \mathscr{O}_X, \omega_P)_{|U_i} \overset{\sim}{\longrightarrow} (\omega_P \otimes \bigwedge^r (\mathscr{I}/\mathscr{I}^2)^\vee)_{| U_i}, 	
	 	\end{align*}
    which after all can be glued together to give the required isomorphism. What I know about the left hand side: As $P$ is noetherian and both $j_* \mathscr{O}_X$ and $\omega_P$ are coherent we have
        \begin{align*}
			\mathscr{E}xt_P(j_* \mathscr{O}_X, \omega_P)_{|U_i} \cong \text{Ext}^r_{A_i}(M_i,N_i)^{\sim}
		\end{align*}
    for some finitely generated $A_i$-modules $M_i,N_i$, satisfying $(j_* \mathscr{O}_X)_{| U_i} \cong M_i^{\sim}$ and ${\omega_P}_{| U_i} \cong N_i^{\sim}$. (Can there something be said about the modules $M_i$ and $N_i$? As $j_* \mathscr{O}_X \cong \mathscr{O}_P / \mathscr{I}$ it feels like there could be something.) What I (think to) know about the right hand side: Using the fact that $\mathscr{I}/\mathscr{I}^2$ is locally free of finite rank we have
        \begin{align*}
			\omega_P \otimes \bigwedge^r (\mathscr{I}/\mathscr{I}^2)^{\vee} \cong \mathscr{H}om_{\mathscr{O}_P}(\bigwedge^r \mathscr{I}/\mathscr{I}^2 , \omega_P).	
		\end{align*}
Can we do something similar with this guy (i.e. writing the restriction of this as some tilde of a module)? Then it would look like we ""only"" had to give an isomorphism from some Ext to some Hom. (Well, and then think about glueing..) Thanks in advance for any comment.","['proof-explanation', 'algebraic-geometry', 'duality-theorems', 'sheaf-theory']"
2911956,"Differentiation of $(x,y)\mapsto g(x+h(x,y))$","Let $g:\mathbb{R}\to\mathbb{R}$ and $h:\mathbb{R}^2\to\mathbb{R}$ be two differentiable functions. I would like to compute the differential of $T:\mathbb{R}^2\to\mathbb{R}$ such that $T(x,y)=g(x+h(x,y))$. I get $D T(x,y) = Dg(x+h(x,y))\circ D(x_1(\cdot)+h(\cdot)) = g'(x+h(x,y))\times \big(1+\partial_1 h(x,y) \quad \partial_2 h(x,y)\big)$. Is it correct?",['derivatives']
2911959,Why is the genus of $y^2 = x^4 + 1$ not $3$ but $1$?,"I saw this , but I don't understand why we can't use the genus-degree formula for this curve.
I think this curve is $V(X^4 + Z^4 - Y^2Z^2)$ in $\mathbb{P}^2_k$, so by the genus degree formula, the genus is $3$. But the answers of this question say this curve is in actually a cubic curve in a projective space.
If so, this curve has the genus $1$. And one answer says this curve has two points at infinity.
But I think this has only $[0:1:0]$ for a point at infinity. What happen?","['algebraic-geometry', 'elliptic-curves']"
2911971,Covering the Euclidean plane with constructible lines and circles,"It is a well-known fact that the set of points which are finitely constructible with straightedge and compass (starting with two points $0$ and $1$) doesn't cover the Euclidean plane $\mathbb{R}\times \mathbb{R}$ because only $\mathbb{Q}^{\sqrt{}}\times \mathbb{Q}^{\sqrt{}}$ is finitely constructible (which is a countable set). [Side question 1: What is the official name (and symbol) for the set which I call $\mathbb{Q}^{\sqrt{}}$, i.e. the set of those numbers that can be defined by addition, substraction, multiplication, division and taking the square root alone (starting from $0$ and $1$). Note that the set of algebraic numbers $\mathbb{Q}^\text{alg}$ allows for taking arbitrary roots.] But in the process of constructing points with straightedge and compass a lot of other points are ""created"", just by drawing the allowed lines and circles that are needed to take intersections (allowed = defined by previously constructed points). Only those points count as constructed that are intersections of such constructed lines and circles with other constructed lines and circles. But the other ones at least have been drawn . My question is: Does it make sense to ask – and how can it be proved or disproved – whether
  $\mathbb{R}^2$ might be ""finitely coverable"" in the sense that for any
  given point $p \in \mathbb{R}^2$ there is a line or circle
  constructible in finitely many steps (starting from points $[0,0]$ and $[1,0]$) which $p$ lies on? The question and answer is not trivial at first glance (at least not for me), because the number of constructed points grows so incredibly fast, and the number of constructed lines and circles grows even faster (roughly quadratically, because each pair of new points gives – roughly – one new line and two new circles). [Side question 2: Can a rough estimate of the growth rate of the numbers of points, lines and circles be given, when starting with $n$ points in general or regular position?] To give a little visual sugar to my question: These are the constructible points, lines and circles after only three steps (starting with two points $0$ (red) and $1$ (orange)). (Each intersection of a line or circle with a line or circle is a constructed point – and there are myriads of them, after only three steps!) This is after two steps: This it how it looks like after only two steps when starting with five points $0, 1, -1, i, -i$. [Side question 3: What might the little (and internally structured) white cross in the middle (around $(0,0)$ (red)) ""mean""?] This is after one step: For the sake of completeness: This is where the two points $0$, $1$ started off: And this is where the five points $0$, $1$, $i$, $-1$, $-i$ started off:","['elementary-set-theory', 'euclidean-geometry']"
2911985,Calculating the boundary of a specific subset of real numbers,"We construct a sequence of open sets $J_i$ as follows. Let $J_0$ be the set $\cup_{n\in \mathbb Z} (2n,2n+1)$ . Having constructed $J_0,J_1,...,J_{m-1}$ let $J_m$ be the union of the open middle thirds of the segments constituting $\mathbb R - \cup_{i=0}^{m-1} J_i$ . ( I am not sure if the following is an algebraic formulation although as far as I remember I obtained it right: Consider the subsets { $J_m| m=0,1,2,...$ } of $\mathbb R$ defined by $$J_m:= \bigcup_{s=0}^{m-1} \bigcup_{n\in \mathbb Z} \left(\frac{3^m(2n+1)+2\times 3^{s}+1}{3^m},\frac{3^m(2n+1)+2\times 3^{s}+2}{3^m}\right),$$ where by definition $2\times 3^0:=0$ . If this algebraic formulation is either wrong or right but not useful, dismiss it.) 1 )  Is it true that the boundary of the union of any infinite subcollection of { $J_m$ } equals the same set (maybe the set of all endpoints of the intervals constituting $J_m$ ’s)? 2 )  If not, according to my book from which this question has arisen why the boundary of $\cup_k J_{3k}$ , the boundary of $\cup_k J_{3k+1}$ , and the boundary of $\cup_k J_{3k+2}$ are the same?","['real-numbers', 'general-topology', 'metric-spaces']"
2911989,Is there an easy proof that every closed curve is contained in a ball?,"I actually wanted to generalize this to higher dimensions (and prove something akin to the statement ""every $\bf{n}$ dimensional closed manifold is contained in an $\bf{n}$ dimensional ball), but I'm very new at this yet so I don't really have a clue. It seems obvious enough (I mean, take any closed planar curve and there exists a circle in which it is contained... the same goes for closed surfaces, I can always find a sphere big enough), but I see no clear path on proving it.","['manifolds', 'differential-geometry']"
2912000,What can be done about this far-out differential equation?,"I am searching for a function $u: \mathbb{R} \to \mathbb{R}$ satisfying the differential equation:
$$0=g(u') u' + g'(u') u'' u + c_1 u g(u') + c_2 u $$ Edit: we also need a starting condition: $ u(0) = c > 0 $ Where $g:\mathbb{R} \to \mathbb{R}$ is smooth and strictly monotone and $c_1,c_2 \in \mathbb{R}$ are some constants. I have a hunch that this might be a really tough question, even assuming $g$ to be linear (read: you're welcome to assume $g$ linear). Any solution/approximation technique would surprise me very much in a positive way :) As to how this equation popped up: There was a conversation about drinking coffee continuously, but at a speed changing with the temperature of the coffee. $u$ is the volume of coffee and $g$ is the inverse of a function describing the speed at which you a drink coffee at a certain temperature ($u' = g^{-1}(T)$). Using some physics: $E = c_3 u T$, $E' = c_4 A (T - T_s)$, where $A = 2 \pi r^2 + r^{-1} u$ is area of the surface of the coffee (a cylinder with height proportional to $u$) and $E$ is the thermic energy of remaining coffee, the formula above was obtained. To further detail: 
The height of the cylinder is proportional to the volume as
$u = h \pi r^2 \iff h = u/(\pi r^2)$ so
$ A = 2 \pi r^2 + h 2 \pi r = 2 \pi r^2 + r^{-1} u$
Since $E = c_3 u T$
$$ E' = c_3 (u' T + u T') = c_4 (c_5 + c_6 u) $$
Choosing the constants so that it fits. Combining with $T = g(u') \implies T' = g'(u') u''$ we can put it all together to get the formula in question.","['mathematical-physics', 'ordinary-differential-equations']"
2912002,How can I find the ages of the people in a word problem involving family ties which seems intricate?,"The problem is as follows: Marvin is the only son of Jennifer's grandfather, and Lindsey is the only daughter-in-law of Marvin's grandfather. If the only children of Jennifer are twins of 7 years of age and in this family it is true that from one generation to another consecutive 19 years have elapsed, what is the sum of the ages of the father and the great-grandfather of Jennifer? The alternatives in my book are: $109$ years $135$ years $128$ years $116$ years $147$ years Normally I would have tried to give an attempt to solve and show my progress but in this case I'm stuck at the very beginning hence I can't offer much other than what I'm already understanding which is described in the following lines. I don't know if the way to go is to build up a single variable equation or is it a square one?. The place where I'm stuck at is Lindsey, what kind of clue does she gives to the problem because I don't know how to relate it with the others. Therefore, this problem has got me go around in circles for several days and I can't find how to get a clue where to begin. Can somebody help me to go in the right direction?. It would help me the most if a suggested answer could include a visual aid to relate family ties. I mean how to tell in a tree where is the great grandfather, grandfather, son (husband) and daughter-in-law and grandchildren. To me this would greatly improve my understanding of this, since I'm lost at here where to tell the difference and understand how to make the math.","['word-problem', 'algebra-precalculus']"
2912006,A problem about the differential mean value theorem $2ηf(1)+(c^2-1)f'(η)=f(ξ)$,"Assume that the function $f : \left[0, 1\right] \to \mathbb{R}$ is continuous on $\left[0,1\right]$ and is differentiable on $\left(0,1\right)$ . Let $c \in \left(0,1\right)$ . Prove that there exist $\xi, \eta \in \left(0, 1\right)$ such that \begin{align}
2 \eta f\left(1\right) + \left(c^2 - 1\right) f^\prime\left(\eta\right) = f\left(\xi\right) .
\end{align} I tried to use the Lagrange mean value theorem and the Rolle mean value theorem on $[0,1]$ , but failed.","['proof-explanation', 'proof-writing', 'analysis', 'real-analysis']"
2912030,Suppose $X$ is infinite and $A$ is a finite subset of $X$. Then $X$ and $X \setminus A$ are equinumerous,"Suppose that $X$ is infinite and that $A$ is a finite subset of $X$. Then $X$ and $X \setminus A$ are equinumerous. My attempt: Let $|A|=n$. We will prove by induction on n. It's clear that the the theorem is trivially true for $n=0$. Assume the theorem is true for all $n=k$. For $n=k+1$, then $|A \setminus \{a\}|=k$ for some $a \in A$. Thus $X \setminus (A \setminus \{a\}) \sim X$ by inductive hypothesis, or $(X \cap \{a\}) \cup (X \setminus A) \sim X$, or $\{a\} \cup (X \setminus A) \sim X$. We have $\{a\} \cup (X \setminus A) \sim X \setminus A$ since the theorem is true for $n=1$. Hence $X \setminus A \sim \{a\} \cup (X \setminus A) \sim X$. Thus $X \setminus A \sim X$. This completes the proof. Does this proof look fine or contain gaps? Do you have suggestions? Many thanks for your dedicated help! Update : Here I prove that the theorem is true for $n=1$. Assume that $A = \{a\}$ and consequently $X \setminus A= X \setminus\{a\}$. It's clear that $|X \setminus A| \le |X|$. Next we prove that $|X| \le |X \setminus A|$. Since $X$ is infinite, there exists $B \subsetneq X$ such that $B \sim X$ (Here we assume Axiom of Countable Choice). Thus $|X|=|B|$. There are only two possible cases. $a \in X \setminus B$ Then $B \subseteq X \setminus \{a\}=X \setminus A$ and consequently $|X|=|B| \le |X \setminus A|$. Thus $|X| \le |X \setminus A|$ and $|X \setminus A| \le |X|$. By Schröder–Bernstein theorem, we have $|X \setminus A| = |X|$. It follows that $X \setminus A \sim X$. $a \in B$. Let $b \in X \setminus B$. We define a bijection $f:X \setminus \{a\} \to X \setminus \{b\}$ by $f(x)= x$ for all $x \in X \setminus \{a,b\}$ and $f(b)=a$. Thus $X \setminus \{a\} \sim X \setminus \{b\}$. Since $b \in X \setminus B$, it follows from Case 1 that $X \setminus \{b\} \sim X$. Hence $X \setminus \{a\} \sim X \setminus \{b\} \sim X$. Thus $X \setminus \{a\} = X \setminus A \sim X$. To sum up, $X \setminus A \sim X$ for all $|A|=1$.","['elementary-set-theory', 'proof-verification', 'infinity']"
2912041,Log transforming an ODE,"I'm doing some numerical simulations of an exponential growth like system which, for simplicity, has the form: $$
\frac{dx}{dt}= ax + bxy \quad\quad \frac{dy}{dt}= cy + dxy 
$$ For some parameter values i get instability in the simulation though I remember reading a paper which used log transformations to prevent this. Any ideas on how I could do this or how to rewrite the equations as: $$
\frac{d log(x)}{dt}= \ldots \quad\quad \frac{d log(y)}{dt}= \ldots
$$","['numerical-methods', 'systems-of-equations', 'simulation', 'ordinary-differential-equations']"
2912043,Range of $y = \frac{x^2-2x+5}{x^2+2x+5}$?,"How do I approach this problem? My book gives answer as $[\frac{3-\sqrt{5}}{2},\frac{3+\sqrt{5}}{2}]$. I tried forming an equation in $y$ and putting discriminant greater than or equal to zero but it didn't work. Would someone please help me? I get $x^2 (y-1) + 2x (y+1) + (5y-5) =0$ and discriminant gives $2y^2 - y + 2 \leq 0$, which has complex roots.",['functions']
2912055,Question about Lemma on Primitive Groups,"I am reading the following lemma in a paper about graph isomorphism: Lemma:
Let P be a transitive p-subgroup of Sym(A) with |A| > 1.  Then any minimal p-block system consists of exactly p blocks.  Furthermore, the subgroup P' which stabilizes all of the blocks has index p in P. Proof:
The quotient P/P' is a primitive p-group (acting on the blocks) and so the order of P/P' = the number of blocks = p. Does p-subgroup mean the order is some power of p or p? I am guessing when they say P' stabilizes all of the blocks, they mean stabilizes all the blocks in a minimal block system, since otherwise one could say each element is a block.  (This paper's definition of a block does not disallow one element blocks).  However, I would think that a minimal block system may not be unique.  Is such a system unique?  Maybe it is. Now, I intuitively understand that P/P' is a primitive group acting on the blocks as we are modding out by everything that stabilizes the blocks and so intuitively we are making those many stabilizing permutations the same one thing in the kernel and sort of merging the blocks into a single element in the quotient.  So, intuitively it makes sense to me.  Can someone prove it for me?  Why is the order of P/P' the number of blocks?  That makes no sense to me.  I would assume that the order of P/P' would be (the number of blocks)! since we can permute them as we please.","['permutations', 'graph-isomorphism', 'group-theory']"
2912077,Show that a matrix is a multiple of the identity [duplicate],"This question already has answers here : If $C$ commutes with certain matrices $A$ and $B$, why is $C$ a scalar multiple of the identity? (3 answers) Closed 5 years ago . Let $A$, $B$, $C$ three complex 2x2 matrices such that $$A^2=B^3=I\;(A\neq I\neq B),\quad ABA=B^{-1},\quad AC=CA,\quad BC=CB.$$ Show that $C=rI$ for some $r\in\mathbb{C}$. I got the minimal polynomials of $A$ and $B$, but I can't go further to find the minimal polynomial of $C$. Any suggestion?","['matrices', 'linear-algebra']"
2912080,"Solution verification post : Problem $12.2$, Mathematical Analysis, Apostol","As a continuation of this post , I'm posting my solutions (or attempts) to the exercise problems of Chapter 12 (Multivariable Differential Calculus), Mathematical Analysis by Apostol . Since I'm essentially self-studying, I'd really appreciate if anyone checks the solutions and let me know if there is any gap in my arguments or if there exists any cleverer solutions. Thank you. Problem $12.2.$ Calculate all first order partial derivatives and the directional derivative $f'(x;u)$ for each of the real-valued functions defined on $\mathbb{R}^n$ as follows :
\begin{align*}
&(a)\,\, f(x)=a \boldsymbol{\cdot} x\\
&(b)\,\, f(x)=\left\Vert x \right\Vert^4\\
&(c)\,\, f(x)=x \boldsymbol{\cdot} L(x), \text{ where } L:\mathbb{R}^n \to \mathbb{R}^n \text{ is a linear function.}\\
&(d)\,\, f(x)=\sum_{i=1}^n \sum_{j=1}^n a_{ij}x_ix_j, \text{ where } a_{ij}=a_{ji}.
\end{align*} Solution. $(a)$ Let $a=(a_1,\dots,a_n)$, $x=(x_1,\dots,x_n)$. Thus,
$$f(x)=a \boldsymbol{\cdot} x=a_1x_1+\dots+a_nx_n \tag1$$
Then we have,
$$D_kf(x)=\frac{\partial f}{\partial x_k}(x)=a_k=a \boldsymbol{\cdot} e_k; \,\,k=1,\dots,n \tag2$$
$D_kf(x)$ is constant for all $k$ $\Rightarrow$ $D_kf(x)$ is continuous for all $k$. We recall the following : Theorem. Assume that one of the partial derivatives $D_1f,\dots,D_nf$ exists at $c$ and that the remaining $n−1$ partial derivatives exists in some $\delta$-ball $B_{\delta}(c)$, and are continuous at $c$. Then $f$ is differentiable at $c$. Using the theorem, we conclude that $f$ is differentiable and hence directional derivative at any direction exists. Then, \begin{align*}
f'(x;u)&=\lim_{h \to 0} \frac{f(x+hu)-f(x)}{h}\\
&=\lim_{h \to 0} \frac{a \boldsymbol{\cdot} (x+hu)-a \boldsymbol{\cdot} x}{h}\\
&=\lim_{h \to 0} \frac{a \boldsymbol{\cdot} (hu)}{h}\\
&=\lim_{h \to 0} \frac{h \sum_{i=1}^n a_iu_i}{h}\\
&=\sum_{i=1}^n a_iu_i=a \boldsymbol{\cdot} u \tag3
\end{align*} Solution. $(b)$
\begin{align*}
f(x)&=\left\Vert x \right\Vert^4=\left(\sum_{i=1}^n x_i^2\right)^2=\sum_{i=1}^n x_i^4+\sum_{i \neq j} x_i^2x_j^2\\
&=x_k^4+\sum_{i \neq k} x_i^4+x_k^2\left(\sum_{i \neq k} x_i^2\right)+\left(\sum_{i \neq k} x_i^2\right)x_k^2+\sum_{i \neq k,\,j \neq k,\,i \neq j} x_i^2x_j^2\\
&=x_k^4+\sum_{i \neq k} x_i^4+2x_k^2\left(\sum_{i \neq k} x_i^2\right)+\sum_{i \neq k,\,j \neq k,\,i \neq j} x_i^2x_j^2 \tag4
\end{align*} Let $k \in \{1,\dots,n\}$. Then,
$$D_kf(x)=4x_k^3+4x_k\left(\sum_{i \neq k}x_i^2\right)=4x_k\left(\sum_{i=1}^nx_i^2\right)=4x_k\left\Vert x \right\Vert^2 \tag5$$
Thus, $D_kf(x)$ exists and is continuous for all $k \in \{1,\dots,n\}$. By the statement stated above, $f$ is differentiable, and hence directional derivative exists in all directions. Let $u=(u_1,\dots,u_n)=u_1e_1+\dots+u_ne_n$. Now,
\begin{align*}
f'(x;u)&=f'(x)(u)=f'(x)(u_1e_1+\dots+u_ne_n)=\sum_{k=1}^n u_kf'(x)(e_k)=\sum_{k=1}^n u_kf'(x;e_k)\\
&=\sum_{k=1}^n u_k\frac{\partial f}{\partial x_k}(x)=\sum_{k=1}^n u_k\left(4x_k\left\Vert x \right\Vert^2\right)=4\left\Vert x \right\Vert^2\sum_{k=1}^n x_ku_k=4\left\Vert x \right\Vert^2 \left(x \boldsymbol{\cdot} u\right) \tag6
\end{align*} $(c)$
\begin{align*}
D_kf_(x)&=\lim_{h \to 0} \frac{f(x+he_k)-f(x)}{h}\\
&=\lim_{h \to 0} \frac{(x+he_k) \boldsymbol{\cdot} L(x+he_k)-x \boldsymbol{\cdot} L(x)}{h}\\
&=\lim_{h \to 0} \frac{(x+he_k) \boldsymbol{\cdot} \left(L(x)+L(he_k)\right)-x \boldsymbol{\cdot} L(x)}{h}\\
&=\lim_{h \to 0} \frac{x \boldsymbol{\cdot} L(x)+x \boldsymbol{\cdot} L(he_k)+he_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(he_k)-x \boldsymbol{\cdot} L(x)}{h}\\
&=\lim_{h \to 0} \frac{x \boldsymbol{\cdot} L(he_k)+he_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(he_k)}{h}\\
&=\lim_{h \to 0} \frac{x \boldsymbol{\cdot} hL(e_k)+he_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} hL(e_k)}{h}\\
&=\lim_{h \to 0} \frac{hx \boldsymbol{\cdot} L(e_k)+he_k \boldsymbol{\cdot} L(x)+h^2e_k \boldsymbol{\cdot} L(e_k)}{h}\\
&=\lim_{h \to 0} \frac{h\left(x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(e_k)\right)}{h}\\
&=\lim_{h \to 0} x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x)+he_k \boldsymbol{\cdot} L(e_k)\\
&=x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x) \tag7
\end{align*} By continuity of $x$ and $L(x)$, we conclude that $D_kf_(x)$ is continuous for all $k \in \{1,\dots,n\}$. Thus, by the theorem stated above, $f$ is differentiable and hence, directional derivative of $f$ exists in all directions. Let $u=(u_1,\dots,u_n)$. Then, \begin{align*}
f'(x;u)&=\sum_{k=1}^n u_kD_kf_(x)\\
&=\sum_{k=1}^n u_k\left(x \boldsymbol{\cdot} L(e_k)+e_k \boldsymbol{\cdot} L(x)\right)\\
&=\sum_{k=1}^n \left(x \boldsymbol{\cdot} u_kL(e_k)+u_ke_k \boldsymbol{\cdot} L(x)\right)\\
&=\sum_{k=1}^n \left(x \boldsymbol{\cdot} L(u_ke_k)+u_ke_k \boldsymbol{\cdot} L(x)\right)\\
&=x \boldsymbol{\cdot} \sum_{k=1}^nL(u_ke_k)+\sum_{k=1}^n\left(u_ke_k\right) \boldsymbol{\cdot} L(x)\\
&=x \boldsymbol{\cdot} L(\sum_{k=1}^n u_ke_k)+\sum_{k=1}^n\left(u_ke_k\right) \boldsymbol{\cdot} L(x)\\
&=x \boldsymbol{\cdot} L(u)+u \boldsymbol{\cdot} L(x) \tag8
\end{align*} $(d)$
\begin{align*}
f(x)&=\sum_{i=1}^n\sum_{j=1}^n a_{ij}x_ix_j\\
&=a_{kk}x_k^2+\sum{i \neq k}a_{ik}x_ix_k+\sum{i \neq k}a_{ki}x_kx_i+\sum_{i \neq k,\,j \neq k,\,i \neq j} a_{ij}x_ix_j \tag9
\end{align*} Then we have,
\begin{align*}
D_kf(x)&=2a_{kk}x_k+\sum_{i \neq k} a_{ik}x_i+\sum_{i \neq k} a_{ki}x_i\\
&=2a_{kk}x_k+2\sum_{i \neq k} a_{ik}x_i \,\left(\text{since } a_{ik}=a_{ki}\right)\\
&=2\sum_{i=1}^n a_{ik}x_i \tag{10}
\end{align*} Thus, $D_kf(x)$ exists and is continuous. Hence, $f$ is differnetiable and thus, it has directional derivative in every direction. Then,
$$f'(x;u)=\sum_{k=1}^n u_k \frac{\partial f}{\partial x_k}(x)=2\sum_{i=1}^n u_k\left(\sum_{i=1}^n a_{ik}x_i\right)=2\sum_{i=1}^n\sum_{i=1}^n a_{ik}x_iu_k=2x^TAu, \tag{11}$$
where $A={(a_{ij})_{i=1}^n}_{j=1}^n$.","['proof-verification', 'analysis', 'real-analysis', 'multivariable-calculus', 'derivatives']"
2912087,Does the domain of the function depend on how you write it?,"What is the domain of the function $f(x,y)= \sqrt{xy} $. In this case, domain is $D=\{(x,y) \in R^2: \}$ Since $ \sqrt{xy} = x^{0.5}y^{0.5} $ I can write the function equivalently as $f(x,y)= x^{0.5}y^{0.5}  $ but in this case domain is only $D=\{(x,y) \in R^2: x \geq 0, y \geq0 \}$ So, which one is the domain finally? Is it possible that the function $f$ has different domain depending on how you write it? This is confusing.",['functions']
2912091,Solving Homogenous First Order ODE Using Substitution,"Problem Find the general solution of the following homogeneous equation. $$ ty' = y + \sqrt { t^2 - y^2} $$ Attempt I'm following the algorithm provided in section 1.5 of the William Adkins & Mark G. Davidson, Ordinary Differential Equations textbook on page 63, which suggests rearranging the equation into the form: $$ y' =  ... $$ where $ y = tv $ and $ y' = v + tv' $ (v is the substitution variable). I haven't been able to get too far in the procedure to make the substitution: $$ y' = \frac{y + \sqrt { t^2 - y^2}}{t} $$
$$ y' = \frac{\frac{1}{t}}{\frac{1}{t}} \frac{y + \sqrt { t^2 - y^2}}{t} $$
$$ y' = \frac{y}{t} + \frac{\sqrt { t^2 - y^2}}{t} $$ Notes Can someone please help me reduce the RHS of the above equation to the form $ y' = f(\frac{y}{t}) $ (an equation in terms of $ (\frac{y}{t}) $), using the substitution $ y = tv $? For bonus points, please provide a solution to the ODE so myself and other viewers can check their answers! Thanks in advance! Solution With the help of Isham, I was able to get the solution. $$ ty' = y + \sqrt { t^2 - y^2 } $$ It is implied that $ t^2 - y^2 \geq 0 $, and so $ t^2 \geq y^2 \gt 0 $ or $ \lt 0 $. $$ ty' - y = \sqrt { t^2 - y^2} $$
$$ \frac {1}{\sqrt {t^2} } (ty' - y) = \frac {1}{\sqrt {t^2} } \sqrt { t^2 - y^2} $$
$$ \frac {1}{t} (ty' - y) = \sqrt { \frac{t^2}{t^2} - \frac{y^2}{t^2}} $$
$$ y' - \frac{y}{t} = \sqrt { 1 - \frac{y^2}{t^2}} $$
$$ y' - \frac{y}{t} = \sqrt { 1 - (\frac{y}{t})^2} $$
$$ y' = \sqrt { 1 - (\frac{y}{t})^2} + \frac{y}{t} $$ Let $ v = \frac{y}{t} $. Then, $ y = tv $ and so $ y' = \frac{dy}{dt} = v + tv' $. Substituting for $ v $ in the above equation we get: $$ v + tv' = \sqrt {1 - v^2} + v $$
$$ tv' = \sqrt {1 - v^2} $$ Note, since $ t^2 \geq y^2 $, $ \frac{y^2}{t^2} = v^2 \leq 1 $, so there are no equilibrium cases to consider. $$ t\frac{dv}{dt} = \sqrt {1 - v^2} $$
$$ \frac{1}{\sqrt { 1 - v^2}}dv = \frac{1}{t}dt $$
$$ \int\frac{1}{\sqrt { 1 - v^2}}dv = \int\frac{1}{t}dt $$
$$ arcsin(v) = ln|t| + C_1, C_1 \in \mathbb R $$
$$ \frac {y}{t} = \sin{(ln|t| + C_1)} $$
$$ y = t \sin{(ln|t| + C_1)} $$","['substitution', 'ordinary-differential-equations']"
2912164,Choosing office hours to maximise number of students who can attend at least one time slot,"I have the following (real world!) problem which is most easily described using an example. I ask my six students when the best time to hold office hours would be. I give them four options, and say that I'll hold two hours in total. The poll's results are as follows (1 means yes, 0 means no): $$\begin{array}{l|c|c|c|c} 
\text{Name} & 9 \text{ am} & 10 \text{ am} & 11 \text{ am} & 12 \text{ pm} \\ \hline
\text{Alice} & 1 & 1 & 0 & 0 \\ 
\text{Bob} & 1 & 1 & 0 & 1 \\ 
\text{Charlotte} & 1 & 1 & 0 & 1 \\ 
\text{Daniel} & 0 & 1 & 1 & 1 \\
\text{Eve} & 0 & 0 & 1 & 1 \\
\text{Frank} & 0 & 0 & 1 & 0 \\ \hline
\text{Total} & 3 & 4 & 3 & 4
\end{array}$$ Naively, one might pick columns 2 and 4, which have the greatest totals. However, the solution which allows the most distinct people to attend is to pick columns 2 and 3. In practice, however, I have ~30 possible timeslots and over 100 students, and want to pick, say, five different timeslots for office hours. How do I pick the timeslots which maximise the number of distinct students who can attend?","['optimization', 'combinatorics']"
2912165,Lower bound for complex polynomial beyond circle or radius R [duplicate],"This question already has answers here : For a polynomial $p(z)$, prove there exist $R>0$, such that if $|z|=R$, then $|p(z)|\geq |a_n|R^n/2$ (2 answers) Closed 5 years ago . If we have a polynomial with $c_i$ a complex number
$$c_nz^n + c_{n-1}z^{n-1} + \cdots + c_1 z + c_0$$
then $$|P(z)| > \frac{|c_n|R^n}{2}$$
When |z| > R for some R I have tried using the triangle inequality where I obtain,
$|P(Z)| \leq |c_n||z|^n + \cdots + |c_0|$. But I seem to keep getting stuck. Any hints on how to move forward? Thank you!","['complex-analysis', 'polynomials', 'upper-lower-bounds']"
2912200,When is an argument without premises valid?,"So the question is how do we know if an argument without premises is valid. First of all, how would that go? I mean, what would an argument without premises look like (in terms of propositional logic)? Would there just be a conclusion? I also read that in case of a tautology, that would be a valid argument and I simply don't understand how there even can be a truth table created if there are no premises. Also, why is $p$ or not $p$ an argument without premises? Isn't $p$ itself a premise? Excuse my probably very simple questions, I'm very new to propositional logic or rather discrete math as a whole.","['propositional-calculus', 'logic', 'discrete-mathematics']"
2912213,How to generate a Penrose tessellation around a given tile?,"Given a starting Penrose tile, I need to build a ""spiraling"" tessellation around it. The following picture illustrates the request: In this example, the starting tile is a ""thin rhombus"" (the pink one). I need to write an algorithm which is able to generate the $n$ tiles (and whose output is, for instance a, SVG file), starting from any given tile, and with the possibility of coloring the tiles according to a given sequence of $n$ colors. Thanks for your help! NOTE: This post is related to this one .","['geometry', 'hamiltonian-path', 'discrete-mathematics', 'math-software', 'tiling']"
2912249,"show $\lim\limits_{(x,y)\to(0,0)}\frac{|x|^a|y|^b}{|x|^c+|y|^d}$ exists and equals $0$? [duplicate]","This question already has an answer here : Show that $ \lim_{(x,y) \to 0} \frac {|x|^ \alpha |y|^ \beta} {|x|^ \gamma + |y|^ \delta} \text {exists} \iff \alpha/\gamma + \beta/\delta > 1.$ (1 answer) Closed 5 years ago . suppose $f(x,y)=\frac{|x|^a|y|^b}{|x|^c+|y|^d}$ and $a,b,c,d >0$ how would you show that if $\frac{a}{c} + \frac{b}{d} >1$ then $\lim\limits_{(x,y)\to(0,0)}\frac{|x|^a|y|^b}{|x|^c+|y|^d}$ exists and equals $0$? I've been trying to use the squeeze theorem and set up an inequality but im really struggling.","['multivariable-calculus', 'limits', 'calculus']"
2912258,Confusion about Nelson's proof of Liouville's theorem,"Nelson's proof of Liouville's theorem (in the case $n=2$) is as follows: Consider a bounded harmonic function on Euclidean space. Since
  it is harmonic, its value at any point is its average over any sphere,
  and hence over any ball, with the point as center. Given two points,
  choose two balls with the given points as centers and of equal radius.
  If the radius is large enough, the two balls will coincide except for an
  arbitrarily small proportion of their volume. Since the function is
  bounded, the averages of it over the two balls are arbitrarily close,
  and so the function assumes the same value at any two points. Thus
  a bounded harmonic function on Euclidean space is a constant. I tried to formalize it. Let $f:\mathbb{C}\to\mathbb{C}$ be a holomorphic bounded function. If $z,w\in\mathbb{C}$ we have that
\begin{align*}
        |f(z)-f(w)| &= \frac{1}{\pi r^2}\left|\int_{D(z,r)}f(x+iy)\:\mathrm{d}x\:\mathrm{d}y - \int_{D(w,r)}f(x+iy)\:\mathrm{d}x\:\mathrm{d}y\right|\\
&= \frac{1}{\pi r^2}\left|\int_{A}f(x+iy)\:\mathrm{d}x\:\mathrm{d}y - \int_{B}f(x+iy)\:\mathrm{d}x\:\mathrm{d}y\right| \\
        &\leq \frac{2}{\pi r^2}(\sup |f|)\int_A 1\:\mathrm{d}x\:\mathrm{d}y \\
        &= \frac{2}{\pi r^2}(\sup |f|) \left(2r^2\cos^{-1}\left(\frac{d}{2r}\right)-\frac{d}{2}\sqrt{4r^2-d^2}, \right)
    \end{align*} where $d=|z-w|$. Observe what are $A$ and $B$ in the following drawing: I would expect that the right hand side tends to $0$ when $r\to\infty$. However, that is not the case. How should I formalize Nelson's proof?",['complex-analysis']
2912297,Finding derivative given $f(x)$ and limit,"this is my very first post here. I'm a longtime lurker, I have learned a lot from this site and I hope I can, even though I'm still a student, be of any help to others. 
I have been working on the following exercise. Given:
$ f(4) = a, f'(4) = b
$ and 
$$ \lim_{x\to 2} = {f(3x-2)-5 \over (x^2-4)} = 9$$
Find a and b.
I already found the value of $a$ following this reasoning: $f(4)$ must be such a value that, when replaced in the given limit, yields zero; otherwise, the limit won't be $9$ but infinity. Under the assumption that I can factor the denominator easily -and simplify $(x-2)$ from $f(x)$ I found $a=f(4)=5$ and it coincides with the answer. I have, however, no reasonable clue of how to approach the $f'(4)=b$ part. All the information I seem to have is that the derivative is continuous, and that $(x-2)$ must be a factor of $f(x)$ .If it's derivative is continuous over Reals, then I infer that it is not a rational function. But I still cant see the right path towards the solution.
I'd be extremely grateful if someone could give me a hint, so I can work it out by myself Thanks in advance!","['functions', 'derivatives']"
2912358,A topological proof of the Nullhomotopical Cauchy Integral Formula from the Circle Cauchy Integral Formula,"I believe there should be a simple topological proof of the Nullhomotopical Cauchy Integral Formula based only on the Cauchy Integral Formula over a Circle, but I can't quite finish the argument and would appreciate some help. (Hopefully I am on the right track.) Nullhomotopical Cauchy Integral Formula : Let $U\subseteq\mathbb{C}$ be an open and path-connected subset, let $z_0\in U$, and let $\gamma\subseteq U$ be some, say, smooth loop such that $z_0\notin\gamma$ and $\gamma\simeq\rm{pt.}$ in $U$, where by abuse of notation $\gamma$ stands for both the curve and its support.
If $f:U\to\mathbb{C}$ is a holomorphic function, then:
$$
f(z_0)\operatorname{ind}(\gamma,z_0) = \frac{1}{2\pi i}\oint_\gamma\frac{f(z)}{z-z_0}\mathrm{d}z
$$ Attempted proof: I know that, being holomorphic, the 1-form
$$
\omega:=\frac{f(z)}{z-z_0}\mathrm{d}z
$$
is $\mathrm{d}$-closed, hence (its integral is) homotopy-invariant.
Since $\gamma\subseteq U\setminus\{z_0\}$ and $[\gamma]=0$ in $\pi_1(U)$, I believe that $[\gamma]$ should induce a well-defined class $[\gamma]'$ in $\pi_1(D\setminus\{z_0\})$, where $D$ is a sufficiently small open disk around $z_0$ contained in $U$, but I don't quite see how to finish this line of reasoning rigorously, assuming it actually makes sense. Any help would be appreciated!","['complex-analysis', 'algebraic-topology']"
2912381,How many combinations of sets / algorithm to generate all possible?,"I've tried searching around, but I think I'm not familiar with the terminology to find the correct terms to search for. Anyways, I've run into this problem where I need to combine certain elements in groups of three - and where each combination has to fulfill some specific criteria. The criteria themselves aren't so important, but I'm stuck on creating all the different possible combinations of elements. Possibly the number of combinations is too large for me to handle the way I plan to, so any help just calculating the number of possible combinations would be helpful as well. Problem:
I have 360 elements that I want to divide into groups of 3 elements in each set. Each element is an actual physical component, so it can only be a part of one group. The order of elements in each group doesn't matter. Since I have 360 elements and there is 3 elements in each group, this means that I'm making 120 groups. But there are many ways to arrange these groups. My problem is:
1. How many unique set of groups can I make? (Order of elements in groups and order of groups doesn't matter).
2. I feel there should be a fairly straight-forward algorithm to create all these possible sets of groups, yet I've twisted my brain all day long without finding out how. Any help would be much appreciated. Example with fewer elements and lower group size: Say I had 4 elements and was to group them in pairs. This means I would make 2 groups per set.
If I start by giving each element just an index number, then I start with the following elements: 1, 2, 3, 4
I could then group them as follows: Set 1: (1, 2) and (3, 4)
Set 2: (1, 3) and (2, 4)
Set 3: (1, 4) and (2, 3) Another example: Say I have 6 elements and want to group them in triplets. This also means 2 groups per set.
So I have the following elements: 1, 2, 3, 4, 5, 6 Set 1: (1, 2, 3) and (4, 5, 6)
Set 2: (1, 2, 4) and (3, 5, 6)
Set 3: (1, 2, 5) and (3, 4, 6)
Set 4: (1, 2, 6) and (3, 4, 5)
Set 5: (1, 3, 4) and (2, 5, 6)
Set 6: (1, 3, 5) and (2, 4, 6)
Set 7: (1, 3, 6) and (2, 4, 5)
Set 8: (1, 4, 5) and (2, 3, 6)
Set 9: (1, 4, 6) and (2, 3, 5)
Set 10: (1, 5, 6) and (2, 3, 4) But how many such sets exists if I have 360 elements to divide into groups of 3? And how do I generate the list of all those sets?",['combinatorics']
2912384,"If $\{f_n\}$ converges uniformly on $(a,b)$, $\{f_{n}(a)\}$ and $\{f_{n}(b)\}$ converge pointwise, then it converges uniformly on $[a,b]$","Prove that if $\{f_n\}$ converges uniformly on $(a,b)$, $\{f_{n}(a)\}$ and $\{f_{n}(b)\}$ converge pointwise. I want to show that $\{f_n\}$ converges uniformly on $[a,b]$ MY TRIAL: Let $\epsilon>0$, since $\{f_n\}$ converges uniformly on $(a,b)$, then $\exists\,N_1=N(\epsilon)$ s.t. $\forall \,n\geq N$,  $\forall x\in (a,b)$
\begin{align}\left|f_n(x)-f(x)\right|<\epsilon\end{align} Since $\{f_n\}$ converges pointwise when $x=a$, then $\exists\,N_2=N(\epsilon,a)$ s.t. $\forall \,n\geq N$, \begin{align}\left|f_n(a)-f(a)\right|<\epsilon\end{align} Also, $\{f_n\}$ converges pointwise when $x=b$, then $\exists\,N_3=N(\epsilon,b)$ s.t. $\forall \,n\geq N$, \begin{align}\left|f_n(b)-f(b)\right|<\epsilon\end{align} Hence, $\forall \,n\geq \max\{N_1,N_2,N_3\},$ $$\lim\limits_{n\to \infty}f_n(x)=\begin{cases}f(a) & x=a,\\f(x)&x\in(a,b),\\f(b)&x=b\end{cases}$$
Hence, $\{f_n\}$ converges uniformly on $[a,b]$. Please, I'm I right? If no, an alternative proof will be highly regarded. Thanks!","['convergence-divergence', 'uniform-convergence', 'analysis', 'real-analysis']"
2912391,Is there any equivalent of calculus in a modular field?,"For example, does $\frac{dy}{dx}\equiv c\mod n$ hold any meaning? How are the rules of calculus modified to allow for this, if possible. For a simple example, $ax\equiv b\mod n$ behaves the same as $ax=b$ for  $x\in[0,n)$, after which there is a discontinuity and then the initial interval output repeats because $ax\equiv((a\mod n)(x\mod n))\mod n$. We could therefore define $\frac{dy}{dx}\equiv a\mod n$ at all points not a multiple of $n$ for lines. It would seem apt to be able to extend this definition to at least higher order polynomials. One may also need to keep in mind $x\mod n=\frac{x}{n}-n\lfloor{\frac{x}{n}\rfloor}$ as a real number extension for issues of continuity, although this makes derivatives even more confuddling.","['number-theory', 'calculus', 'modular-arithmetic']"
2912446,Disprove/Prove Existence of Periodic Solution for Autonomous ODE,Consider the system $\dot x = x^2 + y^2 -1$ and $\dot y = y - 2xy$. I am new in this field. I draw the vector filed and I saw that there is no obvious periodic solution. How can I prove/disprove the existence of periodic solution for this system?,"['stability-theory', 'control-theory', 'ordinary-differential-equations', 'dynamical-systems']"
2912469,"Is it possible to fill 1×1 rectangle with 1×½, ½×⅓, ⅓×¼,...., 1/n×1/(n+1) rectangles?","Is it possible to fill $1\times1$ rectangle with $1 \times \frac{1}{2}$ , $\frac{1}{2} \times \frac{1}{3}$ , $\frac{1}{3} \times \frac{1}{4}$ .., $\frac{1}{n}\times\frac{1}{n+1}$ ... rectangles? This row converges, because when $n \rightarrow \infty$ . $\sum_{i=1}^\infty(\frac{1}{i}\cdot\frac{1}{i+1}) = \sum_{i=1}^\infty (\frac{1}{i} - \frac{1}{i+1}) = 1 + O(\frac{1}{n^2}) = 1$ As i thought, i should prove that if I can place $\frac{1}{n}\times\frac{1}{n+1}$ rectangle, I can also place $\frac{1}{n+1}\times\frac{1}{n+2}$ (following math induction principle). But here I'm facing a problem.
Also I want to know a filling algorithm, if it exists. Update: As Kevin P. Costello mentioned this is an open problem.","['rectangles', 'induction', 'geometry', 'algorithms']"
2912491,Proving that $2xy'=4x^{2}+3y^{2}$ has no real solutions,"The differential equation above is a Ricatti equation. I'm not given any particular solution to it. I'm actually asked to solve it. I tried graphing the direction field to get an idea on the behavior of the curve with no result. However, it is true that this equation has no real solution. To solve it we need to use Bessel equations (I'm still being introduced to second order differential equations). How can we prove that $2xy'=4x^{2}+3y^{2}$ has no real solution? Can we use the existence and unicity theorem? Also, it is possible to prove it without using Lipschitz continuity?","['ordinary-differential-equations', 'real-analysis']"
2912544,Cap product for Hochschild (co)chains,"Let $A$ be an associative algebra and $M$ be an $A$ -bimodule. Then we can form the Hochschild cochains $C^\bullet(A,A)$ and chains $C_\bullet(A, M)$ and define a pairing (cap product) $$
  C^\bullet(A,A) \otimes C_\bullet(A, M)
  \xrightarrow{\enspace\frown\enspace}
  C_\bullet(A, M)
$$ in the following way: for $f\in C^n(A,A)$ and $g = m \otimes a_1 \otimes \dotsb \otimes a_p \in C_p(A,M)$ , $$f\frown g := 
\begin{cases}
  (-1)^n m f(a_1 \otimes \dotsb \otimes a_n) \otimes a_{n+1} \otimes \dotsb \otimes a_n & \text{for $p \geq n$},\\
  0 & \text{for $p < n$},
\end{cases}
  $$ which is an element of $C_{n-p}(A, M)$ . There is the following simple assertion: Proposition. $C_\bullet(A, M)$ is a differential graded module over $C^\bullet(A, A)$ with respect to the cap product, i.e., $$
  d(f \frown g) = d(f) \frown g + (-1)^{\deg(f)} f \frown d(g) \,.
  \tag{1}
$$ The reference for this is https://pbelmans.ncag.info/assets/hh-2018-notes.pdf page 24 or http://www.math.tamu.edu/~sarah.witherspoon/pub/HH-25August2018.pdf page 17. This looked to be an easy exercise, but even in simple cases the computations went wrong. For example, let $f \in C^1(A, A)$ and $g = m \otimes a_1 \otimes a_2 \in C_2(A, M)$ . Then $f \frown g = -m f(a_1) \otimes a_2$ and the left-hand side of $(1)$ is $$
  d(f \frown g)
  = d(-m f(a_1) \otimes a_2)
  = -(m f(a_1) a_2 - a_2 m f(a_1))
  = a_2 m f(a_1) - m f(a_1) a_2 \,.
$$ At the same time, the right-hand side is \begin{align*}
  {}&
  d(f) \frown g + (-1)^{\deg(f)} f \frown d(g) \\
  ={}& m \cdot d(f)(a_1 \otimes a_2) - f \frown (m a_1 \otimes a_2 - m \otimes a_1 a_2 + a_2 m \otimes a_1) \\
  ={}& m a_1 f(a_2) - m f(a_1 a_2) + m f(a_1) a_2 + m a_1 f(a_2) - m f(a_1 a_2) + a_2 m f(a_1) \\
  ={}& 2 m a_1 f(a_2) - 2 m f(a_1 a_2) + m f(a_1) a_2 + a_2 m f(a_1) \,.
\end{align*} These two expressions obviously do not coincide. Where I was wrong? Edit. It seems that there is a sign mistake in the definition of the cap product given above and the right definition should be $$
  f \frown g := m f(a_1 \otimes \dotsb \otimes a_n) \otimes a_{n+1} \otimes \dotsb \otimes a_n \,.
$$","['homological-algebra', 'abstract-algebra', 'hochschild-cohomology']"
2912566,What is the Probability of Choosing 3 Specific Balls from a 15-Ball Set?,"My high school geometry textbook includes the following problem as an example: Three pool balls are chosen at random from a set numbered from 1 to 15.  What is the probability that the pool balls chosen are numbered 5, 7, and 9? My answer is $\frac{1}{455}$.  The text book's answer is $\frac{6}{455}$. Which answer is correct? I arrive at my answer, as follows: The probability of the desired result is the ratio of the number of outcomes that yield the desired result to the total number of possible outcomes. The desired result, balls 5, 7 and 9, together form a single 3-ball combination; a single outcome:
$$
1
$$ The total number of possible 3-ball combinations is:
$$
\frac{15!}{3!(15-3)!} = \frac{(15 \cdot 14 \cdot 13)}{6} = \frac{2730}{6} = 455
$$ Thus, the ratio described in (1), above, is $1\colon455$, and the answer is:
$$
\frac{1}{455}
$$ There are at least two alternative approaches that yield the same answer: Rather than using combinations, one might use permutations, in which case the numerator is the $3!$ permutations that yield the desired result, and the denominator  is the $\frac{15!}{(15-3)!}$ permutations that could occur in choosing 3 balls, which is equal to $\frac{6}{2730}$ = $\frac{1}{455}$. Or, one could use the product of the odds of each selection being a success:  $\frac{3}{15} \cdot \frac{2}{14} \cdot \frac{1}{13}$ = $\frac{6}{2730}$ = $\frac{1}{455}$. HOWEVER , my textbook says the solution is:
$$
\frac{3!}{\frac{15!}{3!(15 - 3)!}} = \frac{6}{455}
$$ The textbook's statement of the problem and solution is here: Lesson 13-3, Permutations and Combinations: Problem 6 .  I believe the textbook's solution compares a number of permutations in the numerator to a number of combinations in the denominator, which seems to me to be a matter of apples and oranges.","['permutations', 'combinations', 'combinatorics', 'probability']"
2912591,"If five cards are drawn randomly from an ordinary deck, what is the probability of drawing exactly three face cards?","From an ordinary deck of $52$ cards, five are drawn randomly. What is the probability of drawing exactly three face cards? (assume no replacement) I wrote the probability as a fraction with denominator $\binom{52}{5}$. For the numerator I wrote $\binom{12}{3}\binom{40}{2}$. My answer was approximately $.0660$.","['combinatorics', 'probability']"
2912624,"Show that $Cov(\bar{y},\hat{\beta_1})=0$","Show that $Cov(\bar{y},\hat{\beta_1})=0$ For those unfamiliar with statistics, Cov(A,B) refers to the covariance function. $\bar{y}$ refers to the average of the response (dependent variable). $\hat{\beta_1}$ refers to the estimator of the slope. The solution goes as follows: $Cov(\bar{y},\hat{\beta_1}) = Cov(\frac{\sum{y_i}}{n},\sum{c_iy_i})
$ Where $c_i = \frac{(x_i-\bar{x})}{S_{xx}}
$ And $S_{xx} = \sum{(x_i-\bar{x})^2}$ $Cov(\frac{\sum{y_i}}{n},\sum{c_iy_i}) = \frac{1}{n}Cov(\sum{y_i},\sum{c_iy_i})
$ Can we bring the $\sum{c_i}$ out of the covariance? If so, we would simply be left with $var(y_i)$.","['self-learning', 'statistics']"
2912638,"Find $a,b,c \in \Bbb N$ such that $n^3+a^3=b^3+c^3$","Is it true that for every $n \in \Bbb N$, there exist  $a,b,c \in \Bbb N$ satisfing
$$
n^3+a^3=b^3+c^3,
$$
where $\operatorname{gcd}(a,b,c)=1$ and $b,c\ne n$? I checked all positive integers less than 1000, it seems true but I don't know how to prove it. We have $n^3+(3 n^3 + 3 n^2 + 2 n)^3=(3 n^3 + 3 n^2 + 2 n + 1)^3 - (3 n^2 + 2 n + 1)^3$, but there is a negative number on the right-hand side.","['number-theory', 'diophantine-equations']"
2912679,Can anyone explain the intuition behind this proof that a rational number exists between any two real numbers?,"I'm new to proofs and I'm trying to understand how to approach them. I understand the proof itself, I'm just wondering what would prompt someone to come up with it and how I can build up enough mathematical intuition to do the same. If $a,b \in \mathbb{R}$ and $a < b$ , then $\exists x \in \mathbb{Q}$ s.t. $a < x < b$ . Proof: Since $\mathbb{Z}$ increases without bound, $\exists n \in \mathbb{Z}$ s.t. $\frac{1}{b-a}<n$ . Therefore $\frac{1}{n} < b-a$ . Choose $m \in \mathbb{Z}$ as large as possible satisfying $\frac{m}{n} \leq a$ . Then $a < \frac{m+1}{n}$ , and also $\frac{m+1}{n}=\frac{m}{n} + \frac{1}{n} < a + (b-a) = b$ $\therefore$ taking $x=\frac{m+1}{n} \in \mathbb{Q}$ satisfies $a < x < b$ . $\square$ I understand this is a direct proof of existence by finding an element in the domain of discourse that satisfies the conditional proposition. These are the proofs I seem to have the most trouble with because they involve a ""wildcard."" For instance, I get why we picked $\frac{m}{n} = a$ initially. It makes sense to start at the lower bound of the interval. But why did we, all of a sudden, decide to check $\frac{m+1}{n}$ . It's not like that's the ""next"" rational number (because there's no such thing). The choice to examine $\frac{m+1}{n}$ inspired the choice of $n$ and the whole proof just came together. But how, on earth, did the author cook up $\frac{m+1}{n}$ . Is there something that would explain his train of thought other than the fact that he's smarter than me? How would you approach this proof?","['proof-explanation', 'proof-writing', 'discrete-mathematics']"
2912710,"Proof of the open interval $(-1,1)$ with respect to $*$ is a group [duplicate]","This question already has answers here : Proving that set $(-1,1)$ with the operation $a*b=\frac{x+y}{1+xy}$ is an abelian group. (4 answers) Closed 5 years ago . Hi,so I  just can't get my head through the solution for 4th problem.I successfully proved associative law and when I wrote an equation for finding identity element I did get the same thing,but then I don't understand how they get $e(1 - x^2) = 0$. I always get $x + e = x^2e + x$. I'd appreciate if someone could explain. I understand that logically speaking, it should be $0$.","['algebra-precalculus', 'abstract-algebra']"
2912739,"What's the difference between the probability of the nth coin flip, vs. the probability of flipping all the coins and getting one outcome","So this is more of a problem with getting an intuitive understanding. I'm sure something like this has been asked before but I couldn't find it because I didn't have a clue what my problem was to start with. Here goes: Question: You have flipped a fair coin 9 times and it has landed on tails all 9 times in a row. What is the probability that the next flip will be tails? My understanding: If you've flipped 9 heads in a row, and are asked what is the probability that the next flip will be a head , that's not the same as asking what's the probability of flipping 10 heads in a row . I do understand that each coin flip is completely independent, and so will always be a 50-50 chance of heads or tails. The problem: Yet, I'm not quite sure why the probability of flipping 10 heads in a row is different from flipping a 10th head. Is it because we're only being asked to calculate the probability of that one event happening rather than the entire set of events? I think I may have inadvertently solved my own problem by asking this question because I had to think so much to ask it haha! I'm going to post it anyway just to ask if you can perhaps give me an example that will help me better grasp this? EDIT: I found another discussion here that asked a better, more illustrative question, and the Gambler's Fallacy, specifically the part on coin tosses, best explains the logical problem in my question. I'm placing this here in the hopes that it helps one of you future readers: Wikipedia: Gambler's Fallacy",['probability']
