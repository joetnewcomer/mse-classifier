question_id,title,body,tags
1455694,Instantaneous Rate of Change/Derivative,"I am having a bit of trouble with a question on my Calc HW. Given P(x)= 3x^2+3, estimate the Instantaneous rate of change at x=5. This is what I have so far. $$f(x+h) = 3(x+h)^2+3-(3x^2+3)$$
      $$= (3x+3h)(x+h)+3-(3x^2+3)$$
       $$= 3x^2+3xh+3xh+3h^2+3-(3x^2+3)$$
       $$= 3x^2+6xh+3h^2+3-3x^2-3$$
      $$ = (6xh+3h^2/h)$$
      $$ = h(6x+3h)/h$$
       eliminate $h$ and left with $6x+3h$ I know this is incorrect, but don't know where I went wrong. I know for the last step, you have to plug in 5 to x and solve to get the Rate of change.","['calculus', 'derivatives']"
1455698,Proving that $\sum_{n=0}^{\infty }\frac{3(n!)^2}{(2n+2)!}=\sum_{n=1}^{\infty }\frac{1}{n^2}=\frac{\pi ^2}{6}$,"Proving that $$\sum_{n=0}^{\infty }\frac{3(n!)^2}{(2n+2)!}=\sum_{n=1}^{\infty }\frac{1}{n^2}=\frac{\pi ^2}{6}$$ I know the proving of second series which is very famous series to give us $\zeta(2)$, but I dont know how to prove the first series which is faster than second series, any help","['sequences-and-series', 'zeta-functions']"
1455711,How to show the commutator of $SO(n)$ is itself?,"I am not very familiar with Lie groups, but I want to show that the commutator subgroup of $SO(3)$ is itself. I have looked up many different sources, and it seems to me that almost all of them require some notion of Lie algebra, so I am wondering if it is possible to show this without much knowledge of Lie groups.","['abstract-algebra', 'lie-groups']"
1455735,"Generating the borel $\sigma$-algebra of $C[0,\infty)$ by the cylinder sets","This is related with this post: Set $M := \{f \colon [0,\infty) \to \mathbb{R} \ |\  f \text{ continuous }\}$ with the metric of the convergence in compact sets given by 
$$d(f,g) = \sum_{n=1}^\infty \frac{\left(\max_{x\in [0,n]} |f(x)-g(x)|\right)\wedge 1}{2^n}$$ Then the Borel $\sigma$-algebra $B(M)$ is generated by the cylinder sets $C(A,t_1,\dots,t_n):=\{f\in M \colon (f(t_1),\dots,f(t_n)) \in A\}$ with $n \in \mathbb{N}$ and $A \in B(\mathbb{R}^n)$ I try to use the technique used in the post above but I could only prove that for $\varepsilon < 1$ $$B(f,\varepsilon) = \bigcup_{m\in \mathbb{N}} \bigcap_{n\in \mathbb{N}} \bigcap_{t \in [0,n]\cap \mathbb{Q}} \pi_t^{-1}\left(\left( f(t) -\left(\frac{1-2^{-m}}{2}\right)\varepsilon \ , \ f(t) +\left(\frac{1-2^{-m}}{2}\right)\varepsilon\right)\right)$$ The $ \varepsilon <1 $ condition comes from the fact that if $g \in B(f,\varepsilon)$ then $\left(\max_{x\in [0,n]} |f(x)-g(x)|\right)\wedge 1 \leq \varepsilon$ and this implies
$\max_{x\in [0,n]} |f(x)-g(x)| < \varepsilon$ only if  $\epsilon < 1$. Any help will be appreciated. EDIT: the equation above is not true, I don't know how to extend the technique on the other post for this case.","['functional-analysis', 'measure-theory']"
1455754,Prove that there must be two distinct integers in $A$ whose sum is $104$.,"Let A be any set of $20$ distinct integers chosen from the arithmetic
  progression ${1,4,7,...,100}$. Prove that there must be two distinct
  integers in $A$ whose sum is $104$. Define $A=\{1+3i\}_{i=0}^{33}$. I know that if two distinct integers $a,b \in A$ are such that $104=a+b=(1+3i)+(1+3j)$, then $i+j=34$ with $0<i,j\leq33$ and $i \not= j$. I think we can play with elements parity or even with the fact that if $i = j$, then $i = 17$ and $(3i + 1) = 52$. However, I am not able to advance further into the question. Are there someone who could help me complete the problem?",['discrete-mathematics']
1455756,Proving a function is a metric,"Define a function $d : \mathbb{Z × Z \to R}$ by setting $d(x, x) = 0$ for all $x ∈ \mathbb{Z}$, and if $x \ne y$, setting $d(x, y) = 3^{-k}$ where $3^k$ is the largest power of $3$ dividing $x − y$. Prove that $d$ is a metric on $\mathbb{Z}$. I've proven the first three parts needed to show it's a metric, but I can't figure out how to prove the triangle inequality for the function.  I know I have to show $d(x,z) \le d(x,y) + d(y,z)$, but I'm confused about how I do that for this function.","['analysis', 'real-analysis']"
1455761,How is the determinant related to the inverse of matrix?,"Whenever I needed to find the inverse of a matrix, I was told to check if its determinant is not zero. However, once I directly applied the Gauss-Jordan's method for finding the inverse of matrix whose determinant was zero. The inverse matrix that I got looked pretty normal like any other (if there wasn't a mistake). I want to know how does the determinant of the matrix is related to inverse of matrix or why is that if determinant is zero then inverse doesn't exist? What exactly is inverse?","['inverse', 'commutative-algebra', 'matrices', 'abstract-algebra', 'linear-algebra']"
1455778,Prove $\sum_{i=1}^n i! \cdot i = (n+1)! - 1$?,"Prove the summation: 
$$\sum_{i=1}^n i! \cdot i = (n+1)! - 1$$
using induction. base case: 
$n=1$:
\begin{align*}
\sum_{i=1}^1 i! \cdot i &= (1+1)! - 1 \\
1 &= 2 - 1 \\
1 &= 1
\end{align*} This is a question from my test review packet, currently have the base case completed and I am a bit lost on where to go from there. Any help/hints are appreciated.","['factorial', 'summation', 'induction', 'discrete-mathematics']"
1455789,Finding a function (?) and computing its definite integral,"So I've come across this exercise from one of my old highschool textbooks: $$\text{If}\ 2f\bigg(\frac{x-2}{x+1}\bigg) +f\bigg(\frac{x+1}{x-2}\bigg) = x$$ Considering this, find : $$\int_{\frac{1}{2}}^{\frac{3}{4}}f(x)dx$$ To be honest, I've been at it for quite a few minutes now and I can't really find a way to crack it and it's kinda embarassing. I've done a little plot to help me guess the function, but that got me nowhere. I'm guessing we should use the fact that $\frac{x-2}{x+1}$ and $\frac{x+1}{x-2}$ are inverses, but I couldn't manage to solve for $f(x)$ . Any ideas?","['functional-equations', 'calculus', 'definite-integrals', 'integration']"
1455814,"What is the ""Tychonoffication""?","In this link: https://mathoverflow.net/questions/23940/why-free-topological-groups-on-tychonoff-spaces I read the following: Let $X$ be a topological space. The Tychonoffication $Y$ of $X$ is the quotient of $X$ by the relation $x\sim y$ iff $f(x)=f(y)$ for all continuous $f:X\to\mathbb{R}$, and we give $Y$ the weak topology induced by all these real-vauled maps. This makes $Y$ a Tychonoff  space that satisfies the universal property: any continuous map from $X$ to a Tychonoff space factors uniquely through $Y$. I don't understand two things: 1) The weak topology of a family of functions $f_i:X\to X_i$ is the topology over $X$ which has the subbase $\{f^{-1}_i(U):...\}$. How can the real-valued functions $f:X\to\mathbb{R}$ give a topology over $Y$? 2) What does it mean ""factors through $Y$""? Thanks.",['general-topology']
1455819,What is a norm topology in functional analysis?,"I am currently reading up about norm topology , I have a background in functional analysis but I do not know anything about topology, aside from that topology is a collection of open sets with some properties on a set. For example: The norm topology or uniform topology or uniform operator topology is defined by the usual norm $\|x\|$ on $B(H)$. It is stronger than all the other topologies below. What does it mean for a norm to ""define"" a topology? A norm is a function which measures distance, a topology is a collection of open sets, what is the intersection between these two concepts? Can someone who is conversant in both fields please elaborate on what it means for an algebra of bounded linear operators to have a topology, specifically what a norm topology is about?","['definition', 'terminology', 'functional-analysis', 'general-topology', 'normed-spaces']"
1455826,"Why can there be an infinite difference between two functions as x grows large, but a ratio of 1?","I learned in grade school that the closer $a$ and $b$ are to one another, the closer $\frac{a}{b}$ is going to be to $1$. For example, $\frac{3}{\pi}$ is pretty close to 1, and $\frac{10^{100}}{42}$ isn't even close to 1. So, why is: $$\lim_{x\to\infty} \frac{x^{2}}{x^{2}+x} = 1$$ But: $$\lim_{x\to\infty}[(x^2+x)-(x^2)] = \infty$$ ? Seems pretty counterintuitive. What's going on here?","['limits', 'soft-question']"
1455846,How can I map a wedge onto the unit disk?,"The wedge is given by $0< \arg(z) < \alpha \pi$, for $0 < \alpha <1$. If I use the complex logarithm, principal branch $-\pi < 0 \le \pi$, then $$z \mapsto w = \operatorname{Log}(z)$$
$$ = \ln|z|+ i\arg(z)$$
$$= u+iv$$ so that $0<v<\alpha\pi$, which shows that the wedge maps onto a semi-infinite, horizontal half-strip, with height ranging from $0$ to $\alpha \pi$. What can I do next?  Is the Logarithm mapping even a good start?  I usually like it, when I see a circular region that I have to start with, but not for very great reasons other than that I know what the mapping does to a circular region... Thanks,","['conformal-geometry', 'complex-analysis', 'geometry', 'linear-transformations', 'mobius-transformation']"
1455916,"Let $(s_n)$ be a sequence of nonnegative numbers, and $\sigma_n=\frac{1}{n}(s_1+s_2+\cdots +s_n)$. Show that $\liminf s_n \le \liminf \sigma_n$.","Let $(s_n)$ be a sequence of nonnegative numbers, and for each $n$ define $\sigma_n=\frac{1}{n}(s_1+s_2+\cdots +s_n)$. Show that $\liminf s_n \le \liminf \sigma_n$. Actually, I showed $\limsup \sigma_n \le \limsup s_n$ in the following way. For any $n \gt M \gt N$, we can get the following inequality, $\sup \{\sigma_n: n\gt M\}\le \frac{1}{M}(s_1+s_2+\cdots +s_N)+\sup\{s_n:n\gt N\}.$ So first taking the limit as $M\to \infty$ then as $N \to \infty$, we get the inequality. However, this method does not work out for the $\liminf$ case, since the above inequality was derived using the fact that $1/n \lt 1/M$. How can I show the inequality for the $\liminf$ case? I would greatly appreciate any suggestions or solutions.","['calculus', 'limits', 'real-analysis', 'analysis', 'limsup-and-liminf']"
1455921,"what does it mean ""converge in probability to a random variable""?","In statistics, a sequence of random variables $X_n$ is said to converge to a random $X$ in probability if $P(|X_n - X| > \epsilon ) \to 0 $. Also,  $X_n$ is said to converge to a constant c if $P(|X_n - c| > \epsilon ) \to 0 $. I can understand the argument with the constant $c$ which means that $X_n$ gets more and more concentrated on $c$, as $n \to \infty$. However, I don't quite understand the ""converge to a r.v."" argument. It seems to me that $X$ has to be degenerated for this to hold true. But the textbook I had in my hand does not elaborate on this, so I am confused. Can you explain it ? or by showing some simple examples?","['probability', 'statistics', 'law-of-large-numbers', 'convergence-divergence']"
1455932,The intersection of an infinite number of prime ideals in a ring of integers,"Let $\mathcal{O}$ be the ring of integers of a number field, $\{\mathfrak{p}_i,\,i \in \mathbb{N}\}$ a sequence of two-by-two pairwise distinct prime ideals. Does it follow that$$\bigcap_i \mathfrak{p}_i = \{0\}?$$","['abstract-algebra', 'algebraic-number-theory', 'commutative-algebra']"
1455944,Mapping $\mathbb{Z}$ onto $\mathbb{Q}$,"There is a problem that calls for the mapping the set of all the positive integers to the set of all positive rational numbers in order to prove they have the same cardinality. I know more the common answer of mapping it this way: Map $\mathbb{Q}$ like a table of ℤ x ℤ: \begin{array}{rl}
  &  &1    &2  &3   &4   &5 \\
\\   
 1 & &1/1  &2/1 &3/1 &4/1 &5/1  \\
2 & &1/2  &2/2 &3/2 &4/2 &5/2 \\
\end{array} Then you show the cardinality of that as in {1/1,2/1,3/1,4/1,/5/1,1/2/3/2 ... } is the same as the carnality of {1,2,3,4,5,6} However, my first idea was much different. Now, I have not yet taken very high level courses, this is my intro to set theory. But why couldn't I do something like this: The first element in the set $\mathbb{Q}$ is: ε, where ε is an infinitesimal approaching 0. 
 the second element of $\mathbb{Q}$ is  ε, where  ε is an infinitesimal approaching the first member, and so forth. so $$S(n) = \sum_{i=1}^{n} ε_i$$ where n ∈ $\mathbb{Z}$ This was the essence of my idea. I am sure it is probably way of base. But could someone explain why? EDIT: I think I understand where I was going wrong. Defining the infinitesimal here is non standard, and I can't think of it like a limit. The sum I use is also not correct. $$S = \{\varepsilon_i : i \in \mathbb Z\}$$ is a far better way to represent what I am trying to say. Then my question boils down to asking if $$S = \{\varepsilon_i : i \in \mathbb Z\}$$ is equal to the set of the positive rational numbers. In which case, no. Because the infinitesimal cant be used in this way. @graydad mentioned another cool / non-standard way to map $\mathbb{Z}$ onto $\mathbb{Q}$ Let $f:\Bbb{Q}^+ \to \left\{2^n3^m :n,m \in \Bbb{Z}^+\right\}$ be defined by $f(n/m) = 2^n3^m$ where $n/m$ is in lowest reduced form. Then by the F.T. of Arithmetic we know $2^n3^m \neq 2^l3^p$ whenever $n/m \neq l/p$. You can show $f$ is a bijection and that the set $\left\{2^n3^m :n,m \in \Bbb{Z}^+\right\}$ has the same cardinality as $\Bbb{Z}$. That is pretty interesting!","['elementary-set-theory', 'infinitesimals']"
1455965,$\mathcal{A}+K$ is norm-closed where $\mathcal{A}$ is a $C^*$-algebra and $K$ is the compact operators.,"Let $\mathcal{A}\subset B(H)$ be a unital $C^*$-algebra and let $K$ be the closed ideal of compact operators. I need to show that $\mathcal{A}+K$ is also a $C^*$-subalgebra of $B(H)$. I am stuck at proving that the space $\mathcal{A}+K$ is norm-closed. I begin with a a sequence $\{B_n\}_{n=1}^{\infty}$ in $\mathcal{A}+K$ with $B_n\rightarrow B$, for some operator $B\in B(H)$. Now, for each $n\in\mathbb{N}$, $B_n=A_n+K_n$, where $A_n\in \mathcal{A}$ and $K_n\in K$. And from here I'm lost. Is there some kind of Bolzano-Weierstrass property for compact operators? Hints are welcome.","['compact-operators', 'operator-algebras', 'operator-theory', 'functional-analysis']"
1455968,"Counting numbers in a sequence - explain ""Add $1$ before you're done"" rule (fencepost error)","I'm studying for the GRE, and my study book uses a rule that it never justifies for counting numbers in a sequence: ""Add $1$ before you're done."" For example, how many multiples of $3$ are between $250$ and $350$? My study book says: $$
348 - 252 = 96
$$ $$
\dfrac{96}{3} = 32
$$ Now ""add one before you're done"": $$
32 + 1 = 33
$$ I follow the first few steps. Start and end with $252$ and $348$ because they are the first and last multiples of $3$ within the range, respectively. We divide by $3$ to count only the multiples of $3.$ But why add $1$?","['gre-exam', 'discrete-mathematics']"
1455971,Behavior of the null set?,"I am trying to work out problems to understand the null set beyond the notion of, ""an empty box"". Therefore, I have tried to work out some problems concerning set operations involving the null set. I was wondering, is my logic correct here? $\emptyset$ $\in$ $\emptyset$ This is false because the empty set is defined as having no elements. This can be expressed via contradiction as $if \; empty \; \rightarrow \; no \; elements$, which yields as $T \rightarrow F$, which is false. $\{\{\emptyset\}\} \subseteq \{\emptyset,\{\emptyset,\{\emptyset\}\}\}$ This would be false because the two sets in $\{\emptyset,\{\emptyset,\{\emptyset\}\}\}$ are $\emptyset$ and $\{\{\emptyset,\{\emptyset\}\}\\$ $\{\emptyset\} \; \cap \{\emptyset,\{\{\emptyset\}\}\} = \emptyset$ This would be true, because the sets of $\{\emptyset,\{\{\emptyset\}\}\}$ include $\emptyset$ and $\{\{\emptyset\}\}$, which don't equal $\{\emptyset\}$. $\emptyset \subseteq \emptyset$ This would be true because the empty set is a subset of all sets.",['elementary-set-theory']
1455975,There is no infinite descending chain of ordinals,"I'm currently reading a set theory book and it uses the fact that There is not infinite descending chain of ordinals. But I can't find the proof. (And also I remember Topology by Munkres uses this fact somewhere) 
I don't know how to prove this. May I get a help?","['ordinals', 'elementary-set-theory']"
1455995,Show that $2 \int f^2 \leq \int |f'| \cdot \int |f|$,"Let $f(x)$ be a continuously differentiable function defined on closed interval $[0, 1]$ for which$$\int_0^1 f(x)\,dx = 0.$$How do I show that$$2 \int_0^1 f(x)^2\,dx \le \int_0^1 |f'(x)|\,dx \cdot \int_0^1 |f(x)|\,dx?$$","['contest-math', 'calculus', 'real-analysis', 'integration']"
1456002,Special case of Schur-Zassenhaus theorem,"The theorem of Schur-Zassenhaus says that if $G$ is a finite group, and $H$ is a normal subgroup, such that $|H|$ and $|G/H|$ are relatively prime, then $G$ contains a subgroup $K$ of order equal to $|G/H|$. (this subgroup $K$ will then be obviously a complement of $H$, i.e. $K\cap H=1$) If we see the proof, then the non-trivial part comes in the case when $H$ is abelian. Suppose we put one more extra condition: $H$ is central in $G$. Then is it easy to prove the theorem without (co)homological methods ? Putting this a a question: Let $G$ be a finite group and $H$ be a subgroup in center of $G$ such that $|H|$ and $|G/H|$ are co-prime. Then prove that $G$ contains a subgroup of order equal to $|G/H|$.","['group-theory', 'finite-groups']"
1456014,Harmonic Oscillators: Differential Equations,"The book being used for this course is Differential Equations, Dynamical Systems, and an Introduction to Chaos by Morris W. Hirsch. The question is as follows. Suppose there are two masses $m_1$ and $m_2$ attached to springs and walls. The springs connecting $m_j$ to the walls both have spring constants $k_1$, while the springs connecting $m_1$ and $m_2$ has spring constant $k_2$. This coupling means that the motion of either mass affects the behavior of the other. Let $x_j$ denote the displacement of each mass from its rest position, and assume that both masses are equal to 1. The differential equation for these coupled oscillators are then given by
  \begin{eqnarray}
x_1^{""}& = & -(k_1+k_2)x_1+k_2x_2\\
x_2^{""}& = & k_2x_1-(k_1+k_2)x_2\\
\end{eqnarray} Write these equations as a first-order linear system. Determine the eigenvalues and eigenvectors of the corresponding matrix. Find the general solution. Let $\omega_1= \sqrt{k_1}$ and $\omega_2=\sqrt{k_1+2k_2}$. What can be said about the periodicity  of solutions relative to the $\omega_j$? Prove this. Now I just want to focus on the first part: to see if my linear system is correct. I first start by introducing the new variable $y_j=x^{'}_j$ for $j = 1,2$, so that the equations can be written as a system. 
\begin{eqnarray}
x^{'}_1 & =& y_1\\
y^{'}_1 & =& -(k_1+k_2)x_1+k_2x_2\\
x^{'}_2 & =& y_2\\
y^{'}_2 & =& k_2x_1-(k_1+k_2)x_2 
\end{eqnarray}
In matrix form, this system is $X^{'}=AX$ where $X=(x_1,y_1,x_2,y_2)$ and 
\begin{align}
A & = & \begin{bmatrix}
0 & 1&0 &0 \\
-(k_1+k_2)& 0& k_2& 0\\
0& 0& 0& 1\\
k_2& 0& -(k_1+k_2)&0\\
\end{bmatrix}
\end{align} Do I have the right linear system? Thanks for your time.","['harmonic-analysis', 'ordinary-differential-equations']"
1456022,How can you prove $\binom {2n}{2} = 2\binom{n}{2} + n^2$? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Is there a way one can prove that this is true: ${2n \choose 2} = 2{n\choose 2} + n^2$ ? I am thinking it may involve binomial theorem.","['binomial-theorem', 'discrete-mathematics']"
1456030,Calculating number of tile sequences,"My daughter (aged 12) came to me with the problem below. I was able to help her to some extent but I could not see an age-appropriate solution. That is, I could imagine solutions involving factorials / combinations or writing a computer program. However she is in Year 7 at school and I could not see how to solve it with that level of knowledge. We settled on a brute-force solution but did not complete it as it would take too long. So can anyone solve this with a simple, logical algorithm using knowledge / techniques that a student just starting high school would be familiar with? Say you have a set of black (B) and white (W) tiles - identical apart from colour. You arrange the tiles in various sequences, eg. B, BW, WWBW, etc. A sequence is considered to be significant if all of its sub-sequences of white tiles are even in length. Thus the following are significant: WW, BBB, WWWWBB; while the following are not: W, BWWW, BWWBBBW. Note: a sequence of length zero would appear to be deemed even. There were a few simple questions we could solve, eg. how many significant sequences of length 1, 2, 3 and 4 are there. Enumerate them. But the question that stumped us was: how many significant sequences of length 20 are there. We could not extrapolate easily from the earlier questions. PS: I wasn't sure what tags to use. Please update as you see fit.","['combinations', 'sequences-and-series', 'induction', 'fibonacci-numbers']"
1456040,$4$ integers are randomly selected from the numbers from $1$ to $10$. The chance that there are at least two successive numbers among those $4$ is,$4$ integers are randomly selected from the numbers from $1$ to $10$.  The chance that there are atleast two successive numbers among those $4$ selected is $(A)\frac{5}{6}\hspace{1cm}(B)\frac{3}{4}\hspace{1cm}(C)\frac{2}{3}\hspace{1cm}(D)\frac{1}{2}\hspace{1cm}$ I calculated answer as $\frac{24}{\binom{10}{4}}$ but this wrong.  Please help me find the right answer.,"['probability', 'combinatorics']"
1456041,Joint density of first k order statistics,"I need to compute the joint density of the first $k$($<n$) order statistics I was trying to do it by getting the marginal density function: $$\int \cdots \int f(x_1)\cdots f(x_n)n!dx_{k+1}\cdots dx_{n}$$ but I dont know my interval of integration; is it $(-\infty,\infty)$ for each integral? I would really appreciate if you can help me with this problem","['order-statistics', 'probability', 'statistics', 'probability-distributions']"
1456045,Eigenvalues and eigenvectors of antidiagonal block matrix,"I have $2$ square matrices $A_m$ and $B_m$ which are symmetric and of size $m\times m$ . And the 3rd matrix is $$C = \begin{bmatrix} 0 & A \\ B & 0\end{bmatrix}$$ Now, I would like to calculate the eigenvalues and eigenvectors of matrix $C$ . How can I get it? Or how does it related to the eigenvalues and eigenvectors of $A$ and $B$ ? Thank you very much in advance!","['eigenvalues-eigenvectors', 'block-matrices', 'linear-algebra', 'matrices']"
1456051,What is the coefficient of $x^3 y^4$ in the expansion of $ (2x-y+5)^8$,I was thinking of doing $\binom{8}{4}$ but not sure if right.,"['binomial-theorem', 'binomial-coefficients', 'discrete-mathematics']"
1456061,Distribution of density function $p(x)$ and $-\log p(x)$,"Suppose $x$ is a random variable with a distribution that is absolutely continuous with respect to Lebesgue measure, such that it has a density $p(x)$. $p(x)$ is a function of a random variable, as is $-\log p(x)$. Are there any general results about what the distributions of $p(x)$ and $-\log p(x)$ are without specifying any further details?","['probability-theory', 'probability-distributions', 'measure-theory', 'statistics', 'probability']"
1456064,Show certain sums go to zero in order to use a weak law of large numbers,"I am working on a problem where I need to show two limits in order to be able to apply a weak law of large numbers. Here is what I know: $p_k=\frac{1}{2^kk(k+1)}, k=1,2,\dots$ and $p_0=1-\sum_{k\geq 1} p_k$. Now let $X_1, X_2,\dots$ be i.i.d. with $P(X_n=-1)=p_0$ and $P(X_n=2^k -1)=p_k$ for $k\geq 1$. Let $S_n=X_1 + \dots + X_n$, let $b_n=n/\log_2(n),$ let $\overline{X_{n,k}}=X_k 1_{\{|X_k|\leq b_n\}}$, and finally let $a_n= \sum_{k=1}^n E(\overline{X_{n,k}}).$ What I would like to show is the following: I) $b_n^{-2}\sum_{k=1}^n E \left(\overline{X_{n,k}}^2\right)\rightarrow 0$, II) $\frac{a_n}{b_n}\rightarrow -1.$ Here is the work I have so far for I):
$$
b_n^{-2}\sum_{k=1}^n E \left(\overline{X_{n,k}}^2\right)=b_n^{-2}\sum_{k=1}^n E \left(X_1 1_{\{|X_1|\leq b_n\}}^2\right)=b_n^{-2}nE \left(X_1 1_{\{|X_1|\leq b_n\}}^2\right)
$$
$$
\leq b_n^{-2}n\sum_{k=0}^{\infty}P\left(X_1 1_{\{|X_1|\leq b_n\}}^2 > k\right)=b_n^{-2}n\sum_{0\leq k \leq b_n}P\left(X_1^2 > k\right)
$$
$$
\leq b_n^{-2}n\left[2+\sum_{2\leq k \leq b_n} P\left( X_1^2 >k \right)\right].
$$ This is where I get stuck. Here is the work I have so far for II):
$$
a_n=\sum_{k=1}^n E\left(\overline{X_{n,k}} \right)=nE \left(X_1 1_{\{|X_1|\leq b_n\}}\right).
$$
So I can bound $a_n$ between 
$$
n\sum_{k=1}^{\infty} P\left(X_1 1_{\{|X_1|\leq b_n\}}\geq k\right)\leq a_n \geq n\sum_{k=0}^{\infty} P\left(X_1 1_{\{|X_1|\leq b_n\}}>k \right).
$$
Then I get stuck here. I am really in need of some help on this problem. Thank you. UPDATE:
  I used a different approach to show I) and succeeded. If instead of bounding the expectation from above like I did you actually calculate it you are able to show that it goes to $0$. Now this is what I have so far in my attempt to show II)
  $$
-1\leq a_n=nE \left(X_1 1_{\{|X_1|\leq b_n\}}\right)=n\left(-p_0 + \sum_{1\leq k\leq b_n}(2^k -1)\frac{1}{2^k k (k+1)}\right).
$$
  Now I somehow want to either show that this goes to $-1$ or it is bounded above by something that goes to $-1$.","['probability-theory', 'probability']"
1456068,"Let $A,B,C,D \subseteq X$ Show that $(A \setminus B ) \bigtriangleup (C \setminus D) \subseteq (A \bigtriangleup C) \cup (B \bigtriangleup D)$","Let $A,B,C,D \subseteq X$
Show that 
 $(A \setminus B ) \bigtriangleup (C \setminus D) \subseteq (A \bigtriangleup C) \cup (B \bigtriangleup D)$ My advances $(A \setminus B ) \bigtriangleup (C \setminus D) $ $\rightarrow  \left [ (A \setminus B ) \cap (C \cap D^{c} )^{c} \right ] \cup$
 $\left [ (C \setminus D ) \cap ( A \cap B^{c} )^{c} \right ]   $ $\rightarrow \left [ (A \cap B^{c}) \cap (C \cap D^{c} )^{c} \right ] \cup$
   $\left [ (C \cap D ^{c}) \cap ( A \cap B^{c} )^{c} \right ]  $ $\rightarrow \left [ (A \cap B^{c}) \cap (C^{c} \cup D) \right] \cup \left [ (C \cap D ^{c}) \cap ( A^{c} \cup B ) \right ]   $ I can't conclude.. I Think, that is necessary find ... $(A \bigtriangleup C ) \cup (B \bigtriangleup D) $ $=\left [ (A \setminus C) \cup (C \setminus  A ) \right ] \cup \left [ (B \setminus  D) \cup (D  \setminus B ) \right ]   $ $\rightarrow \left [ (A \cap C^{c}) \cup (C \cap A ^{c}) \right ] \cup \left [ (B \cap D ^{c}) \cup ( D \cap B^{c}) \right ]   $ but I can't find their relationship..... I need help. Please!!!",['elementary-set-theory']
1456074,Transexponential Functions,"Recall that $\exp(1,x) = e^x$ and $\exp(n+1,x) = e^{\exp(n,x)}$. Recall that $f(x)$ is transexponential if $f(x)$ is eventually greater than $\exp(n,x)$ $\forall n \in \mathbb{N}$ I am looking for a (general) reference on these types of functions (or any paper about these functions, or maybe even a few pages of a textbook). Note: I have tagged model theory (and now logic) since the only context in which I have encountered transexponential functions is in relation to Wilkie's Conjecture (and so model theorists know about these functions). Please note that I am looking for a reference about transexponential function in general, and not a link to an exposition of Wilkie's Conjecture. Note 2: I have added a bounty to this question. I am trying to get my hands dirty with transexponential functions from $\mathbb{R}^+ \to \mathbb{R}^+$. The most helpful answer would be one where I could ""in some sense"" compute the derivative (locally). Please do not answer with ""piecewise continuous segments"" + bump functions.","['logic', 'reference-request', 'real-analysis', 'model-theory']"
1456080,Help with verifying integral inequality.,"I am looking at problems from a released Fall 14 mock exam. The question in particular is number 2: Let $f$ be a continuous function in $[0,1]$ satisfying the condition: $$ \int_x^1 f(t) dt \geq \frac{1-x^2}{2}$$ for $x \in [0,1]$ Prove that: $$\int_0^1 |f(x)|^2 dx \geq \int_0^1 xf(x)dx$$ This is what I have come up with so far: First off we can ""evaluate"" the integral from $0$ to $1$ : $$\int_0^1 f(t) dt \geq \frac{1-0^2}{2} = \frac{1}{2}$$ Next we know from the Cauchy-Schwartz inequality: $$\left|\int_0^1 f(x) dx\right|^2 \leq \int_0^1 |f(x)|^2 dx$$ So: $$\int_0^1 |f(x)|^2 dx \geq \frac{1}{4}$$ Now for the other equation. I used integration by parts: $$\int_0^1 xf(x)dx = xF(x) - \int_0^1 F(x) dx $$ $$\int_0^1 xf(x) dx \geq 1 \cdot \frac{1}{2} - \int_0^1 F(x) dx $$ But notice that: $$F(x)|_0^1 \geq \frac{1}{2}$$ So: $$\int_0^1 xf(x) dx \geq \frac{1}{2} - \frac{1}{2}$$ $$\int_0^1 xf(x) dx \geq 0$$ Which seems to be right so far (I could of course be wrong). I don't have enough info to close anything out, but it seems to be pointing in the right direction. Any further hints or corrections would be greatly appreciated.",['calculus']
1456085,"Is cantor set open when intersecting a closed interval $[0,\frac{1}{3}]$ (in cantor set)？","In our class, our professor said cantor set is clopen (both closed and open). One argument is that the interior of a cantor set intersecting with a closed interval, say $A = \Delta \cap [0,1/3]$ whose interior is $A$ itself. From my point of view, the interior should be empty. Is there any difference if we considered $A$ as a subset of $\Delta$(cantor set itself) rather than $\mathbb{R}$?","['elementary-set-theory', 'real-analysis', 'cantor-set', 'general-topology', 'analysis']"
1456100,"Knapp (Basic Algebra) Prop 8.52, error?","The above proposition says: Let $R$ be a Noetherian ring and let $I$ and $P$ be ideals of $R$ where $P$ is a prime ideal. If $IP=I$, then $I=0$. I feel that this is false. After passing to localization at $P$ and using Nakayama, we can easily get $S^{-1}I=0$ ($S=R-P$), from which we can conclude that $sI=0$ for some $s\in S$. How can we conclude that $I=0$? Alternatively, we can use Corollary 2.5 of Atiyah and Macdonald to directly conclude that $xI=0$, where $x=1+p$ with $p\in P$. The proof given there says since $I$ is a subset of $S^{-1}I$, $I=0$. This is false unless $R$ is a domain, isn't it?","['commutative-algebra', 'ideals', 'abstract-algebra', 'maximal-and-prime-ideals', 'noetherian']"
1456101,Finding a branch for a square root functions,"How can we define a branch for $\sqrt{ 1 + \sqrt{z} } $? I know a branch for $\sqrt{z}$ is the negative real axis. But, how can we deal with the square root of $1+ \sqrt{z} $ ?",['complex-analysis']
1456105,"How to guess whether $X>Y$ or $X<Y$ from $X$ or $Y$ when the distribution of $(X,Y)$ is unknown","Somebody chooses two real random variables, $X$ and $Y$, and writes them on two
  sheets of paper. The distribution of the pair $(X,Y)$ is unknown to you, but you do know that $P(X=Y)=0$. You toss a fair coin (the toss is of course independent of $(X,Y)$) to choose one of
  the sheets, which is then handed to you, so that you see the number on it. Call this random number $W$ and the other number, still unknown to you, $Z$. Your task is to guess whether $W$ is bigger than $Z$ or not. You can generate independent Uniform$([0,1])$ random variables at will, so your strategy could be random. a) Show that if your guess is always that $W>Z,$ then the probability of being correct is exactly $1/2$. b) Exhibit a strategy for which the probability of being correct is $1/2+\epsilon$, for some $\epsilon >0.$ c) Assume that you know that $X$ and $Y$ will be chosen among integers from 0 to 5. Find, with proof, the largest $\epsilon$ that works for all such distributions. I know how to do a) and b). I am incredibly confused how to do c) and I have no idea. I will write out my solutions to a) and b) and I hope that somebody can help me with c). Solution to a): $$
P(W>Z)=P(W=X, X>Y)+P(W=Y,Y>X)
$$
  $$
P(W=X)P(X>Y)+P(W=Y)P(Y>X)=1/2\left( P(X>Y)+P(Y>X) \right)=1/2.
$$ Now here is my solution to b). Solution to b): Let $G$ be an exponential random variable with expectation 1. You can obtain this as $-\log(U)$ where $U$ is uniform on $[0,1]$. Guess that $W>Z$ if $W>G$ and that $W<Z$ if $W<G$. Then we have that:
  $$
P(\text{guess correct})=P(W>Z,W>G)+P(W<Z,W<G)
$$
  $$
=\frac{1}{2}\left[P(X>Y,X>G)+P(Y>X,Y>G)+P(X<Y,X<G)+P(Y<X,Y<G)\right]
$$
  $$
=\frac{1}{2}\left[P(X>Y)+P(X>Y,Y<G<X)+P(X<Y)+P(X<Y,X<G<Y) \right]
$$
  $$
=\frac{1}{2} + \frac{1}{2}P(G \text{ between } X \text{ and } Y).
$$ Any help is greatly appreciated. Thanks!","['probability-theory', 'probability-distributions']"
1456115,Principal Ultrafilters on natural numbers,"Let $E$ be a countable set of subsets of $\mathbb{N} $. Show that the filter generated by $E$ cannot be a non-principal ultrafilter. 
My idea of solution is: Let $D$ be the filter generated by $E$. $D$ is not principal $\iff$  for all $x\in\mathbb{N}$   $\{x\}\notin D $  $\iff \mathbb{N} \setminus \{x\}\in D$ if and only if for all $m\in\mathbb{N} $ and for all $X_1,\dots,X_m\in E$  $X_1\bigcap\dots\bigcap X_m\subset \mathbb{N} \setminus \{x\}$. I'd like to conclude that $E$ is not countable. Is that possible?","['elementary-set-theory', 'filters', 'logic']"
1456125,How to prove $(\frac 1n)^n+(\frac 2n)^n+\cdots+(\frac nn)^n\geqslant\frac{3n+1}{2n+2}$,"How to prove $$
  \Bigl(\dfrac 1n\Bigr)^n + \Bigl(\frac 2n\Bigr)^n + \cdots + 
  \Bigl(\frac nn\Bigr)^n \geqslant \frac{3n+1}{2n+2}  \qquad (n\in\mathbb{N})
$$ I tried: let $f(x)=x^n$ and $f''(x)\geqslant 0$ for $n>1$, let $x_i=\frac in$, and we have $$
  \sum f(x_i)\geqslant nf\biggl(\frac{\sum x_i}{n}\biggr).
$$ But the $RHS<\dfrac{3n+1}{2n+2}$. So it doesn't work. Could someone give me a neat proof? Thanks!","['sequences-and-series', 'calculus', 'inequality']"
1456170,Why do the Borwein integrals stop being $\frac{\pi}{2}$?,"I just received the book ""single digits - In praise of Small Numbers"" by Marc Chamberland. In this book, he gives an interesting integral $$\displaystyle \int_0^\infty \dfrac{\sin x}{x} = \dfrac{\pi}{2}$$ $$\displaystyle \int_0^\infty \dfrac{\sin(x)}{x}\dfrac{\sin(x/3)}{x/3} = \dfrac{\pi}{2}$$ $$\displaystyle \int_0^\infty \dfrac{\sin(x)}{x}\dfrac{\sin(x/3)}{x/3}\dfrac{\sin(x/5)}{x/5} = \dfrac{\pi}{2}$$ $$\displaystyle \int_0^\infty \dfrac{\sin(x)}{x}\dfrac{\sin(x/3)}{x/3}\dfrac{\sin(x/5)}{x/5}\dfrac{\sin(x/7)}{x/7} = \dfrac{\pi}{2}$$ $$\displaystyle \int_0^\infty \dfrac{\sin(x)}{x}\dfrac{\sin(x/3)}{x/3}\dfrac{\sin(x/5)}{x/5}\dfrac{\sin(x/7)}{x/7} \dfrac{\sin(x/9)}{x/9}= \dfrac{\pi}{2}$$ $$\displaystyle \int_0^\infty \dfrac{\sin(x)}{x}\dfrac{\sin(x/3)}{x/3}\dfrac{\sin(x/5)}{x/5}\dfrac{\sin(x/7)}{x/7} \dfrac{\sin(x/9)}{x/9}\dfrac{\sin(x/11)}{x/11}= \dfrac{\pi}{2}$$ $$\displaystyle \int_0^\infty \dfrac{\sin(x)}{x}\dfrac{\sin(x/3)}{x/3}\dfrac{\sin(x/5)}{x/5}\dfrac{\sin(x/7)}{x/7} \dfrac{\sin(x/9)}{x/9}\dfrac{\sin(x/11)}{x/11}\dfrac{\sin(x/13)}{x/13} = \dfrac{\pi}{2}$$ At this point, it is tempting to speculate that this pattern goes on forever, but we run into problems and this is another example of jumping to conclusions too soon. $$\displaystyle \int_0^\infty \dfrac{\sin(x)}{x}\dfrac{\sin(x/3)}{x/3}\dfrac{\sin(x/5)}{x/5}\dfrac{\sin(x/7)}{x/7} \dfrac{\sin(x/9)}{x/9}\dfrac{\sin(x/11)}{x/11}\dfrac{\sin(x/13)}{x/13}\dfrac{\sin(x/15)}{x/15} = \dfrac{467807924713440738696537864469 \pi }{935615849440640907310521750000}$$ I calculated the next several and they are nice approximations to the results above, but not that result $$\dfrac{17708695183056190642497315530628422295569865119 \pi }{35417390788301195294898352987527510935040000000}$$ $$\dfrac{8096799621940897567828686854312535486311061114550605367511653 \pi }{16193600755941299921751838065715269433640150152124763150000000}$$ $$\dfrac{2051563935160591194337436768610392837217226815379395891838337765936509 \pi }{4103129007448718822870650414175026723860506854636748901313920000000000}$$ $$\dfrac{37193167701690492344448194533283488902041049236760438302965167901187323851384840067287863 \pi }{74386376780038719358535506076609218130495936637120586884474907521986965251324791250000000}$$ He states ""The explanation for this change is a bit technical, but the critical reason is that $\dfrac{1}{3} + \dfrac{1}{5} + \ldots + \dfrac{1}{13} \lt 1$ , whereas, adding the next term $\frac{1}{15}$ pushes the sum over $1$ , making a difference in the value of the integral."" He does not mention the researcher, but I'd like to know what is a ""bit technical"" explanation or if there is a more analytical or mathematical rationale or a reference to the research?","['reference-request', 'divergent-series', 'definite-integrals']"
1456188,Puzzle on rolling dice game,"A gambler goes to bet. The dealer has 3 dice, which are fair, meaning that the chance that each face shows up is exactly 1/6. The dealer says: ""You can choose your bet on a number, any number from 1 to 6. Then I'll roll the 3 dice. If none show the number you bet, you'll lose \$1. If one shows the number you bet, you'll win \$1. If two or three dice show the number you bet, you'll win \$3 or \$5, respectively."" Is it a fair game? PROPOSED APPROACH: Let A be event when all three dice show the given number, B be the event that only two dice show the same given number, C be the event that only one dice shows the given number, D be the event otherwise. Now P(A) = $1/216$ P(B) = ${3 \choose 2} * [1*1*5/216] = 15/216$ P(C) = ${3 \choose 1} * [1*5*5/216] = 75/216$ From the law of complementary event the P(D) = 1 - P(A) - P(B) - P(C) = $1 - 1/216 - 15/216 - 75/216 = 125/216$ Now expected value for winning is $\$1*P(C) + \$3*P(B) + \$5*P(A) = 125/216$ expected value for losing is $\$1*P(D) = 125/216$ Given the two values are equal, it is a fair game. I am thinking if there is a quicker or smarter way to do it.","['dice', 'probability', 'gambling']"
1456194,Taylor expansion of the square of the distance function,"Give a smooth Riemannian manifold $(M,g)$, (i) how can one compute Taylor expansion of the square of the Riemannian distance function $d^2(x,x_0)$ at $(x',x_0)$? I've tried to use $dist=\int (g_{\mu\nu} \frac{dx^\mu}{dt} \frac{dx^\nu}{dt} )^{1/2} dt$ to expand $dist^2$, but it was so messy. BTW, I've found that a similar question has been asked at mathoverflow.net . The answer given is actually very nice but it goes a little too technical and quite cryptic to me... In particular, the final result seems to a generalised Cosine law, right?! (ii) Doesn't it make a sense to write $d^2(x,x_0)=d^2(x'+(x-x'),x_0)$ and take it as a scalar function?! Under what conditions can one assume that the square of the distance function is a polynomial? I would appreciate answers not demanding an all profound background...","['differential-geometry', 'metric-geometry', 'riemannian-geometry']"
1456224,Euler-Lagrange Equation has no solution?,"I've been asked to compute the Euler-Lagrange equation and second variation of the functional $$I[y]=\int_{a}^{b}(y'^2+y^4)dx$$
with boundary conditions $y(a)=\alpha$, $y(b)=\beta$. It's easy to see that $$I[y+\delta y]=I[y]+\int_{a}^{b}\delta y(4y^{3}-2y'') dx+\int_{a}^{b}(6y^{2}\delta y^{2}+\delta y'^{2})dx$$
So the Euler-Lagrange equation integrates to give $y^{4}-y'^{2}=k$, where $k$ is a constant of integration. We're then asked to solve this equation when $\alpha=\beta=0$. The equation is separable, but to my shame I can't do the integration (I think it involves special functions), so I looked for a different way. Completing the square on $I$ gives $$\int_{a}^{b}(y'^2+y^4)dx=\int_{a}^{b}(y'+y^2)^{2}dx-\int_{a}^{b}2y^{2}y'dx$$
But the final term is just $\left[\frac{2}{3}y^{3}\right]^{y=0}_{y=0}=0$, and the other two integrals are non-negative. The only way to extremise the RHS (I think) is to minimise it, and besides the second variation is non-negative, so we make the RHS zero by allowing $y'=-y^{2}$. But then the LHS forces both $y'=0$ and $y=0$ on all of $[a,b]$. Does this mean that the only solution is the zero function? Then, since $\delta^{2}I=\int_{a}^{b}\delta y'^{2}dx$, and this is positive unless $\delta y$ is constant (and hence $0$, by the boundary condition), do we have the zero function as the actual solution? Sorry if this sounds a little incoherent, when I started writing the answer I forgot to consider $y(x)=0$ and found the other solutions to $y'=-y^{2}$, which can't possibly satisfy the boundary conditions. In fact, looking at it now, I'm starting to think that even completing the square was unnecessary.","['calculus-of-variations', 'proof-verification', 'ordinary-differential-equations']"
1456238,Which theory is used to calculate the position and energy of a point source?,"Consider an empty room with one point source that emits a stationary signal (constant sound, radioactive radiation, ...). The energy nor the position of the point source is known. We send someone in the room to do some intensity measurements on different positions in the room. With this collected data I want to determine the position and the energy of the point source. Which mathematical theory can be used to do such calculations?","['physics', 'statistics', 'inverse-problems']"
1456248,Degree of $P$ as a smooth function equals its polynomial degree $d$.,"Let $M,N$ be connected oriented manifolds such that $\partial M=\partial N=\emptyset$.
Let $F:M\to N $ be a smooth proper map (i.e. for every $K\subset N$ compact, $F^{-1}(K)$ is compact). We define the degree of $F$ as
$$
deg(F):=\sum_{p\in F^{-1}(q)}\varepsilon(p)
$$
where $q$ is a regular value of $F$ and 
$$
\varepsilon(p)=\begin{cases}
   +1       & \quad \text{if F preserves the orientation near $p$ }\\
    -1  & \quad \text{if F reverves the orientation near $p$}\\
  \end{cases}
$$ Let $P$ be a complex polynomial of degree $d$. Considering $P$ as a map from $\mathbb{R}^2$ to $\mathbb{R}^2$, I proved that $P$ is smooth, proper (if and only if is polynomial degree $d$ is positive). I want to show that the degree of $P$ as a smooth function equals its degree as a polynomial.
(Somehow I think the fundamental theorem of algebra should be involved)
Any hints?","['polynomials', 'manifolds', 'differential-topology', 'differential-geometry', 'complex-analysis']"
1456260,Decomposition of sorting permutation in fewest amount of transpositions,"I found that any permutation cycle of length n can be written as a product of n-1 two-level permutations or transpositions. However, there are many other ways to do this decomposition(but it has to remain even/odd). Is there an algorithm or method to find the transposition decomposition with the fewest amount of transpositions? (I found this is called a 'God's algorithm') I also found dozens of sorting algorithms in my search, but it seems they are made to optimize computational time which comprises to the most extent the amount of equations of numbers. Here, I assume this can be seen instantly, but swapping two levels is a physical thing and takes a lot of effort.","['group-theory', 'algorithms', 'permutations']"
1456267,Help with understanding Rating Systems,"Hey guys so I'm trying to come with a rating system from 1-10 that will be determined based on how close a user is to an average value that I have. I don't have any data(other than the average value) to work with, so standard deviation is not possible. So lets say I want to give a number to a household based off of how many children they have(assume average is 2), if a house has higher than two its value will be a bit higher than 5. Is there any documentation/articles/books I could read to get a better grasp of where to look? Sorry for being a bit vague, but I don't know where the right places to look are in this matter, hope you guys can help!","['statistics', 'recreational-mathematics']"
1456272,A coin is tossed $m+n$ times $(m>n)$.Show that the probability of atleast $m$ consecutive heads is $\frac{n+2}{2^{m+1}}$,"A coin is tossed $m+n$ times $(m>n)$.Show that the probability of atleast $m$ consecutive heads is $\frac{n+2}{2^{m+1}}$. I could not attempt this question,except few initial steps.Let $H$ and $T$ denote turning up of the head and tail.$\therefore P(H)=P(T)=\frac{1}{2}$ Please help me.","['probability', 'combinatorics']"
1456273,Uniqueness of the solution of a PDE,"I have the following initial-value problem: $$ \begin{cases} u_t + c u_x = 0, \quad x \in \mathbb{R}, \ t > 0 \\ u(x,0) = g(x) \end{cases} \qquad [1] $$where $u_t$ and $u_x$ are the partial derivatives. It has been asked me to prove that the problem is well-posed if $g \in C_b ^1 (\mathbb{R})$ - id est to prove that a solution exists and it is unique, and that the solution depends continuously on the initial data in the norm $ \sup_{x \in \mathbb{R}} |g(x)|$. Now, the existence and the continuous dependence are pretty straightforward: it is sufficient to notice that $u(x,t)=g(x-ct)$ is a solution of $[1]$. But what about the uniqueness? I couldn't figure out how to reach a contraddiction by assuming the existence of another solution $v(x,t)$... I would conclude by myself, so just give me an (eventual) hint. Thanks in advance.","['ordinary-differential-equations', 'partial-differential-equations']"
1456283,Prove that if $AA^T=A$ then $A^3=A$,"The approach I'd like to use to prove this particular property necessitates that $A$ be invertible, but I don't wish to assume this (though it would certainly make the task simpler). Is there some property which shows $A$ to be invertible which I am overlooking, or is it that perhaps I need to use a different method of proof?","['inverse', 'linear-algebra', 'matrices']"
1456300,"Prove that $\sum\limits_{\mathrm{cyc}} \left( a+\frac{1}{b} -1\right) \left( b+\frac{1}{c} - 1\right) \ge 3 $ for positives $a, b, c$","for $a, b, c $ positive real numbers $$ \left( a+\frac{1}{b} -1\right) \left( b+\frac{1}{c} - 1\right) +\left( b+\frac{1}{c} -1\right) \left( c+\frac{1}{a} -1\right)$$ $$ +\left( c+\frac{1}{a} -1\right) \left( a+\frac{1}{b} -1\right) \geq 3$$ How we can prove the inequality   above.  Actually it take long time to prove it but I couldn't complete.  How we prove it? . Thanks for help","['algebra-precalculus', 'inequality']"
1456320,Polar decomposition of real matrices,"Can we decompose real matrices in a way that is analogous to polar decomposition in the complex case? What I mean is: given an invertible real matrix $M$, can we always write:
$$
M = OP,
$$ maybe uniquely, where $O$ is orthogonal, and $P$ is symmetric positive-definite? Thanks.","['linear-algebra', 'matrix-decomposition']"
1456345,functions that map to sets?,"It it well-defined to map to a set? That is, $$f(x):= \{x,x+1\}$$ for example? If so, how would one define it? $f: \mathbb{R} \rightarrow ...$ what? What does one need to be wary of in order to ensure well-definedness? What about injectivity/subjectivity issues?",['functions']
1456350,The Supremum and Infimum of a sequence of measurable functions is measurable,"I am reading through Folland's Real Analysis: Modern Techniques and Their Applications , and they have the following proposition and proof: Proposition: If $\{f_{j}\}$ is a sequence of $\bar{\mathbb{R}}$- valued measurable functions on $(X, \mathcal{M})$, then the functions: $$g_{1}(x)=\sup_{j} f_{j}(x)$$ $$g_{2}(x)=\inf_{j} f_{j}(x)$$ are
  measurable. Proof: We have $$g_{1}^{-1}((a,\infty])= \bigcup_{j=1}^{\infty}
 f_{j}^{-1}((a,\infty])$$ $$g_{2}^{-1}([-\infty,a))=
 \bigcup_{j=1}^{\infty} f_{j}^{-1}([-\infty,a))$$ Where the result follows from the fact that $g_{1}^{-1}((a,\infty])\in
 \mathcal{M}$ for all $a \in \mathbb{R} \iff g$ is measurable, and
  $g_{2}^{-1}([-\infty,a))\in \mathcal{M}$ for all $a \in \mathbb{R}
 \iff g$ is measurable. I understand the last part of the proof, but how do we know $g_{1}^{-1}((a,\infty])= \bigcup_{j=1}^{\infty}f_{j}^{-1}((a,\infty])$ and $g_{2}^{-1}([-\infty,a))=\bigcup_{j=1}^{\infty} f_{j}^{-1}([-\infty,a))$? I would appreciate any help with this.","['elementary-set-theory', 'supremum-and-infimum', 'measure-theory']"
1456387,"$T:X\to Y$ a linear operator, $Y$ finite-dimensional, then $\ker T$ is closed iff $T$ is continuous","I have to prove that if $T:X\to Y$ a linear operator on normed spaces, $Y$ finite-dimensional, then $\ker T$ is closed iff $T$ is continuous. On a lot of places I see a proof that looks like this : it starts by assuming $Y=\mathbb{R}$ for simplicity. (I'm also aware of this that use a completely different construction). Anyway, I did not at all find it obvious that there was any kind of generalisation of the first linked proof to higher dimensions. I will give my (quite long a tedious) generalisation here, but my question is: is there a more direct/easier generalisation of the construction given in the first linked proof? (I also wouldn't object to someone checking the actual correctness of this proof) We will take $Y=\mathbb{R}^n$. Take $x_n\in X$ s.t. $||x_n||=1$, $||Tx_n||\to \infty$. w.l.o.g. we may assume that $\frac{Tx_n}{||Tx_n||}\to v_1\in \overline{B(0,1)}\subset \mathbb{R^n}$, by the compactness of the closed unit ball in finite dimensions. We now take $y_n=x_j$ for some $j$ s.t. $||\frac{Tx_n}{||Tx_n||}-v_1||<\frac{1}{n}$. Then we adjust the $y_n$ a little bit: if $||Ty_n||>n$ we set $y_n=\frac{y_n}{||Ty_n||}n$. We now obtain a sequence $y_n$ s.t.
$$
\begin{align*}
||y_n||\leq 1\\
||Ty_n||\leq n\\
||Ty_n||\to \infty
\end{align*}
$$
In particular we have that $||Ty_n-||Ty_n||v_1||<1$ (this is the critical property that we needed). We now take an orthonormal basis $\{v_i\}$ for $\mathcal{R}T$ containing $v_1$, and $v_i'$ s.t. $Tv_i'=v_i$. We conclude that for $j>1$ we must have $(v_j,Ty_n)<1$. We now define $z_n=y_n-\sum_{j>1}(v_j,Ty_n)v_j'$. Then
$$||z_n||=||y_n-\sum_{j>1}(v_j,Ty_n)v_j'||\leq ||y_n||+\sum_{j>1}(v_j,Ty_n)||v_j'||\leq 1+\sum_{j>1}||v_j'||\leq C$$
Hence these $z_n$ are bounded. Furthermore we clearly still have that
$$||Tz_n||\to\infty$$
because the norm of $T\sum_{j>1}(v_j,Ty_n)v_j'$ is bounded. We can now finally define 
$$w_n=v'_1-\frac{z_n}{||Tz_n||}$$
Then 
$$Tw_n=v_1-\frac{1}{||Tz_n||}(Ty_n-\sum_{j>1}(v_j,Ty_n)v_j)=0$$
Since $Ty_n-\sum_{j>1}(v_j,Ty_n)v_j$ is by construction a vector pointing in the direction of $v_1$. But again $w_n\to v'_1$, which contradicts the closedness of the kernel. For the intuition: We take the unbounded sequence in $X$. The the project is onto the unit ball, which is compact, hence the sequence a convergent subsequence. We take the sequence, with limit $v_1$. Then we 'limit the growth' of our unbounded sequence to make sure that if $\frac{Tx}{||Tx||}$ is close to $v_1$, then $Tx$ is close to the line spanned by $v_1$. Then we remove any orthogonal components from the $Tx$, and because $Tx$ is now close to $<v_1>$ we can remove the orthogonal components by subtracting a vector of a certain bounded length. This finally gives a bounded sequence in $X$ with unbounded images in $Y$ all lying on the lines $<v_1>$, and from here the previous argument goes through. Thanks for reading.","['solution-verification', 'alternative-proof', 'functional-analysis', 'linear-transformations']"
1456401,How to prove indirectly that if $42^n - 1$ is prime then n is odd?,"I'm struggling to prove the following statement: If $42^n - 1$ is prime, then $n$ must be odd. I'm trying to prove this indirectly, via the equivalent contrapositive statement, i.e. that if $n$ is even, then $42^n - 1$ is not prime. By definition, for every even number $n$ there exists an integer $k$ with $n = 2k$. We substitute and get
$$42^n - 1 = 42^{2k} - 1 = (42^2)^k - 1.$$ Now, how do I prove that $(42^2)^k - 1$ isn't a prime number? Is this even the right way to approach this proof?","['prime-numbers', 'discrete-mathematics']"
1456403,Area integral over complex plane of non-holomorphic gaussian $e^{-z\bar{z}}$,"Let $A$ be the two-dimensional area integral, over the complex plane, of a gaussian function: $$A = \int_\text{plane} \frac{\mathrm{d}\bar{z} \wedge \mathrm{d}z}{2i} \ \exp[-z\bar{z}].$$ Of course one could evaluate this integral by converting to Cartesian (or alternatively to polar) coordinates, where the answer is evident, $$A = \int_\text{plane}  \mathrm{d}x \wedge \mathrm{d}y \ \exp[-(x^2+y^2)] = \pi.$$ But, is there a way to do the complex area integration directly in the complex coordinates $z,\bar{z}$ ? In particular, let us define the (non-holomorphic) function $f$ , $$f=-\frac{1}{z}\exp[-z\bar{z}], \\ \frac{\partial f}{\partial \bar{z}} = \exp[-z\bar{z}].$$ Then I wonder whether one can proceed with the following manipulations: first ""integrate"" over $\bar{z}$ , $$A = \int_\text{plane} \frac{\mathrm{d}\bar{z} \wedge \mathrm{d}z}{2i} \ \exp[-z\bar{z}]
=  \oint_\text{contour}  \frac{\mathrm{d}z}{2i}\ f(\bar{z},z) .$$ and then evaluate the positive definite Gaussian part at the $z=0$ pole, to use the Residue theorem, $$
=  \oint_\text{contour} \frac{\mathrm{d}z}{2i} \ \frac{1}{z} = \pi ,$$ which produces the correct value of the integral.  Is there a sense in which these manipulations are correct? Can one perform the complex plane area integral by first integrating over $\bar{z}$ , and then over $z$ ?","['contour-integration', 'complex-integration', 'complex-analysis', 'residue-calculus']"
1456411,Why don't we consider $\mathbb{R}^3$ to be an affine space?,"When we're introduced to $\mathbb{R}^3$ in multivariable calculus, we first think of it as a collection of points. Then we're taught that you can have these things called vectors , which are (equivalence classes of) arrows that start at one point and end up at another. At this point $\mathbb{R}^3$ is an affine space, not a vector space: for two points $x, y \in \mathbb{R}^3$, the operation $x + y$ is meaningless (my professor likes to say: ""You can't add Chicago and New York!"") but the operation $x - y$ gives a vector (the vector which points from New York to Chicago). You can also add a point and a vector, which gives you a translated point. The distinction between the point $(0, 1, 2)$ and the vector $\langle 0, 1, 2 \rangle$ is sometimes made. But then we quickly move on to treating $\mathbb{R}^3$ as a vector space, where instead of a point $A$, you have vectors starting at the origin with their tip at $A$. For example, parameterized curves such as $$r(t) = (t, t^2, 3t)$$ are called ""vector-valued functions"" and not ""point-valued functions"". So, my question is, what is the reason that we historically don't define two spaces -- $\mathbb{R}^3$ and $\mathbb{R}^3_{\text{affine}}$? (I'm sure there's better notation). For example, my ""point-valued function"" $r(t)$ would be a function $\mathbb{R} \rightarrow \mathbb{R}^3_{\text{affine}}$, but its derivative $r'(t)$ (the velocity vector ) would be a function $\mathbb{R} \rightarrow \mathbb{R}^3$. What would this make more difficult? In particular, I know that $\mathbb{R}^3 \iff \mathbb{R}^3_{\text{affine}}$ is a bijection, and that we use this sometimes, but how often in multivariable calculus? If we are using it all the time, then it wouldn't make sense to emphasize the distinction.",['multivariable-calculus']
1456444,"Let $S=\{3,4,5,6,7,8,9,10,11,12\}$. Suppose 6 integers are chosen from S. Must there be 2 integers whose sum is 15?","How can I go about solving this Pigeonhole Principle problem? So I think the possible numbers would be: $[3+12], [4+11], [5+10], [6+9], [7+8]$ I am trying to put this in words...","['pigeonhole-principle', 'discrete-mathematics']"
1456520,Prove that if $A$ is a symmetric matric then $A^3$ and $A^2-2A+I$ are symmetric matrices.,"I am uncertain on how to approach this proof. For most everything I've encountered concerning symmetry, it has involved taking the transpose in order to show some property. Here, I'm not certain if and how that would be effective.","['systems-of-equations', 'transpose', 'matrices', 'symmetry', 'linear-algebra']"
1456530,Concrete examples and computations in differential geometry,"I've been studying differential geometry by myself for some time now. I studied a fair amount of the basic general theory and gone through a lot of the exercises from several textbooks. Lately I started to realize how huge (and daunting) differential geometry really is. I think I have a pretty solid grasp of the basic objects. For example I can switch from global to local description with comfort, and do most symbolic calculations using the basic objects of the theory and not get confused about what i'm doing since i understand the operations and the context (e.g. proving Fundamental theorem of Riemannian geometry, proving general properties of connections and their curvature tensors, proving various identities about different derivations, proving frobenius theorem etc.). My problem lies with concrete examples and computations. I've had little to no experience with those and frankly they quite scare me. The only examples I know and tampered with before are spheres and projective spaces (and a pinch of some matrix groups and grassmanians). And even with them i feel my experience is quite brief. Most of the textbook problems I solved were general theory, which is great and rewarding, but I do feel unbalanced at the moment. Why is the pool of examples I found so far in textbooks so small? Is it the case you need a lot of machinary before you can really tackle more examples? What would you recommend me to do? Edit: Here are the main books I've studied from (I admit I wasn't completely thorough - however I'd rarely skip an exercise problem from a chapter i've been reading): guilliam and pollack Liviu - geometry of manifolds (roughly the first third of the book). Jefferey Lee - manifolds and differential geometry (up to chapter 8).","['self-learning', 'differential-geometry', 'examples-counterexamples', 'riemannian-geometry']"
1456537,Regarding Feynman Integration - unnamed theorem (?),"I found a certain theorem, that hasn't been proven in the article and as it seems to require just Calculus 2 (multivariable calculus) knowledge.- I would like to prove it, however I am completely stuck how to approach this problem. Therefore I would be very happy, if someone could provide any sort of constructive hint, comment or answer or recommendation for further reading. As always thanks in advance. The theorem states as follows: Let $ f : [a, b]×Y → \mathbb{R} $ be a function,
with $ [a, b]$ being a closed interval, and Y being a compact subset of $\mathbb{R}^n$. Suppose that both $f(x, y)$ and $\frac{∂}{∂x}f(x, y)$ are continuous in the variables $x$ and $y$ jointly. Then $ \int_Y f(x, y)dy$ exists as a continuously differentiable function of $x$ on $[a, b]$ with: $\frac{d}{dx} \int_Y f(x, y)dy = \int_Y \frac {∂}{∂x} f(x, y)dy $","['compactness', 'measure-theory', 'integration']"
1456552,How do I prove that $I+T^*T$ is invertible?,"Let $T$ be a bounded linear operator in a Hilbert space, $T^*$ the adjoint of $T$ . 
Then how to show that $I + T^*T$ is invertible? Thanks.",['functional-analysis']
1456567,Expected maximum absolute value of $n$ iid standard Gaussians?,"I have a problem where my errors are normally distributed and I want to know what the expected maximum error is if I repeat the process $n$ times. What is the smallest constant $C$ such that the following statement is true for all $n\geq 2$ ? Let $X_1, X_2, \cdots, X_n$ be independent standard Gaussian random variables. Then $$\mathbb{E}\left[\max_{i=1}^n \left|X_i\right| \right] \leq C \sqrt{\log_e n}.$$ I can show that the answer is between $1.35$ and $2$ .","['order-statistics', 'normal-distribution', 'probability', 'expectation']"
1456574,Is it always true that $\sum^{\infty}a_{i}1_{A_{i}}-\sum^{\infty}b_{i}1_{B_{i}}=\sum^{\infty}c_{i}1_{C_{i}}$?,"Suppose $\sum^{\infty}a_{i}1_{A_{i}}\geq \sum^{\infty}b_{i}1_{B_{i}}$, where $a_{i},b_{i}\geq 0$ and the sets possibly intersect i.e. $A_{i}\cap A_{j}\neq \varnothing $ and same with $B_{i}$. Is it true that we can write 
$\sum^{\infty}a_{i}1_{A_{i}}-\sum^{\infty}b_{i}1_{B_{i}}=\sum^{\infty}c_{i}1_{C_{i}}$ for $c_{i}\geq 0$? If we have disjointness i.e. $A_{i}\cap A_{j}=\varnothing $ and same for $B_{i}$ then yes. My concern is that when I proved it the general case for finite order i.e. $\sum^{N}a_{i}1_{A_{i}}$ my proof again relies on rewriting them disjointly. There are sums $\sum^{\infty}a_{i}1_{A_{i}}$ that cannot be written disjointly without making them uncountable sums. For example, $\sum a_{k}1_{A_{k}}$, where $A_{k}=\{x\in \mathbb{R}^{+}:\frac{d_{k}}{10^{k}}$ is in the decimal expansion of x $\}$ and so $x=\sum^{\infty} a_{k}1_{A_{k}}(x)$. Since $\mathbb{R}^{+}$ is uncountable, this sum cannot be written disjointly. Proof for disjoint sets We have from disjointness that $b_{i}\leq \inf(a_{j})$ for all j s.t. $B_{i}\cap A_{j}\neq \varnothing$. Therefore, $\sum^{\infty}a_{i}1_{A_{i}}-\sum^{\infty}b_{i}1_{B_{i}}=\sum^{\infty}a_{i}1_{A_{i}\setminus \bigcup B_{j}}+\sum_{j}\sum^{\infty}a_{i}1_{A_{i}\cap B_{j}}-\sum^{\infty}b_{i}1_{B_{i}}$
$=\sum^{\infty}a_{i}1_{A_{i}\setminus \bigcup B_{j}}+\sum_{j}\sum^{\infty}(a_{i}-b_{j})1_{A_{i}\cap \bigcup B_{j}}.$ Here we used disjointess of $B_{i}$ to split the indicator. Suggestions? Attempt at proof for general case For finite $\sum^{N}a_{i}1_{A_{i}}-b_{i}1_{B_{i}}$ the result follows because we can rewrite them disjointly. So now maybe we can approximate $\sum^{\infty}a_{i}1_{A_{i}}-b_{i}1_{B_{i}}$ by $\sum^{N}c_{i}1_{C_{i}}$. This shows up in proving DCT without having to use positive and negative parts.","['elementary-set-theory', 'real-analysis', 'lebesgue-integral', 'measure-theory']"
1456579,Intuitive reason why the variance of a hypergeometric variable is smaller than the variance of the corresponding Binomial variable,"I am looking for the most cleanest and most intuitive possible description of why the variance of the number of successes when sampling without replacement is smaller than the variance of the number of successes when sampling with replacement.  I desire a verbal explanation, perhaps combined with an illuminating example. Notes:  I can derive the formulas for both variances and see that one is obtained from the other by a correction factor.  I only want a clear description of the ""intuition"" behind the fact that this happens.","['probability', 'probability-distributions', 'random-variables']"
1456583,In how many ways can we add $1$'s and $2$'s to get $11$ (when the order matters)?,"Examples:
$1+1+1+1+1+1+1+1+1+1+1$, $2+2+2+2+2+1$, $1+2+2+2+2+2$ (order matters)
I tried solving it with permutations but I realized it won't work","['number-theory', 'combinatorics']"
1456621,Compute $\lim_{x \to 0} x \lfloor x - \frac{1}{x} \rfloor$,"Let $f: \mathbb{R} \to \mathbb{R}$ be the following function: $f(x) = x \lfloor x - \frac{1}{x} \rfloor$ Show that $f(x)$ admits a limit at zero, and compute its value. Using epsilon delta, I can prove that $\lim_{x \to 0} x \lfloor x - \frac{1}{x} \rfloor = - 1$: $|x \lfloor x - \frac{1}{x} \rfloor + 1|$ $\leq |x \lfloor x - \frac{1}{x} \rfloor| + |1|$ $\leq |x| |\lfloor x - \frac{1}{x} \rfloor| + |1|$ $\leq |x| |x| + |1|$ $\leq |x|^2 + |1| < \epsilon$ if $|x| < \delta = \sqrt{\epsilon - 1}$ Is that right? If not, how can I do prove it? But how do I show that $\lim_{x \to 0} x \lfloor x - \frac{1}{x} \rfloor$ exists?","['calculus', 'limits']"
1456641,Is there a better way to find the minimum value of $\frac{1}{\sin \theta \cos \theta \left ( \sin \theta + \cos \theta \right )}$?,"To find the minimum value of $$\frac{1}{\sin \theta \cos \theta \left ( \sin \theta + \cos \theta \right )}$$ I can see that I can convert it to $$\frac{\sqrt{2}}{\sin (2 \theta) \sin \left ( \theta + \frac{\pi}{4} \right )}$$ And from here I can convert it to a pair of cosecant functions and then proceed with the usual product rule, solving that out with zero etc. Once that is all done, we can find that the function attains a minimum at $\theta= \frac{\pi}{4}$ and this minimum value is $\sqrt{2}$. However, I was wondering if there was perhaps a better way of finding the minimum value of this function that does NOT require heavy amounts of algebra bashing. Perhaps by some means of observation. So for example, one could find the maximum of $\sin \theta$ by realising that it oscillates between $1$ and $-1$. Could a similar idea be used to find the minimum of this function?","['optimization', 'calculus', 'trigonometry']"
1456648,trigonometry equation $3\cos(x)^2 = \sin(x)^2$,"I tried to solve this equation, but my solution is wrong and I don't understand why. the answer in the book is: $x = \pm60+180k$. my answer is: $x= \pm60+360k$.
please help :) 3cos(x)^2 = sin(x)^2
3cos(x)^2 = 1 - cos(x)^2

t = cos(x)^2

3t=1-t
4t=1
t=1/4

cos(x)^2 = 1/4
cos(x) = 1/2
cos(x) = cos(60)

x = +-60+ 360k edited: cos(x) = -1/5
cos(x) = cos(120)
x = +-120 + 360k I still don't get the answer in the book",['trigonometry']
1456658,Prove that $A\geq I$ implies that $A$ is invertible.,"Here's the question: Let $A$ be a positive operator on a (possibly infinite dimensional) Hilbert space.  Let $I$ denote the identity operator.  Suppose that $A \geq I$, which is to say that $A - I$ is a positive operator.  Prove that $A$ is invertible. I think that this is true, but I haven't been able to find a proof one way or the other.  I would like to avoid invoking any heavy machinery (like the spectral theorem) if possible.  I would also be interested in a proof that carries over to more general $C^*$ algebras. Of course, the proof in the case of finite dimensional spaces is fairly obvious, since it suffices to show that the operator has a trivial kernel.  Since that does not suffice here, I really have no clue what my next move should be. Any guidance here would be greatly appreciated.","['c-star-algebras', 'hilbert-spaces', 'operator-theory', 'functional-analysis']"
1456666,Probability Distribution of Infinity Norm of a Vector of Independent Random Variables,"Assuming $V\in \mathbb{R}^k$ to be a vector of independent random variables each with $\sim\mathcal{N}(0,1)$ (Or even more general $\sim\mathcal{N}(a,b)$). I was wondering how I can calculate the distribution (so both $\mathbb{E}$ and $\sigma$) of infinity norm of my vector; i.e. $\|V\|_\infty=\max_{i\in \{1,2, \ldots, k\}} |V_i|$ ?","['analysis', 'probability', 'statistics', 'vector-spaces']"
1456692,Rudin's application of the mean value theorem,"I am studying theorem 6.26 (page 152) in Rudin's ""Functional Analysis"" that presents distributions as derivatives of continuous functions. Right at the beginning of the proof, if $\Omega$ is the usual open subset, $Q = [0,1] ^n \subset \Omega$ is the unit cube and $\psi \in \mathcal D _Q (\Omega)$ (the space of test functions on $\Omega$ with support in $Q$), he applies the mean value theorem and states that $| \psi | \le \max \limits _{x \in Q} | ( \partial _i \psi ) (x) |$. What precise statement of the MVT does he use, and how? What I can think of is that the differential form of the MVT looks like $| \psi (x) - \psi (0) | \le \| \Bbb d \psi (\xi) \| \cdot \|x - 0\|$. Since $\text{supp} \ \psi \subset Q$ and $0 \in \partial Q$, then $\psi (0) = 0$, so the best I can get is $| \psi (x) | \le \| \Bbb d \psi (\xi) \| \cdot \|x\|$. Why is there no $\| x \|$ in Rudin's right-hand side? Does he work with $\| \cdot \| _\infty$, to make it $1$ on $Q$? But what then if $x \in \Omega \setminus Q$? And, in general, what is he doing here?","['distribution-theory', 'calculus', 'multivariable-calculus', 'functional-analysis']"
1456725,Two dimensional function that belongs to $C^{1}$ but does not belong to $C^{2}$.,"Give an example of a two dimensional function that belongs to $C^{1}$ but does not belong to $C^{2}$ globally. The function must have a minimum point and at that minimum point the function must be twice continuously differentiable. In one dimension I have got one, $$f(x)= \begin{cases}-x^3,\quad & x<1 \\  
 x^2-5x+3, \quad & x>=1 \end{cases}
$$
How to make a two-dimensional example?","['continuity', 'multivariable-calculus', 'real-analysis', 'derivatives']"
1456732,basic definition of vector-fields on a manifold,"Many textbooks introduce vector-fields on a manifold $M$ along the lines of $ X = X_i \frac{\partial}{\partial x_i} $ where -without further ado- $\frac{\partial}{\partial x_i}$ is introduced as a basis vector in Tangent space. Given that the same symbol is widely used as a differential operator in mathematics, should I think about $X$ in terms of an operator rather than a vector?  Or is that notation a clever suggestive trick to help ease calculations - similar to physicists speaking of ""ket-"" $ | \psi>$ and ""bra-"" $ < \Phi | $ vectors in a physical Hilbert space that ""magically"" turn into a scalar-product $ <\Phi|\Psi> $ when ""meeting"" in a calculation? Similar conceptual difficulties arise for me when in the the definition of 1-forms $\omega = \omega_1 dx $ the object $dx$ is introduced as a ""unit"" vector when I am used to thinking about $dx$ as an infinitesimal quantity. Cheers!","['vector-fields', 'differential-geometry']"
1456737,Probability of second player winning once,"Team A and Team B are both composed of three members, respectively. Each member is assigned with the order to play (e.g. first, second, and the last) and should obey the following rules. 
(a) The first players of Team A and Team B match against each other 
(b) The player who beat the opponent continues to play a game with the next player of the other team 
(c) If all members of any team are defeated, the game is over 
Find the probability that the second player of Team A wins only once. (The probability of each player winning is 0.5 and all matches end with a winner and a loser (there is no draw)) I think there are 5 cases of the second player winning. I just tried to count all the possible outcomes and got the answer to be 9/32. However i think i am wrong... Can anyone help me with this problem?","['probability', 'statistics']"
1456746,Show that $\arctan\frac{1}{2}+\arctan\frac{1}{3}=\frac{\pi}{4}$,"Show that $\arctan\frac{1}{2}+\arctan\frac{1}{3}=\frac{\pi}{4}$. Attempt: I've tried proving it but it's not equating to $\frac{\pi}{4}$. Please someone should help try to prove it. Is anything wrong with the equation? If there is, please let me know.",['trigonometry']
1456748,"trigonometry equation, $[\cos(2x)]^2-\sin(2x)=1$","I tried to solve this equation: $[\cos(2x)]^2-\sin(2x)=1$ I got different answer from the book.
the answer in the book: $x=-45+180k$ , $x=90k$ Am I right? $x = -45+180k$ is equal to $x= 135+180k$ ? How can I check if it's the same? Or maybe I did a mistake? please help \begin{align*}
[\cos(2x)]^2-\sin(2x) & = 1\\
-\sin(2x)& = 1-\cos(2x)^2\\
-\sin(2x)& = \sin(2x)^2\\
-\sin(2x)-\sin(2x)^2 & = 0\\
\sin(2x)+\sin(2x)^2 & = 0\\
\sin(2x)[1+\sin(2x)] & = 0
\end{align*} \begin{align*}
\sin(2x) & =0 & 1 + \sin(2x) & = 0\\
2x & = 180k & \sin(2x) & = -1\\
x & = 90k &  2x & = 270+360k\\
& & x & = 135+180k
\end{align*}",['trigonometry']
1456818,Is $\sum_{n=1}^{\infty }\frac{8n\cdot\zeta (2n)}{3\cdot 2^{2n}}=\zeta (2)$?,"$$\sum_{n=1}^{\infty }\frac{8n\cdot\zeta (2n)}{3\cdot 2^{2n}}=\zeta(2)$$ By using numerical calculation, I found this relationship between the values of zeta function at even integers and $\zeta(2)$, but this needs proving, any help?","['sequences-and-series', 'zeta-functions']"
1456819,Counting numbers,"How many numbers less than $1000$ can be formed using the digits $0,2,5,8,9$ when repetition is allowed. I proceeded as follows: 1 digit numbers: $5$ 2 digit numbers: $4\times5=20$ 3 digit numbers: $4\times5\times5=100$ So answer should be $100+20+5=125$. But the answer is given as $124$. What mistake am I committing?",['combinatorics']
1456837,Tile a 1 x n walkway with 4 different types of tiles...,"Suppose you are trying to tile a 1 x n walkway with 4 different types of tiles: a red 1 x 1 tile, a blue 1 x 1 tile, a white 1 x 1 tile, and a black 2 x 1 tile a. Set up and explain a recurrence relation for the number of different tilings for a sidewalk of length n. b. What is the solution of this recurrence relation? c. How long must the walkway be in order have more than 1000 different tiling possibilities? This is a problem on my test review and I have no idea how to approach it. We did a similar example in class but only using 1x1 tiles that were all the same (no separate tile colors or sizes). Any help/hints would be appreciated. Thanks in advance! My initial thought is something along the lines of finding all the ways to use the 1 x 1 tiles then multiplying that by 3 to consider each color variant (don't know how the 2x1 factors in to this though).","['discrete-mathematics', 'combinatorics', 'recurrence-relations']"
1456878,Divergence as trace of Levi-Civita connection,"In ""Problems and Solution in Mathematics"" by Ta-Tsien, 2nd Edition, exercice 3314, question b Exercise question For a vector field $X$ define the divergence of $X$, $\text{div}(X)$ as the trace of the operator $Y \rightarrow D_Y X$ where $D$ is the Levi-Civita connection. Find the expression for the divergence of $X$ in a local coordinate system $(x^1,...,x^n)$. Exercise answer Denote: \begin{equation}
X = \sum_i X^i \frac{\partial}{\partial x^i}
\end{equation} Then, by the definition of divergence, we have: \begin{equation}
\text{div}(X) = \sum_{k,l} \left<D_{\frac{\partial}{\partial x^k}} X, \frac{\partial}{\partial x^l} \right> g^{kl}
=
\sum_i \left(\frac{\partial X^i}{\partial x^i} + \sum_k \Gamma^i_{ki}X^k \right)
\end{equation} My question I see $Y \rightarrow D_Y X$ as a ""differential"" operator in which for example the derivatives of the components $X^i$ and $Y^i$ of the vectors fields will appear. On the contrary, a tensor is a ""local"" operator which only permits algebraic operations between those components. My question thus is: What is the coordinate expression of the tensor which trace is the divergence of $X$ ?","['differential-geometry', 'connections']"
1456882,Global sections when we tensor by a degree zero line bundle,"Let $C$ be a smooth projective curve over $\mathbb{C}$. Let $A$ be a degree $d$ line bundle on $C$, and $M$ be a degree 0 line bundle on $C$ such that $M^2=\mathcal{O}_C$, that is, it is a 2-torsion line bundle. Therefore, we have that $deg(A)=deg(A\otimes M)$. Is it in general true that $h^0(C,A)=h^0(C,A\otimes M)$? If we assume that $deg(A)>2g(C)-2$, we get that $h^0(C,A)=h^0(C,A\otimes M)$. But otherwise can we say anything? $\textbf{Edits}$ We have the subvariety $W^r_d(C)=\{A\in Pic^d(C)|h^0(C,A)\geq r+1\}$ of $Pic^d(C)$. A 2-torsion line $M$ acts on $Pic^d(C)$. Assume $0< d<2g-2$, and that $W^r_d(C)\setminus W^{r+1}_d(C)=\{A\in Pic^d(C)|h^0(C,A)=r+1\}$ is nonempty. Then will the action of $M$ on $Pic^d(C)$ take some open subset in $W^r_d(C)\setminus W^{r+1}_d(C)$ to itself?","['algebraic-geometry', 'riemann-surfaces', 'algebraic-curves']"
1456885,Negation of Proper Subset,"Is there a specific symbol to denote that a set is NOT a proper subset of another set? (In other words, that an element of the subset is not a member of the set, or that the subset is equal to the set itself).","['elementary-set-theory', 'notation']"
1457003,Elementary theorems that require AC,"It seems that AC is hiding (maybe concealed?) even in some elementary results.
An example: Theorem: Let $X \subseteq \mathbb R$ and let $x_0 \in \mathbb R$ be an accumulation point of $X$ . Then  there exists a sequence $ \{ a_n \}_{n=1}^\infty $ S.T. $ \{ a_n \} \subseteq X$ and $a_n\xrightarrow{n \to \infty} x_0 $ . Proof: For $\mathbb N \ni n > 0$ we denote $A_n := \{ x \in X :  |x-x_0| < \frac {1}{n} \}$ , since $x_0$ is an accumulation point of $X$ then $ \forall [ 0<n \in \mathbb N ] . A_n \neq \varnothing$ . By A.C. there exists choice function $f:P(X) \setminus \{ \varnothing\} \rightarrow X$ S.T. $\forall [ \varnothing\neq B \subset X] . f(B) \in B$ The sequence $\{ a_n \}$ defined by $a_n := f(A_n)$ satisfies the requirements. Can we avoid the use of AC in the Theorem above?? Can you point out some elementary Theorems that require AC?","['elementary-set-theory', 'alternative-proof', 'proof-verification', 'axiom-of-choice']"
1457015,A prime-finding constant,"Consider $\lfloor 10^n \times 0.731926765612646213686753345587262244668218433356357832021 \rfloor$. For each $n$, reverse the digits. If the number isn't already prime, find the next prime. For the first 24 values of $n$, the result is prime. This start is based on the record left-truncatable prime 357686312646216567629137. After that, one must add the following values to get a prime number
 $(12, 0, 16, 2, 12, 20, 2, 2, 0, 14, 0, 2, 12, 6, 6, 10, 10, 10, 16, 12, 10, 4, 14, 2, 10, 12, 2, 0, 2)$ All values bounded above by 20. If this could go on (it doesn't), it would provide a solution for the finding primes Polymath project . How far can something like this be extended for a given upper bound?  For $n$ going from 1 to 100, what constant minimizes the farthest distance to the corresponding 1 to 100 digit prime? A similar question -- what constants work best without the reverse-digits step? For upper bound 0, $\lfloor 10^n \times .73939133 \rfloor$ could be used to produce primes for $n=1..8$. How far could this be extended with an upper bound of 10? Is the following provable? For any $0<k<1$ and any positive integer $d$ there exists an $n$ such that the following values are composite. $$\lfloor 10^n \times k \rfloor, \lfloor 10^n \times k \rfloor+1, ..., \lfloor 10^n \times k \rfloor +d$$ Added results. Prime to $n=31$, 
 $\lfloor 10^n \times .19992191983120858559939829 \rfloor$ + $(1, 0, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 0, 2, 0, 2, 2, 1, 1, 2)_n$ Prime to $n=43$, 
 $\lfloor 10^n \times .2249916676947644237216638216097398882368887 \rfloor$ + $(0, 1, 3, 2, 2, 2, 1, 1, 0, 1, 0, 3, 2, 1, 3, 3, 1, 0, 2, 1, 0, 3, 3, 0, 1, 1, 0, 1, 1, 0, 2, 0, 0, 3, 1, 3, 1, 0, 1, 1, 3, 1, 0)_n$ Reverse prime to $n=50$, 
 $\lfloor 10^n \times .79946519968197358169933425438229277782582723528486 \rfloor$ + $(0,0,0,2,0,0,2,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,2,2,0,0,2,0,2,2,2,0,2,2,0,2,0,0,2,0,2,2,2,0,2,2,2,0,0)_n$ Reverse prime to $n=64$, 
 $\lfloor 10^n \times .1319788512835399616489432175746323115633163831863127383662373426 \rfloor$ + $(1,0,0,2,2,2,0,2,0,2,2,2,0,0,0,0,0,2,0,2,0,2,2,2,2,2,0,2,0,2,0,0,0,0,0,8,8,6,8,6,2,8,8,0,8,2,0,6,8,2,8,2,8,0,6,6,0,0,6,8,6,8,0,2)_n$","['prime-numbers', 'number-theory', 'recreational-mathematics']"
1457017,Convergence in Probability of a sequence whose variance is going to 0,"Let $X_n \ge 0$ be a sequence of random variables (which are not necessarily independent) satisfying the following conditions: $E(X_n) = \mu_n$ where $l \le \mu_n \le u$ for some constants $l, u < \infty$ (but we don't know whether $\mu_n$ converges to a constant). $\lim_{n \rightarrow \infty} V(X_n) \rightarrow 0$ as $n \rightarrow
   \infty$. By Chebyshev's inequality, we know that $|X_n - \mu_n| \rightarrow 0$ in probability. Can we say something probabilistic about $X_n$ being in the interval $[l,u]$ with high probability? Thanks! EDIT: I've erased a part where I asked whether we can say $X_n \rightarrow \xi$ for $\xi \in [l,u]$, which is not true, as per saz's comment.","['probability-theory', 'probability', 'probability-limit-theorems']"
1457056,Extend a Riemannian metric defined on the boundary.,"If $M$ is a (compact) Riemannian manifold with nonempty boundary and I have a Riemannian metric defined on $\partial M$, Is it possible to obtain a Riemannian metric on the whole $M$ extending the one on the boundary? I think that it is possible to extend it to a tubular neighborhood of the boundary by taking it to be diffeomorphic to $\partial M \times [0,1)$. What about the rest of the manifold?","['differential-geometry', 'riemannian-geometry']"
1457063,Set theory $|A\cup B|=|A|+|B|-|A\cap B|$,"I am utterly confused on how to solve this problem. I found a lemma that says $|A\cup B|=|A|+|B|$ is true if the two sets are disjoint which makes sense, but how do I prove the entire statement.",['elementary-set-theory']
1457065,What does the Grothendieck's period conjecture mean?,"I would like to know how is the Grothendieck's period conjecture about algebraic cycles, defined explicitly ? and, what link has it with the Hodge conjecture for smooth complexe algebraic projective variety ? Which kind of books available on the net, nead we to learn to become able to understand easily this conjecture ? Thanks a lot for your help.","['algebraic-geometry', 'sheaf-cohomology', 'schemes', 'open-problem']"
1457080,Prove that a bounded sequence has two convergent subsequences.,"Let $a_1,a_2\dots$ be a bounded sequence in $\mathbb{R}$ that does not converge. Prove that the sequence has two subsequences that converge to different limits Here is my proof:
Let $s_n$ be a bounded sequence in $\mathbb{R}$ that does not converge. Then by the Bolzano Weierstrass theorem there exists a subsequence, $s_{n_k}$, that converges. Let $a$ be the limit of $s_{n_k}$. By definition $s_n$ does not converge so it can not converge to $a$. That is, there exists $\epsilon >0$ such that for all $N\in\mathbb{R}$, there exists $n>N$ such that $|s_n-a|\ge\epsilon$. So there exists a sequence $s_m$ that is a subsequence of $s_n$ with $m>N$. By its definition $s_m$ does not have a subsequence that converges to $a$. However, $s_m$ is bounded because $s_n$ is bounded. So by the Bolzano Weierstrass theorem there exists a subsequence, $s_{m_j}$, that converges.
Clearly $s_{m_j}$ does not converge to $a$. $\square$ Am I correct in saying ""By its definition $s_m$ does not have a subsequence that converges to $a$."" This is what my proof relies on, but I'm not sure I have justified it correctly. Thanks in advance for any input!","['sequences-and-series', 'convergence-divergence', 'real-analysis', 'proof-verification']"
1457129,How do I calculate $\lim_{x\to+\infty}\sqrt{x+a}-\sqrt{x}$?,"I've seen a handful of exercises like this: $$\lim_{x\to+\infty}(\sqrt{x+a}-\sqrt{x})$$ I've never worked with limits to infinity when there is some arbitrary number $a$. I am not given any details about it. Apparently the answer is $0$. How was that conclusion reached? My guess is that since $x = +\infty$, the result of $x + a$ will still be $+\infty$ so we would have $\sqrt{x}-\sqrt{x} = 0$. But that doesn't convince me. For starters,  we don't know what $a$ is: it could be $-\infty$ or something, so $\infty - \infty$ would be indeterminate...","['radicals', 'infinity', 'calculus', 'limits']"
1457135,Determine whether the following function is (a) injective and (b) surjective,"the function is as follows: $f:\mathbb{R} \rightarrow \mathbb{Z}$ defined by $f(x) =$ the least integer greater than or equal to $x$. here is what I have for the proof of injective: Suppose $f(x)=f(y)$ that is, the least integer $\geq x =$ the least integer $\geq y$ I thought that this pretty obviously implied that $x=y$. I am unsure as to how to begin the proof of surjective. Can someone help?","['real-analysis', 'functions']"
1457152,"Prove/Disprove (alphabet, languages, subset)","I'm trying to prove or disprove the following, but am having some trouble. For all $Z \subseteq \Sigma^*$, $ZZ \subseteq Z$, where $\Sigma$ is some alphabet. For all $Z \subseteq \Sigma^*$, $Z \subseteq ZZ$, where $\Sigma$ is some alphabet. Here I believe that $ZZ$ is simply the concatenation of $Z$ with $Z$ so for the first one if we say the set $Z$ contains $(a,b,c)$, then $ZZ$ would be $(aa,ab,ac, ba,bb,bc,ca,cb,cc)$ and as such $ZZ$ is not a subset of $Z$. Is my reasoning correct? Also using the same reasoning would the second one also be false. Thanks!","['elementary-set-theory', 'discrete-mathematics']"
1457160,How many numbers are in this set?,"Let M a positive integer is called “Minos”  if in its binary representation, any two consecutive appearance of digit 1 are separated by 2 or more 0. Example 36= 100100 (binary) is “Minos” number, but 37=100101 not. How many nonnegative integers that can be represented as binary sequence of length 20 (leading zeros allowed) are ‘Minos’? My tough: C=# of ceros
N=# of ones
T=# total *) Two ceros always must be together, when are all 0 . In this case the number 0 is Minos. *) Now 1 one and 19 ceros 20 different numbers. *) Now 2 ones and 18 ceros then 20Cn2 
In conclusion I’m thinking the solution will be all the sum of all the combination of numbers taken in group of 2. Note: Regarding my answer, I posted because something doesn’t sound quite right, and I cannot see how can I proceed to calculate the correct answer, and how lead into it. Thanks.","['binary', 'combinatorics']"
1457184,A question about Measurable function,"Let $f$ be a real-valued Lebesgue measurable function on $\mathbb{R}$. Prove that there exist Borel measurable functions $g$ and $h$ such that $g(x)=h(x)$ almost everywhere and $g(x)\le f(x) \le h(x)$ for every $x \in \mathbb R$.
I know that $f$ is measurable since there exists a sequence of simple function that converges to $f$. I have no further idea how to tackle this problem.","['lebesgue-measure', 'measure-theory', 'real-analysis', 'analysis', 'lebesgue-integral']"
1457220,Finishing my $\epsilon - \delta$ proof,"Hi fellas I have this limit: $$lim_{x\to-1}\dfrac{x^2-1}{x^2+x}=2$$
$Dem:$ Let $\epsilon >0$ random but fixed then: $$|\dfrac{x^2-1}{x^2+x}-2|=|\dfrac{x^2-1-2x^2-x}{x^2+x}|=|\dfrac{-x^2-2x-1}{x^2+x}|=|\dfrac{(x+1)^2}{x(x+1)}|=|\dfrac{x+1}{x}|<M|x-1|<M(\dfrac{\epsilon}{M})=\epsilon$$ So I have to choose M such that $|\dfrac{1}{x}|<M$ Let's suppose that $|x+1|<c$ for some $c$. (My problem is that I don't know how to choose this $c$) So $|x-1|<c \Rightarrow -c<x-1<c$ and so $-c-1<x<c-1$. The problem is that for a little $c<1$, $-c-1$ and $c-1 $ are both negative and for a bigger c, $-c-1$ is always negative and so I can't find my beloved $M$.","['calculus', 'limits', 'epsilon-delta']"
1457249,Confusion about localization,"I am a bit confused about the following result I read:
Let $(\Omega,\mathfrak{A},\mathfrak{F},\mathbb{P})$ be a filtered probability space. Let $\left\{X_t\right\}_{t\in[0,T]}$ be a continuous and adapted process on this space taking values in $\mathbb{R}$. Then $X$ is locally bounded in the sense that there is a localizing sequence $(\tau_n)_{n\in\mathbb{N}}$ of stopping times (which satisfies $\tau_k\leq\tau_{k+1}$ for $k\in\mathbb{N}$ and for each $\omega\in\Omega$ there is some $n_\omega\in\mathbb{N}$ such that $\tau_n(\omega)=T$ for all $n\ge n_\omega$) such that the stopped process $X(\cdot\wedge\tau_k)$ is bounded (in fact: $|X(t\wedge\tau_k)|\leq k$). Now, my confusion comes: Let's take the filtration $\mathfrak{F}$ to be constant $\mathfrak{A}$. Suppose there is some $Z\in L^{1}(\mathbb{P})\setminus L^2(\mathbb{P})$. If I then define the continuous, adapted process $X$ by $X_t\equiv Z$ I arrive at the local boundedness of this process: For the described sequence of stopping times I get $|Z|=|X(t\wedge\tau_k)|\le k$. But doesn't this imply that $Z$ is square-integrable which I excluded? Can somebody tell me why and where I am wrong? Thank you very much in advance!","['probability-theory', 'stochastic-calculus', 'stochastic-processes']"
