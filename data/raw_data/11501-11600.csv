question_id,title,body,tags
89069,How to explain what is wrong in this?,"My friend showed this to me and I instantly know that this is wrong. However, I cannot explain why this is wrong to my friend. Question. Prove $\displaystyle \frac{100-100}{100-100} = 2.$ Answer. $$\begin{align*}
\frac{100-100}{100-100} &= \frac{(10)^2 - (10)^2}{10(10-10)}\\
&= \frac{(10+10)(10-10)}{10(10-10)}\\
&= \frac{20}{10} = 2.
\end{align*}$$ My argument is that in the third step, where it goes like this: 
$$\frac{(10+10)(10-10)}{10(10-10)}$$
you cannot just cancel out the $(10-10)$ - it doesn't seem right. However, I am at a loss of explaining why exactly you cannot do that and my friend has the argumentative power (is that even a word? I mean he is good with arguments, even if they are not facts) and he has me confused to the point that I am starting to think it can be done. Can anyone please explain why this is wrong? Thanks.","['algebra-precalculus', 'fake-proofs']"
89079,Prove the centralizer of an element in group $G$ is a subgroup of $G$,"We have a group $G$ where $a$ is an element of $G$. Then we have a set $Z(a) = \{g\in G : ga = ag\}$ called the centralizer of $a$. If I have an $x\in Z(a)$, how do I go about proving that the inverse of $x$, $x^{-1}$, is also an element of $Z(a)$? I have already proved step 1, the subgroup test: I just need step 2, described above, and I have no idea how to start.",['group-theory']
89080,How would I approach this identity: $\cos^3x+\sin^3x=(\cos x+\sin x)\cdot(1-\sin x\cdot\cos x)$?,"Prove the identity:
$$
\cos^3x+\sin^3x=(\cos x+\sin x)\cdot(1-\sin x\cdot\cos x)
$$
I was able to change both sides to $\cos x-\cos x\cdot\sin^{2}x+\sin x-\sin x\cdot\cos^{2}x$, which is kind of long. Is there a shorter way, such as factoring the left side? If so, how can I do it?","['trigonometry', 'algebra-precalculus']"
89107,Formula or code to compute number of subgroups of a certain order of an abelian $p$-group,"Given a finite abelian $p$-group and its factorization into groups of the form $\mathbb{Z}/p^k\mathbb{Z}$, does anyone know of a formula that gives the number of subgroups of a certain index/order? As I'm sure such a formula would contain some nasty product or sum, is there a computer algebra system out there that knows how to compute this?","['computer-algebra-systems', 'group-theory']"
89112,On a double sum involving prime numbers,"$$\sum_{i,j=1}^{\infty}\left[\frac{x}{p_ip_j}\right]=x\sum_{p_ip_j\leq x}\frac{1}{p_ip_j}+O(x),p_i< p_j$$
  where $p_i$ is the $i$th prime, and ""[ ]"" represents the largest integer not exceeding... I don't know how to deal with it. Could you give me a proof?","['analysis', 'analytic-number-theory', 'number-theory']"
89114,"What does it mean when it says, ""Reduced cholesterol by 10%?""","When I read often quoted claims like, ""It will reduce your cholesterol by 10%."" Just exactly how much is 10%? 10% out of what? I am guessing that suppose I have cholesterol of 300mg/dL and I am told this product will reduce mine by 10%, does it mean that 10% of 300mg/dL would become 270mg/dL where 10% of 300 is 30 and I subtract 30 from 300 and it becomes 270?","['arithmetic', 'algebra-precalculus']"
89128,How many states in the game of hex?,"I am trying to calculate how many unique states are possible to be in during a game of hex. The upper bound for an $n\times n$ board is $3^{n^2}$. This is ignoring gameplay and simply considering that each space could be any of $3$ states. That number contains many states (all black, all white, etc..) that are impossible to reach in any actual game. In a real game, the constraint will be added that the number of black spaces can be at most one greater than the number of white spaces. I cannot think of a way to quantify that number of states. The number is also reduced by winning states for either problem, which effectively stop any subsequent paths. I have considered that the first move can be any of $n^2$ spaces, the second $n^2-1$...
This gives $(n^2)!$ number of states which is even bigger than my upper bound because of duplicate configurations happening on different paths. I don't care about the path to the state, just what the board looks like. How many board configurations are really possible on an $n\times n$ board?","['recreational-mathematics', 'combinatorics']"
89137,Arnold's Trivium problem 52,Calculate the first term of the asymptotic expression as $k \to \infty$ of the integral $$ \int_{-\infty}^{+\infty}\frac{e^{ikx}}{\sqrt{1+x^{2n}}}dx $$ May I bother you to explain what the problem is asking and what is the intuition behind in this problem? It is from the problem 52 of the article A mathematical trivium by Arnol'd.,"['calculus', 'intuition', 'complex-analysis']"
89152,An application of Eisenstein's criterion,"So, this is Hungerford problem 9 on page 166. Here's the problem in full: Let $f(x) = \sum_{i=0}^n a_i x^i \in \mathbb{Z}[x]$. Suppose that for some $k$, $0 < k < n$, and some prime $p$ such that $p \nmid a_n$, $p \nmid a_k$, and $p \mid a_i$ for all $i = 0, \dots, k-1$, but $p^2 \nmid a_0$. Show that $f$ has a factor of degree at least $k$ that is irreducible in $\mathbb{Z}[x]$. Here's my progress: If we construct a new polynomial $g$ where $g$ is $\sum_{i=0}^k a_i x^i$ , we know by Eisenstein's criterion that $g$ is irreducible in $\mathbb{Q}[x]$. If we somehow knew that $g$ was irreducible in $\mathbb{Z}[x]$ too (that would follow if $g$ were primitive), then we could say that the smallest thing we could possibly factor out of $f$ that isn't a unit has to be of degree $k$ and that $g$ must be a factor of it. I tried writing $f$ as $C(f) \cdot f_1$ where $C(f)$ is the content of $f$, so that we'd be dealing with a primitive $f_1$ (and of course, the nice things about $g$ don't go away since $p$ doesn't divide every term), but this doesn't necessarily result in $g$ being primitive (right?)! So, here's where I'm stuck: how can I show I can even factor $f$? How can I show that $g$ is irreducible in $\mathbb{Z}[x]$, whether by showing it's primitive or by some other means? Thanks so much, everyone!","['abstract-algebra', 'polynomials']"
89154,Eigenvalues of block matrix with a zero diagonal block,"I'm stuck on finding the eigenvalues of
$$
\bar{A} = 
\begin{bmatrix}
0 & S\\
S^\top & A
\end{bmatrix}
$$
Both $S$ and $A$ are square matrices of the same dimension and are invertible. $A$ is symmetric positive definite. Any help is appreciated. :-D","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors', 'block-matrices']"
89204,Characterization of Horizontal Irreducible Divisors,"I´m asking for a proof of a fact used by Arakelov in his paper: Intersection Theory of Divisors on an Arithmetic Surface (page 1169 row 16). He gives no references or explanations for this fact. The setup is the following, $X$ is a smooth complete curve over a number field $K$ with ring of integers $\Lambda$. $V$ is a regular complete model of $X$ over $Spec(\Lambda)$, i.e. a regular fibered arithmetic surface with a proper morphism $f:V \rightarrow Spec(\Lambda)$ such that the fiber over the generic point is the original curve $X$. $D$ is an irreducible horizontal divisor on $V$, i.e. a prime divisor in Hartshorne´s notations such that $f(D)=Spec(\Lambda)$. The claim is the following. There exists a finite field extension $L/K$ with ring of integers $\Gamma$, and a $Spec(\Lambda)$-morphism $\epsilon: Spec(\Gamma) \rightarrow V$ such that $\epsilon(Spec(\Gamma))=D$. And this is my work. My idea was to prove some finiteness condition on the map $f|_D$ in order to use the fact, stated for example in Matsumura´s Commutative Ring Theory, that an extension of rings $A \subset B$ is integral if and only if every element of $B$ is contained in a finitely generated $A$-module. Proceeding in that direction I proved that $f|_D$ is of finite type (clear because $f$ is proper), it is generically finite (because by irreducibility of $D$ its preimage on the generic point is a single closed point of $X$) and it is dominant (because $D$ is 
horizontal). Using Exercise 3.7. of Hartshorne we get that there exists an open dense subset $W\subseteq Spec(\Lambda)$ such that $f|_D:f|_D^{-1}(W) \rightarrow W$ is finite, moreover the function field $K(D)$ of $D$ is a finite field extension of $K$ (so my guess is $L=K(D)$, also because I don´t see any other possible candidate). Unfortunately I have no idea on how to proceed further. There any suggestions? (or complete proves... :) ) Thank you in advance! Edit 1: According to the excellent answer of Matt there is only one point that still bother me. In order to use the Valuative Criterion we need $\Gamma$ to be a Valuation Ring , but to be the ring of integers of a number field is clearly not enough. For example $\mathbb{Z}$ is not a Valuation Ring. So, how can we prove that $\Gamma$ is a Valuation Ring? Edit 2: This is the answer to the question in Edit 1. The proof was sketched by Matt E in a comment to his own answer, I have just expanded it with details (and maybe errors!). I write it here because it could be useful and because I hope that, if my checkings are wrong, someone could point it out! We have a map $\varphi: Spec(L) \rightarrow V$ and we want extend it to a map $Spec(\Gamma) \rightarrow V$. Since $f$ is proper it is of finite type and the point $P$ has a neighborhood of the form $Spec(A)$ such that $A$ is a finitely generated $\Lambda$-algebra. Moreover by integrality of $V$ the fraction field of $A$ is (at least) a subfield of $L$. In specific there is an extension of algebras $A \rightarrow \Lambda[l_1,...,l_n]$ where $l_1,...,l_n \in L$. Define $\lambda$ to be the common denominator of the $l_i$, $\lambda \in \Gamma$. Then the map $A \rightarrow L$ factorizes through $Spec(\Gamma[\frac{1}{\lambda}])$ and we have a natural extension of $\varphi$ to $\hat{\varphi}:Spec(\Gamma[\frac{1}{\lambda}]) \rightarrow Spec(A) \subset V$. Now let $p_1,...,p_m$ the primes of the factorization of $\lambda$ in $\Gamma$. We have natural extensions of $\varphi$ to $\varphi_i:Spec(\Gamma_{p_i}) \rightarrow V$ because of the Valuative Criterion for Properness (Hartshorne, Theorem II.4.7). And because the localization of a Dedekind Domain to a prime ideal is a Discrete Valuation Ring. Now we want to glue together all those morpisms. It is clear that $Spec(\Gamma)$ is covered by the images of $\hat{\varphi}$ and $\varphi_i$, and the intersection of the image of any two such morphisms is $(0)=Spec(L)$. But they are constructed as extension of $\varphi:Spec(L) \rightarrow V$ so they agree on their intersection. By the proof of Theorem II.3.3 of Hartshorne they give rise to a well defined map $\epsilon: Spec(\Gamma) \rightarrow V$ and we are done.","['arithmetic-geometry', 'algebraic-geometry']"
89211,How to Understand the Definition of Cardinal Exponentiation,"I'm having trouble understanding the definition of cardinal exponentiation. Let's start with the definitions / claims I've been given: For any finite sets $A,B$, such that if $|A|=a$ and $|B|=b\neq 0$ then the number of functions from $A$ to $B$ is $b^a$. ( I'm guessing this is only used when both sets are finite ). For any sets $A,B$, $B^A$ is the set of functions from $A$ to $B$. For every set $A$, $|P(A)|=|\left \{ 0,1 \right \}^A|$ Now for the definition and why I don't understand it: Let $a,b$ be cardinal numbers.  Let $A,B$ be sets such that $|A|=a, |B|=b$.  Then $|B^A|=b^a$. How analagous to normal exponentiation is this and where does in differ when we introduce cardinals like $\aleph_0$ or $C$? Is $2^2=4$ and is $\frak c^2=c\cdot c=c\times c$?","['cardinals', 'elementary-set-theory']"
89219,Complex Analysis: Radius of convergence of Power Series,"Let p be a polynomial of degree $k>0$. Prove that $\sum p(n)z^n$ has radius of convergence $1$ and that there exists a polynomial $q(z)$ of degree $k$ such that $$\sum_{n=0}^{\infty} p(n) z^n=q(z)(1-z)^{-(k+1)}, \qquad  (|z|<1)$$
I've shown the radius of convergence is $1$; not sure how to apporach the second part.","['power-series', 'convergence-divergence', 'complex-analysis']"
89227,Does $A$ homeomorphic to $B$ imply $f^{-1}(A)$ is homeomorphic to $f^{-1}(B)$?,"Let $X$ and $Y$ be topological spaces, and $f:X\to Y$ a continuous map. Is the following true: If  $A$ and $B$ are two homeomorphic subspaces of $Y,$ then $f^{-1}(A)$ and $f^{-1}(B)$ are homeomorphic subspaces of $X$.",['general-topology']
89246,Splitting field of a separable polynomial is separable,"Probably a stupid question, but.. Why is the splitting field of a separable polynomial necessarily separable? Thanks. Follow up question Show that if $F$ is a splitting field over $K$ for $P \in K[X]$, then $[F:K] \leq n!$ I've proven this by induction on $n$, but I'm convinced there's a more algebraic approach ($n!$ screams $S_n$). If I knew $P$ were separable, then I'd know $F$ was Galois, so $ |\mbox{Gal}(F/K)| = [F:K] $. Considering the action of $ \mbox{Gal}(F/K) $ on the roots of $P$, we'd get an injective homomorphism into $ S_n $, giving the result. But $P$ is not given to be separable, so $P$ could have repeated roots when factorised in $F$. If $G$ is any finite group of $K$-automorphisms of $L$, then I know that $[F:K] \leq |G| $. Considering the action of $G$ on the set of roots of $P$, say $\Omega$, we get an injective homomorphism of $G$ into $S_{|\Omega|}$, where $|\Omega| \leq n$ (I think). I have a feeling I'm wrong here, since I think $G$ is forced to be $\mbox{Aut}(F/K)$. Any advice would be appreciated.","['galois-theory', 'abstract-algebra', 'field-theory']"
89254,"Champernowne-like squares, are there any?","I read about the Champernowne constant on Wikipedia a couple of days ago, and I got curious about something similar: is there some ""Champernowne-like"" number; that is, a concatenation of all numbers up to some $n \ge 2$ (like $1234567891011$), that is a perfect square? I've done a computer search up to $n=2000$, but I haven't found any. Are there any? If not, how may we prove this? Any thoughts on this are welcome!",['number-theory']
89263,Understanding a proof of Komlós's theorem,"I'm reading a book about probability theory and they use a certain theorem, called Komlós's theorem, which states: For a sequence $ (\xi_n) $ of random variables on $ (\Omega,\mathcal{F},P) $ with $\sup_n E|\xi_n| < \infty $. Then there is a random variable $ \zeta \in L^1$ and a subsequence $ (\zeta_k) = (\xi_{n_k}) $ such that
  $$ \frac{\zeta_1+\cdots+\zeta_k}{k} \to \zeta \text{ a.s. }\tag{1}$$
   Moreover the subsequence $ (\zeta_k) $ can be chosen in such a way that its further subsequence will also satisfy (1). So I found a proof of this theorem in the book ""Two-Scale Stochastic Systems"" of Yu. Kabanov and S. Pergamenshchikov. The proof of the theorem can be found in the Appendix, on page 250. Unfortunately, it is not available online. However, I hope there's someone who owns this book and could help me. The point, where I got stuck is on page 253. It's clear that we are able to choose this increasing sequence $ n_k $ such that for all $ n \ge n_k $ $$ E\eta^2_k \le E(\xi^{(k)}_n)^2 +2^{-k} \text{ and }|E(\xi^{(k)}_n-\eta_k | \gamma_{j_1},\dots,\gamma_{j_m})| \le 2^{-k}$$ for all $ m\le k-1, j_1<j_2<\dots<j_m $, with $ \gamma_j:= D_j(\xi^{(j)}_{n_j}-\eta_j)$. Just for completeness, we set $ \zeta_k:= \xi_{n_k} $. Now I get confused, about the following 3 things: Why is $ |E(\gamma_k \mid \gamma_1,\dots,\gamma_{k-1})|\le 2^{-k+1} $ — the above inequalities hold for $ \xi^{(k)}_n-\eta_k $ instead of $ \gamma_k $? What follows the first two inequalities is not clear: $$ \sum_{i=1}^\infty\frac{1}{k^2}E\gamma_k^2 \le 2\sum_{i=1}^\infty\frac{1}{k^2}E(\zeta_k^{(k)}-\eta_k)^2+ O(1) \le 4 \sum_{i=1}^\infty\frac{1}{k^2}E(\zeta_k^{(k)})^2 +O(1) < \infty.$$ In the last inequality, just calculating: $$ E(\zeta_k^{(k)}-\eta_k)^2 = E(\zeta_k^{(k)})^2 +2 E\,\zeta_k^{(k)}\eta_k + E\eta_k^2 \le 2 E(\zeta_k^{(k)})^2 +2 E\,\zeta_k^{(k)}\eta_k + 2^{-k}. $$ So the term $ 2^{-k} $ can be controlled, but I don't know how to bound the term $ E\,\zeta_k^{(k)}\eta_k$. I would appreciate it very much if someone could explain what's going on here. thx & cheers math Since it seems to be difficult, I state the lemma's which the authors need for the proof. I cite: Lemma 1 : Let $ \eta _n $ be a sequence of random variables convergent 
  weakly in $ L^2 $ to a random variable $ \eta $. Then
  $$ E|\eta| \le \lim\inf E|\eta_n| \tag{2}$$
  $$ E|\eta|^2 \le \lim\inf E|\eta_n|^2 \tag{3}$$ Now a definition: $$ \xi^{c}:=\xi 1\{|\xi|\le c\} $$
$$ D_m(\xi):=\sum_{i=-\infty}^\infty i2^{-m} 1\{\xi\in (i2^{-m},(i+1)2^{-m}]\} $$ They call them truncation and discretization operators on $ L^0 $. Lemma  2 : Assume $ \sup_nE|\xi_n| < \infty $ and for every $ k \in 
 \mathbb{N} $ the sequence $ (\xi_n^{(k)}) $ converges weakly in $ L^2 $ to a  random variable $ \eta_k $. Then there exists $ \eta \in L^1 $ such that $ 
 \eta_k $ tends to $ \eta $ a.s. and in $ L^1 $. And the last lemma Lemma 3 : Let $ \mathcal{G} $ be a $ \sigma $-algebra generated by a finite partition $ A_1,\dots,A_N $ with $ A_i \in \mathcal{F}$. Assume that a sequence of random variables $ (\xi_n) $ converges weakly in $ L^2 $ to zero. Then for any $ \epsilon >0$ there exists $ n_0 =n_0(\epsilon) $ such that 
  $$ E(\xi_n|\mathcal{G})\le \epsilon  $$
  for all $ n\ge n_0 $.",['probability-theory']
89284,Maximal ideals of an algebra,How do I find the maximal ideals of the algebra of holomorphic functions in one variable?thanks.,"['complex-analysis', 'abstract-algebra']"
89287,An application of Gronwall's lemma,"I've come across a creative use of Gronwall's lemma which I would like to submit to the community. I suspect that the argument, while leading to a correct conclusion, is somewhat flawed. We have a continuous mapping $g \colon \mathbb{R}\to \mathbb{R}$ such that $$\tag{1} \forall \varepsilon>0\ \exists \delta(\varepsilon)>0\ \text{s.t.}\ \lvert x \rvert \le \delta(\varepsilon) \Rightarrow \lvert g(x) \rvert \le \varepsilon \lvert x \rvert$$ and a continuous trajectory $x\colon [0, +\infty) \to \mathbb{R}$ such that $$\tag{2} e^{\alpha t}\lvert x(t)\rvert \le \lvert x_0\rvert+\int_0^t e^{\alpha s}\lvert g(x(s))\rvert\, ds. $$ Here $x_0=x(0)$ is the initial datum, which we may choose small as we wish, but $\alpha >0$ is a fixed constant that we cannot alter in any way. Now comes the point. Fix $\varepsilon>0$. The lecturer says: Suppose
  we can apply (1) for all times $t \ge 0$. Then inserting (1) in (2)
  we get $$e^{\alpha t}\lvert x(t) \rvert \le \lvert x_0\rvert + \varepsilon \int_0^t e^{\alpha s} \lvert x(s)\rvert \, ds$$ and from Gronwall's lemma we infer $$\tag{3} \lvert x(t)\rvert \le  e^{(\varepsilon - \alpha)t}\lvert x_0\rvert.$$ So if $\varepsilon <\alpha$ and $\lvert x_0 \rvert < \delta(\varepsilon)$, $\lvert x(s) \rvert$ is small at all times and
  our use of (1) is justified . We conclude that inequality (3) holds. Does this argument look correct to you? I believe that the conclusion is correct, but that it requires more careful treatment. Thank you.","['ordinary-differential-equations', 'inequality', 'analysis']"
89290,Origin of the notation $(x-h)$ and $y-k$ in shifting,Does anyone know the origin of the notation $(x-h)$ and $(y-k)$ when shifting functions in algebra? Why $h$ and $k$?,"['notation', 'algebra-precalculus']"
89291,$f(x)$ and $h(x)$ are absolutely continuous functions. Is $e^{f(x)} |h(x)|$ as well?,"Given that functions $f(x)$ and $h(x)$ are absolutely continuous on $[0,1]$, I want to show that $e^{f(x)} |h(x)|$ is absolutely continuous as well. I know that (1) the product of two absolutely continuous function on $[0,1]$ is absolutely continuous. 
(2) the composition of a Lipschitz continuous function and an absolutely continuous function is absolutely continuous. So $|h|$ is absolutely continuous, But the exponential function $e^x$ is not Lipschitz, so not absolutely continuous. What's the key to solve the problem here?",['real-analysis']
89294,Proof skew-symmetric matrix $A$ exhibits $\vec{x}^T A \vec{x} = 0$,"The condition that $P'^TFP$ is skew-symmetric is equivalent to $\vec{X}^TP'^TFP\vec{X} = 0$ for all $\vec{X}$. [1] The authors say that if A is skew-symmetric than $\vec{x}^T A \vec{x} = 0$. Sadly, there is no further proof of that. Question: Other than expanding the equation (which I did) is there a different proof that holds for any sized skew-symmetric matrix? If so, what is the proof? My thoughts: I can only assume that because (as Wikipedia tells me) a skew-symmetric matrix of odd-size has one eigenvalue equal to zero, there always exists a kernel such that $A\vec{x} = \vec{0}$ but that would be specific to odd-sized matrices and would not hold for all $\vec{x}$. Source: [1] Multiple View Geometry, p.255, Second Edition 2004, Hartley & Zisserman, CUP (Section canonical cameras given F)","['matrices', 'linear-algebra']"
89307,Prove that if $f(x)$ is continuous in $\mathbb R$ then exists $x$ such that $f(x)f(x+1) \ge 0$,I have a homework question to prove that if $f(x)$ is continuous in $\mathbb R$ then exists $x$ such that $f(x)f(x+1) \ge 0$. I am failing to see why this is true and how I can prove this. Can someone please help me out? Thanks :),['calculus']
89330,Can an Ordered Ring be Considered as a Metric Space?,"Consider an ordered ring $R$. One can define a function $$|\cdot|: R \rightarrow R$$ 
by
$$
|x| = \left\{ 

\begin{align}
x &, x \geq 0 \\
-x&, x < 0
\end{align}

  \right.
$$
If we set $d(x,y) = |x - y|$, it can be shown that $d$ satisfies: $d(x,y) \geq 0$ and $d(x,y) = 0$ iff $x=y$ $d(x,y) = d(y,x)$ $d(x,z) < d(x,y) + d(y,z)$ Here, $d$ looks very much like a metric except for the fact that $d$ actually produces an element of $R$, not an element $\mathbb{R}$. Otherwise, $(R, d)$ seems to have all of the features one would expect of a metric space. So, is there a way in which $(R, d)$ can be considered a metric space? Using this ""metric"", we can define Cauchy sequences and other items of interest but since the ""metric"" doesn't actually produce an element $\mathbb{R}$, anything that depends on the output of $d$ being a number would fail.","['abstract-algebra', 'real-analysis']"
89337,"Probability of a number appearing in another number , like 31 in 2315?","How likely is it that a number of consisting of n digits, contains a number consisting of n digits or less? I though that perhaps I could multiply the number of permutations by the chance of such a permutation occuring (as displayed below), but some permutations allow for others to occur simultaneously. Doesn't that make my calculation invalid? Please note: I only want to calculate the chances of a number appearing at all, doesn't matter  if that is once, twice or more.","['probability', 'combinatorics']"
89338,Two $L^p$ space problems,"I have two problems here. 1) I need to show $\|xf(x)\|_2 \le \|f(x)\|_3$ given $f \in L^3[0,1]$. My approach: I know $\|f(x)\|_2 \le \|f(x)\|_3$, and hope to show $\|xf(x)\|_2 \le \|f(x)\|_2$. That's true because $x \in [0,1]$, $xf(x) \le f(x)$. Am I right? 2) If $f \in L^{5}(E)$, $E=[0,1]$ and $\int_{E} f(x)dx=0$, then $\int_{E} |1-f|^5 dx \ge 1$. I think the key is to define $g=1-f$ and use $\|g\|_p \ge \|g\|_2$ for $p \ge 2$. Please give me any confirmation or tell me wrong.","['measure-theory', 'real-analysis']"
89352,What is the Picard group of $z^3=y(y^2-x^2)(x-1)$?,"I'm actually doing much more with this affine surface than just looking for the Picard group. I have already proved many things about this surface, and have many more things to look at it, but the Picard group continues to elude me. One of the biggest problems seems to be that I'm not really sure what tools I have at my disposal to attempt such a problem. This surface has 4 singularities, one of which (the origin) is particularly nasty (the exceptional fiber over the origin when blowing-up is an elliptic curve). Let $$X=\mathcal{Z}(z^3-y(y^2-x^2)(x-1))$$ be the surface. I know that the divisor class group of the surface is $\mathrm{Cl}(X)\cong (\mathbb{Z}/3\mathbb{Z})^{3}\oplus \mathbb{Z}^{2}$ , and that the Picard group is (isomorphic to) a subgroup of this. If we let $p_i,i=1,2,3,4$ be the singular points of $X$ , then there is an exact sequence $$0\rightarrow\mathrm{Pic}(X)\rightarrow\mathrm{Cl}(X)\rightarrow\bigoplus\mathrm{Cl}(\mathcal{\hat{O}}_{X,p_i}),$$ though the hat (for completion) only really matters on the singularity at the origin. I have shown that the three generators for the torsion part of $\mathrm{Cl}(X)$ map to linearly independent elements of this last direct sum, so nothing in the torsion subgroup can be in the kernel of that map, which by exactness equals $\mathrm{Pic}(X)$ . This is where I get stuck. I don't really know what else I can do; most of the things I can find in the literature seems to be only for nonsingular surfaces, or surfaces where the singularities are more simple than the mess at $(0,0,0)$ . I'd like to thank in advance anyone who takes some time to help me out.","['commutative-algebra', 'algebraic-geometry', 'singularity-theory', 'blowup']"
89353,Intuition about the size of $\aleph_k$ with $k>1$,"Assuming CH for simplicity, I know of some more or less intuitive way to think about difference in sizes of $\aleph_0$ and $\aleph_1$. The most straightforward is the distinction of natural/rational numbers and real numbers; and the size of information needed to fully describe each rational number and needed to describe each real number. Almost straightforward variation of this approach uses finite and infinite binary sequences. Recently I have learned of an even more enlightening picture by a consequence of Baire Category theorem: a fact that every nonempty complete metric space without isolated points is at least of cardinality $\aleph_1$. Questions: 1) Are there any natural examples that provide an intuitive picture of
  the size of $\aleph_k$ for $k\in \mathbb{N} : k>1$ ? 2) Are there any theorems about metric spaces (like the one I listed) that would let one visualize the spaces of higher cardinalities?","['intuition', 'cardinals', 'elementary-set-theory', 'visualization']"
89360,Notation for limit points of a minimizing sequence: $\arg \inf$,"Could you tell me what is the accepted notation for the set of limit points of a minimizing sequence. For example, if I have a function $f(x)$ and a sequence $x_t$ such that $\lim f(x_t) = \inf f(x)$ I want a notation for the (possibly empty) set of limit points of all such sequences. I thought of using $\arg \inf$ but it is a bit misleading because no argument may actually achieve the $\inf$. Thanks for your help.","['notation', 'optimization', 'analysis']"
89378,Methodology to solve a Math IQ puzzle?,Note :  Not looking for the solution - just help on how to solve. Here is the math puzzle: Is there a mathematical model/method that I could employ to solve this? Right now my only answer is to use Excel and trial-and-error my way to the solution.,"['puzzle', 'linear-algebra']"
89403,Is there a way to find the first digits of a number?,"Is there a way to find the first digits of a number? For example, the largest known prime is $2^{43,112,609}-1$, and I did sometime before a induction to find the first digit of a prime like that. But, is there a way to find the first digits of a number? To find the last x digits is easy, just calculate it mod $10^x$, but we can do something about the first ones?","['modular-arithmetic', 'elementary-number-theory', 'number-theory']"
89406,The set of arithmetical numbers,"Define $x\in\mathbb{R}$ to be arithmetical number if the set $\{\langle p, q \rangle \in \mathbb{Z}^2 : \frac{p}{q} < x\}$ is an arithmetical set . Define $x\in\mathbb{C}$ to be arithmetical number if its real and imaginary parts are both arithmetical real numbers. This is very wide (although still countable) class of numbers: for example, every computable number is arithmetical, and non-computable Chaitin's constant Ω is also arithmetical. It is quite obvious that sum and difference of two arithmetical numbers is also arithmetical. What we can say about their products, exponents, roots of polynomials with  arithmetical coefficients, volumes of regions in $\mathbb{R}^n$ defined by systems of polynomial inequalities with arithmetical coefficients?","['logic', 'computability', 'algebraic-geometry', 'abstract-algebra']"
89417,Number of ways to put $n$ unlabeled balls in $k$ bins with a max of $m$ balls in each bin,"The number of ways to put $n$ unlabeled balls in $k$ distinct bins is
$$\binom{n+k-1}{k-1} .$$
Which makes sense to me, but what I can't figure out is how to modify this formula if each bucket has a max of $m$ balls. EDIT: What I've tried: I got to the generating function
$$(1-x^{m+1})^k(1-x)^{-k}$$
which ends up giving me
$$\sum_{r(m+1)+r_2=n} \binom{k}{r}(-1)^{r_2}\binom{k+r_2-1}{r_2}$$ But when programing this: def distribute_max(total,buckets,mmax):
  ret = 0
  for r in xrange(total//(mmax+1)+1):
    r_2 = total - r*(mmax+1)
    ret += choose(buckets,r) * (-1)**r_2 * choose(buckets + r_2 - 1,r_2)
  return ret I'm getting terribly wrong answers.  Not sure which step I screwed up.","['discrete-mathematics', 'combinatorics']"
89419,Algorithm wanted: Enumerate all subsets of a set in order of increasing sums,"I'm looking for an algorithm but I don't quite know how to implement it. More importantly, I don't know what to google for. Even worse, I'm not sure it can be done in polynomial time. Given a set of numbers (say, {1, 4, 5, 9}), I want to enumerate all subsets of this set (its power set, really) in a certain order: increasing sum of the elements. For example, given {1, 4, 5, 9}, the subsets should be enumerated in this order, ""smaller"" sets first: {} = 0 {1} = 1 {4} = 4 {5} = 5 {1, 4} = 5 {1, 5} = 6 {9} = 9 {4, 5} = 9 {1, 9} = 10 {1, 4, 5} = 10 {4, 9} = 13 {5, 9} = 14 {1, 4, 9} = 14 {1, 5, 9} = 15 {4, 5, 9} = 18 {1, 4, 5, 9} = 19 This feels like some unholy mix between a breadth-first search and a depth-first search, but I can't wrap my head around the proper way to mix these two search strategies. My search space is very large ($2^{64}$ elements) so I can't precompute them all up-front and sort them. On that note, I also don't need to enumerate the entire search space -- the smallest 4,096 subsets is fine, for example. Can anyone give any pointers or even any clues to google for? Many thanks.","['graph-theory', 'searching', 'algorithms', 'combinatorics']"
89424,A Lebesgue measure question [duplicate],"This question already has an answer here : Closed 12 years ago . Possible Duplicate: Is there a measurable set $A$ such that $m(A \cap B) = \frac12 m(B)$ for every open set $B$? Is there a measurable set $E \subset [0,1]$ such that for any $0 < a < b<1$, the Lebesgue measure $$m(E \cap [a,b])= \frac{b-a}{2}?$$ I am stumped and have no idea.","['measure-theory', 'real-analysis']"
89431,Exercise 7.7.1 in Grimmett & Stirzaker's 'Probability and Random Processes',"I'm having trouble solving exercise 7.7.1 in Grimmett & Stirzaker's Probability and Random Processes , which reads: Let $X_1,X_2,\ldots$ be random variables such that the partial sums
  $S_n=X_1+X_2+ \cdots + X_n$ determine a martingale.  Show that
  $\mathbb{E}\left(X_iX_j\right)=0$ if $i \neq j$. To start, I'm just trying to show $\mathbb{E}[X_{1}X_{2}]=0$. I've tried writing
$$
\begin{align}
X_{1}^{2} &= X_{1} \mathbb{E}[S_{2}|X_{1}] &&\text{(By the martingale property.)}\\
&= \mathbb{E}[X_{1}^{2}|X_{1}] + \mathbb{E}[X_{1}X_{2}|X_{1}] &&\text{(By linearity of conditional expectation.)} \\
&=X_{1}^2 +  \mathbb{E}[X_{1} X_{2}|X_{1}] &&\text{(By properties of conditional expectation.)}
\end{align}
$$
So $\mathbb{E}[X_{1} X_{2}|X_{1}]=0$, though this is not quite what I want. Am I on the right track? Why or why not?","['probability-theory', 'stochastic-processes', 'martingales']"
89442,Show $V - E + F = 2 - 2g$,"I'd like to show that the Euler characteristic ($V - E + F$) of a compact oriented surface without boundary, $S$ $g$ , is of the form $2 - 2g$ where $S$ $g$ is a sphere with $g$ handles. A sphere with handles is obtained by cutting $2g$ disks out of the sphere and gluing in $g$ cylinders along the boundary circles. Hence, $S$ $0$ is the sphere and the torus is $S$ $1$ . I'm pretty new to topology. But I think induction would make for an easy, understandable proof where the base case is the sphere. Any help or solutions are appreciated!","['algebraic-topology', 'differential-geometry']"
89447,Analytic Continuation of $\sum_{n=0}^\infty \frac{z^n}{n+\frac{1}{2}}$,"Okay, this is the editied version. Another old exam question that I can't shake from my mind. I am searching for an intermediate form (most probably the Taylor series) such that it is possible to analytically continue the function
$$f(z) = \sum_{n=0}^\infty \frac{z^n}{n+\frac{1}{2}}$$
outside of the unit circle. The next step will be to write the function $f\,$ in a hypergeometric form. Any thoughts or insight would be greatly appreciated. Thanks","['sequences-and-series', 'complex-analysis']"
89455,Discrete Structures: Bit Strings,"So my professor gave us an HW assignment which includes this question: ""How many bit strings consist of 1 through 5 bits. (Note 10 and 00010 are considered distinct even though they are both representations for 2)"" My answer is 62. Is this correct? Thanks",['discrete-mathematics']
89456,Showing a function of random walk is a martingale,"I would like a hint for the following problem: Consider a biased random walk on the integers with probability $p<1/2$ of moving to the right and probability $1-p$ of moving to the left.  Let $S_n$ be the value at time $n$ and assume that $S_0=a$, where $0<a<N$. Show that $M_n=[(1-p)/p]^{S_{n}}$ is a martingale. I need to show that $\mathbb{E}[M_{n+1}|S_{0}, \dots S_{n}] = M_{n}$. However
$$
\begin{align}
\mathbb{E}[M_{n+1}|S_{0}, \dots, S_{n}] &= \mathbb{E}[[(1-p)/p]^{S_{n+1}}|S_{0}, \dots, S_{n}] \\
&= \mathbb{E}[[(1-p)/p]^{S_{n+1}}|S_{n}] &&\text{(By Markovity.)} \\
&= [(1-p)/p]^{p(S_{n}+1) + (1-p)(S_{n}-1)} \\
&= [(1-p)/p]^{S_{n}+2p-1},
\end{align}
$$
which is not what I need. So it seems that either I'm making a mistake or the problem is wrong. Am I making a mistake? Is $M_n$ a martingale?","['martingales', 'probability']"
89459,What is $E(X\mid X>c)$ in terms of $P(X>c)$?,What is $E(X\mid X>c)$ in terms of $P(X>c)$? I've seen conditional probability/expectation before with respect to another random variable but not to the variable itself. How would I go about interpreting this?,"['probability-theory', 'conditional-expectation']"
89468,"If L is a subgroup of $\mathbb{Z}^{3}$, linearly independent, linear equations","This exercise is from a book called ""Introduction a L'Algebre et L'Analyse Modernes"" de M.Zamansky, I attempted to solve. But I don't know if my solutions are correct (they seem too short to be correct). I am very grateful if somebody could take a look at it. Let L be a subgroup of $\mathbb{Z}^{3}$. Let $ q_{1}\mathbb{Z}$, $(q_{1}\ge 0)$ be the group of all $ x_{1} \in \mathbb{Z}$ with $ (x_{1},0,0) \in L$ and let $\displaystyle u_{1}=(q_{1},0,0)$. Let  $q_{2}\mathbb{Z}$, $(q_{2}\ge 0)$ be the group of all $x_{2} \in \mathbb{Z}$ so that there exists $x_{21} \in \mathbb{Z}$ with $(x_{21},x_{2},0) \in L$. If $q_{2}>0$ then $ u_{2}=(q_{21},q_{2},0) \in L$; otherwise $u_{2}=0$. Let $ q_{3}\mathbb{Z}$, $(q_{3}\ge 0)$ be the group of all $x_{3} \in \mathbb{Z}$ so that there are $x_{31},x_{32} \in \mathbb{Z}$ with $ (x_{31},x_{32},x_{3}) \in L$. If $ q_{3} > 0$ then $ u_{3} = (q_{31},q_{32},q_{3}) \in L$ otherwise $u_{3}=0$ i) It holds that: $L= \mathbb{Z}u_{1} + \mathbb{Z}u_{2}+ \mathbb{Z} u_{3}$ ii) If $L\ne \{0\}$ then the $u_{i}$ with $q_{i}>0 $ are $\mathbb{Z}$ linearly independent. iii) There are $q_{1},q_{2},q_{3}$ for $L= \{(x_{1},x_{2},x_{3}) \in \mathbb{Z}^{3}; 2x_{1}+4x_{2} + 5x_{3} = 0 \}$ Attempt ( with Dylan Moreland's hints) : i) Let $x=(x_{1},x_{2},x_{3})$ be an element of L, because $u_{3}=(q_{31},q_{32},q_{3})$ and the third coordinate is only found in $u_{3}$ and not in $u_{1}, u_{2}$, there $\exists a_{3} \in \mathbb{Z}$ so that $a_{3}u_{3} = x_{3}$. Similarly, there are $a_{1},a_{2} \in \mathbb{Z}$ so that $a_{1}q_{1}+a_{2}q_{21}+a_{3}q_{31} = x_{1}$ and $a_{2}q_{2}+a_{3}q_{32} = x_{2}$. But this is the same as saying $a_{1}u_{1}+a_{2}u_{2}+a_{3}u_{3} = (x_{1},x_{2},x_{3})$, so $L= \mathbb{Z}u_{1} + \mathbb{Z}u_{2} + \mathbb{Z} u_{3}$ ii) Suppose we have $a_{i} \in \mathbb{Z}$ such that:$$a_{1}u_{1}+a_{2}u_{2}+a_{3}u_{3}=0$$ 
If u_{3}=0, then we can ignore it, otherwise we must have $a_{3}=0$. If $u_{2}=0$, then we can ignore it, otherwise we must have $a_{2}=0$. If $u_{1}=0$, then we can ignore it, otherwise we must have $a_{1}=0$. So $u_{i}$ with $q_{i}$ must be $\mathbb{Z}$linearly independent. Can't we  just write the 3 vectors into a  matrix like : $\begin{pmatrix}q_{1} & 0&  0 \\ q_{21} &  q_{2}  & 0 \\ q_{31} & q_{32} & q_{3} \end{pmatrix}$ This matrix is a 3x3 matrix with rank 3, so its vectors must be linearly independent! iii) If $(x_{1},0,0) \in L$, then $2x_{1} = 0$, so we can put $u_{1}=0$. If $(x_{1},x_{2},0) \in L$,then $2x_{1}+4x_{2}=0$, so $x_{1}=-2x_{2}$ and we can put $u_{2}= (-2,1,0)$. If $(x_{1},x_{2},x_{3}) \in L$, then $2x_{1}+4x_{2}= + 5x_{3} = 0$. If we put $x_{3} = x_{1}+x_{2}$, then we get $7x_{1}+9x_{2} = 0$, and this has solutions for $x_{1}=-9$ and $x_{2}= 7$ , so $u_{3} = (-9, 7, 2)$ These are my old attempts : i) Assume  $L=(l_{1},l_{2},l_{3}) $If one can show that  $\mathbb{Z}(q_{1},0,0)+\mathbb{Z}(q_{21},q_{2},0)+ \mathbb{Z}(q_{31},q_{32},q_{3}) - (l_{1},l_{2},l_{3})=0$, then $L=\mathbb{Z}u_{1} + \mathbb{Z}u_{2}+ \mathbb{Z} u_{3}$. Now one can fix $\mathbb{Z}_{1}, \mathbb{Z}_{2}, \mathbb{Z}_{3}$ so that $\mathbb{Z}_{1}u_{1} + \mathbb{Z}_{2}u_{2}+\mathbb{Z}_{3}u_{3}-L$ = 0  with $L=(\mathbb{Z}_{1}q_{1}+\mathbb{Z}_{2}q_{21}+\mathbb{Z}_{3}q_{31}, \mathbb{Z}_{2}q_{2}+\mathbb{Z}_{3}q_{32}, \mathbb{Z}_{3}q_{3})$, so $L=\mathbb{Z}u_{1} + \mathbb{Z}u_{2}+ \mathbb{Z} u_{3} $ ii) Assume $u_{i}$ with $q_{i}>0$ are $\mathbb{Z}$ linearly dependent. Then all possible combinations are: $\mathbb{Z}u_{2} = \mathbb{Z}u_{1}$ or $\mathbb{Z}u_{3};  \mathbb{Z}u_{1}=\mathbb{Z}u_{2}$or $\mathbb{Z}u_{3}, \mathbb{Z}u_{3}=\mathbb{Z}u_{1}$ or $\mathbb{Z}u_{2}$; $\mathbb{Z}u_{1} - \mathbb{Z}u_{2} = \mathbb{Z} u _{3}; \mathbb{Z}u_{1}- \mathbb{Z}u_{3}= \mathbb{Z}u_{2}; \mathbb{Z}u_{3}- \mathbb{Z}u_{2} = \mathbb{Z}u_{1}$ and one of them has to be true. Since no one is, the $u_{i}$ must be  independent. iii) One can set $x_{21},x_{32},x_{31}$ to 0 and let : $x_{3}=x_{1}+x_{2}$, which is the same as $7x_{1}+9x_{2}=0$ and this has solutions for $x_{1}=-9n, x_{2}=7n $ where $n\in \mathbb{Z}$","['group-theory', 'abelian-groups']"
89469,Question about the Dual Statement for Injective Modules,"It's well-known that there are several equivalent definitions of a projective $A$-module $P$: Given an exact sequence $M \xrightarrow{f} M' \to 0$ and a morphism $P \xrightarrow{g} M'$, there exists a morphism $P \xrightarrow{h} M$ such that $fh=g$; Every exact sequence $0 \to M' \to M \to P \to 0$ splits; There exists a module $M$ such that $P \oplus M$ is a free module; and The functor $\mathrm{Hom}_A(P,-)$ is exact. Now the dual versions (i.e., for the definition of an injective module) of 1, 2, and 4 are easy.  My question is, does there exist a dual version of 3? I would actually like to be phrasing this in the context of an arbitrary abelian category, so would 3 be stated as something like ""a subobject of a free object""?  Then what would the dual version of this statement be?  ""A quotient object of..."" something? Thanks so much in advance.","['modules', 'category-theory', 'abstract-algebra']"
89472,There are exactly 116 different groups P where $7\mathbf{Z}^{3} \subset P \subset \mathbf{Z}^{3}$,There are exactly 116 different groups P  where $7\mathbf{Z}^{3} \subset P \subset \mathbf{Z}^{3}$ I don't know how to prove this. Is it provable at all? How?,['group-theory']
89473,How can we prove the locus is a circle?,"Given two fixed points A and B, find the locus of the point P, satisfying PA=2PB. Of course we can use  Cartesian geometry to find the equation of the curve. Let the midpoint of A and B be the origin, and the line AB be the x-axis, then the coordinates of A and B is (a,0), (-a,0). Let the coordinates of P be (x,y). Then 
$\sqrt{(x-a)^2+y^2}=2\sqrt{(x+a)^2+y^2}$,
we get $(x+\frac{5}{3})^2+y^2=(\frac{4}{3}a)^2$. So the locus is a circle. But how can we solve this problem by pure geometry?","['geometry', 'circles']"
89494,Help with solving speed/time/distance traversal problem,"I'm not any sort of math wiz, and I've run up against a problem that is fairly complex for me to solve. A friend suggested this site might be able to provide some help. So let me try to describe the issue as best I can. Let me start out by saying that I had prepared a couple of images to help explain all this, but I'm not allowed to use them in this post as I'm a new user. Hence, some references to graphs are less meaningful. I have tried to describe what the graphs depicted. I have a path of a known distance, that must be traversed in a fixed amount of time. However, I must start the traversal of the path and end the traversal at a specific speed. So, for example, if I need to traverse 1200 feet in 10 seconds, and my entry & exit speeds must both be 120 ft./sec, then I can simply stay at the constant speed of 120 ft./second to accomplish my goal. If I graph speed against time, the area under the graph represents distance traveled as so: (Figure 1 shows speed in the vertical axis, time in the horizontal axis, with points marked for 120 ft./sec. on the vertical and 10 seconds on the horizontal. It shows a rectangular area under the horizontal line at Speed 120 ft/sec. starting a 0 seconds and going until 10 seconds. The area shown represents the 1200 feet that would be traversed). However, if I have to travel only 700 feet in that same 10 second interval, things get ugly. I thought about decelerating at a constant rate until I could then accelerate at a constant rate to end up with my speed curve carving a triangle out of the graph in Figure 1 above, whose area above the curve would be 500 ft. However that would yield a discontinuity in the acceleration/deceleration that is unacceptable. I then figured I could use a segment of a circle to do the same thing as shown below: (Cool image shows a similar graph to the one above, but with a segment of a circle cutting into the shaded area from the image above, such that the segment intersects horizontal line at time = 0 and speed = 120 ft/sec on one side and 120 ft./sec and 10 seconds on the other side, with the segment dipping down to carve out 500 ""feet"" from the area under the horizontal line representing a constant speed of 120 ft/sec) Here the orange area would represent the 500 ft less than the distance traveled by a constant speed. Following the speed curve indicated by the circle segment should be pretty trivial. And so it would seem that I have solved my problem. However, when I try to actually implement this into an algorithm, I run into the problem that the area calculations for the segment of a circle doesn't seem to yield units that make any sense. Perhaps it would be better to say that I don't know how to set up the problem so that the units make sense. Sure I can calculate the area of the segment, but what does 10 seconds mean when used as the chord of the circle, and what should the units of the radius be. I guess the value of theta is still easy at least. ;) Unfortunately I'm sort of stumped on the rest. I'm not even sure that this approach is viable. I'd be just as interested in a numerical approach to the solution as a mathematical approach. Any help you can offer to help me get my head around this would be greatly appreciated. = Ed =","['geometry', 'physics']"
89503,Topology on the space of paths,"Let $X$ be a topological space, and define a path as a continuous map $\gamma : [a,b] \rightarrow X$. Two paths $\gamma : [a,b] \rightarrow X$ and $\phi : [c,d] \rightarrow X$ are equivalent ($\gamma \sim \phi$) iff there exists an increasing homeomorphism $\psi: [a,b] \rightarrow [c,d]$ such that $\phi \circ \psi = \gamma$. The equivalence class of a path is denoted by $[\gamma ]$. Now define the space of paths $P(X) = \lbrace [\gamma]\ \vert\ \gamma : [a,b] \rightarrow X\ \text{is a path} \rbrace$. I am wondering: is there is a useful or a natural topology that can be put on $P(X)$, generated by $X$? Usually topologies are chosen to make a certain type of function continuous, but I can't think of anything in particular that would be a natural type of function on paths.","['general-topology', 'algebraic-topology']"
89505,Analytically compute signed distance of ellipsoid,"I'm trying to generate a 3d signed distance field for a origin centered ellipsoid.  For a sphere this is pretty easy: 
$$\sqrt{x^2 + y^2 + z^2}-r$$
where $r$ is the radius. I'm not sure what the best approach is for an ellipsoid.  I expect plugging values into the implicit equation:
$$f(x,y,z) = \frac{x^2}{a^2}+\frac{y^2}{b^2} + \frac{z^2}{c^2} - 1$$
would give me something close (certainly it should give the correct sign). Any ideas? Thanks!","['geometry', '3d']"
89525,"Binary function that is distributive, associative, commutative","Is there such an operation that is distributive over addition, and is not multiplication? Also, please no operations that are defined piece-wise, or that are trivial. It must apply to all integers, but for all reals for example, the result does not matter.",['functions']
89548,Famous uses of the inclusion-exclusion principle?,"The standard textbook example of using the inclusion-exclusion principle is for solving the problem of derangement counting; using inclusion-exclusion (and some basic analysis) it can be shown that $D(n)=\left[\frac{n!}{e}\right]$ which I consider to be quite a beautiful example since it tackles a problem that does not seem to be solvable with such a closed formula in the first place (and also, who expects inclusion-exclusion to yield a closed formula?) Another standard textbook use is giving a (non-closed) formula for Stirling numbers. This result is less amazing, but is still important enough. My question is whether there are other nice such examples for using inclusion-exclusion for dealing with ""natural"" and ""famous"" problems, preferably problems arising in other fields in mathematics. Edit: I just remembered another nice example: Proving the formula for $\varphi(n)$ (Euler's totient function) directly (there are other methods as well).","['examples-counterexamples', 'inclusion-exclusion', 'big-list', 'combinatorics']"
89575,"If $f(x)$ and $\frac{f(2x)-f(x)}{x}$ have limit $0$ as $x\to 0$, then $\frac{f(x)}{x}\to 0$","Prove that if $\lim_{x\to 0} f(x) = 0$ and $\lim\limits_{x\to 0} \frac{f(2x)-f(x)}{x}= 0$ ,
then $\lim\limits_{x\to 0} \frac{f(x)}{x} = 0.$ I try to solve it in this way: $f(x)$ is infinitesimal because $\lim\limits_{x\to 0} f(x) = 0,$ $\lim\limits_{x\to 0} \frac{f(2x)-f(x)}{x}= 0,\Rightarrow {f(2x)-f(x)}=o({x})\Rightarrow  {f(2x)}=f(x)+o({x}).$ Well $$\lim_{x\to 0} \frac{f(2x)-f(x)}{x}= \lim_{x\to 0} \frac{f(x)+o({x})}{x}= \lim_{x\to 0} \frac{f(x)}{x}+\lim_{x\to 0}\frac{o({x})}{x}=0$$ $\lim_{x\to 0}\frac{o({x})}{x}=0$ , of course; then $$\lim_{x\to 0} \frac{f(x)}{x} = 0$$","['sequences-and-series', 'calculus', 'real-analysis', 'limits']"
89578,crossing number question,"Prove that there exists constant k such that, for all $5v < e$ there is a subgraph of the complete graph of $v$ vertics with crossing number less or equal than $ k e^3/v^2$. Any hints for a way to apply probabilistic argument ?","['graph-theory', 'discrete-mathematics', 'discrete-geometry']"
89593,How many chips drawn with replacement until a duplicate is found?,"Note - this problem is not homework. I'm studying for an exam and this problem is in our text ( A First Course in Probability by Sheldon Ross) with no listed solution. The problem is: A jar contains n chips. Suppose a boy successively draws a chip from the jar, each time replacing the one drawn before drawing another. The process continues until the boy draws a chip that he has previously drawn. Let X denote the number of draws, and compute its probability mass. So we want the probability that it will take i draws to draw a duplicate, for some fixed $0<=i<=n$. I'm using an unofficial solutions manual which claims the following solution: $P\{X=i\} = \frac{i(i-1)}{2n}$ But that doesn't make sense to me because what if $n=6$, $i=4$? Surely $P\{X=4\}$ isn't 1? My solution is as follows: First we choose $i-1$ unique chips with probability $\frac{\binom{n}{i-1}}{n^{(i-1)}}$ and then we choose a duplicate chip with probability $\frac{i-1}{n}$ to get a final probability of $P\{X=i\} = \frac{\binom{n}{i-1}}{n^{(i-1)}} * \frac{i-1}{n}$. If my solution is incorrect (I am suspicious because of the different solution from the manual), what is the flaw in my reasoning?","['probability', 'combinatorics']"
89594,What's the name for the equivalence induced by a function on its domain?,"Any function $f$ with domain $X$ induces an equivalence relation on $X$, with classes $$\{f^{-1}(\{y\})\,:\, y \in \operatorname{im}f\;\} .$$ Is there a name for this equivalence? Thanks!","['relations', 'terminology', 'equivalence-relations', 'elementary-set-theory']"
89598,Mystified by Rotman's proof of normality of index-2 subgroups,"(I'm not sure if this question is suitable for Mathematics SE.  Please comment if not.) On page 43 of his Advanced Modern Algebra (2d ed., 2010), Rotman gives a mystifyingly elaborate proof (by contradiction no less) of the normality of any subgroup of index 2 (Proposition 1.86 (ii)). This proof is not incorrect, AFAICT, but it makes me wonder what's wrong with this naive 1-line proof:  If $H < G$ are groups with $[G\;:\:H\;] = 2$, then (regarding $G, H, aH$, and $Ha$ as sets ): $$aH = G - H = Ha \;,\;\;\;\forall \; a \in G - H$$ (I.e. $G = H \; \cup \; aH = H \; \cup \; Ha$, and both unions are disjoint.) Am I missing something? Thanks! Edit: OK, when transcribing Rotman's proof (in response to Arturo's request), I noticed a footnote that explains the mystery, or most of it (details below).  My apologies for my careless reading. (Perhaps I should point out that, I am not reading this 1000-page behemoth cover-to-cover, but rather I consult it occasionally as a reference.) Anyway, FWIW, here's Rotman's proof: (ii) It suffices to prove that if $h \in H$, then the conjugate $ghg^{-1}\in H$ for every $g \in G$.  If $g \in H$, then $ghg^{-1}\in H$, because $H$ is a subgroup.  If $g \notin H$, then $g = ah_0$, where $h_0\in H$ (for $G = H \cup aH$).  If $ghg^{-1} \in H$, we are done.  Otherwise, $ghg^{-1} = ah_1$ for some $h_1\in H$.  But $ah_1 = ghg^{-1} = ah_0hh_0^{-1}a^{-1}$.  Cancel $a$ to obtain $h_1 = h_0hh_0^{-1}a^{-1}$, contradicting $a \notin H$. The prospect of making sense of this oddly worded, belabored proof was what prodded my lazy brain into casting about for a simpler one... Anyway, as I said earlier, upon transcribing the above I noticed for the first time an unobtrusive footnote marker leading to the remark: ""[a]nother proof of this is given in Exercise 1.57 on page 45.""  Sure enough, exercise 1.57 says: ""(i) Show that if $H$ is a subgroup $bH = Hb = \{hb\;:\;h\in H\;\}$ for every $b\in G$, then $H$ must be a normal subgroup.  (ii) Use part (i) to give a second proof of...: if $H\subseteq G$ has index $2$, then $H \triangleleft G$."" It is surprising to me that Rotman gives such vanishingly discreet billing to the equivalence
$$H \triangleleft G  \;\;\; \Leftrightarrow \;\;\; \forall\,g\in G, \;\; gH = Hg$$ I remember the RHS of this equivalence as the definition of normality (rather than Rotman's $h\in H, g\in G \Rightarrow ghg^{-1}\in H$).  If it is not made the definition of normality, then its equivalence with normality is certainly a more general, and IMHO, important, theorem than the one about the normality of an index-$2$ subgroup.  But, be that as it may, the fact that this result was not available at this point in the text explains, perhaps, the choice of the more complex proof. Once again, my apologies for my careless reading.",['group-theory']
89621,"How to multiply vector 3 with 4by4 matrix, more precisely position * transformation matrix","All geometry in computer graphics are transformed by position * transform matrix; The issue is the fact that position is a vector with 3 components (x,y,z); And transform matrix is a 4 by 4 with one column that can be dumped(at least in my case). So my transform matrix is now a 3 by 4 matrix: axis x          { x, y, z } axis y          { x, y, z } axis z          { x, y, z } position axis   { x, y, z } multiplied with position vector { x, y ,z } If I dump position axis this can be done with standard formula of matrix multiplication. 
But it does not transform it. 
I can make the position vector with 4 components { x, y, z, w } but don't know what to do with the w?
My only solution is a slow one, put position vector in a new transform matrix in position axis and multiply them. But it is computationally expensive. 
How to approach such problem?","['geometry', '3d', 'matrices', 'computational-geometry', 'transformation']"
89629,Evaluate: $\int \sec (x-a) \sec (x-b)\; dx$?,How to do this integration: $$\int \sec (x-a) \sec (x-b)\ dx?$$ I want to this in the shortest possible way. Please guide me through.,"['calculus', 'integration']"
89631,What am I actually doing when I integrate using spherical coordinates in $\mathbb{R}^3$?,"When learning vector fields and using Green's Theorem with the Jacobian to find the area of a level surface, I actually realized that most of the examples shown in my book would be much easier to solve by using polar and spherical coordinates, but not multiplying by the representation of the radius. (Eg: In spherical coordinates I would eliminate the $\rho^2 \sin(\phi)$. I ended up discussing with my professor about it, and he said that the radius should be fixed. I disagreed, saying that the radius is changing, but you are not multiplying by it. Here is how I see the use of Spherical Coordinates for getting the volume: My idea is that, in spherical coordinates, if you take $z = \rho \cos(\theta)$, and look at the Cartesian plane from the z-axis, you are actually adding all the circles on the xy-plane. Then by looking from the y-axis, you have other sets of circles, and that's where you are getting your radius for multiplying $\rho^2 \sin(\phi)$. Is this right or I understood it wrongly? Thanks for your attention","['multivariable-calculus', 'integration', 'spherical-coordinates']"
89644,graph avoiding cycle of length 4,let $G$ be a simple graph on $n$ vertices such that $G$ has no cycle of length $4$. show that $e(G)\le \frac{n}{4}(1+\sqrt{4n-3})$ where $e(G)$ denotes the number of the edges of the graph $G$.,"['graph-theory', 'combinatorics']"
89666,Conditions leading to paracompactness,"If $A$ is a Hausdorff space such that $A = \bigcup\limits_{i = 1}^\infty  {{K_i}} $ where $K_i$ are its compact subsets, is $A$ a paracompact space? If not, what additional conditions should we add? (e.g. locally compact)",['general-topology']
89672,Constructive proof of the existence of an algebraic closure,"It is well-known that, assuming the axiom of the choice (in the form of Zorn's lemma), one can prove that any field $F$ has an algebraic closure. One proof roughly goes as follows: consider the partially-ordered set of algebraic extensions $K/F$, ordered by inclusion; show that it satisfies the hypotheses of Zorn's lemma; then Zorn's lemma implies the existence of a maximal element; show that this maximal element is algebraically closed. Now, in my Algebraic Number theory class, the professor gave a ""constructive"" proof of this fact: let $S$ be the set consisting of all non constant polynomials in $F[X]$, and construct the ring $R=F[X_f:f\in S]$ (that is, a polynomial ring with one variable for each element of $S$); let $A_0$ be the ideal of $R$ generated by polynomial $\{f(X_f):f\in S\}$; show that it is a proper ideal, and so it is contained in a maximal ideal $A$; the quotient $R/A$ is thus a field, which contains $F$ as a subfield; iterate, and take the union of all these fields; then, this union is the algebraic closure of $F$. He also mentionned (if I recall correctly) that one can prove that after only one iteration, you get the algebraic closure. Now, I put quotation marks around ""constructive"", because I suspect that one still needs the axiom of choice at some point to prove that, indeed, you have the algebraic closure (although I haven't checked the details). So, here's my question: Is it the case that this proof is constructive? Another question: Can one prove the existence of an algebraic closure within ZF (without the axiom of choice)? Thanks in advance.","['axiom-of-choice', 'abstract-algebra', 'field-theory']"
89684,Convergence of Fourier Series,Is there an $f\in L^1(\mathbb{T})$ whose Fourier series converges a.e. on $\mathbb{T}$ but not a.e. to $f$?,"['fourier-series', 'fourier-analysis', 'analysis']"
89688,Nonabelian groups of order $p^3$,"From a little reading, I know that for $p$ and odd prime, there are two nonabelian groups of order $p^3$, namely the semidirect product of $\mathbb{Z}/(p)\times\mathbb{Z}/(p)$ and $\mathbb{Z}/(p)$, and the semidirect product of $\mathbb{Z}/(p^2)$ and $\mathbb{Z}/(p)$. Is there some obvious reason that these groups are nonabelian?","['semidirect-product', 'finite-groups', 'group-theory', 'p-groups']"
89710,Matrices for which $\mathbf{A}^{-1}=-\mathbf{A}$,"Consider matrices $\mathbf{A}\in\mathbb{C}^{n\times n}$ (or maybe $A\in\mathbb{R}^{n\times n}$) for which $\mathbf{A}^{-1}=-\mathbf{A}$. A conical example (and the only one I can come up with) would be $\mathbf{A} = \boldsymbol{i}\mathbf{I},\quad \boldsymbol i^2=-1$. Now I have a few questions about this class of matrices: Are there more matrices than this example matrix (I guess yes) or can they even be generally constructed somehow? Are there also real matrices for which this holds? Now each matrix that is both skew-Hermitian and unitary fulfills this property. But does it also hold in the other direction, meaning is each matrix for which $\mathbf{A}^{-1}=-\mathbf{A}$ both skew-Hermitian and unitary (maybe this is simple to prove, but I don't know where to start at the moment, but of course I know if one holds the other has to hold, too)? Do such matrices have any practical meaning? For example I know that Hermitian and unitary matrices are reflections (in a general sense), but what about skew-Hermitian and unitary (if 3 holds)? This is just for personal interrest without any practical application. I just stumbled accross this property by accident and want to know more about its implications and applications.","['matrices', 'linear-algebra']"
89718,change of variables for definite integrals,First of all I would like to start off by asking why do they have different change of variable formulas for definite integrals than indefinite...why cant we just integrate using U substitution as we normally do in indefinite integral and then sub the original U value back and use that integrand for definite integral? I was at one point understanding integration but not when they started coming up with different formulas for definite integrals in U-substitution I got lost and resulted to just forcibly memorizing the formulas... I dont get why for U substitution they sub the upper and lower bounds into U from the original function to find the new upper and lower bounds with the function U. I know that because if you dont want to sub the original value of U in and want to instead stick to U as your function you must use those new upper and lower bound but if you sub in the original value for U then you can use your old upper and lower bound values. My question is what or how does plugging your old lower and upper bound values into U give you the new values of your new function thats expressed as U... Why do they make such a big deal out of it and complicate it when all they have to do is same U sub as indefinite integral and then plug original value of U in and go from there...are these math people just making excuses to come up with more work or is there more logic behind it?,"['definite-integrals', 'calculus', 'algebra-precalculus', 'integration']"
89721,Are all connected manifolds homogeneous,"A topological space $X$ is called homogeneous , if for every two points $x,y \in X$ there exists a homeomorphism $\phi : X \rightarrow X$ s.t. $\phi(x) = y$. It is not hard to prove that all connected 2-manifolds are homogeneous. The proof basically comes down to the fact that if $D$ is the open disk in $\mathbb{R}^2$ then for every $x,y \in D$ there exists a homeomorphism $\phi : \bar{D} \rightarrow \bar{D}$ such that $\phi(x) = y$, and $\phi \vert_{\partial D}$ is the identity. Is it true that a general connected manifold is homogeneous?","['general-topology', 'differential-geometry']"
89727,Explicit differential equations,Can you help me to find all solutions of differential equation $y'^2-(x+y)y'+xy=0$? I wrote this equation as product of explicit equations: $$(y'-x)(y'-y)=0$$ Then I found zeroes: $y'-x=0 \Longrightarrow y'=x \Longrightarrow y=\frac{x^2}2+C_1$ $y'=0$ I don't know what to do here. Maybe to solve as equation 'without $x$'? Am I doing this right?,['ordinary-differential-equations']
89732,Non-isomorphic exact sequences with isomorphic terms,"I love it when an undergraduate catches me out. I'm lecturing a first course in (not necessarily commutative) rings (with 1) and I've spent the last few weeks doing basic module theory. I defined a short exact sequence of (left) $R$-modules and a homomorphism of short exact sequences (a homomorphism from $0\to A\to B\to C\to 0$ to $0\to A'\to B'\to C'\to 0$ is just $R$-module maps $A\to A'$, $B\to B'$ and $C\to C'$ such that the obvious two squares commute). There's hence an obvious notion of an isomorphism of short exact sequences. Today one of the students asked me if it was possible to have a ring $R$ and modules $A,A',B,B',C,C'$ sitting in two short exact sequences as above, and such that $A$ was isomorphic to $A'$, $B$ was isomorphic to $B'$ and $C$ was isomorphic to $C'$, but that the sequences weren't isomorphic. I said ""sure, I'll email you a counterexample later"" (the logic being that if this were a theorem, it would be one I knew about). I thought I'd knock up a counterexample on the tube home -- but I failed :-/ If $R$ is a field then short exact sequences split (yes we're assuming AC) so that's not going anywhere. So I thought that $R=k[X]$ would be a good place to start, $k$ a field. In this case an $R$-module is just a $k$-vector space equipped with an endomorphism and I figured this would give me enough flexibility. I wanted $A,A',C,C'$ to be $R/(x^2)$ and tried some messy matrix calculations to figure out an example, but I couldn't get it to work. I then went for $R=k[x,y]$ but now a 2-dimensional vector space is an $R$-module when we give it two commuting linear maps and somehow this set-up had too many endomorphisms for me to face doing the algebra. I then figured that I might want to try a polynomial ring in two noncommuting variables--but then it was my stop and it was time to start thinking about other things. I am almost certain that there will even be a counterexample with $R$ commutative (that's why I was thinking about the commutative case). Can anyone tell me the trick I'm missing?","['modules', 'linear-algebra', 'exact-sequence']"
89757,Count the number of integer solutions to $x_1+x_2+\cdots+x_5=36$ [duplicate],"This question already has answers here : Counting bounded integer solutions to $\sum_ia_ix_i\leqq n$ (5 answers) Closed 1 year ago . How to count the number of integer solutions to  $x_1+x_2+\cdots+x_5=36$ such that $x_1\ge 4,x_3 = 11,x_4\ge 7$ And how about $x_1\ge 4, x_3=11,x_4\ge 7,x_5\le 5$ In both cases, $x_1,x_2,x_3,x_4,x_5$ must be nonnegative integers. Is there a general formula to calculate things like this?","['integer-partitions', 'diophantine-equations', 'combinatorics']"
89759,How to prove that $2 \arctan\sqrt{x} = \arcsin \frac{x-1}{x+1} + \frac{\pi}{2}$,"I want to prove that$$2 \arctan\sqrt{x} = \arcsin \frac{x-1}{x+1} + \frac{\pi}{2}, x\geq 0$$ I have started from the arcsin part and I tried to end to the arctan one but I failed. Can anyone help me solve it?",['trigonometry']
89769,Intersection point of line segments,"I need to find the intersection point of 2 line segments (lines are finite, i.e., they have end points). e.g. segment 1 from $(x_1, y_1)$ to $(x_2, y_2)$ -- segment 2 from $(x_3, y_3)$ to $(x_4, y_4)$ you can assume $m_1$ and $m_2$ are the gradients of segment 1 and segment 2 respectively similarly, $c_1$ and $c_2$ being the y-intercepts of segment 1 and segment 2 respectively Using $y=mx+c$ I can easily find the equations of both lines and then derive the intersection point from those equations and check if that point actually lies on both the line segments (intersection point may not lie between the end points). My problem is when I calculate the gradients $m_1$ and $m_2$ I do $\frac{y_2-y_1}{x_2-x_1}$ there for if one of the lines is vertical then I am going to have problems. How can I deal with this? Another problem is that if the two segments intersect at a point which is also the same as one of the end points then I want to assume that they are not intersecting. e.g. if segment 1 is from $(2, 3)$ to $(3, 7)$ and segment 2 is from $(3, 7)$ to $(7, 3)$ then I want to assume they don't intersect.",['geometry']
89790,Tychonoff Theorem in the box topology,"A short question: Why does not the Tychonoff theorem (the arbitrary product of compact spaces is compact) hold in the box topology? 
I don't know how to show that there is no finite sub-cover of any open cover of the product space! Can anyone please give me some hint? Thank you!","['general-topology', 'box-topology', 'compactness']"
89796,"Prove the following function is Riemann Integrable on $[0,1]$.","Suppose that $f:[0,1] \to \mathbb{R}$ is defined by $f(x) = 1$ when $x = \frac{1}{n}$ for some positive integer $n$ and $f(x) = 0$ otherwise. How can I prove that $f$ is Riemann integrable on $[0,1]$?","['integration', 'real-analysis']"
89808,What does it mean for a function to be a solution of a differential equation?,"What does it mean for a function to be a solution of a differential equation?  I think I understand, but I still don't have a simple or intuitive understanding.",['ordinary-differential-equations']
89809,Is every convex-linear map an affine map?,"Let's say that a map $f: V \rightarrow W$ between finite-dimensional real vector spaces is convex-linear if $f(\lambda x + (1-\lambda)y) = \lambda f(x) + (1-\lambda)f(y)$ for all $\lambda \in [0,1]$. Let's say that a map $f: V \rightarrow W$ between finite-dimensional real vector spaces is affine if $f(\lambda x + (1-\lambda)y) = \lambda f(x) + (1-\lambda)f(y)$ for all $\lambda \in \mathbb{R}$. From the definition, it seems that the requirement of being convex-linear is weaker than the requirement of being affine. However, I can't think of an example of a map which is convex-linear but not affine, but I also can't prove that convex-linearity implies affinity. Can someone show me an example of a convex-linear map which is not affine? Or tell me how to prove that every convex-linear map is affine? Or give me an appropriate reference? EDIT: With the intuition of Qiaochu Yuan's comment in mind, I've come up with the following proof: Claim: Every convex-linear map is affine. Proof: Let $f$ be convex-linear. For $\lambda \in [0, 1]$, We have that $f(\lambda x + (1-\lambda) y) = \lambda f(x) + (1-\lambda) f(y)$. For $\lambda \notin [0,1]$, we can assume without loss of generality that $\lambda < 0$ (in the other case where $\lambda > 1$, we can interchange the role of $x$ and $y$). We can write
\begin{align}
f(y) = f\left( \underbrace{\frac{1}{1-\lambda}}_{\in [0,1]}(\lambda x + (1-\lambda) y) + \left( 1 - \frac{1}{1-\lambda} \right) x \right). \label{bla}
\end{align}
By the convex-linearity of $f$, this reduces to
\begin{align}
&f(y) = \frac{1}{1-\lambda} f(\lambda x + (1-\lambda) y) + \left( 1-\frac{1}{1-\lambda} \right) f(x)
\end{align}
which in turn can be reduced to
\begin{align}
f(\lambda x + (1-\lambda x)) = \lambda f(x) + (1-\lambda) f(y).
\end{align}","['affine-geometry', 'convex-analysis', 'linear-algebra']"
89814,Positivity and Interchange of Summation,"Suppose I have $\sum\limits_{n = 1}^{\infty}\sum\limits_{m = 1}^{\infty} a_{m, n}$ where $a_{m, n} \geq 0$ for all $m$ and $n$. Can I interchange the two summations? If so why?","['sequences-and-series', 'real-analysis']"
89823,Rules for algebraically manipulating pi-notation?,"I'm a bit of a novice at maths and want to learn more about algebraically manipulating likelihoods in statistics.
There are a lot of equations that involve taking the product of a set of values given a model. I know a few rules for manipulating sigma-notation (e.g., here and here ). What are the basic rules for manipulating sequences of products (i.e, $\prod_{i=1}^{I} ... $)? Is there a web page that you could direct me to? e.g., $\prod_{i=1}^{I} x_i$ $\prod_{i=1}^{I} x_i y_i$ $\prod_{i=1}^{I} a + b x_i$ $\prod_{i=1}^{I} \exp x_i$","['algebra-precalculus', 'products']"
89853,Does the inverse of this matrix of size $n \times n$ approach the zero-matrix in the limit as $\small n \to \infty$?,"Fiddling with another (older) question here I constructed an example-matrix of the type $\small M_n: m_{n:r,c} = {1 \over (1+r)^c } \quad \text{ for } r,c=0 \ldots n-1 $     .
I considered the inverse W :  $\small W_n=M_n^{-1} $ for some small n and observed, that the entries in W from the left columns on tend to zero. Using the LDU-decomposition and inverting that L,D,U -factors, one can observe a simple pattern for the coefficients in the matrices or better: for the terms of the dot-products of that inverses in the leftmost columns which is independent of the matrixsize. For instance, if we denote the inverted L,D,U -factors as K , C , T such that for any n $\small W_n = M_n^{-1} = T \cdot C \cdot K = U^{-1} \cdot D^{-1} \cdot L^{-1}$ then the top-left entry in $\small W_n  $ can be computed by the dot-product of the first row of T , C and the first column of K which shows a simple pattern such that we assume the following type of sum:
 $$\small w_{n:0,0} = \sum_{j=0}^{n-1} (-1)^j (j+1)/j! \qquad \text{ and } \qquad \lim_{n\to \infty} w_{n:00}=0 $$ 
Now for the two first columns it seems, that indeed all dot-products vanish when n increases and that this can be shown by relatively simple modification and linear composition of the formal exponential-series. But for the next columns this becomes more difficult because the patterns are complicated (but seem to be recursive) and the matrices need increasing size to actually approximate the limiting values. So the question:a) does W indeed approach the Null-matrix as n goes to infinity? or in more detail: what are the patterns for the dot-products which occur in the evaluation of the single entries in W ? [update] I seem to have solved the pattern for the matrix of the partial product $\small C \cdot K $ by a good heuristic. Let's denote the entries with the small letter g then $$ g_{r,c} = (-1)^c \cdot (1+c)^r \binom{r+1}{c+1} $$ Because at least the first two rows in T are simple to decode 
$$ \begin{eqnarray}
 t_{0,c} &=& (-1)^{r+c} {1 \over c!} \\ 
 t_{1,c} &=& (-1)^{r+c} \binom{c+1}{2}{1 \over c!} 
\end{eqnarray} $$
the results by the two dot-products for rows r=0 and r=1 $$ \begin{eqnarray}
 w_{0,c} = \sum_{k=0}^{\infty} t_{0,k} \cdot g_{k,c} = 0  \\ 
 w_{1,c} = \sum_{k=1}^{\infty} t_{1,k} \cdot g_{k,c} = 0  \\ 
\end{eqnarray} $$ give for the first two rows in W zeros for the first couple of columns (with index c=0..4 checked using wolframalpha and the formulae) Wolframalpha could even give answers for the general column (colindex=c kept indeterminate)
For the rows r=0 and r=1 in W for all columns c the value $\small w_{r,c}=0 $ Here are the Wolframalpha formulae: $$ \sum_{k=c}^{\infty} \left[ \left({(-1)^k \over k!}\right) * \left((-1)^ c (1 + c )^k \binom{k+1}{ c +1}\right)  \right] $$ ( input )
$$ \sum_{k=c}^{\infty} \left[ \left({(-1)^k \over k! }\binom{k+1}{2} \right) * \left((-1)^ c (1 + c )^k \binom{k+1}{c +1} \right)  \right] $$ ( input )","['matrices', 'limits', 'number-theory']"
89855,surjectivity of group homomorphisms,"I don't know if the next thing is true, but I'm not able to find a counterexample: suppose you have a surjective group homomorphism of finite groups $f:G \rightarrow G'$ and normal subgroups $H \lhd G, H' \lhd G'$, such that the induced homomorphism $f':G/H \rightarrow G'/H'$ is also surjective. Is it true that $f\mid_H: H \rightarrow H'$ is also surjective ? In particular, is this true for decomposition groups and inertia groups in the case of Galois extensions $L/K/\mathbb{Q}_p$ ?","['galois-theory', 'group-theory', 'number-theory']"
89858,Perspective problem - trapezium turned square,"True or false: If you draw a trapezium on the ground, there always exists a point above (but not necessarily directly above) the trapezium such that the trapezium looks like a square from that point. Intuitively this seems true to me, but I'm not sure how you would go about proving/disproving this. Is there an easy-to-understand method of proving/disproving the above statement? (By ""looks like a square"" I mean if you take a photograph of the trapezium from that point, the four corners of the trapezium form a square on the 2D photograph)","['geometry', 'projective-geometry']"
89870,"X,Y are independent standard normal distributed then what is the distribution of $\frac{X}{X+Y}$","X, Y are independent standard normal random variables, what is the distribution of
$$ \frac{X}{X+Y} $$ Could anyone help me with this? Thanks. I have worked the problem by multivariable transformation: Let $$Z=\frac{X}{X+Y} , W=X$$ Consider transformation $$(X,Y)\longrightarrow(Z,W)$$ Then $$X(Z,W)=W , Y(Z,W)=\frac{W(1-Z)}{Z}$$  defines the inverse transformation. The Jacobian is $$J(Z,W)=\frac{w}{z^{2}} $$ So $$f_{Z,W}(z,w)=f_{X,Y}(w,\frac{w(1-z)}{z})\cdot\mid\frac{w}{z^{2}}\mid$$ As X  and Y  are independent. Then the marginal pdf of Z  is $$f_{Z}(z)=\intop_{0}^{\infty}\frac{w}{z^{2}}\cdot f_{X}(w)\cdot f_{Y}(\frac{w(1-z)}{z})dw+\intop_{-\infty}^{0}-\frac{w}{z^{2}}\cdot f_{X}(w)\cdot f_{Y}(\frac{w(1-z)}{z})dw$$ 
After calculation we get $$f_{Z}(z)=\frac{1}{\pi\cdot\frac{1}{2}\cdot(1+(\frac{z-\frac{1}{2}}{\frac{1}{2}})^{2})}$$ Hence $$Z\sim \mathrm{Cauchy}(\frac{1}{2},\frac{1}{2}).$$","['statistics', 'normal-distribution', 'probability-distributions']"
89873,Complex Analysis: Continuity of a Function,"Problem: Let $f$ be defined as $f(z)=\frac{z}{1+|z|}$ . Is $f$ continuous from $\mathbb{C} \to \mathbb{C}$ ? Progress: $f$ is clearly well-defined on $\mathbb{C}$ , but is not holomorphic (Cauchy-Riemann equations are not satisfied as a result of the ' $|z|$ ' term). I think we may need to make use of the ' $\epsilon$ - $\delta$ ' definition of continuity but I'm really not sure how to apply this to complex-valued functions. Any assistance would be very appreciated.",['complex-analysis']
89881,Example of finitely generated subgroups whose intersection is not finitely generated,"I'm reading G.Graetzer's Lattice Theory: First Concepts and Distributive Lattices and working on its exercises.  One of them is to prove $(A, \subset)$, where $A$ is the set of finitely generated subgroups of a group $G$, is a join-semilattice, but not necessarily a lattice.  I proved the former half, but I can't think of a counterexample for the latter half, partly because I'm not familiar to group theory.  What is a simple example of a group whose finitely generated subgroups' intersection is not finitely generated?","['examples-counterexamples', 'group-theory']"
89883,Complex Analysis: Continuity of Function,"Problem Define g to be the function $g(z)=re^{\frac{i\theta}{2}}$ if $z=re^{i\theta}$ with $r>0$ and $-\pi<\theta\le\pi$ , and $g(z)=0$ when $z=0$ . Is $g$ continuous from $\mathbb{C}\longrightarrow\mathbb{C}$ ? Progress Claim: $g$ is not continuous from $\mathbb{C}\longrightarrow\mathbb{C}$ . [It seems that all the function seems to be doing is halving $arg(z)$ for each $z\in\mathbb{C}$ .] If we consider points on the negative real line, i.e. $z=-a$ for $a\in\mathbb{R}$ , then let $\epsilon=\frac{a}{2}$ . If $g$ is continuous, then $\exists \delta>0$ such that $g(B_{\delta}(z))\subset B_{\epsilon}((g)z)$ . Clearly $g(-a-(i\delta/2))\in g(B_{\delta}(-a))$ does not lie in $B_{a/2}(g(-a))$ for any $\delta>0$ as $-a$ is mapped to $ai$ and $-a-(i\delta/2)$ is mapped to a point in the lower half-plane. We can conclude then that $g$ is not continuous from $\mathbb{C}\longrightarrow\mathbb{C}$ . Thoughts Is this proof valid (if I play around to make it a little more formal), i.e. is the reasoning behind the claim correct? Any verification/objection would be appreciated. Thanks, TJO.","['functions', 'complex-analysis']"
89888,A question about finding the convergence of a distribution,"Let $X$ have the Gamma$(s,1)$ and given $X=x$, let $Y$ have the Possion distribution with parameter $x$. Show that $$\frac{Y-E(Y)}{\sqrt{\operatorname{var}(Y)}}\longrightarrow W$$ where $\longrightarrow$ means converges in distribution as $s$ goes to infinity. And $W$ needs to be identified. I have worked out the moment generating function of $Y$,
$$ M_Y(t)=\left(\frac{1}{2-e^{t}}\right)^s$$
Then I work out the mgf of $\frac{Y-E(Y)}{\sqrt{\operatorname{var}(Y)}}$,
$$ M(t)=e^{-\frac{s}{\sqrt{2s}}t}\left(\frac{1}{2-e^{\frac{t}{\sqrt{2s}}}}\right)^s$$
But I don't know what does it converges to. Anything wrong with my above calculation? Thanks.","['statistics', 'probability-distributions']"
89896,Complex Analysis: Cauchy-Riemann Problem,"I'm working through a problem set in Complex Analysis and have encountered the following question: Problem Write down the Cauchy-Riemann equations and explain their connection with holomorphic functions. [Completed] Let $U=\mathbb{C} \diagdown (-\infty , 0]$ . Show that for $z=x+iy\in\mathbb{C}$ , we may define $u(x,y), \ v(x,y)$ by; $$\sqrt{2}u=\left(x+\sqrt{x^2+y^2} \right)^{\frac{1}{2}}$$ $$v=\frac{y}{2u}$$ where we take positive square roots in the definition of $u$ . I can't see how we can identify a computation for $u$ and $v$ when they're seemingly arbitrary in the question. Is this a misprint, or am I missing something? Regards,","['functions', 'complex-analysis']"
89925,Why do the solutions of this recurrence have a limit independent of the initial value?,For $0<a_0<1$ the limit $\lim\limits_{n\rightarrow \infty}\sqrt{1-a_n}= \frac{\sqrt{5}-1}{2}$ for the sequence $a_{n+1}=\sqrt{1-a_n}$. But the problem is that the choice of $a_0$ does not matter. The question is why?,"['sequences-and-series', 'calculus']"
89926,Are three matrices linearly independent and form a basis of $M_2(\mathbb R)$?,"I know how to prove whether or not vectors are linearly independent, but can't apply the same thing to matrices it seems. Given three 2x2 matrices, for example: $$A = \begin {bmatrix} -1&1 \\\\ -1&1 \ \end{bmatrix}$$
 $$B = \begin {bmatrix} 1&1 \\\\ -1&-1 \ \end{bmatrix}$$
 $$C = \begin {bmatrix} -1&1 \\\\ 1&-1 \ \end{bmatrix}$$ I want to test whether or not these are linearly dependent. So with vectors I would do something like: $$ c_1A + c_2B + c_3C = 0$$ Where the $c_i$'s are some scalar constants, and prove that the only solution of that is when $$c_1 = c_2 = c_3 = 0$$ So how do I go about solving this: $$ c_1 \begin {bmatrix} -1&1 \\\\ -1&1 \ \end{bmatrix} + c_2 \begin {bmatrix} 1&1 \\\\ -1&-1 \ \end{bmatrix} + c_3 \begin {bmatrix} -1&1 \\\\ 1&-1 \ \end{bmatrix} = 0$$ Or I am going about this completely the wrong way? Any help would be hugely appreciated.",['linear-algebra']
89941,mean value theorem or intermediate value theorem or extreme value theorem?,"I submitted this assignment and the teacher gave me 1/5 marks for this and I dont know why, she just said not enough information to prove the point. She NEVER tells you the solution even after she has marked the assignments so I feel pretty miserable because I dont know why my answer is not good enough. So I am asking here in hopes someone can help explain this to me. The question is:
A hiker leaves his cabin at 8 am and walks uphill to another cabin and reaches that cabin uphill at 8 pm. the next morning he leaves from his uphill cabin to go back to his downhill cabin. he leaves at 8 am and reaches the downhill cabin at 8 pm. prove that at one point in time the hiker was at the exact same spot when going uphill as he was downhill. Now what I wrote for my solution was basically a graph of the hiker's journey, the graph looks kind of like $-x^2$'s graph. I said in order for him to make it to the cabin uphill or down hill, he must have maintained the same AVERAGE velocity. because the average velocity was the same for both trips, he must have reached one point at the exact same time on both trips. she said not enough info and gave me 1/5..didnt even look at my beautiful graph or explanation","['calculus', 'algebra-precalculus']"
89960,"Calculating $\lim\limits_{x\rightarrow 0^{+}} \frac{x^{\min(a,b)}}{x^a+x^b}$","It seems like $\lim\limits_{x\rightarrow 0^{+}} \dfrac{x^{\min(a,b)}}{x^a+x^b}$ is $1$ if $a\ne b$, regardless of the values of $a$ and $b$, but is this true? What does this limit equal in general? Thanks!","['calculus', 'limits']"
89962,Find all analytic functions such that...,"Here is the problem: find all functions that are everywhere analytic, have a zero of order two in $z=0$, satisfy the condition $|f'(z)|\leq 6|z|$ and such that $f(i)=-2$. Any hint is welcomed.",['complex-analysis']
89966,Laguerre polynomials and inclusion-exclusion,"Does anyone know a reference for the solution of the 
generalized derangement problem via Laguerre polynomials?
The Wikipedia article here says that this is an application
of inclusion-exclusion, but I don't see how. This formula was used by joriki in a nice MSE answer here . The article below solves the generalized derangement problem with
inclusion-exclusion, but without Laguerre polynomials. Reference: Finn F. Knudsen and Ivar Skau, ""On the Asymptotic Solution of a Card-Matching Problem"", Mathematics Magazine 69 (1996), 190-197.","['special-functions', 'inclusion-exclusion', 'reference-request', 'probability', 'combinatorics']"
89977,"If G is a primitive group action on A, then $G_a$ is a maximal subgroup of G","If G is a primitive group action on A, then $G_a$ is a maximal subgroup of G A block is a subset $B \subseteq A$ such that for any $\sigma \in G$, either $\sigma (B) = B$ or $\sigma(B) \cap B = \emptyset$.  A transitive group action is called primitive if the only blocks in $A$ are single elements $a \in A$ or $A$ itself. I'm having trouble showing that the stabilizer of an element $G_a$ is maximal in $G$.  Its fairly easy to show that $G_a \le G_B$, but moving beyond that point is hard.  I googled and found a site saying that there is a one to one correspendance between blocks countaining $a$ and subgroups containing $G_a$, but I don't see why this is true.","['group-theory', 'abstract-algebra']"
89987,Proof that a given value is NOT the limit of a sequence.,"In the sequence: $$\lim _{n\rightarrow \infty }{\frac {n+1}{2\,n+3}}\neq 3$$ I know how to prove that the limit is actually $1/2$, but is there another way to prove that 1 is NOT the limit? I tried to prove by negation showing that if 1 is the limit I can't find an $N$ that for every epsilon etc etc but got a bit lost. I know this is trivial but would appreciate the help. 
Thanks","['calculus', 'limits']"
90006,Differential Equations reference for Putnam preparation,"I have two problem collections I am currently working through, the ""Berkeley Problems in Mathematics"" book, and the first of the three volumes of Putnam problems compiled by the MAA. These both contain many problems on basic differential equations. Unfortunately, I never had a course in differential equations. Otherwise, my background is reasonably good, and I have knowledge of real analysis (at the level of baby Rudin), basic abstract algebra, topology, and complex analysis. I feel I could handle a more concise and mathematically mature approach to differential equations than the ""cookbook"" style that is normally given to first and second year students. I was wondering if someone to access to the above books that I am working through could suggest a concise reference that would cover what I need to know to solve the problems in them. In particular, it seems I need to know basic solution methods and basic existence and uniqueness theorem. On the other hand, I have no desire to specialize in differential equations, so a reference work like V.I Arnold's book on ordinary differential equations would not suit my needs, and I certainly don't have any need for knowledge of, say, the Laplace transform or PDEs. To reiterate, I just need a concise, high level overview of the basic (by hand) solution techniques for differential equations, along with some discussion of the basic uniqueness and existence theorems. I realize this is rather vague, but looking through the two problem books I listed above should give a more precise idea of what I mean. Worked examples would be a plus. I am very unfamiliar with the subject matter, so thanks in advance for putting up with my very nebulous request. EDIT: I found Coddington's ""Intoduction to Ordinary Differential Equations"" to be what I needed. Thanks guys.","['ordinary-differential-equations', 'reference-request']"
90015,A problem related to basic martingale theory,"In our probability theory class, we are supposed to solve the following problem: Let $X_n$, $n \geq 1 $ be a sequence of independent random variables such that 
  $ \mathbb{E}[X_n] = 0, \mathbb{Var}(X_n)  = \sigma_n^2 < + \infty $ and 
  $ | X_n | \leq K, $ for some constant $ 0 \leq K < + \infty,  \ \forall n \geq 1$. Use martingale methods to show that 
  $$ \sum \limits_{n = 1}^{\infty} \ X_n \ \mbox{ converges }  \mathbb P-\mbox{a.s.}  \  \Longrightarrow  \ \sum\limits_{n = 1}^{\infty} \ \sigma_n^2 < + \infty .$$ Could anybody give me a hint? 
Thanks a lot for your help! Regards, Si","['probability-theory', 'martingales']"
90029,random walk along edges of tetrahedron --- which face gets hit last?,"Suppose we have a tetrahedron $abcd$, and start at edge $ab$.  Now walk to any ""adjacent"" edge (i.e. in this case any edge other than $cd$), each with equal probability $1/4$.  This gives a stationary Markov process on the edges. Walking from one edge to another uniquely determines the face that they share.  In this case, we say the face was ""hit."" For example, if you start at edge $ab$ and walk to edge $bd$ then face $abd$ has been hit.  As we continue the random walk indefinitely, the probability approaches $1$ that every face has been hit at least once. What is the probability that face $abc$ is the last of the four faces to be hit?  This tells us the probability for every face, since the probability for $abc$ and $abd$ being the last face hit is the same by symmetry (since we start at edge $ab$), and similarly the probability of $bcd$ and $acd$ being hit last is also the same. The answer that I'd most like is that the probability of being hit last is the same for every face $1/4$, but I don't know if that's true.","['markov-chains', 'probability', 'combinatorics']"
90038,Find $\lim\limits_{x\to \infty} (2\arctan x -\pi)\ln x$,I am trying to find this limit but I failed. I tried to split it and find that the two limits exist and then find an answer but with no luck. I also tried changing but it didn't help. $$\lim_{x\to \infty} (2\arctan x -\pi)\ln x$$ Any hints?,"['calculus', 'limits']"
90045,Is there a name for this problem I can search for approaches on,"I have a collection of a collection of numbers that I need to find the smallest number of groups to put them into whereas the distinct set of numbers in each set does not exceed a threshold. For example: Collection A: 1,2,3
Collection B: 1,2,4,5
Collection C: 1,3,5,7,9
Collection D: 6,7,8,9,10,11
Threshold: 6 A valid solution would be: Grouping 1: A,B {1,2,3,4,5}
Grouping 2: C   {1,3,5,7,9}
Grouping 3: D   {6,7,8,9,10,11} Or Grouping 1: B   {1,2,4,5}
Grouping 2: A,C {1,2,3,5,7,9}
Grouping 3: D   {6,7,8,9,10,11} In my real problem the number of collections is 1000+ each with 50-300 numbers and my Threshold is 600. I am looking for the following. Is there a common mathematical name for this type of problem I can research. Is there a good approach to solving such a problem which results in the smallest number of Groupings. Determining the affinity of 2 Collections is trivial (Union and compare vs each individual collection), but when I start to think about an arbitrary number of collections being grouped I can't seem to think of any non-brute force way to attacking the problem which would take a tremendous amount of time for a computer to solve after even 100 collections.",['combinatorics']
90046,Irreducibility of a cubic in $\mathbb{Q}[X]$,"Problem Show that $$X^3-2008X^2+2010X-2009$$ is irreducible in $\mathbb{Q}[X]$ . Progress I considered applying Eisenstein's Theorem, but there are no primes $p$ such that $p|2008$ , $p|2009$ and $p|2010$ . (This is quite clear as $\nexists p$ prime such that p divides consecutive integers for any choice of p.) I think this may require an application of Gauss's Lemma, but I'm yet to successfully show it. Any help would be appreciated. Regards.","['abstract-algebra', 'polynomials']"
