question_id,title,body,tags
2157090,write the given matrix as a product of elementary matrices,"Please explain this in detail because i simply cannot understand previous explanations I have read for this type of problem.
By the way this is from elementary linear algebra 10th edition section 1.5 exercise #29. There is a copy online if you want to check the problem out. Write the given matrix as a product of elementary matrices.
\begin{bmatrix}-3&1\\2&2\end{bmatrix}",['linear-algebra']
2157091,Non existence of two specific polyhedra,"I'm trying to solve my own question . I almost got the answer (which I'll post in a few days), but there are two things I'm not able to prove but that I know are true ( On Polyhedra with Specified Types of Face ): There does not exists a convex heptahedron with only quadrilateral faces There does not exists a convex $14$-hedron with exactly $14$ pentagonal faces The heptahedron if it exists would have $9$ vertices, $14$ edges and $7$ faces. I can prove that it has $8$ vertices of degree $3$ and one of degree $4$. The $14$-hedron if it exists would have $23$ vertices, $35$ edges and $14$ faces. I can prove that it has $22$ vertices of degree $3$ and one of degree $4$. Question: Why do such polyhedra not exist? Remark: An argument of the non-existence of only one polyhedron is good enough for an answer. I could extrapolate it to the other case.","['polyhedra', 'graph-theory', 'geometry']"
2157112,"Geometric stationary distribution, ergodic theorem and mean return time","A person climbs an infinite ladder. At each jump, the person can jump up one step with probability $1-p$ while the person slips off and falls all the way to the bottom with probability $p$. a) Represent the person's height on the ladder as a Markov chain and show that the stationary distribution is geometric and find its parameter. b) What is the long-run proportion of time for which the frog is on the first step above the bottom of the ladder? c) If the person has just fallen to the bottom, on average how many jumps will it take before he reaches step $k$ So far I have the following: Let $X_n$ be the step number at time $n$. Because $P(X_{n+1} = i+1| X_n =i) = 1-p$ and $P(X_{n+1}=0 | X_n = i)=p$, we can find the following system of equations such that: $\pi_0 =p[\pi_0 +\pi_1 +\pi_2 +...\pi_\infty] = p*1$, $\pi_1 = (1-p)\pi_0$, $\pi_2 = (1-p)\pi_1$,... and thus we find the pattern $\pi_n = p(1-p)^n$; however, I'm not sure if this is correct and how to proceed with the subsequent parts, especially given the infinite state space","['stochastic-processes', 'markov-chains', 'probability-theory', 'ergodic-theory']"
2157121,Small example of Zappa-Szep product of finite groups,"Does anyone know a good place to find an example of a group, $G$, of ""small"" order in particular finite such that there are subgroups $H,K$ of $G$ with $HK=G$ and $G$ a Zappa-Szep product of $H$ and $K$ (in particular neither $H$ nor $K$ is normal?","['finite-groups', 'abstract-algebra', 'group-theory']"
2157147,Elegant solution to $\lim\limits_{n \to \infty}{[n(\sqrt[n]{a} - 1)]}$,Exercise: $$\lim\limits_{n \to \infty}{[n(\sqrt[n]{a} - 1)]} \text{ where a > 0}$$ Attempt: $\lim\limits_{n \to \infty}{[n(\sqrt[n]{a} - 1)]}$ Let $m = \sqrt[n]{a}$. $m = \sqrt[n]{a} \longrightarrow m^n = a \longrightarrow n = \log_ma = \frac{\ln a}{\ln m}$ $\lim\limits_{n \to \infty}{\sqrt[n]{a}} = 1 \longrightarrow \lim\limits_{m \to 1}{[\frac{\ln a}{\ln m}(m - 1)]} = D$ $D = \lim\limits_{m \to 1}{[\frac{\ln a}{\ln m}(m - 1)]} = \ln a \lim\limits_{m \to 1}{\frac{m - 1}{\ln m}} = \ln a (\lim\limits_{m \to 1}{\frac{\ln m}{m - 1}})^{-1} = \ln a C^{-1} = \frac{\ln a}{ln C}$ $C = \lim\limits_{m \to 1}{\frac{\ln m}{m - 1}} = 1$ (omitted steps) $D = \frac{\ln a}{1} = \ln a$ $$\lim\limits_{n \to \infty}{[n(\sqrt[n]{a} - 1)]} = \ln a$$ Request: Is there a more elegant solution to this exercise?,"['real-analysis', 'alternative-proof', 'calculus', 'limits']"
2157192,Extending holomorphic $f$ on the unit disk to the boundary as $1/z$,I found the following problem statement in a book (Stein and Shakarchi): Show that there is no holomorphic function $f$ on the unit disk $\mathbb{D}$ that extends continuously to $\partial D$ such that $f\left(z\right) = 1/z $ for $z\in \partial D$ I know that I need uniform continuity of $f$ on $\mathbb{D}$ in order to show: $$\lim_{r\rightarrow 1^{-}} \int_{C_{r}}f = \int_{C_{1}} f = \int_{C_{1}} (1/z) = 2 \pi  i$$ And then I can get a contradiction. I think I remember $f$ being uniform continuous if it is continuous on a compact set - however I am not sure how to prove this. Also not sure how to prove the integral equations above given this fact. Any suggestions?,"['uniform-continuity', 'complex-analysis']"
2157226,Interesting examples of differentiation under the integral sign?,"I was recently looking through integration techniques when I came upon differentiation under the integral sign (DUIS).  It seems to be pretty powerful, for example: $$f(t)=\int_0^1\frac{x^t-1}{\ln(x)}\ dx\implies f'(t)=\int_0^1x^t\ dx=\frac1{t+1}\\\implies f(t)=C+\ln(t+1)\\f(0)=0\implies C=0\\\implies\int_0^1\frac{x^t-1}{\ln(x)}\ dx=\ln(t+1)$$ Now, on my own, proving that $\int_0^1\frac{x^t-1}{\ln(x)}\ dx=\ln(t+1)$ would've been a hefty task without DUIS, and so I wanted to ask this question: It's hard to produce interesting examples where applying DUIS is almost magical, so what are some good example uses of DUIS?","['leibniz-integral-rule', 'big-list', 'integration', 'calculus']"
2157251,Questions on proof of theorem 2.30 in Baby Rudin.,"Before I ask questions about the proof I am having trouble differentiating between these two definitions that are related to the proof. Def. 1. $E$ is open in $X$ means to each point $p \in E\ \exists r>0$ such that $$ d(p,q)<r,\ q \in X \Rightarrow q \in E. $$ Def. 2. $E$ open relative to $Y$ if to each point $p \in E\ \  \exists r>0$ such that $q \in E$ whenever $$d(p,q)<r\ \&\ q \in Y$$ To me, both of these definitions seem to be the same (and trivial, to be honest). So what is the ""big"" difference? And now, this is the proof my professor gave in class which is more or less the same as Rudin's. I am looking for clarification on some parts (to be honest, most parts) of the proof. My comments and questions in bold refer to the statement above. $\textbf{Theorem}$. Suppose $Y \subseteq X$. A subset $E$ of $Y$ is open relative to $Y$ if and only if $E = Y \cap G$ for some open subset $G$ of $X$. $Proof.$ $(\Rightarrow)$ Suppose $E$ is open relative to $Y$. To each $p \in E,\ \exists r_p >0$ such that $$d(p,q) < r_p,\ q \in Y \Rightarrow q \in E.$$ Let $V_p$ be the set of all $q \in X$ such that $d(p,q) < r_p$ and define $G = \cup _{p \in E}\ V_p$. Why do this? Just because is works? Then $G$ is an open subset of $X$. $G$ is an open subset of $X$ because arbitrary union of open sets is open. Right? Since $p \in V_p\ \forall p \in E$,  it is clear that $E \subseteq G \cap Y.$ This is not ""clear"" like my professor states, please explain. By our choice of $V_p$, we have $V_p \cap E \subseteq E$ for every $p \in E$ so that $G \cap Y \subseteq E$. This makes sense in my head if I think in terms of sets in general. It's always true that the intersection of two sets is a subset of each of the sets, so it's no different in this case. BUT the words ""by our choice"" is bothering me, should it? Lastly, I don't understand why the intersection of $G$ and $Y$ would be a subset of $E$. Hence, $E = G \cap Y$. $(\Leftarrow)$ Conversely, if $G$ is open in $X$ and $E = G \cap Y$, every $p \in E$ has a neighborhood $V_p \subseteq G$. Then, $$V_p \cap Y \subseteq E$$ so that $E$ is open relative to $Y$. $□$ Thank you all in advance for the help.","['general-topology', 'real-analysis']"
2157282,What are the difference between $L^2(S^2) $ and $L^2(T^2)$?,"$S^2$ is 2-dim sphere , $T^2$ is 2-dim torus. $L^2$ is Lebesgue spaces.   What is the difference of $L^2(S^2)$ and $L^2(T^2)$ ? In fact, I don't know how to define the difference ,because isomorphism of Hilbert space can't distinguish them. But I  feel there should be some difference. This is a loose question, I am sorry for this , but I really don't know how to ask it exactly.","['banach-spaces', 'hilbert-spaces', 'differential-geometry', 'general-topology', 'topological-vector-spaces']"
2157338,The deep reason why $\int \frac{1}{x}\operatorname{d}x$ is a transcendental function ($\log$) [duplicate],"This question already has answers here : Demystify integration of $\int \frac{1}{x} \mathrm dx$ (11 answers) Closed 3 years ago . In general, the indefinite integral of $x^n$  has power $n+1$.  This is the standard power rule.  Why does it ""break"" for $n=-1$?  In other words, the derivative rule $$\frac{d}{dx} x^{n} = nx^{n-1}$$ fails to hold for $n=0$.
Is there some deep reason for this discontinuity?","['logarithms', 'indefinite-integrals', 'real-analysis', 'calculus']"
2157372,How to find the value of c that makes this PDF valid?,"Trying to figure out this stats problem, but I'm feeling stuck on it: The PDF for a continuous random variable X is the following: $f(x)$ = $\dfrac{c}{x^4}$ if $x>2$ and $0$ otherwise. What is the value of $c$ that makes this PDF valid? It hints that $\lim_{n\to \infty} n^{-a} = 0$ for any constant $a>0$ . I'm not quite sure how to interpret this hint or how to solve the problem. Thanks for the help!","['statistics', 'probability-distributions']"
2157429,Is there a name for this set identity?,"I proved that the following identity is true. It seems relatively simple and somewhat applicable, so I was wondering if there is a name for it (maybe something like De Morgan's Laws, for example). $$(A \setminus B) \cup (B \setminus A) = (A \cup B) \setminus (A \cap B)$$.",['elementary-set-theory']
2157435,Prove that no integer $w$ exists such that the splitting field of $x^4+x^3+x^2+x+1+w$ is that of $x^4+x^3+x^2+x+1$.,"For $x^2+x+1$ , there exist infinitely many integers $w$ such that the splitting field of $x^2+x+1+w$ is the same field as $x^2+x+1$ .
Examples of such integers $w$ are $$6, 60, 546, 4920,\ldots\,.$$ There is a closed form for odd $k$ , namely $$w=\frac{3^k+1}{4}-1\,.$$ I can't seem to prove / disprove this for the fact of whether there are infinitely many integers $w$ such that the splitting field of $x^4+x^3+x^2+x+1+w$ is the same field as the splitting field $x^4+x^3+x^2+x+1$ . The same with whether there are infinitely many integers $w$ such that the splitting field of $x^6+x^5+x^4+x^3+x^2+x+1+w$ is the same field as the splitting field $x^6+x^5+x^4+x^3+x^2+x+1$ . If there is at least one integer $w$ for degrees $4$ or $6$ , please post them as a counterexample.","['polynomials', 'splitting-field', 'number-theory', 'field-theory', 'cyclotomic-fields']"
2157466,"When and When Not to Use ""Limit Arithmetic""","Off the top of my head, here are examples that do and don't let you use ""limit arithmetic"" (addition in both cases). Ex. 1: $\lim\limits_{x \to \infty}{(\sqrt{x - a} - \sqrt{x})}$ When solving, you cannot do this:
$\lim\limits_{x \to \infty}{(\sqrt{x - a} - \sqrt{x})} = \lim\limits_{x \to \infty}{\sqrt{x - a}} - \lim\limits_{x \to \infty}{\sqrt{x}}$ Ex. 2: $\lim\limits_{x \to 0}{\frac{e^{ax}-e^{bx}}{x}}$ Yet, here you can do this:
$\lim\limits_{x \to 0}{\frac{e^{ax}-e^{bx}}{x}} = \lim\limits_{x \to 0}{(\frac{e^{ax}-1}{x} - \frac{e^{bx}-1}{x})} = \lim\limits_{x \to 0}{\frac{e^{ax}-1}{x}} - \lim\limits_{x \to 0}{\frac{e^{bx}-1}{x}}$ Why is limit arithmetic is valid in only some cases? When can you use it?","['real-analysis', 'calculus', 'limits']"
2157531,How does one show that $\int_{0}^{1}{1\over 1+\phi x^4}\cdot{\mathrm dx\over \sqrt{1-x^2}}={\pi\over 2\sqrt{2}}?$,"Consider $$\int_{0}^{1}{1\over 1+\phi x^4}\cdot{\mathrm dx\over \sqrt{1-x^2}}={\pi\over 2\sqrt{2}}\tag1$$
  $\phi$;Golden ratio An attempt: $x=\sin{y}$ then $\mathrm dx=\cos{y}\mathrm dy$ $(1)$ becomes $$\int_{0}^{\pi/2}{\mathrm dy\over 1+\phi \sin^4{y}}\tag2$$ Apply Geometric series to $(2)$, $$\int_{0}^{\pi/2}(1-\phi\sin^4{y}+\phi^2\sin^8{y}-\phi^3\sin^{12}{y}+\cdots)\mathrm dy\tag3$$ $${\pi\over 2}-\int_{0}^{\pi/2}(\phi\sin^4{y}-\phi^2\sin^8{y}+\phi^3\sin^{12}{y}-\cdots)\mathrm dy\tag4$$ Power of sine seem difficult to deal with How else can we tackle $(1)?$","['golden-ratio', 'integration', 'definite-integrals', 'calculus']"
2157538,Find the smallest positive integer for which x mod 3 = 2 and x mod 4 = 3,"I came across a question on a piece of homework and my solution yields no possible solutions. Find the smallest positive integer for which
x mod 3=2 and x mod 4=3 (and then find the next one) So I figure (by the Quotient-Remainder Theorem) n mod 3 = 2 => n = 3q + 2 n mod 4 = 3 => n = 4q + 3 No problems thus far and everything works out fine logically so I set the equations equal to each other and find that q = -1. Wait, it can't be negative. So I put it on a graph to check my comprehension and like I thought that is the only point at which they meet is outside my possible domain. Could someone point out the error in my logic here? Or perhaps I've misinterpreted the question? Apologies if the answer is simple and I've missed it.","['modular-arithmetic', 'discrete-mathematics']"
2157605,Caculate the probability of sum of normal distribution,"$x_i \sim  \mathcal N(\mu,\sigma)$  $(i = 1,2,...n)$ are i.i.d normal distributed, $a$ is a constant, $a>0$, how to calculate the probability: $$P(x_1 < a, x_1 + x_2 < a, x_1 + x_2 + x_3 < a,..., x_1 + ... + x_{n-1} < a, x_1 + x_2 + ... + x_n >= a)$$ maybe we can translate to: $z_i \sim  \mathcal N(0,1)$  $(i = 1,2,...n)$ are i.i.d normal distributed, $a$ is a constant, we denote $$a_n = \frac{a - n  \mu}{\sigma\sqrt{n}}$$ the probability may transformed into: $P(z_1 < a_1, z_1 + z_2 < a_2, z_1 + z_2 + z_3 < a_3,..., z_1 + ... + z_{n-1} < a_{n-1}, z_1 + z_2 + ... + z_n >= a_n)$ now we denote: $P_1 = P(z_1 < a_1, z_1 + z_2 < a_2, z_1 + z_2 + z_3 < a_3,..., z_1 + ... + z_{n-1} < a_{n-1}, z_1 + z_2 + ... + z_n >= a_n)$ and $P_n = P(z_1 < a_1, z_1 + z_2 < a_2, z_1 + z_2 + z_3 < a_3,..., z_1 + ... + z_{n-1} < a_{n-1}, z_1 + z_2 + ... + z_n < a_n)$ that means $P_{n-1} = P_1 + P_n$ so the problem can be solved by get $P_n$, but how to calculate it?","['statistics', 'probability']"
2157606,"How is ""point"" in geometry undefined? And What is a ""mathematical definition""? [duplicate]","This question already has answers here : What is the exact difficulty in defining a point in Euclidean geometry? (8 answers) Closed 7 years ago . How is ""point"" in geometry undefined? I mean, when we say ""A point in geometry is a location. It has no size, i.e., no width, no length, and no depth,"" is it not a definition?  If it is not a definition, then how can we know whether some statement is definition or not? What are the characteristics of a definition in math?","['definition', 'soft-question', 'geometry']"
2157620,Finding the $3 \times 3$ binary matrix with the least spectral radius greater than $1$,"Let $A$ be a $3 \times 3$ matrix whose elements are only $0$ and $1$. Let $|\lambda_A^{\max}|$ be the spectral radius , i.e., the maximum absolute value of eigenvalues of $A$. How can I choose the matrix $A$ such that $\log |\lambda_A^{\max}|$ has the minimal positive value? Can someone give me a hint? Thank you!","['eigenvalues-eigenvectors', 'matrices', 'discrete-optimization', 'spectral-radius', 'linear-algebra']"
2157661,Help needed on laurent series for a complex function,"I'm struggling to understand the ways in which one could find the laurent series and there for the residuals for: Find the Laurent series expansion and residue at 
  $$
\left(\frac{z}{z-1}\right)^2
$$
  for $z = 1$ Any help that could be provided as to where to start would be appreciated. I attempted differentiating the Laurent series expansion for $$
\frac{1}{z-1}
$$ Aswell as trying to multiply out the coefficients of the Laurent expansion for $$
\frac{1}{z-1}
$$ But have had no luck whatsoever and just get myself into a state.","['laurent-series', 'complex-analysis', 'sequences-and-series']"
2157679,Embedding theorem for finite p-groups,"One knows that any countable group can be embedded in a 2-generated group. Also, any finite group is a subgroup of a 2-generated finite group, namely by the Cayley embedding. Does the same hold if we restrict to finite $p$-groups, where $p$ is some prime, i.e. is any finite $p$-group a subgroup of a 2-generated finite $p$-group? Note that the minimal number of generators of the Sylow subgroup of the symmetric groups grow with their order. Thus the Cayley embedding would not suffice.","['finite-groups', 'group-theory']"
2157699,Is every finite subgroup of $C^*=$ set of all non-zero complex numbers cyclic?,Is every finite subgroup  of $C^*=$ set of all non-zero complex numbers cyclic? I see that the set $A_n=\{z:z^n=1\}$ is a subgroup of $C^{*}$. Any element of $A_n$ is a solution of $z^n=1$.Now the solutions of $z^n=1$ for any $n\in \Bbb N$ are $e^{\frac{{2ki\pi}}{{n}}};1\le k\le n$ and hence the subgroup $A_n$ is generated by $a=e^{\frac{{2ki\pi}}{{n}}}$ and hence cyclic. But is it the case that any finite subgroup of $C^{*}$ is of the form $A_n$ for some $n$? I am having problems to answer this question.If it is true then it answers the original question. Please help.,"['abstract-algebra', 'group-theory']"
2157728,Integral and unit of measurement,"Is there a kind of ""formalism"" which define how unit measures come out from integration? An example: given a point $P(x,y,z) \in \mathbb{R}^3$ there is the concept of mass $m$ associated to this point. Mass is measured in $\text{kg,g,lb,...}$ I indicate the generic unite measure of the mass $[m]$. Now, there is also the concept of density of mass $\rho(x,y,z)$ which is a (scalar) function which represents ""mass per unit volume"", or also $\frac{[m]}{[s]^3}$ (where $[s]$ is the unit measure of space), e.g. if we take a constant density over a volume, to know the mass of the volume it suffices to multiply density $\rho$ with volume $V$. The effect of this multiplication is coherent with unit measures involved. Now, in general for non-constant density function, one needs to integrate the function over the volume to know mass: $m=\int_V \rho(x,y,z)\text{d}\tau$ where $\tau$ is the volume element. Now, integrals are pure mathematical objects, how can I relate the fact that an integral is not only (naively) ""a product of the integrand for the measure of the space of integration"" with the fact that, in the end, there will be $[m]=\frac{[m]}{[s]^3} [s]^3$ Now, naively I can argument something like this $\int_{[s^3]} \frac{[m]}{[s]^3} \text{d}([s^3])$, but the integrand is constant so $\frac{[m]}{[s]^3} \int_{[s^3]} \text{d}([s]^3)=[m]$. But here there is no mathematical formalism, only a naive thought about such an ""integral of unit measures"" sounds to me. Is there actually an ad-hoc formal argument for this problem?","['physics', 'calculus', 'integration', 'definite-integrals', 'measure-theory']"
2157745,State space discretization,"I have a question for you: I'm playing with a continuous state space system and I have to discretize it. I've looked around and it seems pretty easy: $\dot{x}(t) = Ax(t) + Bu(t)\ $ and  $\ y(t) = Cx(t) + Du(t) = Cx(t) \rightarrow$ because $D = 0$; Turns to: $x(k+1) = A_dx(k) + B_du(k)\ $ and $\ y(k) = Cx(k)$ where: $A_d = e^{AT_s}\ $ and $\ B_d = A^{-1}(A_d-I)B$ My problem is that when computing $B_d$, the matrix $A^{-1}$ increases a lot and destroys the subsequent state prediction. I assumed that discretizing my state space was making it unstable, so I checked the eigenvalues of $A_d$. There is one of them that is positive. Is there any solution to avoid this non stability? I need this discrete system. What would you do?","['stability-theory', 'dynamical-systems', 'discrete-mathematics']"
2157748,How do I think of a measurable function?,"Although I have always known the definition of measurability in terms of pre images of measurable sets being measurable, I don't really conceptually understand the purpose of measurable functions or what it means. More specifically, this dawned on me when I was looking at a paper talking about limits of a measurable function, $U:\mathbb{R_+} \rightarrow \mathbb{R_+}$ (the limit was $\lim_{t\rightarrow \infty}\frac{U(tx)}{U(t)}$), without specifying why the function had to be measurable at all. It's left me in a lot of doubt of my own knowledge of measure theory. Is anyone able to give me some intuition and a possible reason for measurability with regards to taking limits?",['measure-theory']
2157769,Stone–Čech compactification as a functor.,"I am now working on Munkres Topology,Stone–Čech compactification  part. He says that the correspondence between a completely regular space and its Stone–Čech compactification is a funtor. To verify this, I need to show that the correspondence preserves the identity mapping and composites of functions. It was easy to show the former, but I am not sure how to do the latter...
The situation is: Let $\beta(X)$ denote a Stone–Čech compactification  of a topological space $X$. Let  $X,Y,Z$ be completely regular spaces. Let $f:X\rightarrow Y$ ,  $g:Y\rightarrow Z$ be continuous maps. Let $\beta(f):\beta(X) \rightarrow \beta(Y)$  extend $\iota \circ f$, where $\iota:Y \rightarrow \beta(Y)$ is an inclusion mapping. What I need to show is that $$\beta(g\circ f)=\beta(g)  \circ\beta(f)$$ How can I show this? It seems obvious if $x\in X$. But how can I show for the case $x\in \beta(X)-X$?
Any help would be really appreciated. Thanks.","['category-theory', 'general-topology']"
2157776,How do I compute hitting time in a finite Markov chain? (to quantify my Machi Koro pwnage),"In a game of Machi Koro, I had the Sushi Bar (active) while my opponent had a Wheat Field and Bakery with no low-roll buildings available. I want to know how long they have to roll before they can afford the Train Station. That is, they're engaged in a Markov chain where the state space is ""0 to 4 coins"" ($\{0, 1, 2, 3, 4\}$) and I want to know the probability, for each $n$, that they reach $4$ for the first time after exactly $n$ transitions (preferably for all starting states, but just for zero is interesting enough).  I might also be interested in the cumulative version: for each $n$, what's the likelihood that they'll have reached $4$ at least once after $n$ transitions. The transition probabilities are as follows: from $k$ coins, $k < 4$: to $1$ with probability $1/6$; otherwise, to $k$ with probability $3/6$ and to $k+1$ with probability $2/6$.  Since the transitions from $4$ to anything are not interesting, I'll just define them arbitrarily as $4$ to $4$ with probability $1$ (really they'll spend all their coins, but the questions don't care what happens after $4$ is reached). Spelled out: $
\left[
\begin{matrix}
3/6 & 3/6 &     &     & \\
    & 4/6 & 2/6 &     & \\
    & 1/6 & 3/6 & 2/6 & \\
    & 1/6 &     & 3/6 & 2/6 \\
    &     &     &     & 1
\end{matrix}
\right]
$ (The probability in row $i$ column $j$ is the probability of transition from $i$ to $j$ coins in a single step.) Some analysis: in the transition graph, there are strongly connected components $\{0\}, \{1, 2, 3\}$ and $\{4\}$ with transitions $0 \rightarrow 1$ and $3 \rightarrow 4$. Maybe it helps to analyse each scc separately. You go from zero coins to one in $k$ steps, $k > 0$, with probability $2^{-k}$; the big one essentially boils down to ""how soon will you increment your state three times in a row (each w. prob. $2/6$) without falling back to $1$ (w. p. $1/6$ each step)"". I'm not sure how to analyze this, though. I could probably spell my way through https://en.wikipedia.org/wiki/Markov_chain but given that my matrix-fu is a little rusty, some hand-holding would be appreciated. Edit : Come to think of it, given the City Hall the transition from $0$ is to $1$ with probability $1$. The 50/50-out-of-0 would occur if I had three Cafés instead of the Sushi Bar and we were playing without the Harbor expansion.  Feel free to analyze either—or both! Also, for purposes of simplifying the math, in both cases I'm ignoring the fact that my opponent earns a coin whenever I roll a 1.","['matrices', 'markov-chains', 'probability', 'dice', 'recreational-mathematics']"
2157778,Prove that for $x>0$ we have $\sin{x}<\frac{x}{\sqrt{1+\frac{x^2}{3}}}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Prove the inequality: $$\sin{x}<\frac{x}{\sqrt{1+\frac{x^2}{3}}}$$
for real $x>0$. Any hints? EDIT: Using calculus and derivatives is perfectly allowed.","['algebra-precalculus', 'inequality', 'trigonometry']"
2157805,Inverse of cosine,"Always we talk about inverse of cosine in $[0 , \pi]$ interval but the cosine function also is one-to-one in $[\pi  , 2\pi] , [2\pi , 3\pi] , \dots$ . If we consider $[\pi  , 2\pi]$ interval the graph of $\arccos$ will be different from $[0 , \pi]$. Is this true ? And if it is true many formulas will change !","['trigonometry', 'inverse-function']"
2157827,Geometric intuition for flat morphisms,"I'm trying to develop some geometric intuition for what it means for a morphism of schemes to be flat. The definition of flatness in Hartshorne says (if I'm correct) that a morphism $f: X \to Y$ is flat iff pullbacks of SESs of quasicoherent sheaves on $Y$ are exact on $X$. But this is very algebraic, and not at all easy to visualise! The most helpful thing I've found in Hartshorne is Prop. 9.7: If (for instance) $X$ and $Y$ are varieties and $Y$ is smooth of dimension 1, then $f$ is flat iff the image of every irreducible component of $X$ is dense in $Y$. Thus the irreducible components of $X$ lie ""flat"" over $Y$, hence the terminology. But what if $Y$ is of dimension bigger than 1? What is the intuition for flatness now? And is there another way to think about flat morphisms which is more intuitive altogether?",['algebraic-geometry']
2157914,Number of conjugacy classes are there over $\mathbb{F}_p$,I am struggling with the next exercise of my HW: How many conjugacy classes are in $GL_3(\mathbb{F}_p)$? And how many in $SL_2(\mathbb{F}_p)$? It's on the topic of Frobenius normal form of finitely generated modules over $\mathbb{F}_p$. I'd appreciate any idea.,"['modules', 'group-theory']"
2157946,Is there a function satisfying the following condition?,"I have a question about measure theory. Let $(X,\mathcal{F},\mu)$ be a measure space. Let $f$ be a $\mu$-integrable nonnegative function on $X$. I'm looking for a function $\varphi$ satisfying the following: $\varphi: \mathbb{R} \to \mathbb{R}$, nondecreasing, $\lim_{x \to \infty} \varphi(x)/x=\infty$, $\int_{X} \varphi \circ f\,d\mu<\infty$. My attempt \begin{align*}
\int_{X} \varphi \circ f\,d\mu&=\int_{0}^{\infty} \varphi(x)\, \nu(dx)=\int_{0}^{\infty} \nu(\{\varphi>t\})\,dt,
\end{align*}
where $\nu(A)=\mu(f^{-1}(A))$, $A \in \mathcal{B}(\mathbb{R})$. From the Markov inequality, 
\begin{align*}
\nu(\{\varphi>t\}) \le \frac{1}{t} \int_{0}^{\infty}\varphi \,d \nu.
\end{align*}
However, $\int_{1}^{\infty} 1/t\,dt=\infty$. Is there sharper upperbound of $\nu(\{\varphi>t\})$ under a suitable condition on $\varphi$? What $\varphi$ should satify? If you know, please let me know.","['real-analysis', 'measure-theory']"
2157983,Prime ideals of the complete ring of quotients,"For a commutative unitary ring $R$, let $Q(R)$ be the complete
ring of quotients of $R$, please see
( http://planetmath.org/completeringofquotients ) for more
information. In general,  the  complete ring of quotients  may be
larger than the ""classical ring of quotients"", that is,
$R\subseteq T(R)\subseteq Q(R)$, where $T(R)=S^{-1}R$ is classical
ring of quotients of $R$ with $S$ is the set of all non-zero
divisors of $R $. There is a well-known characterization for the set of
all prime ideals of $T(R)$. Is there a similar description for
the set of all prime ideals of $Q(R)$?","['localization', 'maximal-and-prime-ideals', 'algebraic-geometry', 'commutative-algebra']"
2158019,"True or False : If $f(x)$ and $f^{-1}(x)$ intersect at an even number of points , all points lie on $y=x$","Previously I have discussed about odd number of intersect points (See : If the graphs of $f(x)$ and $f^{-1}(x)$ intersect at an odd number of points, is at least one point on the line $y=x$? )
Now , I want to know the even condition . For example $f(x) = \sqrt{x}$ and $f^{-1}(x) = x^2 , x\ge 0$ intersects each other in $(0,0)$ and $(1,1)$ points and these points located on $y=x$ line Edit : Consider $f$ is continuous function.","['inverse-function', 'functions', 'graphing-functions']"
2158052,Prove the dual space of $l^p$ is isomorphic to $l^q$ if $\frac{1}{q}+\frac{1}{p}=1$,"Prove the dual space of $\ell^p$ is isomorphic to $\ell^q$ if $\frac{1}{q}+\frac{1}{p}=1$ ($1<p<\infty$) Define a map $J:\ell^q \to (\ell^p)'$ such that $Jy(x)=\sum_{k=1}^\infty x_ky_k,x\in \ell^p,y\in \ell^q$ I have verified that $Jy\in (\ell^p)'$, $J$ is linear and $\lVert Jy \rVert\leq \lVert y \rVert_q$. How to show $\lVert Jy \rVert \geq \lVert y \rVert_q$ and $J$ is surjective?","['functional-analysis', 'lp-spaces', 'banach-spaces', 'dual-spaces']"
2158071,Conversion of symmetric matrix into skew-symmetric matrix by flipping signs of elements,"We are given an $n \times n$ matrix $A=(a_{ij})_{i,j\in\{1,2,\ldots,n\}}$ with the following properties : $A$ is symmetric, $A$ has a zero diagonal, every element of $A$ is a number in $\{0,1,2\}$, every row sum of $A$ is an odd number. We say that we flip the sign of an element of $A$ if we change the element from some $a_{ij}$ to $-a_{ij}$. Prove that it is always possible to perform a finite number of sign flips on $A$ to to obtain a new matrix $B=(b_{ij})_{i,j\in\{1,2,\ldots,n\}}$ such that $B$ is skew-symmetric, and each row sum of $B$ is either $1$ or $-1$.","['matrices', 'combinatorics', 'linear-algebra']"
2158090,"Point out my Fallacy, in combinatorics problem, please.","Five children sitting one behind the other in a five seater merry go round, decide to switch seats so that each child has a new companion in front. In how many ways can this be done? My tries: I try using IEP but didn't work, please point out a fallacy. There are $4!$ without any restriction. Let $p_1$ be the property such that one of them has the same companion in front, similarly for other properties, $p_2,p_3,p_4,p_5$, as well. No. of way in which $p_1$ occur, I used tie method, tie 1st one and the 2nd one, we remain with $3$, which along with tied one can be arranged in $3!$ (circular permutations). Similarly for other properties as well. No of ways in which $p_1\cup p_2$ occur: Now tie three consecutive people, so we remain with $2$, which along with tied peoples can be permuted in $2!$, similarly for other as well. Total results in $5\cdot 2!$. (need to tie consecutive from $5$, not any hence factor with $2$ is not ${{5}\choose{2}}$) No. of ways in which $p_1\cup p_2\cup p_3$ occur: Now, four consecutive people, we remain with $1$, which along with tied can be permuted in $1!$. Total results in $5\cdot 1!=5$ No. of ways in which $p_1\cup p_2\cup p_3\cup p_4$ occur: Now we'll tie $5$ consecutive, so one way. No. of ways in which $p_1\cup p_2\cup p_3\cup p_4\cup p_5$ occur: will be same as No. of ways in which $p_1\cup_2\cup p_3\cup p_4$ occur $=1$ Exploiting IEP:
$$4!-(5\cdot 3!)+(5\cdot 2!)-(5\cdot 1!)+1-1=-1$$ Where I over substracted !!! Please Help.","['permutations', 'combinatorics', 'inclusion-exclusion']"
2158155,Can you get a non-principal ultrafilter on N using Choice but 'avoiding' Zorn's Lemma?,"This question is not about the logical relationships between Choice, Zorn's and the Ultrafilter Lemma, but pedagogical. I am teaching a class and want to go as directly as possible from Choice to a non-principal ultrafilter on $\mathbf{N}$. The Axiom of Choice is much easier to understand than Zorn's Lemma and the definition of a non-principal ultrafilter on the natural numbers is also pretty easy to understand. However, the proof of Zorn's Lemma using the axiom of choice is relatively technical and introducing partially ordered sets and chains just for this purpose feels a little time-consuming. Question. Is there a sneaky construction of a non-principal ultrafilter on $\mathbf{N}$ that directly uses the axiom of choice but avoids introducing the statement of Zorn's Lemma?","['filters', 'alternative-proof', 'elementary-set-theory', 'axiom-of-choice']"
2158159,Dividing a region in two regions of the same area,"Let $\mathscr C$ be a Jordan curve, i.e. a non-self-intersecting continuous loop in the plane. We call $\mathcal R$ the region inside $\mathscr C$ (the region $\mathcal R$ is well-defined because of the Jordan curve theorem ). The curve $\mathscr C$ and the region $\mathcal R$ should look like something like this: Let $P$ be a point of the plane. Does there exist a line $\mathscr D$ such that $P\in \mathscr D$ and $\mathscr D$ divides the region $\mathcal R$ in two smaller regions of the same area? If such a line exists, when is it unique? For the first question, I would tend to think the answer is yes, since the curve $\mathscr C$ is continuous and because of the intermediate value theorem. For the second question, we know that the line $\mathscr D$ can not be unique if $P$ is inside $\mathcal R$ . For example, if $\mathscr C$ is a circle and $P$ is its center, every line $\mathscr D$ works: I do think the line $\mathscr D$ is unique if $P$ is outside the convex hull of $\mathcal R$ .","['curves', 'geometry']"
2158182,What happens to $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$ for functions over curves in $\mathbb{R^2}$?,"Suppose we have $f(x,y)=x+y$, and $\operatorname{domain}(f)=\{(x,y) \mid y=2x\}$. Now, what happens to $\dfrac{\partial f}{\partial x}$ at some point in the domain, say $(x_0,y_0)$? Will it be some real number, $0$ or undefined? According to me, this should be undefined. I think so because one can write $f(x,y)=3x$, and conclude $$\dfrac{\partial f}{\partial x}=3$$
or one may write $f(x,y)=\dfrac{3}{2}y$, and conclude
$$\dfrac{\partial f}{\partial x}=0$$
However, if the function is defined over a curve, then finding the partial derivatives is meaningless, as keeping one quantity fixed is not possible. So, shouldn't this mean that these functions don't have partial derivatives? The question arose in my mind while thinking of finding $\dfrac{df}{dt}$, for parametrisation $$x=t,y=2t$$
If I use total derivatives,
$$\dfrac{df}{dt}=\dfrac{\partial f}{\partial x}\dfrac{dx}{dt}+\dfrac{\partial f}{\partial y}\dfrac{dy}{dt}$$
then I am confused regarding the partials. Shouldn't this operation be undefined for such functions?","['multivariable-calculus', 'partial-derivative']"
2158194,"What values of $a,b\in\mathbb{Z^+}$ satisfy $a^b=b^a+1$?","What values of $a,b\in\mathbb{Z^+}$ satisfy the equation $a^b=b^a+1$ ? I know one answer is $a=3,b=2$, but I know that just by luck. How do I get to the answer? Are there any more of them? Why? Suppose I don't know the answer, how would I start? I started taking logarithms to both sides, then changing $1$ to $b^a/b^a$, then simplifying, but nothing. I always end where I start; I only need a hint to make the train rolling. EDIT Some people are answering erroneously because I define $\mathbb{Z^+}$ as $\{1,2,\dots\}$, and $0\not\in\mathbb{Z^+}$. The correct values by now are $a=3,b=2$ and $a=2,b=1$, but are they the only ones? Why?","['algebra-precalculus', 'diophantine-equations', 'elementary-number-theory']"
2158202,In how many ways can I fill a list of lists,"I have 220 items to fill a list of lists with, where, the outer list consists of 45 inner lists. Each of the 45 inner lists can have: Maximum of 10 items. Can be empty. So, the list of lists can have up to a maximum of 450 items, but we only need to fill 220 spots and the rest will be empty. The order of the items inside the inner list does not matter. An item can only be used once throughout the all 45 lists. The order of the inner lists matters. [[3,4], [1,2]] is not equal [[1,2], [3,4]] E.g: [[1,2],[9,7,55,66,77,99], [], [23,42,51,11], ...]","['statistics', 'probability']"
2158230,Embedding $RP^2$ into $R^4$,"I have a homework Question which asks to show that the map \begin{equation}f:R^3\rightarrow R^4, f(x,y,z)=(x^2-y^2,xy,xz,yz)\end{equation} Induces an embedding of $RP^2$ into $R^4$ Overall I have a fairly good idea of how I want to go about showing this, however I am looking for a ""neat"" way of showing $f$ is injective (take neat to mean whatever you want in this setting). I have an idea that is probably far reaching, but I was wondering if we could somehow use the kernel of $f$ in our argument? If we had a group homomorphism that would be one thing, but this is not so. However it does seem like the only element that gets mapped to zero is $(0,0,1)$ subject to the domain $S^2$, so maybe that's something? Honestly I'm just really lazy and don't want to brute force algebra onto this function to see that it is injective. I am just looking for some good hints here, anything is appreciated!",['general-topology']
2158247,Verify that $\int_0^{\pi/2} f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/2}f(\cos^2 x)\cos x\mathrm dx$,"Let $f\in C([0,1],\Bbb R)$ . Verify that $\int_0^{\pi/2} f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/2}f(\cos^2 x)\cos x\mathrm dx$ Hint: observe $\int_{\pi/4}^{\pi/2}f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/4}f(\sin 2x)\sin x\mathrm dx$ , and make the change of variable $\sin 2x=\cos^2 t$ . Im stuck in this exercise. Following the hint I did $$\int_0^{\pi/2}f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/4}f(\sin 2x)(\sin x+\cos x)\mathrm dx$$ but I dont know how to continue from here. I get the identities $$\sin x+\cos x=\sqrt 2\sin(x+\pi/4),\quad \mathrm dx=\sqrt{\frac{\cos^2 t}{1+\cos^2 t}}\mathrm dt$$ but I dont see how to transform $\sin x+\cos x$ to something related to the change of variable $\sin 2x=\cos^2 t$ . Some help will be appreciated, thank you.","['functional-analysis', 'trigonometric-integrals']"
2158251,Multivariable calculus (m substitution),"I have a question involving multi variable calculus: $$\lim_{(x,y)\to(0,0)} \frac{x^4-4y^2}{x^2+2y^2}$$
When proving that limit exists... we sometimes let $y = mx$ to say that we are considering all straight line paths to the origin. Then we write: $$\lim_{x\to 0} \frac{x^4-4(mx)^2}{x^2+2(mx)^2}$$ I know some example of function but I want to understand the inner working of why the limit can be re-written as so?","['multivariable-calculus', 'limits']"
2158292,Cramer Rao lower bound in Cauchy distribution,I need to calculate the Cramer Rao lower bound of variance for the parameter $\theta$ of the distribution $$f(x)=\frac{1}{\pi(1+(x-\theta)^2)}$$ How do I proceed I have calculated $$4 E\frac{(X-\theta)^2}{1+X^2+\theta^2-2X\theta}$$ Can somebody help,"['statistics', 'statistical-inference', 'expected-value', 'probability-distributions']"
2158316,Limit with integral and power,I'm trying to calculate this limit: $$\lim_{x\to\infty} \left(\int_0^1 t^{-tx} dt\right)^{\frac1x}$$ I tried the squeezing idea without success.,"['definite-integrals', 'calculus', 'limits']"
2158317,How can I find all the matrices that commute with this matrix?,"I would like to find all the matrices that commute with the following matrix $$A = \begin{pmatrix}2&0&0\\ \:0&2&0\\ \:0&0&3\end{pmatrix}$$ I set $AX = XA$, but still can't find the solutions from the equations.","['matrices', 'matrix-equations', 'linear-algebra']"
2158334,Prove that there is a plane in $\mathbb{R}^n$ with the following property,"Let $X \subset \mathbb{R}^n$ be a union of countably many affine subspaces of $\mathbb{R}^n$ with dimension less or equal $n-2$. Prove that $\mathbb{R}^n \setminus X$ is path connected. Assume $n = 2$. Then we need to show, that the plane with countably many points removed is path connected. Take $x_1, x_2 \in \mathbb{R}^2 \setminus X$. We find two nonparallel lines in $\mathbb{R}^2$ going through $x_1$ and $x_2$ which do not cross $X$. It can be done because the cardinality of the set of all lines through $x_i$ is $|\mathbb{R}|$ and the cardinality of $X$ is just $|\mathbb{N}|$. These two lines intersect at some point and we get a path between $x_1$ and $x_2$ in $\mathbb{R}^2 \setminus X$. Now I want to reduce the initial problem to the case with $n=2$. In order to do so I need to prove that for any two points $x_1$ and $x_2$ in $\mathbb{R}^n \setminus X$ there is a plane in $\mathbb{R}^n$ containing both these points and intersecting $X$ in at most countably many points. How can I do that?","['general-topology', 'path-connected', 'geometry']"
2158350,Maximal real subfields of Cyclotomic subfields giving rise to identities,"When fiddling a bit with cyclotomic fields, I discovered the following: If $f(X)=X^3+X^2-2X-1$ is the minimal polynomial for $\eta=\zeta_7+\zeta_7^{-1}$ over $\mathbb Q$, then $f(X^2-2)+f(X)f(-X)=0.$ This is natural, of course, since the Galois Group of  $\mathbb Q(\zeta_7+\zeta_7^{-1})/\mathbb Q$ is generated by $\sigma(\zeta_7)=\zeta_7^2$ and then $\sigma(\eta)=\eta^2-2$. This then led me to consider the polynomial equation $$P(X^2-2)=\pm P(X)P(-X),\ P\in\mathbb Q[x].$$ Let us call the set of solutions $V$. It is obvious that if $f(X),g(X)\in V$, then $f(X)g(X)\in V.$  (also it is obvious that if a polynomial fits the conditions, then its leading coefficient is 1 or -1). Let $P(X)=f_1(X)...f_m(X)\in V$, where the $f_i$ are (not necessarily distinct) irreducibles. Since they are irreducible (i.e. prime), each of them must divide at least one factor on the LHS:
$$f_{a_1}(X)\mid f_{a_2}(X^2-2)$$ for some (perhaps equal) $a_1$ and $a_2$. Denoting $\tau(X)=X^2-2$, we can apply this a finite number of times until we arrive at the original polynomial(this is not entirely true, but since we are cycling through a finite number of polynomials, repetition is bound to happen.We are just going to assume f_a_1 is one that repeats) $$f_{a_1}(X)\mid f_{a_2}(\tau(X))$$
$$f_{a_2}(X)\mid f_{a_3}(\tau(X))$$
$\hspace{80 mm}$ ..........
$$f_{a_k}(X)\mid f_{a_1}(\tau(X))$$ Therefore 
$$f_{a_1}(X)\mid f_{a_2}(\tau(X))$$
$$f_{a_2}(\tau(X))\mid f_{a_3}(\tau^2(X))$$
$\hspace{80 mm}$ ..........
$$f_{a_k}(\tau^{k-1}(X)\mid f_{a_1}(\tau^k(X))$$ This means that $f_{a_1}(X)\mid f_{a_1}(\tau^k(X))$. Let $\alpha$ be a root. We know that $\tau^k(\alpha)$ is also a root of $f_{a_1}(X)$ by the divisibility. Therefore $\tau^k$ is an automorphism of $\mathbb Q(\alpha)$, because $\mathbb Q(\alpha) \simeq\mathbb Q[X]/f(X) \simeq\mathbb Q(\tau^k(\alpha))$ (this works because $f_{a_1}(X)$ is irreducible). Now consider the quadratic polynomial $X^2-\alpha X+1$ and let the roots of it be $\eta$ and $\eta^{-1}$, so $\alpha=\eta+\eta^{-1}$ and $\mathbb Q(\eta)\supset\mathbb Q(\alpha) \supset\mathbb Q$.Extend the automorphism $\tau^k$ to an automorphism of $\mathbb Q(\eta)$ and keep the notation. Now, applying $\tau^k$to the quadratic we have 
$$X^2-\tau^k(\alpha)X+1=(X-\tau^k(\eta))(X-\tau^k(\eta^{-1}))=(X-\eta^{2^k})(X-\eta^{-2^k}),$$ because $$(\eta+\eta^{-1})^2-2=\eta^2+\eta^{-2};(\eta^2+\eta^{-2})^2-2=\eta^4+\eta^{-4}$$ and so on. This means that an automorphism of $\mathbb Q(\eta)$ maps $\eta$ to $\eta^{\pm2^k},$ which means that $\eta$ is a root of unity! Therefore $f_{a_1}(X)$ is the minimal polynomial of $\zeta_n+\zeta_n^{-1}$ over $\mathbb Q$, for some $n$.But since $\zeta_n \rightarrow \zeta_n^{\pm2^k}$ is an automorphism, n must be odd. Conversely, let $f(X)$be the minimal polynomial over Q for $\zeta_n+\zeta_n^{-1}$, where n is odd.Then the roots of $f(X^2-2)$ should be $\pm \sqrt{2+\zeta_n^k+\zeta_n^{-k}},$ where k ranges over the integers (k,n)=1.But since n is odd, we have a j such that $2j\equiv k(n).$ Therefore, the roots of $f(X^2-2)$ are $$\pm \sqrt{2+\zeta_n^{2i}+\zeta_n^{-2i}}=\pm \sqrt{(\zeta_n^i+\zeta_n^{-i})^2}=\pm (\zeta_n^i+\zeta_n^{-i}),$$ which are exactly the roots of $f(X)$ and $f(-X)$.But all of these polynomials have leading coefficients plus or minus 1, and the degrees agree, so $f(X^2-2)=\pm f(X)f(-X)$.So the set V is exactly $\pm  \langle f_n(x), n\in 2N+1 \rangle,$ ranging over the minimal polynomials mentioned. Is this proof correct? ADDED EXAMPLE: When n=19, we have the following identity: $$(x^2 - 2)^9 + (x^2 - 2)^8 - 8 (x^2 - 2)^7 - 7 (x^2 - 2)^6 + 21 (x^2 - 2)^5 + 15 (x^2 - 2)^4 - 20 (x^2 - 2)^3 - 10 (x^2 - 2)^2 + 5 (x^2 - 2) + 1=(x^9 + x^8 - 8 x^7 - 7 x^6 + 21 x^5 + 15 x^4 - 20 x^3 - 10 x^2 + 5 x + 1)(x^9 - x^8 - 8 x^7 + 7 x^6 + 21 x^5 - 15 x^4 - 20 x^3 + 10 x^2 + 5 x - 1)$$ FURTHER QUESTION: My proof heavily relies on constructing the auxiliary quadratic,applying the automorphism to it and explicitly finding the roots.I thought of this, because I observed what happens in cyclotomic fields and then phrased the question, reverse engineering it in a way.This sort of ""deus ex machina"" works quite neatly, but I don't know if there is a deeper reason for it to work except the obvious algebraic one.Is there a more elucidating proof of this, that works in a more general setting?","['polynomials', 'abstract-algebra', 'algebraic-number-theory', 'proof-verification', 'field-theory']"
2158404,Discrete Math (Combination with repetition) - Need confirmation,"Question: A mother has 10 identical coins, and 3 kids. a) If the first kids gets 3 coins, and the last kid gets 2 coins. How many ways can she spread the rest of the coins? b) If the second kid only gets 3 coins. How many ways can she spread the coins? c) If the second kid can have 3 coins or more. How many ways ? d) If the second kid can have at most 3 coins. How many ways ? My solution: a) r = 10 - 3 - 2 = 5 coins leftover r = 5 n = 3 No order, and repetitions are allowed. $\therefore$ $n+r-1\choose r$ $\therefore$ $3+5-1\choose 5$   is the answer b) The second kid is restricted to 3 coins only. Then n = 2, and r = 7. $\therefore$ $n+r-1\choose r$ $\therefore$ $2+7-1\choose 7$   is the answer c) r = 10 - 3 = 7 n = 3 (because the second kid can still receive the rest of the coins) $\therefore$ $n+r-1\choose r$ $\therefore$ $3+7-1\choose 7$   is the answer d) $2+10−1 \choose 10$+$2+9−1 \choose 9$+$2+8−1 \choose 8$+$2+7−1 \choose 7$ Am I right with my logic ?","['combinations', 'analysis', 'discrete-mathematics']"
2158418,How show this implication?,"Let $f$ be a strictly increasing continuous function defined on $(0, 1)$. Prove that
  $f'
 > 0$ almost everywhere if and only if 
  $f^{-1}$
  is absolutely continuous. What I did $(\Leftarrow)$ for $[a,b] \subset (0,1)$ $f$ is of bounded variation and then $f'$ exists a.e., as $f$ is strictly increasing then $f'\geq 0$. $f: [a,b] \rightarrow f([a,b])$ is bijection and $f([a,b])$ is closed interval. $f^{-1}:f([a,b])\rightarrow [a,b]$ is of absolutely continuous then $(f^{-1})'$ exists a.e. Therefore $f' > 0$ a.e. (in this case $(f^{-1})(y)' = \dfrac{1}{f'(x)}$ where $f(x) = y$) As $[a,b]$ is arbritary then $f'> 0$ a. e. on $(0,1)$ $\blacksquare$ I would be very grateful for some help to prove $(\Rightarrow)$","['real-analysis', 'lebesgue-integral', 'measure-theory']"
2158540,What are some interesting calculus facts your calculus teachers didn't teach you? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question I recently learned an interesting fact about what the value of a Lagrange multiplier represents: suppose the maximum of some real-valued function $f(\vec{x})$ subject to a constraint $g(\vec{x})=c$ is $M$ (of course, $M$ depends on $c$), which you obtained via Lagrange multipliers (solving $\nabla f = \lambda \nabla g$).  Then, it's easy to show (using the chain rule) that the multiplier $\lambda$ can be interpreted as the change of the maximum with respect to perturbations of the level set $g=c$. That is,
$$
\lambda = \frac{d M}{dc}
$$
I think this is a pretty cool result and I never it heard about during my time as an undergraduate. What are some interesting calculus (or undergraduate mathematics) results nobody told you about during your calculus (or undergraduate) education?","['multivariable-calculus', 'soft-question', 'calculus']"
2158562,"Max and Min$ f(x,y)=x^3-12xy+8y^3$","First I solved $f_y=0$ then plugged in my variable into $f_x$ to get an output and then plugged that output back into $f_y$ to get a point and did this again for $f_x$ I think I screwed up on the $(-7,\sqrt{3})$ $f_y=-12x+24y^2$ $-12x+24y^2=0$ $-12x=24y^2$ $x=2y^2$ $f_x(2y^2,y)=3(2y^2)^2-12y$ $3(4y^4)-12y=0$ $12y^4 - 36y=0$ $12y(y^3-3=0) \to y=0$ or $y=\sqrt{3}$ $f_y(x,0)=-12x=0 \to (0,0)$ $f_y(x,\sqrt{3}) = -12x+24(3)$ $f_y(x.sqrt{3})= -12x-84=0$ $f_y(x,\sqrt{3})=-12x=84$ $x=-7 \to (-7,\sqrt{3})$ $f_y=-12x+24y^2=0$ $-12x=24y^2$ $x=2y^2$ $f_x(2y^2,y)=3(2y^2)^2-12y=0$ $12y^4-12y=0$ $12y(y^3-1)=0$ $y=0$ or $y=1$ $f_y(x,o)= -12x=0 \to x=0$ $f_y(x,1)=-12x+24=0$ $f_y(x,1)=x=2$ Critical points: $(2,-1),(0,0),(-7,\sqrt{3})$ $D=(6x)(48y)-(-12)^2$ $D(0,0)=-144 so (0,0) is a saddle point $D(,2,-1)= Min D(-7,\sqrt{3})= Saddle point","['multivariable-calculus', 'calculus']"
2158563,Derivation of normal equations for maximum likelihood and least squares,"The question stems from the text Pattern Recognition and Machine Learning by Christopher Bishop, Chapter 3.1.1. To say my maths is rusty is an understatement so I would appreciate any nudge in the right direction regarding the below. The logarithm of the likelihood functions of a standard univariate Gaussian is given below: $$(1)\quad \frac{N}{2} \ln \beta - \frac{N}{2}\ln 2 \pi - \frac{\beta}{2}\sum_{n=1}^N\{t_n - \mathbf w^T \mathbf \phi(\mathbf x_n) \}^2$$ Taking the gradient(derivative) with respect to $\mathbf w$ and setting this equal to zero we get: $$(2)\quad0 = \sum_{n=1}^N\{t_n - \mathbf w^T \mathbf \phi(\mathbf x_n) \}\phi(\mathbf x_n)^T $$ $$(3)\quad0 = \sum_{n=1}^Nt_n \phi(\mathbf x_n)^T - \mathbf w^T(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T)$$ Solving for $\mathbf w$ we get: $$(4)\quad\mathbf w_{ML}=(\mathbf \Phi^T \mathbf \Phi)^{-1} \mathbf \Phi^T \mathbf {\mathtt t}$$ where $$(5)\quad\mathbf \Phi = \phi_j(\mathbf x_n)$$ Now for the questions. (a) In (2) when using the chain rule why do we get $\phi(\mathbf x_n)^T$ instead of $\phi(\mathbf x_n)?$ (b) Is the following valid to get from (3) to (4)? Continuing from (3) $$\mathbf w^T(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T) = \sum_{n=1}^Nt_n \phi(\mathbf x_n)^T$$ $$[\mathbf w^T(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T)]^T = [\sum_{n=1}^Nt_n \phi(\mathbf x_n)^T]^T$$ $$\mathbf w_{ML}(\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T) = \sum_{n=1}^N \phi(\mathbf x_n) t_n $$ Where $\sum_{n=1}^N \mathbf \phi(\mathbf x_n) \phi(\mathbf x_n)^T$ = $(\mathbf \Phi^T \mathbf \Phi)$ and $\sum_{n=1}^N \phi(\mathbf x_n) t_n = \mathbf \Phi^T \mathbf {\mathtt t}$ $$\mathbf w_{ML}(\mathbf \Phi^T \mathbf \Phi)(\mathbf \Phi^T \mathbf \Phi)^{-1}=(\mathbf \Phi^T \mathbf \Phi)^{-1} \mathbf \Phi^T \mathbf {\mathtt t}$$ $$(4)\quad\mathbf w_{ML}=(\mathbf \Phi^T \mathbf \Phi)^{-1} \mathbf \Phi^T \mathbf {\mathtt t}$$ If it is not valid can you please point out where.","['maximum-likelihood', 'machine-learning', 'least-squares', 'linear-algebra']"
2158628,Applications of the open mapping theorem for Banach spaces?,"Does anybody know of any common/standard/famous practical applications of the open mapping theorem for Banach spaces? Textbooks describe the theorem as a ""cornerstone of functional analysis"", and yet I have never come across a practical problem that is solved using it. I do know that the open mapping theorem implies the inverse mapping theorem and the closed graph theorem. I would imagine the closed graph theorem to be of more direct applicability than the open mapping theorem itself. For instance, the closed graph theorem tells you that, in order to prove that a map $f : X \to Y$ between Banach spaces is continuous, you only have to prove that if $\lim_{n \to \infty}x_n = x$ and $\lim_{n \to \infty} f(x_n) = y$, then $y = f(x)$, i.e. it is okay to assume that the limit $\lim_{n \to \infty} f(x_n)$ exists. Still, it bothers me that I have never seen this technique applied to a concrete example that anyone cares about!",['functional-analysis']
2158668,A proof of: The derivative of the determinant is the trace,"I want to solve the following problem: Show that the derivative of $\mbox{det}:GL(n,\mathbb{R})\rightarrow\mathbb{R}$
    at $I\in GL(n,\mathbb{R})$
    is given by 
  $$\mbox{det}_{*}(I)(X)=\mbox{tr}X$$ I would like you to check my proof, and answer the question in the end. My Attempt : I'll denote $N=GL(n,\mathbb{R})$
 . Also, $\simeq$
  will be used for vector space isomorphisms and $\cong$
  will be used for diffeomorphisms. We know that $\mbox{det}_{*}(I):T_{I}N\rightarrow T_{det(I)=1}\mathbb{R}$
 . Let $X\in T_{I}N$
 . We can write $X$
  in a basis of $T_{I}N$
 . So let us find a basis $of T_{I}N$
 : we know that $T_{I}N\simeq M_{n}(\mathbb{R})$
 , so we can get a basis of $T_{I}N$
  from a basis of $M_{n}(\mathbb{R})$
  using an isomorphism. The function $$f:T_{I}N	\rightarrow	M_{n}(\mathbb{R}) \\
[\gamma]	\mapsto	\gamma'(0)$$ is known to be an isomorphism. Furthermore, ${E_{ij}}$
  is a basis for $M_{n}(\mathbb{R})$
 , where $E_{ij}$
  is the $n\times n$
  matrix whose entries are all zero except the entry $i,j$
 , which is $1$
 . Thus, a basis for $T_{I}N$
  is ${f^{-1}(E_{ij})}$
 . Now, $f^{-1}(E_{ij})$
  is the equivalence class of curves $\gamma:\mathbb{R}\rightarrow N$
  such that $\gamma(0)=I$
  and $\gamma'(0)=E_{ij}$
 . Hence, a representative of this equivalence class is $\alpha_{ij}(t)=I+tE_{ij}$
 , and so we can write ${f^{-1}(E_{ij})}={[\alpha_{ij}]}$
 . Hence, we can write $X=\overset{n}{\underset{i,j=1}{\sum}}x_{ij}[\alpha_{ij}]$
 . Let us see how $det_{*}$
  acts on the basis elements $[\alpha_{ij}]$
 . We have $\mbox{det}_{*}(I)([\alpha_{ij}])=[\mbox{det}\circ\alpha_{ij}]_{1}$ by definition of derivative (the subscript 1
  reminds us that the equivalence relation of this equivalence class is different, since it is defined on the set of all curves of the type $\gamma:\mathbb{R}\rightarrow\mathbb{R}$
  such that $\gamma(0)=\mbox{det}(I)=1
 ).$ Now, $\mbox{det}\circ\alpha_{ij}:\mathbb{R}\rightarrow\mathbb{R}$
  is such that $$\mbox{det}\circ\alpha_{ij}=\mbox{det}(\alpha_{ij}(t))=\mbox{det}\left(I+tE_{ij}\right)=\mbox{det}\left(\left[\begin{array}{ccc}
1 &  & \mathbb{O}\\
 & \ddots\\
\mathbb{O} &  & 1
\end{array}\right]+\left[\begin{array}{cccc}
\mathbb{O} &  &  & \mathbb{O}\\
 &  & t\,(i,j\mbox{ entry})\\
\\
\mathbb{O} &  &  & \mathbb{O}
\end{array}\right]\right)$$
 . The matrix is triangular (or simply diagonal), and so the determinant is the product of the diagonal elements. Hence, $\mbox{det}\circ\alpha_{ij}=1+t\delta_{ij}$
 , with $\delta_{ij}$
  the Kronecker delta. Hence, $$\mbox{det}_{*}(I)([\alpha_{ij}])=[1+t\delta_{ij}]_{1}\in T_{1}\mathbb{R}$$ Finally, $$\mbox{det}_{*}(I)(X)=\overset{n}{\underset{i,j=1}{\sum}}x_{ij}\mbox{det}_{*}(I)([\alpha_{ij}])=\overset{n}{\underset{i,j=1}{\sum}}x_{ij}[1+t\delta_{ij}]_{1}$$ Now, I noticed that, if I for some reason use the isomorphism $$g:T_{1}\mathbb{R}	\rightarrow	\mathbb{R} \\
[\gamma]_{1}	\mapsto	\gamma'(0)$$ to “identify” $\alpha_{ij}$
  with $g(\alpha_{ij})=\delta_{ij}$
  and use that instead of $[1+t\delta_{ij}]_{1}$
 , I get $\overset{n}{\underset{i,j=1}{\sum}}x_{ij}\delta_{ij}=\mbox{tr}X$
 . My question is: why is this last step (since ""Now, I noticed..."") legitimate?","['alternative-proof', 'smooth-manifolds', 'differential-geometry', 'proof-verification']"
2158686,Must the complement of countably many disjoint arcs be connected?,"Must the complement of the countable union of disjoint arcs (in the plane) be connected? A (Jordan) arc is the injective image of a continuous function from $[0,1]$ to $\Bbb R^2$. It is known that, if I have finitely many pairwise disjoint arcs, the complement of their union must be connected, and even path-connected. However, this no longer is true when we allow ourselves infinitely many arcs (take the union of line segments of the form $\{x\}\times[0,1]$ for $x\in\Bbb R$). In fact, the ""path-connected"" part is not even true for countably many arcs! I know three counterexamples (special thanks to Balarka and Alessandro): Let $U_r$ be a closed circular arc of $270$ degrees, with the $90^\circ$ opening facing up, of radius $r$. Let $D_r$ be the same but with the opening facing down. Then the complement of:
  $$U_{1/2}\cup D_{2/3}\cup U_{3/4}\cup D_{4/5}\cup\dotsb$$
  is not path-connected. The complement of:
  $$\bigcup_{n=1}^\infty(\{1/n\}\times[-n,n])$$
  is not path-connected. This gives an example made of only straight lines, though it's unbounded. Let $S_1=[-1,1]\times\{1\}$ and $S_{-1}=[-1,1]\times\{-1\}$ be the top and bottom sides of a square. Let $T_{-1,n}=\{-1-\frac1n\}\times[-1,1]$ and $T_{1,n}=\{1+\frac1n\}\times[-1,1]$ be sequences of segments approaching the left and right sides of the same square. Then the complement of:
  $$S_{-1}\cup S_1\cup\bigcup_{n=1}^\infty T_{-1,n}\cup\bigcup_{n=1}^\infty T_{1,n}$$
  is not path-connected. This gives an example that is bounded, is made of straight lines, and has all of its lines the same length. However, while these complements are all not path-connected, they all are connected . Hence the question: Must the complement of countably many disjoint arcs be connected? I've thought about this all day and I haven't been able to solve it. Homology seems useless, since it can only measure path components, not connected components. On the other hand, if we replace $\Bbb R^2$ with other spaces, it can become false (take for example the plane minus open disks of radius $\frac13$ centered at the integers on the $x$-axis), so the proof must use some special property of $\Bbb R^2$.","['general-topology', 'path-connected', 'connectedness']"
2158700,Write following set in set-builder notation [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question {...,−8,−3,2,7,12,17,...} Hello, I need help writing this set in a set-builder notation. How do I find the steps? Thank you",['elementary-set-theory']
2158704,"""Trivial"" geometry behind the intersection pairing on surfaces","The intersection pairing between two divisors on a nonsingular algebraic surface over a field is defined thanks to the following theorem (the reference is Hartshorne's book): One can define a pairing for any couple of invertible sheaves $\mathcal L,\mathcal M\in\operatorname{Pic}(X)$ as follows: $$\mathcal L.\mathcal M:=\chi(\mathcal O_X)-\chi(\mathcal L^{-1})-\mathcal (M^{-1})-\chi( \mathcal L^{-1}\otimes \mathcal M^{-1})\quad\quad (\ast)$$ By using the well known isomorphism between $\operatorname{Pic}(X)$ and the group of divisors up linear equivalence, one can clearly define: $$C.D:=\mathcal O_X(C).\mathcal O_X(D)$$ and the final step is to show that this definition satisfies properties (1)-(4) of the above theorem. So everything is very clear, but I don't understand what is the
  meaning of the definition $(\ast)$. It seems to me that this pairing
  for invertible sheaves appears out of the blue. Can you give any
  intuitive motivation about its nature? Why do we need the Poincare characteristics? Why are we taking the inverse sheaves?","['algebraic-geometry', 'intersection-theory', 'schemes', 'divisors-algebraic-geometry', 'surfaces']"
2158715,Convergent of a series for arbitrary sequence,"Prove that for arbitrary sequence
$x_{1}, x_{2},...$
of $[0,1]$ there is a $x$ in $[0,1]$ such that below series is convergent:
$$\sum_{n=1}^\infty \frac{1}{n^2|x-x_{n}|}.$$","['sequences-and-series', 'analysis']"
2158719,Geometry homework question - enough data?,"I've been asked to help with the following school problem on geometry. In the triangle $\Delta ABC$ one has $AB = 60$, $AC = 80$. Point $O$ is the centre of the circumscribed circle. Point $D$ belongs to the side $AC$. Additionally, one has $AO \perp BD$. One is asked to find $CD$. (just in case, the answer is $35$) I am really puzzled, since the information given clearly does not fix the triangle. I know how to solve the problem under the assumption that point $O$ belongs to $BD$. In this case, the solution goes as follows: Denote $\alpha = \angle OAC$, $\beta = \angle OBC$. $\angle ACB = \dfrac{1}{2}\angle AOB = 45^\circ$. From the sum of angles of the triangle $\triangle ABC$, one has:
$$\alpha + \beta = 45^\circ$$ The law of sines for the triangle $\triangle ABC$ gives:
$$\dfrac{AC}{\sin(\beta + 45^\circ)}=\dfrac{AB}{\sin(45^\circ)}$$
From where one can find $\beta$:
$$\beta = \arccos\left( \dfrac{2\sqrt2}{3} \right) + 45^\circ$$ From the triangle $\triangle AOD$ one finds:
$$CD = AC - AD = AC - \dfrac{AO}{\cos(\alpha)}= AC - \dfrac{AO}{\cos(45^\circ - \beta)}$$ Substituting the value of $\beta$ indeed gives $CD = 35$. Now, I have two questions: Is it possible to get the answer without the assumption I have made (or any other one). Can anyone present an easier solution? (just in case, this is one of $26$ problems in the $9$th grade quiz in Russian middle school — students are obviously limited in time and are not supposed to use Mathematica and even Stack Exchange)","['euclidean-geometry', 'geometry']"
2158734,"Oh Times, $\otimes$ in linear algebra and tensors","Can I have some clarification of the different meanings of $\otimes$ as in the unifying and separating implications in basic linear algebra and tensors? Here is some of the overloading of this symbol... 1.1. Kronecker matrix product : If $A$ is an $m \times n$ matrix and $B$ is a $p \times q$ matrix, then the Kronecker product A ⊗ B is the $mp \times nq$ block matrix: $$A\color{red}{\otimes}B=\begin{bmatrix}a_{11}\mathbf B&\cdots&a_{1n}\mathbf B\\\vdots&\ddots&\vdots\\a_{m1}\mathbf B&\cdots&a_{mn}\mathbf B\end{bmatrix}$$ 1.2. Outer product : $\mathbf u \otimes \mathbf v = \mathbf{uv}^\top = \begin{bmatrix}u_1\\u_2\\u_3\\u_4\end{bmatrix}\begin{bmatrix}v_1&v_2&v_3&v_4\end{bmatrix}=\begin{bmatrix}u_1v_1&u_1v_2&u_1v_3\\u_2v_1&u_2v_2&u_2v_3\\u_3v_1&u_3v_2&u_3v_3\end{bmatrix}$ Definition of the tensor space : $$\begin{align}T^p_q\,V &= \underset{p}{\underbrace{V\color{darkorange}{\otimes}\cdots\color{darkorange}{\otimes} V}} \color{darkorange}{\otimes} \underset{q}{\underbrace{V^*\color{darkorange}{\otimes}\cdots\color{darkorange}{\otimes} V^*}}:=\{T\, |\, T\, \text{ is a (p,q) tensor}\}\\[3ex]&=\{T: \underset{p}{\underbrace{V^*\times \cdots \times V^*}}\times \underset{q}{\underbrace{V\times \cdots \times V}} \overset{\sim}\rightarrow K\}\end{align}$$ Definition of the tensor product : It takes $T\in T_q^p V$ and $S\in T^r_s V$ so that: $$T\color{blue}{\otimes}S\in T_{q+s}^{p+r}V$$ defined as: $$\begin{align}&(T\color{blue}{\otimes}S)(\underbrace{ \omega_1,\cdots,\omega_q,\cdots,\omega_{q+s}, v_1,\cdots,v_p,\cdots,v_{p+r}}_\text{'eats'})\\&:= T(\underbrace{\omega_1,\cdots,\omega_q, v_1,\cdots,v_p}_{\text{'eats up' p vec's + q covec's}\rightarrow \text{no.}})\underbrace{\cdot}_{\text{in the field}}S(\underbrace{\omega_{q+1},\cdots,\omega_{q+s}, v_{p+1},\cdots,v_{p+r}}_{\text{'eats up' p vec's and q covec's} \rightarrow\text{no.}})\end{align}$$ An example of, for instance, some operation like $\underbrace{e_{a_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}e_{a_p}\color{blue}{\otimes}
\epsilon^{b_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}\epsilon^{b_q}}_{(p,q)\text{ tensor}}$ after settling for some basis could be helpful. For clarity this is a fragment of the more daunting expression: $$ T=\underbrace{\sum_{a_1=1}^{\text{dim v sp.}}\cdots\sum_{b_1=1}^{\text{dim v sp.}}}_{\text{p + q sums (usually omitted)}}\underbrace{\color{green}{T^{\overbrace{a_1,\cdots,a_p}^{\text{numbers}}}_{\quad\quad\quad\quad\underbrace{b_1,\cdots,b_q}_{\text{numbers}}}}}_{\text{a number}}\underbrace{\cdot}_{\text{S-multiplication}}\underbrace{e_{a_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}e_{a_p}\color{blue}{\otimes}
\epsilon^{b_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}\epsilon^{b_q}}_{(p,q)\text{ tensor}}$$ showing how to recuperate a tensor from its components. I realize that there is a connection as stated here : The Kronecker product of matrices corresponds to the abstract tensor product of linear maps. Specifically, if the vector spaces $V, W, X$, and $Y$ have bases $\{v_1, \cdots, v_m\}, \{w_1,\cdots, w_n\}, \{x_1,\cdots, x_d\},$ and $\{y_1, \cdots, y_e\}$, respectively, and if the matrices $A$ and $B$ represent the linear transformations $S : V \rightarrow X$ and $T : W \rightarrow Y$, respectively in the appropriate bases, then the matrix $A ⊗ B$ represents the tensor product of the two maps, $S ⊗ T : V ⊗ W → X ⊗ Y$ with respect to the basis $\{v_1 ⊗ w_1, v_1 ⊗ w_2, \cdots, v_2 ⊗ w_1, \cdots, v_m ⊗ w_n\}$ of $V ⊗ W$ and the similarly defined basis of $X ⊗ Y$ with the property that $A ⊗ B(v_i ⊗ w_j) = (Av_i) ⊗ (Bw_j)$, where $i$ and $j$ are integers in the proper range. But it is still elusive...","['abstract-algebra', 'tensors', 'linear-algebra', 'tensor-products']"
2158739,True of False: Every 3-dimensional subspace of $ \Bbb R^{2 \times 2}$ contains at least one invertible matrix. [duplicate],"This question already has answers here : Max dimension of a subspace of singular $n\times n$ matrices (2 answers) Closed 7 years ago . The true or false question states:
""True of False: Every 3-dimensional subspace of $ \Bbb R^{2 \times 2}$ contains at least one invertible matrix."" Here the $ \Bbb R^{2 \times 2}$ represents the space of all two by two matrices. It seems like this is true, but I am not sure how to prove or disprove the statement. (If such is true, then it's easy to see that every 3-dimensional subspace of $ \Bbb R^{2 \times 2}$ contains infinitely many invertible matrices)",['linear-algebra']
2158741,Why is there a difference between coordinate variables and vector components when transforming coordinates?,"My textbook has a table of coordinate transformations. Why are the coordinate variables and the vector components different? For example, for the conversion between spherical to cartesian for ""z"", it is equivalent to $R\cos(\theta)$, which I understand. However, it is also said that the vector component of ""z"" is $A_R\cos(\theta)-A_\theta\sin(\theta)$ Why is this the case that they are different?","['coordinate-systems', 'vector-spaces', 'multivariable-calculus', 'vectors', 'vector-analysis']"
2158750,Length of segment parallel to an edge,"I've tried all the possible side splitter and angle bisector theorem stuff and I still can't come up with the correct answer. I even tried some law of cosine and sine stuff, but nothing. Any help would be gladly appreciated. Thanks.","['angle', 'trigonometry', 'triangles', 'geometry']"
2158764,Floor Function. Let x and y be rational numbers,"let x and y be rational numbers. A. $\left \lfloor x \right \rfloor + \left \lfloor y \right \rfloor 
    = \left \lfloor x+y \right \rfloor$ B. $\left \lfloor x \right \rfloor + \left \lfloor y \right \rfloor 
    \le \left \lfloor x+y \right \rfloor$ C. $\left \lfloor x \right \rfloor + \left \lfloor y \right \rfloor 
    \ge \left \lfloor x+y \right \rfloor$ D. None of the above. Can anyone please explain why the answer is B? Thank You!",['discrete-mathematics']
2158772,Finding null space of matrix.,"I need to make sure I'm understanding this correctly.
I skipped a few steps to reduce typing, but let me know if I need to clarify something. Question asks: Find $N(A)$ for $A$ = \begin{bmatrix}
        -3 & 6 & -1 & 1 & -7 \\
        1 & -2 & 2 & 3 & -1\\
        2 & -4 & 5 & 8 & -4 \\
        \end{bmatrix} First thing I did was put the augmented matrix into reduced echelon row: $\begin{bmatrix}
        1 & -2 & 0 & -1 & 3 & 0 \\
        0 & 0 & 1 & 2 & -2 & 0\\
        0 & 0 & 0 & 0 & 0 & 0 \\
        \end{bmatrix}$ $(1)$ So then... $x=\begin{bmatrix}
x_1\\
x_2 \\
x_3\\
x_4\\
x_5\\
\end{bmatrix} 
= 
\begin{bmatrix}
2x_2 + x_4 - 3x_5\\
x_2 \\
-2x_4 + 2x_5\\
x_4\\
x_5\\
\end{bmatrix}$ $(2) $ Since $x_2, x_4$ and $x_5$ are free variables.. $
x_2
\begin{bmatrix}
2\\
1 \\
0\\
0\\
0\\
\end{bmatrix}
+
x_4
\begin{bmatrix}
1\\
0 \\
-2\\
1\\
0\\
\end{bmatrix}
+
x_5
\begin{bmatrix}
-3\\
0 \\
2\\
0\\
1\\
\end{bmatrix}$ $(3)$ Resulting in.. $N(A)= \left(
\begin{bmatrix}
2\\
1 \\
0\\
0\\
0\\
\end{bmatrix}
,
\begin{bmatrix}
1\\
0 \\
-2\\
1\\
0\\
\end{bmatrix}
,
\begin{bmatrix}
-3\\
0 \\
2\\
0\\
1\\
\end{bmatrix}
\right)$ $(4)$",['linear-algebra']
2158795,Finding Radon-Nikodym derivative,"Let $m$ be Lebesgue measure on $\mathbb R_+=(0,\infty)$ and $\mathcal A = \sigma\left(( \frac 1{n+1} , \frac 1n ]:n=1,2,...\right)$. Define a new measure $\lambda$ on $\mathcal A$, for each $E \in \mathcal A$, by $\lambda(E)= \int_E fdm $, where $f(x)=2x^2$ . Find the Radon-Nikodym derivative $\frac{d\lambda}{dm}$. It is clear that $\lambda $ is absolutely continuous with respect to $m$ by definition, and Lebesgue measure is $\sigma$-finite in $(\mathbb R_+ , \mathfrak M_+)$, where $\mathfrak M_+$ is the collection of all Lebesgue measurable subsets of $\mathbb R_+$, so $m$ is $\sigma$-finite in $(\mathbb R_+, \mathcal A)$ since every element of $\mathcal A$ is also Lebesgue measurable. However, to apply Radon-Nikodym theorem, $\lambda$ must be a finite measure, but $\lambda(E)= \int_E fdm = \infty$ if $E=\mathbb R_+ - ( \frac 12,1]$, so we cannot apply that theorem. Is there any other approach to this problem?","['real-analysis', 'absolute-continuity', 'lebesgue-measure', 'radon-nikodym', 'measure-theory']"
2158796,"Prove that $\{1, \alpha, \alpha^{2}\}$ is an integral basis of $\mathcal O_{K}$","Let $K = \mathbb{Q}(\alpha)$ where $\alpha^3 - 50\alpha- 10= 0$. Prove that $\{1, \alpha, \alpha^{2}\}$ is an integral basis of $\mathcal O_{K}$. I know that the minimal polynomial is $$m_\alpha(x)=x^3 - 50x -10$$ but I'm not sure where to even begin. I have looked at plenty of resources but none of them seem to have concrete examples of how to solve a problem like this. I've tried to understand general examples but I'm not sure how to solve a specific problem like this one. Thanks in advance.","['number-theory', 'galois-theory', 'algebraic-number-theory']"
2158815,Banach spaces exercise,"Let $(C[0,1],\lVert\cdot\rVert_{\infty})$ be the set of continuous functions in $[0,1]$, and consider $X=\{f\mid f(0)=0\}$ and  $Y=\{f \in X\mid \int^1_0f(x)\,\textrm{d}x=0\}$, subspaces of $C[0,1]$. Prove that for all $f \in X$ such that $\lVert f\rVert_{\infty}=1$ we have that  $\inf\{\lVert g-f\rVert_{\infty}\mid g \in Y \}<1$. I proved that $Y$ is a proper closed subspace of $X$, thus is Banach because I also proved that $X$ is Banach. We have that  $\inf\{\lVert g-f\rVert_{\infty}\mid g \in Y \} \leqslant \lVert f-0\rVert _{\infty}=1$. Now if I assume that  $\inf\{\lVert g-f\rVert_{\infty}\mid g \in Y \} =1$, can someone help me to derive a contradiction?","['functional-analysis', 'real-analysis', 'banach-spaces']"
2158851,Hypothesis Testing for uniform distribution,"Let $X_i; i=1,2,3,\dots,n$ be iid samples from the uniform $U(0, θ)$ distribution, with $θ$ being the unknown parameter of interest. Consider the testing problem
$H_0 : θ = θ_0$ versus $H_1 : θ > θ_0.$
One possible test is to reject $H_0$ when $X_{max} > C$ for some $C > 0.$ $1.$ Find $C$ in terms of the proposed 
$θ_0$ such that the probability of Type I
error is 0.05. $2.$  Use the duality between hypothesis testing and conﬁdence interval to write down a level 0.95 lower conﬁdence bound for $θ.$ That is, ﬁnd $L(X_1,\dots , X_n)$ such that $P_θ(θ ≥ L(X_1, \dots , X_n)) = 0.95$ for all $θ > 0.$ $3.$ Choose $C$ as in (a). What is the power of the test in the case 
of $θ_0 = 1,\, θ = 2,\, n = 10?$ I have done $1$ using $P(X_{max}>C|θ = θ_0) = 1 - (C/θ_0)^n = 0.05;$ can anyone tell me if this is right? Also, how to do questions $2$ and $3$? Thanks so much!!!!",['statistics']
2158881,Almost complex structure on $S^6$,It is known that the sphere $ S^6$ admits an almost complex structure by identifying $S^6 $ with the space of unit purely imaginary Cayley numbers. I would like to show that this almost complex structure is not integrable using the Nijenhuis tensor. Can someone explain to me how vector fields look like in $S^6$ and how can we apply them in the Nijenhuis tensor? I found something in Ballmann's book (Kahler manifolds) but I didn't understand it.,"['almost-complex', 'smooth-manifolds', 'differential-geometry']"
2158882,Question about proof of Implicit Function Theorem in *Analysis on Manifolds* by Munkres,"I am reading Analysis on Manifolds by Munkres, and have a question about the proof about the Implicit Function Theorem (both the statement and proof included below): Note (3rd paragraph of the proof) how Munkres chooses $U \times V$ as a neighborhood of $(a,b) \in \mathbb{R}^{k+n}$ .  I know this can be done by restricting the open set guaranteed to exist by the Inverse Function Theorem, but I don't see why we want it to be a Cartesian product. Regarding uniqueness (last paragraph of the proof), why is the argument provided necessary?  It seems unnecessarily complicated.  Here is how I reasoned it: say $(x,g(x)) \in U \times V$ s.t. $f(x,g(x))= \textbf{0}_n$ .  Then $F(x,g(x))=(x,\textbf{0}_n)$ , so $$(x,g(x))=G(x,\textbf{0}_n)=(x,h(x,\textbf{0}_n)).$$ (Here $G=F^{-1}$ and $h$ is the last $n$ coordinate functions of $G$ , following Munkres' notation).  By inspection, $g(x)=h(x,\textbf{0}_n)$ , hence uniqueness is shown because we just derived what $g(x)$ has to be. Many thanks in advance.","['multivariable-calculus', 'real-analysis', 'implicit-function-theorem', 'derivatives']"
2158892,Working out a concrete example of tensor product,"From this entry in Wikipedia : The tensor product of two vector spaces $V$ and $W$ over a field $K$ is another vector space over $K$. It is denoted $V\otimes_K W$, or $V\otimes W$ when the underlying field $K$ is understood. If $V$ has a basis $e_1,\cdots,e_m$ and $W$ has a basis $f_1,\cdots,f_n$, then the tensor product $V\otimes W$ can be taken to be a vector space spanned by a basis consisting of all pairs $(e_i,f_j)$; each such basis element of $V\otimes W$ is denoted $e_i\otimes f_j$.  For any vectors $v=\sum_i v_ie_i\in V$ and $w=\sum_j w_j f_j\in W$ there is a corresponding product vector $v\otimes w$ in $V\otimes W$ given by $\sum_{ij}v_iw_j(e_i\otimes f_j)\in V\otimes W.$  This product operation $\otimes:V\times W \rightarrow V\otimes W$ is quickly verified to be bilinear. As an example, letting $V=W=\mathbb R^3$ (considered as a vector space over the field of real numbers) and considering the standard basis set $\{\hat x, \hat y,\hat z\}$ for each, the tensor product $V\otimes W$ is spanned by the nine basis vectors $\{\hat x \otimes \hat x,\hat x \otimes \hat y,\hat x \otimes \hat z,\hat y \otimes \hat x,\hat y \otimes \hat y, \hat y \otimes \hat z ,\hat z\otimes \hat x,,\hat z \otimes \hat y, \hat z \otimes \hat z  \}$ and is isomorphic to $\mathbb R^9$. For vectors $v=(1,2,3),w=(1,0,0)\in \mathbb R^3$ the tensor product $$\bbox[10px, border:2px solid red]{v\otimes w= 
\hat x\otimes \hat x + 2\hat y\otimes \hat x+3\hat z\otimes \hat x}$$ The above definition relies on a choice of basis, which can not be done canonically for a generic vector space. However, any two choices of basis lead to isomorphic tensor product spaces (c.f. the universal property described below).  Alternatively, the tensor product may be defined in an expressly basis-independent manner as a quotient space of a free vector space over $V\times W$.  This approach is described below. QUESTION: If we decide on the standard Euclidean orthonormal basis, what is the final expression of the $v\otimes w$ product in the red boxed expression? Do we eventually get rid of the vector expressions with hats (as well as the $\otimes$ symbols) to get a number as per the (approximate) idea of a tensor as a map from $V\times W\rightarrow \mathbb R?$ What if we change the bases from orthonormal to $\large\begin{bmatrix}\tilde x\\\tilde y\\\tilde z\end{bmatrix}=\begin{bmatrix}3&4&-1\\0&3&7\\1&3&0.5\end{bmatrix}\begin{bmatrix}\hat x\\\hat y\\\hat z\end{bmatrix}?$","['abstract-algebra', 'tensors', 'linear-algebra', 'tensor-products']"
2158909,Why is there no analogy of completing the square for quartics and higher?,"One use of completing the square in a quadratic is to find its critical value. I thought whether this could also be possible with quartics; if nothing else, it would be a cool way to find extrema witouth taking a derivative and having to solve a cubic; so I tried to factor $x^4 + 4x^3 + 6x^2 + 6x + 2$ as $(x^2 + bx + c)^2+d$, and I arrived at an inconsistent system of equations. Is there any deep reason why we can't solve quartics using the ""completing the square"" method? P.S. If possible, please refrain from using advanced abstract algebra terminology.","['algebra-precalculus', 'abstract-algebra']"
2158926,"What operations can be performed on three variables $x$, $y$, and $z$ and produce one unique sum?","I'm having a programming problem, but I feel this would be more relevant here. I have three unique variables, $x$, $y$, and $z$. What operations can I perform on these variables to produce a unique value? Are there any approaches I could take? Thanks! EDIT: To be more specific, I want some function $f(x, y, z)$ that produces a unique output for each set of inputs (assuming uniqueness in inputs, $1$, $2$, $3$ does not need to output something different than $3$, $2$, $1$). I realize that a generalized case of this is probably not possible, but I would definitely be able to co-opt similar solutions",['functions']
2158942,Discussion on definition of independent random variables,"Definition of Random Variable: Suppose $(\Omega,\Sigma,\mathbb P)$ is a probability space. If $\mathbf Y : \Omega \mapsto \mathbb R$ is measurable w.r.t. the Borel $\sigma$-algebra on $\mathbb R$, then $\mathbf Y$ is called a Random Variable. The $\sigma$-algebra generated by $\mathbf Y$ is: $\sigma(\mathbf Y)=\{ \mathbf Y^{-1}(A):A\in\mathscr B(\mathbb R)\}\subset\Sigma.$ Definition of Independent Random Variables: Two random variables $\mathbf X$, $\mathbf Y$ are independent if events A and B are independent (i.e. $\mathbb P(A\cap B)=\mathbb P(A)\mathbb P(B)$), whenever $A\in\sigma(\mathbf X)$ and $B\in\sigma(\mathbf Y)$. Question 1: Why do we only consider $\mathscr B(\mathbb R)$ here? The measurable collection of set on $\mathbb R$ could be extended beyond the Borel sets if we give a measure (e.g. Lebesgue measure). Although the extension is to include null-sets, we get a lot more sets (by intersection/union of null-sets with Borel sets), why do we do not require $\mathbf Y$ to be measurable over them as well? Plus, even if N is a null-set on $(\mathbb R,\mathscr A,\mu)$, it does not mean $\mathbf Y^{-1}(N)$ is a null-set on $(\Omega,\Sigma,\mathbb P)$ - it could has a positive probability, and thus is not boring/trivial. Question 2: When it comes to independence of $\mathbf X$ and $\mathbf Y$, why do we only consider events in $\sigma(\mathbf X)$ and $\sigma(\mathbf Y)$? Why do we not care about events such as $L\in(\Sigma\setminus\sigma(\mathbf X))$ and $M\in(\Sigma\setminus\sigma(\mathbf Y))$? (i.e. $\mathbb P(L\cap M)\neq\mathbb P(L)\mathbb P(M)$ does not affect the independence of $\mathbf X$ and $\mathbf Y$). Note that for $L\in(\Sigma\setminus\sigma(\mathbf X))$ and $M\in(\Sigma\setminus\sigma(\mathbf Y))$ with $\mathbb P(L\cap M)\neq\mathbb P(L)\mathbb P(M)$, although $L\notin\sigma(\mathbf X)$, it is still somehow relevant to $\mathbf X$, because $\mathbf X$ is defined on every point of the set L, and maps those points to $\mathbb R$. For example, there could be set $P\in\sigma(\mathbf X)$ and $P\cap L\neq\emptyset$, so X could be defined on some points of $L$ despite of the fact that $L\notin\sigma(\mathbf{X})$. UPDATE: summarize answers + my thoughts Big thanks for both answers, very helpful! For Question 1 : To summarize the answers : examples were given about a mapping between probability space (which is e.g. $(\mathbb R,\mathscr M,\mathcal{L})$) to $\mathbb R$. And that mapping will map a $\mathcal{L}$-measurable set inversely back to a non-$\mathcal{L}$-measurable set in probability space. For example (from Halmos's Measure theory §19. Problem 3.) let f(x) = 0.5*(x + c(x)), where c(.) is a Cantor function. And then let $C$ be the Cantor set, $\exists A \subset C$ s.t. $A$ is $\mathcal L$-measurable but not a Borel set, and $F^{-1}(A)$ is not $\mathcal L$-measurable (Refer to this post ). Extended discussion : the above issue could be solved by two ways: 1) we give up Lebesgue sets but go with Borel sets (which is exactly our definition); but we could also do 2) Maintain Lebesgue sets, but give up those mappings as a valid random variable. In terms of why is it important to keep functions like above (i.e. maps Lebesgue measurable sets inversely back to a non-measurable set) to be a random variable, and thus we eventually choose 1) over 2)? - Please see @zoli's comments below For Question 2 : To summarize the answers : The parity example by @zhoraster is a good one, for illustrating ""independence meaning probability able to multiply"" v.s. ""normal meaning of generally not dependent""), but this is NOT the issue confusing me. What really bothers is that the property (independence) of v.r. is defined based on $\mathit subsets$ of $\Omega$ ; yet v.r. itself based on $\mathit elements$ of $\Omega$ . @zoli mentions wiki does give definition of independence with intervals and cdf, but per @Did's comment, and I also personally feel that, independence itself does not necessarily need to rely on those concepts - instead, could be just by the independence of $\sigma$-algebra generated by the r.v. Extended discussion : I think @zoli's comment could be close, that ""Events not belonging to the $\sigma$−algebra generated by a random variable cannot be described using only statements about the random variable."" - My own thoughts on this: although we define $\mathbb P$ over $\Omega$ for all elements in $\Sigma$, what could really be carry through the r.v. $\mathbf X$ is $\mathbb P_X(A)=\mathbb P(\{\omega|\mathbf X(\omega)\in A\})$, where $A\in\mathscr{B}(\mathbb R)$, thus set $L$ or $M$ above might have a measure under $\mathbb P$, we cannot reflect that via $\mathbb{P}_X$ or $\mathbb{P}_Y$. Further , if we do care about those events, we'll pick a r.v. $\mathbf Z$ s.t. $L\in\sigma(\mathbf Z)$. BTW, from wiki: ""the underlying probability space $\Omega$ is a technical device...In practice, one often...just puts a measure on ${\mathbb {R} }$..."" .","['independence', 'probability-theory', 'measure-theory']"
2158959,The natural numbers as the intersection of all inductive sets,"I am currently trying to understand a little bit of axiomatic set theory, following Enderton's fun book ""Elements of Set Theory"" and am a little unclear about the set of natural numbers as defined in Chapter 4. Firstly, let me note that the axioms in the book preceding the study of the natural numbers are the axiom of extensionality, the empty set axiom, pairing, unions, power sets and the axiom for constructing subsets. At the beginning of Chapter 4 is given the definition of the successor $a^{+}=a \cup \{a\}$ of a set $a$.  Enderton then defines $0=\emptyset$, $1=0^{+}$, $2=1^{+}$ and so on. A set $a$ is called inductive if $\emptyset \in A$ and $a \in A \implies a^{+} \in A$.  The axiom of infinity then asserts the existence of an inductive set, and the set of natural numbers $\omega$ is defined to be the intersection of all inductive sets.  The existence of this set follows from the axioms mentioned. Now clearly each set $0,1,2 \ldots$ belongs to the set $\omega$ since it contains $0=\emptyset$ and is closed under successors. My question: is the converse true?  Is every element of $\omega$ obtained from $0=\phi$ by applying the successor operation to $0$ finitely many times? I presume that this can be deduced, but as far as I can tell it is not addressed in the book. Note: If I had a way of constructing the ""set"" $X=\{0,1,2 ,\ldots n,n^{+},\ldots \}$ then it would be inductive, by construction, and therefore contain $\omega$ but I do not know how to construct the aforementioned set from the axioms given.  So an equivalent question is: how can I construct $X$?  (Perhaps the later axiom of replacement might help somehow?) Grateful for any help!",['elementary-set-theory']
2158960,Compute the given limit,"$$\lim_{x\rightarrow0^{+}} x^{0.7}(\ln(e^{x}  - 1))$$ Since $\ln(0)$ is undefined, I know I need to simplify that expression, as of now what I did was take $\ln(e^{x}(1- e^{-x})$. ie Take $e^{x}$ out and then use the property $\ln(ab) = \ln a + \ln b$ which still leaves me with $\ln(1 - e^{-x})$. Is there any way to simplify this expression?","['calculus', 'limits']"
2159000,Why doesn't the average angle made by a function with the $x-axis$ work as expected?,"The average angle made by a curve $f(x)$ between $x=a$ and $x=b$ is:
$$\alpha=\frac{\int_a^b\tan^{-1}{(f'(x))}dx}{b-a}$$
I don't think there should be any questions on that. Since $f'(x)$ is the value of $\tan{\theta}$ at every point, so $tan^{-1}{(f'(x))}$, should be the angle made by the curve at that point. Now, I expected this to hold:
$$\tan^{-1}\left({\frac{f{(b})-f{(a)}}{b-a}}\right)=\alpha=\frac{\int_a^b\tan^{-1}{(f'(x))dx}}{b-a}$$
because, $\tan^{-1}\left({\frac{f{(b})-f{(a)}}{b-a}}\right)$ is also the 'average angle' made by the curve between $x=a$ and $x=b$.
It held only approximately for $f(x)=\log{|\sec{x}|}$ when I checked for $a=0$ and $b=\frac{\pi}{4}$. It obviously holds for linear functions and I checked that it only approximately holds for quadratic functions. I don't know anything beyond high-school calculus, so couldn't check it for polynomials of degree greater than 2. I also tried root-mean-square instead of average angle: 
$$\beta=\sqrt{\frac{\int_a^b(\tan^{-1}{{(f'(x))}})^2dx}{b-a}}$$
 and expected the same to hold but that expression to didn't hold accurately. I took one step further and replaced $\tan^{-1}{x}$ with any function $g(x)$ and expected this to hold:
$$g\left({\frac{f{(b})-f{(a)}}{b-a}}\right)=k=\frac{\int_a^bg{(f'(x))}dx}{b-a}$$
But this one too only holds approximately for some $g(x)$ that I checked. I had worked with $f(x)=x^2$ to obtain the approximate formula for $g(a+b)$ in this previous post of mine: When do these expansions of $log(a+b)$, $tan^{-1}(a+b)$, $sin^{-1}(a+b)$, etc work? because for $f(x)=x^2$, the LHS of these expressions reduces to $g(a+b)$. So, why don't these expressions hold as expected?","['derivatives', 'angle', 'trigonometry', 'functions', 'integration']"
2159026,How smooth is the generalized power mean function?,"Consider the function of power mean of $1$ and $e$ (so that the computation is simpler). That is, define $\displaystyle f(x)=(\frac{1+e^x}{2})^{1/x}$ when $x\neq 0$ and $f(0)=\sqrt{e}$. It's not difficult to use L'Hopital rule with std. exponential trick to verify the function is continuous. The question is, do we know how smooth (in which $C^k$) is this function (and a closed form formula of $f^{(n)}(0)$ if possible)? The only way I can think of is trying to compute $\displaystyle \frac{f^{(n)}(h)-f^{(n)}(0)}{h}$ to verify if $f^{(n+1)}(0)$ exists, but the formula becomes extremely complicated and it's practically impossible to compute that limit even for $n=1$.","['derivatives', 'means', 'real-analysis', 'limits', 'calculus']"
2159042,Evaluating the integral $\frac{1}{2^{2n-2}}\int_0^1\frac{x^{4n}\left(1-x\right)^{4n}}{1+x^2} dx$,"Prove that : $$ \frac{1}{2^{2n-2}}\int \limits_{0}^{1} \dfrac{x^{4n}\left(1-x\right)^{4n}}{1+x^2} dx =$$ $$\sum \limits_{j=0}^{2n-1}\dfrac{(-1)^j}{2^{2n-j-2}\left(8n-j-1\right)\binom{8n-j-2}{4n+j}} + (-1)^n\left(\pi-4\sum \limits_{j=0}^{3n-1}\dfrac{(-1)^j}{2j+1}\right)\,\,\,\,(♣)$$ where $\binom{a}{b}= {}_aC_b=\frac{a!}{b!(a-b)!}$ . I was reading an article on "" $\frac{22}{7}$ exceeds $\pi$ "", when came across the generalized form $(♣)$ . Putting the value of $n=1$ in $(♣)$ , we get the famous Putnam Problem : $$0<\int \limits_0^1\dfrac{x^4(1-x)^4}{1+x^2}dx=\dfrac{22}{7}-\pi.\,\,\,\,(♠)$$ I know how to evaluate $(♠)$ by using expansion and polynomial long-division and then term-wise integration. The other integrals involving $n>1$ provide better approximations of $\pi$ . But I am unable to get even close to evaluating $(♣)$ because of the ' $n$ 's. I tried to do polynomial division but got badly stuck. The $1+x^2$ in the denominator reminds me of $\arctan(x)$ , but what can I do? Can anyone provide a bit of help as to how to evaluate this lovely integral ? Also I would like to know if it is okay to treat $n$ as a real number here instead of a natural number.","['calculus', 'integration', 'definite-integrals', 'pi', 'summation']"
2159062,Smallest positive integral value of $a$ such that ${\sin}^2 x+a\cos x+{a}^2>1+\cos x$ holds for all real $x$,"If the inequality $${\sin}^2 x+a\cos x+{a}^2>1+\cos x$$ holds for all $x \in \Bbb R$ then what's the smallest positive integral value of $a$? Here's my approach to the problem $$\cos^2 x+(1-a)\cos x-a^2<0$$
Let us consider this as a quadratic form respect to $a$. Applying the quadratic formula $a=\frac{-\cos x\pm\sqrt{5\cos^2 x+4\cos x}}2 $
and substituting $\cos x$ with $1$ and $-1$
we get 3 values of where the graph should touch the x axis $-2,0,1$
How should I proceed now?","['inequality', 'trigonometry', 'optimization', 'quadratics']"
2159110,Antiderivative of $e^{x^2}$: Correct or fallacy?,"Please check where is the mistake in this following process. I could not make out. $$e^x=\sum_\limits{n=0}^\infty \frac{x^n}{n!}$$
$$\implies e^{x^2}=\sum_\limits{n=0}^\infty \frac{{(x^2)}^n}{n!}=\sum_\limits{n=0}^\infty \frac{x^{2n}}{n!}$$
$$\implies \int e^{x^2} dx=\int \sum_\limits{n=0}^\infty \frac{x^{2n}}{n!} dx$$
$$\implies \int e^{x^2} dx=\sum_\limits{n=0}^\infty \frac{x^{2n+1}}{n!(2n+1)} + c$$ But $e^{x^2}$ has no antiderivative as such. How is then thus possible? Is this correct or just a fallacy?","['calculus', 'proof-verification', 'indefinite-integrals', 'integration', 'paradoxes']"
2159136,"If $f_k(x)=\frac{1}{k}\left (\sin^kx +\cos^kx\right)$, then $f_4(x)-f_6(x)=\;?$","I arrived to this question while solving a question paper. The question is as follows: If $f_k(x)=\frac{1}{k}\left(\sin^kx + \cos^kx\right)$, where $x$ belongs to $\mathbb{R}$ and $k>1$, then $f_4(x)-f_6(x)=?$ I started as $$\begin{align}
f_4(x)-f_6(x)&=\frac{1}{4}(\sin^4x + \cos^4x) - \frac{1}{6}(\sin^6x + \cos^6x) \tag{1}\\[4pt]
&=\frac{3}{12}\sin^4x + \frac{3}{12}\cos^4x - \frac{2}{12}\sin^6x - \frac{2}{12}\cos^6x \tag{2}\\[4pt]
&=\frac{1}{12}\left(3\sin^4x + 3\cos^4x - 2\sin^6x - 2\cos^6x\right) \tag{3}\\[4pt]
&=\frac{1}{12}\left[\sin^4x\left(3-2\sin^2x\right) + \cos^4x\left(3-2\cos^2x\right)\right] \tag{4}\\[4pt]
&=\frac{1}{12}\left[\sin^4x\left(1-2\cos^2x\right) + \cos^4x\left(1-2\sin^2x\right)\right] \tag{5} \\[4pt]
&\qquad\quad \text{(substituting $\sin^2x=1-\cos^2x$ and $\cos^2x=1-\sin^2x$)} \\[4pt]
&=\frac{1}{12}\left(\sin^4x-2\cos^2x\sin^4x+\cos^4x-2\sin^2x\cos^4x\right) \tag{6} \\[4pt]
&=\frac{1}{12}\left[\sin^4x+\cos^4x-2\cos^2x\sin^2x\left(\sin^2x+\cos^2x\right)\right] \tag{7} \\[4pt]
&=\frac{1}{12}\left(\sin^4x+\cos^4x-2\cos^2x\sin^2x\right) \tag{8} \\[4pt]
&\qquad\quad\text{(because $\sin^2x+\cos^2x=1$)} \\[4pt]
&=\frac{1}{12}\left(\cos^2x-\sin^2x\right)^2 \tag{9} \\[4pt]
&=\frac{1}{12}\cos^2(2x) \tag{10}\\[4pt]
&\qquad\quad\text{(because $\cos^2x-\sin^2x=\cos2x$)}
\end{align}$$ Hence the answer should be ... $$f_4(x)-f_6(x)=\frac{1}{12}\cos^2(2x)$$ ... but the answer given was $\frac{1}{12}$. I know this might be a very simple question but trying many a times also didn't gave me the right answer. Please tell me where I am doing wrong.","['trigonometry', 'functions', 'functional-equations']"
2159138,Let $D$ be a diagonal matrix; find the inverse of $D+ab^T$,"It's an exercise from A Primer on Linear Models written by John F. Monahan. Is there any special property of $ab^T$,where $a$ and $b$ are both vectors?","['matrices', 'linear-algebra', 'inverse']"
2159199,Set Notation Interpretation: Set Of A Set,"Let $S$ be the set of the 16 sequences of length 4 in which each element is a 0 or a 1.
Let $R$ be the relation on $S$ defined by $(s1, s2) ∈ R$ if and only if the entries of $s1$ can be rearranged to obtain $s2.$ Okay, so just ""take a block of any 4 $0$s or $1$s, if they have the same amount of $1$s they can be paired."" For each element $x ∈ S$, define the set $[x]$ = {$s ∈ S : (s, x) ∈ R$}. Every element the set {$[x] : x ∈ S$} is a set. Explicitly list these sets. This is where I'm confused about exactly what I'm supposed to be providing. If I try an provide an English translation, I come up with ""list every possible pair where $s$ is a 4 digit block of $0$s and $1$s, $x$ is a 4 digit block of $0$s and $1$s, and $x$ and $s$ contain the same amount of $1$s."" Do I have this right? It seems like a bizarrely easy and long task if that's the case, so I think I'm probably misinterpreting something in this notation.","['logic', 'elementary-set-theory']"
2159222,Show integral limit,"How to show that: $$I:=\lim_{\epsilon \to 0^+} \int_{\epsilon}^{2\epsilon} \frac{1}{\ln{(1+x)}}dx=\ln{2}$$ Using the mean value theorem for integrals I can show that $\frac{1}{2 }\leq I \leq 1$, but I'm not able to show that $I=\ln{2}$. Any hints?","['calculus', 'limits']"
2159278,Is the Laplace operator w.r.t. all coordinates or not?,"I have the equation
\begin{align}
\begin{cases}
    u_t(\boldsymbol{x},t)-\nabla^2u(\boldsymbol{x},t) = 0, &\boldsymbol{x}\in D, & t>0, \\
    u(\boldsymbol{x},t)=0, & \boldsymbol{x}\in\partial D, & t>0, \\
    u(\boldsymbol{x},0)=u_0(\boldsymbol{x}), &\boldsymbol{x}\in D,
\end{cases}
\end{align}
where $\boldsymbol{x}=(x_1,x_2,x_3)$. Does the Laplace operator acts on the vector $\boldsymbol{x}$ AND $t$? I.e. 
\begin{align}
\nabla^2 u(\boldsymbol{x},t)=\frac{\partial^2 u}{\partial x_1^2}+\frac{\partial^2 u}{\partial x_2^2}+ \frac{\partial^2 u}{\partial x_3^2}+\frac{\partial^2 u}{\partial t^2}
\end{align}
or just
\begin{align}
\nabla^2 u(\boldsymbol{x},t)=\frac{\partial^2 u}{\partial x_1^2}+\frac{\partial^2 u}{\partial x_2^2}+ \frac{\partial^2 u}{\partial x_3^2}
\end{align}
If the second one is correct, why is it so?","['multivariable-calculus', 'vector-analysis', 'partial-differential-equations']"
2159282,Bizarre Definite Integral,"Does the following equality hold? $$\large \int_0^1 \frac{\tan^{-1}{\left(\frac{88\sqrt{21}}{215+36x^2}\right)}}{\sqrt{1-x^2}} \, \text{d}x = \frac{\pi^2}{6}$$ The supposed equality holds to 61 decimal places in Mathematica, which fails to numerically evaluate it after anything greater than 71 digits of working precision. I am unsure of it's correctness, and I struggle to prove it's correctness. The only progress I have in solving this is the following identity, which holds for all real $x$: $$\tan^{-1}{\left( \frac{11+6x}{4\sqrt{21}} \right )} + \tan^{-1}{\left( \frac{11-6x}{4\sqrt{21}} \right )} \equiv \tan^{-1}{\left(\frac{88\sqrt{21}}{215+36x^2}\right)}$$ I also tried the Euler Substitution $t^2 = \frac{1-x}{1+x}$ but it looks horrible. Addition: Is there some kind of general form to this integral? Side thoughts: Perhaps this is transformable into the Generalised Ahmed's Integral, or something similar.","['integration', 'definite-integrals', 'pi']"
2159322,Vietoris Topology,"‎let ‎‎ $ X ‎‎‎‎$ ‎be a‎ ‎topological ‎space ‎and‎ ‎ $ \operatorname{‎Exp}(X‎)‎‎‎ $ ‎is ‎the set of all  ‎closed ‎non-empty subsets of $X$ .‎‎ If $ U , ‎V‎_{‎1‎}‎, V‎_{‎2‎}‎\ldots ‎V_{n}‎$ ‎are ‎the  non-empty open subset ‎in ‎ $ ‎X‎$ ‎‎‎, ‎define:‎
‎ $$ ‎\langle U , ‎V‎_{‎1‎}‎, V‎_{‎2‎}‎, \ldots V_{‎n}‎ \rangle ‎ ‎ =‎ ‎\{ F‎ ‎\in \operatorname{‎Exp}(X‎)‎‎‎‎\mid F‎\subseteq‎ ‎U,‎ \forall 1‎‎ \leq i ‎‎‎\leq n ‎‎‎:‎ F‎\cap ‎V_{i} ‎\neq \emptyset\}‎$$ ‎ families ‎ $ B‎ ‎‎ $ ‎i‎ncludes all sets of the form $\langle‎ U , ‎V‎_{‎1‎}‎, V‎_{‎2‎}‎, \ldots, ‎V_{‎n} \rangle ‎$ ‎is ‎‎the basis for a topology   for ‎ $\operatorname{ ‎Exp} (‎X‎) ‎$ ‎‎
This topology is called the Vietoris topology. My ‎question:‎ if ‎‎ $ ‎X‎$ ‎is ‎‎a $T_{1}$ ‎space, ‎then  ‎is the Vietoris ‎topology ‎ ‎‎ $ ‎T_{1}‎$ ‎?‎
‎
‎",['general-topology']
2159326,Distribution of two random variable generating schemes,"Goal: Generate five numbers from 0 to 1 with sum 1. Method 1: Generate four numbers in range $(0,1)$ (by uniform distribution) to be the cuts of the interval, i.e. say the four random numbers generated is $a_1<a_2<a_3<a_4$, then the five random numbers are $a_1, a_2-a_1, a_3-a_2, a_4-a_3,1-a_4$ Method 2: Generate five numbers in range $(0, n)$ (by uniform distribution), where $n$ is arbitrary number bigger than zero. And then normalise the sum to be 1, i.e. divide all the numbers by their sum. My question is, are the distribution of the two methods equivalent? Is any of the two correspondents to some well-known distributions?","['statistics', 'random-variables', 'probability-distributions']"
2159406,What are some transformations that leave the eigenvalues of a matrix unchanged?,I'm looking for transformations (isometries or not) that leave the spectrum of a matrix unchanged. I've only been able to come up with similarity transformations and transposition (is transposition in fact just a similarity transformation?). What are some others? Thanks.,"['eigenvalues-eigenvectors', 'transformation', 'matrices', 'invariance', 'linear-algebra']"
2159442,Isosceles triangle with two inscribed circles,Let's say we have isosceles triangle $ABC$ s.t. $AC=BC$. Let's define the middle of $AB$ as point $M$. On $AB$ we choose a random point $X$. We inscribe a circle with center $O_1$ in triangle $AXC$ and another circle with center $O_2$ in triangle $XBC$. Proof that the angle $O_1MO_2$ is exactly $90°$.,"['circles', 'trigonometry', 'triangles', 'geometry']"
2159465,Two convex polygons can be homeomorphic?,Can two convex polygons homeomorphic ? I know that the boundary of a polygon is homeomorphic to the circle $S^{1}$ but don't know with the interior ? Anybody help me ?,['general-topology']
2159507,Forms of a Rubik's Snake,"A Rubik's Snake is a game made by Rubik Erno. It is a rod with 24 triangular prisms fixed together on 23 pivots. Each pivot can be twisted with 4 x 90 degree turns to create different shapes. I know I can find the number of forms it can take, provided that it only makes 90 degree turns at each pivot and there is no 'lack' of space (can be easily found out by taking the figures 4 twists available, and 23 pivots for each twist. $4^{23} = 70368744177664$. 
How would I incorporate the fact that there is limited space available, and that not all moves would be accepted? I don't mind whether they are sensitive to orientations, or not. Therefore, go for the easier option.",['combinatorics']
2159523,Term for center of dataset,"There are a number of different ways of averaging a dataset, each with a specific definition. For example, mean: Sum of $N$ values divided by $N$ median: The $(N+1)/2$ term after $N$ values are sorted mode: The most common value in a dataset I am looking for a term for the $(\text{min}+\text{max})/2$, the 'middle' of a set of data. Is there a specific term for this definition already?","['terminology', 'statistics', 'average']"
2159541,closed under disjoint union $\Rightarrow$ closed under union?,"Closed under disjoint union $\Rightarrow$ closed under union? Seems easy, but I can't wrap my head around it. If the implication is not true, I would appreciate a counterexample. For completeness, the definitions made in class: For $A_1, A_2\subset\mathcal{A}$, subsets of a set system, we say that $\mathcal{A}$ is closed under disjoint union $A_1\sqcup A_2$, if $A_1\sqcup A_2$ is in $\mathcal{A}$. With $A_1\sqcup A_2=A_1\cup A_2$ if $A_1\cap A_2\ne \emptyset$.","['examples-counterexamples', 'elementary-set-theory']"
2159544,Alternative proof of Schwarz's theorem using Fubini's theorem,"I want to prove that for a function $f:\mathbb{R}^n\to\mathbb{R}$, if its mixed partial derivatives $D_{i,j}f$ and $D_{j,i}f$ exist and are continuous at a point $a$, then they are equal. I want to do it using Fubini's theorem (this is an excersise from Spivak's Calculus on manifolds ) and I would like you to tell me if the reasoning is correct. This is my approach: Suppose they are not equal, then we may assume $D_{i,j}f(a)>D_{j,i}f(a)$ and hence $D_{i,j}f(x)-D_{j,i}f(x)>0$ for all $x$ in a closed rectangle $R$ containing $a$. Let $[a,b]$ be the projection of $R$ over the $i$th coordinate axis and $[c,d]$ the projection over the $j$th coordinate axis. If we fix all the coordinates of $x$, except the $i$th and the $j$th,  then $D_{i,j}f(x)$ and $D_{j,i}f(x)$ are continuous in the compact region $[a,b]×[c,d]$, so they are integrable in that region and the same for $D_{i,j}f(x)-D_{j,i}f(x)$. If we also fix the $j$th coordinate of $D_{i,j}f(x)$ and the $i$th coordinate of $D_{j,i}f(x)$ we have (because of the FTC): 
  $$\int_{[a,b]}D_{i,j}f(x_i,x_j) d x_i=D_jf(b,x_j)-D_jf(a,x_j)$$
  $$\int_{[c,d]}D_{j,i}f(x_i,x_j) d x_j=D_if(x_i,d)-D_if(x_i,c)$$
  Now we can apply Fubini's theorem to obtain:
  $$\int\int_{[a,b]×[c,d]}D_{i,j}f(x_i,x_j)=f(b,d)-f(b,c)-f(a,d)+f(a,c)$$
  $$\int\int_{[a,b]×[c,d]}D_{j,i}f(x_i,x_j)=f(b,d)-f(b,c)-f(a,d)+f(a,c) $$
  So, we have:
  $$\int\int_{[a,b]×[c,d]}[D_{i,j}f(x_i,x_j)-D_{j,i}f(x_i,x_j)]=0$$
  which contradicts the fact that $D_{i,j}f(x)-D_{j,i}f(x)>0$, so the mixed partials must be equal. Thanks","['multivariable-calculus', 'proof-writing', 'alternative-proof', 'proof-verification']"
