question_id,title,body,tags
4629489,Function that gives nontrivial limit of $\frac{\exp\left\{ f(x) \right\} }{\sqrt{x} f(x)}$,"I am looking for a function $f(x): \Re_{>0} \to \Re_{>0}$ that is monotonically increasing such that \begin{equation} \tag{1} \label{lim} 
\lim_{x \to \infty } \frac{\exp\left\{ f(x) \right\} }{\sqrt{x} f(x)} = C \,, \end{equation} for some finite, nonzero $C$ . It is easily checked that, $$ \tag{2} \label{cases} \lim_{x \to \infty} \frac{\exp\left\{ \beta \log(x)^\alpha \right\} }{\sqrt{x} \beta \log(x)^\alpha} = \begin{cases} 0 & \text{if } \alpha < 1 \text{or } \alpha =1 \text{ and } \beta \leq 1/2,  \\ 
\infty & \text{if } \alpha > 1. \end{cases} $$ Hence, my sharpest result so far is $f(x) = \frac{1}{2} \log(x)$ . The question is whether a function that produces a nontrivial limit exists and how it looks like. Alternatively, I would also be very happy with any rate optimal function $f(x)$ such that for any other function $g(x)$ for which the limit in \eqref{lim} is zero, it also holds that $$ \lim_{x \to \infty} \frac{g(x)}{f(x)} = c \,. $$ for some $c\geq 0$ .","['limits', 'functions', 'upper-lower-bounds', 'real-analysis']"
4629493,"Distribution of $[0,1]$-valued random variable determined by distribution of $\{0,1\}$-valued exchangeable sequence","I'm trying to use de Finetti's theorem to show that if $Y$ is a random variable with values in $[0,1]$ , then the distribution of $Y$ is uniquely determined by its moments $\mathbb E[Y^n] =: m_n$ . This can be proven using separating families of functions (things like characteristic functions, moment-generating functions, etc), but I’d like to use a different strategy to build my intuition about exchangeability and conditional expectation. I've shown previously the following: Suppose $X = (X_n)_{n \in \mathbb Z}$ is an exchangeable sequence of random variables and $X_n \in \{0,1\}$ for all $n$ . Then the distribution of $X$ is uniquely determined by the values $\mathbb E[X_1 X_2 \cdots X_n] =: m_n$ . So, let $X = (X_n)_{n \in \mathbb N}$ be a sequence of random variables with values in $\{0,1\}$ for which, given $Y$ , the sequence is independent and $\mathrm{Ber}_Y$ -distributed. That is, for any finite $J \subset \mathbb Z$ , and $k_j \in \{0,1\}$ for $j \in J$ , $$
\mathbb P\left[ \mathop\bigcap_{j \in J} \{X_j = k_j\} \: \middle|\: Y \right] = \prod_{j \in J} \mathbb P\left[ X_j = k_j\:\middle|\: Y\right] \quad \textrm{and} \quad \mathbb P[X_n = 1\:|\:Y] = 1-\mathbb P[X_n = 0 \:|\: Y] = Y.
$$ (Intuitively, we can think of $Y$ as the random unfair weight of a coin, and $(X_n)$ as the result of flipping the unfairly weighted coin.) It turns out that $\mathbb E[Y^n] = \mathbb E[X_1 \cdots X_n]$ , which follows from a calculation with regular conditional distributions. And, by de Finetti’s theorem, $X$ is exchangeable. So, using the above result, the distribution $\mathcal L[X]$ is uniquely determined by the moments of $Y$ . My question: If $Y$ is a $[0,1]$ -valued random variable, and $X = (X_n)_{n \in \mathbb Z}$ is independent given $Y$ and $\mathrm{Ber}_Y$ -distributed given $Y$ (to put it another way, $X$ is $\mathrm{Ber}_Y^{\otimes \mathbb Z}$ -distributed given $Y$ ), is the distribution of $Y$ determined uniquely by the distribution of $X$ ? De Finetti's theorem also tells us that there is a random variable $Z$ with values in $[0,1]$ for which, given $Z$ , the sequence $(X_n)$ is independent and $\mathrm{Ber}_Z$ -distributed. The issue is this random variable (and its distribution) need not be uniquely defined by that theorem, at least the version I'm familiar with. However, using regular conditional distributions, we note that for $A \subset \{0,1\}^{\mathbb Z}$ measurable: $$
\mathbb P[X \in A] = \int_{[0,1]} \mathbb P[X \in A \: | \: Y = y] \, \mathbb P_Y[dy]  = \int_{[0,1]} \mathrm{Ber}_y^{\otimes \mathbb Z}(A) \, \mathbb P_Y[dy].
$$ In particular, $\int_{[0,1]} \mathrm{Ber}_y^{\otimes \mathbb Z}(A) \, \mathbb P_Y[dy] = \int_{[0,1]} \mathrm{Ber}_y^{\otimes \mathbb Z}(A) \, \mathbb P_Z[dy]$ for all measurable $A \subset \{0,1\}^{\mathbb Z}$ . But concluding that the distribution of $Y$ is uniquely determined from the functions $\mathrm{Ber}_y^{\otimes \mathbb Z}(A)$ is basically using separating classes of functions. I feel like there has to be a more natural way. Any thoughts?","['conditional-probability', 'conditional-expectation', 'probability-theory', 'probability']"
4629562,Can a topological embedding of a metric space into a metrizable space be extended to an isometric embedding for some metric on the codomain?,"Motivation: Consider for example a metric space that is a disjoint union of a point with $\mathbb R$ (with the usual metric on $\mathbb R$ ). Intuitively, it feels like there is some space ""missing"" from it. Topologically we can fix that by embedding it into $\mathbb R^n$ for the smallest $n$ possible: $2$ . Intuitively it feels like $\mathbb R^2$ is some sort of ""empty space completion"" of the original space, in the smallest possible dimention. One natural question to ask is whether we can extend this topological embedding into an isometric embedding, by choosing some suitable metric in $\mathbb R^2$ . I have no idea how to even begin attacking this question. This can clearly be generalized much more. It would be natural to ask: Question: If $M$ is a metric space and $N$ a metrizable space such that $M$ topologically embeds into $N$ , can we choose a metric in $N$ that generates the topology of $N$ such that $M$ isometrically embeds into $N$ (perhaps even with the same embedding as the given one?)? Is the answer to this question known, and/or what are some references studying questions similar to this one?","['metric-spaces', 'reference-request', 'metrizability', 'general-topology', 'isometry']"
4629574,Does dividing the unit square into small enough polygonal regions always yield a region surrounded by at least six others?,"Divide* the unit square $I^{2}=[0,1]\times[0,1]$ into polygonal regions, each having the property that the distance between any two of its points is less than $\frac{1}{30}$ . Question: Must there be a polygon $P$ within $I^{2}$ surrounded by at least six adjacent polygons- that is, touching $P$ in at least a point? If yes, how to prove it? *The polygons in the division may not be convex and the division may have ""gaps"" in the sense that the boundary between two adjacent polygons may be a disconnected polygonal line. The picture below illustrates gaps (white polygons) between the two grey polygons $P$ and $P'$ . $\hskip2.25in$ For example, the division pictured below has gaps. $\hskip2in$ I think the answer may be yes from some drawings. And I do not think it matters if there are gaps. Indeed, these gaps will be produced by polygons in the original division. We can merge them with adjacent polygons in order to produce larger polygons and yield a new division of $I^{2}$ such that any two adjacent polygonal regions have either a point or a connected polygonal line as boundary between them. Since this process only decreases the number of adjacent polygons a polygon has then if the answer is affirmative for this new division, then it must also be affirmative for the original division. But that is as far as I have been able to go towards a solution.","['general-topology', 'geometry', 'polygons']"
4629599,Asymptotic density of an infinite union of subgroups,"Let $1 < a_1 < a_2 < a_3 <{} ...$ be a sequence of integers. For a subset $A \subset \Bbb Z$ , denote by $d(A)$ its natural density (if it exists). Is it true that $$ 
\lim_{N \to +\infty} 
d\Big( \bigcup_{i \leq N} a_i \Bbb Z \Big) 
= 
d\Big( \bigcup_{i = 1}^{\infty} a_i \Bbb Z \Big)$$ ? The density of the finite union $\bigcup_{i \leq N} a_i \Bbb Z$ can be computed via the inclusion-exclusion principle, but it is not clear how it would relate to the infinite union.","['number-theory', 'arithmetic', 'elementary-number-theory', 'probability']"
4629603,Is the symmetric definition of the derivative equivalent?,"Is the symmetric definition of the derivative (below) equivalent to the usual one? \begin{equation}
\lim_{h\to0}\frac{f(x+h)-f(x-h)}{2h}
\end{equation} I've seen it used before in my computational physics class. I assumed it was equivalent but it seems like it wouldn't matter if there were a hole at $x=h$ in the symmetric derivative, whereas with the usual one it wouldn't be defined. Which is kinda interesting... If they're not equivalent - is there a good reason as to why we should use the common one? Or is the symmetric one actually more useful in some sense because it ""doesn't care"" about holes?","['calculus', 'definition', 'derivatives', 'real-analysis']"
4629665,"Generate a set of numbers that have uniqueness on addition, subtraction, multiplication and division [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question The problem is somewhat tricky. I would like to generate a set of number pairs $(x_1, y_1), (x_2, y_2)...$ so that each pair of numbers has uniqueness on all four computations (addition, subtraction, multiplication, and division). For example, $x_1 = 2, y_1 = 2$ is definitely not a good choice here because $2 + 2 = 2 * 2. x_1 = 2, y_1 = 1$ , and $x_2 = 1, y_2 = 2$ are also not good choices because $x_1 + y_1 = x_2 + y_2$ . For a better explanation of my question, please see the below picture. In short, every single cell value in the red box should be unique. For example, if I get the $v_1$ . then I know it is and can only be from row $1$ . . Hope my explanation is clear. Could anyone please provide me with guidance on how to generate a set of numbers like this or directions on where I should look into? I'm not sure what tag should I use it with, please bear with me if I used any wrong tag.","['algebra-precalculus', 'recreational-mathematics']"
4629671,"A proof that $\sum_{i=0}^{p_n-1} f(Y_{t_i^n})(X_{t_{i+1}^n}-X_{t_i^n})^2$ converges a.s. to $\int_0^t f(Y_s)\mathrm{d}\langle X, X\rangle_s$","Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $X = (X_t, t\ge 0)$ be a continuous square-integrable martingale and $\langle X, X\rangle$ its quadratic variation. Let $Y = (Y_t, t\ge 0)$ be a process with continuous trajectories. Let $f:\mathbb R \to \mathbb R$ be continuous. I'm trying to prove below result used in the proof of Itô's lemma, i.e, Theorem For each $t>0$ , there is an increasing sequence $0=t_0^n<\cdots<t_{p_n}^n=t$ of subdivisions of $[0, t]$ whose mesh tends to $0$ and that $$
\sum_{i=0}^{p_n-1} f (Y_{t_i^n}) (X_{t_{i+1}^n}-X_{t_i^n})^2 \quad  \underset{n \rightarrow \infty}{\longrightarrow} \quad \int_0^t f (Y_s) \mathrm{d}\langle X, X\rangle_s
\quad \text{almost surely}.
$$ Could you have a check if my below attempt is fine? Proof Clearly, there is an increasing sequence $0=t_0^n<\cdots<t_{p_n}^n=t$ of subdivisions of $[0, t]$ whose mesh tends to $0$ . We note that $$
\sum_{i=0}^{p_n-1} f (Y_{t_i^n}) (X_{t_{i+1}^n}-X_{t_i^n})^2 = \int_{[0, t]} f(Y_s) \mathrm d \mu_n (s),
$$ where $\mu_n$ is the random (discrete) measure on $[0, t]$ defined by $$
\mu_n := \sum_{i=0}^{p_n-1} (X_{t_{i+1}^n}-X_{t_i^n})^2 \delta_{t_i^n}.
$$ Let $D$ be the set that consists of all $t_i^n$ for $n \geq 1$ and $0 \leq i \leq p_n$ . Then $D$ is dense in $[0, t]$ . Lemma For every sequence $0=t_0^n<\cdots<t_{p_n}^n=t$ of sub-divisions of $[0, t]$ whose mesh tends to $0$ , we have $$
\sum_{i=0}^{p_n-1} (X_{t_{i+1}^n} - X_{t_i^n})^2 \underset{n \rightarrow \infty}{\longrightarrow} \langle X, X \rangle_t \quad \text{in probability}.
$$ By above Lemma , we get for every $r \in D$ , $$
\mu_n([0, r]) \quad \underset{n \rightarrow \infty}{\longrightarrow} \quad \langle X, X\rangle_r \quad \text{in probability}.
$$ So there is a subsequence of values of $n$ such that, along this subsequence, we have for every $r \in D$ , $$
\mu_n([0, r]) \quad \underset{n \rightarrow \infty}{\longrightarrow} \quad \langle X, X\rangle_r \quad \text{almost surely}.
$$ This implies that the sequence $(\mu_n, n \in \mathbb N)$ of random measures converges almost surely to a random measure $\mu$ (whose random c.d.f. is $\langle X, X\rangle_r \mathbf{1}_{[0, t]}(r)$ ) in the sense that $$
\mu_n (\omega) \quad \underset{n \rightarrow \infty}{\longrightarrow}\quad\mu(\omega)  \quad \text{in distribution}
$$ for $\mathbb P$ -a.e. $\omega \in \Omega$ . On the other hand, convergence in distribution is equivalent to weak convergence . Hence $$
\int_0^t f(Y_s ) \mathrm d \mu_n (s) \quad \underset{n \rightarrow \infty}{\longrightarrow} \quad \int_0^t f(Y_s) \mathrm d \mu(s) = \int_0^t f(Y_s) \mathrm{d} \langle X, X\rangle_s  \quad \text{almost surely}.
$$ This completes the proof.","['stochastic-integrals', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4629723,When is the generic fibre geometrically irreducible?,"Let $X \to Y$ be a morphism of schemes. According to Exercise 6.15 in Görtz—Wedhorn , if Y is integral with generic point $\eta$ , then the generic fibre $X_{\eta}$ is irreducible provided that $X$ is irreducible. My questions are as follows: Can someone please supply a proof of the above fact? If X is a scheme over $\mathbf{Q}$ that is geometrically irreducible, would it follow from the above exercise that $X_{\eta}$ is also geometrically irreducible? Thanks for your help.","['arithmetic-geometry', 'algebraic-geometry', 'schemes']"
4629728,Closed form or upper bound for $\lim_{n\to\infty}\sum_{k=0}^n\frac{e^{-k^2x}-e^{-(k+1)^2x}}{2k+1} $?,"Consider the expression $$
f(x,n)=\sum_{k=0}^n\frac{e^{-k^2x}-e^{-(k+1)^2x}}{2k+1}
$$ What is $\lim_{n\to\infty}f(x,n)$ ? If not a closed formula, is it possible to find an upper bound? Note: This seems like some type of telescoping series, weighted by terms of the form $(2k+1)^{-1}$ . The main reason I ask is that, if plotting $f$ for increasing values of $n$ , we get what seems like an approximation to a limiting function of $x$ , as seen here Any ideas?","['limits', 'logarithms', 'exponential-function', 'sequences-and-series']"
4629747,Boxes and dice game,"You have $9$ boxes numbered $1$ through $9$ and then you have two $6$ -sided dice. Each turn you roll the two dice and deduct the sum of the dice from the boxes by removing the number itself or any combination of boxes that sum up to the sum of the dice. For example, let's say you first roll a $4$ and $5$ . You then have the option to remove some combination of boxes that sum up to $9$ (yes, you can remove $9$ itself too). After these boxes are removed you roll both dice again and continue until you roll a sum that's impossible to remove with the remaining boxes. For example, if you're left with boxes $1$ , $3$ , $4$ and you roll a $10$ . The game is over and your final score is $\sum_{i = 1}^9 i = 45$ minus the sum of the remaining boxes. So if you were left with $1$ , $3$ , $4$ you end up with $37$ . Question. What is the optimal strategy?","['algorithmic-game-theory', 'dice', 'combinatorics', 'algebra-precalculus', 'probability']"
4629854,"Suppose $\gcd(a,b) =1$ then prove $\gcd(2a +b, a + b) = 1$. [duplicate]","This question already has answers here : If $a$ and $b$ are relatively prime, prove that $a + 2b$ and $2a+b$ are also relatively prime or have a gcd of $3$. (2 answers) $(m,n)=1$, what could $(3n-4m, 5n+m)$ be? (3 answers) Closed last year . I wish to prove the claim: If $\gcd(a,b) =1$ then $\gcd(2a +b, a + b) = 1$ . I have so far: By Bezouts lemma, there are integers $x,y$ such that $ax+by = 1$ . So, $d = \gcd(2a+b, a+ b)$ , then $d|(2a +b)$ and $d|(a+b)$ . I am not sure what to do after this. I have to use GCD Characterization to prove this.","['gcd-and-lcm', 'discrete-mathematics']"
4629856,Branch of mathematics that deals with repeated operations,"One interesting trait of subtraction is that it can introduce us to negative numbers using just positive whole numbers. For instance, $1-3=-2$ . Division, similarly, can introduce us to a new set of numbers: decimals and fractions. For instance, $3/2=1.5$ . Square roots can also show us a new set of numbers: imaginary. $\sqrt{-1}=i$ . The significance of this is that these operations are all inverses of operations that result from repeating a lower level function. If you could assign a number to this, you could say addition is 1. Multiplication is repeated addition, so it is 2 (it is defined as repeating a level 1 function). Exponentiation would be 3 (it is defined as repeating a level 2 function). Is there any branch of mathematics that deals with how properties of functions change as you go to higher levels? For example, exponentiation is not commutative ( $3^2\neq2^3$ ). This is different from both addition and multiplication, both components of exponentiation (it is repeated multiplication, which is repeated addition). I believe I heard somewhere that as you get a level higher, one property changes, but that may have just been a theory of mine. Is there any branch of mathematics that specifically deals with the properties of different levels of functions? Another, perhaps deeper question involves stretching the limits of mathematics. If we do indeed have a branch of mathematics where people study functions in this way, is there such thing as a level 0 function? Some function that is more fundamental than addition? If there is a study of these operations, mathematics would find a way to answer all of these questions. what about a level -1? i? $\pi$ ?","['exponentiation', 'reference-request', 'discrete-mathematics', 'arithmetic', 'tetration']"
4629878,A game that costs the square root of your winnings,"Imagine a sequence of games that charges you the square root of your total winnings to play. Your winnings at time $n + 1$ are $$S_{n + 1} = S_n + R_{n + 1} - \sqrt{S_n},$$ where $R_{n + 1}$ is your reward at time $n + 1$ . Say that you start at $S_0 = 0$ and your reward is always at least $1$ . Empirically, when $R_n$ are iid variables and $E[R_n] = \mu$ is big, it seems like $S_n$ converges to $\mu^2$ , and that the convergence is better when the variance of $R_n$ is small. Is this true? Why? What kind of convergence is it? Pointwise? In measure? L2? The analogous deterministic sequence $a_{n + 1} = a_n + r - \sqrt{a_n}$ does converge to $r^2$ , but the proof relies on inequalities that don't make sense in the random case. Here's a picture of some realizations of this sequence where the rewards are Poisson random variables with mean $30$ . I expect them to converge to $30^2 = 900$ , and they more or less do.","['probability-theory', 'probability']"
4629894,Reduce System of 3 ODEs to 2 ODEs,"I'm currently self-studying the book ""Nonlinear Dynamics and Chaos"". I got the following system $$
\dot{x} = -kxy \\
\dot{y} = kxy - ly \\
\dot{z} = ly
$$ According to a comment on one of Strogatzs' videos on Youtube one is able to reduce this system to 2 equations. I never did something along the lines so I'm not quite sure how to do this. $\dot{y}$ looks like a combination of the other two. Can I simply say something along the lines that $\dot{y}$ is equal to $-(\dot{x} - \dot{z})$ and simply apply the methods I already know to to $\dot{x}$ and $\dot{z}$ ? In the book is an exercise where they combine the system to a single ODE, but I don't really get how as well. I assume combining it to two ODEs must be simpler. I would appreciate some help or some resource I can check out to understand the process better.","['nonlinear-dynamics', 'analysis', 'ordinary-differential-equations']"
4629909,Linear recurrence with square values,"Let $(a_n)$ a linear recurrence sequence $$a_{n+d}= \sum_{k=1}^d c_k a_{n+d-k}$$ for all $n\ge 0$ ,  ( $c_1$ , $\ldots$ , $c_d$ are fixed integers). Assume that $a_n$ is a perfect square for all $n\ge 0$ . Then there exists an integral linear recurrence sequence $(b_n)$ such that $a_n = b_n^2$ Notes:   There are questions where $a_n$ is a given  integral recurrence sequence and it is required to show that $a_n$ is a perfect square for all $n$ . One approach is to produce a sequence $b_n$ such that $b_n^2 = a_n$ . Moreover, it appears that $(b_n)$ is also a recurrence sequence. The result is true if $a_n = P(n)$ where $P$ is a polynomial ( classic result). Any feedback would be appreciated! $\bf{Added:}$ Unsurprisingly, this turns out to be a particular case of an old problem, the ""Hadamard root problem"". I figure it might not originate with Hadamard, rather it has to do with ""Hadamard square"".  Knowing that the square of a linear recurrence sequence ( the Hadamard square) is again a recurrence sequence, the question is going the other way. Links  to original sources in van de Poorten et al. p. 69.","['elementary-number-theory', 'sequences-and-series']"
4629920,How to justify conclusion that a coin cannot be fair?,"Suppose one flips a coin $N$ times, and every single time it comes out heads (H).  For some sufficiently large $N = N_0$ (10?, 20?, 100?), even someone who knows nothing of probability theory would reach the conclusion that the coin is not fair 1 . For the sake of this question, let's say that $N_0 = 20$ . Suppose that one wanted to use probability theory to justify the conclusion that the coin is not fair.  How would one do it? The argument I am familiar with goes something like this: the probability of getting $N_0 = 20$ heads out of $N_0 = 20$ flips of a fair coin is $(\frac{1}{2})^{N_0} = (\frac{1}{2})^{20} = 9.5\times 10^{-7}$ .  In other words, this result is too improbable, and therefore, we conclude that the coin must not be fair. One problem with this argument, at least the way I worded it, is that it would apply to any sequence of 20 coin flips.  For example, if the sequence of result had been the very respectable-looking HHTTHTTTHTHHTTTHTHTT instead, one could argue exactly as before: the probability of getting this sequence out of $N_0 = 20$ flips of a fair coin is $(\frac{1}{2})^{N_0} = (\frac{1}{2})^{20} = 9.5\times 10^{-7}$ ; this result is too improbable, and therefore, we conclude that the coin must not be fair. Clearly, my reasoning, or at least its wording, can't be right, since it leads to the absurd conclusion that no coin can be fair. What is the correct way to use probability theory to justify the conclusion that a coin that produces heads in every one of $N_0$ flips (for some sufficiently large $N_0$ ) must not be fair? 1 Here and elsewhere in this post, strictly speaking, instead of ""the coin is not fair"", I should have written something like ""it is very unlikely that the coin is fair,"" but I ultimately decided that this additional precision may end up derailing the discussion.  If you feel that the wording I rejected is actually essential to reason properly through the situation, please feel free use it.","['probability-theory', 'probability']"
4629922,"Making the ""Two-Timing"" method rigorous (Perturbation theory)","I'm reading about the method of two-timing in section 7.6 of Nonlinear Dynamics and Chaos by Strogatz, and I have some questions about how to make this concept rigorous. In this section the book considers equations of the form $$ \hspace{4.5cm} \ddot{x} + x + \epsilon h(x,\dot{x}) = 0  \hspace{4.5cm} (1) $$ where $0 \leq \epsilon \ll 1$ , $x: \mathbb{R} \to \mathbb{R}$ , and $h: \mathbb{R}^2 \to \mathbb{R}$ is an arbitrary smooth function. The section first covers regular perturbation theory, which I feel okay with. But then author gets to the method of two-timing for ODEs that exhibit multiple time scales, and this is where the explanation starts to feel a little hand-wavey. Here is Strogatz's description of the method: To apply two-timing to (1), let $\tau = t$ denote the fast $O(1)$ time, and let $T = \epsilon t$ denote the slow time. We'll treat these two times as if they were independent variables. In particular, functions of the slow time $T$ will be regarded as constants on the fast time scale $\tau$ . It's hard to justify this idea rigorously, but it works. [Arg! Show me the rigor!]...Next we turn to the mechanics of the method. We expand the solution of (1) as a series $$ \hspace{3cm} x(t,\epsilon) = x_0(\tau, T) + \epsilon x_1(\tau, T) + O(\epsilon^2). \hspace{3cm} (2) $$ The time derivatives in (1) are transformed according to the chain rule: \begin{align*}
   \hspace{3cm} \dot{x} = \frac{dx}{dt} = \frac{\partial x}{\partial \tau} + \frac{\partial x}{\partial T}
   \frac{\partial T}{\partial t} = \frac{\partial x}{\partial \tau} + \epsilon \frac{\partial x}{\partial T}   \hspace{3cm} (3)
\end{align*} A subscript notation for differentiation is more compact; thus we write (3) as $$ \hspace{5.1cm} \dot{x} = \partial_{\tau} x + \epsilon \partial_T x. \hspace{5.1cm} (4) $$ After substituting (2) into (4) and collecting powers of $\epsilon$ , we find $$ \hspace{3.3cm} \dot{x} = \partial_{\tau} x_0 + \epsilon (\partial_T x_0 + \partial_{\tau} x_1) + O(\epsilon^2). \hspace{3.3cm} (5) $$ Similarly, $$ \hspace{3cm} \ddot{x} = \partial_{\tau \tau} x_0 + \epsilon(\partial_{\tau \tau} x_1 + 2 \partial_{T \tau} x_0) + O(\epsilon^2). \hspace{3cm} (6) $$ My main question : How can we make the above ideas rigorous? My attempt is as follows:  Given the ODE (1), we first assume that the solution $x(t;\epsilon)$ can be expressed as $$ x(t;\epsilon) = X(t,\epsilon t)$$ for some function $X: \mathbb{R}^2 \to \mathbb{R}$ . Now let $\tau, T: \mathbb{R} \to \mathbb{R}$ be functions defined by $\tau(t) := t$ and $T(t;\epsilon) := \epsilon t$ for all $t \in \mathbb{R}, \epsilon > 0$ . We then have $X(\tau(t),T(t)) = x(t;\epsilon)$ for all $t \in \mathbb{R}, \epsilon > 0$ . The chain rule computation is then straightforward: \begin{align*}
   \dot{X} := \frac{dX}{dt} &= \frac{\partial X}{\partial \tau} \frac{d\tau}{dt} + \frac{\partial X}{\partial T} \frac{dT}{dt} =  \frac{\partial X}{\partial \tau} + \frac{\partial X}{\partial T} \epsilon.
\end{align*} The subsequent equations (4)-(6) should now be the same, just with $x$ replaced by $X$ . (Is my interpretation correct so far?) The Taylor expansion part is where I'm a bit perplexed: It seems that (2) is a Taylor expansion of the function $\epsilon \mapsto x(t; \epsilon)$ , or equivalently, $\epsilon \mapsto X(\tau(t), T(t))$ . Now if we had some arbitrary smooth function $f: \mathbb{R}^2 \to \mathbb{R}$ indexed by some parameter $\epsilon$ , then I agree that the map $\epsilon \mapsto f(x,y; \epsilon)$ has some expansion $$ f(x,y; \epsilon) = f_0(x,y) + f_1(x,y) \epsilon + f_2(x,y) \epsilon^2 + \cdots, $$ (provided that it is smooth), where the coefficient functions $f_0, f_1,\ldots$ do not depend on $\epsilon$ . But my problem with (2) is that the ""coefficients"" of the Taylor series, namely $x_0(\tau,T), x_1(\tau,T),\ldots$ are functions of $\epsilon$ , since $T$ depends on $\epsilon$ . How then is this a legitimate Taylor expansion? Also, an additional question: Is there a way to know a prior if the solution $x = x(t;\epsilon)$ to some ODE $\dot{x} = f(x)$ can be expressed in the form $x(t;\epsilon) = X(t, \epsilon t)$ ? Or do we generally make such an assumption based on intuition? In one of the examples that Strogatz uses, the ODE has the analytic solution $x(t) = (1-\epsilon^2)^{-1/2} e^{-\epsilon t} \cos((1-\epsilon^2)^{1/2} t)$ , so clearly $X(t_1, t_2) := (1-\epsilon^2)^{-1/2} e^{-t_2} \cos((1-\epsilon^2)^{1/2} t_1)$ satisfies the requirement. But is there a way to see this a priori (i.e., if we couldn't analytically solve the ODE)? Any insights would be greatly appreciated. This two-timing concept is rather confusing to me, and I think seeing the details spelled out very rigorously would help my understanding.","['nonlinear-system', 'perturbation-theory', 'ordinary-differential-equations', 'dynamical-systems']"
4629947,How can I integrate $\int\sqrt{\frac{x^2+bx+c}{x^2+ex+f}}dx?$,"How can I integrate $$\int\sqrt{\dfrac{x^2+bx+c}{x^2+ex+f}}\,dx?$$ I was thinking a substitution $$t=\frac{x^2+bx+c}{x^2+ex+f},$$ which inverts as follows: $$(x^2+ex+f)t=x^2+bx+c$$ $$(t-1)x^2+(et-b)x+ft-c=0$$ $$x=\dfrac{b-et+\sqrt{(b-et)^2-4(t-1)(ft-c)}}{2(t-1)},$$ but I've never really dealt with integrals of this complexity with the aim of finding a closed form in terms of fundamental integral functions. The best I could do is expand the integrand as a not-so-nice power series.","['integration', 'special-functions', 'calculus', 'indefinite-integrals', 'quadratics']"
4629953,Finding the total number of objects(stones).,"I have $N$ stones. Then the stones are arranged in ascending order of weights. If I remove three stones that are heaviest, then the total weight of the stones decreases by $35$ %. Now if I remove the three lightest stones, the total weight of the stones further decreases by a factor of $\frac{5}{13}$ . Find the value of $N$ . What is tried:Let the total weight be $S$ . After removing three stones that are heaviest the total weight is $0.65 S$ and after further removing three stones that are lightest the total weight is $0.40 S$ . Now using average weights we have: $$\frac{0.65}{N-3} < \frac1N \text{ and } \frac{0.65}{N-3} < \frac{0.4}{N-6}$$ from which we can conculde that $N>8$ and $N< 11$ . So we are having two possible values of $N$ which are $9$ or $10$ . But how to narrow it down between $9$ and $10$ which one is correct? Source: Homework Question.","['algebra-precalculus', 'analysis', 'recreational-mathematics']"
4629967,"Given an equation with multiple variables, all integers, how would I minimize an extra variable (also an integer)?","Consider: $$2x + 4y + 8z + w = 75$$ Or more accurately for what I want to deal with: $$nx + oy + pz + w = q$$ Where $n$ , $o$ , $p$ , and $q$ are all positive integers and constant, and $x$ , $y$ , and $z$ are the variables being adjusted, also all positive integers. $w$ should also be a positive integer. How would I find, whether it's an algorithm or something purely mathematical, a way to minimize $w$ ? That is, find the combination of values of $x$ , $y$ , and $z$ that results in the smallest possible $w$ ? Example: $$14x + 60y + 17z + w = 723$$ This has 20 answers, as an example one of them is $x=9, y=4, z=21$ where $w = 0$ . I think it could also be thought of like this: $$f(x,y,z) = -nx - oy - pz + q$$ Find values of $x$ , $y$ , and $z$ at a minimum of $f(x,y,z)$ .","['optimization', 'functions']"
4629973,True or False: $P(A \cup B \cup C) \geq P(A | B \cup C) P(B) P(B \cup C)$,"I am trying to determine if $$P(A \cup B \cup C) \geq P(A | B \cup C) P(B) P(B \cup C)$$ I tried $$\begin{align}
P(A \cup B \cup C) \geq P(A | B \cup C) P(B) P(B \cup C) = P(A \cap (B \cup C) ) \cdot P(B)
\end{align}$$ which is not leading to anywhere. I feel like I am missing something really obvious. Thank you.","['statistics', 'probability']"
4629997,Why is % difference of aggregation higher than individual % differences?,"I'm calculation the % difference in completion rate (before vs after) of Product A (0.3%), Product B (16.7%) and the combination of Product A and B (17.0%). I'm unable to explain why the combination of Product A and B doesn't have a % difference between the one of Product A and B. (Attached the image with specific calculation) Can someone can explain why the % difference of combination of Product A and B isn't in between 0.3% and 16.7%, as I would expect?","['algebra-precalculus', 'calculator']"
4630044,Evaluation of the infinite series $\sum_{n=1}^\infty (-1)^{n-1} \frac{\overline{H}_n}{n \binom{2n}{n}}$,"I am trying to evaluate the infinite series $$\sum_{n=1}^\infty (-1)^{n-1} \frac{\overline{H}_n}{n \binom{2n}{n}}$$ where $\overline{H}_n = 1 - \frac{1}{2}+ \frac{1}{3} - \ldots+ \frac{(-1)^{n-1}}{n}$ is $n$ -th skew harmonic number. My attempt was to use the beta function: \begin{align*}
\sum_{n=1}^\infty (-1)^{n-1} \frac{\overline{H}_n}{n \binom{2n}{n}}
&= \sum_{n=0}^\infty (-1)^n \frac{\overline{H}_{n+1}}{(n+1) \binom{2n+2}{n+1}}\\
&= \frac{1}{2} \sum_{n=0}^\infty (-1)^n \frac{\overline{H}_{n+1} (n!)^2}{(2n+1)!}\\
&= \frac{1}{2} \sum_{n=0}^\infty (-1)^n \overline{H}_{n+1} \frac{\Gamma^2(n+1) }{\Gamma(2n+2)}\\
&= \frac{1}{2} \sum_{n=0}^\infty (-1)^n \overline{H}_{n+1} B(n+1, n+1)\\
&= \frac{1}{2} \sum_{n=0}^\infty (-1)^n \overline{H}_{n+1} \int_0^1 t^n(1-t)^n\,dt\\
&= \frac{1}{2}\int_0^1 \sum_{n=0}^\infty (-1)^n \overline{H}_{n+1} t^n(1-t)^n\,dt.
\end{align*} Since $$\sum_{n=1}^\infty \overline{H}_n x^n = \frac{\log(1+x)}{1-x},$$ we get that \begin{align*}
\frac{1}{2}\int_0^1 \sum_{n=0}^\infty (-1)^n \overline{H}_{n+1} t^n(1-t)^n\,dt
&= -\frac{1}{2}\int_0^1\frac{\log(1-t(1-t))}{t(1-t)(1+t(1-t))} \,dt\\
&= \frac{1}{2}\underbrace{\int_0^1 \frac{\log(1-t(1-t))}{t-1}\,dt}_{= \frac{\pi^2}{18}} - \frac{1}{2}\underbrace{\int_0^1 \frac{\log(1-t(1-t))}{t}\,dt}_{= \frac{\pi^2}{18}}\\
&- \frac{1}{2}\int_0^1 \frac{\log(1-t(1-t))}{t^2 -t -1}\\
&= \frac{\pi^2}{18}-\frac{1}{2}\int_0^1 \frac{\log(t^3 + 1) - \log(t+1)}{t^2 - t -1}\,dt\\
&= \frac{\pi^2}{18} + \frac{1}{2}\underbrace{\int_0^1 \frac{\log(t+1)}{t^2 - t -1}\,dt}_{= \frac{\pi^2}{5\sqrt{5}}+\frac{1}{\sqrt{5}}\left(\mathrm{Li}_2(-2 - \sqrt{5}) - \mathrm{Li}_2(-2 + \sqrt{5})\right)} - \frac{1}{2}\int_0^1 \frac{\log(t^3+1)}{t^2 - t -1}\,dt\\
&= \frac{\pi^2}{18} + \frac{\pi^2}{10\sqrt{5}} + \frac{\mathrm{Li}_2(-2 - \sqrt{5}) - \mathrm{Li}_2(-2 + \sqrt{5})}{2\sqrt{5}}  - \frac{1}{2}\int_0^1 \frac{\log(t^3+1)}{t^2 - t -1}\,dt.
\end{align*} We are left with the integral $$\int_0^1 \frac{\log(t^3+1)}{t^2-t-1} \,dt.$$ I am having trouble evaluating this integral. When computed by Wolfram Mathematica, it gives a result with a lot of polylogarithms and unit imaginary numbers $i$ : \begin{align*}
\int_0^1 \frac{\log(t^3+1)}{t^2-t-1} \,dt
&= \frac{\text{Li}_2\left(\frac{1}{2}-\frac{\sqrt{5}}{2}\right)}{\sqrt{5}}-\frac{\text{Li}_2\left(2-\sqrt{5}\right)}{\sqrt{5}}+\frac{\text{Li}_2\left(3-\sqrt{5}\right)}{\sqrt{5}}-\frac{2 \text{Li}_2\left(\frac{\sqrt{5}-i \sqrt{3}}{\sqrt{5}+1}\right)}{\sqrt{5}}\\
&-\frac{2 \text{Li}_2\left(\frac{i \sqrt{3}+\sqrt{5}}{\sqrt{5}+1}\right)}{\sqrt{5}}-\frac{\text{Li}_2\left(\frac{2}{\sqrt{5}+3}\right)}{\sqrt{5}}-\frac{2 \text{Li}_2\left(\frac{\sqrt{5}-1}{\sqrt{5}-i \sqrt{3}}\right)}{\sqrt{5}}-\frac{2 \text{Li}_2\left(\frac{\sqrt{5}-1}{i \sqrt{3}+\sqrt{5}}\right)}{\sqrt{5}}-\frac{2 \pi ^2}{3 \sqrt{5}}\\
&+\frac{\log ^2(2)}{2 \sqrt{5}}-\frac{\log ^2\left(-\frac{i}{\sqrt{3}+i \sqrt{5}}\right)}{\sqrt{5}}-\frac{\log ^2\left(\frac{i}{\sqrt{3}-i \sqrt{5}}\right)}{\sqrt{5}}-\frac{3 \log ^2\left(\sqrt{5}+1\right)}{2 \sqrt{5}}\\
&-\frac{\log ^2\left(\sqrt{5}+2\right)}{2 \sqrt{5}}-\frac{2 \log \left(\frac{\sqrt{3}+i}{\sqrt{3}+i \sqrt{5}}\right) \log \left(\sqrt{5}-1\right)}{\sqrt{5}} -\frac{2 \log \left(\frac{\sqrt{3}-i}{\sqrt{3}-i \sqrt{5}}\right) \log \left(\sqrt{5}-1\right)}{\sqrt{5}}\\
&-\frac{2 \log \left(-\frac{i}{\sqrt{3}+i \sqrt{5}}\right) \log \left(\sqrt{5}+1\right)}{\sqrt{5}} -\frac{2 \log \left(\frac{i}{\sqrt{3}-i \sqrt{5}}\right) \log \left(\sqrt{5}+1\right)}{\sqrt{5}}\\
&-\frac{\log \left(\sqrt{5}-1\right) \log \left(\sqrt{5}+3\right)}{\sqrt{5}}+\frac{\log \left(\sqrt{5}+1\right) \log \left(\sqrt{5}+3\right)}{\sqrt{5}}\\
&+\frac{2 \log \left(\sqrt{5}+1\right) \log \left(\frac{\sqrt{3}-i}{\sqrt{3}+i \sqrt{5}}\right)}{\sqrt{5}}+\frac{2 \log \left(\sqrt{5}+1\right) \log \left(\frac{\sqrt{3}+i}{\sqrt{3}-i \sqrt{5}}\right)}{\sqrt{5}}+\frac{\log (64) \log \left(\sqrt{5}-1\right)}{3 \sqrt{5}}\\
&-\frac{\log (64) \log \left(\sqrt{5}+1\right)}{3 \sqrt{5}}-\frac{\log (64) \log \left(\sqrt{5}+3\right)}{6 \sqrt{5}} \approx -0.176256827758015.
\end{align*} How would we evaluate the remaining integral? Is there a better way to evaluate the infinite series? I still hope that its closed form does not include any imaginary units $i$ .","['integration', 'contour-integration', 'complex-integration', 'sequences-and-series']"
4630065,"Is there any book ""economics for mathematicians""? [closed]","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed last year . This post was edited and submitted for review last year and failed to reopen the post: Original close reason(s) were not resolved Improve this question If you search economics book there are plenty of them on the market. We may say the say about ""mathematics for economists"" or ""application of mathematics in economics"". But I've never seen book which tells me about economics based on statistics, optimization and other field of mathematics. Where every law, curve and so on is based on some statistical experiment or real analysis. Example: I'm currently reading https://www.amazon.com/Principles-Microeconomics-N-Gregory-Mankiw/dp/1305971493 and the problem here is that there're no proof for principles. Author just says ""people always  have trade-offs"". But from the other hand there's optimization theory, and if we care only about revenue function - we may find maximum. (So in that case we don't have any trade-offs). Let me know, please, if you know any book ""economics for mathematicians"". Thanks! PS. There's similar question on site, but it's outdated. (But have a little different idea) ( Textbooks of economics for mathematicians )","['economics', 'statistics', 'book-recommendation']"
4630154,understanding the proof that $dB_t^k dB_t^j = 0$ for $j\neq k$ for Brownian Motions,"It is a well known fact that for $j\neq k$ , we have $dB_t^k dB_t^j = 0. $ The argument for this is given by the fact that for independent Gaussians $G$ and $G'$ we have $\frac{1}{\sqrt{2}}(G \pm G')$ are independent Gaussians. In Rene Schilling's Brownian Motion, it is given in the following sense. We have a convergence in probability of $$ J_1 = \sum_{i,j}\sum_l\partial_i\partial_jf(\xi_l)\sum_{k=1}^d \sigma_{jk}(t_{l-1}) \sigma_{ik}(t_{l-1})(\Delta_l B^k)^2 \to \Sigma
$$ where $d$ is the dimension and $$ \Sigma:=\sum_{i,j}\int_0^t \partial_i \partial_j f(X_s) \sum_{k=1}^d \sigma_{jk}(s)\sigma_{ik}(s)ds,$$ $\Delta_lB^k=B_{t_l}^k-B_{t_{l-1}}^k$ . Also $\sigma=(\sigma_{jk})\in \mathbb{R}^{m\times d}$ is in $S_T$ which is the family of all simple processes on $[0,T]$ of the form $$f(t,\omega)=\sum_{j=1}^n \phi_{j-1}(\omega)1_{[s_{j-1},s_j)}(t)$$ where $n\ge 1$ , $0=s_0\le s_1 \le \dots \le s_n\le T$ and $\phi_j \in L^\infty({\mathscr{F}_{s_j}})$ . Now in the theorem, we have $f\in C^2_b$ and take $\xi_l$ to be intermediate values such that $f(\xi_l$ ) is measureable. Here $t_l$ are the partition points of $[0,T]$ and we take the maximum of the partitions to $0$ . The text states that we get the above $J_1$ convergence to $\Sigma$ due to a previous result that gives $$ \sum_l g(\xi_l)(Y_{t_l}-Y_{t_{l-1}})^2 \to \int_0^T g(X_s)\tau^2(s)ds $$ where $Y_t = Y_0 + \int_0^t \tau(s)dB_s + \int_0^t c(s)ds.$ Now the Ito Process $Y_t$ in this result is a $1$ -dimensional Ito process, and the theorem we are trying to prove is for an $m$ -dimensional Ito Process and in this result we have $(Y_{t_l}-Y_{t_{l-1}})^2=(\tau(t_{l-1})(B_{t_l}-B_{t_{l-1}})+c(t_{l-1})(t_l-t_{l-1}))^2$ . And decompose $\sum_{l=1}^n (g(\xi_l))(Y_{t_l}-Y_{t_{l-1}})^2$ into three terms where the other two tends to $0$ and only $\sum_{l=1}^N g(\xi_l)\tau^2(t_{l-1})(B_{t_l}-B_{t_{l-1}})^2$ converges to $\int_0^T g(X_s)\tau^2(s)ds$ .   So how do we extend this result to the $m$ -dimensional case to get the convergence in $J_1$ ? Secondly, the text then says finally, we get for a suitable value $\Sigma'$ $$J_2 =\sum_{i,j}\sum_l\partial_i \partial_jf(\xi_l) \sum_{k \neq k'}\sigma_{jk}(t_{l-1})\sigma_{ik'}(t_{l-1})\Delta_l B^k \Delta_l B^{k'}\to \Sigma' - \Sigma'=0. $$ The text says
this result follows from the convergence of $J_1$ (and we know from other parts of the proof that $J_2$ converges) and the independence of $B^k$ and $B^{k'}$ since we have $2(\Delta_lB^k)(\Delta_lB^{k'})= [\Delta_l(\frac{B^k + B^{k'}}{\sqrt(2)})]^2 - [\Delta_l(\frac{B^k - B^{k'}}{\sqrt(2)})]^2$ where $\frac{1}{\sqrt{2}} (B^k \pm B^{k'})$ are independent one-dimensional Brownian motions. I can't wrap around my head for this. Why does the convergence of $J_1$ ensure that $J_2$ converges to some value minus itself? Isn't it possible that $\sum_{i,j} \sum_l \partial_i \partial_j f(\xi_l) \sum_{k<k^{'}} \sigma_{jk}(t_{l-1}) \sigma_{ik'}(t_{l-1})\Delta_lB^k \Delta_l B^{k'}$ diverges instead of converging to $\Sigma'$ ? And how can we interpret $(\Delta_lB^k)(\Delta_lB^{k'})= [\Delta_l(\frac{B^k + B^{k'}}{\sqrt(2)})]^2 - [\Delta_l(\frac{B^k - B^{k'}}{\sqrt(2)})]^2$ in terms of $\Delta_t B^k = B_{t_l}^k - B^k_{t_{l-1}}$ with both $k$ and $k'$ involved? $\Delta_t B^k = B_{t_l}^k - B^k_{t_{l-1}}$ is the kind of form of Brownian motion differences that allowed the convergence of $J_1$ but I don't know how it still works here.","['stochastic-integrals', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4630185,"Proving $\dim\{B\in {\rm M}(n,\mathbb{R}):BA^T=-AB^T\}=\frac{n(n-1)}{2}$ where $A\in {\rm O}(n)$ [duplicate]","This question already has answers here : Show that an orthogonal group is a $ \frac{n(n−1)}2 $-dim. $ C^{\infty} $-Manifold and find its tangent space (2 answers) Closed last year . I'm reading Lee's Introduction to Smooth Manifolds and trying to show that $$T_A {\rm O}(n)=\{B\in {\rm M}(n,\mathbb{R}):BA^T=-AB^T\}.$$ Here is my attempt about this question: Take any smooth curve $\gamma(t)$ in ${\rm O}(n)$ satisfying $\gamma(0)=A$ , then $$\gamma(t)\cdot\gamma(t)^T=I_n.$$ By taking the derivative of the above formula with respect to $t$ , we can get $$\gamma'(t)\cdot\gamma(t)^T+\gamma(t)\cdot(\gamma(t)^T)'=0,$$ then $$\gamma'(0)\cdot\gamma(0)^T=-\gamma(0)\cdot(\gamma(t)^T)'|_{t=0}=-\gamma(0)\cdot\gamma'(0)^T.$$ Since $\gamma(0)=A$ , we obtain that $\gamma'(0)$ satisfies $$\gamma'(0)\cdot A^T=-A\cdot\gamma'(0)^T,$$ and $\gamma'(0)\in T_A {\rm O}(n)$ is arbitrary, therefore $$T_A {\rm O}(n)\subset\{B\in {\rm M}(n,\mathbb{R}):BA^T=-AB^T\}.$$ Now it suffices to prove that $$\dim\{B\in {\rm M}(n,\mathbb{R}):BA^T=-(BA^T)^T\}=\frac{n(n-1)}{2}$$ where $A\in {\rm O}(n)$ , because $\dim T_A {\rm O}(n)=\dim{\rm O}(n)=\frac{n(n-1)}{2}$ , but I'm stuck here. I know that the dimension of the vector space composed of all skew-symmetric matrices is $\frac{n(n-1)}{2}$ , and the above set is very similar to the vector space composed of all skew-symmetric matrices in form, but I don't know what the connection between them is and how to connect them. Any help would be great appreciated!","['smooth-manifolds', 'matrices', 'linear-algebra', 'manifolds', 'differential-geometry']"
4630198,The minimum rectangle area that can contain both triangle and circle.,"I got this problem during my interview and I would love to find a good answer to this. Problem: Given a circle radius equal to r and an equilateral triangle each side has a length equal to a (as in the image below) image . Calculate the minimum rectangle area that can contain both a triangle and
a circle. A circle can also contain a triangle and a triangle can also contain a circle inside. The lines of the triangle and the circle cannot overlap each other (They can touch each other). The triangle can rotate freely. To solve this, I was thinking about 3 scenarios that could happen: The circle is smaller than the inscribed circle of the triangle. The circle is bigger than the circumscribed circle of the triangle. The circle radius is bound between these values. Here is my code to solve this def my_function(a, r):
    
    diam = r*2
    h_tri = a*math.sqrt(3)/2
    inner_r = a*math.sqrt(3)/6
    outer_r = a*math.sqrt(3)/3

    # Scenario 1: circle is inside the triangle
    if r <= inner_r:
        return a*h_tri

    # Scenario 2: triangle is inside the circle
    elif r >= outer_r:
        return diam**2

    # Scenario 3: The circle radius is bound between these values.
    else:
        return ""I don't know"" The last scenario is a very tricky one and probably my logic is wrong as well. Do you guys have a different approach to this problem?","['packing-problem', 'geometry', 'algorithms']"
4630250,Alice plays a game of choosing numbers and replacing them with some other. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question The following $100$ numbers are written on the board: $$2^1 - 1, 2^2 - 1, 2^3 - 1, \dots, 2^{100} - 1.$$ Alice chooses two numbers $a,b,$ erases them and writes the number $\dfrac{ab - 1}{a+b+2}$ on the board. She keeps doing this until a single number remains on the board. If the sum of all possible numbers she can end up with is $\dfrac{p}{q}$ where $p, q$ are coprime, then what
is the value of $\log_{2}(p+q)$ ? Since many of these types of questions uses invariants, I was trying to find an invariant. But I'm unable to find it. I would like someone to provide hints.","['contest-math', 'invariance', 'recreational-mathematics', 'discrete-mathematics']"
4630256,"($\forall x$) $f(\frac{x}n) \to 0$, $n \to \infty$ $\overset{?}{\Rightarrow} \lim_{x \to 0} f(x) = 0.$","Consider $f: \mathbb{R} \to \mathbb{R}$ . Suppose that for all $x \in \mathbb{R}$ the following condition holds: $$f\Big(\frac{x}n\Big) \to 0, \quad n \to \infty.$$ Does it follow that $\lim_{x \to 0} f(x) = 0?$ If I'm right then there's a counterexample. Question. Is there a simple counterexample? Is there an explicit form of such a function $f$ ? My counterexample. It looks like we may create it this way. Find positive numbers $a_1, a_2, \ldots $ which are linearly independent over $\mathbb{Q}$ and which are s.t. $a_n \downarrow 0$ . Put $f_1 (x) = \frac{1}k$ if $x = \frac{a_1}k$ for some $k \in \mathbb{N}$ and $f_1(x) = 0$ otherwise. Put $f(x) = \inf_n f_n(x)$ .
Notice that $f_m(a_n)=0$ if $m \ne n$ and $f_n(a_n)=1$ . We have $f(a_n)=1$ and $a_n \to 0$ hence $f$ is a counterexample. Addition . As @Martin R said below there's a simplification of this example: we may put $f(x) = 1$ if $x = a_n$ for some $n$ , and $f(x)=0$ otherwise.","['examples-counterexamples', 'alternative-proof', 'calculus', 'functions', 'limits']"
4630398,Calculate $\int_{-\infty}^\infty\frac{x^2}{1+x^8}dx$,"I wanted to practice using the Residue theorem to calculate integrals. I chose an integral of the form $$\int_{-\infty}^\infty\frac{x^n}{1+x^m}dx$$ and then choose random numbers for $n$ and $m$ , which happened to be $$\int_{-\infty}^\infty\frac{x^2}{1+x^8}dx$$ I know there is a general formula for integrals of this form, which is $$\int_{-\infty}^\infty\frac{x^n}{1+x^m}dx=\frac{\pi}{m}\csc\left(\frac{\pi(n+1)}{m}\right)$$ But let's forget about this right now. I used the Residue theorem and the semicircular contour to get that $$\int_{-\infty}^\infty\frac{x^2}{1+x^8}dx=\Re\left(\frac{\pi i}{4}\left(\frac{1}{e^{\frac{5\pi i}{8}}}+\frac{1}{e^{\frac{15\pi i}{8}}}+\frac{1}{e^{\frac{25\pi i}{8}}}+\frac{1}{e^{\frac{35\pi i}{8}}}\right)\right)$$ Is there any other ways to calculate this integral?","['integration', 'definite-integrals', 'analysis', 'complex-analysis', 'calculus']"
4630406,A few $(3)$ questions regarding Spivak's proof of the Inverse Function Theorem.,"I have a few questions regarding Spivak's proof of the Inverse Function Theorem: Theorem: Let $f$ be a function $\mathbb{R}^n\to\mathbb{R}^n$ . If $\ \ \ \ a)$ $f$ is $C^1$ in an open set containing $a\in\mathbb{R}^n$ , and $\ \ \ \ b)$ $f'(a)$ is invertible i.e. $\det f'(a) \ne 0$ , then there is an open set $V$ containing $a$ and an open set $W$ containing $f(a)$ such that $f:V\to W$ is bijective. $f^{-1}:W\to V$ is $C^1$ . The equation $$(f^{-1})'(y) = \left[f'(x)\right]^{-1}$$ holds for any $y\in W$ and $x := f^{-1}(y)$ . Proof: Letting $\lambda = f'(a)$ , we have that $$(\lambda^{-1}\circ f)'(a) = (\lambda^{-1})'(f(a))\circ \lambda = \lambda^{-1}\circ \lambda = \text{Id}.$$ That is, if the theorem holds for $\lambda^{-1}\circ f$ , then it clearly holds for $f$ . Therefore we may assume at the outset that $f'(a)$ is the identity. Whenever $f(a + h) = f(a)$ we have $$1 = \lim_{h\to 0}\frac{|h|}{|h|}
= \lim_{h\to 0}\frac{|f(a+h)-f(a)-\lambda(h)|}{|h|}=0.$$ Meaning we cannot have $f(x)=f(a)$ for $x$ arbitrarily close to, but unequal to, $a$ . Therefore there is a closed rectangle $U$ containing $a$ in its interior such that $f(x)\ne f(a)$ for all $x\in U$ different than $a$ . Since $f$ is $C^1$ in an open set containing $a$ , we can also assume that $\det f'(x) \ne 0$ for all $x\in U$ . $|\partial_jf_i(x) - \partial_jf_i(a)|<1/2n^2$ for all $i,j$ , and $x\in U$ . Observe that $$|x_1-x_2|-|f(x_1)-f(x_2)|
\le |f(x_1)-x_1 - [f(x_2)-x_2]|
\le \frac{1}{2}|x_1-x_2|$$ where the first inequality comes from the Inverse Triangle Inequality , while the second is derived by $(3)$ and a Lemma . We obtain $|x_1-x_2|\le 2|f(x_1)-f(x_2)|$ for $x_1,x_2\in U$ . Now $f(\partial U)$ is a compact set which, by $(1)$ , does not contain $f(a)$ . Therefore there is a number $d>0$ such that $|f(a)-f(x)|\ge d$ for $x\in \partial U$ . Let $$W:= \{ y:|y-f(a)|<d/2 \} .$$ We have $|y-f(a)| < |y-f(x)|$ for any $y\in W$ and $x\in \partial U$ . We will show that for any $y\in W$ there is a unique $x\in \text{Int}(U)$ such that $f(x)=y$ . To prove this consider the function $g:U\to \mathbb{R}$ defined by $$g:x\mapsto |y-f(x)|^2 = \sum_{i=1}^n\big(y_i-f_i(x)\big)^2.$$ This function is continuous and therefore has a minimum on $U$ . If $x\in \partial U$ , then, by $(5)$ , we have $g(a) < g(x)$ . Therefore the minimum of $g$ does not occur on the boundary of $U$ . There is a point $x\in \text{Int}(U)$ such that $\partial_jg(x) = 0$ for all $j$ , that is $$\sum_{i=1}^n 2\big(y_i-f_i(x)\big)\partial_jf_i(x)=0 \ \ \ \ \forall j.$$ By (2) the matrix $(\partial_jf_i(x))$ has non-zero determinant. Therefore we must have $y_i - f_i(x) = 0$ for all $i$ , that is $y = f(x)$ . Question 1: why does the equation above and $(2)$ imply $y_i - f_i(x) = 0$ for all $i$ ? This proves the existence of $x$ . Uniqueness follows immediately from $(4)$ .
If $V = (\text{Int}(U)) \cap f^{-1}(W)$ , we have shown that the function $f:V\to W$ has an inverse $f^{-1}: W \to V$ . We can rewrite $(4)$ as $|f^{-1}(y_1)-f^{-1}(y_2)|\le 2|y_1-y_2|$ for $y_1,y_2\in W$ . This shows that $f^{-1}$ is continuous. Only the proof that $f^{-1}$ is differentiable remains. Let $\mu = f'(x)$ . We will show that $f^{-1}$ is differentiable at $y$ with derivative $\mu^{-1}$ . For $x_1\in V$ , we have $$f(x_1) = f(x) + \mu(x_1-x) + \phi(x_1-x)$$ where $$\lim_{x_1\to x} \frac{|\phi(x_1-x)|}{|x_1-x|}=0.$$ Therefore $$\mu^{-1}\big(f(x_1)-f(x)\big) = x_1 - x + \mu^{-1}\big(\phi(x_1-x)\big).$$ Since every $y_1\in W$ is of the form $f(x_1)$ for some $x_1\in V$ , this can be written as $$f^{-1}(y_1) = f^{-1}(y) + \mu^{-1}(y_1-y) - \mu^{-1}\big(\phi(f^{-1}(y)-f^{-1}(y))\big),$$ and it therefore suffices to show that $$\lim_{y_1\to y}\frac{\left|\mu^{-1}\big(\phi(f^{-1}(y_1)-f^{-1}(y))\big)\right|}{|y_1-y|}=0.$$ Therefore it suffices to show that $$\lim_{y_1\to y}\frac{\left|\phi(f^{-1}(y_1)-f^{-1}(y))\right|}{|y_1-y|}=0.$$ Question 2: the trick here -I believe- requires us to divide (and multiply) by $\phi(f^{-1}(y_1)-f^{-1}(y))$ . How do we know said quantity is non-zero? Now $$\frac{\left|\phi(f^{-1}(y_1)-f^{-1}(y))\right|}{|y_1-y|}
= \frac{\left|\phi(f^{-1}(y_1)-f^{-1}(y))\right|}{|f^{-1}(y_1)-f^{-1}(y)|}
\frac{|f^{-1}(y_1)-f^{-1}(y)|}{|y_1-y|}.$$ Since $f^{-1}$ is continuous we have that $f^{-1}(y_1)\to f^{-1}(y)$ as $y_1\to y$ . Therefore the first factor approaches $0$ . Since, by $(6)$ , the second factor is less than $2$ , the product also approaches $0$ . Question 3: in some versions of the theorem, the function $f^{-1}$ is said to be $C^1$ , yet Spivak does not state -nor prove- such fact. How can it be proven?","['proof-explanation', 'multivariable-calculus', 'analysis', 'real-analysis']"
4630409,"Functions $f$ whose values are between $-1$ and $1$, and so do its derivatives.","Is $f(x)=\sin(x)$ or $f(x)=\cos(x)$ the only function on $\Bbb R$ such that $f^{(n)}(\Bbb R) \subset [-1,1]$ for all $n=0,1,\ldots$ and $f^{(k)}(0)=1$ for some $k\in \Bbb N$ ? I found this question for $k=1$ on Codidact . I originally though that it would be easy to find functions satisfying the criteria, but surprisingly all my attempts have failed. Note that for $k=1$ it is easy to see that out of the parametric family $f(x)=a\sin(bx)$ only the function with $a=b=1$ satisfies the criteria. I though that there would be a way modify the sine to make it oscillate progressively slower as going away from $0$ , but I would always get one of the higher-order derivatives out of the $[-1,1]$ range.","['inequality', 'real-analysis', 'functions', 'trigonometry', 'derivatives']"
4630416,"If the product of $n$ positive definite matrices is symmetric, is it also positive definite?","A well-known result for the product of two PD (or PSD) matrices is: Proposition: Let $A$ and $B$ be positive definite (respectively positive semidefinite) Hermitian matrices of the same size. If $D:=AB$ is Hermitian, then $D$ is also positive definite (respectively positive semidefinite). This can be extended to the product of three PD (or PSD) matrices: Is the product of three positive semidefinite matrices positive semidefinite Proposition: Let $A$ , $B$ , and $C$ be positive definite (respectively positive semidefinite) Hermitian matrices of the same size. If $D:=ABC$ is Hermitian, then $D$ is also positive definite (respectively positive semidefinite). Is there any extension of this to the product of $n$ PD (or PSD) matrices?","['positive-semidefinite', 'matrices', 'linear-algebra', 'symmetric-matrices', 'positive-definite']"
4630492,Prove that every manifold is paracompact,"Following Lee's book on smooth manifolds. I'm trying to understand the proof of the theorem (Every manifold is paracompact) and there are some topological claims that i don't understand. I've marked the parts that i don't get with 4 (Why's). Definition: A $(M,T)$ is locally compact topological space if for every $x \in M$ and an open
neighborhood $(U_x \subset M) $ , $(\exists K \subset M)$ compact : $U_x \subset K$ . Definition: Let $(M,T)$ a Hausdorff. We say the subset $U\subset M$ is precompact if $\overline{U}$ is compact. Definition: Let $(M,T)$ is paracompact if every open cover of M admits an open, locally finite refinement Definition: Let $(M,T)$ a Hausdorff and locally compact topological space. A sequence $(K_i)_{i=1}^{\infty}$ of compact subsets of $M $ is called an exhaustion of M by compact sets if: $1)$ $M = \cup_i K_i$ $2)$ $K_i \subset Int(K_{i+1})$ Proposition 1: Let $(M,T)$ be a Hausdorff and locally compact topological space. Then $M$ has a basis of precompact open subsets. Proposition 2: A second-countable, locally compact Hausdorff space admits an
exhaustion by compact sets. Theorem: A $(M,T)$ Second countable, Hausdorff and locally compact topological space is Paracompact . In fact, given a an open cover $X$ of $M$ ,
and any basis $\mathbb{B}$ for the topology of $M$ , there exists a countable, locally finite open
refinement of X consisting of elements of $\mathbb{B}$ . Proof: Given $M$ ; $X$ , and $\mathbb{B}$ as in the hypothesis of the theorem. Let $(K_j)_{j=1}^{\infty}$ be an
exhaustion of $M$ by compact sets (Proposition 2). For every $j$ , Let: $V_j = K_{j+1}-Int(K_j)$ $W_j = Int(K_{j+2})-K_{j-1}$ $K_j = \emptyset$ is $j<1$ Then $V_j$ is compact (Why 1?) and $W_j$ is open (Why 2?) and $V_j \subset W_j$ (Why 3?) For every $(x \in V_j)(\exists X_x \in \mathbb{X}).$ Because $\mathbb{B}$ is a basis, $(\exists B_x \in \mathbb{B})$ s.t. $x \in B_x \subset X_x \cap W_j$ (Why 4?) Update: Continuing the proof: $\{B_x \in \mathbb{B}\}_{x \in V_j}$ is an open cover of $V_j$ . Then for compactness of $V_j$ there exists: $B_j=\{\{B_{x_i} \in \mathbb{B}\}_{x \in V_j}\}_{i=1}^{n}$ finite cover of $V_j$ . and $\cdot$ $\cup_{j=1}^{\infty} B_j$ is a cover of $M$ (Why 5?) $\cdot$ and a refinement of $X$ (Why 6?)","['smooth-manifolds', 'geometric-topology', 'manifolds', 'general-topology', 'differential-topology']"
4630501,"Which ""difference"" functionals only depend on the margins?","Let the real-valued random variables $(X_1,X_2)$ have joint distribution $P_1\times P_2$ . Let the distribution of $X_1-X_2$ be $P_{\mathrm{diff}}$ . Let $\psi(P_{\mathrm{diff}})$ denote a functional of distribution of the difference. The functional is identified by the margins if $$\psi(P_{\mathrm{diff}})=\psi_m(P_1, P_2)$$ for some functional $\psi_m$ that only depends on the marginal distributions $P_1$ and $P_2$ . My question: What is the class of all functionals of the distribution of the difference which are identified by the margins? Clearly by the linearity of expectation, the expectation-of-difference functional $\psi(P_\mathrm{diff}) = \int_{\mathbb{R}^2} x_1 - x_2 d(P_1\times P_2)(x_1,x_2)$ is identified by the margins. I cannot find other examples or prove a general theorem.","['probability-theory', 'functional-analysis', 'statistics']"
4630515,If $f_n$ converges to $f$ in $L^p$ then does $f_n^2$ converge to $f^2$?,"I want to say no, but what is a counterexample?
More generally, does $L^p$ convergence of a sequence of functions imply any sort of convergence (i.e. in some different $L^{p'}$ space) of a function of that sequence of functions?","['lp-spaces', 'functional-analysis']"
4630518,Trouble With The Notion of Cartesian Product of a Family,"I'm reading Paul R. Halmos' book on Naive Set Theory. Though I'm very fond of the book, I believe it could still provide more details during it's explanations; for, I do experience certain ambiguities while reading. I'm familiar with the Cartesian Product of exactly two sets, expressed using the notation $X \times Y$ , if $X$ and $Y$ are the sets considered. Informally, also, I could define the Cartesian Product of exactly to sets to be all the possible ordered pairs with the first coordinate belonging to the first set, and the second coordinate to the second set. Now, when Paul Halmos talks about Cross Products of families (say, chance) the cross product of the family $\{X_i\}_{i \in I} = \{\{a, b, c\}, \{d, e, f\}, \{g, h, i\}\}$ and $I = \{1, 2, 3\}$ , what exactly is the extension of the so-defined set? Does this informally mean the set of all ordered triples (for the size of $I = 3$ ) such that the first coordinate belongs to the first set, the second to the second, and the third to the third? If so, how do we decide which set is to be called the first, second, or third? Just because I have so labeled them them $1, 2$ and $3$ doesn't set-theoretically denote them to be the first, second and third; perhaps, it does intuitively, but there's a leap that's huge, impossible even, between intuition and mathematical precision. I could have also defined the index set to be ${a, b, c}$ , and if it is to rely on alphabetical order, then I could ultimately just make the indexed set itself the indexing set. I seek elucidation on this matter, oh reader who must be more professional than me. Thank you in advance","['elementary-set-theory', 'definition']"
4630537,Change of variable vs Fundamental Theorem of Calculus,"I was thinking about integrals where we should be careful with the domain because we perform a trivial but not 1-1 change of variables. For example, in $$\int_{-1}^2\dfrac{4x^3dx}{1+x^4},$$ the classic change of variables $u=x^4$ is not injective; it can be evaluated simply by using the FTC, whose application here doesn't even see the fact that the $x^4$ is not 1-1. This fact actually freaked me out a bit: most of the examples that I know of for change of variable can be replaced by a direct (albeit uglier) application of the FTC, completely bypassing the bijectivity and other issues associated with change of variable. So, I am wondering whether there really is a gap in my reasoning, that is, whether $$\int_a^b f'(g(x))g'(x)dx=f(g(b))-f(g(a))$$ actually always holds (assuming of course continuous differentiability), no matter how many oscillations $f$ and $g$ have, no matter how many pre-images every $g(x)$ has, etc. In the above example, I would actually like to perform a change of variable $u=g(x),$ then—just then—apply the FTC; but that would require injectivity of $g,$ etc.","['integration', 'analysis', 'real-analysis', 'calculus', 'substitution']"
4630567,"What is the proper term for the ""n"" and ""r"" in the combination/permutation (nCr, nPr) functions?","Just like when we add, the parameters are called ""addends"", and how division has a ""dividend"", ""divisor"", ""quotient"", and ""remainder"", what is the conventional name for the n and r in combinatorics functions nCr and nPr ? Not looking for a definition (eg, the number of items chosen as a subset from all the choices).  Looking for the precise term. Intuitively I might call the r a ""selector"", but I have no idea if that's the right term.","['notation', 'convention', 'combinatorics', 'terminology']"
4630584,Show $\int_0^{\frac1{\sqrt3}} \frac{\cot^{-1}\sqrt{2-x^2}}{1+x^2}dx=\frac{\pi^2}{30}$,"I am unable to show $$\int_0^{\frac1{\sqrt3}} \frac{\cot^{-1}\sqrt{2-x^2}}{1+x^2}dx=\frac{\pi^2}{30} $$ except with a few observations below. 1). Despite the appearance, I do not believe this integral is related to the Ahmed integral or any variations of known solution. 2). The integral is related to a more complex one posted here . However, the answer offered is non conventional, which I could not fully appreciate. 3). Numerically, I could establish that $$\int_0^{\frac1{\sqrt3}} \frac{\cot^{-1}\sqrt{2-x^2}}{1+x^2}dx
=2 \int_0^{\frac1{\sqrt3}} \frac{\cot^{-1}\sqrt5 x}{1+x^2}dx 
- \int_0^{\sqrt3} \frac{\cot^{-1}\sqrt5 x}{1+x^2}dx  $$ where the RHS can be worked out with an elaborate procedure. So, I would like to see a traditional solution without special functions, or, at least, establish the integral relationship above analytically.","['integration', 'trigonometric-integrals', 'definite-integrals']"
4630594,No invariant proper subspaces imply irreducibility,"Let $\mathscr{H}$ be a complex Hilbert space and $\mathscr{B}(\mathscr{H})$ be the Banach space of bounded linear operators on $\mathscr{H}$ . This is a $C^{*}$ -algebra and we consider a $*$ -subalgebra $\mathscr{M}$ of $\mathscr{B}(\mathscr{H})$ . Define: $$\mathscr{M}' := \{T \in \mathscr{B}(\mathscr{H}): \mbox{$TS = ST$ holds for every $S \in \mathscr{M}$}\}$$ Moreover, if $\mathscr{S}$ is a vector subspace of $\mathscr{H}$ , it is said to be invariant with respect to $\mathscr{M}$ if $S\psi \in \mathscr{S}$ holds for every $\psi \in \mathscr{S}$ and every $S \in \mathscr{M}$ . I want to prove the following: if there are no invariant subspaces of $\mathscr{H}$ with respect to $\mathscr{M}$ , then $\mathscr{M}' = \{\alpha 1: \alpha \in \mathbb{C}\}$ , where $1$ is the identity map on $\mathscr{B}(\mathscr{H})$ . I have no idea how to start it. Can someone give me any help?","['c-star-algebras', 'abstract-algebra', 'functional-analysis']"
4630629,Turn a real number (with a complex closed form) into its trigonometric form,"This post outlines 'fake' complex numbers (real numbers with complex closed form that usually come from the roots of unfactorable cubics (the example I need right now), or they can come from things like $i^i = e^{-\frac{\pi}{2}}$ ), in his own answer he gives the example: $\sqrt[3]{1+i \sqrt{7}}+\sqrt[3]{1-i \sqrt{7}}$ , which from first glance doesn't look like it could simplified any futher, then states: Using Euler's formula we can find a trigonometric representation of this number. $2 \sqrt{2} \cos{\left(\frac{\tan^{-1}{\left(\sqrt{7}\right)}}{3}\right)}$ Which when put into WolframAlpha is confirmed, they are both approximately equal to $2.602$ How did he do that? Euler's formula, $e^{ix} = \cos(x) + i\sin(x)$ doesn't seem relevant, the example has no clear $a+ib$ form (and it shouldn't because it is a real number, $b$ will just be $0$ , so a will just be the answer anyway) and there is an inverse tangent present in the expression, also covered by Euler's formula somehow? How does he just go from to $A$ to $B$ like it's common knowledge? Question applies to the example but it would be nice to have further advice on how to turn other fake complex numbers into real forms like this without WolframAlpha.","['real-numbers', 'trigonometry', 'closed-form', 'complex-numbers']"
4630650,Solve for angle $x$ in a scalene triangle,"This problem was posted by a friend of mine on Instagram, namely, @dare2solve. Here's the problem: Given thus scalene triangle, the goal is to solve for the missing angle $x$ in this figure. This problem was actually posted a few months back (in September) on this site, however it was deleted due to the original asker showing no approach of their own. I'm going to share my own approach as a solution, please let me know if there are any errors in it, and please share your own solutions too. Particularly, is there a way to use Ceva's theorem here to solve for the angles?","['euclidean-geometry', 'geometry', 'solution-verification', 'triangles', 'trigonometry']"
4630801,20 sided die and 12 sided die.,"I have a $12$ sided die and you have a $20$ sided die. We each get two rolls and we can each chose to stop rolling on either one of the rolls, taking the number on that roll. Whoever has the higher number wins, with the tie going to the person with the $12$ sided die. What is the probability that the person with the $20$ sided die wins this game? I am able to easily calculate the Probability incase each player was allowed only $1$ roll by doing:
Let $A$ be the person with the $20$ sided die $$P(A \text{ win}) = P(A \text{ win} | \text{roll} > 12)\cdot P(\text{roll} > 12) + P(A \text{ win} | \text{roll} \leq 12) \cdot P(\text{roll} \leq12)$$ I found $P(A \text{ win} | \text{roll} \leq 12) = \frac{11}{24}$ since both have an equal chance of winning except in case of a tie where $B$ wins which happens with Probability of $\frac{1}{12}$ . $$P(A \text{ win}) = 0.675$$ I'm however, stuck on how to approach it when both players are allowed an optional reroll.","['probability-theory', 'probability']"
4630823,"Contrapositive, Converse and Inverse of statements with multiple quantifiers","My textbook only touched on negation of statements with multiple quantifiers, and I would like to know: For a statement like $\forall M>0, \exists \delta > 0$ such that if $0 < |x-a| < \delta$ then $|f(x)| > M,$ is its contrapositive $\forall M>0, \exists \delta > 0$ such that if ~ $(|f(x)| > M)$ then  ~ $(0 < |x-a| < \delta)\quad?$ Do the converse and inverse similarly just affect the if-else?","['predicate-logic', 'discrete-mathematics']"
4630825,Continuity of a set-valued function,"Let $f:\mathbb{R}^{n}\rightrightarrows \mathbb{R}^{m}$ be a set-valued
function defined by \begin{equation*}
f\left( x\right) =\left\{ y\in \mathbb{R}^{m}:g\left( x\right) +h\left(
x\right) ^{T}y\leq 0\right\} \text{,}
\end{equation*} where $g:\mathbb{R}^{n}\rightarrow \mathbb{R}$ , $h:\mathbb{R}^{n}\rightarrow 
\mathbb{R}^{m}$ are continuous functions. How do you show that $f$ is
continuous in $\mathbb{R}^{n}\diagdown\left\{  0\right\}  $ ?","['real-analysis', 'continuity', 'functions', 'set-valued-analysis', 'optimization']"
4630831,Inequality on summation,"Let $\mathfrak{m},\mathfrak{n}\subset\mathbb{N}$ and we define $|\mathfrak{n}|=n_1+n_2+\ldots$ , for $\mathfrak{n}=\{n_1,n_2,\ldots\}$ . I am wondering is the following inequality true: $$
\sum_{\mathfrak{m}\in\mathfrak{P}(\mathfrak{n})\setminus\{\mathfrak{n}\}}|\mathfrak{m}|! \leq |\mathfrak{n}|!.
$$ Here, $\mathfrak{P}(\mathfrak{n})$ denotes the power set of $\mathfrak{n}$ . I have tried induction but it does not work for me. Can anyone clarify on this? Thank you!","['inequality', 'combinatorics', 'summation']"
4630921,Stationary distribution of gradient-biased random walk,"I'm looking for a result that I suspect should be fairly standard and well-known to probabilists and statistical physicists (and perhaps numerical/financial analysts).  The result in question should say something like this : Guess: Let $\beta>0$ real and let $V \colon \mathbb{R}^n \to \mathbb{R}$ be a smooth function such that $\int\exp(-\beta V)$ converges.  Let $\varepsilon>0$ (“small”).  Now consider the following random process with $x_0$ arbitrary: $$u_i \leftarrow \operatorname{Normal}(0,1)$$ $$x_{i+1} = x_i + \varepsilon u_i - \frac{\beta}{2}\, \varepsilon^2\, \nabla V(x_i)$$ (in other words, $x_i$ is a random walk with normal steps of variance $\varepsilon^2$ except that we also add a gradient descent term along $V$ ). Then $x_i$ converges in distribution (when $i\to+\infty$ ) to a law having density $g_\varepsilon$ , which itself converges to $g \propto \exp(-\beta V)$ when $\varepsilon \to 0$ . (That is, $g = \frac{\exp(-\beta V)}{\int\exp(-\beta V)}$ , and I expect $g_\varepsilon$ to be some kind of convolution of $g$ by a normal distribution.) So the point is, this “random walk plus gradient descent” process should have a stationary distribution, which for $\varepsilon>0$ small should be well approximated by (something proportional to) $\exp(-\beta V)$ .  Intuitively, the greater $V$ , the less time the process spends in the region in question, and exponentially so for larger $V$ . (My goal is to construct a random process whose stationary distribution is proportional to some prescribed function, here represented by $\exp(-\beta V)$ , and which looks locally like a random walk.  I am aware of the Metropolis-Hastings algorithm, but the fact that it can leave the point unchanged is problematic to me.  The above process is supposed to be analogous in spirit to M-H, however, and is perhaps describable as some kind of limit of it when taking a large number of very small steps.) Of course $\beta$ plays no role in what I just wrote, it's just $\beta V$ that appears, but I believe leaving $\beta$ in place makes the result more understandable and perhaps more standard in its notations (in physicists' terms, $V$ should be a kind of “potential” and $\beta$ an “inverse temperature”). I also expect a continuous version of the result to hold, and it is also of interest to, me, for a SDE of the form $dX_t = dB_t - \frac{\beta}{2}\, \nabla V(X_t)$ , as a limit of the above discrete process. So, question : is the above “guess”, or at least some reasonable approximation to it, correct (and a standard result)?  What are the relevant keywords that I could search for to know more about this?  And if my guess is completely wrong, is there something vaguely similar that would give a random walk which converges to a distribution proportional to a given $\exp(-\beta V)$ ?","['stochastic-analysis', 'stochastic-processes', 'gradient-flows', 'probability-theory']"
4630954,Find the minimum of this radical expression,"For $x\in[0,6]$ , find the minimum of $m=x+\dfrac{96-16x}{3\sqrt{x^2+16}}$ . I used derivatives and found that the $x$ which minimizes $m$ is the positive root of
\[9x^6+432x^4-2304x^2-49152x-28672=0.\]
The approximate value is $x\approx4.71183$ , $m\approx5.82339$ . But I want the exact value. Since the polynomial function of degree $6$ has no formula to solve its roots, we need another method. I can think of some alternative methods like using known inequalities or substituting to get rid of the square root, but I can't find any approaches using these methods currently.",['algebra-precalculus']
4630968,How to solve $\int \frac{f(x)}{f^{\prime}(x)} dx$,"Let $f:\mathbb{R}\to\mathbb{R}$ be of class $C^{\infty}$ with $f^{\prime}\not\equiv0$ . There exists a formula to solve the integral $$\int \frac{f(x)}{f^{\prime}(x)} dx?$$ Since I know that is $$\int \frac{f^{\prime}(x)}{f(x)} dx=\log(|f(x)|) +c,$$ I was wondering how to act when in the first case. I hope someone could help. Thank you in advance.","['integration', 'calculus', 'analysis', 'real-analysis']"
4630970,Is there a space that does not include or need a coordinate system?,"A coordinate system (such as cartesian and polar coordinate systems) is expected in a Euclidean space. However, is there a space that does not include or need a coordinate system?
I read that a Vector space does not need a coordinate system although a coordinate system can be defined if needed. Is there any other space that does not need/include a coordinate system? Is there any space that does not accept a coordinate system?","['banach-spaces', 'coordinate-systems', 'vector-spaces', 'hilbert-spaces', 'linear-algebra']"
4631006,Convergence of sum with binomial coefficient,"I found this exercise in a book and I don't know how to start: With the help of probabilistic methods, prove that $$
\lim_{n\rightarrow\infty} \frac{1}{4^n} \sum_{k=0}^n  \binom{2n}{k} = 1/2
$$ One cannot use the binomial theorem because the sum just goes to $n$ and not to $2n$ and besides that, am I supposed to use one of the limit theorems in stochastics? If so, which one and how can I approach this exercise.","['convergence-divergence', 'probability']"
4631014,Range of Trigonometric function having square root,Finding range of function $\displaystyle f(x)=\cos(x)\sin(x)+\cos(x)\sqrt{\sin^2(x)+\sin^2(\alpha)}$ I have use Algebric inequality $\displaystyle -(a^2+b^2)\leq 2ab\leq (a^2+b^2)$ $\displaystyle (\cos^2(x)+\sin^2(x))\leq 2\cos(x)\sin(x)\leq \cos^2(x)+\sin^2(x)\cdots (1)$ And $\displaystyle -[\cos^2(x)+\sin^2(x)+\sin^2(\alpha)]\leq 2\cos(x)\sqrt{\sin^2(x)+\sin^2(\alpha)}\leq [\cos^2(x)+\sin^2(x)+\sin^2(\alpha)]\cdots (2)$ Adding $(1)$ and $(2)$ $\displaystyle -(1+1+\sin^2(\alpha))\leq 2\cos(x)[\sin(x)+\sqrt{\sin^2(x)+\sin^2(\alpha)}]\leq (1+1+\sin^2(\alpha))$ $\displaystyle -\bigg(1+\frac{\sin^2(\alpha)}{2}\bigg)\leq f(x)\leq \bigg(1+\frac{\sin^2(\alpha)}{2}\bigg)$ I did not know where my try is wrong. Please have a look on it But actual answer is $\displaystyle -\sqrt{1+\sin^2(\alpha)}\leq f(x)\leq \sqrt{1+\sin^2(\alpha)}$,['trigonometry']
4631115,Show that $\frac{S_n}{n} \to 0$ almost surely,"For the below question: Suppose that $X_i$ are independent random variables such that $\mathbb{E}(X_i)=0$ and $\mathbb{E}(X_i^2) \leq c \sqrt{i}$ for some fixed but positive constant $c$ . Show that $\displaystyle{\frac{S_n}{n} = \frac{X_1 + \cdots X_n}{n}}$ converges to $0$ a.s. My first thought was to use Chebychev's Inequality to bound the probability, then apply Borel-Cantelli Lemma. However, it seems in this case that I cannot find a convergent series to bound the probability. Any help would be thankful.","['measure-theory', 'probability-theory', 'probability', 'real-analysis']"
4631124,trouble with proving/disproving $\lvert z-x \rvert$=$\sqrt{x+y^2}$ is manifold,"Let $M$ be set of points $(x,y,z) \in \mathbb{R}^3$ such that $\lvert z-x \rvert = \sqrt{x+y^2}$ . Is $M$ two dimensional manifold? I have two different solutions to this problem and in one of them the result is that $\lvert z-x \rvert = \sqrt{x+y^2}$ is manifold but in the second I got that it is not manifold. Clearly one of solution is wrong, I do not know which and I do not know why. First solution. Raise both sides of equation to second power and consider function $$
F(x,y,z) = z^2 - 2xz + x^2 - x - y^2
$$ Gradient of $F$ is: $$
DF(x,y,z) = (-2z+2x-1,\, -2y,\, 2z-2x), 
$$ which is nonzero for all $(x.y,z)$ so $DF(x,y,z)$ is epimorphism in every $(x,y,z)$ and therefore $M$ is two dimensional manifold. (This is by theorem I learned in my analysis class.) Second solution. Fix $x=0$ , we get $\lvert z \rvert = \lvert y \rvert$ so none of $y$ and $z$ are functions of two other variables. Fix $z=0$ , we get $x^2-x = y^2$ and when I graph it on Desmos, I see that for many $x$ 's there are two $y$ 's so $x$ is not a function of $y$ and $z$ . So there are points in $M$ where $M$ is not a graph of a function so $M$ is not manifold.","['real-analysis', 'multivariable-calculus', 'calculus', 'manifolds', 'differential-geometry']"
4631129,Then number of ways of arranging the letters of word ALGEBRA in which either vowels or consonants but not both appear in alphabetical order,"The number of ways of arranging the letters of word ALGEBRA in which either vowels or consonants but not both appear in alphabetical order My attempt: Let $A$ is number of ways in which consonants are in alphabetical order i.e., $\dfrac{7!}{4!\cdot 2!}=105$ . Let $B$ is number of ways in which vowels are in alphabetical order i.e., $\dfrac{7!}{3!}=840$ Let $C$ is number of ways in which both vowels and consonants are in alphabetical order i.e., $\dfrac{7!}{4!\cdot 3!}=35$ So obtained answer by me is $105+840-35=910$ But given answer is $943$ . I want to know where am I going wrong.","['permutations', 'combinations', 'combinatorics']"
4631155,Finding the height of a cone in terms of $R$ and $θ$,"The problem is: A right circular cone is made from a circular piece of paper of radius $R$ by cutting out a sector of angle $\theta$ radians and gluing the cut edges of the remaining piece together. Find the height $h$ of the cone in terms of $R$ and $\theta$ . In the problem before this one, you solve for $r$ , which is the base of the cone. $$
r = R\frac{2\pi - \theta}{2\pi}
$$ The method I've gone with is seeing that a right triangle is made from $h$ , $r$ and $R$ , Use the Pythagorean Theorem to find the value for $h$ . Initially, it looks like this: $$
h^2 = R^2 - \left( R\,\frac{2\pi - \theta}{2\pi} \right)^2
$$ When I follow through with the formula, I come to this step, and can't make any further progress: $$
h = R + \left(-R\, \frac{-2\pi + \sqrt{4\pi\theta - \theta^2}}{2\pi} \right)
$$ The answer given is: $$
h = R\, \frac{\sqrt{4\pi\theta - \theta^2}}{2π}
$$ Am I doing something wrong in applying the Pythagorean formula, or just using it incorrectly? I appreciate any advice or help given.","['trigonometry', 'euclidean-geometry', 'algebra-precalculus', 'geometry']"
4631170,Flux through surface,"Can someone help/correct me in my thinking when working calculating the flux through a surface. Given is the following example: When a vector field is described by: $$
\vec{E} = xz\,\hat{\imath} - yz\,\hat{\jmath} + (x^2 + y^2)\,\hat{k}$$ And a circular disk through which we want to calculate the flux is described by: $$
D \rightarrow\; z=0,\, x^2+y^2 \leq a^2
$$ I am aware that in order to calculate the flux through the surface we need a vector perpendicular to the surface of the disk to evaluate the integral. Since $z=0$ , a unit normal vector is given by $\hat{n} = 0\,\hat{\imath} + 0\,\hat{\jmath} + 1\,\hat{k}$ this could be used. Right? We are taught that another way to determine a vector perpendicular to the surface $D$ is by parametrizing the surface $D$ , as below for example, and then determining $\frac{\partial r}{\partial \rho} \times \frac{\partial r}{\partial \theta}$ . $$
D\rightarrow\; 
r(\rho, \theta) 
= \rho \cos(\theta)\,\hat{\imath} + \rho\sin(\theta)\,\hat{\jmath} + 0\,\hat{k}
$$ with $\rho \in [0, a] \land \theta \in [0, 2\pi]$ . Is it correct that the flux is then given by (question mark indicating I'm not sure about my understanding, and I might have gone wrong somewhere way earlier): $$
\iint_D \vec{B} \cdot d \vec{S} \;\overset{?}{=}\, \iint_D \vec{B}(\vec{r}(\rho, \theta)) \cdot 
\biggl( \frac{\partial r}{\partial \rho} \times \frac{\partial r}{\partial \theta} \biggr) \, dA
$$ The length of this normal vector does matter right. So how has this been taken into account for in the integral with the cross product? Is it because you substitute the parameterization of the surface into the vector field?","['multivariable-calculus', 'calculus', 'vector-analysis', 'polar-coordinates']"
4631246,Is it possible to convert an Ellipse into multiple circular arcs?,I would like to know if it is possible to convert/calculate the minimum number of circular arcs *1 which can make up an ellipse *2 . Im not sure if this is even possible but seems like it should be. Bellow is an example of what I expect to do.  Also a small margin of error would be fine. Edits Updated title and body arcs to circular arcs Changed ellipsis to ellipse,"['trigonometry', 'geometry']"
4631270,What is this combinatorics problem involving polynomials? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question I am trying to find when a certain descriptor for a polynomial equals zero. Given a set of discrete complex points $B \subset \mathbb{C}$ , a map $p(b):B \to \mathbb{N}$ , and a set $A \subseteq B$ such that $\forall a \in A, p(a) = 1$ . I am asking when the following formula is equal to zero: $$0 =\sum_{a \in A} \prod_{\substack{b \in B, \\ a \neq b}} (a - b)^{p(b)}$$ I am fairly certain of a couple of things. For example, if $B = A$ and $|B| > 1$ , then the equation is zero, if $A = \emptyset$ , it is also obviously zero. The answer cannot be zero if $|B| = 2$ and $|A| = 1$ . I am thinking that the formula depends on the size of $A$ , the size of $B$ and the values of $p(b) \mod 2$ , which makes me believe this is a combinatorics problem. It looks very fun if anyone wants a try, but I am feeling stumped for the moment.","['combinatorics', 'analysis']"
4631335,Transforming real matrix to integer matrix while preserving row and column sums,"Let $A$ be an $m \times n$ matrix with real-valued entries such that the rows and columns sum up to integers. I wish to show that there exists a matrix $B$ , with integer entries, such that $A$ and $B$ share the same row and column sums, and $|A_{i,j} - B_{i,j}| < 1$ for all $i,j$ . I tried various methods but none came to fruition. For instance, I tried to induct on $m + n$ . The case where $m = 1$ or $n = 1$ is clearly trivial. The $2 \times 2$ case is also simple: We must have $A = \begin{pmatrix} a & r_1 - a \\ c_1 - a & r_2 - c_1 + a \end{pmatrix}$ , where $r_1,c_1,r_2 \in \mathbb{Z}$ are integers. Then $B = \begin{pmatrix} \lfloor{a}\rfloor & r_1 - \lfloor{a}\rfloor \\ c_1 - \lfloor{a}\rfloor & r_2 - c_1 + \lfloor{a}\rfloor  \end{pmatrix}$ does the trick. Given an $(m + 1) \times n$ matrix $A$ , I considered: $$
A_{i,j}' := 
\begin{cases}
A_{i,j}, &\text{if $i < m$} \\
A_{m,j} + A_{m+1,j}, &\text{if $i = m$}
\end{cases}
$$ Then $A'$ is an $m \times n$ matrix, so by induction hypothesis, we may approximate it with some integer-valued $m \times n$ matrix $B$ . However, I'm not sure how to proceed from there onwards.","['matrices', 'linear-algebra', 'combinatorics']"
4631343,limit of perturbed rank deficient matrix as perturbation goes to zero,"Given a $p \times p$ matrix $X^TX$ such that rank $(X^TX) = r<p$ . How can I evaluate the following limit: $$
\lim_{\lambda \to 0} (X^TX + \lambda I)^{-1}.
$$","['matrices', 'limits', 'statistics', 'linear-algebra']"
4631348,Cremona transformation defined by 7 point correspondences,"I believe it is true that given 7 generic point correspondences $x_i \mapsto y_i$ it should be possible to construct a (unique) quadratic Cremona transformation $f$ such that $f(x_i)=y_i$ . This would make sense because a quadratic Cremona transformation should have 14 degrees of freedom (12 for the 6 exceptional points and two more for homography [essentially scalars $a_1,a_2,a_3$ that you could throw in front of the standard quadratic Cremona $(x_1:x_2:x_3) \mapsto (a_1x_2x_3:a_2x_1x_3:a_3x_1x_2)$ ] without changing the exceptional points) and it works for the examples I have tested, but I haven't been able to find a proof of it anywhere or prove it myself. Does anyone have a proof of this or is anyone able to recommend a text that contains this result? Thanks!","['algebraic-geometry', 'projective-geometry']"
4631367,During constrained optimization with inequalities why must the gradient of the objective be in the same direction as the gradient of the constraint?,"I have been reading about constrained optimization and understood when there are just equality constraints but am having trouble understanding when there are inequality constraints. I was initially following KhanAcademy's multivariable calculus course but wanted to expand from the simple Lagrange Multipliers treatment so I began looking at pdfs online from different college courses. I am thinking of things in 2 variables for now and let's just assume one inequality constraint for simplicity. I understand that during an optimization of the form: $$
\begin{gathered}
\max _{x, y} f\left(x, y\right) \text { subject to : } \\
g\left(x, y\right) \leq b .
\end{gathered}
$$ that there are two possible cases. One is where the candidate point is in the interior of the boundary (i.e. $g(x, y) \lt b$ ) and that case makes sense to me that we are basically looking for unconstrainted local extrema (maxima here) by checking where $\nabla f(x, y)=\mathbf{0}$ . For the other case, the candidate points are on the boundary. I understand that for candidate point $(x^*, y^*)$ , $\nabla g(x^*, y^*)$ will point outwards from the boundary and that $\nabla f(x^*, y^*)$ will be parallel to it. All the resources I am looking at say that when looking for maxima that you only consider the point if the gradient $\nabla f(x^*, y^*)$ points outward/in the same direction as $\nabla g(x^*, y^*)$ . The explanation is that if $\nabla f(x^*, y^*)$ pointed inward then we know that there are some feasible points such that when $f$ is evaluated there that it will be greater than $f(x^*, y^*)$ . Let's say we find such a point $(x^*, y^*)$ where the $\nabla f(x^*, y^*)$ is pointed inward. I agree with the above reasoning that there will be some feasible points in the interior that will have higher values of $f$ . Let's say one of these feasible points in the interior with a higher value of $f$ is $(x^1, y^1)$ . What I don't follow is how we can safely ignore the point $(x^*, y^*)$ without being 100% sure that the local unconstrained maximum case above (so setting $\nabla f(x, y)=\mathbf{0}$ ) will find the point $(x^1, y^1)$ . Couldn't it be the case that although $f$ 'dips' down on the interior of the boundary near $(x^*, y^*)$ that it will rise back up in some way that it won't be a local maximum and therefore the KKT conditions won't find it? In that case wouldn't we want to consider $(x^*, y^*)$ since at the very least we know it is an extrema of the boundary? Apologies in advance if any of the math above feels hand wavey. I am trying to get an intuitive sense of things rather than something super rigorous. My basic question is that if we know there is some point in the interior ( $g(x, y) \lt b$ ) such that $f$ will be greater than $f(x^*, y^*)$ are we guaranteed (or guaranteed under some conditions) to find it by just looking for unconstrained maxima? There might be some theorem I am missing here that'd help. In one dimension I can picture that if $f$ decreases/stays equal within the feasible region (after initially increasing right at the boundary near $(x^*, y^*)$ ) that we immediately have a critical point by Rolle's theorem. And that if $f$ only increases then the other end of the boundary will have the max point. But I can't seem to make that same leap for 2+ dimensions. If it helps I am thinking of a set up like the one in the picture below where the red region is the set of feasible points. Thanks in advance for the help!","['karush-kuhn-tucker', 'lagrange-multiplier', 'real-analysis', 'multivariable-calculus', 'optimization']"
4631375,Can we evaluate $\int \frac{1}{\sin ^ {2n+1} x+\cos ^ {2n+1} x} d x?$,"After investigating the integral $$\boxed{\quad  \int \frac{1}{\sin ^5 x+\cos ^5x} d x \\=\frac{4}{5}\left[-\frac{1}{\sqrt{2}} \tanh ^{-1}\left(\frac{\sin x-\cos x}{\sqrt{2}}\right)+\frac{1}{\sqrt{\sqrt 5-2}} \tan ^{-1}\left(\frac{\sin x-\cos x}{\sqrt{\sqrt{5}-2}}\right)\\ \qquad\ -\frac{1}{\sqrt{\sqrt{5}+2}} \tanh ^{-1}\left(\frac{\sin x-\cos x}{\sqrt{\sqrt{5}+2}}\right)\right]+C}$$ in my post , I try to evaluate $$\int \frac{1}{\sin ^ {7} x+\cos ^ {7} x} d x$$ using the factorisation \begin{aligned}
\sin ^{7} x+\cos ^7 x&=\left(\sin ^2 x+\cos ^2 x\right)\left(\sin ^5 x+\cos ^5 x\right)-\sin ^2 x \cos ^2 x\left(\sin ^3 x+\cos ^3 x\right) \\
&=\left(\sin ^2 x+\cos ^2 x\right)\left(\sin ^3 x+\cos ^3 x\right)   -\sin ^2 x \cos ^2 x(\sin x+\cos x) \\
 &\quad  -\sin ^2 x \cos ^2 x\left(\sin ^3 x+\cos ^3 x\right) \\
&=(\sin x+\cos x)\left(1-\sin x \cos x-2 \sin ^2 x \cos ^2 x+\sin^3x\cos^3x\right)
\end{aligned} Let $t=\sin x-\cos x$ , then $d t=(\cos x+\sin x) d x$ and $\sin x\cos x= \frac{1-t^2}{2}$ and yields $$
\begin{aligned}
I 
=\int \frac{8}{\left(t^2-2\right)\left(t^6+t^4-9 t^2-1\right)} d t
\end{aligned}
$$ where I was stuck in the last integrand with power $6$ .  Your help and comments are highly appreciated. My Question: Is it difficult go further with $$\int \frac{1}{\sin ^ {2n+1} x+\cos ^ {2n+1} x} d x?$$ where $n\geq 3.$","['integration', 'calculus', 'trigonometric-integrals', 'indefinite-integrals', 'trigonometry']"
4631389,Can a constant function be called a function in $x$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 28 days ago . Improve this question Suppose I have $y=c$ , where $c$ is a constant. We can graph it on the coordinate axes too. Now, which statement is correct to say, $y$ is a constant function of $x$ . $y$ is not a function of $x$ . I think that the first statement is correct as if 2nd were correct then vertical line test would have failed. But, I want to confirm if I am thinking correctly or not.","['definition', 'functions']"
4631444,Deriving marginal distribution from a joint distribution,"Let the joint distribution of a random vector be $$f(x,y)=\begin{cases}1,\enspace 0<x<1, x<y<x+1&\\0, \enspace {\rm otherwise}\end{cases}$$ The marginal pdf of $X$ is $Uni(0,1)$ , but what is the distribution of $Y$ ? By definition it should be $$f_Y(y)=\int_0^1 f(x,y){\rm d}x=1$$ but this is not correct. Integration is done over the bounds of $X$ , and when $x\in(0,1)$ then $f(x,y)=1$ . Is this not correct?","['integration', 'statistics', 'marginal-distribution']"
4631508,How to find all possible connections between immediate lattice points in a $n\times n$ square?,"I am not a mathematician so my terminology might be not correct, please ignore it. I want to calculate all possible connections between immediate lattice points in a $n\times n$ square. Here lattice points are all points $(x, y)$ that satisfies $x \in \Bbb{N}, y \in \Bbb{N}$ and $x \le n, y \le n \ (n \in \Bbb{N}, n \ge 2)$ , the square is in the first quadrant and its lower left corner is the origin, and two lattice points are immediate if they can be connected without the connection passing through another lattice point. I have thoroughly investigated cases where $n \in \{2, 3, 4\}$ , and found the following: When $n = 2$ , it is this: It is not interesting, there are $4$ points and $6$ connections, $4$ connections of length $1$ and $2$ connections of length $\sqrt{2}$ , and every connection is valid: ${4 \choose 2} = \frac{4 \cdot 3}{2} = 6 $ . When $n > 2$ , things get interesting. I have manually created the graphs using GeoGebra and identified different types of lattices and colored the connections accordingly for convenience. The above picture shows when $n = 3$ , there are $9$ points and ${9 \choose 2} = \frac{9 \cdot 8}{2} = 36$ , however only $28$ connections are between immediate points, as $8$ connections overlap other connections. More specifically, there are $3$ types of points, $4$ corner points, $4$ midline points and $1$ center, and connections of lengths $\{1, \sqrt{2}, \sqrt{5}\}$ with 12 connections of length 1, 8 connections of length $\sqrt{2}$ and 8 connections of length $\sqrt{5}$ . Opposite points cannot be connected directly. When $n = 4$ : There are $3$ types of points when $n = 4$ , there are $4$ corners, $8$ midline points and $4$ inner points. There are ${16 \choose 2} = \frac{16 \cdot 15}{2} = 120$ total possible connections, but only $86$ connections are valid. In the above picture, there are $24$ connections of length $1$ , $18$ of $\sqrt{2}$ , $24$ of $\sqrt{5}$ , $12$ of $\sqrt{10}$ and $8$ of $\sqrt{13}$ . It is easy to see corner points cannot connect to each other, points on edges can only connect to $3$ other points horizontally and vertically, while points inside the square can connect to $4$ other points horizontally and vertically. Each red point connects to $2$ blue points on the edges that intersect at it and cannot connect to the other $2$ blue points on those $2$ edges. Each red points can connect to the other $4$ blue points that aren't colinear with it. So each red point can form $6$ magenta connections for a total of $4 \cdot 6 = 24$ magenta connections. Each red point can connect to $3$ green points and not the green point opposite it so there are $12$ yellow connections. Each blue point can connect to every other blue point except the $2$ at $3$ indices offset in either direction (clockwise or counterclockwise), so there are $\frac{8 \cdot (7 - 2)}{2} = 20$ blue connections. Each green point can connect to all $4$ blue points that aren't colinear with it but only $2$ blue points from the $4$ colinear points. So there are $4 \cdot 6 = 24$ cyan connections. Lastly the $4$ green points can connect to each other, so there are $6$ green connections, and order $4$ contains order $2$ . For higher orders, the number of total connections blows up quickly so I couldn't do it manually, for example the number of total connections of order $5$ is ${25 \choose 2} = 300$ , but I did write a simple Python program to visualize the linear combinations with all the duplicates: import matplotlib.pyplot as plt
from itertools import combinations, product
from matplotlib.collections import LineCollection

def graph(n):
    points = list(product(range(n), repeat=2))
    segments = list(combinations(points, r=2))

    fig, ax = plt.subplots()
    ax.add_collection(LineCollection(segments))
    ax.set_axis_off()
    plt.axis('scaled')
    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
    plt.show() And below are pictures of orders $5$ and $6$ : As you can see, order $5$ contains order $3$ and order $6$ contains order $4$ . So all odd orders are subsets of higher odd orders and even orders are subsets of higher even orders. I found that .gbb file is .zip file renamed, so I extracted the geogebra.xml file and wrote this Python program to double-check my calculations: from collections import Counter
from lxml import etree
from pathlib import Path

tree = etree.fromstring(Path(""C:/Users/Estranger/Desktop/four/geogebra.xml"").read_bytes())

points = tree.findall('.//element[@type=""point""]')

coords = dict()

for point in points:
    key = point.attrib['label']
    coord = point.find('./coords')
    x = float(coord.attrib['x'])
    y = float(coord.attrib['y'])
    coords[key] = (x, y)

def dist(a, b):
    return ((a[0] - b[0])**2 + (a[1] - b[1])**2)**0.5

segments = tree.findall('.//command[@name=""Segment""]/input')

lengths = Counter()

for segment in segments:
    a = coords.get(segment.attrib['a0'])
    b = coords.get(segment.attrib['a1'])
    lengths[dist(a, b)] += 1

sorted(lengths.items()) Output: [(1.0, 24),
 (1.4142135623730951, 18),
 (2.23606797749979, 24),
 (3.1622776601683795, 12),
 (3.605551275463989, 8)] Unfortunately this is as far as I can go, I lack the mathematical skills to analyze my findings further, I want to know, given order $n$ and two points $(x_0, y_0)$ and $(x_1, y_1)$ , how to determine whether they are immediately connected or not?",['geometry']
4631515,Evaluate $\sum_{r=1}^{\infty} \dfrac{r^2 - 1}{r^4 + r^2 + 1}$,"I was only able to observe that: $\dfrac{r^2 - 1}{r^4 + r^2 + 1} = \dfrac{r^2 - 1}{(r^2 + r + 1)(r^2 - r + 1)}$ This hints at telescoping, but I would need an $r$ term in the numerator. The original question was Evaluate $\sum_{r=1}^{\infty} \dfrac{r^3 + (r^2 + 1)^2}{(r^4 + r^2 + 1)(r^2 + r)}$ I was able to simplify it to the following: $\dfrac{r^3 + (r^2 + 1)^2}{(r^4 + r^2 + 1)(r^2 + r)} = \dfrac{(r^4 + r^3 - r^2 - r)}{(r^4 + r^2 + 1)(r^2 + r)} + \dfrac{3r^2+r+1}{\{(r+1)(r^2 + r + 1)\}\{r(r^2 - r + 1)\}} = \dfrac{r^2 - 1}{r^4 + r^2 + 1} + \left[\dfrac{1}{r(r^2 - r + 1)} - \dfrac{1}{(r+1)(r^2 + r + 1)}\right]$ The second term can be evaluated using telescoping, and the first term is what this post is asking for. Any other ways of solving the original question are also welcome.","['telescopic-series', 'sequences-and-series']"
4631539,How to interpret the square matrix M'M?,"Say, I have a matrix M of shape n x d . M'M results in a square matrix of shape d x d . How can I interpret M'M ? import numpy as np
M = np.array([[1, 2], [2, 1], [3, 4], [4, 3]])
prod = M.T.dot(M)","['matrices', 'inner-products', 'intuition', 'matrix-decomposition']"
4631555,Vanishing of a sum involving second minors of a unitary matrix,"Let $U$ be a $4 \times 4$ unitary matrix and let $M_{\{i_1i_2\}\{j_1j_2\}} = U_{i_1j_1}U_{i_2j_2}-U_{i_1j_2}U_{i_2j_1}$ denote its second minors (where $i_1<i_2$ and $j_1<j_2$ ). If $I$ and $J$ are 2-element subsets of $\{1,2,3,4\}$ , then by the Cauchy–Binet formula we know that $$\sum_K \bar{M}_{KI}M_{KJ} = \begin{cases} 1 & \textrm{if } I = J \\ 0 & \textrm{if } I\neq J \end{cases}$$ where the sum runs over all 2-element subsets of $\{1,2,3,4\}$ and bar denotes complex conjugation. Suppose now that $a_K$ are some given positive, distinct coefficients and the unitary matrix $U$ is such that $$(\star) \qquad \sum_K a_K\bar{M}_{K,\{1,2\}}M_{KJ} = 0$$ for $J = \{1,3\}, \{1,4\}, \{2,3\}, \{2,4\}$ . Does this imply that $(\star)$ holds also for $J = \{3,4\}$ ? I've tried to check it numerically, but I have trouble even with finding examples of unitary matrices that satisfy $(\star)$ . Any hint would be much appreciated. EDIT: I've learned that the problem seems algebraic-geometrical and that the Plucker relations might be of use. If so, how?","['unitary-matrices', 'algebraic-geometry', 'linear-algebra', 'projective-space']"
4631562,Understanding this passage in Borel Cantelli Lemma N.2,"I'm trying to understand a passage in the proof of Borel Cantelli Lemma 2. Be $(\Omega, \mathcal{F}, P)$ a probability space and $(A_n)$ a sequence in $\mathcal{F}$ such that $(A_n)$ are pair independent. If $$\sum^{+\infty} P(A_n) = +\infty \qquad \quad \text{then} \qquad P(\text{lim sup}_n A_n) = 1$$ _ Part of Proof_ We start by denotng $S_n = \sum_{k}^n \mathbb{1}_{A_k}$ ; $\quad$ $S = \sum_{k}^{+\infty} \mathbb{1}_{A_k}$ ; $\quad$ $a_n = \int S_n \text{d}P = \sum^n P(A_n)$ Then from pair independence \begin{equation*}
\begin{split}
\int (S_n - a_n)^2\ \text{d}P & = \sum_{i, k}^n \int \left(\mathbb{1}_{A_i}- P(A_i)\right)\left(\mathbb{1}_{A_k}- P(A_k)\right)\ \text{d}P
\\\\ 
& = \color{red}{\sum_{i}^n \int\left(\mathbb{1}_{A_i}- P(A_i)\right)^2\ \text{d}P}
\\\\
& = \color{blue}{\sum_i^n P(A_i)(1 - P(A_i))} \leq a_n
\end{split}
\end{equation*} I think I got why in the end it's $\leq a_n$ but the not-understood passage is the red coloured one that turns into blue coloured one. How does one obtain that? Thank you!","['measure-theory', 'proof-explanation', 'borel-cantelli-lemmas', 'probability-theory', 'probability']"
4631608,"Let $f(x)\in C^{1}(\mathbb{R}),\ f^{'}(x)>f(f(x)),\ \forall x\in\mathbb{R}. $ Prove: $f(f(f(x))))\le 0,\ \forall x\ge0.$","Let $f(x)\in C^{1}(\mathbb{R}),\ f^{'}(x)>f(f(x)),\ \forall x\in\mathbb{R}. $ Prove: $f(f(f(x))))\le 0,\ \forall x\ge0.$ I have just proved $f$ is bonuded. In fact, if $\displaystyle\lim_{x\to +\infty}f(x)=+\infty,\ $ when $x$ is sufficiently big, $$f(x)>1+\epsilon>1,\ f^{'}(x)>f(f(x))>1+\epsilon$$ so that when x is more sufficiently big, we have $$f(x)>x,\ f^{'}(x)>f(f(x))>f(x),\ e^{-x}f(x)>c>0,\ f^{'}>ce^{f(x)}$$ than $$+\infty>\int_{x_0}^{+\infty}\frac{f^{'}(x)}{e^{cf(x)}}=+\infty$$ that's a contradiction! But then I ran out of ideas.","['inequality', 'derivatives']"
4631616,Series representation of $n$th derivative of $x^n/(1+x^2)$,"Find the nth derivative of $\frac{x^n}{1+x^2}$ . Please I need help in this. They are further asking to show that when $x=\cot y$ the nth derivative can be expressed as $$n!\sin y\sum_{r=0}^{n}(-1)^r {n\choose r}\cos^r y\sin[(r+1)y]$$ My attempt
Since the function inside can be expressed as $P(x)-\frac{x}{1+x^2}$ or $P(x)-\frac{1}{1+x^2}$ with $\deg(P)=n-2$ as per $n$ is odd or even, I tried to write the nth derivative of the function as $(-1)^n n! \cos(n+1)y \sin^{n+1} y$ or $(-1)^n n! \sin(n+1)y \sin^{n+1} y$ as and when n is odd or even and then tried induction to prove this equal to the R.H.S , obviously cancelling out the $n!$ . But at that induction step I am getting entangled. Though i showed this holds for n=1,2.","['summation', 'real-analysis', 'calculus', 'trigonometry', 'derivatives']"
4631644,"Algebraic set $V(\mathfrak{a})$ equals the set of homomorphisms from $A = k[x_1, \dots, x_n]/\mathfrak{a}$ to $k$?","Let $A$ be a finitely generated $k$ -algebra. Then $A = k[x_1, \dots, x_n]/\mathfrak{a}$ . Why is $V(\mathfrak{a})\subset k^n$ equal to $\operatorname{Hom}_{k-alg}(A, k)$ ?","['algebraic-geometry', 'commutative-algebra']"
4631662,How to remember differentiation formula for logarithmic functions,"So, I was trying to memorize the differentiation formulas for logarithmic functions and exponential functions but It's way too much. I keep forgetting whether to use ln and then multiply the expression by it's power and stuff like that. Is there a clean way to remember these formulas without much pain? Thank you!","['logarithms', 'derivation-of-formulae', 'calculus', 'functions', 'exponential-function']"
4631670,"Finding final bearing of a ship given its initial bearing, initial coordinate, and final longitude","So I’m new to this spherical trig and great circle route stuff. The question says: There is a ship departing from point A (84° W, 15°S) on a great circle route. If its initial bearing is 70°, then what is its bearing when it crosses the 34° W longitude line? From what I understand, you need two points to find a final bearing, but this only gives one point and I don’t know how to find the latitude when the ship is on the 34° longitude. The answer to the question is apparently 66° if that helps anyone with reverse checking their math or anything.","['spherical-coordinates', 'geometry', 'spherical-geometry']"
4631735,Layer Cake Representation Intuition,"Let $(X, \mathcal{F}, \mu)$ be a measure space. If $f: X \to [0, +\infty)$ is non-negative and measurable, then $$
\int_X f(x) d\mu(x) =\int_0^\infty \mu(\{ x \in X: f(x)\geq t\})dt
$$ It is not very clear to me how this intuitively makes sense and why it is called the ""layer cake representation"". In particular, the right hand side seems to be ""summing up"" the length segment of all of the level sets of $f$ , whereas the left hand side is the area under the function $f$ . Why are they equivalent intuitively? Is this somehow related to how the abstract integration is defined on measure spaces: Partitioning the range instead of partitioning the domain? I can not seem to relate these concepts tightly together into a good enough intuition. * Update: After some pondering and rethinking about the comment of @BrianTung, here are my thoughts on my question: To understand the equality $$
\int_X f(x) \,d\mu(x) = \int_0 ^\infty \mu(\{ x \in X: f(x) > t \}) \,dt
$$ intuitively, we might as well just situate us in the case where $\mu$ is the Lebesgue measure. Then indeed the left hand side is exactly the area under the graph of $f$ essentially by immediate definition. The expression on the right hand side of the equality like @Brain Tung commented $$
\int_0 ^\infty \mu(\{ x \in X: f(x) > t \}) \,dt
$$ can be intuitively and almost naively understood as treating $\mu(\{ x \in X: f(x) > t \})$ as the length of a rectangle and $dt$ as the width of the rectangle and the integral sign as a sum over these partitioning rectangles. (This naive interpretation actually doesn't correspond to any known integration theory that I know of... It seems to be just purely reading off notations while using a Riemann integral type interpretation). The picture below can be used to describe this procedure: As both sides of the equality are the area under the graph, the equality indeed should hold. * Questions Remaining: Note that the right hand side is another Lebesgue integral: $\int_0 ^\infty \mu(\{ x \in X: f(x) > t \}) \,dt$ . In particular, this right hand side is ""intuitively"" the area under the function $g(t) := \mu(\{ x \in X: f(x) > t \})$ on the domain $t \in [0, +\infty)$ . However, what exactly is this geometrically speaking? I can not seem to even begin to graph this? Is this related to the above picture somehow? * Some Comments on General Lebesgue Integration Intuition: We should note that the first picture above is one of those interpretations for Lebesgue integrals when people say Lebesgue integral ""partitions"" range, but we must be cautious in relating this claim with the above picture. I believe this is not a direct interpretation of Lebesgue integral. A similar concern about this picture has been raised here: How is Lebesgue integration ""partitioning the range""? In fact, the answer with the highest vote has exactly what I think Lebesgue integral is and I will summarize it here: A classical result says: Every Lebesgue measurable nonnegative function can be upward approximated through simple functions of the form that partitions the range of the function into intervals. (One should check the proof of this lemma to convince themselves this is true: Partition the range of the functions as intervals, take the preimages of these partition pieces and form a partition (not necessarily intervals) of the domain. Noting how the range partition and the domain partitions forms bijective relation. In particular, we can match each domain partition with a number in the range partition pieces. Sum them up and we get a simple function. Some technical issues can be taken care of by taking the cutoff of the function and only construct the simple function within the cutoff. Repeat the construction with finer and finer partition of the range and we get the pointwise increasing approximation of simple functions.) Simple function integrals are defined by summing up the product of the function value and it's corresponding range pre-image. This quite literally computes the area under the graph of the simple functions. The Monotone Convergence Theorem tells us we can approximate Lebesgue integral of a nonnegative Lebesgue measurable functions using these kind of upward approximating simple functions. Thus with some sacrifice of the Lebesgue integral ""accuracy"", we can actually approximate the Lebesgue integral of a nonnegative Lebesgue measurable function by simple function integral. A more accurate picture of thinking about Lebesgue integrals can be found below: In fact, this is why intuitively Lebesgue integrals are set up to represent the area under the graph of the nonnegative Lebesgue measurable function $f$ . Now as it doesn't matter how we partition and sum up the total area, instead of evaluating the area using the second picture above, we can actually evaluate the area using the first picture. * Update 2 : After some discussions, I have gained some intuitive understanding of the integral on the right hand side. While not geometrically meaningful, but we can still interpret the integral as the following: We are essentially using the function $g(t) = \mu(\{ x \in X: f(x) \geq t \})$ to build up the integral of $\int_X f(x) \,d\mu(x)$ . That is to say, we wish to integrate the function $g$ from $0$ to $+\infty$ to get the area of the function $f$ . Now note $g(t)$ (height of $g$ at $t$ ) is intuitively the measure/amount of points $x$ such that $f(x) \geq t$ . Now the simple function integral approximate $\int_0 ^\infty g(t) \,dt$ would sum up the product of the g(t) with the measure of the preimage of $g(t)$ . That is, we will sum up the product of the amount of points $x$ such that $f(x) \geq t$ with a certain weight given by the preimage of $g(t)$ (again a measure of how many points are in a given interval in the range of $g(t)$ ). In particular, we get a weighted sum of the amount of points $x$ such that $f(x) \geq t$ to approximate $\int_0 ^\infty g(t) \,dt$ . This makes intuitive sense: Points $x$ such that $f(x)$ is bigger will get double counted many times while points $x$ such that $f(x)$ is small might only be counted once or twice. Indeed, every time we count the points $x$ with $f(x)$ small, we also count the points $x$ with $f(x)$ large. This means points $x$ with higher function value of $f(x)$ will contribute (weight-adjusted contribution) to the integral of $\int_X f(x) \,d\mu(x)$ (Left hand side of the equality) more than the points $x$ with lower function value of $f(x)$ ! This, in fact, explains the name ""Layer Cake Representation"": For every $t \in [0, +\infty)$ (more precisely, for every piece of partition in $[0, +\infty)$ under the interval partition of the range of $g$ ), we butter the base ( $x$ -axis) with one layer of cream (measure/amount of points $x$ such that $f(x)$ is above $t$ ) according to the height of the function $f$ . The places on the base (domain) that have higher value $f$ will be buttered more times than the other places. Therefore, forming a layer cake representation of the area under the graph of $f$ after allowing $t \to \infty$ .","['integration', 'measure-theory', 'real-analysis', 'big-picture', 'intuition']"
4631766,How do I find individual values of $\sin(\varphi)$ and $\cos(\varphi)$ from $x = a\sin^{2}(\varphi) + b\cos^{2}(\varphi)$?,"If $x = a\sin^2\phi+ b\cos^2\phi$ , express $\sin$ and $\cos$ in terms of $x$ ( $a$ and $b$ are real constants) I know how to find values of $T$ ratios in equations like $x = a\sin^2t$ or $x = b\cos^2t$ but how do I find the values of $\sin(t)$ and $\cos(t)$ in expressions like $x = \sin^2t + 5\cos^2t$ or $x = a\sin t + b\cos t$ ?","['trigonometry', 'substitution']"
4631821,Calculate the following integral $ \int\frac{2x+1}{x^{n+2}(x+1)^{n+2}}\ln\left(\frac{2x^2+2x+1}{x^2(x+1)^2}+\frac7{16}\right)dx$,"Hello I am trying to solve a pretty complicated integral. It is a from a set of problems, published in a monthly journal for high school students and they are exercises in preparation for a competition.
So the problem is the following: Calculate the integral $ \int \frac {2x+1}{x^{n+2}(x+1)^{n+2}} \ln\left(\frac {2x^2+2x+1}{x^2(x+1)^2} + \frac{7}{16}\right)dx$ I first tried partial integration, integrating $\frac {2x+1}{x^{n+2}(x+1)^{n+2}}$ , it is happily easy to do it and we will end up with $\int \frac {2x+1}{x^{n+2}(x+1)^{n+2}} = \frac {-1}{x^{n+1}(x+1)^{n+1}}$ . I also differentiated $\ln\left(\frac {2x^2+2x+1}{x^2(x+1)^2} + \frac{7}{16}\right)$ , ending up with $\frac {-64x^3-96x^2-96x-32}{7x^6+21x^5+71x^3+48x^2+16x}$ Now when I try to solve the integral of the product of these two, I am stuck. I also thought about some kind of recursive relationship in terms of $n$ but I am not sure about it. I would happily accept any help in solving this problem. Thanks in advance.","['integration', 'indefinite-integrals', 'calculus', 'fractions']"
4631904,How can I work out the angle between a face and base of a triangular prism,"I'm struggling with a particular 3d problem - https://i.sstatic.net/SmOus.jpg (question 4) To workout the length of $EM$ , forming a right angled triangle from face $EAB$ I get: $$EM =\sqrt{4^2 - 2^2} = 3.46410161514 $$ Making $O$ the middle of a line $MN$ connecting the mid points of both prism faces, I then was trying to form another right angled triangle with the previously acquired length however this is obviously not the slanting height of the prism face and so when I tried to get workout the angle between the face and base by using a trigonometric ratio, I didn't get the right answer. Any help on this question would be greatly appreciated.","['trigonometry', 'pythagorean-triples', 'geometry', '3d']"
4631919,Which functions besides $\ln{x}$ make $ \lim \limits_{N \to \infty} \sum_{n=1}^N f'(n) -f(N) $ converge?,"Using a very hand-wavy argument, I convinced myself that if, instead of $f(x)=\ln{x}$ , we let $f(x)=\sqrt{x}$ , we should still get something finite and small. Wasn't really sure where to start to prove it, so ran a program to see what happens for large N instead. $$ L = \lim \limits_{N \to \infty} \sum_{n=1}^N \frac{1}{2\sqrt{n}}  -\sqrt{N} $$ It seems like $L$ approaches about $-0.73018$ but couldn't really tell if it wasn't just running away to negative infinity really really slowly. What tactics might we use to prove/disprove convergence here? Edit: I've since discovered that this number is exactly $\frac{1}{2}\zeta(\frac{1}{2})$ Why $\zeta (1/2)=-1.4603545088...$?","['limits', 'convergence-divergence', 'euler-mascheroni-constant']"
4631943,"6 permutations of $[1,2,\cdots,n]$ so for all distinct $a,b,c\in \{1,2,\cdots,k\}$, only one permutation has $a$ before $b$ and $b$ before $c$.","Consider an integer $k \ge 3$ . Prove that there exists a $k$ such that there are no $6$ permutations of $1,2,3,\cdots, k$ that satisfy the following condition: For all distinct numbers $a,b,c\in \{1,2,\cdots,k\}$ , there is exactly one permutation out of the six that has the number $a$ before $b$ and $b$ before $c$ . So far, I have found that in the six permutations, the ordering of $a,b,c$ have to be distinct. This is because if there are two decks have the same ordering, say, $b, c, a$ , then the triplet $b,c,a$ won't satisfy the condition. I think I have to find some sort of contradiction from this, but I'm not sure how. Thanks in advance!!",['combinatorics']
4631981,Two poset properties: are they related?,"A bunch of infinite posets $P$ with $\hat 0$ have the following property For every $x\in P$ , the principal filter $\{ y\in P : y\ge x\}$ is isomorphic as a poset to $P$ itself. Examples include ${\bf N}$ under the usual order, ${\bf N}$ ordered by divisibility, and the set of all finite subsets of ${\bf N}$ . Separate from this, there is another property I'm interested in, which is ""transitivity"". (Perhaps it's called ""multiplicativity""?) For every $x\le y\le z$ , we have $\mu(x,y)\mu(y,z) = \mu(x,z)$ . What I'd love to know is whether either/both of 1. and 2. have a name, and whether they are possibly related. I'm pretty sure that 1. does not imply 2., since ${\bf N}$ under the usual order satisfies 1. but we have $$\mu(1,2)\mu(2,3) = (-1)(-1) \ne 0 = \mu(1,3).$$ That said I think that the other two posets I mentioned above have property 2. So perhaps 2. implies 1.? In any case I'm interested in any references (or if there was a name for these properties I could Google on my own!). Edit: I'd also be interested in any sort of natural conditions that are necessary or sufficient for either 1. or 2. to arise.","['reference-request', 'order-theory', 'mobius-function', 'combinatorics', 'terminology']"
4632025,Number of ways of distributing 4 apples and 6 mangoes to 8 children so that each child receives at least one fruit,Question Six mangoes and four apples are to be distributed among eight children so that each child receives at least one fruit. Find the number of different ways in which six children get one each and out of the remaining two children one gets two mangoes and the other gets two apples. seven children get one each and the other student gets three mangoes seven children get one each and the other student gets three friuts My attempt $^8C_1 \times^6C_2\times^7C_1\times^4C_2\times6! =3628800 $ $^8C_1 \times^6C_3\times7! = 806400$ $^8C_1 \times^{10}C_3\times7! = 4838400$ is there anything I need to check or any other ways to get theses answers ?,"['permutations', 'combinatorics', 'generating-functions']"
4632027,Outer signed measure,"I would like to ask whether there is some kind of analogue of outer measure when dealing with signed measures. I would like to assign measure to all the subsets, not just some $\sigma$ -field.
I'm using Evans-Gariepy's book ""Measure theory and fine properties of functions"" and I would like to apply their approach/theory to signed measures. I also know that you can define an outer measure given a non-negative measure $\mu$ on a given $\sigma$ -field $(X, \sigma)$ by defining $$ \mu'(A) := \inf\left\{\sum_{i \in I} \mu(B_i) | B_i \in \sigma, i\in I\ {\rm countable}, \ A \subset \bigcup_{i \in I} B_i\right\} $$ for ANY subsetset $A$ of the underlying set $X$ (so we allow measuring of non-measurable sets in Caratheodory sense). Such an outer measure makes members of $\sigma$ into $\mu'$ -measurable sets and $\mu'(B) = \mu(B)$ for all $B \in \sigma$ just like we would like it to. But this approach doesn't seem to work for signed measures. Maybe Jordan decomposition could work? If you know the book I'm talking about, I will be very thankful for your advice and help.","['measure-theory', 'signed-measures', 'outer-measure']"
4632037,Does Benford's law reveal that smaller natural numbers are more common than bigger ones in real life?,"I have executed two experiments to verify whether smaller natural numbers are more common than bigger ones, spired by some high-performance database software that stores smaller natural numbers with a unique(memory-saving) approach. Experiment one extracted all natural numbers from a text corpus like Jeopardy (Over 200,000 questions from the famed tv show) and computed each number's frequency. The other experiment extracted all natural numbers from the Fibonacci sequence as follows. integers_freq = {}
integers_cnt = 0

# get all substrings from a numeric string
# e.g. n = 144 substrings = ['1', '4', '4', '14', '44', '144']
def get_substrings(s):
    # omit...

# fibonacci numbers
fibonacci = [1, 1]
for i in range(2, 20):
    fibonacci.append(fibonacci[i-1] + fibonacci[i-2])

for n in fibonacci:
    s = str(n)
    substrings = get_substrings(s)
    for substring in substrings:
        integers_cnt += 1
        if substring in integers_freq:
            integers_freq[substring] += 1
        else:
            integers_freq[substring] = 1

# sort the integers by integer value
integers_freq = sorted(integers_freq.items(), key=lambda x: int(x[0]))

for integer, freq in integers_freq:
    print(""count = {} Pr({}) = {:.4f}"".format(freq, integer, freq/integers_cnt)) And I plot the natural numbers(extracted from top 20 Fibonacci sequence) distribution with a bar chart Both experiments expressed that smaller natural numbers are more common than bigger ones. Can we use Benford's law( Generalization to digits beyond the first ) to explain this phenomenon?","['statistics', 'probability-distributions', 'natural-numbers']"
4632044,Perfect groups whose character degrees square divide its order and are not prime-power,"This post is a sequel of this one . Question : Is there a finite perfect group $G$ such that for all (non-trivial) irreducible complex character $\chi$ then $\chi(1)^2$ divides $|G|$ and $\chi(1)$ is not a prime-power? The post mentioned above got an answer containing a construction starting from the simple group $A_5$ whose character degrees are $[[1,1],[3,2],[4,1],[5,1]]$ , and finishing with a perfect group $G$ with character degrees $$
[ [ 1, 1 ], [ 3, 2 ], [ 4, 1 ], [ 5, 7 ], [ 10, 4 ], [ 12, 10 ], [ 15, 14 ],  [ 20, 59 ], [ 30, 164 ], [ 60, 2651 ] ]$$ Observe that the character degrees of $G$ are multiples of the ones of $A_5$ . If this happens also by starting from an other simple group, then I suggest to try with $A_7$ whose character degress are $[ [ 1, 1 ], [ 6, 1 ], [ 10, 2 ], [ 14, 2 ], [ 15, 1 ], [ 21, 1 ], [ 35, 1 ] ]$ , i.e. no prime-power. But it is not clear to me how to make above construction from $A_7$ in practice (assuming that my expectation is true). A positive answer to this question will answer a problem in the theory of fusion rings.","['gap', 'characters', 'representation-theory', 'finite-groups', 'group-theory']"
4632071,Rational matrices whose powers have bounded denominators,"Let $d\ge 1$ be a natural number. Let $A$ be a square $d\times d$ matrix with rational entries : $A \in M_{d}(\mathbb{Q})$ .  The following statements about the matrix $A$ are equivalent The sequence of powers $(A^n)_{n\ge 1}$ has bounded denominators. The characteristic equation of $A$ has integral coefficients. $A$ is conjugated ( over $\mathbb{Q}$ ) to a matrix with integral entries. Notes: I've discovered stumbled upon this property while playing with linear recurrences -- it may well be a classical thing. & 3. can be generalized, but then the test 2. is still elusive $\rightarrow$ 3.  is canonical forms (done right). $\rightarrow$ 1. $\wedge$ 2 is easy Below is my proof, ignore it if you want to solve the problem on your own. Thank you for your interest! $\bf{Added:}$ The part 2. $\rightarrow$ 3. appeared before on this site. $\bf{Added:}$ Indebted to Ewan Delanoy for the case $A$ a companion matrix","['matrices', 'elementary-number-theory', 'abstract-algebra']"
4632074,What is wrong with my solution? Probability that every bridge player gets an ace.,"Answers here and here do not answer my question. We have $4$ players playing bridge. What is the probability that each player gets an ace? The solution provided with the problem first finds the total no. of ways of dealing the cards using the multinomial coefficient : $$\frac{52!}{13! \ 13! \ 13! \ 13!}$$ For favourable outcomes, there are $4!$ ways of dealing out the aces ( $1$ to each player) and the remaining cards can be dealt in (again, by multinomial coefficient) $$\frac{48!}{12! \ 12! \ 12! \ 12!}$$ ways. Thus, the probability becomes: $$4!\frac{48!(13!)^4}{52!(12!)^4}$$ Simplifying: $$24\frac{13^4}{49\cdot 50\cdot 51\cdot 52}$$ $$\frac{13^3}{49\cdot 25\cdot 17} \approx 0.1055$$ The last calculation is still hard without a calculator, so I thought of an (apparently incorrect) approach. My solution (what is the mistake here?): Let us forget about the other cards, and concentrate on the aces. There are $4!$ ways that each player will get an ace. There are $4^4$ total ways of distributing the aces. So, the probability is: $$\frac{4!}{4^4} = \frac{3!}{4^3} = \frac{3}{32} = \color{red}{0.09375}$$ Which is significantly off. So obviously there is a mistake in my argument. My guess is that the other $12$ (or $48$ cards) somehow influence the answer. Could you please point out the mistake in the answer? I would also be grateful if anyone could explain the mistake with a smaller example, which shows how what I've done and the original answer differ.","['combinatorics', 'probability']"
4632076,How do I find probability that a poker hand contains 4 black cards and 2 spades,"Suppose 5 cards are drawn from the standard deck of 52 cards. Denote random variables $X$ to be the number of black cards in the hand and $Y$ to be the number of spades in the hand. How do I calculate $P(X=4,Y=2)$ , the probability that a poker hand contains 4 black cards and 2 spades? My attempt: $$P(X=4,Y=2)=\frac{{13\choose2}{13\choose2}{26\choose1}}{{52\choose5}}$$ where ${13\choose2}$ is the number of ways to select 2 spades, ${13\choose2}$ is the number of ways to select the remaining 2 black cards excluding spades, and ${26\choose1}$ is the number of ways to choose a red card.","['card-games', 'probability']"
4632104,Variance of mean from a model $X_t=\mu+a_t-\theta a_{t-1}$,"Let $X_t$ be generated by the following model: $$X_t=\mu+a_t-\theta a_{t-1}$$ where $a_t\sim N(0,1)$ i.i.d. Let $\bar{X}=(\sum_{t=1}^nX_t)/n$ . Then $\operatorname{var}((\sum_{t=1}^nX_t)/n)=\frac{1}{n^2}\sum_{t=1}^n\operatorname{var}(X_t)+\frac{2}{n^2}\sum_{t=1}^n\sum_{j=1}^{t-1}\operatorname{cov}(X_t,X_j)$ . I am not sure how the last sentence comes out. Can anyone explain a bit?","['statistics', 'probability-theory']"
4632113,Ham sandwich theorem.,"Recently, I have been reading the Borsuk-Ulam theorem from Hatcher's algebraic topology book. In this book, there is an exercise as follows. Let $A_1, A_2, A_3$ be compact sets in $\mathbb{R}^3$ . Use the Borsuk–Ulam theorem to show that there is one plane $P \subset \mathbb{R}^3$ that simultaneously divides each $A_i$ into two pieces of equal measure. I have some doubts regarding the above statement; each $A_i$ divided equal measure this part I did not understand properly. $\mathbb{R}^3$ has its won Lebesgue measure say $\mu$ if I take one of $A_i$ suppose $A_1$ say a unit circle $S^1$ which is compact and $\mu(S^1)=0$ then no need to divided by two equal measure. Is this statement say when I am taking one of them as the lesser dimension, we must take the Lebesgue measure concerning that dimension? Can anyone suggest books or good references where I can find the statement of the ham sandwich theorem and its proofs?","['lebesgue-measure', 'probability-distributions', 'geometric-measure-theory', 'algebraic-topology', 'differential-geometry']"
4632114,Complex series can only have two (or three) real parts,"This is related to a deleted question which asked about the complex series $$S_u = \sum\limits_{n=1}^{\infty}e^{-iun}\frac{\sin(n)}{n} $$ All the terms are real when $u$ is a multiple of $\pi$ , and when it is an even multiple this gives $S_0=\sum\limits_{n=1}^{\infty} \frac{\sin(n)}{n}=\frac{\pi-1}{2}$ , while if it is an odd multiple it gives $S_\pi=\sum\limits_{n=1}^{\infty} (-1)^n\frac{\sin(n)}{n}=-\frac{1}{2}$ , each with $O(\frac1n)$ convergence. The surprising result is what happens for other values of $u$ .  My empirical investigations suggest that: you get conditional convergence to a complex value except when $u=2k\pi \pm 1$ ; $Re(S_u) = \frac{\pi-1}{2}$ when $2k \pi -1 < u <2k \pi +1$ ; $Re(S_u) = -\frac{1}{2}$ when $2k \pi +1 < u <2(k+1) \pi -1$ ; there is no convergence when $u=2k\pi \pm 1$ since the imaginary parts of the partial sums diverge, but the real parts of the partial sums converge on $\frac{\pi-2}{4}$ , halfway between the other two values. My questions here are why the real part of $S_u$ only takes two (or three) values while the imaginary part can take any value, and why one of the real values is in a sense more common than the other real value. The answer to why one value is in a sense more common may be that the two values are different in magnitude but their average over $[0,2\pi]$ is $0$ . To illustrate this, here is the real part of the sum plotted against $u$ with $u \in [-2\pi,2\pi]$ and then the imaginary part of the sum plotted against $u$","['complex-analysis', 'convergence-divergence', 'sequences-and-series']"
4632160,"Does additivity of $ f_1(x)g_1(y) + f_2(x)g_2(y)$ tells us something about $f_1, f_2$?","Let $f_1, f_2, g_1, g_2$ be non-constant continuous real functions, such that $
f_1(x)g_1(y) + f_2(x)g_2(y)
$ is additive in $x,y$ , that is, there exist functions $f,g$ such that $$
f_1(x)g_1(y) + f_2(x)g_2(y) = f(x) + g(y), \forall x,y\in\mathbb{R}.
$$ What does it imply for $f_1, \dots, g_2$ ? Is it true that $f_1(x) = af_2(x)+b$ for some $a,b\in\mathbb{R}$ ? What about the case when we have $n$ pairs of functions, i.e. when $
f_1(x)g_1(y) + \dots + f_n(x)g_n(y)
$ is additive?","['multivariable-calculus', 'functions', 'functional-analysis']"
4632237,Evaluating the limit $\lim_{n \to \infty} \left[\log(n) + 2n \log(4n + 2) - \sum_{k = 1}^{2n} (-1)^k (2k+1)\log(2k+1)\right]$,"I am trying to show that the limit $$\lim_{n \to \infty} \left[\log(n) + 2n \log(4n+2) - \sum_{k=1}^{2n} (-1)^k (2k+1) \log(2k + 1)\right] = \frac{2G}{\pi} - \log(4)$$ where $G$ is Catalan's constant. My attempt was to turn the partial sum into a product: \begin{align*}
\sum_{k=1}^{2n} (-1)^n (2k+1) \log(2k + 1)
&= \log\left(\prod_{k=1}^{2n}(2k+1)^{(-1)^k(2k+1)} \right).
\end{align*} \begin{align*}
a_n := \prod_{k=1}^{2n}(2k+1)^{(-1)^k(2k+1)}
\end{align*} \begin{align*}
a_n 
&= \frac{\left(\prod_{k=1}^{n} (4k+1)^{(4k+1)}\right)^2}{\prod_{k=1}^{2n} (2k+1)^{(2k+1)}} \\
&=\frac{\left(\prod_{k=1}^{n} (4k+1)^{(4k+1)}\right)^2}{\prod_{k=1}^{2n} (2k+1)^{(2k+1)}} \cdot \frac{4^{n (2 n+1)} (H(2 n))^2}{\prod_{k = 1}^{2n} (2n)^{2n}} \quad \quad \Big[H(n) = \text{the hyperfactorial} \Big]\\
&= \frac{4^{n (2 n+1)} (H(2 n))^2}{H(4n+1)}\left(\prod_{k=1}^{n} (4k+1)^{(4k+1)}\right)^2.
\end{align*} Another attempt, I considered using the following definition of the gamma function $$\Gamma(x + 1) = \lim_{n \to \infty} \frac{n^x}{\prod_{k = 1}^n \left(1 + \frac{x}{k} \right)},$$ but I was unable to fund any helpful algebraic moves. I also considered using Abel's summation formula to simplify the partial sum down to a closed form, but quickly realized it was not helpful. From here, I was unable to find a closed form. I also found this paper that I believe may be relevant to this problem. A note on products involving ζ(3) and Catalan’s constant . Are there any algebraic moves that can be done using the gamma function's infinite product representation to derive the result? Is it possible there is an asymptotic formula that can be substituted in to evaluate this limit? Is there a better way to evaluate this limit? If so, what can be done?","['summation', 'logarithms', 'calculus', 'infinite-product', 'limits']"
4632253,Can a symmetric positive definite matrix interpolation have linear trace and determinant?,"Definition. Suppose $A_0, A_1 \in S_+^N$ are real symmetric positive definite $N \times N$ matrices.
An interpolation from $A_0$ to $A_1$ is a continuous / smooth function $A \colon [0, 1] \to S_+^N$ such that $A(0) = A_0$ and $A(1) = A_1$ . Example 1. The linear interpolation $A_{\text{lin}}(t) := (1 - t) A_0 + t A_1$ has linear trace , that is, $\text{tr}(A_{\text{lin}}(t)) = (1 - t) \text{tr}(A_0) + t \text{tr}(A_1)$ , but not linear determinant: $\det(A_{\text{lin}}(t)) \ne (1 - t) \det(A_0) + \det(A_1)$ . Example 2. The logarithmic interpolation $A_{\log}(t) := \exp\left( (1 - t) \log(A_0) + t \log(A_1)\right)$ has linear determinant but not linear trace. My question. Does there exist an interpolation from $A_0$ to $A_1$ with linear trace and determinant? My attempt. I tried to show that this is not possible for $N = 2$ (it clearly is possible for $N = 1$ ).
Suppose $A_k = \begin{pmatrix} a_k & b_k \\ b_k & c_k \end{pmatrix}$ for $k \in \{ 0, 1 \}$ and $A(t) = \begin{pmatrix} a(t) & b(t) \\ b(t) & c(t) \end{pmatrix}$ .
Then we require that \begin{gather*}
a(t) + c(t)
\overset{!}{=} (1 - t) (a_0 + c_0) + t (a_1 + c_1) \\
a(t) c(t) - b(t)^2
\overset{!}{=} (1 - t) [a_0 c_0 - b_0^2] + t [a_1 c_1 - b_1^2].
\end{gather*} We can't simply choose $a(t) = (1 - t) a_0 + t a_1$ and $c(t)$ similarly, because then we are in the case of Example 1 and thus don't have linear determinant. Since trace and determinant both only depend on the eigenvalues, there might be an argument using diagonalization. Plugging the first into the second equation of @AndreasLenz yields $$
\lambda_1(t) \big((1 - t) (\lambda_1(0) + \lambda_2(0)) + t (\lambda_1(1) + \lambda_2(1)) - \lambda_1(t) \big) = (1 - t) \lambda_1(0) \lambda_2(0) + t \lambda_1(1) \lambda_2(1)
$$ and thus \begin{align} 2 \lambda_1(t) & = (1 - t)(\lambda_1(0) + \lambda_2(0)) + t (\lambda_1(1) + \lambda_2(1)) \\ & \quad \pm \sqrt{((1 - t)(\lambda_1(0) + \lambda_2(0)) + t (\lambda_1(1) + \lambda_2(1)))^2 - 4 \lambda_1(0) \lambda_2(0) (1 - t) - 4 \lambda_1(1) \lambda_2(1) t}.\end{align}","['interpolation', 'matrices', 'continuity', 'symmetric-matrices', 'positive-definite']"
4632260,Example of tightness of Sauer-Shelah lemma,"I was given the task of finding an example of a family of events (or hypothesis, concepts, etc.) such that the Sauer-Shelah lemma is tight. The lemma states: Assume that the Vapnik-Chervonenkis dimension (VC-dim) of a family of events $\mathcal{A}$ is $d$ . Then for $n \geq d$ $$\mathcal{S}_\mathcal{A}(n) \leq \sum_{i=0}^d{n \choose i}$$ Where $\mathcal{S}_\mathcal{A}$ is the shatter function. My solution: let $\mathcal{X} = \mathbb{R}$ and $\mathcal{A} = \{x\}_{x \in \mathcal{X}}$ . My understanding of VC-dim tells me that $\mathcal{A}$ has VC-dim = 1, since we are not able to shatter more than one point with sets in $\mathcal{A}$ . Now, choose a set with $n$ points $S =\{x_1, ..., x_n\}$ . Notice there  are $n+1$ different ways to shatter this set using $\mathcal{A}$ (choose each point once and then choose a point $x \not\in S$ ). Moreover, $$n+1 = \sum_{i=0}^1{n \choose i} = \underbrace{n \choose 0}_1 + \underbrace{{n \choose 1}}_n$$ Is this example correct ?","['statistics', 'computational-geometry', 'learning', 'machine-learning', 'probability']"
4632276,"Solve for all $x$ such that $x^3 = 2x + 1, x^4 = 3x + 2, x^5 = 5x + 3, x^6 = 8x +5 \cdots$",Question: Solve for all $x$ such that $\begin{cases}&{x}^{3}=2x+1\\&{x}^{4}=3x+2\\&{x}^{5}=5x+3\\&{x}^{6}=8x+5\\&\vdots\end{cases}$ . My attempt: I sum up everything. $$\begin{aligned}\sum_{i=1}^n x^{i+2} &= (2 + 3 + 5 + 8 + \cdots)x + (1 + 2 + 3 + 5 +...)\\&= (S_n - 2)x + (S_n - 1)\\& = (a_{n+2} - 3)x + (a_{n+2} - 2)\end{aligned}$$ where $S_n$ is sum of first $n$ terms of Fibonacci series and $a_n$ is its $n$ th term. I'm not getting any idea how to solve it further.,"['fibonacci-numbers', 'systems-of-equations', 'recurrence-relations', 'sequences-and-series']"
