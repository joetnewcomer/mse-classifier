question_id,title,body,tags
3569519,Solve $2^{1+\sin(3x)}-8^{\sin(x)+\frac13}+\sin (x) \cos (2x)=\sin (x)$,I want to solve this equation: $$2^{1+\sin(3x)}-8^{\sin(x)+\frac13}+\sin (x) \cos (2x)=\sin (x)$$ This is what I did: with $\cos(2x)=1-2\sin^2 (x)$ : $2^{1+\sin(3x)}-8^{\sin(x)+\frac13}+\sin (x) (1-2\sin^2 (x))=\sin (x)$ $\implies 2^{1+\sin(3x)}-8^{\sin(x)+\frac13}+\sin (x) (1-2\sin^2 (x))=\sin (x)$ $\implies 2\times 2^{\sin(3x)}-8^{1/3}\times 8^{\sin(x)}=2\sin^2 (x)$ $\implies 2^{\sin(3x)}- 8^{\sin(x)}=\sin^2 (x)$ But here I don't know how to continue.,"['algebra-precalculus', 'trigonometry']"
3569553,"If $Y = \sum_{i=1}^nX_i$, how do I express $Var(\frac{1}{n}Y)$ as a function of $Var(X)$?","To give more context, we have $X_i = 1$ if true, $0$ otherwise. The probability that the outcome will be true is 0.6. The sample is independent. This is my work so far, but I feel like it's wrong. $\DeclareMathOperator{\Var}{Var}$ $\Var(\frac{1}{n}Y) = E[(\frac{1}{n}\sum_{i = 1}^nX_i)^2] - E[\frac{1}{n}\sum_{i = 1}^nX_i]^2$ $ = E[\frac{1}{n}\sum_{i = 1}^nX_i\cdot\frac{1}{n}\sum_{i = 1}^nX_i] - (E[X])^2 = E[X^2] - E[X]^2 = \Var(X).$ Where am I going wrong? Thanks. EDIT:
To get these equivilances, I'm using the linearity of the expected value and the fact that $E[AB] = E[A]E[B]$ .","['expected-value', 'statistics', 'variance', 'probability']"
3569590,"Find $\lim\limits_{n \to \infty} n \int_2^e (\ln x)^n \, dx$.","I have to find the limit: $$\lim\limits_{n \to \infty} n \displaystyle\int_2^e (\ln x)^n dx$$ I tried using the Squeeze Theorem but it didn't work (or at least I didn't use it correctly): $$2 \le x \le e$$ I took the log of this inequality $$\ln(2) \le \ln(x) \le 1$$ Raised to the $n$ : $$\ln^n(2) \le \ln^n(x) \le 1$$ Took the integral from $2$ to $e$ : $$\int_2^e \ln^n(2)dx \le \int_2^e \ln^n(x) dx \le \int_2^e 1 dx$$ $$\ln^n(2) \cdot (e-2) \le \int_2^e \ln^n(x) dx \le (e-2)$$ Multiplied by n: $$n \ln^n(2) \cdot(e - 2) \le n\int_2^e \ln^n(x) dx \le n(e-2)$$ And now, since $\ln(2) < 1$ , so $\ln^n(2) = 0$ as $n \to \infty$ , if I take the limit with $n \to \infty$ I get: $$\infty \cdot 0 \le n\int_2^e \ln^n(x) dx \le \infty$$ So I didn't get anywhere trying to use the Squeeze Theorem.","['integration', 'definite-integrals', 'calculus', 'sequences-and-series', 'limits']"
3569599,Solve $y''+y=p(x)$ where $p$ is a polynomial function,"The equation is $$y'' + y=p(x)$$ Let $p(x)=\sum\limits_{k=0}^n a_k x^k$ The homogeneous solution to the equation is $y_h = a\cos(x)+b\sin(x)$ . I can look for a particular solution of the form $y_p=\sum\limits_{k=0}^n b_k x^k$ . 
We get $$\sum\limits_{k=0}^{n-2} ((k+2)(k+1)b_{k+2}+b_k)x^k+b_{n-1}x^{n-1}+b_nx^n=\sum\limits_{k=0}^n a_kx^k$$ Hence, if $0\le k \le n-2$ , $(k+2)(k+1)b_{k+2}+b_k=a_k$ and $b_{n-1}=a_{n-1}$ , $b_n=a_n$ I'm trying to show that $y_p=\sum\limits_{n=0}^{\infty}(-1)^n p^{(2n)}(x)$ from here, how do I proceed?","['recurrence-relations', 'ordinary-differential-equations']"
3569654,Gradient of Gaussian Smoothing,"In Nesterov's "" Random Gradient-Free Minimization of Convex Functions "", a Gaussian smoothing of a continuous convex function with respect to parameter $\mu$ and covariance matrix $B^{-1} \succ 0$ is defined as: $$ f_{\mu} (x) = \frac{1}{\kappa} \int_{\mathbb{R}^n} f(x + \mu u) e^{- \frac{1}{2} u^\top B u} \,\mathrm{d}u \tag{9} $$ where $$ \kappa = \int_{\mathbb{R}^n} e^{- \frac{1}{2} u^\top B u} \,\mathrm{d}u \tag{10}.$$ The paper goes on to derive a gradient of this expectation, $$ \nabla f_{\mu} (x) = \frac{1}{\mu \kappa} \int_{\mathbb{R}^n} f(x + \mu u) e^{- \frac{1}{2} u^\top B u} B u \,\mathrm{d} u \tag{21}, $$ which should mean that $$ \nabla f_{\mu} (x) = \frac{1}{\mu} \mathbb{E}_{u \sim \mathcal{N}(0, B^{-1})} [ f(x + \mu u) B u]. $$ However this seems to conflict with the earlier claim (just below (5)) that $$ \nabla f_{\mu} (x) = \frac{1}{\mu} \mathbb{E}_{u} [ f(x + \mu u) u ]. $$ I am wondering if I am missing something here. Are there some implicit assumptions that I'm not picking up on? Note: a similar question ( Partial derivative for Gaussian Smoothing ) is almost relevant, but assumes $\mathcal{N} (0, I)$ instead of $\mathcal{N} (0, B^{-1})$ .","['expected-value', 'probability-distributions', 'derivatives']"
3569655,Implicit Answer to this DE $e^xyy'=e^{-y}+e^{-2x-y}$,"$$e^xyy'=e^{-y}+e^{-2x-y}$$ I have to find a solution, implict solution for the preceding differential equation. The method that I have to use is the separable method.  My attempt went as the following: \begin{align}
e^xy\frac{dy}{dx}&=e^{-y}+e^{-2x-y} \\
e^xy\frac{dy}{dx}&=e^{-y}+e^{-2x}e^{-y} \\
ye^x\frac{dy}{dx}&=e^{-y}(1+e^{-2x}) \\
ye^ydy&=\frac{(1+e^{-2x})}{e^x}dx \\
ye^ydy&=(e^{-x}+e^{-3x})dx 
\end{align} $$\bbox[5px,border:2px solid red]{ e^y(y-1)=-e^{-x}-\frac{1}{3}e^{-3x}}$$ Is this the implicit way to write the solution, using the separation of variables?","['integration', 'calculus', 'ordinary-differential-equations']"
3569668,Eigenvalues of block Toeplitz matrix with Toeplitz blocks,"Consider integers $m,n$ and a $m \times m$ -block Toeplitz matrix $A$ consisting of two different types of blocks as follows \begin{align}	
A_{mn \times mn} 
&= 
\begin{bmatrix} 
B & C & C & \cdots & \cdots & C \\ 
C & B & C & C & \cdots & C \\ 
C & C & B  & C & \ddots & \vdots \\ 
\vdots & \ddots & \ddots & \ddots &  \ddots & C \\ 
C & \cdots & \cdots & C & B & C \\ 
C & \cdots & \cdots & \cdots & C & B 
\end{bmatrix}
_{mn \times mn} 
,
\end{align} where $B$ 's are diagonal blocks with $B=\frac{1}{m}I_n$ and $C$ 's are multiples of the all-ones matrix $J_n$ , specifically $C=\frac{1}{mn}J_n$ . I want to compute the eigenvalues of $A$ (I am mainly interested in the value of the 2nd largest eigenvalue since it has a special meaning in graph expansion applications). Note that in my problem the following conditions also hold for $m,n$ : $m$ is odd. $n$ is prime. $m<n$ . I have experimented with such matrices on the computer and I have observed a trend for the spectrum of $A$ which consists of the following eigenvalues: $\lambda_1=0$ with algebraic multiplicity $m-1$ . $\lambda_2=1/m$ with algebraic multiplicity $m(n-1)$ . $\lambda_3=1$ with algebraic multiplicity $1$ . I do not claim that this is necessarily the answer but at least it was consistent for the pairs of $m,n$ I tried. Can you suggest how one can go and prove the above claim (if correct) or pinpoint other known results? EDIT After Omnomnomnom's note that \begin{equation}
A = \frac 1{mn}\underbrace{\pmatrix{
0&1&\cdots & 1\\
1&0&\ddots&\vdots\\
\vdots&\ddots&\ddots&1\\
1&\cdots&1&0}}_{= C_{m \times m}} \otimes J_n + \frac 1m I_{mn}
\end{equation} I did some computation of the spectrums of the individual matrices. First, the characteristic polynomial of the all-ones $J_n$ is $(\lambda-n)\lambda^{n-1}$ and hence its spectrum (with the multiplicities) is \begin{equation}
\sigma(J_n)=\{(n,1),(0,n-1)\}.
\end{equation} For $C$ , assume that $\lambda_1,\dots,\lambda_m$ are its eigenvalues. By the facts that $\mathrm{det}(C-(-1)I_m)=det(J_m)=0$ , $C\mathbf{1}_m=(m-1)\mathbf{1}_m$ and $\mathrm{trace}(C)=\sum_i\lambda_i=0$ it turns out that \begin{equation}
\sigma(C)=\{(m-1,1),(-1,m-1)\}.
\end{equation} Suppose that $\mu_1,\dots,\mu_n$ are the eigenvalues of $J_n$ then by the Kronecker product's properties the spectrum of $CJ_n$ consists of the pairwise products $\lambda_i\mu_j, \forall i,j$ .","['eigenvalues-eigenvectors', 'toeplitz-matrices', 'matrices', 'linear-algebra', 'spectral-theory']"
3569725,Showing $\lim_{n \to \infty} \int |f_n| - |f - f_n| = \int |f|$,"I'm trying to show the following without using the Dominated Convergence Theorem : Let $E \subseteq \mathbb{R}^d$ measurable, and $\{f_n\}$ a sequence of integrable functions on $E$ . Assume that $\sup \int_E |f_n| < \infty$ and $f_n \to f$ pointwise a.e. Show that $$\lim_{n \to \infty} \int_E \left(|f_n| - |f - f_n|\right) = \int_E |f|.$$ So far I have that: Since $f_n \to f$ pointwise a.e. we have $$
\begin{align}
\int_E |f| &= \int_E \liminf_{n \to \infty}\left(|f_n| - |f - f_n|\right) \\
&\leq \liminf_{n \to \infty} \int_E (|f_n| - |f - f_n|) \tag{By Fatou's Theorem} \\
&= \int_E |f_n| - \limsup_{n \to \infty} \int_E |f - f_n|.
\end{align}$$ I suppose I now need to show $\int_E |f| \geq \int_E |f_n| - \limsup_{n \to \infty} \int_E |f - f_n|$ , but I am unsure how to proceed. Also, it's clear that $f$ is in $L^{1}(E)$ space, but I'm unsure if/how that is helpful either.","['lebesgue-integral', 'functional-analysis', 'real-analysis']"
3569772,Difference of Chi-squared Variables,"Is this procedure correct? Considering that $X$ is a chi-square random variable with $r$ degrees of freedom? $$E(e^{-tX}) =E \left(\frac{1}{e^{tX}}\right)=\frac{1}{E(e^{tX})} =\frac{1}{(1-2t)^{-r/2}} $$ The following demonstration, despite reaching a true conclusion, is not entirely correct: Let $X_1$ and $X_2$ be a chi-squared random variable with $r_1$ and $r_2$ degrees of freedom respectively, the $Y=X_1-X_2$ is also a chi-squared random variable with $(r_1-r_2)$ degrees of freedom provided that $r_1>r_2$ : $$M_Y(t)=\large{E(e^{tY})=E(e^{t(X_1-X_2)})=\frac{E(e^{tX_1})}{E(e^{-tX_2})}}$$ $$=\large{(1-2t)^{-\frac{r_1}{2}}(1-2t)^{\frac{r_2}{2}}=(1-2t)^{-\frac{(r_1-r_2)}{2}}}$$ therefore, $Y$ is chi-squared with $r_1-r_2$ degrees of freedom.","['statistics', 'probability-distributions', 'probability']"
3569778,Area of triangle in a circle.,"[Edexcel Specimen Papers Set 2, Paper 1H Q22]
The line $l$ is a tangent to the circle $ùë•^2 + ùë¶^2 = 40$ at the point $A$ . $A$ is the point $(2,6)$ .
The line $l$ crosses the $x$ -axis at the point $P$ .
Work out the area of triangle $OAP$ . Any help is appreciated.Thank you.
I have attempted this - but I get the result $48$ (2sf) when the answer is $60$ units $^2$ .","['area', 'geometry']"
3569863,"Is there an algorithm that, given a finite presentation of a free group, outputs a free basis for the group?","I have been stuck on this problem for two weeks, and I'm not sure I believe it to be true anymore. I have no idea how to proceed. I know nothing about algorithms or how to find them, this is my first time trying a question of this sort and I am at the end of my wits. Can someone give me a hint in the right direction?","['group-presentation', 'free-groups', 'algorithms', 'group-theory', 'computability']"
3569876,Branching Process $P[Z_3 > 0]$ where $ùëç_3$ is the size of the third generation.,"In a branching process, suppose $ùëÉ(ùë†) = ùëù + ùëûùë†^2$ for $0 < ùëù < 1$ and $ùëù + ùëû = 1$ .
Assume that the population starts with one ancestor.
Find $ùëÉ[ùëç_3 > 0]$ where $ùëç_3$ is the size of the third generation. For this question, I compute that $P(S)=p+qs^{2}$ follows that $s=p+qs^2$ . Therefore the probability of extinction is $s=\frac{1-\sqrt{1-4pq}}{2q}$ and that the mean progeny is $P'(1)=2q$ . However, I honestly don't know how to move from this to find $P[Z_3 > 0]$ where $ùëç_3$ is the size of the third generation.","['stochastic-processes', 'statistics', 'probability']"
3569894,Outward normal vector on a curve,"Given a simple closed curve $\gamma:[a,b]\subset\mathbb{R}\to\mathbb{R^2}, \gamma(t) = (x(t), y(t))$ , $x(t)$ and $y(t)$ continuous in $\mathbb{R}$ and a point $P(x, y) = \gamma(t)$ for some $t \in [a,b]$ , it is known that two normal unit vectors exist for $P$ , which can be found by solving: $
\begin{equation}
\begin{cases}
\mathbf{n}\cdot\gamma'(t)=0\\
||\mathbf{n}||=1
\end{cases}
\end{equation}
$ Let the solutions be $\mathbf{n_1}$ and $\mathbf{n_2}$ with $\mathbf{n_1} = -\mathbf{n_2}$ . Can one algebraically decide which of these unit vectors points outwards, i.e., without having to resort to plotting the curve and analyzing it?","['vector-analysis', 'differential-geometry']"
3569899,A differential equation involving composition,"I know that $\frac{d}{dx} f(x) = f(x)$ has a solution $f(x) = e^x$ by letting $y= f(x)$ and using separation of variables. Is there a technique to solve (or at least start on) something like $\frac{d}{dx} f(x) = f(2x)$ ? Putting the $2$ anywhere else such as $2f(x)$ , $f(x)+2$ , etc. does not seem to cause much of an issue, but replacing the $x$ with $2x$ stumps me...",['ordinary-differential-equations']
3569967,Tangent space of pulllback manifold,"Suppose $M$ and $N$ are smooth manifolds. Then, the tangent space $T_{(m,n)}(M\times N)$ decompose as direct sum $T_mM\oplus T_nN$ . This has been discussed in many books. I do not see any reference for Tangent space of pullback manifold. Suppose $f:M\rightarrow P$ and $g:N\rightarrow P$ are such that they intersect transversally; you can also assume that one of them is a surjective submersion if you want. Then, we know that there is a nice smooth structure on the pullback $M\times_PN$ . Do we have a nice description of $T_{(m,n)}(M\times_P N)$ in terms of $T_mM$ and $T_nN$ ? I remember proving some time back that $$T_{(m,n)}(M\times_PN)\cong \{(v,w)\in T_mM\times T_nN|f_{*,m}(v)=g_{*,n}(w)\}.$$ But I could not find that notes, I could not prove it quickly now. Is there any quick proof for this result? Is it mentioned in any book?","['smooth-manifolds', 'differential-geometry']"
3570010,What is the general solution to the equation $\sin x + \sqrt{3}\cos x = \sqrt2$,"I need to find the general solution to the equation $$\sin(x) + \sqrt3\cos(x)=\sqrt2$$ So I went ahead and divided by $2$ , thus getting the form $$\cos(x-\frac{\pi}{6})=\cos(\frac{\pi}{4})$$ Thus the general solution to this would be $$x =  2n\pi \pm\frac{\pi}{4}+\frac{\pi}{6}$$ Which simplifies out to be, $$x = 2n\pi +\frac{5\pi}{12}$$ $$ x = 2n\pi -\frac{\pi}{12}$$ But the answer doesn't have the 2nd solution as a solution to the given equation. Did I go wrong somewhere?",['trigonometry']
3570023,How many possible orders can the first Los Santos missions of GTA: San Andreas be completed in?,"This is actually a pretty interesting combinatorics problem, as there's several different branches in the mission tree that split off and join together, and of course there are prerequisites for each mission. Essentially what I'm wondering is - how many possible ways are there to get from Nines And AK's to Reuniting The Families? I attached a screenshot of that part of the mission tree for reference. LOS SANTOS MISSION TREE NOTE: For those who have never played the game, ALL missions in the ENTIRE tree must be completed before Reuniting The Families. Each line coming from the bottom of a box leads to the mission(s) that are unlocked after that one, and each line from the top of a box leads to the mission(s) that must be completed before it.","['game-theory', 'combinatorics', 'combinatorial-game-theory']"
3570031,"Finding a set of maximal sum , such that no two subsets have the same sum.","If $S$ represents the set { $1,2,3,...,10$ } , then find a subset $X$ such that the sum of the elements of $X$ is maximum , and no two subsets of $X$ have the same sum . A bit of trial and error shows that the greedy algorithm works , when the number of elements of $S$ is less . For example , if $S$ = { $1,2,3,4$ } , the greedy algorithm yields $X$ = { $2,3,4$ } , which is indeed the answer , yielding a maximal sum of $2+3+4=9$ . However , I have not been able to prove that the greedy algorithm works . Basically , if $X_n$ represents a subset of maximal sum satisfying the problem-condition (where $n$ represents the cardinality of the subset)  , I have to prove that if there exists $X_{n+1}$ , then $X_n \subset X_{n+1}$ . Any help would be greatly appreciated.","['combinatorics', 'discrete-mathematics']"
3570033,"A property for some finite groups (especially ${\rm PSL}(2,13)$)","Let $G$ be a finite group of order $n$ and consider the following property: (P) for every factorization $n=ab$ there exist subsets $A$ and $B$ such that $|A|=a$ , $|B|=b$ and $G=AB$ . ( $AB=\{ab:a\in A, b\in B\}$ ) Note. If $G$ has the property that for every divisor $d$ of $n$ there exists a subgroup of $G$ with order $d$ or $n/d$ , then we can show that $G$ enjoys (P) . Therefore, (P) is true for all finite abelian groups, and also one can check that the groups $S_n$ , $A_n$ and ${\rm PSL}(2,n)$ , where $n\leq 8$ , have the property. Now, is (P) true for ${\rm PSL}(2,13)$ ? (we think this is a good candidate for a probable counterexample.) Thanks in advance.","['gap', 'group-theory', 'abstract-algebra', 'finite-groups']"
3570048,solve for $(\cos x)(\cos2x)(\cos3x)=\frac{1}{4}$,"solve for $(\cos x)(\cos2x)(\cos3x)=\frac{1}{4}$ . what is the general solution for $x$ .
I wrote $\cos2x$ as $2\cos^2x-1$ and $\cos3x$ as $4\cos^3x-3\cos x$ . Then the expression gets reduced to $\cos x(2\cos^2x-1)(4\cos^3x-3\cos x)=\frac{1}{4}$ . substituted $\cos x=t$ and got $t(2t^2-1)(4t^3-3t)=\frac{1}{4}$ . how do i proceed further?",['trigonometry']
3570076,Non conjugate prior with known posterior,"For a give likelihood function $p(x | \theta)$ , the prior $p(\theta)$ is a conjugate prior if the posterior $p(\theta | x)$ comes from the same family of distributions as $p(\theta)$ . $p(\theta | x) = \frac{p(x | \theta)p(\theta)}{p(x)}$ involves solving an (often intractable) integral so this is very useful. Are there any prior, likelihood pairs such that the posterior's distribution is known but not from the same family as the prior? Meaning, non conjugate priors where the posterior is still easy to find. If so, how is this called?","['statistical-inference', 'statistics', 'probability-distributions', 'bayesian']"
3570121,Simple/natural questions in group theory whose answers depend on set theory,"Take these two questions: ""Given objects $X$ , is there always a group $(X, e, *)$ with those objects?‚Äù (Ans: yes iff Axiom of Choice.) ""Take a group $G$ , its automorphism group ${\rm Aut}(G)$ , the automorphism group of that ${\rm Aut}({\rm Aut}(G))$ , ${\rm Aut}({\rm Aut}({\rm Aut}(G)))$ , etc.: does this automorphism tower terminate (count it as terminating when successive groups are iso)?""  (Ans, Hamkins: Yes, but the very same group can lead to towers with wildly different heights in different set theoretic universes.) Now, these two questions should be readily understood by a student who has just met a small amount of group theory in an introductory course, though their answers depend on set theoretic ideas going far beyond the little bit that appears in their introductory text (e.g. Alan Beardon's  first year Cambridge text Algebra and Geometry ). The question arising: What other questions in group theory are there that would also strike a near-beginning student as simple and natural, and  similarly involve more or less significant amounts of set theory in their answers?","['group-theory', 'set-theory', 'reference-request']"
3570166,$f^{-1}(f(f^{-1}(B))=f^{-1}(B)$,"Let be $f:X\rightarrow Y$ a function. We know that $A\subseteq f^{-1}(f(A))$ for any $A\subseteq X$ and so it follows that $$
f^{-1}(B)\subseteq f^{-1}(f(f^{-1}(B))
$$ for any $B\subseteq Y$ ; analogously we also know that $f(f^{-1}(B))\subseteq B$ for any $B\subseteq Y$ and so it follows that $$
f^{-1}(f(f^{-1}(B))\subseteq f^{-1}(B)
$$ for any $B\subseteq Y$ . So from the two inclusions we have $f^{-1}(f(f^{-1}(B))=f^{-1}(B)$ . It is correct?",['elementary-set-theory']
3570181,Prove quadratic equation ($y=ax^2+bx+c$) has only one line of symmetry,"We can show that the graph of the quadratic equation $y=ax^2+bx+c$ has the line of symmetry $x=-b/2a$ . But how can we show that this is unique? (That is, why is no other line $dx+ey+f=0$ a line of symmetry?) (I've been trying to show that no other line of symmetry can work but have simply been drowning in a sea of algebra. I imagine there must be some cleverer way to do it.)","['symmetry', 'quadratics', 'geometry']"
3570208,Gaussian approximation to arbitrary distribution in Kullback‚ÄìLeibler divergence,"The Kullback‚ÄìLeibler divergence of two densities $p_1,p_2$ over $\mathbb R^d$ is $\def\KL{\mathrm{KL}}$ $$  D_\KL(p_1,p_2)=\int_{\mathbb R^d} p_1(x) \log\frac{p_1 (x)} { p_2(x)}\,\mathrm d x.$$ I know from Gibbs' inequality that $D_{\KL}$ is non-negative. Now I want to prove the following: let $p_2$ be a fixed density, and $p_1$ the density of some Gaussian distribution, then $D_{\KL}(p_1,p_2)$ is minimized exactly when $p_1$ takes the expectation of $p_2$ as its expectation, and the covariance matrix of $p_2$ its covariance matrix. I guess this might require a very technical proof.","['statistics', 'entropy', 'normal-distribution', 'information-theory', 'probability']"
3570241,Is tossing of a coin deterministic experimemt?,"This is a question that I practically encountered while I was playing a game: Is tossing a coin a deterministic experiment? It might seem silly to ask but I had some thought over it. By the definition of the term and apparent look, it is not. But I have a different notion.
Deterministic experiments have the same conditions (it may be physical or regarding apparatus) while conducting an experiment. 
But while tossing a coin do we always apply the same force at the same point of the coin.
We toss with the same side of the coin upwards. If we do so, then then the outcome would be same. A 10‚ÄØkg block would move with 10‚ÄØm/s¬≤ acceleration when a force of 10‚ÄØN is applied. Similarly, if we toss a coin applying the same torque at the same point, then the number of rotations in the air would be the same and give a consistent result. I was scolded by my teacher on asking this question.","['physics', 'chaos-theory', 'probability']"
3570466,Does the planar vector field $f_1 \nabla f_2 - f_2 \nabla f_1$ have a name?,"Doing basic geometry with a vector field in the plane, I encountered what looks like a basic object and wonder if it is a well-known concept, which would help me relate this to known literature. Given a differentiable vector field $\boldsymbol{f}$ in the plane with coordinates $f_1 (x_1,x_2)$ and $f_2 (x_1,x_2)$ , does the vector field $\boldsymbol{g}$ defined as \begin{equation}
\boldsymbol{g} (x_1,x_2) = f_1 (x_1,x_2) \nabla f_2 (x_1,x_2) - f_2 (x_1,x_2) \nabla f_1 (x_1,x_2)
\end{equation} have a name? Can it be easily related to standard vector operators like the curl? This object has interesting properties that quantify how ""parallel"" the field is locally. It is zero when all the vectors point in the same direction, independently of their variations in norm. Conversely, the direction $\boldsymbol{g}$ is pointing at what seems to be the direction of ""maximal variation in direction"" of $\boldsymbol{f}$ .","['geometry', 'vector-analysis', 'differential-geometry']"
3570528,Three ODE with exponentials - proof verification,"Suppose $a>0$ . I am given the following three equations: $$
\begin{align*}
x_1'(t)&=e^{-ax_1(t)}-e^{-ax_2(t)}\\
x_2'(t)&=e^{-ax_2(t)}-e^{-ax_3(t)}\\
x_3'(t)&=e^{-ax_3(t)},
\end{align*}
$$ where all three functions $x_i, i=1,2,3$ , are non-negative. The third equation can be solved explicitly (by separation of variables), $$
x_3(t)=\frac{1}{a}(\ln(a)+\ln(t+C)).
$$ In particular, $x_3(t)\to\infty$ as $t\to\infty$ . Now I would like to prove that (1) $x_1(t)\to\infty$ and $x_2(t)\to\infty$ as $t\to\infty$ , (2) there exists some $T>0$ such that $x_1(t)<x_2(t)<x_3(t)$ for all $t>T$ . Here is what I tried. Proof of (1) First show that $x_2(t)\to\infty$ as $t\to\infty$ . Suppose by contradiction, that $x_2$ is bounded, i.e. there exists some $M>0$ such that $x_2\leq M$ . Then, $x_2'(t)\geq e^{-aM}-e^{-ax_3(t)}$ and, for $t>0$ large enough, $x_2'(t)>0$ since $e^{-ax_3(t)}\to 0$ . This implies that $x_2$ converges since $x_2(t)$ is bounded from above and, eventually, monotonically increasing. Consequently, $x_2'(t)\to 0$ , implying $x_2(t)\to x_3(t)$ . This contradicts the boundedness of $x_2$ . Thus, $x_2(t)\to\infty$ for $t\to\infty$ . Next, since it is proven that $x_2(t)\to\infty$ , the same argument holds for $x_1(t)$ which concludes the proof. Proof of (2) First show that there exists some $T>0$ such that $x_2(t)<x_3(t)$ for all $t>T$ .  To this end, assume by contradiction that for all $T>0$ there exists some $t'>T$ such that $x_2(t')\geq x_2(t')$ , i.e. $x_2'(t')\leq 0$ . Due to $x_2(t)\to\infty$ there also exists some $t''>t'$ such that $x_2'(t'')>0$ . Since $x_3$ increases monotonically, this implies that $x_2$ oscillates around $x_3$ . However, this is not possible since the nullcline $x_2=x_3$ can be crossed only in one direction. More precisely, the set $\{(x_2,x_3): x_2<x_3\}=\{(x_2,x_3): x_2'>0\}$ is forward invariant. Hence, there exists some $T>0$ such that $x_2'(t)>0$ for all $t>T$ . Now, since $x_2$ increases monotonically for $t>T$ , repeat the same argument to show that there exists some $T'>T$ such that $x_1(t)<x_2(t)$ for all $t>T'$ . All together, statement (2) holds for $T'$ . I am very curious to know if my proofs are okay.","['solution-verification', 'ordinary-differential-equations', 'real-analysis']"
3570555,Can an infinite set A be larger than an infinite set B but still have the same cardinality?,"Definition. Two sets have the same cardinality iff they can be put into one-to-one correspondence; or, $$A \simeq B \iff |A| = |B| $$ This definition applies to infinite as well as to finite sets.
It follows from the last three definitions that set $A$ has a larger cardinality than set $B$ iff both a proper subset of $A$ and the whole of $B$ can be put into one-to-one correspondence the whole of $A$ cannot be put into one-to-one correspondence with any
proper subset of $B$ . From: A Crash Course in the Mathematics Of Infinite Sets
Peter Suber, Philosophy Department, Earlham College The last part specifically refers to proper subsets of $B$ .  That excludes at least one subset of $B$ , namely $B$ .  So, is it possible that the whole of A cannot be put into one-to-one correspondence with any proper subset of $B$ but may with $B$ itself? Why did he not say ""any subset of $B$ ""?",['elementary-set-theory']
3570633,Derivative of $\operatorname{arctan2}$,"I'm currently working on some navigation equations and I would like to write down the derivative with respect to $x$ of something like $$f(x) = \operatorname{arctan2}(c(x), d(x))$$ I've searched wherever I've could and the only thing I've come across are the partial derivatives of $\operatorname{arctan2}(y,x)$ with respect to $x$ and $y$ . To be more specific, my equation looks like this: $$\psi = \operatorname{arctan2} \left ( -m_y \cos(\phi + \delta \phi) + m_z \sin(\phi + \delta \phi) \ , \ m_x \cos(\theta + \delta \theta) + m_y \sin(\psi + \delta \psi) \sin(\theta + \delta \theta) + m_x \cos(\phi + \delta \phi) \sin(\theta + \delta \theta)  \right )$$ and I want to know $$\frac{\partial \ \psi}{\partial \ \delta  \phi} \text{ and } \frac{\partial \ \psi}{\partial \ \delta  \theta}$$ I've tried to rewrite the expression to some conditional that checks if $d(x) < 0$ , and if so it sums $\pi$ to $\tan(c(x),d(x))$ . The thing is that the condition depends on both $\delta \theta$ and $\delta \phi$ and I don't know how to derive it. Both $\delta \theta$ and $\delta \phi$ may be assumed to be very small angles. I don't know for sure if the expressions I want exist. I've got no problem in using conditionals as long as the expressions I want are in closed-form. All suggestions are appreciated. Thank you in advance.","['calculus', 'closed-form', 'partial-derivative', 'trigonometry', 'derivatives']"
3570662,Ricci Flow: PDE details?,"Over the past few weeks I have been reading 'Ricci flow: An introduction' (Chow and Knopf) which is, in my opinion, a very well written and quick introduction to the topic. However I find that the book focusses mainly on geometric aspects (which I understand is the real point of the book) rather than on the details of PDE existence-uniqueness-regularity theory. Moreover the book doesn't give sufficient references for some of the PDE theorems they use. For example, after introducing the Ricci-DeTurck flow, the book says that the equation is strictly parabolic and it is a standard result that for any smooth initial metric one has existence of unique short-time solution. I was wondering if someone could point me to some references for such theorems. How do they construct weak solutions? Which sobolev spaces do they work in?","['ricci-flow', 'heat-equation', 'partial-differential-equations', 'regularity-theory-of-pdes', 'differential-geometry']"
3570663,Towards a new proof of infinitude of primes ( with possible unified application to other primes of special forms whose Infinitude is unknown): [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question I'm trying to analyse the primes with the following point of view Consider the following partial sum : $$S(p)=\sum_{n=2}^p\sin^2\left(\frac{œÄ\Gamma(n)}{2n}\right)$$ The summand is zero for non-primes greater than 5 , and finite and non-decreasing for primes (see Connes paper on Wilson's theorem ) I treated this sum with Finite version Abel-Plana Summation Formula (APSF) (as in Olver's book ""Asymptotics and special functions"" ) \begin{align}f(x) = {} & \sin^2\left(\frac{œÄ\Gamma(x)}{2x}\right)\\
\sum_{k=2}^p f(k)= {} & \frac{f(2) +f(p)}2 + \int_2^p f(x) \, dx \\ & {}+ i\int_0^‚àû\frac{f(2+iy) ‚àí f(2‚àíiy)}{e^{2œÄy }‚àí 1} \, dy +i \int_0^‚àû\frac{f(p-iy) ‚àí f(p+iy)}{e^{2œÄy }‚àí 1} \, dy
\end{align} Here the first integral $\int_2^p f(x) \,dx$ is okay (highly oscillatory but we can do something: at least numerically)
( numerical analysts are welcome to provide  graphs of this for large ( at least $10^4$ ) $p$ . I am unable to do so in Mathematica.) See on computational science SE $$i \int_0^‚àû\frac{f(p-iy) ‚àí f(p+iy)}{e^{2œÄy }‚àí 1} \,dy$$ This integral is very tricky, I tried to get growth condition , upper and lower bounds on it but in vain. Then, I tried to attach a weight such that: $$F(z) = \omega(z)\sin^2\left(\frac{œÄ\Gamma(z)}{2z}\right)$$ Here, $\omega(z)$ is a weight we have to construct . The following three condition should hold for $\omega(z)$ : $$\omega(z)>\frac{1}{z},\ \forall z\in\mathbf{R}$$ ( More generally this condition is added for divergence of $\int_0^\infty F(x)dx$ So , $\omega(z)$ can even be complex valued for real domain as long as the given integral is divergent ) $$\lim_{ y‚Üí‚àû}|F(x ¬± iy)|e^{‚àí2œÄy }= 0$$ $$\int_0^\infty |F(x + iy) ‚àí F(x ‚àí iy)|e^{‚àí2œÄy} \,dy<+\infty$$ for every $x‚â•1$ and tends to zero as $x\to\infty$ . This is to eliminate the tricky 2nd integral in the formula. I can't find a weight that satisfies this; nor do I know if it is even possible to find one (!?) (1) Is there any other way to eliminate this second integral as $p\rightarrow\infty$ ? (2) Is there a better summation technique to analyse this (type of) problem? (3) Can we twist (the frequency part of)/change $F(x)$ to make the second integral more sane? (i.e. to make the magnitude of $f$ 's imaginary part satisfy condition 3) Note : I know these type of trig primality-tests are not practical but this one interests me so .....( interest is due to the fact that $\Gamma$ is nicely analytic). Also this could provide a new insights to deal with primes ( if it's workable at all). If argument can't work please explain why(?). UPDATE : Instead of finding the weight ; I considered $\sin^2$ term as function of some other function such that: Construct a generalized function such that: $$ F_*(z, s) = \dfrac{\phi(\sin^2[œÄ\Gamma(z)/(2z)])}{z^s} $$ (1) $\phi(x) =0$ if $x$ is zero ; and 'suitably' finite otherwise (Here , 'suitably' means a value which guarantees the expected divergence of sum (very close to 1 or greater than or equal to 1) ) (2) condition (3) holds for such function A very 'close' example : $$ F(z, s) = \dfrac{\sinh(\sin^2[œÄ\Gamma(z)/(2z)])}{z^s} $$ Let us restrict $s\in[0,1]$ Hence , Now , $$
I(x,s) =\int_0^\infty\mathrm dy \frac{F(x + \mathrm iy, s) ‚àí F(x ‚àí\mathrm iy, s)}{\mathrm e^{2œÄy}-1},
$$ Questions remain: Can we get 'sharp' numerical asymptotic of $I(x)$ as as $x\rightarrow \infty$ ? Also, can we get quantitative upper and lower bound estimations on the functional ? Also, other possible candidates for $\phi$ ? And henceforth the above analogous analysis as above ? See: https://mathoverflow.net/q/354962/145581 Related Question : Infinitude of primes using series introduced in Connes' paper on Wilson's theorem Possible Unified Applications: We can apply it to other primes of special forms whose Infinitude is unknown. (as Œì is nicely analytic). $$S_2(p)=\sum_{n=2}^p\sin^2\left(\frac{œÄ\Gamma(n)}{2n}\right)\sin^2\left(\frac{œÄ\Gamma(n+2)}{2(n+2)}\right)$$ For more details see : On a growth condition satisfied by given functional : Also in this post there is list of values of functional for small X's . Any Comments from numerical methods- experts are welcome (numerical analysis of first and second integral for  large (at least $10^2$ ) values of respective variable ): See related numerical-method based question  : Comparison of integrals with a function (at least numerically): One sentence summary question:
How to get hold of second integral ( growth conditions, roots and other properties ) so that we can use it for our purpose? Or How to get rid of second integral using a weight or composite function method described above ? I modified second integral to various forms to make it workable with given conditions . But if you have a version that works with given conditions as mentioned please add and explain . As one can see I have various doubt about this approach. But I need some expert comments with technical details why this approach is less likely workable.","['summation-method', 'real-analysis', 'complex-analysis', 'analytic-number-theory', 'prime-numbers']"
3570694,Solve the following system of differential equations,"I have to find $x(t)$ and $y(t)$ knowing that: $$\begin{cases} x'= y-\int_0^tx(u)\cos(t-u)\,du \\ y'=x+\cos(t) - 1 \end{cases}$$ Where $x=x(t)$ , $y=y(t)$ and $x(0)=1$ , $y(0)=0$ . I've noticed that the integral in the first equation is the convolution between $x(t)$ and $cos(t)$ , so I can apply the Laplace Transform to determine the originals $x$ and $y$ . $$\begin{cases} sF(s)-1=G(s)-F(s)\frac{s}{s^2+1} \\ sG(s)=F(s)+\frac{s}{s^2+1}-\frac{1}{s} \end{cases}$$ Is there another way to find the original functions $x$ and $y$ ? Thanks!","['complex-analysis', 'calculus', 'laplace-transform', 'ordinary-differential-equations']"
3570708,Interpretations of Topological Space as a Heyting Algebra,"I have recently learned about Heyting algebras which I find quite fascinating, as I am more intuitionistically inclined. One of the main examples of Heyting algebras are given by topological spaces as follows: Let $(X,\tau)$ be a topological space. For $U,V\in\tau$ , define $$U\wedge V:=U\cap V,\hspace{.5cm} U\vee V:=U\cup V, \hspace{.5cm}U\Rightarrow V:=\mathrm{Int}(U^c\cup V),\hspace{.5cm}1:=X,\hspace{,3cm}0:=\varnothing$$ And of course, as usual we define $\neg U:=U\Rightarrow 0$ . Then it turns out that $(\tau,\wedge,\vee,\Rightarrow)$ forms a Heyting algebra! This fact alone is quite interesting, but I was wondering if we can go further. Most references I could find online simply use topological spaces as an example and stop investigations after showing $\tau$ is a Heyting algebra. My question is this: Can we make a dictionary that translates between properties of topological spaces and properties/statements about logic? On my own, I came up with a few that are quite easy to see: $\begin{align*}
\neg U&=X-\overline{U}&(\neg U=0)&\Longleftrightarrow U\text{ is dense}\\\neg\neg U&=\mathrm{Int}(\overline{U})&(\neg\neg U=0)&\Longleftrightarrow U\text{ is nowhere dense}\\U\vee\neg U&=X-\partial U&(U\vee \neg U=1)&\Longleftrightarrow U\text{ is clopen}\\&&(U\Rightarrow V=1)&\Longleftrightarrow U\subseteq V
\end{align*}$ What about other topological properties that we know and love? What does it mean about the corresponding Heyting algebra if $X$ is compact or Hausdorff or Regular or path connected, etc? What about continuous maps between topological spaces and all of the properties they might have? What do those imply about the induced morphisms between the Heyting algebras? Could we, for example, transport the definition of the fundamental group through this correspondence to get something meaningful in terms of the Heyting algebra? Any thoughts or references would be greatly appreciated!","['constructive-mathematics', 'general-topology', 'heyting-algebra', 'logic']"
3570826,Solve differential equation using Laplace Transform,"Given the following differential equation $$y''(t)+4y(t)=\frac{1}{4+\cos{2t}}$$ with $y(0)=1$ , $y'(0)=0$ , find $y(t)$ . Using the Laplace Transform, I've got that $$Y(s)=\frac{s}{s^2+4}+\frac{F(s)}{s^2+4}$$ where $F$ is the Laplace transform of $\frac{1}{4+\cos{2t}}$ .
One can immediately observe how the first term is the image of $\cos{2t}$ . But what about the second one?
If I use the inverse Laplace Transform of the product $\cfrac{F(s)}{s^2+4}$ , I have to compute the convolution between $\cos{2t}$ and $\cfrac{1}{4+\cos{2t}}$ , which is $$\int_0^t \frac{\sin(2t-2u)}{4+\cos(2u)}\,du$$ Now, I could use the fact that $\sin(a-b)=\sin a\cos b-\sin b\cos a$ . I was wondering, do I really have to compute this integral? I could also use the variation of parameters method, but I would run into the same integral.","['complex-analysis', 'calculus', 'laplace-transform', 'ordinary-differential-equations']"
3570883,Is the following distribution a form of another well-known one?,"In my research, I derived the following distribution. However, it does not seem to resemble the form of any well-known distributions. I wanted to see if anyone is seeing something I am not or if I am probably correct. Thanks for taking a look! \begin{equation}
p(X=x)
 = \left(1-e^{-x^2/2}\right)
   e^{-x+\mathrm{erf}\left(x/\sqrt{2}\right) \sqrt{\pi/2}}
   I_{[0,\infty)}(x)
\end{equation}","['statistics', 'probability-distributions', 'probability']"
3570899,Every not empty finite subset of a totally ordered set has a maximum and minimum,"Theorem Let be $(X,\le)$ a totally ordered set: then for any not empty finite subset $A$ of $X$ there exist the maximum  element and minimum element. proof . Let be $(X,\le)$ a totally ordered set and we prove by induction that any not empty finite subset $A$ of $X$ has a minimum element. Since $X$ is a totally ordered set, previously we observe that any its subset $Y$ (finite or infinite) is a chain. Obviously any subset $A$ of one element $a$ has trivially a minimum.
So we suppose that any subset of $n$ elements has a minimum element and then  we consider a subset $A$ of $n+1$ elements: since $A$ is finite there exists a bijection $\phi$ from $A$ onto some natural number $m$ , that is the successor of $n$ , and so we can organize the elements of $A$ in a finite succession, that is $A=\{a_1,...,a_{n+1}\}$ . Now we consider the subset $B=\{a_h\in A:h\le n\}$ : obviously $X$ is a subset of $A$ that has $n$ element and so it has a minimum element $b$ ; so since $A=B\cup\{a_{n+1}\}$ and since $A$ is a chain (remember what before we observed), it must be or $a_{n+1}\le b$ or $b<a_{n+1}$ and so for the transitivity property of the order relation $\le$ in any case $A$ has a minimum element. So now we only have to prove that any not empty finite subset $A$ of $X$ has a maximum element. So we consider the inverse relation $\preccurlyeq$ defined as $x\preccurlyeq y\iff y\le x$ for any $x,y\in X$ : clearly $\preccurlyeq$ is a total order, since indeed $\le$ is a total order, and any minimum in $\preccurlyeq$ is a maximum in $\le$ and so since any not empty finite subset $A$ has a minimum in $\preccurlyeq$ it follows that any not empty finite subset in $\le$ has a maximum element. So we concluded the proof. Is my proof correct? If not how prove the theorem? Could someone help me, please?","['elementary-set-theory', 'order-theory', 'induction', 'solution-verification']"
3570915,Uniform Dominated Convergence Theorem,"I'm wondering if the classic dominated convergence theorem can be generalized to the following uniform version. Let $\{\mu_n:n=1,2,...\}$ be a countable set of probability measures. Let $\{f_k: k=1,2,\cdots\}$ be a sequence of measurable functions. Suppose for each $n$ , $f_k(x)\to 0$ holds $\mu_n$ -almost surely. Suppose there is a measurable function $g$ such that ( i ) $\sup_k |f(x)|\le g(x)$ for all $x$ ; ( ii ) $\sup_n \int g(x) \text{d}\mu_n < \infty$ . Then, do we have $\sup_n |\int f_k(x)\text{d}\mu_n| \to 0$ as $k\to\infty$ ? Based on the classic dominated convergence theorem, we know $\int f_k(x)\text{d}\mu_n\to0$ for each $n$ . The question is that does it convergences uniformly in $n$ as stated above? If not, what extra conditions do we need?","['measure-theory', 'probability-theory', 'uniform-convergence', 'real-analysis']"
3570998,Is this a typo or I have misunderstood the group action?,"Let $k$ be an algebraically closed field. Let $G$ be an affine group over field $k$ and $V$ be an affine variety over field $k$ with compatible left $G$ action.(i.e. $G\times V\to V$ is an algebraic morphism. Denote $k[V]$ the ring of functions of $V$ over algebraically closed field $k$ . Let $f\in k[V],g\in G$ . Denote $f^g(x)=f(gx)$ . Suppose $G$ acts on $V$ from left.(Then I would expect $G$ acts on $k[V]$ from right due to $Hom(V,k)$ is contravariant in first slot.) Let $x,x'\in G$ . Denote $U_x:f\to f^x$ "" $U_{xx'}(f)(v)=f^{xx'}(v)=(f^{x'})^x(v)=(U_x\circ U_{x'})f(v)$ ."" $\textbf{Q:}$ Why above is correct? Consider $(f^{x'})^x(v)$ . Denote $g(w)=f^{x'}(w)=f(x'w)$ by definition. Now $(f^{x'})^x(v)=g^x(v)=g(xv)$ which amounts to evaluate $w=xv$ . Then I get $g(xv)=f(x'xv)$ which differs from $f^{xx'}(v)=f(xx'v)$ by action ordering. I would say $U_{xx'}=U_{x'}\circ U_x$ .(i.e. This becomes a right action on the ring of functions.) My guess is either that $f^x(v)=f(x^{-1}v)$ instead or it should become right action. Ref. Fogarty, Invariant Theory Chpt 2, Statement right before Definition 2.11","['algebraic-geometry', 'group-theory', 'abstract-algebra']"
3571007,"Let X be a Hausdorff space and Y be a subset of X. Then, Y with the subspace topology is a Hausdorff space.","Question: Let X be a Hausdorff space and Y be a subset of X. Then, Y with the subspace topology is a Hausdorff space. This is what I did, can someone verify this and let me know if I am correct or wrong? Also, kindly let me know if my proof need some changes or modifications due to bad notations. Any help will be greatly appreciated.",['general-topology']
3571028,Complex Analysis: Showing analytic function is zero,"How I can solve this problem: Let $f: D \to D$ be an analytic function where $D$ is the unit open disc in $\mathbb C$ . Suppose there is a positive number $\delta > 0$ such that , $$\lim_{z \to e^{iŒ∏}} ‚Å°f(z)= 0; \qquad \forall \ |\theta| < \delta.$$ Show that $f \equiv 0$ on $D$ . Thanks Note: An easier version of this Privalov's Theorem is an exercise problem from Stein and Shakarchi's Complex Analysis textbook which additionally assumes the holomorphic function converges uniformly to $0$ on the portion of the arc. See [1] , [2] , [3] . The absence of this 'uniform' non-tangential limit on the portion of the arc makes it a slightly more difficult problem.",['complex-analysis']
3571049,"Given functions such that $f = \tilde{f}$ and $g = \tilde{g}$, then $g\circ f = \tilde{g}\circ\tilde{f}$.","Verify  the substitution property: if $f,\tilde{f}:X\rightarrow Y$ and $g,\tilde{g}:Y\rightarrow Z$ are functions such that $f = \tilde{f}$ and $g = \tilde{g}$ , then $g\circ f = \tilde{g}\circ\tilde{f}$ . MY ATTEMPT We say two functions $f,g:X\rightarrow Y$ are equal iff $f(x) = g(x)$ , for all $x\in X$ . Hence, for arbitrary values of $x\in X$ , we have that $$\begin{align}
(g\circ f)(x) &= g(f(x))\\
& = g(u) \\
&= \tilde{g}(u)\\
& = \tilde{g}(f(x)) \\
&= \tilde{g}(\tilde{f}(x)) \\
&= (\tilde{g}\circ\tilde{f})(x),
\end{align}$$ which proves that $g\circ f = \tilde{g}\circ\tilde{f}$ . Can someone check if I am reasoning rightly?","['functions', 'solution-verification']"
3571095,"Show that if $g\circ f = g\circ\tilde{f}$ and $g$ is injective, then $f = \tilde{f}$","Let $f:X\rightarrow Y$ , $\tilde{f}:X\rightarrow Y$ , $g:Y\rightarrow Z$ , and $\tilde{g}:Y\rightarrow Z$ be functions. Show that if $g\circ f = g\circ\tilde{f}$ and $g$ is injective, then $f = \tilde{f}$ . Is the same statement true if $g$ is not injective? Show that if $g\circ f = \tilde{g}\circ f$ and $f$ is surjective, then $g = \tilde{g}$ . Is the same statement true if $f$ is not surjective? MY ATTEMPT We have to prove that $f(x) = \tilde{f}(x)$ for every $x\in X$ . We know that a function $h:X\rightarrow Y$ is an injection if, given $x\in X$ and $y\in X$ , $h(x) = h(y)$ implies that $x = y$ . Based on such definition and the property that $g$ is injective, one has that \begin{align*}
(g\circ f)(x) = (g\circ\tilde{f})(x) \Longrightarrow g(f(x)) = g(\tilde{f}(x)) \Longrightarrow f(x) = \tilde{f}(x)
\end{align*} which implies the desired result. In the case where $g$ is not injective, it does not hold in general. Consider, for instance, that $g(x) = 0$ . Then we have that \begin{align*}
(g\circ f)(x) = g(f(x)) = 0 = g(\tilde{f}(x)) = (g\circ\tilde{f})(x)
\end{align*} independently of the expression of $f$ and $\tilde{f}$ . We have to prove that $g(y) = \tilde{g}(y)$ for every $y\in Y$ . We know that a function $h:X\rightarrow Y$ is surjective if for every $y\in Y$ there is an $x\in X$ such that $y = h(x)$ . Based on the assumption that $f:X\rightarrow Y$ is surjective, for every $y\in Y$ there corresponds an $x\in X$ such that $f(x) = y$ . Consequently, for every $y\in Y$ , we have that \begin{align*}
g(y) = g(f(x)) = \tilde{g}(f(x)) = \tilde{g}(y) \Longrightarrow g = \tilde{g}
\end{align*} which is the desired result. If $f$ is not surjective, the same counter-example $f(x) = 0$ works, since we obtain that $g(0) = \tilde{g}(0)$ , but we do not know what happens to the other points. I would like to know if someone could check I am reasoning rightly as well as propose less artificial counter-examples.","['functions', 'solution-verification']"
3571127,About definition of superset,"I have read definition of superset somewhere as ""a set containing all elements of a smaller set."" . This implies that for set A to be a superset of B, B must be a proper subset of A, that is, A must contain at least one element which is not in B. Does it mean that a set A cannot be a superset of itself? or does there exist terms such as Superset and Proper Superset ?","['elementary-set-theory', 'definition']"
3571173,"Show that if $g\circ f$ is injective, then $f$ must be injective. Show as well that if $g\circ f$ is surjective, then $g$ must be surjective.","Let $f:X\rightarrow Y$ and $g:Y\rightarrow Z$ be functions. Show that if $g\circ f$ is injective, then $f$ must be injective. Is it true that $g$ must also be injective? Show that if $g\circ f$ is surjective, then $g$ must be surjective. Is it true that $f$ must also be surjective? MY ATTEMPT Let us prove the first statement first. Assume $g\circ f$ is injective. Thus we have that \begin{align*}
f(x) = f(y) \Longrightarrow g(f(x)) = g(f(y)) \Longrightarrow (g\circ f)(x) = (g\circ f)(y) \Longrightarrow x = y
\end{align*} where it has been used the fact that $g\circ f$ is injective. Hence $f$ is injective. However it does not necessarily hold that $g$ is injective. Consider, for instance, the functions $f:\{0,1\}\rightarrow\{0,1,2\}$ such that $f(0) = 0$ and $f(1) = 1$ and $g:\{0,1,2\}\rightarrow\{0,1\}$ such that $g(0) = 0$ , $g(1) = 1$ and $g(2) = 0$ . Consequently, $g\circ f:\{0,1\}\rightarrow\{0,1\}$ is given by $(g\circ f)(0) = 0$ and $(g\circ f)(1) = 1$ . Although $f$ is injective, $g$ is not. We may now proceed and prove the second part. We need to show that $g(Y) = Z$ . We already have that $g(Y)\subseteq Z$ . Therefore we have to prove that $Z\subseteq g(Y)$ . Indeed, one has that \begin{align*}
(g\circ f)(X) = g(f(X)) = Z \subseteq g(Y)
\end{align*} since $f(X)\subseteq Y$ . Consequently, $g(Y) = Z$ and $g$ is surjective. Similarly, $f$ need not be surjective. Consider, for instance, that $f:\{0,1\}\rightarrow\{0,1,2\}$ is given by $f(0) = 0$ and $f(1) = 1$ and $g:\{0,1,2\}\rightarrow\{0,1\}$ such that $g(0) = 0$ , $g(1) = g(2) = 1$ . We have that $g\circ f:\{0,1\}\rightarrow\{0,1\}$ is given by $(g\circ f)(0) = 0$ and $(g\circ f)(1) = 1$ is surjective as well as $g$ , but $f$ is not surjective. Could someone double-check my solution? Any other counterexamples would be appreciatted.","['functions', 'solution-verification', 'real-analysis']"
3571345,Convergence almost surely,"Let $(X_n)_{n\ge 1}$ be a sequence of dependent nonnegative random variables, where $X_n$ has density w.r.t. Lebesgue on $[0,n]$ and $\mathbb{E}(X_n) < \infty$ . We know that $X_n$ converges weakly to $X$ which has density w.r.t. Lebesgue on $[0,\infty]$ and $\mathbb{E}(X) < \infty$ . Question: $X_n/\log(n)$ converges almost surely to $0$ ? My attempt: Applying Slutzsky's theorem I can say that $X_n/\log(n)$ converges in probability to $0$ , but from it I cannot say anything a.s. Furthermore, I know that convergence of marginal distributions does not say anything about a.s. convergence. However, here $1/log(n)$ is a deterministic sequence going to $0$ and so the intuition is that to have a negative answer we need $X_n(\omega)$ diverges on a subset of $\Omega$ (the space where $X_n$ are defined) of positive probability, which seems against the hypothesis of $X_n$ converges weakly to $X$ . Thanks for the help!","['statistics', 'probability-limit-theorems', 'almost-everywhere', 'convergence-divergence', 'probability-theory']"
3571460,"Is the convex combination of a convex and strictly convex set, strictly convex?","Let $S \subset C \subset \mathbb{R}^d$ be two subsets of $\mathbb{R}^d$ , one included in the other. For the sake of simplicity, assume that they are both compact and $\boldsymbol{0}$ belongs to both of their interiors. My question is the following. If $C$ is convex and $S$ is strictly convex, is it true that for all $\lambda \in [0,1)$ , their convex combination $$
(1-\lambda)S + \lambda C = \bigl\{(1-\lambda) \boldsymbol{s} + \lambda\boldsymbol{c} : \boldsymbol{s} \in S, \boldsymbol{c} \in C \bigr\}
$$ is strictly convex? It is known that the Minkowski sum $A+B = \{a+b:a\in A, b\in B\}$ of any two convex sets is itself convex, but my intuition (see pic. below) tells me that (at least under the assumptions above) the strict convexity of just one of them should imply the strict convexity of the convex combination.","['convex-hulls', 'convex-geometry', 'normed-spaces', 'functional-analysis', 'convex-analysis']"
3571464,Is the set of symmetric positive semi-definite matrices a smooth manifold with boundary,"Let $P_n(\mathbb{R}):=\left\{
A \in \operatorname{Mat}_{n\times n}(\mathbb{R}):\,
A=A^T \mbox{ and }  (\forall i)\,
\lambda_i^A \geq 0
\right\}$ , where $\left\{\lambda_i^A\right\}_i$ are the eigenvalues of $A$ .  Is this a manifold with boundary which can be decomposed as: $$
P_n(\mathbb{R}) = P_n^+(\mathbb{R}) \cup Sym_n^0(\mathbb{R}),
$$ where $Sym_n^0(\mathbb{R})$ is the set of symmetric $n\times n$ matrices satisfying $x^TAx=0$ for some non-zero vector and $P_n^+(\mathbb{R})$ is the set of symmetric positive-definite matrices with real entries? I know that $P_n^+(\mathbb{R})$ is a smooth manifold since I can explicitly write down its global chart using the matrix exponential map and (a slight variant of) the vectorization operation .  However, I'm not sure if $Sym_n^0(\mathbb{R})$ is of dimension $\frac{n(n+1)}{2} -1$ .","['manifolds-with-boundary', 'matrices', 'geometric-topology', 'manifolds', 'differential-geometry']"
3571524,Determine if two cables have crossed,"I have the following problem. A body in space is connected to by cables to two fixed points. In the initial position, a high fixed point is connected to a low point on the body and a low fixed point is connected to a high point on the body. This means that the cables are already quite close to crossing.
The body can be rotated within certain constraints, but not more than say +/- 30 degrees. (It will not turn upside down.) The movement of the body must be constrained, so that the cables do not cross. The cable positions are defined by the end points of each cable, so there are four points in all. 
The initial position is known. Given two new positions on the body, I want to determine whether the cables would have crossed to reach this position (i.e. is it a legal position). My idea is, that I can define a plane based on three of the four points. The fourth point must then be constrained to always lie on the one side of that plane. Is there a mathematically elegant way to determine this? Edit The application is something like the system shown in the following image from this page","['vectors', 'geometry', '3d']"
3571600,$f: A \to B$ is invertible if and only if bijective,"I seem to have misplaced an old textbook that contains the standard proof of this fact. I was hoping someone could critique the proof I've written. A function $f: A \to B$ is bijective if and only if it is invertible. Suppose $f$ is bijective. Since $f$ is surjective, for all $b \in B$ , there exists $a \in A$ such that $f(a) = b$ . Since $f$ is injective, fixing one $b \in B$ , its preimage is unique. Define the map $g(b) = a$ . This is a total function since every $b \in B$ is included in the image of $f$ (by surjectivity). Furthermore, for any $a \in A$ whose image under $f$ is some $b \in B$ , we have: \begin{align*}
(g \circ f)(a) = g(f(a)) = g (b) = a.
\end{align*} Since $a$ was arbitrary, \begin{align*}
g \circ f = \text{id}_A. 
\end{align*} Similarly, for arbitrary $b \in B$ with unique preimage $a \in A$ , we have: \begin{align*}
(f \circ g)(b) = f(g(b)) = f(a) = b.
\end{align*} Since $b$ was arbitrary, \begin{align*}
(f \circ g) = \text{id}_B.
\end{align*} Hence, $g = f^{-1}$ , so $f$ is invertible. Now, suppose that $f$ is invertible. That is, there exists an inverse $f^{-1} : B \to A$ such that $f \circ f^{-1} = \text{id}_B$ and $f^{-1} \circ f = \text{id}_A$ . We will prove that $f$ is injective and surjective. To establish the latter, let $b \in B$ . Since $f^{-1}$ is a total function on $B$ , there exists a unique $a \in A$ such that $f^{-1} (b) = a$ . Applying $f$ on the left gives $$f(f^{-1} (b)) = f(a).$$ By the definition of composition, $$(f \circ f^{-1})(b) = f(a).$$ But $f \circ f^{-1} = \text{id}_B$ : $$\text{id}_B (b) = f(a)$$ By definition, $$b = f(a)$$ Hence, there exists $a \in A$ such that $f(a) = b$ , so $f$ is surjective. Now, we show that $f$ is injective. Let $a_1, a_2 \in A$ , and suppose that $f(a_1) = f(a_2)$ . Applying $f^{-1}$ on the left gives $$f^{-1}(f(a_1)) = f^{-1} f(a_2).$$ By the definition of composition, $$(f^{-1} \circ f)(a_1) = (f^{-1} \circ f)(a_2)$$ But $f^{-1} \circ f = \text{id}_A$ : $$\text{id}_A (a_1) = \text{id}_A (a_2).$$ By definition, $$a_1 = a_2.$$ Hence, $f$ is injective. Since $f$ is injective and surjective, $f$ is bijective.","['elementary-set-theory', 'solution-verification']"
3571620,Pseudo Inverses of covariance matrices that are close to each other,"I am trying to understand inverses of covariance matrices that are close to each other. I start with a positive semidefinite matrix $\mathbf{\Sigma^{-1}}$ , then I take the pseudo-inverse to get covariance matrix $\mathbf{\Sigma}$ . Then this is transform into correlation matrix, $$\mathbf{R = D^{-1}\ \Sigma\ D^{-1}} \ \ \text{where}\ \ \mathbf{D} =  \sqrt{\operatorname{diag}(\mathbf{\Sigma})}$$ and then I generate correlated random variables $\mathbf{x}$ which have correlation matrix $\mathbf{R^{'}}$ , which is not exact same as $\mathbf{R}$ but is very close. I then convert $\mathbf{R^{'}}$ as above to get $\mathbf{\Sigma^{'}}$ which is close to original $\mathbf{\Sigma}$ . I am trying now to invert this $\mathbf{\Sigma^{'}}$ to get the $\mathbf{\Sigma^{'-1}}$ which I would expect is good approximation of $\mathbf{\Sigma}$ . However when I actually perform this procedure, I get a $\mathbf{\Sigma^{'}}$ that is indeed close to $\mathbf{\Sigma}$ but the inverse $\mathbf{\Sigma^{'-1}}$ is completely different from $\mathbf{\Sigma^{-1}}$ . It is not only the non-zero entries that are different, the pattern of zero is also completely different. I am doing the above to test out part of an algorithm for coursework assignment, but am not able to proceed because $\mathbf{\Sigma^{'-1}}$ is clearly not even close to $\mathbf{\Sigma^{-1}}$ . I understand from other questions on this site that it is not guaranteed that matrices close to each other will always produce inverses that is close. In this case is there procedure to invert $\mathbf{\Sigma^{'}}$ which will give a good approximation for $\mathbf{\Sigma^{-1}}$ ?","['numerical-linear-algebra', 'statistics', 'pseudoinverse', 'linear-algebra']"
3571621,What exactly is Cramer-Wold device? (What's the difference b/w Cramer-Wold theorem and Cramer-Wold device?),"I'm a bit confused about the exact statement of the Cramer-Wold device. I've read multiple sources which seem to be giving different definitions (the first one is more common). Random vectors $X_n \in \mathbb{R}^d$ satisfy $X_n \rightarrow_d X$ if and only if $\langle a, X_n \rangle \rightarrow_d \langle a, X \rangle$ for all $a \in \mathbb{R}^d$ . ( Source : Proposition 2.7) If $\langle a, X_n \rangle \rightarrow_d \langle a, X \rangle$ for all $a \in \mathbb{R}^d$ with $\|a\| = 1$ , then $X_n \rightarrow_d X$ . ( Source : Theorem 13) Also, What's the difference between Cramer-Wold theorem and Cramer-Wold device? Is there a difference?","['measure-theory', 'probability-limit-theorems', 'probability-theory']"
3571637,"Change of Coordinates and property of invertible matrices (Sec 2.4 Theorem 8, Hoffman Kunze, Linear Algebra)","I was reading Linear Algebra by Hoffman Kunze, and encountered this in the Theorem 8 of the Chapter Coordinates, the theorem is stated below : What I get about is in the most bottom line (uniqueness), it says $""... it\: is\: clear\: that $ $$ \alpha'_{i}=\sum_{i=1}^{n} P_{ij}\alpha_{i}."" \tag{a}$$ Is there any easy way to see why it is clear? I tried many ways and what I found was that we start from scratch, (starting the same with the proof, with $\scr \overline{B}$ and then we find an invertible matrix, say $Q$ , which we don't know equal to $P$ or not, such that the property (a) above holds with $Q_{ij}$ instead of $P_{ij}$ . Then now what we are left to show is that $P$ = $Q$ , and we currently have: $$ x_{i}=\sum_{j=1}^{n} P_{ij} x'_{j}\tag{from (i)}$$ and, $$ x_{i}=\sum_{j=1}^{n} Q_{ij} x'_{j}$$ So together, $$ \sum_{j=1}^{n} Q_{ij} x'_{j}=\sum_{j=1}^{n} P_{ij} x'_{j}$$ $$ \sum_{j=1}^{n} (Q_{ij} - P_{ij}) x'_{j}= 0$$ The way I showed that this implies that $P$ = $Q$ is by asserting that $P - Q \neq 0^{n\times n}$ and find a contradiction. Suppose $A := P - Q \neq 0^{n\times n}$ then we can choose a row $r$ such that the k-th entry is non-zero, we can plug in $0$ to any other entries other then the k-th and we are left with something non zero equals to $0$ which is a contradiction, so that $A = 0^{n\times n}$ and $P = Q$ , and since $(a)$ holds for $Q$ and $P = Q$ , it follows that $(a)$ holds for $P$ . I'm pretty sure that the proof is correct since we can plug in various values to $x'_{1}, ... , x'_{n} \in F$ , since $F$ is a field, it sure contains $0$ and $1$ , but this proof seems to be lengthy and is not as clear as how it was written to be by Hoffman and Kunze, I think I'm missing something here, and will be very thankful for a good explanation. Thanks!","['matrices', 'change-of-basis', 'linear-algebra']"
3571690,"Functions $f(x,y)$ such that $\left[f\left(x^{\frac{1}{t}},y^{\frac{1}{t}} \right)\right]^t$ does not depend on $t$?","Function $f(x,y)$ is defined for $x>0$ and $y>0$ . It satisfies the following property: $\left[f\left(x^{\frac{1}{t}},y^{\frac{1}{t}} \right)\right]^t$ does not depend on $t$ $\forall t\ne0,x>0,y>0$ . Obviously, $f(x,y)=x^{a_x}y^{a_y}$ satisfies this property. Are there any other functions like that?",['functions']
3571788,Are differential-algebraic equations more expressive than ordinary differential equations?,"I am interested in systems of differential-algebraic equations (DAE), i.e., systems of equations of the following form $$\dot{x} = f(x,y,t)\\0 = g(x,y,t)$$ I am confused about their relation to ordinary differential equations (ODE):
Are there functions that can be described by DAEs but not by ODEs? I.e., functions that are a solution for some DAE but not for any ODE?",['ordinary-differential-equations']
3571804,Convergence of Random Series; Determine Limit Infimum,"Let $(Y_{n})_{n=1}^{\infty}$ be independent random variables such that $P(Y_{n} = n - 1) = \frac{1}{n}$ and $P(Y_{n} = - 1) = 1 - \frac{1}{n}$ . Check that $E[Y_{n}] = 0$ and $X_n := \sum_{i=1}^{n} Y_{n}$ is a martingale. I would like to determine $\liminf X_{n}$ and $\limsup X_{n}$ . By second Borel-Cantelli lemma, we have $$P(Y_{n} = n-1 \text{ i.o.}) = 1 = P(Y_{n} = -1 \text{ i.o.}).$$ The following argument shows that $\limsup_{n\to\infty} X_{n} = + \infty$ a.s. Let $T_{0} \equiv 1$ and $T_{k} = \inf\{ n > T_{k-1} : Y_{n} = n - 1\}$ . Check that $T_{k}$ are a.s. finite (strictly increasing) stopping times, and that $$
		X_{T_{1}} = Y_{1} + \dots + Y_{T_{1} - 1} + Y_{T_{1}} = 1 = T_{0},
	$$ because $Y_{1} \equiv 0, Y_2 = \dots = Y_{T_{1} - 1} = - 1$ and $Y_{T_{1}} = T_{1} - 1$ . Inductively, we can observe that $$
		X_{T_{2}} = X_{T_{1}} + (X_{T_{2}} - X_{T_{1}}) =  T_{0} + T_{1}
	$$ and $$
		X_{T_{n}} = \sum_{k=0}^{n-1} T_{n} \geq n
	$$ since $T_{k}$ is strictly increasing and $T_{0} = 1$ . It follows that $\limsup_{n\to\infty} X_{n} = + \infty$ a.s. But it is unclear to me if $\liminf_{n} X_{n} = + \infty$ or $\liminf_{n} X_{n} = - \infty$ .",['probability-theory']
3571813,Curvature of canonical connection on 4-manifold with self-dual harmonic 2 form,"Let $X$ be an compact oriented Riemannian 4-manifold with $b_2^+\geq1$ . Let $\omega$ be a self-dual harmonic two form vanishing transversely. On $X\setminus \omega^{-1}(0)$ , the spinor bundle $S_+$ can be written as $E\oplus K^{-1}E$ , where $E$ is the $-\sqrt{2}i\lvert\omega\rvert$ -eigensubbundle of the clifford multiplication $c_+(\omega)$ ; $K^{-1}E$ is the $+\sqrt{2}i\lvert\omega\rvert$ -eigensubbundle. There is a Spin $^c$ structure such that $E$ is trivial, and a canonical connection $A_0$ on $K^{-1}$ (interpreted as the determinant line bundle) such that the induced spin covariant derivative satisfies that $\nabla_{A_0}(1,0)$ has zero $E$ component. My question : To establish the following inequality (at a neighbourhood of $\omega^{-1}(0)$ ) : $$\lvert F_{A_0}\rvert\leq Cdist(\cdot,\omega^{-1}(0))^{-2},$$ for some $C$ depending only on the Riemannian metric, $\omega$ , and the Spin $^c$ structure. My attepmt : Since $\omega$ vanishes transversely, I am quite sure the distance function must comes from the inequality $$\lvert\omega\rvert\geq Cdist(\cdot,\omega^{-1}(0)).$$ I tried to develop a differential inequality starting with $d^*d\lvert F_{A_0}\rvert$ and play with the Weitzenbock formula (on forms), however I cannot relate this with $\lvert\omega\rvert$ . On the other hand, I tried to start from $\nabla_{A_0}\nabla_{A_0}\psi$ for some specific spinor $\psi$ , however I cannot single out $\lvert F_{A_0}\rvert$ by solely playing with the defining condition on $\nabla_{A_0}(1,0)$ . In fact, $K^{-1}$ can be identify with the anti-canonical bundle of the symplectic manifold $(X\setminus \omega^{-1}(0),\omega)$ , however I am not sure whether this is useful or not. Background : This is an inequality appearing in a paper of Taubes in 1999, on the relationship between Seiberg-Witten theory and nearly symplectic geometry. Taubes stated the inequality without any further explanation. The bound helps proving $\lvert F_{A_0}\rvert$ is integrable over all of $X$ (even crossing $\omega^{-1}(0)$ ), which is a minor part needed so that the curvature induces a well-defined current over whole $X$ . Any help is appreciated!","['symplectic-geometry', 'connections', 'spin-geometry', 'gauge-theory', 'differential-geometry']"
3571823,Partial trace and preserving positive semidefiniteness,"This question is related to topics in quantum information but I will present it as a linear algebra question here. Consider some matrix $\delta_{AB}$ that lives in a bipartite Hilbert space $H_A\otimes H_B$ . The partial trace map traces over one of the Hilbert space so we obtain the reduced matrices as below $$\delta_A = \text{Tr}_B(\delta_{AB}), \, \, \delta_B = \text{Tr}_A(\delta_{AB})$$ Denote $A\geq B$ to mean that $A-B$ is positive semidefinite. Let $\rho_{AB}$ and $\sigma_{AB}$ both be positive semidefinite matrices with trace $1$ . Let $\lambda, \lambda_1, \lambda_2$ each be the smallest real numbers such that the following relationships hold $$\rho_{AB} - \lambda\sigma_{AB} \geq 0$$ $$\rho_{A} - {\lambda_1}\sigma_{A} \geq 0$$ $$\rho_{B} - {\lambda_2}\sigma_{B} \geq 0$$ What is the relationship between $\lambda$ , $\lambda_1$ and $\lambda_2$ ? I'm looking for some kind of non-trivial inequality that relates all three. By partial tracing over the conditions above, one sees that $\lambda\geq \lambda_1$ and similarly $\lambda\geq\lambda_2$ but I feel that more can be said.","['positive-semidefinite', 'trace', 'matrices', 'linear-algebra', 'inequality']"
3571853,Dirichlet to Neumann operator in the unit ball with Fourier Analysis,"I am working on exercise in Fourier analysis, but it really confused me since it involves some differential equation. Define $B_{1}:=\{x\in\mathbb{R}^{2}:x_{1}^{2}+x_{2}^{2}<1\}$ . Note that $\partial B_{1}=\mathbb{S}^{1}$ . Let $f\in C^{\infty}(\mathbb{S}^{1})$ and $u$ be the harmonic extension of $f$ to $B_{1}$ . Let $\nu$ be the unit outer normal direction of $\mathbb{S}^{1}$ , we define the Dirichlet to Neumann operator $\mathcal{A}$ by $\mathcal{A}f:=\dfrac{\partial u}{\partial \nu}.$ (a) Let $\alpha>0$ , $\alpha\notin\mathbb{N}$ . If $f\in C^{\infty}(\mathbb{S}^{1})$ is a solution to $\mathcal{A}f+\alpha=e^{f},$ then show that $f$ must be a constant, i.e. $f=\log\alpha$ . (b) What happens if $\alpha\in \mathbb{N}$ ? For the first one, I tired to use Fourier expansion of $f$ to compute the Fourier coefficients but I failed... For the second one,  what is the difference between $\alpha\in\mathbb{N}$ and $\alpha\notin\mathbb{N}$ ? I am sorry for not giving enough details since I really don't have idea about this exercise.. Thank you! Edit 1: (Partial Solution) Okay, I figured out a proof for $\alpha\notin\mathbb{N}$ .  I also worked out some part of $\alpha\in\mathbb{N}$ but could not finish, so if anyone has a refined proof, please let me know. I believe if $\alpha\notin\mathbb{N}$ does not make the solution different, but the case of $\alpha\in\mathbb{N}$ is much much more complicated, and I could not prove it completely. I have answered my own post.","['fourier-analysis', 'ordinary-differential-equations', 'harmonic-analysis', 'partial-differential-equations', 'fourier-series']"
3571868,Find the Laplace Transform of $\sin\sqrt{t}$,"To find the Laplace Transform of $\sin\sqrt{t}$ , I use the general formula $F(s)=\int_0^\infty e^{-st}f(t)\,dt$ and I get that: $$\mathcal{L}[\sin\sqrt{t}]=\int_0^\infty e^{-st}\sin\sqrt{t}\,dt=\int_0^\infty e^{-st}\sin\frac{\sqrt{st}}{\sqrt{s}}\,dt$$ Now I make the substitution $st=u \implies dt=\frac{du}{s}$ $$\mathcal{L}[\sin\sqrt{t}]=\int_0^\infty \frac{e^{-u}}{s} \sin\left(\sqrt{\frac{u}{s}}\right)\,du=\frac{1}{s}\int_0^\infty e^{-u}\sin\left(\sqrt{\frac{u}{s}}\right)\,du=\frac{1}{s}\int_0^\infty e^{-u}\sum_{n=0}^\infty \frac{(-1)^n}{(2n+1)!}\frac{\sqrt u \cdot u^n}{\sqrt s \cdot s^n}\,du=\frac{1}{s\sqrt s}\int_0^\infty e^{-u}\sum_{n=0}^\infty \frac{(-1)^n}{(2n+1)!}\frac{\sqrt u \cdot u^n}{s^n}\,du$$ How to keep solving this integral?","['complex-analysis', 'calculus', 'laplace-transform']"
3571903,"How can I prove this version of the Martinkiewicz Zygmund Inequality, preferably from the Khintchine Inequality?","A text I was reading recently quoted the following version of the Martinkiewicz-Zygmund Inequality, for which I have not been able to find a proof: Let $X_1, \dots, X_n$ be centered, independent random variables. Then we have $$\mathbb{E} \left( \sum_{i=1}^n X_i\right)^p \leq C (\sqrt p)^p \mathbb{E} \left( \left(\sum_{i=1}^n |X_i|^2\right)^{1/2}\right)^p$$ A few notes: Cauchy-Schwarz does not work because it introduces a factor of $(\sqrt n)^p$ . It obviously fails if the $X_i$ are not independent or not centered. It is strongly reminiscent of the Khintchine inequality, which states that if $\epsilon_i$ are independent Rademacher random variables (ie, taking the value $\pm 1$ with 50/50 probability), and $x_i \in \mathbb{C}$ , then $$\mathbb{E} \left( \sum_{i=1}^n \epsilon_i x_i\right)^p \leq  C (\sqrt p)^p \left(\sum_{i=1}^n |x_i|^2\right)^{p/2}$$ there is a straightforward generalization of this inequality to the Martinkiewicz-Zygmund inequality above, provided the $X_i$ are not just centered but symmetric. However, I have not been able to prove the full strength statement. One potential approach I thought would work would be to extend the proof of the Khintchine inequality to work not just for scaled Rademacher random variables but for any centered random variables taking 2 values, but this has proven difficult. Any help would be appreciated. I would just like to see a proof of the referenced Martinkiewicz-Zygmund inequality, but I would especially like it if it was a corollary of the Khintchine inequality. EDIT: I've decided to include some of my approach, so that maybe someone can just finish it up. If we try to extend the proof of the Khintchine inequality to work not just for scaled Rademacher random variables but for any centered random variables taking 2 values, following the idea of proof in http://people.seas.harvard.edu/~minilek/cs229r/fall15/lec/lec11.pdf , then it suffices to show that for each $X_i$ , which is centered and only takes 2 values, we can find a random variable $g_i$ such that $g_i$ is independent of $X_j$ for all $j$ (including $j=i$ ) and of every $g_j$ for $j \neq i$ , $g_i X_i$ is normally distributed with the same variance as $X_i$ , and $\mathbb{E} g_i >b >0$ for some constant $b$ independent of $n$ , $p$ , or the distributions of any $X_i$ or $g_i$ . But I have had difficulty in proving the existence of such a collection of $g_i$ 's.","['probability-theory', 'probability', 'real-analysis']"
3571913,Logical equivalence of $x \in \{y\}$,"Is it correct to state that $x \in \{y\}$ is logically equivalent to $x=y$ , i.e. $\forall z[z \in x \leftrightarrow z \in y]$ ? This idea seems a bit odd to me, because, if it is true, then $x \in \{y\} \leftrightarrow y \in \{x\}$ would be true as well.",['elementary-set-theory']
3571927,"Two players alternatively shoot a target, first person who hits two consecutive shots win.","Two players take turns shooting at a target, with each shot by player $i$ hitting the target with probability $p_i$ , $i=1,2$ . Shooting ends when two consecutive shots hit the target. What is the probability that the player who shoots first will win? I understand this problem on a more simple level, when the win condition is only one hit, although I do not know how to solve it given two consecutive hits.",['probability']
3571961,Probability of one sample mean being maximal among a set of other sample means,"Let $\mathcal{D}$ be a distribution, and consider a finite set of sample means $x_i$ of $k_i$ draws from $\mathcal{D}$ ( $k_i$ can be different for each mean). I want to show that if $x_i$ and $x_j$ have $k_i\leq k_j$ draws, $\mathbb{P}(x_i =\max_\ell x_\ell)\geq \mathbb{P}(x_j =\max_\ell x_\ell)$ . I also suspect that if $\mathcal{D}$ meets some kind of regularity condition, then if the first inequality is strict, so is the second. I know that sample variance decreases as $\dfrac{var(\mathcal{D})}{k_i}$ , but I'm not sure how to prove the full result. EDIT: Not true in general. Is it true for the uniform distribution on $[0,1]$ ? If so, why? Here's the distribution of sample means for uniform $\mathcal{D}$ : For $k_1=1,k_2=3,k_3=10$ , we have $x_1$ maximal about 42% of the time, $x_2$ maximal about 31% of the time, and $x_3$ maximal about 27% of the time. So far, numerical evidence supports the conjecture for the following distributions: uniform normal Bernoulli( $.5$ ) geometric( $.5$ ), geometric( $.1$ ) beta( $.5,.5$ ), beta( $2,5$ ) Numerical evidence contradicts the conjecture for: Bernoulli( $.2 $ ) exponential (the conjecture holds for $k=[1,5,10,27]$ but not for $k=[1,5,10]$ or $k=[1,10]$ ) library(ggplot2)
n = 50000 # number of draws for histogram

k = c(1,3,10)

out <- c()
for (i in 1:length(k)) {
  out[[i]] = replicate( n, mean( runif(k[[i]]) ) )
}

df <- data.frame(
  samples=factor(rep(k, each=n)),
  value=c(out, recursive=TRUE)
  )
p<-ggplot(df, aes(x=value, fill=samples, color=samples))+
  geom_histogram(position=""identity"", alpha=0.5, binwidth=.01)
p

# Count how often each sample mean is optimal among x1,...,x(length(k))
tallies = c(1:length(k))*0
for (i in 1:n) {
    samples <- c()
    for (j in 1:length(k)) {
      samples[[j]] = sample(out[[j]],1)
   }
   tallies[[which.max(samples)]] = tallies[[which.max(samples)]] + 1
}
tallies = tallies/n

print(k)
print(tallies)","['probability-distributions', 'probability']"
3571994,Hilbert space has countable basis,"Let $H$ be Hilbert space. I want to show that if $H$ has a countable orthonormal basis, then every orthonormal basis for $H$ must be countable. I spent almost a day, but could not solve this problem. I am trying to solve it with some kind of contradiction. I started by assuming that there exist an uncountable basis. Then I want to derive something crazy but cannot find it. Could you give me some hints or suggestions?","['hilbert-spaces', 'linear-algebra', 'functional-analysis', 'real-analysis']"
3572008,Find the height of a parallelpiped from its side lengths and angles?,"I am attempting to find the height of a v-fold for a popup book. I've been pondering this problem, and I think the best way to abstract it is by imagining that I'm actually dealing with a full parrallelpiped (instead of the half one that actually exists) and then attempt to find the height of it. In my research I have found one formula ( here ) for calculating the height: h = ‚à• c ‚à•|cosœï| Where c is the vector of the ""vertical"" side and œï is the angle between the vector c and the vertical. That all makes sense, but it's not very helpful since I don't know the angle œï And I don't have enough experience to know how to calculate it. As for my level of knowledge: I am an undergraduate student, and I have not yet taken a course in 3D Geometry. I would love to see theorems (since I'm quite interested in math). But I would also really appreciate a formula/algorithm for calculating the height given the side lengths and angles between them, if that is possible. Thank you for your time!
--Beka","['solid-geometry', 'trigonometry', 'geometry']"
3572015,Cardinality of integral domain equals cardinality of its field of quotients?,"Let $D$ be an integral domain and $F$ be its field of quotients. Let $[(x, y)]$ denote the equivalence class of $(x, y)$ . If $D$ is finite, then $D \simeq F$ , so $|D| = |F|$ . If $D$ is infinite, then the function mapping from $D$ to $F$ given by $d \mapsto [(d, 1)]$ is an injection. But what function is an injection from $F$ to $D$ ? I know there is an injection from $F$ to $D \times D$ given by $[(p, q)] \mapsto (p, q)$ , but is there an injection from $D \times D$ to $D$ ? Is it even true that $|F| = |D|$ in this case? Is it true that $|S \times S| = |S|$ for any infinite set $S$ ? If any of these are true, where could I find a proof?","['elementary-set-theory', 'abstract-algebra']"
3572016,How does this L1 regularization derivation follow? (Proof it makes sparse models),"I'm reading the ""Deep Learning""(Goodfellow et al, 2016) book and on pages 231-232(you can check them here ) they show a very unique proof how L1 regularization makes model sparse. You can skip to the last two expressions for the actual question, but some context if you want it: The regularized objetive function $\tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})$ is given by: $$ \tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha\|\boldsymbol{w}\|_{1}+J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})$$ where $\boldsymbol{w}$ is the parameter vector and $\boldsymbol{X}, \boldsymbol{y}$ are the design matrix(inputs) and the outputs of the model, respectively. If we make a second order Taylor series approximation of the unregularized loss function of our model, which we will assume it is a linear model to ensure it has a clean analytical solution we have $$\hat{J}(\boldsymbol{\theta})=J\left(\boldsymbol{w}^{*}\right)+\frac{1}{2}\left(\boldsymbol{w}-\boldsymbol{w}^{*}\right)^{\top} \boldsymbol{H}\left(\boldsymbol{w}-\boldsymbol{w}^{*}\right)$$ where $\boldsymbol{H}$ is the Hessian matrix of $J$ with respect to $\boldsymbol{w}$ evaluated at $\boldsymbol{w}^{*}$ and there is no Ô¨Årst-order term in this quadratic approximation because $\boldsymbol{w}^{*}$ is deÔ¨Åned to be a minimum. The minimum of $\hat{J}$ occurs where its gradient $$\nabla_{\boldsymbol{w}} \hat{J}(\boldsymbol{w})=\boldsymbol{H}\left(\boldsymbol{w}-\boldsymbol{w}^{*}\right)$$ If we make the further assumption that the Hessian is diagonal $\boldsymbol{H}=\operatorname{diag}\left(\left[H_{1,1}, \ldots, H_{n, n}\right]\right), $ where each $ H_{i, i}>0$ (more details on the book), we have that our quadratic approximation of the L1 regularized objective function decomposes into a sum over the parameters: $$\hat{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=J\left(\boldsymbol{w}^{*} ; \boldsymbol{X}, \boldsymbol{y}\right)+\sum_{i}\left[\frac{1}{2} H_{i, i}\left(\boldsymbol{w}_{i}-\boldsymbol{w}_{i}^{*}\right)^{2}+\alpha\left|w_{i}\right|\right]$$ Then they say ( and this is where I don't see how they did it, specifically that max function ) The problem of minimizing this approximate cost function has an analytical solution(for each dimension i), with the following form: $$w_{i}=\operatorname{sign}\left(w_{i}^{*}\right) \max \left\{\left|w_{i}^{*}\right|-\frac{\alpha}{H_{i, i}}, 0\right\}$$ How did they get that expression? I've reached a similar one, but without that max function... For further insights, please refer to the book.","['normed-spaces', 'machine-learning', 'linear-algebra', 'derivatives', 'regularization']"
3572040,Prove that $\lambda$ is a homotopy equivalence.,"The  based mapping cylinder of $f$ , written $Cyl f$ , is the based pushout of the maps $f:X\to Y$ and $i_0:X\to X\wedge I_+$ . We know that maps $f\wedge 1:X\wedge I_+\to Y\wedge I_+$ and $i_0: Y\to Y\wedge I_+$ exist and that $f\wedge1 \circ i_0=i_0\circ f$ . This means there exists a base point preserving continuous map $\alpha:Cyl f\to Y\wedge I_+$ that makes the appropriate diagram commute. We call $f$ a based cofibration if $\alpha$ has a base point preserving continuous retract. Let $f:X\to Y$ be a based cofibration. We call $X\wedge I$ the cone on $X$ , where $I$ has basepoint $1$ . The mapping cone $Cf$ is the pushout of the maps $f$ and $i_0:X\to X\wedge I$ . We know that there exists a unique basepoint preserving continuous function $\lambda:Cf\to Y/f(X)$ induced by the  pushout. How would I prove that $\lambda$ is a homotopy equivalence? I was told that $f$ being a based cofibration was sufficient to prove this but I don't know how.","['general-topology', 'category-theory', 'algebraic-topology']"
3572048,"What, in general, can one say about $f^{-1}(f(S))$ and $S$? What about $f(f^{-1}(U))$ and $U$?","Let $f:X\rightarrow Y$ be a function from one set $X$ to another set $Y$ , let $S$ be a subset of $X$ , and let $U$ be a subset of $Y$ . What, in general, can one say about $f^{-1}(f(S))$ and $S$ ? What about $f(f^{-1}(U))$ and $U$ ? MY ATTEMPT (EDIT) One can say that $S\subseteq f^{-1}(f(S))$ . Indeed, let us take $s\in S$ . Then $f(s)\in f(S)$ . Consequently, $s\in f^{-1}(f(S))$ , since we have \begin{align*}
x\in f^{-1}(f(S)) \Longleftrightarrow f(x) \in f(S)
\end{align*} However the inclusion $S\supseteq f^{-1}(f(S))$ is false in general. Let us consider, for instance, that $f:\{0,1\}\rightarrow\{0\}$ is given by $f(0) = f(1) = 0$ . If we choose $S = \{0\}$ , one has that $f(S) = \{0\}$ . Consequently, $f^{-1}(f(S)) = \{0,1\}$ , which is different from $S$ . On the other hand, one has that $U\supseteq f(f^{-1}(U))$ . Indeed, if $u\in f(f^{-1}(U))$ , then one may claim \begin{align*}
u\in f(f^{-1}(U)) & \Longrightarrow u = f(x)\,\,\text{for some}\,\,x\in f^{-1}(U)\\\\
& \Longrightarrow (\exists x)(u = f(x))\wedge(f(x)\in U) \Longrightarrow u\in U
\end{align*} However the inclusion $U\subseteq f(f^{-1}(U))$ is not true in general. Consider, for instance, the function $f:\{0\}\rightarrow\{0,1\}$ such that $f(0) = 0$ . Consequently, for $U = \{0,1\}$ , one has that $f^{-1}(U) = \{0\}$ and, consequently, $f(f^{-1}(U)) = \{0\}$ , which is different from $U$ .","['functions', 'solution-verification']"
3572082,Show that $f^{-1}(U\cup V) = f^{-1}(U)\cup f^{-1}(V)$,"Let $f:X\rightarrow Y$ be a function from one set $X$ to another set $Y$ , and let $U,V$ be subsets of $Y$ . Show that (a) $f^{-1}(U\cup V) = f^{-1}(U)\cup f^{-1}(V)$ , (b) $f^{-1}(U\cap V) = f^{-1}(U)\cap f^{-1}(V)$ , (c) $f^{-1}(U\backslash V) = f^{-1}(U)\backslash f^{-1}(V)$ . MY ATTEMPT (a) According to the definition of inverse image, one has \begin{align*}
x\in f^{-1}(U\cup V) & \Longleftrightarrow f(x) \in U\cup V\\\\ & \Longleftrightarrow (f(x)\in U)\vee (f(x)\in V)\\\\
& \Longleftrightarrow (x\in f^{-1}(U))\vee(x\in f^{-1}(V))\\\\ & \Longleftrightarrow x\in f^{-1}(U)\cup f^{-1}(V) 
\end{align*} (b) Analogously, one has \begin{align*}
x\in f^{-1}(U\cap V) & \Longleftrightarrow f(x) \in U\cap V\\\\ & \Longleftrightarrow (f(x)\in U)\wedge(f(x)\in V)\\\\
& \Longleftrightarrow (x\in f^{-1}(U))\wedge(x\in f^{-1}(V))\\\\ & \Longleftrightarrow x\in f^{-1}(U)\cap f^{-1}(V).
\end{align*} (c) Finally, one has that \begin{align*}
x\in f^{-1}(U\backslash V) & \Longleftrightarrow f(x)\in U\backslash V\\\\
& \Longleftrightarrow (f(x)\in U)\wedge(f(x)\not\in V)\\\\
& \Longleftrightarrow (x\in f^{-1}(U))\wedge(x\not\in f^{-1}(V))\\\\ & \Longleftrightarrow x\in f^{-1}(U)\backslash f^{-1}(V).
\end{align*} Could someone double-check my answer or provide any comments?","['functions', 'solution-verification']"
3572147,What is the expected number of dice rolls to roll any number n times?,"I know that the expected number of rolls to roll a number x is 6, and from that I'm guessing that the expected number of rolls to roll an x n times is 6n. But I don't know the expected value if we're not looking for a specific x, but rather any number that gets rolled n times first.","['expected-value', 'probability-distributions', 'probability']"
3572151,Counting Questions for precalculus,"1) In a coin collection, each coin has some combination of the following characteristics: One of five different colors (white, black, silver, gold, copper) One of three different shapes (circle, square, hexagon) One of three letters imprinted on it (A, B, C) There is exactly one coin with each combination of characteristics. There is one black circle coin with an A on it, one gold square coin with a C on it, and so on. a. How many coins are in this collection? I think this is just the Fundamental Counting principle so it is just $5*3*3=45$ b. How many silver coins are in the collection? I'm confused on this one.. There are $3*2=6$ total outcomes of shapes and letters. Do I just multiply that by $5C1$ c. How many coins have the letter A on them? Again, I'm confused, is it: $3C1 * 5*3$ ? 2) Jeff and Caitlin are playing a game. Jeff chooses 4 balls from a bucket of 18 balls numbered 1 to 18. To win, Caitlin must correctly guess the numbers on the four balls. a. How many ways can Jeff choose four balls? $18C4$ b. What is the probability that Caitlin correctly guesses the four numbers? I'm confused about this too. Is it $1/18*1/18*1/18*1/18$","['permutations', 'combinatorics', 'probability']"
3572173,Find the maximum value of $\sin (A) + \sin( 2A )+ \sin (3A)$,"From the graph, it seems that the answer is 2.5. Can someone please help me to solve this problem manually by using trigonometric equations.","['trigonometry', 'functions']"
3572217,Evaluating $\int\limits_0^\infty x\operatorname{sech}^3x\ln(\operatorname{sech}x)\ dx$,"How to prove that $$I=\int_0^\infty x \operatorname{sech}^3x\ln{(\operatorname{sech}x)}\ dx=\frac{\pi^3}{32}+\frac{\pi}{8}\ln^22+\frac14(3+2G)-2\ \operatorname{Im}\operatorname{Li}_3(1+i)\ ?$$ This problem was proposed by a friend and here is his solution Using the identity $$\int_0^\infty x^a\operatorname{sech}^bx\ dx=\frac{2^b\Gamma(a+1)}{\Gamma(b)}\sum_{n=0}^\infty\frac{\Gamma(n+b)}{\Gamma(n+1)}\frac{(-1)^n}{(2n+b)^{a+1}}\tag1$$ which he proved by writing $$\operatorname{sech}^bx=\left(\frac{2e^{-x}}{1+e^{-2x}}\right)^b=2^b\sum_{n=0}^\infty {b+n-1\choose n}(-1)^n e^{-(2n+b)x}$$ and differentiating $(1)$ with respect to $b$ then setting $a=1$ and $b=3$ we get $$I=4\sum_{n=0}^\infty(n+2)(n+1)\frac{(-1)^n}{(2n+3)^2}\left(\ln2+H_{n+2}-H_2-\frac{2}{2n+3}\right)$$ $$=4\sum_{n=1}^\infty\frac{n(n+1)(-1)^{n-1}}{(2n+1)^2}\left(\ln2+H_{n+1}-H_2-\frac{2}{2n+1}\right)$$ $$=\sum_{n=1}^\infty\left((-1)^{n-1}+\frac{(-1)^n}{(2n+1)^2}\right)\left(\ln2+H_{n}+\frac{1}{n+1}-\frac32-\frac{2}{2n+1}\right)$$ $$=\left(\ln2-\frac32\right)(\Omega_1+\Omega_5)+\Omega_2+\Omega_3+\Omega_6+\Omega_7+2(\Omega_4+\Omega_8)$$ where $$\Omega_1=\sum_{n=1}^\infty (-1)^{n-1}=\frac12$$ $$\Omega_2\sum_{n=1}^\infty(-1)^{n-1}H_n=\frac{\ln2}{2}$$ $$\Omega_3=\sum_{n=2}^\infty\frac{(-1)^n}{n}=1-\ln2$$ $$\Omega_4=\sum_{n=1}^\infty\frac{(-1)^n}{2n+1}=\frac{\pi}{4}-1$$ $$\Omega_5=\sum_{n=1}^\infty\frac{(-1)^n}{(2n+1)^2}=G-1$$ $$\Omega_6=\sum_{n=1}^\infty\frac{(-1)^nH_{n+1}}{(2n+1)^2}=\frac{3}{32}\pi^3+\frac{\pi}{8}\ln^22-G\ln2+2\ \operatorname{Im}\operatorname{Li}_3(1+i)$$ $$\Omega_7=\sum_{n=1}^\infty=\frac{(-1)^n}{(2n+1)^2(n+1)}=2G-\frac{\pi}{2}+\ln2-1$$ $$\Omega_8=\sum_{n=1}^\infty\frac{(-1)^n}{(2n+1)^2}=\beta(3)-1=\frac{\pi^3}{32}-1$$ By combining theses results, the closed form of $I$ follows. My friend ( the answerer) and I are not happy with this approach as $\Omega_1$ and $\Omega_2$ are divergent series, so the question is how to void this issue? Still, he got the right closed form. The other question is about my following approach, $$I=\int_0^\infty x\operatorname{sech}^3x\ln(\operatorname{sech}x)\ dx=\int_0^\infty x\left(\frac{2e^{-x}}{1+e^{-2x}}\right)^3\ln\left(\frac{2e^{-x}}{1+e^{-2x}}\right)\ dx$$ $$\overset{e^{-x}=u}{=}-\int_0^1\frac{\ln u}{u}\left(\frac{2u}{1+u^2}\right)^3\ln\left(\frac{2u}{1+u^2}\right)\ du$$ $$\overset{u=\tan\theta}{=}-2\int_0^{\pi/4}\ln(\tan\theta)\sin^2(2\theta)\ln(\sin(2\theta))\ d\theta$$ $$\overset{2\theta\to\theta}{=}-\int_0^{\pi/2}\ln\left(\tan\frac{\theta}{2}\right)\sin^2(\theta)\ln(\sin(\theta))\ d\theta$$ From here, I took two different paths, the first one is using the Fourier series of $\ln\left(\tan\frac{\theta}{2}\right)=-2\sum_{n=0}^\infty\frac{\cos((2n+1)\theta)}{2n+1}$ and the second one is using $\tan\frac{\theta}{2}=\frac{\sin\theta}{1+\cos\theta}=\frac{1-\cos\theta}{\sin\theta}$ but both didn't work for me. Continuing my work or providing different ideas would be appreciated. Thank you","['integration', 'divergent-series', 'harmonic-numbers', 'calculus', 'sequences-and-series']"
3572275,Splitting set $S$ of size $n$ into two disjoint subsets,"Let $S$ be a set of size $n$ . For any $N$ subsets of $S$ of size $m$ , if $$N<2^{m-1}\,,$$ then prove that there exists a way to split $S$ into two disjoint subsets $A$ and $B$ (with $A\cup B=S$ ) such that none of the smaller $N$ subsets are subsets of either $A$ or $B$ . I have been trying to do this question for hours, but have gotten nowhere closer to the solution. Any hints as to how to begin this would be appreciated.","['probabilistic-method', 'combinatorics', 'discrete-mathematics', 'elementary-set-theory', 'probability-theory']"
3572301,Why are the numbers of two different permutations the same?,"Define $a(k,m,n)$ as the number of ordered sets $\sigma$ which are permutations of $\{m, m+1, \dots, m+n-1\}$ , such that $$\text{for all integers }j\text{ where }1 \leq j \leq n-1, \quad \frac{\sigma(m+j-1)}{k+j} \color{red}{>} \frac{\sigma(m+j)}{k+j+1},$$ where $\sigma(j)$ denotes the $j\text{'th}$ element of the ordered set $\sigma$ . Similalrly, define $b(k,m,n)$ as the number of $\sigma$ of $\{m, m+1,\dots, m+n-1\}$ such that $$\text{for all integers }j\text{ where }1 \leq j \leq n-1, \quad \frac{\sigma(m+j-1)}{k+j} \color{red}{\geq} \frac{\sigma(m+j)}{k+j+1}.$$ Is the following statement true? $$\text{For all integers }m\text{ where }0 \leq m \leq k+1,\quad a(k,m,n+1) = b(k,m,n).$$ Also, is the following true? $$\text{For all integers }m'\text{ and }m\text{ where }0 \leq m' \leq m \leq k+1, \quad b(k,m,n) = b(k, m-m', n+m').$$ Example 1 We have $a(0,1,4) = 3$ , since $$
\frac{2}{1} > \frac{3}{2} > \frac{4}{3} > \frac{1}{4}, \\
\frac{3}{1} > \frac{4}{2} > \frac{2}{3} > \frac{1}{4}, \\
\frac{4}{1} > \frac{3}{2} > \frac{2}{3} > \frac{1}{4}.
$$ We have $b(0,1,3) = 3$ , since $$
\frac{1}{1} \geq \frac{2}{2} \geq \frac{3}{3}, \\
\frac{2}{1} \geq \frac{3}{2} \geq \frac{1}{3}, \\
\frac{3}{1} \geq \frac{2}{2} \geq \frac{1}{3}.
$$ So $a(0,1,4) = b(0,1,3)$ . Example 2 We have $a(1,1,4) = 2$ , since $$
\frac{3}{2} > \frac{4}{3} > \frac{2}{4} > \frac{1}{5}, \\
\frac{4}{2} > \frac{3}{3} > \frac{2}{4} > \frac{1}{5}.
$$ We have $b(1,1,3) = 2$ , since $$
\frac{2}{2} \geq \frac{3}{3} \geq \frac{1}{4}, \\
\frac{3}{2} \geq \frac{2}{3} \geq \frac{1}{4}.
$$ So $a(1,1,4) = b(1,1,3)$ . Example 3 We have $a(1,1,6) = 5$ , since $$
\frac{3}{2} > \frac{4}{3} > \frac{5}{4} > \frac{6}{5} > \frac{2}{6} > \frac{1}{7}, \\
\frac{4}{2} > \frac{5}{3} > \frac{6}{4} > \frac{3}{5} > \frac{2}{6} > \frac{1}{7}, \\
\frac{5}{2} > \frac{6}{3} > \frac{4}{4} > \frac{3}{5} > \frac{2}{6} > \frac{1}{7}, \\
\frac{6}{2} > \frac{4}{3} > \frac{5}{4} > \frac{3}{5} > \frac{2}{6} > \frac{1}{7}, \\
\frac{6}{2} > \frac{5}{3} > \frac{4}{4} > \frac{3}{5} > \frac{2}{6} > \frac{1}{7}.
$$ We have $b(1,1,5) = 5$ , since $$
\frac{2}{2} \geq \frac{3}{3} \geq \frac{4}{4} \geq \frac{5}{5} \geq \frac{1}{6}, \\
\frac{3}{2} \geq \frac{4}{3} \geq \frac{5}{4} \geq \frac{2}{5} \geq \frac{1}{6}, \\
\frac{4}{2} \geq \frac{5}{3} \geq \frac{3}{4} \geq \frac{2}{5} \geq \frac{1}{6}, \\
\frac{5}{2} \geq \frac{3}{3} \geq \frac{4}{4} \geq \frac{2}{5} \geq \frac{1}{6}, \\
\frac{5}{2} \geq \frac{4}{3} \geq \frac{3}{4} \geq \frac{2}{5} \geq \frac{1}{6}.
$$ So $a(1,1,6) = b(1,1,5)$ . Other calculation results are written here . P.S. Define $a(j,k,m,n)$ as the number of ordered sets $\sigma$ which are permutations of $\{m, m+1, \dots, m+n-1\}$ , such that $$\sigma(m) = j$$ $and$ $$\text{for all integers }j\text{ where }1 \leq j \leq n-1, \quad \frac{\sigma(m+j-1)}{k+j} \color{red}{>} \frac{\sigma(m+j)}{k+j+1},$$ where $\sigma(j)$ denotes the $j\text{'th}$ element of the ordered set $\sigma$ . Similalrly, define $b(j,k,m,n)$ as the number of $\sigma$ of $\{m, m+1,\dots, m+n-1\}$ such that $$\sigma(m) = j$$ $and$ $$\text{for all integers }j\text{ where }1 \leq j \leq n-1, \quad \frac{\sigma(m+j-1)}{k+j} \color{red}{\geq} \frac{\sigma(m+j)}{k+j+1}.$$ Is the following statement true? $$\text{For all integers }m\text{ where }0 \leq m \leq k+1,\quad a(j+1,k,m,n+1) = b(j,k,m,n).$$ Example 4 $a(2,0,1,4) = 1, a(3,0,1,4) = 1, a(4,0,1,4) = 1.$ $b(1,0,1,4) = 1, b(2,0,1,4) = 1, b(3,0,1,4) = 1.$ Example 5 $a(3,1,1,4) = 1, a(4,1,1,4) = 1.$ $b(2,1,1,4) = 1, b(3,1,1,4) = 1.$ Example 6 $a(3,1,1,6) = 1, a(4,1,1,6) = 1, a(5,1,1,6) = 1, a(6,1,1,6) = 2.$ $b(2,1,1,5) = 1, b(3,1,1,5) = 1, b(4,1,1,5) = 1, b(5,1,1,5) = 2.$ Other calculation results are written here .","['permutations', 'combinatorics']"
3572310,"Weak convergence on $L^\infty([0,1])$","It is well known that the dual space of $L^\infty([0,1])$ is pretty large and I do not really have a feeling for it to be honest. Currently, I am interested in the following question: Suppose that you have a bounded sequence $(f_n)_n$ in $L^\infty([0,1])$ that converges almost everywhere to some function $f \in L^\infty([0,1])$ . Does it follow that $f_n \to f$ weakly? I know that $\langle f_n, g \rangle \to \langle f, g \rangle$ for each $L^1([0,1])$ by Lebesgue's dominated convergence theorem but that suffice to show that $f_n \to f$ weakly? In particular, is $L^1$ dense in the dual space of $L^\infty$ ? If so, does the result still hold in for the Bochner space $L^\infty([0,1]; X)$ , where $X$ is some Banach space? It would be also nice to see some reference, if the answer to that question is well known.","['integration', 'measure-theory', 'reference-request', 'real-analysis', 'functional-analysis']"
3572432,Understanding the Giry monad,"I would like to understand the Giry monad, which is used to reason about probability in category theory. The issue is that I'm hitting a stumbling block understanding monads in general, in the category theoretical sense. This is partly because most of the information I can find about monads is in the context of functional programming. (I understand that in some sense those monads are the same kind of thing as the Giry monad, but I don't understand the relationship.) Some details about monads in the mathematical sense are given for example here , which I more or less understand, but there is still a rather large gap in between digesting the definition of a monad and being able to read Giry's paper. Aside from the technical nature of the paper, I think the issue is that the definition of a monad doesn't, by itself, help me to understand what to do with the concept. If I try to imagine how I might use the concept to reason about probability, I fail. Consequently, I'm wondering if a brief outline can be given, of how monads are used in practice in the context of mathematics rather than programming, and what the Giry monad is specifically, and why it's useful/interesting in relation to probability theory. I'm looking for an explanation in terms of fairly basic notions from category theory. (i.e. functors, natural transformations, products and coproducts, monoidal categories, that sort of thing.) If a complete and concise explanation can be given in elementary terms that would be great, but if that's not possible it would be helpful at least to have an overview of what it is and why it's useful, along with some pointers toward other resources.","['monads', 'probability-theory', 'category-theory']"
3572486,System of equations $x^3+y=y^2\ \& \ y^3+z=z^2\ \& \ z^3+x=x^2$,"Solve over reals: $$ \begin{cases} x^3+y=y^2\\ y^3+z=z^2\\ z^3+x=x^2\\ \end{cases} $$ I think the only solution is $x=y=z=0$ . If the variables are non-zero multiplying the equations: $$(x-1)(y-1)(z-1)=x^2y^2z^2 > 0$$ Here, there are two cases: all three variables are greater than $1$ or only one. If $x,y,z>1$ , it's easy to prove there are no solutions because $y^2>x^3$ a.s.o. gives $xyz<1$ . How can I prove there are no solutions when only variable, say $x$ , is greater than $1$ ?","['algebra-precalculus', 'systems-of-equations', 'symmetric-polynomials']"
3572565,Integrate $\int \frac{1}{1+x+x^4}dx$,"Find $$\int \frac{{\rm d}x}{1+x+x^4}.$$ WA gives a result, which introduces complex numbers. But according to the algebraic fundamental theorem, any rational fraction can be integrated over the real number field. How to do this?","['integration', 'real-analysis', 'calculus', 'indefinite-integrals', 'quartics']"
3572571,"Why is the ""solving for cubic equation roots general rule"" sometimes not applicable while the equation obviously has roots?","the general rule: we have $ax^3+bx^2+cx+d=0$ $\Delta_0=b^2-3ac$ $\Delta_1=2b^3-9abc+27a^2d$ $C=\sqrt{\Delta_1^2-4\Delta_0^3}$ $D=(\frac{\Delta_1+C}{2})^\frac{1}{3}$ $x=-\frac{1}{3a}(b+D+\frac{\Delta_0}{D})$ imagine $x^3-6x^2+11x-6=0$ we know its roots are $x=1$ , $x=2$ and $x=3$ . but when you use the general rule, you will find a negative $\Delta_1^2-4\Delta_0^3$ and thus; you can't continue the process! Also, when you use this rule, you will just find ONE real root (remember the last process to find final $x$ ); while that equation has 3 real roots and no imaginary roots. so how to find the other real roots using the general rule?!","['functions', 'linear-algebra', 'roots']"
3572620,Computing $\sum_{k=1}^{\infty}\frac{1}{k}\int_{\pi k}^{\infty}\frac{\sin(x)}{x}dx$,"I was asked to evaluate the following sum: $$\sum_{k=1}^{\infty}\frac{1}{k}\int_{\pi k}^{\infty}\frac{\sin(x)}{x}dx$$ I'm trying to use $$\int_{0}^{\infty} \frac{\sin(x)}{x}dx=\frac{\pi}{2}$$ However, it doesn't seem to work. Any help is greatly appreciated.","['sequences-and-series', 'real-analysis']"
3572648,Find the probability that no more than three attempts will be required to open the lock,"Of the five keys, one is suitable for the lock. The key that did not
  fit when trying to open the lock is put aside. We need to find the probability that no more than three attempts will
  be required to open the lock. I tried to use the following idea: Let's write down the events: A = lock opened by the first key B = lock opened by the second key C = lock opened by the third key Then we have probabilities: We have 1 good and 4 bad keys (5 total): P(A) = 1/5 = 0.2 Then we have the probability 4/5 that we take the wrong key. Now we have 1 good and 3 bad keys (4 total): P(B) = 4/5 * 1/4 = 1/5 = 0.2 Then we have the probability 3/4 that we take the wrong key. Now we have 1 good and 2 bad keys (3 total): P(C) = 4/5 * 3/4 * 1/3 = 1/5 = 0.2 This way we can sum it up and get a probability is 0.6. I fear that my solution is poor and unfounded, and I ask for help to valid and improve it. You can also show your solution with your thought",['probability']
3572689,How is defined the output of an exponential function when the input is not a rational number?,"Suppose $f$ is a function such that $f(x)= 2^x$ . Certainly, $f$ is defined when $x$ is an integer. It is also defined when $x$ is a rational number such that $x=\frac mn$ ( with $m$ and $n$ integers), for in that case we have an exponent rule telling us that : $2^\frac mn$ = $\sqrt[n]{2^m}$ . But since function $f$ is continuous on $R$ ( the set of real numbers), it must also be defined when the exponent $x$ is not a rational number, and hence cannot be written as a fraction. So for example, how is $f(\pi)=2^\pi$ defined?","['exponentiation', 'definition', 'algebra-precalculus', 'soft-question', 'exponential-function']"
3572711,"If $X_k$ are iid standard normal variables, there exists a constant $C$ such that for sufficiently large k $|X_k|\le C\sqrt{\log k}$","Question: Show that if $X_k$ , $k\in\Bbb N$ are iid standard normal variables, then there exists a constant $C$ such that for sufficiently large $k\ge2$ we have $|X_k|\le C\sqrt{\log k}$ almost surely. To me this isn't even true at first glance because the law of $X_k$ doesn't depend on $k$ and for any $r\in\Bbb R$ (taking $r$ as an arbitrary bound, in particular it can be equal to $C\sqrt{\log k}$ ) $\Bbb P(|X|> r)>0$ so $\Bbb P(|X|\le r)<1$ not matter how big we choose $C$ or $k$ . Am I wrong or is the question wrong?","['probability-theory', 'asymptotics', 'normal-distribution']"
3572718,Strong law of large numbers modified,"This is exercise 6 from Chung's book: course in probability theory page 137. If $(X_n)_n$ are independent and identically distributed with $E[X_1]=0$ and $(c_n)_n$ is a sequence of bounded real numbers, then $\frac{1}{n}\sum_{k=1}^n c_kX_k$ converges a.s to 0. He put a hint : Truncate $X_n$ at $n$ and proceed as in the proof of the SLLN. I succeeded solving the problem using his hint, but I have a question why to truncate $X_n$ at $n$ and not $c_nX_n$ at $n$ ? Also do we have the same result for the Marcinkiewics‚ÄìZygmund strong law of large numbers?","['probability-limit-theorems', 'probability-theory']"
3572721,Weighted sum of cosines with different phase offsets,"A finite sum of cosine functions weighted with different amplitude and phase, but with a fixed frequency, $$f(x) = \sum_{n=1}^{N}A_{n}cos(x+\phi_{n})$$ the question is if I were to fit $f(x)$ with $cos(x)$ , what will be the amplitude and phase offset?","['trigonometric-series', 'trigonometry']"
3572843,Find $\lim _{x\to \infty }\left(x\left(\arctan2x\:-\arccos\left(\frac{1}{x}\right)\right)\right)$,Find $$\lim _{x\to \infty }\left(x\left(\arctan(2x)-\arccos\left(\frac{1}{x}\right)\right)\right)$$ My idea was to let $t=\frac{1}{x}$ then I get $$\lim _{t\to 0 }...$$ but i did not know what to do with $\arctan(2x)$ any hint how to solve this ? thanks,"['limits', 'calculus']"
3572844,Prove or disprove: $f$ is an injection iff $\exists g \forall x\ g(f(x)) = x$ ($f$ and $g$ are total functions from D to D),"Problem Let $f:D \to D$ be a total function from some non-empty set $D$ to itself. $x$ and $y$ are variables ranging over $D$ , and $g$ is a variable ranging over total functions from $D$ to $D$ . Determine whether or not the proposition $\exists g \forall x\ g(f(x)) = x$ is equivalent to the proposition that $f$ is an injection. Attempt I claim that the proposition $\exists g \forall x\ g(f(x)) = x$ is equivalent to the proposition that $f$ is an injection. Proof: First, I show one direction of the equivalence: If $\exists g \forall x\ g(f(x)) = x$ , then $f$ is injective. I will show it by contrapositive: If $f$ is not injective, then, for every $g$ , there must be an $x$ such that $g(f(x)) \neq x$ . Assume $f$ is not injective, and $g$ is any total function from $D$ to $D$ . Since $f$ is not injective, there must be $x_0,x_1,y_0 \in D$ such that $x_0 \neq x_1$ and $f(x_0) = f(x_1) = y_0$ . Then, we have that: $g(f(x_0)) = g(y_0)$ and $g(f(x_1)) = g(y_0)$ . Note that, since $g$ is total, we know that $g(y_0)$ exists. From the above, $g(f(x_0)) = g(f(x_1))$ . To show that this implies that there must be an $x$ such that $g(f(x)) \neq x$ , there are two cases: Case 1.1. If $x_0 \neq g(y_0)$ , then $g(f(x_0)) = g(y_0) \neq x_0$ . So, $x_0$ is an $x$ such that $g(f(x)) \neq x$ . Case 1.2. If $x_0 = g(y_0)$ , then $g(f(x_1)) = g(y_0) = x_0$ . However, since $x_0 \neq x_1$ , this means that $g(f(x_1)) \neq x_1$ . So, $x_1$ is an $x$ such that $g(f(x)) \neq x$ . So, either way, there is an $x$ such that $g(f(x)) \neq x$ . Now, I show the other direction of the equivalence: If $f$ is injective, then $\exists g \forall x\ g(f(x)) = x$ . Assume $f$ is total and injective. I will construct a $g$ such that $g$ is total and $\forall x\ g(f(x)) = x$ . First, note that, since $f$ is total and injective, $f^{-1}$ is a surjective function, and $f^{-1}(x) = x$ for all $x$ in $f(D)$ . So, for all $x$ where $f^{-1}(x)$ is defined, $g$ can be defined as $f^{-1}(x)$ . However, $f^{-1}$ is not necessarily total, and $g$ needs to be total. To complete the definition of $g$ , we need to define it for all $x \in D$ for which $f^{-1}(x)$ is not defined. For example, we can define $g(x) = x$ for all $x$ such that $f^{-1}(x)$ is not defined. So, $g$ can be constructed as: $g = \left\{\begin{matrix}f^{-1}(x) & \text{ if }f^{-1}(x)\text{ is defined}\\x & \text{ if }f^{-1}(x)\text{ is not defined}\end{matrix}\right.$ Is this correct? Thank you in advance.","['solution-verification', 'discrete-mathematics']"
3572913,"Is it possible to approximate the natural logarithm by integrating $x^n$ with respect to $x$, where $n \approx -1$?","In general, $$
\int x^{n} dx = \frac{x^{n+1}}{n+1} + C
$$ However, this rule does not work when $n=-1$ because it leads to division by zero. Instead, $\int x^{-1} dx = \ln(|x|) + C$ . This standard result made me wonder if the natural logarithm could be approximated by using $n$ -values that are close to, but not equal to, $-1$ . For example, setting n  as equal to $-0.99$ : $$
\int x^{-0.99} dx = \frac{x^{0.01}}{0.01} +C = 100x^{0.01}+C
$$ The $C$ value that translated the graph so that it became a good approximation was $-100$ . Here is the general case. My suspicion is that as $k$ tends to infinity, the approximation becomes better and better: $$
\int x^{-1+1/k} dx = \frac{x^{1/k}}{1/k}+C=kx^{1/k}+C
$$ Using $-k$ as the constant of integration, I produced the following result: As you can see, it is difficult to see any difference between the two graphs. I have two questions: Why does $C$ have to equal $-k$ in order for the approximation to work? Can this approach be formalised by using limits? E.g. as $k \to \infty$ , $kx^{1/k}-k$ becomes arbitrarily close to $\ln(x)$ ?","['approximation', 'logarithms', 'real-analysis', 'indefinite-integrals', 'limits']"
3572959,The norm of the difference of two projections in a C$^\ast$-algebra,"I'm trying to show that if $p$ and $q$ are projections in a $C^*$ -algebra (i.e. $p=p^2=p^*$ ), then the norm of their difference is
  less than or equal to $1$ . Any help would be greatly appreciated!","['c-star-algebras', 'banach-algebras', 'functional-analysis']"
3573019,"Find $\int_0^\pi\int_0^{2\pi}\exp{\bigg[x\cos(\phi)\sin(\theta)+y\sin(\phi)\sin(\theta)+z\cos(\theta))\bigg]}\sin(\theta)\,d\phi \,d\theta$","I want to solve the following integration $$I = \int_0^\pi\int_0^{2\pi}\exp{\bigg[x\cos(\phi)\sin(\theta)+y\sin(\phi)\sin(\theta)+z\cos(\theta))\bigg]}\sin(\theta)\,d\phi \,d\theta $$ My Attempt: First solve the $\phi$ part $$I =
\int_0^\pi\exp{[z\cos(\theta)}]\sin(\theta) \Bigg[\int_0^{2\pi}
\exp\bigg[x\cos(\phi)\sin(\theta)+y\sin(\phi)\sin(\theta))\bigg] \, d\phi\Bigg] \, d\theta\\
I = \int_0^\pi\exp[z\cos(\theta)]\sin(\theta) \, d\theta  I_2
$$ where $$I_2 = \int_0^{2\pi}
\exp\bigg[x\cos(\phi)\sin(\theta)+y\sin(\phi)\sin(\theta))\bigg] \, d\phi$$ Nothing seems to work here. I have tried integration by parts and substitution method but both just keeps expanding the terms. How can I solve this. Please help.","['integration', 'definite-integrals']"
3573046,"Given $a_n=\sum_{k=1}^n\frac{k}{n^2}e^{k/n^2}$, find $a,b\in\Bbb R$ with $ \lim_{n\to\infty}(na_n-an)=b$.","It is given that: $$a_n=\sum_{k=1}^n \frac{k}{n^2} e^{\frac{k}{n^2}}$$ and $\lim\limits_{n\to \infty} a_n=\frac{1}{2}$ . I want to find reals $a,b$ so $ \lim\limits_{n\to \infty} (n a_n-a n)=b $ . Actually I was wondering if there is a theorem such as Stolz-Cesaro to solve it. I can't find something. Thank you","['limits', 'sequences-and-series']"
3573051,Find a generating function involving a tiling problem,"Let $h_n$ be the number of ways to tile a $1 \times n$ rectangle with $1 \times 1$ tiles that are red or blue and $1 \times 2$ tiles that are green, yellow, or white. Find a closed formula for $$H(x)=\sum_{n\geq0}h_nx^n.$$ I am unsure how to start this problem. Is there a way to solve this without using recurrence relations?","['recurrence-relations', 'combinatorics', 'discrete-mathematics', 'generating-functions', 'tiling']"
3573137,"Let $X_1, X_2, X_3$ be i.i.d exponential random variables with mean $1$. What is $\operatorname{Pr}(X_1 < X_2 < X_3)$?","Ive been working on this question and just want to know if i'm on the right track of if im completely off. The join pdf for the order statistic is $f_{x_{(1)}x_{(2)}x_{(3)}}(y_1,y_2,y_3)$ = $3!e^{-(y_1 +y_2 + y_3)} $ by integrating, i get the densities for $x_{(1)}$ and $x_{(2)}$ , and for $x_{(2)}$ and $x_{(3)}$ $f_{x_{(1)}x_{(2)}}(y_1,y_2)$ = $6(e^{-(y_1 + y_2)} - e^{-(y_1 + 2y_2)})$ $f_{x_{(2)}x_{(3)}}(y_1,y_2)$ = $6(e^{-(2y_2 + y_3)} - e^{-(y_2 + y_3)})$ We need to fine $Pr(X_1 < X_2 < X_3)$ $Pr(X_1 < X_2 < X_3)$ = $Pr(X_2 - X_3) - Pr(X_1 < X_2)$ where $Pr(X_2 < X_3)$ = $\int_{0}^\infty \int_{0}^{x_{3}} 6(e^{-(2y_2 + y_3)} - e^{-(y_2 + y_3)}) $ $Pr(X_1 < X_2)$ = $\int_{0}^\infty \int_{0}^{x_{2}} 6(e^{-(y_1 + y_2)} - e^{-(y_1 + 2y_2)}) $ Thus $Pr(X_1 < X_2 < X_3)$ = $\int_{0}^\infty \int_{0}^{x_{3}} 6(e^{-(2y_2 + y_3)} - e^{-(y_2 + y_3)}) $ - $\int_{0}^\infty \int_{0}^{x_{2}} 6(e^{-(y_1 + y_2)} - e^{-(y_1 + 2y_2)}) $ This is where im at so far. Is this correct?","['statistics', 'probability-distributions', 'order-statistics', 'probability']"
3573167,$L^p$ space is separable if and only if measure space separable,"I am currently dealing with the following question: Let $(E,\mathcal{A},\mu)$ be measure space, $p\in[0,+\infty)$ . Show that $L^{p}(\mu)$ has a countable dense set iff there exists $(A_n)\in \mathcal{A}^{\mathbb{N}}$ such that $\forall n\in \mathbb{N},\ \mu(A_n)<\infty$ and $\forall \epsilon>0,\ \forall A \in \mathcal{A}\ s.t.\ \mu(A)<\infty, \exists n\in \mathbb{N}\ s.t. \mu(A_n\Delta A)\leq \epsilon$ . For the first part, to show $L^p$ separable, I have a sketch of proof that I am not sure of: let $\mathcal{F} = \{ \text{finite sum of functions in the form } f_{n,k} = q_k \mathbb{1}_{A_n}\}$ , where $q_k\in \mathbb{Q}$ and $A_n$ is a member of the countable dense set. Then $\mathcal{F}$ would be dense in the set of integrable simple functions. By that simple functions are dense, we could reach that $\mathcal{F}$ is dense in $L^p$ . But for the converse way, I really have no idea. Thanks!","['measure-theory', 'lp-spaces', 'separable-spaces']"
3573169,Pattern of Nodes on a Graph,"For iteration $i=0$ , there exists one node on $(0,0)$ . For the next, and all subsequent iterations, nodes can be placed by the following rule. If there exists a node on $(a,b)$ , then one can choose to remove that node and add nodes at points $(a+1,b)$ and $(a,b+1)$ . If diagonal $m$ be defined by $a+b+1=m$ , generalize how many iterations are required for the first $n$ diagonals to contain no nodes. Further, find the maximum number of diagonals for which there exist no nodes in the diagonal or in lesser diagonals. I found this problem in the Hungarian Problem Book IV . Currently, I am unable to compute by hand any diagonal $n\ge 3$ , so I'm unable to find a pattern that way. I was thinking it could be mapped onto some form of binary tree, and thus I would be able to find some Catalan number that would fit the pattern, but I wasn't able to do this either. Thank you in advance for your help!","['graph-theory', 'number-theory', 'computational-mathematics', 'computational-geometry']"
3573195,Show a map from $L^p$ to $L^1$ is continuous,"I am working with the following map: Let $(E,\mathcal{A},\mu)$ be a measure space. $p\in[1,\infty)$ . Define $$\Phi:\begin{cases}
L^p(\mu)\to L^1(\mu)\\
f \to |f|^p
\end{cases}
$$ How could we show that this map is continuous? I was trying to write $\|\Phi(f+h) - \Phi(f)\|_1 = \int \left||f+h|^p - |f|^p\right| d\mu$ and I am stuck here.","['measure-theory', 'lp-spaces', 'lebesgue-integral']"
3573201,How strong is the Second Ratio Test?,"The Second Ratio Test is a relatively new convergence test, which we can give as Let $\sum_n a_n$ be a series with positive terms. For $k \in \{0,1\}$ we write $$ L_k = \limsup_{n \to \infty} a_{2n+k}/a_n , \quad l_k = \liminf_{n \to \infty} a_{2n+k}/a_n $$ If $ \max\{ L_0,L_1 \} < 1/2 ,$ $\sum_n a_n$ converges. If $ \min\{ l_0,l_1 \} > 1/2 , $ $\sum_n a_n$ diverges. Otherwise, the test is inconclusive. (The second condition can probably be replaced by $a_{2n+k}/a_n \geq 1$ for all sufficiently large $n$ if we want to be as strong as possible.) We recall that a convergence test A is stronger than one B if A resolves the convergence/divergence of any series that B resolves. (One may wish to separate the convergence test from the divergence test for greater specificity.) For example, we all know that the Root Test is stronger than the Ratio Test. A natural question is thus if the Second Ratio Test has a similar relationship to the Root Test, or other more venerable tests. Currently known: The Second Ratio Test resolves the convergence/divergence of the $p$ -series $\sum_n n^{-p}$ , for $p \neq 1$ anyway. The Root Test does not. The Second Ratio Test is stronger than the Ratio Test and Raabe's Test, at least on the convergence side. These are proved in this paper cited in the Wikipedia article . Also: The Second Ratio Test requires $a_n \neq 0$ , whereas the Root Test avoids this. But this still raises the question of whether one is stronger than the other in their common domain of validity. The Second Ratio Test is not as strong as the Condensation Test where both apply: it does not resolve $n^{-1} (\log n)^{-p}$ , whereas the Condensation Test does. But the Condensation Test requires a monotonically decreasing sequence of terms. In this case, there is a follow-up paper that discusses more general cases. More interestingly, Bertrand's Test (third in the De Morgan Hierarchy) does resolve $n^{-1} (\log n)^{-p}$ , so the Second Ratio Test is not stronger than Bertrand's Test. (For the definitions of the other tests mentioned above, the linked Wikipedia article has them.) So this leaves two questions: If $a_n>0$ , is the Second Ratio Test stronger than the Root Test? Is Bertrand's Test stronger than the Second Ratio Test?","['limsup-and-liminf', 'convergence-divergence', 'sequences-and-series']"
