question_id,title,body,tags
4029286,"Is there a strategy to win this ""move color"" game?","My girlfriend has been playing a game on her smartphone for a while. The rules are: there are $n+2$ tubes (with $n\ge 1$ ) and $n$ colored liquids. Two tubes are left empty and the others are filled with four doses of liquid. A tube cannot contain more than $4$ doses of liquid. There are exactly $4$ doses of each liquid. Liquids of different colors are non-miscible. The goal is to move liquids so that the $4$ doses of each liquid are in a single tube. Only the doses of liquid on the top of a tube can be moved. They have to be moved on top of a liquid of the same color. All doses of the same color at the top have to be moved at the same time. The only exception is when there is not enough space available when moving into a new tube. Only the doses of the same color that can be moved are then moved. For example, imagine that tube 1 contains three red doses and tube 2 contains a red dose above a blue one. In that case, two red doses from tube 1 can be moved to tube 2. Here is a picture that illustrates the problem (with $n=9$ ): For example, the pink liquid at the top of the fifth tube can be moved to an empty tube. After that, the two doses of red liquid from the same tube can be moved on the remaining empty tube. It is not possible to move only one dose of red liquid. Is there a general strategy to solve this problem? My intuition tells me that I could use induction (like the Hanoi towers problem), or graphs, or invariants (defining the degree of disorder of a configuration, the solution of the problem corresponding to a degree of disorder of $0$ ), but I don't really know how to get started in any of these directions. Note: I am not sure that it is possible to win from any starting configuration.","['puzzle', 'discrete-mathematics', 'algorithms']"
4029302,"What does the term ""unbiased estimator"" mean?","In my textbook for my statistics class, it says that $s^2$ , sample variance is a ""unbiased estimator"" for population variance, $\sigma^2$ . Does this mean that when we use $s^2$ as a point estimator for $\sigma^2$ , it precisely equals $\sigma^2$ ? So $s^2$ is not even an estimation/approximation for $\sigma^2$ ? What does unbiased estimator mean? Thank You!",['statistics']
4029310,"What algorithm is Newton using in the ""De analysi"" to extract the square root of a polynomial?","I'm reading a 1745 English translation of Newton's De analysi (apparently the most up-to-date there is, surprisingly). The Latin is here . In this tract he shows how to use the integral power rule for rational exponents (together with a sum rule: the area under a sum of curves being the sum of the areas) to find quadratures, and he gives examples in three types of how to deal with the case of integrating $y$ when it is not a polynomial: examples using long division e.g. $y=a^2/(b+x)$ , examples using the extraction of square roots e.g. $y=\sqrt{a^2+x^2}$ , and examples using ""the resolution of affected equations"" when $y$ is defined implicitly via a polynomial $f(x,y)=0$ . For the second example, $y=\sqrt{a^2+x^2}$ , he uses an algorithm to extract the root, but I don't recognize the algorithm. Here's an image: Note that Newton is not applying his binomial series: he is doing something else. I realize this is equivalent to the following procedure. Assume $y=\sum b_ix^i$ and we are given $y^2=\sum a_ix^i$ ; the goal is to solve for the $b_i$ . Using the Cauchy product, squaring the first equation gives $y^2=\sum c_ix^i$ where $c_i=\sum_{k=0}^ib_kb_{i-k}$ . Pattern matching the coefficients $c_i=a_i$ allows us to recursively solve for the $b_i$ , because $c_i$ is the first term to use $b_i$ . Thus $$c_0=b_0^2$$ $$c_1=2b_0b_1$$ $$c_2=2b_0b_2+b_1^2$$ $$c_3=2b_0b_3+2b_1b_2$$ $$c_4=2b_0b_4+2b_1b_3+b_2^2$$ etc. But Newton is clearly doing something else: drawing on some established algorithm with a visual representation. I don't think it's his algorithm. So: Three Questions How does the algorithm work? Does the algorithm have a name? Who first used the algorithm? EDITED After puzzling it out a little, I at least see the answer to my first question. Here is the procedure applied to $\sqrt{a^2+x^2}$ . Step 1: Guess $a$ . Store this as the latest estimate $S_0$ . Step 2: Square, giving $a^2$ , then subtract from the radicand: $x^2$ . Store this as the latest remainder $R_0$ . Step 3: Update $S_i$ to $S_{i+1}$ by adding a term $y_i$ to $S_i$ so that $2y_iS_0$ agrees with the lowest-degree term of $R_i$ . In this case, we want to add $y_0$ so that the lowest-degree term of $2y_0a$ agrees with $x^2$ . Hence $y_0=x^2/2a$ . The estimate $S_0$ is thus updated to $S_1=a+x^2/2a$ . Step 4: Multiply $y_i$ by $(2S_i+y_i)$ , then subtract from the last remainder and store the result as the latest remainder $R_{i+1}$ . In this case, we multiply $x^2/2a$ by $(2a+x^2/2a)$ , giving $x^2+x^4/4a^2$ . Subtracting from the last remainder gives $-x^4/4a^2$ . Step 5: Repeat steps 3/4. Just to illustrate for the next term, we want to add $y$ to that $2ya=-x^4/4a^2$ . Thus we add $y=-x^4/8a^3$ and the estimate is updated to $a+x^2/2a-x^4/8a^3$ . Now we multiply $$-\frac{x^4}{8a^3}\left(2\left(a+\frac{x^2}{2a}\right)-\frac{x^4}{8a^3}\right)=-\frac{x^4}{4a^2}-\frac{x^6}{8a^4}+\frac{x^8}{64a^6}$$ Subtracting from the previous remainder $-x^4/4a^2$ yields the new remainder $$\frac{x^6}{8a^4}-\frac{x^8}{64a^6}$$ and so on.","['calculus', 'algorithms', 'radicals', 'math-history', 'sequences-and-series']"
4029385,how to find the line with minimum sum of distances to multiple Points?,"I am programming on a little physics simulation right now. The mathematical problem i am facing now, is pretty hard for me, a 17 year old high school student. But since i really need to solve this one, i thought i might just ask if somebody knows something about the problem. The Problem: All in 2D. Given a certain number of points, find a line that approximates these points the closest. I know this sounds much like a regression line, but a regression line minimizes the y distances from line to points. I want to approximate not in a statistical sense like regression line, but a geometrical sense. I want to minimize the actual right angled shortest distances from points to the line. distances to minimize (red) The only solution i could think off was to make a function (f(m , h)) that takes the parameters of a line (y = mx + h) and gives the sum of distances squared. But finding a minimum of such a long multivariable function was not possible for me. Since this seems like basic stuff to me i thought this problem is well documented, but i could not find anything about it. Please redirect me if you know the name of this problem. Thanks.","['euclidean-geometry', 'maxima-minima', 'geometry']"
4029417,"Any neat way to solve the integral $\int_{-a}^a \int_{-b}^b\frac{1}{\left(x^2+y^2+z^2\right)^{3/2}}\,dxdy$?","Straight to the point: given the integral $$\iint_Q \frac{1}{\left(x^2+y^2+z^2\right)^{3/2}}\,dxdy$$ where $Q=[-a,a]\times[-b,b]$ , can you think of any neat way to solve it? At a first glance it looked quite innocent. No need to say that I’ve changed my mind. EDIT: Integrating first w.r.t. y, using the integral $$\int \frac{1}{\left(\xi^2+\alpha^2\right)^{3/2}}\,d\xi = \frac{\xi}{\alpha^2\sqrt{\xi^2+\alpha^2}} + \text{constant}, $$ one gets to $$ 2b \int_{-a}^a \frac{1}{\left(x^2+z^2\right)\sqrt{x^2+b^2+z^2}}\,dx $$ and here I get pretty stuck to be honest, so if someone could give me any hints that would be appreciated. Still, I think there should be some nicer way to approach this from the start. Since I was asked for the source of this problem: I simply asked myself a basic physics question, and trying to find an answer led to this integral.","['integration', 'multivariable-calculus', 'multiple-integral']"
4029450,Let $\{a_n\}$ be a positive sequence and $\sum_{n=1}^\infty a_n^2=A$ . Calculate $\lim_{N\rightarrow\infty}\frac{1}{\sqrt{N}}\sum_{n=1}^N a_n.$,"Let $\{a_n\}$ be a positive sequence and $\sum_{n=1}^\infty a_n^2=A$ . Calculate $$\lim_{N\rightarrow\infty}\frac{1}{\sqrt{N}}\sum_{n=1}^N a_n.$$ I have found that $$\Big(\frac{1}{\sqrt{N}}\sum_{n=1}^N a_n\Big)^2=\Big(\sum_{n=1}^N \frac{a_n}{\sqrt{N}}\Big)^2\leqslant\Big(\sum_{n=1}^N\frac{1}{N}  \Big)\Big( \sum_{n=1}^N a_n^2 \Big)=\sum_{n=1}^N a_n^2 \underset{N\rightarrow\infty}{\longrightarrow} A $$ by Cauchy Inequalty. But if I assume $a_n=\frac{1}{n}$ and $A=\frac{\pi^2}{6}$ , then $\lim_{N\rightarrow\infty}\frac{1}{\sqrt{N}}\sum_{n=1}^N a_n=0$ is not $A$ . I cannot find a way to prove the limit is $0$ . Thank you for helping!","['analysis', 'sequences-and-series']"
4029485,Can it be argued that most choices of $f:M^{\rm2} \mapsto \mathbf{\mathbb{R}^{\mathrm{3}}}$ will be mostly injective?,"It seems intuitively that if $f:M^{\rm2} \looparrowright \mathbf{\mathbb{R}^{\mathrm{3}}}$ for some choice of function(s), then most functions that are in some sense “well behaved” should produce a surface in $\mathbf{\mathbb{R}^{\mathrm{3}}}$ that is mostly injective. Is this intuition sensible?","['manifolds', 'general-topology', 'intuition']"
4029487,Comparison test for $\sum_{n=1}^{\infty} \frac{\sqrt{n^5 + n^3 - n}}{2n^3 + 3n^2 + 1}$,"I'm trying to use comparison test to show that this series diverges: $$\sum_{n=1}^{\infty} \frac{\sqrt{n^5 + n^3 - n}}{2n^3 + 3n^2 + 1}$$ Here are the steps I have come up with but I am not sure if it is right. $$\sum_{n=1}^{\infty} \frac{\sqrt{n^5}}{3n^3}\ \text{diverges (by integral test)}$$ $$0 < \frac{\sqrt{n^5}}{3n^3} \leq a_n\ \text{(for large n)}$$ $$\therefore a_n\ \text{diverges}$$ Note: I added ""for large n"" since according to Wolfram|Alpha, it is only true for n > 2.671... is this permissible or does it mean I have made an error?","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4029499,Number of points on the elliptic curve $y^2 = x^3 - x$ over $\mathbb{F}_q$,"I need to know the value of $\sum_{x \in \mathbb{F}_q} \chi(x^3 - x)$ , where $q = p^r$ with $p$ prime and $\chi$ is the Jacobi symbol. If we take $-x$ , we have $\chi((-x)^3 - (-x)) = \chi(-1)\chi(x^3 - x)$ and $\chi(-1) = -1$ since $q \equiv 3 \; (mod \; 4)$ . Thus, the terms for $x$ and the terms for $-x$ cancel. Therefore, $\sum_{x \in \mathbb{F}_q} \chi(x^3 - x) = 0$ . But, if $q$ is even or $q \equiv 1 \; (mod \; 4)$ , then $\chi(-1) = 1$ . How can I find the value of $\sum_{x \in \mathbb{F}_q} \chi(x^3 - x)$ when $q$ is even or $q \equiv 1 \; (mod \; 4)$ ?","['elliptic-curves', 'finite-fields', 'algebraic-geometry', 'abstract-algebra', 'group-theory']"
4029511,Why are there exactly eight convex polygons that can appear in a Penrose tiling?,"Problem Suppose that we have a P $3$ Penrose tiling , using rhombi of smallest angles $36^\circ$ and $72^\circ$ : Which convex shapes can be found as a union of rhombi in this tiling? The answer appears to be $8$ , as shown below: The regular decagon here can be formed in another way, but both have the same union so I am counting them once. (As a remark, these eight convex polygons must show up in any Penrose tiling, because all Penrose tilings contain every legal finite patch infinitely often within them.) Note that there are many convex polygons that could be formed from the Penrose rhombs, but it seems that only those pictured above can be extended to a tiling that respects the matching rules. Classification We can represent any convex polygon formed from Penrose rhombs by describing its side lengths along each of ten possible directions ( $0^\circ, 36^\circ, \ldots, 324^\circ$ ), since there are only $5$ orientations of undirected edges in a Penrose tiling. It is also easy to see (by considering what happens when we repeatedly cross from one edge of a rhombus to the opposite parallel edge) that opposite edges in such a polygon must be of the same length. Subject to these restrictions, it appears that the permissible cyclic orders of sides in such a polygon fall into one of two classes, if we normalize the rhombs to have unit edge length: All side lengths are either $0$ or $1$ (i.e. we have some kind of unit equilateral quadrilateral, hexagon, octagon, or decagon). All side lengths are either $1$ or $2$ , and no two sides of length $2$ touch. The above two categories exactly describe the eight convex polygons I have found. However, it is not apparent to me why this should be the case; why can't we have a polygon with sides in cyclic order $(2,1,2,0,0,2,1,2,0,0)$ , for instance? Many such polygons can be tiled with the rhombi, just in ways that turn out to be incompatible with the Penrose matching rules. Proof strategies It seems straightforward, if tedious, to classify all possible ways that one can legally arrange tiles along the border of a convex curve and discard those which prove incompatible with the P $3$ matching rules until all cases have been exhausted. However, I am hopeful that a more enlightening proof exists, given all the nice properties that Penrose tilings are known to have and the apparent simplicity of the resulting classification. One avenue of attack might be to show several local rules about Penrose tiles that together imply this classification. For instance (I believe both of these are in fact true): Three parallel edges cannot meet end-to-end (so a polygon cannot have edges of length $\ge3$ ). No vertex in the tiling is the endpoint of two different ""spokes"" of two end-to-end parallel edges (so edges of length $2$ cannot be adjacent). Some other properties? Put together, these might be able to show that no polygons other than the ones pictured are possible, but this sort of approach alone would not show existence for the cases that are possible; given that these configurations are inevitable in any tiling, it would be nice to see a natural proof that they must appear which does not manually construct each one and show that it fits into a valid Penrose tiling.","['convex-geometry', 'geometry', 'tiling']"
4029555,$\lim_{x\to 3} (x−3)f(x) = 0$ and $\lim_{x\to 3} f(x) = \infty$,I need help with a calculus/limit problem. I suppose I am looking for a function that: $f(x)$ such that it has $(x-3)^2$ in the denominator $f(x)$ such that $f(x)(x-3)$ equals $y=x-3$ . I struggle to understand how the above is possible. How is it possible to have a function that: $\lim_{x \to 3} f(x) = \infty$ $\lim_{x \to 3} (x-3)f(x) = 0$ Any help or pointers are deeply appreciated.,"['limits', 'calculus', 'functions', 'real-analysis']"
4029599,If $X$ and $Y$ are fundamental matrices then exists $C$ invertible such that $Y=XC$,"If $X(t)$ and $Y(t)$ are fundamental matrices then exists $C\in M_n$ invertible such thta $Y(t)=X(t)C$ Both for the homogeneous linear system $x'=A(t)x$ . attempt: Since $X$ is fundamental, it spans the solution space i.e., $$X'(t)=A(t)X(t)$$ but doing the same for $Y$ I go nowhere on finding $C$ invertible. I now that since $X$ is fundamental there is $t_0$ such that $\det(X(t_0))\neq 0$ , even more $\det(X(t))\neq 0$ for all $t$ , but again go nowhere.","['matrices', 'ordinary-differential-equations', 'fundamental-solution']"
4029632,How many different identifiers can a fictitious programming language have?,"How many different identifiers can a fictitious programming language have, when the following rules must be obeyed: Only upper case letters are allowed, so elements of $\{A, ..., Z\}$ , The identifier must be at least of length $1$ and at most length $5$ . The keywords AND, OR, IF, THEN and GOTO are not allowed to appear in any form in the identifier. I approached this by looking at the allowed lengths individually and then add all the combinations up. Length = 1:
Here we simply have only $26$ possibilities. Length = 2:
This time we have $26^2 - 2$ combinations, since IF and OR can't appear. Length = 3:
This one is also quite simple. We have $25^3 - 1 - 2*26 - 2*26$ combinations, since AND is not allowed to appear and all combinations that contain IF (IF. and .IF) or OR (OR. and .OR) are invalid. Length = 4:
This is where I start to have my problems. I tried to do the same thing here, but I am not quite sure. There are $26^4$ total combinations. This time THEN and GOTO can't appear, so $-2$ to accommodate this. AND also can't appear, so $-2*26$ . But for IF and OR I am not certain how to calculate it. At first I thought it would simply be $-3*26^2 - 3*26^2$ (IF.. , .IF. , ..IF , OR.. , .OR. , ..OR), but then I thought this would count certain combinations more than once. The combination IFOR for example would be counted twice, so I would have to add $1$ to make up for that. Then there are the combinations IFIF and OROR, which also would be subtracted twice so I would have to add $+1+1$ again. And I am not sure if I missed some of these. So for Length > 3 I am not quite sure how to tackle this, so I don't miss any combinations and everyone is counted only once.",['combinatorics']
4029636,In set theory what is A* and how does it differ from the power set of A?,"I am having a difficult time understanding the use of $A^*$ and how it differs from the usage of a power set. $A^*$ is defined as ""the set of all (finite) lists over A"" . I understand $\wp(A)$ to be the power set of A defined as ""the set of all subsets of A."" How is ""all lists over A"" different from ""all subsets of A"" ? Here are both definitions as denoted in my textbook: $$A^* := \{[a_1,...,a_n]\ |\ n \in \mathbb N;\ a_1,...,a_n \in A\}$$ $$\wp(A) = \{X\ |\ X \subseteq A\}$$",['elementary-set-theory']
4029693,Confusion colouring path graphs,"I'm a bit confused about how to count the number of colourings on a path graph. In my textbook, the following theorem is given: In general, if $G$ is a path on $n$ vertices, then $P(G,\lambda)=\lambda(\lambda - 1)^{n-1}$ Where $\lambda$ is the number of colours available. Obviously, this is the number of proper colourings of $P_n$ (the path graph on $n$ vertices), where no two adjacent vertices have the same colour. Am I correct that this also assumes that the graph is labelled? If the graph weren't labelled then the colouring $RGB$ would be the same as the colouring $BGR$ and shouldn't be counted twice. This brings me to my main question. I'm being asked to count the number of colourings of $P_n$ with the colours red, green, and blue, such that no two adjacent vertices are blue. I'm confused as to whether I should be assuming that $P_n$ is labelled or not? I know the theorem from the text isn't exactly relevant to my question but it assumes that $P_n$ is labelled, but my understanding of paths untill now had been that they're to be taken as unlabelled. I'm looking for advice on how to interpret and begin to approach this problem, and hopefully clear up some of my confusion. Thanks","['graph-theory', 'discrete-mathematics']"
4029779,Solve $\frac{dy}{dx}=\frac{y+y^2+x^2}{x}$,"Suppose I didn't know to begin with that $y=x\tan x$ satisfies the differential equation below; how I would I go about solving this differential equation? $$\frac{dy}{dx}=\frac{y+y^2+x^2}{x}$$ It seems very unlikely that I can use some clever manipulation to convert this into a differential equation that can be solved by separating the variables, and I also don't think that I'd be able to use the integrating factor method here. The only thing that comes to mind as possibly useful is to use some substitution, but I cannot yet see any substitution that is useful. Thank you for your help.","['calculus', 'derivatives', 'ordinary-differential-equations', 'substitution']"
4029832,Coefficients of $p\circ p$ for polynomial $p(x)=\sum_{k=0}^d a_kx^k$,"Let $p(x)=\sum_{k=0}^d a_kx^k$ be a polynomial of degree $d$ . Then \begin{align*}
(p\circ p)(x) &= \sum_{k=0}^da_k\left(\sum_{j=0}^da_j x^j\right)^k\\[3pt]
&=\sum_{k=0}^d \sum_{\substack{k_0+\cdots+k_d=k\\[3pt]k_0,\dots,k_d\geqslant0}}a_k\frac{k!}{k_0!k_1!\cdots k_d!} x^{k_1+2k_2+3k_3+\cdots+d k_d}\prod_{t=0}^da_t^{k_t}
\end{align*} by the multinomial theorem . Is there a nicer way I can express the coefficients of $p\circ p$ ? Where can I read more about this polynomial and study its properties?","['algebra-precalculus', 'polynomials']"
4029859,Are $SO(3)$ and $SU(2)$ abstractly isomorphic?,"The groups $SO(3)$ and $SU(2)$ are usually ""the"" classic example of nonisomorphic Lie groups whose corresponding Lie algebras are isomorphic. While I understand the isomorphism of their algebras, it was not obvious at all to me that we must have $SU(2) \not\cong SO(3)$ , I think one can conclude that by looking at the topological structure, for example the fact that their fundamental groups are nonisomorphic should suffice (I think ). What if we forget about the topology and consider them just as abstract groups? I've been trying to find some obstruction but all the facts I know about them, for example $SO(3) \cong SU(2)/Z(SU(2))$ , don't seem to be enough! Am I missing something obvious?","['algebraic-topology', 'group-theory', 'general-topology', 'lie-groups', 'differential-geometry']"
4029916,Question about the notation $\{x \in \mathbb{Z}: p(x)\}$,"A book I'm currently working through has the following exercise. $1.5$ Write each of the following sets in the form $\{x \in \mathbb{Z} : p(x)\}$ , where $p(x)$ is a property concerning $x$ . $$(a)\ A = \{-1, -2, -3,...\}\\
(b)\ B = \{-3, -2, ..., 3\}\\
(c)\ C = \{-2, -1, 1, 2\}
$$ I'm not exactly sure what the $p(x)$ is asserting here, do they want me to come up with a transformation for each $x$ by means of some function? The only way I can even conceive of answering these would be something like the following $(a)\ A = \{x\in \mathbb{Z}: x< 0\}$ But this isn't some transformation $p(x)$ . I don't know a function that is not piecewise who will take some $x$ and transform it to $-x$ but leaves $x$ positive if it's positive? $(b)\ B = \{x\in \mathbb{Z} : |x|\leq3\}$ $(c)\ C = \{x\in \mathbb{Z} : 0 \lt|x|\leq2\}$ Again neither of my solutions $(c)$ or $(b)$ are of the form $p(x)$ I'm just giving some hard rules on what x has to be here. If anyone has any ideas on how I can approach these differently I would appreciate it.
Regrettably, my book comes with exactly zero solutions.","['elementary-set-theory', 'solution-verification', 'analysis']"
4029983,A σ-algebra may have no non-empty atoms at all?,How to prove the fact that a $\sigma-$ algebra may have no non-empty atoms at all? Definition: An atom of a $\sigma$ -algebra $\mathscr{A}$ is a non-void set $\emptyset \neq A \in \mathscr{A}$ that contains no other set of $\mathscr{A}$ .,"['measure-theory', 'analysis', 'real-analysis']"
4029991,Is there a symbol for set operation $A \cup B^\complement$?,"Many of the 16 binary set operations (corresponding to the 16 logical connectives) don’t seem to have common dedicated symbols. Instead, they are often simply expressed in terms of $\cup, \cap, \setminus$ , etc. My question is whether there is any known precedent of a symbol for the operation $A \cup B^\complement$ . This would complete the following (where ? is a placeholder for the operation in question): $A \setminus B = A \cap B^\complement$ $A \operatorname{?} B = A \cup B^\complement$ For more context (based on discussion in the comments below): In everyday work, of course it makes sense to use just a few symbols (e.g. complement, $\cup, \cap, \setminus$ ) and express other operations in terms of these. This approach represents a practical tradeoff between two extremes: At one extreme, only one symbol is needed (just NAND or NOR, cf. functional completeness ). The drawback is that most expressions will be quite large when written this way. At the other extreme, we would have a unique symbol for each of the 16 operations. Expressions may be written more compactly with these symbols, but then there is more to memorize. (Why are the complement, $\cup, \cap, \setminus$ operations in particular given special status? My only guess is that they correspond with concepts in spoken language: NOT, OR, AND, WITHOUT, which, in turn, may correspond to which operations are most naturally computed by our brains, but I digress...) A comment below suggests simply using $(B \setminus A)^\complement$ to represent $A \cup B^\complement$ , but the same could be said of any operation, e.g. why do we need a symbol for set difference $A \setminus B$ when we can just write $A \cap B^\complement$ ? Or why do we need a special logical connective for implication (which actually corresponds to the set operation in question here)? My point is that sometimes things can be written more concisely with dedicated symbols for the uncommon cases. It may be rare that these symbols are needed, but in those instances where one of the uncommon set operations is heavily utilized ( $A \cup B^\complement$ in my case here), it would be nice to have some agreed-upon symbol. And here I simply want to know if there is any precedent in the literature for those weird cases. The diagram below (originally found on Wikipedia here or here ) shows that all 16 logical connectives have dedicated symbols. Why not extend this luxury to the corresponding set operations?","['elementary-set-theory', 'notation']"
4030035,Can there exist a unique solution to an initial value problem if the hypotheses of the existence and uniqueness theorem are not satisfied?,"I have been thinking about this question for a while. I haven't found a definite answer, but I am led to believe that there can be a unique solution to an IVP outside of interval of validity. I just fail to prove it.","['initial-value-problems', 'ordinary-differential-equations']"
4030076,Are Radon measures on Polish spaces $\sigma$-finite?,"If $\Omega$ is a Polish space and $\mu$ is a Radon measure on $\Omega$ (i.e. an inner-regular Borel measure), is $\mu$ $\sigma$ -finite? I know that Radon measures in general need not be $\sigma$ -finite, and $\sigma$ -finite measures need not be Radon. The standard counterexample to the former is an uncountable set with the discrete topology and counting measure (which is Radon, but not $\sigma$ -finite), and one counterexample to the latter is $\Omega = \mathbb R$ with $\mu$ the counting measure on $\mathbb Q$ (which is $\sigma$ -finite, but not Radon, or even Borel). But what if $\Omega$ is Polish? If $Q \subset \Omega$ is countable and dense, since $\mu$ is Borel, every $q \in Q$ has an open neighborhood $U_q \ni q$ for which $\mu(U_q) < \infty$ . But a priori , there's no guarantee that $W:=\bigcup_{q \in Q} U_q = \Omega$ . We know that by density of $Q$ in $\Omega$ , we have that $W^c$ has empty interior, but $W^c$ could still have infinite measure in principle (e.g. let $\mu = \lambda^2 + \lambda^1$ on $\mathbb R^2$ , where $\lambda^2$ is the $2$ -dimensional Lebesgue measure on $\mathbb R^2$ and $\lambda^1$ is the $1$ -dimensional Lebesgue measure on the $x$ -axis, and take $Q = \left\{(p,q) \in \mathbb Q^2 : p \neq 0\right\}$ ). And taking the closures $\overline U_q$ might not work because if $\Omega$ is an infinite-dimensional Banach space, for example, $\overline U_q$ need not be compact, so $\mu\left(\overline U_q\right) = \infty$ is possible. I'm really not sure one way or the other about the answer to this question. I can't think of a counterexample, but I can't think of a proof, either. Anything I'm not thinking of?","['polish-spaces', 'measure-theory', 'lebesgue-measure', 'real-analysis']"
4030113,Null Space of a Differential Equation,"I am studying the differential equations section of Calc 2 — Homogeneous Linear 2nd Order O.D.Es with constant coefficients (what a mouthful). The example given is $y''+2y'-8y=0$ , with general solution $y(x)=Ae^{2x}+Be^{-4x}$ . I was introduced to the idea that differentiation is a linear transformation in my linear algebra course. If we take the differential equation, $y''+2y'-8y$ to be our linear transformation, does that mean that the general solution, $y(x)$ , is our nullspace? Furthermore, could we say that a basis for our nullspace is the vectors $e^{2x}$ and $e^{-4x}$ . My course mentions nothing about this, but the ideas feel similar. Could anyone provide some insight into if these seemingly separate topics of study are connected? Any further reading on this link would be great.","['soft-question', 'linear-algebra', 'ordinary-differential-equations', 'linear-transformations']"
4030145,$f(x) = a_1^x + a_2 ^x +... + a_n^x$ is increasing and $a_1a_2...a_n = 1$,"I have encountered this problem in an Olympiad material provided by my teacher and I do not know how to deal with it.
If $a_1, a_2, \cdots, a_n > 0$ and $a_1a_2...a_n = 1$ , prove the function $f:[0, \infty] \to \mathbb{R}, f(x) = a_1^x + a_2 ^ x + ... + a_n ^ x$ is increasing. One of my friends managed to solve it using derivatives, but I would like an approach which does not involve calculus, since it is a 10th grade problem.","['contest-math', 'functions', 'exponential-function']"
4030222,"Continuity of $x^2y^2/(x^2+xy+y^2)$ at $(0,0)$","I want to check whether the function $f(x,y)=\frac{x^2y^2}{x^2+xy+y^2}$ with $f(0,0)=0$ is continuous at $(0,0)$ . But due to term $xy$ in addition to squares in the denominator, I am unable to proceed. Can one give some hint for it? I tried to find partial derivatives. Both partial derivatives are $0$ on $x$ as well as $y$ axis. But at other points, we get as: $f_x=\frac{x^2y^3+2xy^4}{(x^2+xy+y^2)^2}$ ; I couldn't get any direction to see whether this partial derivative w.r.t. $x$ is bounded near $(0,0)$ .","['multivariable-calculus', 'calculus', 'derivatives', 'real-analysis']"
4030276,How many solutions are there for $3x_1+3x_2+x_3+x_4=30$?,"Find how many solutions there are for $$ 3x_1+3x_2+x_3+x_4=30$$ I know how to solve this: $x_1+x_2+x_3+x_4=30$ and I read this link , but I am still not sure about my answer.
I wrote: $x_4+x_3=30-3(x_1+x_2) $ then I get this: $0 \le x_1+x_2 \le 10$ (that I know to solve: $66$ combination)
and now I'm not sure what to do with $x_3+x_4$ .","['contest-math', 'combinations', 'combinatorics', 'discrete-mathematics']"
4030282,"How to evaluate double limit of multifactorial $\lim\limits_{k\to\infty}\lim\limits_{n\to 0} \sqrt[n]{n\underbrace{!!!!\cdots!}_{k\,\text{times}}}$","Define the multifactorial function $$n!^{(k)}=n(n-k)(n-2k)\cdots$$ where the product extends to the least positive integer of $n$ modulo $k$ . In this answer , I derived one of several analytic continuations of this function to the real numbers, which is as follows $$x!^{(k)}=k^{x/k}\Gamma\left(1+\frac xk\right)\prod_{i=1}^{k-1}\left(\frac{ik^{-i/k}}{\Gamma(1+i/k)}\right)^{\sin(\pi(x-i))\cot(\pi(x-i)/k)/k}.$$ The limit of original interest $$F(k)=\lim_{x\to0}\,(x!^{(k)})^{1/x}=\left[\frac k{e^\gamma}\prod_{i=1}^{k-1}\left(\frac{\Gamma(1+i/k)}{ik^{-i/k}}\right)^{\pi(-1)^i\cot\frac{\pi i}k}\right]^{1/k}$$ is a straightforward consequence of the above result. Out of curiosity I plotted $F(k)$ and it appears that the double limit $$m=\lim_{k\to\infty}\lim_{x\to0}\,(x!^{(k)})^{1/x}$$ exists, converging rapidly to around $0.852$ when $k\in(s-1/2,s+1/2)$ for all positive odd integers $s$ , but converging much slower to the same value when $k\in(s+1/2,s+3/2)$ . We can rewrite the limit as \begin{align}m&=\lim_{k\to\infty}\exp\left(\frac{-\gamma+\log k+\pi\sum\limits_{i=1}^{k-1}(-1)^i\cot\frac{\pi i}k\log\left(\frac{\Gamma(1+i/k)}{ik^{-i/k}}\right)}k\right)\\&=\lim_{k\to\infty}\exp\left(\frac\pi k\sum\limits_{i=1}^{k-1}(-1)^i\cot\frac{\pi i}k\log\frac{\Gamma(1+i/k)}{ik^{-i/k}}\right)\\&=\exp\left(\lim_{k\to\infty}\frac\pi k\sum\limits_{i=1}^{k-1}(-1)^i\cot\frac{\pi i}k\log\frac{k^{i/k}\Gamma(i/k)}k\right)\end{align} using L'Hopital on $(-\gamma+\log k)/k$ . The term inside the exponential looks very much like a Riemann sum but I'm not sure where to go after that. It seems that none of the terms in the logarithm can be split additively to evaluate the limit as each component by itself is divergent. Is there a closed form for this double limit?","['factorial', 'real-analysis', 'gamma-function', 'closed-form', 'limits']"
4030289,Are inverse unique in unital algebra?,"Let $A$ be a unital algebra over a field $F$ with unity $1$ . If $a,b,c\in A$ such that $ab=ba=1=ac=ca$ , does this imply that $b=c$ ? We have $c(ab)=c$ . However, algebras are not necessarily associative so we can't conclude $c=c(ab)=(ca)b=b$ . Are there examples of algebras over a field with elements having multiple inverses?","['nonassociative-algebras', 'ring-theory', 'abstract-algebra', 'associativity']"
4030380,Null measure set and integrals,"Let $f,g:[a,b] \rightarrow \mathbb{R}$ be integrable functions and $$X=\{x \in [a,b];f(x) \ne g(x)\}$$ be a set with zero measure. Prove that $$\int_a^b f(x)dx=\int_a^b g(x)dx$$ and show two functions $f \ne g$ such that $X$ is infinite and has zero measure. How can i do this in a context of Riemann integration? I saw some answers in other questions but it wasn't very helpful.","['integration', 'measure-theory', 'real-analysis']"
4030392,"""Tooth"" function and its Lipschitz property","Suppose that $\varphi:[0,1]\to \mathbb{R}$ is given by $$\varphi(x) =
\begin{cases}
x, & \text{if } x\in[0,1/2],\\
1-x, & \text{if } x\in [1/2,1].
\end{cases}$$ Let's continue this function over $\mathbb{R}$ with period $1$ and label the new function as $f:\mathbb{R}\to\mathbb{R}$ . This function is used when we want to construct an example of continuous, nowhere differentiable function (van der Waerden function). I am going to prove that $|f(x)-f(y)|\leq C|x-y|$ and probably $C=1$ , i.e. $f(x)$ is Lipshitz  function. I have some difficulties to prove it. It is enough to consider the case when $|x-y|<\frac{1}{2}$ . Suppose that $x\in [n,n+1)$ . Then $x\in[n,n+\frac{1}{2})$ or $x\in [n+\frac{1}{2},n+1)$ . So suppose that $x\in[n,n+\frac{1}{2})$ Since $|y-x|<\frac{1}{2}$ , then $y\in (n-\frac{1}{2},n+1)$ . Then we can consider the following 3 cases: I. $y\in (n-\frac{1}{2},n)$ ; II. $y\in [n,n+\frac{1}{2})$ ; III. $y\in [n+\frac{1}{2},n+1)$ . For example, in the first case we know that: $f(x)=x-n$ and $f(y)=n-y$ . Then $|f(x)-f(y)|=|x-n-(n-y)|=|x+y-2n|$ . How to show that $|f(x)-f(y)|\leq |x-y|$ in that case? I'd be thankful for help!",['functions']
4030435,Trigonometry - worm thread related word problem,"The Question The machine tool diagram shows a symmetric worm
thread, in which a circular roller of diameter 1.5 inches sits.
Find the amount d that the top of the roller rises above the
top of the thread, given the information in the diagram. (Hint:
Extend the slanted sides of the thread until they meet at a point.) [ My Understanding I found w and h using basic trig For w $$\sin15°= \frac{\text{opposite}}{\text{hypotenuse}} = \frac{0.75}{w}$$ $$w = \frac{0.75}{\sin15°}$$ $$\sin15°= \frac{\sqrt{6} - \sqrt{2}}{4}$$ $$\Longrightarrow w = 2.897$$ For h $$\tan15° =  \frac{0.85}{h}$$ $$h = 3.172$$ $$h-w = 0.2744$$ I've been trying to find angles I could use, but nothing comes to mind, so I'll be grateful if you give me some hints. English is not my first language, so sorry for possible mistakes.","['word-problem', 'trigonometry']"
4030456,$ \sum_{n=1}^{\infty} \frac{f(n)}{n^2} < +\infty$ [duplicate],"This question already has answers here : Infinite series, injective function and rearrangement inequality (3 answers) Closed 3 years ago . Let $ f: \mathbb N  \to \mathbb N$ be a bijective function such that: $$ \sum_{n=1}^{\infty} \frac{f(n)}{n^2} < +\infty$$ Now my question is does any such $f$ exists?","['sequences-and-series', 'real-analysis']"
4030460,"Regarding Picard's iteration its relation to numerical methods, such as Euler's method.","I've been told numerical methods in solving ODEs, such as Euler's method and Runge-Kutta, are all in some way approximations to Picard's iteration, and I'm trying to understand how. Suppose we have a differential equation on an interval $[x_0,x_L]$ : $$\frac{dy}{dx}=f(x,y)$$ with initial condition $y(x_0)=y_0$ I would like to numerically solve the equation on a set of points $\{x_0<x_1<\dots<x_n\}$ , i.e. obtain approximations $y_i$ to the true solution $y(x_i)$ for each $x_i$ . Picard's iteration works as follows: $$y_{0,0}=y_0$$ $$y_{0,k}(x_1)=y_0+\int_{x_0}^{x_1} f(x,y_{0,k-1}(x))dx \;\; \mathrm{for} \;\; k \geq 1$$ suppose we stop for $k=m$ , then take $y_1=y_{0,m}(x_1)$ We then repeat the process for $i \geq 1$ . $$y_{i,0}=y_i$$ $$y_{i,k}(x_{i+1})=y_i+\int_{x_i}^{x_{i+1}} f(x,y_{i,k-1}(x))dx \;\; \mathrm{for} \;\; k \geq 1$$ $$y_{i+1}=y_{i,m}(x_{i+1})$$ So is the idea of a numerical method (e.g. Euler's method) to replace the integral $\int_{x_i}^{x_{i+1}} f(x,y_{i,k-1}(x))dx$ with an approximation such as $(x_{i+1}-x_{i})f(x_i,y_i)$ (Euler's method)? What I don't understand is why numerical methods only iterate once for each point $x_i$ (in other words, $m=1$ ) but Picard's iteration suggests you should iterate multiple (potentially many) times for each $x_i$ ?","['numerical-calculus', 'numerical-methods', 'ordinary-differential-equations']"
4030539,"Troubles calculating $\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx$","I want to evaluate the integral $$\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx$$ with the Cauchy principal value $\mathrm{p.v}$ . This integral converges, but just to be sure I also looked at the much simpler integral $$\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx<\mathrm{p.v.}\int_0^1\frac{x}{\pi}\frac{2}{1-2x}=-\frac{1}{\pi},$$ this confirms the initial integral to be finite (in the Cauchy sense). To find the antiderivative I tried using Leibniz's formula on $$\int\tan(ax)\,dx=-\frac{\ln\lvert\cos(ax)\rvert}{a}$$ to arrive at $$\int\tan(\pi x)x\,dx=\frac{d}{da}\left.\int\tan(ax)\,dx\right|_{a=\pi}=\frac{\ln\lvert \cos(\pi x)\rvert}{\pi^2} + \frac{x \tan(\pi x))}{\pi}.$$ Using the definition of the Cauchy principal value, namely \begin{align}
\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx &=\lim_{\varepsilon\to0^+}\left[\int_0^{\frac{1}{2}-\varepsilon}\tan(\pi x)x\,dx+\int_{\frac{1}{2}+\varepsilon}^1\tan(\pi x)x\,dx\right]\\[5pt]
&=\lim_{\varepsilon\to0^+}\left[\frac{\ln\cos\left\lvert\pi (\frac{1}{2}-\varepsilon)\right\rvert}{\pi^2} + \frac{(\frac{1}{2}-\varepsilon) \tan\left(\pi (\frac{1}{2}-\varepsilon)\right)}{\pi}\right.\\[5pt]
&\phantom{=\lim_{\varepsilon\to0^+}\left[\right.}\left.-\frac{\ln\cos\left\lvert\pi (\frac{1}{2}+\varepsilon)\right\rvert}{\pi^2} - \frac{(\frac{1}{2}+\varepsilon) \tan\left(\pi (\frac{1}{2}+\varepsilon)\right)}{\pi}\right]
\end{align} the limit diverges for $\varepsilon\to0^+$ . I then also tried to manipulate the integral to become $$\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx=\int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx\approx-0.22063560.$$ The value seems to be right, as I tried to evaluate the integral graphically, but I'd like to have the exact result. The other problem I encountered was that the antiderivative of the right integral also diverges for the upper limit: $$\int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx=\left[2\left(\frac{\ln\lvert \cos(\pi x)\rvert}{\pi^2} + \frac{x \tan(\pi x))}{\pi}\right)+\frac{\ln\lvert \cos(\pi x)\rvert}{\pi} \right]_0^{\frac{1}{2}}\to\infty.$$ I really don't understand where things went wrong and any help is highly appreciated. Edit: I forgot to differentiate $\tan(a x)$ -.-, so the antiderivative isn't correct. Therefore my next question would be, if $$\int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx$$ has a nice result. Edit2: After consulting computer algebra systems I got a result for the integral: $$\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx=\int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx=-\frac{\ln(2)}{\pi}.$$ I don't know how it did the evaluation, but it seems consistent with the numerics. Edit3: Ok I got it. If we start from $$\int_0^z\pi x\tan(\pi x)\,dx,$$ after recognizing that $\pi\tan(\pi x)=-\frac{d}{dx}\ln\cos(\pi x)$ , we can do integration by parts to arrive at $$-z\ln\cos(\pi z)+\int_0^z\ln\cos(\pi x)\,dx.$$ After some manipulations, we can recognize the Clausen function: \begin{align}&-z\ln(2\cos(\pi z))+\int_0^z\ln(2\cos(\pi x))\,dx\\=&-z\ln(2\cos(\pi z))+\frac{1}{2\pi}\int_0^{2\pi z}\ln\left(2\cos\Big(\frac{x}{2}\Big)\right)\,dx\\=&-z\ln(2\cos(\pi z))+\frac{1}{2\pi}\mathrm{Cl}_2(\pi-2\pi z).\end{align} Now going back to the original integral, we have \begin{align}\int_0^{z}\tan(\pi x)(2x-1)\,dx&=2\int_0^{z}\tan(\pi x)x\,dx-\int_0^{z}\tan(\pi x)\,dx\\&=-\frac{2z}{\pi}\ln(2\cos(\pi z))+\frac{1}{\pi^2}\mathrm{Cl}_2(\pi-2\pi z)+\frac{\ln\cos(\pi z)}{\pi},\end{align} which in the limit $z\to\frac{1}{2}$ gives $-\frac{\ln(2)}{\pi}$ .","['integration', 'calculus', 'definite-integrals', 'cauchy-principal-value']"
4030541,"Stopping times: bounded vs unbounded, example for a bounded stopping time","Part of Doobs Optional Stopping theorem is that for an almost surely finite stopping time $T$ and a Martingale $(X_n)_{n \in \mathbb{N_0}}$ it is enough to assume that T is bounded to get: $E[X_T] = E[X_0]$ We defined bounded as: There exists $N \in \mathbb{N}_0$ such that $P(T \leq N) = 1$ I encountered many examples of a.s. finite stopping times that are unbounded like e.g.: $T = min\{n \in \mathbb{N}|S_n = A\}$ for the simple random walk $S_n$ and some $A \in \mathbb{Z}$ . Please give an example of a bounded stopping time and explain what the difference is w.r.t. structure. (with structure I mean e.g.: first hitting times of random walks are usually unbounded) As John Dawkins commented: If $T$ is any stopping time, then for $n \in \mathbb{N}$ fixed, the $\min(n, T)$ is a bounded stopping time. An example for that would be to gamble until you won a certain amount or until the casino closes. A constant stopping time would of course also be bounded. I'm still hoping for a less trivial example though, if there is one. Thank you!","['stochastic-processes', 'stopping-times', 'probability-theory', 'martingales']"
4030549,A New Proof of Pythagorean theorem?,"A right triangle with side lengths $a,b,c$ where the hypotenuse( $c$ ) intercepts the height( $h$ ) such that $c$ is the sum of $2$ side lengths, $c = m + n$ . Image here: I found an elegant proof of Pythagorean theorem that seems to be related to Einstein's proof: First, slopes of $a$ and $b$ give: $$(\frac{h}{m})(\frac{h}{n}) = 1\implies h^2 = mn$$ Than with area formula, we find the intercept $(x,y)$ of the height and hypotenuse: $$bx = hm\implies x = \frac{am}{c}$$ $$ay = hn\implies y = \frac{bn}{c}$$ Next plugging this into the height's equation: $$\frac{a}{b}(\frac{am}{c}) = 
\frac{bn}{c}\implies a^2m = b^2n$$ Using $c = m  + n$ : $$\frac{c}{\sqrt{mn}} = \sqrt{\frac{m}{n}} + \sqrt{\frac{n}{m}}$$ $$\frac{c^2}{ab} = \frac{a}{b} + \frac{b}{a}$$ $$a^2 + b^2 = c^2$$ Q.E.D. Thoughts?","['proof-writing', 'solution-verification', 'geometry']"
4030564,2021 AMC 10 A Problem #22,"Question: Hiram's algebra notes are $50$ pages long and are printed on $25$ sheets of paper; the first sheet contains pages $1$ and $2$ , the second sheet contains pages $3$ and $4$ , and so on. One day he leaves his notes on the table before leaving for lunch, and his roommate decides to borrow some pages from the middle of the notes. When Hiram comes back, he discovers that his roommate has taken a consecutive set of sheets from the notes and that the average (mean) of the page numbers on all remaining sheets is exactly $19$ . How many sheets were borrowed? $\textbf{(A)} ~10\qquad\textbf{(B)} ~13\qquad\textbf{(C)} ~15\qquad\textbf{(D)} ~17\qquad\textbf{(E)} ~20$ Does anyone have a solution that's easy to understand? So here's what I've tried so far: I set $x$ as the first page his roommate took and $y$ as the total number of sheets his roommate took. I tried setting up an equation to find the average (19): $$
\frac{1275-\text{sum of the  pages his roomate took}}{50-2y}=19.
$$ 1275 is the sum of all the pages and 50-2y is the number of pages that are left. But I can't figure out how to express sum of the  pages his roommate took in terms of x and y","['contest-math', 'algebra-precalculus']"
4030644,Prove that the set of words (from a countable infinite alphabet) is countable,"Here is the question: Define a word as a finite string of letters from a countable infinite alphabet $\{ a_{1}, a_{2}, a_{3},...\}$ a) Prove that the set of three letter words is countable.
b) Prove that the set of all finite-letter words is countable. I'm having a little trouble with part a). My first instinct was to define a function $f: \mathbf{W} \to \mathbf{N}$ (where $\mathbf{W}$ is the set of three letter words) as $f(w) = ijk$ where $w \in \mathbf{W}, w = a_{i}, a_{j}, a_{k}$ for some $i,j,k \in \mathbf{N}$ . But this wouldn't be a bijection, since natural numbers can have multiple sets of factors so more than one word would map to the same natural number. I've seen a similar proof for words in the English language being countable, but the thing that's tripping me up here is the fact that the alphabet is infinite, so I don't think I could use the same method of proof as I would for a finite (English) alphabet. Am I on the right track here? Some guidance would be appreciated.",['elementary-set-theory']
4030657,Show that a set $ F $ is uncountable,"Let $$ \{a,b,c\}^n = \big\{(x_1,x_2,\dots,x_n): x_j \in \{a,b,c\} \big\}$$ and $$ \{a,b,c\}^{*} = \bigcup_{n=1}^{\infty} \{a,b,c\}^n.$$ Consider the set $ F = \big\{ f: \{a,b,c\}^{*} \to \{a,b,c\} \big\}$ and show that $ F $ is not countable. I've tried a couple of solutions: i) No bijection between $ F \to \mathbb{N} $ . Although I've tried to use Cantors diagonalizing argument I'm getting nowhere, since the arguments of $ f$ is such a mess. ii) $ F $ is clearly an infinite set. Then it's sufficient to show that $ \mid F \mid \neq \mathbb{N}^0$ (aleph-null) or $ \mid F \mid > \mathbb{N}^0 $ . How do I solve this?","['elementary-set-theory', 'elementary-number-theory']"
4030661,Suspicious diagrams on wiki about group-like structures,"It seems to me that the diagrams on wiki about group-like structures are not quite right. For example, the following https://en.wikipedia.org/wiki/Monoid#/media/File:Algebraic_structures_-_magma_to_group.svg that appears on the wiki page for monoid https://en.wikipedia.org/wiki/Monoid The problems I see are that (nonempty) quasigroup + associativity $\Longleftrightarrow$ (nonempty) group, strictly stronger than (nonempty) inverse semigroup; semigroup + divisibility (in the sense of how we define quasigroup) $\Longleftrightarrow$ group, also strictly stronger than inverse semigroup; inverse semigroup + identity $\nRightarrow$ group, the former is sometimes called an inverse monoid. Similar diagrams appear on other group-like structures, such as https://en.wikipedia.org/wiki/Quasigroup where the labels ""associativity"" and ""divisibility"" do not appear in the lower part of the diagram as in the diagram for monoid but
somehow imply similar things, and even if the arrows do not mean the said labels, they still do not make sense since an inverse semigroup
is not necessarily a quasigroup. Did I make any mistake or misunderstand the wiki pages?","['magma', 'monoid', 'quasigroups', 'semigroups', 'group-theory']"
4030689,What is the degree of an n-fold branched cover over a trefoil?,"The order-2 cyclic branched cover over a trefoil has degree 6, meaning the preimage of any point off the trefoil has cardinality six. (You can find a wonderful video of this here , made by Moritz Sümmermann.) The order-3 cyclic branched cover over a trefoil has degree 24. What is the formula for the degree of an order- $n$ cyclic branched cover over a trefoil? (I'm unsure of the proper terminology for this.) I'm pretty sure this is $|G_n|$ , where $$G_n=\langle x,y\mid xyx=yxy,~x^n=y^n=1\rangle;$$ however, I have no idea how to find the size of this group.","['knot-theory', 'general-topology', 'knot-invariants']"
4030756,Close-form for triple integral $ \int_0^c \int_0^b \int_0^a \sqrt{x^2+y^2+z^2} dx dy dz$,"I am able to work out the double integral $$\int_0^b \int_0^a \sqrt{x^2+y^2} dx dy $$ with brute-force (i.e. integrating $x$ , then $y$ ) to arrive at the close-form result $$\frac13ab\sqrt{a^2+b^2} +\frac16a^3\sinh^{-1}\frac ba +\frac16 b^3 \sinh^{-1}\frac ab$$ which has the expected parity between $a$ and $b$ . However, it gets unwieldy to tackle the triple-integral extension $$\int_0^c \int_0^b \int_0^a \sqrt{x^2+y^2+z^2} dx dy dz$$ this way and I am unable to slug it out. Does anyone know the corresponding close-form expression for the triple version?","['integration', 'multiple-integral']"
4030770,$\operatorname{Aut}(\mathbb{H})$ acts on circles,"Suppose $\mathbb{H}=\{z \in \mathbb{C}\mid \operatorname{Im}(z)>0\}$ .
Is it true that $\operatorname{Aut}(\mathbb{H})$ can map any circle in $\mathbb{H}$ to any circle in $\mathbb{H}$ (i.e. for any two circles $C_1,C_2$ with all their points in $\mathbb{H}$ there's some element of $\operatorname{Aut}(\mathbb{H})$ that carries $C_1$ to $C_2$ )? The only useful fact I know is that $f\in \operatorname{Aut}(\mathbb{H}) \Rightarrow f=\frac{az+b}{cz+d}, a,b,c,d \in \mathbb{R}, ad-bc \neq 0 $ . Thank you for any help!","['complex-analysis', 'circles', 'complex-numbers', 'mobius-transformation']"
4030819,Is a binary function a set of ordered pairs?,"The definition of a function, according to the book I have, is: a function is a set of ordered pairs, no two of which have the same first component . Thus an ordered pair in this set can be expressed as $\pmb{(a,b)}$ or $\pmb{(a, f(a))}$ . However, a binary function, from what I've read, implies a ternary relation, expressed as $\pmb{(a, b, f(a,b))}$ , which is not an ordered pair but a triplet. Thus does not satisfy the definition of a function? Shouldn't it be expressed as ( $\pmb{(a,b)}$ , $\pmb{f(a,b)}$ )? Because only then will it become an ordered pair. Makes sense because of the definition. If we defined the relation of sets $\pmb{A}$ , $\pmb{B}$ and, let's say, $\pmb{C}$ as the cartesian product, then we get $\pmb{A \times B \times C}$ which implies taking the product of a set of ordered pairs, $\pmb{A \times B}$ , and a set, $\pmb{C}$ , whose elements aren't ordered pairs, so the possible combinations will be of the form ( $\pmb{(a,b)}$ , $\pmb{f(a,b)}$ ) or ( $\pmb{(a,b)}$ , $\pmb{c}$ ) where $\pmb{a}$ , $\pmb{b}$ and $\pmb{c}$ are elements of $\pmb{A}$ , $\pmb{B}$ and $\pmb{C}$ respectively, satisfying the definition previously stated and not ( $\pmb{a}$ , $\pmb{b}$ , $\pmb{c}$ ), or is it a mere notation meant to simplify ( $\pmb{(a,b)}$ , $\pmb{f(a,b)}$ )?",['elementary-set-theory']
4030896,Do you need the axiom of choice if the non-empty sets in question are defined recursively? [duplicate],"This question already has answers here : Given an arbitrary infinite set $S$, how to construct an injection $\mathbb{N} \to S$ with the Axiom of Choice? (2 answers) Recursive use of the Axiom of Choice (3 answers) Closed 3 years ago . Suppose I pick (using a single choice) an element $q_1$ from a nonempty set $U$ . I then pick $q_2$ from the set $U/\{q_1\}$ , $q_3$ from the set $U/\{q_1, q_2\}$ and so on. At each stage, the set in question is guaranteed to be non-empty. Thus, I have constructed a sequence $(q_n)_{n\in\mathbb{N}}$ using a single choice each time. Thus, I can safely assume that no (countably infinite) axiom of choice was used in the construction of this sequence, right?","['algebra-precalculus', 'foundations', 'logic']"
4030914,$A \in M_n(\mathbb{C})$ invertible and $A^2$ is diagonalizable. Prove $A$ is diagonalizable,"I'm interested to know if it's true because I saw that if $A^2$ is diagonalizable then $A$ is not necessary Diagonalizable. I have a feeling it's true but I'm not sure how to prove it. This is my proof but I'm not sure if it's good to assume that : My try By Diagonalizable we know that exist invertible matrix $P$ such that : $P^{-1}AAP=diag(\lambda_1,...,\lambda_n)$ $AA=Pdiag(\lambda_1,...,\lambda_n)P^{-1}$ Set $D=Pdiag(\sqrt{\lambda_1},...,\sqrt{\lambda_n})P^{-1}$ So : $D^2=(Pdiag(\sqrt{\lambda_1},...,\sqrt{\lambda_n})P^{-1})(Pdiag(\sqrt{\lambda_1},...,\sqrt{\lambda_n})P^{-1})=Pdiag(\lambda_1,...,\lambda_n)P^{-1}=AA$ Here I wonder - $A=D=Pdiag(\lambda_1,...,\lambda_n)P^{-1} \rightarrow A$ is diagonalizable.","['diagonalization', 'linear-algebra']"
4030915,Is this set $\left\lbrace\frac{1}{n}: n \in \mathbb{N}\right\rbrace$ a $G_\delta$ set?,"Is the set $$\left\lbrace\frac{1}{n}: n \in \mathbb{N}\right\rbrace,$$ a $G_{\delta}$ set? I know that this set is $F_\sigma$ , but not sure if it is $G_\delta$ . Edit: Thanks to everyone who commented very quickly. I guess my attempt would be to create the sets $A_n$ such that $A_n = (1-\frac{1}{n},1+\frac{1}{n}) \cup (\frac{1}{2}-\frac{1}{n},\frac{1}{2}+\frac{1}{n}) \cup \cdots$ . Then taking the intersection over all natural numbers leads to the desired set. Is this correct?","['measure-theory', 'real-analysis']"
4030919,What is the least possible value of $|\mathcal{C}|$ if $\mathcal{C}$ satisfies some conditions...,"Let $\mathcal{C}$ be a family of subsets of $A=\{1,2,\dots,10\}$ satisfying the following two conditions: Every $9$ element subset of $A$ is in $\mathcal{C}.$ For any non empty subset $C\in\mathcal{C}$ there is $c\in C$ such that $C\setminus\{c\}\in  \mathcal{C}.$ What is the least possible value of $|\mathcal{C}|$ ? I can find a family $\mathcal{C}$ with $10+5+5+3+3+2+2+2+2+1=35$ members: Clearly there are $10$ 9-element subsets, now we delete some elements in a sucsession and get: 8-element subsets: $\{1,2,3,4,5,6,7,\color{red}8\}$ , $\{1,2,3,4,5,6,9,\color{red}{10}\}$ , $\{1,2,3,\color{blue}4,7,8,9,10\}$ , $\{1,2,5,\color{blue}6,7,8,9,10\}$ , $\{3,4,5,6,7,8,9,10\}$ . 7-element subsets: $\{1,2,3,4,5,6,\color{red}{7}\}$ , $\{1,2,3,4,5,6,\color{red}9\}$ , $\{1,2,\color{blue}3,7,8,9,10\}$ , $\{1,2,\color{blue}5,7,8,9,10\}$ , $\{3,4,5,6,7,8,\color{green}9\}$ . 6-element subsets: $\{\color{red}1,2,3,4,5,6\}$ , $\{1,2,7,8,9,\color{blue}{10}\}$ , $\{3,4,5,6,7,\color{red}8\}$ . 5-element subsets: $\{\color{red}2,3,4,5,6\}$ , $\{1,2,7,8,\color{blue}9\}$ , $\{3,4,5,6,\color{red}7\}$ . 4-element subsets: $\{3,4,5,6\}$ , $\{1,2,7,8\}$ 3-element subsets: $\{3,4,5\}$ , $\{1,2,7\}$ 2-element subsets: $\{3,4\}$ , $\{1,2\}$ 1-element subsets: $\{3\}$ , $\{1\}$ and empty set $\{\}$ But, can it be done better?","['contest-math', 'extremal-combinatorics', 'combinatorics', 'discrete-mathematics', 'elementary-set-theory']"
4030931,"How to evaluate $\lim_{(x,y)\to (0,0)} \frac{x^4+y^4}{2x-y}$?","Find the limit : $$\lim_{(x,y)\to (0,0)} \frac{x^4+y^4}{2x-y}$$ I have tried using polar substitution but the problem is the denominator. Denominator can go to $0$ for some value of theta and is not dependent on only $r$ .","['limits', 'multivariable-calculus', 'limits-without-lhopital']"
4030943,Existence of holomorphic function $f$ such that $f^{(n)}(0) = n^{2n}$,"I need help with the following problem: Is there a holomorphic function $f$ in an open disk around $0$ such that \begin{align}
f^{(n)}(0) = n^{2n}
\end{align} for all $n \in \mathbb{N}$ ? I thought quite long about this and came up with the following idea: Suppose there exists such a function. Then we can write it as \begin{align}
f(z) = \sum_{n=0}^{\infty} \frac{n^{2n}}{n!}z^n.
\end{align} We can compute the radius of convergence $R$ of this power series: \begin{align}
R = \lim_{n \to \infty} \left| \frac{n^{2n}}{n!} \cdot \frac{(n+1)!}{(n+1)^{2n+2}} \right| = \lim_{n \to \infty}\left( \frac{n}{n+1}\right)^{2n} \cdot \frac{1}{n+1} \leq \lim_{n \to \infty} \frac{1}{n+1} = 0.
\end{align} This contradicts the assumption that $f$ is holomorphic in an open disk around $0$ or with other words: $R > 0$ . I am not very confident looking at my solution. (Is it true?) Apart from that I was wondering whether or not there is an easier way to find an answer to this question. The problem reminds me of the Identity Theorem or even the generalized Cauchy Integral Formula. Still I failed to see if (and how) those two could have been used to solve this. In order to gain a better understanding of problems of this kind I would be glad if someone has another good idea for this question!","['complex-analysis', 'entire-functions']"
4030946,Let T be a tree such that every leaf is adjacent to a vertex of degree at least 3. Show that there are two leaves with a common neighbor.,"I came up with a proof for this but I think it might be a little hand-wavy and I can't figure out why. Some advice on how to make this proof clearer will be really appreciated! Problem: Let $T$ be a tree such that every leaf is adjacent to a vertex of degree at least $3$ . Show that
there are two leaves with a common neighbor. Proof: Suppose by contradiction that there are no two leaves with a common neighbor in T. Then let there be $k$ leaves in T and because each leaf is adjacent to a vertex of degree at least $3$ , then there are k vertices in T that has degree at least $3$ . For vertices other than the leaves and the leaves' neighbor, there are $V(T)-2k$ of those vertices and because they are not leaves they have degree of at least 2. So the minimum sum of degree of vertices in $T$ is $$\sum_{v\in T}d(v)=k+3k+2\cdot (V(T)-2k)=2V(T)$$ however by degree sum formula there is exactly $\sum_{v\in T}d(v)=2V(T)-2$ , a contradiction.","['trees', 'graph-theory', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
4031057,"Show that $T(X)=(R,V)=\left( X_{(n)}-X_{(1)},\frac{X_{(n)}+X_{(1)}}{2} \right)$ is a minimal sufficient statistic for $\theta$.","Let $X_{1}, X_{2}, ..., X_{n}$ be a random sample from $\text{Uniform}(\theta,\theta+1)$ population with $-\infty<\theta<\theta+1< \infty$ show that $T(X)=(X_{(1)},X_{(n)})$ is a minimal sufficient statistic for $\theta$ . Also, show that $T(X)=(R,V)=\left( X_{(n)}-X_{(1)},\frac{X_{(n)}+X_{(1)}}{2} \right)$ is a minimal sufficient statistic. For the first part I did the following $$f(x|\theta,\theta+1)=I_{(\theta,\theta+1)}(x_{(1)},x_{(n)})$$ then $$\frac{f(x|\theta,\theta+1)}{f(y|\theta,\theta+1)}=\frac{I_{(\theta,\theta+1)}(x_{(1)},x_{(n)})}{I_{(\theta,\theta+1)}(y_{(1)},y_{(n)})}$$ This is a constant function in $\theta$ iff $x_{(1)}=y_{(1)}$ and $x_{(n)}=y_{(n)}$ s.t. $T(X)=(X_{(1)},X_{(n)})$ is a minimal sufficient statistic for $\theta$ . However, I am not sure how to proceed to show that $T(X)=(R,V)=\left( X_{(n)}-X_{(1)},\frac{X_{(n)}+X_{(1)}}{2} \right)$ is a minimal sufficient statistic. Can some help me with this?","['statistical-inference', 'statistics', 'uniform-distribution', 'order-statistics']"
4031076,"Given $\sigma$ a coprime automorphism of $G$ a finite group, must the centre of $G^\sigma$ be contained in the centre of $G$?","Let $G$ be a finite group, $\sigma$ an automorphism of order coprime to $|G|$ . Let $K$ denote the subgroup of $\sigma$ fixed points of $G$ . Then do we necessarily have that every element of $G$ commutes with every element of the centre of $K$ ? I haven't been able to prove this or come up with a counterexample, but in the direction of trying to prove it, I have the following. Bootstrapping from the prime power order case, we see that there is no fusion from $K$ into $G$ (Glaubermans lemma), so if $g\in G$ doesn't commute with $x\in Z(K)$ , then $xgx^{-1}$ isn't in $K$ . In a similar way, if $g$ isn't in $N_G(Z(K))=Z_G(Z(K))$ , then $Z(K)^g\cap Z(K)=\{e\}$ , so we have some trivial intersection behaviour.","['group-theory', 'group-isomorphism', 'finite-groups']"
4031085,"Prove that if $f^{(k)}(f^{-1}(x))$ exists, and is nonzero, then $(f^{-1})^{(k)}(x)$ exists.","I think I found an error and just want to confirm. This is from Calculus by Michael Spivak, 3rd edition, Chapter 12 (Inverse Functions), problem 21. 12-21. Prove that if $f^{(k)}(f^{-1}(x))$ exists, and is nonzero, then $(f^{-1})^{(k)}(x)$ exists. This appears to be false. For example, take $f(x) = x^3$ . In this case $$f^{-1}(x) = x^{\frac{1}{3}}$$ $$f(0) = f^{-1}(0) = 0$$ $$f^{(3)}(0) = 6$$ If the statement is true, $(f^{-1})^{(3)}(0)$ would exist. However, $(f^{-1})'(0)$ doesn't exist (because $f'(0) = 0$ ) so higher order derivatives cannot. I think the question should instead say ""Prove that if $f^{(k)}(f^{-1}(x))$ exists, and $f'(f^{-1}(x))$ is nonzero, then $(f^{-1})^{(k)}(x)$ exists. Does that sound about right to y'all? Edit: As discussed with Paul Frost, the revised version of the statement Prove that if $f^{(k)}(f^{-1}(x))$ exists, and $f'(f^{-1}(x))$ is nonzero, then $(f^{-1})^{(k)}(x)$ exists is missing one other thing. We need $f$ to be one-one on some interval containing $f^{-1}(x)$ . For cases $k\geq 2$ , $f$ one-one is implied by $f'(f^{-1}(x))≠0$ , and the existence of $𝑓″(f^{-1}(x))$ (which gives us the existence of $f'$ for all points in some interval containing $f^{-1}(x)$ and continuity of $f'$ at $f^{-1}(x)$ ). We can prove the (corrected) statement for $k≥2$ . However, we don't know if $f$ is one-one for the case $k=1$ . As problem 11-63 shows, $f'(f^{-1}(x))\neq 0$ by itself isn't enough to guarantee $f$ is increasing (or decreasing) over some interval containing $f^{-1}(x)$ But that's ok. The higher derivative cases are the ones we're interested in anyway, so we can take $k = 2$ as the base case and use induction from there. The corrected corrected statement reads:
""Prove that if $f^{(k)}(f^{-1}(x))$ exists, and $f'(f^{-1}(x))$ is nonzero, then $(f^{-1})^{(k)}(x)$ exists for all $k\geq 2$ , $k\in \mathbb{N}$ For the $k=1$ case, we have theorem 12-5.","['calculus', 'solution-verification', 'derivatives', 'inverse-function']"
4031107,Existence of periodic solution in an ODE system,"Let the ODE system, \begin{cases}x'(t)=x^3+x^5 \\ y'(t)=y+y^7 \end{cases} defined in $\mathbb{R}^2$ . I need to say if this system supports periodic solution with period $T>0$ . This is a question of Analysis in $\mathbb{R}^n$ not ODE, that is, I need to demonstrate this using concepts presented in Analysis in $\mathbb{R}^n$ , my textbook is Analysis on Manifolds by Munkres. The hint is, A periodic solution of the system above is a closed curve $\gamma(t) = (x(t), y (t))$ that satisfies the system. I believe I have to use Stokes, but I am not able to proceed with the solution.","['analysis', 'ordinary-differential-equations', 'real-analysis']"
4031114,Is comparison lemme required for Hanson-Wright Inequality?,"I am reading Chapter 6 of Vershynin’s book on High Dimensional probability. He proves the following theorem `Hanson-Wright’ in Theorem 6.2.1. (I am modifying it slightly to make my point succinct.) $\textbf{Theorem}$ Let $X=(X_1, \ldots, X_n)$ be a random vector with independent mean $0$ sub-Gaussian coordinates. Let $A$ be an $n\times n$ matrix with $A_{ii}=0$ . Then, for every $t>0,$ we have $$P(|X^TAX|\ge t)\le 2\exp\left(-c\min\left(\frac{t^2}{K^4||A||_F^2}, \frac{t}{K^2||A||}\right)\right),$$ where $K=\max_i ||X_i||_{\psi_2}.$ The proof basically follows by controlling the moment generating function of $X^TAX.$ To this end, one uses a decoupling argument that allows us to replace $X^TAX$ by $X^TAX’$ where $X’$ is an independent copy of $X.$ This is the step that requires each coordinate of $X_i$ To be independent. More precisely, we get $$\mathbb{E}(\exp(\lambda X^TAX))\le \mathbb{E}\exp(4\lambda X^TAX’).$$ The next step is to compare the moment generating function of $X^TAX$ With $g^TAg’$ Where $g, g’$ are independent $N(0, I_n)$ random vector. Using the comparison we obtain $$\mathbb{E}(4\lambda X^TAX’)\le \mathbb{E}(\exp(C\lambda g^TAg’)).$$ At this point, we compute the moment generating function (it is easier in Gaussian case) and do the usual optimization over $\lambda$ To get the desired inequality. Now my question is the following. If I have $X$ and $X’$ Independent, using that $X$ is subgaussian with norm $K,$ I can show that $\mathbb{E}(\exp(\lambda X^TAX’))\le \mathbb{E}\exp(C\lambda^2 K^2 ||AX’||_2^2)$ for some $C.$ Now I use the fact that $||AX’||_2^2\le ||A||^2||X’||^2.$ Using that coordinate of $X’$ Are independent, I can show that $||X’||^2$ is subexponential and therefore I can bound the moment generating function of $||X’||_2^2.$ I did not work out the exact constants, but it seems to me that I am getting the same inequality. Can anyone tell me if this approach is fine? Does it give the same inequality or not? My point is to understand if comparison lemma is just a tool to better organize the proofs or it really adds something that I do not get otherwise?","['inequality', 'probability-theory']"
4031118,What is the point of giving a tensor identity in normal coordinates?,"I have been confused about this for some time. For example, the curvature tensor in general for the Levi-Civita connection of $g$ in the local frame $\{\partial_i\}$ induced by the coordinates $\{x^i\}$ is $R_{ijk}^l = \partial_i \Gamma_{jk}^l - \partial_j \Gamma_{ik}^l + \Gamma_{jk}^m \Gamma_{im}^l - \Gamma_{ik}^m\Gamma_{jm}^l$ and if you take the partials of the Christoffel symbols and grind through you can ultimately get $R_{ijk}^l = \frac{1}{2}g^{ls}(\partial_i \partial_k g_{js} - \partial_i \partial_s g_{jk} -\partial_j \partial_k g_{is} + \partial_j\partial_s g_{ik}) + g^{lm}g_{st}(\Gamma_{jm}^s\Gamma_{ik}^t - \Gamma_{im}^s\Gamma_{jk}^t)$ which is fine. But in every book I read everyone just says ""at a point $p$ in normal coordinates we have..."" $R_{ijk}^l = \frac{1}{2}g^{ls}(\partial_i \partial_k g_{js} - \partial_i \partial_s g_{jk} -\partial_j \partial_k g_{is} + \partial_j\partial_s g_{ik})$ which of course follows right away from what I already have, but what is the point of this second formula at all? Are they trying to tell me that if I change coordinates back to the coordinate frame in the latter formula that I'm supposed to get the former? I've been trying to do this with no success, am I failing because I'm making some computational mistake or because my logic is wrong altogether? Since $R$ is a tensor its value at a point $p$ does not depend on the coordinates: Does this mean that if $\tilde\partial_j$ is the frame induced by the coordinates $\{y^j\}$ then $g(R(\partial_i, \partial_j) \partial_k, \partial_l))=\tilde{g}(\tilde{R}(\tilde\partial_i, \tilde\partial_j)\tilde\partial_k,\tilde\partial_l)$ as real numbers at every point $p$ in the overlap of the two charts? Does this mean that if for some reason I wanted to integrate the component $R_{123}^4$ over some compact $M$ I could just integrate its formula in normal coordinates? If the questions seem vague its because I really am confused by this. Thanks for whatever clarification you can give.","['riemannian-geometry', 'coordinate-systems', 'tensors', 'curvature', 'differential-geometry']"
4031162,"Show that $V\otimes V\simeq L(V^*,V^*,\mathbb{R})$","I am trying to understand tensor products and I would like to show that $V\otimes V\simeq L(V^*,V^*,\mathbb{R})$ . In Lee's book about smooth manifolds is the following proof for the case $V^*\otimes V^*\simeq L(V,V,\mathbb{R})$ : According to the book it follows directly from $V\simeq V^{**}$ , since everything is assumed to have finite dimension. I am fine with that. Nevertheless  I was wondering if it couldn't be done as it is done in the above proof. For $(v,w)\in V\times V$ and $\omega,\eta\in V^*$ I would define $\Phi :V\times V\to L(V^*,V^*,\mathbb{R}),\Phi(v,w)[\omega,\eta]:=\omega(v)\cdot\eta(w)$ . I suppose it is a bilinear map. If it is, then it decents to a linear map $\tilde{\Phi} :V\otimes V\to L(V^*,V^*,\mathbb{R}),  \tilde{\Phi}(v\otimes w)[\omega,\eta]:=\omega(v)\cdot\eta(w)$ . Now I am stuck. I am not sure what the basis vectors for $L(V^*,V^*,\mathbb{R})$ are or how to show that it is a bijection. I am also not sure if the apprach is okay or complete nonsense. Thank you very much in advance!","['multilinear-algebra', 'abstract-algebra', 'tensor-products', 'differential-geometry']"
4031192,"Equilateral triangle $ABC$ with $P$ inside, $PA= x$, $PB=y$, $PC=z$ and $z^2 =x^2+y^2$. Find side length of $ABC$","$ABC$ is an equilateral triangle $ABC$ with $P$ inside it such that $PA= x$ , $PB=y$ , $PC=z$ . If $z^2 =x^2+y^2$ , find the length of the sides of $ABC$ in terms of $x$ and $y$ ? If $z^2=x^2+y^2$ then how can I find measures of angles around $P$ so that the sides can be expressed in terms of $x$ and $y$ . I've tried everything I can think of.",['geometry']
4031201,"Prove that for any integer $n>0$ which is a perfect square, $n+4$ is not a perfect square.","I have been told that this proof is incorrect, however, I'm having a hard time seeing the issue. Prove that for any integer $n > 0$ which is a perfect square, $n+4$ is not a perfect square: Proof: $n > 0$ Since $n$ is a perfect square, $a^2=n$ The subsequent perfect square can be written as ${b^2=\left(a+1\right)}^2$ The difference between consecutive squares is then ${|b}^2-a^2|=|2a+1|$ $\forall a\in\mathbb{Z}(|2a+1|\neq 4)$ $\square$ Edit: Thanks everyone, appreciate it :)","['proof-explanation', 'proof-writing', 'solution-verification', 'discrete-mathematics']"
4031240,Phase space and collision map of a billiard,"I am working through Ch. 2 of Chaotic billiards by Chernov and Markarian. A few things are puzzling me about the basic construction and definitions. 2.5. Phase space for the flow. The state of a moving particle at any time is specified by its position $q \in \mathcal{D}$ and velocity $v \in S^1$ . Thus the phase is $\Omega = \mathcal{D} \times S^1$ . It then goes on to say that $\Omega$ is a doughnut with cross section of $\mathcal{D}$ . This is intuitive, but what do phase space trajectories look like? Presumably if for every $\mathcal{D}$ we associate a velocity in $S^1$ , then each transversal 'slice' of $\Omega$ will show line segments that are all oriented in the same direction. Is this the correct picture? At each regular boundary point $q$ it is convenient to identify the pairs $(q, v^-)$ and $(q, v^+)$ related by the collision rule $v^+ = v^- - 2\langle v^-, n \rangle n$ ( $v^\pm$ and $n$ are the incoming, outgoing and inward normal vectors at the moment of collision, respectively), which amounts to gluing $\Omega$ along its boundary . What does it mean to glue $\Omega$ along its boundary? I understand that the motivation for doing this is to allow the billiard flow $\Phi^t$ to be smooth. But if that is the case, then my 'picture' of what a phase space trajectory should look like must be wrong: if we collapse $S^1$ into a half circle, making $\Omega$ a half doughnut, nothing really becomes 'smoother' - there are still discrete jumps between one 'slice' of the doughnut and another. Finally, I become really confused when the collision map is introduced. Given a flow $\Phi^t : \Omega \to \Omega$ , one finds a hypersurface $M \in \Omega$ transversal to the flow so that each trajectory crosses $M$ infinitely many times. Won't $M$ be always equal to $\mathcal{D}$ , bar the singular points? Moreover, saying that the flow will pierce $M$ suggests to me that phase space trajectories are actually little curves moving 'through' the doughnut $\Omega$ . It goes on to redefine $\Omega$ as $$\Omega = \{ (x,s) : x \in M, 0 \le s \le L(x) \}$$ where $L(x) = \min \{ s > 0 : \Phi^s(x) \in M\}$ is a return time function and thus the collision map $F : M \to M $ is expressed as $F(x) = \Phi^{L(x)}(x)$ . This new $\Omega$ makes the flow a suspension flow.","['general-topology', 'billiards', 'dynamical-systems']"
4031264,Probability of CDC data,"All of this is my work, for (c) I believe its $1 - pbinom(144, 138, 2.9)$ which yields $.0192 $ For (d) I know that when D is negative, that would mean the woman is taller than the man, so I think it is just $ 69.2/63.8$ ? Please correct me if I am wrong, Thank you in advance!","['statistics', 'probability']"
4031272,A power series equality involving binomial coefficients,"I believe the following identity holds for any non-negative integers $m,k$ : $$\binom{m+k}{k}^2 = \sum_{n=0}^m \binom{k}{m-n}^2\binom{2k+n}{n}.$$ It seems like there should be a slick double counting way of proving it, but I am not able to see it. I got this identity because I believe the following power series equality holds for $|x| < 1$ , $$(1-x)^{2k-1} \sum_{m=0}^\infty \binom{m+k-1}{k-1}^2 x^m = \sum_{j=0}^{k-1} \binom{k-1}{j}^2 x^j. $$ The binomial identity above comes from dividing by $(1-x)^{2k-1}$ , applying the binomial theorem replacing $k$ with $k+1$ and comparing the coefficients of both sides. But this power series equality doesn't seem any easier to prove than the binomial coefficient identity, since I don't really have a handle on the sums on either side. Does anyone know a reference for an identity of this nature or have an idea for a double counting argument?","['power-series', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
4031296,How to evaluate cumulative distribution function w/ sigma of given SRS?,"I have been given a set of 10 positive values that make up an SRS (Simple Random Sample) of a population, which has a normal distribution. I then am asked to evaluate $a = \sum_{i=1}^n\frac{(x_{i}-\mu)^2}{\sigma^2}$ and provide the distribution. Since it is a sum, I assumed a would simply be a number. Finally, I am asked to evaluate the following CDF: P ( $\sum_{i=1}^n\frac{(x_{i}-\mu)^2}{\sigma^2} \le a$ ). How should I interpret this? If the values in the SRS are all positive, then how could any subset less than the size of $n$ have the value of $a$ ? Wouldn't the probability of this just be $1$ ?",['statistics']
4031330,Does the following necessarily converge to a normal random variable in distribution?,"Suppose $X_1, \dots, X_n$ are i.i.d. random variables with $E(X_1) = 0$ and $\operatorname{Var}(X_1) = 1$ . Let $$
S_n = \frac{1}{n}\sum_{i=1}^n\sqrt{i}X_i.
$$ Does $S_n$ converge to a normal random variable? Originally, I attempted to use Lindeberg CLT to prove this. However, I ran into a wall because I can't figure out a way to check the Lindeberg condition that for $\forall \varepsilon > 0$ $$
\sum_{i=1}^nE(|Y_i|^2\mathbf{1}(|Y_i| > \varepsilon)) \to 0
$$ where $Y_i = \sqrt{i}X_i/s_n$ and $s_n^2 = \sum_{i=1}^ni\operatorname{Var}(X_i) = \frac{n(n+1)}{2}$ . If I can prove this, then I can use Slutsky, then we are done. But I have no idea what $X_i$ actually is so I don't know how to verify the condition. Then I tried using characteristic functions and try to do expansion and approximation. However, I also hit a wall due to the changing index of $i$ . I also tried finding counterexample, but nothing came up. Can anyone provide some hint? Thank you!","['central-limit-theorem', 'weak-convergence', 'probability-distributions', 'convergence-divergence', 'probability-theory']"
4031336,"Arrangements of $5$ A's,$7$ B's and $4$ C's to form a $16$ letter word so that there are at least $3$ $CA$ pairs occurring in the word.","In how many ways can we arrange $5$ A's, $7$ B's and $4$ C's to form a $16$ letter word so that there are at least $3$ $CA$ pairs occurring in the word.(In other words there are at least $3$ occurrences of a $C$ immediately followed by an $A$ ) My Attempt:
I get different answers Method 1: I arrange the following: $(CA),(CA),(CA),C,A,A,B,B,B,B,B,B,B$ in $$\frac{(13)!}{3!2!7!}=102960$$ ways. Method 2: Pairing the four C's with four A's we can arrange the 4 CA's,1 A and 7 B's in $$\frac{12!}{4!1!7!}=3960$$ ways. Now we will count the arrangements with exactly 3 CA's. We place 3 CA's,1 C and 7 B's in a row in $$\frac{11!}{3!1!7!}=1320$$ ways. Off the 12 gaps that are formed we need to choose 2 out of 11(leaving out the gap just after C) o place the 2 A's which can be done in $$\binom{11+2-1}{2}=66$$ ways. Thus the number of arrangements with exactly 3 CA's is $$1320\times 66=87120$$ So total number of arrangements with at least three occurrences of CA is $$87120+3960=91080$$ So which cases are getting over counted.","['permutations', 'combinations', 'combinatorics', 'discrete-mathematics']"
4031376,$\prod\limits_{n=1}^{x}\left(1+\frac{1}{n}\right)=x+1$. What is this formula called? Is it even useful?,"So I was playing around with some equations and I got to this formula: $$
\prod\limits_{n=1}^{x}\left(1+\frac{1}{n}\right)=x+1 
$$ I checked it with particular values and it holds true when x is a positive integer. I never saw this formula before. Is it just a useless formula? If not, does it have a name?",['algebra-precalculus']
4031560,"Why $a_{n+3}=a_{n+2}+2a_{n+1}-a_n$ for $n\geq8$, where $a_{n+1}$ is the second smallest number that is not the sum of any earlier terms?","Let $a_n$ be a sequence of numbers defined as $a_0=0$ and $a_{n+1}$ for $n\geq0$ is the second smallest number that is greater than $a_n$ and is not the sum of any earlier distinct terms. This sequence should be {0, 2, 4, 7, 10, 18, 33, 38, 86, 162, 284, 522, 928, 1688, 3022, 5470, 9826, 17744, 31926, 57588, 103696, 186946, 336750, 606946, 1093500, 1970642, 3550696, 6398480, 11529230, 20775494, 37435474, 67457232, 121552686, 219031676, 394679816, 711190482, 1281518438, 2309219586, 4161065980, 7497986714...}. Edit:
For example, $a_4=10$ because from $0,2,4,7$ , we can get $0,2,4,6(=2+4),7,9(=2+7),11(=4+7),13(=2+4+7)$ . So, the numbers that is not the sum of any earlier distinct terms are $1,3,5,8,10,12,14,15,\ldots$ , and the second smallest number that is bigger than $a_3=7$ is $a_4=10$ . Edit 2: On the other hand, for instance, if we define the number sequence $\{a_n\}$ as the first smallest number , we get 0,1,2,4,8,16,32,64,... Using Wolfram Alpha and some other tools, I noticed that $a_{n+3}=a_{n+2}+2a_{n+1}-a_{n}$ holds for $n=5,n\geq8$ and at least $n<40$ . It seems strange to me that this relation holds only for n greater than 4, so I changed the first terms to see what relation holds, then all the ones I've tried satisfied $a_{n+3}=a_{n+2}+2a_{n+1}-a_{n}$ or $a_{n+12}=31a_{n+6}-4a_{n}$ except for the first few terms.
I would like to know why these relations hold and why they don't hold for the first few terms. Examples of $\{a_n\}$ : (Edit 3: I checked up to $n<10000$ ) $a_0=0$ : $\{a_n\}={0,2,4,7,10,18,33,38,86,162,284,522,928,\cdots}$ $a_{n+3}=a_{n+2}+2a_{n+1}-a_{n}$ for $n=5,n\geq8$ $a_0=1$ : $\{a_n\}={1,3,6,11,16,29,53,61,138,260,456,838,1490,\cdots}$ $a_{n+3}=a_{n+2}+2a_{n+1}-a_{n}$ for $n=0,5,n\geq8$ $a_0=2$ : $\{a_n\}={2,4,7,10,18,33,38,86,162,284,522,928,1688,\cdots}$ $a_{n+3}=a_{n+2}+2a_{n+1}-a_{n}$ for $n=4,n\geq7$ $a_0=3$ : $\{a_n\}={3,5,7,11,17,34,64,73,167,313,548,1007,1790,\cdots}$ $a_{n+3}=a_{n+2}+2a_{n+1}-a_{n}$ for $n=5,n\geq8$ $a_0=0,a_1=1$ : $\{a_n\}={0,1,3,6,11,16,29,53,61,138,260,456,838,1490,\cdots}$ $a_{n+3}=a_{n+2}+2a_{n+1}-a_{n}$ for $n=1,6,n\geq9$ $a_0=2,a_1=3$ : $\{a_n\}={2,3,6,10,17,31,55,79,158,299,519,959,1698,2437,\cdots}$ $a_{n+3}=a_{n+2}+2a_{n+1}-a_{n}$ for $n=0,\ 2,3,5,\ 8,9,11,\ 14,15,17,\ 20,21,23,\ 26,27,29\ldots$ $a_{n+12}=31a_{n+6}-4a_{n}$ for $n\geq1$ Try it online! (Shows $n,a_n$ which doesn't satisfy $a_n=a_{n-1}+2a_{n-2}-a_{n-3}$ for $a_n=0,\ n<1000$ )","['elementary-number-theory', 'recurrence-relations', 'sequences-and-series']"
4031576,Determining if ellipse is fully inside another ellipse,"I have two ellipses which are both not rotated, but have different sizes. I want to know if the smaller ellipse 1 is fully inside the larger ellipse 0. In my special case both ellipses have the same axis ratio, so $a_0/b_0=a_1/b_1$ , but I doubt this makes a difference for the calculation . I have started to work on a solution, but I am not so sure if this is the right approach and if it will ultimately lead to success. Here is what I have come up with so far: The radius of ellipse 0 can be calculated for any point on the ellipse as $$
r_0=\sqrt{(x-x_0)^2+(y-y_0)^2}
$$ The general equation for an ellipse gives a relation between $x$ and $y$ for points on the ellipse. $$
\frac{(x-x_0)^2}{a^2}+\frac{(y-y_0)^2}{b^2}=1 
$$ $$
(y-y_0)^2=b*\sqrt{1-\frac{(x-x_0)^2}{a^2}} 
$$ This can be used to calculate radius of both ellipses depending on the x coordinate. $$
r_0 = \sqrt{(x-x_0) + b_0^2(1-\frac{(x-x_0)^2}{a_0^2})}
$$ $$
r_1 = \sqrt{(x-x_1) + b_1^2(1-\frac{(x-x_1)^2}{a_1^2})}
$$ To calculate the distance $s$ between both ellipses at a certain $x$ coordinate, we simply have to add the distance $d_{01}$ between the center points to radius 1 and subtract this from radius 0. $$
s=r0 - (d_{01} + r1)
$$ where $$
d_{01} = \sqrt{(x_1-x_0)^2 + (y1-x_0)^2}
$$ Now I have an equation where $s$ only depends on $x$ . My idea is to get the minimum of $s$ through the first derivative. If the minimum is non-negative, ellipse 1 is inside or on ellipse 0. If it is negative, part of ellipse 1 is outside ellipse 0. Here are my questions: Am I on the right way so far? Is calculating the minimum for s the right approach? Do I have a chance to actually calculate the minimum of $s$ ? Note: Ther is a similar question here , but I think my case is much simpler because the ellipses are not rotated similar.","['conic-sections', 'geometry']"
4031616,Limit of $(x^3 - 3x^2 + 2x)$ as $x \to 1$ using epsilon delta definition,"I need to prove the following limit using epsilon delta method. I have come up this on my own just to practice skills. $$\lim \limits_{x \to 1} \,(x^3 - 3x^2 + 2x) = 0 $$ So, I need to come up with some $\delta >0$ such that given $0 < \lvert x - 1 \rvert <\delta$ , I need to prove that $$ |(x^3 - 3x^2 + 2x) - 0| < \varepsilon \tag{1}$$ I can simplify eq $1$ as follows. $$ |x| |x-1||x-2| < \varepsilon $$ When we have a non linear term, then we restrict $x$ to some distance from $1$ . So, suppose we have $ 0 < |x-1| < 10$ . This leads to $-10 < x-1 < 10$ . Doing some algebra, we can get inequalities $$ 0 \leqslant |x| < 11 \tag{2}$$ $$ 0 \leqslant |x-2| < 11 \tag{3}$$ From eq 2 and 3, it follows that $$ 0 \leqslant |x||x-2| < 121 $$ And since $0 < |x-1| $ , it follows that $$ |x||x-1||x-2| < 121 |x-1| \tag{4}$$ To show that $|x||x-1||x-2| < \varepsilon$ , its sufficient to show that $ 121 |x-1| < \varepsilon$ . And this will happen if $ |x-1| < \frac{\varepsilon}{121} $ . So, this is another restriction on $|x-1|$ . So, we can let $$\delta = \text{min}\left(10, \frac{\varepsilon}{121}\right) $$ So, we can let this be our choice of $\delta$ . Its clear that $\delta > 0$ . So, now the official proof will follow. Let $\varepsilon > 0$ be some arbitrary real. Suppose $0 < |x-1| < \delta$ . With the choice of $\delta$ we have done, this means that $ 0 < |x-1| < 10$ . As demonstrated above, equation 4 follows from this inequality $$ |x||x-1||x-2| < 121 |x-1| $$ But now, we also have $ |x-1| < \frac{\varepsilon}{121} $ . It means that $ 121 |x-1| < \varepsilon $ . Using this, we get that $$ |x||x-1||x-2| < \varepsilon  $$ which can be rewritten as $$ | (x^3 - 3x^2 + 2x) - 0 | < \varepsilon  $$ Since, $\varepsilon > 0$ was arbitrary to begin with, its proven that $$\lim \limits_{x \to 1} \,(x^3 - 3x^2 + 2x) = 0 $$ Is the proof correct ?","['limits', 'calculus', 'epsilon-delta', 'real-analysis']"
4031702,Intuition behind multiplicative unitaries,"A multiplicative unitary on a Hilbert space $H$ is a unitary $V: H \otimes H \to H\otimes H$ such that the pentagon identity $V_{[12]} V_{[13]}V_{[23]}= V_{[23]} V_{[12]}$ holds. Here the leg-numbering notation is employed. For example, $$V_{[12]}= V \otimes 1 \in B(H \otimes H \otimes H).$$ Is there any intuition behind this pentagon identity? I can't memorise the identity, but I need it often (in the context of quantum groups), and I think if I would have some intuition I would be able to memorise it better.","['operator-theory', 'functional-analysis', 'intuition', 'operator-algebras']"
4031710,Prove that $\frac{d}{dt}(E[x])=E\left[\frac{dx}{dt}\right]$,"I'm studying about Kalman filter from the book of ""optimal state estimation"" and I have one exercise where I need to prove the following equation: $$\frac{d}{dt}(E[x])=E\left[\frac{dx}{dt}\right].$$ I'm not quite sure how to approach this problem, because it is not clear to me are we taking the expectations with respect to what random variables? Is it the random variable $x$ , or is the $t$ here also a random variable? First, I assumed the expectations are taken w.r.t to $x$ , so I started to expand the left side of this equation first: $$\frac{d}{dt}(E[x]) = \frac{d}{dt}\left(\int x\,f(x)\,dx\right)=\int \frac{d}{dt}\left(x\,f(x)\right)\,dx=\int \left[\frac{dx}{dt}f(x)+x\frac{df}{dx}\frac{dx}{dt}\right]\,dx$$ and then I looked at the right side of the equation: $$E\left[\frac{dx}{dt}\right] = \int \frac{dx}{dt}\,f(x)\,dx$$ so now I'm stuck a little bit. If I take the expectations w.r.t $t$ I get: $$\frac{d}{dt}(E[x]) = \frac{d}{dt}\left(\int x\,f(t)\,dt\right)=\int \frac{d}{dt}\left(x\,f(t)\right)\,dt=\int \left[\frac{dx}{dt}f(t)+x\frac{df}{dt}\right]\,dt$$ $$E\left[\frac{dx}{dt}\right] = \int \frac{dx}{dt}\,f(t)\,dt.$$ or should I take the expectations w.r.t to different random variables? Another point of confusion: are the differential coefficients constant in this problem? Or should I treat them as functions of $t$ (that is if expectations are taken w.r.t. $x$ )?","['integration', 'expected-value', 'derivatives']"
4031770,Countable set of finite sets is countable (without the axiom of choice). [duplicate],"This question already has answers here : Choice function for collection of arbitrary finite sets. AC required? (2 answers) Closed 3 years ago . So i have the infinite family of sets: $$\{A_0, A_1, A_2 \dots\}$$ and $\forall i \in \omega,\; A_i$ is finite. I need to show that $\cup A_i$ is countable  ( $\exists f: \cup A_i \rightarrow \omega $ and f is bijection).
I seem to understand it intuitively, but I can't prove it. I also don't have to use the axiom of choice. Any tips ?","['elementary-set-theory', 'set-theory']"
4031772,Find the Laurent expansion of $(1-z)e^{1/z}$ - When can we use Taylor series to find Laurent series?,"I'm currently taking a course in mathemathical tools, where we are covering complex analysis (Note that this course is not very rigorous and we cover complex analysis in only 4 lectures). The Laurent series have been introduced as: Laurent series $$
\begin{array}{l}
f(z)=\sum_{n=-\infty}^{\infty} a_{n}\left(z-z_{0}\right)^{n} \\
a_{n}=\frac{1}{2 \pi i} \oint_{C} \frac{f\left(z^{\prime}\right) d z^{\prime}}{\left(z^{\prime}-z_{0}\right)^{n+1}}
\end{array}
$$ However, I noticed that the above integral isn't used in the solutions to any of the problems regarding laurent series. So I'm trying to understand why we can avoid using it.
What I understood so far, is that the difference between the Taylor and Laurent series, is that the Laurent series also contains negative powers. Where it's not possible to expand a Taylor series around a point $z_0$ where f is not analytically, the opposite applies for the Laurent series.
In the case we expand f around a point where f is analytically, the Laurent series and Taylor expansion will be the same.
If that is the correct understanding, please help me understand the solution to the following problem: Find the Laurent series for $f(z)=(1-z)e^{1/z}$ about $z=0$ What I thought should be my approach is using the formula above solving the contour integral, since $f(z)$ is non-analytically at $z=0$ . However, the solution uses directly that $e^{1/z}=\sum_{n=0}^{\infty} \frac{z^{-n}}{n !}$ . So here comes my first question: Why is the Laurent series of $e^{1/z}=\sum_{n=0}^{\infty} \frac{z^{-n}}{n !}$ . It seems to me, that they have just substituted $z \rightarrow 1/z$ in the Taylor series for $e^z$ . But $e^{1/z}$ is not analytic at $z=0$ , so the Taylor expansion around that point shouldn't exist? And if that is not the Taylor series, how do we know it's the Laurent series? After this they multiply the two expressions together: $(z-1) e^{1 / z}=(z-1) \sum_{n=0}^{\infty} \frac{z^{-n}}{n !}=z-\sum_{n=1}^{\infty}\left(\frac{n}{n+1}\right) \frac{z^{-n}}{n !}$ Is that because $z-1$ is a polynomial and thus is analytically around $z=0$ and is it's own Taylor series and Laurent series. So we can just find the Laurent series of each factor and multiply them together to find the final Laurent series? Fx: Consider $h(z)=f(z)g(z)$ , if we want to find the Laurent series around $z=z_0$ . If the Laurent series of f(z) around $z_0$ is $\sum_{n=-\infty}^{\infty} a_{n}\left(z-z_{0}\right)^{n}$ and of $g(z)$ is $\sum_{n=-\infty}^{\infty} b_{n}\left(z-z_{0}\right)^{n}$ . Is the Laurent series of $h(z)$ then: $h(z)=(\sum_{n=-\infty}^{\infty} a_{n}\left(z-z_{0}\right)^{n}) (\sum_{n=-\infty}^{\infty} b_{n}\left(z-z_{0}\right)^{n})$ ?","['complex-analysis', 'taylor-expansion', 'laurent-series']"
4031780,Using the chain rule correctly with matrices as the partial derivative,"I hope this question fits the math.stackexchange environment. I try to understand how a Neural Network calculates the derivatives along the Backpropagation with the chain rule correctly, but I struggle a bit with some explanations with tensors. What I have calculated For my example I used the last hidden layer of a small network with only three neurons on the hidden layer and two outputs, which results in: $$
W = \begin{bmatrix}w_{1,1} & w_{1,2} & w_{1,3} \\w_{2,1} & w_{2,2} & w_{2,3} \end{bmatrix}, \vec{b} = \begin{bmatrix}b_1 \\ b_2 \end{bmatrix}, \vec{z} = \begin{bmatrix}z_1 \\ z_2 \end{bmatrix}
$$ and $$
\vec{a} = \begin{bmatrix}a_1 \\ a_2 \\ a_3 \end{bmatrix}
$$ with $$
z_1 = a_1\cdot w_{1,1} + a_2\cdot w_{1,2} + a_3\cdot w_{1,3} + b_1
$$ and $$
z_2 = a_1\cdot w_{2,1} + a_2\cdot w_{2,2} + a_3\cdot w_{2,3} + b_1
$$ or in vectorform: $$
\vec{z} = W \cdot \vec{a} + \vec{b}
$$ The loss function cross-entropy is used which results in the derivative: $$
\nabla_\vec{z} Loss = \vec{q} - \vec{1}_y
$$ with $\vec{q}$ being the vector presentation of the softmax-function and $\vec{1}_y$ being a zero vector with a $1$ at position $y$ . So on this Basis I started to calculate the partial derivatives, beginning with the bias-vector: $$
\frac{\partial \vec{z}}{\partial \vec{b}} = \begin{bmatrix} \frac{\partial z_1}{\partial b_1}=1 && \frac{\partial z_1}{\partial b_2}=0 \\  \frac{\partial z_2}{\partial b_1}=0 && \frac{\partial z_2}{\partial b_2}=1  \end{bmatrix}
$$ With this Jacobi-Matrix I am able to use the chainrule $\biggl(\frac{\partial \vec{z}}{\partial \vec{b}}\biggr)^\top \nabla_\vec{z} Loss$ to calculate the upgrade vector for $\vec{b}$ If I do now the similar with the Matrix $W$ I result in a tensor which I represent in two seperate matrices: $$
\frac{\partial z_1}{\partial W} = \begin{bmatrix} \frac{\partial z_1}{\partial w_{1,1}}=a_1 && \frac{\partial z_1}{\partial w_{1,2}}=a_2 && \frac{\partial z_1}{\partial w_{1,3}}=a_3 \\  \frac{\partial z_1}{\partial w_{2,1}}=0 && \frac{\partial z_1}{\partial w_{2,2}}=0 && \frac{\partial z_1}{\partial w_{2,3}}=0 \end{bmatrix}
$$ and $$
\frac{\partial z_2}{\partial W} = \begin{bmatrix} \frac{\partial z_2}{\partial w_{1,1}}=0 && \frac{\partial z_2}{\partial w_{1,2}}=0 && \frac{\partial z_2}{\partial w_{1,3}}=0 \\  \frac{\partial z_2}{\partial w_{2,1}}=a_1 && \frac{\partial z_2}{\partial w_{2,2}}=a_2 && \frac{\partial z_2}{\partial w_{2,3}}=a_3 \end{bmatrix}
$$ Where I now struggle I now from different source (Andrew Ng, Goodfellow) the result of the following calculation, which should result in the upgrade matrix: $$
\begin{bmatrix} a_1 \cdot (q_1 - 0) & a_2 \cdot (q_1 - 0) & a_2 \cdot (q_1 - 0) \\ a_1 \cdot (q_2 - 1) & a_2 \cdot (q_2 - 1) & a_2 \cdot (q_2 - 1) \end{bmatrix}
$$ if $y=1$ (zero-based indexing) My first problem I have is to calculate the Transpose of a tensor, since I am not able to figure how to define which axis is responsible for the transpose. My second problem is the calculation of $\biggl(\frac{\partial \vec{z}}{\partial W}\biggr)^\top \nabla_\vec{z} Loss$ . I am able to visualize at least in my mind which calculations are necessary to get the result shown above, but I think the dot-product is not the right product to get this result. Since my background is neither maths or physics, I had problems to figure out how to calculate with tensor correctly but I wanted to show a more accurate representation, than the shortcuts which I see plenty on ""TowardsDataSience"" and Co.","['tensors', 'derivatives']"
4031806,Name for this map of power sets associated with a function?,"Given a function $f: X \to Y$ , there are two associated maps of power sets:  the direct image $f^{\to}: \mathcal{P}(X) \to \mathcal{P}(Y)$ and the inverse image $f^{\leftarrow}: \mathcal{P}(Y) \to \mathcal{P}(X)$ . Consider the map $$
f^?: \mathcal{P}(X) \to \mathcal{P}(Y)
$$ defined by $$
f^?(A) = \{y \in Y: f^\leftarrow(\{y\}) \subset A\}
$$ In words, $f^?(A)$ is the set of all $y \in Y$ so that every $x$ which maps to $y$ is in $A$ . Does this map have a standard name and notation? Note:  I became interested in this while reading this nCategory Cafe post .  Knowing the context is not needed to answer the question, but $f^?$ is associated with universal quantification in the sense of the post.","['logic', 'category-theory', 'functions', 'elementary-set-theory', 'set-theory']"
4031818,When can we affirm this equality about integral?,"Suppose we have $f(x) = \int G(x_{0},x_{1},...)\,dx_{0}\,dx_{1}\dotsb dx_{n}$ When can we affirm that $$df = G(x_{0},x_{1},...)? \tag1$$ Basically, I am having trouble to understand how to deal with differentials of functions, intuitively I thought that we can do that: $$\delta f(x) = \int \sum\left(\frac{\partial G}{\partial dx_{i}}\right)\,dx_{0}\,dx_{1}
\dotsb dx_{n} \tag2$$ But I am not sure how $(2)$ reduces to $(1)$ .","['integration', 'calculus', 'derivatives']"
4031854,Find a function discontinuous everywhere but $\ x = 0$,Find a function discontinuous everywhere but $\ x = 0$ After considering modifications to the Dirichlet function however I have not got anywhere. Is there a method to find such function or is it purely intuition with a bit of creativity?,"['limits', 'functions', 'continuity']"
4031856,1990 BMO1- Maximising/minimising $PC/PD$ where $P$ is a point on the line $AB$ and $ABCD$ is a square,"Recently, I've asked for help from my teacher, as well as gathering help from other people. At the moment, what troubles me is how to even start to find the maximum and minimum points without a calculator, other than that there is some symmetry in the question, so if $PC/PD$ is a maximum where point $P$ is one side of the line, then if point $P$ was on the opposite side of the line $AB$ , then it would be the minimum instead. As well as this, given that this was once an Olympiad question, I haven't found any simple solutions to the problem without the use of a computer or calculator to maximise and minimise it. From some research, the only other source of help I had found were hints from The Mathematical Olympiad Handbook- an Introduction to Problem Solving based on the First 32 British Mathematical Olympiads 1965-1996 If anyone has solutions to the question without using computation, it would be much appreciated. Preferably, simplify it and show each step, as this is meant to be a question aimed at students, and I just want to see a solution in full and see the process. Here is the question written here in full: (1990 British Mathematical Olympiad Round 1 - Question 2) $ABCD$ is a square and $P$ is a point on the line $AB.$ Find the maximum and minimum values of the ratio $PC/PD$ , showing that these occur for the points $P$ given by $$AP \times BP=AB^2$$","['contest-math', 'optimization', 'geometry', 'ratio']"
4031865,Formal Proving Discrete maths,Prove using logical equivalence rules/laws that $$~[𝑎 ∧ ~𝑏 ∧ (𝑎 ∨ 𝑐)] ∨ (~𝑎 ∧ 𝑏) ∧ ~(𝑎 ∨ ~𝑏 ∨ ~𝑐) ≡ ~𝑎 ∨ b$$,"['logic', 'discrete-mathematics']"
4031932,$\lim_{n\to\infty} \frac{2^n(n^4-1)}{4\cdot 3^n + n^7}$,"How do I solve this example? I tried to point out the fastest growing term $2 ^ n$ and $3 ^ n$ , but that doesn't seem to lead to the result. I know the limit is $0$ that's obvious, but I don't know how to work on it. $$\lim_{n\to\infty} \frac{2^n(n^4-1)}{4\cdot 3^n + n^7}$$ Or can I make a power estimate of a theorem on a tightened sequence, for example, that $2 ^ n$ / $13 ^ n$ (smaller) goes to zero and the other side $2 ^ n$ / $3 ^ n$ (bigger) also goes to zero? $\lim_{n\to\infty} 2 ^ n$ / $13 ^ n$ < $\lim_{n\to\infty} \frac{2^n(n^4-1)}{4\cdot 3^n + n^7}$ < $\lim_{n\to\infty} 2 ^ n$ / $3 ^ n$","['limits', 'calculus', 'limits-without-lhopital']"
4031934,"Is $C(\Bbb R)$ homeomorphic to $C([a,b])$?","Let $C(\Bbb R)$ be the space of all continuous self-maps of $\Bbb R$ in the compact convergence topology (which is same as compact-open topology) and $C([a,b])$ be the space of all continuous self-maps of [a,b] in the uniform topology. Is $C(\Bbb R)$ homeomorphic to $C([a,b])$ ?","['continuity', 'general-topology', 'functions', 'real-analysis']"
4031962,Number of inequivalent one-dimensional representations of a group of order $p^n$,"This is from an old comprehensive exam: Prove that any group with $p^n$ elements has a nontrivial center
and use this to prove that any group with $p^n$ elements has at
least $p$ inequivalent one-dimensional complex representations.
Here $p$ is a prime number. The part about having a nontrivial center is easily done with the class equation, but I'm having trouble with the part about $p$ inequivalent one-dim representations. The number of one dimensional representations is given by the order of the abelianization of the group: $$G/[G,G]$$ If the group is order $p$ or $p^2$ I can show that this holds, given that groups of order $p$ are cyclic, and therefore abelian, so the commutator subgroup is trivial. I also know that groups of order $p^2$ are abelian, so the commutator subgroup is again trivial. But besides these cases I'm struggling to see how having a nontrivial center will yield the result. Any help would be appreciated!","['group-theory', 'representation-theory']"
4031976,Show that $\prod_{k=0}^n |x-k| \le (n-1)!/2$ for $1 \le x \le n-1$.,"Let $n \ge 3$ , $x \in \Bbb R$ such that $1 \le x \le n-1$ . Show that $\prod_{k=0}^n |x-k| \le (n-1)!/2$ . For $n=3$ , $1 \le x \le 2$ we want to show $|x(x-1)(x-2)(x-3)|=x(x-1)(2-x)(3-x) \le 1$ .
We see that some large bounds are easy to guess: $$
x(x-1)(2-x)(3-x) \le 2 \times 1 \times 1 \times 2=4
$$ but not precise enough. Another try: $$
x(x-1)(2-x)(3-x) \le x(3-x) \le 9/4
$$ the last inequality is from the study of $f(x)=x(3-x)$ .
Same problem.
For $n=3$ and $1 \le x \le 2$ let $0 \le y=x-1 \le 1$ .
We get using AM-GM \begin{align}
|x(x-1)(x-2)(x-3)| &= |(y+1)(y)(y-1)(y-2)|\\
&=y(1-y^2)(2-y)\\
&\le (\frac{3-y^2}{3})^3\\
&\le  1
\end{align} For the general case I tried to use the same approach:
let $1 \le x \le n-1$ ie. $0 \le y = (x-1)/(n-2) \le 1$ . We have \begin{align}
\prod_{k=0}^n |x-k|
&=
\prod_{k=0}^n |(n-1)y +1-k|\\
&=
\prod_{k=0}^l |(n-1)y +1-k| \prod_{k=l+1}^n |(n-1)y +1-k|\\
&=
\prod_{k=0}^l ((n-1)y +1-k) \prod_{k=l+1}^n (k-1-(n-1)y)\\
\end{align} where $l = \lfloor (n-1)y+1\rfloor$ . Applying AM-GM doesn't really help here. Suppose $y<1$ ie. $l<n$ , we see that $$
\prod_{k=0}^l ((n-1)y +1-k) \le \prod_{k=0}^l (n-k)=\frac{n!}{(n-l-1)!}
$$ and $$
\prod_{k=l+1}^n (k-1-(n-1)y) \le \prod_{k=l}^{n-1} (k) = \frac{(n-1)!}{(l-1)!}
$$ So $$
\prod_{k=0}^n |x-k|\le
\frac{(n-1)!}{(l-1)!}\frac{n!}{(n-l-1)!}
$$ The bound is too big...
I also tried this approach: let $f(x)= \prod_{k=0}^n (x-k)$ , $f'(x)=f(x)(\sum_{k=0}^n \frac{1}{x-k})$ .
I'm trying to find informations about $x$ such that $f'(x)=0$ . Let $x \in \Bbb R - [|0,n|]$ . So $\sum_{k=0}^n \frac{1}{x-k}=0$ . But I'm stuck here. My questions are: Does someone have a hint or a proof? Can someone give me some advice on how to handle such problems during an exam? Thanks :)","['factorial', 'real-analysis', 'calculus', 'polynomials', 'inequality']"
4032015,Dirichlet series for $\zeta(s) \zeta(2s)\zeta(3s) \zeta(6s)^{-1}$,"Let $\kappa(n)$ be defined, for $n=p_1^{a_1}\dots p_k^{a_k}$ , by $\kappa(n)=a_1\cdot a_2\dots \cdot a_k$ . Prove that $$
\sum_{n=1}^{\infty}\frac{\kappa(n)}{n^s}=\frac{\zeta(s)\cdot \zeta(2s) \cdot \zeta(3s)}{\zeta(6s)}.
$$ I would like to prove this statement, however I'm not sure what to do with the $\zeta(n)$ function. Are there any useful properties I can use?","['riemann-zeta', 'number-theory', 'zeta-functions', 'elementary-number-theory']"
4032067,"Norm turning $C^\infty(\overline{B(0,1)})$ into a Banach space?","I've been trying to find a proof that for $K=\overline{B(0,1)} \subseteq \mathbb R^n$ , we can find some norm or notion of convergence (I am indifferent as to how we specify the topology) on $\mathcal C^\infty (K, \mathbb R) = \mathcal C^\infty (K)$ such that it is a Banach space. To my surprise, I couldn't find anything of the sort. This fact is stated in the answer here , so I have reason to believe it's true. Also, I saw here how this works but only for $n=1$ , and there the proof of the limit function also having a continuous derivative depends on being able to express the limit function with respect to the integral of the derivative. Can this approach be generalised to $\mathcal C^\infty(K)$ (there would be an obvious problem  of convergence with taking the sum of the supremum norms of all partial derivatives of all orders), or how would one endow $\mathcal C^\infty (K)$ with a topology such that it is complete?","['general-topology', 'functional-analysis', 'uniform-convergence', 'real-analysis']"
4032071,Prove that $ \sum \frac{2^k}{k}$ is divisible by $2^M$,"For each integer $M > 0$ , there ${\bf exists}$ an $n$ such that $$ \sum_{k=1}^n \dfrac{ 2^k}{k} $$ is divisible by $2^M$ ${\bf try}$ Im struggling a bit to visualize this exercise. So, I tried to see for concrete number, for instance take $M=1$ , then $n=2$ works: as $$ 2 + \dfrac{2^1}{2} = 2^1 (1 + 2 )$$ Now, take $M=2$ and factor $$ 2^{2} \underbrace{ \left( \dfrac{1}{2} + \dfrac{1}{2} + \dfrac{2}{3} + \dfrac{2^4}{4} ... + \dfrac{2^{n-2} }{n} \right) }_{(*)}$$ now, we need to choose $n$ so that $(*)$ is an integer. Im unable to do so. Any help?","['calculus', 'arithmetic']"
4032072,Doubts in the solution of a Riemann Hilbert problem,"Consider the following Riemann-Hilbert problem as given on Page 9 of this paper : $$\Phi^+(t)- \Phi^-(t) = 2u(t)$$ $$\Phi^+(t)+ \Phi^-(t) = \frac{P}{\pi i } \int_{t_1}^{t_2} \frac{d\zeta}{\zeta} u(\zeta) \frac{\zeta +t}{\zeta - t} = E(t - t^{-1})$$ where $E\in \mathbb{R}$ , $P$ denotes the principal value of the integral, and $\Phi^{\pm}(t)$ denote the values of the function $\Phi$ as the point $t$ on the counterclockwise arc $(t_1, t_2)$ is approached from the left and the right respectively. The function $u(t)$ takes the boundary values $u(t_1) = u(t_2) = 0$ . We have an additional constraint here as well: $$\Phi(\infty) = -1.$$ The author then proceeds to write down the solution of this problem which I am confused by. The first confusion is the claim is that ""the function $h(z) = \left[ (z-t_1)(z-t_2)\right]^{1/2}$ solves the problem $h^+ + h^- =0$ upto an entire function."" I am not able to see the same. The author then writes the ansatz for the function $\Phi(z) = h(z)H(z)$ . Using the Plemelj formula and calculating the residues, the author then obtains the expression for $H(z)$ as: $$H(z) = \frac{E}{2} \left[ \frac{z-z^{-1}}{\sqrt{(z-t_1)(z-t_2)}}+1 + \frac{1}{z}\right] $$ Is there an argument why the minus sign in the first term's numerator of the above equation gets converted into a plus sign in the following equation for $\Phi(z)$ ? $$\Phi(z) = \frac{E}{2} \left( z+ \frac{1}{z}\right) + \frac{E}{2} \left(\frac{1}{z} + 1\right) \sqrt{(z-t_1)(z-t_2)}$$ Using the above equation the author substitutes $\Phi(\infty) = -1$ to obtain: $$\frac{1-\cos \alpha}{2} = \frac{1}{E}$$ This follows if I set $z = e^{i\alpha}$ . However it does not seem reasonable to do so when $z=\infty$ . Is there some other way to see this?",['complex-analysis']
4032092,Solution of an IVP cannot be extended beyond an Interval.,"Consider the following  differential equation $\dfrac{dy}{dx} = 1 + y^2$ , $y(0) = 1$ I need to show the solution cannot be extended beyond $(-3\pi/4, \pi/4)$ By uniqueness and existence theorem I can get an interval which guarantees a unique solution, but I don't really understand how should I use this to prove the above claim ? A complete solution will be helpful here, since I am really struggling with these kind of problems.","['ordinary-differential-equations', 'real-analysis']"
4032112,Shifrin Differential Geometry Exercise $1.2.27$ -- A Differential Equation For Bikes,"The Question Suppose the front wheel of a bicycle follows the arclength-parametrized plane curve $\vec{\alpha}$ . Determine the path $\vec{\beta}$ of the rear wheel, $1$ unit away. As the hint explains, the goal is a differential equation involving $\theta$ , the angle of the front wheel with the axle of the bike, and $\kappa$ , the curvature of $\vec{\alpha}$ . This question is very interesting to me, and I haven't seen a solution written up anywhere. This is sort of shocking to me as it seems like it should be a very relevant problem for e.g. autonomous driving. What I've Tried I've only been able to make minimal progress. The hint tells us to write $\vec{\alpha} - \vec{\beta}$ in terms of $\theta$ , $\vec{T}$ (i.e. $\vec{\alpha}'$ ), and $\vec{N}$ (i.e. $\frac{\vec{\alpha}''}{\kappa}$ ). We obviously have $\| \vec{\alpha} - \vec{\beta}\|^2 = 1$ . Differentiating, we obtain $$
(\vec{\alpha}' - \vec{\beta}') \cdot (\vec{\alpha} - \vec{\beta}) = 0
$$ That is: $$
(\vec{T} - \vec{\beta}') \cdot (\vec{\alpha} - \vec{\beta}) = 0
$$ Differentiating again, we obtain $$
(\kappa \vec{N} - \vec{\beta}'') \cdot (\vec{\alpha} - \vec{\beta}) + (\vec{T} - \vec{\beta}') \cdot (\vec{T} - \vec{\beta}') = 0
$$ Now it seems to me we should have $$
\vec{T} \cdot \vec{\beta}' = \|\vec{\beta}'\| \cos \theta
$$ So we can expand $$
(\vec{T} - \vec{\beta}') \cdot (\vec{T} - \vec{\beta}') = 1 - 2\|\beta'\| \cos\theta + \|\vec{\beta}'\|^2
$$ And that's as far as I've gotten. It seems like I'm going about this all wrong. In particular, I have no idea what to do with the derivatives of $\vec{\beta}$ . If only $\vec{\beta}$ were arclength parametrized I feel like I could make some progress, but I don't think there is any reason it should be. The only thing I can think is that we should have $$
\vec{\alpha} - \vec{\beta} = \lambda \vec{\beta}'
$$ for some $\lambda$ that could depend on the arclength of $\vec{\alpha}$ . I didn't push too far in this direction, though, since it required introducing yet another unknown. I thought of yet another line of attack. Since $\|\vec{\alpha} - \vec{\beta}\| = 1$ , we can say that $\|\vec{\alpha} - \vec{\beta}\|$ is just $\vec{T}$ rotated by $\theta$ , i.e. $$
\vec{\alpha} - \vec{\beta} = \cos (\theta) \vec{T} + \sin(\theta) \vec{N}
$$ With this expression I've gone as far as the hint suggests, but I don't see what to do next. What am I missing? If this post summons Ted Shifrin, and he'd rather answers to his textbook questions not be given out, I'd be happy to delete this question and post it as a reference request instead. I really am shocked I haven't been able to find this problem written about anywhere. I'm guessing it's because I'm bad at searching the literature, not because it actually hasn't been written about.","['curvature', 'ordinary-differential-equations', 'differential-geometry']"
4032121,Proof that group homomorphisms are associative,"Let $\varphi:G\to H$ and $\psi: H\to I$ and $\chi:I\to J$ be homomorphisms of the groups $G,H,I,J$ .  I want to prove that homomorphisms are associative, that is to say $\varphi\circ (\psi\circ \chi) = (\varphi\circ\psi)\circ \chi$ .  My proof is as follows: All homomorphisms are functions.  All functions are associative.  Therefore all homomorphisms are associative. However, my tutor claimed that this was a weird and over-complicated proof, and that homomorphisms are just associative by definition (and that most mathematicians would be ""boggled at the weird gyrations"" that I made in my proof).  I wanted to see if I have some kind of profoundly weird or mistaken understanding here.  I claimed that homomorphisms are not defined to be associative, as I don't see that in the Wikipedia article or any textbook.  ( https://en.wikipedia.org/wiki/Homomorphism ) (Of course my proof relies on the principle that all functions are associative, which I believe for the purpose of this proof I can assume.)","['group-homomorphism', 'group-theory', 'functions', 'function-and-relation-composition']"
4032148,Expressing functions $n\to n$ as evaluation of polynomials mod $n$,"Consider $f:n\to n$ where $n:=\{0,1,2,\ldots,n-1\}$ . We can associate to $f$ a polynomial $P_f$ of degree $<n$ by defining $P_f(x)=f(0)x^0+f(1)x^1+\cdots +f(n-1)x^{n-1}$ . This gives another way to evaluate $f$ : we have the literal function evaluation $x\mapsto f(x)$ , and the ""fancy"" evaluation $x\mapsto \sum_{k<n}f(k)x^k$ . I wish to understand a connection between these two evaluations. Question: For $f:n\to n$ , is there some polynomial $P(x)$ which evaluates to $P(x)\equiv f(x)$ (mod $n$ ) for all $x\in \mathbb{N}$ ? What I've done: Assuming $n$ is prime, we only need to consider polynomials of degree $<n$ since by Fermat's Little Theorem $x^n\equiv x$ . My idea was to find polynomials $i_k(x)$ which evaluate to the indicator function of the singleton $\{k\}$ so that $i_k(x)\equiv \begin{cases}1&x=k\\0&\text{else}\end{cases}$ Once we have these, every function can be written as a linear combination of the $i_k$ by $\left(\sum_{k<n}f(k)\cdot i_k\right)(x)\equiv f(x)$ . To find these $i_k$ , I used linear algebra to go in reverse. We start with the $n\times n $ matrix of the powers of $k<n$ reduced mod $n$ . That is, $A_{i,j}=i^j$ . For example, for $n=3$ this matrix is $A_3:=\begin{bmatrix}1&0&0\\1&1&1\\1&-1&1\end{bmatrix}$ . For $n=4$ , we have $A_4:=\begin{bmatrix}1&0&0&0\\1&1&1&1\\1&2&0&0\\1&-1&1&-1\end{bmatrix}$ and for $n=5$ it is $A_5:=\begin{bmatrix}1&0&0&0&0\\1&1&1&1&1\\1&2&-1&-2&1\\1&-2&-1&2&1\\1&-1&1&-1&1\end{bmatrix}$ . If the inverse of $A_n$ exists, it gives us the conversion matrix. In the case $n=3$ we have $A_3^{-1}=\begin{bmatrix}1&0&0\\0&-1&1\\-1&-1&-1\end{bmatrix}$ . Consider for example the function $0\mapsto 1,1\mapsto 2, 2\mapsto 0$ . We compute $\begin{bmatrix}1&0&0\\0&-1&1\\-1&-1&-1\end{bmatrix}\begin{bmatrix}1\\2\\0\end{bmatrix}\equiv\begin{bmatrix}1\\1\\0\end{bmatrix}$ so our desired polynomial is $x^0+x^1$ . Indeed, this evaluates correctly mod 3. For $n$ prime, this matrix seems to always be invertible (at least up to 11 according to wolfram). Moreover, the leftmost column of the inverse matrix is always $\begin{bmatrix}1\\0\\\vdots\\0\\-1\end{bmatrix}$ so the indicator $i_0(x)$ is the polynomial $1-x^{p-1}$ . This corresponds to the corollary of Fermat's Theorem that $p\not\mid x\Rightarrow x^{p-1}\equiv 1$ . Also, the bottom row of the inverse matrix always seems to be $\begin{bmatrix}-1&-1&\cdots&-1&-1\end{bmatrix}$ . I have not been able to prove that $A_p$ is always invertible for $p$ prime. For $n$ composite the $A_n$ seems never to be invertible, though I haven't been able to prove this either. However a proof of this wouldn't even answer the original question. Fermat's Theorem doesn't hold for composites, so we might theoretically find a suitable polynomial of degree $\geq n$ . I am also wondering about potential applications of this idea. If the symmetric group $S_p$ can be represented using polynomials in this way, group multiplication would correspond to composition of the polynomials (mod $p$ ). For example, we would have that $S_3\cong \{x,x+1,x+2,2x,2x+1,2x+2\}$ . We don't typically think of $S_3$ as being linear polynomials (mod 3) under composition, but apparently (up to isomorphism) it is ! We could take this idea even further: for every $n$ , there's always a least prime $p\geq n$ so we could potentially pick an embedding $S_n\leq S_p$ . Then by Cayley's Theorem we could represent any finite group as composing polynomials (mod $p$ ) for some prime. Perhaps representing groups  this way would give new insights into group theory? I would also appreciate any other ideas for potential applications. Thank you!","['group-theory', 'functions', 'prime-numbers']"
4032222,$(a_n)_{n=1}^\infty$ & $(b_n)_{n=1}^\infty$ are seq st $(a_n)_{n=1}^\infty$ & $[{(a_n)_{n=1}^\infty + (b_n)_{n=1}^\infty}]$ con. Prove $(b_n)$ con,"Suppose $(a_n)_{n=1}^\infty$ and $(b_n)_{n=1}^\infty$ are sequences such that $(a_n)_{n=1}^\infty$ and $[{(a_n)_{n=1}^\infty + (b_n)_{n=1}^\infty}]$ converge. Prove that $(b_n)_{n=1}^\infty$ converges I can say $b_n=(a_n + b_n)-a_n$ . Since both $(a_n)_{n=1}^\infty$ and $[{(a_n)_{n=1}^\infty + (b_n)_{n=1}^\infty}]$ converge, isnt' there a subtraction rule that says that because those both converge, that $b_n$ would also converge?","['complex-analysis', 'calculus', 'analysis', 'real-analysis']"
4032239,Point within the interior of a given angle,The point $M$ is within the interior of given angle $\alpha$ . Find the distance between $M$ and the vertex of the angle ( $OM=?$ ) if $a$ and $b$ are the distances from $M$ to the sides of the angle. We can see that $$OM^2=OP^2+PM^2=OK^2+KM^2$$ Let $OP=x;PK=y$ . Then $$2OM^2=OP^2+PM^2+OK^2+KM^2\\=x^2+b^2+y^2+a^2.$$ I can't approach the problem further. How can we use the given angle $\alpha$ ? Thank you in advance!,"['euclidean-geometry', 'reflection', 'geometry', 'trigonometry', 'geometric-transformation']"
4032241,Independence of two intertwined random variables,"Let $X_0$ , $X_1$ and $X_2$ be three mutually independent random variables. We define two more random variables $D_1$ and $D_2$ as follows: $$D_1 = X_1 + X_0\\[1ex]
D_2 = X_2 + X_0$$ We're interested in arguing (in the most effective and simple way) if $D_1$ and $D_2$ are independent $D_1|X_0=x_0$ and $D_2|X_0=x_0$ are independent If $D_1$ and $D_2$ are independent, it must be $\text{Cov}(D_1,D_2)=0$ : $$\text{Cov}(D_1, D_2)=\mathbb{E}[D_1 D_2]-\mathbb{E}[D_1]\mathbb{E}[D_2] =\\
=\mathbb{E}\left[ (X_1+X_0)(X_2+X_0) \right] - \mathbb{E}\left[ X_1+X_0 \right]\mathbb{E}\left[ X_2+X_0 \right]=\\
=\mathbb{E}\left[ X_1 X_2 + X_1 X_0 + X_0 X_2 + X_0^2 \right] - \mathbb{E}\left[ X_1 \right]\mathbb{E}\left[ X_2 \right] - \mathbb{E}\left[ X_1 \right]\mathbb{E}\left[ X_0 \right] - \mathbb{E}\left[ X_0 \right]\mathbb{E}\left[ X_2 \right] - \left(\mathbb{E}\left[ X_0 \right]\right)^2=\\
=\mathbb{E}\left[ X_1 \right]\mathbb{E}\left[ X_2 \right] + \mathbb{E}\left[ X_1 \right]\mathbb{E}\left[ X_0 \right] + \mathbb{E}\left[ X_0 \right]\mathbb{E}\left[ X_2 \right] + \mathbb{E}\left[ X_0^2 \right] - \mathbb{E}\left[ X_1 \right]\mathbb{E}\left[ X_2 \right] - \mathbb{E}\left[ X_1 \right]\mathbb{E}\left[ X_0 \right] - \mathbb{E}\left[ X_0 \right]\mathbb{E}\left[ X_2 \right] - \left(\mathbb{E}\left[ X_0 \right]\right)^2=\\
=\mathbb{E}\left[ X_0^2 \right] - \left(\mathbb{E}\left[ X_0 \right]\right)^2 = \text{Var}(X_0)\neq 0$$ So, if not for the trivial case in which $X_0 = \text{constant}$ , $\mathbf{D_1}$ and $\mathbf{D_2}$ are not independent . 2.
One could do the same thing of point 1. all over again or just notice that this case is exactly the trivial one in which $X_0 = x_0$ , a fixed ""constant"". So, $\mathbf{D_1|X_0 =x_0}$ and $\mathbf{D_2|X_0 =x_0}$ are independent . MY QUESTION Are my proofs correct? Is there a simpler way to prove the same thing or even just argument the same results in a better, more intuitive way? A BONUS QUESTION How can I test these theoretical results with the following example? (In order to really ""see"" what I found) $X_0$ is the outcome of the fair coin ""COIN"": $X_0=1$ if Heads , $X_0=0$ if Tails . $X_1$ is the outcome of the 6-faced fair dice ""DICE1"": $X_1=\{1,2,3,4,5,6\}$ $X_2$ is the outcome of the 6-faced fair dice ""DICE2"": $X_2=\{1,2,3,4,5,6\}$ SOMETHING THAT PERPLEXES ME Intuitively and ""constructively"", I would say that the event to observe $D_1=k$ is independent from that of observing $D_2 = h$ , infact the first is the event of obtaining $X_1 + X_0 = k$ with a throw of DICE1 and a toss of COIN while the second is the event of obtaining $X_2 + X_0 = k$ with a throw of DICE2 and a different toss of COIN: these two ""procedures"" are totally independent from each other and so should be their probabilities (?) But this contradicts my analytical results, doesn't it! Why is that?","['statistics', 'covariance', 'variance', 'conditional-probability', 'probability']"
4032246,Is any subset of $\mathbb{R}$ a Borel set?,"Consider $S \subset \mathbb{R}$ . Define for all $n \in \{1,2,3,\ldots\}$ \begin{align*}
S_n := \bigcup_{a \in S} \left (a - \frac{1}{n}, a + \frac{1}{n}\right)
\end{align*} Since arbitrary union of open sets is open, $S_n$ is open for all $n$ . Now \begin{align*}
S = \bigcap_{n=1}^{\infty} S_n
\end{align*} Hence $S$ is the intersection of countably many open sets, therefore, it is a Borel set. Are my steps correct or is there something wrong? If there is something wrong, could you point out which step exactly?","['borel-sets', 'measure-theory', 'real-analysis']"
4032256,Constructing a Multivariate Probability Distribution Formula,"Suppose there are three individuals playing a simple game, Player $1$ , Player $2$ , and Player $3$ . In each round of the game, one of the players is uniformly randomly selected to receive the point for that round.  The game ends when any player achieves a score of $10$ points. How can a multivariate probability distribution formula be constructed that would express the probability of each possible combination of the total number of rounds a game will contain, and an arbitrarily chosen player's (i.e., Player $1$ 's) score at the end of the game?  How does this formula change if we introduce the assumption that the arbitrarily chosen player did not win the game? I started by noting that the chosen player has a $\frac{1}{3}$ probability of winning each round.  I think that for a fixed number of rounds $n$ , the formula would be the binomial distribution ${n \choose a} (\frac{1}{3})^a (\frac{2}{3})^{1 - a}$ , where by plugging a possible point score into $a$ one obtains the probability of the chosen player attaining that exact point score, but I'm not sure if this is right or where to go from here.","['probability-distributions', 'combinatorics', 'probability-theory', 'probability', 'random-variables']"
4032282,What do we actually rotate with rotational matrices,"We just learned about rotational matrices in a more theoretical context, but we only ""glanced"" at them. My question: What exactly can a matrix rotate (vectors, other matrices, etc...?) and I still don't really get it how it actually rotates another object. Can somebody please give me a simple explanation (it can also be rigorous).","['abstract-algebra', 'linear-algebra', 'real-analysis']"
4032313,Probability of Yahtzee straight with strict re-roll rules,"Assuming you have 5 fair dice and up to 3 re-roll attempts (during which you can re-roll any dice, as per standard Yahtzee rules) what is the probability of succesfully rolling a ""Small Straight"" (aka 1-2-3-4, 2-3-4-5, or 3-4-5-6)? Also, what is the probability of rolling a ""Large Straight"" (aka 1-2-3-4-5 or 2-3-4-5-6)? In either case, assume the following rules for keeping and re-rolling dice: Keep exactly one of each 2, 3, 4, and 5 result Only keep a 1 or 6 result if it is already part of a Small Straight or Large Straight Unfortunately I'm quite unfamiliar with even basic probability problems let alone something with this level of complexity, and online research hasn't helped much. I'm much more comfortable backing into probabilities by simulating roll outcomes like this in Python, though I've hit a wall with those efforts as well for straights in particular, even with these defined rules. Would deeply appreciate some help. Thanks!","['dice', 'probability']"
