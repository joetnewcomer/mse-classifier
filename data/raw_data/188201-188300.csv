question_id,title,body,tags
3506964,Find $\lim\limits_{n \to \infty} \int\limits_0^n \frac1{1 + n^2 \cos^2 x} dx$.,"I have to find the limit: $$\lim\limits_{n \to \infty} \displaystyle\int_0^n \dfrac{1}{1 + n^2 \cos^2 x} dx$$ How should I approach this? I kept looking for some appropriate bounds (for the Squeeze Theorem) that I could use to determine the the limit, but I didn't come up with anything useful.","['integration', 'limits', 'calculus']"
3506984,Can the fact that $\bigcap _{g\in G}gHg^{-1}\lhd G$ be used to prove that certain groups are not simple?,"If $G$ is finite, and $H$ a proper subgroup (which existence is often easy to show by using Sylow Theory), it would be sufficient to prove $core_G(H)=\bigcap _{g\in G}gHg^{-1}\neq \{ 1\}$ to conclude that $G$ is not simple since we know $core_G(H)\lhd G$ . Also, some proofs regarding groups of particular order (say, groups of order $p^nq$ for primes $p$ & $q$ ) not being simple are somewhat long and I'm just in hopes of not having to learn them for my exam. In the infinite case there is some merit to $core_G(H)$ in this regard, since by Poincare's Theorem (namely, if $G$ has a subgroup $H$ with finite index, then $core_G(H)$ also has a finite index in $G$ ) if an infinite group has a subgroup of finite index, then it is not simple. For a finite group $G$ and $H< G$ , I've tried without success to find useful conditions for which $core_G(H)\neq 1$ , so I'm wondering if there are non-trivial sufficient conditions for which $$core_G(H)\neq 1$$","['simple-groups', 'group-theory', 'abstract-algebra', 'soft-question']"
3507063,Large prime factors in a sequence of consecutive numbers,"When trying to solve a number theory problem I came across this other problem which sounds interesting. Let $n$ be a positive integer, and consider $n$ consecutive positive integers $a_1, \ldots, a_n$ that are at most $n^2$ . What is an upper bound for the number of integers in this kind of list that have a prime factor greater than $n$ ? What's interesting is that for any such prime factor, it appears only once as a factor in the list, and there can only be at most $n$ such primes. I'm guessing that $n$ is too large an upper bound and cannot be reached, i.e. there is always at least one number with prime factors all less than or equal to $n$ . I don't have any results besides checking some lists of numbers, and I don't really know how to approach this. Any ideas?","['number-theory', 'elementary-number-theory']"
3507075,Concrete Mathematics: Josephus Problem: Generalised table for small values of $n$,"I am trying to understand how table 1.12 is constructed; where they start with $f(1)=Î±$ and working up from there. This is in the context of trying to find a closed form of the generalised Josephus recurrences: $f(1)=\alpha$ , for $n \geqslant 1$ $f(2n)=2f(n)+\beta$ , for $n \geqslant 1$ $f(2n+1)=2f(n)+\gamma$ , for $n \geqslant 1$ The book then constructs the following table for small values of $n$ $$
\begin{array}{|c|lc|}
\hline
n& f(n) 
\\ \hline
1 &\ \ \alpha
\\ \hline
2 & 2\alpha+\beta
\\ 3 & 2\alpha\ \ \ \ \ \ \ \ \ \ \ \ +\ \ \gamma\\ 
\hline
4 & 4\alpha + 3\beta\\
5 & 4\alpha + 2\beta\ \ \ +\ \ \gamma\\
6 & 4\alpha + \ \ \beta\ \ \ +2\gamma\\
7 & 4\alpha + \ \ \ \ \ \ \ +3\gamma\\
\hline
8 & 8\alpha + 7\beta\\
9 & 8\alpha + 6\beta\ \ \ \ + \gamma
\\ \hline
\end{array}
$$ Do they construct the generalised table from scratch somehow or use the existing concrete table as a stepping stone? So far I've been using the concrete table of small values as a stepping stone to attempt to understand the generalised table. By ""concrete table of small values"" I mean this one $$
\begin{array}{|c|c|}
\hline
n & 1 & 2\ \ 3 &4\ \ 5\ \ 6\ \ 7\ & 8\ \ 9\ \ 10\ \ 11\ \ 12\ \ 14\ \ 15 & 16
\\
\hline
J(n) &1&1\ \ 3&1\ \ 3\ \ 5\ \ 7 &1\ \ 3\ \ 5\ \ 7\ \ 9\ \ 11\ \ 13\ \ 15 & 1
\\
\hline
\end{array}
$$ Where the output of $J(n)$ is grouped by power of two. Using the concrete table (and thus $\alpha = 1$ , $\beta = -1$ , and $\gamma = 1$ )I think I understand why $\alpha$ 's coefficient is $n$ 's largest power of 2 i.e. we're just representing symbolically what the concrete table has. Similarly for $\beta$ 's coefficient decreasing: the $J(n)$ output starts small so we need a negative $\beta$ of larger magnitude at the start to get 1, then reducing the magnitude i.e. for $n=4$ to $n=7$ it is $3\beta$ , $2\beta$ , and $\beta$ . Then we need to start counting upwards with $\gamma$ so $J(n)$ 's output increases: using example above of $n$ 4 to 7 it is $\gamma$ , $2\gamma$ , and $3\gamma$ . It almost seems like there is some kind of ""middle"" that $\beta$ and $\gamma$ are relative to. Assuming the above makes sense, is there a way to mechanically construct that generalised table without needing to refer to anything else?","['recurrence-relations', 'discrete-mathematics']"
3507099,"Gaussian integration by parts, Wick's / Isserlis' theorem?","Wikipedia's entry on Isserlis' theorem has the following identity: $$ \mathbb{E}\Big[ X_1 f(X_1, ..., X_n) \Big] = \sum_{i=1}^n \mathbb{E}[X_1 X_i] \mathbb{E}\Big[ \frac{\partial}{\partial X_i} f(X_1, ..., X_n) \Big],$$ where $X_1, ..., X_n $ is a zero-mean multivariate Gaussian random vector. My questions: What is this theorem called? Is this some incarnation of Wick's theorem? (I do not understand the notation on this page at all). What are the conditions required  on $f$ for this identity to hold?","['integration', 'probability', 'gaussian-integral']"
3507129,Proving (or disproving) multivariable limit existence,"I've come across a problem I think is impossible, but I could be wrong. It goes Let $$f(x, y) = \begin{cases}0,& \text{ if }y = 0\\ \ \\ y + x\sin\tfrac1y,&\ \text{ otherwise}\end{cases}$$ Show that the limits $\lim\limits_{(x y) \to (0,0)}$ and $\lim\limits_{ y \to 0}$ $\lim\limits_{ x \to 0}$ $f(x,y)$ exist, while $\lim\limits_{ x \to 0}$ $\lim\limits_{ y \to 0}$ $f(x,y)$ does not exist. In the original equation, if you let $y=0$ , then $f(x,0) = 0 + x\sin(1/0)$ , which isn't $0$ because it's undefined, right? So how can we even continue with the problem? And, if we can continue, how can the first limit $\lim\limits_{x y \to (0,0)}$ exist if $xy$ can only map to $0$ , not $(0,0)$ ?","['multivariable-calculus', 'calculus']"
3507142,Factoring $3n^3 - 39n^2 + 360n + 20$,I am wondering how to factor $$f(n) = 3n^3 - 39n^2 + 360n + 20$$ the right way. I think the factors are equal to $$(n - 39.9762)(n - 12.0791)(n + 0.055248)$$,"['cubics', 'algebra-precalculus', 'factoring', 'polynomials']"
3507152,Is a closed form for quaternary sequences avoiding some patterns known?,"Let $a_n$ be the number of quaternary strings over $\{0,1,2,3\}$ of length $n\geq 1$ not containing the substrings $02,03,12,20,21,$ and $30.$ Is there a known closed form for $a_n$ which has appeared in the literature?","['fibonacci-numbers', 'combinatorics', 'reference-request']"
3507159,How many digits are there in $99^{99}$? [duplicate],"This question already has answers here : How many digits is $99^{99}$ without a calculator? (5 answers) Closed 4 years ago . Question: How many digits are there in $99^{99}$ ? My attempt: Observe that $$99^{99} = (100\times 0.99)^{99} = 100^{99}\times 0.99^{99}.$$ Note that $100^{99} = 10^{198}$ has $199$ digits and $$0.99^{99} = \left(1-\frac{1}{100}\right)^{99} \approx e^{-1} \approx 0.37.$$ Therefore, there are $198$ digits in $99^{99}.$ Is my reasoning above correct?","['mental-arithmetic', 'algebra-precalculus']"
3507180,Understanding the roles of rapid decay & smoothness in the Fourier transform,"1) If we define a rapidly decaying function in the usual way, it says nothing about derivatives; rather, just that its decay beats any polynomial growth. 2) A Schwartz class function is then simply a smooth function that is also rapidly decaying. 2) A Schwartz class function is not only smooth, but it and all of its derivatives are also rapidly decaying. 3) The Fourier transform is an isomorphism from the Schwartz class functions to themselves. This all feels very beautiful and deep, like a truth underlying something much bigger. At first glance it seems like rapid decay is such a powerful condition, and that smoothness is something quite common and boring. Because of these assumptions, I wouldn't have been surprised if the rapid decay condition for Schwartz class functions was the ""more important"" of the two conditions (whatever that means... perhaps that there is a loosening of the condition that could still lead to some interesting analysis? ) But as soon as you take away the smoothness condition, things start to unravel: Obviously $e^{-|x|}$ is rapidly decreasing, but it of course fails to be smooth at $x=0$ . And its Fourier transform ends up being $\frac{2}{1+\omega}$ , which is certainly not rapidly decreasing any longer. Strange... But for all $\epsilon > 0$ , the family of functions $f_{\epsilon}(x) = e^{-\sqrt{\epsilon + x^2}}$ is smooth and rapidly decreasing (hence Schwartz class). Therefore $\widehat{f_\epsilon}(\omega)$ is also Schwartz class, even though $f_\epsilon \to f$ uniformly as $\epsilon \to 0$ . Again, quite strange... We could have also created a compactly supported bump function, $\beta(x)$ , that is identically $1$ on some $\epsilon$ -neighborhood of $x=0$ . Then you can use $1-\beta(x/\epsilon)$ as a family of smooth cutoffs to eliminate the point of non-differentiability: $g_{\epsilon}(x) = (1-\beta(x/\epsilon))e^{-|x|}$ . This does the same thing as before, with $\widehat{g_{\epsilon}}(\omega)$ being Schwartz class for all $\epsilon > 0$ , with $g_{\epsilon} \to g$ as $\epsilon \to 0$ . And even if we go one level deeper and consider a function which is $C^1$ , just not smooth, things aren't any better. Consider the function $h(x) = x|x|e^{-x^2}$ . The exponential term, $e^{-x^2}$ , is a Gaussian which is pretty much as nice as it gets when it comes to Fourier transforms; and the other term, $x|x|$ , has a derivative equal to $2|x|$ and hence it is $C^1$ . But sure enough, $\widehat{h}(\omega)$ involves some polynomial terms and the Dawson function , and ends up being $O\left( \omega^{-3} \right)$ . Similar computations can be done for any function of the form $h_k(x) = x^k |x| e^{-x^2}$ , with $k\in \mathbb{N}$ , where each $h_k \in C^{k}(\mathbb{R})$ , and yet none of these have a Fourier transform that has rapidly decay. So clearly being $C^k$ and rapidly decreasing is still not very much better than simply being $C^0$ and rapidly decreasing; and certainly nowhere close to being as good as being smooth and rapidly decreasing. Again, I'm not disputing any of these facts, and these kinds of phenomena where sequences of ""nice"" functions converge to ""not nice"" functions are abundant in analysis. I'm just looking for some deeper understanding or insight (dare I say, intuition ) as to the role smoothness plays when it comes to Fourier transforms. This then would also beg the question of what role rapid decay has as well? How do these two unrelated ideas come together so perfectly for the Fourier transform? And is there analogous concepts when it comes to the more general Fourier transform on locally compact abelian groups ?","['fourier-transform', 'locally-compact-groups', 'functional-analysis']"
3507238,$y^{IV} - y = 0$,$y^{IV} - y = 0$ I solved this differential equation and got an answer : $$y = C_1e^x + C_2e^{-x} +C_3\cos x + C_4\sin x .$$ But the book gives different answer: $$y = C_1\cos2x + C_2\sin2x. $$ Could you please explain me how did they get this?,['ordinary-differential-equations']
3507246,The set of bounded functions are dense in Hilbert Space?,"I'm reading the book ""Semiparametric Theory and Missing Data"" by Anastasios A. Tsiatis and I'm having trouble trying to understand the following fact: Let $Z$ be a random variable. We define $\mathcal{H}$ as the Hilbert space of q-dimensional mean-zero real-valued functions. We also require that the covariance matrix is nonsingular. That is, $\mathcal{H} = \{ h : supp(Z) \rightarrow \mathbb{R}^{q} \ | \ \mathbb{E}[h(Z)] = 0^{q \times 1}, \mathbb{E}[hh^T]^{-1} \text{exists} \}$ The space $\mathcal{H}$ is equipped with the covariance inner product $\langle h_1, h_2 \rangle \triangleq \mathbb{E}[h_1^T h_2]$ . On page 69, the author mentioned that ""any element of $\mathcal{H}$ can be approximated by a sequence of bounded $h$ "" (or equivalently, the set of bounded functions is dense in $\mathcal{H}$ ). Does anyone have any idea why this is true?","['statistics', 'functional-analysis']"
3507412,"If a part of the series diverges, does the whole diverge?","I couldn't find an easy to way to check for the convergence of the series. The sum is supposed to be from 2 to infinity and there is a little mistake in the pic. Since $\frac1{n-1}$ diverges by comparison test with $\frac1n$ . Would it be right to say that the whole series is divergent because one of the part of the sum is divergent ? 
Do l still need to prove that the other part diverges or not ?","['convergence-divergence', 'sequences-and-series']"
3507429,A challenging problem on continuity-MADHAVA-2020.,"Suppose $f$ be a continuous function on $(0,\infty)$ such that $f(\frac{x}{x+1})=f(x)+2$ .Is the function monotonically decreasing?I have tried to prove it in different ways but still I could not find any way out.Also how to show $f(x) \to \infty$ as $x\to 0+$ .Actually it is a question from MADHAVA-2020 which took place on 12 Jan,2020.","['real-analysis', 'continuity', 'sequences-and-series', 'limits', 'problem-solving']"
3507486,"Converse, contrapositive and inverse of only-if statements","I know that ð â ð is the converse of ð â ð. Â¬ðâ Â¬ ð is the contrapositive of ð â ð Â¬ ðâ Â¬ ð is the inverse of ð â ð However, I'm not sure if it's the same for bidirectional statements. Am I right in the following statements? Original statement: Weâll win the ICG cup only if we have enough players.

Converse: Only if we have enough players, will we then win the ICG cup.

Contrapositive: Only if we do not have players, will we then not win the ICG cup.

Inverse: Weâll not win the ICG cup, only if we do not have enough players. If not, how do I go about obtaining the correct statements?","['logic', 'discrete-mathematics']"
3507491,Prove $P(X=Y) = 0$ when $X$ and $Y$ are two random variables on the same probability space.,"Suppose that $X$ and $Y$ are two independent random variable on the same probability space. We assume: $$P(X=x) = P(Y=x) = 0$$ and I want to prove $P(X=Y) = 0$ . 
But we don't know whether the distribution of r.v-s are continuous or not. I tried to solve with the fact that $$
P(X = Y) = E(1_{X=Y})$$ But unfortunately it didn't work well. How to solve this problem in measure theory-wise?","['measure-theory', 'fubini-tonelli-theorems', 'probability-theory', 'probability']"
3507540,IC complex on $\Bbb C = \Bbb C^* \sqcup \{0\}$,"Let $X = \Bbb C$ stratified as $\Bbb C^* \sqcup \{0\}$ . Let $\mathscr L$ be a local system on $\Bbb C^*$ . How to describe $IC(U, \mathscr L)$ ? By describe I mean : compute its stalk at $0$ , maybe find a short exact sequences of complexes involving $IC(U, \mathscr L)$ , ... My thoughts : I don't think we can have $IC(U, \mathscr L) \cong j_* \mathscr L[1]$ or $j_!\mathscr L[1]$ . Indeed, they are ""dual"" (up to changing $\mathscr L$ to $\mathscr L^{\vee}$ ) to each other, and $\Bbb D_X(IC(U, \mathscr L)) \cong IC(U, \mathscr L^{\vee})$ ). But in general, $0$ is in the support of $j_* \mathscr L[1]$ but is not in the support of $j_! \mathscr L$ . However I don't see other candidates. The definition is the image in the category of perverse sheaves of $^pj_! \mathscr L[1] \to \ ^pj_* \mathscr L[1]$ but I don't know how to compute it. Another related question is : If $V$ is a skyscraper sheaf at $0$ , how to describe extensions between $\mathscr L[1]$ and $V$ , in the category of perverse sheaves ? I know everything can be phrased in term of quivers but if possible I would like to use the basic definitions only.","['derived-categories', 'algebraic-geometry', 'sheaf-cohomology', 'sheaf-theory']"
3507591,"For a connected, simply connected Lie group what does the Lie algebra tell me?","By Lie's third fundamental theorem we know that for each finite dimensional Lie algebra $\mathfrak{g}$ there is a unique simply connected and connected Lie group $G$ such that $T_eG \simeq \mathfrak{g}$ . Without knowing anything else about $G$ , what can we use the Lie algebra for in order to get as much information about $G$ In particular, I want to classify the adjoint representation and consequently the adjoint orbits, of $G$ , given a Lie algebra. For example take $\mathfrak{g}=\mathfrak{so}(3)$ . Now I want to pretend I know nothing about the Lie group and classify the adjoint orbits. How can I proceed? What exponential can I use if I know nothing about $G$ ? I am trying to do this in as general a way as possible.","['abstract-algebra', 'lie-algebras', 'lie-groups']"
3507612,Smoothness of total spaces with nice singular fibers,"Let $f:X \to Y$ be a (flat and projective) family of hypersurfaces over a smooth curve $Y$ . If every fiber of $f$ is smooth, it is clear that the total space $X$ is also smooth. I would like to know if the smoothness still holds when we also allow singularities of ordinary double points: If $f$ has finitely many singular fibers but every singular fiber contains only one ordinary double point, is it still true that $X$ is smooth? Here an ordinary double point means the tangent cone is nondegenerate. Since the total space of a Lefschetz pencil is always smooth (as blow-up a smooth one along a smooth locus is smooth), I guess it is reasonable to expect smoothness with those nice singular fibers. Thanks in advance.","['complex-geometry', 'algebraic-geometry', 'deformation-theory', 'reference-request']"
3507657,Prove that $2n\mid m$ is asymmetric,"Let $R$ be a relation on $\Bbb{N}$ given by $$nRm\iff2n\mid m.$$ Prove that $R$ is asymmetric. I know that a symmetric relation is $\forall a,b\in A(aRb\to b\not Ra)$ i.e. $\forall a,b\in A\;\neg(aRb\wedge bRa)$ . From this, suppose that $2n\mid m$ i.e. there exists a $k\in\Bbb{Z}$ such that $m=2kn$ . We need to prove that $n\neq2pm$ , where $p\in\Bbb{Z}$ i.e. $2m\not\mid n$ , right? If it is right then I am not able to prove it. If $m=2kn$ then $(2p)m=(2p)2kn$ . What is next? EDIT Since $2pm=4pkn$ we conclude that $2pm\neq n$ i.e. $2m\not\mid n$ but $2pm=4pkn$ , right?","['divisibility', 'elementary-number-theory', 'proof-writing', 'relations', 'discrete-mathematics']"
3507796,"Prove $\min M$ exists for $M := \{x \in [a,b]: f(x) = 0 \}$","Let $a < b$ be real numbers and $f:I:=[a,b] \to \mathbb{R}$ continuous and $f$ having a zero in $I$ . How can one prove that $f$ has a smallest zero, meaning that $$\min M$$ exists for $$M := \{x \in [a,b]: f(x) = 0\}$$ Can I say that the set $M := \{x \in [a,b]: f(x) = 0\}$ is the inverse image of $\{0\}$ , meaning that $f^{-1}(\{0\}) = M$ . $\{0\}$ is a closed set. Since $f$ is continuous, its inverse image $M$ is closed. In closed sets it holds that $\partial M \in M$ , therefore $\min M = \inf M$ , so a minimum exists in $M$ (and $M$ was the set of zeros in $f$ ) Is that correct?","['continuity', 'functions', 'solution-verification', 'real-analysis']"
3507907,Complex Trigonometry using a Complex Triangle,"When using the trigonometric functions with real numbers we often use a right triangle to visualize and simplify certain calculations. The same can be done when using the trigonometric functions with complex numbers with the triangle shown below: In this triangle all the sides and angles are complex. Additionally, those sides and angles satisfy the following properties: $$
z_1+z_2=z_3=\frac{\pi}{2}\\
w_1^2+w_2^2=w_3^2\\
$$ All the complex trigonometric functions obey the expected SOHCAHTOA relations on this triangle, and the inverse functions behave as expected as well. In particular, deriving the inverse trigonometric functions is made very easy by using this approach. For example, if we are given that: $$\sin^{-1}u=-i\ln\left(iu+\sqrt{1-u^2}\right)$$ and we are asked to derive a similar formula for $\cos^{-1}u$ then we can simply draw a complex right triangle with $w_1=\sqrt{1-u^2}, w_2=u, w_3=1$ , from which it is immediately clear that: $$\cos^{-1}u=\sin^{-1}\left[\sqrt{1-u^2}\right]=-i\ln\left[i\sqrt{1-u^2}+\sqrt{1-\left[\sqrt{1-u^2}\right]^2}\right]\\
=-i\ln\left(u+i\sqrt{1-u^2}\right)$$ The only drawback with this approach is that the real lengths and angles of the triangle have no connection with the complex sides and angles. If anything, the shape of the triangle should just be treated as a visual mnemonic. My question is this: why have I never heard of this approach being used when dealing with trigonometric functions of complex variables?","['complex-analysis', 'trigonometry']"
3507916,"In a topological space $X, G$ is open iff $G\cap\overline{A}\subseteq \overline{G\cap A}$","In a topological space $X$ , $G$ is open iff $G\cap\overline{A}\subseteq \overline{G\cap A}$ , where $A$ is an arbitrary set in $X$ . To prove this I started from definition of an open set, I could not complete it.
I even tried to prove it in metric space using def I could not complete it","['general-topology', 'analysis']"
3507935,Greatest common divisor of consecutive square free numbers,"I guess that every prime number occurs as the greatest common divisor of two consecutive square free numbers, which I don't expect a proof of. But I've done some experiments indicating that: If $m, n$ are consecutive square free numbers then $\gcd(m, n)$ is not composite. Is that true and can it be proved?","['conjectures', 'number-theory', 'gcd-and-lcm', 'elementary-number-theory', 'prime-numbers']"
3508010,Trivial subsigma algebra of the Borel algebra,"Let $(\Omega, \mathcal{F}, P)$ be the unit interval $[0,1]$ equipped with the Borel algebra and the Lebesgue measure (restricted to the Borel algebra). I am trying to verify whether $P$ is 0-1 on the following sub-sigma algebra: for each $x$ , let $A_x=\{y\in [0,1]: y-x\in\mathbb{Q}\}$ . Clearly each $A_x$ is countable and therefore Borel. Moreover $\mathcal{A}=\{A_x: x\in[0,1]\}$ is a partition. Let $\mathcal{G}\subset\mathcal{F}$ denote the collection of $\mathcal{A}$ -saturated sets (i.e.~ $\mathcal{G}=\{F\in\mathcal{F}: \forall x, A_x\subset F\vee A_x\subset F^c\}$ , equivalently the elements of $\mathcal{G}$ are borel sets that are unions of $A_x$ 's). It is easy to verify that $\mathcal{G}$ is in fact a sigma-algebra. I want to check if $P$ is 0-1 on $\mathcal{G}$ . I can't find a way to apply either the Kolmogorov 0-1, Herwitt-Savage 0-1 or the ergodic theorem, and don't know any other way to prove this (or perhaps it is false - but I don't know how to show that either). Any help would be greatly appreciated. I think I have a proof idea, but it relies on certain assumptions that I am not sure are true. So I put it here for scrutiny: Say a rational $q$ is an $n$ -ary rational if $q=\frac{k}{n^i}$ for some $n, i, k$ (e.g.~ $q$ is a dyadic rational if $q=\frac{k}{2^i}$ ). Let $A_x^{n}$ denote the set of $y\in[0,1]$ that differs from $x$ by an $n$ -ary rational. Clearly $\mathcal{A}^n=\{A_x^n:x\in[0,1]\}$ also forms a partition and $A_x=\bigcup_nA_x^n$ . Now let $\mathcal{G}_n$ denote the collection of $\mathcal{A}^n$ -saturated sets in $\mathcal{F}$ . We claim that $P$ is 0-1 on $\mathcal{G}_n$ for all $n$ . This is straightforward when $n=2$ : identify $[0,1]$ with the Cantor space, then $\mathcal{G}_n$ is the tail-sigma-field generated by infinite iid coin tosses. I think the same reasoning applies to $n>2$ . Suppose the reasoning above is cogent. Then let $A\in\mathcal{G}$ . Note that $A\in\mathcal{G}$ only if $A\in\mathcal{G}_n$ for all $n$ . Since $P$ is 0-1 on $\mathcal{G}_n$ , it follows that $P(A)\in\{0,1\}$ .","['measure-theory', 'probability-theory']"
3508107,ANOVA with complex numbers,"I want to analyze my data with respect to their within- and between-group variance (an ANOVA analysis). While I am familiar with the typical approach for real numbers, I have in my case complex numbers at hand (coherence values within a signal processing project).
Can ANOVA be implemented in the canonical manner for complex numbers? Does the use of the F-statistic requires to transform the terms relevant for an ANOVA to real numbers?","['complex-analysis', 'statistics', 'analysis']"
3508153,Prove that $\sup S \leq \inf T$,Could you help me here? Let $S$ and $T$ be nonempty subsets of $\mathbb{R}$ and suppose that for all $s \in S $ and $t \in T$ we have $s \leqslant t$ . Prove that $\sup S \leqslant  \inf T $ PS: My idea was to try 2 cases: when $S \subset T$ and $ S \cap T = \emptyset $ . Am I right? I was able to prove that $t \geq \sup S$ but the final part I couldn't. Any help?,"['analysis', 'real-analysis', 'solution-verification', 'inequality', 'supremum-and-infimum']"
3508167,"KKT multipliers and ""active"" and ""inactive"" constraints on the generalized Lagrangian $L$","The textbook Deep Learning by Goodfellow, Bengio, and Courville, says the following in a section on constrained optimization : The inequality constraints are particularly interesting. We say that a constraint $h^{(i)}(\mathbf{x})$ is active if $h^{(i)}(\mathbf{x}^*) = 0$ . If a constraint is not active, then the solution to the problem found using that constraint would remain at least a local solution if that constraint were removed. It is possible that an inactive constraint excludes  other solutions. For example, a convex problem with an entire region of globally optimal points (a wide, flat region of equal cost) could have a subset of this region eliminated by constraints, or a nonconvex problem could have better local stationary points excluded by a constraint that is inactive at convergence. Yet the point found at convergence remains a stationary point whether or not the inactive constraints are included. Because an inactive $h^{(i)}$ has negative value, then the solution to $\min_{\boldsymbol{\mathcal{x}}} \max_{\boldsymbol{\mathcal{\lambda}}} \max_{\boldsymbol{\mathcal{\alpha, \alpha}}\ge 0} L(\boldsymbol{\mathcal{x}}, \boldsymbol{\mathcal{\lambda}}, \boldsymbol{\mathcal{\alpha}})$ will have $\alpha_i = 0$ . We can thus observe  that at the solution, $\mathbf{\alpha} \odot \mathbf{h(x)} = \mathbf{0}$ . In other words, for all $i$ , we know that at least one of the constraints $\alpha_i \ge 0$ or $h^{(i)}(x) \le 0$ must be active at the solution. To gain some intuition for this idea, we can say that either the solution is on the boundary imposed by the inequality and we must use its KKT multiplier to influence the  solution to $\mathbf{x}$ , or the inequality has no influence on the solution and we represent this by zeroing out its KKT multiplier. Prior to this section, the authors define the aforementioned functions as follows: To define the Lagrangian, we first need to describe $\mathbb{S}$ in terms of equations and inequalities. We want a description of $\mathbb{S}$ in terms of $m$ functions $g^{(i)}$ and $n$ functions $h^{(j)}$ so that $\mathbb{S} = \{ \mathbf{x} | \forall i, g^{(i)}(\mathbf{x}) = 0 \ \text{and} \ \forall j, h^{(j)}(\mathbf{x}) \le 0 \}$ . The equations involving $g^{(i)}$ are called the equality constraints , and the inequalities involving $h^{(j)}$ are called the inequality constraints . So it seems that the $h^{(i)}$ being described here is the ""inactive"" constraint. Furthermore, when the authors say that, because an inactive $h^{(i)}$ has negative value, the solution to $\min_{\boldsymbol{\mathcal{x}}} \max_{\boldsymbol{\mathcal{\lambda}}} \max_{\boldsymbol{\mathcal{\alpha, \alpha}}\ge 0} L(\boldsymbol{\mathcal{x}}, \boldsymbol{\mathcal{\lambda}}, \boldsymbol{\mathcal{\alpha}})$ will have $\alpha_i = 0$ , this is because, in the section prior to saying this, they introduce new variables $\lambda_i$ and $\alpha_j$ for each constraint (these are called KKT multipliers), and define the generalized Lagrangian as $$L(\boldsymbol{\mathcal{x}}, \boldsymbol{\mathcal{\lambda}}, \boldsymbol{\mathcal{\alpha}}) = f(\mathbf{x}) + \sum_i \lambda_i g^{(i)} (\mathbf{x}) + \sum_{j} \alpha_j h^{(j)}(\mathbf{x})$$ So therefore, evidently, if we are calculating $\min_{\boldsymbol{\mathcal{x}}} \max_{\boldsymbol{\mathcal{\lambda}}} \max_{\boldsymbol{\mathcal{\alpha, \alpha}}\ge 0} L(\boldsymbol{\mathcal{x}}, \boldsymbol{\mathcal{\lambda}}, \boldsymbol{\mathcal{\alpha}})$ , then, in order to satisfy $\max_{\boldsymbol{\mathcal{\alpha, \alpha}}\ge 0}$ , we require $\alpha_j = 0$ , since, again, $h^{(j)} \le 0$ (that is, it is ""inactive""). But this section still leaves me unclear on a number of things: When the authors refer to ""convergence"" in this context, they're referring to convergence to the stationary point, right? Why is it the case that an ""inactive"" constraint potentially excludes other solutions? Is this just due to the fact that the values being considered during convergence are constrained to $h^{(j)}(\mathbf{x}) \le 0$ , and so we are not also considering the values $h^{(j)}(\mathbf{x}) \ge 0$ ? I'm not understanding the following explanation and the idea of how the KKT multipliers influence the solution: To gain some intuition for this idea, we can say that either the solution is on the boundary imposed by the inequality and we must use its KKT multiplier to influence the  solution to $\mathbf{x}$ , or the inequality has no influence on the solution and we represent this by zeroing out its KKT multiplier. I would greatly appreciate it if people would please take the time to clarify these points.","['karush-kuhn-tucker', 'lagrange-multiplier', 'machine-learning', 'multivariable-calculus', 'optimization']"
3508191,"In regard to $L^p$ spaces, is $L^p \subset L^q$ for $p<q$?","Attempting to understand if there are relationships between $L^p$ spaces and can't find a conclusive answer in Kreyszig's 'Introduction to Functional Analysis with Applications' nor in a solid browsing of Wikipedia. More context: Dirichlet conditions seem to indicate that if a function is absolutely integrable it can be represented as a fourier series. To me, this means that the function must lie within $L^1$ space, i.e. $\int | f(x) | \ dx < \infty$ . I began this hunt in an attempt to understand why functions in $L^2([0,1])$ space can be represented as fourier series, or more specifically, why the basis components of $L^2([0,1])$ are functions of $x$ (e.g. $e^{-2 \pi i n x}$ ). If $L^2([0,1])$ functions are contained within $L^1$ then I think I understand, though this seems like a leap. Appreciate any help.","['harmonic-analysis', 'lp-spaces', 'lebesgue-integral', 'functional-analysis']"
3508203,General solution for a second order linear ODE,"Given the equation \begin{equation}
\frac{d^2y}{dx^2}=f(x)y\,,
\end{equation} is it somehow possible to find an expression for $y$ in terms of $f(x)$ in the interval $x>0$ , assuming that we know that $\lim_{x \to \infty}f(x) \sim 1/x^2$ and $\lim_{x \to 0}f(x) \sim 1/x^2$ and that $f(x)$ and $y$ are everywhere smooth.","['ordinary-differential-equations', 'real-analysis']"
3508257,Proof for $\sum_{k=1}^n k2^k \binom{2n-k-1}{n-1} = n\binom{2n}{n}$?,"I haven't seen any similar combinatorial identities of this form in textbooks. I had the left hand side in the first place and the right hand side was given by Mathematica. I'm not sure how Mathematica was able to solve it but I do expect that this has a simple combinatorial proof. I tried to look for one but I was stuck. Any help? PS: I am a high school student self-studying combinatorics, so combinatorial proofs or generating-function-based proofs are the best for me. If things like hypergeometric functions or gamma functions cannot be avoided I am glad to accept them as well:) Just means that I need to go deeper. At least it's not some Mathematica magic:)","['combinatorics', 'combinatorial-proofs']"
3508258,Differential equations and linearly independent solutions,"If $$y_1(x)=\sin 2x$$ and $$y_2(x)=\cos 2x$$ are two solutions of $$y^{,,}+4y=0,$$ show that $y_1(x)$ and $y_2(x)$ are linearly independent solutions. I think to prove linearly independent,the equation will be $$c_1\sin 2x+c_2\cos 2x=0$$ ,where $c_1$ and $c_2$ have to be equal to zero,but how to prove that??,please help me..",['ordinary-differential-equations']
3508263,Searching for rock-hard integers.,"For integer $k\geq 0$ written $d_jd_{j-1}\dots d_1$ , where $d_i$ are single-digit integers, define $$R(k) := 
\begin{cases} d_j^{d_1}d_{j-1}^{d_2} \dots d_{j/2}^{{d_{j/2 + 1}}}, & j \text{ is even.}\\
d_j^{d_1}d_{j-1}^{d_2} \dots d_{(j+1)/2 -1}^{d_{(j+1)/2+1}} d_{(j+1)/2}, & j \text{ is odd.}
\end{cases}
$$ Which is a bit of an eyesore, though some examples may help: $R(1392) = 1^2 * 3^9$ , $R(793) = 7^3*9$ , $R(12345678) = 1^8 * 2^7*3^6 * 4^5$ . Now, say an integer $n \geq 0$ is rock-hard if $R(n) =n$ . $0, 1, 2, 3 \dots 9$ are rock-hard. There are no rock-hard two-digit integers. This can be verified in about five seconds with the question: is there a power of $n$ between $10n$ and $10(n+1)$ for $0 < n < 10$ ? I do not believe there are any three-digit rock hard integers. My question: is there a more efficient way to search for rock-hard numbers than just checking all numbers? Certainly, we can omit $111$ , $121 \dots 191$ , $11111,$ $11211,$ etc., but can we further narrow down what numbers we check for rock-hardness? Motivation: John Conway did something similar to this, though I can't remember what he did nor find a reference. I believe N.J.A. Sloane chipped in as well. Various comments: $256$ is almost rock-hard but not quite! $256 = 2^6*4$ .","['searching', 'number-theory', 'decimal-expansion', 'algorithms']"
3508268,"Show that $\left\{ f_{n}\right\} _{n=1}^{\infty}$ converges to $f$ in $L^{p}$, i.e., $\left|\left|f_{n}-f\right|\right|_{p}\to0$.","Let $1\leq p\leq\infty$ , $\alpha\in\mathbb{R}$ and $I=(0,1]$ with $2\alpha p<1$ . For $n\in\mathbb{N}$ , Define $f_{n}:I\to\mathbb{R}$ by $f_{n}\left(x\right)=\frac{n}{nx^{\alpha}+1}$ and that $f\left(x\right)=\lim_{n\to\infty}f_{n}\left(x\right)$ , $x\in I$ . Show that $\left\{ f_{n}\right\} _{n=1}^{\infty}$ converges to $f$ in $L^{p}$ , i.e., $\left|\left|f_{n}-f\right|\right|_{p}\to0$ . What I tried: Proof. WTS that $\lim_{n\to\infty}\int_{0}^{1}\left|f_{n}-f\right|^{p}=0$ . $f\left(x\right)=\lim_{n\to\infty}f_{n}\left(x\right)=\lim_{n\to\infty}\frac{n}{nx^{\alpha}+1}=\frac{1}{x^{\alpha}}.$ Now, $$\left|f_{n}-f\right|^{p}	=\left|\frac{n}{nx^{\alpha}+1}-\frac{1}{x^{\alpha}}\right|^{p}
	=\left|\frac{-1}{nx^{2\alpha}+x^{\alpha}}\right|^{p}
	\leq\frac{1}{nx^{2\alpha p}+x^{\alpha p}}
	\leq\frac{1}{nx^{2\alpha p}}$$ and this is where i don't know how to proceed. From what I have read, $\frac{1}{nx^2}$ is unbounded. So I'm guessing that $\frac{1}{nx^{2\alpha p}}$ is unbounded. But it says that $2\alpha p<1$ , this must have something to do with $\frac{1}{nx^{2\alpha p}}$ . Do you have any ideas guys? I will really appreciate it.","['measure-theory', 'lp-spaces', 'convergence-divergence', 'real-analysis']"
3508276,Understand notation of motion equation in the context of differential equations.,"I'm working on practice problems to get familiar with differential equations, but can't quite understand the notation of the following problem: The motion equation $\dot v = g - \alpha v |v|$ is given for velocity v with acceleration $g > 0$ and quadratic friction of a sinking body with $\alpha > 0$ and initial value $(0) = v_0 \in \mathbb{R}$ . Now it is asked to find out for which initial value $v$ will be constant. Since I have zero experience in physics and have just gotten started with differential equations, I'm struggling with the notation. I understand that $\dot v$ probably denotes $y^{\prime}$ and therefore $v$ will be $y$ , but what is $g$ ? I would be very thankful for any clarifications.","['initial-value-problems', 'physics', 'mathematical-physics', 'ordinary-differential-equations']"
3508284,Equivalent definition of a tangent space?,"I am trying to work through some basic degree theory on manifolds and I found this nice pdf ( http://www.math.uchicago.edu/~may/VIGRE/VIGRE2011/REUPapers/Bosshardt.pdf ) which gets me exactly where I want to go. However, it seems that the tangent space of a manifold $M \subseteq \mathbb{R}^{n}$ has a definition which I have never seen before. Summarizing the text: Let $x \in \mathbb{R}^{n}$ . Then a local parametrization near $x$ is a map $$ \phi: U \rightarrow V $$ such that $U \subseteq \mathbb{R}^{n}$ is open about $0$ , $V \subseteq \mathbb{R}^{n}$ is open, and $\phi(0)=x$ . Let $M \subseteq \mathbb{R}^{n}$ be a manifold. Then the tangent space at $x \in M$ is defined in the following way: For some parametrization $ \phi: U \rightarrow V $ near $x$ , let $\phi_{0}$ be the Jacobian matrix of $\phi$ evaluated at $0$ , which can be seen as a linear transformation from $\mathbb{R}^{n}$ to $\mathbb{R}^{n}$ .Then the tangent space of $M$ at $x$ is defined as $$T_{x}(M)= \phi_{0}(\mathbb{R}^{n})$$ I have always worked with the tangent space as being the set of derivations at a point. I know that there is also an equivalent definition using equivalence classes of curves. However, this seems to simply be a collection of vectors in $\mathbb{R}^{n}$ . Again, the tangent space of an $n-$ manifold is isomoprhic to $\mathbb{R}^{n}$ , but I cannot seem to find any formal equivalence/isomorphism dealing with the formulation given above. Can anybody point me in the right direction? Thanks!","['manifolds', 'differential-topology', 'differential-geometry', 'real-analysis']"
3508300,How to understand conditional expectation?,"The definition of conditional expectation is: Given probability space $(\Omega, \mathcal{F}, P)$ , let $\mathcal{D}$ be a sub-sigma field of $\mathcal{F}$ (i.e., $\mathcal{D}\subset \mathcal{F}$ and $\mathcal{D}$ is a $\sigma$ -algebra), and let $X$ be a integrable random variable. Then there is a unique (up to $P$ -null set) random variable $E(X|\mathcal{D})$ such that: $E(X|\mathcal{D})$ is $\mathcal{D}$ measurable. $\int_D E(X|\mathcal{D})dP=\int_DXdP$ , for all $D\in\mathcal{D}$ . We call $E(X|\mathcal{D})$ as the conditional expectation of $X$ given $\mathcal{D}$ . I cannot understand the purpose of conditional expectation. Why do we need to define such a concept? According to Durrett, the interpretation of it is ""the best guess of the value of $X$ given the information $\mathcal{D}$ we have"". Then, my questions are: What are we trying to learn about $X$ ? Are we tring to learn its distribution, or $\int_DXdP$ for all $D\in\mathcal{D}$ , or $X(\omega)$ for all $\omega\in\Omega$ , or $P(X\in D)$ for $D\in\mathcal{D}$ ? What do we know about $X$ ? Do we assume that we cannot observe the realization of $X$ (Otherwise, why do we need to ""guess""?)? Do we know the integral of $X$ over $\mathcal{D}$ -measurable sets (Otherwise, how can we construct a conditional expecation since we cannot verify the second requirement in the definition?)? What is the meaning of a realization (i.e., $E(X|\mathcal{D})(\omega)$ for some $\omega\in\Omega$ ) of $E(X|\mathcal{D})$ ? I am totally confused about this concept. I do not know what is the intention here. Any explanation will be appreciated.","['conditional-expectation', 'probability-theory']"
3508312,"If the probability of getting $12$ is $0.4$ and the remaining numbers have uniform distribution of $0.6,$ which number would you pick?","The following is an interview question. Given $2$ dice. You and your opponent pick a number represents the sum of $2$ dice. If the probability of getting $12$ is $0.4$ and the remaining numbers have uniform distribution of $0.6,$ which number would you pick? I think we should pick a number with the highest probability of occurrence. Without the unfairness assumption (that is, dice are fair), we should pick $7$ as it has the highest probability $\frac{6}{36} = \frac{1}{6}.$ But with the additional assumption, I do not know which number has the highest occurrence probability.","['discrete-mathematics', 'combinatorics', 'probability']"
3508363,"Solve the initial value problem $xy'=y(xy-1), y(e^{-1})=e$","Solve the initial value problem $xy'=y(xy-1), y(e^{-1})=e$ I was given the hint to use the substitution, but don't know how to find the proper substitution. What's the general rule of thumb in finding a substitution?","['initial-value-problems', 'ordinary-differential-equations']"
3508370,"If $U\sim\chi_{m}^2$ independently of $V\sim\chi_n^2$ then prove that $\frac{V}{U+V}\sim\beta\left(\frac n2,\frac m2\right)$","If $U\sim\chi_{m}^2,V\sim\chi_n^2$ and $U,V$ are independent then prove that $\frac{V}{U+V}\sim\beta\left(\frac n2,\frac m2\right)$ The joint pdf of $U$ and $V$ is, \begin{align}
f_{UV}(u,v)&=\frac{1}{2^{\frac m2}\Gamma\left(\frac m2\right)}u^{\frac m2-1}e^{-\frac u2}\frac{1}{2^{\frac n2}\Gamma\left(\frac n2\right)}v^{\frac n2-1}e^{-\frac u2}\\
&=\frac{1}{2^{\frac{m+n}{2}}\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}u^{\frac m2-1}v^{\frac n2-1}e^{-\frac12(u+v)}
\end{align} Now let $Y=\frac{V}{U+V}$ then CDF of $Y$ is, \begin{align}
F_Y(y)&=\mathbb P(Y\le y)\\
&=\mathbb P\left(\frac{V}{U+V}\le y\right)\\
&=\mathbb P\left(\frac VU\le \left(\frac{y}{1-y}\right)\right)\\
&=\mathbb P\left(V\le \left(\frac{y}{1-y}\right)U\right)\\
&=\int_{u=0}^{\infty}\int_{v=0}^{\left(\frac{y}{1-y}\right)u}\frac{1}{2^{\frac{m+n}{2}}\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}u^{\frac m2-1}v^{\frac n2-1}e^{-\frac12(u+v)}\:dv\:du
\end{align} Now we can get $f(y)$ using Leibniz integral rule, \begin{align}
f_y(y)&=\frac{1}{2^{\frac{m+n}{2}}\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}\underbrace{\int_{u=0}^{\infty}\frac{u}{(1-y)^2}u^{\frac m2-1}{\left(\frac{yu}{1-y}\right)}^{\frac n2-1}e^{-\frac12\left(u+\frac{yu}{1-y}\right)}\:du}_{I}
\end{align} But it seems I am far away to $\beta\left(\frac n2,\frac m2\right)$ . Is there other way to proof it $?$ Any hint or solution will be appreciated. Update : [For seek of completeness]Using @NCh answer, Replace $t=u\left(\frac{1}{2(1-y)}\right)$ , $u=2(1-y)t$ , $du=2(1-y)\,dt$ : $$
I=\frac{y^{\frac{n}2-1}}{(1-y)^{\frac{n}{2}+1}}\cdot 2^{\frac{n+m}{2}}(1-y)^\frac{n+m}{2}\underbrace{\int_{t=0}^{\infty}t^{\frac{n+m}{2}-1}e^{-t}\:dt}_{\Gamma\left(\frac{n+m}{2}\right)}$$ $$f_Y(y)=\frac{\Gamma\left(\frac{n+m}{2}\right)}{\Gamma\left(\frac m2\right)\Gamma\left(\frac n2\right)}y^{\frac{n}2-1}(1-y)^{\frac m2-1}$$ Hence $f_Y(y)\sim \beta\left(\frac n2,\frac m2\right)$","['integration', 'probability-distributions', 'probability', 'density-function']"
3508373,Taking the gradient of $f(\mathbf{x}) = \frac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2$,"Section 4.5 of the textbook Deep Learning by Goodfellow, Bengio, and Courville, says that the gradient of $$f(\mathbf{x}) = \dfrac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2$$ is $$\nabla_{\mathbf{x}} f(\mathbf{x}) = \mathbf{A}^T (\mathbf{A}\mathbf{x} - \mathbf{b}) = \mathbf{A}^T \mathbf{A} \mathbf{x} - \mathbf{A}^T \mathbf{b}$$ My understanding is that $f(\mathbf{x}) = \dfrac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2$ is the square of the Euclidean norm . So we have that $$\begin{align} f(\mathbf{x}) = \dfrac{1}{2}\|\mathbf{A} \mathbf{x} - \mathbf{b}\|_2^2 &= \dfrac{1}{2} \left( \sqrt{(\mathbf{A} \mathbf{x} - \mathbf{b})^2} \right)^2 \\ &= \dfrac{1}{2} (\mathbf{A} \mathbf{x} - \mathbf{b})^2 \\ &= \dfrac{1}{2} (\mathbf{A} \mathbf{x} - \mathbf{b})(\mathbf{A} \mathbf{x} - \mathbf{b}) \\ &= \dfrac{1}{2} [ (\mathbf{A}\mathbf{x})(\mathbf{A} \mathbf{x}) - (\mathbf{A} \mathbf{x})\mathbf{b} - (\mathbf{A} \mathbf{x})\mathbf{b} + \mathbf{b}^2 ] \ \ \text{(Since matrix multiplication is distributive.)} \\ &= \dfrac{1}{2} [(\mathbf{A} \mathbf{x})^2 - 2(\mathbf{A} \mathbf{x})\mathbf{b} + \mathbf{b}^2] \ \  \text{(Note: Matrix multiplication is not commutative.)} \end{align}$$ It's at this point that I realised that, since we're working with matrices, I'm not really sure how to take the gradient of this. Taking the gradient of $f(\mathbf{x})$ with respect to $\mathbf{x}$ , we get something like $$\nabla_{\mathbf{x}} f(\mathbf{x}) = \dfrac{1}{2} [2 (\mathbf{A} \mathbf{x}) \mathbf{A}] - \dfrac{1}{2}[2(\mathbf{A} \mathbf{A} \mathbf{x})\mathbf{b}]$$ So what is the reasoning that leads us to get $\nabla_{\mathbf{x}} f(\mathbf{x}) = \mathbf{A}^T (\mathbf{A}\mathbf{x} - \mathbf{b}) = \mathbf{A}^T \mathbf{A} \mathbf{x} - \mathbf{A}^T \mathbf{b}$ ? Where did the transposed matrices come from? I would greatly appreciate it if people would please take the time to clarify this.","['matrices', 'multivariable-calculus', 'matrix-calculus', 'linear-algebra', 'derivatives']"
3508407,Waiting time for a pattern in coin tossing,"Let $\{X_t\}$ be an iid sequence of fair coin tosses and $\tau_{HTH} = \inf\{t\geq 3: X_{t-2}X_{t-1}X_t=HTH\}$ . I want to determine $E\tau_{HTH}$ . I don't understand a part of the explanation which is: Gamblers place bets on each individual toss. On each bet, gambler pays an entrance fee of $k$ and is paid $2k$ in return if the outcome is $H$ or $0$ if the outcome is $T$ . $k$ can be negative which corresponds to a bet on $T$ . Suppose that at each unit of time until $HTH$ first appears, a new gambler enters and employs the following strategy: on his first bet, he wagers $1$ on $H$ . If he loses, he stops. If he wins and $HTH$ has not yet appeared, he wagers his payoff of $2$ on $T$ . If he loses, he stops. If he wins and $HTH$ has not yet appeared, he wagers his payoff of $4$ on $H$ . This is the last bet placed bu this particular gambler. The gambler who started at $\tau_{HTH}$ is paid $2$ and the gambler who started at $\tau_{HTH}-2$ is paid $8$ and every gambler has paid an initial entry fee of $1$ . At time $\tau_{HTH}$ , the net profit to all gamblers $=10-\tau_{HTH}$ and since the game is fair $E\tau_{HTH}=10$ Q1) I can see that the net profit for all gamblers is $10-\tau_{HTH}$ if the outcome string is say $TTTHTH$ and this wouldn't be the net profit if atleast one $H$ appears before $HTH$ , am I correct? (say the outcome string is $HHHHTH$ ). Q2) The explanation also says $\tau_{HTH}/3$ is bounded by a Geometric(1/8) random variable. How can I see that?","['stochastic-processes', 'probability-theory', 'martingales']"
3508411,Basics: Defining sets,"I think it is a really basic question, but I've been struggling to get it right. a) A set of pairs: for every $x$ there is exactly one partner and it can't be partner with itself. My solution: $E_1=\{(x,y)|\forall x \in V_1 \exists!y \in V_1 \land x\neq y\}$ b) For every $y$ there are exactly $y$ number of pairs and $x$ can't be partner with itself: $E_2= \{(x,y)|\forall y \in V_2: $ # $E_2 = y \land x\neq y\}$ c) number of pairs where $x$ is the first component can't be equal to the number of pairs where $x$ is the second component, $x$ can't be partner with itself and there has to exist at least one pair. $E_3= \{(x,y)|....\}$ I'm not allowed to use the dot notation (e.g.: $M=\{1,2,3,...\}$ ) I'm not sure about $E_2$ . For $E_3$ I don't know how to define that.",['elementary-set-theory']
3508416,"If $f(z)$ is analytic, and $\overline{f(z)}$ is analytic, then is $f$ necessarily a constant function?","I need help with verifying my following proof. It feels a little fishy to me. If $f(z)$ is analytic, and $\overline{f(z)}$ is analytic, then is $f$ necessarily a constant function? We know $f(z)=u(x,y)+iv(x,y)$ and $\overline{f(z)}=u(x,y)+iv'(x,y)$ , where $v'=-v$ . $f$ satisifies the Cauchy Riemann equations, thus, For $f$ , one has that: $u_x=v_y, v_x=-u_y$ . For $\overline{f}$ , one has that: $u_x=v'_y=-v_y$ $v'_x=-v_x=--u_y$ . One has $u_x=-v_y=v_y$ , which forcefully makes $v_y=0$ . Also, $u_y=v_x=-v_x$ , so $v_x=0$ . So for all $z$ , $f'(z)=0$ and this shows that $f$ is a constant function. Does this proof work?","['complex-analysis', 'solution-verification', 'analytic-functions']"
3508417,"Is a set ""trapped"" between two convex sets, convex?","Let $A, C$ be two strictly-convex sets in $\mathbb{R}^n$ . Let $B$ be a set such that: $$A\subseteq B\subseteq C.$$ Is $B$ strictly-convex too? Not necessarily. For example, $A$ and $C$ can be to concentric discs, and $B$ can be a crazy amoeba-shaped figure trapped between them: But what if $A$ and $C$ only differ in their boundary? Consider two cases: $A$ is the interior of some set and $C$ is the closure of the same set. Then, as shown by Anonymous below, $B$ might be arbirary $A$ is the interior of $C$ and $C$ is the closure of $A$ , so that $B$ contains $A$ plus some part of the boundary. In this case, is $B$ always strictly-convex?","['convex-geometry', 'general-topology', 'convex-analysis']"
3508531,Why are we allowed to divide by $x$ in the following differential equation?,"Suppose we have a second order linear differential equation $x\frac{d^2y}{dx^2}+\frac{dy}{dx}-xy=xe^x$ .Often we divide both sides by $x$ to get $\frac{d^2y}{dx^2}+\frac{1}{x}\frac{dy}{dx}-y=e^x$ ,and then proceed towards solution.Why does this make no harm as far as the solution of the differential equation is considered.We were trying to find $y=\phi(x)$ whose domain is $\mathbb R$ .But,now we are finding a solution $y=\psi(x)$ whose domain is $\mathbb R-\{0\}$ .Of course it is clear that the family of solutions $\phi(x)$ and $\psi(x)$ are equal except one has $0$ in domain and other does not ,I mean we can extend the other one to $\mathbb R$ by assigning some value at $0$ so that the extension is continuous.Does this make sense?Is this why we can divide by $0$ without worrying about losing anything?","['ordinary-differential-equations', 'singularity', 'real-analysis', 'multivariable-calculus', 'partial-differential-equations']"
3508532,Separation axioms on a finite space imply discrete topology?,"I saw somewhere on this site, a claim that the only topology on a mertizable finite space is the discrete one. I think this stems even more generally from such a space being Hausdorff. The strongest version of which I am sure is that when the space is finite and $T_1$ then the topology has to be discrete, but I was struggling with deciding whether this is also true when the space is finite and $T_0$ . I have the following counter-example: Is the example $X=\{1,2 \}$ and $T=\Big\{ \emptyset,\{1\}, X  \Big\}$ valid or am I missing something? Can this 'proposition' still be generalized using some seperation axiom?","['general-topology', 'solution-verification', 'separation-axioms']"
3508547,Showing $\Bbb Z_4 $ and $\Bbb Z_2 \times\Bbb Z_2$ are the only abelian group with $4$ elements.,"We consider the two abelian groups $\Bbb Z_4$ with addition modulo $4$ . $\Bbb Z_2 \times\Bbb Z_2$ with component-by-component addition modulo $2$ . a) Show there is no isomorphism  between these groups. b) Show that this groups are the only abelian groups( except isomorphism) with $4$ elements. So I was able to solve a): all elements in $\Bbb Z_2 \times\Bbb Z_2$ have order $ \leq 2 $ , but $1$ in $\Bbb Z_4$ have order $4$ . But how can I solve b) ? Do you have a hint? Maybe I can use a)?","['finite-groups', 'group-theory', 'abstract-algebra', 'discrete-mathematics', 'abelian-groups']"
3508582,Recommendations for differential geometry textbooks that develop geometric intuition.,"I'm currently self-studying complex analysis (CA), and reading ""Visual Complex Analysis"" by Tristan Needham. I'm absolutely fascinated by how much geometric intuition he provides for the key findings in CA. It has been a very enticing read so far. I have a mechanical engineering background, I've previously self-studied general/algebraic topology, and I'm interested in self-studying differential geometry (DG) after finishing Needham's book. I know that Needham is in the process of releasing his next book, ""Visual Differential Geometry"". But the exact date of release is hard to find. Can anyone recommend a few good DG textbooks that (a) pay special attention to developing the geometric intuition of the reader (and perhaps less attention to rigorous mathematical proofs), and (b) would be appropriate for a reader with my aforementioned background?","['riemannian-geometry', 'big-list', 'book-recommendation', 'reference-request', 'differential-geometry']"
3508608,A Particular Two-Variable System in a Group,"Suppose $a$ and $b$ are elements of a group $G$.
If $a^{-1}b^{2}a=b^{3}$ and $b^{-1}a^{2}b=a^{3}$, prove $a=e=b$. I've been trying to prove but still inconclusive. Please prove to me. 
Thanks very much for proof.","['group-theory', 'abstract-algebra']"
3508647,Probabilistic inequality for an antisymmetric function,"Let $X$ be a random variable with $\mathbb P(-1 \leq X \leq 1) = 1$ . Does $\mathbb E(X) \geq 0$ imply $$\mathbb E[X(1-|X|)] \geq 0?$$ The function is antysymmetric around $0$ and has more probability mass on the positive side. Intuitively, it should be correct. If not, is it true if I assume $\mathbb E(X) > 0$ ?","['integration', 'probability-theory']"
3508676,Does $\alpha=\beta f \Rightarrow f$ isomorphism?,"For a positive integer $n$ , let be: $K$ and $H$ finite groups of order $n$ ; $S_n$ the symmetric group of degree $n$ ; $\alpha\colon K \hookrightarrow S_n$ and $\beta\colon H \hookrightarrow S_n$ embeddings; $f\colon K \rightarrow H$ bijection. Does $\alpha=\beta f \Rightarrow f$ isomorphism? If not in general, is that true for some conditions on $\alpha$ and $\beta$ ? Edit based on @Matthias Klupsch's hint: $(\beta f)(xy)=\beta(f(xy))$ ; but $\beta f$ and $\beta$ are, in particular, homomorphisms, so: $(\beta f)(xy)=((\beta f)(x))((\beta f)(y))=(\beta(f(x))(\beta(f(y))=\beta(f(x)f(y))$ ; therefore, $\beta(f(xy))=\beta(f(x)f(y))$ ; but $\beta$ is injective, so $f(xy)=f(x)f(y)$ , and $f$ is homomorphism and hence isomorphism.","['group-theory', 'abstract-algebra', 'finite-groups', 'group-isomorphism']"
3508702,Question about the sequence of moments of a continuous function,"Let $f:[0,1]\to\mathbb{R}$ be continuous. Consider for every $n\in\mathbb{N}$ : $$M_n(f)=\int_0^1t^n\,f(t)\,dt$$ It is easy to see that the sequence $\left(M_n(f)\right)_{n\in\mathbb{N}}$ converges to $0$ . But is it possible to choose $f$ in such a way that : $$\forall n\in\mathbb{N},\,M_n(f)=e^{-\lambda n^2}$$ where $\lambda$ is some positive constant ? It can be seen that such a function could not be positive and would necessarily verify $f(1)=0$ , but I wasn't able to get much more than that ... Any hint would be appreciated :)","['integration', 'laplace-transform', 'asymptotics']"
3508739,"Coloring the integers with red, blue, and green","Can all integers can be painted either red, blue, or green such that there exists at least $1$ integer of each color, and if any $3$ integers $a$, $b$, and $c$ have the same color then $a+b$, $a+c$, $b+c$, and $a+b+c$ also have the same colour?",['combinatorics']
3508785,Generated $\sigma$-algebras with cylinder set doesn't contain the space of continuous functions,"Consider $\mathbb R^{[0,1]}$ the space of all functions from $[0,1]$  to $\mathbb R$  and the cylindrical sigma algebra $\mathcal B$  on it. The question is: how to prove that $C[0,1]\notin \mathcal B$.","['elementary-set-theory', 'stochastic-processes', 'measure-theory']"
3508793,"Is there any way to visualize the inner product of two continuous functions on $[a,b]$.","Suppose we consider $C([a,b])$ and define an inner product on the vector space by $$\left<f,g\right>=\int_a^b f(t)g(t)dt$$ where $f,g$ are real valued functions on $[a,b]$ . My question is, can we visualize this inner product, i.e. how it graphically looks for two given continuous functions? I tried to visualize it as area under $f(t)g(t)$ curve between the two bounds but that does not help me much.I want a more clear visualization. A diagram would also help.","['inner-products', 'normed-spaces', 'continuity', 'linear-algebra', 'riemann-integration']"
3508832,homology with integral coefficients vs homology with field coefficients,"Suppose that $f:X\rightarrow Y$ is a continuous mapping between connected topological spaces such that for any field $k$ , $H_{\ast}(X,k)\rightarrow H_{\ast}(Y,k)$ is an isomorphism. Does it follow that $H_{\ast}(X,\mathbf{Z})\rightarrow H_{\ast}(Y,\mathbf{Z})$ is an isomorphism in integral homolgy ? Do we need to assume that $X$ and $Y$ are simply connected ?","['general-topology', 'homology-cohomology', 'algebraic-topology']"
3508845,What is the cardinality of the set of all subgroups of $S_\alpha$ for infinite $\alpha$?,"Letâs define $S_\alpha$ as a group of all bijections on a set with cardinality $\alpha$ under composition. Letâs denote the set of all subgroups of a group $G$ as $Sub(G)$ . What is the cardinality of $Sub(S_\alpha)$ for $\alpha \geq \aleph_0$ ? Currently I only know the following bounds: $$2^\alpha \leq |Sub(S_\alpha)| \leq 2^{2^\alpha}$$ Proof of the lower bound: There are known to be $2^\alpha$ non isomorphic groups of order $\alpha$ , and each of them is isomorphic to some subgroup of $S_\alpha$ by Cayley theorem. Proof of the upper bound: $$Sub(S_\alpha) \subset P(S_\alpha)$$ Thus $$|Sub(S_\alpha)| \leq |P(S_\alpha)| = 2^{|S_\alpha|} = 2^{2^\alpha}$$","['symmetric-groups', 'group-theory', 'infinite-groups', 'set-theory']"
3508847,"Domino tilings in a specific figure, math olympiad problem","There was a reddit post a month ago in learnmath about this question: ""Prove that the number of possible domino tilings in this figure is a square number"" The last paragraph was the wrong try by this person to solve the problem (wrong because they didn't prove that you can always separate the shape in two symmetric shapes without cutting any domino piece in all the possibilities). I'm just curious about the answer, I have thought about it from time to time and I have searched properties about domino tilings but didn't find anything that would help here. I think the question is from a previous math olympiad test, I don't know what year or even what country.","['contest-math', 'combinatorics', 'tiling']"
3508849,"Finding $a_n$ when $a_1=5$, $a_n a_{n+1}=2a_n-1$ $(n \in \mathbb{N})$ (Question Edited)","When $a_1=5$ , $a_n a_{n+1}=2a_n-1$ $(n \in \mathbb N)$ , I inducted that $a_n=\frac{4n+1}{4n-3}$ it by manually calculating $a_1, a_2, a_3...$ and finding out the pattern of the numerator and denominator. But it doesn't seem that accurate- how can I deduct $a_n$ from the given conditions? Edit: I'm really sorry- I didn't write my question clearly. $a_n=\frac{4n+1}{4n-3}$ isn't one of the 'given conditions'. The question is deducting ' $a_n=\frac{4n+1}{4n-3}$ ' itself from $a_1=5$ , $a_n a_{n+1}=2a_nâ1$","['induction', 'recurrence-relations', 'sequences-and-series']"
3508856,Number of sequences that contain a certain subsequence,"Consider a given sequence of length $k$ . I want to calculate the number of sequences of length $n$ that contains the given sequence as a subsequence. The alphabet used to generate the string consists of $|A|$ values. For example, a sequence ""120"" is given. in this case, k=3. Consider the alphabet to be $A=\{0,1,2,3\}$ and $n=5$ . In this case, two of the possible sequences are: 10230
12320 And the question becomes the total number of sequences of length 5 which contain ""120"" as a subsequence. The important part here is that the given subsequence is not necessarily contained in the sequence, which is clear by the provided examples. I know this problem can be solved by using the principle of inclusion-exclusion. However, I was looking for a more straightforward and probably a closed-form equation for this problem. Thank you in advance for your help.",['combinatorics']
3508864,Help understanding a statement from a proof that is supposedly 'easy to check'.,"I am reading a book on the Banach Tarski Paradox by Stan Wagon, and I came across a part of the proof of the Banach Schroder Bernstein Theorem that I can't understand. Let $f:A \to B_{1}$ and $g:A_{1} \to B$ where $A_{1} \subseteq A$ and $B_{1} \subseteq B$ . (We know these bijections exist by an earlier part of the proof.)
  Let $C_{0}=A$ \ $A_{1}$ and for $n \geq 1$ , $C_{n+1}=g^{-1}f(C_{n})$ .
  Let $C=\bigcup_{n=0}^{\infty}C_{n}$ . Then it is easy to check that $g(A$ \ $C)$ = $B$ \ $f(C)$ It says it is easy to check, but I can't see why it is true! I guess it's something simple that I am missing, and would appreciate it if someone could explain the statement. Thanks for your help!","['functions', 'set-theory']"
3508881,Choosing infinitely many subrectangles,"Let $R_1,R_2,R_3,...$ be any infinite sequence of pairwise disjoint closed rectangles in the unit square $[0,1]^2$ . Is it possible to pick a sequence of subrectangles $S_n=[a_n,b_n]\times [c_n,d_n]\subseteq R_n$ such that the the open intervals $(a_n,b_n)$ , $n\in\mathbb{N}$ are pairwise disjoint, i.e. so the projections of $int(S_n)$ onto the x-axis don't intersect? Note: As pointed out in the comments, this equivalent to the question: given any sequence $[x_n,y_n]$ of subintervals of $[0,1]$ , is it always possible to find subintervals $(a_n,b_n)\subseteq [x_n,y_n]$ such that the collection $\{(a_n,b_n)\mid n\in\mathbb{N}\}$ is pairwise disjoint.","['elementary-set-theory', 'general-topology', 'geometry', 'real-analysis']"
3508910,Dissecting a hypercube into specific hypercubes,"The problem We define "" $m$ -cube"" as an $m$ -dimensional hypercube. ( $m=2$ is square, $m=3$ is cube, ...) $(\mathbf Q):$ Given a container $m$ -cube of integer side length $a$ , and $n$ many $m$ -cubes of integer side lengths $1\lt a_1,a_2,\dots,a_n\lt a$ , determine if we can fit all of them into the container such that the remaining empty space is fully filled with unit $m$ -cubes. You can assume that the given lengths of sides are sorted: $a_1\ge a_2 \ge  \dots a_n \ge 2$ . I'm looking for references to known algorithms that can be applied here. Motivation: the problem "" For what natural n does there exist a cube composed of n cubes and more "" can be reduced to sets of $(\mathbf Q)$ problems. So far I know that if $m=1$ or $n\le 2^m$ , we can answer $(\mathbf Q)$ in $O(n)$ time. (See below.) Case $(n\in\mathbb N, m=1)$ and case $(n\le2^m,m\ge 2)$ Since $a$ -side $m$ -cube has at most $a^m$ unit cubes, we have the least necessary condition: $$\sum_{i=1}^{n}a_i^m\le a^m \tag{$\mathbf n_1$}$$ If $m=1$ , we have a $1$ -cube which is a segment. In this case, $(\mathbf n_1)$ is a sufficient condition. That is, if $m=1$ then $(\mathbf Q)$ can be answered true if and only if $(\mathbf n_1)$ holds. From now on, assume $m\ge 2$ . WLOG assume $a_{\text{max}}=a_1\ge a_2 \ge \dots \ge a_n\ge 2$ . It is not hard to consider placing $m$ -cubes in corners of the container, to conclude: $$ (\forall i\in\{2,3,\dots,n\})(a_i\le a-a_1) \tag{$\mathbf n_2$}$$ After placing the largest (side length $a_1$ ) $m$ -cube in the (side length $a$ ) container $m$ -cube, any next $m$ -cube can't be larger than $a-a_1-j$ where $j\ge 0$ is the minimal distance from the placed $m$ -cube and any of the sides of the container. To minimize $j$ (and maximize allowed space for next $m$ -cube), we place the $a_1$ into any of the $2^m$ corners of the container $m$ -cube. If $n\le 2^m$ , then $(\mathbf n_2)$ is a sufficient condition. (Place the $m$ -cubes in the corners). That is, If $n\le 2^m$ then $(\mathbf Q)\iff(\mathbf n_2)$ . From now on, we can assume $n\ge 2^m+1,m\ge 2$ . The question now is, how to continue with this analysis further? Can we find necessary and sufficient conditions for more general cases?","['reference-request', 'combinatorial-geometry', 'combinatorics', 'discrete-mathematics', 'algorithms']"
3508936,Cohomology computation,"Let $\Sigma_2$ be the second Hirzebruch surface and $f : \Sigma_2 \to X$ that contracts the exceptional section $E$ . Since $E^2=-2$ , $X$ has a quadratic singularity at $p := f(E)$ . If $U'$ is a small ball around $p$ , $U = U' \cap X$ and $V = X \backslash \{p\}$ , then $X = U \cup V$ , $V$ is a line bundle over $\Bbb P^1$ and $U \cap V$ is a $\Bbb C^*$ -bundle over $\mathbb P^1$ . They are both classified by a number $a \in \mathbb Z$ . Question 1 :  What is $a$ ? (My guess is $a = \pm 2$ ). Since $U$ is contractible we get a long exact sequence $$ 0 \to H^1(X) \to H^1(V) = 0 \to H^1(U \cap V) \to  H^2(X) \to H^2(V) \to H^2(U \cap V) \to H^3(X) \to H^3(V) = 0 \to H^3(U \cap V) \to H^4(X) \to 0$$ Hence we get $H^4(X) = \Bbb C, H^1(X) = 0$ , and an exact sequence $$ 0 \to H^2(X) \to \Bbb C \to H^2(U \cap V) \to H^3(X) \to 0 $$ Question 2 : How to compute $H^2(X)$ and $H^3(X)$ ? I suspect $H^2(X) = \Bbb C$ and $H^3(X) = 0$ . Moreover I would like to understand : Question 3 : How to compute the mixed Hodge structure on $X$ associated to the resolution ? It's clear for $H^0$ and $H^4$ . I don't really know what happens for $H^2(X)$ . In fact, I read that in such setting, the Hodge structure is never pure, which is really what motivated my question.","['hodge-theory', 'algebraic-geometry', 'homology-cohomology']"
3508940,Function being differentiable vs. derivative expression being undefined,"Here's my confusion: My teacher, as well as some online sources such as Khan Academy, seem to assume that the derivative expression being undefined at a point implies that the function being differentiated is not differentiable at that point. Consider, for example, $g(x)=x^{1/3}$ . The second derivative is $g''(x)=-2/9*x^{-5/3}$ . In this Khan Academy video , the speaker concludes that the second derivative doesn't exist at $x=0$ because if you plug zero into the $g''$ expression you end up dividing by zero. But why does that conclusion follow? How can we be sure the expression is defined wherever the function is twice differentiable? Along similar lines, when doing implicit differentiation in class, we were taught that a function $y$ fails to be differentiable when the expression we get for $dy/dx$ is undefined. For example, if $x^2+2xy+2y^2=1$ , then we found $\frac{dy}{dx}=\frac{(-2x-2y)}{(2x+4y)}$ . We were told that to find where $y$ fails to be differentiable, we should set $2x+4y = 0$ , because that is the denominator of our derivative expression and we can't divide by zero. But again, as asked above, why can we be certain that having the derivative expression undefined implies that $y$ isn't differentiable? Finally, I'll note that there's at least one example where I've noticed disconnect between where the expression is defined and where the derivative exists . That example is $f(x)=\ln(x)$ . That is obviously not differentiable for $x<0$ , yet the derivative expression, $f'(x)=1/x$ , is defined for $x<0$ (it's only undefined if $x=0$ ). How do we reconcile that with what I've written in the preceding paragraphs? I hope my question is clear. I can clarify if necessary - I realize it's a bit complicated and lengthy.","['calculus', 'derivatives']"
3509005,Let $f$ be an entire function,"I'm working on this problem: Let $f$ be an entire function. Suppose $|f(z)|=1$ if $|z|=1$ and $f$ has only one zero in the unit disk $D_1(0)$ . Prove that $f(z)=cz$ for some constant $c$ . proof: I write down what I want to prove: $(1)$ I would like to prove that $\frac{f(z)}{z}$ is entire. $(2)$ I would like to prove $\left|\frac{f(z)}{z} \right|$ is bounded. For $(1)$ I think I must apply Riemann's removable theorem to extend analytically $\frac{f(z)}{z}$ to $\mathbb{C}$ . It happens if it is bounded at that singularity. I'm not sure if this holds by our second assumption. For $(2)$ I observe $
\left| \frac{f(z)}{z} \right|\leq 1\quad \ \forall z \in \partial D_1(0).
$ So, finally I could apply Liouville's  theorem. I'll appreciate if someone could help me out. Thanks.",['complex-analysis']
3509015,An old and interesting problem in combinatorics from Russia Mathematics Olympiad,"Can the numbers from $1$ to $81$ be written on a $9 \times 9$ board, so that the sum of the numbers in each $3\times 3$ square is the same? I believe I have not made much progress and am missing the key insight here. Any advice?","['contest-math', 'combinatorics']"
3509019,Finding integral points on an elliptic $y^2-3y=x^3+x^2$ curve using the LMFDB-database,"I have the following elliptic curve that I want to look up in the LMFDB-database : $$\text{k}:\space\space\space y^2-3y=x^3+x^2$$ Using the Weierstrass form of my elliptic curve, I wrote my equation in the form: $$y^2+a_1xy+a_3y=x^3+a_2x^2+a_4x+a_6$$ Which gives $a_1=0,a_2=1,a_3=-3,a_4=0$ and $a_6=0$ . Then I will get two parameters of this elliptic curve: Discriminant: $$\Delta=-b_2^2b_8-8b_4^3-27b_6^2+9b_2b_4b_6$$ Where $b_2=a_1^2+4a_2,b_4=2a_4+a_1a_3,b_6=a_3^2+4a_6$ and $b_8=a_1^2a_6+4a_2a_6-a_1a_3a_4+a_2a_3^2-a_4^2$ . Using my values I get: $$\Delta=-2331$$ j-invariant: $$\text{j}=\frac{\left(b_2^2-24b_4\right)^3}{\Delta}$$ Using my values I get: $$\text{j}=-\frac{4096}{2331}$$ But when I tried to look up my elliptic curve using the discriminant I didn't find mine. Where did I go wrong?","['elliptic-curves', 'number-theory', 'elementary-number-theory', 'integers', 'polynomials']"
3509038,Field lines as solutions to differential equation,"Instead of using arrows to represent a planar vector field, one sometimes uses families of curves called field lines. A curve $y = y(x)$ is a field line of the vector field $F(x, y)$ if at each point $(x_0, y_0)$ on the curve, $F(x_0, y_0)$ is tangent to the curve. Show that the field lines $y = y(x)$ of a vector field $F(x,y) = P(x,y)i + Q(x,y)j$ are solutions to the differential equation $dy/dx = Q/P$ . Find the field lines of $F(x, y) = yi + xj$ .","['vectors', 'ordinary-differential-equations', 'vector-fields', 'calculus', 'vector-analysis']"
3509039,Calculate Homography with and without SVD,"I've rendered an example for this question. 
With the trick of https://math.stackexchange.com/a/2619023/741822 I was able to calculate what seems to be the homography matrix (it passed 2 tests on $p_5$ and $p_6$ ). Unfortunately, I'm not able to get the result via SVD which is why I created that question 3D Data $p_0 = (0|0|0)$ $p_1 = (-7|0|0)$ $p_2 = (3|-6|0)$ $p_3 = (7|-4|0)$ $p_4 = (3|2|0)$ $p_5 = (6|1|0)$ $p_6 = (4|7|0)$ 2D Data $p_0 = (700|288)$ $p_1 = (93|63)$ $p_2 = (293|868)$ $p_3 = (1207|998)$ $p_4 = (1218|309)$ $p_5 = (1540|502)$ $p_6 = (1679|128)$ Calculation General Formula $PH = \begin{bmatrix} -x_1 \quad -y_1 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_1x_1' \quad y_1x_1' \quad x_1' \\ 0 \quad 0 \quad 0 \quad -x_1 \quad -y_1 \quad -1 \quad x_1y_1' \quad y_1y_1' \quad y_1' \\ -x_2 \quad -y_2 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_2x_2' \quad y_2x_2' \quad x_2' \\ 0 \quad 0 \quad 0 \quad -x_2 \quad -y_2 \quad -1 \quad x_2y_2' \quad y_2y_2' \quad y_2' \\ -x_3 \quad -y_3 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_3x_3' \quad y_3x_3' \quad x_3' \\ 0 \quad 0 \quad 0 \quad -x_3 \quad -y_3 \quad -1 \quad x_3y_3' \quad y_3y_3' \quad y_3' \\ -x_4 \quad -y_4 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_4x_4' \quad y_4x_4' \quad x_4' \\ 0 \quad 0 \quad 0 \quad -x_4 \quad -y_4 \quad -1 \quad x_4y_4' \quad y_4y_4' \quad y_4' \\ \end{bmatrix} \begin{bmatrix}h1 \\ h2 \\ h3 \\ h4 \\ h5 \\ h6 \\ h7 \\ h8 \\h9 \end{bmatrix} = 0$ Add new row to get a $9 \times9$ matrix $PH = \begin{bmatrix} -x_1 & -y_1 & -1 & 0 & 0 & 0 & x_1x_1' & y_1x_1' & x_1' \\ 0 & 0 & 0 & -x_1 & -y_1 & -1 & x_1y_1' & y_1y_1' & y_1' \\ -x_2 & -y_2 & -1 & 0 & 0 & 0 & x_2x_2' & y_2x_2' & x_2' \\ 0 & 0 & 0 & -x_2 & -y_2 & -1 & x_2y_2' & y_2y_2' & y_2' \\ -x_3 & -y_3 & -1 & 0 & 0 & 0 & x_3x_3' & y_3x_3' & x_3' \\ 0 & 0 & 0 & -x_3 & -y_3 & -1 & x_3y_3' & y_3y_3' & y_3' \\ -x_4 & -y_4 & -1 & 0 & 0 & 0 & x_4x_4' & y_4x_4' & x_4' \\ 0 & 0 & 0 & -x_4 & -y_4 & -1 & x_4y_4' & y_4y_4' & y_4' \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\ \end{bmatrix} \begin{bmatrix}h1 \\ h2 \\ h3 \\ h4 \\ h5 \\ h6 \\ h7 \\ h8 \\h9 \end{bmatrix} = \begin{bmatrix}0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\1 \end{bmatrix}$ Fill in values $P = \displaystyle \left[\begin{matrix}-93 & -63 & -1 & 0 & 0 & 0 & -651 & -441 & -7\\0 & 0 & 0 & -93 & -63 & -1 & 0 & 0 & 0\\-293 & -868 & -1 & 0 & 0 & 0 & 879 & 2604 & 3\\0 & 0 & 0 & -293 & -868 & -1 & -1758 & -5208 & -6\\-1207 & -998 & -1 & 0 & 0 & 0 & 8449 & 6986 & 7\\0 & 0 & 0 & -1207 & -998 & -1 & -4828 & -3992 & -4\\-1218 & -309 & -1 & 0 & 0 & 0 & 3654 & 927 & 3\\0 & 0 & 0 & -1218 & -309 & -1 & 2436 & 618 & 2\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\end{matrix}\right]$ Invert matrix $P$ and multiply with homogeneous vector to get matrix $H$ $H = P ^{-1} * \displaystyle \left[\begin{matrix}0\\0\\0\\0\\0\\0\\0\\0\\1\end{matrix}\right]$ Fill in and calculate result $H= \displaystyle \left[\begin{matrix}\frac{13724393133269}{1982067999646851}\\\frac{28367198390048}{1982067999646851}\\- \frac{5924445050578537}{660689333215617}\\\frac{11075363038922}{1982067999646851}\\- \frac{29759444826748}{1982067999646851}\\\frac{281612087155126}{660689333215617}\\\frac{2748231707}{1982067999646851}\\\frac{1890176860316}{1982067999646851}\\1\end{matrix}\right]$ Convert to $3 \times 3$ matrix $H= \displaystyle \left[\begin{matrix}\frac{13724393133269}{1982067999646851} & \frac{28367198390048}{1982067999646851} & - \frac{5924445050578537}{660689333215617}\\\frac{11075363038922}{1982067999646851} & - \frac{29759444826748}{1982067999646851} & \frac{281612087155126}{660689333215617}\\\frac{2748231707}{1982067999646851} & \frac{1890176860316}{1982067999646851} & 1\end{matrix}\right]$ In decimal numbers $H= \displaystyle \left[\begin{matrix}0.0069243 & 0.014312 & -8.9671\\0.0055878 & -0.015014 & 0.42624\\1.3865 \cdot 10^{-6} & 0.00095364 & 1.0\end{matrix}\right]$ Check General Formula to calculate point with the homography matrix $\left[\begin{array}{c}{x^{\prime} * \lambda} \\ {y^{\prime} * \lambda} \\ {\lambda}\end{array}\right]=\left[\begin{array}{lll}{h_{11}} & {h_{12}} & {h_{13}} \\ {h_{21}} & {h_{22}} & {h_{23}} \\ {h_{31}} & {h_{32}} & {h_{33}}\end{array}\right] \cdot\left[\begin{array}{l}{x} \\ {y} \\ {1}\end{array}\right]$ Check with point $p_5$ $(6|1|0)$ in 3d, and $(1540|502)$ in 2d $\displaystyle \left[\begin{matrix}0.00692 & 0.0143 & -8.97\\0.00559 & -0.015 & 0.426\\1.39 \cdot 10^{-6} & 0.000954 & 1.0\end{matrix}\right] * \displaystyle \left[\begin{matrix}1540\\502\\1\end{matrix}\right] = \displaystyle \left[\begin{matrix}8.8809\\1.4942\\1.4809\end{matrix}\right]$ $x^{\prime} = 8.8809 / 1.4809 \approx 6$ $y^{\prime} = 1.4942 / 1.4809 \approx 1$ Check with point $p_6$ $(4|7|0)$ in 3d, and $(1679|128)$ in 2d $\displaystyle \left[\begin{matrix}0.00692 & 0.0143 & -8.97\\0.00559 & -0.015 & 0.426\\1.39 \cdot 10^{-6} & 0.000954 & 1.0\end{matrix}\right] * \displaystyle \left[\begin{matrix}1679\\128\\1\end{matrix}\right] = \displaystyle \left[\begin{matrix}4.4907\\7.8863\\1.1244\end{matrix}\right]$ $x^{\prime} = 4.4907 / 1.1244 \approx 4$ $y^{\prime} = 7.8863 / 1.1244 \approx 7$ SVD Also described in the same linked question https://math.stackexchange.com/a/1289595/741822 , the last Vector of $V$ of the SVD result should be the homography matrix . Unfortunately, when I calculate it, I get something which doesn`t compare to the outcome before. Im using a python script and the svd_r function described here http://mpmath.org/doc/current/matrices.html to do the job from sympy import *
from mpmath import mp
from IPython.display import display

x_1 = [93,-7]
y_1 = [63,0]
x_2 = [293,3]
y_2 = [868,-6]
x_3 = [1207,7]
y_3 = [998,-4]
x_4 = [1218,3]
y_4 = [309,2]

P = Matrix([
 [x_1[0]*-1 ,y_1[0]*-1 ,-1 ,0 ,0 ,0 ,x_1[0]*x_1[1] ,y_1[0]*x_1[1] ,x_1[1]],
 [0 ,0 ,0 ,x_1[0]*-1 , y_1[0]*-1 ,-1 ,x_1[0]*y_1[1] ,y_1[0]*y_1[1] ,y_1[1]],
 [x_2[0]*-1 ,y_2[0]*-1 ,-1 ,0 ,0 ,0 ,x_2[0]*x_2[1] ,y_2[0]*x_2[1] ,x_2[1]],
 [0 ,0 ,0 ,x_2[0]*-1 , y_2[0]*-1 ,-1 ,x_2[0]*y_2[1] ,y_2[0]*y_2[1] ,y_2[1]],
 [x_3[0]*-1 ,y_3[0]*-1 ,-1 ,0 ,0 ,0 ,x_3[0]*x_3[1] ,y_3[0]*x_3[1] ,x_3[1]],
 [0 ,0 ,0 ,x_3[0]*-1 , y_3[0]*-1 ,-1 ,x_3[0]*y_3[1] ,y_3[0]*y_3[1] ,y_3[1]],
 [x_4[0]*-1 ,y_4[0]*-1 ,-1 ,0 ,0 ,0 ,x_4[0]*x_4[1] ,y_4[0]*x_4[1] ,x_4[1]],
 [0 ,0 ,0 ,x_4[0]*-1 , y_4[0]*-1 ,-1 ,x_4[0]*y_4[1] ,y_4[0]*y_4[1] ,y_4[1]], 
])

U, S, V = mp.svd_r(P)
display(U)
display(S)
display(V) which gives me $\displaystyle \left[\begin{matrix}0.0873968428008291 & 0.0682224411681068 & 7.78775669283991 \cdot 10^{-5} & -0.0307611970870902 & -0.047164344898438 & -4.31617782272004 \cdot 10^{-5} & -0.734415741368853 & -0.667210380053951 & -0.000763782447496494\\-0.1843309239413 & -0.0151678791434499 & -8.61313928411903 \cdot 10^{-5} & -0.177510439386387 & -0.196972727023475 & -0.000286352439937897 & 0.63453250533834 & -0.702034214967351 & -0.000453769669195275\\0.178502576804415 & 0.311156872128848 & 0.000325321190586577 & 0.77104600249753 & 0.446971881836657 & 0.000626740830458092 & 0.180594126282578 & -0.210730877512211 & -0.000435956847243289\\-0.669632242817859 & -0.535613720571974 & -0.000939878794635243 & 0.478237602951333 & -0.137967801791192 & 0.000272230101249859 & -0.129682866265852 & -0.0120304219769976 & -0.000973739592376422\\0.686925430229282 & -0.552023064176252 & 0.000248883448694341 & 0.254558652994286 & -0.38595799679894 & -0.000493796526964928 & 0.0865993870367576 & -0.046238819234477 & -0.00158707187572933\\-0.0790313367881196 & 0.55378718349052 & 0.0023403071836456 & 0.28199111075643 & -0.768889925477268 & -3.75757014301404 \cdot 10^{-5} & -0.0327206087187983 & 0.123639937181458 & 0.00038524047574376\\0.000450569782323263 & -0.0016191693558424 & 0.111042503882465 & 0.000924901054479592 & -0.000202764323620636 & 0.00454096268914545 & -0.00017046030825094 & -0.00107219827445236 & 0.993802818493512\\0.000319779779068856 & -0.000370212652047504 & 0.0463933646371045 & -0.000563323598922266 & -0.000441631139009571 & 0.998875291759086 & 0.000114547728791255 & -0.000112905332478902 & -0.0097483164699673\end{matrix}\right]$ No data in that matrix compares to the calculated homography matrix above. Creating a matrix with the last column and doing the check will end in $\approx (0|0)$ $\displaystyle \left[\begin{matrix}-0.000763782447496494 & -0.000453769669195275 & -0.000435956847243289\\-0.000973739592376422 & -0.00158707187572933 & 0.00038524047574376\\0.993802818493512 & -0.0097483164699673 & 1\end{matrix}\right] * \displaystyle \left[\begin{matrix}1679\\128\\1\end{matrix}\right] = \displaystyle \left[\begin{matrix}-1.3409\\-1.8377\\1668.3\end{matrix}\right]$ What am I doing wrong here? Am I missing a step when working with SVD or is it that the 1st result (via the trick) just works by pure chance?","['matrices', 'projective-geometry', 'svd']"
3509050,"Find a continuous, real valued function without maximum and show that a set is compact in $\ell^1$","I am working on following task: Let $$ \ell^1(\mathbb{R}) = \{ x=(x_k)_{k=1}^\infty : x_k \in \mathbb{R}, \|x\|=\sum_{k=1}^\infty |x_k| < \infty \}.$$ Fix $y \in \ell^1$ and define $$ B=\{x \in \ell^1: \|x\|\leq1\}$$ and $$ M = \{x\in \ell^1: |x_k|\leq |y_k|\}.$$ Find a continuous, real valued function on $\ell^1$ that does not have a largest value on $B$ and show that $M$ is a compact subset of $\ell^1$ . My first thought was that such a function doesn't exist, since $B$ is compact and a continous function will take its minimum and maximum value. But then I recognized that that $\ell^1$ is an infinite dimensional space, so the closed unit ball doesn't have to be compact anymore.
Do you have some good idea to define such a function? For the compactness of $M$ I thought about using the definition, that every sequence in $M$ has a convergent subsequence in $M$ . If I take a sequence $x^n$ in $M$ , then $|x^n_k| \leq |y_k|$ for every $k$ (coordinates) and $n$ (member of the sequence). But how can I construct a convergent subsequence out of $x^n$ ? Some hints or solutions would be really helpful.","['continuity', 'functional-analysis', 'compactness']"
3509227,Is $f(\sqrt{x}) + f(-\sqrt{x})$ always rational?,"Suppose that $f$ is a rational function over $\mathbb{R}$ (or $\mathbb{C}$ , or whatever field makes this easier to answer). Is $$\frac{f(\sqrt{x}) + f(-\sqrt{x})}{2}$$ always a rational function of $x$ ? This question is inspired by a sequence question. Given a sequence $a_n$ with a rational generating function, is it true that, say, $a_{2n}$ also has a rational generating function? If $f$ is the generating function for $a_n$ , then $(f(\sqrt{x}) + f(-\sqrt{x})) / 2$ is the generating function for $a_{2n}$ . As an example, take $f(x) = 1 / (1 - x)$ , which corresponds to the all-ones sequence $a_n = 1$ . Then $$\frac{f(\sqrt{x}) + f(-\sqrt{x})}{2} = \frac{1}{1 - x},$$ as we would expect. This specific case might be ""easy"" to answer (not for me), but when I first heard about this someone mumbled something about Galois theory and field extensions that I didn't understand. I would like to know more about this if it's relevant, since this question generalizes to seemingly harder cases.","['generating-functions', 'abstract-algebra', 'polynomials', 'sequences-and-series']"
3509248,Show $\frac{P^2(A)}{P(A+1)}$ is Bounded when $P$ is Gaussian Measure,"Suppose that $P$ is a standard Gaussian measure. Can we show that $$\sup_{\cal A} \left( \frac{P^2(\cal A)}{P(\cal A + 1 )}  \right) $$ is bounded? Here $\cal A + 1 $ means that we perturb each element of $\cal A$ by 1. Also, can we show that $$\sup_{\cal A} \left( \frac{P(\cal A)}{P(\cal A + 1 )}  \right) $$ is bounded? I believe that the latter is unbounded but the former should be bounded. Although, I do not know a formal way of approaching either problem.","['measure-theory', 'probability-distributions', 'probability-theory', 'probability']"
3509252,Reference request: generating functions of Beatty sequences,"Iâm interested in certain generating functions of Beatty sequences like the following: $$f_\alpha (x)=\sum_{n\ge 1} x^n \lfloor n\alpha\rfloor$$ $$g_\alpha (x,y)=\sum_{n\ge 1} x^n y^{\lfloor n\alpha\rfloor}$$ where $\alpha$ is an irrational number. Iâd like to know if there are any nontrivial special values or functional equations known for these functions (in particular, Iâm looking for results that arenât direct translations of theorems from number theory into GFs). Can anyone suggest any papers/online resources with actual calculated special values for these functions or nontrivial functional equations? I already know the following functional equations: $$f_\alpha(x)=g_{1/\alpha}(1,x)$$ $$\frac{1-x}{x}g_\alpha(x,y)+\frac{1-y}{y}g_{1/\alpha}(y,x)=1$$ Despite these nice and pleasingly symmetrical functional equations, I havenât been able to work out any nontrivial special values of these functions. I already know about this paper connecting Beatty sequences to generating functions. However, it does not suit my purposes because it uses GFs as a technique to prove results about Beatty sequences, whereas Iâm interested in explicitly calculating special values of the generating functions.","['functional-equations', 'ceiling-and-floor-functions', 'irrational-numbers', 'reference-request', 'sequences-and-series']"
3509261,Dominated convergence theorem counterexample,"In the Dominated Convergence Theorem, we usually assume that |fn|â¤g for some integrable function g. However, what is a counter-example where fn are not dominated by an integrable function? And I found this example but there is one thing I don't understand and I would like your help. So I considered the sequence of functions on $(0,1)$ $$ f_n(x) = \begin{cases}
n & \text{ if } x \in (0,1/n)\\
0 & \text{ otherwise}
\end{cases}$$ We have $\lim_{n \to \infty} f_n(x) = 0 = f(x)$ . However, $$\lim_{n \to \infty} \int_0^1f_n(x)dx = 1 \neq 0 = \int_0^1 f(x) dx$$ Question: (1) I'm having a problem with this limit here: $\lim_{n \to \infty} f_n(x) = 0 $ because there will be always point between $0$ and $1/n$ not matter how big $n$ is. Therefore, this limit will approach infinity. (2) Why is this integral 1? $\lim_{n \to \infty} \int_0^1f_n(x)dx = 1$","['integration', 'measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'measurable-functions']"
3509277,A triangle whose only lattice points are exactly its vertices has area $1/2$. Help formalize my proof?,"Consider a triangle $ABC$ in the plane. Its vertices are all lattice points (their coordinates are integers.) Further, there are no other lattice points in this triangle, either on its boundary or interior. It can be shown in a number of ways that such a triangle has area $\frac{1}{2}$ . There are several questions on this site asking for proofs, but I have an idea for an argument that I haven't seen in the answers to those questions. I'm having a little trouble formalizing it, though, and I'm not entirely sure the idea can lead to a rigorous proof. My idea is this: Every triangle whose vertices are all lattice points necessarily has area of the form $\frac{n}{2}$ where $n$ is a positive integer (this would follow directly from the expression for the area of a triangle in terms of its vertices' coordinates.) If a triangle $ABC$ contains a lattice point $A'$ in its edge or interior, we can construct a new triangle $A'BC$ by replacing the vertex $A$ with $A'$ . Visually, it seems to me that such a triangle has strictly smaller area than $ABC$ does, and its vertices are also all lattice points, so its area is of the form $\frac{m}{2}$ with $0 < m < n$ . It should then follow immediately that any such $ABC$ has area greater than $\frac{1}{2}$ . It is easy to see that there are some lattice-point triangles having area $1/2$ , so the only lattice-point triangles having area $\frac{1}{2}$ are those that do not contain other lattice points. Something about this proof feels handwavy to me (in particular, I haven't justified that by replacing a vertex of a triangle with a point on its edge or in its interior, you obtain a strictly smaller triangle), but I think the basic outline is promising. Moreover, I've only established that any triangle containing lattice points has area $> 1/2$ , not that every triangle not containing lattice points has area $1/2$ . Can someone help with this?","['analytic-geometry', 'coordinate-systems', 'geometry', 'solution-verification', 'recreational-mathematics']"
3509279,Does the sign of curvature remain constant under the sum of their metrics?,"Does the sign of curvature remain constant under the sum of their metrics over a fixed manifold? e.g. If $(M,g_1)$ and $(M,g_2)$ are closed Riemannian manifolds of positive curvature then $(M,g_1+g_2)$ is also of positive curvature? Update: Q2: What if we weaken the problem by allowing manifolds having boundary?","['curvature', 'differential-topology', 'riemannian-geometry', 'differential-geometry']"
3509310,Bernoulli number generating function proof problem,A sequence $(b_n)_n$ is given as $b_0 = 1$ and for every $n \in \mathbb N$ $$\sum_{k=0}^n\binom{n+1}{k}b_k=0\tag{1}$$ The task is to find its exponential generating function $f(x)$ From $(1)$ we get $$\sum_{k=0}^n\binom{n}{k}b_k=\sum_{k=0}^{n-1}\binom{n}{k}b_k+b_n=b_n$$ $$f(x)=\sum_{n=0}^\infty\frac{b_n}{n!}x^n$$ $$f(x)e^x=\left(\sum_{n=0}^\infty\frac{b_n}{n!}x^n\right)\left(\sum_{n=0}^\infty\frac{1}{n!}x^n\right)=\sum_{n=0}^\infty\frac{1}{n!}x^n\left(\sum_{k=0}^n\binom{n}{k}b_k\right)=\sum_{n=0}^\infty\frac{b_n}{n!}x^n=f(x)$$ $$f(x)(e^x-1)=0$$ Now i am not sure what to do or if i made a mistake.,"['generating-functions', 'fake-proofs', 'combinatorics', 'bernoulli-numbers', 'discrete-mathematics']"
3509327,Induction step is not clear in BÃ³na's proof that the generating function of the alternating runs has $-1$ as a root of a certain multiplicity,"I am reading the book Combinatorics of Permutations (2nd edition) by MiklÃ³s BÃ³na, and in $\S$ 1.2 Alternating Runs , he states and proves a lemma on page 29 whose proof I do not fully understand. Assume $n > 1$ . Let $p := p_1 p_2 \dotsm p_n$ be a permutation in $S_n$ . We say that $p$ changes direction at $i \in \left\{2,3,\ldots,n-1\right\}$ if either $p_{i-1} < p_i > p_{i+1}$ or $p_{i-1} > p_i < p_{i+1}$ . We say that $p$ has $k$ alternating runs if $p$ changes direction at $k-1$ points. Let $r(p)$ denote the number of alternating runs of $p$ . Let $G(n,k) := \lvert\{ p \in S_n : r(p) = k\}\rvert$ , that is, $G(n,k)$ equals the number of permutations in $S_n$ having $k$ alternating runs. Consider the generating function $$
G_n(x) := \sum_{p \in S_n} x^{r(p)} = \sum_{k = 1}^{n-1} G(n,k) x^k
$$ Observe that the coefficients of $G_n(x)$ are all even, because if $p \in S_n$ has $k$ alternating runs, then so does $p^c$ , the complement of $p$ , given by $p^c(i) := n + 1 - p(i)$ . One also empirically observes that $-1$ is a root of $G_n(x)$ with multiplicity $\lfloor n/2 \rfloor - 1$ , by computing $G_n(x)$ for many small values of $n$ . BÃ³na gives a combinatorial proof that the second observation holds for all $n > 1$ , and this is essentially the content of the lemma on page 29. Some more definitions before stating it: Let $1 \leq j \leq \lfloor n/2 \rfloor$ and $p \in S_n$ . We say that $p$ is $j$ -half-ascending if the last $j$ disjoint pairs of elements in the sequence $p_1,p_2,\dotsc,p_n$ are ascending, that is, if $p_{n+1-2i} < p_{n+2-2i}$ for all $1 \leq i \leq j$ . For a $(j+1)$ -half-ascending permutation $p \in S_n$ , define $r_j(p)$ to be the number of alternating runs of the subsequence $p_1,p_2,\dotsc,p_{n-2j}$ , and $s_j(p)$ to be the number of descents of the subsequence $p_{n-2j},p_{n-2j+1},\dotsc,p_n$ (the number of descents of a sequence $u_1, u_2, \ldots, u_k$ is the cardinality of the set $\{i \in [k-1] : u_i > u_{i+1}\}$ ). Define $t_j(p) := r_j(p) + s_j(p)$ , and define $$
G_{n,j}(x) := \sum_{p} x^{t_j(p)},
$$ where the sum is taken over all the $(j+1)$ -half-ascending permutations $p$ in $S_n$ . Now, we have the following lemma on page 29: LEMMA 1.41 For all $n > 1$ and $0 \leq j \leq \lfloor n/2 \rfloor -1$ , we have $$
\frac{G_n(x)}{2(1+x)^j} = G_{n,j}(x).
$$ The proof is purportedly by induction. The base cases are easy: this is how they go. Base case I: $j=0$ and $n \geq 2$ . A $1$ -half-ascending permutation $p \in S_n$ satisfies the single constraint $p_{n-1} < p_n$ . Clearly, $r_0(p) = r(p)$ and $s_0(p) = 0$ , so $t_0(p) = r(p)$ . Since $r(p) = r(p^c)$ , and for every $p \in S_n$ either $p$ or $p^c$ is $1$ -half-ascending, we see that $$
\frac{G_n(x)}{2} = G_{n,0}(x),
$$ as required. Base Case II: $j = 1$ and $n \geq 4$ . A similar argument as in the previous case shows that $$
\frac{G_n(x)}{2} = \sum_{p : p_{n-3} < p_{n-2}} x^{r(p)}.
$$ Let $I \colon S_n \to S_n$ be the involution defined by $I(p_1\dotsm p_{n-2}p_{n-1}p_n) := p_1\dotsm p_{n-2}p_n p_{n-1}$ , that is, $I$ swaps $p_{n-1}$ and $p_n$ . One can check that for every $p \in S_n$ such that $p_{n-3} < p_{n-2}$ , $r(p)$ and $r(I(p))$ differ by $1$ . Let $q \equiv q(p)$ be that permutation in the set $\{ p, I(p) \}$ with smaller number of alternating runs. Then, $$
\sum_{p : p_{n-3} < p_{n-2}} x^{r(p)} = \sum_{q(p)} \bigl(x^{r(q)} + x^{r(q)+1}\bigr) = (1+x) \sum_{q(p)} x^{r(q)}.
$$ Let $q' \equiv q'(p)$ be that permutation in the set $\{ p, I(p) \}$ which is $2$ -half-ascending. Then, it turns out that $r(q) = t_1(q')$ , so that $$
\sum_{q(p)} x^{r(q)} = \sum_{q'(p)} x^{t_1(q')} = G_{n,1}(x).
$$ Hence, $$
\frac{G_n(x)}{2(1+x)} = G_{n,1}(x),
$$ as required. Now, how does one check that $r(q) = t_1(q')$ ? Note that it suffices to check this in the case $n = 4$ , because it is only the last $4$ terms in the sequence $p_1,\dotsc,p_n$ that are really relevant here. Thus, one just writes down all $6$ pairs of elements of the form $(p,I(p))$ where $p \in S_4$ is $2$ -half-ascending, and checks that $t_1(p)$ equals the number of alternating runs of that permutation between $p$ and $I(p)$ which has smaller number of alternating runs. This completes the second base case. Induction Step. Now, this is what BÃ³na has to say regarding the induction step (on pages 29â30): Now let us assume that we know that the statement holds for $j-1$ and prove it for $j$ . Apply $I$ to the two rightmost entries of our permutations to get pairs as in the initial case, and apply the induction hypothesis to the leftmost $n-2$ elements. By the induction hypothesis, the string of the leftmost $n-2$ elements can be replaced by a $j$ -half-ascending $(n-2)$ -permutation, and the number of runs can be replaced by the $t_{j-1}$ -parameter. In particular, $p_{n-3} < p_{n-2}$ will hold, and therefore we can verify that our statement holds in both cases ( $p_{n-2} < p_{n-1}$ or $p_{n-2} > p_{n-1}$ ) exactly as we did in the proof of the initial case. $\blacksquare$ Quite frankly, this is completely confusing and I can find no interpretation of this paragraph that actually works. For example, what does . . .apply the induction hypothesis to the leftmost $n-2$ elements. . . mean? How does one apply the induction hypothesis to individual elements? This is just one of many points due to which the given outline for the inductive step does not go through, in my opinion. Can anyone help me find out how to fix the proof, or even exhibit how the inductive step works in the case $n = 6$ ? References BÃ³na, MiklÃ³s , Combinatorics of permutations , Discrete Mathematics and its Applications. Boca Raton, FL: CRC Press (ISBN 978-1-4398-5051-0/hbk; 978-1-4398-5052-7/ebook). 458Â p. (2012). ZBL1255.05001 . BÃ³na, MiklÃ³s; Ehrenborg, Richard , A combinatorial proof of the log-concavity of the numbers of permutations with $k$ runs , J. Comb. Theory, Ser. A 90, No. 2, 293-303 (2000). ZBL0951.05002 .","['permutations', 'symmetric-groups', 'combinatorics', 'generating-functions']"
3509357,"Prove that $\sqrt[3]{18 + \sqrt{325}} + \sqrt[3]{18 - \sqrt{325}} = 3$ without using Cardano's formula. (Hint, what is $(3\pm \sqrt{13})^3$","Prove that $\sqrt[3]{18 + \sqrt{325}} + \sqrt[3]{18 - \sqrt{325}} = 3$ without using Cardano's formula. (Hint, what is $(3\pm \sqrt{13})^3$ I have that $$(3 + \sqrt{13})^3 = 144  + 40 \sqrt{13} $$ and $$(3 - \sqrt{13})^3 = 144  - 40 \sqrt{13} $$ A cursory look into Bombelli's method led me to  the following system of equations: $$\sqrt[3]{18 + \sqrt{325}} = a + b^{1/2}$$ $$\sqrt[3]{18 - \sqrt{325}} = a - b^{1/2}$$ I am unsure how to solve this system of equations without making a mess of the radicals...I know however that the given cube roots on the LHS of the above system are solutions to the cubic $x^3 + 3x = 36 $","['cubics', 'algebra-precalculus', 'nested-radicals']"
3509388,About the limit of a recursive sequence,"The question is the following: $x_0 >0$ , $\forall n \in \mathbb{N},x_{n+1}=|x_n - n|$ . Prove that $\lim_{n\to \infty} \frac{x_n}{n} = \frac{1}{2}$ I tried to remove the absolute value treating two cases: if, for any n, the n-th term of the sequence is greater than n, then the sequence $({x_n})$ converges and thus $(\frac{x_n}{n})$ tends to 0 as n tend to infinity. I wasn't able to deduce anything when $x_n<n$","['limits', 'sequences-and-series', 'real-analysis']"
3509406,Find $\lim\limits_{n \to \infty} \left ( n - \sum\limits_{k = 1} ^ n e ^{\frac{k}{n^2}} \right)$.,"I have to find the limit: $$\lim\limits_{n \to \infty} \bigg ( n - \displaystyle\sum_{k = 1} ^ n e ^{\frac{k}{n^2}} \bigg)$$ This is what I managed to do: $$ e^{\frac{1}{n^2}} + e^{\frac{1}{n^2}} + ... + e^{\frac{1}{n^2}}
\le e^{\frac{1}{n^2}} + e^{\frac{2}{n^2}} + ...e^{\frac{n}{n^2}} \le
e^{\frac{n}{n^2}} + e^{\frac{n}{n^2}} + ... e^{\frac{n}{n^2}}$$ $$ n e^{\frac{1}{n^2}}
\le \displaystyle\sum_{k = 1} ^ n e ^{\frac{k}{n^2}} \le
ne^{\frac{1}{n}}$$ $$ -n e^{\frac{1}{n}}
\le - \displaystyle\sum_{k = 1} ^ n e ^{\frac{k}{n^2}} \le
- n e^{\frac{1}{n^2}}$$ $$ n - n e^{\frac{1}{n}}
\le n - \displaystyle\sum_{k = 1} ^ n e ^{\frac{k}{n^2}} \le
n - n e^{\frac{1}{n^2}}$$ Here I found that the limit of the left-hand side is equal to $-1$ , while the limit of the right-hand side is $0$ . So I got that: $$-1 \le n - \displaystyle\sum_{k = 1} ^ n e ^{\frac{k}{n^2}} \le 0$$ And I cannot draw a conclusion about the exact limit. What should I do?","['limits', 'calculus', 'summation']"
3509408,Show that $|f(z)| \leq |z|$ on annulus,"Let $D = \{ z \in \mathbb{C} : 2 < |z| < 3 \}$ . Suppose that $f$ is holomorphic on $D$ and $f$ is continuous on $\overline{D}$ . Suppose that $\max \{ |f(z)| : |z| = 2\} \leq 2$ and $\max \{ |f(z)| : |z| = 3 \} \leq 3$ . Show that $\forall z \in D, |f(z)| \leq |z|$ . The solution I am thinking of I believe is wrong, but here is what it is. Assume $f$ is non-constant. By the maximum modulus principle $|f|$ assumes its maximum on $\partial D$ , Hence, $|f(z)| \leq 3, \forall z \in D$ . Define $g(z) = \frac{f(z)}{z}$ , which is analytic. Consider $r \in (2,3)$ . For $|z| = r$ , $|g(z)| \leq \frac{3}{r}$ . Since for $|z| = 2$ , $|g(z)| \leq 1$ , by the maximum modulus principle, $\forall |z| \in (2,r), |g(z)| \leq \frac{3}{r}$ . Let $r \rightarrow 3$ , then $\forall |z| \in (2,3), |g(z)| \leq 1 \implies |f(z)| \leq |z|$ . I essentially followed the same technique as in Schwarz's Lemma, but I am not sure if this is right.",['complex-analysis']
3509453,Does this condition imply isomorphism?,"Let $k$ be a field and let $X$ and $Y$ be objects of a category $C$ enriched over $k$ -vector spaces. Composition gives us a linear map $\hom(X,Y) \otimes \hom(Y,X) \to \hom(X,X) \oplus \hom(Y,Y)$ , that sends an elementary tensor $f\otimes g$ to $(g\circ f, f \circ g)$ . Assume that $(id_X,id_Y)$ is in the image of this map. Is it then true that $X$ and $Y$ are isomorphic? I tried finding a counterexample when $C$ is the category of vector spaces, but couldn't find one. I also tried proving it by taking a sum of elementary tensors in the preimage of $(id_X,id_Y)$ and trying to construct an elementary tensor out of them. I didn't succeed in doing this either, even in the case of a sum of two elementary tensors. Counterexamples or proofs (maybe with more hypothesis) are much appreciated.","['enriched-category-theory', 'linear-algebra']"
3509471,Triangle area from uniformly distributed points along a line,3 random numbers from independent uniform distributions between 0 and 1 are selected. How to calculate the expected triangle area if the points are arranged as in the sketch?,"['geometric-probability', 'triangles', 'probability']"
3509493,Avoiding catastrophic cancellation with $\sqrt{1+x} - 1$ for $x$ close to $0$,"I'm trying to figure out how to avoid catastrophic cancellation for the following expression $$\sqrt{1+x} - 1$$ for $x$ being a number very close to $0$ . Of course, the answer would come to $0$ unless the expression is changed around. Any help is appreciated! Thanks!","['algebra-precalculus', 'catastrophic-cancellation']"
3509582,circle envelope tangent in another circle,"As the picture shows, One big circle , $(0,0)$ ,radius=R, there is a small circle in it, $(m,0)$ ,radius=r . G is on the big circle. From G ,we can do two tangent lines about the small circle. Get the points of intersection E and F. line EF has a envelope about G , which seem like a circle. How to prove it? Since calculating it requires much effort. Some additional infomation: By picking special points,I get the radius of envelope circle is $$\frac{R \left(m^4-2 m^2 \left(r^2+R^2\right)-2 r^2 R^2+R^4\right)}{\left(m^2-R^2\right)^2}$$ and the circle center $$\left(\frac{1}{2} \left(\frac{R \left(-m^2+2 m R+2 r^2-R^2\right)}{(R-m)^2}-\frac{R \left(-m^2-2 m R+2 r^2-R^2\right)}{(m+R)^2}\right),0\right)$$","['envelope', 'tangent-line', 'circles', 'geometry']"
3509584,Is $\tan^{-1}\tan^{-1}1$ irrational?,"Here , it is proven that $\arctan(2)$ is irrational. Here , it is proven that $\arctan(x)$ is irrational for natural $x$ . By a proof similar to that from the last linked post, it can easily be shown that $\arctan \frac 1x$ is irrational for natural $x$ . Here , it is proven that $\arctan(x)$ is a rational multiple of $\pi$ iff $(1+xi)^n$ is a real number for some positive integer $n$ . With these in mind, I am wondering if $\tan^{-1}(\tan^{-1}(1))$ is irrational. It probably is, but I have yet to prove it. We can write $\tan^{-1}1$ as $\frac{\pi}4$ which follows from the fact that $(1+i)^4 = -4$ , but I am not sure how to use this information further. I suspect the proof of this is unreachable, though MSE has surprised me in the past . With this in mind, I have a few related questions, in order of how unlikely they are to be answered: Is $\tan^{-1}\tan^{-1}1$ transcendental? Is $\tan^{-1}\tan^{-1}1$ irrational? Is there any literature on whether $\tan^{-1}\tan^{-1}1$ or a related evaluation of $\arctan$ is irrational/transcendental? Are there any open conjectures which, if true, the irrationality/transcendentality of $\tan^{-1}\tan^{-1}1$ would follow?","['irrational-numbers', 'reference-request', 'transcendental-numbers', 'trigonometry', 'rational-numbers']"
3509641,Group of order $3k$ has subgroup of index $3$ - simple proof,"Suppose $G$ is a group of order $3k$ with $gcd(k,6) = 1$ (so $2 \nmid k, 3 \nmid k$ ). Why does $G$ always have a subgroup of index $3$ ? If there is a subgroup of index $3$ then it will be normal - this question is about the existence of such a subgroup though. The result actually follows from Feit-Thompson and existence of Hall subgroups but this is too overpowered. I'm looking for an simple proof, something like: If $G$ is a counterexample with $|G|$ minimal and $H$ is a nontrivial normal subgroup with $3 \nmid |H|$ , then $G/H$ has a subgroup of index $3$ , and its preimage in $G$ would then be index $3$ in $G$ . So WLOG every nontrivial normal subgroup of $G$ has order divisible by $3$ . How can one finish the argument?","['group-theory', 'abstract-algebra', 'finite-groups']"
3509648,Integrability of composite of Riemann integrable functions given condition,"This is a question from a past qualifying exam, which I am studying for. The question has been asked before here , and has an answer, but the answer uses Lebesgue's criterion for Riemann integrability, which is disallowed on the exam. Is there a more elementary way to solve this question? Let $f: [0,1] \to \mathbb{R}$ and $g: [0,1] \to [0,1]$ be two Riemann integrable functions. Assume that $|g(x) - g(y)| \geq \alpha |x-y|$ for any $x,y \in [0,1]$ and some fixed $\alpha \in (0,1)$ . Show that $f \circ g$ is Riemann integrable. Some thoughts have been bounding the intervals in which $f$ has a large oscillation by its integrability, and trying use the condition on $g$ to control the growth of these interval lengths. However, I am unsure how to apply the Riemann integrability of $g$ .","['riemann-integration', 'analysis', 'real-analysis']"
3509664,Limit calculation using derivative,"I encountered this exercise:
Let $f(x)$ be a differentiable function, and suppose  that there exists some $a$ where $f'(a) \ne 0 $ .
Calculate the limit: $$ \lim_{h\rightarrow0}\frac{f(a+3h)-f(a-2h)}{f(a-5h)-f(a-h)}. $$ I have no clue how I can solve this. I was trying to separate into two terms, and multiply and divide by $h$ , but it solves just the numerator limit. What can be done with the denominator limit?","['limits', 'calculus', 'real-analysis']"
3509666,On the sums $\sum\limits_{i=0}^n\frac{i}{n}\ln(\frac{i}{n}) $ and $\sum\limits_{i=0}^n(-1)^i\frac{i}{n}\ln(\frac{i}{n}) $,"I was thinking about
unimodal sequences,
and the two which
occurred to me are $\binom{n}{i}$ and $\dfrac{i}{n}\ln(\dfrac{i}{n})
$ ,
both for $i=0$ to $n$ (for the second,
its value is $0$ at $i=0$ ). For the first,
it is well known that $\sum_{i=0}^n 
\binom{n}{i}
=2^n
$ and $\sum_{i=0}^n 
(-1)^i\binom{n}{i}
=0
$ . I naturally wondered about
the corresponding results for $A_n
=\sum_{i=0}^n\dfrac{i}{n}\ln(\dfrac{i}{n})
$ and $A_n^{\pm}
=\sum_{i=0}^n(-1)^i\dfrac{i}{n}\ln(\dfrac{i}{n})
$ . Here's what I have shown. $$A_n
= -\dfrac{n}{4}+\dfrac{\ln(n)}{12n}+\dfrac1{4n}+O\left(\dfrac1{n^2}\right)
$$ $$A_{2n}^{\pm}
=\dfrac{3\ln(n)}{8n}+O\left(\dfrac1{n}\right)
$$ $$A_{2n+1}^{\pm}
=\dfrac{\ln(n)}{8n}+O\left(\dfrac1{n}\right)
$$ I have verified these computationally. My proofs,
as they often are,
are fairly messy,
especially for $A_{n}^{\pm}
$ ,
so my questions are
(ya gotta have a question) How well known are these results? Are there reasonably simple proofs of them? Is there a simple proof that $A_{n}^{\pm}
\to 0$ as $n \to \infty$ ?","['limits', 'summation', 'logarithms']"
3509682,Alternative proof of the generalized associative law for groups,"The generalized associative law for groups claims that the value of $a_1\star a_2\star ... \star a_n$ is independent of how it is bracketed, where the symbols denote the usual notations of group theory. While attempting a proof on my own, I discovered a method that seems right to me but does not appear anywhere as a standard proof. Please comment on the validity of this proof, since it is likely I messed up somewhere. Throughout the proof we consider elements belonging to a group $G$ . We attempt a proof by induction on the number of elements in the expression. The base cases of 1,2,3 are seen to be true trivially or by the associative property. Now assume that the value of any n-element expression is independent of how the expression is bracketed. (Induction Hypothesis) Now consider any $(n+1)$ -element expression given by $a_1\star a_2\star ... \star a_n\star a_{n+1}$ . All bracketings of this expression may be divided into $n$ types as follows (these types are not necessarily disjoint ): bracketings containing $(a_1\star a_2)$ bracketings containing $(a_2\star a_3)$ . . . n. bracketings containing $(a_n\star a_{n+1})$ Let $(a_i\star a_{i+1})$ = $a_{(i,i+1)}$ , which is also an element of $G$ (where $i$ ranges from $1$ to $n$ ).
Now for any type $i$ considered above, the corresponding $(n+1)$ - element expression can be reduced to an $n$ - element expression by substituting $a_{(i,i+1)}$ instead of $(a_i\star a_{i+1})$ . Then by IH, all bracketings of this type evaluate to a bracketing- independent value which we shall call $A_i$ . It remains to show that $A_1 = A_2 = A_3 = .... = A_n$ . But note that for any $i$ , $A_i$ = $(..((a_1\star a_2)\star a_3)..\star a_{i-1}) \star ((a_{i}\star a_{i+1})\star a_{i+2})\star(..((a_{i+3}\star a_{i+4})\star a_{i+5})..\star a_{n+1})$ = $(..((a_1\star a_2)\star a_3)..\star a_{i-1}) \star (a_{i}\star( a_{i+1}\star a_{i+2}))\star(..((a_{i+3}\star a_{i+4})\star a_{i+5})..\star a_{n+1})$ = $A_{i+1}$ . So for all $i$ from $1$ to $n$ , $A_i$ = $A_{i+1}$ . This implies $A_1 = A_2 = A_3 = .... = A_n$ , as required. This proves the result for the case of $n+1$ . The result is then true by induction for all natural $n$ .","['alternative-proof', 'group-theory', 'abstract-algebra', 'induction']"
3509705,Inequality proof. Bounded second derivative.,Given $|fââ(x)| < c$ for all Real values of $x$ and a $c > 0$ . Prove the following inequality: $$| f(x+1) + f(x-1) - 2f(x)| < 2c $$ So Iâm relatively sure it has something to do with the second difference formula. But Iâm not entirely sure how to make sense of it. Difference formula: $$ fââ(x) =  ( f(x + h) + f( x- h ) - 2f(x))/h.  (h -> 0)$$,"['calculus', 'derivatives', 'inequality', 'real-analysis']"
3509709,Proving a solution doesn't exist of a DE,"Find all solutions of differential equation $xy'+(1-x)y=0$ and $xy'+(1-x)y=1$ . Also does there exist a solution such that $y(0)=0$ and $y(0)=1$ for both these D.Es ? My attempt : $xy'+ (1-x)y=0 \implies$ $\int \frac{dy}{y} = \int \frac{x-1}{x} dx$ for $x\ne 0$ and $y(x)\ne 0$ for any $x \in \mathbb{R}$ . Hence $y=\frac{k}{x}e^x$ for constant $k\in \mathbb{R}-\{0\}$ . But $y(x)=0 \,\forall\, x$ is also a valid solution of this D.E. Hence there exist a solution such that $y(0)=0$ . All possible solutions of this D.E are: $$y=\left \{
	\begin{array}{ll}
		\frac{k}{x}e^x  & \mbox{if } x \ne 0 \\
		0 & \mbox{if } x =0
	\end{array}
\right. $$ where $k\in \mathbb{R}$ .  But $y(0)=1$ is not possible since $0\cdot \frac{dy}{dx}\mid_{x=0}+(1-0)y(0)=0$ yields $y(0)=0$ . I am not sure wether this proves the first part of the question. For the second part, we have the D.E $xy'+(1-x)y=1 \implies$ $\int d(xe^{-x}y) = \int e^{-x}dx$ . Hence $xy=-1+ce^{x}$ where $c \in \mathbb{R}$ . In this case, all possible solutions are: $$y=\left \{
	\begin{array}{ll}
		\frac{-1}{x}+\frac{c}{x}e^x  & \mbox{if } x \ne 0 \\
		1 & \mbox{if } x =0
	\end{array}
\right. $$ $y(0)=0$ is not possible since $0\cdot \frac{dy}{dx}\mid_{x=0}+(1-0)y(0)=1$ yields $y(0)=1$ . From the definition of $y$ above, is that a valid solution such that $y(0)=1$ ? Is this proof correct ?","['solution-verification', 'ordinary-differential-equations']"
3509716,Efficient algorithm to determine which of 2 sums of quantities infinitesimally close to 1 is greater,"Let $\gamma$ be infinitesimally close to 1 (or more precisely, let $\gamma=1-\epsilon$ and consider this problem for sufficiently small $\epsilon$ ). Suppose I have two quantities which consist of finite sums of powers of $\gamma$ , for example, $a=\gamma+\gamma^4$ and $b=\gamma^2+\gamma^3$ . Is there an efficient way to determine which of $a$ or $b$ is greater without doing a binomial expansion? (i.e. decide if $a-b>0$ ?) Obviously, if $a$ and $b$ consist of a different number of terms, then whichever has more terms is greater. In this case, however, they both have 2 terms. One straightforward way to decide which is bigger is to expand: $$ a=(1-\epsilon)+(1-\epsilon)^4 = (1-\epsilon)+(1-4\epsilon+6\epsilon^2-4\epsilon^3+\epsilon^4)=2-5\epsilon+6\epsilon^2-4\epsilon^3+\epsilon^4$$ $$b = (1-\epsilon)^2+(1-\epsilon)^3=(1-2\epsilon+\epsilon^2)+(1-3\epsilon+3\epsilon^2-\epsilon^3)=2-5\epsilon+4\epsilon^2-\epsilon^3$$ So for sufficiently small $\epsilon$ , we get $b<a$ since $4\epsilon^2<6\epsilon^2$ . However, ideally I'd like to implement this on a computer efficiently, and in particular, binomial coefficients can grow quite large and require arbitrary precision arithmetic if the array which decides whether the $i$ th power of $\gamma$ is present is large. I was hoping that there would be a straightforward, obvious way to just 'look' at them and decide which is bigger. One constraint I have is that for each $i$ , $\gamma^i$ appears in at most one of $a$ or $b$ with coefficient $0$ or $1$ . One result I discovered is that if we start with $\gamma$ and for the $n$ th power, add or subtract $\gamma^n$ to bring the current sum closer to $0$ , we get the Thue-Morse sequence with the signs. For example: We start with $\gamma$ . Since $\gamma>0$ , subtract $\gamma^2$ . Since $\gamma-\gamma^2>0$ , subtract $\gamma^3$ . Since $\gamma-\gamma^2-\gamma^3<0$ , add $\gamma^4$ etc. The sequence is $\gamma-\gamma^2-\gamma^3+\gamma^4-\gamma^5+\gamma^6+\gamma^7-\gamma^8-\dots$ . Since the Thue-Morse sequence is easy to compute, I was hoping this would give an easy way to generalize this to comparing arbitrary sums of powers of $\gamma$ by comparing them to the Thue-Morse sequence. Alternatively, a proof of the Thue-Morse property would be nice, and might give me some insight.","['computational-mathematics', 'discrete-mathematics', 'infinitesimals']"
3509769,Is set Dedekind continuous?,"The set is called Dedekind continuous when there is no way to make a $A||A'$ or $A)(A'$ cuts where: $A||A'$ cut means there is the biggest element in $A$ set and the lowest element in $A'$ set $A)(A'$ cut means there is no biggest element in $A$ and no lowest element in $A'$ We all know that $\mathbb{R}$ is Dedekind continuous. The question is whether the following subset of $\mathbb{R}$ is Dedekind continuous or not: $$ M = (-\infty; 2) \cup [3; +\infty) $$ It is obvious this set can't be ""geometry"" continuous since there is a gap between $2$ and $3$ but I can't find a way to make $A||A'$ or $A)(A'$ cuts to prove it is not Dedekind continuous.","['elementary-set-theory', 'continuity', 'real-numbers', 'real-analysis']"
3509803,Using the implies symbol in proofs,"Is it valid to use the implies symbol ( $\implies$ ) or the if and only if symbol ( $\iff$ ) in mathematical proofs to replace writing assumptions? For example, if I want to prove, $$\text{If } A \subseteq B, \text{ then } A \cup B = B$$ Can I write my proof as follows? Let A be a subset of B. $$
x \in A \cup B \\
\iff x \in A \vee x \in B \\
\implies x \in B \vee x \in B (\because x \in A \implies x \in B) \\
\iff x \in B \\
\therefore A \cup B \subseteq B \\
x \in B \\
\implies x \in B \vee x \in A \\
\iff x \in A \cup B \\
\therefore B \subseteq A \cup B \\
\therefore A \cup B = B
$$","['elementary-set-theory', 'proof-writing', 'logic']"
3509953,Are surjective sheaf morphisms locally surjective?,Let $\mathcal F$ and $\mathcal G$ be sheaves of groups of some topological space $X$ and let $\varphi : \mathcal F \to \mathcal G$ be a surjective morphism. That means that for each $x \in X$ the induced group morphism $\varphi_x : \mathcal F_x \to \mathcal G_x$ is surjective. Does this imply that for each $x\in X$ there exists an open $U \subseteq X$ with $x \in U$ such that $\varphi_U : \mathcal F(U) \to \mathcal G(U)$ is surjective? I don't think so but would be glad to see an easy counter example.,"['group-theory', 'sheaf-theory']"
