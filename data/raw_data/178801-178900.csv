question_id,title,body,tags
3240478,Are products of spheres $\prod_i\Bbb S^{n_i}$ different?,"If $(n_i)_{i\leq k}$ and $(n_i^\prime)_{i\leq k^\prime}$ are increasing (not necessarily strictly) sequences of non-zero integers, do we have the following? $$\prod_{i\leq k}\Bbb S^{n_i}\simeq \prod_{i\leq k^\prime}\Bbb S^{n_i^\prime}\iff (n_i)_{i\leq k}=(n_i^\prime)_{i\leq k^\prime}$$ Here I am being a bit vague about "" $\simeq$ "". This is because I am interested in two aspects of this problem. First aspect I am interested in (which has been answered): Is this known to be true if "" $\simeq$ "" denotes homotopy equivalence? If not, is this known to be true if "" $\simeq$ "" denotes homeomorphic spaces? I don't really know about higher homotopy groups and fiber bundles which I guess could be powerful tools to solve this, but feel free to use any of such tools, I don't mind having to do some research about these subjects. I am not really looking for a full proof, this is more for culture (a reference will do the job). Second aspect I am interested in (which hasn't been answered): Is this known to be true if "" $\simeq$ "" denotes diffeomorphic smooth manifolds, with canonical structures on spheres? For this last meaning of $\simeq$ , I am more interested in finding invariants from differential geometry to solve this in special cases (fixed dimension for example). Some necessary conditions: If $\prod_{i\leq k}\Bbb S^{n_i}$ and $\prod_{i\leq k^\prime}\Bbb S^{n_i^\prime}$ are diffeomorphic manifolds, then their dimension are equal: $$\sum_{i\leq k}n_i=\sum_{i\leq k^\prime}n_i^\prime$$ Also using fundamental group we have $$\left\lbrace i\leq k~\vert~n_i=1\right\rbrace=\left\lbrace i\leq k^\prime~\vert~n_i^\prime=1\right\rbrace.$$ An example of what I am interested in the smooth case: The two conditions above can help use to solve the problem when the dimension is $4$ . In this case we have $5$ spaces: $$\Bbb S^4,\quad\Bbb S^2\times \Bbb S^2,\quad\Bbb S^1\times\Bbb S^3,\quad\Bbb S^1\times \Bbb S^1\times \Bbb S^2,\quad\Bbb S^1\times \Bbb S^1\times\Bbb S^1\times \Bbb S^1.$$ Here fundamental group tells us that the last $3$ spaces are different from the other $4$ , and $\pi_4(\Bbb S^4)\approx\Bbb Z$ but $\pi_4(\Bbb S^2\times \Bbb S^2)\approx \pi_4(\Bbb S^2)\times \pi_4(\Bbb S^2)\approx \Bbb Z_2\times \Bbb Z_2$ so all these space are different. But this answer doesn't satisfy me that much. What I find very nice is for example to prove that $\Bbb S^2\times \Bbb S^2$ and $\Bbb S^1\times\Bbb S^3$ are not diffeomorphic by using parallelism as done here . Is there any other similar examples to distinguish products of spheres? One thing I thought about is the minimal dimension in which we can embed the product of spheres in euclidean space, but I am pretty sure this will fail (see here ). Maybe we can change ""euclidean space"" with something else?.. Let me know if the question is too vague and I'll try to modify it. Thanks in advance!","['general-topology', 'differential-topology', 'spheres', 'algebraic-topology']"
3240481,Compute the exterior product of $n$ copies of $\omega=dx_1 \wedge dx_2+\cdots+dx_{2n-1}\wedge dx_{2n}$,"Let $\omega$ be the $2$ -form in $\mathbb{R}^{2n}$ given by $$\omega=dx_1 \wedge dx_2+dx_3\wedge dx_4+\cdots dx_{2n-1}\wedge
 dx_{2n}$$ Compute the exterior product of $n$ copies of $\omega$ . (Chapter 1, Exercise 7 in Differential Forms and Applications by Manfredo P. do Carmo) The wording of the problem is kind of difficult for me to understand, but I suppose that the exercise asks us to calculate $\omega \wedge \cdots \wedge \omega$ . So, I think the only way one can get a non-zero coefficient is that if we permute the parentheses $(dx_1 \wedge dx_2)$ up to $(dx_{2n-1}\wedge dx_{2n})$ . There are $n!$ such permutations. Since each permutation can be arranged to $dx_1\wedge dx_2 \wedge dx_3 \wedge dx_4 \wedge \cdots \wedge dx_{2n-1}\wedge dx_{2n}$ with an even number of transpositions, we always get $+1$ as the coefficient of the differential form. So, the answer should be $$\omega\wedge\cdots\wedge\omega=\color{green}{n!}\,dx_1\wedge dx_2 \wedge dx_3\wedge dx_4 \wedge \cdots \wedge dx_{2n-1}\wedge dx_{2n}$$ Is that right? If I write my argument exactly like here, would it be considered a complete calculation and receive full points in an exam of differential manifolds?","['manifolds', 'multivariable-calculus', 'differential-forms']"
3240520,Prove the Hadamard product representation,"Let $A$ and $B$ be $m \times n$ matrices with low-rank structures: $$
A = U_{A}\Sigma_{A}V_{A}^{T},\quad B= U_{B}\Sigma_{B}V_{B}^{T},
$$ Prove that Hadamard product $A\circ B$ admits the following representation $$
A\circ B = (U_{A}^T\odot U_{B}^T)^T (\Sigma_{A}\otimes\Sigma_{B})(V_{A}^{T}\odot V_{B}^{T}),
$$ where $\odot$ represents the Khatri-Rao product, and $\otimes$ the Kronecker product.","['svd', 'hadamard-product', 'matrices', 'kronecker-product', 'matrix-decomposition']"
3240531,Partial fractions for any function?,"I recently discovered a property of polynomials that have all roots distinct. That is, if $a_i \neq a_j$ for $i \neq j$ , and if $$f(x) = \prod_{i=1}^{n} (x-a_i)$$ Then, $$\sum_{i=1}^{n} \frac {f(x)}{(x-a_i)f'(a_i)} = 1$$ Which can be proved by noting that the LHS is an $n-1$ degree polynomial and if it equals the RHS for $n$ distinct $x$ , it must be true always. But it's obviously true for the $n$ roots of $f(x)$ (in the limit as $x$ tends to $a_i$ ) This was useful because if I had the reciprocal of such a polynomial, I could easily break it into partial fractions like: $$\boxed{\frac {1}{f(x)} = \frac{1/ f'(a_1)}{x-a_1} +\frac{1/f'(a_2)}{x-a_2} + ... + \frac{1/f'(a_n)}{x-a_n}}$$ Now, my question is: Is this property satisfied by ANY function (apart from polynomials) that doesn't have repeated roots? (Continuous, differentiable)
For example, consider $f(x) = \tan x$ Then, $f(x)$ has roots at $x=k \pi$ for every integer $k$ And it doesn't have repeated roots, i.e., if $f(t)=0$ then $f'(t) \neq 0$ So, if we want to verify the equation in the box, it would be: $$\cot x = \sum_{n= -\infty}^{\infty} \frac {1}{x-n \pi}$$ Which is correct because it is the same as the well known $$\pi \cot \pi x - \frac {1}{x} = \sum_{n=1}^{\infty} \frac {2x}{x^2 - n^2}$$ So, would ANY continuous, differentiable function with no repeated roots satisfy this? If not, then what issues do I need to care about other than convergence?","['calculus', 'functions', 'partial-fractions', 'summation']"
3240532,Von Neumann Algebras and Convergence in the Strong$^\star$ Topology,"Let $\mathbb{A}$ be a von Neumann algebra, and let $f : \mathbb{A} \rightarrow \mathbb{A}$ be an ultra-continuous mapping (normal mapping) and let $(x_n)_{n\in \mathbb{N}}$ be a bounded sequence of $\mathbb{A}$ such that $x_n \rightarrow x$ in the strong $^\star$ topology. In this case, can it be inferred that the sequence $(f(x_n))_{n\in \mathbb{N}}$ is convergent in the strong $^\star$ topology to $f(x)$ ?","['von-neumann-algebras', 'functions', 'functional-analysis', 'operator-algebras']"
3240536,Convergence of Student's t-distribution to a standard normal,"I was looking at this question where it is shown that a Student's t-distribution converges to a standard normal distribution as the degrees of freedom tend to infinity.We start with the Student's t-distribution: $$f_T(t) = \frac{\Gamma\left(\frac{k+1}{2}\right)}{\sqrt{k\pi}\Gamma\left(\frac{k}{2}\right)}\left(1+\frac{t^2}{k}\right)^{-\frac{k+1}{2}}$$ for $t\in \mathbb{R}$ and where $k$ represent the degrees of freedom. Then let $k \to \infty$ so \begin{align}
\lim_{k \to \infty} f_T(t) &= \lim_{k \to \infty} \frac{\Gamma\left(\frac{k+1}{2}\right)}{\sqrt{k\pi}\Gamma\left(\frac{k}{2}\right)}\left(1+\frac{t^2}{k}\right)^{-\frac{k+1}{2}}\\
&= \lim_{k \to \infty}\frac{\Gamma\left(\frac{k+1}{2}\right)}{\sqrt{k\pi}\Gamma\left(\frac{k}{2}\right)}\cdot \lim_{k \to \infty}\left(1+\frac{t^2}{k}\right)^{-\frac{k+1}{2}}
\end{align} and then the answer suggests that using Stirlings approximation gets us that $$\lim_{k \to \infty}\frac{\Gamma\left(\frac{k+1}{2}\right)}{\sqrt{k\pi}\Gamma\left(\frac{k}{2}\right)}=\frac{1}{\sqrt{\pi}}\lim_{k \to \infty} \frac{\sqrt{k/2}}{\sqrt{k}} \tag{1}$$ I tried using the fact that, for big $k$ we have that $$\Gamma(k) \approx \sqrt{\frac{2 \pi}{k}}\left(\frac{k}{e} \right)^k $$ but simply couldn't make the algebra work. How can we see that $(1)$ is true? Any help is appreciated.","['proof-explanation', 'probability']"
3240601,Sum with Bernoulli numbers,"How to prove that: $$\sum_{k=0}^n \binom n k 2^k B_k = (2-2^n)B_n$$ In this sum, $B_n$ is the Bernoulli number with $B_1 = -\frac 1 2$ . Thanks for your attention!","['generating-functions', 'combinatorics', 'bernoulli-numbers', 'sequences-and-series']"
3240667,Intuition: Portmanteau-Theorem,An important theorem in probability theory about weak convergence of measures is the Portmanteau-Theorem . Why should it be true - intuitively - though? EDIT: Our version of Portmanteau's Theorem is: The following statements are equivalent $\mu_n {\to} \mu$ weakly $\int f \ d \mu_n \to \int f \ d\mu$ for all uniformly continuous and bounded $f:S \to \mathbb{R}$ $\limsup_{n \to \infty} \mu_n(F) \leq \mu(F)$ for all measurable closed subsets $\liminf_{n \to \infty} \mu_n(U) \geq \mu(U)$ for all measurable open subsets $U$ $\lim_{n \to \infty}\mu_n(A) = \mu(A)$ for all measurable $A$ with $\mu(\partial A) = 0$,"['measure-theory', 'probability-theory', 'weak-convergence']"
3240671,Decay of Fourier Transform of a Schwartz Function,"Suppose we have a function $f(x)\in \mathcal{S}(\mathbb R)$ ; that is, it is a function in Schwartz space . Further, suppose we know that $$|f(x)|\leq Ce^{-|x|}.$$ If it is helpful, we can actually replace the exponent on $|x|$ by any $1<c<2$ (in other words, it doesn't seem to be ""too far"" from a Gaussian). With this information, is there anything we can say about the decay of the Fourier transform of $f(x)$ beyond the fact that it is in the Schwartz class? In particular, does it necessarily decay like $f(x)$ , or could it decay much slower, say like $\exp(-(1+x^2)^c)$ for some arbitrarily small $c>0$ ? I've tried looking online and haven't found much. What I have found is: The Fourier transform is a linear isomorphism of the Schwartz space; in particular, we know that the Fourier transform is also in the Schwartz space The Gaussian, $g(x)=e^{-x^2}$ , is essentially a fixed point of this isomorphism (we introduce some constants, but the decay of the function and the decay of the transform is identical - since I'm only worried about the decay, I'm using the term ""fixed point"" a bit loosely). Some more information that might be helpful, though I couldn't find any way to use it specifically: $f(x)$ is essentially the characteristic function of a given random variable, which means that the Fourier transform is the corresponding density function. Specifically, this means the Fourier transform takes a maximum value at $0$ (which is equal to $1$ ) and decreases to $0$ as $|x|\to\infty$ . Even without anything specific, references would be appreciated. I've tried looking in Folland's book as well as Stein/Shakarchi's books, but these have not offered any insight for this problem.","['fourier-transform', 'fourier-analysis', 'reference-request', 'real-analysis']"
3240686,Bound on supremum of local martingale,"Let $M$ be a continuous local martingale starting at $0$ . How can I prove $$
P(\sup_{s\leq t}M_s>a,\langle M\rangle_t\le b)\leq 4\frac b{a^2}
$$ for all $a,b>0$ and $t>0$ ?","['local-martingales', 'martingales', 'inequality', 'quadratic-variation', 'probability-theory']"
3240720,Function space with $\|\cdot\|_g$ norm,"I'm reading some papers that utilize functional analysis. I came across the following definition: Let $\mathcal{X}$ be a metric space and let $g:\mathcal{X}\rightarrow\mathbb{R}$ be a continuous, strictly positive function. We define $C_g(\mathcal{X})$ to be the space of all continuous $f:\mathcal{X}\rightarrow \mathbb{R}$ such that $\|f\|_g<\infty$ , where $$\|f\|_g := \sup_{x\in\mathcal{X}} \,\frac{\vert f(x)\vert}{g(x)}.$$ I am looking for a reference that talks about this space in more detail. I couldn't find anything with a quick google search. (I'm not even sure if this space has a name.) Apparently $C_g(\mathcal{X})$ is a Banach space w.r.t. the norm $\|\cdot\|_g$ . But I would like to understand this space better.","['banach-spaces', 'general-topology', 'functional-analysis']"
3240733,Calculating how much light gets through steel mesh (commonly used to make cages),I have expanded steel mesh that I use to make garden cages: I would like to know how much sunlight the mesh lets through. I think I need to calculate the area of the mesh's negative space. And then from there I could calculate a percentage of sunlight that is let through . How can I calculate how much light would get through?,"['area', 'geometry', 'percentages']"
3240779,prove $\ln(1+x^2)\arctan x=-2\sum_{n=1}^\infty \frac{(-1)^n H_{2n}}{2n+1}x^{2n+1}$,I was able to prove the above identity using 1) Cauchy Product of Power series and 2) integration but the point of posting it here is to use it as a reference in our solutions. other approaches would be appreciated.,"['real-analysis', 'harmonic-numbers', 'calculus', 'closed-form', 'generating-functions']"
3240786,Existence of continuous $r(t)$ with $\lim_{t \to \infty} \frac{f(r(t))}{g(t)} = 1$,"Let $ \ f,g: \mathbb{R} \to \mathbb{R} \ $ be continuously differentiable functions such that $$\lim_{t \to \infty} f(t) = \infty = \lim_{t \to \infty} g(t) \ \ . $$ My question is: Is there a continuous function $ \ r: \mathbb{R} \to \mathbb{R} \ $ such that $$\lim_{t \to \infty} \frac{f \big( r(t) \big)}{g(t)} = 1 \ \ \ \ ? $$ I would like hints for a proof or a counterexample. You may feel free to modify the assumptions about $f$ and $g$ as you please. Thanks in advance.","['limits', 'calculus', 'analysis', 'real-analysis']"
3240789,Explanation of specific parts of Urysohn's Lemma proof,"While reading and trying to understand the proof I got stuck in some parts of the proof. Why can we take all the $V_{r_i}$ in such manner? Shall we say first w.l.o.g ?: Suppose $n\ge2$ and $V_{r_1},...,V_{r_n}$ have been chosen in such a manner that $r_i<r_j$ implies $\overline V_{r_j}\subset V_{r_i}$ . How can we know that $\overline V_j$ are all compact sets? We need this hypothesis so that Theorem 2.7 can be applied. Why is it enought to mention that $f$ is lower-semicontinuous and $g$ is upper-semicontinuous to have continuity of $f$ ? The author wrote it's clear that $f$ has it's support in $\overline V_0$ . Actually we need the support to be in $V$ ; by (2) it's done. Here is how I proved, please verify. Let's see that $\{x:f(x)\neq 0\}\subset V_0$ (we then take closures on both sides). Let $x\in\{x:f(x)\neq 0\}$ and let´s see that $x\in V_0.$ We have $f(x)\neq 0$ . By definition $f=\sup_r f_r.$ Thus $\sup_r f_r\neq 0$ . Now we must have $f_r=r$ if $x\in V_r$ otherwise $f_r=0,\sup_r f_r=0=f$ , contradiction. If $x\in V_r,f_r=r$ . We have $r>0,$ thus $\overline V_r\subset V_0.$ Thus $x\in V_0$ Hence $\{x:f(x)\neq 0\}\subset V_0$ . Taking closures, we have the result.","['proof-explanation', 'general-topology', 'analysis', 'real-analysis']"
3240805,Random Walk $\mathbb P(T_0>n $ and $S_n=a) = \mathbb P(T_a=n) =\frac{a}{n} \mathbb P(S_n=a)$,"Consider the random Walk $S_n$ on $\mathbb Z$ starting in $x=0$ . Let $a\in \mathbb Z$ . Define $T_a(\omega)=\min\{n\in \mathbb N : S_n(\omega)=a\}$ . Show for $a> 0$ $\mathbb P(T_0>n $ and $S_n=a) = \mathbb P(T_a=n) =\frac{a}{n} \mathbb P(S_n=a)$ I tried looking the different possible paths that lead to the different outcomes i.e. $T_0>n $ and $S_n=a$ or $T_a=n$ and it seems reasonable but in no way am I able to rigorously proof this. 
I am sorry if this is something very basic to show but when I googled ""random walk"" or similar terms I only found more complicated models and nothing similar to this particular statement.","['random-walk', 'probability-theory', 'probability']"
3240832,Geometric meaning of second Covariant Derivative,"This other question exists, but it doesn't answer my question: Geometric interpretation of the second covariant derivative I know the Riemann Tensor can be written as the commutator of the second covariant derivative (assuming the connection is torsion-free): $$R(u,v)w = \nabla_{u,v}^2 w - \nabla_{v,u}^2 w$$ where $\nabla_{u,v}^2 w  = \nabla_u \nabla_v w - \nabla_{\nabla_u v}w$ is the ""second covariant derivative"". What I'm missing here is the ""why"". I don't understand the geometrical meaning of $\nabla_{u,v}^2 w$ , or why anyone bothered to invent this expression. How was the formula for the ""second covariant derivative"" invented, and what is its meaning? I tried drawing some diagrams showing how the vectors are positioned, but they did not bring me much insight. The other question has better diagrams in the answers. Here is a visualization of $\nabla_u \nabla_v w$ : Here is a visualization of $\nabla_{\nabla_u v}w$ :","['covariance', 'tensors', 'derivatives', 'differential-geometry']"
3240888,Limit with integral - L'Hospital's rule,Find $$ \lim_{x \rightarrow 0^+} \frac{\int_{0}^{x^2} (1 + \sin(t))^{1/t} dt}{x \sin(x)} $$ Let $$ F(t) \mbox{ such that } F'(t) = (1 + \sin(t))^{1/t}  $$ Then we use L'Hospital's rule: $$ \lim_{x \rightarrow 0^+} \frac{\int_{0}^{x^2} (1 + \sin(t))^{1/t} dt}{x \sin(x)} = \frac{F'(x^2) - F'(0) }{\sin(x) + x \cos(x)} $$ but $$ F'(0) $$ is not defined (we have $1/t$ part as exponent),"['integration', 'limits']"
3240925,Proof that the matrix multiplication is associative – is commutativity of the elements necessary?,"This is a picture of the proof, we assume that the elements of the matrix are elements of a ring: I don't know how the associativity is proved here without using commutativity. I have changed the notation myself in order to understand the proof better: $$d_{ji}=(a_{j1}b_{11}+...+a_{jn}b_{n1})c_{1i}+...+(a_{j1}b_{1l}+...+a_{jn}b_{nl})c_{li}$$ is because of distributivity $$(a_{j1}b_{11}c_{1i}+...+a_{jn}b_{n1}c_{1i})+...+(a_{j1}b_{1l}c_{li}+...+a_{jn}b_{nl}c_{li})$$ which is because of associativity the same as $$a_{j1}b_{11}c_{1i}+...+a_{jn}b_{n1}c_{1i}+...+a_{j1}b_{1l}c_{li}+...+a_{jn}b_{nl}c_{li}\tag{*}$$ which means I can put the parenthesis where I want. I want to show that this is equal to: $a_{j1}(b_{11}c_{1i}+...+b_{1l}c_{li})+...+a_{jn}(b_{n1}c_{1i}+...+b_{nl}c_{li})$ . However, because of distributivity and associativity, this is equal to $$a_{j1}b_{11}c_{1i}+...+a_{j1}b_{1l}c_{li}+...+a_{jn}b_{n1}c_{1i}+...+a_{jn}b_{nl}c_{li}\tag{**}$$ The array $(*)$ has a different order than the array $(**)$ . Therefore the commutativity was used but the proof says only associativity and distributivity is used. Is there a mistake in my reasoning or is commutativity unnecessary? EDIT Definiton of matrixmultiplication:","['matrices', 'linear-algebra']"
3240948,Are there equations which have solutions in all groups but which are not algebraicly solvable,"I am not sure exactly how to phrase this problem so I appologise if it is not clear, also this is somewhat long but I wanted to explain exactly where I was with the problem. If you have any questions feel free to ask. Description of Problem Given a set of variables $\{x,y,z,...\}$ and a variable $o$ is it possible to define a finite product of these variables and their inverses $\sigma(x,y,z,...,x^{-1},y^{-1},z^{-1},...,o,o^{-1})$ (i.e. a finite sequence made up of these variables and their inverses) such that; 1) For any group $G$ and any assignment of values from $G$ to $\{x,y,z,...\}$ there exists a unique element $g$ of $G$ such that if $o$ is set to $g$ ; $\sigma(x,y,z,...,x^{-1},y^{-1},z^{-1},...,o,o^{-1})=1$ and 2) There does not exist a finite product $\gamma(x,y,z,...,x^{-1},y^{-1},z^{-1},...)$ such that; $o=\gamma(x,y,z,...,x^{-1},y^{-1},z^{-1},...)$ for all groups $G$ Rough explaination as to why I am asking here My intuition is no but I am unsure how to prove this. There are clearly examples of equations like these solvable in all groups (i.e. $xo=1$ ) but these have algebraic solutions (in that example $o=x^{-1}$ ) and there are examples of these equations which are solvable in wide classes of groups (i.e. $o^{n!+1}x=1$ is solvable in any group of order less than $n$ with $o=x^{-1}$ ) but these are not solvable in all groups. In addition some equations are solvable in all groups but not uniquely (i.e. $o^2=1$ has many solutions in groups with elements of order 2 but can always be solved with $o=1$ ) Progress on proof (or proof of falsehood) It can be shown that $\sigma$ must contain exactly $\pm1$ total occurences of $o$ (where $o^{-1}$ counts as $-1$ occurence of o) using the following argument. If $G$ is abelian then $\sigma$ can be written as $Ao^n$ for some $A$ which is a product of the other variables. For this to be solvable $o^n=A^{-1}$ must be solvable in every abelian group. If $A=1$ $o$ is not uniquely defined for groups of order $|n|$ . If $A\neq1$ and $|n|\neq1$ then $o$ is not defined for groups of order $|n|$ or $n=0$ and so $o$ is not unique. Therefore $|n|=1$ and so the total number of occurences of $o$ in $\sigma$ must be $\pm1$ . In addition it is clear that there must be an odd number of occurences of $o$ greater than $1$ (this time counting $o^{-1}$ as $1$ occurence). This follows as otherwise there is a clear definition of $\gamma$ (if there is $1$ occurence) or the observation above is violated (if there are an even number of occurences). This is where I am and I am not sure how to proceed. Appologies again for this being overly long. Any information or advice would be appreciated.",['group-theory']
3240954,Why can't we assume $0 < |x - a| \leq \delta$ (equality) in an epsilon delta proof?,"Suppose we want to prove that $$ \lim_{x \to a} f(x) = L,$$ that is $$ \forall \epsilon > 0, \exists \delta > 0, \forall x \in \mathbb{R}, 0 < |x - a | < \delta \Longrightarrow |f(x) - L| < \epsilon.$$ Why are we constraining $|x - a|$ to be less than $\delta$ ? What happens if $0 < |x - a | \leq \delta$ , or more specifically, $0 < |x - a | = \delta$ ? Could we prove then that $|f(x) - L| \leq \epsilon$ , or is there some other problem?","['limits', 'calculus', 'epsilon-delta']"
3240987,Is showing that $x_n \rightarrow x_0\Rightarrow f(x_n) \rightarrow f(x_0)$ for a single sequence enough to prove continuity?,"For all my homework in real analysis, when I've been asked to show that a function is continuous, I just found a single $x_n \in D$ and showed that when $x_n \rightarrow x_0$ , $f(x_n) \rightarrow f(x_0)$ . Apparently, the sequence definition (as opposed to the epsilon delta definition) is (basically) only used to prove a function is not continuous, and I can't prove a function is continuous because then I'd have to show this is true for all possible sequences? Am I doing the math wrongly? Should I always use the epsilon delta definition when trying to prove that a function is continuous?","['continuity', 'real-analysis']"
3241008,"$f(x) = \frac{4 + x}{2 + x - x^2}$, calculate $f^{(9)}(1)$","$f(x) = \frac{4 + x}{2 + x - x^2}$ , calculate $f^{(9)}(1)$ , where $f^{(9)}$ is the $9$ -th derivative of $f$ . Domain of $f$ is $\mathbb{R} - \{-1, 2\}$ . I've got that $f(x) = \frac{1}{1 - (-x)} + \frac{1}{1 - \frac{1}{2}x} = \sum_{n=0}^\infty ((-1)^n + 2^{-n})x^n$ , but there is a problem that $\frac{1}{1 - (-x)} = \sum_{n=0}^\infty (-1)^nx^n$ is convergent only for $|x| < 1$ , so not for $1$ . How can I go about this?","['derivatives', 'taylor-expansion', 'partial-fractions', 'sequences-and-series']"
3241013,Find the critical region and the power when $H_0$ is false.,"In a sample $N(0, \sigma ^2)$ we have two hypothesis. $H_0: \sigma ^2 =16$ and $H_1: \sigma ^2 =4$ (a )For a sample of size n, find the form of the best critical region. (b) If  n = 10 and $\alpha = 0.10$ , find the critical region  and the power when $H_0$ is false. (c) Whith the next sample: 0.05, 1.58, 1.41, -0.01, -0.40, -1.39, -1.78, 1.03, 1.27, 0.23; ¿Which hypothesis should be accepted? a) I use the Neyman-Pearson Lemma: $$\frac{L(16)}{L(4)} =\frac{\prod_{i=0}^n 1/\sqrt{32\pi} e^{\frac{-x_i^2}{32}}}{\prod_{i=0}^n 1/\sqrt{8\pi } e^{\frac{-x_i^2}{8}}} \le K$$ $$\iff \sum_{i=0}^n{X_i^2 \le c}$$ then $\gamma: $ Reject $H_0$ if $ \sum_{i=0}^n{X_i^2 \le c}$ thus the critical region is $C^*=\{(X_1,...,X_n) |  \sum_{i=0}^n {X_i^2} \le c \}$ b)I think I should find the value of c using $P($ Reject $ H_0 | H_a)$ and $\pi_\gamma (4) $ Is this correct? In this case: $P($ Reject $ H_0 | H_a) =P(\sum_{i=0}^n {X_i^2} \le c | \sigma^2=4)$ . Since $X_i^2=4(\frac{X_i}{2})^2 \sim$ Gamma $(1/2, 8)$ then $\sum_{i=0}^n {X_i^2} \sim$ Gamma $(5,8)$ .  If $\alpha=.1 $ , c is the value of the quantile .1 c) $P($ Reject $ H_0 | H_0) =P(\sum_{i=0}^n {X_i^2} \le c | \sigma^2=16)$ using the same argument in b) I get: $\sum_{i=0}^n {X_i^2} \sim$ Gamma $(5,32)$ and  c=77.84, using the values $\sum_{i=0}^n {X_i^2}=12.47$ then $H_0 $ is rejected","['statistical-inference', 'statistics', 'estimation', 'parameter-estimation', 'hypothesis-testing']"
3241038,Solution of $ty'' +(2t+3)y' +(t+3)y = 3e^{-t}$ via Laplace transform,"A recent question which was put on hold due to lack of context by the OP was the following: Solve the following ODE using Laplace transforms. $$ty'' +(2t+3)y' +(t+3)y = 3e^{-t}, \qquad y(0)=0$$ Putting the equation into the form $$t(y^{\prime\prime}+2y^\prime+y)+3(y^\prime+y)=3e^{-t} $$ which has $y_c=ce^{-t}$ as a solution of its complementary equation immediately suggests $y=Ate^{-t}$ as a particular solution.
And this is borne out by substitution, with $A=1$ . Applying the initial condition yields the solution $$ y=te^{-t} $$ So why would the original OP want the equation solved using Laplace transforms? Is there a shorter path using Laplace transforms than the following? Use the fact that $(3e^{-t})^\prime+3e^{-t}=0$ to get the homogeneous equation $$ [t(y^{\prime\prime}+2y^\prime+y)+3(y^\prime+y)]^\prime+[t(y^{\prime\prime}+2y^\prime+y)+3(y^\prime+y)]=0 $$ This simplifies to the homogeneous equation $$ 
t(y^{\prime\prime\prime}+3y^{\prime\prime}+3y^\prime+y)+4(y^{\prime\prime}+2y^\prime+y)=0 $$ Taking the Laplace transform of this involves quite a bit of tedium which I will spare the reader, but yields the following: \begin{eqnarray}  
   (s+1)^3Y^\prime+2(s+1)^2Y&=&0\\\  
   (s+1)^2Y^\prime+2(s+1)Y&=&0\\\  
   \left[(s+1)^2Y\right]^\prime&=&0\\\  
   Y&=&\frac{c}{(s+1)^2}\\\  
   y&=&cte^{-t}  
   \end{eqnarray} So, with $c=1$ , yielding the same solution found much more easily by inspection.","['laplace-transform', 'ordinary-differential-equations']"
3241074,"If $\int_a ^b f df=0$ and f is continuous, then f is the function constant $0$","I have been studied some properties of Riemann Stieltjes integral, and i found this: If $\int_a ^b f df=0$ and f is continuous for every $a<b$ in $R$ , then f is the function constant $0$ without proof I know, by definition $\int_a ^b f df = \sum_{i=0} ^{n} f(\lambda_i)(f(x_{i})-f(x_{i-1}))$ and if $f$ is constant, is clear $(f(x_{i})-f(x_{i-1}))=0$ for every $i$ but why $f$ must be a constant zero?","['integration', 'analysis']"
3241082,"In “An element of a set can never be a subset of itself”, what does ‘itself’ stand for?","I have just begun learning about sets. My first language isn't English. I'm in high school. Here's an example problem I found in my textbook: Example 11: Let $A, B$ and $C$ be three sets. If $A∈B$ and $B⊂C$ , is it true that $A⊂C$ ? If not, give an example. Solution: No. Let $A=\{1\}, B=\{\{1\}, 2\}$ and $C=\{\{1\}, 2, 3\}$ . Here $A∈B$ as $A=\{1\}$ and $B⊂C$ . But $A⊄C$ as $1∈A$ and $1∉C$ . Note that an element of a set can never be a subset of itself. The link to the textbook's chapter. What does “itself” stand for here? Does it mean an element of a set can't be it's own (the element's) subset? Or does that mean an element cannot be both an element and a subset of a set at the same time? If $P=\{p\}, Q=\{\{p\}, q\}$ , and $R=\{\{p\}, q, r\}$ , we can say that $P∈Q$ . But, can we say that both $Q∈R$ and $Q⊂R$ are true? Is it so that $Q$ cannot be both an element and a subset of $R$ ? Is $\{\{p\}, q, r\}$ not the same as $\{p, q, r\}$ ?",['elementary-set-theory']
3241170,Lagrange multipliers - confused about when the constraint set has boundary points that need to be considered,"Consider the constraint $$S_1 = \{(x, y) \; |\; \sqrt{x} + \sqrt{y} = 1 \}$$ How to use Lagrange Multipliers, when the constraint surface has a boundary? In this case, after the Lagrange multiplier method gives candidates for maxima/minima, we need to check the ""boundary points"" of $S_1$ , namely, $(1,0)$ and $(0,1)$ to get the global max/min. I can see that these two are ""boundary points"" intuitively when I plot the curve. However, instead if the constraint set be $$S_2 = \{ (x, y) \; |\; x^2 + y^2 = 1\},$$ then in this question, one answer states that  for this constraint set, there is no ""boundary point"". Constrained Extrema: How to find end points of multivariable functions for global extrema The only difference I see is that  pictorially, one is a closed curve, but the other is not. However, I am unable to see what is the mathematical definition that will  allow me to conclude that $S_1$ has boundary points $(0, 1)$ and $(1,0)$ and $S_2$ has none? Q) What is the definition of ""end point"" or ""boundary point"" being used here that explains both $S_1$ , $S_2$ .","['optimization', 'multivariable-calculus', 'lagrange-multiplier', 'constraints']"
3241183,Question regarding surjective mapping,"I have come across a question while solving practice papers on the topic ' Functions '. The question is as follows - If $f : X \to Y $ , find $f (X)$ , when $f $ is a surjective or onto mapping. Here $X $ and $Y $ are non-empty sets Here is my approach - As $f $ is a surjective mapping of $X$ to $Y$ , then for each $y \in Y $ , there exists an $x \in X $ , such that $f (x) = y $ . Thus $f (X) = Y$ . Can I be provided with a more formal proof ? Suggestions for correction in my answer and a detailed answer with explanation would be helpful.","['elementary-set-theory', 'functions', 'discrete-mathematics']"
3241199,Calculating the volume with double integral,"Hello i am trying to calculate the volume for a double integral but i am having problem with define the integral because it is not given in a pure form. I have $z = xy$ , $x+y+z=1$ $z=0$ my approach is to set the function for a integral to be $$\int_Dxy$$ and to find the $limits for$ $dy$ i set $z$ to be zero it is also given by definition and i get $y = 1-x$ after that i set both $z$ and $y$ to zero and i get $x = 1$ so i have the following limits $$\int_0^1 \int_0^{x-1}xy$$ but i am not getting the right answer after evaluating the integral. What confusing me here is that the integral is not given by default here also the other thing that confuses me is i have the same problem but to be solved with triple integral. I am thinking maybe for the volume i just need $dydx$ without a function but i am not sure. Thank you for any help in advance.","['integration', 'multivariable-calculus', 'calculus', 'volume']"
3241232,For which distribution same pdf is generated for given random variable,"For which of the distribution same pdf is generated for random variable X and 1/X. 
  Is it F(2,2)",['statistics']
3241233,Solve for x : $\sqrt{2}\sin(x)+\sqrt{6}\cos(x) = \sqrt{3} +1$,Solve $\sqrt{2}\sin(x)+\sqrt{6}\cos(x) = \sqrt{3} +1$ for $x$ I started by multiplying both sides of the equation by $\frac{1}{2\sqrt{2}}$ to obtain $$\displaystyle\frac{\sin(x)}{2}+\frac{\sqrt{3}\cos(x)}{2} = \frac{\sqrt{3} +1}{2\sqrt{2}}$$ $$\iff \sin(60+x) = \frac{\sqrt{3} +1}{2\sqrt{2}}$$ I am stuck here. Any hints on solving the R.H.S will be appreciated.,"['trigonometry', 'systems-of-equations']"
3241237,"What is the maximal number of subsets of a finite set, such that no one is the unions of some other ones? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Suppose $S$ is a set with $n$ elements.  What is the maximal value of $m$ such that there exists a collection $\{A_k\}_{k=1,\dots m}$ of subsets of $S$ such that each $A_i$ is not the union of some other elements of $\{A_k\}_{k=1,\dots m}$ ? Is there a elegant formula for this number?",['combinatorics']
3241265,How to prove $\sum_{i=1}^{p-1}\sqrt[p]{\frac{i}{p}} $ is an algebraic integer?,"When I read an introductory algebraic number theory textbook, there is a problem like this: If $p$ is an odd prime, prove $\displaystyle\sum_{i=1}^{p-1}\sqrt[p]{\frac{i}{p}} $ is an algebriac integer. Note that it is in the first chapter, so I think that it does not require much knowledge, but I have no idea how to solve it.","['number-theory', 'algebraic-number-theory', 'prime-numbers']"
3241309,What is the intuition behind uniform continuity?,"There’s another post asking for the motivation behind uniform continuity. I’m not a huge fan of it since the top-rated comment spoke about local and global interactions of information, and frankly I just did not get it. Playing with the definition, I want to say uniform continuity implies there’s a maximum “average rate of change”. Not literally a derivative, but the rate of change between two points is bounded in the domain. I’m aware that this is essentially Lipschitz continuity, and that Lipschitz implies uniform. This implies there’s more to uniform continuity than just having a bounded average rate of change. And also, how is it that $ f(x)=x$ is uniform yet $f(x)f(x)=g(x)=x^2$ is not? I understand why it isn’t, I can prove it. But I just don’t understand the motivation and importance of uniform continuity.","['lipschitz-functions', 'uniform-continuity', 'intuition', 'real-analysis']"
3241363,Combinatorial proof of $\sum_{i = 0}^{n} \binom{i}{r - 1} = \binom{n + 1}{r}$ and then use result to find a formula for $1^2 + 2^2 + \ldots + n^2$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question a) Give a combinatorial proof that for every $n \geq r \geq 1$ that: $$\sum_{i = 0}^{n} \binom{i}{r - 1} = \binom{n + 1}{r}$$ And use (a) to concoct a formula for $1^2 + 2^2 + ... + n^2$","['combinatorics', 'discrete-mathematics']"
3241370,Circumcircle bisects the segment connecting the vertices of two regular even-sided polygons,"For any two unequal even-sided regular polygons, the circumcircle around them bisects the segment connecting the vertices of the two polygons. Here are some images illustrating the question: For any two even-sided regular polygons with equal number of sides, I observed that the following relationship always hold: $$CO=C_1O$$ Just for clarification, we can construct a circle with three points. In the above cases, the three circumcircles are formed by three-point pairs $(D,A,D_1),~(E,F,E_1,),~(F,G,F_1)$ respectively. Point $O$ is where the circumcircle intercepts the segment connecting the vertices of the polygons.
I have stumbled on this problem for a while, and couldn't figure out how to prove it. Any hints would be appreciated.",['geometry']
3241398,Solving an ODE: closed forms of $x(t)$ and $y(t)$,"I am given the following system. \begin{split}
\overset{.}{x}&= y\\
\overset{.}{y}&= k(1-y^2)^{3/2}e^{-x}
\end{split} writing $dx= \overset{.}{x}dt$ and $dy= \overset{.}{y}dt$ we arrive at $$\frac{ydy}{(1-y^2)^{3/2}}= ke^{-x}dx$$ Integrating both sides yields $$(1-y^2)^{-1/2}= ke^{-x}+c.$$ How do I get close forms of $x(t)$ and $y(t)$ in terme of $t$ ?","['calculus', 'analysis', 'ordinary-differential-equations', 'real-analysis']"
3241426,Properties of a derivative on a compact interval,"Suppose a function $F$ is differentiable on an interval $(a,b) \supset [0,1]$ . Denote its derivative by $f$ , and suppose that $f > 0$ on $[0,1]$ . Question 1: Is it true that $f$ can be bounded away from $0$ on $[0,1]$ , i.e. that there exists some $c > 0$ such that $f(x) > c$ for all $x \in [0,1]$ ? If $f$ is continuous, this is clearly true, as a continuous function attains its minimum on a compact set, and this minimum is $> 0$ by assumption. If $f$ were an arbitrary function (not a derivative), this is clearly false; for instance, consider the function $f(x) = 1$ when $x = 0$ and $f(x) = x$ elsewhere. But this function has a jump discontinuity, and therefore is not the derivative of any function. Question 2: Is it true that $f$ is bounded on $[0,1]$ ? Note that if we remove the $f > 0$ requirement, this is not true (for instance, consider $F(x) = x^2 \sin(1/x^2)$ , with $F(0) = 0$ ; $F$ is differentiable, so $f$ exists, but $f$ is not bounded). This question may be relevant, but it doesn't directly answer the above.","['examples-counterexamples', 'real-analysis', 'calculus', 'upper-lower-bounds', 'derivatives']"
3241435,$xf$ smooth $\Rightarrow$ $f$ smooth?,"Question. Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous function such that $x\mapsto xf(x)$ is smooth. Is $f$ necessarily smooth? Here are some thoughts on the problem: Clearly $f$ is smooth on $\mathbb{R}\backslash 0$ so one only needs to show that the derivatives $f^{(j)}(x)$ (for $x\neq 0$ ) have the same limit as $x\rightarrow 0$ from left and right. If $xf$ is analytic, then also $f$ is analytic. This is clear as its power series around $0$ is obtained from the one of $xf$ by factoring out one $x$ . For smooth functions one would need to know something about the regularity of $x\mapsto x^{-1}R_kf(x)$ , where $R_kf$ is the remainder of the order $k$ Taylor expansion. Showing smoothness of this is essentially equivalent to the original question. One can without loss of generality assume that $f$ has compact support. Then the Fourier transform satisfies $\hat f\in C_0$ and $\frac{d}{d\xi}\hat f \in \mathcal{S}$ (Schwartz functions). If this would imply that $\hat f\in \mathcal{S}$ , then the question would be solved.","['calculus', 'real-analysis']"
3241460,A characterization of Weak Convergence in $L^p$ spaces,"I'm working on the following problem, I'm having trouble with the reverse direction. My question is bolded below. Also could someone check my forward direction?: Let $(X, \mathcal{M}, \mu)$ be a $\sigma$ finite measure space and $\{f_n\},f \in L^P(X)$ . Prove that $f_n \rightharpoonup f$ in $L^p(X)$ iff $\|f_n\|_p \leq c$ for all $n$ and $\int_A f_n\, d\mu \rightarrow \int_A f \, d\mu$ for all $A$ with $\mu(A) < \infty$ . For the reverse direction, we can use the characteristic functions in $L^q$ to build arbitrary functions in $L^q$ and use Monotone Convergence on $A$ equals a ball. Then increase the radius of the ball at each step making error $\epsilon/2^n$ . However, I'm having trouble seeing how I use the boundedness of the sequence $f_n$ ) (For the forward direction, choosing $\chi_{A}\in L^q(X)$ will get the integral condition and the $\|f_n\|_p$ were bounded because the sequence originally lived in $L^p(X)$ .)","['weak-convergence', 'proof-verification', 'real-analysis', 'lp-spaces', 'functional-analysis']"
3241503,How to calculate the shape of a curve given y coordinates and slope?,"I apologise in advance if my description of the problem does not use the correct terminology but I'm still learning! Let me know if something is ambiguous or not clear and I'll try to rephrase it. The Problem: I would like to draw a curve/figure out the shape of a curve given the information in the following graph: On the y axis, I'm showing the slope of my curve, and on the x axis I'm showing the height/y-coordinate of each point in my curve. I am missing information about the x-coordinates of my curve. In this case, because the graph I have shown above is quite simple (a straight line indicating ever increasing slope), I can intuitively understand that my curve will have a shape similar to this: However, I don't know exactly how to go about deducing this shape in the case of more complex curves, or even how to go about figuring out the shape of the curve I just showed you in a principled manner. Importantly, I am also happy to make the assumption that a slope of 1 equals a change of one unit in the x axis of the graph where I'll draw my curve (I'm more interested in the shape of the curve, and it's shape relative to other curves deduced using the same method, than in the absolute value of the x axis coordinates). It just seems like this is the kind of problem that might have already been solved by someone, but I don't really know how to search for the answer since I don't know what keywords to use. In the example above, the graph telling me information about the curve showed a line y = 2x. However, I might have a graph whose information is not given by a simple mathematical equation (imagine a local regression curve, or a complicated spline). Many thanks in advance.","['curves', 'ordinary-differential-equations']"
3241528,Reference request on the Riemann Zeta function,"Let $\zeta^{-1}(x)$ be the functional inverse of the Riemann zeta function so that $$\zeta(s) = x \implies \zeta^{-1}(x) = s$$ The Riemann zeta function is injective in $1 < s < \infty$ where $1 < \zeta(s) < \infty$ . Hence in this interval $\zeta(s) = x$ has exactly one solution so we can ask for an asymptotic expansion of $\zeta^{-1}(x)$ ? Question : What is known about the function $\zeta^{-1}(x)$ as $x \to \infty$ ? Is there any asymptotic expansion in literature or any other information. Examples of things I am looking for : @GEdgar's comment suggests that the question may not be clear to some hence I am giving some examples. I was working with inverse of the zeta function in real $x > 1$ . For every $x > 1$ we can show using the Stieltjes series expansion of the Riemann zeta function about $s = 1$ that there exists a constant $0 < c_x < 1-\gamma$ such that $\zeta(1 + \frac{1}{x-1+c_x}) = x$ hence a first approximation is $\zeta^{-1}(x) = 1 + \frac{1}{x-\gamma} + O(\frac{1}{x^2})$ . This is sufficient to yield simple results like $$ \sum_{n \le x} \zeta^{-1}(n) = x + \log x -0.9267119... + O\Big(\frac{1}{x}\Big) $$ $$ \sum_{p \le x} \zeta^{-1}(p) = \pi(x) + \log\log x + 0.6332... + O\Big(\frac{1}{\log x}\Big) $$ where I have computed the values of their constants after proving their existence. In this example the true value is in finding a complete asymptotic for $\zeta^{-1}(x)$ i.e. what will be the general term of it asymptotic expansion? So in general, I would like reference to any work done on the inverse of the Riemann Zeta function.","['number-theory', 'reference-request', 'real-analysis', 'riemann-zeta', 'prime-numbers']"
3241551,Why do some books define $\cot x$ as reciprocal of $\tan x$?,"I believe $\cot x$ should be defined as $ \dfrac{\cos x}{\sin x}$ and not as $\dfrac{1}{\tan x}$ Because $\cot x$ and $\dfrac{1}{\tan x}$ aren't even the same function,  they have different domains. So for instance we know $\cot (π/2) = 0$ but $\dfrac{1}{ \tan (π/2)}$ is not even defined. Am I correct in believing this?",['trigonometry']
3241554,"Is $(1+a)^{\frac{1+a}{a+x}}(1+x)^{\frac{1+x}{a+x}}$ concave in $x$ in the support $[0,a]$?","I am studying the following function, defined in the support $[0,a]$ : $f(x)=(1+a)^{\frac{1+a}{a+x}}(1+x)^{\frac{1+x}{a+x}}$ In particular I would like to prove that the function $f(x)$ is concave in $x$ , so I just need to prove that the function has only a critical point, but I cannot find it (or them) in any analytical way.",['functions']
3241564,What are some neat ways to differentiate $\arctan(\frac yx)$?,"By choosing the branch cut to be the line $\gamma=\{it\in \Bbb C: t\le 0\}$ , we can write the complex logarithm as $$\text{Log}(z) = \log(|z|) + i\arg(z),$$ where $\arg(z) \in (-\pi,\pi].$ As a function of $x$ and $y$ (in $z=x+iy)$ , we write $$
\arg(z) = \arctan\left(\frac yx\right).
$$ How do we rigorously justify the following equalities $$\begin{align}
\partial_x \left(\arctan\left(\frac yx\right)\right) &= \frac{-y}{x^2+y^2}, \\
\partial_y \left(\arctan\left(\frac yx\right)\right) &= \frac{x}{x^2+y^2}?
\end{align}$$ I usually see the above expressions come up when one want to show that the Cauchy-Riemann equations holds for $f(z) = \text{Log}(z)$ . We can blindly apply the chain rule the get the answer but that is not rigorous because for $x=0$ the fraction $\frac yx$ is undefined. To make it rigorous, we can rather write $\arg(z) = A(x,y)$ where $$
A(x,y) = \begin{cases} \arctan\left(\frac yx\right) &; x\ne 0 \\
\pi/2 &;x=0, y>0 \\
-\pi/2 &; x=0, y<0
\end{cases}
$$ and compute the derivatives of this function carefully case by case instead but I find this method to be troublesome. Is there a neat way to get the result without having to consider $x=0$ as a separated case?","['complex-analysis', 'multivariable-calculus', 'complex-numbers', 'real-analysis']"
3241651,Relationship between derivative of a function at two points and the function itself.,"Suppose I am given a function $u$ defined in an interval $[a,b]$ . Suppose I know $u'(a)$ , $u'(b)$ and $u((a+b)/2)$ . Is it possible to determine the values of $u(a)$ and $u(b)$ using the given information. Note: In the solution of this exercise it states that $$u(a)=u((a+b)/2)+(a-b)(u'(a)+u'(b))/4$$ and similarly $$u(b)=u((a+b)/2)-(a-b)(u'(a)+u'(b))/4$$ How exactly was this derived? Is there an underlying theorem that was used and I am missing?","['derivatives', 'analysis']"
3241671,Transform the partial differential equation with new independent variables,"Transform the following equation to new independent variables $u = x$ , $v = x^2+y^2$ $$ y\frac{\partial z}{\partial x} - x \frac{\partial z}{\partial y} = 0  $$ Notce you don't have to actually solve the pde, the problem is to transform it. The official answer is $ \frac{\partial z}{\partial u} = 0 $ My work: $$ y\frac{\partial z}{\partial x} = x\frac{\partial z}{\partial y} \longrightarrow \frac{1}{x}\frac{\partial z}{\partial x} = \frac{1}{y}\frac{\partial z}{\partial y}  $$ $$ \frac{\partial z}{\partial y} = \frac{\partial z}{\partial v}\frac{\partial v}{\partial y} \longrightarrow \frac{\partial z}{\partial y} = \frac{\partial z}{\partial v}2y $$ Substituting, you get $$ \frac{1}{x} \frac{\partial z}{\partial x}  = 2\frac{\partial z}{\partial v}    $$ By the same procedure you can get $$ \frac{\partial z}{\partial x} = \frac{\partial z}{\partial u} $$ Finally, I got $$ \frac{1}{u}\frac{\partial z}{\partial u} = 2\frac{\partial z}{\partial v} $$ Which is obviously not the official answer. Am I doing something wrong and if not, how do I finish the problem? Thanks.","['multivariable-calculus', 'partial-differential-equations']"
3241694,"Prove that the excenter of $A$ in $\triangle ABC$, the midpoint of $BC$ and $H$ are collinear.","$D$ is the incenter of $\triangle ABC$ . $DE \perp BC$ ( $E \in BC$ ). $AE \cap \bigcirc(A, B, C) = F$ ( $F \not\equiv A$ ). $G$ is the midpoint of the larger arc of $BC$ . $GF \cap \bigcirc(B, C, D) = {H}$ ( $GH < GF$ ). Prove that the excenter of $A$ in $\triangle ABC$ , the midpoint of $BC$ and $H$ are collinear. Let the midpoint of $BC$ and the excenter of $A$ in $\triangle ABC$ be respectively $I$ and $K$ . What I am trying to prove is that $HI \parallel AE$ and $HK \parallel AF$ . (Perhaps $EFIH$ and $AFKH$ are parallelograms.) But I don't exactly know how.","['euclidean-geometry', 'geometry']"
3241745,"Finite non-Abelian group $|G| = pq$, $p>q$ primes, prove: $q\ |\ p-1$","I have a finite noncommutative group $G$ with $pq$ elements, where $p, q$ are prime numbers. So $|G| = pq$ and $p > q$ . I need to prove that $p-1$ is divisible by $q$ . (so that $q\ |\ p-1$ ) I think I am supposed to use centralizers. (Centralizers for element $a \in G$ is a set $R(a) = \{g^{-1}ag\ |\ g \in G\}$ .) I have proved that there exist one and one only subgroup with $p$ elements and that there are $p-1$ elements with order $p$ in the group $G$ .
I am not sure if this is useful. How could I prove that $p-1$ is divisible by $q$ ? EDIT: Thank you for your answers, I will look into it. I haven't learned about Sylows theorems or groups yet. Is there any other way to prove this without using Sylow?","['group-theory', 'abstract-algebra', 'finite-groups']"
3241749,2-Wasserstein distance between empirical distributions,"I am trying to validate the earth-mover distance implementation from the python optimal transport library https://pot.readthedocs.io/en/stable/all.html#module-ot . It would be great if somebody could point at any flaws in what I write below. Given two measures $\mu_1, \mu_2$ on $\mathbb{R}^N$ , the 2-Wasserstein distance between them is $$W_2^2(\mu_1,\mu_2) = \inf \mathbb{E} \left[ \|x-y\|_2^2 \right]$$ where the infimum is taken over all random vectors $(x,y)\in \mathbb{R}^n \times \mathbb{R}^n$ with $x\sim \mu_1$ and $y\sim \mu_2$ . Specializing to Gaussian measures $\mu_1\sim N(m_1,\Sigma_1)$ and $\mu_2\sim N(m_2,\Sigma_2)$ , one can show that $$W_2^2(\mu_1,\mu_2) = \|m_1-m_2\|_2^2 + \text{trace} \left( \Sigma_1 + \Sigma_2 - 2 \left( \Sigma_1^{1/2} \Sigma_2 \Sigma_1^{1/2} \right)^{1/2} \right)$$ (See Remark 2.30 in Peyre-Cuturi's text). Now let's focus on the zero-centered case $m_1=m_2=0$ with diagonal covariance matrices $\Sigma_{1,ii}=1$ and $\Sigma_{2,ii}=4$ . In dimensions 10, 25, 50 and 100, the above expression yields 3.16, 5.0, 7.1 and 10.0 respectively, and I seek to recover these values from the corresponding empirical distributions. For measures with discrete support $\mu_1=\sum_{i=1}^n a_i \delta_{x_i}$ and $\mu_2=\sum_{j=1}^n  b_j \delta_{y_j}$ (with weight vectors $a,b$ belonging to the n-dimensional simplex), evaluation of the Wasserstein distance corresponds to solving a network flow problem $$W_2^2 = \min_{T\in \Pi(a,b)}\text{trace}(T^TM)$$ where $M$ is the cost matrix with entries $M_{ij}=\|x_i-y_j\|_2^2$ and where $\Pi(a,b)$ denotes the transportation polytope $$\Pi(a,b) = \left\{ T \in \mathbb{R}^{n\times n}: \quad T 1_n=a, \quad T^T 1_n=b \right\}.$$ As far as I am concerned, the solution of this network flow problem is implemented in function ot.emd. If I draw samples from the two Gaussian disributions above and I apply ot.emd (with $a_i=b_i=\frac{1}{n}$ ) the results I get for different dimensions $N$ and sample sizes $n$ are displayed in the figure below and are very different from the expected theoretical results $W_{2,N=10}=3.16$ , $W_{2,N=25}=5.0$ , $W_{2,N=50}=7.1$ and $W_{2,N=100}=10.0$ . The graphs represent the average over 20 independent runs and the varince is negligible. In high dimensions, they seem to be off by a factor of 2 plus some downward bias . There is also a considerable upward bias associated with this estimator and a very slow convergence - $\mathcal{O}(n^{-1/N})$ , c.f. Peyre-Cuturi, Section 8.4 - which makes me wonder how these estimators are used in practice . Below I plot the self-distance between two standard Gaussians as a function of sample size in different dimensions, again using the average of 20 independent draws. I am familiar with the Sinkhorn-Knopp approximation to the EMD, which solves the network flow problem faster, but not more accurately.","['optimal-transport', 'network-flow', 'probability-theory']"
3241779,Calculate the sum $\sum_{n=1}^\infty {(-1)^n\over 1+n^2}$,"The question: Calculate the sum $$I:=\sum_{n=1}^\infty {(-1)^n\over 1+n^2}$$ My attempt: Notation: In a previous question I have calculated $$\sum_{n=1}^\infty{1\over n^2+1}={1\over 2}\left(\pi{e^\pi+e^{-\pi}\over e^\pi-e^{-\pi}}-1\right)$$ and if possible, I would like to use it. On the one hand: $$\sum_{-\infty}^\infty {(-1)^n\over 1+n^2}=1+2I$$ On the other hand: $$\sum_{-\infty}^\infty {(-1)^n\over 1+n^2}=-Res((-1)^z\cdot{\pi \cot(\pi z)\over 1+z^2},i)-Res((-1)^z\cdot{\pi\cot(\pi z)\over 1+z^2},-i)
\\ Res((-1)^z\cdot{\pi \cot(z\pi)\over 1+z^2},i)={(-1)^i\over 2i}\cdot\cot(\pi i)
\\ Res((-1)^z\cdot{\pi \cot(z\pi)\over 1+z^2},-i)={(-1)^{-i}\over -2i}\cdot\cot(-\pi i)
$$ And in general: $$
\sum_{-\infty}^\infty={\pi\over 2i}((-1)^i\cot(-\pi i)-(-1)^i\cot(\pi i))
$$ But I don't know how to keep evaluate it.","['complex-analysis', 'summation', 'residue-calculus']"
3241799,"$\text{SL}(2,\mathbb{R})$ is homeomorphic to $S^1 \times \mathbb{R}^2$ [duplicate]","This question already has answers here : $M= \{ A \in Mat_{2 \times 2}{\mathbb{R}}| \det(A)=1 \}$ is homeomorphic to $S^{1} \times \mathbb{R}^{2}$ (2 answers) Closed 5 years ago . Using the left action of $\text{SO}(2)$ on $\text{SL}(2,\mathbb{R})$ ,
  show that $\text{SL}(2,\mathbb{R})$ is homeomorphic to $S^1 \times
 \mathbb{R}^2$ I tried defining the action by $\psi: \text{SO}(2) \times \text{SL}(2,\mathbb{R}) \to \text{SL}(2,\mathbb{R})$ by $(A,B) \mapsto AB$ . The kernel of this map are pairs $(A,A^{-1})$ such that $A \in \text{SO}(2)$ . Clearly this map is surjective, so I wanted to show that I can now find a canonical homeomorphism $\phi: \text{SO}(2) \times \text{SL}(2,\mathbb{R})/ \ker \psi \to S^1 \times
 \mathbb{R}^2$ . For instance, maybe the map $(A,B)\ker\psi \mapsto (A,(\cos xb_{1,1} -\sin xb_{1,2}, \sin xb_{2,1} +\cos xb_{2,2}))$ where $A$ is rotation by $x$ . However, I'm not entirely sure this is well defined, and even if so, if it is injective and open. Using the theory of Lie groups , how should I approach this problem?","['general-topology', 'lie-groups']"
3241917,A co-meagre topology on $\aleph_{\omega}$,"My topology professor asked us to give a topology in which every singleton is $G_{\delta}$ , yet it is not first-countable. I found an example, but since I like set theory, I thought I would try being creative, and observe the co- $<\aleph_{\omega}$ topology (meaning, the open sets are the empty set and the sets who have complements of smaller cardinality than the space). It looks like a singular cardinal might work here, due to their interesting properties, and indeed every singleton is $G_{\delta}$ . But I can't, for the life of me, prove or disprove whether this topology has a countable local base for every point. I first thought that a good collection of sets that could not possibly all be contained in an element from a countable collection is $\aleph_{\omega}\setminus\{\aleph_{n}+\alpha\mid n\in\mathbb N\}$ where $\alpha$ runs through the entire space, but this backfired, since $U_{i}=\aleph_{\omega}\setminus\{\aleph_{n}+j\mid j<\aleph_{i},n\in\mathbb N\}$ works (an interesting note here is that I discovered that the pidgeonhole principle ""fails"" for singular cardinals $\kappa$ and functions $f:\kappa\rightarrow \lambda $ when $cf(\kappa)\leq\lambda$ , since we can have instead of an element of $\lambda$ with $\kappa$ preimages, an element in $\lambda$ with $\beta\in cf(\kappa)$ images for all such $\beta$ ).
My question is, is this space first-countable? If so, what would be a local base for each point (if we show that it is not second-countable, it is the same in co-topologies)? If not, which collection of open sets in this topology refute first countability?","['general-topology', 'set-theory']"
3241967,Probability of a fraction $a/b$ that cannot be simplified [duplicate],"This question already has answers here : Probability that two random numbers are coprime is $\frac{6}{\pi^2}$ (2 answers) Closed 3 years ago . Locked . This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions. Let $a$ and $b$ be random integers chosen independently from the uniform distribution on $\{1, 2,\dotsc, N\}$ . As $N \rightarrow \infty$ , what is the probability that the fraction: $$\frac{a}{b}$$ cannot be simplified? Note: As specified in the comments, the question is the same as this one .","['elementary-number-theory', 'gcd-and-lcm', 'probability']"
3241970,A function $g(x)$ has one and only one real root if $g'(x)\leq k <0$.,"$g : \mathbb{R} \to \mathbb{R}$ is differentiable on $\mathbb{R}$ .  Then $g(x)$ has one and only one real root if $g'(x)\leq k <0$ . Proof attempt: Let us assume the contrary, i.e. $g(x)$ has no real zero at all. Therefore, being continuous, $g(x)$ cannot be both positive and negative on $\mathbb{R}$ . So, firstly we assume that $g(x) <0$ for every $x \in \mathbb{R}$ . We take some $a>0$ . (WLOG, take $a=1$ ). Now, $g(1)/1<0$ and $g(1)/k >0$ . So, $\displaystyle\frac{g(-g(1)/k)-g(1)}{-g(1)/k-1} \leq k <0 \implies -k \leq \displaystyle\frac{g(-g(1)/k)-g(1)}{g(1)/k+1} \implies 0<-k \leq g(-g(1)/k) $ So, we have found at least one  point in the domain, where $g(x)$ is positive. So, $g(x)$ must have a zero. Now, $g'(x)<0, \ \  \forall x \ \in \mathbb{R}$ makes the function one-to-one. [Note that the numerator must be positive, since $1>- g(1)/k \implies g(-g(1)/k)>g(1)$ ] For the assumption that $g(x)>0$ , we consider the points $a<0$ and $-g(a)/k$ (WLOG, take $a=-1$ ). Everything else is kept the same. Are the statement and the proof both correct, or is there any mistake? Please verify.","['proof-verification', 'roots', 'real-analysis', 'rolles-theorem', 'derivatives']"
3241994,Showing that $\sum_{n=1}^{\infty}\left(\frac{\sin(22n)}{7n}\right)^3=\frac{1}{2}\left(\pi-\frac{22}{7}\right)^3$,"How to show that? $$\sum_{n=1}^{\infty}\left(\frac{\sin(22n)}{7n}\right)^3=\frac{1}{2}\left(\pi-\frac{22}{7}\right)^3$$ I have no ideas to prove it, but it seems correct via Wolfram's calculator","['pi', 'sequences-and-series']"
3242010,Finding $\lim_{x\rightarrow 0^+} \frac{x^{-x}-1}{x}$,I'm trying to solve the limit $$\lim_{x\rightarrow 0^+} \frac{x^{-x}-1}{x}$$ I think we should use L'Hospital rule and the limit becomes $$\lim_{x\rightarrow 0^+} -x^{-x}(\log x + 1)=\lim_{x\rightarrow 0^+} \frac{\log x + 1}{-x^{x}}= +\infty$$ Is it right? I've tried to modify the form and not use L'Hospital's rule but without success.,"['limits-without-lhopital', 'limits', 'exponential-function', 'real-analysis']"
3242011,Finding the closest two points on two lines in N-dimensions.,"Let $a,b,c,d$ be vectors in $\mathbb R^n$ , where $b$ and $d$ are linearly independent. Further define $x(s) = a+sb$ and $y(t)=c+td$ ( $s,t \in \mathbb R$ ) The question now is which $(s,t)$ minimizes $||x(s)-y(t)||_2^2$ Because the vectors defining the direction are linearly independent, the lines are not parallel. Now, if the lines intersect at a point, than that point is the minimum. My first question now is, how one can write that intersection point in N dimensions. I have not found a good way to parametrize it. If the lines are skew lines, that is, they neither intersect nor are they parallel, the points of the closest distance are given by the connecting line which is orthogonal to both lines. I am not sure however, why that is the case, why the minimum is unique and subsequently how one finds the minimal $(s,t)$ .","['optimization', 'geometry', 'real-analysis']"
3242029,How exactly does Hahn-Banach theorem explain duality of vector spaces?,"Serge Lang's Linear Algebra textbook just introduced me to the concept of dual space in very formal terms: space of all functional transformations having co-domain as $1$ -dimensional vector space over the field $\mathbb{K}$ (since in essence, field $\mathbb{K}$ is a vector space over itself). But the textbook did not explain the exact purpose of the term ""duality"", thus I decided to go little further and dive into some basic functional analysis. The Uncertainty Principle by Terrence Tao ( reference ): Terrence Tao wrote a really nice article on the concept of duality, that is explained in terms of local and global perspectives. In his first example: Vector space duality A vector space ${V}$ over a field ${F}$ can be described either by the set of vectors inside ${V}$ , or dually by the
set of linear functionals ${\lambda: V \rightarrow F}$ from ${V}$ to the
field ${F}$ (or equivalently, the set of vectors inside the dual space ${V^*}$ ). (If one is working in the category of topological vector
spaces, one would work instead with continuous linear functionals; and
so forth.) A fundamental connection between the two is given by the Hahn-Banach theorem (and its relatives). As you see in the last sentence (in italic font), Tao mentions that Hahn-Banach theorem displays the fundamental connection between some vector space $V$ and its dual $V^*$ . Therefore I've decided to investigate this concept a little further. Hahn-Banach Theorem and Dual space : There is a question regarding some similar connection on Math SE , but I'm not certain whether or not it is the answer to my question. From my understanding of answers below the referenced question, Hahn-Banach theorem states that for any arbitrary vector $v \in V$ , there exists a functional $L \in V^*$ such that $|L(v)|=||v||_{V}$ and $||L||_{V^*}=1$ . The definition of norm on the dual space is: $$||L||_{V^*}=\textrm{sup}\{|L(v)|: v \in V, |v| \leq 1 \}$$ where $\textrm{sup}$ denotes the supremum of set. I also know that every $L \in V^*$ is a linear transformation with norm $1$ that is bounded (i.e $\exists C \in \mathbb{K}, ||T(v)||_{V^*} < C||v||_{V}, \forall v \in V$ , where $C$ is called operator norm). This (along with definition of dual norm) shows another interesting relation: $$||v||_{V}=\textrm{sup}\{|L(v)|: v \in V, ||L||_{V^*}=1 \}$$ Riesz-representation theorem (Extension) : According to comments made by Berci below this post, complete inner product spaces (or Hilbert spaces) have special relationship with their dual spaces. Let $H$ be a Hilbert Space on the field $\mathbb{R}$ , this relationship can be seen by Riesz-representation theorem which asserts that $H$ and $H^*$ are isometrically isomorphic (whereas in complex field case, they are anti-isomorphic). In more specific details, it shows that there exists $g \in H$ such that for any functional $L \in H^*$ and any $x \in H$ : $L(x) = \langle{} f, g \rangle{}$ . Moreover, as a consequence to isometric connection: $||x||_{H} = ||L(x)||_{H^*}$ . This theorem establishes interesting connection between inner product and functionals. In fact, I believe it can be utilized as extension for Hahn-Banach theorem to see the deeper connection from geometric perspective, since the isomorphic isometric connection that Riesz Representation gives is equivalent of hyper-plane corresponding to its normal unit vector (and this seems to be a consequence of Hahn-Banach theorem, proving the existence of unit functionals). This can be more intuitively understood by specific cases of $L^P$ spaces, since they have interesting properties such as natural isomorphism of their duals, but I don't believe I have sufficient experience to group this information yet. Question : How exactly does the Hahn-Banach theorem show the fundamental connection between a vector space and its dual, as mentioned by Terrence Tao? Is it just that every vector $v$ has a corresponding functional which has the norm $||v||$ ? Is there more abstract explanation involving the idea of dual norm? Thank you!","['duality-theorems', 'linear-algebra', 'functional-analysis', 'linear-transformations']"
3242046,When is this rearrangement theorem for integrals true?,"Bernhard Riemann proved that if $(a_n)$ is a sequence in $\mathbb{R}$ , then the sum of the infinite series $\Sigma_{n=1}^\infty a_n$ stays the same regardless of how you rearrange the terms if and only if the series $\Sigma_{n=1}^\infty |a_n|$ is convergent.  I’d like to see if something analogous for integrals is true. My question is, for what functions $f:[a,b]\rightarrow\mathbb{R}$ is it true that $\int_a^b f(g(x)) dx = \int_a^b f(x) dx$ for all bijective functions $g:[a,b]\rightarrow[a,b]$ ? Or is that too stringent a condition to be interesting, and do we need to impose some conditions on $g$ to get a more meaningful result?","['measure-theory', 'lebesgue-integral', 'real-analysis', 'calculus', 'riemann-integration']"
3242050,Sum with Bernoulli polynomial,"I'm trying to prove the following identity: $$\sum_{k=0}^n \dfrac {\binom n k B_k(x)} {(n-k+1)} = x^n$$ I transformed this identity as follow: $$\dfrac{1}{(n+1)}\sum_{k=0}^n \binom {n+1} k B_k(x) = x^n$$ Also I tried to do the following: $$\sum_{k=0}^n \dfrac {C_n^k B_k(x)}{(n-k+1)} = n!\sum_{k=0}^n \dfrac {B_k(x)}{k!} \dfrac {1}{(n+1-k)!}$$ I add and subtract the (n+1)th summand: $$n!\sum_{k=0}^{n+1} \dfrac {B_k(x)}{k!} \dfrac {1}{(n+1-k)!}-\dfrac{B_{n+1}(x)}{n+1}$$ Denote: $$ a_k= \dfrac {B_k(x)}{k!}, b_{n+1-k}=1$$ Hence: $$A(t)=\sum_{k=0}^{\infty} \dfrac {B_k(x)}{k!}t^k=\dfrac {te^{xt}}{e^t-1}$$ $$B(t)=\sum_{k=0}^{\infty} \dfrac {t^k}{k!}=e^t$$ $$n!A(t)B(t)=n!\dfrac {e^{t(x+1)}t}{e^t-1}$$ I want to prove it: $$\sum_{k=0}^n \dfrac {\binom n k B_k(x)} {(n-k+1)} = x^n$$ So, I find the exponential generating function for the right side of the equality: $$\sum_{k=0}^{\infty} \dfrac {(x^nt)^k}{k!}=e^{x^nt}$$ So I reformulated this task as follow.
Can we prove that: $$n!\dfrac {e^{t(x+1)}t}{e^t-1}=\sum_{k=0}^{\infty} \dfrac {B_k(x)}{k}t^k +e^{x^nt}$$ Unfortunately, I can't find $$\sum_{k=0}^{\infty} \dfrac {B_k(x)}{k}t^k$$ Hope you can help me to prove this identity. Thanks for your attention!","['bernoulli-polynomials', 'combinatorics', 'generating-functions']"
3242086,Is $\Bbb Z[\sqrt{6}]$ a UFD?,"I want to show that $\Bbb Z[\sqrt{6}]$ is not a UFD. We know that this is a Dedekind domain. (Hint: For the ideals $\mathfrak{p}=(2,4+\sqrt{6})$ , $\mathfrak{q}=(5,4+\sqrt{6})$ , compute $\mathfrak{p}^2$ , $\mathfrak{q}\bar{\mathfrak{q}}$ , $\mathfrak{p}\mathfrak{q}$ and $\mathfrak{p}\bar{\mathfrak{q}}$ .) Note that $\mathfrak{p}^2=(4,8+2\sqrt{6},22+8\sqrt{6})$ , $\mathfrak{q}\bar{\mathfrak{q}}=(5, 20-5\sqrt{6},20+5\sqrt{6})$ , $\mathfrak{p}\mathfrak{q}=(10,8+2\sqrt{6},20+5\sqrt{6},22+8\sqrt{6})$ , and $\mathfrak{p}\bar{\mathfrak{q}}=(10,8-2\sqrt{6},20+5\sqrt{6})$ . Also, we observe that $10=(4+\sqrt{6})(4-\sqrt{6})=(-1+\sqrt{6})(2+\sqrt{6})(1+\sqrt{6})(-2+\sqrt{6})$ and $10=2.5=(2+\sqrt{6})(-2+\sqrt{6})(-1+\sqrt{6})(1+\sqrt{6})$ . But, this doesn't imply that $\Bbb Z[\sqrt{6}]$ is not a UFD. I am not sure how to use the hint. Thanks!","['ring-theory', 'abstract-algebra', 'algebraic-number-theory']"
3242112,"Why is ""division by $(z-1)$"" valid here?","Is there an easy way to justify: $$x(x-1)(x+1) \equiv x(x^2-1) \Rightarrow (x-1)(x+1) \equiv x^2-1,$$ even for $x=0$ ? I seemingly have to divide by $x$ which should place the restriction $x \neq 0$ on the final result. Does this work only for polynomials? EDIT: thank you for the comments, in light of the suggestions to do case work I'll update with a more involved example to demonstrate why I am not looking for this approach. I'm sorry to move the goal posts a bit, let me know if this should be a new question. Let's suppose that $w=e^{2\pi i/n}$ where $n$ is an integer. Let's say I've deduced that $$(z-1)(z-w)(z-w^2)...(z-w^{n-1}) \equiv (z-1)(1+z+z^2+...+z^{n-1}).$$ I want to conclude here that $(z-w)(z-w^2)...(z-w^{n-1}) \equiv 1+z+z^2+...+z^{n-1}$ including $z=1$ - it's not easy to verify by cases anymore since I am actually trying to use this factorisation to show that $(1-w)(1-w^2)...(1-w^{n-1})=n$ .","['roots-of-unity', 'algebra-precalculus', 'polynomials', 'complex-numbers']"
3242133,Conditional Probability and Shark Attacks,Probability of being attacked on day n by shark given that you have not being attacked on days before n is $\cfrac{1}{n + 1}$ . What is the probability of the number of the day when shark attacks for the first time is n. Is it ok to think in the following way: $\mathbb{P}($ day of first attack = n $) = \mathbb{P}($ attack on day n $\land$ $\neg$ attack on day $(n-1)$ $\land$ $...$ $\land$ $\neg$ attack on day 1 $)$ $= \mathbb{P}($ attack on day n $|$ $\neg$ attack on day $(n-1)$ $\land$ $...$ $\land$ $\neg$ attack on day 1 $) \cdot \mathbb{P}($ $\neg$ attack on day $(n-1)$ $\land$ $...$ $\land$ $\neg$ attack on day 1 $)$ $=\cfrac{1}{n + 1} \cdot \Big( 1 - \cfrac{1}{n} \Big)\cdot ... \cdot \cfrac{1}{2} = \cfrac{1}{n + 1} \cdot \cfrac{n - 1}{n} \cdot \cfrac{n}{n - 1} \cdot ... \cdot \cfrac{1}{2} = \cfrac{1}{(n + 1)\cdot n}$,"['conditional-probability', 'proof-verification', 'probability']"
3242176,Find the curve of the maximum value of work done?,"Suppose $C$ is a simple close curve (i.e. it doesn’t intersect itself) in the first quadrant. If $F = (y^2/2 + x^2y, -x^2 + 8x)$ , find the curve that produces the maximum amount of work done by $F$ .
What is the maximum value of work?","['integration', 'greens-theorem', 'multivariable-calculus', 'calculus', 'stokes-theorem']"
3242225,Calculate $\lim_{x\rightarrow1}\frac{(1-x^2)}x$ by definition,How do I calculate the $\lim_{x\rightarrow1}\frac{(1-x^2)}x$ using the δ/ε definition? I'm losing my mind on this :c I'm like lost after something like this: | $\frac{(1+x)  (1-x)}x$ - L| < ε,"['limits', 'calculus', 'definition']"
3242226,What does the symbol $\omega$ stand for in statistics?,"I was reading an answer here in math stackexchange and it mentioned this: Linearity of Expectation then follows from its definition. $\begin{align} \mathsf E(X+Y) =&~ \sum_{\omega\in\Omega}
(X+Y)(\omega)~\mathsf P(\omega) \\[1ex] =&~ \sum_{\omega\in \Omega}
X(\omega)~\mathsf P(\omega)+\sum_{\omega\in \Omega} Y(\omega)~\mathsf
P(\omega) \\[1ex] =&~ \mathsf E(X)+\mathsf E(Y) \end{align}$ What does the symbol $\omega$ stand for?","['expected-value', 'statistics', 'probability-theory', 'random-variables']"
3242227,Does a set of nested subspaces in a Hilbert space have a concise name?,"Let ${\cal H}$ be a Hilbert space (separable, if it matters), and let $X$ be a set of closed subspaces with the property that for all ${\cal P},{\cal Q}\in X$ , we have either ${\cal P}\subset {\cal Q}$ or ${\cal Q}\subset {\cal P}$ . We can assume that $X$ includes the trivial subspaces ${\cal H}$ and $\varnothing$ if that makes a difference. The set $X$ of closed subspaces is not necessarily countable. Does such an $X$ have a concise name? I'm not a mathematician. I tried searching for keywords like ""sequence of subspaces"", ""nested subspaces"", and ""filter"" (just guessing), but I didn't recognize anything relevant. I also tried looking in the context of ""resolution of the identity,"" which I suppose is what $X$ would be called if it were described in terms of projection operators instead of subspaces, but I didn't find any clear statements about whether or not that name still applies when $X$ is described in terms of closed subspaces.","['hilbert-spaces', 'functional-analysis', 'terminology']"
3242298,Prove $0.9999^{\!101}<0.99<0.9999^{\!100}$,"Prove $$0.9999^{\!101}<0.99<0.9999^{\!100}$$ I think its original idea is $$(1-x)^{(1-\frac{1}{x})}<(1+x)^{(\frac{1}{x})} \tag{I can't prove!}$$ For $x=100^{\!-1}\,\therefore\,(1-x)^{(1-\frac{1}{x})}<(1+x)^{(\frac{1}{x})}\,\therefore\,0.99^{\!-99}<1.01^{\!100}$ . Furthermore $0.99^{\!-99}\times0.99^{\!100}=0.99<1.01^{\!100}\times0.99^{\!100}=0.9999^{\!100}$ . For $x=-100^{\!-1}\,\therefore\,(1-x)^{(1-\frac{1}{x})}<(1+x)^{(\frac{1}{x})}\,\therefore\,1.01^{\!101}<0.99^{\!-100}$ . Furthermore $1.01^{\!101}\times0.99^{\!101}=0.9999^{\!101}<0.99^{\!-100}\times0.99^{\!101}=0.99$ .","['exponentiation', 'logarithms', 'inequality', 'derivatives', 'exponential-function']"
3242360,What math is this?,I am trying to figure out what payment is necessary to cover both the payment processing fees and the state's gross receipts tax. The payment processing fee is $0.30 plus 2.9% of the purchase price plus gross receipts tax and the gross receipts tax is 1.5% of the purchase price plus the payment processing fee. The variables are: $x$ = purchase price $y$ = necessary payment $a$ = payment processing fee $b$ = gross receipts tax The formulas are: $y=(x+a+b)$ $a=(x+b) \cdot 0.029+0.3$ $b=(x+a) \cdot 0.015$ The equation grows infinitely since I can't solve $a$ without solving $b$ and I can't solve $b$ without solving $a$ . I don't remember what this type of math is called so I can't research how to solve for $y$ .,"['systems-of-equations', 'functions', 'linear-algebra']"
3242398,$\sqrt{11}$ as rational number within $10^{-4}$?,"Use continued fractions to find a rational number which approximates $\sqrt{11}$ to
within $10^{−4}$ . I know how to solve for continued fractions like this: $$\sqrt{11}=3+x$$ $$11=9+6x+x^2$$ $$11=9+(6+x)x$$ $$2=x(6+x)$$ $$x=\frac{2}{6+x}=\frac{1}{3+\frac{x}2}=\frac{1}{3+\frac{1}{6+x}}$$ therefore, $$\sqrt{11}=[3;\overline{3,6}]$$ How do I find out where to terminate the fraction so as to obtain close value with error less than  the mentioned limit","['number-theory', 'continued-fractions', 'rational-numbers']"
3242443,"What is the cardinality of $A=\left\{f\in C^1[0,1]:f(0)=0,\ f(1)=1,\ \left|f'(t)\right|\le 1\ forall\ t\ \in [0,1].\right\}$","What is the cardinality of the following set? $A=\left\{f\in C^1[0,1]:f(0)=0,\ f(1)=1,\ \left|f'(t)\right|\le 1\ forall\ t\ \in [0,1].\right\}$ My try: Since $f\in C^1[0,1]$ . So, $f$ satisfy the proposition of Lagrange's mean value theorem. So, there is $c\in (0,1)$ : $f'(c)=1.$ That is slope of the tangent at $(c,f(c))$ is $1$ .When I tried to sketch possible graphs with this much information. there exists a point in $(0,1)$ such that derivative has to be more than $1$ . Only graph with this property is $y=x$ .","['continuity', 'derivatives', 'real-analysis']"
3242482,Difference between gradient and derivative.,"My question may be a bit stupid, but this morning I tried to explain the gradient to someone, and he makes a parallel with derivative of function $f:\mathbb R\to \mathbb R$ . What he says is that for a function $f:\mathbb R\to \mathbb R$ , the gradient and the derivative are the same. I agree that the scalar value are the same, but I'mnot sure that the meaning behind is the same. For example, take $f(x)=x^2$ . For me the gradient of $f$ is going to be the vector field $\nabla f(x)=2x\cdot 1$ , where $1$ is the basis of $\mathbb R$ , so it should look like that whereas the derivative $f'(x)$ is really the rate of the function, and if it would be a vecteur field, it would be a vector field over the range of $f$ , and not on the domain of $f$ as the gradient is. What do you think ? To illustrate, I would say that the derivative field is in red and blue, and the gradient is in pink.",['real-analysis']
3242492,"The ""true"" metric on the Grassmannian: Plücker vs Projective-Frobenius embeddings","There are several references in the literature to some kind of ""most natural"" metric on the Grassmannian manifold, often called the ""geodesic distance"" or the ""Binet-Cauchy"" distance. There are several equivalent ways to derive this metric: perhaps the simplest is to embed the (signed) Grassmannian into Euclidean space via the Plücker embedding as a projective manifold, and then the restriction of the $\ell_2$ Euclidean metric to the embedded projective manifold gives the distance between two (weighted) subspaces. A good reference on this can be found in this paper , including equivalent characterizations via ""principal angles"" between subspaces. The thing is, I don't understand why this is the metric on the Grassmannian. There is another perfectly good Euclidean embedding and associated metric for the Grassmannian: the projective embedding , where each subspace is represented via the unique orthogonal projection matrix mapping to that subspace. The Frobenius norm on projection matrices then gives another perfectly good distance between subspaces. Put simply, suppose $M$ is a full-column-rank $n \times r$ matrix, the columns of which are an orthonormal basis for some $r$ -dimensional subspace of $\Bbb R^n$ . Then: The Plücker embedding is given by the $r$ 'th compound matrix of $M$ , notated $C_r(M)$ The projective embedding is given by the projection matrix $M M^+$ , where $M^+$ is the pseudoinverse of $M$ In both cases, you get: one matrix for each subspace, regardless of which basis you choose (up to sign in the first case) a set of polynomial equations that uniquely define the embedded variety an induced Euclidean distance on the result (the Frobenius norm) But these two distances are not the same. So what claim does the Plücker embedding have to inducing the ""true"" metric on the Grassmannian? And what does this mean, that the Grassmannian corresponds to two different algebraic varieties?","['riemannian-geometry', 'grassmannian', 'algebraic-geometry', 'manifolds', 'differential-geometry']"
3242495,"Yaw, Pitch and Roll composition","I'm trying to understand composition of rotations using eulers angles and rotation matrices.
I am facing a counterintuitive situation performing two rotations about different angles.
My setting is the following: first rotation of an angle $\psi$ about the Z body-axis (yaw) second rotation of an angle $\theta$ about the Y body-axis (pitch) third rotation of an angle $\varphi$ about the X body-axis (roll) Denoting with $$R_X=\begin{bmatrix}1&0&0\\
0&\cos(\varphi)&-\sin(\varphi)\\
0&\sin(\varphi)&\cos(\varphi)\end{bmatrix}$$ $$R_Y=\begin{bmatrix}\cos(\theta)&0&\sin(\theta)\\
0&1&0\\
-\sin(\theta)&0&\cos(\theta)\end{bmatrix}$$ $$R_Z=\begin{bmatrix}\cos(\psi)&-\sin(\psi)&0\\
\sin(\psi)&\cos(\psi)&0\\
0&0&1\end{bmatrix}$$ the rotation matrices relative to elementary rotation about the three body axis, I obtain that a general rotation of eulers angles $(yaw,pitch,roll)=(\psi,\theta,\varphi)$ is given by: $$R=R_X\cdot R_Y\cdot R_Z$$ Suppose now that I would like to perform this 2 consecutive rotations: Rotation of $(yaw,pitch,roll)=(0,\frac{\pi}{4},0)$ Rotation of $(yaw,pitch,roll)=(\frac{\pi}{2},0,0)$ The matrices relative to these two rotations are the following: $$R_1=\begin{bmatrix}\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\\
0&1&0\\
-\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\end{bmatrix}$$ $$R_2=\begin{bmatrix}1&0&0\\
0&0&-1\\
0&1&0\end{bmatrix}$$ and composing them I obtain the total rotation matrix $$R=R_2\cdot R_1 = \begin{bmatrix}0&-1&0\\
\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\\
-\frac{\sqrt{2}}{2}&0&\frac{\sqrt{2}}{2}\end{bmatrix}$$ If I want to recover the euler angles sequence relative to $R$ I apply these equations: $$\theta=\arcsin(R_{13})\\
\psi=-\arctan2\left(\frac{R_{12}}{\cos(\theta)},\frac{R_{11}}{\cos(\theta)}\right)\\
\varphi=-\arctan2\left(\frac{R_{23}}{\cos(\theta)},\frac{R_{33}}{\cos(\theta)}\right)$$ obtaining $(yaw,pitch,roll)=(\frac{\pi}{2},0,-\frac{\pi}{4})$ . However applying this sequence of rotation I do not recover the initial frame (sequence of rotations 1-2) unless the rotations 1-2 are performed on different axis respect the body ones. Probably I miss something and I'm making confusion with these concepts.
Please, could you help me to understand where I miss?","['matrices', 'rotations']"
3242535,Sequence Lemma and statistical convergence,"We know that from the sequence lemma : if a sequence $\{x_n\}$ in $A$ converges to $\ell$ , then $\ell\in \bar A$ .
Conversely, if the space is first countable, then $\ell\in \bar A$ implies that $\exists$ a sequence $\{x_n\}$ in $A$ converges to $\ell$ . Also, every convergent sequence is statistically convergent sequence (and in this case limits & statistical limits are same). But the converse isn't true. My question : If a sequence $\{x_n\}$ in $A$ converges statistically to $\ell$ , does $\ell$ belong to $\bar A$ ? Thanks in advance.","['general-topology', 'convergence-divergence', 'functional-analysis', 'sequences-and-series']"
3242605,"Solving the$\underset{[0,{\pi}]\times[0,{\pi}]}{\iint}{\cos}{(x+y)}dxdy$ with change of variables","so I need to solve the following integral using some change of variables: $$\underset{[0,{\pi}]\times[0,{\pi}]}{\iint}{\cos}{(x+y)}dxdy$$ It's easy without a change of variables, of course, but any suggestion as to how to do it with a change of variables? Thanks","['integration', 'multivariable-calculus', 'change-of-variable']"
3242620,Why are there two formulas for variance of random variables?,"I'm using an introductory statistics textbook and it mentioned this: Definition: If $X$ is a random variable with mean $E(X)  = \mu$ , then the variance  of $X$ is defined by $Var(X) = E((X−\mu)^2)$ . I thought the formula for the variance of X was: $$Var(X) = \sum_{i=1}^n p_i \cdot (x_i - μ)^2$$ How come it's different?","['statistics', 'variance', 'intuition', 'probability-theory', 'random-variables']"
3242625,"Integrable in the extended sense: Question about Spivak ""Calculus on Manifolds"" definition.","On Spivak ""Calculus on Manifolds"" he builds the concept of integration on an incremental fashion: He starts by defining the integral $\int_R  f$ on a rectangle R; Next he define the concept of characteristic function: \begin{equation}
 X_C = 1 \text{ if }x\in C \text{ else } 0.
 \end{equation} And use this concept for generalize the definition of integral for a 
region $C$ , by defining $\int_C  f = \int_R  f \cdot X_C$ for $C$ contained in a rectangle $R$ . This concept works for all the cases when C 
boundary has measure 0 and $X_C$ is integrable (see theorem 3-9 of the 
same book). Then he defines partitions of the unit to generalize this concept even further. Using the concept of partition of the unit he defines the integral in the extended sense as: \begin{equation}
\sum_{\phi \in \Phi}\int_A\phi \cdot f
\end{equation} where $\Phi$ is a collection of functions such that $\phi \in \Phi$ . Some properties of this functions are described next Be $A$ a bonded region and $O$ and open cover to it, it can be proved
     that (see theorem 3-11 of the same book)  there exist a collection $\Phi$ of $C^\infty$ functions such: $0 \le\phi(x) \le 1$ A finite number of $\phi(x)$ is different than zero in a open set containing $x \in A$ $\sum_{\phi \in \Phi} \phi(x) = 1$ For each $\phi \in \Phi$ there is an open set $U  \in O$ such that $\phi=0$ outside of some closed set contained in $U$ . Let us call this closed set $C$ . So my question is: how can we prove $\int_A\phi \cdot f$ is integrable? My understanding about the question is the following: From the above definition it follows that $\int_A\phi \cdot f = \int_C\phi \cdot f$ . So if $C$ boundary has measure $0$ we could use the previous definition of integration to say this function is integrable in this region...But how can we prove that this is indeed the case?","['integration', 'calculus', 'riemann-integration', 'real-analysis']"
3242640,How do I formally prove a universal implication?,"A textbook I am reading (Discrete Mathematics and its Applications by Rosen) went from introducing formal propositional and predicate logic (including popular rules of inference like Modus Ponens, Modus Tollens, and Universal Generalization) to introducing direct methods of proof for theorems of the form ∀n(P(n)->Q(n)). Apparently, most mathematical proofs of any kind of theorem are ""informal"" and omit many logical rules of inference and argumentative steps for the sake of conciseness. However, because the textbook doesn't provide even one example of a detailed ""tedious"" proof that expresses most or all rules of inference and axioms used in the proof, though I have a general idea of the connection between the two, I have been struggling to fully tie together the ideas of formal logic to the ideas of mathematically proving theorems of the form ∀n(P(n)->Q(n)). Can anyone provide an example of a detailed mathematical proof of a simple theorem that omits few (if any) logical steps in the argument? I have personally struggled with (as a personal exercise) meticulously proving the theorem ""for all integers, if n is odd then the square of n is odd"", but any logically detailed argument proving a simple theorem similar to that would be very useful.","['formal-proofs', 'proof-writing', 'logic', 'discrete-mathematics', 'natural-deduction']"
3242699,"Why is A union B also called ""A or B""?","In A union B, the element either belongs to A or B, or A and B right?
So shouldn't it be called A and/or B? Due to this I am unable to solve a problem in my textbook.",['elementary-set-theory']
3242756,"Suppose $f(x)$ has continuous second-order derivative over $(a,+\infty)$, and $f(x)>0$,$f''(x)\leq 0$. Prove $f'(x)\geq 0$.","Suppose $f(x)$ has continuous second-order derivative over $(a,+\infty)$ , and $f(x)>0$ , $f''(x)\leq 0$ . Prove $f'(x)\geq 0$ . Proof Consider proving by contradiction. If the conclusion does not hold, then $$ \exists x_0 \in (a,+\infty):f'(x_0)<0.$$ Since $f''(x)\leq 0$ , $f'(x)$ is nonincreasing. Therefore $$\forall x \in [x_0,+\infty):f'(x)\leq f'(x_0)<0.$$ One can claim that $f'(x)$ has a limit as $x \to +\infty$ , which is either a finite negtive number or the negative infinity.Thus, by L'Hôpital's rule, $$\lim_{x \to +\infty}\frac{f(x)}{x}=\lim_{x \to +\infty}f'(x)<0,$$ which contradicts, just noticing that $f(x)/x$ is positive with a sufficiently large $x$ , according to the assumption condition. Please correct me if I'm wrong!","['calculus', 'proof-verification', 'derivatives']"
3242801,Probability of no pair of consecutive heads in $n$ flips of a coin,"I am trying to solve the following problem. A biased coin shows heads with probability $p=1-q$ when it is flipped. Let $u_{n}$ be the probability that in $n$ flips, no pair of heads occur successively. Show that for $n \geq q$ , $$u_{n+2} = qu_{n+1} + pqu_{n}.$$ I know I need to use the partition theorem $P(X) = \sum P(X|B_{i})P(B_{i})$ where the set of events $B_{i}$ partition the sample space (the question event gives the hint ""use partition theorem with $B_{i}$ the event that first $i-1$ flips yield heads and the $i$ th yields tails""), but I still have no idea as to how to proceed and to how to choose the partition. I intuitively see why the answer is what it is, but I can't rigorously formulate the approach to the solution in my head.","['conditional-probability', 'probability-theory', 'probability']"
3242813,The notation for fundamental group!,"I am new in algebraic topology and I am wondering why the notation for the fundamental group is $\pi_1$ ? I mean what is this ""1"" for? I searched on the Web and did not find anything!  Any idea? Also why the name of the fudamental group is fundamental group? It is group! ok! But why fundamental group? Which information it says that it isbdeserved to be called fundamental! I know that it has some interesting application between topology and algebra.  But there should be something more interesting.","['general-topology', 'fundamental-groups', 'algebraic-topology']"
3242814,Maximizing the area between a function and its linear approximation,"Let $f: [a, b] \to [0, 1]$ be a monotonously increasing function and $y_f: [a, b] \to \Bbb R,\,\,x \mapsto \alpha x + \beta$ the corresponding linear approximation with constants $\alpha$ , $\beta \in \Bbb R$ . Hereby, the coefficients $\alpha$ and $\beta$ are determined by minimizing \begin{align}
\int_a^b (f(x) - y_f(x))^2 dx.
\end{align} I derived the values of the constants. They are of the form \begin{align}
\alpha = \alpha_1 \int_a^b f(x) dx + \alpha_2 \int_a^b x f(x)
\end{align} and \begin{align}
\beta = \beta_1 \int_a^b f(x) dx + \beta_2 \int_a^b x f(x),
\end{align} with $\alpha_1$ , $\alpha_2$ , $\beta_1$ and $\beta_2$ depending only on $a$ and $b$ . The area enclosed by $f$ and $y_f$ is given by \begin{align}
A(f) = \int_a^b |f(x) - y_f(x)|dx.
\end{align} My question: How to prove that it holds for the maximum value of the area $\max(\{A(f), f: [a, b] \to [0, 1], \,\,f\,\,\text{monotonously increasing}\}) = \frac{1}{4}(b - a)$ ? This is the value of $A(f)$ for the function $f(x) = \theta(x - (\frac{1}{2} (b - a) + a))$ which I assume to give the maximum value of $A(f)$ . In other words, I'm looking for the function that has the largest deviation from its linear approximation.","['integration', 'geometry', 'real-analysis']"
3242866,How many distinct arrangements of the letters in HEELLOOP are there in which the first two letters include a H or a P (or both)?,"CONTEXT: Question made up by uni lecturer. How many distinct arrangements of the letters in HEELLOOOP are there in which the first two letters include a H or a P (or both)? Note: There are 9 letters in total (one H, one P, two E's, two L's and three O's) When attempting this question, I tried splitting it up into different cases: First letter H, second letter P First letter P, second letter H First letter H, second letter not P (either an E, L or O) First letter P, second letter not H (either an E, L or O) First letter not H (either an E, L or O), second letter P First letter not P (either an E, L or O), second letter H I know for cases (1) and (2), there are $2!\cdot\frac{6!}{3!\cdot2!\cdot2!}=60$ ways to arrange it since there are $2!$ ways to arrange H and P, and for each, there are $\frac{6!}{3!\cdot2!\cdot2!}$ distinct ways to arrange 6 letters (where there are two E's, two L's and three O's). It is cases (3) to (5) where I get a bit lost, because letters you get to choose from for the 6 end letters depend on which letter is chosen to accompany the H or P in the first and second position. For example, in case (3), the first letter is a H, and the second letter can either be an E, L or O. If say, for example, it is an O, then the six remaining letters will consist of one P, two L's, two O's and two E's. But, if it were an E, then the six remaining letters would consist of one P, two L's, three O's and one E. The existence of these two different scenarios are what get me. Any help on how to approach this would be greatly appreciated.","['permutations', 'combinatorics', 'discrete-mathematics']"
3242905,Derivative function continuous iff partial derivatives continuous,"Let $f:\mathbb{R} ^{n}\rightarrow \mathbb{R} ^{m}$ be differentiable. The derivative function $Df:\mathbb{R} ^{n}\rightarrow L\left( \mathbb{R} ^{n},\mathbb{R} ^{m}\right)$ is continuous in respect to the operator norm $\left\| A \right\|_{L\left( \mathbb{R} ^{n},\mathbb{R} ^{m}\right)}:=\sup _{\left\| v\right\| =1}\left\| Av\right\|$ , iff the partial derivatives $\dfrac {\partial f_{i}}{\partial x_{j}}$ are continuous for all $i\in \left\{ 1,\ldots ,m\right\}$ and $j\in \left\{ 1,\ldots ,n\right\}$ . How can I show this?","['continuity', 'derivatives', 'functional-analysis']"
3242910,How many $2\times2$ orthogonal matrices $A$ with $A^3=I$ and $A^2=A^T$ are there?,"How many $2 × 2$ matrices $A$ satisfy both $A^3 = I_2$ and $A^
2 = A^t$ , where $I_2$ denotes the $2 × 2$ identity
  matrix and $A^
t$ denotes the transpose of $A$ ? A bit of manipulation gives me $AA^t=A^tA=I_2$ .So this is orthogonal. Now is there a fixed number of $2\times2$ orthogonal matrices? I have no idea. Please help.",['matrices']
3242984,"A ""distinguishing"" family of subsets","Suppose $A$ is a finite set, $B$ is a collection of subsets of $A$ , satisfying the following condition: $$\forall a, b \in A, a \neq b: \exists C \in B: (a \in C) \land (b \notin C)$$ What is the least possible size of $B$ . Currently, I know that the minimal size of $B$ is not less than $\lceil \log_2 |A| \rceil$ (by pigeonhole principle), and it does not exceed $2\lceil \log_2 |A| \rceil$ (an example of that size can trivially be constructed). However, I do not know the exact answer to the question.","['elementary-set-theory', 'pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
3243016,Existence of improper integral.,"I want to show that the improper integral $$\int_0^{\infty} \frac{\sin(x)}{x(1+x)}dx$$ exists. In order to do that, I split the integral into $\int_0^1 \frac{\sin(x)}{x(1+x)}dx + \int_1^{\infty} \frac{\sin(x)}{x(1+x)}dx$ , so that I can compute a value for $\int_0^1 \frac{\sin(x)}{x(1+x)}dx$ , which proves the existence for that part. For the other part, is it sufficient to show that $\int_1^{\infty} \frac{\sin(x)}{x(1+x)}dx < \int_1^{\infty} \frac{1}{x^2}dx$ knowing that $\int_1^{\infty} \frac{1}{x^2}dx$ exists, to say that $\int_1^{\infty} \frac{\sin(x)}{x(1+x)}dx$ exist and therefore the whole improper integral exists? Thanks for an answer in advance.","['integration', 'calculus', 'improper-integrals']"
3243025,Calculate a fourier transform of inverse of polynomial $\frac{1}{Q(x)}$,"I'm working with Exercise 4 of Chapter 4, Stein & Shakarchi ""Complex Analysis"" . The problem is Suppose $Q$ is a polynomial of degree $\geq2$ with distinct roots, none lying on the real axis. Calculate $$F(\xi) = \int_{-\infty}^{\infty} \dfrac{e^{-2\pi i x \xi}}{Q(x)}\ dx,\space\xi\in\mathbb R$$ in terms of the roots of $Q$ . What happens when several roots coincide? For $\xi=0$ , I approached by residue formula using upper half circle $C_R^+$ and $R\to\infty$ for the first summation and $C_R^-$ for the second summation and got the below: $$\dfrac{F(0)}{2\pi i} = \sum_{\Im(s_i)>0} \operatorname{Res}\dfrac{1}{Q(z)}=-\sum_{\Im(s_i)<0} \operatorname{Res}\dfrac{1}{Q(z)} $$ where $s_i$ 's are distinct roots of $Q$ . I think there would be a better representation for this using $Q(x)=A\prod(x-s_i)$ so that I can get a ""Fourier transform formula"" of $\frac{1}{Q(x)}$ , but I can't go further. For $\xi<0$ and $\xi>0$ , I find that I should use $C_R^+$ and $C_R^-$ respectively but that's all. I don't know about the the role of coinciding several roots too. How can I get some meaningful formulas for this problem? Edit Thanks for hints and answer. Using all I got for $\xi\geq 0$ , $$ F(\xi) = -2\pi i \sum_{\Im(s_i)<0} \operatorname{Res} \dfrac{e^{-2\pi i x \xi}}{Q(x)} = -2\pi i \sum_{\Im(s_i)<0} \dfrac{e^{-2\pi i s_i \xi}}{Q’(s_i)} $$ where the second equality holds when $Q(x)$ has all distinct roots and $Q’(s_i)=A\prod_{s_j\neq s_i}(s_j-s_i)$ . Also for $\xi\leq 0$ , $$ F(\xi) = 2\pi i \sum_{\Im(s_i)>0}\operatorname{Res} \dfrac{e^{-2\pi i x \xi}}{Q(x)} = 2\pi i \sum_{\Im(s_i)>0} \dfrac{e^{-2\pi i s_i \xi}}{Q’(s_i)} $$ For multiple roots, the residue formula of $\frac{1}{Q(z)}$ where $Q(z)=A\prod(z-s_i)^{r_i}$ would be different, but again I can’t approach. The residue formula for $n$ -th order poles requires $(n-1)$ -times of differentiation, and I feel this direct route is too dirty. How can I approach this?","['complex-analysis', 'fourier-transform']"
3243033,Can Naive Set Comprehension survive in multi-valued logic?,"If $\phi$ is a formula in which $x$ is not free, then: $(\exists x \ \forall y \ (y \in x \leftrightarrow\ \phi)) $ is an axiom. This is the inconsistent Naive comprehension axiom. Is this a paradox limited to classical [binary valued] first order logic? I mean is this still paradoxical in multi-valued logic. The idea is that in multi-valued logic we can have different truth values for $y \in x$ , one can in some sense understand the different truth values as difference in probability of membership of $y$ in $x$ . Lets interpret the formula $y \not \in x$ as there is non zero probability of $y$ not being an element of $x$ , in other words the probability of $y$ being a member of $x$ is not 1. According to this I see the paradox disappear! The idea is that the asserted set $x$ cannot have probability of $x \in x$ being 1, and also cannot have probability of $x \not \in x$ being 1 for obvious reasons that derives the paradox in binary logic, but it can have intermediate truth value, like for example $0.5$ probability of being in itself (and of course of not being in itself). This way I don't see any problem.","['elementary-set-theory', 'multivalued-logic', 'logic', 'first-order-logic']"
3243034,Computing the determinant of $X^*X$ given $X$.,"I am trying to prove that if $u_i\in \mathbb{R}$ for $i=1,...,n$ and $$X =\begin{pmatrix} 1 & 0 & 0 & \cdots & 0 \\
 0 & 1 & 0 & \cdots & 0 \\
 0 & 0 & 1 & \cdots & 0 \\
\ & \vdots & \ & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1 \\
u_1 & u_2 & u_3 & \cdots & u_n
\end{pmatrix}$$ then $\det(X^{T}X)=1 +u_1^2 + u_2^2 + \cdots + u_n^2$ , where $X^T$ is the transpose of $X$ . Here is what I know. $X$ is an $(n+1)$ -by- $n$ matrix, and $X^T$ is an $n$ -by- $(n+1)$ matrix; thus $X^TX$ is an $n$ -by- $n$ square matrix, so we can take its determinant. Also, the formula is simple to verify for the cases $n=2,3$ . The $(i,j)$ entry of the  matrix $X^TX$ is \begin{align*} (X^TX)_{i,j} &= \sum_{k}(X^T)_{i,k}X_{k,j} = \sum_k X_{k,i}X_{k,j} \\
&= \langle X_{\cdot,i},X_{\cdot, j}  \rangle = \delta_{i,j} + u_iu_j
\end{align*} where $X_{\cdot,j}$ denotes the $j$ th column of $X$ . Since I know all the entries, I could do some combination of induction and cofactor expansion, but I couldn't make it work. I also tried computation with ""block"" matrices, which is something I am not very familiar with. If $I_n$ is the $n$ -by- $n$ identity matrix and $\vec{u}=(u_1,\ldots,u_n)$ then we have $$ \vec{u} = \begin{pmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{pmatrix} \;\;\; \text{ and}\;\;\; \left( \vec{u}\right)^T = \begin{pmatrix} u_1 & u_2 & \cdots & u_n\end{pmatrix} $$ so that $$ X^T = \begin{pmatrix} I_n & \vec{u} \end{pmatrix} \;\;\; \text{ and}\;\;\; X = \begin{pmatrix} I_n \\ \left(\vec{u}\right)^T \end{pmatrix}. $$ Then if we are presumptuous, we can treat $X^T$ and $X$ as $1$ -by- $2$ and $2$ -by- $1$ matrices respectively. Then $$ X^TX = 1 + \vec{u}\left(\vec{u} \right)^T = 1 + \|u\|^2 $$ which clearly would imply the result I want, if this computation can be justified. I was unable to find this specific problem elsewhere, but I am sure it has been asked other times on this site. Please let me know your thoughts and tips for this problem, and help me justify or refute the block matrix computation I performed. If we cannot justify it, then why does it hint at the right answer? Thanks.","['determinant', 'matrices', 'linear-algebra', 'transpose', 'block-matrices']"
3243043,I need help with this rate of change problem,"The solid shown in the figure below consists of a cylinder of the radius (r) and height (h)
and a hemispherical void of the radius (r). The dimensions at a given instant (t) are: $$h(t)=3t^2+2;r(t)=8-\frac{t^2}{4}$$ Find the rate of change of the volume (V) and surface area (S) of the solid at $t = 4$ seconds. State whether (V) and (S) are increasing, decreasing or neither.
Note: The volume of Sphere = $\frac{4}{3}πr^3$ $(Ans: \frac{dV}{dt} = - 1105.84 (Deceasing))$ I tried this: $$\frac{dV}{dt}=\frac{\partial{V}}{\partial{r}}*\frac{dr}{dt}$$ and said: $$t=4$$ $$r(4)=4$$ $$\frac{\partial{V}}{\partial{r}}=4πr^2;\frac{dr}{dt}=\frac{-t}{2}$$ so: $$\frac{dV}{dt}=(4πr^2)*(\frac{-t}{2})$$ and the final result I get: $$\frac{dV}{dt}=128π$$ What am I doing wrong?","['volume', 'calculus', 'linear-algebra', 'partial-derivative', 'derivatives']"
3243045,Estimating quality of projection,"Suppose we are given a vector $v$ and vectors $\mu_i$ : $v = \mu_1+\mu_2+...+\mu_m$ , where $\mu_i \in R^n$ , all $\mu_i$ are of unit length. Oracle will give me $k$ vectors $\mu_{j_1}, \mu_{j_2},...\mu_{j_k}$ from the original set such that when I project $v$ onto subspace spanned by these vectors the length of the projection is highest possible. In other words, from the set of all combinations of $k$ vectors from $[\mu_1,...\mu_n]$ the $[\mu_{j_1}, \mu_{j_2},...\mu_{j_k}]$ give highest length of projection. Lets denote by $v_{\text{proj}}$ projection of $v$ onto $[\mu_{j_1}, \mu_{j_2},...\mu_{j_k}]$ I want to estimate quality of projection before oracle gives me this $k$ vectors. I want to give upper bound on $||v - v_{\text{proj}}|| $ As far as I understood it is very difficult to obtain these $k$ vectors by myself. However, I know  that for any two vectors $\mu_i, \mu_j$ , $||\mu_i-\mu_j|| \leq \alpha$ , where $\alpha$ is  a given positive number. Small values of $\alpha$ will tell me that all $\mu_i$ are close to each other and heading towards same direction. I would suspect then that projection will be good, and its length will be close to the length of original vector. How can I use this to give an upper bound $||v - v_{\text{proj}}|| $ ? My attempts : Without loss of generality lets assume that $k$ optimal vectors are first $k$ vectors in the list, i.e $\mu_1,\mu_2,...\mu_k$ . Lets denote by $P$ projection operator on the space spanned by $\mu_1,\mu_2,...\mu_k$ . $\|v - v_{\text{proj}}\|  = \|v - P(v)\| = \|v - P(\mu_1+\mu_2+...+\mu_m)\| = $ $\|v - P(\mu_1) - P(\mu_2) - ... - P(\mu_m)\| = $ $ \| v - \mu_1 - \mu_2 - ... - \mu_k - P(\mu_{k+1}) - P(\mu_{k+2}) - ... - P(\mu_m)\| = $ $\|\mu_{k+1} - P(\mu_{k+1}) + \mu_{k+2} - P(\mu_{k+2}) + ... + \mu_{m} - P(\mu_{m})\|$ $\|v - v_{\text{proj}}\| \leq \|\mu_{k+1} - P(\mu_{k+1})\| + \|\mu_{k+2} - P(\mu_{k+2}) + ... + \|\mu_{m} - P(\mu_{m})\|$ $\|v - v_{\text{proj}}\| \leq (m-k)\alpha$ So in order to make $\|v - v_{\text{proj}}\| \leq \epsilon$ , we need $k \geq \frac{m\alpha - \epsilon}{\alpha}$ I am not satisfied with this result because $k$ grows linearly with $m$ . I want it to grow much slower, something like $\log(m)$ . My goal is to show that under some constraints on $\mu_i$ , we need only approximately $\log(m)$ vectors to approximate $v$ . I think the bound can be improved substantially. First Cauchy inequality isn't very tight and second, I used $|\mu_{k+1} - P(\mu_{k+1})\| \leq \alpha$ which is also very loose. I am open for additional constraints on $\mu_1,...\mu_m$ to achieve logarithmic growth As Alex Ravsky has noted, we also need a constraint on $\alpha$ in order to achieve logarithmic growth. Assume that $m$ $\leq n$ , $\mu_i$ is th $i$ -th standard ort of the space $\mathbb{R}^n$ , and $\alpha = \sqrt{2}$ . Then $\|v -  v_{\text{proj}}\| = \sqrt{m-k}$","['projection', 'inequality', 'linear-algebra']"
3243071,Examine differentiability of $f$,"Examine the differentiability of $f$ : $$f(x,y) = \begin{cases}\displaystyle \frac{x^3}{x^2+y^2} &   (x,y) \neq (0,0) \\\\ 0 & (x,y) = (0,0) \end{cases}\:\:. $$ Let's check if $f$ is continuous: let $(x,y) \longrightarrow (0,0)$ . Then $$ \left |\frac{x^3}{x^2+y^2}\right| \le |x|\cdot \left|\frac{x^2}{x^2+y^2}\right | \le |x| \longrightarrow 0, $$ so this is ok. Now let's check the continuity of partial derivatives: \begin{align} \frac{\partial f}{\partial y}(x,y) &= -\frac{2 x^3 y}{\left(x^2+y^2\right)^2}, \\
\frac{\partial f}{\partial x}(x,y) &= \frac{3 x^2}{x^2+y^2}-\frac{2 x^4}{\left(x^2+y^2\right)^2}. \end{align} But I am not sure what should I do now?","['multivariable-calculus', 'derivatives']"
3243088,Pythagoras theorem is a^2 + b^2 = c^2 and a circle has an equation x^2 + y^2 = a^2 .Is there a relation between a right angle triangle and a circle?,I was just curious about  the fact that whether such a relation exists when I came across the equation of a circle.(I maybe absolutely  wrong)  .,"['euclidean-geometry', 'trigonometry', 'circles', 'triangles']"
3243121,"There are no Lebesgue measurable sets $A,B \subset \mathbb{R}$ such that $A \times B \subset S$.","Let $S = \mathbb{R}^2 \setminus \{ (x,y): x+y \in \mathbb{Q} \}.$ Show that there are no Lebesgue measurable sets $A, B \subset \mathbb{R}$ of positive Lebesgue measure for which $A \times B \subset S$ . I don't really know what I'm given and what to work with. It's obvious that $A, B$ cannot have any rational numbers. But I'm not sure how to start. Could someone help me with this?","['measure-theory', 'lebesgue-measure']"
3243131,Use real integral to calculate $\int_R \frac{x^2 \cos (\pi x)}{(x^2 + 1)(x^2 + 2)}dx$,Problem : Evaluate $$\int_{-\infty}^{\infty} \frac{x^2 \cos (\pi x)}{(x^2 + 1)(x^2 + 2)}dx$$ Use only real integral. What I did : $$\int_{-\infty}^{\infty} \frac{x^2 \cos (\pi x)}{(x^2 + 1)(x^2 + 2)}dx$$ $$=2\int_{0}^{\infty} \frac{x^2 \cos (\pi x)}{(x^2 + 1)(x^2 + 2)}dx$$ $$=2\int_{0}^{\infty} \left( \frac{2\cos(\pi x)}{x^2+2} - \frac{\cos (\pi x)}{x^2 + 1}\right) dx$$ Any easy way to calculate this? or idea like differentiate under the integral sign?,"['integration', 'calculus', 'definite-integrals']"
