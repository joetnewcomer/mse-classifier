question_id,title,body,tags
1946054,Borel measurability of a set where convergence occurs,"Let $F:\mathbb R^2\to\mathbb R$ be a jointly Borel measurable function. I am wondering if the set $$\left\{x\in\mathbb R\,\bigg|\,\lim_{y\to0}F(x,y)=0\right\}$$ is Borel measurable. The main difficulty is that even though the set $$\left\{x\in\mathbb R\,\bigg|\,\lim_{n\to\infty}F(x,y_n)=0\right\}$$ is Borel measurable for a given sequence $(y_n)_{n\in\mathbb N}$Â converging to $0$, this must hold for all, uncountably many , such sequences. Therefore, taking intersections does not help. Any input would be greatly appreciated.","['real-analysis', 'measure-theory']"
1946085,Proof of Binomial Sum via Double Counting,I have attempted to double count the following equivalence but to no avail. I'm unable to arrive at the Left Hand Side. $\displaystyle \sum_{k=0}^n \frac{(-1)^k}{k+3} \binom{n}{k} = \frac{2}{(n+1)(n+2)(n+3)} = \frac{1}{n+1}-\frac{2}{n+2}+\frac{1}{n+3}$,"['combinatorics', 'binomial-coefficients', 'probability', 'combinatorial-proofs']"
1946147,"If $[G:H]=m$, and $N\unlhd H$ why $G/N\cong K\leq \text{S}_m$?","I have read this statement, but I don't find why it's true: Let $G$ be a group and $H\leq G$ with $[G:H]=m$. Let $N$ be the largest normal subgroup of $G$ contained in $H$. Then there is a group homomorphism $\varphi\colon G\longrightarrow \text{S}_m$ with kernel $N$. In particular $G/N$ is isomorphic to a subgroup of $\text{S}_m$. I understand that we can see $G/N$ as a set of cosets, but I don't see why this lets me make the homomorphism. Can anyone explain me why is true?","['permutations', 'group-actions', 'group-theory', 'symmetric-groups']"
1946161,$f:X\rightarrow X$ such that $f(f(x))=x$,"Let $X$ be a metric space and $f:X\rightarrow X$ be such that $f(f(x))=x$, for all $x\in X$. Then $f$ is one-one and onto; is one-one but not onto; is onto but not one-one; need not be either. From the given condition I have that $f^2=i$ where $i$ is the identity function. If $f$ itself is the identity function then the conditions are satisfied as well as $f$ is bijection. Is that the only such function or are there other possibilities ? My guess is that it will be bijection i.e. option $1$ will be correct . For see, if $$f(x_1)=y \ \text{and}\ f(x_2)=y \ \text{then} \ f(y)=x_1\ \text{and}\ f(y)=x_2$$ will be possible iff $x_1=x_2$. So this is injective. Now an injection from a set to itself is trivially surjective so it is bijective.   Is my proof correct?","['elementary-set-theory', 'functions']"
1946188,Prove De Morgan's Laws (Set Theory),"Statement: Let $A$ and $B$ be subsets of $R$.  Show that $(A \cup B)'=A'\cap B'$. My attempt: To show that two sets are equal to one another we have to show that $(A \cup B)' \subset A'\cap B'$ and $  A'\cap B'\subset (A \cup B)'$.  Beginning with the former, suppose that $x \in (A \cup B)'$ then $x\notin A \cup B$. This implies that $x\notin A$ or $x\notin B$, which in turn should imply that $x\in A'$ or $x\in B'$. However the author of my textbook states that: $$x\notin A \text{ or } x\notin B \Rightarrow x\in A' \text{ and } x\in B'$$ Whereas, I think that the following should be true: 
$$x\notin A \text{ or } x\notin B \Rightarrow x\in A' \text{ or } x\in B'$$ Why did the or change into an and ?",['elementary-set-theory']
1946243,Is there a nontrivial analytic function which takes the same values on disjoint circles?,"Given two disjoint circles $C_1$ and $C_2$ in the complex plane, is there a nontrivial analytic function $f(z)$ whose values on $C_1$ are the same as its values on $C_2$ up to rotation and dilation? What little intuition I have about this suggests not. If we had a function $f = u+i~v$ and think of the harmonic functions $u,v$ as potential distributions, we could consider the special case of two nearly osculating circles of very different size. Informally (and far too vaguely) the mutual patterns of influence of the two circles seem qualitatively different, so it seems unreasonable to suppose the functions could be the same. This argument does not work for circles of the same size. As a guess we would have that $f(z)=u(x,y)+i~v(x,y) = f(w(z))$ for some Mobius transformation $w(z)$ and maybe the Cauchy-Riemann equations cannot be satisfied? I was hoping for an obvious geometric argument but maybe it's not possible? Edit: It seems that this may be possible for same-radius circles (I didn't want to rule these out) and I would upvote/accept an answer along these lines, but would add 100 points to an answer that proves it's impossible [ re-edit : and kal v'chomer possible] for different-radius circles.","['complex-analysis', 'analytic-functions', 'harmonic-functions']"
1946245,Solving linear Ordinary differential equations with variable coefficients,$$x^3y'' - 3x^2y' + xy = \sin \ln x + 1; y' = \frac{d}{dx}$$ How can I solve this equation?,['ordinary-differential-equations']
1946285,What is wrong with the solution?,"For given conditions, $\frac{\partial u}{\partial t}=0$ at $x=0$ & $u=0$ at $t=0$ . The solution of the differential equation $$\frac{\partial^2u}{\partial x\partial t}=e^{-t}\cos x$$ is given by a) $(1-e^{-t})\sin x\qquad $ b) $(-e^{-t})\sin x+g(x)\qquad $ c) $(1+e^{-t})\cos x\qquad $ d) $(-e^{-t})\sin x\qquad $ my try: $$\frac{\partial}{\partial x}\left(\frac{\partial u}{\partial t}\right)=e^{-t}\cos x$$ $$\int \partial \left(\frac{\partial u}{\partial t}\right)=\int e^{-t}\cos x\ \partial x$$ $$\frac{\partial u}{\partial t}  =e^{-t}\sin x+f(t)$$ putting $x=0$ & $\frac{\partial u}{\partial t} =0$ i get $f(t)=0$ so $$\frac{\partial u}{\partial t}  =e^{-t}\sin x$$ $$\int \partial u =\int e^{-t}\sin x\partial t$$ $$u=-e^{-t}\sin x+g(x)$$ now, putting $t=0$ & $u=0$ i get $g(x)=\sin x$ , so the solution is $$u=-e^{-t}\sin x+\sin x$$ $$=(1-e^{-t})\sin x$$ so the option is (a) but my book says the correct option is (d) i don't know why.
please explain where i am wrong or what should be the right answer?  thank you very much","['partial-derivative', 'ordinary-differential-equations', 'calculus']"
1946295,Is this Galois theory proof of Fundamental Theorem of Algebra correct?,"I am studying Galois theory through Lang's Algebra and Dummit-Foote's Abstract Algebra. While studying the Fundamental Theorem of Algebra's proofs from both books I spent a lot of time to understand and in the process tried to simplify or rewrite the proof. I like to do this several times. For the proof we need two facts or results as follows: (a) There are no non-trivial finite extensions of $\Bbb R$ of odd degree. (b) There are no quadratic extensions of $\Bbb C$ . Fundamental Theorem of Algebra : $\Bbb C$ is algebraically closed. Proof : Since $\Bbb R$ has characteristic $0$ , every finite extension is separable. Hence $\Bbb R(i)/ \Bbb R$ is separable (Because $\Bbb R(i)/ \Bbb R$ is finite extension). $\Bbb R(i)=\Bbb C$ is contained in a finite Galois extension $K$ over $\Bbb R$ . ( By Corollary 23(Dummit-Foote): Let $E/F$ be any finite separable extension. Then $E$ is contained in an extension K which is Galois over $F$ and is minimal in the sense that in a fixed algebraic closure of $K$ any other Galois extension of $F$ containing $E$ contains $K$ . We used $E=\Bbb R(i)$ and $F=\Bbb R$ . ) Let $G$ be the Galois group of $K/ \Bbb R$ . Using fact (a) , since there are no non-trivial finite extensions of $\Bbb R$ of odd degree, we have $|G|$ is even. Therefore $|G|=2^n m$ where $m$ is an odd number and $n \ge 1$ . Let $H$ be a sylow $-2-$ subgroup of $G$ and $F$ be the fixed field of $H$ . Hence $|G:H|=m=|F:\Bbb R|$ . But again by fact (a) , $|G:H|=m=1$ $\Rightarrow G=H$ is a $2-$ group. We know that p-groups have subgroups of all orders and they all are normal subgroups. Also $[K:\Bbb R]=[K: \Bbb R(i)][\Bbb R(i): \Bbb R] \Rightarrow 2^n=[K: \Bbb R(i)](2) \Rightarrow [K: \Bbb R(i)]=2^{n-1}.$ Hence Gal $(K/\Bbb R(i))$ is a $2-$ group of order $2^{n-1}$ where $n \ge 1$ where $n \gt 1$ would mean that this group is non-trivial and $n=1$ would mean that it is trivial. If $n \gt 1$ , Since $2-$ groups have subgroups of all orders (Being p-groups), there exists an extension of $\Bbb R(i)=\Bbb C$ of order $2$ which is contradiction to fact (b) . So we can say that $n=1$ and Gal $(K/ \Bbb R(i))=1.$ Hence $K=\Bbb R(i)=\Bbb C$ . I have ommited proofs of facts (a) and (b) as they are precisely the same as in Dummit and Foote. Also I have mentioned only those things of which I want to be sure whether they are correct or not.","['galois-theory', 'p-groups', 'proof-verification', 'group-theory', 'field-theory']"
1946332,Why are (S)pinor representations *restrictions* of Clifford algebra representations?,"Firstly, just to clarify my notation: Let $Cl(V,q)$ denote the Clifford Algebra of a quadratic vector space $(V,q)$ and denote by $Cl(V,q)_{0\vert 1}$ the even/odd part in the $\mathbb{Z}_2$-grading of $Cl(V,q) = Cl(V,q)_0 \oplus Cl(V,q)_1$ of the Clifford-algebra. Now for the subgroups $Pin(V,q) \subset Cl(V,q)$ and $Spin(V,q)\subset Cl(V,q)_0$ is is defined: A pinor representation is the restriction of an irreducible
  representation of $Cl(V,q)$ onto $Pin(V,q)$. Similary a spinor
  representation is the restriction of an irreducible representation of
  $Cl(V,q)_0$ onto $Spin(V,q)$. My question is: What is the reason in defining pinor/spinor-representations as the restrictions of Clifford algebra representations , rather then just as usual group-representations of the groups themselves? Remark: ""Physical"" explanations (as: 'The so defined spinor fields wouldn't behave like spinors, since...') are also very welcome.","['spin-geometry', 'mathematical-physics', 'representation-theory', 'clifford-algebras', 'differential-geometry']"
1946359,Is there a second derivative test in polar coordinates ? How to calculate concavity in the radial direction?,I searched google for articles talking about second derivative test in polar coordinates and found nothing at all. Could you please refer me to an article or a book about it? I'm asking because I want to calculate the concavity of a surface at origin in the $radial$ direction as a function of $\theta$. Thanks for help.,"['multivariable-calculus', 'partial-derivative', 'polar-coordinates']"
1946363,Find N from $2^N - 1= R$,"I have the following formula, which gives the range of bits when provided a number of bits which is $N$: $$2^N-1=R$$ How would we transform the equation to be like: $$N = \ldots$$","['number-theory', 'limits']"
1946385,Proof of differentiability of $f(x)$ [duplicate],"This question already has answers here : Show that $f$ is differentiable on $(â1, 1)$. (3 answers) Closed 3 years ago . Let $f$ be a function defined on the interval $(-1,1)$ such that for all $x,y\in(-1,1)$, $f(x+y)=\frac{f(x)+f(y)}{1-f(x)f(y)}$. Suppose that $f$ is differentiable at $x=0$. Show that $f$ is differentiable on $(-1,1)$. For me I can see that $\tan(x)$ is obviously a case of $f$, but I cannot find a general proof for this $f$. Could any kind soul help?","['derivatives', 'calculus']"
1946390,Find all primes $p$ for which $\frac{2^{p-1}-1}p$ is a perfect square,Find all primes p for which the quotient $\frac{2^{p-1}-1}p$ is a perfect square. It is a number theory problem. By guessing I found the value of $p$ is 3. But how do I prove it and how do I found the other primes?,['number-theory']
1946397,"Probability that a Brownian motion with drift hits +1 before hitting -1 before time 1, and similar events","[Searched for this question and found something similar, but not identical. I hope this is not a duplicate. I don't know if this is a very easy question, I took my stochastic processes course too much time ago.] Let $(B_t)_{0 \leq t \leq 1}$ be a standard Brownian motion and consider the process (for $0 \leq t \leq 1$, $\mu \in \mathbb{R}$, $\sigma > 0 \in \mathbb{R}$) $$
X_t = \mu t + \sigma B_t
$$ Consider the two stopping times $$\tau_1 = \inf \left\{ 0 \leq t \leq 1: X_t =1 \right\}$$ and $$\tau_{-1} = \inf \left\{ 0 \leq t \leq 1: X_t = -1 \right\} $$ How can I compute the following quantities? $$
\mathbb{P}(\tau_1 \leq \tau_{-1} \leq 1)
$$ $$
\mathbb{P}(\tau_{-1} \leq \tau_1 \leq 1)
$$ $$
\mathbb{P}( \left\{ \tau_1 > 1 \right\} \cap \left\{\tau_{-1} > 1\right\})
$$","['probability-theory', 'brownian-motion']"
1946422,Prove the transitivity of $<$ relation in natural numbers using the classical definition and the associativity of addition,"In the book Notes on Mathematical Analysis the set $\mathbb{N}$ of natural numbers was defined using the classical Peano's axioms. Then $<$ was defined as if exist $k\in \mathbb{N}$ such that $b=S^k(a)$ then we say $a<b$ . Then it was asked to prove the transitivity of $<$ relation. I proceed as follows: if $a<b$ then by definition there is a $k$ , $b=S^k(a)$ . Same for $b<c$ , $c=S^l(b)$ . Then it will comes that $c=S^{k+l}(a)$ , which means $a<c$ . I can't find errors in my proof but in the book's hint this transitivity should be proved by the associativity which proved before. I can't find the required proof either. Any help will be greatly appreciated. ps.
Moreover, in the next section another definition of $\mathbb{N}$ appears as: Given a set $N$ and a map $S:N\rightarrow N$ , such that a) $1\notin S(N)$ ; b) $S$ is injective; c) The set $N$ is well-ordered, that is, for any non-empty subset $E$ of $N$ , there exists $m\in E$ , $\forall n\in E(m\le n)$ . The $<$ definition is the same as above. In this case it is also asked to prove the transitivity of $<$ , as well as $N=\mathbb{N}$ . This also confuse me since I can't tell the difference between this two definitions, and I think the prove to things like ""among $a<b, b<a, a=b$ exactly one is correct"" and ""1 is the minimal number in $N$ "" is the same as before. Please help. Thanks in advance.","['real-analysis', 'elementary-set-theory']"
1946438,Solving $e^{e^z}=1$: am I missing something?,"I solved the equation $e^{e^z}=1$ and it seemed to easy so I suspect I must be missing something. Would someone please check my answer? My original answer: $e^{e^z}=1$ if and only if $e^z = 2\pi i k$ for $k\in \mathbb Z$ if and only if $z=\ln(2\pi i k)$ for $k\in \mathbb Z$. Edit After reading the comments and answers I tried to do it again. Unfortunately, I still do not get the same result as in the answers. My second attempt: We have $$ e^x = 1 \iff x = 2 \pi i k$$ hence $$ e^z = 2 \pi i k$$ for some $k$ in $\mathbb Z$. Letting $e^z = e^x (\cos y + i \sin y)$ we get $$ e^x \cos y + i e^x \sin y = 2 \pi k i$$ which implies that $\cos y = 0$ which happens if and only if $y_j = {\pi \over 2} + \pi j$ where $j\in \mathbb Z$. At $y_j$ we have
$\sin y = \pm 1$ hence if $j$ is even $$ e^x = 2 \pi i k$$ and if $j$ is odd $$ e^x = -2 \pi i k$$ Hence if $j$ is even, $$ x = {\pi \over 2} + \ln(2 \pi k)$$ and if $j$ is odd, $$ x = {3\pi \over 2} + \ln(2 \pi k)$$ So we see that the solutions are $$
z_{t,k}=\begin{cases}
 {\pi \over 2} + \ln(2 \pi k) + i ({\pi \over 2} + 2t \pi )\\ 
 {3\pi \over 2} + \ln(2 \pi k) + i({\pi \over 2} + (2 +1)t \pi )
\end{cases} 
$$
for $k,t \in \mathbb Z$. What am I doing wrong?","['complex-analysis', 'proof-verification']"
1946476,An alternating sum with binomial coefficients,"How to calculate this sum
$$
\sum_{k=0}^{n-1}(-1)^k{n \choose k} {3n-k-1 \choose 2n -k }
$$
without complex integral technique? Or how to calculate asymptotic nature this sum without calculation of this sum?","['combinatorics', 'summation']"
1946502,kronecker product of three matrices,"Facts: For matrices $A_i\in \mathbb{R}^{n\times n}$ with $i=1, 2, 3$, we have the following equation:
$$ A_1\otimes A_2 \otimes A_3 = (A_1\otimes I_{n^2})(I_{n}\otimes A_2 \otimes I_n)(I_{n^2}\otimes A_3),
$$
where $I_n$ represents an $n$ by $n$ identity matrix, and $\otimes$ denotes the Kronecker product. My question is what would happen in the equation if matrices $A_i$ are not square matrices, i.e., $A_i\in \mathbb{R}^{m\times n}$, where $m,n$ are not necessarily equal. Is there a way to prove it? Thank you very much! Pulong","['matrices', 'kronecker-product']"
1946511,"For vector p-norm, can we prove it is decreasing without using derivative?","For vector p-norm defined as $(â_{i=1}^n x_i^p )^{\frac{1}{p}}$ for any $p\ge 1$ and vector ${\bf{x}}=\{x_1,...,x_n\}$. The following proves it is decreasing with respect to $p$ by taking derivative (you don't need to read the whole proof, just have a look), However, I am thinking if there is another approach without using the derivative. Is there any proof for monotonicity of p-norm without using derivatives? The upper bound can be proved by Holder's inequality by Relations between p norms We have plenty of inequalities that lead to the definition of p-norm: Young's inequality, Jensen's inequality, Holder's inequality, Minkowskiâs inequality. Maybe there is a proof using those inequalities?","['normed-spaces', 'linear-algebra']"
1946514,Number of bit strings of length $n$ with no $k_1+1$ consecutive 0s and no $k_2+1$ consecutive 1s.,"Just as the question asks. I am trying to calculate the number of bit strings of length $n$ with a maximum of $k_1$ consecutive $0s$ and $k_2$ consecutive 1s. Of course we assume $k_1+k_2\leq n$. I am trying to set up a recurrence but I am completely puzzled. I know that to not get two consecutive 0s, we have the recurrence $a_n=a_{n-1}+a_{n-2}$ with $a_1=2$ and $a_2=3$. However I can't seem to generalize this to no more than $k_1$ $0s$ especially that you have the second condition of no more than $k_2$ 1s.","['computer-science', 'statistics', 'permutations', 'combinatorics', 'discrete-mathematics']"
1946517,Convergence of sums of independent random variables,"Hey I need some help with a problem: Let $X_{n1},X_{n2},...,X_{nn}$ be independent random variables with a common distribution as follows:
$$P(X_{nk}=0)=1-\frac{1}{n}-\frac{1}{n^2}, P(X_{nk}=1)=\frac{1}{n}, P(X_{nk}=2)=\frac{1}{n^2}, $$ where k=1,2,.. and n=2,3,... $S_{n}=X_{n1}+X_{n2}+...+X_{nn}$, $n>=2$ Show that $S_{n}\stackrel{d}{\rightarrow}Po(1)$ as n $\rightarrow \infty$ I've started like this: $$E[e^{tS_{n}}]=E[e^{tX_{n1}+tX_{n2}+...+tX_{nn}}]=\prod_{k=1}^{n}E[e^{tX_{nk}}]=\prod_{n=2}^{\infty}(1-\frac{1}{n}-\frac{1}{n^2}+\frac{1}{n}e^{t}+\frac{1}{n^2}e^{2t})$$ but I'm not sure I'm going with the right approach. Does someone have any tips for me?","['probability-theory', 'expectation', 'statistics']"
1946570,Rudin's statement of Hahn-Banach's theorem,"Rudin's statement of the Hahn-Banach theorem in Functional Analysis (Theorem 3.2) involves a linear form $f$ defined on some subspace of a real vector space, and which is bounded by a sublinear function, as usual. But the specific hypotheses assumed about this sublinear function are: $p(x+y) \leq p(x)+p(y)$ For non-negative $t$ , $p(tx)=p(x)$ (!!) Shouldn't it be $p(tx)=tp(x)$ ? The proof seems to implicitly use the latter condition, and not condition (2) quoted above. But I can't find references to any errata online. Is the theorem still true with this different assumption on $p$ ? If so, how do you deduce from it that it also works when $p$ is a semi-norm? Rudin basically just says that if $p$ is a semi-norm, the result is ""contained in Theorem 3.2"", but a semi norm does not satisfy $p(tx)=p(x)$ .",['functional-analysis']
1946604,Power sets in set theory,"What is the set $\mathcal{P}(\mathcal{P}(\mathcal{P}( \emptyset )))$? Would it be $\{\{\{ \emptyset\}\}\}$? I understand that $\mathcal{P}(\{a,b,c\}) = \{ \{ \}, \{a\}, \{b\}, \{c\}, \{a,b\}, \{a,c\}, \{b,c\}, \{a,b,c\} \}$.",['elementary-set-theory']
1946618,"Finding $E[X]$, $\operatorname{Var}(X)$ and distribution of $X$ in Bernoulli trials","Let $X$ be the number of Bernoulli ($p$) trials required to produce at least one success and at least one failure. What is the distribution of $X$? Also, what is $E[X]$ and $\operatorname{Var}(X)$? To find $\operatorname{Var} (X)$ I know that I'll need to use the formula $E\left[X^2\right]-\left(E\left[X\right]\right)^2$ after I find $E[X]$ and also $E\left[X^2\right]$. How would I do that? I think that $X$ has a geometric distribution, but how do I show that?","['probability-theory', 'probability', 'probability-distributions']"
1946654,Reference request: multivariable (homogeneous) polynomial division algorithm,"Reference request: multivariable (homogeneous) polynomial division algorithm I am looking for a reference for a result that shows that a (homogeneous) polynomial in an even number of variables (or any number of variables) can be divided by linear polynomials corresponding to roots. Context: The specific application I have in mind is rigorously proving the following facts related to resultants: 1. Show that $\det (Syl (f,g)) = \lambda\ \Pi_{i=1,j=1}^{i=n,j=m} (r_i - s_j)$ , where $Syl(f,g)$ is the Sylvester matrix of the degree $n$ polynomial $f$ and the degree $m$ polynomial $g$ . 2. Given: $\det (Syl (f,g)) = \lambda\ \Pi_{i=1,j=1}^{i=n,j=m} (r_i - s_j)$ , show that $\lambda = 1$ . Basically I want to show that since the points $(r_i : s_j)$ are obviously zeros of the determinant of the Sylvester matrix, the corresponding linear polynomials divide $\det(Syl(f,g))$ (for the first part). This could be done I think if I could cite the existence of a suitable multivariable polynomial division algorithm. For the second part, I'll figure it out, but now I have no idea besides Vieta's formulas. Related questions whose answers don't answer my question (I think): Algorithms for factoring multivariate polynomials Division algorithm of multivariate polynomial Division algorithm for multivariate polynomials? How to show that a root of a bivariate homogeneous polynomial divides the polynomial?","['algebra-precalculus', 'abstract-algebra', 'reference-request', 'algebraic-geometry']"
1946659,About the differentiability of a Weierstrass-like function,"Let we consider: $$f(x)=\sum_{n=1}^{\infty}\frac{\sin(2^nx)}{n^2}$$
Since $\left|\sin x\right|\leq 1$ and $\sum_{n\geq 1}\frac{1}{n^2}<2$, the above series is uniformly convergent and $f(x)$ is a continuous function. The series associated with the formal derivative, $\sum_{n\geq 1}\frac{2^n\cos(2^n x)}{n^2}$, looks like it is not convergent for any $x\in\mathbb{R}$. Is it sufficient to prove this fact, to deduce that $f(x)$ is not differentiable at any $x\in\mathbb{R}$?","['derivatives', 'real-analysis']"
1946690,Inverse trig function equation [duplicate],This question already has answers here : Why it's true? $\arcsin(x) +\arccos(x) = \frac{\pi}{2}$ (8 answers) Closed 7 years ago . How would you suggest I go about solving this question? I've been thinking about it for ages and nothing comes to mind. $$\arcsin x + \arccos x = \frac{\pi}{2}$$,['trigonometry']
1946713,Inverse of a $2 \times 2$ block matrix,"Let $$S := \pmatrix{A&B\\C&D}$$ If $A^{-1}$ or $D^{-1}$ exist, we know that matrix $S$ can be inverted. $$S^{-1} = \pmatrix{A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1}&-A^{-1}B(D-CA^{-1}B)^{-1}\\-(D-CA^{-1}B)^{-1}CA^{-1}&(D-CA^{-1}B)^{-1}}$$ But, what if $A^{-1}$ and $D^{-1}$ do not exist? Can we invert matrix $S$ ? For example, $$S = \pmatrix{0&1\\1&0}$$ or $$S = \pmatrix{2&3&1&1\\4&6&1&2\\1&1&3&1\\4&1&12&4}$$ both their $A^{-1}$ and $D^{-1}$ don't exist, but $S^{-1}$ exists.","['block-matrices', 'matrices', 'schur-complement', 'inverse', 'linear-algebra']"
1946714,"Finding $m$ so that the roots of $x^2 - (3m - 1)x + m^2 - 2 = 0$ lie inside the interval $(1, 5)$","Find $m$ so that the roots of the equation $x^2 - (3m - 1)x + m^2 - 2 = 0$ lie inside the interval $(1, 5)$. Having $x_1, x_2 \in (1, 5)$, we get $x_1 + x_2 \in (2, 10)$ and $x_1x_2 \in (1, 25)$. By solving these two inequations I got $m \in \left( \sqrt3, \frac{11}{3} \right)$. However, this result is wrong. By substituting $m$ with $2$, for example, the given conditions are not reached. So, please, explain what I did wrong and how to get the right solution. Thank you in advance!","['algebra-precalculus', 'quadratics']"
1946722,Tangent space of Jets pace,"I would like to understand what the tangent space of a jet space is. For example if I have a map $f:X \to Y$, where $X$ and $Y$ are manifolds and I have the k-jet extension $j^kf(x):X \to J^k_x(X,Y)$ which can be understood a section into the jet space, then how can I picture $T_{j^kf(x)}J^k(X,Y)$. Intuitively I would say that this should be related with the space $J^{k-1}(X,Y)$ but I can't get my head around this. If somebody knows a good source for this topic I would be really thankful.","['differential-geometry', 'differential-topology', 'jet-bundles']"
1946734,When is the center manifold attractive and how does this dictate asymptotic behaviour?,"Assuming some knowledge of center manifold theory, I would like more details on the following statement found on the Wikipedia page https://en.wikipedia.org/wiki/Center_manifold . ""The center manifold emergence theorem then says that the neighborhood may be chosen so that all solutions of the system staying in the neighborhood tend exponentially quickly to some solution $y(t)$ on the center manifold. That is, $x(t) = y(t) + O( e^{â\beta t} )$ as  $t \to \infty$ for some rate $\beta$."" I have never seen a statement of the center manifold theorem where the proof of this result is explicit. The attractivity part of the Theorem in Ioos and Adelmeyer (referenced by Wikipedia) is stated without proof. In Kuznetsov's book it states that a necessary condition for a center manifold to be attractive is that the unstable manifold vanishes, but not that this condition is sufficient. Guckenheimer + Holmes only state the Theorem of Henry and Carr that says if the unstable manifold vanishes and the origin is a stable solution of the reduced equation, then it is a stable solution of the whole system. In summary, I am looking for a proof of the intuitive fact that, if the unstable manifold is empty, for large time any trajectory starting in the neighborhood of the equilibrium point (taken w.l.o.g. to be the origin) approaches a solution of the reduced equation on the center manifold, regardless the nature of the origin as an equilibrium of the reduced equation. Motivating example is the system
\begin{equation*} \dot{x} = A x + f(x,y), \quad \dot{y} = By +g(x,y)\end{equation*} 
where $A \in \Bbb R^{2 \times 2}$ has an imaginary pair of eigenvalues and all eigenvalues of $B \in \Bbb R^{n \times n}$ have negative real part. Suppose the origin is a center of the reduced equation $\dot{x}=Ax+f(x,h(x))$ on the center manifold $y= h(x)$. Then my intuition says solutions approach a periodic solution of reduced equation that is realized as a limit cycle of the total system. When is this the case? What happens if the reduced equation admits a family of periodic orbits?","['asymptotics', 'ordinary-differential-equations', 'dynamical-systems']"
1946748,complex number equation,"I have this equation:
$$z^2=-i$$ All I can figure out from my knowledge is that $$z^2=r^2\cos(2x)+i\sin(\cos(2x))$$ in this case is: $$\cos(3\pi/4)+i\sin(3\pi/4))$$
and that will be $$e^{3\pi/4}$$ because $-i =0-i$ so that $\cos2x=0$ and $\sin2x=-1$. Am I on the right track? I feel like I'm missing something. The answer should be $\pm(-1+i)/\sqrt{2}$. where does the $\sqrt{2}$ come from?","['trigonometry', 'complex-numbers']"
1946768,Solving $\lim\limits_{x \to 0} \frac{\ln(1+x)}{\ln(1+4x+x^2)}$ without L'Hospital or Sandwiching,"Using the L'Hospital I got: $\lim\limits_{x  \to 0} \frac{\ln(x+1)}{\ln(1+4x+x^2)} = \lim\limits_{x  \to 0}\frac{\frac{1}{1+x}}{\frac{4+2x}{1+4x+x^2}}= \frac{1}{4}$, I then wondered, if I could sandwich it andobserved that for: $\frac{x-1}{x} \le \ln(x) \le x-1 $, that:$ \frac{1}{4} \xleftarrow{x \to 0} \frac{\frac{x+1-1}{x+1 }}{1+4x+x^2-1} \le \frac{\ln(x+1)}{\ln(1+4x+x^2)} \le \frac{x+1-1}{\frac{1+4x+x^2-1}{1+4x+x^2}} \xrightarrow{x \to 0}\frac{1}{4} $ I then however wondered, if it were directly possible to get the result by cleverly manipulating the variables, so that the term simplifies itself to $\frac{1}{4}$ in the limit. Simply substituting $x+1$ or $1+4x+x^2$ with $e^u$ seems rather cumbersone, if (!) it is constructive at all. I am afraid, that there is just no real nice simplification of above. But maybe someone knows another nice, elegant way to get this limit :) I am always happy to expand and practice my toolkit. 
As always thanks in advance.","['alternative-proof', 'limits']"
1946825,"Question on one to one correspondence implies same cardinality, for finite sets","I have been taking for granted the fact that given two finite sets $A$ and $B$, if there exists a bijective map then the number of elements in $A$ and $B$ are the same, because I thought this was a trivial fact. But then the same doesn't really hold (in some sense) when $A$ and $B$ have infinite elements, because if $A = 2 \mathbb{N}$ and $B = \mathbb{N}$ surely there is a bijective map from $A$ to $B$, but $A$ is a strict subset of $B$. This made me think that perhaps in the proof of the fact that given two finite sets $A$ and $B$ the existence of a bijective map implies $|A| = |B|$, the assumption that $|A|$ and $|B|$ are finite must be used. I was wondering if someone could tell me how to prove this fact(this very basic fact!) and point to me where the assumption is being used. Thank you very much!","['soft-question', 'elementary-set-theory', 'definition']"
1946881,Continuous time Markov processes on general state spaces,"Looking around I have found lots of material on continuous time Markov processes on finite or countable state spaces, say $\{0,1,\ldots,J\}$ for some $J\in\mathbb{N}$ or just $\mathbb{N}$. Similarly I have earlier worked with (discrete time) Markov chains on general state spaces, following the modern classic by Meyn & Tweedie. My question concerns monographs on continuous time Markov processes on general state spaces, say some subset of $\mathbb{R}^k$, $k\in\mathbb{N}$. Are there any good references - preferably but not necessarily suited for an ambitious master student - on this topic?","['reference-request', 'probability-theory', 'markov-process']"
1946897,"Is my proof valid for: If $A-B \subseteq C$ and $A \not \subseteq C,$ then $A \cap B \not = \emptyset$.","Just starting to learn proofs and could use some feedback: Statement:  Let A, B, and C be nonempty sets.  If $A-B \subseteq C$ and $A \not \subseteq C,$ then $A \cap B \not = \emptyset$. Proof:  Since $A \not \subseteq C$, there exists an $x \in A$ such that $x \not \in C.$ Since $x \not \in C$ and $A - B \subseteq C$, $x \not \in A - B$.  Since $x \in A$ and $x \not \in A - B$, $x \in B$. Hence $x \in A \cap B$.  That is, $A \cap B \not = \emptyset.$ Thank you. Idle Math Guy",['elementary-set-theory']
1946899,Probability of 4 consecutive heads in 10 coin tosses [duplicate],"This question already has answers here : Probability of tossing a fair coin with at least $k$ consecutive heads (5 answers) Closed 5 years ago . I am trying to compute the probability of having 4 (or more) consecutive heads in 10 coin tosses. I tried using recursion but it led to a complicated expression so i think i did not quite manage. I saw similar questions asked here that were solved with difficult approaches, but this problem looks like it could be solved in a couple of lines so I must be doing something wrong. If anyone could help me understand it or propose a different approach for solving the problem i would be very grateful. Have a nice day!","['probability-theory', 'probability', 'discrete-mathematics']"
1946906,Sum of series : $1+11+111+...$,"Sum of series $1+11+111+\cdots+11\cdots11$  ($n$ digits) We have: $1=\frac {10-1}9,$ $11=\frac {10^2-1}9$ . . . $11...11= \frac {10^n-1}9$ (number with $n$ digits) and summing them we find the sum ($S$) as: $S=(10^{n+1}-9n-10)/81$ Also the general form of terms is: $s(n)=(10^{n+1}-10^n-9)/81$ Now consider the function: $f(x)=10^{x+1}â10^x-9$ Since $\Delta x= 1$, due to definition of integral we can write: $S=(1/81)\sum (10^{x+1}â10^x-9), [1, â]$ $ =(1/81)â«(10^{x+1}-10^x-9) dx ;[0, 1]$ but it does not work. Can someone say what went wrong, i.e, Why doesn't the integral give $S$ as I mentioned first? I realized now that this is more a sequence rather than a series. A sequence is 
a set of numbers which are resulted from a general term where as a series is a 
set of functional elements; the derivative of elements of a sequence is zero and 
its integration is pointless. So using the integration of general term of a 
sequence to find its sum is just not needed.","['algebra-precalculus', 'arithmetic-progressions', 'summation', 'geometric-progressions']"
1946917,**Intuition** behind Lagrange Multipliers,"Why does this method actually work ?I learnt that a while ago and have been using it ever since. I apply it to almost every optimisation problem. This method just seemed too good to be true. Now, I wonder why this works. 
(The answer does not need to be rigorous, an intuitive sketch will work just fine for me.)","['lagrange-multiplier', 'calculus']"
1946919,Solving $\int_0^\infty\dfrac{dx}{(1+x^n)^n}=1$,"A friend has challenged me to find the value of $n$ such that $\displaystyle\int_0^\infty\dfrac{dx}{(1+x^n)^n}=1.$ I know that the value of $n$ is equal to $\phi,$ but I do not know how to prove this.","['integration', 'definite-integrals', 'calculus', 'closed-form']"
1946938,Notation for set excluding element,"Consider the set $\mathbb{A}=\{a,b,c\}$. I want to refer to ""the set $\mathbb{A}$ excluding the element $a$"" Is the notation $\mathbb{A} \sim  \{a\}$ equivalent to $\mathbb{A} \setminus \{a\}$? Is the former an abuse of logic notation? The latter is what I am used to.","['notation', 'elementary-set-theory']"
1946977,proving a non-zero polynomial $f$ has a point where it's not $0$ (on a field with infinite elements) [duplicate],"This question already has an answer here : Proving the algebraic set corresponding to a polynomial is infinite. [duplicate] (1 answer) Closed 7 years ago . Let $k$ be a field with infinite elements. Let $0 \not = f \in k [x_1, ..., x_n]$. I want to prove that there exists $P \in k^n$ such that $f(P) \not =0.$ I think I could do it if I use induction on the number of variables or degree of $f$ or something, but it looks like it gets a bit messy, and I am sure there is a ""slicker"" proof. Could someone please tell me such a proof (that is not too messy) if there is one out there? Thank you very much!","['abstract-algebra', 'field-theory', 'algebraic-geometry']"
1946992,Measurability and integrability,"If $f:X\to \mathbb{R}$, where $X$ is a measurable space equipped with a measure $\mu$ and $f$ is non-negative, we define $\int_X f = \sup\{\int_X s \mid s \text{ is a non-negative simple function less than } f\}$. But, I was thinking that this definition could be used to define an integral of $f$ even if it is not measurable. So, why do we talk only about integrals of measurable functions?",['measure-theory']
1947003,If $\sin(h(x))=f(x)$ and $\cos(h(x))=g(x)$ can we determine $h(x)$?,"If $\sin(h(x))=f(x)$ and $\cos(h(x))=g(x)$ for some fixed functions$f,g:\mathbb{R}\rightarrow [-1,1]$ can we determine $h(x)$? To be honest, I was asked to solve a problem with functions of the form $f,g:\mathbb{R}\rightarrow [-1,1]$ involved. So I thought if we could substitute $f,g$ with sines and cosines without loss of generality,(the same way we substitute real numbers with trigonometric functions when solving classical inequalities problems) since we were given that $f^2(x)+g^2(x)=1 \quad \forall x \in \mathbb{R}$. I know that functions are not the same with real numbers, but this is actually the whole reason of my question. Therefore my thought is the following: Since it is $f^2(x)+g^2(x)=1 \quad \forall x \in \mathbb{R}$ we can say that  $f(x)=\sin(h(x))$ and $g(x)=\cos(h(x))$ for some function $h(x)$ that its formula depends on $f(x)$ and $g(x)$. If the sine/cosine function was invertible in all its domain then we could determine $h(x)$ by applying the inverse function of sine in both sides of the equation $f(x)=\sin(h(x))$. But I'm stuck here...","['real-analysis', 'trigonometry', 'functions', 'calculus', 'analysis']"
1947071,"Proof for Let A, B be nonempty sets and let C, D be sets. If $A \times B \subseteq C \times D$, then $A \subseteq C$ and $B \subseteq D$.","Trying to develop a proof for the following statement.  I don't think my proof is quite correct. Statement: Let A, B be nonempty sets and let C, D be sets.  If $A \times B \subseteq C \times D$, then $A \subseteq C$ and $B \subseteq D$. Proof:  Let a be any element of A and let b be any element of B.  Then (a,b) $\in A \times B$.  Since $A \times B \subseteq C \times D$, (a, b) $\in C \times D$.  Hence, $a \in C$ and $b \in D$.  Therefore, $A \subseteq C$ and $B \subseteq D$.","['elementary-set-theory', 'proof-verification']"
1947090,Does this trig equation have no solution? $\sqrt 2\sin \left(\sqrt 2x\right)+\sin (x)=0$,"Solve the following trig. equation for $x$ 
  $$\sqrt 2\sin \left(\sqrt 2x\right)+\sin (x)=0$$ My try: I divided by $\sqrt{\left(\sqrt2\right)^2+1^2}=\sqrt 3$,
$$\frac{\sqrt 2}{\sqrt 3}\sin\left(\sqrt 2x\right)+\frac{1}{\sqrt 3}\sin (x)=0$$ $$\sqrt{\frac{2}{3}}\sin\left(\sqrt 2x\right)+\frac{1}{\sqrt 3}\sin (x)=0$$
I got stuck here, I have no clue to proceed to find the values of $x$.","['trigonometry', 'calculus']"
1947106,Convexity of the solution of an o.d.e.,"Are there any results on proving the convexity of a function defined implicitly through an ode? More specifically I'm interested in proving convexity of a function
$f(t,u):\mathbb{R}_{+}^{2}\rightarrow\mathbb{R}_{+}$ in $u$ ($0 \leq u  \leq 1$) for
all $t$ given that it satisfies the following ode:
$$
\frac{\partial f(t,u)}{\partial t}=a-\min(u,f(t,u)),
$$
with initial condition $f(0)$. Update 2: Here is a more basic question (which can help answering the original one): can we show that $f(t,u)$ is nonincreasing in $u$ for all $t$? (without solving the ode) This is very intuitive: one can think of $f$ as the content of a buffer/tank at time $t$ with input rate $a$ and output rate $\min(u,f(t,u))$. The output rate is at most $u$ but also cannot exceed the current content level. By increasing $u$ we are basically increasing the potential output rate and hence it must be that $f$ is nonincreasing in $u$. Update 1: Eventually I'm interested in showing convexity when $a$ is also a function of $t$, i.e., $f$ solves
$$
\frac{\partial f(t,u)}{\partial t}=a(t)-\min(u,f(t,u)).
$$
This is why I'm looking for an implicit approach to prove convexity (rather than solving the ode). 
When $a(t)=a$ the ode can be solved (as @daw mentioned) by considering the two cases $u<f(t,u)$ and $u \geq f(t,u)$. This leads to two ode's with solutions given by
$$
f_{1}(t,u)=f_{1}(s)+(t-s)(\lambda-u),\\
f_{2}(t,u)=(f_{2}(s)-a) \exp(-(t-s))+a,
$$
which characterize the solution of the original ode: Given an initial condition $f(0)$ the solution is given by one of the equations above with the possibility of crossing over to the other equation at most once. Here is a plot of the trajectory of $f$ for a fixed $u$: As you see given initial condition $f(0)=1$ the solution is given by the first (linear) equation $f_{1}$ but once it reaches $u=0.9$ it takes the exponential form of $f_{2}$. The solution is also continuous in $u$ and I'm pretty sure convex in $u$ for all $t$ even when $a=a(t)$. Here is a plot of $f(t,u)$ as a function of $u$ for differnet values of $t$: I believe something can be done by differentiating the ode with respect to $u$ and trying to show the monotonicity of the derivative, but I haven't had any luck yet.","['real-analysis', 'dynamical-systems', 'convex-analysis', 'convex-optimization', 'ordinary-differential-equations']"
1947108,Proving that $\iint\limits_{x^2+y^2\leq 1} e^x \cos y dxdy=\pi$,"I want to prove that  $$\iint\limits_{x^2+y^2<1} u(x,y) dxdy=\pi$$
where $u(x,y)=e^x \cos y$. There is a theorem which says that if $u\in C^2(\Omega)$ and $\nabla^2 u=0$ in a domain $\Omega\subseteq \mathbb{R}^n$, then for any ball $B=B_R(v)$ with $\overline{B}\subset\Omega$, 
$$u(v)=\frac{1}{\omega_n R^n}\int_B u dx$$
where $\omega_n$ is the volume of the ball of radius 1 in $\mathbb{R}^n$.
The double integral above can be seen as a particular case of the theorem, since $\omega_2=\pi$, $R=1$ and $u(0,0)=1$. It's also clear that $\nabla^2 u=0$ (It's the real part of $e^z,z\in\mathbb{C}$). I want to prove it without using this mean value theorem. In a standard way, I get to the integral
$$2\int_{-1}^1 e^x\sin \sqrt{1-x^2}dx$$
which seems crazy. Numerically it seems to be $\pi$ efectively. How could I calculate the integral?",['multivariable-calculus']
1947128,Compact Support of Fourier-like Differential Form $\sigma(\xi)$,"Let $f\in$$L^1(\mathbb{R})$ and $g(y)=\int_{\mathbb{R}}f(x)e^{-iyx}dx.$ Then if $f$ has compact support, $g=\hat f(y)$ can not have compact support unless $f \equiv0$. Similarly, let $\Omega_c^{1}(X)$ denote the space of continuous one-forms with compact support on a Riemann surface $X.$ Is there a generalization of the above theorem for differential forms, whereby if $\omega\in\Omega_c^{1}(X)$ has compact support, and $$\sigma(\xi)=\int_{\partial X}e^{-i\xi z}\omega$$ for $\omega=f(z)dz,$ then $\sigma(\xi)=\hat \omega(\xi)$ can not (necessarily) have compact support unless $\omega\equiv0?$ Proposition/Attempted Proof: Let $\pi$ be a diffeomorphism $\pi:\mathbb{R} \to X,$ such that $$\sigma(\xi)=\int_{\partial X}e^{-i\xi z}f(z)dz\equiv\int_{\pi(\mathbb{R})}e^{-i\xi \pi(x)}f(\pi(x))\pi^{\prime}(x)dx,$$ where $\pi(x)=z,\pi(y)=\xi.$ Similarly, let $t=Re(\pi)(x)=\frac{\pi+\bar \pi}{2}$  be a change of variables, then $$\sigma(\xi)=\int_{\mathbb{R}}e^{-i\xi t}f(t)\frac{dt}{dx}dx=\int_{\mathbb{R}}e^{-i\xi t}f(t)dt,$$ where $\pi(\mathbb{R})$ is transformed into $\mathbb{R}$ under $t=Re(\pi)(x).$ This reduces the above case into the less general $g(y)=\int_{\mathbb{R}}f(x)e^{-iyx}dx.$ However, there seems to be a logical fallacy in this proof. Thanks in advance.","['real-analysis', 'complex-geometry', 'fourier-analysis', 'differential-forms', 'complex-analysis']"
1947143,"Why $\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n Cov(X_i,X_j)=\frac{1}{n^2}\sum_{i-j=-n}^n (n-|i-j|)\gamma(i-j)$","I'm having trouble to understand a passage in the mean square error of an estimator. Let $\{X_t\}$ be a stationary process of a time series with mean $\mu$, thus the sample mean estimator is $$\overline{X}_n=\frac{1}{n}(X_1+X_2+\dots +X_n)$$
The mean squared error of this estimator is
$$E[\overline{X}_n-\mu]^2=Var(\overline{X}_n)$$
since that $\overline{X}_n$ is a unbiased estimator of $\mu$. Then $$Var(\overline{X}_n)=\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n Cov(X_i,X_j)\qquad (1)$$ $$=\frac{1}{n^2}\sum_{i-j=-n}^n (n-|i-j|)\gamma(i-j)\qquad (2)$$ $$=\frac{1}{n}\sum_{h=-n}^n \Big(1-\frac{|h|}{n}\Big)\gamma (h)$$ where $\gamma (i-j)=Cov(X_{t+(i-j)},X_t)$ and $\gamma(h)=Cov(X_{t+h},X_t)$ (autocovariance function). I can't figure out what they make from (1) to (2). I make a test with $n=1$ and it works, but I don't understand what they did, since in (1) I have a sum with $n^2$ terms and in (2) I have just $2n+1$ terms.","['stochastic-processes', 'statistics', 'sequences-and-series', 'time-series']"
1947145,Why does this derivative equation hold?,"$$\frac{d(Q/x)}{dx} = \frac{x(\frac{dQ}{dx})-Q(\frac{dx}{dx})}{x^2}$$ Assume $Q$ is a function of $x$. This equation is in my microeconomics textbook, but I don't know how we can get from the left-hand side to the right-hand side. Can someone please explain?","['derivatives', 'calculus']"
1947161,Sanity check on factorization of $\langle 5 \rangle$ in $\mathbb{Z}_{10}$,"Looking at $\mathbb{Z}_{10}$, consisting of $0, 1, 2, 3, 4, 5, 6, 7, 8, 9$ with addition and multiplication adjusted so the ring is closed under both operations. Clearly $5 = 3 \times 5 = 5 \times 5 = 5 \times 7 = 5 \times 5 \times 5 = \ldots$. It's a little weird that $5$ is its own square and cube, but so it is. As I was taught here a few weeks ago, this domain is Noetherian, which means that ideals can be factorized finitely and uniquely. Then $\langle 5 \rangle$ is not a prime ideal but can be factorized as a product of prime ideals. One factor might be $\langle 3, 5, 7 \rangle$, but since $3 = -7$, then $\langle 3, 5 \rangle$ will do. And then $\langle 3, 5 \rangle \langle 3, 5 \rangle = \langle 9, 5 \rangle$, but $9 = -1$, which therefore means $\langle 5 \rangle = \langle 3, 5 \rangle^2$. Is this right? Or did I go wrong somewhere?","['abstract-algebra', 'prime-factorization', 'proof-verification', 'maximal-and-prime-ideals', 'ring-theory']"
1947171,The topology of sets,"Normally limits come from topologies. However the set-theoretic limit is defined without reference to a specific topology. Therefore I wonder: What is the topology of sets that corresponds to this limit? Using this answer to another question of me, I figured out that the set of all subsets with at most $n$ elements of a given set is closed, and that the set of all finite subsets of an infinite set is not closed, but that by itself doesn't really give me an idea what the topology looks like. What I'd like is a general rule when some set is open or closed under this topology.","['general-topology', 'elementary-set-theory']"
1947184,$\lim \limits_{x \rightarrow \infty} \sqrt[]{x^2+2x+3}-\sqrt[]{x^2-x+5}.$ [duplicate],"This question already has answers here : Evaluating $\lim\limits_{x\to \infty}\sqrt[6]{x^{6}+x^{5}}-\sqrt[6]{x^{6}-x^{5}}$ (5 answers) Closed 7 years ago . I am trying to formally evaluate the following limit:
$$\lim \limits_{x \rightarrow \infty} \sqrt[]{x^2+2x+3}-\sqrt[]{x^2-x+5}.$$
Empirically, the limit seems to be converging to $1.5$, although I am not sure how to formally prove this.  I had one idea thus far: it appears the constant terms within the square roots do not matter, so I rewrote the limit as
$$\lim \limits_{x \rightarrow \infty} \sqrt[]{x^2+2x-3}-\sqrt[]{x^2-x-6}
=\lim \limits_{x \rightarrow \infty} \sqrt[]{(x+3)(x-1)}-\sqrt[]{(x+2)(x-3)}.$$ In each square root, there are two factors.  I assumed that in the limit the geometric mean of the two factors (i.e. square root of the product) is equal to their arithmetic mean.  This is a step I am not quite certain of, and if it is true I would like to prove it. However, as I found it led to the correct answer, since
$$\lim \limits_{x \rightarrow \infty} \frac{(x+3)+(x-1)}{2}-\frac{(x+2)+(x-3)}{2}=\lim \limits_{x \rightarrow \infty} (x+1)-(x-0.5)=1.5.$$ I am not sure if the result was coincidental, but if not, I would like some help formalizing each of my steps.  I would also like to hear of alternate approaches that do not alter the original problem in the way I did.","['real-analysis', 'analysis', 'limits']"
1947229,How many different ways to fill a shelf with toys of three different sizes?,"I am looking at the following problem.  There are three types of toys: a small, a medium, and a large.  The medium is twice as long as the small and the large is thrice as long as the small.  I have a shelf that can accommodate exactly 10 small toys lengthwise.  How many different ways can this shelf be filled using different combinations of the toys (where order matters)? It is easy to solve for shorter shelves; for instance, for shelf length $3$, there are $4$ ways: small ($3x$); small, medium; medium, small; large.  For shelf length $4$, there are $7$ ways: small ($4x$); small ($2x$), medium; small, medium, small; medium, small ($2x$); medium ($2x$); small, large; large small.  It is more tedious to enumerate for shelves larger than 4 units; I am looking for a more elegant and systematic method to enumerate the different ways to fill the $10$-unit shelf, ideally one that can be easily adapted to a larger shelf size.","['combinatorics', 'discrete-mathematics']"
1947261,Morphism that is not a mapping,"I have encountered a statement in Lang's Algebra (Revised Third Edition, page 53) concerning morphisms of objects that seems strange to me: ""In practice, in this book we shall see that most of our morphisms are actually mappings, or closely related to mappings."" I have always been under the impression that the terms 'mapping' and 'morphism' are synonymous in the context of categories. Perhaps it is just the case that Lang defines the two in a way that they disagree, but I can't find such an instance. Are they in fact different? If so, what is an example of a morphism that is not a mapping?","['category-theory', 'abstract-algebra']"
1947305,Why does this integral converge for all $s \in \mathbb C$?,"I'm trying to understand the proof of analytic continuation of the Riemann zeta function.  These notes ( http://math.bu.edu/people/jsweinst/Teaching/MA843/TatesThesis.pdf ) have been very helpful.  I'm right at the end of the proof. Here $\omega(x) = \sum\limits_{n =1}^{\infty} e^{-\pi n^2x}$.  To finish, the proof, we need the fact that that last integral converges for all $s \in \mathbb{C}$, and hence $\Lambda(s) = \pi^{-\frac{s}{2}} \Gamma(\frac{s}{2})\zeta(s)$ admits an analytic continuation to the whole complex plane with poles at $s =0$ and $s = 1$.  Is it obvious that this integral always converges?","['number-theory', 'complex-analysis', 'analytic-number-theory']"
1947309,"Use mathematical induction to show that when n is an exact power of 2, the solution of the recurrence...","So, I am trying to understand the algebraic simplifications going on in this proof and I understand the answer to the proof, but I just don't understand the algebra. How do they get to the highlighted step?  How do they get rid of the T?","['algebra-precalculus', 'discrete-mathematics']"
1947318,Proving if it is possible to write 1 as the sum of the reciprocals of x odd integers,"Let $x$ be an even number. Is it possible to write 1 as the sum of the reciprocals of $x$ odd integers? Write a proof supporting your answer. I tried a lot of these, and I think it is no because I didn't find any possible combinations.","['algebra-precalculus', 'summation', 'rational-numbers', 'elementary-number-theory']"
1947363,"Is there a name for a 3D shape that looks like a circle when viewed from one axis, a square from another, and a triangle from the third?",The shape can be constructed by taking a cylinder with its height equal to its diameter and cutting a triangle out of it when viewing it from the side with the base of the triangle matching one end of the cylinder and the opposite point of the triangle in the center of where the opposite end of the cylinder was. Is there a name for this shape?,"['3d', 'geometry']"
1947402,Is there an exact solution to the differential equation: $\frac{d^2x}{dt^2} = -a\sinh(x)$?,"I was wondering if the equation $\frac{d^2x}{dt^2} = -a\sinh(x)$ had an exact solution because the differential equation $\frac{d^2x}{dt^2} = -a\sin(x)$ has an exact solution by way of the elliptical integral. This leads me to believe that, since $\sinh(x) = -i\sin(ix)$, $\frac{d^2x}{dt^2} = -a\sinh(x)$ should have a similar solution. Is this so?","['ordinary-differential-equations', 'elliptic-integrals']"
1947421,Proving the continued fraction expansion of $\sqrt2$ [duplicate],"This question already has answers here : Proving the continued fraction representation of $\sqrt{2}$ (3 answers) Closed 7 years ago . I am working on a multi-part problem, which shows that $$\sqrt{2} = 1+\frac{1}{2+\frac{1}{2+...}}$$ I have shown that given some rational approximation $\frac{m}{n}$ to $\sqrt2$ , we can take $\frac{m'}{n'} = \frac{m+2n}{m+n}$ and get a better approximation. I also know that this iteration goes back and forth between being larger and smaller than $\sqrt2$. Say if we start with $\frac{m}{n}<\sqrt2$, then iterating will give us $$\frac{m}{n}<\frac{m''}{n''} = \frac{3m+4n}{2m+3n}<\sqrt2$$ Finally, I am asked to prove that the sequence obtained by iterating in this fashion, starting with $$1,\frac{3}{2},\frac{7}{5},...$$ which I have shown is given recursively by $$q_1=1, q_{n+1} = 1+\frac{1}{1+q_n} $$ converges to $\sqrt2$. My professor's ""hint"" is to consider the ""odd"" and ""even"" subsequences, and show that they both converge to the same limit. This makes sense to me, since both sequences are clearly monotone and bounded. Most (all?) of my classmates simply wrote that because they are both monotone and bounded by $\sqrt2$, then they must both converge to $\sqrt2$. However, just because they are bounded above and below, respectively by $\sqrt2$, it does not necessarily mean that $\sqrt2$ is the limit for both (or either!) of them. Some classmates have tried to prove by contradiction that if there were some other limit for one of the subsequences, we could iterate again and get closer to $\sqrt2$. I think that this is a faulty argument, because it assumes that we reach the limit. Remember: the sequence is ""fixed"" as soon as we choose to start it at $q_1 = 1$ ! My counterargument towards most of my classmates has been the sequence given my $\frac{1}{n}$. The sequence is bounded below by $-99$, and no matter how many times we ""iterate"" (in this case, just taking the next value of $n$) we can always get closer and closer to $-99$, but that doesn't mean that $-99$ is the limit. If someone could offer me a resolution to this problem I would be hugely appreciative. It's been driving me crazy for the past week. EDIT: I realize this post has been marked as a duplicate, however I think it should be left up, as the answer given is a bit different from that in the ""original"".","['real-analysis', 'limits', 'convergence-divergence', 'continued-fractions', 'sequences-and-series']"
1947446,Lottery Ticket Question,"A lottery ticket contains six different numbers, chosen from 1 to 39. The winning ticket will match all six numbers in the correct order, plus a bonus number, which may match the other six numbers. The second prize matches the six winning numbers in the correct order, but not the bonus number. What is the probability of winning first or second prize?","['permutations', 'probability', 'data-analysis']"
1947457,Why the rectification theorem fails on a global scale?,"The rectification theorem states that if $y$ in the domain with the vector field $v(y)\neq 0$, then there is a change of coordinates for a small neighborhood of $y$ such that the vector field becomes a constant vector field. Now suppose that a vector field $V$ has no singular points in a domain $U$. Can we rectify the field $V$ in the whole domain $U$? Wikipedia explained that there is a ""global constraint"", where the trajectory starts out in a patch, and after visiting a series of other patches, comes back to the original one. If the next time the orbit loops around phase space in a different way, then it is impossible to rectify the vector field in the whole series of patches. I don't understand Wikipedia's explanation. Could someone please shed some light on this ? Much appreciated.","['ordinary-differential-equations', 'dynamical-systems']"
1947466,Prove Morera's Theorem in circles cases.,"Suppose that f is continuous on C, and
$$
\oint_C f(z)dz=0
$$
for every circle $C\in \mathbb C$. Prove f is holomorphic in C. How to deal with this cirlce case?",['complex-analysis']
1947491,$P_n(A)\rightarrow P(A)$ implies $P$ is a probability measure?,"Here is the question: Let $\{P_n\}$ be a sequence of probability measures on $\sigma$-field (Also called $\sigma$-algebra) $\mathcal{F}$. Suppose that there exists some function $P$ on $\mathcal{F}$ satisfying that $P_n(A)\rightarrow P(A)$ for all $A\in\mathcal{F}$. Prove that $P$ is a probability measure. What I try: The only problem is to verify countable additivity of $P$. But I can only prove it under the assumption that $P(A_n)\rightarrow 0$ (or $\sup_nP_n(A_k)\rightarrow0$ as $k\rightarrow0$) when $A_n\downarrow0$. About $\sup_nP_n(A_k)\rightarrow0$: For any $k\in\mathbb{N}$ and any $\epsilon>0$, there exists $N=N(k,\epsilon)\in\mathbb{N}$, such that $|P_n(A_k)-P_N(A_k)|\leqslant\epsilon$ for all $n\geqslant N$. Since $\sup_nP_n(A_k)$ is decreasing, we have $$\lim_k\sup_nP_n(A_k)\leqslant\sup_nP_n(A_k)\leqslant\max_{n\leqslant N}P_n(A_k)+\epsilon.$$ Now I don't know what to do: we can't just let $k\rightarrow\infty$ 'cause $N$ is related to $k$. I can prove the needed assumption: for any $\{A_n\}\subset\mathcal F$ with $A_n\downarrow\emptyset$, we have $P(A_n)\rightarrow0$. So this problem is now solved.","['real-analysis', 'probability']"
1947507,How was this geometry problem created?,"This is a standard High School Olympiad problem and for an experienced problem solver a quite easy solve. But how was this problem created. To pose a problem, I believe is much harder, than to solve a posed problem. Here the problem poser had to first make the figure up and then simultaneously realise that $ND$ had the wonderful property of being equal in magnitude to the circumradius. Is there a nifty way to find out these wonderful geometric properties?","['contest-math', 'geometry']"
1947508,A Different Dominated Convergence Theorem in Probability.,"I'm trying to prove the following proposition, which is another version of Dominated Convergence Theorem in Probability Theory: Suppose $X_{n}\overset{p}\rightarrow X,$ i.e., $X_{n}\rightarrow X$ in probability, and there is a continous function $g$ with $g(x)>0$ for large $x$ with $\frac{|x|}{g(x)}\rightarrow 0$ as $|x|\rightarrow\infty$ so that $E(g(X_{n}))\leq C<\infty$ for all $n.$ Then $E(X_{n})\rightarrow E(X).$ My attempt is based in the convergence of $\frac{|x|}{g(x)}$ to $0.$ So, for an $\epsilon=1$ we have $|x|< g(x)$ for $x$ large. Then $E(|X_{n}|)<E(g(X_{n}))\leq C.$ Then $X_{n}$ are integrable and because of continuity of $g$ $g(X_{n})\rightarrow g(X)$ in probability. So $E(g(X_{n}))\rightarrow E(g(X)),$ that is because of Dominated Convergence Theorem with almost sure convergence. But I don't know how to use this to prove the convergence of $E(X_{n})$ to $E(X).$ Any kind of help is thanked in advance.","['probability-theory', 'measure-theory']"
1947535,For what values of $k$ is $p(x) = k(1-r^2)^x$ a valid probability mass function,"The question is asking for values for $k$ making this a valid probability mass function. where $$P(x) =  k(1-r^2)^x$$ for $x = 0,1,2...$ and $P(x)= 0$ otherwise $r$ is elected from interval $(0,1)$ I'm thinking this is a discrete probability distribution that needs to sum to 1, but the text is very light on and I'm at a bit of a loss how to solve this. Any help appreciated","['statistics', 'probability', 'probability-distributions']"
1947552,Runners and their chance of winning [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Six runners are entered in a track meet, and have equal ability. What is the probability that a) they will finish in ascending order of their ages? b) Shanaze will finish first or Tanya will finish second? c) Shanaze and Tanya will not finish back-to-back?","['combinatorics', 'probability']"
1947580,Computing differential of a map of affine schemes,"I'm working on an example I made up to try to understand how etale maps work. Let $X = Spec(k[x,y]/(y-x^2))$ be the parabola over an algebraically closed field. Consider the projection map to $\mathbb{A}^1 = Spec(k[t])$ down to the $x$-coordinate, i.e. $\pi: X \to \mathbb{A}^1$ is the map induced by $t \mapsto x$. For this map to be etale it has to be a local diffeomorphism, i.e. it induces an isomorphism on tangent spaces. But I'm having a little trouble working out the computation explicitly. Here's what I have on my own. We know $T_{X,p}$ for $p = (\alpha, \alpha^2)$ (i.e. corresponding to the ideal $(x-\alpha, y-\alpha^2)$ is equal to the kernel of the Jacobian map (writing $f$ for $y-x^2$) $( \partial{f}/\partial x(\alpha) \text{  |    } \partial{f}/\partial y(\alpha)) = (-2\alpha \text{ | } 1)$. This is the line given by $y = 2\alpha x$. Now how do I explicitly compute $d\pi: \{(x,2\alpha x)\} \to T_{\mathbb{A}^1, \pi(p)}$? As $\pi(p) = (t-\alpha)$ corresponds to just the point $(\alpha, 0)$, my guess is the induced map is also projection to the first coordinate (hence an isomorphism), but perhaps I'm missing a good enough definition to see why it should be that (I'm trying to do this by piecing together a few things I've heard about Zariski tangent spaces). If there's a good source to do these types of explicit computations, especially anything related to explicit etale maps I'd appreciate seeing it. This example is far too simple to be the end of the story, so I'd like to try a harder one after I get this down. Edit: In particular, a source with a general formula/proof for how to explicitly compute differential maps on tangent spaces would be helpful.",['algebraic-geometry']
1947600,The Lebesgue point theorem for locally average function,"Let $Q=(0,1)\times(0,1)\subset \mathbb R^2$ and $u\in L^\infty(Q)$ be given. Define, for $N\in\mathbb N$, that a small cube
$$
Q_{N}(i,j):=(i/N,(i+1)/N)\times (j/N,(j+1)/N)
$$
where $0\leq i,j<N$. Hence, $Q=\bigcup_{0\leq i,j<N}Q_N(i,j)$ and each $Q_N$ are mutually disjoint. My question: do we have 
$$
\lim_{N\to\infty}\sum_{0\leq i,j<N}\left\|u-\frac{1}{|Q_N(i,j)|}\int_{Q_{N}(i,j)}u(y)dy\right\|_{L^2(Q_N(i,j))}=0
$$
hold? At the beginning I though this is just an easy conclusion from Lebesgue point theorem, but later I realize it may not be that easy... Any help is really welcome!","['real-analysis', 'measure-theory']"
1947642,Evaluate : $\int _0^{\infty } \sin (ax^2)\cos \left(2bx\right)dx$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How can I solve $$\int_{0}^{\infty} \sin  \left (ax^2 \right) \cos \left (2bx \right)dx$$ Thanks!","['integration', 'trigonometric-integrals', 'calculus']"
1947665,How to Solve $x^2=2^x$ [duplicate],"This question already has answers here : How to solve this equation $x^{2}=2^{x}$? (4 answers) Closed 7 years ago . I was thinking of random math problems to myself when I came across this: $x^2 = 2^x$ It seemed very simple at first, but after trying multiple ways to solve it, I had no idea what to do. So, I began doing a lot of searches for this problem, and I found out that in order to solve $x^2 = 2^x$, you have to use some function of which I have never heard before. So, I have three main questions regarding this problem. 1) is it possible to solve $x^2 = 2^x$ arithmetically? If so, how would one do so? If not, why not? In other words, why would it be impossible to solve this problem arithmetically? 2) Why is it that addition and multiplication are commutative, but exponentiation is not? I understand not all forms of multiplication in mathematics are non-commutative, but for the sake of this problem, exponentiation feels like the ""next step"" right after multiplication. Here's what I mean by this: Addition is just adding a number to a number. Multiplication is just adding a number $N$ times to a number. Exponentiation is just multiplying a number $N$ times by a number. 3) Would it be possible to invent your own function to solve $x^2 = 2^x$, or rather, $x^y = y^x$? And after you found a way to represent your answer, how could you go about evaluating your answer without using a calculator? If anything doesn't make sense above, I can do my best to clarify. I also know some of these questions have been asked before, but I didn't really understand any of the answers, and the questions weren't as specific as mine.","['algebra-precalculus', 'logarithms', 'exponentiation']"
1947672,If $abc=1$ so $\sum\limits_{cyc}\sqrt{\frac{a}{b+c}}\geq\frac{9}{\sqrt{a+b+c+15}}$,"Let $a$, $b$ and $c$ be positive numbers such that $abc=1$. Prove that:
$$\sqrt{\frac{a}{b+c}}+\sqrt{\frac{b}{a+c}}+\sqrt{\frac{c}{a+b}}\geq\frac{9}{\sqrt{a+b+c+15}}$$ It seems nice enough. I proved this inequality by Holder, but it quits very ugly. Maybe there is something nice? Thank you!","['contest-math', 'inequality', 'calculus']"
1947686,How does Lebesgue integral handle functions with non-zero infimum?,"How do Lebesgue integral handle functions with non-zero intercept? For example, take dyadic coefficients. The $blue$ boxes are summed by$$\sum_ka_k1_{A_k}$$ but what about the area coloured $green$? Only $\emptyset$ is mapped to that part of $A_k$'s so the simple function doesn't give any weight on those $A_k$'s. However to get an ""area under curve"", one should include that part to preserve the notion of integration.","['real-analysis', 'functional-analysis', 'integration', 'probability', 'measure-theory']"
1947718,Pila's Algorithm for factoring cyclotomics,"In the famous paper( link ) of J.Pila he gave an Algorithm on finding $r^{th}$ root of unity in ${\mathbb{F}_p}^{*}$. The time complexity of that algorithm is mentioned as $O(\log ^{\Delta}{p})$, where $\Delta$ is dependent only on embedding space of A, the number of equations defining A, addition law and their degrees. Here A refers to an abelian variety over $\mathbb{F}_q$. I want to find the dependence of $\Delta$ on $r$, more precisely is it exponential or double exponential (As he claims result for only constant $r's$).","['finite-fields', 'reference-request', 'cyclotomic-polynomials', 'algebraic-geometry']"
1947736,How did Do Carmo get the following differential of the Gauss map?,"Below is an example from Do Carmo's Differential Geometry page 139 ""The Geometry of the Gauss Map"". Let us analyse the point $p=(0,0,0)$ of the hyperbolic paraboloid $z=y^2-x^2$. For this, we consider a parametrisation $\textbf{x}(u,v)$ given by 
$$\textbf{x}(u,v)=(u,v,v^2-u^2),$$ and compute the normal vector $N(u,v)$. We obtain successively $\textbf{x}_u=(1,0,-2u),$ $\textbf{x}_v=(0,1,2v),$ $N=\Big(\frac{u}{\sqrt{u^2+v^2+\frac{1}{4}}},\frac{-v}{\sqrt{u^2+v^2+\frac{1}{4}}},\frac{1}{2\sqrt{u^2+v^2+\frac{1}{4}}}\Big)$. Notice that at $p=(0,0,0)$ $\textbf{x}_u$ and $\textbf{x}_v$ agree with the unit vectors along the $x$ and $y$ axes, respectively. Therefore, the tangent vector at $p$ to the curve $\alpha(t)=\textbf{x}(u(t),v(t))$, with $\alpha(0)=p$, has, in $\mathbb{R}^3$, coordinates $(u'(0),v'(0),0)$. I understand up until this point. Now my question is what follows: How can Do Carmo get the following: Restricting $N(u,v)$ to this curve and computing $N'(0)$, we obtain $N'(0)=(2u'(0),-2v'(0),0)$ I have little clue on how can he get $2u'(0)$ and $2v'(0)$? Could somebody please help clarify this confusion? Thanks.",['differential-geometry']
1947739,Show that $ f(t) = \begin{cases} e^{-1/t} & x > 0 \\ 0 & x \le 0 \end{cases}$ is in $C^{\infty}$.,"Show that
\begin{equation}
   f(t) =
   \begin{cases}
      e^{-1/t} & x > 0 \\
      0        & x \le 0
   \end{cases}
\end{equation}
is in $C^{\infty}$. To show that this is true, I think we have to show that $f^{(k)}$ is continuous for all $k \in \mathbb{N}$, where $f^{(k)}$ is the $k$-th derivative of $f$ ($f^{(0)} := f$). Since the only point where these could be discontinuous is at $x = 0$, I think I need to show that
\begin{equation}
   f^{(k)}(0) = 0
\end{equation}
for all $k$. I've calculated the first few derivatives in order to see a pattern emerge and create a closed form, but I can't do it. Here are the derivatives. \begin{align}
f^{(0)} & = e^{-1/t}\\
f^{(1)} & = {\frac {1}{{t}^{2}}{{e}^{-1/t}}}\\
f^{(2)} & = -2\,{\frac {1}{{t}^{3}}{{e}^{-1/t}}}+{\frac {1}{{t}^{4}}{{e}^{-1/t}}}\\
f^{(3)} & = 6\,{\frac {1}{{t}^{4}}{{e}^{-1/t}}}-6\,{\frac {1}{{t}^{5}}{{e}^{-1/t}}}+{\frac {1}{{t}^{6}}{{e}^{-1/t}}}\\
f^{(4)} & = -24\,{\frac {1}{{t}^{5}}{{e}^{-1/t}}}+36\,{\frac {1}{{t}^{6}}{{e}^{-1/t}}}-12\,{\frac {1}{{t}^{7}}{{e}^{-1/t}}}+{\frac {1}{{t}^{8}}{{e}^{-1/t}}}\\
\end{align} I can see that 
\begin{equation}
   f^{(k)}(t) = f^{(0)}(t) \sum_{i = k+1}^{2k} (-1)^{i}t^{-i} c_i
\end{equation} where $c_i \stackrel{?}{=} k!$ or something like that. So in fact, I just need to find an expression for $c_i$. Any ideas?","['derivatives', 'sequences-and-series', 'closed-form']"
1947767,"Why do we need finiteness of the first set in ""continuity from above""?","If $E_1 \supset E_2 \supset ...$ and $\mu(E_1)<\infty$ then $\mu(\bigcap  E_j)=\lim \mu(E_j)$. But why need $\mu(E_1)<\infty$? Is $(-\infty,-n)$ an counter example?","['real-analysis', 'probability', 'measure-theory']"
1947781,Union of two countable sets is countable [Proof],"Theorem : If $A$ and $B$ are both countable sets, then their union $A\cup B$ is also countable. I am trying to prove this theorem in the following manner: Since $A$ is a countable set, there exists a bijective function such that $f:\mathbb{N}\to A$. Similarly, there exists a bijective function $g:\mathbb{N}\to B$. Now define $h:\mathbb{N}\to A\cup B$ such that: $$h(n)=\begin{cases} 
      f(\frac{n+1}{2})&\text{, n is odd}\\
      g(n/2) & \text{, n is even} \\
   \end{cases}$$
So in essence, $h(1)=f(1)$, $h(2)=g(1)$, $h(3)=f(2)$ and so on. Now we have to show that h is a bijection. h(n) is one-one : Proof: If  $h(n_1)=h(n_2)$ then, if $n_1$ and $n_2$ are both either odd or even, we get $n_1=n_2$. But if, suppose $n_1$ is odd and $n_2$ is even, this implies that: $$f\left(\frac{n_1+1}{2}\right)=g\left(\frac{n_2}{2}\right)$$ How can one deduce from this equality that $n_1=n_2$? I tried to think about this and realized that if $A\cap B=\phi$ then this case is impossible as it would imply that there is a common element in both sets. On the other hand, if we assume that $A\cap B\neq \phi$, then either $f\left(\frac{n_1+1}{2}\right)\in A\cup B$ or $g\left(\frac{n_2}{2}\right)\in A\cup B$....Beyond this I'm clueless. Edit : Solution by the author-","['real-analysis', 'proof-verification']"
1947821,expected value and variance of function,"I am confused about calculating the expected value of a function that is split, the question looks as follows: and my solution is: Which are incorrect.. can someone please help me to correct the answers. Or tell me what I'm doing wrong. Thank you",['statistics']
1947851,elementary abelian subgroup of finite non-cyclic $p$-group [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $G$ be a finite non-cyclic $p$-group of order $p^n$, $n >1$, where $p$ is odd prime. I need to prove(By elementary methods)that $G$ has a subgroup isomorphic to $\Bbb{Z}_p \times \Bbb{Z}_p$?","['finite-groups', 'p-groups', 'group-theory']"
1947874,The curves $y^2=x^{2k+1}$ are never isomorphic to each other,"For every $k \in \Bbb N$, we consider the affine variety $X_k=\Bbb V(y^2-x^{2k+1}) \subset \Bbb A^2$. Gathmann's note contain the following exercise: Show that the curves $X_k$ are pairwise nonisomorphic. Equivalently, We need to show that the rings $$\frac{\Bbb C[x,y]}{(y^2-x^{2k+1})}$$ are pairwise nonisomorphic. The fact that $X_1, X_0$ are not isomorphic is well -known: this is just saying that the cuspidal curve is not isomorphic to $\Bbb A^1$, since one is smooth and the other is not. It is suggested as a hint to look at the blow-up of $X_k$ at the origin, and it surely suffices to show that these blow-ups $\tilde X_k \subset \Bbb A^2 \times \Bbb P^1 $ are pairwise nonisomorphic. I'm also interested in knowing whether this is the ""standard"" proof, or there are other methods that work. (like the ring-isomorphism approach above.) If we denote the coordinates of $\tilde {\Bbb A^2}$ by $((x_1,x_2),(y_1:y_2))$ then Gathmann shows that the blow-up of $X_k$ is given in $U_1=\{y_1 \neq 0\}$ by the equation $y_2^2-x_1^{2k-1}=0$. Is it enough to show that these curves are pairwise nonisomorphic in $\Bbb A^2$?","['algebraic-curves', 'algebraic-geometry']"
1947881,"Is $C[0,1]$ a Banach space for the $L^1$ norm?","Is $C[0,1]$ a Banach space with respect to the norm $\|f\| = \int\limits_0^1|f(t)| \, dt$? People keep telling me it is, but lets consider:
$f_n(x) = x^n$. This function defines a Cauchy sequence, yet the limit clearly isn't a continuous function!","['functional-analysis', 'banach-spaces']"
1947896,"Some confusion about normal vector, curvature and normal curvature in Do Carmo's textbook.","There is this part of a text from Do Carmo's Differential Geometry that I don't quite understand. I understand the definitions of curvature, normal curvature, normal section etc. But what I am confused at is the part that says ""In a neighbourhood of $p$, a normal section of $S$ at $p$ is a regular plane curve on $S$ whose normal vector $n$ at $p$ is $\pm N(p)$ or zero; its curvature is therefore equal to the absolute value of the normal curvature along $v$ at $p$."" First, is the neighbourhood of $p$ a part a curve $C$, a surface $S$ or a neighbourhood of $p$ on the normal section? It is not that clear. Especially when it mentioned the curvature it seems that it refers to the curve $C$. Another thing that confuses me is why is the normal vector either $N(p)$ or zero? And why is the curvature equal the absolute value of the normal curvature? Here is how I worked it out: $k_n=k\cos\theta$ and $\theta=0$, so $k_n=k$, where $k_n$ is the normal curvature. Am I correct? Or should I reason as follows: $n=0$, so $\cos\theta=<0,N>=0$, so $k_n=0$? I have been looking back and forth for the definitions and tried to work it out myself but still couldn't really understand the text. Could someone please help clarify the confusion? Thanks.",['differential-geometry']
1947898,Plane determined by two lines,"Show that the lines $x=-2+t,y=3+2t,z=4-t$ and $x=3-t,y=4-2t,z=t$ are parallel. Find the equation of the plane they determine. Here what is the meaning of ""they determine""?",['linear-algebra']
1947907,Curious hyperbolic metric interesting?,"Knowing that distance in polar coordinates is minimized by metric $ ds^2 = (r^2 + r^{\prime 2})\,d\theta^2 $ as representing a straight line geodesic in the plane, I was curious to know about the metric: $$ ds^2 = (r^2 - r^{\prime 2}) \, d\theta^2 \tag{0}$$ $$ s = \int \sqrt{ r^2 - r^{\prime 2}} d\theta  \tag{1}$$ (Primed with respect to $\theta$). With Euler-Lagrange equation in Calculus of variations  we find solutions $$ F = \sqrt{ r^2 - r^{\prime 2}} \tag{2}$$ $$ F - r^{\prime}\,\partial F/ \partial r^{\prime} = const \tag{3}$$ $$ \frac{r^2}{\sqrt{ r^2 - r^{\prime 2}}} = const. \tag{4} $$ $$ r^{\prime \prime }  = 2 r^{\prime 2}/r - r  \tag{5}$$ Please note that by a simple change of sign of above, the straight line  can be obtained by integration with constants $ (p,\alpha) $  in polar form as: $$ r^{\prime \prime }  = 2 r^{\prime 2}/r + r, \quad p =  r \cos(\theta- \alpha)  \tag{6}$$ Now (5) integrates to : $$ 1/r = e^\theta /2a + e^ {-\theta}/2b \tag{7}$$ $(a,b) $ are arbitrary constants. When $a=b,\, r = a\, sech \,\theta $ This is a spiral falling to the origin, the projection is exactly the same as the polar projection of the central (Beltrami) pseudosphere asymptotic line or geodesic. So the metric is fundamentally and qualitatively a hyperbolic metric . It remains to be shown that the lines could be hyperbolcally geodesic as well in $\mathbb R^3 $ in this particular case. An integrand of differential equation gives plot of the curve with B.C. $ r(0) =2, r^{\prime }(0) = 0 $ with plot: The condition $ a=b$ implies hyperbolic geodesics on Beltrami pseudosphere. When $ a\ne b$ typically meridian resembles a hypo pseudosphere containing the curious metric. $ \phi =\psi$ (slope and inclination to meridian) for the former and $ \phi +\psi = \pi/2  $ for meridian shown. I expected in a not too wild and off-tangent imagination to mirror a skewed Pythagorean relation ( hyperbolic?) $ c^2 = (a^2-b^2) $ of equation tagged (0) geometrical relation being valid and should show up somewhere, but I cannot recognize it anywhere. Sorry for the subjective nature of query, I am posting it despite whatever vagueness that is going with it ... and I do appreciate not down-voting as stimulus to such research inquiries..","['hyperbolic-geometry', 'calculus-of-variations', 'ordinary-differential-equations']"
1947917,How can I know whether the point is a maximum or minimum without much calculation?,"Find the maximum and minimum of this function and state whether they
  are local or global: $$f: \mathbb{R} \ni x \mapsto \frac{x}{x^{2}+x+1} \in \mathbb{R}$$ \begin{align*} 
f'(x)&= \frac{-x^{2}+x}{\left(x^{2}+x+1\right)^{2}}\\
f'(x)&=0 \iff \frac{-x^{2}+x}{\left(x^{2}+x+1\right)^{2}}=0 \iff -x^2+x=0 \iff x(1-x)=0, \end{align*}
which gives $x_{1}=0, x_{2}=1$. Here comes the disturbing part, we need to know if these are maximum or minimum and for this we usually used the second derivative. But this would be soo exhausting, I don't even  want think of doing it. There must be an easier way and I remember someone here has even recommended me using monotony somehow. But how can we do this here? Please do tell me, at home I got enough time to use second derivative but surely not in the exam : /","['derivatives', 'curves', 'functions', 'calculus', 'analysis']"
1947963,Proof that uniform convergence implies convergence in norm of function space,"In an arbitrary normed space $X$ consider a sequence of functions $f_n:X\to \mathbb{R}$ which converges uniformly to some function $f: X\to \mathbb{R}$. Now consider an arbitrary function space $Y$ (over $\mathbb{R}$) consisting of functions from $X$ to $\mathbb{R}$ with arbitrary norm $\|\cdot\|_Y$, which contains $f_n$ for each natural $n$ and contains $f$. Now intuitively I feel that uniform convergence should imply that $\|f-f_n\|_Y \xrightarrow[\infty]{n}0$. It is easy enough to prove this in any concrete function space like the $L^p$ spaces, but I'm struggling to find a proof for the general case, which is frustrating because it seems intuitively so obvious. I feel the proof should go along the lines of, because for any $\varepsilon>0$ we can find a natural $N$ such that for all $n>N$
$$|f(x)-f_n(x)|<\varepsilon \quad \forall x\in X,$$
then in some sense $f-f_n$ is in some sense within an $\varepsilon$ ""distance"" of $0$. I'm struggling to make this argument concrete using only the ideas of general normed spaces. I now believe that there exists a strange norm which makes this untrue. Translated to metric spaces obviously the discrete metric would give us a suitable counterexample. I can't find a suitable analogue in a normed space because of scalar multiplication. If anyone can give a proof or provide a counterexample as to whether uniform convergence implies convergence in the norm, or can direct me to a reference on the topic I'd be very appreciative. EDIT: I'd like to rephrase the question to deal exclusively with the kind of spaces I had in mind, which Daniel Fischer uncannily knew. As he pointed out a counter example will be any space $L^P(X)$ where $\mu(X)=\infty$. So let us rephrase the question and deal with a compact subspace of $X$, say $Z$. Then if we consider a sequence $f_n:Z\to \mathbb{R}$ converging to $f:Z\to \mathbb{R}$, and redefine $Y$ to be a function space consisting of functions from $Z$ to $\mathbb{R}$ with some arbitrary norm, does uniform convergence then imply convergence in the norm? As my measure theory course was rather disappointing I'm not entirely sure that the measure of a compact subset is finite. If not then obviously the answer stays the same. Are there any conditions that force the statement to be true?","['functional-analysis', 'general-topology', 'convergence-divergence', 'uniform-convergence']"
1947966,"Maximize $\int_\Gamma\,\langle{y,x}\rangle^2\, dx$","Given a bounded, compact, closed surface $\Gamma\subset\mathbb{R}^n$, I'm searching
$$
\max_{y\in\mathbb{R}^n, \|y\|=1} \int_\Gamma \langle y, x\rangle^2.
$$
Without the square, and with the enclosed volume $\Omega$,
$$
\max_{y\in\mathbb{R}^n, \|y\|=1} \int_\Omega \langle y, x\rangle,
$$
I'm guessing $y_\text{max}$ points towards the centroid of $\Omega$. Not sure how to prove that though. Any hints?","['integration', 'optimization', 'geometry']"
1947971,Show that the map $A : l^p \rightarrow l^q $ is a bounded linear map,"Let $1 \leq p,q \leq \infty$ and $A= (a_{ij})$ be a scalar matrix. Suppose for every $x= (x_j)\in l^p$ , the series $\sum_{1}^{\infty}a_{ij}x_j$ is convergent for every $i$ and that $y=(y_i) \in {l}^q$ where $y_i = \sum_{1}^{\infty}a_{ij}x_j = (Ax)_{j}$ . I need to show that the map $A : l^p \rightarrow l^q $ is a bounded linear map. The hint given to me was to use Closed graph theorem but I absolutely don't know how to do it !","['functional-analysis', 'lp-spaces', 'operator-theory', 'closed-graph']"
1948038,Why do we say in limits that $x$ must approach a but not be equal to a,"Having a little bit of unrigorous foresight as to what will come after limits, I'm wondering why in the definition of the limit we require $x$ to approach a but not be equal to $a$. What is the mathematical reason to this; what situations does it help avoid? To me it seems arbitrary.","['elementary-set-theory', 'calculus', 'limits']"
1948054,Generators of a direct product of symmetric groups,"This question is a generalization of Minimum size of the generating set of a direct product of symmetric groups to a product of arbitrary arity, and the only answer to that question is incomplete (it doesn't talk about $S_m \times S_n$ with both $m$ and $n$ even). So, the question is: how can one build a minimal set of generators for the direct product $S_{n_1} \times S_{n_2} \times \ldots \times S_{n_k}$?","['abstract-algebra', 'direct-product', 'group-theory']"
1948062,"How to prove that there exist $a$ and $b$ in [0,1] such that $\left| f\left( a \right) +g\left( b \right) -ab \right| \ge \frac { 1 }{ 4 } $?","Let $f$ and $g$ be any real-valued functions defined on [0,1],Prove that there exist $a$ and $b$ in [0,1] such that $$\left| f\left( a \right) +g\left( b \right) -ab \right| \ge \frac { 1 }{ 4 }  $$ I don't know how can I start with this question. So , I need some hints which can help me to approach this problem .","['algebra-precalculus', 'functional-analysis', 'functions']"
1948097,Choice needed in equiv. of two definitions of well-founded set,"(old question: the usual proof of Well-founded induction uses Dependent choice. Is it necessary?
I've heard that Well-founded induction is equivalent to dependent choice - I highly doubt that, but I couldn't find any references.) EDIT : Hagen's answer suggests, that I have a different definition of well-founded set and my question is - at the end of the day - about their equivalence. I should've been more precise in my original question, let me correct that. We work with a set with a binary relation $(X, R)$, let's assume $R$ is irreflexive for it makes things technically smoother. (Hagen's) $(X, R)$ is well-founded, when every nonempty subset $S\subseteq X$ has a minimal element, meaning an element $m$ such that there's no $s\in S$ satisfying $sRm$. (Mine) $(X, R)$ is well-founded, when there's no infinite sequence $(a_i)_{i=0}^{\infty}$ such that $$\dots Ra_1Ra_0.$$ To get from the first to the second is obvious, the first take on the other direction uses dependent choice. This really should be the question: How much choice do we need to make these definitions equivalent? Is the full dependent choice necessary? Just for the completeness, the well-founded induction: Let $P$ be a property on elements of  a well-founded set $(X,R)$, then $$\forall x.(\forall y.(yRx\implies P(y))\implies P(x)) \implies \forall x.P(x).$$
This is a very general version of induction principle, generalizing transfinite or structural induction.","['induction', 'elementary-set-theory', 'axiom-of-choice']"
1948119,Curves of constant curvature,"In planar case, curves of constant curvature are lines and circles. In spatial case, if torsion is also constant, then it must be circular helix. However, if torsion is arbitrarily given, such as $\tau(s)=e^s$, can we solve it explicitly? If not, I wonder what characteristic properties it satisfies. (For example, $\tau/\kappa$ is constant iff general helix.) Any help will be appreciated.","['real-analysis', 'curves', 'calculus', 'ordinary-differential-equations', 'differential-geometry']"
1948128,"Can locally ""a.e. constant"" function on a connected subset $U$ of $\mathbb{R}^n$ be constant a.e. in $U$?","Consider a non-empty connected open subset $U$ of $\mathbb{R}^n$. Suppose a measurable function $u:U\to\mathbb{R}$ is locally constant on $U$, then it must be constant on $U$ according to this question . Here is my question: What if one changes ""locally constant"" to ""locally a.e. constant""? More precisely, assume that for every $x\in U$ there is an open neighborhood $V$ of $x$ in $U$ such that $u$ is constant a.e. in $V$. Can one conclude that $u$ is constant on $U$ a.e.? [Motivation] This question is mostly for a rigorous last step in the proof of this problem.","['general-topology', 'real-analysis', 'measure-theory']"
1948132,"Solvability of a system related to the subsets of {1,2,3}","Question : Is there a solution for the following system? (if so I'm interested in an explicit solution) $x_1>x_2, \ \ x_1>x_3, \ \ x_1>x_4,$ $ x_2>x_5, \ \ x_2>x_6, \ \ x_3>x_5, \ \ x_3>x_7, \ \ x_4>x_6, \ \ x_4>x_7,$ $x_5>1, \ \ x_6>1, \ \ x_7>1,$ $x_2 x_3 \le x_1 x_5, \ \ x_2 x_4 \le x_1 x_6, \ \ x_3 x_4 \le x_1 x_7$ $x_5 x_6 \le x_2, \ \ x_5 x_7 \le x_3, \ \ x_6 x_7 \le x_4$ $x_1-x_2-x_3-x_4+x_5+x_6+x_7-1 \le 0$ Motivation : Let $B_n$ be the rank $n$ boolean lattice (i.e. the subset lattice of $\{1,2, \cdots , n\}$). A labeling $f: B_n \to \mathbb{R}_+$ is called allowed if it satisfies: $f(\emptyset) = 1$ $a \subset b \Rightarrow f(a) < f(b)$ $  f(a)f(b) \le f(a \cup b)f(a \cap b)$, $\forall a,b \in B_n$ Let $\varphi(f) := (-1)^n\sum_{a \in B_n} (-1)^{|a|} f(a)$ be the Euler totient of the labeling $f$. Remark : For $n=1$, we see directly that $\varphi(f)>0$, and for $n=2$ it is proved in this answer . Our system admits a solution iff there is an allowed labeling of $B_3$ with a negative Euler totient. To understand that take $f(\{1,2,3\})=x_1$, $ f(\{1,2\})=x_2 $, $f(\{1,3\})=x_3$, $f(\{2,3\})=x_4$, $f(\{1\})=x_5$, $f(\{2\})=x_6$, $f(\{3\})=x_7$ and $f(\emptyset)=1=x_8$. $ \ \  \ \ \ \ \ \ \ \  \  \ $ Remark : If $f$ maps to $\mathbb{N}$ and $x_1 < 100$ then $\varphi(f) > 0$ (checked by SAGE). If moreover $a \subset b \Rightarrow f(a) \mid f(b)$, then we get the same for $x_1 < 10^5$. If $  f(a)f(b) = f(a \cup b)f(a \cap b)$ then $\varphi(f) = (x_5-1)(x_6-1)(x_7-1)>0$. Bonus question : What's the lowest $\varphi(f)$ for $f$ an allowed labeling of $B_3$? Lemma : $\varphi(f)>-1$. Proof : First $(x_1-x_2)(x_1-x_3)(x_1-x_4)>0$, so $x_1^3 + x_1(x_2x_3+x_2x_4+x_3x_4) > x_1^2(x_2+x_3+x_4)+x_2x_3x_4$, then $x_1 + \frac{x_2x_3}{x_1}+\frac{x_2x_4}{x_1}+\frac{x_3x_4}{x_1} > x_2+x_3+x_4+\frac{x_2x_3x_4}{x_1^2}$, but $x_5 \ge \frac{x_2x_3}{x_1}, x_6 \ge \frac{x_2x_4}{x_1}, x_7 \ge \frac{x_3x_4}{x_1}$. It follows that $\varphi(f) > \frac{x_2x_3x_4}{x_1^2}-1>-1$. $\square$ The following shows that $\frac{x_2x_3x_4}{x_1^2} \le 1$, so that we can't get a better lower bound for $\varphi(f)$ by this method. Lemma : $x_2x_3x_4 \le  x_1^2$. Proof : First $(x_2x_3)(x_2x_4)(x_3x_4) \le (x_1x_5)(x_1x_6)(x_1x_7)$, so $(x_2x_3x_4)^2 \le x_1^3x_5x_6x_7$, Then, $(x_2x_3x_4)^4 \le x_1^6(x_5x_6)(x_5x_7)(x_6x_7) \le x_1^6(x_2x_3x_4)$. It follows that  $(x_2x_3x_4)^3 \le  x_1^6$, so $x_2x_3x_4 \le  x_1^2$. $\square$","['inequality', 'systems-of-equations', 'lattice-orders', 'computer-algebra-systems', 'combinatorics']"
