question_id,title,body,tags
3993527,Confused about one Concept,"Today in my differential equation lecture, the teacher begin to write some examples in which differential equations appear, first begin with the classic example of a rocket, the example of the mass and the resort, and then he request to us give to he a example of a family of lines, and I tell to he $y=mx+b$ where $m,b$ are parameters and then he put $\frac{dy}{dx}=m$ and then he remplaze it in the original equation $y=\frac{dy}{dx}x+b$ and put that this are the ""Equivalent differential form of the family of curves. Then he  put an exercise and exactlly say the following Consider $y=A \sin x+ B \cos x$ familly of curves with two parameters $A,B$ .Find it´s equivalent differential form. I Never watch these kind of substitution in my life, and since he say that is a differential form I´m confused because the differential form of one equation is an expression like a $dz=f(x,y)dx+g(x,y)dy$ Is correct the teacher or it´s wrong, in the second case, can someone explain me or put the name of the concept. I think that is wrong with the word differential form because if I don´t make a mistake
the differential of the equation $y=mx+b$ is $dy=mdx$","['multivariable-calculus', 'calculus', 'soft-question', 'ordinary-differential-equations']"
3993547,is zero-measured set always closed?,"Is zero-measured set in $\mathbb{R}^n$ always closed?
Or can you please give an example of zero-measured non-closed set in $\mathbb{R}^n$ ? Here, measure means Lebesgue measure.","['measure-theory', 'lebesgue-measure']"
3993563,"Effect on expected value of conditioning on inequality between random variables (do we have E[X | X>S] ≥ E[X | X>S, Y>S]?)","I've been trying to prove the following inequality: $$
\mathbb{E}[X \mid X > S] \geq \mathbb{E}[X  \mid  X>S, Y>S]
$$ where $X$ , $Y$ , $S$ are mutually independent real-valued random variables (let's make them absolutely continuous to simplify). The intuition why it might be true is simple enough: if you condition on $Y>S$ , the posterior on $S$ is shifted “downwards”, and then the conditioning on $X>S$ is not as informative and raises the expected value of $X$ less. To make it simple, I have not succeeded proving this and I'm starting to wonder if it's in fact true, so I would like to know if anybody has an idea. For completeness, my most promising attempt used the correlation inequality. First we write: $$
\mathbb{E}[X \mid X>S, Y>S] = \frac{1}{P(X>S, Y>S)}\int_{-\infty}^{+\infty}p_S(s)P(Y>s)\int_s^{+\infty} p_X(x) x \,dx \,ds
$$ whence: $$
\mathbb{E}[X \mid X>S, Y>S] = \frac{1}{P(X>S, Y>S)}\int_{-\infty}^{+\infty}p_S(s)P(Y>s, X>s)\mathbb{E}[X \mid X>s] \,ds
$$ Take $f(s) = \mathbb{P}(X > s, Y > s)$ and $g(s) = \mathbb{E}[X  \mid  X > s]$ . Because $f$ is decreasing and $g$ is increasing, we get: $$
\mathbb{E}[X \mid X>S, Y>S] \leq \frac{1}{\dots} \left(\int_{-\infty}^{+\infty} p_S(s) P(Y>s, X>s) \,ds\right) \left(\int_{-\infty}^{+\infty} p_S(s) \mathbb{E}[X \mid X>s] \,ds \right)
$$ i.e.: $$
\mathbb{E}[X \mid X>S, Y>S] \leq \int_{-\infty}^{+\infty} \frac{p_S(s)}{\mathbb{P}(X > s)} \int_s^{+\infty} p_X(x) x \,dx \,ds
$$ This is frustratingly close but not quite it; if you apply the correlation inequality again you get something that is too lax (by Jensen's inequality), and I'm worrying what I got here is too lax already, or that I made a mistake. (Why do I care? It happens to be relevant to a Bayesian model of certain linguistic phenomena. Basically, if this is true, following a certain model, you predict that upon hearing “Alice and Bridget are tall”, people will not imagine Alice to be as tall as if they hear “Alice is tall”.)","['conditional-probability', 'conditional-expectation', 'inequality', 'probability-theory', 'random-variables']"
3993570,Is the following definition of an elliptic curve correct?,Im new to algebraic geometry so I want to make sure im getting my definitions right. I know there are a few ways to state what an elliptic curve is (ex a smooth projective curve of genus one with distinguished $K$ -rational point). But I am wondering if the following is equivalent. For simplicity lets just work over $\mathbb{C}$ .: $\textbf{Definition:}$ An elliptic curve $E$ is a non-singular projective curve in $\mathbb{P}^2$ of the form $$E: Y^2Z + a_1XYZ + a_3YZ^2 = X^3 + a_2X^2Z + a_4XZ^2 + a_6Z^3 $$ I am wondering if this is sufficient to define an elliptic curve?,"['number-theory', 'abstract-algebra', 'algebraic-geometry', 'elliptic-curves']"
3993591,Proving a solution to ODE exists without solving it,"I am analysing the ODE $$\frac{\mathrm{d}}{\mathrm{d}x}\left(\ln y(x)+\frac{c}{y(x)}\right)=1,\quad y(1)=1.$$ I would like to show, without attempting to solve the ODE, that $\exists$ a solution $y(x)>c$ if $\ln c-c<-2$ . I am wondering what techniques I should employ to do so? I attempted to stuff it into a form involving Grönwall's inequality, but have not had much progress with that. What should I do? Thanks!","['inequality', 'ordinary-differential-equations']"
3993596,Intuition behind the exponential loss function,"I'm reading about AdaBoost from the book The Elements of Statistical Learning .
The book mentions that, to train the model, the exponential loss function is used: $$L(y, f (x)) = e^{−y f (x)},$$ where $y$ is the expected output and $f(x)$ is the model output given the feature $x$ .
The problem I have understanding this loss function is that ""same errors"" give different loss. For example, $y=1$ and $f(x)=0.8$ then $L(y, f (x)) = e^{−0.8}$ $y=0$ and $f(x)=.2$ then $L(y, f (x)) = e^{0.2} \neq e^{−0.8}$ In both cases, $y-f(x)= 0.2$ , but the loss functions give different values. Could anyone tell me what I'm missing here?
Thank you!","['machine-learning', 'statistics', 'intuition']"
3993602,Norm of a smooth function is smooth?,"My lecture notes seem to use this, although I can't shake the feeling that it's wrong. Let $\phi: [a,b] \to \mathbb{R}^n$ for $n \ge 2$ be smooth (infinitely differentiable) satisfying $|\phi '(t)| \neq 0$ for all $t \in [a,b]$ . Then, $|\phi'(t)|$ is also a smooth function. Does anyone have a nice proof for this? (My first 'counterexample' was $\log$ , but it doesn't satisfy the condition $|\phi '(t)| \neq 0$ .) Also, is $|\phi(t)|$ a smooth function too? Although this is from a course in elementary differential geometry, my question is essentially a calculus question...","['real-analysis', 'multivariable-calculus', 'calculus', 'derivatives', 'differential-geometry']"
3993619,Proof that a sequence $(x_n)_{n \ge 1}$ such that $|x_{n+1} - x_n| < \frac{1}{n}$ converges.,"I'm studying for a test and I came across this exercise: Let $(x_n)_{n \ge 1}$ be a sequence such that $x_{2n-1} \le x_{2n+1} \le x_{2n+2} \le x_{2n}, \hspace4ex |x_{n+1} - x_n| < \frac{1}{n}$ Prove that the sequence converges. I've been stuck on this question for a while now, by doing some arithmetics I've gotten a few inequalities, namely: (1) $x_{2n} - x_{2n+1} < \frac{1}{2n}$ (2) $x_{2n+2} - x_{2n+1} < \frac{1}{2n+1}$ (3) $x_{2n} - x_{2n-1} < \frac{1}{2n-1}$ But I have no idea where to go from here, or even if those inequalities are useful at all.","['sequences-and-series', 'real-analysis']"
3993760,Symmetric group has a minimal set of generators of any size for $n \geq 4$,"For $n \geq 4$ , I want to show that $S_n$ has a minimal set of generator of size $k$ for $k \in \{2, \cdots, n - 1 \}$ , and also that it does not have a minimal set of generator of size $n$ . So I already know that $S_n$ can be generated by $\{ (1, \cdots, n), (1, 2) \}$ and also a set of transpositions, but I am not sure how to proceed from here. Can I somehow extend from $k = 2$ to more?","['symmetric-groups', 'finitely-generated', 'group-theory']"
3993782,Derivative of quadratic form for matrices and vectors,"I'm not very familiar with multivariable calculus as it relates to matrices. Could someone explain, in detail, why $$\frac{\partial}{\partial x} \left[ x^T A x \right] = (A + A^T)x$$ In the case of a symmetric matrix and $$\frac{\partial}{\partial x} \left[ x^T A x \right] = 2Ax$$ if the matrix is not symmetric. I'm mainly confused about how we even arrive at the first derivative. However, I understand how the first derivative simplifies to the second in the case that A is symmetric.","['partial-derivative', 'machine-learning', 'multivariable-calculus', 'matrix-calculus']"
3993825,Probability of getting an ace in the 2nd draw without knowing the first draw,"The question is: Given a 52-card deck, two cards are drawn without replacement. What is the probability that the second card is an ace? So I think I already found the answer (correct me if my answer is incorrect), but I still have a question. Here's my answer: $$\frac{48}{52}\cdot\frac{4}{51}+\frac{4}{52}\cdot\frac{3}{51}=0.07692$$ So in my answer, I'm just adding the cases. The first case is where the first draw isn't an ace and the second case is where the first draw is an ace. Now to my question. How come the expression below seem to overcount, despite seemingly doing the same thing ? $$\frac{\binom{48}{1}\binom{4}{1}}{\binom{52}{2}} + \frac{\binom{4}{2}}{\binom{52}{2}} = 0.1493213$$ My guess is that the first term is double counting, but I really can't grasp why.","['discrete-mathematics', 'combinatorics', 'probability']"
3993836,The Soup Problem: how to asymptotically fairly split a geometric series and a constant one using a single pattern?,"Literally every time I'm serving some soup I'm thinking of this little mathematical problem I devised. Imagine you have a very large (= infinite, for the purposes of the actual problem) bowl of soup and want to divide it into two halves using a (finite) ladle. The trouble is that some of the good stuff floated to the surface and every scoop takes a part $p$ of it, $p < \frac12$ (so its amount goes down geometrically with a quotient of $1-p$ ). The other good stuff is evenly dissolved in the volume. The protocol is that you always take a full scoop in this manner, so the only freedom is in what order do you empty the ladle into $A$ or $B$ 's plate. What order would you take so that, asymptotically, both parties get fair shares in both respects? Obviously the answer can't be $ABABAB\ldots$ , because $A$ would always get $1/(1-p)$ -times more than $B$ . $\overline{ABBA}$ does not work either because $1+(1-p)^{-3}$ is always greater than $(1-p)+(1-p)^2$ . In fact (except for a countable number of special values of $p$ (*)) it can't be any periodic infinite word of $A$ and $B$ for a slight variant of the same reason. One more thing I found is that the greedy algorithm (go $A$ until it has more than $B$ , then swap) won't work either because while it would always satisfy splitting of the geometrical part, it would break the linear one (number of scoops). That's all I know at the moment. Rephrased: for a given $1-p =: q \in (\frac12, 1)$ , find a subset $M ∈ \mathbb{N}$ such that both $\displaystyle\lim_{N→∞} \frac{\#\left( M \cap \{1,\ldots,N\} \right)}{N} = \frac12$ , $\displaystyle\sum_{n=1}^∞ q^n \chi_M(n) = \frac12 \sum_{n=1}^∞ q^n$ . I'm pretty sure the answer will lie somewhere in the theory of non-integer base systems or combinatorics on words or both, but I know little of those. (*) For example, for $q = φ - 1$ the equality $q + q^2 + q^3 = 1 + q^4 + q^5$ holds, so for $p = 2 - φ \approx 0.381966$ the sequence $\overline{ABBBAA}$ would work.","['summation', 'puzzle', 'combinatorics-on-words', 'number-systems', 'sequences-and-series']"
3993861,Cosine and sine rule on a triangle,"$\triangle ABC$ is isosceles with legs $AC=BC=b$ and angle $\measuredangle ACB=\gamma$ . Let $CD$ and $BM$ be altitudes that intersect at $H$ . Find $MD$ . I have been struggling with this problem for an hour now. I really don't know how to start. I was thinking about using cosine or sine rule, but it seems useless at the end. For example $$AB^2=AC^2+BC^2-2\cdot AC\cdot BC\cdot \cos\gamma=2b^2-2b^2\cos\gamma$$","['euclidean-geometry', 'trigonometry', 'geometry']"
3993863,zero set of complex analytic functions has zero measure,"Based on the answer here , the following result is clear: ``Given a complex analytic function $f(x)$ with $x$ is in an open connected set $D  \subseteq 
  \mathbb{C}^n$ , the zero set $$F = \{x \in D | f(x) = 0\}$$ has $2n$ -dimensional Lebesgue measure zero"" Now consider a slightly different situation: A complex analytic function $g(z)$ with $z \in D \times V$ , where $D$ is an open connected
subset of $\mathbb{C}^n$ and $V$ is an open connected subset of $\mathbb{R}^m$ . Is it true that the zero set of $g(z)$ $$F_2 = \{z \in D \times V | g(z) = 0\}$$ has $(2n+m)$ -dimensional Lebesgue measure? How to prove ( or to modify the proof in this book , Corollary 10, p.9. to obtain) the above result?","['complex-analysis', 'analytic-functions']"
3993902,Central limit theorem for dependent Bernoulli random variables on the edges of a sequence of growing hypercubes?,"Imagine a growing sequence of hypercube graphs. For each integer n, you have the $n$ -hypercube with $2^n$ vertices.
Now, assign $\text{Bernoulli}(1/2)$ i.i.d. random variables to all the vertices.
On each edge, you define a r.v. taking values in $\{0,1\}$ such that it is $1$ if the Bernoulli r.v.s of the two vertices it touches are equal, and it is $0$ otherwise.
So for each given n, you have Bernoulli r.v.s that are correlated on the edges and the correlation decreases with the distance (of the shortest path on the $n$ -hypercube).
Note that going from one vertex to another on the hypercube requires that EXACTLY one binary component of the position changes by 1.
For example, with n=3, you can go from (1,1,0) to (1,1,1) because exactly one component (the third one) changes by 1.
This generalizes to all dimensions.
Now, the question is, if you take the sum of the (dependent) Bernoulli r.v.s on the EDGES (not vertices) and normalize it correctly (i.e. you translate by the expectation and divide by the standard deviation), does this normalized sequence converges to a $\text{Normal}(0,1)$ distribution ? I know there are versions of the CLT for strong mixing sequences, but I didn't find a version that would suit the problem here.","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
3993921,Can two distinct sets of N numbers between -1 and 1 have the same sum and sum of squares?,"Is is possible to find two different sets of numbers $\{ a_1, a_2, \dots, a_N\}$ and $\{ b_1, b_2, \dots, b_N\}$ with $a_i,b_i\in[-1,1]$ such that $\sum a_i = \sum b_i$ and $\sum a_i^2 = \sum b_i^2$ are both true at the same time? EDIT: Also, no number in each set may be $0$ , i.e. $a_i,b_i \neq 0$ . John Omielan has posted an example for $N=3$ , before I edited the question. For $N=1$ , it is obvious that this is impossible, because $a_1 = b_1$ (sum over elements in the set) and $a_1 \neq b_1$ (the sets must be different) cannot be true at the same time. Is there a minimum number of $N$ for which it is possible to find such sets, or is it never possible?","['sums-of-squares', 'algebra-precalculus', 'summation']"
3993924,I can't find my error in solving line integral in two different methods,"I have this problem: given the points $A(-1,-2)$ and $B(2,1)$ the path $T$ from $A$ to $B$ is on the circule $(x-0.5)^{2}+(y+0.5)^{2}=4.5$ caculate: $$\int _{T}\frac{ydx-xdy}{x^{2}+y^{2}}$$ after a short calculate I found out that: $P_{y}=Q_{x}$ and with this following simply connected space $D$ , making $\vec{F}$ conservative vector field: Therefore I can choose a different path between the two points.
Now my main error is that I calculate the integrals with two different paths and I've given different results. for the path $C$ on the circle $x^{2}+y^{2}=5$ I've got that: the red path is part of the circule. $$\left\{\begin{matrix}
x= \sqrt[]{5}\cos(t) & dx = (-1)\sqrt[]{5}\sin(t)dt \\  
y= \sqrt[]{5}\sin(t) & dy = \sqrt[]{5}\cos(t)dt
\end{matrix}\right.$$ and by using caculator: $$\int _{T}\frac{ydx-xdy}{x^{2}+y^{2}} = \int _{C}\frac{((\sqrt[]{5}\sin(t)\cdot (-1)\sqrt[]{5}\sin(t))-(\sqrt[]{5}\cos(t)\cdot\sqrt[]{5}\cos(t)))dt}{5}$$ $$=-1\cdot\int _{C}1\cdot dt = -1\cdot\int_{\arccos (\frac{-1}{\sqrt[]{5}})}^{\arccos (\frac{2}{\sqrt[]{5}})} 1 \cdot dt = \frac{\pi }{2}$$ but for the path $E$ wich is the sum of the following paths: $E1$ , $E2$ I'm getting diffrent result. $$E_{1}(t) = (t,-2)\rightarrow\left\{\begin{matrix}
dx= dt   \\  
dy= 0 
\end{matrix}\right., -1\leq t\leq 2$$ $$\int _{E_{1}}\frac{ydx-xdy}{x^{2}+y^{2}} = \int_{-1}^{2}\frac{-2}{t^{2}+4}dt$$ $$E_{2}(t) = (2,t)\rightarrow\left\{\begin{matrix}
dx= 0   \\  
dy= dt 
\end{matrix}\right., -2\leq t\leq 1$$ $$\int _{E_{2}}\frac{ydx-xdy}{x^{2}+y^{2}} = \int_{-2}^{1}\frac{-2}{t^{2}+4}dt$$ in this picture the paths $E_{1}$ and $E_{2}$ are drwan in pink: and I'm getting using caculator that: $$\int _{T}\frac{ydx-xdy}{x^{2}+y^{2}} = \int_{-1}^{2}\frac{-2}{t^{2}+4}dt + \int_{-2}^{1}\frac{-2}{t^{2}+4}dt \approx -2.49$$ so, where is my error?","['integration', 'multivariable-calculus', 'path-connected']"
3993943,Does the set intersection operation has unit,"I'm trying to get better understanding of binary operations, and I came across this problem: namely on one online discussions I saw that set intersection as binary operation doesn't have a unit, however I think it has: The set intersection operation for any set universe $U$ is defined on the $\mathcal P(U) \times \mathcal P(U)$ set. From the definition of unit for binary operation, wouldn't it be true that the $U$ set is the unit of this operation. Any set in $PU$ intersected with $U$ will give back the set, and it works in both directions. Am I doing something wrong, or the discussion on that place was wrong?","['elementary-set-theory', 'binary-operations']"
3993990,Book Recommendation on Time Series Statistics,"Professionally I am analysing high-frequency data coming from motion sensors and alike.
I would like to ""up my theoretical background game"" in this area and am therefore looking for recommendations to books about (I guess) statistics and probability theory concerning time series pehomena. Concerning my background: I hold a master's degree in theoretical mathematics (Did some Probability Theory, statistics und stochastical analysis, though it was not my focus) Any hints would be greatly appreciated.
Thanks in advance!","['time-series', 'statistics', 'book-recommendation', 'reference-request']"
3994014,Mean Gaussian Curvature using non-unit vectors.,"Pg.248 of ""Textbook in Tensor Calculus and Differential Geometry"" by Prasun Nayak. Let us suppose that $\lambda_{h|}^i$ is not a unit vector and therefore, the mean curvature $M_h$ in
this case is given by $$M_h=-\frac{R_{ij}\lambda_{h|}^i\lambda_{h|}^j}{g_{ij}\lambda^i_{h|}\lambda^j_{h|}} \tag{1}$$ A proof was not given so I am trying to derive this. For when $\lambda_{h|}^i$ are components of an orthornormal set of vectors $$M_h=\sum_{k}K_{hk}=\sum_k\frac{\lambda_{h|}^l\lambda_{k|}^i\lambda_{h|}^j\lambda_{k|}^rR_{lijr}}{\lambda_{h|}^l\lambda_{k|}^i\lambda_{h|}^j\lambda_{k|}^r(g_{lj}g_{ir}-g_{lr}g_{ij})}=\sum_k\frac{\lambda_{h|}^l\lambda_{k|}^i\lambda_{h|}^j\lambda_{k|}^rR_{lijr}}{(1)\cdot(1)-0}$$ $$=\lambda_{h|}^l\lambda^j_{h|}R_{lijr}g^{jr}=-\lambda_{h|}^l\lambda^j_{h|}R_{lj}$$ where $\sum_k\lambda^i_{k|}\lambda^r_{k|}=g^{ir}$ . Now for when $\lambda_{h|}^i$ are not components of an orthonormal set of vectors, succumbing to the same approach gives $$M_h=\sum_k K_{hk}=\frac{\lambda_{h|}^l\lambda_{h|}^j R_{lijr}}{(g_{lj}\lambda_{h|}^l\lambda_{h|}^j)g_{pq}}\sum_k\frac{\lambda_{k|}^i\lambda_{k|}^r}{\lambda_{k|}^p\lambda_{k|}^q} \tag{2}$$ It is not obvious to me how $(2) \to (1)$ . Modifying the definition of $M_h$ $$M_h=\frac{\sum_k\lambda_{h|}^l\lambda_{k|}^i\lambda_{h|}^j\lambda_{k|}^rR_{lijr}}{(g_{lj}\lambda_{h|}^l\lambda_{h|}^j)\sum_k g_{pq}\lambda^p_{k|}\lambda_{k|}^q}=-\frac{R_{ij}\lambda_{h|}^i\lambda_{h|}^j}{g_{ij}\lambda^i_{h|}\lambda^j_{h|}N} \tag{3}$$ which is the nearest I can get to $(1)$ , but with an extra factor $\frac{1}{N}$ . What's gone wrong? Is there a way to derive $(1)$ or is $(1)$ a definition itself?","['riemannian-geometry', 'tensors', 'geometry', 'curvature', 'differential-geometry']"
3994062,Confidence interval for likelihood-ratio test,"I've been trying to solve the following problem: Let $X_1,\ldots,X_n$ be a sample of a random variable $X\sim\text{Exp}(\lambda)$ , i.e., the exponential distribution with parameter $\lambda>0$ . Consider the null hypothesis $H_0: \lambda=\lambda_0$ and the alternative hypothesis $H_1: \lambda\neq\lambda_0$ . Construct a confidence interval for the likelihood-ratio test with significance level $\alpha$ . I know that $\sum\limits_{i=1}^nX_i\sim\text{Gamma}(n,\lambda)$ , so $2\lambda n\mathbf{\overline X}\sim\text{Gamma}(n,\frac{1}{2})\sim\chi^2_{2n}$ , and normally I'd use that to construct the confidence interval picking $c_1$ and $c_2$ quantiles of the $\chi^2_{2n}$ distribution such that $P(c_1\le2\lambda n\mathbf{\overline X}\le c_2)=1-\alpha$ and then isolate $\lambda$ (usually I take $c_1$ and $c_2$ such that there is a probability of $\frac{\alpha}{2}$ on both sides of the opposite inequality). However, now I must use the likelihood-ratio test first, and then construct the confidence interval, and that's where I'm stumped. I obtained the likelihood-ratio test statistic: $\lambda(\mathbf{x})=\lambda_0^n\,\mathbf{\overline x}^ne^n\exp(-n\mathbf{\overline x}\lambda_0)$ , so the test would be $\begin{cases}\text{Reject }H_0\text{ if }\lambda(\mathbf{x})\le c\\\text{Not reject }H_0\text{ otherwise}\end{cases}$ with $c$ such that $P_{\lambda_0}\big(\lambda(\mathbf X)\le c\big)=\alpha$ , where the subindex indicates we calculate that probability assuming $\lambda=\lambda_0$ . I can isolate a nicer part of the likelihood-ratio test in the inequality $\lambda(\mathbf x)\le c\Rightarrow\lambda_0\,\mathbf{\overline x}\exp(-\lambda_0\mathbf{\overline x})\le\dfrac{\sqrt[n] c}{e}$ , so naming $k=\dfrac{\sqrt[n] c}{e}$ we get $P_{\lambda_0}\big(\lambda(\mathbf X)\le c\big)=P_{\lambda_0}\Big(\lambda_0\,\mathbf{\overline X}\exp(-\lambda_0\,\mathbf{\overline X})\le k\Big)$ . This is the point of the problem where I need help, since I have no idea how to proceed. Any ideas? Thanks in advance!","['statistics', 'confidence-interval', 'maximum-likelihood']"
3994067,Substitute $x = \sec u$ to evaluate $\int\sqrt{x^2-1}\ dx$: but why $\tan u= \sqrt{x^2 - 1}$ rather than $\tan u= -\sqrt{x^2 - 1}$ is used?,"I tried evaluating $\int\sqrt{x^2-1}\ dx$ using the substitution $x = \sec u$ I know my method gets to the same answer as WolframAlpha , namely: $$\int\sqrt{x^2-1}\ dx = \frac{1}{2} \left(\ x \sqrt{x^2-1} - \ln\left|x + \sqrt{x^2-1}\right|\ \right) + C,\quad (*)$$ but there is one step I can't justify. When I got to $$\int \tan^2(u)\sec(u)\ du = \frac{1}{2}\left(\ \tan(u)\sec(u) - \ln\left|\sec(u)+\tan(u)\right|\ \right) + C,$$ I then have to substitute stuff back in terms of $x$ . Now $$x=\sec(u) \implies \tan^2(u) = x^2 - 1,$$ but I don't see how this implies $\tan(u) = \sqrt{x^2 - 1}$ . Comparing the graphs of $\sec(u)$ and $\tan(u)$ , I don't see why not: $\ \tan(u) = -\sqrt{x^2 - 1}$ , which would give: $$\int\sqrt{x^2-1}\ dx = \frac{1}{2} \left(\ -x \sqrt{x^2-1} - \ln\left|x - \sqrt{x^2-1}\right|\ \right) + C,\quad (**)$$ which is a different answer than $(*)$ ? Now I noticed that $(*) = -(**)\ $ (ignoring the $C \to -C)$ . I can see from the graph of $\sqrt{x^2-1}$ that for $x>1$ , the definite integral $\int^x_1\sqrt{t^2-1}\ dt = (*),$ and for $x<-1,\ \int^{-1}_x\sqrt{t^2-1}\ dt = (**)$ So is the indefinite integral sort of poorly defined, or would you say it is: $$\int\sqrt{x^2-1}\ dx = \pm \frac{1}{2} \left(\ x \sqrt{x^2-1} - \ln\left|x + \sqrt{x^2-1}\right|\ \right) + C?$$","['integration', 'indefinite-integrals', 'substitution']"
3994126,An other total-variation inequality of mine,"Given differentiable, continuous $f\left ( x \right )$ on the interval $\left [ 0, 1 \right ]$ so that $\int_{0}^{1}f\left ( x \right ){\rm d}x= 0.$ Prove that $$\left | \int_{0}^{1}xf\left ( x \right ){\rm d}x \right |\leq \frac{1}{12}\max\left | {f}'\left ( x \right ) \right |$$ I think I should transform the constant $1/12$ into an integral like $k\int_{0}^{1}x^{2}{\rm d}x,$ but $k$ is very unusual, I need to your helps, even an example of $f\left ( x \right )$ so that $\int_{0}^{1}f\left ( x \right ){\rm d}x= 0$ in order to know what I must do with the constant. Thanks a real lot.","['integration', 'examples-counterexamples', 'substitution', 'constants', 'derivatives']"
3994206,Find an extreme points for $f(x) ={x\ln x\over x^2-1}$,"I had a hard time with finding an extreme points for $$f(x) ={x\ln x\over x^2-1}$$ We have $$f'(x)= {(x^2-1)-(x^2+1)\ln x \over (x^2-1)^2}$$ Since solving $f'(x)=0$ is the same as solving this transcendental equation $$\ln x = {x^2-1\over x^2+1}$$ for $x \ne 1$ , I observe this function $$g(x) = {x^2-1\over x^2+1} -\ln x$$ which is decreasing and thus no solution. Is there any more direct way, without involving $g(x)$ ?","['maxima-minima', 'algebra-precalculus', 'functions', 'real-analysis']"
3994213,Sum of even and odd naturals,"I want to prove that the sum of the first even $k$ natural numbers is $k^2+k$ given that the sum of the first odd $k$ naturals is $k^2$ . So, \begin{align*} \underbrace{1+3+5}_{k=3}&=k^2=3^2=9 \\ &\vdots \\ \underbrace{1+3+5+\cdots+(2k-1)}_{k \ terms}&=k^2, n\in\mathbb N \end{align*} needs to be used to prove that \begin{align*}
\underbrace{2+4+\cdots+(2k)}_{k \ terms}= k^2+k, n\in\mathbb N
\end{align*} This would be easier to do for me if I could write this using summation notation, but I can't figure out a way to do this because $k$ isn't an upper limit, but the number of terms. All I can say is that if the given sum is true, then if I add $1$ to every term $k$ times, then every term on the lefthand side would be even and the righthand side would add a $k$ term. Like so, \begin{align*}1+3+\cdots+(2k-1)&=k^2 \\ 1+1+\cdots+1&=k 
\\ \implies 2+4+\cdots+(2k)&=k^2+k\end{align*} But It feels like this explanation is sort of hand-wavy.","['algebra-precalculus', 'summation', 'discrete-mathematics']"
3994314,"Why we consider counterimages to define continuos, measurable and measure-preserving functions?","I will specify the question. In a conference by Arnold (on continued fractions, you can find it on YouTube) he says that the fact that we consider preimages when defining measure preserving maps is something deep, which links to controvariant and covariant concepts in mathematics. What does this mean? I know the definition of measure preserving map, and I know Ergodic Theory and all the consequences, but is there something more deep about why we have to take the preimage and not the image when defining measure preserving functions? We have a similar phenomena in topology and measure theory, when we define continuos and measurable maps: to say that a map preserve some kind of structure, we ask something involving the preimage. Is there something deep also in those definitions? In topology for example we have that the continuous  image of a compact or of a connected is compact or connected, but the preimage of an open or a closed is an open or a closed (in topology we can define open and closed maps, but they are less used that continuous maps) I have never asked myself this question before because in topology the definition of continuous map arises very easily from the epsilon-delta definition in metric space, and in measure theory the definition of measurable map it works well with integrals. But in Ergodic Theory if I were asked to chose the definition of measure preserving I would have said that the measure of the image is equale to the one of the starting set. Are there similar example in Maths of this phenomena?","['measure-theory', 'ergodic-theory', 'category-theory', 'general-topology', 'probability-theory']"
3994347,Cardinality of a quotient set $\mathbb{N}^\mathbb{N}/\sim$,"I'm having trouble with the following task: On the set of all functions $\mathbb{N}^\mathbb{N}$ an equivalence relation $\sim$ is defined in the following way: $f \sim g \iff \forall n \in \mathbb{N}. f(f(n)) = g(g(n))$ . Prove that the quotient set $\mathbb{N}^\mathbb{N}/\sim$ is uncountable. I've tried to find an injective function from $\mathbb{N}^\mathbb{N}$ to $\mathbb{N}^\mathbb{N}/\sim$ and I've tried proving that every equivalence class has only one element, but I haven't succeeded in either of the attempts. I can't even figure out what would be the main idea needed for the proof.","['cardinals', 'equivalence-relations', 'functions', 'sequences-and-series', 'set-theory']"
3994376,Show with a counterexample that $A\nsubseteq A^A$,"Definition A set $R$ is a (binary) relation if any its element is a ordered pair, that is $z\in R$ if and only if there exist $x$ and $y$ such that $z=(x,y)$ . In particular if $R$ is contained in the cartesian product of two set $A$ and $B$ we say that $R$ is a relation of between $A$ and $B$ and if $B$ is equal to $A$ we say that $R$ is a relation in $A$ . Definition If $R$ is a relation we call domain of $R$ that set whose elements are a first coordinate of some pair of the relation, that is $$
\text{dom}\,R:=\{x:\exists\,y\,\text{such that}\,(x,y)\in R\}
$$ and analogously the range of $R$ is that set whose elements are the second coordinate of the relation, that is $$
\text{rank}\,R:=\{y:\exists\,x\,\text{such that}\,(x,y)\in R\}
$$ So it is possible to prove that the above two defined set exist using the $ZFC$ 's formalism but this now has not matter so we proceed to give the following well know definition. Definition A function $f$ is a relation such that if $(x,y)$ and $(x,z)$ are such that $(x,y),(x,z)\in f$ then $y=z$ . Definition Let $A$ and $B$ sets. So the set whose element are functions from $A$ to $B$ is denoted by the symbol $B^A$ . Again it is possible to prove that the above defined set exist using $ZFC$ 's formalism but now this again has not matter. So by the previous things I thought that any element $a$ of a (not empty) set $A$ could be regarded as a costant function so that I ask to me if $A$ is a subset of $A^A$ but this seems to me very stranger and so I would like to know the opinon of anyone with more competences of mine. In particular I have observed that if $a\in A^A\cap A$ then $a\subseteq A\times A$ and so either $a=\emptyset$ either there exist $c,d\in A$ such that $a=(c,d)\in\{\{c\},\{c,d\}\}$ but by this I did not deduce any contradiction. So could someone help me, please?","['elementary-set-theory', 'solution-verification', 'set-theory']"
3994388,Zariski closure of an algebraic linear group,"Let $G$ and $H$ algebraic linear groups and $\phi : G \to H$ a regular group homomorphism. I wonder if $\overline{\phi(G)}$ (the Zariski closure of $\phi(G)$ ) is again a subgroup and how this could be proven. First of all, I know Zariski closure $\overline{Z}$ of a set $Z$ is defined as $\overline{Z} = V(I(Z))$ . I imagine this definition is also valid for algebraic linear groups, since $G$ is an affine variety. However, I can’t seem to find a strategy for this proof: any ideas?","['zariski-topology', 'algebraic-geometry', 'algebraic-groups']"
3994430,if $\lim _{x\rightarrow \infty}{f'(x)}=0$ then does $\lim_{x \rightarrow \infty}{f(x)}$ exist in the broad sense,"Let $f$ be a differentiable a function in $\mathbb{R}$ , and let $\lim _{x\rightarrow \infty}{f'(x)}=0$ Does $\lim_{x \rightarrow \infty}{f(x)}$ exist in the broad sense? I'm really lost here. This exercise is from a section on MVT, and intuitively it seems to be correct, but I can't seem to find a lead. If someone could just give me a hint that would be great. So far my best shot has been using Heine's definition of the limit, but no dice.","['limits', 'calculus', 'derivatives', 'real-analysis']"
3994452,Closed form of $\cos(\frac{\pi}{7})$,"I saw someone derive the closed form for $\cos(\frac{\pi}{5})=\frac{\varphi}{2}$ , and got inspired to try to find a closed form expression for $\cos(\frac{\pi}{7})$ using the same method. In doing this, you get that $\cos(\frac{\pi}{7})$ is one of the solutions to the following polynomial. $$8x^3-4x^2-4x+1$$ If there is an accessible closed form for the roots of this polynomial, I cant seem to find it. I have tried factoring, guessing and also looked at wolfram alpha, which only gives massive closed forms involving imaginary numbers. So if anyone can find a closed form or show that none is accessible, that would be great!","['trigonometry', 'closed-form', 'roots', 'polynomials']"
3994530,The differential complex of smooth forms,"I'm  learning my self  about differential forms from the book by Loring Tu, and I've come across this sentence which seems important by I didn't understand it: "" The differential complex of smooth forms on a manifold can be pulled back under a smooth map, making the complex into a contravariant functor called the de Rham complex of the manifold"" I was thinking that the de Rham complex is the same as the complex of differential forms ! Could you please enlight the difference between de Rham complex and the complex of differential forms and what it is meant by ""contravariant functor"" and say more about the above sentence . Thanks a lot!","['differential-forms', 'de-rham-cohomology', 'algebraic-topology', 'differential-geometry']"
3994531,What kind of categorical construct is the tensor product of Hilbert spaces?,"Let $H_1$ and $H_2$ be Hilbert spaces. We can form the algebraic tensor product $H_1 \odot H_2$ and complete it with respect to the inner product $$\langle \xi_1 \otimes \xi_2, \eta _1 \otimes  \eta_2 \rangle:= \langle \xi_1, \eta_1\rangle \langle \xi_2, \eta_2\rangle$$ In this way, we obtain the Hilbert space $H_1 \otimes H_2$ . Consider the category HILB of Hilbert spaces, say with contractive linear maps as morphisms. Can we view $H_1 \otimes H_2$ as some kind of limit in this category? Does it satisfy some useful universal property? Really, any answer can be as broad as you like. I am aware that $\otimes$ defines a bifunctor that gives us a monoidal structure on this category.","['hilbert-spaces', 'functional-analysis', 'category-theory']"
3994563,Symplectic Hodge Star and Koszul differential,"Let $M = \text{Spec}(R)$ be a symplectic affine variety of dimension $n$ with the symplectic form $\omega$ . There is the symplectic Hodge star $\star: \Omega^k_{M} \to \Omega^{2n-k}_{M}$ given by the condition that for any $\alpha, \beta \in \Omega^k$ we should have $$\beta \wedge \alpha = \Lambda^k G(\beta, \alpha) \cdot v_n$$ where $$G(-, -): TM^* \times TM^* \to R$$ is a bivector field corresponding to $\omega$ and $v_n = \omega^n/n!$ . I want to understand the first nontrivial (for me) case: $M = T^* \mathbb{A}^2$ . In this case $\omega = dx_1 dx_2 + dx_3 dx_4$ and $v_2 = dx_1 dx_2 dx_3 dx_4$ .
Also, $$G = \frac{\partial}{\partial_{x_1}} \frac{\partial}{\partial_{x_2}} + \frac{\partial}{\partial_{x_3}} \frac{\partial}{\partial_{x_4}}.$$ It seems to me that we have the following $\star(f) = f dx_1 dx_2 dx_3 dx_4$ $\star(\alpha) = \alpha \wedge \omega$ where $\alpha \in \Omega^1_{M}$ $\star(\zeta) = -\zeta$ for $\zeta \in \Omega^{2}_{M}$ $\star(\eta) = i_{G} \eta$ where $\eta \in \Omega^{3}_{M}$ and $i_{G}$ is the contraction with $G$ . OK, then let us see how the Koszul differential works and it is the place where I met a contradiction in math (of course, the problem is with me but I cannot figure it out). $\textbf{Setup.}$ The Koszul differential $B: \Omega^{k}_{M} \to \Omega^{k-1}_{M}$ has two interpretations, the first one is $B = [i_G, d]$ . The second is $B = (-1)^{k+1} \star d \star.$ $\textbf{My misunderstanding.}$ Let $x \in \Omega^3_{M}$ . The first definition gives $Bx = i_G dx - d i_{G}x$ . The second definition gives $Bx = \star d \star x = \star d i_{G} x = -d i_{G} x$ . As we see, we should have $i_{G} dx=0$ which is not the case, for example, for $x_1 dx_2 dx_3 dx_4.$ $\textbf{My quesion.}$ Where is the mistake? Or, can we write down the star isomorphism in the general case? I think it is related to the multiplication by $\omega^{n-k}$ , but mostly I'm interested how to write down the formula for $\star: \Omega^{k}_{M} \to \Omega^{2n-k}_{M}$ for $k>n$ . Is there any reference?","['algebraic-geometry', 'symplectic-geometry']"
3994564,Show that $Z_t = \sqrt{\sqrt{2t}e^{-\sqrt{2t}}} B_{e^{\sqrt{2t}}-1}$ solves $dZ_t = f(t)Z_tdt +dM_t$ and find $f(t)$.,"Let $B=(B_t)_{t\geq0}$ be a standard Brownian motion started at $0$ and define $M=(M_t)_{t\geq0}$ by $$M_t = \int_0^{e^{\sqrt{2t}}-1} \sqrt{\frac{\log{(1+s)}}{1+s}} dB_s$$ so that M is also a standard Brownian motion. Show that the process $Z=(Z_t)_{t\geq0}$ defined by $$Z_t = \sqrt{\sqrt{2t}e^{-\sqrt{2t}}} B_{e^{\sqrt{2t}}-1}$$ solves the following stochastic differential equation $$dZ_t = f(t)Z_tdt +dM_t$$ with $Z_0 = 0$ and determine the function $f$ explicitly. I'm not really sure how to go about this, any help would be appreciated, thanks.","['finance', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3994588,"Fallacy in solution attempt to Hartshorne exercise chapter I, 2.11","I am somewhat of a beginner in algebraic geometry and have been working on the following problem from Hartshorne: Let $Y \subset \mathbb P^n$ be a projective linear variety, that is its ideal $I(Y)$ can be generated by homogeneous polynomials of degree $1$ . Then for any $r \in [n]$ , $\dim Y = r$ if and only if $I(Y)$ can be minimally generated by $n-r$ homogeneous linear polynomials. This is how I started: since $Y$ is a projective variety, $I(Y)$ is a prime ideal in the integral domain $S:= k[X_0, \cdots , X_n]$ whence by proposition 1.8A(b) (section I.1), we have $$\text{ht} I(Y) = \dim S - \dim S(Y) = n+1-r$$ and Krull's Principal Ideal Theorem (in conjunction with the Noetherianity of $S$ via the Hilbert Basis Theorem) shows that the minimal number of generators $\mu(I(Y))$ of the ideal $I(Y)$ satisfies the inequality $$\mu (I(Y)) \geq \text{ht} I(Y) = n-r+1.$$ On the other hand, what I have to show is that $\mu(I(Y))=n-r$ . I do not know where the fallacy in the above argument is and would really appreciate some help regarding the same. It would also be nice to know if this argument shows promise. Thank you.","['algebraic-geometry', 'polynomials', 'projective-varieties']"
3994632,"Given a traffic matrix $A$ what does it mean for the pair-wise dot products of rows to be non-zero, and can we enforce this somehow?","The traffic matrix $A$ encompasses the information regarding the paths of origin-destination nodes in a directed graph. Columns represent od-pairs and rows the directed edges of the underlying graph. The element $(i, j)$ in the matrix is one if the edge $i$ belongs to the (directed) path in the od-pair $j$ , and otherwise it is zero. Paths for the od-pairs chosen are arbitrary. That is, we can and we will leave out some of the paths. For example in the directed $K_3$ the path between nodes $1$ and $2$ could be $(1, 2)$ or $(1, 3), (3, 2)$ . What I would like to know is that what do we know about such matrix $A$ that satisfy the following property: the pair-wise dot products of the rows are non-zero. In other words, each pair of rows has at least one common column full of ones. So, how could we talk about this in the language of graph theory/linear algebra/lattices and, does this question result in a decision problem. Specifically, if we want to enforce the aforementioned property, do we have an algorithm to reformulate the matrix $A$ ?","['graph-theory', 'matrices', 'linear-algebra', 'combinatorics', 'discrete-mathematics']"
3994657,On the integral $\int_{0}^{1} \frac{\mathrm{d}x}{\sqrt{3x^4+2x^2+3}}$,"I came across this integral $$\mathcal{J} = \int_{0}^{1} \frac{\mathrm{d}x}{\sqrt{3x^4+2x^2+3}}$$ According to W|A it equals $\frac{1}{2}$ . However, I cannot find a way to crack it. It smells like a Beta integral , but I do not see any obvious subs. One could start by setting $u=x^2$ but there is no clear path after that. A promising way might be the following \begin{align*}
\int_{0}^{1} \frac{\mathrm{d}x}{\sqrt{3x^4+2x^2+3}} &= \int_{0}^{1} \frac{\mathrm{d}x}{\sqrt{\left ( 3x^2 +2 \right ) x^2 +3}}\\ 
 &=\int_{0}^{1}\frac{\mathrm{d}x}{\sqrt{3\left ( x^2+ \frac{1}{3}   \right )^2 + \frac{8}{3}}} \\ 
 &= \cdots
\end{align*} A clever trigonometric sub might clear things but I don't see something. On the other hand , I don't the theory of elliptic integrals is needed here nor complex analysis ( would be interesting to see a solution using contours, though ) So, any ideas how to evaluate it? P.S : Are there techniques available for these type of problems?","['integration', 'definite-integrals', 'real-analysis']"
3994662,"$(X,d)$ is metric space. $(X,d)$ is compact if and only if any continuous function on $X$ has a maximum.","$(X,d)$ is metric space. $(X,d)$ is compact if and only if any continuous function on $X$ has a maximum. I dont know whether these functions real valued or not but only real valued functions may make sense, I think. In that case $\Rightarrow$ is easy. About $\Leftarrow $ My 1st attempt: I thought some special functions that I can use e.g. $$\varphi:X\to \mathbb R\\ \varphi(x)=d(x_0,x)$$ for some fix $x_0\in X$ Since this is a continuous function then it attains its maximum on $X$ , that implies $X$ is bounded . With this motivation I can define $$\psi:X\to \mathbb R \\\psi(x)=d(x,X)=\inf\limits_{a\in X}d(x,a)$$ since it attains its maximum so $X=\overline X$ So I have boundness and closedness but these dont really imply compactness in metric spaces. My 2nd attempt was: Considering any sequences in $X$ and using $f(x_{k_n})$ is convergent iff $x_{k_n}$ is convergent. By showing every sequence has convergent subsequence and using continuity of any $f:X\to\mathbb R$ . I am stuck is there hint or answer you can give me?",['real-analysis']
3994664,"Prove that the map $f:[0,1]\to l_{\infty}([0,1];\Bbb{R})$ defined by $t\mapsto 1_{[t,1]}$ is Riemann integrable.","A map $f:[0,1]\to F$ (where $F$ is a banach space) is said to be Riemann
integrable if $\exists I\in F$ such that for every $\epsilon>0$ there
is $\delta>0$ satisfying $$\left\lVert I-\sum\limits_{i=1}^n
 f(\xi_i)(t_i-t_{i-1})\right\rVert_F<\epsilon$$ for all partions $P=\{0=t_0<t_1<\cdots<t_n=1\}$ of $[0,1]$ with $\lVert
 P\rVert:=\underset{i}{\text{max}}(t_i-t_{i-1})<\delta$ and for every
choice of $\xi_i\in(t_{i-1},t_i)$ Here $l_{\infty}([0,1];\Bbb{R}):=\left\{f:[0,1]\to\Bbb{R}|\ \lVert f\rVert_\infty=\underset{t}{\text{sup}}| f(t)|<\infty\right\}$ is banach space with the sup norm $\lVert\cdot\rVert_\infty$ . The map $f:[0,1]\to l_{\infty}([0,1];\Bbb{R})$ is defined as $\displaystyle{f(t)=1_{[t,1]}}\ \forall t\in[0,1]$ . Recall that- $$
1_{[t,1]}(x)=\begin{cases}
1&\text{if }x\in [t,1]\\
0&\text{if }x\in [0,t) 
\end{cases}$$ I have to find $I\in l_{\infty}([0,1];\Bbb{R})$ such that the expression above in the definition of riemann integrebility is satisfied. I don't have any idea how to think about this $I$ . Initially, I have tried with $I=1_{[0,1]}$ (which is basically the constant function $1$ ), but it will not work because for any partion $P=\{0=t_0<t_1<\cdots<t_n=1\}$ we have $\left| 1_{[0,1]}(0)-\sum\limits_{i=1}^n
 f(\xi_i)(0)(t_i-t_{i-1})\right|=\left| 1-1_{[\xi_1,1]}(0)t_1\right|=1$ (as $\xi_1>0$ ). This implies $\left\lVert 1_{[0,1]}-\sum\limits_{i=1}^n
 f(\xi_i)(t_i-t_{i-1})\right\rVert_\infty\ge 1$ . So, $I=1_{[0,1]}$ fails. Then I tried with constant function $0$ , but it fails in the similar fashion (by evaluating at $1$ ). Can anyone help me in this regard? Thanks for help in advance.","['banach-spaces', 'normed-spaces', 'riemann-integration', 'analysis']"
3994666,Parametric equation of an ellipse in the 3D space,"I have found here that an ellipse in the 3D space can be expressed parametrically by $$\mathbf x (t)=\mathbf c+(\cos t)\mathbf u+(\sin t)\mathbf v$$ with $\mathbf c = (c_1,c_2,c_3)$ being the center of the ellipse and the lenghts of the half-axis being the lengths of the vectors $\mathbf u = (u_1,u_2,u_3)$ and $\mathbf v = (v_1,v_2,v_3)$ . How could these three vectors $\mathbf c$ , $\mathbf u$ and $\mathbf v$ be related to the directions of the axis of the ellipse? Is there maybe a parametric equation for the ellipse in an arbitrary plane of the space whose elements have a more intuitive meaning?","['parametric', 'conic-sections', 'geometry', '3d']"
3994693,Prove the collection of isolated points of a set in $\mathbb{R}^n$ is countable.,"Let $S \subset \mathbb{R}^n$ be a bounded set and let $B$ be the set of isolated points of $S$ ( $x \in S$ is isolated if there exists an open neighbourhood $U$ of $x$ such that $U \cap S = \{x\}$ . I claim $B$ is countable. First I will assume there exists a countable basis $\beta$ for the topology on $\mathbb{R}^n$ .  Then for each $x \in B$ , there exists an open set, call it $U_{x}$ , in $\beta$ such that $U_{x} \cap S = \{x\}$ .  Then we can see that $B$ is in bijective correspondence with the collection of sets $\{U_{x} \}_{x \in B } $ .  Since $\{U_{x} \}_{x \in B }$ is a subset of the countable set $\beta$ , it is countable and hence $B$ is countable. Questions about my proof:  Is my proof correct?
Also, did I use the axiom of choice in choosing the $U_{x}$ ?  If so, is there a proof without it?","['general-topology', 'solution-verification']"
3994695,Is there a rigorous definition for matrix derivatives?,"I know that, A function $f: \mathbb{R}^n \to \mathbb{R}$ is said to be differentiable at $x$ if there exists a vector $v$ such that, $$
    \lim_{h \to 0} \frac{f(x+h) - f(x) - v^Th}
    {\|h\|} = 0.
$$ When $v$ exists, it is given by the ""gradient"" $\nabla f(x) = \left(\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{\partial x_n}\right)(x)$ Does there exist a similar definition for ""matrix derivative"" https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_matrices","['multivariable-calculus', 'matrix-calculus', 'definition', 'optimization', 'convex-analysis']"
3994714,UMVUE for $e^{-\lambda}$,"Let $X_1\dots,X_n$ be $Poisson(\lambda)$ . Show that $$T= \left( \frac{n-1}{n} \right)^{\sum_{i=1}^{n}X_i}$$ is an UMVUE for $e^{-\lambda}$ I know that $\sum_{i=1}^{n}X_i$ it is a sufficient and complete statistic since the distribution belongs to the exponential family. But I don't know how to go on to show that $T$ is an UMVUE. Any hint?","['statistics', 'means']"
3994724,Failing a basic integration exercise; where did I go wrong?,"(This is a basic calculus exercise gone wrong where I need some feedback to get forward.) I've attempted to calculate an integral by first integrating it by parts and then by substituting. The result I got is not correct though. Can I get a hint about where started to make mistakes? Here are my steps: $$\int \sqrt{x}\cdot\sin\sqrt{x}\,dx$$ Applying integration by parts formula $$\int uv' \, dx = uv - \int u'v \, dx$$ where $u=\sqrt{x}$ , $v'=\sin\sqrt{x}$ , $u'=\frac{1}{2\sqrt{x}}$ and $v=-\cos\sqrt{x}$ , resulting in: $$\int \sqrt{x}\cdot\sin\sqrt{x}\,dx = \sqrt{x} \cdot -\cos\sqrt{x} - \int \frac{1}{2\sqrt{x}} \cdot -\cos\sqrt{x} \, dx$$ Next, I tried substituting $x$ with $g(t)=t^2$ in the remaining integral, i. e. replacing each $x$ in the integral with $t^2$ and the ending $dx$ with $g'(t)=2t\,dt$ . I would later bring back the $x$ by substituting $t=\sqrt{x}$ after integration. Continuing from where we left by substituting: $$\sqrt{x} \cdot -\cos\sqrt{x} - \int \frac{1}{2\sqrt{x}} \cdot -\cos\sqrt{x} \, dx$$ $$\Longrightarrow \sqrt{x} \cdot -\cos\sqrt{x} - \int \frac{1}{2\sqrt{t^2}} \cdot -\cos\sqrt{t^2}\cdot2t \, dt$$ Then I pulled out the constant multipliers from the integral: $$\sqrt{x} \cdot -\cos\sqrt{x} - \frac{1}{2} \cdot -1 \cdot 2 \int \frac{1}{\sqrt{t^2}} \cdot \cos\sqrt{t^2}\cdot t \, dt$$ which turned out to eliminate each other (resulting in just $\cdot1$ ), so we end up with: $$\sqrt{x} \cdot -\cos\sqrt{x} \cdot \int \frac{1}{\sqrt{t^2}} \cdot \cos\sqrt{t^2}\cdot t \, dt$$ Reducing the integrand by reducing $\frac{1}{\sqrt{t^2}}\Rightarrow\frac{1}{t}$ , which again is eliminated by multiplying with the integrand's $t$ , and reducing $\cos\sqrt{t^2}\Rightarrow\cos t$ . Therefore resulting in: $$\sqrt{x} \cdot -\cos\sqrt{x} \cdot \int \cos t \, dt$$ where the integral can be solved as $\int \cos t \, dt = \sin t + C$ . So now we are at: $$\sqrt{x} \cdot -\cos\sqrt{x} \cdot \sin t + C$$ The correct answer however is $$\int \sqrt{x} \sin\sqrt{x} \, dx = 4 \sqrt{x} \sin\sqrt{x} - 2 (x - 2) \cos\sqrt{x} + C$$ So something somewhere in my process went horribly wrong. What?",['integration']
3994760,Proof of the theorem that there is no counterexample,"If $\rho(A) $ is a spectral radius and $ \|\cdot\|$ is norm, the relation $\rho(A) \leqslant \|A\| $ is always established.
I want to prove that if $\rho(A) <1$ then $ \|A\|<1$ . Note that using the computer, I checked a large number of random matrices and there were no Counterexample. This is probably a mathematical theorem, but I can not prove it. please help. Thankful $$ $$ A friend found a counterexample for it so I change the condition to $0<\rho(A) <1$ . Can the theorem be proved now?","['matrices', 'examples-counterexamples']"
3994781,Divergence free vector fields in general measure space,"I'm reading Ambrosio's Gradient Flows in Metric Spaces and the Space of Probability Measures textbook, and I'm stuck on a part of Lemma 8.4.2. Rather than give the whole Lemma, I'll state the specific claim I'm confused by, and I'll simplify it by taking $p = 2$ ( $p = 2$ is the only case I'm interested in), $X = \mathbb{R}^n$ (rather than some other separable Hilbert space $X$ ). Here $\mu$ is a finite Borel measure on $X$ , and $v,w$ are $L^2(\mu)$ vector fields (the integral of $|v|^2$ w.r.t. $\mu$ is finite). The statement is as follows: \begin{equation*}
\int_X v \cdot w \, d \mu(x) = 0 \, , \text{ for any $w \in L^2(\mu)$ s.t. $\nabla \cdot (w \mu) = 0$}
\end{equation*} is equivalent to \begin{equation*}
v \text{ belongs to the $L^2(\mu)$ closure of } \{\nabla \phi \, : \, \phi \in C_c^\infty(X)\} \, . 
\end{equation*} Some clarifications: $\nabla \cdot (w \mu) = 0$ means that, for any test function $\phi \in C_b^1(X)$ , $$\int_X \nabla \phi \cdot w \, d \mu = 0 \, . $$ The second statement means that there is some sequence $\{\phi_n\} \subseteq C_c^\infty(X)$ so that $$\lim_{n \to \infty} \int_X |\nabla \phi_n - v|^2 \, d \mu = 0 \, . $$ Why is the stated equivalence true??? (The textbook states it without justification.) I think the backward implication is clear: if we can write $v = \nabla \phi$ for some $\phi$ , then $\nabla \cdot (w\mu) = 0$ means precisely that $\int_X v \cdot w \, d \mu = 0$ . However, the forward implication is unclear to me. If $\mu$ were just the Lebesgue measure/some absolutely continuous measure w.r.t. Lebesgue then I would try integration by parts (looks like some Sobolev thing going on), but I don't believe this is available to us here? I can't find any integration by parts formulas for arbitrary measures in the textbook.","['gradient-flows', 'measure-theory', 'optimal-transport', 'analysis']"
3994784,Does $\sum_{k=0}^\infty \frac{16(2k+1)^2}{(4m^2 - (2k+1)^2)^2}=\pi^2$?,"I think the following series has the value $$\sum_{k=0}^\infty \frac{16(2k+1)^2}{(4m^2 - (2k+1)^2)^2}=\pi^2$$ for any positive integer $m$ , but I am not sure how to go about computing it. Plugging the above sum into a computer for a few different values of $m$ seems to verify the result. I don't have much experience computing any infinite sums of the above form, so I am not sure where to begin. The context for computing this series is to show that $\cos(2mx)$ can be written as an infinite sum of the functions $\sin(kx)$ . On a related note, I would also like to show that $$\sum_{k=0}^\infty \frac{64m^2}{(4m^2-(2k+1)^2)^2} = \pi^2$$ where again, $m$ is a positive integer; this sum appears in showing the opposite relation, that $\sin(2mx)$ can be written as a sum of $\cos(kx)$ . Again, I have verified on a computer that this is true for at least the first few values of $m$ . This second sum would be reasonable to compute if it were an integral, but I'm not sure of a way I can immediately apply that idea (if we consider $f(x) = c^4/(c^2-x^2)^2$ then we could compute the integral of this function from $0$ to $\infty$ by substitution $x=c\tanh\theta$ fairly easily). I'm aware that there are easier ways to show these relations between sines and cosines, but I'm invested enough in this approach that I would like to find a way to compute these sums. Any tips would be appreciated. Edit: I am actually not 100% sure that the first sum converges to $\pi^2$ for all $m$ ; in Wolfram Alpha, the sum comes out to be $\pi^2$ for $m=1,2$ but for any $m\geq 3$ it simply approximates the sum, and the approximated value is clearly less than $\pi^2$ , so I may have made an error. The value of the second sum definitely seems to be $\pi^2$ though.",['sequences-and-series']
3994814,Find the vector given the dot product and cross product of a set of vectors.,"This question comes from Ted Shifrin's Multivariable Mathematics . The question states: Given the nonzero vector $\textbf{a} \in \mathbb{R}^{3}$ , $\textbf{a} \cdot \textbf{x} = b \in \mathbb{R}$ , and $\textbf{a} \times \textbf{x} = \textbf{c} \in \mathbb{R}^{3}$ , can you determine the vector $\textbf{x} \in \mathbb{R}^{3}$ ? If so give a geometric construction of $\textbf{x}$ . I'm trying to visualize what the vector $\textbf{x}$ 's would look like in this scenario, but I think I'm getting confused by everything happening. So $\textbf{a} \cdot \textbf{x} = b$ is giving me the equation of a plane, in particular an affine plane from the one at the origin. As well $\textbf{a} \times \textbf{x} = \textbf{c}$ is giving me vector orthogonal to $\textbf{a}$ and $\textbf{x}$ respectively. The norm of this cross product also gives me the area of the parallelogram spanned by $\textbf{a}$ and $\textbf{x}$ . Even with all these properties I'm still having trouble figuring out how to solve for $\textbf{x}$ . Would I be able to get some assistance? If I'm lucky maybe the man himself actually might pop in to provide guidance.","['determinant', 'inner-products', 'real-analysis', 'multivariable-calculus', 'linear-algebra']"
3994826,Every infinite set has a countably infinite subset.,"Prove every infinite set has a countably infinite subset. I was wondering if this approach is completely correct for this problem, my approach was to recursively define a function $f: \mathbb{N} \rightarrow$ countably infinite subset of an infinite set. Attempt:
Suppose $A$ is infinite. We will construct a function $f: \mathbb{N} \rightarrow B$ , where $B \subset A$ , and $B$ is countably infinite. Since $A \neq \emptyset$ , we can choose an element $a_{1} \in A$ . Set $f(1)=a_1$ . Since $A$ is infinite, choose an element $a_2 \in A-\{a_1\}$ .Set $f(2)=a_2.$ Assume for each $m<n$ , $f(m)$ has been chosen. Since $A$ is infinite there exists in element $a_{n} \in A-\{a_1,...a_{n-1}\}$ . Set $f(n)=a_n$ . Now that $f$ has been defined inductively, $B=\{a_1,a_2,...\} \subset A$ is countably infinite.",['elementary-set-theory']
3994838,"For $x_{n+1}=x_n-x_n^3$, with $|x_1|>1$. What about the convergence?","I see Let $\{ x_n \}_{n=1}^{\infty}$ such that $x_{n+1}=x_n-x_n^3$ and $0<x_1<1$ that if $|x_1| \le 1$ , then $x_n\to 0$ . However, if $|x_1|>1$ , what can we say about this sequence? It seems hard to find the tendency of it. For $x_1$ near $1$ , it converges to $0$ , but for $x_1$ large, it is not the case.","['limits', 'calculus']"
3994842,Finding the intersection of three sets,"80 students were asked if they like math, science or humanities. 24
students did not like either of the subjects, 9 liked math only , 16
liked science only , 9 liked humanities only , 12 liked math and
humanities, 7 liked math and science and 9 liked humanities and
science. a) How many students like all three subjects? b) How many students like math or science? c) How many students don't like humanities? Here's a venn diagram displaying the given information: a) finding the intersection of sets M, S and H |M∩S∩H|=|M∪S∪H|−(|M|+|S|+|H|)+|M∩S|+|M∩H|+|S∩H| -2 |M∩S∩H|= (80 - 24) - (9 + 16 + 9) - (12 + 7 + 9) -2 |M∩S∩H|= 56 - 34 - 28 -2 |M∩S∩H|= 22 - 28 -2 |M∩S∩H|= -6 |M∩S∩H|= 3 I can't do b) or c) because when I say the intersection is 3, then all the other numbers in the venn diagram change (obviously). For example, if the intersection is 3, then the number of people who like math and science = 4 (7 - 3 ) and the number of people who like math and humanities = 9 (12 - 3 ). But when I add up the newfound numbers (3 + 9 + 4), I get 16 and I can't do 9 - 16 (which is -5, A NEGATIVE NUMBER!!!) Could someone please let me know what I have done wrong and how the heck I'm supposed to figure out the intersection of three sets?! Any help would be greatly appreciated.","['elementary-set-theory', 'discrete-mathematics']"
3994867,How many possible ways are there to print 125 pages on 5 printers?,"I think I understand this problem but I want to make sure I do get it thoroughly. In my mind, this problem is asking you to assign a single page to a single printer, but it doesn't really matter the order in which they are printed. I.E., we don't care if page 1 is sent to printer 1 first, or if printer one gets page 2 first and then page 1 is sent. In my mind, then, we would simply take a page and say, how many choices do I have to print it? That would be 5 for the 1st page. Now page 2 also has 5 choices, and so on and so on, so that means the solution would be 5 x 5 x ... x 5 = 5 ^ 125 . Is this the correct answer or am I implicitly slipping in order some how and need to remove the number of permutations from this number? Thanks for your help in understanding this problem!","['permutations', 'combinatorics']"
3994880,Determining which function grows the most over time,"I have five logarithmic functions, 1 for each country I'm investigating. x = an integer value for years, y = share of 1-year-olds vaccinated against rotavirus. So if x = 2, y would tell you how many 1-year-olds have been vaccinated against rotavirus (%) in the country's second year of progress. Time Interval of x (Years) a b y = a + bln(x) Morocco 1-9 32.4 33.8 y = 32.4 + 33.8ln(x) Sudan 2-9 22.3 34.7 y = 22.3 + 34.7ln(x) Rwanda 3-9 41.8 27.9 y = 41.8 + 27.9ln(x) Ghana 3-9 36.6 28.3 y = 36.6 + 28.3ln(x) Botswana 3-9 75.4 6.18 y = 75.4 + 6.18ln(x) This is for a school project, and I'm interested in determining which function grows the most over the course of its time interval -> so which country makes the most progress in vaccinations for the years of progress recorded. I'm totally new to calculus but I want to include a calculus concept to determine this. I was looking into derivatives for calculating rates of change but I'm not sure if that's the simplest and most effective approach for what I want to do - would appreciate a suggested calculus approach potentially other than this that isn't too difficult to learn as a beginner!","['data-analysis', 'calculus', 'functions']"
3994981,Confirmation of proof that homeomorphism preserves dimension,"It seems that I am being able to show that the dimension of a homeomorphic image of a topological space is equal to the dimension of the original space itself. In what follows, I define the dimension of a topological space $X$ in accordance with Hartshorne: $$\dim X:= \sup \{n \in \mathbb N : \exists Z_0 \subsetneq \cdots \subsetneq Z_n \text{ s.t. } Z_j \subset X \text{ is closed and irreducible for all }j \in \{0, \cdots , n\}\}$$ Let $X$ and $Y$ be topological spaces and $f$ a homeomorphism from $X$ to $Y$ and let $r:= \dim X$ . Then there exists a chain $$V_0 \subsetneq \cdots \subsetneq V_n$$ of closed irreducible subsets of $X$ . $f$ being a closed map, each of $f(V_0), \cdots , f(V_n)$ must also be closed and clearly $$f(V_0) \subset \cdots \subset f(V_n).$$ Moreover, for each $j \in \{1, \cdots , n\}$ , considering $z_j \in V_j \setminus V_{j-1}$ gives us from the injectivity of $f$ , $f(z_j) \in f(V_j) \setminus f(V_{j-1})$ , so that all the containments mentioned in the above chain are proper. Finally, we note that each $f(V_j)$ is irreducible simply because the image of an irreducible set under a continuous map is also irreducible: indeed for any irreducible closed set $Z$ of $X$ , if $f(Z) = Y_1 \cup Y_2$ for two nonempty proper closed subsets $Y_j$ of $Y$ , then $$Z =  f^{-1}(f(Z)) = f^{-1}(Y_1 \cup Y_2) = f^{-1}(Y_1) \cup f^{-1}(Y_2),$$ with each $f^{-1}(Y_j)$ closed (in $X$ ), non-empty and proper by the continuity, surjectivity and injectivity of $f$ respectively, thereby contradicting the irreducibility of $Z$ . Thus, $\dim Y \geq r = \dim  X$ and the opposite inequality follows by switching the roles of $X$ and $Y$ , since the inverse of a homeomorphism is also a homeomorphism. It seems that homeomorphisms are preserving the (topological) dimension of a topological space, contrary to the counterexample provided here: Do homeomorphic sets have to be in the same dimension? I would be really obliged if someone could please find the fallacy in the above argument. Edit: I changed the title after Moishe Kohan's clarification.",['general-topology']
3994983,Are these 2 graphs isomorphic (with illustrations and counter examples that appear not to preserve adjacencies)?,"The following labeled slide was presented in a first year discrete mathematics course. However,  the adjacencies don't appear to be preserved (see the second illustration below). If the vertices in the above graph on the left are arbitrarily labeled A to F, and we select the arbitrary vertex A for the vertex labeled A in the graph on the right, there appears to be no way to label the adjacent vertices such that it will not create an edge that connects two vertices (the other two A-adjacent vertices) that are not adjacent in the original graph. Of the 3 nodes that that are connected to vertex A, whichever is chosen to be the endpoint at the bottom right of the supposedly isomorphic graph, will lead to a situation where the other two nodes adjacent to A, will also be adjacent to each other, despite this not being the case in the original graph. If these are really isomorphic, how can the second graph given in the slide preserve all the adjacencies of the original graph? Obviously, the degrees of the vertices and the sum of degrees of the graphs are the same, and the second graph is a bijection of the first, but what about the adjacencies?","['graph-theory', 'graph-isomorphism', 'discrete-mathematics']"
3994985,Why are we treating du/dx as a fraction? [duplicate],This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 3 years ago . When I was learning implicit differentiation in my class I was told to not think of $dy/dx$ as a fraction. Now I am doing integration by $u$ -substitution and we treat $du/dx$ as a fraction and solve for $dx$ . Why is this?,"['integration', 'differential-operators', 'fractions', 'calculus', 'derivatives']"
3994992,"finding the volume of a cube, why doesn't my solution work","Question : the distance from a corner to the centre of a cube is 6. what is the volume of the cube? Answer: 332.55 I drew a figure of a cube and two lines from a corner to their opposite corner and labeled the middle of the cube e . the triangle aec is right angled because the 2 lines make 4 equal angles when they intersect at e . knowing this and that the distance from a corner to e is 6, I calculated ac to be 8.49 or √72 using the Pythagorean theorem. Since I know ac is 8.49, I calculated x to be 6. 6³ is 216, so the volume of the cube is 216. Please tell me where I went wrong, Thank You.","['geometry', 'volume']"
3995010,How to find $\left(\sqrt{\theta^2+1}\right)\sin \beta$ when a rotating wheel goes up in an incline?,"The problem is as follows: A wheel of radius $r$ given in inches starts moving over an incline from point $O$ which is a tangential point. The wheel moves up in the incline due an exterior force. Assuming that the wheel stopped at point $T$ as indicated in the figure and the angle swept by the wheel is $\theta$ . Find: $$\left(\sqrt{\theta^2+1}\right)\sin \beta$$ The alternatives given in my book are as follows: $\begin{array}{ll}
1.&\cos\alpha-\theta\sin \alpha\\
2.&\sin \alpha+\theta\cos \alpha\\
3.&\cos\alpha+\theta\sin \alpha\\
4.&\sin\alpha - \theta \cos\alpha\\
\end{array}$ What's exactly the right way to approach this problem?
The part which confuses me the most is how to use the information of the angle $\beta$ from the drawing? The reason for that is angle $beta$ is in the dotted line but not in the slope of the incline. Thus this is confusing. It seems that the intended strategy is to use the radius of the wheel as an auxiliary variable to get the whole thing in terms of $\alpha$ . But how to do this methodically?. Can someone help me here?. What's exactly the right way to do this without much fuss?.","['algebra-precalculus', 'trigonometry']"
3995017,What is the meaning of Matlab's ODE solver naming conventions?,"MatLab has a number of built-in ODE solvers and are named using the convention odepq, for exampleL ode45, ode23, etc. I understand that $p$ here stands for the order of the numerical method. For example, when using Euler's method, this is an order 1 because you only need to substitute the value of the function once before going on the update equation. As opposed to 2 times for Heun's and midpoint method (hence order 2). Classical RK method is order 4. What I don't understand is the $q$ part. What is it?","['numerical-methods', 'matlab', 'ordinary-differential-equations']"
3995034,Integrability of Fourier Transform of Lipshitz functions,"If a function $f$ is Lebesgue integrable and satisfies Lipschitz condition, must its Fourier transform $\hat{f} $ be Lebesgue integrable or not? There's a theorem about Fourier series, saying the Fourier series of periodic Lipschitz function must be absolutely convergent, and I'm just thinking about if it has a similar analog in Fourier transform. I have proven that the improper integral $$\displaystyle{\int _{- \infty }^{\infty }\hat {f} (\xi )e^{2 \pi i \xi x} d \xi }$$ is uniformly convergent for $x\in \mathbb{R}$ . My thought is to find a sequence of Lipschitz integrable functions, with $L^1$ -norm and Lipschitz norm both $\le 1$ , whose Fourier transform have an increasing $L^1$ -norm, then by the closed graph theorem, there must exist an integrable Lipschitz function whose Fourier transform is not integrable. But I find that kind of construction extremely difficult to be accomplished. Anyone has ideas?","['fourier-analysis', 'fourier-transform', 'real-analysis', 'complex-analysis', 'functional-analysis']"
3995146,Affine connections v.s. Lie derivative,"It is often advertised that affine connections are a way to differentiate a vector field along another (see e.g. here ). For the little I know, Lie derivatives are advertised in the same way. On a more or less intuitive level, what distinguishes the two notions? Below is a possible answer that I couldn't formally verify. Let $M$ be a smooth manifold and $X$ , $Y$ be two smooth vector fields on $M$ . Naively, the ""derivative of $Y$ along $X$ at a point $p$ "" is $$
\lim_{\epsilon \rightarrow 0} \frac{Y_{p + \epsilon X_p} - Y_p}{\epsilon} .
$$ But of course, $p + \epsilon X_p$ doesn't make sense. Furthermore, $Y_p$ and $Y_{p'}$ don't live in the same tangent space if $p \neq p'$ . The idea with Lie derivative is to consider the flow $\theta$ of $X$ as a mediator: $$
(L_X Y)_p = \lim_{\epsilon \rightarrow 0} \frac{(d \theta_{-\epsilon}) Y_{\theta_\epsilon (p)} - Y_p}{\epsilon} .
$$ Based on this, maybe $(\nabla_X Y)_p$ does the same thing, but instead of mediating with the derivative of the flow of $X$ , it uses parallel transport along a (arbitrary) curve passing through $p$ with speed $X_p$ ? Could you clarify the situation for me? Thanks in advance.",['differential-geometry']
3995235,"$P(E_i) = 0, \forall i\in I \Leftrightarrow P(\bigcup_{i\in I} E_i) =0$ and $P(E_i) = 1, \forall i\in I \Leftrightarrow P(\bigcap_{i\in I} E_i) =1$","Prove that $P(E_i) = 0, \forall i\in I \Leftrightarrow P(\bigcup_{i\in I} E_i) =0$ and $P(E_i) = 1, \forall i\in I \Leftrightarrow P(\bigcap_{i\in I} E_i) =1$ , where $I$ is a countable index set. $P$ is a probability measure, and we have generalized Boole's inequality, to begin with: $$0\le P\left(\bigcup_{i=1}^\infty E_i \right) \le \sum_{i=1}^\infty P(E_i)$$ Is the following proof okay? $P(E_i) = 0$ for all $i\in I$ then $P(\cup_{i\in I}E_i)=0$ by Boole's inequality above. If $P(\cup_{i\in I}E_i)=0$ holds, then Boole's inequality is probably of no use. I tried a proof by contradiction. Let's suppose there exists $j\in I$ such that $P(E_j) \neq 0$ . Then $P(\cup_{i\in I}E_i)=0$ is definitely absurd - but how do I put this in mathematical terms? $(\star \star \star)$ In the other case, we have $P(E_i^c) = 0$ for all $i\in I$ , which tells us (with the help of the first part) that $$P\left(\bigcup_{i\in I}E_i^c\right)=0 \implies P\left(\bigcap_{i\in I}E_i\right)^c=0 \implies P\left(\bigcap_{i\in I}E_i\right) = 1$$ and the proof is complete. If everything above sounds fine, I only need help with the line marked $(\star \star \star)$ . Thanks!","['probability-theory', 'probability']"
3995283,I don't understand $\ln$ properties when it comes to absolute value.,"Let's say we have a function with absolute value like: $f(x) = \ln\vert x\vert$ where $x$ is any real number except 0 Now, when we get rid of the absolute value, we get this: $f(x) = \ln(x)$ where $x$ is positive $f(x) = \ln(-x)$ where $x$ is negative But here is the thing, $\ln$ properties allow us to do something like $\ln(xy) = \ln(x) + \ln(y)$ So we apply this to the second form of function $f(x) = \ln(-x)$ means $f(x) = \ln(-1) + \ln(x)$ But $\ln$ is undefined for negative real numbers so I don't get it, how does this work?","['real-numbers', 'absolute-value', 'logarithms', 'calculus', 'algebra-precalculus']"
3995313,"Integration in a non-standard way: evaluating $ \int \frac 1 { x ^ 2 } \, \mathrm d x $ without applying the power rule - Is it nonsense?","I want to evaluate the integral $$ \int \frac 1 { x ^ 2 } \, \mathrm d x $$ in a non-standard way and without applying the power rule. Let $ x \ne 0 $ and $$ \int \frac 1 { x ^ 2 } \, \mathrm d x = f ( x ) + C \text . $$ For any $ a \in \mathbb R \setminus \{ 0 \} $ we have $$ f ( a x ) = \int \frac 1 { a ^ 2 x ^ 2 } \, \mathrm d ( a x ) = \frac 1 a \int \frac 1 { x ^ 2 } \, \mathrm d x = \frac 1 a \bigl ( f ( x ) + C \bigr ) \text . $$ Then, we have $$ f ( x ) = a f ( a x ) \text , $$ for any $ a \in \mathbb R \setminus \{ 0 \} $ . Putting $ x = 1 $ , we have $$ f ( a ) = \frac { f ( 1 ) } a \text . $$ This is the critical point for me. I'm not sure I'm not wrong here. I will just continue. Here, I will replace the constant $ a $ with the variable $ x $ . My argument for doing this is as follows: $ a $ covers all nonzero real numbers. So, I will write the last equation like this (but, I'm not sure): $$ f ( x ) = \frac { f ( 1 ) } x \text . $$ We continue. Let $ u = x $ and $ v = f ( x ) $ , so that $ \mathrm d u = \mathrm d x $ and $ \mathrm d v = \frac 1 { x ^ 2 } \, \mathrm d x $ . $$ \int x \cdot \frac 1 { x ^ 2 } \, \mathrm d x = x \cdot f ( x ) - \int f ( x ) \, \mathrm d x = \ln x + C _ 1 \text ; $$ $$ \implies x \cdot \frac { f ( 1 ) } x - \int \frac { f ( 1 ) } x \, \mathrm d x = \ln x + C _ 1 \text ; $$ $$ \implies f ( 1 ) - f ( 1 ) \left ( \ln x + C _ 2 \right ) = \ln x + C _ 1 \text ; $$ $$ \implies \bigl ( f ( 1 ) + 1 \bigr ) \ln x = f ( 1 ) ( 1 - C _ 2 ) - C _ 1 \text . $$ The right side is a constant. The left side must also be a constant. $$ \implies f ( 1 ) + 1 = 0 \implies f ( 1 ) = - 1 \text . $$ Finally, we get $$ f ( x ) = - \frac 1 x \text ; $$ $$ \int \frac 1 { x ^ 2 } \, \mathrm d x = - \frac 1 x + C \text . $$ Remark 1. My goal was to find antiderivative of $ \frac 1 { x ^ 2 } $ . Because, $$ \int \frac 1 { x ^ 2 } \, \mathrm d x = \text {antiderivative of } \frac 1 { x ^ 2 } + \text{constant.} $$ In short, the function $ f ( x ) $ was an antiderivative. Remark 2. Initially $ a $ is a constant. However, it covers all real numbers except zero. Therefore, I replaced it with the variable $ x $ . How much of my work does math allow? Is the method I am using a nonsense? Otherwise, can the errors it contains be corrected?","['integration', 'indefinite-integrals', 'calculus', 'solution-verification']"
3995325,"Given the minimal polynomial of a matrix $A^2$, what could the minimal polynomial of $A$ be?","It is given that the minimal polynomial of $A^2$ is $\phi_{A^2}(x) = (x-1)^2$ , where $A$ is a complex $4\times4$ matrix. The question is, what are the possible minimal polynomials for $A$ ? From the given that $\phi_{A^2}(x) = (x-1)^2$ , I can derive several things. First off, I know that the characteristic polynomial $f_{A^2}$ of $A^2$ must have the same irreducible factors, which implies that the only eigenvalue of $A^2$ is $1$ . Secondly, it is easily seen that $(A^2 -I)^2 = O$ and $A^2 - I \neq O$ . Therefore, the matrix $A^2 - I$ is nilpotent with index 2. This implies that the invariant system is either $\{2,1,1\}$ or $\{2,2\}$ . Putting this together, there are two possible Jordan forms of $A^2$ : $$ J_{A^2,1} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{bmatrix} \quad\text{or}\quad J_{A^2,2} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 1 & 1\end{bmatrix} $$ Great, all these things I know about $A^2$ ... but how does any of it lead to information about the minimal polynomial of $A$ ? Or just any information about $A$ at all? I'm kind of stuck on this.","['matrices', 'minimal-polynomials', 'abstract-algebra', 'linear-algebra']"
3995340,Combinatoric proof of an algebric expression [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How can we prove that $\left(\frac{n}{k}\right)^k\le \binom{n}{k}$ I try with induction but I don't get the result I want.","['solution-verification', 'combinatorics', 'discrete-mathematics']"
3995401,infinite sum of harmonic number powers $\sum_{n=0}^{\infty}x^{H_n}$,"I am looking for a simplified form of the infinite sum of harmonic exponentials $$
f(x) = \sum_{n=0}^{\infty}x^{H_n}
= 1 + x + x^{3/2} + x^{11/6} + x^{25/12} + x^{137/60} + x^{49/20} + \ldots\\
=1+x\left(
1+x^{1/2}\left(
1+x^{1/3}\left(
1+x^{1/4}\left(
1+x^{1/5}\left(
1+x^{1/6}\left(
1+\ldots
\right)\right)\right)\right)\right)\right)
$$ for $x\in[0,1/e)$ . Can it be reduced to a standard special function? Update: a stable way of calculating $f(x)$ numerically Inspired by Claude Leibovici's answer , we write $$
f(x) = 1 + \sum_{n=1}^{\infty}x^{\gamma+\log(n)} - \underbrace{\sum_{n=1}^{\infty}\left[x^{\gamma+\log(n)}-x^{H_n}\right]}_{g(x)}\\
= 1+x^{\gamma}\zeta[-\log(x)]-g(x)
$$ with $g(x)$ converging rapidly on the entire interval $x\in[0,1/e]$ (and beyond) without singularities: Specifically, $$
g(1/e) = \sum_{n=1}^{\infty}\left[\frac{e^{-\gamma}}{n}-e^{-H_n}\right]
\approx 0.35370447459102146589\ldots\\
g'(1/e) \approx 0.38530115152397563963\ldots\\
g''(1/e) \approx -0.82751608018475931083\ldots
$$ give the approximate series expansion of $f(x)$ around $x=1/e$ , $$
f(x) \approx
\frac{e^{-\gamma-1}}{1/e-x}
+ 0.36556578362553594918\ldots
+ (1/e-x)\cdot0.11500038261356357678\ldots
+ (1/e-x)^2\cdot0.11786065053337882776\ldots
+O[(1/e-x)^3]
$$ Unfortunately, neither WolframAlpha nor AskConstants recognize any of these numbers.","['harmonic-numbers', 'summation', 'sequences-and-series']"
3995448,Expected value for a continuous random variable [duplicate],"This question already has answers here : Why is expectation defined by $\int xf(x)dx$? (3 answers) Closed 3 years ago . Let's fix the generic probability space $(\Omega,\Sigma,P)$ , and consider the generic random variable $X\colon \Omega \to \mathbb{R}$ . The expected value of $X$ (supposed to exist) is then defined as $E(X):=\int_{\Omega} X\,\text{d}P \in [-\infty,+\infty]$ , namely using the Lebesgue integral of $X$ with respect to the probability measure $P\colon \Sigma \to [0,1]$ . When $X$ is a continuous variable, namely when there exists $f_X\colon \mathbb{R} \to [0,+\infty)$ such that $\forall \,x \in \mathbb{R} \quad F_X(x):=P(X \le x)=\int_{-\infty}^x f_X(t)\,\text{d}t$ , we have also that $E(X)=\int_{-\infty}^{+\infty} xf_X(x)\,\text{d}x$ . My question is: how can I show that the general definition of $E(X)$ reduces to the last one for continuous random variables? Can you suggest me a textbook or a source where I can find some help? P.S. I have found here Wikipedia, pushforward measure that $\int_{\Omega} X\,\text{d}P=\int_{\mathbb{R}} \text{d}\mu_X$ , where $\mu_X\colon \mathcal{B}(\mathbb{R}) \to [0,1] \mid \mu_X(A)=P(X \in A)$ is the pushforward measure of $X$ , with $\mathcal{B}(\mathbb{R})$ the Borel $\sigma$ -algebra over $\mathbb{R}$ . I know also that $F_X'=f_X$ and that $F_X(x)=\mu_X((-\infty,x])$ , so maybe $\text{d}\mu_X=\text{d}F_X=f_X\,\text{d}x$ , but I seriously think this is totally wrong. Thank you for your time and help!","['measure-theory', 'lebesgue-integral', 'real-analysis', 'expected-value', 'probability-theory']"
3995459,$L^p$ space: measure theory,"Suppose that $(X,m,\mu)$ is a measure space. Prove that $$L^{1}(\mu)=L^{\infty}(\mu)  \iff \dim L^{1}(\mu)<\infty$$ It's obvious that if $\mu(X)<\infty$ , then $L^{\infty}(\mu)\subseteq L^{1}(\mu)$ which does not hold in this case. Also don't know how to use the finiteness of $\dim L^{1}(\mu)$ . Any help would be appreciated. Thanks in advance!","['measure-theory', 'lp-spaces']"
3995517,"Prove $(x-1)(y-1)>(e-1)^2$ where $x^y=y^x$, $y>x>0$.","Let $x,y$ be different positive real numbers satisfying $x^y=y^x$ .
Prove $(x-1)(y-1)>(e-1)^2$ . We may suppose $a=\frac{y}{x}>1$ . Then we obtain $x=a^{\frac{1}{a-1}},y=a^{\frac{a}{a-1}}.$ But how to go on?","['calculus', 'inequality', 'real-analysis']"
3995563,Is this true: adding a random DIAGONAL perturbation can make a nondiagonalizable matrix diagonalizable?,"For any nondiagonalizable matrix $A\in\mathbb{R}^{n\times n}$ , let $\hat{A}=A+E$ where $E$ is a small random DIAGONAL matrix (e.g. diagonal entries of $E$ are i.i.d. sampled from $N(0,\epsilon^2)$ for arbitrarilly small $\epsilon>0$ ). Can we conclude that $\hat{A}$ is diagonalizable with probability $1$ (Here $A$ is fixed and the randomness is only from $E$ )? My question is similar to this question , but here the perturbation matrix is required to be diagonal.","['matrices', 'linear-algebra', 'probability']"
3995577,On the definition of cohomological dimension,"Let $G$ be a group and $R$ a commutative unital ring. We define the $R$ -cohomological dimension of $G$ to be $$cd_R(G) := \sup \{ n : H^n(G, M) \neq 0 \text{ for some } R[G]\text{-module } M \}.$$ I have read somewhere that this may be equivalently defined as $$cd_R'(G) := \sup \{ n : H^n(G, R[G]) \neq 0 \},$$ but in a recent discussion on a previous post it was pointed out to me that this is only true if $cd_R(G) < \infty$ . Is this the case? What are some counterexamples? Could you point me to a reference? I would like to understand this in general but I am especially interested in a specific case: $G$ is finitely presented, $R$ is finite, and $cd_R'(G) \leq 1$ . Can I deduce in this case that $cd_R(G) = cd_R'(G)$ , or even just that $cd_R(G) \leq 1$ as well?","['finite-rings', 'abstract-algebra', 'group-cohomology', 'group-theory', 'homology-cohomology']"
3995627,Why is $x(t)$ is $(k+1)$ continuously differentiable?,"Hello in my ODE lecture I have the following statement but I don't see why exactly it's true: Let the following first-order differential equation : $$\dot x = f(t,x(t)) \tag{1}$$ Defined by the function $f(t,x)\colon \mathbb R \times \mathbb R ^n \rightarrow \mathbb R^n  $ , s.t $f(t,x)$ is $k$ continuously differentiable for all the variables $(t,x_1,...,x_n)$ If we suppose that $x = x(t) \colon I \rightarrow \mathbb R^n  $ is a continuously differentiable solution of $(1)$ . Hence $x(t)$ is $(k+1)$ continuously differentiable. If anyone can explain why this statement is necessarily true it would be a lot appreciated. Thanks in advance","['continuity', 'derivatives', 'ordinary-differential-equations']"
3995636,point of intersection of two lines in barycentric coordinate system,"I am looking for an efficient way to determine the intersection point of two lines which go through a triangle (face) of a 3D triangular surface mesh. For both lines I know the two points at which they intersect with the edges of a triangle (face).
Denoted $P_A^1, P_B^1$ for the first line and $P_A^2, P_B^2$ for the second line ( See illustrative example figure ). Based on my investigations my preferred approach would be to: Transform the the points of intersection for both lines ( $P_A^1, P_B^1$ and $P_A^2, P_B^2$ ) into barycentric coordinates resulting in $B_A^1, B_B^1$ and $B_A^2, B_B^2$ . Define two lines: $L_1$ which goes through $B_A^1, B_B^1$ and $L_2$ going through $B_A^2, B_B^2$ based on the two-point form defined in section 4.1.1. of this document Determine the point $B_I$ as the barycentric coordinates where $L_1$ and $L_2$ intersect based on the equations in section 4.3 of this document . Transform $B_I$ back into the cartesian coordinate system to have its 3D coordinates, denoted as $P_I$ . Unfortunately steps 2 and 3 do not lead to meaningful results (i.e., the intersection points fall outside the triangle) and I have doubts that I am applying equations in section 4.1.1. and section 4.3 properly. Even when using a simple example based on the upper illustration in this figure here by defining $B^1_A=(1,0,0), B^1_B=(0,1/2,1/2)$ and $B^2_A=(0,1,0), B^2_B=(1/2,0,1/2)$ I cannot determine $B_I$ correctly as $B_I=(1/3,1/3,1/3)$ with the above procedure.","['geometry', 'meshing', 'finite-element-method', 'triangles', 'barycentric-coordinates']"
3995668,Notation in the definition of random variable,"I am trying to understand definition of random variable. Given a probability space $(\Omega, \mathcal F, P),$ a random variable $X$ is a function from the sample space $\Omega$ to the real numbers $\mathbb R.$ Once the outcome $\omega \in \Omega$ of the experiment is revealed, the corresponding $X(ω)$ is known as realization of the random variable. Consider two sample spaces $\Omega_1$ and $\Omega_2$ and a sigma-algebra $\mathcal F_2$ of sets in $\Omega_2.$ Then, for $X$ to be a random variable, there must exist a sigma-algebra $\mathcal F_1$ in $Ω_1$ such that for any set $S$ in $\mathcal F_2$ the inverse image of $S,$ defined by: $$X^{-1}(S) := \{ω \mid X(ω) ∈ S\}$$ I do not know how to read this and I am hoping some one can explain each part to me.","['notation', 'statistics', 'definition', 'probability']"
3995708,"""Math Lotto"" Tickets - finding the minimum winning set","""Math lotto"" is played as follows:  a player marks six squares on a 6x6
square. Then six  ""losing squares"" are drawn. A player wins if none of the losing squares
are marked on his lottery ticket. 1)Prove that one can complete nine lottery tickets in such a way that at least one of
them wins. 2)Prove that this is not possible with only eight tickets. My attempt is as follows; First I divided the square into 6 rectangles (figure 1). If one rectangle doesn't contain a cross then some ticket (ticket 1 to ticket 6) would win the game. Now we consider the case where each rectangle has one cross each. Now take the two rectangles on the top left of the square (figure 2). These have a total of two crosses. The first two columns together contains one cross and the third and fourth columns together contains one cross. There are four cases and we need at least four tickets (ticket 7 to ticket 10) to ensure win. I am only getting a minimum of ten tickets. How do I prove only nine tickets is required and for eight tickets it is not possible? Reference: Combinatorics by Stephan Wagner, Page 42, Problem 49
. https://math.sun.ac.za/swagner/Combinatorics.pdf","['combinatorial-designs', 'coloring', 'pigeonhole-principle', 'combinatorics', 'lotteries']"
3995732,"Extreme values of $f(x, y)=\frac{(x+y)^{2}}{2}+\frac{(x-y)^{3}}{3}$ on $D=\left\{(x, y) \in \mathbb{R}^{2}:|y| \leq 1-|x|\right\}$","Extreme values of $f(x, y)=\frac{(x+y)^{2}}{2}+\frac{(x-y)^{3}}{3}$ on $D=\left\{(x, y) \in \mathbb{R}^{2}:|y| \leq 1-|x|\right\}$ I resolved by taking the partial derivatives and computing the Hessian matrix, so $(0,0)$ is a saddle point. For the first edge I tried to parametrize $D$ $$\begin{cases}x=1-t\\y=t\end{cases}$$ and resolve for $f(x,y)$ : $$f(t) = \frac{1}{2} + \frac{(2t-1)^3}{3}$$ $$f'(t)=4t-2=0 \implies t=\frac{1}{2}$$ and follows that $\left(\frac{1}{2},\frac{1}{2}\right)$ could be and extreme. It's clear that $(1,0)$ is a max value, but I can't find it.
Can someone help me out in this?",['multivariable-calculus']
3995766,$R$-module with local endomorphism ring is also an $R_P$-module for some $P\in\operatorname{Spec} R$,"Let $R$ be a commutative ring, and suppose that $U$ is an $R$ -module with local endomorphism ring. (In particular, note that $U$ is indecomposable.) Consider the ring morphism $f:R\to\operatorname{End}_RU$ taking $r\in R$ to the multiplication-by- $r$ map, $f_r$ . The maximal ideal of $\operatorname{End}_RU$ is the set of non-automorphisms in $\operatorname{End}_RU$ , and so its contraction under $f$ is the prime ideal $$P:=\{r\in R:f_r\notin\operatorname{Aut}_R U\}\in\operatorname{Spec}R.$$ Now $U$ admits unique division by all elements of $R$ outside $P$ , and so can be considered as an $R_P$ -module in the natural way. An immediate corollary of this is that $U$ can also be considered as an $R_M$ module for some maximal ideal $M<R$ ; just take any maximal ideal containing $P$ . However, $P$ itself does not have to be maximal: for instance, consider $\mathbb{Q}$ as a $\mathbb{Z}$ -module. This has endomorphism ring isomorphic to $\mathbb{Q}$ , and so $P$ in this case is $\{0\}$ , which is of course not maximal in $\mathbb{Z}$ . Question: Under what circumstances is $P$ maximal? Are there any ""nice"" characterizations of when this is the case, perhaps with some additional constraints on $U$ or $R$ ? Context: By a theorem of Ziegler, indecomposable and pure-injective modules have local endomorphism rings, so the fact above shows that classifying those modules over a commutative ring is equivalent to classifying them over the localizations at all maximal ideals. (One also needs to show that an $R_M$ -module is pure-injective as an $R_M$ -module if and only if it is pure-injective as an $R$ -module, but this is not hard.) This was the context that prompted the question for me, so I'm especially interested in the special case when $U$ not only has local endomorphism ring but is also pure-injective. Can anything be said about the question about in that case?","['localization', 'modules', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
3995784,On a tempered distribution which comes from a locally integrable function,"Let $ f\colon \mathbb{R}^n \to \mathbb{C} $ be a measurable function. We say that $ f $ is locally integrable if $ f|_K $ is integrable for all compact subsets $ K \subseteq \mathbb{R}^n $ . A locally integrable function $ f $ yields a distribution $ T_f $ , which is given by $$
\langle T_f, \phi \rangle = \int_{\mathbb{R}^n} f(x) \phi(x) \,dx
$$ for $ \phi \in \mathscr{D}(\mathbb{R}^n) $ (test functions). Question. Suppose that $ T_f $ is tempered as a distribution. Then $ T_f $ is extended so that $ \langle T_f, \phi \rangle $ is defined for all $ \phi \in \mathscr{S}(\mathbb{R}^n) $ (rapidly decreasing functions). Is $ f \phi $ integrable for all $ \phi \in \mathscr{S}(\mathbb{R}^n) $ ? For $ \phi \in \mathscr{S}(\mathbb{R}^n) $ with $ f \phi $ integrable, is it true that $ \langle T_f, \phi \rangle = \int_{\mathbb{R}^n} f(x) \phi(x) \,dx $ ?","['integration', 'analysis', 'distribution-theory']"
3995822,about noetherian topological space [duplicate],"This question already has an answer here : Equivalent conditions for a topological space to be Noetherian (1 answer) Closed 1 year ago . Definition. A topological space $X$ is called noetherian if it satisfies the descending chain condition for closed subsets: for any sequence $Y_{1} \supseteq Y_{2} \supseteq \ldots$ of closed subsets, there is an integer $r$ such that $Y_{r}=Y_{r+1}=\ldots$ . Show that the following conditions are equivalent for a topological space $X$ : (i) $X$ is noetherian; (ii) every nonempty family of closed subsets has a minimal element; (iii) $X$ satisfies the ascending chain condition for open subsets; (iv) every nonempty family of open subsets has a maximal element. proof : i can show $(i) \iff (iii)$ (because complement of open set is close and if $A \subset B \implies B^c \subset A^c $ ) and $(ii) \implies (i)$ : Suppose $(ii)$ holds and let $\{F_n\}_{n \in \mathbb N}$ be a sequence of decreasing closed subsets. By hypothesis, there is $k \in \mathbb N$ such that $F_k \subset F_n$ for all $n$ . But since the sequence is decreasing, we have $F_n \subset F_k$ for all $n \geq k$ . From here it follows directly $X$ is noetherian. I think we can use  Zorn's Lemma for $(iii) \implies (iv)$ .","['general-topology', 'algebraic-geometry', 'noetherian']"
3995845,Summable function that is not infinitesimal,"This comes out from this exercise: Let $u$ Harmonic on $\mathbb{R}^n$ and $\int_{_{\mathbb{R}^n}}u(x)^2 dx<+\infty$ prove that $u(x)=0$ . I want to use that $\mathbb{R}^n$ and $\int_{_{\mathbb{R}^n}}u(x)^2 dx<+\infty$ implies $\lim_{|x|\to+\infty}|u(x)|=0$ . I am not sure of this property, is it real? I mean, one can prove the exercise and than it shows that is real, but to prove the exercise one use Cauchy Shwartz on $\langle 1,u\rangle$ (i can give more datails). So the question is when does $\mathbb{R}^n$ and $\int_{_{\mathbb{R}^n}}u(x)^2 dx<+\infty$ implies $\lim_{|x|\to+\infty}|u(x)|=0$ in general?","['measure-theory', 'improper-integrals', 'real-analysis']"
3995849,Malthus Model - Solution Differential Equation,"I have this equation of a time dependent Malthus model with a term representing a time dependent immigration: $$N'(t)=r(t)N(t) + m(t)$$ with $r(t)$ and $m(t)$ both continuous and periodic with Period $T$ .
I have to prove that the function $$N_\infty (t)=\int_{-\infty}^t exp\biggl(\int_s ^t r(\sigma)d\sigma\biggr)m(s)ds$$ is a solution of the ODE above. I tried to derive $N_\infty (t)$ to obtain our ODE, but unsuccessful. I think that I do somethings wrong on the calculation.
I know that the derivative of an integral is so computed:
Let $G(t):=\int_a^x f(t)dt$ . Then $G'(t)=f(x)$ So in our case I did: $$N'_\infty (t)= exp\biggl(\int_s ^t r(\sigma)d\sigma\biggr)m(t)$$ But I think I miss something. Someone can help me please?","['solution-verification', 'ordinary-differential-equations', 'integro-differential-equations']"
3995897,Find all $f(x)$ so that $[y(x)]^2\cos x + y(x)f(x)y'(x)=0$ is exact,"Finde all functions $f(x)$ , so that the ODE is exact... $$[y(x)]^2\cos x + y(x)f(x)y'(x)=0$$ First I rewrote the ODE slightly... \begin{align*}
    &[y(x)]^2\cos x + y(x)f(x)y'(x)=0 \\
    \Longrightarrow \quad & [y(x)]^2\cos x + y(x)f(x)\frac{dy}{dx}=0 \\
    \Longrightarrow \quad & ([y(x)]^2\cos x)dx + y(x)f(x)dy=0 \\
\end{align*} Then I identified $P$ and $Q$ ... \begin{align*}
    &P_x(x,y) = [y(x)]^2\cos x\\
    &Q_y(x,y) = y(x)f(x) \\ 
    \\
    \Longrightarrow \quad &P_{x,y}(x,y) = 2y(x)\cos x \\
    \Longrightarrow \quad &Q_{x,y}(x,y) = y'(x)f(x) + y(x)f'(x) \\
\end{align*} Since $P_{x,y}$ must be equal to $Q_{x,y}$ for the ODE to be exact, I put them equal to find out $f(x)$ but following equations doesn't look solveable... \begin{align*}
    y'(x)f(x) + y(x)f'(x) \overset{!}{=}2\cos(x) y(x)
\end{align*} Any suggestions where I went wrong?","['integration', 'ordinary-differential-equations']"
3995920,How is the Dirac delta $\delta (x^3)$ different from $\delta(x)$?,"How is Dirac delta $\delta (x^3 )$ different from $\delta (x)$ ? It is my understanding that $$
\delta [\psi]:=\langle\delta,\psi\rangle=\int_{\Omega}\delta(x)\psi(x)\,dx=
\begin{cases}
\psi(0) & \text{if $0\in \Omega$}\\
0 & \text{otherwise}
\end{cases}
$$ But $x^3=0$ iff $x=0$ so does this not imply, $$\delta(x^3) = \delta(x) = 0$$ Apologies if I am completely misunderstanding the topic, I am very new to distributions.","['dirac-delta', 'real-analysis']"
3995924,Comparing between regular polygons,"In this experiment, I am adding the inradius (let's call it $A$ ) and circumradius (let's call it $B$ ) of different polygons with equal sides each equal $1$ (starting with a square and adding one side each time). The result is $A+B=C$ when side of polygon = 1. When comparing the $C$ of one polygon with the $C$ of a polygon with one side more, the difference seems to go smaller, as if approaching a version of $\pi$ number with $0.$ before (possibly such as 0.314159265359...). Can anyone confirm it or elaborate on it? I can not go over a polygon with 1000 sides in my computation power, and would like to know what to expect while going towards a polygon with infinity sides. Here are some examples: 4 sided polygon: $0.5 + 0.707106781 = 1.207106781$ 5 sided polygon: $0.68819096 + 0.850650808 = 1.5388417680000002$ (Difference of 0.33173498700000015 from previous result) 6 sided polygon: $0.866025404 +1 = 1.866025404$$ (Difference of 0.3271836359999998 from previous result) 7 sided polygon: $1.0382607 + 1.15238244 = 2.1906431399999997$ (Difference of 0.32461773599999977 from previous result) 8 sided polygon: $1.20710678 + 1.30656296 = 2.51366974$ (Difference of 0.3230266000000004 from previous result) 9 sided polygon: $1.37373871 + 1.4619022 = 2.83564091$ (Difference of 0.32197116999999986 from previous result) 10 sided polygon: $1.53884177 + 1.61803399 = 3.15687576$ (Difference of 0.3212348500000002 from previous result) 11 sided polygon: $1.70284362+ 1.77473277 = 3.47757639$ (Difference of 0.3207006299999997 from previous result) 12 sided polygon: $1.8660254+ 1.93185165 = 3.79787705$ (Difference of 0.32030066 from previous result) 13 sided polygon: $2.02857974+ 2.08929073 = 4.11787047$ (Difference of 0.31999341999999986 from previous result) 14 sided polygon: $2.19064313 + 2.2469796 = 4.43762273$ (Difference of 0.31975226000000045 from previous result) 15 sided polygon: $2.35231505+ 2.40486717 = 4.757182220000001$ (Difference of 0.3195594899999996 from previous result) ... 999 sided polygon: $158.995264 + 158.99605 = 317.991314$ 1000 sided polygon: $159.154419 + 159.155205 = 318.309624$ (Difference of 0.31830999999999676 from previous result)","['pi', 'geometry', 'polygons']"
3995983,Does any sequence with values in a directed set have a monotone subsequence?,"Definition: a directed set is a set $M$ together with a preorder $\geq$ (reflexive and transitive order) such that every pair of elements in $M$ has an upper bound ( $\forall x,y \in M, \ \exists z \in M,\ z \geq x \wedge z \geq y$ ) For sequences in the reals, there always exists a monotone subsequence. The proof of this fact uses the total order property of the real numbers. But I was wondering, would it work for directed sets? In other words, does a sequence with values in a directed set $(M, \geq)$ always have a monotone subsequence? I tried to find a counterexample, but I have yet to work with directed sets to be able to visualize them at this level.","['order-theory', 'sequences-and-series']"
3996033,Deriving $-nE\left[\frac{\partial ^2\ln\left(f\left(X\right)\right)}{\partial \theta ^2}\right]$,"The information about θ in a random sample of size n is also given by $$-nE\left[\frac{\partial ^2\ln\left(f_\theta\left(X\right)\right)}{\partial \theta ^2}\right]$$ where $f_\theta(x)$ is the value of the population density at x, provided that the extremes of the region for which $f_\theta(x)$ is not equal to 0 do not depend on θ. The derivation of this formula takes the following steps: Differentiating the expressions on both sides of $$\int f_\theta\left(x\right)dx=1$$ with respect to $\theta$ , show that $$\int \frac{\partial \ln\left(f_\theta\left(x\right)\right)}{\partial \theta }\left(f_\theta\left(x\right)\right)dx=0$$ by interchanging the order of integration and differentiation. Differentiating again with respect to θ, show that $$E\left[\left(\frac{\partial \ln\left(f_\theta\left(X\right)\right)}{\partial \theta }\right)^2\right]=-E\left[\frac{\partial ^2\ln\left(f_\theta\left(X\right)\right)}{\partial \theta ^2}\right]$$ I'm a little confused about what the question is asking. I am not sure how the $\ln$ comes in, in the first part and then how I relate that to the second part of the question?","['statistics', 'probability']"
3996034,The Automorphism Group $\Gamma(\mathbb{Q}(\sqrt[n]{2}):\mathbb{Q})$ is trivial if $n$ is odd.,"The full question is: Given a group $F_n=\mathbb{Q}(\sqrt[n]{2})$ then prove that: (A) If $n$ is odd then $\Gamma(F_n:\mathbb{Q})$ ={ $\rm{id}$ } and (B) If $n$ is even then $\Gamma(F_n:\mathbb{Q})\cong\mathbb{Z_2}$ I am confused as to how to find either, I know that $F_n:\mathbb{Q}$ is a non-normal extension. We were given the hint: ""For any $\tau\in\rm{Aut}_\mathbb{Q}(F_n)$ we must have that $\tau(\sqrt[n]{2})\in F_n\subseteq\mathbb{R}$ is again a real $n$ -th root of 2""","['galois-theory', 'abstract-algebra', 'automorphism-group', 'extension-field']"
3996067,Tricks to interpret $\mathbb{R}^{\mathbb{R}}$,"So I have the following question of how to interpret $\mathbb{R}^{\mathbb{R}}$ . Is this just an ""infinite"" tuple of real numbers? Since for example $ (x, y) \in \mathbb{R}^2$ , $(x, y, z) \in \mathbb{R}^3$ , etc. Is $(x_1, x_2, ... , x_n) \in \mathbb{R}^n$ ? Or is there another way that I should interpret this?","['elementary-set-theory', 'real-numbers']"
3996110,Pointwise and uniform convergence of $f_n(x)=\cos \frac{nx}{1+n^2}$,"Study the pointwise and uniform convergence of $f_n(x)=\cos \frac{nx}{1+n^2}$ for $x \in \mathbb{R}$ . If $x=0$ it is $f_n(0)=1$ , for $x \ne 0$ it is $$\lim_{n \to \infty} \cos\frac{nx}{1+n^2}=\cos 0=1$$ So $f_n(x)$ converges pointwise to $f(x)=1$ for all $x \in \mathbb{R}$ . To study the uniform convergence I must evaluate $$\lim_{n \to \infty} \sup_{x \in \mathbb{R}}\left|\cos \frac{nx}{1+n^2}-1\right|=\lim_{n \to \infty} \sup_{x \in \mathbb{R}}\left(1-\cos \frac{nx}{1+n^2}\right)$$ Since the derivative of $1-\cos \frac{nx}{1+n^2}$ exists for all $x\in\mathbb{R}$ , it is $$\frac{d}{dx} \left(1-\cos \frac{nx}{1+n^2}\right)=\frac{n}{1+n^2} \sin \frac{nx}{1+n^2} \geq 0$$ $$\iff 2k\pi \leq \frac{nx}{1+n^2} \leq (2k+1)\pi \iff \frac{1+n^2}{n}2k\pi \leq x \leq \frac{1+n^2}{n}(2k+1)\pi$$ For $k\in\mathbb{Z}$ , so $$\lim_{n \to \infty} \sup_{x \in \mathbb{R}}\left(1-\cos \frac{nx}{1+n^2}\right)=\lim_{n \to \infty} \left(1-\cos \frac{n \left(\frac{1+n^2}{n}(2k+1)\pi\right)}{1+n^2}\right)=\lim_{n\to\infty}(1-\cos\pi)=2$$ So $f_n$ is not uniformly convergent in $\mathbb{R}$ .
Is this correct? Some more questions: I've noticed that $f_n(\frac{1+n^2}{n})=\cos 1$ , can I conclude that since there exists at least one $\bar{x}=\frac{1+n^2}{n}$ such that $|f_n(\bar{x})-f(\bar{x})|=1-\cos 1 \ne 0$ so $f_n$ cannot be uniform convergent in whole $\mathbb{R}$ because it it will never be $|f_n(x)-f(x)|<\varepsilon$ for all $x \in \mathbb{R}$ because of $\bar{x}$ . Is this correct? It is $\cos \frac{nx}{1+n^2} \approx 1-\frac{n^2 x^2}{2(1+n^2)^2}$ as $n \to \infty$ , can this be useful to evaluate the limit of the supremum? Or is it useless because the asymptotic behaviour can't give informations about a supremum? Is there a way to identify if there are some subsets of $\mathbb{R}$ where $f_n$ can converge uniformly to $f$ ?","['uniform-convergence', 'analysis', 'sequences-and-series']"
3996123,coefficient $[q^n]\sum_{m\ge1}\frac{q^{\alpha m}}{1-q^{\beta m}}$ where $0<\alpha<\beta$,"Problem: I am looking for a finite-sum expression for the coefficient $c_n=c_n(\alpha,\beta)$ , where $$C(\alpha,\beta;q)=\sum_{m\ge1}\frac{q^{\alpha m}}{1-q^{\beta m}}=\sum_{n\ge1}c_n(\alpha,\beta)q^n,$$ with $\alpha,\beta\in\Bbb Z$ , $0<\alpha<\beta$ , and $|q|<1$ . Context: I know such $c_n$ exist, because $|q|<1$ ensures that $\frac{q^{\alpha m}}{1-q^{\beta m}}$ is analytic about $q=0$ for $m\ge1$ . I was motivated to attempt this because of the well-known $$\begin{align}
\vartheta_3^2(q)&=1+4\sum_{m\ge1}\frac{q^m}{1+q^{2m}}\\
&=1+4\left(C(3,4;q)-C(1,4;q)\right),
\end{align}$$ which implies that $r_2(n)=4c_n(3,4)-4c_n(1,4)$ for $n\ge1$ . I know that finite-sum expressions for $c_n(3,4)$ and $c_n(1,4)$ , along with $c_n(\alpha,\beta)$ exist because the following (more general) result is true: $$A(a,b;q)=\sum_{m\ge1}\frac{(aq)^m}{1-bq^m}=\sum_{n\ge1}q^n\sum_{d|n}a^db^{n/d-1}.\tag1$$ My Attempts: Let $i(n,m)=1$ when $n|m$ and $i(n,m)=0$ otherwise, and note that for positive integer $v$ , $$\frac{x^v}{1-x^v}=\sum_{r\ge v}x^ri(v,r).$$ Then let $u=\beta-\alpha$ so that $$\begin{align}
C(q)&=\sum_{m\ge1}\frac{q^{\alpha m}}{1-q^{\beta m}}\\
&=\sum_{m\ge1}q^{-um}\frac{q^{\beta m}}{1-q^{\beta m}}\\
&=\sum_{m\ge1}q^{-um}\sum_{r\ge\beta}q^{mr}i(\beta,r)\\
&=\sum_{r\ge\beta}i(\beta,r)\sum_{m\ge1}q^{(r-u)m}\\
&=\sum_{t\ge\alpha}i(\beta,t+u)\sum_{m\ge1}q^{tm}\\
&=\sum_{t\ge\alpha}i(\beta,t+u)\sum_{s\ge t}q^{s}i(t,s)\\
&=\sum_{t\ge\alpha}\sum_{s\ge t}q^{s}i(\beta,t+u)i(t,s)\\
&=\sum_{t\ge\alpha}\sum_{j\ge 0}q^{j+t}i(\beta,t+u)i(t,j+t)\\
&=\sum_{j\ge0}\sum_{t\ge \alpha}q^{j+t}i(\beta,t+u)i(t,j+t)\\
&=\sum_{j\ge0}\sum_{l\ge 0}q^{j+l+\alpha}i(\beta,l+\beta)i(l+\alpha,j+l+\alpha).
\end{align}$$ Then write $n=j+l+\alpha$ . Since $C(0)=0$ we know that $n\ge1$ . Thus $$\begin{align}
C(q)&=\sum_{n\ge1}\sum_{\,\,\,j,l\ge0\\ j+l=n-\alpha}q^{j+l+\alpha}i(\beta,l+\beta)i(l+\alpha,j+l+\alpha)\\
&=\sum_{n\ge1}\sum_{\,\,\,j,l\ge0\\ j+l=n-\alpha}q^{n}i(\beta,l+\beta)i(l+\alpha,n)\\
&=\sum_{n\ge1}q^n\sum_{\,\,\,j,l\ge0\\ j+l=n-\alpha}i(\beta,l+\beta)i(l+\alpha,n).
\end{align}$$ Since $j,l\ge0$ and $j+l=n-\alpha$ , we have $0\le l\le n-\alpha$ . Then upon writing $d=l+\alpha$ , we have $$\begin{align}
C(q)&=\sum_{n\ge1}q^n\sum_{l=0}^{n-\alpha}i(\beta,l+\beta)i(l+\alpha,n)\\
&=\sum_{n\ge1}q^n\sum_{d=\alpha}^{n}i(\beta,d+u)i(d,n)\\
&=\sum_{n\ge1}q^n\sum_{d|n,\,d\ge\alpha}i(\beta,d+u).\tag{*}
\end{align}$$ This would imply that $$c_n(\alpha,\beta)=\sum_{d|n,\,d\ge\alpha}i(\beta,d+u).$$ Question: Is $(*)$ correct? If not, what is the expression for $c_n$ that I'm looking for? Thanks :)","['number-theory', 'analytic-number-theory', 'divisor-sum', 'sequences-and-series', 'q-series']"
3996218,Can a quadratic equation not equal to zero,Well I wanted to know whether or not $y = x^2 + x + 7$ is a quadratic equation since the general form is $ax^2 + bx + c = 0$ here the equation $y=x^2+x+7$ . Isn't equal to zero so I'm a bit confused,"['algebra-precalculus', 'quadratics']"
3996242,Prove that $A^T \cdot A$ equal $I$ for an orthonormal matrix $A$ directly using matrix multiplication,"Apparently it is always true that $A \cdot A^T=A^T \cdot A=I$ for an orthonormal matrix $A$ . I know this can be proven using theorems regarding the rank and invertibility of matrices, but I would like to show it directly with matrix multiplication. Suppose that $A = 
\begin{bmatrix}
u_1 &v _1 \\
u_2 &v_2 \\
\end{bmatrix}
$ is an orthonormal matrix.
Then we know that $\begin{bmatrix} u_1\\u_2 \end{bmatrix}^T \begin{bmatrix} u_1\\u_2 \end{bmatrix}=1$ and $\begin{bmatrix} v_1\\v_2 \end{bmatrix}^T \begin{bmatrix} v_1\\v_2 \end{bmatrix}=1$ , and also that $\begin{bmatrix} u_1\\u_2 \end{bmatrix}^T \begin{bmatrix} v_1\\v_2 \end{bmatrix}=0$ However, we must have that $A \cdot A^T = \begin{bmatrix}
u_1^2+v_1^2 & u_1u_2+v_1v_2 \\
u_1u_2+v_1v_2  & u_2^2+v_2^2 \\
\end{bmatrix} 
$ , and I do not see why $u_1^2+v_1^2 =1$ or why $u_1u_2+v_1v_2=0.$ It seems like $A \cdot A^T = I$ is not necessarily true. Where is my mistake?","['orthonormal', 'orthogonality', 'matrices', 'orthogonal-matrices', 'linear-algebra']"
3996299,"Conditional expectation $\mathbb E(Y \mid X \in B)$ instead of $\mathbb E(Y \,|\, X = x)$ (generalization of Shorack's PFS Notation 7.4.1)","In Shorack's Probability for Statisticians Notation 7.4.1, he notes that the conditional expectation (defined in the measure theoretic way) $\mathbb E(Y\mid X)$ is $g(X)$ for some measurable $g :(\mathbb R, \mathcal B_\mathbb R) \to (\mathbb R, \mathcal B_\mathbb R)$ . He then defines $\mathbb E(Y \mid X=x)$ as simply $g(x)$ . For conditional probabilities, I'm pretty sure this means that $P(A \mid X=x)$ will be defined as $\mathbb E(1_A \mid X=x)$ . I'm not entirely confident that this measure-theoretic definition of conditional probability conditioned on $X=x$ matches with the classical notion of conditional probability, so if someone could shed some light on that too that'd be great. Here's a picture of the relevant section from the book. My question is: is there a generalization of this definition to $\mathbb E(Y\mid X\in B)$ for some Borel set $B \in \mathcal B_\mathbb R$ ? Looking at this question/answer When do the measure-theoretic and elementary definitions of conditional probability/expectation coincide? , it seems like the generalization would require dividing $P(X\in B)$ , which may not be possible since it could be $0$ . In that case, why is it that we can have a general definition of $\mathbb E(Y\mid X=x)$ but not of $\mathbb E(Y\mid X\in B)$ ?","['conditional-probability', 'definition', 'probability-theory', 'conditional-expectation']"
3996338,"For the given function $f(x)=\frac{1}{\sqrt {1+x}} +\frac{1}{\sqrt {1+a}} + \sqrt{\frac{ax}{ax+8}}$, prove that $1<f(x)<2$ for positive a and $x\ge 0$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question For the given function $f(x)=\frac{1}{\sqrt {1+x}} +\frac{1}{\sqrt {1+a}} + \sqrt{\frac{ax}{ax+8}}$ , prove that $1<f(x)<2$ for +ve $a$ and $x\ge 0$ I tried the most rudimentary method ie. differentiating wrt x and hoped to get at least 2 solutions for $f’(x)$ , unfortunately I wasn’t able to find a solution for implying function is monotonous . Now whether it’s sis increasing or decreasing still remains a mystery, given that we don’t know what $a$ is. What method can be used to solve this?","['calculus', 'functions', 'inequality']"
3996345,"Vector bundles of rank $q+k$ in $K(Y)$, where $Y$ is a projective scheme of dimension $q$ over some field.","Given a vector bundle $G$ of rank $q+k$ on a projective scheme $Y$ of dim $q$ over some field,show that there exists an injection $0\rightarrow O_Y^k\rightarrow G(n)$ for some sufficiently large $n$ . I met this question when considering $[G(n)]$ in $K(Y)$ .Thanks for your help.",['algebraic-geometry']
3996363,How many solutions does $2 (\sin^{-1}x)^2 -5\sin^{-1}x+2=0$ have?,Number of solutions of the equation $2 (\sin^{-1}x)^2 -5\sin^{-1}x+2=0 $ Let $t=\sin^{-1}x $ $2 t^2 -5t+2=0$ $(t-2)(2t-1)=0 $ $t=2$ or $t=1/2$ Then $\sin^{-1}x=2\quad$ or $\quad   \sin^{-1}x=1/2$ $x=\sin 2\quad$ or $\quad  x=\sin(1/2)$ Did I do something wrong here? I found a solution here => https://www.toppr.com/ask/question/number-of-solutions-of-the-equation-2sin1x25sin1x20/ The solution given on this site says only one solution exists. But I can't understand why $x=\sin 2$ is rejecting. isn't $t$ is an angle and $x$ is a value? Please help.,"['algebra-precalculus', 'solution-verification', 'quadratics', 'trigonometry']"
3996425,$\lim\limits_{n\to\infty}n\big(\sum_{k=1}^n\frac{k^2}{n^3+kn}-\frac{1}{3}\big)$?,"calculate $$\lim\limits_{n\to\infty}n\left(\sum\limits_{k=1}^n\dfrac{k^2}{n^3+kn}-\dfrac{1}{3}\right).$$ I got it $$\lim\limits_{n\to\infty}\sum\limits_{k=1}^n\dfrac{k^2}{n^3+kn}=\lim\limits_{n\to\infty}\dfrac{1}{n}\sum\limits_{k=1}^n\dfrac{(\frac{k}{n})^2}{1+\frac{k}{n^2}}.$$ Use Squeeze theorem we have $$\frac{1}{n+1}\sum\limits_{k=1}^n(\frac{k}{n})^2<\dfrac{1}{n}\sum\limits_{k=1}^n\dfrac{(\frac{k}{n})^2}{1+\frac{k}{n^2}}<\dfrac{1}{n}\sum\limits_{k=1}^n(\frac{k}{n})^2$$ So $$\lim\limits_{n\to\infty}\sum\limits_{k=1}^n\dfrac{k^2}{n^3+kn}=\int_0^1x^2\mathrm{d}x=\frac{1}{3}.$$ Use $$\lim\limits_{n\to\infty}n\left(\int_0^1f(x)\mathrm{d}x-\frac{1}{n}\sum\limits_{k=1}^{n}f\left(\frac{k}{n}\right)\right)=\frac{f(0)-f(1)}{2}.$$ Hence $$\lim\limits_{n\to\infty}n\left(\sum\limits_{k=1}^n\dfrac{k^2}{n^3+kn}-\dfrac{1}{3}\right)=\frac{1}{2}.$$ If our method is correct, is there any other way to solve this problem? Thank you","['integration', 'limits', 'solution-verification', 'real-analysis']"
3996441,Minimum of $\left|1-\left(ab+bc+ca\right)\right|+\left|1-abc\right|$,"If $a,b,c\in\mathbb{R}$ and $a+b+c=1$ , then what is the minimum value
of $\left|1-\left(ab+bc+ca\right)\right|+\left|1-abc\right|$ . I used Wolfram Alpha and it says the minimum value is $\dfrac{44}{27}$ for $\left(a,b,c\right)=\left(\frac{1}{3},\frac{1}{3},\frac{1}{3}\right)$ . Obviously $uvw$ method doesn't help. and since we have absolute value, I think The Buffalo doesn't help. I wrote $ab+bc+ca$ this way: $abc\left(\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\right)$ . We know that $$\frac{3}{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}}\le\frac{a+b+c}{3},$$ so $$\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\ge9\Rightarrow ab+bc+ca\ge9abc$$ I don't know if it helps (or is even true). I think finding the minimum of $\left(1-\left(ab+bc+ca\right)\right)^2+\left(1-abc\right)^2$ has the same procedure. If anyone knows how to find it, it helps a lot (I think).","['contest-math', 'triangle-inequality', 'inequality', 'absolute-value', 'algebra-precalculus']"
