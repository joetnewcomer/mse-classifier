question_id,title,body,tags
2213277,Do $u$ and $v$ real-differentiable and $u$ and $v$ satisfy Cauchy-Riemann equations hold if and only if $f$ is complex-differentiable?,"Let $f:\mathbb{C}\to\mathbb{C}$, and let $u,v:\mathbb{R}^2\to\mathbb{R}$ by $u(x,y)=\Re f(x+iy)$, $v(x,y)=\Im f(x+iy)$. Is it true that $f$ is complex-differentiable at $z_0$ if and only if $u$ and $v$ are differentiable at $(\Re z_0,\Im z_0)$ and $u_x=v_y$ and $u_y=-v_x$ at $(\Re z_0,\Im z_0)$? I think I proved that the answer is yes. However, I also know that $f$ is complex-differentiable at $z_0$ if and only if $u_x=v_y$ and $u_y=-v_x$ and $u$ and $v$ have continuous first partials at $z_0$. (See Churchill and Brown (7th ed.), the theorem in Section 21 and the corollary in Section 48.) This implies that for any $u,v$ satisfying $u_x=v_y$ and $u_y=-v_x$ at a point, $u$ and $v$ are differentiable at that point if and only if $u$ and $v$ have continuous first partials at that point. It seems like this statement is definitely false, though I can't come up with any counterexamples at the moment.
...So what is going on?","['multivariable-calculus', 'complex-analysis', 'real-analysis']"
2213294,How to calculate sum for $k\ge 1\in\mathbb N\quad\sum\limits_{i=1}^\infty\frac1{i(i+1)(i+2)...(i+k)}$,"$$\forall k\ge 1\in\mathbb N\\\displaystyle\sum\limits_{i=1}^\infty\frac1{i(i+1)(i+2)...(i+k)}=?$$ Try$(1)$I tried to apply vieta If it be considered like this ; $$\frac1{i(i+1)..(i+k)}=\dfrac{A_{0}}{i}+\dfrac{A_{1}}{i+1}+...+\dfrac{A_{k}}{i+k}$$ Try$(2)$ $$\frac1{i(i+1)..(i+k)}=\dfrac{(i-1)!}{(i+k)!}=\dfrac{(i-1)!}{(i+k)!}\dfrac{(k+1)!}{(k+1)!}=\dfrac{1}{(k+1)!}\dfrac{1}{\dbinom{i+k}{i-1}}$$ But I couldn't get any usefull equalition.I want to calculate this series but how? By the way, I exactly know that this series is convergence.",['sequences-and-series']
2213321,Trigonometric equation $\cos (5x) = \sin (x)$ - How to find $5x$?,"I want to find the general solution of the following equation for $x \in \mathbb{R}$: $$\cos (5x) = \sin (x)$$ I know it might sound silly, but I don't know how to bring $5x$.",['trigonometry']
2213354,"If $f(a) =f(b) =0,f(x)>0 \forall x\in(a, b) $ and $f(x) +f''(x) >0\forall x\in[a, b] $ then $b-a\geq\pi$.","I need help in the following problem : Assume that the function $f:[a,b]\to \mathbb{R}$ satisfies $f(a)=f(b)=0, \forall x\in (a,b): f(x)>0$, for all $x\in [a,b]$ and $f+f''>0$. Prove that $b-a\geq \pi$ . Here is when I am stuck:  let $h=f+f''$ , $h$ is a positive function in $[a,b]$ and $f$ is a solution of the differential equation $y+y""=h$ By standard method of resolution we can prove that there existe two constante $u,v\in \mathbb{R}$ such that 
$f(x)=u\cos(x)+v\sin(x)+\int_{a}^{x} h(t)\sin(x-t)dt$ We can notice that 
$f(x)+f(x+\pi)=\int_{0}^{\pi} h(u+x)\sin(u)du \geq 0 $ , but I don't see how to conclude that $b-a \geq \pi $ , any idea ? thank you .","['real-analysis', 'calculus']"
2213363,How to calculate this pretty determinant smartly?,"Let $$f(x):=a_1+a_2 \sin(x)+a_3 \sin(x)^2$$
$$g(x):=b_1+b_2 \sin(x)+b_3 \sin(x)^2$$
$$h(x):=c_1+c_2 \sin(x)+c_3 \sin(x)^2$$ then Mathematica shows that the determinant of the matrix $$\left(\begin{matrix} f(x) & g(x)& h(x) \\ f'(x) & g'(x) & h'(x) \\ f''(x) & g''(x) & h''(x) \end{matrix} \right)$$ is just $$2 (-a_3 b_2 c_1 + a_2 b_3 c_1 + a_3 b_1 c_2 - a_1 b_3 c_2 - a_2 b_1 c_3 + 
   a_1 b_2 c_3) \cos(x)^2$$ which is a suprisingly simple result given that the expressions from the chain-rule may become rather cumbersome. I would like to know, is there a smart way to conclude this result without explicitly calculating everything and regrouping terms in order to see the result?","['matrices', 'determinant', 'ordinary-differential-equations', 'linear-algebra', 'analysis']"
2213395,The rotations of bidiagonal matrices,"Let $B$ is upper bidiagonal matrix. Consider the algorithm: $ B_0 = B, C_k = B_{k-1}Q_k, B_k = Z_kC_k $, where $B_k$ is upper bidiagonal, $C_k$ is lower bidiagonal and $Q_k$ and $Z_k$ are unitary. So at each iteration, we change the upper bidiagonal matrix to the lower and vice versa. Prove that matrices $B_k$ and $C_k$ converges to the same diagonal matrix. Really I have no ideas how to prove it. I suppose that matrices $Q_k$ and $Z_k$ can be implemented as a Givens rotations. But maybe there are another variants of transforms $Q_k$ and $Z_k$. Also I noticed that after each iteration the absolute value of $\{B_k\}_{11}$ and $\{C_k\}_{11}$ is growing because the unitary transform saves the second norm. And because of the same reasons the absolute value of $\{B_k\}_{nn}$ and $\{C_k\}_{nn}$ is decreasing. But I can not say anything about the intermediate diagonal elements. Maybe this statements is a part of some algorithm of constructing SVD, but I do not know this algorithm. Thanks for any help of ideas!","['matrices', 'svd', 'convergence-divergence', 'numerical-methods', 'linear-algebra']"
2213398,"How many sets of four consecutive positive integers are there such that the product of the four integers is less than $100,000$?","I know how to get the solution by experimentation. $(16, 17, 18, 19) = 93,024$
is the largest set which produces a product that is less than $100,000$, thus, there are 16 sets of four consecutive positive integers that meet the criteria. What I'm interested in knowing is if there is a neater solution to this problem that does not involve much calculation. Can we use estimation or some nice properties?",['combinatorics']
2213453,Joint distribution of uniform variables,"$$X_{1}\sim \mathcal{Uniform}([0,2])$$
$$X_{2}\sim \mathcal{Uniform}([1,2])$$ $X_{1}$ and $X_{2}$ are independent random variables. My question is whether joint distribution of those variables is:
$$f(x_{1},x_{2})=f_{1}(x_{1})\,f_2(x_{2})=1/2$$ ... for all $(x_1, x_2) \in\ [0,2]{\times}[1,2]$? If answer is incorrect, then how to approach this issue? If answer is correct, then I am thinking wheter exist more rigorous and formal approach to derive this?","['uniform-distribution', 'probability', 'probability-distributions']"
2213459,Help with solving trig identity problem [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question It's been 20 years since I did trig, and this one seems a little tricky. How would I solve 
$$
\tan^2(x) -2\tan(x)=1
$$
with steps?",['trigonometry']
2213518,Show that $\sum_{n=1}^\infty \frac{n^2}{(n+1)!}=e-1$,"Show that:
$$\sum^\infty_{n=1} \frac{n^2}{(n+1)!}=e-1$$ First I will re-define the sum:
$$\sum^\infty_{n=1} \frac{n^2}{(n+1)!} = \sum^\infty_{n=1} \frac{n^2-1+1}{(n+1)!} - \sum^\infty_{n=1}\frac{n-1}{n!} + \sum^\infty_{n=1} \frac{1}{(nm)!}$$ Bow I will define e:
$$e^2 = 1+ \frac{2}{1!} + \frac{x^2}{2!} + ... + \infty$$
$$e' = 1 + \frac{1}{1!} + \frac{1}{2!} + ... + \infty$$
$$(e'-2) = \sum^\infty_{n=1} \frac{1}{(n+1)!}$$ Now I need help.","['exponential-function', 'sequences-and-series']"
2213537,"Finding the limit of $f$ given $f(0)>0, f(x) \leq f'(x).$","Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be a differentiable function, with $f(0) > 0$, so $f(x) \leq f'(x)$. Prove that $$\lim_{x\rightarrow \infty } f(x)=\infty.$$ I tried to use lagrange, but this doesn't lead me to the conclusion.","['real-analysis', 'ordinary-differential-equations']"
2213540,Evaluate $\int x^2e^{x^2} dx$,"Evaluate $\displaystyle\int x^2e^{x^2} dx$ Try($1$) (integral by parts) (unsuccessful) $$\displaystyle\int x^2e^{x^2}dx=x^2\left(\int e^{x^2} dx\right)-2\int x\left(\int e^{x^2} dx\right)dx$$ I don't know how to calculate $\left(\displaystyle\int e^{x^2} dx\right)$, as well. Try($2$) (integral by parts) (unsuccessful) $$\displaystyle\int x^2e^{x^2}dx=e^{x^2}x^3/3-2/3\int x^4e^{x^2}dx $$$$\rightarrow$$$$\int x^4e^{x^2}dx=x^5e^{x^2}/5-2/5\int x^6e^{x^2}dx$$$$\vdots$$ Try($3$) (Integration by substitution) (unsuccessful) $$x=\sqrt t$$$$\displaystyle\int x^2e^{x^2}dx=1/2\int \sqrt t\;e^t\; dt$$
Let's apply ""integral by parts"" $$\int \sqrt t\;e^t\; dt=\sqrt t\;e^t-1/2\int\dfrac{e^t}{\sqrt t}dt$$$$\vdots$$ Try($4$) (Integration by substitution(trigonometric)) (unsuccessful) $$x=\sin u$$
$$\displaystyle\int x^2e^{x^2}dx=\int e^{\sin^2 u}\sin^2 u\cos u du$$","['integration', 'calculus']"
2213548,Quadratic formula on matrices $aX^2 + bX + cI = 0$,"PROBLEM STATEMENT I would like to solve the equation of type: $aX^2 + bX + cI = 0$ for $a,b,c$ ; where $X$ is some square $(N \times N)$ matrix (see footnote 1 below), $I$ is the identity matrix $(N \times N)$, $0$ is the zero matrix $(N \times N)$, and $a,b,c$ are some scalars FOOTNOTES/CAVEATS 1) I understand there are probably some nuances to $X$, such as invertibility or associativity. ATTEMPTED APPROACH It appears that there is some analogy to solving a similar algebra problem where instead of matrices, $X$ is some root to the 2nd degree polynomial and one can simply apply the quadratic formula to get $x$. However, I am stumped on how to proceed after the substitution (see question 2 below). QUESTIONS 1) What area in math should I investigate to understand this type of problem better? What relevant key words should I use in searching literature? 2) If applying the quadratic formula analogy is a valid step, how do I approach the result $X = {RHS}$, where ${RHS}$ is the right-hand side of the equation that is composed of some square roots of $I$, some scalar multiples $aI, bI, cI$, etc.? 3) If there are constraints to $X$, what are they to make this solvable?","['matrices', 'linear-algebra']"
2213582,"What is the definition of ""rotation"" in a general metric space? (Or a Finsler manifold?)","Question: What is the correct definition of ""rotation"" in a general metric space? Is the following correct? Let $(X,d)$  be a metric space. Let $G_x$ be the group of isometries of $X$ which fix the point $x \in X$. Then any isometry in $G_x$ is a ""rotation about $x$"". Also note that, if $X$ is an orientable vector space, I am apathetic or agnostic about whether rotations should be allowed to reverse orientation or if they must preserve orientation. A definition for the full generality of arbitrary metric spaces would be preferred, but one valid only for, say, Finsler manifolds or even just (finite-dimensional) normed spaces would also be appreciated, since it would still answer my previous question . Context: My working assumption right now is that, for a vector space, it is the group of isometries which fix the origin, see my previous question to which this is a follow-up. This question on MathOverflow seems like it might be using this definition. However, I don't understand what is meant by ""isotropic space"" -- it seems more general than isotropic manifold . The transitivity seems related to a theorem in Spivak's Comprehensive Introduction to Differential Geometry which I have mentioned in two previous questions (1) (2) . Also this answer on Math.SE repeats the claim that ""unit balls with respect to other norms are not rotationally invariant"", which, as I pointed out in my previous question is either trivial or insightful depending upon one's definition of ""rotation"", which seems to never be specified in this instance. The answer seems like it might also have something to do with CAT(0) spaces, since CAT(0) spaces apparently satisfy a sort of parallelogram law , and a norm is induced by an inner product if and only if it satisfies the parallelogram law . However, the parallelogram law is equivalent (I think) to the Pythagorean theorem (at least for $L^p$ norms) and the Pythagorean theorem is equivalent to the parallel postulate and a bunch of other conditions, at least for Euclidean space. ( See also this question .) But one of these formulations seems to have more to do with geodesic completeness than any obvious notion of angle, see my previous question . This might be true since CAT(0) spaces have unique geodesics, see this answer . It is often said that an inner product ""induces notions of length and angle"" and, as far as I can tell, at least in simple spaces the notions of rotation and angle are intimately related. (Although for arbitrary metric spaces angles seem to have more to do with equivalence classes of triples of points ( see also Section 3.6.5. here ) and similarity transformations than point-fixing isometries.) At the very least, the part of inner product which corresponds to both its notion of angle and its corresponding orthogonal group (group of rotations?) is the conformal structure it belongs to. However, conformal structures do not seem to be interesting objects of study (see my previous question ) which suggests that they are not actually important in defining a notion of rotation. This answer mentions compact one-parameter groups of ""rotations"" without elaborating. All of these questions by @Asaf Shachar are of interest: (1) (2) (3) (4) (5) . Reading them either taught me or confirmed for me (I don't remember) that the orthogonal group (and thus a notion of rotation?) is only unique up to a scalar multiple for an inner product. TL;DR Context: I don't understand what is ""fundamental"" about the notion of rotation even for the simplest example of rotations: the orthogonal group on Euclidean space. (See also: [1] [2] ) Is it the inner product up to scalar multiple? The parallelogram law? The parallel postulate? Unique geodesics? The Cat(0) inequality? Point-fixing isometries? etc. Thus I am very uncertain of how to generalize the notion of rotation from Euclidean space to arbitrary metric spaces -- the orthogonal group has so much structure, it is hard for me to tell which part of the structure is ""essential"" for codifying the ""notion of rotation"".","['reference-request', 'terminology', 'geometry', 'metric-geometry', 'definition']"
2213600,Understanding why Lagrange Interpolation works,"I'm confused as to why dividing $p(x) = q(x) / q(x_1)$ works. I understand that $q(x)$ is designed such that any $x_i$ for $2 \leq i \leq d+1$ will make $q(x)=0$. I know that $q(x_1)$ will not make $q(x) =0$ because there is no $(x-x_1)$. So, you keep generating a $q(x)$ that is non-zero for $x_i$ for all $2 \leq i \leq d+1$, and then divide by their respective $q(x_i)$ ? How does this give us $p(x)$ ?","['polynomials', 'lagrange-interpolation', 'discrete-mathematics']"
2213603,How can we demonstrate that $\sqrt{\phi}=5^{1/4}\cos\left({1\over 2}\tan^{-1}(2)\right)?$,"$$\text{Let}\ \sqrt{\phi}=5^{1/4}\cos\left({1\over 2}\tan^{-1}(2)\right)\tag1$$
  Where $\phi={\sqrt5+1\over 2}$ An attempt: $$\tan^{-1}(2)={\pi\over 2}-\tan^{-1}{1\over 2}=2\tan^{-1}{1\over \phi}\tag2$$ Then $(1)$ becomes $$\sqrt{\phi}=5^{1/4}\cos\left({\pi\over 4}-{1\over 2}\tan^{-1}{1\over 2}\right)\tag3$$
Apply compound formula
$$\sqrt{\phi}=5^{1/4}\cdot{\sqrt{2}\over 2}\left[\cos\left({1\over 2}\tan^{-1}{1\over 2}\right)+ \sin\left({1\over 2}\tan^{-1}{1\over 2}\right)\right]\tag4$$ It seems to be getting more complicated than before, so how else can we prove $(1)?$","['golden-ratio', 'trigonometry']"
2213630,Circle is a straight line,"I was thinking about regular polygons when I tried this out.
The sum of the internal angles of an $n$ - sided polygon is $\frac{180(n-2)}{n}$. The limit of this function as n approaches infinity is $180$. Since a circle has infinite sides, the internal angles of the circle are $180$ degrees, which is a straight line. Is there a flaw in my reasoning? If so, what is it?",['geometry']
2213634,T/F: If $\limsup x_n = 1$ then $x_n \leq 1$ for sufficiently large $n$,"I was asked this question on a homework and I believe the statement is false. My reasoning is, imagine the sequence $x_n = 1 + \frac{1}{n}$ for $n = 1, 2, 3, ...$, which has $\limsup x_n = 1$ but $x_n \neq 1, \ \forall  n$. Is this logic okay, or am I missing something? I meant $1 + \frac{1}{n}$, my mistake","['real-analysis', 'limsup-and-liminf', 'sequences-and-series']"
2213655,Can we define a function as a cartesian product of *tuples*? Could we make use of the implict indices of the variables when defining the function?,"First, as noted in the comments, this question is on notation and so it might not be the easiest to understand. The crux of my question is whether the notation that I am using makes sense and whether one could use such ideas (e.g., making use of the value of the index of a variable within some tuple) We will say that an n-tuple $(a_0, a_1,...,a_n)$ is the ordered collection that has $a_i$ as its i-th element for all integers i such that $0 \leq i \leq n$ Two n-tuples are equal if each pair of corresponding entries are equal. That is, $(a_0, a_1,...,a_n) = (b_0, b_1,...,a_n)$ if and only if $a_i = b_i$ for all $i \in$ {$0,1,2,...,n$} Also note the following: We will be considering only finite tuples where no tuple contains repeated elements. That is, each element in the tuples we are
  considering is distinct. Now, as for the definition of an n-tuple we could get more technical, but I don't believe it should matter. The main point I want to utilize about tuples is that they have index values (that is, their elements are ordered). And in our case, I want to utilize the fact that the  each element corresponds to a unique index. This holds in our case since no tuple has repeated elements. I will now give some examples of what I would like to do. I am wondering if such things are conventional, and if not, whether they would still be acceptable. Consider the 2 tuples $t = (a, b, c)$ and $r = (a, d, f)$ and the set $S = $ {$ {a,b,c}$ } First: Does it make sense to ask something like ""Is $a \in t$?""  as we do when we ask (for example as exercises in some introduction to set-theory book) ""Is $a \in S$?"" ? Does the symbol  $'\in$'make sense when applied to tuples? I am working on something where I am explicitly working with tuples and I would like to be able to pose statements such as ""if x is an element of tuple t, then..."". But I am not sure if that even makes mathematical sense . To talk of ""elements"" of a tuple, I think intuitively makes sense, but I am not sure if technically the '$\in$' symbol is how one goes about doing it. Remark : And if I am to use '$\forall x \in t$' I mean all values in the tuple $t$ in the same way that $\forall x \in S$ means all values in $S$. Second: Could we have a Cartesian product of the 2 tuples? If using the $\in$ symbol like I did in the first question makes sense, then can we define the Cartesian-product of two tuples as: The Cartesian-Product of two tuples $t$ and $r$ denoted by ($t \times r$) is the set of ordered-pairs $(a,b)$ where $a \in t$ and $b \in r$. That is, (just like the definition w.r.t. sets), we have that ($t \times r) := ${  $(a, b) : a \in t \wedge b \in r$} Is this used at all, if not, would it be acceptable? Example: Using the tuples $t$, $r$ defined earlier, ($t \times r) =$        {$(a,a) ,(a,d) , (a,f) , (b,a) , (b,d) , (b,f) , (c,a) , (c,d) , (c,f) $} Note, this results in a set , not a tuple. Third: First, let us also say that for any tuple $t$ (where the domain consists of only the kinds of tuples we are considering), $I_t(x) $ denotes the value-of-the-index associated with the element x of the tuple t, for all x in t. Ok, now I am also curious about whether the following kind of statement makes sense or not. What I want to do is define a function but when defining this function I want to make use of the implict-indices associated with the values of the tuples we are considering. What I want is to have a tuple itself be the domain . That is, I want to have a function a $f$: $t \rightarrow$ $\mathbb{N}$ So what does this mean? I want the domain to be all the values of the tuple $t$, but I don't just want the values. I also want to make use of the values of the indices . That is, I want those values to in some way be part of the input-argument . So why not just create a set whose elements are all those elements in the tuple $t$? Well, the problem is that there would be no associated index-values with those elements. Only the elements of a tuple have an associated index (in our case, each value has a unique associated index). I want to define $f$ as something like: for each tuple $t$, we define a function $f: t \rightarrow \mathbb{N}$ as being $f(x) = I_t(x) + 2$ ,  $\forall x \in t $. Example: Using the tuple $r$ defined in the beginning, $f(d) = 1 + 2 = 3$ The point is, that I want the value of the function to—in part—be determined by the index that the argument is associated with/resides in, with respect to the tuple that it comes from. My question is whether this is done anywhere in mathematics, and if it isn't, would it be acceptable to do so provided I give the explanation I am giving here (with slightly more rigor)? Fourth: Taking things one step further, I would also like to have a function that goes from the Cartesian-product of 2 tuples $a$ and $b$ to the natural-numbers. That is I would like to have, $g : (a \times b) \rightarrow \mathbb{N}$
And then define $g$ to be something like $g( (x,y) ) = I_a(x) + I_b(y) + 2$ for all $x \in a$ and $b \in b$ Example: Again using the already defined tuples $t$ and $r$, g((c,d)) = 2 + 1 + 2 = 5 The thing about mathematics though is that it is about creation in a lot of ways, as long as you are precise enough. So even if these ideas aren’t really conventional, am I coming off (more-or-less) precise enough so that I could use these ideas? My concern is that maybe having a tuple itself be a domain, for example, is too outside the realm of normalcy. Thank you for taking the time to read it all, I hope it makes sense.","['notation', 'convention', 'logic', 'elementary-set-theory']"
2213659,"Following the definition of Kontsevich and Zagier, is it known if conjecturally $1/\zeta(m)$, with $m\geq 2$ an integer, should be a period?","In [1], Kontsevich and Zagier provide us a definition of a period , (page 3)  and examples of complex numbers being periods and complex numbers that aren't periods (pages 3-5). Question. Let $\zeta(s)$ the Riemann's Zeta function. (Since I believe that it is an unsolved problem and very difficult) I am asking about if you can provide us an heuristic to deduce conjecturally  if $$\frac{1}{\zeta(s)}$$ for integers $s\geq 2$, are periods? Since I don't know if this problem was in the literature, add the reference, if you know it, as an answer. Many thanks. I believe that find such integral representation for $1/\zeta(m)$ with $m\geq 2$ an integer should be very difficult or unknown, then I am asking about an heuristic with the purpose to answer the question. What are you saying? References: [1] Kontsevich and Zagier, Periods , Institut des Hautes Études Scientifiques (2001). [2] A different reference, in spanish, is page 555 of Waldschmidt, Una introducción elemental a valores zeta múltiples , La Gaceta de la RSME, Volumen 17, número 3 (2014).","['reference-request', 'algebraic-number-theory', 'number-theory', 'integration', 'rational-functions']"
2213670,Orthogonal Matrix with Determinant 1 is a Rotation Matrix,"I am confused with how to show that an orthogonal matrix with determinant 1 must always be a rotation matrix. My approach to proving this was to take a general matrix $\begin{bmatrix}a&b \\c&d\end{bmatrix}$ and using the definition of a matrix being orthogonal, work out some restrictions on $a,b,c,d$ such that the matrix must be a rotation matrix. Doing this; I end up with $\begin{bmatrix}a&b \\c&d\end{bmatrix}^T\begin{bmatrix}a&b \\c&d\end{bmatrix}=\begin{bmatrix}a^2+c^2&ab+cd \\ab+cd&b^2+d^2\end{bmatrix}=\begin{bmatrix}1&0 \\0&1\end{bmatrix}$ We also know $ad-bc=1$ from the determinant restriction. I thought we could say since $a^2+c^2=1$ then we could say $a=\cos(\theta)$ and $c=\sin(\theta)$. From here we could chose the second column of the matrix to a vector such that $b=-\cos(\pi/2-\theta)$ and $d=\sin(\pi/2-\theta)$ which gives $b=-\sin(\theta)$ and d=$\cos(\theta)$. This would give $\begin{bmatrix}\cos(\theta)&-\sin(\theta) \\\sin(\theta)&\cos(\theta)\end{bmatrix}$ which is the rotation matrix I need to show. Is this correct and is there a better approach to help me prove that any orthogonal matrix with determinant 1 must be a rotation matrix? EDIT: We can say $a=\cos(\theta)$ and $c=\sin(\theta)$ as we know $a^2+c^2=1$ and so $a$ and $c$ must lie on the unit circle, hence we can parameterise the variables in the matrix as such.","['matrices', 'orthogonal-matrices', 'rotations']"
2213674,Trig identities - stuck solving $\tan^2\theta = -\frac 32 \sec\theta$,"Solve the equation on the interval $0\leq \theta < 2\pi$ $$\tan^2 \theta = -\frac{3}{2}\sec \theta $$ Here are the steps I have so far: Identity: $\tan^2 \theta = \sec^2 \theta -1 $ Substitute: $$\sec^2 \theta -1  = -\frac{3}{2}\sec \theta $$ $$2\sec^2 \theta -2 = {-3}\sec \theta $$ $$2\sec^2 \theta +3\sec \theta - 2 = 0 $$ Is this factoring correct?: $$(2\sec\theta+4)(\sec\theta-1) = 0 $$ $$2\sec\theta+4 =0$$ $$2\sec\theta = -4 $$ $$\sec \theta = -2$$ $$(2\pi/3), (4\pi/3) $$ $$\sec\theta - 1 = 0$$ $$\sec\theta = 1 $$ $\sec\theta=1$ would evaluate to $0$, for some reason that is an invalid answer? (according to my assignment) So is $\ (2\pi/3), (4\pi/3) $ the full answer?",['trigonometry']
2213685,Integrating $y'(x)y''(x)$ with respect to $x$,"I'm having trouble with integrating $y'y''$, because I can think of two different answers that seem correct to me. 1)  $$y''y' = y'' \frac{dy}{dx}$$
$$y''y'dx = y''dy $$
$$\int y''y'dx = y' + C$$ 2) $$y''y' = \frac{d}{dx} (\frac{1}{2} (y')^2)$$
So $$\int y''y'dx = (\frac{1}{2} (y')^2)$$ These answers seem quite different, but I can figure out which is correct (assuming one is) and why the other would be incorrect.","['derivatives', 'integration', 'implicit-differentiation']"
2213691,"Open, closed and dense sets - examples","I am currently learning the basics of functional analysis and wanted to know if I understand correctly the basic concepts. Therefore I would be thankful for verifying my proofs of a couple of simple exercises. Let $X:=C[a,b]$, where $-\infty<a<b<\infty$ and $||u||:=max_{a\leq x \leq b}|u(x)|$. We have to show that: a) $M:=\{ u\in X: u(a)>0  \}$ is an open, not dense subset of $X$. b) $M:= \{ u\in X: u(a) = 1 \}$ is a closed, not dense subset of $X$. c) $M:= \{ u\in X: u(x)=0 \text{ on } [c,d] \}$ is not dense provided $a\leq c < d \leq b$. Ad. a) $M$ is open iff $\forall u \in M$ $\exists \epsilon > 0$ s.t $\epsilon$-neighbourhood $U_{\epsilon}(u)$ ($U_{\epsilon}(u):= \{ v\in X: ||v-u||<\epsilon \}$) of $u$ is contained in $M$. If $||v-u||<\epsilon$, then it implies that
$$
|v(a)-u(a)|<\epsilon,
$$
so if we take $\epsilon>0$ such that $u(a)-\epsilon >0$, then for all $v \in X$ satisfying $||v-u||<\epsilon$ we will have
$$
u(a)-\epsilon<v(a)< u(a) + \epsilon \implies 0<u(a)-\epsilon < v(a),
$$
so $v \in M$ and thus $U_{\epsilon}(u) \subseteq M$. For the density part observe that $\bar{M} = \{ u\in X: u(a)\geq0 \}$ and hence $\bar{M}\neq X$ (because there are functions $v \in X$ such that $v(a)<0$), so M is not dense. Ad. b) Take some sequence $\{ u_{n} \} \subset M$ such that $u_n \to u$ as $n \to \infty$. Then, by the sup norm, $u_n \to u$ uniformly and thus the limiting function $u$ is continuous. Hence we can take the limit and obtain
$$
u_{n}(a)=1 \quad \forall n\in\mathbb{N} \implies u(a) = 1,
$$
so $u\in M$, which is equivalent to the set $M$ being closed. Then, since $M$ is closed we have $M=\bar{M}$ and of course $\bar{M}\neq X$, so $M$ is not dense. Ad. c) Since $M$ is closed we have $M=\bar{M}$ and of course $\bar{M}\neq X$ (since there are continuous functions in $X$ which can take a nonzero values in any given interval $[c,d]$) I would be very thankful for any feedback as I'm still learning the basics and want to know that I understand them well.","['functional-analysis', 'general-topology']"
2213703,How to use open mapping theorem in this case?,"If I have $X,Y$ Banach spaces and $T : X \to Y$ a linear and surjective map then $T$ is a open map. I want to prove that if $(y_n)$ is a bounded sequence on $Y$ then there exists a bounded sequence $(x_n)$ on $X$ such that $T(x_n) = y_n$. I want this because I am trying to prove that if $y_n \to 0$ then there exists a sequence $(x_n)$ in $X$ that converges to $0$ and $T(x_n) = y_n.$ Since $T$ is surjective, clearly there exists $(x_n) \in X$ such that $T(x_n) = y_n$. This is easy. Once $(y_n)$ is bounded it lies on a ball for a radius big enough. So, how can I find a ball in $X$ such that $(x_n)$ lies in a ball? The radius of such ball must be related with the radius of the ball containing $(y_n)$ somehow. How can I do this? It is obviously open map theorem, but how?",['functional-analysis']
2213716,Why cant I row reduce and get the same Eigen values?,"A example is: $$A=
 \begin{bmatrix}
    3 & 2   \\
    1 & 4   
  \end{bmatrix} \leftrightarrow
 -\begin{bmatrix}
    1 & 4   \\
    3 & 2   
  \end{bmatrix} \leftrightarrow
 -\begin{bmatrix}
    1 & 4   \\
    0 & -10   
  \end{bmatrix} = H
$$ The row operations I used are $$(1)   R_1 \leftrightarrow R_2$$ $$(2) -3R_1 + R_2 \to R_2$$ The original matrix $$|A - \lambda I| = (\lambda-2)(\lambda-5) $$ The reduced matrix: $$|H - \lambda I| = -(1-\lambda)(-10-\lambda) $$ Why are they different? For a couple other questions this was working EDIT: A matrix that gave the same eigen value as the one reduced: $$A=
 \begin{bmatrix}
    2 & 0 & 1  \\
    6 & 4 & -3 \\
    2 & 0 & 3  
  \end{bmatrix} \leftrightarrow  \begin{bmatrix}
    1 & 0 & 1  \\
    9 & 4 & -3 \\
    -1 & 0 & 3  
  \end{bmatrix} \leftrightarrow \begin{bmatrix}
    1 & 0 & 0  \\
    9 & 4 & -12 \\
    -1 & 0 & 4 
  \end{bmatrix} = H
$$ Column operations were $$-C_3 + C_1 \to C_1$$ $$-C_1 + C_3 \to C_3$$ The eigen values are the same for A and H","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2213735,System of differential equations (3x3),"I am stuck in solving the following system:
$$y_1'=-2y_1+y_2-2y_3$$
$$y_2'=y_1-2y_2+2y_3$$
$$y_3'=3y_1-3y_2+5y_3$$ EDIT: My book gives the following solutions:
$$y_1(x)=-C_1e^{3x}+(C_2-2C_3)e^{-x}$$
$$y_2(x)=C_1e^{3x}+C_2e^{-x}$$
$$y_3(x)=3C_1e^{3x}+C_3e^{-x}$$ Here is my attempt: We differentiate the first equation:
$$y_1''=-2y_1'+y_2'-2y_3'$$$$=-y_1+2y_2-4y_3$$ Now we have a $2x2$ system for $y_2,y_3$, using the first equation from the system and the last equation for $y_1''$: $$y_1'=-2y_1+y_2-2y_3$$
$$y_1''=-y_1+2y_2-4y_3$$ Multiplying the first equation by $-2$ and adding these two equations gives:
$$y_1''-2y_1'-3y_1=0$$ This gives: $$\lambda_1=3,\lambda_2=-1\Rightarrow y_1(x)=C_1e^{3x}+C_2e^{-x}$$ This doesn't match with the above solution. Is there a mistake? I will show my full attempt with this (maybe wrong) first solution. Now, for finding $y_2,y_3$ we have a new $2x2$ system such that $y_1$ is known:
$$y_2'=y_1-2y_2+2y_3$$
$$y_3'=3y_1-3y_2+5y_3$$ Again, the same procedure. We differentiate the first equation:
$$y_2''=y_1'-2y_2'+2y_3'$$
$$=y_1'-2y_1+4y_2-4y_3$$ Now, for finding $y_3$ we have a new $2x2$ system such that $y_1,y_2$ are known: $$y_2'=y_1-2y_2+2y_3$$
$$y_2''=y_1'-2y_1+4y_2-4y_3$$ By adding these two, we get:
$$y_3=-\frac{1}{2}y_2''-\frac{1}{2}y_2'+\frac{1}{2}y_1'-\frac{1}{2}y_1+y_2$$ Now we differentiate the equation
$$y_2''=y_1'-2y_1+4y_2-4y_3$$
to obtain $$y_2'''=y_1''-2y_1'+4y_2'-4y_3'$$ By plugin previously evaluated $y_3$ we get:
$$y_2'''+4y_2''+4y_2'=y_1''+2y_1'$$ We already have $y_1$ (note that this may be wrong), so:
$$y_1'=3C_1e^{3x}-C_2e^{-x}$$
$$y_1''=9C_1e^{3x}+C_2e^{-x}$$ We have:
$$y_2'''+4y_2''+4y_2'=15C_1e^{3x}-C_2e^{-x}$$ Now we will use variation of parameters: $$\lambda^3+4\lambda^2+4\lambda=0$$
$$\lambda_1=0,\lambda_2=\lambda_3=-2$$ This gives multiple roots so it might again be a mistake. I will stop here. Could someone check this? Where are mistakes so far? EDIT: Could someone show the approach with linear algebra, eigenvalues/eigenvectors?","['ordinary-differential-equations', 'systems-of-equations']"
2213737,Is this (deceptive) pattern just a coincidence?,"I've come across this nice pattern start:
$$
\begin{align}
1^2-2^2+3^2 &= 6\\
1^4-2^4+3^4 &= 66\\
1^6-2^6+3^6 &= 666\\
\end{align}
$$
I read somewhere the third line, I remembered that I already knew the first, I immediately tested the second and I thought ""eureka, there is a pattern!"". I was already hoping to be able to find an explanation when I realized that the next would-be term fails to follow the ""rule"" ($1^8-2^8+3^8=6306$). Now, lacking a pattern, I shouldn't need a proof. Nevertheless I find it hard to believe that those three lines can be mere coincidence. I'm wondering whether some sort of ""reason"" can anyway be found (for them being what they are, and for the next ones not following). Thank you!","['number-theory', 'recreational-mathematics', 'pattern-recognition', 'elementary-number-theory']"
2213754,Universal bundle over $\mathbb P^{n}$ and direct sum of bundles,"Let $T(\mathbb{R}P^{n})$ be a tangent bundle to a $n$-dimensional projective space and let also denote $\gamma_{1}$ to be a trivial bundle of rank $1$. Moreover, let $E \rightarrow \mathbb{R}P^{n}$ be a universal (tautological) bundle. I would like to prove that $$T(\mathbb{R} P^{n}) \oplus \gamma_{1} = E \oplus E \oplus \ldots E = \bigoplus_{i=1}^{n} E$$ On the one hand, it may looks as if it follows from the existence of the Euler exact sequence of sheaves, written in the following way:
$$ 0 \rightarrow \mathcal{O}_{\mathbb{P}^{n}} \rightarrow \mathcal{O}(1)^{\oplus (n+1)} \rightarrow \mathcal{T}_{\mathbb{P}^{n}} \rightarrow 0$$
One can show that this sequence splits (though not canonically???). Are there any ways to derive the statement from the propositon above or are there any easier ways to obtain the desired result (maybe, by pulling back the $T(\mathbb{R}P^{n})$ via $f: S^{n} \rightarrow RP^{n}$?)","['tangent-bundle', 'fiber-bundles', 'vector-bundles', 'differential-geometry']"
2213755,"If $f(x)\leq g(x)$ for all $x\in [a,b]$ then $\displaystyle \int_a^b f(x)dx\leq \int_a^b g(x)dx$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I'm having trouble constructing a proof for the following theorem: Let $f$ and $g$ be Riemann integrable functions on $[a,b]$ . Show that:  if $f(x)\leq g(x)$ for all $x\in [a,b]$ then: $$\int_a^b f(x)dx\leq \int_a^b g(x)dx$$ Any suggestions on how to start, or an answer would be appreciated!","['real-analysis', 'integration', 'riemann-integration', 'solution-verification']"
2213759,Is a positive linear functional on $L^p$ necessarily bounded?,"I was reading a source that suggested that if $X$ is a measure space (or perhaps just the interval with Lebesgue measure), and if $1 \leq p < \infty$, then any non-negative linear  functional $T$ on $L^p(X)$ is continuous. ($f \geq 0$ a.e. implies $T(f) \geq 0$ as real numbers.) Is this true, or even obviously true? I read here that positive linear functionals on $C^*$ algebras are continuous, but $L^p$ spaces don't have this structure in general... I also  learned that it is true when $p = \infty$ and $X$ is a countable set with counting measure: Any positive linear functional $\phi$ on $\ell^\infty$ is a bounded linear operator and has $\|\phi \| = \phi((1,1,...)) $ ... however, I don't think the same technique carries over. There is also this question, which is somewhat weaker and more general: Positive linear functional on an involutive Banach algebra I also found some discussion  here ( question (2) ): application of positive linear functionl","['functional-analysis', 'lp-spaces', 'lebesgue-measure']"
2213760,complex conjugate of trace of matrices,Is $[Tr(AB)]^{*} = Tr(AB)^{\dagger} = Tr(B^{\dagger}A^{\dagger})$? The second equal sign is trivial but the first equal sign is what I am puzzled about. $*$ means the complex conjugate of a number. $\dagger$ means the complex conjugate of a matrix.,"['matrices', 'linear-algebra']"
2213778,Questions about the proof of the Sturm oscillation theorem,"I'm trying to understand the proof of the Sturm oscillation theorem and I hit the roadblock. Theorem: Let $E_0<E_1<\dots$ be the eigenvalues of $H=-\frac{d^2}{dx^2}+V(x)$ on $L^2(0,a)$ with boundary conditions $u(0)=u(a)=0$. Then $u(x,E_n)$ has exactly $n$ zeros in $(0,a)$. Part of the proof: Suppose $u_n$ has $m$ zeros $x_1<\dots<x_m$ in $(0,a)$. Let $v_0,\dots,v_m$ be the function $u_n$ restricted successively to $(0,x_1),(x_1,x_2),...,(x_m,a)$. The $v$'s are continuous and piecewise $C^1$ with $v_l(0)=v_l(a)=0$. Thus they lie in the quadratic form domain of $H$ and 
$$
<v_j,Hv_k>=\int_0^a v'_j v'_k + \int_0^a Vv_jv_k=\delta_{jk}E\int_0^a v_j^2dx
$$
since if $j=k$, we can integrate by parts and use $-u''+Vu=Eu$.
It follows that for any $v$ in the span of $v_j$'s, $<v,Hv>=E\|v\|^2$, so by the variational principle, $H$ has at least $m+1$ eigenvalues in $(-\infty, E_n]$, that is, $n+1\geq m+1$ Questions: Why is it possible to express $<v_j,Hv_k>$ as in the formula above and how the expression is integrated by parts? What is the variational principle mentioned above that allows to determine the number of eigenvalues in the interval $(-\infty, E_n]$?","['functional-analysis', 'sturm-liouville', 'operator-theory']"
2213783,"If the scheme $X$ is locally factorial, can it be covered by affine opens of the form $\operatorname{Spec} A$ for $A$ a UFD?","More precisely, if $X$ is a scheme such that for all $x\in X$, the ring $\mathscr{O}_{X,x}$ is a UFD, then is it true that we can cover $X$ by affine opens $U = \operatorname{Spec}A$ such that $A$ is a UFD? Clearly the converse holds true; if we are given an affine scheme of the form $X=\operatorname{Spec}A$ for $A$ a UFD, then since the property of being a UFD is preserved by localization, $X$ is locally factorial. However, it is emphatically not true that given a locally factorial scheme $X$, any affine open $\operatorname{Spec}A$ has $A$ a UFD – for a counterexample, we need only consider $X=\operatorname{Spec}A$ where $A$ is a Dedekind domain with nontrivial ideal class group. Therefore, my question reduces to the following problem in commutative algebra. Given a ring $A$ such that $A_\mathfrak{p}$ is a UFD for all $\mathfrak{p}\in\operatorname{Spec}A$, then for all $\mathfrak{p}\in\operatorname{Spec}A$, can we find some element $f\in A\setminus\mathfrak{p}$ such that $A_f$ is a UFD? I don't see any clear answer to this question. My hunch is that it's not true, however.","['algebraic-geometry', 'commutative-algebra']"
2213829,"If $f'(x):\mathbb R^n \to \mathbb R^n$ is a isometry, then $f(x)=T(x)+a$.","Hi folks, I'm trying to solve this one problem: Let $f:\mathbb R^n \to \mathbb R^n$ $\in \mathrm C^1$ such that for all $x \in \mathbb R^n$, $f'(x):\mathbb R^n \to \mathbb R^n$ is a isometry (i.e. $||f'(x)\cdot v||=||v||$) with respect to the Euclidean norm. Show that there is a Linear transformation $T:\mathbb R^n \to \mathbb R^n$ and a vector $a\in\mathbb R^n$ such that $$f(x)=T(x)+a, \forall x\in \mathbb R^n$$ I don't know if this information will be useful, but I've already showed that $f$ is also a isometry (i.e. $||f(x)-f(y)||=||x-y||$)","['isometry', 'real-analysis', 'ordinary-differential-equations']"
2213882,"Find the minimum value of $f$ on $\{(a,b,c)\in \mathbb{R^3}\mid a+b+c=1\}$ where $f(a,b,c)=\int_{0}^1 (a+bx+cx^2)^2dx$.","Find the minimum value of $f$ on $\{(a,b,c)\in \mathbb{R^3}\mid a+b+c=1\}$ where $f(a,b,c)=\int_0^1 (a+bx+cx^2)^2 \, dx$. What I did is 
$$
\int_0^1 (a+bx+cx^2)^2 \, dx=a^2+a \left( b+\frac{2c}{3} \right) + \frac{b^2}{3} + \frac{bc}{2}+\frac{c^2}{5}.
$$
Then we consider the function $F(a,b,c)=a^2+a(b+\frac{2c}{3})+\frac{b^2}{3} + \frac{bc}{2} + \frac{c^2}{5}-\lambda(a+b+c-1)$. Thus, we can consider $\nabla F=0$ with $a+b+c=1$ and try to find $a,b$ and $c$. However, since the function and the constraint seem to be related, I am curious whether there is some other way to see the minimum without computing Lagrange multiplier. Thank you.","['multivariable-calculus', 'real-analysis', 'optimization']"
2213890,Inconsistencies in the definition of derivative of a polynomial over a field,"A problem I came across defines a particular differentiation operator $D$ over the set of polynomials $\{P\}$ over a field $F$ with ""the normal formula; that is $D(\sum_{i=0}^n a_nx^i) = \sum_{i=1}^n na_nx^{i-1}$."" However, there seem to be some problems with this definition. First, we cannot assume that $F$ contains the natural numbers as a subset. For example, what is $D(x^2)$ if $F = \mathbb{Z_2}$? Furthermore, the derivative may not be well-defined, depending on the definition of equality for polynomials. For example, if $F = \mathbb{Z_3}$, then $2x^2 + x = 0$ for $x \in \{0, 1, 2\}$, but $D(2x^2 + x) = 2(2)x+1 = x + 1 \neq 0 = D(0)$. The problem then defines the derivative of rational functions $P/Q$ with the standard quotient rule formula $D(P/Q) = \frac{P'Q - PQ'}{Q^2}$ and then asks to prove that that definition is well-defined, but I can't do that without knowing that the derivative of polynomials exists and is well-defined. Can anyone shed some light on how to interpret the definition given, or what restrictions need to be made to make it work?","['abstract-algebra', 'differential-algebra', 'analysis']"
2213916,About Terence Tao's blog-notes on complex analysis,"I'm planning to have a first-read in complex analysis before I study advanced texts like Stein-Shakarchi or Remmert . So I was looking for some basic notes or book geared towards setting strong foundations in the concepts. Terence Tao 's complex analysis notes are posted in his blog (Note that the order of the notes are newest-first, i.e. the first post is at the bottom of the page and the last is on the top). Here's the link: https://terrytao.wordpress.com/category/teaching/246a-complex-analysis/ What are the pre-requisites of these sets of notes? Thanks in advance.","['complex-analysis', 'reference-request']"
2213917,Find the derivative of integral $f(x)/(x^2(x-5)^7)$ when $f(x)$ is a quadratic function.,"This question is quite tricky. It's for my Calculus 2 assignment and I can't seem to figure out how to integrate this function in order to get its derivative. I tried partial fractions, u-sub with x-5 and x^2, but nothing seems to work. All I conluded is that the quadratic function y-coordinate is -4. Can someone help me with this? Regards,
You Xiao Ruan.","['derivatives', 'partial-fractions', 'integration', 'calculus']"
2213921,Proof of $\frac{1-r^2}{1-2r \cos \theta + r^2}=1+2\sum_{k=1}^{\infty} r^k \cos k\theta$ for $0 < r < 1$,"How to prove the following: For $0 < r < 1,$
$$\frac{1-r^2}{1-2r \cos \theta + r^2}=1+2\sum_{k=1}^{\infty} r^k \cos k\theta.$$ I started with writing the left hand side as 
$$\frac{1-r^2}{1-2r \cos \theta + r^2}=\textrm{Re} \left( \frac{1+r e^{i \theta}}{1 - r e^{i \theta}} \right),$$
but wasn't successful. Any help is much appreciated.","['complex-analysis', 'analysis']"
2213928,Using the quotient rule to find $f'$ where $f(x) = \dfrac{6x}{5 \ln(x)}$,Below is a problem and the process by which the first derivative is found: I used the quotient rule but did not get the same result. Can someone explain this to me? Here is what I got after using the quotient rule:,"['derivatives', 'calculus']"
2213938,Step on Hopf-Rinow via regularity.,"On a step of proving (geodesics defined for all time)=>(for any $p,q$ there exists a minimizing geodesic connecting both), besides the traditional approach of taking smooth curves $c_n$ such that $L(c_n) \to d(p,q)$, Sakai mentions that one can use Arzela-Ascoli (changing the assumption of geodesics converging for all time to bounded closed balls being compact) to show that there exists a subsequence of those curves converging to a continuous path $c$. By continuity, we have that this path is length-minimizing. Therefore, it is a geodesic . However, I feel that the result that a continuous length-minimizing path must be a geodesic via the series of exercises and lemmata which are mentioned is very technical and kind of goes against the purpose of the argument of being cleaner. My question is: is there a way to make a shortcut here via some regularity theorem? For example, maybe arrive somehow at the fact that the limit is a weak solution to the geodesic equation and therefore must be smooth? (I don't know any references which approch the subject in this way*, so this would be nice too, if possible). *Except Klingenberg's , but his intentions are others.","['riemannian-geometry', 'ordinary-differential-equations', 'differential-geometry', 'differential-topology']"
2213960,Optimising $x_1x_2+x_2x_3+\cdots+x_nx_1$ given certain constraints,"To seek the maximum value of $S=x_1x_2+x_2x_3+\cdots+x_nx_1$ on this domain: $x_1+x_2+\cdots+x_n=0$ and 
$x_1^2+x_2^2+\cdots+x_n^2=1$. I have made some trivial observations: 1) $S\in[-1,1)$ by the rearrangement inequality. 2) We can make $S$ arbitrarily close to $1$ by increasing $n$. 3) An equivalent problem is to minimise $(x_1-x_2)^2+\cdots+(x_n-x_1)^2$. But does the maximum have a meaningful closed form  for each $n$?","['algebra-precalculus', 'contest-math', 'optimization']"
2213968,Relationship between Frechet derivative and this one,"My book presents this differenciability definition and says that it's due to Frechet and Stols: Given $f:U\to \mathbb{R}$, with $u\subset \mathbb{R}^n$, let $a\in U$.
  We say that our functions is differentiable at point $a$ when there
  are constants $A_1,\cdots, A_n$ such that, for every vector $v =
 (\alpha_1,\cdots,\alpha_n)\in\mathbb{R}^n$, with $a+v\in U$, we have: $$f(a+v) = f(a) + A_1\alpha_1 + \cdots + A_n\cdot \alpha_n + r(v)$$ when $\lim_{v\to 0}\frac{r(v)}{|v|} = 0$ However, in this question , the derivative is different, it's a limit. Which one is the right one? If they're different, then what's the explanation for this one in my book? I liked the explanation given in that question.","['derivatives', 'real-analysis', 'calculus']"
2214010,Why does the complex gain include the initial amplitude?,"While solving a differential equation like $(D^2+4)x=A\cos\omega t$, for example, the complex gain is said to be $\dfrac{1}{4-\omega^2}A$. Is there any deeper reason why it is defined this way, as opposed to simply $\dfrac{1}{4-\omega^2}$, which is the actual ratio of the solution $x(t)$ to the input signal $A\cos\omega t$? Or is the input signal itself defined as $\cos\omega t$, rather than as $A\cos\omega t$? If so, why?","['signal-processing', 'ordinary-differential-equations', 'calculus']"
2214029,Axiomatic Dimension Theory,"While reading Eisenbud's book in introductory commutative algebra, at the start of the chapter on dimension theory, he introduces several axioms for dimension theory, and heuristics as to why they are reasonable axioms. These axioms are: D1) Dimension is a local property in the sense that the dimension of a ring should be the supermum of the dimensions of the localizations of $R$ at prime ideals, and that completing this localization should preserve dimension. D2) That nilpotents should not effect dimension in the sense that the dimension of the ring before and after quotienting by a nilpotent ideal should be the same. D3) Dimension is preserved by maps with finite fibers. D4) If $k$ is a field, then the dimension of the formal power series ring over $k$ in $r$ variables should have dimension $r$. Eisenbud explains that these 4 axioms characterize the Krull dimension in the case of Noetherian rings. He then goes on to give several other notions of dimension, and explain when they coincide and when they do not, as well as provide geometric intuition for them. This is all well and good, and I find the geometric intuition helpful. On the other hand, the many different theories are a lot to swallow all at once, and there are many notions that follow. I think it would help me to have an overarching system that I can construct these notions within, an axiomatized treatment of dimension theory in commutative algebra which I could read in parallel, since Eisenbud does a pretty good job giving me the intuition I need to understand what's going on. This would be especially helpful because I imagine that the situation is not unlike what happens in homology theory in topology. There is a class of spaces you can study with some homology theory, and when you work outside that kind of space or theory, some of the axioms needs to be weakened, replaced or just dropped, leading to exotic theories. For me, having this spelled out helps me to stay mentally organized, and keeps me from confusing notions and theories. As such, a treatment of dimension theory in commutative algebra which does this would be absolutely wonderful. Part of me is suspicious that a good chunk of what I'm looking for belongs to algebraic geometry - the notion of dimensions being tied up intrinsically with the varieties these objects correspond to. I've seen some introductory algebraic geometry in Shafarevich, so I don't mind looking around at some other references, but I'm trying to stay away from behemoths like Hartshorne until I understand some of these notions better. Either way, for this reason I've included the AG tag.","['reference-request', 'ring-theory', 'algebraic-geometry', 'commutative-algebra']"
2214044,Residue of $f'/f$ is equal to $m$.,"So I know that if $f$ is analytic and has a zero of order $m$ at its center, then $f'$ has a zero of order of $m-1$, which can be easily proved. However, I'm not sure, which is stated in the question, how this is related to residues, and how to prove that the residue of $f'/f$ is equal to $m$ if $f$ is analytic and has a zero of order $m$.  Any help would be much appreciated, thank you. If $f$ is analytic in $|z - z_0| < R$ and has a zero of order $m$ at $z_0$, show that $$Res\left( \frac{f'}{f} ; z_0 \right) = m.$$","['complex-analysis', 'complex-numbers']"
2214053,Placing $4$ plastic and $4$ glass rooks on a chessboard,"How many ways can we place $8$ rooks on an $8\times 8$ chessboard, if $4$ are plastic and $4$ are glass? Assuming they don't attack their own type (can share rows and/or columns). I'm also curious, if they couldn't share rows/columns with their own type, would this be the same as $8$ identical rooks? Which is $8! = 40,320$? I assume not, but why?","['combinatorics', 'problem-solving', 'discrete-mathematics']"
2214058,Compute $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x^2+(x-y)^2+y^2)}dxdy$,"Compute $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x^2+(x-y)^2+y^2)}dxdy$. I tried to do this by using polar coordinate. 
Let $x=r\cos t,\ y=r\sin t$, and then $$
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x^2+(x-y)^2+y^2)}dxdy=\int_{0}^{2\pi}\int_{0}^{\infty}e^{-2r^2}e^{2\cos t\sin t}rdrdt=\int_{0}^{2\pi}e^{\sin(2t)}\int_{0}^{\infty}e^{-2r^2}rdrdt=\frac{1}{4}\int_{0}^{2\pi}e^{\sin (2t)}dt.
$$
But, I have no idea how to compute $\int_{0}^{2\pi}e^{\sin (2t)}dt$. Please give me some hint or suggestion. Thanks.","['multivariable-calculus', 'real-analysis', 'definite-integrals']"
2214071,relations and functions discrete mathematics,"let $B=\{1,2,3,4\}$. let $\mathscr T$ be the set of all functions from $B$ to $B$. let $\mathbb R$ be the following relation: for all $k,h \in \mathscr T$, $k\mathbb Rh $ if and only if $k(m) \le h(m)$ for some $m \in B$. 1) is $\mathbb R $ reflexive? symmetric? transitive? prove it. I think it is transitive but I don't how to prove it. Also, are my proofs for reflexive and symmetric correct? I'm kind of confused because it says for some m in $B$. does that mean that I don't have to prove it for all $m$? is it is enough to do it for one m like I did for reflexive proof? Thank you","['relations', 'discrete-mathematics']"
2214092,"$a+b+c = 13$; if $b/a=c/b$, find the maximum and minimum values of $a$ and the corresponding $b$ and $c$","Question : The sum of $3$ integers $a,b$ and $c$ is $13$. If $\dfrac{b}{a}=\dfrac{c}{b}$, find the maximum and minimum values of $a$ and the corresponding $b$ and $c$. To tackle this problem I let $x=\dfrac{b}{a}=\dfrac{c}{b}$ because I wanted to create a quadratic equation in order to use the discriminant theorem. 
From the equation above I can deduce that $b=ax$ and $c=ax^2$.
Because $a+b+c=13$. Therefore; $$a+ax+ax^2=13$$
$$\implies 1+x+x^2-\frac{13}{a} = 0 $$  (where $a \ne 0$, $b \ne 0$, $c \ne 0$) I can only work up to here. I do not know how to use the discriminant theorem to work out the maximum and minimum of $a$, $b$ and $c$.","['functions', 'quadratics']"
2214093,Smoothness of the squared distance function on a Riemannian manifold,"Let $\left(M, g\right)$ be a complete Riemannian manifold. Let us fix a point $p \in M$ and consider the squared distance function
$$
r(x) := \operatorname{dist}(x, p)^2.
$$
It is well-known that $r$ is a smooth function on $M\setminus Cut(p)$, where $Cut(p)$ is the cut locus of $p$, see here and here . So it may happen that $r$ is singular on some points in $Cut(p)$. Now, embed $M$ into the Euclidean space isometricly, then the squared distance function $r$ is the restriction of the Euclidean squared distance function to the submanifold $M$. In particular, $r$ is smooth on $M$. In the book Morse theory by J.Milnor, this distance function is used as a Morse function. I found it very confusing to have these two seemingly contradict statements. Did I miss something? Are there any good examples?","['riemannian-geometry', 'differential-geometry']"
2214129,How can one demonstrate that $\lim_{n\to \infty} \sum_{k=1}^{n}{2\over 2k-1}-\ln{4n}=\gamma$ is correct?,Let: $$\lim_{n\to \infty} \sum_{k=1}^{n}{2\over 2k-1}-\ln{4n}=\gamma\tag1$$ Where $\gamma$ is Euler-Mascheroni Constant How can one demonstrate that $(1)$ is correct? Normally $\gamma$ it is defined  by $$\lim_{n\to  \infty}\sum_{k=1}^{n}{1\over k}-\ln{n}=\gamma \tag2$$ Numerically check-out that $(1)$ converges faster than $(2)$,"['sequences-and-series', 'euler-mascheroni-constant', 'limits']"
2214137,How many positive integer solutions does the equation $a+b+c=100$ have if we require $a<b<c$?,How many positive integer solutions does the equation $a+b+c=100$ have if we require $a<b<c$? I know how to solve the problem if it was just $a+b+c=100$ but the fact it has the restriction $a<b<c$ is throwing me off. How would I solve this?,"['permutations', 'combinatorics', 'combinations']"
2214200,"If two different numbers are taken from the set {0,1,2,3, ......, 10} ...","If two different numbers are taken from the set {0,1,2,3, ......, 10} then what is the probability that their sum as well as absolute difference are both multiples of 4 Here is my work out The sample space here is equal to 55. Now to me the possible combinations are {0,4},{0,8},{2,6},{2,10},{4,8},{6,10} so to me the answer is 6/55",['probability']
2214212,Find the double integral limit.,"I want to solve this problem by using $\varepsilon-\delta $ definition of a limit, but failed. How to compute this limit? $f\in C\left( \left[ 0,1 \right] \times \left[ 0,1 \right] \right) $ $$\mathop {\lim }\limits_{n \to \infty } {\left( {\frac{{\left( {2n + 1} \right)!}}{{{{\left( {n!} \right)}^2}}}} \right)^2}\int_0^1 {\int_0^1 {{{\left[ {xy\left( {1 - x} \right)\left( {1 - y} \right)} \right]}^n}f\left( {x,y} \right)} } {\text{d}}x{\text{d}}y$$ This problem appears in the ""15th Annual Vojtěch Jarník International Mathematical Competition""
on 6th April 2005 as the third problem from Category ll. A solution is provided in the following pdf . Are there also alternatives ways to find the limit?","['contest-math', 'integration', 'calculus', 'limits']"
2214217,SU(n) is a manifold via charts,"I'm trying to construct an atlas the painful way for $SU(n)$, using charts. According to Wiki, SU(n) is a real manifold of real dimension $n^2-1$; I can believe this because you can perform a Cayley Transformation for any $U \in SU(n)$ such that the map $C:M_n(\mathbb{C})^{*}\longrightarrow M_n(\mathbb{C})$ $$ C : U \longmapsto C(U) = (1_n - U)(1_n+U)^{-1} 
$$ Which is well defined because any unimodular unitary matrices are non exceptional (that is that have $\det(1_n+U)\neq 0$). Thus the Cayley transformation defines a homeomorphism, given that you can use the Lie algebra $$\mathfrak{su}(n) = \{X \in GL_n(\mathbb{C}): X^{\dagger}+X=0,\; Tr(X)=0\}
$$ to explicitly compute the dimension. But what I want to try is to find explicit charts for these guys. This is what I've tried so far (restricting to $SU(3)$ to see the machinery involved); given that $$SU(3) = \left\{\;\;\left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right) \; \left| \right. \left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right)^{\dagger}\left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right)=\left(\begin{smallmatrix}1&0&0\\ 0 & 1 & 0\\ 0 & 0 & 1\end{smallmatrix}\right) \; \& \;|ei-hf|^2+|gf-di|^2+|dh-ge|^2 +|dh-ge|^2=1    \;\;\right\}
$$ Where you get a whole bunch of conditions from the unitary conditions, and thus a whole set of equivalent conditions for the determinant. I got these by setting the inverse equal to the hermitian conjugate: Unitary Conditions \begin{eqnarray}
a=\overline{ei-hf}\\
b=\overline{gf-di}\\
c=\overline{dh-ge}\\
d=\overline{ch-bi}\\
e=\overline{ai-gc}\\
f=\overline{bg-ah}\\
g=\overline{bf-ce}\\
h=\overline{dc-af}\\
i=\overline{ae-bd}
\end{eqnarray} Determinant Examples \begin{eqnarray}
a(ei-hf)-b(di-gf)+c(dh-ge)=1\\
|ei-hf|^2+|gf-di|^2+|dh-ge|^2=1\\
|a|^2+|b|^2+|c|^2=1 \;\;\; \text{etc.}
\end{eqnarray} Now in trying to construct a chart, going by the fact $a,\ldots,i \in \mathbb{C}$, there are 18 real degrees of freedom/9 complex. Since $\mathbb{C}^4 \cong \mathbb{R}^8$, I think it may be easier to work with $\mathbb{C}^4$, since you can always express the matrix entries explicitly using four entries. And then this is where I think it best to write out a chart, say for $U_a :=\{ei-hf\neq 0 \Leftrightarrow a\neq 0\}$, which is open, as 
$$\phi_a : U_a \longrightarrow \phi_a(U_a) \subseteq GL_2(\mathbb{C}) \subseteq \mathbb{C}^4 
$$ $$\phi_a (\left(\begin{smallmatrix}a&b& c\\ d & e & f\\ g & h & i\end{smallmatrix}\right)) = (e,f,h,i) \;\;\;\text{equivalently} \left(\begin{smallmatrix}e & f\\h& i\end{smallmatrix}\right)
$$ Then, trying to invert it, things go a bit awry, and when I try to recalculate the explicit inverse, my construction seems to fail, since everytime I try to include the unitary condition and determinant condition in general, things go funny. Any ideas would be really appreciated!","['matrices', 'representation-theory', 'differential-geometry', 'lie-algebras', 'lie-groups']"
2214263,Derivative of $ f(x) = \sum\limits_{n=1}^{\infty} \frac{1}{n^2}e^{-n^2x^2}$ at 0,"Let $ f(x) = \sum_{n=1}^{\infty} \frac{1}{n^2}e^{-n^2x^2}$ I know that the serie of the derivatives $g(x) = \sum_{n=1}^{\infty} -2xe^{-n^2x^2}$ is uniformely convergent on every set $]-\infty;-b] \cup [b;+\infty[ , b>0 $. This implies that $f$ is differentiable on $\mathbb{R} \setminus \{0\}$. My question is, is it differentiable on $0$ ? I tried to apply L'hopistal rule but I cannot figure out if $g(x)$ has a limit when $x \to 0^+$ or $x \to 0^{-}$.
I also tried to compute the derivative at $0$ but I don't see how to carry the computations. Many thx for any help","['derivatives', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
2214287,Trig - Difference formulas to solve $\sqrt3\sin \theta- \cos\theta = 1$,"My exam review states that I need to utilize the difference formula for sine to solve the equation on the interval $0 \leq \theta < 2\pi $ $$\sqrt3\sin \theta- \cos\theta = 1$$ I know that: $\sin \frac\pi3 = \frac{\sqrt3}{2}$
and $\cos\frac\pi3 = \frac12 $, so I divide each term by 2 and rewrite the equation: $$\frac{\sqrt3\sin\theta}{2} - \frac{\cos\theta}{2} = \frac12$$ from here, I apply the difference formula for sine: $$\sin(\alpha-\beta) = (\sin\alpha \cdot \cos\beta)  - (\cos\alpha  \cdot \sin\beta)$$ (this is the step that I believe I'm doing incorrectly. I've tried plugging in the corresponding numerical values for sin/cos into the equation instead, but I was still unable to go past this step.) $$\sin(\alpha-\beta) = \left(\frac {\pi}{3} \cdot \frac{\pi}{3}\right)  - \left(\frac{\pi}{6} \cdot \frac{\pi}{6}\right) = \frac12$$",['trigonometry']
2214289,Weak Convergence of Composition in Sobolev Space,"Let $\Omega\subset \mathbb R^d$ be a bounded domain with Lipschitz boundary, $1<p<\infty$, and suppose $h\colon \mathbb R\to \mathbb R$ is bounded and Lipschitz, $u_n \rightharpoonup u$ in $W^{1,p}(\Omega)$ ($\rightharpoonup$ denoting the weak convergence). Is it true that $h(u_n) \rightharpoonup h(u)$ in $W^{1,p}(\Omega)$? Are there other, similar convergence principles which  generalize this finding? What, if $p=1$? Note, that $h$ is differentiable almost everywhere and that the generalized chain rule states that for $\Omega$ as given, $h$ Lipschitz and $u\in W^{1,p}(\Omega)$ one has $h(u) = h\circ u \in W^{1,p}(\Omega)$ and
$$D_i(h(u)) = h_B(u)D_iu,$$ 
where $h_B\colon \mathbb R\to\mathbb R$ is a Borel-measurable function such that $f_B = f'$ a.e. in $\mathbb R$. As an example of interest, one can take for $h$ the truncation $\tau_t\colon \mathbb R\to \mathbb R$, defined by
$$\tau_t(s) = \begin{cases} 
t, &\text{if }s\geq t,\\
s, &\text{if }-t < s < t,\\
-t, &\text{if }s\leq -t,
\end{cases}$$
which is clearly Lipschitz. Suppose for some measurable functions $u$, $u_n$ that $\tau_t(u_n) \rightharpoonup \tau_t(u)$ in $W^{1,p}(\Omega)$ for every $t>0$ and let $\varphi \in W^{1,p}(\Omega)\cap L^\infty(\Omega)$. Then, for every $\varepsilon > 0$,
$$\tau_\varepsilon(u_n-\varphi)\rightharpoonup \tau_\varepsilon(u-\varphi)\quad\text{in }W^{1,p}(\Omega),$$
as, for $k=\|\varphi\|_{L^\infty(\Omega)}$,
$$\tau_\varepsilon(u_n-\varphi) = \tau_\varepsilon(\tau_{\varepsilon +k}(u_n) - \varphi).$$","['functional-analysis', 'weak-convergence', 'sobolev-spaces']"
2214304,$x^2+y^2=z(4z+1)$ solutions,"For a small project I am working on, I wish to find the solutions for
$$x^2+y^2=z(4z+1)$$
in natural numbers $x,y,z$.
I wish to automate finding solutions for $z$ up to a maximum value as efficient as possible, running through $z$ values from 1 and then try to find possible $x$ and $y$. One thing that I found is that I can disregard $z \equiv 3,6,7 \mod 8$, as the sum of two squares can only be $\equiv 0,1,2,4,5 \mod 8$.
But I wonder which other criteria I can use to exclude $z$ values. I also wonder whether for given $z$, which $x$ I can ignore, so that I only test those $x$ values that could give a solution.","['diophantine-equations', 'divisibility', 'number-theory', 'congruences', 'elementary-number-theory']"
2214311,Complex plane iteration of z^z,"How can I identify when the iterative sequence $z \to z^z$ is not going to head for infinity for some starting value $z$? For example, $z=1$ is fixed, since $1^1 = 1$. If $z$ is real and between 0 and 1, the iteration heads for 1, where it stays. When starting $z$ is real and greater than 1, then the iteration heads for infinity. When $z$ starts out negative, however, complex numbers quickly come into play. $-1+0i$, for instance, quickly zeros in on $1+0i$; however, $-0.5+0i$ quickly exceeds computing parameters. Restating $z$ as $x + yi$ and showing the iterations to the point of exceeding computing capacity are as follows: $$-0.5 \to -1.414213562i \to 691.6428072-369.0287525i \to NAN.$$ Guidelines for complex exponentiation here: http://mathworld.wolfram.com/ComplexExponentiation.html This reminds me of the iteration behind the Mandelbrot set: $z\to z^2+c$. In the case of the Mandelbrot set, however, it's easy to identify when a $z$ is reached that will head for infinity (when the absolute value of $z$ exceeds 2). When plotting the Mandelbrot set on the complex plane, the program merely needs to catch when 2 is exceeded, and excludes the point being evaluated from the set. I would really like to see a plot of the set of points $x+iy$ on the complex plane where the iteration $x+yi \to (x+yi)^{x+yi}$ does not head for infinity. I suspect all such points zero-in on the value $1+0i$ but I am not sure they do. There may be other 'equilibrium' values, or there may be values where a repeated cycle forms (as is the case for points in the Mandelbrot set). I have done a plot for this, however, in this case I was simply assuming that the iteration was headed for infinity whenever $x$ or $y$ exceeded 10 - I'm not sure this is the case! Another problem is that the plot sometimes exclude points when they come close to $0+0i$ (calculation problem - I could fix this but the other problem of excessively high values remains). Here's the plot image: Obviously, the set is fractal in nature, so I would really like to see it calculated correctly. Most of the points excluded in the plot are excluded because they yield an iteration where $x$ or $y$ exceeds 10. I am pretty sure, however, that there are some very high numbers that do not head for infinity - having the imaginary unit in the exponent means that a very high positive input can become a very high negative input in the next iteration, such that the high negative real number in the exponent creates a real number in the iteration following that one that is very close to zero, thus zeroing-in on $1 + 0i$ in iterations following that. I even suspect that numbers that exceed my computers computing capacity (something like $10^{360}$ or something) may eventually come back to $1+0i$ if my computer could just compute further iterations. Might this in fact be the case for all starting values that contain a non-zero imaginary component? Could this be proven one way or the other?","['complex-analysis', 'complex-dynamics', 'exponentiation']"
2214330,Find description of $\mathscr H_0 \subset Hilb^3(\mathbb C^2)$,"I am trying to understand the Hilbert scheme of points in $\mathbb C^2$ and I have very naive questions about it. Consider $\mathscr H_0 \subset Hilb^3(\mathbb C^2)$ the subscheme of lenght $3$, supported at the origin, or equivalenty all the ideals $I \subset \mathbb C[x,y]$ with $\dim_{\mathbb C} \mathbb C[x,y]/I = 3$ and $Z(I) = (0,0)$. If we look at $A = \mathbb C[x,y]/I$, it's a $\mathbb C$-algebra of dimension $3$. There is a linear term $l \in A$ and if $l^2 \neq 0$ then $A = \mathbb C \{1,l,l^2 \}$. Else, there is another linear form $l' \in A$, but two linears form have the same $\mathbb C$-span as ${x,y}$, i.e $A = \mathbb C\{1,x,y\}$ or equivalently $I = (x,y)^2$. In the first case, $I = (l^3,l')$ where $l,l'$ form a basis of the linear form in $x,y$. Of course it does not change anything if we scale $l$ by $\lambda \in \mathbb C$. So really, if $Y = \mathbb P^1 \times \mathbb P^1 \backslash \Delta$ and, I believe the map $\phi : Y \to \mathscr H_0, (l,l') \mapsto (l^3,l')$ should be almost an isomorphism except that $(x,y)^2$ is not in the image. Now Eisenbud and Harris, in the book ""Geometry of Schemes"", page 63, claim that $\mathscr H_0$ can be embedded as a cubic cone in $\mathbb P^3$ with vertex = $(x,y)^2$. I have no ideas why. Can you help me ? I also have another problem. When I try to draw $p \in \mathscr H_0$ I just drawn 3 points on the same line $l$ which will collapse at $(0,0)$. I know point does not always arise this way but I like this way of thinking about them. However, for the ideal $(l^3,l')$ I only need $l$ for make this picture. So where does appear this auxiliary direction $l'$ on this picture ? Thanks in advance !","['schemes', 'algebraic-geometry']"
2214357,Intuition behind this interesting calculus result?,"So, $$\lim_{A\rightarrow \infty}\int^A_1 \frac{\ln x}{x} dx = \infty$$ All good. But, if you take the volume of revolution of the curve, rotated about the $x$ axis $2\pi$ radians, from $1$ to $A$, the volume remains finite as $A \rightarrow \infty$ (it approaches $2\pi$):
$$\lim_{A\rightarrow \infty}\pi\int^A_1 \left(\frac{\ln x}{x}\right)^2 dx = 2\pi$$ Pretty cool result, can anyone help me get my head round this intuitively? You'd assume an infinite area rotated around the x axis would produce an infinite volume... right?","['intuition', 'improper-integrals', 'calculus']"
2214370,Expected value of stopping time for Brownian motion,"$\newcommand{\P}{\mathbb{P}}$I would like to prove that stopping time: $\sigma=\inf\{t\geqslant0: |B_t|=\frac{1}{2}(1+\sqrt{1+t})\}$ is integrable. This is part of the answer to the question here . I've tried to estimate ${\P}(\sigma\geqslant t)$ in the following way:
$$\begin{array}{rcl}\P(\sigma\geqslant t) & = & \P(\forall s<t |B_s|\leqslant \frac{1}{2}(1+\sqrt{1+s} ) \\ & = & \P(\forall s<t  |B_1|\sqrt{s}\leqslant \frac{1}{2}(1+\sqrt{1+s}))\\ &= &\P\left(\forall s<t  |B_1|\leqslant \frac{1}{2}\left(\frac{1}{s}+\sqrt{1+\frac{1}{s}}\right)\right) \end{array}$$ I'm not sure if it is correct (I get something which doesn't converge). Thank you in advance.","['stochastic-processes', 'probability-theory', 'brownian-motion', 'stopping-times']"
2214476,A rank one matrix can be written in a special form,"Any rank one matrix can be written in the form $uv^{t}$, where $u,v$ are column vectors considered as matrices and $t$ denotes transposition. Why? How?","['matrix-calculus', 'matrix-decomposition', 'functional-analysis', 'matrix-rank', 'linear-algebra']"
2214482,"Is there a general theory of the ""improper"" Lebesgue integral?","I like the Lebesgue integral a lot more than the other alternatives, because of its connections to measure theory; it's a way of thinking about integration that's as ""liberated"" from the structure of the real line as possible, which cannot be said of, for example, the Henstock-Kurzweil integral . Sometimes, however, the improper version is needed. For example, here is a famous example of a function that has an improper Riemann integral, but no Lebesgue integral - but we can solve the problem by taking a limit of Lebesgue integrals, of course. And, if I understand correctly, the Henstock-Kurzweil integrals were invented to integrate functions that have neither a Lebesgue integral, nor an improper Riemann integral. Again, I'm under the impression that we can usually get away with taking a limit of Lebesgue integrals most of the time. But improper Lebesgue integrals aren't a purely measure-theoretic concept. So my question is: Question. Is there a notion of ""measure space equipped with further structure"" such that each such thing is associated with an improper Lebesgue integral? When applied to the real line, this ought to define an integral that equals or supersedes both the Henstock and ordinary Lebesgue integrals insofar as the space of functions that can be integrated. I was thinking, in particular, of measure spaces equipped with a notion of ""small"" subset, perhaps they form a bornology or something. We proceed to define that the improper Lebesgue integral of a function is a limit of its Lebesgue integrals on small subsets, and hopefully we can prove that every function that is Henstock-Kurzeil integrable is improperly Lebesgue integrable. Is anything like this out there?","['reference-request', 'axioms', 'lebesgue-integral', 'measure-theory', 'definition']"
2214499,Finding z-score of probability,I am learning the finance topic value at risk and trying to understand the number of standard deviations. I dont quite get why was 1.65 used to multiply against the std dev instead of 1.644853 as shown in the calculator below (used to retrieve the z-score from the probability),['statistics']
2214518,Reformulating a discrete model into a continuous model?,"Past Paper Question: Given a model of a ﬁsh population undergoing constant effort harvesting, where $N_i$ is the total population at time point $i$: $$N_{i+1}=N_{i}+BN_{i}-DN_{i}^{2}-EN_{i},$$ where $BN_{i}$ are the number of births, $DN_{i}^{2}$ are the number of death, and $EN_{i}$ is the harvesting rate. Reformulate the above model as a discrete model for constant yield ﬁshing, then take a small time limit to ﬁnd the equivalent continuous model. My Attempt: Not sure about the first part, but for the second part I think you make the substitution $N_{i+1}=N\left( t+\delta t\right)$ to get the form:
  $$\dfrac {N\left( t+\delta t\right) -N\left( t\right) } {\delta t}=BN-DN^{2}-EN,$$
  let $\delta t\rightarrow 0$ to give:
  $$\dfrac {dN } {dt}=BN-DN^{2}-EN$$ My Question: Is this the correct approach and what is the final answer?","['biology', 'ordinary-differential-equations', 'mathematical-modeling', 'calculus']"
2214544,"Binomial Coefficient Identity, Double Series, Floor Function","Show that for any natural numbers $m$ and $n$ such that $ m \le n $ that: $$ \sum_{i=0}^{n}{ \sum_{j=0}^{m}{ \left(-1 \right)^{\lfloor \frac{i}{2} \rfloor+j}2^{n-i}\binom{n-\lfloor \frac{i+1}{2} \rfloor}{j}\binom{n-\lfloor \frac{i+1}{2} \rfloor-j}{\lfloor \frac{i}{2} \rfloor}\binom{i}{m-j}}}= \left(-1 \right)^{m} \binom{2n+1}{2m+1} $$
I would like to avoid using generating functions but would like to use other techniques which regard coefficients of polynomials; I would like to avoid using calculus, trigonometric functions, chabyshev polynomials, complex numbers and mathematical induction. I would like to find an algebraic and combinatorial proofs. After some work with binomial identities, the LHS becomes:
$$\sum_{i=0}^{n}{ \left(-1 \right)^{\lfloor \frac{i}{2} \rfloor}2^{n-i}\binom{n-i+\lfloor \frac{i}{2} \rfloor}{n-i} \left(\sum_{j=0}^{m}{ \left(-1 \right)^{j}\binom{n-i}{j}\binom{i}{m-j}}\right)}$$
This form seems like it should be quite usefull but I haven't found the right binomial identities to keep going. Update: cross posted to MO","['binomial-coefficients', 'discrete-mathematics', 'combinatorics', 'summation', 'ceiling-and-floor-functions']"
2214555,"If $f \in L^p$ and $g \in L^q$, what can we say about $fg$?","We learned in class that if $f \in L^p$ and $g \in L^q$ and $\frac{1}{p}+\frac{1}{q} =1$, then $fg \in L^1$. I've been searching around for a more general formula for awhile, but it's surprisingly hard to find. I suppose that means that the obvious generalization, namely $$\frac{1}{p}+\frac{1}{q}=\frac{1}{r} \rightarrow fg \in L^r$$ isn't true. My question is simply: is this true, and if not, what's a simple counterexample? Also, is there anything we say about $fg$ at this level of generality? Here's a special case that actually does work.","['functional-analysis', 'lp-spaces', 'measure-theory']"
2214567,modulus in number theory,"Find all solutions of x^11 ≡ 1 (mod 23). Justify your work If I attempt to apply power of 11 to all values from 1-23, I get too large a value to then be able to see if it can be reduced modulo 23. Is there a simpler way? any help would be appreciated","['number-theory', 'modular-arithmetic']"
2214641,Shifting eigenvalues of a matrix,"Is there a way to shift a specific eigenvalue of a matrix $A$ without changing any of its eigenvectors? In other words, can I find some $\Delta A$ such that $B = A + \Delta A$ that has the almost the same eigensystem except at 1 eigenvalue? I understand that I can shift or scale all the eigenvalues at once, but not a particular one. Brauer's theorem (below) is the closest thing I find to what I need. However, although the eigenvector corresponding with $\lambda_0$ is not changed, it is not guaranteed that the remaining eigenvectors stay the same. Any reference/suggestions are much appreciated!","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra', 'vector-spaces']"
2214644,Considering operators on the direct sum of Hilbert spaces as operator valued matrices,"If $(\mathcal H_i)_{i \in I}$ is a family of Hilbert spaces, we can form their Hilbert space direct sum $$
\tilde{\mathcal H} := \oplus^{\ell^2}_{i \in I} \mathcal H_i
$$
where the $\ell^2$ expresses that norms of elements of $\tilde{\mathcal H}$ have to summable with respect to the 2-norm. Now, for simplicity, let $\mathcal H_i = \mathcal H$ for one fixed Hilbert space and let
$$
\tilde{\mathcal H} := \oplus^{\ell^2}_{i \in I} \mathcal H = \ell^2(I, \mathcal H),
$$
using the notation of Bochner-Lebesgue spaces ( I think I will have to restrict $\mathcal H$ to be a separable space, right? ). Let $\mathfrak B(\mathcal H)$ denote the space of all bounded operators on $\mathcal H$. From the algebraical viewpoint we have that 
$$
\mathfrak B(\ell^2(I, \mathcal H)) \subseteq M_I(\mathfrak B(\mathcal H)) := \left\{ (x_{ij})_{ij} \colon x_{ij} \in \mathfrak B(\mathcal H) \right\},
$$
i.e. every element of $\mathfrak B(\ell^2(I,\mathcal H))$ is a ""matrix"" with entries in $\mathfrak B(\mathcal H)$. Can we characterize an element $x$ of $\mathfrak B(\ell^2(I,\mathcal H))$ in terms of its ""entries"" $x_{ij}$? Here are my thoughts: Let $x = (x_{ij})_{ij}$ and $\xi = \oplus_i \xi_i \in \tilde{\mathcal H}$. If we want $x \in \mathfrak B(\tilde{\mathcal H})$ the following calculation could provide some necessary conditions:
$$
\| x\xi\|_2^2 = \left\| \bigoplus_i \left(\sum_j x_{ij} \xi_j\right) \right\|_2^2 = \sum_i \left\| \sum_j x_{ij} \xi_j \right\|^2
\leq \\ 
\sum_i \sum_j \|x_{ij}\|^2 \|\xi_j\|^2 
\leq \underbrace{\sum_i \left( \sup_j \|x_{ij}\| \right)^2}_{=: C < \infty} \sum_j \|\xi_j\|^2,
$$
where for the last inequality to hold I assumed that for each $i \in I$ the family $(x_{ij})_{j \in I}$ is uniformly bounded and that the family $s_i := \sup_j \|x_{ij}\|$ is in $\ell^2$. This would give me something like $x \in \ell^2( I, \ell^\infty(I, \mathfrak B(\mathcal H))$, but this is clearly not an algebra. I am happy about comments on my approach. Maybe someone can share a reference on this matter?","['bochner-spaces', 'hilbert-spaces', 'operator-theory', 'functional-analysis', 'operator-algebras']"
2214649,Resolutions in Algebraic Geometry,"I guess that the answer to this question can be given using Gröbner basis among many other computational methods, but my goal is to see if there is a more elementary way of approaching this problem. While trying to find a free graded resolution of $k[X_0,X_1, X_2,X_3]/(X_0,X_1)$,  I noticed that there's a pretty general method that consists on noticing that we have an exact sequence 
$$S(-1)^2\rightarrow S\rightarrow S/(X_0,X_1)\rightarrow 0$$
where the first morphism is given by the matrix $(X_0\phantom{-}X_1)$ and then, since we want the sequence to be exact, we impose $X_0f+X_1g=0$, which implies that $X_0| g$ and $X_1| f$, hence $g=hX_0$ and $f=h'X_1$. If we substitute this in the equality $X_0f+X_1g=0$, we obtain that $h=-h'$, therefore we have another morphism 
$$S(-2)\rightarrow S(-1)^2$$
given by $h\mapsto (X_0h, -X_1 h)$. Since the only element in the kernel is $0$, we have an exact sequence 
$$0\rightarrow S(-2)\rightarrow S(-1)^2\rightarrow S\rightarrow S/(X_0,X_1)\rightarrow 0$$
therefore we have a resolution of the initial module. My question is, can apply this method to solve other similar problems? For instance, I noticed that it worked perfectly in order to find a resolution for the twisted cubic or $k[X,Y]/(X^2,Y^2)$... but what happens for example in the case where $k[X,Y]/(X^2,XY,Y^3)$? I am not even sure if there is a faster method actually.","['projective-geometry', 'algebraic-geometry', 'commutative-algebra']"
2214651,Do any of these concepts relating to similarity in general metric spaces have names?,"Definition Let a metric space $(X,d)$ be given. A similitude is a function $f: X \to X$ such that, for all $x,y \in X$, $d(f(x),f(y)) = r \cdot d(x,y)$ for some positive real number $r>0$. Questions: 1. What is the name of metric space in which every similitude is surjective? For example $\mathbb{R}$ is such a space (I think) but $\mathbb{Z}$ is not. 2. The set of all surjective similitudes is always a group. However, unlike in $\mathbb{R}^n$, it is not always the case that, for each $r>0$, there exists a similitude $f:X \to X$ with scaling factor $r$. What is the name of such a space, i.e. a space for which $SurjSim(X) \cong \mathbb{R}^+ \times Iso(X)$? (Where $Iso(X)$ is the group of isometries of $X$.) 3. For any space as in 2. , does $Iso(X)$ only determine $d$ up to a positive constant? This is true for the Euclidean metric on $\mathbb{R}^n$. 4. Is 2. equivalent to the isometry group $Iso(X)$ being a homogeneous space with respect to $\mathbb{R}^+$? Or even to $X$ itself being a homogeneous space with respect to $\mathbb{R}^+$? For example, any finite-dimensional vector space satisfies these properties -- is there a name for more general spaces which are ""scaleable"" like this? Homogeneous space can refer to any type of group action, including the group action of a group $G$ on its corresponding torsor which denotes a notion of translation, not of scaling. Note: I think these may have something to do with the notions of (complete) convex metric spaces (for $r<1$) and (complete) externally convex metric spaces$^*$ (for $r>1$). *See the third page, p. 231, here . Also note that any geodesic space is a convex metric space as a result of Lemma 2.2. (ii) here . So I think it is possible to show that an externally convex geodesic space satisfies 2. , using the fact that any geodesic space is complete , Lemma 2.2(ii) , and binary search .","['terminology', 'reference-request', 'metric-geometry', 'geometry']"
2214663,"Is there a single real-valued function, continuous $f:[100,\infty]\to\mathbb R$ such that $f(f(x)) = \log x$ for every $x$ in its domain?","Consider the function $L_i(x)=\log^{(i)} x$ whose value is determined by taking the log of $x$ --  $i$ times. For example $L_2(x) = \log\log x$. Now, I want to extend this notion to non-integer $i$'s, and specifically $f(x)\triangleq L_{0.5}(x)$. In order to do so, I thought we can define $f(x)$ by the equation 
$$f(f(x)) = \log x$$ Is there a single real-valued continuous function $f$ such that on its domain it satisfies $f(f(x))=\log x$? Without the continuity requirement, the implicit assumption that $f(f(x))=\log x$ doesn't seem to be enough for the function to be unique. If it is indeed not, can anyone suggest a better way of generalizing $\{L_i\}_{i\in\mathbb N}$ to non-integer $i$'s? Is there a name for this function?","['implicit-function', 'functions']"
2214667,Calculating inverse of $(7+9i)$,How can I calculate $(7+9i)^{-1}$? So I have: $(7+9i)^{-1}$ $(a+bi) \cdot (7+9i)$ $7a + 9ai + 7 bi + 9bi^2 = 1 + 0i$ $7a + (9a + 7b)i - 9b = 1$ So there are two equations: 1) $7a - 9b = 1$ 2) $9a + 7b = 0$ So getting a from the first equation: $a = \frac{9}{7}b$ Inserting it in the second one: $9 \cdot \frac{9}{7}b + 7b = 0$ $\frac{81}{7}b + 7b = 0$ $b = \frac{130}{7}$ The correct solution should be: $\frac{7}{130}- \frac{9i}{130}$ Question : My solution looks close but wrong is wrong. Where is my mistake here?,"['complex-numbers', 'discrete-mathematics']"
2214696,Why is $ \emptyset$ considered a set? [duplicate],"This question already has answers here : What is an Empty set? (8 answers) Closed 7 years ago . My question is short and concise. Here it goes - In my book the definition of a set is given as a well defined collection of things and in mathematicse they are well defined collection of mathematical objects. Then why is $\emptyset$ which has nothing is even considered as a set. Is it merely a mathematica convention or is it that it has a special significance ? Though it is pretty general, I want to know the reason behind it. Thanks for your help .","['soft-question', 'elementary-set-theory']"
2214797,Euler totient $\varphi(n)$ is power of 2 if and only if $n$ is a product of power of 2 and a number of Fermat's prime numbers,"So, I want to prove the theorem in the title. The implication that if $n$ is product of power of 2 and some Fermat's prime numbers, then $\varphi(n)=2^k$ (where $\varphi$ denotes Euler's totient function and $k$ is some natural number) is, I think, obvious (directly from that formula). The problem is the other implication. If we have $\varphi(n)=2^l$ then first $2\mid n$ and we could write $n=2^kn_1$ where $k \in \mathbb{N}$ and $2$ does not divide $n_1$. Then, if $p_1,\ldots,p_k$ were all different prime divisors of $n$, then now we have $p_1=2$, while $n_1$ has prime divisors $p_2,\ldots,p_k$, none of which is $2$. Note that if $p_i^{k_i}$ divided $n_1$ for some $i$ , where $k_i>1$, then that $p_i$ divides $\varphi(n)$, and $\varphi(n)$ is not the power of $2$. So, $n_1=p_2p_3\cdots p_n$ and now when we write formula for $\varphi(n)$ we get $$\varphi(n) = 2^k(p_2-1)\cdots(p_k-1)$$ and $\varphi(n)$ is a power of $2$, meaning all its divisors are power of $2$, meaning $p_i = 2^{k_i}+1$ for all $2 \leq i \leq n$ and for some $k_i$, such that $p_i$ is a prime. The problem is that I can not conclude that $k_i = 2^{m_i}$, which I need for Fermat's numbers (of type $2^{2^n}+1$). Please help.",['number-theory']
2214830,Given $|f(x) - f(y)| \leq 7(x-y)^2$ show $f$ is a constant function [duplicate],"This question already has answers here : Function on $[a,b]$ that satisfies a Hölder condition of order $\alpha > 1 $ is constant (2 answers) Closed 6 years ago . I think that I've managed to figure this one out, I was just wondering if someone could look it over because there are a few areas that I'm not sure about my steps. Here is the full question: Suppose $f$ is a real valued function defined on all $x$ and $|f(x) - f(y)| \leq 7(x-y)^2$ for all $x$ and $y$. Prove $f$ is a constant function. First, define $y = x+h$ and take $h \neq 0$ , then $|f(y) - f(x)| \leq 7(x-y)^2$ becomes $|f(x+h) - f(x)| \leq 7h^2$. Dividing by $|h|$ we get: $\left|\frac{f(x+h) - f(x)}{h}\right| \leq 7|h|$. Clearly, $\lim_{h\rightarrow 0} 7|h| = 0$. Then by the sandwich theorem (squeeze theorem): $\lim_{h\rightarrow 0} \left|\frac{f(x+h) - f(x)}{h}\right| = 0.$ And thus $f'(x) = 0$ for arbitrary $x$. So $f(x)$ is a constant function. Added crucial condition on $h$ thanks to help from comments","['derivatives', 'real-analysis', 'proof-verification']"
2214839,Exactly how does the equation n(n-1)/2 determine the number of pairs of a given number of items (n)?,"I understand the purpose for division. Multiplication is simply repeated addition and division is simply repeated subtraction. So lets say we have 4 total items. Plugging 4 into the equation we get 4(4-1)/2 = 12/2 = 6.
So there are 6 possible combinations with 4 items. Applying the intuitive understanding of division as repeated subtraction, we can plot 12 on a numberline, and then since we are dividing by 2, we count backwards by 2 until we reach 0. We can do this 6 times. That makes sense to me. My issue is with the numerator. Why must a number of items be multiplied by one number less than itself? I understand the basic reason for multiplying the number of items by itself: pairs include two items, so the 4 takes care of the possible numbers on the first item, but why does the second item only receive 3 possible combinations? My hypothesis is that it prevents overlap, but I am having difficulty interpreting the logic of this. How exactly does it overlap? Can this be visualized on a matrix? Why does 4*4/2 = 8 not give the correct number of unique pairs?",['combinatorics']
2214853,How often can a number occur in Pascals Triangle?,"Each number occurs at least twice as $a={a\choose1}={a\choose a-1}$. If a number occurs somewhere else in the triangle (most likely twice, if it's not of the form ${2a\choose a}$) then that number occurs $4$ times. After that, it becomes interesting. I found the following with a simple script: $$120={120\choose1}={16\choose 2}={10\choose3}={10\choose7}={16\choose 14}={120\choose119}$$ $$210={210\choose1}={21\choose 2}={10\choose4}={10\choose6}={21\choose 19}={210\choose209}$$ $$1540={1540\choose1}={56\choose 2}={22\choose3}={22\choose19}={56\choose 54}={1540\choose1539}$$ $$7140={7140\choose1}={120\choose 2}={36\choose3}={36\choose33}={120\choose 118}={7140\choose7139}$$ And a special one, that did not only occur six times, but eight: $$3003={3003\choose1}={78\choose 2}={15\choose5}={14\choose6}={14\choose8}={15\choose10}={78\choose 76}={3003\choose3002}$$ I only checked the numbers up to $10000$; so here's my question Besides $1$, are there other numbers that occur infinitely often? Is there an upper bound known to how many times a number can occur in Pascal's Triangle?","['number-theory', 'binomial-coefficients']"
2214866,Computing orders of some irreducible finite Coxeter groups,"There is a particular method in Reflection groups and Coxeter groups by Humphreys to compute the orders of various irreducible finite Coxeter groups in Chapter 2.11. The method involves using group action of a finite group $G$ acting as a permutation group on a set $X \ni x$ so that $|G|=|Gx||G_x|$, where $Gx$ is the orbit of $x$ and $G_x$ is the isotropy group of $x$. In the context of irreducible finite Coxeter groups $W$, $W$ acts on the root system $\Phi$. The main exercise in that section asks to compute the orders of groups of type $\mathrm{A}_n$, $\mathrm{B}_n$, and $\mathrm{D}_n$ ($\mathrm{A}_n$ is not the alternating group of order $n!/2$ and $\mathrm{D}_n$ is not the dihedral group of order $2n$). My question is, how can we figure out the orbits and isotropy groups for the groups in the exercise? I'm finding it trickier to do this for the groups of type $\mathrm{B}_n$ and $\mathrm{D}_n$. Also, even though I know that the group of type $\mathrm{A}_n$ is the symmetric group $S_{n+1}$ of order $(n+1)!$, I'm not entirely sure how to end up with this order using the above suggested method. Are there any good references I could use?","['reference-request', 'root-systems', 'group-theory', 'coxeter-groups']"
2214954,Expressing the solid determined by a triple integral in spherical coordinates,"The problem prompts me to describe the solid determined by the triple integral $$ \int_{\pi/4}^{\pi/2} \int_{0}^{2\pi} \int_{0}^{3\csc\theta} f(\rho, \theta, \phi) \; \rho^2 \sin\phi \; d\rho \; d\theta \; d\phi $$ I am stuck at figuring out what $ \rho = 3\csc\theta $ (spherical coordinates) represents. In polar coordinates, $ r = \csc\theta $ is the line $ y = 1 $. Would this mean that $ \rho = 3\csc\theta $ is a rectangle of some sort? Edit: It has been confirmed that there was an error in the problem. $ \rho = 3\csc\theta $ should be $ \rho = 3\csc\phi $. The integral is therefore $$ \int_{\pi/4}^{\pi/2} \int_{0}^{2\pi} \int_{0}^{3\csc\phi} f(\rho, \theta, \phi) \; \rho^2 \sin\phi \; d\rho \; d\theta \; d\phi $$","['multivariable-calculus', 'spherical-coordinates']"
2214984,Does the Cauchy integral formula apply to negative values of $n$?,"The Cauchy integral formula is as follows: $$f^{n}(a) = \dfrac{n!}{2 \pi i}\oint_C \dfrac{f(z)}{(z-a)^{n+1}}\,\mathrm{d}z.$$ However in every source I can find describing the Cauchy integral formula does not state the domain of $n$ and I have only ever seen it used for positive values of $n$.  So my question is does the formula hold for negative values of $n$?  If so what does $f^{n}$ mean when $n$ is negative?",['complex-analysis']
2214993,Kullback-Leibler divergence for binomial distributions P and Q,"Note: this is an assignment question Let $X$ be a discrete random variable with values in $\{1,...,n\}$ . $P$ denotes the distribution on $\{1,...,n\}$ when $X$ ~ $bin(n,p)$ and Q denotes the distribution on $\{1,...,n\}$ when $X$ ~ $bin(n,q)$ for $p,q \in (0,1)$ . Compute the Kullback-Leibler-distance $D(P || Q)$ . We write $X$ ~ $bin(n,p)$ if it is Binomial-distributed with parameters $n,p$ , that is $$P[X=k]=\binom{n}{k} p^k (1-p)^{n-k}$$ I have started to write down the definition of the KL divergence which is : $$D(P||Q)=\sum_{x \in X}{p(x)*log_2 \frac{p(x)}{q(x)}}.$$ After inserting my values this is: $$D(P||Q)=\sum_{x \in X}{ \binom{n}{x}p^x(1-p)^{n-x} *log_2 \frac{\binom{n}{x}p^x(1-p)^{n-x}}{\binom{n}{x}q^x(1-q)^{n-x}}}.$$ from which I can factor out $\binom{n}{x}$ in the fraction: $$D(P||Q)=\sum_{x \in X}{ \binom{n}{x}p^x(1-p)^{n-x} *log_2 \frac{p^x(1-p)^{n-x}}{q^x(1-q)^{n-x}}}.$$ I don't see how I can further simplify this term, can someone give me a hint?",['probability']
2215014,Density in function space,"Let $$A = \{ f\colon{\mathbb{R}^2} \to \mathbb{R}\} $$
be the set of all function with two varibles
and let $$B = \{ f(x,y) = X(x)Y(y)\} $$ represent the set of all function with two variables that can be written as the product of two separate functions.
The question is: is $B$ dense in $A$? Thanks.","['functional-analysis', 'real-analysis', 'density-function']"
2215018,What is the easiest way to solve the following differential equation?,"I am only just starting online courses on the subject of differential equations, and I passed the following differential equation $$\frac{dy}{dx}= -2x + 3y - 5$$ I'm a bit confused by the use of two variables in the equation. What is the easiest way to solve this? In other words, in a way that a complete differential equation beginner like me can understand it.",['ordinary-differential-equations']
2215035,Axiom of Choice in Mathematical Analysis,"I'm an undergraduate student and I'm looking for a book, suitable for self-study, that ""explains"" the applications of the axiom of choice in mathematical analysis. I'm not familiar with the axiom of choice, so I'd prefer a beginner-level book. If anybody could help me with this, I'd be grateful. I really don't know where to look for something like this.","['reference-request', 'book-recommendation', 'analysis', 'axiom-of-choice']"
2215041,Thermodynamic/Partial Differentiation Quesiton,"I have a question which is somewhat physics-y but it's still a maths question at heart, I hope someone here can help. Basically, here is the problem, I get to this intermediate stage in a thermo proof: $$
dH = T \bigg( \frac{\partial S}{\partial T} \bigg)_P dT
$$ The next stage just skips to 
$$
\bigg( \frac{\partial H}{\partial T} \bigg)_P = T \bigg( \frac{\partial S}{\partial T} \bigg) _P
$$ Did they just divide by dT? I thought that, that was not something that is even possible (in terms of proper maths). I've tried applying the product rule but then I end up in the precarious position of trying to to figure out what the derivative of dT is wrt to T, can someone please guide me through how you get from the first equation to the second in formal math terms please! Thank you.","['derivatives', 'physics', 'partial-derivative', 'calculus']"
2215066,Evaluate $\lim_{n \to \infty} \sqrt[n]{3^n+4^n}$ [duplicate],"This question already has answers here : How to show $\lim_{n \to \infty} \sqrt[n]{a^n+b^n}=\max \{a,b\}$? [duplicate] (3 answers) Closed 6 years ago . $$\lim_{n \to \infty} \sqrt[n]{3^n+4^n}$$ Is there there a way to solve this without using $e^{ln(3^n+4^n)}$? Maybe:
$\displaystyle\lim_{n \to \infty} \sqrt[n]{4^n}=4\,\leq\,\lim_{n \to \infty} \sqrt[n]{3^n+4^n}\,\leq\,\lim_{n \to \infty} \sqrt[n]{2\cdot4^n}=4$?","['radicals', 'limits']"
2215084,Parallel Transport Equations,"I have a question about parallel transport that I'm very confused about and would appreciate some help. The question reads: What vector field $X$ on the unit 2-sphere in $\mathbb{R}^3$ has rotations around the $z$-axis as flows? The orbits of these flows would be the lines of latitude. Solve the parallel transport equations $\nabla_X(V_i)=0$, for $V_1, V_2$ the elements of a basis of the tangent plane, along the curve of latitude 45 degrees. What do the $V_i$ come back to once one has transported them all the way around the circle? I am confused by which vector field the question is talking about. Would this field just be the sphere parametrized as: $$(x,y,z) = (r\cos(\theta)\sin(\varphi), r\sin(\theta)\sin(\varphi), r\cos(\varphi))$$
Where $\varphi$ is constant?","['vector-fields', 'gradient-flows', 'spherical-geometry', 'differential-geometry']"
2215087,$\mathbb{Z}[\sqrt{11}]$ is norm-euclidean,"I'm trying to show that $\mathbb{Z}[\sqrt{11}]$ is Euclidean with respect to the function $a+b\sqrt{11} \mapsto|N(a+b\sqrt{11})| = | a^2 -11b^2|$ By multiplicativity, it suffices to show that $\forall x \in \mathbb{Q}(\sqrt{11}) \exists n \in \mathbb{Z}(\sqrt{11}):|N(n-x)| < 1$ For the analogous statement for $\mathbb Z [\sqrt6]$, it worked by considering different cases, so I tried to do the same thing here. Here is what I did so far: Let $x+y\sqrt{11} \in \mathbb Q (\sqrt{11})$ Case 1: Suppose there exists a $b \in \mathbb Z$ s.t. $|y-b| < \frac{1}{\sqrt{11}}$, then we can choose such a $b$ and a $a \in \mathbb Z$ s.t. $|x-a| \leq \frac{1}{2}$, then we have $|N(x+y\sqrt{11}-(a+b\sqrt{11}))| < 1$ From now on suppose $\forall b \in \mathbb Z: |y-b| > \frac{1}{\sqrt{11}}$ Case 2: Suppose there exists a $b \in \mathbb Z$ s.t. $|y-b| < \sqrt{\frac{5}{44}}$ Then we have $1 < 11 (y-b)^2 < \frac{5}{4}$, so we can choose $a \in \mathbb Z$ such that $\frac{1}{2} \leq |x-a| \leq 1$, then we have $|N(x+y\sqrt{11}-(a+b\sqrt{11}))| < 1$ From now on suppose $\forall b \in \mathbb Z: |y-b| > \sqrt{\frac{5}{44}}$ Case 3: Suppose there exists a $b \in \mathbb Z$ s.t. $|y-b| < \sqrt{\frac{2}{11}}$ Then we can choose $a \in \mathbb Z $ s.t. $1 \leq |x-a| \leq \frac{3}{2}$, then we have $|N(x+y\sqrt{11}-(a+b\sqrt{11}))| < 1$ From now on, we may suppose that $|y-b| > \sqrt{\frac{2}{11}}$. This is where I'm stuck. I tried choosing $b \in \mathbb Z$ s.t. $\frac{1}{2} \geq |y-b| > \sqrt{\frac{2}{11}}$, but then I run into problems, whether I choose $a \in \mathbb Z$ s.t. $1 \leq |x-a| \leq \frac{3}{2}$ or s.t. $ \frac{3}{2} \leq |x-a| \leq 2$","['abstract-algebra', 'euclidean-domain', 'algebraic-number-theory', 'ring-theory', 'quadratic-forms']"
2215147,"Let $R\to S$ be a ring map, then $\mathfrak{p}\in\operatorname{Spec}R$ lies under some $\mathfrak{q}$ in $S$ iff $S\otimes_R\kappa(\mathfrak{p})\ne0$","Fix a ring map $\varphi:R\to S$, and let $f:\operatorname{Spec}S\to\operatorname{Spec}R$ be the corresponding map of affine schemes. Remark 10.16.8 of the Stacks project claims that for some prime $\mathfrak{p}\in\operatorname{Spec}R$, we have that $\mathfrak{p} = f(\mathfrak{q}) = \mathfrak{q}^\mathrm{c}$ for some $\mathfrak{q}\in\operatorname{Spec}S$ if and only if $S\otimes_R\kappa(\mathfrak{p})\ne 0$. However, the proof uses a very elaborate diagram that I can't make sense of, so I'm wondering if there's a simpler proof of this fact.",['algebraic-geometry']
2215161,Prove that $\lim\limits_{x\rightarrow 4} \sqrt{2x+7} = \sqrt{15}$.,"Prove that $\displaystyle \lim_{x\rightarrow 4} \sqrt{2x+7} = \sqrt{15}$ using the epsilon-delta definition. This is what I have, but I know my delta value is incorrect. My professor said that it was the right path but my delta is incorrect. Proof: Let $\varepsilon>0$. Choose $\delta$ such that $0<\delta<\min(\varepsilon,1)$. This means that both $\delta<1$ and $\delta<\varepsilon$. Let $x\in\mathbb{R}$ such that $0<|2x-8|<\delta$. Since $\delta<1$, we have $$\begin{array}{cccccc}
 &-1 &< & 2x-8 & < & 1\\
\Rightarrow & 7 &<& 2x &<& 9 \\
\Rightarrow & 7/2 & < & x & < & 9/2
 \end{array}$$ Since $7/2<x<9/2$, $$\begin{array}{cccccc}
 &7/2 & < & x & < & 9/2\\
\Rightarrow & 7 &<& 2x &<& 9 \\
\Rightarrow & 7+7 & < & 2x+7 & < & 9+7 \\
\Rightarrow & \sqrt{14} & < & \sqrt{2x+7} & < & \sqrt{16} \\
\Rightarrow & \sqrt{14} + \sqrt{15} & < & \sqrt{2x+7}+\sqrt{15} & < & \sqrt{16}+\sqrt{15}\\
\Rightarrow & \displaystyle \frac{1}{\sqrt{14} + \sqrt{15}} & > & \displaystyle \frac{1}{\sqrt{2x+7}+\sqrt{15}} & > & \displaystyle \frac{1}{\sqrt{16} + \sqrt{15}}\\
 \end{array}$$ This implies $$\left|\frac{1}{\sqrt{2x+7}+\sqrt{15}}\right|< \frac{1}{\sqrt{14} + \sqrt{15}}<1.$$ Therefore, $$\begin{align*}
\left|\sqrt{2x+7}-\sqrt{15}\right|
&= \left|\left(\sqrt{2x+7}-\sqrt{15}\right) \cdot \left(\frac{\sqrt{2x+7}+\sqrt{15}}{\sqrt{2x+7}+\sqrt{15}}\right)\right| \\
&= \left|2x+7-15\right| \cdot \left| \frac{1}{\sqrt{2x+7}+\sqrt{15}}\right|\\
&=\left|2x-8\right|\cdot \left|\frac{1}{\sqrt{2x+7}+\sqrt{15}}\right|\\
&< \delta \cdot 1 \\
&< \varepsilon \cdot 1\\
\end{align*}$$ Thus, $|\sqrt{2x+7}-\sqrt{15}|<\varepsilon$. So, $\displaystyle \lim_{x\rightarrow 4} \sqrt{2x+7} = \sqrt{15}$.","['epsilon-delta', 'calculus', 'limits']"
2215227,Confusion with notions of reducibility of representations,"I'm taking a first course on linear groups. I've never read anything about group representations before. Consider the following three conditions on an object of a pointed category. Simple - has only the identity and the terminal arrow as quotients. Indecomposable - is not a product. Semi-simple - is a product of simple objects. If the category in question is abelian, the bijection between subobjects and quotients, along with the existence of biproducts, give simpler characterizations of these conditions. I learned from the nlab that these conditions are studied in representation theory. However, the lecturer gave seemingly different definitions in class. I don't feel as comfortable with these. Definition 1a. Let $V$ be an $\mathbb F$-vector space and $G\leq \mathrm{GL}(V)$ a linear group. A linear subspace $W\leq V$ is a $G$ -invariant subspace if $gW=W$ for every $g\in G$. A minimal $G$-invariant subspace is one which does not contain a non-trivial $G$-invariant subspace. $G$ is called reducible if $V$ admits a non-trivial proper $G$-invariant subspace $W$. $G$ is called irreducible if it isn't reducible. $G$ is completely reducible if there exist minimal $G$-invariant subspaces $W_1,\dots,W_k$ such that $V=W_1\oplus\cdots\oplus W_k$. Definition 1b. Let $\rho:G\to \mathrm{GL}(V)$ be a representation of $G$. A linear subspace $W\leq V$ is called a $G$ -invariant subspace if it's an $\mathrm{Im}\rho$-invariant subspace. Analogously for reducibility, irreducibility, and complete reducibility. Definition 1a feels ""wrong"" to me. It feels $V$ shouldn't be mentioned at all, and that we should look at the category of all vector spaces equipped with $G$-actions (respecting the linear structure). Then I'm somehow hoping that ""minimal"" can be replaced with ""simple"" in a corresponding category. Then, definition 1b will be much more reminiscent of simplicity and semisimplicity. Moreover, definition 1b simply looks different to me from the definition at the nlab (as a simple object in the category of representations). Why are these definitions equivalent? In particular, why is ""complete reducibility"" equivalent to being a semisimple object in the category of representations? The minimal $G$-invariant subspaces are minimal elements of the subposet $\mathrm{Fix}(G\curvearrowright \mathrm{Sub}(V))$ of $G$-fixed points of the action of $G$ on the lattice of subspaces of $V$. I don't understand how this coincides with simple objects in some category not mentioning $V$ explicitly..","['category-theory', 'representation-theory', 'group-theory', 'linear-groups']"
2215246,Evaluate $\sum_{n=1}^{64}(-1)^n \left\lfloor \frac{64}{n} \right\rfloor \varphi(n)$,"Evaluate $$\sum_{n=1}^{64}(-1)^n \left\lfloor \dfrac{64}{n} \right\rfloor \varphi(n).$$ We have $$S = \sum_{n=1}^{64}(-1)^n \left\lfloor \dfrac{64}{n} \right\rfloor \varphi(n) = \sum_{n=1}^{32}\left\lfloor\dfrac{64}{2n}\right\rfloor \varphi(2n)-\sum_{n=1}^{32}\left\lfloor\dfrac{64}{2n-1}\right\rfloor \varphi(2n-1),$$ but this didn't seem to help. Can we relate this sum to the fact that $\displaystyle \sum_{k=1}^n\left\lfloor\dfrac{n}{k}\right\rfloor \varphi(k) = \dfrac{n(n+1)}{2}$? Also, $64$ is a power of $2$ so we may need to use that fact.","['number-theory', 'summation', 'totient-function', 'ceiling-and-floor-functions']"
2215359,Showing that matrix $Q=UV^T$ is the nearest orthogonal matrix to $A$.,"Let $A$ be an $m \times n$ matrix with a singular value decomposition $A=U\Sigma V^T$. Show that the matrix $Q=UV^T$ is the nearest orthogonal matrix to $A$, i.e., $$\min_{Q^TQ=I_{n \times n}} \|A-Q\|_F$$","['optimization', 'matrices', 'procrustes-problem', 'orthogonal-matrices', 'linear-algebra']"
2215376,"Taylor Series F"" [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question I'm looking to derive Taylor’s Series F’’ to 4th order: The final answer is 
F’’=(-F(x+2h)+16F(x+h)-30F(x)+16F(x-h)-F(x-2h))/(12h^2) + O(h^4) I'm just not sure how to get -1  16  -30  16  -1, Can anyone give me some guidance?",['multivariable-calculus']
