question_id,title,body,tags
1697839,Functions satisfying 4 out of 5 inner product properties,"Let us consider function $s:K^m \times K^m \mapsto K$ (here $K = \mathbb{R}$ or $K = \mathbb{C}$ ). If $\forall x, y, z \in K^m, \forall \lambda \in K$ $s(x + y, z) = s(x, z) + s(y, z)$ $s(\lambda x, y) = \lambda s(x, y)$ $s(y, x) = \overline{s(x, y)}$ $s(x, x) \geq 0$ $s(x, x) = 0 \implies x = 0$ then $s$ is called inner product. Problem. For each $n = 1, 2, 3, 4, 5$ find a function $s$ that doesn't satisfy the $n$ -th property and satisfies the remaining four. First consider $K = \mathbb{R}$ . I found the following: $n = 3, s(x, y) = xy^3$ $n = 4, s(x, y) = -xy$ $n = 5, s(x, y) \equiv 0$ How can I approach $n = 1, 2$ ? Perhaps I need to choose $K = \mathbb{C}$ for those? Edit : I changed the domain of $s$ from $\mathbb{R} \times \mathbb{R}$ to $K^m \times K^m$ because if $\lambda \in \mathbb{C}$ then $\mathbb{R}$ is not closed w.r.t. scalar multiplication and if $s: K \times K \mapsto K$ and 2-5 hold then 1 must hold.","['linear-algebra', 'functional-equations', 'inner-products']"
1697870,Height of $n$-simplex,"$n$-simplex is a generalization of triangle or tetrahedron (with $n + 1$ vertices). The problem is to find its height. I kindly ask to check my solution. I am not fluent with $n$-dimensional space yet, and can make a mistake. $h^2 + r_0^2 = 1$, $h$ is height, $r_0$ is the radius of the circle, described around the $n-1$-simplex (which is the side of our simplex). $r_0 = \sqrt{\frac{n(n-1)}{2n^2}}$ (I am pretty sure in it, it is easy to calculate). So $h = \sqrt{1 - \frac{n(n-1)}{2n^2}} = \sqrt{\frac{n^2 + n}{2n^2}}$. Still not sure I generalized it correctly, because all the time I used tetrahedron to imagine the problem.","['combinatorial-geometry', 'geometry']"
1697915,Is it possible to have a connected manifold that is a double cover of a 2-sphere?,"I have come up with a branched covering, but it necessarily has two branch points. From that I'm assuming that it can't be done, possibly related to the hairy ball theorem, but I don't know how to prove it either way. The branched covering is two spheres which both have a great circle segment between two branch points along which they are connected. So if you're traveling on the first sphere and cross that segment, you are now traveling on the second sphere and vice-versa. Of course, at the branch points there is a discontinuity. Is there a simple way to demonstrate that there is no such manifold? Or is there some construction which avoids the branch points? I apologize for any misuse of terminology. Corrections welcome.","['manifolds', 'spheres', 'covering-spaces', 'geometry']"
1697936,Kummer surfaces are smooth,"Let $X$ be the Kummer surface associated to an abelian surface $A$. I will denote by $\epsilon : \tilde{A} \rightarrow A$ the blow-up of $A$ at the 16 fixed points of the involution $i : A \rightarrow A$ sending $a \mapsto -a$, by $\sigma : \tilde A \rightarrow \tilde A$ the extension of $i$ to $\tilde A$, and by $\pi : \tilde A \rightarrow \tilde A/\sigma = X$ the projection map. I am attempting to understand the proof in Beauville's book that $X$ is a smooth. The problem is that I have only recently completed introductory courses in algebraic geometry, and haven't done many examples on this sort of thing. So, Beauville first remarks that it is only necessary to prove smoothness at points $\pi(q)$, where $q \in E_i$ - one of the 16 exceptional divisors on $\tilde A$. Then he says that writing $A$ as $V/\Gamma$, where $V = \mathbb{C}^2$, one gets local coordinates $(x,y)$ on $A$, in a neighbourhood of $p_i \in A$ (where $p_i$ is the fixed point of $i$ corresponding to the exceptional divisor $E_i$), such that $i^*(x)=-x$ and $i^*(y)=-y$. Now, I don't understand this last statement. What is $i^*$ in this context? And where does this $i^*(x)=-x$ and $i^*(y)=-y$ come from? Then he proceeds to set $x' := \epsilon^*x$ and $y' := \epsilon^*y$, and says that we can assume that $x'$ and $t := \frac{y'}{x'}$ are local coordinates on $\tilde A$ near $q$. How do we know that they generate the maximal ideal of the local ring $\mathcal{O}_{\tilde A, q}$? He concludes by saying that $\sigma^*(x') = -x'$ and $\sigma^*t = t$, so that $t$ and $u := x'^2$ are local coordinates near $\pi(q)$ on $X$, but again, I don't see why they generate the maximal ideal of $\mathcal{O}_{X, \pi(q)}$. Thanks for your help.",['algebraic-geometry']
1697981,"If X ∼ N(0, σ2), find the pdf of Y = |X|.","If X ∼ N(0, $σ^2$
), find the pdf of Y = |X|. So far I have $F_Y(y) = P(\lvert x \rvert < y) = P(-y < x < y) = F_X(y) - F_X(-y)$ but I don't know where to go from there","['statistics', 'probability', 'probability-distributions']"
1697991,Do Diagonal Matrices Always Commute?,"Let $A$ be an $n \times n$ matrix and let $\Lambda$ be an $n \times n$ diagonal matrix. Is it always the case that $A\Lambda = \Lambda A$ ? If not, when is it the case that $A \Lambda = \Lambda A$ ? If we restrict the diagonal entries of $\Lambda$ to being equal (i.e. $\Lambda = \text{diag}(a, a, \dots, a)$ ), then it is clear that $A\Lambda = AaI = aIA = \Lambda A$ . However, I can't seem to come up with an argument for the general case.","['matrices', 'linear-algebra']"
1697996,Serre duality explicitly on curves,"Consider a Riemann surface $X$, with genus and marking so that a suitable moduli space exists. It's a well-known fact that the tangent space to that moduli space at $X$ (in other words, the space of first-order deformations of $X$) is dual to the space $\Omega_X^{\otimes 2}(X)$ of holomorphic quadratic differentials on $X$. This fact goes back at least to Teichmuller, who used quadratic differentials to explicitly deform the Cauchy-Riemann operator and get a new complex structure. A more algebraic approach proceeds by noting that this tangent space equals the space of lifts of $X\to \mathbb C$ to $\mathbb C[\epsilon]/\epsilon^2$. (See Harris and Morrison Moduli of Curves .) For a reason I'm not sure of, such a lift is Zariski-locally trivial (i.e. isomorphic to $U\times_\mathbb{C} \mathbb C[\epsilon]/\epsilon^2$, and so can be described by a cover of $C$, together with gluing automorphisms on the overlaps
$$\mathscr O_X(U_i\cap U_j)[\epsilon]/\epsilon^2\to \mathscr O_X(U_i\cap U_j)[\epsilon]/\epsilon^2$$
satisfying a cocycle condition. An $R$-automorphism of the dual numbers over $R$ is the same thing as a derivation of $R$, so we have a system of derivations satisfying a cocycle condition. In other words, a first-order deformation gives a 1-Cech cocycle taking values in the tangent bundle, and by Serre duality $H^1(X,T_X)$ is dual to the space of quadratic differentials. I'd really like to see a transparent way to turn a smooth curve $\mathfrak{X}\to\mathbb C[\epsilon]/\epsilon^2$ into a quadratic differential on the special fiber, and while the first step of the above is pretty explicit, Serre duality is less so. Is there, in general or in this specific case (a smooth curve, Riemann surface, or even just for the tangent bundle) an explicit way to write down a pairing on Cech cocycles that descends to Serre duality? In particular, can we explicitly describe a pairing between the set of first-order deformations and the quadratic differentials?","['riemann-surfaces', 'complex-geometry', 'algebraic-geometry', 'deformation-theory', 'sheaf-cohomology']"
1698019,Name for the fact that a mattress can't be evenly rotated by repeatedly applying the same transformation?,"Please excuse any errors in terminology or notation, I am neither a mathematician nor do I play one on TV. I'm pretty sure this is a known problem, probably named, but I lack the background knowledge to even know where to start. All the searching I've done has gotten me advice on how to rotate my mattress, and information on different cycles -- carnot, nitrogen, biogeochemical, etc. You're supposed to rotate and flip a mattress so that your head rests on one end 1/4 of the time. Let's label the mattress so that it has a top and a bottom, and a north side and a south side: +---T---+
|       |
+---B---+

+---N---+
|       |
|       |
|       |
|       |
+---S---+ There are four states the mattress can be in: {TN, TS, BN, BS}. To flip a mattress exchanges (T,B), to rotate it exchanges (N,S). There is no combination of flipping and rotation that when continuously repeated will visit all the states -- if you flip the (N,S) state remains the same; if you rotate the (T,B) state remains the same; if you flip and rotate you still flip-flop between two states. If I'm not mistaken the number of states you can achieve by cycling through substates with degrees $m, n, ...$ is $LCM(m, n, ...)$. What is the name of this property, and the fact that this will enumerate all states iff $m, n, ...$ are all co-prime?","['finite-groups', 'combinatorics', 'group-theory']"
1698022,"Suppose that X ∼ U ( $− π/2$ , $π/2$ ) . Find the pdf of Y = tan(X).","Suppose that X ∼ U
$(−π/2,π/2$)
. Find the pdf of Y = tan(X). Make sure to define the
support of the density function. My work so far: 
$F_Y(y) = P(Y < y) = P (tan(x) < y) = P (x < tan^{-1}(y)) = F_X(tan^{-1}(y)) = \int_{-\pi/2}^{tan^{-1}(y)} 1/\pi = tan^{-1}(y)/\pi + 1/2 -> derivative -> 1/(\pi + \pi y^{2})$ This is the cauchy distribution. Thanks for the help in the comments!","['statistics', 'probability', 'probability-distributions']"
1698033,"Prove or disprove: For $2\times 2$ matrices $A$ and $B$, if $(AB)^2=0$, then $(BA)^2=0$.","My goal is to prove or disprove the following claim: For $2\times 2$ matrices $A$ and $B$ , if $(AB)^2=0$ , then $(BA)^2=0$ . My thoughts on the question: I know that $AB=O$ does not imply that $BA=O$ , so my first impression was that it is false. I tried the counter-example I know but it leads to $(BA)^2=O$ . ( edited ) As pointed out by Friedrich Philipp in the comments, $A$ or $B$ is not invertible. If one of them is invertible, then the question is easily shown to be true. I wish to avoid density arguments though, so I am still stuck with the case where $A$ and $B$ are both singular. The question is in a list of  prove or disprove questions for square matrices of any order. This specific one precise that $A$ and $B$ are of order $2$ , so maybe the property is true for these matrices. I seems to remember that if $M$ is a square matrix of order $n$ and is nilpotent, then the order of nilpotence (the smallest $p$ such that $M^p=O$ ) is at most $n$ , but I don't know how to use it here. Beside this, I am clueless. I am looking for ideas or hints on the problem.",['matrices']
1698039,Can someone explain why this happens? (Dividing variables with exponents),"Alright, so let's say I have $$\frac{x^{-6}}{-x^{-4}}$$ The answer is $\dfrac{1}{x^2}$, but why isn't it $\dfrac{1}{-x^2}$?","['algebra-precalculus', 'fractions', 'exponentiation']"
1698055,"Suppose that X ∼ Exp(1). For α > 0, β > 0, and −∞ < ν < ∞, find the pdf of $Y = αX^ {1/ β} + ν$.","Suppose that X ∼ Exp(1). For α > 0, β > 0, and −∞ < ν < ∞, find a) the pdf of $Y = αX^
{1/β} + ν$. b) E[Y ] Working on this problem right now but it seems complicated so I'm anticipating needing help. Will edit work into this part as I go along. a)
$F_Y(y) = P(Y < y) = P(αx^
{1/
β} + ν < y) = P(x < ({y- ν}/α)^β) = F_X(((y- ν)/α)^β) = 
\int_0^{((y- ν)/α)^β}e^{-x}dx $ -> integrate & differentiate -> weibull distribution pdf for y > v b) Thanks for the help below! 
$E[Y] = E[αX^{1/β} + ν] = αE[X^{1/β}] + ν = α\int_0^\infty x^{1/β}e^{-x}dx + ν = α\int_0^\infty x^{(1/β + 1) - 1}e^{-x}dx + ν = α\Gamma((1/ \beta) + 1) + ν = (\alpha / \beta) \Gamma(1/\beta) + ν$","['statistics', 'probability', 'probability-distributions']"
1698074,"Which distributions can be combined linearly, assuming they are independent?","Let $X$ and $Y$ be two independent distributions. I want a list of the distributions that can be combined linearly. For example, if $X\sim \text{Poisson}(\lambda)$ and 
$Y\sim \text{Poisson}(\lambda)$ and $Z= X + Y$, then $Z\sim \text{Poisson}(2\lambda)$ What are the distributions for which this is true? Useful Link: http://www.randomservices.org/random/special/Divisible.html","['probability-theory', 'statistics', 'probability-distributions']"
1698088,Are all computable functions continuous or vice-versa?,"A famous result in intuitionistic mathematics is that all real-valued total functions are continuous. Since the requirements for a function to be admitted intuitionistically is that it must define a procedure or algorithm, all functions are computable. This seems to suggest that all computable functions are continuous. My questions are: Is this true for all total recursive or just primitive recursive functions? Where can I find such a proof? Conversely, what are examples of continuous functions that are not computable? By the way, I heard here and here that most intuitionistic formal systems can only prove that there are no real-valued total discontinuous functions, not the stronger positive result that all functions are continuous.","['real-analysis', 'intuitionistic-logic', 'constructive-mathematics', 'computability', 'general-topology']"
1698097,Proving $\sum_{i=1}^n i(i!) = (n+1)! -1$ by Mathematical Induction,"Theorem: For any integer n $\ge$ 1. $$\sum_{i=1}^n i(i!) = (n+1)! -1$$
Prove by mathematical induction. I have this problem and I know how to go about it, but I don't understand what I should do with the last part of the proof. What I did was: I made the property P(n) equal to $$\sum_{i=1}^n i(i!) = (n+1)! -1$$ I then used P(1) as the basis, so I plugged in 1 into n like so: $$\sum_{i=1}^1 i(i!) = (1+1)! -1$$
The left hand side of the equation is equal to 1 and (2)! - 1 is equal to 1 for the right hand side. Thus, since the left hand and the right hand equal to each other, the statement is true. I then assumed P(k) was true. $$\sum_{i=1}^k i(i!) = (k+1)! -1$$ Afterwards, I had to prove P(k+1) was true. $$\sum_{i=1}^{k+1} i(i!) = (k+2)! -1$$
So: 
$$\sum_{i=1}^k (k+1)! - 1 + (k+1)[(k+1)!] = (k+2)! -1$$ I really don't understand step 4 much, but so far am I doing it correct? If so, I don't understand how to make those two equations equal so the proof can be correct. Any help?","['induction', 'proof-verification', 'discrete-mathematics']"
1698114,Motivation of paracompactness,"""A paracompact space is a topological space in which every open cover admits a locally finite open refinement"" is the definition of paracompactness on Wikipedia. Comparing with the definition of compactness, ""a topological space is called compact if each of its open covers has a finite subcover"". In a first understanding, the difference you notice is that, to be compact, the space must have a finite subcover for EACH of its open covers, so every compact space would be paracompact. I would like to know what is the motivation on that definition, in what way that concept helps in ""extending"" the notion of compactness and some examples in which paracompactness is an important property; if they are examples from differential topology or functional analysis, even better. Thanks in advance.","['differential-topology', 'functional-analysis', 'compactness', 'general-topology', 'analysis']"
1698115,"Derivative with a ""mixed"" discontinuity","I read that the derivative of a function can never have a ""jump"" discontinuity, but only essential discontinuity. My question is, can the derivative have a ""half essential and half jump"" discontinuity, where $\lim_{x\to a^-}f(x)$ does NOT exist, $\lim_{x\to a^+}f(x)$ DOES exist, and $\lim_{x\to a^+}f(x)\not= f(a)$ ?","['derivatives', 'real-analysis', 'calculus', 'limits']"
1698116,Mathematicians shocked(?) to find pattern in prime numbers,"There is an interesting recent article "" Mathematicians shocked to find pattern in ""random"" prime numbers "" in New Scientist . (Don't you love math titles in the popular press? Compare to the source paper's Unexpected Biases in the Distribution of Consecutive Primes .) To summarize, let $p,q$ be consecutive primes of form $a\pmod {10}$ and $b\pmod {10}$, respectively. In the paper by K. Soundararajan and R. Lemke Oliver, here is the number $N$ (in million units) of such pairs for the first hundred million primes modulo $10$, $$\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
&a&b&\color{blue}N&&a&b&\color{blue}N&&a&b&\color{blue}N&&a&b&\color{blue}N\\
\hline
&1&3&7.43&&3&7&7.04&&7&9&7.43&&9&1&7.99\\
&1&7&7.50&&3&9&7.50&&7&1&6.37&&9&3&6.37\\
&1&9&5.44&&3&1&6.01&&7&3&6.76&&9&7&6.01\\
&1&1&\color{brown}{4.62}&&3&3&\color{brown}{4.44}&&7&7&\color{brown}{4.44}&&9&9&\color{brown}{4.62}\\
\hline
\text{Total}& & &24.99&& & &24.99&& & &25.00&& & &24.99\\
\hline
\end{array}$$ As expected, each class $a$ has a total of $25$ million primes (after rounding). The ""shocking"" thing, according to the article, is that if the primes were truly random , then it is reasonable to expect that each subclass will have $\color{blue}{N=25/4 = 6.25}$. As the present data shows, this is apparently not the case. Argument : The disparity seems to make sense. For example, let $p=11$, so $a=1$ . Since $p,q$ are consecutive primes , then, of course, subsequent numbers are not chosen at random. Wouldn't it be more likely the next prime will end in the ""closer"" $3$ or $7$ such as $q=13$ or $q=17$, rather than looping back to the same end digit, like $q=31$? (I've taken the liberty of re-arranging the table to reflect this.) However, what is surprising is the article concludes, and I quote, ""...as the primes stretch to infinity, they do eventually shake off the pattern and give the random distribution mathematicians are used to expecting."" Question: What is an effective way to counter the argument given above and come up with the same conclusion as in the article? (Will all the $N$ eventually approach $N\to 6.25$, with the unit suitably adjusted?) Or is the conclusion based on a conjecture and may not be true? P.S: A more enlightening popular article "" Mathematicians Discover Prime Conspiracy "". (It turns out the same argument is mentioned there, but with a subtle way to address it.)","['random', 'number-theory', 'congruences', 'prime-numbers', 'modular-arithmetic']"
1698124,Hamiltonian system; breakdown for different level set values,"I have a system of differential equations defined by the hamiltonian of the scalar function $H=y^2+e^{-xy}-c$, for some $c>0$. I am asked to describe what happens for $c=1$. I can tell there is a problem using mathematica (the level sets for $c=1$ seem to cross, violating uniqueness of solutions to the ode defined by these partials which travel around level sets). Since finding a function $H(x(t),y(t))$ in the first place for a system like this would seem to rely on the implicit function theorem, I am looking for a vanishing partial at the value c=1 to show that for this point the ift fails. I also tried thinking about the gradient of this function being not surjective for this value of c, since this would imply a singularity (or not regular point), I think. However, I think this would involve plugging the expression for $y(x)$ into my grad function, which seems a bit laborious and i want to make sure the thinking is correct first.","['multivariable-calculus', 'real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
1698143,How to expand $f(z)$ as a Laurent Series,"Given $$\frac{z}{(z-1)(z+2i)}$$ expand $f(z)$ in the following regions: $|z|<1$, $1<|z|<2$, $|z|>2$ I'm preparing for an exam and Laurent Series are a weakness of mine. I would love advice regarding interpretation, understanding, and solution of the problem.","['laurent-series', 'complex-analysis', 'sequences-and-series']"
1698149,$T^i$ functors in Hartshorne's Deformation Theory,"In chapter 3 of Hartshorne's Deformation Theory , he defines functors $T^i$ for $i=0,1,2$ that take as input a ring homomorphism $A\rightarrow B$ and a $B$-module $M$ and outputs $T^i(B/A,M)$, a $B$-module. Among the properties given are that a short exact sequence of $B$-modules gives a long exact sequence of the $T^{i}$'s and $T^0(B/A,M)={\rm Hom}_B(\Omega_{B/A},M)$. What is the difference between the $T_i$'s and taking the derived functor associated to ${\rm Hom}_B(\Omega_{B/A},\cdot)$ and does this difference measure anything? The construction first creates a complex $L_2\rightarrow L_1\rightarrow L_0$, where $L_1,L_0$ are free and give a resolution $L_1\rightarrow L_0\rightarrow \Omega_{B/A}$, so I think $T^0$ and $T^1$ agree with the derived functor cohomology, but I don't know about $T^2$.","['deformation-theory', 'homological-algebra', 'algebraic-geometry', 'commutative-algebra']"
1698155,Deriving heat equation from brownian motion,"Today my prof gave me an equation of random walk:
$$p(x_i,t+\Delta t)=\frac{1}{2}(p(x_i-\Delta t)+p(x_i+\Delta t))-p(x_i,t)$$
Using this he get$$P_t=P_{xx}$$ when $\Delta t<<1$ But how and what's actually the meaning of the probability equation? e.g. one half of particles goes left and another half goes right, while nothing left in the middle. But why divided by 2 an how does this actually make sense?","['real-analysis', 'ordinary-differential-equations', 'probability', 'brownian-motion']"
1698185,Why are Unique Factorization Domains (UFD's) geometrically significant?,"We know that for $A$ a UFD, it's class group is trivial. More generally, for a factorial (stalks are UFD's) scheme $X$ (that is also noetherian and normal), we have an isomorphism between it's Picard group and it's Class group. Should I expect this result to make sense before proving it? Is there a geometric significance to UFD's? I do not know of any other significant theorem that needs as its assumptions factoriality. Why is it suddenly important here and what does it have to do with divisors/line bundles?","['intuition', 'unique-factorization-domains', 'algebraic-geometry', 'commutative-algebra']"
1698201,Prove that if $x = \sqrt{a^{\sin^{-1} t}}$ and $y = \sqrt{a^{\cos^{-1}t}}$ then $\frac{dy}{dx}$ = $-\frac{y}x$,"Prove: If $x = \sqrt{a^{\sin^{-1} t}}$ and $y = \sqrt{a^{\cos^{-1}t}}$ where $\sin^{-1}$ and $\cos^{-1}$ are inverse trig function, show that $\frac{dy}{dx}$ = $-\frac{y}x$ Unfortunately I don't see how this can be done. By differentiating x and y separately via parametric differentiation, I get the following two results: $\frac{dx}{dt} = \frac{\log a}{2\sqrt{1 - t^2}}$ and $\frac{dy}{dt} = -\frac{\log a}{2\sqrt{1-t^2}}$ Unfortunately I don't see how I can use that to arrive at $\frac{dy}{dx} = -\frac{y}x$","['derivatives', 'parametric']"
1698220,Constructing sets of certain measure from classes of bijections on the continuum,"Suppose that for each $\alpha < 2^\omega$, $f_\alpha:2^\omega \rightarrow 2^\omega$ is a bijection. I want to know whether it's always possible to construct an $X\subseteq Y\subseteq 2^\omega$ such that: $X$ has measure 0. $Y$ has positive measure. $\{\alpha \mid \min f_\alpha(Y) = \min f_\alpha(X)\}$ has positive measure. (For $Z\subseteq 2^\omega$, $\min Z$ denotes the smallest element under the ordinal ordering.)","['set-theory', 'measure-theory']"
1698254,Rational Exponents on Calculator,I'm confused as to why my calculator is saying that $$x^\frac{n}{n}=x$$ when $n$ is an even integer. I was under the impression that $$x^\frac{n}{n}=\sqrt[n]x^n=|x|$$ when $n$ is an even integer. Thanks for any clarification.,"['algebra-precalculus', 'calculator', 'computer-algebra-systems']"
1698286,Solving $\sin(5x) = \sin(x)$,"If I have an equation: $$\sin(5x) = \sin(x)$$ In what case can I equate $$5x = x$$ Is it only when there is a multiply of $2\pi n$ on either side, where n is any integer so $$ 5x = x+2\pi n$$ Also with this method can I get every possible solution or does that not work? I know I can use the sine addition formula but I want to see others I way I can solve this.",['trigonometry']
1698304,convolution with integration by parts,"I have a question for an equation from a paper. The paper says,
$$ \frac{\partial c}{\partial t}*g=\int_0^t \frac{\partial c(t-\tau)}{\partial \tau}g(\tau) d\tau=c*\frac{\partial g}{\partial t}+cg_0-c_0g $$
When I use the rule of integration by parts, I got different answer. More specifically, my derivation shows that the right hand side is in negative. Here is my derivation,
$$ \frac{\partial c}{\partial t}*g=c(t-\tau)g(\tau)|_0^t-\int_0^t c(t-\tau)\frac{\partial g(\tau)}{\partial \tau}d\tau=-(c*\frac{\partial g}{\partial t}+cg_0-c_0g ) $$
I have been struggling for this for the whole day. Hope someone can help me. Thanks in advance.","['algebra-precalculus', 'functional-analysis', 'calculus']"
1698310,"Show that the subspace $Y = \{ x \in \mathcal{C}[a,b] \mid x(a) = x(b) \} \subset \mathcal{C}[a,b]$ is complete","Now I understand that the function space $\mathcal{C}[a,b]$ of continuous functions from the closed interval $[a,b]$ to $\mathbb{R}$ is complete, with the metric: $ d(x, y) = \underset{t \ \in \ [a,b]}{\max} \vert x(t) - y(t) \vert $ I also understand the theorem which states that a subspace $M$ of a complete metric space $X$ is itself complete if and only if the set $M$ is closed in $X$. Finally I understand that $M$ is closed if and only if the situation $x_n \in M, x_n \longrightarrow x$ implies that $x \in M$. What is don't know what to do is how to put this all together to show that the given subspace $Y$ is closed in  $\mathcal{C}[a,b]$.","['functional-analysis', 'complete-spaces', 'metric-spaces']"
1698449,"Quadratic Formula, nature of roots with Trigonometric Functions","The original problem: If $0\le a,b\le 3$ and the equation $$x^2+4+3\cos(ax+b)=2x$$ has at least one real solution, then find the value of $a+b$ $$$$ At first, on rearranging, I got the following expression:
$$x^2-2x+(4+3\cos(ax+b))=0$$ I thought this was a quadratic in $x$, and thus from the quadratic formula(and that at least one real root exists), $D\ge 0$ ie $$4-4(4+\cos(ax+b))\ge 0$$ 
$$$$ However I'm not really sure about this. I've treated $\cos(ax+b)$ as a constant term even though the argument of the cosine includes $x$: the variable in which the quadratic expression is.
$$$$Under these circumstances, is it correct to use $3\cos(ax+b)$ as a constant? If not, how could I use the quadratic formula to find values of $x$ satisfying $$x^2-2x+(4+3\cos(ax+b))=0$$ Many thanks in anticipation!","['trigonometry', 'calculus', 'algebra-precalculus', 'geometry', 'quadratics']"
1698452,Maximum of a upper semicontinuous function,"If $f:\mathbb{R}^N\rightarrow\mathbb{R}$ is a continuous function and $\lim_{|x|\rightarrow \infty}f(x)=-\infty$, so for definition for all $N>0$ exists a $M>0$ such that $|x|>M$ implies $f(x)<-N$. Since I want search a global maximum, I can search it in $A=\{x\in\mathbb{R}^N : |x|\leq M\}$. It is a compact and so I can say that $f$ attains its maximum in some point $x_{0}$. How can I extend this for a upper semicontinuous function? Thank you.","['real-analysis', 'semicontinuous-functions', 'analysis']"
1698476,What would happen if I define 4 as the imaginary unit?,"In complex analysis, $\sqrt{-1} = i$ holds by definition. But why did people choose for this, instead of some other unit? What would happen if we define it to be a real number such as $\sqrt{-1} := 4 \in \mathbb{R}$?","['complex-analysis', 'real-analysis', 'complex-numbers']"
1698512,Isotrivial family: different definitions,"Let $f:X\to B$ a flat morphism of varieties over an algebraically closed field $k$. If $f$ is flat and with connected fibres we say that  $f:X\to B$ is a family over $B$. In literature you can find two definitions of ""isotriviality"" that seems to be not equivalent: (Mainly used when $X$ is a surface and $B$ is a curve). $f:X\to B$ is called isotrivial if the smooth fibres of $f$ are isomorphic. $f:X\to B$ is called isotrivial if there exists a dense open set $U\subseteq B$ such that $f^{-1}(x)\cong f^{-1}(y)$ for every $x,y\in U$. Clearly $1)\Rightarrow 2)$ but the converse seems to be false. Am I right?","['schemes', 'algebraic-geometry', 'fibration']"
1698561,Is distance function defined on a convex set is always convex?,"I am looking for an answer to the following question: Is the distance function defined on a convex set always convex? Obviously the convex set in question is metric. In particular I am interested in the case when the convex set is the set of probability measures. In formal terms: Let $\Delta(X)$ be the space of probability measures over $X$ (and assume for the sake of simplicity that $X$ is just a compact convex subset of $\mathbb{R}$). Distance is a function: $d:\Delta(X)\times \Delta(X)\rightarrow [0,1]$ that verifies the standard conditions, including the triangle inequality (and it metrizes a topology on $\Delta(X)$). My question is then if the following is true for any $d$ on $\Delta(X)$: $$d(\alpha \mu_1+(1-\alpha)\mu_2,\mu_3)\leq \alpha d (\mu_1,\mu_3)+(1-\alpha)d(\mu_2,\mu_3)$$ Thanks for any hints or references.","['general-topology', 'metric-spaces', 'measure-theory', 'probability-theory']"
1698586,Automorphisms of rationals as a group [duplicate],"This question already has an answer here : Automorphism group of $\mathbb{Q}$ considered as a group under addition (1 answer) Closed last year . Given to me was the following assignment: Prove that for each $a\in\mathbb Q^{*}$, the mapping from $\mathbb Q$ to $\mathbb Q$ which sends $x$ to $ax$ is an automorphism of the additive group $(\mathbb{Q},+)$. Vice versa, every automotphism of $(\mathbb{Q},+)$ is of this form. Conclude that $\mathrm{Aut}(\mathbb{Q},+)$ is isomorphic to $\mathbb Q^{*}$. My first problem was that I didn't really 'understand' this problem (still don't). I do understand the definitions. And my idea was that I was asked to basically prove that each homomorphism from $(\mathbb{Q},+)$ to itself can be assigned to a unique $a\in\mathbb Q^{*}$?","['abstract-algebra', 'group-theory']"
1698591,Evaluation of $\int\frac{(1+x^2)(2+x^2)}{(x\cos x+\sin x)^4}dx$,"Evaluation of $$\int\frac{(1+x^2)(2+x^2)}{(x\cos x+\sin x)^4}dx$$ $\bf{My\; Try::}$ We can write $$x\cos x+\sin x= \sqrt{1+x^2}\left\{\frac{x}{\sqrt{1+x^2}}\cdot \cos x+\frac{1}{\sqrt{1+x^2}}\cdot \sin x\right\}$$ So we get $$(x\cos x+\sin x) = \sqrt{1+x^2}\cos(x-\alpha)\;,$$ Where $\displaystyle \alpha = \tan^{-1}\left(\frac{1}{x}\right)$ So Integral $$I = \int\frac{(1+x^2)(2+x^2)}{(1+x^2)^2\cdot \cos^4 (x-\alpha)}dx = \int\frac{(2+x^2)}{(1+x^2)}\cdot \sec^4 (x-\alpha)dx$$ Now how can i solve after that,Help me Thanks",['calculus']
1698604,Solving $10x \equiv 1 \pmod{11^2}$,$$10x \equiv 1 \pmod{11^2}$$ I know $10x \equiv 1 \pmod{11}$. I Have $x \equiv 109 \pmod{11^2}$ but this took me a lot of time to find. Is there a quicker way?,['number-theory']
1698637,How do we take this expectation?,"Suppose $X \sim \mathcal{N}\left(\mu, \Sigma\right)$ and $A$ is a symmetric matrix. How do I evaluate the following expectation:
$$\mathbb{E}\left[e^{X'A'X}\right] $$?","['expectation', 'multivariable-calculus', 'statistics', 'integration', 'probability']"
1698642,Find minimum value in complex plane?,"Minimum value of
$$|z+1|+|z-1|+|z-i|$$ for all $z$ from complex numbers? 
Is there any particular way to handle these type of problems?","['complex-analysis', 'optimization', 'complex-numbers']"
1698648,Finding the general solution of a second order PDE,"I want to find the general solution of $$\frac{\partial^2f(x,y)}{\partial x^2}+\frac{\partial^2f(x,y)}{\partial y^2}+A^2f(x,y)=0,$$
  with $f$ a real valued function and $A$ a real constant. I know that $f(x,y)=\sin(\frac{m}{\sqrt{2}}(x+y))$ is a solution of this, but how do I find a general solution?","['ordinary-differential-equations', 'partial-differential-equations']"
1698695,arrangement of $n$ oranges and $n$ apples around a circle,what is the total number of distinct arrangements of $n$ oranges and $n$ apples around a round table? I have no idea how to go about.,"['permutations', 'combinatorics', 'combinations']"
1698722,Why is the function $a^z$ multi-valued?,"For $a \in \mathbb C$, $z \in \mathbb C \mapsto f(z) = a^z$ is multi-valued. Why so? Can you please explain this to me?","['complex-analysis', 'complex-numbers']"
1698727,ordered pair contains sets or only members?,"Is this: $(\{1,2,3\} , \{4,5\})$ is a legal ordered pair? I mean , can ordered pair contain 2 sets and not just 2 numbers?","['logic', 'elementary-set-theory']"
1698748,How can I prove that the argument of a transcendental function must be dimensionless?,"We all know from school that arguments of transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities. But is there a simple way to prove it?",['analysis']
1698765,Prove that $|\sin z| \geq |\sin x|$ and $|\cos z| \geq |\cos x|$,"For any $z=x+iy$, prove the following:
$$|\sin z| \geq |\sin x|$$ 
$$|\cos z| \geq |\cos x|$$ 
$\epsilon$-$\delta $ proof is not required. I don't really know how to proceed. I know in order to remove the absolute values I can square both sides and I have tried proving this statement using the hyperbolic forms and then the exponential forms but I keep running into circles and I am getting no where... So for the first one all I have is $$ |\sin z| \geq |\sin x|  
$$ $$|\sin z|^2=\sin^2x +\sinh^2y \geq \sin^2x $$ or $${1\over4} |e^{iz}-e^{-iz} |^2≥{1\over4}|e^{ix}-e^{-ix}|^2$$And at this point I think I am beginning to overthink how to square absolute values.","['hyperbolic-functions', 'complex-analysis', 'trigonometry', 'proof-verification']"
1698783,Understanding matrix multiplication [duplicate],"This question already has answers here : Matrix multiplication: interpreting and understanding the process (2 answers) Closed 8 years ago . I have a hard time understanding, on an intuitive level, what matrix multiplication actually does. I have used it a lot, but I do not really know what it does. I know that $Ax = y$, where $A$ is a matrix and $x$ is an $n$-tuple, is just another way writing a system of equations. Seeing it this way, matrix addition is quite intuitive. Since every elementary matrix can be seen as representing an elementary row operation, I can understand what matrix multiplication does with invertible matrices. However, that still does not explain anything when we are working with matrices that are not invertible. What is a good way of thinking about matrix multiplication?","['matrices', 'linear-algebra']"
1698812,Lipschitz-constant gradient implies bounded eigenvalues on Hessian,"I've read in a few places that if we have a Lipschitz gradient $$\|\nabla f(x) - \nabla f(y)\|\leq L\|x-y\|,\, \forall x,y, $$
we can equivalently say $\nabla^2f\preceq LI.$ But I'm having a hard time showing this. (Equivalently, I want to show $z^T \nabla^2f(x)z\leq z^TLIz=Lz^Tz,\forall\, x,z $.)","['eigenvalues-eigenvectors', 'hessian-matrix', 'matrices', 'convex-analysis', 'lipschitz-functions']"
1698826,Factoring inequalities on Double Summation (Donald Knuth's Concrete Mathematics),"If you have the Concrete mathematics book please refer to page 40 and 41. So how come this given sum 
$$
\sum_{1 \le j < k + j \le n} \frac{1}{k}
$$ becomes $$
\sum_{1\le k \le n}\sum_{1\le j \le n-k} \frac{1}{k}
$$
? I do not understand how this is proven since to my understanding i am following from the book's method of factoring inequalities, $$
[1\le j < k+j \le n] = [1 \le j \le n][j < k+j\le n] \text{ or } [1\le k+j \le n][1 \le j < k+j]
$$ And it doesn't look like the double summation above. I Tried simplifying the two above and it leads me to nowhere near the answer.","['inequality', 'summation', 'discrete-mathematics']"
1698829,Compact subsets of metric space with French railway metric,"Let $A=\{0,1,2,...\}$ with $f$ the French railway metric that has centre $0$ and $f(a,0)=1$ for all $a\in A$ with $a\neq0$. How do I show that the metric space $(A,d)$ is complete? How do I show that $A$ is bounded, but not totally bounded? What is an example of a sequence in $A$ without convergent subsequence? What is an example of an open cover for $A$ without a finite subcover? How do I find all the compact subsets of $A$? What I know: For the French railway metric we know that for $a,b\in A$ with $a\neq b$ we have $$f(a,b)=f(a,0)+f(0,b)=1+1=2$$ (provided that neither $a$ nor $b$ are $0$, otherwise $f(a,b)=1$). Being complete means that all Cauchy sequences in $A$ converge. I think the trick to proving this lies in the above described French metric, but I don't see it at the moment. Being totally bounded means that there are finite $a_1,...,a_n$ in $A$ such that $A=\bigcup_{i=1}^nB_\epsilon(a_i)$. It seems intuitive that this is not the case, but how do I prove this exactly? Sadly, I have no idea how to handle this one. An open cover is a collection $X$ of open subsets of $A$ such that $A\subset\bigcup_{U\in X}U$. Then we need to find one such that no finite subcolection of $X$ is an open cover for $A$. I was thinking we could take all the finite subsets of $A$; since being compact means that for all open covers of such a subset, there are finitely many elements in that open cover such that these elements are an open cover. But I am not at all sure of this, or even wheteher these would be all the compact subsets.","['general-topology', 'metric-spaces', 'convergence-divergence', 'compactness']"
1698840,Approaching a seemingly intractable problem,"This is a soft question that questions how to be efficient in approaching a problem that seems intractable to solve. In particular in abstract algebra, I feel that certain proofs are 'magic'. When I approach a certain problem I always restrict the methods in the proof that I give to the material that has been discussed in the textbook so far. However, there are also times that when I see the proof (after trying really hard at attempting) that it seems trivially obvious. I then ask myself how I could have not. thought about that. But then again, I did spend the time in approaching the problem; so if it was so obvious, why didn't I solve it right away? I'm thinking that I lack some kind of efficiency when approaching problems in abstract algebra but I fail to see how I can improve this substantially. To give a random example, here is such a question (I'm not looking for an answer to this question, in this post, btw). Show that the order of any cyclic subgroup of $S_n $ is a divisor of $n! $. (Lagrange's theorem and cosets aren't discussed yet; just the definition of a (sub)group, the sign function and the concept of equivalence classes.) This post is written in the spirit of the broadest sense of how to approach a certain question efficiently, some times after many (failed) attempts. I may not have formulated the post in the clearest way possible. But then again, this is meant as a soft question. What I usually do when I'm stuck with a certain question, is reviewing the discussed material and then giving it another go. 
  Some times I search up Wikipedia on the concept, but then I feel that a question is posed to be solved with only the discussed material at hand; so this feels like cheating.","['abstract-algebra', 'group-theory', 'soft-question']"
1698849,Why is the total differential of a function just the sum of the components?,"In a multivariable calculus class, I was taught if $f$ is a function of $x, y, z$, the total differential of the function is:
$$df = f_xdx + f_ydy + f_zdz$$
Intuitively, it seems that there are missing terms to me. For example, shouldn't there be some sort of interactive term to account for how $x$ and $y$ change together? Or do these terms just ""go away"" under a technical derivation? Or maybe these terms or never there to begin with? Any help is appreciated!","['multivariable-calculus', 'chain-rule']"
1698878,Prime numbers are related by $q=2p+1$,"Let primes $p$ and $q$ be related by $q=2p+1$.  Prove that there is a positive multiple of $q$ for which the sum of its digits does not exceed $3$. My work so far: $p,q -$ primes and $q=2p+1 \Rightarrow \exists n,k \in \mathbb N: n=qk$ and $S(n) \le 3$ $p=2 \Rightarrow q=5 \Rightarrow 5|10=n; S(10)=1 \le3$. $p=3 \Rightarrow q=7 \Rightarrow 7|21=n; S(21)=3 \le3$. $p=5 \Rightarrow q=11 \Rightarrow 1|11=n; S(11)=2 \le3$. $p=7 \Rightarrow q=15=5 \cdot3 $. $p=11 \Rightarrow q=23 \Rightarrow 23|n; S(n) \le3$.",['number-theory']
1698879,Necessary condition for a polynomial of several complex variables to vanish at a point,"For a $1$ -variable polynomial $f(x)\in\Bbb C[x]$ it is well-known that $$f(a)=0\iff f(x)=(x-a)g(x)$$ for some polynomial $g(x)\in\Bbb C[x]$ . Question: Does that principle carries over to multivariable polynomials? That is if $f(x_1,\ldots,x_n)\in\Bbb C[x_1,\ldots,x_n]$ and $f(a_1,\ldots,a_n)=0$ for some $(a_1,\ldots,a_n)\in\Bbb C$ , does it follow that $$f(x_1,\ldots,x_n)=(x_1-a_1)\cdots(x_n-a_n)g(x_1,\ldots,x_n),$$ for some $g\in\Bbb C[x_1,\ldots,x_n]$ ? Attempt: We can fix $x_2,\ldots,x_n$ and consider $f(x_1,\ldots,x_n)$ as a $1$ -variable polynomial in $x_1$ that vanishes at $a_1$ and hence $f(x_1,\ldots,x_n)=(x_1-a_1)g(x_1)$ , but I am not sure why $g(x_1)$ would be a polynomial in $x_2,\ldots,x_n$ .","['complex-analysis', 'several-complex-variables']"
1698945,How to prove that the sum of the areas of triangles $ABR$ and $ CDR$ triangle is equal to the $ADR$?,"In the convex quadrilateral $ABCD$, which is not a parallelogram, the line passing through the centers of the diagonals $AC$ and $BD$ intersects the segment $BC$ at $R$. How to prove that the sum of the areas of triangles $ABR$ and $CDR$ is equal to the area of triangle $ADR$? I have no idea how to do this. Can this be proved with simple geometry?",['geometry']
1698972,solving ODE with power law solution,If I have this equation: $$ 3R' ^2  =-2R \frac{d^2R}{dt^2}$$ How can I show that If $$R=R_0 t^\alpha $$ then $$\alpha = 2/5$$,['ordinary-differential-equations']
1699025,Showing a morphism of affine varieties is surjective,"Let $X$ and $Y$ be affine varieties such that the coordinate ring $A(Y)$ is a subring of $A(X)$. Let
$$\pi:X\to Y$$
be the morphism induced by the inclusion $A(Y)\subseteq A(X)$. I need to show that $\pi$ is surjective if it satisfies the following condition: If $J=(g_1,\ldots,g_n)\subseteq A(Y)$ is an ideal such that $J\cdot A(X)=A(X)$, then $J=A(Y)$. I have a lot of trouble understanding how this condition can imply that $\pi$ is surjective. I don't know where to start. I start with ""let $P\in Y$"", but then what ideal $J$ do we take?",['algebraic-geometry']
1699049,Integration of a polynomial about a cricle,"Let $p(z)=a_0+a_1z+a_2z^2+...+a_nz^n$ $(a_n \neq0)$. Show that for sufficiently large r, $$\frac{1}{2\pi i} \int_{C(0,r)}\frac{p'(z)}{p(z)}dz=n$$ What can be said about a small r? I am trying to manipulate Cauchy's formula to achieve what I need but I can not seem to reach the conclusion. Also wouldn't the answer be the same for any positive r no matter the size?","['complex-analysis', 'real-analysis']"
1699063,Calculate the surface integral $\iint_{S}F\cdot dS$,"Calculate the surface integral $$\iint_{S}F\cdot dS$$
where $F\left(x,y,z\right) = xy\mathbf i + yz\mathbf j+zx\mathbf k$ and S is the surface of the cylinder $x^2+y^2 \le 1, 0 \le z\le 1$ oriented by the outwards normal.","['multivariable-calculus', 'surface-integrals', 'integration']"
1699103,"Ambiguous grammar $S\to ABA$, $A\to aA|\varepsilon$, $B\to bB|\varepsilon$","I need to show that: $$S \rightarrow ABA $$
$$A \rightarrow aA|\varepsilon$$
$$B \rightarrow bB|\varepsilon$$
is ambiguous and find an equivalent unambiguous grammar. I can't seem to see how this is ambiguous to begin with. Can someone explain how it is?","['context-free-grammar', 'discrete-mathematics']"
1699105,"Show that if $n$ points are such that any three lie in a circle of radius $1$, then all of them lie in a circle of radius $1$ [duplicate]",This question already has answers here : How to show that all points are inside of unit circle? (3 answers) Closed 3 years ago . Consider a set of $n$ points in the plane such that any three of them are contained in a circle with radius $r=1$. Prove by induction that all $n$ points are contained in a circle with radius $r=1$.,"['circles', 'induction', 'plane-geometry', 'geometry']"
1699130,Equation of convolution of measures,"Let $\mu_1,\mu_2$ be two locally finite complex regular Borel measures on $[0,+\infty)$ and $\delta_x$ be the Dirac measure at point $x\in[0,+\infty)$. Suppose that for all $x\in(0,+\infty)$ 
$$\delta_x*\mu_1=\delta_x*\mu_2.$$
Could we say that $\mu_1=\mu_2$? Where $*$ is convolution of measures which is defined for measures $\mu,\nu$
$$\int_0^\infty\psi(x)d(\mu*\nu)(x)=\int_0^\infty\int_0^\infty\psi(x+y)d\mu(x)d\nu(y), \quad \psi\in C_0([0,+\infty)).$$
In other world could we conclude that 
$$\lim_{n\to\infty}\delta_{\frac{1}{n}}*\mu=\delta_0*\mu$$
for all locally finite complex regular Borel measure $\mu$?","['real-analysis', 'convolution', 'lebesgue-measure', 'complex-analysis', 'measure-theory']"
1699141,Eigendecomposition and eigenvalue scaling for tensor networks,"I've recently been interested in studying tensors and networks of tensors. Consider the following quantity:
$$Tr(A^N)$$
Where $A$ is a square matrix and $N$ is large. This is extremely easy - all we need to do is find the eigenvalues and power each to $N$. In the limit that $N$ grows large, the trace is dominated by the largest eigenvalue. In tensor network notation, this is just a bunch of vertices on a line. The vertices are the matrices, and the lines connecting them represent contraction of the corresponding index. $$A_{ab}A_{bc}A_{cd}A_{de}A_{ef}....$$ My problem concerns the analogous scenario with a tensor $W_{abcd}$. Instead of contracting in a line, there are multiple copies of this tensor are arranged in an $N$ x $M$ square lattice and contracted with their nearest neighbors. The north, south, east, and west legs correspond to the a, b, c, and d indices. Is there a way to see how the 'eigenvalues' of this new tensor, or even the trace of the whole quantity, scale with $N$ and $M$? I'm pretty confident that the trace scales as the largest eigenvalue powered to the area of the rectangle, but I'm not sure how to prove it. EDIT: Also, even some simple info about related work on this topic would be greatly appreciated. EDIT 2: Something of note is that if we don't trace out the uncontracted indices, the number of uncontracted indices will scale as the perimeter of the rectangle, and thus the tensor will grow in dimension of its indices. EDIT 3: Some pictures!","['matrices', 'eigenvalues-eigenvectors', 'tensors', 'linear-algebra']"
1699147,If $|A|=|B|$ and $|C|=|D|$ then $|A\times C|=|B\times D|$ proof,I have done previous questions regarding these but this one seems too abstract. I have tried to build bijective functions with no success. Any help would be appreciated.,['elementary-set-theory']
1699198,Why are trigonometric functions defined in the context of right triangles?,"Couldn't we just use a different, fixed angle? Is there something special about the ratios of the sides in a right triangle? Is the pythagorean theorem somehow involved?","['trigonometry', 'functions']"
1699298,Arnold's proof of Liouville's Theorem on integrable systems,"My question happens to be almost identical to the one left unanswered/closed here , which gives a bit of background information - it may not be necessary. I hope the reason it was closed on mathoverflow is not a reason for it to be closed here, please let me know if I should reformulate the question. To reiterate, I refer to page 278 of Arnold's book 'Mathematical Methods of classical mechanics', namely the part after Lemma 3 is proved but before section 50 on Action-angle variables. An online version can be referred to here . My confusion first occurs when $p$ is stated as being a chart for $\mathbb{T}^k\times\mathbb{R}^{n-k}$, since it is certainly not 1-1 (all $f_i$ are mapped to 0 under it). This leads on to confusion with problem 10 - I simply cannot see a way to prove $\tilde{A}$ is a diffeomorphism, given that neither $p$ nor $g$ are diffeomorphisms. This seems to be the only gap for me at the moment - at first glance I am not sure about Problem 11 either, but will post about that separately. Any help on this would be much appreciated.","['dynamical-systems', 'mathematical-physics', 'symplectic-geometry', 'classical-mechanics', 'differential-geometry']"
1699301,Why doesn't the Sine or Cosine rule work when I'm resolving moments about P in this question?,"I originally posted this on Physics stack exchange, but some users said it was more suited here, although the moderator who reviewed it decided not to migrate it, as it has some physics jargon- but I don't think there's too much physics involved here, just moments. To work out the resultant moment about P, I started off by working out the perpendicular distance between the 3N force and P. To do this, I worked out the missing angles in the triangle, so in the end it looked like this: So all that is needed is to work out X, which will be the perpendicular distance, and multiply this by the 3N, which will give one moment. But when I use the sine rule, either with the 6m and 122⁰ or the 5m and 40⁰, I get different answers for X. To quickly show this: $$\frac{6 }{\sin122^\circ} =7.08$$
$$\frac{4}{\sin40^\circ} =7.78$$ Both of the above equations should be the same, In order to work out X. I use the cosine rule, I get an answer less than 2, and from inspection alone, I know that can’t be possible. So my conclusion is that the triangle given is not a valid triangle, as my method for resolving moments has worked for other questions. By my calculations, if $\frac{6 }{\sin122^\circ}$ is valid, then the 5m side should be 4.54m.  However, I am probably missing something, so what is it that I’m doing wrong? Why couldn’t I get a valid perpendicular distance for the 3N force using the sine or cosine rule?","['trigonometry', 'rotations']"
1699339,Show that R is an equivalence relation on set of integers.,"x,y into a Z (set of integers) and x related y if and only if x-y is a multiple of 3. Show that R is an equivalence relation on Z (set of integers). I need to show proof. 
I don't know how to prove Transitivity. This is what I have done but don't know if is right. Reflexive: x-x=0. So 0 is a multiple of 3. And 0 is an integer. Symmetric: if x-y is a multiple of 3 then y-x is a multiple of 3. Transitive: if x-y is a multiple of 3. Then y-z is a multiple of 3. So this leads to x-z is a multiple of 3.",['discrete-mathematics']
1699390,"Prove that for $n\ge 2$, $2{n \choose 2}+{n \choose 1} = n^2$","Problem : Prove that for $n\ge 2$ , $2{n \choose 2}+{n \choose 1} = n^2$ My Approach : I would assume that we can prove by induction. Base case $n=2$ . $$=2{2 \choose 2}+ {2 \choose 1}= (2\cdot 1)+2 =4$$ $$n^2 =2^2 = 4.$$ Assume for $n\ge 2$ , $n\ge 2$ , $2{n \choose 2}+{n \choose 1} = n^2$ for $n \le k$ . Let $n=k+1$ $$2{k+1 \choose 2}+{k+1 \choose 1} = (k+1)^2$$ And, that's as far as I got. I get stuck with the ${k+1 \choose 2}$ part.","['combinations', 'induction', 'combinatorics', 'natural-numbers', 'solution-verification']"
1699425,Are linear algebraic groups always finite-dimensional?,"Recall that a complex linear algebraic group $G$ is an affine algebraic group (i.e. an affine group variety over $\mathbb C$).
It's a well-known fact that there is always a closed embedding $$G \hookrightarrow \operatorname{GL}(n, \mathbb C).$$
My concern is that this seems to imply that $G$ is finite-dimensional as a variety, just because $\operatorname{GL}(n, \mathbb C)$ is for any particular integer $n$ (it has codimension 1 in $\mathbb A^{n^2}$, hence dimension $n^2-1$). This doesn't mesh well with my intuition, because I vaguely know that if $R$ is a non-Noetherian $\mathbb C$-algebra, then $\operatorname{Spec} R$ should be infinite-dimensional as a variety. Unfortunately I don't know any of these examples well-enough to try to construct a group operation on one of them to figure out what's happening. I'd appreciate it if someone could explain why it should be true that any $G$ should be finite-dimensional; for example if having the group law places some restriction on the size of $G$ that I'm not seeing. It's also possible that my claim is wrong or doesn't make sense in some way; if so I'd also be happy if someone could point out the mistake. Thanks. EDIT: To clarify on Qiaochu's comment: a linear algebraic group $G$ for me is an $k$-variety $G$ equipped with the structure of a group such that both the group multiplication $G \times G \to G$ and the inversion $G \to G$ are morphisms of varieties. But I think the issue was actually that I don't know the definition of variety: these need to have finite type. Thanks all.","['algebraic-groups', 'algebraic-geometry']"
1699446,An Injective Composition of Linear Transformation,"Suppose that A is a linear transformation from vector spaces U to V and that B is a linear transformation from vector spaces V to W. Suppose further that B composed of A is an injective composition of the aforementioned linear transformations. For this composition to be injective, do A and B have to be injective? My intuition tells me that B HAS to be injective while it is not required for A to be injective. Also, how would one prove that either A or B have to be injective. Also, I am quite fairly new to this site and I have no idea how I can use all of the math symbols and notation that everyone else here is using, so if someone can help me out on that then that would be great.","['function-and-relation-composition', 'linear-algebra', 'elementary-set-theory', 'linear-transformations']"
1699487,A conjecture concerning the irreducibility of characteristic polynomials of Arndt matrices,"Letting $n \in \mathbb{N}$, let $M_{n}$ denote the $n \times n$ binary matrix with ones along the main antidiagonal and everywhere below the main antidiagonal and ones along the antidiagonal two positions above the main antidiagonal, and with zeros everywhere else. For example, we have that: $$M_{7} =\left( 
\begin{matrix}
  0 & 0 & 0  & 0  & 1  & 0  & 1 \\
  0 & 0 & 0  & 1  & 0  & 1  & 1 \\
  0 & 0 & 1  & 0  & 1  & 1  & 1 \\
  0 & 1 & 0  & 1  & 1  & 1  & 1 \\
  1 & 0 & 1  & 1  & 1  & 1  & 1 \\
  0 & 1 & 1  & 1  & 1  & 1  & 1 \\
  1 & 1 & 1 &  1  &  1  &  1   &  1 
 \end{matrix}\right).$$ I refer to matrices of this form as Arndt matrices , based on the following conjecture due to Joerg Arndt (see https://oeis.org/A047211 ) which has been tested up to $n=177$: Conjecture (Arndt, 2011): The characteristic polynomial of $M_{n}$ is irreducible over $\mathbb{Q}$ if and only if $n$ is congruent to an element in $\{ 2, 4 \}$ modulo $5$. I have been interested in this conjecture for some time, and have made numerous attempts to prove this conjecture. My first attempt at proving this conjecture was to try to find a general technique for row-reducing matrices of the form $x I_{n} - M_{n}$ in order to evaluate $\text{det}(x I_{n} - M_{n})$. However, the process of row-reducing matrices of this form is very complicated. I have also considered using the Leibniz formula for determinants and cofactor expansion to evaluate $\text{det}(x I_{n} - M_{n})$, but this also seems to be very complicated. I have also considered using a recursive/inductive approach, by considering the possibility of expressing $\text{det}(x I_{n} - M_{n})$ in terms of expressions of the form $\text{det}(x I_{m} - M_{m})$ for $m<n$, but it is not clear how to construct such a recursive formula. I have several questions related to Arndt's conjecture, listed below: (1) Is there a simple way of evaluating $\text{det}(x I_{n} - M_{n})$? Is there a simple combinatorial formula for the coefficients of $\text{det}(x I_{n} - M_{n})$? (2) Is there an intuitive/heuristic explanation as to ""why"" the above conjecture may be true? (3) Do you have any suggestions or general insights as to how to approach the problem of proving the above conjecture?","['matrices', 'sequences-and-series', 'determinant']"
1699531,"On the graph of $f(x)=i^x$ being sinusoidal: reasons, related ideas, and relevant literature","Today I was thinking about imaginary numbers and hypothesized the graph of $i^x$ would be sinusoidal based on the simple exponent rules of i. I think this can be explained with logarithms, which I've never studied in depth so cannot be sure. I tried graphing real and imaginary parts of $i^x$ with sympy (a python symbolic maths package) and did end up with this sinusoidal curve. I'm wondering if someone here can comment on this curve - if there is some formula involving a trigonometric function that is equal to $i^x$, how one would find the derivative of $i^x$, and any other related ideas. Also, links to relevant literature would also be great. Update: I was encouraged to define $Re(i^x)$ which would make the answer clear to me. $$\operatorname{Re}(i^x)=\operatorname{sign}(i^x)^x$$ Additionally, according to sympy, $\frac{d}{dx}[i^x]=\frac{i \pi}{2} i^{x}$. This just leads to more questions. Where can I read more about these issues? While I invite all criticism, please take into account that I'm in 10th grade and haven't any serious formal math schooling and so am prone to make beginners mistakes. Asking these questions is my attempt to simulate the rigorous education in math I hope to receive in college","['algebra-precalculus', 'trigonometry', 'complex-numbers']"
1699532,Intersection of Algebraic Geometry and Algebraic Topology,"What mathematical areas lie at the intersection of algebraic geometry and algebraic topology? I'm aware of certain ones such as derived algebraic geometry and motivic homotopy theory, which all involve a fair amount of higher category theory. Are there any areas at the intersection that focus more on the algebraic geometry/algebraic topology themselves rather than more on the category theory? Do certain aspects of derived geometry satisfy this criteria, such as the recent work on derived symplectic geometry [PTVV]? For instance, reading some books such as Lurie's HTT and Higher Algebra seems more focused on category theory rather than homotopy theory itself.","['algebraic-topology', 'soft-question', 'algebraic-geometry']"
1699534,A Christmas tree has the shape of the conical helix. Find the length of the Christmas tree.,"A Christmas tree has the shape of the conical helix. The helix has the circular base of 1 foot diameter, 
and it rises three complete turns. Find the length of the Christmas tree. Image the goes with the problem Please explain the steps. Thanks!","['multivariable-calculus', 'calculus']"
1699593,Sum of elements in row of character table is positive integer.,"If $G$ is a (finite) group, how can I prove that in the corresponding character table, the sum of the elements in any row is a non-negative integer? The hint in the book says that I should let $G$ act on $G$ by conjugation and then consider the permutation character. I really don't understand this hint, can someone help?","['representation-theory', 'group-theory']"
1699600,Exact sequence splits?,"I am stuck in the following problem: Show that every group of order 4 is an extension of $\mathbb{Z}_{2}$ by $\mathbb{Z}_{2}$. Which of the exact sequences splits? Ok, i have to consider two cases for a group of order 4: $\mathbb{Z}_{4}$ and $\mathbb{Z}_{2}\times \mathbb{Z}_{2}$, so i get two exact sequences: $0\rightarrow \mathbb{Z}_{2}\overset{\psi }{\rightarrow}\mathbb{Z}_{4}\overset{\varphi }{\rightarrow}\mathbb{Z}_{2}\rightarrow 0$  and $0\rightarrow \mathbb{Z}_{2}\overset{\tilde{\psi} }{\rightarrow}\mathbb{Z}_{2}\times \mathbb{Z}_{2}\overset{\tilde{\varphi} }{\rightarrow}\mathbb{Z}_{2}\rightarrow 0$ For both cases i checked the conditions of exact sequence and proved that these are exact sequences. Now i have a problem how to show if they split or not. 
We say that an exact sequence splits if for the first sequence there exists a group homomorphism $\pi :\mathbb{Z}_{2}\rightarrow \mathbb{Z}_{4}$ with $\varphi \circ \pi =id_{\mathbb{Z}_{2}}$ and for the second $\tilde{\varphi} \circ \tilde{\pi} =id_{\mathbb{Z}_{2}}$, right? But how to show if these group homomorphisms exist or not? Can anybody help me, please? I would appreciate any hints and comments.
Thank you in advance!","['abstract-algebra', 'exact-sequence', 'group-theory']"
1699625,You are dealt five cards from a standard deck. What is the probability that you'll have at most three kings in your hand?,"You are dealt five cards from a standard and shuffled deck of playing cards. Note that a standard deck has 52 cards and four of those are kings. What is the probability that you'll have at most three kings in your hand? I know that the answer is $\frac{54144}{54145}$ from the answer key, and I know that the sample space is ${^{52}\mathrm C_5}$. What I don't get is how to find the event. Do I just add the combination for each number of kings together (${^5\mathrm C_2} + {^5\mathrm C_3}$)? Or do I need to multiply as well to account for the other cards in the 5-card hand?","['combinations', 'combinatorics', 'probability']"
1699665,Lie bracket of exact differential one-forms,"Let $(M,g)$ be a Riemannian manifold. The musical isomorphisms $^\flat:\chi(M) \to \Omega^1(M)$ and $^\sharp:\Omega^1(M) \to \chi(M)$ allow the space of differential one-forms $\Omega^1(M)$ to be identified with the space of vector fields $\chi(M)$. If I'm not mistaken, I can define the Lie bracket of two differential one forms $\alpha,\beta$ by $$[\alpha,\beta] := [\alpha^\sharp, \beta^\sharp]^\flat.$$ Now suppose that $\alpha$ and $\beta$ are exact; i.e. there exist smooth functions $A,B:M\to \mathbb{R}$ such that $\alpha = dA$ and $\beta = dB$, where $``d""$ denotes the exterior derivative. Is it necessarily true that $[dA,dB] = 0$? Here is the motivation for my question: On a $k$-manifold $M$, the integral curves of $k$ vector fields linearly independent at the point $x \in  M$ are the coordinate curves of a local coordinate system centered at $x$ if and only if their pairwise Lie brackets are zero (see, e.g., the discussion here ). On a Riemannian manifold, differential one-forms also have integral curves after identifying these one-forms with vector fields. I would like to know if there are ""natural"" conditions on a collection of $k$ differential one-forms that determine whether their integral curves similarly form the coordinate lines of a coordinate chart.","['differential-geometry', 'differential-topology', 'vector-analysis']"
1699693,"(without Phragmén-Lindelöf) $f$ is of exponential type and bounded on the real axis, then there exists $C>0$ such that $|f(x+iy)|\leq Ce^{\tau |y|}$","Question: Without using Phragmén-Lindelöf, show that if $f$ is of exponential type and uniformly bounded on the real axis, then there exists $C>0$ such that $|f(x+iy)|\leq Ce^{\tau |y|}.$ Definitions: An entire function $f$ is of exponential type if there exist $K>0$ and $\tau>0$ such that $|f(z)|\leq Ke^{\tau z}.$ Relevant information: I'm told to consider $f(z)e^{-\tau z}$ for $x\geq 0$ and $f(z)e^{\tau z}$ for $x\leq 0$ and use the maximum principle on appropriate rectangles. Attempt I am unsure of how to start this problem.","['complex-analysis', 'maximum-principle']"
1699718,"Is true that $\sum_{k=1}^n\frac{[kx]}{k}\leq[nx]$, for every $x\in\mathbb{R}$ and for every $n\in\mathbb{Z}^+$?","Is true that $\displaystyle\sum_{k=1}^n\dfrac{[kx]}{k}\leq[nx]$, for every $x\in\mathbb{R}$ and for every $n\in\mathbb{Z}^+$? My work: Let $x\in\mathbb{R}$ be given. For every $k=1,\dots,n$, define $i_k\in\{0,1,\dots,k-1\}$ such that 
$$
[x]+\frac{i_k}{k}\leq x<[x]+\frac{i_k+1}{k}\hspace{20mm} (*).
$$ On one hand, we have that $[kx]=k[x]+i_k$ for every $k=1,\dots,n$. So  $\displaystyle\sum_{k=1}^n\dfrac{[kx]}{k}=n[x]+\displaystyle\sum_{k=1}^n\dfrac{i_k}{k}=[nx]+\displaystyle\sum_{k=1}^n\dfrac{i_k}{k}-i_n$ Thus  $$\displaystyle\sum_{k=1}^n\dfrac{[kx]}{k}\leq[nx]\Leftrightarrow\displaystyle\sum_{k=1}^n\dfrac{i_k}{k}\leq i_n\,.$$ On the other hand, from $(*)$ is direct that $i_k=[k(x-[x])]$.
Thus  $$\displaystyle\sum_{k=1}^n\dfrac{[kx]}{k}\leq[nx]\Leftrightarrow\displaystyle\sum_{k=1}^n\dfrac{[k(x-[x])]}{k}\leq [n(x-[x])]\,.$$ In other words, we can assume that $x\in[0,1)$. Now the questions is: Is true that $\displaystyle\sum_{k=1}^n\dfrac{[kx]}{k}\leq[nx]$, for every $x\in[0,1)$ and for every $n\in\mathbb{Z}^+$? Any simpler approach? Any thoughts?","['inequality', 'ceiling-and-floor-functions', 'number-theory', 'elementary-number-theory', 'discrete-mathematics']"
1699723,Show that $S$ is a group if and only if $aS=S=Sa$.,"Let $S$ be a semigroup.  Show that $S$ is a group if and only if $aS=S=Sa$ for all $a\in S$. Since it is if and only statement, we have to show that if $S$ is a group then $ aS=S=Sa$, which I already know how to do. The other part: if you have $aS=S=Sa$, then prove that $S$ is a group. (NB: See the comments for the original thoughts on the problem.)","['abstract-algebra', 'semigroups', 'group-theory']"
1699834,"Integrating $\int_0^{\pi/2} \frac{\sin^2 ax}{\sin^2 x}\,dx$","I am looking for ways to evaluate: $$\int_0^{\pi/2} \frac{\sin^2 ax}{\sin^2 x}\,dx$$ where, $a$ is any positive real-number. (Real-analytic or complex integration techniques, either will do.) Sidenote to down/close votes: It was discussed in SE chat room 36 yesterday (incase you haven't checked robjohn's comment) and I decided to post it on main so that robjohn can share his awesome solution!","['integration', 'definite-integrals']"
1699847,Find height lines of the function $e^{x^2+y}$,"Find height lines of the function $e^{x^2+y}$. Well, I'm not sure it's called ""height lines"", but here's the solution: (see picture below) My question: How do I find those height lines? In other words, how do I find $y$? I know it's really simple but I couldn't manange to understand why the equation is $e^{x^2+y} = h$ and then they just put $ln$ in LHS and RHS and they get $y$. Is that how I do with every function? I just make it equal to $h$ and find $y$?","['calculus', 'functions']"
1699889,"Intersection point of two functions - one linear, the other with logarithmic and sqrt terms","I would like first to appreciate everything that is being done on this forum and to greet you all! I have namely two functions and the goal is to find the intersection point of them. $y_1 = a + \dfrac{1}{bc} (x-x_0)$ $y_2=\ln\dfrac{\sqrt{\dfrac{2dx}{c}+1}-1}{d} + {\sqrt{\dfrac{2dx}{c}+1}-1}$ where $a$, $b$, $c$, $d$, $x_0$ are known. To find the point $x$ where the two functions intersect , firstly I set these two functions equal $y_1 = y_2$:
$$\ln\dfrac{\sqrt{\dfrac{2dx}{c}+1}-1}{d} + {\sqrt{\dfrac{2dx}{c}+1}-1} = a + \dfrac{1}{bc} (x-x_0).$$ It is obvious that I must solve this equation numerically in order to find $x$. I've plotted the two equations in Matlab, wrote a code to find the minimum difference between $y_1$ and $y_2$ for different $x$ (ex. from 0 to 0.5 in 1000 steps), and then found the $x$ where $y_1$ nearly equals $y_2$, and I've got results that are near the expected. One thing is that the term under the sqrt is nearly in all cases negative, so it yields complex numbers and it has to logarithm these complex numbers. It's now my job to write a program to find the $x$ numerically, and I am at the moment craving for an idea to start with. What algorithm could I use to find the sqrt and ln (able to do the calculation with negative numbers)? How could I start solving the equations? I am pretty new in this field and I would appreciate and be grateful for every tip.","['logarithms', 'intersection-theory', 'functions', 'algorithms', 'numerical-methods']"
1699906,Expressing the Domain of a Function Correctly,"Say we have the function $y=\frac{1}{\sqrt{x^2-3}}$ and we are asked to find the domain. So basically the function will be undefined when $x = 0$, and is between  $-\sqrt{3}$ and $0$ and also, $0$ and $\sqrt{3}$. What would be the best way to express the function's domain? Could you say something like, $x:x\neq(-\sqrt{3}, 0 ] $ $\cup$  $[0, \sqrt{3})$ Or is that not mathematically correct?","['functions', 'graphing-functions']"
1699952,Determinant of $n\times n$ matrix with parameter,"Problem: Let $\delta \in \mathbb{R}^+$ and $n\in \mathbb{N}$. The matrix $A_n = (a_{i,j}) \in \mathbb{R}^{n\times n}$ is defined as $$
a_{i,j} = \prod_{k=0}^{i-2}\left((j-1)\delta +n-k\right)
$$
  Prove that
  $$\det A_n = \delta ^{\frac{1}{2}n(n-1)}\prod^{n-1}_{k=0}k!$$ So there is $$
A_1 =
\pmatrix{
1\\
}
$$ $$
A_2 =
\pmatrix{
1&1\\
2&\delta+2\\
}
$$ $$
A_3 =
\pmatrix{
1&1&1\\
3&\delta+3&2\delta+3\\
6&(\delta+3)(\delta+2)&(2\delta+3)(2\delta+2)\\
}
$$ $$\vdots$$ I eventually managed to prove it by converting the matrix to upper triangular using elementary row operations, but the proof is just too complicated, involves things like $(k−2)\delta+n−(i−(k−1))+1)$-th multiples of certain rows (for that matter it is quite long, so I am not fully including it here). So it somehow feels like not the best possible way to do this. What are some another ways to prove this?","['alternative-proof', 'matrices', 'reference-request', 'determinant', 'linear-algebra']"
1699955,Directional derivative of determinant at the identity is the trace of the matrix?,"Let $f:A\mapsto \rm{det}(A)$, Prove that $\left(Df\right)_{{\rm id}}\left(H\right)={\rm tr}\left(H\right)$
    for all $H\in\mathcal{L}\left(\mathbb{R}^{n}\to\mathbb{R}^{n}\right)$. The question appears also here: Directional derivative of the determinant but with no answers apart from that of the poster itself, and his solution uses some identities regarding the characteristic polynomial I do not understand. Additionally I think his approach assumes $H$ is invertible, which we are not given. Naturally I have also tried calculating the directional derivative, giving $$ \lim_{t\to0}\frac{\det(tH+\rm{id})-\det{\rm{id}}}{t}=\lim_{t\to0}\frac{\det(tH+\rm{id})-1}{t}
$$ where I want to prove it inductively by extracting the first row to have $$\rm{det}(tH+\rm{id}) = \rm{det}\pmatrix{1\ 0\ \dots \ 0 \\(tH+\rm{id})_2\\\ \vdots \ \\(tH+\rm{id})_n} + t\rm{det}\pmatrix{H_{1,1}\ H_{1,2}\ \dots \ H_{1,n} \\(tH+\rm{id})_2\\\ \vdots \ \\(tH+\rm{id})_n}
$$ (where $(A)_i$ is just the $i$th row of the matrix)
where I want to say that the left determinant is the sum of $tH_{i,i}$ for $i\geq 2$ by induction which leaves me with showing the right side is $tH_{1,1}$, but I'm not sure how to proceed with that.","['derivatives', 'real-analysis', 'determinant']"
1699987,Convergence of $f_n(x) = x+ (1/n) $ for $x \in \mathbb R$,"Let $f_n(x) = x+ (1/n) $ for $x \in \mathbb  R$; I must prove that $f_n$ converges to $F(x) = x$ uniformly on $\mathbb R$, and that $f_n^2$ does not converge uniformly. How can this be proven?","['uniform-convergence', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
1700036,Generalized measures,"This question is kind of broad. I welcome any answer, but I'll be more than happy to look it up myself in some book, if you could mention some title. Here's the setting: there's a Lie group $G$ acting via endomorphisms on an algebra $A$ with the action $\rho$, and there's its Lie algebra $\mathfrak g$ acting via derivations with the action $\gamma$. We require the two actions to be compatible by the equation
\begin{equation}
\frac{d}{dt}|_{t=0}(\rho(\exp(t\xi))(a))=(\gamma(\xi)(a))\ \forall a\in A,\xi\in\mathfrak g
\end{equation}
The problem is: how to define this derivative in general? The book I'm studying on (""Supersymmetry and equivariant de Rham Theory"", by Guillemin and Sternberg) claims this is possible if $A$ is equipped with ""some kind of topology"" or if any $a\in A$ is $G-$finite (i.e. the orbits of the $G-$action are all finite).
I don't see how one should do this: I can accept, in principle, that giving a topology one can define limits, but I don't see clearly how the argumentation goes.
As for $G-$finiteness, I understand that the fact that $\rho(g)(a)$ can only be a finite number of elements forces the limit we want to define to converge somewhere - but again: does it? And this is only an idea, I can't formalize anything. The second point is analogous. Pick the Lie group to be compact, consider an Haar measure $\mu$. We want to make sense of the assignment
\begin{equation}
A\ni a\mapsto \int_G\rho(g)(a)d\mu(g)\in A
\end{equation}
hence we need to be able to define the integral. The claim of the author is again that this is possible in the cases mentioned above - but they still don't mention how, and I keep not seeing it. Any idea?","['reference-request', 'measure-theory']"
1700050,The hypothetical fibre over an infinite prime in $\mathbb{A}^1_{\mathbb{Z}}$,"This is just a speculative question I've been wondering about; I hope that others may find it interesting too, but if it's too vague please let me know! We can picture $\mathbb{A}^1_{\mathbb{Z}} = \text{Spec}\mathbb{Z}[x]$ as a funny surface like the one below taken from Mumford's Red Book with ""vertical"" lines corresponding to all the primes, and various horizontal lines, curves etc intersecting these corresponding to different algebraic numbers, or equivalently different nonmaximal prime ideals of $\mathbb{Z}[x]$. We can also construct $\mathbb{P}^1_{\mathbb{Z}} = \text{Proj}\mathbb{Z}[x]$ which splits up as a disjoint union $\mathbb{P}^1_\mathbb{Z} \cong \mathbb{A}^1_{\mathbb{Z}} \sqcup \text{Spec}\mathbb{Z}$ consisting of the open subset isomorphic to the affine space $\mathbb{A}^1_\mathbb{Z}$ plus a closed ""line at infinity"" isomorphic to $\text{Spec}\mathbb{Z}$. The natural place for one to place this line at infinity in the picture above (as it is done in Eisenbud and Harris' The Geometry of Schemes ) is as a horizontal line right at the top of the picture ""above"" all the other horizontal lines. It's a vague picture like many others associated to schemes over $\mathbb{Z}$, but it makes some degree of sense. Granted this, we managed to ""compactify"" the vertical axis of $\mathbb{A}^1_\mathbb{Z}$ by adding the horizontal line at infinity isomorphic to $\text{Spec}\mathbb{Z}$. Is there a similar thing we can do for the horizontal axis? Given the infinitude of the primes, we'd need to have something like a fibre over an ""infinite prime"", possibly made rigorous using valuation theory. (From a geometrical viewpoint I wouldn't say the generic fibre of the morphism $\mathbb{A}^1_{\mathbb{Z}}\to\text{Spec}\mathbb{Z}$ was this new vertical line because I consider this fibre to be ""spread out"" over the whole of $\mathbb{A}^1_{\mathbb{Z}}$, but again this is just because of how I think about generic points - perhaps somebody can correct me on this.) I don't think this new $\mathbb{A}^1_{\mathbb{Z}}$ with a ""compactified arithmetic axis"" would be a scheme, but is there a way we can enlarge the category of schemes to formalise this compactification for ""infinite primes"", e.g. in the sense of valuations?","['schemes', 'algebraic-number-theory', 'algebraic-geometry']"
1700082,Prove this : $\left(a\cos\alpha\right)^n + \left(b\sin\alpha\right)^n = p^n$,"I have this question: 
If the line $x\cos\alpha + y\sin\alpha = p$ touches the curve $\left(\frac{x}{a}\right)^\frac{n}{n - 1} + \left(\frac{y}{b}\right)^\frac{n}{n - 1} = 1$ then prove that $\left(a\cos\alpha\right)^n + \left(b\sin\alpha\right)^n = p^n$ I know that the equation given is an equation of the line in normal form with perpendicular distance $p$ from origin. Also the slope of given line is $-\cot\alpha$ and this slope of line will be equal to the slope of the curve. But equating both is not yielding the desired result. The only little progress I seem to make after substituting $x$  and $y$ from the equation of line to the equation of curve seems futile to prove this further. I seem to make no further progress in this question. What should I do?","['plane-curves', 'curves', 'differential-geometry', 'calculus']"
1700141,Fermat's Challenge of composition of numbers,"In his letter to Carcavi (August 1659), Fermat mentions the following challenge There is no number, one less than a multiple of $3$, composed of a
  square and the triple of another square. He says that he has solved it using infinite descent. The next problem that he proposes in the same letter is also solved by him using infinite descent (which was discovered in his copy of Diophantus Arithematica). However, this question is not particularly clear to me, as to what is being asked to prove here. Can anyone please restate it using modern notations and provide a proof using Infinite Descent? Thanks in advance.","['number-theory', 'math-history', 'quadratic-forms', 'infinite-descent']"
1700172,Lower bound on $\pi(x)$,The book I am working through uses the bound $\pi(x)>\frac{x}{ \log x}$ without proof. Is it possible to prove this in a simple way using Sieve methods?,"['number-theory', 'prime-numbers', 'sieve-theory']"
1700186,Central limit theorem for distribution peak rather than mean,"The central limit theorem states that the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined expected value and well-defined variance, will be approximately normally distributed. i.e. $$
\lim_{n \to \infty}
   \sqrt{n}\left(\frac{1}{n}\sum_i X_i - \mu\right)
   = \mathcal{N}(0,\sigma^2).
$$ My question is whether a similar result exists which allows for some combinations of samples to tend towards a normal distribution centred about the peak (i.e. mode) of the distribution of the $X_i$, this obviously corresponds to the mean for normal distributions but I am interested in non-symmetric distributions where this is not the case. So I am asking if there exists some function $H$ s.t. $$
\lim_{n \to \infty} \alpha(H(X_1,...,X_n) - \phi) = \mathcal{N}(0,\sigma^2),
$$ where $\alpha$ is some number which might depend on $n$ and $\phi$ is the mode of the distribution from which the $X_i$ are drawn.","['statistics', 'probability', 'central-limit-theorem']"
1700204,Evaluation of $\lim_{n\rightarrow \infty}\sqrt[n]{\sum^{n}_{k=1}\left(k^{999}+\frac{1}{\sqrt{k}}\right)}$,"Evaluation of $$\lim_{n\rightarrow \infty}\sqrt[n]{\sum^{n}_{k=1}\left(k^{999}+\frac{1}{\sqrt{k}}\right)}$$ $\bf{My\; Try::}$ First we will calculate $$\sum^{n}_{k=1}\left(k^{999}+\frac{1}{\sqrt{k}}\right)=n^{1000}\sum^{n}_{k=1}\left(\frac{k}{n}\right)^{999}\cdot \frac{1}{n}-\frac{1}{\sqrt{n}}\sum^{n}_{k=1}\sqrt{\frac{n}{k}}$$ Now How can I solve afeter that, Help me Thanks in Advanced",['limits']
1700322,How is the principal branch of logarithm defined?,"In my textbook, it is defined as: $$\operatorname{Log} z = \ln |z| + i \operatorname{Arg} z$$ Where $\operatorname{Arg}$ is the principal branch of $\arg$, that's, the function which outputs the unique argument of $z$ in the interval $(-\pi, \pi]$. However, I am reading from Ahlfors' Complex Analysis, and on page $71$, it is written: ""...define the principal branch of logarithm by the condition $| \operatorname{Im log} z | < \pi$"". Which one is the correct definition? They are essentially different definitions because one includes the possibility that $\operatorname{Im log} z = \pi$.","['logarithms', 'complex-analysis', 'branch-cuts']"
1700347,The limit of the function $(1-x^2)/\sin(\pi x)$ as $x\to 1$,What is $${\lim_{x \to 1}} \frac{1-x^2}{\sin(\pi x)} \text{ ?} $$ I got it as $0$ but answer in the book as $2/ \pi$. Can you guys tell me what's wrong?,"['trigonometry', 'limits']"
1700381,Expected value of a coin toss,You flip a coin. If you get heads you win \$2 if you get tails you lose \$1. What is the expected value if you flip the coin 1000 times? I know that the expected value of flipping the coin once is $\frac{1}{2}(2) - \frac{1}{2}(1) =0.50$ Would the expected value be 500?,['probability']
1700402,Etymology of transpose of morphisms in an adjunction,"Let $F: \mathbf{C} \to \mathbf{D}$ be left adjoint to $G : \mathbf{D} \to \mathbf{C}$, witnessed by the family of bijections between hom-sets, natural in objects $X, Y$: $$\operatorname{Hom}_{\mathbf{C}}(X,GY) \overset{\psi_{X,Y}}{\longrightarrow} \operatorname{Hom}_{\mathbf{D}}(FX,Y).$$ I've noticed that among some authors, the image $\psi_{X,Y}(f)$ of a map $X \overset{f}{\to} GY$ is called the transpose of $f$. Does this have anything to do with a categorical realization of transpositions in linear algebra? Is there a canonical motivating example for adjoint functors involving transposes of linear operators, involving (say) adjoint representations of Lie groups?","['category-theory', 'linear-algebra', 'soft-question']"
1700407,Let $f(x) =\int^x_1 \frac{\ln t}{1+t}dt ;$ for $x >0$ then find the value of $f(e) +f(\frac{1}{e})$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Problem : Let $f(x) =\int^x_1 \frac{\ln t}{1+t}dt ;$ for $x >0$ then find the value of $f(e) +f(\frac{1}{e})$ please prove some hint on this as I am not getting any clue how to proceed here, thanks","['integration', 'definite-integrals', 'calculus']"
