question_id,title,body,tags
4879527,What does $sin^{-1}x+cos^{-1}y+cos^{-1}xy=π/2$ represent? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question The question asks me to ascertain what $sin^{-1}x+cos^{-1}y+cos^{-1}xy=π/2$ represents. I tried adding them and then trying to simplify the expression, but the expression just keeps getting more and more jumbled and doesn't lead to anything.",['trigonometry']
4879547,Group extension analogous to the symmetric group.,"Recently, My Professor taught us about group extension. It is the following: A group $G$ is an extension of $Q$ by $N$ if we have the following short exact sequence: $1 \rightarrow N \rightarrow G \rightarrow Q \rightarrow 1$ . After learning this definition I observed that for $n \geq 3$ , the symmetric group on $n$ letters $S_{n}$ is an extension of $\mathbb{Z_{2}}$ by the alternating group on $n$ letters $A_{n}$ , that is, we have an exact sequence: $1 \rightarrow A_{n} \rightarrow S_{n} \rightarrow \mathbb{Z_{2}} \rightarrow 1$ . From here, I came up with the following question that I am completely stuck at. If $G$ is any nontrivial group, does there exist a group $H$ that is an extension of a group $Q$ by $G$ such that $N \unlhd H$ implies $N \subseteq G$ ? (Here, in this question, I have assumed that $H \neq G$ .) Please help me.","['group-extensions', 'normal-subgroups', 'group-theory', 'abstract-algebra']"
4879557,Polynomial iterations II: Strange behaviour of $x^2-7x+5$ under iteration,"About a month ago, I asked a similar question about a certain class of polynomials that seem to defy the odds of ""eventual divisibility"", i.e. iterating an ""eventually divisible"" polynomial we can guarantee that any input will yield a number that is divisible by a certain prime $p$ at least once (and thus infinitely often). Please refer to that question for a thorough explanation of the idea. After some careful case elimination, I was left with (i.e. could not prove/disprove the property for) three quadratic polynomials where linear and constant coefficients are absolutely smaller than $10$ : $(1)$ $x^2-7x-7$ (which is checked up to $p \leq 4 \cdot 10^6$ by Mike Daas with no hits) $(2)$ $x^2-7x+5$ (which is checked up to $p \leq 6881261$ by Mike Daas with no hits) $(3)$ $x^2+5x+7$ (which was shown by Oscar Lanzi to be provably non-eventually divisible) Since the property seems to hold except for trivial exceptions (see other question), it seems highly unusual that these polynomials just happen to not have it by pure chance (even up in the millions as it turns out!). The method that was used for $(3)$ can be applied similarly, but does not yield an exhaustive proof: Let's focus on $(2)$ , since it has the smaller discriminant $\Delta = 29$ . A key idea for disproving eventual divisibility is $$x^2-7x+5 \equiv 0 \mod p \text{ is not solvable} \iff \left(\frac{\Delta}{p}\right) = \left(\frac{29}{p}\right) = -1$$ Using quadratic reciprocity, we can figure out that $$\left(\frac{29}{p}\right) = \left(\frac{p}{29}\right) = -1 \iff p \equiv 2,3,8,10,11,12,14,15,17,18,19,21,26,27 \mod 29$$ i.e. $p$ congruent to one of these residues need not to be checked since the existence of a cycle at $0$ is disproven by the lack of solutions leading to $0$ when iterated. The same argument can also be applied to the fixed-point-method in the mentioned question, but the CRT guarantees that we will never cover all primes with this (and the discriminants get very, very big leading to extra casework). Thus, the list of $p$ left after this procedure begins $$p = 5,7,13,23,53,59,67,71,83,103,107,109,139,\ldots$$ For convenience, let's list the cycles present in the iteration graphs for each of these primes: $$\begin{array}{c|c}
p & \text{Cycles} \\
5 & (0),(3) \\
7 & (2),(6) \\
13 & (0,5,8),(9,10) \\
23 & (3,16,11) \\
53 & (12),(16,43),(30,6,52,13) \\
59 & (0,5,54,6,58,13,24),(14,44,40,27) \\
67 & (15,58),(12,65,23,38,44,25,53,31) \\
71 & (12,65),(48,56,51) \\
83 & (33),(58),(36,53) \\
103 & (48,16,46) \\
109 & (20,47,32,42,58) \\
139 & (21),(126),(85,102,104)
\end{array}$$ As a bonus, here is a quite bizarre pattern that I spotted while generating the graph for $x^2-7x+5$ with $p=43$ : The erratic behaviour of the cycle's lengths and their residue classes left me clueless and it does not spark much hope for a proof, but nonetheless: Is it possible that the non-eventual divisibility of $x^2-7x-7$ and $x^2-7x+5$ happens to be just by chance, i.e. some anti-""strong law of small numbers"" is at work here? Is there another way of proving this property besides the one given by Oscar Lanzi?","['graph-theory', 'number-theory', 'polynomials', 'recreational-mathematics', 'prime-numbers']"
4879562,A complete local ring is finite over $\mathbb{Z}_p$ if and only if it has finitely many $\overline{\mathbb{Q}}_p$-valued points?,"Let $\mathbb{Z}_p$ be the ring of $p$ -adic integers and $\overline{\mathbb{Q}}_p$ a fixed algebraic closure of the field $\mathbb{Q}_p$ of $p$ -adic numbers. Let $R$ be a complete Noetherian local $\mathbb{Z}_p$ -algebra. It is not hard to see that if $R$ is a finite $\mathbb{Z}_p$ -algebra, then the set $\operatorname{Hom}_{\mathbb{Z}_p}(R,\overline{\mathbb{Q}}_p)$ of continuous homomorphisms of $\mathbb{Z}_p$ -algebras is finite. A natural question, then, is the reverse also true? Let $R$ be a complete Noetherian local $\mathbb{Z}_p$ -algebra of characteristic zero such that the set $\text{Hom}_{\mathbb{Z}_p}(R,\overline{\mathbb{Q}}_p)$ is finite. Is it true that $R$ is a finite $\mathbb{Z}_p$ -algebra?","['algebraic-number-theory', 'p-adic-number-theory', 'ring-theory', 'algebraic-geometry', 'commutative-algebra']"
4879563,"In calculus, if $\frac{dy}{dx}$ is not in fact a fraction, is the equation below for geometric brownian motion technically incorrect?","Consider the following stochastic differential equation for geometric Brownian motion: $$
{\displaystyle dS_{t}=\mu S_{t}\,dt+\sigma S_{t}\,dW_{t}}
$$ I was reading on Wikipedia about geometric Brownian motion ( https://en.wikipedia.org/wiki/Geometric_Brownian_motion ) and this question popped into my mind. To be clear, I understand the interpretation of the above equation, but is it technically correct? Clearly, if they weren't talking about derivatives (that is not in the limit as $dt$ goes to $0$ ), then I'd see no problem; but it looks like they simply multiplied both sides of the ""proper equation"" by $dt$ . By the ""proper equation"" I mean this: $$
{\displaystyle \frac{dS_{t}}{dt}=\mu S_{t}+\sigma S_{t}\,\frac{dW_{t}}{dt}}
$$ Am I missing something?","['calculus', 'stochastic-differential-equations', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4879574,How many ways are there to distribute $19$ distinct objects into $3$ distinct boxes such that no two boxes contain same number of element,"How many ways are there to distribute $19$ distinct objects into $3$ distinct boxes such that no two boxes contain same number of element,i.e, each boxes have distinct number of elements. No empty boxes allowed. My work: I chose using principle of inclusion exclusion principle and generating functions , so All - at least two boxes have same number of element All : Stirling number of second kind $3! \times S(19,3)=3! \times193448101$ or $\bigg[\frac{x^{19}}{19!}\bigg](e^x-1)^3$ by generating functions. Now, lets calculate two boxes have same number of element. Firstly, select which ones they are by $\binom{3}{2}$ .Because of these two boxes have same number of elements, they can take $(1,1),(2,2),(3,3),..,$ . So, if we select the elements for these two boxes at the same time, and after the selection we disperse selected elements between them equally. So, our E.G.F is equal to $$\binom{2}{1}\frac{x^2}{2!}+\binom{4}{2}\frac{x^4}{4!}+\binom{6}{3}\frac{x^6}{6!}+...+ =\sum_{n \geq1}\frac{x^{2n}}{(n!)^2}$$ So $$\binom{3}{2}\bigg[\frac{x^{19}}{19!}\bigg]\bigg(\sum_{n \geq1}\frac{x^{2n}}{(n!)^2}\bigg)e^x$$ where $e^x$ is for the rest box. Now, lets calculate three boxes have same number of elements using the same way. $$\binom{3}{3}\bigg[\frac{x^{19}}{19!}\bigg]\bigg(\sum_{n \geq1}\frac{x^{3n}}{(n!)^3}\bigg)$$ Then, the answer is $$\bigg[\frac{x^{19}}{19!}\bigg]\bigg[(e^x-1)^3-\binom{3}{2}\bigg(\sum_{n \geq1}\frac{x^{2n}}{(n!)^2}\bigg)e^x+\binom{3}{3}\bigg(\sum_{n \geq1}\frac{x^{3n}}{(n!)^3}\bigg)\bigg]$$ First of all I want you to check my method whether it is correct or not. Secondly,I want to simplify my generating functions into more closed form. I assume that $\bigg(\sum_{n \geq1}\frac{x^{3n}}{(n!)^3}\bigg)$ or $\bigg(\sum_{n \geq1}\frac{x^{2n}}{(n!)^2}\bigg)$ can be represented more closed form such as trigonometric , logarithmic or any others forms. Note: Any other easier methods are accepted, please make contribution to find more practical methods. $\color{red}{NOTE 2}$ : It would be nice to see generalized solution for n distinct balls into r distinct bins such that no bins contain same number of elemnt","['solution-verification', 'combinatorics', 'discrete-mathematics', 'generating-functions']"
4879594,What is the maximum number of non-zero entries of a matrix $A$ with non-negative entries that fulfills $A^2 = 0$,"A question I found and that I could not answer so far. Assuming that $A$ is a $n \times n$ matrix with non-negative entries, that fulfills the equation $A^2= 0$ , where $0$ is the zero matrix. What is the maximal number of positive entries for $A$ ? So far I have tried going through the specific entries, namely to solve $0 = \sum_{k=1}^n a_{i,k} \cdot a_{k,j}$ for all $i,j$ , but I was hoping for a simpler way. An example for $n=5$ has been $
\begin{bmatrix}
0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
$ which fulfills the equation, but I do not know if there are better possibilities","['matrices', 'graph-theory', 'linear-algebra']"
4879601,Strategy for Black&White game,"Consider the following game. Let $n$ be a positive integer. There are two players, $\newcommand\A{\mathrm{A}}\A$ and $\newcommand\B{\mathrm{B}}\B$ , and a referee. $\A$ and $\B$ first agree on a strategy. After this step, $\A$ and $\B$ cannot communicate. Initially, the referee chooses a sequence $\{c_i\}$ of length $n$ , where each entry is either “black” or “white”. $\A$ is made aware of this entire sequence, but $\B$ is not. There are a series of $n$ rounds, where $\A$ and $\B$ simultaneously guess either “black” or “white”. During the $k^\text{th}$ round, the team wins a point if $\A$ ’s guess and $\B$ ’s guess are both equal to the $k^\text{th}$ entry of the referee’s sequence. After each round, $\B$ is told what $\A$ ’s guess was, and what the referee’s pick was, for that round. This means $\B$ makes each decision while aware of all information from previous rounds. Find a strategy for $\A$ and $\B$ so that they can guarantee winning $g(n)$ times, where $\lim\limits_{n\to+\infty}\frac{g(n)}n=\frac34$ . I have a strategy for which $\lim\limits_{n\to+\infty}\frac{g(n)}n=\frac35$ . We will prove that $\A$ and $\B$ can win $3$ times in $5$ games. Here is the strategy. In the first game, $\A$ will guess the color that appears more in the subsequence $(c_2,c_3,c_4)$ , and $\B$ will guess this color in game $2$ to $4$ . If $c_2=c_3=c_4$ , then $\A$ and $\B$ can win games $2$ to $4$ (so $3$ games have been won). Otherwise, $\A$ and $\B$ will lose exactly one game among $2$ to $4$ . But $\A$ knows which game that will be, so they can guess $c_5$ in that game so that $\A$ and $\B$ win the fifth game. I have no strategy for which $\lim\limits_{n\to+\infty}\frac{g(n)}n=\frac34$ though.","['combinatorics', 'probability', 'information-theory']"
4879605,Orbits of a descending chain of subgroup actions,"Suppose a countable group $G$ acts transitively on a countable set $X$ . Let $G = G_0\supset G_1\supset\cdots$ be a descending chain of finite index subgroups. Suppose $x,y\in X$ lie in the same orbit of $G_i$ for all $i$ . Must they lie in the same orbit of $\cap_i G_i$ ? What if we assume that the stabilizer of $x$ is cyclic? Or that $G$ is residually finite? The case I’m particularly interested in is when $G$ is $PSL(2,\mathbb{Z})$ acting on the set of cusps $\mathbb{Q}\cup\infty$ .","['group-theory', 'abstract-algebra', 'combinatorics']"
4879631,"Is it possible to create triangle ∆ABC Depending on the points H,I,G?","There is an engineering question that occurred to me several years ago, but I could not solve it Assuming $H$ is the point of intersection of the heights, $I$ is the point of intersection of the bisectors, and $G$ is the point of intersection of the averages in triangle $∆ABC$ , and the points $H$ , $I$ , and $G$ are only data, is it possible that by relying on them we can create the vertices $A$ , $B$ , and $C$ using the construction of the ruler and compass? Please do not ask me to give my attempt to solve this question because it really does not show any insight or thread to reach the answer. I will be happy if someone can answer this question in the comments. Edit: I chose these three centers because they do not lie on a single line; Because I know that there would be no hope in the general case of encrypting a triangle uniquely with three collinear centers.
Yes, George Cantor's theory states that there is a correspondence between a line and a plane, but this opposition is not connected on any field However, the centers of the triangle follow the movement of the vertices in continuous paths, which prevents the possibility of a solution in cases such as taking the centers from Euler’s line.","['euclidean-geometry', 'geometry']"
4879649,A discussion on different versions of the Lebesgue Differentiation Theorem.,"The simplest formulation of the Lebesgue Differentiation Theorem (LDT) (that I am aware of) is the following: LDT (Simplest formulation). Given a function $f \in L^1_{\text{loc}}(\mathbb R^n)$ , we have that $$ \lim_{r \to 0} \frac{1}{|B(x,r)|} \int_{B(x,r)} f(y) \, dy = f(x), $$ for almost every $x \in \mathbb R^n$ . In this post , it is presented a version of the above LDT for open sets $\Omega \subset \mathbb R^n$ , which is stated as follows: LDT (Simplest formulation for open sets). Let $\Omega \subset \mathbb R^n$ be an open set. Given $f \in L^1_{\text{loc}}(\Omega)$ , we have that $$ \tag{1} \lim_{r \to 0} \frac{1}{|B(x,r)|} \int_{B(x,r)} f(y) \, dy = f(x), $$ for almost every $x \in \Omega$ . $\color{red}{\textbf{QUESTION.}}$ There is one concern I have about the last formulation I've presented. More precisely, I am wondering about the ""validity"" of the integral $$ \int_{B(x,r)} f(y) \, dy. $$ To explain my point I start by recalling that $f$ is only defined on $\Omega$ . Therefore, this integral only makes sense if $B(x,r) \subset \Omega$ . So essentially my question is pretty simple: how does one guarantee that in $(1)$ we are only considering $(x,r)$ such that $B(x,r) \subset \Omega$ ? Note. Obviously if we take $f(y)\chi_{\Omega}(y)$ in $(1)$ instead of $f(y)$ this solves the problem but this is not the formulation that is presented. Thanks for any help in advance.","['lebesgue-integral', 'real-analysis', 'functions', 'functional-analysis', 'limits']"
4879653,Finding the values of $x$ that satisfy $\sin x+\sin2x+\sin3x+\cdots+\sin nx\le\frac{\sqrt3}{2}$ for all $n$,"If the exhaustive set of $x\in(0,2\pi)$ for which $\forall n$ the inequality $$\sin x+\sin2x+\sin3x+\cdots+\sin nx\le\frac{\sqrt3}{2}$$ is valid is $l_1\le x\le l_2$ , find $l_1$ and $l_2$ . Let $\displaystyle\sum_{i=1}^n \sin(ix)$ = $S$ then I have managed to show that $$S=\frac{\sin\left(\frac{nx+x}{2}\right)\cdot\sin\left(\frac{nx}{2}\right)}{\sin\left(\frac{x}{2}\right)}$$ I do not know what to do next. How can I handle the case for all $n?$ Any help is greatly appreciated.","['inequality', 'trigonometry', 'sequences-and-series']"
4879688,"Number of compositions of $n$ into an odd number of part, each of which is at least $3$","I'm having trouble with the following question: Let $a_n$ be the number of compositions of $n$ into an odd number of parts, each of which is at least $3$ . Prove that $a_n = [x^n]\frac{x^3-x^4}{1-2x-x^2-x^6}.$ This is what i've done so far: Let $S=N_{\text{odd}\geq 3}^k$ where $N_{\text{odd}\geq 3}= \{3,5,7,...\}.$ Then we have that \begin{align}
[x^n]\Phi_S(x) &= [x^n]N_{\text{odd}\geq 3}^k(x) \\
               &= [x^n](N_{\text{odd}\geq 3}(x))^k \:\: \text{by product lemma} \\
               &= [x^n](x^3+x^5+x^7+...)^k \\
               &= [x^n](x^3(1+x^2+x^4+...))^k \\
               &= [x^n](\frac{x^3}{1-x^2})^k \:\: \text{by geometric series}
\end{align} Since we want an odd number of parts where eacht part is at least $3,$ we obtain $(\frac{x^3}{1-x^2})^3 + (\frac{x^3}{1-x^2})^5 + (\frac{x^3}{1-x^2})^7 + ...$ I have no clue on how to continue from here to obtain the expression I want to prove. Maybe again some geometric series? Did I even this the previous part right. Hints and/or solutions are very welcome.",['combinatorics']
4879796,Generalization of the fact that pre-images of a function preserve more set operations than images,"Given a function $f: A \rightarrow B$ images of subsets of $A$ preserve only inclusion and union of sets whereas pre-images of subsets of $B$ are better behaved, so to speak, and preserve (in addition to the above) intersections and differences of sets as well.
I have both the intuition and the proofs for the this statement. I am trying to understand what's the deeper truth behind this. I think that given a set $C$ , a collection of sets $\mathcal{D}$ , and a function $g:C\rightarrow\mathcal{D}$ with the property that for every $c1,c2 \in C$ we have that $c1\neq c2 \implies g(c1) \cap g(c2) = \emptyset$ then $g$ preserves all set operations on subsets of $C$ . The pre-image is clearly such a function from individual elements of (or singleton sets in) $B$ to the powerset of $A$ so it follows that the pre-image applied to arbitrary subsets of $B$ (not necessarily singletons) preserves set operations. Is this correct? Is this the deepest / most general intuition one can have as to why the pre-image of a function preserves more set operations than the image?","['elementary-set-theory', 'functions', 'relations']"
4879806,Summation with indexing variable in term [duplicate],"This question already has answers here : How to compute the  formula $\sum  \limits_{r=1}^d r \cdot 2^r$? (9 answers) Closed 4 months ago . I am trying to solve $$\sum_{j=0}^{n-1}j2^j$$ but I don't know how to proceed with the $j$ in front of $2^j$ . What is making it difficult for me is it is an indexing variable, so I don't think I can factor it out. How would I proceed? Thank you","['summation', 'discrete-mathematics']"
4879820,Suslin measurable sets and the smallest field containing all analytic sets,"Let $X$ be a Polish space. Recall that the Suslin operation is the operation $\mathcal{A}$ such that for any Suslin scheme $\{A_s : s \in \omega^{<\omega}\}$ of subsets of $X$ , we have: $$
\mathcal{A}\{A_s : s \in \omega^{<\omega}\} := \bigcup_{a \in \omega^\omega} \bigcap_{n<\omega} A_{a\upharpoonright n}
$$ The $\sigma$ -algebra of Suslin measurable $\mathcal{S}$ subsets of $X$ is the smallest $\sigma$ -algebra, containing all open subsets of $X$ , such that $\mathcal{S}$ is closed under the Suslin operation. Since every analytic set is the application of the Suslin operation to a Suslin scheme of closed subsets of $X$ , $\mathcal{S}$ contains all analytic subsets of $X$ . Is $\mathcal{S}$ exactly the smallest $\sigma$ -algebra containing all analytic subsets of $X$ ?","['measure-theory', 'descriptive-set-theory', 'polish-spaces', 'general-topology', 'set-theory']"
4879867,Cutting a square and gluing it back together,"I have a $1 \times 1$ square and I cut the triangle that has vertices at the center and bottom right and left corners of the square. Then I glue this triangle to the right edge of the square as shown in the picture: The task is to cut the second shape in two such that after rotating and maybe flipping those two shapes, we get the unit square back. The cut can be any curve that separates the shape into two pieces. For example, the cut shown by the color green trivially gives a square after we rotate the triangle back into the place where we cut it from.  The question is whether there are other cuts, and if they exist whether they are finite or infinite. By arguing that a distance larger than $\sqrt2$ cannot exist, I managed to reduce the possible curves to the ones starting at the Yellow line and ending at the Red zigzag as shown in the image above. But I couldn't go further. Also, a continuation of this problem would be to show the existence or finiteness of solutions given any regular polygon.","['analytic-geometry', 'triangles', 'geometry']"
4879908,"If we remove the diagonal from $X\times X$, is it necessarily disconnected?","If $X$ is a compact, connected Hausdorff space, we know that the diagonal $\Delta_X=\{(x,x)\in X\times X\}$ is closed in $X\times X$ by Hausdorffness. But is $X\times X\setminus\Delta_X$ disconnected in general? I know that, if $X$ has a total order $<$ such that the induced order topology is contained in the original topology (i.e. it is coarser), then we can write $$X\times X\setminus\Delta_X=\{(x,y)\in X\times X:x<y\}\sqcup\{(x,y)\in X\times X:x>y\}$$ Also, if $\Delta_X$ is a zero-set , then we also have the disconnectedness of $X\times X\setminus\Delta_X$ since, if $\Delta_X=f^{-1}[\{0\}]$ , then we have the disjoint open sets $f^{-1}[(-\infty,0)],f^{-1}[(0,+\infty)]$ . But we don't have that $\Delta_X$ is a zero-set in general as $X$ may not be first countable and $\Delta_X$ being $G_\delta$ implies $X$ first countable for compact Hausdorff spaces .","['connectedness', 'general-topology', 'examples-counterexamples', 'compactness']"
4880016,Property of $p$-variations in probability,"Let $X$ be a continuous process. Suppose that for some $p, t > 0$ , the $p$ -th variation over $[0, t]$ exists and is finite: along any sequence of partitions $\pi_n = \{0 = t_0 < t_1 < \cdots < t_{m_n} = t\}$ of $[0, t]$ with mesh size $\|\pi_n\| \to 0$ , $$\sum_{t_k \in \pi_n} |X_{t_k} - X_{t_{k-1}}|^p \to L_t, \quad n \to \infty,$$ where $L_t$ is almost surely finite and the convergence holds in probability. Prove that for $p' > p$ , the $p'$ -th variation of $X$ over $[0, t]$ equals $0$ ) almost surely. Moreover, prove that for $p' < p$ , the $p'$ -th variation equals $+\infty$ on the set $\{L_t > 0\}$ . I have an attempted solution for the first part, and it is as follows. Given the $p$ -th variation over $[0, t]$ converges to $L_t$ for some $p, t > 0$ , we consider a partition $\pi_n = \{0 = t_0 < t_1 < \cdots < t_{m_n} = t\}$ of $[0, t]$ with mesh size $\|\pi_n\| \to 0$ . For $p' > p$ , we examine the $p'$ -th variation: $$V^{p'}_n = \sum_{t_k \in \pi_n} |X_{t_k} - X_{t_{k-1}}|^{p'}.$$ Given $p' > p$ and $|X_{t_k} - X_{t_{k-1}}| \geq 0$ , properties of exponents give us: $$|X_{t_k} - X_{t_{k-1}}|^{p'} < |X_{t_k} - X_{t_{k-1}}|^{p} \text{ for } |X_{t_k} - X_{t_{k-1}}| < 1.$$ As the process is continuous and $\|\pi_n\| \to 0$ , for a sufficiently large $n$ , $|X_{t_k} - X_{t_{k-1}}|$ becomes smaller for all intervals in the partition. Thus, as $n \to \infty$ , terms in the $p'$ -th variation are dominated by those in the $p$ -th variation, which converges to $L_t$ . Since the terms in the $p'$ -th variation are smaller, it follows that $V^{p'}_n \to 0$ almost surely as $n \to \infty$ . However, I'm unable to start on the second, and I would appreciate either a proof check of the first part or a proof for the second.",['probability-theory']
4880045,Algebraic equation of a specific six-degree polynomial,"During my research, I was analyzing a linear system with characteristic polynomial of six order, $p(x)=x^6+3ax^5+3a^2x^4+a^3x^3 - a^3b_1b_3b_3$ . I used Mathematica and it found all roots as the following: I was quite impressed since it is not obvious for me how to factor this equation. I am curious how Mathematica find these solutions? What is the algorithm? Thank you for reading!","['algebra-precalculus', 'abstract-algebra']"
4880049,"Only $x_{6n}, y_{6n}$ doesn't have the same simplest denominator","I'm studying this series \begin{align*}
a_1=\mathrm{i}, \quad a_{n+1}=\mathrm{i} + \frac{\mathrm{i}}{a_n},
\end{align*} where $\mathrm{i}$ is the imaginary unit, in order for me to see clearly the structure of this sequence, I separate the real part and the imaginary part as a sequence, as follows. \begin{align*}
x_{n+1} = \frac{y_n}{x_n^2+y_n^2}&, \quad y_{n+1}=\frac{x_n}{x_n^2+y_n^2}+1, \\
&a_n = x_n + y_n\mathrm{i},
\end{align*} with $x_1=0, y_1=1$ , when I try to find the first few patterns, I found Only $x_{6n}, y_{6n}$ doesn't have the same simplest denominator The following are $x_n$ , $y_n$ for the first 20 items ( $x_n$ is on the left and $y_n$ is on the right) and has undergone preliminary programming verification on a larger scale. But I don't really know how to prove this, Can anyone give me an idea or prove this wrong? Addition Information If we consider a more complex situation, given two nonnegative integers X and Y, make the following sequence: \begin{align*}
x_{n+1} = \frac{y_n}{x_n^2+y_n^2}+X&, \quad y_{n+1}=\frac{x_n}{x_n^2+y_n^2}+Y, \\
\end{align*} with $x_1=0, y_1=1$ , The corresponding recursive expression of the complex sequence is \begin{align*}
a_{n+1}=X+Y\mathrm{i}+\frac{\mathrm{i}}{a_n}
\end{align*} For ease of description, we denote $p_k$ as the number of terms in $x_n, y_n$ where the $k^{th}$ simplest denominator is different. For instance, for the initial case of $X=0, Y=1$ , we have $p_1=6, p_2=12, \cdots$ . We also denote $\Delta_{p_n} = p_{n+1} - p_n$ . In the initial case, our conjecture is to prove that $p_n$ forms an arithmetic sequence with a common difference of $6$ , or $\Delta{p_n}=6$ . To explore the general case, a natural idea is that $p_n$ could potentially be a combination of terms from multiple arithmetic sequences, for example, $p_{2k+1}=4k+3, p_{2k}=2k+2$ . Of course, in this scenario, $p_n$ would not form an arithmetic sequence, but ${\Delta p_n}$ would have a periodic sequence (this is intuitively easy to prove. For two arithmetic sequences with periods $a, b$ where $a \nmid b$ and $b \nmid a$ , it seems that the period of ${\Delta p_n}$ is $\frac{a+b}{gcd(a,b)}-1$ ). Through programming, I tested the potential periods for $p_n \leq 5000, 0 \leq X \leq 14, 0 \leq Y \leq 16, X \neq Y$ , The parameter table corresponding to periodic $\Delta p_n$ is as follows: X Y period $\Delta p_n$ items within a period 0 1 1 [6] 0 2 1 [24] 0 4 1 [15] 0 7 6 [6, 3, 3, 6, 6, 6] 0 8 2 [125, 50] 0 13 0 [6, 3, 3, 6, 6, 6] 1 9 2 [588, 504] 1 13 2 [183, 488] 2 0 7 [5, 10, 5, 5, 5, 5, 10] 2 3 23 [2, 10, 12, 8, 4, 11, 1, 12, 12, 5, 7, 8, 4, 12, 12, 2, 10, 5, 7, 12, 11, 1, 12] 3 6 1 [12] 5 2 1 [12] 5 11 2 [24, 36] 5 14 16 [5, 7, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 7, 12, 12, 12] 6 5 180 [6, 4, 2, 6, ..., 6, 6, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6] 7 0 15 [6, 12, 6, 6, 6, ..., 6, 6, 6, 6, 6] 7 6 111 [6, 6, 6, ..., 6, 6, 6, 3, 3, 6] 7 15 2 [75, 100] 8 9 55 [6, 6, 12, 3, ..., 6, 6, 12] 8 11 57 [12, 12, 7, 5, ..., 12, 12, 12, 12] 8 16 2 [244, 427] 9 7 7 [5, 5, 10, 5, 10, 5, 5] 10 13 61 [12, 12, 12, ..., 12, 3, 9, 12] 11 5 2 [84, 1008] 12 14 2 [48, 120] 13 6 1 [12] 13 9 2 [24, 144] 13 16 16 [12, 12, 12, ..., 6, 6, 12] 14 6 1 [15] For entries not included in the table, it may be the case that no period exists, or alternatively, the period might be too large to have been identified. Additionally, regarding Eric's discovery, verification found it to be true only when $X=0, Y=1$ . As for the findings of Ivan Neretin, similar properties were observed across different $X, Y$ values, particularly evident when $X=0, Y=3$ (in this case, the period of this property is $6$ ). However, due to difficulties in programming implementation, no tables were produced to illustrate this. Sadly, despite all the discoveries mentioned above, a reliable method to prove them has yet to be found.","['algebra-precalculus', 'recurrence-relations', 'discrete-mathematics', 'sequences-and-series']"
4880109,"$AA^\top + A + A^\top = 0$, then $|\det A|\leq 2^n$","Let $A\in \mathbb R^{n\times n}$ such that $AA^\top + A + A^\top = 0$ . Prove that $|\det A|\leq 2^n$ . $AA^\top + A + A^\top = 0$ rewrites as $(A+I_n)(A^\top +I_n) = I_n$ , hence $A+I_n$ is an orthogonal matrix and $\det(A+I_n)\in \{-1,1\}$ . If $\lambda$ is a real eigenvalue of $A$ (with an eigenvector in $\mathbb R^n$ ) then $\lambda\in \{0,-2\}$ . However $A$ may have complex eigenvalues, or the eigenvectors may have complex entries. I cannot make further progress. I'm not supposed to know that an orthogonal matrix can be diagonalized over $\mathbb C$ with eigenvalues having modulus $1$ , I'm thus looking for a solution which does not leverage this fact.","['matrices', 'inequality', 'determinant', 'linear-algebra']"
4880115,Why is $P_{\rho}$ is a probability measure on the Borel subsets of $H\ $?,I am going through a paper on Operator Probability Theory by Stan Gudder. The author introduced the notion of probability distribution of self-adjoint operators on a Hilbert space where the self-adjoint operators are thought of as complex valued random variables relative to a fixed state. Let $A \in \mathcal S (H)$ (self-adjoint operator) and $\rho$ be a state on $H.$ Let $P^A$ be the spectral measure corresponding to the self-adjoint operator $A.$ Then for a Borel subset $\Delta \subseteq \sigma (A)$ (spectrum of $A$ ) we define $$P_{\rho} (A \in \Delta) = \text{tr} \left (\rho P^A (\Delta) \right ).$$ So the expectation of $A$ is given as $:$ $$E_{\rho} (A) = \int_{\sigma (A)} \lambda\ \text{tr} \left (\rho P^A (d \lambda) \right ) = \text{tr} (\rho A).$$ But I don't understand why $P_{\rho}$ is a valid probability measure. Also I don't follow why $E_{\rho} (A)$ evaluates to $\text {tr} (\rho A).$ Any suggestion in this regard would be warmly appreciated. Thanks for your time.,"['operator-algebras', 'operator-theory', 'probability-distributions', 'expected-value', 'probability-theory']"
4880216,UMVUEs for the means of $3$ independent normal distributions with the sum of means being $1$,"Let $\theta_1, \theta_2$ and $\theta_3$ be nonnegative parameters with the constraint $\theta_1+\theta_2+\theta_3=1$ . We observe $X_{i 1}=\theta_1+\epsilon_{i 1}, X_{i 2}=\theta_2+\epsilon_{i 2}, X_{i 3}=\theta_3+\epsilon_{i 3}$ for $i=1,2, \ldots, n$ , where $\epsilon_{i k} \sim N(0,1)$ are independent normal random variables $(k=1,2,3)$ . Derive the UMVUE for $\theta_1$ . My problem in solving it is, I don't know how to accurately use the constraint $\theta_1+\theta_2+\theta_3=1$ , and I also don't have any idea what  steps would it be like when finding the UMVUE like this. Can you provide any suggestion/solution for me? Thank you!","['statistical-inference', 'statistics', 'umvue', 'probability-theory', 'probability']"
4880227,Mathematical definition of probability current,"Probability currents play a role in Quantum Mechanics and Statistical Mechanics. There is a Wikipedia page on such a concept, where I read In quantum mechanics, the probability current (sometimes called probability flux) is a mathematical quantity describing the flow of probability. I would like to know if there is a related concept with a clean mathematical definition (even an implicit definition) within probability theory. I can find only physics-oriented bibliographies on this subject.","['probability-distributions', 'probability-theory', 'probability']"
4880352,Derivative between topological vector spaces - equivalence in definition of tangent to $0$,"In Serge Lang's Differential and Riemannian manifold, he defined derivative between two topological vector spaces with the concept tangent to $0$ : Let $\mathbf{E}, \mathbf{F}$ be two topological vector spaces, and $\varphi$ a mapping of a neighborhood of $0$ in $\mathbf{E}$ into $\mathbf{F}$ . We say that $\varphi$ is tangent to 0 if, given a neighborhood $W$ of 0 in $\mathbf{F}$ , there exists a neighborhood $V$ of 0 in $\mathbf{E}$ such that $$
\varphi(t V) \subset o(t) W
$$ for some function $o(t)$ . If both $\mathbf{E}, \mathbf{F}$ are normed, then this amounts to the usual condition $$
|\varphi(x)| \leqq|x| \psi(x)
$$ with $\lim \psi(x)=0$ as $|x| \rightarrow 0$ .
Let $\mathbf{E}, \mathbf{F}$ be two topological vector spaces and $U$ open in $\mathbf{E}$ . Let $f: U \rightarrow \mathbf{F}$ be a continuous map. We shall say that $f$ is differentiable at a point $x_0 \in U$ if there exists a continuous linear map $\lambda$ of $\mathbf{E}$ into $\mathbf{F}$ such that, if we let $$
f\left(x_0+y\right)=f\left(x_0\right)+\lambda y+\varphi(y)
$$ for small $y$ , then $\varphi$ is tangent to 0 . It then follows trivially that $\lambda$ is uniquely determined, and we say that it is the derivative of $f$ at $x_0$ . We denote the derivative by $D f\left(x_0\right)$ or $f^{\prime}\left(x_0\right)$ . It is an element of $L(\mathbf{E}, \mathbf{F})$ I think I might understand the intuition of this tangent to 0 concept. However, why is this definition equivalent to the definition stated in the normed space case? I don't really see how to write a proof to show these definitions are equivalent. $\|\varphi(x)\|_{F}\leq \|x\|_{E} \psi(x)$ is the definition I know before, but I'm having trouble to relate to the general definition, where it specifically refers to ""given a neighborhood $W$ "". So why are these two definitions equivalent?","['riemannian-geometry', 'functional-analysis', 'manifolds', 'differential-topology', 'differential-geometry']"
4880379,"""Encode"" all $n$-permutations with the fewest number of swaps","The goal is to find $m$ swaps $s_1, s_2, \dots, s_m$ such that any $n$ -permutation can be encoded as a binary sequence of length $m$ , $x_1, x_2, \dots, x_m$ , where $x_i$ indicates whether to perform the $s_i$ swap (if $x_i=1$ ) or skip it (if $x_i=0$ ). For example, if we list the swaps as $s_1=(1,2),\; s_2=(2,3),\; s_3=(3,1)$ , then the permutation from $ABC$ to $CAB$ can be encoded as $1,0,1$ : first swap the $(1,2)$ elements to get $BAC$ , then skip the swap of $(2,3)$ elements because $x_2=0$ , and finally swap the $(3,1)$ elements to get $CAB$ . Is there a way to make $m$ as small as possible (perhaps as close as possible to $\log_2{n!}$ ) while ensuring that every $n$ -permutation can be represented? Background information: This problem stems from my attempt to construct a ""permutation circuit"". This circuit accepts $n$ elements as input and produces a permutation of these elements as output. The circuit exclusively utilizes a component known as a ""swap"" gate. This gate takes two elements as input and is controlled by an additional input bit. If this bit is set to $1$ , the gate swaps the two input elements before outputting them. If the bit is set to $0$ , the gate outputs the input elements directly without swapping. My goal is to construct such a circuit using the fewest possible number of these gates, enabling it to generate any permutation of these $n$ elements. enter image description here","['permutations', 'coding-theory', 'discrete-mathematics', 'computer-science']"
4880413,Does every triangle satisfy $a^c + b^c - c^c < \pi$,"Let $(a,b,c)$ be the sides of a triangle and let its circumradius be $1$ . Is it true that $$
a^c + b^c - c^c < \pi
$$ My progress : In the special case where at least two of the three sides are equal, I have been able to show that the upper bound is about $3.13861$ . If two sides are equal and the third side is $c = x$ then the length of the two equal side are $a = b = \sqrt{2 + \sqrt{4-x^2}}$ each. Maximizing the expression using Wolfram Alpha $$
2\left(\sqrt{2 + \sqrt{4-x^2}}\right)^x - x^x
$$ gives $3.13861$ as the upper bound in this case. Further more, simulation show that this is also the unconditional maxima but I have not been able to prove it. Since $3.13861$ is very close to $\pi$ so, I expressed the above inequality in terms of $\pi$ to present it in an elegant form but it seems the proximity is just a coincidence unless I am missing something.","['inequality', 'geometry', 'maxima-minima', 'triangles', 'algebra-precalculus']"
4880522,Symmetry group of the unit circle in $\mathbb{R}^2$ versus $\mathbb{R}/\mathbb{Z}$.,"I am studying a set of lecture notes on group theory, and I don't think I understand a point the author makes about the unit circle and its symmetry group in relation to $\mathbb{R}/\mathbb{Z}$ . Let $S^1$ denote the set of $(x,y) \in \mathbb{R}^2$ with $x^2 + y^2 = 1$ . (This could be complex, I believe, and we'd arrive at the same conclusion. That could be the missing link my understanding.) The symmetry group of $S^1$ consists of rotations and reflections, which is infinite. I cannot fully visualize this, but my mental picture is that it takes a point $(x,y)$ on the circle and either moves it counterclockwise along the unit circle (in polar coordinates, the value of $\theta$ increases mod $2\pi$ ) or reflects it in any line through the origin. The author claims that the rotation of $S^1$ is isomorphic to $\mathbb{R}/\mathbb{Z}$ . I'm not certain I fully understand this. I believe the complex unit circle is isomorphic to $\mathbb{R}/\mathbb{Z}$ with isomorphism given by (denoting the complex unit circle by $C^1$ ) $$
f: \mathbb{R}/\mathbb{Z} \to C^1, \; t \mapsto \exp(2\pi t i).
$$ Here I am thinking of $\mathbb{R}/\mathbb{Z}$ as the set $[0,1)$ , so varying $t$ amounts to varying the polar angle in the complex plane. Since $\mathbb{C} \cong \mathbb{R}^2$ as a vector space, it seems to me that this conclusion should be unchanged. But then $\mathbb{R}/\mathbb{Z}$ is isomorphic to both the unit circle in $\mathbb{R}^2$ and the group of rotations of the unit circle, so the unit circle is isomorphic to its rotation group. This doesn't make sense to me, so something must be wrong with my understanding.","['symmetric-groups', 'group-theory', 'group-isomorphism']"
4880537,Show that $\int_0^\pi\arctan\left(\frac{a\cos x+b\sin x}{c\cos x+d\sin x}\right)dx=\pi\arctan\left(\frac{ac+bd}{|bc-ad|+c^2+d^2}\right)$,"Given that $a,b,c,d\in\mathbb{R}$ and that $c$ and $d$ are not both $0$ , show that $$\int_0^\pi\arctan\left(\frac{a\cos x+b\sin x}{c\cos x+d\sin x}\right)\mathrm dx=\pi\arctan\left(\frac{ac+bd}{|bc-ad|+c^2+d^2}\right).$$ I was trying to answer a question and ended up with this integral. I don't think it helped me answer that other question, but I thought this integral is interesting by itself. I teased out the RHS expression by first noticing that $\tan \left(\frac{1}{\pi}\int_0^\pi\arctan\left(\frac{a\cos x+b\sin x}{c\cos x+d\sin x}\right)\mathrm dx\right)$ seemed to be always rational when $a,b,c,d\in \mathbb{Z}$ , then setting two of $a,b,c,d$ equal to $1$ and then looking for patterns in the value of the integral in terms of the other two. That's all the progress I've made so far.","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'trigonometry']"
4880544,Writing $\exp{tA}$ as finite sum,"Let $A \in \mathcal{M}_{n\times n} (\mathbb{R})$ such that $A^2=\alpha A$ for some $\alpha \neq 0$ . Under this assumption, we have by induction that $A^{n}=\alpha^{n-1}A$ ; $n \geq2$ ; then: $$\exp(tA)=I+ tA+\frac{t^2A^2}{2!}+ \dots \ =I+At+\frac{A\alpha t^2}{2!} + \frac{A\alpha^2 t^3}{3!} + \dots = \\ = I+ \frac{1}{\alpha}A(1+ \alpha t+ \frac{\alpha^2 t^2}{2!} +\dots)-  \frac{A}{\alpha}=I+\frac{1}{\alpha}A\exp({\alpha t})- \frac{A}{\alpha}=I+A(\frac{\exp(\alpha t)-1}{\alpha})
$$ Are these steps correct?","['matrix-exponential', 'ordinary-differential-equations']"
4880639,Proving that $\text{SL}_2 (\mathbb{Z})$ is closed under inverses,"I am trying to verify that $\text{SL}_2 (\mathbb{Z})$ is a group under matrix multiplication. The only property I am not certain on is closure under inverses. Given $A \in \text{SL}_2 (\mathbb{Z})$ , I know that $A^{-1}$ exist (it has non-zero determinant). I need to show that $A^{-1}$ has determinant $1$ and integer entires. It's true in general that $\det(A^{-1}) = \frac{1}{\det(A)}$ , so if $\det(A) = 1$ , then $\det(A^{-1}) = 1$ . Proving that $A^{-1}$ has integer entries is a bit tougher, because I don't think it is necessarily true that if $\det(A^{-1}) = 1$ , then all of its entries are integers (though I struggle to think of a counterexample). The only proof I could think of involves the adjugate matrix, $\text{adj}(A)$ . In general, we have $$
A \text{adj}(A) = \det(A) I$. 
$$ As $\det(A) = 1$ , the inverse of $A$ is the adjugate matrix. The adjugate matrix is the transpose of the matrix of cofactors, whose entries are partial determinants around entry $(i,j)$ , up to a sign. The entries of $A$ are integers, so every entry of the matrix of cofactors is an integer ( $\mathbb{Z}$ is closed under addition and multiplication), so the adjugate matrix contains only integers. Is this reasoning sound?",['group-theory']
4880668,How to show a function is holomorphic,"Let $f:\mathbb{D}\to\mathbb{D}$ be a holomorphic function s.t $0$ is a
zero of order $k\geq 1$ .  Prove that $f(z)=z^kg(z)$ , in which $g:\mathbb{D}\to\mathbb{D}$ is holomorphic. My attempt: Since $f$ is holomorphic on $\mathbb{D}$ , then by Taylor's expansion we have $$f(z)=\sum\limits_{n=0}^{\infty}c_nz^n,\quad c_n=\dfrac{f^{(n)}(0)}{n!}.$$ Since $0$ is zero of order $k\geq 1$ , we have $c_1=...=c_{k-1}=0, c_k\neq 0$ . $\Rightarrow\quad f(z)=z^kg(z)$ , in which $g(z)=c_k+c_{k+1}z+...$ My question is how can I show such $g$ is holomorphic on $\mathbb{D}$ and $|g(z)|<1$ ? Could someone help me? Thanks in advance!",['complex-analysis']
4880719,Real-valued dimension,"Let $\overline{\mathbb{R}}_{\geq 0} = \mathbb{R}_{\geq 0} \cup \{\infty\}$ . Given a commutative ring with unity $R$ , $R\operatorname{-Mod}$ denotes the category of $R$ -modules. Given $R$ -modules $A$ and $B$ , $A \cong B$ denotes isomorphism of $A$ and $B$ as $R$ -modules. Does there exist an example of the following? A commutative ring with unity $R$ A mapping $\operatorname{d}: R\operatorname{-Mod} \to \overline{\mathbb{R}}_{\geq 0}$ with the following properties: If $A \cong B$ , $\operatorname{d}(A) = \operatorname{d}(B)$ $\operatorname{d}(\{0\}) = 0$ $\operatorname{d}(R) = 1$ $\operatorname{d}(A \oplus B) = \operatorname{d}(A) + \operatorname{d}(B)$ $\operatorname{d}(A \otimes_R B) = \operatorname{d}(A) \cdot \operatorname{d}(B)$ If $A \subseteq B$ is a submodule, then $\operatorname{d}(A) \leq \operatorname{d}(B)$ If $A \subseteq B$ is a submodule, then $\operatorname{d}(B/A) \leq \operatorname{d}(B)$ If $A \subseteq B$ is a submodule, then $\operatorname{d}(A) + \operatorname{d}(B/A) = \operatorname{d}(B)$ $\operatorname{d}$ is surjective The idea is that $\operatorname{d}$ would be analogous to the dimension $\operatorname{dim}$ in the case that $R$ is a field. However, fields do not satisfy the condition since then $\operatorname{dim}$ only takes values in $\overline{\mathbb{N}}_{\geq 0}$ and is therefore not surjective as a mapping to $\overline{\mathbb{R}}_{\geq 0}$ .","['abstract-algebra', 'dimension-theory-algebra']"
4880731,"One more time, ZF based proof that set of all sets does not exist","I know it has been asked several times, but there is always one step that I don't see. My argumentation goes as it follows: Given the set $\mathbb{V}$ set of all sets, then $V \in V$ . Because of the Axiom of Schema of Specfication we can define a certain property $P(x) = x \not\in x$ , and prove that $\exists A \forall x, A = \{x \in V : x \not\in x\}$ We can prove that this is a contradiction by checking if $A \in A$ (since $A \in V$ but $A \not\in A$ , but then if $A \not\in A$ then $A \in A$ , which is a contradiction). This proves that such set $A$ does not exist, but I cannot see why can we conclude that this proves that $\nexists V$ , since we could have started the analysis with any other set than $V$ (this is, the proof is independent of the initial set) that contains $A$ .","['elementary-set-theory', 'axioms', 'set-theory']"
4880749,One variable equation on real numbers,"Find the real solutions of the following equation $$\frac{4x-1}{x^2-2x+2}+\frac{4x+7}{x^2+2x+2}=\frac{4(2x+3)}{x^2+2}.$$ I think the problem was given in a junior olympiad. My best idea was to denote $x-1=a$ , $x+1=b$ and after adding $2$ the LHS becomes $$\frac{(a+2)^2}{a^2+1}+\frac{(b+2)^2}{b^2+1}$$ which looks like a Cauchy-Schwarz inequality. Unfortunately, I couldn't find a good form for the RHS. I also tried $x^2-2x+2=a$ , $x^2+2x+2=b$ which leads to a somewhat nice looking $$\frac{b-1}{a}+\frac{7-a}{b}=\frac{4(b-a+6)}{a+b}$$ but the computations are not nice. By plotting, there are four solutions among which are $\pm\sqrt{2}$ .","['contest-math', 'real-numbers', 'algebra-precalculus']"
4880787,"Evaluating $\lim_{(x, y) \rightarrow (0, 0)} \frac{x^2 y + x \sin y}{\sqrt{x^2 + y^2 - x y}}$","I'm trying to find the following limit: $$\lim_{(x, y) \rightarrow (0, 0)} \frac{x^2 y + x \sin y}{\sqrt{x^2 + y^2 - x y}}$$ In both iterated limits I have obtained $0$ . I have also tried to get the limit in the line $y = x$ , and I also get $0$ . I don't know how to deal with that numerator. I have also tried: $$\frac{x^2 y + x \sin y}{\sqrt{x^2 + y^2 - x y}} = \frac{x (x y + \sin y)}{\sqrt{x^2 + y^2 - x y}} = \frac{x y (x + \frac{\sin y}{y})}{\sqrt{x^2 + y^2 - x y}}$$ I would like to use something like $|x| \le \sqrt{x^2 + y^2}$ and $|y| \le \sqrt{x^2 + y^2}$ , but the negative term in the square root doesn't allow me to use this method.","['limits', 'multivariable-calculus', 'real-analysis']"
4880791,Tricky integral $\int_0^\pi{12\cos x\ \mathrm{sech}(\frac \pi2 \tan\frac x2)}\mathrm{d}x=\pi^2$,"I need to show that the following tricky integral: $$\int_0^\pi{12\cos x\ \mathrm{sech}\left(\frac {\pi}2\tan\frac x2\right)}\mathrm{d}x$$ is equal to exactly $\pi^2$ . I have no idea how to start. I tried the substitution $u=\tan\frac x2$ and ended up with: $$\int_0^\infty24\ \mathrm{sech}\frac{\pi u}{2}\frac{1-u^2}{(1+u^2)^2}\mathrm{d}u$$ I might have to use residue theorem or Fourier transformation here, but I'm lost. Thank you!","['integration', 'definite-integrals', 'fourier-transform', 'calculus', 'trigonometric-integrals']"
4880792,"Character table of groups of order 96, coincidence or there is a reason?","It is well known that every column sum in the character table of a finite group is an integer.
Nevertheless, it is not easy to find an example where a column sum is negative. The smallest example has order 96.
For example, one can take SmallGroup(96,3) in the notation of GAP, whose character table is presented under https://people.maths.bris.ac.uk/~matyd/GroupNames/73/C2%5E3.3A4.html The column sum for the involution class 2B is negative. On the other hand, it is also well known that not every element in the commutator subgroup of a group is a commutator. It is not
easy to find a counterexample either and the smallest example has also order 96. Moreover, one can take the same group
SmallGroup(96,3), and it is the same conjugacy class 2B which lies in the commutator subgroup, but does not consist of commutators!
The reason is the well known criterion of Burnside that an element is a commutator
if and only if $\sum_{\chi\in Irr(G)}\frac{\chi(g)}{\chi(1)}\ne 0$ . I cannot believe myself that this is a pure coincidence. Do you see an explanation for it?","['group-theory', 'characters']"
4880797,A few questions regarding random points on a disk.,"Today a few questions popped up in my head, and I'm curious to know the answers. a). Let $C$ be a disk of radius $R$ centered at the origin. Given a point on the disk $P = (x,y)$ , what is the expected distance between $P$ and $P'$ , where $P'$ is a point chosen uniformly at random somewhere on the disk. b). Let $C,R,P $ and $P'$ be as above. What is the probability that the distance between $P$ and $P'$ is $\geq r$ , with $r\in [0,2R]$ . Help with either would be greatly appreciated, thanks in advance!","['statistics', 'probability-distributions', 'probability']"
4880879,"The Area bounded by $y=e^x, 2y+2k-2kx-1-e^2=0$ is minimum for $ k=k_1$","The Area bounded by $y=e^x, 2y+2k-2kx-1-e^2=0$ is minimum for $ k=k_1$ ; then (where [.] denotes greatest integer function) A. $[2k_1]\gt5$ $\qquad$ B. $|2k_1-2\pi|\lt1$ $\qquad$ C. $k_1 \in \mathbb I$ $\qquad$ D. $k_1=\frac pq; p, q$ are integers $q\not=0,1, {-1}$ my attempt I had drawn the graphs of the exponential function and the the straightline and found that for $k\lt0$ we have $\infty$ area enclosed so $k$ must be $\gt0$ .  and also the line $$\frac y{\frac{e^2+1}2-k}+ {x\over1-\frac{e^2+1}{2k}}=1$$ passes through a fixed point $P\equiv\left(1,\frac{e^2+1}2\right)$ so as $k$ varies from $0$ to $\infty$ the line rotates about this fixed point. and this point P is above the graph of $y=e^x$ . I reached upto this I could not make any further conclusion from here. I had a thought to find the points of intersection in terms of $k$ then integrate but to find the points of intersection seems to be impossible. Graph of the question","['integration', 'area', 'calculus', 'algebra-precalculus', 'derivatives']"
4880954,Probability of each type of inscribed octahedron,"Fix a $V\in\mathbb{N}$ with $V\ge 4$ . Randomly pick $V$ points on a sphere (independently and uniformly with respect to the surface area measure). You may think of the convex hull of these $V$ points. With probability 1, this constitutes a nondegenerate convex polyhedron. In fact, all faces would be triangles (probability 1)(*), so this should be a $(2V-4)$ -hedron (from Euler's $F-E+V=2$ when $3F=2E$ ). But what is the probability for each topological type? I think(**) the first ""interesting"" case is for $V=6$ random points on the sphere, so random octahedra, so let me ask about that. What is the probability that 6 randomly chosen points on a sphere span an octahedron which
is topologically like a regular octahedron (so all 6 vertices have same vertex order). Both an exact answer and an approximate value from simulating the random process would be interesting. Notes: (*) I learn that a polyhedron all of whose faces are triangles can be called a simplicial polyhedron . (**) A bit more research leads to the beautiful table Counting Polyhedra which confirms that out of the 257 distinct octahedra, there are 2 distinct ones with $V=6$ . One is the regular octahedron. The other one seems to be called the biaugmented tetrahedron or bicapped tetrahedron although I am not sure if it has other names.","['solid-geometry', 'polyhedra', 'geometric-probability', 'probability']"
4880982,"Deciphering Salvador Dali's geometric formula $\frac{R}{2} \sqrt{n-2 \sqrt{5}}$ in ""Leda Atomica""","Salvador Dali applied in his 1947–1949 painting "" Leda Atomica "" a concept that relates to the golden ratio or its conjugate. It is well known that Dali used mathematical formulas to elaborate his works (e.g. golden ratio, catastrophe theory, etc.), this said, we are not talking here about a layman. On the bottom right of the preliminary sketch, he writes a mathematical formula, from which I have only this example with limited resolution: It seems to relate the pentagon/pentagram with the circle envelope via the conjugate of golden ratio: $\frac{R}{2} \sqrt{n-2 \sqrt{5}}.$ I used a placeholder $n$ , not being sure what exactly this might be, and suppose $R$ denotes the radius of the circle. My question is, what exactly does this mathematical formula describe specific in relation to the geometry in the painting, or generally in geometric terms? Please pay attention, that this question is purely mathematical, and please avoid any kind of interpretations in the context of art. Image source with permission of the author: https://medium.com/@vaseghisam/the-grand-equation-of-imagination-salvador-dal%C3%ADs-mathematical-surrealoscope-9a51d478cc61","['golden-ratio', 'number-theory', 'geometry', 'euclidean-geometry']"
4881017,What does exponential tilting actually do?,"I met the definition of exponential tilting when reading about Cramer's theorem, and am struggling about this definition: for every $\theta\in\mathbb R$ , we define a random variable $X_1^\theta$ with law given by $$\mathbb E[f(X_1^\theta)]:=\mathbb E[f(X_1)\frac{e^{\theta X_1}}{M(\theta)}]$$ for $f\geq 0$ measurable. What does this exponential tilting actually do? How does the new random variable look like? I tried to come up with a simple example: Example. $\theta=2$ and $f(x)=x$ . Then $\mathbb E[X\frac{e^{2X}}{M(2)}]=\mathbb E[X\frac{e^{2X}}{\mathbb E[e^{2X}]}]=\frac{\mathbb E[Xe^{2X}]}{\mathbb E[e^{2X}]}$ . So the expectation of the new random variable is equal to $\frac{\mathbb E[Xe^{2X}]}{\mathbb E[e^{2X}]}$ . But I still don't have the feeling of it. Could someone give a basic description and give intuition on how it makes Cramer's theorem works by tilting?","['probability-theory', 'probability']"
4881076,Property weaker than continuity,"Let $f:X\to Y$ be a map between matric spaces. I'm interested in the following property: For every $x\in X$ , there is a sequence $x_n\to x$ , with $x_n\ne x$ for every $n$ , such that $f(x_n)\to f(x)$ . Question: What's known about functions with this property? Does some well studied class of functions have it? Clearly every continuous function has the property above (if the space has no isolated points), but it can be violated by changing the value of the continuous function in one single point. On the other hand, sharply discontinuous functions can have the property, as for instance the Dirichlet function or even Conway base-13 function. Thanks.","['metric-spaces', 'sequences-and-series']"
4881077,Unclear step in the proof of Inverse Function Theorem,"There is one step the necessity of which I don’t understand in the proof of Inverse Function Theorem in Jerry Shurman’s $Calculus$ $and$ $Analysis$ $in$ $Euclidean$ $Space$ (available here https://www.professores.uff.br/diomarcesarlobao/wp-content/uploads/sites/85/2017/09/Multi_calculus_BOOK.pdf ). After having established injectivity of $f$ in some closed ball $\overline B$ around $a$ and introducing an open ball $W$ around $f(a)$ s.t. $|f(a) - y| < |f(x) - y|$ for every $y\in W$ and every $x\in \partial \overline B$ , the author goes on to prove, using Critical Point Theorem and Chain Rule, that for every $y\in W$ there is one and only one $x\in int\overline B$ s.t. $f(x) = y$ . Why do we need to do this, or, more concretely, why does the proof of this fact need to be so sophisticated? Why can’t we just take $V$ to be the pre-image of $W$ , i.e. $V = f^{-1}(W)$ (every point of $W$ has a pre-image in $\overline B$ , because $W$ is a subset of $f(\overline B)$ ; moreover, this pre-image should be unique due to injectivity of $f$ and not lie on the boundary of $\overline B$ by construction), and then $f$ becomes a bijective, and thus invertible map from V to W?","['inverse-function', 'analysis', 'multivariable-calculus', 'calculus', 'theorem-provers']"
4881095,De Rham isomorphism for relative and compactly supported cohomology,"I am currently reading up on cohomology with compact supports, and am looking for a reference as to whether the De Rham isomorphism $H_\textrm{DR}^*(M)\simeq H^*(M;\mathbb R)$ exists for the compactly supported cohomologies $H_\textrm{DR,c}^*(M)$ and $H_c^*(M;\mathbb R)$ as well. I think it probably does, because I suspect two more things to be true as well: that $H_\textrm{DR,c}^*(M)$ is the direct limit of relative cohomology groups $H_\textrm{DR}^*(M,M\setminus K)$ in the same way that $H_c^*(M;\mathbb R)$ is the direct limit of $H^*(M,M\setminus K;\mathbb R)$ , and that the De Rham isomorphism $H_\textrm{DR}^*(M)\simeq H^*(M;\mathbb R)$ exists for the relative cohomology groups $H_\textrm{DR}^*(M,M\setminus K)$ and $H^*(M,M\setminus K;\mathbb R)$ too. I have however not been able to find a source for that at all, neither for the De Rham isomorphism on compactly supported cohomology nor for the two weaker statements that together would imply it. The only thing I was able to find is this answer on MathOverflow, which refers to a similar-looking theorem in a book on sheaf theory that I don't really understand. Can anyone maybe point me to a more elementary proof, or outline one here?","['de-rham-cohomology', 'reference-request', 'homology-cohomology', 'algebraic-topology', 'differential-geometry']"
4881148,Cardinality of a set of random variables (measurable functions): $|L_0|=|L_p|=|L_\infty|$ for any $p >0$?,"How can we compare the cardinalities of   different sets of random variables on a given probability space $(\Omega, \mathcal F, P)$ . Is $|L_1|>|L_\infty|$ , while $L_\infty$ is dense in $L_1$ ? When does the following $$|L_p|=|L_\infty|$$ hold for all $p\ge1$ ? How large can be the set $L_0$ of all random variables compared to $L_\infty$ ? How much are we lucky if a given distribution has a finite mean or finite variance? How restrictive is if a statement only holds for random variables with mgf? How are these cardinalities connected to the cardinality of $\Omega$ , See Wikipedia for definition of $L_p$ spaces. Here, $L_0$ denotes the set of all random variables (Borel measurable functions) defined on the probability space. Specifically interested in two cases of $(\Omega, 2^\Omega)$ where $\Omega$ is a finite set and $(\mathbb R, \mathcal B(\mathbb R)).$ For the latter, from here , we know that $$|L_0|=|\mathbb R|,$$ whereas the cardinality of the set of all Lebesgue measurable functions is $|2^\mathbb R|$ (which shows how small is the set of random variables, but note that they have the same cardinality of $|\mathbb R|$ considering equivalence classes). From the above, I think we have $$|L_{\frac{1}{p}}|=|L_p|=|\mathbb R|$$ for all $p\ge1$ as $$|\mathbb R|=|L_0|\ge|L_{\frac{1}{p}}|\ge|L_p|\ge|L_\infty|\ge|\mathbb R|.$$ On the other hand, when $\Omega$ is finite, we have $$|L_0|=|L_p|=|L_{\frac{1}{p}}|=|L_\infty|$$ for any $p\ge1$ . Can we generalize these two observations as follows? 1- For any given $(\Omega, \mathcal F, P)$ , we always have $$|L_0|=|L_{\frac{1}{p}}|=|L_p|=|L_\infty|$$ for any $p\ge 1$ . If yes , then 2- How can we define a bijection (one-to-one correspondence) for each pair of these spaces? In question 1, I also guess considering equivalence classes does not change the result. PS: Related Stack Exchange questions on sets with $|\mathbb R|$ cardinality: Cardinality of set of real continuous functions Cardinality of the borel measurable functions Cardinality of the set of entire functions Cardinality of the set of Lebesgue measurable functions under equivalence The cardinality of the Riemann integrable functions is $|2^\mathbb R|.$ Cardinality of the set of Riemann integrable functions on [0,1]","['measure-theory', 'statistics', 'real-analysis', 'elementary-set-theory', 'probability']"
4881155,On a property for normed spaces,"I came upon the following specific property for a normed space $X$ , and I am looking for a characterization of the normed spaces where it holds true: If a sequence $x_n$ in $X$ satisfies $\displaystyle \lim_{n\to\infty}(\|x_n+y\|-\|x_n\|)=\|y\|$ for all $y\in X$ , then $\displaystyle \lim_{n\to\infty}x_n=0$ . This is not true in $l_1$ ; take $x_n=e_n$ , the unit vector basis. The same counterexample doesn't seem to work in $c_0$ , $l_\infty$ , and $l_p$ for $p\neq 1$ . Is this actually true in these spaces, or is this property, in fact, never satisfied?","['banach-spaces', 'normed-spaces', 'functional-analysis']"
4881201,Proving a criterion for flatness of modules,"I am following Qing Liu's textbook ""Algebraic Geometry and Arithmetic Curves,"" and have come upon the following statement (the truth of which is well-known): Theorem : Let $M$ be an $A$ -module. Then $M$ is flat if and only if for every ideal $I$ of $A$ , the canonical homomorphism $I \otimes_A M \rightarrow IM$ is an isomorphism. The proof goes as follows: first, Liu shows it for free modules of finite rank, then for free modules of arbitrary rank, then finally for general modules. The first two parts of the proof I have no trouble with. However, I am struggling to understand the justification for the final step. Here is how the proof goes: Let $N$ be an arbitrary $A$ -module, and suppose we have an injective map $N' \rightarrow N$ . There exists a free $A$ -module $L$ and a surjective homomorphism $p \colon L \rightarrow N$ . Set $L' = p^{-1}(N')$ . We have a commutative diagram: $\require{AMScd}$ \begin{CD}
\ker p @>>> L' @>>> N' @>>> 0 \\
@| @VVV @VVV \\
\ker p @>>> L @>>> N @>>> 0
\end{CD} whose horizontal lines are exact, whence... This is where I am stuck; I don't understand why we have a map $\ker p \rightarrow L'$ . The module that would go there such that the horizontal row is exact is $\ker[L' \rightarrow N']$ , but this module in particular contains only elements of $L' \subseteq L$ . It is easy to show that we can make the leftmost arrow (which is currently an equality) into an injection, but this does not help us since we are trying to prove that $M$ is flat. I do not see in general why $\ker [L' \rightarrow N'] \cong \ker [L \rightarrow N]$ .","['category-theory', 'algebraic-geometry', 'abstract-algebra', 'tensor-products', 'commutative-algebra']"
4881237,Clarification on a theorem of E. Michael [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . Improve this question In Lemma 7.3.1 in Topologies on spaces of subsets Michael states that if $X$ is a connected Hausdorff space and $f : \mathcal{F}_n(X) \to X$ is a selection, then $f(E)$ is the first (<) element of $E$ for every $E \in \mathcal{F}_n(X)$ , where $x<y$ if and only if $f(\{x,y\})=x$ . His proof goes as follows: My problem is in part (b). I think that if $y\in E$ , then it is not true that $E\in \mathfrak{M}'$ (since $y\not\in V$ ) and, therefore, $\mathfrak{M}'$ is not useful for verifying that $\mathfrak{E}$ is open as he says. If my reasoning is correct, I would please like to know if it is possible to fix Michael's proof, or if there is some other way to prove his original statement.","['proof-explanation', 'general-topology']"
4881252,Number of ways to sit six people at a circular table such that two of the people cannot sit immediately beside each other,"Problem: Six people are to sit at a circular table. Two of the people are not
to sit immediately beside each other. Find the number of ways that the
six people can be seated I'm really stuck on this question. The correct answer is $72$ , but my solutions seem to be incorrect (I get $432$ ways as an answer). I've tried to solve the problem in two ways, and these are my workings so far Method 1: Call the two restricted people $p_1$ and $p_2$ There are 6 choices for the first person $p_1$ to be seated, so hence $6$ ways. The second person $p_2$ cannot sit immediately beside $p_1$ , so there are $5-2 = 3$ ways the second person $p_2$ can sit. After sitting $p_1$ and $p_2$ , the remaining people can be seated in $4!$ ways. So the number of ways with $p_1$ not next to $p_2$ = $6 \cdot3\cdot4! = 432$ ways Method 2: With no restrictions 6 people can be seated in $6!$ ways. Again, there are 6 choices for the first person $p_1$ to be seated, so hence $6$ ways. There are 2 ways for $p_2$ to sit next to $p_1$ . The remaining people can be seated in $4!$ ways. The six may be seated (with $p_1$ and $p_2$ next to each other) in $6\cdot2\cdot4! = 288$ ways. So the number of ways with $p_1$ and $p_2$ not next to each other are $6!-288 = 432$ ways. What am I doing wrong?",['combinatorics']
4881375,Winding numbers in QCD and winding numbers in complex analysis. Is there a relation through a differential geometric generalization?,"I have a background in theoretical physics and the first time I came across winding numbers was in the context of the vacuum of QCD. By the way physicists treat this topic, I thought it had little to do with Winding numbers in complex analysis.  Now I, came across some results of complex Brownian motion applied to proofs of Picard's theorems in complex analysis, related to winding and tangling of curves, and also to similarstochastic techniques applied to path integral formulation of quantum mechanics and related QFT topics. Thus, I am asking myself if winding numbers in QCD are related to closed loops on a certain bundle related to the SU(3) group. The paths could be the brownian motion that provides an equivalent measure to the vacuum state.  I am very familiar with stochastic calculus applied to QFT, differential geometry and functional analysis,  what I am lacking to answer the question in the title is the interpretation of the QCD winding numbers in this sense. I have also been thinking about Berry phases  for U(1) groups, and in that case the Berry phase gained through closed loops in the base of a fibre bundle. Is this related to an analogous question to the one of the winding numbers of QCD for this (way simpler) abelian group? Thanks.","['winding-number', 'stochastic-calculus', 'differential-geometry']"
4881411,An example of a topology on $\mathbf{R}^2$ in which addition is discontinuous and multiplication by a scalar is continuous,"I am trying to build an example of a topology on $\mathbf{R}^2$ in which addition is discontinuous and multiplication by scalar is continuous. I think that such a topology is generated by the collection of all intervals on all straight lines passing through the origin $(0,0)$ . Then, for example, adding points $(1,1)$ and $(-1,1)$ gives a point $(0,2)$ , but we will not have neighborhoods of points $(1,1)$ and $(1,-1)$ whose sum is contained in a neighborhood of the point $(0,2)$ . But multiplication by a scalar in such a topology seems to me to be continuous, since for any neighborhood of the product $ax$ we can specify a corresponding neighborhood $x$ so that all points from it, when multiplied by $a$ , fall into the neighborhood of $ax$ . Namely, it seems to me that given an arbitrary neighborhood of $ax$ we can multiply by $1/a$ and get the necessary neighborhood of $x$ . Can you please tell me if my reasoning is correct?","['continuity', 'general-topology', 'functional-analysis']"
4881446,"Prove the limit $\lim_{(x,y)\to(0,0)} x^y (x>0)$ doesn't exist","Given the function $f(x,y)= x^y (x>0)$ Prove that the limit $\lim_{(x,y)\to(0,0)} f(x,y)$ does not exist I think I would choose 2 sequences $(x,y) \to (0,0)$ such that the two limits do not agree, concluding that the limit doesn't exist. But I just found a sequence $\left(\left(\frac{1}{n},0\right)\right)_{n\geq1}$ and found that $\lim_\limits{n\to\infty} f\left(\frac{1}n, 0\right) =1$ , but I don't know how to choose the second sequence. Can you explain or any way to prove it","['multivariable-calculus', 'limits', 'calculus']"
4881525,"Understanding predicates and quantifiers ∀ , ∃ (my reasoning)","So lets say we are trying to figure out if: P(x, y) is the predicate of $x^2 < y$ as defined for x, y ∈ Z. This means using quantifiers: ∀ and ∃, I can logically say: 1. ∀x∃y of P(x,y) is possible! This is because we can say $ y = x^2 + 1 $ which means $ x < x^2 $ . This means for every x there is a y. This also means ∀y∃x of P(x,y) is possible! There can exist any x that if squared is less than y such as if y was 5 and x was 2 ($2^2 < 5). This means for every y there exists a x. BUT ∃y∀x of P(x, y) is not possible. Why? Because if x becomes $ x=y+1 $ then $y+1^2 > y$ . This contradicts our understanding that $ x^2 < y $ . There does not exist a y for every x. Does my explaining of logic seem comprehensive and reasonable?","['first-order-logic', 'logic', 'discrete-mathematics', 'quantifiers']"
4881611,Understanding subspace-restricted isomorphisms and global isomorphisms,"(It may be noted that the author was not aware of Endomorphisms & Automorphisms at the time of writing this question) I'm trying to better understand the concept of a linear transformation acting as an isomorphism on a specific subspace of the vector space, even though it may not be an isomorphism on the entire vector space. To elaborate, I am referring to a situation where a linear transformation acts like an isomorphism but only when we look at a smaller, specific part of the space (a subspace). Consider a linear transformation, $T: \mathbb{R}^3 \to \mathbb{R}^3$ defined by a rank 2 matrix: $$
A = \begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 0
\end{pmatrix}
$$ $T$ is not an isomorphism on $\mathbb{R}^3$ , but it acts as an isomorphism on the subspace $W = \text{span}{((1, 0, 0), (0, 1, 0))}$ . I have a few related questions: What is the proper terminology for a linear transformation that acts as an isomorphism on a specific subspace but not necessarily on the entire space? I've tentatively called this a ""subspace-restricted isomorphism"" or ""restricted isomorphism."" How does the matrix representation of a subspace-restricted isomorphism differ from the matrix representation of the linear transformation on the entire space? Specifically, if we choose a basis that includes a basis for the invariant subspace, what structure does the matrix have? In contrast to a subspace-restricted isomorphism, what would be an appropriate name for a linear transformation that acts as an isomorphism on the entire vector space? I've considered terms like ""global isomorphism"" or ""space isomorphism."" Are there any standard names or conventions for such local (not global) properties in abstract algebra literature? I'd appreciate any insights, clarifications, or references to relevant literature that could help me better understand these concepts and their proper terminology. Thank you!","['vector-space-isomorphism', 'matrices', 'abstract-algebra', 'linear-algebra', 'linear-transformations']"
4881707,help computing an integral/hankel transform,"I'm having some trouble computing the following integral/Hankel Transform: $$\int_0^\infty \frac{ue^{-u}}{u^2 + a^2} J_0(2\sqrt{u}) du$$ where $a$ is some real number. The actual integral I want to compute is $$\int_0^\infty \frac{ue^{-u}}{u^2 + (2\pi n)^2} I_0(2\sqrt{bui}) du$$ where $n$ is a positive integer, and $b$ is a real, positive number. I'm sure the computations are symmetric, so the first integral (probably) is sufficient. An integral I have encountered in my search that could help are $$\int_0^\infty \frac{x \sin(ax)}{x^2 + b^2} J_0(yx) dx = \frac{\pi}{2} e^{-ab} I_0(by)$$ for $y \leq a$ and $$\int_0^\infty \frac{x \cos(ax)}{x^2 + b^2} J_0(yx) dx = \cosh(ab) K_0(by)$$ for $y \geq a$ from the link: Double integral with Hankel transform Since $\sin,\cos$ is just related to the exponential, this could help. Thank you so much!","['integration', 'definite-integrals', 'analysis', 'complex-analysis', 'bessel-functions']"
4881757,"If $x\mapsto f(x,y)$ and $y\mapsto f(x,y)$ is continuous differentiable, then whether $f(x,y)$ is continous?","Here is the question in my exercise. Let $f(x,y)$ be a function defined on a rectangle $[a,b]\times [c,d]$ . Assume that for each fixed $y \in[c,d]$ the function $x\mapsto f(x,y)$ is a continuously differentiable function of $x$ on $[a,b]$ ; and for each fixed $x \in[a,b]$ the function $y\mapsto f(x,y)$ is a continuously differentiable function of $y$ on $[c,d]$ . Can you conclude that $f(x,y)$ is a continuous function on the rectangle $[a,b]\times [c,d]$ ? If your answer is Yes then give a proof; if your answer is
No then give a counterexample. Here is my thinking: Fixed any point $P_0$ in $[a,b]\times [c,d]$ . If $x\mapsto f(x,y)$ and $y\mapsto f(x,y)$ are both continuously differentiable, which means the partial deravatives $f_x$ and $f_y$ are continuous. So $f(x,y)$ is differentiable at $P_0$ . Then $f(x,y)$ is continuous. Is my thinking right? If it's wrong, what mistake I made?","['multivariable-calculus', 'calculus']"
4881759,"Prove $\sum\limits_{i=1}^{2024}a_i<314$ where $a_1=2$, $a_i=2\sin\frac{a_{i-1}}2$","Let $a_1=2$ and $a_i=2\sin\frac{a_{i-1}}2$ for $i\ge2$ . Prove that $\sum\limits_{i=1}^{2024}a_i<314$ . In fact, $\sum\limits_{i=1}^{2024}a_i\approx298.796$ , so the inequality is very strong. I tried to establish inequalities with Taylor's series, and got $\sum\limits_{i=1}^{2024}a_i<582$ . This is not enough. We need better (more accurate) ways to estimate the series. This question is from the "" $\pi$ day math contest"" of THU, which has ended.","['contest-math', 'recursion', 'real-analysis', 'sequences-and-series', 'inequality']"
4881832,A polynomial from character theory,"Let $G$ be a finite group, and define $a(n)=\#\{g\in G\mid o(g)=n\}$ . In problem 5.18 of Isaacs' Character Theory of Finite Groups , the following polynomial is defined: $$F_G(X)=\frac{1}{|G|}\sum_ma(m)X^{|G|/m}$$ Theorem: $F_G(k)$ is an integer for $k\in\mathbb{Z}$ . Setting $G$ to be the cyclic group with $n$ elements, define: $$g_n(X)=\frac{1}{n}\sum_{d|n}\varphi(d)X^{n/d}$$ Questions: Does $g_n$ have some other significance, in some other context? Perhaps in number theory? Is there a more direct way to show that $g_n(k)$ is an integer for every integer $k$ ? In the special case that $n=p$ for some prime $p$ , $$g_p(X)=\frac{1}{p}(X^p+(p-1)X)$$ . It follows easily from Fermat's Little Theorem that $g_p(k)$ is an integer for integer values of $k$ .","['number-theory', 'characters']"
4881905,Connection between statistical and functional dependence of random variables,"For simplicity lets consider finite probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and two random variables $\xi, \eta$ on it. I wonder is there any connection between statistical and functional dependence of random variables? For example, if I have $\xi = f\circ \eta$ for some $f$ , does it imply that $\xi$ and $\eta$ statistically dependent and vice versa ? Or, if they are functionally independent, does it imply that they are statistically independent and vice versa?","['statistics', 'probability-theory', 'probability', 'random-variables']"
4881924,Pointwise absolute and uniform convergence of a series of functions for $0<k<1$,"To study for $k\in\Bbb R^+$ the pointwise, absolute and uniform convergence in $\Bbb R$ of the following
series of functions: $$\sum_{n=1}^\infty\frac{\cos n^k x}{1+n^k}$$ One has obviously, for each $x\in\Bbb R$ : $$\left|\frac{\cos n^k x}{1+n^k}\right|\leq\frac1{1+n^k}\qquad\forall x\in\Bbb R,$$ and the series $\sum_{n=1}^\infty1/(1+n^k)$ converges if $k>1$ , so in that case there is total convergence (hence uniform absolute, pointwise over all $x\in\mathbb R$ ). If $k=1$ the series becomes $$\sum_{n=1}^\infty\frac{\cos(nx)}{1+n};$$ this series is shown, by the Abel-Dirichlet criterion, or by techniques peculiar to Fourier series, to be punctually convergent, except for $x\in2\pi\Bbb Z$ ; and it is also shown to converge uniformly on any set that has strictly positive distance from $2\pi\Bbb Z$ again by techniques related to this criterion. But I would not be able to discuss its absolute convergence, and nothing I could say in the case $0<k<1$ .","['calculus', 'sequences-and-series', 'real-analysis']"
4881963,"If rank($J_f(x))=n$, then there exists a neighborhood such that for each of its points $y$, $rank(J_f(y))\geq n$","Let $f: U \rightarrow \mathbb{R}^p$ be a $C^1$ function defined on an open subset $U \subseteq \mathbb{R}^q$ . If for some $x \in U$ , the jacobi matrix $J_f(x)$ has $rank$ equal to $n$ , then there exists an open neighborhood $V$ of $x$ for each $x \in V$ , the jacobi matrix $J_f(y)$ has rank at least $n$ . My approach: I do not really have an approach.
I know that for a matrix $A$ , $rank(A):= \dim im(A)$ . What also comes to mind is that $rank(A)=n$ , would mean that $A$ has $n$ linearly independent columns. If I would have to guess, I would say it has something to do with the $C^1$ , i.e. $J_f$ is continuous. And somehow using the continuity the result should follow. But I don't know how to approach this. Any hints would be appreciated.","['multivariable-calculus', 'linear-algebra', 'analysis', 'real-analysis']"
4881976,Definition of orbit equivalence,"I have a doubt regarding the definition of orbit equivalence as given by Fisher & Hasselblatt in their book Hyperbolic Flows . We say that two flows $\phi, \psi$ on $X, Y$ respectively are orbit equivalent if there exists a homeomorphism $h : X \to Y$ takes orbits of $\phi$ to orbits of $\psi$ .
Let $x \in X$ and $O^{\phi}(x)$ its orbit under $\phi$ . Is the definition of orbit equivalence $h(O^{\phi}(x)) \subseteq O^{\psi}(h(x))$ or $h(O^{\phi}(x)) = O^{\psi}(h(x))$ ? It seems to me that in order for orbit equivalence to be an equivalence relation we need $h$ to take the orbits of $\phi$ onto the orbits of $\psi$ . Or does this come for free given just the inclusion $\subseteq$ ?","['topological-dynamics', 'differential-geometry', 'ordinary-differential-equations', 'group-actions', 'dynamical-systems']"
4881981,"$|\vec{a}|=4,|\vec{b}|=2|\vec{c}|,(\vec{a}-\vec{c})\cdot (\vec{b}-\vec{c})=3$, try minimize $|\vec{a}-\vec{b}|$","$\vec{a},\vec{b},\vec{c}$ are vectors on a plane, which satisfy $$
|\vec{a}|=4,|\vec{b}|=2|\vec{c}|
$$ and the equation $$
(\vec{a}-\vec{c})\cdot (\vec{b}-\vec{c})=3
$$ Find the minimum value of $|\vec{a}-\vec{b}|$ . I've checked the reference solution of this problem, which notes $$
\vec a=\vec{OA},\vec{b}=\vec{OB},c=\vec{OC}
$$ then notes $$
\vec{OM}=\frac{1}{2}(\vec{OA}+\vec{OB})
$$ so we can translate the equation $(\vec{a}-\vec{c})\cdot (\vec{b}-\vec{c})=3$ into $$
|\vec{CM}|^2-\frac{1}{4}|\vec{a}-\vec{b}|^2=3
$$ (It's better to draw a picture there to show the geometric motivation, but my technique is limited. I'm sorry...) So $|\vec{a}-\vec{b}|$ get the minimum value iff $|\vec{CM}|$ get the minimum value. But the next step just assumes $O,C,M$ were on the same line. It really confuses me, because this step is naive only if C was free to move on the whole circle determined by the half length of $\vec{b}$ . However, if $\vec{a},\vec{b}$ was determined, the $\vec{c}$ was also determined, instead of being wherever of a circle. I've tried another order to determine the variables but it didn't work. I'm seeking for your assitance, or you can just show another solution all by yourself. EDIT.","['maxima-minima', 'inequality', 'vectors', 'geometry']"
4881988,Problem about inverse function and derivative,"Let $k$ be a real number. $$f(x)=x^3-3x^2+6x+k$$ and let $g(x)$ be the inverse function of $f(x)$ .  The equation $$4f'(x)+12x-18=(f'\circ g)(x)$$ has a real root on $[0, 1]$ . Find $m^2+M^2$ where $m$ is the minimum value of $k$ and $M$ is the maximum value of $k$ . My work: $$f'(x)=3x^2-6x+6\\\implies 4f'(x)+12x-18=12x^2-12x+6$$ Since $(g\circ f)(x)=x$ , plug $x=f(x)$ to the equation: $$12(f(x))^2-12f(x)+6=f'(x)$$ Now let $$h(x)=12(f(x))^2-12f(x)+6-f'(x)$$ then $h(0)h(1)\leq 0$ given that $h$ is continuous. $f(0)=k$ , $f(1)=k+4$ implies $h(0)=12k(k-1)$ , $h(1)=12k^2+84k+147=3(2k+7)^2$ . Thus I have $m=-7/2$ and $M=1$ . But the correct answer is $m=-8$ and $M=1$ .","['calculus', 'inverse-function', 'derivatives']"
4882001,Generating function for binary strings not containing $0110$ or $11010$ as a substring,"Find the generating function for the binary strings that do not contain $0110$ or $11010$ as a substring. I am familiar with proofs similar to this post, where the 'symbolic method' is used for counting combinatorial objects. As such I would be able to find generating functions for binary strings not containing $0110$ as a substring and the binary strings not containing $11010$ as a substring. But I'm having a hard time finding the generating function for the binary strings not containing either of these as substrings. I have tried to consider where they overlap? Perhaps the symbolic method as in the linked post is not applicable and another approach is necessary, but I'm not sure what exactly. Any help is appreciated. Thanks in advance.","['regular-expressions', 'combinatorics', 'generating-functions']"
4882079,Prove that a Lebesggue outer measure on $\mathbb{R}^d$ is an outer measure which assigns to each $d$-dimensional interval its volume.,"I just started to self-study some measure theory. I need to prove the following proposition Proposition $\quad$ Lebesgue outer measure on $\mathbb{R}^d$ is an outer measure, and it assigns to each $d$ -dimensional interval its volume. where Definition $\quad$ Let $X$ be a set, and let $\mathcal{P}(X)$ be the collection of all subsets of $X$ . An outer measure on $X$ is a function $\mu^*:\mathcal{P}(X)\to[0,+\infty]$ such that $\mu^*(\emptyset) = 0$ , if $A \subseteq B \subseteq X$ , then $\mu^*(A)\leq\mu^*(B)$ , and if $\{A_n\}$ is an infinite sequence of subsets of $X$ , then $\mu^*(\bigcup_nA_n)\leq\sum_n\mu^*(A_n)$ . and Definition $\quad$ We define Lebesgue outer measure on $\mathbb{R}^d$ as follows. A $d$ -dimensional interval is a subset of $\mathbb{R}^d$ of the form $I_1 \times \dots \times I_d$ , where $I_1, \dots, I_d$ are subintervals of $\mathbb{R}$ and $I_1 \times \dots \times I_d$ is given by \begin{align*}
    I_1 \times \dots \times I_d = \{(x_1,\dots,x_d):x_i \in I_i\ \text{for } i=1,\dots,d\}.
\end{align*} Note that the intervals $I_1, \dots, I_d$ , and hence the $d$ -dimensional interval $I_1 \times \dots \times I_d$ , can be open, closed, or neither open nor closed. The volume of the $d$ -dimensional interval $I_1 \times \dots \times I_d$ is the product of the lengths of the intervals $I_1, \dots, I_d$ , and will be denoted by $\text{vol}(I_1 \times \dots \times I_d)$ . For each subset $A$ of $\mathbb{R}^d$ let $\mathcal{C}(A)$ be the set of all sequences $\{R_i\}$ of bounded and open $d$ -dimensional intervals for which $A \subseteq \bigcup_{i=1}^{\infty}R_i$ . Then $\lambda^*(A)$ , the Lebesgue outer measure of $A$ is the infimum of the set \begin{align*}
    \left\{\sum_{i=1}^{\infty}\text{vol}(R_i):\{R_i\}\in\mathcal{C}(A)\right\}.
\end{align*} Here is my attempt: Proof $\quad$ We begin by verifying that $\lambda^*$ is an outer measure. The relation $\lambda^*(\emptyset)=0$ holds, since for each positive number $\epsilon$ there is a sequence $\{R_i\}$ of bounded and open $d$ -dimensional intervals (whose union necessarily includes $\emptyset$ ) such that $\sum_i\text{vol}(R_i)<\epsilon$ ; for example, let $R_i = (a_1^i,b_1^i)\times\dots\times(a_d^i,b_d^i)$ where $b_1^i-a_1^i = \dots = b_d^i-a_d^i = \left(\frac{\epsilon}{3 \cdot 2^{i-1}}\right)^{\frac{1}{d}}$ , so that $\text{vol}(R_i) = \frac{\epsilon}{3 \cdot 2^{i-1}}$ , which implies that $\sum_i\text{vol}(R_i) = \frac{2\epsilon}{3} < \epsilon$ . For the monotonicity of $\lambda^*$ , note that if $A \subseteq B$ , then each sequence of bounded open $d$ -dimensional intervals that covers $B$ also covers $A$ , and so $\lambda^*(A) \leq \lambda^*(B)$ . Now consider that countable subadditivity of $\lambda^*$ . Let $\{A_b\}_{n=1}^{\infty}$ be an arbitrary sequence of subsets of $\mathbb{R}^d$ . If $\sum_n\lambda^*(A_n) = +\infty$ , then $\lambda^*(\bigcup_nA_n) \leq \sum_n\lambda^*(A_n)$ certainly holds. So suppose that $\sum_n\lambda^*(A_n) < +\infty$ , and let $\epsilon$ be an arbitrary positive number. For each $n$ choose a sequence $\{R_{n,i}\}_{i=1}^{\infty}$ of bounded open $d$ -dimensional intervals that covers $A_n$ and satisfies \begin{align*}
\sum_{i=1}^{\infty}\text{vol}(R_{n,i}) < \lambda^*(A_n) + \frac{\epsilon}{2^n}.
\end{align*} If we combine these sequences into one sequence $\{R_j\}$ , then the combined sequence satisfies \begin{align*}
\bigcup_nA_n \subseteq \bigcup_jR_j
\end{align*} and \begin{align*}
\sum_j\text{vol}(R_j) < \sum_n\left(\lambda^*(A_n)+\frac{\epsilon}{2^n}\right) = \sum_n\lambda^*(A_n) + \epsilon.
\end{align*} These relations, together with the fact that $\epsilon$ is arbitrary, imply that $\lambda^*(\bigcup_nA_n) \leq \sum_n\lambda^*(A_n)$ . Thus, $\lambda^*$ is an outer measure. Now we compute the outer measure of the $d$ -dimensional intervals of $\mathbb{R}^d$ . First consider a closed bounded, and thus compact, $d$ -dimensional interval $K$ . We cover $K$ with a sequence $\{R_i\}$ of bounded open $d$ -dimensional intervals in which the first $d$ -dimensional interval is barely larger than $K$ , and the sum of the volumes of the other $d$ -dimensional intervals is very small; for example, let $\epsilon$ be an arbitrary positive number and let $K \subseteq \bigcup_iR_i$ where $\text{vol}(R_1)=\text{vol}(K)+\epsilon$ and $\text{vol}(R_i)=\frac{\epsilon}{2^{i-1}}$ for $i>1$ . Then, $\lambda^*(K) < \text{vol}(K)+2\epsilon$ for all $\epsilon>0$ , and so $\lambda^*(K) \leq \text{vol}(K)$ . We turn to the reverse inequality. Note that if $K$ is a compact $d$ -dimensional interval and if $\{R_i\}_{i=1}^{\infty}$ is a sequence of bounded and open $d$ -dimensional for which $K \subseteq \bigcup_{i=1}^{\infty}R_i$ , then there is a positive integer $n$ such that $K \subseteq \bigcup_{i=1}^nR_i$ , and $K$ can be decomposed into a finite collection $\{K_j\}_{j=1}^m$ that overlap only on their boundaries and are such that for each $j$ the interior of $K_j$ is included in some $R_i$ (where $i \leq n$ ). From this it follows that \begin{align*}
\text{vol}(K) = \sum_{j=1}^m\text{vol}(K_j) \leq \sum_i\text{vol}(R_i)
\end{align*} and hence that $\text{vol}(K) \leq \lambda^*(K)$ . Therefore, $\text{vol}(K) = \lambda^*(K)$ . The outer measure of an arbitrary bounded $d$ -dimensional interval is its volume, since such a $d$ -dimensional interval includes and is included in closed bounded intervals of volume arbitrarily close to the volume of itself. Finally, an unbounded $d$ -dimensional interval has infinite outer measure, since it includes arbitrarily larger closed bounded $d$ -dimensional intervals. Could someone please help me check if my proof is correct and rigorous? Thanks a lot in advance! To be more specific, I am not confident about the following parts of my proof: When I prove $\lambda^*(\emptyset) = 0$ , I constructed a sequences of bounded open $d$ -dimensional intervals such that the series is less than $\epsilon$ . However, I am not sure whether my construction is appropriate. When proving the countable subadditivity under the assumption of $\sum_n\lambda^*(A_n)<+\infty$ , without specifying what exactly the sequence $\{R_{n,i}\}$ is, I only expressed that I want $\sum_{i=1}^{\infty}\text{vol}(R_{n,i}) < \lambda^*(A_n) + \frac{\epsilon}{2^n}$ to be true. Is it rigorous enough to simply put it there? If not, how should I construct a sequence $\{R_i\}$ with its exact elements specified such that the inequality holds? Similarly, when computing the outer measure of a compact $d$ -dimensional interval, I failed to specify what exactly the covering sequence of bounded open $d$ -dimensional intervals is; instead, I simply expressed that I want $\text{vol}(R_1)=\text{vol}(K)+\epsilon$ and $\text{vol}(R_i)=\frac{\epsilon}{2^{i-1}}$ for $i>1$ to hold. Is this correct and rigorous? If not, how should I improve it? Finally, if it is necessary, how should one rigorously show that "" $K$ can be decomposed into a finite collection $\{K_j\}_{j=1}^m$ that overlap only on their boundaries and are such that for each $j$ the interior of $K_j$ is included in some $R_i$ (where $i \leq n$ ).""","['measure-theory', 'lebesgue-measure', 'proof-writing', 'real-analysis', 'solution-verification']"
4882113,why is Terence Tao's definition 3.3 of set intersection so complicated?,"Terence Tao's Analysis I 4th edition defines set intersection as follows: Given any non-empty set $I$ , and given an assignment of a set $A_{\alpha}$ to each $\alpha \in I$ , we can define the intersection $\bigcap_{\alpha \in I}A_{\alpha}$ by first choosing some element $\beta$ of $I$ (which we can do since $I$ is non-empty), and setting $$ \bigcap_{\alpha \in I} A_{\alpha} := \{ x \in A_{\beta} : x \in A_{\alpha} \text{ for all } \alpha \in I  \} $$ which is a set by the axiom of specification. Question My question is - why does he define it in this complicated way? A simpler definition might be: The intersection of a family of sets $A_\alpha$ is the set whose
elements exist in every set $A_\alpha$ . That is, $$ \bigcap_{\alpha \in I} A_{\alpha} := \{ x : x \in A_{\alpha} \text{ for all } \alpha \in I  \} $$ Clearly I am missing something important. Thoughts I recall Tao discussing earlier in the book how subsets of sets are sets, but sets defined by logical statements can cause paradoxes. Is his definition an attempt to start with a set and then use axioms he introduced to guarantee the intersection is also a set?",['elementary-set-theory']
4882135,Any $\ell^2$-closed subspace of $\ell^2 \cap \ell^1$ is finite-dimensional,"Let $X$ be a closed subspace of $\ell^2$ such that $X$ is contained in $\ell^1$ . It is easy to show that the inclusion operator $J \colon X \hookrightarrow \ell^1$ is closed, hence, by the closed graph theorem $J$ is bounded. Is it true that $X$ is automatically finite dimensional? I would really appreciate any hints.","['functional-analysis', 'closed-graph']"
4882153,Proving the equality case of the Rising Sun Inequality,"I'm stuck at the finish line on a pair of Exercises from Tao's Introduction to Measure Theory: Exercise 1.6.12 (Rising Sun Inequality). Let $f : \mathbb{R} \to \mathbb{R}$ be an absolutely integrable function, and let $f^* : \mathbb{R} \to \mathbb{R}$ be the one-sided signed Hardy-Littlewood maximal function $$
f^*(x) := \sup_{h>0}\frac{1}{h}\int_{[x,x+h]}f(t)\,dt.
$$ Establish the rising sun inequality $$
\lambda m(\{x\in\mathbb{R} : f^*(x) > \lambda\}) \leq \int_{\{x \in\mathbb{R} : f^*(x) > \lambda\}}f(t)\,dt
$$ for all real $\lambda$ (note here that we permit $\lambda$ to be zero or negative) [...] Exercise 1.6.13. Show that the left- and right-hand sides of Exercise 1.6.12 are in fact equal when $\lambda > 0$ . ( Hint : One may first wish to try this in the case when $f$ has compact support [...]) I've proven Exercise 1.6.12, and for 1.6.13, following the hint I proved equality for the case when $f$ has compact support. My issue is on generalizing 1.6.13 to all absolutely integrable $f$ . Here's my thinking so far. If $f$ is absolutely integrable, we can define functions $f_n = f1_{[-n,n]}$ for $n \in \mathbb{N}$ and apply the compact-support case to the $f_n$ . Namely, letting $A_n := \{x \in \mathbb{R} : f_n^*(x) > \lambda\}$ and $A := \{x \in \mathbb{R} : f^*(x) > \lambda\}$ we know that $$
\lambda m(A_n) = \int_{A_n}f_n(t)\,dt
$$ for each $n$ . I was hoping we could let $n \to \infty$ on both sides and be done, but it's not quite so simple, since the relationship between the sets $A_n$ and $A$ is a bit subtle. One can verify that $A = \lim_{n\to\infty} A_n$ in the sense that $1_{A_n} \to 1_A$ pointwise, but each $A_n$ is not necessarily a  subset of $A$ nor is it necessarily a subset of $A_{n+1}$ . (The issue occurs at the points in $A_n$ which are less than $-n$ .) Since $f$ is absolutely integrable we do have $\int_{A_n}f_n(t)\,dt \to \int_{A}f(t)\,dt$ by the dominated convergence theorem, but I can't (yet) justify that $m(A_n) \to m(A)$ . At best I can use Fatou's Lemma to say that $m(A) \leq \lim_{n\to\infty} m(A_n)$ , but that just yields the rising sun inequality again, which I already know. Is there something I'm missing, or should I find another method to generalize 1.6.13 from the compact-support case?","['measure-theory', 'lebesgue-integral', 'inequality', 'real-analysis']"
4882184,Sufficient conditions for finitely supported measures being dense,"Let $(X,\mathcal{B})$ be a Hausdorff topological space with its Borel $\sigma$ -algebra. What are some general conditions we could impose on $X$ so that finitely supported measures (i.e. finite affine combinations of dirac measures) are weakly dense in the space $\mathcal{M}(X)$ of Borel probability measures of $X$ ? Also, it is not hard to prove that if $X$ is a Polish space, then for each Borel probability measure $\mu$ in $X$ there is a sequence $(\mu_n)_n$ of finitely supported measures weakly convergent to $\mu$ , but could someone provide a reference so that I can just quote it? Context: At some point in my research I need to weakly approximate measures in the Pontryagin dual of a countable discrete group by sequences finitely supported measures. So the result for polish spaces is enough for me, but I was wondering how generally can we approximate measures by finitely supported ones.","['measure-theory', 'weak-convergence', 'borel-measures', 'reference-request', 'functional-analysis']"
4882196,How to find $\frac{\tan(x+y)}{\tan(x)}$ if $5\sin(2x+y)=7\sin(y)$,I saw this problem Find $\frac{\tan(x+y)}{\tan(x)}$ if $5\sin(2x+y)=7\sin(y)$ I tried to use $$\sin(x+y)\cos(x)+\cos(x+y)\sin(x)= \frac{7}{5}\sin(y)$$ $$\tan(x+y)=\frac{\frac{7}{5}\frac{\sin(y)}{\cos(x+y)}- \sin(x) }{\cos(x)}$$ $$\frac{\tan(x+y)}{\tan(x)}={\frac{7}{5}\frac{\sin(y)}{\sin(x)\cos(x+y)}- 1 }$$ Which lead to nothing. I also tried to use $\tan(a+b)$ formula but it also didn't lead to anything useful.,"['algebra-precalculus', 'trigonometry']"
4882215,References for a statement on linear systems,"Suppose we have a linear system $\dot{x} = A x$ , where $x \in \mathbb{R}^n$ and $A$ is a constant matrix, such that all the trajectories of the system are bounded and bounded away from the origin (i.e., none of them converge to the origin). This I believe implies that all the eigenvalues of $A$ have 0 real parts and that $A$ is diagonalizable. I also believe that for this type of system, there exists a symmetric matrix $P$ such that $x^t P x$ is constant along the trajectories. I am interested in this as I use it in my research. I have a proof of it, though it seems to be a rather simple result, and I would prefer to simply cite a reference for it. Does anyone know of any references to these facts? (e.g., a book or paper that states/proves these statements)","['linear-algebra', 'ordinary-differential-equations', 'reference-request']"
4882219,If $g \in L^1(\mathbb{R})$ and $g' \in L^p$ then $\lim_{|x| \to \infty} g(x) = 0$,"I am working on this problem from Le Gall's measure theory book with 2 parts. Let $f$ be in $L^p(\mathbb{R})$ for $1\leq p< \infty$ . Let $q$ be the conjugate exponent of $p$ . Let $F(x) = \int_{0}^{x} f(t) dt$ . Verify that $F$ is well-defined and $$\lim_{h \to 0^+} \frac{\sup_{x \in \mathbb{R}} \lvert F(x+h) - F(x)\rvert}{h^{1/q}}= 0$$ Let $g$ be continuously differentiable and in $L^1(\mathbb{R})$ . Suppose $g' \in L^p$ for some $p \in [1, \infty)$ . Then $\lim_{|x| \to \infty} g(x) = 0$ . I can show the first part using the density of continuous and compactly supported functions in $L^p$ but I need help with the second part. If I define $G(x) = \int_{0}^{x} g'(t)dt = g(x) - g(0)$ then the first part only gives me that $$\lim_{h \to 0^+} \frac{\sup_{x} \lvert g(x+h) - g(x)\rvert}{h^{1/q}} = 0$$ This only gives a lipschitz like continuity bound and doesn't seem connected to the tails, at least on the surface.","['measure-theory', 'real-analysis']"
4882281,In how many ways can the letters of the word PANACEA be arranged so that the three As are NOT all together?,"PANACEA has 7 letters with 3 As. There are 4! ways to arrange (P, N, C, E) _ P_N_C_E _. Between these letters, there are 5 slots. So, to arrange 3 As in 5 slots is 5P3. We divide it by 3! for the 3 As to avoid duplicates. We have: $4!\cdot \frac{5P3}{3!} = 240$ This is the correct answer: Total arrangement - 3 A's are together = 3 A's are not together $\frac{7!}{3!}-5! = 720$ I need someone to clarify why the first method didn't work or did I made a mistake?",['combinatorics']
4882286,Organizing the content of Euclidean geometry with pictorial mind maps [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed last month . Improve this question There is an idea that has been on my mind for a while, and I would like to share it so that it turns into a snowball. Perhaps it will be useful and attractive to engineering enthusiasts. I ask you to help me develop this model that I am thinking of... Most geometric theorems are proven by putting several theorems together, but there are one or two theorems that represent the core idea of ​​the proof. If you know the basic theorem you will use to do your proof, you still have to do some work to arrive at the proof, even though the basic step has already been done; Making a very large mind map would be impractical, but small mind maps can be made to express some conclusions A lot of written mind maps can be done, but I think it would be fun and useful for geometry enthusiasts to have pictorial mind maps so they can guess a good visualization of the proof. I think that pictorial mind maps are an excellent idea for engineering, as the engineering student has a good connection between the engineering content, and engineering mind maps still carry some challenge as they require work to complete the proof. Most geometry enthusiasts can understand the statements and requirements of the theorem just from the picture, which is why Arseny Akopyan's book is so popular I'll post some examples of mind maps I like and I hope my list of answers will expand... The ground truth in the previous map is the one shown in the upper image; It may be intuitive, and the arrows point to conclusions, not entirely trivial It is worth noting that the details of the proof need to be done; This is different from what is known as proof without words; If I wanted to turn the proof of the theorem on the right into a proof without words, it would be something like this: This makes these mental diagrams a narrative of the theorem(s) that can be used in proof; But it still carries the challenge of actually doing this proof Let's take a second example If an engineering theorem can be used to prove theorems and perform geometric constructions, a ruler and compass can be placed next to the geometric construction to distinguish it, as in the previous picture. It can be done the other way around, with one conclusion and many different ways to reach it. It may be possible to reach one conclusion in many different ways Double arrows can be used if the inference can be made in both directions, as is the case, for example, with the previous image. However, there must be a well-understood stereotype for both the premises and the conclusions involved in mind maps of this type. I would be happy to expand my list with more mind maps. Are there any fans of this idea? Edit: What I'm looking for in this particular question is more examples of geometry mental diagrams like mine, in which each arrow points to an actual geometric proof","['euclidean-geometry', 'big-list', 'soft-question', 'geometry']"
4882310,"Difficult calculus question that includes composite functions, primes, roots, etc","Let $f(x)$ be a cubic whose coefficient of the leading term is positive and $g(x) = e^{\sin(\pi x)} - 1$ . The composite function $h(x) = g(f(x))$ is defined in the set of all numbers, and has a local maximum at $x=0$ . In the open interval $(0,3)$ the function $h(x)$ intersects the line $y= 1$ seven times. Given that $f(3) =1/2$ , $f'(3) = 0$ , and $f(2) = \frac{q}{p}$ , find the value of $p+q$ given that $p$ and $q$ are coprime natural numbers This is my working so far: Since $h(x)$ has a local maximum at $x=0$ , this means that at $x=0$ the derivative of $h(x) = 0$ , i.e. $$h'(0) = g'f(0) \cdot f'(0) = 0$$ We need to find the derivative of g(x) first to find an expression for the derivative of h(x), so $$\frac{d}{dx} g(x)$$ $$= \frac{d}{dx} (e^{\sin(\pi x)} - 1)$$ $$g'(x)= \pi \cos(\pi x) \cdot e^{\sin(\pi x)}$$ Hence $$h'(0) = \pi \cdot \cos(\pi f(0)) \cdot e^{\sin(\pi f(0))} \cdot f'(0) = 0$$ $$\cos(\pi f(0)) \cdot e^{\sin(\pi f(0))} \cdot f'(0) = 0$$ Given that any exponential function of the form $e^x$ is only defined for $x>0$ , $e^{\sin(\pi f(0))} \neq 0$ . So we are left with $$\cos(\pi f(0)) \cdot f'(0) = 0$$ Case 1: $$\cos(\pi f(0)) = 0$$ $$\pi f(0) = \frac{\pi}{2} + k\pi$$ where $k \in \mathbb{Z}$ $$f(0) = \frac{1}{2} + k$$ Because $f(x)$ is a cubic, we know it is of the form $f(x) = ax^3 + bx^2 + cx + d$ Hence, $$f(0) = d$$ So $d= \frac{1}{2} + k$ Case 2: $$f'(0) = 0$$ $f'(x) = 3ax^2 + 2bx + c$ , so $$f'(0) = c = 0$$ So $c=0$ Hence $f(x) = ax^3 + bx^2 + d$ , where $a>0$ and $d = \frac{1}{2} + k$ This is a part where I think I might have done something wrong. I assumed both $c=0$ and $d=\frac{1}{2} + k$ , but only one of them have to be true right? Because at first I thought both conditions would need to be true, but only one of $\cos(\pi f(0))$ or $f'(0)$ needs to be equal to $0$ for $h'(0) = 0$ to hold true (remember that $h'(0) = \cos(\pi f(0)) \cdot f'(0)$ ) Carrying on, $$f(3) = \frac{1}{2}$$ $$a \cdot 3^{3} + b \cdot 3^{2} + d= \frac{1}{2}$$ $$27a + 8b + d = \frac{1}{2}$$ $$f'(3) = 0$$ $$ 3a \cdot 3^{2} + 2b \cdot 3 = 0$$ $$ 27a + 6b = 0$$ $$f(2) = \frac{q}{p}$$ $$a \cdot 2^{3} + b \cdot 2^{2} + d= \frac{q}{p}$$ $$ 8a + 4b + d = \frac{q}{p}$$ So know we have the system of equations $$27a + 8b + d = \frac{1}{2}$$ $$ 27a + 6b = 0$$ $$ 8a + 4b + d = \frac{q}{p}$$ Let's eliminate $a$ from equations $1$ and $3 $ using equation $2$ . First, we solve equation $2$ for $27a$ : $ 27a = -6b $ Now we substitute $ -6b $ for $ 27a $ in equations $1$ and $3$ : $ -6b + 8b + d = \frac{1}{2} $ $ -\frac{6}{27} \cdot 8b + 4b + d = \frac{q}{p} $ Simplify those equations: 1. $  2b + d = \frac{1}{2} $ 3. $ -\frac{48}{27}b + 4b + d = \frac{q}{p} $ Let's simplify the coefficients in equation $3$ : $ -\frac{16}{9}b + 4b + d = \frac{q}{p} $ Multiply every term by 9 to get rid of the fraction: $ -16b + 36b + 9d = \frac{9q}{p} $ Now combine like terms: $ 20b + 9d = \frac{9q}{p} $ Now we have two equations: $ 2b + d = \frac{1}{2} $ $ 20b + 9d = \frac{9q}{p} $ We can multiply the first equation by 10 to align the coefficients of $b$ : $ 20b + 10d = 5 $ Now we have the system: $ 20b + 10d = 5 $ $ 20b + 9d = \frac{9q}{p} $ Subtract the second equation from the first: $ (20b + 10d) - (20b + 9d) = 5 - \frac{9q}{p} $ This simplifies to: $ d = 5 - \frac{9q}{p} $ Now we can substitute this value for $d$ back into one of the previous equations to find $b$ . Let's use the modified version of equation 1: $ 2b + (5 - \frac{9q}{p}) = \frac{1}{2} $ Solve for $b$ : $ 2b = \frac{1}{2} - (5 - \frac{9q}{p}) $ $ 2b = \frac{1}{2} - 5 + \frac{9q}{p} $ $ 2b = -\frac{9}{2} + \frac{9q}{p} $ $ b = -\frac{9}{4} + \frac{9q}{2p} $ Now we can substitute the value of $b$ back into equation 2 to solve for $a$ : $ 27a + 6b = 0 $ $ 27a + 6(-\frac{9}{4} + \frac{9q}{2p}) = 0 $ $ 27a - \frac{27}{2} + \frac{27q}{p} = 0 $ $ 27a = \frac{27}{2} - \frac{27q}{p} $ $ a = \frac{1}{2} - \frac{q}{p} $ Now we have expressions for $a$ , $b$ , and $d$ : $$ a = \frac{1}{2} - \frac{q}{p} $$ $$ b = -\frac{9}{4} + \frac{9q}{2p} $$ $$ d = 5 - \frac{9q}{p} $$ But remember that $d=\frac{1}{2} + k$ , where $k$ is an integer, so $$  \frac{1}{2} + k = 5 - \frac{9q}{p} $$ $$ k = \frac{9}{5} - \frac{9q}{p} $$ To make $k$ an integer, $\frac{9}{5} - \frac{9q}{p}$ must be an integer. This implies that $\frac{9q}{p}$ must be a fraction that can be expressed with a denominator of 5 since the first fraction has a denominator of 5. This is necessary for their difference to be an integer. So we can write: $$ \frac{9q}{p} = \frac{m}{5} $$ where $m$ is an integer such that when subtracted from $9$ , the result is a multiple of $5$ (ex. $m=4$ ). This is because when we subtract $\frac{m}{5}$ from $\frac{9}{5}$ , we get an integer. $$ 5 \cdot 9q = m \cdot p $$ Since $q$ and $p$ are coprime, $p$ must be a divisor of $5$ to satisfy this equation. The only natural number divisors of $5$ are $1$ and $5$ itself, but $1$ isn't prime , so $p=5$ If $p = 5$ , then $m = 9q$ Since we're looking for coprime $p$ and $q$ , since $p = 5$ , $q$ must not include the factor $5$ , and it can be any other natural number. $$f(x) = ax^3 + bx^2 + d$$ $$f(x)= (\frac{1}{2} - \frac{q}{p})x^3 + (-\frac{9}{4} + \frac{9q}{2p})x^2 + 5 - \frac{9q}{p}$$ $$ f(x)= (\frac{1}{2} - \frac{q}{5})x^3 + (-\frac{9}{4} + \frac{9q}{10})x^2 + 5 - \frac{9q}{5}$$ Now I'm stuck The number of different real roots of the equation $h(x) = 1$ in the open interval $(0,3)$ is $7$ . I don't even know what to do with this information 😭💀 I tried setting $h(x) = 1$ and solving for $x$ $$e^{\sin(\pi f(x))} - 1 = 1$$ $$e^{\sin(\pi f(x))} = 2$$ $$\sin(\pi f(x)) = \ln(2)$$ but this led nowhere again Please can someone help me solve this 😢","['calculus', 'functions', 'derivatives', 'polynomials']"
4882396,When is the center of group contained in the derived subgroup,"Let $N$ be a group. Assume that $N$ is torsion-free, finitely generated and nilpotent. I read somewhere that $$ Z(N) \subset [N,N] \iff N \text{ cannot be written as a direct product of groups } N = A \times B \text{ where }A \text{ is non-trivial abelian.}$$ One implication is clear to me: $\implies$ . I prove it by contraposition: assume $N$ can be decomposed as $A\times B$ where $A$ is non-trivial abelian. Then it is clear that $$ Z(N) = Z(A) \times Z(B) = A\times Z(B).$$ On the other hand, we have that $$ [N,N] = [A,A] \times [B,B] = \{1\} \times [B,B].$$ If we had that $Z(N) \subset [N,N]$ , we would need that $A\subset \{1\}$ , which is clearly not possible as $A$ was non-trivial. So by contraposition, we have proven the first implication. Now my problem is with the other implication. I don't know how I can prove the converse. I don't even know how to go about doing that. My gut suggests contraposition again, but then I have to use the assumption that $Z(N) \not\subset [N,N]$ to somehow construct a direct product decomposition of $N$ which has a non-trivial abelian factor, and I don't see how I can go about doing something like that. Does anyone have any suggestions? Thanks in advance! Edit: I added the assumptions that $N$ is torsion-free, finitely generated and nilpotent. These are the only groups I am interested in in the context of my research.","['derived-subgroup', 'direct-product', 'group-theory', 'abstract-algebra']"
4882446,"Coin flip puzzle, prove $P(X>Y) > P(Y>X)$","I recently encountered the following riddle: Let a fair coin be flipped $N > 2$ times. Player A gets a point every time there are two heads in a row, and player B gets a point every time a tail is followed by a head, the player with more points wins. Thus, for example, for the sequence THHH , player A wins with 2 vs. 1 points while in the sequence THHTHH , neither wins, as they both score two points. Clearly, the expected number of points is equal, as the sequences HH and TH are both equally likely. Now the riddle itself: Are player A's chances of winning higher, lower or equal to that of player B? I found it surprising that player A's chances of winning are lower than that of player B's. Though there are intuitive explanations, I'd like to rigorously prove this.","['puzzle', 'probability-distributions', 'probability']"
4882472,Infinite products that are equal to their geometric product integral,"Background In the following question and references therein, a number of ""Sum equals integral"" identities are described. For instance, we have $$ \sum_{n = -\infty}^{+\infty} {\rm sinc} (x)^{N}\, = \int_{-\infty}^{+\infty} {\rm sinc} (x)^{N}\, dx =  \pi, \tag{1}\label{1}$$ for $1\leq N \leq 6$ , about which one can find more information in this paper (PDF) by Baillie, Borwein, and Borwein from 2008. Moreover, there is the binomial identity $$\sum_{n = -\infty}^{+\infty} \binom{\alpha}{n} e^{int} = \int_{-\infty}^{+\infty} \binom{\alpha}{x} e^{itx} \, dx = (1+e^{it})^\alpha, \; \alpha  >-1 \tag{2}\label{2}$$ which is due to Pollard & Shisha (1973). Finally, the authors Dominici, Gill, and Limpanuparb (2012) state the following identity involving the Bessel J function in their article (PDF) $$\sum_{t=-\infty}^\infty \frac{J_y (at) J_y(bt)}{t}=\int_{-\infty}^\infty \frac{J_y (at) J_y(bt)}{t}\, \text{d}t. \tag{3}\label{3}$$ Product integrals I wonder whether similar identities exist involving infinite products and their corresponding product integral . The latter is a continuous analogue of the discrete product operator. There are multiple types of product integrals. For the purposes of this particular question, we stick to product integrals that are referred to as Type II in the wiki page referenced above: \begin{align*}
\prod_{a}^{b} f(x)^{dx} &:= \lim_{\Delta x \to 0} \prod f(x_{i})^{\Delta x} \newline 
&= \exp \left( \int_{a}^{b} \ln f(x) \ dx \right). \tag{4} \label{4}
\end{align*} This is called the geometric product integral . So what I'm looking for are identities of the form $$ \prod_{n=a}^{\infty} f(n) = \prod_{a}^{\infty} f(x)^{dx}. \tag{5}\label{5}  $$ Here, $a=0$ , $a=1$ , or $a=-\infty$ . In other words, I'm looking for identities that satisfy $$ \prod_{n=a}^{\infty} f(n) = \exp \left( \int_{a}^{\infty} \ln f(x) \ dx \right). \tag{6}\label{6}$$ If the identity holds when the limits of the product (integral) need to be shifted slightly on either side of the equation to make it work, that would also be a good example in my eyes. Own work and Question I've gone through a number of possibilities listed on this page . One example that comes somewhat close, but not quite, is the one associated with equation (30). It states that $$ \prod_{n=1}^{\infty} \left(1+\frac{1}{n^3} \right) = \frac{1}{\pi} \cosh \left( \frac{1}{2} \pi \sqrt{3} \right). \tag{7}\label{7} $$ Moreover, we have $$ \exp \left( \int_{0}^{\infty} \ln \left[ 1+ \frac{1}{x^3} \right] \ dx \right) = \exp \left( \frac{2 \pi}{ \sqrt{3}} \right). \tag{8}\label{8} $$ We know that $\cosh(x) := \frac{\exp(x)+\exp(-x)}{2}$ , so it appears there are some similarities in the final expression. (Notice that, in this case, we have slightly shifted the limits of the geometric product integral.)  However, \eqref{7} and \eqref{8} do no amount to the same number. So there is more work to do. Therefore, my question is: Are there any infinite products that are equal to their geometric product integral, thus satisfying equation \eqref{6} or some slight variant of it?","['big-list', 'definite-integrals', 'analysis', 'products']"
4882537,"What is the order of the following subgroup $\langle (1 \ 2 \ \cdots \ n), (a \ b)\rangle$ of $S_n$?","Let $n\geq 2$ be an integer, and consider $H=\langle (1 \ 2 \ \cdots \ n), (a \ b)\rangle\subset S_n.$ It is known that $H=S_n$ if and only if $b-a$ and $n$ are coprime. Question. What is the order of $H$ in general ? For the moment, I think I have managed to prove the following facts: Let $d=\gcd(n, b-a)$ . Then $H$ contains all the transpositions $(k \ k+\frac{n}{d})$ In particular, $H$ contains the subgroup generated by $(1 \ n/d+1 \ 2n/d+1\cdots )$ , which is isomorphic to $S_d$ , and $d!$ divises the order of $H$ . $H$ contains all the transpositions $(1 \ 1+ \ell(b-a)),$ and that's pretty much it. Thoughts. I wonder if $H$ is  isomorphic to a wreath product $S_d\wr S_{n/d}$ or something like that, which would solve the problem. Of course, if one may show that $H$ is a maximal subgroup of $S_n$ , then it would do the trick, since $H$ is transitive and imprimitive if $d>1$ . I woud be extremely surprised if the result was not already known. Thanks for your help.","['symmetric-groups', 'group-theory', 'finite-groups']"
4882565,How to determine a basis for the tangent space given a local trivialization.,"I heard the following: For a smooth submanifold $M \subseteq \mathbb{R}^n$ ,
given a local trivialization one can easily find a basis for
the tangent space. I want to know how.
So first I should maybe formalize the question:
Suppose $M \subseteq \mathbb{R}^n$ is a smooth $k$ -dimensional submanifold and for $p \in M$ , $\phi:U \rightarrow V$ a local trivialization of $M$ such that $x \in M \cap U$ . Then there exist functions $b_1,...,b_k$ such that $B=\{b_1(x),...,b_k(x)\}$ form a basis of $T_xM$ . My idea:
If $\phi:U \rightarrow V$ is a local trivialization, then $\frac{\partial}{\partial x_1} \phi(x)_{|x=p},...,\frac{\partial}{\partial x_k}\phi(x)_{|x=p}$ should be a basis for the tangent space $T_pM$ .
My problems at the moment: I don't know how to explicitly show my assumption. I.e. I need to show that for $v \in T_pM$ , $v$ can be written as a linear combination of $\frac{\partial}{\partial x_1} \phi(x)_{|x=p},...,\frac{\partial}{\partial x_k}\phi(x)_{|x=p}$ . And I also need to show that $\frac{\partial}{\partial x_1} \phi(x)_{|x=p},...,\frac{\partial}{\partial x_k}\phi(x)_{|x=p}$ are linearly independent. Question: How do I show above-mentioned? Does my idea work or am I wrong? Edit: I wanted to add the Definitions I use:
Let $M \subseteq \mathbb{R}^n$ . $M$ is a smooth $k$ -dimensional submanifold if for each $x \in M$ , there exists open subsets $U,V \subseteq \mathbb{R}^n$ were $x \in U$ and there exists a diffeomorphism $\phi:U\rightarrow V$ such that $\phi(U \cap M) = V \cap \mathbb{R}^k$ . I call $\phi$ a local trivialization. As far as I know, $\phi$ is sometimes also called a local parametrization. I want to show that there exist local coordinates $b_1(x),...,b_k(x)$ of the tangent space $T_xM$ .","['submanifold', 'tangent-spaces', 'differential-geometry']"
4882635,Does improvement on regularity implies compactness?,"Let $M$ be a compact metric space and $\mathcal C^0(M)=\{f:M\to\mathbb R; f\ \text{is continous}\}$ . Let $T:\mathcal C^0(M)\to \mathcal C^0(M)$ be a bounded linear transformation. Suppose that for every $f\in\mathcal C^0(M)$ we have that $Tf$ is $1/2$ -Hölder. Does this imply that $T$ is compact? It is clear that if $ | T f | _ {\mathcal C^{1/2} (M)}\leq K$ for some $ K> 0$ for every $\| f\|_{\mathcal C^0(M)}\leq 1$ , then Arzela-Ascoli implies the result. However, I believe that this is not generally true. Also, I am not able to imply that $T(B(0,1))$ is equicontinuous.","['functional-analysis', 'real-analysis']"
4882713,Stability of the null solution of a differential equation,"Is there a differential equation: $$\ddot x(t)=f(t, x(t)),\ f\in C^1(\mathbb{R}^2), \forall t\in \mathbb{R}\ f(t,0)=0;$$ that for any solution x(t) the following is true: $$\forall \epsilon>0\ \exists \delta>0\ (|x(0)|<\delta \implies \forall t\ge0\ |x(t)|<\epsilon)\ ?  $$ I guess that there is no such an equation. I have the following intuition. Let's look at the equation thinking that f is a force and x(t) is a coordinate of a particle that moves under that force. If we provide sufficiently enough initial speed $$\dot x(0)$$ to the particle, which is not limited, the particle will reach any high coordinate no matter where it was at the initial moment of time, because force f does not depend on the speed of the particle.","['stability-theory', 'ordinary-differential-equations']"
4882716,Direct method with integral constraint,"Let $\Omega\subset\mathbb{R}^n$ be nonempty, open and bounded with $C^1$ boundary. Let $p\in[1,n)$ . Let $g\in C(\mathbb{R})$ satisfy $$|g(y)|\leq C(1+|y|^q)$$ for some $C<\infty$ and some $q$ with $1\leq q<p^*$ . Let $(u_k)_k$ be a sequence with $u_k\in W^{1,p}(\Omega)$ and $$\int_{\Omega}g(u_k(x))dx=0 \text{  for every } k\in\mathbb{N}.$$ Show: When $u_k$ converges weakly to $u_*$ in $W^{1,p}(\Omega)$ , then $$\int_{\Omega}g(u_*(x))dx=0.$$ My attempt: By using the boundness of weakly convergent sequences, embedding theorems and Arzelà–Ascoli theorem, I obtained a uniformly convergent subsequence $(u_k)_k$ (without renaming), whose limit is $u_*$ due to the uniqueness of the limit. This yields: $$\left\lvert \int_{\Omega}g(u_*(x))dx\right\rvert\leq\int_{\Omega}\lvert g(u_k(x))-g(u_*(x))\rvert dx\leq \varepsilon \lvert\Omega\rvert$$ for $||u_k-u_*||_C<\delta$ using the continuity of $g$ .
What surprises me now is that I didn't use the growth condition of $g$ . Can you assist me with this? Any help is greatly appreciated! Edit: Equicontinuity: Morrey yields $W_0^{1,4}(\Omega)\subset C^{0,\frac{1}{4}}(\Omega)$ , so $[u_k]_{C^{0,\frac{1}{4}}(\Omega)}\leq c$ . This implies $$|u_k(x)-u_k(y)|\leq c|x-y|^{\frac{1}{4}}.$$ Choosing $\delta=(\frac{\varepsilon}{c})^4$ : For $|x-y|<\delta$ $$|u_k(x)-u_k(y)|\leq c\left(\left(\frac{\varepsilon}{c}\right)^4\right)^{\frac{1}{4}}=\varepsilon$$","['arzela-ascoli', 'calculus-of-variations', 'lebesgue-integral', 'functional-analysis', 'constraints']"
4882722,Proof of first step in SVD,"I have to proof the following statement: Prove that for a given $A\in \mathcal M _{n \times m}(\mathbb R)$ there exist two orthogonal matrices $U \in \mathcal O(n)$ , $V \in \mathcal O(m)$ such that: $$UAV^T=\begin{pmatrix}||A||_2 & 0\\ 0 & A_1 \end{pmatrix},$$ where $||A||_2$ is the 2 norm of the matrix and $A_1\in \mathcal M _{n-1 \times m-1}(\mathbb R)$ . The professor told me to use Gram-Schmidt orthonormalization process as a hint. My try so far has been the following. I need to find two rotation matrices that rewrite $A$ in the way above. To find the first one, I've thought of generating a set of orthonormal vectors $\{v_1, v_2, ..., v_m\}$ where $v_1\in \mathbb R^m$ is a vector that fulfills $||Av_1||_2=||A||_2$ . To find the second matrix, I've thought of using another rotation matrix that changes from the canonic basis in $\mathbb R^n$ to a orthogonal basis containing $\frac{Av_1}{||Av_1||_2}=\frac{Av_1}{||A||_2}$ as one of the vectors. This way, I have two orthogonal matrices that applyed to the original matrix give the first column as I want it to be. However, I can't find a way to prove that the rest of the first row has to be al zeros other than the first element. I think the solution has something to do with which linearly independent vectors $\{x_1,..., x_m\}$ I choose to perform Gram-Schmidt. If i could somehow find a way to find those vectors sutch that $Av_1 \perp \{Av_2, ..., Av_m\}$ id then have that the rest of the first row are zeros too.","['matrices', 'svd', 'linear-algebra', 'linear-transformations']"
4882732,Showing the uniqueness of a solution to an ordinary differential equation [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 months ago . Improve this question $$ \dot{y}(x)+y(x)=u(x) $$ $$ y_c(x) = ce^{-x} + \int_{0}^{x} u(t) e^{(t-x)} dt $$ Show that if u is periodic with a period of T, then there exists exactly one solution to the differential equation with the period T. Where here $y_c(0) = c$ holds. The use of the second equation is hinted. I am stuck trying to demonstrate the proposed property. Could someone provide a hint on how to proceed? Your help will be greatly appreciated.","['periodic-functions', 'ordinary-differential-equations']"
4882760,"Given $ye^{-{xe^{y-2x}}} = 2xe^{-x},$ where $x>\frac{1}{\sqrt{2}}, y<\sqrt{2}$. Show that $y=y(x)$ decreases.","Suppose $y$ is defined by the following implicit equation: $ye^{-{xe^{y-2x}}} = 2xe^{-x},$ where $x,y\geq 0.$ I want to show that $y$ decreases as $x$ increases, when $x>\frac{1}{\sqrt{2}}$ and $y<\sqrt{2}$ . Here is my work: Suppose by contradiction that there exist some $x_1, x_2> \frac{1}{\sqrt{2}}$ such that $x_1<x_2 \implies y_1<y_2.$ From the above relation, we have $$2x_1 = y_1e^{x_1(1-e^{y_1-2x_1})} \tag{1}$$ and $$2x_2 = y_2e^{x_2(1-e^{y_2-2x_2})} .\tag{2}$$ Subtracting this results in the following: $$2(x_2-x_1) = y_2e^{x_2(1-e^{y_2-2x_2})}- y_1e^{x_1(1-e^{y_1-2x_1})}.$$ From here, I am not sure how to proceed. The goal is to arrive at the contradiction $y_1>y_2$ with the assumption but getting this inequality seems somewhat challenging. Following @jean's comment: Take the natural logarithm on both sides of $(1)$ and $(2)$ , and subtract: \begin{align*}\ln{x_2}-\ln{x_1} &= \ln{y_2}-\ln{y_1} + x_2(1-e^{y_2-2x_2})- x_1(1-e^{y_1-2x_1})\\
& = \ln{y_2}-\ln{y_1} + (x_2-x_1) + (e^{y_1-2x_1}-e^{y_2-2x_2}).
\end{align*} This looks much better compared to the exponential expression.","['implicit-function', 'calculus', 'derivatives', 'inequality']"
4882799,Contour Integral of $1/z dz$ with triangle,"I was asked to find the contour integral $dz/z$ of a triangle with vertices $1-2i$ , $-2+i$ and $1+i$ . I am not sure how to begin this but my professor said that it would be: $$\int_{-1}^{2} \frac{-i}{1-ti} dt$$ But this doesn't make sense to me. why would $z$ be equal to $1-ti$ ? I can see he may be working from the vertically connected vertices but that would be $1+iy$ right?","['complex-analysis', 'contour-integration']"
4882801,Maximize $f(\mathbf n)=\dfrac{N!}{\prod_{j=1}^M n_j!}$ subject to $\sum_{j=1}^M n_j=N$ and $\sum_{j=1}^M e_jn_j=E$,"The following exercise is a recap on probability and maths for statistical mechanics: Maximize $$f(n_1,n_2,\, ...,\, n_M)=\dfrac{N!}{\prod_{j=1}^M n_j!}$$ subject to $$\sum_{j=1}^M n_j=N\ \text{ and }\ \sum_{j=1}^M e_jn_j=E,$$ with $e_j$ and $E$ constants. Physical background: This problem can be understood as a problem of $M$ states and $N$ independent particles. The aim is to determine what is the distribution of particles among the different states that maximizes the number of microstates compatible with fixed values of $N$ and $E$ . Attempt: I should impose $$\frac{\partial}{\partial n_i}(f(\mathbf n)-\lambda A(\mathbf n)-\beta B(\mathbf n))=0,\text{ for } i=\{1,\,...,M\},$$ where $A(\mathbf n)\equiv\sum_{j=1}^M n_j-N$ and $B(\mathbf n)\equiv \sum_{j=1}^M e_jn_j-E$ , and so, $$\frac{\partial}{\partial n_i}\left(\dfrac{N!}{\prod_{j=1}^M n_j!}-\sum_{j=1}^M(\lambda+\beta e_j)n_j -\lambda N-\beta E\right)=0$$ $$\dfrac{N!n_i!}{\prod_{j=1}^M n_j!}\dfrac{\partial}{\partial n_i}\left(\frac{1}{n_i!}\right)-(\lambda+\beta e_i)=0,$$ but now I'm stuck here as I don't really know how to calculate the derivative of $1/n_i!$ or even how to proceed once I do it because the derivative should include digamma or gamma functions...","['multivariable-calculus', 'combinatorics', 'lagrange-multiplier', 'statistical-mechanics']"
4882802,Computing the the exterior derivative of the connection matrix on a hermitian vector bundle,"If $D$ is the Chern connection on a holomorphic hermitian vector bundle $(E,h)$ and $e$ is a holomorphic frame, then $$\vartheta=\partial h \cdot h^{-1}$$ where $h_{ij}=h(e_i,e_j)$ . Here $\vartheta$ is the connection matrix and $\partial h$ means the del-derivative of the matrix $h$ . I was going through the following calculation $$
\begin{align*}
d\vartheta &= (\partial+\bar{\partial})\vartheta =\bar{\partial}\vartheta+\partial(\partial h\cdot h^{-1})= \bar{\partial}\vartheta-\partial h \wedge \partial h^{-1}\\
&= \bar{\partial}\vartheta + \partial h\wedge h^{-1}\partial hh^{-1} = \bar{\partial}\vartheta + \partial hh^{-1}\wedge\partial hh^{-1}.
\end{align*}
$$ I don't see where the last three equalities come from, firstly why do we have $$
\bar{\partial}\vartheta+\partial(\partial h\cdot h^{-1})= \bar{\partial}\vartheta-\partial h \wedge \partial h^{-1},
$$ is this an application of the Leibniz rule? Secondly why does this equal $$
\bar{\partial}\vartheta-\partial h \wedge \partial h^{-1}=\bar{\partial}\vartheta + \partial h\wedge h^{-1}\partial hh^{-1}
$$ and $$
\bar{\partial}\vartheta + \partial h\wedge h^{-1}\partial hh^{-1} =\bar{\partial}\vartheta + \partial hh^{-1}\wedge\partial hh^{-1}?
$$","['complex-geometry', 'differential-geometry']"
4882804,Pierpont pseudoprimes,"A Pierpont number is of the form $ p=2^a.3^b+1$ . (I used a>0) Trying to explore them, I did a very simple loop to test for primality: loop on g over primes if $g^{p-1}\mod p \neq 1$ , then p is not prime (Fermat's little theorem) else if order of g is p-1, then p is prime (can be easily checked as p-1 has at most two prime divisors, 2 and 3) else try next prime for g (either g is not a generator for prime p or p is a pseudo prime for base g) Surprisingly, almost all Pierpont non prime numbers can be detected with g=2. For numbers < $10^{360}$ , I only found 8 pseudoprime Pierpont numbers for base 2. 6 of them are not pseudoprimes for 3, 1 of them is pseudo prime for 2,3 and 5 (but not 7) and one (small) is pseudo prime for 2,3, 5, 7, 11 (but not 13). Also 2 and 3 are rarely generators for Pierpont primes, while 5 seems to be very often.
I was wondering if there was more precise results on all of that somewhere. Can it be proven that at some point, larger Pierpont numbers can't be pseudoprimes for base 2 (making primality test almost trivial on them)? Edit: no pseudoprimes for 2 found from $10^{360}$ to $10^{500}$ .","['number-theory', 'prime-numbers']"
4882955,Representation of $V$ as $\mathbb{C}^{2}$,"Let $V$ be a finite-dimensional complex inner product space and suppose that there is an operator (a matrix) $A$ on $V$ that satisfies the following anti-commutation relations: $$AA + AA = 0$$ $$A^{*}A + AA^{*} = I,$$ where $I$ is the identity matrix and $A^{*}$ is the adjoint of $A$ . In this case, one can show that $V$ has an even dimension and that $A$ has a representation as a $2 \times 2$ matrix: $$A = \begin{pmatrix}
0 & 1 \\
0 & 0 
\end{pmatrix}
\tag{1}\label{1}
$$ This is done in Simon's book . The basic idea is that $V$ has an orthogonal direct sum decomposition $V = \operatorname{Ker}(A) \oplus \operatorname{Ker}(A^{*})$ and, thus, every vector $\phi \in V$ can be seen as a vector: $$\phi = \begin{pmatrix}
\varphi \\
\psi
\end{pmatrix}
\tag{2}\label{2}$$ with $\varphi \in \operatorname{Ker}(A)$ and $\psi \in \operatorname{Ker}(A^{*})$ . In this case, $A$ acts as a $2 \times 2$ matrix on these vectors. I have some questions regarding some terminology and concepts written on Simon's book. First, I suppose the representation (\ref{1}) should be, strictly speaking, $$A = \begin{pmatrix}
0 & I \\
0 & 0
\end{pmatrix}
$$ that is, with $1$ replaced by the identity matrix $I$ on $V$ , but then I can simply see $I$ as $1$ in some sense. This is fine. The terminology ""representation"" (as in $A$ has a representation...) is a bit more troublesome. A representation of a group $G$ in a vector space $U$ is a group homomorphism $\rho: G \to \operatorname{GL}(U)$ . Of course, the space of all linear operators (i.e. complex matrices) on $V$ is a group, and we can talk about a representation of these matrices on $U = \mathbb{C}^{2}$ . But the above setting only deals with one matrix $A$ , not all of them , so I am a bit confused with the usage of the term representation in this context. Does it have any rigorous meaning or it is just a way of writing $A$ can be informally seen as the right hand side of (\ref{1})? More importantly, Simon seems to imply that the ""representation"" of $A$ as the $2 \times 2$ matrix in (\ref{1}) imply that $V$ itself can be viewed as $\mathbb{C}^{2}$ . This is very confusing to me. On one hand, it makes sense because $A$ can be seen as a $2 \times 2$ matrix, so it should act on elements of $\mathbb{C}^{2}$ . However, the only decomposition of vectors of $V$ being used is (\ref{2}), and it is absolutely not clear why this can be seen as $\mathbb{C}^{2}$ if each entry $\varphi$ and $\psi$ is not uniquely determined by a complex number (that is, $\phi$ in (\ref{2}) is not a a vector on $\mathbb{C}^{2}$ ). Can someone please help me with finding the right definitions of the concepts being used by Simon and justifying its way of seing $V$ as $\mathbb{C}^{2}$ ?","['representation-theory', 'vector-spaces', 'analysis', 'abstract-algebra', 'linear-algebra']"
4882970,How to construct an algebraic function that is not rational?,"How to construct a algebraic irrational function $f(x)$ such that $$f(x)= \sum_{i=1}^{\infty}a_i x^i$$ with $a_1, a_2,\dots, a_i,\dots \in \mathbb{N}$ . Reference is appreciated. Update : an instance is $$f(x)=\frac{2}{1+\sqrt{1-4x}} =\sum  C_n x^n$$ with $c_n$ being the $n_{th}$ Catalan number. Thank to Jyrki Lahtonen. And some are here Hope more examples.","['power-series', 'algebraic-geometry', 'functions', 'reference-request']"
4882983,"A standard 6-sided fair die is rolled until the last 3 rolls are strictly ascending. What is probability that the first such roll is a 1,2,3, or 4?","A standard $6$ -sided fair die is rolled until the last $3$ rolls are strictly ascending. What is probability that the first such roll is a $1$ , $2$ , $3$ , or $4$ ? My attempt We can investigate the $3-$ roll (when there are exactly $3$ rolls), the $4-$ roll, the $5-$ roll, the $n-$ roll.
I did that via SQL, basically n times a cartesian product of a table with digits $1-6$ , with some constraints in place. There are $20$ ascending triplets that can be thrown. Starting with $1: 123, 124, 125, 126, 134, 135, 136, 145, 146, 156$ I call those triplets $T_1$ . There are $10$ $T_1$ ‘s.
Likewise $T_2$ : $234, 235, 236, 245, 246, 256$ . There are 6 $T_2$ ’s. $T_3$ : $345, 346, 356$ . There are $3$ $T_3$ 's. $T_4: 456$ . There is only one $T_4$ . We can start with the $3-$ roll: The probability of a $3-$ roll is $\frac{20}{216}=\frac{5}{54}$ . Each triplet has the same probability of being rolled. So the probability of each triplet is $\frac{1}{20}$ . The $4-$ roll. Not any digit can be the first digit. For instance ' $456$ ' can't be prepended by ' $1$ ' or ' $2$ ' or ' $3$ ', as that would have resulted in a $3-$ roll. If we break it down we have $6$ digits before a $T_1$ , $5$ digits before a $T_2$ , $4$ digits before a $T_4$ . In total $6*20+5*6+4*3+3*1=105$ quartets. Of those $6$ belong to a specific $T_1$ , $5$ to a specific $T_2$ , $4$ to a specific $T_3$ , $3$ to a specific $T_4$ . The probabilities of a specific $T_1$ is $\frac{6}{105}; T_2: \frac{5}{105}; T_3:\frac{4}{105}, T_4: \frac{3}{105}$ .
The probability of a $4 -$ roll happening is $ \frac{105}{6^4}=\frac{35}{432}$ For the $5-$ roll we can prepend any digit, so there will be 6*105 quintets and $6*6^4$ total ways, so the probability is still $\frac{6.105}{6.6^4}=\frac{105}{6^4}=\frac{35}{432}$ .
The probabilities of a specific $T_1$ is $\frac{6.6}{6.105}=\frac{6}{20}$ ; $T_2$ : $\frac{6.5}{6*105}=\frac{5}{105}$ ; $T_3$ : $\frac{6.4}{6*105}=\frac{4}{105}$ , $T_4$ : $\frac{6.3}{6.105}=\frac{3}{105}$ . All probabilities are the same compared the $4-$ roll. Now look at the $6-$ roll. Like with the $4-$ roll not all digits can be prepended. An sql query tells me there are $3381$ ways to have an 6-roll and that makes the probability $\frac{3381}{6^6}=\frac{1127}{15552}$ .  To tie them into the T-groups I've reworked the formula's from the $5-$ roll to $6-\frac{\frac{20}{36}}{\frac{2217}{12}}$ for a $T_1$ ; $5-\frac{\frac{20}{36}}{\frac{2217}{12}}$ for a $T_2$ ; $4-\frac{\frac{20}{36}}{\frac{2217}{12}}$ for a $T_3$ ; $3-\frac{\frac{19}{36}}{\frac{2217}{12}}$ for a $T_4$ . This is all matching up. The formula's, just as the sql yield, $1960$ $T_1$ 's, $960$ $T_2$ 's, $372$ $T_3$ 's, $89$ $T_4$ 's$. Now the $7-$ roll, and this is where I get stuck. I assumed/hoped we could still use the formulas from the $6-$ roll, and I assumed they could be used for the $nth-$ roll, where $N>6$ . But in the $7-$ roll up to the $10-$ roll (the $10-$ roll is the last one I checked), the T-groups are not 'hit' by the invalid paths proportionally. Although there's only a slight deviation, this means we can't use the formula's from the $6-$ roll, and we can't get a precise answer on our question. So this whole approach seems flawed. Is there another way or a better approach, is what I'd like to know.","['dice', 'probability']"
4882987,Random walk where Increments have exponential distribution. Probability of never reaching a negative value after $n$ steps.,"Consider the random walk $S_n = \sum_{i=1}^n (X_i-1)$ where $X_i$ are i.i.d. with exponential distribution and mean $1$ , i.e. $P(X_i \leq x) = 1-e^{-x}$ . I am trying to figure out the probability $p_n$ , that the random walk has never reached a negative value after $n$ steps. I have obtained the exact values for the first small $n$ : $p_1 = e^{-1}$ $p_2 = 2e^{-2}$ $p_3 = \frac{9}{2}e^{-3}$ $p_4 = \frac{64}{6}e^{-4}$ I have noticed the pattern, that apparently $p_n = \frac{n^{n-1}}{(n-1)!}e^{-n}$ . I have also done some numeric simulations that seem to confirm that this is in fact the answer. However, I have been unable to prove this. I have tried using the law of total probability, but there I then need to determine the probability that a random walk like above never reaches a negative value if it starts at a specific (positive) value, which I don't know how to compute. If the exact value is too difficult to prove, I would also be content with an upper bound that also converges to 0 for large $n$ . Any insight is greatly appreciated.","['random-walk', 'exponential-distribution', 'probability']"
4883064,"$a^2-b^2=37$, evaluate $ a^2+b^2$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 months ago . Improve this question Given $a^2-b^2=37$ and also a and b are integers, can we evaluate $a^2+b^2$ possible values? Are those many or just some? I found that $a^2+b^2$ can be only $685$ . But how to prove it? I just guessed, but can we somehow evaluate it?",['algebra-precalculus']
4883066,What is the probability of drawing 5 cards of the same type from a deck of 52 cards?,"My textbook has the following problem: There is 52 cards in a deck and 13 cards of each type/color. You are drawing 5 cards. Whats the probability of all these 5 cards being the same type? My solution: There is a $\frac{52}{52}$ probability of drawing the first card, then a $\frac{12}{51}$ chance of drawing a second card of the same type as the first one and so on... $$\frac{52}{52} \cdot \frac{12}{51} \cdot \frac{11}{50} \cdot \frac{10}{49} \cdot \frac{9}{48} = \frac{33}{16660}$$ I solved the same problem by calculating the individual probabilities for each card type and then adding all the probabilities together: $$4\cdot(\frac{13}{52} \cdot \frac{12}{51} \cdot \frac{11}{50} \cdot \frac{10}{49} \cdot \frac{9}{48}) = \frac{33}{16660}$$ My textbook says the solution is $(\frac{13}{51} \cdot \frac{12}{50} \cdot \frac{11}{49} \cdot \frac{10}{48})$ without any explanation. Although I don't see how you arrive at this answer. Whats wrong with my way of solving the problem and how do you arrive at the textbooks solution?","['combinatorics', 'card-games', 'probability']"
