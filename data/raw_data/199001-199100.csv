question_id,title,body,tags
3864674,Definition of local complete intersection,"In Hartshorne, there is the following definition of a “local complete intersection”: Now, what does “locally generated by $r$ elements at every point"" mean? I think one of the following: $X$ can be covered by affine open sets $\{U_i＝\operatorname{Spec} A_i\}$ , such that ideal $I_i\subset A_i$ (corresponding to $\mathscr{I}_i\subset \mathscr{O}_{U_i}$ ) can be generated $r$ elements. For all $x\in X$ , the ideal $\mathscr{I}_x\subset \mathscr{O}_{x,X}$ can be generated $r$ elements. Which is the correct definition? Or are these two definitions equivalent?","['ring-theory', 'algebraic-geometry']"
3864698,Is this generalization of the Brier score strictly proper?,"Let $\Omega$ be a set, and let $\mathcal P$ be the set of finitely additive probability measures defined on $2^\Omega$ . If $\Omega$ is finite with $|\Omega| = N$ , then the Brier score , defined by $$B(P, \omega) = \frac{1}{N}\sum_{A \subseteq 2^\Omega} (P(A) - 1_A(\omega))^2,\tag{1}$$ for all $P \in \mathcal P$ and $\omega \in \Omega$ , is strictly proper , that is: $$\sum_\omega B(P, \omega) P(\omega) \leq \sum_\omega B(Q,\omega)P(\omega)\tag{2}$$ for all $Q \in \mathcal P$ , and the inequality is strict unless $Q = P$ . The proof involves nothing more than basic calculus. I'm wondering if this result can be generalized to arbitrary $\Omega$ . Here's the idea. First, we replace the summation in (1) with some bounded linear functional. In particular, Let $X$ be the linear space of real-valued bounded functions on $2^\Omega$ , and let $f_{P, \omega}$ be the function in $X$ defined by $f_{P, \omega}(A) = (P(A) - 1_A(\omega))^2$ . Let $\ell$ be a positive bounded linear functional on $X$ . Now generalize the definition in (1) by defining $$B(P, \omega) = \ell(f_{P, \omega}).\tag{3}$$ Next, we replace the sums in (2) with integrals. Since $B(Q, \cdot)$ is non-negative for all $Q \in \mathcal P$ , we can define its integral with respect to $P$ as the supremum of the $P$ -integrals of simple functions that are dominated by $B(Q,\cdot)$ , where the finitely additive integral of a simple function is defined in the usual way. (See this , for example.) My question, then, is Is it true that, with $B$ defined by (3), $$\int B(P, \omega)P(d\omega) \leq \int B(Q, \omega)P(d\omega)$$ holds for all $P,Q \in \mathcal P$ , with strict inequality unless $Q = P$ ? One idea I had for approaching the problem is to fix $P \in \mathcal P$ and consider the function $g_P: Q \mapsto \int B(Q,\omega)P(d\omega)$ . I want to know whether $g_P$ achieves a minimum uniquely at $P$ . Now, by Jensen's inequality $f_{\lambda P_1 + (1-\lambda) P_2, \omega} \leq \lambda f_{P_1, \omega} + (1-\lambda) f_{P_2, \omega}$ holds for all $P_1,P_2 \in P$ , $\omega \in \Omega$ , and $\lambda \in [0,1]$ . Thus, because $\ell$ and the $P$ -integral are both order-preserving, $g_P$ is a convex function. So my question is essentially a convex optimization problem. I'm uncertain how to proceed from here.","['statistics', 'measure-theory', 'functional-analysis', 'convex-analysis', 'probability-theory']"
3864711,Maximizing the value of $\int_0^1 f(x)f^{-1}(x)\ \mathrm dx$,"I am trying to find the maximum size of the integral $\int_0^1 f(x)f^{-1}(x)\ \mathrm dx$ for differentiable, increasing $f$ with $f(0)=0$ and $f(1)=1$ . I made up this exercise for myself and thought it would be easy, but I can't do it. I feel the answer should be $\frac 1 3$ intuitively, which comes from $f(x)=x$ . So far I've tried integration by parts but then I don't know what to do. Edit: here is the integration by parts I tried, though I think it doesn't lead anywhere: $$\int^1_0 f(x)f^{-1}(x)\ \mathrm dx=\int_0^1f^{-1}(x)\ \mathrm dx-\int_0^1f'(x)\left(\int_0^x f^{-1}(t)\ \mathrm dt\right)\ \mathrm dx\text.$$ I thought this could help because $f'(x)>0$ since $f$ is increasing and the other factor in this integral is also positive by default.","['integration', 'optimization', 'calculus']"
3864743,"Must any $\phi \in \operatorname{Hom}_G(V, L^2(G))$ have continuous values?","Let $G$ be a compact group and $V$ a finite-dimensional vector space with a continuous $G$ -action. Consider a linear map $\phi: V \to L^2(G)$ satisfying that for any $v \in V, h \in G$ : $$
    \phi(v)(g h) = \phi(h \cdot v)(g) \quad \text{for almost all $g \in G$}
$$ Must $\phi(v)$ be continuous for any $v$ ? This is used (implicitly) in Serganova's A Journey Through Representation Theory (Chapter 2, Lemma 2.3) to prove that matrix coefficients $$
\begin{align}
    V^* \times V \times G &\to \mathbb{C} \\
    \alpha, v, g &\mapsto \alpha(g \cdot v)
\end{align}
$$ provide an isomorphism $V^* \cong \operatorname{Hom}_G(V, L^2(G))$ , and ultimately prove the Peter-Weyl theorem.","['measure-theory', 'lp-spaces', 'representation-theory', 'topological-groups']"
3864755,"Proving $\sum_{k=1}^{n} \prod_{1 \leq i \leq n, i \neq k} \cot(a_k-a_i) = \sin \frac{n \pi}{2}$","Show that for any real numbers $a_1, a_2, \dots , a_n$ where no numbers differ by a multiple of $\pi$ $$\sum_{k=1}^{n} \prod\limits_{\substack{1 \leq i \leq n\\ i \neq k}} \cot(a_k-a_i) = \sin \frac{n \pi}{2}$$ This looks like a determinant of a certain matrix. There are a number of similar constructions like Showing that $\sum_{i = 1}^m \frac{1}{\prod_{j = 1, j \neq i}^m (a_j - a_i)}$ is zero The sum of difference products related to determinants with factorials Prove determinant of a matrix $=\prod_{j<i}(a_i-a_j)$ And then maybe we can associate that with the geometric definition of determinants?","['complex-analysis', 'algebra-precalculus', 'linear-algebra', 'trigonometry']"
3864780,Birthday problem: solving with permutations vs combinations,"I have a problem with an exercise: If k people are at a party, what is the probability that at least two of them have the same birthday? Suppose that there are n=365 days in a year and all days are equally likely to be the birthday of a specific person. What is the probability for k=23? And according to the solution sheet the right answer is: $Pr=1-\frac{P_{365,k}}{365^k}$ and $Pr_{k=23} \approx 0.5073$ I think I understand the way of thinking behind this solution but my way of thinking was quite different: Let's assume that $\#B$ is the number of pairs having the same birthday. The probability of NOT having the same birthday for a single pair is $p_b=1-\frac{1}{365}=\frac{364}{365}$ so for all the pairs we have: $P(\#B \ge 1)=1-P(\#B =0)=1-(\frac{364}{365})^{C_{k,2}}$ where $C_{k,2}$ is the number of possible pairs. This seems quite different than the solution from the solution sheet and the exact result for $k=23$ seems also to be a little bit different because (according to Wolfram alpha) I get: $Pr_{k=23} \approx 0.5004771$ What am I doing wrong? Or maybe the two solutions are equivalent and the difference in the results for $k=23$ is caused by some numerical approximations? Thank you in advance for your help.","['permutations', 'combinations', 'birthday', 'combinatorics', 'probability']"
3864844,Representation for the survival function of the multinomial distribution in terms of the Dirichlet density?,"Let $K_p\sim \text{Binomial}(n,p)$ where $n\in \mathbb{N}$ and $0 \leq p \leq 1$ . Simple computations show that, for $1 \leq k \leq n-1$ and $p_0\in (0,1)$ , \begin{align}
    \mathbb{P}(K_{p_0} \geq k)
    &= \int_0^{p_0} \frac{d}{d p} \mathbb{P}(K_p \geq k) d p \notag \\[1mm]
    &= \int_0^{p_0} \left[\frac{d}{d p} \sum_{i=k}^n \binom{n}{i} p^i (1 - p)^{n - i}\right] d p \notag \\[1mm]
    &= \int_0^{p_0} \left[\hspace{-1mm}
        \begin{array}{l}
            n \sum_{i=k}^n \binom{n-1}{i-1} \, p^{i-1} (1 - p)^{(n-1) - (i-1)} \\[1mm]
            - n \sum_{i=k}^{n-1} \binom{n-1}{i} \, p^i (1 - p)^{(n-1) - i}
        \end{array}
        \hspace{-1mm}\right] d p \notag \\[1mm]
    &= \int_0^{p_0} \left[\hspace{-1mm}
        \begin{array}{l}
            n \sum_{i=k}^n \binom{n-1}{i-1} \, p^{i-1} (1 - p)^{(n-1) - (i-1)} \\[1mm]
            - n \sum_{j=k+1}^n \binom{n-1}{j-1} \, p^{j-1} (1 - p)^{(n-1) - (j-1)}
        \end{array}
        \hspace{-1mm}\right] d p \quad (\text{with } j = i + 1) \notag \\
    &= \int_0^{p_0} \left[n \binom{n-1}{k-1} \, p^{k-1} (1 - p)^{(n-1) - (k-1)}\right] d p \notag \\
    &= \frac{n!}{(k-1)! (n - k)!} \int_0^{p_0} p^{k-1} (1 - p)^{n-k} d p.
\end{align} This turns the survival function of the binomial distribution into an integral over the density of the beta distribution. Is it possible to derive an analogous representation for the survival function of the multinomial distribution in terms of an integral over the density function of the Dirichlet distribution ?","['multivariable-calculus', 'calculus', 'statistics', 'probability']"
3864847,Extension of Galois Correspondence to Solenoids,"It is well-known that covering spaces of a sufficiently nice space $X$ are in bijective correspondence with subgroups of $\pi_1(X)$ . However, ""limiting"" phenomena do not play nice with this correspondence. For example, if I take the double cover of the circle, then, topologically, this is just another circle. But it is more instructive to think of the two sheeted cover as an actual cover, with monodromy data telling me that when I pass around a loop, I flip to the other sheet. One can iterate this construction on the resulting circle, and in fact can do this infinitely. You can ask for the projective limit of the system of covering spaces, each of degree $2$ . This is not a covering space anymore, as the fiber is not discrete anymore, but is in fact homeomorphic to a Cantor set(!), which can be seen by showing it is homeomorphic to the ends of the infinite binary tree. This viewpoint can give us a heuristic for why the space should not be path-connected, which you could probably turn into a proof if you try. The idea would be that two generic points of the solenoid live on sheets indexed by points in the Cantor set, which in general, involve traversing something like $2^k$ loops in order to reach the correct sheet. On the other hand, the solenoid is compact, so if I take any sequence of points $x_k$ differing by $2^k$ sheets, there is a limit point, but there shouldn't be a path between them, because it is ""too long"" to reach. It's rather like running from one end of the infinite binary tree back to finite space and then running to a different end. You just can't do this with a closed interval. Is there some notion of ""completion"" of the fundamental group, in this case just $\mathbb{Z}$ , where subgroups of this new object correspond to such whacked out ""coverings?"" It would be most interesting if a solenoid where you always take the $p$ -fold cover ""corresponded"" to the $p$ -adic integers, but let's not yet dare to dream too much.","['general-topology', 'algebraic-topology', 'covering-spaces']"
3864871,Clarification question about the definition of irreducible topological space.,"We say that a topological space $X$ is reducible if $X$ can be written as a union of two proper non-empty closed subsets $X_1$ and $X_2$ i.e. $$X=X_1\cup X_2.$$ Is there an equivalent definition of an irreducible set $X$ where we take $X_1$ and $X_2$ to be open? I think that I found one good example which suggests ""No"" as an answer. Let's say that we want to prove a proposition that every Noetherian space $X$ can be written as a finite union of irreducible component $X_i$ where $X_i$ are open for all $i$ . Then we will struggle to show the existence since we use the fact that $X_i$ are closed for all $i$ in the original prove to construct a proper chain of closed sets which will stabilize. If $X_i$ are open, then we cannot do it anymore. So, are there other reasons why $X_i$ should be closed? Also, the original definition of $X$ being irreducible can be reformulated that any open proper subset $Y$ of $X$ is dense in $X$ . So, if we are going to mirror the definition, then $Y$ cannot be dense in $X$ as $Y$ is closed i.e. $\overline{Y}=Y$ .","['zariski-topology', 'general-topology', 'algebraic-geometry']"
3864897,"In trapezoid $ABCD$, $AB \parallel CD$ , $AB = 4$ cm and $CD = 10$ cm.","In trapezoid $ABCD$ , $AB \parallel CD$ , $AB = 4$ cm and $CD = 10$ cm. Suppose the lines $AD$ and $BC$ intersect at right angles and the lines $AC$ and $BD$ when extended at point $Q$ form an angle of $45^\circ$ . Compute the area of $ABCD$ . What I Tried :- Here is the picture :- Now to find the area of $ABCD$ , I just need to find its height, but I cannot find it. I can see that $\Delta AOB \sim \Delta COD$ . So :- $$\frac{AB}{CD} = \frac{AO}{OD} = \frac{BO}{OC} = \frac{2}{5}$$ So I assumed $AO = 2x$ , $BO = 2y$ , $CO = 5y$ , $DO = 5x$ . Now in $\Delta AOB$ , by Pythagorean Theorem :- $AO^2 + OB^2 = AB^2$ $\rightarrow 4x^2 + 4y^2 = 16$ $\rightarrow x^2 + y^2 = 4$ Also $\Delta QAB \sim \Delta QDC$ . So:- $$\frac{QA}{AC} = \frac{QB}{BD}$$ I get $AC$ and $BD$ by Pythagorean Theorem again, which gives me :- $$\frac{QA}{\sqrt{4x^2 + 25y^2}} = \frac{QB}{\sqrt{25x^2 + 4y^2}}$$ I don't know how to proceed next, as this result only gives me that $\left(\frac{QA}{OB}\right)^2 = \frac{21y^2 + 16}{21x^2 + 16}$ . Also I couldn't think of any way to use the $45^\circ$ angle, except that I can figure out that the triangle is cyclic. Can anyone help?","['quadrilateral', 'euclidean-geometry', 'problem-solving', 'geometry']"
3865049,Which one of the following groups is isomorphic to the group $G\ $?,"Let group $$G = \left \langle a,b,c\ |\ a^2 = b^2 = c^2 = 1, aba = bab, bcb = cbc, ac = ca \right \rangle.$$ Which one of the following groups is isomorphic to the group $G\ $ ? (1) $D_8,$ Dihedral group of order $8.$ (2) $\Bbb Z_2 \oplus \Bbb Z_2 \oplus \Bbb Z_2.$ (3) $S_3,$ Symmetric group of order $6.$ (4) $S_4,$ Symmetric group of order $24.$ What I find is that $ab,ba,bc,cb$ are all distinct elements in $G$ of order $3.$ Moreover $ab$ and $ba,$ $bc$ and $cb$ are inverses of each other. Since $3$ doesn't divide $8,$ $G$ can't be isomorphic to $D_8$ or $(\Bbb Z_2 \oplus \Bbb Z_2 \oplus \Bbb Z_2).$ Also since $G$ has at least four distinct elements of order $3,$ it can't be isomorphic to $S_3$ as well. Hence the group $G$ has to be isomorphic to $S_4.$ So the last option is the only correct option. But I can't able to show it explicitly that $(4)$ is indeed the correct option. Can anybody please check my reasoning above and suggest some technique to actually prove it explicitly? Thanks in advance.","['group-isomorphism', 'combinatorial-group-theory', 'finitely-generated', 'abstract-algebra', 'group-theory']"
3865066,Existence of an analytic function on disk,"Let $D=\{z\in\mathbb C:|z|<1\}\subset\mathbb C$ . Does there exist an nonconstant analytic function $g:D\to\mathbb C$ satisfying $$g\bigg(\frac1n\bigg)=g^2\bigg(-\frac{1}n\bigg), 
\,\,\,\,\,\text{for every integer $n$ greater than 1?}
$$ Here is what I have done: I am assuming the answer is no, so suppose, for the sake of contradiction, that such $g$ exists. Consider the sequence $(1/2,1/3,1/4,\dots)$ who's terms are all in $D$ . This sequence is convergent to $0$ . Define a function $h(z)=g(z)-g^2(-z)$ , which is analytic. We see that $$
h(1/2)=h(1/3)=h(1/4)=\dots=0,
$$ From the Identity Theorem, $h$ is zero everywhere on $D$ . So I conclude that $g(z)=g^2(-z)$ everywhere. What must I do from here? Thanks.","['complex-analysis', 'analytic-functions']"
3865089,"Riemann volume forms: Absolute value of the determinant of the metric, or not?","When using differential geometry in the context of General Relativity, you are usually taught that the general invariant scalar volume element reads $$ \mathrm{d}V=\sqrt{|g|}\mathrm{d}x^n $$ for an n-dimensional Riemannian manifold. With $g$ being the determinant of the metric tensor. Cf. for example [1,2] and also the Wikipedia article on volume forms [3]. Now, my question is: Is it just a convention that we use the $\textit{absolute value}$ of the determinant when defining the volume form? In the Wikipedia article [3] it is mentioned that volume forms are in general not unique. Does this imply that we could equally say that $\mathrm{d}V=\sqrt{g}\mathrm{d}x^n$ is the volume form of choice? PS: Note that this would imply for the Minkowski metric ( $g=\mathrm{diag}(-1,1,1,1)$ ), that in one incident that volume form would be real and in the other imaginary. See for example this article [4] and the consequences for the complex structure of Quantum Mechanics. [1] http://www.blau.itp.unibe.ch/newlecturesGR.pdf Eq. 4.47 and 4.51 [2] https://ned.ipac.caltech.edu/level5/March01/Carroll3/Carroll2.html Eq. 2.48 [3] https://en.wikipedia.org/wiki/Volume_form#Riemannian_volume_form [4] Lindgren, Liukkon 2019: https://www.nature.com/articles/s41598-019-56357-3","['riemannian-geometry', 'differential-geometry']"
3865144,"Laplace equation in polar coordinates, using matrices","I saw that the method shown below could be used to derive the Laplace equation for polar coordinates using less calculations. \begin{aligned}
&\nabla^{2} u=\frac{\partial^{2} u}{\partial r^{2}}+\frac{1}{r} \frac{\partial u}{\partial r}+\frac{1}{r^{2}} \frac{\partial^{2} u}{\partial \theta^{2}}\\
&\left(\begin{array}{c}
\frac{\partial}{\partial r} \\
\frac{\partial}{\partial \theta}
\end{array}\right)=\left(\begin{array}{ll}
\frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\
\frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial \theta}
\end{array}\right)\left(\begin{array}{c}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y}
\end{array}\right) \quad \longrightarrow \quad\left(\begin{array}{c}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y}
\end{array}\right)=\left(\begin{array}{ll}
\frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\
\frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial \theta}
\end{array}\right)^{-1}\left(\begin{array}{c}
\frac{\partial}{\partial r} \\
\frac{\partial}{\partial \theta}
\end{array}\right)\\
&\nabla^{2}=\left(\begin{array}{c}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y}
\end{array}\right) \cdot\left(\begin{array}{c}
\frac{\partial}{\partial x} \\
\frac{\partial}{\partial y}
\end{array}\right)
\end{aligned} Even after computing the following matrix: \begin{pmatrix}\frac{\partial x}{\partial r} & \frac{\partial y}{\partial r} \\
\frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial \theta}
\end{pmatrix} (using $x=r\cos\theta, y=r\sin\theta$ ), I still do not know how the Laplacian $\nabla^{2} u=\frac{\partial^{2} u}{\partial r^{2}}+\frac{1}{r} \frac{\partial u}{\partial r}+\frac{1}{r^{2}} \frac{\partial^{2} u}{\partial \theta^{2}}$ is gotten. What especially confuses me is the dot product and how the $2\times1$ operator gets applied to a $2 \times2$ matrix.","['laplacian', 'multivariable-calculus', 'linear-algebra', 'partial-differential-equations', 'partial-derivative']"
3865180,"Prove that $\sum\limits_{j=0}^k\,j\,\binom{n}{j}\,\binom{n-j}{2k-2j}\,2^{2k-2j}=n\binom{2n-2}{2k-2}$","I encountered this in my homework. I derived two ways to solve the problem and the answer which I have tested using programming, seem to be the same,  but I am not sure how to prove this equation. Let $n$ and $k$ be nonnegative integers with $k\leq n$ .  Prove that $$\sum\limits_{j=0}^k\,j\,\binom{n}{j}\,\binom{n-j}{2k-2j}\,2^{2k-2j}=n\binom{2n-2}{2k-2}\,.$$ The original problem is the following: A shoe rack has n pairs of shoes.  Of those, 2k individual shoes are chosen at random, k ≤ n. Calculate the expected number of matching shoes among 2k chosen shoes. The left hand side is from directly calculating expectation, while the right hand side is using sum of indicator variables of each pair being chosen. The expectation is just the equation divided by $\binom{2n}{2k}$ .","['summation', 'binomial-coefficients', 'combinatorics', 'analytic-combinatorics']"
3865185,Comparing $L_p$ norms of sums of Gaussians and Bernoulli random variables,"Problem : Let $(\epsilon_i), (g_i)$ be sequences of independent Bernoulli $(\{1,-1\},0.5)$ and Gaussian $(0,1)$ random variables respectively (the sequances are also independent from each other). Show that $$\| \sum_{i=1}^n g_ia_i\|_{L_p} \ge \frac{\sqrt{2}}{\sqrt{\pi}} \| \sum_{i=1}^n \epsilon_i a_i\|_{L_p}$$ for any $p \ge 1$ and any fixed real sequence $(a)$ and fixed $n \in \mathbb N.$ Attempt: The hint is to start by showing that $\epsilon_i |g_i|$ is also a standard Gaussian random variable which is reasonably doable. Furthermore noticing that $\sqrt{2/\pi}= \|g_i\|_{L_1}$ we have $\| g_N \|_1 \| \sum a_i\epsilon_i\|_{L_p} =\left[(\int |g_N|)^p(\int| \sum a_i \epsilon_i|^p) \right]^{1/p}$ and $$ \left[(\int |g_N|)^p(\int| \sum a_i \epsilon_i|^p) \right]^{1/p} \overbrace{\le}^{Jensen} \left[(\int |g_N|^p)(\int| \sum a_i \epsilon_i|^p) \right]^{1/p} \overbrace{=}^{independence} \left[\int (|g_N|\times| \sum a_i \epsilon_i|)^p \right]^{1/p}$$ but carrying on from here seems difficult because we're losing the independence at the last step. Context: The goal is to show that the Khinchine inequalities (top of page 14 in the source) for Bernoulli random variables (ie the $\psi_2$ character of the finite sum $\sum_1^n \epsilon_i a_i$ for an arbitrary real sequence $(a)$ and IID random variables $\epsilon_i$ following the Bernoulli law on $\{1,-1\}$ with parameter $p=1/2$ ).
One known way to do this is via the Hoeffding inequality. However here we want to do it via the $\psi_2$ character of $\sum_1^n a_i g_i$ for $g_i$ iid standard gaussian variables. Let $(\epsilon_i), (g_i)$ be sequences of independent Bernoulli $(\{1,-1\},0.5)$ and Gaussian $(0,1)$ random variables (the sequances are also independent from each other). Q1. Show that $\epsilon_i |g_i|$ is also a standard Gaussian random variable. This question is relatively simple and follows from the independence and the fact that it is enough to check that $P(\epsilon_i |g_i|>t)= P(g_i>t)$ for all $t$ . Q2. Show that $\| \sum_{i=1}^n g_ia_i\|_{L_p} \ge \frac{\sqrt{2}}{\sqrt{\pi}} \| \sum_{i=1}^n \epsilon_i a_i\|_{L_p}$ for any $p \ge 1$ and any real sequence $(a)$ . Deduce the Khinchine inequalities. For the first part, I tried to remark that by independence $\| \sum_{i=1}^n \epsilon_ia_i\|_{L_p} \| \sum_{i=1}^n f(g_i)\|_{L_p} =\| (\sum_{i=1}^n \epsilon_ia_i)( \sum_{i=1}^n f(g_i))\|_{L_p}  $ for $f(.) = |.|$ but I don't see how to proceed from here. Solution for the deduction: It is enough to establish that for all $p\ge 1$ we have control over the $p$ norms $\| \sum_{i=1}^n \epsilon_i a_i\|_{L_p} \le c p^{1/2} $ for some $c>0$ which directly follows from the $\psi_2$ character of $\sum_{i=1}^n g_i a_i$ and the initial inequality. A hint on how to carry on would be more than welcome. Source: https://webusers.imj-prg.fr/~dario.cordero/Docs/M2/2020_2021/chap3_new.pdf page 15","['statistics', 'concentration-of-measure', 'real-analysis', 'inequality', 'probability-theory']"
3865202,Proof of $(x^n)' = nx^{n-1}$ using the natural log and the chain rule,"Let's say $y=x^n$ is defined for all real number $x$ , including $0$ . I tried out the following proof of the statement $(x^n)' = nx^{n-1}$ . $$y = x^n$$ $$\ln y = n \ln x$$ $$\frac{1}{y} \times y' = n \times \frac{1}{x}$$ $$y' = n \times \frac{y}{x} = n \times \frac{x^n}{x} = nx^{n-1}$$ However on the third line there appears the terms $1/y$ and especially $1/x$ which make the proof invalid for any case where $x = 0$ or $y = 0$ (which always exists in the case of $y=x^n$ ). Is there any way to complement this to make proof complete?","['calculus', 'derivatives', 'logarithms']"
3865212,How to consider an integral of the form $\int\limits_{a<u_{1}<...<u_{n}<b}dX_{u_{1}}\otimes...\otimes dX_{u_{n}}\in (\mathbb R^{d})^{\otimes n}$,"Let $X: [a,b] \to \mathbb R^{d}$ be some smooth path. I am having trouble understanding what is actually being expressed in the integral of the form $$\int\limits_{a<u_{1}<...<u_{n}<b}dX_{u_{1}}\otimes...\otimes dX_{u_{n}}\in (\mathbb R^{d})^{\otimes n}.$$ Is it just riemann integration in every ""component"" of the tensor? Any intuition behind the actual expression is much appreciated. EDIT: For greater context, I am looking at the definition of a signature of a path from Differential Equations driven by Rough Paths by Terry Lyons on page 30. Here the signature is denoted as $\textbf{X}$ , where $$\textbf{X}=(1,\textbf{X}^{1},...,\textbf{X}^{n},....)$$ such that $$\textbf{X}^{n}=\int\limits_{a<u_{1}<...<u_{n}<b}dX_{u_{1}}\otimes...\otimes dX_{u_{n}}\in (\mathbb R^{d})^{\otimes n}$$","['integration', 'real-analysis', 'stochastic-processes', 'tensor-products', 'differential-geometry']"
3865319,Bounded Square-Integrable Martingale,"I am stumped by the following question. Suppose $(X_n, \mathbb{F}_n)_{n\ge 0}$ is a square-integrable martingale and $\sup_k |X_k| \le C$ for non-random $C$ . Let $D_k = X_k - X_{k-1}$ for $k\ge 1$ . Show $$
E\left[\left(\sum^k_{j=1} D_j^2 \right)^2 \right] \le 6 C^4
$$ for any $k$ . I've tried expanding, using the fact that $D_j$ are mean-zero, uncorrelated, but I can't find a bound that doesn't grow with $k$ .","['martingales', 'inequality', 'quadratic-variation', 'probability-theory']"
3865322,Solving a least square problems where the vectors are known and the unknown is the matrix,"I'm trying to solve a least-square problem of the form $Fc \approx d$ where $c$ and $d$ are known $3\times24$ matrices and $F$ is an unknown $3\times3$ matrix. I was asked to try and solve a different problem in order to find $F$ : reinterpreting $F$ as a $9\times1$ vector $x$ and solving $Ax = b$ , but I have no idea what $A$ and $b$ should represent, only that they'd be found from $c$ and $d$ . Does anyone have an idea as to how I could do this? Thanks in advance","['matrices', 'least-squares', 'linear-algebra']"
3865373,"Prove that for each positive integer $k$, there exist two powers of $7$ whose difference is divisible by $k$.","This is a question involving the use of the generalized pigeonhole principle.
Basically, here are the things that I know (but unsure whether they're relevant to the problem): Powers of $7$ are always odd. Hence, their differences are always even Their differences are always divisible by $7$ . Here's what I attempted to do: Let $a = 7^x$ and $b = 7^y$ , so $x = \log_7 a$ and $y = \log_7 b$ : $x\log_7 = \log a$ $y\log_7 = \log b$ $(a-b) \% k = 0$ I really have no idea how to set this up so I can use the generalized pigeonhole principle for this proof. I'm not even sure if I'm on the right track to solving this problem. Thanks for the help.","['pigeonhole-principle', 'divisibility', 'discrete-mathematics']"
3865421,Proof that addition of an ordinal with a non-zero limit ordinal is always a limit ordinal,"I wanna show that $\alpha + \beta$ is a limit ordinal where $\alpha$ is an ordinal  and $\beta$ is a non-zero limit ordinal. The less-smart way is to say that it is clearly non-zero since $\beta$ is non-zero and then show that $\alpha + \beta$ is not a successor ordinal, therefore has to be limit ordinal. How can this be proven? Can somebody prove this or perhaps directly prove that it is a limit ordinal? Secondly, is it true that $\alpha . \beta $ and $\beta . \alpha$ are also limit ordinals? My argument is that using another ordinal $\gamma$ , where $\alpha = \gamma + 1$ , we can write the multiplications as $\alpha. (\gamma + 1 ) = \alpha . \gamma + \alpha$ and $ (\gamma + 1 ) . \alpha = \gamma . \alpha + \alpha$ respectively. Using the previous part, the results are also limit ordinals.","['elementary-set-theory', 'logic']"
3865459,Multivariate optimal transport,"Optimal transport offers a way to find the optimal transport plan between 1 source distribution $P_r$ and 1 target distribution $P_\theta$ , where $\boldsymbol\Gamma$ is the optimal transport plan and $\boldsymbol D$ is the distance matrix. How or can the model instead be extended to the case where there are multiple source distributions , $P_r^1, P_r^2, \dots, P_r^k$ (or perhaps a weighted combination of them so that they collapse into a single distribution), from which we would like to move towards the target distribution $P_\theta$ ?","['statistics', 'probability-distributions', 'optimal-transport', 'linear-programming', 'optimization']"
3865480,How to find first derivative of function $y=x \ln(x)$ by limit definition using this formula $y'=\lim_{h\to 0}\frac{f(x+h)-f(x)}h$?,"How to find first derivative of function $y=x \ln(x)$ by limit definition, that is using this formula $$y'=\lim_{h\to 0}\frac{f(x+h)-f(x)}h$$ not product rule or L'Hopital rule are allowed. Thank you in advance :)","['limits-without-lhopital', 'calculus', 'limits', 'trigonometry', 'derivatives']"
3865501,"If $1+n+n^2+n^3$ is a perfect square, then $n=1$ or $n=7$","I want to prove that if $1+n+n^2+n^3$ is a perfect square then $n=1$ or $n=7$ . I managed to prove that $1+n+n^2+n^3=(n^2+1)(n+1)$ and that $(n^2+1,n+1)$ is either $1$ or $2$ .
I found out that it could not be $1$ , and then $\frac{1}{2}(n^2+1,n+1)=1$ . From here I concluded that $n^2+1=2a^2$ and $n+1=2b^2$ for some $a,b\in\mathbb{N}$ and it is here where I need some help. Please only provide hints.","['number-theory', 'elementary-number-theory']"
3865591,On the cruelty of really solving hyperelliptic integrals,"I want to derive closed-form parametric expressions describing the Schwarz H minimal surface starting from the Weierstrass–Enneper parametrisation, much like Gandy et al. did for the Schwarz D surface and its associates ( Schwarz P and gyroid). I have the W–E parametrisation from Sven Lidin (1988), Ring-like minimal surfaces, Journal of Physics 49 , 421–427 : $$x=\operatorname{Re}\int_w^1(1-t^2)R(t)\,dt$$ $$y=\operatorname{Re}\int_w^1i(1+t^2)R(t)\,dt$$ $$z=\operatorname{Re}\int_w^12tR(t)\,dt$$ $$R(t)=\left(\left(t^4+\frac{2(7s^2-4)}{(s+2)^2}t^2+\left(\frac{s-2}{s+2}\right)^2\right)\left(t^2+\frac{s+1}{s-1}\right)(t^2-1)\right)^{-1/2}$$ $s$ is a free parameter in $(0,1)$ determining the unit cell height and the domain of integration is $\operatorname{Re}w\ge0,\operatorname{Im}w\ge0,\left|w+\frac1{\sqrt3}i\right|\le\frac2{\sqrt3}$ . (The equations presented in the paper are not quite correct, but the presented derivation is sound, so I re-did the derivation. The paper also uses $E\in(0,1)$ as the free parameter where $s^2+E^2=1$ ; my formulation conveniently avoids square roots in the coefficients.) While I can certainly numerically integrate each coordinate, it's quite slow. The problem I'm encountering here is that the integrals do not appear easily reducible to simpler, faster forms. Of course we can substitute $u=t^2$ to get $$x=\frac12\operatorname{Re}\int_{w^2}^1\frac{1-u}{\sqrt uS(u)}\,du\tag{*}$$ $$y=\frac12\operatorname{Re}\int_{w^2}^1i\frac{1+u}{\sqrt uS(u)}\,du\tag{*}$$ $$z=\operatorname{Re}\int_{w^2}^1\frac1{S(u)}\,du$$ $$S(u)=\sqrt{\left(u^2+\frac{2(7s^2-4)}{(s+2)^2}u+\left(\frac{s-2}{s+2}\right)^2\right)\left(u+\frac{s+1}{s-1}\right)(u-1)}$$ and the new $z$ integral is obviously elliptic*, but the $x$ and $y$ integrals are hyperelliptic and I don't see how to proceed from here. When it came to the Schwarz D/G/P family, where $R(t)=(t^8-14t^4+1)^{-1/2}$ and $S(u)=\sqrt{u^4-14u^2+1}$ , Gandy et al. reduced the hyperelliptic integrals to elliptic ones by the transformation $q=u+\frac1u$ , but that requires the quartic under the root in $S(u)$ to be palindromic, which is not the case for the Schwarz H surface. Can the hyperelliptic $x$ and $y$ integrals after the $u=t^2$ substitution (the ones marked with $*$ ) be solved in any way not involving straight numerical integration, whether by elliptic integrals, hypergeometric functions or otherwise? What transformations must be made to accomplish this? If this is not possible, can I transform it into something like Carlson's symmetric form for faster numerical integration? * $z$ integral using elliptic integrals, Mathematica/mpmath convention: $$z=gF(\varphi,m)$$ $$m=\frac12\left(1+\sqrt{\frac3{4-s^2}}\right)$$ $$g=\frac1{\sqrt{AB}},A=\frac{4s}{s+2},B=\frac{2s}{1-s}\sqrt{\frac{3(2-s)}{s+2}}$$ $$\varphi=\cos^{-1}\frac{J-K}{J+K},J=2(1+s-w^2+sw^2),K=(1-w^2)\sqrt{3(4-s^2)}$$ The title was inspired by this other question of mine .","['minimal-surfaces', 'closed-form', 'parametrization', 'differential-geometry']"
3865681,Weierstrass Approximation generalization,"By the standard Weierstrass theorem, if $f:[0,1]\to R$ is continuous then $$\sum_{j=0}^n f(j/n)\binom{n}{j}x^j(1-x)^{n-j}$$ converges to $f(x)$ uniformly for $0\le x\le 1$ . I am wondering if there was an elementary to prove an easier multivariate generalization of this. That is $$\sum_{0\le j_1,...,j_k\le n} f\left(\frac{j_1}{n},...,\frac{j_k}{n}\right)\prod_{i=1}^k\binom{n}{j_i}x_i^{j_i}(1-x_i)^{n-j_i} \to f(x_1,x_2,...,x_k) $$ uniformly. Note that the true generalization of this would have $n_1,n_2,...n_k$ rather than just one $n$ . As such the proof of which gets very complicated. Is there any easy proof for the case where we have the same $n$ for our binomial coefficients?","['weierstrass-approximation', 'partial-differential-equations', 'probability-theory', 'real-analysis']"
3865697,Find the value of p such that X wins with probability of 1/2,"Consider a badminton game where there is no deuce and the first player to score
four points wins the game. Suppose player X wins each point independently with probability p
and Y wins with probability q=1-p Suppose that player Y won the first two points, so now Y needs 2 points to win and X still needs 4 points. Find approximately the value of p such that X wins with probability 1/2 . I have so far found the equation of X wins to be $p^4+4[p^4(1-p)]=1/2$ as X needs 4 points to win so $p^4$ with Y having 4 possible point sequences to gain an additional point. But I'm now stuck with the previous equation of $5p^4-4p^5=1/2$ I'm not sure what to do next. Is my equation wrong? If not, then can someone advise me on what to do next?","['statistics', 'probability']"
3865724,If $x \in R$ is non-invertible implies $x^2 \in \{\pm x\}$ and $|R| >9$ odd then $R$ is a field,"Let $(R, +, \cdot)$ be a commutative ring with $2n+1$ elements, for some $n\neq 4$ a positive integer. Suppose also that $R$ also satisfies the following condition: If an element $x\in R$ is non-invertible, then $x^2 \in \{\pm x\}$ . Prove that $(R,+,\cdot)$ is a field. Here it's my attempt: Suppose $x\in R$ is a non-invertible element, with $x\neq 0$ . Then also, $2x$ is non-invertible, so we have that $x^2 \in \{\pm x\}$ and $4x^2 \in \{\pm 2x\}$ . It follows that $3x=0$ , so $3$ is non-invertible which means that $9\in \{\pm 3\}$ $\Rightarrow$ $3=0$ . How can i continue? I am trying to prove that the ring should have $9$ elements in this case.","['ring-theory', 'finite-rings', 'abstract-algebra']"
3865835,"Is there an established notation for $A_1A_2\dots A_n$, a product of matrices?","I've taken a course or two in numerical analysis and often several matrices in a family- say $A_i\in \mathbb{C}^{m\times m},1\le i\le n$ - need to be multiplied. I haven't encountered an established notation for this, so I often defined and used the usual notation for products with success: $$
A_1 A_2\dots A_{n-1}A_n = \prod_{k=1}^{n}A_k
$$ If instead we are multiplying in the opposite order, one could just 'flip' the product: $$
A_n A_{n-1}\dots A_2 A_1 = \coprod_{k=1}^{n} A_k \left(=\prod_{k=1}^{n}A_{n+1-k}\right)
$$ Is there a more conventional notation for this procedure?","['matrices', 'notation', 'numerical-linear-algebra', 'linear-algebra']"
3865882,Boundary of the sequence $1/n$,"Consider $\mathbb{R}$ with its usual topology. For an arbitrary set of $A$ of $\mathbb{R}$ , let $A'$ be the complement of $A$ and $A^-$ be the closure of $A$ . Finally, let $\partial A$ be the boundary of the set $A$ . Let $E=\lbrace 1/n : n\in\mathbb{N} \rbrace$ . What is $\partial E$ ? At first, I thought that the boundary would be the empty set. But then, using the definition of the boundary $\partial E=E^-\cap E^{'-}$ I have that, since all the points in $E$ are isolated, $E^-=E$ . Further, some limit points in the complement $E^-$ can be the elements of $E$ , i.e. $E\subset E^{'-}$ . Therefore $\partial E = E$ . Does this make sense?","['general-topology', 'real-analysis']"
3865992,$n$ is odd if and only if there exists an $a \in \mathbb{Z}$ such that $n^4=16a+1$,"Write a formal proof. Suppose that $n \in ℕ$ . Prove $n$ is odd if and only if there exists $a \in \mathbb{Z}$ such that $n^4 = 16a + 1$ . There exists $k\in \mathbb{Z}$ such that $n=2k+1$ . So I've used this formula for odd numbers $$n^4=(2k+1)^4 = 16k^4+32k^3+24k^2+8k+1$$ since our goal is to match this to the above $n^4$ , $$n^4=8(2k^4+4k^3+3k^2+k)+1$$ unfortunately this form does not match $16a+1$ This is where I got stuck and could use help on the proof. A random solution I thought of is below, however I'm looking for a better answer. Let $a=2k$ ; I randomly though of a substitution. $$n^4=8(2*16a^4+16*8a^3+3*4a^2+2a)+1$$ $$n^4=16(16a^4+64a^3+6a^2+a)+1$$ this has the form $16a+1$ with ' $(16a^4+64a^3+6a^2+a)$ = an integer'. Note Thanks everyone for the help the proof totally makes sense now!
Special shoutout to @lulu and @fleablood The part that solidified it for me was the breakdown of the 2 cases 𝑘*(3𝑘+1)2, k being even or odd!","['proof-writing', 'solution-verification', 'discrete-mathematics']"
3866018,The (standard) definition of a group.,"Edited to incorporate suggestions from the comments and responses: Typically, the definition of a group is as follows: Definition: If $S$ is a set, $*$ is a binary operation on $S$ , and $e \in S$ , then $G = (S,e,*)$ is called a group if (i) $(ab)c = a(bc)$ , $\forall a,b,c \in S$ (associativity); (ii) $\exists e \in S$ such that $ae = a = ea$ , $\forall a \in S$ (identity); and (iii) $\forall a \in S$ , $\exists b \in S$ such that $ab = e = ba$ (inverse). Consider the following definition. Definition: If $S$ is a set, $*$ is a binary operation on $S$ , and $e \in S$ , then $G = (S,e,*)$ is called a group if (i) $(ab)c = a(bc)$ , $\forall a,b,c \in S$ (associativity); (ii) $\exists e \in S$ such that $ae = a$ , $\forall a \in S$ (right identity); and (iii) $\forall a \in S$ , $\exists b \in S$ such that $ab = e$ (right inverse). It an be shown that these axioms imply that every right inverse is a left inverse and that $e$ is a left identity . (Of course, there's nothing special about using right identity and right inverse and that we could also take left identity and left inverse as axiomatic.) Question 1: In most undergraduate textbooks in abstract algebra I've seen (I realize this is anecdotal), the first definition is used. Is there a reason that authors use the first definition and not a variant of the second one? This seems strange to me given that it is desirable to make definitions as lean as possible. Question 2: Alternately, are there textbooks that employ the second definition (or a variant thereof)?","['group-theory', 'abstract-algebra', 'education', 'reference-request']"
3866087,The order of elements in infinite quotient groups,"There are two statements that my professor made today that I'm hoping I can get some more clarification on. The first is that $\mathbb{Q}/\mathbb{Z}$ is an infinite quotient group where every element has finite order. The second is that $\mathbb{R}/ \mathbb{Q}$ is also an infinite quotient group but every element except the identity has infinite order. I'm having trouble even imagining an infinite quotient group...I'm familiar with groups like $\mathbb{Z} / n\mathbb{Z}$ , but how would you even notate these other groups? I understand that in order for an element $xH$ , where $H$ is the subgroup, to have finite order, $x^n$ must be in $H$ for some $n$ . So if every element of $\mathbb{Q}/\mathbb{Z}$ is finite, does that imply that every rational number is in $\mathbb{Z}$ ? Obviously that is not true, but I'm having trouble figuring out where I'm going wrong.","['quotient-group', 'group-theory', 'abstract-algebra', 'infinite-groups']"
3866104,"Does $\exists$ a differentiable function $f:\mathbb{R}\to \mathbb{R}, f(x) \neq x+c$ s.t. every interval $(a,b)$ contains a point $p$ with gradient 1?","Does there exist a differentiable function $f:\mathbb{R} \to \mathbb{R}, f(x) \neq x+c$ such that every interval $(a,b)$ contains a point $p$ with gradient $1$ ? I would guess no, but I've no idea how to prove it. Now that I think about it, isn't this question similar to: Does there exist a non-constant differentiable function $f:\mathbb{R} \to \mathbb{R}$ , such that every interval $(a,b)$ contains a point $p$ with gradient $0$ ? Which I may or may not have seen on the site somewhere - can't remember. I guess the disproof would be something like: $f(x)$ has gradient $0$ almost everywhere $\implies f(x)$ has unbounded variation somewhere (e.g. in some interval) $\implies$ f(x) is not everywhere differentiable. I'm not that familiar with bounded variation other than having skim-read this thread once, but bounded variation may in fact not be necessary to answer this question, I've no idea. Maybe the mean value theorem is more relevant here.","['calculus', 'derivatives', 'real-analysis']"
3866105,Finding a bijection of the set of $n-$bitstrings,"Let $w$ be a bitstring of length $n$ i.e. it has $n$ bits of $0$ or $1$ . Let $S$ be the set of all possible bitstrings of length $n$ . Let $H(w)$ be the Hamming weight of $w$ i.e. $H(w)$ is the number of $1'$ s in the string. The space I am looking at is such that strings with same Hamming weight are close to each other. So starting from a $w$ , and after applying $f$ twice, I would want to be in $w'$ s.t. $H(w')\neq H(w)$ . So starting from a fixed $w$ , I am looking for a formula for a bijection $f:S\to S$ s.t.: $$H(f(f(w))\neq H(w)$$ For example, if $f$ is a bijection such that it flips the bits i.e. $1$ to $0$ and vice-versa then the above condition is not true. So the bijection, qualitatively, has to look like: $f$ maps $\{w: H(w)=1\}$ to $\{w:H(w)=2\}$ and map $\{w:H(w)=2\}$ to $\{w:H(w)=3\}$ and so on or something else of that kind. But obviously $$|\{w: H(w)=i\}|\neq |\{w: H(w)=j\}|\text{ for i }\neq j$$ and hence it is hard to find such a bijection. Any suggestions? Thanks.","['abstract-algebra', 'combinatorics', 'discrete-mathematics']"
3866128,Bug moving on number line in random directions,"A bug at the origin moves on the number line either left or right $u$ units every second, where $u$ is am arbitrary real between -1 and 1. For example, the bug could go from 0 to -0.4 in one second, then -0.4 to 0.3 in the next second, etc.. How long (in seconds) do you expect the bug to take before it is more than or at least a distance 1 from the origin (i.e. its position is less than or equal -1 or greater than equal to 1)? EDIT #1 : I tried representing the expected value as a function of the position it is currently at, but couldn't get any further EDIT #2 : I ran a computer simulation for about 100 million trials and got an answer of around $5.36457$ , similar to @DreiCleaner . @Mike Earnest had a promising approach expanding on my [EDIT #1] . His appraoch: Let $f(x)$ be the expected value of remaining steps if you are currently at position $x$ . Then it can be shown that $f(x)=1+\frac12\int_{\max(x-1,-1)}^{\min(x+1,1)} f(t)\,dt$ . I do not see why this is true yet.","['ordinary-differential-equations', 'expected-value', 'combinatorics', 'probability-theory', 'probability']"
3866165,"Prove that matrices of this form have eigenvalues $0,1,\ldots , n-1$","Fix arbitrary real numbers $x_1,\ldots ,x_n$ which are pairwise distinct, i.e. so that $x_i \neq x_j$ for any pair $i \neq j$ . Let $A = (a_{ij})$ be the following $n \times n$ matrix: Its diagonal entries are given by the equation, $$a_{ii}=\sum_{j\neq i}\frac{x_i}{x_i-x_j},$$ while its off-diagonal entries given by the equation, $$a_{ij}=\frac{x_i}{x_i-x_j}$$ , for $i\neq j$ . For instance when n=2, the matrix looks like: $$A=\begin{pmatrix}
\frac{x_1}{x_1-x_2} & \frac{x_1}{x_1-x_2}\\ 
\frac{x_2}{x_2-x_1} & \frac{x_2}{x_2-x_1}
\end{pmatrix}$$ Prove that the set of eigenvalues for the matrix A is of the form $\left \{0,1,\ldots,n-1\right \}$ . I'm completely lost as to how to continue. I've tried to work on the determinant of $A-\lambda I$ for $2 \times 2$ and $3 \times 3$ matrices $A$ but I haven't managed to find anything helpful towards the proof. Update 1 I'm not entirely sure how to write this formula nicely as a mathematical expression, but as code in python I have that the $k$ th element of $v_0$ is p = product([L[i-1] - L[j-1] for i in [1..n] for j in [i+1..n] if i != k and j != k])
        v[k-1] = p if k % 2 == 1 else -p where L refers to the list [x_1,...,x_n] , and I want for $1 \leq i < j \leq n$ and for $i \neq j \neq k$ I also have that $v_i$ is equal to $diag(x_1,...,x_n)^{i}v_0$ Update 2 the eigenvectors for the case where $n=4$ are, $$ v_0 = \begin{pmatrix}
1\\ 
-\frac{x_1^2 - x_1x_3 - (x_1 - x_3)x_4}{x_2^2 - x_2x_3 - (x_2 - x_3)x_4}\\ 
\frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_4}{x_2x_3 - x_3^2 - (x_2 - x_3)x_4}\\ 
-\frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_3}{x_2x_3 - (x_2 + x_3)x_4 + x_4^2}
\end{pmatrix},$$ $$v_1 = \begin{pmatrix}
1\\ 
-\frac{x_1^2x_2 - x_1x_2x_3 - (x_1x_2 - x_2x_3)*x_4}{x_1x_2^2 - x_1x_2x_3 - (x_1x_2 - x_1x_3)x_4}\\ 
-\frac{(x_1 - x_2)x_3x_4 - (x_1^2 - x_1x_2)x_3}{x_1x_2x_3 - x_1x_3^2 - (x_1x_2 - x_1x_3)x_4}\\ 
-\frac{(x_1^2 - x_1x_2 - (x_1 - x_2)x_3)x_4}{x_1x_2x_3 + x_1x_4^2 - (x_1x_2 + x_1x_3)x_4}
\end{pmatrix},$$ $$v_2 = \begin{pmatrix}
1\\ 
-\frac{x_1^2x_2^2 - x_1x_2^2x_3 - (x_1x_2^2 - x_2^2x_3)x_4}{x_1^2x_2^2 - x_1^2x_2x_3 - (x_1^2x_2 - x_1^2x_3)x_4}\\ 
-\frac{(x_1 - x_2)x_3^2x_4 - (x_1^2 - x_1x_2)x_3^2}{x_1^2x_2x_3 - x_1^2x_3^2 - (x_1^2x_2 - x_1^2x_3)x_4}\\ 
-\frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_3)x_4^2}{x_1^2x_2x_3 + x_1^2x_4^2 - (x_1^2x_2 + x_1^2x_3)x_4}
\end{pmatrix},
$$ $$v_3 = \begin{pmatrix}
1\\ 
-\frac{x_1^2x_2^3 - x_1x_2^3x_3 - (x_1x_2^3 - x_2^3x_3)x_4}{x_1^3x_2^2 - x_1^3x_2x_3 - (x_1^3x_2 - x_1^3x_3)x_4}\\ 
-\frac{(x_1 - x_2)x_3^3x_4 - (x_1^2 - x_1x_2)x_3^3}{x_1^3x_2x_3 - x_1^3x_3^2 - (x_1^3x_2 - x_1^3x_3)x_4}\\ 
-\frac{x_1^2 - x_1x_2 - (x_1 - x_2)x_3)x_4^3}{x_1^3x_2x_3 + x_1^3x_4^2 - (x_1^3x_2 + x_1^3x_3)x_4}
\end{pmatrix}$$ Update 3 I managed to work out a formula for the $j$ th element of the eigenvector with $eigenvalue=\lambda$ and $size=n$ , $$(-1)^{j+1}\frac{x_j^\lambda}{x_1^\lambda} \prod_{k\neq 1,k\neq j}^n \frac{x_1-x_k}{x_j-x_k}
$$ I'm just not sure now as to how to use the formula for one entry of an eigenvector to prove the set of eigenvalues","['abstract-algebra', 'linear-algebra', 'eigenvalues-eigenvectors']"
3866182,LOTUS for non-continuous funtion,"Let $f_n:[a,b]\to\mathbb{R}$ be a sequence of continuous and uniformly bounded functions. Suppose, $f_n(x)\to f(x)$ pointwise for all $x\in [a,b]$ . Now, let us take $X\sim \text{Unif}[a,b]$ . Then, as the $f_n$ 's are continuous, so by the Law of the Unconscious Statistician (LOTUS) , we can say that $$E[f_n(X)]=\frac{1}{b-a}\int_a^b f_n(u)~du$$ Also, due to the uniformly bounded criterion, by DCT , it can be easily shown that $E[f_n(X)]\longrightarrow E[f(X)]$ . But the problem is that $f$ is not necessarily a continuous function. So I'm not allowed to use LOTUS on $f$ and write $E[f(X)]$ as a Riemann Integral . Can someone please help to resolve this issue without using any measure theory arguments ? You may assume that $f$ is Riemann integrable. Thanks in advance. P.S. : What I want to show is : $$\int_a^b f_n(t)~dt ~\longrightarrow~\int_a^b f(t)~dt$$ where the integrals are Riemann integrals.","['probability-theory', 'real-analysis', 'expected-value', 'pointwise-convergence', 'probability']"
3866211,A general notion of the support of a measure?,"Let $(\Omega, \mathcal A, \mu)$ be a finite measure space. If $\Omega$ is finite and $\mathcal A = 2^\Omega$ , then the support of $\mu$ is $s(\mu) = \{\omega: \mu\{\omega\} > 0\}$ . As far as I know, there isn't a very broad generalization of $s(\mu)$ to uncountable $\Omega$ . The most common approach seems to involve assuming that $\Omega$ comes equipped with a topology $\mathcal T$ that generates $\mathcal A$ . One then defines $$s(\mu) = \{\omega: \omega \in U \in \mathcal T \implies \mu(U) > 0\}.$$ That is, $\omega$ is in the support of $\mu$ if every open set containing $\omega$ has positive $\mu$ -measure. This definition coincides with the one for the finite case when $\mathcal T$ is the discrete topology. It seems to me that it would be nice to have a general notion of support that doesn't rely on $\Omega$ having a topology. Here is an idea I had. Say that a measure $\mu$ on $(\Omega, \mathcal A)$ is discrete if it is of the form $\mu = \sum_{i=1}^n \alpha_i \delta_{\omega_i}$ with $\alpha_i$ positive and $\sum_{i=1}^n\alpha_i = 1$ , and where $\delta_{\omega_i}$ is point mass at $\omega_i$ . The general definition of support that I will propose is motivated by the following interesting fact: Every finitely additive finite measure on $(\Omega, \mathcal A)$ is the pointwise limit of a net of discrete measures. That is, for all $\mu$ there is a net $(\mu_d)$ of discrete measures such that $\mu(A) = \lim_d \mu_d(A)$ for all $A \in \mathcal A$ . Let $\mathcal P$ be the set of all finitely additive measures on $(\Omega, \mathcal A)$ equipped with the topology of pointwise convergence. Now say that $\omega$ is in $s(\mu)$ if and only if every open subset of $\mathcal P$ containing $\mu$ contains a discrete measure $\nu = \sum_{i=1}^n \alpha_i \delta_{\omega_i}$ such that $\omega_i = \omega$ for some $i$ . It's clear that this definition of support generalizes the one given when $\Omega$ is finite. And while it does utilize some topology (on $\mathcal P$ ), it doesn't assume that $\Omega$ is a topological space, and can therefore be applied to any measure space whatsoever (even a finitely additive one). This is open-ended, but my question is basically: Is this a good definition of support? Has it been studied before? Does anyone have any interesting observations or comments about the definition? One potentially interesting observation is that on any space there will always be finitely additive measures $\mu$ with full support , i.e. $s(\mu) = \Omega$ . Proof. Suppose not. Then for every $\mu \in \mathcal P$ there is an open subset $N_\mu$ of $\mathcal P$ and $\omega_\mu \in \Omega$ such that no discrete measure $\nu = \sum_{i=1}^n \alpha_i \delta_{\omega_i}$ in $N_\mu$ is such that $\omega_i = \omega_\mu$ for some $i$ . The collection $\{N_\mu: \mu \in \mathcal P\}$ is an open cover of $\mathcal P$ , and thus for some $n$ and $\mu_1,...,\mu_n$ the collection $\{N_{\mu_i}: 1 \leq i \leq n\}$ covers $\mathcal P$ because $\mathcal P$ is compact ( $\mathcal P$ is a closed subset of $[0,1]^\mathcal A$ with the product topology.) But now for any positive $\alpha_i$ , $1 \leq i \leq n$ , summing to $1$ the discrete measure $\sum_{i=1}^n \alpha_i \delta_{\omega_{\mu_i}}$ is not in any of the $N_{\mu_i}$ , which is a contradiction. This raises the question: Under what conditions can one guarantee that there is a countably additive measure with full support?","['measure-theory', 'reference-request']"
3866274,Polynomial Division - Remainder when divisor is squared??,"Let $f(x)$ be a polynomial. If $(x-a)$ is a factor of $f(x)$ , prove that if $f(x)$ is divided by $(x-a)^2$ , the remainder will be equal to $n(x-a)$ for some real value $n$ . I have tried setting $f(x) $ to equal $g(x)(x-a)^2+k(x-a)$ , but all that really proves that an equation with this form will satisfy it. I have no idea where to go from here, and to say that any equation following this form will work seems really flawed. Any help is appreciated.","['algebra-precalculus', 'polynomials']"
3866309,Does a strictly convex and continuous function always exist?,"Let $X$ be a locally convex topological vector space, and let $C$ be a nonempty convex subset of $X$ . A real-valued function $f: C \to \mathbb R$ is strictly convex if for all $\lambda \in (0,1)$ and distinct $x,y \in C$ $$f(\lambda x + (1 - \lambda)y) < \lambda f(x) + (1-\lambda) f(y).$$ Does a strictly convex, continuous, real-valued function exist on every nonempty convex $C$ ?","['topological-vector-spaces', 'locally-convex-spaces', 'convex-analysis', 'functional-analysis']"
3866336,$N$ kids with $k$ balls. Reshuffle. Find distribution of number of balls brought back by same kids when $N \rightarrow \infty$,"$N$ kids each brought $k$ balls to a party. When they leave each kid brings back $k$ balls randomly. Let $X$ be the total number of balls brought back by their original owners. We fix $k$ . Find the distribution of $X$ when $N \rightarrow \infty$ [EDIT, removed my earlier incorrect attempt] As the comment pointed out when $k=1$ it's a rather simple Poisson distribution. But how do we prove this case for arbitrary $k$ ?","['probability-distributions', 'probability-theory', 'probability']"
3866346,Induced Bernoulli measures,"Given a sequence of Bernoulli trials with paramater $p$ . We have by Kolmogorov's extension theorem, a probability measure $P^\mathbb{N}$ on the space of binary sequences. Now letting $P$ denote the measure on $[0,1]$ that corresponds to $P^\mathbb{N}$ if we represent numbers in their binary representation. I would like to prove that $P((0,x])$ is continuous. Furthermore, since each $0<p<1$ induces a new measure $P_p$ . Is it always true that these measures are mutually singular? For the latter, I need to construct disjoint sets $A_p$ such that $P_p(A_p)=1$ and $P_p(A_q)=0$ for $p\ne q$ . But I am unsure how to do this. I believe LLN is useful but we have uncountable many measures. I have got $P((0,x])=\sum_n \frac{x_n}{2^n}$ where $x\in[0,1]$ is represented in binary by $(x_n)_n$","['stochastic-processes', 'measure-theory', 'lebesgue-measure', 'probability-theory']"
3866364,What's the minimum number of $2$s needed to write a positive integer?,"This is just for fun and inspired by Estimating pi, using only 2s . For a positive integer $n$ , let $f(n)$ denote the minimum number of $2$ s needed to express $n$ using addition, subtraction, multiplication, division, and exponentiation, together with the ability to concatenate $2$ s, so for example $2 \times 22^2 + \frac{222}{2}$ is a valid expression. Other variants involving different sets of allowed operations are possible, of course. This function is very far from monotonic, so to smooth it out let's also consider $$g(n) = \text{max}_{1 \le m \le n} f(m).$$ For example, $f(1) = 2$ ( $1 = \frac 22$ ) $f(11) = 3$ ( $11 = \frac{22}{2}$ ) Question: What can you say about $f(n)$ and $g(n)$ ? Can you give exact values for small values of $n$ ? Can you give (asymptotic or exact) upper bounds? Lower bounds? As a simple example we can write any positive integer $n$ in the form $2^k + n'$ where $n' < 2^k$ ( $2^k$ is just the leading digit in the binary expansion of $n$ ), which gives $f(n) \le f(k) + 1 + f(n')$ . If we write $\ell(n) = \lfloor \log_2 n \rfloor$ then iterating this gives something like $$g(n) \le \sum_{k=1}^{\ell(n)} \left( g(k) + 1 \right).$$ This gives an upper bound growing something like $\ell(n) \ell^2(n) \ell^3(n) \dots$ which I think is pessimistic. For example, in my answer to the linked question I show that $$f(14885392687) \le 36$$ and $\ell(14885392687) = 33$ so maybe we can expect something as good as $g(n) = O(\log n)$ for an upper bound. I have no idea about a lower bound, other than to write down an upper bound on the number of possible expressions that can be made with a given number of $2$ s. Edit: A related question involving $4$ s and more allowed operations: How many fours are needed to represent numbers up to $N$ ?","['optimization', 'recreational-mathematics', 'combinatorics']"
3866396,Does a subspace of R^2 being connected imply an epsilon neighborhood of that space is connected?,"I'm working on a topology homework problem and a hint would be appreciated. The idea is, we have a connected subspace $A$ of $\mathbb{R}^{2}$ and we define $A_{\varepsilon}$ to be the set of all points in $\mathbb{R}^{2}$ that are within $\varepsilon$ of some point in $A$ , and we want to prove $A_{\varepsilon}$ is path connected. $A_{\varepsilon}$ is open in $\mathbb{R}^{2}$ , so it suffices to show it is connected. The way I've been trying to go about this is by showing if $A_{\varepsilon}$ isn't connected, then $A$ isn't connected (which we've assumed it is). So suppose $A_{\varepsilon}$ isn't connected, take some separation $U, V$ , and use it to produce a separation $U$ intersect $A$ , $V$ intersect $A$ of $A$ . That works great if $U$ and $V$ both contain points in $A$ . But what if, say, $U$ contains all of $A$ , and $V$ contains none of $A$ ? Then our separation as defined above isn't a separation because one of the sets will be empty. Is it possible this can happen? Intuitively it feels to me like somehow if $V$ has no points in $A$ then the points in $V$ are somehow ""too far"" to have been in $A_{\varepsilon}$ . But I'm getting kind of lost in the weeds here. Can anyone tell me if my approach is salvageable?",['general-topology']
3866419,Foiling vectors with a matrix term,"Please excuse the simple question, but I can't seem to find anything relevant online. I have a term of the form: $$(\mathbf x-\mathbf y)^\top A(\mathbf x-\mathbf y)$$ and I can't figure out how to multiply these out to separate the terms as with FOILing with scalar variables as the matrix term is throwing me off. Is there any way to do that?","['matrices', 'linear-algebra', 'vectors']"
3866495,Bijection proof.,"Let $A$ and $B$ be finite sets, prove that $|A\times B|=|A||B|$ . Since $A$ and $B$ are finite sets, then it can be represented as follows, $$A=\{a_1,\ldots,a_m\}\quad \text{and}\quad B=\{b_1,\ldots, b_n\}$$ Then i defined a function $$\varphi:A\times B\to \{1,2,3,\ldots,mn\}=I_{mn}$$ given by $$\varphi(a_i,b_j)=(i-1)n+j$$ Who helps me to prove or how to prove that this function is indeed bijective.",['functions']
3866584,Show that the sum of a function series is bounded,"For every $k\in\mathbb{N}$ , with $k\geq 1$ , let $f_k=f_k(x,t)$ be the real-valued function defined over the set $(x,t) \in [-\pi,\pi]\times[0,+\infty)$ by $$f_k(x,t)=(-1)^{k+1}\frac{2}{k}e^{-k^{2} t}\sin(kx).$$ We then consider the corresponding function series $$\sum_{k=1}^{+\infty}f_k(x,t)=\sum_{k=1}^{+\infty}(-1)^{k+1}\frac{2}{k}e^{-k^{2} t}\sin(kx). \tag{1}$$ It is simple to show that $(1)$ converges pointwise on the whole $[-\pi,\pi]\times[0,+\infty)$ , and uniformly on every $[-\pi,\pi]\times[t_0,+\infty)$ , with $t_0>0$ . Let $u=u(x,t)$ be its sum, i.e $$u(x,t)=\sum_{k=1}^{+\infty}f_k(x,t)=\sum_{k=1}^{+\infty}(-1)^{k+1}\frac{2}{k}e^{-k^{2} t}\sin(kx).$$ The sum $u$ it is a superposition of sinusoids of increasing frequency $\frac{k}{2\pi}$ and of strongly damped amplitude because of the negative exponential, at least when $t > 0$ . For this reason, it is simple to show that $u$ is smooth on the set $[-\pi,\pi]\times(0,+\infty)$ , ie $u\in C^{\infty}([-\pi,\pi]\times(0,+\infty))$ . Notice also that: $u=u(x,t)$ is the solution of the one-dimensional Heat Equation problem with periodic boundary conditions $$\begin{cases} u_t-u_{xx} = 0 \qquad &x \in (-\pi,\pi),t>0 \\
u(x,0) = x \qquad &x \in (-\pi,\pi) \\ 
u(-\pi,t) = u(\pi,t) \qquad &t \geq 0 \end{cases}. \tag{2}$$ For every $x_0\in (-\pi,\pi)$ one has $$\lim_{(x,t)\to(x_0,0)}u(x,t)=x_0,$$ and then $u$ is also continuous at every point of the open segment $(-\pi,\pi)\times\{0\}$ . The limits $$\lim_{(x,t)\to(\pm \pi,0)}u(x,t)\qquad \nexists.$$ I'm not able to prove that $u$ is bounded on the whole $[-\pi,\pi]\times [0,+\infty)$ . By uniform convergence (as suggested me in comments), we just need to prove that partial sums of $(1)$ are uniformly bounded on $[-\pi,\pi]\times [0,+\infty)$ , but I really don't know how to obtain this uniform bound. Any hint would be really appreciated.","['heat-equation', 'real-analysis', 'partial-differential-equations', 'fourier-series', 'sequences-and-series']"
3866589,Proving that the Lie Bracket is zero if and only if the flows commute,"I am trying to show that if we have $X,Y \in \mathcal{X}(M)$ complete vector fields then for all $s,t$ we have that $\phi_X^t\circ \phi_Y^s=\phi_Y^s\circ \phi_X^t $ if and only if $[X,Y]=0$ . Now trying to show this I am using a result that says that $[X,Y](p)=\frac{d}{d\epsilon}\phi_Y^{-\sqrt \epsilon} \circ \phi_X^{-\sqrt \epsilon} \circ \phi_Y^{\sqrt \epsilon}\circ \phi_X^{\sqrt \epsilon}(p)|_{\epsilon=0}$ . Now assuming the flows commute it's easy to see that $[X,Y]=0$ .  Now if we assume that $[X,Y]=0$ I can show that for any $t$ we have $\phi_X^t\circ \phi_Y^t=\phi_Y^t\circ \phi_X^t$ . Now the idea for proving the case when $s\neq t$ would be to use property of the flows that say that $\phi_X^{t+s}=\phi_X^t\circ \phi_X^s$ . Now I have tried to do this but I never got what I wanted. Does anyone know if this approach is going to work ? Thanks in advance.","['lie-derivative', 'differential-geometry']"
3866678,"Prove that $S = \{ f: [0,1]\rightarrow \mathbb{R} \ \text{continuous} : x\in\mathbb{Q}\implies f(x) \in \mathbb{Q}\}$ is. uncountable","Prove that $$S = \{ f: [0,1]\rightarrow \mathbb{R} \ \text{continuous} : x\in\mathbb{Q}\implies f(x) \in \mathbb{Q}\}$$ is uncountable. I know that reals are uncountable,  so I look for injection from reals to set $S$ , i.e., if we can define a function as above for each real number, then we are done. Am I going in the right direction? Please help with some hint/solution.","['elementary-set-theory', 'continuity', 'real-analysis']"
3866723,Do we really get extra freedom if one conditions on probability zero events?,"Just to make things clear, I'm not here claiming I broke probability theory. It is just that I got myself into a bad situation questioning if life is even worth it. So here is a problem: Problem: Let $(B_t)_{t\geq 0}$ be a standard BM  and let us condition on $\{B_1=0\}$ . Let $A\in \mathcal F_1$ (where $\mathcal F_t$ is the canonical filtration of $(B_t)_{t\geq 0}$ ). For example we can have something like $A=\{B_t\leq 1 $ for all $t\in [0,1]\}$ . Find $\mathbb P(A\mid B_1=0)$ . I saw such problem in the book "" A first course in stochastic processes "" by Karlin and Taylor (exercise 6, p 386). My solutions to the problem. I can give the simple answer ""It is zero"" i.e. $$\mathbb P(A\mid B_1=0)=0$$ (actually say any number between 0 and 1).  On the other hand, of course, I can do some calculations and provide an answer that is better accepted. So now my question is: My question: on what basis can one actually tell me my first answer, where I claim it is zero, is wrong? My own thoughts: We want to find a ""nice"" function $g$ for which $g(B_1)=\mathbb P(A|B_1)$ a.s., and then the answer is $g(0)$ . But then we get the problem $\mathbb P(A|B_1)$ is not unique on null sets so we can find another $h$ for which $g(0)\neq h(0)$ and still $h(B_1)=\mathbb P(A\mid B_1)$ a.s.. That apparently is not strong enough to give us a unique answer for our original problem. Let's go for something stronger and say that we want a regular conditional probability $g(x,A)$ for which $g(B_1,A)=\mathbb P(A|B_1)$ a.s.. But in this case too, nothing stops me from making a new function $h(x,A)$ making it equal to $g(x,A)$ except at $x=0$ , I make it whatever I want. And yes that new $h$ is also a regular conditional probability. Is limits the only way to make this give us a unique answer? I mean that we condition on something like $U_{\varepsilon}:=\{B_1\in (-\varepsilon,\varepsilon)\}$ . And then we consider the limit as $\varepsilon\to 0^+$ of $\mathbb P(A\mid U_\varepsilon)$ . And that we take as a definition.  I hate to say this, but if this is the case, does this always work for any type of process? Something like Doob's $h$ -transform maybe? I still have the feeling that this won't make it unique either. I actually feel super flawed. I've seen this many times and never made a big deal out of it, but after I was solving a related problem I got this question where I was wondering who told me that any other answer is actually wrong? I could not prove it. Also I know that probabilist's work was not for nothing, so I'm sure there is a way to make $\mathbb P(A\mid B_1=0)$ so precise that we get only one correct right answer for the mentioned problem.","['measure-theory', 'conditional-probability', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3866782,Vector space for which finite-dimensionality is open,"Is there a vector space for which it is unknown whether it is finite-dimensional or infinite-dimensional? This vector space should be somehow "" naturally occuring "" in the sense that it is interesting as a vector space itself. Otherwise one could take e.g. the set of all twin primes (or any set of which finiteness is unknown) and consider it as a free vector space over some field to get a trivial example.","['linear-algebra', 'functional-analysis', 'vector-spaces']"
3866825,Why doesn't Scipy's wasserstein_distance use linear programming?,"Question Kantorovich's formulation of the optimal transport problem using the Wasserstein distance is $$\begin{align}\begin{aligned}W_p(a,b)=(\min_\gamma \sum_{i,j}\gamma_{i,j}\|x_i-y_j\|_p)^\frac{1}{p}\\s.t. \gamma 1 = a; \gamma^T 1= b; \gamma\geq 0\end{aligned}\end{align}$$ It is often taught that this is solved using linear programming: $$\begin{align}\begin{aligned}OT(a,b)=\min_\gamma \quad \sum_{i,j}\gamma_{i,j}M_{i,j}\\s.t. \gamma 1 = a; \gamma^T 1= b; \gamma\geq 0\end{aligned}\end{align}$$ How then is the function scipy.stats.wasserstein_distance able to solve Wasserstein/OT without linear programming? What approach/method is the function using? My guess I am aware that wasserstein_distance is for 1D distributions only, and there so happens to be a closed-form analytical solution in the 1D case which is $$\mathcal{W}_{p}(\mu, \nu) =\left(\int_{0}^{1}\left|F_{\mu}^{-1}(z)-F_{\nu}^{-1}(z)\right|^{p} \mathrm{d} z\right)^{\frac{1}{p}}$$ The formula above contains inverse probability functions $F^{-1}$ . The code, shown next, however, does not contain inverse probability functions from what I can see. Code I know this sounds like a stackoverflow question, but it's not (and I need to write the math formulas): I'm trying to figure out what formula/approach is being used by the code since the code does not resemble anything above: It doesn't use linear programming, otherwise we would see scipy.optimize.linprog somewhere, and it doesn't use inverse probability, otherwise we would see .ppf somewhere: def wasserstein_distance(u_values, v_values, u_weights=None, v_weights=None):
    r""""""
    Compute the first Wasserstein distance between two 1D distributions.
    This distance is also known as the earth mover's distance, since it can be
    seen as the minimum amount of ""work"" required to transform :math:`u` into
    :math:`v`, where ""work"" is measured as the amount of distribution weight
    that must be moved, multiplied by the distance it has to be moved.
    .. versionadded:: 1.0.0
    Parameters
    ----------
    u_values, v_values : array_like
        Values observed in the (empirical) distribution.
    u_weights, v_weights : array_like, optional
        Weight for each value. If unspecified, each value is assigned the same
        weight.
        `u_weights` (resp. `v_weights`) must have the same length as
        `u_values` (resp. `v_values`). If the weight sum differs from 1, it
        must still be positive and finite so that the weights can be normalized
        to sum to 1.
    Returns
    -------
    distance : float
        The computed distance between the distributions.
    Notes
    -----
    The first Wasserstein distance between the distributions :math:`u` and
    :math:`v` is:
    .. math::
        l_1 (u, v) = \inf_{\pi \in \Gamma (u, v)} \int_{\mathbb{R} \times
        \mathbb{R}} |x-y| \mathrm{d} \pi (x, y)
    where :math:`\Gamma (u, v)` is the set of (probability) distributions on
    :math:`\mathbb{R} \times \mathbb{R}` whose marginals are :math:`u` and
    :math:`v` on the first and second factors respectively.
    If :math:`U` and :math:`V` are the respective CDFs of :math:`u` and
    :math:`v`, this distance also equals to:
    .. math::
        l_1(u, v) = \int_{-\infty}^{+\infty} |U-V|
    See [2]_ for a proof of the equivalence of both definitions.
    The input distributions can be empirical, therefore coming from samples
    whose values are effectively inputs of the function, or they can be seen as
    generalized functions, in which case they are weighted sums of Dirac delta
    functions located at the specified values.
    References
    ----------
    .. [1] ""Wasserstein metric"", https://en.wikipedia.org/wiki/Wasserstein_metric
    .. [2] Ramdas, Garcia, Cuturi ""On Wasserstein Two Sample Testing and Related
           Families of Nonparametric Tests"" (2015). :arXiv:`1509.02237`.
    Examples
    --------
    >>> from scipy.stats import wasserstein_distance
    >>> wasserstein_distance([0, 1, 3], [5, 6, 8])
    5.0
    >>> wasserstein_distance([0, 1], [0, 1], [3, 1], [2, 2])
    0.25
    >>> wasserstein_distance([3.4, 3.9, 7.5, 7.8], [4.5, 1.4],
    ...                      [1.4, 0.9, 3.1, 7.2], [3.2, 3.5])
    4.0781331438047861
    """"""
    return _cdf_distance(1, u_values, v_values, u_weights, v_weights)

def _cdf_distance(p, u_values, v_values, u_weights=None, v_weights=None):
    r""""""
    Compute, between two one-dimensional distributions :math:`u` and
    :math:`v`, whose respective CDFs are :math:`U` and :math:`V`, the
    statistical distance that is defined as:
    .. math::
        l_p(u, v) = \left( \int_{-\infty}^{+\infty} |U-V|^p \right)^{1/p}
    p is a positive parameter; p = 1 gives the Wasserstein distance, p = 2
    gives the energy distance.
    Parameters
    ----------
    u_values, v_values : array_like
        Values observed in the (empirical) distribution.
    u_weights, v_weights : array_like, optional
        Weight for each value. If unspecified, each value is assigned the same
        weight.
        `u_weights` (resp. `v_weights`) must have the same length as
        `u_values` (resp. `v_values`). If the weight sum differs from 1, it
        must still be positive and finite so that the weights can be normalized
        to sum to 1.
    Returns
    -------
    distance : float
        The computed distance between the distributions.
    Notes
    -----
    The input distributions can be empirical, therefore coming from samples
    whose values are effectively inputs of the function, or they can be seen as
    generalized functions, in which case they are weighted sums of Dirac delta
    functions located at the specified values.
    References
    ----------
    .. [1] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,
           Munos ""The Cramer Distance as a Solution to Biased Wasserstein
           Gradients"" (2017). :arXiv:`1705.10743`.
    """"""
    u_values, u_weights = _validate_distribution(u_values, u_weights)
    v_values, v_weights = _validate_distribution(v_values, v_weights)

    u_sorter = np.argsort(u_values)
    v_sorter = np.argsort(v_values)

    all_values = np.concatenate((u_values, v_values))
    all_values.sort(kind='mergesort')

    # Compute the differences between pairs of successive values of u and v.
    deltas = np.diff(all_values)

    # Get the respective positions of the values of u and v among the values of
    # both distributions.
    u_cdf_indices = u_values[u_sorter].searchsorted(all_values[:-1], 'right')
    v_cdf_indices = v_values[v_sorter].searchsorted(all_values[:-1], 'right')

    # Calculate the CDFs of u and v using their weights, if specified.
    if u_weights is None:
        u_cdf = u_cdf_indices / u_values.size
    else:
        u_sorted_cumweights = np.concatenate(([0],
                                              np.cumsum(u_weights[u_sorter])))
        u_cdf = u_sorted_cumweights[u_cdf_indices] / u_sorted_cumweights[-1]

    if v_weights is None:
        v_cdf = v_cdf_indices / v_values.size
    else:
        v_sorted_cumweights = np.concatenate(([0],
                                              np.cumsum(v_weights[v_sorter])))
        v_cdf = v_sorted_cumweights[v_cdf_indices] / v_sorted_cumweights[-1]

    # Compute the value of the integral based on the CDFs.
    # If p = 1 or p = 2, we avoid using np.power, which introduces an overhead
    # of about 15%.
    if p == 1:
        return np.sum(np.multiply(np.abs(u_cdf - v_cdf), deltas))
    if p == 2:
        return np.sqrt(np.sum(np.multiply(np.square(u_cdf - v_cdf), deltas)))
    return np.power(np.sum(np.multiply(np.power(np.abs(u_cdf - v_cdf), p),
                                       deltas)), 1/p)","['programming', 'statistics', 'python', 'optimal-transport', 'linear-programming']"
3866878,Split a regular tetrahedron into four equal сonvex polyhedrons.,"A point is marked on the face of a regular tetrahedron. Prove that the tetrahedron can be split into four equal сonvex polyhedrons in a way that this point is a vertex of one of them. This problem is from my group theory course. I think there should be used a group action and Burnside's lemma. But I can't come up with a right set to be acted on. The only group connected with a regular tetrahedron I know is a group of all symmetries, so I think I should use it. Please, give me a hint (not a solution). Thanks.
(Also I know how to do it if the point is a middle point of some face. Can I use it?)","['group-theory', 'group-actions', 'geometry']"
3866952,Infection in a village,"Consider the following problem: Suppose a lonely wanderer infected with a virus came into an isolated village with $M$ villagers and stayed there. Every week each of the infected villagers coughs onto $n$ random other villagers (each of them chosen uniformly and independently among everyone) and then develops antibodies becoming immune to it. All villagers who are coughed upon become infected if they are not immune. Nobody left or entered the village after the arrival of the lonely wanderer. Consider time to be discrete and measured in weeks. We say, that the virus survives as long as someone is infected with it. For what $n$ is the expected time of its survival the longest? The extremum clearly is not achieved in the border cases here. Indeed, if $n = 0$ the lonely wanderer becomes immune before being able to infect anyone else, thus the virus will survive only for $1$ week. If $n \to \infty$ the probability that the lonely wanderer infects everyone in the first week tends to $1$ . Thus the expected time of the survival of the virus tends to $2$ in this case. So, we must look for optimal $n$ somewhere in between. However, I have no idea how to find it (or even its asymptotic for large $M$ )… At the first glance the problem looked to be somewhat similar to two well studied problems: branching processes (villagers infected by a given infected villager - their descendants in terms of branching processes) and coupon collector problem (uninfected villagers as coupons to be collected). However, it is different from both of them (the number of ‘descendants’ changes each turn here, which makes it different from a Galton-Watson branching process, and the number of ‘coupons collected per turn’ depends on the number of ‘coupons collected on the previous turn’, which makes it different from a classical coupon collector) and methods, similar to the ones used to solve them, are unlikely to work here.","['optimization', 'coupon-collector', 'probability', 'stochastic-processes']"
3866995,Silverman Rational Points on Elliptic Curves Exercise 4.3c: Counting points on the Fermat curve over mod p,"Is there any somewhat elementary way to prove the Hasse-Weil bound for the Fermat curve? i.e. Let $p$ be a prime and $m$ be some factor of $p-1$ . Let $C$ be the projective curve $x^m+y^m+z^m=0$ . Then $$\left|\#C\left(\mathbb F_p\right)-p-1\right|\leq(m-1)(m-2)\sqrt p$$ Silverman also hinted that this is easier when $m$ is a prime. I've tried a few approached but none seems to reach the answer. My closest attempt was: Let $\chi$ be a character of $\mathbb F_p$ that has values in the $m$ th roots of unity, so $\chi\left(x\right)=1$ iff $x\equiv y^m\pmod p$ for some $y$ . Then with $0^0=1$ \begin{align*}
\#C\left(\mathbb F_p\right)&=\underbrace{m}_{(x:y:0)}+\sum_{x=0}^{p-1}\underbrace{\sum_{i=0}^{m-1}\chi\left(-x^m-1\right)^i}_{\text{Number of points on }x^m+y^m+1=0}\\
&=m+p+\sum_{i=1}^{m-1}\underbrace{\sum_{x=0}^{p-1}\chi\left(-x^m-1\right)^i}_{\gamma_i}\\
&=1+p+\sum_{i=1}^{m-1}\Re\left(\gamma_i+1\right)
\end{align*} however I can't seem to find anyway to prove that $\left|\Re\left(\gamma_i+1\right)\right|\leq(m-2)\sqrt p$ , which I've tested to be true numerically for the first 50 primes. Alternatively I've considered the sum ( $\zeta_p=e^{\frac{2\pi i}p}$ ) \begin{align*}
\frac1{p-1}\left(\sum_{x=0}^{p-1}\sum_{y=0}^{p-1}\sum_{z=0}^{p-1}\frac1p\sum_{k=0}^{p-1}\zeta_p^{k\left(x^m+y^m+z^m\right)}-1\right)&=\frac1{p^2-p}\left(\sum_{x=1}^{p-1}\sum_{y=1}^{p-1}\sum_{z=1}^{p-1}\sum_{k=0}^{p-1}\zeta_p^{k\left(x^m+y^m+z^m\right)}\right)+3m\\
&=\frac1{p^2-p}\left(\sum_{k=1}^{p-1}\sum_{x=1}^{p-1}\zeta_p^{kx^m}\sum_{y=1}^{p-1}\zeta_p^{ky^m}\sum_{z=1}^{p-1}\zeta_p^{kz^m}+(p-1)^3\right)+3m
\end{align*} For this I'm looking to have some bound on the power exponential sum via Fourier transforms(similar to quadratic gauss sums): Let $f(x)=\zeta_p^{kx^m}$ , since $\frac{f(x)+f(x+1)}2=\sum c_n$ where $c_n$ are the Fourier coefficients for the function $f(u+x)$ defined on $u\in[0,1]$ , we can express the sum $\sum_{x=0}^{p-1}f(x)$ as \begin{align*}
\sum_{x=0}^{p-1}f(x)&=\sum_{n=-\infty}^\infty\sum_{x=0}^{p-1}\int_0^1f(u+x)e^{-2\pi inu}du\\
&=\sum_{n=-\infty}^\infty\int_0^pf(x)e^{-2\pi inx}dx\\
&=\sum_{n=-\infty}^\infty\int_0^pe^{\frac{2\pi ikx^m}p}e^{-2\pi inx}dx\\
&=\sum_{n=-\infty}^\infty\int_0^p\zeta_p^{kx^m-pnx}dx\\
&=\sum_{n=-\infty}^\infty\int_0^p\zeta_p^{k\left(x^m-\frac{pn}kx\right)}dx\\
&=\sum_{n=-\infty}^\infty\int_0^p\zeta_p^{k\left(\left(x-\frac1m\sqrt[m-1]{\frac{pn}k}\right)^m+\dots+\frac1{m^m}\sqrt[m-1]{\frac{pn}k}^m\right)}dx\\
\end{align*} However it doesnt look like the sum can be split up into smaller parts and the whole integral looks rather hard to bound. As Silverman suggested trying the case that $m$ is prime first, it seems quite likely that there is a somewhat elementary solution, but I'm kinda lost at what to do currently, any suggestions for how should I continue this problem or a different method?","['analytic-number-theory', 'number-theory', 'finite-fields', 'algebraic-geometry']"
3867095,Solve a system of nonlinear differential equation.,"I am doing some exercises from applied math, but it boils down to solving the following system of nonlinear differential equation: $$\left\{
	\begin{array}{ll}
	8A'(\tau)=-3A^{3}(\tau)-3A(\tau)B^{2}(\tau)=-3A(\tau)(A^{2}(\tau)+B^{2}(\tau))\\
	8B'(\tau)=-3A^{2}(\tau)B(\tau)-3B^{3}(\tau)=-3B(\tau)(A^{2}(\tau)+B^{2}(\tau)),
	\end{array}
\right.$$ with initial conditions $A(0)=0$ and $B(0)=1$ . I am not sure if there is a general formula for such system, so I just tried to play with them to see if there any beautiful relation. Firstly I found that $$A'(\tau)B(\tau)=B'(\tau)A(\tau)\implies A'(\tau)B(\tau)-B'(\tau)A(\tau)=0,$$ but this does not help me since it does not necessarily form a product rule. Then, I actually expect $A(\tau)$ and $B(\tau)$ are of some forms of $\cos(a\tau)$ and $\sin(a\tau)$ , since if so then $A^{2}(\tau)+B^{2}(\tau)=1$ , and we have $$A'(\tau)=-\frac{3}{8}A(\tau)\ \ \text{and}\ \ B'(\tau)=-\frac{3}{8}B(\tau), $$ but then I got suck again since if, say, $A(\tau)=\cos(a\tau)$ , then $A'(\tau)=-\frac{3}{8}A(\tau)$ gives us $$-a\sin(a\tau)=-\frac{3}{8}\cos(a\tau)\implies \frac{3}{8}\cos(a\tau)-a\sin(a\tau)=0.$$ We can indeed play with trig identity $\sin(x-y)=\sin(x)\cos(y)-\cos(x)\sin(y)$ here, so we have $$\sin(x-a\tau)=0,$$ where $x$ is such that $\sin(x)=\frac{3}{8}$ and $\cos(x)=a$ . Then what I should do? I am kind of confusing myself. Thanks! Edit 1: Wolframalpha gives me the general solution $$A(\tau)=-\dfrac{2}{\sqrt{3c_{1}^{2}\tau-8c_{2}+3\tau}}\ \ \text{and}\ \ B(\tau)=-\dfrac{2c_{1}}{\sqrt{3c_{1}^{2}\tau-8c_{2}+3\tau}},$$ but this contradicts the initial condition. In this form $A(0)$ cannot be $0$ . This seems suggest that $A(\tau)=0$ for all $\tau$ , but I am not sure how to show it.","['partial-differential-equations', 'ordinary-differential-equations', 'real-analysis']"
3867098,Find the maximum area of an isosceles triangle inscribed in the ellipse $\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1.$,"Here's the question-
Find the maximum area of an isosceles triangle inscribed in the ellipse $x^2/a^2 + y^2/b^2 = 1$ .
My teacher solved it by considering two arbitrary points on the ellipse to be vertices of the triangle, being $(a\cos\theta, b\sin \theta)$ and $(a\cos\theta, -b\sin \theta)$ . (Let's just say $\theta$ is theta) and then proceeded with the derivative tests(which i understood)
But, he didn't indicate what our $\theta$ was,and declared that these points always lie on an ellipse.
Why so? And even if they do, what's the guarantee that points of such a form will be our required vertices?
One more thing, I'd appreciate it if you could suggest another way of solving this problem. Thank you!","['calculus', 'derivatives']"
3867107,"Fill in a partly filled in table such that it makes the magma $(M,*)$ associative, commutative, has an identity element and has no zero-elements.","Below is a partly filled in table for a binary operation ( $*$ ) on the set $M=\{a,b,c,d\}$ . I am trying to fill in the rest such that the magma $(M,*)$ becomes associative, commutative, has an identity element and has no zero-elements. Using the fact that $(M,*)$ is supposed to be commutative we can fill in a few cells and get: Now we want the magma to be associative too, so I can get the following information: $(c*a)*b = c*(a*b)$ $a*b = c*b$ $b*c = b = c*b$ and we end up with: $(a*c)*d = a*(c*d)$ $a*d = a*c$ $a*d = a = d*a$ and we end up with: $(b*c)*d = b*(c*d)$ $b*d = b*c$ $b*d = b = d*b$ and we end up with: Now we see that $d$ must be the identity element due to the d-row being equal to the heading row and the d-column is equal to the heading column . we end up with: Now this is where I get stuck, I can't figure out what the two last cells need to be. I have gained some information though, the magma $(M,*)$ does not form a group. This due to the fact that we for example have duplicate entries on the a-row [_ b a a] which would not be allowed if the magma was a group. We also notice that not every element seems to have an inverse, which again is not allowed if the magma was to be a group.","['magma', 'cayley-table', 'semigroups', 'discrete-mathematics', 'binary-operations']"
3867125,Parametric trigonometric integral $\int_{0}^{\pi}{\frac{\cos(nx)-\cos(na)}{\cos x-\cos a}}dx$,We want to calculate the following parametric integral $$\int_{0}^{\pi}{\frac{\cos(nx)-\cos(na)}{\cos(x)-\cos(a)}}dx$$ I tried using the substitution $$\cos(nx)=\frac{1}{2}(e^{inx}+e^{-inx})$$ but didn't get much further with the integrations. I'm thinking it's possibly a recurrence but I don't seem to get a way to reduce the $n$ value in the integral,"['integration', 'calculus', 'definite-integrals', 'trigonometric-integrals']"
3867135,"Let $n ≥ 3$. Find the number of NE lattice paths from $(0, 0)$ to $(n, n)$ that touch the diagonal $y = x$ at least twice","Let $n ≥ 3$ . Find the number of NE lattice paths from $(0, 0)$ to $(n, n)$ that touch the
diagonal $y = x$ at least twice (other than at the starting and ending points). Your final answer should not include $\sum$ but may include binomial coefficients. My idea is to take all of the lattice paths from $(0,0)$ to $(n,n)$ and then subtract off any paths that do not touch the line $y=x$ at all and all of the paths that hit $y=x$ exactly once (other than at the end points). The number of lattice paths from $(0,0)$ to $(n,n)$ is $$\binom{2n}{n}$$ We can see that the number of paths that never hit the line $y=x$ are simply the number of Dyck paths with $2n-2$ steps (i.e get rid of the first and last step, and you're left with a paths that are contained in the upper half-place $y \geq x+1$ ). Because we can either start below or above the line $y=x$ and never hit it, we have exactly $$2C_{n-1}$$ paths that never hit the line $y=x$ . To determine the number of paths that hit the line $y=x$ exactly once, we split our path into two. If we reflect the portion of the graph below $y=x$ , we are left with a ballot sequence of length $2n$ . We know that ballot sequences split uniquely into two ballot sequences by removing the first number and the first point at which the partial sum is exactly $0$ . This leave us with two new Dyck paths, one with $2k$ steps and one with $2l$ steps, where $k+l=n-1$ . The number of paths of $2k$ steps is exactly the number of Dyck paths of length $2k$ contained in the upper-half plane $y\geq x+1$ , and he number of paths of $2l$ steps is exactly the number of Dyck paths of length $2l$ contained in the upper-half plane $y\geq x+1$ . This is exactly $$C_{k-1}C_{l-1}$$ If we index over all $k+l=n-1$ , we recover $$\sum_{k+l=n-1}C_{k-1}C_{l-1}$$ This is exactly the recurrence for the Catalan numbers, so by induction we see that $$C_{n-1}=\sum_{k+l=n-1}C_{k-1}C_{l-1}$$ Because we can either start below or above the line $y=x$ and never hit it, we have exactly $$2C_{n-1}$$ paths that hit the line $y=x$ exactly once. Thus, the number of lattice paths from $(0, 0)$ to $(n, n)$ that touch the diagonal $y = x$ at least twice (other than at the starting and ending points) is $$\binom{2n}{n}-4C_{n-1}$$ For the simple case $n=3$ , the correct answer should be $8$ , but my formula doesn't yield that. Any help would be amazing!","['catalan-numbers', 'solution-verification', 'combinatorics']"
3867157,Is every subset of a product a product of subsets?,"Is every subset of a product a product of subsets ? i.e Let $E$ and $F$ two non empty sets and we define the Cartesian product $E \times F$ . Now given a non empty subset $A$ of $E\times F$ , can we write $A$ as the product of two subsets of $E$ and $F$ : i.e is there $E_1 \subset E$ and $F_1 \subset F$ such that $$A=E_1 \times F_1$$ My idea is that this statement is false, and some counter-example I thought of is $$\{(x,y) \in \mathbb{R}^2, \,\, x^2+y^2=1\}$$ $$\{(x,1/x), \,\, x\in \mathbb{R}^*\}$$ But I could not find a way to prove we can't write these two sets as product of two subsets of $\mathbb{R}.$",['elementary-set-theory']
3867200,Convergence in probability over compact set,"Let $(\Omega,\mathcal{F},P)$ be a probability space. Suppose $(X_n)\subseteq (\mathbb{R}^k)^{\Omega}$ is a sequence of random vectors converging in probability to the vector $c\in \mathbb{R}^k$ . Let $(\Psi_n)$ be a sequence of functions $\Psi_n:\mathbb{R}^k\times \Omega\to \mathbb{R}$ , such that for all $n$ and all $x\in \mathbb{R}^k$ , $\Psi_n(x,\cdot)$ is measurable. Consider also a continuous function $\Psi:K\to \mathbb{R}$ where $K$ is a compact subset of $\mathbb{R}^k$ containing $c$ . Do we have that: $$
\forall \varepsilon>0,\ \lim\limits_nP(\sup\limits_{x\in K}|\Psi_n(x,\cdot)-\Psi(x)|>\varepsilon)=0\Rightarrow \forall \varepsilon>0,\ \lim\limits_nP(|\Psi_n(X_n,\cdot)-\Psi(c)|>\varepsilon)=0\ ?
$$","['measure-theory', 'convergence-divergence', 'probability-theory']"
3867201,Polynomial with root $α = \sqrt{2}+\sqrt{5}$ and using it to simplify $α^6$,"Find a polynomial $\space P(X) \in \mathbb{Q}[X]\space$ of degree 4 such that $$\alpha = \sqrt{2} + \sqrt{5}$$ Is a root of $P$ . Using this polynomial, find numbers $\space a, b, c, d \space$ such that $$\alpha^{6} = a + b\alpha + c\alpha^{2} + d\alpha^{3}$$ What have I tried so far? I know obviously that for $\alpha$ to be a root of $P$ , then $(x-\alpha)$ must be part of the polynomial. Hence, $(x-\sqrt{2} - \sqrt{5})$ will be a factor of the polynomial. Where I’m getting stuck is what to do next in order to find the other factors of the polynomial such that I get values $a, b, c$ and $d$ that satisfy the equation with $\alpha^{6}$ . Any help would be greatly appreciated!","['field-theory', 'abstract-algebra', 'roots', 'polynomials']"
3867210,Drawing triangle with correct side length and angles,So I have the following question: You have a triangle with one angle that is $135°$ . The opposite side is $2 *\sqrt{{10}}$ and the adjacent side is $2 *\sqrt{{2}}$ . I used Pythagoras equation and got $4 *\sqrt{{3}}$ My problem is I dont know how I would draw my triangle with the exact angles and sides measurement that is given.,"['euclidean-geometry', 'angle', 'triangles', 'plane-geometry', 'trigonometry']"
3867244,Find solutions for $f'(\sin x) f(\cos x)=\sin x$,"We are given $$f'(\sin x) f(\cos x)=\sin x$$ and we need to find out the function which respects this condition. The answer is $$f(x)=\sqrt{c}e^{\frac{1}{2c}(x^2-\frac{1}{2})}$$ I tried to expand the initial condition just as on the other problems, but I can't seem to be getting anywhere, and having trigonometrical functions doesn't make it better. The closest I've gotten to the answer is through an integration after which nothing else works.","['integration', 'complex-analysis', 'trigonometry', 'derivatives']"
3867258,What novel results are there in Euclidean geometry in the last 50-100 years?,"Most summaries of the history of geometry conclude with an overview of developments in non-Euclidean geometry over the 18th-20th century. Some mention reformulations of Euclidean geometry based on different postulates with similar implications, more sophisticated treatments might mention algebraic approaches. But I'm wondering what new theorems (ideally proven via Euclidean techniques) there may be in the last century or so, or if there are current questions practitioners may be pursuing answers to.","['euclidean-geometry', 'math-history', 'geometry']"
3867262,Under what conditions is this product greater than 1?,"I have the following product: $$
\frac{x^T(X^TX)^{-1}x}{x^Tx}
$$ Where x $\in$ R $^{N}$ and X $\in$ R $^{M\times N}$ . In my application, $X$ is a design matrix for linear regression. $x$ is a vector of inputs. Under what conditions is this product greater than $\frac{1}{x^Tx}$ ? I few ideas: This looks like Rayleigh quotient $X^TX$ is positive semi-definite See the ""Special case of covariance matrices"" here I think $(X^TX)^{-1}$ yields a Hermatian matrix","['statistics', 'linear-algebra', 'eigenvalues-eigenvectors', 'real-analysis']"
3867291,Exponential-like power series with Fibonnaci coefficients,"Background I am considering a problem (for fun, not homework!) of the exponential of a certain matrix, \begin{equation}
A = \begin{bmatrix} 0 & 0 & a & b\\ 0 & 0 & c & 0\\ a & -c & 0 & 0\\b & 0 & 0 & 0\end{bmatrix}.
\end{equation} This has relevance as a certain generator of transformations in SO(3,1), for example. By computing powers of the matrix, I found that only four matrices are involved in its powers: $A$ itself, these two: \begin{equation}
P = \begin{bmatrix} a^2+b^2 & -ac& 0&0\\ac&-c^2&0&0\\0&0&a^2-c^2&ab\\0&0&ab&b^2\end{bmatrix},\quad \tilde{A} = \begin{bmatrix} 0 & 0 & 0 & c\\ 0 & 0 &-b&a\\0&b&0&0\\c&-a&0&0\end{bmatrix}
\end{equation} and the identity matrix. Defining scalars $\alpha = a^2+b^2-c^2$ and $\beta = bc$ , the powers of $A$ are: $
A^0 = I\\
A^1 = A\\
A^2 = P\\
A^3 = \alpha A + \beta\tilde{A}\\
A^4 = \alpha P + \beta^2I\\
A^5 = (\alpha^2+\beta^2)A + \alpha\beta\widetilde{A}\\
\quad\quad\cdots
$ because $A\tilde{A} = \beta I$ . I have been able to find series solutions for $\alpha = 0$ and $\beta = 0$ separately (and of course $\alpha=\beta=0$ ), because there is a clear pattern in the series coefficients for $\exp(A)$ . These correspond to certain special cases in the vectors describing the transformation (here, $\vec{A}=(0,a,b)$ and $\vec{B}=(0,0,c)$ , so the cases correspond to $\alpha=|A|^2-|B|^2=0$ and $\beta=\vec{A}\cdot\vec{B}=0$ respectively). My question is about a third interesting special case, where $\alpha=\beta\neq 0$ . In this case, the pattern in the matrix products generates a Fibonnaci sequence, so that the exponential series may be grouped into four series, \begin{equation}
\exp(A) = A\sum_{k=0}^\infty \frac{\text{Fib}_{k+1}}{(2k+1)!}\alpha^k + \text{ other matrices}
\end{equation} with the other series also containing similar sequences. My question is , is anyone was familiar with the function: \begin{equation}
f(x) = \sum_{k=0}^\infty\frac{\text{Fib}_{k+1}}{(2k+1)!}x^{2k+1}
\end{equation} I know that it has an infinite radius of convergence by ratio test. Does it relate to any special functions? Edit I have mainly resolved this thanks to Professor Vector's help! The odd powers of A can be written \begin{equation}
A^{2n+1} = c_{n+1}A + d_{n+1}\tilde{A}
\end{equation} where the coefficients satisfy the recurrence relations \begin{equation}
c_n = \alpha c_{n-1} + \beta^2 c_{n-2},\quad c_0 = 0, c_1 = 1
\end{equation} and $d_n = \beta c_{n-1}$ . This is a second-order linear recurrence relation and has closed-form solutions (omitted for brevity). In the special case $\alpha = \beta$ , then the coefficients are $c_n = F_n\alpha^{n-1}$ . In the matrix exponential, one finds that function as the coefficient of $A$ , \begin{equation}
\sum_{n=0}^\infty\frac{F_{k+1}}{(2k+1)!}\alpha^k = \frac{\sinh(\sqrt{\phi\alpha}) - \sin(\sqrt{\alpha/\phi})}{\sqrt{5}\alpha}
\end{equation} where $\phi$ is the Golden ratio, using Binet's formula for $F_n$ , as Professor Vector pointed out! Interesting to see $\phi$ show up here. Thank you again for your help.","['real-analysis', 'matrices', 'calculus', 'sequences-and-series', 'group-theory']"
3867305,"Let $A \in \mathbb{R}^{n \times n}$ such that for all $x \in \mathbb{R}^n, x^TAx \geq 0$. Prove that $\ker(A) = \ker(A^T)$","Let $A \in \mathbb{R}^{n \times n}$ such that for all $x \in \mathbb{R}^n, x^TAx \geq 0$ . Prove that $\ker(A) = \ker(A^T)$ . My idea:
if we can prove $A$ is symmetric, then we can solve it?","['matrices', 'linear-algebra']"
3867307,"If $f$ is Lebesgue integrable and $f''$ exists and is Lebesgue integrable, what can we say about the integrability of $f'$?","Let $\newcommand{\RR}{\mathbb{R}}f: \RR \to \RR$ be a twice differentiable function that is in $\mathcal{L}^1$ . Suppose that also $f'' \in \mathcal{L}^1$ . Can we conclude that $f' \in \mathcal{L}^1$ ? What about the case where we know $f \in \mathcal{L}^1$ and $f^{(m)} \in \mathcal{L}^1$ , can we say something about the integrability of $f^{(k)}$ for $k \leq m$ ? In general we can't say that the derivative of a smooth integrable function is integrable, but maybe the integrability of $f''$ helps?","['integration', 'measure-theory', 'lebesgue-integral', 'real-analysis', 'lp-spaces']"
3867330,formal book about Generalized linear models,"I'm searching for a book who treat GLM in a formal way, with a measure theoretic approach. Someone could help me?","['measure-theory', 'probability-theory', 'statistics']"
3867372,How to evaluate $\lim_{{n}\to\infty}{\sum_{{k}\leq{n}}{\left\lvert\frac{\sin{k}}{\ln{n^k}}\right\rvert}}$?,"Show that $$\sum_{k=1}^n{\mspace{-2mu}\frac{\left\lvert\sin{k}\right\rvert}{k}}\sim\frac{2}{\pi}\mspace{-1.5mu}\sum_{k=1}^n{\mspace{-2mu}\frac{1}{\mspace{-1mu}k}}$$ as $n\to\infty$ . Alternatively, since $\displaystyle\frac{1}{\ln{x}}\mspace{-1.5mu}\int_0^x{\mspace{-2mu}\frac{\left\lvert\sin{t}\right\rvert}{t}\operatorname{d}\!t}$ converges to $\dfrac{2}{\pi}$ as $x\to{+\infty}$ and $\displaystyle\lim_{n\to\infty}{\frac{{\it{H}}_n}{\ln{n}}}=1$ , how can we prove that $$\sum_{{1}\leq{n}\leq{\left\lfloor{x}\right\rfloor}}{\mspace{-2mu}\frac{\left\lvert\sin{n}\right\rvert}{n}}\sim\int_0^x{\mspace{-2mu}\frac{\left\lvert\sin{t}\right\rvert}{t}\operatorname*{d}\!t}$$ as $x\to{+\infty}$ ? It may seem like just one simple application of the so-called Euler–Maclaurin formula . Nonetheless, $\dfrac{\left\lvert\sin{u}\right\rvert}{u}$ is not always continuously differentiable on $\left[0, x\right]$ , hence the use of the Euler–Maclaurin summation formula shall be inadmissible in fact. So does the Stolz–Cesàro theorem . Some ""similar"" problems can be seen in many posts such as How can we prove that … , How to prove the convergence of the series? and How to find the limit… .","['asymptotics', 'real-analysis', 'analytic-number-theory', 'calculus', 'limits']"
3867401,Showing $Y(t)$ is a solution to $X'=AX$,"I am struggling with the problem: Let $A$ be a $n\times n$ matrix with eigenvalue $\lambda$ with multiplicity 3 and suppose the associated eigen vector $X_0\in N((A-\lambda I)^3)$ (Nullspace). Show that $$Y(t)=e^{\lambda t}\bigg[I+t(A-\lambda I)+\frac{t^2(A-\lambda I)^2}{2!}\bigg]X_0$$ is a solution to $X'=AX$ (without the use of a power series). My attempt : I know that for $Y(t)$ to be a solution, it must satisfy $$Y'(t)=AY(t).$$ Finding $Y'(t)$ we have $$Y'(t)=\lambda e^{\lambda t}\bigg[I+t(A-\lambda I)+\frac{t^2(A-\lambda I)^2}{2!}\bigg]X_0+e^{\lambda t}\bigg[I+A-\lambda I+t(A-\lambda I)^2\bigg]X_0$$ and expanding the right hand side we have $$AY(t)=e^{\lambda t}A\bigg[I+t(A-\lambda I)+\frac{t^2(A-\lambda I)^2}{2!}\bigg]X_0$$ I tried many things to get the LHS and RHS to equal. I know we have to use the fact that $(A-\lambda I)^3X_0=0$ somewhere, but i just cannot figure it out. Please if anyone can help out, it would be much appreciated.","['calculus', 'ordinary-differential-equations']"
3867463,"In the ring $\mathbb{Z}_p$, $p$ is prime, $(a+b)^p=a^p+b^p$ proof? [duplicate]","This question already has answers here : A freshman's dream (2 answers) Which fields satisfy the Freshman's Dream? (1 answer) Closed 3 years ago . In the ring, $\mathbb{Z}_p$ , $p$ a prime , prove that $(a+b)^p=a^p+b^p$ .
The hint that is given to us says that the binomial expansion works in commutative rings, but I think I used something much more simple? I said that $\mathbb{Z}_p$ of prime order is cyclic, so $\langle x\rangle = \mathbb{Z}_p$ is generated by $x$ . So therefore $x^p \bmod p = x$ . Thus in $\mathbb{Z}_p,(a+b)^p = a+b = a^p +b^p$ The only thing I am unsure of is if I have to prove that $x^p \bmod p = x$ , and then if I can apply it to $(a+b)^p$ . If this way is super is super goofy and, even if it works somehow, requires a lot of proof, then how might I get started on the binomial theorem? Thanks team.","['ring-theory', 'cyclic-groups', 'abstract-algebra', 'binomial-theorem']"
3867495,Square Roots of Diffeomorhpisms of Manifold Conjugate?,"Is this known: given a smooth manifold $Q^n$ , a diffeomorphism $f: Q \to Q$ that is isotopic to the identity, and two different ""square roots of $f$ "", that is, $g_1: Q \to Q$ and $g_2: Q \to Q$ with $g_1 \ne g_2$ , $g_1$ and $g_2$ both also diffeomorphisms and isotopic to the identity, and $g_1^2 = f = g_2^2$ , is it necessarily the case that $g_1$ and $g_2$ are conjugate, that is, that there is a diffeomorphism $q: Q \to Q$ with $q \circ g_1 = g_2 \circ q$ ? (It may be the case that $Q$ is actually some kind of tangent bundle, $Q = TT\ldots TQ' = T^nQ'$ , in which case we would want $q$ to be a bundle map, I think.) (See also this post and this post .)","['differential-topology', 'ordinary-differential-equations', 'dynamical-systems']"
3867555,"$X_1,X_2,\dots$ are i.i.d. and $X_1\thicksim\text{Exp}(\lambda)$, then $P\left(\limsup_{n\to\infty}\frac{X_n}{\log(n)}=\frac{1}{\lambda}\right)=1$","Problem: Let $X_1,X_2,\dots$ be i.i.d. random variables with $X_1\thicksim\text{Exp}(\lambda)$ . Show that $$P\left(\limsup_{n\to\infty}\frac{X_n}{\log(n)}=\frac{1}{\lambda}\right)=1.$$ My Approach: Let $\varepsilon>0$ be given and define the events $$A_n=\left\{\frac{X_n}{\log(n)}>\frac{1+\varepsilon}{\lambda}\right\}\quad\text{for }n\in\mathbb N.$$ Then $$\sum_{n=1}^\infty P(A_n)=\sum_{n=1}^\infty\int_{\lambda^{-1}(1+\varepsilon)\log(n)}^\infty \lambda e^{-\lambda x}\,dx=\sum_{n=1}^\infty\frac{1}{n^{1+\varepsilon}}<\infty.$$ It follows from the Borel-Cantelli lemma that $$P\left(\frac{X_n}{\log(n)}\leq\frac{1+\varepsilon}{\lambda}\text{ for all but finitely many }n\right)=1.$$ Since $\varepsilon>0$ was arbitrary, we have $$P\left(\limsup_{n\to\infty}\frac{X_n}{\log(n)}\leq\frac{1}{\lambda}\right)=1.$$ Next, we also have that $$\sum_{n=1}^\infty P\left(\frac{X_n}{\log(n)}>\frac{1}{\lambda}\right)=\sum_{n=1}^\infty\frac{1}{n}=\infty.$$ Since the events $\left\{\frac{X_n}{\log(n)}>\frac{1}{\lambda}\right\}$ are independent due to the random variables being independent, the second Borel-Cantelli lemma implies that $$P\left(\limsup_{n\to\infty}\frac{X_n}{\log(n)}>\frac{1}{\lambda}\right)=1.$$ It follows that $$P\left(\limsup_{n\to\infty}\frac{X_n}{\log(n)}=\frac{1}{\lambda}\right)=1.$$ Do you agree with my proof above? Any feedback is much appreciated. Thank you for your time.","['solution-verification', 'probability-theory', 'probability']"
3867572,What do mathematicians use to make graphs for analysis textbooks?,Apologies if this is not the correct SE for this. I have been transferring all of my notes to LaTeX and I have been wanting to include some custom diagrams. I often find my self having to find images to add to my notes but they are not quite what I want. I have tried using different graphic software but nothing seems to do it properly. Could anybody suggest to me any software through which I could make diagrams like this?:,"['complex-analysis', 'publishing']"
3867612,"Probability task, dispersion","$N$ men are sitting at the table. Each of them tosses the dice. Let $A$ be the random variable representing number of people, who got rolled number that is equal to both neighbour's . Find $\mathbb{E}X$ and $\mathbb{D}X$ . ( $\mathbb{D}$ - stands for dispersion/variance here). I managed to calculate $\mathbb{E}X$ , still haven't found $\mathbb{D}X$ . May you help me with $\mathbb{D}X$ ? I've been thinking about using somehow the following property: It is known that $\mathbb{D}X=\mathbb{E}X^2-(\mathbb{E}X)^2$ While knowing what $\mathbb{E}X$ is equal to we may find $(\mathbb{E}X)^2$ easily, in order to find $\mathbb{E}X^2$ I want to use following property: for $B= \varphi (A): \mathbb{E} B= \sum_{k} \varphi (x_k) \cdot P(ξ = x_k) $","['variance', 'probability', 'random-variables']"
3867629,Modular form and cohomology,"What is the relationship between modular forms (and modular functions) over $\Gamma \subseteq SL_2(\mathbb Z)$ , modular curves $Y(\Gamma)$ , its singular cohomology $H^k(Y(\Gamma), \mathbb Z)$ , and the group cohomology $H^k(\Gamma, M)$ for a suitable $M$ ? Details From what I understand (e.g. this question ), a modular form of weight $k$ is equivalent to a symmetric $k$ -tensor on the modular curve $Y(\Gamma)$ : $$f(\frac{az+b}{cz+d}) = (cz+d)^{2k} f(z) \iff f(g \cdot z) (\text{d}(g \cdot z))^k = f(z) \text{d} z^k \text{ where } g = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$$ where value at cusps are controlled by growth conditions on the modular forms. But then what do the differential forms and the de Rham cohomology on $Y(\Gamma)$ mean? Meanwhile, I remember from Akshay Venkatesh's Fields medal lecture that modular forms can be understood as elements of a group cohomology over the arithmetic group $\Gamma \subseteq SL_2(\mathbb{Z})$ , although I can't remember what module it was over.","['number-theory', 'modular-forms', 'homology-cohomology', 'differential-forms']"
3867634,Brownian motion: law of iterated logarithm,"I am doing a homework question. But I get confused. $\{B_t: t \geqslant 0\}$ is a standard Brownian motion. Show that there exists $t_{1}<t_{2}<\cdots$ with $t_{n} \rightarrow \infty$ such that with probability one, $$
\limsup _{n \rightarrow \infty} \frac{B_{t_{n}}}{\sqrt{t_{n} \log \log t_{n}}}=0
$$ But there is a theorem: (Law of the Iterated Logarithm for Brownian motion) Suppose $\{B_t: t \geqslant 0\}$ is a standard Brownian motion. Then, almost surely, $$
\limsup _{t \rightarrow \infty} \frac{B(t)}{\sqrt{2 t \log \log (t)}}=1
$$ is it a contradiction? Actually I tried $t_n=\exp(\exp(n))$ and apply the borel cantelli lemma, it seems to have: for any $\epsilon>0$ $$
\limsup _{n \rightarrow \infty} \frac{B_{t_{n}}}{\sqrt{t_{n} \log \log t_{n}}}< \epsilon
$$ But $t_n$ always go to infinite, so the theorem should give us $\sqrt{2}$ , really confused...","['empirical-processes', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
3867642,Closing the loophole in this real analysis paper,"In this paper , Theorem 1 states, given $F$ an arbitrary ordered subfield of $\mathbb{R}$ , $F$ is complete iff every continuous function defined on a closed and bounded interval has a uniformly differentiable anti-derivative. The authors mention that they have not been able to show whether or not the ""uniformly differentiable"" hypothesis can be dropped without penalty. So, my question is, has anyone done further research to show whether or not this loophole can be closed? Can the uniform differentiability hypothesis be dropped without affecting the theorem?",['real-analysis']
3867675,How to take the first derivative of the entropy-regularized Wasserstein distance?,"In optimal transportation theory, it is better to regularize the Wasserstein distance with an entropy constraint because it is differentiable, unlike its unregularized counterpart. By being differentiable, it can then be treated as a loss function that is compatible with common optimization algorithms. The entropy-regularized Wasserstein distance (aka Sinkhorn distance) is: $$ \text{inf} _{\gamma \in \Pi } \sum \|x - y\| \enspace \gamma(x,y)- \epsilon H(\gamma)$$ where $H(\gamma) = -\sum \gamma \text{ log}(\gamma)$ is the Shannon entropy of the transport plan $\gamma$ , and $\epsilon $ is the regularization parameter. What then is the derivative of the above formula, could someone show how to derive it?","['entropy', 'ordinary-differential-equations', 'optimal-transport', 'partial-differential-equations', 'derivatives']"
3867689,Proving $\lim_{x \to 1}\frac{x+1}{x-2} + x = -1$ using definition,"I got a question regarding my answer of proving limit using epsilon-delta, here's the question Prove $\lim_{x \to 1}\frac{x+1}{x-2} + x = -1$ Here's the answer I've come up so far let $f(x) = \frac{x+1}{x-2} + x$ by algebra manipulation, we get $|f(x) - (-1)| = |\frac{x+1}{x-2} + x +1| $ $=|\frac{x^2 - 1}{x-2}|$ $=|\frac{(x-1)(x+1)}{x-2}|$ let $|x-1| < 1$ , by triangle inequality we get $|x| < 2$ , then $|x + 1| < 3$ and $|x - 2| < 1$ now, using the definiton of limit, for every $\epsilon > 0$ , there exist $\delta = min\{1, \frac{\epsilon}{3}\}$ such that if $0 < |x - 1| < \delta$ then, $|f(x) - (-1)| = |\frac{x+1}{x-2} + x +1|$ $=|\frac{(x-1)(x+1)}{x-2}|$ $=|\frac{1 \cdot 3}{1}|$ $< \epsilon$ Is this correct? honestly I'm not sure on getting the upper bound of $|x-2|$ , so I used the assumption of $|x-1| < 1$ Any tips would help, thanks beforehand.","['limits', 'calculus', 'epsilon-delta', 'real-analysis']"
3867735,Let n be a positive integer such that $2\varphi(n)=n-1$. Prove that $n$ is not divisible by $3$,"$\varphi(n)$ is the Euler-Totient function So what I've got currently is that $\varphi(n)=(n-1)/2$ . The result of the Euler-Totient function is always a positive integer and is even, so $(n-1)/2$ must be both of those things as well. So, that would mean that $(n-1)$ must be even, and it must be divisible by $4$ , as $(n-1)/2$ must also be even. So I guess I'm left with $n \equiv 0 \;(\bmod\; 3)$ and $n-1 \equiv 0 \;(\bmod\; 4)$ . Assuming I'm even right about this, I'm trying to solve these 2 equivalence relations and trying to find a contradiction, which I don't how to do.",['number-theory']
3867743,hypoid gears described mathematically,"I bought a worm drive electric saw recently. The claim is that, rather than the worm drive I expected, the drive involves hypoid gears . From what I can see, hypoid is an abbreviation of hyperboloid. The claim is that the ""envelope"" of the teeth is a hyperboloid of revolution. Also that the teeth (the outer edges, I guess) are helical. At this point I don't know which gear they are talking about, a drive is two gears. I do have some engineering books from my father, but I can't find anything on this. Gear manufacture books seem to be insanely expensive. I suspect there is no Gear Manufacture for Dummies. Does anyone know the mathematics of this? I see the picture below as possibly a hyperboloid of two sheets for the large gear, one sheet matching two sheets for the small gear.
It is just that the small one is from the other side.  The part about helices might be true. The design goes back to the same Hero of Alexandria responsible for https://en.wikipedia.org/wiki/Heron%27s_formula according to Pliny the Elder. He credits Hero with the nut and bolt (in wood) and the first worm drive. https://en.wikipedia.org/wiki/One_Good_Turn_(book) https://en.wikipedia.org/wiki/Witold_Rybczynski Pictures below are from the 3-D printing website that Jean Marie found.","['reference-request', 'geometry', 'differential-geometry']"
3867760,Does a net converging to a point $x$ have a countable subset $\{x_n\}_n$ such that it converges to $x$ in a sequentially compact space?,"I was thinking for example, in functional analysis, we can talk about a Banach space $E$ and it's dual space $E^{*}$ with it's $w^{*}$ -topology. In the special case that $E$ is infinite dimensional and reflexive, we can say that the $w^{*}$ -topology in $E^{*}$ is the same as the weak topology. I also know that because $E^{*}$ must be infinite dimensional, we have that $S := \{\phi \in E^{*} : \left \| \phi \right \| = 1 \}$ has its topological closure equal to $B_{E^{*}}:=\{ \phi \in E^{*} : \left \| \phi \right \| \leq 1 \}$ in the weak topology. What is more, $B_{E^{*}}$ is compact in the $w*$ -topology (which is the same as the weak topology) by Banach-Alaoglu theorem (one can argue the same thing saying that $E^{*}$ is also reflexive), but by Eberlein–Šmulian theorem it is sequentially compact. I would like to know if this implies that, because I have a net of elements in $S$ converging to $0$ weakly, this implyies that there is a sequence of elements in $S$ converging to $0$ weakly. I have the feeling that sequential compactness doesn't imply that. Probably because this is similar to say that every infinite dimensional reflexive space has Schur's property, which is not true, for example, if we take an infinite dimensional separable hillbert space $H$ (in particular reflexive), because then it's dual is separable and infinite dimensional which implies (exercise) that $H$ doesn't have Shur's property.","['weak-convergence', 'reflexive-space', 'functional-analysis', 'general-topology', 'nets']"
3867762,Bounded operators on complex Banach space $X$ are commutative exactly when $X$ is one-dimensional?,"I am trying to prove that for a Banach space $X$ over $\mathbb{C}$ , dim $(X)=1$ if and only if $\mathfrak{B}(X)$ is commutative. From this StackExchange question ( Bounded linear operator commuting with every compact operators ), we can see that $A \in \mathfrak{B}(X)$ commuting with each $K \in \mathfrak{K}(X)$ (the space of compact operators) means that $A = \lambda I$ for some scalar $\lambda \in \mathbb{C}$ . I can convince myself that the result follows (every bounded operator being commutative means that they commute with compact operators, so they are of the form $\lambda I$ ), but I don't know how to rigorously get to the conclusion that dim $(X)=1$ , let alone how one would show the other direction. Any hint or help is highly appreciated.","['banach-spaces', 'operator-theory', 'compact-operators', 'functional-analysis']"
3867782,Is the collection $\mathcal{M}$ of $\mu$-measurable sets maximal so that $\mu|_{\mathcal{M}}$ is a measure?,"Let $\mu:2^{X} \to [0, \infty]$ be an outer measure. The collection $\mathcal{M}$ of $\mu$ -measurable sets are then defined as those sets $A$ satisfying $\mu(S)=\mu(S \cap A) + \mu(S \setminus A)$ for each $S \subset X$ . It's proven in any measure theory course that $\mathcal{M}$ is a $\sigma$ -algebra, with $\mu|_{\mathcal{M}}$ a complete measure. I've always found this to be an elusive definition. This has been discussed elsewhere . A more natural approach to defining the collection $\mathcal{M}$ of $\mu$ -measurable sets, in my mind, is that we want it to satisfy the following property: Let $\mathcal{U}_{\mu} \subset 2^{2^{X}}$ denote the collection of $\sigma$ -algebras $\mathcal{F}$ of $X$ with the property that $\mu|_{\mathcal{F}}$ is a complete measure on $\mathcal{F}$ . Then $\mathcal{M}$ is inclusion-wise maximal in $\mathcal{U}_{\mu}$ . In simple terms, we want $\mathcal{M}$ to be the largest set possible on which $\mu$ is a measure. This isn't a good definition for $\mathcal{M}$ , unfortunately, since we have no guarantee a priori that $\mathcal{U}_{\mu}$ has a unique maximal element. So I have some questions: Is $\mathcal{M}$ , as defined in the first paragraph, in fact inclusion-wise maximal in $\mathcal{U}_{\mu}$ ? Does $\mathcal{U}_{\mu}$ has a unique inclusion-wise maximal element? If the question to the latter question is ""no"", then what can we say about inclusion-wise maximal members of $\mathcal{U}_{\mu}$ distinct from $\mathcal{M}$ as defined in paragraph one?","['measure-theory', 'real-analysis']"
3867803,Finite disjoint union of the proper differences of the compact set forms a ring,"It would be great if someone can give a proof that in a locally compact Hausdorff space, the class of sets that are all sets which are the finite disjoint unions of the proper difference of the compact sets forms a ring. EDIT
To clarify, the ring discussed here is the ring of sets defined as
sets that are closed under operations difference (i.e., relative compliment) union that is,
if $A,B \in R$ $A-B \in R$ $A \cup B  \in R$ finite disjoint union of the Proper difference of the compact set means $$\cup_n E_n$$ where each $E_n$ is disjoint sets ranging from n=1,...,N and $$E_n=(A_n-B_n)$$ where it is assumed that $B_n \subset A_n$ $ A_n$ and $B_n\in C$ and $C$ is a class of compact sets in a locally compact Hausdorff space.","['elementary-set-theory', 'general-topology', 'measure-theory', 'compactness']"
3867834,Finding the sum of two angles,"I gotta find the value of $x+y$ in the following image I have no info about if a point is the middle point of a length or even the figure is a square. I can prove that $$x-y=30°$$ And I tried to use similarity between triangles (and so, parallelism)  but I got no clue at all.","['euclidean-geometry', 'geometry']"
3867940,Doubt about the existence of the gradient.,"Consider $$ f(x,y) = \begin{cases} \frac{xy}{x^2+y^2} & , \text{if } \ (x,y)\neq(0,0) \\
\hspace{0.5cm}0 &, \text{if } \ (x,y)=(0,0) \end{cases} $$ Computing the limit at zero $$
\left\{ 
\begin{array}{c}
x=rcos\theta \\ 
y=rsin\theta \\ 
\end{array}
\right.
$$ $$ (x,y) \to (0,0) \Longleftrightarrow r \to 0 $$ $$ \lim_{ r \to \ 0 }  \frac{ r^2cos\theta sin\theta }{ r^2( cos^2 \theta + sin^2 \theta) } = cos \theta sin \theta   $$ Then the limit does not exist. Hence this function is not continuos at zero. $$ \nabla f(x,y) = \begin{cases} \left(  \frac{y(y^2-x^2)}{(x^2+y^2)^2} , \frac{x(x^2-y^2)}{(x^2+y^2)^2} \right) & , \text{if } \ (x,y)\neq(0,0) \\
\hspace{0.5cm} (0,0) &, \text{if } \ (x,y)=(0,0) \end{cases} $$ So it doesn't matter that the function is not continuous at zero? or this function does not differentiable at zero?","['continuity', 'multivariable-calculus', 'derivatives', 'vector-analysis']"
3867955,"Prove that if $B\subseteq A$, then $A\backslash (A\backslash B) = B$.","So I tried this problem and I just wanted to make sure I did it right. Here it is: Proof: Given: $B \subseteq A$ I.) Let $x\in A\backslash(A\backslash B)$ . It follows that $x\in A$ AND $x\notin A\backslash B$ , but $x\notin A\backslash B$ is logically equivalent to $x\notin A$ OR $x\in B$ . By the distributive law for logic that gives ( $x\in A$ AND $x\notin A$ ) OR ( $x\in A$ AND $x\in B$ ), which is just False OR ( $x\in A$ AND $x\in B$ ). That leaves $x\in A$ AND $x\in B$ which implies that $x\in B$ , and thus $A\backslash(A\backslash B) \subseteq B.$ II.) Let $x\in B$ . Then its clear that $x\in A$ (because $B \subseteq A$ ) and $x\notin A\backslash B$ (because $x\in B$ ). However that's the same thing as saying that $x\in A\backslash(A\backslash B)$ , so $B\subseteq A\backslash(A\backslash B)$ . Therefore $A\backslash (A\backslash B) = B$ if $B\subseteq A$ . Is my proof correct?",['elementary-set-theory']
3867994,"Proving or disproving: If $0<a<b<1$, then $(1-a)^b>(1-b)^a$","Prove or disprove If $0<a<b<1$ , then $$(1-a)^b>(1-b)^a$$ I think this looks true when evaluating the differential equation $\frac{dy}{dx}=-y$ with initial condition $y(0)=1$ using euler method As an example with step size a=0.2 and b=0.3 with step size $a=0.2$ Evaluate the value of $y$ , 3 times by Euler method with step size a=0.2 will get $y_{a3}=1(1-0.2)^3=1(1-0.2)^{10(0.3)}$ While using step size of $0.3$ and repeat 2 times $y_{b2}=1(1-0.3)^2=1(1-0.3)^{10(0.2)}$ Plotting this in (x,y) axis, one can compare at $x=0.6 ,y_{a3}>y_{b2}$ . I tried with different value of a and b, the inequality looks true as saying Euler method with smaller step size will be the upper boundary for bigger step size for this differential equation. Which also mean the exact solution of this exponential function is the uppest boundary for all step size bigger than dx. I tried binomial expansion, but it only makes it much complicated. $(1-a)^b=1-ab+\frac{b(b-1)}{2!}(-a)^2+..$ $(1-b)^a=1-ab+\frac{a(a-1)}{2!}(-b)^2+..$ I can only show for the 3rd term $\frac{b(b-1)}{2!}(-a)^2>\frac{a(a-1)}{2!}(-b)^2$ since $b>a$ I do not found any way to proof nth term of $(1-a)^b$ will always bigger than $(1-b)^a$ , this is where i stuck.","['logarithms', 'analysis', 'functions', 'inequality', 'exponential-function']"
3868051,Calculate area of polygon inside unit square,"ABCD is a unit square and E is a point inside it, such that angle CED is right and $\frac {DE}{AE} = \frac {4}{5}$ .
Calculate the area of ECBAE (green area). Although Geometry is not my strong point, I have tried the following:
By drawing a vertical from E to AD (h = altitude of triangle AED), this splits the side AD into 2 parts, say x and y. Knowing also that $ED = 4k$ and $EA = 5k$ , we apply Pythagoras twice, having also $x+y=1$ . By this, we get a relation between x and y: $y-x = 9k^2$ .
But we have 3 unknowns, so I am not getting anywhere...
Then we could also apply Pythagoras in right triangle CED and calculate EC and then get the areas of both triangles and deduct from the area of the square, which is 1. By the way, Geogebra gives a number near 0.41 for the area in question. Thank you in advance!","['euclidean-geometry', 'systems-of-equations', 'area', 'circles', 'geometry']"
3868062,Interpretation of the Weil pairing in the complex torus,"The point addition on an elliptic curve corresponds to the vector addition on a complex torus (with suitable choice of the lattice and of the base point). Is there a similar interpretation for the Weil pairing? And for the Tate pairing? Furthemore, the determinant of two vectors in $\mathbb{C}$ (considered as $\mathbb{R}^2$ ) is also an non-degenerate alternating form. Is there a corresponding pairing?","['algebraic-geometry', 'elliptic-curves']"
3868078,from differential geometry to complex analysis,"I'm an undergraduate in physics and all I know about the classical complex analysis is Cauchy's integral formula and residue theorem. I'm recently reading John Lee's Introduction to Smooth Manifolds, and it seems OK to apply Stokes' theorem directly to complex functions on complex plane $\mathbb{C}\simeq\mathbb{R}^2$ , then one obtains the whole theory of complex analysis. Is there any textbook or lecture note concerning this topic?","['complex-analysis', 'reference-request', 'differential-geometry']"
3868131,What is the difference between standard deviation and variance?,"I have been learning discrete probability and know that variance and sd are both measures of spread. I also know that sd is just the variance squared. But I don't know the difference between them. Do they measure different types of spread? Also, I know that the expected value is used in terms of a full census so the mean is called the expected value. In a sample, however, the mean is just called the mean and is only an approximation of $\mu$ (E(x)). Similarly, $S$ is an approximation of $\sigma$ . But is there such thing as an approximation of $Var(x)$ in a sample?",['statistics']
3868135,(Yet) Another attempt at embedding free group on three letters inside a free group on two letters,"This question could have been asked by several users. Here is my attempt. This could possibly be completely in the wrong direction. So I attempt to show $F(\{x,y,z\}) $ is a subgroup of $F(\{a,b\})$ . My intention is to exploit the universal mapping property (UMP) (and it could be in the wrong direction but anyway I'll give it a try for reasons stated below). To begin with, we have the natural injection $i:\{x,y,z\}\to F(\{x,y,z\})$ s.t $i(x)=x,i(y)=y,i(z)=z$ and the map $f:\{x,y,z\}\to F(\{a,b\})$ given by $f(x)=a^2,f(y)=ab,f(z)=b^2$ . So by UMP of $F(\{x,y,z\})$ there is a unique $\phi: F(\{x,y,z\})\to F(\{a,b\})$ (homomorphism) such that $f=\phi \circ i$ i.e $\phi(x)=a^2,\, \phi(y)=ab,\, \phi(z)=b^2$ . So $F(\{x,y,z\})/ker(\phi)$ is (isomorphic to) a subgroup of $F(\{a,b\})$ and hence free. So can we conclude from this that $ker(\phi)$ is trivial ?? We will be done if this happens. A more general question (possibly wrong) can be formulated as follows : If $\psi : F(X)\to F(Y)$ is a surjective homomorphism between free groups, is $ker(\psi)$ trivial ?? (Edit): So may be a better way to formulate the general question would be : If $F(X)$ is a free group and a factor group $F(X)/N$ is also free then does that imply the normal subgroup $N$ to be trivial ?? I am having a feeling that the answer to this question is in affirmative but I am nowhere near a solution. P.S: There could be some possible directions to prove this using fundamental groups, but my algebraic topology is quite rusty (hence didn't tag it) so I would prefer algebraic attempts. But anyways, all ideas are welcome and thanks in advance. Also I am not really good at finding counterexamples in questions related to free group, so if this question is stupid, please ignore.","['combinatorial-group-theory', 'universal-property', 'group-theory', 'free-groups']"
