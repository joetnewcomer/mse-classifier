question_id,title,body,tags
4888553,Law of large number with subset of the variables,"Let $(X_i, Y_i)_{i=1}^{\infty}$ be iid continuous random vectors with continuous joint density, where $X_1$ have support $\mathcal{X}$ . Let $B_n\subset \mathcal{X}\subset\mathbb{R}$ be decreasing subsets (open intervals) such that $\cap B_n= x_0\in\mathcal{X}$ . Let $S = \{i\leq n: X_i\in B_n\}$ .
I want to show that $$
\frac{1}{|S|}\sum_{i\in S}Y_i \overset{P}{\to} \mathbb{E}[Y_1\mid X_1=x_0], \,\,as\,\,n\to\infty. 
$$ I assume that the necessary condition for this convergence is $|S|\to\infty$ or that $nP(X_i\in B_n)\to\infty$ . Is it sufficient? Is there some theory that describes this?","['statistical-inference', 'statistics', 'central-limit-theorem', 'law-of-large-numbers']"
4888561,Law of Trichotomy for Well-Orderings,"Often in beginning set-theory courses, and in particular in Jech's book Set Theory , it is proved from scratch that given any two well-orderings, they are isomorphic or one is isomorphic to an initial segment of the other. This is the law of trichotomy for well-orderings, and while simple is not exactly trivial to prove. However, it is also proven in an apparently independent manner that every well-ordered set is isomorphic to an ordinal, and for any two ordinals $\alpha$ and $\beta$ either $\alpha \subseteq \beta$ or $\beta \subseteq \alpha$ . It is then trivially seen from this fact that that either $\alpha = \beta$ or $\alpha$ is an initial segment of $\beta$ or vice versa. From these facts about ordinals, the law of trichotomy for well-orderings follows immediately. My question is: is there any reason to prove from scratch the law of trichotomy? Is it secretly used somewhere to prove some of these basics facts about ordinals, and I simply missed it? Or can this proof just be omitted?","['elementary-set-theory', 'order-theory', 'well-orders', 'set-theory']"
4888567,"Suppose we want to prove that a property $P$ is true for every integer in $ℕ_{odd}$ = $\{1,3,5,7,9,...\}$.","Suppose we want to prove that a property $P$ is true for every integer in $ℕ_{odd}$ = $\{1,3,5,7,9,...\}$ . Consider the following induction mechanism: Base case: Verify the property $P(1)$ Inductive step: Prove that for all $k ≥ 1, P(k) ⇒ P(k + 1)$ (a) Why might the above mechanism not constitute a valid proof? (b) How would you modify the inductive step to obtain a valid proof? (c) Use your modified mechanism to prove that every integer $n$ ∈ $ ℕ_{odd}$ satisfies $2^n+3^n = 5m$ , where $m$ is an integer. What I did A) What I thought is that it doesn't constitute a valid proof because it is not complete. In that it should have been a strong induction for the set of all odd numbers in the set $ℕ_{odd}$ . It only verifies $P(1)$ . B) I am not sure, but to continue on from the previous one, maybe continue by changing the inductive step to constitute for all odd numbers in the set $ℕ_{odd}$ . C) I am also not sure, but most likely this is an inductive proof where base case = $2^1+3^1 = 5(1) = 5$ . Inductive step, assuming $k$ is in the set of all odd integers. $P(K)$ = $2^k+3^k= 5m $ and prove this for $k+1$ . $P(K+1)$ = $2^{2k+1}+3^{2k+1} = (2^k)^2 \times 2  + (3^k)^2 \times 3 = 5m$ , which leads to $P(K)$ . Question I need help with B) and C) for how to modify the inductive step to obtain a valid proof, and use that modified mechanism to satisfy the proof.","['induction', 'parity', 'discrete-mathematics']"
4888587,Counting the amount of injective functions with restriction,"First I will say that the exact same question has been uploaded to the site already few years ago but it seems no final answer were given there. Let $$X = \{a,b,c\}, \quad Y = \{1,2,3,4,5,6,7\}.$$ I need to count how many injective functions $f: X\to Y$ there are such that $$f(a)\ne 1,2,\quad  f(b)\ne 2 ,\quad f(c) \ne 3.$$ My instictive approach to this was counting all the injective functions $X\to Y$ without restrictions which is $7 \cdot 6 \cdot 5 = 210$ and subtracting from this the amount of ""illegal"" functions. My issue is counting the illegal functions Intried to use inclusion exclusion since $f(a)$ and $f(b)$ share a restriction but I reached a dead end. Is there a simpler way to approach this question?","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
4888593,"Prove that that the function $f:\mathbb R^3\to \mathbb R$ defined by $f(x,y,z)=ye^x+xz^2$ is differentiable at the point (0,-4,2) using definition.","Prove that that the function $f:\mathbb R^3\to \mathbb R$ defined by $f(x,y,z)=ye^x+xz^2$ is differentiable at the point $\vec a=\langle 0,-4,2\rangle$ using definition. My attempt:- Let $\vec{h}=\langle h_1,h_2,h_3\rangle.$ Then we need to prove $$\lim_{\vec{h}\to \vec 0}\frac{f(\vec a+\vec{h})-f(\vec a)-f_x(\vec a)h_1-f_y(\vec a)h_2-f_z(\vec a)h_3}{||\vec h||}=0.$$ Consider the LHS $$\lim_{\vec{h}\to \vec 0}\frac{f(\vec a+\vec{h})-f(\vec a)-f_x(\vec a)h_1-f_y(\vec a)h_2-f_z(\vec a)h_3}{||\vec h||}=\\\lim_{\vec{h}\to \vec 0}\frac{(-4+h_2)e^{h_1}+h_1(2+h_3)^2+4-h_2}{\sqrt{h_1^2+h_2^2+h_3^2}}$$ I can use $(-4+h_2)\leq h_2$ . I don't know how to proceed after that. I used spherical coordinated. I used all the possibilities available for me. Could you help me?","['multivariable-calculus', 'derivatives']"
4888631,Topological properties vs homeomorphisms,"I'm studying general topology and a question has come to my mind. We have defined a topological property to be a property which a (viz. any) topological space can satisfy or not satisfy, and such that, if satisfied by a space, is also satisfied by every space homeomorphic to it. I can see the ambiguity of this definition lying in its lacking to specify the language in which the properties are expressed. Anyway, I was wondering if, in some appropriate language, topological properties are enough to capture the notion of homeomorphisms. More precisely, is it true that, if two spaces satisfy the same topological properties written in an appropriate (formal) language , then they are homeomorphic? Feel free to make assumptions on the language of the properties. Disclaimer If we think about properties in the most general and informal sense, then the answer is yes. Indeed, given a topological space X, ""being homeomorphic to X"" is a topological property. As a result, given another space Y having the same topological properties as X, it is indeed homeomorphic to X. The question may get more interesting restricting the language in which properties can be expressed.","['general-topology', 'logic', 'geometry', 'model-theory']"
4888657,Solving for $DE$ in a Geometric Puzzle,"I hope this message finds you well. I am contacting you to seek your help in solving a fascinating geometry problem that I encountered in a recent competition. Despite my diligent attempts, I have not been able to find a solution. I am keen to acquire insights that will undoubtedly enhance my comprehension of this geometric puzzle. Problem Description In $\Delta$ $ACD$ , $AC=AD=CD$ , and $AB$ = $3$ , $BC$ = $6$ , find the value of $DE$ Approach: I tried this way to find the values of $x$ and $y$ .  therefore, I can apply the sine law. I made a note of this method, and then I tried to determine the values of $x$ & $y$ but I was unable to do so... I would be extremely grateful for any help or advice in figuring out the intricacies of this problem. Thank you for your expertise and support.","['analytic-geometry', 'geometry']"
4888665,"Is this a valid ""easy"" proof that two free groups are isomorphic if and only if their rank is the same?","I have read on different sources that it is not possible to give a simple proof that ""two free groups are isomorphic if and only if they have the same rank"" using only what ""a student who has just read the definition of free group as a set of words over an alphabet"" would know. See for example the answers to this question Is there a simple proof of the fact that if free groups $F(S)$ and $F(S')$ are isomorphic, then $\operatorname{card}(S)=\operatorname{card}(S')?$ . I think I have come with such a proof, but I would like to know if it is valid. The proof goes as follows. If A and B have the same cardinality, we can define a bijection between letters on A alphabet and letters on B alphabet. This establishes a bijection between (reduced) words on A and (reduced) words on B , and the isomorphism between the free groups F(A) and F(B) . This proves the ""if"". Now suppose that | A |<| B |. We can define a bijection between letters on A and a subset of the letters on B . Put differently, we can “relabel” the letters on A and say that B contains all the letters on A plus, at least, one letter that is not in A . Let x be this “extra” letter. Let w be any reduced word on A , that is, any element of F(A) . Then wx is a reduced word on B , but not a valid word on A . That is, there is at least one element of F(B) that is not on F(A) . Therefore, F(A) and F(B) cannot be isomorphic. This proves the ""only if"".","['group-theory', 'solution-verification', 'free-groups', 'group-isomorphism']"
4888666,What does a parametric equation mean?,"I am following the last module of Differential Calculus on Khan Academy, that deals with Parameteric equations. Here are the parametric equations described in the lecture. $x(t) = 5t + 10$ $y(t) = 50 - 5t^2/2$ However, I really don't understand what parametric equations really mean. How do they differ from normal equations. According to Wikipedia : ""In mathematics, a parametric equation defines a group of quantities as functions of one or more independent variables called parameters."" I really don't understand what this definition is trying to convey. From what I observed, if two functions share a variable, it typically gets defined as a parametric equation. But that seems to be a loose definition. Regarding my prerequisite knowledge, I have a Masters in Engineering. Therefore I understand the formulae of calculus quite well. I just never bothered to understand some of the underlying concepts. Therefore, I am revisiting it by through Khan Academy.",['calculus']
4888724,Applying Whittaker's and Watson's Lagrange theorem to $\sin$,"I want to use the Lagrange inversion theorem from Whittaker and Watson , p. 133, to find the power series of $\sin^{-1}$ at the origin. It should be a valid procedure because $\sin^{-1}$ is analytic at the origin. The theorem is stated as follows: Let $f(z)$ and $\phi (z)$ be functions of $z$ analytic on and inside a contour $C$ surrounding a point $a$ , and let $t$ be such that the inequality $$|t\phi (z)|\lt |z-a|$$ is satisfied at all points $z$ on the perimeter of $C$ ; then the equation $$\zeta=a+t\phi (\zeta),$$ regarded as an equation in $\zeta$ , has one root in the interior of $C$ ; and further any function of $\zeta$ analytic on and inside $C$ can be expanded as a power series in $t$ by the formula $$f(\zeta)=f(a)+\displaystyle\sum_{n=1}^\infty \dfrac{t^n}{n!}\dfrac{d^{n-1}}{da^{n-1}}[f'(a)\{\phi (a)\}^n].$$ First, I transform that theorem to a more tractable form for my problem. We can obviously take $f=\operatorname{id}$ . Let $$\phi (z)=\dfrac{z-a}{g(z)-g(a)}.$$ Then $$\zeta =a+t\dfrac{\zeta-a}{g(\zeta)-g(a)}$$ which gives $$t=g(\zeta)-g(a).$$ So far, we have $$\zeta=a+\displaystyle\sum_{n=1}^\infty \dfrac{(g(\zeta)-g(a))^n}{n!}\displaystyle\lim_{z\to a}\dfrac{d^{n-1}}{da^{n-1}} \left(\dfrac{z-a}{g(z)-g(a)}\right)^n.$$ Then let $\zeta=g^{-1}(z)$ , so finally $$g^{-1}(z)=a+\displaystyle\sum_{n=1}^\infty \dfrac{(z-g(a))^n}{n!}\displaystyle\lim_{z\to a}\dfrac{d^{n-1}}{da^{n-1}} \left(\dfrac{z-a}{g(z)-g(a)}\right)^n.$$ We have to find $z$ such that $$|t \phi(z)|\lt |z-a|$$ for all $z$ on the perimeter of $C$ ; i.e. $$\left|(g(\zeta)-g(a))\dfrac{z-a}{g(z)-g(a)}\right|\lt |z-a|,$$ i.e. $$|g(\zeta)-g(a)|\lt |g(z)-g(a)|$$ which, after using $\zeta=g^{-1}(z)$ and $a=0$ and $\sin 0=0$ simplifies to $$|z|\lt |g(z)|$$ but there exists no contour $C$ surrounding the origin such that for all points $z$ on its perimeter we have $|z|\lt |\sin z|$ . What should I do?","['complex-analysis', 'inequality', 'sequences-and-series']"
4888744,Particular sum of roots of unity,"I've gotten stuck on a particular sum, to which I think I know the answer (thanks to Wolfram:Alpha), but not the method leading to it. I wonder if someone here can help me solve it. Let $d$ be a positive, odd integer, and define $u$ to be the first nontrivial $d$ th root of unity (that is, $u = \exp\left(\frac{2\pi i}{d}\right)$ ). Furthermore, let $k$ and $m$ be two integers s.t. $0\leq k,m \leq d-1$ . I would like to evaluate the sum: $$S =\sum_{j=0}^{d-1}\frac{u^{-kj}}{1+u^{mj}}$$ Now, after experimenting in W:A for a bit, I managed to find an interesting result. Letting $A = [0,\frac{d-1}{2}]$ and $B = [\frac{d+1}{2},d-1]$ ... $$S=\begin{cases}-\frac{1}{2}d \hspace{10pt}\text{ if } (k\in A\land m\in B)\lor (k\in B \land m\in A) \\
\frac{1}{2}d \hspace{18pt}\text{ if } (k\in A\land m\in A)\lor (k\in B \land m\in B) \text{ or } k=0\\
0 \hspace{26pt}\text{ if } m=0
\end{cases}$$ Such an elegant solution indeed, but I have no earthly idea on how to solve this. I've tried to conjugate $S$ , fiddling with the exponents a bit and such, which gave rise to a few nice identities, but none proving useful enough to actually solve this. Is this correct? How would I show this? Any suggestions?","['summation', 'complex-analysis', 'roots-of-unity', 'sequences-and-series', 'complex-numbers']"
4888750,Help with the indefinite integral $\int \frac{dx}{2x^4 + 3x^2 + 5}$,"I start by rewriting the denominator, $2x^4+3x^2+5$ , as a squared term plus a constant. To do this, we notice that the first two terms already have a common factor of $2x^2$ . We can complete the square by taking half of the coefficient of our $x^2$ term, squaring it, and adding it to both the numerator and denominator. Since the coefficient of our $x^2$ term is $3$ , half of it would be $\frac{3}{2}$ , and squaring it gives us $\frac{4}{9}$ . So we add $\frac{4}{9}$ to both the numerator and denominator. I started like that but I couldn't go further. Can you please help me to solve it.","['integration', 'algebra-precalculus', 'completing-the-square', 'quadratics']"
4888791,"Proving that, for positive integers $k_i$, there exists $x_0\in[0,\pi]$, such that $\frac12+\sum_{i = 1}^m\cos(k_ix_0)<0$","$k_i$ is a positive integer, $i=1,\ldots,m$ , please try to prove that there exist a point $x_0 \in [0,\pi]$ , such that $\frac{1}{2}+\sum\limits_{i = 1}^m {\cos ({k_i}{x_0})} < 0$ . My attempt: If $k_i$ are all equal, this question is easy; If $k_i$ forms an arithmetic sequence, using the product to difference formulas, we can simplify the expression $\frac{1}{2}+\sum\limits_{i = 1}^m {\cos ({k_i}{x})}$ , for example, $\frac{1}{2} + \sum\limits_{i = 1}^n {\cos (ix)}  = \frac{{\sin \frac{{(2n + 1)}}{2}x}}{{2\sin \frac{x}{2}}}$ . Now, however, $k_i$ are disorganized.","['trigonometry', 'sequences-and-series', 'real-analysis']"
4888807,Number of lines on singular cubic surfaces,"Bruce and Walls in their paper On the classification of cubic surfaces state at the end the ""final observation"" that a number of distinct lines on cubic surface with $k$ isolated du Val singularities is given by formula $$
\binom{8-c}{2} + k - 1
$$ where $c$ is the codimension of space parameterising surfaces of this type in $\mathbb PH^0(\mathcal O_{\mathbb P^3}(3)) \simeq \mathbb P^{19}$ . However, if I am understanding correctly, their proof relies on checking directly all possible cases, which is not very convincing. Could anyone sugest a reference with more intersection-theoretic proof of this fact?","['surfaces', 'complex-geometry', 'complex-analysis', 'algebraic-geometry', 'intersection-theory']"
4888832,How to approximate $\sum_{n = 1}^{\infty} \frac{n}{n^3 + x}$ for $x$ approaching $\infty$?,How to find the approximate value of $f(x) = \sum_{n = 1}^{\infty} \frac{n}{n^3 + x}$ as a function of $x$ when $x$ goes to $\infty$ ? I want a closed form function $g(x)$ s.t. $f(x) \sim g(x) $ for $x \to \infty$ . I have tried to integrate $f(x)$ : $$ \int f(x) dx= \sum_{n = 1}^{\infty} n\int\frac{1}{n^3 + x} dx = \sum_{n = 1}^{\infty} n \log(n^3 + x).$$ But this isn't helpful. Any other idea?,"['calculus', 'taylor-expansion', 'sequences-and-series']"
4888844,"Showing $\int_{-1}^{1} \frac{125}{12}\sqrt[10]{\frac{1 + x}{1 - x}} (x^2 - x) \, dx = \phi\pi$","While exploring possible applications for this new trick , I stumbled upon an entire family of integrals that ""always"" yield $a\pi^n$ , where $a$ is an algebraic number and $n$ is a natural number. The following integral captivated me greatly: $\boxed{\int_{-1}^{1} \frac{125}{12}\sqrt[10]{\frac{1 + x}{1 - x}} (x^2 - x) \, dx = \color{red}{\phi}\color{blue}{\pi}}\tag{1}$ . The family: $\int_{-1}^{1}(\frac{1 + x}{1 - x} )^{\frac{1}{2}}(x^2 - x) dx = 0$ . $\int_{-1}^{1} (\frac{1 + x}{1 - x})^{\frac{1}{4}} (x^2 - x) dx = \frac{\pi}{8\sqrt{2}}$ . $\int_{-1}^{1} (\frac{1 + x}{1 - x})^{\frac{1}{6}} (x^2 - x) dx = \frac{10\pi}{81}$ . $\int_{-1}^{1} \left(\frac{1 + x}{1 - x}\right)^{\frac{1}{8}} (x^2 - x) \, dx = \frac{\sqrt{3}}{100} \left(\frac{22787}{479}\right)^{\frac{1}{4}} \pi^2.$ $\int_{-1}^{1}(\frac{1 + x}{1 - x} )^{\frac{1}{10}} (x^2 - x) dx = \frac{12}{125}\phi\pi$ . $\int_{-1}^{1}(\frac{1 + x}{1 - x})^{\frac{1}{12}} (x^2 - x) dx = \frac{15455288\pi}{94257441}$ . The family seems to beg for a generalization. Is such a thing possible?","['integration', 'definite-integrals']"
4888917,How are these two conditions equivalent?,"I'm reading an article and I quote the author here : The condition $\sum_{n=1}^{\infty} n^t L(n) \operatorname{Pr}\left(|X|>n^{1 / r}\right)<\infty$ is equivalent to the moment condition $E\left[|X|^{(t+1) r} L(X)\right]<\infty$ . $t \geq 0, 0<r <2$ , $X$ is an arbitrary random variable and $L(\cdot)$ is a slowly varying function but shouldn't matter here, I know that generally for positive random variables we got $\mathrm{E}[X] \sim \sum_{n=0}^{\infty} \mathrm{P}(X>n)$ but it doesn't seem to me that there's a an obvious way of arranging terms to get that equivalence of conditions. if somebody can help that'll be cool.","['integration', 'probability-theory', 'sequences-and-series']"
4888920,Is there a closed form for the linear operator $T$ such that $T(x^n) =f(n)x^{n-1}$?,"When I first learnt calculus I was so surprised to learn that there is a meaningful mathematical operator $D$ that $$
D(x^n)= n x^{n-1}.
$$ It seemed to be a very random thing to multiply the exponent by the function and subtract one from the exponent to get the correct result. Again, this sounded very strange for me when I first learnt about derivatives, I mean if I was asked before I learnt the calculus of $D$ I would have dismissed this as a random useless property. Back then I thought about this question: is there a mathematical operator $T$ such that $$
T(x^n) =f(n)x^{n-1}\;?
$$ After $6$ years, now I remembered the question and I modified it as follows: Is there a closed form for the  linear operator $T$ defined on the vector space of all vector space of all polynomials and power series be such that $$
T(x^n) =f(n)x^{n-1}\quad \ n \ne 0 
$$ for any continuous $f$ on $\mathbb{R}$ ? After some thought I think the answer is no. Even a simple function like $f(x)=c\ne 0$ is very  hard to find and I couldn't find such representation.","['calculus', 'operator-theory', 'functional-analysis', 'real-analysis']"
4888985,Havel Hakimi Theorem -- What happens if there are no enough numbers?,"I am learning about Havel Hakimi. I know how the algorithm works, but I'm not sure what happens when there are no enough numbers. For example, (2,3,3,4). I remove 4, then I'm supposed to subtract 1 from the next 4 numbers ın the sequence but there only 3 numbers. Does that mean it's not graphical, or should I subtract 1 from all the remaining numbers anyway?","['graph-theory', 'discrete-mathematics']"
4888992,"What is in the image of the exponential of $\frak{sl}(n,\mathbb{R}$)? What do you need to get all of $\mathrm{SL}(n,\mathbb{R}$)?","This question discusses how $\mathrm{S}L(2,\mathbb{R}$ ) coincides with $\pm\exp(z)$ with $z\in \frak{sl}(n,\mathbb{R}$ ) (the real traceless matrices). Is it known what happens for $n>2$ ? Namely, one can represent all invertible matrices with determinant $1$ and trace greater than or equal to $-2$ . Also, just by providing a sign (more precisely, up to the center of the group, which is $\{ I, -I\}$ ) one is able to obtain all of $\mathrm{SL}(2,\mathbb{R})$ ). Is such a result known for the general case of $\mathrm{SL}(n,\mathbb{R})$ , which characterizes the image of the exponential and what one needs to ""add"" in order to obtain the entire group (i.e. what is the ""minimal"" set $A\subseteq G$ such that $\mathrm{SL}(n,\mathbb{R}) = A \exp(\frak{sl}(n,\mathbb{R}))$ )? I would also be interested in the more general question regarding classical Lie groups, although this would already be a great help towards a greater understanding of what happens in the exponential (even just for $n=3$ ).","['lie-algebras', 'matrix-exponential', 'matrices', 'linear-algebra', 'lie-groups']"
4889017,How do we get an estimator for $\operatorname E\left[\int_0^\tau f(X_t)\:{\rm d}t\right]$?,"Let $(X_t)_{t\ge0}$ be a time-homogeneous Markov process with transition semigroup $(\kappa_t)_{t\ge0}$ and $\tau$ be a finite stopping time adapted to $(X_t)_{t\ge0}$ . Say I run the following algorithm: Say my abort criterion ensures that $\sum_{i=0}^{n-1}\tau_i$ is a sample from $\tau$ . Is this enough to show $$\sum_{i=0}^{n-1}\tau_if(x_i)\approx\operatorname E\left[\int_0^\tau f(X_t)\:{\rm d}t\right]\tag1?$$ Clearly, if $\tau$ is constant and the grid $(\tau_0,\ldots,\tau_{n-1})$ is ""fine"" enough, $(1)$ is satisfied by the ergodic theorem. But what can we say for random $\tau$ ? Clearly, the grid still needs to be fine enough, but can we express the error of the approximation in $(1)$ in terms of that grid size maybe?","['measure-theory', 'monte-carlo', 'markov-chains', 'markov-process', 'probability-theory']"
4889043,"Reduce $\frac{d^{n-1}}{dw^{n-1}}\frac{4^{-n/{\sqrt w}}}{\sqrt w}\Big|_1=\frac1{\sqrt\pi}G^{3,0}_{1,3}\left(^{3/2-n}_{0,1/2,1/2};(\ln(2)n)^2\right)$","In this answer to Is there any valid complex or just real solution to $\sin(x)^{\cos(x)} = 2$ ? , one must calculate $$\frac{d^{n-1}}{dw^{n-1}}\left.\frac{4^{-\frac{n}{\sqrt w}}}{\sqrt w}\right|_1=\sum_{m=0}^\infty\frac{\Gamma\left(\frac12-\frac m2\right)(-\ln(4)n)^m}{\Gamma\left(\frac32-n-\frac m2\right)m!}=\text H^{1,1}_{1,2}\left(^{\left(\frac12,-\frac12\right)}_{(0,1),\left(n-\frac12,-\frac12\right)};\ln(4)n\right)=\frac1{\sqrt\pi}\text G^{3,0}_{1,3}\left(^{\frac32-n}_{0,\frac12,\frac12};(\ln(2)n)^2\right)\tag1$$ where one can convert Fox H into Meijer G functions using the Wolfram repository function FoxHToMeijerG . @Mariusz Iwaniuk simplified it further using Maple: $$\frac{d^{n-1}}{dw^{n-1}}\left.\frac{4^{-\frac{n}{\sqrt w}}}{\sqrt w}\right|_1 =
\frac1{\sqrt\pi}\text G^{3,0}_{1,3}\left(^{\frac32-n}_{0,\frac12,\frac12};(\ln(2)n)^2\right) \\
=\frac{\sqrt\pi}{\Gamma\left(\frac32-n\right)}\,_1\text F_2\left(n-\frac12;\frac12,\frac12;(\ln(2)n)^2\right)\\
+\ln(4)(-1)^n n!\,_1\text F_2\left(n;1,\frac32;(\ln(2)n)^2\right)\tag2$$ which was tested and it matches the derivatives. However, there is seemingly no other way to find this result. MeijerGToHypergeometricPFQ did not work. Also, there is a formula for converting $\text G^{3,0}_{1,3}\left(^{\ \ \ \ \ a_1}_{b_1,b_2,b_3};z\right)$ into a sum of $_1\text F_2$ functions, but part of it involves $\csc(\pi (b_2-b_3))$ which is undefined if $b_3=b_2$ , like in $(1)$ , so $\lim\limits_{b_2\to\frac12}$ must be taken. This problem occurs in other cases where $b_{m+1}=b_{m+j}$ , so understanding how to reduce Meijer G in these cases helps. Is there any way to find $(2)$ without using Maple, like maybe with a Wolfram function or a reduction formula?","['special-functions', 'mathematica', 'limits', 'derivatives', 'hypergeometric-function']"
4889045,Structure of a homogeneous vector bundle on the tangent bundle T(G/B),"Let $G$ be a reductive algebraic group over the field $\mathbb C$ , $B\subset G$ a Borel subgroup and $G/B$ their quotient variety. For every $B$ -module $(M,\rho)$ we can construct the associated $G$ -equivariant vector bundle $G\times^BM\to G/B$ , where $G\times^BM$ is the quotient of $G\times M$ by the relation $(g,m)\sim (gh,h^{-1}\,m)$ . In quite a few papers I read, the tangent bundle $T(G/B)$ is identified with the associated bundle $G\times ^B T_{eB}(G/B)$ , where $B$ acts on $T_{eB}(G/B)$ via the adjoint representation. Why do we have this identification? I tried to split this problem into two different steps: Proof that every homogeneous vector bundle over $G/B$ is of the form $G\times ^BM$ for some $H$ -module $M$ . Show that $T(G/B)$ is homogeneous and that the representation descends to the adjoint representaiton on the tangent space $T_{eB}(G/B)$ . For the first step let $\pi:E\to G/B$ be a homogeneous vector bundle. Since $B$ acts trivially on $G/B$ , it stablizes $M= \pi^{-1}(eB)$ , so we can regard $M$ as a $B$ -module. Now $[(g,m)]\mapsto g\, m$ induces a $B$ -equivariant morphsm of vector bundles $G\times^B M\to E$ over $G/B$ , which is by definition an isomorphism of the fibers. Is this enough to ensure that those bundles are isomorphic, like it is e.g. in topology? For the second step I found in Gagliardi and Hofscheiers paper ""Gorenstein spherical fano varieties"" a $G$ -linearization on the tangent sheaf, which corresponds to a equivariant vector bundle, but since I am quit new to algebraic geometry as a whole, I dont know how to check, if it induces the adjoint action on the tangent space: The pullback of differential forms with respect to the action morphism $\alpha:G\times G/B\to G/B$ , $g\cdot xB=(gx)B$ yields the inverse of a $G$ -linearization of the cotangent sheaf, namely $\hat\alpha^{-1}:\alpha^*\Omega_{G/B}\to \pi^*_{G/B}\Omega_{G/B}$ .
As $G/B$ is smooth, we may dualize $\hat\alpha^{-1}$ and obtain a $G$ -linearization of the tangent sheaf $\mathcal T_{G/B}$ , namely $\hat\beta=(\hat \alpha^{-1})^\vee:\pi^*\mathcal T_{G/B}\to \alpha^*\mathcal T_{G/B}.$ For $U\subset G/B$ open affine, $g\in G$ acts on a local section $\delta\in \mathrm{Der}_\mathbb C (\mathbb C[U],\mathbb C[U])=\Gamma(U,\mathcal T_{G/B})$ by $$g\cdot\delta=\hat\beta|_{g\times G/B}(\delta)=\lambda^\#_g\circ \delta\circ \lambda^\#_{g^{-1}}\in \mathrm{Der}_\mathbb C (\mathbb C[g\cdot U],\mathbb C[g\cdot U]),$$ where $\lambda_g:G/B\to G/B$ is given by $x\mapsto g\cdot x$ .","['algebraic-geometry', 'representation-theory', 'tangent-bundle']"
4889046,Does this type of tensor appear anywhere?,"Antisymmetric, or skew-symmetric tensors on a subset of indices are those that get multiplied by $-1$ when any of the indices from the subset are transposed. This type of tensor is widely used in physics and mathematics. Now imagine I have, say, a tensor of type $(3, 0)$ that gets multiplied by $e^{2\pi i\over3}$ when I apply a cyclic permutation to the indices. More generally, let $h$ be an element of order $k$ in the multiplicative group of the underlying field, and say there is a tensor that gets multiplied by $h$ when a certain permutation of order $k$ is applied to its indices. This property does not seem to depend on the choice of basis. Do such tensors appear anywhere in physics or mathematics? I'm sorry if this question isn't well-motivated, I'm just curious.","['representation-theory', 'tensors', 'linear-algebra']"
4889165,The distribution of the first hitting time for the Constant Elasticity of Variance process.,"The Constant Elasticity of Variance (CEV) process is a one dimensional diffusion process given by the following stochastic differential equation. \begin{equation}
d X_t = \mu X_t \cdot dt + \sigma X_t^\beta \cdot d B_t \tag{1}
\end{equation} where $\mu,\sigma,\beta$ are positive real parameters and $B_t$ is the Brownian motion.
In what follows we assume that the starting value of the process reads $X_0 = x > 0$ and that $\beta > 1 $ .
The infinitesimal generator of this process reads $ {\mathfrak G}_z := \mu z d/d z + \sigma^2/2 z^{2 \beta} d^2/d z^2$ . The eigen-functions of this operator $ {\mathfrak G}_z \phi^{\pm} (z) = \lambda \phi^{\pm}(z) $ to the eigenvalue $\lambda > 0 $ are given below: \begin{eqnarray}
\phi^{+}(z) &=&U\left(\frac{\lambda}{2(-1+\beta) \mu}, 1+ \frac{1}{2(-1+\beta)}, \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2} \right) \tag{2a} \\
\phi^{-}(z) &=& L_{-\frac{\lambda}{2(-1+\beta) \mu}}^{(\frac{1}{2(-1+\beta)})} \left( \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2}\right) \tag{2b} 
\end{eqnarray} where $U$ is the confluent hypergeometric function and $L$ is the generalized Laguerre polynomial. We have checked that $U(z)$ is a strictly increasing function of $z$ . Now, by using the theory of diffusion processes, see section 4.6 pages 128-134 in Ito, K.; McKean, H. P. jun. , Diffusion processes and their sample paths, Berlin-Heidelberg-New York: Springer-Verlag. XVII, 321 p. (1965). ZBL0127.09503 . , we have found the Laplace transform of the first hitting time $\tau_y := inf(s>0:X_s = y)$ of a horizontal barrier $b$ by this process. The quantity in question reads: \begin{eqnarray}
E_x \left[ e^{-\lambda \tau_y} \right] = \frac{\phi^{+}(x)}{\phi^{+}(y)} \quad \mbox{for $x \le y$} \tag{3}
\end{eqnarray} Now, by inverting the Laplace transform in $(3)$ , by using the Bromwich integral and then the Cauchy theorem, we have expressed the probability density function of the first hitting time $n_x(t;y) := P_x\left( \tau_y \in dt\right)/dt $ as follows: \begin{eqnarray}
n_x(t;y)  = \sum\limits_{p=1}^\infty 
\underbrace{
\frac{U\left( -\zeta_p^{(y;\mu,\sigma,\beta)},
              1+ \frac{1}{2(-1+\beta)}, \frac{\mu x^{2-2\beta}}{(-1+\beta) \sigma^2}  \right)}{
\zeta_p^{(y;\mu,\sigma,\beta)}
U^{(1,0,0)}\left( -\zeta_p^{(y;\mu,\sigma,\beta)},
              1+ \frac{1}{2(-1+\beta)}, \frac{\mu y^{2-2\beta}}{(-1+\beta) \sigma^2}  \right)
}
}_{{\mathfrak w}_p^{(y;\mu,\sigma,\beta)}}
\cdot
\underline{(2 (-1+\beta) \mu \zeta_p^{(y;\mu,\sigma,\beta)} ) \cdot e^{-2 (-1+\beta) \mu \cdot \zeta_p^{(y;\mu,\sigma,\beta)} \cdot t}} \tag{5}
\end{eqnarray} As we can see the quantity in $(5)$ is an infinite linear combination of exponential distributions with weights $ \left( {\mathfrak w}_p^{(y;\mu,\sigma,\beta)} \right)_{p=1}^\infty $ that sum up to unity.
Here $ \left( \zeta_p^{(y;\mu,\sigma,\beta)} \right)_{p=1}^\infty $ are zeros of the function $ {\mathbb R}_+ \ni \lambda \rightarrow U(-\lambda,1+ \frac{1}{2(-1+\beta)}, \frac{\mu y^{2-2\beta}}{(-1+\beta) \sigma^2}) \in {\mathbb R} $ . Now we took the following process parameters $\mu,\sigma,\beta = 3/2,1/2,5/2$ and the first value and the barrier $x,y = 3/2, 5/2 $ and we plotted the quantity $(5)$ below. We also verified the normalization numerically. Here we go: {\[Mu], \[Sigma], \[Beta]} = {3/2, 1/2, 5/2};
(*Here x\[LessEqual]y*)
{x, y} = {3/2, 5/2};

SetOptions[FindRoot, WorkingPrecision -> mprec, PrecisionGoal -> prec];
mzeros = \[Lambda] /. 
   Table[FindRoot[
     HypergeometricU[-\[Lambda], 
       1 + 1/(2 (-1 + \[Beta])), (\[Mu] y^(
        2 - 2 \[Beta]))/((-1 + \[Beta]) \[Sigma]^2)] == 0, {\[Lambda],
       n}], {n, 0, 50}];
mzeros = Sort[#[[1]] & /@ Tally[mzeros]];

ts = Array[# &, {300}, {1/100, 3}];
vals = {#, 
     Total[Table[ 
       HypergeometricU[-mzeros[[p]], 
         1 + 1/(2 (-1 + \[Beta])), (\[Mu] x^(
          2 - 2 \[Beta]))/((-1 + \[Beta]) \[Sigma]^2)]/
\!\(\*SuperscriptBox[\(HypergeometricU\), 
TagBox[
RowBox[{""("", 
RowBox[{""1"", "","", ""0"", "","", ""0""}], "")""}],
Derivative],
MultilineFunction->None]\)[-mzeros[[p]], 
         1 + 1/(2 (-1 + \[Beta])), (\[Mu] y^(
          2 - 2 \[Beta]))/((-1 + \[Beta]) \[Sigma]^2)] (2 (-1 + \
\[Beta]) \[Mu]) Exp[-2 (-1 + \[Beta]) \[Mu] mzeros[[p]] #], {p, 1, 
        Length[mzeros]}]]} & /@ ts;
ListPlot[vals, PlotRange :> All, 
 AxesLabel -> {""t"", ""\!\(\*SubscriptBox[\(n\), \(x\)]\)(t,y)""}]
f = Interpolation[vals];
NIntegrate[f[xi], {xi, 0.01, 3}] As you can see the distribution in question has a correct shape and a correct normalization. The negative values close to the origin are due to a numerical error. Having said all this my question would be how do we evaluate the limit of $\beta \rightarrow 1_+$ . In this case the process tends towards the geometric Brownian motion and as such we should have: \begin{equation}
\lim_{\beta \rightarrow 1_+} n_x(t;y) \stackrel{(??)}{=} 
\frac{\left| \log(\frac{y}{x} )\right|}{\sqrt{2 \pi t^3} \sigma} e^{-\frac{1}{2 \sigma^2 t} \left[ \log(\frac{y}{x} - (\mu - \frac{\sigma^2}{2} ) t\right]^2} 
\end{equation} as shown in a previous question on a similar topic . How do we work out this limit analytically in our framework? Update: We have verified numerically that the Laplace transform $ (3) $ approaches the correct limits when $ \beta \rightarrow 1_+ $ . See code below: {lmb, mu, sig} = RandomReal[{0, 1}, 3, WorkingPrecision -> 50];
    x = RandomReal[{0, 1}, WorkingPrecision -> 50];
    y = RandomReal[{x, 2}, WorkingPrecision -> 50];
    NN = 100;
    HypergeometricU[lmb/mu NN, NN, NN (2 mu/sig^2) x^(-1/NN)]/
     HypergeometricU[lmb/mu NN, NN, NN (2 mu/sig^2) y^(-1/NN)]
    (x/y)^((-2 mu + sig^2 + 2 Sqrt[2 lmb sig^2 + (mu - sig^2/2)^2])/(
      2 sig^2))

0.275584165705622319278498109409236772998453 + 0.*10^-50 I

0.2729011283963510407220223125184479965801097988605","['inverse-laplace', 'ordinary-differential-equations', 'stochastic-processes', 'stopping-times', 'hypergeometric-function']"
4889188,Exhaustion by compact sets eventually include fixed compact set.,"Suppose that $\Omega \subseteq \mathbb R^n$ non-empty, open, and $n \ge 1$ . We have an increasing collection $(K_i)_{i \in \mathbb N}$ of compact subsets of $\Omega$ with union $\Omega$ . For $K \subset \Omega$ fixed and compact, is it necessarily the case that $K \subseteq K_i$ for $i$ sufficiently large? I know that this is true when we have the constraint that $K_i \subseteq \mathrm{interior}(K_{i+1})$ for each $i \in \mathbb N$ . I am not convinced that this will be the case without this constraint, but I have had some trouble cooking up a counterexample. I think one could arise if we took $K$ to have non-empty interior and $K_i$ to have empty interior for each $i$ , by taking some pathological choice of $K_i$ .","['general-topology', 'set-theory', 'compactness']"
4889220,Solving $\ge 2$ congruences by CRT = Chinese Remainder Theorem,"How do I get the solution given by CRT to match another solution, e.g. the least positive? For example say I have X = 1234 . I choose m i as 5, 7, 11, 13 . This satisfies the simple requirements of Mignotte's threshold secret sharing scheme . More precisely given in my example k = n = 4 , and the product of any k - 1 is smaller then X how come simply computing the remainder of each won't give equations that solve to X = 1234 . In the case of the example, x = 4 mod 5
x = 2 mod 7
x = 2 mod 11
x = 12 mod 13 Which resolves to 31264 (won't CRT produce the smallest?) Any hints? Below is the solution linked to on ""Math Celebrity"" (cached to avoid link rot).","['elementary-number-theory', 'chinese-remainder-theorem', 'modular-arithmetic']"
4889238,What was the gap in Ariane Papke's proof that the minimum number of sudoku clues is 17?,"I was reading McGuire's paper on why the minimum number of clues in a Sudoku puzzle is 17 when I came across a curious comment: In 2008, a 17-year-old girl submitted a proof of the nonexistence of a 16-clue sudoku puzzle as
an entry to Jugend forscht (the German national science competition for high-school students).
She later published her work in the journal Junge Wissenschaft (No. 84, pp. 24–31). However,
when Sascha Kurz, a mathematician at the University of Bayreuth, Germany, studied the proof
closely, he found a gap that is probably very difficult, if not impossible, to fix. I was able to find the work he was referring to (I think) here . Not knowing German or anyone who speaks German, I had to settle and read a machine translation of the paper, which wasn't very good. By my understanding, the proof went along the lines of this: We start out by expanding the grid into 3D space with a $9$ by $9$ by $9$ cube, and for each square of the puzzle, we place a 1 in the n'th cube from the front and zeros everywhere else behind the square. So for example if we have a 5 in a square of the puzzle, all cubes behind that specific square of the puzzle will contain the number zero except the 5th one, which will contain a 5. We can then create sets of equations with variables which represent the values inside the cubes, and these equations represent the different constraints of Sudoku. Finally, we consider an ""optimal"" configuration of 16 clues which will eliminate as many variables as possible and we find that there are not enough equations to solve for every variable, meaning that a solution to the puzzle wouldn't be unique if it had 16 clues. With all of this being said, I can't seem to find the gap which McGuire mentions. To me, the idea of finding an ""optimal"" configuration does seem a little bit hand wavy, but at the same time the logic does seem sound. I've looked around to see if Sascha Kurz had published something about the matter, but I can't seem to find anything and not knowing German only worsens my predicament. Is the gap something much more subtle, for example a simple miscount, or is it something else? What is the gap in Papke's proof?","['sudoku', 'combinatorics', 'integer-programming']"
4889246,Is this recursive function always surjective? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 months ago . The community reviewed whether to reopen this question 3 months ago and left it closed: Original close reason(s) were not resolved Improve this question Let $S:\mathbb{N}\to\mathbb{Z}^+$ satisfy $$
\left\{
\begin{aligned} 
S(n) &= S(A^n \bmod n) + S(B^n \bmod n) \\
S(0) &= 1
\end{aligned}
\right.
$$ My question is, is this function always surjective for all values of $A, B \geq 2$ with $A \neq B$ ? (When $A$ 's and $B$ 's set of prime factor are completely the same, odd numbers are unobtainable. Is this the only case where surjectivity fails?)","['number-theory', 'recurrence-relations', 'modular-arithmetic', 'elementary-number-theory']"
4889279,Coefficients of the Inverse Modified Bessel Function,"I am curious if there is a closed form that represents the coefficients of the inverse of the modified Bessel function of the first kind $I_{0}(x)$ . I can find the series representation using InverseSeries[Series[BesselI[0,x],{x,0,20}]] in Mathematica. $$I^{-1}_{0}(x)=\sum^{\infty}_{n=1}{(-1)^{n+1}a_{n}(x-1)^{n-\frac{1}{2}}}$$ The first few coefficients $a_{n}$ are: $2$ , $\frac{1}{4}$ , $\frac{47}{576}$ , $\frac{161}{4608}$ , $\frac{565571}{33177600}$ , $...$ Or the series can be expressed as: $$I^{-1}_{0}(x)=2\sqrt{x-1}\sum^{\infty}_{n=0}{(-1)^{n}b_{n}(x-1)^{n}}$$ In this case, the coefficients would just be divided by two: $1$ , $\frac{1}{8}$ , $\frac{47}{1152}$ , $\frac{161}{9216}$ , $\frac{565571}{66355200}$ , $...$ Other than the coefficients, the inverse Bessel function is very similar to the $\cosh^{-1}(x)$ series. If there is no closed form, or neat closed form for either of the coefficients $a_{n}$ or $b_{n}$ , is there a better way, or more condensed way to represent these sums? Update: I took care of the denominator. $$I^{-1}_{0}(x)=\sum^{\infty}_{n=0}\,{\frac{(-1)^{n}c_{n}(x-1)^{n-\frac{1}{2}}}{\Gamma^{2}({n+1})\left(2^{(n+1)}(n+1)\right)^{2}}}$$ All I have left is $c_{n}=8,16,752,185472,144786176,…$ The only way I can meaningfully simplify this sequence is by realizing that all these terms are divisible by 8. Is it possible to put these coefficients in a closed-form?","['inverse-function', 'functions', 'combinatorics', 'discrete-mathematics', 'sequences-and-series']"
4889281,Strict chromatic vector coloring of $K_n$,"In a finite simple graph $X$ , for any $t\in\mathbb{R}$ , a vector $t$ -coloring of $G$ is a mapping $\phi_t: V(X)\longrightarrow S^m$ for some $m\in\mathbb{N}$ (where $S^m$ is the $m$ -sphere in $\mathbb{R}^{m+1}$ ) such that for any $x, y\in V(X)$ , $\langle\, \phi_t(x) \,,\, \phi_t(y) \,\rangle \leq -\dfrac{1}{t-1}$ whenever $x\sim y$ . The vector chromatic number of $G$ is the infimum among all real numbers $t\in\mathbb{R}$ such that $G$ has a vector $t$ -coloring. The definition and more details can be found in this link . Further, for any $t\in \mathbb{R}$ , a strict vector $t$ -coloring is a mapping $\psi_t:V(X) \longrightarrow S^m$ for some $m\in\mathbb{N}$ such that $\langle\, \psi_t(x) \,,\, \psi_t(y) \,\rangle = -\dfrac{1}{t-1}$ , and the strict vector chromatic number $\chi_{sv}(G)$ is defined similarly. Clearly $\chi_v(G)\leq \chi_{sv}(G)$ and in the link above it is proved that $\omega(G)$ the max clique number of $G$ is less than or equal to $\chi_v(G)$ . My question is that how to show $\chi_{sv}(K_n)\leq n$ ?","['graph-theory', 'inner-products', 'linear-algebra', 'discrete-mathematics']"
4889283,On the p-th coefficients of the $\eta(\tau)\eta(23\tau)$,"I am trying to prove the the well-known result: $a_{p}=0$ if $\left( \frac{p}{23}  \right)=-1$ $a_{p}=2 $ if $\left( \frac{p}{23}  \right)=1$ and $p$ is represented by $x^2+xy+6y^2$ $a_{p}=-1 $ if $\left( \frac{p}{23} \right)=1$ and $p$ is represented by $ 2x^2+xy+3y^2 $ where $a_{p}$ is the coefficient of $q^{p}$ in $\eta(\tau)\eta(23\tau)=q\prod_{k=1}^{\infty}(1-q^k)(1-q^{23k})
$ I know that $N_{p}(f)=a_{p}+1$ where $N_{p}$ is the number of roots of the polynomial $f(x)=x^3-x-1 \in \mathbb{F}_p[x]$ , but I am trying to compute $ a_{p}$ from the $q$ -expansion. Here is my attempt. Using the Euler pentagonal number theorem, we have $$\eta(\tau)\eta(23\tau)=\prod_{k=1}^{\infty}(1-q^k)(1-q^{23k})= $$ $$\sum_{x,y \in \mathbb{Z}} (-1)^{n+m} q^{(3n^2+n+69m^2+23m+2)/2}
=  $$ $$ \sum_{x,y \in \mathbb{Z}} (-1)^{n+m} q^{((6m+1)^2+23(6n+1)^2)/24}.
$$ It remains to determine when p is represented by the quadratic form $Q(x,y)=x^2+23y^2$ . We know that p must verify $ \left( \frac{p}{23}\right)=1$ , but I could not go any further.","['number-theory', 'elementary-number-theory', 'quadratic-forms']"
4889336,Can't seem to get the correct result by diagonalizing a matrix.,"I am trying to understand parts of the authors solution given to the following question: The generators of $\mathrm{SO}(3)$ can be chosen as $t^1=\begin{pmatrix}0 & 0 & 0\\ 0 & 0 & -i \\\ 0 & i & 0 \\ \end{pmatrix},\ t^2=\begin{pmatrix}0 & 0 & i\\ 0 & 0 & 0 \\ -i & 0 & 0 \\ \end{pmatrix},\ t^3=  
\begin{pmatrix}0 & -i & 0\\ i & 0 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}.$ Diagolizing $t^3$ , find the $\mathrm{SO}(3)$ group element $\mathrm{R}(\theta)=\exp(-i\theta t^3)$ . The author's solution is: To diagonalize $t^3$ , we first have to find its eigenvalues by solving $\det(t^3-\mathbb{I}\lambda)=\det\begin{pmatrix}-\lambda & -i & 0\\ i & -\lambda & 0 \\ 0 & 0 & -\lambda\\\end{pmatrix}=(\lambda^2-1)\times(-\lambda)=0.$ The eigenvalues are therefore $\lambda=0$ and $\lambda=\pm 1$ . The eigenvector corresponding to $\lambda=0$ is clearly $(0,0,1)^T$ and the eigenvectors corresponding to $\lambda=\pm 1$ are found by solving $\begin{pmatrix}0 & -i & 0\\ i & 0 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}a\\ b \\ 0\\ \end{pmatrix}=\begin{pmatrix}-ib\\ ia \\ 0\\ \end{pmatrix}=\pm \begin{pmatrix}a\\ b \\ 0\\ \end{pmatrix}\tag{1}$ which gives $\frac{1}{\sqrt{2}}\left(1,\pm i,0\right)\tag{2}.$ Therefore we can write $$t^3=U^\dagger \hat t U\tag{3}$$ where $U=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i & 0\\ 1 & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix},\tag{4}$ . $\hat t=\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\tag{5}$ I don't need to type any more of the solution as I don't understand eqns. $(2)-(4)$ Should eqn. $(2)$ actually read $\frac{1}{\sqrt{2}}\left(1,\pm i,0\right)^\dagger=\frac{1}{\sqrt{2}}\begin{pmatrix}1\\ \mp i \\ 0\\ \end{pmatrix}, \, \text{for} \ \lambda=\pm 1?$ If true then $t^3$ has eigenvectors $\frac{1}{\sqrt{2}}\begin{pmatrix}1\\ -i \\ 0\\ \end{pmatrix} \text{if}\,\,\lambda = +1\,\quad\ \frac{1}{\sqrt{2}}\begin{pmatrix}1\\ i \\ 0\\ \end{pmatrix} \text{if}\,\, \lambda = -1,\quad\ \frac{1}{\sqrt{2}}\begin{pmatrix}0\\ 0 \\ \sqrt{2}\\ \end{pmatrix} \text{if}\, \lambda = 0\tag{a}$ Then writing the matrix of eigenvectors, $U$ , with the eigenvectors of $(\mathrm{a})$ as columns in the same order as the eigenvalues in $(5)$ should yield $U=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1 & 0\\ -i & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}=\begin{pmatrix}1/\sqrt2 & 1/\sqrt2 & 0\\ -i/\sqrt2 & i/\sqrt2 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix}$ which is not the same as eqn. $(4)$ . In fact, this is the transpose of eqn. $(4)$ . For eqn. $(3)$ , $\det U = i$ , $U$ is unitary (as the rows are orthonormal), so $U^\dagger=U^{-1}$ . In order to find the inverse of $U$ , I first take its transpose $U^T=\begin{pmatrix}1/\sqrt2 & -i/\sqrt2 & 0\\ 1/\sqrt2 & i/\sqrt2 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix}\tag{b}$ then replacing each element of $(\mathrm{b})$ with its cofactor with the associated signature $\begin{pmatrix}+ & - & +\\ - & + & - \\\ + & - & + \\ \end{pmatrix}$ .   I find that $U^{-1}=\frac{1}{\det U}\begin{pmatrix}+\begin{vmatrix}i/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & -\begin{vmatrix}1/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & 0\\ -\begin{vmatrix}-i/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & +\begin{vmatrix}1/\sqrt2 & 0\\ 0 & 1 \\ \end{vmatrix} & 0 \\ 0 & 0 & +\begin{vmatrix}1/\sqrt2 & -i/\sqrt2\\ 1/\sqrt2 & i/\sqrt2 \\ \end{vmatrix} \\ \end{pmatrix}$ . $=-i\begin{pmatrix}i/\sqrt2 & -1/\sqrt2 & 0\\ i/\sqrt2 & 1/\sqrt2 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix}=\frac{1}{\sqrt2}\begin{pmatrix}1 & i & 0\\ 1 & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix}$ . Putting this altogether and omitting the calculation details , $U^\dagger \hat t U=\frac12\begin{pmatrix}1 & i & 0\\ 1 & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix}\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}1 & 1 & 0\\ -i & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}$$$$=\begin{pmatrix}0 & 1 & 0\\ 1 & 0 & 0 \\\ 0 & 0 & 0 \\ \end{pmatrix}\ne t^3\tag{c}$ . But using the version for $U$ as given in eqn. $(4)$ of the authors solution and again omitting the calculation details , $U^\dagger \hat t U=\frac12\begin{pmatrix}1 & 1 & 0\\ i & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix}\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}1 & -i & 0\\ 1 & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}$$$$=\begin{pmatrix}0 & -i & 0\\ i & 0 & 0 \\\ 0 & 0 & 0 \\ \end{pmatrix} = t^3\tag{d}$ Interestingly, if I write (with the omission of the calculation details ), $U \hat t U^\dagger=\frac12\begin{pmatrix}1 & 1 & 0\\ -i & i & 0 \\ 0 & 0 & \sqrt{2} \\ \end{pmatrix}\begin{pmatrix}+1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}\begin{pmatrix}1 & i & 0\\ 1 & -i & 0 \\ 0 & 0 & \sqrt2 \\ \end{pmatrix}$$$$=\begin{pmatrix}0 & i & 0\\ -i & 0 & 0 \\\ 0 & 0 & 0 \\ \end{pmatrix}=-t^3\tag{e}$ I've stared at this for sometime now but I just cannot understand why the author's solution, $(\mathrm{d})$ , gives the correct result, but my attempt in eqn. $(\mathrm{c})$ $(\text{or}\, (\mathrm{e}))$ does not. Can someone please explain where I am going wrong here?
(Sorry for the lengthy post, I've trying to regain some linear algebra skills so needed to show a lot of working).","['eigenvalues-eigenvectors', 'proof-explanation', 'matrices', 'linear-algebra', 'diagonalization']"
4889346,N perfect Logicians box opening game,"Question: There is a game that involves $n$ ordered boxes each with a hidden value associated with it. The value is sampled from a probability distribution density function $P(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2}$ . You observe each box’s value in order from box 1 to $n$ . After observing any given box’s value, you must decide to pick it or leave it, forever discarding the option to choose its value again. A player wins this game if and only if they pick the box with the highest sampled value. There is a group of perfect Logicians $L_1, L_2, \ldots, L_m$ competing in the game. The only information they know is that they are all perfect logicians, the rules of the game, and if any other logicians have lost or won the game. They play the game in order starting with $L_1$ . The game resets for each logician, but the random sampling is the same. a) Prove that if $m > n$ , a win would always occur b) We find out that only $L_m$ wins the game. What is the winning box? Give your answer in terms of $n, m$ . Thoughts so far: Initially, I was confused about how any given logician is influenced by the results of previous logicians since they can't see the outcome of their decisions, only whether they won or lost. My best guess is that, for instance, if Logician 2 knows Logician 1 lost, then Logician 2 can infer that the optimal choice Logician 1 would have made led to failure. Therefore, by the time Logician 2 encounters the box that Logician 1 would have chosen using the optimal strategy, he would know that this particular box was not the winning choice. Additionally, logicians can observe the number of failures preceding their turn, although I’m not sure how this would impact their decision-making process. For part a, my intuition tells me that logicians can always deduce the choices made by their predecessors because the outcome is deterministic, regardless of the random seed. Since each logician knows the choices of the previous logicians, once the number of logicians $m$ equals the number of boxes $n$ , they would, by the pigeonhole principle, be guaranteed to find the solution. For part b, I'm uncertain how the distribution of random sampling affects the optimal stopping problem. Is there a more optimal strategy when the distribution is known? Furthermore, once an optimal strategy is discovered, how does this information benefit subsequent logicians in their turn order? Edit: The logicians are not working together nor have they discussed a strategy beforehand together. They wish only to maximize their own chances of winning","['game-theory', 'statistics', 'logic']"
4889494,How to prove this integral inequality $\int_{0}^{1}[f''(x)]^2dx\ge 12$?,"Let $f\in C^{2}[0,1]$ such that $f(0)=0,f(1/2)=f(1)=1$ . Show that $$\int_{0}^{1}[f''(x)]^2dx\ge 12.$$ My idea: try to find a function $f(x)$ such that the equality holds and use Cauchy-Schwarz. So I tried some ""good"" functions to achieve the minimum value. Assume $f(x)=ax^3+bx^2+cx$ is a polynomial. Then I find that when $f(x)=-2x^2+3x$ , the integral $\int_{0}^{1}[f''(x)]^2dx$ has minimum value $16$ . It seems it doesn't help to solve the problem. Since $f''(x)=(f(x)-ax-b)''$ and replace $f(x)$ by $f(x)-x$ , we can change the condition to $f(0)=f(1)=0, f(1/2)=1/2$ . I find that $f(x)=\frac{1}{2}\sin(\pi x)$ satisfies the condition and $\int_{0}^{1}[f''(x)]^2dx=\frac{\pi^2}{8}=12.1761...$ which is very closed to $12$ . But it still doesn't help. It seems that it isn't easy to find such $f(x)$ such that the equality holds. So how to prove this inequality? And what is the minumum value of this integral? May be not 12? Thank you!","['integration', 'inequality', 'integral-inequality', 'real-analysis']"
4889551,Examining why the proof for the intersection of (finite) collections of open sets being open in a metric space does not extend to infinite collections,"In a metric space, we have that the intersection of countably many open sets is open: Let $A_1, A_2, \ldots, A_n$ be open sets and $A = \bigcap_{i=1}^n A_i$ . For $x \in A$ , $x \in A_i$ for all $i$ , and since each $A_i$ is open, $\exists r_i > 0$ with $B_{r_i}(x) \subseteq A_i$ . Set $r = \min\{r_1, r_2, \ldots, r_n\}$ , then $B_r(x) \subseteq A$ . Since $x$ was arbitrary, $A$ is open. I am trying to understand why this proof, does not hold for uncountably infinite sets and I believe the problem lies in this step: $r = \min\{r_1, r_2, \ldots, r_n\}$ I think this because: For any collection of elements of any space that I am considering the intersection of, there is a associated $r_i$ with each element. Then since we are considering a metric space $\{r_1, r_2, \ldots, r_i, \ldots\} \subset \mathbb{R}$ . Then it is not necessarily true that there exists a minimum $r_i$ . As say $\{r_1, r_2, \ldots, r_i, \ldots\}$ was the set $(0,1)$ . I am aware that I can consider the many counterexamples for the the uncountable union of open sets, but I want to identify, which will help me examine when the future proofs I do hold, where the proof given falters when I try and make a more general statement. If someone could point out if the reasoning I provided is the correct way to think about things, that would be great!","['general-topology', 'analysis', 'real-analysis']"
4889560,Does this show $\bar{Y}$ is a sufficient statistic?,"Question If $Y_1,\dots, Y_n \sim \; \textrm{iid geometric(p)}$ , show that $\bar{Y}$ is a sufficient statistic for p. My work Factorization Theorem If $Y_1, \dots, Y_n \sim \; \textrm{iid}$ then U is a sufficient statistic if $$
L(\theta) = g(U,\theta)h(Y_1,\dots,Y_n)
$$ My actual work The likelihood function for $Y_1, \dots, Y_n$ is \begin{align}
L(p) &= \prod_{i=1}^n{f_{Y_i}(y)} = \begin{bmatrix} p(1-p)^{y_i-1} \end{bmatrix}^n \notag \\
&= p^{n}(1-p)^{\sum_{i=1}^n{(y_i - 1)}} \notag \\
&= p^{n}(1-p)^{(n\bar{Y} - n)} = p^n(1-p)^{n(\bar{Y} - 1)} \notag
\end{align} Applying the above factorization theorem gives us \begin{align}
&g(\bar{Y},p) = p^n(1-p)^{n(\bar{Y} - 1)} \notag \\
&h(Y_1, \dots, Y_n) = 1 \notag
\end{align} Therefore, we conclude that $\bar{Y}$ is a sufficient statistic for p.","['statistics', 'probability-distributions', 'probability', 'sufficient-statistics']"
4889630,Explanation of the proof of Theorem 6.6 in Rudin's Functional Analysis,"Everything that follows is from Rudin's Functional Analysis : $\def\L{\Lambda} \def\DDD{\mathcal{D}} \def\sbe{\subseteq} \def\W{\Omega} \def\RR{\mathbb{R}} \def\CC{\mathbb{C}} $ Below $\DDD$ is the space of test (smooth, compactly supported) functions from an open set $\Omega\sbe\RR^n$ to $\CC$ . Such space is given a complete, unmetrizable topology $\tau$ . Similarly $\DDD_K$ is the space of test functions $\Omega\to\CC$ whose support lies in the compact set $K$ . Each $\DDD_K$ has a Fréchet space topology $\tau_K$ which corresponds with the subspace topology under $\tau$ . Theorem 6.6: suppose $\L$ is a linear mapping of $\DDD$ into a lctvs $Y$ . Then the following are equivalent: a) $\L$ is continuous. b) $\L$ is bounded. c) If $\phi_i\to 0$ in $\DDD(\W)$ , then $\L\phi_i\to0$ in $Y$ . d) The restriction of $\L$ to any $\DDD_K\sbe\DDD(\W)$ is continuous. The proof shows a) $\implies$ b) $\implies$ c) $\implies$ d) $\implies$ a). I struggle to understand the steps b) $\implies$ c) and d) $\implies$ a): b) $\implies$ c): there is a compact subset $K\sbe\W$ that contains the support of every $\phi_i$ , thus the $\phi_i$ are members of the subset $\DDD_K$ and $\phi_i\to 0$ holds in $\DDD_K$ . The restriction of $\L$ to this $\DDD_K$ is bounded (why?) and since $\DDD_K$ is metrizable it follows that $\L\phi_i\to 0$ in $Y$ . d) $\implies$ a): let $U$ be a convex balanced neighborhood of $0$ in $Y$ , and put $V=\L^{-1}(U)$ . Then $V$ is convex and balanced (why?) . Now $V$ is open if and only if $\DDD_K\cap V\in\tau_K$ for every compact $K\sbe \W$ (I understand the only if implication only) . This proves the equivalence of a) and d) (how?) .","['proof-explanation', 'functional-analysis', 'analysis', 'distribution-theory']"
4889706,Question About The Remark after Proposition 1.4.11 from Measure Theory by Donold Cohn,"My Question Define subsets $G$ , $G_0$ , and $G_1$ of $\mathbb{R}$ by \begin{align*}
    G &= \{x:x=r+n\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\},\\
    G_0 &= \{x:x=r+2n\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\},\ \text{and}\\
    G_1 &= \{x:x=r+(2n+1)\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\}.
\end{align*} Define a relation $\sim$ on $\mathbb{R}$ by letting $x \sim y$ hold when $x-y\in G$ ; the relation $\sim$ is then an equivalence relation on $\mathbb{R}$ . Use the axiom of choice to form a subset $E$ of $\mathbb{R}$ that contains exactly one representative of each equivalence class of $\sim$ . Let $A = E + G_0$ (that is, let $A$ consist of the points that have the form $e+g_0$ for some $e$ in $E$ and some $g_0$ in $G_0$ ). I got confused by the book's remark that ""the set $A$ defined above is not Lebesgue measurable: if it were, then both $A$ and $A^c$ would include (in fact, would be) Lebesgue measurable sets of positive Lebesgue measure"". Could someone please help me explain why this is true? Background Information The above remark is made after the follow proposition: Proposition 1.4.11 $\quad$ There is a subset $A$ of $\mathbb{R}$ such that each Lebesgue measurable set that is included in $A$ or in $A^c$ has Lebesgue measure zero. Proof $\quad$ Define subsets $G$ , $G_0$ , and $G_1$ of $\mathbb{R}$ by \begin{align*}
    G &= \{x:x=r+n\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\},\\
    G_0 &= \{x:x=r+2n\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\},\ \text{and}\\
    G_1 &= \{x:x=r+(2n+1)\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\}.
\end{align*} One can prove that $G$ and $G_0$ are subgroups of $\mathbb{R}$ (under addition), and $G_0$ and $G_1$ are disjoint, that $G_1 = G_0 + \sqrt{2}$ , and that $G = G_0 \bigcup G_1$ . Define a relation $\sim$ on $\mathbb{R}$ by letting $x \sim y$ hold when $x-y\in G$ ; the relation $\sim$ is then an equivalence relation on $\mathbb{R}$ . Use the axiom of choice to form a subset $E$ of $\mathbb{R}$ that contains exactly one representative of each equivalence class of $\sim$ . Let $A = E + G_0$ (that is, let $A$ consist of the points that have the form $e+g_0$ for some $e$ in $E$ and some $g_0$ in $G_0$ ). We now show that there does note exist a Lebesgue measurable subset $B$ of $A$ such that $\lambda(B)>0$ . For this let us assume that such a set exists; we will derive a contradiction. Proposition 1.4.10 implies that there is an interval $(-\epsilon,\epsilon)$ that is included in $\text{diff}(B)$ and hence in $\text{diff}(A)$ . Since $G_1$ is dense in $\mathbb{R}$ , it meets the interval $(-\epsilon,\epsilon)$ and hence meets $\text{diff}(A)$ . This, however, is impossible, since each element of $\text{diff}(A)$ is of the form $e_1-e_2+g_0$ (where $e_1$ and $e_2$ belong to $E$ and $g_0$ belongs to $G_0$ ) and so cannot belong to $G_1$ (the relation $e_1-e_2+g_0=g_1$ would imply that $e_1=e_2$ and $g_0=g_1$ , contradicting the disjointness of $G_0$ and $G_1$ ). This completes our proof that every Lebesgue measurable subset of $A$ must have Lebesgue measure zero. One can check that $A^c = E + G_1$ and hence that $A^c = A + \sqrt{2}$ . It follows that each Lebesgue measurable subset of $A^c$ is of the form $B+\sqrt{2}$ for some Lebesgue measurable subset $B$ of $A$ . Since $A$ has no Lebesgue measurable subsets of positive measure, it follows that $A^c$ also has no such subsets, and with this the proof is complete. The definition of $\text{diff}$ is the following: Definition $\quad$ Let $A$ be a subset of $\mathbb{R}$ . Then $\text{diff}(A)$ is the subset of $\mathbb{R}$ defined by \begin{align*}
\text{diff}(A) = \{x-y:x \in A\ \text{and}\ y \in A\}.
\end{align*} Proposition 1.4.10 is the following: Proposition 1.4.10 $\quad$ Let $A$ be a Lebesgue measurable subset of $\mathbb{R}$ such that $\lambda(A) > 0$ . Then $\text{diff}(A)$ includes an open interval that contains 0. Any help will be really appreciated!","['measure-theory', 'lebesgue-measure', 'outer-measure', 'analysis', 'real-analysis']"
4889719,"Criteria for the maximum of $f(x) = axe^{bx}$ at $(2,1)$","I have this question and have been trying for a long time to fully solve it, hopefully you guys can help me. Here is the question: Consider the function $f(x) = axe^{bx}$ , where $a$ and $b$ are constants and real numbers. Find the possible value(s) of $a$ and $b$ such that $f(x)$ has an absolute maximum of $(2, 1)$ on the interval $[0, 50]$ . Give your answer in exact form. I got one answer, but can't seem to prove or find out other possible answers. Here is my solution for one possible answer: $f(x) = axe^{bx}$ $1=f(2) =2ae^{2b}$ $ae^{2b}=\frac12$ $f'(x) = ae^{bx}(1+xb)$ $0=f'(2) = ae^{2b}(1+2b)=\frac12(1+2b)$ $b = -\frac12$ $ae^{2(-1/2)}=\frac12$ $a =\frac e2$ . So one of my possible answers is $a=\frac e2$ and $b=-\frac12$ .
Now my only problem is the question asked for the possible value(s), so I don't know if there is more than one possible answer. Then question did specify on the interval from $[0,50]$ . So I don't know if there is a function that is greater in the negative, but has a maximum on the interval $[0,50]$ on the positive end, or sort of a U-shaped function where it has a maximum at $(2,1)$ goes below, but then possible goes back up after $x=50$ , which would still satisfy the criteria. Just looking for some help if I got the only answer, or if there are more answers. *Note: the question did say that $a$ and $b$ are constants and real numbers.","['maxima-minima', 'calculus', 'derivatives']"
4889746,Applying linearity of a differential operator to solve ODE,"I'm attempting to solve the following ODE: $$xy''+y'-y=0$$ According to Frobenius' theorem, there exists a solution to this ODE in the form of a series given by the linear combination of two solutions such that: $y=C_1y_1+C_2y_2$ , where: $$y_1=\displaystyle \sum_{n=0}^\infty a_nx^n$$ $$y_2=y_1log|x|+\displaystyle \sum_{n=0}^\infty b_nx^n$$ My issue is not how to solve the ODE, I've already managed to do it, but rather how to find the second solution, $y_2$ , in a more elegant way instead of deriving the expression twice, substituting the series in the equation and identifying a pattern for the general term of the sum. In order to do this, following my professor's advice, I have defined the differential operator: $$L=x\frac{d^2}{dx^2}+\frac{d}{dx}-1$$ where, of course, $L[y_1]=L[y_2]=0$ since they are both solutions to my equation. Applying this operator on the second solution, I get: $$L[y_2]=L[y_1]log|x|+y_1L[log|x|]+L[u_2]=0$$ Since the first term is null, then i end up with: $$y_1L[log|x|]+L[u_2]=0$$ and from here I get: $$-y_1+L[u_2]=0$$ although, according to the calculations I did earlier, before using operators, what I should really obtain is: $$2y_1'+L[u_2]=0$$ Why is this? I'm assuming it is because I'm considering my operator is linear when it really isn't, although I don't quite understand why it wouldn't be, since it fullfils the basic properties of linearity.","['operator-theory', 'taylor-expansion', 'ordinary-differential-equations', 'sequences-and-series']"
4889784,Product of incircle and excircles,"How can I obtain a polynomial formula for the product (i.e. union) of a triangle's incircle and excircles , avoiding the use of radicals? Assume three lines in general position are given by three equations $$a_ix+b_iy+c_i=0\quad\text{for }i\in\{1,2,3\}$$ A circle with center $(p,q)$ and radius $r$ may be given by the equation $$(x-p)^2+(y-q)^2-r^2=0$$ which can also be written as $$(x,y,1)\begin{pmatrix}1&0&-p\\0&1&-q\\-p&-q&p^2+q^2-r^2\end{pmatrix}
\begin{pmatrix}x\\y\\1\end{pmatrix}=0$$ Using the adjugate matrix to switch from primal to dual conic, we can say a line is tangent to that circle if it satisfies $$(a,b,c)\begin{pmatrix}
p^2-r^2 & pq & p \\
pq & q^2-r^2 & q \\
p & q & 1
\end{pmatrix}
\begin{pmatrix}a\\b\\c\end{pmatrix}=0$$ So by plugging in the three lines from the start, we get three non-linear equations. In general we get 4 distinct solutions for $p,q,r^2$ . (We actually get 8 solutions if we solve for $r$ instead of $r^2$ , but these are only $r=\pm\sqrt{r^2}$ and by convention we'd pick the positive radius. All the formulas only use $r^2$ not $r$ so it makes sense to treat that as the variable.) These 4 solutions correspond to the incircle and the three excircles of the triangle formed by the lines. The underlying quartic equation behind this would introduce plenty of radicals, and rules on how to match the solutions, which complicates subsequent work. It would be much better if we could avoid taking roots here. And I think that by dealing with all four circles together, that should be possible. Specifically I'm looking for the product of the four circle equations, which corresponds to an algebraic curve of degree 8 that represents the union of the four circles. I conjecture that the product of the four circles, i.e. the formula $$0=\prod_{i=1}^4 (x-p_i)^2+(y-q_i)^2-r_i^2$$ can be stated as a polynomial of combined degree 8 in $x,y$ where the coefficients themselves can be given as polynomials in the coordinates of the lines, namely $a_i,b_i,c_i$ . If the coordinates of the lines are rational, then the coefficients in the product of circles will be rational, too. I have checked the last part of this conjecture, the rationality of coefficients, using one specific triangle, chosen fairly arbitrarily (using three rational points on the unit circle): \begin{align*}
a_1 &= 23 & b_1 &= 41 & c_1 &= -47 \\
a_2 &= 4 & b_2 &= -7 & c_2 &= 4 \\
a_3 &= 3 & b_3 &= -5 & c_3 &= 3
\end{align*} For this I got four circles characterized by \begin{align*}
244205p^4 - 366418p^2 + 243984p - 45648 &= 0 \\
244205q^4 - 1098812q^2 + 1268540q - 411845 &= 0 \\
59636082025r^8 - 598843408280r^6 + 529183150574r^4 - 356490680r^2 + 60025 &= 0
\end{align*} Matching the correct roots of these polynomials to get consistent solutions: \begin{align*}
p_1 &\approx \phantom+0.47089 & q_1 &\approx \phantom+0.86139 & r_1^2 &\approx 0.00032872 \\
p_2 &\approx \phantom+0.50761 & q_2 &\approx \phantom+0.88289 & r_2^2 &\approx 0.00034533 \\
p_3 &\approx -1.49989 & q_3 &\approx \phantom+0.85359 & r_3^2 &\approx 0.97839603 \\
p_4 &\approx \phantom+0.52138 & q_4 &\approx -2.59788 & r_4^2 &\approx 9.06255888
\end{align*} The product of these four circles, scaled to avoid divisions, is then the following: \begin{align*}
59636082025&\,x^8 \\
+ 238544328100&\,x^6y^2 \\
+ 357816492150&\,x^4y^4 \\
+ 238544328100&\,x^2y^6 \\
+ 59636082025&\,y^8 \\
- 241134854740&\,x^6 \\
+ 424846368960&\,x^5y \\
- 1438821671300&\,x^4y^2 \\
+ 849692737920&\,x^3y^3 \\
- 2154238778380&\,x^2y^4 \\
+ 424846368960&\,xy^5 \\
- 956551961820&\,y^6 \\
+ 108802118880&\,x^5 \\
+ 265528980600&\,x^4y \\
- 1771920221760&\,x^3y^2 \\
+ 3788213456560&\,x^2y^3 \\
- 1880722340640&\,xy^4 \\
+ 3522684475960&\,y^5 \\
- 12729722876&\,x^4 \\
+ 1691695948800&\,x^3y \\
- 2241047404232&\,x^2y^2 \\
+ 2683644936960&\,xy^3 \\
- 6476277331836&\,y^4 \\
- 484880944704&\,x^3 \\
+ 468260157040&\,x^2y \\
- 1625087765184&\,xy^2 \\
+ 7083496190160&\,y^3 \\
+ 48256725504&\,x^2 \\
+ 293698179840&\,xy \\
- 4644695250832&\,y^2 \\
- 13129818624&\,x \\
+ 1675829775360&\,y \\
- 243236814336&\quad=0
\end{align*} This polynomial will factor into four circles over $\mathbb R$ or $\mathbb A$ but is irreducible over $\mathbb Q$ . How can I get these coefficients without going through the detour of the four distinct circles and their irrational parameters? How can I do this in situations where the coordinates of the lines themselves might contain unknowns? Background for my question is Probability that the centroid of a triangle is inside its incircle . For an exact solution there, radicals would make one's life really hard. But at the same time, since the centroid will never lie within an excircle, considering the sign of the product of circles should work just as well, and when combined with a rational parametrization of the circle might lend itself to some nice algebraic approach for that question. That's what got me thinking, but at the moment I'm actually more intrigued by this question here for its own merit. I feel like I'm missing some very useful tool in my arsenal, but don't know how to learn more. I'm including the Galois theory tag as I have the rough understanding that Galois theory deals with the relationship between the different roots of a polynomial. So if I want to understand how the different solutions interact when I multiply the circles, I assume that topic might have contributions. But so far my knowledge of Galois theory is pretty much exhausted by getting my computer algebra system to compute the Galois group of some polynomial, and then using that to decide whether a number is constructible or not. Update: For a moment I thought that the tool I had missed might be Vieta's formulas . But some of my coefficients will be combinations of roots of different polynomials. So I can't predict all my coefficients by just looking at the defining polynomial for one of my circle parameters, and when I look at two then the problem of how to match the roots returns. It seems to me that just the two polynomials won't have enough information on how to do that. To expand on this idea: If we define $$d_i:=-2p_i\qquad e_i:=-2q_i\qquad f_i:=p_i^2+q_i^2-r_i^2$$ then the product of circles is $$0 = \prod_{i=1}^4 (x+y)^2 + d_ix + e_iy + f_i$$ By treating $(x^2+y^2)$ as a single variable we can achieve a form where each monomial has a more direct correspondence with the $3\times4$ coefficients from the individual circles: \begin{align*}
0 = \Bigl(59636082025 &\, (x^2+y^2)^4 \\
-\,357924430760 &\, (x^2+y^2)^2x^2 \\
+\,476656901760 &\, (x^2+y^2)x^3 \\
-\,178359517440 &\, x^4 \\
+\,424846368960 &\, (x^2+y^2)^2xy \\
-\,778885009760 &\, (x^2+y^2)x^2y \\
+\,353718243840 &\, x^3y \\
-\,1073341537840 &\, (x^2+y^2)^2y^2 \\
-\,1512867557760 &\, (x^2+y^2)xy^2 \\
+\,2460406401440 &\, x^2y^2 \\
+\,2478270485600 &\, (x^2+y^2)y^3 \\
+\,1345667232000 &\, xy^3 \\
-\,1609193731600 &\, y^4 \\
+\,116789576020 &\, (x^2+y^2)^3 \\
-\,367854782880 &\, (x^2+y^2)^2x \\
+\,354562216320 &\, (x^2+y^2)x^2 \\
-\,103527290880 &\, x^3 \\
+\,1044413990360 &\, (x^2+y^2)^2y \\
+\,1337977704960 &\, (x^2+y^2)xy \\
-\,2253031422720 &\, x^2y \\
-\,4678151178480 &\, (x^2+y^2)y^2 \\
-\,1243734111360 &\, xy^2 \\
+\,4362204610400 &\, y^3 \\
-\,188932421756 &\, (x^2+y^2)^2 \\
-\,381353653824 &\, (x^2+y^2)x \\
+\,537823817856 &\, x^2 \\
+\,2721291579760 &\, (x^2+y^2)y \\
+\,293698179840 &\, xy \\
-\,4155128158480 &\, y^2 \\
-\,489567092352 &\, (x^2+y^2) \\
-\,13129818624 &\, x \\
+\,1675829775360 &\, y \\
-\,243236814336 & \Bigr)/59636082025
\end{align*} We can get the parameters of the individual circles characterized by \begin{align*}
244205d^4 - 1465672d^2 - 1951872d - 730368 &= 0 \\
244205e^4 - 4395248e^2 - 10148320e - 6589520 &= 0 \\
59636082025f^4 - 116789576020f^3 - 188932421756f^2 + 489567092352f - 243236814336 &= 0
\end{align*} Now for example the coefficient of $x^4$ is $$d_1d_2d_3d_4=-\frac{730368}{244205}=-\frac{178359517440}{59636082025}$$ This is a straight-forward application of Vieta, dividing the constant last coefficient by the leading one. Similarly the coefficient for $(x^2+y^2)^3$ is $$f_1+f_2+f_3+f_4=\frac{2164}{1105}=\frac{116789576020}{59636082025}$$ again following straight from the polynomial for $f$ , dividing the second coefficient by the leading one and flipping the sign. But other coefficients require miyed combinations of letters $d,e,f$ . For example the one for $(x^2+y^2)^2xy$ is $$\sum_{i=1}^4\sum_{\substack{j=1\\j\neq i}}^4d_ie_j=\frac{7872}{1105}=\frac{424846368960}{59636082025}$$ I guess I might be able to somehow use $$\sum_{i=1}^4\sum_{\substack{j=1\\j\neq i}}^4d_ie_j=
\left(\sum_{i=1}^4d_i\right)\left(\sum_{i=1}^4e_i\right)-\left(\sum_{i=1}^4d_ie_i\right)$$ if I were to also compute the defining quartic polynomial for the product $d_ie_i$ , but if I get a polynomial for the product by combining the polynomials for $d$ and $e$ then I get a degree $4\times 4=16$ which has all the combinations. In my example I find $$59636082025(de)^4 + 424846368960(de)^3 - 318329227424(de)^2 - 3822912737280(de) + 4812774543360 = 0$$ is actually quartic, but I don't know how to find this polynomial without going through the irrational individual circles first. For some of the other coefficients from the big equation, the inclusion-exclusion formulas I need to get all the combinations of letters with different indices would be even more complicated.","['galois-theory', 'triangles', 'algebraic-geometry', 'geometry']"
4889804,Matrix function derivative. Introduction,"The author of this question was close to determining the derivative of the function of dual variable, when we consider matrices isomorphic (algebraically and topologically) to dual numbers: $$(a+\epsilon b) \sim \begin{bmatrix}
    a       & 0  \\
    b       & a  \\
\end{bmatrix}.$$ So, using the fact we can define the derivative (in the Fréchet sense) for functions $F$ for with an argument in the form of such a matrix and a value in the form of such a matrix: $$F\big(\begin{bmatrix}
    x+s       & 0  \\
    y+t       & x+s  \\
\end{bmatrix}\big)-F\big(\begin{bmatrix}
    x       & 0  \\
    y       & x  \\
\end{bmatrix}\big)=\begin{bmatrix}
    u'       & 0  \\
    v'      & u'  \\
\end{bmatrix}\begin{bmatrix}
    s       & 0  \\
    t       & s  \\
\end{bmatrix}+o\bigg(\bigg|\bigg|\begin{bmatrix}
    s       & 0  \\
    t       & s  \\
\end{bmatrix}\bigg|\bigg|\bigg),$$ where $\bigg|\bigg|\begin{bmatrix}
    s       & 0  \\
    t       & s  \\
\end{bmatrix}\bigg|\bigg|=\max\{|s|,|t|\}$ and all elements of all matrices are real. Therefore, the existence of such a matrix $\begin{bmatrix}
    u'       & 0  \\
    v'      & u'  \\
\end{bmatrix}$ (which we will call derivative at $\begin{bmatrix}
    x       & 0  \\
    y       & x  \\
\end{bmatrix}$ ) means differentiability of $F$ at $\begin{bmatrix}
    x       & 0  \\
    y       & x  \\
\end{bmatrix}$ . I'm interested in to what extent can this approach be generalized in defining a matrix-valued function of a matrix argument? I mean the case, when the derivative is an object of the same nature as variables (in opposed to the definition of the derivative of a function $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ which is a (Jacobian) matrix). Can anyone share links to material with respect to such kind of derivatives?","['frechet-derivative', 'matrix-calculus', 'dual-numbers', 'isometry', 'derivatives']"
4889812,Are $O(n)$ and $SO(n)\times Z_2$ homeomorphic as topological spaces?,"I'm working through Problem 4.16 in Armstrong's Basic Topology , which has the following questions: Prove that $O(n)$ is homeomorphic to $SO(n) \times Z_2$ . Are these two isomorphic as topological groups? Some preliminaries: Let $\mathbb{M_n}$ denote the set of $n\times n$ matrices with real entries. We identify each matrix $A=(a_{ij}) \in \mathbb{M_n}$ with the corresponding point $(a_{11},a_{12},...,a_{1n},a_{21},a_{22}...,a_{2n},...,a_{n1},a_{n2},...,a_{nn}) \in \mathbb{E}^{n^2}$ , thus giving $\mathbb{M_n}$ the subspace topology. The orthogonal group $O(n)$ denotes the group of orthogonal $n \times n$ matrices $A \in \mathbb{M_n}$ , i.e. with $det(A)=\pm{1}$ . The special orthogonal group $SO(n)$ denotes the subgroup of $O(n)$ with $det(A)=1$ . $Z_2=\{-1, 1\}$ denotes the multiplicative group of order 2. My attempt For odd $n$ , the answer to both questions is yes , as we verify below. Consider the mapping $f:O(n)\to SO(n)\times Z_2, A \mapsto(det(A)\cdot A, det(A))$ . We have the following facts about $f$ : It is injective. If $f(A)=f(B)$ then $(det(A)\cdot A, det(A))=(det(B)\cdot B, det(B))$ .
Therefore, $det(A)=det(B) \neq 0$ so $A=B$ . It is surjective. For $(D,d) \in SO(n) \times Z_2$ , we can take $dD \in O(n)$ ,
giving $f(dD)=(det(dD)\cdot dD, det(dD))=(d^n\cdot det(D) \cdot dD,d^n \cdot det(D))=(d^{n+1}D, d^n)=(D,d)$ , since $n$ is odd. It is a homomorphism. $f(AB)=(det(AB)\cdot AB, det(AB))=(det(A)det(B)\cdot AB, det(A)det(B))$ $=((det(A)\cdot A)(det(B)\cdot B), det(A)det(B))=f(A)f(B)$ . It is continuous. Let $\mathcal{O} \in SO(n) \times Z_2$ be open. Then $\mathcal{O}=U \times V$ for $U$ open in $SO(n)$ and $V$ open in $Z_2$ .
Since $SO(n)$ is open in $O(n)$ , $U$ is therefore open in $O(n)$ . $-U=\{-A\mid A\in U\}$ is also open in $O(n)$ . But $f^{-1}(\mathcal{O})=f^{-1}(U\times V)=U\cup -U$ . Since $O(n)$ is compact and $SO(n)\times Z_2$ is Hausdorff, we therefore have that $f$ is a homeomorphism. Thus, they are isomorphic as topological groups. For even $n$ , this mapping is not well-defined: if $A \in O(n)$ with $det(A)=-1$ then, $det(det(A)\cdot A)=(det(A))^{n+1}=-1$ , so $det(A)\cdot A \notin SO(n)$ . My question then is are they homeomorphic as topological spaces if $n$ is even? From the related questions, it seems like for even $n$ , the two groups cannot be isomorphic due to one being abelian while the other is not and them having different centers and derived subgroups (I don't fully understand these arguments but I will brush up on them). So they cannot be isomorphic as topological groups. But can they be homeomorphic as topological spaces? Related questions: Are $SO(n)\times Z_2$ and $O(n)$ isomorphic as topological groups? Two topological groups $\mathrm{O}(n)$ (orthogonal group) and $\mathrm{SO}(n)\times \mathbb{Z}_2$ understanding $O(n)$ homeomorphic to $SO(n)\times \Bbb Z_2$ proof Why is the orthogonal group $\operatorname{O}(2n,\mathbb R)$ not the direct product of $\operatorname{SO}(2n, \mathbb R)$ and $\mathbb Z_2$?","['topological-groups', 'general-topology', 'abstract-algebra', 'algebraic-topology']"
4889836,What's the product rule for the exponential differential operator?,"So I was thinking say you have a linear differential operator such as the exponential differential one which is renown in some fields in physics: $$e^{\mathrm D_x}\equiv\sum_{n=0}^\infty\frac{\mathrm D_x^n}{n!},\text{ where } \mathrm D_x^n\equiv\frac{\mathrm d^n}{\mathrm dx^n},$$ and you applied it to a product of functions $u(x)\cdot v(x)$ . Then what would the ""product rule"" be for this operator? I.e., $$e^{\mathrm D_x}[u\cdot v]=?$$ APPROACH: Is this it? $$\sum_{n=0}^\infty\frac{1}{n!}\frac{\mathrm d^n}{\mathrm dx^n}(u\cdot v)=\sum_{n=0}^\infty\frac{1}{n!}\sum_{i=0}^n{n\choose i}u^{(n-i)}v^{(i)}=\sum_{n=0}^\infty\sum_{i=0}^n\frac{u^{(n-i)}}{(n-i)!}\frac{v^{(i)}}{i!}\overset{j=n-i}{=}\sum_{i=0}^\infty\sum_{j=0}^\infty\frac{u^{(j)}}{j!}\frac{v^{(i)}}{i!}$$","['differential-operators', 'calculus', 'derivatives', 'taylor-expansion']"
4889850,Is this Proof on 1-Form with Compact Support Correct?,"Question Let $\alpha=\sum_{i=1}^m\alpha_i(x)dx^i\in\Omega^1(\mathbb{R}^m)$ be a closed 1-form on $\mathbb{R}^m$ , and $f(x)=\sum_{i=1}^mx^i\int_0^1\alpha^i(ux)du$ define a smooth function satisfying $df=\alpha$ . Show that if $\alpha$ has compact support and $m>1$ , then there exists a smooth function $g$ with compact support such that $dg=\alpha$ . Show that this doesn't need to hold for $m=1$ . Attempt Since $\alpha$ has a compact support in $\mathbb{R}^m$ with $m>1$ , by the Poincaré lemma, there exists a function $g$ such that $dg=\alpha$ . Let’s construct $g$ so that it has compact support. Consider a bump function $\beta$ that is 1 on the support of $\alpha$ , and smoothly decreases to zero outside a slightly larger compact set. Then $\beta g$ will have compact support and still satisfy: $d(\beta g)=\beta dg=\beta \alpha=\alpha$ . (because $\alpha=0$ outside the support of $\alpha$ , so is $d\alpha$ ). Now, suppose $m=1$ , then $\mathbb{R}$ is not simply connected on any bounded interval. So, the Poincaré lemma does not apply. For example, if $\alpha=dx$ on $\mathbb{R}$ , then the function $g(x)=x$ is such that $dg=dx=\alpha$ , but $g$ does not have compact support. In other words, if there is a function with compact support on $\mathbb{R}$ whose derivative is $dx$ everywhere, the integral of such function will have to be constant outside its support, which would contradict the fact that the derivative is $dx$ . Note I am not sure I attempted this the right way, or at least there are a couple of things I left out in this proof. Your help will be appreciated.","['manifolds', 'differential-forms', 'smooth-manifolds', 'differential-geometry']"
4889865,"Is the function $f(x)=\int_0^\pi \ln |x+\cos(t)| dt$ differentible in interval (-1, 1)？","I tried to garph the function $f(x)=\int_0^\pi \ln |x+\cos(t)| dt$ and $f'(x)$ in desmos as above. Even $f(x)$ behaves like a constant function in $(-1,1)$ , but it seems not differentible in $(-1,1)$ . Why? Thank you for your help.","['integration', 'definite-integrals', 'calculus', 'functions', 'derivatives']"
4889871,What is the meaning of the notation ${\cal O}_{\mathbb{CP}^n}(k)$?,"I am studying some papers in which the notation ${\cal O}_{\mathbb{CP}^1}(-1)$ , ${\cal O}_{\mathbb{CP}^1}(-2)$ and ${\cal O}_{\mathbb{CP}^1}(1)$ appear. I am not familiar with that notation and while I have already found out ${\cal O}_{\mathbb{CP}^1}(-1)$ is the tautological line bundle, I haven't found what the others mean and want to understand this more generally, because surely there has to be some uniform definition that recovers the tautological line bundle as a special case. I believe this is not special to $\mathbb{CP}^1$ , and I also don't think there is something special about those numbers $-1,-2$ and $1$ . So I imagine there is some general meaning for $\mathcal{O}_{\mathbb{CP}^n}(k)$ where $k\in \mathbb{Z}$ . My question is: what does $\mathcal{O}_{\mathbb{CP}^n}(k)$ mean and how to understand it? Is it really something defined for any $k\in \mathbb{Z}$ or for only some values like $-2,-1$ and $1$ ? If only for some numbers, why is it the case?","['riemann-surfaces', 'definition', 'algebraic-geometry', 'algebraic-topology', 'differential-geometry']"
4889880,Prove $\int_0^1\frac{1}{\sqrt{1-x^2}}\arccos\left(\frac{3x^3-3x+4x^2\sqrt{2-x^2}}{5x^2-1}\right)\mathrm dx=\frac{3\pi^2}{8}-2\pi\arctan\frac12$.,"There is numerical evidence that $$I=\int_0^1\frac{1}{\sqrt{1-x^2}}\arccos\left(\frac{3x^3-3x+4x^2\sqrt{2-x^2}}{5x^2-1}\right)\mathrm dx=\frac{3\pi^2}{8}-2\pi\arctan\frac12.$$ How can this be proved? Wolfram does not find an antiderivative. Here is the graph of $y=\frac{1}{\sqrt{1-x^2}}\arccos\left(\frac{3x^3-3x+4x^2\sqrt{2-x^2}}{5x^2-1}\right)$ . Based on recent experience with integrals involving inverse trigonometric functions ( example ), I guess a proof may involve a lot of substitutions. But I don't have any insight on how to approach this. A search on approachzero did not turn up anything similar. Context If this can be proved, then we can answer the question Probability that the centroid of a triangle is inside its incircle , via @user170231's answer . How I found the conjectured closed form In my comment to @user170231's answer to the linked question, I note that the probability in that question is $1-\frac{12}{\pi^2}\int_0^{\pi/2}(y-x_+)dy$ , which is equivalent to $1-\frac{12}{\pi^2}(\int_0^{\pi/2}ydy-I)\approx0.457993176$ , where $I$ is the integral in this question. Wolfram suggests that $0.457993176$ is $4-24\left(\frac{\arctan(1/2)}{\pi}\right)$ , which implies that $I=\frac{3\pi^2}{8}-2\pi\arctan\frac12$ .","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'closed-form']"
4889892,"The operation $ (a,b)(c,d)=(ac-bd,ad+bc) $ on $\Bbb R\times\Bbb R\backslash (0,0)$ yields a group","Here is the binary operation $ *: \mathbb{R}\times \mathbb{R} \backslash (0,0) $ defined by $ (a,b)(c,d)=(ac-bd,ad+bc) $ . My idea is that to show this is a group ( $\mathbb{R}\times \mathbb{R} \backslash (0,0), * $ ), I need to show that $ * $ is well-defined and associative and then show it has an identity and inverse. I am struggling to do the first part. How do I show $ * $ is well-defined (and is the first part required)? Is showing that $ ac-bd=0,ad+bc=0 $ will only be true if $a=b=c=d=0$ sufficient?","['functions', 'group-theory', 'abstract-algebra', 'binary-operations']"
4889911,Confusion about different definitions of integral in measure theory.,"I've recently noticed that the definition of the integral in my measure theory course seems to be quite different from the usual definition, in particular the one another one of my courses uses. What really confuses me though, is that it seems to have much stronger properties than it should have, going by other texts. So the definition I saw in my measure theory course notably assumes $\mu$ to be a Radon (outer!) measure on $\mathbb{R}^n$ , meaning it is Borel regular and finite on any compact subset of $\mathbb{R}^n$ . Also, $\Omega \subseteq \mathbb{R}^n$ is assumed to be $\mu$ -measurable. We then define a simple function $g : \Omega \to \mathbb{R}^n$ to be a function of the form $$
g(x) = \sum_{i=1}^{\infty} d_i \chi_{A_i}(x)
$$ for $d_i \in \mathbb{R}^n$ and $A_i \subseteq \Omega$ mutually disjoint with $\bigcup_{i=1}^{\infty} A_i = \Omega$ . This already is different from the usual definition of a simple function, where the sum is only finite. Then the integral of a nonnegative simple $\mu$ -measurable function $g: \Omega \to \overline{\mathbb{R}}$ is defined as $$
\int_{\Omega} g \ d\mu = \begin{cases} \sum_{0 \leq y \leq \infty} y \mu(g^{-1}) \leq \infty & g < \infty \ \mu \text{-a.e.} \newline \infty & \text{otherwise} \end{cases},
$$ where $0 \cdot \infty = 0$ and the sum is well-defined since the range of a simple function is at most countable. Then we say a simple, $\mu$ -measurable function $g : \Omega\to [-\infty, \infty]$ is $\mu$ -integrable if either the integral of $g^+$ or $g^-$ in the above sense is finite. If so, then we define its integral as $$
\int_{\Omega} g \ d\mu = \int_{\Omega} g^+ \ d\mu - \int_{\Omega} g^- \ d\mu
$$ given that $|g| < \infty$ almost everywhere (otherwise we set it as $\pm\infty$ if $\mu(g^{-1}\{\pm \infty\})> 0$ ). Finally, for $f : \Omega \to [-\infty, \infty]$ $\mu$ -measurable we define the upper integral as $$
\overline{\int_{\Omega}} = \inf \left\{ \int_{\Omega} g \ d\mu \mid g \text{ is } \mu \text{-integrable and simple with } g \geq f \ \mu-\text{a.e.} \right\}
$$ and the lower integral as $$
\underline{\int_{\Omega}} = \sup \left\{ \int_{\Omega} g \ d\mu \mid g \text{ is } \mu \text{-integrable and simple with } g \leq f \ \mu-\text{a.e.} \right\}
$$ to conclude by calling $f$ $\mu$ -integrable if the upper and the lower integrals coincide. Then the notes go on to state that any nonnegative measurable function is integrable in this sense. On the other hand, in my course on Fourier theory, there is a small recap of measure theory where the definitions are quite different. First of all, we are in the setting of classical measures and consider an arbitrary measure space $(X, \mathcal{F}, \mu)$ . Then, simple functions are defined as finite sums $$
\sum_{j=1}^n a_j \chi_{E_j},
$$ where $a_1, \dots, a_n \in \mathbb{R}$ and $E_1, \dots, E_n$ satisfy $\mu(E_i) < \infty$ . Then, we define the integral of a simple function $s$ as $$
\int_X s \ d\mu = \sum_{j=1}^n a_j \mu(E_j).
$$ Next, the integral for a measurable $f : X \to [0, \infty]$ is defined as $$
\int_X f \ d\mu = \sup \left\{ \int_X s \ d \mu \mid s \text{ is simple and } s(x) \leq f(x) \text{ for all } x \in X \right\}
$$ and a general measurable $f : X \to \mathbb{R}$ is called integrable if either the integral of $f^+$ or $f^-$ is finite. If it is integrable, its integral is simply defined as the difference of the integrals of $f^+$ and $f^-$ .
In this definition, we didn't use the upper integral at all. After looking at some other textbooks, for example Terry Tao's Introduction to Measure Theory, it seems like there is a reason that the upper integral is not usually used in the definition. Tao uses a similar construction and claims that $f : \mathbb{R}^n \to [0, \infty]$ measurable needs to be bounded and vanishing outside a finite measure set for the upper and lower integrals to coincide. This in particular seems very different from the statement in the former construction, that said any nonnegative measurable function is ""integrable"" (where integrable means that the upper and lower integrals coincide). I'm getting lost in all the different terminology and assumptions that are made, so I can't see how the two constructions relate. Why do we require the upper and lower integrals to coincide in the first but not in the second? Does this result in a different notion of integrability? Is one stronger than the other? Does it have anything to do with the assumption that $\mu$ is Radon in the first? I have an exam on both of these classes in the summer, so I'd really appreciate any help on this. Update: It seems that the main difference comes from the fact that my measure theory course defined simple functions as countable sums (resulting in functions with countable range) while the other resources I saw define them as finite sums (resulting in functions with finite range). The former definition seems to be following the text by Evans and Gariepy while I've seen the latter definition in many other textbooks, like Tao. I think if you define the upper and lower integrals via simple functions with countable range, they match for all non-negative measurable functions. If you define them using simple functions with finite range, you get functions whose lower integral is finite but whose upper integral is infinite (an example being $e^{-x^2}$ ). At least this is the conclusion I drew from posts like this . I'm not sure whether this is the full story though.",['measure-theory']
4889952,Recursive coupon collector problems,"Consider the well-known coupon collector problem with $n$ potential coupons. If I collect a coupon per day, I know it will take me $n \cdot H_n$ days to finally collect all of them, where $H_n$ is the $n$ -th harmonic number, and $H_n$ to collect each coupon on average. For example, if the coupons are the positive integers up to 12, one potential sequence of coupons could look like r1=[5     2    12     9     3     2     1     8     5     6     7    11    11    11     3     4     2     9     3     5     8    12     8     3    10] Note that for this example, it took me 25 days to collect them all, which is “kind of” close to $12 \cdot H_{12}=37$ , with an average per coupon of $t_1=|r_1|/n=25/12=2.08$ . Now, once I obtain all of the coupons, then I will observe which ones I have obtained exactly once. I will eliminate this set $k_1$ of coupons, and take a look at how long it took me to collect the remaining $n-|k_1|$ coupons this time. Note that in this process I am always eliminating the last coupon to appear, but possibly more. In the previous example, the coupons that appear once are k1=[1     4     6     7    10] which are the ones which I will remove. And the ones that can stay are b1=[2     3     5     8     9    11    12] Searching how long it took me in the initial sequence to get all the values I haven’t removed, I obtained: r2=[5     2    12     9     3     2     8     5    11] I would expect that this should have taken me on average $H_{n-|k_1|}$ days?  Here $|k_1|=5$ so this would be $H_7=2.59$ . This is not that close to the actual time that it took me, which is $t_2=|r_2|/(n-|k_1|)=9/7=1.29$ in our example. I will now check, in the second iteration, which are the coupons that I have collected exactly once, and remove them again from the sample. The coupons that appear only once are k2=[3     8     9    11    12] I remove them and obtain the shortest list that contains all elements in $[1:12]\setminus \{k_1 \cup k_2\}$ . This gives me r3=[5     2] Again, I would expect that this should have taken me on average $H_{n-|k_1|-|k_2|}$ days?  Here $|k_1|=5$ and $|k_2|=5$ , which gives $1.5$ , which is also more than what it actually took me, which was only $t_3=|r_3|/(n-|k_1|-|k_2|)=2/2=1$ day on average. In the end, I am interested in the average time it took me to take each coupon in each of the $r$ sequences, call this $T_n$ . In our example, it took me 2.08 days on average to get my first coupons in $r_1$ , this is multiplied by the number of elements that appeared only once in $r_1$ , i.e. $|k_1|=5$ . Then, it took me, on average, $1.29$ days to get each of the coupons in $r_2$ , this is multiplied by $|k_2|=5$ , and finally it took me an average of 1 day to collect each of the coupons in $r_3$ , and this is multiplied by $k_3$ . Finally, we divide all of it by $n$ . In our example, $$T_{12}=(5\cdot 2.08+5 \cdot 1.29+2\cdot 1)/12= 1.57$$ Thus, $T_n$ can be defined more generally as: $$T_n=\left[\frac{1}{n}\right]\left[\frac{|k_1||r_1|}{n}+\frac{|k_2||r_2|}{n-|k_1|}+\frac{|k_3||r_3|}{n-|k_1|-|k_2|}+\ldots\right]$$ I am interested in the approximate expected value of $T_n$ , particularly in its asymptotic behavior and upper/lower bounds. My guess was that one could approximate $T_n$ by the recursive use of the initial approximation $H_n$ , in which we fix that in each round we remove exactly one coupon. Therefore, we obtain $\frac{H_n + H_{n-1}+ \ldots}{n}$ , which asymptotically converges to $H_n$ . But, as I have shown before in the example, this approximation is not that good because the recursive approximation using $H_n$ starts getting worse and worse. In simulations, I have found the following values for $T_n$ : Vaue of n Output 25 2.1325 50 2.3378 100 2.4772 200 2.6004 500 2.7388 If it helps, note that the expected size of $k_1$ is $H_n$ , see Section 4 in Myers, Amy N., and Herbert S. Wilf. ""Some new aspects of the coupon collector's problem."" SIAM review 48.3 (2006): 549-565. Edit: using the number of singletons, I was able to obtain a lower bound described in this post , but unfortunately there is no nice way to write the bound. I believe this bound is a very good approach in solving the question (but is still off by some margin).","['coupon-collector', 'combinatorics']"
4889969,Find an equivalence relation over all of $\mathbb{Z}$ which has infinitely many equivalence classes with infinitely many elements in each,"I want to find an equivalence relation defined on all integers (that is, all of $\mathbb{Z}$ ) where The equivalence relation partitions $\mathbb{Z}$ into infinitely many equivalence classes; and Every equivalence class contains an infinite number of elements. I've been thinking about this interesting question for a while, and I have come up with two ideas which are close, but not complete solutions. My first idea was to define the equivalence relation $\sim$ where $x \sim y \iff x = \pm p^m, y = \pm p^n$ for some prime number $p$ , and some integers $m, n$ . This will certainly create infinitely many equivalence classes where each equivalence class will essentially contain all powers of one prime number (positive or negative). Since there are infinitely many prime numbers, there will be infinitely many equivalence classes, and each equivalence class will contain infinitely many elements. However, there are two problems with this idea: firstly, $1$ and $-1$ , which are equal to $\pm p^0$ for all prime numbers $p$ , will be in all of the equivalence classes. Secondly, $0$ will need to be placed in an equivalence class by itself, which does not satisfy the condition that every equivalence class must contain an infinite number of elements. My second idea was to say that two integers $x$ and $y$ a are equivalent iff the largest power of $2$ that divides them is the same. Essentially, I say that $x$ and $y$ are equivalent iff the number of zeroes at the end of their binary expansion is the same. However, this fails to account for the number $0$ ; we cannot place $0$ into an unique equivalence class because that would fail to satisfy the condition that every equivalence class must contain an infinite number of elements. Can anyone provide a hint or a solution to this problem?","['infinitary-combinatorics', 'equivalence-relations', 'integers', 'examples-counterexamples', 'discrete-mathematics']"
4890023,A formula for $\pi$,"I discovered in an exercise that $$\boxed{\pi=12\int_{0}^{2-\sqrt3}\frac{dt}{1+t^2}}$$ since $\frac{\pi}{12}=\arctan( 2-\sqrt3)$ , what we were made to demonstrate with formulas with half angles. I tried the following $$\forall |t|<1, \frac{1}{1+t^2}=1-t^2+t^4-t^6+...$$ So $$\pi\approx12\left( \alpha-\frac{\alpha^3}{3}+\frac{\alpha^5}{5} \right)\approx 3.1418$$ with $\alpha=2-\sqrt3$ Since it's an alternating series, we get $$|\pi-12(\alpha+...+(-1)^n\frac{\alpha^{2n+1}}{2n+1}|\leq \frac {12}{2n+3}\frac{3^{2n+3}}{10^{2n+3}}$$ since $\alpha<0.3$ For example, with $n=5$ , $$3.1415925<\pi<3.1415928$$ with $n=8$ , $$\frac{88081345207880}{51051} - \frac{84756313789596 \sqrt3}{85085}= 3.141592653...$$ As far as I can judge, one advantage of this process is that it gives approximations of type $a+b\sqrt3,(a,b)\in \mathbb Q^2$ and thus reduces the approximation of $\pi$ to that of $\sqrt3$ . What are some mathematical arguments for mathematically criticizing what I probably naively call "" one advantage""?","['power-series', 'calculus']"
4890033,How did Artin discover the function $f(x)=\frac{(x^2-x+1)^3}{x^2(x-1)^2}$ with the properties $f(x)=f(1-x)=f(\frac{1}{x})$?,"In Artin's ""Galois Theory"" P38, he said the function $$f(x) = \frac{(x^2 - x + 1)^3}{x^2(x-1)^2}$$ satisfies the properties of $f(x)=f(1-x)=f(\frac{1}{x})$ . Is the function given by some rational step or just by a flash of insight？ If $f(0)$ is a number, then $f(0) = f(\frac{1}{0})$ . So that the domain of definition of f(x) does not include 0.(maybe. I know it's not rigorous) Then the domain of definition of f(x) does not include 1 either. Thus I think it is a function like $f(x)=\frac{g(x)}{x^a(x-1)^bh(x)}, h(0)*h(1) \neq 0$ . Then I tried $a=1, b=1$ , failed. but $a = 2, b = 2$ succeed. However, I think that's a really weird way to go about it. Does the question like"" $f(x)$ is a rational function that satisfies the properties of $f(x) = f(g_1(x)) = f(g_2(x)) = ... =f(g_n(x)). \forall k \in \mathbb N^+, g_k(x)$ is a rational function. Now give a example of f(x)."" has an easy way to solve?","['galois-theory', 'functions', 'rational-functions', 'polynomials']"
4890040,Finding the tangent line to curve to the ellipse $(x-3)^2+\frac{(y-4)^2}{4}=1$ through the origin,"I am told to find the two tangent lines to the ellipse that pass through the origin, but have been stuck for far too long with my approach, hence am thinking that my approach may be flawed. Here is what I have so far: If I interpret the ellipse as the level curve of some function $f(x,y)=1$ , then I can use the fact that the gradient vector is perpendicular to the ellipse at every point $(a,b)$ on it. Computing the partials, I get that the gradient vector at any point $(a,b)$ is $$\nabla f(a, b) = \left( 2(a-3), \frac{(b-4)}{2}\right),$$ thus, the tangent vector at $(a,b)$ is $$\left(-\frac{(b-4)}{2}, 2(a-3)\right),$$ thus the equation of any tangent line to the ellipse passing through the origin is $$k\left(-\frac{(b-4)}{2}, 2(a-3)\right), k\in \mathbb Z.$$ However, I really don't get how I'm supposed to find... another tangent line? Have I made an oversight in one of the steps of my reasoning? To resolve this, I tried also tried the approach of parametrizing the ellipse into a vector-valued function $$\vec r(t)=(\cos t +3, 2\sin t +4),$$ which can equivalently be interpreted as the orbit of some moving partical. Then, I can differentiate this to get the velocity function of the particle: $$\vec r(t) = (-2\sin t,\cos t).$$ But then, how am I to ensure that the tangent lines pass through the origin?","['conic-sections', 'multivariable-calculus', 'linear-algebra', 'parametrization']"
4890052,Calculating mean absolute deviation for Poisson distribution,"I'm trying to solve the ex 5.11 from ""Probability Essentials"", which asks to show that if X is Poisson(λ) then 𝐸{|𝑋−𝜆|}= $\frac{2\lambda^\lambda e^{-\lambda}}{(\lambda -1)!} $ . I've shown that 𝐸{|𝑋−𝜆|} = ${2\lambda^\lambda e^{-\lambda}} $ $\sum_{k=1}^\infty \left(\frac{k\lambda^k}{(k+\lambda)!}\right) $ but I'm having trouble calculating the sum of this series: $\sum_{k=1}^\infty \left(\frac{k\lambda^k}{(k+\lambda)!}\right) $ . Could somebody help? EDIT: This is what I have done 𝐸{|𝑋−𝜆|} = $\sum_{j=0}^\infty \left(\frac{|j-\lambda|\lambda^je^{-\lambda}}{j!}\right) $ = $\sum_{j=0}^\lambda \left(\frac{(\lambda - j)\lambda^je^{-\lambda}}{j!}\right)$ + $\sum_{j=\lambda+1}^\infty \left(\frac{(j-\lambda)\lambda^je^{-\lambda}}{j!}\right) $ = $\sum_{j=0}^\infty \left(\frac{(\lambda - j)\lambda^je^{-\lambda}}{j!}\right)$ - $\sum_{j=\lambda+1}^\infty \left(\frac{(\lambda-j)\lambda^je^{-\lambda}}{j!}\right) $ + $\sum_{j=\lambda+1}^\infty \left(\frac{(j-\lambda)\lambda^je^{-\lambda}}{j!}\right) $ = $2\sum_{j=\lambda+1}^\infty \left(\frac{(j-\lambda)\lambda^je^{-\lambda}}{j!}\right) $ = $2\sum_{k=1}^\infty \left(\frac{k\lambda^{k+\lambda}e^{-\lambda}}{(k+\lambda)!}\right) $ = $2 e^{-\lambda} \lambda^{\lambda}\sum_{k=1}^\infty \left(\frac{k\lambda^{k}}{(k+\lambda)!}\right) $ .","['calculus', 'probability', 'sequences-and-series']"
4890065,$2^x-x^2+x+\cos(x)=0$ has one real root [duplicate],"This question already has answers here : Find the number of solution(s) of equation $f(x)=2^x-x^2+x+\cos x$ (2 answers) Closed 3 months ago . We just need to show that $2^x-x^2+x+\cos(x)=0$ has exactly one real root. How I approached it : Let $f(x)=2^x-x^2+x+\cos(x)$ $f$ is differentiable with $f'(x)=\log2 \cdot 2^x -2x +1 - \sin(x)$ We can see that for $x \in (-\infty,0) , f'(x) \gt 0 \implies f$ is always increasing . Now we can find the range of $f$ for $x<0$ and it turns out to be $f((-\infty ,0))=(-\infty , 2)$ . So $f$ has exactly one root for $x<0$ But this is how far I've come so far . For possitive $x$ I cannot seem to figure out how to work it out. Any help would be much appreciated!","['calculus', 'functions']"
4890148,Solve the equation $\left(\frac{1+\sqrt{1-x^2}}{2}\right)^{\sqrt{1-x}} = (\sqrt{1-x})^{\sqrt{1-x}+\sqrt{1+x}}$,"Solve in $\mathbb{R}$ : $
\left(\frac{1+\sqrt{1-x^2}}{2}\right)^{\sqrt{1-x}} = (\sqrt{1-x})^{\sqrt{1-x}+\sqrt{1+x}}
$ My approach:
Let $a = \sqrt{1-x}$ and $b = \sqrt{1+x}$ so $a^2 + b^2 = 2$ . The equation becomes $\left(\frac{1+ab}{2}\right)^a = a^{a+b}$ , which is equivalent to $\left(\frac{1+ab}{a^2+b^2}\right)^a = a^{a+b}$ . After taking the natural logarithm, we get $a \ln(1+ab) - a \ln(a^2+b^2) = a \ln(a) + b \ln(a)$ . I thought of considering a function but I couldn't find it. Any help is appreciated.","['systems-of-equations', 'logarithms', 'functions', 'inequality', 'exponential-function']"
4890156,Integrals with residue theory [ANSWERED],"I'm having some problems solving this integral: $$ I = \mathcal{P} \int_{-\infty}^{+\infty} \frac{1-e^{2ix}}{x^2} \ dx$$ where $\mathcal{P}$ is the Cauchy principal value. The exercise suggests to use the fact that: $$I_* = \frac{1}{2} \operatorname{Re} \left[I\right]=\mathcal{P} \int_{-\infty}^{+\infty} \frac{\sin^2 x}{x^2} \ dx$$ since $\sin^2 x = \frac{1}{2} \left(1- \cos(2x)\right)$ . My solution. I went on and tried to solve $I_*$ as follows: I used the fact that the analytic extension of the integrand has no poles, which makes the integral equals to $0$ by using residue theory: $$\lim_{R\to + \infty}\oint_{\Gamma_R} \frac{\sin^2z}{z^2} \ dz = \mathcal{P} \int_{-\infty}^{+\infty} \frac{\sin^2 x}{x^2} \ dx = 0$$ where the second equality is true since $$\oint_{\Gamma_R}\frac{\sin^2z}{z^2} \ dz =\left(\int_{-R}^{+R} + \int_{C_R}\right) \frac{\sin^2z}{z^2} \ dz$$ where $C_R = \{z = r e^{i \theta}\in \mathbb{C} : 0\le r \le R\}$ and $$\left\lvert \int_{C_R} \frac{\sin^2 z}{z^2} \ dz \right\rvert \le \int_{C_R} \frac{1}{|z^2|} \ dz \le \int_{C_R} \frac{1}{|R^2|} \ dz \to 0, \ R\to +\infty$$ $$\lim_{R\to+\infty} \int_{-R}^{+R} \frac{\sin^2z}{z^2} \ dz = \lim_{R\to+\infty} \frac{\sin^2 x}{x^2} \ dx \equiv \mathcal{P} \int_{-\infty}^{+\infty} \frac{\sin^2 x}{x^2} \ dx$$ Since the residues of this function are all $0$ , this means that also $$\mathcal{P} \int_{-\infty}^{+\infty} \frac{\sin^2 x}{x^2} \ dx =0 $$ Ok, now, since this implies that $\operatorname{Re}I = 0$ , I thought that $I$ must have just an imaginary part; for this reason, I then tried to calculate the following: $$\operatorname{Im} [I] = \mathcal{P} \int_{-\infty}^{+\infty} \frac{\sin(2x)}{x^2} \ dx \equiv \mathcal{P}\int_{-\infty}^{+\infty} h(x) \ dx$$ I extended $h(x)\to h(z)$ , which has a first order pole in $z=0$ : $$\operatorname{Res}\left[h(z) , z=0\right]=\lim_{z\to 0 } \left(z \frac{\sin 2z}{z^2}\right) = 2$$ Then I integrated $h(z)$ as follows: $$\oint_{\Gamma_{r,R} } \frac{\sin 2z}{z^2} \ dz = \left(\int_{-R} ^{-r} + \int_{C_r^-} +\int_{r}^{R} + \int_{C_R}     \right) \frac{\sin 2z}{z^2} \ dz $$ where: $$\lim_{r\to 0} \int_{C_r^-} \frac{\sin 2z}{z^2} \ dz \to -i\pi\operatorname{Res}\left[h(z), z=0\right] = -2i\pi $$ $$\left\lvert \int_{C_R} \frac{\sin 2z}{z^2} \ dz \right\rvert \le \int_{C_R} \frac{1}{R^2}\to 0, \ R\to+\infty $$ $$\lim_{r \to 0, \ R\to +\infty} \left(\int_{-R}^{-r} + \int_{r} ^{R}   \right) h(z) \ dz \equiv \mathcal{P}\int_{-\infty} ^{+\infty} h(x) \ dx $$ Putting all of this together, we get: $$\mathcal{P}\int_{-\infty} ^{+\infty} \frac{\sin 2x}{x^2 } \ dx = -2i\pi$$ What bothers me the most and that makes me think I did something wrong is that the result of this real valued integral is an imaginary number. Moreover, this would mean $\operatorname{Im} (I) = -2i\pi\Rightarrow I\stackrel{?}{=} 2\pi$ or $I \stackrel{?}{=} -2i\pi$ . Did I make some errors? Can you help me getting to the correct solution? I really need help with this because I feel like I'm missing something very important. Thanks a lot in advance for the help!! EDIT 1: This is the solution by calculating directly $I$ with the residues. So we define $F(z) = \frac{1-e^{2iz} }{z^2}$ , which is the analytical extension of the integrand of $I$ . This clearly has a first order pole in $z=0$ , which residue is obtained by: $$\operatorname{Res}\left[F(z), z=0\right] = \lim_{z \to 0} \frac{d}{dz}\left(z^2 \frac{1-e^{2iz} }{z^2}\right) = -2i $$ The complex integral we need to calculate would be: $$\lim_{r\to 0, \ R \to +\infty} \oint_{\Gamma_{r,R} } \frac{1-e^{2iz} }{z^2}\ dz = \lim_{r\to 0, R\to+\infty} \left(\int_{-R}^{-r} + \int_{C_r^-} +\int_{r} ^R + \int_{C_R}   \right) \frac{1-e^{2iz} }{z^2}\ dz =0 $$ since of the first order pole in $z=0$ (and it is equal to $0$ because there are no poles inside the contour taken), it is needed to create a small arc of circumference $C_r$ to avoid calculating the function in $z=0$ . After making sure that the integral on $C_R$ goes to $0$ , which is done by using Jordan's Lemma and the fact that $\frac{1}{|z^2|}=\frac{1}{R^2}\to 0$ as $R\to+\infty$ , we can calculate the integral on $C_r^-$ (where the minus sign is because it is ``walked'' clockwise); this should be integral that gives the residue in $z=0$ of $F(z)$ when $r\to 0$ because of the theorem that states that: $$\lim_{r  \to 0} \int_{C_{r_\alpha } } F(z) \ dz = i \alpha \operatorname{Res}\left[F(z), z=0\right] $$ (which is true just for first order poles). This theorem implies automatically that $$\lim_{r \to 0} \int_{C_r^-} \frac{1-e^{2iz} }{z^2} \ dz = -i \pi (-2i) = 2\pi =-2\pi$$ Since the other two integrals are such that: $$\lim_{r \to 0, \ R\to+\infty} \left(\int_{-R} ^{-r} + \int_{r} ^R \right) \frac{1-e^{2iz} }{z^2} \ dz \equiv \lim_{r \to 0, \ R\to+\infty} \left(\int_{-R} ^{-r} + \int_{r} ^R \right) \frac{1-e^{2ix} }{x^2} \ dx  \equiv \mathcal{P}\int_{-\infty} ^{+\infty} \frac{1-e^{2ix} }{x^2} \ dx  $$ it means that: $$\mathcal{P}\int_{-\infty} ^{+\infty} \frac{1-e^{2ix} }{x^2} \ dx = 2\pi$$ As pointed out in the comments, this is the most concise and easy solution for the problem; nonetheless why does the exercise, which is directly taken from a past exam that my professor made public, suggests to use the integral $I_*$ ? EDIT 2: I'll contact my professor to ask him why he gave that hint in the exercise. I'll update this post afterwards to let you know what he replied to me. EDIT 3: My professor told me that that was not an hint, rather it was possibile to evaluate that in integral ( $I_*$ ) once obtained the main one, since $I_*$ is not obtainable by standard integration.","['integration', 'complex-analysis', 'cauchy-principal-value', 'residue-calculus', 'complex-numbers']"
4890157,Magical trigonometric inequality (feat. number theory),"A couple of days ago I found an interesting problem on the Art of Problem Solving website. It says: For odd and coprime positive integers $p$ and $q$ , the following inequality holds $$ \sum_{m=1}^{p} \sum_{n=1}^{q} \frac{2}{\cos(\frac{2m\pi}{p})+\cos(\frac{2n\pi}{q})} \le pq(|p-q|+1). $$ The magical thing is that the equality holds for some non-trivial examples, such as $(p,q)=(23,31), (29,31)$ (I have checked via WolframAlpha), while for some other $(p,q)$ the bound is rather loose. This suggests that crude boundings cannot apply, and there might be an identity behind it. The strange equality case also hints at the relation to prime numbers. Pairing $m \leftrightarrow p-m$ and $n \leftrightarrow q-n$ seems plausible, but appears to be of little use. I tried directly compute $\sum_{n=1}^{q} \frac{2}{\cos(\frac{2m\pi}{p})+\cos(\frac{2n\pi}{q})}$ for a fixed $m$ , but I find it hard to deal with. Also, the cosine terms may related to eigenvalues of circulant matrices, but I don't know how to interpret $\cos(\frac{2m\pi}{p})+\cos(\frac{2n\pi}{q})$ . Any ideas or comments are welcome. Edit: I numerically compute the values for all odd numbers $p \le q<50$ (without requiring $\gcd(p,q)=1$ ). Exclude trivial cases $p=q$ or $p=1$ , other equality cases are as follows: $$ \color{red}{(3,7)},(5,7),\color{red}{(5,11)},\color{red}{(7,15)},(9,11),(9,13),\color{red}{(9,19)},(11,15),\color{red}{(11,23)},(13,15),(13,19),\color{red}{(13,27)},\color{red}{(15,31)},(17,19),(17,21),(17,23),(17,25),\color{red}{(17,35)},(19,23),\color{red}{(19,39)},(21,23),(21,31),\color{red}{(21,43)},(23,31),\color{red}{(23,47)},(25,27),(25,29),(25,31),(25,37),(27,31),(29,31),(29,35),(29,39),(29,43),(33,35),(33,37),(33,41),(33,49),(35,39),(35,47),(37,39),(37,43),(39,47),(41,43),(41,45),(41,47),(43,47),(45,47). $$ It appears that $q \le 2p+1$ should hold for equality to occur, and $q=2p+1$ indeed gives equality. But I don't find other structures.","['number-theory', 'inequality', 'trigonometry', 'prime-numbers']"
4890196,The restriction of Lebesgue measure to the $\sigma$-algebra of Borel subsets of $\mathbb{R}$ is not complete.,"I am new to measure theory. I am confused by the following claim from Measure Theory by Donald Cohn (Section 1.5 Completeness and Regularity): Claim $\quad$ The restriction of Lebesgue measure to the $\sigma$ -algebra of Borel subsets of $\mathbb{R}$ is not complete. In this post, I will denote the Lebesgue outer measure by $\lambda^*$ . According to the textbook, Definition $\quad$ The restriction of Lebesgue outer measure on $\mathbb{R}$ (or on $\mathbb{R}^d$ ) to $\mathcal{B}(\mathbb{R})$ or to $\mathcal{B}(\mathbb{R}^d)$ is called Lebesgue measure and will be denoted by $\lambda$ . The restriction of Lebesgue outer measure on $\mathbb{R}$ (or on $\mathbb{R}^d$ ) to the collection of Lebesgue measurable subsets of $\mathbb{R}$ (or of $\mathbb{R}^d$ ) is also called Lebesgue measure and will be denoted by $\lambda$ as well. Definition $\quad$ Let $(X,\mathcal{A},\mu)$ be a measure space. The measure $\mu$ (or the measure space $(X,\mathcal{A},\mu)$ ) is complete if the relations $A\in\mathcal{A}$ , $\mu(A)=0$ , and $B \subseteq A$ together imply that $B\in\mathcal{A}$ . So, my understanding of this claim is the following: Let $(\mathbb{R},\mathcal{B}(\mathbb{R}),\lambda)$ be a measure space. Let $A$ be a Borel subset of $\mathbb{R}$ with $\lambda(A)=0$ . There exists a subset $B$ of $A$ such that $B \notin \mathcal{B}(\mathbb{R})$ . I saw that this post used a Vitali set as an example to show that such a set $B$ does exist. However, I am very confused by its explanation. Let $V$ be a Vitali set in $[0,1]$ . I know that, since $V$ is not measurable with respect to the Lebesgue outer measure $\lambda^*$ , it follows that $V \notin \mathcal{B}(\mathbb{R})$ . I think we would want to find a set $A$ such that $A \in \mathcal{B}(\mathbb{R})$ , $\lambda(A)=0$ , and $V \subseteq A$ . But what exactly is this $A$ in the example? In that post I mentioned above, it seems to me that it wanted to say that the Vitali set $V$ is of Lebesgue measure zero in $\mathbb{R}^2$ and itself is not a Borel subset of $\mathbb{R}^2$ . However, if $V$ is not even Lebesgue measurable (i.e., $V$ is not measurable with respect to the Lebesgue outer measure), wouldn't $\lambda(V)$ be not defined? In addition, I couldn't see how his example showed the claim is true, because there was no such set $A \subseteq \mathcal{B}(\mathbb{R})$ . Basically, I am completely lost. I would really appreciate it if someone could help me clarify it or present another example! As @LeeMosher pointed out. The claim should have been understood in the following way: Let $(\mathbb{R},\mathcal{B}(\mathbb{R}),\lambda)$ be a measure space. For some Borel subset $A$ of $\mathbb{R}$ with $\lambda(A) = 0$ , there exists a subset $B$ of $A$ where $B \notin \mathcal{B}(\mathbb{R})$ .","['measure-theory', 'lebesgue-measure', 'proof-explanation', 'analysis', 'real-analysis']"
4890222,Let $S$ be infinite and $A\subset S$ be finite. Prove that $|S| = |S\setminus A|$ [duplicate],"This question already has an answer here : Proof that subtracting a finite set from an infinite one retains cardinality (1 answer) Closed 3 months ago . Let $S$ be infinite and $A\subset S$ be finite. Prove that $|S| = |S\setminus A|$ Given Solution Let $A = \{s_1,\ldots s_n\}$ . Since $S$ is infinite the set $S\setminus A$ is non-empty. Pick any $s_{n+1}\in S\setminus A = S\setminus \{s_1,\dots, s_n\}$ . Next, since $S\setminus \{s_1,\dots , s_{n+1}\}\neq\emptyset$ we can choose $s_{n+2}\in S\setminus \{s_1,\ldots, s_{n+1}\}$ . Proceeding by induction we can construct $s_{m+1}\in S\setminus \{s_1,\dots, s_m\}$ for any $m\geq n$ . Now define $f : S\to S\setminus A$ by the formula $f (s_i) = s_{i+n}$ for any $i$ and $f (x) = x$ if $x\in S\setminus \{s_1, s_2,\ldots\}$ . By construction, $f$ is 1-1 and onto. Similar but concrete example I could fully understand $(0,1)\sim [0,1]$ by $$f(x) = 
\begin{cases} 
\frac{1}{10} & \text{if } x = 0, \\
\frac{1}{100} & \text{if } x =1.\\
\frac{1}{10^{n+2}} & \text{if } x=\frac{1}{10^n}.\\
x, & \text{otherwise }
\end{cases}$$ The subtle difference here is that $[0,1]$ is uncountably infinite, so we can split out a infinite sequence (countable) to map recursively.  However, in the previous question, we only know the set is infinite, not sure if it is countable or not, how does it function define like below $f : S\to S\setminus A$ by the formula $f (s_i) = s_{i+n}$ for any $i$ and $f (x) = x$ if $x\in S\setminus \{s_1, s_2,\ldots\}$ . I try to let $S=\mathbb{N}$ and $A=\{1,\ldots,10\}$ , it is not injective. Moreover, how come the finite $A$ becomes infinite as $x\in S\setminus \{s_1,s_2,\ldots\}$ all of sudden","['proof-explanation', 'analysis', 'real-analysis', 'elementary-set-theory', 'set-theory']"
4890234,Line integral where $C$ is the boundary of the square -Green's Theorem,"I am trying to solve the following problem but I face difficulties with the results. Any suggestion? The line integral $\int_C y^2 dx - x dy$ , where $C$ is the boundary of the square $[-1,1]\times[-1,1]$ oriented counterclockwise, can be evaluated in two ways: a) Using the definition of the line integral:
To compute the line integral directly from its definition, we break the path $C$ into four segments corresponding to the sides of the square. For each segment, we parameterize the path, compute $dx$ and $dy$ , substitute into the integral, and evaluate. b) Using Green's theorem:
Green's theorem relates a line integral around a simple closed curve $C$ to a double integral over the plane region $D$ bounded by $C$ . It states that $\int_C P dx + Q dy = \int\int_D \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right) dA$ where $P(x,y) = -x$ and $Q(x,y) = y^2$ . We can compute the partial derivatives, substitute them into the double integral, and evaluate over the region $D$ , which is the square \$[-1,1]\times[-1,1]$. SOLUTION a)
To solve the line integral $\int_C y^2 dx - x dy$ using the definition, we need to consider the path $C$ which is the boundary of the square with vertices at $(-1,-1)$ , $(-1,1)$ , $(1,1)$ , and $(1,-1)$ , oriented counterclockwise. We'll break $C$ into four segments corresponding to the sides of the square: $C_1$ from $(-1,-1)$ to $(-1,1)$ where $x = -1$ and y varies from $-1$ to $1$ . $C_2$ from $(-1,1)$ to $(1,1)$ where $y = 1$ and $x$ varies from $-1$ to $1$ . $C_3$ from $(1,1)$ to $(1,-1)$ where $x = 1$ and $y$ varies from $1$ to $-1$ . $C_4$ from $(1,-1)$ to $(-1,-1)$ where $y =-1$ and $x$ varies from $1$ to $-1$ . Now we'll compute the integral over each segment and sum them: On $C_1$ , dx = 0 $ (since $ x$ is constant), so the integral becomes:
$$
\int_{-1}^{1} -x dy = \int_{-1}^{1} -(-1) dy = \int_{-1}^{1} dy = y\Big|_{-1}^{1} = 2
$$ On $C_2$ , $dy = 0$ (since $y$ is constant), so the integral becomes: $$
   \int_{-1}^{1} y^2 dx = \int_{-1}^{1} 1^2 dx = \int_{-1}^{1} dx = x\Big|_{-1}^{1} = 2
   $$ On $C_3$ , similar to $C_1$ , $dx = 0$ , so the integral becomes: $$
   \int_{1}^{-1} -x dy = \int_{1}^{-1} -(1) dy = \int_{-1}^{1} dy = y\Big|_{-1}^{1}  = 2
   $$ On $C_4$ ,  so the integral becomes: $$
   \int_{1}^{-1} y^2 dx = \int_{1}^{-1} (-1)^2 dx = -\int_{-1}^{1} dx = -x\Big|_{-1}^{1}  = -2
   $$ Therefore, the total integral around $C$ is $2 + 2 + 2 -2 = 4$ . b)To correctly apply Green's Theorem for the line integral around the boundary $C $ of the square $[-1,1]\times[-1,1]$ , we need to consider the functions $ P $ and $ Q $ as given in the integral: [
\int_C y^2 dx - x dy
] Here, $ P(x, y) = y^2 $ and $ Q(x, y) = -x $ . According to Green's Theorem, the line integral over the closed curve $ C $ can be transformed into a double integral over the region $ R $ enclosed by $ C $ : $$
\oint_C P\,dx + Q\,dy = \iint_R \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right) dA
$$ First, we calculate the partial derivatives of $ P$ and $ Q $ : $$\frac{\partial Q}{\partial x} = \frac{\partial (-x)}{\partial x} = -1$$ $$(\frac{\partial P}{\partial y} = \frac{\partial (y^2)}{\partial y} = 2y$$ So, the double integral becomes: $$
\iint_R (-1 - 2y) dA
$$ The region $ R $ is the square $[-1,1] \times [-1,1]$ , thus the integral is over $ x $ from $-1$ to $1$ and $ y $ from $-1$ to $1$ . Calculating the double integral: $$
\int_{-1}^{1} \int_{-1}^{1} (-1 - 2y) dx dy=-4$$","['integration', 'greens-theorem', 'multivariable-calculus', 'line-integrals']"
4890235,A remarkable Definite Integral by Glasser 2013,"Just was stuck by "" A remarkable Definite Integral "" From formula (3), if I substitute $a=1$ , then \begin{align}
\int_{-\infty}^{\infty}\frac{2e^{-(x^2+i\pi x)t}}{e^x+e^{-x}}dx=\pi e^{-\frac{\pi^2}{4}t}
\end{align} But I couldn't proceed further. Would someone can give hints on what tricks can be used to solve this integral? EDIT reminded by the comments that I have limited myself through converting $\cosh$ to $e^x+e^{-x}$ and then missed possibility of the pole $\frac{i\pi}{2}$ .","['integration', 'analysis']"
4890262,Maximum number of colors for embedding all colored perfect matchings in the complete graph,"For given $n>1$ I'm looking for the maximum number $k$ of colors such that there exists a $k$ -coloring of the edges of the complete graph on $2n$ vertices with the property that a copy of every possible $k$ -colored perfect matching is present in the colored complete graph. For example, we know that there are $(n+1)$ 2-colored perfect matchings. On the other hand, we know that the complete graph can be decomposed into $(2n-1)$ perfect matchings (cf. Theorem 3.2 in 1-factorizations by Wallis). This gives $k>1$ . On the other hand, for $(2n-1)$ colors we need to give each of the disjoint perfect matchings a dedicated color to accomodate the monochromatic perfect matchings. But then a perfect matching with $(n-1)$ edges of color $1$ and one edge of color $2$ cannot be present. So we have $k<2n-1$ . Can we improve upon these bounds? EDIT : The problem seems to be harder than I thought, so I narrow it down. Prove or disprove that $k=\omega(1)$ .","['graph-theory', 'combinatorics', 'ramsey-theory', 'discrete-mathematics', 'extremal-graph-theory']"
4890279,The fundamental group of $\Pi_{n=1}^\infty S^1$ with the $\operatorname{sup}$-metric,"Consider $S^1 \subseteq \mathbb{C}$ with the Euclidean metric. Let $X = \Pi_{n=1}^\infty S^1$ as a set and define $d_\infty: X \times X \to [0, 2], d_\infty((x_n)_{n=1}^\infty, (y_n)_{n=1}^\infty) = \operatorname{sup}_{n=1}^\infty d(x_n, y_n)$ Now $(X, d_\infty)$ is a metric space. What is the fundamental group of this space? Let $G = \{(a_n)_{n=1}^\infty \in \Pi_{n=1}^\infty\mathbb{Z} \mid \exists C \in \mathbb{Z}_{\geq 0} \forall n \in \mathbb{N} : |a_n| \leq C\}$ . I believe that we should have $\pi_1(X) = G$ . The corresponding paths are defined by $\varphi_{(a_n)_{n=1}^\infty}: S^1 \to X, z \mapsto (z^{a_n})_{n=1}^\infty$ for $(a_n)_{n=1}^\infty \in G$ . The reason I believe that $\pi_1(X) = G$ is that the fundamental group of $X$ endowed with the product topology is simply $\Pi_{n=1}^\infty\mathbb{Z}$ , and $G \subseteq \Pi_{n=1}^\infty\mathbb{Z}$ is precisely the subgroup of elements whose corresponding paths are still continuous when viewed as mappings from $S^1$ to $X$ equipped with $d_\infty$ . Is this correct?","['fundamental-groups', 'general-topology', 'homotopy-theory', 'algebraic-topology']"
4890290,Prove that removing an open 2-cell from $S^2$ results in a contractible space,Let $X$ be a cellular decomposition of $S^2$ . I want to show that if $r\in X^{(2)}$ then $X\setminus \text{Int}(r)$ is a contractible space. I don't know much topology so I don't know if this it completely trivial or requires a bit of work. I can see why it should be true but I cannot rigorously prove it.,"['general-topology', 'cw-complexes', 'algebraic-topology']"
4890338,weak closedness of a set with bounded functions,"Let $A$ be a compact topological space equipped with the Borel $\sigma$ -algebra, and $X=B_b(A)$ be the vector space of bounded   measurable functions. Let $Y=\mathcal M(A)$ be the vector space of finite signed measure on $A$ . Define the dual pair $<\cdot, \cdot>$ between $(X,Y)$ such that $
<f, \mu >=\int_A f(a)\mu(da)
$ .
Let $\sigma(X,Y)$ be the weakest topology such that for all $\mu\in Y$ , the linear map $X\ni f\mapsto <f,\mu>\in \mathbb R$ is continuous. Define the set $U=\{f\in X\mid \sup_{a\in A}|f(a)|\ge 1\}$ . Is the set $U$ closed in the $\sigma(X,Y)$ topology?
I am not sure how to proceed to prove or disprove the claim.","['weak-convergence', 'general-topology', 'measure-theory']"
4890429,Question on literature for contraction rates,"I read in some lecture notes the following definition of contraction rate: Definition (Posterior rate of contraction) The posterior distribution $\Pi_n\left(\cdot \mid X^{(n)}\right)$ is said to contract at rate $\epsilon_n \rightarrow 0$ at $\theta_0 \in \Theta$ if $\Pi_n\left(\theta: d\left(\theta, \theta_0\right)>M \epsilon_n \mid X^{(n)}\right) \rightarrow 0$ in $P_{\theta_0}^{(n)}$ probability, for a sufficiently large constant $M$ as $n \rightarrow \infty$ . Q : Is there literature that treats results of the type: For any $\eta >0$ , $$\Pi_n\left(\theta: d\left(\theta, \theta_0\right)> \eta \epsilon_n \mid X^{(n)}\right) \rightarrow 0$$ in $P_{\theta_0}^{(n)}$ probability,  as $n \rightarrow \infty$ . This is a slightly stronger type of contraction, but I could not find anything in the literature. Any reference is highly appreciated. What I am really interested in is: under which conditions, under the posterior: $$ \epsilon_n^{-1} d\left(\theta, \theta_0\right) \rightarrow 0 $$","['measure-theory', 'statistics', 'bayesian', 'nonparametric-statistics', 'probability-theory']"
4890451,Reference for the following claim,"In the paper On The Closure of Characters and the Zeros of
Entire Functions by Beurling and Malliavin they make the following claim in the introduction. The closure radius $\rho = \rho(\Lambda)$ defined as the upper bound of the numbers $r$ such that set $\{e^{i \lambda x}\}_{\lambda \in \Lambda}$ span the space $L^2(-r, r)$ (by span we mean that the span of the set $\{e^{i\lambda t}\}_{\lambda \in \Lambda}$ is dense in $L^2(-r, r)$ ). The claim is that $\rho(\Lambda)$ does not change if the metric is replaced with any other $L^p$ metric. In other words, if I understand correctly the claim is that $\rho(\Lambda)$ is independent of $p$ in $L^p$ . How can be true? Surely, the topology should affect this somehow.","['reference-request', 'fourier-analysis', 'functional-analysis', 'real-analysis']"
4890488,Union of two events is at least as likely as the product of the events' probabilities [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . Improve this question If $A$ and $B$ aren't disjoint and $A \cup B \neq \Omega$ , then is $P(A \cup B) \geq P(A)P(B)$ ? My only idea is to use $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ but there's a minus in front of the intersection and the events don't have to be independent.",['probability']
4890519,dual space of l2 with strange norm,"Consider $\displaystyle(\ell_2, \lVert\cdot\rVert_\star), \lVert x\rVert_\star = \sum\limits_{k=1}^{\infty}\frac{|x(k)|}{k}$ . What is its dual space? Is this space reflexive? My idea is to consider $\displaystyle\varphi: (\ell_2, \lVert\cdot\rVert_\star) \rightarrow (\ell_1, \lVert\cdot\rVert_1), \varphi(\{x(k)\}_{k=1}^\infty) = \left\{ \frac{x(k)}{k} \right\}_{k=1}^\infty$ . Then $\lVert\varphi(x)\rVert_1 = \lVert x\rVert_\star$ and $\varphi$ is isometric isomorphism. But $\text{Im}\varphi \subsetneq \ell_1$ becasue, for example, $\displaystyle\left\{\frac{1}{k^{3/2}}\right\} \in \ell_1, \left\{\frac{1}{k^{1/2}}\right\} \notin \ell_2$ . So how to find dual space in this case?","['banach-spaces', 'normed-spaces', 'functional-analysis', 'dual-spaces', 'sequences-and-series']"
4890575,Show that $ \mathbb P(X \in U)=\sup\limits \left \{\mathbb P(X \in K): K \subseteq U \text { is compact} \right \}.$,"Show that for any real random variable $X$ and any proper open subset $U$ of $\mathbb{R}$ , we have $$\mathbb P(X \in U)=\sup\limits \left \{\mathbb P(X \in K): K \subseteq U \text { is compact} \right \}.$$ My Attempt $:$ Let $U \subseteq \mathbb R$ be open. Let $$\alpha : = \sup\limits \left \{\mathbb P (X \in K)\ :\ K \subseteq U\  \text {compact} \right \}.$$ For each $n \geq 1,$ consider the set $$K_n : = \left \{x \in U\ :\ d \left (x, U^c \right ) \geq \frac {1} {n}\ \text {and}\ \left \lvert x \right \rvert \leq n \right \}.$$ Then each $K_n$ is compact since the function $f : x \mapsto d \left (x, U^c \right )$ is
continuous and $$K_n = f^{-1} \left ( \left [\frac {1} {n}, \infty \right ) \right ) \cap [-n,n].$$ Thus each $K_n,$ being the intersection of a closed set and a compact set
(in the Hausdorff space $\mathbb R),$ is compact.
Now for each $x \in U,$ there exists $m_1 \in \mathbb N$ such that $\left \lvert x \right \rvert \leq m_1.$ Since $U^c$ is closed it follows that $d \left (x, U^c \right ) > 0$ and hence by Archimedean property there exists $m_2 \in \mathbb N$ such that $d \left (x, U^c \right ) \geq \frac {1} {m_2}.$ Let $m_3 : = \max\limits \left \{m_1, m_2 \right \}.$ Then $x \in K_{m_3} \subseteq \bigcup\limits_{n=1}^{\infty} K_n.$ Since $K_n \subseteq U$ for each $n \geq 1,$ it follows that $$U = \bigcup\limits_{n=1}^{\infty} K_n.$$ Since finite union of compact sets is compact, replacing $K_n$ by $K_n^{\prime} : = \bigcup\limits_{j=1}^{n} K_j \subseteq U,$ it follows that $U$ can be written as a countable
increasing union of compact sets $K_n^{\prime}$ contained in $U.$ Then by the continuity of the
probability measure from below it follows that $$\mathbb P (X \in U) = \lim\limits_{n \to \infty} \mathbb P \left (X \in K_n^{\prime} \right).$$ Hence $\alpha \geq \mathbb P (X \in U).$ But since $\mathbb P (X \in K) \leq \mathbb P (X \in U),$ for each
compact set $K \subseteq U,$ we also have $\alpha \leq \mathbb P (X \in U).$ Thus $\alpha = \mathbb P (X \in U),$ as required. $\square$ Is it fine what I did? Thanks for reading.","['measure-theory', 'probability-theory']"
4890586,How to rigorously prove that $\sum\limits_{n=1}^ \infty( \frac{1}{4n-1} - \frac{1}{4n} )=\frac{\ln(64)- \pi}{8}$?,"How to rigorously prove that $\sum\limits_{n=1}^ \infty\left( \frac{1}{4n-1} - \frac{1}{4n}\right) =\frac{\ln(64)- \pi}{8}$ ? My attempt $$f_N(x):= \sum_{n=1}^ N \left(\frac{x^{4n-1}}{4n-1} - \frac{x^{4n}}{4n}\right)$$ $$f_N'(x) = \sum_{n=1}^ N( x^{4n-2}- x^{4n-1})= x^{4}\left(\frac{1-x}{x^2}  \right)\frac{x^{4N+4}-1 }{x^4-1}$$ I need to show that $x^{4}\left(\frac{1-x}{x^2}  \right)\frac{x^{4N+4}-1 }{x^4-1}$ converges uniformly to be able to interchange the derivative and the summation, but I don't think $f_N'$ converges uniformly because $$f_N'(x) = \frac{-x^2}{(1+x)(1+x^2)}\cdot (x^{4N+4}-1)  $$ and $(x^{4N+4}-1)$ doesn't converge uniformly on $[0,1 )$ . Here I got stuck but for some reason it works, i.e., $\int_0 ^1 \frac{x^2}{(1+x)(1+x^2)}= \frac{\ln(64)- \pi}{8} $ so the derivative could be interchanged with the summation here, but how ?","['summation', 'real-analysis', 'uniform-convergence', 'limits', 'derivatives']"
4890617,The selection of the direction of the auxiliary curve when applying green's theorem to line integral with a singular point seems to change the answer,"Problem Compute $$
\oint_L\frac{xdy-ydx}{4x^2+y^2}
$$ where $L$ is a circle centered at $(1, 0)$ with a radius of $R > 1$ , and the direction of $L$ is counterclock-wise. Solution To by pass the singular point at $(0, 0)$ which is inside the circle $L$ surrounds, adding an auxiliary curve: $$
C:4x^2+y^2 = \delta^2
$$ The integral can then be computed by applying Green's theorem. However, different choices of the direction of $C$ produce different answers. If we choose counterclock-wise: $$
\oint_L\frac{xdy-ydx}{4x^2+y^2}=\oint_{L+C}-\oint_{C}=-\oint_{C}=-\pi
$$ ， if we choose clockwise: $$
\oint_L\frac{xdy-ydx}{4x^2+y^2}=\oint_{L+C^{-1}}-\oint_{C^{-1}}=\oint_{C}=\pi
$$ What I got wrong here?","['multivariable-calculus', 'calculus', 'line-integrals']"
4890626,Source of the definition of integrating a form along a curve in a manifold,"Suppose that $M$ is a smooth manifold. Let $\omega$ be an $n-$ form on $M$ with compact support. Then we define $\int_M\omega$ using partitions of unity. If $M$ is covered by a single chart $h:M\to \mathbb R^n$ , then we define $\int_M\omega:= \int_{\mathbb R^n} (h^{-1})^\ast \omega$ , where $\ast$ denotes pullback. $\tag 1$ But often the following definition is stated: $\int_{\gamma} \omega := \int_{[0,1]} \gamma^\ast \omega$ , where $\gamma:[0,1]\to M$ is a smooth curve and $\omega$ is a $1$ - form on $M$ . $\tag 2$ My questions are: $(a)$ what is the source of the definition in $(2)$ ? $(b)$ Does this somehow follow from the definition in $(1)$ ? I think the answer to $(b)$ is no because taking the definition in $(1)$ to be a general definition, the term $\int_{\gamma} \omega$ makes sense iff $\gamma[0,1]$ is a $1$ - manifold but that's not the case in general: Smooth image of a $1$ - manifold is not necessarily a manifold. That brings me back to $(a)$ . I didn't find the definition $(2)$ neither in Tu's book nor in Lee's book nor in Spivak's. Can anyone please direct me to where the definition $(2)$ has been stated?","['integration', 'reference-request', 'multivariable-calculus', 'algebraic-topology', 'differential-geometry']"
4890641,Existence of canonical form for cubic and quartic form?,"I am a post graduate student who is currently studying some optimization for quadratic form. From the class lecture, I know that we can always turn any quadratic functions into theirs corresponding canonical form using some changes of variable. For example: $f\left( {x,y} \right) = x \times y$ can be rewritten as $f\left( {X,Y} \right) = \frac{1}{4}{X^2} - \frac{1}{4}{Y^2}$ through the following change of variable $\left\{ {\begin{array}{*{20}{c}}
{X = x + y}\\
{Y = x - y}
\end{array}} \right.$ . Out of pure curiosity, my question is: Does "" canonical cubic form "" and "" canonical quartic form "" for function of the form $f\left( {x,y,z} \right) = xyz$ and $g\left( {x,y,z,t} \right) = xyzt$ respectively even existed ? If these canonical form existed then what would be a systematic way to find them ? Thank you for your enthusiasm !","['canonical-transformation', 'algebraic-geometry', 'linear-algebra', 'transformation', 'quadratic-forms']"
4890645,"Find the inverse Laplace transform of F(s) = 1/(s+exp(-sτ)), where τ is a positive real parameter.","I'm looking for the inverse Laplace transform of $$F(s) = \frac{1}{s + e^{-s\tau}}$$ where τ is a positive real parameter. I am trying to use general inverse formula of Laplace transformation to solve it. But then, I need to find the singularities of F(s), that is, $ s + e^{-s\tau} = 0$ . Transform the euqation, I can get $ \tau = \frac{log(-s)}{(-s)}$ . Seems that the number of singularities depends on the value of parameter $\tau$ .Then, question comes to me, how to find the residue at those possible singulaties? And then how to proceed the calculation for the general inverse formula? Many thanks in advance for your advice.","['inverse-laplace', 'laplace-transform', 'complex-analysis', 'partial-differential-equations', 'residue-calculus']"
4890654,"Prove that $0 \leq u(x) \leq 1$ for every $x \in [0, a]$ if $u''(x) + u(x) (1 - u(x)) = 0$ and $u(0) = u(a) = 0$, where $a > 0$","Let $a$ be a positive number. Suppose that $u$ : $[0, a] \to \mathbb{R}$ is a continuous function, that $u(0) = u(a) = 0$ , that the first- and second-order derivatives of $u$ are continuous in $(0, a)$ , and that $u''(x) + u(x) (1 - u(x)) = 0$ for every $x \in (0, a)$ . Prove that $0 \leq u(x) \leq 1$ for every $x \in [0, a]$ . One part is easy. Assume on the contrary that $u(t_0) > 1$ for some $t_0 \in [0, a]$ . Since $u$ is continuous, its maximum exists. There exists some $t_1 \in [0, a]$ such that $u(t_1) \geq u(x)$ for every $x \in [0, a]$ . Then $u(t_1) \geq u(t_0) > 1$ and by calculus, $u''(t_1) \leq 0$ (note that $t_1$ cannot be $0$ or $a$ ). Then $$
0 = u''(t_1) + u(t_1) (1 - u(t_1)) < 0,
$$ a contradiction. The other part does not seem to be easy, however. I tried to do it by mimicking the way in which I solved the part above, but I did not get a contradiction. The reason for which I raised the question: I take a course in partial differential equations. The instructor uses the second edition of An Introduction to Nonlinear Partial Differential Equations by J. David Logan as the textbook. I am actually concerned about the existence of the solution to some boundary value problem. Here is the context. $D$ is an open, bounded, connected subset of $\mathbb{R}^n$ . The operator $L$ is ""uniformly elliptic"" in $D$ , which means that $L$ is of the form $$
Lu := \sum_{1 \leq i,j \leq n} {a_{i,j} (x) \frac{\partial^2 u}{\partial x_i \partial x_j}} + \sum_{1 \leq j \leq n} {b_j (x) \frac{\partial u}{\partial x_j}},
$$ where the coefficients $a_{i,j}$ and $b_j$ are continuously differentiable functions on $D$ that are continuous on $D$ , and that there exists a positive number $\mu$ such that $$
\sum_{1 \leq i,j \leq n} {a_{i,j} (x) \xi_i \xi_j} \geq \mu \sum_{1 \leq j \leq n} {\xi_j^2}
$$ for every $x \in D$ and $\xi_j \in \mathbb{R}$ . (Note that there are typos in (7.2.7) and (7.2.8); $(x, y)$ , instead of $x$ , should be used.) The example intends to demonstrate how the existence theorem is used. The author tries to find an upper solution $\overline{u}$ , and a lower solution $\underline{u}$ that is not greater than $\overline{u}$ . I do not understand why the author says ""Then $\overline{u}(x) \geq 0$ on $\partial D$ and $-\Delta \overline{u} = -{\overline{u}}'' = \overline{u} (1 - \overline{u}) \geq 0$ ."" That $\overline{u} \leq 1$ has been justified above, but the other part has not. After reading your replies, I guess that the author makes a mistake here. If so, what should I do in order to amend the demonstration?","['calculus', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
4890717,"Attaching a copy of $S^1$ at each point of $[0, 1]$ and $S^1$ and the fundamental groups of the resulting spaces","Given a set $X$ and a disjoint collection $R \subseteq \operatorname{Pot}(X)$ , define a relation $\sim$ on $X$ via $x \sim y \Leftrightarrow x = y \text{ or } \exists S \in R: x \in S \text{ and } y \in S$ . Define $X/R = X/\sim$ as the set of equivalence classes. Also, if $X$ is equipped with a topology, equip $X/R$ with the quotient topology. Consider $S^1 \subseteq \mathbb{C}$ . For each $t \in [0, 1]$ , let $S_t = S^1$ be a copy of the circle, one for each point of $[0, 1]$ . Let $A = \bigsqcup_{t \in [0, 1]} S_t$ be their disjoint union. Let $B = A/\{\{1 \in S_t \mid t \in [0, 1]\}\} $ . This means that $B$ is obtained by gluing all circles in $A$ at one point. Let $C = ([0, 1] \sqcup A)/\{\{t \in [0, 1], 1 \in S_t\} \mid t \in [0, 1]\}$ . This means that $C$ is obtained by gluing a point of each circle in $A$ to a point of $[0, 1]$ . Is the mapping $\varphi: C \to B, t \in [0, 1] \mapsto 1 \in B, z \in S_t \setminus \{1\} \mapsto z \in S_t \setminus \{1\}$ that contracts $[0, 1]$ to the point a homotopy equivalence? What is the fundamental group $\pi_1(B)$ ? What is the fundamental group $\pi_1(C)$ ? I would guess that $\pi_1(B)$ is the free group on uncountably many generators, since $B$ is the wedge sum of uncountably many circles. For each $z \in S^1$ , let $S_z = S^1$ be a copy of the circle, one for each point of $S^1$ . Let $D = \bigsqcup_{z \in S^1} S_z$ be their disjoint union. Let $E = (S^1 \sqcup D)/\{\{z \in S^1, 1 \in S_z\} \mid z \in S^1\}$ . This means that $E$ is obtained by gluing a point of each circle in $D$ to a point of $S^1$ . What is the fundamental group $\pi_1(E)$ ?","['fundamental-groups', 'general-topology', 'homotopy-theory', 'algebraic-topology']"
4890816,The map $N\rightarrow N^N\bmod (2N+1)$ and the number $13612$,"Recently, i've been experimenting with the map $N \rightarrow N^N \bmod (2N+1)$ . What i would do is repeated apply this map to some numbers. There's a lot of cycles with this map, but i found a particularly massive one: $13612, 15106, 27724, 27553, 29074, 53239, 76162, 135319, 103369, 201064, 323761, 202351, 24889, 15556$ This cycle has a period of $14$ . It is very long and i couldn't find any other cycle that is nearly as long as this one (and not for lack of trying). The best i could find are $2$ cycles of length $7$ : $$554782, 923989, 578686, 1081525, 827113, 1001092, 634036$$ $$603229, 661333, 1166386, 1343245, 1772455, 1085395, 1819786$$ Can anyone prove or disprove the claim that the cycle starting with $13612$ is the largest cycle there is?","['number-theory', 'modular-arithmetic', 'elementary-number-theory', 'sequences-and-series']"
4890833,Radius of Convergence of Laurent Series Confusion,"Determine the largest number $R$ such that the Laurent series of $$f(z)= \dfrac{2sin(z)}{z^2-4} + \dfrac{cos(z)}{z-3i}$$ about $z=-2$ converges for $0<|z+2|<R$ ? I know the maclaurin series for sine and cosine which are valid for all complex numbers. For $\frac{1}{z^2-4} = -\frac{0.25}{z+2} + \frac{0.25}{z-2} = \frac{-0.25}{z+2} + \frac{0.25}{-4+(z+2)} = \frac{-0.25}{z+2} + \frac{-1}{4}\frac{0.25}{1-(\frac{z+2}{4})}$ , which is only valid for | $\frac{z+2}{4}$ | < 1 which means $R=4$ as of now when applying the geometric series. For $\frac{1}{z-3i} = \frac{1}{z+2-(2+3i)} = \frac{-1}{2+3i}\frac{1}{1-\frac{z+2}{2+3i}}$ . When applying the geometric series only valid on $|\frac{z+2}{2+3i}| < 1$ so $R = \sqrt{13}$ . Is this right?","['complex-analysis', 'taylor-expansion', 'analysis', 'sequences-and-series']"
4890844,"If $x,y\in\mathbb{N},\varepsilon>0$ then are there infinitely many positive integer pairs $(n,m)$ s.t. $\vert\frac{x^n}{y^m}- 1\vert < \varepsilon?$","Proposition: If $x,y\in\mathbb{N}_{\geq2}$ then for any $\varepsilon>0,$ there
are infinitely many pairs of positive integers $(n,m)$ such that $$\frac{\left\lvert
 y^m-x^n \right\rvert}{y^m} < \varepsilon,$$ i.e. $\displaystyle\large{\frac{x^n}{y^m}} \to 1\ $ as these pairs $(m,n) \to (\infty,\infty).$ I think this is true, and I want to prove it. For all integers $n,$ we have $$\frac{x^n}{y^{ {n\log_y x}}} = 1.$$ Therefore, we want to find integers $n$ such that $n\log_y x$ is, in some sense, extremely close to an integer. This above question can also be stated as follows. If $x,y\in\mathbb{N}_{\geq2}$ and $x>y,$ then either $\ \displaystyle\limsup_{n\to\infty} \frac{x^n}{y^{\lceil n(\log_y x)\rceil}} = 1 $ or $\ \displaystyle\liminf_{n\to\infty} \frac{x^n}{y^{\lfloor n(\log_y x)\rfloor}} = 1. $ Can we use Dirichlet's approximation theorem to prove this, or the fact that $\{ n\alpha: n\in\mathbb{N} \} $ is dense in $[0,1]$ for irrational $\ \alpha\ ?$ Or do we have to use other tools?","['number-theory', 'diophantine-approximation']"
4890877,Deriving the Likelihood Ratio for a Normal Distribution Hypothesis Testing,"I'm exploring the likelihood ratio principle in hypothesis testing, specifically within the context of normal distributions, and I've encountered a challenge in deriving a specific likelihood ratio. The principle is typically used to select a suitable statistic for testing hypotheses, where we compare the likelihood of the data under the null hypothesis against an alternative hypothesis. Consider a scenario where we have a set of samples $x_1, \ldots, x_n$ that are independently and identically distributed (i.i.d.) from a normal distribution $N(\mu, \sigma^2)$ with unknown parameters $\mu$ and $\sigma^2$ . We're interested in testing the null hypothesis $H_0: \mu = \mu_0$ against the alternative $H_a: \mu \neq \mu_0$ , where $\mu_0$ is a specified value. Let $L(\widehat{\Omega}_0)$ denote the maximum likelihood of observing the samples given $\mu = \mu_0$ , and let $L(\widehat{\Omega})$ denote the maximum likelihood over all possible values of $\mu$ and $\sigma^2$ . According to the likelihood ratio principle, the rejection region for $H_0$ is determined by the ratio $\frac{L(\widehat{\Omega}_0)}{L(\widehat{\Omega})}$ being less than or equal to a critical value $c$ , which is chosen based on the desired level of statistical significance $\alpha$ . I am trying to show that, for this particular setup, the likelihood ratio simplifies to $\left(1 + \frac{t^2}{n-1}\right)^{-\frac{n}{2}}$ , where $t$ is the test statistic defined as $t = \frac{\overline{x} - \mu_0}{s / \sqrt{n}}$ , with $\overline{x}$ being the sample mean and $s^2$ the unbiased sample variance. I've made several attempts to derive this expression from the definition of the likelihood ratio, considering the probability density function of the normal distribution, but I'm not sure how to proceed. Could someone guide me through the derivation or point out any resources that could help with understanding this specific case of the likelihood ratio in hypothesis testing for normal distributions? $\overline{x}= \frac{1}{n}\Sigma_{i=1}^{n} x_i $ and $s^2 = \frac{1}{n-1}\Sigma_{i=1}^{n}\left(x_i-\overline{x} \right)^2 $","['statistical-inference', 'statistics', 'hypothesis-testing']"
4890886,Ratio of Expected Hitting Times of Brownian Motion with Drift,"Suppose $W_t$ is Brownian motion and consider the following two stopping times: $$\tau_a \equiv \inf \{t \ge 0 : W_t + at \ge b(t) \} \wedge T$$ and $$\tau_{-a} \equiv \inf\{t \ge 0: W_t - at \ge b(t)\} \wedge T$$ for some $T > 0$ , $a > 0$ , and an arbitrary (strictly positive) boundary $b(\cdot)$ . Obviously $\tau_a \leq \tau_{-a}$ by construction.  Is it true that $$\frac{E(\tau_{-a})}{E(\tau_{a})} \leq C$$ for some constant $C$ ? I have tried to prove it in the following way: We convert to a measure (denoted by $\widetilde{E}$ ) where $B_t \equiv W_t - 2at$ is Brownian motion via Girsanov's theorem. In particular, we have, denoting the stochastic exponential by $\mathcal{E}$ , $$E(\tau_{-a}) = \widetilde{E}(\tau_{-a} \mathcal{E}(-2aB_{\tau_{-a}}))$$ Noting that $\tau_{-a} = \inf\{t \ge 0 : B_t + at \ge b(t)\}$ , we see that $(B_{\tau_{-a}},\tau_{-a})$ has the same law under the measure denoted $\widetilde{E}$ as $(W_{\tau_a}, \tau_{a})$ has under the original measure denoted $E$ . Hence, it suffices to bound the following: $$\frac{E(\mathcal{E}(-2aW_{\tau_a}) \tau_{a})}{E(\tau_{a})}.$$ Is this possible?","['stochastic-analysis', 'stochastic-processes', 'stopping-times', 'probability-theory', 'stochastic-calculus']"
4890910,Does every $\sigma$-homomorphism of a probability algebra come from a measure preserving transformation on the probability space?,"Let $(X,\mathcal{A},\mu)$ be an atomless probability space and let $A\sim B$ whenever $\mu(A\vartriangle B)=0$ for each $A$ and $B$ in $\mathcal{A}$ . This way, if $\mathbb{A}$ is the set of $\sim$ -equivalences classes in $\mathcal{A}$ , then $\mathbb{A}$ inherits $\cap,\cup,\cdot^c$ and $\mu$ from $(X,\mathcal{A},\mu)$ , becoming a probability algebra. We can define a complete metric in $\mathbb{A}$ by $d([A],[B]):=\mu(A\vartriangle B)$ , making it a structure in the sense of continuous model theory. Given any measure preserving trasnformation $T:X\rightarrow X$ , it defines a measure preserving $\sigma$ -homomorphism $\check{T}:\mathbb{A}\rightarrow\mathbb{A}$ by $\check{T}([A]):=[T^{-1}(A)]$ . Is it true that for any measure preserving $\sigma$ -homomorphism $\tau:\mathbb{A}\rightarrow\mathbb{A}$ there exists a measure preserving transformation $T:X\rightarrow X$ such that $\tau=\check{T}$ ? Is there a reference for such a theorem? If not, could you give a counterexample of such a $\tau$ ? I know that this is true if $(X,\mathcal{A},\mu)$ is a Lebesgue-standard space (i.e. if $\mathbb{A}$ is separable), it is a theorem in Royden. But is it still true if $\mathbb{A}$ has a higher metric density? It would be perfectly ok if this answer is restricted to just automorphisms. Thanks, I appreciate any answer.","['measure-theory', 'model-theory', 'ergodic-theory', 'logic', 'analysis']"
4890923,Markov Property of a Ito Process,"This is the excercise: Let $B_t$ be 1d Brownian motion with $B_0=0$ . Define $$
X_t=X_t^x=x\cdot\exp\left(ct+\alpha B_t\right)
$$ where $c,\alpha$ are constants and $x$ is non-random. Prove directly from the definition that $X_t$ is a Markov process, i.e., that $$
\mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]=\mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right]
$$ for bounded Borel-measurable $f$ . Here is my attempt Let $\mathcal{F}_t^X:=\sigma\left(X_s:s\le t\right)$ and $\mathcal{F}_t^B:=\sigma\left(B_s:s\le t\right)$ . Since $X_s$ is $\mathcal{F}_t^B$ -measurable then $\mathcal{F}_t^X\subset\mathcal{F}_t^B$ we have, for any bounded Borel function $f(x)$ , $$
\begin{aligned}
\mathbb{E}^x\left[f\left(X_{t+h}\right)\bigg|\mathcal{F}_t^X\right]&=\mathbb{E}^x\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\bigg|\mathcal{F}_t^X\right]\\
&=\mathbb{E}^x\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\bigg|\mathcal{F}_t^B\right]\\
&=\mathbb{E}^{B_t}\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\right]\in\sigma(B_t)=\sigma(X_t).
\end{aligned}
$$ So $\mathbb{E}^x\left[f\left(X_{t+h}\right)\bigg|\mathcal{F}_t^X\right]=\mathbb{E}\left[f\left(X_{t+h}\right)|X_t\right]$ Let $\mathcal{M}_t=\mathcal{F}_t^X:=\sigma\left(X_s:s\le t\right)$ and $\mathcal{F}_t=\mathcal{F}_t^B:=\sigma\left(B_s:s\le t\right)$ . Since $X_s$ is $\mathcal{F}_t^B$ -measurable then $\mathcal{F}_t^X\subset\mathcal{F}_t^B$ we have, for any bounded Borel function $f(x)$ , $$
\begin{aligned}
\mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]&=\mathbb{E}\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\bigg|\mathcal{F}_t\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+ch+\alpha B_{t+h}\pm\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha\left( B_{t+h}-B_{h}\right)+ch+\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha\left(B_{t+h}-B_{h}\right)\right)\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right]
\end{aligned}
$$ Notice that $\hat{B_{t}}=B_{t+h}-B_{h}$ is a Brownian Motion and it is equal in distribution to $B_{t}$ , so $$
\begin{aligned}
\mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]&=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha\left(B_{t+h}-B_{h}\right)\right)\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right]\\
&=\mathbb{E}\left[f\left(\exp\left(ct+\alpha\hat{B_{t}} \right)\cdot x\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(B_s:s\le t\right)\right]\\
&=\mathbb{E}\left[f\left(\exp\left(ct+\alpha\hat{B_{t}} \right)X_h\right)\bigg|\sigma\left(B_s:s\le t\right)\right]\\
&=\mathbb{E}\left[f\left(\exp\left(ct+\alpha B_t \right)X_h\right)\bigg|\sigma\left(B_s:s\le t\right)\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha B_t \right)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(B_s:s\le t\right)\right]
\end{aligned}
$$ And at the same time, if we where to start at $y=X_t=x\cdot\exp(ct+\alpha B_t)$ , we would have that $$
\begin{aligned}
\mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right]&=\mathbb{E}\left[f\left(y\cdot\exp\left(ch+\alpha B_{h}\right)\right)\right]\bigg|_{y=X_t}\\
&=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|X_t\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(X_s:s\le t\right)\right].
\end{aligned}
$$ Since $\sigma(X_t)=\sigma(B_t)$ , we have that $$
\begin{aligned}
\mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right]&=\mathbb{E}\left[f\left(y\cdot\exp\left(ch+\alpha B_{h}\right)\right)\right]\bigg|_{y=X_t}\\
&=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(X_s:s\le t\right)\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(B_s:s\le t\right)\right].
\end{aligned}
$$ In other words, we have just prove that $$
\mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]=\mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right].
$$ Could anyone please check if my reasoning is correct? Thanks","['analysis', 'stochastic-processes', 'markov-process', 'stochastic-differential-equations', 'brownian-motion']"
4890928,$r$-fold cover of an elliptic curve,"I'm trying to understand the first part of the proof of Lemma 2 in https://www.emis.de/journals/BAG/vol.46/no.2/b46h2hei.pdf . Given $r > 0$ , they claim that for an elliptic curve $E$ there exists another elliptic curve $\widetilde{E}$ with $\pi_r: \widetilde{E} \rightarrow E$ such that $E = \widetilde{E}/G$ where $G = \mathbb{Z}/r$ . They provide a construction as follows: take $M$ to be a line bundle of order $r$ , and let $\widetilde{E}= \mathcal{Spec}(\mathscr{O}_E \oplus M \oplus \cdots \oplus M^{\otimes {r-1}})$ . My first question is why does such a line bundle exist (can I always take the $r$ th root of a line bundle on an elliptic curve?), and why does $\widetilde{E}$ have the desired properties? (I don't even understand why it ends up being an elliptic curve). I understand that over $\Bbb C$ , I can just take the $r$ -fold covering space of the complex torus, but I'm looking for an answer that works more generally. Thanks.","['algebraic-geometry', 'elliptic-curves']"
4890984,Question regarding the nature of derivative of a function.,"This question is in regard to the following problem The complete range of values of $a$ such that $(\frac{1}{2})^{|x|} = x^2 - a$ is satisfied for maximum of values $x$ is: How i approached this problem is as follows, plugging in 0 in the place of $x$ gives the value of $a = -1$ rearranging the equation. $$a = x^2 - (\frac{1}{2})^{|x|}$$ If we can prove that this function is strictly increasing after $x = 0$ then we can prove that the answer to this question is one of the options. The derivative of the above function is, $$2x - \frac{ln(2)\cdot x}{2^{|x|}\cdot |x|}$$ By plugging in some arbitrary values and some observation it is quite intuitive that this derivative will always be positive from $x = 0$ to $x = \infty$ . However i wanted a concrete proof for this, could someone point me in the right direction, any help would be appreciated.",['derivatives']
4890995,"Let $Y_1, \dots, Y_n \sim \; \textrm{iid}$ with pdf $f_Y(y)$. Show that the UMVUE of $\theta$ is given by $\frac{n-1}{\sum_{i=1}^n Y_i}$ [duplicate]","This question already has an answer here : Finding UMVUE of $\theta$ when the underlying distribution is exponential distribution (1 answer) Closed 3 months ago . I'm having a difficult time figuring out where to go here. Question: Let $Y_1,\dots, Y_n$ be iid random variables with pdf $f_Y(y) = \theta e^{-\theta y} \;,\; y >0\;,\;\theta >0.$ Show that the uniformly minimal variance unbiased estimator (UMVUE) of $\theta$ is given by $
\frac{n-1}{\sum_{i=1}^n Y_i}.$ We were given the following theorem in class: Theorem: If we have $Y1,\ldots,Yn$ iid random variables and Y belongs to an exponential family (with a single parameter θ), then, under some technical conditions, we say that $
U = \sum_{i=1}^n t(Y_i)$ is a *complete and sufficient statistic for $\theta$ . My work: Our pdf belongs to an exponential family with $h(y) = 1$ , $c(\theta) = \theta$ , $w(\theta) = -\theta$ , and $t(y) = y$ . Therefore, $U = \sum_{i=1}^n Y_i$ is a complete and sufficient statistic for $\theta$ . Now, to show that it is the UMVUE of $\theta$ , I need to show that it is unbiased. $B(\hat{\theta}) = E[\hat{\theta}] - \theta$ . We can rewrite our pdf to be $\frac{1}{\frac{1}{\theta}}e^{\frac{-y}{\frac{1}{\theta}}}$ , which is the exponential with $\beta = \frac{1}{\theta}$ , so $E[Y] = \frac{1}{\theta}$ . Therefore $\begin{align}
E[\hat{\theta}] &= E[\sum_{i=1}^n Y_i] = \sum_{i=1}^n E[Y_i] = nE[Y] \notag \\
&nE[Y] = \frac{n}{\theta} \notag \\
&B(\hat{\theta}) = \frac{n}{\theta} - \theta \notag
\end{align}$ which will be unbiased if $\hat{\theta} = \frac{\theta^2}{n}E[nY] = Var(\bar{Y})E[nY]$ . At this point I feel like I've sort of lost the plot and am no longer sure what to do.","['statistics', 'sufficient-statistics', 'exponential-family', 'probability-distributions', 'parameter-estimation']"
4891008,"Is there a way to prove $\sin^{2}{A}+\sin^{2}{B}+\sin^{2}{C}\leq\frac{9}{4}$ for a triangle, without Leibniz's inequality?","Given a triangle with sides $a,b,c$ and internal angles $A,B,C$ I want to prove that $$\sin^{2}{A}+\sin^{2}{B}+\sin^{2}{C}\leq\frac{9}{4}$$ I can do this by using the circumradius of the triangle (proof below), but I want to know if there is an alternative that does not use the Leibniz inequality. (The Leibniz inequality states that for a triangle with sides length $a,b,c$ and circumradius $R$ , $a^{2}+b^{2}+c^{2}\leq 9R^{2}$ ) By the sine rule: $$\frac{\sin{A}}{a}=\frac{\sin{B}}{b}=\frac{\sin{C}}{c}=\frac{1}{2R} \tag1$$ So $$\sin^{2}{A}+\sin^{2}{B}+\sin^{2}{C}=\frac{a^{2}+b^{2}+c^{2}}{4R^{2}} \tag2$$ And since $a^{2}+b^{2}+c^{2}\leq 9R^{2}$ , it follows that $$\sin^{2}{A}+\sin^{2}{B}+\sin^{2}{C} \leq \frac{9R^{2}}{4r^{2}}=\frac{9}{4} \tag3$$ Is there a way to do this without the Leibniz inequality? Preferably without reference to the circumradius at all...","['alternative-proof', 'trigonometry']"
4891018,Convergence of series from inverse of Cauchy product,"The Cauchy product of two real or complex infinite series $\sum_{n\in\mathbb{N}} a_n$ and $\sum_{n\in\mathbb{N}} b_n$ is defined as: $$ \forall n\in\mathbb{N}, c_n = \sum_{k=0}^n a_k b_{n-k} $$ Mertens' theorem states that if one of the two series $\sum_{n\in\mathbb{N}} a_n$ and $\sum_{n\in\mathbb{N}} b_n$ is absolutely convergent then the series $\sum_{n\in\mathbb{N}} c_n$ also converge. If both $\sum_{n\in\mathbb{N}} a_n$ and $\sum_{n\in\mathbb{N}} b_n$ are absolutely convergent then $\sum_{n\in\mathbb{N}} c_n$ is also absolutely convergent. Now that we have two convergent series $\sum_{n\in\mathbb{N}} a_n$ and $\sum_{n\in\mathbb{N}} b_n$ , when $b_0\neq 0$ , by recursively defining $c_n$ as: $$ c_n = \frac{a_n - \sum_{k=0}^{n-1} c_k b_{n-k}}{b_0}, $$ we can make a series that satisfies: $$ \forall n\in\mathbb{N}, a_n = \sum_{k=0}^n c_k b_{n-k}. $$ which resembles the inverse of a Cauchy product. My question is: what's the condition on the series $\sum_{n\in\mathbb{N}} a_n$ and $\sum_{n\in\mathbb{N}} b_n$ for thus defined series $\sum_{n\in\mathbb{N}} c_n$ to be convergent? Absolutely convergent?","['real-analysis', 'complex-analysis', 'absolute-convergence', 'sequences-and-series', 'convergence-divergence']"
4891042,Differential vs derivative in Differential Geometry,"In differential geometry one shows that, associated to a smooth map between manifolds $f:M\to N$ there is a linear map $d_pf: T_pM \to T_{f(p)}N$ for each $p\in M$ which is called the differential of $f$ at $p$ . My question is: why is it called “differential” and not derivative? I understand the expression in local coordinates is in fact a Jacobian matrix. In my mind “differential” stands for the “change in $f$ ” and derivative means the rate of change of $f$ compared to the rate of change of the variable. What is the reason for this name?",['differential-geometry']
4891043,"Why is it that in some problems that involve systems of equations, solving for one unknown value, also yields the other in the second solution?","Like take the following scenario as an example: Two spherical objects have a combined mass of 200 kg. The gravitational attraction between them is $7.34 × 10^{−6}$ N when their centers are 15.0 cm apart. $$
M + m = 200
$$ $$
7.34 × 10^{−6} = \frac{GMm}{0.15^2}
$$ Solving for M through substitution, the resulting quadratic equation gives its value as one solution, but also gives m as the second solution, even though I was just solving for M. If I recall correctly, there are some problems like this where this is not the case and one would have to plug in the value into the original equation to get the second value, I suppose when it's more than just adding them together, or is it always the case, and if so, why?","['systems-of-equations', 'roots', 'intuition', 'algebra-precalculus', 'quadratics']"
4891044,On the solution of these equations,"Do not forget to see the Good News at the end of the problem. This problem is linked to the previous one , up to a changes of coordinates. However, that question is actually about only the first three equations here. I add three new equations and hope that we can treat them as a system of 6 variables. Let $(a,b,c,u,v,w)\in\mathbb{R}^6$ , $$
T := -a u -b v- c w,\quad S := T + u +v +w.
$$ Consider the following 6 equations: $$
(1-a)S^2= u^2  -a T^2,
$$ $$
(1-b)S^2 =v^2 -b T^2,
$$ $$
(1-c)S^2=w^2 -c T^2,
$$ $$
\Big( (1-a)^2 S -a^2 T-u\Big)\Big((1-b)^2 S -b^2 T-v\Big)=\Big((1-a)(1-b)S-abT\Big)^2,
$$ $$
\Big( (1-a)^2 S -a^2 T-u\Big)\Big((1-c)^2 S -c^2 T-w\Big)=\Big((1-a)(1-c)S-acT\Big)^2,
$$ $$
\Big( (1-b)^2 S -b^2 T-v\Big)\Big((1-c)^2 S -c^2 T-w\Big)=\Big((1-b)(1-c)S-bcT\Big)^2.
$$ I want to find the solutions. Original problem is: Consider the homogeneous function $$
f(u,v,w) = (u+v + w +T)^3 - (u^3 + v^3 + w^3 + T^3).
$$ I want to show if $x_0$ is a critical point $f$ , that is, $\nabla f(x_0)=0$ , then $\nabla^2 f(x_0)$ has rank 2, where $\nabla^2 f$ is the Hessian of $f$ . So I get the six equation above. How can I figure it out? Maybe the symmetry is helpful. Good News: After expressing $a,b,c$ in terms of $(T,S,u,v,w)$ from the first three equations, and then put them into the left three equations to get a system of 4 equations on $(T, u, v, w)$ . Mathematica tells me that the non-zero solutions of the original 6 equations must have the form $$
(1,0,0,u,0,0)\ \ \mbox{or}\ \ (0,1,0,0,v,0)\ \ \mbox{or} \  \  (0,0,1,0,0,w).
$$ In fact, the four new equations for $(T,u,v,w)$ now are: $$
(eq.1)\ \ S(v(T^2-u^2)^2+ u(T^2-v^2)^2)+ (u^2-v^2)^2 ST = uv(T^2-S^2)^2+ T((u^2-S^2)^2 v + (v^2-S^2)^2 u)
$$ $$
(eq.2)\ \ S(w(T^2-u^2)^2+ u(T^2-w^2)^2)+ (u^2-w^2)^2 ST = uw(T^2-S^2)^2+ T((u^2-S^2)^2 w + (w^2-S^2)^2 u)
$$ $$
(eq.3)\ \ S(w(T^2-v^2)^2+ v(T^2-w^2)^2)+ (v^2-w^2)^2 ST = vw(T^2-S^2)^2+ T((v^2-S^2)^2 w + (w^2-S^2)^2 v)
$$ $$
(eq.4) \ \ S^3 = T^3 + u^3 + v^3 + w^3,\ \ \mbox{where} \ S= T+u+v+w.
$$ You can only work on them, Mathematica tells me the real solution of this system must take the form, e.g. [ $T=-u$ , $v=w=0$ ] or [ $T=w=0$ , $u=-v$ ] (other solution are similar by symmetry).","['nonlinear-system', 'systems-of-equations', 'analysis', 'real-analysis']"
4891052,Proving that $\sum_{n=1}^{\infty}\left (\left|a_{n}\right|+\left|b_{n}\right|\right) <\infty $.,"Let $\frac{a_{0}}{2}+\sum_{n=1}^{\infty} a_{n} \cos n x+b_{n} \sin n x$ is absolutely convergent on $E$ , where $E$ is a measurable set with $mE>0$ . Prove that $\sum_{n=1}^{\infty}\left (\left|a_{n}\right|+\left|b_{n}\right|\right) <\infty $ . $a_{n} \cos n x+b_{n} \sin n x=r_{n} \cos \left (n x+\theta_{n}\right) $ , where $r_{n}=\sqrt{a_{n}^{2}+b_{n}^{2}} $ .   How to do then? I know Cantor-Lebesgue theorem: $a_n\cos nx+b_n\sin nx\to 0, x\in E$ implies that $a_n\to 0, b_n\to 0$ . But here? Using Cauchy criteria: $\sum_{k=m}^n |a_k\cos kx+b_k\sin kx|\to 0$ , as $m\to\infty$ ? But it seems not easy (or impossible) to change it to the form $a_n\cos nx+b_n\sin nx$","['sequences-and-series', 'real-analysis']"
4891095,"Please explain Figure 24.4 in ""Analysis on Manifolds"" by James R. Munkres.","I am reading ""Analysis on Manifolds"" by James R. Munkres. I am reading the proof of Theorem 24.4. I don't understand Figure 24.4. Is the solid enclosed by the red closed curve in Figure 24.4 $A\cap N$ ? Is the blue surface in Figure 24.4 $A\cap M$ ? Why is the image by $F$ in Figure 24.4 a rectangular parallelepiped?","['multivariable-calculus', 'manifolds-with-boundary']"
4891123,Rudin theorem $6.19$.,"There is theorem $6.12$ : Let $\mu$ be a complex measure on $\sigma$ -algebra $\mathfrak {M}$ in $X$ . Then there is a measurable function $h$ such that $|h(x)| = 1$ for all $x \in X$ and such that $$ d \mu = h d |\mu|.$$ There is the definition of $C_0(X)$ :
A complex function $f$ on a locally compact Hausdorff space $X$ is said to vanish at infinity if to every $\epsilon \gt 0 $ there exists a compact set $K \subset X$ such that $|f(x)| < \epsilon $ for all $x$ not in $K$ .
The class of all continuous $f$ on $X$ which vanish at infinity is called $C_0(X)$ . There is our theorem:
If $X$ is a locally compact Hausdorff space, then every bounded linear function $\Phi$ on $C_0(X)$ is represented by a unique regular complex Boreal measure $\mu$ , in the sense that $$ \Phi f = \int_X f d\mu $$ for every $f \in C_0(X)$ . Moreover, the norm of $\Phi$ is the total variation of $\mu$ : $$ || \Phi || = |\mu|(X).$$ There is the proof: We first settle the uniqueness question. Suppose $\mu$ is a regular complex Borel measure on $X$ and $\int f d\mu = 0 $ for all $f \in C_0(X)$ . By Theorem $6.12$ there is a Borel function $h$ , with $|h| = 1 $ , such that $ d \mu = h d |\mu|.$ For any sequence $\{f_n\}$ in $C_0(X)$ we then have $$ | \mu |(X) = \int_X (\bar{h} - f_n) h d |\mu| \leq \int_X | \bar{h} - f_n| d |\mu|$$ , and since $C_c(X)$ is dense in $L^{1}(|\mu|)$ , $\{f_n\}$ can be so chosen that the last expression tends to $0$ as $n \to \infty$ . I don't understand why is $|\mu|(X)$ equal to $\int_X  (\bar{h} - f_n) h d |\mu| $ . Any help would be appreciated.","['measure-theory', 'analysis', 'real-analysis', 'complex-analysis', 'functional-analysis']"
4891141,Geometry problem with external tangent of two circles,"Let's consider two circles $C_1$ and $C_2$ .
We note their intersections $A$ and $B$ . Then we draw a circle $C_3$ which is tangent interiorly with $C_1$ and $C_2$ .
Now we note $X$ the common point of $C_1$ and $C_3$ and $Y$ the common point of $C_2$ and $C_3$ .
Now we place $Z$ the intersection of $[AB]$ and $C_3$ that is closer to $A$ .
Finaly, we draw $P$ the other intersection of $XZ$ and $C_1$ and $Q$ the intersection of $YZ$ and $C_2$ How do we show that the place of $P$ and $Q$ is still the same for every choice of $C_3$ ?
I see that $PQ$ is the common tangent of $C_1$ and $C_2$ and therefore is independant of the choice of $C_3$ . But could you help me proving that $PQ$ is the external tangent ?","['euclidean-geometry', 'circles', 'geometry']"
