question_id,title,body,tags
2817663,"Finding $a_i:U→\mathbb R$ of class $C^{k-1}$ such that $f(x,y)=\sum\limits_{j=0}^ia_j(x,y)x^jy^{i-j}$","Let $f: U \to \mathbb{R}$ of class $C^{k}$ ($i \leq k \leq \infty$) in the convex open $U \subset \mathbb{R}^{2}$ containing $(0,0)$. Suppose that $f$ all partial derivatives of order $\leq i$ vanish in $(0,0)$. Prove that there exists functions $a_{0},..., a_{i}: U \to \mathbb{R}$ of class $C^{k-1}$, such that $$f(x,y) = \sum_{j=0}^{i}a_{j}(x,y)x^{j}y^{i-j}. \quad \forall (x,y) \in U$$ I didn't have a good idea to start solving, but I don't want a complete solution, I just want some hints to get started. My professor hint me to use Taylor Formula, I tried to do this, but I couldn't see how to use.","['multivariable-calculus', 'real-analysis', 'derivatives']"
2817673,Is there a standard name for doing $B^T A B$ when $B$ is not necessarily square?,"Given a (generic, rectangular) matrix $B$ and a (square) matrix $A$, is there a name for doing: $$
B^T A B\ ?
$$ My memory wanted to call this ""conjugating $A$ with $B$,"" but 
according to mathworld this is used to refer to $$
B^{-1} A B\ .
$$ (Sometimes $B^T = B^{-1}$, but obviously not usually in the general case).","['matrices', 'transpose', 'terminology', 'linear-algebra', 'definition']"
2817739,Is it possible for the derivative of a function to grow arbitrarily faster than the function itself?,"We know that there exist some functions $f(x)$ such that their derivative $f'(x)$ is strictly greater than the function itself. for example the function $5^x$ has a derivative $5^x\ln(5)$ which is greater than $5^x$ . Exponential functions in general are known to be proportional to their derivatives.
The question I have is whether it is possible for a function to grow ""even faster"" than this. To be more precise let's take the ratio $\frac{f'(x)}{f(x)}$ for exponential functions this ratio is a constant. For most elementary functions we care about, this ratio usually tends to $0$ . But are there functions for which this ratio grows arbitrarily large? If so, is there an upper limit for how large the ratio $\frac{f'(x)}{f(x)}$ can grow? I also ask a similar question for integrals.","['derivatives', 'integration', 'calculus', 'functions']"
2817803,Closed set in metric space,"In $C\left( {\left[ {0,1} \right]} \right)$, we define metric 
$$d\left( {f,g} \right) = \mathop {\max }\limits_{t \in \left[ {0,1} \right]} t\left| {f\left( t \right) - g\left( t \right)} \right|.$$
We define $A = \left\{ {f \in C\left( {\left[ {0,1} \right]} \right):f\left( 0 \right) = f\left( 1 \right)} \right\}$. Is $A$ an open or closed set in $C\left( {\left[ {0,1} \right]} \right)$ with metric $d$??",['functional-analysis']
2817852,"On Atiyah-Macdonald, Exercise 1.22 [duplicate]","This question already has an answer here : Show that a ring with disconnected spectrum is a product of two subrings. [duplicate] (1 answer) Closed 6 months ago . I am trying to solve the following question, which appears as the part of an exercise in Atiyah-Macdonald Chapter I.22: For a ring $A$, if $\mathrm{Spec}(A)$ is disconnected, then we have $A\cong A_1\times A_2$ for some rings $A_1,A_2$. Actually, the original question is to show three equivalent conditions and we can avoid the above. But I would like to see if there is a possibility to prove this. Here is my attempt: Let $\mathrm{Spec}(A)=V(I_1)\cup V(I_2)$, where $V(I_i)$ are proper closed sets and $V(I_1)\cap V(I_2)=\emptyset$. Then we have 
$$I_1+I_2=(1)$$ So if we have $I_1\cap I_2=\{0\}$, we can use the Chinese remainder theorem to show 
$$A\cong A/\{0\}\cong (A/I_1)\times (A/I_2)$$ 
then we are done. Unfortunately, from $V(I_1)\cup V(I_2)=V(I_1\cap I_2)=\mathrm{Spec}(A)$, we can only obtain 
$$I_1\cap I_2\subset \mathfrak{N}$$
where $\mathfrak{N}$ is the nilradical of $A$. So I was wondering if we can shrink $I_1$ and $I_2$ so that complete the above argument. The obvious try is $I_1$ and the ideal generated by $I_2\setminus I_1$, but it is apparent to see $I_1\cap \langle I_2\setminus I_1 \rangle\neq \{0\}$... I really appreciate any kind of help!","['abstract-algebra', 'commutative-algebra']"
2817855,Existence of isometry-invariant measures,"One of the wonderful things about the development of measure theory is that the most important theorems (Littlewood's principles, Dominated convergence, Fubini's theorem, etc.) can be proven in the context of a general measure space. The only extra properties we get out of Lebesgue measure on $\mathbb R^n$ is the fact that it is a Borel measure and its invariance under rigid motions (isometries). Inspired by this, I was wondering what conditions one would have to impose on a metric space in order to guarantee the existence of a Borel measure which is invariant under isometries. That is, Desired Result: If (X,d) is a metric space satisfying condition $(P)$, then there exists a Borel measure $\mu$ on $X$ that is invariant under isometries, i.e. given an isometry $T:X\to X$, $$\mu(TB)=\mu(B)\textit{ for every Borel set }B\subset X$$ A neat example of a sufficient condition is given by the following theorem: Theorem: Let $(X,d)$ be a locally compact metric topological group. If $d$ is left invariant, i.e.
  $$\forall x,y,z\in X:d(zx,zy)=d(x,y)$$
  then a left-invariant Haar measure $\mu$ on $X$ is invariant under isometries. For the proof of this theorem, see here: https://www.ams.org/journals/proc/1983-087-01/S0002-9939-1983-0677233-2/S0002-9939-1983-0677233-2.pdf (be warned: it's short, but involved!). Recall that this condition is sufficient, thanks to Haar's theorem: Theorem: Let $G$ be a locally compact topological group. Then there exists a unique (modulo a multiplicative constant) left-invariant Haar measure $\mu$ on $G$. That is, $\mu$ has the property that $$\mu(gA)=\mu(A)\text{ for every Borel set }A\subset G$$ However, topological groups have a prior sort of invariance structure, namely, the action of group elements. The construction of Haar measure exploits this using the covering number
$$(A:B)=\inf\{k:\exists x_1,\dots,x_k\in X,A\subset\cup_{i=1}^kx_iB\}$$
(for $A,B\subset X$) which yields a sufficient measure (pun intended) of relative size. I have yet to find other references which prove the existence of such measures on a wider class of spaces. I suspect that the notion of 'relative size' given by group action might be totally integral to the construction of a measure, but I would love to be surprised! Can anyone provide me a reference for other such existence proofs? Or better, does anyone know of a generalization of the above theorem?","['borel-measures', 'topological-groups', 'measure-theory', 'haar-measure', 'metric-spaces']"
2817930,Definition of Chain Rule,"I learned from a textbook that if $f(x)$ can be expressed as a function of $u(x)$, for example $$f(x) = u(x)^3,$$
and if $\delta f$, $\delta u$, $\delta x$ are small finite quantities, then 
$${\delta f \over \delta x} = {\delta f \over \delta u} {\delta u \over \delta x}.$$ 
As the quantities become infinitesimally small, we get $$\frac{df}{dx} = \frac{df}{du} \frac{du}{dx}.$$
The above definitions seems to suggest the the quantities ${\delta u}$ and $du$ have to cancel in the expressions. I expanded the above definition using the gradient definition of a derivative into 
$${df \over dx}=\lim_{\delta u \to 0} {f(u+\delta u)-f(u) \over \delta u} \times \lim_{\delta x \to 0} {u(x+\delta x)-u(x) \over \delta x}$$ Does this imply that the $\delta u$ and $u(x+\delta x)-u(x)$ cancel each other?If that is so, why are they equal? It does not seem intuitive to me that they equal.","['algebra-precalculus', 'real-analysis', 'multivariable-calculus', 'calculus']"
2817963,Value of $\cos\frac{2k\pi}{n}$,"We know that $\cos\frac{2\pi}{5} = \frac{-1 + \sqrt{5}}{4}$ and $\cos\frac{4\pi}{5} = \frac{-1 - \sqrt{5}}{4}$. In general is there any formula for 
$$\cos\frac{2k\pi}{n}$$
where $k\leq n/2$.","['trigonometry', 'geometry']"
2817982,When are $V$ and $V^{*}$ canonically isomorphic?,"Generally speaking, for a finite dimensional vector space $V$, $V$ and $V^*$ are not canonically isomorphic after we fix a basis for $V$ . One of my questions is: If we do not fix a basis for $V$, can we say that $V$
  and $V^*$ are canonical isomorphic? For example, the following is a special case. Let $L$ be a finite dimensional vector space over a field $K$, and let $V:=\text{Hom}(L,L)$. We know that $$\text{Hom}(L,L)\cong L^*\otimes L\cong L^*\otimes L^{**}\cong(L\otimes L^*)^*\cong\text{Hom}(L,L)^*,$$
and the above isomorphisms are canonical, so $V\cong V^*$ canonicaly. The other question is: Are there other vector spaces $V$ such that $V$ and $V^*$ are
  canonically isomorphic?
  Or, is the above the only case such that $V$ and $V^*$ are canonically isomorphic?","['linear-algebra', 'dual-spaces']"
2817987,"Finding Absolute Minimum and Absolute Maximum of $f(x,y)=xy$","Let $\ f(x,y)=xy$. Use the method of Lagrange multipliers to find the maximum and minimum values of the function f on the circle $\ x^2+y^2=1$ First we note that the function $f$ is continuous and the set $S={(x,y):x^2+y^2=1}$ is compact, hence extrema are guaranteed. 
Using the method Lagrange multipliers, I set $\nabla f=\lambda\nabla g$, where $g(x,y)=x^2+y^2-1$. Following through the calculations, I arrived at four critical points:
$$\Big(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\Big),\Big(\frac{1}{\sqrt{2}},-\frac{1}{\sqrt{2}}\Big),\Big(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\Big),\Big(-\frac{1}{\sqrt{2}},-\frac{1}{\sqrt{2}}\Big)$$
Substituting these points into the function $f$, I obtained a maximum at 
$$\Big(\pm\frac{1}{\sqrt{2}},\pm\frac{1}{\sqrt{2}}\Big)=\frac{1}{2}$$
and a minimum at
$$\Big(\pm\frac{1}{\sqrt{2}},\mp\frac{1}{\sqrt{2}}\Big)=-\frac{1}{2}$$ My question is, how do we now find the absolute maximum and absolute minimum of the function $f$ on the unit disc $x^2+y^2\leq 1$? My attempt so far: We want to find all the critical points. So to find stationary points, we set 
$$\nabla f=\vec{0}$$
Solving this, we find that $(0,0)$ is a stationary point.
So, $f(0,0)=0$.
Hence the absolute maximum is $\frac{1}{2}$ and the absolute minimum is $-\frac{1}{2}$, as these are all the critical points of $f$.
This does not sit well with me, as I am unsure of my working/logic. Can this be improved on?","['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
2818031,Show that if $\sum_{n=0}^{\infty}a_n^k=\sum_{n=0}^{\infty}b_n^k$ for all $k\in \Bbb N^*$ then $(a_n) = (b_n)$,"Let $(a_n),(b_n)$ two summable sequences.
Show that if $\sum_{n=0}^{\infty}a_n^k=\sum_{n=0}^{\infty}b_n^k$ for all $k\in \Bbb N^*$ then $(a_n) = (b_n)$ more or less a permutation What I tried: to deal with the ""more or less a permutation"", 
I decided to re-index the sequences by: decreasing modulus as first If there is equality, decreasing real part if still equal, decreasing imaginary part (Assuming that the sequences are complexes).","['summability-theory', 'sequences-and-series']"
2818037,A book for complex analysis,"I am currently learning complex analysis in my undergraduate studies and it's quite hard to understand for me. During my mathematical study, I learned that nothing is too hard to understand, it's just necessary to find the right source for me. When I read lecture notes or books, they feel mostly too ""dry"" for me and it's not so enjoyable for me to read. In the past I learned first with books like e.g. analysis I or II for dummies and afterwords I learned with proper mathematical books, which I then could enjoy and understand fully, and use as my main source. But now as I progress to more advanced subjects, I find it hard to find books that help me. I learn best with a visual approach: the more graphics the better.
And it's always good when the author explains a lot and is not assuming too many things as trivia. I hope I made myself clear what kind of book I need and maybe someone can help me. PS: I'm not restricted to books; if you know some good lecture notes, you are more than welcome. My native language is German, so I'm fine with sources in English and German.","['complex-analysis', 'book-recommendation', 'reference-request']"
2818075,Closest cyclotomic integer to a cyclotomic number?,"Let's take a cyclotomic field of the form $K=\mathbb{Q}(\zeta_n)$ where $\zeta_p$ is the $n$th root of unity. Then the ring of integers of $K$ is $\mathcal{O}_K= \mathbb{Z}(\zeta_n)$. Is there a generalisation of the rounding function $\left \lfloor \cdot \right \rceil: \mathbb{Q} \to \mathbb{Z}$ to some rounding function $\left \lfloor \cdot \right \rceil_K : K \to \mathcal{O}_K $ for cyclotomic fields that rounds a cyclotomic number to its ""nearest"" cyclotomic integer? EDIT: I found something that might be useful. The following definition comes from https://hal.archives-ouvertes.fr/hal-00632997v1/document : Definition : For any $\eta \in K$, the real number $m_K(\eta)= \min_{z \in \mathcal{O}_K}|N_{K/\mathbb{Q}}(\eta - z)|$ is the Euclidean minimum of $\eta$. Does this give us a generalisation of the rounding function, and if so does the ""rounding"" function only hold for Euclidean domains?","['number-theory', 'abstract-algebra', 'algebraic-number-theory', 'cyclotomic-fields']"
2818176,Verification of proof on Bounded Variation.,"Let $f:[-1,1]\rightarrow \mathbb R$ be a function given by $f(x) =
 \begin{cases} x^2\cos(\frac{1}{x}),  & \text{if $x\neq 0$ } \\ 0, &
 \text{if $x=0$ } \end{cases}$then $(a)$ $f$ is of bounded variation on $[-1,1]$. $(b)$ $f'$ is of bounded variation on $[-1,1]$. $(c)$ $\vert f(x) \vert\le1 \forall x\in [-1,1]$ is of bounded
  variation on $[-1,1]$. $(d)$ $\vert f(x) \vert\le3 \forall x\in [-1,1]$ is of bounded
  variation on $[-1,1]$. Solution:$f'(x)=2x\cos(\frac{1}{x})-\sin(\frac{1}{x})\implies \vert f'(x)\vert\le3$ on $[1,1]\implies f$ is of bounded variation on $[-1,1]$.So,option $(a),(b)$ are correct. Please check the following argument: Since, the only problem creator for $f'$ being of bounded variation is $0$. Now, if I take a partition of $[-1,1]$ which contains $0$ as one of its points, then the limit of $f'$ as $x$ approaches zero does not exist, but the variation remains bounded, hence $f'$ is of bounded variation on $[-1,1]$. Please provide some argument which can discard option $c$","['derivatives', 'real-analysis', 'proof-verification', 'interval-arithmetic', 'bounded-variation']"
2818183,Finding limit for infinite quantities.,"Find $$\lim_{x\rightarrow 0}{\color{red}{x}} \cdot\bigg(\dfrac{1}{1+x^4}+\dfrac{1}{1+(2x)^4}+\dfrac{1}{1+(3x)^4}\cdots\bigg)$$ As terms are written infinitely my intuitions doesn't let to give answer as $0$. So tried calculated that weird sum something like, $T_n=\dfrac{1}{1+(nx)^4}=\dfrac{1}{(n^2x^2+\sqrt{2}nx+1)\cdot(n^2x^2-\sqrt{2}nx+1)}$ now this not results in telescopic sum, what should I do, is answer $0$, if so then why should we assume that $\color{red}x$ will stay in numerator no matter whatever happens. Please help.","['limits', 'algebra-precalculus', 'limits-without-lhopital', 'summation', 'sequences-and-series']"
2818196,Does every function have a graph?,"I'm having trouble understanding this. My friend said it doesn't, but I disagree, though I'm not sure. Given a function $f:A \to B$, the graph of $f$ is defined by $G(f) = \{(x,y)| x \in A , y = f(x)\}$. Then, is it true that if a function exists, its graph exist (Even though there may be no geometric interpretation)? I think this is true, since in the ""worst"" case it would be the empty set, which exists...","['elementary-set-theory', 'functions']"
2818207,Check whether $G$ is group or not,"Let $G=\{0,1,2\}$ define $*$ on $G$ such that $a*b=|a-b|$ . Check if $G$ is a group. Edit : As @egreg correctly pointed out if you draw the Cayley table then one can directly see it is not a group, I was making a mistake in drawing up Cayley table. My solution :
I can prove that $G$ is closed, has an identity ( $0$ ), has an inverse (each element is inverse of itself). What I am not sure is whether the group is associative under this binary operation, at the face of it $|a-|b-c||\neq||a-b|-c||$ but if we check all possibilities then both do come out to be equal if $a,b,c \in G$ . Any hint/help will be appreciated","['abstract-algebra', 'group-theory', 'binary-operations']"
2818211,"Prob. 2 (b), Sec. 8.14, in Apostol's CALCULUS Vol 2: How to find the directional derivative of this scalar field?","Let $f$ be the scalar field defined by the formula 
$$ f(x, y, z) = (x/y)^z, $$
for all points $(x, y, z) \in \mathbb{R}^3$ for which the formula makes sense. Then what is the directional derivative of $f$ at the point $(1, 1, 1)$ in the direction of $2 \mathbf{i} + \mathbf{j} - \mathbf{k}$? My Attempt: Let us put $\mathbf{a} \colon= (1, 1, 1)$ and $\mathbf{y} \colon= (2, 1, -1)$. Then $\mathbf{a} + h \mathbf{y} = ( 1 + 2h, 1 + h, 1 - h)$, and so 
  $$
\begin{align}
 \frac{ f( \mathbf{a} + h \mathbf{y} ) - f( \mathbf{a} ) }{ h } &= \frac{ [ (1+2h) / ( 1+h ) ]^{1-h} - 1 }{ h } \\
&= 
\end{align}
$$ What next? How to find 
$$ \lim_{ h \to 0 }  \frac{ f( \mathbf{a} + h \mathbf{y} ) - f( \mathbf{a} ) }{ h }? $$
Or, does this limit exist?","['derivatives', 'real-analysis', 'calculus', 'multivariable-calculus', 'analysis']"
2818264,What topics I need to cover in order to understand Black Scholes Option Pricing Model?,"I have been searching the web on the prerequisites of understanding Black Scholes Option Pricing Model. And I have drawn the following diagram to summarize the relationship between different topics: However, I do not know how far I should go in each topic. Linear Algebra, real analysis, to my understanding, involve lots of topics. So, my question is under each area (linear algebra, real analysis, etc), what topics should I cover? Where should I stop before moving on to the next area? Many thanks!","['stochastic-processes', 'real-analysis', 'multivariable-calculus', 'stochastic-calculus', 'linear-algebra']"
2818425,"Find 2nd order homogenous ODE from solutions $x^2, e^{-x}$","I'm trying to find the 2nd order ODE given those two solutions. I usually achieve this by finding the characteristic polynomial by multiplying the root factors, however the solution $x^2$ is giving me some trouble. As far as I understand, $x^2$ implies that $0$ is a triple root, but by finding the characteristic polynomial $x^3(x+1)$ I only get a 4th degree non quadratic equation, which I do not know how to ""convert"" to a 2nd degree one. Am I doing it wrong or should another method be used, ie. differentiating the solutions up to the 2nd derivative and proceeding by trying different polinomials to multiply each derivative?
Thanks in advance.","['problem-solving', 'ordinary-differential-equations']"
2818442,Why use A|BC and not ABC?,"Currently I'm reading Probability Theory The Logic of Science and there it's not really clear for me what exactly $A|BC$ means.
So I have two questions:
Is this a general notation?
Is $A|BC$ equal to $ABC$? If it is — that means it's just for the sake of order/consecution, like $A(BC)$ (but according to the book it's not so)?","['probability-theory', 'probability', 'notation']"
2818543,Question about random walk,"Consider $X_1, X_2, X_3$ ... random variables i.i.d. such that  $P(X_i=1)=p$ and  $P(X_i=-1)=1-p$. Consider the random walk $(S_n)_{n\ge 0} $ with $S_0=0$ and for $n\ge 1 $,  $S_n = \displaystyle\sum^{n}_{i=1} X_i$. Let $d=2p-1=E X_i$ be the drift of $S_n $. Assume  that  $d>0.$ Define $T_x= \inf \{n\ge0; S_n=x\} $. I want to prove that 
 $\displaystyle\sup_{y>0}\{T_y-\frac {2(y-1)}{d}\}$ is finite.
It's clear that $T_y $ is finite, because $S_n$ is transient to right, but I can not control the difference. I thought of using the fact that the time the walker needs to go from $0$ to $y$ is of the ordem of $\frac {y}{d}$ since d is intuitively the speed of the random walk. Help?","['random-walk', 'probability']"
2818589,Is there a special case for equations of curves that have parallel oblique asymtotes?,"I am given the function $f(x,y)=(x-y)^2(x^2+y^2)-10(x-y)x^2+12y^2+2x+y$  and asked whether the curve $f(x,y)=0$ has two parallel oblique asymptotes, two unique asymptotes, or if there is no oblique asymptote at all. Do I manually find the asymptote equations or there is some shortcut to this?","['algebra-precalculus', 'algebraic-curves', 'asymptotics', 'calculus']"
2818629,Counterexample $X_n \to X$ in distribution but $\lim_{n \to \infty} E[ \log(1+X_n) ]\neq E[ \log(1+X) ]$,"Let $X_n \to X$ in distribution where we only consider non-negative random variables. I am looking for a counterexample that for the followig limit
\begin{align}
\lim_{n \to \infty} E[ \log(1+X_n) ]=  E[ \log(1+X) ].
\end{align} Here is a counterexample that I have created. Let $X_n$ have a probability mass function according to $P[ X_n=0]=(1-\frac{1}{n})$ and  $P[ X_n= 2^n]=\frac{1}{n}$. 
Then
\begin{align}
\lim_{n \to \infty} E[ \log(1+X_n) ]= \lim_{n \to \infty}  \frac{1}{n} \log(1+2^n) = \log(2).
\end{align}
On the other hand, we have that  $X_n \to X= 0$ in distribution so 
\begin{align}
E[ \log(1+X) ]=  \log(1)=0.
\end{align} My question: Can we create a counterexample such that  $\sup_{n} E[X_n]<\infty$ and $E[X]<\infty$? Also, is there such an example? Note that in my counterexample, we have that
\begin{align}
\sup_{n} E[X_n]= \sup_{n} \frac{1}{n} 2^n=\infty.
\end{align}","['expectation', 'examples-counterexamples', 'probability-theory', 'weak-convergence', 'convergence-divergence']"
2818640,BAC─CAB rule used on del operators,"In my textbook, it is stated that $\nabla\times(\nabla\times A)=\nabla(\nabla\cdot A)-\nabla^2A $ So, I thought that del can be treated as if it were a vector (although it was an operator). However, 
When solving $\nabla\times (k\times r)$ where $r= x \hat{i}+y\hat{j}+z\hat{k}    $, the results I obtained from using the BAC CAB rule (while treating $\nabla$ as a constant) was different from the results obtained by doing it in the normal order. Why is this the case? Is it because, since $\nabla$ is an 'operator', you can't use it like a vector, and therefore cannot use the BAC CAB rule? But in the textbook, it uses the BAC CAB rule as the following: $$\nabla\times(\nabla\times A)=\nabla(\nabla\cdot A)-\nabla^2A $$ So does this mean that the textbook's derivation of $\nabla\times(\nabla\times A)$ was incorrect, and I should use Levi-Civita definition of cross products? In summary, my question is: when is it safe to regard $\nabla$ as a vector?","['derivatives', 'vector-fields', 'notation']"
2818644,Reference request for Statistical Learning Theory,"I want to learn about Statistical Learning Theory. I have a strong background in measure theory, probability theory and functional analysis. What would be a good resource/book to start learning Statistical Learning Theory in the most general setting? Thanks a lot in advance.","['reference-request', 'probability-theory', 'machine-learning', 'statistics', 'probability']"
2818652,Let $f$ be holomorphic with zeros at $\frac{1}{2}$ and $\frac{-1}{2}$ such that $|f(z)| < 1$ on $D$. Show $|f(i/2)| \leq \frac{8}{17}$.,"Let $f$ be holomorphic on the unit disc $D$ with zeros at $\frac{1}{2}$ and $-\frac{1}{2}$ such that $|f(z)| < 1$ on $D$. Show $|f(i/2)| \leq \frac{8}{17}$. I apply a Schwarz Lemma -esque argument. Define $g(z)$ to be $\frac{f(z)}{(z-\frac{1}{2})(z+\frac{1}{2})}$ on $D\setminus\{\pm \frac{1}{2}\}$, and $\pm4f'(\pm\frac{1}{2})$ at $z = \pm\frac{1}{2}$. So $g$ is holomorphic. Now, we can apply maximum modulus principle on any smaller disc $D_r$ with radius $\frac{1}{2}<r<1$. This gives, for $z \in D_r$, $$ |g(z)| \leq \sup_{|z|=r} |g(z)| \leq \sup_{|z|=r} \frac{|f(z)|}{|z^2-1/4|} \leq \frac{1}{r^2-1/4}.$$ The last step, I use the assumption on $f$, and the triangle inequality on the denominator:
$|z^2-1/4| \geq |z^2| - 1/4 \ ( = r^2 - 1/4 \text{ on bdry of }D_r).$
From here, taking $r \to 1$ we get $|g(z)|\leq 4/3$. Thus $$|f(z)| = |g(z)||z^2-1/4| \implies |f(i/2)| \leq \frac{2}{3}.$$ This is the best bound I can get, but it's not good enough :(. Any tips appreciated! I'd prefer to keep a Schwarz Lemma type argument (as that was a hint given to me).",['complex-analysis']
2818675,"interplay: exterior algebra, tensor algebra, differential forms","I'd like to understand how to evaluate a differential form on vector fields, and how to 'embed' exterior algebra into tensor algebra. Some people define differential forms as alternating tensor fields, the others as sections of 'exterior algebra bundle'. In the former case, it is clear how such a field acts on vector fields. I am not sure how it works in the second case and what is the interplay between these two definitions. Here is what I mean. Let $V^*$ be the dual space of the $\mathbb{F}$-vector space $V$, where $\mathbb{F}$ is either $\mathbb{R}$, or $\mathbb{C}$. Consider the tensor algebra $Tensor(V^*)$ and its quotient by the two-sided ideal $I$ generated by all elements of the form $x\otimes x$ for $x\in V^*$ together with the natural projection $\pi$, i.e. corresponding exterior algebra $\pi:Tensor(V^*)\to\Lambda(V^*)=Tensor(V^*)/I$. The exterior algebra then inherits the multiplication from the tensor product which is called the wedge product, hence $\pi(a)\wedge\pi(b):=\pi(a\otimes b)$. The other part of the story are alternating tensors. There is an endomorphism of the tensor algebra, called the alternation map $Alt$ , which is actually a projection with the kernel $Ker(Alt)=I$, and the image of $Alt$ are precisely alternating tensors $AltTen(V^*)$. Therefore, one has the vector space isomorphism $\overline{Alt}:\Lambda(V^*)\cong AltTen(V^*)$ which is given by $\pi(a)\mapsto Alt(a)$. This construction works also in the case if we start with $TM$ instead of $V$, where $TM$ is the tangent bundle to a manifold $M$. In that situation, sections of the 'exterior algebra bundle' are differential forms. Let $dx^1,dx^2\in T^*M$ such that $dx^i(\partial_j)=\delta^i_j$, where $x^i:M\to\mathbb{R}$ are coordinate functions and $d$ is the exterior derivative. People write $dx^1\wedge dx^2$. Does this mean $\pi(dx^1)\wedge\pi(dx^2)$, and hence $\pi(dx^1\otimes dx^2)$? How such a differential form $dx^1\wedge dx^2$ acts on $(\partial_1,\partial_2)$? Is this correct; $\pi(dx^1)\wedge\pi(dx^2) (\partial_1,\partial_2)=(dx^1\otimes dx^2+I)(\partial_1,\partial_2)=dx^1(\partial_1)dx^2(\partial_2)+I(\partial_1,\partial_2)=1+0?$ If one uses the isomorphism $\overline{Alt}$ the form $dx^1\wedge dx^2$ corresponds to $\frac{1}{2}(dx^1\otimes dx^2-dx^2\otimes dx^1)$ which evaluates on $(\partial_1,\partial_2)$ as $\frac{1}{2}$. This seems to me as the correct way because $\frac{1}{2}(dx^1\otimes dx^2-dx^2\otimes dx^1)$ is something like 'pure' representative of the class $\pi(dx^1\otimes dx^2)$. If $\pi(dx^1),\pi(dx^2)\in\Lambda(T^*M)$ we may consider their product which is $\pi(dx^1\otimes dx^2)$, which in turn corresponds to $\frac{1}{2}(dx^1\otimes dx^2-dx^2\otimes dx^1)$ under the map $\overline{Alt}$. Here is the wedge product defined differently. According to that definition $dx^1\wedge dx^2=dx^1\otimes dx^2-dx^2\otimes dx^1$. Thank you for any comments.","['differential-forms', 'differential-geometry', 'multilinear-algebra']"
2818873,Isotopy classes of embedded closed curves in a torus,"Can anyone explain to me why the Isotopy classes of embedded closed curves in a torus are classified by “slopes” $r\in\mathbb{Q}\cup \{\infty\}$? Why can we identify (up to isotopy) simple closed curves ($(p,q)$-curves) in a Torus to $\mathbb{Q}\cup \{\infty\}$? I thought this is because of the fact that the fundamental group of the Torus is $\mathbb{Z}\times\mathbb{Z}$ but I am not really sure. I have seen the above fact being used several times. (see Ulrich page 2, Hatcher page 1).","['algebraic-topology', 'geometric-topology', 'general-topology']"
2818919,$16$ rooks on a chess board.,"Place $8$ pairwise non-attacking white rooks and black rooks on a $8\times8$ chess board. If one can swap rows and columns, is it possible for the black rooks to take the initial position of the white rooks and vice versa? My attempt: Let the rows be $a_1,a_2,...,a_8$ and columns $b_1,b_2,...,b_8$. If there are white rooks at ($a_i$,$b_j$) and ($a_j$,$b_a$) as well as black rooks at ($a_i$,$b_i$) and ($a_j$,$b_j$), then we can reduce it to a $2\times2$ and a $6\times6$. I don't know what to do next. Any help appreciated.","['combinatorics', 'chessboard']"
2818925,How to solve $\lim_{x\to1}=\frac{x^2+x-2}{1-\sqrt{x}}$?,"let $f(x)=\dfrac{x^2+x-2}{1-\sqrt{x}}$ How do I solve this limit?
$$\lim_{x\to1}f(x)$$ I can replace the function with its content
$$\lim_{x\to1}\dfrac{x^2+x-2}{1-\sqrt{x}}$$ Then rationalizing the denominator
$$\lim_{x\to1}\dfrac{x^2+x-2}{1-\sqrt{x}}\cdot\dfrac{1+\sqrt{x}}{1+\sqrt{x}}$$ With $(a + b)(a - b) = a^2 - b^2$, I can remove the irrational denominator. $$\lim_{x\to1}\dfrac{(x^2+x-2)(1+\sqrt{x})}{1-x}$$ I the multiply the two parenthesis
$$\lim_{x\to1}\dfrac{\sqrt{x} \cdot x^2+\sqrt{x}\cdot x- \sqrt{x} \cdot 2 + x^2 + x - 2}{1-x}$$ I'm not sure where to continue to solve this limit.",['limits']
2818930,Help with a definition: number theory,"I found on some papers in number theory the following: 'Let $K$ be a number field and let $\mathfrak p$ be a prime ideal of $K$ with absolute degree $1$"". What does absolute degree mean? I have ever heard about it. Thank you for your help.","['number-theory', 'abstract-algebra', 'algebraic-number-theory', 'definition']"
2818943,confusion in the combinatorial analysis in the game of baccarat,"Update (14th Jun 18) My argument here is that if we assume all hands are 6-card hands, we have created a lot of extra invalid combinations to the ""total"". For example in this game, an extra of 37446746112/31=1207959552 combinations were created. Am I correct? OP Baccarat is a popular CASINO game played by a lot of people. One of my friend recently asked me a probability question and further brought my interest in this as I have a PhD in Statistics. I read on this page where Combinatorial Analysis is used. Looking at the example here, Player’s two cards: 1, 4 Banker’s two cards: 2, 4 Card 5: 2 (dealt to Player) Card 6: 7 (not used) I understand the method of working out the number 4030726144. But my question is Why would we consider the combinations of the SIX cards? As we only use FIVE card for this hand (according to the rules of this game). The answer for this hand would be 4030726144/32 = 125960192. It seems to me that the total 4,998,398,275,503,360 is an agreed number where it's been referred to on many sites, for example, the calculator seem to be using the same method. But none of which seem to explain why. As far as I am concerned, we should only consider VALID hands. So the example above should only be considered as a FIVE-card-hand Can someone help me understand this better?","['statistics', 'probability', 'card-games', 'combinatorics', 'combinatorial-game-theory']"
2818951,A triangle with angle bisector and altitudes,"In triangle ABC, AB=125, AC=117, BC=120. The angle bisector of A intersect BC at L and the angle bisector of B intersect AC at K. Let M and N be the feet of perpendiculars from C to BK and AL respectively. Find MN. I tried to coordinate bash this with the origin C. But after I calculated the coordinate of A and B I gave up. Point to a line with square roots is just crazy. Is there another way to do this (without crazy calculation)? Any help appreciated.",['geometry']
2818968,Compound Poisson Process Expected Exit Time,"It is very well known that expected time for a standard Brownian motion to exit from interval $[a,b]$ (where $a<0$ and $b>0$) is $-ab$. In one of my projects, I wanted to calculate the similar quantity for a compound poisson process. I am not an expert in handling point processes and would be grateful for any help. Question : $X$ is a compound poisson process starting at position $0$ with arrival rate $\lambda$ and the distribution of jumps as $F(dz)$. What would be the expected time $\tau$ for $X$ to exit $[-a,a]$? Also, what would be the distribution of $X_\tau$?","['stochastic-processes', 'reference-request', 'probability-distributions', 'markov-chains', 'probability']"
2819027,No Non trivial subgroup means prime order?,"Suppose that $G$ is a group with more than one element, $G$ had no proper, non-trivial subgroup then prove that $|G|$ is prime. Attempt. Claim $G$ is finite If not then for any $x\neq e$ we have $\langle x^2 \rangle$ a non-trivial subgroup of $G$.
  Hence $G$ is finite. Now given $G$ is finite.Let $|G|=m$
  For any $x\neq e$ we have $\langle x \rangle$ a subgroup of $G$. Now because there exists no non trivial subgroup,
  we have $\langle x\rangle=G$ Hence $G=\langle x \rangle$ Hence $G$ is cyclic. How do I show that $|G|$ is prime? Kindly do not use Cauchy Theorem.
Use Lagrange's Theorem only, or topics taught before Lagrange Theorem.","['group-theory', 'cyclic-groups']"
2819057,"When defining independence of random variables,do they need to be on the same probability space?","Random variable X and Y with cumulative distribution functions $ F_X(x)$ and $F_Y(y)$,are independent iff the combined random variable $(X, Y)$ has a joint cumulative distribution function
  $$F_{X,Y}(x,y) = F_X(x) F_Y(y)$$ In the above definition,they use joint distribution of $(X,Y)$. According to the definition of joint probability distribution of random vector such as $(X,Y)$, $X$ and $Y$ must be defined on the same probability space. Does it mean $X$ and $Y$ when defining independence should be on the same probability space? relate question: Can we define a joint probability distribution over different sample spaces / probability spaces?",['probability']
2819097,Inverse of Gaussian Kernel Matrix,"Let a gaussian kernel be defined as $K(x_i, x_j) \equiv \exp(-\alpha |x_i-x_j|^2)+\beta \delta_{ij}$, and define the kernel matrix of some set of datapoints $\{x_i\}_{i=1}^n$ as the $n\times n$ matrix $K$ with $K_{ij} = K(x_i, x_j)$. This is a common construction in various fields, e.g. Gaussian Processes. Is there a fast way of calculating the inverse of the kernel matrix? Thoughts: if we could break down the matrix $K$ into the form $\beta I + u u^T$ for some column vector $u$, we could use the Sherman–Morrison formula to quickly calculate the inverse, however, we know such a $u$ would be infinite dimensional. Is there another trick one could use?","['linear-algebra', 'inverse']"
2819107,Use a quadratic equation to find two consecutive even integers if their product is $168$,"All I have so far is $xy=168$, and I know I need a second equation to make a quadratic formula. So how do you write ""$2$ consecutive even integers"" as a formula? Answer: 12 and 14","['algebra-precalculus', 'quadratics']"
2819201,Eigenvectors of discrete Laplace matrix for 2D unit square under Neumann boundary condition,"Eigenvectors of discrete Laplace matrix for 2D unit square with free boundary is simply
$$
\phi(x,y)= \cos(\frac{\pi}{n} kx) \cos(\frac{\pi}{m} ly) 
$$
It is easy to see that its 2nd order derivative equals itself (scaled). For example, the Laplace matrix of a 4 by 4 grid is The numerically computed eigenvectors are consistent with the expression $\cos(\frac{\pi}{n} kx) \cos(\frac{\pi}{m} ly)$. My question is what is the following matrix: Precisely put, what is the differential equation on a continuous 2D unit square domain corresponds to this discrete operator? The value 4 corresponds to the inner nodes, value 3 for the boundary nodes, value 2 for the corner nodes. Is this the Laplacian under Neumann boundary condiction? Second question: what are the eigenvectors of such matrix? Of course one can compute them numerically, but is there an analytic expression for these  eigenvectors ? I find this related entry and this though they did not resolve my question.","['operator-theory', 'ordinary-differential-equations', 'linear-algebra', 'eigenfunctions']"
2819223,Help understanding Casella & Berger's explanation of a sufficient statistic,"This is from Casella and Berger's Statistical Inference: Definition: A statistic $T(\mathbf{X})$  is a sufficient statistic for $\theta$ if the conditional distribution of the sample $\mathbf{X}$ given the value of $T(\mathbf{X})$ does not depend on $\theta$. In the discrete case, Let $t$ be a possible value of $T(\mathbf{X})$ , that is, a value such that $P_\theta(T(\mathbf{X})  = t) > 0$. We wish to consider the conditional probability $P_\theta(\mathbf{X} = \mathbf{x}|T(\mathbf{X}) = t)$. If $\mathbf{x}$ is a sample point such that $T(\mathbf{x}) \neq t$, then clearly, $P_\theta(\mathbf{X} = \mathbf{x}|T(\mathbf{X}) = t) = 0$. Thus, we are interested in $P(\mathbf{X} = \mathbf{x}|T(\mathbf{X}) = T(\mathbf{x}))$. By the definition, if $T(\mathbf{X})$ is a sufficient statistic, this conditional probability is the same for all values of $\theta$ so we have omitted the subscript. This is the part I'm having trouble with: A sufficient statistic captures all the information about $\theta$ in this sense.  Consider Experimenter 1, who observes $\mathbf{X} = \mathbf{x}$ and, of course, can compute $T(\mathbf{X} = T(\mathbf{x})$. To make an inference about $\theta$, he can use the information that $\mathbf{X} = \mathbf{x}$ and $T(\mathbf{X}) = T(\mathbf{x})$. Now consider Experimenter 2, who is not told the value of $\mathbf{X}$ but only that $T(\mathbf{X}) = T(\mathbf{x})$. Experimenter 2 knows $P(\mathbf{X} = \mathbf{y}|T(\mathbf{X}) = T(\mathbf{x}))$, a probability distribution on $A_{T(\mathbf{x})} = \{\mathbf{y}: T(\mathbf{y}) = T(\mathbf{x})\}$, because this can be computed from the model with knowledge of the true value of $\theta$. So far, so good. But below, what exactly is this random variable $\mathbf{Y}$? I'm having trouble unraveling why exactly this conclusion means that Experimenter 2 has the same information that Experimenter 1 has regarding the parameter $\theta$. I apologize for not framing my question better -- I'm just quite confused by the point the author is trying to make in the paragraph below. I will update with an edit if I can clarify my question further. Thus, Experimenter 2 can use this distribution and a randomization device, such as a random number table, to generate an observation $\mathbf{Y}$ satisfying $P(\mathbf{Y} = \mathbf{y}|T(\mathbf{X}) = T(\mathbf{x})) = P(\mathbf{X} = \mathbf{y}|T(\mathbf{X}) = T(\mathbf{x}))$. It turns out that, for each value of $\theta$, $\mathbf{X}$ and $\mathbf{Y}$ have the same unconditional probability distribution, as we shall see below. So Experimenter 1, who knows $\mathbf{X}$, and Experimenter 2, who knows $\mathbf{Y}$ have equivalent information about $\theta$, but surely the use of the random number table to generate $\mathbf{Y}$ has not added to Experimenter 2's knowledge of $\theta$. All his knowledge about $\theta$ is contained in the knowledge that $T(\mathbf{X}) = T(\mathbf{x})$. So Experimenter 2, who knows only $T(\mathbf{X}) = T(\mathbf{x})$, has as much information about $\theta$ as does Experimenter 1, who knows the entire sample $\mathbf{X} = \mathbf{x}$. To complete the above argument, we need to show that $\mathbf{X}$ and $\mathbf{Y}$ have the same unconditional distribution, that is, $P_\theta(\mathbf{X} = \mathbf{x}) = P_\theta(\mathbf{Y} = \mathbf{x})$ for all $\mathbf{x}$ and $\theta$. Note that the events $\{\mathbf{X} = \mathbf{x}\}$ and $\{\mathbf{Y} = \mathbf{x}\}$ are both subsets of the event $\{T(\mathbf{X}) = T(\mathbf{x})\}$ Also recall that
  $$ P(\mathbf{X} = \mathbf{x}|T(\mathbf{X}) = T(\mathbf{x})) = (\mathbf{Y} = \mathbf{x}|T(\mathbf{X}) = T(\mathbf{x})) $$
  and these conditional probabilities do not depend on $\theta$. Thus, we have
  $$ P_\theta(\mathbf{X} = \mathbf{x}) = P_\theta(\mathbf{X} = \mathbf{x} \text{ and } T(\mathbf{X}) = T(\mathbf{x})) \\
=  P(\mathbf{X} = \mathbf{x}|T(\mathbf{X}) = T(\mathbf{x}))P_\theta(T(\mathbf{X}) = T(\mathbf{x})) \\
=  P(\mathbf{Y} = \mathbf{x}|T(\mathbf{X}) = T(\mathbf{x}))P_\theta(T(\mathbf{X}) = T(\mathbf{x})) \\
= P_\theta(\mathbf{Y} = \mathbf{x} \text{ and } T(\mathbf{X}) = T(\mathbf{x}))\\
= P_\theta(\mathbf{Y} = \mathbf{x})$$","['probability-theory', 'statistics', 'statistical-inference']"
2819246,Number of permutations of unspecified length,"We can see subsets as combinations of unspecified length of a set's elements, and there are, of course, $$\sum_{k=0}^n {n \choose k}=2^n$$ subsets of a set with $n$ elements. I was wondering whether there is a neat expression (analogous to $2^n$) for  the number of permutations of unspecified length of the elements of a set. For example, if we have five distinct items, the number of permutations for each possible length is given below; the total number of permutations (i.e., of unspecified length) would simply be the sum of these numbers. Length of permutation | Number of permutations
      ----------------------------------------------
          0                 |    1
          1                 |    5
          2                 |    5⋅4
          3                 |    5⋅4⋅3
          4                 |    5⋅4⋅3⋅2
          5                 |    5⋅4⋅3⋅2⋅1 Of course we can express the total number of permutations of unspecified length of $n$ items as $\displaystyle\sum_{k=0}^n {}_n\text P_k$ but is there is a neater expression that this sum evaluates to, or at least a conventional notation for this number?","['combinatorics', 'elementary-set-theory']"
2819261,Is there a Continuous Function mapping $\ S_1\ $ onto $\ S_2\ $,"Let
  $$
S_1=\{(x,y)\in\mathbb{R^2}:1<x^2+y^2<2\}
$$
  $$
S_2=\{(x,y)\in\mathbb{R^2}:2x^2<x^2+y^2<4\}
$$
  Is there a continuous function $f$ mapping $\ S_1\ $ onto $\ S_2\ $? I believe that there is no function $f$ that maps $\ S_1\ $ onto $\ S_2\ $. My reason for this is that while $S_1$ is path connected, $S_2$ is not path connected and $S_1\cap S_2$ is also not path connected. Hence you cannot map $S_1$ onto $S_2$. Is this logic correct? How can I build on this explanation?","['multivariable-calculus', 'general-topology', 'continuity', 'path-connected']"
2819345,Why is the notion of analytic function so important?,"I think I have some understanding of what an analytic function is — it is a function that can be approximated by a Taylor power series. But why is the notion of ""analytic function"" so important? I guess being analytic entails some more interesting knowledge rather than just that it can be approximated by Taylor power series, right? Or, maybe I don't understand (underestimate) how a Taylor power series is important? Is it more than just a means of approximation?","['analytic-functions', 'taylor-expansion', 'power-series', 'analysis']"
2819416,Characteristic function of the sum of random variables,"The problem statement, all variables and given/known data I am trying to understand the very last equality for (let me replace the tilda with a hat) $$\widehat{P_X(K)}=\widehat{P(k_1=k_2=\cdots=k_N=k)} \tag 1$$ Relevant equations I also thought that the following imaginary exponential delta identity may be useful, due to the equality of the $k_i$, but see comments below: $$\int dk \, \exp(ikx) = \delta(x=0) $$ The attempt at a solution So it sees to me the goal is something like expressing $\widehat{P_{X}(K)}$ in terms of $P(\widehat{k}_i)$ ? So these are given by $\widehat{P(k_1)\cdots P(k_n)}= \int d^N \vec{x} \, p(\vec{x}) \exp\left( -i \sum_j x_j k_j\right) $ I thought I'd first try to look at the simplified case of independent random variables to understand $(1)$ but still can't seem to get it. So in this case $p(\vec{x}) = \prod_i p(x_i)$ And then we have (If I am correct in that the notation is that $\prod_i dx_i = d^N (\vec{x}) $)
$$\widehat{P(k_1)\cdots P(k_n)}= \int \prod_i dx_i \, p(x_i)  \exp\left( -i \sum_j x_j k_j\right) $$
and then you can seperate the integrals and so we have: $$\widehat{P(k_1)\cdots P(k_n)}= \widehat{P_{x_1}(k_1)} \cdots \widehat{P_{x_n}(k_n)} \tag 2 $$ Now if I consider the independent case in $\widehat{P_{X}(K)}$ I have: \begin{align}
\widehat{P_{X}(K)} & = \int \prod_i dx_i \, p(x_i) \exp\left( -i k \sum_j x_j  \right) \\[10pt]
& = \int  dx_1 \, p(x_1) e^{-ix_1 k} \int dx_2 \, p(x_2) e^{-ix_2 k} \cdots \int dx_N \, p(x_N) e^{-ikx_N} = \widehat{P_{x_1}(k)} \cdots \widehat{P_{x_n}(k)}
\end{align} So if I compare this to $(2),$ and can reason( I'm not sure you can) that it does matter whether you have $k_i$ or $k$, this is just the label of the fourier transform, but look at the lower notation that gives the distribution, that $\widehat{P_{x_1}(k)}= \widehat{P_{x_1}(k_1)} $ and then I have $k_1=k$ and can do the same for each $k_i$ etc. Without independence instead i have: $$\int d^N \vec{x} \, p(\vec{x}) \exp\left( -i k \sum_j x_j\right) $$ and I can't see how you can make any conclusions without knowing what $ p(\vec{x}) $ is? Many thanks","['characteristic-functions', 'statistical-mechanics', 'probability-distributions', 'statistics', 'random-variables']"
2819445,"Prob. 3, Exercises 8.14, in Apostol's CALCULUS Vol. 2: Find the point(s) and the direction(s) of maximum directional derivative of this scalar field","Here is Prob. 3, Exercises 8.14, in the book Calculus Vol. II by Tom M. Apostol, 2nd edition: Find the points $(x, y)$ and the directions for which the directional derivative of $f(x, y) = 3x^2+y^2$ has its largest value, if $(x, y)$ is restricted to be on the circle $x^2 + y^2 = 1$. My Attempt: We note that, at any point $(x, y) \in \mathbb{R}^2$, we have 
  $$ \frac{ \partial f (x, y)}{ \partial x} = 6x, \qquad \mbox{ and } \qquad \frac{ \partial f (x, y)}{ \partial y } = 2y. $$
  Thus these partial derivatives exist and are continuous everywhere in $\mathbb{R}^2$. Hence $f$ is differentiable everywhere in $\mathbb{R}^2$. Therefore we must have 
  $$ f^\prime( (x, y); (u, v) ) = \nabla f(x, y) \cdot (u, v) = ( 6x, 2y) \cdot (u, v) = 6xu + 2yv $$
  for any point $(x, y) \in \mathbb{R}^2$ and for every direction $(u, v) \in \mathbb{R}^2$. Now at any point $(x, y) \in \mathbb{R}^2$, the derivative $f^\prime( (x, y) ; (u, v) )$ is largest in the direction of the gradient vector $(6x, 2y)$ and this largest value is 
  $$ \lVert \nabla f(x, y) \rVert^2 = 36x^2 + 4y^2, $$
  and, for all points $(x, y) \in \mathbb{R}^2$ for which $x^2 + y^2 = 1$, this largest value is 
  $$ \lVert \nabla f(x, y) \rVert^2 = 36x^2 + 4y^2 = 36 \left(x^2 + y^2 \right) - 32 y^2 = 36 - 32 y^2, $$
  and this last expression attains its maximum value when $y = 0$, that is, at either of the points $(1, 0)$ and $(-1, 0)$. Thus the largest value of $f^\prime( (x, y); (u, v) )$ for any points $(x, y) \in \mathbb{R}^2$ for which $x^2+y^2=1$ is attained at the points $( 1, 0)$ and $(-1, 0)$ in the direction of the vectors 
  $$\nabla f(1, 0) = ( 6, 0) $$
  and 
  $$\nabla f(-1, 0) = (-6, 0),$$
  respectively, and this largest value in either case is $36$. Is my solution correct? Is each and every step in it correct and clear enough? If not, then where are the issues?","['derivatives', 'real-analysis', 'calculus', 'multivariable-calculus', 'analysis']"
2819556,"Prob. 4, Exercises 8.14, in Apostol's CALCULUS Vol II: Computing the gradient vector and a directional derivative for a scalar field","Here is Prob. 4, Exercises 8.14, in the book Calculus Vol II by Tom M. Apostol, 2nd edition: A differentiable scalar field $f$ has, at the point $(1, 2)$, directional derivative $+2$ in the direction toward $(2, 2)$ and $-2$ in the direction toward $(1, 1)$. Determine the gradient vector at $(1, 2)$ and compute the directional derivative in the direction toward $(4, 6)$. My Attempt: Let $(a, b)$ be the gradient vector $\nabla f(1, 2)$ of the scalar field $f$ at the point $(1, 2)$. The vector $\mathbf{u}$ from $(1, 2)$ to $(2, 2)$ is given by 
  $$ \mathbf{u} = (2, 2) - (1, 2) = (1, 0). $$
  Now as $f$ is differentiable at $(1, 2)$, so the directional derivative of $f$ in the direction of $\mathbf{u}$ is 
  $$ \nabla f(1, 2) \cdot \mathbf{u} = (a, b) \cdot (1, 0) = a. $$
  Therefore we obtain $a = 2$. The vector $\mathbf{v}$ from $(1, 2)$ to $(1, 1)$ is given by 
  $$ \mathbf{v} = (1, 1) - (1, 2) = (0, -1). $$
  Once again as $f$ is differentiable at $(1, 2)$, so the directional derivative of $f$ in the direction of $\mathbf{v}$ is 
  $$ \nabla f(1, 2) \cdot \mathbf{v} = (a, b) \cdot (0, -1) = -b. $$
  Therefore we obtain $b = 2$. Thus the gradient vector of $f$ at $(1, 2)$ is given by 
  $$ \nabla f(1, 2) = (2, 2). $$ Now the vector $\mathbf{w}$ from $(1, 2)$ toward $(4, 6)$ is given by 
  $$ \mathbf{w} = (4, 6)- (1, 2) = (3, 4). $$
  So the directional derivative of $f$ at $(1, 2)$ in the direction toward $\mathbf{w}$ is 
  $$ \nabla f(1, 2) \cdot \mathbf{w} = (2, 2) \cdot (3, 4) = 14. $$ Is there any error --- either in logic or calculation --- in this solution?","['derivatives', 'real-analysis', 'calculus', 'multivariable-calculus', 'analysis']"
2819597,Compute $\lim_{x\to 0^{+}}x^{x^{x}}=?$,"Problem: find $\lim_{x\to 0^{+}}x^{x^{x}}$. Solution:
$$\lim_{x\to 0^+}x^{x^{x}}=\lim_{x\to 0^+}e^{x^{x}\ln x}=\lim_{x\to 0^+}e^{e^{\ln x^{x}}\ln x}=\lim_{x\to 0^+}e^{e^{x\ln x}\ln x}$$ Now what should I do?",['limits']
2819601,Scaling data sets to match each other with least error,"I have two data sets: A & B with values: $a_1,a_2...a_n$ and $b_1,b_2...b_n$ that represent the values for the same elements ($x_1,x_2...x_n$). For instance, $x_1 = a_1$ in the first data set and $x_1 = b_1$ in the second data set. These data sets have very different values, but its relative values should be the same ($\frac{a_i}{a_j}=\frac{b_i}{b_j}$). This is not the case because the data come from experiments. I would like to obtain a scaling constant to multiply data set B to match data set A with the least error. What is the best method to do this? Edit: Also, each value in B has an uncertainty measurement, how can I take into a account this effect? As I must be more focused on matching the values that have the least uncertainty.","['machine-learning', 'statistics', 'statistical-inference']"
2819659,Ring Around the Robot - Chance of ending on specific node,"$N$ nodes $(Node_1 .. Node_N)$ are arranged in a circle, and a robot is placed at $Node_1$. The robot moves clockwise with probability $p$ and counter-clockwise with prob. $(1-p)$. Given integers $S, B \in \mathbb{N}$, where $1<=B<=N$, what's the probability of it landing on $Node_B$ after taking $S$ steps? I know there's a mathematical formulation of this problem, but I haven't been able to shift from an algorithmic frame of mind. What I've come up with is: E = minabs(S - B) # How many steps is the minimum allowed to get to Node_B?
if(S < E) { Prob = 0 } # There aren't enough steps to get to Node_B
if (S == E) && (S-B < 0) { Prob = (1 - p)^S } # Only S counter-clockwise steps will get to Node_B.
if (S == E) && (S-B > 0) { Prob = p^S } # Only S clockwise steps will get to Node_B. However, I'm stuck trying to think through the possible scenarios when $S > E$. Extracting the cases where the robot goes completely around the ring in either direction, or where a move in one direction is countered by the opposite move have me overwhelmed, and thinking there must be a better way.","['combinatorics', 'statistics']"
2819684,Can non-linear continuous and odd function $f(x)$ satisfy $f(2x) = 2f(x)$?,"Consider the following problem: Is it true, that any odd continuous function satisfying $f(2x) = 2f(x)$ is linear? Does the following function (which I found here ) can serve as a counterexample? $$
f(x) = \begin{cases} x\cos(2\pi \log_2(x)), & \mbox{if } x>0 \\ 0, & \mbox{if } x=0 \\ x\cos(2\pi \log_2(-x)) & \mbox{if } x<0 \end{cases}
$$ If it can, are there any ""simpler"" ones? I mean, that task was given on exam, and it would be rather difficult to come up with such a function on your own. By what strategy can one construct a similar counterexample from scratch? It feels like some piecewise linear function could do the trick, but I can't figure out, how to build it.","['functional-analysis', 'continuity', 'calculus']"
2819695,How can I compute the discriminant of the field $\mathbb{Q}(\sqrt[3]{28})$?,"$$\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\al}{\alpha}
\newcommand{\bcal}{\mathcal{B}}
\newcommand{\qroot}{\sqrt[3]}
\newcommand{\froot}{\sqrt[4]}
$$ I have a problem which consist in 3 problems. I solve part 1 and 2, but I want to make sure it is correct. Also I dont know how to solve part 3. Anyone ? Thanks Consider the number field $E = \Q(\sqrt[3]{28})$ . Find $T_{E|\Q}(\alpha)$ and $N_{E|\Q}(\alpha)$ for every $\alpha \in E$ . Let $\mathcal{O}[\qroot{28}]$ be the set of all integral elements in $E$ . Show that if $\beta = \frac{1}{3}(1 + 7\qroot{28} + 2\qroot{98})$ , then $$\beta \in \mathcal{O}[\qroot{28}]$$ Consider the set $$ \mathcal{B} = \{\qroot{28},\qroot{98},\frac{1}{3}(1 + 7\qroot{28} + 2\qroot{98}) \}$$ Assuming that $\mathcal{B}$ is an integral basis for $\mathcal{O}(\qroot{28})$ , Calculate the field discriminant of the number field $E$ . $\textbf{Solution for 1}$ : One ordered basis for the field extension is $\bcal = \{1,\qroot{28},\qroot{98}\}$ . Any element $\al \in E$ is of the form $$ \al = a + b\qroot{28} + c\qroot{98}$$ Multiplying $\al$ for each basis elements we obtain $$ \al\cdot1 = a + b\qroot{28} + c\qroot{98}$$ $$ \al\cdot\qroot{28} = a\qroot{28} + 2b\qroot{98} + 14c$$ $$ \al\cdot\qroot{98} = a\qroot{98} + 14b + 7c\qroot{28}$$ Hence, $$[\al]_\bcal = \begin{bmatrix} a & 14c & 14b \\ b & a & 7c \\ c & 2b & a \end{bmatrix}$$ and we obtain that $$T_{E|\Q}(\alpha) = 3a$$ $$N_{E|\Q}(\alpha) = Det([\al]_{\bcal}) =a^3 - 42 abc + 28 b^3 + 98 c^3\hspace{5pt} \square $$ $\textbf{Solution for 2:}$ Consider $\beta = \frac{1}{3}(1 + 7 \qroot{28} + 2 \qroot{98})$ . Let $\gamma = \beta^3 - \beta^2$ . Then we must find $c \in \mathbb{Z}$ such that $\gamma + c\beta \in \mathbb{Z}$ . We obtain that $$\gamma = \frac{1}{3}(1154 + 455\qroot{28} +130\qroot{98})$$ Hence, for $c = -65$ we obtain $$\gamma + c\beta =  \frac{1154}{3} - \frac{65}{3} = 363$$ . $$ \therefore \beta^3 - \beta^2 - 65\beta - 363 = 0$$ $$ \therefore \beta \in \mathcal{O}[\qroot{28}] \hspace{5pt} \square$$","['abstract-algebra', 'integral-extensions', 'extension-field', 'field-theory']"
2819696,Explaining the resolution rule.,"It is nice to have inference rules explained informally. For example, the rule of Disjunctive Syllogism $((x \lor y) \land \neg y)\rightarrow x$ can be explained as follows: since $x \lor y$ is true, so either $x$ or $y$ is true. But $y$ is false and so $x$ must be true. I was trying to give a similar informal explanation of why the resolution rule is true but I could not come up with one:
$$((x \lor y) \land (\neg x \lor z))\rightarrow (y \lor z)$$
Is there any such convincing argument?","['propositional-calculus', 'logic', 'proof-theory', 'discrete-mathematics']"
2819734,Mean of series is always less than the last element,I'm working on an algorithm's efficiency and wanted to know if there was a way to show the following is true. As an example I did this with $n = 10$ and $n=4$ and it was true. I want to know if it is always true but not sure how to go about proving it. $$n^{n-1} < \frac{\sum_{k=0}^{n} {n^k}}{n} \le n^{n}$$,"['means', 'sequences-and-series', 'geometric-series']"
2819748,"What is the full name for the ""ARD"" of ARD kernel","See below, my question is not technical, but just asking what is the full name for the ""ARD"" in ARD kernel.","['statistics', 'linear-algebra']"
2819776,Evaluating the complex integral $\int_{0}^{\infty} \frac {\sin (\ln x) dx }{x^2 + 4} $,"I want to evaluate following integral: $\int_{0}^{\infty} \frac {\sin (\ln x) dx }{x^2 + 4}  $ Obviously $x$ $ \gt $ $0$ and the function we want to integrate isn't even nor odd. And I need to avoid $0$. We got two first order poles at $+2i$ and $-2i$ $f(z) = \frac {\sin (\ln z)  }{z^2 + 4} = \Im \{\frac {e^{(i\ln z)}  }{z^2 + 4} \} $ and we are dealing with a complex logarithm The residue of the function above for $+2i$ is equal to $res(f, +2i) =\frac {e^{(i Ln 2)}}{4i}$ and I am considering calculating the residue for $-2i$ (it would just switch a sign) The solution is according to textbook: $\frac {\pi \sin (Ln(2)}{4 \cosh(\pi /2)}$ My questions : 1) How to deal with the integral from $0$ to $\infty$, not from $- \infty$ to $\infty$ in this case. I was used to deal with even functions where it was obvious. 2) I have no clue about the integration path (a semi-circle with branch cuts?) 3) Why that $cosh$ in that solution? This type of the complex integral is very new to me and I would appreciate any help!","['residue-calculus', 'complex-analysis', 'integration', 'complex-numbers']"
2819848,Evaluate the Limit $\lim_{x \to 0}\frac{x}{\sqrt{1-e^{-x^2}}}$,Evaluate the Limit $$L=\lim_{x \to 0}\frac{x}{\sqrt{1-e^{-x^2}}}$$ Now it is in Indeterminate form $\frac{0}{0}$ I Tried using L'Hopital's Rule as below: $$L=\lim_{x \to 0}\frac{1}{\frac{1}{2\sqrt{1-e^{-x^2}}}{\left(-e^{-x^2}\right)}{(-2x)}}$$ $\implies$ $$L=\lim_{x \to 0}\frac{\sqrt{1-e^{-x^2}}}{x}=\frac{1}{L}$$ hence $$L=1$$ is this right approach?,"['algebra-precalculus', 'derivatives', 'limits']"
2819849,Can we split/decompose a multivariable function into several single-variable functions?,"If I have a multivariable function, can I split/decompose it into several single-variable functions? For instance: Given $f:\mathbb R^2 \rightarrow \mathbb R$, I introduce the functions $g:\mathbb R\rightarrow \mathbb R$ and $h:\mathbb R\rightarrow \mathbb R$ so
$$
f(x,y)=g(x)h(y)
$$
Or
$$
f(x,y)=g(x)+h(y)
$$ Is this mathematically correct? Is function composition the right name? Ex. 1: The function $f:\mathbb R^2\rightarrow \mathbb R$ is given by $f(x,y)=2xy$. Introduce the functions $g,h:\mathbb R\rightarrow \mathbb R$ and write
$$
f(x,y)=2xy=g(x)h(y)
$$
where $g(x)=2x$ and $h(y)=y$. Ex. 2: Or if $f(x,y)=2x+y$, we write
$$
f(x,y)=2x+y=g(x)+h(y)
$$
where $g(x)=2x$ and $h(y)=y$.","['multivariable-calculus', 'real-analysis', 'calculus', 'analysis']"
2819906,Number of rectangles with odd area.,"We have a $10\times 10$ square. How many rectangles with odd area are on the picture? I say lets choose a vertex first, there are $11\cdot11=121$ possibilities. Now, choose odd width and side (left or right) and odd length and side (up or down). There are $5$ possibilities to each, so in total we have $\dfrac{121\cdot 25}{4}$ rectangles. We divide by $4$ because every rectangle is counted $4$ times, one time for each vertex of the rectangle. But, the result is not a whole number. Where am I wrong? Thanks.","['combinatorics', 'geometry']"
2819919,Estimation $\mu^2$ under certain conditions.,"Let $X_1,X_2,....,X_n$ be a random sample of size $n$ from a population with cdf $F()$. Let $E(X)=\mu$ exist. Then estimate $\mu^2$ unbiasedly for the following three cases:- (i) $Var(X)=\sigma^2$ exists and is known. (ii) $Var(X)=\sigma^2$ exists and is unknown. (iii) $Var(X)$ does not exist. Now for (i) & (ii) I have obtained the following solutions:- (i) We define $$\bar X=\frac1n\sum_{i=1}^n X_i$$
The UE of $\mu^2$ is given as follows $$\bar X^2-\frac{\sigma^2}{n}$$
(ii) We define $$S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar X)^2$$
The UE of $\mu^2$ is given as follows $$\bar X^2-\frac{S^2}{n}$$ For (iii) I need help. And please let me know if my answers thus far are correct. Thanks.","['parameter-estimation', 'statistics', 'estimation', 'statistical-inference']"
2819941,Remainder of Taylor series and CLT proof,"I'm trying to understand part of the proof of the Central Limit Theorem in Casella Berger 2E. I realize there are many functionally equivalent ways to prove the CLT, but there's a specific step the authors use that I'm trying to find justification for. To summarize the salient parts, suppose we have a random variable $Y$ with mean 0 and variance 1, and its MGF exists for |$t$| < some positive $h$. We're interested in the limiting behavior of $M_Y(\frac{t}{\sqrt{n}})$, where $M_Y(t)$ is the MGF of $Y$. The Maclaurin expansion follows as: $M_Y(\frac{t}{\sqrt{n}}) = 1 + \frac{(t/\sqrt{n})^2}{2} + R_Y(\frac{t}{\sqrt{n}})$ The authors then use $lim_{n\to\infty}\frac{R_Y(t/\sqrt{n})}{(t/\sqrt{n})^2}=0$ in the proof, and they appear to justify that step by citing later ""Taylor's major theorem, which we will not prove here, is that the remainder from the approximation $g(x)-T_r(x)$ always tends to 0 faster than the highest-order explicit term"". Which result or formulation of Taylor's Theorem justifies this limiting behavior?","['self-learning', 'probability-theory', 'taylor-expansion', 'central-limit-theorem']"
2819957,Semisimple representation is determined by characteristic polynomials?,"Let $\rho, \rho' : G \to GL(V)$ be two finite dimensional semisimple representations of a group $G$ (possibly infinite). If the characteristic polynomials of $\rho(g)$ and $\rho'(g)$ are equal for any $g \in G$, can we conclude that $\rho$ is equivalent to $\rho'$ ? I did not find any reference for this. Thank you!","['representation-theory', 'group-theory']"
2819958,Generalisation of Schur's lemma,"Proposition: Given two square matrices $A,B \in \mathbb{C}^{n \times n}$ which fulfil $AM = MB$ for all $M \in \mathbb{C}^{n \times n}$ they must be identical and a scalar multiple of the identity matrix: $A = B = \lambda \mathbb{1}$, $\lambda \in \mathbb{C}$. Proof: Since the relation $AM = MB$ holds for any matrix $M$, we can in particular choose the matrices $(M_{mn})_{ij} = \delta_{mi} \delta_{nj}$ (all elements are zero, except for $(m,n)$, which is $1$). Thus,
$$
\sum_{j=1}^n A_{ij} (M_{mn})_{jl} = \sum_{j=1}^n (M_{mn})_{ij} B_{jl}
\Leftrightarrow
\sum_{j=1}^n A_{ij} \delta_{mj} \delta_{nl} = \sum_{j=1}^n \delta_{mi} \delta_{nj} B_{jl}
\Leftrightarrow
A_{im} \delta_{nl} = \delta_{mi} B_{nl} \,.
$$
Now, we distinguish the cases $n=l$, $m=i$: $A_{ii} = B_{ll}$, i.e. all diagonal elements must be the same: $A_{ii} = B_{ii} = \lambda$. $n=l$, $m\neq i$: $A_{im} = 0$, i.e. all off-diagonal elements of A must vanish. $n\neq l$, $m=i$: $0 = B_{nl}$, i.e. all off-diagonal elements of B must vanish. Thus, $A=B=\mathrm{diag}(\lambda,\dots,\lambda)$. Question: This proposition reminded me a bit of Schur's lemma from group representation theory, but I think that this case is not covered by Schur's lemma since $A$ and $B$ can be different and $M$ does not need to be invertible. I was wondering whether the proposition is just a special case of a more general form of Schur's lemma or whether there is some other well known theorem that I could refer to instead of the proof above. Bonus question: Is there a proof of this statement that doesn't require to go to component notation?","['matrices', 'linear-algebra']"
2819961,Lagrange multiplier when decisions variables are not in the same set,"Find the maximum of $2x+y$ over the constraint set $$S = \left\{ (x,y) \in \mathbb R^2 : 2x^2 + y^2 \leq 1, \; x \leq 0 \right\}$$ I want to use Lagrange multipliers to find the optimal solution. However, Lagrange requires $\vec x \in A$. In our case $R$, however $x$ can only be negative or zero. How can I get rid of this constraint? My idea is to do $x=w-z, w-z \le 0, w,z \in R$, but I am not sure if this is the right way to do it.","['optimization', 'multivariable-calculus', 'qclp', 'convex-optimization', 'lagrange-multiplier']"
2819964,Proof involving generating function,"The following is part of a proof that the number of ways of associating a product with $n$ terms (different ways of inserting parentheses) is $$
a_1 = 1,\ a_n = \frac{1}{n} \binom{2n-2}{n-1},$$
and the relationship$$
a_{n+1} = a_1a_n + a_2a_{n-1} + a_3a_{n-2} + \cdots + a_na_1$$
is already established. The proof starts with the generating function $$f_A(x) = \sum_{n=1}^\infty a_nx^n,$$
then by the relationship, $$f_A(x) = x + \sum_{n=2}^\infty (a_1a_{n-1} + \cdots + a_{n-1}a_1)x^n.$$
I am unclear about the next step:
$$f_A(x) = x + \left(\sum_{n=1}^\infty a_nx^n\right)\left(\sum_{n=1}^\infty a_nx^n\right).$$ I tried using an upper bound of three in each expression. For the first expression I obtained $$a_1^2x^2 + 2a_1a_2x^3.$$
For the second expression I obtained $$a_1^2x^2 + 2a_1a_2x^3 + a_2^2x^4 + 2a_2a_3x^5 + a_3^2x^6.$$
As it appears the first two terms are the same. Can we ignore the extra terms as the upper bound approaches infinity?","['generating-functions', 'sequences-and-series', 'elementary-number-theory']"
2819990,Lagrange multipliers: find minimum with constraints,"Find the minimum of $f(x,y,z) = z$ subject to the constraints $x + y + z = 1$ and $x^2 + y^2 = 1.$ So far, I have $$(0,0,1)=\lambda(1,1,1)+\mu(2x,2y,0)$$ $$0=\lambda+2\mu x \implies 2\mu x=-1 \implies x=-1/2\mu$$ $$0=\lambda+2\mu y \implies 2\mu y=-1 \implies y=-1/2\mu$$ $$1=\lambda$$ I feel like I'm not on the right path as there is no $z$ variable. Any input is appreciated.",['multivariable-calculus']
2820005,Lebesgue Measure and Isometries on the Unit Square,"This is a question I had on an exam from awhile ago and was rather curious about. ""Let $m$ denote the Lebesgue measure on the unit square $I^2 = [0,1]^2$. Define two mappings $S,T: I^2 \to I^2$ as follows: $S(x,y) = (y,x)$ and $T(x,y) = (x,\bar{y})$ where $\bar{y} = y + \frac{1}{\sqrt{2}}$ if $y \leq 1 - \frac{1}{\sqrt{2}}$ and $\bar{y} = y + \frac{1}{\sqrt{2}} - 1$ otherwise. Now suppose that $A \subset I^2$ is a measurable subset such that $S(A) = A$ and $T(A)=A$. Show that either $m(A) = 0$ or $m(A)=1$."" My thoughts/attempt: $S$ is a reflection of $I^2$ along the diagonal and $T$ is a translation of the $y$ coordinate ""up"" by $\frac{1}{\sqrt{2}}$. But if it were to translate $y$ too far; i.e. leave $I^2$, it loops around so we can think of these isometries acting on a cylinder where we identify the top and bottom of the square. On the other hand, $STS(x,y) = ST(y,x) = (x + \frac{1}{\sqrt{2}},y)$ (or the other expression, depending on what $x$ is). So $STS$ is a horizonal translation which also loops around so now, it seems we should think of $S,T$ as isometries on the torus. Now, this set $A$ is invariant under both maps as is $m$ so it makes intuitive sense that it should be either measure-theoretic large or small. Since $\frac{1}{\sqrt{2}}$ is irrational, if we apply our horizontal or vertical translations to a point $p \in A$ and consider its orbit, I believe its orbit should be dense in $I^2$. So suppose that $0 < m(A)$. It seems like on the one hand, we can ""cover"" all of $I^2$ by translations of $A$ since $A$ has positive measure; i.e. its orbit should have full measure $1$. On the other hand, $A$ is invariant under $S,T$ so it equals its orbit. I'm just not clear on how to prove this rigorously. My hunch is to use something like Lebesgue density. The definition of the density of $A$ at a point $p$ is (when the limit exists) $$D_A(p) = \lim_{r \to 0}\frac{m(A \cap B(r,p))}{m(B(r,p))}.$$ The Lebesgue Density Theorem says that if $E \subset \mathbb{R}^n$ is any Borel set, then for almost every point $p \in E$, $D_E(p) = 1$ and for almost every $p \notin E$, $D_E(p) = 0$. We can assume $E$ to be Borel as there is a theorem which basically allows us to write $E$ as some $G_\delta$ set minus a null set and $G_\delta$ sets are Borel. So let $p \in A$ be such a point where $D_A(p) = 1$. Let $\mathcal{O}(p)$ be the orbit set of $p$ under $S,T$. It seems that for each $q \in \mathcal{O}(p)$, $D_A(q) =1$ because we may take a small ball around $p$ and intersect that with $A$. Now apply some finite combination of $S,T$ to this intersection set; it is moved about but will still end up in the set $A$ and since these are rigid transformations, the measure of this intersection set should be the same. Moreover, this should hold for every small ball so the density ought to still be 1 around $q \in \mathcal{O}(p)$. But $\mathcal{O}(p)$ is dense in $I^2$, topologically. So for each point in a dense set in $I^2$, the Lebesgue density at that point is 1. Question: Can we conclude that since $$D_A(p) = \lim_{r \to 0}\frac{m(A \cap B(r,p))}{\pi r^2} = 1,$$ for sufficiently small $r$, for every $q \in \mathcal{O}(p)$, $m(A \cap B(r,q)) \approx \pi r^2$, for the same $r$? If so, can we say that the since the measure of $A$ locally is positive in a dense set, then $m(A) = 1$? Problem: If $q \in \mathcal{O}(p)$ happens to be a corner or edge point in the square, then maybe the density should be $\frac{1}{2}$ or $\frac{1}{4}$.","['lebesgue-measure', 'measure-theory']"
2820017,Finding reason for removal of the logarithm in ODE,"I'm given an ODE
$$y' = \frac{x + 2y + 1}{2x + 3},~y(5/2) = 1/4.$$
To solve this I'm doing the following: Solve system of equations
$$2x_0 + 2y_0 = 1,$$
$$2x_0  = 3,$$
this gives me solutions $x_0 = \frac{3}{2}$, $y_0 = -\frac{1}{4}$. Then, I'm transforming variables $s = x + \frac{3}{2}$, $t = y - \frac{1}{4}$. Transform the ODE using those new variables to the form
$$t' = \frac{s + 2t}{2s}$$
and using substitution of the form $t = s\cdot u(s)$ the equation is reduced to the
$$u' = \frac{1}{2s} \implies u' = \frac{1}{2}\ln|C\cdot s|.$$
Using initial conditions constant C can be found using $y(5/2) = 1/4$, $s(5/2) = 4$, $t(4) = 0$, $u(4) = 0$, hence $0 = \ln|4C| \implies C = \pm \frac{1}{4}$. Putting it all together yields me
$$y = \frac{1}{2}\left(x + \frac{3}{2}\right)\ln\left|\frac{x + 3/2}{4}\right| + 1/4$$
with two domains $I_1 = (-3/2, +\infty)$ and $I_2 = (-\infty, -3/2)$.
But my textbook states that the answer is 
$$y = \frac{1}{2}\left(x + \frac{3}{2}\right)\ln\frac{x + 3/2}{4} + 1/4,~I = (-3/2, +\infty)$$ Hence the following two questions: What was the reason for removal of absolute value in the logarithm? Why is mine domain of validity wrong?","['homogeneous-equation', 'ordinary-differential-equations', 'initial-value-problems', 'absolute-value']"
2820020,Is there a characterization of the permutations arising from this 'buggy' shuffle?,"The classical 'correct' way of shuffling a deck of $n$ items is to use the Fisher-Yates shuffle : for (i = n; i > 1; i--) {
  choose j randomly in [1..i];
  swap deck[i] and deck[j];
} It's not hard to show that this algorithm works: there are $n!$ different paths through the code (there are $n$ possible choices for $j$ in the first iteration of the loop, then $n-1$ choices in the second iteration, giving $n\cdot(n-1)\cdot(n-2)\cdot\ldots=n!$ possbilities in total), and each of these generates a different permutation. Now consider an identical algorithm, but with $j$ chosen randomly from the range $[1\ldots (i-1)]$; in other words, at each step, the value at position $i$ is explicitly swapped with some other value (it can't be left in place). It's straightforward to show that every permutation that can be generated this way is a derangement: when we reach entry $k$ in the array, either its value is still $k$, in which case it's swapped with a lower entry and then entry $k$ is locked at a value different from $k$, or its value is no longer $k$, in which case $k$ was swapped into some entry of higher index and it can never be swapped back 'home'.  On the other hand, not all derangements can be obtained this way; for one thing, it's clear that the number of paths is $(n-1)!$, which is smaller than the number of derangements $D_n=n!(1-\frac1{2!}+\frac1{3!}-\ldots\pm\frac1{n!})$ for all $n\gt 3$; for another thing, all permutations generated by this algorithm for a given $n$ have the same parity (which depends directly on the parity of $n$). Is there any 'nicer' characterization of the permutations that can be obtained this way than the one given by this algorithm? (Equivalently, I think: is there a 'nice' way of describing the embedding $\mathcal{S}_{n-1}\hookrightarrow\mathcal{S}_n$ generated by this algorithm?)","['permutations', 'combinatorics', 'algorithms']"
2820034,"For a map $F: \mathbb{H} \rightarrow \mathbb{D}$, prove that $|F(z)| \leq |z-i|/|z+i|$ for all $z \in \mathbb{H}$.","For notation let $\mathbb{H}$ denote the upper-half plane and $\mathbb{D}$ the open unit disk. There was exercise on which I was stuck on: Let $F: \mathbb{H} \rightarrow \mathbb{C}$ be a holomorphic function that satisfies
  $$|F(z)| \leq 1$$ and $$F(i)=0.$$
  Prove that $|F(z)| \leq |z-i|/|z+i|$ for all $z \in \mathbb{H}$. So far: The condition $|F(z)| \leq 1$ is basically saying that we are mapping $\mathbb{H}$ to the disk $\mathbb{D}$. We also have that the map $\varphi(z) = (z-i)/(z+i)$ is a map from $\mathbb{H}$ to $\mathbb{D}$ that sends $i \mapsto 0$. Suppose we consider the map $F(\varphi^{-1}(z))$; then this is self-analytic map of the disk that fixes the origin. We can therefore invoke the Schwarz-lemma that tells us that $|F(\varphi^{-1}(z))| \leq |z|$. I'm having trouble seeing how to finish the problem from here, I was wondering if I could get recommendations on either a different approach or on how to finish.","['complex-analysis', 'conformal-geometry']"
2820042,Are the Lie groups $GL_n(\mathbb R)$ and $GL_n(\mathbb C)$ unimodular?,"I want to know whether $GL_n(\mathbb R)$ is a unimodular Lie group, that is, whether it has a Haar measure that is both left and right invariant. What I've tried so far is seeing $GL_n(\mathbb R)$ as an open submanifold of $\mathbb R^{n^2}$ and I've tried to prove that for any $A\in GL_n(\mathbb R)$ and any open $U\subseteq GL_n(\mathbb R)$ we have $\mu(A\cdot U)=\mu(U)$, where $\mu$ is the Lebesgue measure of $\mathbb R^{n^2}$, but I've had no success. I don't know whether the same strategy works for $GL_n(\mathbb C)$.","['haar-measure', 'measure-theory', 'linear-groups', 'lie-groups']"
2820090,Why $(y-\overline y)^t[f(x)-f(\overline x)]\ge-\Vert y-\overline y\Vert\Vert f(x)-f(\overline x)\Vert$?,Let $f:\mathbb R^n\to\mathbb R^{m+l}$ be continuous and $\overline y\in\mathbb R^{m+l}$. Why $0\ge (y-\overline y)^t[f(x)-f(\overline x)]\ge-\Vert y-\overline y\Vert\Vert f(x)-f(\overline x)\Vert$ ? What relation exists between the transpose of a vector and the norm with minus sign? Thank you. Edit: sorry I forgot the 0 at the beginning of the inequality.,"['inequality', 'continuity', 'proof-explanation', 'linear-algebra', 'analysis']"
2820097,"Prob. 6, Exercises 8.14, in Apostol's CALCULUS Vol II: Given a scalar field differentiable at a point ...","Here is Prob. 6, Exercises 8.14, in the book Calculus Vol. II by Tom M. Apostol, 2nd edition: Given a scalar field differentiable at a point $\mathbf{a}$ in $\mathbb{R}^2$, suppose that $f^\prime(\mathbf{a}; \mathbf{y})=1$ and $f^\prime(\mathbf{a}; \mathbf{z})=2$, where $\mathbf{y} = 2\mathbf{i}+3\mathbf{j}$ and $\mathbf{z} = \mathbf{i} + \mathbf{j}$. Make a sketch showing the set of all points $(x, y)$ for which $f^\prime(\mathbf{a}; x\mathbf{i}+y\mathbf{j})=6$. Also, calculate the gradient $\nabla f(\mathbf{a})$. My Attempt: As $f$ is differentiable at $\mathbf{a}$, so for every $\mathbf{b} \in \mathbb{R}^2$ the directional derivative $f^\prime(\mathbf{a}; \mathbf{b})$ exists and is given by 
  $$ f^\prime(\mathbf{a}; \mathbf{b}) = \nabla f(\mathbf{a}) \cdot \mathbf{b}.  $$ Now let $(u, v)$ be the gradient vector of $f$ at the point $\mathbf{a}$. Then from the given information we can conclude that 
  $$ 2u + 3v = 1 \qquad \mbox{ and } \qquad u+v = 2, $$
  and upon solving these two equations simultaneously for $u$ and $v$ we get $u = 5$ and $v=-3$. Thus 
  $$ \nabla f(\mathbf{a}) = (5, -3). $$ Am I right? Thus the set of all the points $(x, y)$ for which $f^\prime(\mathbf{a}; x\mathbf{i} + y\mathbf{j})=6$ is 
  $$ \left\{ \ (x, y) \in \mathbb{R}^2 \ \colon \ 5x-3y=6 \ \right\}, $$
  which is the straight line in the plane through the point (0, -2) and having the slope $5/3$. Am I right?","['derivatives', 'real-analysis', 'calculus', 'multivariable-calculus', 'analysis']"
2820135,Evaluation of a Fresnel type integral.,"I was evaluating the following complex integral via gamma function: $\int_0^\infty \sin (x^p) \,dx$ $\;$ for $p \gt 1$ ,
so I expressed it as an imaginary part of $\int_0^\infty \exp(-ix^p) \, dx$ $\;$ for $p \gt 1$ The formula of the gamma function is $\Gamma (z) = \int_0^\infty x^{z-1} e^{-x} \, dx  $ I used the substitution $-y^{1/p}=xi$ , $\;$ $\;$ $dx= \frac 1 p y^{\frac{1}{p}-1}i \, dy$ $\;$ $\;$ and $\;$ $\;$ $\frac {1}{p} = \alpha$ Then $\int_0^\infty \alpha i y^{\alpha-1}e^{-y} \, dx = \alpha i \Gamma (\alpha) = \ i \frac {1}{p} \Gamma (\frac {1}{p})$ The solution according to my textbook is $\  \frac {1}{p} \Gamma (\frac {1}{p}) \sin (\frac {\pi}{2p})$ But I think $\sin (\frac {\pi}{2p})$ is right if I have ${i}^p$ , but I got just $i$ . My solution is then $\  \frac {1}{p} \Gamma (\frac {1}{p}) \sin (\frac {\pi}{2}) =\frac {1}{p} \Gamma (\frac {1}{p})$ . Did I miss something important? EDIT I tried to calculate this integral for $p = 2$ and the textbook is right, but why?","['complex-analysis', 'integration', 'calculus']"
2820143,How to solve the given integral of the type: $\int_{0}^{\infty}\tfrac{1}{(a + b x)(c + d x) (1 + p x)}dx$,"Can anyone give a hint how to solve integral of the following type: $$\int_{0}^{\infty}\dfrac{1}{(a + b x)(c + d x) (1 + p x)}dx$$ The problem is if we proceed with partial fraction, we get
$$\dfrac{1}{(a + bx)(c + dx)(1 + px)} = \dfrac{b^2}{(bc - ad)(b - ap)(a + bx)} + \dfrac{d^2}{(bc - ad)(cp - d)(c + dx)}- \dfrac{p^2}{(b - ap)(cp-d)(1 + px)}$$ Now solving the integral for the first part, $$\int_{0}^{\infty}\dfrac{b^2}{(bc - ad)(b - ap)(a + bx)} dx = \left.\dfrac{b \ln(a + bx)}{(bc - ad)(b - ap)}\right\vert_0^{\infty}$$ Hence a $\ln(\infty)$ term is encountered. Same is the case with all the three parts. Any hints appreciated. Edit: The final solution using Mathematica:","['logarithms', 'integration', 'definite-integrals', 'limits']"
2820212,Circumradius of a tetrahedron,"I put the following formula for the circumradius of a tetrahedron on the Wikipedia page on the tetrahedron, but it was deleted for lack of a citation.  Does anyone have a reference for it? Here, $a,b,c$ are three edges that meet at a point; $A,B,C$ are the opposite edges; and $V$ is the volume of the tetrahedron.
$$R = \frac{\sqrt{(aA+bB+cC)(aA+bB-cC)(aA-bB+cC)(-aA+bB+cC)}}{24V}$$","['reference-request', 'geometry']"
2820221,Tangent Plane for a Level Set,"I am currently looking at the following theorem which I know how to prove from one side but not from the another side: Theorem : If $a$ is a regular value of a smooth function $F:U\subset\mathbb{R}^3 \rightarrow \mathbb{R}$ and $p \in F^{-1}(a)=S$, then the tangent plane $T_pS=(\nabla F(p))^{\perp}$. The definition of surfaces and tangent planes follow the textbook ""Differential Geometry of Curves and Surfaces"" by M. do Carmo. Now, I know how to prove $T_pS\subset(\nabla F(p))^{\perp}$ simply by chain rule. But I have no idea how to prove the another side of the subset relation, because what I need to do is that given $u \in (\nabla F(p))^{\perp}$, I need a curve $\alpha:(-\epsilon,\epsilon)\rightarrow S$ such that $\alpha(0)=p$ and $\alpha'(0)=u$, but I have no idea how to find such a curve.","['multivariable-calculus', 'differential-geometry']"
2820230,confused about $\omega_{\pm}-$limit set of a subset X example,"I am reading Ordinary Differential Equations and Dynamical Systems by G.Theschl, and I am confused about the definition of the $\omega_{\pm}-$limit set of a set $X \subseteq M$. 
The definition states that $\omega_{\pm}(X)$ is the set of all points $y \in M$ for which exists sequences $t_n \rightarrow \pm \infty$ and $x_n \in X$ with $\Phi(t_n,x_n) \rightarrow y$ Now, consider the system $\dot{x}=x(1-x^2)$, $\dot{y}=-y$. Clearly, the $x-$direction has two stable fixed points at $x=\pm1$ and an unstable fixed point at $x=0$. It is also clear that the $y-$direction has one stable fixed point at $y=0$. The author claims that $\omega_+(B_r(0))=[-1,1] \times\{0\} $, $r>0$. It is clear to me that $(0,0),(-1,0),(1,0)$ are in $\omega_+(B_r(0))$. However, I don't get why the whole interval $[-1,1]$ is also there. For a given point in $[-1,1] \times \{0\}$, what would be the two sequences $t_n$ and $x_n$ that satisfy the definition? I would really appreciate if someone can clarify this for me. Thanks!","['general-topology', 'ordinary-differential-equations', 'dynamical-systems']"
2820234,"Does the sequence $n+\tan(n), n \in\mathbb{N}$ have a lower bound?","Is the sequence $n+\tan(n), n \in\mathbb{N}$ bounded below? Intuitively I think it is not bounded below, but I have no idea how to prove it. It is like a Diophantine approximation problem, but most theorems seem to be too weak.","['diophantine-approximation', 'trigonometry', 'number-theory', 'sequences-and-series', 'approximation']"
2820260,Trace cauchy schwarz inequality,"This was an interesting result I found while reading Holevo's Quantum Information theory book. They call it the non-commutative operator Cauchy Schwarz. Let $S$ be a state i.e. a non negative operator with unit trace on a Hilbert space $\mathcal{H}$ in $\mathbb{C}$. Then for arbitrary operators $X$ and $Y$ on $\mathcal{H}$, 
\begin{equation}
 |Tr(SX^*Y)|^2 \leq Tr(SX^*X)Tr(SY^*Y)\label{Thi}
\end{equation} I was able to show this by mirroring the usual Cauchy Schwarz inequality (I assumed RHS is finite). Then a friend suggested that it might follow easily if we show that $Tr(SX^*Y)$ is an inner product on the operator space. I was able to show that it is a semi inner product. To prove it is an inner product, I needed to show 
$$Tr(SX^*X) = 0 \Rightarrow X=0$$ I wasn't successful with this one. I was able to show it if $S$ is a strictly positive operator (at least I think I did) but not for non negative. My guess is that in general, $Tr(SX^*Y)$ is a seminorm but not necessarily a norm. But I was unable to furnish an $X$ such that $X\ne 0$ but $Tr(SX^*X) = 0$. I seek such an example. Kindly give me some ideas. We could think in terms of matrices here. Update: Upon further inspection, I realized that my ""proof that mirrors cauchy schwarz"" was in fact flawed. I assumed implicitly that both terms in RHS were $>0$. In one of the steps, I divide by the square root of RHS, which would be illegal if it were $0$. This wasn't a problem in usual Cauchy schwarz because we were given that LHS is square of mod of an inner product. This changes my perception and I now believe that the above is an inner product but I need to show the final step. Update: I'll show my work so far. Let $\langle X,Y\rangle_T \triangleq Tr(SX^*Y)$. Then Linearity in second argument (In Quantum theory, it is the second argument) and Conjugation: Easy to show. Non-Negative for equal arguments: We have 
$$Tr(SX^*X) = Tr(XSX^*)$$
Since $S \geq 0$, $XSX^* \geq 0$ and hence $Tr(XSX^*) \geq 0$. Also if $X=0$, then $\langle X,X\rangle_T = 0$. Need to show if $Tr(SX^*X) = 0$ then $X=0$.
If $S$ is strictly positive definite, then for any orthonormal basis $e_i$ $$ 0 = \langle X,X\rangle_T = Tr(\sqrt{S}X^*X\sqrt{S}) = \sum_{i=1}^d \langle e_i,\sqrt{S}X^*X\sqrt{S}e_i\rangle \\
   = \sum_{i=1}^d \langle X\sqrt{S}e_i,X\sqrt{S}e_i\rangle = \sum_{i=1}^d \|X\sqrt{S}e_i\|^2$$ Hence $\|X\sqrt{S}e_i\| =0 $ for every $i$ and this further implies $X\sqrt{S} = 0$. Since $\sqrt{S}$ is invertible owing to strict positivity, we get $X=0$.This completes the proof when $S$ is strictly positive definite. For non-negative definite, I don't know how to tackle it as $S$ may have a non-trivial kernel/nullspace.","['cauchy-schwarz-inequality', 'operator-theory', 'functional-analysis', 'trace', 'linear-algebra']"
2820276,Doubt in proving $\ln(2)$ is irrational,I tried to prove $\ln(2)$ is irrational using the method of contradiction knowing that $e$ is irrational: Let $$\ln2=\frac{p}{q}$$ be in simplest form where $p$ and $q$ are positive integers. Now we have $$e^{\frac{p}{q}}=2$$ $$2^q=e^p$$ Now $2^q$ is always an even positive integer. How can we reason that $e^p$ can never be an integer?,"['algebra-precalculus', 'logarithms', 'irrational-numbers', 'ratio']"
2820297,Convergence of $a_n=(1/2)^{(1/3)^{...^{(1/n)}}}$,"The sequence $a_n=(1/2)^{(1/3)^{...^{(1/n)}}}$ doesn't converge, but instead has two limits, for $a_{2n}$ and one for $a_{2n+1}$ (calculated by computer - they fluctuate by about 0.3 at around 0.67). Why is this?","['sequences-and-series', 'power-towers']"
2820317,Equality of Expecations Regarding i.i.d Random Variables.,"If we have $n$ i.i.d random variables $X_1,\ldots,X_n$, and some real-valued function $g: \mathbb{R} \to \mathbb{R}$, is it true that $\mathbb{E}(g(X_1)) = \cdots = \mathbb{E}(g(X_n))$? I think this is true since all the random variables follow the same distribution, when it comes down to it, calculating the expectation for each random variable will follow the same procedure, resulting in the same answer. Can anybody provide a more rigorous justification/proof? Or is this a fine proof? let $i,j \in \{1,\ldots,n\}$ with $i \neq j$. Discrete case: Since $X_1,\ldots,X_n$ are i.i.d, they all have the same associated probability function, so $$\mathbb{E}(g(X_i)) = \sum_{\text{all }x} g(x)\mathbb{P}(X_i = x) = \sum_{\text{all} \ x} g(x)\mathbb{P}(X_j = x) = \mathbb{E}(X_j)$$ Continuous case: Since $X_1,\ldots,X_n$ are i.i.d, they all have the same associated pdf $f$, so $$\mathbb{E}(g(X_i)) = \int_{\text{all } x} g(x) f(x) \, dx = \mathbb{E}(g(X_j))$$ Hence, the result.","['expectation', 'statistics', 'random-variables', 'probability-distributions']"
2820352,Can the gradient of a non-constant scalar field be zero?,"Let $S$ be an open set in $\mathbb{R}^n$, and let $f \colon S \to \mathbb{R}$ be a scalar field such that all the partial derivatives $D_1 f (\mathbf{a}), \ldots, D_n f (\mathbf{a})$ exist for all points $\mathbf{a} \in S$. If the gradient $\nabla f(\mathbf{a})$ of $f$ is zero for all points $\mathbf{a} \in S$, then we can show that $f$ is constant on $S$. Am I right? If we know that the gradient of $f$ is zero on $S$, then we can even show that $f$ is constant on the closure of $S$. Am I right? Now my question is, if we know that the gradient of $f$ is zero on $S$, then can we conclude that $f$ is constant on some set in $\mathbb{R}^n$ that properly contains the closure of $S$? What is the most general statement of this sort that could be made?","['derivatives', 'real-analysis', 'calculus', 'multivariable-calculus', 'analysis']"
2820362,Conformal map from $\mathbb C\setminus \{0\}$ to the open unit disk.,"Is there a conformal/analytic map from $\mathbb C\setminus \{0\}$ to the open unit disk? I think the answer is no but I'm not sure how to prove it. I know that the former is not simply connected and the latter is simply-connected, but I'm not sure why this is useful or relevant since analytic maps are not necessarily homeomorphisms.",['complex-analysis']
2820409,"Solution of $\frac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x$ given $x=0$, $y=\mathrm{e}$","$\dfrac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x$ , $x=0$ and $y=\mathrm{e}$ . Find the particular solution. Attempt 1 $$
\dfrac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x\implies\dfrac{\mathrm{d}y}{y}=\mathrm{e}^x\,\mathrm{d}x\implies \log|y|=\mathrm{e}^x+C\\
x=0,\,y=\mathrm{e}\implies\log|\mathrm{e}|=\log\mathrm{e}=1=1+C\implies C=0\\
\log|y|=\mathrm{e}^x\implies|y|=\mathrm{e}^{\mathrm{e}^x}\implies \color{red}{y=\pm\mathrm{e}^{\mathrm{e}^x}}
$$ Attempt 2 $$
\dfrac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x\implies\dfrac{\mathrm{d}y}{y}=\mathrm{e}^x\,\mathrm{d}x\implies \mathrm{e}^x=\log|y|+\log|C_1|=\log|C_1y|\\
\mathrm{e}^{\mathrm{e}^x}=|C_1y|=\pm C_1y=Cy\\
x=0,\,y=\mathrm{e}\implies\mathrm{e}=C\mathrm{e}\implies C=1\\
\implies \color{red}{y=\mathrm{e}^{\mathrm{e}^x}}
$$ My reference also gives the solution $\log y=\mathrm{e}^x$ as in attempt 2. Why do I seem to get positive and negatve solutions in attempt 1 ? How do I eliminate the solution $y=-\mathrm{e}^{\mathrm{e}^x}$ in attempt 1 ? Note: I am not quite familiar with the idea of singularity or intermediate value theorem, as i have only done preliminary maths on first order differential equations.","['logarithms', 'exponential-function', 'ordinary-differential-equations', 'absolute-value']"
2820414,Understanding this proof for $\sin(a+b)=\sin(a)\cos(b)+\sin(b)\cos(a)$ from Gelfand,We would like to have a small question concerning this proof for $\sin(a+b)=\sin(a)\cos(b)+\sin(b)\cos(a)$ from Gelfand's Trigonometry Why is it true that $\frac{q}{d}=\frac{a}{d}$. Is it necessary to use properties of similar triangles?,"['trigonometry', 'proof-explanation', 'geometry']"
2820423,"Maximize $f(x,y)=xy$ subject to $x^2-yx+y^2 = 1$","Use Lagrange multipliers method to find the maximum and minimum values of the function 
  $$f(x,y)=xy$$
  on the curve
  $$x^2-yx+y^2=1$$ Attempt: First I set let $g(x,y)=x^2-xy+y^2-1$ and set $$\nabla f=\lambda\nabla g$$
so
$$(y,x)=\lambda(2x-y,2y-x)$$
then
$$\begin{cases} 
\lambda=\frac{y}{2x-y} & (1) \\
\lambda=\frac{x}{2y-x} & (2)\\
x^2-yx+y^2=1
\end{cases}
$$
Solving $(1)$ and $(2)$ simultaneously, I get that $$y^2=x^2$$
Substitutiting into $(3)$ and following through with the arithmetic, I get four candidates for max and min, namely $$(1,1),(-1,-1),\big(-\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}}\big),\big(\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}}\big)$$
 Evaluating these points on $f$, I get that the maximum value is $$1 \ \text{at} \ (\pm1,\pm1)$$
and the minimum value is $$-\frac{1}{3} \ \text{at} \ \big(\pm\frac{1}{\sqrt{3}},\mp\frac{1}{\sqrt{3}}\big)$$
Am I correct? I am unsure if there are indeed four critical points.","['optimization', 'proof-verification', 'multivariable-calculus', 'maxima-minima', 'lagrange-multiplier']"
2820433,Integral $\int_0^{\pi} \frac{\cos(2018x)}{5-4\cos{x}}dx$,"I wish to evaluate $$I(2018)=\int_{0}^{\pi}\frac{\cos(2018x)}{5-4\cos x} dx$$ Considering $$X=I(k)+iJ(k)=\int_{-\pi}^{\pi}\frac{\cos{kx}}{5-4\cos x} dx +i\int_{-\pi}^{\pi}\frac{\sin{kx}}{5-4\cos x} dx=\int_{-\pi}^{\pi}\frac{e^{ikx}}{5-4\cos x} dx$$ let us substitute $$e^{ix}=z\rightarrow dx=\frac{dz}{iz} \, ,|z|=1$$ Due to Euler's formula we can rewrite $$\cos x=\frac{z^2+1}{2z}$$ $$X=\oint_{|z|=1} \frac{z^k}{5-4\frac{z^2+1}{2z}}\frac{dz}{iz}=\frac{1}{i}\oint_{|z|=1} \frac{z^k}{-2z^2+5z-2}dz$$ $$-2z^2+5z-2=-\frac{1}{2}((2z)^2-5(2z)+4)=-\frac{1}{2}(2z-4)(2z-1)=-2(z-2)(z-\frac{1}{2})$$ Now let us notice that in our contour $|z|=1\,$ only the pole $z_2=\frac{1}{2}$ is found. Thus the integral we seek to evaluate is $$\frac{1}{i} \cdot 2\pi i \, \text{Res} (f(z),z_2)$$ where $f(z)=\frac{z^k}{-2(z-2)(z-\frac{1}{2})}$ $$X=2\pi \lim_{z\to z_2} (z-z_2)\frac{z^k}{-2(z-2)(z-z_2)}=\frac{2}{3}\pi \frac{1}{2^k}$$ therefore $$I(k)=\Re (X) =\frac{2\pi}{3}\frac{1}{2^k}$$ And $$\int_{0}^{\pi}\frac{\cos(2018 x)}{5-4\cos x} dx=\frac{\pi}{3}\cdot\frac{1}{2^{2018}}.$$ Now someone told me that the answer is $0$ and I am wrong (also wolfram gives $0$ as an answer). Could you please clarify? Or maybe give another solution to this integral if it's $0$ or another answer?","['integration', 'proof-verification', 'complex-integration']"
2820491,"Using the Lagrange Multipliers Method to prove $\ |h(x,y)|\leq 1$","Using the Lagrange multipliers method, I have found that the  maximum and minimum values of the function 
  $$f(x,y)=xy$$
  on the curve
  $$x^2-yx+y^2=1$$ are
  $$1 \ \text{at} \ (\pm1,\pm1) \ \ \ \text{and} \ \ -\frac{1}{3} \ \text{at} \ \big(\pm\frac{1}{\sqrt{3}},\mp\frac{1}{\sqrt{3}}\big) \ \ \text{respectively.}$$ 
  Using this, prove that $$\Big|\frac{xy}{x^2-yx+y^2}\Big|\leq1 \ \ \ \forall(x,y)\neq0$$ I don't really know where to start. I thought of multiplying across as the inequality will be preserved, which yields
$$|xy|\leq|x^2-yx+y^2|$$
then
$$|xy|-|x^2-yx+y^2|\leq 0$$
At this point, I'm relying on my algebraic manipulation skills to yield something true. I'm not using the information above obtained by the Lagrange multipliers method.","['multivariable-calculus', 'inequality', 'optimization', 'lagrange-multiplier']"
2820496,Fourier transform of distributions on the circle,We can easily define Fourier transform of tempered distributions on the real line (or any Euclidean space $R^n$). My question is: can we define Fourier transform of distributions on the circle in a similar way? Here I think we can define distributions as the dual space of smooth functions on the circle (smooth periodic functions).,"['functional-analysis', 'fourier-series', 'fourier-analysis', 'analysis']"
2820498,Explicit expression for $b$ as a function of $a$ where $\log_b a = (a/b)^{1/2}$,"I was talking with a friend about how to find a more explicit formula for the relationship between the positive numbers $a$ and $b$ in the equation
$$
\log_b a = (a/b)^{1/2}
$$
after some rearranging we get that this is equivalent to
$$
a^{a^{-1/2}}=b^{b^{-1/2}}.
$$
The function $f(x)=x^{x^{-1/2}}$ achieves its global maximum of $e^{2/e}$ at $x=e^2$ and is strictly increasing on $(0,e^2)$ and strictly decreasing on $(e^2,\infty)$. Moreover $\lim\limits_{x\to-\infty}f(x)=-\infty,$ $\lim\limits_{x\to-\infty}f(x)=1$ and $f(1)=1$. Hence for any $a \in (1,e^2)$ there exists exactly one other number $b=b(a) \in (e^2,\infty)$ so that $f(a)=f(b).$ Here is a plot for the number $a$ with $f(a)=2:$ $a$ is the $x$-value of the first intersection of the two lines, $b$ is the $x$-value of the second intersection. The function $b:(1,e^2)\to(e^2,\infty)$ is differentiable, strictly decreasing and satisfies $\lim\limits_{a\downarrow 1}b(a)=\infty$ and $\lim\limits_{a\uparrow e^2}b(a)=e^2.$ I think it should be possible to find its derivative given what we have but I'm not sure how to do this. Question: Is it possible to find an explicit expression for the function $b$? Also, is there a name for this way of defining a function?","['real-analysis', 'functions', 'terminology', 'recreational-mathematics', 'analysis']"
2820514,Why is a line integral of a conservative vector field independent of path?,"I am looking for intuition behind why the ability of a vector field $\vec{F}(x,y)$ to be written in the form $\vec{F}(x,y)=\nabla f$ where $f$ is simply a function of $x,y,$ and $z$ implies that a line integral on that conservative vector field is independent of path. Furthermore, what does a conservative vector field look like visually ? What properties do conservative vector fields have in the $xy$ plane that non-conservative vector fields don't? As a window to complex analysis, does there exist a relationship between the fact that line integrals of conservative vector fields in $\mathbb{R}^n$ on closed curves  is $0$ and Cauchy's Integral Theorem?","['multivariable-calculus', 'complex-analysis']"
2820548,Prove this $\sin(nx)$ identity,Without induction How to prove that $$\sin(2nx)=2n\sin x \cos x \prod_{k=1}^{n-1}\left(1-\frac{\sin^2(x)}{\sin^2\frac{k\pi}{2n}}\right)$$ for Natural n I Tried by several way and the last try is to use euler formula which lead me to $$\sin(2nx)=\sin x \cos^{2m+1} x \sum_{k=0}^{n-1} \left( (-1)^k\binom{2n}{2k+1}(\cos x \sin x )^{2k}\right)$$ and no idea how to continue or if this method lead to the answer,['trigonometry']
2820554,How do you read $\pm ?$ Why does $|x|=3\implies x=\pm 3 ?$,"Question 1 How is the symbol '±' read? ""Plus AND Minus"" or Plus OR Minus""? All this time I've been reading it as Plus-Minus. My math teacher says that it's the former; but then in $$x^2 = 9 \implies x= ± 3, $$ how does $x$ equal two values? $x$ should equal $+3$ OR $-3,$ right? Question 2 My teacher says that $$|x| ≠ ± x\tag A$$ but that $$|x| = 5 \implies x = ±5.\tag B$$ Wait... what? What is going on here? He also says that $$\sqrt{x^2}= |x|.$$ But this means that $$|3|=\sqrt{9} =3,$$ which, by $(\text B),$ implies that $$ |3| = ± 3;$$ doesn't this contradict $(\text A) ?$","['algebra-precalculus', 'notation', 'absolute-value']"
2820559,Number of ways to distribute 10 things among 6 people given that the number of things given to two people doesn't exceed 4?,"Here is a more specific question: Find the number of ways of giving $10$ identical gift boxes to 6
  people : $A$, $B$, $C$, $D$, $E$, $F$ in such a way that total number
  of boxes given to $A$ and $B$ together does not exceed $4$. I am currently learning about problems which are related to Combinations with Repetitions , for the most part of it I am able to solve the basic questions, but this one stumped me. Can you suggest how to approach this problem?","['combinations', 'combinatorics']"
2820570,Equality of Hom groups via 5 lemma,"Reading a paper I found the following statement: given two spectra $A, B$ since multiplication by $p$ induces the same endomorphism in $[A,B]$ we have $[A \wedge M, B]\cong [A, \Sigma^{-1}M \wedge B]$, where $M$ is the mod $p$ Moore spectrum. I think the idea is simple: since the smash with $M$ is just taking the cone of the multiplication by $p$ we have exact triangles in the stable homotopy category
$A \xrightarrow{p} A \rightarrow A \wedge M$ and $\Sigma^{-1}M \wedge B \rightarrow B \xrightarrow{p} B$. Thus applying respectively the functors $[-, B]$ and $[A,-]$ we get two long exact sequences in the form $[A,B]_{*+1} \xrightarrow{p} [A,B]_{*+1} \rightarrow Z \rightarrow [A,B]_{*} \xrightarrow{p}[A,B]_{*}$ where $Z$ is $[A \wedge M, B]_* $ or $[A, \Sigma^{-1}M \wedge B]_*$, so we should deduce that these two groups are isomorphic via the 5 lemma. The point of my question is that there is no canonical map between them, so I do not know if the 5 lemma can be applied.
We have two maps $[A \wedge M, \Sigma^{-1} M \wedge B] \rightarrow [A \wedge M, B]$ and $[A \wedge M, B] \rightarrow [A,B]$ but I do not understand if I can obtain a map making the diagram of long exact sequences commute. The other option I see is that  we can produce a map between the two groups via diagram chase using the fact that the other morphisms are invertible: I tried to do this but I cannot conclude the result. Since this kind of proof is immediate I suppose that this claim is false: it would be like proving that in the 5 lemma the vertical map in the middle is not needed. Thanks in advance for any help.","['abstract-algebra', 'exact-sequence', 'stable-homotopy-theory']"
2820593,Find the surface area of the part of the sphere $x^2 + y^2 + z^2 = 16$ inside the cylinder $x^2 - 4x + y^2 = 0$,"Find the surface area of the part of the sphere $x^2 + y^2 + z^2 = 16$
  inside the cylinder $x^2 - 4x + y^2 = 0$ First of all, I need to find the equation of the plane along which these two solids intersect. Solving for $y^2$ from the equation of the cylinder and substituting to the sphere, we get
$$z^2+4x=16$$
And so we have the integration are with respect to $z$ and $x$. The transform we want are considering is
$$T(x,z) = (x, z, 16-x^2-z^2)$$
We are integrating on the $z^2+4x=16$ surface. and so we need to set bounds for $z$ and $x$.
$$z_0 = 0 \le z \le z_1 =4 $$
$$x_0 = -\sqrt{16-z^2} \le x  \le x_1 = \sqrt{16-z^2}$$
Because of symmetry, we can find the are above the $xy$ plane and then multiply by $2$.
And so, using the formula for the surface integral, we get that the surface are
$$S = \int_{z_0}^{z_1}\int_{x_o}^{x_1}\sqrt{1 + (\frac{\partial y}{\partial z})^2+ (\frac{\partial y}{\partial x})^2}$$ Is my attempt to solve this correct? I have never calculated a surface integral before and so I am not sure about the bounds and the method.","['multivariable-calculus', 'surface-integrals', 'integration', 'calculus']"
2820595,"Is it always true, that $\lim_{n \to \infty} \frac{|A_n \cap H|}{|A_n|} = \frac{1}{[G:H]}$?","Suppose, $G$ is a finitely generated group. Suppose $A_1$ is a finite symmetric generating set. (That means $A_1 \subset G$, $|A_1|$ is finite, $\langle A_1 \rangle = G$, $e \in A_1$, $\forall a \in A_1 (a^{-1} \in A_1)$.) Suppose the sequence $\{A_n\}_{n=1}^{\infty}$ of subsets of $G$ is defined by recurrent relation: $\forall n \in \mathbb{N} (A_{n+1} = A_nA_1)$, where $A_nA_1$ denotes subset product. Suppose $H$ is a subgroup of $G$ of finite index. Is it always true, that $\lim_{n \to \infty} \frac{|A_n \cap H|}{|A_n|} = \frac{1}{[G:H]}$? For finite groups the answer is obvious. However, I do not know how to deal with this problem in the case, when $G$ is infinite. Any help will be appreciated.","['additive-combinatorics', 'limits', 'abstract-algebra', 'finitely-generated', 'group-theory']"
2820616,"Why is it incorrect to define an ""impure"" function?","While doing a math question on piecewise functions, I came across an answer which seemed wrong to me but I could not explain it mathematically. The question states (summarised): A lottery dealer makes 6 cents on each ticket sold. If the lottery dealer sells more than 25,000 tickets, she gets an extra 2 cents for every ticket after the 25,000th ticket. Express her profits as a function of the number of sold tickets. The correct (or expected) answer was the piecewise function:
$$
f(x) = \begin{cases}
6x,       & \text{when } 0 \le x \le 25000 \\
8x-50000, & \text{when } x \gt 25000
\end{cases}
$$ However, another answer I saw was: $$
\text{let } x \text{ be the number of tickets sold } \le 25000 \\
\text{let } z \text{ be the number of tickets sold } \gt 25000 \\
f(x) = 6x + 8z
$$ I found this answer intuitively incorrect. To me, all mathematical functions are pure (a concept from programming). In particular, all values in a function should not be determined by anything other than it's inputs. In this case, $f(x)$ is defined with an extra $z$ which is not given as an input. But this is a concept from programming, not mathematics. After reading the definition of a function on Wikipedia, I don't see anything that explicitly says that additional variables are allowed or not allowed in function definitions. Is there something I missed in the definition or is the second answer acceptable? EDIT To clarify and amplify the problem, here is another example. Let's say I want to define a function that adds $x$ to the current hour (in 24 hours). For example, $$
\text{let } t \text{ be the current 24 hour time, } 0 \le t \le 23, t \in \mathbb{Z} \\
f(x) = t + x
$$ Similar as before, this introduces a variable $t$ that is not given as an input to $f(x)$. My question is this: What part of the mathematical definition of a function disallows this?",['functions']
