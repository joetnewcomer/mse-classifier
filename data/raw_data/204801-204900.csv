question_id,title,body,tags
4062338,Structure Theorem for non-abelian finite groups or rings,"How many structure theorems do we have in Abstract Algebra for finite algebraic structures? I know some of the following theorems: If $G$ is a finite abelian group, then $G$ is a product cyclic groups of prime power order. If $R$ is a finite commutative ring, then $R$ is Artinian and hence a direct product of local rings. I am wondering if there are other structure theorems for finite algebraic structures.
Is it possible to write a non-commutative or a non-abelian group in this way? Why are the theorems available for abelian cases but not for non-abelian ones? If someone can please help me find some references where I can get structure theorems for non-commutative or a non-abelian group/rings, I will be thankful. Please help.","['finite-rings', 'finite-groups', 'ring-theory', 'abstract-algebra', 'group-theory']"
4062394,How to go about finding whether this limit exists: $\lim_{x \to 0} x [[\frac{1}{x}]]$?,"I am trying to solve few challenge questions on Real Analysis from Kaczor and Nowak's Problems in Mathematical Analysis , to become more proficient and stimulate thinking. I'd like someone to (a) verify if my proof is correct (b) is there a way to rigorously show that the limit does not exist? Note. $[[x]]$ is the greatest integer less than or equal to $x$ for all $x \in \mathbf{R}$ . Find the limits or state that they do not exist. Problem 1.1.1 (b) $\lim_{x \to 0}x\cdot[[\frac{1}{x}]]$ Proof. Consider the sequences $a_n:=\frac{1}{n}$ and $b_n=-\frac{1}{n}$ . Both these sequences converge to zero. The corresponding image sequences are, \begin{align*}
(f(a_n)) &= \frac{1}{1}\cdot 1, \frac{1}{2} \cdot 2, \frac{1}{3}\cdot 3, \ldots \\
&= 1,1,1,1,1,\ldots
\end{align*} \begin{align*}
(f(b_n)) &= -\frac{1}{1}\cdot (-1), -\frac{1}{2} \cdot (-2), -\frac{1}{3}\cdot (-3), \ldots \\
&= 1,1,1,1,1,\ldots
\end{align*} Also, consider the sequence $c_n:=\frac{1}{\sqrt{n}}$ . The sequence $(c_n)$ also converges to zero. \begin{align*}
(f(c_n)) &= \frac{1}{\sqrt{1}}\cdot [[\sqrt{1}]], \frac{1}{\sqrt{2}} \cdot [[\sqrt{2}]], \frac{1}{\sqrt{3}}\cdot [[\sqrt{3}]], \frac{1}{\sqrt{4}} \cdot [[\sqrt{4}]], \ldots \\
&= 1,\frac{1}{\sqrt{2}},\frac{1}{\sqrt{3}},1,\frac{2}{\sqrt{5}},\frac{2}{\sqrt{6}},\frac{2}{\sqrt{7}},\frac{2}{\sqrt{8}},1,\ldots
\end{align*} Thus, the image sequence oscillates between $0$ and $1$ and is not convergent. This violates the definition of functional limits. We require that, for all sequences $(x_n)$ in the domain of the function $f$ , such that $(x_n) \to a$ , $x_n \ne a$ for all $n \in \mathbf{N}$ , the image sequence $f(x_n)$ converges to $L$ . Then, $\lim_{x \to a}f(x) = L$ . So, the above limit does not exist.","['limits', 'solution-verification', 'real-analysis']"
4062448,Is there a Graph theory way to solve this iteration problem,"This thought came when i was trying to find the number of functions $f(x)$ from $\{1,2,3,4,5\}$ to $\{1,2,3,4,5\}$ that satisfy $f(f(x)) = f(f(f(x)))$ for all $x$ in $\{1,2,3,4,5\}$ .  What i did was took cases when only one , two...etc  values of x such that all those are identity functions from that i got the answer as 756 which is correct.  But i was thinking if there was some kind of GT use can be done here  which can solve it more easily than case bash?","['graph-theory', 'combinatorics']"
4062470,$L^1$ Approximation of BV functions by piecewise constant function,"Let $f \in BV(\mathbb{R}) \cap L^1(\mathbb{R}),$ consider the piecewise constant function  defined by \begin{eqnarray}
f^{\delta}(x):= \frac{1}{\delta}\int\limits_{k\delta}^{(k+1)\delta}f(y)dy \quad \text{for } x\in [{k\delta},{(k+1)\delta})
\end{eqnarray} Then clearly $f^{\delta} \in L^{1}(\mathbb{R})$ and $f^{\delta} \rightarrow f$ pointwise a.e.in $\mathbb{R}.$ Now, my doubt is ""Do we have $\|f^{\delta}-f\|_{L^1(\mathbb{R})}\leq C \delta$ for some $C$ independent of $\delta?$ ""
If so how to prove it?","['bounded-variation', 'measure-theory', 'analysis', 'real-analysis']"
4062473,Solutions of $u_{xx} + 2 \mathrm i u_{xy} + u_{yy} = 0$,"A problem in A. Friedman, Partial Differential Equations, 1969, chapter 1, section 14, is concerned with the elliptic, but not strongly elliptic PDE on the open unit disk $\mathbb D = \{z= x+ \mathrm iy \mid |z| < 1\}$ given by \begin{equation*}
u_{xx} + 2\mathrm i u_{xy} + u_{yy} = 0, \qquad u(z) = 0, \forall z\in \partial \mathbb D.
\end{equation*} Friedman claims that for any complex analytic function $f(z)$ defined on the closed unit disk $\bar{ \mathbb D}$ , the function $u(z) = (1-|z|^2) f(z)$ solves the Dirichlet problem. However, taking $f = 1$ already shows that this is not true. Can you explain that? Can you give a better example?","['complex-analysis', 'elliptic-equations', 'partial-differential-equations']"
4062477,what do holomorphic functions 'look like'? Please provide references.,"Question 1 : What do holomorphic functions 'look like'? Not really sure what is meant by this, but I heard this was asked this in an interview for graduate school admissions. What I have in mind is that A. holomorphic functions are conformal, infinitely complex differentiable or something (not sure of the precise wording, but i guess it's like 'complex smooth'), equivalent to complex analytic (therefore we can just drop the qualifier 'complex') and have harmonic real and imaginary parts (i didn't learn in elementary complex analysis, but in miranda's book 'harmonic' is defined precisely as this. in elementary complex analysis, i learned holomorphic implies harmonic parts). Not really sure how to graph any of these, but the conformal thing is on wiki . Here's the picture: B. when you're asked what continuous functions $f: \mathbb R \to \mathbb R$ 'look like', intuitively/heuristically, you can draw a part of it on a paper without lifting your writing utensil from the paper. so you probably just do something that looks like sin/cos or a polynomial. (i remember in calc 1, the 1st continuous functions we're introduced to are these smooth functions sin/cos and polynomials.) I think this is kind of what the question means. C. from the very definition itself, i just think to draw some circles like $g: \mathbb C \to \mathbb C$ holomorphic at $z=w$ means complex differentiable in some disc/disk around $z=w$ . but i guess i'm just making drawings on its domain rather drawing the function itself. Question 2 : what books answer this? Question 3 : In relation to (B) above I actually find this question kind of weird. I mean does it make sense to ask what differentiable real functions $f: \mathbb R \to \mathbb R$ 'look like' ? twice differentiable functions $f: \mathbb R \to \mathbb R$ 'look like'? thrice? smooth functions $f: \mathbb R \to \mathbb R$ ? real-analytic functions? Question 4 : What do poles look like? what do poles 'look like'? Please provide references. Update : Based on the comments below, this version of the Maximum modulus principle seems to be relevant: Let $D \subseteq \mathbb C$ be bounded, non-empty and open. Suppose $f: \overline D \to \mathbb C$ is continuous on $\overline D$ and holomorphic on $D$ . Then $|f|$ attains a maximum at some point in boundary of $D$ (i guess topological boundary, and i guess equal to $\overline D \setminus D$ ). The comments Severin Schraven include: It makes quite a lot of sense to ask this question in my opinion. The point is that holomorphic functions have a lot of structure and force them to have certain features. (...) The maximum principle is a very strong property. General continuous functions can be extremely wild! A comment of Moishe Kohan: Strictly speaking, the question is meaningless unless the meaning of ""looks like"" is specified. For this reason, no self-respecting textbook will discuss this, nor an interviewer at a grad school interview. They may ask: ""What local properties of holomorphic functions do you know?"" This would be a reasonable ""long list"" question. God bless you, Moishe Kohan.","['complex-analysis', 'maximum-principle', 'book-recommendation', 'intuition']"
4062487,How to solve this step to get this,"$\sqrt{2A^2(1+\cos \theta)} $ to $\sqrt{4A^2\cos^2 \dfrac {\theta} {2}}$ (Here, divided by $2$ is only under $\theta$ ) I have solved till : $\dfrac {2A\sqrt{1+\cos\theta}}{2}$ (divided by $2$ is whole under $1 + \cos \theta$ )",['trigonometry']
4062489,Why does the intersection of prime ideals uniquely determine the prime ideals?,"For context, I'm going through an introductory course in algebraic geometry. The course is such that it doesn't assume any commutative algebra knowledge so many results are taken on belief, but on the other hand we spend some time proving small results to get a feel for things. Claim: Let $P_1, \ldots, P_k$ be prime ideals where $i \neq j \implies P_i \nsubseteq P_j$ . A finite intersection of prime ideals $I = \bigcap \limits^k_{i=1} P_i$ uniquely determines the set of prime ideals $\{P_1, \ldots, P_k\}$ which $I$ is an intersection of. Attempt: The key idea is to use prime avoidance lemma, but I have no clue how it is applied here. It is supposed to be used as follows. Say we have another way to write the intersection with some other prime ideals: $I = \bigcap\limits^l_{i=1}Q_i$ . We see that $\bigcap \limits^l_{i=1}Q_i \subseteq P_1$ , so supposedly now prime avoidance lemma implies that there exists $Q_j$ such that $Q_j \subseteq P_1$ . Buying this result, the claim follows easily, but I cannot for the life of me prove this by using prime avoidance lemma. I think that a way to show this without using the prime avoidance lemma is by noticing that $Q_1\cdots Q_l \subseteq \bigcap\limits^l_{i=1}Q_i \subseteq P_1$ . Then, since $P_1$ is prime, it follows that $\exists j: Q_j \subseteq P_1$ and we are done. A search online has yielded either extensive articles on commutative algebra or proofs of prime avoidance lemma, which is not what I am looking for. I also haven't studied commutative algebra in an organized way yet, I will do so next semester, so I have no way with textbooks like Atiyah or Eisenbud to find what I am looking for. I spent more than an hour proving this and trying to find a way to use prime avoidance lemma, but it would seem that I am missing some obvious detail and I will probably hate myself for spending too much time on a triviality like this. Just hints would be most appreciated.","['algebraic-geometry', 'commutative-algebra']"
4062498,Proving that a function of the form f(x+y)=f(x)*f(y) is continuous,"I have the to solve the following problem: Let $f$ be a function from the real numbers to the real numbers. The function satisfies $f(x+y) = f(x)f(y)$ for all real $x,y$ . Prove that if $f$ is continuous in $0$ then $f$ is continuous in every point. I think I have a solution but I would like to know if it's correct: By letting $y=0$ we get that $f(x)=f(x)f(0)$ meaning that $f(0)=1$ . Thus we know that for every $\epsilon > 0$ and for every $x$ there exists a $\delta$ such that $|f(x)-1| < \frac{\epsilon}{|f(a)|} $ when $|x| < \delta$ . Since this holds for all $x$ we can replace $x$ with $x-a$ . Rewriting this we get that whenever $|x-a| < \delta$ we have that $\,|f(a)f(x-a)-f(a)|<\epsilon \implies |f(x)-f(a)| < \epsilon$ .","['continuity', 'calculus', 'solution-verification', 'analysis']"
4062538,Is maximum of sum exists if each function has a maximum?,"Let $f,g:D \rightarrow \mathbb R$ be multivariable functions over the open set $D$ . Assume that $f,g$ both have a global maximum point inside $D$ . Can we be sure that $f+g$ also has a global maximum point inside $D$ ? I know that the maximum point if exists must satisfying $\max(f+g) \leq \max f + \max g$ . But will the maximum point exist in general? If not, is there any condition that is needed for the existence of the global maximum?","['optimization', 'multivariable-calculus', 'maxima-minima', 'analysis']"
4062585,How to show this integral inequality?,"For $t \in \mathbb{R}$ , let $$
  I_1(t) = \int_{0}^1 \frac{x e^x}{1 + \cosh(t x)} dx
$$ and $$
  I_2(t) = \int_{0}^\infty \frac{x e^{-x}}{1 + \cosh(t x)} dx.
$$ I want to show $I_1(t) \geq I_2(t)$ with equality only if $t = 0$ . Numerical integration suggests this is true. Any suggestions?","['integration', 'trigonometry']"
4062616,"Finding the smallest $\alpha>0$ for which $\exists\beta(\alpha)>0$ so that $\sqrt{1+x}+\sqrt{1-x}\le 2-\frac{x^\alpha}\beta,\forall x\in[0,1]$.","Find the smallest $\alpha>0$ for which there is $\beta(\alpha)>0$ so that the following inequality holds $\forall x\in[0,1]$ : $$\sqrt{1+x}+\sqrt{1-x}\le 2-\frac{x^\alpha}\beta$$ and find $\min\beta(a)$ for that particular $a$ . Source: Elezović, N., Odabrani zadatci elementarne matematike, Zagreb, 1992 EDIT: By $\beta(\alpha)$ I denoted a positive $\beta$ depends on $\alpha$ . If $\beta>0$ exists for some $\alpha>0$ , it doesn't have to be unique. My thoughts: Function $f(x)=\sqrt{1+x}+\sqrt{1-x}$ is concave on $(-1,1)$ , and particulary on $(0,1)$ , which follows from the fact that $$f''(x)=-\frac14\left(\frac1{\sqrt{(1+x)^3}}+\frac1{\sqrt{(1-x)^3}}\right)<0,\space\forall x\in(0,1).$$ Since $f$ is concave, each of its tangents is above the graph $\Gamma(f)$ . My first idea was to observe the polynomial $g(x)=f'(c)(x-c)+f(c),c\in (0,1),\deg g=1,$ which made me suspect that $\alpha\ge 1$ . I noticed that a function $f_2(x)=-\frac{x^\alpha}\beta$ is concave for $\alpha>1,$ convex for $0<\alpha<1$ and both convex and concave for $\alpha=1,$ when $f_2$ is linear. Then, $$\alpha\ge 1\implies f_3(x)=2-\frac{x^\alpha}\beta\text{ is also concave}.$$ and this polynomial $f_3$ and the function $f$ should have at most one intersection and at $x=0$ . However, I'm not able to justify my claim that this is the only possibility. Also, $f'$ is strictly decreasing on an open interval, and hence, my attempt to express $\min\beta(\alpha)$ in terms of $f'$ in the second part of the task failed. May I ask for advice on solving this task? Thank you very much in advance!","['functions', 'real-analysis']"
4062655,Gradient of a quadratic form with respect to a complex vector,"How would I go about calculating the derivative with respect to $x$ of $$Q(x) = x^H A x $$ with $A$ a real matrix (not necessarily symmetric) and $x$ a complex valued vector? Here $(\cdot)^H$ denotes the conjugate transpose. EDIT: This question came up in the context of the maximization problem: $$ \text{arg}\max_x \, x^H A x \qquad \text{s.t.} \quad x^Hx = 1$$ I think for solving this I need to solve $ \nabla_x \, Q (x) \stackrel{!}{=} 0$ , so what I am looking for is the complex gradient vector. Am I correct in assuming (following the Matrix Cookbook and using that $A$ is real) that: $$  \nabla_x \, Q(x) = \frac{\partial Q}{\partial Re(x)} + i \frac{\partial Q}{\partial Im(x)} = \, \dots \, =  2 A x$$ where I used that $Q(x) = (Re(x) - i\, Im(x))^T A \,(Re(x) + i\,Im(x)) $ ? EDIT2: The above seems to only hold if $x^H A \, x$ is real (additional question: for which matrices $A$ would that be the case? Maybe when I can decompose $A$ into $A = B^TB$ or respectively $B^HB$ for complex A?).","['scalar-fields', 'complex-analysis', 'multivariable-calculus', 'derivatives', 'quadratic-forms']"
4062656,Are the ordinary least squares regression coefficients uniformly integrable?,"In my previous question I asked whether a set of asymptotically normal random variables $\{X_n\}_{n \ge 1}$ are uniformly integrable. In the accepted answer the poster showed that this does not hold in general. Now what about a specific case. Consider the standard ordinary least squares (OLS) regression coefficients $\hat{\beta}_n = \beta + (X_n^T X_n)^{-1} X_n^T \varepsilon$ where I have used the subscript $n$ to indicate that these are the OLS coefficients for a regression model with $n$ observations. Is $\{\hat{\beta}_n\}_{n \ge 1}$ uniformly integrable? Of course, $\hat{\beta}_n$ is a random vector, so I am asking whether the elements of this vector, which are all asymptotically normal, are uniformly integrable. Note that the $X_n$ above are stochastic regressors, not fixed. Edit: Attempt for the case of single regressor with no intercept Here is an attempt for the case of a single regressor with no intercept, i.e., the regression model is $$
Y = X \beta + \varepsilon,
$$ where we have $n$ i.i.d observations $Y = [Y_1,Y_2,\dots,Y_n]^T$ and regressors $X = [X_1,X_2,\dots,X_n]^T$ , and $\varepsilon = [\varepsilon_1,\varepsilon_2,\dots,\varepsilon_n]^T$ . The errors are mutually independent with $E[\varepsilon_i|X] = 0$ and they all have the same finite variance $\sigma^2$ . Let $\hat \beta_n$ denote the OLS estimate for $\beta$ where the subscript $n$ indicates the number of observations in the regression model. Note that because we only have a single regressor that $(X^T X)^{-1}$ reduces from a matix to a scalar: $$
(X^T X)^{-1} = \bigg(\sum_{i=1}^N X_i^2\bigg)^{-1}. 
$$ From what I understand, to show that $\{\hat \beta_n\}_{n \ge 1}$ is uniformly integrable, it is enough to show that there exists a $\delta >0$ such that $\sup_n E[|\hat\beta_n|^{1 + \delta}] < \infty$ . First, I think that wlog we can consider $\hat\beta_n - \beta$ instead of $\hat\beta_n$ in the above expectation. Let's take $\delta = 1$ and define $A = (X^TX)^{-1}$ . Then \begin{align}
E[|\hat\beta_n - \beta|^2]
&= 
E[|A\varepsilon|^2] \\
&= 
E[A\varepsilon \varepsilon A^T] \\
&= 
E[A E[\varepsilon \varepsilon|X]A^T] \\
&= 
\sigma^2 E[AA^T] \\
&= 
\sigma^2 E[(X^TX)^{-1}].
\end{align} It is commonly assumed that $\text{plim}_{n\to \infty} (X^TX/n) \stackrel{p}{\to} Q$ where $Q$ is a positive constant (usually a positive definite matrix, but since we only have a single regressor its a scalar in our case). And thus $$
\text{plim}_{n\to \infty} (X^TX/n)^{-1} \stackrel{p}{\to} Q^{-1}.
$$ Write $(X^TX)^{-1} = n^{-1}(X^TX/n)^{-1}$ and substitute into the expectation above to get \begin{align}
E[|\hat\beta_n - \beta|^2] \
& = \sigma^2 E[(X^TX)^{-1}] \\
& = \sigma^2 n^{-1} E[(X^TX/n)^{-1}].
\end{align} Now we know $(X^TX/n)^{-1} \stackrel{p}{\to} Q^{-1}$ and if can convert this convergence in probability into convergence in expectation we will have shown that $E[|\hat\beta_n - \beta|^2] < \infty$ (in fact it will go to zero due to the $n^{-1}$ coefficient), and thus the OLS regression coefficients are uniformly integrable. So it seems the problem of uniform integrability of the OLS regression coefficients reduces to the uniform integrability of $(X^TX/n)^{-1}$ . So here are my questions now: Does my analysis seem correct? Is $(X^TX/n)^{-1}$ uniformly integrable? If $(X^TX/n)^{-1}$ is not uniformly integrable by default is it reasonable to assume it is uniformly integrable? How strong an assumption would this be? P.S. We have $(X^TX/n)^{-1} = \bigg(\frac{1}{n} \sum_{i=1}^n X_i^2\bigg)^{-1}$ . So is the inverse of the second sample moment uniformly integrable?","['measure-theory', 'regression', 'uniform-integrability', 'probability', 'random-variables']"
4062666,"Show that the limit in $(0,0)$ of $\frac{xy}{x^3-y}$ doesn't exist.","I'm trying to show that the limit below doesn't exist, I think it's quite simples but I couldn't until now. $$\lim_{(x,y)\to (0,0)}\dfrac{xy}{x^3-y}$$ My approach is try to find two curves in the domain that have different limits in $(0,0)$ I couldn't find any curve that the limit isn't $0$ . Any tips? Thanks.","['limits', 'multivariable-calculus']"
4062789,Section 4.3 Page 26 in Loring Tu's Differential Geometry,"On Section 4.3, Page 26 in Loring Tu's Differential Geometry : The underlined part is what I cannot understand. Why is $\dfrac{dV}{dt}$ in general not defined? What does ""a canonical frame of vector fields"" exactly mean? I got confused at this point, and I hope someone could explain it explicitly for me.","['smooth-manifolds', 'differential-geometry']"
4062893,What's wrong in my approach in evaluating $\cos^{-1}(\cos\ 10)$,"The Question $\large\cos^{-1}(\cos 10)$ My Approach $\large\cos^{-1}(\cos\ (10-2\pi))$ As $2\pi$ is the period of $\cos x.$ Now our angle is- Now we know that for $\large\cos^{-1}(\cos x) = x$ , $\large x\ \epsilon\ [0,\pi]$ but currently our $\large x$ is in the 3rd quadrant, we need to get it into 1st or 2nd quadrant. Therefore let us subtract $\large \pi$ from our current angle i.e $\large 10-2\pi$ $\large\cos^{-1}(\cos\ -((10-2\pi)-\pi))$ Now you must be wondering why did I put this minus sign, w.k.t $\large \cos x$ is -ve in third quadrant, by subtracting $\large \pi$ we brought it into 1st quadrant and $\large \cos x$ is +ve in 1st quad, so to compensate that I have put a minus sign. Now its within range so we can use $\large\cos^{-1}(\cos x) = x$ . So Final answer $\large -((10-2\pi)-\pi)$ = $\large 3\pi - 10$ But this answer is wrong and the correct answer is $\large 4\pi - 10$ . They have used the property $\large \cos\  (\pi - \theta) = \cos\  (\pi + \theta)$ But why my approach is wrong? Kindly help.","['trigonometry', 'functions', 'inverse', 'inverse-function']"
4063015,Smallest example of two non-isomorphic 1-isomorphic groups,"I just stumbled upon the definition of a map between to groups $$\varphi:G \to H$$ being a 1-isomorphism. It means that it is bijective and satisfies $$\varphi(g)^n=\varphi(g^n)$$ for all $g \in G$ and $n \in \mathbb Z$ . So my question is: What is the smallest example of two non-isomorphic 1-isomorphic groups $G$ , $H$ ? Smallest here in terms of cardinality. For $|G|=|H|=16$ there are two such pairs. But maybe for even smaller cardinalities such pairs exist. I don't want to know just the answer and proof by computer. I want to know if there is a simple argument why no smaller examples exist.","['group-theory', 'finite-groups']"
4063050,Cauchy's integral formula when $z_0$ lies outside of contour w/o residue thrm,"I understand there are many questions regarding this topic (Complex integral where poles are outside of the given region) I just wanted to double check with everyone what I gathered so far: $$f(z_0) = \frac{1}{2\pi i}\int_\Gamma \frac{f(z)}{z-z_0}dz = 0, \\ $$ if $z_0$ lies outside of the given contour and there exists a closed loop around the pole where the loop is analytic everywhere (this is my understanding of holomorphic ). I think this is true due to the path independence but I can't prove it exactly. Can anyone please help? One specific example will be $$\int_C \frac{ze^z}{2z-3}dz$$ where C be the circle $|z-1.5| = 2$ traversed once in the positive sense. I calculated that $z_0 = 3/2$ does not lie inside of the curve ( $z$ intersects with the real axis at $\pm \sqrt{1.75}$ ). Hence, I claim according to the above assumption because the pole does not lie inside of the region of interest ( $C$ ), the integral is $0$ . I stated w/o residue thrm because we didn't learn it. I see many answers use residue thrm in their answer so it will be great if you could explain it w/o residue thrm!","['integration', 'complex-analysis', 'contour-integration']"
4063053,Are signed measures badly defined?,"I'm reading Real Analysis - Modern Techniques and Their Applications by Folland. On page 85, signed measures are defined as follows. Let $(X, \mathcal{M})$ be a measurable space. A signed measure on $(X, \mathcal{M})$ is a function $\nu : \mathcal{M} \rightarrow [-\infty,\infty]$ such that: $\nu(\varnothing) = 0$ ; $\nu$ assumes at most one of the values $\pm \infty$ ; if $\{E_j\}$ is a sequence of disjoint sets in $\mathcal{M}$ , then $\nu(\bigcup_1^\infty E_j) = \sum_1^\infty \nu(E_j)$ , where the latter sum converges absolutely if $\nu(\bigcup_1^\infty E_j)$ is finite. My issue is with the third condition. Specifically, what is it saying if $\nu(\bigcup_1^\infty E_j)$ is not finite? The summation might not even make sense, depending on the values of the $\nu(E_j)$ (e.g. if it is the alternating series $\sum (-1)^j$ ). Is this condition saying that the sum will always make sense? Or is it only saying the equality holds when the sum makes sense? How can this condition be formulated correctly? And is there a textbook with a precise, rigorous formulation? (I couldn't find one.)","['measure-theory', 'definition', 'signed-measures', 'real-analysis']"
4063063,Uniformly integrability of inverse of sample moments,"I am interested in the uniform integrability of the set $\{Y_n\}_{n\ge 3}$ where $$
Y_n = \bigg(\frac{1}{n} \sum_{i=1}^n X_i^k\bigg)^{-1},
$$ where the $X_i$ 's are i.i.d observations of a continuous random variable $X$ , $k\ge 1$ , and $E[X] < \infty$ . Is $\{Y_n\}_{n\ge 3}$ uniformly integrable for $k = 1$ which corresponds to the inverse of the sample mean? What about $k > 1$ ? In particular, the inverse of the second sample momemt, i.e. , $k=2$ , is what I am most interested in. If it is not uniformly integrable in general, is it uniformly integrable if we restrict the allowable $X$ ?","['measure-theory', 'statistics', 'uniform-integrability', 'probability', 'random-variables']"
4063066,Question on the interpretation of topological entropy,"The standard intuitive explanation for topological entropy is that it measures the exponential growth rate of the number of distinguishable orbits.  I'm not quite sure why this is the case, any thoughts?  Thanks. Edit:  The definition I'm using is for a continuous transformation T and an open cover $\alpha$ we define $h(T,\alpha)= \frac{1}{n} lim_{n \to \infty}H(\bigvee_{i=0}^{n-1}T^{-i}\alpha)$ .  Where $\vee$ is the adjoin (intersection) of all the covers and $H$ is the minimal covering of the adjoin.  Take the entropy to be the supremum over all the $\alpha$ .","['measure-theory', 'entropy', 'ergodic-theory', 'real-analysis', 'dynamical-systems']"
4063089,Uniformiser in book by Miranda,"I am doing Exercise V.2D in Miranda's book Algebraic Curves and Riemann Surfaces. Let $X$ be the projective plane curve defined by the equation $y^2 z=x^3-xz^2$ . Let $p_0=[0:1:0]$ , $p_1=[0:0:1]$ , $p_2=[1:0:1]$ and $p_3=[-1:0:1]$ . Show that $p_1+p_2+p_3\sim 3p_0$ . Taking $f=Y/Z$ , the points where it has zeros and poles are precisely the $p_i$ above. When I want to calculate the order of vanishing at $[0:0:1]$ , I go to the affine chart $Z\neq 0$ , then the point becomes $(0,0)$ and I have to calculate $ord_{(0,0)}(y)$ . I believe this is one, because there is no power in the $y$ . However, I know from the theory of uniformisers in algebraic geometry that this is not necessarily the case. For example, when I do the same trick for $
[0:1:0]$ then the order of vanishing should be -3 but is $ord_{(0,0)}(1/z)=-1$ when I reason like this. I can not find anywhere in Miranda's book the theorem that says that a function that vanishes at a point but is not the tangent line is a uniformiser. Could someone help me out ??
Also, in Miranda's book it seems that the order of vanishing is defined by the lowest non-zero term in the Taylor expansion, but isn't this $y$ ?","['riemann-surfaces', 'divisors-algebraic-geometry', 'algebraic-geometry']"
4063094,Reference request for 3-manifold,"I am asking a soft question. I am planning to learn $3$ -manifold using the book ""Geometry and topology of three-manifolds"" by William Thurston. I want to know how much of Riemannian geometry, Algebraic topology, Smooth manifolds, Complex Geometry do I need to learn $3$ - manifold. Also, I am looking for references of $3$ -manifolds which contain exercises as I noticed that  William Thurston's contain does not contain exercises. Also, it will be very much appreciatable if you also advise on book recommendations of $3$ -manifolds. Please advise me. Thanking in advance.","['reference-request', 'hyperbolic-geometry', 'low-dimensional-topology', 'algebraic-topology', 'differential-geometry']"
4063181,"How to define a sequence $(a_n)_{n\geq 0}$ in $\mathbb{R}$ such that each convergent sub-sequence has limit 1, but $(a_n)_{n\geq 0}$ doesn't converge? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Part of a weekly submission assignment is about: How to define a sequence $(a_n)_{n\geq 0}$ in $\mathbb{R}$ such that each convergent sub-sequence of $(a_n)_{n\geq 0}$ has limit 1, but $(a_n)_{n\geq 0}$ does not converge? Given that $(V, d_v)$ is a metric space, $(a_n)_{n\geq 0}$ is a sequence in $V$ with $ a \in V$ However, me and my fellow students have been trying to figure it out for some time now, but we can't...","['limits', 'proof-writing', 'sequences-and-series', 'real-analysis']"
4063207,Why is my proof of the integral of the dirac function being equal to 1 incorrect?,"I know that there are questions on this website on how to prove that the integral of the Dirac function is $1$ . However, I am interested in knowing why my proof is wrong and if there is a way to salvage it. So here's the question : The Dirac delta function can be defined by the limit of a short pulse: $$\delta(t-t_0) = \lim_{\Delta \rightarrow 0}({f_{\Delta}(t)})$$ where $f_{\Delta}(t) = \frac{1}{\Delta}$ for $t_0\leq t\leq t_0 + \Delta$ and $f_{\Delta}(t) = 0$ otherwise. Convince yourself that the integral $\int_{t_1}^{t_2} \delta(t-t_0) dt = 1$ if $t_1\leq t_0 \leq t_2 $ Here's my approach: $$\int_{t_1}^{t_2} \delta(t-t_0) dt = \int_{t_1}^{t_2} \lim_{\Delta \rightarrow 0}({f_{\Delta}(t)}) dt$$ $$=\int_{t_1}^{t_2} \lim_{\Delta \rightarrow 0}(\frac{1}{\Delta})dt$$ $$=\lim_{\Delta \rightarrow 0}(\frac{1}{\Delta})\int_{t_1}^{t_2}dt$$ $$=\lim_{\Delta \rightarrow 0}(\frac{1}{\Delta}) \cdot (t_2-t_1)$$ $$=\lim_{\Delta \rightarrow 0}(\frac{t_2-t_1}{\Delta})$$ If we let $t_1 = t_0$ and $t_2 = t_0 + \Delta$ , then we get $$\lim_{\Delta \rightarrow 0}(\frac{\Delta}{\Delta})$$ $$ = 1$$ Here's why I doubt my proof. First, in the third line of my development, I took out the limit from the integral. My reasoning behind the maneuver is that this limit does not depend on $t$ and therefore I can treat it as a constant. However, upon re-reading my proof, I am wondering if I am really ""allowed"" to do this. Since I am treating the limit as a constant, I am assuming that it exists and is finite, which is not the case, right? (since $\lim_{x \rightarrow 0} (1/x)$ does not exist). Secondly, to get to the 6th line, I defined $t_2 = t_0 + \Delta$ . Am I allowed to do this? I defined it that way only because it was mathematically convenient, not because it actually makes sense, which is why I doubt its validity. I personally think that the proof is incorrect, but I'm not sure. Perhaps one could argue otherwise and I would be interested to know how. So, can you confirm to me that my proof is incorrect, and can you please let me know if there's anything I can do to ""salvage"" it, or if I need to try another approach","['integration', 'limits', 'dirac-delta']"
4063214,"Can we construct groups of several discrete ""loops""?","I know that we have cyclic groups. Maybe the groups which in the most easy-to-access way can explain the concept of a generator . But do there also exist groups of two or more loops? For example $\cases{{g_1}^{n_1} = e\\{g_2}^{n_2} = e}$ With $$\{g_k,{g_k}^{1},\cdots,{g_k}^{n_k}\} \text{  distinct}$$ and $${g_1}^{m}{g_2}^n={g_2}^n, \forall n\neq kn_2$$ In other words the only way to enter a loop is through $e$ and trying to loop in the other direction if we already are in another loop will do nothing. Kind of like a state machine that starts counting one direction and then cannot count in the other direction until we have looped a whole circle around to $e$ . Is this possible or will this violate some axiom for groups? Is there some other algebraical concept which it would be possible for?","['finite-state-machine', 'reference-request', 'abstract-algebra', 'group-theory', 'soft-question']"
4063235,"Given that 𝑎 and 𝑏 are positive integers with $𝑎\mid𝑏$, how would one prove that $\gcd(𝑎, 𝑏) = 𝑎$? [duplicate]","This question already has answers here : Prove that if $a \mid b$ then $\gcd(a,b) = |a|$ (5 answers) Closed 3 years ago . I'm having trouble showing that if 𝑎 and 𝑏 are positive integers with 𝑎|𝑏, then 𝑔𝑐𝑑(𝑎, 𝑏) = a. I'm aware that the greatest common divisor of two numbers is the largest number that is a divisor of both numbers. Ex) $\gcd(4,2) = 2 $ . But in this case $a=4$ and $b=2$ so the expression above would not work. I suppose what I'm asking is if variable $a$ is always the smallest of the two numbers given. Note I now realize that I should have given the problem some more thought before posting on here. I had been overlooking the 𝑎|𝑏 portion of the expression. Thank you for the help!","['euclidean-algorithm', 'gcd-and-lcm', 'discrete-mathematics']"
4063241,Where can I find the proof of this theorem about the smoothness of the initial data?,"What to follow is a reference from the text Differential forms by Victor Guillemin and Peter Haine So clearly the theorems $2.2.4$ and $2.2.5$ follow directely by the Cauchy theorem for ordinary differential equation provided that the vector field if locally lipschitz or of class $C^r$ for $r\ge 1$ since any function of calss $C^1$ is locally lipschitz. However how prove the last theorem? In particular I know that if the vector field $\vec v$ is of class $C^r$ for $r\ge 1$ then the solution of the correspondent system of differential equations is of class $C^r$ too but unfortunately this did not help me to prove the theorem. So could someone idicates where I can find the proof of the last theorem, please? I point out I did NOT study lebesgue integration and measure theory so that I ask courteously to not use them, thanks. MY PROOF ATTEMPT So by the regularity theorem we know that the function $\gamma_p$ is of class $C^\infty$ so that if $A_p$ is an open neighborhood of any $p\in V$ contained in $V$ then the set $J:=\gamma^{-1}_p[A_p]$ is open an open interval containing $a\in I$ and so if $\pi$ is the projection of $V\times I$ onto $I$ the statement follows proving that $$
h(q,t)=[\gamma_p\circ\pi](q,t)
$$ for any $(q,t)\in A_p\times J$ because the set $A_p\times J$ is open and the function $\gamma_p\circ\pi$ is of class $C^\infty$ being a composition of such functions. So how prove the last equality? Is effectively it hold?","['integration', 'ordinary-differential-equations', 'real-analysis', 'calculus', 'derivatives']"
4063294,"Does Lyapunov theorem extend to dynamical system that have ""equilibrium sets""?","Suppose that I have some dynamical system $$\dot x = f(x)$$ $f$ is locally Lipschitz, etc. I know that $f(x) = 0$ whenever $x \in \Gamma$ , where $\Gamma$ is some closed (and possibly bounded) set in $\mathbb{R}^n$ , could be a small disc, a line, a plane. In other words, not just an equilibrium. How can dynamical system theory deal with this case? Normally the approach is through Lyapunov theorem, but it is usually with respect to a single point. Is it possible to extend the Lyapunov theorem by defining a Lyapunov function $V$ such that it is $V(\Gamma) = 0$ and $V(x) > 0$ elsewhere and proceed as usual by showing $\dot V(\Gamma) = 0$ and $\dot V(x) < 0$ elsewhere? Is this possible? Are there any catch for using the above? I have went through several books but could not find this extension.","['ordinary-differential-equations', 'lyapunov-functions', 'control-theory', 'reference-request', 'dynamical-systems']"
4063337,Evaluating $\int_0^1 (1-x^2)^n dx$ [duplicate],"This question already has answers here : Find the value of a given integral sequence: $I_n=\int_{0}^{1}(1-x^2)^ndx$ [duplicate] (3 answers) Closed 3 years ago . In an exercise I'm asked the following: a) Find a formula for $\int (1-x^2)^n dx$ , for any $n \in \mathbb N$ . b) Prove that, for all $n \in \mathbb N$ : $$\int_0^1(1-x^2)^n dx = \frac{2^{2n}(n!)^2}{(2n + 1)!}$$ I used the binomial theorem in $a$ and got: $$\int (1-x^2)^n dx = \sum_{k=0}^n \left( \begin{matrix} n \\ k \end{matrix} \right) (-1)^k \ \frac{x^{2k + 1}}{2k+1} \ \ \ + \ \ C$$ and so in part (b) i got: $$\int_0^1 (1-x^2)^n dx = \sum_{k=0}^n \left( \begin{matrix} n \\ k \end{matrix} \right) \ \frac{(-1)^k}{2k+1}$$ I have no clue on how to arrive at the expression that I'm supposed to arrive. How can I solve this?","['integration', 'definite-integrals', 'real-analysis']"
4063354,Conditions for monotonic convergence in the CLT?,"(Revised to assume $\text{support}(X_i)=\mathbb{N}$ in response to counterexamples noted in comments.) Let $S_n=X_1+...+X_n$ , where the $X_i$ are i.i.d. with $\text{support}(X_i)=\mathbb{N}$ and $0<VX_i<\infty.$ Then we know by the CLT that $$\lim_{n\to\infty}P({S_n<ES_n})={1\over 2}.$$ Q1 : What are necessary and/or sufficient conditions for the convergence of $P({S_n<ES_n})$ to be monotonic ? For the $X_i$ -distributions that I've examined numerically (e.g., Negative Binomial and Poisson), computations suggest that $EX_i\in\mathbb{N}$ is neccesary & sufficient -- but I've been unable to prove it. For example, if $X_i\sim\text{Geometric($p$)}$ , then $EX_i\in\mathbb{N}$ iff $1/p\in\mathbb{N}$ , and indeed the convergence appears monotonic iff $p$ is the reciprocal of a positive integer. Similarly, if $X_i\sim\text{Poisson($\lambda$)}$ , then $EX_i\in\mathbb{N}$ iff $\lambda\in\mathbb{N},$ and indeed the convergence appears monotonic iff $\lambda$ is a positive integer. Here are some typical plots of $P({S_n<ES_n})$ v. $n$ : $X_i\sim\text{Geometric($p=1/3$)},\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \text{Geometric($p=1/3.1$)}$ $X_i\sim\text{Poisson($\lambda=1$)},\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \text{Poisson($\lambda=1.1$)}$ Here's my attempt to prove (strict) monotonicity when $X_i\sim\text{Geometric($p=1/3$)}$ ; i.e., $S_n\sim\text{NegativeBinomial($n,p=1/3$)},$ for which $ES_n=nq/p=2n$ , with $q=1-p.$ The pmf of $S_n$ is the following: $$P(S_n=k) = \binom{k+n-1}{n-1} p^nq^k\quad(k=0,1,2,...),$$ and we want to show that for all $n$ , $$\begin{align}P(S_{n+1}<ES_{n+1}) &< P(S_{n}<ES_{n}) \\[2ex]
\sum_{k=0}^{2(n+1)-1}\binom{k+n}{k}p^{n+1}q^k &< \sum_{k=0}^{2n-1}\binom{k+n-1}{k}p^{n}q^k\\[2ex]
\sum_{k=0}^{2n+1}\binom{k+n}{k}q^k &< \sum_{k=0}^{2n-1}\binom{k+n}{k}{3n\over k+n}q^k\\[2ex]
\sum_{k=2n}^{2n+1}\binom{k+n}{k}q^k &< \sum_{k=0}^{2n-1}\binom{k+n}{k}\left({3n\over k+n}-1\right)q^k\\[2ex]
\binom{3n}{2n}q^{2n}+ \binom{3n+1}{2n+1}q^{2n+1}&< \sum_{k=0}^{2n-1}\binom{k+n}{k}\left({3n\over k+n}-1\right)q^k\\[2ex]
{12n+5\over 12n+3}\binom{3n}{2n}\left({2\over 3}\right)^{2n}&< \sum_{k=0}^{2n}\binom{k+n}{k}\left({2n-k\over k+n}\right)\left({2\over 3}\right)^k\tag{*}\\[2ex]
...\ & ?\ ...
\end{align}$$ Q2 : Any suggestions on how to proceed, or alternative approaches? (I tried using the cdf in the form of a regularized beta function, but it led to similar difficulty.) Anyone know of binomial identities/inequalities that will further simplify (*)?","['probability-limit-theorems', 'central-limit-theorem', 'probability-distributions', 'reference-request', 'probability-theory']"
4063369,Proving that given a triangle $ABC$ the line $AB$ is parallel to $CK$,"Supposing a triangle $ABC$ , where $|AC|\neq |BC|$ , denote the incentre $I$ and the points of tangency between the inscribed circle and $BC$ , $CA$ , $AB$ to be $D,E,F$ respectively. $M$ is the midpoint of $AB$ . $K$ is the intersection of the perpendicular to $CM$ passing through $I$ , and line $DE$ . The task is to prove that $CK||AB$ . I have realized that $DE$ is a side of the Gergonne triangle of $ABC$ , but I haven't been able to connect it with angle bisectors or anything pertaining to the incentre, nor with medians. I have also tried constructing the circumscribed circle and looking at the perpendicular bisectors but without much success. I'd really appreciate your help.","['triangles', 'circles', 'geometry']"
4063405,Show that $\sum_{n\geq 1} \frac{a_n}{n^z}$ defines a holomorphic function in $\{ \operatorname{Re}z > 1\}$,"I am trying to make the following demonstration: let $(a_n)_n\subset \mathbb{C}$ be a sequence fulfilling that for all $\delta>0$ , $$\sup_n \dfrac{|a_n|}{n^\delta}<+\infty. $$ Show that the series $\displaystyle \sum_{n\geq 1} \dfrac{a_n}{n^z}$ , $\textrm{Re}\, z>1 $ , defines a holomorphic function on the half-plane $\textrm{Re}\, z> 1 $ . What I have done is: I first show that $\displaystyle \sum_{n\geq 1} \dfrac{a_n}{n^z}$ converges uniformly in every compact rectangle $R=[a, b] \times [c, d] \subset \mathbb {C} $ with $ a> 1 $ . For this I will see that the sequence of partial sums is a uniformly Cauchy sequence. Let $ \beta> \alpha $ , and $ z = x + iy \in R $ with $ x> 1 $ \begin{align*}
		    \left| \sum_{n=1}^\beta \dfrac{a_n}{n^z}-\sum_{n=1}^\alpha \dfrac{a_n}{n^z}\right| &=\left| \sum_{n=\alpha +1}^\beta \dfrac{a_n}{n^z}\right|\leq \sum_{n=\alpha+1}^\beta \dfrac{|a_n|}{|n^z|}=\sum_{n=\alpha+1}^\beta \dfrac{|a_n|}{n^x} \\
		    & \leq \sum_{n=\alpha+1}^\beta \dfrac{|a_n|}{n^a} \leq \sum_{n=\alpha+1}^\beta M=M(\beta -\alpha)
		\end{align*} where $M=\displaystyle \sup_{n\in \{ \alpha+1,\ldots, \beta\}} \dfrac{|a_n|}{n^a}$ . If we see that there exists an integer $m$ such that if $m$ exists, then $m\to 0$ then we will have that the succession of partial sums is Cauchy uniform. This is true because as we increase $n$ , $n^a$ will increase. Therefore $\displaystyle \sum_{n \geq 1} \dfrac{a_n}{n^z}$ converges uniformly on every compact subset in the interior of the half-plane of convergence $\textrm{Re} \, z> 1 $ . Then, $ f (z) = \displaystyle \sum_ {n \geq 1} \dfrac{a_n}{n ^ z} $ is holomorphic in the half-plane $ \textrm {Re} \, z> 1 $ and the sequence of derivatives $ (f'_n) _n $ where $ f_n (z) = \dfrac {a_n} {n ^ z} $ converges uniformly on each compact subset of the half-plane of convergence to the derivative $ f '$ obtained in deriving term by term . I don't know if it is well resolved. What I don't know how to justify very well is the step that $M$ goes to $0$ for a sufficiently large $N$ .","['complex-analysis', 'dirichlet-series', 'sequences-and-series']"
4063414,Factorization $x^4+px^3+qx^2+r x +s=(x^2+a x +b)(x^2+\bar a x +\bar b)$,"Question: Under what condition, does the quartic polynomial with rational coefficients $p$ , $q$ , $r$ and $s$ factorizes as $$x^4+px^3+qx^2+r x +s= (x^2+a x +b)(x^2+\bar a x +\bar b) $$ with $a$ , $b$ complex numbers, along with their conjugates $\bar a $ , $\bar b$ . Examples: $$x^4+2x^3+6x^2+2x+1=( x^2 +(1-i \sqrt3)x +1) (x^2 +(1+i \sqrt3)x +1) $$ $$x^4+2x^3+4x^2+2=( x^2 +(1+i)x +(1-i)) (x^2 +(1-i)x +(1+i))  $$ Note that the symmetry of coefficients leads to such factorization, as seen in the first example; but not exclusively so, as shown by the second example. Is there any test on the coefficients $p$ , $q$ , $r$ and $s$ that can be carried out to determine the possibility of such factorization? I reviewed here the discriminate tests on the nature of roots for quartic equations and did not find anything applicable.","['roots', 'factoring', 'polynomials', 'algebra-precalculus', 'quartics']"
4063423,Proof verification: $\sum_{n=0}^{\infty}\frac{(-1)^n}{2n+1}=\frac{\pi}{4}$,"While exploring the various applications of integral reduction formulae, I stumbled into what I believe to be a beautiful elementary proof of the equality $$\sum_{n=0}^{\infty}\frac{(-1)^n}{2n+1}=\frac{\pi}{4}$$ I'd greatly appreciate it if someone took the time to review it! To arrive at the desired result, we'll need to establish the following: Lemma : For all $n\geq 1$ , $$\int_{0}^{\frac{\pi}{4}}\tan^{2n}(x)dx=(-1)^n\left(\frac{\pi}{4}-\sum_{k=0}^{n-1}\frac{(-1)^k}{2k+1}\right)$$ We use mathematical induction to do this. The base case $n=1$ can be proven as follows: \begin{align*}
\int_{0}^{\frac{\pi}{4}}\tan^2(x)\text{ }dx &= \int_{0}^{\frac{\pi}{4}}\left[\sec^2(x)-1\right]dx\\
&= \left[\tan(x)-x\right]_{0}^{\frac{\pi}{4}}\\
&= 1-\frac{\pi}{4}\\
&= -\left(\frac{\pi}{4}-1\right)\\
&= (-1)^1\left(\frac{\pi}{4}-\sum_{k=0}^{0}\frac{(-1)^k}{2k+1}\right)\\
&= (-1)^1\left(\frac{\pi}{4}-\sum_{k=0}^{1-1}\frac{(-1)^k}{2k+1}\right)
\end{align*} Thus, the equality holds for $n=1$ . If it holds for some $m\in\mathbb{N}$ , then it follows that \begin{align*}
\int_{0}^{\frac{\pi}{4}}\tan^{2(m+1)}(x)\text{ }dx &= \int_{0}^{\frac{\pi}{4}}\tan^{2m+2}(x)\text{ }dx\\
&= \int_{0}^{\frac{\pi}{4}}\tan^{2m}(x)\tan^2(x)\text{ }dx\\
&= \int_{0}^{\frac{\pi}{4}}\tan^{2m}(x)\left[\sec^2(x)-1\right]dx\\
&= \int_{0}^{\frac{\pi}{4}}\left[\tan^{2m}(x)\sec^2(x)-\tan^{2m}(x)\right]dx\\
&= \int_{0}^{\frac{\pi}{4}}\tan^{2m}(x)\sec^2(x)\text{ }dx-(-1)^m\left(\frac{\pi}{4}-\sum_{k=0}^{m-1}\frac{(-1)^k}{2k+1}\right)
\end{align*} Here, we make the substitution $u=\tan(x)$ . This gives \begin{align*}
\int_{0}^{\frac{\pi}{4}}\tan^{2m}(x)\sec^2(x)\text{ }dx-(-1)^m\left(\frac{\pi}{4}-\sum_{k=0}^{m-1}\frac{(-1)^k}{2k+1}\right) &= \int_{0}^{1}u^{2m}\text{ }du-(-1)^m\left(\frac{\pi}{4}-\sum_{k=0}^{m-1}\frac{(-1)^k}{2k+1}\right)\\
&= \frac{1}{2m+1}-(-1)^m\left(\frac{\pi}{4}-\sum_{k=0}^{m-1}\frac{(-1)^k}{2k+1}\right)\\
&= \frac{(-1)^m(-1)^m}{2m+1}-(-1)^m\left(\frac{\pi}{4}-\sum_{k=0}^{m-1}\frac{(-1)^k}{2k+1}\right)\\
&= (-1)^m\left(\frac{(-1)^m}{2m+1}-\frac{\pi}{4}+\sum_{k=0}^{m-1}\frac{(-1)^k}{2k+1}\right)\\
&= (-1)^m\left(-\frac{\pi}{4}+\sum_{k=0}^{m}\frac{(-1)^k}{2k+1}\right)\\
&= (-1)^{m+1}\left(\frac{\pi}{4}-\sum_{k=0}^{(m+1)-1}\frac{(-1)^k}{2k+1}\right)
\end{align*} This shows that the equality is true for $n=m+1$ . Thus, mathematical induction implies that it is true for all $n\geq 1$ . With that settled, notice that \begin{align*}
\left|\frac{\pi}{4}-\sum_{k=0}^{n-1}\frac{(-1)^k}{2k+1}\right| &= \left|(-1)^n\left(\frac{\pi}{4}-\sum_{k=0}^{n-1}\frac{(-1)^k}{2k+1}\right)\right|\\
&= \left|\int_{0}^{\frac{\pi}{4}}\tan^{2n}(x)\text{ }dx\right|\\
&= \int_{0}^{\frac{\pi}{4}}\tan^{2n}(x)\text{ }dx
\end{align*} because $\tan^{2n}(x)\geq 0$ . We now obtain an upper bound for the integral $\int_{0}^{\pi/4}\tan^{2n}(x)dx$ . Making the substitution $x=\tan^{-1}(u)$ gives $$\int_{0}^{\frac{\pi}{4}}\tan^{2n}(x)\text{ }dx=\int_{0}^{1}\frac{u^{2n}}{1+u^2}du$$ It's clear that $u^{2n}/(1+u^2)< u^{2n}$ except when $u=0$ , so $$\int_{0}^{1}\frac{u^{2n}}{1+u^2}du< \int_{0}^{1}u^{2n}\text{ }du=\frac{1}{2n+1}$$ Thus, we have that $$\left|\frac{\pi}{4}-\sum_{k=0}^{n-1}\frac{(-1)^k}{2k+1}\right|<\frac{1}{2n+1}$$ and since $1/(2n+1)\to 0$ as $n\to\infty$ , it immediately follows from the squeeze theorem that $$\lim_{n\to\infty}\left(\frac{\pi}{4}-\sum_{k=0}^{n-1}\frac{(-1)^k}{2k+1}\right)=0$$ which is equivalent to $\sum_{n=0}^{\infty}\frac{(-1)^n}{2n+1}=\frac{\pi}{4}$ . $\blacksquare$ Let me know what you think! Edit : This is harmless, but should still be addressed: the inequality $u^{2n}/(1+u^2)<u^{2n}$ should have been $u^{2n}/(1+u^2)\leq u^{2n}$ , since $u^{2n}/(1+u^2)$ and $u^{2n}$ are equal if and only if $u=0$ . The inequality of integrals is still strict, though.","['solution-verification', 'sequences-and-series']"
4063432,What is the relationship between the language accepted by DFA $M_1$ and the language accepted by DFA $M$?,"Let $M = (Q, Σ, δ, q_0, F)$ be a DFA. Let $M_1 = (Q, Σ, δ, q_0, F_1)$ be an AFD identical to $M$ except for the set of final states, where $F_1$ is defined as the set of states $q ∈ Q$ for which $\hat{δ} (q, z) ∈ F$ for some $z$ . What is the relationship between the language accepted by $M_1$ and the language accepted by $M$ ? Justify your answer. (Hint: Use the fact that $\hat{δ} (q, xy) = \hat{δ} (\hat{δ}(q, x), y))$ Hi I'm answering this question, and by drawing some DFA, I think the realtionship between the language accepted by $M_1$ and the language accepted by $M$ , is that the language acceptes by $M_1$ contains all the prefix from all the strings in $M$ . But I can't seem a way to justify using the hint. Can someone help?","['formal-languages', 'automata', 'discrete-mathematics']"
4063444,Non-oriented Rubiks Cube Group,"Consider the (non-oriented, so we do not care about the center of each side) rubiks cube group, $G$ , which is a subgroup of $S_{48}$ and is generated by the letters: $\{F,B,U,D,L,R\}$ , i.e. I mean that $G = \langle F,B,U,D,R,L\rangle$ (subject to the canonical relations for the non-oriented $3\times 3$ rubiks cube). I am trying to just manually verify if the superflip is in the center (just by verifying if it commutes with each of the generators) using https://rubikscu.be/#cubesolver , but this is exceptionally cumbersome and time consuming. Let $h\in G$ denote the superflip. I have seen other websites, MSE pages, and pdfs describe the center as simply only containing $1_G$ and $h$ , but these pages very rarely distinguish if they mean the oriented cube (the subgroup of $S_{54}$ ) or non-oriented, I am not really sure if it matters either (but furthermore, I am really sure how one would rigorously prove this in any case, the arguments I can find are mostly fairly ambiguous).","['permutations', 'group-theory', 'rubiks-cube']"
4063445,Prove $\displaystyle \dfrac{n}{n + 1} > M$ whenever $n > N$ for some integer $N$,"Q: If $M$ is a positive number less than 1. Prove the terms in $\displaystyle \left\{\dfrac{n}{n + 1}\right\}_{n = 1}^{\infty}$ exceed $M$ for sufficiently large $n$ ; that is, prove $\displaystyle \dfrac{n}{n + 1} > M$ whenever $n > N$ for some integer $N$ . Not exactly sure how to prove this but here is my attempt so far. By the definition of the limit, $\displaystyle \lim_{n \to \infty} \dfrac{n}{n + 1} > M$ , we have $\forall M\in(0, 1)\exists N\in\mathbb{N}$ such that if \begin{equation*}
n > N \hspace{1cm} \rightarrow \hspace{1cm} a_n > M
\end{equation*} If $a_n = \dfrac{n}{n + 1}$ , then we want to show that it exceeds $M$ as $n$ gets larger. This is the part where I am kind of lost. I am not exactly sure how to proceed from here. Some tips or advice would be useful. Thanks","['limits', 'calculus', 'epsilon-delta', 'sequences-and-series']"
4063479,Convergence of partial sums to Fourier series?,"I am relatively new to Fourier series. Let $f \in L^2([-\pi, \pi])$ with $f(-\pi) = f(\pi)$ . The Fourier coefficients of $f$ are given by \begin{align*}
\hat{f}(n) = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) e^{-int} \, dt,
\end{align*} the partial sums of the Fourier series of $f$ are given by \begin{align*}
s_N(t) = \sum \limits_{-N}^N \hat{f}(n) e^{int},
\end{align*} and the Fourier series of $f$ is given by \begin{align*}
F(t) = \sum_{-\infty}^\infty \hat{f}(n) e^{int}.
\end{align*} My question is: do the functions $s_N$ converge pointwise to $F$ ? It seems to me that this should both be true, but then I derive a contradiction. It is known that the $s_N$ converge to $f$ in the $L^2$ norm, and so a subsequence $(s_{N_k})_k$ converges pointwise to $f$ . This implies that $f=F$ almost everywhere, but since $s_N \rightarrow F$ pointwise, we get $s_N \rightarrow f$ pointwise almost everywhere. But as far as I can tell, pointwise convergence of the Fourier series to the original function $f$ is a very difficult question. Is the ""almost everywhere"" what makes a difference? Did I make a mistake in my argument? Or do the $s_N$ just not converge pointwise to $F$ ?","['fourier-analysis', 'analysis', 'hilbert-spaces', 'lp-spaces', 'fourier-series']"
4063495,Does a bounded function need to be defined everywhere inside an interval in order to be Riemann integrable?,"If a bounded function $f$ is not defined on a finite set $A$ of numbers inside an interval $[a,b]$ , is it Riemann integrable over that interval $[a,b]$ ? I have seen that some people jump over this question by defining the function in an arbitrary way, $0$ for example, on the set of numbers where it is not defined. But this seems wrong to me, since then you are modifying the domain, and therefore integrating a different function, not the original one. My confusion increases, because it seems to me that the $f$ might be Darboux integrable, without the need of defining the function in the set $A$ . But I have read that Riemann and Darboux are equivalent. Any help would be welcome. Thanks in advance.","['integration', 'calculus', 'riemann-integration', 'real-analysis']"
4063558,Does analytic imply mixed paritials exist?,"Let $f(z,w)$ be defined on $\Bbb{C}\times\Bbb{C}$ . Assume for each fixed $z$ , $f(z,w)$ is analytic for all $w$ . Likewise, assume for each fixed $w$ , $f(z,w)$ is analytic for all $z$ . Is it the case that the mixed partial derivatives must exist? That is, WLOG, for a fixed $w_0$ , will $$\left.\frac{\partial}{\partial w}f(z,w)\right|_{w=w_0}$$ be analytic for all $z$ ? If not, what is the counterexample?","['complex-analysis', 'multivariable-calculus', 'several-complex-variables', 'partial-derivative']"
4063566,"Probability of n heads in 2n tosses, given at least one head in n tosses","Suppose we toss a fair coin $2n$ times. Conditioned on the fact that the coin came up heads at least once in the first $n$ tosses, what is the probability that we get precisely a total of $n$ heads over all $2n$ heads?","['probability-distributions', 'discrete-mathematics', 'probability']"
4063594,Are there topological invariants for this path-finding game?,"There is a game I've seen recently ( link ; note that I have no affiliation with this game) which involves finding a Hamiltonian path connecting a graph of points, as illustrated below. ...... The rules are: Your starting point is fixed (it is the highlighted dot in the first picture). You must form a path connecting all dots on the board, moving only up, down, left or right, and not hitting any dot twice. You can't pass through the black squares. Typically, the endpoint is fixed as the only dot with valency 1. Solutions to this game are not unique, but I initially conjectured that they were path homotopic (viewing the paths as embedded in $\mathbb{R}^2$ punctured at the locations of the squares).  This turns out to not be true, as the following alternative solution to the above board demonstrates: Question: Are there any topological invariants to the solutions of this game? Here are two more boards with multiple solutions for reference: ... ... ... ...","['graph-theory', 'general-topology', 'algebraic-topology', 'hamiltonian-path']"
4063617,"Prove that among any five distinct real numbers there are two, $a$ and $b$, such that $|ab+1|\gt|a-b|$. Solution without trigonometry?","Prove that among any five distinct real numbers there are two, $a$ and $b$ , such that $\lvert ab+1\lvert 
 \ \gt \ \vert a-b \vert$ . Solution: I have solved the above problem using Trigonometrical substitution by writing $-90^\circ <\tan\ x_k<90^\circ$ , $k=1,2,3,4,5$ ,  and considering the intervals $(-90^\circ,-45^\circ)$ ; $(-45^\circ,0^\circ)$ ; $(0^\circ,45^\circ)$ ; $(45^\circ,90^\circ)$ . Then by Pigeonhole Principle , at least two of $x_1,x_2,x_3,x_4,x_5$ will lie in the same interval. Let those two be $(x_i,x_j)$ as $\vert x_i-x_j\vert\lt 45^\circ$ setting $a=\tan x_i$ and $b=\tan x_j$ we get the desired inequality. $\square$ However, I was wondering whether we could approach this problem in an algebraic way because this question is in an Algebraic Book which I have. Any help would be appreciated.
Thanks!","['inequality', 'pigeonhole-principle', 'combinatorics', 'substitution', 'algebra-precalculus']"
4063649,"Showing $\int_{-1}^1\frac{m(2m-1)x^{2m-2}(1-x^{2m})+m^2x^{4m-2}}{(m^2x^{4m-2}+1-x^{2m})\sqrt{1-x^{2m}}}dx=\pi$, algebraically","Is there a nice algebraic way to solve the following geometrically motivated integral? $$\int_{-1}^1\frac{m(2m-1)x^{2m-2}(1-x^{2m})+m^2x^{4m-2}}{(m^2x^{4m-2}+1-x^{2m})\sqrt{1-x^{2m}}}dx,$$ where $m$ is a positive integer. In fact, this integral can be shown to be the integral of the curvature of the plane curve $x^{2m}+y^2=1, y\geq0$ , which is the angle rotated by the tangent vector of the curve as it traverses along the curve. So this integral is $\pi$ , but I would like to see some alternative, algebraic solutions.","['definite-integrals', 'curvature', 'differential-geometry']"
4063682,"MLE of $(\theta_1,\theta_2)$ in a piecewise PDF","I am trying to find the MLE of $\theta=(\theta_1,\theta_2)$ in a random sample $\{X\}_{i=1}^n$ with the following pdf $$f(x\mid\theta)= \begin{cases}
(\theta_1+\theta_2)^{-1}\exp\left(\frac{-x}{\theta_1}\right) &,  x>0\\ 
(\theta_1+\theta_2)^{-1}\exp\left(\frac{x}{\theta_2}\right) &,  x\le0\\
\end{cases}
$$ If I let $\bar{X}_1$ be the average of the $n_1$ values where $X_1>0$ and $\bar{X}_2$ the average of $n_2$ values where $X_i\le 0$ and $n_1+n_2=n$ Then the likelihood function is: $$L(\theta\mid  X)=\left(\frac 1 {\theta_1+\theta_2}\right)^n\exp\left(\frac{-n_1\bar{X}_1}{\theta_1}+\frac{n_2\bar{X}_2}{\theta_2}\right)$$ but I am having trouble maximizing this function.","['statistical-inference', 'statistics', 'parameter-estimation', 'maximum-likelihood']"
4063683,I am having trouble finding $f'(x)$ given $f(x) = \frac{4}{x}$?,I am having trouble finding $f'(x)$ given $f(x) = \frac{4}{x}$ . I used the definition of a derivative but I got stuck. I think the right answer is $\frac{-4}{x^2}$ but I am stuck at $\frac{-4h}{x(x+h)}$ . My work is posted down below. Thank you.,"['limits', 'derivatives']"
4063781,"Hatcher A.1 , difficulty in understanding a certain step","In hatcher (algebraic topology) proposition A.1 page 520 in the appendix section, I don't see why we have $\phi_{\alpha}^{-1}(S)$ necessarily closed in $\partial D_{\alpha}^n$ , once we have this I can see why $$\Phi_{\alpha}^{-1}(S)$$ contains at most one more point and is thus closed, but I struggle to understand the first part. Here is the proposition for reference:","['proof-explanation', 'general-topology', 'geometry', 'algebraic-topology']"
4063784,Show that $\det(AC) \ge 0$ for real matrices with $(A+iB)^{-1} = C+iD$.,"Let $H=A+Bi$ be a complex $n \times n$ invertible matrix where $A,B$ are real matrices, with inverse $H^{-1}=C+Di$ for $C,D$ real. Prove $\det(AC)\geq 0$ . My attempt so far, $$I=HH^{-1}=(A+Bi)(C+Di)=AC-BD+(AD+BC)i$$ So $$AC-BD+(AD+BC)i=I$$ Solving by $AC$ and taking the determinant in both sides, I end up with $$
\det(AC)=\det(I-(AD+BC)I+BD)$$ Since $AC-BD+(AD+BC)i$ is the identity then it is a real matrix so it must be true that $(AD+BC)=O$ ,
So finally I get $$\det(AC)=\det(I+BD).$$ But this is where I am stuck, is $\det(I+BD)$ non-negative? Or did I take a completely wrong approach?","['matrices', 'determinant', 'linear-algebra']"
4063795,"Showing that losing positions in Wythoff's game are generated by $(\lfloor n\phi\rfloor, \lfloor n\phi^2\rfloor)$, where $\phi$ is the golden ratio","A very neat problem in combinatorial game theory: In Wythoff's Game , how do I show that all the losing positions are generated by the formula $$\left(\lfloor n\phi\rfloor, \lfloor n\phi^2\rfloor\right)$$ where $n \in \mathbb{N}$ and $\phi$ is the golden ratio? More precisely, the $k^{\text{th}}$ losing position is $\left(\lfloor k\phi\rfloor, \lfloor k\phi^2\rfloor\right)$ where $k \in \mathbb{N}$ and $\phi$ is the golden ratio. I have tried this problem for a while and got a way to show that the sequences $\lfloor n\phi\rfloor$ and $\lfloor n\phi^2\rfloor$ are disjoint and their union is $\mathbb{N}$ , this can be easily showed using the Beatty's theorem, but I'm unsure how to prove that all the losing positions are generated by the formula $\left(\lfloor n\phi\rfloor, \lfloor n\phi^2\rfloor\right)$ . I have gone through wikipedia and its references for the proof, but it doesn't seem like the proof for the formula is present anywhere in the sources or references. Any help or hint would be highly appreciated. Thanks.","['pattern-recognition', 'puzzle', 'combinatorics', 'combinatorial-game-theory']"
4063800,Conjecture about a point inside an equilateral triangle divided by integer angles,"In the equilateral triangle below, $CD, AD$ , and $BD$ concur at point $D$ . The measures of the angles ( $a, b, c, d, e,$ and $f$ ) are also defined as given in the figure below. Conjecture :
There aren't any $a, b, c, d, e,$ and $f$ such that all are different integers without using the combinations of the numbers: $6, 54, 42, 18, 48$ , and $12$ . (to clarify, all six of these numbers can not be used together, but they can be used together in groups)
How can I prove this conjecture mathematically, and what is so special about these six numbers? My thoughts and work on the problem I am intentionally omitting the degree signs for convenience. Using, the Trigonometric Ceva's Theorem, I obtained the following expression: $$\frac{\sin(a)}{\sin(b)}\cdot\frac{\sin(c)}{\sin(d)}\cdot\frac{\sin(e)}{\sin(f)}= 1$$ which is equivalent to: $$\begin{aligned} \sin(a) \sin(c) \sin(e) = \sin(b) \sin(d) \sin(f)
 \end{aligned} \label{a}\tag{1}$$ We also know that, $$a+b=c+d=e+f=60$$ To convert the triple sine product to summations, I used the following ""trick"". Since, $$\sin(a+c+e)=\sin(b+d+f)$$ and also due to $Eq. (1)$ , it is true that: $$\sin(-a+c+e)+\sin(a-c+e)+\sin(a+c-e)$$ $$=\sin(-b+d+f)+\sin(b-d+f)+\sin(b+d-f)$$ Let's substitute $b=60-a, d=60-c$ , and $f=60-e$ and let $x=(c+e-a), y=(a+e-c),$ and $z=(a+c-e)$ to simplify things a bit: $$\sin(x) +\sin(y) +\sin(z)=\sin(60-x)+\sin(60-y) +\sin(60-z)$$ I expanded the $(60-x)$ 's using compound angle formulae: $$\sqrt{3}/2 (\cos(x) + \cos(y) + \cos(z)) - 3/2 (\sin(x) + \sin(y) + \sin(z)) = 0$$ which simplifies to: $$\sin(30-x) +\sin(30-y) +\sin(30-z)=0$$ but I feel lost in the algebra and don't know what to do next. I want to get to a point where I can easily plug numbers and show that no other numbers satisfy this equation, other than the combinations of the six numbers given at the beginning. The trigonometric proof of one of the cases may aid to formulate a proof, so I decided to include it as well:
For $a=6, b=54, c=42, d=18, e=48, f=12$ , $Eq. (1)$ is satisfied, so let's prove it. Proof. $$ \sin(12°) \sin(18°) \sin(54°)\stackrel{?}{=} \sin(6°)\sin(42°)\sin(48°)$$ Now we use the ""trick"": $$ \sin(60°)+ \sin(48°)- \sin(24°)\stackrel{?}{=} \sin(84°)+\sin(12°)+\sin(0°)$$ $$\sin(60°)=\frac{\sqrt{3}}{2} \stackrel{?}{=} \sin(12°)+ \sin(24°) - (\sin(48°) -\sin(84°))$$ $$ = 2 \sin(18°) \cos(6°) + 2 \cos(66°) \sin(18°)$$ $$ = 2 \sin(18°) (\cos(6°) + \cos(66°)$$ $$ = 2 \sin(18°) (2\cos(36°)\cos(30°) )$$ $$1 \stackrel{?}{=} 4 \sin(18°) \cos(36°)$$ $$\frac{1}{2} =  \cos(36°) \cos(72°)$$ which is true as it is a well-known identity. Here is the figure of this triangle (rotated): Given only 6 and 12 degrees (the red angles) the rest of the angles can be found. For the sake of brevity, I will omit the synthetic proofs. For the interested reader, see this link , as it also shows the beauty of synthetic solutions in these types of problems from the perspective of a geometrician. The combinations of $6, 54, 42, 18, 48$ , and $12$ often come up in synthetic geometry problems: I even managed to find one in SE . @KorayUlusan helped me develop a code to test the conjecture by brute force in Python. Turns out that the conjecture is correct, but I'm looking for a mathematical explanation. Here is the code: import math
    
epsilon = 2.22044604925e-14  # for integer overflow
    
for a in range(60):
    b = 60 - a  # because a+b=60
    for c in range(60):
        d = 60 - c
        for e in range(60):
            f = 60 - e
            if a != b and b != c and c != d and d != e and e != f:  
                try:
                    rightHandSide = (
                        math.sin(math.radians(a))
                        * math.sin(math.radians(c))
                        * math.sin(math.radians(e))
                    ) / (
                        math.sin(math.radians(b))
                        * math.sin(math.radians(d))
                        * math.sin(math.radians(f))
                    )
    
                    if abs(1 - rightHandSide) < epsilon:
                        print(f""a is {a}, b is {b}, c is {c}, d is {d}, e is {e}, f is {f}"")
    
                except ZeroDivisionError:
                    pass The Python output for the code (of course, most of these results are the same triangles, just rotated): a is 6,  b is 54, c is 42, d is 18, e is 48, f is 12
a is 6,  b is 54, c is 48, d is 12, e is 42, f is 18
a is 12, b is 48, c is 18, d is 42, e is 54, f is 6 
a is 12, b is 48, c is 54, d is 6,  e is 18, f is 42
a is 18, b is 42, c is 12, d is 48, e is 54, f is 6 
a is 18, b is 42, c is 54, d is 6,  e is 12, f is 48
a is 42, b is 18, c is 6,  d is 54, e is 48, f is 12
a is 42, b is 18, c is 48, d is 12, e is 6,  f is 54
a is 48, b is 12, c is 6,  d is 54, e is 42, f is 18
a is 48, b is 12, c is 42, d is 18, e is 6,  f is 54
a is 54, b is 6,  c is 12, d is 48, e is 18, f is 42
a is 54, b is 6,  c is 18, d is 42, e is 12, f is 48 Thanks in advance for any sort of help or contribution.","['euclidean-geometry', 'number-theory', 'geometry', 'plane-geometry', 'trigonometry']"
4063847,Find all cosets of $\begin{pmatrix} a & b \\ 0 & c \end{pmatrix}$ in $\operatorname{GL}_2(\mathbb{R})$,"I am trying to tackle this question: Find all left and right cosets of $$ H = \begin{pmatrix} a & b \\ 0 & c \end{pmatrix} $$ in $G = \operatorname{GL}_2(\mathbb{R})$ , where $a, c \in \mathbb{R}^*$ and $b \in \mathbb{R}$ . Supposedly, the solution is to take $$ g_1 = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \in G $$ and $$ h_1 = \begin{pmatrix} 1 & -\frac{b}{a} \\ 0 & 1 \end{pmatrix} \in H $$ to get $$ g_2 = g_1 h_1 = \begin{pmatrix} a & 0 \\ c & \frac{ad-bc}{a} \end{pmatrix}, $$ and then take $$ h_2 = \begin{pmatrix} \frac{1}{a} & 0 \\ 0 & \frac{a}{ad-bc} \end{pmatrix} \in H $$ to yield $$ g_3 = g_2 h_2 = \begin{pmatrix} 1 & 0 \\ \frac{c}{a} & 1 \end{pmatrix} . $$ Apparently $g_3$ for all $c \in \mathbb{R}$ is a coset. It's also claimed $$ \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} $$ is the only other coset. Two questions: How are you supposed to go about choosing matrices like $g_1$ , $h_1$ , and $h_2$ ? It almost seems like they were pulled out of thin air. How would you approach a question like this (still in $\operatorname{GL}_2(\mathbb{R})$ ) that has a lot of cosets, e.g. 4 or more, without being bogged down by the sheer number of possibilities to compute? How do we know $g_3$ is a complete coset and can't be ""broken down"" further?","['group-theory', 'linear-algebra']"
4063908,Canonical morphism of dualizing sheaves under normalization.,"The following appears in a paper by Hwang and Oguiso. Let $V'$ be a complex variety with the property that
its normalization is smooth and the dualizing sheaf $ω_{V'}$ is invertible. Denoting
by $ν: V \to V'$ the normalization map, we have a natural injective sheaf map $ω_V \to ν^* ω_{V'}$ . My question: How is the sheaf map $\omega_V \to \nu^* \omega_{V'}$ defined? I think this may be related to Hartshorne, Exercise III 7.2 Let $f: X \to Y$ be a finite morphism of projective schemes of the same dimension over a field $k$ , and let $\omega_X$ be a dualizing sheaf for $Y$ . Show that $f^! \omega_Y$ is a dualizing sheaf for $X$ . If $X$ and $Y$ are both nonsingular, and $k$ algebraically closed, conclude that there is a natural trace map $t: f_* \omega_X \to \omega_Y$ . The trace map from 2. actually exists for the dualizing sheaves, even if $Y$ is not smooth, so I do get a morphism $$ f_* \omega_{V} \to \omega_{V'},$$ but taking the pull-back yields $f^* f_* \omega_V \to f^* \omega_{V'}$ , which is not the desired result.","['complex-geometry', 'algebraic-geometry', 'singularity-theory', 'sheaf-theory']"
4064013,Structures that are reciprocally homomorphic images but are not isomorphic,"Consider two algebras $\mathscr{A}=\langle A,O_i\rangle$ and $\mathscr{B}=\langle B,P_i\rangle$ with the same type. Suppose that there is a homomorphism from $A$ onto $B$ , and a homomorphism from $B$ onto $A$ . Clearly (edit: not really ), $|A|=|B|$ . It is easy to prove that if the algebras are finite, they must be isomorphic, but can we conclude the same thing for infinite algebras? I can find a counterexample for the same question about relational structures (sets equipped with relations) but my counterexample uses a one-many relation (the structures are $\big\langle\mathbb{N},\{\langle1,1\rangle,\langle1,2\rangle\}\big\rangle$ and $\big\langle\mathbb{N},\{\langle1,1\rangle\}\big\rangle$ ).","['universal-algebra', 'model-theory', 'abstract-algebra']"
4064080,Proving a series of matrices to be positive,"Assume that $A \geq 0$ , $(I-A)$ is invertible. I have shown that if we have a sequence $$ S_n = I + A + A^2 + \cdots $$ that $$(I-A)S_n = I - S^{n+1} = I$$ as $n \rightarrow \infty$ . How can I now show that $S_{n \rightarrow \infty} \geq 0$ ? ""Positive"" here means that all entries of the matrix are $\geq 0$ .","['matrices', 'matrix-calculus', 'positive-matrices', 'sequences-and-series']"
4064085,Generalization of Bézout's theorem,"Let $$p_1(x_1,x_2,\dots,x_n),p_2(x_1,x_2,\dots,x_n),\dots,p_n(x_1,x_2,\dots,x_n)$$ be polynomials with $n$ variables. I would like to to find out if $$p_1(x_1,x_2,\dots,x_n)=0\\
p_2(x_1,x_2,\dots,x_n)=0\\
\vdots\\
p_n(x_1,x_2,\dots,x_n)=0
$$ have finite number of solutions? Is there something like  Bézout's theorem for this problem? I know that if $n=2$ , it means that I have only $2$ polynomials of $2$ variables $p_1(x_1,x_2)$ and $p_2(x_1,x_2)$ which are coprime and homogeneous, where $deg(p_1)=m$ and $deg(p_2)=n$ then number of solutions of $$p_1(x_1,x_2)=0\\
p_2(x_1,x_2)=0\\
$$ is at least $m\cdot n$ . Are there something general which can be use for my problem? It is enough to say that if polynomials $$p_1(x_1,x_2,\dots,x_n),p_2(x_1,x_2,\dots,x_n),\dots,p_n(x_1,x_2,\dots,x_n)$$ are coprime and homogeneous, that number of solutions of equations $$p_1(x_1,x_2,\dots,x_n)=0\\
p_2(x_1,x_2,\dots,x_n)=0\\
\vdots\\
p_n(x_1,x_2,\dots,x_n)=0
$$ is finite. Any help will be appreciated. Thank you very much.","['algebraic-curves', 'algebraic-geometry', 'geometry']"
4064108,How would you go about learning the combination of coins the man has?,"I am trying to identify the branch of math that would help to solve the following problem: A man has picked $10$ coins out of a bag and has laid them in a row. You cannot see them for yourself, and the heads/tails ratio is unknown. Your goal is to identify the correct heads/tails sequence of the coins by repeatedly picking $10$ coins of your own from the bag, with the man telling you how many match the position of his each time. You cannot decide whether the coins are heads or tails (for the sake of the problem assume they are the same on both sides) and your sequence depends on the order you remove them from the bag. Note that the man only selects his coins once and his never change. For example, if the man has $HTTHHHHTTT$ and you propose $TTTHTHTHHT$ he will let you know that $5$ are a match, however he will not tell you which ones. If on your second try you pull out $THHTTTHTHT$ he will inform you that $3$ match. Without trying every possible combination $(2^{10})$ , how would you go about learning the combination of coins the man has? Comparing the two sample results I see that they share $3$ in common, but I'm not sure if or how that is useful information since I don't know which ones actually match. I suspect it might require statistics/combinatorics, and I was told it could be solved with ""either high school or college math"". This isn't a homework problem so it's possible I'm just being trolled by the person. Any information you can provide for approaching this type of problem is much appreciated. Thank you.","['statistics', 'puzzle', 'combinatorics', 'discrete-mathematics', 'probability']"
4064124,a DET with one of its corner sub-triangles missing can tiled with trapezoids made of 3 triangles,"In this post, I've described what is DET . The following question is also about DET and induction Show that a DET with one of its corner sub-triangles removed can be tiles with trapezoids built out of three equilateral triangles. My proof: Let induction hypothesis $P(n)$ be ""a triangle cut into $4^n$ equilateral triangles and one corner missing can tiled with trapezoids made up of 3 standard equilateral triangles"" Base case $(n=1)$ : A triangle made of $4$ triangles with one corner missing is exactly a trapezoid made of $3$ triangles. Inductive step: Assume that a tiling exists for a triangle of size $4^n$ . Show that a tiling exists for a triangle of size $4^{n+1}$ . For a triangle with $4^{n+1}$ equilateral triangles, consider it to be made of $4$ larger triangles, each $4$ times the area. I want to continue like this: Take the $3$ adjacent corner sub-triangles from each of $3$ triangles of size $4^n$ . You can replace these $3$ sub-triangles with $1$ trapezoid, clearly, and, by induction hypothesis, tile these $3$ triangles with trapezoids consisting of 3 sub-triangles. There left $1$ triangle of size $4^n$ . Since one of corner sub-triangles must be removed, by induction hypothesis, this can be done. We're done. a triangle of size $4^{n+1}$ with one of its corner sub-triangles removed is tiled with trapezoids made of $3$ triangles. We can conclude by induction principle that $\forall n \geq 1: P(n)$ holds. $\blacksquare$ Is this part Ok? I think it can expressed better than this, but can't find a way to do it. I appreciate your answers. NOTES: all triangles are equilateral triangles and trapezoids are made of 3 standard (standard means 1) equilateral triangles.","['induction', 'solution-verification', 'discrete-mathematics']"
4064184,Showing $\sum_{k=0}^{\infty} \sum_{i+j=k}a_i b_j = \sum_{k=0}^{\infty} a_k \sum_{k=0}^{\infty} b_k$,"I want to show $\sum_{k=0}^{\infty} \sum_{i+j=k}a_i b_j = \sum_{k=0}^{\infty} a_k \sum_{k=0}^{\infty} b_k$ rigoursly. How I can prove rigorously? (Of course suppose $\sum_{k=0}^{\infty}a_k < \infty$ , $\sum_{k=0}^{\infty}b_k < \infty$ condition is required)","['power-series', 'sequences-and-series']"
4064206,Mumford's regularity theorem,"I am reading the section on Castelnuovo-Mumford regularity from Lazarsfeld's Positivity in algebraic geometry. Theorem 1.8.3 in the book reads as follows: Let $F$ be an $m$ -regular sheaf on $P^n$ . Then for every $k \geq 0$ (i). $F(m+k)$ is generated by its global sections. (ii). The natural maps $$H^0(P^n, F(m))\otimes H^0(P^n,O_{P^n}(k))\rightarrow H^0(P^n,F(m+k))$$ are surjective. (iii). $F$ is $(m + k)$ -regular. The proof claims (i) is a consequence of (ii): it says that, since for $l\gg 0$ , $F(m+l)$ is globally generated, the surjectivities in (ii) imply that $F(m)$ itself must be globally generated. I am not able to understand this statement. Choose $l\gg 0$ such that $F(m+l)$ itself is globally generated. Now consider the morphism $$H^0(F(m))\otimes O_{P^n}\rightarrow F(m)\,.$$ We need to prove that this is surjective. Tensoring by $O(l)$ , we get $$H^0(F(m))\otimes O_{P^n}(l)\rightarrow F(m+l)\,.$$ A priori we do not know the injectiveness or surjectiveness of the above morphism. But if we take global sections, by (ii), the global sections morphism is surjective. Does this mean $H^0(F(m))\otimes O_{P^n}(l)\rightarrow F(m+l)$ is surjective? If so, I can tensor back by $O(-l)$ and get my required result. Thanks in advance.",['algebraic-geometry']
4064210,Problem in Double Integral by Change of Order,"$$\text{Evaluate by changing the order} \int^1_0\int^y_{4y}e^{x^{2}}dx\ dy.$$ I am unable to solve the following question. I have tried using the following approach. First I formed the equation of lines using the limits of the inner integral which gave equations of $2$ lines. $$ y = x $$ $$ y = x/4 $$ and the limits of the outer integral gave me the total bounded region as below From what I understood, I have to calculate the volume of function $e^{x^2}$ within the bounded area.
Since, from the current order, calculating the integral is difficult, I tried to change the order of integral (as given in question) so this is what I did : First I thought the current integral divides the current region into small $dA$ and first integrating by $dx$ means that we are taking a strip parallel to $X$ -axis with length between $y = x$ and $y = x/4$ and then integrated it all the way above from 0 to 1. Then I tried to change this order and thought of integrating first w.r.t. $dy$ as this will mean I will divide the region into strips parallel to $Y$ -axis but the equation will be divided into two with the inner limits of first being $x/4$ to $x$ and second from $x/4$ to $1$ . and the outer limit will change to 0 to 1 for first and 1 to 4 for second. I was solving the integral but then I encountered a problem. I am unable to integrate the highlighted term any further. Please tell me where I went wrong. Note: I forgot the $x$ in the first term, it will be $\frac{3xe^{x^2}}{4}$","['multivariable-calculus', 'multiple-integral', 'definite-integrals']"
4064246,Two notions of vector distributions and differential operators in $\mathbb{R}^3$,"Two possible ways of defining vector/vector valued distributions in $\mathbb{R}^3$ are: $$ X := [\mathcal{D}(\mathbb{R}^3; (\mathbb{R}^3)^{*})]^{*} = \{ T: \mathcal{D}(\mathbb{R}^3; (\mathbb{R}^3)^{*}) \simeq \mathcal{D}(\mathbb{R}^3; \mathbb{R}^3) \rightarrow \mathbb{R} \}, $$ and $$ Y := \mathcal{D}^{*}(\mathbb{R}^3;\mathbb{R}^3) = \{ T : \mathcal{D}(\mathbb{R}^3) \rightarrow \mathbb{R}^3 \} \simeq \mathcal{D}'(\mathbb{R}^3;\mathbb{R})^3 .  $$ Note that $\mathbf{u} \in L^1_{loc}(\mathbb{R}^3)^3$ induces the distribution $\langle T_{\mathbf{u}}, \phi \rangle = \int_{\mathbb{R}^3} \mathbf{u} \cdot \phi$ , where in the first case the test function is vector valued while in the second case it is scalar valued. As explained here , these two space are ""equal"" (i.e., linearly homeomorphic) in this framework. In general, that is if the ambient space is some infinite dimensional Banach space $U$ , the $Y$ definition is to be preferred. Now if $T$ is a vector valued distribution, one usually defines its curl via duality: $$ \langle \operatorname{curl} T, \phi \rangle :=  \langle T, \operatorname{curl} \phi \rangle, \qquad \phi \in \mathcal{D}(\mathbb{R}^3; \mathbb{R}^3); $$ since $\operatorname{curl} \phi \in \mathcal{D}(\mathbb{R}^3; \mathbb{R}^3)$ and $T$ acts on it, a posteriori it follows that $T$ should be in $X$ and $\operatorname{curl} T \in X$ too (assume continuity holds)! If instead $T = (T_1, T_2, T_3) \in Y$ , I would write $$ \langle \underbrace{\operatorname{curl} T}_{\in Y}, \phi \rangle := (\langle \partial_3 T_2 - \partial_2 T_3, \phi \rangle, \langle \dots, \phi \rangle, \langle \dots, \phi \rangle) \qquad \phi \in \mathcal{D}(\mathbb{R}^3; \mathbb{R}).  $$ For what concerns the gradient of a distribution $F \in \mathcal{D}'(\mathbb{R}^3; \mathbb{R})$ we have the same issue: we can either define it by duality with the divergence, resulting in $\nabla F \in X$ , or as the triplet of distributions $(\partial_{x_1} F, \partial_{x_2} F, \partial_{x_3} F) \in Y$ . All considered, is there a more standard or desirable choice among these definitions? For some reason, my taste would prefer the $X$ definition (with corresponding operators), perhaps because of the ""perfect dualities"", but this doesn't sound like a robust motivation. EDIT: There is a good chance that part of the answer somehow lies in differential geometry, which I am not quite good at. What properties we would like these diffential operators to have? Almost certainly, the validity of a Poincaré lemma, namely that if $T \in \mathcal{D}'$ and $\operatorname{curl} T = 0$ , then $T = \nabla F$ for some distribution $F$ (and similar for div and grad). I know that there is such an abstract result for currents , but I am not able to put all pieces together with the correct definitions and identifications (for instance, in the standard $\mathbb{R}^3$ setting, even if in principle grad, curl and div are differential operators acting on $0,1,2$ -forms resp., we usually identify the divergence with the scalar function it is represented by etc...)","['topological-vector-spaces', 'distribution-theory', 'functional-analysis', 'differential-geometry']"
4064256,Prove : $\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b}\geq\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2(a+b+c)}{(a+b)(b+c)(c+a)}}$,"It's an inequality based on two found on the website MSE (see the reference): Let $a,b,c>0$ then we have: $$\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b}\geq\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2(a+b+c)}{(a+b)(b+c)(c+a)}}$$ Lemma 1 : $a,b,c>0$ then we have : $$\sum_{cyc}\frac{a}{b+c}\geq P(a,b,c)=\sqrt{\frac{9}{4}+\frac{9}{4}\frac{(c^4+a^4+b^4-c^2a^2-b^2a^2-c^2b^2)}{((a+b+c)\frac{3}{4}+\frac{3}{4}(abc)^{\frac{1}{3}})(a+b)(b+c)(c+a)}+\frac{(c^2+a^2+b^2-ca-ba-cb)(a+b+c)}{(a+b)(b+c)(c+a)}}$$ Proof of lemma 1 : First we remark that the inequality is homogenous and we can try the substitution $3u=a+b+c$ , $3v^2=ab+bc+ca$ and $w^3=abc$ and apply the uvw's method . We have : $$a^4+b^4+c^4=(9u^2-6v^2)^2-2(9v^4-6uw^3)$$ $$a^2b^2+b^2c^2+c^2a^2=9v^4-6uw^3$$ $$a^2+b^2+c^2=9u^2-6v^2$$ And : $\left(\frac{((3u)((3u)^2-4(3v^2))+5w^3)}{3u3v^2-w^3}+2\right)^2\geq \frac{9}{4}+\frac{(9/4)((9u^2-6v^2)^2-2(9v^4-6uw^3)-(9v^4-6uw^3))+(2.25u+0.75w)(9u^2-9v^2)(3u)}{(2.25u+0.75w)(3u3v^2-w^3)}$ it's enough to find an extreme value of our expression for the extreme value of $w^3$ wich happens for an equality case of two variables . Since the last inequality is homogeneous, we can assume that $b=c=1$ . $$\frac{2}{a+1}+\frac{a}{2}\geq\sqrt{\frac{9}{4}+\frac{9}{4}\frac{(a^4+1-2a^2)}{((a+2)\frac{3}{4}+\frac{3}{4}(a)^{\frac{1}{3}})(a+1)^2(2)}+\frac{(a^2+1-2a)(a+2)}{(a+1)^2(2)}}$$ Now it seems to be clear : we get a polynomial  with a root equal to one . See the factorization by Wolfram alpha . End of the proof of the lemma 1 Remains to show that $ a\geq b \geq c>0$ : $$P(a,b,c)\geq\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2(a+b+c)}{(a+b)(b+c)(c+a)}}$$ Wich is not hard I think . Question : How to show it ? Reference : M. A. Rozenberg, “uvw–Method in Proving Inequalities”, Math. Ed., 2011, no. 3-4(59-60), 6–14 If $x,y,z>0$, prove that: $\frac{x}{y+z}+\frac{y}{x+z}+\frac{z}{x+y}\ge \sqrt{2}\sqrt{2-\frac{7xyz}{(x+y)(y+z)(x+z)}}$ Stronger than Nesbitt inequality","['multivariable-calculus', 'inequality']"
4064306,Using quotient to show that a map $\mathbb{RP}^2\rightarrow\mathbb{R}^3$ is an immersion at all but six points,"We want to show that $\phi:\mathbb{RP}^2\rightarrow \mathbb{R}^3$ given by $\phi([x,y,z])=\frac{(yz,xz,xy)}{x^2+y^2+z^2}$ is an immersion at all but 6 points. Recall that a map $f$ is an immersion at $x$ if the matrix $Df|_x$ is injective. We know that $\mathbb{RP}^2$ can be given by the quotient map $q:\mathbb{R}^3-\{0\}\rightarrow\mathbb{RP}^2$ by identifying one dimensional subspaces of $\mathbb{R}^3$ . I believe that we have a natural map $(\phi\circ q)(x,y,z)=\frac{(yz,xz,xy)}{x^2+y^2+z^2}$ . I think that the points where $\phi\circ q$ fails to be an immersion correspond to the points where $\phi$ fails to be an immersion. Then I wrote down the Jacobian of $D(\phi\circ q)|_{(x,y,z)}$ in hopes of obtaining $6$ one-dimensional subspaces where the matrix was non-injective, that is, has zero determinant. However, setting the determinant to be zero, I obtained the equation $xyz(-x^2+y^2+z^2)(x^2-y^2+z^2)(x^2+y^2-z^2)=0$ , which in fact gives us three two-dimensional subspaces and $3$ cone-shapes in $\mathbb{R}^3$ . This does not seem to correspond to $6$ points in $\mathbb{RP}^2$ . What am I misunderstanding here? This was a ""simple"" question in a qualifying topology/geometry exam, so I do not believe the solution is too involved.","['quotient-spaces', 'differential-topology', 'jacobian', 'differential-geometry']"
4064336,How to solve this limit: $\lim_{x\to 0}\frac{\sin(x\sin(2x))}{x^2}$,How to solve this limit? $$\lim_{x\to 0} \frac{\sin(x\sin(2x))}{x^2}$$ I tried to solve with trigonometric formulas but no result yet. It seems that need to use this $$ \lim_{x\to 0} \frac{\sin(x)}{x} $$,"['limits', 'calculus']"
4064339,"If an exact form vanishes on a submanifold, can I find a primitive that also does?","Let $M$ be a closed smooth manifold and $Q\subset M$ a closed embedded submanifold.
Furthermore, let $\omega$ be an exact differential form $\omega\in\Omega^k(M)$ and vanishing identically on $Q$ (i.e. $\omega_q=0$ for any $q\in\ Q$ ). Can we always find a primitive $\alpha\in\Omega^{k-1}(M)$ (i.e. such that $d\alpha=\omega$ ), whose restriction to $Q$ also vanishes? This question came up during class and maybe it is obvious but I can't even seem to convince myself whether it is true or not, so any help is greatly appreciated. Edit: counterexamples are given in the comments for $k=\dim Q+1$ and for the case of $\omega$ being $1$ -form with $Q$ disconnected. In the context of the class, we were specifically considering $\omega$ to be a $2$ -form, but I am also interested in the general case. Edit2: There is an answer dealing with the condition of the pullback of $\omega$ to $Q$ being $0$ . However, I meant that $\omega$ itself vanishes identically in points that belong to the submanifold $Q$ .","['exterior-derivative', 'submanifold', 'smooth-manifolds', 'differential-forms', 'differential-geometry']"
4064357,$\Bbb Z\left [\frac{-1+\sqrt{-19}}{2}\right ]$ is not a Euclidean domain,"Definition : A universal side divisor, is an element $s\in R\setminus R^\times$ such that for every $x\in R$ either $s\mid x$ , or there is some unit $u\in R^\times$ such that $s\mid x+u$ . Fact : A Euclidean domain $R$ has universal side divisors. Proof : Let $s$ be a non unit of minimal norm $N(s)$ . Let $x\in R$ and write $$x=sq+r$$ Then either $r=0$ , and so $s\mid x$ , or $N(r)<N(s)$ . But since $N(s)$ is minimal we must have $r$ a unit, so $s\mid x-r$ . Thus $s$ is a universal side divisor. I'm doing the following excercises: Suppose $R=\Bbb Z[\alpha]$ , where $\alpha=\frac{-1+\sqrt{-19}}{2}$ . Prove that $R$ is not a Euclidean domain by showing that it has no universal side divisors. a) Find the units of $R$ . b) Take the usual norm $N$ on $R$ , $N(z)=z\bar z$ . Show that the only elements of norm less than $5$ are $\{0,\pm 1,\pm 2\}$ . c) Taking $x=2$ in the definiton of a universal side divisor, show that any universal side divisor must be a non-unit divisor of $2$ or $3$ . d) Find all non-unit divisors of $2$ and $3$ . e) Now taking $x=\alpha$ in the definition of a universal side divisor, find a contradiction. What I would like to know is: Is my proof correct? Why is the step b) needed? Here we go: a) Find the units of $R$ . Let $N(a+b\alpha)=a^2-ab+5b^2=1$ , there are two cases to consider: $a\geq b$ , then $1\geq 5b^2$ , so $b=0$ and $a=1$ $b\geq a$ , then $1\geq a^2+4b^2$ , so again $b=0$ and $a=-1$ . Thus the only units are $\pm 1$ . b) Take the usual norm $N$ on $R$ , $N(z)=z\bar z$ . Show that the only elements of norm less than $5$ are $\{0,\pm 1,\pm 2\}$ . Again, there are two cases to consider: $a\geq b$ , then $5>a^2-ab+5b^2\geq 5b^2$ , so $b=0$ , and $a=0,1,2$ $b\geq a$ , then $5>a^2-ab+5b^2\geq a^2+4b^2$ , so $b=0$ , and $a=0,-1,-2$ Thus the elements $0,\pm 1, \pm 2$ is the only elements with norm less than $5$ . c) Taking $x=2$ in the definiton of a universal side divisor, show that any universal side divisor must be a non-unit divisor of $2$ or $3$ . Since $R^\times=\{\pm 1\}$ a universal side divisor $s$ must divide one of $2, (2+1), (2-1)$ . Since $s$ is not a unit, we must have $s\mid 2$ or $s\mid 3$ . d) Find all non-unit divisors of $2$ and $3$ . Both $2,3$ are irreducible in $R$ . To see this assume that $N(2)=N(x)N(y)=4$ , and $N(3)=N(z)N(w)=9$ . Is is then enough to show that $N(x),N(y)\neq 2$ and $N(z),N(w)\neq 3$ . There are again two cases: If $a\geq b$ , then $a^2+4b^2\geq 2,3\geq 5b^2$ , and so $b=0$ and $a^2=2,3$ , which is impossible. If $b\geq a$ , then $5b^2\geq 2,3\geq a^2+4b^2$ , and so $b=0$ . Contradiction. So both $2,3$ are irreducible. e) Now taking $x=\alpha$ in the definition of a universal side divisor, find a contradiction. If $R$ is a Euclidean domain, then there is a universal side divisor $s$ such that $s\mid 2$ or $s\mid 3$ . As both $2,3$ are irreducible we have $s=\pm 2,\pm 3$ . Therefore $N(s)=4,9$ . Also $s$ divides one of $\alpha,\alpha-1,\alpha+1$ . Computing norms we have: $N(\alpha)=5$ $N(\alpha+1)=5$ $N(\alpha-1)=6$ As $4$ or $9$ divides none of them, we must have that $s$ doesn't divide any of $\alpha,\alpha-1,\alpha+1$ . Contradiction. Therefore $R$ is not a Euclidean domain.","['euclidean-domain', 'abstract-algebra', 'solution-verification']"
4064391,Use of the right Jacobian in SO(3) to approximate $\exp([\phi + \delta\phi]_\times)$,"I'm currently starting with Lie Groups, focusing on their applications in estimation (I have an engineering background). Having a look at posts like: "" jacobian involving SO(3) exponential map: $\log(R\exp(m))$ "" is stated that the right Jacobian is given by: $$ \mathtt{J}_r(\phi) =  \frac{\partial\exp([\phi]_\times)}{\partial \phi} = I - \frac{1-\cos(\Vert\phi\Vert)}{\Vert\phi\Vert^2}[\phi]_\times + \frac{\Vert\phi\Vert-\sin(\Vert\phi\Vert)}{\Vert\phi\Vert^3}[\phi]_\times^2$$ However, I'm having difficulties to understand how by using the right Jacobian $\mathtt{J}_r(\phi)$ we are able to relate an additive perturbation $\delta\phi$ in the tangent space to a multiplicative perturbation on the manifold SO(3). Given that a first-order approximation of $\exp([\phi + \delta\phi]_\times)$ , as noted in On-Manifold Preintegration for Real-Time
Visual-Inertial Odometry (page 4), is given by: $$\exp([\phi + \delta\phi]_\times) \approx \exp([\phi]_\times) \exp([\mathtt{J}_r(\phi)\delta\phi]_\times)$$ On the one hand, I don't understand how we can compute the skew matrix $[\cdot]_\times$ of $\mathtt{J}_r\delta\phi$ because I believe that $\mathtt{J}_r\delta\phi$ is already a matrix and not a vector $\in\mathbb{R}^3$ . On the other hand, given that $\mathtt{J}_r(\phi) =  \frac{\partial\exp([\phi]_\times)}{\partial \phi}\to$ Doesn't this mean that ( $\mathtt{J}_r(\phi)\delta\phi$ ) belongs to SO(3)? Following this, would the following expression be a valid way of computing the first-order approximation?: $$ \exp([\phi + \delta\phi]_\times) \approx \exp([\phi]_\times)\mathtt{J}_r(\phi)\delta\phi$$ Thanks in advance!","['optimization', 'matrix-exponential', 'lie-groups', 'differential-geometry']"
4064421,I state $x=2^{2002}$. How many integers are there between $\sqrt{x^2+2x+4}$ and $\sqrt{4x^2+2x+1}$,"I state $x=2^{2002}$ . How many integers are there between $\sqrt{x^2+2x+4}$ and $\sqrt{4x^2+2x+1}$ ? I tried to solve it as follows: I state $a\in\mathbb{N}$ , so that $\sqrt{x^2+2x+4}<a<\sqrt{4x^2+2x+1}$ Hence $x^2+2x+4<a^2<4x^2+2x+1$ $4x^2+2x+1-x^2-2x-4=3x^2-3=3*2^{4004}-3$ . Here is where I got stuck. Could you please explain to me how to solve this question?","['algebra-precalculus', 'inequality']"
4064527,Modified sum of three squares problem,"My teacher proposed this problem some time ago: We have four different integers $x,y,z,k$ such that $x^2+y^2+z^2=3k^2$ . Prove that the difference between the largest one of $x,y,z$ and the smallest one of $x,y,z$ is greater than $\sqrt{k\,}$ . The time for submissions is over so I thought I might ask here. I'm not sure it's ok with the rules here, especially that I haven't made any progress. I will still share what I've tried: $k$ is a quadratic mean, however I could not find any mean bigger than it; sub $y = x + t$ and $z = x+v$ doesn't seem to help show $t > \sqrt{k\,}$ or $v > \sqrt{k\,}$ geometric thinking of a sphere equation and looking for integer points is even more complicated","['number-theory', 'inequality', 'diophantine-equations']"
4064590,"Connection between two analytic notions of ""spectrum""","In Spectral Theory and Analytic Geometry over Non-Archimedean Fields Berkovich defines the spectrum of a commutative, unital Banach ring $A$ to be the set of all bounded multiplicative seminorms on $A$ with the weak topology with respect to the family of functions $$\phi_f: || \mapsto |f|$$ In another class, I learned the definition of spectrum $\sigma(T)$ of an element of a Banach algebra to be the set of all the $\lambda$ such that $T - \lambda I$ is non-invertible. Many times I try to look something up related to Berkovich's text (such as the Gel'fand transform, characters),  and I find a concept by the same or similar name that seems similar but is definitionally different. So are the two definitions connected somehow? This section of Wikipeda seems promising, as well as the page on the Gel'fand representation, but I could use some help connecting the dots.","['analytic-geometry', 'spectral-theory', 'functional-analysis', 'banach-algebras']"
4064602,How to get an original function from the limit definition of a derivative?,"Say I have $$\lim_{h\to0} \frac{e^h-1}{h}$$ If $$\frac{d}{dx}(e^x)|_{x=0} =
\lim_{h\to0} \frac{e^{0+h}-e^0}{h},$$ how would I “back engineer” the derivative limit definition to satisfy the expression meaning expression-I equals expression-II. But we’re only given expression-II the limit expression so how do we find the “mystery” expression-I from expression-II. I basically want to remove the guess work required to satisfy the two expressions, and if there’s even a general “algorithm” to follow?","['calculus', 'reverse-math', 'algorithms', 'limits', 'derivatives']"
4064684,What use is $L^2$-convergence for Fourier series?,"I'm working through some notes for my signal processing class and there's something elementary that baffles me. We spent lots of hours and dozens of pages setting up the entire theory of Hilbert spaces in order to define the Fourier series of a square integrable periodic function in terms of the orthogonal basis of exponentials $ e_n(t):[0,2\pi] \to \mathbb{C}: t \to e^{int} $ . Then all of the sudden, out of the blue, the notes assault me with a seemingly unrelated theorem about pointwise convergence of the series, and I find out that pointwise convergence isn't guaranteed by $L^2$ -convergence. So my (probably naive) question is: what was all that work good for? What use is $L^2$ -convergence if it doesn't guarantee pointwise convergence?","['harmonic-analysis', 'fourier-analysis', 'functional-analysis']"
4064710,Sequence queue - converges or diverges?,"I have the next sequence queue: $\sum_{n=1}^{\infty}{\frac{n^2-n-1}{n^4+n^2+1}}$ . Does the queue converges or diverge? My attempt: I have tried to show that $\frac{1}{n^2}>|\frac{n^2-n-1}{n^4+n^2+1}|$ for any $n>0$ , and then by the comparation test we get that since $\sum_{n=1}^{\infty}{\frac{1}{n^2}}$ converges, we have that $\sum_{n=1}^{\infty}{|\frac{n^2-n-1}{n^4+n^2+1}|}$ converges too, and by a theorem we have that since $\sum_{n=1}^{\infty}{|\frac{n^2-n-1}{n^4+n^2+1}|}$ $\implies$ $\sum_{n=1}^{\infty}{\frac{n^2-n-1}{n^4+n^2+1}}$ converges. Is that right?","['limits', 'calculus']"
4064730,partial derivative with respect to a vector?,"I encountered what I can only understand as a partial derivative with respect to a vector, used in a taylor series expansion, and would like to understand it better. In the book I'm reading, $x(t)$ is an $n$ -dimensional real vector, that is $x: \mathbb{R} \to \mathbb{R}^n$ , and $u(t)$ is a real valued function, $t\in \mathbb{R}$ . We have the equation $$
\dot{x} = f(x, u)
$$ The author writes that $f$ can be linearized around a stationary point, that is a point $(x_0, u_0)$ such that $f(x_0, u_0) = 0$ : By looking at small deviations from $x_0$ and $u_0$ $$
x = x_0 + \Delta x\\  
u = u_0 + \Delta u
 $$ we get with taylor expansion $$
\dot{x} = \Delta \dot{x} = f(x_0 + \Delta x, u_0 + \Delta u) \approx \\
f(x_0, u_0) + f_x(x_0, u_0)\Delta x + f_u(x_0, u_0)\Delta u
$$ where higher order terms in $\Delta x$ and $\Delta u$ are discarded. $f_x, f_u, h_x$ and $h_u$ denotes partial derivatives with respect to $x$ and $u$ . $f_x$ is a $n \times n$ matrix who's $i, j$ element is $$
\dfrac{\partial}{\partial x_j}f_i(x, u)
$$ where $f_i$ is the i:th row in $f$ , and the corresponding is true for $f_u$ . I have never seen this type of partial derivative before, $f_x$ is a partial derivative with respect to $x$ and $x$ is a vector. I've only seen partial derivatives with respect to a real variable before, and the wikipedia article on partial derivatives does not seem to mention it. Not surprisingly I've also never seen this kind of taylor expansion, the wikipedia article does not seem to use partial derivatives with respect to vectors, but I've never even worked with taylor series of several variables so I might be misunderstanding something. I can't find much information about this, am I right that this is a special kind of partial derivatives and taylor series, and in that case does anyone know of some relatively easy to understand source of information?","['partial-derivative', 'multivariable-calculus', 'taylor-expansion']"
4064934,largest singular value of real symmetric matrix,"I am trying to find a proof of the following fact: Let $M$ be a real symmetric matrix, then the largest singular value satisfies: $$
\sigma_1(M) = \lim_{k \to \infty} \left[\text{Trace}(M^{2k})\right]^{\frac{1}{2k}}
$$","['trace', 'reference-request', 'matrices', 'linear-algebra', 'singular-values']"
4064954,Proving $(A+B)^{-1} = A^{-1} + B^{-1}$ when there exists $J$ so that $J^2 = -I$,"I'm having trouble proving this biconditional statement: Prove that there exist $A, B \in \mathcal{M}_{n\times n}(\mathbb{R}^n)$ such that $(A+B)^{-1} = A^{-1} + B^{-1}$ if and only if there exists $J \in \mathcal{M}_{n \times n}(\mathbb{R}^n)$ such that $J^2 = -I_n$ . Here, $I_n$ is the $n \times n$ identity matrix. So far, none of my ideas have panned out and I don't feel like I have enough information to solve this problem. Any hints would be appreciated!","['matrices', 'linear-algebra', 'inverse']"
4064961,Extrema of $(1+\sin x)(1+\cos x)$,Find the extrema of $(1+\sin x)(1+\cos x)$ without using calculus . I was able to figure out the minima by observing that each of the brackets range from $0$ to $2$ . Therefore the minima has to be $0$ when either one of the brackets is zero. However I couldnt figure out the maxima. I tried expanding it to complete the square but it didnt quite work out well. Any hint is appreciated!,"['maxima-minima', 'trigonometry']"
4064969,"$ X>0, E(X)=1, E(X^2)=b, \forall a\in (0,1): P(X>a)\geq \frac{(1-a)^2}{b} $","Suppose that $ X>0, E(X)=1, E(X^2)=b $ And We should prove for every $ a $ such that $ 0 < a < 1 $ the following statement: $ P(X>a)\geq \frac{(1-a)^2}{b} $ This is a preliminary course of probability and we learned only basic formulas and inequalities of Markov and Chebyshev's. I would be happy if you keep the answer simple as much as possible.
Thanks.",['probability']
4065011,Weak Law of Large Numbers Confusion,"If I have a sequence of IID random variables $\{X_i\}_{i=1}^{n}$ with $\mathbb{E}[X_i]=\mu$ then the WLLN implies that $\bar{X}_n$ converges to $\mu$ . This is where my confusion lies. It is my understanding that RV are deterministic functions $X:\Omega\rightarrow \mathbb{R}$ , where the random component comes from $\omega\in\Omega$ . If we are defining $\bar{X}_n(\omega)=\frac{1}{n}\sum_{i=1}^{n}X_i({\omega})$ , if $X_i$ are iid then it is not assumed that $X_i(\omega)=X_j(\omega)$ so have an average of different values. When we apply the weak law of large numbers in real life we take the average of a sequence of iid random variables, but doesn't the WLLN require that the RV's be evaluated in the same $\omega$ , why are we able to assume that this holds. Am I understanding the WLLN wrong?","['statistics', 'probability-theory', 'probability']"
4065019,Is the difference between stable nonlinear systems Lyapunov stable?,"If there are two nonlinear systems with stable equilibria $x_1 = x_2 = 0$ $$\dot x_1 = f(x_1, u) \qquad \dot x_2 = g(x_2, u)$$ with identical inputs $u$ , is the difference between the system states $x_1, x_2 \in \mathbb{R}^n$ $$e = x_1 - x_2$$ stable? Trivial case If the systems are LTI with identical system and input matrices $A$ and $B$ $$f(x_1 = x, u) = g(x_2 = x, u) = Ax + Bu,$$ the system dynamics is $$\dot e = Ae.$$ A Lyapunov function $$V(e) = e^T P e$$ results in $$\dot V(e) = \dot e P e^T + e^T P \dot e = e^T (A^T P + P A) e.$$ If $A$ is Hurwitz, then $A^T P + P A = -Q$ with a positive definite real symmetric $P$ and a positive definite $Q$ . As $\dot V(e) = -e^TQe < 0\ \forall x \neq 0$ , the difference between the two asymptotically stable LTI systems is asymptotically stable. My Question Can this be generalized to arbitrary nonlinear globally/locally/asymptotically/... stable systems, where $f = g$ ? (1) where $f \neq g$ ? (2) Thought experiment : An equilibrium is stable if for each $\epsilon > 0$ , there is a $\delta$ such that with $|| x || < \delta$ at $t = t_0$ the state remains within $|| x || < \epsilon$ . Wouldn't that imply that the vector $e$ connecting two $x_1, x_2$ remains within a hypersphere $\mathcal{B}$ that includes both $|| x_1 || < \epsilon_1$ and $|| x_2 || < \epsilon_2$ ? Then for $|| e || < \min(\delta_1, \delta_2)$ at $t = t_0$ , it would hold $|| e || < 2r_\mathcal{B}$ with $r_\mathcal{B}$ radius of $\mathcal{B}$ ? At the same time : Starting from $$V(e) = \frac{1}{2} e^T e,$$ I quickly get stuck at $$\dot V(e) = e^T \dot e = e^T \left(f\left(x_1, u\right) - g\left(x_2, u\right)\right) =\ ...?$$ Update Arastas answer has a counterexample for $f \neq g$ (loosely quoting): $\dot x_1 = u$ and $\dot x_2 = -u$ leads to $\dot e = 2u$ . With $u=1$ , we get $e(t) \rightarrow \infty$ for $t \rightarrow \infty$ . In this SISO example, the derivatives of $f$ and $g$ w.r.t. $u$ differ: $$\frac{\partial}{\partial u} f(x_1, u) = 1 \neq -1 = \frac{\partial}{\partial u} g(x_2, u).$$ Speaking more generally, the (transposed) gradient matrices of $f$ and $g$ w.r.t. $u$ differ: $$(\nabla_u f)^T = \frac{\partial}{\partial u} f(x_1, u) \neq \frac{\partial}{\partial u} g(x_2, u) = (\nabla_u g)^T.$$ What can we say about the stability of $e$ for arbitrary $f, g$ , but allowing for restrictions on the gradients $\nabla_u f$ , $\nabla_u g$ ? (3) If the restriction were made $\nabla_u f = \nabla_u g$ , (3) would capture case (1).","['ordinary-differential-equations', 'lyapunov-functions', 'control-theory', 'stability-in-odes', 'nonlinear-system']"
4065059,Prove $\frac{a^{2}}{b^{2}} +\frac{b^{2}}{c^{2}} +\frac{c^{2}}{a^{2}} +\frac{15abc}{4}\geq \frac{27}{4}$ for $a^{2}+b^{2}+c^{2}+abc=4$,"Prove: $$\frac{a^{2}}{b^{2}} +\frac{b^{2}}{c^{2}} +\frac{c^{2}}{a^{2}} +\frac{15abc}{4}\geq \frac{27}{4}$$ for $a,b,c>0$ such that $$a^{2}+b^{2}+c^{2}+abc=4.$$ I tried to note $a=\cos A$ ,... because of a known identity, I used $s=\frac{a^{2}}{b^{2}} +\frac{b^{2}}{c^{2}} +\frac{c^{2}}{a^{2}} \geq \frac{a}{b} +\frac{b}{c} +\frac{c}{a}$ and $s\geq \frac{a}{c} +\frac{b}{a} +\frac{c}{b}$ , and then I added them. Another suggestion? Please!
I also noted $s=a+b+c,p=ab+bc+ac, r=abc$ . The condion is equivalent with $s^2-2p+r=4$ . Using the 2 inequalities which I mentioned above $\implies s\ge(a+b)(b+c)(a+c)/(2abc)-1$ , and then is enought to prove that $2sp+15r^2\ge33r$ , but I don't know if the last one is correct inequality. You can try to prove that. I'm desperate.","['contest-math', 'algebra-precalculus', 'inequality']"
4065070,Matrix with the same value in all entries — which notation to use?,"Pretty trivial but for a matrix \begin{bmatrix}x&x&\ldots&x&x\\x&x&\ldots&x&x\\\vdots&\vdots&\ddots&x&x\\x&x&\ldots&x&x\end{bmatrix} with $N\times N $ , is there is simpler notation. I had $A=[x_{i,j}]\in \Bbb R^{N\times N}$ in my mind, but I can't find anywhere were it is used Edit: I forgot to mention but, I need to work with the elements in the matrix, so like doing scalar multiplication (to do a proof by the principle of mathematical induction question). But it a $3\times 3$ so I figured out I do have to write every single element 9 times in a row. Is there any notation that can make my math simpler?","['matrices', 'notation']"
4065119,Big O for error terms,"The following link from wikipedia explains the Big O notation really good. I have only one problem, which is to formalize the usage of Big O notation for error terms in polynomials. In the  example give here  we have $$
e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+...=1+x+\frac{x^2}{2!}+\mathcal{O}(x^3)=1+x+\mathcal{O}(x^2)
$$ as $x\rightarrow 0$ . Now we find a similar notation also for the error terms in taylor polynomials. I would like to understand why this is right just formaly. 1.) Why is $\frac{x^3}{3!}+\frac{x^4}{4!}+... = \mathcal{O}(x^3)$ Is this because $$
\frac{x^3}{3!}\frac{1}{x^3}+\frac{x^4}{4!}\frac{1}{x^3}+\frac{x^5}{5!}\frac{1}{x^3}+...=\frac{1}{3!}+\frac{x}{4!}+\frac{x^2}{5!}+...\leq M, \quad x\rightarrow 0
$$ for some $M$ that has to be bigger than $\frac{1}{3!}$ or can one show this in a different way formaly? 2.) Why is $\frac{x^2}{2!}+\mathcal{O}(x^3) = \mathcal{O}(x^2)$ ? I appreciate your help! :)","['calculus', 'asymptotics', 'analysis', 'real-analysis']"
4065164,Prove that $\mathcal{B}(\mathbb{R}^n)=\sigma(S_1)=\sigma(S_2)$.,"QUESTION: Given the following collection of subsets of $\mathbb{R}^n$ , $$S_1=\{F\subset\mathbb{R}^n; F\; \text{is closed}\;\}$$ and $$S_2=\{(a_1, b_1)\times\cdots \times (a_n, b_n)\in \mathbb{R}^n; \; \text{where}\; (a_i, b_i)\subset \mathbb{R}, \; i=1, \cdots, n. \}.$$ Prove that $\mathcal{B}(\mathbb{R}^n)=\sigma(S_1)=\sigma(S_2)$ . MY ATTEMPTY: First let's prove that $\mathcal{B}(\mathbb{R}^n)=\sigma(S_1)$ . We have $\mathcal{B}(\mathbb{R}^n)=\sigma(\mathcal{O})$ where $\mathcal{O}$ is a collections of open subsets in $\mathbb{R}^n$ . Now, remembering that a subset in $\mathbb{R}^n$ is open iff its complement is closed. Considering $S_1=\mathcal{O}^c$ , since the Borel $\sigma$ -algebra in $\mathbb{R}^n$ is also a $\sigma$ -algebra then $$\mathcal{B}(\mathbb{R}^n)=\sigma(\mathcal{O})=\sigma(\mathcal{O}^c)=\sigma(S_1).$$ Now let's prove that $\mathcal{B}(\mathbb{R}^n)=\sigma(S_2)$ .
Remembering that open rectangles $(a_i, b_i)\times\cdots\times (a_j, b_j)\in \mathbb{R}^n$ provides a generator bases of the topology of $\mathbb{R}^n$ , such that any open subset can be represented by a countable union of rectangles, hence, writting $$E_i=\displaystyle\bigcup_{i=1}^{\infty}\left[(a_i, b_i)\times \cdots \times (a_j, b_j)\right]_i$$ and $$\displaystyle\prod_{k=1}^{n}(a_k, b_k)=\left[(a_i, b_i)\times \cdots \times (a_j, b_j)\right].$$ Thereby, $$E_i=\displaystyle\bigcup_{i=1}^{\infty}\prod_{k=1}^{n}(a_k, b_k).$$ On the one hand ones has $$S_2\subset S_1^c\implies \sigma(S_2)\subset\sigma (\mathcal{O})=\mathcal{B}(\mathbb{R}^n).$$ On the other hand, if $E_i\in S_1^c=\mathcal{O}$ then exists open rectangles $\prod_{k=1}^{n}(a_k, b_k), \; k=1, 2, \cdots, n$ such that $$E_i=\displaystyle\bigcup_{i=1}^{\infty}\prod_{k=1}^{n}(a_k, b_k).$$ Thus, $E_i\in \sigma(S_2)$ , this is, $S_1^c=\mathcal{O}\subset\sigma(S_2)$ , therefore $$\mathcal{B}(\mathbb{R}^n)=\sigma(S_1^c)\subset\sigma(\sigma(S_2))=\sigma(S_2).$$ MY DOUBT: Would someone scan for mistakes in my proof? I feel that it is not completely right. For example, I'm not sure about the first step if I can conclude that equality. And didn't persuade myself with the proof I've provided in the second equality.","['measure-theory', 'probability-theory', 'probability']"
4065275,Limit exists with definition but not with polar coordinates,"I would like to know why if I try to prove with the delta epsilon definition that the limit as $(x,y)$ tends to $(0,0)$ is $0$ of this function: $$\frac{x^2 +y}{\sqrt{x^2+y^2}}$$ I get a positive result: for $\delta = \varepsilon -1$ , I get that $|f(x,y)-0|$ is less than $\varepsilon$ . But if you evaluate the limit with polar coordinates, you get that the limit depends on the path, and thus it doesn't exist.","['epsilon-delta', 'real-analysis', 'multivariable-calculus', 'polar-coordinates', 'limits']"
4065319,A question about limit [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Can anyone help this questions? Find the limit of $$1+\sqrt{2+\sqrt[3]{3+\sqrt[4]{4+\sqrt[5]{5+....+\sqrt[n]{n}}}}}$$ I can only solve that this formula is less than 3 but can not find the exact answer for this.","['nested-radicals', 'limits']"
4065347,A proof of J.L. Lions' Lemma,"I started a study on the Navier Stokes equations and my supervisor gave me a first result to work on which is the J.L. Lions Lemma which basically states the following: If $f\in H^{-1}(\Omega)$ and $\nabla f\in H^{-1}(\Omega)$ . Then $f\in L^2(\Omega)$ My supervisor told me that since $\Omega$ is a Lipschitz domain and from the proof of the Global approximation by functions smooth up to the boundary theorem and its Evans's Partial Differential Equations: making the convolution of $f\in H^{-1}(\Omega)\, \nabla f\in H^{-1}(\Omega)$ , we are in $L^2$ . We can then use the Necas inequality. My ideas: I think that the step 1 and 2 of proof is for build the open covers $\{V_i\}_{i=0}^{q}$ where $V_i=\Omega\cap B(x_0,\frac{r_i}{2})$ and we can choose $\Omega = \cup_{i=0}^q V_i$ . We can also take a classic regularizing sequence $(\rho_n)$ for $\epsilon>0$ , let $\eta_\epsilon = \rho_{[1/\epsilon]+1}$ or again $f_\epsilon(x) = f(x^\epsilon) = f(x+\lambda\epsilon e_n)$ and I can take $$g_\epsilon = \eta_\epsilon * f_\epsilon.$$ I think it must be proven that $g_\epsilon \in L^2(V_i)$ and with that we can later use the Necas' inequality Then, we seek to provide hypotheses to ensure the existence of the unit partition $\theta_0,\,\theta_2\,...\,\theta_q$ associated with the open cover $\{V_i\}$ and we take $h = \theta_i g_\epsilon$ and then prove that (by Necas' inequality) $$\|h\|_{L^2(\Omega)} = \|h\|_{L^2(V_i)}\leq C(\|g_\epsilon\|_{H^{-1}(\Omega)}+\|\nabla g_\epsilon\|_{H^{-1}(\Omega)}),$$ and passing to the limit when $\epsilon \rightarrow 0 $ we would get $$\|f\|_{L^2(\Omega)}\leq C(\|f\|_{H^{-1}(\Omega)}+\|\nabla f\|_{H^{-1}(\Omega)})<\infty.$$ We can conclude that $f\in L^2(\Omega)$ Any indications or suggestions are welcome","['partial-differential-equations', 'sobolev-spaces', 'functional-analysis', 'real-analysis']"
4065370,Tangential polygons: Conditions on edge lengths?,"Given any $L = (\ell_1,\ell_2,\ldots,\ell_n)$ edge lengths, it is possible to
construct a cyclic (inscribed) convex polygon. This can be seen by viewing the
edges are rigid bars and the vertices as universal joints. Then place a polygonal
chain with those lengths in a large-radius circle, and shrink the radius until the
chain closes to a polygon: However, the same proof idea does not work for tangential polygons.
A tangential polygon (circumscribed)
has each edge tangent to a circle: (Image by Claudio Rocchini in Wikipedia .) My question is: Q . Which length sequences $L = (\ell_1,\ell_2,\ldots,\ell_n)$ can be realized
as the edge lengths of a tangential polygon of $n$ vertices?
In other words, characterize the realizable $L$ . For tangential quadrilaterals, $\ell_1+\ell_3 = \ell_2+\ell_4$ , and
this generalizes for all even $n$ .
But I am not sure that this necessary condition is also sufficient. It is for $n=4$ , but what is known for even $n$ greater than $4$ ?
Nor do I know of conditions for odd $n$ .","['geometry', 'polygons', 'plane-geometry']"
4065418,"Why do we use G, H, and 'K' for groups?","In many cases of mathematics, we tend to set several variables in ascending alphabetical order. $x,y,z \dots$ for normal variables $A,B,C\dots$ for sets, coefficients, matrices $V,W\dots$ for vector spaces ${I,J}\dots$ for indices, ideals $\mathcal{C,D}\dots$ for categories But when we need 3 groups, we tend to use $G, H$ and suddenly, $K$ , Which is unexpected. Here are some examples that groups are denoted by $K$ . Prove that quotient group K/H is normal subgroup of quotient group G/H https://en.wikipedia.org/wiki/Group_homomorphism#The_category_of_groups https://proofwiki.org/wiki/Pullback_of_Quotient_Group_Isomorphism_is_Subgroup Why do we denote groups by $K$ regardless of the alphabetical order?","['notation', 'group-theory', 'abstract-algebra', 'soft-question']"
4065459,About Ultrafilters on Infinite Sets,"Blackburn's Modal Logic, Ex $2.5.4$ : Let $W$ be an infinite set. Recall that $X\subseteq W$ is co-finite iff $W\setminus X$ is finite. (a) Prove that the collection of co-finite subsets of $W$ has the finite intersection property. (b) Show that there are ultrafilters over $W$ that do not contain any finite set. (c) Prove that an ultrafilter is non-principal if and only if it contains only infinite sets
if and only if it contains all co-finite sets. (d) Prove that any ultrafilter over $W$ has uncountably many elements. (a) is already answered here . To prove (b), we can prove (c) and argue that non-principal ultrafilters exist. So I shall try $\color{red}{(c)}$ first. I want to show $$\text{non-principal} \stackrel{1}{\Longleftrightarrow} \text{contains only infinite sets} \stackrel{2}{\Longleftrightarrow} \text{contains all co-finite sets}$$ What is a principal ultrafilter? The principal ultrafilter generated by $w$ , is $\{X
\in P(W): w\in X\}$ .
The second double-implication is easier to prove, so let's do that first. Keep an ultrafilter $u \subset P(W)$ in mind for the entire proof. Suppose $u$ contains only infinite sets, and there is some co-finite set $A$ not contained in $u$ . Since $u$ is an ultrafilter, $W\setminus A \in u$ , but $W\setminus A$ is finite, which is a contradiction. Suppose $u$ contains all co-finite sets. If possible, let $u$ contain some finite set $B$ . Then, $W\setminus B \notin u$ . However, $B$ is finite $\implies W\setminus B$ is co-finite. This is a contradiction. So, the second double implication, i.e. $\stackrel{2}{\Longleftrightarrow}$ is proved. Now, let us turn to $\stackrel{1}{\Longleftrightarrow}$ . Suppose $u$ contains only infinite sets. If possible, let $u$ be principal. Then there exists some $w\in W$ such that $u = \{X
\in P(W): w\in X\}$ . $w\in \{w\}$ , obviously. So, $\{w\} \in u$ , but $\{w\}$ is a finite set. This is a contradiction. Thus, $u$ is non-principal. For the other direction, suppose $u$ is non-principal. Suppose, for a contradiction, that $u$ contains some finite set $C$ . So, $W\setminus C$ is infinite. Also, let $C = \{c_1,c_2,\ldots,c_n\}$ . Stuck here! I want to show that $C\in u$ makes $u$ a principal filter, somehow, in order to find a contradiction. For that, I must find a $w\in W$ so that $\{w\} \in u$ . Correct? Please help me out here. Update for part $\color{red}{(c)}$ : If $u$ is an ultrafilter, we know that $X\cup Y \in u$ if and only if $X\in u$ or $Y\in u$ . $C\in u$ means that $\{c_1,c_2,\ldots,c_{n-1}\} \cup \{c_{n}\} \in u$ . So, either $\{c_1,c_2,\ldots,c_{n-1}\} \in u$ or $\{c_n\}\in u$ . If $\{c_n\} \in u$ , we are done. If not, we can recursively apply the same argument to $\{c_1,\ldots,c_{n-1}\}$ . At every step, we drop an element of this finite set, and so the process terminates. We will be able to find a singleton $\{c_i\}$ so that $\{c_i\}\in u$ . This makes $u$ a principal filter (right?) , which is the required contradiction. Once (c) is proved, I must explicitly show that non-principal ultrafilters exist, in order to prove $\color{red}{(b)}$ . Any ideas for this? Perhaps the author wanted something else, since they've put (b) before (c) - but I hope this way works too. Please drop some hints or ideas on how to proceed and conclude (b) from here! Now, for part $\color{red}{(d)}$ : Suppose $u\subset P(W)$ has at most countable number of elements. Then, $u$ is either finite or $u = \{X_1,X_2,...\}$ . How do I find a contradiction from here? Looks like I'm stuck again! Thanks a lot for your help. Let me know if any of your solution ideas are different from mine, I'd be happy to see different approaches. Also, please help me complete my solutions, if possible!","['elementary-set-theory', 'filters']"
4065485,"For all real numbers satisfying $a < b$, there exists an $n \in \mathbb{N}$ such that $a + 1/n < b.$","From Stephen Abbott's Understanding Analysis 1.2.11 : For all real numbers satisfying $a < b$ , there exists an $n \in \mathbb{N}$ such that $a + 1/n < b.$ My try: $$\forall a\in \Bbb R, \forall b\in \Bbb R, \exists n \in \Bbb N \space \text{that satisfies}: \space a<b \Rightarrow a+\frac 1n<b$$ $\because a<b $ $\therefore\exists m\in\Bbb R, m>0$ such that $b=a+m$ . So the statement we are proving becomes (substitute $b$ by $a+m$ ): $$\forall a\in \Bbb R, m \in \Bbb R, \exists n\in\Bbb N \space \text{that satisfies}: a<a+m \Rightarrow a+\frac 1n < a+m$$ Which is (take $a$ from both sides): $$\forall m\in \Bbb R, \exists n\in\Bbb N \space \text{that satisfies}:0<m \Rightarrow \frac 1 n<m$$ We write this as: $$\forall m\in \Bbb R, 0<m, \exists n\in\Bbb N \space \Rightarrow \frac 1 n<m$$ We define set $M = \{m\mid M=\Bbb R \cap m>0\}$ And set $N = \{\frac 1n \mid n \in \Bbb N\}$ $\because\forall q \in \Bbb Z, p\in\Bbb Z, \frac p q\in\Bbb Q$ $\space\space\space n\in\Bbb N\subset\Bbb Z$ $\therefore\frac 1n\in\Bbb Q$ Mention that $m\in\Bbb R$ $\because\Bbb Q\subsetneq\Bbb R$ $\therefore\forall n\in\Bbb N, \exists m \in \Bbb R \Rightarrow \frac 1n \geqslant m$ This statement is equivalent to: $$\forall m\in \Bbb R, 0<m, \nexists n\in\Bbb N \space \Rightarrow \frac 1 n<m$$ Which disproves the statement, which is wrong - What is wrong with my disproof? Also, I am learning how to use mathematical notations properly (because I am only in year 11 but anyway). Please tell me if there is any error in my expressions. Thanks a lot!","['real-numbers', 'analysis', 'real-analysis']"
4065513,About the transference of perfect sets under Polish group actions,"Let $G$ be a Polish group, $X$ a Polsih space on which $G$ acts continously and consider the orbit equivalence relation on $X$ with respect to $G$ . Suppose $A\subseteq X$ is a perfect set of pairwise non orbit equivalent elements, $B\subseteq X$ is closed and for all $a\in A$ , the $G$ orbit of $a$ contains exactly one $G$ orbit of $B$ . My question: Does it follow that $B$ contains a perfect set of pairwise non orbit equivalent elements? I assume the answer is positive but I cannot prove it. One idea I have been entertaining is to show that for some $g\in G$ , there are uncountably many $a\in A$ such that $g\circ a\in B$ , where $\circ$ denotes the action of $G$ on $X$ . Clearly, if this can be done, then by the perfect set theorem the question is answered. Any help is appreciated.","['general-topology', 'logic', 'descriptive-set-theory', 'topological-groups']"
4065613,Real part of a primitive root of unity is not an algebraic integer for n>4.,"I'm trying to prove the following. Suppose that $\alpha$ is an $n$ th root of unity whose real part is an algebraic integer. Then $\alpha^4 = 1$ . I've thought about this for a while, and have the following ideas/questions. If I can show that the algebraic norm $\frac{1}{2}(\alpha + \alpha^{-1})$ is not an integer for $n> 4$ , then I will be done. Since $x^2-(\alpha +\alpha^{-1})x +1$ is the minimal polynomial of $\alpha$ over $\mathbb{Q}(\alpha+\alpha^{-1})$ , we have that $$
N\left(\frac{1}{2}(\alpha + \alpha^{-1})\right) = \frac{1}{2^{\phi(n)/2}}N(\alpha+\alpha^{-1})
$$ where $\phi(n)$ is the Euler $\phi$ function. So we just need to consider $N(\alpha+\alpha^{-1})$ . Is there a way that we can bound $N(\alpha + \alpha^{-1})$ ? If we can show that this norm is less than $2^{\phi(n)/2}$ then we're done. Another thought is that $\frac{1}{2}(\alpha+\alpha^{-1})$ is a root of $T_n(x)-1$ where $T_n$ is the $n$ th Chebyshev polynomial. This however is not generally irreducible, so the fact that $T_n$ is not monic does not tell us that $\frac{1}{2}(\alpha+\alpha^{-1})$ is not an algebraic integer. Thanks for any thoughts or hints.","['field-theory', 'number-theory', 'algebraic-number-theory']"
4065748,Minimize the standard deviation in list of numbers by subtracting from each value,"Suppose we have a list of values, $L = (x_1, x_2, ..., x_n)$ , and another list of numbers, $Q = (q_1, q_2, ..., q_n), q_i$ being the maximum value we can subtract from $x_i$ (so from $x_i$ we can subtract any number in $[0; q_i]$ . I want to find a combination of subtractions such that the standard deviation of the resulting list will be minimal. (In other words, I want to find a set of values, $R = (r_1, r_2, ..., r_n)$ , $r_i \in [0; q_i]$ such that the standard deviation of $L' = (x_1-r_1, x_2-r_2, ..., x_n-r_n)$ is minimal.) A graphical example (the blue points are the plotted $x_i$ ): Any help would be appreciated. Thanks.","['optimization', 'statistics', 'standard-deviation', 'probability']"
4065767,How to prove $\sum_{n=0}^\infty \frac{(2n)!}{(2^nn!(2n+1))^2} = \frac{\pi}{2}\ln(2)$?,"I was messing around with some integrals and series when I arrived at the result (and WolframAlpha agrees ): $$\sum_{n=0}^\infty \frac{(2n)!}{(2^nn!(2n+1))^2} = \sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n} \frac{1}{(2n+1)^2} = \frac{\pi}{2}\ln(2)$$ I therefore have a proof, but I would like to see how a proof would be done the other way around (i.e. starting with this series). Notice that the series could also be expressed with double factorials: $$\sum_{n=0}^\infty \frac{(2n)!}{(2^nn!(2n+1))^2} = \sum_{n=0}^\infty \frac{(2n-1)!!}{(2n)!!}\frac{1}{(2n+1)^2}$$ My Derivation My derivation is quite simple. I was considering (and trying to solve) the integral $$\int_0^1 \frac{\arcsin(x)}{x} \, dx$$ I couldn't work it out, however I found the exact same question here on math exchange and the answers show that the integral equals $\frac{\pi}{2}\ln(2)$ . What I did found however, was that I could plug the Taylor series for $\arcsin(x)$ (the proof of the taylor series can be found here ) into the original series to only be left with a series. Here's the Taylor series for $\arcsin(x)$ : $$
  \arcsin(x) = \sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n} \frac{ x^{2n+1}}{2n+1}
$$ So plugging in the series above into the integral gives us: $$\int_0^1 \frac{1}{x} \sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n} \frac{ x^{2n+1}}{2n+1} \, dx$$ Now here we have to be careful, but since the series works on the integral's interval $[0,1]$ and the summand is only positive, we can interchange the the integral and the sum, leaving us with: $$\sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n} \frac{1}{2n+1} \, \int_0^1 \frac{x^{2n+1}}{x} \, dx$$ And then solving the integral in the sum gives us the final result: $$\sum_{n=0}^\infty \frac{1 }{2^{2n}}\binom{2n}{n} \frac{1}{(2n+1)^2} = \frac{\pi}{2}\ln(2)$$ And then expanding the binomial gives the original series in the title.","['integration', 'sequences-and-series']"
4065772,"Prove that if $f_n \rightarrow f$ in measure and $\phi$ is continous, then $\phi \circ f_n \rightarrow \phi \circ f$ in measure.","Let $f_n,f: E \rightarrow \mathbb{R}$ where $E$ has finite measure, suppose that $\phi:\mathbb{R} \rightarrow \mathbb{R}$ is continuous. Then $\phi \circ f_n \rightarrow \phi \circ f$ in measure. Show that this can fail when $E$ has infinite measure. Here is my work so far: Since $f_n \rightarrow f$ in measure there exists a subsequence $f_{n_k}$ such that $f_{n_k} \rightarrow f$ pointwise a.e. on $E$ . Then by continuity we have $\phi\circ f_{n_k} \rightarrow \phi \circ f$ pointwise a.e. on E. Of course this implies that $\phi \circ f_{n_k} \rightarrow \phi \circ f$ in measure. My trouble is that this only shows a subsequence converges in measure and not the full sequence itself. I feel like I almost there but I am just confused about how to end the proof. Any help is appreciated!","['measure-theory', 'real-analysis']"
4065788,Definite integral $\int_0^{\pi} \sin^{n} x \ln(\sin x) dx$?,"Is the integral $$I(n) = \int_0^{\pi} \sin^{n}(x) \ln(\sin(x)) dx$$ analytically tractable for $n \in \mathbb{N}$ ? If not, are there good upper and lower bounds? To clarify, although this has a fairly standard-looking form, I haven't been able to find an answer anywhere and I've tried a few of the usual tricks (although I'm a tad rusty). I have tried integration by parts to reduce to a simpler or recursive form but this seems to make the expression more complex, and likewise with the substitution $u=\sin(x)$ . Equally, it is relatively easy to see that the expression under the integral is non-positive. Taking a derivative $$ \frac{d}{dx} \sin^{n}(x) \ln(\sin(x)) = \sin^{n-1}(x) \cos(x) (1 + n \ln(\sin(x))) = 0$$ gives a has minimum at $-\frac{1}{en}$ . Together these give the reasonably simple bounds: $$-\frac{\pi}{en} < I(n) < 0.$$ But numerically these don't appear to be particularly tight, so I'm still hopeful of an exact solution or an improvement.","['integration', 'definite-integrals']"
4065811,Sequential characterization of integrability,"I'm having some difficulties trying to understand a step from the following proof, which is about the sequential characterization of integrability. Here's the theorem: And here's the part of the proof I cannot totally understand: I understand why squeeze theorem can be applied, and I agree with the fact that it implies: $$
\lim_{n\rightarrow \infty}{[U(f;P_n)-U(f)]} = 0
$$ Yet still I don't see why the above limit can be 'split' into two different limits, since, in general, the fact that: $$
\lim_{n\rightarrow \infty}{(a_n-b_n)} = 0
$$ does not imply the existence of the following limits: $$
\lim_{n\rightarrow \infty}{a_n}, \,\,\,\,\,\,\,\,
\lim_{n\rightarrow \infty}{b_n}. 
$$ Thanks beforehand!!","['riemann-integration', 'limits', 'proof-explanation', 'real-analysis']"
