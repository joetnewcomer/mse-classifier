question_id,title,body,tags
4466797,Examples of self-dual categories,Call a category $C$ self-dual if there exists an equivalence of categories $F: C \rightarrow C^{op}$ . I am looking for examples of self-dual categories. Rigid monoidal categories and more generally star-autonomous categories are self-dual essentially by definition. What are other classes of examples? Are there self-dual categories that do not admit a star-autonomous monoidal structure?,"['monoidal-categories', 'examples-counterexamples', 'category-theory', 'abstract-algebra', 'duality-theorems']"
4466850,"Why is ""weak convergence of a sequence to $0$"" equivalent to ""accumulation points of the sequence are all $0$"" in the proof of mean ergodic theorem?","For some context, I am referring to a proof of the Mean Ergodic Theorem from Ergodic Theory and Dynamical Systems by Yves Coudene. Suppose $H$ is a Hilbert space, and let $U \colon H \to H$ be a linear map with $|| Uf || \leq ||f||$ . Set $$S_n(f) = \sum_{k=0}^{n-1} U^k f \qquad Inv = \{ f \in H : Uf = f\}.$$ Then $$ \frac{1}{n} S_n(f) \to Pf $$ where $P$ is the orthogonal projection onto $Inv$ . We want to prove that $|| \tfrac{1}{n}S_n (f) || \to 0$ if $f \in Inv^\perp$ . The statement I am having difficulty understanding is We have the equality $$ || \tfrac{1}{n}S_n (f) ||^2 = \langle f, \tfrac{1}{n}{S_n}^*\tfrac{1}{n}S_n(f)\rangle. $$ We must therefore verify, for every $f \in Inv^\perp$ , that the sequence $\tfrac{1}{n}{S_n}^*\tfrac{1}{n}S_n(f)$ converges weakly to $0$ , or equivalently
that the accumulation points of this sequence are all $0$ . Now I do not understand the equivalence here. The left to right implication is clear.","['hilbert-spaces', 'ergodic-theory', 'functional-analysis', 'weak-convergence']"
4466904,"An example of a non-diagonalisable matrix in $\mathrm{SL}(5, \mathbb{Z})$ whose Jordan blocks don't have determinant $1$","Does there exists a matrix $M \in \mathrm{SL}(5, \mathbb{Z})$ , such that: $M$ is not diagonalisable; Let $J$ be the Jordan normal form of $M$ , $J$ has one Jordan block of size $3$ and one Jordan block of size $2$ , and none of the Jordan blocks have a determinant with an absolute value of $1$ ? So $J$ looks like this, $$
J =
\left(
\begin{array}{ccc}
\lambda_1  & 1  & 0 & 0& 0\\
 0 &  \lambda_1 & 0 & 0& 0 \\
 0 & 0 & \lambda_2 & 1& 0 \\
 0 &  0 & 0 & \lambda_2& 1 \\
 0 &  0 & 0 & 0& \lambda_2 \\
\end{array}
\right).
$$ with $|\lambda_1|^2 \ne 1$ and $|\lambda_2|^3 \ne 1$ ? Where I got this question from : This is a follow-up question to the one I ask here . (Thank you, TheSilverDoe, who answered my previous questions). So far, all the examples are made up of ""smaller"" $2$ by $2$ submatrices. I was wondering if there are matrices that are not of this form, e.g. a matrix with an odd degree. Thanks for your time reading and help in advance.","['eigenvalues-eigenvectors', 'matrices', 'abstract-algebra', 'linear-algebra', 'group-theory']"
4466913,How can I use nine-point circle to solve this concyclic problem,"I saw this problem on the Discord Math channel. H is the orthocenter of △ ABC . D , E and F are the foot of the altitudes of △ ABC passing through A , B and C respectively.  Lines EF and BC intersect at R .  The line parallel to EF passing through D intersects AB and AC at P and Q respectively. M is the midpoint of BC .  Prove that M , P , Q and R are concyclic. Someone said that this can be proved using the nine-point circle, but I tried hard drawing figures to see things like ∠ FEM = ∠ BDF = ∠ BAC .  I also know that BM = CM = EM = FM .  However, when P and M come together, I find the angle calculations hard because M is defined by a side length, and P is defined by an angle.  I can't see a way without using trigonometry functions. I've derived on my own the following basic result that the orthic triangle △ DEF admits H as its incenter.  I've used the result (1) in the previous paragraph. I showed this by the converse of the intersecting chords theorem instead of nine-point circle.  I've chosen this approach for two reasons: I've got lost in the figures while calculating angles. M is defined by a side length, so it'll be easier to calculate side lengths. MD ⋅ DR = (0.5 BC − CD ) ( DC + CR )      ⋯⋯ ① We calculate CR to eliminate R from this product.  I used Ceva's Theorem and Menelaus's Theorem to see that | BR : RC | = BD : DC .  This gives RC : CB = RC : | BR − RC | = DC : ( BD − DC ) = CD : ( BC − 2 CD ) CR = BC ⋅ CD / ( BC − 2 CD )      ⋯⋯ ② Substitute ② into ①: MD ⋅ DR = (0.5 BC − CD ) ( DC + CR ) = 0.5 ( BC − 2 CD ) [1 + BC / ( BC − 2 CD )] CD = 0.5 [( BC − 2 CD ) + BC ] CD = ( BC − CD ) CD = BD ⋅ CD ⋯⋯ ③ Observe that B , Q , C and P are concyclic. observation reason ∠ CQD = ∠ AEF corr . ∠ s , EF // PQ ∠ AEF = ∠ ABC result (1) in above screenshot ∴ ∠ CQD = ∠ DBP i.e. B , Q , C and P are concyclic. converse of ∠ s in same seg . BD ⋅ DC = PD ⋅ DQ chord thm . MD ⋅ DR = BD ⋅ DC ③ ∴ MD ⋅ DR = PD ⋅ DQ i.e. M , P , Q and R are concyclic. converse of chord thm . Q.E.D.","['euclidean-geometry', 'circles', 'geometry', 'triangles', 'triangle-centres']"
4466926,Integrating $\frac{1}{x}$,"There is a general (mis)conception that $$\int \frac{1}{x} \, \mathrm{d}x = \ln|x| + C \label{1} \tag{1}$$ However, taking the indefinite integral as the set of all functions whose derivative is $\frac{1}{x}$ , the technically correct answer should be $$\int \frac{1}{x} \, \mathrm{d}x = \begin{cases} 
 \ln(x) + C & \text{if $x > 0$} \\  
 \ln(-x) + D & \text{if $x < 0$}  
 \end{cases} \label{2} \tag{2}$$ See Your calculus prof lied to you (probably) for more details. Is $\eqref{2}$ correct and is $\eqref{1}$ incorrect? Or are they both acceptable? I suppose $\eqref{2}$ is more correct but it perhaps doesn’t have any additional value compared to $\eqref{1}$ as the definite integral will give the same result for both (as the integral diverges if we move from the negative to positive $x$ -axis). Is there any other example where failing to define a piecewise function (with separate constants) as an integral can have serious consequences?","['integration', 'indefinite-integrals', 'calculus']"
4466958,"Let $f$ be a real-valued function on $[0,1]$ with a continuous second derivative. Assume that $f(0)=0, f'(0)= 1, f''(0)\neq 0$, and $0<f'(x)< $ ....","Let $f$ be a real-valued function on $[0,1]$ with a continuous second derivative. Assume that $f(0)=0, f'(0)= 1, f''(0)\neq 0$ , and $0<f'(x)<1 $ for all $x\in(0, 1]$ . Let $x_1, x_2,\ldots$ be a sequence with $0< x_1 \leq 1$ and with $$x_n = f\left(\dfrac{x_1+x_2+\cdots+x_{n-1}}{n-1}\right)$$ for $n \geq 2$ . Prove that $\displaystyle\lim_{n\to\infty}x_n\ln n = −2/f'' (0)$ . I tried to use fixed point theorem by converting it to form $x_n=\phi(x_{n+1})$ but it is not coming. Please help regarding this.",['real-analysis']
4466994,Complex function maps into lines and circles,"We have the following function: $$f(z) = \frac{z-1}{z-i}$$ What is $f$ of the real line (so $f(\mathbb{R}))$ ? What we did in the solutions was: $$f(\infty) = 1 \\
f(0) = 1/i = -i\\
f(1) =0\\$$ Then we said: $$|z-(a+bi)| = r\\
|1-a-bi| = r\implies (1-a)²+b²=r² \implies a = 1/2 \\
|-i-a-bi| = r\implies a²+(b+1)^2 = r²\implies b = -1/2\\
|a+bi| = r \implies\\
\implies a²+b² = r² \implies r = \sqrt{1/4 + 1/4} = \\
=\sqrt{2}/2 \implies |z-(1/2-i/2)| = \sqrt{2}/2\\$$ However I dont understand what we just did above. Can somebody explain what the procedure was and why. What would $f$ do to the imaginary line $\mathbb{I}$ or maybe the outside of the unit circle? I really need help with understanding the procedure for exercises like this one.","['complex-analysis', 'calculus', 'complex-numbers', 'mobius-transformation']"
4466998,Is it okay to say that $|{x}|=\sqrt{x^2}$ in a function?,"While analyzing a certain function I was asked to integrate $$\int \left|f'\left(x\right)\right|f\left(x\right)dx$$ Since I don't know how to integrate an absolute value function, I noticed that it could be re-written as $$\int \sqrt{f'\left(x\right)^2}f\left(x\right)dx$$ and evaluated that integral, which gave me the correct answer. My question is, can you do that for every function? And how is $\sqrt{f'\left(x\right)^2}$ different than $f'\left(x\right)$ ? It's supposed to be just $$\left(f\left(x\right)^2\right)^{\frac{1}{2}} = f\left(x\right)$$ Isn't it?",['algebra-precalculus']
4467007,Evaluating $\int_0^\infty \frac{\arctan (x^2) \ln(1+x^4)}{x(x^8+x^4+1)} \mathrm dx$,"So, my friend has challenged me to solve the following integral $\displaystyle \tag*{} I=\int_0^\infty  \frac{\arctan (x^2) \ln(1+x^4)}{x(x^8+x^4+1)} \mathrm dx$ I started by doing $x^2=t$ , so $\displaystyle \tag{1} I=\frac 12 \int_0^\infty  \frac{\arctan (x) \ln(1+x^2)}{x(x^4+x^2+1)} \mathrm dx$ Now, I first tried to solve this generalized integral: $\displaystyle \tag*{} \mathcal L(m,n,a) = \int_0^\infty \frac{\arctan(mx)\ln(1+n^2x^2)}{x(a^2+x^2)} \ \mathrm dx$ We have $\mathcal L(0,1,a)=0$ and $\mathcal L(1,0,a)=0$ . We now differentiate $\mathcal L(m,n,a)$ w.r.t $m$ first and then w.r.t $n$ . We get: $\displaystyle \tag{2} \begin{align} \partial m \ \partial n  \ \mathcal L(m,n,a) &= \int_0^\infty \frac{2nx^2}{(a^2+x^2)(1+m^2x^2)(1+n^2x^2)} \ \mathrm dx \\\\
 &= \frac{\pi n}{(m+n)(am+1)(an+1)} \\\\
 &= \frac{\pi}{2(am+1)(an+1)} , \ \ \text{via symmetry} \end{align}$ And now taking the double integeral gives $\displaystyle \tag{3} \mathcal L(1,1,a) = \int_0^1\int_0^1 \frac{\pi}{2(am+1)(an+1)} \ \mathrm dm \ \mathrm dn$ Using the elementary result: $\displaystyle \tag*{} \int_0^1 \frac{1}{ay+1} \ \mathrm dy = \frac{\log(a+1)}{a}$ We find the value of $(3)$ as $\displaystyle \tag*{} \mathcal L(1,1,a) = \frac{\pi \log^2(a+1)}{2a^2}$ Finally, we conclude that $\displaystyle \tag*{}\int_0^\infty \frac{\arctan(x)\ln(1+x^2)}{x(a^2+x^2)} \ \mathrm dx={\color{Red} { \frac{\pi \log^2(a+1)}{2a^2}}}:= {\color{Blue}{ f(a)}}$ Now by completing square and performing, we have: $\displaystyle \tag*{} \begin{align}\frac{1}{x^4+x^2+1} &=\frac{1}{\left(x^2+\frac 12\right)^2+\frac 34 }\\ &= \frac{1}{\left(x^2+\frac 12 - \frac{i \sqrt 3}{2}\right)\left(x^2+\frac 12 + \frac{i \sqrt 3}{2}\right)} \\ &= \frac{2}{i \sqrt 3} \left(\frac{1}{\left(x^2+\frac 12 - \frac{i \sqrt 3}{2}\right)}-\frac{1}{\left(x^2+\frac 12 + \frac{i \sqrt 3}{2}\right)}\right)\end{align} $ Adding all the pieces together, we get $\displaystyle \tag{4} I=\frac{1}{i \sqrt 3} \left(f\left(\sqrt{\frac{1-i\sqrt3}{2}}\right) - f\left(\sqrt{\frac{1+i\sqrt3}{2}}\right)\right)$ My question is to simplify $(4)$ (if all my steps are correct) and also is there any short ways (methods others than contour integration) to destroy this integral? Thanks :).","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'solution-verification']"
4467014,"Stone-Weierstrass theorem with ""nowhere-vanishing"" premise","Let $K$ be a compact space and $\mathscr A\subseteq C(K,\mathbb R)$ be a subalgebra. Let us assume the ""usual"" form of the real Stone-Weierstrass theorem: If $\mathscr A$ separates points in $K$ and contains a nonzero constant function , then it is uniformly dense in $C(K,\mathbb R)$ . Rudin proves this with weaker premises (hence, his version is stronger), namely, replacing the bold part with the requirement that the $\mathscr A$ vanish nowhere (baby Rudin, Theorem 7.32). I'd like to prove Rudin's stronger version, but in the fastest way possible, given that I've already proven the usual form given above. In particular, I only need to prove that $1\in\overline{\mathscr A}$ . I think this can be done without repeating all of Rudin's proof. Current progress/idea: Repeat the first part of Rudin's proof, which uses the Weierstrass theorem (for polynomials) to show that $f,g\in\overline{\mathscr A}\implies\max(f,g)\in\overline{\mathscr A}$ . The nowhere-vanishing condition can be used to find functions $f_x\in\mathscr A$ such that $f_x(x)=1.$ Using compactness of $K$ and closure of $\mathscr A$ under finite maximums, we can constrct a function $f\in\mathscr A$ such that $f(x)>1-\varepsilon$ for small $\varepsilon>0$ . Using some constructions like this and combining them appropriately, I imagine we can find a function that falls uniformly within $\varepsilon$ of $1$ , but so far I'm stuck here.","['weierstrass-approximation', 'functional-analysis', 'analysis', 'real-analysis']"
4467026,Creating my own change of variables to evaluate an integral,"The question asks me to evaluate the integral $$\iint_{R} e^{\frac{x+y}{x-y}} dA$$ where $R$ is trapezoid region with the vertices $(1,0), (2,0), (0,-2), (0,-1)$ . I'm supposed to suggest a possible transformation and integrate and sketch the two regions. My work : Let the transformation be $u=x-y$ , $v=x+y$ Then with some algebra, I get $x=\frac{u+v}{2}$ , and $y=-\frac{1}{2} (u-v)$ $J(u,v)=\begin{vmatrix}
\frac{1}{2} & \frac{1}{2}\\ 
-\frac{1}{2} & \frac{1}{2}
\end{vmatrix}=\frac{1}{2}$ When I sketch the region I have something like this on the xy plane On the uv plane the transformation looks like: So the integral becomes $$\int_{1}^{2}\int_{-u}^{u} e^{\frac{v}{u}}*\frac{1}{2} dv du$$ $$\frac{1}{2}\int_{1}^{2}u\Big(e-\frac{1}{e}\Big)du$$ $$=\frac{1}{2}\Big(e-\frac{1}{e}\Big)*\frac{3}{2}=\frac{3}{4}\Big(e-\frac{1}{e}\Big)$$ Does this look correct?","['multivariable-calculus', 'change-of-variable']"
4467032,$|F| \leq 2{n \choose \lfloor n/2 \rfloor}$ with $F \subset P([n])$,"Let $F \subset P([n])$ be a family that doesn't contain three different sets $A, B, C \in F$ with $A \subset B \subset C$ . I have to show that $|F| \leq 2{n  \choose \lfloor n/2 \rfloor}$ and that it's optimal for uneven $n$ . We could look at $|P([5])|= {5  \choose 0} + {5  \choose 1} +{5  \choose 2} +{5  \choose 3} +{5  \choose 4}  +{5  \choose 5} =1 + 5 + 10 + 10+5+1$ . ${n  \choose k}$ is symmetric, we will have $2$ biggest numbers for uneven $n$ and only $1$ biggest number for even $n$ . We have $\binom nk <  \binom n{k+1}$ for $n-k > k+1$ , it follows immediately that the maximum is in the middle, ${n  \choose \lfloor n/2 \rfloor}$ . It's clear to me that we will get the biggest family if we take the $2$ biggest sets but how exactly can I show that? It's also clear to me that we shouldn't take the empty set ${n  \choose 0}$ if we want the biggest family, the empty set is a subset of all sets; and similarly for ${n  \choose n}$ . But I don't know how to do the rest. And if we stick to our example we could also take like $\{1,2\},\{3,4,5\},\{1,2,3,4\},...$ , how do I show that something like this can't be a bigger family?","['combinatorics', 'extremal-combinatorics', 'discrete-mathematics']"
4467035,"Periodic solutions of $x'' = Ax$, with matrix $A$. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question $x'' = Ax$ . Let $A = \begin{pmatrix}
1 & 1\\
0 & \epsilon
\end{pmatrix}$ . Find $\epsilon$ such that it has periodic solution. I don't know what are the conditions that the matrix has to meet?","['matrices', 'ordinary-differential-equations']"
4467036,Calculate an integral using contour integration,"I have to calculate the following integral using contour integration: $$\int_0^1 \frac{dx}{(x+2)\sqrt[3]{x^2(x-1)}}$$ I've tried to solve this using the residue theorem, but I don't know how to calculate the residue of the function $$f(z) = \frac{1}{(z+2)\sqrt[3]{z^2(z-1)}}$$ Then I tried to make a substitution in the real integral, so that I would get a function whose residue I know how to calculate, but I couldn't figure out what substitution would do the trick. I would really appreciate if someone could help.","['integration', 'complex-analysis', 'contour-integration', 'residue-calculus']"
4467042,"$\sqrt {x^2} = |x|$ : how to relate $\sqrt {x^2}$ to the concept of ""distance to $x$ from $0$""?","A basic algbraic fact is that : $ \sqrt {x^2} = |x|$ . There must be a purely algebraic proof of this fact, involving only manipulation of symbols. But since absolute value has a geometric interpretation , it would be nice to have a geometric interpretation of the number $\sqrt {x^2}$ in terms of "" distance of number $x$ from $0$ "", in order to show not only the identity of denotation of the two expressions, but also their conceptual identity . How can this be achieved? Is it correct to say : the distance of a number from $0$ is the distance of the point $P=(x, 0)$ from the point $O=(0,0)$ , and then to apply the distance formula to point $P$ ? How to make this rigorous?","['analytic-geometry', 'algebra-precalculus', 'soft-question']"
4467050,Fitting exponential model to data,"I have a series of measurements of some value, about 10000 numbers. The numbers come timestamped in real-life clock seconds, at regular time intervals. All real-life measurements have errors, both systematic and random sensor noise, this one is no exception. An oversimplified mathematical model of the physical process says that that at least the middle portion of the data is probably described by the following differential equation: $$\frac{df(t)}{dt} = \alpha·S^{\beta}·f(t)^{\beta}$$ where $\alpha$ and $\beta$ are some unknown constants i.e. they don’t change over time (we also know $0 < \beta < 1$ ), and S is a known constant $10^{-5}$ . The formula basically says the time derivative of the function is proportional to some power of the functions’s current value. How can I find values of $\alpha$ and $\beta$ which would best fit (e.g. minimum squared error or similar) the measured experimental data of $f(t)$ function? Update: according to a symbolic math software, the above differential equation has the following solution: $$f(t) = {(t·\alpha·S^\beta·( 1 - \beta ) + C1)^{\frac{1}{1-\beta}} }$$ But the question remains, how to best fit that function to the measured data to get these constant numbers? Here's the source data","['numerical-methods', 'derivatives']"
4467055,Let $G$ be a group and let prime $p\mid \vert G\vert$. Suppose further that $\vert G\vert < p^2 $. Show that $G$ has a normal subgroup of order $p$.,"Suppose $G$ is a finite group and $p$ is a prime that divides $\vert G\vert$ . Suppose further that $\vert G\vert < p^2 $ . Show that $G$ has a normal subgroup of order $p$ . Attempt: We know there is an element of order $p$ by Cauchy's theorem. Say $x\in G$ has order $p$ . Let $H$ be the cyclic subgroup generated by $x$ .
My idea was to show that the only elements of order $p$ in $G$ are in $H$ and then, since conjugation preserves order of an element, $gHg^{-1} = H$ .
However I am finding this difficult to prove. I would appreciate some help as to whether this is a sensible idea or if I should be doing something else?","['normal-subgroups', 'group-theory', 'abstract-algebra', 'finite-groups']"
4467063,"If $A$ exists and $m$ is in $A$, then $m$ exists","I have a doubt. Let us consider two statements (considered true): There is a set $A$ . $m$ is an element of $A$ . Can we then ensure that $m$ also exists? My question arose when reading the book Introduction to Modern Set Theory (third edition), by Judith Roitman: Infinity Axiom: ∃x Ø ∈ x and x is inductive. And soon after: Proposition 28: ∃x x = Ø (The Proposition is proved on the basis of the given Axiom) Well... I think the Axiom already guarantees the existence of the set Ø. I think so, but I'm very unsure about it.","['elementary-set-theory', 'logic']"
4467075,Need help understanding solution for using change of variables to evaluate integral,"Use the change of variables $x=u^2-v^2$ , $y=2uv$ to evaluate $$\iint_{R}y dA$$ where $R$ is the region bounded by the x-axis, the parabolas $y^2=4-4x$ and $y^2=4+4x, y\geq0$ I'm following along with this solution: I don't understand why the preimage of the domain is $[0,1]\times [0,1]$ I understand how we get $u=\pm 1$ and $v=\pm 1$ but I don't know how the bounds are from $0$ to $1$ ? Why isn't it $$\int_{-1}^{1}\int_{-1}^{1} 2uv(4u^2+4v^2)dudv$$ EDIT: verify that there is some typo for the Jacobian, Because it should be $\begin{vmatrix}
2u & -2v \\
2v & 2u 
\end{vmatrix}=4(u^2+v^2)$","['multivariable-calculus', 'change-of-variable']"
4467108,Intermediate Algebra Quadratic Inequalities,"For what values of $a$ does the equation $$(a^{2}+2a)x^{2}+(3a)x+1=0$$ yield no real solutions $x$ ? Express your answer in interval notation. What I did so far: The only time a quadratic equation has no real solutions is when the discriminant is less than $0$ . In this case, the discriminant equals $9a^2-4a^2-8a$ , and that is less than $0$ . Simplifying, I got $5a^2-8a<0$ , so $a(5a-8)<0$ . I have $2$ cases: $$a<0$$ $$5a-8>0$$ and $$a>0$$ $$5a-8<0.$$ The first case is impossible because $a$ must be positive and negative at the same time. The second case says that $a>0$ and $a<\frac{8}{5}$ , which can be written as $(0,\frac{8}{5})$ in interval notation, but that answer wasn't correct. Could I get a little help? Thanks!","['algebra-precalculus', 'quadratics', 'inequality']"
4467124,Is this a sound line of reasoning to conclude that $\sqrt[n]{n!} \sim \frac{n}{e}$?,"In the paper Decomposable Searching Problems I. Static-to-Dynamic Transformations by Bentley and Saxe, the authors state without proof that $$\sqrt[n]{n!} \sim {\frac{n}{e}}\text.$$ I have a line of reasoning below that I think correctly proves this, but I'm a bit shaky about whether I can apply tilde approximations this way. Is this reasoning correct? Using Stirling's approximation, we know that $$n! \sim \left(\frac{n}{e}\right)^n \sqrt{2\pi n}\text.$$ Taking $n$ th roots then gives $$\sqrt[n]{n!} \sim \left( \frac{n}{e} \right) \sqrt[2n]{2 \pi n} \sim \frac{n}{e}\text.$$ The step I'm uncomfortable about is whether I can take $n$ th roots of both sides of the tilde expression. This feels like it ""ought"" to work, but I've been surprised by tilde expressions in the past and wouldn't want to bet the farm on it. :-)","['factorial', 'asymptotics', 'discrete-mathematics', 'roots']"
4467148,Should we always regard a $1\times 1$ matrix as a scalar?,"Should we always regard a $1\times 1$ matrix as a scalar? (I think, ""yes"".) And if so, how should we address this in our elementaty Linear Algebra courses?
Let me give an example to illustrate my question. Suppose $A= \left[\begin{array}{rr}1 & 2 \\-2 & 1\end{array}\right],$ $B = \bigl[-2\,,2\bigr],$ and
and $C = \left[\begin{array}{r}4 \\5\end{array}\right].$ Then the calculation \begin{equation}  A(BC) = \left[\begin{array}{rr}1 & 2 \\-2 & 1\end{array}\right]
  \left(\begin{array}{r} \bigl[-2\,,2\bigr] \\ \rule{1pt}{0pt} \end{array}\left[\begin{array}{r}4 \\5\end{array}\right] \right) 
   = \left[\begin{array}{rr}1 & 2 \\-2 & 1\end{array}\right]\cdot 2 = \left[\begin{array}{rr}2 & 4 \\-4 & 2\end{array}\right]
\end{equation} seems completely reasonable, doesn't it? And yet it's technically incorrect, since $A$ is a $2\times 2$ matrix and $BC$ is a $1\times 1$ matrix. I've ben trying to come up with a technically correct way to conclude that $A(BC)$ can indeed by computed as above. And here's the best I can
come up with. There's an obvious bijection, let's call it $J$ , from the the $1\times 1$ matrices to the scalars, with $x = J([x])$ for
any scalar $x$ . If we want to be able to carry out `` $A(BC)$ '' as above, what we really mean is that it is equal to $AJ\bigl([BC]).$ But someone could ask how we know when it's appropriate to interpret $A(BC)$ as $2A$ and when it's
appropriate to interpret that product as undefined. My own answer is that it depends on context or something, but that seems unsatifying to me. Does anyone know of a good way to address this matter, which is both rigorous at the foundational level and can easily be inserted into an
elementary discussion? For example, when we define multiplication of two matrices, should we add a caveat that any $1\times 1$ matrix should be
be regard as a scalar? But then, is there ever a situation where we want to regard a $1\times 1$ matrix as just that, and calling it scalar would
mess something else up at the level of foundations/definition? Thanks in advance. -JGW","['matrices', 'linear-algebra']"
4467170,Help to show that f is not differentiable.,"Let $ 0 < \alpha < 1 $ . If $\vert f(x)\vert \geq \vert x\vert^\alpha$ for all $x$ and $f(0) =0$ , then $f$ is not differentiable at $x=0$ . I would appreciate some help to prove this. Edit : I supposed that $f$ is differentiable at 0 and used the definition of derivative as a limit. Since $f(0)=0$ , I get $ \lim_{h \to 0} \frac{f(h)}{h} \geq  \lim_{h \to 0} \frac{1}{h^{1- \alpha}} $ by hypothesis and the fact that limit preserves inequality. Since the second limit goes to infinity, then the first limit goes to infinity too, and this is a contradiction with the fact that $f’(0)$ exist. Its that right?","['calculus', 'derivatives']"
4467189,Are there any non-compact topologies such that every real-valued continuous function has a maximum value?,"I asked my topology teacher this and she didn't know.
Does there exist a non-compact topology $E$ such that for every continuous function $f : E \to \mathbb{R}$ , $\text{max}(f(E))$ exists?",['general-topology']
4467211,Triangulation Theorem for semialgebraic maps,"Benedetti and Risler's ""Real algebraic and Semi-algebraic sets"" book on Semialgebraic Geometry has the following theorem: Theorem 2.6.14 Let $f:V \to Y$ be a continuous semialgebraic mapping defined on the compact semialgebraic set $V \subset \mathbb{R}^{n}$ onto $Y \subset \mathbb{R}^{m}$ . Consider its graph $X \subset \mathbb{R}^{m+n}$ . Set $s = m+n$ , let $(x_{1},...,x_{m},y_{1},...,y_{n}) = (x,y)$ be the coordinates of $\mathbb{R}^{s}$ with $$\mathbb{R}^{s} \supseteq \mathbb{R}^{s-1}  = \{y_{n}=0\} \supseteq \mathbb{R}^{s-2} = \{y_{n-1} = y_{n} = 0\} \supseteq \cdots \supseteq \mathbb{R}^{m}$$ and let $\pi_{j}: \mathbb{R}^{j+1} \to \mathbb{R}^{j}$ , for $j=s-1,...,m$ . Assume that, for every $j$ , all the polynomials involved in some representation of $X_{j} = \pi_{j} \circ \pi_{j+1} \circ ... \circ \pi_{s-1}(X) (X_{s} = X, X_{m} = Y)$ have constant leading coefficients with respect to the indeterminate $Y_{j}$ . Then there exists a triangulation $G:Y \to |H|$ , a triangulation $F:X \to |K|$ and a simplicial map $f':|K| \to |H|$ such that $F$ and $G$ satisfy the conditions of the Triangulation Theorem $G \circ f = f' \circ F$ . There are some questions here. The first is, $f$ is defined on $V$ , and not on $X$ . So either the $f$ on item 2 is the natural projection, or the triangulation is of $V$ , not $X$ . In the first case, the theorem is immediate, since $X$ is compact (because $V$ is), and thus there is a triangulation of $X$ that is compatible with the projections. In particular, the projections of the vertices of the polyhedron have vertices as images, and thus the natural projection is simplicial. But is doesn't seem to be the case here, because then we wouldn't need the hypothesis about the polynomials involved in some representation of $X$ . I can't understand why this hypothesis is necessary, and how it could be used in a proof. The book does not prove it. Instead, it says right after proving the Triangulation Theorem for compact semialgebraic sets that ""by the way, we actually proved this theorem"" and states it. The book even has a example to show why it is necessary, which I'll write here(by the way, all simplices considered in this question are open): ""Set $V = \mathbb{S}^{2}$ , $S = V \cap \{x=0\}, f:V \to \mathbb{R}^{4}, (x,y,z) \mapsto x^{2}(x,y,z,1), W=f(V)$ . Note that (a) $f$ is an analytic isomorphism of $V\setminus S$ onto $W\setminus \{0\}$ (b) $f(S) = 0$ That is, $f$ ""collapses"" $S$ onto the point $0 \in \mathbb{R}^{4}$ . We claim that $f$ cannot be made simplicial as in Theorem 2.6.14. In fact, any such triangulation of $V$ should have $S$ as a subpolyhedron, and there would exist at least one $2$ -simplex $T$ of $V$ with exactly one $1$ -face $T'$ contained in $S$ . The image of $T$ via $f$ cannot be a simplex of $W$ "" As for why the image of $T$ cannot be a simplex, let $<a,b>$ be the $T'$ , and $v$ the other vertex of $T$ . Since $f$ collapses $S$ onto $0$ , $f(a) = f(b)$ , but $f(T)$ has dimension $2$ , thus the resulting set is not a simplex. I appreciate any help on the understanding of this theorem. I'll put the Triangulation Theorem for compact semialgebraic subsets for reference: Triangulation Theorem : Let $X$ be a compact semialgebraic subset of $\mathbb{R}^{n}$ , and $X_{1},...,X_{h}$ a finite semialgebraic partition of $X$ . There exists a triangulation $F:X \to |K|$ such that: Each $X_{i}$ equals the union of some $F^{-1}(T_{j})$ , where $T_{j}$ are simplices of $|K|$ . The collection of sets of the form $F^{-1}(T_{j})$ is a stratification of $X$ The restriction of $F$ to $F^{-1}(T_{j})$ is a real analytic isomorphism of analytic manifolds.","['algebraic-geometry', 'real-algebraic-geometry', 'semialgebraic-geometry']"
4467227,Is every Zariski-closed real matrix Lie group the joint stabilizer of some list of mixed tensors?,"Consider a finite-dimensional real vector space $V$ ,
and an embedded real Lie subgroup $G \subset \mathrm{GL}_\mathbb{R}(V)$ . In what follows, $V^*$ denotes the real dual vector space of $V$ . Def: Let us say $G$ has property $\mathcal{P}$ if there exist some finitely many $\alpha_1,\ldots,\alpha_k$ in the mixed real tensor algebra $TV$ (including tensor factors of both $V$ and $V^*$ ) of $V$ ,
such that $G= \bigcap_{j=1}^{k}\mathrm{Stab}_{\mathrm{GL}_{\mathbb{R}}(V)}(\alpha_j)$ .
Here we use the induced action of $\mathrm{GL}(V)$ on the mixed tensor algebra $TV$ . My question is whether every Zariski-closed Lie subgroup of $\mathrm{GL}_{\mathbb{R}}(V)$ has property $\mathcal{P}$ ?
(Here we use the Zariski topology on $\mathrm{End}_{\mathbb{R}}(V)$ coming from real polynomials, e.g. from a choice of basis, or just in the multilinear sense.) Examples having property $\mathcal{P}$ : For $V = \mathbb{R}^n$ , for $G = \mathrm{O}(n,\mathbb{R})$ ,
we can use $\alpha \in V^* \otimes V^*$ given by the standard real-bilinear inner product. For $V = \mathbb{R}^n$ , for $G = \mathrm{SL}(n,\mathbb{R})$ ,
we can use $\alpha \in V^{\otimes n}$ the standard volume form. For $V = \mathbb{R}^{2n}$ , for $G = \mathrm{GL}(n,\mathbb{C}) \subset \mathrm{GL}(2n,\mathbb{R})$ ,
we can use $\alpha = J = \left(\begin{smallmatrix} 0 & -1 \\ 1 & 0 \end{smallmatrix}\right)  \in V \otimes_\mathbb{R} V^*$ . For $V = \mathbb{R}^{2n}$ , for $G = U(n)$ ,
if I'm not mistaken, we can use both $\alpha = J = \left(\begin{smallmatrix} 0 & -1 \\ 1 & 0 \end{smallmatrix}\right)  \in V \otimes_\mathbb{R} V^*$ ,
and the $\beta_1,\beta_2 \in V^* \otimes_\mathbb{R} V^*
$ which correspond to the real and imaginary parts of the standard Hermitian inner product $\mathbb{C}^{n} \times \mathbb{C}^{n} \rightarrow \mathbb{C}$ but treated as a real-bilinear map. For $V = \mathbb{R}^n$ and $G = \mathbb{R}^\times 1 \subset \mathrm{GL}(n,\mathbb{R})$ the nonzero multiples of the identity,
we should be able to take any $\alpha_1,\ldots,\alpha_{n^2} \in V \otimes_\mathbb{R} V^* \simeq \mathrm{M}(n,\mathbb{R})$ such that the $\alpha$ 's form an $\mathbb{R}$ -linear basis of $\mathrm{M}(n,\mathbb{R})$ . (This uses the fact that only the scalar multiples of the identity commute with all other square matrices.) Lemma: If $G$ has property $\mathcal{P}$ , then $G \subset \mathrm{GL}_{\mathbb{R}}(V)$ is Zariski-closed;
i.e. it is the intersection of $\mathrm{GL}_{\mathbb{R}}(V)$ with a Zariski-closed subset (a real polynomial-vanishing subset) of $\mathrm{End}_{\mathbb{R}}(V)$ . Proof: After picking bases, each ""stabilizing equation"" $g \cdot \alpha_j = \alpha_j$ can be converted into a polynomial equation in the entries of $g$ and $g^{-1}$ , and the $g^{-1}$ factors can be multiplied out to get a polynomial in just $g$ 's entries;
thus $G$ is the vanishing set in $\mathrm{GL}_{\mathbb{R}}(V)$ of a collection of real polynomials of $\mathrm{End}_{\mathbb{R}}(V)$ . Rmk:
For instance, as pointed out in the comments below, $\mathrm{GL}_+(n,\mathbb{R})$ is not Zariski-closed in $\mathrm{GL}_(n,\mathbb{R})$ , hence $\mathrm{GL}_+(n,\mathbb{R})$ cannot have property $\mathcal{P}$ . My question is: does every Zariski-closed, real Lie subgroup of $\mathrm{GL}_{\mathbb{R}}(V)$ have property $\mathcal{P}$ ? (I've updated the question to reflect the suggestions noticing that every $G$ having property $\mathcal{P}$ is Zariski-closed; now I'm wondering about the converse.)","['representation-theory', 'matrices', 'algebraic-geometry', 'linear-algebra', 'lie-groups']"
4467244,"Is this type of ""definable bijective solution"" of combinatorial identities symmetric?","Say that a combinatorial problem is a pair of first-order sentences $\langle\varphi,\psi\rangle$ in (disjoint and relational, for simplicity) finite languages $\Sigma=\{R_1,...,R_l\},\Pi=\{S_1,...,S_m\}$ respectively such that for each finite set $X$ , the sets $Mod_X(\varphi),Mod_X(\psi)$ of isomorphism types of models of $\varphi,\psi$ respectively with domain $X$ have the same cardinality. (The components of a combinatorial problem are just particularly simple combinatorial species .) I'm interested in the class of combinatorial problems $\langle\varphi,\psi\rangle$ for which we can, in a first-order-definable way, establish a bijection between $\varphi$ -models and $\psi$ -models (on a fixed finite set) possibly with the additional help of some auxiliary structure on that set. Formally, say that $\langle\varphi,\psi\rangle$ is definably solvable iff there is a language $\Gamma=\{T_1,...,T_n\}$ disjoint from both $\Sigma$ and $\Pi$ , a first-order $\Gamma$ -sentence $\lambda$ , and a tuple of $\mathsf{FOL}[\Sigma\cup\Gamma]$ -formulas $(\theta_i)_{1\le i\le m}$ such that the following hold for every finite set $X$ : Whenever $Mod_X(\varphi)\not=\emptyset$ we also have $Mod_X(\lambda)\not=\emptyset$ . For each $1\le i\le m$ , $\theta_i$ is a formula with the same arity (in terms of free variables) as the symbol $S_i$ . If $Mod_X(\varphi)\not=\emptyset$ , $\mathcal{X}$ is a $\Sigma\cup\Gamma$ -structure with domain $X$ such that $\mathcal{X}\models\lambda\wedge\varphi$ , then: The structure $(X;(\theta_i^\mathcal{X})_{1\le i\le m})$ is a model of $\psi$ . For every $\Sigma\cup\Gamma$ -structure $\mathcal{Y}\models\lambda\wedge\varphi$ with domain $\mathcal{X}$ , we have $$\mathcal{X}\upharpoonright\Sigma\cong\mathcal{Y}\upharpoonright\Sigma\quad\iff\quad(X;(\theta_i^\mathcal{X})_{1\le i\le m})\cong (X;(\theta_i^\mathcal{Y})_{1\le i\le m}).$$ For example, with $E$ a binary relation symbol let $\pi_2$ = "" $E$ is a partition of the domain into pairs or singletons"" and let $\pi^2$ = "" $E$ is a partition of the domain into two pieces."" This question sketches why the combinatorial problem $\langle\pi_2,\pi^2\rangle$ is definably solvable. With a bit of tedium, one can show that $\langle\pi^2,\pi_2\rangle$ is also definably solvable. I'm curious whether this always holds: If $\langle\varphi,\psi\rangle$ is a definably solvable combinatorial problem, must $\langle\psi,\varphi\rangle$ also be definably solvable? (I would also be interested in results for the analogous notion defined with some not-too-strong logic $\mathcal{L}$ in place of first-order logic. Even second-order logic isn't so strong, at first glance, as to trivialize the situation! But my main focus is on $\mathsf{FOL}$ .)","['model-theory', 'logic', 'combinatorics']"
4467257,"If $q_1<q_2\in(0,\frac{1}{2})\subset\mathbb{Q}$, then $f(q_1,q_2)=\cos(q_1\pi)-\cos(q_2\pi)$ is bijective between the domain and the image set?","When I was studying on a conjecture, I noticed this problem: If $q_1<q_2\in(0,\frac{1}{2})\subset\mathbb{Q}$ , then $f(q_1,q_2)=\cos(q_1\pi)-\cos(q_2\pi)$ is bijective between the domain and the image set? I find the only counter example with the help of wolfram mathematica: $$
\cos \left(\frac{2 \pi }{7}\right)-\cos \left(\frac{3 \pi }{7}\right)=\cos \left(\frac{\pi }{7}\right)-\cos \left(\frac{\pi }{3}\right)
$$ It is confused why there has the only counter example. This reflects a somehow special property of trigonometric functions, because it does not always hold in other functions, for example: If $q_1<q_2\in\mathbb{Q}$ , then $f(q_1,q_2)=q_2^2-q_1^2$ is not bijective because there are counter examples : $25^2-20^2=39^2-36^2$ and so on. More details: I find another conjectures with the help of wolfram mathematica. $\textbf{Conjecture 1.}$ If integer $n\geq 9,n=2^a p^b, a,b \geq 0$ , p is a prime and $a\geq2$ if $b=1$ , then when there are linear relationships in $\left\{ \cos(\frac{k\pi}{n}):k\in \left[1,\left\lfloor \frac{n-1}{2}\right\rfloor \right]\subset \mathbb{Z}\right\}$ , the number of $\cos(\frac{k\pi}{n})$ in each set of linear relationships is $p$ . For example, if $n = 40$ , we have $$
\cos \left(\frac{3 \pi }{8}\right)+\cos \left(\frac{\pi }{40}\right)-\cos \left(\frac{7 \pi }{40}\right)-\cos \left(\frac{9 \pi }{40}\right)+\cos \left(\frac{17 \pi }{40}\right)=0\\
-\cos \left(\frac{\pi }{4}\right)+\cos \left(\frac{\pi }{20}\right)-\cos \left(\frac{3 \pi }{20}\right)+\cos \left(\frac{7 \pi }{20}\right)+\cos \left(\frac{9 \pi }{20}\right)=0\\
-\cos \left(\frac{\pi }{8}\right)+\cos \left(\frac{3 \pi }{40}\right)-\cos \left(\frac{11 \pi }{40}\right)+\cos \left(\frac{13 \pi }{40}\right)+\cos \left(\frac{19 \pi }{40}\right)=0
$$ each set of linear relationships is $5$ , $5$ is the largest prime factor of $40$ . I'm just an undergraduate student at the university and I don't have any idea to prove these two conjectures. It would be very useful if someone could give a counter example or provide some ideas for proof.","['functions', 'rational-numbers']"
4467312,2D Random Walk or just simple math?,"I'm looking into a physical problem that involves the following sum: $$r(t)=\sum_{\substack{z_l=0,1\\1\leq l \leq N}}|b_{z_1z_2\dots z_N}|^2e^{-2i\epsilon_{z_1z_2\cdots z_N}t}$$ where $\epsilon_{z_1z_2\cdots z_N}$ are random phases and $\sum |b_{z_1z_2\dots z_N}|^2=1$ , where the $b$ 's are normally distributed. By trying to say something about it, one could look at it as a 2D random walk problem with fixed total lenth (sum of all step lengths). Apparently that yields $$\langle|r(t)|^2|\rangle\propto \langle |b_{z_1z_2\cdots z_N}|^2\rangle=2^{-N}\tag{1}$$ Is it really necessary to look at it this way? Isn't it just a simple calculation involving some calculation rules of $\langle \cdot\rangle$ ? For further information on the physics behind this problem, see the book from Schlosshauer on Decoherence: 978-3-540-35775-9 (p. 91). I myself tried to do it with ""simple math"" although couldn't make it: I calculated $$\mathbb{E}(|r(t)|^2)=\mathbb{E}\left(\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}|b_{z_1z_2\dots z_N}|^2e^{-2i\epsilon_{z_1z_2\cdots z_N}t}\right)\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}|b_{z_1z_2\dots z_N}|^2e^{2i\epsilon_{z_1z_2\cdots z_N}t}\right)\right)=\left(\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}\mathbb{E}(|b_{z_1z_2\dots z_N}|^2)\mathbb{E}(e^{-2i\epsilon_{z_1z_2\cdots z_N}t})\right)\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}\mathbb{E}(|b_{z_1z_2\dots z_N}|^2)\mathbb{E}(e^{2i\epsilon_{z_1z_2\cdots z_N}t})\right)\right)=2^{-N}\cdot 2^N \mathbb{E}(e^{-2i\epsilon_{z_1z_2\cdots z_N}t}) \cdot 2^{-N}\cdot 2^N \mathbb{E}(e^{2i\epsilon_{z_1z_2\cdots z_N}t})$$ with $$\mathbb{E}(e^{-2i\epsilon_{z_1z_2\cdots z_N}t})$$ the characteristic function of a random variable $\epsilon$ with (continuous) unform distribution on $[0,2\pi]$ (where no $N$ occurs). As you can see, the factor $N$ is not in the final formula, although $(1)$ says so. Am I making a mistake? This question has been cross-posted on PSE .","['random-walk', 'expected-value', 'stochastic-processes', 'physics', 'probability']"
4467326,"Will $h(x)=\frac{ax+b}{cx+d}$, where $c\neq0$, always satisfy $h(h\cdots(h(x))\cdots)=x$ in a finite number of iterations?","Consider function $f(x) = \frac{x-5}{x-3}$ , after four iterations we arrived at that $f(f(f(f(x))))) = x$ , likewise similarily for $g(x) = \frac{1}{1-x}$ we have $g(g(g(x))) = x$ . So, is it always possible that the function of form $$h(x) = \frac{ax+b}{cx+d}$$ where $c \neq 0$ will satisfy $h(h\cdots(h(x))\cdots) = x$ in a finite number of iterations?","['algebra-precalculus', 'functions']"
4467357,Rank of limit points of a conjugacy class,"Problem: Let $t \to P_t$ be a one parameter subgroup $\mathbb{C}^* \to \text{Gl}_{n}(\mathbb{C})$ . Let $X$ be a $n \times n$ nilpotent matrix. I want to show that if $lim_{t \to 0} P_t^{-1}XP_t = Y$ then $\text{rank}(X) \geq \text{rank}(Y)$ . I have checked that this is true if $X$ is in the Jordan canonical form and the one parameter subgroups are in diagonal form. Firstly in this case  we can assume that $X$ is indecomposable(hence rank is n-1) and so only one jordan block(since the actions are blockwise due to the nature of $P_t$ ), then if $P_t(i, i) = t^{i}$ , then the $Y$ as defined above has $t^{i_k +i_{k+1}}$ on the super diagonal entries and $0$ elsewhere. So, the rank is at most $n-1$ . But for an arbitrary nilpotent matrix I don't know how to show. We can make one simplification: if $Z$ is a nilpotent matrix then $Z  = A^{-1}XA$ where $X$ is in Jordan canonical form, but then we cannot assume that $P_t$ are diagonal and so on... So, either we prove that given $X$ in Jordan Canonical form and $P_t$ in arbitrary form.. or $X$ in arbitrary form and $P_t$ in special form that the rank cannot increase on taking limits. I am interested in this question since combined with fact that any point in the Zariski closure of a conjugacy class  in the affine variety of these nilpotent matrices is captured by a one parameter subgroup If $Y$ is in closure of $X$ then, $Y^2$ is in closure of $X^2$ and so on.... Partition(there is one partition corresponding ot every class) of $X$ majorizes partition of $Y$ iff $rank^{i}(X) \geq rank^{i}(Y)$ we would have shown that if $Y$ is in the closure of the conjugacy class of $X$ then the partition corresponding to $X$ majorizes that of $Y$ . I have shown the converse using one parameter subgroups and block matrix identitities here","['integer-partitions', 'algebraic-groups', 'matrices', 'nilpotence', 'affine-varieties']"
4467461,When does the square root of $f:\mathbb{N} \rightarrow \mathbb{N}$ exist? [duplicate],"This question already has an answer here : Characterising functions $f$ that can be written as $f = g \circ g$? (1 answer) Closed 8 months ago . Let $f:\mathbb{N} \rightarrow \mathbb{N}$ . When does there exist a $g: \mathbb{N}\rightarrow \mathbb{N}$ such that $f(n)=g(g(n))$ for all $n$ ?. I don't think its always possible, for example $f(n)=n+1$ , doesn't seem likely that $g$ exists. The problem could be easier if we say $f$ is injective. Then $g$ must be injective. I didn't find this problem anywhere so I don't have a reference, I just thought of it. Im hoping dealing with $\mathbb{N}$ will give a nice answer. The problem has a nice answer for invertible matrices.","['contest-math', 'analysis', 'functions', 'natural-numbers', 'algebra-precalculus']"
4467474,Transform the partial differential equation $(y-z)\frac{∂z}{∂x}+(y+z)\frac{dz}{dy}=0$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question Transform the P.D.E. $(y-z)\frac{\partial z}{\partial x}+(y+z)\frac{\partial z}{\partial y}=0$ so that the new equation contains $x$ as a new function, and $u=y-z, v=y+z$ are new independent variables.","['partial-derivative', 'multivariable-calculus', 'transformation', 'partial-differential-equations']"
4467475,Calculate double integral $\iint_D\frac{\sqrt{x^2+y^2 - a^2}x}{(x^2+y^2)^2}dxdy$ over unbounded region $D$,"Calculate $$I = \iint\limits_D\frac{\sqrt{x^2+y^2 - a^2}x}{(x^2+y^2)^2}\,dx\,dy$$ where the region of integration is: $$D = \{(x,y) \in \mathbb{R}^2 \mid x+y - a\sqrt{2} \geq 0, -x+y +a\sqrt{2} \geq 0, a > 0\}$$ I tried using polar coordinates, but then, I don't know how to proceed because of the region $D$ and of the integrand: $$I = \iint\limits_D\frac{\sqrt{r^2-a^2}\cos\theta}{r^2}\,dr\,d\theta$$","['integration', 'improper-integrals', 'multivariable-calculus', 'definite-integrals']"
4467497,Why the definition of smooth boundary defined like this and how does it imply the interior ball property?,"I am trying to understand what the definition of smooth boundary is. From the following lectures notes on analysis 3 : So intuitively, a smooth boundary ( in this case , it is a $C^1$ boundary) should have no edges and corners. So I think condition $b$ takes care of that. But why do we have condition $a$ ? Also why does having a smooth boundary imply $\Omega$ has the interior ball property?
The interior ball property is : for each $x\in \partial \Omega$ , there exists a ball $B \subset \Omega$ such that $x\in \partial B$ . Intuitively, this seems like it should be true, but I don't see how we get from the definition of smooth bondary to this.","['submanifold', 'analysis', 'real-analysis']"
4467505,Hint for solving differential equation,"In $\text{Shimer (2012)}^1$ the author describes the evolution of the unemployment rate as follows: $$\dot{u}_{t+\tau} = \dot{u}_t^s(\tau)-\left(u_{t+\tau}-u_t^s(\tau)\right)f_t,$$ where $f_t = -log(1-F_t \geq 0 \dots$ arrival rate of a Poisson process, $F_t \dots$ job finding probability, $u_t^s(\tau)\dots$ short term unemployment rate, $\tau \dots$ time between passed between two measurement points in a panel data set with monthly interviews. With $u_t^s(0)=0$ as an initial condition we should be able to come up for a solution for $u_{t+1}$ and $u^s_{t+1} \equiv u^s_t(1)$ : $$u_{t+1}=(1-F_t)u_t + u^s_{t+1}.$$ Trying to get there on my own, I struggle to find a proper solution approach for this kind of differential equations, since this resembles nothing of what I've learnt so far. I would appreciate if you could hint me towards the right direction so that I may solve this problem eventually. $^1 \text{Shimer, Robert (2012) Reassessing the ins and outs of unemployment, doi:10.1016/j.red.2012.02.001}$",['ordinary-differential-equations']
4467506,Does this two equation system have a solution for l(1)?,"I'm having a debate with a friend over this problem: $$\eqalign{
k(x) &= l(x)+8 \cr
l(x) &= 14-k(x)
}$$ What is the value of $l(1)$ ? I think this isn't solvable because of the circular reference/dependency, but my friend thinks that it can be done with substitution, resulting in constant answers: $$\eqalign{
l(x)&=14-[l(x)+8] \cr
2l(x)&=6 \cr
l(x)&=3
}$$ which leads to $$\eqalign{
k(x)&=3+8 \cr
k(x)&=11
}$$ Thoughts?","['systems-of-equations', 'functions']"
4467520,"Calculating the index of a subgroup of ${\rm PSL}(2,\mathbb{Z})$","Suppose I have a finitely generated subgroup of $G:={\rm PSL}(2,\mathbb{Z})$ defined by $H=\langle A,B,C\rangle$ where $$A = \begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix},\quad
B = \begin{bmatrix}
-1 & 0 \\
3 & -1
\end{bmatrix},\quad
C = \begin{bmatrix}
-5 & 4 \\
6 & -5
\end{bmatrix}.$$ I have reason to believe this subgroup has finite index in $G$ , and I'm trying to calculate it. Following a suggestion the top answer of this MO post , I first tried calculating the intersection of $H$ with $P\Gamma(2)$ , where $P\Gamma(2)$ are those matrices which are congruent to the identity matrix when we reduce elements mod $2$ . After some work I was able to show that $A^2=-I, B^2, C, (AB)^3, (BA)^3\in H\cap P\Gamma(2)$ . I think(?) these and their inverses are the smallest length words with this property. Does that mean $H\cap P\Gamma(2) = \langle B^2, C, (AB)^3, (BA)^3\rangle$ ? I'm not sure what else to do to find the intersection. After that, the answer seems to suggest rewriting the generators of the intersection in terms of the standard generators of $P\Gamma(2)$ , say $$x = \begin{bmatrix}
1 & 2 \\
0 & 1
\end{bmatrix}, \quad
y = \begin{bmatrix}
1 & 0\\
-2 & 1
\end{bmatrix}.$$ It was indicated in the MO post that GAP could somehow rewrite the intersection in terms of these generators, and this would somehow be useful information. There was also a comment suggesting the Nielson-Schreier Index Formula could be used. Can someone spell out this argument in greater detail for me? Won't we just fine that the intersection is rank 4 and has index 3 in $P\Gamma(2)$ ? How does one find the index of $H$ in the entirety of ${\rm PSL}(2,\mathbb{Z})$ from there? I feel like I am missing something. One last note: I am aware that one can use Fuchsian group methods to calculate the index as well, but I am more interested in understanding the method proposed in the MO thread (or some other algebraic method, if anyone has something better).","['matrices', 'modular-forms', 'group-theory', 'abstract-algebra']"
4467522,Lie Derivative weird form,"I am reading a physics paper, in which the Lie derivative is presented in this strange (to me) way: $$\mathcal{L}_Y=Y^A\partial_A+\frac{i}{2}D_AY_BS^{AB}$$ where $A,B=\{z,\bar{z}\}$ with $\{z,\bar{z}\}$ being the coordinates on the complex sphere (i.e. connected to the usual polar/azimuthal coordinates via $z=e^{i\phi}\tan\frac{\theta}{2}$ ) and $S_{AB}$ is the pullback of the spin operator on the complex two sphere. The antisymmetric spin operator is given by $$S_{12}=\frac{h(1-z\bar{z})}{1+z\bar{z}},\ S_{13}=\frac{ih(z-\bar{z})}{1+z\bar{z}},\ S_{23}=\frac{h(z+\bar{z})}{1+z\bar{z}}$$ where the indices run from zero to three. Can someone shed some light on how do we manage to write the Lie derivative without the tensor it is supposed to be acted upon and on how can one write it in terms of the pullback of some operator?? I do not know if my question makes sense, but basically I can not understand why the Lie derivative is given in the form it is given. I have taken a geometry of GR class, so I am familiar with the basic definitions and with finding the components of the Lie derivative, when acted upon an arbitrary tensor. Thanks a lot.","['lie-derivative', 'general-relativity', 'riemannian-geometry', 'differential-geometry']"
4467543,What is the reason that isomorphisms between vector spaces are invertible?,"I ask this soft-question because I am confused about the intuition of the mechanics of proving something is an isomorphism: My professor gives the following steps to check for isomorphism: The function is one-to-one The function is onto The function preserves addition The function preserves multiplication However, I looked up another video to further clarify the intuition behind isomorphism, and the video states: ""Suppose V and W are vector spaces over the same field. We say that $V$ and $W$ are isomorphic if there exists an invertible linear transformation $T: V\to W$ "" I think I understand why the preservation of addition and multiplication fits into the ""invertibility"" as they are commutative. But I don't know how the ""1-to-1 and onto (i.e., bijection)"" contribute to invertibility as defined by the video? Thanks!","['vector-space-isomorphism', 'soft-question', 'linear-algebra', 'inverse']"
4467546,How do we make sense of normal derivatives in PDE at boundary points?,"In PDE, the normal derivative on a boundary comes up a lot (defined to be the $<\nabla u, v>$ where $v$ is the unit normal vector of our domain. I want to ask how exactly do we define the gradient on boundary points because plenty of textbooks for analysis in $R^n$ explicitly state that we need open sets for partial derivatives to make sense. Is there some adapted definition for derivatives for boundary points? Now closely related to PDE and normal derivatives is Hopf lemma which I was just reading about: https://en.wikipedia.org/wiki/Hopf_lemma It states that if a harmonic functions assumes a strict maximum on a boundary point, then its normal derivative must also be strictly positive. Now if the point was an interior point, then we know the gradient must be zero since it is a extrema, but since we have boundary point, this is not the case. So intuitively, are we suppose to imagine that our domain is actually part of some larger space and our function is actually increasing in the direction of the normal at this point in this larger space?","['derivatives', 'analysis', 'partial-differential-equations']"
4467549,How to prove the following inequality between $u$ and integral of its gradient,"I'm reading a paper and it states that the following result is well known.
I can't prove it myself but would like to see a proof before I continue the reading, can anybody help? $\text{Let}\ u\in C_0^{\infty}(B_R(x))\ \text{where}\ B_R(x)\ \text{is a ball of radius}\ R\ \text{centered at}\ x\ \text{then}$ $$|u(x)| \leq c \int_{B_{R}(x)} \frac{|\nabla u(y)|}{|x-y|^{n-1}} d y$$","['multivariable-calculus', 'analysis']"
4467556,Express Two way ANOVA with interaction as matrix form,"Consider a two-way ANOVA design with interaction: $y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk}$ , $i=1,...,I$ , $j=1,...,J$ , $k=1,...,n$ , where $\epsilon_{ijk} \sim N(0,\sigma^2)$ are independent. I am trying to find out appropriate expressions for $A_I$ and $A_M$ from $$
y^T(I - n^{-1}J)y = y^T(I-H)y + y^T A_I y + y^T A_M y
$$ ,where $y^TA_My$ is the sum of squares due to the main effect, $H$ is a hat matrix of $X$ , and $J$ is a matrix of one. I figured out that above equation can be expressed as SSTotal = SSError + SSA + SSB + SSInteraction where $SSTotal = \sum_i \sum_j \sum_k (y_{ijk} - \bar{y})^2$ , $SSError = \sum_i \sum_j \sum_k (y_{ijk} - \bar{y}_{ij})^2$ , $SSA = \sum_i \sum_j \sum_k (\bar{y}_{i} - \bar{y})^2$ , $SSB = \sum_i \sum_j \sum_k (\bar{y}_{j} - \bar{y})^2$ , $SSInteraction = \sum_i \sum_j \sum_k (\bar{y}_{ij} - \bar{y}_i - \bar{y}_j + \bar{y})^2$ and $\bar{y} = \sum_i \sum_j \sum_k \frac{y_{ijk}}{nIJ}$ , $\bar{y}_i = \sum_j \sum_k \frac{y_{ijk}}{nJ}$ , $\bar{y}_j = \sum_i \sum_k \frac{y_{ijk}}{nI}$ , $\bar{y}_{ij} = \sum_k \frac{y_{ijk}}{n}$ . My two questions: How could I show that cross terms are zero from $$
SSTotal = \sum_i \sum_j \sum_k (y_{ijk} - \bar{y})^2 = \sum_i \sum_j \sum_k \Big((y_{ijk} - \bar{y}_{ij} )+(\bar{y}_i - \bar{y}) + (\bar{y}_j - \bar{y}) + (\bar{y}_{ij} - \bar{y}_i - \bar{y}_j + \bar{y}) \Big)^2
$$ I only show the $\sum_i \sum_j \sum_k (\bar{y}_i - \bar{y})(\bar{y}_j - \bar{y})=0$ . How could I express the above equation by using matrix?","['matrices', 'sums-of-squares', 'statistics', 'anova']"
4467599,"Does there exists a sequence $b_n$, s.t. $\lim b_n = 0$ and for every divergent series $\sum a_n$, The series $\sum a_n b_n$ also diverges?","I am trying to detemine whether the following statement is true or false: Does there exists a positive sequence $b_n$ , s.t. $\lim b_n = 0$ and for any positive
divergent series $\sum a_n$ , The series $\sum a_n b_n$ also diverges? At first I believed the statement is false, Because it does make sense to me that we can always find a divergent series $\sum a_n$ such that $a_n$ will need a ""little enough push"" so that the series will do converge. when looking at some very slow growing sequences such as $b_n =1/ \ln\ln\ln\ln\ln(n)$ , I was not able to find such $a_n$ . So the question is, Is it possible to find such $b_n$ which converge to $0$ slow enough? Any hints will be appericiated.","['calculus', 'sequences-and-series', 'real-analysis']"
4467601,Are there infinitely many primes of primes?,"Primes of primes are prime numbers where each digit is also prime. I have written a computer program to generate such primes from the first 2 billion primes ! So far, the program has given me $75,249$ primes of primes (and counting). Is it known whether there are infinitely many such primes?","['number-theory', 'prime-numbers']"
4467631,"Proving that $ \lim_{(x,y) \to (0,0)}\frac{xy^3}{x^2+2y^4} $ doesn't exist [duplicate]","This question already has answers here : Method to prove limit in $\mathbb{R}^2$ (3 answers) Closed 2 years ago . I want to prove that $ \lim_{(x,y) \to (0,0)}\frac{xy^3}{x^2+2y^4} $ doesn't exist. Every path I choose in real plane tends to 0. Is it possible to do it that way?","['multivariable-calculus', 'real-analysis']"
4467676,Is there an element-free proof that preimages of radical ideals are radical?,"Suppose we have a ring homomorphism $\phi:A\rightarrow B$ and an ideal $J\subseteq B$ . I just spent way too much time on an exercise in commutative algebra, because the element-free definition of the radical of $J$ as $$\sqrt{J} = \bigcap \limits_{q\in \operatorname{Spec}B, J\subseteq q} q$$ suggested to me that there might be a discrepancy between $$\phi^{-1}(\sqrt{J}) = \bigcap \limits_{q \in \operatorname{Spec} B, J \subseteq q} \phi^{-1}(q)$$ and $$\sqrt{\phi^{-1}(J)} = \bigcap \limits_{p\in \operatorname{Spec} A, \phi^{-1}(J)\subseteq p} p$$ (the latter intersection appears to be smaller in total) But using the element-wise definition of a radical in terms of powers one can deduce that both sets always coincide! So I am wondering, whether I should have noticed it somehow while working with the element-free definition…","['radicals', 'abstract-algebra', 'commutative-algebra', 'ideals']"
4467699,How exactly does Halmos define an ordered pair?,"Halmos writes in ""Naive Set Theory"": Given the set ${X}$ , consider the collection $W$ of all well-ordered subsets of $X$ . Explicitly: an elememt of $W$ is a subset $A$ of $X$ together with a well ordering of $A$ . We partially order $W$ by continuation. Then he goes on and writes: The collection $W$ is not empty, because, for instance $\emptyset\, 
 \epsilon \,W$ . If $X \neq \emptyset$ , less annoying elements of $W$ can be exhibited; one such is {( $x, x$ )}, for any particular element $x$ of $X$ . My problem is that Halmos defined earlier in section 6 about Ordered Pairs that an ordered pair is defined as follows: $(a, b) = \{\, \{a\}, \{a, b\}\}$ .
And if $a=b$ then we have $(x,x)= \{\{x\}\}$ .
My problem/question is that if $W$ contains subsets of $X$ and if $x$ is an element of $X$ , then why is
{( $x, x$ )} in $W$ ? By the definition of an ordered pair that Halmos gave we can only conclude that $(x,x)\, \epsilon \,W$ .","['elementary-set-theory', 'well-orders']"
4467739,Do quasitransitivity and completeness imply transitivity?,"Let $X$ be a set and let $R$ be a binary relation on $X$ , i.e. $R \subseteq X^2$ . For $(a,b) \in X^2$ , let $a R b$ denote $(a,b) \in R$ . Let $P$ be the antisymmetric subset of $R$ . Define the properties: Completeness (C): $a \lnot R b \implies b R a$ Quasitransitivity (Q): $a P b P c \implies a P c$ Transitivity (T): $a R b R c \implies a R c$ Here's my confusion: I can't tell if C+Q imply T. Argument 1. C+Q imply T . Suppose $a \lnot R b \lnot R c$ . Completeness implies $c P b P a$ ; Quasitransitivity implies $c P a$ . So $R$ is transitive since $c R b R a$ and $c R a$ . Argument 2. C+Q do not imply T . Let $\succ_i$ and $\succ_j$ be complete strict transitive relations on $X$ , and suppose they are as follows: $b \succ_i c \succ_i a$ and $c \succ_j a \succ_j b$ . Define $R$ as follows: for all $(a,b) \in X^2, a R b \iff a \succ_k b \text{ for some } k \in \{i,j\}$ . Clearly, $R$ is complete. It is also quasitransitive,  since $a \lnot R b \lnot R c$ implies that $a \succ_k c$ for all $k$ due to transitivity of $\succ_k$ , and thus $c \lnot R a$ .  But it is not transitive: we have $a R b R c$ and yet $a \lnot R c$ . Where is my mistake?","['elementary-set-theory', 'relations']"
4467801,"Is it possible to find the coordinates of a point on the circumference of a circle, without using trigonometric functions?","I don't have a particularly good reason to want to do this, and I'm just asking out of curiosity. I am looking for the coordinates of point $\pmb B$ , a point on the circumference of a circle.
If I know the following: The equation of the circle (the coordinates of the center $\pmb O$ , and its radius $\pmb R$ ) The coordinates of a point on the circumference, point $\pmb A$ The central angle $\widehat{AOB}$ (I know the angle might be useless without trigonometry) The length of the arc $\newcommand{arc}[1]{\stackrel{\Large\frown}{#1}}\arc{AB}$ Points $\pmb A$ and $\pmb B$ are both in the upper half of the circle, and the angle $\widehat{AOB}$ is less than $\frac{\pi}{4}$ radians (for this specific example) Is it possible to find the ccoordinates of $\pmb B$ without using any trigonometric functions (sin,cos,tan,etc)? I'm not sure how to create an image for this, otherwise I would've included one for clarity. Edits: I realize that the arc length can be calculated from the central angle and the radius, so point 4 is redundant.
Someone else pointed out that there would be two possible points given the angle and arc length. Would it be possible to find either one or both without using trig?","['arc-length', 'angle', 'circles', 'geometry']"
4467817,Evaluate the line integral along a parabola,"Evaluate the line integral of $f(x,y)=-y+x$ along part of the parabola $y=2(x+1)^2$ from the point $(0,2)$ to the point $(-1,0)$ I need help trying to find a good parameterization for this because what I've done just lands me in a mess. My work so far: Let $x=t, y=2(t+1)^2$ $$
\begin{split}
r(t)  &= \left<t,2(t+1)^2\right>, \quad -1\leq t \leq 0 \\
r'(t) &= \left<1,4(t+1)\right>,   \quad -1\leq t \leq 0 \\
\|r'(t)\| &= \sqrt{1+16(t+1)^2} \\
-y+x &= -2(t+1)^2+t\\
     &=-2t^2-3t-2\\
\end{split}
$$ So we have $$
\int_0^{-1}\left(-2t^2-3t-2\right)\sqrt{1+16(t+1)^2}dt
$$ This integral is really ugly. so then I tried a different method: $$
\begin{split}
y &= 2(x+1)^2 \\
\frac{dy}{dx} &= 4x+4 \\
dS &= \sqrt{1+(4x+4)^2} dx\\
   &=\sqrt{16x^2+32x+17} dx
\end{split}
$$ So we get $$\int_0^{-1} \left(-2(x+1)^2+x\right)\sqrt{16x^2+32x+17}dx$$ Again, very ugly. Can someone please help me solve this?","['multivariable-calculus', 'line-integrals']"
4467823,How to solve $Cx^2 y'' + xy'- y = 0$?,"How to solve the differential equation $Cx^2 y'' + xy'- y = 0$ , if $C$ is positive? Attempt: I use power series and let $$y = \sum_{n=0}^{\infty} a_n x^{n+c}$$ be the solution. Getting the first and second derivatives and substituting these into the given DE, I obtained $$\sum_{n=0}^{\infty} [C(n+c)(n+c-1)] a_n x^{n+c} + \sum_{n=0}^{\infty} (n+c) a_n x^{n+c} - \sum_{n=0}^{\infty} a_n x^{n+c} =0$$ What should we continue here? If this is not the optimal solution or approach, what would you recommend?","['power-series', 'derivatives', 'ordinary-differential-equations']"
4467825,Bourgain's paper on kakeya conjecture,"What is the necessary background in additive number theory/arithmetic combinatorics  required for someone  to be able to read and understand Bourgain's paper: ON THE DIMENSION OF KAKEYA SETS AND
RELATED MAXIMAL INEQUALITIES  on the arithmetic method and the Kakeya conjecture? Can you recommend some sources/books to learn the basics in these two disciplines? Thank you in advance!","['harmonic-analysis', 'additive-combinatorics', 'book-recommendation', 'real-analysis', 'combinatorics']"
4467834,Exchange of $\frac{d}{dx}$ and $\lim_{n\to\infty}$ in terms of continuity of the differential operator,"Under certain conditions, the derivative and the limit of a real function can be exchanged. $$\lim_{n\to\infty}\frac{df_n}{dx}=\frac{d}{dx}\lim_{n\to\infty}f_n$$ Can this be understood as the continuity of the differential operator on some metric space $\frac{d}{dx}:X\to Y$ ?","['function-spaces', 'calculus', 'functional-analysis', 'derivatives', 'soft-question']"
4467835,Determine the image of the unit circle $S^1$ by the action of the matrix $e^A$.,"We have: $$e^{ \begin{pmatrix}
-5 & 9\\
-4 & 7
\end{pmatrix} }$$ I need to determine the image of the unit circle $S^1$ by the action of the matrix $e^A$ . I think that I know how to calculate $e^A$ : I get the Jordan decomposition: $$A = \begin{pmatrix}
		-5 & 9\\
		-4 & 7
	\end{pmatrix}  =
	\begin{pmatrix}
		-6 & 1\\
		-4 & 0
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		1 & 1\\
		0 & 1
	\end{pmatrix}
	\cdot
	\frac{1}{4}
	\begin{pmatrix}
		0 & -1\\
		1 & -6
	\end{pmatrix}
	$$ With eigenvalues: $\lambda$ = 1, algebraic multiplicity = 2, eigenvecotrs: $\left\{ \begin{pmatrix}
		1\\
		0
	\end{pmatrix}, \begin{pmatrix}
	0\\
	1
	\end{pmatrix} \right\}$ $$ \displaystyle e^A = \sum^{\infty}_{i = 0} \frac{A^i}{i!}$$ $$e^A =
	\begin{pmatrix}
		-6 & 1\\
		-4 & 0
	\end{pmatrix}
	\cdot
	\left(
	\begin{pmatrix}
		1 & 0\\
		0 & 1
	\end{pmatrix}
	+
	\displaystyle \sum^{\infty}_{i = 1} \frac{ \begin{pmatrix}
		1 & 1\\
		0 & 1
	\end{pmatrix}}{i!}
	\right)
	\cdot
	\frac{1}{4}
	\begin{pmatrix}
		0 & -1\\
		1 & -6
	\end{pmatrix}$$ $$e^A = \begin{pmatrix}
		-5 & 9\\
		-4 & 7
	\end{pmatrix}  =
	\begin{pmatrix}
		-6 & 1\\
		-4 & 0
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		\displaystyle \sum^{\infty}_{i = 1} \frac{1}{i!}& \displaystyle \sum^{\infty}_{i = 1} \frac{2^{i-1}}{i!}\\
		0 & \displaystyle \sum^{\infty}_{i = 1} \frac{1}{i!}
	\end{pmatrix}
	\cdot
	\frac{1}{4}
	\begin{pmatrix}
		0 & -1\\
		1 & -6
	\end{pmatrix}$$ Where: $$\displaystyle \sum^{\infty}_{i = 1} \frac{2^{i-1}}{i!} = \frac{1}{2} \sum^{\infty}_{i = 1} \frac{2^{i}}{i!} = \frac{1}{2}(e^2 - 1)
	$$ So: $$e^A =
	\begin{pmatrix}
		-6 & 1\\
		-4 & 0
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		e & \displaystyle \frac{e^2}{2} - \displaystyle \frac{1}{2}\\
		0 & e
	\end{pmatrix}
	\cdot
	\frac{1}{4}
	\begin{pmatrix}
		0 & -1\\
		1 & -6
	\end{pmatrix} =
	\begin{pmatrix}
		\displaystyle \frac{-3e^2 + e + 3}{4} & \displaystyle \frac{9e^2 - 9}{2}\\
		\displaystyle \frac{-e^2 + 1}{2} & 3e^2 + e - 3
	\end{pmatrix}
	$$ Now, I don't know if I did it correctly up to this point and what I should do next - to operate on my unit circle. Solution: Because of @Oscar Lanzi we know that: $$e^{\begin{pmatrix}-5 & 9\\-4 & 7\end{pmatrix}}=e\begin{pmatrix}-5 & 9\\-4 & 7\end{pmatrix}$$ Then because of that: Equation of unit circle under linear transformation - can't understand role of inverse matrix (answer by @Prototank)
We know that the image of unit circle in action of the matrix $A$ is given by: $$65x^{2}-166xy+106y^{2}=1$$ Now we need to scale by $e$ and we get the image of unit circle in action of the matrix $e^A$ : $$65x^{2}-166xy+106y^{2}=e^2$$","['matrices', 'matrix-exponential', 'linear-algebra']"
4467888,Asymptotic behavior of integral with Laplace's method,"I am working on the following integral $\int_0^1 dx\int_0^1 dT \sqrt{1-(1-\sqrt{x}+\sqrt{xT})^2} e^{-n xT},$ as $n\rightarrow \infty$ . The goal is to find the asymptotic behavior of the integral to the leading order of $n$ . Obviously, there is a saddle point at $(x,T)=(0,0)$ , which is the main difficulty of this calculation. It seems this is related to the Laplace's method introduced in particular in Chapter VIII of [Wong, R. (2001). Asymptotic approximations of integral]. But when I try to change the variable $y_1=(x+T)/2$ , $y_2=(x-T)/2$ , $y_1=\sqrt{\xi}\cosh{\mu}$ , $y_2=\sqrt{\xi}\cosh{\mu}$ . It seems all the higher order terms of $\xi$ in the expansion of $\sqrt{1-(1-\sqrt{x}+\sqrt{xT})^2}$ will contribute to the results. Thanks for the satisfying solutions. I think the question is already addressed. This question is made up by myself with the goal of addressing a more complex problem posted here Follow up question about Asymptotic behavior of integral with Laplace's method This may need some more efforts and possibly a more general method.","['integration', 'laplace-method', 'approximate-integration', 'asymptotics']"
4467894,"Does a Markov chain with Gaussian transitions $p(x_t|x_{t-1})=\mathcal N(\sqrt{1-\beta_t}x_{t-1},\beta_tI)$ tend to $\mathcal N(0,I)$?","The background of this question is a generative process called reverse diffusion process , where one starts with a data distribution $x_0\sim p_{\rm data}(x_0)$ (each sample lies in $\mathbb{R}^D$ ) and defines a Markov chain (called diffusion process) $x_0,x_1,\cdots,x_T$ with $T$ sufficiently large, where the transitions are $$p(x_t|x_{t-1})=\mathcal N(\sqrt{1-\beta_t}x_{t-1},\beta_tI),\quad\beta_t\in(0,1).$$ The generative process learns to reverse the diffusion process in order to model $p_{\rm data}(x_0)$ . An assumption is made that $p(x_T)=\mathcal N(0,I)$ , so that the reverse process can start from $\mathcal N(0,I)$ , from which numerical sampling is rather easy. My question is whether this assumption is mathematically valid: does $p(x_T)$ tend to $\mathcal N(0,I)$ when $T\to\infty$ ? Intuitively this makes sense because: Each transition adds some Gaussian noise to the previous one; it makes sense for the limiting distribution (if there is one) to be completely Gaussian. $\mathcal N(0,I)$ is invariant under transitions of the form $p(x'|x)=\mathcal N(\sqrt{1-\beta}x,\beta I)$ : $$p(x')=\int p(x'|x)p(x){\rm d}x=\int\frac{1}{(2\pi\beta)^{D/2}}e^{-|x'-\sqrt{1-\beta}x|^2/(2\beta)}\frac{1}{(2\pi)^{D/2}}e^{-|x|^2/2}{\rm d}x=\frac{1}{(2\pi)^{D/2}}e^{-|x'|^2/2}$$ $$\implies x'\sim\mathcal N(0,I).$$ However I cannot prove that the limiting distribution is indeed $\mathcal N(0,I)$ . Any help is appreciated.","['conditional-probability', 'markov-chains', 'markov-process', 'gaussian', 'probability-theory']"
4467929,Analyzing the proof of Kirszbraun's theorem,"Kirszbraun's theorem 1934 Let $A \subset \mathbb{R}^n$ . If $f \colon A \rightarrow \mathbb{R}^m$ is a $L$ -Lipschitz function, then there exists an extension $F \colon \mathbb{R}^n \rightarrow \mathbb{R} ^m$ of $f$ which is also $L$ -Lipschitz. I'm studying that important Lipschitz function extension theorem, better known as Kirszbraun's theorem. The following questions arise through the proof, which I have not been able to solve yet. If anyone has ideas to clearly answer these questions, I would greatly appreciate it. Why are the sets in the equation $(1)$ compact and why is $K_{\gamma} = \bigcap_{i=1}^{n}{B(y_i , \gamma r_i)} \neq \emptyset$ for any sufficiently large value of $\gamma$ ? Why is it enough to show that $\gamma_{0} \leq L$ ? If $0 \notin \text{conv}\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ , why should there be an $m-1$ dimensional plane separating the origin from $\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ and why would we have $B(0 , \varepsilon)$ on the opposite side of the plane from $\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ for all $\varepsilon$ sufficiently small, and why does that contradict the definition of $\gamma_0$ ? Here is the proof of the theorem: Proof If $m = 1$ , the result follows directly from McShane's theorem. Suppose $m>1$ and consider the family $\mathscr{F}$ of $L$ -Lipschitz extensions of $f$ to some set $T$ with $A \subset T$ . This collection is non-empty because it contains at least the original function $f$ . We define a partial ordering on $\scr{F}$ as follows: suppose that $g_1 \colon T_1 \rightarrow \mathbb{R}^m$ and $g_2 \colon T_2 \rightarrow \mathbb{R}^m$ are both elements of $ \scr{F}$ , then $g_1 \preceq g_2$ if and only if $g_2$ is an extension of $g_1$ , that is, $A \subset T_1 \subset T_2$ and $g_{1}(x) = g_{2}(x)$ for all $x \in T_1$ . (The same partial ordering is defined if we recall that a function from a subset of $\mathbb{R}^n$ to $\mathbb{R}^m$ is a set of elements of $\mathbb{R} ^n \times \mathbb{R}^m$ , and we partially order $\scr{F}$ by inclusion). By Hausdorff's maximal principle, $\scr{F}$ has a maximal totally ordered subfamily $\tilde{\scr{F}}$ . Let $\tilde{A}$ be the union of the domains of the functions in $\tilde{\scr{F}}$ . Let us define the function $F \colon \tilde{A} \rightarrow \mathbb{R}^m$ by $F(x) = g(x)$ where $g \in \tilde{\scr{F}}$ and $x \in \text{dom}(g)$ . Clearly $F$ is Lipschitz and $\text{Lip}(F) = L$ . We claim that $\tilde{A} = \mathbb{R}^n$ . If not, then we can fix $x_0 \in \mathbb{R}^n \setminus \tilde{A}$ . A contradiction would be reached if we show that there is $y_0 \in \mathbb{R}^m$ such that $\|y-y_0\| \leq L\|x-x_0\|$ whenever $y=F(x)$ and $x \in \tilde{A}$ , that is, if we show that \begin{equation}\tag{1}
\bigcap_{x \in \tilde{A}}{B(F(x),L\|x-x_0\|)} \neq \emptyset
\end{equation} Since $(1)$ involves an intersection of compact sets, it suffices to show that
any such finite intersection is non-empty. Accordingly, let $x_1 , x_2, \cdots , x_n \in \tilde{A}$ be fixed. Let $y_i = F(x_i)$ , $r_i = \|x_i -x_0\|$ , for $i = 1,2, \cdots , n$ , and $r^{*} = \sup\{r_1 , r_2 , \cdots , r_n\}$ . We know that for any sufficiently large value of $\gamma$ $$K_{\gamma} = \bigcap_{i=1}^{n}{B(y_i , \gamma r_i)} \neq \emptyset$$ Let $$\gamma_0 = \inf\{\gamma \colon K_{\gamma} \neq \emptyset\}$$ Since \begin{equation}\tag{2}
K_{\gamma_{0}} = \bigcap_{\gamma > \gamma_{0}}{K_{\gamma}}
\end{equation} and the intersection of any finitely many of the sets on the right-hand side
of $(2)$ is non-empty, we see that $K_{\gamma_0} \neq \emptyset$ . It will suffice to show that $\gamma_{0} \leq L$ . We many, of course, assume $\gamma_{0} >0$ . Note that $K_{\gamma_{0}}$ must contain exactly one point, because if $k_1 ,k_2 \in K_{\gamma_{0}}$ , then \begin{align*}
\left\|\dfrac{k_1 + k_2}{2} - y_i\right\|^2 &= \dfrac{\|k_1 +k_2\|^2}{4} + \|y_i\|^2 - ( k_1 - k_2) \cdot y_1 \\
&=\frac{\|k_1\|^2}{2} + \frac{\|k_2\|^2}{2} - \frac{\|k_1 - k_2\|^2}{4} + \ |y_i\|^2 - k_{1} \cdot y_i - k_{2} \cdot y_i \\
&= \dfrac{\|k_1 - y_i\|^2 + \|k_2 -y_i\|^2}{2} - \dfrac{\|k_1 - k_2\|^2}{4} \\
&\leq \gamma_{0}^2 r_{i}^2 - \dfrac{\|k_1 - k_2\|^2}{4(r^{*})^2}r_{i}^2
\end{align*} holds for $i=1,2, \cdots , n$ , and $\dfrac{k_1 + k_2}{2} \in K_{\gamma}$ with $\gamma = \sqrt{\gamma_{0}^ 2 - \dfrac{\|k_1 - k_2\|^2}{4(r^{*})^2}} < \gamma_{0}$ contradicting the definition of $\gamma_{0}$ . By translating the coordinate system if necessary, we can assume that $K_{\gamma_{0}} = \{0\}$ . Consequently, we have $\|y_i\| \leq \gamma_{0} r_i$ , for $i = 1,2, \cdots, n$ . We now claim that $0 \in \text{conv}\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ . Were that not the case,
there would exist an $m-1$ dimensional plane separating the origin from $\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ , but then for all sufficiently small $\varepsilon >0$ we would have $B(0 , \varepsilon)$ on the opposite side of the plane from $\{y_i \colon \|y_i\| = \gamma_{0} r_i\}$ , again contradicting the definition of $\gamma_{0}$ . Thus we can choose non-negative scalars $\lambda_1 , \lambda_2 , \cdots, \lambda_n$ such that $$\|y_i\| < \gamma_{0} r_i \implies \lambda_i = 0$$ and $$0 = \sum_{i=1}^n{\lambda_i y_i} \hspace{0.5cm} \text{con} \hspace{0.4cm} \sum_{i=1}^n{\lambda_i} = 1$$ It follows that \begin{align*}
   0 &= 2 \left\| \sum_{i=i}^n {\lambda_i y_i}\right\|^2 \\
     &= 2 \sum_{i,j=1}^n {\lambda_i \lambda_j y_i \cdot y_j} \\
     &= \sum_{i,j =1}^n {\lambda_i \lambda_j \left[\|y_i\|^2 + \|y_j\|^2 - \|y_i -y_j\|^2\right]}\\
     &\geq \sum_{i,j=1}^n {\lambda_i \lambda_j \left[\gamma_{0}^2 r_{i}^2 + \gamma_{0}^2 r_{j}^2 - L^2 \|x_i - x_j\|^2\right]}\\
     &= \sum_{i,j=1}^n {\lambda_i \lambda_j \left[2(x_i -x_0)\cdot \gamma_{0}^2(x_j - x_0) + (\gamma_{0}^2 - L^2)\|x_i - x_j\|^2\right]}\\
     &=2 \left\|\gamma_{0} \sum_{i=1}^n{\lambda_i (x_i - x_0)}\right\|^2 + (\gamma_{0}^2 - L^2)\sum_{i,j=1}^n {\lambda_i \lambda_j \|x_i - x_j\|^2} \tag{3}
\end{align*} If there were but one non-zero $\lambda_i$ , then the second term in $(3)$ would
vanish and the first would be positive, a contradiction. Thus there are at
least two non-zero $\lambda_i$ 's and the second term in $(3)$ must be non-positive,
forcing $\gamma_{0} \leq L$ , as desired. $\square$","['well-orders', 'proof-explanation', 'lipschitz-functions', 'analysis', 'compactness']"
4467939,An unexpectedly difficult geometry problem,"I've run into a geometry problem that feels like it should have an easy answer. But short of numerical integration, I can't find a way to solve it. Consider a filled circle on top of a filled ellipse with their origins overlapping, and the major axis of the ellipse on the X axis (minor axis aligned with y). (See picture) The radius of the circle and major/minor axes of the ellipse are known constants. Find the green shaded area. If at all possible, I would like a function $A(Green) = f(a,b,R)$ . I've tried finding the intersection point of the two shapes in the first quadrant and integrating, but the integrand does not lend itself to a simple analytical integration technique, and Mathematica has been chewing on this problem for about an hour now to no success. Numerical integration could work for my purposes, and I may need to resort to that. Is there a better way to proceed before I do that?","['integration', 'area', 'conic-sections', 'circles', 'geometry']"
4467977,A quadrilateral version of fagnano's problem.,"Here is a problem from the german national math olympiad 2021. This problem looks very similar to fagnano's problem except it is quadrilateral.
The problem is as follows. Let $P$ on $AB$ , $Q$ on $BC$ and $R$ on $CD$ and $S$ on $DA$ be points on the interior of the sides of a convex quadrilateral; $ABCD$ . Show that the following two statements are equivalent. $(1)$ There is a choice for $P, Q, R, S$ for which $PQRS$ has the minimal perimeter. $(2)$ The $ABCD$ is a cyclic quadrilateral with its circumcenter in its interior. I couldn't do any better than this can anyone help me find the proper solution?","['contest-math', 'euclidean-geometry', 'geometry', 'recreational-mathematics', 'problem-solving']"
4468074,Balkan MO 2018 Problem 1 about a complete cyclic quadrilateral,"(BMO 2018 Q1) A quadrilateral $ABCD$ is inscribed in a circle $k$ , where $AB > CD$ and $AB$ is not
parallel to $CD$ . Point $M$ is the intersection of the diagonals $AC$ and $BD$ and the
perpendicular from $M$ to $AB$ intersects the segment $AB$ at the point $E$ . If $EM$ bisects
the angle $CED$ , prove that $AB$ is a diameter of the circle $k$ . My solutions are different from the official solution in https://bmo2018.dms.rs/wp-content/uploads/2018/05/Solutions.pdf , I am not sure these solutions are correct. Any constructive comments on my solution will be much appreciated. Solution-1: Let $O$ be the center of circle $k$ , Let $AB ∩ DC = P$ and $AD ∩ BC = Q$ . We have a completed cyclic quadrilateral ( $ABCDPQ$ ). Let $OM ∩ PQ = N$ . By the properties of completed cyclic quadrilateral, we have $O,M,N$ are collinear.
By Brocard Theorem, $△PMQ$ is self-polar and $O$ is the orthocenter of $△PMQ$ , this implied that $QM ⊥ PO = E'$ and $OM ⊥ PQ = N$ , we are given $ME ⊥ AB$ , and we have $OP ≡ AB$ , $⇒ E'= E$ , $⇒ O$ must on $AB$ , and $AB$ is the diameter. Solution-2: Let $AB ∩ DC = P$ and $AD ∩ BC = Q$ . We have a completed cyclic quadrilateral ( $ABCDPQ$ ). Apply “Cevians induces harmonic lemma” on $△QAB$ we have -1 = ( $P,E;A,E$ ), by the converse of “inversion induces harmonic lemma”, we can conclude that AB is the diameter of the circle $k$ .","['contest-math', 'quadrilateral', 'geometry']"
4468112,How can we prove $\int_a^bf\:{\rm d}g=\int_a^bf(s)g'(s)\:{\rm d}s$ if $g'$ is not continuous?,"Let $a,b\in\mathbb R$ with $a<b$ , $$\mathcal D_{[a,\:b]}:=\{(t_0,\ldots,t_k):k\in\mathbb N\text{ and }a=t_0<\cdots<t_k\}$$ and $$\mathcal T_\varsigma:=\{(\tau_1,\ldots,\tau_k):\tau_i\in[t_{i-1},t_i]\text{ for all }i\in\{1,\ldots,k\}\}\;\;\;\text{for }\varsigma=(t_0,\ldots,t_k)\in\mathcal D_{[a,\:b]}.$$ Moreover, let $f:[a,b]\to\mathbb R$ be continuous and $g:[a,b]\to\mathbb R$ be of bounded variation. We can show that $$\int_a^bf\:{\rm d}g:=\lim_{\substack{|\varsigma|\to0+\\\varsigma\in\mathcal D_{[a,\:b]}\\\tau\in\mathcal T_\varsigma}}S_{\varsigma,\:\tau}(f,g)$$ is well-defined, where $$|\varsigma|:=\max_{1\le i\le k}(t_i-t_{i-1})\;\;\;\text{for }\varsigma=(t_0,\ldots,t_k)\in\mathcal D_{[a,\:b]}$$ and $$S_{\varsigma,\:\tau}(f,g):=\sum_{i=1}^kf(\tau_i)(g(t_i)-g(t_{i-1}))\;\;\;\text{for }\varsigma=(t_0,\ldots,t_k)\in\mathcal D_{[a,\:b]}\text{ and }\tau\in\mathcal T_\varsigma.$$ Assuming that $g$ is differentiable (not necessarily continuously differentiable), are we able to show that $$\int_a^bf\:{\rm d}g=\int_a^bf(s)g'(s)\:{\rm d}s\tag1?$$ Let $\varsigma=(t_0,\ldots,t_k)\in\mathcal D_{[a,\:b]}$ . By the mean value theorem, there is a $\tau\in\mathcal T_\varsigma$ with $$S_{\varsigma,\:\tau}(f,g)=\sum_{i=1}^kf(\tau_i)g'(\tau_i)(t_i-t_{i-1})=S_{\varsigma,\:\tau}(fg',\operatorname{id}_{[a,\:b]})\tag2,$$ but does the right-hand side tend to the right-hand side of $(1)$ as $|\varsigma|\to0+$ ? This is clearly the case when $g'$ is continuous though ...","['measure-theory', 'stieltjes-integral', 'riemann-integration', 'real-analysis']"
4468117,"Is there a name for ""non-irreducible varieties""?","A variety is often defined as a scheme over a field $f:X\to\mathrm{Spec}(k)$ satisfying: $X$ is reduced; $X$ is irreducible; $f:X\to\mathrm{Spec}(k)$ is of finite type; $f:X\to\mathrm{Spec}(k)$ is separated. I realize some authors (for example V. Hoskins in her GIT notes ) like to ignore the irreducible condition (condition 2 above). So $\mathrm{Spec}(k[x,y]/(xy))$ would be called a ""variety"" by them. In informal conversations, I sometimes call such schemes ""reducible variaties"" or ""non-irreducible varieties"". However these names will lead to a confusing statement such as ""An irreducible 'reducible variety' is a variety."" What can be a good name for such schemes?","['algebraic-geometry', 'soft-question']"
4468150,Why does the external bisector of a triangle cross the opposite side?,"Why is that if a triangle is not isosceles, the external bisectors cross the opposite (extended) sides of the triangle?",['geometry']
4468152,Estimate the limit or bounds of the smallest eigenvalue of a symmetric Toeplitz matrix,"I have a symmetric matrix $K\in \mathbb{R}^{(2N+1)\times(2N+1)}$ , with $i,j=-N,\dots,N$ , $$ K_{ij}=-\frac{N}{2\pi[{(i-j)}^2-1/4]} $$ Because the matrix is strictly diagonally dominant with positive diagonals, all eigenvalues are positive. I want to know if the limit of the smallest eigenvalue $$\lim\limits_{N\to\infty}\lambda_0$$ exists. Numerical tests suggest that $\lambda_0\approx0.58$ increases very slowly with $N$ . How can I prove that the limit exists?","['eigenvalues-eigenvectors', 'toeplitz-matrices', 'matrices', 'linear-algebra', 'symmetric-matrices']"
4468226,find an equation satisfied by the three bisectors of a right triangle,"let $a,b,c$ be the sides of a generic triangle and $\alpha, \beta, \gamma$ the bisectors of the respective opposite angle. It is well know that, for example, $\alpha$ divides the side $a$ in two segments, $x$ and $a-x$ , and that we have $(a-x)c=bx$ , in addition $\alpha$ satisfies the equation $\alpha^2=cb-x(a-x)$ , thus we can eliminate $x$ obtaining $(b^2+2bc+c^2)\alpha^2=-a^2bc+b^3c+2b^2c^2+bc^3$ . Hence I have three equations involving only the sides and the bisectors, assuming that such triangle is right-angled (let $a$ be the hypotenuse) , can we find other relations in order to (computationally) eliminate $a, b, c$ and get an equation in the only $\alpha, \beta, \gamma$ ? My attempts have not been successful so far.","['euclidean-geometry', 'triangles', 'systems-of-equations', 'geometry']"
4468263,Solution to $\int^{\infty}_0 \frac{1}{q} e^{-aq^2} dq.$,"My apologies if this question has already been asked. I've tried using the search function but could not find the answer. I'm looking to solve the following integral: $$
\int^\infty_0 \frac{1}{q} e^{-aq^2} dq,
$$ where $a >0 $ and $q$ denotes the magnitude of the momentum.
Now obviously this integral does not converge on the given domain but I am looking for find a way to extract a sensible approximation from this equation. The context is that I am calculating decoherence rates for various quantum mechanical interactions and when calculating the decoherence rate for møller scattering I am left with a bunch of constants and this integral. I would really appreciate any thoughts on this. Thank you in advance! Edit_1: In response to the question of the wider physical context I will briefly outline the problem below. I am calculating the rate at which several electromagnetic interactions cause a loss of entanglement in the spatial superposition of a charged particle in free-fall. The particles are entangled with their environment such that their reduced density matrix becomes (not quite sure how the bra-ket notation works here as the braket latex package is not loading): $$
\rho_{\mathcal{S}} = \sum_{n,m = 1}^{N} c_{n}c_{m}^{*} |\psi_{n}><
\psi_{m}| <E_{m}|E_{n}>.
$$ Since the entanglement is generated using spin-coupling to a magnetic field we can express this using singlet spin states as: $$
\rho_{\mathcal{S}} = \frac{1}{2} \begin{pmatrix} 1 & <E_{2}|E_{1}> \\ <E_{2}|E_{1}> & 1 \end{pmatrix}.
$$ After several steps this gives us the time evolution of the off-diagonal terms as: $$
\rho_\mathcal{S}(\textbf{x},\textbf{x}^{\prime},t) =  \rho_\mathcal{S}(\textbf{x},\textbf{x}^{\prime},0)e^{-\gamma t}
$$ This $\gamma$ is called the decoherence rate and is given by $\Gamma \equiv \int dq \varrho(q) v(q) \sigma_{tot}(q)$ . The calculation of this $\Gamma$ is giving me problems. Taking møller scattering (electron-electron scattering) as the relevant interaction we find that: $$
\sigma \approx \frac{14\pi\alpha^2\hbar^2c^2m^2}{4\textbf{q}^4},
$$ $\varrho$ is given by the Maxwell-Boltzmann distribution: $$
\varrho = \frac{N}{V} 4\pi q^2 \left( \frac{1}{2\pi m k_b T}\right)^{3/2} exp\left[-\frac{q^2}{2mk_bT}\right],
$$ and $v = \frac{\textbf{q}}{m}$ . This gives: $$
\Gamma = \frac{N}{V} 14 m \pi^2 \alpha^2 \hbar^2 c^2 \left( \frac{1}{2\pi m k_b T2}\right)^{3/2} \int_0^\infty dq \frac{1}{q} exp\left[-\frac{q^2}{2mk_bT}\right].
$$ Where the final integral is the one which I am asking the question about. Since q is here is dependent on the thermal velocity (the environmental particles are in a gas) and the temperatures are low, on the order of 1-5K, we have that q is likely limited to not be extremely large. Perhaps this helps in understanding the problem.","['integration', 'physics', 'convergence-divergence']"
4468269,Signed sum of sin and cos,"In the study of graphical representations of the Ising model I have encountered the following sum for natural numbers $a,b$ such that $b \leq a$ $$
\sum_{\theta \in \{ \frac{ 2 \pi k}{q}, k = 0, \dots , q-1 \}} (-\sin(\theta))^{a-b} \cos(\theta)^b. 
$$ Does this expression have a closed formula?","['trigonometry', 'closed-form', 'summation']"
4468270,Projecting Skolem's Paradox Upwards,"My understanding of the resolution of Skolem's Paradox is that although in a countable model of ZFC there does not exist a bijection between a countable set and its powerset, we can still construct a bijection working outside the model which is a bijection. But given this, why is there a reason to believe that this cannot always be done for any model? In other words, how do we know that ""uncountable"" ever means something other than undefinable and that Cantor's Theorem points to combinatorial properties of levels of infinity rather than just an inherent limitation of definable functions, making everything essentially $\aleph_0$ ? EDIT I realize I probably wasn't clear in what I was asking originally so I'll try again: From what I've learned so far about set theory it seems that bijections within a model can fail for possibly one of two reasons: Case I: There are ""more"" elements in one set than the other. This certainly happens with finite sets of different cardinalities and I see this also in the presentation of the diagonal argument on a proposed enumeration of the real numbers in the interval $(0,1)$ Case II: The sets are both subsets (at least in an informal sense) of countable sets but a bijection is ruled out for logical reasons: Like if we make a Godel numbering on the von Neumann integers and set $A$ equal to the subset of Godel numbers of all sentences true in that given model. Then $A$ is a subset of a countable set but there can't be a bijection between it and all the von Neumann integers because that would violate Tarski's Theorem. (or so I think). This would also be the case in a countable model, where a countable set and its powerset are both countable viewed externally but no bijection exists internally because of Cantor's Theorem. So my question is how do we know that Case I is a real possibility (besides for finite sets or a finite and infinite set)? In other words, could it  be that cardinal numbers greater than $\aleph_0$ just exist because of extensions of Case II above and aren't really new mathematical objects in their own right? Hopefully this is clearer, but I'll stop if not.","['elementary-set-theory', 'model-theory', 'diagonalization', 'infinity']"
4468314,Reference request: Euler's 36 officer problem,"I am recently reading MacNeish's 1922 paper ""Euler Squares,"" in which he mentions that Euler's 36 officer problem has been proven first by Tarry in 1901 via analyzing all possible cases. However, MacNeish then wrote on page 1 that A geometric proof by methods of Analysis Situs has been given by J. Petersen (Annuaire des Mathématiciens, 1901-02, pp. 413-426). so I wonder whether there are any English publications that explain Peterson's solution of Euler's 36 officer problem.","['graph-theory', 'general-topology', 'latin-square', 'reference-request']"
4468348,Calculate a triple integral where the integrand has some symmetry,"Calculate $$I = \iiint\limits_{V_n}x^{n-1}y^{n-1}z^{n-1}\sqrt{1-x^n-y^n-z^n}\,dx\,dy\,dz$$ where $$V_n = \{(x,y,z) \in \mathbb{R}^3 \mid x^n + y^n + z^n \leq 1,\ x \geq
0,\  y \geq 0,\ z \geq 0\},\ n \in \mathbb{Z}^+,\ n \geq 1$$ I tried changing variables, and applying symmetry arguments. I think I
can use mathematical induction, but I have to guess what is the final
result. Any hints in these directions?","['multivariable-calculus', 'definite-integrals']"
4468371,Maximum $n$-dimensional volume of set satisfying $(\forall x) \: (\forall y) \: (\| x - y \| \le 1)$,"Let $S \subseteq \mathbb{R}^n$ be a set satisfying the property: $$P(S) \equiv (\forall x \in S) \: (\forall y \in S) \: (\| x - y \| \le 1) $$ where $\| \cdot \|$ is the Euclidean norm. Let $\mathcal{S}$ be a set of all maximal sets $S$ (maximal by set inclusion) s.t. $S$ satisfies the property $P(S)$ . Then what is the value of $V = \max_{T \in \mathcal{S}} \operatorname{vol}(T) $ where $\operatorname{vol}(\cdot)$ represents the $n$ -dimensional volume of a set. I can't figure out the shape of the set for even $n=2$ , is it a circle with radius $\frac{1}{2}$ , or a square with side $\frac{1}{\sqrt{2}}$ , I can't tell.","['general-topology', 'lebesgue-measure', 'measure-theory']"
4468436,Compact subset of banach space is dentable,"Let $X$ be a real Banach Space and $K\subset X$ be compact. We've to show that $K$ is dentable. A bounded set $B$ of $X$ is said to be dentable if $\forall \epsilon>0$ , there exists $x_{\epsilon}\in B$ such that $x_{\epsilon}\not\in \overline{co}(B\setminus B(x_{\epsilon},\epsilon))$ I know two results- For compact $K$ , $\overline{co}(K)$ is compact. If $\overline{co}(B)$ is dentable implies $B$ is dentable. Using these two we can WLOG assume $K$ is compact and CONVEX . Then by Krein Milman Theorem , $\text{Ext}(K)\neq\emptyset$ and $K=\overline{co}(\text{Ext}(K))$ . (Here $\text{Ext}(K)$ denotes the set of all extreme points of $K$ ) So I pick $x_0\in\text{Ext}(K)$ . My intuition is this $x_0$ will serve our purpose for all $\epsilon>0$ i.e. $x_0\not\in \overline{co}(K\setminus B(x_0,\epsilon))$ . But I cannot prove that. Can anyone help me finish the argument? Thanks for your help in advance.","['banach-spaces', 'normed-spaces', 'functional-analysis', 'convex-hulls']"
4468469,Two artists want to paint all sand grains in 2 colors. They stop if one artist picks a grain already painted by the other. In mean how many picks?,"Let $N$ be the (big) number of sand grains. Artist $B$ want to paint them blue. Artist $R$ want to paint them red. We start with all sand grains unpainted. They decide $B$ can start. $B$ picks a random sand grain (all always chance $\frac{1}{N}$ ) and paint it blue. After this it's $R$ turn. $R$ picks a random sand grain and paints it red (if not already blue). If an artist picks a sand grain which is: not painted yet the artists paints it with the own color and place it back afterwards. After this it's the other artist turn again. already colored in the own color the same artist will pick a new sand grain (this can repeat multiple times). already colored but not the own color the drawing session will end for both artists. (That means they do fair share. They have either the same amount of colored sand grains or $R$ has just $1$ more.) What is the expected/mean count of picks required for this experiment in relation to $N$ ? We assume $N$ can grow as big we want (but not $\infty$ ). Does it converge to a equation $f(N)$ ? The expected number of picks $n$ could be written as: $$\mathbb{E}[n,N] = \sum_{i=2}^{\infty} i\cdot p_i$$ Can we approximate it for large $N$ . Does it converge to something?","['statistics', 'expected-value', 'coupon-collector', 'card-games', 'probability']"
4468488,Does every connected compact metric space have a unique always attainable average distance?,"Problem statement Let $(X,d)$ be a connected compact metric space. Then does there always exist a unique value $\alpha\geq0$ with the following property? For every $x_1,\ldots,x_k\in X$ there exists $x\in X$ such that the average distance from $x$ to $x_i$ for $i=1,\ldots,k$ is exactly $\alpha$ . Proof of existence The following proves such a value always exists, so the main question is whether it is unique. For $v\in X^n$ , define \begin{align*}
f_v(x)&:=\frac{d(x,v_1)+\ldots+d(x,v_n)}n,
\\a(v)&:=\min_{x\in X}f_v(x),
\\b(v)&:=\max_{x\in X}f_v(x).
\end{align*} Because $X$ is compact and $f_v$ is continuous, the image of $f_v$ is also compact and thus indeed has a minimum and a maximum. Let $v\in X^n$ and $w\in X^m$ . We have $$a(w)\leq\frac{f_w(v_1)+\ldots+f_w(v_n)}n=\frac{f_v(w_1)+\ldots+f_v(w_m)}m\leq b(v).$$ It follows that there exists a supremum $\alpha$ of $a$ and an infimum $\beta$ of $b$ , and that we have $\alpha\leq\beta$ . Let $v\in X^n$ , so we have $a(v)\leq\alpha\leq\beta\leq b(v)$ . Since $X$ is connected, the image of $f_v$ is connected, so since it contains $a(v)$ and $b(v)$ , it also contains $[a(v),b(v)]$ . It follows that there exists $x\in X$ such that $f_v(x)=\alpha$ . Note that uniqueness is thus equivalent with stating that $\alpha=\beta$ in the above proof. Looking for counterexamples The most straight forward connected compact metric spaces that come to mind are $[0,1]^n$ . Let $v(t):=(t,\ldots,t)$ . We have $$a(v(0),v(1))=b(v(1/2))=\sqrt{n}/2=\alpha=\beta.$$ Consider a regular $n$ -gon $P_1,\ldots,P_n$ with center $O$ . We have $$a(P_1,\ldots,P_n)=b(O)=|P_1O|=\alpha=\beta.$$ If we have the boundary of a unit square $ABCD$ , let $E,F,G,H$ be the centers of the edges. We have $$a(A,B,C,D)=b(E,F,G,H)=\frac14(1+\sqrt5)=\alpha=\beta.$$ If we have the unit circle, let $v_n$ denote $n$ evenly spaced points. We have $$\lim_{n\to\infty}a(v_n)=\lim_{n\to\infty}b(v_n)=\frac1{2\pi}\int_0^{2\pi}|1-e^{i\theta}|\ \mbox{d}\theta=\frac4\pi=\alpha=\beta,$$ so we can essentially approximate any measure, in this case giving us the average distance between two points on a circle. Consider an arbitrary acute triangle $ABC$ with circumcenter $O$ . Note that $b(O)=|AO|$ . Since we can approximate any measure, we can introduce weights $w_A,w_B,w_C$ . Fixing $w_A$ , we find unique values of $w_B,w_C$ such that $f_{w_AA,w_BB,w_CC}(P)$ has gradient $0$ at $P=O$ , which gives $$a(w_AA,w_BB,w_CC)=b(O)=|AO|=\alpha=\beta.$$ So it seems like $\alpha=\beta$ always holds, but I can not come up with a proof. I even tried some really ugly non-symmetric non-convex sets, and some metric spaces which are not subsets of $\mathbb{R}^n$ , and while it becomes more work each time, I always end up proving $\alpha=\beta$ . All proofs rely on constructing measures $\mu,\nu$ such that $a(\mu)=b(\nu)$ , but I can not figure out the bigger picture of what these measures are in general. I would not even know how to define measures in arbitrary metric spaces to begin with. Edit: A measure theory approach We can phrase the question in terms of probability measures as follows. Let $(X,d)$ be a connected compact metric space with Borel $\sigma$ -algebra $B(X)$ . Then does there always exist a unique value $\alpha\geq0$ with the following property? For every probability measure $P$ on $(X,B(X))$ , there exists $x\in X$ , such that $\mathbb{E}_{y\sim P}(d(x,y))=\alpha$ . The existence proof is completely analogous if we use $f_P(x):=\mathbb{E}_{y\sim P}(d(x,y))$ , for $P$ a probability measure on $(X,B(X))$ , because $f_P$ is still continuous. Indeed, $f_P$ has Lipschitz constant $1$ . Equivalence with the finite average approach follows from discrete probability measures are dense in separable metric spaces . Indeed, since $X$ is compact, it is separable. Optimal probability measures The generalization to arbitrary probability measures allows us to do the following. Since the set of probability measures $P$ on $(X,B(X))$ is sequentially compact, see Theorem 10.2 , there exist probabability measures $P_a$ and $P_b$ on $(X,B(X))$ such that $a(P_a)=\alpha$ and $b(P_b)=\beta$ . Claim: We have $\alpha=\beta$ if, and only if, we have $f_{P_a}(x)=\alpha$ for all $x\in\mbox{supp}(P_b)$ , and vice versa. Proof: For the left implication, since $X$ is strongly Lindelöf, we have $$\alpha=\mathbb{E}_{x\sim P_b}(f_{P_a}(x))=\mathbb{E}_{x\sim P_a}(f_{P_b}(x))=\beta.$$ For the right implication, note that, if $f_{P_a}(x)>\alpha$ for some $x\in\mbox{supp}(P_b)$ , then $$\alpha<\mathbb{E}_{x\sim P_b}(f_{P_a}(x))=\mathbb{E}_{x\sim P_a}(f_{P_b}(x))\leq\beta.$$","['measure-theory', 'metric-spaces', 'real-analysis', 'average', 'probability-theory']"
4468554,The correct understanding of limits,"I want to be sure that I'm understanding the concept of limits correctly . When I faced the concept of limits for the first time I've been told that the definition of a limit is: $\lim _{x\to \:a}$ $f(x)$ $=$ $L$ Is that when the values of $x$ get closer to the value $a$ , the values of $f(x)$ get closer to the value $L$ ? But I see this definition is a vague and wrong one. Besides if we consider the constant function $f(x) = c$ the definition doesn't hold: ''if $x$ gets closer to $a$ the values of $f(x)$ stay the same and don't get closer to any value"". Then I read Tom Apostol's Calculus and the definition the book provides is the statement $\lim _{x\to \:a}$ $f(x)$ $=$ $L$ means that we can make the values of $f(x)$ as close as we please to the number ( $L$ ), provided that we make the values of $x$ sufficiently close to $a$ . This definition provides no ambiguities and makes perfect sense with every function. My question: Is the first definition I wrote really a wrong one because I see any one that introduces limits begin with definition and am I really understanding what Tom Apostol really wants to say? Note: What I wrote about the definition that Tom Apostol provides is how I understand it and I know that the the rigor definition is the $\epsilon -\delta$ but I see that the $\epsilon -\delta$ definition is just a rigor translation of what I've said about the definition that Tom Apostol provides. Correct me if I've written anything wrong.","['limits', 'calculus', 'epsilon-delta']"
4468593,Prove that $a_{n+1} = a_n^2 - a_n+1$ for all $n$ large enough,"Prove that if $a_1,a_2,\ldots$ are positive integers so that $a_{n+1}\ge \prod_{k=1}^n a_k$ and $\sum_{n=1}^\infty \frac{1}{a_n}$ is rational, $a_{n+1} = a_n^2 - a_n+1$ for all sufficiently large $n$ . I don't have a concrete idea of what to do; maybe a proof by contradiction could work? Would solving the following simpler problem be of any use: given $a_1 = 2, a_{n+1} = a_n^2 - a_n+1, n>0$ , prove that if $m\neq n, a_m$ and $a_n$ are coprime and $\sum_{i=1}^\infty \frac{1}{a_i} = 1$ . The difficulty with this problem is that I'm not sure how I can make use of the given conditions.","['calculus', 'inequality', 'sequences-and-series']"
4468608,Limit of a double integral,"I have to analyse the following limit \begin{equation}
\lim_{n\rightarrow \infty}\int_{-n}^{n}\int_{-n}^{n}\sin(x^2+y^2)dxdy
\end{equation} I've tried to see it in polar coordinates but it didn't work. Do you have any idea how to work this problem? I put this problem in Wolfram Alpha and I get the result is $\pi$ and the double integral it's some special","['integration', 'limits', 'calculus']"
4468653,How to actually plot Riemann Surfaces?,"I took a course in complex variables and I remember reading something about Riemann surfaces on Wikipedia . There are even some examples in this page: I am curious about the following: How do we actually plot them? The definition in there doesn't look like anything I could translate into points $(x,y,z)$ . I've been trying to guess for a while how to plot the Riemann surface for $f(z)=z^{\frac{1}{2}}$ but with no success. I think we need to compute the branches for $f(z)=z^{\frac{1}{2}}$ and we can do that with the following formula: $$f_k(z)=|z|^{\frac{1}{n} } \left( \cos\left(\frac{\arg z + 2 k \pi}{n}\right) + i\sin\left(\frac{\arg z + 2 k \pi}{n}\right) \right) \quad k=0,1,2,..,n-1 $$ So in our case, we have $f_0$ and $f_1$ with $n=2$ . Now I tried to plot the following sets of points: $$(a,b,\arg f_0(a+bi))\qquad (a,b,\arg f_1(a+bi))$$ And I got this: $\quad \quad \quad \quad \quad \quad \quad $ Which kinda looks like it can be cut and glued like the figure I gave previously. Is it correct? EDIT: I had some progress on it, I read on Agarwal's Introduction to Complex Analysis: I tried to do as he describes and am plotting the function $z=w^2$ , with the restrictions he gave in the text. By plotting the pairs $(\Re(z),\Im(z),\arg(z))$ I obtained this: I don't know if this is the correct result but it still doesn't look like the one I found on Wikipedia:",['complex-analysis']
4468657,How to find the equation of a plane with two identical lines?,"How do you find the equation of a plane with two identical lines? Example: In each case, determine whether or not the given pair of lines intersect. Also, find all planes containing the pair of lines. $$\langle x,y,z \rangle = \langle 3,2,-2 \rangle + s \langle -2,-2,2 \rangle, \quad \langle x,y,z \rangle = \langle 2,1,-1 \rangle + s \langle 1,1,-1 \rangle.$$ I equated the two lines together but didn't get a sufficient value to continue with the problem. NOTE: This not an assignment problem it's just for practice before I start multivariable calculus.","['multivariable-calculus', 'linear-algebra']"
4468670,Isomorphism of Brauer groups,"A recent big result proved by $\mathrm{\check{C}}$ esnavi $\mathrm{\check{c}}$ ius states that For a regular, integral, noetherian scheme $X$ and an open subset $U \subset X$ whose complement is of codimension at least $2$ , the restriction map $\mathrm{Br}(X) \rightarrow \mathrm{Br}(U)$ is an isomorphism. This is called purity for Brauer groups . I wonder how much of the result can be extended to curves. Say, for example, we have an elliptic curve $$E: Y^2Z = X^3 + aXZ^2 + bZ^3$$ and we remove the point of origin $O$ to obtain the affine model $$C:y^2 = x^3+ax+b$$ whose complement $\{O\}$ is of codimension $1$ . How much can be said about the restriction map $\mathrm{Br}(E) \rightarrow \mathrm{Br}(C)$ ? By a result of Bertuccioni in Brauer groups and cohomology , Let $X$ be an separated noetherian scheme and $U \subset X$ be a nonempty open subscheme. Assume that $U$ contains every generic point and every singular point of $X$ . Then the restriction map $\mathrm{Br}(X) \rightarrow \mathrm{Br}(U)$ is an injective homomorphism. I would like to know if the example given has any chance of being an isomorphism. Also, what does purity even mean?","['algebraic-geometry', 'brauer-group', 'arithmetic-geometry']"
4468695,Probability that a random permutation of n elements has a cycle of length k > n/2,"When $k > n/2$ , the probability that a random permutation of an $n$ -element set has a cycle of length $k$ is $1/k$ . This is well-known and easy enough to see.  We can just count permutations with a cycle of length $k$ and divide by $n!$ .  This is made easier by the fact that any permutation has at most one cycle of length $k > n/2$ . Take an $n$ -element set $X$ .   Choosing a permutation of $X$ with a cycle of length $k$ is the same as choosing a $k$ -element subset $S \subseteq X$ , a cyclic ordering on $S$ , and an arbitrary permutation of $X - S$ . There are $\binom{n}{k}$ choices of $S$ , $(k-1)!$ cyclic orderings on $S$ , and $(n-k)!$ permutations of $X - S$ .   Multiplying these, we get $$  \binom{n}{k} (k-1)! (n-k)! = \frac{n! (k-1)! (n-k)!}{k! (n-k)!} = \frac{n!}{k}  $$ Dividing by $n!$ we get $1/k$ as desired. Here is my question, raised in email by Emily Zhang: is there a way to directly get the answer $1/k$ without all the cancellations required in the above approach? Perhaps to do this we need more sophisticated math of some sort.",['combinatorics']
4468716,Decomposition of trace zero real matrix with purely imaginary eigenvalues,"I would like to prove that for a matrix $A\in sl(2,\mathbb{R})$ , i.e. a real matrix with $\text{tr}(A)=0$ , if the eigenvalues of $A$ are $\pm i\alpha$ , for $\alpha\in \mathbb{R}$ , then there is a real matrix $M$ with $\det(M)>0$ and $$
MAM^{-1}=\alpha J
$$ where $J=\begin{pmatrix}0&1\\-1&0\end{pmatrix}$ . It seems like it should be easy, but I am having some difficulty proving it by hand. I am wondering if there is a more abstract solution (I don't know any Lie theory). Thanks for any help!","['matrices', 'linear-algebra', 'lie-algebras']"
4468729,Radical representation of $\cos\frac {2\pi}{11}$,"I want to find radical representation of $\cos\dfrac {2\pi}{11}$ . My attempt Consider the 11th root of unity: $$
\begin{aligned}
ω&= e^{2πi/11}\\
ω^n&=ω^{n\bmod 11}
\end{aligned}
$$ From Euler's formula: $$
\begin{aligned}
\cos\frac{2\pi}{11}&=\frac{1}{2}\left(ω^1+ω^{10}\right)\\
\cos\frac{4\pi}{11}&=\frac{1}{2}\left(ω^2+ω^9\right)\\
\cos\frac{6\pi}{11}&=\frac{1}{2}\left(ω^3+ω^8\right)\\
\cos\frac{8\pi}{11}&=\frac{1}{2}\left(ω^4+ω^7\right)\\
\cos\frac{10\pi}{11}&=\frac{1}{2}\left(ω^5+ω^6\right)\\
\end{aligned}
$$ Define $$ω^{a,b,c, \cdots} = ω^a + ω^b + ω^c + \cdots$$ Let $$
\begin{aligned}
σ_{0}
&=ω^{\{1,2,3,4,5,6,7,8,9,10\}}=-1
\end{aligned}
$$ and $$
\begin{aligned}
σ_{1}&=ω^{\{1,10\}}\\
σ_{2}&=ω^{\{2,9\}}\\
σ_{3}&=ω^{\{3,8\}}\\
σ_{4}&=ω^{\{4,7\}}\\
σ_{5}&=ω^{\{5,6\}}\\
\end{aligned}
$$ With $$
\begin{aligned}
σ_1+σ_2+σ_3+σ_4+σ_5&=σ_0 \\
\sum_{\rm{cycle}}{σ_i σ_j}&=4σ_0 \\
\sum_{\rm{cycle}}{σ_i σ_j σ_k}&=10+7σ_0 \\
\sum_{\rm{cycle}}{σ_i σ_j σ_k σ_m}&=10+7σ_0 \\
σ_1 σ_2 σ_3 σ_4 σ_5&=2+3σ_0 \\
\end{aligned}
$$ According to Vieta's formulas, $(σ_{1},σ_{2},σ_{3},σ_{4},σ_{5})$ is the root of $x^5+x^4-4 x^3-3 x^2+3 x+1$ . This is a solvable quintic equation, but I don't know how to solve the radical.","['galois-theory', 'trigonometry', 'quintics']"
4468751,Minimum of $\begin{aligned}\frac{a^2+b^2}{c^2}\end{aligned}$ in $\Delta ABC$,"In $\triangle ABC$ , $\sin B=-\cos C$ . Find the minimum of $\begin{aligned}\frac{a^2+b^2}{c^2}\end{aligned}$ . According to the law of sines, $\begin{aligned}\frac{a^2+b^2}{c^2}=\frac{\sin^2A+\sin^2B}{\sin^2C}\end{aligned}$ . Solution $1$ Let $\sin B=-\cos C=k$ , then $\sin B=k>0$ , so $\cos C=-k<0$ , meaning that $\begin{aligned}C>\frac\pi2\end{aligned}$ . Thus $\begin{aligned}B<\frac\pi2\end{aligned}$ , so $\cos B=\sqrt{1-k^2}$ . Now $\sin A=\sin(B+C)=k(-k)+\sqrt{1-k^2}\times\sqrt{1-k^2}=1-2k^2$ . So $$\frac{\sin^2A+\sin^2B}{\sin^2C}=4-4k^2+\frac{10}{1-k^2}-13\ge2\sqrt{40}-13=4\sqrt{10}-13.$$ Solution $2$ From $\sin B=-\cos C>0$ we have $$B=C-\frac{\pi}{2}, \sin B=\sin \left(C-\frac{\pi}{2}\right)=-\cos C ,
\sin A=\sin (B+C)=\sin \left(2 C-\frac{\pi}{2}\right)=-\cos 2 C$$ . So \begin{aligned}\frac{\sin ^{2} A+\sin ^{2} B}{\sin ^{2} C}&=\frac{\cos ^{2} 2 C+\cos ^{2} C}{\sin ^{2} C} \\
&=\frac{\left(1-2 \sin ^{2} C\right)^{2}+\left(1-\sin ^{2} C\right)}{\sin ^{2} C} \\
&=\frac{2+4 \sin ^{4} C-5 \sin ^{2} C}{\sin ^{2} C}=\frac{2}{\sin ^{2} C}+4 \sin ^{2} C-5 \\
& \geqslant 2 \sqrt{\frac{2}{\sin ^{2} C} \cdot 4 \sin ^{2} C}-5=4 \sqrt{2}-5,
\end{aligned}",['trigonometry']
4468758,Covariant derivative on associated vector bundle under change of section,"Let $(P,\pi,M;G)$ be a principal bundle with connection form $A\in\mathcal{C}(P)$ and
let $\rho:G\rightarrow\mathrm{GL}(V)$ be a representation of $G$ on some finite-dimensional vector space $V$ . From these data we can construct an associated vector bundle $E=P\times_\rho V$ with typical fibre $V$ . Using the parallel transport induced by $A$ on $E$ we can define a covariant derivative $$\nabla^A:\Gamma(E)\longrightarrow\Omega^1(M,E)$$ on $E$ . There is also another (equivalent) way to introduce this covariant derivative: start with equation (1) (derived in the following) and show its invariance under a change of local section. $\textbf{This is where I am stuck}$ . See below for details. Let $s:U\rightarrow P$ be a local section of $P$ and $\Phi:U\rightarrow E$ a local section of $E$ . Then we can find a smooth map $\phi:U\rightarrow V$ such that $\Phi(x)=[s(x),\phi(x)]$ on $U$ . We can also pull back the connection $A$ to $A_s=s^\ast A\in\Omega^1(U,\mathfrak{g})$ . The covariant derivative of $\Phi$ can then be written as \begin{equation}
(\nabla^A_X\Phi)(x)=[s(x),\mathrm{d}\phi(X(x))+\rho_\ast(A_s(X(x))\phi(x)],
\qquad\qquad\qquad\qquad (1)
\end{equation} where $X\in\mathfrak{X}(U)$ is a vector field. There is another way to introduce the covariant derivative which is maybe more familiar to physicists: we start with the local formula (1) and show its covariance under a change of the section $s$ . But this calculation is where I am stuck. Let $s':U'\rightarrow P$ be a nother section such that $U\cap U'\neq\emptyset$ . Then there is a transition function $g:U\cap U'\rightarrow G$ of $P$ such that $s=s'\cdot g$ . We also find another smooth map $\phi':U'\rightarrow V$ such that $\Phi=[s',\phi']$ . By the definition of $E$ it then follows that $[s,\phi]=[s',\phi']$ if and only if $\phi=\rho(g)^{-1}\phi'$ on $U\cap U'$ . We calculate $$(\nabla^{A_s}_X\phi)(x)=\mathrm{d}(\rho(g(x))^{-1}\phi')(X(x))
+\rho_\ast\left(\mathrm{Ad}(g(x)^{-1})A_{s'}(X(x))+g^\ast\mu_G(X(x))\right)\rho(g(x))^{-1}\phi'(x)$$ I assume this has to be equal to $\rho(g(x))^{-1}(\nabla^{A_{s'}}_X\phi')(x)$ . For the first differential I get $$\mathrm{d}(\rho(g(x))^{-1}\phi')(X(x))=\rho(g(x))^{-1}\mathrm{d}\phi'(X(x))-\rho_\ast\left(g^\ast\mu_G(X(x))\right)\rho(g(x))^{-1}\phi'(x),$$ so that the second term cancels the last term in the equation before. The thing that bothers me is the adjoint representation in the remaining argument of $\rho_\ast$ . How do I get rid of that? Or is my calculation of the differential wrong?","['connections', 'mathematical-physics', 'differential-geometry']"
4468771,"Consider sequence of numbers $a_r,\;r\geq0\;$ with $a_0=1\;$ and $a_{r+1}^2=1+a_r\cdot a_{r+2}.\;$ Then which of the following is/are true?","Let $\alpha, \beta$ are roots of equation $x^2-a_1 x+1=0$ and consider sequence of numbers $a_r,\;r\geq0\;$ with $a_0=1\;$ and $a_{r+1}^2=1+a_r\cdot a_{r+2}.\;$ Then which of the following is/are true? (A) $a_r+a_{r+2}=a_1 \cdot a_{r+1}$ (B) $a_r+a_{r+2}=2\cdot a_{r+1}$ (C) $a_n=\dfrac{\alpha^{n+1}-\beta^{n+1}}{\alpha-\beta}$ (D) $a_n=\dfrac{\alpha^{n+1}+\beta^{n+1}}{\alpha+\beta}$ My Approach: Since it is given that $a_{r+1}^2=1+a_r\cdot a_{r+2}.\; \implies  a_{r+1}^2-1=a_r\cdot a_{r+2}.\;$ Now using option (A) $\;a_r+a_{r+2}=a_{r}+\dfrac{a_{r+1}^2-1}{a_r}\implies a_r+a_{r+2}=\dfrac{a_{r}^2-1+a_{r+1}^2}{a_r}\implies a_r+a_{r+2}=\dfrac{(a_{r-1}+a_{r+1})\cdot a_{r+1}}{a_r}.$ Now $a_{r-1}+a_{r+1}=a_{r-1}+\dfrac{a_{r}^2-1}{a_{r-1}}\implies a_{r-1}+a_{r+1}=\dfrac{a_{r-1}^2-1+a_{r}^2}{a_{r-1}}\implies a_{r-1}+a_{r+1}=\dfrac{(a_{r-2}+a_{r})\cdot a_{r}}{a_{r-1}}$ When i continue this process i obtain $a_{r}+a_{r+2}=\dfrac{(a_0+a_2)a_2}{a_1}$ But answer given is (A), How do i obtain answer as option (A) which depend upon $\;a_{r+1}?\;$ Also how to obtain option (C) and (D)?","['algebra-precalculus', 'quadratics', 'recurrence-relations', 'recursion']"
4468818,General form of integral possibly related to arctan function?,"I am dealing with an integral which has the form: $$
I = \int_{-\gamma}^{+\gamma} \frac{\alpha}{\left(\beta^2 + \alpha^2 z^2 \right)^{\frac{n}{2}}} \,\text{d}z,
$$ with real constants $\alpha$ and $\beta$ , and integer $n \geq 2$ . I think I am right in saying that for $n=2$ , this integral becomes $$
I = \frac{1}{\beta}\int_{-\gamma}^{+\gamma} \text{d}\arctan \left(\frac{\alpha}{\beta}z \right).
$$ But I'm wondering whether there is a general result for any $n$ ? Appreciate your thoughts.","['integration', 'real-analysis', 'calculus', 'trigonometric-integrals', 'derivatives']"
4468826,"For a number field $K$, is an integer a sum of two $K$'s elements squares iff it is a sum of two integer squares?","Sorry for my bad English. There is a famous problem of elementally number theory as follow; Is an integer a sum of two rational squares iff it is a sum of two integer squares? Now I want know about this generalization for a number field. i.e. is the next Prop. true? Let $K$ be any algebraic number field ,and $\mathscr{O}_K$ be ring of integers  of $K$ .
For any $a\in \mathscr{O}_K$ , $a$ can be written as $r^2+s^2$ for some $r,s\in K$ iff can be written as $b^2+c^2$ for some $b,c\in \mathscr{O}_K$ .","['number-theory', 'sums-of-squares', 'algebraic-number-theory']"
4468841,Etale cohomology groups as Galois representations,"I am trying to understand how etale cohomology groups are Galois representations.  Let $X$ be a scheme over a perfect field $K$ and write $\overline{X}=X \times_{{Spec }K} \textrm{Spec } \overline{K}$ for its base change to $\overline{K}$ . For each $\sigma \in \textrm{Gal}(\overline{K}/K)$ , we get (by the universal property of fibre products) an induced morphism $\overline{\sigma}: \overline{X} \rightarrow \overline{X}$ . By the functorial property of etale cohomology, we get a map $H^i(X, \mathbb Q_\ell) \rightarrow H^i(X, \mathbb Q_\ell)$ , thus making $H^i(X, \mathbb Q_\ell)$ into a Galois representation. My question is: What exactly is the map $\overline{\sigma}$ here and how should one think about it? For instance, if $X$ is a variety defined by a single polynomial equation, what would the map $\overline{\sigma}$ look like?","['etale-cohomology', 'number-theory', 'algebraic-geometry']"
4468846,Sketch in $x-y$ plane based on set builder notation,"Problem: Would the sketch of the following set of points in the x-y plane be infinitely many diagonal lines ( $y=x$ ) each passing through points at integer intervals on the y-axis, e.g. $(0, y):y\in\mathbb Z$ ? $$
\{(x, x+y):x\in\mathbb R,y\in\mathbb Z\}
$$ What I know: The y-coordinate value is described as $x + y$ with $y\in\mathbb Z$ . So, for $x=0$ , the point is $(0,y)$ . In other words, at $x$ equal to $0$ , the sketch would include points at each integer on the y-axis. Similarly, for $x=1$ , the point is $(1,1 + y)$ . And for $x=0.000001$ , the point is $(0.000001,0.000001 +y)$ . Fixing $y=1$ , the point is $(x,x + 1)$ . In other words, the line is diagonal like $y=x + 1$ . What I don't know: Well, I don't know what I don't know and just asking for confirmation here. citation : ""Book of Proof"" by Richard Hammock, Chapter 1 Exercise question 49",['elementary-set-theory']
4468883,Infinitude of prime number of the form $x^2+14y^2$,"In the book "" Primes of the form $x^2+ny^2$ "", David Cox had shown that: $$p=x^2+14y^2 \Longleftrightarrow (-14/p)=1 \;\text{and}\; (x^2+1)^2=8 \mod p \; \text{has an integer solution.} $$ Is this imply that there are infinitely many primes of the form $x^2+14y^2$ ? It is easy to see that there are infinitely many primes that the equation $(x^2+1)^2=8 \mod p$ has an integer solution but I don't have any clue to check if some of them can take $-14$ as their quadratic residue.","['class-field-theory', 'number-theory']"
4468888,How to apply Bayes' theorem when the prior is not known,"I'm a little confused about how to use Bayes' theorem when I don't have any way to establish an initial prior. Say I have a sensor that can detect whether there is flouride in a water sample. Let's denote $P(F | M)$ as the probability that a positive measurement is correct, i.e. there is indeed flouride in the water. And let's add some numbers as an example: $P(F|M) = 0.7$ (and therefore $P(\bar{F}|M) = 0.3$ ) $P(F|\bar{M}) = 0.1$ (and therefore $P(\bar{F}|\bar{M}) = 0.9$ ) where $\bar{M}$ denotes a negative measurement (indicating that there isn't flouride) And $\bar{F}$ denotes that there really is no flouride in the water. Let's compute $P(M|F)$ . Using Bayes: $$
\begin{align}
P(M|F) &= \frac{P(F|M) \cdot P(M)}{P(F)} \\
&= \frac{P(F|M) \cdot P(M)}{P(F|M)P(M) + P(F|\bar{M})P(\bar{M})} \space\space\space\space \text{(total probability on denominator)} \\
&= \frac{0.7 \cdot P(M)}{0.7 \cdot P(M) + 0.1 \cdot P(\bar{M})}
\end{align}
$$ So now what do I do about the $P(M)$ and $P(\bar{M})$ ?","['probability-theory', 'bayes-theorem']"
4468918,Equivalent condition for Cauchy Sequences,"I am trying to prove the following : Let $E$ be a topologic vector space. $(x_n)\subset E$ is a Cauchy sequence in $E \ $ iff $\ \lim_{k\to\infty}(x_{m_k}-x_{n_k})=0$ for any $n_k, m_k$ pair of strictly increasing sequences of $\Bbb N$ . I know that $x_n$ is a Cauchy sequence in $E \ $ means $\forall V\in\mathscr V_0 \ \ \ \exists n_0\in \Bbb N \ \ \textrm{such that} \ \ \forall n,m\geq n_0 \ \ x_n-x_m\in V$ where $\mathscr V_0 $ is collection of nbds of zero. I am sorry for this easy question but I even don't know how should I start because it seems as if there is nothing to prove.  Thanks in advance for any help for any direction of this proposition. I appreciate any help.","['general-topology', 'cauchy-sequences', 'analysis']"
4469039,I made a pretty cool formula does it exist anywhere else?,So I’ve made a formula that takes 2 angles and a base and depending on which angle you put in first gives you one of the 2 other sides and by using certain parts of the formula and it’s inverses you can solve pretty much any triangle problem (I think) I’d like to see if this formula already exists with that being said here’s the formula edit#1: Hey I’m seeing a lot of downvotes on this post I’m a little confused on why please explain in the comments! That would be supper great of you thanks and have a good day! edit#2: here is the work I did to get this formula it’s a little messy but overall it gives a good idea on how I made it https://docs.google.com/file/d/10w19VOlA_dAIrL6ZFsB6pq7sd1JdffhB/edit?usp=docslist_api&filetype=msword,"['trigonometry', 'triangles']"
4469068,Solve equation by shooting method,"I've got homework to numerical methods to solve this equation using shooting method $ y'' - (1-e^{-x})y = 0, \quad y(0) = 1, \quad y(x\to+\infty) = 0 $ Hints I've got: Easily you can find asymptotic solution, using this solution you can use functional substition, that will reduce functional values of solution You can use substition of independent variable to make integration domain finite interval. Hints above also say: Graph of function is unbounded in $x$ and $y$ axis. Using suitable  substitution you can make graph bounded in $\mathbb{R}^2$ Can anyone help me with transformations? Thanks!","['numerical-calculus', 'numerical-methods', 'ordinary-differential-equations']"
4469079,Calculating the intersection of $u^2 - v^3$ with a 3-sphere,"For context, I'm coding a 3D visualisation of the Milnor fibration of a Trefoil knot. I've found some code https://www.unf.edu/~ddreibel/research/milnor/milnor-fibers.nb that calculates the intersection of the real and imaginary parts of $u^m - v^n$ with a 3-sphere radius $\sqrt2$ , and stereographically projects this to $\mathbb{R}^3$ . I don't understand the following substitution into $(x + iy)^m - (z + iw)^n$ (presumably to calculate the intersection): $$x \rightarrow \frac{\sqrt2 (-2+x^2+y^2+z^2)}{2+x^2 + y^2 + z^2}, y \rightarrow \frac{4x}{2+x^2 + y^2 + z^2},z \rightarrow \frac{4y}{2+x^2 + y^2 + z^2},w \rightarrow \frac{4z}{2+x^2 + y^2 + z^2}$$ after which the expression is multiplied by $(2+x^2+y^2+z^2)^{Max(m,n)}$ Any insights on how this works, or are there any alternative methods?","['mathematica', 'fibration', 'algebraic-geometry', 'stereographic-projections', 'differential-geometry']"
4469100,"Is there any polynomial algorithm to check if an integer is of the form $x=p^r q^s$, where $p$ and $q$ are primes?","It is well known and easy to design a polynomial algorithm to test whether $\omega(x)=1$ or $\omega(x)\neq 1$ , where $\omega(\cdot)$ is the omega prime function, i.e. , $\omega(x)$ denotes the number of distinct prime factors of $x$ . But I don't know of a polynomial algorithm for the test of $\omega(x)=2$ . If someone can give me some reference in case there is a polynomial algorithm or in case it is conjectured or known that there is no polynomial algorithm.","['number-theory', 'computational-complexity', 'elementary-number-theory']"
4469115,How to show P(A|A∪B) ≥ P(A|B)?,"How would one show $P(A\,|\,A\cup B)\ge P(A\,|\,B)$ ? I first tried to expand the LHS using the definition of conditional probability and therefore it becomes: $\frac{P(A \cap (A \cup B))}{P(A \cup B)}$ Then I tried to use distribution law to further expand the numerator and get: $P((A \cap A) \cup (B \cap A))$ Since $A \cap A$ is just A and it becomes $P(A \cup (B \cap A))$ . However, if I use distribution law again, it looks like it will go back to the previous step and I'm just repeating doing the same thing. In the solution, it mentioned the total probability law, but I'm wondering how to link these two together and what should be the correct way to prove this.","['conditional-probability', 'statistics', 'probability']"
4469128,"Help with a set theory proof that $\bigcap\limits_{x\in[0,1]}[x,1]\times[0,x^2] =\{(1,0)\}$.","I’m on a self teaching mathematical journey, and learning about proof writing. I’m desperate for constructive feedback on how to improve my skills. This is an exercise from The Book of Proof, which I am following. $\bigcap\limits_{x\in[0,1]}[x,1]\times[0,x^2] =\{(1,0)\}$ . My Proof: $\forall x\in[0,1],1\in[x,1]$ and $0\in[0,x^2]$ . This means $\forall x\in[0,1],(1,0)\in([x,1]\times[0,x^2])$ , so $(1,0)\in\bigcap_{x\in[0,1]}[x,1]\times[0,x^2]$ . Now, we prove that no other element is in the intersection: Let $(a,b)\in\bigcap_{x\in[0,1]}[x,1]\times[0,x^2]$ . This means $(a,b)\in[x,1]\times[0,x^2], \forall x\in[0,1]$ . In particular, if $x=0$ , $(a,b)\in([0,1]\times[0,0])$ and if $x=1, (a,b)\in([1,1]\times[0,1])$ . Then, $(a,b)\in([0,1]\times[0,0])\cap([1,1]\times[0,1])={(1,0)}$ . Thus, $(a,b)=(1,0)$ . QED Is this correct? Any tips on how to make the proof better?","['elementary-set-theory', 'proof-writing', 'solution-verification', 'discrete-mathematics']"
