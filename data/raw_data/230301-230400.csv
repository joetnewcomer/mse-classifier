question_id,title,body,tags
4776855,Probability of selected ball color,"You are on a game show. There are 3 bins in front of you labelled ùê¥, ùêµ, ùê∂, with bin ùê¥ having 1 white balls and 1 black ball, bin ùêµ have 3 White and 2 Black balls, and bin ùê∂ having 5 White and 2 black balls. Without you seeing, the game show host chooses one of the bins at random (so that bin ùê¥ gets selected with probability 1/3, the same for bins ùêµ, ùê∂) and then selects a ball at random from the bin selected. a) What is the probability the ball selected by the game show host is White?
b) Conditional on the event that the ball selected is White, what is the chance the game show host selected bin ùê∂ was selected? I am honestly very lost on both of these questions- I think we'd have to do the probability of the white over the probability of all outcomes for a, which I think could be (1/3)(1/2) + (1/3)(3/5) + (1/3)(5/7) = 127/210. For b. (5/7)(1/3)/(127/210). Can anyone let me know if I'm doing something wrong, and how I should go about the problem?","['statistics', 'probability']"
4776876,Why can we pass limit under integral sign in proof of solving Poisson's equation? (Evans PDE),"On page 23 of Lawrence Evans' Partial Differential Equations text (2nd edition) he claims that $$\frac{ f( x + he_i - y) - f( x-y)}{h} \to \frac{ \partial f}{ \partial x_i} ( x-y)$$ uniformly on $\mathbb{R}^n$ as $h \to 0$ . So $$\frac{ \partial u}{ \partial x_i } (x) = \int_{\mathbb{R}^n }\Phi(y) \frac{ \partial f}{ \partial x_i} ( x- y) dy$$ Why does this hold true? I have proved uniform convergence as follows: Proof: It suffices to show that $$\underbrace{ \frac{ f( x + h e_i - y ) - f( x-y) }{ h} - \frac{ \partial f}{ \partial x_i } ( x-y)}_{g_h(y)}  \to 0 $$ uniformly as $h \to 0$ . But by the Mean Value Theorem, for each $h$ there exists a $\theta_h$ such that $0 < \theta_h < 1$ and $\frac{ f( x + h e_i - y ) - f( x-y) }{ h}  = f_{x_i} ( x+ \theta_h h e_i - y) $ . So \begin{align*} g_h(y) &=  \frac{ f( x + h e_i - y ) - f( x-y) }{ h} - \frac{ \partial f}{ \partial x_i } ( x-y) \\
&=  f_{x_i} ( x+ \theta_h h e_i - y)  -  f_{x_i} ( x-y) 
\end{align*} but $f_{x_i}$ is uniformly continuous ( $f \in C_c^2$ so $f_{ x_i} \in C_c^1$ and therefore continuous on a compact set, so uniformly continuous). Therefore, regardless of $y$ , for small enough $h$ this difference can be made arbitrarily small. So $g_h(y) \to 0$ uniformly, as we intended to show. QED. But how does uniform convergence allow us to move the derivative through the integral sign?","['poissons-equation', 'linear-pde', 'analysis', 'partial-differential-equations']"
4776886,"Proving that in Spherical geometry, Interior angles of triangles don't add up to $180^{\circ}$. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 9 months ago . Improve this question I've gotten a germ of the proof, which involves using the standard metric on the two sphere $ds^2=d\theta^2 + \sin^2{\theta}d\phi^2$ . Thing is, how do I continue from here? How do I use this metric to calculate those interior angles regardless of the lengths of the legs of the triangle?",['differential-geometry']
4776903,Interesting infinite product $\sqrt{2}-1=\dfrac{1\cdot7\cdot9\cdot15\cdot17\cdot23\cdots}{3\cdot5\cdot11\cdot13\cdot19\cdot21\cdots}$,"I have found an interesting family of infinite products. The most interesting one of them being: $\sqrt{2}-1=\dfrac{1\cdot7\cdot9\cdot15\cdot17\cdot23\cdots}{3\cdot5\cdot11\cdot13\cdot19\cdot21\cdots}$ The numerators follow the pattern $+6,+2,+6,+2,\cdots$ and the denominators follow $+2,+6,+2,+6,\cdots$ The family of products was derived from assuming: $\sin(x)-\dfrac{1}{\sqrt{2}}=-\dfrac{1}{\sqrt{2}}\cdot\left(1-\dfrac{4x}{\pi}\right)\left(1-\dfrac{4x}{3\pi}\right)\left(1+\dfrac{4x}{5\pi}\right)\left(1+\dfrac{4x}{7\pi}\right)
\cdots$ And subsequently replacing $x$ as $\dfrac{\pi}{2}$ The keyword here being ""assumed"", so I don't know if this product can be proven. The values do seem to be equal after a few iterations though. However, I would greatly appreciate any proofs or alternate derivations.","['infinite-product', 'sequences-and-series']"
4776966,Cup product structure on $\mathbb{R}P^2$,"I don't understand a passage in my lecture notes. Here is the passage with my questions added in italics: $H^\ast(\mathbb{R}P^2;\mathbb{Z}/2\mathbb{Z})$ is $\mathbb{Z}/2\mathbb{Z}$ in degrees $1$ and $2$ . Let $\gamma$ be the a generator of $H^1(\mathbb{R}P^2;\mathbb{Z}/2\mathbb{Z})$ . Let us compute $\gamma \cup \gamma$ . Recall the presentation $\mathbb{R}P^2$ as a circle ( It should be ""disk"" and not ""circle‚Äú, right? ) with the two halves of the boundary identified. We fix a base point $\ast$ . Let $\ast_1$ be the constant $1$ -simplex and $\ast_2$ the constant $2$ -simplex at the base point $\ast$ . Let $c$ be the $1$ -simplex that is half the boundary of the disk, it is easy to see this is a generator for $\pi_1(\mathbb{R}P^2, \ast)$ and thus for $H_1(\mathbb{R}P^2;\mathbb{Z}/2\mathbb{Z})$ . ( How do I see that $c$ is a generator for $\pi_1(\mathbb{R}P^2, \ast)$ ? ). Lastly, let $s$ be the $2$ -simplex that maps onto the disk homeomorphically, with boundary $2\gamma -\ast_1$ . ( This should read "" $2c-\ast_1$ "" and not "" $2\gamma -\ast_1$ ""? ). It follows that $s-\ast_2$ is a generator of $H_2(\mathbb{R}P^2)$ . ( Why does that follow? ). We then must have $\gamma(c)=1$ and $\gamma(\ast_1)=0$ . ( $\gamma(\ast_1)=0$ follows from $\ast_1=\partial \ast_2$ and $\gamma(c)=1$ from $\gamma\neq 0$ and $c$ being a generator of $H_1(\mathbb{R}P^2;\mathbb{Z}/2\mathbb{Z})$ , correct? ). We therefore find $\gamma \cup \gamma (s)=1$ and $\gamma \cup \gamma (\ast_2)=0$ . We still need to show that $\gamma\cup \gamma$ is not a coboundary ( Why is this not obvious? ).","['general-topology', 'homology-cohomology', 'algebraic-topology', 'projective-space']"
4777074,Tangent spaces of $\Bbb P^1$,"Consider the following descriptions of the complex projective
line $\Bbb P^1$ : The unit sphere $\{(u,v,w) \in \mathbb{R}^3 \mid u^2+v^2+w^2=1\}$ which is identified with the Riemann sphere $\Bbb C \cup \{\infty\}$ . The set of non-zero pairs of complex numbers $(z_0,z_1) \in \Bbb C^2 \setminus \{0\}$ with the equivalence relation $(z_0,z_1) \sim (w_1,w_2)$ if $(z_0,z_1)$ and $(w_0,w_1)$ lie on the same line through the origin. Describe the space of holomorphic vector fields on $\Bbb P^1$ in terms of each presentation; determine, in particular, which vector fields correspond to rotations of the sphere. Similarly describe the space of holomorphic $1$ -forms. So I'm trying to figure out these and I'm stuck with not knowing what is the tangent bundle $T\Bbb P^1$ . If I knew this, then the vector fields would be the sections $\Gamma(T\Bbb P^1)$ and similarly I could look at the cotangent bundle $T^*\Bbb P^1$ and the $1$ -forms would be the sections $\Gamma(T^*\Bbb P^1)$ . So the question is how do I figure out this tangent bundle? By definition it is $$T\Bbb P^1 = \bigsqcup_{p\in\Bbb P^1} T_p\Bbb P^1$$ but in order to understand this I need to understand $T_p\Bbb P^1$ . So here I'm stuck I know that $\dim_\mathbb{C}\Bbb P^1 = 1$ so $T_p\Bbb P^1$ is also $1$ -dimensional as a complex vector space. The way the question is set up seems like there are different descriptions for $T_p\Bbb P^1$ in either case which I'm not able to figure out.","['complex-manifolds', 'projective-space', 'differential-geometry']"
4777105,Good pair vs. well-pointed space,"Call a pair of topological spaces $(X,A)$ a good pair if $A\subseteq X$ is a closed subspace and there exists an open neighborhood $A\subseteq U \subseteq X$ such that $A$ is a deformation retract of $U$ . I have seen two definitions of the notion of a well-pointed topological space : It is a pointed topological space $(X,\ast)$ which is a good pair. It is a pointed topological space $(X,\ast)$ such that the inclusion $\{\ast\}\hookrightarrow X$ is a closed Hurewicz cofibration. Are these two definitions equivalent?","['cofibrations', 'general-topology', 'definition', 'algebraic-topology']"
4777158,Extending smooth maps from $\mathbb{R}^3$ to $S^3$,"Suppose I have a smooth map $f\colon \mathbb{R}^3 \longrightarrow S^2$ . If I identify $\mathbb{R}^3$ with $U_S = S^3 - \{(0,0,1)\}$ via stereographic projection, $\varphi_S\colon U_S \longrightarrow \mathbb{R}^3$ , $\varphi_S(y^1, y^2, y^3, y^4) = \left(\frac{y^1}{1-y^4}, \frac{y^2}{1-y^4}, \frac{y^3}{1-y^4} \right)$ , and suppose $f(\vec{y})$ approaches a constant as $\lVert \vec{y}\rVert \rightarrow \infty$ , then $f \circ \varphi_S$ defines a map from $U_S$ to $S^2$ that extends via continuity to a map from $S^3$ to $S^2$ . My question now is, in what circumstance can we say that this extended map is also smooth?","['differential', 'multivariable-calculus', 'calculus', 'geometry']"
4777165,Abelianization of Non-Abelian Groups,"Let $\mathbf{Grp}$ be the category of groups and $\mathbf{Ab}$ be the category of abelian groups, whose $\text{Hom}$ sets are group homomorphisms. We can define a forgetful functor $\mathcal{F}: \mathbf{Ab}\to\mathbf{Grp}$ by ""forgetting"" that the group is abelian; i.e. $\mathcal{F}(G) = G$ , $\mathcal{F}(f) = f$ for all abelian groups $G$ and abelian group homomorphisms $f$ . Likewise, we can define an abelianization functor $\mathcal{G}:\mathbf{Grp}\to\mathbf{Ab}$ by $\mathcal{G}(G) = G/[G, G]$ , where $[G, G]=\{ghg^{-1}h^{-1}\mid g, h\in G\}$ is the commutator of $G$ , and for any $f\in \text{Hom}_{\mathbf{Grp}}(G, H)$ , we have $\mathcal{G}(f):G/[G, G]\to H/[H, H]$ given by $x[G, G]\mapsto f(x)[H, H]$ . Now one can trivially check that $\mathcal{G}$ is a surjective functor; that is, every abelian group is the abelianization of some group (for instance, itself). But this conclusion didn't seem satisfying to me. Consider the category $\mathbf{Nab}$ of non-abelian groups. We define a functor $\mathcal{H}:\mathbf{Nab}\to\mathbf{Ab}$ similarly to $\mathcal{G}$ ; by abelianizing the group. My question: Is $\mathcal{H}$ surjective? That is, is every abelian group the abelianization of a non-abelian group?","['group-theory', 'abelian-groups', 'category-theory']"
4777170,How to prove the equations have only one real solution?,"There are $n$ equations. I need answer for the case $n=3$ . $$
\frac{1}{x_1}(1-x_1)^2+\frac{1}{x_2}(1-x_2)^2+\cdots+\frac{1}{x_n}(1-x_n)^2=0, 
$$ and $$
\frac{1}{x_1}(1-x_1^2)^j+\frac{1}{x_2}(1-x_2^2)^j+\cdots+\frac{1}{x_n}(1-x_n^2)^j=0,\; 2\leq j\leq n,$$ where $x_j\geq 1$ or $x_j \leq -1$ . If $n=2,3,4$ , computer told me that the only real solution is $(1,1,\cdots,1)$ . I did not tell computer the condition $|x_j|\geq 1$ . I do not know if some inequality can be applied like chebyshev's inequality and rearrangement inequality. It seems also a little like the Vandermonde determinant. Comments: : The following three answers perfectly solve the case $n=3$ , two of them drop the condition $|x_j|\geq 1$ . The case $n=3$ is enough for me, however, for more interested you can try with larger $n\geq 4$ . I am also very curious about that.","['rearrangement-inequality', 'inequality', 'roots', 'analysis']"
4777198,Graded-commutativity of cup product: Non-commuative coefficient ring,"For $R$ a commutative ring and $X$ a topological space the cup product on the singular cohomology $H^{\ast}(X)$ is graded commutative. I have a question about the proof of this claim. Some definitions first: Given topological spaces $X,Y$ and open subsets $A\subseteq X$ , $B\subseteq Y$ , pick a chain homotopy equivalence $$EZ\colon S_{\ast}(X\times Y,X\times B \cup A \times Y;R)\rightarrow S_{\ast}(X,A;R)\otimes S_{\ast}(Y,B;R).$$ For $\alpha \in S^p(X,A;R)$ and $\beta \in S^q(Y,B;R)$ , define the cohomology cross product as $$\alpha \times \beta:=\mu\circ(\alpha\otimes \beta)\circ EZ_{p+q}.$$ The proof I know goes as follows: We first show that the cohomology cross product is graded commutative. The claim then follows from $\alpha\cup \beta=\Delta^{\ast}(\alpha \times \beta)$ , where $\Delta\colon X\rightarrow X\times X$ is the diagonal map and $\alpha \in H^p(X)$ , $\beta \in H^q(X)$ . Denote by $T\colon X\times Y \rightarrow Y\times X$ the map that swaps the arguments and by $\tau \colon S_{\ast}(Y)\otimes S_{\ast}(X) \rightarrow S_{\ast}(X)\otimes S_{\ast}(Y)$ the usual braiding in the monoidal category of chain complexes (i.e. $x\otimes y \mapsto (-1)^{\vert x \vert \vert y \vert}y\otimes x$ ). The chain map $\tau \circ EZ \circ T$ agrees with $EZ$ on $S_0$ . Therefore it is chain homotopic to $EZ$ and we could have used $\tau \circ EZ \circ T$ to define the cross product. If we use $\tau \circ EZ \circ T$ to define the cohomological cross product (denote this operation by $\times^{\tau}$ ), we find $\alpha \times^{\tau} \beta= (-1)^{\vert \alpha \vert \vert \beta \vert}\beta\times \alpha.$ The claim follows. Where did we use that $R$ is commutative? Why does the fact that the chain map $\tau \circ EZ \circ T$ agrees with $EZ$ on $S_0$ imply that it is chain homotopic to $EZ$ ? I know that if the domain of $EZ$ is a projective resolution, this is just the fundamental lemma of homological algebra. But in this case the domain of $EZ$ is generally not an exact sequence.","['homological-algebra', 'solution-verification', 'general-topology', 'homology-cohomology', 'algebraic-topology']"
4777204,Are the group-like elements of a finite dimensional Hopf algebra finite?,Let $H$ be a finite dimensional Hopf algebra over a field $k$ . Let $G(H)$ be the set of group-like elements of $H$ . Is $G(H)$ finite?,"['grouplike-elements', 'abstract-algebra', 'hopf-algebras']"
4777296,Faces of the cap product,"Let $X$ be a topological space and $A\subseteq X$ an open subspace. Let $R$ be an associative unital ring. Define the cap product $$\cap\colon S^q(X,A;R)\otimes S_{p+q}(X,A;R)\rightarrow S_p(X;R)$$ on singular simplices $a$ by $\beta \cap (a\otimes r):=F^p(a)\otimes \beta(R^q(a))r$ and then extend linearly. Here $F^p(a)=\partial_{p+1}\circ\ldots\circ\partial_{p+q}(a)$ is the $p$ -dimensional front face and $R^q(a)=(\partial_0)^{\circ p}(a)$ is the $q$ -dimensional rear face. My question: This specific definition seems very random to me. The basic idea is clear: A cochain eats a face of a higher degree simplex and leaves another face untouched. What seems mysterious to me though is why this cochain should eat the rear face (and not any other face)? Also why is the value of $\beta(R^q(a))r$ tensored with the front face and not with any other face?","['poincare-duality', 'simplex', 'definition', 'general-topology', 'algebraic-topology']"
4777357,Solutions to $f(x^2) = 2f(x)^2-1$,"I'm looking for solutions over $\mathbb{R}$ to the functional equation: $$f(x^2) = 2f(x)^2 -1$$ The left hand side is very similar to the expansion for $\cos(2\theta)$ . But the right hand side involves a power of $2$ , not a multiple of $2$ . Wondering if anyone can classify the set of solutions.","['functional-equations', 'trigonometry', 'real-analysis']"
4777376,Sylvester matrix without coordinates and its geometry,"The (transpose of) the Sylvester matrix of two polynomials $f,g\in A[x]$ represents the following $A$ -linear morphism w.r.t the monomial bases of all $A$ -algebras involved. $$\tfrac{A[x]}{\langle g\rangle}\oplus\tfrac{A[x]}{\langle f\rangle}\overset{(u,v)\mapsto uf+vg}{\longrightarrow} \tfrac{A[x]}{\langle fg\rangle}$$ At least over a (possibly multivariate) polynomial ring over a field, I know this $A$ -linear map is invertible iff $f,g$ are coprime i.e have no roots in common. There's an obvious geometric map that behaves the same, at least over a field: $$\mathbf Z(f)\amalg \mathbf Z(g)\longleftarrow \mathbf Z(f)\cup \mathbf Z(g)\subset \mathbb A^1.$$ It's morally clear this map is invertible iff the union is disjoint i.e the intersection is empty i.e $f,g$ have no common root. By the duality of (a subcategory of?) rings and a (subcategory of?) affine scheme, I'd expect the latter, that is very canonical in geometry, to come from some $A$ -algebra morphism. But it does not seem to be the one represented by the Sylvester matrix, since $(1,v)\mapsto f+vg,(u,1)\mapsto uf+g$ and the product $(u,v)\mapsto uf+vg$ which is not (I think) the product of the values $(f+vg)(uf+g)=uf^2+vg^2\in \tfrac{A[x]}{\langle fg\rangle}$ . Questions. What am I missing? What is the geometric interpretation of the linear map represented by the (transpose of) the Sylvester matrix w.r.t the monomial bases?","['algebraic-geometry', 'linear-algebra', 'polynomials', 'resultant', 'commutative-algebra']"
4777416,How does the common definition of a function satisfy the precise definition of a function?,"Before my reading on Linear Algebra by Hoffman/Kunze, I was under the impression that a function was "" a rule (or mathematical object) that maps/assigns each $x \in X$ (domain) to an element $y \in Y$ (codomain) "". However, stumbling across Hoffman/Kunze's LA book, in the appendix, it defines a functions as a mathematical object which consists of: Domain $\longrightarrow$ set of possible inputs Co-domain $\longrightarrow$ set of possible values outputted Rule $\longrightarrow$ associates each element in the domain to a single element in the codomain This got me confused as I previously interpreted the rule being the whole function, not being only a part of the function. This changed my understanding because functions were no longer just ""a rule that maps..."" but instead an object that consisted of a rule and other parts. I did some researching and stumbled across a few ""more precise"" (but less common) definitions where it also stated that function as three parts. For example, Joe's answer here states that a function is... Definition. A function is an (ordered) triple ( $X$ , $Y$ , $f$ ), where $X$ and $Y$ are sets, and $f$ is a subset of $X \times Y$ satisfying the following properties: For every $x \in X$ , there is a $y \in Y$ such that ( $x$ , $y$ ) $\in f$ . For every $x \in X$ , and for all $z$ , $z' \in Y$ , if ( $x$ , $z$ ) $\in f$ and ( $x$ , $z'$ ) $\in f$ then $z=z'$ . Additionally, a few other sources (that I could find online) define it nearly the same way (with a few saying $f$ is a graph instead of a rule), such as Asaf Karangila's answer , Reed College's  Math 111 Lecture Note , Topoi by Robert Goldblatt , etc. Eventually, I concluded my search and problems by accepting the ordered triple definition of the function. I told myself that the reason why the less precise definition of the function defined it as a rule was because of something that Joe and my LA book mentioned in common. Taken from the same answer by Joe, it says: Commentary. If our definition is to be taken seriously, then $f$ is not the function: rather it is the graph of the function. Nevertheless, it is conventional to abuse notation and refer to the triple ( $X$ , $Y$ , $f$ ) as $f$ for short. Similarly, in Hoffman/Kunze's LA book, it comments: If ( $X$ , $Y$ , $f$ ) is a function, we shall also say $f$ is a function from $X$ into $Y$ . This is a bit sloppy, since it is not $f$ which is the function; $f$ is the rule of the function. However, this use of the same symbol for the function and its rule provides one with a much more tractable way of speaking about functions. To clarify, I told/convinced myself that functions are commonly defined as rules because people would ""sloppily"" abuse the notation $f$ and refer it to both the function and its rule, which therefore caused an adaptation to the definition of a function being a rule. However, I still feel that my reasoning isn't perfect and in some way, still don't understand whether the definition of a function as a rule is correct or not and why people define it like that. Is it saying something different from the precise definition of a function, or am I not understanding something correctly? Are there any flaws to the rule definition of the function?","['definition', 'functions']"
4777434,Is a function $\mathbb{R}^n \to \mathbb{R}$ which has a closed and connected graph necessarily continuous?,"It has been proved here and here that a function $\mathbb{R} \to \mathbb{R}$ which has a closed and connected graph is continuous.
This fact is also proved in a nice article by Burgess . I don't know how to generalize these proofs to the case of a function $\mathbb{R}^n \to \mathbb{R}$ , $n \ge 2$ , however. Is a function $f : \mathbb{R}^n \to \mathbb{R}$ which has a closed and connected graph necessarily continuous? The following fact, which is stated in Munkres (Exercise 26.8), may be useful. For a topological space $X$ and compact Hausdorff space $Y$ , a function $X \to Y$ is continuous if and only if its graph is a closed subset of $X \times Y$ . So it would be enough to show that the function $f$ is bounded on compact sets.",['general-topology']
4777525,Expected Maximum Value of 10 Randomly Selected Balls from an Urn,"There are $20$ balls in an urn labeled from $1$ to $20$ .
You randomly pick $10$ balls out of this urn. What is the expected maximum value of the $10$ balls you picked out? I was able to solve the problem using quite tedious combinatorics as shown below. Is there any other method to solve it? My Solution: $$\frac{20\cdot\binom{19}{9} + 19\cdot\binom{18}{9} +\dots+10\cdot\binom{9}{9}}{\binom{20}{10}}  = \frac{210}{11} $$","['expected-value', 'binomial-coefficients', 'combinatorics', 'order-statistics', 'probability']"
4777579,Prove that $e^{x_1+x_2+\dots+x_n}\geq\frac{1}{2024}\sqrt[2024]{(x_1+1)(x_2+1)\dots(x_n+1)}\big(2023x_1+2023x_2+\dots+2023x_n+2024\big)$,"Let $n\in\Bbb N$ and $x_1,x_2,\dots,x_n\in\Bbb R_{\geq -1}$ . Prove that the inequality $$
\begin{aligned}e^{x_1+x_2+\dots+x_n}\geq\frac{1}{2024}\sqrt[2024]{(x_1+1)(x_2+1)\dots(x_n+1)}\big(2023x_1+2023x_2+\dots+2023x_n+2024\big)
\end{aligned}
$$ holds. The above inequality was shared by a mathematics book author for high school students in my location.  I wanted to share this inequality here.  Although it is stated that solution techniques only require high school knowledge, all the techniques applied here will add color to the solutions. I noticed that the equality occurs if and only if $x_1=x_2=\dots=x_n=0$ . First I tried to get the $2024$ th power of each side.  But the calculations made the inequality more complicated. The inequality $e^x\geq x+1$ gives $e^{x_1+x_2+\dots+x_n}=x_1+x_2+\dots+x_n+1$ . So it is sufficient to prove $x_1+x_2+\dots+x_n+1\geq\frac{1}{2024}\sqrt[2024]{(x_1+1)(x_2+1)\dots(x_n+1)}\big(2023x_1+2023x_2+\dots+2023x_n+2024\big)$ . But I think this procedure seems not easy. I will edit the question in a few hours and add my own progress here and if a solution comes along, I will gladly share it with you here. Thanks in advance for your interest.","['contest-math', 'algebra-precalculus', 'inequality']"
4777595,A modification of a cyclic group that seems to break being a group. What is it?,"Background: I came up with this trying to answer an actually silly question of ""when can $1+1=3$ be true"" ? Consider a set $\mathcal S = \{0,\cdots, N-1\}$ coupled with an operation ""+"" that takes any pair of elements $a,b\in\mathcal S$ $$a+b \to \text{mod}(N-\text{mod}(a+b,N),N)$$ In other words like a cyclic group but after the rotation a reflection is carried out. My question is: can we classify what I have constructed? If we analyze the group axioms Closure is trivial to prove. Inverse element is easy to assign, for $s$ just pick $N-s$ , the rotation takes us to $0$ and the subsequent reflection does not move $0$ anywhere. Identity element is much trickier. $0+0 = 0$ , but for any $s+0$ , this will not be $s$ . However, applied twice , it will act as identity $s+0+0 = s$ as we will reflect back again. So in the language of abstract algebra, what have I done!?","['group-theory', 'abstract-algebra', 'finite-groups', 'cyclic-groups']"
4777682,Prove $\int_{0}^{1}\frac1k K(k)\ln\left[\frac{\left(1+k \right)^3}{1-k} \right]\text{d}k=\frac{\pi^3}{4}$,"Is it possible to show $$
\int_{0}^{1}\frac{K(k)\ln\left[\tfrac{\left ( 1+k \right)^3}{1-k}  \right] }{k}
\text{d}k=\frac{\pi^3}{4}\;\;?
$$ where $K(k)$ is the complete elliptic integral of the first kind with modulus $k$ . One proof is to compute the twins first: \begin{aligned}
&\int_{0}^{1} \frac{K(k)\ln(1+k)}{k}\text{d}k
=-2G\ln(2)-4\Im\operatorname{Li}_3\left ( \frac{1+i}{2}  \right )
+\frac{5\pi^3}{32} +\frac\pi8\ln(2)^2,\\
&\int_{0}^{1} \frac{K(k)\ln(1-k)}{k}\text{d}k
=-6G\ln(2)-12\Im\operatorname{Li}_3\left ( \frac{1+i}{2}  \right )
+\frac{7\pi^3}{32} +\frac{3\pi}{8}\ln(2)^2,
\end{aligned} where $G$ denotes Catalan's constant and $\text{Li}_3(.)$ trilogarithm. The simplicity of the result make me believe that it could be obtained by some implicit approaches. I would appreciate if you could offer some insights or ideas.","['integration', 'definite-integrals', 'calculus', 'polylogarithm', 'elliptic-integrals']"
4777702,How to prove that the axis of an ellipse are perpendicular,"Some visuals are so obvious that you would think proofs are not needed.
But then trying to proof them rigouresly is a whole other kettle of fish. I was stumped by the following puzzle I made for myself: (really it is no homework question) How do you proof that the axis of an ellipse are perpendicular? Yes you can see it, it is obvious but seeing in itself is no proof. Yes you cannot construct a countermodel, but again that is no proof. I am really stumped with this one, it is so obvious, and easy to see, but a proof? As definition of an ellipse I want to use: (reused from Wikipedia) An ellipse is a plane curve surrounding two focal points, such that for all points on the curve, the sum of the two distances to the points is a constant. As definition of the axis i want to use: (all made up by myself, so maybe incorrect, the second one, defining the minor axis,  was a real struggle ;) The first axis of the ellipse is the line containing the longest segment possible between two points on the ellipse. The second axis of the ellipse is the line containing the midpoint of the two focal points and the shortest segment possible between two points on the ellipse,","['proof-writing', 'geometry']"
4777703,Why is the average of $\sin^2 wt = 1/2$ and $\cos^2 wt = 1/2$ [closed],Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 months ago . Improve this question Why is the average of both $\sin^2 = \frac{1}{2}$ and $\cos^2 = \frac{1}{2}$ I was revising Simple Harmonic motion notes and in the average of Kinetic energy derivation $$KE = \frac12 k  A^2  \cos^2(\omega t)$$ And the solution is given as $\frac{1}{4} kA^2$ I haven't gone that deep in integration help,"['integration', 'trigonometry', 'simple-functions', 'means']"
4777761,Limit of composition function,"Let $f,g:(0,\infty)\rightarrow \mathbb{R}$ and $g(x)$ is monotone increasing. Suppose $\lim_{x\to +\infty}g(f(x))=+\infty.$ Prove: $\lim_{x\to +\infty}f(x)=+\infty,\lim_{x\to +\infty}g(x)=+\infty.$ I know that, as $\lim_{x\to +\infty}g(f(x))=+\infty,\forall\ M>0 $ there exist $K>0$ such that $\forall\ x>K, |g(f(x))|>M.$ And, for $a\leq b, g(a)\leq g(b)$ But I do not know how to move forward from this point. Any hint is appreciated.","['limits', 'functions', 'real-analysis']"
4777776,Values of $m$ such that $|x^2-2x+m| + 2x + 1$ has $3$ extrema,I was given the following question: Find the values of $m$ for which the curve $y=|x^2-2x+m| + 2x + 1$ has $3$ extrema. My teacher suggested that we should use the quadratic formula $(b^2-4ac)$ and check every case of it. I am sure that there are three main cases of the inside of absolute value being $<0 ; =0 ; >0$ . The solution is $m<0$ but I do not know how to dig down into each case.,"['calculus', 'quadratics', 'derivatives', 'absolute-value']"
4777778,Proof of statement with modular arithmetics,"I have the following question: Let $p$ and $q$ are distinct prime numbers. Let $a \in \mathbb{N}$ such that $p \nmid (a^{pq}-1)$ and $q \nmid (a^{pq}-1)$ . Prove that $\exists$ prime number $r$ such that $r \mid (a^{pq} - 1)$ and $pq \mid (r-1)$ . My steps to solve it: Firstly, I understood that the first statement is definitely true: if $p,q \nmid (a^{pq}-1)$ then some prime $r \neq p, r \neq q,r \neq p \cdot q$ definitely exists as a prime factor of $(a^{pq}-1)$ . Secondly, by little Fermat theorem we get $a^{r-1} \equiv 1~(\textrm{mod}~r)$ (if $gcd(a,r) = 1$ and we don't know this thing also). From the 1) we get $a^{pq} \equiv 1~(\textrm{mod}~r)$ . It means that $ord(a) \mid (r-1)$ . So $p$ and $q$ could be factors of $r-1$ . There are three possible variants: $p \mid (r-1)$ or $q \mid (r-1)$ or $pq \mid (r-1)$ . Here I am stuck. I don't know how to prove that exists $r$ for which $pq \mid (r-1)$ .","['modular-arithmetic', 'discrete-mathematics']"
4777782,"If $a_1,\ldots, a_n$ are numbers such that $a_1^k+\cdots + a_n^k = x$ for $1\leq k\leq n$, determine the product $a_1\cdots a_n$ in terms of $x$.","If $a_1,\ldots,a_n$ are numbers such that $a_1^k+\cdots + a_n^k = x$ for $1\leq k\leq n$ , determine the product $a_1\cdots a_n$ in terms of $x$ . $\textbf{My work:}$ Note that $e_n(a_1, \cdots, a_n) = a_1\cdots a_n$ , and $p_k(a_1, \cdots, a_n) = \sum^n_{i=1}a^k_i = x$ for $1\leq k \leq n$ , where $e_n$ is the $n-$ th elementary symmetric polynomial and $p_k$ is the $k-$ th power sum symmetric polynomial. Using Newton's identity I get: $p_1 + (-1)e_1 = 0 \implies e_1 = p_1 = x$ $p_2 - p_1e_1 + 2e_2 = 0 \implies e_2 = \frac{1}{2}p^2_1 - \frac{1}{2}p_2 = \frac{1}{2}x^2 - \frac{1}{2}x$ $p_3 - p_2e_1 + p_1e_2 - 3e_3 = 0 \implies e_3 = \frac{1}{6}(x^3 - 3x^2 + 2x)$ $ \vdots $ Somewhere I found that the answer should be $$a_1\cdots a_n = \frac{x(x-1) \cdots (x - (n-1))}{n!} = \binom{x}{n}$$ Unfortunately, I'm not getting something similar to $$\binom{x}{n}$$ Any hints on how to arrive at this answer? Thanks!","['abstract-algebra', 'symmetric-polynomials', 'combinatorics', 'polynomials', 'complex-numbers']"
4777810,Are all nilpotent matrices strictly upper triangularizable (over the complex and real fields)?,"According to this question, strictly upper triangular matrices are nilpotent (over any field). Does it also hold tha nilpotent matrices $N$ are strictly upper triangularizable? (i.e. they are conjugate that an strictly upper triangular matrix $N = JUJ^{-1}$ with $J$ and $U$ both complex). In the complex case, it seems trivial to me that the Jordan canonical form has zeros on its diagonal and is thus strictly upper triangular. However, I have not seen this characterization of complex nilpotent matrices so I might be missing something.
This would be related to the Jordan decomposition of matrices/endomorphisms into a semisimple and nilpotent part. For the complex case, semisimple is the same as diagonalizable, and, if my intuition is correct, the nilpotent part is the same as strictly upper triangularizable. This perfectly relates to the Jordan canonical form of complex matrices, which implies that all complex matrices are triangularizable. Also, what about the real case? Does my statement about nilpotent matrices being strictly upper triangularizable still hold? The Jordan canonical form should be the same (with given signature), although they might not be conjugate via a real matrix, so I'm not sure whether it is still true. For the real case, the semisimple part is conjugate to a block decomposition where you might also have some two-dimensional blocks which are non-scalar/not proportional to the identity (as per this question), so ""kind of"" diagonalizable. Does something similar hold for real nilpotent matrices - ""kind of"" strictly upper triangularizable? Hope my questions were clear, thanks!","['matrices', 'real-numbers', 'linear-algebra', 'complex-numbers']"
4777835,"Definite Integral $\int \tanh(a\cdot \operatorname{atanh}(x)+b)\,\mathrm dx$","I am looking for any type of solution (closed form/recursive integral/special functions/...) or at least a good approximation for this definite integral: $$
\int\limits_{\scriptsize 0}^{\scriptsize y}{\frac{1}{2}+\frac{1}{2}\,\tanh\left(a\cdot\,\operatorname{atanh}\left(2\,x-1\right)+b\right)}{\;\mathrm{d}x}
$$ where $a>0$ , $x$ domain is from $0$ to $1$ .
Other form of function after reduction is: $$
\int_0^y{\frac{1}{1+c\,\left(\frac{1-x}{x}\right)^{a}}}{\;\mathrm{d}x}
$$ where $a>0$ , $c=e^{-2b}$ , $c>0$ , $x$ domain is from $0$ to $1$ . I have been trying to solve it for 1 month right now, but I reached my knowledge threshold so I need to ask for help. PS: Taylor series is not satisfying as it converges only in a small radius when $a$ is big value. Maclaurin series for function centered around $0$ also fails for some $b$ values. Edit: Most promising method I have tried so far was residue method, when I replace x so that bounds of integration goes from $-\infty$ to $\infty$ . $$
u=\operatorname{atanh}\left(\frac{2x}{y}-1\right)
$$ Only in this situation integral across arc of semi-cirle vanishes (acc. to Jordan's lemma) and we are left with integral of real $x$ . So then I should just calculate the Residues to get the result, but... plot of the function shows that there are inifinite numbers of singularities in points $0+i(1/2\pi +k\pi )$ . I think there is no way to converge inifite series of residues. Another good attempt was to switch to inifite series: $$
\int_0^y{\frac{1}{1+c\,\left(\frac{1-x}{x}\right)^{a}}}{\;\mathrm{d}x}=\sum_n{(-c)^n\int_0^y{\left(\frac{1-x}{x}\right)^{an}}{\;\mathrm{d}x}}=\sum_n{(-c)^nB(y,1-an,1+an)}
$$ So we end up with Incomplete Beta Function, which I don't know how to ""roll back"" to fraction or any other way to solve the series. Maybe you know if there exists a function "" $f$ "" so: $$
B(y,1-an,1+an)=f^n(B(y,A,B))
$$","['integration', 'hyperbolic-functions', 'calculus', 'sequences-and-series', 'residue-calculus']"
4777874,"Does the sequence $x_n := n^2/\sqrt{n^6+1} + n^2/\sqrt{n^6+2} + . . . + n^2/\sqrt{n^6+n}$ converge? If it does, what value does it converge to?","Does the sequence converge? If it does, to what value?
In my solution, I have rewritten the expression as $x_n = \frac{1}{\sqrt{n^2+\frac{1}{n^4}}} + \frac{1}{\sqrt{n^2+\frac{2}{n^4}}} + . . . + \frac{1}{\sqrt{n^2+\frac{n}{n^4}}}$ since for any $i$ s.t. $1\le i\le n$ , we can take $x_n = \frac{1}{\sqrt{n^2+\frac{a_1}{n^3}}} + \frac{1}{\sqrt{n^2+\frac{a_2}{n^3}}} + . . . + \frac{1}{\sqrt{n^2+\frac{a_n}{n^3}}}$ s.t $a_n = i/n \implies a_n \le 1$ now we can write $p = n^2$ and $q = \frac{1}{n^3}$ Now, to find the limit can I apply partial limits as given below? $\lim_{p \to \infty} \lim_{q \to 0} \frac{1}{\sqrt{x+a_1y}} + \frac{1}{\sqrt{x+a_2y}} + . . . + \frac{1}{\sqrt{x+a_ny}}$ $\implies \lim_{p \to \infty} \frac{1}{\sqrt{x}} + \frac{1}{\sqrt{x}} + . . . (n times)$ $ \implies \lim_{p \to \infty} \frac{n}{\sqrt{x}} $ $ \implies \implies \lim_{n \to \infty} \frac{n}{\sqrt{n^2}} = 1$","['limits', 'sequences-and-series', 'real-analysis']"
4777915,Misunderstanding of Fourier series and differential equation,"Let's assume simple differential equation: $$\frac{d^2}{dx^2}y=2 \quad y(\pm1)=0$$ the solution for the equation is: $$y=x^2-1$$ and the Fourier transform for the $y(x)$ in the solution interval $[-1,1]$ : $$y(x)=\frac{-2}{3}+\sum_{n=1}^{\infty}{\frac{4(-1)^n\cos(n\pi x)}{n^2\pi ^2}}$$ by substituting the Fourier series inside the original equation: $$\frac{d^2}{dx^2}y(x)=\sum_{n=1}^{\infty}{4(-1)^{n+1}\cos(n\pi x)\neq2}$$ But we can solve the equation by assuming Fourier series that fulfill Boundary conditions $$y(x)=\sum_{n=0}^{\infty}A_n \cos(\frac{2n+1}{2}\pi x)$$ The constant term can be transformed to $$2=\sum_{n=0}^{\infty}\frac{8(-1)^n}{(2n+1)\pi}\cos(\frac{2n+1}{2}\pi x)$$ $$\frac{d^2}{dx^2}y(x)=\sum_{n=0}^{\infty}A_n\frac{-(2n+1)^2\pi^2}{4} \cos(\frac{2n+1}{2}\pi x)$$ $$A_n\frac{-(2n+1)^2\pi^2}{4}=\frac{8(-1)^n}{(2n+1)\pi}$$ $$A_n=\frac{32(-1)^{n+1}}{(2n+1)^3\pi^3}$$ which can be proven to satisfy the required solution: $$y(x)=\sum_{n=0}^{\infty}A_n \cos(\frac{2n+1}{2}\pi x)=x^2-1$$ What I misunderstand? Why the first attempt is wrong? But it actually uses orthogonal basis in the solution interval.","['fourier-series', 'ordinary-differential-equations']"
4777948,Proving an integral inequality involving Lipschitz functions,"I am trying to prove the following inequality: $$\int_a^b \Big( f(a)+f(b)-2f(x) \Big)dx\leq \frac{1}{2}\text{Lip}(f) (b-a)^2$$ where $f$ is a Lipschitz function and $a<b$ . This inequality is numerically true for $\sin(x)$ , $\cos(x)$ , $\exp(x)$ , polynomials and $|x|$ . I wonder if it is possible to rigorously prove this inequality for all Lipschitz functions? However, I can prove a weaker version: $$\int_a^b \Big( f(a)+f(b)-2f(x) \Big)dx\leq \text{Lip}(f) (b-a)^2$$ Proof: $$|f(x)-f(y)|\leq \text{Lip}(f) |x-y| \,\forall x,y \in [a,b] \text{ and } |x-a|+|x-b|=b-a,\forall x\in [a,b]\implies$$ $$\int_a^b \Big( f(a)+f(b)-2f(x) \Big)dx \leq \int_a^b |f(a)-f(x)| + |f(b)-f(x)|\leq \text{Lip}(f) (b-a)^2$$ But this estimate is too rough to get $\frac{1}
{2}\text{Lip}(f) (b-a)^2$ as an upper bound. Could you please give me some idea? Thank you very much!","['integration', 'inequality', 'lipschitz-functions', 'analysis']"
4777974,Find $\lim\limits_{x \to \infty} \int_{0}^1 \dfrac{xt^x}{1+t^x}dt$,"I have to find $\lim\limits_{x \to \infty} \displaystyle\int_{0}^1 \dfrac{xt^x}{1+t^x}dt$ . I know the answer is $\ln(2)$ . To prove it i tried to compare $\dfrac{xt^x}{1+t^x}$ to $\dfrac{1}{1+t}$ because $\displaystyle\int_{0}^1 \dfrac{1}{1+t}dt = \ln(2)$ but it didn't work. Moreover, we have that $1-\displaystyle\frac{t^x}{1+t^x}= \dfrac{1}{1+t^x}$ and therefore that $\displaystyle\int_{0}^1 \dfrac{xt^x}{1+t^x}dt=x\left(\displaystyle\int_{0}^1 1-\dfrac{1}{1+t^x} \right)$ but it doesn't seem useful. Thanks for your help!","['integration', 'limits', 'real-analysis']"
4777983,Prove with induction if $A= \begin{pmatrix}2&1\\-1&0 \end{pmatrix}$ $\forall n \in \mathbb{N}$ $A^n= \begin{pmatrix} n+1&n\\-n&1-n \end{pmatrix}$,"Prove with induction if $$A = \begin{pmatrix}
2 & 1 \\
-1 & 0 
\end{pmatrix}$$ then $\forall n \in \mathbb{N}$ $$A^n = \begin{pmatrix}
n + 1 & n \\
-n & 1 - n 
\end{pmatrix}$$ For $n = 1$ we have $$A^1 = \begin{pmatrix}
1 + 1 & 1 \\
-1 & 1 - 1 
\end{pmatrix} = \begin{pmatrix}
2 & 1 \\
-1 & 0 
\end{pmatrix}$$ so the base case is correct. EDIT: we assume it holds true for $n$ (Thank you @gdcvdqpl !) For $n+1$ we have $$A^{(n+1)} = \begin{pmatrix}
(n+1) + 1 & (n+1) \\
-(n+1) & 1 - (n+1) 
\end{pmatrix} = \begin{pmatrix}
n + 2 & n + 1 \\
- n - 1 & - n 
\end{pmatrix}$$ And also $$A^nA^1 = \begin{pmatrix}
n + 1 & n \\
-n & 1 - n 
\end{pmatrix} 
\begin{pmatrix}
2 & 1 \\
-1 & 0 
\end{pmatrix} = 
\begin{pmatrix}
2(n+1) - n & n + 1 \\
-2n - (1 - n) & -n 
\end{pmatrix} =
\begin{pmatrix}
n + 2 & n + 1 \\
- n - 1 & - n 
\end{pmatrix}
$$ So it also holds for $n+1$ . Is that correct ? We don't have solutions to this exercise. Feel free to point out any inconsistency. Thank you for helping me","['matrices', 'induction', 'linear-algebra']"
4777990,A way to order trees produced by TREE(3) and find the last tree (or at least count nodes in that tree),"So there is this famous TREE(3) function producing various trees. Is there at least one way to compare any pair of such trees so that they can be viewed as ordered sequence? If there is such a way, can we find the latest tree for TREE(3)? Or at least count how many seeds (nodes) this tree contains? I am assuming that the actual number of nodes will be growing much slower than the order sequence, so is it possible that the number of nodes is actually computable? Perhaps we could even compute that last tree in full? I assume here is that if we know generation rules for TREE(3) trees, we can zip them in any direction so we can iterate backwards and essentially get the latest one without computing a really large number of their precursors first. What got me thinking about that: There's a huge number of e.g. MD5 hashes, but it's not hard to figure out that the biggest MD5 hash is simply ffffffffffffffffffffffffffffffff . Perhaps that the biggest tree produced by TREE(3) is actually not that big? Or perhaps it is? Any information that we have about it?","['trees', 'discrete-mathematics']"
4777998,How to prove that nth differences of a sequence of nth powers would be a sequence of $n!$,"Given an infinite sequence of numbers, first differences denote a sequence of numbers that are pairwise differences , second differences denote a new sequence of pairwise differences of this sequence, and so on. 1  2  4  7 11 16 22 29
 1  2  3  4  5  6  7    -- first differences
  1  1  1  1  1  1      -- second differences
   0  0  0  0  0        -- third differences Sequences of $n$ th powers of numbers exhibit an interesting property. Take a sequence of squares: (1,4,9,16,25,...). First differences: (3,5,7,9,11...) Second differences: (2,2,2,2,2...) Take a sequence of cubes: (1,8,27,64,125...) First differences: (7,19,37,61,91...) Second differences: (12,18,24,30,36...) Third differences: (6,6,6,6,6...) Take a sequence of powers of 4: (1,16,81,256,625,...) First differences: (15,65,175,369,671,...) Second differences: (50,110,194,302,434,...) Third differences: (60,84,108,132,156,...) Fourth differences: (24,24,24,24,24,...) 1, 2, 6, 24... that's clearly a factorial! So a sequence of powers of $n$ will have a sequence of repeating $n!$ as its $n$ th differences. How do I prove this relation holds? Bonus points if you give just enough theoretical information for a non-mathematician, but leave the proof as an exercise. Note: It actually doesn't matter if our sequence includes negative numbers too, sequence (...,-2,-1,0,1,2,...) still has (...,1,1,1,1,1,...) as its first differences.","['factorial', 'recurrence-relations', 'sequences-and-series']"
4778014,Existence a uniqueness theorems for ODEs: two proofs compared,"Lately I've been studying the existence [and uniqueness] theorem for ODEs. As is often the case with beautiful theorems, there are several proofs of this fact. Today I want to focus on two elementary proofs, and compare them somewhat. First of all, let's write down exactly what the theorem says in the case of a non-autonomous vector field. Theorem . Let $ E $ be a normed space ( $ \mathbb R^n $ will suffice). Let $ U\subset E $ be open. Let $ I\subset \mathbb R $ be an open interval. Let $ X\colon U\times I\to E $ be continuous and locally uniformly Lipschitz in the second variable. Let $ x_0\in U $ and $ t_0\in E $ . Then there exists an open ball $ B\subset U $ around $ x_0 $ , an open interval $ J = \left]t_0-\alpha,t_0+\alpha\right[\subset I $ around $ t_0 $ , and a unique integral curve $ x\colon J\to U $ of $ X $ such that $ x(t_0) = x_0 $ and such that $ x(t)\in B $ for every $ t\in J $ . A common ground Both the proofs I have in mind start by noticing that there exists an open ball $ B(x_0,\epsilon)\subset U $ and an open interval $ \left]t_0 - \delta,t_0 + \delta\right[\subset I $ where $ \lVert X(x,t) \rVert\leqq M $ for some $ M > 0 $ . This is kinda easy to prove. Moreover, let's call $ K_0 $ some local Lipschitz constant of $ X $ around $ (x_0,t_0) $ . The first proof Let $ \alpha $ be such that $ \alpha < \epsilon/M $ and $ \alpha < \delta $ . One way to prove the Theorem is to define inductively $ x_n\colon J = \left]t_0 - \alpha,t_0 + \alpha\right[\to U $ for every $ n\in \mathbb N $ by $$
x_0(t) = x_0\text{,}\qquad x_{n + 1}(t) = x_0 + \int_{t_0}^t X(x_n(\zeta),\zeta)\,\mathrm d\zeta\text{.}
$$ By induction then $ x_n(t)\in B(x_0,\epsilon) $ and $$
\lVert x_n(t) - x_{n + 1}(t)\rVert\leqq \frac{MK^n}{(n + 1)!}\lvert t - t_0\rvert\text{,}
$$ thus the $ x_n $ s form a uniformly Cauchy sequence that converges to an integral curve of $ X $ . The second proof The second proof I had in mind uses the celebrated Banach fixed point theorem, or rather one of its corollaries. We basically define the operator $$
T\colon H\to K\text{,}\qquad (Tx)(t) = x_0 + \int_{t_0}^t X(x(\zeta),\zeta)\,\mathrm d\zeta
$$ where $ K $ is the space of bounded continuous functions $ x\colon J\to U $ equipped with the sup-norm $ \lVert\phantom{x}\rVert_\infty $ , and $ H $ is the open ball of radius $ \epsilon $ in $ K $ around the constant function $ \bar x_0 $ at $ x_0 $ . It is a general easy fact that if $ \lVert Tx - \bar x_0\rVert_\infty < (1 - C)\epsilon $ , where $ C $ is the contraction constant of $ T $ , then $ T $ has a fixed point that lies in $ H $ . A couple of questions The first one . The estimate to be made on $ \alpha $ in the second proof seems stronger. For the fixed point argument to work we need, in order: $ \alpha < \delta $ , $ \alpha < \epsilon/M $ (this ensures that $ Tx $ is again a bounded continuous mapping and thus lies in $ K $ ), and some other condition related to the fact that $ \lVert Tx - \bar x_0\rVert_\infty < (1 - C)\epsilon $ must hold. For example, Loomis&Sternberg's Advanced Calculus claims that $ \alpha $ should be smaller than $ \epsilon/(M + C\epsilon) $ . On the other hand, the first proof seems to work for all $ \alpha < \delta $ and $ \alpha < \epsilon/M $ . This surprises me. Did I do something wrong? The second one . This is more philosophic. I don't know anything about computational mathematics, but I was wondering: If one of these two iterative proofs were to be implemented on a computer, which one would it be? What's the differences between the two approaches. Or, on the contrary: How are they similar? I will follow a Dynamical Systems course next term and I hope it will clarify me some doubts, but for now I'm just leaving it there.","['dynamical-systems', 'ordinary-differential-equations', 'real-analysis']"
4778031,Why is a $\sigma$-algebra closed only under countable unions?,"I've been told the properties of a $\sigma$ -algebra $\mathcal{F}$ over some set $\Omega$ are: $\Omega \in \mathcal{F}$ $\mathcal{A} \in \mathcal{F} \implies \mathcal{A}^c \in \mathcal{F}$ $\mathcal{A}_1, \mathcal{A}_2, \mathcal{A}_3, \ldots \in \mathcal{F} \implies (\bigcup_{i=1}^{\infty} \mathcal{A}_i) \in \mathcal{F}$ , i.e., countable unions of members of $\mathcal{F}$ are also in $\mathcal{F}$ . My questions are: Why do we have to specify ""countable"" unions? Obviously this means that finite unions of members are also in the $\sigma$ -algebra, as well as countably infinite unions; but it seems meant to contrast uncountably infinite unions. If countably infinite unions doesn't get you outside the set, how would uncountably infinite unions do it? What does it really mean to take uncountably infinite unions? In case it doesn't really make sense to do so, why do we specify countable? In the case that it does, can you give an example of taking uncountable unions of elements of a $\sigma$ -algebra where the result is outside the $\sigma$ -algebra? Is the only difference between an algebra and a $\sigma$ -algebra over $\Omega$ the ""countable"" part? That is, an algebra is closed over finite unions?",['measure-theory']
4778052,A Series of Integrals like $\sin (x^2)/x^2$,"Is there a general solution or pattern in the following series of integrals? $$S(n)=\idotsint_{-\infty}^{\infty} \frac{\sin(x_1^2+x_2^2+...+x_n^2)}{x_1^2+x_2^2+...+x_n^2}\, dx_1 \dots dx_n$$ I can solve the first integral $$S(1)=\int_{-\infty}^\infty\frac{\sin(x^2)}{x^2} dx$$ $$=2\int_{0}^\infty\frac{\sin(x^2)}{x^2} dx$$ Subbing $t$ for $x^2$ : $$=\int_{0}^\infty\frac{\sin(t)}{t^{3/2}} dt$$ Using the Laplace Transform functions, $\mathcal{L}(\sin t)=\frac{1}{1+p^2}$ and $\mathcal{L}^{-1}(t^{-3/2})=\frac{2}{\sqrt\pi}\sqrt{p}$ : $$=\frac{2}{\sqrt{\pi}}\int_{0}^{\infty}\frac{\sqrt{p}}{1+p^2} dp
=\frac{4}{\sqrt{\pi}}\int_{0}^{\infty}\frac{x^2}{1+x^4} dx$$ And the last integral has been solved here through various methods to be $\frac{\pi}{2\sqrt{2}}$ , meaning $S(1)=\sqrt{2\pi}$ . As another quick example, the second integral $$S(2)=\iint_{-\infty}^\infty\frac{\sin(x^2+y^2)}{x^2+y^2} dx\, dy$$ can be solved by converting to polar coordinates where $r^2=x^2+y^2$ . $$=\int_0^{\infty}\int_0^{2\pi}\frac{\sin(r^2)}{r} d\theta\, dr
=2\pi\int_0^{\infty}\frac{\sin(r^2)}{r} dr
=\pi\int_0^{\infty}\frac{\sin(x)}{x} dx
=\frac{\pi^2}{2}$$ Obviously, these techniques don't work as more variables of integration are added. What are the higher values of $S(n)$ and is there a general formula?","['integration', 'definite-integrals', 'closed-form', 'sequences-and-series']"
4778056,Contour Integral of Gamma Functions from Knuth Paper,"I was attempting to go through the addendum of Chapter 21 of Donald Knuth's Selected Papers on Analysis of Algorithms . In a lengthy derivation, Knuth asymptotically expands the following integral: $$ \frac{1}{2 \pi i} \int_{c-i \infty}^{c + i \infty } \frac{\Gamma(s-5/2)\Gamma(4-s) m^s \zeta(s)}{\Gamma(3/2)} ds \sim \frac{\Gamma(-3/2)\Gamma(3)}{\Gamma(3/2)} m + \sum_{n=0}^\infty \frac{(-1)^n}{n!} \frac{\Gamma(3/2+n)}{\Gamma(3/2)} m^{5/2-n} \zeta(5/2-n) $$ where $\frac{5}{2} < c < 4$ . Knuth only states that the asymptotic series is obtained ""by decreasing c as far as we like and adding up the residues of the poles of the integrand"" - however I am having difficulty tracing the logic in how he did so. Considering the more general integral $$ \frac{1}{2 \pi i} \int_{c-i \infty }^{c + i \infty } \frac{\Gamma(s-\alpha) \Gamma(\alpha+\beta-s)m^s\zeta(s)}{\Gamma(\beta)} $$ where $\alpha = A+\frac{1}{2}, \beta = B-\frac{1}{2}$ and both $A,B \in \mathbb{N}$ . The domain of integration is taken as the limit of the line integral from $c-iR$ to $c+iR$ as $R \to \infty$ . Note first that the integrand has poles whenever $s = k, k + \frac{1}{2}, k \in \mathbb{Z}$ . This function stems from an inverse Mellin transform with a fundamental strip of $\{ z \in \mathbb{C} : \alpha < \Re(z) < \alpha + \beta \}$ , so the integrand is only defined on this strip but can be analytically continued to the whole complex plane. Consider the contour $C = L \cup S$ where $L$ is the line segment going from $s=c-iR$ to $s=c+iR$ and $S$ be the semicircular contour going from $s=c+iR$ to $s=c-iR$ extending to the negative real axis (i.e. parametrized by $S(t) = c + Re^{i \theta}$ for $\theta \in (\frac{\pi}{2}, \frac{3\pi}{2})$ ). By the Residue Theorem, we have that $$\int_C f(z) dz = 2 \pi i \sum_{z \in [c, c-R] \cap \left( \mathbb{Z} \cup \mathbb{Z} + \frac{1}{2} \right)} \text{Res}\left(f; z\right)$$ where the sum is taken over the residues inside the contour (i.e. all integers and half integers in the interval $[c, c-R]$ ). We also have: $$ 2 \pi i \sum \text{Res} f(z) = \int_C f(z) dz = \int_{c-iR}^{c+iR} f(z) dz + \int_S f(z) dz $$ If we have that $\int_S f(z) dz \to 0$ as $R \to \infty$ , then Knuth's claim follows - however this is where I am stuck. I have tried to manipulate the inner gamma function terms with Stirling's approximation and have gotten nowhere, so some assistance here would be appreciated. Note: I am aware that there are some theorems involving Mellin transforms that I think are mentioned in the original paper that Knuth cites - however, I am trying to avoid them as it is made to seem as though this is a straightforward computation. Any assistance would be appreciated!","['integration', 'complex-integration', 'mellin-transform', 'gamma-function']"
4778063,Is there anyone among us who can identify a certain SUS space?,"The property US (""Unique Sequential limits"") is a classic example of property implied by $T_2$ and implying $T_1$ . In fact, it's the weakest assumption out of a chain of several distinct properties studied in the literature: $T_2$ $k_1$ -Hausdorff KC Weakly Hausdorff $k_2$ -Hausdorff (or $k_2H$ ) US $T_1$ During a recent meeting of the Carolinas topology seminar, some comments of Alan Dow got me to thinking about another intermediate property. We say that a long sequence is a continuous function $f:\kappa\to X$ for an infinite cardinal $\kappa$ , and it has a limit $x\in X$ provided for every neighborhood of $x$ , the neighborhood contains $f[\kappa\setminus\alpha]$ for some $\alpha<\kappa$ . Then let's call a space SUS (""strongly US"") if whenever a long sequence has a limit, this limit is unique. It's immediate that all SUS spaces are US. Furthermore, every $k_2H$ space $X$ is SUS: let $x,y$ be limits of a sequence $f:\kappa\to X$ . Then consider the compact Hausdorff space $K=(\kappa+1)\times\{0,1\}$ and the continuous function $g:K\to X$ defined by $g(\alpha,i)=f(\alpha)$ , $g(\kappa,0)=x$ , and $g(\kappa,1)=y$ . Note that there do not exist open neighborhoods $U,V$ of $(\kappa,0),(\kappa,1)$ with $f[U],f[V]$ disjoint, so it follows that $f(\kappa,0)=f(\kappa,1)$ and $x=y$ . The standard example of a US-not- $k_2H$ space is $\omega_1+1$ with the endpoint doubled : it's US as only trivial $\omega$ -length sequences converge to $\omega_1$ , but it fails to be SUS as the identity on $\omega_1$ is a long sequence with two distinct limits at the doubled endpoint. So, is it possible to construct a SUS space that's not $k_2H$ ? Let's call such a space an imposter . EMERGENCY MEETING: This space seems pretty SUS: $X=[0,1]\cup\{\infty\}$ where points of $[0,1]$ have their usual neighborhoods, and neighborhoods of $\infty$ must contain an open dense subset of $[0,1]$ . Of course its open subspace $[0,1]$ is definitely SUS: it's metrizable and thus Hausdorff. So it's sufficient to show that if $x\in[0,1]$ is a limit of a long sequence, $\infty$ is not a limit of that sequence. Let $x\in[0,1]$ be the limit of a long sequence given by $f:\kappa\to X$ . Then for each $n<\omega$ , there exists $\alpha_n<\kappa$ such that $B(x,1/2^n)$ contains $f[\kappa\setminus\alpha_n]$ . Suppose $\sup_{n<\omega}\alpha_n=\alpha<\kappa$ . Then $f[\kappa\setminus\alpha]=\{x\}$ , and $X\setminus\{x\}$ is a neighborhood of $\infty$ missing $f[\kappa\setminus\alpha]$ . Suppose $\sup_{n<\omega}\alpha_n=\kappa$ . Then $f\upharpoonright\{\alpha_n:n<\omega\}$ is a countable sequence converging to $x$ in $[0,1]$ . Then $\{\alpha_n:n<\omega\}$ is a nowhere dense subset of $[0,1]$ , so $X\setminus\{\alpha_n:n<\omega\}$ is a neighborhood of $\infty$ that fails to contain a final subsequence of $f$ . However, while I can tell that this space is not weakly Hausdorff (it contains a non-closed copy of the compact Hausdorff space $[0,1]$ ), it's not clear to me whether or not it is $k_2H$ (and therefore whether or not it is an imposter). Update: This space is not an imposter. Whoops. Here's why it's $k_2H$ . Let $K$ be compact Hausdorff, let $f:K\to X$ be continuous, and let $f(l)\not=f(k)$ . To check the $k_2H$ criterion, we need only consider the case where $f(l)=\infty$ as pairs of points in $[0,1]$ are separated by open sets in $X$ . Let $V$ be an open neighborhood of $l$ whose closure is contained in $f^\leftarrow[X\setminus\{f(k)\}]$ ; in particular, so $f(k)\not\in f[cl(V)]$ . It follows that $cl(V)$ is compact and thus $f[cl(V)]$ is compact. We claim it is closed in $X$ , so let $x$ be a limit point. If $x=\infty$ , $x=f(l)$ and we're done. Otherwise, pick $x_n\in B(x,1/2^n)\cap f[cl(V)]$ for each $n<\omega$ . Then the open collection $\{X\setminus\{x_n:N\leq n<\omega\}:n<\omega\}$ covers $X\setminus\{x\}$ but has no finite subcover for $\{x_n:n<\omega\}$ . It follows it cannot be a cover of $f[cl(V)]$ , showing $x\in f[cl(V)]$ . Since $f(k)\not\in f[cl(V)]$ , let $U=f^\leftarrow[X\setminus f[cl(V)]]$ . We have now obtained open neighborhoods $U,V$ for $k,l$ such that $f[U]\cap f[V]\subseteq (X\setminus f[cl(V)])\cap f[cl(V)]=\emptyset$ . Sidenote: after presenting related work to the Pitt topology seminar earlier today, Paul Gartside pointed out that perhaps a more natural definition for a ""strong US"" property would involve something like continuous functions from arbitrary Hausdorff spaces, rather than ordinals, and then consider unique limits (where a limit must have neighborhoods with co-compact intersection with the image). If we called this SUS', then we'd have $k_2H$ implies SUS' implies SUS. I think it's more likely for SUS' to be equivalent to $k_2H$ than the ordinal-focused SUS discussed here, but there's something else for folks to consider thinking about.","['separation-axioms', 'general-topology', 'compactness']"
4778096,Ramanujan's identity concerning a quotient of Dedekind's eta functions,"In his paper On certain Arithmetical Functions (published in Transactions of the Cambridge Philosophical Society , XXII, No. 9, 1916, pp. 159-184) Ramanujan  presents the following identities (as if they are quite well known and rather obvious) \begin{align} 
f(q) &= q^{1^2/24}-q^{5^2/24}-q^{7^2/24}+q^{11^2/24}+\dots\tag{1}\\
f^3(q)&=q^{1^2/8}-3q^{3^2/8}+5q^{5^2/8}-7q^{7^2/8}+\dots\tag{2}\\
\frac{f^5(q)}{f^2(q^2)}&=q^{1^2/24}-5q^{5^2/24}+7q^{7^2/24}-11q^{11^2/24}+\dots\tag{3}\\
\frac{f^5(q^2)}{f^2(-q)}&=q^{1^2/3}-2q^{2^2/3}+4q^{4^2/3}-5q^{5^2/3}+\dots\tag{4}
\end{align} where $$f(q) =q^{1/24}(1-q)(1-q^2)(1-q^3)\dots\tag{5}$$ and $1,2,4,5, \dots$ are the natural numbers without the multiples of $3$ and $1,5,7,11,\dots$ are the natural odd numbers without the multiples of $3$ . The function $f(q) $ is Dedekind's eta function and due to the presence of 24th root the symbol $f(-q) $ is confusing. What is really meant by $f(-q) $ is the expression $$q^{1/24}(1-(-q))(1-(-q)^2)(1-(-q)^3)\dots$$ so that we change the sign of $q$ in factors other than $q^{1/24}$ . The formulas $(1),(2)$ are really well known as Euler's pentagonal theorem and Jacobi identity respectively and both of these can be derived using the Jacobi triple product identity $$\prod_{n=1}^{\infty}(1-q^{2n})(1+q^{2n-1}z)(1+q^{2n-1}z^{-1})=\sum_{n=-\infty}^{\infty}z^nq^{n^2}\tag{6}$$ Formula $(3)$ was recently discussed in this question and a proof based on Quintuple product identity is given in one of the answers. The formula $(4)$ is the one which is left to prove and the similarity of its left hand side with that of $(3)$ indicates that it may be amenable to Quintuple product identity as well. However I have been unable to establish it so far using Quintuple product identity. The identity is given in the form $$\frac{(\eta(3z)\eta(12z))^2}{\eta(6z)}=\sum_{n=1}^{\infty}\left(\frac{n}{3}\right) nq^{n^2}\tag{7}$$ (where $q=\exp(2\pi i z) $ and $\eta(z) =f(q)$ ) as a part of a larger set of identities in the paper Eta-quotients and theta functions by Robert J. Lemke Oliver and it is established using the theory of modular forms. I am searching for a proof which avoids modular forms and is of a more elementary character.","['q-series', 'theta-functions', 'sequences-and-series']"
4778112,"$X$ is sub-Gaussian, then $X^2$ is sub-exponential","My goal is to show the following statement without using sub-exponential or sub-Gaussian norm; Let $X$ be a zero-mean sub-Gaussian random variable with the variance proxy $\sigma^2$ . Then $X^2$ is sub-exponential with parameters $(\nu, \alpha) = (16\sigma^2, 16\sigma^2)$ . For reference, the original material is on this site (complete lecture note) in lemma 1.12. Also, $X \in SG(\sigma^2)$ with $E[X] = \mu$ means sub-Gaussian and variance proxy $\sigma^2$ ; $$E[e^{t(X-\mu)}] \le e^{\frac{\sigma^2 t^2}{2}}, \forall t \in \mathbb R,$$ and $Z \in SE(\nu, \alpha)$ with $E[z] = \theta$ means sub-exponential with nonnegative parameter $(\nu, \alpha)$ ; $$E[e^{t(Z-\theta)}] \le e^{\frac{\nu^2 t^2}{2}}, \forall |t| < 1/\alpha.$$ Here is the partial proof; Let $Z = X^2 - E[X^2]$ for simplicity. Then, for $s \in \mathbb R$ , $$e^{sZ} = \sum_{k = 0}^{\infty} \frac{(sZ)^k}{k!} = 1 + \frac{sZ}{1!} + \frac{(sZ)^2}{2!} + ...$$ Using this property, the proof goes as follows; $$\begin{align}
E[e^{sZ}] &= 1 + 0 +  \sum_{k = 2}^{\infty} \frac{s^kE[Z^k]}{k!} \sim (1) \\ 
&= 1 +  \sum_{k = 2}^{\infty} \frac{s^kE[(X^2 - E[X^2])^k]}{k!} \sim (2) \\ 
& \le 1 +  \sum_{k = 2}^{\infty} \frac{s^k2^{k‚àí1}(E[X^{2k}]+(E[X^2])^k)}{k!} \sim (3) \\ 
& \le  1 +  \sum_{k = 2}^{\infty} \frac{s^k4^kE[X^{2k}]}{2(k!)} \sim (4)\\ 
& \le 1 +  \sum_{k = 2}^{\infty} \frac{s^k4^k2(2\sigma^2)^k k!}{2(k!)} \sim (5)\\ 
&= 1 + (8s\sigma^2)^2\sum_{k = 0}^{\infty} (8s\sigma^2)^k \sim (6) \\ 
&= 1 + 128s^2\sigma^4 \sim (7)\\ 
& \le exp(128s^2\sigma^4) \sim (8)\\ 
\end{align}
$$ for $ |s| \le \frac{1}{16\sigma^2} $ . Now, let me explain what I can't understand for sure. It claims that (2) -> (3) holds due to the Jensen's inequality , but I can't obtain the inequality using Jensen. (for reference, (4) -> (5) holds due to the property that if $X \in SG(\sigma^2)$ , then $E[|X|^k] \le (2\sigma^2)^{k/2}k\Gamma(k/2)$ , which is on the lemma 1.4 of the site) Any help about this proof would be grateful. Thank you.","['statistics', 'jensen-inequality', 'probability-theory', 'probability']"
4778159,Solving a floored recurrence relation $F_n = c \cdot \left\lfloor \frac{F_{n-1}}{d} \right\rfloor$,"I initially wanted to solve the following recurrence relation: $$F_n = c \cdot \left\lfloor \frac{F_{n-1}}{d} \right\rfloor \text{ for } F_0, c, d \in \mathbb{N} \tag{1}$$ Out of interest, I've also approached the problem more generally: \begin{align*}
F_n &= a \cdot \lfloor b \cdot F_{n-1} \rfloor + c \tag{2}
\\
C_n &= a \cdot \lceil b \cdot C_{n-1} \rceil + c, \text{ for } a,b,c \in \mathbb{Q}^+
\end{align*} I wish to solve these generally. Special case for (1) where $d|c$ and $c|d$ A special case for (1) where $c | d$ and $d|c$ can be found relatively easily. If $d | c$ , then: $$F_n = c \left(\frac{c}{d}\right)^{c-1} \left\lfloor \frac{F_0}{d} \right\rfloor$$ If $c|d$ , then: $$F_n = c \left\lfloor \left(\frac{c}{d}\right)^{n-1} \frac{F_0}{d} \right\rfloor$$ The first is proven by using $d | c \implies \frac{c}{d} \in \mathbb{Z}$ , and $\forall n,m \in \mathbb{Z} \left(\lfloor nm \rfloor = nm\right)$ on: $$F_n = c \cdot \left\lfloor \frac{c}{d} \cdot  \left\lfloor \frac{F_{n-2}}{d} \right\rfloor \right\rfloor = c \cdot \frac{c}{d} \cdot  \left\lfloor \frac{F_{n-2}}{d} \right\rfloor $$ The second is proven using $c|d \implies \exists n \in \mathbb{Z} \left(\frac{d}{c} = \frac{1}{n}\right)$ and the property $\left\lfloor \frac{\lfloor x / m \rfloor}{n} \right\rfloor = \left\lfloor \frac{x}{mn} \right\rfloor$ : $$F_n = c \cdot \left\lfloor \frac{c}{d} \cdot  \left\lfloor \frac{F_{n-2}}{d} \right\rfloor \right\rfloor = c \cdot \left\lfloor \frac{c}{d} \cdot  \frac{F_{n-2}}{d} \right\rfloor $$ Example for (1) where $c=3, d=2$ The simplest case with some interesting behaviour is $c = 3$ and $d = 2$ . Despite different initial conditions $F_0$ , quite a few of sequences become the same sequence when they share the same values: e.g.: $F_0 = 4, F_1 = 6, F_2 = 9,\color{red}{F_3 = 12, F_4 = 18,\dots}$ $F_0 = 8 ,\color{red}{F_1 = 12, F_2 = 18,\dots}$ For clarity, I'll refer to the sequence $F_0, F_1, F_2, \dots$ as $\mathscr{F}(F_0)$ (e.g.: So $\mathscr{F}(4)$ is the sequence $4, 6, 9, \dots$ ) Some observations that can be made are: Given sequence $(F_i)$ the following sequences share values: $\mathscr{F}(F_i)$ $\mathscr{F}\left(\frac{2}{3}F_i\right)$ if $\frac{2}{3}F_i \in \mathbb{N}$ $\mathscr{F}(F_i + 1)$ if $F_i$ is even It also seems every $\mathscr{F}(4 + 6n)$ is always a ""new"" sequence? It seems that if $\mathscr{F}(n)$ where $n < 4 + 6n$ Then $\mathscr{F}(n)$ and $\mathscr{F}(4 + 6n)$ do not share any values. Keeping track of every time an odd number appears in $\mathscr{F}(4)$ results in A087791 . Special case for (2) A special case for (2) can be solved using Theorem 3.10 from Concrete Mathematics: Let $f(x)$ be any continuous, monotonically increasing function with the property that: $$f(x) \in \mathbb{Z} \implies x \in \mathbb{Z}$$ Then: $$\left\lfloor f(x) \right \rfloor = \left\lfloor f(\lfloor x \rfloor) \right\rfloor$$ Using $f(x) = abx + bc$ , we can prove by induction that: $$F_n = a\lfloor f^n(bF_0) \rfloor +c \text{ where } f^n(x) = f(\underbrace{f(f(\dots))}_\textrm{n times}$$ We can then prove $f^n(x_0) = (ab)^n x_0 + \frac{1-(ab)^n}{1-ab}cb$ , and obtain the special case solution: If $a,b,c \in \mathbb{R^+}$ satisfies $\left(abx + bc \in \mathbb{Z} \implies x \in \mathbb{Z}\right)$ Then $$F_n = a \left\lfloor (ab)^{n-1} bF_0 + \frac{1-(ab)^{n-1}}{1-ab}cb \right\rfloor + c $$ Admittedly, I'm not quite happy with this result as the conditions are incredibly restrictive. It also does not solve my original problem (even for cases $d|c$ or $c|d$ ?) Are there general solutions to my problem?","['ceiling-and-floor-functions', 'recurrence-relations', 'discrete-mathematics']"
4778204,Proof of Poincar√© duality,"I am working through a  proof of Poincar√© duality. I don't understand the one step marked in bold. Let $R$ be a ring. Pick an $R$ -orientation $(o_x; x\in\mathbb{R}^m)$ of $\mathbb{R}^m$ . Pick $r\in \mathbb{N}$ and let $o_{B_r}$ be the unique $R$ -orientation of $\mathbb{R}^m$ along $B_r=\{x\in \mathbb{R}^m \vert \ \ \vert x \vert \leq r\}$ . This exists since $B_r$ is compact. We want to show that the map $$(-)\cap o_{B_r}: H^m(\mathbb{R}^m, \mathbb{R}^m\setminus B_r;R)\rightarrow H_0(\mathbb{R}^m;R)$$ is an isomorphism. Recall that for $\alpha \in H^p(X),\beta \in H^q(X), c\in H^{p+q}(X)$ we have $\langle \alpha \cup \beta, c\rangle=\langle \alpha, \beta \cap c\rangle$ . In particular $\langle \beta, o_{B_r}\rangle=\langle 1, \beta \cap o_{B_r}\rangle$ for all $\beta \in H^m(\mathbb{R}^m, \mathbb{R}^m\setminus B_r;R)$ . By the Universal Coefficient theorem the map $\kappa: H^m(\mathbb{R}^m, \mathbb{R}^m\setminus B_r;R)\rightarrow \operatorname{Hom}(H_m(\mathbb{R}^m, \mathbb{R}^m\setminus B_r;R),R), \beta \mapsto \langle \beta,-\rangle$ is an isomorphism (the Ext term vanishes by the reduced long exact sequence of the pair $(\mathbb{R}^m, \mathbb{R}^m\setminus B_r;R)$ ). We conclude that the map $(-)\cap o_{B_r}$ is an isomorphism . Why does the bijectivity of $\kappa$ together with $\langle \beta, o_{B_r}\rangle=\langle 1, \beta \cap o_{B_r}\rangle$ imply that $(-)\cap o_{B_r}$ is an isomorphism?","['poincare-duality', 'proof-explanation', 'manifolds', 'general-topology', 'algebraic-topology']"
4778254,Square to octagon dissection - how to cut the square?,"How to cut the square which tessellates to octagon using straightedge and compass? What are the exact measures of colored sides? What is the angle marked with red color? Edit (I added vertices): Edit. Acknowledgement after answers. Thanks to Jean Marie, Blue and Daniel Mathias for tackling the problem. I accepted Blue's answer for its breakthrough, but I also weighed Daniel Mathias' solution for its geometric simplicity. Initially, I didn't expect the problem to be so laborious. I invite you to my puzzle in a similar flavor of dissections, which after many years still has no answer nor proof that the solution does not exist. Variation of Haberdasher problem of Henry Dudeney - dissection of equilateral triangle into square with flipping pieces","['dissection', 'tessellations', 'geometry']"
4778272,"If $f:A\to B$ and $g:B\to A$ are injections, then there's a bijection $h:A\to B$.","Is this proof valid? I'm pretty sure the idea is correct but not sure if ive written it correctly. Theorem. If $f:A\rightarrow B$ and $g:B\rightarrow A$ are injections, then there's a bijection $h:A\rightarrow B$ . Proof. So we have injections $f:A\rightarrow B$ and $g:B\rightarrow A$ . Assume that $B\setminus \text{ran}(f)$ is not empty, i.e. $f$ is not surjective. For any $\displaystyle b\notin ran( f)$ , let its sequence of distinct $\displaystyle a$ 's be defined by: $$a_{1} =g(b)$$ $$a_{n+1} =g( f( a_{n}))$$ To see why they are distinct, consider $\displaystyle a_{i}$ and $\displaystyle a_{j}$ for $\displaystyle i< j$ . Then since $\displaystyle f$ and $\displaystyle g$ are injective, the equality $\displaystyle a_{i} =a_{j}$ can be reduced to $\displaystyle b=g( f( a_{j-i}))$ , which is not possible since $\displaystyle b\notin ran( f)$ . As an example, consider $\displaystyle a_{3} =g( f( g( f( g( f( b))))))$ and $\displaystyle a_{5} =g( f( g( f( g( f( g( f( g( f( b))))))))))$ .
Then $\displaystyle a_{3} =a_{5}$ is same as saying $\displaystyle b=g( f( g( f( b))))$ . Each $a \in A$ is either in exactly one sequence or in none of them. Define $h:A\rightarrow B$ by $$
h(a) = 
\begin{cases} 
g^{-1}(a) & \text{if $a$ in some sequence} \\
f(a) & \text{else}
\end{cases}
$$ To prove that $h$ is injective, we prove that we can't have $h(a_1) = h(a_2)$ with $h(a_1) = g^{-1}(a_1)$ and $h(a_2) = f(a_2)$ . Let $g^{-1}(a_1) = b_1$ . Now $a_1$ is an element of a sequence, so either $b_1 \notin \text{ran}(f)$ or $b_1 \in \text{ran}(f)$ . Case 1. $b_1 \notin \text{ran}(f)$ Then $b_1$ i.e. $g^{-1}(a_1) \neq f(a_2)$ . Case 2. $b_1 \in \text{ran}(f)$ Then $f^{-1}(b_1) = a'_{1}$ . Now $a'_{1} \neq a_{2}$ , since $a'_{1}$ is in a sequence but $a_{2}$ is not. That implies $f(a'_{1}) \neq f(a_{2})$ , which means $b_1$ i.e. $g^{-1}(a_1) \neq f(a_2)$ . To see that $h$ is surjective, take $b \in B$ and let $a = g(b)$ . If $a$ is in a sequence, then $h(a) = g^{-1}(a) = b$ . Otherwise $a$ is not in a sequence, so it must be that $b \in \text{ran}(f)$ . Then $f^{-1}(b) = a'$ is not in a sequence either, so $h(a') = f(a') = b$ .","['functions', 'solution-verification']"
4778311,How to check if a sphere passes through another sphere when both travels in a straight line through 3d space,"Given I know two sphere's centre coordinates and their radius. Let's say both spheres travels in some direction in a straight line. At t=0, sphere 1 is at (0, 0, 0) and sphere 2 is at (50, 0, 0), their radius is both 30. At t=1, sphere 1 is at (100, 0, 0) but sphere 2 is at (-50, 0, 0). How do I check if sphere 2 have passed through sphere 1? (Just touching also count as ""passed through""). By just touching, I mean the path the two spheres took touched but didn't overlap. I can imagine the volume of the two spheres gouging out a capsule, and if this capsule encompasses any portion of the other capsule, then sphere 2 have gone through sphere 1 (true), other wise false. But I don't know how to verify if a portion of a volume is in another volume. Scrappy visual of what I'm saying: One sphere travels from yellow to grey, another travel from maroon to purple, how do I tell if the two paths overlap at some point. Update: Emac have provided an amazing break down of the problem, but I have failed to extend it to more complex scenarios. Take this scenario: All spheres have a radius of 0.5. The centre coordinates of Red and Blue spheres at time t=0 are at (5, 2, 4) and (1, 3, 6) respectively. At t=1, the red sphere remained stationary, and the blue sphere have travelled to (6, 2, 3.5). Examine this images, where I have lined up the camera with the end and start position of the blue sphere (final position of blue sphere is now green): We can see clearly that the path of the blue sphere will intersect the red sphere at some point in time. I attempted Emac's answer on this scenario. In my mind, the formula should still work. The derivative of the squared distance between two points in 3D space, then solving for the minimum of that formula seems like a universal solution to all these scenarios, but when I applied the formula, I got t = (4, 0, -2), and I can't make sense of a time vector. I thought perhaps I needed to find the magnitude of this time vector, and that will be my answer, but it returned a time larger than 1, which also doesn't make sense. To go even further, what if both spheres are both in motion, and they each move diagonally through space, eventually grazing each other at some point. Their radius must be accounted, intersection does not mean full overlap.","['volume', 'spheres', 'geometry', '3d']"
4778450,Discrete convolution and supports: can we have $\operatorname{supp}(f * g) \subsetneq \operatorname{supp}(f) + \operatorname{supp}(g)$?,"Let $G$ be a finite abelian group. For $f,g:G\rightarrow \mathbb{C}$ functions. Define their convolution in a point $x\in G$ as $$(f*g)(x)=\sum_{y\in G}f(x-y)g(y)$$ From the definition of convolution is easy show that $$\operatorname{supp}(f*g)\subseteq \operatorname{supp}(f)+\operatorname{supp}(g)$$ When $\operatorname{supp}(f)$ denotes the set of points in $x\in G$ for which $f(x)\neq 0$ , and with similar definitions with $\operatorname{supp}(g)$ and $\operatorname{supp}(f*g)$ . But, I spent a pair of hours trying to find functions $f$ and $g$ for which the equality does not hold in general. Since I do a lot of examples in finite abelian groups of the form $\mathbb{Z}/n\mathbb{Z}$ and I always get the equality (is important remark the fact that I always use indicator functions), I'm interested in know examples in finite abelian groups for which the equality does not hold. Any suggestion or example is welcome.","['convolution', 'discrete-mathematics', 'discrete-calculus']"
4778461,Is there a good reason why continuity assumed in the definition of differentiability in do Carmo?,"I am reading Do Carmo's Differential Geometry of Curves and Surfaces. He defines differentiability as follows: A continuous map [emphasis mine] $\varphi: V_1 \subset S_1 \to S_2$ of an open set $V_1$ of a regular surface to a regular surface $S_2$ is said to be differentiable at $p \in V_1$ if, given parametrizations $$x_1 : U_1 \subset \mathbb{R}^2 \to S_1 \quad x_2 : U_2 \subset \mathbb{R}^2 \to S_2 $$ with $p \in x_1(U_1)$ and $\varphi(x_1(U_1)) \subset x_2(U_2),$ the map $$x_2^{-1} \circ \varphi \circ x_1:U_1 \to U_2$$ is differentiable at $q = x_1^{-1}(p)$ . I believe that when he says continuous, and when he says that $V_1$ is open, he is referring to the topologies inherited from $\mathbb{R}^3$ . I was wondering why he assumes $\varphi$ to be continuous in the definition. I thought that it must be an easy consequence that $\varphi$ is continuous, if you know that all maps of the form $x_2^{-1} \circ \varphi \circ x_1$ are differentiable (hence continuous). However, I am starting to think that the continuity requirement is not superfluous. I think that for the definition to be correct, we must do one of two things. Either Assume $\varphi$ is continuous. In the definition, change ""given parametrizations"" to ""there exist parametrizations"". If we do neither, then I think there can be very badly behaved, discontinuous functions $\varphi$ where it is impossible to find parametrizations $x_1, x_2$ with $\varphi(x_1(U_1)) \subset x_2(U_2)$ . Hence, $\varphi$ would vaccuously be a diffeomorphism, even though it is discontinuous. However, if we do drop continuity but make change in $2$ , I think we can prove that $\varphi$ is continuous, and we get an equivalent definition to do Carmo's. Is my analysis above correct? EDIT: Proposition: Let $\varphi$ be any function from $S_1$ to $S_2$ . If for each point $p \in S_1$ there exist parametrizations $$x_1 : U_1 \subset \mathbb{R}^2 \to S_1 \quad x_2 : U_2 \subset \mathbb{R}^2 \to S_2 $$ with $p \in x_1(U_1)$ and $\varphi(x_1(U_1)) \subset x_2(U_2)$ such that the map $$x_2^{-1} \circ \varphi \circ x_1:U_1 \to U_2$$ is differentiable at $q = x_1^{-1}(p),$ then $\varphi$ is a continuous map from $S_1 \to S_2$ (with the topologies inherited from $\mathbb{R}^3$ ). $\textit{Proof: }$ Fix $p \in S_1$ and take parametrizations $x_1, x_2,$ and the point $q$ , as given in the claim. We will show that $\varphi$ is continuous at $p$ . Let $O_{S_2}$ be any open subset of $S_2$ containing $\varphi(p)$ , and let $O_{U_2} = x_2^{-1}(O_{S_2})$ , which is open because (by definition of parametrization) $x_2$ is a homeomorphism. Now $x_2 ^{-1} \circ \varphi \circ x_1$ is differentiable at $q$ , so it is continuous at $q$ . Since $O_{U_2}$ contains $x_2 ^{-1} \circ \varphi \circ x_1(q)$ , there exists an open subset $O_{U_1}$ of $U_1$ , containing $q$ , such that $x_2 ^{-1} \circ \varphi \circ x_1 (O_{U_1}) \subseteq O_{U_2}.$ Applying $x_2$ to both sides, we get $\varphi \circ x_1 (O_{U_1}) \subseteq x_2(O_{U_2}) = O_{S_2}.$ Finally, let $O_{S_1} = x_1(U_1)$ , which is open because $x_1$ is a homeomorphism. Substituting above, we have $\varphi(O_{S_1}) \subseteq O_{S_2}$ , which shows that $\varphi$ is continuous at $p$ . Since $p$ was arbitrary, $\varphi$ is continuous on $S_1.$","['manifolds', 'differential-topology', 'differential-geometry']"
4778462,When can $\textbf{SpecMax}(R)$ be a scheme?,"Let $R$ be a commutative ring, and let $(\text{Spec(R)},\mathcal{O}_R)$ be the affine scheme associated to $R$ . Let $\text{SpecMax}(R)$ the subspace the $\text{Spec(R)}$ of the maximal ideals. My question is if there exists any conditions on $R$ such that $\text{SpecMax(R)}$ have strucutre of a scheme? Thanks","['affine-schemes', 'ring-theory', 'algebraic-geometry', 'ringed-spaces', 'schemes']"
4778466,The Probability of Two Contestants Meeting (Ross),"This problem from Ross has been giving me grief, and none of the resources I‚Äôve found have been helpful in elucidating where I‚Äôve gone astray. $2^n$ players are paired off at random in a contest where each contestant is $50\%$ likely to win. The $2^{n-1}$ winners are paired off again randomly, and so on, until a single winner remains. Consider two contestants A and B, and events $A_i, i = 1, 2, ‚Ä¶, n$ , and $E$ , defined by $A_i$ : A plays in exactly i contests. $E$ : A and B ever play each other during the course of the contest. (Not never!) What is $P(A_i)$ ? And what is $P(E)$ ? $P(A_i)$ is straightforward; $P(A_i) = (1/2)^i$ for $i=1,2,‚Ä¶,n-1$ and $P(A_n) = (1/2)^{n-1}$ . As for $P(E)$ , we can condition on the $A_i$ as they partition the space, but the trouble comes with calculating $P(E|A_i)$ . $P(E|A_1) = \frac{1}{2^n -1}$ . $P(E | A_2)$ to me seems to be $\frac{1}{2^n -1} + \frac{1}{2^n -2} \frac{1}{2}$ , as A can either meet B in round 1, or in round 2, if B wins in round 1. The formula for $P(E)$ gets messy in this case, and I‚Äôm further dissuaded from this answer by the hint that Ross provides, which is the formula $\sum_{i=1}^{n-1}ix^{i-1} = \frac{1-nx^{n-1} + (n-1)x^n}{(1-x)^2}$ Which suggests to me the possibility that $P(E|A_i) = \frac{i}{2^n -1}$ , which I could reason as A plays $i$ opponents, all of which are equally likely. But this does not arrive at the proper solution of $ P(E) = \frac{1}{2^{n-1}}$ either. What am I missing in my understanding of $P(E | A_i)$ ?","['combinatorics', 'probability']"
4778480,Expected value of surplus process at the moment of ruin,"This problem comes from actuarial exams. We consider a continuous surplus process $$ U(t) = ct - \sum_{k=1}^{N(t)} X_k,$$ where $N(t)$ is Poisson process such that $\mathbb{E}N(t)=\lambda t$ , and $X_k$ are iid random variables such that $\mathbb{P}(X_1 \in [0,10])=1$ and $\mathbb{E}X_1 = 2$ . We also know that $c> 2\lambda$ .
I am trying to prove the following inequality: $$ 1 \leq \mathbb{E}[U(\tau)|\tau<\infty] \leq 5,$$ where $\tau = \inf(t\geq0: U(\tau) < 0).$ I'm not sure where to start from. It is clear that a similar inequality holds for $0$ and $10$ instead of $1$ and $5$ . Moreover, $$\mathbb{E}[U(\tau)|\tau<\infty] = (c-2\lambda) \mathbb{E}[\tau|\tau<\infty],$$ which can suggest that the right way to approach this problem may be by investigating conditional expectation of $\tau$ . This question feels like it has a tricky way to solve it, though. I would appreciate any help.","['conditional-probability', 'stochastic-processes', 'probability-theory', 'actuarial-science']"
4778663,Finding the asymptotic of the sequence $a_{n+1}=a_n+\frac{1}{f(a_n)}$,"We define $f(x)$ be a differentiable function with $f^\prime\ge 0$ , $f^\prime$ bounded and $f\to+\infty$ when $x\to+\infty$ . Define the sequence $\{a_n\}$ as follows: $$a_0=1, a_{n+1}=a_n+\frac{1}{f(a_n)},n\in\mathbb{N}_+.$$ My question is, how can we find the asymptotic of $a_n$ ? For instance, if $f(x)=x$ , then clearly $f(x)$ satisfies the conditions and the sequence turned out to be $$a_{n+1}=a_n+\frac{1}{a_n},$$ which we may give an estimate that $a_n=\sqrt{2n}+o(\sqrt{2n})$ . However, what about a randomly chosen $f$ ? How can we compute the asymptotic of $\{a_n\}$ ? Thanks in advance.","['closed-form', 'recurrence-relations', 'sequences-and-series']"
4778666,Solve the integral $\int_0^1 \frac{\ln^2(x+1)-\ln\left(\frac{2x}{x^2+1}\right)\ln x+\ln^2\left(\frac{x}{x+1}\right)}{x^2+1} dx$,"I tried to solve this integral and got it, I showed firstly $$\int_0^1 \frac{\ln^2(x+1)+\ln^2\left(\frac{x}{x+1}\right)}{x^2+1} dx=2\Im\left[\text{Li}_3(1+i) \right] $$ and for other integral $$\int_0^1 \frac{\ln\left(\frac{2x}{x^2+1}\right)\ln x}{x^2+1} dx=\int_0^1 \frac{\left(\ln2+\ln x\right)\ln x}{x^2+1} dx-\int_0^1 \frac{\ln\left(x^2+1\right)\ln x}{x^2+1} dx$$ then by using series and its value $$ \int_0^1 \frac{\left(\ln2+\ln x\right)\ln x}{x^2+1} dx=-G\ln2+\frac{\pi^3}{16}$$ $$ \int_0^1 \frac{\ln\left(x^2+1\right)\ln x}{x^2+1} dx=\sum_{k=1}^{\infty}(-1)^{n-1}H_n \int_0^1x^{2n}\ln x dx $$ $$=\sum_{k=1}^{\infty} \frac{(-1)^{n} H_n}{(2n+1)^2} =2\Im\left[\text{Li}_3(1+i) \right]+\frac{3}{32}\pi^3+\frac{\pi}{8}(\ln2)^2-G\ln2$$ then got the result $$\int_0^1 \frac{\ln^2(x+1)-\ln\left(\frac{2x}{x^2+1}\right)\ln x+\ln^2\left(\frac{x}{x+1}\right)}{x^2+1} dx=\frac{\pi}{8} \left(\frac{\pi^2}{4}+(\ln2)^2\right)$$ MY QUESTION How can I get the result with simple way without using complicated constants and values of some series which are long to prove it?","['integration', 'logarithms', 'harmonic-numbers', 'polylogarithm', 'catalans-constant']"
4778683,When wedge power of a vector nulls,"Let $l$ be even and $v \in \bigwedge^\ell \mathbb{C}^{\ell n}$ . What conditions on vector $v$ imply $v^{\wedge n} \neq 0$ ? Note, that for $\ell = 2$ the criteria is that $v$ must be non-degenerate (skew-symmetric bilinear form). Is there any reference of idea how one can approach at least the implication ""condition on $v$ $\implies$ $v^{\wedge n}$ not nulls""?","['differential-geometry', 'complex-geometry', 'linear-algebra', 'differential-forms', 'exterior-algebra']"
4778694,Looking for (overkill) usages of indicator functions,"I am going to give a presentation about the indicator functions , and I am looking for some interesting examples to include. The examples can be even an overkill solution since I am mainly interested in demonstrating the creative ways of using it. I would be grateful if you share your examples. The diversity of answers is appreciated. To give you an idea, here are my examples. Most of my examples are in probability and combinatorics so examples from other fields would be even better. Calculating the expected value of a random variable using linearity of expectations. Most famously the number of fixed points in a random permutation. Showing how $|A \Delta B| = |A|+|B|-2|A \cap B|$ and $(A-B)^2 = A^2+B^2-2AB$ are related. An overkill proof for $\sum \deg(v) = 2|E|$ .","['characteristic-functions', 'big-list', 'combinatorics', 'soft-question', 'probability']"
4778733,A property related to cofinal similarity,"A partial order $\le$ is reflexive, transitive and antisymmetric. It is directed , if for all $x, y$ in the set there is $z$ such that $x \le z$ and $y \le z$ . Let $X, Y$ be partially ordered, directed sets. A subset $T$ of $X$ is called cofinal (in $X$ ), if $\forall x \in X \space \exists t \in T \space (x \le t)$ . $\text{cof}(X) := \text{min} \{|T|: T \text{ is cofinal in } X\}$ . $X, Y$ are called cofinally similar , if there is a partially ordered set $Z$ in which both X and Y can be embedded as cofinal subsets.
In this case, $\text{cof}(X) = \text{cof}(Y)$ .
Of course, if $T$ is a cofinal subset of $X$ , then $X, T$ are cofinally similar. This is a well-treated concept, see for instance here and the references given in this paper.
It is relevant for many other branches, for instance topology or the theory of ultrafilters.
This paper mentions an equivalent statement to cofinal similarity in terms of certain maps. This immediately implies that cofinal similarity is transitive. Now let's define: $X, Y$ are strongly cofinally similar , if there is a partially ordered set $T$ , such that $T$ can be embedded into both, $X$ and $Y$ , as a cofinal subset. Obviously, if $X, Y$ are strongly cofinally similar, they are cofinally similar.
This gives rise to the following questions: Does the converse hold, i.e., if $X,Y$ are cofinally similar, are they strongly cofinally similar? (formerly question 3) Does the converse hold in certain classes, at least? Notes Of course, 2. is only relevant, if 1. is false. A partial answer to 2. is: yes, if $X$ is cofinally similar to a linearly ordered set.
(Since in this case, the regular cardinal $\text{cof}(X)$ embedds into $X$ and $Y$ as a cofinal subset.) It should be noted that the answer to 1 is affirmative, if and only if strong cofinal similarity is transitive. Thanks to M W for pointing this out. See his comment below for the simple argument. Consistently , each directed, non-empty set of size $\le \aleph_1$ is cofinally similar to one of these five sets, each equipped with its standard (partial) order: $1, \omega, \omega_1, \omega_1 \times \omega, [\omega_1]^{< \omega}$ . See the above-mentioned paper for the (very complicated) proof (where it is easy to see that these five ones are pairwise not cofinally similar).
Thus, a reasonable source for a counter-example might be a directed set cofinally similar to $\omega_1 \times \omega$ or to $[\omega_1]^{< \omega}$ . Update Why am I asking this question? I'm mainly interested in topology. In this area, it's reasonable to compare neighborhood bases just by their cofinality type (= equivalence classes of cofinal similarity). Here, of course, we consider the ordering defined by reverse set inclusion. For instance, any open neighborhood base is strongly cofinally similar to the neighborhood base. Or, more interestingly, if $x$ is a point in a dense subset $D$ of a regular topological space $X$ , the open neighborhood bases of $x$ in $D$ and in $X$ are strongly cofinally similar. P-points or points of first countability are completely characterized by the cofinality type of their neighborhood base. These statements may already motivate question 1: Is it really necessary to distinguish between cofinal similarity and the strong one?
And: can we use the benefits of transitivity to conclude that the neighborhood bases are strongly cofinally similar in the dense subset mentioned above? Update 2023-10-04 Finally, I found a counter-example, see below.","['filters', 'order-theory', 'general-topology']"
4778755,Can Orthogonal Trajectories have Multiple Solutions? And how do I know it's correct as an autodidact without teacher?,"TLDR: Are there different solutions to orthogonal trajectories, and if so, how can I as an autodidact check whether I'm doing things correctly or not? The reason I'm asking this is because the textbook and the solution of someone with a PhD in Mathematics have different answers (or at least, someone claiming to have it). I'll show both and then my approach. Problem: find the orthogonal trajectory of $r = c(1 + \cos(Œ∏))$ Textbook solution without work: $r = c(1 - \cos(Œ∏))$ PhD Mathematics solution: $$(1)\ \frac{dr}{dŒ∏} = -c\sin(Œ∏)$$ $$(2)\ \frac{1}{r}\frac{dr}{dŒ∏} = -\frac{c}{r}\sin(Œ∏)$$ $$(3)\ \frac{1}{r}\frac{dr}{dŒ∏} = \frac{r}{c\sin(Œ∏)}$$ $$(4)\ \frac{dr}{r^2} = \frac{1}{c}\csc(Œ∏)dŒ∏$$ Solve r by seperating variables $$(5)\ \frac{dr}{r^2} = \frac{1}{c}\csc(Œ∏)dŒ∏$$ After integrating both sides $$(6)\ -\frac{1}{r} = -\frac{1}{c}ln|\csc(Œ∏) + \cot(Œ∏)|- C$$ $$(7)\ r = -\frac{1}{\frac{1}{c}ln|\csc(Œ∏) + \cot(Œ∏)| + C}$$ Which is a different answer. Now I'm writing down how I did it. My Solution: $$(1)\ c = \frac{r}{1+\cos(Œ∏)}$$ $$(2)\ c^{-1}r = 1 + \cos(Œ∏)$$ $$(3)\ \frac{d}{dr}(c^{-1}r) = (1 + \cos(Œ∏))\frac{d}{dr}$$ $$(4)\ c^{-1} = -\sin(Œ∏)\frac{dŒ∏}{dr}$$ $$(5)\ -\frac{1}{c\sin(Œ∏)} = \frac{dŒ∏}{dr}$$ After substituting into c, multiplying both sides by r to get into the right form and simplifying we get $$(6)\ -\frac{1+\cos(Œ∏)}{\sin(Œ∏)} = -\frac{dr}{rdŒ∏}$$ $$(7)\ \frac{\sin(Œ∏)}{1+\cos(Œ∏)} = \frac{rdŒ∏}{dr}$$ $$(8)\ \frac{1+\cos(Œ∏)}{\sin(Œ∏)}dŒ∏ = \frac{1}{r}dr$$ $$(9)\ \int \frac{1}{\sin(Œ∏)} + \frac{\cos(Œ∏)}{\sin(Œ∏)}dŒ∏ = \int \frac{1}{r}dr$$ After integration $$(10)\ Œ∏\arcsin(Œ∏) + \sqrt{1-Œ∏^2} + ln|\sin(Œ∏)| + C_1 = \ln|r| + C_2$$ After isolating $$(11)\ r = e^{Œ∏\arcsin(Œ∏) + \sqrt{1-Œ∏^2} + ln|\sin(Œ∏)| + C}$$ My approach looks really bad, but its more about whether different solutions are possible and if it is how I will know whether my solution is a correct one. I'm self-studying and the books I'm using didn't mention multiple solutions being possible, so I'm a bit lost.","['trigonometry', 'ordinary-differential-equations']"
4778759,Range of $(\tan^{-1}x)^3+(\cot^{-1}x)^3$,"Finding range of function $$\ \ f(x)=(\tan^{-1}(x))^3+(\cot^{-1}(x))^3$$ I am trying to solve above using Inequality We know that $\tan^{-1}(x),\cot^{-1}(x)$ is defined for $x\in(-\infty,\infty)$ Using Inequality $\displaystyle \frac{(\tan^{-1}(x))^3+(\cot^{-1}(x))^3}{2}\geq \bigg(\frac{\tan^{-1}(x)+\cot^{-1}(x)}{2}\bigg)^3=\frac{\pi^3}{64}$ So we have $\displaystyle \bigg(\tan^{-1}(x)\bigg)^3+\bigg(\cot^{-1}(x)\bigg)^3 \geq \frac{\pi^3}{32}$ Equality hold when $\displaystyle x=1$ Also at $\displaystyle f(-\infty)= -\frac{\pi^3}{8}$ Also at $\displaystyle f(\infty)=\frac{\pi^3}{8}$ So range of function as $\displaystyle \bigg[\frac{\pi^3}{32},\frac{\pi^3}{8}\bigg)$ But in wolframalpha , it shows answer as $\displaystyle \bigg(-\frac{\pi^3}{8},-\frac{\pi^3}{32}\bigg]\cup \bigg[\frac{\pi^3}{32},\frac{\pi^3}{8}\bigg)$ Please have a look on that problem, Thanks","['trigonometry', 'functions']"
4778794,On sub and super solutions; Teschl and others,"I'm reading Ordinary Differential Equations by Andersson and B√∂iers. There is a comparison theorem I have some questions about. I have also checked Teschl's Ordinary Differential Equations and Dynamical Systems, but there I have problems with his definition of a sub solution. I'll elaborate below. What follows is the comparison theorem in the book I first stated: Theorem . Assume that $f(t,x)$ is a continuous function in the strip $\{(t,x); t_0\leq t\leq t_1\}$ and satisfies a Lipschitz condition in a neighborhood of every point there. Furthermore, assume that $x(t)$ and $y(t)$ satisfy $$x'(t)=f(t,x)\quad\text{and}\quad y'(t)\geq f(t,y)$$ respectively, when $t_0\leq t\leq t_1$ . Then $$x(t_0)=y(t_0)\implies x(t)\leq y(t)\quad\text{when }t_0\leq t\leq t_1.$$ This definition is not made in the book, but I guess $y(t)$ is called a super solution. What confuses me in this theorem are the inequalities and how the theorem is modified when we change some of the inequalities to strict inequalities. First, I assume a corresponding result holds for a function $w(t)$ that satisfies $w'(t)\leq f(t,w)$ , so that $x(t_0)=w(t_0)\implies x(t)\geq w(t)$ when $t_0\leq t\leq t_1$ , right? Second, I'm working a problem where a function $y(t)$ satisfies $y'(t)> f(t,y)$ on a half-open strip, i.e. $t_0\leq t<t_1$ (because it is undefined at $t_1$ ). So how is the conclusion of the theorem modified if we change the assumptions to $y'(t)> f(t,y)$ and a half-open strip? Finally, in Teschl's book, he defines a sub solution $w(t)$ to be a function that satisfies $w'(t)< f(t,w)$ for $t_0\leq t<t_1$ . However, in my problem, I have a function $w(t)$ that satisfies $w'(t)\leq  f(t,w)$ for $t_0\leq t<t_1$ (in particular, $w'(t_0)=f(t_0,w(t_0))$ . Is this not a sub solution then? For completion, I post the proof of the theorem here. You can skip this of course. It uses the following lemma, stated without proof for the sake of brevity; Lemma . Let $x(t)$ be a differentiable function such that $$x'(t)\leq
> Mx(t)+a,$$ where $M\neq 0$ and $a$ are fixed constants. Then $$x(t)\leq e^{M(t-t_0)}x(t_0)+\frac{a}{M}(e^{M(t-t_0)}-1),\quad t\geq
> t_0.$$ Proof (of theorem). Assume that there is some point $\tau$ in the interval $[t_0,t_1]$ where $x(\tau)>y(\tau)$ . Then let $\bar t$ be the largest $t$ in $[t_0,\tau]$ with $x(t)\leq y(t)$ . Put $z(t)=x(t)-y(t)$ . Then $z(t)>0$ in $(\bar t,\tau]$ and $z(\bar t)=0$ . Furthermore, for $t$ near $\bar t$ , $$z'(t)=x'(t)-y'(t)\leq f(t,x(t))-f(t,y(t))\leq L(x(t)-y(t))=Lz(t).$$ The first inequality comes from the assumptions on $x(t)$ and $y(t)$ , the second one makes use of the Lipschitz condition. [The] lemma (with $a=0$ ) now implies, for $t$ in a right neighborhood of $\bar t$ , $$z(t)\leq e^{L(t-\bar t)}z(\bar t)=0.$$ We have arrived at a contradiction.",['ordinary-differential-equations']
4778821,Prove that $PA^2+PB^2+PC^2+PH^2=4R^2.$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 9 months ago . Improve this question A $\Delta ABC$ has orthocentre $H,$ nine-point centre $N$ and circumradius $R.$ For any point $P$ on its nine-point circle, prove that $$PA^2+PB^2+PC^2+PH^2=4R^2.$$ I can prove that $NA^2+NB^2+NC^2+NH^2=3R^2.$ But don‚Äôt know how to proceed further. How does the nine-point circle come into play? Any kind of help/suggestions will be highly appreciated.","['triangles', 'trigonometry', 'circles', 'geometry']"
4778850,Is there a name for the fact that an automorphism of a finite set produces periodical trajectories?,"Let S be a finite set. f a bijection over S. If p is a point of S, I call ""trajectory"" of p the list ( $f^0(p)$ , $f^1(p)$ , $f^2(p)$ , $f^3(p)$ ... etc). It appears obvious to me that under such conditions any trajectory would be periodical. Is that correct? Is there a name for this property?",['abstract-algebra']
4778869,Integral Residue Calculation,"Integral: $I =\int^{\infty}_{-\infty}\frac{\cos x-1}{x^2(x^2+a^2)}\mathrm dx$ where $a \in \mathbb{R}$ and $a > 0$ . Method 1: We observe that there is a removable singularity at $x=0$ . Thus a semicircle contour in the upper plane centered at $z=0$ and with a sufficiently large radius $R$ is used to calculate the integral. We express $cosx$ as Re $e^{iz}$ . A simple calculation of the residue shows that $I = \frac{\pi  (1-e^{-a})}{a^3}$ . Method 2: The same as Method 1, instead we directly evaluate the residue without expressing $cosx$ as Re $e^{iz}$ . It follows that $I = \frac{\pi (1-\cosh a)}{a^3}$ . Method 3: Express $cosx$ as Re $e^{iz}$ in the first place. Let $f(z)=\frac{e^{iz}-1}{z^2(z^2+a^2)}$ , then $I = $ Re $ \int^{\infty}_{-\infty}f(z)\mathrm dz $ . Observe that there is a simple pole at $z = 0$ . Use the following contour $C$ where $R$ sufficiently large and $\epsilon$ sufficiently small. Residue calculation shows that $J = \int_C f(z)\mathrm dz = \frac{\pi(1-e^{-a})}{a^3}$ . $I = J - \int_{C_{\epsilon}} f(z)\mathrm dz$ , the latter one equals $\frac{\pi}{a^2}$ via another residue argument. Then $I = \frac{\pi (1-e^{-a})}{a^3}-\frac{\pi}{a^2}$ . Question: The results of 3 different approaches are different. WolframAlpha tells me that Method 3 yields the correct result, but I have no idea why the other two methods are incorrect. Please help point out the mistakes I made in Method 1 and Method 2.","['complex-analysis', 'residue-calculus']"
4778893,Conditional Gaussian Comparison Inequality,"I am trying to prove a conditional gaussian comparison inequality. Specifically, I am looking to prove that for two $n$ -dimensional Gaussian variables $X_1,\cdots,X_n$ and $Y_1,\cdots,Y_n$ , assume that $\mathbb{E}[X_i]=\mathbb{E}[Y_i]=0$ , $\mathbb{E}[X_i^2]=\mathbb{E}[Y_i^2]$ , $\mathbb{E}[X_iX_j]\leq \mathbb{E}[Y_iY_j]$ , we have for all $u_1,\cdots, u_{n-1} < 0$ : $$\mathbb{P}\left[\bigcap_{i=1}^{n-1} \{X_i \geq u_i \} \mid  X_n=0 \right]\leq \mathbb{P}\left[\bigcap_{i=1}^{n-1} \{Y_i \geq u_i \}\mid  Y_n=0 \right]$$ Intuitively this makes sense because $Y_i$ are more correlated with each other and thus should have a lower tail probability of being all far away from zero, but I have having a hard time proving it rigorously. I know Kahane's inequality and thought it can get me there, but the fact that I am conditioning on $X_n/Y_n=0$ makes a direct application difficult, so I was wondering if I am overlooking an easier approach.","['normal-distribution', 'probability-theory', 'random-variables']"
4778909,"The space of all square matrices with the same distinct real eigenvalues is a smooth submanifold of $\mathrm{Mat}(n,\mathbb{R})$","Problem: Let $\lambda_1,\dots,\lambda_n$ be distinct real numbers; set $\Lambda$ to be the matrix with $\Lambda_{ii}=\lambda_i$ and $$H = \{Q\in\mathrm{Mat}(n,\mathbb{R}):Q\text{ has eigenvalues }\lambda_1,\dots,\lambda_n\}\\=\{Q\in\mathrm{Mat}(n,\mathbb{R}):Q\text{ is similar to }\Lambda\}$$ I want to show that $H$ is a smooth submanifold of $\mathrm{Mat}(n,\mathbb{R})$ . Previous Work: Since the $\lambda_i$ are distinct, then any two matrices are similar to $\Lambda$ if and only if they have the same distinct eigenvalues $\lambda_1,\dots,\lambda_n$ . Let $G=\mathrm{GL}(n,\mathbb{R})$ and $M=\mathrm{Mat}(n,\mathbb{R})$ . Consider the smooth left action $\alpha:G\times M\to M$ given by conjugation, i.e., $\alpha(g,m)=gmg^{-1}$ . Considering the orbit of $\Lambda$ , we note that $H=G\cdot\Lambda$ . I am trying to show that $H$ is an embedded submanifold via a more direct approach: First, I want to show that $\alpha_{\Lambda}:G\to H\subset M$ is an immersion. Here is my attempt: Fix $g\in G$ and $X\in \mathrm{T}_gG=M$ . Let $\gamma_g:\mathbb{R}\to G$ be the curve given by $\gamma_g(t)=ge^{g^{-1}X}$ so that $\gamma_g(0)=g$ and $\gamma_g'(0)=X$ . We may use this curve to compute the differential of $\alpha_{\Lambda}$ at $X$ . Working with $\alpha_{\Lambda}\circ\gamma_g$ first, we have \begin{align*}
(\alpha_{\Lambda}\circ\gamma_g)(t)
&=\gamma_g(t)\Lambda\gamma_g(t)^{-1}\\
&=\gamma_g(t)\Lambda\gamma_g(-t)\\
&=ge^{g^{-1}Xt}\Lambda ge^{-g^{-1}Xt}\\
&=e^{Xg^{-1}t}\alpha_{\Lambda}(g)e^{-Xg^{-1}t}
\end{align*} so that \begin{align*}
\mathrm{D}_g\alpha_{\Lambda}(X)=\frac{\mathrm{d}}{\mathrm{d}t}e^{Xg^{-1}t}\alpha_{\Lambda}(g)e^{-Xg^{-1}t}\bigg|_{t=0}=Xg^{-1}\alpha_{\Lambda}(g)-\alpha_{\Lambda}(g)Xg^{-1}
\end{align*} Now, for this differential to be injective everywhere, i.e., $\mathrm{D}_g\alpha_{\Lambda}(X)=0\implies X=0$ , we have the condition $$Xg^{-1}\alpha_{\Lambda}(g)=\alpha_{\Lambda}(g)Xg^{-1}$$ $$\text{ or equivalently}$$ $$g^{-1}X\Lambda = \Lambda g^{-1}X$$ that is, $g^{-1}X$ commutes with $\Lambda$ . Since $\Lambda$ is a diagonal matrix with distinct entries, then $g^{-1}X$ has to be a diagonal matrix. This is where my progress stops. Alternative: I have found this math stack exchange post outlining a proof of a more general statement, that if Lie group acts smoothly on a manifold, then the orbits are immersed submanifolds. Moreover, if the action is proper, then the orbits are embedded submanifolds. The question I would have in this case is: how do I show that conjugation is a proper action? Question: I would be willing to give up on my previous work above, but not without asking someone else first. I am asking for a second look by someone with some more insight. Am I making an error anywhere in my previous work? Is there some connection I am not making to show that $X$ is zero? Any help is appreciated. Thank you.","['smooth-manifolds', 'linear-algebra', 'differential-topology', 'derivatives', 'lie-groups']"
4779012,Show that a family is uniformly integrable,"I consider a family of sub sigma algebra $(\mathcal{F}_s)_{s\in S}$ on ( $\Omega,\mathcal{A}, \mathbb{P}$ ) and $X\in L^1(\Omega,\mathcal{A}, \mathbb{P})$ . I want to show that $Y_s =\mathbb{E}(X | \mathcal{F}_s)$ is uniformly integrable. My attempt is the following : I use the caracterization in terms of boundedness of a uniformly integrable family. First we notice that there exists $M\geq 0$ such that $\mathbb{E}(\lvert X\rvert)\leq M$ . Now we notice that for all $s\in S$ we have $$
\lVert Y_s \rVert_{L^1} = \mathbb{E}\left\lvert[\mathbb{E}(X | \mathcal{F}_s)\right\rvert] \leq \mathbb{E}[\mathbb{E}(\lvert X 
\rvert | \mathcal{F}_s)] = \mathbb{E}(\lvert X\rvert)\leq M
$$ Which shows that the family is bounded in $L^1$ and thus uniformly integrable. Is this seems correct ? Edit : This is false, my memory was totally wrong as my intuition. To solve the problem unfortunately I have not found other solutions than strenghten  hypothesis by considering that $X\in L^p$ in order to use a characterization in terms of epsilon delta of the uniform integrability. First we notice that there exists $M$ such that for all $s\in S$ $\lVert Y_s\rVert_{L^p}\leq M$ Let $\epsilon>0$ . Take $\delta = \frac{\epsilon^q}{M^q}$ . We have $$
\mathbb{E}(\lvert Y_s\rvert 1_{A})\leq \lVert Y_s\rVert_{L^p}(\mathbb{P}(A))^{1/q}\leq M(\mathbb{P}(A))^{1/q}\leq M\frac{\epsilon}{M}
$$ I am almost sure the hypothesis I have made is superficial","['uniform-integrability', 'measure-theory', 'probability-theory', 'probability']"
4779021,Explicitly showing the finite dimensional distribution of Brownian motion is consistent,"I wish to explicitly prove the family of finite dimensional distributions used to define Brownian motion is consistent. The definition of consistent I am using is taken from Stochastic analysis and diffusion processes by Gopinath Kallianpur and Padmanabhan Sundar. A family of finite dimensional distributions is consistent if the following condition holds: Let $k \in \mathbb{N}$ and $ 0 \leq t_1 < \ldots < t_k$ be arbitrary. Let $\{B_i\}_{i=1}^k$ be an arbitrary collection of 1-dimensional Borel sets. Then for any $i$ we have, $$
\mu_{t_1,\ldots,t_k}(B_1 \times \ldots \times B_{i-1} \times \mathbb{R} \times B_{i+1} \times \ldots B_k) = \mu_{t_1,\ldots,t_{i-1},t_{i+1},\ldots,t_k}(B_1 \times \ldots \times B_{i-1} \times B_{i+1} \times \ldots B_k).
$$ For simplicity let $k=2$ , $0 \leq t_1 \leq t_2$ and fix an $x \in \mathbb{R}$ . Then for any two Borel sets $B_1$ and $B_2$ define, $$
\mu_{t_1,t_2}(B_1 \times B_2) = \int_{B_1 \times B_2} \frac{\exp(-\frac{|x-y|^2}{2t_1})}{\sqrt{2 \pi t_1}} \cdot \frac{\exp(-\frac{|y-z|^2}{2(t_2-t_1)})}{\sqrt{2 \pi(t_2- t_1)}} d\lambda^2(z, y),
$$ where $d\lambda^2(z ,y)$ is the Lebesgue measure on $\mathbb{R}^2$ . Now to prove this is consistent we need to show $\mu_{t_1,t_2}(\mathbb{R} \times B_2)=\mu_{t_2}(B_2)$ (the case where $B_2=\mathbb{R}$ is easy). By Fubini's theroem we can write, \begin{align*}
    \mu_{t_1,t_2}(\mathbb{R} \times B_2) & = \int_{\mathbb{R} \times B_2} \frac{\exp(-\frac{|x-y|^2}{2t_1})}{\sqrt{2 \pi t_1}} \cdot \frac{\exp(-\frac{|y-z|^2}{2(t_2-t_1)})}{\sqrt{2 \pi(t_2- t_1)}} d\lambda^2(z, y)\\
    &= \int_{\mathbb{R}} \int_{B_2} \frac{\exp(-\frac{|x-y|^2}{2t_1})}{\sqrt{2 \pi t_1}} \cdot \frac{\exp(-\frac{|y-z|^2}{2(t_2-t_1)})}{\sqrt{2 \pi(t_2- t_1)}} dz dy.
\end{align*} My issue is how to evaluate this integral as $y$ appears in both exponents. Am I missing a simple trick? Pretty much every author says this is trivial and leaves it as an exercise. Any help is greatly appreciated. Edit: To add extra clarity to my question as advised in a comment, I am interested in showing consistency using the explicit densities. The reason for this is that in order to claim existence of Brownian motion you need to explicitly construct the finite dimensional distribution denoted $\mu_{t_1,\ldots,t_k}$ on $\mathbb{R}^k$ and show said construction is consistent. Thus apriori you cannot make any claims about any stochastic process such as $\mathbb{P}(X_{t_1} \in B_1, X_{t_2} \in B_2 )=\mathbb{P}(X_{t_2} \in B_2, X_{t_1} \in B_1 )$ , as neither the probability measure or process exist.","['stochastic-processes', 'probability-distributions', 'probability-theory']"
4779043,The set of homomorphism on $C_\infty(X)$ for a locally compact $X$,"Let $X$ be a locally compact Hausdorff space, which is not compact. $C_b(X)$ is a Banach algebra of all bounded continuous functions with the sup norm.
Let $C_\infty(X)$ be the Banach algebra of continuous functions such that for every $f \in C_\infty(X)$ , for every $\epsilon >0$ there exists a compact subset $K$ such that if $x \notin K, |f(x)|_\infty < \epsilon$ .
Let $p \notin X$ and define $\tilde{X} = X \cup p$ . Topology on $\tilde{X}$ is topology on $X$ plus all complements of compact subsets of $X$ . Notice that $\tilde{X}$ is compact Hausdorff. Set $\mathcal{B} = C(\tilde{X})$ . $\mathcal{B}$ can be identified with the closed subalgebra of $C_b(X)$ generated by all $f \in C_\infty(X)$ and the constant function 1. Let $\mathcal{A} = C_\infty(X)$ . It is Banach algebra without unit. I need to determine $\hat{\mathcal{A}}$ , a set of homomorphisms into R that are not identically zero. Using the fact that any homomorphism on $C(\tilde{X})$ is of the form $\phi(f) = f(y)$ for some $y \in \tilde{X}$ , I figured any homomorphism in $\widehat{C_\infty(X)}$ is also of the form $\phi(g) = g(x)$ for some $x \in X$ . Is this correct?","['group-homomorphism', 'banach-algebras', 'functional-analysis']"
4779106,Does $\displaystyle \lim_{t\to 0} \frac{g(\gamma(t))}{t}=0$ imply $\displaystyle\lim_{v\to 0} \frac{g(v)}{\|v\|}=0$?,"Let $g:U\subset \mathbb R^2 \to\mathbb R$ be a function defined in a neighborhood $U$ of $(0,0)$ . I'd like to understand the relationship between the following three limits. $\displaystyle \lim_{v\to 0} \frac{g(v)}{\|v\|}=0$ $\displaystyle \lim_{v\to 0} \frac{g(tv_0)}{t}=0$ , for all $v_0\in\mathbb R^2$ . $\displaystyle\lim_{t\to 0} \frac{g(\gamma(t))}{t}=0$ , for all differentiable path $\gamma:(-\varepsilon,\varepsilon)\to U$ such that $\gamma(0)=(0,0)$ . Remarks: Limits having the form 1 appear in the definition of differentiability. It is clear that 1 implies 2 (what is used, for example, to prove existence of the directional derivatives of differentiable functions). In general, 2 does not imply 1 (what can be seen, for example, by defining $g(0,0)=0$ and $g(x,y)=\frac{x^3y}{x^6+y^2}$ otherwise). Of course, 2 is a much weaker version of 3 (with $\gamma$ restrict to linear paths of the form $\gamma(t)=tv_0$ ). Question: Does 3 imply 1? I couldn't prove it (for example, I'm not able to connect $|v|$ with $t$ in a $\delta$ - $\epsilon$ proof; I also cannot see how could we find a $\delta$ that works for all $v$ inside a disk since each $\delta=\delta_\gamma$ works only for $v$ in the image of $\gamma$ ). Also, I couldn't construct a counterexample. In fact, in every case that I have seen in which 1 fails, there are differentiable paths that do not satisfy the limit in 3.","['limits', 'multivariable-calculus', 'vector-analysis']"
4779227,Can a relation have a derivative?,"In most textbooks when they talk about derivatives, they always say ""derivative of a function"" , but my question is, can derivatives be defined for relations other than functions? Like when one value of x gives two values of y, as in a circle or a parabola? Eg: $x^2+y^2=r^2$ or $y^2=4ax$ . If they are defined for relations as well, then why is it always mentioned as derivative of a function?","['calculus', 'derivatives']"
4779248,The set of characters,"Let $\mathcal{A}$ be a $C*$ subalgebra of $C_b(R)$ , generated by $C_\infty(R)$ (functions that are arbitrarily small outside some compact set) and the function $e^{it}$ . I need to determine the set of characters of $\mathcal{A}$ , $\hat{\mathcal{A}}$ , the set of all unital homomorphisms into complex numbers.
I know what $\widehat {C_\infty(R)}$ is, or at least I hope I do. But I am not sure this is the right way to go. Any hints? Edit: I figured what this object is: $C*$ subalgebra of $C_b(R)$ , generated by $C_\infty(R)$ (functions that are arbitrarily small outside some compact set) and the function $e^{it}$ . Any element in this subalgebra is a sum of a periodic function and a function that vanishes at infinity. But I still do not know how to proceed from here. Is the set of characters on C* algebra just the set of all extensions of characters on a subalgebra?","['characters', 'functional-analysis']"
4779276,How to find the characteristic polynomial of the given matrix?,"I am stuck at finding the characteristic polynomial of the following matrix. $$A=\begin{bmatrix}
		\ell a^2 & a & a &  a&\dotso & \dotso& a &a
		\\
		a &  t &0&0
	&	\dotso & \dotso &0&0
	\\
	a & 0 & t &0 &\dotso
	&	\dotso & 0 &0
	\\
	a & 0& 0& t &\dotso
	&	\dotso &0 &0
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	a&  0 &0&0
	&	\dotso & \dotso&t&0
	\\
	a&  0 &0&0
	&	\dotso & \dotso&0&t
			\end{bmatrix}_{(\ell+1) \times (\ell +1)}.$$ Here $t,l,a$ are constants. My try : The characteristic polynomial of the matrix is $\det(xI-A). We have, $\det(xI-A)=\begin{bmatrix}
		x-\ell a^2 & -a & -a &  -a&\dotso & \dotso& -a &-a
		\\
		-a &  x-t &0&0
	&	\dotso & \dotso &0&0
	\\
	-a & 0 & x-t &0 &\dotso
	&	\dotso & 0 &0
	\\
	-a & 0& 0& x-t &\dotso
	&	\dotso &0 &0
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso & \dotso
	\\
	-a&  0 &0&0
	&	\dotso & \dotso&x-t&0
	\\
	-a&  0 &0&0
	&	\dotso & \dotso&0&x-t
			\end{bmatrix}_{(\ell+1) \times (\ell +1)}.$ I expanded along the last row.
I got $(x-t)\times\det(M)-a \times \det (N)$ where $M=\begin{bmatrix}
		x-\ell a^2 & -a &-a &  -a&\dotso & \dotso& -a 
		\\
		-a &  t &0&0
	&	\dotso & \dotso &0
	\\
	-a & 0 &t &0 &\dotso
	&	\dotso & 0 
	\\
	-a & 0& 0& t &\dotso
	&	\dotso &0 
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso
	\\
	-a&  0 &0&0
	&	\dotso & \dotso&t\end{bmatrix}$ and $N=\begin{bmatrix}
		-a &-a &  -a&\dotso & \dotso& -a 
		\\
		 x-t &0&0
	&	\dotso & \dotso &0
	\\
	0 &x-t &0 &\dotso
	&	\dotso & 0 
	\\
	0& 0& x-t &\dotso
	&	\dotso &0 
	\\
	\dotso& \dotso& \dotso &\dotso
	&	\dotso & \dotso
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso 
	\\
	\dotso & \dotso& \dotso& \dotso &\dotso
	&	\dotso 
	\\
	0 &0&0
	&	\dotso & x-t&0\end{bmatrix}$ . But I am stuck from here. Can someone please help me out? Also, it is my request to everyone that please don't delete my question. This is my first question. I am trying really hard to show to the community what I have tried in the question. I am not so good at writing, I am a first year student studying Bachelors in Mathematics.","['matrices', 'determinant', 'linear-algebra', 'characteristic-polynomial']"
4779277,"Finding the range of $\frac{\sin(\alpha+\beta+\gamma)}{\sin\alpha+\sin\beta+\sin\gamma}$, where $\alpha,\beta,\gamma\in\left(0,\frac\pi2\right) $","How do I find the range of : $$
\dfrac{\sin(\alpha +\beta +\gamma )}{\sin\alpha + \sin\beta + \sin\gamma}
$$ Where, $$ \alpha , \beta\; and \;\gamma \in \left(0, \frac{\pi}{2}\right) $$ I tried using jensen's inequality on $\alpha, \beta \;and\; \gamma $ , and got : $$ \frac{\sin\alpha+\sin\beta+\sin\gamma}{3} \;\le\; \sin\left(\frac{\alpha + \beta+\gamma}{3}\right) $$ $$ \implies \sin\alpha+\sin\beta+\sin\gamma \;\le\; 3\sin\left(\frac{\alpha + \beta+\gamma}{3}\right) $$ Now if I assume that $\alpha+\beta+\gamma \; \le\; \pi $ then I can say: $$ \frac{\sin\left(\alpha + \beta + \gamma\right)}{\sin\alpha+\sin\beta+\sin\gamma}\; \ge\; \frac{\sin\left(\alpha + \beta + \gamma\right)}{3\sin\left(\frac{\alpha + \beta+\gamma}{3}\right)}$$ Solving this I got: $$ \frac{\sin\left(\alpha + \beta + \gamma\right)}{\sin\alpha+\sin\beta+\sin\gamma}\; \ge\; 1\; - \; \frac{4}{3}\sin^2\left(\frac{\alpha + \beta+\gamma}{3}\right) $$ $$\implies \frac{\sin\left(\alpha + \beta + \gamma\right)}{\sin\alpha+\sin\beta+\sin\gamma}\; > \; -\frac{1}{3}$$ But this is not useful as it is based on the assumption that $\alpha+\beta+\gamma \le \pi $ . So , I would like someone to provide me with a solution to my problem (I am not familiar with methods of higher mathematics so it would be helpful if these are not used, however use of complex numbers or any other basic inequalities in the solution is much appreciated ). It would also be helpful if I can get more than one solution. Thank you.",['trigonometry']
4779316,Compute $\int_{-\infty}^{\infty}\frac{dx}{1+\left(f_4(x)\right)^2}$ where $f_{i+1}(x)=x-\frac{1}{f_i(x)}$ and $f_0(x)=x$,"I am very confused and intrigued by this integral.
According to WolframAlpha, it says that the answer is $\pi$ . I find this hard to believe because the function inside the parenthesis is simplified as $$\int_{-\infty}^{\infty}
\frac{dx}
{1+\left(
x-\frac{1}{x-\frac{1}{x-\frac{1}{x-\frac{1}{x}}}}  
\right)^2}
=\int_{-\infty}^{\infty}
\frac{dx}{1+\left(
x-\frac{x^3-2x}{x^4-3x^2+1}  
\right)^2}.$$ Does Glasser's Master Theorem still apply?? If so, how??? If not, how is this solved? Or is there a secret trick I don't know about?","['integration', 'definite-integrals']"
4779390,Logarithmic asymptotics of an integral: order unity correction,"For a physics problem I'm working on I am struggling with the following integral: $$I(\varepsilon)=\int_0^\infty \frac{x}{ 1+e^{\varepsilon x}\bigl(1+2x^2+2x\sqrt{1+x^2}\bigr) }\,dx,\;\;\varepsilon\geq 0.$$ I have not found a closed-form expression (I don't think it exists), but my main interest is in the small- $\varepsilon$ asymptotics. Since for $\epsilon=0$ the integrand decays for large $x$ as $1/4x$ , the leading small- $\varepsilon$ behaviour is logarithmic, $$I(\varepsilon)=-\tfrac{1}{4}\ln\varepsilon+{\cal O}(1).$$ I would want to know the order unity correction, what is the limit $$\lim_{\varepsilon\rightarrow 0}\left[I(\varepsilon)+\tfrac{1}{4}\ln\varepsilon\right]=c_0.$$ A numerical integration gives $c_0\approx -0.096$ . Is there an exact value for this coefficient?","['integration', 'definite-integrals', 'asymptotics']"
4779399,What is the global complex moduli space for dimensions higher than 1?,"I am trying to read the book 'Mirror Symmetry and Algebraic Geometry' by D. Cox and S. Katz. In the book it claims that 'the space of all complex structures on a given manifold $V$ is a well known object in algebraic geometry'. Although I have seen a (very) little bit of deformation theory, I am unfamiliar with thinking about global moduli spaces. I also saw that for curves , there is a very nice moduli stack $\mathcal{M}_{g,n}$ that is in a definite sense a global moduli space, which solves a well-defined moduli problem. But I could not find any accesible references to global moduli spaces for dimensions $\geq2$ . Also it is claimed that the full complex moduli space of a Calabi-Yau manifold $V$ is a smooth manifold, but the references also had only local statements. So what exactly is this moduli space, and in what sense is it a moduli space?","['complex-geometry', 'algebraic-geometry', 'mirror-symmetry', 'moduli-space']"
4779406,Is $N_1+N_2\cong N_1\oplus N_2$ iff $N_1\cap N_2=(0)$?,"Let $A$ be a commutative ring with $1$ , $M$ an $A$ -module and $N_1,N_2$ two submodules. It is easy to see that $$N_1\cap N_2=(0)\implies N_1+N_2\cong N_1\oplus N_2.$$ I am trying to understand if the converse is true given that $N_1$ and $N_2$ are finitely generated. If we don't make this assumption, then it's possible to produce a counterexample. Consider, for example, the $\mathbb{Z}$ -module $M=\prod_{n\in\mathbb{N}}\mathbb{Z}$ with component-wise addition and scalar multiplication, and let $N_1=\mathbb{Z}\times0\times0\times\cdots$ and $N_2=M$ . Then, $N_1+N_2=M\cong N_1\oplus N_2$ but clearly, $N_1\cap N_2\neq(0)$ , so in this case, the converse implication is not true. However, I haven't been able to find a counterexample in the case when both $N_1$ and $N_2$ are finitely generated. Any help would be greatly appreciated!","['modules', 'finitely-generated', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
4779425,Standard area form on a Riemannian manifold.,"If I have a Riemannian manifold $(M^m,g)$ , and on it I have an embedded submanifold $i:N^{m-1} \hookrightarrow M$ , we can define the surface area of $N$ to be $\text{Area}(N)=\int_N \sigma$ where $\sigma = i_n\Omega$ where $n$ is the normal vector to $N$ , and $\Omega$ is the standard volume $m$ -form on $M$ . My question is, is there an $(m-1)$ -form $\omega$ on $M$ such that $$\text{Area}(N) = \int_N \omega = \int_M i^*\omega$$ I ask since, on a Symplectic manifold $(M^{2n},\omega)$ We have the notion of Symplectic area: $$\text{area}_\omega S := \int_S \omega $$ For any 2-dimensional submanifold $S$ .","['differential-topology', 'symplectic-geometry', 'differential-forms', 'differential-geometry']"
4779449,"To find all subsets contain 7 elements from the set $ [{1, 2, ..., 14} ]$ such that the sum of the elements is divisible by 14","anyone can help solve this problem? How many subsets contain 7 elements from the set $[ {1, 2, ..., 14}]$ such that the sum of the elements is divisible by 14",['discrete-mathematics']
4779490,"To find rate of change of area of triangle when rate of change and value of length of base and height are 3cm/min, 5cm/min and 8cm,10cm respectively.","I am trying understand very simple related rates problem ( area of triangle on youtube): The base of a right triangle is increasing at 3cm/min while the height of the triangle is increasing at a rate of 5cm/min. How fast is the area of the triangle changing when the base and height are 8cm and 10cm long respectively. Base of the triangle is defined as $b = 8cm$ , height as $h = 10cm$ . Base increases 3 cm per min so $\frac{db}{dt} = 3 cm$ Height increases 5cm per min so $\frac{dh}{dt} = 5cm$ Question is about finding how fast is area of triangle changes. Triangle area is of course defined as $A = \frac{1}{2}bh$ and its derivative is $\frac{dA}{dt} = \frac{1}{2}\frac{db}{dt}h + \frac{1}{2}\frac{dh}{dt}b$ By substituting numbers we know we get $\frac{dA}{dt} = \frac{1}{2}*3*10 + \frac{1}{2}*5*8 = 35$ So my understanding is that area of triangle is changing $35 cm^2$ per minute. So far everything is clear but I am confused when I tried to compute real area after base and heigh is increased. With $b = 8cm$ and $h = 10cm$ area is $40 cm^2$ . After one minute if area is increasing by $35cm^2$ it should be $75 cm^2$ . But when I substitute new base and height I got different number: $A = \frac{1}{2}*11*15 = 82.5$ Here I am confused as I would expect area to match original area + rate of change. What exactly that $35 cm^2$ means and why those numbers does not match?","['related-rates', 'calculus']"
4779525,Tensor of globally generated is globally generated,"Consider $X$ a scheme, $\mathcal F, \mathcal G$ globally generated sheaves. I want to show that $\mathcal F\otimes \mathcal G$ is globally generated. By definition we have surjective morphisms $f:\oplus_{I\in I}\mathcal O_X\to \mathcal F$ , $g:\oplus_{j\in J}\mathcal O_X\to \mathcal G$ . From them I considered for $U\subseteq X$ open $(\oplus_{I\in I}\mathcal O_X)\otimes(\oplus_{j\in J})(U)\to \mathcal F(U)\otimes \mathcal G(U)$ given by $u\otimes v\mapsto f(u)\otimes g(u)$ By considering stalks I think this is surjective (we can't deduce surjectivity from maps on open sets). So we have a surjective morphism $\varphi$ from $(\oplus_{I\in I}\mathcal O_X)\otimes(\oplus_{j\in J}\mathcal O_X)$ to the tensor presheaf. However we want a map which goes in the tensor sheaf, which have the same stalks. At first I considered the sheafification map $\theta: ( F\otimes \mathcal G)^{\operatorname{pre}}\to \mathcal F \otimes \mathcal G$ and said $\theta_p\circ \varphi_p=\varphi_p$ but reading it again made me realize it could be false, so $\theta \circ \varphi$ might not being a good choice despite being the most natural way to have a map to the sheaf. Is there a way to make this work ?","['algebraic-geometry', 'sheaf-theory']"
4779547,Is it possible to redefine measurable spaces such that measurable functions are not defined in terms of pre-image?,"We know that a measurable space is defined as a tuple $(X,\mathcal A)$ , where $X$ is a set and $\mathcal A$ is a $\sigma$ -algebra defined on $X$ . A $\sigma$ -algebra is traditionally defined as a collection of subsets such that the whole set is in collection and it is closed under complements and countable union. A function $f:(X,\mathcal A)\to (Y,\mathcal B)$ is called measurable if for every $U\in \mathcal B,\ f^{-1}(U)\in\mathcal A$ . We note that this definition uses the pre-image as a central notion. An analogous construction is a topology $\tau$ on a set $X$ , where $\tau$ is defined as a collection of subsets of $X$ such that it contains the whole set and the empty set and is closed under arbitrary union and finite intersection. A function $f:(X,\tau)\to (Y,\tau')$ is called continuous if for every $U\in \tau',\ f^{-1}(U)\in\tau$ . We again note that this definition uses the pre-image as a central notion. But in the case of topology, there exists an equivalent characterization using the so called Kuratowski closure operator $\bf c$ in terms of which continuous functions are defined as a function $f:(X,\mathbf{c})\to(Y,\mathbf{c}')$ such that $f(\mathbf c(U)) \subseteq \mathbf c'(f(U))$ for all $U\subseteq X$ . We note that this definition does not use pre-images and is equivalent to the traditional definition of continuous functions. Is there a similar reformulation of $\sigma$ -algebras such that measurable functions also does not use pre-images in it's definition? In fact, is there a reformulation such that measurable functions can be defined as exactly those functions such that $f(g(U))\subseteq g'(f(U))$ for all $U\subseteq X$ , where $g,g'$ are functions from power set of $X,Y$ to power set of $X,Y$ respectively?","['measurable-sets', 'general-topology', 'measurable-functions', 'measure-theory']"
4779592,Classification of algebraic groups of the types $^1\! A_{n-1}$ and $^2\! A_{n-1}$,"Let $G$ be a simply connected absolutely simple group
of one of the types $^1{\sf A}_{n-1}$ (inner) or $^2{\sf A}_{n-1}$ (outer) over a field $k$ . All such groups are described on page 55
of Tits, Classification of algebraic semisimple groups,
Proc. Sympos. Pure Math. 9 (Boulder), 1966, pp. 33-61.
The descriptions are as follows: Type $^1{\sf A}_{n-1}$ :
Special linear group ${\rm SL}_m(D)$ , where $D$ is a central division algebra of degree $d$ over $k$ , and $n=md$ . Type $^2{\sf A}_{n-1}$ :
Special unitary group ${\rm SU}_m(D,h)$ , where $D$ is a central division algebra of degree $d$ over a quadratic extension $K$ of $k$ with an involution of the second kind $\sigma$ such that $k=\{x\in K\ |\ x^\sigma=x\}$ , $$h\colon D^m\times D^m\to D$$ is a nondegenerate hermitian form relative to $\sigma$ , and $n=md$ . Question. I am looking for a down-to-earth proof
that all such groups are indeed of the form either ${\rm SL}_m(D)$ or ${\rm SU}_m(D,h)$ .
(For me, the Book of Involutions is not down-to-earth.) I know that my group becomes ${\rm SL}_n(\bar k)$ over an algebraic closure $\bar k$ of $k$ .","['algebraic-groups', 'galois-cohomology', 'reference-request', 'algebraic-geometry', 'group-theory']"
4779624,Analytic continuation of double factorial,"This question is somewhat informative. In Wikipedia the definition of double factorial continued in the complex arguments is provided $$
k!!=\sqrt{\frac{2}{\pi}}2^{\frac{k}{2}}\Gamma[k/2+1]
$$ Clearly, this definition does not work for positive even numbers. Is there any double factorial formula in terms of Gamma functions which works for both odd and even?",['functions']
4779665,Basic Understanding of derivatives as a rate of change,"I am teaching myself calculus and trying to understand derivatives from a physics point of view. I am faced with this question: the edge of a square is being enlarged at $ 
2 \frac{cm}{sec}$ . What is the rate of change of the area when the edge is 6 cm long. The solution in the book (Serge Lang, A first course in Calculus, 5th ed) is as follows: $A = x^2 \\
\frac{dA}{dx} = 2x$ Using chain rule: $ \frac{dA} {dt} =\frac{dA} {dx} * \frac{dx} {dt} \\
= 2x * 2 = 4x $ When the edge is 6: $\frac{dA}{dt} = 4*6 = 24$ . My main issue is that intuitively I had assume I can do the following: At time $t_1, A_1 = x^2$ $\\ $ At time $t_1+1, A_2 = (x+2)(x+2) =
x^2 + 4x + 4$ Then $\frac {dA}{dt}= \frac{A_2 - A_1} {t_1 +1 - t_1} = 4x+4$ Which does not match the answer, also, calculating difference of area when edge goes from 4 to 6 is 36 - 16 = 20 and not 24. I would like to understand what was wrong in my logic. Also I considered this might be due to the fact that I am taking a long interval ( 1 sec. and derivatives evaluate the rate of change at exactly the point of change). But again the rate which we are using is defined as 2 $\frac {cm}{sec}$ ,we don't have any information about how it grows in shorter intervals, so I don't know if it make sense to think of shorter intervals than 1 second. When I googled about derivates from definition are different from formulas I came against something called backward and forward differences and centered differences, it seems to be related to discrete mathematics which I have no experience with yet. But using the idea to evaluate the derivate at the center yields: $(x+1)(x+1) - (x-1)(x-1) = 4x$ This also is correct as difference between 49 and 25.
Maybe someone can shade light on this?
(I guess same result can be reached if we take two differences, one forward and one backward and average them on the time interval of 2 second). Also one final remark that confused me, the fact I can evaluate the area when $x = 5$ and $x = 6$ is based on the initial condition of the length of the edge. That is, is not there a hidden assumption that the increase is uniform everywhere during this one second? What I mean to point out is that if the edge was being increased in pulses and not uniformly then I can not assume that $x-1$ and $x+1$ (despite having a distance of 2 separating them) are actually legal to evaluate, because the only information I was given that $x_{next } = x_{old} + 2$ for every second that passes Thanks.","['calculus', 'area', 'derivatives']"
4779687,"The $n\times n$ matrices $A^n, A^{n-1},\dots,\mathrm{id}_n$ are linearly dependent","Let $A$ be an $n\times n$ matrix over some field $K$ . Then its characteristic polynomial $\mathrm{char}_A(X)\in K[X]$ is monic of degree $n$ and annihilated by $A$ (Cayley-Hamilton). It follows that Corollary. $A^n, A^{n-1},\dots,\mathrm{id}_n$ are linearly dependent. I wonder if this Corollary can be obtained without making use of the determinant (which is required to define $\mathrm{char}_A(X)$ )? Otherwise, can we show that there is some $m<n^2$ for which $A^m, A^{m-1},\dots,\mathrm{id}_n$ are linearly dependent? Note that if we take $m=n^2$ , then the statement is obvious, because the $n\times n$ matrices form a vector space of dimension $n^2$ , and $A^m, A^{m-1},\dots,\mathrm{id}_n$ are $n^2+1$ matrices.","['matrices', 'linear-independence', 'linear-algebra']"
4779749,optimal rotations around 3D sphere using only two axes,"This is an optimization problem and I am wondering if there's any intuitive (geometric) way to understand the solution of the problem. More complex analytical solutions are also appreciated. In a 3D sphere, let's start from an initial point located at coordinates $(1, 0, 0)$ , where each element corresponds to the $(x, y, z)$ coordinates in the plane. Our only allowed actions are rotating the point around either the $X + Z$ or $-X + Z$ axis (the axis of rotations would be $(X+Z)/\sqrt{2}$ and $(-X + Z)/\sqrt{2}$ respectively. They are vector addition of $X$ and $Z$ and $-X$ and $Z$ ). The total rotation amount (i.e. the total angle of rotation) must be equal to $\theta ~ (< \pi)$ . What is the optimal strategy in order to minimize the Euclidean distance between the final point and the initial point? Upon numerical investigation, it appears that rotating by an angle of $\theta/2$ around the $X + Z$ axis, followed by an additional rotation of $\theta/2$ around the $-X + Z$ axis, gives the optimal solution for this problem. An example of this solution is depicted in the figure below. If this is indeed an optimal solution, why would it be? What is a good way to understand this solution in terms of the geometry of the problem, or any other tools? Figure","['optimization', 'geometry', '3d', 'rotations']"
4779775,"Showing that $\bigcap_{x\in(1,10)} [0, x)=[0, 1]$","I want to show that $$\bigcap_{a\in(1,10)} [0, a)=[0, 1].$$ I know how to prove that $\text{RHS}\subseteq\text{LHS}$ but I am struggling with the other direction. My attempt: Let $x\in\text{LHS}$ . This means for all $a\in(1, 10)$ , $0\le x<a$ ... I am trying to find a way to show that $0\le x\le 1$ to complete the proof. All I know is that for all $a\in (1, 10)$ , $a>1$ . How do I proceed?",['elementary-set-theory']
4779800,Determine the horizontal tangent to the graph of the trigonometric function $f(x) = 2x + \sqrt{3}\cos(x)$,"Given $y = 2x + \sqrt{3}\cos{x}$ , find all horizontal tangents and write a general solution for x. To find horizontal tangents, we derive and equal to 0: $y' = 2 - \sqrt{3} \sin{x}$ ; $y'=0$ $2 - \sqrt{3} \sin{x} = 0$ $\sin{x}= \frac{2}{\sqrt{3}}$ $x = \arcsin(\frac{2}{\sqrt{3}})$ However, $\arcsin(\frac{2}{\sqrt{3}})$ is not a real number. Upon graphing, I found the horizontal tangents to be $x = \frac{œÄ}{2} + œÄn$ where n is an integer. But I don't know how to find the x values without graphing. Any ideas? Thank you!","['calculus', 'derivatives', 'trigonometry', 'complex-numbers']"
4779814,"Find the largest subset of $S=\{1,2,3,....,20\}$ such that its no two elements differ by $4$ or $7$.","Problem :- Consider a subset of $S=\{1,2,3,....,20\}$ with the property that no two elements of this subset differ by $4$ or $7$ . What is the largest number of elements this subset can have? My attempt :- Case 1:- Considering the elements from start $1,2,3,4,12,13,14,15$ - this gives me $8$ elements Case 2:- Considering the elements from end $6,7,8,9,17,18,19,20$ - this too gives $8$ elements The answer given is $10$ elements.  What would be the logical way to arrive at the answer ?","['elementary-set-theory', 'combinatorics']"
4779865,Problem from the 1960 Putnam Olympiad involving the sum of a series,"Suppose that $\sum\limits_{i=1}^n x_i $ is a convergent series of positive terms that monotonically decrease
(that is, $ x_1 \geqslant x_2 \geqslant x_3 \geqslant.. $ ). Let P denote the set of all numbers that are sums of some
(finite or infinite) subseries of $\sum_{n=1}^{\infty}$ xi  . Show that P is an interval if and only if $$x_n \leq \sum_{i=n+1}^{\infty}x_i$$ for every integer n,which we call condition(1). This is the condition of the problem, I guessed part of the solution, but I am as sure of it as I am uncertain. My solution. Let P be the interval (a,b), where b>a>0. Since b is the largest sum of all possible subsequences, it includes all elements and equal $\sum\limits_{i=1}^n x_i = S$ . To get the smallest element, you need to take the smallest terms from sequense, since by condition the series converges,that $\lim_{n\to \infty}x_n=0$ .This means that to get a we can take terms tending to zero,a= $0$ .P=( $0$ ,S). For any c from the interval we can find a subsequence whose sum is equal to c.Let us assume here that $\exists x_k > \sum_{i=k+1}^{\infty}x_i$ .Take $c > x_k$ and $c < x_{k-1}$ (Such as c is exists since the sequence is monotonic).Then for $c-x_k$ exists subsequence which sum equal $x_k$ (it is obvious that all elements of the sequence $< x_k$ ) ,but $x_k > \sum_{i=k+1}^{\infty}x_i$ .That mean $c-x_k +  \sum_{i=k+1}^{\infty}x_i <c$ contradiction. On the contrary, I don‚Äôt really understand how to prove.","['problem-solving', 'sequences-and-series']"
4779869,Finding rational points on the elliptic curve for $A^4+B^4 = C^4+D^4$?,"I. Third Powers As background, the complete solution to $x_1^3+x_2^3 = x_3^3+x_4^3$ was given by Euler and Binet using deg- $4$ polynomials. However, Elkies also found a complete solution using deg- $3$ polynomials in $3$ variables. Define. $$f(a,b,c) = 9a^3 + 9a^2b + 3a^2c + 3a b^2 - 6a b c + 3a c^2 + 3b^3 + 3b^2c + b c^2 + c^3$$ then, $$f(a,b,c)^3+f(-a,b,-c)^3=f(a,b,-c)^3+f(-a,b,c)^3$$ II. Fourth Powers In contrast, no complete polynomial solution to $x_1^4+x_2^4 = x_3^4+x_4^4$ is known (or maybe even possible). However, analogous to Elkies' solution, there are at least five of form, $$f(a, b)^4 + f(b, -a)^4 = f(a, -b)^4 + f(b, a)^4$$ where $f(a,b)$ is a polynomial of degree $k = 6n+1 = 7,13,19,25,31$ given in the addendum below. Whether there are more remains to be seen. (Deg- $25$ was found by yours truly.) III. Elliptic curve Using Euler's approach, let, $$(p+q)^4+(r-s)^4=(p-q)^4+(r+s)^4$$ and define $p = (a^3 - b),\, q = a y,\, r = b  (a^3 - b),\, s = y.\,$ Then the equation above transforms to the simple form, $$(a^3 - b) (b^3 - a) = y^2$$ This is birationally equivalent to an elliptic curve . ( Update : This curve has been analyzed by Deyi Chen in this MO answer .) Known rational points of small height are, $$a=n,\quad b =n\,\frac{(4 + n^2 + 10n^4 + n^6)}{(1 + 10n^2 + n^4 + 4n^6)}\quad$$ $$a=n^3,\quad b =n\,\frac{(1 - 2n^2 + n^4 + n^6)}{(1 + n^2 - 2n^4 + n^6)}\quad$$ $$\quad a=n,\quad b =n\,\frac{(9 - 44n^2 + 190n^4 + 100n^6 + n^8)}{(1 + 100n^2 + 190n^4 - 44n^6 + 9n^8)}$$ $$\quad a=n^3,\quad b =n\,\frac{(1 - 4n^2 + 10n^6 + 8n^{10} + n^{12})}{(1 + 8n^2 + 10n^6 - 4n^{10} + n^{12})}$$ Note the symmetry of $b$ 's numerator and denominator. The first two points $b$ yield the same deg- $7$ solution (after removing common factors) while the last two $b$ yield the deg- $13$ and deg- $19$ solutions. For the deg- $25$ , we have $a=n,$ and, $\qquad\qquad b =\frac{n(16 - 543n^2 + 4632n^4 + 15100n^6 + 10632n^8 + 22758n^{10} + 6568n^{12} + 5820n^{14} + 552n^{16} + n^{18})}{(1 + 552n^2 + 5820n^4 + 6568n^6 + 22758n^8 + 10632n^{10} + 15100n^{12} + 4632n^{14} - 543n^{16} + 16n^{18})}$ found by yours truly (though I may have missed a point of smaller height). The deg- $31$ solution has a rational point $b$ that is of higher height. IV. Question Q: Given some fixed $a$ (like $a=n$ ) and the elliptic curve $(a^3 - b)(b^3 - a) = y^2,$ can you find a rational point $b = P(n)/Q(n)$ that is of height less than deg- $24$ ? Note : Points that yield an identity of form $f(a, b)^4 + f(b, -a)^4 = f(a, -b)^4 + f(b, a)^4$ are preferred. An example of a point that does not do so was found by Lander (and which leads to a second deg- $19$ identity). Addendum: Below are the five known polynomials $f(a,b)$ of deg $k = 6n+1 = 7,13,19,25,31.$ (For ease of copy-paste, they are purposely not in Latex format for those who want to test them.) The deg- $7$ , or versions thereof, have been independently found by many including Euler, Gerardin, Swinnerton-Dyer (at the tender age of 16), and others. Deg 7 f(a, b) = a^7 + a^5 b^2 - 2 a^3 b^4 - 3 a^2 b^5 + a b^6 Deg 13 f(a, b) = a^13 + a^12 b - a^11 b^2 + 5 a^10 b^3 - 6 a^9 b^4 - 12 a^8 b^5 + 4 a^7 b^6 +  7 a^6 b^7 + 3 a^5 b^8 - 3 a^4 b^9 - 4 a^3 b^10 + 2 a^2 b^11 + a b^12 + b^13 Deg 19 f(a, b) = a^19 + 6 a^17 b^2 - 18 a^15 b^4 + 6 a^14 b^5 - 5 a^13 b^6 + 12 a^12 b^7 - 12 a^11 b^8 + 36 a^10 b^9 - 24 a^9 b^10 - 12 a^8 b^11 + 19 a^7 b^12 + 36 a^6 b^13 + 6 a^5 b^14 + 12 a^4 b^15 - 6 a^3 b^16 + 6 a^2 b^17 + a b^18 Deg 25 f(a, b) = a^25 + 7 a^23 b^2 - 2 a^21 b^4 - 3 a^20 b^5 - 25 a^19 b^6 - 63 a^18 b^7 +  43 a^17 b^8 + 36 a^16 b^9 - 134 a^15 b^10 + 213 a^14 b^11 + 179 a^13 b^12 - 333 a^12 b^13 - 128 a^11 b^14 + 207 a^10 b^15 + 136 a^9 b^16 - 57 a^8 b^17 - 97 a^7 b^18 + 9 a^6 b^19 + 4 a^5 b^20 - 9 a^4 b^21 + 10 a^3 b^22 - 3 a^2 b^23 + a b^24 Deg 31 f(a, b) = a^31 - a^30 b + 11 a^29 b^2 + a^28 b^3 + 42 a^27 b^4 + 24 a^26 b^5 - 19 a^25 b^6 - 32 a^24 b^7 - 154 a^23 b^8 - 254 a^22 b^9 + 266 a^21 b^10 +  718 a^20 b^11 + 126 a^19 b^12 - 303 a^18 b^13 - 478 a^17 b^14 - 830 a^16 b^15 + 770 a^15 b^16 + 916 a^14 b^17 - 738 a^13 b^18 +  21 a^12 b^19 + 350 a^11 b^20 - 434 a^10 b^21 + 50 a^9 b^22 + 142 a^8 b^23 - 91 a^7 b^24 + 76 a^6 b^25 + 15 a^5 b^26 - 3 a^4 b^27 + 8 a^3 b^28 - 8 a^2 b^29 + a b^30 - b^31 P.S. A similar question was asked by emacs in this post .","['number-theory', 'polynomials', 'elliptic-curves', 'diophantine-equations']"
4779895,Growth of powers of symmetric subsets in a finite group,"Let $G$ be a finite group, and let $A$ be a symmetric subset of $G$ containing the identity (i.e., $A^{-1}=A$ and $1\in A$ ). Then the powers of $A$ will form a chain $A\subsetneq A^2\subsetneq A^3\subsetneq\cdots\subsetneq A^d=\langle A\rangle$ . It turns out that if $A$ is large, then $d$ must be small. But how small? That is my question (formulated precisely at the end). For example, the following lemma is a slight modification of Lemma 2.1 in this paper of Eberhard. Lemma. If $(r+1)\lvert A\rvert>\lvert G\rvert$ , then $d\leq3r-1$ . Proof. If $d\geq3r$ , then we can find $g_i\in A^{3i}\setminus A^{3i-1}$ for $i=1,\ldots,r$ . Then $g_iA\subseteq A^{3i+1}\setminus A^{3i-2}$ , so $A,g_1A,\ldots,g_rA$ are disjoint subsets of $G$ each of size $\lvert A\rvert$ , so $(r+1)\lvert A\rvert\leq\lvert G\rvert$ . $\square$ But this lemma is not optimal. The proof finds two translates of $A$ inside $A^4$ , but it is actually possible to find two translates of $A$ inside $A^3$ . This gives a bound of $3r-2$ . Lemma. If $r\geq2$ and $(r+1)\lvert A\rvert>\lvert G\rvert$ , then $d\leq3r-2$ . Proof. If $d\geq3r-1$ , then we can find $g_i\in A^{3i-1}\setminus A^{3i-2}$ for $i=2,\ldots,r$ . Then $g_iA\subseteq A^{3i}\setminus A^{3i-3}$ . Since $d>2$ , we can find $g_0\in A$ such that $g_0A^2\neq A^2$ . Pick $g_1\in A^2\setminus g_0A^2$ . Then $g_1A$ and $g_2A$ are disjoint translates of $A$ inside $A^3$ . Then $g_0A,g_1A,g_2A,\ldots,g_rA$ are disjoint subsets of $G$ each of size $\lvert A\rvert$ , so $(r+1)\lvert A\rvert\leq\lvert G\rvert$ . $\square$ If $k\lvert A\rvert>\lvert G\rvert$ , what is the optimal upper bound on $d$ in terms of $k$ ? If $2\lvert A\rvert>\lvert G\rvert$ , then $d\leq2$ is optimal (set $A=\{-1,0,1\}\subseteq\mathbb{Z}/5\mathbb{Z}$ ). If $3\lvert A\rvert>\lvert G\rvert$ , then $d\leq4$ is optimal (set $A=\{-1,0,1\}\subseteq\mathbb{Z}/8\mathbb{Z}$ ).","['group-theory', 'finite-groups', 'additive-combinatorics']"
4779991,"Fubini-like statement, reference request","Let $(X,\Sigma_X,\mu)$ and $(Y,\Sigma_Y,\nu)$ be finite measure spaces (the measures are finite, not the sets).
Let $k:X\times\Sigma_Y\to[0,1]$ be a Markov kernel which disintegrates $\nu$ , i.e. with the property that for every $B\in\Sigma_Y$ , $$
\int_X k(B|x)\,\mu(dx) = \nu(B) .
$$ This way, we can form the measure $\mu k$ on $X\times Y$ , given for all measurable $A\subseteq X\times Y$ by $$
\mu k (A) = \int_X k(A_x|x) \,\mu(dx) ,
$$ where $$
A_x = \{y\in Y : (x,y)\in A\} .
$$ The measure $\mu k$ has $\mu$ and $\nu$ as marginals. Suppose now that $f:X\times Y\to\Bbb{R}$ is $\mu k$ -integrable. That is, $$
\int_{X\times Y} |f(x,y)|\,\mu k(dx\,dy) < +\infty .
$$ Can we conclude that the integrals $$
\int_Y |f(x,y)| \,k(dy|x)
$$ are finite for $\mu$ -almost all $x$ , and so that $$
\int_{X\times Y} f(x,y)\,\mu k(dx\,dy) = \int_X \left( \int_Y f(x,y)\,k(dy|x) \right) \mu(dx) ,
$$ as in the usual Fubini theorem? If so, what is the name of this theorem? (Note that the usual Fubini theorem, at least the one I know about, is the special case where $k$ does not depend on $x$ .)","['integration', 'measure-theory', 'lebesgue-integral', 'reference-request', 'fubini-tonelli-theorems']"
4779996,Basic contour integral of $\frac{\sin z}{z - i}$,"I'm trying to evaluate a basic contour integral, but am getting an incorrect factor of 2 in my answer. Evaluating $$\int_{-\infty}^{\infty} \frac{\sin z}{z - i} = \Im \left( \int_{-\infty}^{\infty} \frac{e^{i z}}{z - i} \right) \, ,$$ I express this as a closed semi-circular contour of radius $R$ in the upper-half plane and take $\lim R \to \infty$ . By Jordan's lemma, the integral over the arc contour vanishes and we can evaluate the integral by the residue theorem $$\int_{-\infty}^{\infty} \frac{\sin z}{z - i} = \Im \left( 2 \pi i \, \left. \mathrm{Res}\left( 
\frac{e^{i z}}{z - i} \right) \right|_{z = i} \right) = \frac{2 \pi}{e} \, ,$$ while the correct answer is $\pi / e$ . I've stared at this for a bit and still cannot see what I've done wrong. Can anyone quickly point out how the factor of 2 cancels?","['complex-analysis', 'contour-integration']"
4780014,Diophantine equation with 1 and 3,"In a certain exercise I am asked to say how many possible solutions are there for the diophantine equation $x_1 + x_2 + \cdots + x_{20} = 50$ , with the condition that $x_i$ can only be or 1 or 3. Here is my attempt: To begin with, one has to choose how many 3 can be. Taking into account that there are at most 20 $x_i$ that can add up to the number on the right side and that those $x_i$ can only be 1, I ended up with the fact that the only possible solution would be having 15 threes and therefore the sum of the remaining 5 members would be 5, so the remaining members would all be 1. The only thing missing is ordering those threes: the answer is therefore $20 \choose 15$ . Is this correct, or am I missing something?","['combinatorics', 'discrete-mathematics']"
4780026,Is $A \leq B$ with probability $p>1/2$ a partial order on random variables?,"It seems reflexive, antisymmetric and transitive. My mathematics training is limited though and I would love proof or contradiction. A quick search turns up many references cataloging on stochastic orderings - e.g. stochastic dominance, hazard rate order, etc. None so far seems to mention this most basic one though. Is the relation defined as $(A,B) \in \leq_q$ if $p(A  \leq B) \geq q$ a partial order too? I feel like it should be - at least for $q>1/2$ , and maybe assuming something about the dependence between any pair $A,B$ ? - but need help to prove it.","['order-theory', 'probability-theory', 'probability']"
4780029,"$\mathbf{P}(S_{n(i)}=i \mid S_1=i),\mathbf{P}(S_{n(i)+1}=i\mid S_1=i),......$ are zeros?","$\left\{\xi_{n}\right\}_{n\in\ \mathbb{N}_{+}}$ is a sequence of independently and identically distributed random variables, each taking a finite number of integer values. $\mathbf{E}(\xi_1)\ne 0,$ For any $n\in \mathbb{N}_{+},$ define $S_{n}:=\sum_{i=1}^{n}\xi_{i}.$ Show that $\left\{S_{n}\right\}_{n\in\ \mathbb{N}_{+}}$ is a Markov chain, with each state being transient. My question is how to prove whose each state is transient.I attempt to demonstrate $\sum_{n=1}^{\infty}\mathbf{P}(S_{n}=i\mid S_1=i)<\infty $ for each $i$ in the state space $S$ , by Kolmogorov's strong law of large numbers and  Borel‚ÄìCantelli lemma. Maybe we can use this equivalence $$\frac{S_n}{n} \xrightarrow[]{a.s.}\mathbf{E}(\xi_1)\Longleftrightarrow \displaystyle \lim_{ n\to \infty}\mathbf{P}\left(\bigcup_{k=n}^{\infty}\left\{\left|\frac{S_k}{k} -\mathbf{E}(\xi_1)\right|\ge\varepsilon\right\}\right)=0, \forall \varepsilon>0.$$ to create a paradox to support that starting from some positive integer $n(i)$ , all subsequent terms in $\sum_{n=1}^{\infty}\mathbf{P}(S_{n}=i\mid S_1=i)$ i.e. $\mathbf{P}(S_{n(i)}=i\mid S_1=i),\mathbf{P}(S_{{n(i)+1}}=i\mid S_1=i),......$ are zeros. $\left\{S_{n}\right\}_{n\in\ \mathbb{N}_{+}}$ is a time-homogeneous Markov chain.
Someone have objections, and the reasons are as follows: From Chapman-Kolmogorov equation , $k+m$ -step transition probability \begin{align}
   \mathbf{P}(S_{n+k+m}=j\mid S_{n}=j)&=:\mathbf{P}^{(k+m)}(j,j)\\ &=\sum_{r\in S}\mathbf{P}^{(k)}(j,r)\cdot\mathbf{P}^{(m)}(r,j) \\
&\ge\mathbf{P}^{(k)}(j,j)\cdot\mathbf{P}^{(m)}(j,j).\end{align} If both $\mathbf{P}^{(k)}(j,j)>0$ and $\mathbf{P}^{(m)}(j,j)>0$ , then $\mathbf{P}^{(k+m)}(j,j)>0.$ That will lead to $\mathbf{P}(S_{n+k+m}=j)>0,$ further steps will result in infinitely many $\mathbf{P}(S_{n}=j)>0.$ What is your opinion on this? I thought from the definition of time-homogeneous, $\mathbf{P}(S_{n+1}=j|S_{n}=i)=\mathbf{P}(S_{m+1}=j|S_{m}=i),\forall i,j\in S \& \forall n,m\in \mathbb{N},$ both $\mathbf{P}(S_{n})>0$ and $\mathbf{P}(S_{m})>0$ seem to be needed.","['self-learning', 'markov-chains', 'stochastic-processes', 'probability-theory', 'probability']"
4780047,Principal components of Vandermonde matrix,"Recall that the Vandermonde matrix of a collection $\{x_0,\ldots,x_m\}$ of points is $$ V = \begin{pmatrix} 1 & x_0 & x_0^2 & \cdots & x_0^n \\
1 & x_1 & x_1^2 & \cdots & x_1^n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_m & x_m^2 & \cdots & x_m^n \end{pmatrix} $$ Take your points to be uniformly spread out over the interval $[-1, 1]$ , with step sizes $2/m$ . Now use SVD to extract the first $r$ principal components of $V$ . Numerical analysis suggests that the principal components stabilize as $m$ and $n$ tend to $\infty$ . Below I plot the case $r = 5$ , $m = 20\,000$ , $n = 1000$ to illustrate. Is there a name for these functions?","['calculus', 'linear-algebra', 'statistics', 'numerical-methods']"
