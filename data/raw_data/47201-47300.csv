question_id,title,body,tags
474873,Why is every Noetherian zero-dimensional scheme finite discrete?,"In the book The geometry of schemes by Eisenbud and Harris , at page 27 we find the exercise asserting that Exercise I.XXXVI. The underlying space of a zero-dimensional scheme is discrete; if the scheme is Noetherian, it is finite. Thoughts I thought that a zero-dimensional scheme is one such that, in every local ring $\mathscr O_{X,p}$, the only prime ideal is its unique maximal ideal $\mathfrak m_{X,p}.$ Hence, supposing that $X=\operatorname{Spec} R$ is affine, we deduce that every prime ideal is maximal, so each ponit in $X$ is closed. But how does this imply the discreteness? It only implies the Hausdorff-ness of $X$, right? Furthermore, I cannot perceive what the ""Noetherianity"" has to do with the finiteness claimed here. In the affine case, it seems to be claiming that there are only a finite number of prime ideals in a Noetherian ring, if its scheme is zero-dimensional? For example, $\mathbb Z$ is a Dedekind domain, hence its scheme is zero-dimensional, but it has infinitely many primes: does this contradict the statement? Thanks in advance for any reference or hint, and point out any inappropriate point if any is presented. Edit I see why my example cannot work now: it is one-dimensinoal: don't forget the pirme $(0)$. But I am still wondering why the statement is true...","['krull-dimension', 'algebraic-geometry', 'schemes']"
474874,Intuition about absolute Continuity/Singularity of measures,"Let $\mu$, $\nu$ be two measures, $X= d\nu/d\mu$ their Radon-Nikodym derivative (in general a random variable). I want to gain an intuition about the following statements: $$\int X\; d\mu = 1 \Leftrightarrow \nu << \mu \Leftrightarrow X < \infty, \nu-a.s.$$
and
$$X=0, \mu-a.s. \Leftrightarrow \nu \perp \mu \Leftrightarrow X = \infty, \nu-a.s.$$ Can someone explain to me what absolute continuity/singularity implies for their Radon-Nikodym derivative?","['probability-theory', 'measure-theory']"
474915,Determinant of $2 \times 2$ block matrix with commuting blocks,"Let $A,B,C$ and $D$ be $n \times n$ matrices such that $AC = CA$ . Prove that $$\det \begin{pmatrix} A & B\\ C & D\end{pmatrix} = \det(AD-CB)$$ The solution is to first assume that $A$ is invertible and then consider the product $$\begin{pmatrix}
I & O\\
-CA^{-1} & I
\end{pmatrix}\begin{pmatrix}
A & B\\
C & D
\end{pmatrix}=\begin{pmatrix}
A & B\\
O & D-CA^{-1}B
\end{pmatrix}$$ then it is not hard to prove that the claim is true if $A$ in invertible. Finally, we use the fact that the set $GL_n$ form a dense open subset of $M_n$ to get rid of the invertibility assumption. My question is: how to come up with such a weird matrix $\begin{pmatrix}
I & O\\
-CA^{-1} & I
\end{pmatrix}$ ? thank you so much. Is there any other problems that uses the technique of assuming invertibility? (one of which i know is to prove $\det (I+AB) = \det(I+BA)$ ), thanks in advance","['contest-math', 'matrices', 'linear-algebra', 'block-matrices', 'determinant']"
474931,Solutions of a cubic diophantine equation in $\mathbb{Z}/p\mathbb{Z}$,"Suppose $p\in\mathbb{Z}$ is prime and $p\equiv 1\pmod{3}$. Is there an estimate of the number of  solutions of $x^3+y^3=z^3$ in $\mathbb{Z}/p\mathbb{Z}$, preferably using elementary number theory and algebra ? Can something be said about the ""expected"" ( in the sense of Probability Theory ) number of solutions ? I am particularly interested in large values of $p$ and elementary techniques.","['probability-theory', 'elementary-number-theory', 'abstract-algebra', 'number-theory']"
474934,Proof for Euler's Beta function for positive integers,"I googled this, but I didn't really find anything good. So I just want a proof that for positive integers $x$ and $y$: $$\int_{0}^{1} t^{x-1} \cdot (1-t)^{y-1} dt = \frac{(x-1)! \cdot (y-1)!}{(x+y-1)!}$$ Any help is appreciated.","['definite-integrals', 'calculus', 'integration', 'number-theory']"
474939,Diagonalizable matrix with only one eigenvalue,"I have a question from a test I solved (without that question.. =) 
""If a matrix $A$ s.t $A$ is in $M(\mathbb{C})$ have only $1$ eigenvalue than $A$ is a diagonalizable matrix"" That is a false assumption since a ( $n\times n$ matrix) a square matrix needs to have at least $n$ different eigenvalues (to make eigenvectors from) - but doesn't the identity matrix have only $1$ eigenvalue?...","['matrices', 'eigenvalues-eigenvectors']"
474955,How prove this $\sum_{k=1}^{n}\frac{k}{3^k-2^k}<\frac{5}{3}$,"prove that
$$\sum_{k=1}^{n}\dfrac{k}{3^k-2^k}<\dfrac{5}{3}$$ my idea: use 
$$3^k-2^k>2^k(k\ge 2)$$
then
$$\Longleftrightarrow 1+\sum_{k=2}^{n}\dfrac{k}{3^k-2^k}<1+\sum_{k=2}^{n}\dfrac{k}{2^k}<1+\sum_{k=2}^{\infty}\dfrac{k}{2^k}=1+\dfrac{3}{2}>\dfrac{5}{3}$$
use this methods,I can't prove it other idea:
$$3^k-2^k>2\cdot 2^k(k\ge 3)$$ But This same can't prove it
$$\Longleftrightarrow \sum_{k=3}^{\infty}\dfrac{k}{3^k-2^k}<\sum_{k=3}^{\infty}\dfrac{k}{2^k}<\dfrac{2}{3}-\dfrac{2}{5}$$ so This problem have other nice methods? Thank you","['inequality', 'sequences-and-series']"
474975,Logic: cardinality of the set of formulas,"How can you proof that $||L||=|L|$ if $L$ is infinite (where $||L||$ stands for the cardinality of the set of all $L$-formulas and $|L|$ the number of all constants, function and relation symbols)? The case where $L$ is finite is clear for me, because in that case you can just count all the formulas by an induction argument, so $||L||=|\mathbb{N}|$.","['logic', 'elementary-set-theory', 'model-theory']"
474978,$\sec^2\theta+\csc^2\theta=\sec^2\theta\csc^2\theta$,"I was playing around with trigonometric functions when I stumbled across this
$$\sec^2\theta+\csc^2\theta=\sec^2\theta\csc^2\theta$$
Immediately I  checked it to see if it was flawed so I devised a proof
$$\begin{align}
\sec^2\theta+\csc^2\theta&=\sec^2\theta\csc^2\theta\\
\frac1{\cos^2\theta}+\frac1{\sin^2\theta}&=\frac1{\cos^2\theta\sin^2\theta}\\
\frac{\cos^2\theta+\sin^2\theta}{\cos^2\theta\sin^2\theta}&=\frac1{\cos^2\theta\sin^2\theta}\\
\frac1{\cos^2\theta\sin^2\theta}&=\frac1{\cos^2\theta\sin^2\theta}\blacksquare\\
\end{align}$$
I checked over my proof many times and I couldn't find a mistake, so I assume that my claim must be true. So my questions are: Is there a deeper explanation into why adding the squares is the same as multiplying? Is this just a property of these trigonometric functions or do similar relationships occur with other trigonometric functions? And finally as an additional curiosity what does this translate into geometrically?",['trigonometry']
475002,How many factors does 6N have?,"Given a number $2N$ having 28 factors another number $3N$ having 30 factors, then find out the number of factors of $6N$.",['number-theory']
475016,Leibniz notation for high-order derivatives,What is the reason for the positioning of the superscript $n$ in an $n$-order derivative $\frac{d^ny}{dx^n}$?  Is it just a convention or does it have some mathematical meaning?,"['notation', 'calculus', 'derivatives']"
475036,Trig identity problem from Gelfand's Trigonometry,"Having a problem with an exercise from Gelfand and Saul's Trigonometry , in the section dealing with the half-angle formulae. The exercise (7.a. on p.151) asks the reader to show that: $$\tan(\alpha/2)\tan(\beta/2)\ +\ \tan(\beta/2)\tan(\gamma/2)\ +\ \tan(\gamma/2)\tan(\alpha/2)\ =\ 1$$ where $\alpha + \beta + \gamma = \pi$. I made the cotangent substitution for one of the variables and checked it on Wolfram Alpha, which confirms the identity, but for some reason I still can't see how to actually prove it myself. I'm sure I'm missing something very obvious!","['trigonometry', 'algebra-precalculus']"
475053,Euler character of a numerically trivial divisor,"Let $X$ be a complex projective variety (might be singular) and $D$ be a Cartier divisor on $X$. Suppose $D$ is ${numerically}$ trivial, then is the Euler character $\chi(X,D)= \chi(X, \mathcal{O}_X)$? Here numerically trivial means the intersection number of the divisor $D$ with any curve is zero. I saw somebody mentioned this result followed from Riemann-Roch, but did not see the reason (even for the smooth case).",['algebraic-geometry']
475054,On the existence of a notation in matrix calculus,"Is there any especial operator on the function $f(x)$ to represent the following matrix: \begin{bmatrix}
f & \frac{\mathrm{d}}{\mathrm{d} x}f & \frac{\mathrm{d^2}}{\mathrm{d} x^2}f & ... \\ 
f^2 & \frac{\mathrm{d}}{\mathrm{d} x} f^2 & \frac{\mathrm{d^2}}{\mathrm{d} x^2} f^2 & \cdots \\ 
f^3 & \frac{\mathrm{d} }{\mathrm{d} x}f^3 & \frac{\mathrm{d^2}}{\mathrm{d} x^2} f^3 & \cdots\\
\vdots  & \vdots  &\vdots &\ddots
\end{bmatrix} If there is not such operator, how could we express this by means of available operators?","['matrices', 'calculus', 'derivatives']"
475060,Partition of Unity in Spivak's Calculus on Manifolds,"I have a question about partitions of unity specifically in the book Calculus on Manifolds by Spivak. In case 1 for the proof of existence of partition of unity, why is there a need for the function $f$? The set $\Phi = \{\varphi_1, \dotsc, \varphi_n\}$ looks like is already the desired partition of unity. Following is the theorem and proof. Only Case 1 in the proof is relevant.","['multivariable-calculus', 'real-analysis']"
475073,Finding $\lim_{x\to 0} \large \frac {\sqrt{x}}{\sin x}$,"Using L'Hospitals rule I keep on getting $\frac{0}{0}$... But Im not sure if this is correct? $$\lim\limits_{x\to 0} \frac{\sqrt{x}}{\sin x}$$ $$\frac{\frac{1}{2}x^{-\frac{1}{2}}}{\cos x}$$
  $$\frac{-\frac{1}{4}x^{-\frac{3}{2}}}{-\sin x}$$ Thanks in advance for any help.","['calculus', 'limits']"
475102,Why sets that aren't closed can't be compact?,"In $\mathbb{R}^n$ we prove that a set is compact (using the definition about open covers) if and only if it's closed and bounded. It is pretty clear that if $\mathcal{O}$ is an open cover of one unbounded set $X$, then $\mathcal{O}$ cannot have a finite subcover, it'll clearly need in general infinitely many sets to cover the set $X$. Now, if a set isn't closed, I cannot see in which way it fails to be compact. For instance, if $X$ is the closed unit ball centered at the origin, then it is compact. If on the other hand we consider $Y=X\setminus\{0\}$, then it's not compact anymore, because $Y$ isn't closed (the point $0$ is a limit point of $X$ and so, $0 \in \operatorname{Cl}(X)$ and on the same time $0 \notin Y$. So, what should be the intuition about this? How can we intuitively see that $Y$ isn't compact? Thanks very much in advance!","['general-topology', 'compactness', 'real-analysis']"
475124,Curvature form of Riemannian manifold in the principal bundle language,"I am confused by the relation between the principal bundle and ""usual"" approaches to the curvature of a Riemannian manifold $M$ of dim $n$. In the ""usual"" approach the Riemann curvature tensor is defined as
$R(X,Y)Z =(\nabla_X\nabla_Y-\nabla_Y\nabla_X) Z $, in components
$R ^a_{\phantom{a}bcd} =e^a R(e_c,e_d)e_b$. It is globally defined on $M$ and can be viewed as an element $\mathcal{R}\in\Omega^2(M,\mathfrak{o}(n))$, $\mathcal{R}=\mathcal{R}^a_b\; e_a\otimes e^b$, with each $\mathcal{R}^a_b$ a 2-form on $M$, $\mathcal{R}^a_b=(1/2)R^a_{bcd} \;e^c\wedge e^d$. In the principal bundle approach one considers the frame bundle $P$ associated to $M$, an $O(n)$-bundle with base $M$. The curvature $\Omega$ of $P$ is an element of $\Omega^2(P,\mathfrak{o}(n))$, which, being tensorial, can be identified with an element $F_{\Omega}\in \Omega^2(M,\mathrm{ad} P)$, NOT of $\Omega^2(M,\mathfrak{o}(n))$ Provided that what I said above is correct, how do I recover $\mathcal{R}$ from $\Omega$? I would have expected $F_{\Omega}=\mathcal{R}$, but this cannot be the case since they take values in different spaces. NOTATION: $\Omega^2(M,\mathfrak{o}(n))$ denotes the space of 2-forms on $M$ taking values in the Lie algebra of $O(n)$; $\mathrm{ad} P$ is the adjoint bundle of $P$.",['differential-geometry']
475133,When does convergence in $L^p$ imply convergence of the p-th moment?,"Suppose $X_n$ is a sequence of random variables on some probability space $(\Omega,\mathcal{F},\mathbb{P})$. When does convergence in $L^p$,i.e. $$\mathbb{E}[\vert X_n - X \vert ^p]\rightarrow 0,$$ imply convergence of the p-th moment, i.e. $$\mathbb{E}[X_n^p] \rightarrow \mathbb{E}[X^p]$$",['probability-theory']
475138,How to calculate the dimension of the intersection of projection of varieties?,"In $\mathbb P_{n+1}$ we consider $d$ varieties $V_1,\ldots V_d$, each of them is defined by $d-1$ equations, as follows: we have $d-1$ polynomials $f_2(X_0,\ldots,X_n),\ldots,f_d(X_0,\ldots,X_n)$ and $d\cdot(d-1)$ nonzero coefficents $l_{ij}$ ($i=2,\ldots,d$, $j=1,\ldots,d$). The variety $V_j$ is then defined by the polynomials $f_i(X_0,\ldots,X_n)-l_{ij}X_{n+1}^i$ for $i=2,\ldots,d$. Finally, we consider the projection $\pi:\mathbb P_{n+1}\rightarrow\mathbb P_n$ onto the first $n+1$ coordinates. My questions are: how do I compute the dimension of $\bigcap_{j=1}^d\pi(V_j)$? How can I relate this dimension to other quantities which could be easier to calculate, such as the dimension of the $V_j$'s? Answers with assumptions on the given data are as welcome as more general ones.","['algebraic-geometry', 'projective-geometry']"
475144,How to determine if binomial events are independent?,"I have a sequence of binary experiment results, something like 1100010000100... My first hypothesis is that these events are independent, but I'd like to know if there is some way to test this.  I could look at the probability of a 1 immediately following another 1, for example, and see if it is close to the overall probability of a 1, but that's just one possible kind of correlation/dependence.  Is there some more general way to look for patterns? Two things that seem like they might be helpful are Fourier transforms and hidden markov models, but I don't really know enough about either to say whether they apply to this situation.  Even just some pointers for further reading would be very helpful.","['statistics', 'probability']"
475145,Modelling with exact differential equations?,"I'm teaching some very elementary differential equations to engineering students, and their constant question to me is ""What's the use of this?"" or alternatively ""Where would we use this?""  Now, I'm not an applied mathematician, but most of the ""standard"" applications of de's you see in texts seem to include population growth, radioactive decay, cooling, mixing, predator-prey, all of which can be modelled with separable or linear de's. So what I'm trying to find is a fairly simple example - suitable to first year students with limited mathematics - of a practical ""real-world"" application which is most easily modelled by an exact differential equation.  My web searching has drawn a blank, so I'm open to suggestions!","['applications', 'ordinary-differential-equations', 'education']"
475147,Why does Stokes' Theorem allow any surface to be used when calculating a line integral.,"I'm trying to understand Stokes' Theorem, what I don't get is how it allows you to pick any surface as long as the boundary is the same. Let's say that the vector field is increasing in strength along the z-axis, wouldn't then the curl be stronger if you chose a parabola shaped surface with a non-zero z-value compared to the unit disk where z would be zero?","['multivariable-calculus', 'calculus', 'integration']"
475183,Find the coefficient $c_{-3}$ in the Laurent series $g(z) = \frac{e^{iz}-1}{\cos z-1}$,"The function $\displaystyle g(z) = \frac{e^{iz}-1}{\cos z-1}$ has a Laurent expansion of the form $\sum_{n=-\infty}^{+\infty} c_{n}z^{n}$ in the region $2\pi<|z|<4\pi$. Find the coefficient $c_{-3}$. I am not sure how to proceed with problems of this sort, the only method I know to find Laurent series is to manipulate geometric series. I assume in this case you have to do residue calculations? I would appreciate any input very much, I am studying before an exam in complex analysis.","['residue-calculus', 'laurent-series', 'complex-analysis']"
475193,When is a divisible group a power of the multiplicative group of an algebraically closed field?,"It is known that for any algebraically closed field $\mathbb{F}$ its multiplicative group $\mathbb{F}^*$ is a divisible group, and consequently any power $\mathbb{F}^*\times\cdots\times \mathbb{F}^*.$ Now if we have an abelian divisible group $G,$ i would like to know under which conditions $G$ is a power of some algebraically closed field. Thanks.","['abelian-groups', 'group-theory', 'abstract-algebra', 'field-theory']"
475198,Evaluating integration with Laplace transform,I am taking a differential equation class and for Laplace transformations and I have to find $$\displaystyle \int_0^\infty \dfrac{\sin t}{t}dt.$$ How can I do that?,"['ordinary-differential-equations', 'laplace-transform']"
475202,Geometry problem for circle,"Any tips how to proceed?
It's for sure that Ac is the radius but what next?",['geometry']
475235,"Find $\,\,e^{-1/4}\,$ to within $0.0002$","I am using Taylor's Inequality to solve this problem but this formula (page 607) is getting on my nerves and the textbook does not do a good job of explaining how to do this problem and I've gone to khanacademy and patrickjmt and they do not help me. But I will try to interpret the problem anyways.  I know that M is the max value for the nth term I want but added to another.  The remainder must be less than or equal to $.0002$, so $R_n(x) \leq .0002$.  I'm guessing that the max value of M is $1$ but I'm not sure.  I do not know how to solve for what n is then.  I do know the Taylor series expansion for $e^x$ though but I do not know what term I should stop at.  Please help?  I'm extremely frustrated.",['sequences-and-series']
475243,Law of Iterated Expectations example,"I have a question that I hope can be shown using the LIE. There is a urn which contains 3 marbles (2 white and 1 black). The person who gets to pick the black marble gets to win $100 whereas the individual who picks the white marble walks away with nothing. The marbles that are taken out are not replaced. Assuming that there are only 2 individuals, how to show, using LIE that it does not matter whether you are the first or second to pick the marble?
Thanks",['probability-theory']
475251,Explanation of the binomial theorem and the associated Big O notation,"I'm currently following the MIT Single Variable lectures online and the professor states that the binomial theorem for the expansion $(x + \Delta x)^{n} = x^{n} + nx^{n-1}\Delta x + O((\Delta x)^{2})$ How is this derived, and what does the big O term of the expansion represent in terms of the binomial theorem. Just to put this in context, this expression was used when computing the derivative of $x^{n}$ using the limit definition.","['asymptotics', 'calculus', 'algebra-precalculus']"
475255,Path Connectedness of a Set,"I am finding it very difficult to prove or disprove the following statement. If $A$ is a family of countably many lines in $\mathbb{R}^3$ then $\mathbb{R}^3\setminus A$ is path connected. I would appreciate it if somebody could give me an elementary proof of this, perhaps using standard geometry and linear algebra. Thanks for any help.","['general-topology', 'connectedness']"
475256,If this is a telescoping series then how does it collapse? $\frac{3r+1}{r(r-1)(r+1)}$,"Express $$\frac{3r+1}{r(r-1)(r+1)}$$ in partial fractions. Hence, or otherwise, show $$\sum_{r=2}^n\frac{3r+1}{r(r-1)(r+1)}=\frac52-\frac2n-\frac{1}{n+1}$$ So, I have obtained the partial fractions $$\frac{3r+1}{r(r-1)(r+1)}=\frac{2}{r-1}-\frac{1}{r}-\frac{1}{r+1}$$ then for $r=(2,3,4,5...(n-1),n)$ $$
\begin{align}
& {}+2-\frac12-\frac13 \\[8pt]
& {}+1-\frac13-\frac14 \\[8pt]
& {}+\frac23-\frac14-\frac15 \\[8pt]
& {}+\frac12-\frac15-\frac16 \\[8pt]
& {}+\cdots \\[8pt]
& {}+\frac{2}{n-2}-\frac{1}{n-1}-\frac{1}{n} \\[8pt]
& {}+\frac{2}{n-1}-\frac{1}{n}-\frac{1}{n+1}
\end{align}
$$ If this is a telescopic series how does it collapse?","['sequences-and-series', 'algebra-precalculus']"
475259,why $x^2 = y^3$ is not smooth?,"I read a definition of a smooth curve on the plane:
A smooth curve is a map from $[a,b] \to \mathbb R^2: t\mapsto ( f(t),g(t) )$, where $f$ and $g$ are infinitely differentiable functions. According to this definition, $x^2 = y^3$ can be parametrized by $t \mapsto (t^2, t^3)$, so it should be smooth? But it has a special point at $(0,0)$? Can anyone help me please?","['plane-curves', 'calculus', 'differential-geometry']"
475263,Are there any natural measures that can be put on the space of models?,"I've already asked this question is Philosophy.SE and is a 'soft' question. An undecidable proposition, as in Gödel's Incompleteness theroem, is one whose truth value cannot be determined, because it evaluates as true in some models, and false in others. One could argue that this is an opening for a more complex notion of truth. Is there some natural way of establishing a magnitude for this, so perhaps expanding the binary notion of truth (false/true) into a more sophisticated one. Perhaps if a natural probability measure is available on the space of models?","['logic', 'philosophy', 'measure-theory']"
475275,Is every continuous closed surjection also open?,"$f:X\rightarrow Y$ is a continuous closed surjection, $X$ and $Y$ are topological spaces. Is $f$ also open?","['general-topology', 'functions']"
475295,Etale spaces of a presfeaf and the associated sheaf,"Given a presheaf $\mathcal{F}$on a topological space $X$, one can construct the etale space $\pi_1 : Y_1\to X$. Let us now look at the associated sheaf $\mathcal{F}^+$ as a presheaf and construct the etale space $\pi_2 :Y_2\to X$ corresponding to the presheaf $\mathcal{F}^+$. My question is : what is the relation of $Y_1$ to $Y_2$ ? There is a natural map $\tau : \mathcal{F} \to \mathcal{F}^+ $ which gives rise to maps at the stalk level (by taking direct limits) and hence a map $\tau_{ES} : Y_1 \to Y_2 $. Is this map a homeomorphism ? (If $\mathcal{F}$ were a sheaf to begin with, $\tau$ would have been an isomorphism and hence $\tau_{ES}$ would have been a homeomorphism. However I am not sure if this is the case even if we begin with a presheaf which is not a sheaf.)","['general-topology', 'sheaf-theory', 'algebraic-geometry']"
475310,Non-convergence of Cauchy Random Variables,"Suppose $X_1,X_2,\ldots$ is a sequence of Cauchy random variables with density $$f(x)=\frac{1}{\pi(1+x^2)}, \hspace{3mm}x\in \mathbb{R}$$
and let $S_n=X_1+\ldots+X_n$. It's easy to show that $\frac{S_n}{n}$ converges in distribution using characteristic functions (in fact, $S_n/n$ has the same distribution as $X$ for every $n$). On the other hand, $S_n/n$ does not converge in probability. Does this follow from the fact that these random variables are not integrable (if they were, we could apply WLLN)? Or do we need to use the definition of convergence in probability directly?","['probability-theory', 'law-of-large-numbers', 'convergence-divergence', 'probability-distributions']"
475321,"showing $\mathbb{R}\sim{(0,1)}$","Using the function $f:\mathbb{R}\rightarrow\mathbb{R}$ defined by
$$f(x)=\frac{1}{2}\left(1+\frac{x}{1+|x|}\right)$$
Show that $\mathbb{R}\sim(0,1)$ I think this an exercise to show the uncountablility of both sets above, although the chapter I'm reding is on cardinality, so it cleary fits.
So I need to show a bijection. so for injection I must show that
$$f(x_1)=f(x_2)\Rightarrow{x_1}=x_2$$
So
$$\frac{1}{2}\left(1+\frac{x_1}{1+|x_1|}\right)=\frac{1}{2}\left(1+\frac{x_2}{1+|x_2|}\right)$$
$$\frac{x_1}{1+|x_1|}=\frac{x_2}{1+|x_2|}$$
$$x_1(1+|x_2|)=x_2(1+|x_1|)$$
$$x_1+x_1|x_2|=x_2+x_2|x_1|$$
So now the key is to show the equality of this last statement's second second argument, $x_1|x_2|=x_2|x_1|$.  SO I was thinking,
$$\frac{|x_2|}{x_2}=\frac{|x_1|}{x_1}\Rightarrow{x_1}|x_2|=x_2|x_1|$$
Is this valid?  I was thinking it was since 
$$\frac{|x|}{x}=
\begin{cases}
 -1,  & \text{if $x\lt0$} \\ 
  1,  & \text{if $x\gt0$} \\
  0,  & \text{if $x=0$}   \\
\end{cases}
$$","['elementary-set-theory', 'functions']"
475344,Rational function with absolute value $1$ on unit circle,"What is the general form of a rational function which has absolute value $1$ on the circle $|z|=1$? In particular, how are the zeros and poles related to each other? So, write $R(z)=\dfrac{P(z)}{Q(z)}$, where $P,Q$ are polynomials in $z$. The condition specifies that $|R(z)|=1$ for all $z$ such that $|z|=1$. In other words, $|P(z)|=|Q(z)|$ for all $z$ such that $|z|=1$. What can we say about $P$ and $Q$?",['complex-analysis']
475354,How to show that $A^3+B^3+C^3 - 3ABC = (A+B+C)(A+B\omega+C\omega^2)(A+B\omega^2+C\omega)$ indirectly?,"I found this amazingly beautiful identity here . How to prove that $A^3+B^3+C^3 - 3ABC = (A+B+C)(A+B\omega+C\omega^2)(A+B\omega^2+C\omega)$ without directly multiplying the factors? (I've already verified it that way). Moreover, how could someone possibly find such a factorization using complex numbers? Is it possible to find such a factorization because $A^3+B^3+C^3 - 3ABC$ is a symmetric polynomial in $A,B,C$?","['complex-numbers', 'algebra-precalculus']"
475356,Why is such an operator continuous?,"These two questions were in one question of a list of exercises. Let $E$ be a Banach space and $T : E \longrightarrow E^*$ be linear. If $\langle T(x),x \rangle \geq 0$ holds for all $x \in E$, then $T$ is continuous. If $\langle T(x),y \rangle = \langle x ,T(y) \rangle$ holds for all $x, y \in E$, then $T$ is continuous. I tried to expand it, as in the proof of the Cauchy-Schwarz inequality, to get a polynomial of degree $2$. Any solution or hint?","['operator-theory', 'functional-analysis']"
475363,Knights on Multidimensional Torus Chessboard,"How many knights can be placed on an $n \times n \times n \times\dots\times n$ torus board with $n$ odd so that if two knights are in non-attacking position (the knight moves by two squares in one dimension, one square in another), then the distance between them is greater than one king's move (assuming the king takes the shortest path between squares - for instance to move right one step, it does not move diagonal up right and come down)?",['combinatorics']
475370,Quadratic extensions of $\mathbb Q$,"This is a question from Lang's ANT, Thm 6, ch.IV, $\S2$. It states that every quadratic extension of $\mathbb{Q}$ is contained in a cyclotomic extension and that it's a direct consequence of the following result: Let $\zeta_n$ be a primitive $n$-th root of unity for $n$ odd and $$S=\sum_\nu\left(\frac{\nu}{n}\right)\zeta^\nu_n$$ the sum being taken over non-zero residue classes mod $n$. Then $$S^2=\left(\frac{-1}{n}\right)n$$ Here, I guess the brackets denote the Jacobi symbol though Lang doesn't make it clear. Anyway, assuming this, I understand that if the quadratic extension $K$ looks like $\mathbb{Q}(\sqrt{n})$ where n is a positive square-free odd integer then $K\subset \mathbb{Q}(\zeta_n)$. Question : how does the result help us if either $n$ is a negative or an even (square-free) integer? Do we need some other results on cyclotomic extensions? Many thanks in advance.","['quadratic-reciprocity', 'algebraic-number-theory', 'number-theory']"
475403,Is this proof that all metric spaces are Hausdorff spaces correct?,"Let $x$ and $y$ be distinct points of a metric space $M$. Prove that there exist in $M$ disjoint open sets $U$ and $V$ with $x \in U$ and $y \in V$. Let $U$ and $V$ be open balls centered at $a$ and $b$, respectively. Also, $a, b \in M, x \in U, y \in V$. Since $M$ is a metric space, $M$ has a real defined distance function $D(x,y) < \epsilon$. Since $x\ne y$, $y$ is a limit point of $U$ for $D(x,y)<\epsilon.$ That is, $B_{r}(x)\subset U.$ Thus, an open set is contained in $U$, since open balls are open sets. Similarly, $x$ is a limit point of $V$. Applying the same definitions as we did to $U$, we find an open set in $V$. These open sets are disjoint. QED.","['general-topology', 'metric-spaces', 'proof-verification']"
475408,Understanding the definition of the order of an entire function in Ahlfors's Complex Analysis,"Let $f: \mathbb C \to \mathbb C$ be an entire function. The order of $f$ is defined by $$\lambda=\limsup_{r \to \infty} \frac{\log \log M(r)}{\log r}, $$ where $$M(r)=\max_{|z|=r} |f(z)| .$$ Ahlfors in his Complex Analysis claims that ""According to this definition $\lambda$ is the smallest number such that
      $$M(r)\leq e^{r^{\lambda+\varepsilon}} $$ for any given $\varepsilon > 0$ as soon as $r$ is sufficiently large."" Why is this true? My attempt: We know that $$\lambda=\lim_{\rho \to \infty} \sup_{r \geq \rho} \frac{\log \log M(r)}{\log r}. $$ From the definition of the limit we have that for any $\varepsilon>0$, there exists some $\rho_0>0$, such that $$\left\lvert \sup_{r \geq \rho} \frac{\log \log M(r)}{\log r}-\lambda \right\rvert \leq \varepsilon ,$$ for every $\rho \geq \rho_0$. In other words $$\frac{\log \log M(r)}{\log r} \leq \lambda+\varepsilon $$ for every $r \geq \rho_0$. From here it is easy to see that $$M(r)\leq e^{r^{\lambda+\varepsilon}}, $$ for all $r \geq \rho_0$. I cannot see why $\lambda$ is the smallest number with this property. Thanks in advance.","['complex-analysis', 'limsup-and-liminf']"
475412,"Understanding a proof from Serge Lang's ""Linear Algebra"" p. 15","I am working through Serge Lang's undergraduate text: ""Linear Algebra"" and I've gotten hung up on a particular claim in one of his proofs on p. 15. The result is: Theorem 3.1: Let $V$ be a vector space over the field
$K$. Let $\left\{ v_{1},\ldots,v_{m}\right\} $ be a basis of $V$
over $K$. Let $w_{1},\ldots,w_{n}$ be elements of $V$, and assume
that $n>m$. Then $w_{1},\ldots,w_{n}$ are linearly dependent. His proof starts out in the following way. Proof: Assume that $w_{1},\ldots,w_{n}$ are linearly independent. Since $\left\{ v_{1},\ldots,v_{m}\right\} $ is a basis, there exists elements $a_{1},\ldots,a_{m}\in K$ such that $w_{1}=a_{1}v_{1}+\cdots+a_{m}v_{m} $. And here is the part that I am having trouble with: By assumption, we know that $w_{1}\neq O$ and hence some $a_{i}\ne0$. I do not understand how he deduces that $w_{1}\neq O$. He has assumed that $w_{1},\ldots,w_{n}$ are linearly independent, which means that if we have a a linear combination of the form $a_{1}w_{1}+\cdots+a_{n}w_{n}=O$ then all of the coefficients $a_{1},\ldots,a_{m}$ must be zero. But I do not understand how to deduce from this that a particular $w_{i}\neq O$. It seems like there should be a simple explanation, but I have not been able to puzzle it out. Thanks in advance for the help!",['linear-algebra']
475439,Help with a simple problem involving a functional inequality (trying to prove Gronwall's inequality),"So while trying to prove Grownwall's inequality, my proof led me to the following statement: $h'(x) \le h(x)g(x)$. Now when $h'(x)=h(x)g(x)$ the following holds: $h(x)=k \exp G(x)$, where $k$ is a constant number. Can I conclude from this that $h(x) \le k \exp G(x)$ (since $h'(x)<h(x)g(x)$)?","['inequality', 'calculus', 'integration', 'real-analysis', 'analysis']"
475459,How prove this inequality generalized from 1969 IMO problem 6,"Let 
  $x_{1},x_{2},y_{1},y_{2},z_{1},z_{2},w_{1},w_{2} $ are all positive numbers, and such
  $$x_{1}y_{1}z_{1}-w^3_{1}>0,\; \text{ and }\;x_{2}y_{2}z_{2}-w^3_{2}>0.$$
  show that
  $$\dfrac{16}{(x_{1}+x_{2})(y_{1}+y_{2})(z_{1}+z_{2})-(w_{1}+w_{2})^3}\le\dfrac{1}{x_{1}y_{1}z_{1}-w^3_{1}}+\dfrac{1}{x_{2}y_{2}z_{2}-w^3_{2}}.$$ This problem is created by me, and the background is the 1969 IMO problem 6, please see: http://www.artofproblemsolving.com/Forum/viewtopic.php?p=363659&sid=efafc24957d8919546afe4254638aea6#p363659 Thank you everyone for showing a proof, I still can't prove it.","['multivariable-calculus', 'convex-analysis', 'inequality', 'contest-math']"
475484,Addition inequalities with lim sup and lim inf,"$\lim\inf a_n+\lim\sup b_n\le\lim\sup(a_n+b_n)\le\lim\sup a_n+\lim\sup b_n$ For the right inequality, I assume $A=\lim\sup a_n, B = \lim\sup B_n$. Hence for any $\varepsilon$, there exists $n_0$ such that $a_{n}<A+\varepsilon$ for all $n\ge n_0$. Similarly for $b_n$. Hence for any $\varepsilon$, there exists $n_0$ such that $a_n+b_n<A+B+\varepsilon$ for all $n\ge n_0$. Therefore $\lim\sup(a_n+b_n)\leq A+B$. Now for the left inequality, assume $A=\lim\inf a_n, B = \lim\sup B_n$. How can this be compared to $\lim\sup(a_n+b_n)$?",['real-analysis']
475499,Non strictly singular operators,"Let $X$ be a separable Banach space and let $T:X\to X$ be a bounded operator that is not strictly singular. Can we always find an infinite dimensional, closed, and complemented subspace $Y$ of $X$ such that $T:Y\to T(Y)$ is an isomorphism? I cannot show this is true, and I cannot think of a counterexample. The only idea of counterexample I had was looking at HI spaces, where every operator is of the form $\lambda I+S$, where $S$ is strictly singular. However, as far as I know, such operators (when $\lambda\neq 0$) are isomorphisms on a closed, finite-codimensional subspace, thus complemented.","['operator-theory', 'functional-analysis', 'banach-spaces']"
475500,A basic question on determinant and rank of a matrix,How to prove that if the determinant of a $n \times n$ matrix is zero then the rank is less than $n$. I can prove the converse. Only a hint is enough. My definition of rank is the maximum number of linearly dependent columns.,"['matrices', 'linear-algebra', 'determinant']"
475506,$\lim_{n \to \infty} 2^n\cos\left(\frac{\pi}{2^n}\right)\sin\left(\frac{\pi}{2^n}\right)$ (Without L'Hospital),"I'm trying to find
$$\lim_{n \to \infty} 2^n\cos\left(\frac{\pi}{2^n}\right)\sin\left(\frac{\pi}{2^n}\right)$$
I think the answer is $\pi$, but I don't know how to find it.
Could you please show me the shortcut?",['limits']
475528,Functions with the same derivatives [duplicate],"This question already has answers here : Prove that $C e^x$ is the only set of functions for which $f(x) = f'(x)$ (9 answers) Closed 10 years ago . Functions with the same derivatives Besides, $f(x)=e^x$,
 what other non-zero functions are the same as their derivatives no matter how many times the derivative is taken?","['calculus', 'functions']"
475556,Book searching in Pluripotential theory,"Can anyone recommend me a book on pluripotential theory with an intuitive approach? I have some course notes on that subject, but it's really abstract and theoretical. I want to understand why pluripotential/subharmonic/... were introduced and so on. My teacher wanted us to learn the book's Z.Blocki... Ex: I don't understand :( 1/ Suppose that $u: \Omega \to \mathbb{R} $, a function $u$ is called harmonic if $u$ is continouns and $$u(x_0)=\dfrac{1}{V_n(B(x_0,r))}\int_{B(x_0,r)}u(x)\mathrm{d}V_n(x)$$. 2/ We have $u(x_0)=\dfrac{1}{\sigma(\partial B(x_0,r))}\int_{\partial B(x_0,r)}u(x)\mathrm{d}\sigma(x)$. Your comments & suggestions are ALWAYS appreciated.","['measure-theory', 'several-complex-variables', 'harmonic-analysis', 'reference-request', 'complex-analysis']"
475579,Finding local and global extrema even when the determinant of the Hessian zero,"I am trying to solve the following problem: Let $f: \mathbb R^2\rightarrow\mathbb R$  be a function defined by
  $$
f(x,y) = x^{2n} + y^{2n} - nx^2 + 2nxy - ny^2,
$$
  where $n$ is a natural number greater than 1.
  Decide whether $f$ has a (global) minimum.  Also find all the points at which $f$ attains its local maxima and local minima. I calculated the partial derivatives $f_x = 2nx^{2n-1}-2nx+2ny$, $f_{xx} = 2n(2n-1)x^{2n-2}-2n$, $f_{xy} = 2n$, and so on.  But I got an equation system $f_x = f_y = 0$ of degree three, which I had hard times solving it.  I found out that $(x,y) = (0,0)$ is one of its solutions, but at that point the the determinant of the Hessian is zero, from which I could not conclude whether it was a local extremum. I was not sure about how to prove that the minimum value of $f$ exists, either. I would be most grateful if you could help me solve this problem.,","['multivariable-calculus', 'calculus']"
475592,Proof that $(A-B)\cap C = (A\cap C)-B$.,"Let $A,B,$ and $C$. Prove that $(A-B)\cap C = (A\cap C)-B$. Suppose $x\in (A-B)\cap C$, then $x$ is in $(A-B)$ and $C$. If $x\in (A-B)$ then $x\notin B$ but $x\in A$ hence $x\in A\cap C$, but not in $B$ which means $x\in (A\cap C)-B$ therefore $(A-B)\cap C \subset (A\cap C)-B$. Now suppose $y\in (A\cap C)-B$ then $y$ is in $A$ and $C$, but not in $B$ which is the same as $y\in (A-B)\cap (C-B)$, but $ (A-B)\cap (C-B) \subset (A-B)\cap C$ therefore $(A\cap C)-B \subset (A-B)\cap C$. Did I do this correctly?",['elementary-set-theory']
475613,Nonempty set mapped to $\emptyset$ and vice-versa,"$(a)$ How many functions are there from a nonempty set $S$ into
  $\emptyset$? $(b)$ How many functions are there from $\emptyset$ into an arbitrary
  set $S$? This question seems very simplistic but I don't know the answer. I think for $(a)$ that there isn't a function that maps a set $S$ into a empty set? For $(b)$ I assume it to be all function that map the empty set to an arbitrary set since all sets contain the empty set?",['elementary-set-theory']
475655,Infinite Series $\sum\limits_{n=1}^{\infty}\frac{(m-1)^n-1}{m^n}\zeta(n+1)$,How to prove the following identity? $$\sum_{n=1}^{\infty}\frac{(m-1)^n-1}{m^n}\zeta(n+1)=\pi\cot\left(\frac{\pi}{m}\right)$$,"['complex-analysis', 'closed-form', 'sequences-and-series', 'real-analysis']"
475657,"If $S$ is a nonempty subset of group $G$, then $S^{|G|}$ is a subgroup of $G$.",Let $G$ be a group with $|G| = n$ and let $ \emptyset \ne S \subseteq G$ . I want to show that $S^n$ is a subgroup of $G$ where by $S^n$ I mean the set $\lbrace s_1\cdots s_n \; | \; s_i \in S\rbrace$ .,"['finite-groups', 'group-theory', 'abstract-algebra']"
475664,Is an ideal generated by multilinear polynomials of different degrees always radical?,"Definition. A polynomial $f\in\Bbbk[x_0,\ldots,x_n]$ is called multilinear if $\deg_{x_i}(f)=1$ for each $0\le i \le n$. In other words, $f$ is linear in each variable. If $f$ is homogeneous of degree $d$, then $f$ is a linear combination of monomials of the form $x_{i_1}\cdots x_{i_d}$ with $0\le i_1<i_2<\cdots<i_d\le n$. I tried to answer a question and ended up with an ideal $I=(f_1,\ldots,f_r)\subseteq\Bbbk[x_0,\ldots,x_n]$ with the property that the $f_i$ are irreducible, homogeneous, multilinear polynomials of (pairwise) different degrees. The question is now whether such an ideal is always radical. If it is false, I would love to see a counterexample. If it is true, then I am sure that the assumption on the degree can not be dropped (see this example of an ideal generated by irreducible, homogeneous, multilinear polynomials which is not radical). I am not so sure if it matters for the polynomials to be irreducible. I would also love to see a proof, of course. Thanks a lot in advance.","['algebraic-geometry', 'abstract-algebra', 'polynomials', 'commutative-algebra', 'ideals']"
475679,Mathematical Analysis help,"Let $f$ be differentiable on $(0,\infty)$ and suppose $\lim_{x\rightarrow\ \infty}(f(x)+f'(x))=L$. Then, $\lim_{x\rightarrow\ \infty}f(x)= L$ and $\lim_{x\rightarrow\ \infty}f'(x)= 0$ My approach: $\lim_{x\rightarrow\ \infty}f(x)= \lim_{x\rightarrow\ \infty}\frac{f(x)e^x}{e^x} =\lim_{x\rightarrow\ \infty}\frac{f(x)e^x+f'(x)e^x}{e^x }=L$. But this is only valid if we can show $\lim_{x\rightarrow\ \infty}f(x)e^x=\infty$. Could anyone advise please?","['functions', 'convergence-divergence', 'real-analysis', 'limits']"
475689,local parametrization of regular surface,"I am doing excercises of Do Carmo's dg of curves and surfaces Chapter 2.2 and need some help with the following excercise: Show that the set $S=\{(x,y,z)\in R^3;z=x^2-y^2\}$ a regular surface and check (a) and (b) are parametrizations of $S$. Also : answer: which part of S do these parametrizations 
cover? (a) $x(u,v)=(u+v,u-v,4uv),(u,v)\in R^2$ (b) $x(u,v)=(u \cosh v, u \sinh v, u^2), (u,v)\in R^2 , u\neq 0$ Part (a) is straightforward. Since S is a regular surface, by prop 2.2.4 it suffices to chek x is differentiable , one-to-one and  and differential is one-to-one. And it's quite straightforward to check these properties. Also, a should cover the whole $S$. However, for (b) although x is differentiable and I can check that dx is one-to-one, x is simply not one-to-one and hence not a homeomorphism. I am wondering if there's any other definition for local coordinates (or parametrization) so that we don't need one-to-one propert. For example , I only need to show it is locally homeomorphic.Also, I am not quite sure which part of $S$ that x in (b) covers?","['multivariable-calculus', 'manifolds', 'differential-geometry']"
475722,Approximation of measurable function by simple functions,"This is from Tao Exercise 19.1.3. Let $\Omega \subseteq \mathbb R^n$ measurable and $f: \Omega \rightarrow \mathbb R$ be measurable. Assume $f \geq 0$ on $\Omega$ . Then there exists a sequence $\{f_n: \Omega \rightarrow \mathbb R\}_{n=1}^\infty$ s.t. all $f_n$ are simple, non-negative and increasing and $f_n \rightarrow f$ pointwise on $\Omega$ . Tao gives the hint to define $$f_n(x) := \sup \left \{\frac j {2^n} : j \in \mathbb Z, \frac j {2^n} \leq \min(f(x),2^n) \right \}$$ Can someone help me to prove why all $f_n$ are measurable ?!",['measure-theory']
475723,Groups under Multiplication,"Let $G=GL(2,\mathbb R)$ and $ H =\left\{ 
\left[\begin{array}{ccc|c}
a & 0 \\
0 & b
\end{array} \right]:\mbox {$a$ and $b$ are nonzero integers }\right \}$ under the operation matrix multiplication. Disprove that $H$ is a subgroup of $G=GL(2,\mathbb R)$. Well I have learned that I have to prove that: 1) Show that e∈H (where e is the identity) 2) Assume that a∈H , b∈H 3) Show that ab∈H 4) Show that $(ab)^{-1}$ (Inverse) So I know that It does not hold, But how do I prove that? When I try to prove that e∈H I only get the Identity matrix and that holds because I get that a and b are nonzero integers. When I prove that a and b is in the set I get that it is because both a and b are nonzero integers and that is the identity matrix. Now I proved that a.b∈H as follows: $$ H = 
\left[ \begin{array}{ccc|c}
a & 0 \\
0 & b
\end{array} \right]
\left[ \begin{array}{ccc|c}
c & 0 \\
0 & d
\end{array} \right] = 
\left[ \begin{array}{ccc|c}
ac & 0 \\
0 & bd
\end{array} \right]
$$
With a = b = c = d = 1 I get the Identity matrix again. Now I know if a = b = 2 the subgroup will not hold because the inverse will be a set of rational numbers and H is only a subgroup if it contains only Integer. Is my reasoning correct and if not where did I go wrong?","['matrices', 'group-theory', 'abstract-algebra']"
475732,Finding the limit as $n\to\infty$,This is how I proceeded with this question but the answer in my book is given to be 0. Is my method correct? Please help so that I can correct myself in case of any mistake.,['limits']
475760,Prove that $\Gamma_I(\frac{M}{\Gamma_I(M)})=0$,"I was trying to prove this theorem (problem): Suppose that $R$ is a commutative ring with identity, $I\unlhd R$, and $M$ an $R$-module. We define: $$\Gamma_I(M)=\bigcup_{n\geq0}\operatorname{Ann}_M(I^n)$$
  in which for each natural $n\geq 0$:
  $$\operatorname{Ann}_M(I^n)=\{x\in M\;;\;I^nx=0\}.$$
  Prove $$\Gamma_I\left(\frac{M}{\Gamma_I(M)}\right)=0.$$ Note that $\Gamma_I(\cdot)$ will be defined for any $R$-module naturally.
I can show $\Gamma_I(M)\leq M$, but couldn't prove the latter claim.
please show me how to prove this theorem. Thanks in advance.","['local-cohomology', 'ring-theory', 'abstract-algebra', 'modules', 'commutative-algebra']"
475763,A basic question on diagonalizability of a matrix,"I am following a book where the ""diagonalizability"" has been introduced as follows: Consider a basis formed by a linearly independent set of eigen vectors $\{v_1,v_2,\dots,v_n\}$ . Then it is claimed that with respect to this
  basis, the matrix $A$ is diagonal. I am confused at the word ""basis"" here. In some other books it is said that a  matrix $A$ is called diagonalizable if there exists matrix $P$ such that $P^{-1}AP$ is a diagonal matrix. I don't think that diagonalizability has anything to do with basis. It just happened that the set of eigen vectors form a basis of $\Bbb R^n$ . Also I don't think having a set of $n$ linearly independent eigen vectors is a necessary condition for a matrix to be diagonalizable.","['matrices', 'linear-algebra', 'diagonalization']"
475769,Prime Harmonic Series $\sum\limits_{p\in\mathbb P}\frac1p$,"We have following identity: ($p$ is a prime number)
$$\left(1+\frac{1}{p}\right)\sum_{k=0}^n\frac{1}{p^{2k}}=\sum_{k=0}^{2n+1}\frac{1}{p^k}$$
Now, How to derive the following inequality from the above identity? Or use this identity to prove following inequality?
$$\prod_{p<N}\left(1+\frac{1}{p}\right)\sum_{k<N}\frac{1}{k^2}\ge\sum_{n<N}\frac{1}{n}.$$
This inequality shows that $\sum\frac{1}{p}$ is divergent.","['number-theory', 'analytic-number-theory', 'real-analysis', 'analysis', 'prime-numbers']"
475786,How to compute $\lim_{n\rightarrow\infty}\frac1n\left\{(2n+1)(2n+2)\cdots(2n+n)\right\}^{1/n}$,"If
$\displaystyle f(n)=\frac1n\Big\{(2n+1)(2n+2)\cdots(2n+n)\Big\}^{1/n}$, then $\lim\limits_{n\to\infty}f(n)$ equals:
$$
\begin{array}{}
(\mathrm{A})\ \frac4e\qquad&(\mathrm{B})\ \frac{27}{4e}\qquad&(\mathrm{C})\ \frac{27e}{4}\qquad&(\mathrm{D})\ 4e
\end{array}
$$
I couldn't get the right way to start off with this problem. But, as the options include the constant $e$ I think I will have to work out with logarithms. So, this is what I did. 
$$
\lim_{n \to \infty} f(n)\\
=\mathrm{exp}\left(\ln\left(\lim_{n \to \infty} f(n)\right)\right)\\
=\mathrm{exp}\left(\lim_{n \to \infty}\left[\ln\left(f(b)\right)\right]\right)\\
=\mathrm{exp}\left(\lim_{n \to \infty}\left[\frac{1}{n^2}\left(\ln(2n+1)+\ln(2n+2)+\ldots+\ln(2n+n)\right)\right]\right)
$$
I'm not able to proceed further. In case my method is correct please give me hints on proceeding further and in case it is wrong give me the same on another method.","['radicals', 'logarithms', 'limits']"
475797,Graph the following function: $y = \sin \frac{1}{2}\left(x-\frac{\pi}{6}\right)$,"I am working my way through Trigonometry by Gelfand and Saul. I am trying to work out the following question on p 183: Graph the following function:
  $$y = \sin \frac{1}{2}\left(x-\frac{\pi}{6}\right)$$ I would like to check my understanding is correct. Firstly the authors talk about the importance of ensuring the equation is in the standard form of $y = a \sin k (x - \beta)$. I think $y = \sin \frac{1}{2}\left(x-\frac{\pi}{6}\right)$ is already in this standard form. Secondly I have drawn the graph (it is the lower one). Is it correct? Sorry it is hand drawn, I tried using WolframAlpha, but it would not let me format the x-axis using radians how I wanted. Finally, does $$\sin \frac{1}{2}\left(x-\frac{\pi}{6}\right) = \sin\left(\frac{x}{2} -\frac{\pi}{12}\right)$$ so that $y = \sin \frac{1}{2}\left(x-\frac{\pi}{6}\right)$ is equivalent to writing  $y =\sin\left(\frac{x}{2} -\frac{\pi}{12}\right)$?",['trigonometry']
475800,Logic: Teichmüller-Tukey Lemma and the Axiom of Choice [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question How can you proof that the Teichmüller-Tukey Lemma (which says that if $S$ is nonempty and of finite character, $S$ contains a maximal element with respect to the subset ordering), implies the Axiom of Choice? Any hints or solutions will be appreciated!","['logic', 'elementary-set-theory', 'axiom-of-choice']"
475802,Automorphism groups of self-complementary graphs,Does every self-complementary graph has a non-trivial automorphism group?,"['graph-theory', 'group-theory', 'combinatorics']"
475803,How can I find which function corresponds to a set of data points?,"Suppose I have a set of data points like this: 1;1
2;4
3;9
4;16
5;25
6;36
... The first column is the input of the function and the second one is the result. I can tell if I look at it that the function here is y = f(x^2) . The problem is that the data points are not always this obvious and I want to write a program which can tell me the definition of the function. How can I do so? Some context: I'm trying to write a program which can be used to analyze asymptotic time complexity of algorithms. So in fact I'm only interested in a predefined set of functions: y = f(n) y = f(log(n)) y = f(n * log(n)) y = f(n ^ m) y = f(2 ^ n)","['interpolation', 'extrapolation', 'functions']"
475820,Singular solution to $(x+2y)y'=1$,"I have a problem and I got most of the solution, but don't understand how to proceed. The problem is to solve: $$(x+2y)y'=1, \qquad y(0)=-1.$$ Here is my reasoning: Substitute $z = x+2y$. Then $z'=1+2y'$. So $y'=\frac{z'-1}{2}$. So $\frac{z'-1}{2} = \frac{1}{z}$. So $z' = \frac{2+z}{z}$. So $\frac{dz}{dx}$ ($x$ taken without loss of generality) = $\frac{2+z}{z}$. So $\frac{zdz}{2+z} = dx$. So $z - 2\ln|z+2| = x+C$. Therefore, 
$$(x+2y) - 2\ln|x+2y+2| = x+C.$$ Am I correct that there is no member of this one-parameter family of solutions that satisfies the initial condition $y(0)=-1$, because plugging in does not work? Also, is there a singular solution, and how to find it?? Thanks.","['singular-solution', 'ordinary-differential-equations']"
475824,Differentiate $x \sqrt{1+y}+y \sqrt{1+x}=0$ [duplicate],"This question already has answers here : If $x\sqrt{1+y}+y\sqrt{1+x}=0$ find $y'$ (3 answers) Closed 6 years ago . If $x \sqrt{1+y}+y \sqrt{1+x}=0$, prove that $(1+x^2)\frac{dy}{dx}+1=0.$ The answer I got is $$\frac{dy}{dx}= -\frac{2 \sqrt{1+x} \sqrt{1+y}+y}{x+2 \sqrt{1+x}\sqrt{1+y}}$$ but I cannot simplify it further. Please provide your assistance.",['derivatives']
475834,Restriction of a differentiable map $R^3\rightarrow R^3$ to a regular surface is also differentiable.,"This is again an excercise from Do Carmo's book. Prove: if $f:R^3 \rightarrow R^3$ is a linear map and $S \subset R^3$ is a regular surface invariant under $L,$ i.e, $L(S)\subset S$, then the restriction $L|S$ is a differentiable map and $$dL_p(w)=L(w), p\in S,w\in T_p(S).$$ My attempt: Since any linear map on $R^3$ can be represented by a linear transformation matrix , it must be differentiable.  By definition I have to show that for any local parametrization of S say $(U,x)$, map defined by $x^{-1}\circ L \circ x:U\rightarrow U  $ is differentiable locally. Now, both $x$ and $L$ are differentiable , however , $x^{-1}$ is not necessarily differentiable. Moreover, example 3, page 74 of Do Carmo's says : Let $S_1$ and $S_2$ be regular surfaces. Assume that $S_1\subset V \subset R^3$ where $V$ is an open subset of $R^3$, and that $\phi:V \rightarrow R^3$ is a differentiable map such that $\phi(S_1)\subset S_2$. Then the restriction $\phi|S_1: S_1\rightarrow S_2$ is a differentiable map. This fact is left without proof, but I think it might be useful for the question. Can anyone give me some help ? Thanks in advance","['differential-topology', 'surfaces', 'differential-geometry']"
475837,Is the number 8 special in turning a sphere inside out?,"So after watching the famous video on youtube How to turn a sphere inside out I noticed that the sphere is deformed into 8 bulges in the process. Is there something special about the number 8 here? Could this be done with any number of bulges, including 2? Image: Video: How to Turn a Sphere Inside Out","['general-topology', 'differential-topology', 'soft-question']"
475840,A formula for n-derivative of the inverse of a function?,"Let $y=f^{-1}(x)$. As we know: \begin{align}  \frac{\mathrm{d} y}{\mathrm{d} x}=\frac{1}{{f}'(y)}  \end{align}
Thereof we have: \begin{align}  \frac{\mathrm{d^2} y}{\mathrm{d} x^2}=\frac{-{f}''(y)}{({f}'(y))^3}  \end{align} \begin{align}  \frac{\mathrm{d^3} y}{\mathrm{d} x^3}=\frac{3({f}''(y))^2-{f}'(y){f}'''(y)}{({f}'(y))^5}  \end{align}
Is there a general rule for \begin{align}  \frac{\mathrm{d^n} y}{\mathrm{d} x^n}=?\end{align}","['calculus', 'functions', 'inverse', 'induction', 'derivatives']"
475841,Absolutely convergent series has constant sum,"Show that the sum of an absolutely convergent series does not change if the terms are rearranged. Let the absolutely convergent series be $a_1,a_2,\ldots$, and let the rearrangement be $b_1,b_2,\ldots$. By the Cauchy criterion of convergence, since the series $|a_1|,|a_2|,\ldots$ converges, it follows that for any $\epsilon$, there exists $N$ such that $|a_m|+|a_{m+1}|+\ldots+|a_n|<\epsilon$ for all $n>m>N$. We'll show that the series $b_i$ converges. Let $N'$ be the greatest index such that $b_{N'}$ comes from one of $a_1,a_2,\ldots,a_N$. Then for all $n>m>N'$, we have $|b_m+b_{m+1}+\ldots+b_n|\leq |b_m|+|b_{m+1}|+\ldots+|b_n|<\epsilon$. So the series $b_i$ converges. How can we show it converges to the same sum as $a_i$?",['real-analysis']
475859,"Modulo Problem, Fermat's little theorem","Find the value of the unique integer x satisfying $O \le x \le 17$ for which $$
4^{1024000000002} \equiv x\pmod{17}
$$ I think this is related to Fermat's little theorem. I'm knowledgeable with the Chinese remainder theorem and just need some advice on solving this .","['modular-arithmetic', 'congruences', 'number-theory']"
475862,Step Function and Simple Functions,"Definitions: Simple Function: Any functions that can written in the form:$$s(x)=\sum_{k=1}^na_n\chi_{A_n}(x).$$
Note the finite terms here. It should follow that neither all simple functions are step functions, nor all step functions are simple function. e.g. Would not Cantor Function or Devil's Staircase  be example of step function but not simple (note again the finite)? I am asking just to be clear because I read on online notes somewhere that all step functions are simple but not converse.","['measure-theory', 'terminology', 'characteristic-functions']"
475884,Confused about differentiation,"I'm new to calculus and have been taught that $\displaystyle \frac{dy}{dx}$ is the rate of change of y with respect to x. Does $\displaystyle \frac{dy}{dx}$ show how much the variable y changes as x changes? Is there more to understanding this part of calculus, I feel as I'm missing the fundamentals behind differential calculus. Also one thing that I didn't understand is when doing u-substitution integration if we let $u=2x +1$ for example, sometimes I see $\displaystyle du = 2dx \therefore dx = \frac{du}{2}$.
What is this known as and why does it work? My teacher school finds $\displaystyle \frac{du}{dx}$ and rearranges this to make $dx$ the subject. Is this an incorrect practice? I have searched for this on here and cannot find a definite answer. What is the correct notation to be used? When differentiating y = f(x) are we always operating on y as in $\displaystyle \frac{d}{dx} (y)$ = $\displaystyle \frac{dy}{dx}$. If you differentiate x^2 w.r.t x as in $\displaystyle \frac{d}{dx}x^2$, are you finding how much x^2 changes as x changes e.g if $x = 1, x^2 = 1, x = 2, x^2 = 4, x = 3, x^2 = 9$ so $x^2$ is 2 times the value of x? If we have something like $y^3$, what does it mean to differentiate $y^3$ with respect to x as in $\displaystyle \frac{d}{dx} y^3$ and how is it done? thanks, I have been looking for the solutions to my problems for quite a while but cannot find an answer that leaves me satisfied. Sorry if questions likethese are not to be asked here.","['nonstandard-analysis', 'calculus', 'intuition', 'soft-question']"
475887,Bounded operators with prescribed range - part II,"This is a continuation of the question bellow, in a more particular case. Bounded operators with prescribed range If $X$ is a separable Banach space and $Y$ is a closed , infinite dimensional subspace of $X$, can one always find an bounded operator $T$ on $X$ with $\rm{Range}(T)=Y$? Even weaker version: Can one always find an bounded operator $T$ with infinite dimensional range such that $\rm{Range}(T)\subseteq Y$? The last question feels much weaker and I suspect it is true. Intuitively, it would be strange if a large 'chunk' of Banach space is not 'visited' by any bounded operator (defined on the entire space).","['operator-theory', 'functional-analysis', 'banach-spaces']"
475893,An infinite sum from Green function,"When reading the Green function for differential equation, I faced this sum and feel hard to deduce its result directly: $$\frac{2}{\pi^2} \sum_{n=1}^\infty \frac{\sin n\pi x \sin n\pi y}{n^2}$$ I knew the result: When $x \leqslant y, \qquad (1-y)x$ ; when $x >y, \qquad (1-x)y$ My question is how to obtain the result from the infinite sum ? Thank you for your time. ++++++++++++++++++++ Update ++++++++++++++++++++ Thank @njguliyev a lot. I am exciting since this problem has a simple solution, if one know the series njguliyev gave. In fact, I keep this problem for a long time, which comes from the book by Hilbert & Courant, they simply gave the final result without analysis. I had a solution, but I didn't satisfied with it, I will give the detail below, so anyone find anything improper, please tell me your idea. Not like njguliyev's solution,  my way is not so elegant. Tow formulas is needed: Poisson summation formula : 
$$\sum_{n=-\infty}^\infty \psi(2\pi n) = \frac{1}{2\pi} \sum_{\nu = \infty}^\infty \int_{-\infty}^\infty \psi(\tau) e^{-i\nu \tau} \mathrm{d}\tau
$$ The Fourier Transformation :
$$
\int_{-\infty}^{+\infty} \frac{1}{t^2} \sin^2(at) e^{-i\omega t} \mathrm{d} t = \begin{cases} \pi\left(|a| - \frac{|\omega|}{2}\right), &\mbox{if  } |\omega| \leq 2a \\
0, &\mbox{if   } |\omega| > 2a\end{cases}
$$ And $x,y \in [0,1]$, if $n=0$ in the original sum formula, I took the limit value, as the well-known result $\lim\limits_{x \rightarrow 0} \frac{\sin x}{x} = 1$:
$$
\frac{\sin n\pi x\ \sin n\pi y}{n^2}\bigg|_{n=0} = \pi^2 xy
$$ therefor:
$$
\begin{align*}
\frac{2}{\pi^2} \sum_{n=1}^\infty \frac{\sin n\pi x \sin n\pi y}{n^2} 
&= \sum_{n=-\infty}^\infty \frac{1}{\pi^2} \frac{\sin n\pi x \sin n\pi y}{n^2} - xy \\
&= 4\cdot \sum_{n=-\infty}^\infty\frac{1}{(2n\pi)^2}\bigg[ \sin^2(2n\pi \frac{x+y}{4}) - \sin^2(2n\pi \frac{x-y}{4})\bigg] - xy\\
 &= \frac{4}{2\pi}\sum_{\nu = -\infty}^\infty \pi \bigg(\frac{x+y}{4} - \frac{|\nu|}{2}\bigg) - \frac{4}{2\pi}\sum_{\nu=-\infty}^\infty \pi\bigg(\frac{|x-y|}{4} -\frac{|\nu|}{2}\bigg) - xy \\
&=\begin{cases} (1 - y)x \qquad & x\leq y \quad \text{note: } x,y \in [0,1]\\
(1-x)y \qquad & x >y
\end{cases}
\end{align*}
$$","['sequences-and-series', 'ordinary-differential-equations', 'calculus']"
475894,What is a principal orbit,"I am currently reading a paper about Einstein manifolds. There is a comment where I don't know exactly the meaning of the words, namely a certain metric has a group of isometries of dimension $4$ which admits a principal orbit of dimension $3$. What does the principal orbit describe? Many thanks for your help!","['ordinary-differential-equations', 'lie-groups', 'riemannian-geometry']"
475907,Convergence of $\{nz^n\}_1^{\infty}.$,"Discuss completely the convergence and uniform convergence of the sequence $\{nz^n\}_1^{\infty}.$ If $|z|\geq 1$, then $|nz^n|=n|z|^n\geq n$ diverges, so the sequence $nz^n$ also diverges. If $|z|<1$, it should converge to $0$. So for any $\varepsilon$, we must find $N$ such that $|nz^n|=n|z|^n<\varepsilon$, or in other words $|z|^n<\dfrac{\varepsilon}{n}$ for all $n\geq N$. It should be true since the left hand side converges rapidly to $0$, but how to prove it rigorously? Then finally, the sequence doesn't converge uniformly in the open disk $|z|<1$, because if it did, for any $\varepsilon$ we must have $N$ such that $|z|^n<\dfrac{\varepsilon}{n}$ for all $|z|<1$ and all $n\geq N$. But we can choose $|z|$ large enough (close enough to $1$) to break this inequality. So my question is: how to prove that for any $a\in(-1,1)$ and any $\epsilon>0$, there exists $N$ such that $a^n<\dfrac{\varepsilon}{n}$ for all $n\geq N$.",['complex-analysis']
475917,"How to find position of a point based on known angle, radius and center of rotation?","I'm having a hard time remembering trig, and I have spent some time trying to solve this. How do I find the coordinate of a point on a circle for certain angle if we know radius of circle and a center of circle coordinate?",['trigonometry']
475921,Is there any relationship between Cauchy-Riemann equations and vector fields on manifolds?,"Well, suppose we have $f : \mathbb{C} \to \mathbb{C}$ analytic, then if $f = u + iv$ the functions $u,v : \mathbb{C} \to \mathbb{R}$ satisfy the Cauchy-Riemann equations: $D_1u=D_2v$ and $D_2u=-D_1v$. Now, suppose we pick $(x,\mathbb{C})$ the cartesian coordinates in $\mathbb{C}$. Then we have: $$\begin{cases}\dfrac{\partial u}{\partial x^1}&=\phantom{-}\dfrac{\partial v}{\partial x^2} \\ \\ \dfrac{\partial u}{\partial x^2} &= -\dfrac{\partial v}{\partial x^1}\end{cases}$$ This can also be written as simply: $$i\dfrac{\partial f}{\partial x^1}=\dfrac{\partial f}{\partial x^2}$$ But now here is the interesting thing I've noticed. When we work with arbitrary smooth manifolds, the partials operators relative to some coordinate system are tangent vectors to the coordinate lines. So that $\partial /\partial x ^1$ and $\partial/\partial x^2$ are tangent vectors to the coordinate lines. When we work with points of $\mathbb{C}$ and we understand then as vectors (identifying the tangent space at the origin with the space $\mathbb{C}$ itself), multiplying by $i$ is the same as rotating a vector by $\pi/2$. In the standard cartesian coordinates, $\partial/\partial x^1$ is pointing in the direction of the $x$ axis and $\partial/\partial x^2$ is pointing in the direction of the $y$ axis. In that case, $\partial /\partial x^2$ is simply $\partial /\partial x^1$ rotated $\pi/2$ in the counterclockwise direction. And using $i$ to express rotations, this is exactly what is written up there. I'm not sure if I've made myself clear, but the question is: ""is there any relationship between the Cauchy-Riemann equations and the vector fields defined in $\mathbb{C}$ as a smooth manifold that gives us deeper understanding of what analytic functions do when transforming $\mathbb{C}$ into another $\mathbb{C}$?"" Thanks very much in advance!","['multivariable-calculus', 'manifolds', 'complex-analysis']"
475933,Is there some elementary proof of invariance of domain?,"Invariance of domain at least in statement seems a simple result. I mean, the first time I saw the statement I thought: ""the proof can't be that bad"", but when I searched for it I saw that it needs even algebraic topology to prove this result. My doubt is: isn't there any other proof of invariance of domain that don't need to use algebraic topology? Is there some more elementary proof of this result? Thanks very much in advance!","['general-topology', 'alternative-proof']"
475936,"$A=\{f \mid f:\mathbb{Z}_+ \to \{0,1\}\}$ is uncountable","Consider the set $A=\{f \mid f:\mathbb{Z}_+ \to \{0,1\}\}.$
  I need to show that it is uncountable. I was trying to find a bijection between $A$ and $\mathbb{R}$
or if I can show that there is no injection from $A$ to $\mathbb{Z}_+$ then also it'll work !",['elementary-set-theory']
475951,At what angle do these curves cut one another?,"I'm working on an exercise that asks this: At what angle do the curves $$y = 3.5x^2 + 2$$ and $$y = x^2 - 5x + 9.5$$ cut one another? I have set these equations equal to one another to find two values for x. Namely, $x = 1$ and $x = -3$ as intersections. How should I proceed? Most everyone here is always extremely helpful so I can't thank you all enough in advance for any assistance.","['calculus', 'derivatives']"
475966,Proof that a subset closed under group operation of a finite group is a subgroup,"I have some idea, lets take any element $a$ from that subgroup and repeatedly apply group operator over it, since group is finite that element $a$ will appear again after some time, $\{a, a^2, a^3,...,a\}.$ Then element previous to 2nd $a$ should be identity element. And prior to it is $a$'s inverse. Hence it has all property of groups. Is this proof correct. I am not sure about that can 2nd $a$ appear only if we have identity element in it. Please help. Reference: Fraleigh p.58  Question 5.50 in A First Course in Abstract Algebra",['group-theory']
475970,"For $a_n>0$ such that $\sum a_n $ converges, show that there exist $c_n>0$ such that $c_n\to \infty$ and $\sum a_n c_n$ is finite.",A problem From PhD Pre lims Exam: Let $ a_{n} > 0 $ for all $ n\in\mathbb{N}$ such that $\sum a_{n} $ converges. Show that there exist $ c_{n} > 0 $ ($n\in\mathbb{N}$) such that $ \lim \limits_{n\to\infty} c_{n}= \infty $ and $\sum a_{n}c_{n} $ is finite.,"['sequences-and-series', 'real-analysis']"
475972,Definition of Substitution (Equality vs Equivalence relations),"I'm confused as to wether substitution is a consequence of the properties about equivalence relations (transitivity, reflexivity, and symetry) or rather it is an independent property of equality. For example, in general $(x=y) \wedge (x<z) \Longrightarrow (y<z)$. But what if I have an equivalence relation , say $""\equiv$"", instead of equality. Can I use substitution directly or do I need to prove it? EDIT: I have to admit that I asked this question without being very clear and also without thinking much about it. So, my appologies for that. I'll do some aditional details and please if you consider that my question doesn't make sense, vote to close it. Like I said, I'm confused because I don't know if ""equality"" ($=$) is a term more general than ""equivalence relation"" (~) or else they are the same. I was asking about the property of substitution because aparently when using ""$=$"" everything can be substituted whereas it is not the case when using ""~"". Let's define, for example, ""$\equiv_{n}$"" such that $\forall a,b\in \mathbb{Z}(a\equiv_{n} b \iff n|b-a)$. Now, suppose that $a\equiv_{n} b$ and $b=6^{m}$ for some $m\in \mathbb{Z}$. Then $a\equiv_{n}6^{m}$ by substitution. Also if $m=k+7$ we can again make a substitution to get $a\equiv_{n}6^{k+7}$. But, this same process cannot always be done if we consider $b\equiv_{n}6^{m}$ and $m\equiv_{n}k+7$. Since both ""$=$"" and ""~"" are transitive, reflexive and symmetric my guess was then that substitution is a property that belongs to $""=""$ but no to ""~"", but I'm not sure and I'm totally confused.","['logic', 'elementary-set-theory']"
476011,"How to show that $\{\sqrt m - \sqrt n : m,n \in \mathbb N\}$ is dense in $\mathbb R$? [duplicate]","This question already has answers here : Prove that the following set is dense (3 answers) Closed 10 years ago . Show that $S=\{\sqrt m - \sqrt n : m,n \in  \mathbb N\}$ is dense in $\mathbb R$. I know the definition of dense as: A set S is dense in $\mathbb R$ if there exists $a,b \in \mathbb R$ such that $S \cap(a,b) \neq \emptyset.$ I don't understand how to proceed. Please help.","['general-topology', 'real-analysis']"
476018,"Ways to make a series diverge ""faster"" to show divergence","I'm interested if there are more techniques to make a series diverge ""faster"" to show it diverges.  Below are the specific tricks/theorems I know of to do this. I recalled reading that the sum of inverse primes $1/2 + 1/3 + 1/5 + \ldots$ has its $n$th partial sum growing like $\log \log n$, which made me think that it would be potentially easier to show divergence by taking the exponential of the sum to make it diverge faster, and this indeed can be made to work to obtain a very short proof: $$e^{\sum_{n=1}^\infty 2/p_n} > \prod_{n=1}^\infty (1 + 2/p_n) > \prod_{n=1}^\infty \sum_{k=0}^\infty 1/p_n^k = \sum_{n=1}^\infty 1/n = \infty$$ Similarly taking the exponential of the harmonic series and using $e^x > 1 + x$ gives a telescoping infinite product whose $n$th partial product is $n+1$, which clearly diverges. I also recall a result that a series of positive decreasing terms $\sum_n a_n$ diverges if the series $\sum_k 2^k a_{2^k}$ diverges, and this transformation can also make a series diverge ""faster"", e.g. it makes the harmonic series look like $1 + 1 + 1 + \ldots$ so that the harmonic series clearly diverges, and it makes $\sum_{n=2}^\infty 1/n \log n$ look like the harmonic series hence divergent. Or a known divergent series could be used to construct a slower growing divergent series that we can then show grows the same or more slowly than a given series we wish to show divergent.  I recall one such result that says that if a series of positive terms $\sum_n a_n$ diverges, then $\sum_n a_n/s_n$ also diverges where $s_n = \sum_{i=1}^n a_i$.  Putting all $a_n =1$ shows the harmonic series diverges, and putting $a_n = 1/n$ shows that $\sum_{n=2}^\infty 1/n \log n$ diverges assuming that we know that the harmonic series partial sums grow asymptotically like $\log n$. Anyway, I was wondering if there are other theorems or tricks out there that show that a series diverges by doing something to make the series diverge ""faster"" if the series diverges (but obviously also leaves a convergent series still convergent). Or some alternative ways to take a divergent series and construct a slower growing divergent series. E.g. in a comment someone pointed out ""Series Acceleration"" methods designed to make a convergent series converge faster; if someone can show an example of how these techniques can also be used to make an example divergent series diverge faster to show divergence, that would be great.",['sequences-and-series']
476021,Is it always possible to factorize $(a+b)^p - a^p - b^p$ this way?,"I'm looking at the solution of an IMO problem and in the solution the author has written the factorization $(a+b)^7 - a^7 - b^7=7ab(a+b)(a^2+ab+b^2)^2$ to solve the problem. It seems like it's always possible to find a factorization like $(a+b)^p - a^p - b^p=p\cdot(ab)^{\alpha_1}(a+b)^{\alpha_2}(a^2+ab+b^2)^{\alpha_3}P_{\omega}(a,b)$ for any prime number p where $P_{\omega}(a,b)$ is an irreducible homogenous polynomial of degree $\omega$ and $\alpha_1 + \alpha_2 + \alpha_3 + \omega = p-1$. I wonder if it's true in general. Here are some examples: $(a+b)^2 - a^2 - b^2 = 2ab$ $(a+b)^3 - a^3 - b^3 = 3ab(a+b)$ $(a+b)^5 - a^5 - b^5 = 5ab(a+b)(a^2+ab+b^2)$ $(a+b)^7 - a^7 - b^7 = 7ab(a+b)(a^2+ab+b^2)^2$ $(a+b)^{11}- a^{11} - b^{11} = 11 ab(a + b)(a^2 + a b + b^2)(a^6 + 3 a^5 b + 7 a^4 b^2 + 9 a^3 b^3 + 7 a^2 b^4 + 3 a b^5 + b^6)$ I've proved the following statements: For any prime number p: $p|(a+b)^p - a^p-b^p$ $ab|(a+b)^p - a^p-b^p$ For any odd prime number: $(a+b) | (a+b)^p - a^p-b^p$ because $(a+b)^p - a^p-b^p$ vanishes when $a=-b$ if p is odd. For any prime number $p \geq 5$: $ (a^2 + ab + b^2) | (a+b)^p - a^p - b^p$ Proof: Any prime number greater than 3 is of the form $p=6k+1$ or $p=6k+5$. On the other hand, if we set $\Large \omega= e^\frac{2\pi i}{3}$ we have: $1 + \omega + \omega^2 = 0$, therefore $(1+\omega)^3 = 1 + 3\omega + 3\omega^2 + \omega^3 = 2 + 3(\omega+\omega^2) = -1$ $(1+\omega^2)^3 = 1 + 3\omega^2 + 3\omega^4 + \omega^3 = 2 + 3(\omega^2+\omega) = -1$ We have $a^2+ab+b^2 = (a-b\omega)(a-b\omega^2)$ in $\mathbb{Z}[\omega]$, I'll show that both $a-b\omega$ and $a-b\omega^2$ divide $(a+b)^p - a^p - b^p$ for every $p>3$: if $p=6k+1$ and we replace $a=b\omega$ then: $(a+b)^p=(b\omega+b)^{6k+1} = b^{6k+1} (1+\omega)^{6k} (1+\omega) = b^{6k+1}(1+\omega)$
$(a+b)^p - a^p - b^p = b^{6k+1}(1+\omega) - (b\omega)^{6k+1} - b^{6k+1} = b^{6k+1}(1+\omega) - b^{6k+1}(\omega+1) = 0$ If $p=6k+5$ and we replace $a=b\omega$ then: $(a+b)^p=(b\omega+b)^{6k+5} = b^{6k+5} (1+\omega)^{6k} (1+\omega)^5 = (-1)b^{6k+1}(1+\omega)^2$
$(a+b)^p - a^p - b^p = (-1)b^{6k+5}(1+\omega)^2 - (b\omega)^{6k+5} - b^{6k+5} = (-1)b^{6k+5}(1+\omega)^2 - b^{6k+5}(\omega^2+1) = p^{6k+5}(-1-2\omega-\omega^2-\omega^2-1)=0$ The same could be shown for $a=b\omega^2$. Therefore both $(a-b\omega)$ and $(a-b\omega^2)$ are factors of $(a+b)^p-a^p-b^p$ but I'm not sure if that is sufficient to conclude $a^2+ab+b^2=(a-b\omega)(a-b\omega^2) | (a+b)^p - a^p - b^p$ So, I guess I have proved that $p$, $ab$, $a+b$ and $a^2+ab+b^2$ all divide $(a+b)^p-a^p-b^p$, but I have no idea how to show that $P_{\omega}(a,b)$ must be irreducible. I have verified this conjecture up to $p=97$ and it's true I think. Any ideas on how to prove that?","['elementary-number-theory', 'algebra-precalculus']"
476035,Show that $A$ is similar to a diagonal matrix iff $b=c=d=e=f=g=0$,"Show that $A$ is similar to a diagonal matrix iff $$b=c=d=e=f=g=0$$ 
$$A= \left(\begin{array}{cccc}a & b & c & d \\ 0 & a & e & f\\ 0 & 0 & a & g\\ 0 & 0 & 0 & a \end{array}\right)$$ Attempt : If $$b=c=d=e=f=g=0$$, then clearly $A$ is similar to a diagonal matrix. $$A = I^{-1}AI$$ Conversely, if $A$ is similar to a diagonal matrix, then $$A=P^{-1}DP$$ where $P$ is non-singular. Suppose $$b,c,d,e,f,g \neq 0$$ We know that the only eigenvalue of $A$ is $a$. To be diagonalizable, we know that $A$ must have 4 eigenvectors. With the assumption of $$b,c,d,e,f,g \neq 0$$ we get that the only eigenvector w.r.t. the eigenvalue $$\lambda=a$$ is $$(1,0,0,0)$$ This contradicts the fact that $A$ is diagonalizable. Hence $$b=c=d=e=f=g=0$$ Is the above logic wrong? Is there any other way I can prove this?","['matrices', 'linear-algebra']"
476037,Taking the limit of $\frac{1}{x^2} - \frac{1}{x\sin(x)}$ as $x$ approaches $0$,"Plugging zero into $x$ gives me infinity-infinity which is indeterminate.  I then try to multiply the function by $$\frac{\frac{1}{x^2} + \frac{1}{x\sin(x)}}{\frac1{x^2} + \frac1{x\sin(x)}}$$ which gives me another undeterminate and harder function...  I know I need to use L'Hospital's rule, but I can't seem to find the right algebraic form to use that rule.","['calculus', 'limits']"
476062,Improving basic maths & algebraic manipulation skills,I am currently doing a-levels in the UK (received an A in AS) so doing calculus (differential equations/trig integrals)/linear algebra/mechanics/statistics etc. However I feel that a lot of my basic mathematics skills are almost non-existent (don't know how I made it this far). Can anyone recommend any good books that provide examples and exercises to help me improve this vital skill (or set of skills)? Thanks,"['self-learning', 'algebra-precalculus', 'soft-question']"
