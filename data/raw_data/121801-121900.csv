question_id,title,body,tags
1829838,Schedule 8 teams for 6 events. Each team plays each event twice.,"I have a seemingly simple question. I'm holding a Beer Olympics at my house tomorrow. There are 8 teams competing in 6 different events (Beer Pong, Beerball, Can Jam, Corn Hole, Beersbee, and Flip Cup). Each event is seeing two teams compete. Is there a way to arrange the schedule so that each team plays each event twice? I understand teams will play some teams twice in different events, but I don't want them to play the same event against the same opponent they played the first time (example - team 1 plays team 2 in cornhole, later on team 1 plays a different team in cornhole). Round 1: All 6 events can happen at the same time (Obviously only 4 of the games will be played at once due to 8 teams) 
Round 2: All 6 events happen at the same time. 
And so on until Round 12.",['combinatorics']
1829844,Is the limit $\lim_{x\rightarrow0}\frac{\sin{[x]}}{[x]}$ a one sided limit or not?,"Is the limit $\displaystyle\lim_{x\rightarrow0}\frac{\sin{[x]}}{[x]}$ a one sided limit or not? Here $[\, \cdot\, ]$ is the greatest integer function. According to me the right hand limit will be not defined and the left hand limit will be $\dfrac{-\sin1}{-1}$.","['ceiling-and-floor-functions', 'trigonometry', 'limits']"
1829893,What are the elements of the set $\overline{X} = \{10-x \ \ |\ \ x \in \varnothing\}$?,"I am having some doubts about enumerating elements of this set: $$\overline{X} = \{10-x \ \ |\ \ x \in \varnothing\}$$ Can you enumerate what elements are in there? I know this is a really elementary question, I'm sorry. According to me it should be $\overline{X} = \{10\}$ but I'm not really sure. Care to explain? Thanks!",['elementary-set-theory']
1829894,"What is the opposite of ""sparsity"" in a matrix?","If a sparse matrix has only 1% non-zero entries, I find it weird to speak of ""1% sparsity"". In particular, ""increasing sparsity"" goes along with a smaller percentage of non-zero entries, so this is determined to cause confusion. However, I have never read about a matrix with ""99% sparsity"", expressing that the matrix has 1% non-zero entries. The number of non-zero entries seems to be what's used to characterize a sparse matrix. So which word to use? The dictionary proposes ""density"", but that may sound odd. A sparse matrix with 1% density? Sure, if there's nothing better. Or is there? Non-subjective variant of my question: What are terms used by other authors for the fraction of non-zero entries in a sparse matrix?","['numerical-linear-algebra', 'matrices', 'terminology', 'sparse-matrices', 'linear-algebra']"
1829921,Do different methods of calculating fractional derivatives have to be equal?,"Do different methods of calculating fractional derivatives have to be equal?  Or do they sometimes end up differently? An example would be nice, and if possible, an explanation as too why such formulas can disagree with one another would be exceptional. The main reason behind this is because I noted that if we could take the fractional derivative of a function through its Taylor series, this would imply that $\frac{d^q}{dx^q}e^x-\frac{d^{q+1}}{dx^{q+1}}e^x={x^{-q-1}\over\Gamma(-q)}$, which tends to $0$ as $q$ tends to become a whole number, but still, this goes against what I would expect. And of course, explanation on the role of constants of integration would be nice.","['derivatives', 'fractional-calculus']"
1829940,Upper bound on the minimum distance between $N$ points chosen inside the unit circle?,"I guess this is a well-known problem but I'm not sure where to find it on the web. $N \ge 2$ points are chosen in the interior or the boundary of the unit circle. What is the best upper bound on the minimum distance between two of these points? Given a configuration of $N$ such points, let's call the minimum distance $d_{\min}(N)$.We seek $\max\{d_{\min}(N)\}$. Some examples for small $N$: $\max\{d_{\min}(2)\}=2$ (diameter) $\max\{d_{\min}(3)\} \ge \sqrt{3}$ (equilateral triangle) $\max\{d_{\min}(4)\} \ge \sqrt{2}$ (square) $\max\{d_{\min}(5)\} \ge 2\sin(\pi/5)$ (regular pentagon) $\max\{d_{\min}(6)\} \ge 1$ (regular hexagon) $\max\{d_{\min}(7)\} \ge 1$ (regular hexagon plus the center) This shows the answer is not $2\sin(\pi/N)$, which you would get from distributing the points equally along the circumference (the pattern breaks for $N=7$).","['discrete-geometry', 'combinatorics', 'packing-problem', 'geometry']"
1829952,When is a norm induced by an inner product? [duplicate],"This question already has answers here : Norms Induced by Inner Products and the Parallelogram Law (5 answers) Closed 4 years ago . Do inner products come from a norm, 
or norms come from an inner product? How do I prove that this is true? Just want to know what comes from what, the chicken or the egg came first kind of thing.","['normed-spaces', 'functional-analysis', 'inner-products', 'geometry', 'linear-algebra']"
1829959,How many integer solutions are there of the equation $|x_{1}|+|x_{2}|+\cdots +|x_{k}|=n$?,"How many solutions are there to the equation $$|x_{1}|+|x_{2}|+\cdots +|x_{k}|=n$$ for $n,k\in \mathbb N$ and $\forall\ 1\leq i\leq k,\ x_{i}\in \mathbb Z$? Any ideas? I don't know how to approach this question. (all I know that if my  $ x_{i}$'s were in $\mathbb N$ the solution would be $\binom{n+k-1}{k-1}$  )","['diophantine-equations', 'combinatorics', 'discrete-mathematics']"
1829970,Prove that $\frac{2^a+1}{2^b-1}$ is not an integer,Let $a$ and $b$ be positive integers with $a>b>2$. Prove that $\frac{2^a+1}{2^b-1}$ is not an integer. This is equivalent to showing there always exists some power of a prime $p$ such that $2^a+1 \not \equiv 0 \pmod{p^a}$ but $2^b-1 \equiv 0 \pmod{p^a}$. How do we prove the statement from this or is there an easier way?,['number-theory']
1829992,Weak convergence of finite measure preserving transformations,"I am reading King's paper ""The commutant is the weak closure of the powers, for rank-1 transformation"" and I am not able to show that: (0.1) ""If the $T_i$ are invertible measure preserving transformations on Lebesgue probability space, commuting with each other and converging weakly to $S$ then $S$ is invertible and $T_i^{-1}\rightarrow S^{-1}$ weakly."" Does anybody have any ideas how the proof my go? Btw, the result is not true if $T_i$ are not assumed to be commutative which I also don't  see why. http://journals.cambridge.org/action/displayFulltext?type=1&fid=2235584&jid=ETS&volumeId=6&issueId=03&aid=2140108","['ergodic-theory', 'dynamical-systems', 'lebesgue-measure', 'measure-theory']"
1830004,Topology of a circle,"I'm currently trying to learn topology by myself, before I started I knew topology had something to do with shaped stretched but not torn or glued. Now that I have started to learn I see talk about open and closed sets, neighborhoods, limits and such, but I don't understand how this relates to my previous understanding of topology. I would be interested to know what is the topology, or the topological space of a circle (or the equivalent of a circle in topology, as a circle is the same as a square and many other things). Thanks.",['general-topology']
1830010,Coin flipping probability taking turns between players,"I've worked out the result to a basic coin flipping problem and want to generalize it. The basic problem is: there are N players, and they take turns of flipping a coin (in the same cyclical order) until the first person to get 1 heads wins. Coin is not necessarily fair so give it probability p of Heads. The answer I worked out is as follows. Say P(m) represents the probability that player m (of the N players) wins. Then: $$
P(m) = \frac{(1-p)^{m-1}p}{1-(1-p)^{N}}
$$
 This is because for player m to win, the m-1 preceding players must get tails with probability (1-p). It also uses the convergence formula for the sum of a geometric series. ---------------------------------------------------------------------------- Now my question is how to generalize this from: ""winning""= first person to get 1 Heads, to ""winning""= first person to get K heads? In other words, players take turns (in the same order as before) of flipping a coin a single time. Once all the players have flipped, then as before, the cycle repeats from player 1. But this time, the winner is the first person to get K heads (not necessarily in a row) before anyone else gets K heads. I'm having difficulty because the tree of possibilities expands very quickly. For instance, I'd be happy to at least see the derivation for the case of K=2.",['probability']
1830042,Is the empty set a topological space? [duplicate],"This question already has answers here : How can I define a topology on the empty set? (2 answers) Closed 8 years ago . If so, is the empty function  from it to any other space  considered a continuous function? I can't really convince myself either way.","['continuity', 'general-topology']"
1830048,Invariance of subharmonicity under a holomorphic map,"If $f:U_1\rightarrow U_2$ is holomorphic and $u$ is subharmonic on $U_2$, then prove that $u\circ f $ is subharmonic on $U_1$. I know how to prove the same argument with $f$ being conformal. In that proof I had to use $f^{-1}$. But here $f$ is just holomorphic and not necessarily invertible. So how could I prove the above statement? Any help is appreciated.",['complex-analysis']
1830071,Real Analysis Folland Problem 1.5.28 Borel Measures,"Problem 1.5.28 - Let $F$ be increasing and right continuous, and let $\mu_F$ be the associated measure. Then $\mu_F(\{a\}) = F(a) - F(a-)$, $\mu_F([a,b)) = F(b-) - F(a-)$,$\mu_F([a,b]) = F(b) - F(a-)$, and $\mu_F((a,b)) = F(b-)- F(a)$ Proof - $\mu_F$ has been constructed on the algebra of h-intervals that takes the values $$\mu_F((a,b]) = F(b) - F(a) \forall a,b$$ So $\mu_F$ is a finite bounded set on $\mathbb{R}$ We can represent $\{a\}$ as $$\{a\} = \bigcap_{n=1}^{\infty}(a - 1/n, a]$$ The interval $(a - 1/n, a]$ forms a decreasing sequence of sets so by theorem 1.8d we have $$\mu_F(\{a\}) = \lim_{n\rightarrow \infty}\mu_F((a - 1/n, a]) = \lim_{n\rightarrow \infty}\left[ F(a) - F(a - 1/n)\right]$$ Since $F$ is increasing $$\lim_{n\rightarrow \infty}\mu_F(a - 1/n) = \lim_{x\rightarrow a-}F(x) = \sup\left\lbrace F(x):x < a\right\rbrace  = F(a-)$$ Thus $$\mu_F(\{a\}) = F(a) - F(a-)$$ Now, consider the $\mu_F([a,b))$, where $b$ might be $\infty$. We can represent $[a,b) = \{a\} \cup (a,b)$ (disjoint union) so $$mu_F([a,b)) = \mu_F(\{a\}) + \mu_F(a,b) = F(a) - F(a-) + F(b-) + F(a) = F(b-) - F(a-)$$ Note this is valid if $F(b-) = F(\infty) = \infty$ Next, consider $\mu_F([a,b])$ We can represent $[a,b] = \{a\} \cup (a,b]$ (disjoint union) so $$mu_F([a,b]) = \mu_F(\{a\}) + \mu_F([a,b]) = F(a) - F(a-) + F(b) - F(a) = F(b) - F(a-)$$ Finally, consider $\mu_F((a,b))$ so either $a$ or $b$ will be $\infty$ in the interval. If $b$ is $\infty$ then $(a,\infty)$ is an h-interval and the definition of $\mu_F$ yields $$\mu_F((a,\infty)) = F(\infty) - F(a)$$ so the proposed formula $$\mu_F((a,b)) = F(b-)- F(a)$$ is correct because $F(\infty) = \lim_{x\rightarrow \infty}F(x)$ The part I don't understand is why $$\lim_{n\rightarrow \infty}F(a-1/n) = F(a-)$$ Try to explain this as simply as possible.","['real-analysis', 'measure-theory']"
1830076,Find the closed form for $\int_{0}^{\infty}\cos{x}\ln\left({1+e^{-x}\over 1-e^{-x}}\right)dx=\sum_{n=0}^{\infty}{1\over n^2+(n+1)^2}$,"$$I=\int_{0}^{\infty}\cos{x}\ln\left({1+e^{-x}\over 1-e^{-x}}\right)dx=\sum_{n=0}^{\infty}{1\over n^2+(n+1)^2}\tag1$$ $$\ln\left({1+e^{-x}\over 1-e^{-x}}\right)=2\sum_{n=0}^{\infty}{e^{-(2n+1)x}\over 2n+1}\tag2$$ Sub $(2)$ into $(1)\rightarrow (3)$ $$I=2\sum_{n=0}^{\infty}{1\over 2n+1}\int_{0}^{\infty}e^{-(2n+1)x}\cos{x}dx\tag3$$ Apply integration by parts to $(4)$ Hence $$I=\int_{0}^{\infty}e^{-(2n+1)x}\cos{x}dx={2n+1\over (2n+1)^2+1}\tag4$$ Apply $(4)$ into $(3)$ Hence $$I=\sum_{n=0}^{\infty}{2\over (2n+1)^2+1}\tag5$$ Simplify $${2\over (2n+1)^2+1}={2\over 4n^2+4n+2}={1\over n^2+(n+1)^2}$$ Therefore $$I=\sum_{n=0}^{\infty}{1\over n^2+(n+1)^2}\tag6$$ I am not able to determine the closed form for $(1)$, can anyone please help? Edit(hint from Marco) $$I=\int_{0}^{\infty}\cos{x}\ln\left({1+e^{-x}\over 1-e^{-x}}\right)dx={\pi\over 2}\tanh\left({\pi\over 2}\right)\tag1$$ Can anybody prove $(1)$ using another method?","['integration', 'definite-integrals', 'sequences-and-series', 'calculus']"
1830092,How to arrive at Ramanujan's nested radicals?,Ramanujan found that $\sqrt[3]{\cos\left(\frac {2\pi}{7}\right)}+\sqrt[3]{\cos\left(\frac {4\pi}{7}\right)}+\sqrt[3]{\cos\left(\frac {8\pi}{7}\right)}=\sqrt[3]{\frac {1}{2}\left(5-3\sqrt[3]{7}\right)}$ on the last page of Ramanujan's second notebook. He also found that $\sqrt[3]{\sec\left(\frac {2\pi}{9}\right)}+\sqrt[3]{\sec\left(\frac {4\pi}{9}\right)}-\sqrt[3]{\sec\left(\frac {\pi}{9}\right)}=\sqrt[3]{6\sqrt[3]{9}-6}$ My question: How do we find the denesting with only the knowledge on the nested radical? Example: How would you find the denestings for $\sqrt[3]{\frac {1}{2}\left(5-3\sqrt{7}\right)}$ without knowledge on $\sqrt[3]{\cos\left(\frac {2\pi}{7}\right)}+\sqrt[3]{\cos\left(\frac {4\pi}{7}\right)}+\sqrt[3]{\cos\left(\frac {8\pi}{7}\right)}$?,"['algebra-precalculus', 'radicals', 'nested-radicals']"
1830093,"Real Analysis, Folland Problem 1.5.29 Lebesgue measurable set","1.5.29 - Let $E$ be a Lebesgue measurable set. a.) If $E\subset N$ where $N$ is the nonmeasurable set described in section 1.1, then $m(E) = 0$ . b.) If $m(E) > 0$ , then $E$ contains a nonmeasurable set. (It suffices to assume $E\subset [0,1]$ . In the notation of section 1.1, $E = \bigcup_{r\in R}E\cap N_r$ ). Proof a.) Let $E\subset N$ and $N\subset [0,1)$ . In the notation of section 1.1, let $$E_r = \{x + r: x\in E\cap [0,1-r)\}\cup \{x+r-1:x\in E\cap [1-r,1)\}$$ Then for all $r\in R = \mathbb{Q}\cap [0,1)$ , since $E_r \subset N_r$ and the collection $\{N_r\}_{r\in R}$ is a disjoint collection, it follows that $\{E_r\}_{r\in R}$ is a disjoint collection. By the translation invariance and finite additivty of Lebesgue measure, we have that each $E_r$ is measurable and $m(E_r) = m(E)$ for all $r\in\mathbb{R}$ . So $$1\geq m\left(\bigcup_{r\in R}E_r\right) = \sum_{r\in R}m(E_r) = \sum_{r\in R}m(E)$$ and therefore $m(E) = 0$ ? Not sure if this is right. Proof b.) Suppose that $E\subset [0,1]$ is a subset with the property that $E$ is measurable. Then for each $r\in R$ , the set $E\cap N_r$ is measurable. By the translation invariance and finite additivity of Lebesgue measure, the set $E_{1-r}\cap N$ is therefore measurable, and hence must have measure $0$ by part a.). Since the collection $\{N_r\}_{r\in R}$ is disjoint we have that $$m(E) = m\left(\bigcup_{r\in R}(E\cap N_r)\right) = \sum_{r\in R}m(E\cap N_r) = \sum_{r\in R}m(E_{1-r}\cap N) = 0$$ Not sure if this is right either. Any suggestions on these is greatly appreciated.","['real-analysis', 'measure-theory']"
1830122,"Is there a commutative ring with a ""generalized determinant""?","Does there exist a commutative ring(-with-a-1) $R$ and positive integer $n$ and function $\hspace{.04 in}f$ from [the set of $n$-by-$n$ matrices over $R$] to $R$ such that $f$ is linear in each row and each column separately and $f$ of the $n$-by-$n$ identity matrix is $1_R$ and for all $n$-by-$n$ matrices $M\hspace{-0.03 in}$, if $M$ is invertible then $\hspace{.04 in}f(M)$ is a unit and $f$ is not the restriction of determinant to $\hspace{.04 in}f\hspace{.02 in}$'s domain ? I'm inspired by this answer .","['multilinear-algebra', 'linear-algebra', 'commutative-algebra']"
1830124,Moment Generating Function for $r$th central moment,"When using moment generating functions, to find the $n$th raw moment (""$n$th moment about the origin""), you take the $n$th derivative of the MGF and evaluate at $t=0$. To find the $m$th central moment (""$m$th moment about the mean""), e.g. $m=2$ for the variance, you need to evaluate the MGF twice (once for $m=1$, once for $m=2$) and use the relationship: $\text{var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$ ------------------------------------------------------------- Is there an alternative kind of generating function such that to find the $r$th central moment (""$r$th moment about the mean"") you only need to evaluate that generating function a single time?","['probability-distributions', 'generating-functions', 'cumulants', 'probability', 'moment-generating-functions']"
1830138,Finding the combination where p of the items are identical.,"Suppose we have $n$ objects in which $p$ items are identical. Of course, $n-p$ elements are distinct. Then what is the combination of $n$ objects taken $r$ at a time? That is, what is $C(n,r)$, but keep in mind all $n$ object are not distinct here? For example, suppose a population consist of five values $1,3,3,4,5$. A random sample of $3$ values is drawn without replacement from this population. Then how many such samples of cardinality $3$ are possible? There are $7$, right? But what is the general formula?","['combinatorics', 'discrete-mathematics']"
1830157,"If function is measurable on an interval, is it measurable on its subinterval?","This is exercise 2.3 from ""A User-Friendly Introduction to Lebesgue Measure and Integration"" by Gail S. Nelson: Let $[c,d]\subseteq[a,b]$. Show that if $f$ is measurable on $[a,b]$, then $f$ is measurable on $[c,d]$. By definition, function $f$ is Lebesgue measurable on an interval $I$ if for every $s\in\mathbb{R}$ the set $\{x\in I\mid f(x)>s\}$ is a Lebesgue measurable set. Here, obviously $\{x\in[c,d]\mid f(x)>s\}\subseteq\{x\in[a,b]\mid f(x)>s\}$, but at the moment I don't see how to proceed.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1830194,Maximal unit lengths in 3D with $n$ points.,"Given $n$ points in 3D space (V), what is the maximal number of unit distance lengths (E) between those points?  Here are a few possibilities.  Some of them are chromatic spindles . 
A collection of these best known configurations has been placed at Maximal Unit Lengths in 3D . V --  E -- figure  
 4 --  6 -- tetrahedron  
 5 --  9 -- triangular bipyramid  
 6 -- 12 -- octahedron   
 7 -- 15 -- 8 configurations  
 8 -- 18 -- 26 configurations  
 9 -- 22 -- 5 configurations  
10 -- 27 -- 1 configuration   
11 -- 31 -- 1 configuration      
12 -- 35 -- 1 configuration     
13 -- 39 -- 1 configuration      
14 -- 44 -- 1 configuration   
15 -- 48 -- 4 configurations   
16 -- 52 -- 8+ configurations  
17 -- 57 -- 1+ configuration      
18 -- 62 -- 1+ configuration    
19 -- 66 -- 2+ configurations   
20 -- 71 -- 1+ configuration For 21-39 vertices, {75, 79, 84, 90, 93, 97, 101, 106, 110, 116, 120, 127, 131, 136, 140, 145, 150, 155, 160} unit edges are possible. The maximal log(unit edges)/log(points) is 1.43392 with 14 points producing 44 unit edges.  The points are {{0,0,0}, {3,0,3}, {3,3,0}, {3,-3,0}, {3,0,-3},{0,3,-3}, {6,0,0}, {2,-1,1}, {-1,2,1}, {-1,-1,-2}, {-1,-1,4}, {-1,-4,1},{-4,-1,1}, {2,-4,4}}/(3 sqrt(2)).  They look like the following: Which of the above objects can be improved upon, with either more unit edges or a more interesting object with the same number of edges?","['graph-theory', 'polyhedra', 'combinatorics', 'recreational-mathematics', 'coloring']"
1830199,Is any real-valued function in physics somehow continuous?,"Consider the following well-known function:
$$
\operatorname{sinc}(x) = \begin{cases} \sin(x)/x & \text{for } x \ne 0 \\
1 & \text{for } x =0 \end{cases}
$$
In physics, the sinc function has applications with for example spectrography.
Mathematically speaking, there would be no objection against an alternative like this:
$$
\operatorname{suck}(x) = \begin{cases} \sin(x)/x & \text{for } x \ne 0 \\
0 & \text{for } x =0 \end{cases}
$$
But in physics such an alternative proposal would be void of applications. It is silently assumed that $\operatorname{sinc}(x)$ is continuous at $x=0$. Physicists do not even think about a discontinuous alternative. The sinc function is only an example of a far more general claim, uttered by one of my heroes, the great mathematician L.E.J. Brouwer . It is
(not very well) known as Brouwer's Continuity Theorem, grossly stating that every real-valued function is continuous. More precisely, as quoted from Strong Counterexamples :
""In intuitionistic mathematics, the Brouwer Continuity Theorem states that all total real functions are (uniformly) continuous on the unit interval"". Real valued, physical quantities have uncertainties. That is one of the fundamental properties of physics. And it isn't just due to quantum considerations. Take an average metal bar. It has no exact length. There
are for example temperature fluctuations (atoms in motion) which will cause the bar's length to fluctuate. This effectively means that any real number in physics is accompanied with an uncertainty, an error , often denoted as $\delta$ or $\varepsilon$. Consider the classical mathematical definition of continuity of a function. All numbers are assumed to be real-valued. A function $f(x)$ is said to be continuous at $x=a$ if and only if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that if $|x-a| < \delta$ then $|f(x)-f(a)| < \varepsilon$, where it may be that $x \ne a$. A physical interpretation of this might be formulated as follows: an error in a continuous function can be made as small as desired by adapting the error in the function's argument accordingly.
Due to the errors, $|x-a|<\delta$ is physically the same as $x\approx a$ ($x$ equals $a$ approximately) and this can be said for $f(x)$ and $f(a)$ as well. So we can even write $\; x\approx a \,\Longrightarrow\, f(x)\approx f(a)\;$ , as a (sloppy) definition of continuity. The latter formulation is even closer to Brouwer's Continuity Theorem, if we replace the $\,\approx\,$ by a common equal sign: $\; x=a \,\Longrightarrow\, f(x)=f(a)\;$ , expressing the idea that a function is continuous where it really is .. a function! Now consider again the above suck function. Whatever small it might be, inevitably there is an error in the argument, meaning that $x=0$ should actually be replaced by an interval $|x-0| < \delta$. There are values $x\ne 0$ in that interval, though, and $\,\lim_{x\to 0} \operatorname{suck}(x) = 1$. Hence, physically speaking, $\operatorname{suck}(0) = 1\,$ and $\operatorname{suck}(0) = 0\,$ must be true at the same time. Which is impossible. IMHO this is the reason why $\operatorname{suck}(0) = 1\,$ is involved automatically in physics, resulting inevitably in our old friend the sinc function and nothing else. I'm well aware of the fact that this way of physical reasoning does not involve all sorts of continuity that mathematicians might think of. So the question is what sorts of continuity are sensitive to the automatism that is present in the sinc function and what sorts of continuity are distinct from this. It's a somewhat vague question, but I am a humble physicist by education and I do not know of any better way to formulate it. EDIT. A far more simple example of a function with the same sort of ""automatism"" as with the $\operatorname{sinc}$ function is given by:
$$
f(x) = \begin{cases} (x^2-1)/(x-1) & \text{for } x \ne 1 \\ 2 & \text{for } x=1 \end{cases}
$$
Which is physically the same as $\,f(x) = x+1$ . A counter example is the function $\,g(x) = 1/x$ , much like the one given by snulty . So it seems that some singularities are ""essential"" (physically speaking) while others are not. Can someone be more specific? Because
I find it a can of worms, as is exemplified by related Q&A in MSE and elsewhere: Cauchy distribution instead of Coulomb law? Could this be called Renormalization? Does this limit exist and if so what is it's value? Can monsters of real analysis be tamed in this way? Computability, Continuity and Constructivism Delta function that obeys inverse square law outside its (-1; 1) range and has no 1/0 infinity Critical Mass Flow","['real-numbers', 'physics', 'soft-question', 'functions']"
1830201,Missing crucial step in the derivation of the Stirling's Formula via the Poisson Distribution and CLT,"I believe that this is a particular neat example that we've done in class. Unfortunately there is one step I do not quite understand and my Professor had to skip due to the lack of time. I think this approach might be of interest, so I will post it despite its incompleteness. Feel free to skip directly to the question tagged with (?) Here is the idea: Let $X \sim $Poisson(1) i.e. we have $P(X=k)= \frac{e^{-1}}{k!}$ for $k=0,1,2, \dots,$ and furthermore $E(X)=1=$Var$(X)$ Now consider a sequence of i.i.d. Poisson(1) RVs $X_1,X_2, \dots ,$ we then know that $(X_1 + \dots + X_n) \sim $Poisson($n$) Lets now define yet another RV $Y_n$ as $$Y_n:= \frac{X_1 + \dots + X_n -n}{\sqrt{n}} $$
Then by the CLT we have that $Y_n \implies \mathcal{N}(0,1)$ where the arrow denotes weak convergence or convergence in distribution. This is equivalent to saying that $$P(Y_n >x) \to P(\mathcal{N} >x) \ , \forall x \in \mathbb{N} \text{ as } n \to \infty $$
But on the LHS of the above we obtain by Chebyshev's Inequality that  $$ P(Y_n >x) = P(S_n-n > x \sqrt{n}) \leq \frac{1}{(x \sqrt{n})^2}\text{Var}(S_n)= \frac{1}{x^2} $$
Furthermore we have $\int_1^\infty 1/x^2 dx < \infty$, which justifies by Lebesgue dominate convergence that $$ \int_0^\infty P(Y_n >x) dx \to \int_0^\infty P ( \mathcal{N}>x)dx=  \frac{1}{\sqrt{2 \pi}}\int_0^\infty y \exp \left( \frac{-y^2}{2}\right)dy = \frac{1}{\sqrt{2 \pi}} $$
My questions concerns the LHS of the above statement. Namely my Professor said that $$ \int_0^\infty P(Y_n >x)dx = E(Y_n^+)\overset{?}= e^{-n} \sum_{j=n+1}^\infty \frac{n^j}{j!} \left( \frac{j-n}{\sqrt{n}} \right) \overset{\checkmark}= \frac{e^{-n}}{\sqrt{n}} \frac{n^{n+1}}{n!} \tag{?} $$
Maybe I am missing something obvious here, I do understand that we want to compute expected value of the positive part of the random variable $Y_n$ denoted by $Y_n^+$, so I believe what my Professor actually is calculating is $$E(Y_n^+)=E(Y_n 1_{S_n \geq n}) $$
But I haven't dealt with such expected values in Class. Can maybe someone elaborate on the 'missing' steps?","['weak-convergence', 'probability-theory', 'central-limit-theorem']"
1830223,Equal partial derivatives implies symmetry,"The statement reads as follows: Let $f$ be continuously differentiable if $\frac{\partial f}{\partial x}(x,y)=\frac{\partial f}{\partial y}(x,y)$ then $f(a,-a)=f(-a,a)$ I would like to know if this statement is true or false. Intuitively it seems possible since having the same slope from either direction implies a sort of symmetry. Thus I think this statement is true. Is there a way to prove/falsify it?","['multivariable-calculus', 'real-analysis']"
1830280,There are at least two solutions such that $2p_n=p_a+p_b$ ($p$ being prime),"I've stumbled across this playing around and summing primes at random during a boring lecture. Is this a known conjecture? Can it be proven? My conjecture: There exists at least one non trivial solution such that $2p_n = p_a + p_b$ (the trivial being obviously $a=b=n$) for $n > 2$. Tested by starting at the trivial $2p_n = p_n + p_n$ then incremented the right as I decremented the left until both were prime again or I've ran below $2$ with the left number. The second condition never occurred though, and I've tested for the first $1000$ primes by writing a simple program. It fascinated me for the fact that this would mean that $p_b = 2p_n - p_a$ where $b > n > a$ so by knowing primes up to the $n$th you would have enough information to evaluate the $(n + 1)$-th?","['number-theory', 'conjectures', 'prime-numbers']"
1830282,Integral over Julia Set (Is my math correct?),"So I was answering this question about whether or not the Julia Set was self-similar in a known way. Of course it is, and that got me thinking. Even though the self similarity is nonlinear, what if you could exploit it to integrate over a normalized haussdorf...so I started small, I just wanted to see if I could get, $$(1) \quad \int_J z \ \mu(dz)$$ The Julia Set, $J$, is the attractor of the dynamical system (IFS), $$(2) \quad F \rightarrow w_1(F) \cup w_2(F)$$
$$w_1(z)=\sqrt{z+\lambda} \ , \quad w_1(z)=-\sqrt{z+\lambda}$$ So, the integral can be rewritten as, $$(3) \quad \int_J z \ \mu(dz)=\int_{w_1(J)} z \ \mu(dz)+\int_{w_2(J)} z \ \mu(dz)$$ The first an second integrals on the right can be written after the substitutions $z \rightarrow w_1(z)$ and $z \rightarrow w_2(t)$ as, $$(4) \quad \int_J z \ \mu(dz)=\int_{J} \sqrt{z+\lambda} \ \mu \left(\cfrac{dz}{\sqrt{z+\lambda}} \right)+\int_{J} -\sqrt{z+\lambda} \ \mu \left(-\cfrac{dz}{\sqrt{z+\lambda}} \right)=0$$ Which is rather boring. So, next I tried, $$(5) \quad \int_J z^2 \ \mu(dz)$$ After applying the self-similarity, and applying a change of variables,  I get, $$(6) \quad \int_J z^2 \ \mu(dz)=\int_J z+\lambda \ \mu \left(\cfrac{dz}{\sqrt{z+\lambda}} \right)+\int_J z+\lambda \ \mu \left(-\cfrac{dz}{\sqrt{z+\lambda}} \right)$$ I remember the value of $(1)$ so combining the integral into one with that in it is a good choice. I also remember that the negative sign inside the measure is irrelevant, so I get, $$(7) \quad \int_J z^2 \ \mu(dz)=2 \int_J z+\mu \left(\cfrac{dz}{\sqrt{z+\lambda}} \right)+2 \lambda \int_J \mu \left(\cfrac{dz}{\sqrt{z+\lambda}} \right)=2 \lambda \int_J \mu \left(\cfrac{dz}{\sqrt{z+\lambda}} \right)$$ Then changing variables back using $z \rightarrow w^{-1}_1(z)$, I get, $$(8) \quad \int_J z^2 \ \mu(dz)=2 \lambda \int_{w_1(J)} \mu \left( dz \right)=\lambda$$ So my result for $(5)$ is, $$(9) \quad \int_J z^2 \ \mu(dz)=\lambda$$ Is this correct? If not, what's the correct way? Anyone see any tricks I could've used to speed this up?","['integration', 'fractal-analysis', 'measure-theory', 'fractals']"
1830287,Corollaries of the Yoneda Lemma in Analysis?,"I am looking for some simple examples of how the Yoneda Lemma can be applied in analysis and probability theory and related fields. A simple candidate example that I can think of and somewhat understand is any linear transformation on a finite-dimensional vector space. These can be represented by a matrix, which determines the action of the transformation on a certain basis of the vector space. But seemingly we can characterize the ""inherent/intrinsic"" linear transformation in a coordinate-free way by specifying its action on every possible set of bases. (This is basically the idea that tensors are actually coordinate-free objects.) There are also many other coordinate-free representations of objects in analysis, and all seem to be characterized by the interactions of the objects with all possible coordinate systems. Is this related to the Yoneda Lemma at all? It sounds very similar to the particle physics/probing analogy given frequently. If it is, I think this improves my understanding of such objects tremendously. But I might be leading myself along a false path. Motivation/Context: For example, in Qiachu Yuan's answer to a similar question , the Yoneda Lemma shows us that every element in a poset can be determined by either the set of all elements it is greater than or equal to, or the set of all elements which is less than or equal to, essentially each element is equivalent to a Dedekind cut. If we consider the partial order of equivalence classes of Cauchy convergent sequences of rational numbers, then this reasoning seems to justify the construction of the real numbers from Dedekind cuts (although I am not entirely sure). Why do I think that the Yoneda Lemma might be related to coordinate-free representations of vectors? Because the automorphism group on a vector space (which consists of changes of basis and which is a monoidal category with one object), can be lifted to the category whose objects are the various representations of a single vector in different coordinate systems, using the group action. Whether or not this lifting constitutes a functor, I don't know. In any case, this lifting from a category with one object to the category of all possible coordinate representations of a vector suggests intuitively that we can consider all of those coordinate representations to be a single object. My question is whether or not the Yoneda lemma formalizes that intuition. In another answer on MathOverflow , which was mentioned towards the end of Tom LaGatta's talk about category theory at the NYC Lisp meetup , an analogy to particle physics was made. Basically the intuition behind Yoneda's Lemma is supposed to be that one can characterize an object (up to equivalence I guess) by probing it via its interactions (i.e. morphisms) with all other objects. In the above example, we would be smashing a vector against all possible changes of basis in order to understand it completely. Despite being an analyst/probabilist, Tom LaGatta did not go further into examples besides this particle physics metaphor. (He did say something along the lines of ""You will find it meaningful in your own context, I guarantee it"" around 1:34:00.) However, I am curious, because this analogy suggests to me coordinate-free representations of objects.","['tensor-products', 'probability-theory', 'soft-question', 'category-theory', 'analysis']"
1830327,Proving that a positive-integer valued random variable has the lack of memory property iff it has a geometric distribution.,"Suppose that $X$ is a positive-integer valued random variable with the lack of memory property which states: Given that $X>n$, then $\mathbb{P}(X=n+k) = \mathbb{P}(X=k)$. Consider the case where $p=\mathbb{P}(X=1)$ and let $q_n = \mathbb{P}(X>n)$ for each $n \in \Bbb N$ and $k=1$. $$\mathbb{P}(X=n+1\mid X>n)=\dfrac {\mathbb{P} (X>n, X=n+1)}{\mathbb{P}(X>n)}$$ which by the conditional probability rule $$= \frac {\mathbb{P} (X=n+1)}{\mathbb{P}(X>n)} = \frac {\mathbb{P} (X=n+1)}{q_n}$$ Here's where I get tripped up: My textbook shows that $$\frac {\mathbb{P} (X=n+1)}{q_n} = \frac {q_n-q_{n+1}}{q_n}.$$ How is that $q_n-q_{n+1}$ can be substituted for $\mathbb{P}(X=n+1)$? I guess this is really a question of algebra, but I'm confused because my textbook skips over so many steps when it is showing how to derive certain equations.","['probability', 'geometric-probability', 'probability-distributions']"
1830329,Different sizes of infinity,"Correct me if I'm wrong, but this is what they taught us in precal:
$$\lim_{x\rightarrow\infty}x=\infty$$
$$\lim_{x\rightarrow\infty}x^{2}=\infty$$
But, we also know that $n^{2}>n$ if $n\notin [0,1]$ Does that mean that some infinities are greater than others? Why don't we explicitly define infinity so that we can show differences in sizes? Thank you in advance!","['infinity', 'limits']"
1830334,The jumping times of a càdlàg process are stopping times.,"Protter first proves this theorem: Let $X$ be an adapted càdlàg stochastic process, and let $A$ be a
  closed set. Then the random variable: $T(\omega)=\inf\{t: X_t(\omega)\in A \text{ or } X_{t^-} \in A\}$, is a stopping time. Then he says that it follows from this theorem : If $K>0$ is a positive number, then: $T^1 = \inf\{t >0 : |\Delta X_t|>k\}$ $T^{n+1}=\inf\{t >T^n : |\Delta X_t|>k\}$, are stopping times. I've tried proving this from the above theorem, but I am not able to do it. attempt regarding $T_1$ I need to show that $\{\omega : T^1(\omega) \le t\}\in \mathcal{F}_t$. If an $\omega$ is in this set, then since a càdlàg process only have a finite number of jumps larger than $k/2$ in the interval $[0,t+1]$, we must have that at least one jump $>k$ occurring  in the interval $[0,t]$. My guess in using the theorem is that A maybe should be $[-k,k]$ ? But I don't see how this works? attempt regarding induction I also have a problem with the induction part. Lets say that we have that $T^1,\ldots,T^n$ are stopping times. Why is then $T^{n+1}$ a stopping time? I have that $\{T^{n} \le t\}\subset\{T^{n+1} \le t\}$. But my guess is that I must write $\{T^{n} \le t\}\cup C=\{T^{n+1} \le t\}$, where $C \in \mathcal{F}_t$. But I can't really see what $C$ this must be. Do you see how to show this? I have seen this argument used in many books, maybe it is very easy to show. Is there a book that shows the argument?","['stochastic-processes', 'probability-theory', 'stopping-times']"
1830354,Series of independent Bernoulli variables,"Let $X_1, X_2, \ldots$ be independent, identically distributed random variables with distribution $\text{Ber}(\frac{1}{2})$ . Define the random varible: $$Y:=\sum_{n=1}^\infty \frac{X_n}{2^n}$$ Prove that $Y$ is uniformly distributed over the unit interval $[0,1]$ . The only way I currently know how to attack this problem is by using Levy's theorem: For $Y_n:=\sum_{k=1}^n \frac{X_k}{2^k}$ , it's enough to prove that the characteristic function of $Y_n$ converges to the characteristic function of a uniform random variable, i.e.: $$\lim_{n\to\infty}\mathbb{E}[e^{itY_n}]=\frac{e^{it}-1}{it}$$ Since $X_1, X_2, \ldots$ are independent, we have: $$\mathbb{E}[e^{itY_n}]=\prod_{k=1}^n \mathbb{E} \left[e^{it\frac{X_k}{2^k}} \right] = \prod_{k=1}^n \frac{1+e^{it/2^k}}{2}.$$ I'm basically stuck because I don't know what to do with the product as $n\to\infty$ . Is there a trick to it? Is there maybe a simpler solution? Thanks in advance!","['probability', 'random-variables']"
1830364,The inner product matrix with zero determinant implies that all the vectors are linearly dependent.,"Let $(V,\langle \cdot,\cdot\rangle)$ be an inner product space over the real field $\mathbb{R}$ and $v_1,\dots,v_n\in V$. Suppose that $A=(a_{ij})\in M_n(\mathbb{R)}$ with $a_{ij}=\langle v_i,v_j\rangle$ for all $1\leq i,j\leq n$. Show that if $\det A=0$, then $v_1,\dots,v_n$ are linearly dependent in $V$. Here I try to work out for $M_2(\mathbb{R})$. So
$$A=\begin{pmatrix}\langle v_1,v_1\rangle &\langle v_1,v_2\rangle\\\langle v_1,v_2\rangle &\langle v_i,v_j\rangle\end{pmatrix}$$
Since its determinant is zero, we get $$||v_1||^2||v_2||^2-\langle v_1,v_2\rangle^2=0$$
So here I don't have idea how to show that $v_1=\alpha v_2$ for some $\alpha\in \mathbb{R}$.","['matrices', 'linear-algebra']"
1830388,Is $t_n=\frac{1}{n} \left(1+\frac{1}{\sqrt2}+\cdots+\frac{1}{\sqrt n}\right)$ convergent?,"Let $$t_n=\frac{1}{n} \left( 1+\frac{1}{\sqrt2}+\cdots+\frac{1}{\sqrt n} \right)$$ Is the following series convergent? If we let $a_n=\frac{1}{\sqrt n}$ , then $$\lim_{n\to\infty}a_n=0\rightarrow \lim_{n\to\infty}\frac{1}{n}(a_1+a_2+\cdots+a_n)=0.$$ I don't know whether this is correct or not.","['convergence-divergence', 'limits']"
1830415,How much area in a unit square is not covered by $k$ disks of area $1/k$ centered at random points within the square?,"1 . Paint a $1\times 1$ square in blue. 2 . Take $k$ points randomly and uniformly from the square. 3 . Paint $k$ disks of area $1/k$ centered at each point in red. What is the expected remaining blue area? Remark: the sum of the areas of the disks is $k\cdot 1/k=1$ , the same as the area of the square, but we have to account for: Disks intersecting each other. Disks intersecting the outside of the square.","['expectation', 'probability']"
1830439,Can I factor a rational expression of the form...,"Given two equations $\displaystyle P_1 = \frac{1-X^2}{1-X^2}$ and $\displaystyle P_2 = \frac{1-aX^2}{1-bX^2}$ I am told that there is a relationship between P1 and P2 $P_1 \geq P_2$ It is clear that by visually comparing P1 and P2 , the coefficients a and b are responsible for determining the relationship between P1 and P2 . That being said, I would like to express the relationship $P_1 \geq P_2$ as $P_1 \geq P_1f(a,b)$ $1 \geq f(a,b)$ Clearly f(a,b) can be defined as P2/P1 , so disregard this solution for the time being. My question is, can I express f(a,b) as a function of a and b alone? In other words, can I factor out P1 from P2 and be left with a function of a and b alone? If so, what is the form for f(a,b) ? Is it a rational expression? Is it a transform matrix? Is it another polynomial? A convolution? My objective is to evaluate f(a,b) , thus interpret the relationship between P1 and P2 . In other words if f(a,b)=1 then I know P1=P2 . I don't think I can make it any more clear. If I can't extract an expression for f(a,b) as a function of a and b alone, that's fine. I'm under the impression I can't but I thought someone on here might know more than I, so I figured I would ask.","['matrices', 'matrix-decomposition', 'linear-algebra', 'linear-transformations']"
1830456,Why is finding the roots of a polynomial equation so important? What is to gain? [duplicate],"This question already has answers here : Roots of functions / polynomials (3 answers) Closed 8 years ago . I have just started a pre calculus class, and our first lessons have been reviews on polynomial equation, quadratics and finding roots or solutions to equations. The topic is fairly simple but I just have to know why is it so important to find an X value of a function that results in it y value to become zero? Why do mathematicians go to such great lengths like creating a quadratic formula? There must be some reason and my lessons have failed to tell why.",['algebra-precalculus']
1830459,"How to show for a distribution $T$ and a test function $\varphi,~~T'[\varphi]\equiv -T[\varphi']\;?$","For a generalized
  function $T,$ we define
  $$T'[\varphi] ~≡~ −T[φ']~~~~~~\forall φ ∈ \mathcal D(Ω).$$ where $\mathcal D(\Omega)$ denotes the test function space. I'm not getting how they deduced this relation. Can anyone tell me how to prove the relation?","['functional-analysis', 'distribution-theory', 'derivatives']"
1830507,Proving $J_n(x)N_{n+1}(x)-J_{n+1}(x)N_n(x)=-\dfrac{2}{\pi x}$: Part $2$ of $3$,"The following question is the second part to this previous question : Prove that $$J_p(x)J_{-p}^{\prime}(x)-J_{-p}(x)J_{p}^{\prime}(x)=-\frac{2}{\pi x}\sin(p\pi)\tag{1}$$ from $$\frac{\mathrm{d}}{\mathrm{d}x}\left[x\left(J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}\right)\right]=0\qquad\longleftarrow\text{(Proved in Part 1)}$$ it follows that $$J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}=\frac{c}{x}\tag{2}$$ To find $c$ use $$\fbox{$J_p(x)=\sum_{n=0}^\infty\frac{(-1)^n}{\Gamma(n+1)\Gamma(n+1+p)}\left(\frac{x}{2}\right)^{2n+p}$}\tag{3}$$ for each of the $4$ functions and pick out the $\dfrac{1}{x}$ terms in the products. Then use $$\fbox{$\Gamma(p)\Gamma(1-p)=\frac{\pi}{\sin(p\pi)}$}\tag{4}$$ Writing out the $4$ functions: $$J_p(x)=\sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\tag{a}$$ $$J_p^{\prime}(x)=\sum_{n=0}^\infty\frac{(-1)^n(2n+p)\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\cdot\frac{1}{x}\tag{b}$$ $$J_{-p}(x)=\sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\tag{c}$$ $$J_{-p}^{\prime}(x)=\sum_{n=0}^\infty\frac{(-1)^n(2n-p)\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\cdot\frac{1}{x}\tag{d}$$ Insertion of $(\mathrm{a})$, $(\mathrm{b})$, $(\mathrm{c})$, $(\mathrm{d})$ into $(2)$ gives $$\bbox[#AFF]{
\sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\cdot\sum_{n=0}^\infty\frac{(-1)^n(2n-p)\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\cdot\frac{1}{x}\quad-\\\sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\cdot\sum_{n=0}^\infty\frac{(-1)^n(2n+p)\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\cdot\frac{1}{x}=\frac{c}{x}
}$$ So unless I'm mistaken this means that the highlighted equation above requires that 
$$J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}=c\tag{5}$$ If this is the case then insertion of $(5)$ into $(2)$ implies that
$$J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}=\frac{J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}}{x}\implies x=1$$ I'm not sure if what I have done so far is correct, also I have no idea how to make use of $$\fbox{$\Gamma(p)\Gamma(1-p)=\frac{\pi}{\sin(p\pi)}$}$$ Could anyone please provide some feedback, hints or tips on how I can prove that 
$$\fbox{$\color{red}{J_p(x)J_{-p}^{\prime}(x)-J_{-p}(x)J_{p}^{\prime}(x)=-\frac{2}{\pi x}\sin(p\pi)}$}$$ Much appreciated.","['bessel-functions', 'ordinary-differential-equations']"
1830524,If the area of $ ABP$ is $ 192 $ find $ PA*PC $,"Let $ABCD$ be an isosceles trapezium with bases $ AB=32 $ and $CD=18$.
Inside $ABCD$ there's a point $P$ such that $ \angle PAD= \angle PBA $ and $ \angle PDA =\angle PCD $.
If the area of $ ABP$ is $ 192 $ find $ PA*PC $. My try : lead by $P$ perpendicular to the basics; it intersects the base points $E$ and $F$. $S_{ABP}=\frac12AB\cdot PE$ $\Rightarrow$ $PE=12$. How prove $AE=EB$?",['geometry']
1830543,A matrix being symmetric/orthogonal/projection matrix/stochastic matrix,"I am trying to do some practice questions and wanted to check the following properties and confirm my definition of projection matrix: Let $$A = \left[\begin{matrix} 1/2 & 0 & 1/2 \\ 0 & 1 & 0 \\ 1/2 & 0 & 1/2 \end{matrix}\right]$$ Is the following true or false ? i) $A$ is orthogonal ii) $A$ is a projection matrix I think that $A$ is not orthogonal since $AA^T \neq I$ but is a projection matrix given that a projection matrix is defined as $P^2 = P$ . Also accordingly, a projection matrix is orthogonal iff $P^T = P$ and in that case, is an orthogonal projection matrix different from the idea of an orthogonal matrix itself ? It appears that $A^T = A$ but yet $AA^T \neq A$ . Would someone mind helping me fill up the gap for this misconception, any advice is appreciated. Thank you","['matrices', 'linear-algebra']"
1830554,"Examine whether the function $f(x, y)$ is continuous on $\Bbb R^2$ or not","Given, $f: \Bbb R^2 \rightarrow \Bbb R,$ $$f(x, y) := |\frac y {x^2}| e^{-|\frac y {x^2}|}, x \neq 0, y \in \Bbb R,$$ $$f(x, y) := 0, x = 0,$$ I have to decide whether the function is continuous on $\Bbb R^2$ or not. I'm still new to multivariable calculus, so I'm not sure how to work with the critical values yet, but I gave it a try. Assume $f(x, y)$ would be continuous on $\Bbb R^2.$ Then, $f(x, y)$ must be continuous in $f(x, y) = 0,$ which means that, given any sequence $(x, y)_{_n \in \Bbb N},$ $$\lim_{n\to \infty} (x, y)_n = 0 \Rightarrow \lim_{n\to \infty} f(x, y)_n = f(\lim_{x \rightarrow 0}, 0)$$ Now, in my opinion, this can't be true. When $\lim_{n\to \infty} (x, y)_n = 0$ , then $|\frac y {x^2}|$ converges to $1,$ since $x^2$ becomes (at a certain point) smaller than $1,$ such that $x^2$ converges much faster to $0$ than $y$ does. Therefore, the fraction becomes bigger. Transfering this result to $e^{-|\frac y {x^2}|},$ this term should simply converge to $1 \over e$ , and so does the whole expression. On the other hand, putting in $y = 0$ doesn't give the same result, therefore, the function can't be continious on $\Bbb R^2$ .","['multivariable-calculus', 'continuity']"
1830580,Infimum of lower semicontinuous functions,"The following proposition is from the book Nicolae Dinculeanu Integration on Locally Compact Spaces: Let $H$ and $K$ be two compact Hausdorff spaces and $\alpha$ a continuous mapping of $H$ onto $K$. If $h\geq 0$ is a lower semicontinuous function on $H$ then the function $g(s)=\inf_{\alpha(t)=s}h(t)$ is lower semicontinuous on $K$. If $\alpha$ is one-to-one then it is not that hard to prove this proposition, but $\alpha$ here is not necessary one-to-one, can anyone suggest a proof? The following proof is from the author, the bold text is where I get confused, does the author do the quotient relation so we can change the $\alpha$ to be one-to-one? And it seems that the condition that $h\geq 0$ is redundant in the following proof. We remark first that for every point $s\in K$ there exists a point $t_{s}\in H$ such that $\alpha(t_{s})=s$ and $g(s)=h(t_{s})$. In fact, $\{s\}$ is a closed set in $K$ and $\alpha$ is continuous on $H$, therefore $\alpha^{-1}(s)$ is closed in $H$ and hence compact. Since $h$ is lower semicontinuous and $\alpha^{-1}(s)$ is compact, there exists a point $t_{s}\in \alpha^{-1}(s)$ such that $h(t_{s})=\inf_{\alpha(t)=s}h(t)$, hence $g(s)=h(t_{s})$. Let now $a$ be an arbitrary real number and prove that the set $A=\{s\in K: g(s)>a\}$ is open. It will then follow that $g$ is lower semicontinuous. Let $s_{0}\in A$. We have $g(s_{0})>a$, therefore $h(t)>a$ for every $t\in\alpha^{-1}(s_{0})$. Since $h$ is lower semicontinuous, there exists a neighbourhood $V$ of $\alpha^{-1}(s_{0})$ such that $h(t)>a$ for every $t\in V$. In addition, we can consider the set $V$ saturated for the equivalence relation $\alpha(t)=\alpha(t')$. Then $\alpha(V)$ is a neighbourhood of $s_{0}$. For every $s\in\alpha(V)$ there exists $t\in\alpha^{-1}(s)$ such that $h(t)=g(s)$. Therefore, we have $g(s)>a$ for every $s\in\alpha(V)$, hence $A$ is open.","['real-analysis', 'functional-analysis', 'measure-theory', 'general-topology', 'analysis']"
1830592,Discover to which batch a coin belongs,"The following question is taken from a book, in a chapter on probabilty: You have two batches of unbalanced coins. One has coins which turn up head with probability $p_1$, and the other has coins that turn up head with probability $p_2$. All you know about $p1$ and $p2$ is that $|p_1-p_2| \ge 0.01$. A coin was accidentaly dropped from one of the batches. Design an experiment that will determine which batch the coin belongs to, with error probability of no more than $10^{-12}$. You should use a reasonable number of coin tosses. As far as I can tell, any experiment will essentially boil down to something like this: Take a coin from each batch and toss it $n$ times. Let $X$ and $Y$ be the sample means of the tosses of the coins from the first and second batch, respectively. Then toss the coin in question $n$ times, and let $Z$ be the sample mean of the tosses of that coin. Then find $n$ for which:
$$
\mathbb{P}[\,|X-Z|<|Y-Z|\,] \le10^{-12}
$$
when the coin in question is from the second batch. I tried to play around with the inquality $|X-Z|<|Y-Z|$ to get something to which I may apply the Chernoff bound. But the best I got required $n\approx 3\cdot10^6$, which does not seem reasonable at all. Any help will be appreciated.",['probability']
1830623,Covering by open subfunctors and epimorphisms of sheaves.,"I am trying to learn about the functor of points approach to algebraic geometry.  Given the category of locally ringed spaces $GSp$ (geometric spaces) we have a functor 
$$\mathcal{G}: GSp \to Set^{Rng} \\
X \mapsto Hom(\operatorname{Spec}-,X)$$ It is clear to me that if $X \cong \operatorname{Spec}A$ then $\mathcal{G}(\operatorname{Spec}A)\cong Hom(A,-)$. I am having problems to understand what happens when $X$ is a general scheme. First of all, a scheme has an affine cover by $\{ \operatorname{Spec}A_i\}$ which is mapped by $\mathcal{G}$ to a family of open subfunctors of $\mathcal{G}X$ in the sense of this Trying to understand open (closed) subfunctors . Also since $\{ \operatorname{Spec}A_i\}$ is a covering it is clear that the induced morphism $\bigsqcup \operatorname{Spec}A_i \to X$ is an epimorphism. If the notion of covering by open subfunctors is analogous to the topological one then we should have an epimorphism $$\bigsqcup Hom(A_i,-) \to \mathcal{G}X.$$ This condition is translated in the book of Demazure by saying that for every field $K$, $\bigsqcup Hom(A_i,K) \to \mathcal{G}X(K)$ is an epimorphism. I think this condition is essentially the same as the one I stated before but since we are working with sheaves on the Zariski site instead of general presheaves the condition of being a sheaf epimorphism can be checked locally on fields. Am I right? In this case how can I proof the fact that both ideas are equivalent? A general statement for this would be: Given two sheaves $\mathcal{F},\mathcal{G}$ in the Zariski site is it true that $\mathcal{F} \to \mathcal{G}$ is a sheaf epimorphism $\iff$ $\mathcal{F}(K) \to \mathcal{G}(K)$ is surjective for every field?","['sheaf-theory', 'topos-theory', 'algebraic-geometry', 'schemes', 'category-theory']"
1830667,Connection after a metric rescaling and compatibility,"It's known (see here for example) that after a rescaling of the metric $\tilde{g}=e^{2\omega}g$, we can find a new connection $\tilde\nabla$ associated to the new metric: $
\tilde\nabla _X Y = \nabla _X Y + (X \omega )Y + (Y \omega )X - g(X,Y) \operatorname{grad}\omega \tag{1},
$ where $\nabla$ is the Levi-Civita connection of $(M,g)$. In coordinates: $
\tilde\Gamma^{k}_{ij}=\Gamma^{k}_{ij} + \delta_{i}^{k} \partial_j \omega  + \delta_{j}^{k} \partial_i \omega - g_{i j} g^{k l} \partial_{l} \omega. \tag{2}
$ My question is: is the new connection $\tilde\nabla$ compatible with the new metric $\tilde g$? I am using Equation (1) together with the property $
X[\tilde g(Y,Z)] = (\tilde\nabla_X\tilde g)(Y,Z)+\tilde g(\tilde\nabla_XY,Z)+\tilde g(Y,\tilde\nabla_XZ)\tag{3},
$ but I am not getting $\tilde\nabla\tilde g = 0$. Instead $\tilde\nabla\tilde g$ is proportional to the new metric. Is this right?","['riemannian-geometry', 'differential-geometry']"
1830671,Cardinality of Surjective only & Injective only functions,"I'm a college student just beginning to study the very basic of set theory. In studying about Surjective & Injective functions & how they map their domain to their codomain, it came to my mind a question that I really want to confirm: 1) Is it possible to build a ""surjective non-injective"" function with a domain which has a ""lower"" cardinality than it's codomain? 2) Is it possible to build an ""injective non-surjective"" function with a domain which has a ""higher"" cardinality than it's codomain? The context of these questions are: This post is a related (but different) question to this post here (which is also posted by myself) but I was asked to open a new question (read the comments in the related post) Both the domain & the codomain of the function must have a finite number of elements. They are both finite sets. And also, both must not be an empty set. The function must not be a partial function & must not be a multi-valued function. The function must be injective only or surjective only. I'm not expecting complex answers that explain using axioms, morphisms, complex notations, etc, which I cannot understand as of yet, since I'm just ""beginning"" to study the basics of set theory. I already tried building such ""function"" but never really succeeded. The result is always the ""reverse"" of what my question is asking. And this post is really just to ""make sure"" that it's true that building such function is never really possible (for ""finite"" sets only). Unless... umm I am missing something. As I'm typing this post, I already read all the questions in the ""similar questions"" and in the ""Questions that may already have your answer"" panel. I did found some questions that are similar, but not actually detailed enough, requiring specific conditions & using specific context such as those that I'm posting here. I'm still posting this since those are not satisfying enough of a question & answer for me, so not quite what I'm looking for. Hope I don't get marked as duplicate. Since I'm just a newbie, let me know in the comment if there is something in the question that still needs to be cleared out or not in accordance to the rules of this forum. I hope the context is clear enough. Best Regards,","['elementary-set-theory', 'functions']"
1830690,Finding extreme value,"It is given: $f\left(x,y\right)=-7x^{2}-5xy+4y^{2}   $ and I should find x coordinate of the extreme value with condition $x-4y=4$. I think I know how to do this, but my solution is not the correct. I hope someone can tell me where is my mistake. What I did: From condition I got $y=\frac{x}{4}-1 $. I put that in my function and got this: $-8x^{2}+3x+4=0 $. I derivative it and got: -16x+3=0. So solution should be $\frac{3}{16} $. But it is not. What is my mistake?","['multivariable-calculus', 'calculus']"
1830694,Is this notation mathematically-correct? $\cot\alpha\pm\tan\alpha=\frac2{{\sin\atop\tan}(2\alpha)}$,"I have a question.
Look at the following expression: $$\cot\alpha\pm\tan\alpha=\frac2{{\sin\atop\tan}(2\alpha)}$$ Is it written well, according to the laws of mathematical language? In that formula, I meant that in case of ' $+$ ' we will use the $\sin()$ function, and in case of ' $-$ ' we will use the $\cos()$ function. If it's not good, is there a good way (a mathematically-correct way) of writing both cases (both ' $+$ ' and ' $-$ ') in one formula? by the way,
do you think that this general explanatory is good and understandable ? $a \pm b = {func1\atop func2}(c)$ means: $a+b=func1(c)$ $a-b=func2(c)$ Thank you,
Tom","['trigonometry', 'notation']"
1830695,How to rotate a 3D vector on the surface of a plane by a known angle?,Available data The plane β which is defined by a normal vector n and point P. The vector v which lies on the surface of the plane.(the angle between v and n is 90 degrees). The angle α to which v should be rotated. How to obtain the rotated vector(vrot) ? Note that the vectors are 3D.,"['linear-algebra', 'vectors', 'geometry']"
1830702,Proving $\frac {\sin x}{1-\sin x}-\frac {\sin x}{1+\sin x}\equiv 2\tan^2 x$,"I need assistance with proving the following identity: $$\frac {\sin x}{1-\sin x}-\frac{\sin x}{1+\sin x} \equiv 2\tan^2 x$$ What I have done so far is expanded them: $$\frac {\sin x\;(1+\sin x)}{(1-\sin x)(1+\sin x)}-\frac {\sin x\;(1-\sin x)}{(1+\sin x)(1-\sin x)}$$ So therefore: 
$$\frac {\sin x+\sin^2 x}{1-\sin^2x}-\frac {\sin x-\sin^2 x}{1-\sin^2 x}$$ I'm completely stuck on what to do next.  Any pointers would be appreciated.  Thanks for your time!",['trigonometry']
1830707,"How can we prove that $\langle\int_0^t\Phi_s{\rm d}W_s,x\rangle_H=\sum_{n\in\mathbb N}\int_0^t\langle\sqrt{λ_n}\Phi_se_n,x\rangle_H{\rm d}B_s^{(n)}$?","Let$^1$ $U$ and $H$ be separable $\mathbb R$-Hilbert spaces $Q\in\mathfrak L(U,H)$ be nonnegative and symmetric operator on $U$ with finite trace $(e_n)_{n\in\mathbb N}$ be an orthonormal basis of $U$ with $$Qe_n=\lambda_ne_n\;\;\;\text{for all }n\in\mathbb N$$ for some $(\lambda_n)_{n\in\mathbb N}\subseteq[0,\infty)$ $(\Omega,\mathcal A,\operatorname P)$ be a probability space and $(\mathcal F_t)_{t\ge 0}$ be a filtration of $\mathcal A$ $W$ be a $Q$-Wiener process on $(\Omega,\mathcal A,\operatorname P)$ with respect to $\mathcal F$, $$B^{(n)}:=\begin{cases}\frac{\langle W,e_n\rangle_U}{\sqrt{\lambda_n}}&\text{, if }n\in\mathbb N\text{ with }\lambda_n>0\\0&\text{, else}\end{cases}$$ and $$W^{(n)}:=\sum_{i=1}^n\sqrt{\lambda_i}B^{(i)}e_i\;\;\;\text{for }n\in\mathbb N$$ Note that the $B^{(n)}$ are independent $\mathcal F$-Brownian motions on $(\Omega,\mathcal A,\operatorname P)$, $$\left\|W_t^{(n)}-W_t\right\|_{L^2(\operatorname P,\;U)}\stackrel{n\to\infty}\to 0\;\;\;\text{for all }t\ge 0\tag 1$$ and $$\operatorname P\left[\left\|W^{(n)}-W\right\|_{C^0([0,\;t],\;U)}\stackrel{n\to\infty}\to 0\right]=1\;\;\;\text{for all }t\ge 0\tag 2$$ Now, let $(\Phi_t)_{t\ge 0}$ be a $\mathfrak L(U,H)$-valued $\mathcal F$-adapted and locally bounded $^2$ stochastic process on $(\Omega,\mathcal A,\operatorname P)$ with $$\Phi_t=\sum_{i=1}^n\xi_{i-1}1_{(t_{i-1},t_i]}\;\;\;\text{for all }t\ge 0\tag 3$$ for some $\mathfrak L(U,H)$-valued random variables $\xi_0,\ldots,\xi_{n-1}$ on $(\Omega,\mathcal A,\operatorname P)$ and some $0=t_0<\cdots<t_n$. Moreover, let $$\int_0^t\Phi_s{\rm d}W_s:=\sum_{i=1}^n\xi_{i-1}\left(W_{t_i\wedge t}-W_{t_{i-1}\wedge t}\right)\;\;\;\text{for }t\ge 0\;.$$ Let $x\in H$. How can we prove that $$\langle\int_0^t\Phi_s{\rm d}W_s,x\rangle_H=\sum_{n\in\mathbb N}\int_0^t\langle\sqrt{\lambda_n}\Phi_se_n,x\rangle_H{\rm d}B_s^{(n)}\tag 4$$ in $L^2(\operatorname P)$, for all $t\ge 0$? Let $$X^{(n)}_t:=\langle\sqrt{\lambda_n}\Phi_te_n,x\rangle_H\;\;\;\text{for }n\in\mathbb N\text{ and }t\ge 0\;.$$ By definition of the Itō integral with respect to a Brownian motion , $$\int_0^tX^{(n)}_s{\rm d}B_s=\sqrt{\lambda_n}\sum_{i=1}^n\langle\xi_{i-1}e_n,x\rangle_H\left(B^{(n)}_{t_i\wedge t}-B^{(n)}_{t_{i-1}\wedge t}\right)\;\;\;\text{for all }n\in\mathbb N\text{ and }t\ge 0\;.\tag 5$$ $\color{red}{\text{Now comes the crucial part!}}$ In the following equation chain, why do $(6)$ and $(7)$ hold? \begin{equation}
\begin{split}
\langle\int_0^t\Phi_s{\rm d}W_s,x\rangle_H&=\sum_{i=1}^n\langle\xi_{i-1}\left(W_{t_i\wedge t}-W_{t_{i-1}\wedge t}\right),x\rangle_H\\
&\stackrel{(6)}{\color{red}=}\sum_{i=1}^n\langle\xi_{i-1}\sum_{k\in\mathbb N}\sqrt{\lambda_k}\left(B^{(k)}_{t_i\wedge t}-B^{(k)}_{t_{i-1}\wedge t}\right)e_k,x\rangle_H\\
&=\sum_{i=1}^n\sum_{k\in\mathbb N}\sqrt{\lambda_k}\langle\xi_{i-1}e_k,x\rangle_H\left(B^{(k)}_{t_i\wedge t}-B^{(k)}_{t_{i-1}\wedge t}\right)\\
&\stackrel{(7)}{\color{red}=}\sum_{k\in\mathbb N}\sum_{i=1}^n\sqrt{\lambda_k}\langle\xi_{i-1}e_k,x\rangle_H\left(B^{(k)}_{t_i\wedge t}-B^{(k)}_{t_{i-1}\wedge t}\right)\\
&=\sum_{k\in\mathbb N}\int_0^tX^{(k)}_s{\rm d}B^{(k)}_s
\end{split}\tag 8
\end{equation} for all $t\ge 0$. In particular, I don't really understand in which sense the equation chain $(8)$ holds. In the sense of $L^2(\operatorname P)$-convergence or in the sense of $\operatorname P$-almost sure convergence? $^1$ Let $\mathfrak L(U,H)$ denote the space of bounded, linear operators from $U$ to $H$. $^2$ i.e. $$\sup_\Omega\left\|\Phi_t\right\|_{\mathfrak L(U,\;H)}<\infty\;\;\;\text{for all }t\ge 0\;.$$","['stochastic-processes', 'real-analysis', 'functional-analysis', 'stochastic-integrals', 'stochastic-analysis']"
1830748,Holonomy of a curve in case of principal $U(1)$ bundle,"Suppose $\pi : P\rightarrow M$ is principal $U(1)$ bundle. Let $\gamma$ be a loop in $M$ based at $x_0$ and write $iA$ as connection 1-form on $P$ where $A\in \Omega(P)$. Now define $hol_{\gamma}(A)\in U(1)$ as follows: let $\tilde{\gamma}$ be a horizontal lift of $\gamma$ with $\tilde{\gamma}(0)=p_0$ and $\pi(p_0)=x_0$. Then there is a unique element $hol_{\gamma}(A)$ in $U(1)$ such that $\tilde{\gamma}(1)=p_0\cdot hol_{\gamma}(A)$ because the action is free and transitive. I now want to calculate explicitly what is this number $hol_{\gamma}(A)$. I know already it has to be $hol_{\gamma}(A)=e^{i\int_{\gamma}A}$, but I have no idea how this is calculated as I am new to holonomy. Any hint is useful for me. Thanks","['principal-bundles', 'differential-geometry']"
1830778,On the definition of a reductive group.,Wikipedia 1 defines a reductive group $G$ as an algebraic group with trivial unipotent radical. The radical is the connected component of identity in the maximal normal solvable subgroup of $G$. The unipotent radical is the set of the unipotent elements of the radical. An element $u$ is unipotent if $(u-1)$ is nilpotent. Question 1 - This last line is where everything breaks down in my head. If we talk of unipotent then there is a clear difference between what $1$ is and what $0$ is. In this case we have to be in a ring . But here we are just in a group $(G)$. What am I missing? EDIT - Question 2 - In this book (chapter 5) I have come across the definition that an affine algebraic group $G$ is reductive if its maximal connected solvable subgroup is a torus. (This is what Tobias mentions in his comment). But I am not sure how the definitions are equivalent when $G$ is affine. Thank you. 1 Link to the revision of the Wikipedia article at the time of this post.,"['algebraic-groups', 'algebraic-geometry', 'definition']"
1830807,Proof that the function is uniformly continuous,"Proof that the function $f: [0, \infty) \ni  x \mapsto \frac{x^{2}}{x + 1} \in \mathbb{R}$ is uniformly continuous. On the internet I found out that a function is uniformly continuous when $\forall \varepsilon > 0 \ \exists \delta > 0: \left | f(x)-f(x_{0}) \right | < \varepsilon$ whenever $\left | x - x_{0} \right | < \delta .$ Because I don't know how to prove it calculative, I have drawn the function and showed its uniform continuity like that.
But I'd like to know how to do it the other, more professional and efficient way. I've watched some videos but anyway couldn't find a solution. Also tried to for almost 2 hours myself but nothing came out, too. For the drawing, I think there is actually a mistake, at the beginning the epsilon (first one I haven't drawed) seems smaller than the others, while the others have same size.
(In addition I skipped the other part of the function because it's trivial). Here is the picture:","['calculus', 'analysis']"
1830852,Find projection matrix using partitioned matrices,"If X is a ($n$, $p+1$) design matrix, partition $X$ to be $X$=[$J$ $X$*]  where $J$ is a ($n$,$1$) vector of all $1$'s, and $X$* is a ($n$,$p$) matrix. Let $H_X$ be a projection matrix, where $H_X$ = $X{(X'X)}^{-1}X'$. I'm trying to prove: $H_X$ = $H_J$ + $H_X*$ And in doing so, I've been trying to deduce the above equation using: Suppose the design matrix $X$ can be decomposed by columns as $X$= [$A$ $B$]. 
  Define the hat or projection operator as $P${$X$} = $X{(X'X)}^{-1}X'$. Similarly, define the residual operator as $M${$X$}=$I$-$P${$X$}. 
  Then the projection matrix can be decomposed as follows: $P${$X$} = $P${$A$} + $P${$M${$A$}$B$}. I found this formula (the ""Blockwise Formula"") on Wikipedia.
The link is: https://en.wikipedia.org/wiki/Projection_matrix I've been trying to prove this formula, and from that deduce the equation I'm trying to solve. But I still haven't found a way to prove either. How can I prove the equation above? Will the formula I found on Wikipedia be helpful?","['matrices', 'projection-matrices', 'statistics', 'linear-algebra']"
1830870,Capacity of a set in $\mathbb{R}^n$,"The $2$-capacity of a set $\Omega$ sitting inside an open set $V \subset \mathbb{R}^n$ is given by $$\text{cap}_2(\Omega, V) = \inf_{u \in C^\infty_0(V), u|_\Omega \equiv 1} \int_V |\nabla u|^2 dx.$$ Now, if we assume that $\Omega$ is deep inside $V$, that is, suppose $\overline{\Omega} \subseteq V$, then do we have $\text{cap}_2(\Omega, V) = \text{cap}_2(\Omega, \mathbb{R}^n)$? Or at least $\text{cap}_2(\Omega, V) \leq C\text{cap}_2(\Omega, \mathbb{R}^n)$, where $C$ is a positive constant?","['real-analysis', 'reference-request', 'potential-theory', 'functional-analysis', 'measure-theory']"
1830929,"In $T_1$ space, all singleton sets are closed?","The definition of $T_1$ -Space is: A topological space $X$ is said to be $T_1$ if for each pair of distinct points $a,b, $ $\exists$ open sets $U,V$ s.t $a\in U, b\notin U, a\notin V, b\in V$ . What I'm confused about is in a $T_1$ space, all singleton subsets of $X$ are closed. Let $t,v \in X$ . Then I think the singleton sets $\{t\}$ , $\{v\}$ satisfy the definition of $T_1$ in $U$ and $V$ what I wrote above. (i.e $t \in\{t\}$ , $v\notin \{t\}$ , $t\notin\{v\}$ , $v \in\{v\}$ .) I learned the theorem showing this result and I can understand the proof of it, but I'm still confused as to why this is not a counterexample.","['general-topology', 'separation-axioms']"
1830950,"Given $\mathcal{F}$ a filter on $X$, and either $A$ or its complement are in $\mathcal{F}$, then $\mathcal{F}$ is ultrafilter","I am trying to prove the following fact: Given $\mathcal{F}$ a filter on $X$, Suppose $\forall A \subseteq X$
  where  either $A$ or $X\backslash A$ are in $\mathcal{F}$, then
  $\mathcal{F}$ is ultrafilter We know that $\mathcal{F}$ is an ultrafilter if for all $F$ a filter on $X$,
  $\mathcal{F} \not\subset F$ How to proceed with this question? I'm stuck at first step trying a contradiction proof. Suppose $\forall A \subseteq X$ where  either $A$ or $X\backslash A$ are in $\mathcal{F}$,  then let $F$ be another filter on $X$, we will show that $F\subset \mathcal{F}$. Proceed by contradiction, suppose that instead $\mathcal{F}\subset F$, we will show that this is not the case...","['filters', 'elementary-set-theory']"
1830967,Group theory - prove that $\forall x((x^{-1})^{-1}=x)$,"so I got this question for homework:
Prove that this property can be deduced from group theory: The inverse of an inverse is the identity: $\forall x((x^{-1})^{-1}=x)$ I tried building this statement from the group's axioms but was having difficulties. Some help/hints? Thanks","['group-theory', 'discrete-mathematics']"
1831015,Characterizing spaces with no nontrivial covers,I know that simply connected locally path-connected spaces have no nontrivial covers. Is there a characterization of spaces with this property?,"['path-connected', 'connectedness', 'covering-spaces', 'algebraic-topology', 'general-topology']"
1831017,A question about arithmetic progressions and prime numbers,"I took number $3$ and observed: $3$ is an arithmetic progression of length one. $3,5$ is an arithmetic progression of length two. $3,5,7$ is an arithmetic progression of length three. Then I took number $5$ and observed: $5$ is an arithmetic progression of length one. $5,7$ is an arithmetic progression of length two. $5,11,17$ is an arithmetic progression of length three. $5,11,17,23$ is an arithmetic progression of length four. $5,11,17,23,29$ is an arithmetic progression of length five. So surely it would be nice to know: Is it true that for every prime number $p$ there exists $p$ arithmetic progressions, first of length one, second of length two, ... , $p$-th of length $p$ so that every of those arithmetic progressions has number $p$ as its first term. I know about Green-Tao theorem but I do not know does this can follow from it or can the combination of Green-Tao theorem with some proven or unproven facts answer this question? It is clear that with $p$ as a starting point we cannot have an arithmetic progression of primes with $p+1$ terms because $(p+1)$-st term would be $p+pd$ which is composite. If this question has affirmative answer then we would have that for every natural $k$ we have an infinite number of arithmetic progressions of primes with $k$ terms so I guess that this is extremely hard but would like to hear opinions and suggestions for how could this be attacked.","['number-theory', 'arithmetic-progressions', 'prime-numbers']"
1831027,"Schwarzschild metric, speed of ball as measured by observer who catches the ball, just before ball is caught?","The Schwarzschild metric, describing the exterior gravitational field of a planet of mass $M$ and radius $R$, is given by$$ds^2 = -(1 - 2M/r)\,dt^2 + (1 - 2M/r)^{-1}\,dr^2 + r^2(d\theta^2 + \sin^2\theta \,d\phi^2).$$A tower has its base on the surface of this planet ($r = R$) and its top at radial coordinate $r = R_1$. A ball is held at rest by an observer at the top of the tower. It is then dropped and caught by an observer at the bottom of the tower. What is the speed, $v$, of the ball as measured by the observer who catches the ball, just before the ball is caught? Here, we are not assuming that $R \gg 2M$ or that $R_1 - R \ll R$. Also, I want the physical speed here, $v$, as would be measured, e.g. by a radar gun, not a coordinate speed, such as $dr/dt$.","['mathematical-physics', 'physics', 'differential-geometry', 'general-relativity']"
1831075,Uncountable subcollection of open sets with nonempty intersection,"Given an uncountable collection of open intervals in $\mathbb{R}$, can we choose an uncountable subcollection which will have unempty intersection? I read that indeed, we could do that. But on the other hand, I don't quite see why would this be impossible to have uncountably many open sets which all but for inifnitely many have an empty intersection. I know only countably many can be disjoint, but that doesn't mean that uncountably many are all NOT disjoint. It could happen there are these two in every subfamily of cardinality continuum that are disjoint, at least that's how I see it (bad intuition though). Is there any constructive proof for this problem?",['elementary-set-theory']
1831078,Is the probability addition rule commutative?,"The probability of $A$ and $B$ is the intersection between the venn diagrams for $A$ and $B$. Then is $P(A \cap B) = P(B \cap A)$? If so, then surely $\frac{P(A)}{P(B)} = \frac{P(A|B)}{P(B|A)}$. Is that correct? Let me rephrase the question.. Why is it always the case that the probability of A happening, then B happening is the same when the order is flipped?","['statistics', 'probability']"
1831101,Strong Law of Large Numbers for a i.i.d. sequence whose integral does not exist,"Prove : let $X_1 ,X_2 , ... $ be i.i.d. random variables with $\mathbb{E}[X_1^+]=\mathbb{E}[X_1^-]=+\infty$ , and $S_n=\sum_{i=1}^{n}{X_i}$ .
Then $$\limsup_{n\rightarrow\infty}{\frac{S_n}{n}=+\infty}\text{ a.s., }\liminf_{n\rightarrow\infty}{\frac{S_n}{n}=-\infty}\text{ a.s.}$$ I have proven that $$\mathbb{P}\left(\left\{\omega :\limsup_{n\rightarrow\infty}{\left|\frac{S_n}{n}(\omega)\right|=+\infty}\right\}\right)=1,$$ Since the events $$\left\{\omega:\limsup_{n\rightarrow\infty}{\frac{S_n}{n}(\omega)=+\infty}\right\} \text{ and }
\left\{\omega:\liminf_{n\rightarrow\infty}{\frac{S_n}{n}(\omega)=-\infty}\right\}$$ are asymptotic with regard to the sequence $X_1,X_2,...$ their probability is $0$ or $1$ , and at least one has probability one, but I don't know how to prove both of them. Update: According to the paper The strong law of large numbers when the mean is undefined (Erickson K B, 1973) , this proposition is wrong.","['probability-limit-theorems', 'law-of-large-numbers', 'probability-theory']"
1831147,Prove that $ f(x) = e^x + \ln x $ attains every real number as its value exactly once,"Prove that the function $$ f(x) = e^x + \ln x $$ attains every real number as its value exactly once. First, I thought to prove that this function is a monotonic continuous function. But then I wasn't sure if that is how to prove the result, and if it is, I wasn't sure of how exactly to prove it that way.",['calculus']
1831166,Range of $f\in C_c(X)$ is compact subset of complex plane,"The collection of all continuous complex functions on $X$ whose support is compact is denoted by $C_c(X)$. Book's proof is quite not detailed and I will write a detailed proof. Proof: Let $f\in C_c(X)$ with compact support $K$. Then it's easy to show that $f(X)\subset f(K)\cup \{0\}$.
If $X$ is compact then $f(X)$ is also compact by 2.10 Theorem. Suppose that $X$ is NOT compact. Then $\{0\}\in f(X)$ since otherwise $X=\{x\in X: f(x)\neq 0\}$ and we get that $K=X$ which is contradiction because $X$ is not compact. Since $K\subset X$ $\Rightarrow$ $f(K)\subset f(X)$ $\Rightarrow$ $f(K)\cup \{0\}\subset f(X)\cup \{0\}=f(X).$ We get that $$f(K)\cup \{0\}= f(X).$$ Since $\{0\}$ is compact then $f(X)$ is also compact since it's union of finite number of compact. Can anyone check my proof please?","['general-topology', 'real-analysis', 'elementary-set-theory']"
1831183,$A$ has a countable dense subset. How to describe a possible countable dense subset in $M_n(A)$?,"Let $A$ be a C$^*$-algebra and $M$ be a countable dense subset in $A$. Let $M_n(A)$ be the $C^*$-algebra of $n\times n$-matrices with entries in $A$, $n\in\mathbb{N}$. Then $M_n(A)$ should have a countable dense subset as well. Is $L=\{(a_{ij})_{i,j=1,\ldots,n}\in M_n(A):\;  a_{ij}\in M\; \text{for all}\; i,j \}$ such a subset? It is clear that $L$ is countable, but is $L$ dense in $M_n(A)$?","['functional-analysis', 'c-star-algebras']"
1831188,Calculating the matrix $M^{2006}$,"Say you have the matrix $M$:
$$\begin{bmatrix}\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\-\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\end{bmatrix}$$
How do you find $M^{2006}$?  My thinking was that you can find that $M^8 = I$, so if $\frac{2006}{8} = 250\frac{3}{4}$, then $M^{2003} = I$, so if you multiply this by $M$, $3$ times, you would get $M^{2006}$.  Though, there seems to be something wrong with my arithmetic or else you cannot do this with matrix powers, as this is the incorrect answer. The correct answer is:
$$\begin{bmatrix}0&-1\\1&0\end{bmatrix}$$ How do I get there?","['matrices', 'matrix-equations']"
1831191,Why is/isn't the derivative of a differentiable function continuous?,"I am confused about the following Theorem: Let $f: I \to \mathbb{R}^n$ , $a \in I$ . Then the function $f$ is differentiable at $a$ if and only if there exists a function $\varphi: I \to \mathbb{R}^n$ that is continuous in $a$ , and such that $f(x) - f(a) = (x - a)\varphi(x)$ , for all $x \in I$ ; furthermore, $\varphi(a) = f'(a)$ . I understand the proof of this theorem, but something confuses me. Doesn't this theorem state that the derivative of a function in a point is always continuous in that point, since $f'(a) = \varphi(a)$ is continuous in $a$ ? This would mean that the derivative of a function is always continuous on the domain of the function, but I have encountered counterexamples. I have probably misinterpreted something; any help would be welcome.","['derivatives', 'real-analysis', 'continuity', 'analysis']"
1831219,Let $\vert G \vert = p^n m$ where $m < 2p$. Show that $G$ has a normal subgroup of order $p^{n-1}$ or $p^n$,"Let $G$ have order $p^n m$ where p is a prime and $p \nmid m$. Suppose $m < 2p$. Show that $G$ has a normal subgroup of order $p^{n-1}$ or $p^n$. I have tried to apply the Sylow Theorems but I can't see how to continue in the case when the Sylow $p$-subgroup is not normal. Let $P$ be a Sylow $p$-group. Then $\vert P \vert = p^n$. If $P$ is normal in $G$ we are done, otherwise $k := \vert \operatorname{Syl}_G(p) \vert \ge 2$. By Sylow's Theorem $k \mid m$. Using our assumption that $m < 2p$ we get that $m/k < p$. Am I on the right track? Since $m=[G:P]$, $k=[G:N_G(P)]$ and $[G:P]=[G:N_G(P)][N_G(P):P]$ we get that $m/k = [N_G(P):P] < p$. Not sure if this is useful. I know that $P$, being a $p$-group, has a normal subgroup of order $p^{n-1}$. I don't see why it would be normal in $G$ though. I prefer hints to full answers.","['finite-groups', 'abstract-algebra', 'group-theory']"
1831227,Is the flow of an analytic vector field also analytic?,Let $X$ be an analytic vector field on a smooth manifold. Is it true that the flow $\Phi_t:M\to M$ associated to that vector field is also analytic?,"['analytic-functions', 'differential-geometry', 'analytic-geometry']"
1831243,Is it ok to use Kronecker delta function to find if one of its variables belongs to a half open interval?,"Kronecker ""delta"" function is generally defined as 
$\delta(i,j)=1$ if $i$ is equal to $ j$, otherwise $0$. How about if $j$ is not an integer? I mean let $j$ is a half open interval defined as $j=(0,1]$ and $ i$ has any value on interval $[0,1]$,
then can we use Kronecker delta to find if $i$ belongs to $j$? In other words, can we define like: $\delta(i,j)=1$ if $i$  is in $j$, $0$ otherwise?","['linear-algebra', 'functions']"
1831266,If $C\cap D=\emptyset$ Prove that $f^c(C)\cap f^c(D)=\emptyset$,"Is this the proper way to go about proving this? By showing $C\cap D$ = $f^c(C)\cap f^c(D)=\emptyset$ ? Any feedback would be greatly appreciated. I don't have any other way of getting feedback for my proofs and I want to improve. Let $f : X \rightarrow Y$ be a function, $C$ , $D$ subsets of $Y$ . If $C\cap D=\emptyset$ Prove that $f^c(C)\cap f^c(D)=\emptyset$ i)Assume $x \in f^c(C \cap D)$ . Then $f(x)\in C \cap D$ so that $f(x) \in C$ and $f(x)\in D$ .
Since $f(x)\in C$ , $x \in f^c(C)$ . Since $f(x)\in D$ , $x \in f^c(D)$ . Since $x \in f^c(D)$ and $x \in f^c(D)$ it follows that $x\in f^c(C) \cap f^c(D)$ . ii)On the other hand, assume $x \in f^c(C) \cap f^c(D)$ . Then $f(x)\in C$ and $f(x)\in D$ . Thus, $f(x) \in C \cap D$ so that $x \in f^c(C \cap D)$ .","['elementary-set-theory', 'functions']"
1831270,Different ways of evaluating $\int_{0}^{\pi/2}\frac{\cos(x)}{\sin(x)+\cos(x)}dx$,"My friend showed me this integral (and a neat way he evaluated it) and I am interested in seeing a few ways of evaluating it, especially if they are ""often"" used tricks. I can't quite recall his way, but it had something to do with an identity for phase shifting sine or cosine, like noting that $\cos(x+\pi/2)=-\sin(x)$ we get:
$$ I=\int_{0}^{\pi/2}\frac{\cos(x)}{\sin(x)+\cos(x)}dx=\int_{\pi/2}^{\pi}\frac{-\sin(x)}{-\sin(x)+\cos(x)}dx\\
$$
Except for as I have tried, my signs don't work out well. The end result was finding
$$ 2I=\int_{0}^{\pi/2}dx\Rightarrow I=\pi/4
$$
Any help is appreciated! Thanks.","['integration', 'trigonometry', 'calculus']"
1831305,Polynomial ring with isomorphic quotients [duplicate],"This question already has answers here : What is necessary and/or sufficient for polynomials to provide isomorphic quotient rings? (2 answers) Closed 7 years ago . If $R$ is a commutative ring and $f(x), g(x) \in R[x]$ two polynomials such that $R[x]/f(x)\cong R[x]/g(x)$ as $R$-algebras, what can we say about $f$ and $g$? Or given $f(x)\in R[x]$, what can we say about the set $\{g \in R[x]\mid R[x]/f(x) \cong R[x]/g(x)\}$? Can we conclude that $\mathrm{deg}(f) = \mathrm{deg}(g)$? Is there an automorphism of $R[x]$ taking $f$ to $g$? I'm most concerned about the cases where $R=k$ is a field, $R=\mathbb{Z}$, or $R=\mathcal{O}_k$ is the ring of integers of a number field, but I'd be interested in the most general results possible.","['abstract-algebra', 'ring-theory', 'polynomials', 'commutative-algebra']"
1831308,Any reason not to define a derivative as the average of the derivatives on all sides?,"We all know $\operatorname{abs}$ is not differentiable in a classical sense, but one question that's always bothered me is, why not define the derivative as the average derivative in each direction? i.e., why not say: $$f'(z) = \frac{f'(z^+) + f'(z^-)}{2}$$ Of course, for complex $z$, we can generalize this to $$f'(z) = \int_{-\pi}^{+\pi}\frac{1}{\theta}\lim_{\Delta z \to 0}\frac{f(z + e^{i\theta}\Delta z) - f(z)}{\Delta z}\,d\theta$$ It coincides with the classical definition wherever the original exists, and it has the same useful properties (e.g. linearity) while being more general.    It also seems intuitive. So why not define it this way? Is it not useful? I don't imagine no one's tried it before!","['derivatives', 'calculus']"
1831336,"If a multiple integral is zero over some region, can I say the integrand is zero?","Consider the following problem Let the integral of a real function of 3 real variables $F(x,y,z)$ over some volume $V$ of $R^{3}$ vanish, $\int$$\int$$\int$$dxdydz$ $F(x,y,z)$$=0$ Now assume this continues to hold for any finite volume $V$ contained in a larger volume $V'$ of $R^{3}$. Can I say the integrand is necessarily zero over $V'$? This seems to be valid for definite integrals on a line (although I'm also not sure), and I feel the function would have to vanish, but I don't see how to prove this.","['multivariable-calculus', 'multiple-integral', 'calculus']"
1831340,Why is $\lim_{x\to \infty} x(\sqrt{x^2+1} - x) = 1/2$,"I've been doing some calculus problems lately out of an old Russian book, and I came across something I didn't fully understand: One of the problems said that $$\lim_{x\to \infty} x(\sqrt{x^2+1} - x) = \frac{1}{2}$$ 
Could someone please explain to my why this is the case? Thanks a lot.","['limits-without-lhopital', 'calculus', 'limits']"
1831358,Finding the ACF of a VAR(1) model,"I want to know the first order autocorrelation of a VAR(1) model. That can be calculated as follows $\rho(1)=\frac{\gamma(1)}{\gamma(0)}$ where $\gamma(1)$ denotes the covariance and $\gamma(0)$ denotes the variance. A VAR(1) model can be described as follows: $$r_t=c+\Gamma r_{t-1}+\varepsilon_t$$ with $\varepsilon_t \sim N(0,\Sigma)$. Furthermore, $r_t$, $r_{t-1}$ and $\varepsilon_t$ all are a $N\times 1$ vector and the parameter matrix $\Gamma$ is a $N\times N$ matrix. Taking the variance of $r_t$ gives us $\gamma(0) = Var(r_t) = Var(c+\Gamma r_{t-1}+\varepsilon_t)=\Gamma\times Var(r_{t-1})\times \Gamma' + \Sigma$. So this gives us $\gamma(0) = \Gamma\times \gamma(0)\times \Gamma' + \Sigma$. For $\gamma(1)$ I get $\gamma(1)=cov(r_t,r_{t-1})=cov(c+\Gamma r_{t-1}+\varepsilon_t,r_{t-1})=\Gamma cov(r_{t-1},r_{t-1})=\Gamma \gamma(0)$. I just do not know how to implement the last part, i.e. $\rho(1)=\frac{\gamma(1)}{\gamma(0)}$, because of the matrices. I hope someone can help me with that? I do know that the $\rho(1)$ of an AR(1) model is simply $\phi$ (i.e. the parameter of the lagged variable in an AR(1) model, see Finding the ACF of AR(1) process for instance). For its multivariate case, I also want to know the autocorrelation coefficient, hence this question.","['time-series', 'statistics', 'vector-auto-regression', 'probability']"
1831359,Finite Borel measure on a compact Metric Space,"Suppose that $\mu$ is a finite Borel measure on a compact metric space $X$ and that $\mu(\{x\}) = 0$ for all $x\in X$. Show that for every $\epsilon > 0$ there is a $\delta > 0$ such that for all $x\in X$, $\mu(B(x,\delta))<\epsilon$ where $B(x,\delta)$ denotes the ball of radius $\delta$ centered at $x$. Attempted proof - Let $\epsilon > 0$ and suppose there is $\delta > 0$ such that there is an ball $B(x,\delta)$ of radius $\delta$ centered at $x$. Further, suppose that $\mu$ is a finite Borel measure on a compact metric space $X$ and $\mu(\{x\}) = 0$. Then from continuity from above $$\lim_{\delta\rightarrow 0}B(x,\delta) = 0$$ Thus there exists a $\delta_x$ such that $B(x,\delta_x)<\epsilon$. Now, since $X$ is a compact metric space, we can choose points $x_1,\ldots, x_n$ such that $X\subset \bigcup_{1}^{n}B(x_j,\delta_{x_j/2})$. Let $\delta = \min\{\delta_{x_1/2},\ldots,\delta_{x_n/2}\}$ and suppose $x\in X$. Then for some $j$, $x\in B(x_j,\delta_{x_j/2})$ thus $B(x,\delta)\subset B(x_j,\delta_{x_j})$. Applying monotonicity we have $$\mu(B(x,\delta))\leq \mu(B(x_j,\delta_{x_j}) < \epsilon$$ Not sure if this is correct any suggestions is greatly appreciated.","['real-analysis', 'measure-theory']"
1831367,$\lim_{x\to \infty} \ln x=\infty$,I'm reading the following reasoning: Since $\underset{n\to \infty}{\lim}\ln 2^n=\underset{n \to \infty}{\lim}n\cdot(\ln 2)=\infty$ then necessarily $\underset{x\to \infty}{\lim}\ln x =\infty$. I don't understand how the generalisation was done from $\lim_{n\to \infty}\ln 2^n=\infty$ to $\lim_{x\to \infty} \ln x=\infty$. Thank you for your help!,"['logarithms', 'calculus', 'limits']"
1831373,Differential equation questions,I am studying differential equations of order $1$ and $2$ and I had these questions on my mind: 1. Is it true that every differential equation has infinitely many solutions if there are no initial conditions specified? 2. Is it true or false that the solution of every $2$nd order differential equation always has $2$ independent constants? Thank you for helping me out :),"['multivariable-calculus', 'ordinary-differential-equations']"
1831401,How do you prove that $\{ Ax \mid x \geq 0 \}$ is closed?,"Let $A$ be a real $m \times n$ matrix. How do you prove that $\{ Ax \mid x \geq 0, x \in \mathbb R^n \}$ is closed (as in, contains all its limit points)? The inequality $x \geq 0$ is interpreted component-wise. This fact is used in some proofs of Farkas's lemma.  It seems like it should be easy, but the proof I've seen seems to be unexpectedly complicated.  Is there a very clear / easy / obvious proof of this fact? (Note that linear transformations do not always map closed sets to closed sets, as discussed in this question .  For example, let $S = \{ (x,y) \in \mathbb R^2 \mid y \geq e^x \}$ and let $T:\mathbb R^2 \to \mathbb R^2$ such that $T(x,y) = (0,y)$.  Then $S$ is closed, but $T(S)$ is not closed.) Edit: Here is a simple proof in the case where $A$ has full column rank.  (A very similar proof is given in Nocedal and Wright, in the Notes and References at the end of chapter 12.)
Let $y^*$ be a limit point of $\Omega = \{ Ax \mid x \geq 0, x \in \mathbb R^n \}$.  There exists a sequence $(x_i)_{i=1}^\infty$ of points in $\mathbb R^n$ such that $x_i \geq 0$ for all $i$ and $A x_i \to y^*$ as $i \to \infty$.  Let $B$ be a left inverse for $A$. Then $B A x_i \to B y^*$ as $i \to \infty$.  In other words, $x_i \to x^*$ as $i \to \infty$, where we have defined $x^* = B y^*$.  Clearly, $x^* \geq 0$ and $A x^* = y^*$. This shows that $y^* \in \Omega$. (Alternatively, you could just note that if $A$ has full column rank then the mapping $x \mapsto Ax$ is a homeomorphism between $\mathbb R^n$ and $R(A)$, so it maps closed sets to closed sets.  This shows that $\Omega$ is a closed subset of $R(A)$, and it follows that $\Omega$ is a closed subset of $\mathbb R^m$.)","['general-topology', 'real-analysis', 'convex-optimization', 'convex-analysis']"
1831423,"Let $f_n$ integrable in $[a,b]$ for all $n$. Show that if $(f_n)\to f$ uniformly in $[a,b]$ then $f$ is integrable in $[a,b]$","I want a check of this proof because I'm not completely sure about the manipulation in some inequalities. Let $f_n$ integrable in $[a,b]$ for all $n$ . Show that if $(f_n)\to f$ uniformly in $[a,b]$ then $f$ is integrable in $[a,b]$ Using Darboux upper and lower sums if $f_n$ is integrable in $[a,b]$ this mean $$\forall \varepsilon>0,\exists P\in\mathcal P: \sum_{j=1}^{H} (M_j-m_j)\Delta x_j<\varepsilon\tag{1}$$ where $M_j$ and $m_j$ are the supremum and infimum of $f_n(x)$ in $[x_j,x_{j+1}]$ , and where $\mathcal P$ is the set of all partitions of $[a,b]$ . And we have that if $f(x)\to f$ uniformly in $[a,b]$ then $$\forall\varepsilon>0,\exists N\in\Bbb N,\forall x\in[a,b]:|f_n(x)-f(x)|<\varepsilon,\,\forall n\ge N\tag{2}$$ Then due to $(2)$ there exists $N$ such that $|f_n(x)-f(x)|<\frac{\varepsilon}{3(b-a)},\forall n\ge N$ and $\forall x\in[a,b]$ . And due to $(1)$ for $\frac{\varepsilon}{3}$ and $f_n$ exists some $P_{\varepsilon,n}\in\mathcal P$ such that $$\sum_{j=1}^{H} (M_{n,j}-m_{n,j})\Delta x_j <\frac{\varepsilon}{3}$$ If $\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j$ is the difference between upper and lower sum of $f$ using the partition $P_{\varepsilon,n}$ then $$\sum_{j=1}^{H} (M_{n,j}-m_{n,j})\Delta x_j - \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j+\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3}$$ $$\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3}-\sum_{j=1}^{H} (M_{n,j}-m_{n,j})\Delta x_j + \sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j$$ $$\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \sum_{j=1}^{H} ((M_{j}-M_{n,j})+(m_{n,j}-m_{j}))\Delta x_j$$ $$\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \sum_{j=1}^{H} (|M_{j}-M_{n,j}|+|m_{n,j}-m_{j}|)\Delta x_j$$ $$\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \sum_{j=1}^{H} \frac{2\varepsilon}{3(b-a)}\Delta x_j=\frac{\varepsilon}{3} + \frac{2\varepsilon}{3(b-a)}\sum_{j=1}^{H}\Delta x_j$$ $$\sum_{j=1}^{H} (M_{j}-m_{j})\Delta x_j<\frac{\varepsilon}{3} + \frac{2\varepsilon}{3(b-a)}(b-a)=\varepsilon$$ Then for $f$ exists partitions where the difference between upper and lower sum is arbitrarily close to zero, so $f$ is integrable in $[a,b]$ .","['real-analysis', 'proof-verification']"
1831441,Show that $a(n) = (1/n)^{1+1/n}$ is monotonically decreasing,"$a(n)$ tends to $0$, as $n$ tends to $\infty$, but I am having trouble showing $a(n) > a(n+1)$. 
I tried to use ln (n+1) - ln n >= 1/(n+1). so ln (n+1) >= ln n +1/(n+1) => 1/(e^(ln n + 1/(n+1)) >= 1/(e^ln (n+1)) => 1/(e^ln n)^(1+1/(n+1)) >= 1/(e^(ln n + 1/(n+1))^(1+1/(n+1)) >= 1/(e^ln (n+1))^(1+1/(n+1)). I think. 
I tried some additional ideas like multiplying both sides by 1/(e^ln n)^(1/(n^2 + n))
 but could not get the inequality I needed.","['sequences-and-series', 'calculus']"
1831463,How is it that $(2^k+1)3 \gt (2^k+1)2\gt 2^{k+1}+2\gt 2^{k+1}+1$?,"I am working on a mathematical induction worksheet, and my professor gave us the key. I have run across something that makes zero sense to me, so please explain if you can. Additional info: $k \ge2$ Here is what she has: What needs to be shown: $$ 3^{k+1} \gt 1 +2^{k+1} \tag0$$ $$3^{k+1} = 3^k(3) \gt (2^k+1)3 \tag1$$ This is where I am confused.. $$ \gt (2^k+1)2 \tag2$$ $$ \gt 2^{k+1}+2 \tag3$$ $$ \gt 2^{k+1}+1 \tag4$$","['algebra-precalculus', 'induction', 'discrete-mathematics']"
1831478,Prove that $f$ is invertible,"Did I show enough to prove $f$ is invertible? Alternatively is there a more efficient way to do so? Thanks in advance for any help. Let $f : X \rightarrow Y $a nd $g : Y \rightarrow X$ be functions. Assume $g\circ f$ is injective and
  $f\circ g$ is surjective. Prove that $f$ is invertible i)First assume $a, b \in X$ and $f(a) = f(b)$. Then $(g \circ f)(a) = g(f(a)) = g(f(b)) =(g \circ f)(b)$. However, $g \circ f$ is injective. Therefore $a = b$. ii)Next assume $a \in X$. Since $f \circ g$ is surjective. Then there is $b\in Y$ such that $(f \circ g)(b) = a$. Set $c = g(b)$. Then $f(c) = f(g(b)) = a$ and $a \in Range(f)$. Since $a$ is arbitrary it follows that $Range(f) = X$. Thus since $f$ is both injective and surjective it is bijective and equivalently invertible.","['functions', 'proof-verification']"
1831484,"Real Analysis Folland, Proposition 2.6 Measurable functions","Question: Proposition 2.6 - If $f,g: X\rightarrow \mathbb{C}$ are $M$ -measurable, then so are $f + g$ and $fg$ . Attempted proof/brainstorm - Suppose $f,g: X\rightarrow \mathbb{C}$ are $M$ -measurable. We want to show that $f + g\in M$ and $fg\in M$ . Since $f^{-1}$ and $g^{-1}$ preserves unions, intersections, and complements, is there a way of using Proposition 2.3 to show that $f+g$ and $fg$ are $M$ -measurable? I don't really understand Folland's approach. Any suggestions is greatly appreciated. Background information: Proposition 2.3 -  If $(X,M)$ is a measurable space and $f:X\rightarrow \mathbb{R}$ , the following are equivalent: a. $f$ is $M$ -measurable b. $f^{-1}((a,\infty))\in M \ \ \forall a\in\mathbb{R}$ c. $f^{-1}([a,\infty))\in M \ \ \forall a\in\mathbb{R}$ d. $f^{-1}((-\infty,a))\in M \ \ \forall a\in\mathbb{R}$ e. $f^{-1}((-\infty,a])\in M \ \ \forall a\in\mathbb{R}$","['real-analysis', 'measure-theory']"
1831485,"Determine whether or not the poset $(\mathbb{N}, \propto)$ has a least element","Define a relation $\propto$ on the natural numbers $\mathbb{N}$ by declaring that for $x, y \in \mathbb{N}$ , $x \propto y \iff (x=y)$ or $(3x \leq y)$ a) Show that $\propto$ is a partial order on $\mathbb{N}$ b) Determine whether or not the subset $\{2, 6, 8\}$ of $\mathbb{N}$ is totally ordered with respect to $\propto$ . Explain your answer. c) Determine whether or not the poset $(\mathbb{N}, \propto)$ has a least element. For part a), I showed that it was reflexive, transitive and antisymetric. For part b) I determined that the subset was NOT totally ordered because 6 $\not \propto 8$ and $8 \not \propto 6$ I'm unsure about part c). My guess is that there is no least element because $1 \not \propto 2$ so $1$ cannot be the least element. However $\forall n \in \mathbb{N}$ with $n>1$ , $n \not \propto 1$ , so there is no number $n \not = 1$ which is the least element. But I'm sure I'm phrasing this really badly...","['order-theory', 'relations', 'discrete-mathematics']"
1831498,"What are all the different classes of functions upon real numbers and what do they mean, exactly? [closed]","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question I have been hearing terms like ""piecewise C1"", ""continuous"", ""linear"", ""piecewise constant"", ""trigonometric"", ""logarithmic"", ""exponential"", ""elementary"", etc. functions for many years. I know what most of those terms mean (I do not know the meaning of the first one). However, what are the major branches of functions, and what do they mean? I don't expect people to list all of the minor one's or the one's that everyone study up through basic calculus, mind you. That would be incredibly excessive. I am more interested in the large subsets relating to continuity and calculus. However, referring to all function branches: Are they all references to the sort of graphs they give or are there special classes based upon other kinds of properties such as recursive definitions? The idea is what do the types mean. Sometimes they seem to refer to the functions used to define and express them. Other times it seems to be abstract graphical concepts? Are there certain branches of types of functions? How do they all connect in terms of classification? This part of the question does not neccessarily have to be answered, but if one can it would be very nice in a visual sort of way of connecting things together... Note: Just so people are aware, I know what the definition of a function is. This question is about how they are classified , and what properties are looked at and used when classifying.","['terminology', 'functions']"
1831523,"Is $0\in (\bigcap\limits_{n=3}^{\infty}([0,1]-[1/n,1-1/n]))$","At first I thought no since $\lim\limits_{n\to \infty}1/n=0$. But then I checked that $0\in ([0,1]-[1/n,1-1/n]) $ for any positive integer $n\geq 3$. So it is in every set. Which is the exact opposite of what I thought. Edit: sorry people meant intersection",['elementary-set-theory']
1831584,A question on mapping inside unit disc,"For an analytic function $f:\bar D→\bar D$ where $\bar D$ is the closed unit disc centered at origin.Suppose $\bar D=D\cup$$\delta D$, where $\delta D$ is the boundary of open disc $D$ and $f$ is onto, then is it true that $f(\delta D)=\delta f(D)$ i.e. boundary of domain maps onto the boundary of image? My attempt: I think the answer is yes. I took $f(z)$=$(z-a)/(1-$$\bar a z$) where $a\in D$ which gives $|f(exp(i\theta)|=1$.","['complex-analysis', 'analytic-functions', 'functions']"
1831585,How to obtain this factorization of $x^4+4$?,"$x^4 + 4 = (x^2 + 2x +2)(x^2 - 2x +2)$ I am curious how would one obtain this factorization? Clearly, once the factorization is known it is routine to verify it, however the hard part is how to find the factorization in the first place? Thanks! I observed that $(A+B)(A-B)=A^2-B^2$ can be applicable here with $A=x^2+2$, $B=2x$. Is there any other trick?","['algebra-precalculus', 'polynomials']"
1831606,Is the plane minus a line segment homeomorphic with punctured plane?,"Is $\mathbb R^2$ minus a line segment i.e. $\mathbb R^2 \setminus ([0,1]\times \{0\}) $ homeomorphic with a punctured plane $\mathbb R^2\setminus \{(0,0)\}$ ?","['continuity', 'general-topology', 'metric-spaces']"
1831632,Test function with bounded gradient,How to construct a test function (radial) which is zero outside a ball of radius $2r$ and $1$ on the ball of radius $r$ but the gradient is bounded by $\dfrac{1}{r}$ ?,"['analysis', 'partial-differential-equations']"
1831653,"Bayes Theorem problem, from Finan #9.4: $P(A\mid B ∩ C)$","The Problem: You are given $\Pr(A) = 2/5, \Pr(A ∪ B) = 3/5, \Pr(B\mid A) = 1/4, \Pr(C\mid B) = 1/3,$ and
$\Pr(C\mid A ∩ B) = 1/2$. Find $\Pr(A\mid B ∩ C)$. My work: I know that $\Pr(A\mid B) \Pr(B) = \Pr(B\mid A) \Pr(A) = \Pr(A \cap B)$ . To solve the problem I need to solve   $\Pr(A\mid B ∩ C) = \frac{\Pr(A \cap B \cap C) }{\Pr(B \cap C)}$ I worked out that $\Pr(B \cap C) = \Pr(C\mid B) \Pr(B) = 1/3 \cdot \Pr(B) = \Pr(B\mid C)\Pr(C)$ So, now I want to find either $\Pr(B)$ or $\Pr(B\mid C)$ & $\Pr(C)$. Then, I found that $\Pr(A \cap B \cap C) = 1/20$ by working out that $\Pr(C\mid A \cap B) = \frac{\Pr(A \cap B \cap C) }{\Pr(A \cap B)} = 1/2$ I've been trying to go further than this, but end up with no new information. I'm not sure how knowing $\Pr(A ∪ B) = 3/5$ would help. Any suggestions?","['bayes-theorem', 'probability', 'actuarial-science']"
1831666,Compound interest: why does everyone get it wrong?,"The compound interest formula is:
$$A_t=A_0(1+r)^t$$
There is a simple derivation for this which works by starting with $A_1$ and then considering $A_2$ and then extrapolating. The above formula can be manipulated to solve any related problem we have. Rates are almost always stated as APR's, which once stated are a legal obligation so daily or monthly rates must be derived from the APR. This can be done thus:
$$(\frac{A_t}{A_0})^{1/t}-1=r$$
So, for example, if the APR is 4 and we want the monthly rate we can say:
$$1.04^{1/12}-1=0.003274$$
since we know that $A_t/A_0=1.04$ and if we want to work out the amount after five months from a deposit of $300$ we can say:
$$\frac{A_t}{A_0}A_0=(r+1)^tA_0=(1+0.04)^{5/12}\times300=304.942867$$
Crucially, this is less than $305.033445$ which is what we would get if we set the monthly rate at $0.04/12$ which is how it's done in the formula:
$$A_t=A_0(1+\frac{r}{n})^t$$
This formula, which is most usually given in books and websites, gives a total of $312.222463$ at $t=12$ rather than $312$. The commonly used formula can therefore not possibly be correct ! Yet no one seems too bothered. Everyone just goes on using the wrong formula getting misleading results. Why? What can be done to change the situation?","['algebra-precalculus', 'finance']"
