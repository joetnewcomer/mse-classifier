question_id,title,body,tags
2083685,newton's binomial,"Formulate Newton's binomial's solution and, using that, deduce that this formula is true: 
$${n\choose 0} + {n\choose 1} + \cdots + {n\choose n-1}+{n\choose n} = 2^n$$ I know that Newton's binomial is 
$$(a+b)^n={n\choose 0}a^n + {n\choose 1}a^{n-1}b +\cdots +{n\choose n-1}ab^{n-1} + {n\choose n}b^n$$ Can anybody help me with the second part? 
Thank you",['discrete-mathematics']
2083723,Prove that $\frac{bc}{a(a-b)(a-c)}+\frac{ca}{b(b-c)(b-a)}+\frac{ab}{c(c-a)(c-b)}=\frac{ab+bc+ca}{abc}$,"Prove that $$\frac{bc}{a(a-b)(a-c)}+\frac{ca}{b(b-c)(b-a)}+\frac{ab}{c(c-a)(c-b)}=\frac{ab+bc+ca}{abc}$$ To do this I unified three terms on the left side with a common denominator and then factored the numerator (with the aid of Wolfram Alpha... as the numerator is a 5th order expression).
$$\frac{b²c²(c-b)+c²a²(a-c)+a²b²(b-a)}{abc(a-b)(b-c)(c-a)}=\dots=\frac{(ab+bc+ca)(a-b)(b-c)(c-a)}{abc(a-b)(b-c)(c-a)}$$ My question is would there be a better or easier way to lead from the left side to the right side? Actually, the original question was to 'simplify' the left side without showing the right side. I am not sure if I could do so without Wolfram Alpha. So that's why I'm asking an easier way.
What I've thought of was, for example,
$$f(x):=(x-a)(x-b)(x-c)$$
$$\text{LHS}=abc\left(\frac1{a^2f'(a)}+\frac1{b^2f'(b)}+\frac1{c^2f'(c)}\right)$$
which didn't help.",['algebra-precalculus']
2083733,Using Geodesics to show that every direction is an eigenvector of Shape Operator,"Suppose that every geodesic of a connected surface $M \subset \mathbb{R}^3$ has positive curvature $\kappa > 0$ and is contained in a plane. a. Prove that every direction $T_p M$ is an eigenvector of for the
      shape operator $S_p$, for all $p \in M$ b. Prove that $M$ is
      contained in a sphere. My starting work is that I know $\alpha $ is geodesic iff $\alpha''(s)$ is colinear to $U(\alpha(s))$ for each $s$ in the domain of $\alpha$ and $U$ is the unit normal to $M$.  This then implies that $\alpha''(s) \cdot \alpha'(s) = 0$ and $\alpha''(s) \cdot U'(\alpha'(s)) =0$ as $\alpha'(s), \ U'(\alpha'(s)) \in T_{\alpha(s)} M $.  Finally, (I think) $\alpha$ contained in a plane implies that $U(\alpha(s))$ is constant.  I would then jump to say that $U'(\alpha'(s)) = 0$.  Is this a correct path to take? For a., I know that I want to show $k_1 = k_2$, where $k_1$ and $k_2$ are the principal directions of $M$ and are the eigenvalues of $S_p$.  Is this the","['differential-geometry', 'surfaces', 'geometry']"
2083742,Let $f\in Hol(\Bbb{C}\setminus \Bbb{R})\cap C(\Bbb{C})$ then $\int_{\gamma}f(z)dz=0$ for all closed curve $\gamma$,For $f\in Hol(\Bbb{C}\setminus \Bbb{R})\cap C(\Bbb{C})$ show that $\int_{\gamma}f(z)dz=0$ for all closed  curve $\gamma$ piecewise-continuous. I am completely clueless but this is primarily because I feel hopeless regarding the winding numbers. Can you give me an advice/guidance?,['complex-analysis']
2083751,Almost sure limit of a martingale process (Kakutani's theorem),"Let $Z_1,Z_2, \ldots$ be independent non-negative random variables defined on a probability space $(\Omega,\mathcal{F},\mathbb{P})$ with $\mathbb{E} Z_n = 1$ for all $n \in \mathbb{N}$ . The process $(M_n)_{n \in \mathbb{N}}$ defined by $M_n = \prod_{i=1}^n Z_i$ is a non-negative martingale. We know that $M_\infty$ exists as an almost sure limit of $M_n$ . We want to prove that $$\mathbb{E}M_\infty = 1 \iff \prod_{k=1}^\infty r_k > 0. \qquad (*)$$ First, we introduce $R_n = Z_n^\frac{1}{2}$ and subsequently, using Jensen's equality, we see that $$(\mathbb{E}R_n)^2 \leq \mathbb{E}R_n^2 = \mathbb{E} Z_n = 1.$$ It follows that $r_n := \mathbb{E}R_n \leq 1$ for all $n \in \mathbb{N}$ , since $R_n = Z_n^\frac{1}{2}$ are non-negative random variables.
Secondly, we let $N$ be the martingale defined by $N_n = \prod_{i=1}^n \frac{R_i}{r_i}$ . In order to show that \begin{align}
\mathbb{E}M_\infty = 1 \iff \prod_{k=1}^\infty r_k > 0, \qquad (*)
\end{align} we first have to show that $N$ is bounded in $\mathcal{L}^2$ and that consequently $M$ is uniformly integrable. However, I face difficulties regarding the elaboration of this last statement and the proof of the equivalence. Any help is appreciated! EDIT: The three statements to prove: $N$ is bounded in $\mathcal{L}^2$ and that subsequently $M$ is uniformly integrable. $\prod_{i=1}^\infty r_k > 0 \implies \mathbb{E}M_\infty =1$ . $\mathbb{E}M_\infty =1 \implies \prod_{i=1}^\infty r_k > 0$ . Proof 1. \begin{align}
\mathbb{E}N^2 &= \mathbb{E}\bigg[ \prod_{i=1}^n \bigg( \frac{R_i}{r_i}\bigg)^2 \bigg]\\
&= \mathbb{E}\bigg[ \prod_{i=1}^n \bigg( \frac{Z_n}{\mathbb{E}[R_n]^2}\bigg) \bigg]\\
\end{align} How to show that the above expression is finite? Jensen's inequality does not seem to work on $\mathbb{E}[R_n]^2$ . And if $N$ is bounded in $\mathcal{L}^2$ , why does this imply that $M$ is U.I.? For the proofs of 2 and 3 I have no suggestions.","['martingales', 'real-analysis', 'convergence-divergence', 'probability-theory']"
2083758,Convex Combination - Two points,"Description // Two points on space
p = [2, 3]
q = [-1, -1]

// Points between the difference of the points above
w = q - p = [-3, -4]

// Which gives us the parametric form
f(x) = (1 - t)[2, 3] + t[-3, -4]

// Sample parameter
// Which seems to overflow w = [-3, -4]
f(2) = -1[2, 3] + 2[-3, -4] = [-8, -11]

// The coefficients sum up to one
-1 + 2 = 1 As far as I can understand convex combination suffices if the sum of the coefficients are equal to 1 (l1 + l2 + ... + li = 1) My Question If it's a convex combination, doesn't it have to be between w = [-3, -4] ?","['vector-spaces', 'linear-algebra', 'geometry']"
2083763,"Let $X$ be standard normal and $a>b>0$, prove that $\lim\limits_{\epsilon\to 0}\epsilon^2\log P(|\epsilon X -a|<b)=-\frac{(a-b)^2}{2}$","Let $X$ be a standard normal random variable, with $a,b>0$ and $a-b>0$,  prove that $$\lim_{\epsilon\to 0}\epsilon^2\log P(|\epsilon X -a|<b)=-\frac{(a-b)^2}{2}$$ I'm studying for a qual and this was a previous problem (problem 6b). From part a, we know $\lim_{\epsilon\to 0} P(|\epsilon X -a|<b)=0$ I tried using L'Hopital's rule: $$\frac{\log P(|\epsilon X -a|<b)}{1/\epsilon^2}=\frac{1}{-2/\epsilon^3}\frac{1}{P(|\epsilon X -a|<b)}\frac{d}{d\epsilon }P(|\epsilon X -a|<b)$$ Then we need to compute $\frac{d}{d\epsilon }P(|\epsilon X -a|<b)$: $$\frac{d}{d\epsilon }P(|\epsilon X -a|<b)=\frac{d}{d\epsilon} \int_{(a-b)/\epsilon}^{(a+b)/\epsilon} \frac{1}{\sqrt{2\pi}}e^{-t^2/2}~dt$$ Here is where I am stuck. Of course I can use fundamental theorem of calculus but it turns into a huge mess. How do I proceed from here?","['large-deviation-theory', 'probability-theory', 'laplace-method', 'normal-distribution']"
2083783,"Prove $D(fg)=D(f)D(g)res(f,g)^2$ where D is the discriminant and res the resultant","Let $R$ be a commutative ring and $R[x]$ the polynomial ring over $R$ . Let $f,g \in R[x]$ . We let $D(fg)$ denote the discriminant of $fg$ and $\operatorname{res}(f,g)$ the respective resultant.
I need to prove that the equation $$D(fg)=D(f)D(g)\operatorname{res}(f,g)^2$$ holds. So far I fiddled around to no avail. My ideas so far are to use the properties of the definition of the Sylvester-Matrix that is used in $D(f)$ and $\operatorname{res}(f)$ and the determinant. The definitions of $D(f)$ and $\operatorname{res}(f,g)$ I am working with are as follows: $D(f)=(-1)^{n(n-1)/2}\operatorname{res}(f,f')$ and $\operatorname{res}(f,g)=\det(S(f,g))$ . $S(f,g)$ is the Sylvester-Matrix of $f$ and $g$ , $f'$ denotes the derivative of $f$ .
Help is much appreciated.","['abstract-algebra', 'algebraic-geometry']"
2083786,Functional equation of the complete $L$-function of the twisted $L$-function of a cuspidal modular form,"Let $f(z)=\sum a(n)n^{(k-1)/2}q^n\in S_k(\Gamma_0(N),\chi)$ a cuspidal modular form of integral weight with nebentypus $\chi.$ I am looking for the expression of $\Lambda(\psi\otimes f,s)$ the complete $L$-function (In the sense of Iwaniec's book Analytic number theory page 94) of the twisted $L$-function of  $f$ by a caracter $\psi\; :$
$$L(\psi\otimes f,s)=\sum_{n\ge1}\frac{a(n)}{n^s}\psi(n)$$
and his functional equation. Can someone give me a reference in which i can find it ? Thanks!","['analytic-number-theory', 'dirichlet-series', 'l-functions', 'number-theory', 'modular-forms']"
2083793,"How to show rigorously that $\Omega=\mathbb{R}^3\backslash\{(0,0,z)\ \mid\ z\in\mathbb{R}\}$ isn't simply connected?","As indicated in the title, I want to show with outmost possible rigor and without using too advanced techniques that $\Omega=\mathbb{R}^3\backslash\{(0,0,z)\ \mid\ z\in\mathbb{R}\}$ isn't simply connected (which intuitively is obvious). Here is my attempt: Suppose $\Omega$ is simply connected, then for $\alpha,\beta:[0,\pi]\to\mathbb{R}^3$, $\alpha:t\mapsto (\cos t,\sin t,0)$, $\beta:t\mapsto(\cos t,-\sin t,0)$ there exists $F\in C^0([0,\pi]\times[0,1],\Omega)$ (we write $F=(F^1,F^2,F^3)$) such that $F(t,0)=\alpha(t)$ and $F(t,1)=\beta(t)$ for all $t\in[0,\pi]$ as well as $F(0,s)=(1,0,0)$ and $F(\pi,s)=(-1,0,0)$ for all $s\in[0,1]$. As for $s\in[0,1]$ fix we have $F^1(0,s)=1$ and $F(\pi,s)=-1$, there exists $f(s)\in (0,\pi)$ such that $F^1(f(s),s)=0$. If we could prove that there exists such an $f:[0,1]\to(0,\pi)$ such that $f$ is continuous, then we see that $F^2(f(0),0)=1$ and $F^2(f(1),1)=-1$ and thus there would have to be an $s\in(0,1)$ such that $F^2(f(s),s)=0$ and thus $F(f(s),s)\notin\Omega$, contradiction. However, I don't see how to prove the global existence of such an $f$ (intuitively it should exist though). By the inversion theorem, we can conclude that such an inverse exists locally around $s=0$ and $s=1$, but I fail to see how to extend this argument.","['general-topology', 'real-analysis', 'multivariable-calculus', 'connectedness']"
2083804,How to verbally state $f(y\mid x)\;?$,"How do we verbally state: $\large f(y\mid x)\;?$ I'm familiar with $f(x)$ as ""$f$ of $x$"" and $f(x,y)$ as ""$f$ of $x$ and $y$"" (or $f$ of $x, y$), but what does the vertical line mean and how to state this verbally so that a screen reader would read it correctly?","['terminology', 'notation', 'functions']"
2083827,Differentiation of a scalar function w.r.t. a vector,"I'd like to know how to take a differentiation w.r.t. a vector as follows:
$$\frac{\partial \log f({\bf x}-{\bf y})}{\partial {\bf y}}$$ 
where $f:{\mathbb R}^p \to {\mathbb R}$ and ${\bf x},{\bf y}\in{\mathbb R}^p$.
My calculation is 
$$\frac{\partial \log f({\bf x}-{\bf y})}{\partial {\bf y}}
=-\frac{f'({\bf x}-{\bf y})}{f({\bf x}-{\bf y})}{\bf I_p}$$
But it should be a vector because this is a differentiation of a scalar function
$f(\cdot)$ with respect to a vector ${\bf y}$. 
Any comments would be appreciated.","['derivatives', 'chain-rule', 'matrix-calculus', 'vector-analysis']"
2083837,When is the union of a family of subspaces of a vector space also a subspace?,"It is not difficult to prove that the union of a chain (or, more generally, a directed family) of subspaces of a vector space $V$ is a subspace of $V$. Given a family $\mathcal{F}$ of subspaces of a vector space $V$ such that the union of $\mathcal{F}$ is a subspace of $V$, is it true that $\mathcal{F}$ is a directed family? If not, is there a ""nice"" characterization of families of subspaces whose union is a subspace?",['linear-algebra']
2083855,Show given property for a sequence $a_{n}$,"The sequence $a_n$ is defined as 
$ a_0$ is an arbitrary real number,
$ a_{n+1}$ = $\lfloor a_{n}\rfloor$ ($a_{n} - \lfloor{a_{n}}\rfloor$) Show that for every $ a_0$: $$\exists m\geq0, \forall n \geq m, a_{n+2}= a_n$$ Floor function $\lfloor x \rfloor$, example, $\lfloor 3.2 \rfloor = 3$ and $\lfloor -3.2 \rfloor = -4$ Here is my attempt: [link] What I have noticed is that due to the floor function denoted as $\lfloor x \rfloor$ all of these sequences will approach zero. I am not sure if this sequences has a divergent property of periodically switching between a few particular elements, but maybe. Though why I think it approaches zero: Let $ a_0= 16.2 \Rightarrow 
a_1 = 16 (16.2 - 16) = 16 \cdot 0.2 = 3.2 \Rightarrow 
a_2 = 3 (3.2 - 3) = 3 \cdot 0.2 = 0.6 \Rightarrow 
a_3 = 0 (0.6 \cdot 0) = 0$","['sequences-and-series', 'discrete-mathematics']"
2083888,Polar transformation of a probability distribution function,"I am working through Dirk P Kroese ""Monte Carlo Methods"" notes with one section based on Random Variable Generation from uniform random numbers using polar transformations (section 2.1.2.6). The polar method is based on the polar coordinate transformation $ X=R \cos \Theta$, $Y=R\sin \Theta$, where $\Theta \sim \text{U}(0,2\pi)$ and $R\sim f_R$ are independent. Using standard transformation rules it follows that the joint pdf of $X$ and $Y$ satisfies: $$f_{X,Y}(x,y)=\cfrac{f_R(r)}{2\pi r}$$ with $r=\sqrt{x^2+y^2}$. I don't fully understand how expression  $f_{X,Y}(x,y)$ is obtained from ""standard transformation rules"", please help or hint.","['probability-theory', 'probability', 'transformation', 'polar-coordinates']"
2083894,Why can't I take the derivative of $x^x$ as $x(x^{x-1})$?,Why can't I take the derivative of $x^x$ as  $x(x^{x-1})$? I don't understand why I have to convert it to $e^{x\ln(x)}$ first.,"['derivatives', 'chain-rule', 'calculus']"
2083896,Meaning of a monotone sequence of fucntions?,"Dini's theorem: if a monotone sequence of continuous functions converges pointwise on
  a compact space and if the limit function is also continuous, then the
  convergence is uniform. What's the meaning of monotone sequence of functions ? How can a sequence of functions be monotone? If I have a sequence $f_n(x)$, this means that $f_1(x) \le f_2(x) \le f_3(x) \le ... \le f_n(x)$ (Assume increasing) for every $x$? Edit: Does that mean that for all $x$, $f_1(x) \le f_2(x_1)$ for any $x_1$ or that for all $x$, we fix $x$ and then $f_1(x) \le f_2(x)$? There is probably a definiton out there, I just couldn't find. Please clear this up for me.","['monotone-functions', 'uniform-convergence', 'convergence-divergence', 'functions']"
2083906,Injecting a group into itself,"Let $G$ be a group with elements $g_1, g_2\in G$  and injective endomorphisms $\phi_1, \phi_2$  s.t.  $\phi_1 (g_1)=g_2$ and $\phi_2(g_2)=g_1$. Does this imply there is an automorphism $\psi$ with $\psi(g_1)=g_2$. I expect the answer to be no but I'm unsuccessful producing a counter example.","['group-homomorphism', 'infinite-groups', 'group-theory']"
2083916,How to solve $8n^2=64n\cdot \log_2(n)$?,"How would you be able to solve for n in this setup? $$8n^2=64n\cdot \log_2(n)$$ I tried tons of online step-by-step solutions but the only one who got it right was wolfram, but I can't watch the step-by-step solution. Any help will be appreciated!","['algebra-precalculus', 'logarithms']"
2083927,Deriving a Series Representation of an Integral (Riemann Zeta Function),"It is known that $$\int_0^\infty\frac{x^n}{e^x-1}dx=n!\zeta(n+1)$$ for integer values of $n$ (this is also generalises, but that's not important for this question). I have also had a look at how to arrive at this expression, starting from the series representation of the Riemann Zeta function. However, just out of interest, I've tried to see if I could manage to derive the series representation for the case that sparked my interest in the first place $(n=3)$ on my own - that is, working backwards from this formula to the series that initially defines the Riemann Zeta function. I did manage to arrive at a series representation, but it's not the correct series. Here's what I did: \begin{align}
\int_0^\infty\frac{x^3}{e^x-1}dx&=\int_0^\infty\frac{x^3(e^x+1)}{e^{2x}-1}dx\\
&=\int_0^\infty\frac{x^3}{e^{2x}-1}dx+\int_0^\infty\frac{x^3e^x}{e^{2x}-1}dx\\
&=\frac{1}{16}\int_0^\infty\frac{x^3}{e^x-1}dx+\int_0^\infty\frac{x^3e^x}{e^{2x}-1}dx
\end{align}
Subtracting the first term on the RHS, we have
\begin{align}
\frac{15}{16}\int_0^\infty\frac{x^3}{e^x-1}dx=\int_0^\infty\frac{x^3e^x}{e^{2x}-1}dx
\end{align}
Multiplying across the fraction, we have
\begin{align}
\int_0^\infty\frac{x^3}{e^x-1}dx&=\frac{16}{15}\int_0^\infty\frac{x^3e^x}{e^{2x}-1}dx\\
&=\frac{1}{15}\int_0^\infty\frac{x^3e^{\frac{x}{2}}}{e^x-1}dx
\end{align} Letting $z:=e^x$, then $x=\log z\implies dx=\frac{dz}{z}$. \begin{align}
\int_0^\infty\frac{x^3}{e^x-1}dx&=\frac{1}{15}\int_1^\infty\frac{\log^3z\sqrt{z}}{z(z-1)}dz
\end{align} Letting $u:=\frac{1}{z}$, we have $z=\frac{1}{u}\implies dz=-\frac{du}{u^2}$, $u(1)=1$ and $u(z\rightarrow\infty)=0$. Thus \begin{align}
\int_0^\infty\frac{x^3}{e^x-1}dx&=\frac{1}{15}\int_0^1\frac{\log^3\left(\frac{1}{u}\right)\sqrt{u}}{u^2}\frac{1}{1-\frac{1}{u}}du\\
&=-\frac{1}{15}\int_0^1\frac{\log^3u\sqrt{u}}{u}\frac{1}{u-1}du\\
&=\frac{1}{15}\int_0^1\frac{\log^3u}{\sqrt{u}}\frac{1}{1-u}du\\
&=\frac{1}{15}\int_0^1\frac{\log^3u}{\sqrt{u}}\sum_{k=0}^\infty u^k du\\
&=\frac{1}{15}\sum_{k=0}^\infty\int_0^1u^{k-\frac{1}{2}}\log^3u du
\end{align} Letting $t:=\log u$, then $u=e^t\implies du=e^tdt$, $t(u\rightarrow 0)=-\infty$ and $t(1)=0$. \begin{align}
\int_0^\infty\frac{x^3}{e^x-1}dx&=\frac{1}{15}\sum_{k=0}^\infty\int_{-\infty}^0 t^3e^{\left(k+\frac{1}{2}\right)t}dt
\end{align} Letting $p:=-t$, then $dt=-dp$, $p(t\rightarrow -\infty)=\infty$ and $p(0)=0$. \begin{align}
\int_0^\infty\frac{x^3}{e^x-1}dx&=\frac{1}{15}\sum_{k=0}^\infty\int_0^\infty p^3e^{-\left(k+\frac{1}{2}\right)p}dp
\end{align} Letting $s:=\left(k+\frac{1}{2}\right)p$, then $dp=\frac{ds}{k+\frac{1}{2}}$. The bounds of integration are unchanged, and thus \begin{align}
\int_0^\infty\frac{x^3}{e^x-1}dx&=\frac{1}{15}\sum_{k=0}^\infty\frac{1}{\left(k+\frac{1}{2}\right)^4}\int_0^\infty s^3e^{-s}ds\\
&=\frac{\Gamma(4)}{15}\sum_{k=0}^\infty\frac{1}{\left(k+\frac{1}{2}\right)^4}\\
&=\frac{2}{5}\sum_{k=0}^\infty\frac{1}{\left(k+\frac{1}{2}\right)^4}
\end{align} EDIT: Apparently, this is simply a convoluted way of deriving the correct answer, as the series evaluates to $\frac{\pi^4}{6}$, making the final expression equal to the known value of $$\int_0^\infty\frac{x^3}{e^x-1}dx = 6\sum_{k=1}^\infty\frac{1}{k^4} = \frac{\pi^4}{15}$$ Many thanks to Simple Art for verifying this result!","['riemann-zeta', 'integration', 'definite-integrals', 'sequences-and-series']"
2083938,"Probs. 24 (c) and (e), Chap. 3, in Baby Rudin: Completion of a metric space","Here is Prob. 24, Chap. 3, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $X$ be a metric space. (a) Call two Cauchy sequences $\left\{ p_n \right\}$ , $\left\{ q_n \right\}$ in $X$ equivalent if $$ \lim_{n \to \infty} d \left( p_n, q_n \right) = 0.$$ Prove that this is an equivalence relation. [ This is easy.] (b) Let $X^*$ be the set of all equivalence classes so obtained. If $P \in X^*$ , $Q \in X^*$ , $\left\{ p_n \right\} \in P$ , $\left\{ q_n \right\} \in Q$ , define $$ \Delta (P, Q) = \lim_{n \to \infty} d \left( p_n, q_n \right); $$ by Exercise 23, this limit exists. Show that the number $\Delta (P, Q)$ is unchanged if $\left\{ p_n \right\}$ and $\left\{ q_n \right\}$ are replaced by equivalent sequences, and hence that $\Delta$ is a distance function in $X^*$ . [ I don't have any problem with that either.] (c) Prove that the resulting metric space $X^*$ is complete. [ How to do this? ] (d) For each $p \in X$ , there is a Cauchy sequence all of whose terms are $p$ ; let $P_p$ be the element of $X^*$ which contains this sequence. Prove that $$ \Delta \left( P_p, P_q \right) = d(p, q) $$ for all $p, q \in X$ . In other words, the mapping $\varphi$ defined by $\varphi (p) = P_p$ is an isometry (i.e. a distance-preserving mapping) of $X$ into $X^*$ . [ This isn't a problem either. ] (e) Prove that $\varphi(X)$ is dense in $X^*$ , and that $\varphi(X) = X^*$ if $X$ is complete. [ How to? ] By (d), we may identify $X$ and $\varphi(X)$ and thus regard $X$ as embedded in the complete metric space $X^*$ . We call $X^*$ the completion of $X$ . My effort: Prob. 24 (c): Let $\left\{ P_k \right\}$ be a Cauchy sequence in $X^*$ . Then, given any $\varepsilon > 0$ , we can find a natural number $K$ such that $$\Delta \left( P_k, P_r \right) < \varepsilon$$ for all $k, r \in \mathbb{N}$ such that $k > K$ and $r > K$ . For each $k \in \mathbb{N}$ , let $\left\{ p_{kn} \right\}$ be an element of $P_k$ . Then, for each $k \in \mathbb{N}$ , the sequence $\left\{ p_{kn} \right\}$ is a Cauchy sequence in $(X, d)$ and also $$ \lim_{n \to \infty } d \left( p_{kn}, p_{rn} \right) < \varepsilon$$ for all $k, r \in \mathbb{N}$ such that $k > K$ and $r > K$ . What next? Prob. 24 (e): Let $P \in X^*$ and let $\varepsilon > 0$ be given. We need to find an element $x \in X$ such that $$ \Delta (P, \varphi(x) ) < \varepsilon.$$ Let $\left\{ p_n \right\}$ be a Cauchy sequence in $P$ . Then there exists a natural number $N$ such that $$ d \left( p_m , p_n \right) < \frac{\varepsilon}{2} $$ for all $m, n \in \mathbb{N}$ such that $m > N$ and $n > N$ . Let $P_{p_{N+1}}$ be the element of $X^*$ that contains the Cauchy sequence $$ \left\{ p_{N+1}, p_{N+1}, p_{N+1}, \ldots \right\}.$$ Then we see that $$ \Delta \left( P, P_{p_{N+1}} \right) = \lim_{n \to \infty} d \left( p_n, p_{N+1} \right) \leq \frac{\varepsilon}{2} < \varepsilon. $$ And, $P_{p_{N+1}} = \varphi(p_{N+1})$ . Is this argument correct? If not, then where lies the flaw? What it the correct proof? An afterthought: Here's a proof of Prob. 24 (c) that I propose. Let $\left\{ P_k \right\}_{k \in \mathbb{N}}$ be a Cauchy sequence in $X^*$ . Then, given a real number $\varepsilon > 0$ , we can find a natural
number $K$ such that $$ \Delta \left( P_k, P_r \right) < \frac{\varepsilon}{2}$$ for all $r, k \in \mathbb{N}$ such that $r > 
K$ and $k > K$ . For each $k \in \mathbb{N}$ , since $P_k \in X^*$ , each $P_k$ is an equivalence class of Cauchy sequences in $X$ . Let $\left\{ p_{kn} \right\}_{n \in \mathbb{N}}$ be a Cauchy sequence in the equivalence class $P_k$ , for each $k \in \mathbb{N}$ . Then we can conclude that $$ \Delta \left( P_k, P_r \right) = \lim_{n \to \infty} d \left( p_{kn}, 
p_{rn} \right) < \frac{\varepsilon}{2}$$ for all $r, k \in \mathbb{N}$ such that $r > K$ and $k > K$ . Therefore we can find a natural number $N$ such that $$ d \left( p_{kn}, p_{rn} \right) < \frac{\varepsilon}{2}$$ for all $r, k, n \in \mathbb{N}$ such that $r > K$ , $k > K$ , and $n > N$ . So $$ d \left( p_{kn}, p_{K+1, \ N+1} \right) < \frac{\varepsilon}{2}$$ for all $k, n \in \mathbb{N}$ such that $k > K$ and $n > N$ . Now $$\left\{ p_{K+1, \ N+1}, \  p_{K+1, \ N+1}, \ p_{K+1, \ N+1}, \  \ldots \right\}$$ is a Cauchy sequence in $X$ . Let $P \in X^*$ be the equivalence class of this Cauchy sequence. Now, for each $k \in \mathbb{N}$ , since $\left\{ p_{kn} \right\}_{n \in \mathbb{N}}$ is a Cauchy sequence in the equivalence class $P_k$ , we can conclude that, for all $k \in \mathbb{N}$ such that $k > K$ , the following is true: $$ \Delta \left( P_k, P \right) = \lim_{n \to \infty} d \left( p_{kn}, p_{K+1, \ N+1} \right) \leq \frac{\varepsilon}{2} < \varepsilon.$$ Since $\varepsilon > 0$ was arbitrary, we can conclude that our original sequence $\left\{ P_k \right\}_{k \in \mathbb{N}}$ converges in $\left( X^*, \Delta \right)$ to the point $P \in X^*$ . Hence $\left( X^*, \Delta \right)$ is a complete metric space. Is this proof correct? If not, then where lies the problem?  I wonder if the $P\in X^*$ in my argument is dependent on our choice of $\varepsilon$ and is not uniform.","['complete-spaces', 'real-analysis', 'metric-spaces', 'analysis']"
2083960,"Prove $\sum _{n=1}^{\infty }\:\frac{x^n}{n}$ is equal to $-\ln\left(1-x\right)$, for $|x| < 1$","Prove $$\sum _{n=1}^{\infty }\:\frac{x^n}{n} = -\ln\left(1-x\right),$$ for $|x| < 1$. Pretty much title I've searched and seen some solutions using Taylor Series but isn't there a more intuative way to do this?","['logarithms', 'taylor-expansion', 'sequences-and-series', 'discrete-mathematics']"
2083962,Solving $\lim\limits_{x \to \infty} \frac{\sin(x)}{x-\pi}$ using L'Hôpital's rule,"I know how to solve this using the squeeze theorem, but I am supposed to solve only using L'Hôpital's rule $$\lim\limits_{x \to \infty} \frac{\sin(x)}{x-\pi}$$ I tried :
$$\lim\limits_{x \to \infty} \frac{\sin(x)}{x-\pi} = \lim\limits_{x \to \infty} \frac{d/dx[\sin(x)]}{d/dx[x-\pi]} = \lim\limits_{x \to \infty} \frac{\cos(x)}{1}$$ From here I am stuck because the rule no longer applies and using $\infty$ for $x$ doesn't not help to simplify. Logically the limit is $0$ because $\sin(x)$ can only be $-1$ to $1$, but this is using squeeze theorem. Is still there any way to solve this without using the squeeze theorem?","['calculus', 'limits']"
2084035,"Confused about the definition of a random sample, statistics and estimators/estimates","I'm currently studying basic statistics, and I don't really understand the definition of random sample in the book I'm reading ( Introduction to Probability and Statistics: Principles and Applications for Engineering and the Computing Sciences, 4th Edition ). The book defines random samples as follows: [...] the term ""random sample"" is used in three different but closely related ways in applied statistics. It may refer to the objects selected for study, to the random variables associated with the objects to be selected, or to the numerical values assumed by those variables. A random sample of size $n$ from the distribution of $X$ is a collection of $n$ independent random variables, each with the same distribution as $X$. [...] The objects selected generate $n$ numbers $x_1,x_2,x_3,\ldots,x_n$ which are the observed values of the random variables $X_1,X_2,X_3,\ldots,X_n$. At this point, I don't really see the point of thinking of a random sample as a collection of random variables, considering the data gathered for an experiment is a set of constants. A statistic is defined by the book as ""a random variable whose numerical value can be determined from a random sample"". As an example, the sample mean statistic of a sample (a set of random variables) $\{1, 2, 3\}$ is defined as
$$ \bar{X} = \frac{X_1 + \cdots + X_n}{n} $$
where $X_{1..n}$ are random variables (which means that $\bar{X}$ is also a random variable), as opposed to
$$ \bar{x} = \frac{x_1 + \cdots + x_n}{n} $$
where $x_{1..n}$ are constant numbers (which means that $\bar{x}$ is a single number). Now, since $X_1, \ldots, X_n$ are random variables but each have a single, known value ($X_i$ assumes the value $x_i$), $\bar{X}$ is also a random variable which assumes a known value -- i.e., $(x_1 + \cdots + x_n)/n$. (Please correct me if I'm wrong here.) Here's my confusion. I think I've understood the concept of statistics and estimators -- to provide an estimation for population parameters using the characteristics of a sample. However, I don't see a reason behind thinking of a random sample as a set of random variables -- as opposed to the (in my eyes) more ""natural"" way of thinking of them as simply a set of numerical constants -- or thinking of a statistic like the sample mean, median, range, etc. as a random variable rather than a single, constant numerical value. This is my first post here, so the question probably has problems with structure, length and clarity, but what I'm essentially asking is for some ""justification"" for thinking of a) a random sample as a set of random variables as opposed to a set of numbers, and b) a statistic as a random variable rather than a constant.","['random', 'statistical-inference', 'statistics', 'probability', 'random-variables']"
2084047,Can the following construction be used to measure countable sets?,"New Edit: The Measure has several problems. My answer is stated below the post I previously stated: ""I manipulated the Construction so that if $T_1=T_2$ then $\mu(T_1)=\mu(T_2)$. If the construction can be
  improved let me know. "" I was wrong. I cannot manipulate the construction to have a consistent measure for equivalent sets with different non-reduced forms. However, if the equivalent sets had different reduced forms, their measures would be the same. Moreover, to compare the size of non-equivalent sets, the sets must be simplified first. See the section ""Sets Must Be Simplified Before Being Measured"" The construction of the measure is similar to Natural Density and Counting Measure . Consider a group of countably infinite sets dense in $[a,b]$ $T_1=\left\{\left.a<\frac{M_1(n_1)}{R_1(q_1)}<b\right|n_1,q_1\in\mathbb{Z}\right\}$ $,T_2=\left\{\left.a<\frac{M_2(n_2)}{R_2(q_2)}<b\right|n_2,q_2\in\mathbb{Z}\right\},...,$ $T_m=\left\{\left.a<\frac{M_m(n_m)}{R_m(q_m)}<b\right|n_m,q_m\in\mathbb{Z}\right\}$ I want to create a measure that compares the ""size"" of each countable dense set to all the sets. User @Mikhail suggested: ""It is indeed possible to get measures that behave in a nontrivial fashion on countable subsets, and indeed the axiom of choice or some weaker forms thereof are relevant here.  Namely, one can get a finitely-additive ( not $\sigma$-additive) measure $\xi$ on the set of subsets of $\mathbb{N}$ such that $\xi(S)=0$ for any finite set $S\subseteq\mathbb{N}$ and $\xi(S)=1$ for any cofinite subset (i.e., subset with a finite complement). Here $\xi$ takes only two values, zero or one. For sets that are neither finite nor cofinite, the measure behaves in a nontrivial way but always exactly one of a pair of complementary sets has measure one. Such measures are used in ultraproduct constructions."" However, due to my lack of understanding of ultra-product constructions and axiom of choice, I created an alternate construction which may or may not follow the principles of measure theory. The construction below involves cardinality and ratios. The Construction We begin by taking the cardinality of $T_c=\left\{\left.a<\frac{M_c(n_c)}{R_c(q_c)}<b\right|n_c,q_c\in\mathbb{Z}\right\}$  (where $c$ is an integer and $1\le c \le m$) \begin{equation}
\left|\left\{\left.a<\frac{M_c(n_c)}{R_c(q_c)}<b\right|n_c,q_c\in\mathbb{Z}\right\}\right|
\end{equation} The countably infinite set can divided into an infinite union of finite sets. \begin{equation}
\left|\lim_{t\to\infty}\bigcup_{q_c=-t}^{t}\left\{\left.a<\frac{M_c(n_c)}{R_c(q_c)}<b\right|n_c\in\mathbb{Z}\right\}\right|
\end{equation} The union can be modified inorder for the ""compactness"" of $R_c(q_c)$ to be included in the measure; however, $T_c$ must be simplified first (There is a section below defending simplification). Here are the steps to fully reducing the set. One must be able to factor $h_c$ out of all coefficients of $M_c(n_c)$ and factor $p_c$ out of all coefficients of $R_c(q_c)$. $h_c/p_c$ must be fully reduced to get ${h_c}'/{p_c}'$. If the denominator is irrational then it must be rationalized. After multiplying the ${h_c}'$ to the numerator ${p_c}'$ to the denominator we end up with $M_c^{'}(n_c)$ and $R_c^{'}(q_c)$. (All coefficients must not be in the form of fractions) We end up using ${R_c}^{'}(q_c)$ to modify the previous equation. $$s_c(t)=\left|\bigcup_{v_c\in \left\{\left.-t\le {{R_c}^{'}(q_c)} \le t \right|q_c\in \mathbb{Z} \right\}}\left\{\left.a<\frac{M_c(n_c)}{v_c}<b\right|n_c\in\mathbb{Z}\right\}\right|$$ The function $s_c(t)$ is the modified cardinality of $T_c$. Similarly $s_1(t),s_2(t),..s_m(t)$ are the modified cardinalities of $T_1,T_2,..,T_m$. If the finite sets in the union of $s_c(t)$ were disjoint, $s_c(t)$ would equal $$=\sum_{v_c\in \left\{\left.-t\le {{R_c}^{'}(q_c)}\le t \right|q_c\in \mathbb{Z} \right\}}\left|\left\{\left.a<\frac{M_c(n_c)}{v_c}<b\right|n_c\in\mathbb{Z}\right\}\right|$$ However, in most cases, the finite sets are not disjoint. Hence $s_c(t)$ would be less than the sum above. This makes the modified cardinalities countably sub-additive. The ""construction"" of the measure of $T_c$ ends up being $$\mu(T_c)=\lim_{t\to\infty}\frac{s_{c}(t)}{s_1(t)+s_2(t)+...+s_m(t)}$$ $$\mu(T_c)=\lim_{t\to\infty}\frac{s_c(t)}{\sum_{p=1}^{m} s_{p}(t)}$$ Sets must be Simplified before being measured In order to show one set is a subset of another set, both sets must be simplified. Consider sets $T_1=\left\{\left.\frac{2n+1}{4p+2}\right|n,p\in\mathbb{Z}\right\}$ and $T_2=\left\{\left.
\frac{8r+6}{16v+12}\right|r,v\in\mathbb{Z}\right\}$. We can prove $T_2\subset{T_1}$, if the numerator of $T_2$ is a subset of the numerator of $T_1$ and the denominator of $T_2$ is a subset of the denominator of $T_1$. However $\left\{\left.8r+6\right|r\in\mathbb{Z}\right\}\not\subset\left\{\left.2n+1\right|n\in\mathbb{Z}\right\}$ and $\left\{\left.16v+12\right|v\in\mathbb{Z}\right\}\not\subset\left\{\left.4p+2\right|p\in\mathbb{Z}\right\}$. We have to simplify both sets! Set $T_1$ is fully reduced but $T_2$ can be simplified into $\left\{\left.\frac{4r+3}{8v+6}\right|r,v \in \mathbb{Z}\right\}$. We find that $\left\{\left.4r+3\right|r\in\mathbb{Z}\right\}\subset\left\{\left.2n+1\right|n\in\mathbb{Z}\right\}$ and $\left\{\left.8n+6\right|v\in\mathbb{Z}\right\}\subset\left\{\left.4p+2\right|p\in\mathbb{Z}\right\}$. Hence $T_2\subset T_1$ but both sets must be simplified before being compared to one another. If we set $T_1=T_2$, with the sets in different unreduced forms, the sets must be simplified into reduced forms. Otherwise the sets cannot be properly compared to one another. Infact, if the equivalent reduced sets were measured, both set would have equal measure. I will give an example but not go in detail with the calculations. If we set $T_1=\left\{\left.\frac{2p+1}{2q+1}\right|p,q\in\mathbb{Z}\right\}$ and $T_2=\left\{\left.\frac{2p+3}{2q+5}\right|p,q\in\mathbb{Z}\right\}$ and compare the size of the sets between [1,2]; both sets are already reduced and should have a measure of $1/2$. This is easy to prove, since the cardinalities of the sets divided into finite unions are the same and so are the elements of the sets' denominators when restricted by $[-t,t]$. Example for Applying the Measure (If $T_2\subset T_1$ then $\mu(T_2)<\mu(T_1)$) Say we set $T_1=\left\{\left.\frac{\sqrt{n}}{q^2}\right|n,q\in\mathbb{Z}\right\}$ and $T_2=\left\{\left.\frac{\sqrt{2m+1}}{r^4}\right|m,r\in\mathbb{Z}\right\}$ and compare the size of the set between $[1,2]$ (Note that $T_2\subset{T_1}$.) Since $T_1$ is fully simplified, we can apply the measure. $$s_1(t)=\left|\bigcup_{v_1\in\left\{\left.-t<q^2<t\right|q\in\mathbb{Z}\right\}}\left\{\left.1<\frac{\sqrt{n}}{v_1}<2\right|n\in\mathbb{Z}\right\}\right|$$ $$=\sum_{q=1}^{\left\lfloor\sqrt t\right\rfloor}\left\lfloor(2q^2)^2 \right\rfloor-\left\lceil(q^2)^2 \right\rceil+1$$ (Note we do not sum elements that are undefined or repeated in other finite sets.) Since $T_2$ is simplified, we can apply the measure. $$s_2(t)=\left|\bigcup_{v_2\in\left\{\left.-t<{r^4}<t\right|q\in\mathbb{Z}\right\}}\left\{\left.1<\frac{\sqrt{2m+1}}{v_2}<2\right|m\in\mathbb{Z}\right\}\right|$$ $$=\sum_{r=1}^{\lfloor\sqrt[4]{t}\rfloor}\left\lfloor\frac{(2r^4)^2-1}{2}\right\rfloor-\left\lceil\frac{(2r^4)^2-1}{2}\right\rceil+1$$ The measure of each set ends up being $$\mu(T_1)=\lim_{t\to\infty}\frac{s_1(t)}{s_1(t)+s_2(t)}=1$$ $$\mu(T_2)=\lim_{t\to\infty}\frac{s_2(t)}{s_1(t)+s_2(t)}=0$$ Hence $T_1$ has a measure of 1 and $T_2$ has a measure of 0. Properties of the Construction If $T_1 \subset T_2$ then $\mu(T_1)<\mu(T_2)$ If $T_1$ and $T_2$ are fully reduced and $T_1=T_2$ then $\mu(T_1)=\mu(T_2)$. (Note that $T_1$ and $T_2$ can be in different reduced forms and still be equal) (If $>>$ means ""growth rate is infinitely greater than as $x\to\pm\infty$"", $<<$ means ""growth rate is infinitely smaller than as $x\to\pm\infty$"", $\sim$ means ""growth rate is porportional as $x\to\pm\infty$"", and any listed function eventually increases without any bound, the following is also true.) If ${|M_1(x)|}>>{|M_{2}(x)|}$ and $|R_1(x)|\sim|R_2(x)|$ ; $|M_{1}(x)|\sim |M_{2}(x)|$ and $|R_{1}(x)|>>|R_{2}(x)|$ ; or $|M_{1}(x)|>>|M_{2}(x)|$ and $|R_{1}(x)>>R_{2}(x)|$, then $\mu(T_1)=0$ and $\mu(T_2)=1$. If ${|M_1(x)|}<<{|M_{2}(x)|}$ and $|R_1(x)|\sim|R_2(x)|$ ; $|M_{1}(x)|\sim |M_{2}(x)|$ and $|R_{1}(x)|<<|R_{2}(x)|$ ; or $|M_{1}(x)|<<|M_{2}(x)|$ and $|R_{1}(x)<<R_{2}(x)|$, then $\mu(T_1)=1$ and $\mu(T_2)=0$. If $|M_{1}(x)|>>|M_{2}(x)|$ , $|R_{1}(x)|<<|R_{2}(x)|$ , and $|R_{1}(x)|>>|M_{2}(x)|$ , then $\mu({T_1})=0$ and $\mu({T_2})=1$. If $|M_{1}(x)|>>|M_{2}(x)|$ , $|R_{1}(x)|<<|R_{2}(x)|$ , and $|R_{1}(x)|<<|M_{2}(x)|$ , then $\mu({T_1})=1$ and $\mu({T_2})=0$ However, as suggested in a previous question, there is no ""best way"" of measuring the size of countable sets. Can the following construction be used to measure countable sets? If not, how can we alter the construction, to make a measure possible? What other measures can be used?","['set-theory', 'measure-theory']"
2084079,Show that $\mu\left(\bigcup_{j=1}^{N}E_j\right) \geq \frac{1}{6}\sum_{j=1}^{N}\mu(E_j)$,"This came from an old qualification exam for measure theory: Suppose $(X,M,\mu)$ is a measure space and $E_1,\ldots, E_N\in M$ with $\mu(E_j\cap E_k)\leq \mu(E_j)/N$ for each $j\neq k$. Show that $$\mu\left(\bigcup_{j=1}^{N}E_j\right) \geq \frac{1}{6}\sum_{j=1}^{N}\mu(E_j)$$ Thoughts: I thought that for $E_1,\ldots,E_n\in M$ that $$\mu\left(\bigcup_{1}^{n}E_j\right)\leq \sum_{1}^{n}\mu(E_j)$$ when $E_j$'s are not disjoint. I know we can use the disjointification trick to get equality. I am not sure how to show the latter though. Any suggestions are greatly appreciated.",['measure-theory']
2084088,How to prove that a function is differentiable everywhere?,"The function I come up  with is: $f(x)= -5x$ and I think this function can be differentiated everywhere because the domain is $\mathbb R$ right? $f'(x) = -5$ which is constant, but my question is that how can I use theorem to prove my answer.","['derivatives', 'proof-verification', 'calculus', 'functions']"
2084089,Is there a better way to tell if a function is approaching positive or negative infinity without looking at the graph?,"Let's suppose I have the graph y = $\frac 1x$ and that I do not know how it looks visually/graphically. I know there is an asymptote at $x = 0$, but do not know if the graph is approaching positive or negative infinity at both sides of the asymptote. I would like to know if it is possible to tell algebraically just from a given equation where the graph goes on both sides of the asymptote. I would like to do this without plugging in two values for $x$ (smaller than $x$ /greater than $x$), unless that that is the only solution to this problem.","['algebra-precalculus', 'calculus', 'functions']"
2084095,How fast does $\lim_{x\to 1^-} \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)$ go to $\alpha$?,"In this question: $\lim_{x\to 1^-} \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)$ it is established that $$\lim_{x\to 1^-} \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right) = \alpha$$
 for all $\alpha > 0$. I'd like to know how fast it approaches $\alpha$. In particular, to show that the rate of approach is such that for all $\alpha$ 
$$\lim_{x\to 1^-}\frac{\alpha -  \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)}{1-x}$$ exists, and that when $\alpha$ is very small, $$\lim_{\alpha\to 0^+} \left[ \lim_{x\to 1^-}\frac{\alpha -  \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)}{1-x}
\right]=\frac16$$","['euler-maclaurin', 'lacunary-series', 'digamma-function', 'limits']"
2084116,Rotate Object About a Point With Crank Mechanism,I am trying to figure out how to create this physics simulation but I need some guidance on how to go about calculating it. Below is an image of the system I am working to solve. Here I am trying to find out how to calculate the angle at which to rotate the black triangle in order to maintain the length of 111 for the blue linkage as the circle crank rotates continuously. I have used the law of cosines to calculate all the angles and the changing length (x). What I am stuck on is how to take all the variables and put them together to calculate the proper triangle rotation to maintain the 111 length. Any help would be greatly appreciated and if I can give any more information please let me know.,['trigonometry']
2084177,Maximum Number of Regions in the Plane Using Zig Lines?,"I'm having trouble understanding how the book came up with the formula for calculating $Z_n$, the maximum number of regions created using $n$ zig lines (a zig line is a line containing just one sharp bend). Earlier in the book (Page 5), we derived $L_n$, the maximum number of regions created using $n$ straight lines. What follows is the derivation of $Z_n$: First, I'm not exactly sure what the underlined sentence means. From the picture of $Z_2$, my only guess at its meaning is that none of the zigs can contain the zig point of another zig. My other question is about the assumptions that the book seems to be making in its argument (written in color): 1) Imagine $n$ zigs, and assume they split the plane into $Z_n$ regions. Extend all the lines as in the example. This creates a plane split by $2n$ lines $\color{red}{\text{into } L_{2n} \text{regions}}.$ 2) Now, erase the extensions. For each extension we erase, $\color{blue}{\text{we lose 2 regions}}.$ 3) Therefore $Z_n$ zigs split the plane into $L_{2n}-2n$ regions. The blue assumption surely has something to do with my underlined sentence, but its not obvious to me that in very complicated figure with many intersections, we lose exactly two regions per deletion of one extensions. Could someone help make the justification for these assumptions more clear? Thanks.","['algebra-precalculus', 'intuition', 'plane-geometry', 'recursion']"
2084182,If $(1+ \cos A) (1+ \cos B) (1+\cos C)= y = (1- \cos A) (1-\cos B) (1-\cos C)$ then prove that $y = \pm \sin A \sin B \sin C$,"If $$(1+ \cos A) (1+ \cos B) (1+\cos C)= y = (1- \cos A) (1-\cos B) (1-\cos C)$$ then prove that
$$y = \pm \sin A \sin B \sin C$$ My Work:
 If $y=\prod(1+\cos A)=\prod(1-\cos A)$ $y^2=y\cdot y=\prod(1+\cos A)\cdot\prod(1-\cos A)$ How, should I move further? I am not getting any idea.",['trigonometry']
2084193,Lebesgue measure on $L^2$,"This came from an old qualification exam for measure theory: Suppose that $f:\mathbb{R}\to \mathbb{R}$ is monotonically increasing and absolutely continuous, and let $f'$ denote its Lebesgue a.e. define derivative. a.) Show that if $f'\in L^2(m)$ ( $m$ is Lebesgue measure on $\mathbb{R}$ ) then there is a $C < \infty$ such that for every Lebesgue measurable set $E$ $$m(f(E)) \leq C(m(E))^{1/2}$$ b.) Provide a counterexample to show that if we instead assume $f'\in L^1(m)$ then such a $C$ may not exist. I normally don't post a question without some what of an attempt, but I really have no idea how to do this. Any suggestions or comments are greatly appreciated, I will edit this once I think of something.","['measure-theory', 'absolute-continuity']"
2084196,Unique decomposition of a positive definite matrix into a sum of outer products ${\bf x}_k{\bf x}_k^{\rm T}$ and a diagonal matrix?,"Given a (symmetric) positive definite matrix ${\bf A}\in\mathbb{R}^{N\times N}$, I know that it can always be expressed as a sum of $N$ rank-one matrices using the singular value decomposition ${\bf U\Sigma V^\ast}$ of the corresponding Cholesky matrix $\bf L$:
$${\bf A} = {\bf L}{\bf L}^{\rm T} = {\bf U\Sigma V^\ast}({\bf U\Sigma V^\ast})^{\rm T} = \sum_{i,j=1}^N \sigma_i \sigma_j {\bf u}_i {\bf v}^{\rm T}_i {\bf v}_j {\bf u}^{\rm T}_i= \sum_{i=1}^N \sigma_i^2 {\bf u}_i {\bf u}^{\rm T}_i,$$
with ${\bf u}_i$ being the $i$-th column of $\bf U$. So the matrix terms here are all outer products (dyads) of a vector with itself. Can such a matrix also be expressed as the sum of a diagonal positive definite matrix $\bf D$ of full rank with $D_{ii} = d_i^2 > 0$ and a series of $M$ outer products ${\bf x}{\bf x}^{\rm T}$ (${\bf x}\in \mathbb{R}^N$)?
That is: $${\bf A} = {\bf D} + \sum_{k=1}^M {\bf x}_k{\bf x}_k^{\rm T}$$ Basically, this is just one large system of $N(N+1)/2$ quadratic equations with $N(M+1)$ unknowns $\{d_i, x_{k,i}\}$. Using small test values for $N$ and $M$, I was able to find numerical solutions to this
equation system for specific example matrices, but it would be nice to have an analytic solution.
I suspect this should always be possible, at least for large enough values of $M$. If so, what is the minimal value of $M$ for which such a decomposition exists? What can be said about the relationship of ${\bf d}={\rm diag}({\bf D})$ and the different ${\bf x}_k$?
Is there some $M$ for which the decomposition is unique? If not, how can a counterexample be constructed? Some thoughts on this: for $N=1$, this is trivial, as one can always choose $\bf D=A$ and $M=0$ for $N\geq2$, I thought of choosing $D_{ii} = A_{ii}$ as a starting point and applying the Cholesky/SVD strategy on the difference ${\bf A}-{\bf D}$ to get the ${\bf x}_k$. However, for $N=2$ at least, this doesn't work, since the difference matrix is not positive definite. So I guess a related question is: For which $N\times N$ diagonal matrices ${\bf D}$ does ${\bf A}-{\bf D}$ remain positive definite? from a geometric perspective, a positive definite matrix ${\bf A}$ defines an $N$-dimensional ellipsoid $\{{\bf z}\in\mathbb{R}:{\bf z}^{\rm T}{\bf A}{\bf z}={\rm const.}\}$. If the matrix is diagonal, the semi-axes of this ellipsoid are aligned with the coordinate axes. 
In constrast, an outer-product-like term ${\bf x}{\bf x}^{\rm T}$ defines a pair of hyperplanes $\{{\bf z}\in\mathbb{R}:{\bf z}^{\rm T}{\bf x}{\bf x}^{\rm T}{\bf z}=\|{\bf z}^{\rm T}{\bf x}\|^2={\rm const.}\}$. So the above problem is equivalent to asking whether any $N$-dimensional ellipsoid can be understood as a ""superposition"" of an axis-aligned ellipsoid and a series of hyperplane pairs, although the meaning of ""superposition"" is somewhat vague in this context... the above system of equations can be written using the tensor product $\otimes$:
$
\left(\sum_{i,j} a_{i,j}\,{\bf e}_i\otimes{\bf e}_j\right)
=
\left(\sum_i d_i^2\,{\bf e}_i\otimes{\bf e}_i\right)
+
\sum_k
  \left(\sum_i x_{k,i}{\bf e}_i\right) \otimes
  \left(\sum_j x_{k,j}{\bf e}_j\right)
$. Maybe some tensor algebra can come in handy?","['systems-of-equations', 'positive-definite', 'matrix-decomposition', 'symmetric-matrices', 'linear-algebra']"
2084201,Probability that two multinomial samples are the same?,"Say I take $N$ samples (with replacement) from an urn with $M$ balls each a distinct color. I note the result, then repeat the process once again, and note the second result. I'm interested in the probability that the two samples are the same (that is, both had same number of each of the colors present in the sample.) This is obviously the sum of the squares of the PMF of the required multinomial distribution over the weak compositions of $N$ of length $M$, and since the balls are equiprobable, this simplifies to a more efficient scheme by doing the same over the integer partitions of $N$ with appropriate multiplications. For large $N$ this quickly becomes intractable. Is there a better way to calculate the desired result, or a reasonably accurate estimator for cases where $N>>M$?","['combinatorics', 'probability', 'balls-in-bins']"
2084202,Trigonometry and Quadratics,"If $\tan A$ and $\tan B$ are the roots of $x^2+px+q=0$, then prove that $$\sin^2(A+B)+p \sin(A+B) \cos(A+B) + q \cos^2(A+B) = q$$ My Attempt: Using the sum and product formulae we have, $q=\tan A\tan B, $        $-p=\tan A+\tan B$ And, $\tan (A+B)=\frac{\tan A+\tan B}{1-\tan A\tan B} \Rightarrow \tan (A+B)=\frac{-p}{1-q}$",['trigonometry']
2084214,"Two sets are equal, Definition.","Two sets A and B are the same, if $ \displaystyle \forall X: [X \in A \Leftrightarrow X \in B] \land [A \in X \Leftrightarrow B \in X]$ Ehm. I know one explanation that says, ""two sets are equal, if they own the same elements"". If so, why do we need the second part of the definition? If I read this, I see: ""Two sets are equal, if they own the same elements and are elements of some other thing"". But then, they are elements of that that is an element of themselves. What are these circles of mind? Can somebody explain me the sacral meaning of this second part? Thanks.",['elementary-set-theory']
2084259,A question regarding further studies of continuous probability distributions,"I recently finished studying a book in elementary probability theory. I am particularly interested in continuous probability distributions, but since the book was just an introduction to probability theory it didn't cover those to a satisfactory extent. I never really got to put anything I learned to use other than solving a couple of basic assignments. If I want to learn more about continuous distributions, what book should I pick? I haven't had statistics yet, would that perhaps the be correct choice? Is there perhaps some book in differential equations that is centered around probability theory?","['reference-request', 'probability-theory', 'probability-distributions', 'statistics', 'book-recommendation']"
2084265,Writing a set as a sum of cartesian products,"Informally, I am given a subset $Q$ of a Cartesian product $A_1\times\ldots \times A_n$ and I would like to write $Q$ as a disjoint sum of Cartesian products of subsets of the $A_i$ . I would like the decomposition to be optimal in some sense—e.g. the number of terms in the disjoint sum is as small as possible. Here's a formal statement of the problem: Given a collection of finite disjoint sets $A_1, \ldots, A_n$ and a subset $Q$ of the Cartesian product $A_1 \times A_2 \times \ldots \times A_n$ , find an integer $k$ and subsets $\{ B_{i,j} \subseteq A_j\;: \; i\in [1,k], j\in[1,n]\}$ such that $$Q \equiv \bigcup_{i=1}^k B_{i,1} \times \ldots B_{i,n}  $$ and where $k$ is the smallest integer for which such a decomposition is possible. I feel like this problem must have been asked before, but I haven't been able to find a solution online, perhaps because I lack the necessary terminology.","['optimization', 'elementary-set-theory']"
2084274,What is the longest terminating (or repeating) decimal created by the division of two natural numbers in which neither number is more than two digits?,"I came across a problem today while teaching in which a long division problem created an unusually long terminating decimal.  It intrigued me into more thinking on this. My question is: What is the longest terminating (or repeating) decimal created by the division of two natural numbers in which neither number is more than two digits?  And, is there a way to prove this besides using brute force in trying all possibilities?","['number-theory', 'elementary-number-theory']"
2084280,"Find all $n$ such that $n \mid x_n$ where $x_n = x_{n-1} + \lfloor n^2 / 4 \rfloor$, $x_0 = 0$","$X_n$ is sequence such that $x_n=x_{n-1}+[\frac{n^2}{4}]$ and $x_0=0$. Find all positve integers $n$ for which $x_n$ is divisible by $n$. 
[X] means integer part.","['divisibility', 'sequences-and-series', 'elementary-number-theory']"
2084282,Canonical almost complex structure on symplectic manifold,"I'm trying to prove the following (well-known) theorem in symplectic geometry. Theorem . For $(M, \omega)$ a symplectic manifold with Riemannian metric $g, \exists$ a canonical almost complex structure $J$ compatible with $ω$.
( source ) Could someone give me a sketch and I can fill in the details. Thank you.","['complex-geometry', 'almost-complex', 'differential-geometry', 'symplectic-geometry']"
2084313,Proof that conditional probabilities sum up to one,"I've read on Wikipedia that the sum (or integral, for continuous $Y$) of $P(Y=y|X=x)$ over $y$ is always equal to one. I've attempted a proof of this statement for the discrete (sum) case: Proof: By the Kolmogorov definition of conditional probability and the Law of Total Probability, $$\sum_k P(A_k |  B) = \sum_k \frac{P(A_k \cap B)}{P(B)} = \frac{1}{P(B)}\sum_kP(A_k \cap B) = \frac{1}{P(B)}P(B)=1.\ \square$$ Is this a correct proof? How would I proof the above statement for the continuous case, i.e. that $\int_y P(Y=y|X=x)$ always equals one?","['probability', 'proof-verification']"
2084320,Taking a derivative from both sides of the equation,"Our teacher tried to explain to us how to find a slope at a given point of the function $y^2 = 2px$ by taking derivative from both sides of the equation, he did this:
$$y^2= 2px $$
$$2yy' = 2p$$
$$ y' = \frac{p}{y}$$
I haven't quite understood the second part of the process, will be thankful for a logical and clear explanation :)","['derivatives', 'calculus', 'proof-explanation']"
2084326,PDF of a ratio of functions of order statistics,"Question : Let $X_1,\dots,X_n$ be iid from a distribution with cdf $F(\cdot)$. Find the pdf of $\dfrac{1-F(X_{(2)})}{1-F(X_{(1)})}$. I know that $F(X) \sim \mathcal{U}(0,1)$ for any r.v. $X$, so in this case, we're looking for the pdf of $\dfrac{1- U_{(2)}}{1-U_{(1)}}$, where $U_1,U_2,\dots,U_n$ are standard uniform. From here, I'm sort of stuck. This is a test question, so it was meant to be reasonably quick. Some ideas: I can calculate $f_{U_{(1)}, U_{(2)}} (u,v) = n(n-1) \cdot \left[ 1 - v \right]^{n-2}\cdot  \mathbb{1}[0 <  u < v< 1] $. Then I suppose I could find $P \left( \frac{1-U_{(2)}}{1-U_{(1)}} \le x \right) $ by integrating and go from there. The hint was that for $U_1,\dots,U_n \stackrel{iid}{\sim} \mathcal{U}(0,1)$, we have the following fact: given $ U_{(n)} = u$, $(U_{(1)},\dots,U_{(n-1)})$ is conditionally distributed as order statistics on $[0,u]$. So I could condition on $U_{(3)}$? This is still a pain, though. The above hint makes me think there's a typo, as it doesn't seem to help. Is there a clever way to do this problem?",['statistics']
2084365,"Standard deviation of $x_1,x_2,x_3,....,x_{18}$","If $\sum^{18}_{i=1} (x_i-8)=9$ and $\sum^{18}_{i=1} (x_i-8)^2=45$, then find standard deviation of $x_1,x_2,x_3,...,x_{18}$ Using $\sum^{18}_{i=1} (x_i-8)=9$, I got mean mean of $x_1,x_2,x_3,...,x_{18}$ as $\frac{17}{2}$ but how to use second condition to find variance so that standard deviation can be found?","['statistics', 'euler-mascheroni-constant']"
2084371,Is it possible that $a_n>0$ and $\sum a_n$ converges then $na_n \to 0$? (without assuming $a_n$ is decreasing),"Can it be proven that : If $a_n>0$ and $\sum a_n$ converges then $na_n \to 0$ (without assuming $a_n$ is decreasing) Please note This question does not require $a_n$ to be decreasing as condition, 
Is it possible to prove $na_n \to 0$ without requiring $a_n$ to be decreasing?
I keep thinking that $a_n>0$ and $\sum a_n$ converging then that implies that $a_n$ must be decreasing anyway(not monotonically decreasing necessarily ) Is it possible that $a_n>0$ and $\sum a_n$ converges then $na_n \not \to 0$? There are already answered questions that assume $a_n$ to be decreasing as a requirement: (this question does not assume $a_n$ is decreasing) If $(a_n)$ is a decreasing sequence of strictly positive numbers and if $\sum{a_n}$ is convergent, show that $\lim{na_n}=0$ Prove that if $\sum a_n$ converges, then $na_n \to 0$. This was the post that made me ask this question, a counter example of monotonoic decreasingness of $a_n$ was given in comments by the OP.","['sequences-and-series', 'convergence-divergence', 'limits']"
2084384,Why does $\log x$ go to $-\infty$ so slowly?,"While checking convergence of integrals, I always have to remind myself that $\log x$ goes to infinity very slowly. This is almost like and axiom to me. I don't intuitively get why it goes to infinity so slowly. On a graph, it sure doesn't look that way. Any thoughts? More precisely:
$$\forall\delta>0: \lim_0x^\delta\log x=0$$","['intuition', 'functions']"
2084412,"$t^l\leq \Phi(t)+1,\forall t\geq1$","I have that $\Phi:\mathbb{R}\rightarrow [0,+\infty)$  is a continuous, increasing and convex function such that 
$$t^l\Phi(1)\leq \Phi(t)\leq t^m \Phi(1), \;\;\forall t\geq1$$ where $1<l<m$. and $\displaystyle\lim_{t\rightarrow0}\frac{\Phi(t)}{t}=0~\text{and}~\lim_{t\rightarrow+\infty}\frac{\Phi(t)}{t}=+\infty.$ I want to prove that $$t^l\leq \Phi(t)+1,\;\;\forall t\geq1$$ Can someone help me?","['functional-analysis', 'real-analysis', 'functions']"
2084429,Prove that the infinite sum $\sum_{n=1}^{\infty} \frac{F_{n}}{ 10 ^ n }$ converges to a rational number,"How do you prove that the following infinite sum
\begin{align}
     &0.1
\\+\;&0.01
\\+\;&0.002
\\+\;&0.0003
\\+\;&0.00005
\\+\;&0.000008
\\+\;&0.0000013
\\ \;&\quad\vdots
\end{align}
converges to a rational number? Notice that the above sum can be written as $$\sum_{n=1}^{\infty} \frac{F_{n}}{ 10 ^  n }$$ where $F_{n} $ is a Fibonacci sequence.","['fibonacci-numbers', 'sequences-and-series', 'convergence-divergence']"
2084474,"Doing definite integration $\int_0^{\pi/4}\frac{x\,dx}{\cos x(\cos x + \sin x)}$","We have to solve the following integration
$$
\int_0^{\pi/4}\frac{x\,dx}{\cos x(\cos x + \sin x)}
$$
I divided both in Nr and Dr by $\cos^2 x$. But after that I stuck.","['integration', 'definite-integrals', 'calculus']"
2084479,Chapman - Kolmogorov equation explained,"Now In (2.8) I have a really hard time understanding why we integrate with respect to $x_2$. $P\left(x_3,t_3|x_1,t_1\right)=\sum _n P\left(x_3,t_3|x_n,t_2\right)P\left(x_n,t_2|x_1,t_1\right)$ This to me in a discrete case looks correct. But integrating over $x_2$ to me from the picture looks really strange. Is by integrating over $x_2$ am I integrating over the entire vertical line at $t_2$ and in that case it is in the x direction hence why not why not integrate over dx instead of $dx_2$ And most important which variable would you integrate on if there where more steps. Example $p(x_5,t_5|x_1,t_1)$","['probability-theory', 'markov-process']"
2084505,What is the difference between a 1-form and a co-vector?,"My understanding has been that co-vectors are defined on a (pseudo-) Riemannian manifold as dual vectors with respect to the given metric. However, 1-forms do not need the notion of a metric and have nothing to do with vectors except that one feeds on the other. However, while reading this answer on the local representation of a tautological form (which is defined on a general manifold irrespective of the presence of a metric structure), I cannot seem to comprehend the difference between a 1-form and a co-vector, which difference seems important to the answer. Please let me know what I am missing. If possible, please also answer the linked question bearing in mind my lack of comprehension of this difference. Thank you.","['symplectic-geometry', 'differential-geometry']"
2084512,How to show that $f(z)=\frac{z^{5}}{|z|^4}$ satisfies the Cauchy Riemann equations at $z=0$ but not differentable at $z=0$?,"$$
f(z) = \left\{
        \begin{array}{ll}
            \frac{z^{5}}{|z|^4} & \quad z \neq 0 \\
            0 & \quad z= 0
        \end{array}
    \right.
$$
I am trying to solve this past exam question. Similar question was asked in Show that $f(z)=\frac{z^5}{|z|^4}$ but has not been answered. My attempt: Let $z=x+iy$ then we can separate the real and imaginary parts of $f(z)$ as $$
f(z)=u+iv=\left(\frac{x^5-10 x^3 y^2+5 x y^4}{\left(x^2+y^2\right)^2}\right)+i\left(\frac{5 x^4 y-10 x^2
   y^3+y^5}{\left(x^2+y^2\right)^2}\right)
$$ $$
\frac{\partial u}{\partial x}=\frac{5 x^4-30 x^2 y^2+5 y^4}{\left(x^2+y^2\right)^2}-\frac{4 x
   \left(x^5-10 x^3 y^2+5 x y^4\right)}{\left(x^2+y^2\right)^3}
$$
$$
\frac{\partial v}{\partial y}=\frac{5 x^4-30
   x^2 y^2+5 y^4}{\left(x^2+y^2\right)^2}-\frac{4 y \left(5 x^4 y-10 x^2
   y^3+y^5\right)}{\left(x^2+y^2\right)^3}
$$
$$
\frac{\partial u}{\partial y}=\frac{20 x y^3-20 x^3 y}{\left(x^2+y^2\right)^2}-\frac{4 y \left(x^5-10
   x^3 y^2+5 x y^4\right)}{\left(x^2+y^2\right)^3}
$$
$$
\frac{\partial v}{\partial x}=\frac{20 x^3 y-20 x
   y^3}{\left(x^2+y^2\right)^2}-\frac{4 x \left(5 x^4 y-10 x^2
   y^3+y^5\right)}{\left(x^2+y^2\right)^3}
$$ I am not sure what to do next. If I substitute $z=0\rightarrow x=0,y=0$ in the equations above then they become infinite because the bottom term $(x^{2}+y^{2})$ becomes zero. So how can I show that $f(z)$ satisfies the Cauchy Riemann equations at $z=0$. Also, how can I show that $f(z)$ is not differentiable at $z=0$? Thanks.",['complex-analysis']
2084529,Why is $\int_{0}^{1}x^x(1-x)^{2-x}\sin(x\pi)dx=\int_{0}^{1}x^{1+x}(1-x)^{1-x}\sin(x\pi)dx?$,"I don't know why these two integrals yield the same results. $$\int_{0}^{1}x^x(1-x)^{2-x}\sin(x\pi)dx=\int_{0}^{1}x^{1+x}(1-x)^{1-x}\sin(x\pi)dx$$ Any hints, clues and ideas how to go about dealing these integrals(showing that they are the same and determine the closed form)? $$\int_{0}^{1}x^x(1-x)^{2-x}\sin(x\pi)dx=\int_{0}^{1}x^{1+x}(1-x)^{1-x}\sin(x\pi)dx$$I know it is similar to the sophere's dream integral.  I think you can possible use integration by parts. Because there are three functions are involved applying by parts it would be very long. I try to make a substitution of $u=1-x$ $$-\int_{0}^{1}u^{1+u}(1-u)^{1-u}\sin((u+1)\pi)du$$ Q: show that they are the same and evaluate its closed form. Edited $$\int_{0}^{1}u^{1+u}(1-u)^{1-u}\sin(u\pi)du$$ Let try and applying by parts $u=x^x(1-x)^{2-x}$ $du=x^x(1-x)^{2-x}\ln{x\over 2-x}dx$ $v=-{\cos{(x\pi)}\over \pi}$ $$=-{1\over \pi}x^x(1-x)^{2-x}\cos{(x\pi)}+{1\over \pi}\int_{0}^{1}x^x(1-x)^{2-x}\cos{(x\pi)}\ln{x\over 2-x}dx$$ This looked more complicate than before, so ideally, no, it is not a good way of tackling this problem.",['integration']
2084539,Quadratic equation system $A^2 + B^2 = 5$ and $AB = 2$,"Given a system of equations $A^2 + B^2 = 5$ $AB = 2$ what is the correct way to solve it? I see immediately that the answers are $A=1, B=2$ $A=2, B=1$ $A=-1, B=-2$ $A=-2, B=-1$ but I don't understand the correct way of getting there. I have tried to isolate one of the variables and put the resulting expression into one of the equations, but this didn't get me anywhere. What is the correct way of solving this problem?","['algebra-precalculus', 'quadratics', 'systems-of-equations']"
2084557,Why is adjoining a unit the algebraic counterpart to the one point compactification?,"There is an interesting relation in which the action of adjoining a unit to an algebra and the one-point compactification come together: Let $X$ be a non compact but locally compact Hausdorff space and let $C_0(X)$ denote the non-unital algebra of continuous functions on $X$ which vanish at infty (w.r.t. the sup-norm this is a C-*-algebra). We can adjoin a unit element $e$ to $C_0(X)$ which yields the unital C*-Algebra $\widetilde{C_0(X)}$. On the other side, we can compactify $X$ to $X^*$ and consider the C-*-algebra of continuous functions $C(X^*)$: $$
X \to C_0(X) \to \widetilde{C_0(X)} \\
X \to X^* \to C(X^*)
$$ Interestingly both paths have the same result modulo isomorphic equivalence, i.e.
$$
\widetilde{ C_0(X) } \overset{\text{C*}}{\simeq} C(X^*).
$$ Reading about this in Pedersen's Analysis Now , p.129: If a Banach algebra $\mathfrak A$ has no unit, we may try to embed it isometrically into a larger unital Banach algebra $\mathfrak B$, in such a way that $\mathfrak A$ becomes an ideal in $\mathfrak B$, so large that every nonzero ideal of $\mathfrak B$ has a nonzero intersection with $\mathfrak A$ (an essential ideal). This process is the algebraic counterpart of the com­pactification of a topological space (1.7.2). For the classical Banach algebras there is often a natural way of adjoining a unit; but there is always an abstract procedure - the counterpart of the one-point compactification. Another place in the web where I found a similar statement was in this post: Adjoining a unit corresponds to passing to the one-point compactification. I am wondering if this ""phenomenon"" can be described in terms of category theory (as e.g. in the case of the Banach-Stone theorem) and if there are further examples of this interplay between the one-point compactification and adjoining a unit. In particular, I am interested in which sense one should understand the terms counterpart or corresponds in the two cited sources. For the sake of completeness, here is my proof of the mentioned equivalence. Let $X$ be a locally compact Hausdorff space and $X^* = X \cup \{\infty\}$ its one-point compactification. Furthermore, let $(C_0(X), \|\cdot\|_\infty)$ be the C*-Algebra of continuous functions vanishing at infinity and $(C(X^*)), \|.\|_\infty)$ be the C*-algebra of continuous functions on $X^*$. Then,
$$
\widetilde{C_0(X)} \simeq C(X^*),
$$
where $\widetilde{\mathcal A}$ denotes the unification of a non-unital algebra $\mathcal A$. Proof . Consider the mapping
$$
\varphi\colon \widetilde{C_0(X)} \to C(X^*), \quad (f, \alpha) \mapsto \overline f + \alpha := \begin{cases} f(x) + \alpha, \quad & x \in X, \\ \alpha, \quad & x = \infty. \end{cases}
$$
We want to show
$$
\|\varphi(f, \alpha)\|_\infty = \|(f,\alpha)\|_{\widetilde{C_0(X)}}.
$$
For this, we use the following Lemma . For $f \in C(X^*)$, we have
$$
s_1 := \sup_{x \in X} |f| = \sup_{x \in X^*} |f| =: s_2.
$$ Proof . Let $x_\alpha$ be a net in $X$ with $\lim_\alpha x_\alpha = \infty$. Then, $f(x_\alpha) \leq s_1$ and thus $f(\infty) \leq s_1$. This gives us $s_2 \leq s_1$. The other inequality is obvious. $\square$ On the one hand, we have
$$
\|\varphi(f,\alpha)\|_\infty = \sup_{x \in X^*} |\;\overline f(x) + \alpha\;|
= \sup_{x \in X} |\;\overline f(x) + \alpha\;|
= \sup_{x \in X} |\;f(x) + \alpha\;|
= \|f + \alpha \|_\infty.
$$
On the other hand, we have
$$
\|(f, \alpha)\|_{\widetilde{C_0(X)}} 
= \sup_{g \in C_0(X), \|g\| \leq 1} \|fg + \alpha g\|_\infty 
\leq \sup_{g \in C_0(X), \|g\|\leq 1} \|g\|_\infty \|f + \alpha\|_\infty
\leq \|f + \alpha\|_\infty = \|\varphi(f, \alpha)\|_\infty .
$$ Now, let $(g_i)_{i \in I}$ be an approximate unit in $C_0(X)$. Then,
$$
\|f g_i + \alpha g_i\|_\infty \to \|f + \alpha\|_\infty
$$
which in turn implies
$$
\|(f, \alpha ) \|_{\widetilde{C_0(X)}} = \|\varphi(f, \alpha)\|_\infty.
$$
This proves the claim. $\square$ Feel free to edit this proof or leave a comment if I made a mistake.","['c-star-algebras', 'compactification', 'operator-theory', 'category-theory', 'general-topology']"
2084573,"How to construct symmetric and positive definite $A,B,C$ such that $A+B+C=I$?","In an attempt to formulate a answer to this (in)famous question How does one prove the matrix inequality $\det\left(6(A^3+B^3+C^3)+I_{n}\right)\ge 5^n\det(A^2+B^2+C^2)$? I'm trying to construct three $n\times n$ matrices $A,B,C$ that are (a) symmetric, (b) positive definite, (c) add to $I_n$ .
Note that I've already decided to restrict attention to the reals and I have replaced Hermitian by symmetric (which IMO is
difficult enough). My unsuccessful tries are a wild mixture of two extremes: Make a random square matrix $H$ and form $A = H^TH$ . Make another random square matrix $H$ and form $B = H^TH$ .
In the same way, form $C = H^TH$ . Then $A,B,C$ are symmetric and positive definite. But in general $A+B+C \ne I$ . Generate random numbers for $A_{ij} = A_{ji}$ , $B_{ij} = B_{ji}$ and form $C_{ij} = C_{ji} = I_{ij}-A_{ij}-B_{ij}$ .
Then $A,B,C$ are symmetric and $A+B+C = I$ , but it cannot be guaranteed that these are positive definite matrices. So the question is: how can the three requirements (a) , (b) , (c) be fulfilled at the same time, while keeping $A,B,C$ yet as
random as possible? My plan is to do numerical experiments and eventually find a counter example. I have all the ingredients to do it, except this.","['matrices', 'inequality', 'linear-algebra']"
2084576,How can we prove Sylvester's determinant identity?,"Sylvester's determinant identity states that if $A$ and $B$ are matrices of sizes $m\times n$ and $n\times m$, then $$ \det(I_m+AB) = \det(I_n+BA)$$ where $I_m$ and $I_n$ denote the $m \times m$ and $n \times n$ identity matrices, respectively. Could you sketch a proof for me, or point to an accessible reference?","['matrices', 'abstract-algebra', 'linear-algebra', 'determinant']"
2084591,"How to compute $\frac{d}{dt} \int_{a(t)}^{b(t)} f(x,t) dx$ [duplicate]","This question already has answers here : How do I differentiate this integral? (3 answers) Closed 7 years ago . I want to differentiate $ \int_{a(t)}^{b(t)} f(x,t) dx$ this integral with respect to $t$ $i.e$, \begin{align}
\frac{d}{dt} \int_{a(t)}^{b(t)} f(x,t) dx
\end{align} I only know for the case of constant $a,b$. For this case how can i differentiatie?","['multivariable-calculus', 'calculus']"
2084619,Tangent plane equation of an implicit equation of a surface,"How am I supposed to find the equation of a tangent plane on a surface that its equation is not explicit defined in terms of z? The equation of the surface is:
$$
x^{2} -y^{2} -z^{2} = 1
$$ And I need to find all the points where the tangent plane is parallel to the plane:
$$z = x + y
$$ How can I do that? My approach at the moment was to think about the gradient vector of the function as being the normal vector of the plane... But the gradient vector of that surface has only two components (partial x and y), and the vector the normal vector of the plane has three components (1,1,-1)...","['multivariable-calculus', 'implicit-differentiation']"
2084652,Is there a better notation for integrating twice?,"I'm studying kinematics with varying acceleration. To calculate the displacement from the acceleration you have to integrate the acceleration with respect to t, then integrate that with respect to t, this time with limits. I've been writing this: But it looks a little messy. Is there a better way? The notation on this webpage is good but seems to be aimed at having a) limits on both integrals (for me the inner integral is indefinite) and b) different variables - in differentiating with respect to t both times.","['kinematics', 'integration', 'notation', 'calculus']"
2084660,Angle between pair of vectors from two planes at an angle mutually.,"Suppose I have 2 planes mutually at an angle to each other. Then two vectors, one from each, would make same angle between them. Is that so?",['geometry']
2084670,A continuous function whose left derivative at a point is awlays twice its right derivative is constant,"Let $f: \mathbb{R}\to\mathbb{R}$ be a continuous function such that for all $x \in \mathbb{R}$, the right derivative of $f$ at $x$ is twice the left derivative of $f$ at $x$. Does it follow that $f$ is constant?","['derivatives', 'real-analysis', 'continuity']"
2084681,Differentiating a function of a variable with respect to the variable's derivative,"Suppose $x:\mathbb{R}\to \mathbb{R}$ is parameterised by $\lambda$. What does it mean to take a derivative of a function $f(x)$ with respect to $\dot{x} = \frac{dx}{d\lambda}$. 
i.e. what does $\frac{df(x)}{d\dot{x}}$ mean? How do we compute it? Is $\frac{d}{d\dot{x}}=\frac{d}{d\frac{dx}{d\lambda}}=^{??} \frac{d\lambda}{dx}=^{??} 0$ ??? For example, how would one compute
$\frac{d}{d\dot{x}} e^x$? (This question has arisen from an undergraduate relativity course, in trying to compute the Euler-Lagrange equations, given a certain metric).","['calculus-of-variations', 'differential-geometry', 'calculus', 'general-relativity']"
2084702,Uniform convergence and interchange of limits,"We consider $f_n(x)=x^n$ on $[0,1]$. Each function $f_n(x)$ is continuous, but the limit function $f(x)$ is not continuous:
$$
    f(x)=\left\{
                \begin{array}{ll}
                  0, 0\leq x<1\\
                  1, x=1\\
                \end{array}
              \right.
$$ Question 1: How can we prove that:
$$
\lim_{n\rightarrow \infty}(\lim_{x\rightarrow 1}f_n(x))=1
$$
$$
\lim_{x\rightarrow 1}(\lim_{n\rightarrow \infty}f_n(x))=0
$$ Question 2: Is uniform convergence of $f_n(x)\rightarrow f(x)$ a sufficient condition to get the same result if we interchange the limits?","['functional-analysis', 'convergence-divergence', 'uniform-convergence']"
2084704,What boundary condition is imposed when Fourier transform is used for solving differential equation on infinite domain?,"It's a question that has puzzled me for a long time. Every PDE textbook I've ever seen tells me that, Fourier transform can be used to solve linear constant-coefficient differential equations on an infinite domain, but none of them includes an explanation about what boundary condition is actually used when Fourier transform ""kills"" the derivative. Some materials, for example this seems to suggest that, the boundary condition is $0$ at $\pm\infty$, but it's not true. A counter example is $$y'(x)+y(x)=\sin (x)$$ The general solution of this equation is (* Here's the corresponding Mathematica code *)
DSolve[y'[x] + y[x] == Sin[x], y[x], x] $$y(x)= c_1 e^{-x}+\frac{1}{2} (\sin (x)-\cos (x))$$ while the solution given by Fourier transform and inverse Fourier transform is (* Here's the corresponding Mathematica code *)
fou = FourierTransform[#, x, w] &;
fou[y'[x]] + fou@y[x] == fou@Sin[x] /. HoldPattern@FourierTransform[__] :> Y[w]
Solve[%, Y[w]][[1, 1, -1]]
InverseFourierTransform[%, w, x] $$y(x)=\frac{1}{2} (\sin (x)-\cos (x))$$ Clearly the boundary condition isn't $y(\pm\infty)=0$ or $y'(\pm\infty)=0$. What boundary condition / restriction is imposed when Fourier transform is used for solving differential equations?","['ordinary-differential-equations', 'fourier-analysis', 'fourier-transform', 'partial-differential-equations']"
2084707,Elimination of constants in pde's. How does it appears the arbitrary function in the general solution?,"Considering the method of characteristics to solve quasi-linear PDE's, I don't totally catch why one of the constants from the ODE's are related functionally one to the other (two independent variables only, if necessary to specify). I understand how it works and how the arbitrary function as part of the general solution appears, but not why the constants have such a relation.
Somehow, and here is where I need some clarification, the point is that the two families of curves must be in the same surface solution. I cannot ""jump"" from here to the functional dependence of the constants.
Sorry for my english and thanks in advance. EDIT ADDED: (Without errors, I hope)
Consider this PDE $yuu_x+xuu_y=xy$ Solving the associated ODE's we get first $x^2−y^2=c_1$ and second $u^2−y^2=c_2$. Now we make this identification (the one I don't understand where does it come from) $c_2=f(c_1)$ being f a single variable arbitrary function. We obtain the general solution: $u^2−y^2=f(x^2−y^2)$","['self-learning', 'surfaces', 'functions', 'partial-differential-equations']"
2084756,About a type of integral domains,"I was reading the notes about factorization written by Pete L. Clark and I found that he uses the name "" EL-domain "" to refer to those integral domains in which every irreducible element is prime (page 16). So my question is: is the name ""EL-domain"" an accepted terminology? I made some google search and I haven't found anything. Perhaps this terminology has been used in some papers, but I don't know. Any help is appreciated.","['terminology', 'abstract-algebra', 'ring-theory', 'integral-domain']"
2084773,A Viéte-like infinite product for $\sin (\pi/7)$,"In the article ""The unruly $\sin (\pi/7)$ of Samuel Moreno, the following infinite product is given: Also, the same article shows that $\sin (\pi/7)$ is equal to the infinite nested radical $ \sin (\pi/7) = \frac{1}{2}\sqrt{2 - \sqrt{2 - \sqrt{2 - \sqrt{2 + \sqrt{2 - \sqrt{2 - \sqrt{2 + \sqrt{2 - \sqrt{2 - \sqrt{2+...}}}}}}}}}}$ The question is: are these two expressions fundamentally different for $\sin(\pi/7)$, or is it possible to rewritte the infinite nested radical into that infinite product by simple algebraic manipulations?","['real-analysis', 'infinite-product', 'trigonometry', 'functions', 'nested-radicals']"
2084774,"How do I find the slope of an angle bisector, given the equations of the two lines that form the angle?","The equation for the first line is $y = \frac{1}{2}x - 2$, and the equation for the second line is $y = 2x + 1$. They intersect at $(-2, -3)$. Someone told me I can just average the slopes of the two lines to find the slope of the bisector, but I'm not sure if it's right.",['geometry']
2084858,An Inequality in Calculus,"Let $f(0)>0$ and $f(x)$ increases on $[0,1]$. There exists a positive number $s$ such that
  $$\int_0^1xf(x)\mathrm{d}x=s\int_0^1f(x)\mathrm{d}x$$
  Prove that
  $$\int_0^sf(x)\mathrm{d}x\le\int_s^1f(x)\mathrm{d}x$$ This is a problem from my homework. Let $\displaystyle{g(x)=\int_0^xf(t)\mathrm{d}t},x\in[0,1]$. If $f$ IS CONTINUOUS, we have $g(0)=0, g(1)=1$(WLOG, we can presume that) and $s=\displaystyle{\int_0^1xg'(x)\mathrm{d}x=1-\int_0^1g(x)\mathrm{d}x}$, which is the same to $\displaystyle{\int_0^1g(x)\mathrm{d}x=1-s}$. If $g(s)>\frac{1}{2}$ and $f$ IS CONTINUOUS, we have $g(x)$ is convex on $[0,1]$. I used some inequalities to show that 
$$\int_0^1g(x)\ge\frac{1-s}{2(1-g(s))}$$ and we get $g(s)\le\frac{1}{2}$, follws the contradiction. But the question is that I want to find a solution WITHOUT continuity (or this solution can be transformed) and I'd like this progress to be more ""analytical"" because I established a coordinate system to prove the inequality. Thanks for your help!","['integration', 'calculus']"
2084885,Dirac delta function forcing in 1st order ODE?,"We have $$\frac{dT}{dt}=-a(T-T_{\infty})-\delta (t-1)$$ where $T_{\infty}$ is a constant and $\delta$ is the Dirac delta. Determine the jump (discontinuity) condition for $T$ at $t=1$ and
  hence find $T(t)$ for $t>1$. I'm just a little puzzled as this is first order. I have only ever done second order before. Is $T$ continuous at $t=1$ here? What is the jump condition? I think I just need a run down of what is actually going on here, for me this is very much a method at the moment, I see these questions I have a procedure to solve them rather than actually comprehending everything that's going on. My method doesn't work here do I can't do it. Any help is appreciated.
Thank you.","['dirac-delta', 'ordinary-differential-equations']"
2084898,understanding holomorphic Functions,"I'm a bit unsure about how to know when a function is holomorphic or not for example:
$$ f(z)= 2Re(z)-iz^2 $$
for what values of z is f holomorphic","['complex-analysis', 'holomorphic-functions']"
2084911,Integrability of $f(t) =\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}} \frac{\Gamma \left( \frac{it+1}{1.5} \right) }{\Gamma \frac{ it+1}{2} }$,"Can we show that the following  function is integrable \begin{align}
f(t) =\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}} \frac{\Gamma \left( \frac{it+1}{1.5} \right) }{\Gamma \left(\frac{ it+1}{2} \right)},
\end{align}
where $t \in \mathbb{R}$ and $i=\sqrt{-1}$. That is can we show that 
\begin{align}
\int_{-\infty}^{\infty} |f(t)| dt<\infty.
\end{align} I was wondering if Stirling's approximation can be used, since this is a complex case? Note if Stirling's approximation can be used than
\begin{align}
f(t) \approx  \sqrt{\frac{1.5}{2}}\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}} \frac{ \left( \frac{it+1}{1.5 e} \right)^{\frac{1+it}{1.5}} }{ \left(\frac{ it+1}{2 e} \right)^{\frac{1+it}{2}}}.
\end{align} Note that 
\begin{align}
|f(t)| &\approx \left| \frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}}\right| \left|  \frac{ \left( \frac{it+1}{1.5 e} \right)^{\frac{1+it}{ 1.5}} }{ \left(\frac{ it+1}{2 e} \right)^{\frac{1+it}{2}}}\right|\\
&=\left| \frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}}\right| \left|  \frac{ \left( it+1\right)^\frac{1+it}{1.5} }{ \left( it+1 \right)^{\frac{1+it}{2}}}\right|    \left|\frac{ \left(2 e\right)^{\frac{1+it}{2 }}} {(1.5 e)^{\frac{1+it}{ 1.5}}} \right|
\end{align} Also we have that 
\begin{align}
&\left|\frac{2^{\frac{it+1}{1.5}}}{2^{\frac{it+1}{2}}}\right|= 2^{\frac{2}{3}-\frac{1}{2}} \\
& \left|\frac{ \left(2 e\right)^{\frac{1+it}{2 }}} {(1.5 e)^{\frac{1+it}{ 1.5}}} \right| =\frac{ \left(2 e\right)^{\frac{1}{2 }}} {(1.5 e)^{\frac{1}{ 1.5}}}
\end{align} So, in the end we if everthing is corect we have to show that 
\begin{align}
g(t)=\left|  \frac{ \left( it+1\right)^\frac{1+it}{1.5} }{ \left( it+1 \right)^{\frac{1+it}{2}}}\right| 
\end{align}
is integrable, but I am not sure how to show if the above equation is integrable or not? Any ideas would be greatly appreciated. Thank you. Edit The integrability of $g(t)$ has been shown in one of the answers. My question now is the following: Since we have that
\begin{align}
|f(t)| =|g(t)| +e 
\end{align}
and $g(t)$ is integrable does this mean that $f(t)$ is inegrable?  Can we prove that the error term $e$ is also integrable?","['complex-analysis', 'real-analysis', 'integration', 'gamma-function']"
2084925,A trick to simplify $z\ln z+\overline{z}\ln\overline{z}$?,"Q: Given complex conjugates $z = a+bi$ and $\overline z = a-bi$, what would be substitution needed to find $R$,
  $$R= z\ln z+\overline{z}\ln\overline{z}\tag1 $$
  such that $R$ is an expression without imaginary numbers? This question arose as a special case of this post . Given $x^3-x-1=0$, let $x_1\approx1.3247$ be its real root (the plastic constant ) and $x_2,x_3$ its complex roots. Then $$\begin{aligned}\sum\limits_{n=0}^\infty\frac{n!\,(2n)!}{(3n+2)!}
&=  x_1\ln(1+x_1)+x_2\ln(1+x_2)+x_3\ln(1+x_3)\\ 
&= \frac32\,x_1\ln(1+x_1)-\frac{1}{2}\sqrt{\frac{3-x_1}{x_1}}\arccos\Big(\frac{x_1-6}{6x_1+2}\Big)\\[2mm]
&= 0.5179778\dots\end{aligned}$$ The first is by yours truly, while the second is a simplified version of a result by Reshetnikov . How do we show the equality of the first two lines? In general, how do we get rid of the imaginary unit in $(1)$, and can we always express it as the sum of $\ln$ and $\arccos$ of real numbers?","['logarithms', 'real-numbers', 'trigonometry', 'complex-numbers']"
2084945,Inequality for Hardy-Littlewood type function,"Consider $r\gt0,x\in R^n$ and the average  of a 1-Lipschitz function $u:R^n\longrightarrow R$ over the $B(x,r) $ $$u_{r}(x)=\frac{1}{Vol(B(x,r))}\int_{B(x,r)}u(z)dz$$ Then there is an absolute constant such that $$\Vert\nabla{u_{r}}(x)-\nabla{u_{r}(y)}\Vert_{2}\leq{\frac{c\sqrt{n}}{r}}\Vert{x-y}\Vert_{2}$$ we can transform and use the unit ball instead along with $u(x+rz)$ and Leibniz rule to differentiate under the integral.Then the $\sqrt{n} $ comes from bounding the norm of the gradient of $u$...but i can't get to the ratio $1/r$ which seems to come from the ratio of surface/volume...","['multivariable-calculus', 'harmonic-analysis', 'lipschitz-functions', 'geometric-inequalities']"
2084975,How to solve distribution function integral,"Given $f_{X,Y}(x,y)=\begin{cases}\frac{1}{2}x^2e^{-x}e^{-y} \ &x,y >0 \\ 0 & \text{O.T.} \end{cases}$ and $Z=X+Y$ Find $f_Z(z)$ My idea is to first find the distribution function $F_Z(z)=P(Z\leq z)=P(X+Y\leq z)=P((X,Y)\in A)$, where $A=\{(x,y)\mid x\leq z-y\}$ This leads to the double integral $$\int_0^\infty \int_0^{z-y} \frac 1 2 x^2e^{-x} e^{-y} \ \ dx \ dy$$ From here I´m kinda stuck as i can´t get a useful result out of the integral. Is the integral wrong?","['statistics', 'probability-distributions']"
2084979,"If, in a triangle, $\cos(A) + \cos(B) + 2\cos(C) = 2$ prove that the sides of the triangle are in AP","By using the formula : 
$$
\cos(A)+\cos(B)+\cos(C) = 1 + 4 \sin\left(\frac{A}{2}\right)\sin\left(\frac{B}{2}\right) \sin\left(\frac{C}{2}\right)
$$ I've managed to simplify it to : $$
2\sin\left(\frac{A}{2}\right)\sin\left(\frac{B}{2}\right)=\sin\left(\frac{C}{2}\right)$$ But I have no idea how to proceed.","['trigonometry', 'triangles']"
2084982,BdPhO 2016 Category C Set 4 Problem 10,"In the above picture, the floor is a mirror. So, $\angle AOP = \angle POB$ and $AO + BO = 12cm$ , find $y$ . I started with $OB = 12-AO$ but that is leading me to too much complexity that I can't handle. How should I start? Full solution of any hint will be helpful. Note : This is a problem from BdPhO 2016. But as it is truly math related so I posted it here.","['algebra-precalculus', 'contest-math', 'plane-geometry', 'trigonometry']"
2085040,Singularity positive semidefinite,The determinant of a matrix equals the product of its eigenvalues. A positive semidefinite matrix is a symmetric matrix with only nonnegative eigenvalues. A positive definite matrix is a symmetric matrix with only positive eigenvalues. Combining (1) and (3) yields that a positive definite matrix is always nonsingular since its determinant never becomes zero. Is is true that for a positive semidefinite matrix at least one of its eigenvalues equals zero and thus its determinant always equals zero => a positive semidefinite matrix is always singular? You would say that specifically having a positive semidefinite matrix instead of a positive definite matrix implies that at least one of the eigenvalues equals zero. Is this correct?,"['matrices', 'positive-definite', 'positive-semidefinite', 'determinant']"
2085072,Sampling Distribution of sample mean for Poisson Distribution,"I am particularly struggling with part b, I don't know where to begin. For part a, I think the answer is that the sampling distribution is a Poisson(n$\lambda$).",['statistics']
2085075,A finite non commutative ring of a specific cardinal,"All rings are supposed to have a unit element $1$ here. Let $N\subset \mathbb N_{\geqslant 2}$ be the set such that $n\in N $ if, and only if, there does not exist a non commutative ring of cardinal $n$. For instance, $16\notin N$ because $A=M_2(\mathbb F_2)$ is a non commutative ring and $\vert A\vert=2^4=16$. In other words: $$N=\{n\geqslant 2,\ \text{all rings with unit of cardinality $n$ are commutative}\}.$$ My conjecture is the following (I do not have a strong believe of its veracity though): Conjecture. The set $N$ is equal to the set $\mathbb P$ of prime numbers. What I did. We can start by proving that $\mathbb P\subset N$. Let $(A,+,\times)$ be a ring of cardinal $p$ where $p$ is a prime number. Then $(A,+)$ is a group, and because $p$ is a prime number: $$(A,+)\simeq \mathbb Z/p\mathbb Z.$$ So if $a\in A\setminus \{0\}$, then $a$ generate $(A,+)$, so there exists $n\in \mathbb Z$ (such that $p\nmid n$) such that $na=1$. Since we can look at $na$ in $\mathbb Z/p\mathbb Z$, $a$ is invertible in $A$, so $A$ is a field. We can than conclude by Wedderburn theorem that $A$ is commutative, so $p\in N$. We can prove that $n\notin N$ where $n$ is of the form: $$n=p^{km^2}$$ where $p$ is a prime number, $k\in \mathbb N_{\geqslant 1}$ and $m\in \mathbb N_{\geqslant 2}$. We can look at $A=M_m(\mathbb F_{p^k})$ which is a non commutative finite ring under our hypotheses. We can prove that $n\notin N$ where $n$ is of the form: $$n=p^{k\frac{m(m+1)}2}$$ where $p$ is a prime number, $k\in \mathbb N_{\geqslant 1}$ and $m\in \mathbb N_{\geqslant 2}$. We can look at $A=T_m(\mathbb F_{p^k})$ the subset of upper triangular matrix of $M_m(\mathbb F_{p^k})$ which is a non commutative finite ring under our hypotheses. My questions. I would tend to think that $4$ and $6$ are elements of $N$, but I am unable to find a counter-example. I wonder whether or not $p^q\in N$ where $p,q$ are prime numbers and $q>3$. More generally, I am interested of any kind of information about the set $N$.","['abstract-algebra', 'ring-theory', 'noncommutative-algebra', 'prime-numbers']"
2085080,Proof of Almost uniform convergence implies Convergence almost everywhere,"I read the same proof that almost uniform convergence implies convergence almost everywhere on several sources (Friedman's Foundations of Modern Analysis and online sources), and they all seem to use the same proof: Proof taken from Proofwiki, https://proofwiki.org/wiki/Convergence_a.u._Implies_Convergence_a.e. However, I have a problem with this proof. While the set on which $f_n$ does not converge to $f$ is definitely a subset of $B$, I do not see why the reverse inclusion (that $f_n$ does not converge to $f$ for every element of $B$) is true. Hence, might it not be possible that $f_n$ converges to $f$only on a proper subset of $B$ that just happens to be non-measurable? (Which is possible since the measure space is not specified to be complete.) Then the proof would be false. I would really appreciate help in understanding why my critique of the proof does not hold. Thanks in advance.","['almost-everywhere', 'uniform-convergence', 'measure-theory', 'convergence-divergence', 'analysis']"
2085082,Where is the mass of a hypercube?,"Question : Consider a hypercube $C := \{\vec{x}\in\mathbb{R}^n : \forall(i)\: |x_i|\leq 1\}$ and hypersphere $S_r := \{\vec{x}\in\mathbb{R}^n : |\vec{x}| = r\}$.  Let $R(n)$ be the radius of the sphere for which the intersection $C\cap S_{R(n)}$ has maximal area.  What is the asymptotic behavior of $R(n)$ as $n\rightarrow \infty$? Background : For an n dimensional ball, the mass on a shell of radius $r$ grows like $r^{n-1}$ right up to the boundary of the ball.  Knowing where the mass is located on a high dimensional object is useful for e.g. statistical mechanics. By comparison to the ball case, you might expect that the mass of a cube would also be near the farthest extremity, which for the case of $C$, above, would be at the vertices, distance $\sqrt{n}$ from the center.  However, there is an argument why there is not much mass near the vertices:  Consider centering little balls on each of the vertices and looking at the volume of the intersection of the balls with the cube (see figure).  Each ball intersects the cube only in a sector of $1/2^n$ of the ball, and there are $2^n$ vertices, so the total volume of the balls' intersections is just the volume of one ball.  Thus there is the same volume in a neighborhood of the vertices as there is in a neighborhood of the center of the cube (or any other interior point)! Hence the above question: What shell of the cube has the most mass?  It's easy to see that $1<R(n)<\sqrt{n}$.  Can you do better?","['volume', 'area', 'polytopes', 'geometry']"
2085085,"Is a matrix diagonalizable, if one of its eigenvalues is zero?","I checked weather the following matrix is diagonalizable. $$A=\begin{bmatrix}
4 & 0 & 4\\
0 & 4 & 4\\
4 & 4 & 8
\end{bmatrix}$$ And the corresponding eigenvalues were $0$, $4$, $12$. Now if we write the similar diagonal matrix to $A$ it would be, $$D=\begin{bmatrix}
0 & 0 & 0\\
0 & 4 & 0\\
0 & 0 & 12
\end{bmatrix}$$ which is theoretically not a diagonal matrix. Now is $A$ diagonalizable?","['eigenvalues-eigenvectors', 'diagonalization', 'linear-algebra']"
2085091,Decrease rate of the shadow,"If a man of height $6ft$ moves with $5ft/sec$ velocity towards a lamp hanging at $15ft$ height, then at what rate his shadow will decrease? My Work : Let the initial distance between the man and the lamp post be $x$ and the length of shadow be $y$. Then, $\frac{x}{9} = \frac{y}{6} \implies y = \frac{2x}{3}$ Now how do I handle the $5ft/sec$ velocity and the decrease rate of the shadow? I think the rest part is so easy. But I am missing somewhere. Note : This is a problem from BdPhO 2016 but completely math related. So I posted it here.","['derivatives', 'contest-math', 'linear-algebra', 'calculus']"
2085108,A certain kind of set of integers,"Let us call a set $S$ of integers sparse if for any four integers $a,b,c,d \in S$ such that $a<b<c<d$, we have that $a+d \ne b+c$. Define $P(n)$ to be maximal size of a sparse subset of $\{1,2,\dotsc,n\}$. Is it true that $\lim\limits_{n\to \infty} P(n)/n=0$?","['number-theory', 'additive-combinatorics']"
2085120,The Poker Hand or Don't Call Out the Professor,"This goes back to a problem given to my statistics class, 50 years ago. You are dealt 5 cards from a standard deck of 52. What are the odds of you having the Queen of Spades? The Professor said to work on it and be prepared to answer the following day. I said I could give the correct answer right then, which I  did. We took most of the next class discussing/arguing about the answer. I may have won the argument but lost the war (grade). What is the correct answer?",['statistics']
2085132,why is $\mathbb{C}P^n$ a strong neighbourhood-deformation retraction of $\mathbb{C}P^{n+1}$,"Let $X$ be a topological space. A subspace $A\subset X$ is said to be a strong neighbourhood-deformation retract of $X$, if there is an open neighbourhood $U$ of $A$ and a continuous map $h:U\times [0,1]\to U$ such that $h(u,0)=u$, $\;h(u,1)\in A$ and $h(a,t)=a$ for all $u\in U$ and $(a,t)\in A\times [0,1]$. (Note that this is not exactly the same definition of $A$ strong deformation rectract in $X$ as you can see here https://en.wikipedia.org/wiki/Retract .. However, $A$ is a strong deformation retract in $U$). Consider the complex projective space $\mathbb{C}P^n:=\mathbb{C}^{n+1}\setminus \{0\} /\sim$, where $x,y\in \mathbb{C}^{n+1}\setminus \{0\}$ satisfy $x\sim y$ $:\iff \exists \lambda\in \mathbb{C}\setminus \{0\}$ such that $x=\lambda y$. I know that $\mathbb{C}P^n\cong S^{2n+1}/x\sim \lambda x,\;\lambda\in S^1$. My question: Why is $\mathbb{C}P^n$ a strong neighbourhood-deformation retraction of $\mathbb{C}P^{n+1}$? I need this fact for other calculations, but I don't know why this is true. I appreciate your help. Edit : Can I use that $S^{n-1}$ is a strong neighbourhood deformation retract of $D^n$ (with $U=\mathbb{R}^n\setminus \{0\}$ ) ?","['algebraic-topology', 'general-topology', 'projective-space']"
2085183,Definition of closed immersion.,"Here is the definition of closed immersion given on Stacks Project. In Hartshorne (II, Section 3), a closed immersion of schemes is only defined by the first two properties.  What does ""locally generated by sections"" mean?  And why is it considered to be an essential characteristic of a closed immersion?",['algebraic-geometry']
2085184,How to compute $E[\log(X)]$ when $X$ follows a beta distribution?,"Given a Beta variable $X \sim B(\alpha\ge 2,\beta)$ , how do I compute the expectation of its logarithm $E[\log(X)]$ ? This is deemed ""obvious"" on MO , but I see no easy way to compute $\int_0^1 x^{\alpha-1}(1-x)^{\beta-1}\log x \; dx$ . Differentiating Beta function $B(\alpha,\beta)=\int_0^1 x^{\alpha-1}(1-x)^{\beta-1}dx$ by $\alpha$ results in Digamma function - is this really the way to go? Thanks.",['probability']
2085206,Dominated or Monotone convergence for $\dfrac{1+nx^2}{(1+x)^n}$,"I am having a particularly difficult time working with the sequence of functions given by: $$f_n(x)=\dfrac{1+nx^2}{(1+x)^n}.$$ I am working only on $[0,1]$. It is clear that these are measurable functions which go to $0$ pointwise, (though $f_n(0)=1\;\forall n\in\mathbb{N}$) and I would like to use either the Dominated or Monotone Convergence Theorems to show that: $$\lim\limits_{n\to\infty}\int\limits_0^1f_n(x)=0.$$ I know it is true based on the context in which I received the problem, but it is proving to be a tough nut to crack. I'm usually good at these. A little noodling around on Desmos shows that the sequence is in fact dominated on $[0,1]$ by any constant function greater than 1, and that for $n\geq 3$ it is decreasing on $[0,1]$. For $n=1$ or $n=2$, it has a minimum between $0$ and $1$. But these things are, for the most part, incredibly tedious to show rigorously. This was a question on a prior preliminary exam in my department, and was written to take a few minutes at most. Is there a quicker way? All these details have taken me almost an hour to write out!","['real-analysis', 'calculus', 'analysis']"
2085208,What does the denominator in the second derivative mean? [duplicate],"This question already has answers here : Why is the 2nd derivative written as $\frac{\mathrm d^2y}{\mathrm dx^2}$? (6 answers) Closed 4 months ago . This occurred to me a few days ago. We know that the derivative of a function $y=f(x)$ is $\frac{dy}{dx}$.
This is because it represents how $y$ changes with $x$, which is the rate of change of $y$, or more specifically, the gradient of a function. Then the second derivative is the rate of change of rate of change, or the rate of change of gradient. Since a general rate of change is $\frac{d}{dx}$, the second derivative is $(\frac{d}{dx})(\frac{dy}{dx})$. Thus, the expanded form is $\frac{d^2y}{dx^2}$. My question is, is the denominator $d(x)^2$ or is it $(dx)^2$? Surely, it would be the latter, because when you expand $(\frac{d}{dx})(\frac{dy}{dx})$, the $(dx)(dx)$ would become $(dx)^2$. But then why is it never written with brackets? I'm sure that would confuse some people and I only realised it myself when I started thinking about the second derivative properly, in terms of what it actually means.","['derivatives', 'ordinary-differential-equations', 'implicit-differentiation']"
2085222,A morphism of sheaves of sets is an isomorphism if it induces an isomorphism of all stalks.,"This is Exercise 2.4.E. from Vakil's notes . Show that a morphism of shaves of sets is an isomorphism if and only if it induces an isomorphism of all stalks. My failed attempts: I found some similar questions here and here . But I still couldn't figure out the details. I have shown the ""only if"" part. I have also shown that if the morphism induces an isomorphism of stalks, it is injective. The surjective part confused me a lot. Here is a commutative diagram, where the vertical maps are injections: $$\require{AMScd}
\begin{CD}
\mathscr{F}(U) @>{\phi(U)}>> \mathscr{G}(U)\\
@VVV @VVV \\
\prod_{p\in U} \mathscr{F}_p @>{\phi_p}>> \prod_{p\in U} \mathscr{G}_p
\end{CD}$$ To show surjectivity, suppose $g\in \mathscr{G}(U)$ . Its image in the stalks at $p$ is $(g,U)_p$ . Since $\phi_p$ is an isomorphism for all $p$ , we have $(f_p,U_p)\in \mathscr{F}_p$ , such that $\phi_p(f_p,U_p)=(g,U)_p$ for all $p\in U$ . Now I don't know how to glue the $f_p$ together. Since even we have the pairs $(f_q,U_q)\in \mathscr{F}_q$ , we cannot show that their restrictions on $U_p\cap U_q$ are equal. We have to find a small enough open set to glue them. I also tried the open cover argument from the right hand side: Since $\prod(g,U)_p$ consists of compatible germs, there exists an open cover $U_i$ , and section $g_i$ in $\mathscr{G}(U_i)$ , such that $(g,U)_p=(g_i,U_i)$ for all $p\in U_i$ . But this doesn't help much. Since $\phi_p$ are different maps, we still cannot conclude anything about the $(f_p,U_p)$ . Any help will be greatly appreciated!","['category-theory', 'sheaf-theory', 'algebraic-geometry']"
2085240,Notation for removal of row / column from matrix,"Is there some common notation for the result of removing the $i$th row, the $j$th column or both of them from a matrix given $A$?","['matrices', 'notation', 'linear-algebra']"
2085253,Integer solutions to nonlinear system of equations $(x+1)^2+y^2 = (x+2)^2+z^2$ and $(x+2)^2+z^2 = (x+3)^2+w^2$,"Do there exist integers $x,y,z,w$ that satisfy \begin{align*}(x+1)^2+y^2 &= (x+2)^2+z^2\\(x+2)^2+z^2 &= (x+3)^2+w^2?\end{align*} I was thinking about trying to show by contradiction that no such integers exist. The first equation gives $y^2 = 2x+3+z^2$ while the second gives $z^2 = 2x+5+w^2$. How can we find a contradiction from here?","['number-theory', 'diophantine-equations', 'nonlinear-system', 'systems-of-equations']"
2085268,Does $i^4$ equal $1?$,"I can't seem to find a solution to this for the life of me. My mathematics teacher didn't know either. Edit: I asked the teacher that usually teaches my course today, and she said it was incredible that the other teacher didn't know. My logic goes as follows: any real number: $x$ to the fourth power is equal to $(x^2)^2$. Using this logic, $i^4$ would be equal to $(i^2)^2$. This would result in $(-1)^2$, and $(-1)^2 = 1$. Obviously, this logic can be applied to any real numbers, but does it also apply to complex numbers?","['algebra-precalculus', 'complex-numbers', 'proof-verification', 'exponentiation']"
2085291,Holomorphic Function is Injective in a Neighborhood where it has Nonzero Derivative,"Suppose $f : \Omega \to \mathbb{C}$ is a holomorphic function on some open set $\Omega.$ If $f'(z)\neq 0$ for some $z\in \Omega,$ does there necessarily exist a neighborhood $U$ of $z$ where $f$ is injective?",['complex-analysis']
2085313,Asymptotic behavior of Harmonic-like series $\sum_{n=1}^{k} \psi(n) \psi'(n) - (\ln k)^2/2$ as $k \to \infty$,"I would like to obtain a closed form for the following limit:
$$I_2=\lim_{k\to \infty} \left ( - (\ln k)^2/2 +\sum_{n=1}^{k} \psi(n) \, \psi'(n)  \right)$$ Here $\psi(n)$ is digamma function. Using the method detailed in this answer , 
I was able to compute simpler, related series:
$$\lim_{k\to \infty} \sum_{n=1}^{k} \left (\psi'(n) -1/n \right) =1 $$
$$\sum_{n=1}^{\infty} \psi''(n) =-\frac{\pi^2}{3} $$
But $I_2$ seems to be tougher because of the product of two digamma's.
The divergence of $(\ln k)^2$ is matched by the first terms of asymptotic series for $\psi(n) \psi'(n)$,  via the definition of the Stieltjes number , $$ \lim_{k\to \infty} \left ( \sum_{n=1}^{k} \frac{\ln n}{n}  - (\ln k)^2/2 \right ) =\gamma_1 $$
but I am stuck with the reminder term. Side remark : the problem originates in physics, see my older question . In particular, I was able to show that $\langle x \rangle \approx -0.251022$ defined in that question actually equals exactly  $-(1+\gamma_0)/(2 \pi)$ where $\gamma_0$ is Euler's constant. This answer I seek here is the only piece missing on my path to closed form $\langle x^2 \rangle$.","['polygamma', 'asymptotics', 'sequences-and-series', 'stieltjes-constants']"
