question_id,title,body,tags
4212726,Coupon collectors derivation,"Yesterday, a user posted the following derivation of the coupon collectors problem but unfortunately deleted his question: There are $n$ distinct characters one could obtain from a card package,
whereby each card package you buy (one such package contains exactly one card) contains each of those characters
with probability $\frac{1}{n}$ . Compute the expected number of card packages
you need to buy so that you have each of the $n$ distinct characters
at least once. Let $\Omega$ be our sample space and $X\colon \Omega\to \mathbb{R}$ the random
variable that counts the number of card packages we have to buy in order
to obtain each of the $n$ distinct characters at least once. Moreover,
let $X_i$ be an indicator variable with $$
X_i = \begin{cases}
	1, \ \text{the $i$th character we obtain that is distinct to all
	others we have collected so far}\\
	0, \ \text{otherwise}
\end{cases}
$$ Now we find that $P(X_i) = 1-\frac{i-1}{n} = \frac{n-i+1}{n}$ since the probability that we get one of the $i-1$ characters that we have already
is $i-\frac{1}{n}$ . Moreover, it's clear that $\Omega= X_1 \uplus X_1^c$ where $X_1^c$ denotes the complementary event of $X_1$ .
Now I used the total expectation theorem to obtain \begin{align*}
	\mathbb{E}[X] = (\mathbb{E}[X|X_1] + 1)\cdot P(X_1) + 
	(\mathbb{E}[X|X_1^{c}] + 1) \cdot P(X_1^{c})
.\end{align*} whereby the $+1$ denotes the fact that we have bought a card package.
We notice that $\mathbb{E}[X|X_i^{c}] = \mathbb{E}[X]$ since we haven't obtained
any new card. With this we obtain \begin{align*}
	&\mathbb{E}[X] = \mathbb{E}[X|X_1]\cdot P(X_1) + P(X_1)
	+ \mathbb{E}[X]\cdot  P(X_1^{c})+ P(X_1^{c})
	\\[15pt]
	\iff &\mathbb{E}[X]\cdot (1-P(X_1^{c})) = \mathbb{E}[X|X_1]\cdot P(X_1) + P(X_1)
	+ P(X_1^{c})
.\end{align*} Since $P(X_1^{c}) = 1-P(X_1)$ this becomes \begin{align*}
	\mathbb{E}[X] \cdot P(X_1) = \mathbb{E}[X|X_1] \cdot P(X_1) + 1
	\iff \mathbb{E}[X] = \mathbb{E}[X|X_1] + \frac{1}{P(X_1)}
	= E[X|X_1] + \frac{n}{1}
.\end{align*} We can repeat this procedure since $X_2 \uplus X_2^{c}$ is again a disjoint
partition of our sample space so we obtain \begin{align*}
	\mathbb{E}[X] = \mathbb{E}[X|X_1, X_2] + \frac{n}{2} + \frac{n}{1}
.\end{align*} I would now argue inductively that \begin{align*}
	\mathbb{E}[X] = \mathbb{E}[X|X_1, X_2, \ldots, X_{i}] + \sum_{k = 1}^{i} \frac{n}{i}
.\end{align*} so in the end one has \begin{align*}
	\mathbb{E}[X] = \sum_{k =  1}^{n} \frac{n}{k} =  n\sum_{k = 1}^{n}\frac{1}{k}
\end{align*} since the ""recursion"" ends with $\mathbb{E}[X|X_1, \ldots, X_n] = 0$ the ""recursion"" ends. My problem is the $+1$ he has in the application of the total expectation, which is intuitively clear to me but how would one explain this formally since the total expectation theorem actually would look like $$
\mathbb{E}[X] = \mathbb{E}[X|X_1] \cdot P(X_1) + \mathbb{E}[X|X_1^{c}]
\cdot (1-P(X_1))
$$ without any $1$ 's, so how could one make this more rigorous?
How does one explain $\mathbb{E}[X] = \mathbb{E}[X|X_1^{c}]$ rigorously? Edit: I think it suffices for the latter to just say that since $X_1^c$ means that we haven't obtained any new card this doesn't influence the expected value of $X$ .","['coupon-collector', 'solution-verification', 'combinatorics', 'probability']"
4212829,"Range of $y=a\sin(x)+b\,\mathrm{cosec}(x)$ where $a$ and $b$ are real and $a,b\ne0$","I’m trying to figure out the range of the function $y=a\sin(x)+b\,\mathrm{cosec}(x)\;$ where $a,b\in\mathbb R\setminus\{0\}$ , without using calculus (as I have not started it yet) Here is my approach: $y=a\sin x+b\csc x$ $\implies a\sin^2(x)-y\sin(x)+b=0$ $\implies \sin(x)=\dfrac{y\pm\sqrt{y^2-4ab}}{2a}$ now the inequality under the square root yields $y=\left(-\infty,-2\sqrt{ab}\,\right]\cup\left[2\sqrt{ab},+\infty\right)\;\;$ if $\,ab>0$ or, $\;y=\mathbb R\;\;$ if $\,ab<0\;.$ But $\;\sin(x)=\dfrac{y\pm\sqrt{y^2-4ab}}{2a}\in[-1,1]\setminus\{0\}$ the interval should impose additional restrictions on the value of "" $y$ "". So how do I solve for "" $y$ "" from the interval? And what is the range? After analysing some graphs of asin(x)+bcosec(x) for different values of a and b,this is what I came up with: 1.If ab>0 and |a|≤|b|; y=(-∞,-|a+b|]∪[|a+b|,+∞) 2.If ab>0 and |a|>|b|;  y=(-∞,-2√(ab)]∪[2√(ab),+∞) 3.If ab<0 and |a|<|b|;  y=(-∞,-|a+b|]∪[|a+b|,+∞) 4.If ab<0 and |a|≥|b|;  y= R (where a and b are the coefficients of sin(x) and cosec(x) respectively) Is it correct?","['algebra-precalculus', 'functions', 'trigonometry']"
4212850,Tangent basis to Ellipsoid,"I have an ellipsoid centered at $0$ (the contour of a Gaussian distribution centered at $0$ with covariance matrix $\Sigma=\Lambda^{-1}$ ) $$
x^\top \Lambda x = \gamma
$$ and I know that the gradient at a point is given by $$
g(x) = -\Lambda x
$$ Is there an expression for the tangent at a point? All I know is that $t(x)^\top g(x) = 0$ . In practice the tangent plane will be a hyperplane so there will be many vectors to choose from. However, I am looking for a tangent basis","['ellipsoids', 'analytic-geometry', 'geometry', 'tangent-spaces', 'differential-geometry']"
4212855,Definition of Sampling,"I am no expert in statistics, I know statistics at engineering level.
I've been reading through measure theory and probability theory lately (still not an expert but definitely more solid than my statistics background at the moment). Suppose you have a measure space $(\Omega,\mathcal{F},\mu)$ where $\mu$ is a probability measure. Suppose $X$ is a random variable. What I am looking for is a rigorous (if there's any) definition of ""sampling"" of that random variable. Or maybe what I am looking is more how to get ""samples"" from a probability distribution (again a rigorous definition). I know a standard ""algorithm"" to perform sampling, however I am more looking for a mathematical definition of sampling. I am either looking for an answer or a some rigorous definition somewhere. Just to give an analogue in signal processing if you have a signal $s : \mathcal{T} \to \mathbb{R}$ (time set to real values) we can define the sampling at rate say $T = \frac{1}{f}$ as $\left\{ s(nT) \right\}_{n \in \mathbb{Z}}$ and this is a well understood mathematical definition to me which I can rephrase as composition of $s$ with the map $I_T : \mathbb{N} \to \mathcal{T}$ that given $n$ returns $nT$ hence $$
s(nT) = \left(s \circ I_T\right)(n)
$$ I do struggle however with sampling in probability terms.
Can you help?","['measure-theory', 'statistics', 'definition', 'probability']"
4212875,Making sense of 'Floppy' Structures on Manifolds,"Roger Penrose in his book ""The Road to Reality"" (Section 14.8 - Symplectic Manifolds) loosely defines a ""floppy"" structure to be one which if we apply two variants of it on two copies of the same manifold, the two manifolds are locally isomorphic (and thus locally indistinguishable from one another). The example he brings is that of two symplectic manifolds with the same dimension and signature. I think what he is referring to (although he does not make it explicit) is Darboux's Theorem which again asserts that two symplectic manifolds of the same dimension are locally isomorphic (more accurately: symplectomorphic ). In his exact words and I quote: The local structure of a symplectic manifold is an example of what
might be called a ‘Floppy’ structure. There is, for example, no notion
of curvature for a symplectic manifold, which might serve to
distinguish one symplectic manifold from another, locally. If we have
two real symplectic manifolds of the same dimension (and the same
‘signature’, cf. §13.10), then they are locally completely identical
(in the sense that for any point p in one manifold and any point q in
the other, there are open sets of p and q that are identical). This
is in stark contrast with the case of (pseudo-) Riemannian manifolds,
or manifolds in which merely a connection is specified. In those
cases, the curvature tensor (and, for example, its various covariant
derivatives) defines some distinguishing local structure which is
likely to be different for different such manifolds. This makes sense to me. Then he goes on to describe two more examples of manifolds one of which is floppy while the other isn't. In particular he makes the following two claims: Let $M_1$ be a real manifold with a nowhere vanishing vector field $F$ on it. Then $M_1$ is a floppy manifold. In contrast, let $M_2$ be a real manifold with two general vector fields. Then $M_2$ is NOT a floppy manifold Question 1: For the 1st part, I think the family of tori would be a good place to start since assigning a (smooth) nowhere vanishing vector field is always possible. I can also see how we can use $F$ to define a global frame on the entire $M_1$ starting by setting $\partial_1 := F \neq 0$ etc. However, in what sense are all tori (with $F$ ) locally indistinguishable? For example, shouldn't we able to (locally) distinguish a torus with circular cross sections versus one with elliptical just by looking at the difference in curvatures between a circle and an ellipse? Question 2: Now to his second claim. First of all, what do you think he means by ""general"" vector fields? Am I right to assume that they are linearly independent and by extension neither one of these can be anywhere vanishing (as this would violate independence)? And how does the existence of the second field makes them distinguishable?","['symplectic-geometry', 'smooth-manifolds', 'soft-question', 'mathematical-physics', 'differential-geometry']"
4212877,"Given finite sets $A$ and $B$, is $\max(|A|,|B|) = |A \cap B| \iff A = B$ true?","Given finite sets $A$ and $B$ , is $$
\max(|A|,|B|) = |A \cap B| \iff A = B
$$ true? Here is my attempt to prove that this statement is true. To prove $$
\max(|A|,|B|) = |A \cap B| \Longleftarrow A = B,
$$ suppose that $A = B$ . Then, $$
\max(|A|,|B|) = \max(|A|,|A|) = |A| = |A \cap A| = |A|.
$$ To prove $$
\max(|A|,|B|) = |A \cap B| \Longrightarrow A = B,
$$ suppose that $\max(|A|,|B|) = |A \cap B|$ . If $|A| \geq |B|$ , then $$
|A| = |A \cap B|.
$$ At this point, I am not sure if this implies that $A = A \cap B$ . Even if it does, this would then imply that $A \subset B$ , but I am not able to show that $B \subset A$ as well assuming that $|A| \geq |B|$ .",['elementary-set-theory']
4212907,Question about the definition of intersection in Zermelo–Fraenkel set theory,"I was following the ""Lectures on the Geometric Anatomy of Theoretical Physics"" of Frederick Schuler and at some point he gives as a homework to define intersection $\cap x$ , where $x$ is a set. At that point in the lecture he didn't finish stating all the axioms yet. Now, my first instinct was to write $$\bigcap x =: \{y \in x \mid \forall w \in x : y \in w\},$$ which I also later found in many places when I searched on the internet... (for example on wiki: https://en.wikipedia.org/wiki/Intersection_(set_theory)#Definition ) But then we got to the axiom of foundation, which implies that we cannot write $x \in x$ . So, wouldn't the correct definition for intersection be $$\bigcap x =: \{y \in x \mid \forall w \in x : y \subseteq w\},$$ since we want for example: $\bigcap\{y,\{y,z\}\} = y$ , which has a problem with the first definition of intersection (as $y\in \cap\{y,\{y,z\}\}$ implies that $y\in\{y,z\}$ , but also $y\in y$ )?",['elementary-set-theory']
4212925,"Prove that a line can be found that divides the plane into $2$ parts, each of which contains exactly $n$ points","On the plane for $2n$ distinct points. Prove that: a) A line can be found that divides the plane into $2$ parts, each of which contains exactly $n$ points. b) Assuming that no $3$ of the $2n$ points are collinear, prove that there exist at least $n$ lines passing through the $2$ points that divide the plane into $2$ parts, each containing exactly $n-1$ points. Here all i did : a) We proceed to connect all two points together. Since the number of points on the plane is finite, the number of line segments is finite. So we can find a line that is not parallel to all the given line segments. We call that line the line $d$ . Consider a coordinate system $Oxy$ whose vertical axis $ Oy$ coincides with the line $d$ . Thus, the points on the plane will have distinct diaphragm degrees. Call those diaphragm degrees $ x_1,x_2,...,x_{2n}$ respectively. Proceed to choose the coordinate $m$ so that $x_1<x_2<...<x_n<m<...<x_{2n}$ . So the line $y = m$ is the line to find. But when it comes to the next part, I have no idea at all. I feel that if I deal with the problem like the previous part, I will get nowhere. I hope to get help from everyone. Thanks a lot.","['combinations', 'combinatorics', 'geometry']"
4212948,"Characterization of finite covering spaces, and the existence of a non-finite analogue","For this post I want to assume that $X$ is connected, Hausdorff, locally compact, and semi-locally simply connected topological space but, honestly, I’d be happy to assume that $X$ is a topological manifold (or even a complex manifold). There is a well-known result which characterizes when a map $f\colon Y\to X$ (where $Y$ is also Hausdorff) is a finite covering space in more generally used topological terms. Fact (e.g. see [Ho, Lemma 2]): The map $f\colon Y\to X$ is a finite covering space if and only if the following two conditions
hold: $f$ is a local homeomorphism, $f$ is proper (*). I am interested in understanding whether or not (again perhaps with stronger assumptions on $X$ ) there exists a version of this fact with ‘finite covering space’ replaced by ‘covering space’. Or, perhaps more realistically (since I doubt such a general characterization exists), a more general class of covering spaces which can be characterized in terms similar to this fact. Of course, what needs to be relaxed is that $f$ is proper. For reasons related to my motivation from algebraic geometry, I would like to replace “ $f$ is proper” with “ $f$ is $P$ ” where, intuitively, one has an ‘equality’ $$P=\text{proper}-“\text{quasi-compact}”.$$ For instance, by the contents of Tag 005M (and more explicitly Tag 005R ) one has that $$\text{proper}=\text{quasi-proper}+\text{closed},$$ where one defines a map $f\colon Y\to X$ to be quasi-proper if $f^{-1}(V)$ is quasi-compact for all quasi-compact subsets $V\subseteq X$ (note that since my spaces are going to generally be Hausdorff one can replace ‘quasi-compact’ with ‘compact’). Thus, one might try to replace “proper” with “closed” in the statement of this fact. Of course, “closed” is a very poor choice. For one thing it doesn’t even include the example of the universal covering $\mathbb{R}\to S^1$ . Moreover, “closed” doesn’t even jive with what I am intuitively going for. But maybe this gives an indication of the sort of direction I’d be interested in going. Any suggestions would be greatly appreciated! (*) Here by proper I mean universally closed as in Tag 005O , but I think that one can do away with the assumption that $X$ (and $Y$ ) are Hausdorff by using a slight modification which is more aligned with my algebro-geometric background. Indeed, it might be more correct to write ‘proper’ to mean ‘universally closed and separated’ (where separated is as in Tag 0CY0 ). Of course, if $Y$ and $X$ are Hausdorff, separatedness is automatic. References: [Ho] Ho, C.W., 1975. A note on proper maps. Proceedings of the American Mathematical Society, 51(1), pp.237-241. https://www.ams.org/journals/proc/1975-051-01/S0002-9939-1975-0370471-3/S0002-9939-1975-0370471-3.pdf","['general-topology', 'algebraic-topology', 'covering-spaces']"
4212962,Solve for $x$: $ 4^{4x}-4^x=(4x)!$,Solve for $x$ : $$ 4^{4x}-4^x=(4x)! $$ (for all real values) What is the biggest problem is that induction is not taught to me yet and hence can't be used. Attempt $1$ : Assume $4^x=t$ . So we get $t^4-t=24 (x!)$ . Letting $u$ denote $x!$ $$ t^4-t-24u=0 \implies u=\frac{t}{24}(t^3-1).$$ Attempt $2$ : Taking $4^x$ out as common $$ 4^x(4^{3x}-1)=4x(4x-1)(4x-1)...(2)(1) $$ However this also yielded nothing. Attempt $3$ : What I tried was did it by brute force- Our equation is $4^{4x}-4^x-(4x)!=0$ Let $x$ be $-1$ and only taking the LHS $$ 4^{-4}-4^{-1}-(4(-1))! = \text{ Infinity }$$ We can exclude all negative cases as the value can't be negative but negative factorial don't exist. So putting the value of $0$ : $$ 4^0-4^0-0! \implies -1 $$ Using value of $1$ : $$ 4^4-4-4! \implies 228 $$ This implies the value will be between 1 and 2. However Wolfram Alpha result (which I will discuss at end) don't agree with me. So how do I find the answer to this question without induction? Also why didn't Approach 3 worked for me? Also according to Wolfram Alpha over here how can I find such complex roots? Please help to solve this question?,"['exponentiation', 'algebra-precalculus', 'factorial']"
4212969,Density of invariant measure of stochastic differential equation,"I have a question: is it possible that an SDE has a ""nice"" density, but its invariant measure does not have a ""nice"" density? More precisely, consider a stochastic differential equation $$
dX_t=b(X_t) dt + \sigma(X_t) dW_t,
$$ where $W$ is a standard Brownian motion and $b,\sigma$ are continuous smooth functions (no further assumptions on uniform ellipticity of $\sigma$ or that $b$ is globally Lipschitz or that $b$ is bounded). Suppose that we know that this equation has a unique strong solution and that the transition kernel has a smooth density $p_t(x,y)$ . Assume that we know that this equation has a unique invariant measure $\pi$ . Question: Is it true (without any further assumptions) that $\pi$ has a smooth density $p$ ? Indeed, it is immediate to see that for any measurable set $A$ one has $$
\pi(A)=\int_A\Bigl(\int_{\mathbb{R}^d} p_t(x,y) \pi(dx)\Bigr)dy,
$$ which implies that $\pi$ has a density $p$ such that $$
p(y)=\int_{\mathbb{R}^d} p_t(x,y) p(x)dx.
$$ However it is not clear to me why $p$ is finite everywhere or why it is differentiable everywhere. Can one construct a counter-example here?","['stochastic-processes', 'markov-process', 'partial-differential-equations', 'probability-theory', 'stochastic-calculus']"
4212982,"If the zeros of a holomorphic function $f$ have an accumulation point, then $f$ is constant.","I'm having trouble with the proof of the next statement: Let $U\subset \mathbb{C}$ be open and connected. If $f$ is defined in $U$ and is holomorphic, and the set of its zeros have and accumulation point, then $f$ is constant on $U$ . I know how to prove this when that accumulation point is in $U$ , but I don't know what to do in the other case. Hope you can help me, please. Thank you.","['complex-analysis', 'constants', 'derivatives', 'analysis']"
4213005,Tensor product and dual of $k$-vector spaces over a $k$-algebra: what is the dimension?,"Here is the setting: let $k$ be a field, $V$ a finite dimensional $k$ -vector space with $k$ -dual $V^*$ , $E$ a $k$ -subalgebra of $\mathrm{End}_k(V)$ . $V$ has thus a canonical structure of faithful left $E$ -module. The object that we are interested in is $$ V \otimes_E V^* = V \otimes_k V^* / \langle \{f(v) \otimes \eta - v \otimes f^*(\eta) \mid f \in E, v \in V, \eta \in V^*\} \rangle,$$ where $f^*: \eta \mapsto \eta \circ f$ is the canonical faithful right action of $E$ on $V^*$ . If $V$ is free as an $E$ -module, then so is $V^*$ , and they have the same $E$ -dimension, so that $$ \dim_k(V \otimes_E V^*) = \dim_E(V)^2 \cdot \dim_k(E) = \frac{\dim_k(V)^2}{\dim_k(E)}.$$ If $V$ is not $E$ -free, then the central expression is not defined, but the first and the last one are. Which leads to the question: Question 1: is it always true that $\dim_k(V \otimes_E V^*) =\frac{\dim_k(V)^2}{\dim_k(E)}$ ? If not, does this at least give an upper or a lower bound? Remark: More in general, one could ask whether, given two finite-dimensional $E$ -modules (with $E$ a finite-dimensional $k$ -algebra), we always have $$\dim_k(V \otimes_E W) =\frac{\dim_k(V) \cdot \dim_k(W)}{\dim_k(E)}.$$ This, however, is false , see for instance here for an example. However, maybe the fact that in my case the action of $E$ is faithful and $W=V^*$ are somehow of help... So far for the tensor product. My second question, which is probably closely related, concerns the dual: Question 2: Consider the dual of $V$ as an $E$ -module, i.e., $V^\circ:= \mathrm{Hom}_E(V,E)$ . What is its relationship with $V^*$ ?","['abstract-algebra', 'linear-algebra', 'tensor-products', 'modules']"
4213009,"For a process $A_t$, does $|| \mathbb{E}[A_{t+1}] ||_\infty \leq \delta ||A_t||_\infty$ imply that $A_t \to 0$? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm currently studying some stochastic approximation-like algorithms and their convergence properties. I'm wondering whether some convergence results from deterministic sequences can be transferred to random-processes. Consider a (non-random) sequence $(a_t)_t$ which satifies, for constant $\delta \in (0, 1)$ : $$|a_{t+1}| \leq \delta |a_t| \ \forall t$$ Then, $a_t \rightarrow 0$ . Now consider a random process $(A_t)_t$ with: $$|| \mathbb{E}[A_{t+1}] ||_\infty \leq \delta ||A_t||_\infty$$ Does $A_t \rightarrow 0$ as well? Or are there other requierements that have to be fullfilled? Thanks in advance!","['stochastic-processes', 'convergence-divergence', 'probability-theory', 'probability', 'random-variables']"
4213018,Denoting $2\le\frac{6-4x}5\le3$ by $|mx-n|\le5$. What is the value of $|n-m|$?,"If we denote solution set of the inequality $2\le\frac{6-4x}5\le3$ by $|mx-n|\le5$ , what is the value of $|n-m|$ ? $1)7\qquad\qquad2)5\qquad\qquad3)21\qquad\qquad4)23$ I solved this problem with following approach: $$2\le\frac{6-4x}5\le3\quad\Rightarrow-\frac94\le x\le-1$$ And $|mx-n|\le5$ is equivalent to $\frac{-5+n}m\le x\le\frac{5+n}{m}$ . Hence we have $\frac{-5+n}m=-\frac94$ and $\frac{5+n}m=-1$ and by solving system of equations I got $m=8$ and $n=-13$ . So the final answer is $21$ . I wonder, can we solve this problem with other approahces?","['algebra-precalculus', 'inequality']"
4213056,Proving that $g(x) = \operatorname{sup}_{j}f_{j}(x)$ is measurable,"I'm studying the proof that $g(x) = \operatorname{sup}_{j}f_{j}(x)$ is measurable. Here, $\{f_{j}\}_{j\in \mathbb{N}}$ is a sequence of functions $f_{j}: X \to \overline{\mathbb{R}}$ , where $X$ is equipped with the $\sigma$ -algebra $\mathbb{X}$ . The proof uses: $$g^{-1}((a,\infty]) = \bigcup_{j=1}^{\infty}f_{j}^{-1}((a,\infty]) \tag{1}\label{1}$$ Why does (\ref{1}) hold? If $g$ was defined by $g_{k}(x) = \operatorname{sup}_{j \ge k}f_{j}(x)$ , would (\ref{1}) become $$g^{-1}((a,\infty]) = \bigcup_{j=k}^{\infty}f_{j}^{-1}((a,\infty])$$ instead?","['measure-theory', 'functions', 'real-analysis']"
4213089,Why does Chow's theorem imply that compact complex manifolds embedded in projective space are algebraic?,"I am trying to understand when compact complex manifolds are algebraic.  In many sources, they prove a certain complex manifold can be embedded into projective space (e.g. with the Kodaira embedding theorem) and then state that by Chow's theorem, it must be algebraic.  My question is on this last implication. Chow's theorem states that a compact analytic subvariety of $\mathbb{P}^n$ is algebraic.  An analytic subvariety is defined as one that is locally the vanishing set of some holomorphic functions.  So, if an embedded complex manifold is indeed analytic, then Chow's theorem shows it is algebraic. My problem is that I cannot see why this is necessarily true.  For example (if I understand correctly), in the setting of the Kodaira embedding theorem one constructs a line bundle on a Hodge manifold whose global sections give an embedding into some $\mathbb{P}^n$ .  It is not at all clear to me  though why this image should be locally the vanishing set of some holomorphic functions.  Why is this? After some searching I found a candidate answer: the ""proper mapping theorem"" (Griffiths-Harris, p. 395) which seems to imply this.  However, this seems to be quite a nontrivial statement and I never saw it cited along with the application of Chow's theorem, so I'm wondering if I'm missing something simpler.","['complex-geometry', 'algebraic-geometry']"
4213099,Hodge star duality and the metric,"Let $X$ be a smooth compact Riemannian manifold of even dimension $2n$ . Using the Hodge star $*: \Omega^r(X) \to \Omega^{2n-r}(X)$ one can define self-dual and anti-self-dual $n$ -forms on $X$ , $$
\omega = * \omega \,, \quad \eta = - * \eta \,.
$$ A property of a non-trivial (anti-)self-dual form $\xi$ is that it must have positive (negative) square-integral $\int_X \xi \wedge \xi$ , \begin{align}
\int_X \omega \wedge \omega &\,=\, ~~\int_X \omega \wedge * \omega \,\equiv ~~(\omega , \omega ) > 0 \\
\int_X \eta \wedge \eta ~&\,=\, - \int_X \eta \wedge * \eta ~\, \equiv - (\eta , \eta) \,< 0 .
\end{align} My question is whether the converse statement holds: Can any smooth $n$ -form with positive (negative) square-integral be made (anti-)self-dual by a choice of smooth metric?","['hodge-theory', 'de-rham-cohomology', 'differential-forms', 'differential-geometry']"
4213176,Power set of empty set has exactly _____ subset,"I am wondering why the answer is not ""Two"": Formulating the problem as per my understanding $\rightarrow count(subsets(P(Ø)))$ $∵$ ""The power set of the empty set is
the set containing only the empty set"" $\rightarrow count(subsets(\{Ø\}))$ $∵$ ""[... $|S| = n$ ] the number of all the subsets of $S$ is $|P(S)| = 2^n$ "" $\rightarrow 2^1$ ( $n=1$ , not zero, because the set $\{Ø\}$ contains a single element, namely, $Ø$ ) Basic exponentiation $\rightarrow 2$ $\rightarrow$ ""Two"" $2^{2^0}$ would have been a much concise mode of conveying the method I used to obtain the answer above, but I considered emphasizing the steps in that procedure as it may increase the chances of finding a mistake, if it exists — a quick googling revealed many answer keys pointing to the answer ""One"": I have cited all these sources (inside Markdown) so as to satisfy the relevant licensing obligations to the best of my knowledge. One of such explanations particularly caught my attention: Upon scrutinizing that justification, I am not sure whether or not to conclude that the author has misinterpreted ""has"" as in ""comprise"" because if the objective was explicitly not to find the number of subsets of the resultant powerset as a characteristic possession but to obtain the count of sets comprising the powerset, then the question might have been something appropriately phrased like (in increasing order of precision based on my limited interpretation of the intended question): Powerset of an empty set has exactly _____ set(s) How many subsets does the powerset of an empty set contain? How many sets does the powerset of an empty set contain? Source: Google Dictionary It could perhaps be the case that I am misinterpreting or overlooking words (e.g. ""exactly"" hinting at proper subsets?) or missing something within the context of Set Theory since I am not more formally qualified than a student pursuing these only as a complementary subject. The intent of my post is not to assault their answer keys (also why I did not noticeably render hyperlinks to any of those sites) but to achieve a comprehensive analysis of both the answers through the Math SE community, even if either/all of it is incorrect, in which case, indicating the mistake(s) would be more helpful to arrive at the correct answer, within and/or outside the restricted options. So, in general, how to fill in that blank technically?",['elementary-set-theory']
4213214,Is $\sum_{n=1}^{\infty} \frac{\sin(n) +\cos(n)}{n^2}$ convergent?,My apologies if this question is a duplicate. My attempt to show that it is convergent: $\left| \sum_{n=1}^{\infty} \frac{\sin(n) + \cos(n)}{n^2} \right|= \left| \sum_{n=1}^{\infty} \frac{\sin(n)}{n^2} + \sum_{n=1}^{\infty} \frac{\cos(n)}{n^2} \right|$ $ \left| \sum_{n=1}^{\infty} \frac{\sin(n)}{n^2} + \sum_{n=1}^{\infty} \frac{\cos(n)}{n^2} \right| \leq \left| \sum_{n=1}^{\infty} \frac{\sin(n)}{n^2} \right| + \left|\sum_{n=1}^{\infty} \frac{\cos(n)}{n^2} \right| \leq  \sum_{n=1}^{\infty} \left|\frac{\sin(n)}{n^2} \right| + \sum_{n=1}^{\infty} \left| \frac{\cos(n)}{n^2} \right| \leq \sum_{n=1}^{\infty} \frac{2}{n^2} = 2 \sum_{n=1}^{\infty} \frac{1}{n^2}$ Since $\sum_{n=1}^{\infty} \frac{1}{n^2}$ is convergent then by the Comparison Test $\sum_{n=1}^{\infty} \frac{\sin(n) + \cos(n)}{n^2}$ is absolutely convergent. Therefore $\sum_{n=1}^{\infty} \frac{\sin(n) + \cos(n)}{n^2}$ is convergent. I would like to know if the above is correct.,"['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4213233,Conditional inequality $2a^3+b^3≥3$,"Non-negative $a$ and $b$ such that $a^5+a^5b^5=2$ . How then do I prove
the following inequality $2a^3+b^3≥3$ ? So, we can try using the Lagrange multiplier method: Let $f(a, b)=2 a^{3}+b^{3}+\lambda(a^{5}+a^{5} b^{5}-2), \quad a, b \geq 0$ $$\tag1 \frac{\partial f}{\partial a}=6 a^{2}+\lambda(5 a^{4}+5 a^{4} b^{5})=0 \ldots$$ $$\tag2 \frac{\partial f}{\partial b}=3 b^{2}+\lambda(5 a^{5} b^{4})=0 \ldots$$ $$\left\{\begin{array}{c} 6+\lambda(5 a^{2}+5 a^{2} b^{5})=0 \\ 3+\lambda(5 a^{3} b^{4})=0 \end{array}\right. $$ $$ \lambda=-\frac{6}{5 a^{2}+5 a^{2} b^{5}}=-\frac{3}{5 a^{3} b^{4}} $$ Multiply by $a^3$ , $\,2 a^{6} b^{4}=a^{5}+a^{5} b^{5}=2$ . $$ a^{6} b^{4}=1,\, a^{3} b^{2}=1 \Rightarrow b=\frac{1}{a^{\frac{3}{2}}}\quad a, b \geq 0 $$ $$ \begin{gathered} a^{5}+a^{5} b^{5}=2 \Rightarrow a^{5}+\frac{a^{5}}{a^{\frac{15}{2}}}=2 \Rightarrow a^{5}+\frac{1}{a^{\frac{5}{2}}}=2 \Rightarrow a^{5}-2 a^{\frac{5}{2}}+1=0 \\ \Rightarrow\left(a^{\frac{5}{2}}-1\right)^{2}=0 \Rightarrow a=1 \end{gathered} $$ And the minimum is at $(a,b)=(1,1)$ . I'm not sure I solved this inequality correctly, I would like to see a more beautiful way.","['multivariable-calculus', 'inequality']"
4213385,Showing that $f(x)=2xe^{x}+1$ is injective for $x \geq -1$,"How do you show that $f(x)=2xe^{x}+1$ is injective for $x \geq -1$ ? If you choose two points $a,b \in D_f$ , and say $f(a)=f(b)$ you'll get $ae^a=be^b$ , but isn't this like solving the equation $x=e^x$ ? The idea was to end up with $a=b$ .","['calculus', 'functions', 'inverse-function', 'real-analysis']"
4213406,Problem involving integration and mean value theorem,"Let $f:[0,1]\rightarrow \mathbb{R}$ be a continuous function and $\int_0 ^1 f(t)dt =0$ . Show that there is a $c\in (0,1)$ such that $(a)$ $f(c)=\int_0 ^c f(t)dt$ . $(b)$ if $f(0)=0$ , then $cf(c)=(1-c)\int_0 ^c f(t)dt$ . $(c)$ $\int_0 ^c tf(t)dt=0$ . Now, using the functions $h(t)=e^{-t}\int_0 ^t f(u)du$ and $h(t)=\frac{e^{t}}{t}\int_0 ^t f(u)du$ and using MVT, $(a)$ and $(b)$ can be proved. But I can not see how to find a solution for $(c)$ . How should I proceed? Thank you.","['integration', 'calculus']"
4213417,Find $f(1729)$ if $n^2\int_{x}^{x+\frac 1 n} f(t)\;\text{d}t=nf(x)+0.5$,"I was asked this question- Let $f$ be a real continuous function satisfying $f(0)=0$ and for each natural number $n$ $$n^2\int_{x}^{x+\frac 1 n} f(t)\;\text{d}t=nf(x)+0.5$$ Then find the value of $f(1729)$ I couldn't make much of a progress. But I tried using $$F(x)=\int_0^x f(x)\;\text{d}x$$ so that $$\int_{x}^{x+\frac 1 n} f(t)\;\text{d}t=F\left(x+\frac 1 n\right)-F(x)$$ and then maybe letting $n$ go to infinity such that we can think about the behaviour of $$\lim_{n\to \infty} n^2\cdot \left(F\left(x+\frac 1 n\right)-F(x)\right)$$ But, soon I realised that we don't even know whether this limit exists, since we don't have any idea of whether the limit in the RHS exists, i.e., we don't know whether $$\lim_{n\to \infty} nf(x)$$ exists. However, if this would somehow exist, we could have argued that the limit of the LHS also exists, and thus, $$\left(F\left(x+\frac 1 n\right)-F(x)\right)=\frac {h(x)}{n^2}$$ But still, there's no progress. Another approach that I tried was to sum up the equation from $1$ to $k$ to get $$\sum_{n=1}^k n^2\int_{x}^{x+\frac 1 n} f(t)\;\text{d}t = \frac {k(k+1)}2 f(x) + \frac k 2$$ But how to calculate the sum on the LHS? I tried writing it as $$\sum_{n=1}^k n^2\cdot \left(F\left(x+\frac 1 n\right)-F(x)\right)=\sum_{n=1}^k n^2\cdot F\left(x+\frac 1 n\right) - \sum_{n=1}^k n^2\cdot F(x)$$ where the second term maybe easy to calculate, but what about the first term? Any help would be appreciated. Also, I'm not really sure about the tags- feel free to edit them.","['calculus', 'functions', 'algebra-precalculus']"
4213424,"Number of ways to arrange objects in a circle, some of which may be identical","I know that the number of ways to arrange $n$ distinct objects in a circle in $(n-1)!$ from Circular Permutation . But suppose we have $n_1$ identical objects of Type $1$ , $n_2$ identical objects of Type $2$ and so on upto $n_k$ identical objects of Type $k$ , with the total number of objects equal to $n$ . How many ways are there to arrange these objects in a circle? I tried selecting the first object in $1$ (say of Type $1$ ) way (since it is a circle) and then considering the remaining objects to be permuted in a line, so we get the total number of ways as: $$\dfrac{(n-1)!}{(n_1 - 1)! n_2 ! \cdots n_k !} + \dfrac{(n-1)!}{n_1 ! (n_2 - 1)! \cdots n_k !} + \cdots + \dfrac{(n-1)!}{n_1! n_2 ! \cdots (n_k-1) !}$$ But this is clearly an overcount, since if we take the problem of arranging $9$ blue and $3$ red beads in a circle, the answer we get from the above is $\dfrac{11!}{8! \cdot 3!} + \dfrac{11!}{9! \cdot 2!} = 220$ , while the correct answer (from here ) is $19$ . Can anyone solve the general case?
Thanks in advance.","['algebra-precalculus', 'combinatorics', 'necklace-and-bracelets', 'discrete-mathematics']"
4213458,"Language and probability - ""given that""","In the book ""An Undergraduate Introduction to Financial Mathematics"" there is a following simple problem: ""Suppose cards will be drawn without replacement from a standard 52-card deck. What is the probability that the fourth card drawn will be an ace given that the first three cards drawn were all aces?"" The correct answer in the book is given as 4/52 * 3/51 * 2/50 * 1/49 - What strikes me a bit odd is that we already know that the first three draws are aces (""given that""). Why is the correct answer not simply 1/49? I mean first three draws came and went and we are only concerned about the probability of drawing the ace on the fourth draw. To me the answer would be for question ""What is the probability that first four cards drawn from the card deck without replacements are all aces?""",['probability']
4213460,A JEE Exam problem on determinants and matrices,"I am first stating the question: Let $A=\{a_{ij}\}$ be a $3\times 3$ matrix, where $$a_{ij}=\begin{cases} 
(-1)^{j-i}&\text{if $i<j$,}\\
2&\text{if $i=j$,}\\
(-1)^{i-j}&\text{if $i>j$,}
\end{cases}$$ then $\det(3\,\text{adj}(2A^{-1}))$ is equal to __________ I solved this in the following manner: $$
A=\left[\begin{array}{lcc}
2 & (-1)^{2-1} & (-1)^{3-1} \\
(-1)^{2+1} & 2 & (-1)^{3-2} \\
(-1)^{3+1} & (-1)^{3 + 2} & 2
\end{array}\right]=\left[\begin{array}{ccc}
2 & -1 & 1 \\
-1 & 2 & -1 \\
1 & -1 & 2
\end{array}\right]
$$ $$\begin{aligned}|A| &=2(4-1)+(-2+1)+(1-2) \\ &=6-1-1=4 \end{aligned}$$ $$
\begin{aligned}
& \operatorname{det}\left(3 \operatorname{adj}\left(2 A^{-1}\right)\right) \\
=& 3^{3}\left|\operatorname{adj}\left(2 A^{-1}\right)\right| \\
=& 3^{3}\left|2^{3} \operatorname{adj}\left(A^{-1}\right)\right| \\
=&(3 \times 2)^{3} \times\left(\left|A^{-1}\right|\right)^{2}\\=&6^3\times\Big(\frac14\Big)\\=&13.5
\end{aligned}
$$ Original image Is my solution correct? Note: The problem came in the JEE Main Exam of India, on the 20th of July. The answer given for this question in the Answer Key is 108.","['matrices', 'determinant']"
4213574,"In a metric space, is every open subset a non-redundant union of open balls?","A union of subsets $ \bigcup_{i \in I} A_i $ is non-redundant if no subset is contained in the union of the others, i.e. for any $ j \in I $ , $ A_j \nsubseteq \bigcup_{i \in I - \{j\}} A_i $ . In a metric space, is every open subset a non-redundant union of open balls? Edit for quality:
The open balls form a basis, and it is the canonical basis, it seems interesting to see if they can cover open subsets efficiently. Before asking the question, I had tried to construct it inductively, adding balls as big as possible, unsuccessfully.","['general-topology', 'metric-spaces']"
4213579,Ideal topology on commutative ring,"One small question on the book 'Real and Functional Analysis' by S. Lang: Example 6 on page 21: Let $R$ be a commutative ring. We define a subset $U$ of $R$ to be open if for each $x\in U$ there exists an ideal $J$ in $R$ such that $x+J\subseteq U$ . This is called the ideal topology. It seems to me that this definition is useless: All sets will be open because we can always pick $J=0$ . Unfortunately I couldn't find anything related to this 'ideal topology' via a Google search, so I don't know what is meant in the book. Perhaps requiring $J\ne0$ ?","['general-topology', 'abstract-algebra']"
4213581,A simple proof of $\lim_{n\to \infty} \frac{\ln n}{n}=0$ for students of a high school,"The question refers to the mathematics course for the students of a fifth scientific high school, whereas the order of the arguments of the textbook is almost identical to what I treated when I was studying at the university. Here a graph of the arguments: $$\color{brown}{\text{sequences}}\to \color{red}{\text{topology in }\Bbb R \text{ and }\Bbb R^2}\to\color{gray}{\text{limits of functions}}\to$$ $$\color{magenta}{\text{continuity}}\to \color{cyan}{\text{discontinuity}}\to\color{teal}{\text{derivates}}\to$$ $$\color{blue}{\text{max and min}}\to \color{green}{\text{study of functions in reals}}\to\color{orange}{\text{indefinite and definite integration}}$$ etc. We suppose that we have this limit $$\lim_{n\to \infty} \frac{\ln n}{n}$$ it goes to $0$ , because for $n\in \Bbb N$ large, I have $0\leq \ln n<n$ , i.e. the natural logarithm sequence of $n$ , i.e. $\{\ln n\}$ is much slower than the sequence $\{n\}$ . i.e. we say that the sequence $\{n\}$ is predominant to $\{\ln n\}$ ; hence it is "" similar "" to have $$\bbox[yellow,5px]{\lim_{n\to \infty} \frac{\text{constant}}{n}=0}$$ Is there an alternative clear proof that $$\lim_{n\to \infty} \frac{\ln n}{n}=0, \quad ?$$ Is there also something like: $$\bbox[orange,5px]{\lim_{n\to \infty} \frac{\ln(f(n))}{g(n)}}$$ Is there any known limit if $f(n)$ and $g(n)$ are two polynomials with $$\deg(f(n))\gtreqless\deg(g(n))\quad ?$$","['limits-without-lhopital', 'education', 'sequences-and-series', 'limits', 'soft-question']"
4213594,Optimization problem that involve square root in the constraint?,"I am a researcher in Telecommunication and currently I am running into a difficult optimization with a huge square root. Minimize $A$ such that the following constraints $C_1$ and $C_2$ are satisfied $\begin{gathered}
  {C_1}:{A_{Min}} \leqslant A \leqslant {A_{Max}} \hfill \\
  {C_2}:\frac{{a{A^2}\sin (\theta )\left( {\cos (\theta )\sqrt {\frac{{{B^2}\left( {\frac{1}{{{{\sin }^2}(\theta )}}} \right)}}{{{A^2}}} - 1}  + \sin (\theta )} \right) - a{B^2}}}{{{A^2} - {B^2}}} + D \leqslant \frac{{a{C^2}\sin (\theta )\left( {\cos (\theta )\sqrt {\frac{{{B^2}\left( {\frac{1}{{{{\sin }^2}(\theta )}}} \right)}}{{{C^2}}} - 1}  + \sin (\theta )} \right) - a{B^2}}}{{{C^2} - {B^2}}} \hfill \\ 
\end{gathered}$ $\theta$ is an angle that satisfied $0 \leqslant \theta  \leqslant \frac{\pi }{2}$ $a,A,B,C$ are all positive $D$ is strictly negative $A_{min}$ and $A_{max}$ are the upper bound and lower bound of $A$ respectively. From the first glance, it seems that there are a lot of symmetry in the left and right hand side of constraint $C_2$ of this problem but I do not know how to exploit it. The only effort so far is to get rid of the trigonometry function by using the Weierstrass substitution by letting $t = \tan \left( {\frac{\theta }{2}} \right)$ where $t \in \left[ {0,1} \right]$ because $0 \leqslant \theta  \leqslant \frac{\pi }{2}$ . After that, we have the following: $\begin{gathered}
  \sin (\theta ) = \frac{{2t}}{{1 + {t^2}}} \hfill \\
  \cos (\theta ) = \frac{{1 - {t^2}}}{{1 + {t^2}}} \hfill \\ 
\end{gathered}$ With the Weierstrass substitution, I guess the optimization problem is now purely in polynomial form but nonetheless still posed a major challenge for me. I have some guts feeling that this problem might have something to do with all the square term (i.e quadratic optimization flavor). Constraint $C_2$ then becomes $\frac{{a{A^2}(2t)\left( {\frac{{{S_1}\left( {1 - {t^2}} \right)}}{{{t^2} + 1}} + \frac{{2t}}{{{t^2} + 1}}} \right)}}{{\left( {{t^2} + 1} \right)\left( {{A^2} - {B^2}} \right)}} + D \leqslant \frac{{a{C^2}(2t)\left( {\frac{{{S_2}\left( {1 - {t^2}} \right)}}{{{t^2} + 1}} + \frac{{2t}}{{{t^2} + 1}}} \right)}}{{\left( {{t^2} + 1} \right)\left( {{C^2} - {B^2}} \right)}}$ Where $\begin{gathered}
  {S_1}^2 = \frac{{{B^2}{{\left( {\frac{{1 + {t^2}}}{{2t}}} \right)}^2}}}{{{A^2}}} \hfill \\
  {S_2}^2 = \frac{{{B^2}{{\left( {\frac{{1 + {t^2}}}{{2t}}} \right)}^2}}}{{{C^2}}} \hfill \\ 
\end{gathered} $ Please help me with this, thank you for your enthusiasm ! Clarification: 1/ Regarding the question of fixed and varied parameter. $A$ is the decision variable that needs to be minimized and the rest of the variable $a,B,C$ are fixed (Physical interpretation: they are some measurements value that can be captured very quickly and accurate so I considered them as fixed value). The answer of this problem should be something in the form $A$ equal to some combination of the other fixed parameters. Also, ${A_{\min }}$ and ${A_{\max }}$ are fixed values depend on the transmission standard and not obtain from measure. However, in practice the angle $\theta$ will also varied as well albeit very very slowly. Therefore, I plan to solve for the case when $\theta$ is fixed first. 2/ It is not always possible to guarantee ${A^2} - {B^2}$ to be positive but if this assumption make the analysis easier I think it is possible to accept it although it limit the generality of the analysis. Note that, from my domain knowledge $0<B<1$ is always guarantee. Also, I think it is fine to assume $C^2 - B^2$ to be positive too.","['optimization', 'trigonometry', 'nonlinear-optimization']"
4213644,Finding the inverse Laplace transform of $Y(s)=\frac{1-e^{-{\pi}s}}{(s^2+1)(s^2+4)}+\frac{s+1}{s^2+4}$,"I've encountered an IVP, and I need to solve it by applying the Laplace transform. The problem is: $$y^{''}+4y=g(t), y(0)=y^{'}(0)=1$$ where $g(t)=\sin(t)$ for $0\leq{t}\leq\pi$ and $g(t)=0$ for $\pi\leq{t}$ Now, the Laplace transform is: LHS: $L\{y^{''}\}+L\{4y\}=s^2Y(s)-sy(0)-y^{'}(0)+4Y(s)=(s^2+4)Y(s)-s-1$ RHS: $L\{g(t)\}=L\{\sin(t)-\sin(t)u(t-\pi)\}=\frac{1-e^{-{\pi}s}}{s^2+1}$ Then I get: $$Y(s)=\frac{1-e^{-{\pi}s}}{(s^2+1)(s^2+4)}+\frac{s+1}{s^2+4}$$ Now I'm struggling with finding the inverse transform of $Y(s)$ . Can anyone help me with this probelm. Thank you in advance!","['inverse-laplace', 'ordinary-differential-equations', 'initial-value-problems', 'complex-analysis', 'residue-calculus']"
4213681,"Finding $\displaystyle \lim_{n\to \infty} (x_0 x_1 \cdots x_n)\sqrt{n}$ where $x_{n+1}=x_n^3-x_n^2+1$, $x_0=\frac{1}{2}$","Problem. Let $(x_n)$ be the sequence defined by $x_0=\frac{1}{2}$ and $x_{n+1}=x_n^3-x_n^2+1$ for any $n\in \mathbb{N}\cup \{0\}$ . Find $$ \lim_{n\to \infty} (x_0 x_1 \cdots x_n)\sqrt{n}.$$ According to the answer sheet, this limit equals $1$ . However, I can't manage to solve it. Here is what I've done. Obviously, $x_{n+1}-x_n=(x_n-1)^2(x_n+1)>0$ (it is easy to observe that all the terms of the sequence are positive), so $(x_n)$ is a strictly increasing sequence. Let us now prove by induction on $n$ that $x_n<1$ for all $n\in \mathbb{N}\cup \{0\}$ . The base case is obvious, so suppose it holds for $n$ and prove it for $n+1$ . $x_{n+1}=x_n^2(x_n-1)+1<1$ by the induction hypothesis and we are done. Hence, $(x_n)$ is monotone and bounded, so it is convergent. It is easy to see now that $\displaystyle \lim_{n\to \infty}x_n=1$ . Now I pretty much got stuck. I tried to use the epsilon definition of a limit, trying to exploit $\displaystyle \lim_{n\to \infty}x_n=1$ , but it didn't help. Maybe I should use Stolz–Cesàro on the limit that I want to compute?","['contest-math', 'limits', 'sequences-and-series', 'real-analysis']"
4213690,How does every real number have a decimal representation?,"I am a little bit confused at how every real number can have at least one decimal representation, because: There are uncountable many real numbers There are countable strings of infinite length over a finite alphabet I would expect that there can be no injection from the real numbers to the infinite strings over {0,1,2,3,4,5,6,7,8,9,.} .
Is there a mistake in my assumption or reasoning? Does this only mean that some different real numbers must have the same decimal representation?","['elementary-set-theory', 'real-numbers', 'decimal-expansion']"
4213694,Measure on free sigma algebra,"I know that there exist no measures (i.e. no non-negative, $\sigma$ -additive, extended real-valued function) on a free $\sigma$ -algebra with $\mathbb{N}$ generators. Where can I find a proof of that? Measure on free Boolean sigma-algebras addresses the issue, but I don't understand the argument, especially what $x_A$ represents. Thanks","['boolean-algebra', 'measure-theory']"
4213699,Ways to introduce B-splines,"I asked this on overflow, but it hasn't gotten many responses so I'll try here as well. I have the option of mentoring some undergrads in a topic lying within approximation theory and I really want to do $B$ -splines. Mostly because I have recently found applications of them in my own research and I think it's a good opportunity for me to further learn the material. (And to show them cool stuff as well of course.) Suppose we are given a sequence of knots t $= (t_i)_{i \in \mathbb{Z}} \subset \mathbb{R}$ .
I am aware of two ways in which $B$ -splines can be defined. Method 1: First define the $B$ -splines of order $1$ (or degree $0$ ) to be the characteristic functions $B_{i1} = \chi_{[t_i,t_{i+1})}$ . Then we define the $B$ -splines of higher order by the recurrence relation $$B_{ik} =  \lambda_{ik}B_{i,k-1} + (1 - \lambda_{i+1,k})B_{i+1,k-1}$$ where \begin{equation*}
\lambda_{ik}(t) =  \left\{
        \begin{array}{ll}
            \frac{t - t_i}{t_{i+k-1} - t_i} &  \quad  \text{if} \ \ \ t_i \neq t_{i+k-1} \\
            0 & \quad \text{otherwise}
        \end{array}
    \right.
\end{equation*} I understand that this is a computationally practical way of defining $B$ -splines and that many of the early theorems about $B$ -splines have simple proof when given this definition. However, I am of the opinion that you wouldn't introduce $B$ -splines this way unless you really want to bore your audience as this recurrence relation is highly unmotivated and, until you begin to actually prove theorems with it, it simply doesn't look interesting. The next way is longer but the idea is more natural (at first at least). I don't want to make this post too long so I will skip details. I include some details for completion, but I suspect someone with an answer to this post is likely familiar with everything I mention below. Method 2: Suppose we are investigating the problem of finding a basis for the space of piece-wise polynomials of order $k$ (or degree $k-1$ ) with breakpoints at $(t_i)$ with some specified smoothness condition at each $t_i$ . To find this basis we first make the problem easier by finding a basis for the space of piece-wise polynomials on $\mathbb{R}$ with a finite set of breakpoints and some specified smoothness condition at these breakpoints. If this finite knot sequence is $\{x_1, \dots x_n\} $ , we get led to the truncated power basis $\{(t - x_i)_+^{j} | 1 \leq i \leq n , 0 \leq j \leq k-1\}$ . We then find some linear combination of these truncated powers to begin constructing compactly supported piece-wise polynomials supported on closed intervals with end points belonging to our knot sequence t . (I have skipped many details here). But in order to actually define the $B$ -splines, we need to find out the coefficients of the truncated powers that yield them. This takes us to the divided difference operators and I am not a fan of these operators either. I also find considering them to be somewhat unmotivated (albeit not as unmotivated as the first idea). My question: Are there other ways to introduce $B$ -splines aside from the $2$ methods I have given? I suspect the solution to my dilemna is to understand these divided difference operators more in depth, but I want to know if there are other ways. I've been reading through a book on Box Splines which are defined as distributions. It seems interesting, but I've only begun and don't yet fully see how they generalize $B$ -splines. Even if this approach would work as well, I am unsure whether it would be accessible to undergrads.","['approximation-theory', 'fourier-transform', 'spline', 'polynomials', 'functional-analysis']"
4213738,Showing a collection of half open intervals generate a borel sigma algebra,"Let $\mathcal{H} = \{ (a, \infty) : a \in \mathbb{R} \} $ . Let $\mathcal{B}$ be borel sigma algebra generated by the family of open sets. I have already shown that $\mathcal{B}$ is also generated by open intervals. Now I want to show that $\sigma( \mathcal{H} ) = \mathcal{B} $ . TRY: We know intervals of the form $(a, \infty) $ are open sets. Hence, $\mathcal{H} \subseteq \mathcal{B} $ and so $\sigma( \mathcal{H} ) \subseteq \mathcal{B} $ If $\mathcal{I}$ is the collection of all intervals of the form $(a,b]$ , we have shown in my class that $\sigma( \mathcal{I} ) = \mathcal{B} $ . Consequently, if we can show that $\sigma( \mathcal{I} ) \subset \sigma( \mathcal{H} ) $ , then we are done, and to do this it is enough to show that $\mathcal{I} \subset \sigma( \mathcal{H} )$ . But this follows since we know $$(a,b] =  (a, \infty) \cap (b, \infty)^c$$ Is this a correct solution?",['real-analysis']
4213744,A statistic to capture the degree of mean reversion,"Given a realization of a stochastic process, $x_{t_1}, x_{t_2}, \ldots, x_{t_n}$ , is there a simple statistic that captures the degree to which the stochastic process is mean reverting? For example, such a statistic would give a high value for a realization of an Ornstein-Uhlenbeck process, and a low value for Brownian motion or geometric Brownian motion.","['time-series', 'stochastic-processes', 'statistics', 'stationary-processes']"
4213747,Integration of $\sin^{-1} (x)$ using Lagrange notation,"I would like to find the anti-derivative of $\sin^{-1} (x)$ using Lagrange notation.
For derivation, Lagrange uses $f'(x)$ , $f''(x)$ , etc. However, for anti-derivation he uses $\int$ (elongated $S$ ) symbol and $f(x)$ in it. We would like to compute the derivative of $\sin^{-1} (x)$ . Let $y = \sin^{-1} (x)$ . Here $y$ is the function of $x$ , so $f(x) = \sin^{-1}  (x)$ . Now, $\sin (y) = x$ . then, we can also write it as $\sin [f(x)] = x$ , since $y = f(x)$ . Then, differentiating using the chain rule, $$\cos[f(x)] f'(x) = 1.$$ $$f'(x) = 1/\cos[f(x)].$$ Using trigonometry, $$\sin^2 [f(x)] + \cos^2 [f(x)] = 1,$$ $$\cos[f(x)] = \sqrt{ 1 - \sin^2[f(x)] }.$$ Which is equivalent to $\sqrt{1 - x^2}$ . So $f'(x) = 1 / \sqrt{1 - x^2}$ . Here $f'(x)$ means the derivative of function of $x$ w.r.t $x$ , and $y$ is a function of $x$ .
So by Leibniz notation, we can also write $dy/dx = f'(x)$ , here $dy/dx$ also means the derivative of a function of $x$ w.r.t $x$ since $y$ is a function of $x$ . Similarly, how can I find the anti-derivative of $\sin^{-1} (x)$ using Lagrange notation, using as few symbols as possible; I don't want to treat $dy/dx$ like a ratio.","['integration', 'notation', 'calculus', 'derivatives']"
4213766,New method for constructing six-point circles?,"I would like to present a circle associated with a triangle that can
be constructed as follows: Let A' be the first intersection point of the bisector of
angle A with the inscribed circle of the triangle ABC and
define B' and C' cyclically. I is the incenter of
the triangle ABC . Then circumcircles of the triangles A'B'I , B'C'I , A'C'I intersect the sides of the original triangle at six points, that are always concyclic : Geogebra dynamic sketch I wonder what triangle centres (if any) might be lying on this ""red circle"" and what  is known about it? Point Z (the center of the ""red circle"" that is passing through six points) is not included into the Clark Kimberling's Encyclopedia ... Mathworld.wolfram.com doesn't mention it as well. P.S.
The same construction can be also applied to other
remarkable circles: Geogebra dynamic sketch for the Taylor point/circle. Geogebra dynamic sketch for a Cevian circle of the Symmedian point . <slightly different construction principle was applied here, but overall it is the same idea. X(6) is not the center of the Symmedian circle> Geogebra dynamic sketch for the nine point circle So perhaps this principle holds true for all central circles and there might be    some sort of a generalisation?","['triangles', 'conjectures', 'circles', 'geometry']"
4213769,Verifying 'standup maths' homeomorphism claim,"In his newest video , Matt Parker claims that a sphere with three holes (a pair of trousers) and a torus with one hole (a pair of trousers with the legs sewn) are homeomorphic. I assume he meant removing closed discs, since he wanted them to be manifolds. As justification, he shows that they're homotopy equivalent (to $S^1\vee S^1$ ) and then claims 'because they have thickness' they are homeomorphic. I'm not fully convinced, but cannot show either way. My thoughts are that they are homeomorphic, but that it's not as simple as he suggested (he's essentially suggesting 'rotating' the perpendicularly glued annuli so that they are glued parallel to each other, in some sort of projection, I'm not convinced this is well-defined). Thoughts?","['geometric-topology', 'general-topology', 'popular-math']"
4213794,"Let $f: A \to B$ be a function. Then $\forall C\subseteq A, f^{-1}(f(C))=C$ if and only if $f$ is an injection.","Claim: Let $f: A \to B$ be a function. Then $\forall C\subseteq A, f^{-1}(f(C))=C$ if and only if $f$ is an injection. Proof. ( $\Rightarrow$ ) Assume $f$ is not an injection. Then $\exists x,y \in A$ such that $f(x)=f(y)$ and $x \ne y$ . We need only to find some subset $C$ of $A$ such that $f^{-1}(f(C)) \ne C$ . Let $C=\{x\} \subseteq A$ . Because $x \ne y, y \notin C$ . Note that $f^{-1}(f(C))=\{a \in A|f(a) \in f(C)\}$ .
And that $f(C)=\{b \in B|b=f(x)\}=\{f(x)\}=\{f(y)\}$ , so $y \in f^{-1}(f(C))$ . Since two sets are equal if and only if they have the same elements, it follows that $f^{-1}(f(C)) \ne C$ .
Thus if $f^{-1}(f(C))=C$ , then $f$ is an injection. ( $\Leftarrow$ ) Suppose $f$ is an injection. Then $\forall x,y \in A, f(x)=f(y) \Rightarrow x=y$ . ( $\subseteq$ ) Let $x \in f^{-1}(f(C))$ . Then $x \in \{a \in A |\exists b \in f(C)$ s.t. $f(a)=b\}$ . Thus $f(x)=b$ for some $b \in f(C)=\{d \in B | \exists y \in C$ s.t. $d=f(y)\}$ . And since $b \in f(C)$ , then $b=f(y)$ for some $y \in C$ . As $b=f(x)$ and $b=f(y)$ , then $f(x)=f(y)$ . Now $f$ is an injection, so $f(x)=f(y) \Rightarrow x=y$ . This means that $x \in C$ , and hence $f^{-1}(f(C)) \subseteq C$ . ( $\supseteq$ ) Let $x \in C$ . Then $f(x) \in f(C)$ , and thus $x \in f^{-1}(f(C))$ . Thus $C \subseteq f^{-1}(f(C))$ . As $f^{-1}(f(C)) \subseteq C$ and $C \subseteq f^{-1}(f(C))$ , then $f^{-1}(f(C))=C$ . Therefore, $f^{-1}(f(C))=C$ if and only if $f$ is an injection. Is this correct?","['elementary-set-theory', 'functions', 'solution-verification']"
4213800,Question About Central Limit Theorem,"When I google the central limit theorem it says the following: The central limit theorem states that if you have a population with mean $μ$ and standard deviation $σ$ and take sufficiently large random samples from the population with replacement, then the distribution of the sample means will be approximately normally distributed. My question is the following: does the central limit theorem only apply to sampling distributions of the sample mean? Or can it apply to any sampling distribution of any statistic? Such as sample variance, sample proportion, or difference between two sample means, etc.. Thank You For Your Time","['statistics', 'central-limit-theorem', 'probability']"
4213811,Omega Limit set containing only equilibria,"I am struggling to come to an answer the satisfies myself with the current problem. I have a Lipschitz vector field, restricted to an hyperbox $\mathcal{B}= [a_i,b_i] \times[a_2,b_2]\times\dots\times[a_N,b_N]$ . Such a set $\mathcal{B}$ is forward invariant, therefore for every initial condition $\xi \in \mathcal{B}$ the corresponding forward orbit is contained in a compact set. For my particular system I can show that: For each $\xi$ the corresponding omega limit set $\omega(\xi)$ is made only of equilibria, i.e. every point in $\omega(\xi)$ is an equilibrium. Is 1. enough to conclude that every solution converges to a certain, single equilibrium, even without assuming that the equilibria are isolated? If not, why not?","['vector-fields', 'nonlinear-dynamics', 'ordinary-differential-equations', 'dynamical-systems']"
4213819,Why does $E(E(Y-\gamma^TX|A)^2) = E[A(A^TA)^{-1}A^T(Y-\gamma^TX)^2]$,"I am reading this paper , and in equation (7) on page 9, it's written that $$E(E(Y-\gamma^TX|A)^2) = E[P_A(Y-\gamma^TX)^2]$$ where $Y, X, A$ are random, $\gamma$ is fixed, $P_A$ is the linear $L_2$ projection onto the column space spanned by $A$ . I think $P_A = A(A^TA)^{-1}A^T$ . However, I'm still not quite sure why $$E(E(Y-\gamma^TX|A)^2) = E[A(A^TA)^{-1}A^T(Y-\gamma^TX)^2].$$","['self-learning', 'statistics', 'conditional-expectation', 'expected-value', 'probability']"
4213829,Decompose elements in $\Gamma_0(N)$,"Consider the groups \begin{align*}
  \Gamma_0(N)
\; &:= \;
  \biggl\{
    \begin{pmatrix} a & b \\ c & d \end{pmatrix} \in \mathrm{SL}_2(\mathbb{Z})
    \;:\;
    c \equiv 0 \mod N
  \biggr\}
\\
  \Gamma_\infty
\; &:= \;
  \biggl\{
    \begin{pmatrix} \pm 1 & n \\ 0 & \pm 1 \end{pmatrix} \in \mathrm{SL}_2(\mathbb{Z})
    \;:\;
    n \in \mathbb{Z}
  \biggr\}
.
\end{align*} Via Sage or GAP (see for example https://www.gap-system.org/Manuals/pkg/congruence/doc/chap4.html#X79C44528864044C5 ) we can compute the generators of $\Gamma_0(N)$ , where I am interested in how one can decompose any given element $\gamma \in \Gamma_\infty \backslash \Gamma_0(N)$ in terms of those generators. Given an $\gamma = \begin{pmatrix} * & * \\ c & d \end{pmatrix} \in \Gamma_\infty \backslash \Gamma_0(N)$ , we only need to care about the decomposition of $(c, d)$ by right-side multiplication. My current method have been to Make sure that all the generators $g_1, \ldots, g_k$ are on the form $T = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$ and possibly $J = \begin{pmatrix} -1 & 0 \\ 0 & -1 \end{pmatrix}$ , and the rest being on the form $\begin{pmatrix} a & b \\ c & d \end{pmatrix}$ such that $a, c$ have the same sign, and opposite sign of $b, d$ . Make the transform $\gamma \mapsto \gamma \, T^n = \gamma_1$ such that the $c(\gamma_1) = c(\gamma)$ and $|d(\gamma_1)| < |c(\gamma_1)|$ where $d(\gamma_1)$ has the opposite sign to $c(\gamma_1)$ . Try to minimize $c(\gamma_1)$ by looking at the ratio $| d(\gamma_1) / c(\gamma_1)|$ and try to find a generator $g$ (or its inverse $g^{-1}$ ) which has a similar ratio $|a(g) / c(g)|$ . When such a generator is found, map $\gamma_1 \mapsto \gamma_1 g = \gamma_2$ . ""Repeat"" previous step until $\gamma_i$ can be expressed in terms of a generator. Now, as an example consider $\Gamma_0(10)$ , which is generated by the elements \begin{align*}
  T &= \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}
\\
  U_1 &= \begin{pmatrix} 3 & -1 \\ 10 & -3 \end{pmatrix}
\\
  U_2 &= \begin{pmatrix} 19 & -7 \\ 30 & -11 \end{pmatrix}
\\
  U_3 &= \begin{pmatrix} 11 & -5 \\ 20 & -9 \end{pmatrix}
\\
  U_4 &= \begin{pmatrix} 7 & -5 \\  10 & -7 \end{pmatrix}
\end{align*} whose inverses are \begin{align*}
  T^{-1} &= \begin{pmatrix} 1 & -1 \\ 0 & 1 \end{pmatrix}
\\
  U_1^{-1} &= \begin{pmatrix} -3 & 1 \\ -10 & 3 \end{pmatrix}
\\
  U_2^{-1} &= \begin{pmatrix} -11 & 7 \\ -30 & 19 \end{pmatrix}
\\
  U_3^{-1} &= \begin{pmatrix} -9 & 5 \\ -20 & 11 \end{pmatrix}
\\
  U_4^{-1} &= \begin{pmatrix} -7 & 5 \\  -10 & 7 \end{pmatrix}
.
\end{align*} Let's say we are given $\gamma = \begin{pmatrix} * & * \\  10 & 9 \end{pmatrix}$ , which we want to decompose. After step one, we find $\gamma_1 = \gamma \, T^{-1} = \begin{pmatrix} * & * \\  10 & -1 \end{pmatrix}$ . The closest ratio we find is given by $U_1$ , and after this step we find $\gamma_2 = \gamma_1 U_1 = \begin{pmatrix} * & * \\ 20 & -7 \end{pmatrix}$ . Here the closest ratio is given by $U_2^{-1}$ , where $\gamma_3 = \gamma_2 U_2^{-1} = \begin{pmatrix} * & * \\ -10 & 7 \end{pmatrix} \sim U_4$ . Thus \begin{equation*}
\gamma = U_4 U_2 U_1^{-1}\, T.
\end{equation*} I have found this algorithm to work for many examples, but not for every example. For prime $N$ I have seem to find an extension to this technique which has worked for example I have tried. However, one example that I am not able to find any decomposition for is \begin{equation*}
\gamma = \begin{pmatrix} * & * \\ 40 & -3 \end{pmatrix} \in \Gamma_0(10).
\end{equation*} Does anyone know how to decompose this $\gamma$ ? How can I find a more general technique for finding decompositions?","['modular-arithmetic', 'number-theory', 'elementary-number-theory', 'modular-group', 'modular-forms']"
4213832,Why $f(z+1)=f(z)$ implies $f$ can be expressed as a function of $e^{2\pi iz}$,"I am reading modular forms from J.P.Serre's book, where I came across a complex function which satisfies property $f(z+1)=f(z)$ . Then, it is mentioned that we can express $f$ as a function of $e^{2\pi iz}$ . I can see that any function expressed as a function of $e^{2\pi iz}$ always satisfies the above property, but how the converse is true?","['complex-analysis', 'modular-forms', 'analysis']"
4213861,Arcwise connected part of $\mathbb R^2$,"Here's a question that I share:
 Show that if $D$ is a countable subset of $\mathbb R^2$ (provided with its usual topology) then $X=\mathbb R^2 \backslash D $ is arcwise connected.","['general-topology', 'path-connected', 'connectedness']"
4213875,Proving $\mathbb{R}$ is Hausdorff with final topology induced by a function $f$.,"Consider a Hausdorff topological space $(X,\tau)$ . Suppose $(X,\tilde{\tau})$ is the minimal normalization of $(X,{\tau})$ , that is, for every given normal topology $\sigma$ , where $\tau \subset \sigma$ we have $\tau \subset \tilde{\tau} \subset \sigma$ . Suppose $A\subsetneq X$ and $d$ is a metric function for $(X,\tilde{\tau})$ . Let $f(-)=d(A,-):(X,\tau) \to \mathbb{R}$ . Induce the final topology $\alpha$ on $\mathbb{R}$ such that $f$ is continuous, is the space $(\mathbb{R},\alpha)$ a Hausdorff space? While I have stated my main problem, I am unsure about the existence of $\tilde{\tau}$ as the minimal normalization of $\tau$ , in case there is a counterexample (which I would be grateful to hear about it), suppose that such $\tilde{\tau}$ exists. As the problem itself does not seem related to category theory, here is a brief explanation: If my assumption is correct and the mentioned space is indeed Hausdorff, one can use $f$ and $2f$ to show that for any function $g:Z \to (X,\tau)$ where $g(Z)=A$ , when the image of $g$ is not dense in $X$ , then $g$ is not epic. That is, in the category of Hausdorff spaces, epis are maps with dense image.","['general-topology', 'metric-spaces', 'category-theory', 'separation-axioms']"
4213876,"Why choose sets to be the primitive objects in mathematics rather than, say, tuples?","Sets are defined in such a way that $\{a,a\}$ is the same as $\{a\}$ , and $\{a,b\}$ is the same as $\{b,a\}$ . By contrast, the ordered pair $(a,a)$ is distinct from $(a)$ , and $(a,b)$ is distinct from $(b,a)$ . Intuitively, it would seem useful to draw a distinction between two collections if they are ordered differently, or if one collection has a different number of copies of an element to the other. For instance, this would mean that the collection of prime factors of $6$ would be different to that of $12$ . However, it is the set, rather than the tuple, that is chosen as the primitive object. Why is it useful for the foundations of mathematics that sets have very little ""structure"", and would their be any difficulties in choosing tuples to be the primitive object instead?","['elementary-set-theory', 'foundations', 'motivation']"
4213888,Is the boundary of a $k$-manifold with corners a $(k-1)$-manifold with corners too?,"First of all  we remember some elementary definitions and results about manifolds with corners. Definition A function $f$ defined in a subset $S$ of $\Bbb R^k$ is said of class $C^r$ if it can be extended to a function $\phi$ (said $C^r$ -extension) that is of class $C^r$ in a open neighborhood of $S$ . Lemma If $f$ is a function defined in a subset $S$ of $\Bbb R^n$ such that for any $x\in S$ there exists a function $f_x$ defined in a neighborhood of $x$ that is of class $C^r$ and compatible with $f$ on $U_x\cap S$ then $f$ is of class $C^r$ . Lemma If $U$ is an open set of $H^n_k:=\Bbb R^{n-k}\times[0,+\infty)^k$ for any $k\le n$ then the derivatives of two different extensions $\phi$ and $\varphi$ of a $C^r$ -function $f$ agree in $U$ . Definition A $k$ -manifold with corners in $\Bbb R^n$ of class $C^r$ is a subspace $M$ of $\Bbb R^n$ whose points have a neighborhood $V$ in $M$ that is the immage of a homeomorphism $\phi$ of calss $C^r$ defined an open set $U$ of $\Bbb R^k$ or of $H^k_m$ and whose derivative has rank $k$ . Definition A point $y$ of a $k$ -manifold with corners $M$ is said interior point or boundary point if there exist a coordinate patch about $y$ defined in an open set of $\Bbb R^k$ or in an open set of $H^k_l$ respectively. Theorem Let be $M$ a $k$ -manifold with corners in $\Bbb R^n$ . So if $\phi:U\rightarrow V$ is a coordinate patch about any point boundary point $y$ of $M$ then necessarily $$
y=\phi(x)
$$ for any $x\in U\cap\operatorname{bd}H^k_l$ . So with the previous definitions and efforts I ask to explain ( rigorously ) why the boundary points set $\partial M$ of a $k$ -manifolds with corners is or is not a $(k-1)$ -manifolds with corners too: indeed at the page $253$ of the text Introduction to Smooth Manifolds by John M. Lee it is explicitely said that generally the boudary of a smooth manifolds with corners is not a smooth manifolds with corners although it is not said that it is not a ( not smooth ) manifold with corners and so this confunsed me. Moreover I do not know if my definition of manifold with corners is effectively compatible with John M. Lee definition although it seemed to me it is. So provided that the result is true I arranged a proof that I show to follow: I point out that the proposed solution is imperfect because I did not able to prove a little but important thing as me myself I am showing. So could someone help me, please? MY PROOF ATTEMPT Well by the preceding theorem we know that if $y$ is an element of $\partial M$ then there exist a coordinate patch $\phi:U\rightarrow V$ defined in an open set of $H^k_l$ such that $$
y=\phi(x)
$$ for any $x\in\operatorname{bd}H^k_l$ and thus without loss of generality we can assume that $x^k$ is zero: indeed if $x\in\operatorname{bd}H^k_l$ then  it must be $$
x^i=0
$$ for any $i=(k-l)+1,\dots,k$ so that if $\psi$ is the diffeomorphism that interchanges the $i$ -th coordinate with the last (observe that $\psi$ is an involution that maps $H^k_l$ onto $H^k_l$ ) then $\phi\circ\psi$ is a coordinate patch about $y$ having the desired property. So we remember (click here for details) that if $W$ is an open set of $\Bbb R^k$ then $$
W\cap\big(\Bbb R^{k-1}\times\{0\}\big)=A_W\times\{0\}
$$ for any open set $A_W$ of $\Bbb R^{k-1}$ . Now we first assuem that it is $l=1$ . So in this case the restriction of $\phi$ to $U\cap\operatorname{bd}H^k_1$ carries this set in a one to one fashion onto the open set $V\cap\partial M$ of $\partial M$ . Now if $U$ is open in $H^k_1$ then there must exist an open set $W$ of $\Bbb R^k$ whose intersection with $H^k_1$ is equal to $U$ and thus putting $$
\varphi(x):=\phi(x,0)
$$ for any $x\in A_W$ then we prove that $\varphi$ is a coordinate patch about $y$ . So clearly $\varphi$ is of class $C^r$ because $\phi$ is; moreover the derivative of $\phi$ has rank $(k-1)$ because $D\varphi(x)$ consists simply of the first $(k-1)$ columns of the matrix $D\phi(x,0)$ ; finally $\phi$ is injective and its immage $V\cap\partial M$ is open in $M$ and the inverse function $\varphi^{-1}$ is continuous because it is equal to the function $\phi^{-1}$ followed by the projection of $\Bbb R^k$ onto its first $(k-1)$ coordinates. So we conclude that $\varphi$ is a coordinate patch about $y$ defined in an open set of $\Bbb R^{k-1}$ . Now we let assume that $l>1$ . So since $U$ is open in $H^k_l$ then $$
U=W\cap H^k_l
$$ for any open set $W$ of $\Bbb R^k$ and thus $$
U\cap\operatorname{bd}H^k_1=(W\cap H^k_l)\cap\operatorname{bd}H^k_1=(W\cap\operatorname{bd}H^k_1)\cap H^k_l=\\\big(W\cap(\Bbb R^{k-1}\times\{0\})\big)\cap(H^{k-1}_{l-1}\times[0,+\infty))=(A_W\times\{0\})\cap(H^{k-1}_{l-1}\times[0,+\infty))=(A_W\cap H^{k-1}_{l-1})\times\{0\}
$$ so that we let to prove that the function $\varphi$ defined in $A_W\cap H^{k-1}_{l-1}$ through the equation $$
\varphi(x):=\phi(x,0)
$$ for any $x\in A_W\cap H^{k-1}_{l-1}$ is a coordinate patch about $y$ . So clearly $A_W\cap H^{k-1}_{l-1}$ is open in $H^{k-1}_{l-1}$ and by analogous arguments applied above $\varphi$ is a $C^r$ injective function whose derivative has maximum rank and whose inverse is continuous but unfortunately I did not able to show that the immage of $\varphi$ is effectively open in $\partial M$ so that I can not claim that $\varphi$ is a coordinate patch about $y$ .","['smooth-functions', 'manifolds-with-boundary', 'examples-counterexamples', 'manifolds', 'differential-geometry']"
4213897,Expected value of squared distance after $2016$ knight moves,"From PUMaC 2016: https://static1.squarespace.com/static/570450471d07c094a39efaed/t/58b0dab8d1758e013286f070/1487985337234/PUMaC2016_CombinatoricsA.pdf A knight is placed at the origin of the Cartesian plane. Each turn, the knight moves in an chess L-shape ( $2$ units parallel to one axis and $1$ unit parallel to the other) to one of eight possible locations, chosen at random. After $2016$ such turns, what is the expected value of the square of the distance of the knight from the origin? Shouldn't this just be $0$ ? Imagine the $8$ possible moves each turn to be vectors with magnitude $\sqrt{5}$ , pair each one off with the one that's $180$ degrees in the other direction, and they all cancel out to $0$ . What am I missing? Edit: Honestly, I still don't understand the solutions given. At a turn starting from $(a, b)$ at most $1$ of the $8$ possible moves is parallel to the vector from $(0, 0)$ to $(a, b)$ , and that would increase the square of the distance to the origin by $5$ . But by the triangle inequality, the other $7$ necessarily increase the square of the distance to the origin by less than $5$ , and in some cases decrease it. So the idea that the average increase of the square of the distance to the origin is $5$ doesn't make sense to me, intuitively it should be less than $5$ . Can someone explain what's wrong with this thinking?","['expected-value', 'combinatorics', 'geometry', 'probability']"
4213912,"If $|\frac{c_{n+1}}{c_n}|\leq1+\frac{a}{n}$, where $a<-1$, $a$ does not depend on $n$, then the series $\sum_{n=1}^\infty c_n$ converges absolutely","Question: If $|\frac{c_{n+1}}{c_n}|\leq1+\frac{a}{n}$ , where $a<-1$ and $a$ does not depend on $n$ , then the series $\sum_{n=1}^\infty c_n$ converges absolutely. My attempt: Let $\epsilon>0$ .  To show $\sum c_n$ converges absolutely, we want to show that there exists an $N\in\mathbb{N}$ such that $|\frac{c_{n+1}}{c_n}|$ converges uniformly to some constant $n>N$ .  Since $a<-1$ , there is some $N_0$ such that $||\frac{c_{n+1}}{c_n}|-1|<\epsilon$ whenever $n>N_0$ .  Thus, $|\frac{c_{n+1}}{c_n}|$ converges uniformly (to $1^-$ ) hence $\sum c_n$ converges absolutely. I actually asked this question a little over a year ago here: Showing a series converges absolutely and got a couple neat answers, but I was wondering if this would be a more ""direct"" (I don't know if that is the right word) of doing it.   Or, is there something I messed up?  Any help is, as always, greatly appreciated!  Thank you.","['complex-analysis', 'solution-verification', 'absolute-convergence', 'uniform-convergence', 'convergence-divergence']"
4213926,"Validity of proof $\zeta(s) \neq 0$ for $\sigma\gt1$, where $\sigma$ is the real part of $s$","The Riemann zeta function is can be expressed as an infinite series, as well as an infinite Euler product over primes $p$ . $$ \zeta(s) = \sum_n 1/n^s = \prod_p(1-1/p^s)^{-1} $$ Here $s=\sigma+it$ and the function is defined for $\sigma>1$ . A natural question to ask is whether there are any zeros in the region $\sigma>1$ . Question: Is the following outline proof sufficient and correct? Step 1 We note that none of the factors $(1-1/p^s)^{-1}$ is ever zero, because $p^s = e^{s\ln(p)} \neq 0$ for $\sigma>1$ . Step 2 The previous step is insufficient. We also need to demonstrate that the infinite product doesn't converge to zero. To do this, we make use of the following convergence criteria: if $\sum|a_n|$ converges, then $\prod(1+a_n)$ converges to a finite non-zero value. Step 3 In this case $\sum |a_n| = \sum 1/p^s$ , which we know converges for $\sigma > 1$ . Therefore the Euler Product converges to a non-zero finite value.","['complex-analysis', 'analytic-number-theory', 'euler-product', 'riemann-zeta', 'convergence-divergence']"
4213948,Is the Monster Group the largest finite simple group only divisible by supersingular primes?,"The Monster group has order $2^{46} · 3^{20} · 5^{9} · 7^{6} · 11^2 · 13^3 · 17 · 19 · 23 · 29 · 31 · 41 · 47 · 59 · 71 \approx 8*10^{53}$ , and the primes that occur in its prime factorization are the supersingular primes. Is there any finite simple group larger than the Monster, with a prime factorization only consisting of supersingular primes? Note the first prime not in the Monster is $37$ ; the largest alternating group satisfying the condition is $A_{36}$ , having order around $10^{41}$ , not quite there.","['group-theory', 'simple-groups', 'finite-groups']"
4213994,On the meaning of a linear combination of simplexes,"I'm struggling a bit to understand the concept of a chain in Geometry/Topology, as a linear combination of simplexes, and even more to understand it geometrically (if it possible). So, let's start simply. Let $\Delta = [a,\,b,\,c]$ be the triangle generated by the points $a,\,b,\,c$ in our space, such that $\partial \Delta$ is the chain $[a,\,b] + [b,\,c] - [a,\,c]$ . If we interpret the boundary $\partial \Delta$ of this triangle as ""paths"", with a orientation, it's easy to see that $\partial \Delta$ is the polygonal path connecting $a$ to $c$ , with $b$ in between. Ok, but what about the chain $[a,\,b] + 2\,[b,\,c] - 5\,[a,\,c]$ , how could we interpret it geometrically? Now, let's go to $\mathbb{R}^n$ ; considering continuous paths of the form $c_i \colon [0,\,1] \longrightarrow \mathbb{R}^n$ , is there a intuitive way of understanding chains of the form $\Gamma = \sum_i n_i \, c_i$ , with $n_i \in \mathbb{Z}$ ? In my point of view, the integral coeficients of $\Gamma$ implies ""how many times"" we travel the path $c_i$ , i.e., if $\Gamma = 3\,c_1 - c_2 + 2\,c_3$ , we travel $3$ times along $c_1$ , then one time along $c_1$ (in the opposite way), and then $2$ times along $c_3$ . My goal is to understand properly the Generalized Stoke's Theorem, as well the homologic version of Cauchy's Theorem, and both requires the language of chains. Thanks in advance!","['algebraic-topology', 'geometry', 'simplex', 'homology-cohomology', 'differential-forms']"
4214068,An exercise about the Pasting Lemma.,"I have difficulty with the following exercise from Introduction
to Topology (by Tej Bahadur Singh) (Exercise 9 on p. 36): Let $f: X\to Y$ be a function between topological spaces, and assume
that $A\cup B= X$ , where $A-B\subseteq A^\circ$ , and $B-A\subseteq B^\circ$ . If $f|_{A}$ and $f|_{B}$ (endowed with the relative
topologies)  are continuous, show that $f$ is continuous. I tried to use the following facts (from the book mentioned above): Definition (locally finite). A family $\{A_i\}$ of subsets of a space $X$ is called locally finite if each point of $X$ has a neighborhood $U$ such that $U\cap A_i\neq \varnothing$ for at most finitely many indices $i$ . (1) Let $\{U_\alpha\}$ be a family of open subsets of a space $X$ with $X = \bigcup_\alpha U_\alpha$ . Then a function $f$ from $X$ into a space $Y$ is continuous if and only if $f|_{U_\alpha}$ is continuous for each index $\alpha$ . (See Exercise 8 on p. 36) (2)  If a space $X$ is the union of a locally finite family $\{A_i\}$ of closed sets, then a function $f$ from $X$ to
a space $Y$ is continuous if and only if the restriction of $f$ to each $A_i$ is continuous. (See Corollary 2.1.10 on p. 33) Using the fact (1), we obtain that $f|_{A^\circ\cup B^\circ}$ is continuous. Clearly, $f|_{A\cap B}$ is continuous. Since $A-B\subseteq A^\circ$ and $B-A\subseteq B^\circ$ , $A^\circ \cup B^\circ \cup (A\cap B)= X$ . But $A\cap B$ is not open, so I cannot use the fact (1) again, I have not idea   what to do next. Any ideas would be appreciated.","['elementary-set-theory', 'continuity', 'general-topology']"
4214110,Criteria for finite time blow up for two simple ODEs,"I've got two simple questions on criteria for a finite time blow up of solutions of two simple ODEs: 1 : If $u$ is a solution of $u'=f(u)\ge0$ , $u(0)=u_0$ , how do we see that $u$ blows up in finite time (i.e. there is a $T>0$ s.t. $|u(t)|\xrightarrow{t\to T-}\infty$ ) if and only if $$\int_{u(0)}^\infty f^{-1}(s)\:{\rm d}s<\infty\tag1?$$ I've only got an idea for this if we additionally assume that $f$ is Lipschitz continuous. We then see that if $u:I\to\mathbb R$ is a solution of $u'=f(u)$ on some compact interval $I:=[a,b]$ , then: If $f(u(t_1))=0$ for some $t_1\in I$ , then $u\equiv u(t_1)$ on $I$ by uniqueness (for which we need the Lipschitz continuity of $f$ ). So, since $f\ge0$ , we see that $u$ can only blow up if $f>0$ on $[u(a),\infty)$ ... But how do we need to proceed? And can we drop the Lipschitz assumption? And what's happening for general, possibly negative, $f$ ? 2 : If $p>1$ , the solution of $u'=u^p$ is given by $$u(t)=((p-1)(T_0-t))^{-\frac1{p-1}}\;\;\;\text{for }t<T_0\tag2$$ for some $T_0\in\mathbb R$ . We see that $$T_0=\frac1{(p-1)u_0^{p-1}}\tag3$$ and hence $$u(t)=\left(\frac{u_0^{p-1}}{1-(p-1)u_0^{p-1}t}\right)^{\frac1{p-1}}\tag4.$$ I think we should have $u(t)\xrightarrow{t\to T_0-}\infty$ ; at least if $u_0>0$ . But what happens if $u_0\le0$ ? Does the solution then exists at all?","['blowup', 'ordinary-differential-equations']"
4214143,Does the fraction of positive integers not being a Carmichael value have a limit?,"Let $f(n)$ be the number of positive integers $x\le n$ such that $\lambda(k)=x$ has no solution, where $\lambda(k)$ denotes the Carmichael-function. Does $$\lim_{n\rightarrow \infty} \frac{f(n)}{n}$$ exist , and if yes, is it $1$ or some smaller value ? The last few lines in a numerical analysis were : 579000000  0.77069874093264248704663212435233160622
580000000  0.77070391551724137931034482758620689655
581000000  0.77070938382099827882960413080895008606
582000000  0.77071472164948453608247422680412371134
583000000  0.77071994511149228130360205831903945112
584000000  0.77072559931506849315068493150684931507
585000000  0.77073061196581196581196581196581196581 This indicates a slow increase, but when I tried small ranges with larger values , the frequency seemed to still increase (above $0.8$ ). Any ideas ?","['limits', 'carmichael-function', 'elementary-number-theory']"
4214153,Can blow-up of a surface be a product of two curves? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Is there any smooth projective surface $S$ over $k=\bar{k}$ , such that the blow up $\tilde{S}$ along some point $x\in S$ can be written as $\tilde{S}=C_1\times C_2$ for two curves $C_i$ ?","['algebraic-geometry', 'blowup']"
4214211,Is there an analogous concept for De Rham cohomology in the framework of Clifford algebras?,"I’ve recently read about Clifford’s geometric algebra being a more general framework for differential geometry than differential forms , simpler for the study of spaces with a metric tensor, and equivalent to the latter in some cases. I’ve mostly learned about differential forms in the context of de Rham cohomology so I couldn’t help but asking myself if there’s an equivalent concept in the framework of Clifford algebras (I don’t know a whole lot about Clifford algebras yet so excuse me if this question doesn’t make sense).","['differential-geometry', 'de-rham-cohomology', 'geometric-algebras', 'differential-forms', 'clifford-algebras']"
4214220,"Evaluating $\sqrt{7\sqrt{7\sqrt{7\sqrt{7\,\cdots}}}}$. How can we know $0$ is an extraneous solution? [duplicate]","This question already has answers here : $\sqrt{7\sqrt{7\sqrt{7\sqrt{7\sqrt{7\cdots}}}}}$ approximation [closed] (7 answers) Closed 2 years ago . The task is to find the the value of this expression: $$\sqrt{7\sqrt{7\sqrt{7\sqrt{7\,\cdots}}}}$$ We can assume $$\sqrt{7\sqrt{7\sqrt{7\sqrt{7....}}}} = x$$ Then we replace the nested part with $x$ $$\sqrt{7x} = x$$ Then square both sides and solve the equation $$\begin{align}
7x &= x^2 \\[4pt]
x^2 - 7x &= 0 \\[4pt]
x(x - 7) &= 0 \\[1em]
x_1 = 7,\quad& x_2 = 0
\end{align}$$ The problem is I assume $0$ is not a valid solution, but I can't explain why. It almost makes sense it could be $0$ as the numbers get progressively smaller. Can someone explain how can we reject the solution $0$ ? Edit: I think I understand it better now. But what I'm most confused about is why we get the invalid solutions here in the first place.
Could it be that the problem is in the beginning where we could write the equation in different ways, like $\sqrt{7\sqrt{7x}} = x ?$ This way we would get 4 different solutions.","['algebra-precalculus', 'sequences-and-series']"
4214244,"Wrong solution: Given three bins and three balls, what is the possibility that exactly one bin is empty?","I would like to know why the 'solution' with which I came up is wrong. I gave the correct answer at the bottom. Each ball has probability $\frac{1}{3}$ of going into either bin, and there are a total of $3^3=27$ ways to distribute the balls among the bins. My solution: There are $3$ ways to select the bin for the first ball, $2$ ways to select the bin for the second ball (because it must not go into the bin selected for the first ball), and $2$ ways to select the bin for the third ball (because it must go into either of those selected before). This gives a probability of $\frac{3\cdot 2\cdot 2}{27}=\frac{12}{27}=\frac{4}{9}$ . Correct answer: $\frac{{{3}\choose{2}}(2^3-2)}{27}=\frac{2}{3}$ , because there are ${3}\choose{2}$ ways to select the bins into which the balls should go, $2^3$ ways to put the $3$ balls into these bins, and in $2$ of those cases all balls are in the same bin. As I wrote in the opening, I don't quite see what's wrong with the first attempt.","['permutations', 'combinations', 'combinatorics', 'probability']"
4214266,Uniqueness of Taylor expansion?,"I am struggling with proof of Morse's lemma in my textbook. The context you need to know: Function $f: \mathbb{R}^m \rightarrow \mathbb{R}$ , $f \in C^3$ . It has nondegenerate critical point at $x=0$ . $f(0)=0$ Now what happened. In the proof we managed to represent our function $f$ the following way: $$
f(x) = x^T \cdot H(x) \cdot x
$$ Where $H(x)$ is symmetric (can be viewed as a quadratic form at each point $x$ ) $m \times m$ matrix. Every entry of $H(x)$ is a $C^1$ function of $x$ . Now they tell "" since the uniqueness of a Taylor expansion and continuity of entries of $H(x)$ we can conclude that $H(0)=\textbf{H}_f(0)$ "" ( $\textbf{H}_f$ denotes hessian matrix of the function $f$ ). This ""conclusion"" actually strikes me out. Let's look at the Taylor's expansion of a function $f$ near $x=0$ : $$
f(x)-f(0) = x^T \cdot \nabla f(0) + x^T \cdot \textbf{H}_f(0) \cdot x + o(||x||^2)
$$ And since $f(0)=0$ and $\nabla f(0)=0$ (critical point) we now can rewrite $f(x)$ as: $$
f(x)=x^T \cdot \textbf{H}_f(0) \cdot x + o(||x||^2)
$$ Compare it with earlier result: $$
f(x) = x^T \cdot H(x) \cdot x
$$ Does it straightly follows that $H(0)=\textbf{H}_f(0)$ ? I think ""the uniqueness of Taylor expansion"" is not applicable here since $x^T \cdot H(x) \cdot x$ is not a polynomial function of coordinates of $x$ . UPDATE As I understood, we need to show that this condition $$
x^T \cdot (H(x) - \textbf{H}_f(0)) \cdot x = o(||x||^2) \quad \text{as} \space x \to 0
$$ implies $H(0) = \textbf{H}_f(0)$","['continuity', 'multivariable-calculus', 'taylor-expansion']"
4214350,Using Rouché's theorem to infer the amount of zeroes inside the given domain,"Given $p(z)=i z^{5}-8 z^{4}-\pi$ , How many zeros there is for $p(z)$ inside $ D_{1}(0) \cap\{z \mid \operatorname{Im}(z)>0\}$ ? I can use Rouché theorem to infer how many zeros there are in the whole unit disk, but how do I infer the amount of zeros in the given domain?","['complex-analysis', 'rouches-theorem']"
4214384,proving an equality in euclidean geometry,"Let $ABCD$ be a cyclic quadrilateral, $O = AC \cap BD$ . Let $M, N, P, Q$ be the midpoints of $AB,BC,CD$ and $DA$ , respectively, and $X, Y , Z, T$ be the projections of $O$ on $AB, BC,
CD$ and $DA$ , respectively. Let $U = MP \cap Y T$ and $V = NQ \cap XZ$ . Prove that $\frac{UO}{VO} = \frac{AB \times CD}{BC \times DA}$ From drawing, I noticed a few things that should be true, such as that the quadrilaterals $XNYZ; QTPM; QXMN; BPZT; QXZT; PMNY$ should be cyclic,
and that the segment pairs $(QN, ZX) (TY, PM)$ should both be perpendicular. I also thought about using Ptolemy's theorem, but I don't think any of the things I listed above can help with finding the LHS of the equation, and I'm out of ideas. Edit: I've made some progress thanks to @saulspatz's suggestion about UOV being collinear, as it made me realize that perhaps I can do something using the circle with diameter UV, as it should pass through both the intersections of QN and PM, and ZX and TY. I'm still not quite sure how to prove this though, and how it would be useful.","['euclidean-geometry', 'geometry']"
4214411,"Question about Graham, Knuth, and Patashnik's technique for turning first-order recurrences into summations","In Concrete Mathematics , Graham, Knuth, and Patashnik describe the following technique for turning first order recurrence relations of the form \begin{equation}
a_nT_n = b_nT_{n-1} + c_n
\end{equation} into summations. Step 1. Find a function $s_n$ with the property that $$s_nb_n = s_{n-1}a_{n-1}$$ Step 2. Multiply both sides of the recurrence by $s_n$ , giving you $$s_na_nT_n = s_nb_nT_{n-1} + s_nc_n$$ or equivalently, $$s_na_nT_n = s_{n-1}a_{n-1}T_{n-1} + s_nc_n$$ Step 3. Define $$S_n = s_na_nT_n$$ and rewrite the recurrence as $$S_n = S_{n-1} + s_nc_n$$ Step 4. Write $S_n$ as the sum $$S_n = s_0a_0T_0 + \sum_{k = 1}^n s_kc_k = s_1b_1T_0 + \sum_{k = 1}^n s_kc_k$$ Step 5. Find a closed form for the summation $S_n$ . Step 6. To find the closed form for $T_n$ , simply multiply the closed form of $S_n$ by $\frac{1}{s_na_n}$ . Additionally, they claim that the appropriate value of $s_n$ is always given by $$s_n = \frac{a_1a_2\cdots a_{n-1}}{b_2b_3\cdots b_n}$$ which they justify by reasoning as follows: Since $b_ns_n = s_{n-1}a_{n-1}$ , we know that $$s_n = \frac{s_{n-1}a_{n-1}}{b_n}$$ plugging in the value of $s_{n-1}$ , we find that this is equal to $$\frac{s_{n-2}a_{n-2}a_{n-1}}{b_{n-1}b_n}$$ and by continuing in this fashion, we ultimately find that $$s_n = \frac{a_1a_2\cdots a_{n-1}}{b_2b_3\cdots b_n}$$ But when I continue in this fashion what I find is that $$s_n = \frac{s_1a_1a_2\cdots a_{n-1}}{b_2b_3\cdots b_n}$$ Notice the $s_1$ in the numerator . What am I doing wrong here?","['summation', 'recurrence-relations', 'discrete-mathematics']"
4214418,"show that the set of numbers in $(0, 1)$ that have a decimal expansion with one hundred consecutive 4s is a Borel set and find its Lebesgue measure","I have solved the following problem and I would like to know if my proof is correct and/or if it could be improved, thanks. ""show that the set of numbers in $(0, 1)$ that have a decimal expansion with one hundred consecutive 4s is a Borel set and find its Lebesgue measure"" My proof: Let $E:=\{x\in (0,1):x\text{ has as decimal expansion with one hundred consecutive }4\text{s}\}$ and define (note that, for example, $0.\underbrace{44\dots4}_{99}5=0.\underbrace{44\dots4}_{100}99999\dots$ ): $E_0=[0.\underbrace{44\dots4}_{100}, 0.\underbrace{44\dots4}_{99}5]$ , $E_1=\bigcup_{n_1\in\{0,1,\dots,9\}}[0.n_1\underbrace{44\dots4}_{100}, 0.n_1\underbrace{44\dots4}_{99}5]$ $E_2=\bigcup_{n_1,n_2\in\{0,1,\dots,9\}}[0.n_1n_2\underbrace{44\dots4}_{100}, 0.n_1n_2\underbrace{44\dots4}_{99}5]$ $E_3=\bigcup_{n_1,n_2,n_3\in\{0,1,\dots,9\}}[0.n_1n_2n_3\underbrace{44\dots4}_{100}, 0.n_1n_2n_3\underbrace{44\dots4}_{99}5]$ $\vdots$ $E_k=\bigcup_{n_1,n_2,\dots, n_k\in\{0,1,\dots,9\}}[0.n_1n_2\dots n_k\underbrace{44\dots4}_{100}, 0.n_1n_2\dots n_k\underbrace{44\dots4}_{99}5]\  \dots$ ; now, closed intervals in $\mathbb{R}$ are Borel sets and each $E_j$ is the union of such intervals so the $E_j$ s are Borel sets too and since $E=\bigcup_{k=0}^{\infty}E_k$ we can conclude that $E$ is a Borel set. To determine the Lebesgue measure of $E$ represent the numbers in base $10^{100}$ : then we may envision $(0,1)$ as a segment divided into $10^{100}$ smaller segments of length $\frac{1}{10^{100}}$ . Of these sub-segments the $\underbrace{444\dots 44}_{100}$ th one must be taken, since it represent numbers in $(0,1)$ whose decimal expansion begins with one hundred $4$ s. Each one of the remaining $10^{100}-1$ sub-segments can be divided into $10^{100}$ sub-segments, each of length $(\frac{1}{10^{100}})^2$ and of these, as before, we have to take the $\underbrace{444\dots 44}_{100}$ th one, because it contains a run of one hundred $4$ s and continuing in this fashion we have that $\mu(E) =1\cdot\frac{1}{10^{100}}+ (10^{100}-1)\cdot (\frac{1}{10^{100}})^2+(10^{100}-1)^2 (\frac{1}{10^{100}})^3+\dots +(10^{100}-1)^k (\frac{1}{10^{100}})^{k+1} +\dots =\sum_{k=0}^{\infty}+(10^{100}-1)^k (\frac{1}{10^{100}})^{k+1} =\frac{1}{10^{100}}\sum_{k=0}^{\infty} (1-\frac{1}{100})^k=\frac{1}{10^{100}}\cdot\frac{1}{1-(1-\frac{1}{100})}=\frac{1}{10^{100}}\cdot 10^{100}=1.\ \square$","['measure-theory', 'solution-verification', 'lebesgue-measure', 'real-analysis']"
4214425,Integrating the derivative of a function with respect the components of a vector,"I'm stuck determining the integrals of the partial derivatives of a scalar function $F:\mathbb{R}^3\times\mathbb{R}^3\rightarrow\mathbb{R}$ with respect to the components of a vector. In the case of the problem I am trying to solve, $F=F(\vec x, \vec y)$ and the following relations hold $$
y_i=\frac{\partial F}{\partial x_i} \tag{1}$$ $$
x_i=\frac{\partial F}{\partial y_i}\tag{2}
$$ My attempt at a solution Performing the ""partial integration"" over $(1)$ and $(2)$ , $$F(\vec x, \vec y)=\int\frac{\partial F}{\partial x_i}dx_i=\int y_idx_i+g_i(x_j, \vec y)=x_iy_i +g_i(x_j, \vec y) \tag{3}$$ $$
F(\vec x, \vec y)=\int\frac{\partial F}{\partial y_i}dy_i=\int x_idy_i+h_i(\vec x,y_j)=x_iy_i +h_i(\vec x,y_j) \tag{4}$$ Where $j\neq i$ in the functions $g_i$ and $h_i$ . I see that adding for $i=1,2,3$ the RHS $(3)$ and $(4)$ we can write $$3F(\vec x, \vec y)=\vec x · \vec y + \sum_{i=1\\j\neq i}^3 g_i(x_j, \vec y) \tag{5}$$ $$3F(\vec x, \vec y)=\vec x · \vec y + \sum_{i=1\\j\neq i}^3 h_i(\vec x,y_j) \tag{6}$$ Obtaining this relation for the functions $g_i$ and $h_i$ $$\sum_{i=1\\j\neq i}^3 g_i(x_j, \vec y)=\sum_{i=1\\j\neq i}^3 h_i(\vec x,y_j) \tag{7}$$ Would this approach be correct? Could we tell something else about the integrating ""constants"" $g_i$ and $h_i$ in addition to $(7)$ ?","['integration', 'multivariable-calculus', 'partial-derivative', 'partial-differential-equations']"
4214451,Formula for $f(1) + f(2) + \cdots + f(n)$: Euler-Maclaurin summation formula,"Let $f\colon \mathbb{R}\to \mathbb{R}$ be a function with $k$ continuous derivatives. We want to find an expression for $$
S=f(1)+f(2)+f(3)+\ldots+f(n).
$$ I'm currently reading Analysis by Its History by Hairer and Wanner. They first consider the shifted sum and arrive at the expression $$f(n)-f(0)=\sum_{i=1}^{n} f^{\prime}(i)-\frac{1}{2 !} \sum_{i=1}^{n} f^{\prime \prime}(i)+\frac{1}{3 !} \sum_{i=1}^{n} f^{\prime \prime \prime}(i)-\frac{1}{4 !} \sum_{i=1}^{n} f^{\prime \prime \prime \prime}(i)+\ldots$$ using Taylor series (provided that the Taylor series actually converges to $f$ ). In order to turn this formula for $\sum f^{\prime}(i)$ into a formula for $\sum f(i)$ , we replace $f$ by its primitive (again denoted by $f$ ): $$\sum_{i=1}^{n} f(i)=\int_{0}^{n} f(x) d x+\frac{1}{2 !} \sum_{i=1}^{n} f^{\prime}(i)-\frac{1}{3 !} \sum_{i=1}^{n} f^{\prime \prime}(i)+\frac{1}{4 !} \sum_{i=1}^{n} f^{\prime \prime \prime}(i)-\ldots$$ The second idea is to remove the sums $\sum f^{\prime}, \sum f^{\prime \prime}, \sum f^{\prime \prime \prime}$ , on the right by using the same formula, with $f$ successively replaced by $f^{\prime}, f^{\prime \prime}, f^{\prime \prime \prime}$ etc. I don't really understand the step which replaces $f$ by its primitive. Using $F$ for denoting the primitive of $f$ I obtain $$F(n)-F(0)=\sum_{i=1}^{n} F^{}(i)-\frac{1}{2 !} \sum_{i=1}^{n} F^{\prime}(i)+\frac{1}{3 !} \sum_{i=1}^{n} F^{\prime \prime}(i)-\frac{1}{4 !} \sum_{i=1}^{n} F^{\prime \prime \prime}(i)+\ldots$$ but I don't obtain any  expression in terms of an integral. Clearly, $$
F(n) - F(0) = \int_0^n f(x) \textrm{d}x
$$ but since the author mentions that he again denotes the primitive by $f$ this doesn't match up with the above formula. Can anyone explain me what my  mistake is here?","['integration', 'summation', 'euler-maclaurin', 'real-analysis']"
4214473,Compact subset of infinite-dimensional space has empty interior,"My question is related to this question.
My space is the set of all Borel probability measures on $\Theta=[0,1]$ , which is a compact metric space under the Prokhorov metric. Call this space $\Delta \Theta$ . This is a compact subset of the space of all signed measures on $[0,1]$ which is a vector space with the total variation norm. As we know, the interior of a compact set in an infinite dimensional TVS is empty. So that means $int(\Delta \Theta)=\emptyset$ . So $\Delta \Theta$ cannot contain any open sets. Take any continuous function $f: \Delta \Theta \rightarrow \mathbb{R}$ . The inverse image of any open set in $\mathbb{R}$ under $f$ - which is, by definition, a subset of $\Delta \Theta$ - is an open set by continuity of $f$ . This seems like a contradiction and I'm totally confused. I think the apparent contradiction is arising because of the change of topology - the Prokhorov metric on the $\Delta \Theta$ space is not the same as the metric induced by the TV norm on the VS in which $\Delta \Theta$ lives. And maybe under the TV metric $\Delta \Theta$ is not compact? Any help in resolving this is highly appreciated. Thanks in advance.","['measure-theory', 'vector-spaces', 'metric-spaces', 'total-variation', 'compactness']"
4214504,Is there a bijective map between subgroups of a finite abelian group and subgroups of its dual?,"Let $(G,+)$ be a finite abelian group, $\widehat{G}$ the dual group of $G$ (i.e. the set of homomorphisms from $G$ to $\mathbb{C}^*$ ).
If $X$ is a subset of $G$ , we define $X^{\bot}$ as the subgroup of $\widehat{G}$ whose elements are the trivial characters on $X$ , that is: \begin{equation}
X^{\bot}=\{\chi \in \widehat{G};\: X \subset \ker(\chi)\}
\end{equation} Let $K$ be any subgroup of $\widehat{G}$ and if $Y$ is a subset of $\widehat{G}$ , let’s write $Y^°$ for the following subgroup of $G$ : \begin{equation}
Y^°= \bigcap_{\chi \in Y} \ker(\chi)
\end{equation} Show that the following applications \begin{equation}
\displaystyle H \longmapsto H^{\bot}\: \: \text{and} \: \: K \longmapsto K^°
\end{equation} are two reciprocal bijections between the subgroups of $G$ and the subgroups of $\widehat{G}$ . My idea (but it was apparently a bad idea) was to consider the following bijective map , call it $f$ , that takes a subgroup $H$ of $G$ to the set of characters of $\widehat{G}$ whose kernel contains $H$ (i.e. $H^{\bot}$ ): \begin{equation}
f(H)=\{\chi \in \widehat{G};\: \chi(h)=1, \: \forall h \in H \}.
\end{equation} Then consider the inverse bijective map, say $g$ , defined as: \begin{equation}
g\big(f(H)\big)=\{g \in G; \: \chi(g)=1,\: \forall \chi \in f(H)\}
\end{equation} We first show that this set contains $H$ and by an inclusion reversing argument, that this set is $H$ itself !
We conclude that $g \circ f$ is the identity map. We proceed the same way for $f \circ g$ .
I thought it could work but it doesn’t because the map $K \longmapsto K^°$ defined in the exercise is totally different from mine: it is defined on any subgroup $K$ in the dual group $\widehat{G}$ and sends $K$ to $K^°$ .
I’m really confused about that. Any help would be appreciated.
Many thanks.","['abelian-groups', 'group-theory', 'finite-groups', 'characters']"
4214531,"Find $\int_0^e f(x) \ dx$, where $f(y) e^{f(y)} = y$","The Question: (a) Prove that for all $y \ge 0,$ there exists a unique real number $x$ such that $$xe^x = y.$$ (b) By part (a), for $y \ge 0,$ we can let $f(y)$ be the unique real number such that $f(y) e^{f(y)} = y$ . Find $\int_0^e f(x) \ dx.$ What I know: I know how to prove part a, you graph $xe^x=y$ , and it's clear to see that for all $y \ge 0,$ there exists a unique real number $x$ such that $xe^x = y.$ My struggle is with part b. I just don't know how to write the differential equation to find the $f(x)$ present. Any help on that front is greatly appreciated. Wikipedia articles, anything.","['integration', 'calculus', 'ordinary-differential-equations']"
4214550,Leibniz rule for wedge product of differential forms with values in associated vector bundles,"Before stating the claim, let my define all the objects which I need: To start with, let me fix notation: Let $P$ be a principal $G$ -bundle over a (smooth, compact, oriented) manifold $\mathcal{M}$ (possibly with boundary) and $\mathfrak{g}$ be the Lie algebra of $G$ . Furthemore, let us choose a connection $1$ -form $A\in\Omega^{1}(P,\mathfrak{g})$ . Let $(V,\rho)$ be a (finite-dimensional real/complex) representation of $G$ and $E:=P\times_{\rho} V$ be the associated vector bundle. Let $\langle\cdot,\cdot\rangle_{V}$ be a non-degenerate symmetric bilinear form on $V$ . Then it is a general fact that this induces a bundle metric $\langle\cdot,\cdot\rangle_{E}\in\Gamma(E^{\ast}\otimes E^{\ast})$ on $E$ via $$\langle [p,v],[p,w]\rangle_{E_{x}}:=\langle v,w\rangle_{V}$$ for all $x\in\mathcal{M}$ and for all $[p,v],[p,w]\in E_{x}\cong P_{x}\times_{\rho}V$ . Furthermore, I need two further definitions: First of all, the wedge-product $\mathrm{tr}(\cdot\wedge\cdot):\Omega^{k}(\mathcal{M},E)\times\Omega^{l}(\mathcal{M},E)\to\Omega^{k+l}(\mathcal{M})$ is defined in the obvious way, i.e. $$\mathrm{tr}(\alpha\wedge\beta)_{x}(v_{1},\dots,v_{k+l}):=\frac{1}{k!l!}\sum_{\sigma\in\mathfrak{S}^{k+l}}\mathrm{sgn}(\sigma)\langle \alpha_{x}(v_{\sigma(1)},\dots,v_{\sigma(k)}),\beta_{x}(v_{\sigma(k+1)},\dots,v_{\sigma(k+l)})\rangle_{E_{x}}$$ for all $x\in\mathcal{M}$ and for all $v_{1},\dots,v_{k+l}\in T_{x}\mathcal{M}$ . Secondly, I need the ""exterior covariant derivative induced by a connection 1-form $A$ "", which is the exterior covariant derivative induced by a connection $\nabla^{A}$ on $E$ , whose definition is not so important right now. This is a map $\mathrm{d}_{A}:\Omega^{k}(\mathcal{M},E)\to\Omega^{k+1}(\mathcal{M},E)$ defined using a local frame $\{e_{a}\}_{a}\subset\Gamma(U,E)$ defined on some open set $U\subset\mathcal{M}$ via $$\mathrm{d}_{A}\alpha\vert_{U}:=\sum_{a}(\mathrm{d}\alpha^{a}e_{a}+(-1)^{k}\alpha^{a}\wedge\nabla^{A}e_{a})$$ where $\alpha\vert_{U}=\sum_{a}\alpha^{a}e_{a}$ for coordinates $\alpha^{a}\in\Omega^{k}(U)$ . (Strictly speaking, I should write $\alpha^{a}\otimes e_{a}$ , but let me keep notation simple) Now I would like to prove the following: $$\mathrm{d}(\mathrm{tr}(\alpha\wedge\beta))=\mathrm{tr}(\mathrm{d}_{A}\alpha\wedge\beta)+(-1)^{k}\mathrm{tr}(\alpha\wedge\mathrm{d}_{A}\beta)$$ I have to say that I am not sure if this is actually true in this form. It is rather an educated  guess. For context, the reason for this is that something like this is implicitely used on page $4$ of arXiv:gr-qc/9905087 in the proof of invariance of the BF-action under translational symmetry. (In this context, the bundle $E$ is given by the adjoint bundle $\mathrm{Ad}(P)$ ). In this paper, the authors used ""integration by parts"" for an expression of the type $\mathrm{tr}(\mathrm{d}_{A}\eta\wedge F)$ , where $\eta\in\Omega^{d-3}(\mathcal{M},\mathrm{Ad}(P))$ and where $F\in\Omega^{2}(\mathcal{M},\mathrm{Ad}(P))$ denotes the curvature of $A$ . Now my attempt is the following: I think it is easier to proof this in the local frame on $U$ . Let us write $\alpha\in\Omega^{k}(\mathcal{M},E)$ and $\beta\in\Omega^{l}(\mathcal{M},E)$ in this frame, i.e. $$\alpha\vert_{U}=\sum_{a}\alpha^{a}e_{a}\hspace{1cm}\text{and}\hspace{1cm}\beta\vert_{U}=\sum_{a}\beta^{a}e_{a}$$ for real-valued coordinate forms $\alpha^{a}\in\Omega^{k}(U)$ , $\beta^{a}\in\Omega^{l}(U)$ . Then the above defined trace-wedge product is in coordinates given by $$\mathrm{tr}(\alpha\wedge\beta)\vert_{U}=\sum_{a,b}(\alpha^{a}\wedge\beta^{b})\langle e_{a},e_{b}\rangle_{E}$$ where $\langle e_{a},e_{b}\rangle_{E}$ is defined in the obious way, i.e. $\langle e_{a},e_{b}\rangle_{E}(x):=\langle e_{a}(x),e_{a}(x)\rangle_{E_{x}}$ . With this, the left-hand side of the conjectured equation is given by $$\mathrm{d}(\mathrm{tr}(\alpha\wedge\beta))\vert_{U}=\sum_{a,b}(\mathrm{d}\alpha^{a}\wedge\beta^{b}+(-1)^{k}\alpha^{a}\wedge\mathrm{d}\beta^{b})\langle e_{a},e_{b}\rangle_{E}$$ where we just used the standard Leibniz rule for real-valued forms. Now for the right-hand side, let us firstly write $\nabla^{A}e_{a}$ in terms of local connection $1$ -forms ${\omega^{i}}_{j}\in\Omega^{1}(U)$ via $$\nabla^{A}e_{a}=\sum_{b}{\omega^{b}}_{a}e_{b}.$$ With this, we have that $$\mathrm{d}_{A}\alpha\vert_{U}:=\sum_{a}(\mathrm{d}\alpha^{a}e_{a}+(-1)^{k}\sum_{c}(\alpha^{a}\wedge{\omega^{c}}_{a})e_{c})=\sum_{a}(\mathrm{d}\alpha^{a}+(-1)^{k}\sum_{c}(\alpha^{c}\wedge{\omega^{a}}_{c}))e_{a}$$ Now we know how the coordinate forms of $\mathrm{d}_{A}\alpha$ look like and hence we can compute the right-hand side: First of all, we have that $$\mathrm{tr}(\mathrm{d}_{A}\alpha\wedge\beta)\vert_{U}=\sum_{a,b}(\mathrm{d}\alpha^{a}\wedge\beta)\langle e_{a},e_{b}\rangle_{E}+(-1)^{k}\sum_{a,b,c}(\alpha^{c}\wedge{\omega^{a}}_{c}\wedge\beta^{b})\langle e_{a},e_{b}\rangle_{E}$$ Completely analogues, we find that $$\mathrm{tr}(\alpha\wedge\mathrm{d}_{A}\beta)\vert_{U}=\sum_{a,b}(\alpha^{a}\wedge\mathrm{d}\beta^{b})\langle e_{a},e_{b}\rangle_{E}+(-1)^{l}\sum_{a,b,c}(\alpha^{a}\wedge\beta^{c}\wedge{\omega^{b}}_{c})\langle e_{a},e_{b}\rangle_{E}$$ Hence, if the above formula is true, we must have that $$\sum_{a,b,c}(\alpha^{c}\wedge{\omega^{a}}_{c}\wedge\beta^{b}+(-1)^{l}\alpha^{a}\wedge\beta^{c}\wedge{\omega^{b}}_{c})\langle e_{a},e_{b}\rangle_{E}\stackrel{!}{=}0$$ I can't see why this is the case. Again, I should stress that I am not even sure if the claimed equality is true. Maybe it is false, or maybe the factor $(-1)^{k}$ is different. Furthemore, in the paper cited above this is only used for the case $E=\mathrm{Ad}(P)$ , $k=d-2$ and $l=2$ , where $d=\mathrm{dim}(\mathcal{M})$ , so maybe it is only true in this specific case. Any help and comment is appreciated!","['principal-bundles', 'vector-bundles', 'solution-verification', 'differential-forms', 'differential-geometry']"
4214563,Why is the component of a differentiable function also differentiable?,"I'm self studying with the Spivak book, and I'm trying to see a result that I know to be true but I'm having some time seeing how the maths pans out.  Supposing a function $f$ is differentiable.  Then given the projection operator $\pi^i$ where $\pi^i(f)=f^i$ , $f^i$ must also be differentiable. I've been trying to work it out by applying the chain rule $$Df^i(a)=D(\pi^i\circ f)(a)=D\pi^i(f(a))\circ Df(a)$$ since the projection operator is linear, it's derivative is simply itself $$Df^i(a)=\pi^i(f(a))\circ Df(a)$$ $$Df^i(a)=f^i(a)\circ Df(a)$$ I'm fairly sure the steps leading up to this point are correct, however it doesn't seem to make sense.  For example, let $f:\mathbb{R}\rightarrow\mathbb{R}^2$ $$f=\begin{bmatrix}x^2\\x\end{bmatrix}$$ then $$Df=\begin{bmatrix}2x\\1\end{bmatrix}$$ So if we let $i=1$ , $$\begin{align}
Df^1(a)&=f^1(a)\circ Df(a) \\
&=a^2\circ\begin{bmatrix}2a\\1\end{bmatrix}
\end{align}$$ Which I am certain is wrong, since $Df:\mathbb{R}\rightarrow\mathbb{R}^2$ , but $f^1:\mathbb{R}\rightarrow\mathbb{R}$ so it doesn't match up.  I think I have misunderstood either the derivative of the projection operator or how $f^i(a)$ forms a composition with $Df(a)$ .  Could someone point me in the right direction?  Thanks!","['self-learning', 'multivariable-calculus']"
4214569,How to obtain a certain set of matrices in GAP?,"I'd like to ask the following question: Let $A$ be a finite set of complex numbers. I would like to obtain in GAP the set $B$ of all $n\times n$ - matrices where in each row and column there is exactly one non-zero entry and this entry is from the set $A$ . Thus this set $B$ of $n\times n$ -matrices should have cardinality $m^n \cdot n!$ where $m$ is the cardinality of $A$ . Is there an easy way to obtain this set $B$ in GAP? Thank you very much. (Example: When $A$ is equal to the set $\{1\}$ , then we get for $B$ the set of $n\times n$ permutation matrices.)","['matrices', 'gap', 'group-theory', 'finite-groups']"
4214584,How to parametrise a hyperplane without making assumptions on the coefficients of its normal vector?,"Consider a vector $w\in\mathbb R^n$ , and its orthogonal complement: $$w^\perp\equiv\left\{v\in\mathbb R^n: \langle v,w\rangle\equiv\sum_{i=1}^n v_i w_i=0\right\}.$$ I'm looking into ways to parametrise $w^\perp$ . As also discussed in Parametric equation for a plane perpendicular to a vector , the trivial way to do this is to assume one parameter to be nonzero, and remove one free parameter in the standard way. In other words, assuming e.g. $w_n\neq 0$ , we have the parametrisation $$w^\perp=\left\{
t_1 e_1+ ...+t_{n-1}e_{n-1} - \frac{1}{w_n}\left(\sum_{k=1}^{n-1} w_k t_k\right) e_n: \,\, t_k\in\mathbb R
\right\},$$ using $n-1$ free parameters, as expected. This approach has, however, the downside of having to make assumptions on $w$ --- namely, it works when some coefficient is nonzero. While there is always some nonzero coefficient, this makes the solution somewhat unelegant. A slightly different approach is given in this answer to the question linked above. The idea, as far as I understand it, is that we can quite elegantly parametrise $w^\perp$ , if we accept using an overcomplete basis. This relies on the easy observation that all the vectors of the form $$v_{ij}\equiv w_i e_j - w_j e_i,$$ where $i\neq j$ , satisfy $\langle v_{ij},w\rangle=0$ . One can then write $$w^\perp = \left\{
\sum_{i<j} c_{ij} v_{ij} : \,\, c_{ij}\in\mathbb R
\right\}.$$ This uses $\binom{n}{2}$ coefficients to parametrise an $(n-1)$ -dimensional hyperplane, and thus is clearly overcomplete.
Geometrically, we are writing the orthogonal complement as a linear combination of vectors that are orthogonal to the projections of $w$ on two-dimensional subspaces.
From this point of view, a straightforward generalisation of the same idea would be to use as ""basis"" a set of vectors orthogonal to the projections of $w$ onto $k$ -dimensional subspaces, for $k=2,...,n-1$ . In some comments to the question linked above , a possible link with affine/projective geometry is hinted, but not fully explored. I'm curious to know if there is a good way to understand why this type of ""overparametrisation"" is possible from the point of view of projective geometry, and why it doesn't require making assumptions on some coefficients, which seems to be inevitable for any parametrisation that only uses the ""right"" amount of coefficients.","['analytic-geometry', 'projective-geometry', 'geometry', 'linear-algebra', 'affine-geometry']"
4214614,Why are these integrals equivalent to the Riemann Hypothesis?,"Riemann Hypothesis is equivalent to the integral equation $$\int_{-\infty}^{\infty} \frac{\log \mid \zeta (1/2+it)\mid }{1+4t^2} \ dt =0$$ Many other integral equations exist that are equivalent. How to show that they are equivalent ? They usually include absolute value of a function. Why is that ? I assume it came from a contour integral on the riemann sphere. The growth rate of the zeta function on the critical line also probably relates to all those integrals , not ? Another example is Establishing the exact value $$\int_{0}^{\infty}\frac{(1-12t^2)}{(1+4t^2)^3}\int_{1/2}^{\infty}\log|\zeta(\sigma+it)|~d\sigma ~dt=\frac{\pi(3-\gamma)}{32}$$ is equivalent to the Riemann Hypothesis. More examples : Riemann's Hypothesis is true if and only if $$\frac{1}{\pi}\int_0^{\infty} \log\left|\frac{\zeta(\frac{1}{2}+it)}{\zeta(\frac{1}{2})}\right|\ \frac{dt}{t^2}=\frac{\pi}{8}+\frac{\gamma}{4}+\frac{\log 8\pi}{4}-2$$ Take $a\in R$ with $\frac{1}{2}\leq a<1$ . Riemann's $\zeta$ -function has no zeros in $\Re(s)>a$ if and only if $$\frac{1}{\pi}\int_0^{\infty} \log\left|\frac{\zeta(a+it)}{\zeta(a)}\right|\ \frac{dt}{t^2}=\frac{\zeta'(a)}{2\zeta(a)}-\frac{1}{1-a}$$ And many more exist. I have no idea how to get to such conclusions or prove them. Im not even sure how to prove integrals for ""slightly easier"" cases , meaning not famous open problems but zero's of other nontrivial functions that are not on a half-plane.","['calculus', 'definite-integrals', 'riemann-hypothesis']"
4214689,Can an differential equation be a member of many classes of differential equations?,"I just started learning ODE online and I'm not sure I fully understand when I should use one method or the other. For example, I know that $\frac{dy}{dx}=\frac{y}{x}$ is separable, but is it homogeneous? I tried to solve it using the homogeneous approach but I get nonsense. Another example is bernoulli equations, like $\frac{dy}{dx}-\frac{y}{x}=xy^{n}$ whenever $n=1$ I also get nonsense, can anyone help me?",['ordinary-differential-equations']
4214706,Example of a function of $\mathbb{Z} / 4\mathbb{Z}$ in itself that is not polynomial.,"I am trying to give an example of a function of $\mathbb{Z} / 4\mathbb{Z}$ in itself that is not polynomial. For that I thought to give some characterization of the polynomial functions of $\mathbb{Z} / 4\mathbb{Z}$ in itself. I took a polynomial $P\in \mathbb{Z} / 4\mathbb{Z}[X]$ such that $P=\sum_{i=0}^{n}a_iX^i$ with $a_i\in \mathbb{Z} / 4\mathbb{Z}$ . Then: $$
P(2)=a_12+a_0
$$ Then if $f$ is a function of $\mathbb{Z} / 4\mathbb{Z}$ such that $f(0)=0$ and $f(2)=1$ . Then if $f$ is polynomial function so there is $P\in \mathbb{Z} / 4\mathbb{Z}[X]$ such that $P=\sum_{i=0}^{n}a_iX^i$ and $f(x)=P(x)$ for all $x\in \mathbb{Z} / 4\mathbb{Z}$ . But $$
f(0)=a_0=0\qquad \text{and}\qquad f(2)=a_1 2+a_0=a_1 2=1
$$ So $2\in (\mathbb{Z} / 4\mathbb{Z})^{×}$ . Contradiction. Is this proof correct?","['functions', 'abstract-algebra']"
4214736,Differential equation involving Newton's law of gravitation,"Newton's law of gravitation states that the acceleration of an object at a distance $r$ from the centre of an object of mass $M$ is given by $$\frac{d^2r}{dt^2}=-\frac{GM}{r^2},$$ where $G$ is the universal gravitational constant. (a) Use the identity $$\frac{d^2r}{dt^2}=\frac{d}{dr}\left(\frac12v^2\right),$$ combined with integration with respect to $r$ . Determine the resulting constant of integration using the condition $u=v(R)$ and show that $$v^2-u^2=\frac{2GM}{r}-\frac{2GM}{R}$$ (b) Now write $r=R+s$ where $s$ is the height of the object above the surface
of the Earth, radius $R$ and mass $M$ . Use the binomial series to expand the
factor $(1+s/R)^{-1}$ to show that, close to the surface of the Earth, $$v^2\approx u^2-2gs,$$ for some constant $g$ . Find the expression for $g$ . Reminder: The binomial series is $(1+x)^{-1}=1-x+x^2-x^3+\cdots$ , which converges for $|x|\lt 1$ . (In case it is unclear, I am assuming that $G$ and $M$ are constants, and $v$ is purely a function of $r$ , since that's what the question appears to mean.) So far I have done the following: I equated the two expressions for $\frac{d^2r}{dt^2}$ (which I'm not sure is correct, since if two functions have the same derivative, they may differ by a constant, so if their second derivatives are equal then I feel as though there should be two constants, but I haven't included any constants), then integrated both sides with respect to $r$ . $$\int \frac{d}{dr} \left(\frac12v^2\right) dr = -GM \int \frac{1}{r^2} dr$$ $$\implies\frac12v^2=\frac{GM}{r}+C$$ $$\implies v^2=\frac{2GM}{r}+C$$ $$\implies v=\sqrt{\frac{2GM}{r}+C}$$ $$u=v(R)=\sqrt{\frac{2GM}{R}+C}$$ $$\implies u^2=\frac{2GM}{R}+C$$ Since the constants are the same, they cancel when subtracted. $$v^2-u^2=\frac{2GM}{r}-\frac{2GM}{R}$$ I'm doubtful if this method is correct because of the problem with the constants I referred to before, and also because I couldn't ""determine the resulting constant of integration using the condition $u=v(R)$ "" as the question asked, rather I just cancelled the constants with subtraction. Is equating the two expressions for $\frac{d^2r}{dt^2}$ allowed? As for part (b), I think I need the constant of integration from (a) to get the full expression for $v^2$ , which I do not know how to find.",['ordinary-differential-equations']
4214743,"Understanding total, quadratic, and $\Phi$ variation of functions","I've started to study stochastic calculus on my own recently (I'll read Fima's book for a simpler introduction and then Steele's for a maybe more formal approach). I've come across the definition of the total variation, quadratic variation and other such as $\Phi$ -variation. For the more general definition, the $\Phi$ -variation of a function $g$ in the interval $(0,t]$ is calculated as $$ V_{\Phi}(g) := \sup \sum_{i=1}^{n} \Phi\left(|g(t^n_i) - g(t^n_{i-1})|\right) $$ for a partition $0= t_0 < t_1 < \ldots < t_n = t$ of the interval $(0,t]$ . The most used for stochastic calculus as I see for now is the quadratic varaition, where $\Phi(u) = u^2$ and the total variation where $\Phi(u) = u$ . I have a few questions about it though. First, I'd like to see if my understading is right. As I see, the total variation of a function measures the oscillation of the function in the interval, thus $V_{\Phi}(g)(t) < \infty$ for $\Phi (u) = u$ means that the function is ""well behaved"". For instance, a function such as $f(t)=t\sin(1/t)$ has infinite variation for whatever interval that contains the zero because near the $0$ the function oscillates a lot. Is my understanding correct or am I missing something for the meaning of the total variation? Second, I guess so far that the use of quadratic variation is important because of the moments of stochastic process: I imagine that in the future, when we want to write Ito's integral, we will want the process to have finite second moment and there might be a link between this and the quadratic variation. But I can't see any use for $\Phi$ -variation. Can someone point me out a usage for that in stochastic calculus?","['probability-theory', 'bounded-variation', 'quadratic-variation', 'total-variation', 'stochastic-calculus']"
4214746,Difference between value of a function at a point and its limit at that point?,"As a high school student,I understand the basic,theoretical difference between the two, as in, limit is what that function approaches as the input approaches something (but never equal to it) or how it behaves near that point etc etc. But sometimes it doesn't seem to make sense.
Like Some textbooks, when evaluating certain simple limits like this: They just substitute the value 4 and evaluate it like this: But then how is it different than evaluating the function itself at that point? One can say that here the limit as well as the value at that point will be same, but I'm pointing out the method used here, SIMPLE SUBSTITUTION! Along with a better understanding of difference between the two, I would also like to understand what it means when both the value at that point and limit are defined but still they are different .Because I have been told that limits are used to evaluate undefined values/expressions like $\frac{0}{0}$ , $\infty / \infty$ , $0^{0}$ etc. For Ex- Both are well defined, still different.What does that difference mean in cases like these?","['limits', 'calculus', 'continuity']"
4214753,How can one integrate $(z^3)/[(z-6)(z^5-z+6)]$ along the circular path $|z|=5$?,"My aim is to determine the value of $$\int_C f(z) \, dz = \int_{|z|=5} \frac{z^3}{(z-6)(z^5-z+6)}\,dz.$$ The quintic factor is troublesome. By splitting it as $(z^5)+(-z+6)$ and applying Rouché's Theorem, we see that all five roots are within our contour. By Decartes' Rule and by looking at its derivative and intercept, it has only one real root $\rho_1$ , which is negative. (We can factor via this root, but I don't see the use: $(z-\rho_1)(z^4+\rho_1 z^3 +\rho_1^2 z^2+\rho_1^3 z +\rho_1^4-1)$ .) By writing $z^3=((z-6)+6)^3=(z-6)^3 +3(6)(z-6)^2 +3(36)(z-6)+216,$ we can reduce our problem to finding $$\int_C f(z)\,dz=216\int_{|z|=5}\frac{1}{(z-6)(z^5-(z-6))}\,dz.$$ I have not gotten much further. That the quintic can be written as $z^5-(z-6)$ or $z(z^4-1)+6$ occurred to me. I also thought that if I could show that the five roots are distinct, then the poles are simple ones and the formula $$
\operatorname*{Res}_{z=\rho_j}f(z)=\lim_{z\to \rho_j}(z-\rho_j)g(z)
$$ might apply, but then we'd express the answer in terms of the $\rho_j$ ? I am thinking rather that a concrete number can be given for the answer, since this is a past qualifying exam question.","['complex-analysis', 'complex-integration', 'residue-calculus']"
4214756,Examples of polynomial injections $f:\mathbb{N}\times \mathbb{N}\to \mathbb{N}$,"I've seen that there are polynomial bijections $f:\mathbb{N}\times \mathbb{N}\to \mathbb{N},$ for example $f(m,n)=\frac{1}{2}(n+m)(n+m-1)+m.$ I'm looking for more examples of injective polynomials from $\mathbb{N}\times \mathbb{N}\to \mathbb{N}.$ Are these common? Are there simple examples that are easy to prove injective? Also, I should clarify that I am looking for an example that is fundamentally different than the example I gave, not just modifications of it. Preferably there would be a simple explanation of why this example was injective.","['number-theory', 'soft-question', 'polynomials', 'natural-numbers']"
4214818,What does $\mathbb R \setminus \left\{ 1 \right\}$ mean with respect to a domain? [duplicate],This question already has answers here : Meaning of the backslash operator on sets (2 answers) Closed 2 years ago . I have a true/false question asking if a composite function's domain is $\mathbb R \setminus \left\{ 1 \right\}$ . Whether or not this is true is unimportant. I'm just unfamiliar with the forward slash curly bracket folowing the all real numbers symbol $\mathbb R $ . Does anyone know what that $\setminus \left\{ 1 \right\}$ means? Thanks in advance.,"['notation', 'algebra-precalculus']"
4214827,Lower-bounding minimal eigenvalue via the Schur complement,"Suppose that $$M=\left(
\begin{array}{cc}
A & B\\
B^\top & C 
\end{array}
\right)$$ for some symmetric matrices $A$ and $C$ , and $C$ is invertible. Is it true that: $$\lambda_{\min}(M) \ge \min\left\{\lambda_{\min}(C)~,~\lambda_{\min}(A-BC^{-1}B^\top)\right\}~?$$ Here, $\lambda_{\min}$ refers to the minimum eigenvalue of a matrix. Even if the above inequality is incorrect, is there a way to lower bound $\lambda_{\min}(A)$ in terms of $\lambda_{\min}(C)$ and $\lambda_{\min}(A-BC^{-1}B^\top)$ ? You may even assume that $M$ is non-negative definite, and $$A=u^\top u~, ~B = u^\top X~\textrm{and}~C= X^\top X$$ for some vector $u$ and some matrix $X$ , if that helps! Any help will be greatly appreciated!","['schur-complement', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'upper-lower-bounds']"
4214903,Local Models for shifted symplectic structures,"I have a problem to get the signs right in the calculation in the proof of Prop 1.21 of the paper https://arxiv.org/pdf/1111.3209.pdf on shifted symplectic structures. The authors obtain a ètale local model for the canonical $n$ -shifted symplectic structure $\omega$ on the $n$ -shifted derived cotangent stack $T^{\ast}X[n]$ of a derived Deligne Mumford stack $X$ . So localy $X=Spec(k[x_{1},...,x_{k}])$ with $|x_{i}|\leq 0$ is quasi free and $T^{\ast}X[n] = Spec(k[x_{1},...,x_{k},y_{1},...,y_{k}])=B$ with $|y_{i}| = n-|x_{i}|$ . The symplectic structure is given locally as the element $\omega = \sum_{i=1}^{k} (-1)^{|y_{i}|}d_{dR}y_{i} \wedge d_{dR}x_{i} \in (Sym^{2}(\Omega_{T^{\ast}X[n]}[1])[n-2])^{0}$ . I am interested in the associated morphism $\omega^{\#}:T_{T^{\ast}X[n]} =\oplus_{i=1}^{k} B\partial x_{i} \oplus_{i=1}^{k} B \partial y_{i}\rightarrow \Omega_{T^{\ast}X[n]}[n] = \oplus_{i=1}^{k} B d_{dR}x_{i}[n] \oplus_{i=1}^{k} B d_{dR}y_{i}[n] $ It is claimed by the authors that the morphismis given on generators by $\partial x_{i} \mapsto -(-1)^{|y_{i}|}dy_{i}[n]$ and $ \partial y_{i} \mapsto (-1)^{|x_{i}|}dx_{i}[n]$ Is there an easy way of seeing this? I have tried to reproduce thsi morphism many times, but I never obtained the right signs. Thank you very much for your help.","['homological-algebra', 'algebraic-geometry', 'symplectic-geometry']"
4214907,How to understand this helping lemma for mean value theorem which involves inner product?,"In Kolk's Multidimensional Real Analysis I: Differentiation He used the following helping lemma 2.5.1 to prove the following mean value theorem 2.5.3 Equivalent to the equation (2.16) is the equation: $\langle a, f(x)-f(x') - Df(\xi)(x-x') \rangle=0$ , from which we can conclude 2 cases: case 1, $f(x)-f(x') - Df(\xi)(x-x') = 0$ . It is easy to understand the meaning as in the 1-dimensional situation: we can find a point $\xi$ in between the segment $xx'$ at which the directional derivative in the direction $x-x'$ (or,  the rate of increase of the function $f$ at the point $\xi$ in this direction) is the same as $f(x)-f(x')$ case 2, $f(x)-f(x') - Df(\xi)(x-x') \neq 0$ but still orthogonal to $a$ . Questions:
Is my interpretation of case 1 good enough? How to interpret case 2? Note that the reason the author used inner product to prove this lemma is to lay the foundation of proving the mean value theorem which uses ""norm"" in its statement.","['inner-products', 'multivariable-calculus', 'real-analysis']"
4214953,Combinatoric question about difficult and easy problems,"Three children attempted one hundred problems. Each child solved sixty problems, and every problem was
solved by at least one child. A problem is called difficult if it was only solved by one child, and called easy if it
was solved by all three children.
How many more easy problems were there than difficult problems? I have tried to make progress on this question but I always end up with more difficult questions than easy questions. I got this question from a homework sheet from my teacher but I don't know the textbook it is from. I have been working on this question for over an hour and I have made no progress. I used the example that: Child 1 solved questions $1-60$ , but not $61-100.$ Child 2 solved $41-100$ but not $1-40$ (so each question is at least solved by one child) Child 3 solved questions $1-60$ but not $61-100.$ So the questions where all three children solved them was $41-60,$ but questions $61-100$ was where only one person solved it. This means that there is $40$ hard questions and $20$ easy questions, meaning there was $-20$ more easy problems than hard questions. It is quite an interesting problem because it seems easy to explain, but not easy to solve. I did get an answer of $-20 $ when I tried an example, but it didn't seem right to me because I did not expect the answer to be negative, so I thought my method was wrong. I want to know how to solve these types of questions in the future.","['inclusion-exclusion', 'combinatorics']"
4214985,Find the sum of squares of all eigenvalues of a matrix,"I found this question which asks to find the sum of squares of all eigenvalues (possibly complex, not necessarily distinct) of (I used a picture because it was hard to write matrix down without making any mistakes) Now, I used this to find the eigenvalues, and squared them manually to see that the answer is $38$ . But, of course, that's not the way to solve it. There must be some patterns in this matrix that I am missing. All I can find is that this is a $14\times 14$ matrix, and there is an $11$ -triangle of zeroes at the left bottom, and a $10$ -triangle of zeroes at the right up. Also, most of the matrix is full of zeroes and almost all the entries are at the diagonal. But, these are not enough to solve the problem. Also, is there any tricks in finding sum of squares (without finding the exact eigenvalues) that will reduce our effort? As achille hui pointed out, sum of square of eigenvalues = trace of square of matrix. So, now I need to have ideas of squaring this matrix. It doesn't look simple enough to just multiply using traditional methods, there must be some tricks. Thanks in advance","['matrices', 'contest-math', 'eigenvalues-eigenvectors']"
4215030,Baby Rudin Theorem 7.18,"Here is Theorem 7.18 from Baby Rudin: There exists a real continuous function on the real line which is nowhere differentiable. Here is a proof of the theorem: Define $$\tag{34} \varphi(x) = \lvert x \rvert \qquad \qquad (-1 \leq x \leq 1) $$ and extend the definition of $\varphi(x)$ to all real $x$ by requiring that $$ \tag{35} \varphi(x+2) = \varphi(x). $$ Then, for all $s$ and $t$ , $$\tag{36}  \lvert \varphi(s) - \varphi(t) \rvert \leq \lvert s-t \rvert. $$ In particular, $\varphi$ is continuous on $\mathbb{R}^1$ . Define $$ \tag{37} f(x) = \sum_{n=0}^\infty \left( \frac{3}{4} \right)^n \varphi \left( 4^n x \right). $$ Since $0 \leq \varphi \leq 1$ , Theorem 7.10 shows that the series (37) converges uniformly on $\mathbb{R}^1$ . By Theorem 7.12, $f$ is continuous on $\mathbb{R}^1$ .
Now fix a real number $x$ and a positive integer $m$ . Put $$ \tag{38} \delta_m = \pm \frac{1}{2} \cdot 4^{-m} $$ where the sign is so chosen that no integer lies between $4^m x$ and $4^m \left( x + \delta_m \right)$ . This can be done, since $4^m \left\lvert \delta_m \right\rvert = \frac{1}{2}$ . Define $$ \tag{39} \gamma_n = \frac{ \varphi \left( 4^n \left( x + \delta_m \right)  \right) - \varphi \left( 4^n x \right)  }{ \delta_m }. $$ When $n > m$ , then $4^n \delta_m$ is an even integer, so that $\gamma_n = 0$ . When $0 \leq n \leq m$ , (36) implies that $\left\lvert \gamma_n \right\rvert \leq 4^n$ .
Since $\left\lvert \gamma_m \right\rvert = 4^m$ , we conclude that $$
\begin{align}
\left\lvert \frac{ f \left( x + \delta_m \right) - f(x)  }{ \delta_m  }  \right\rvert &= \left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert \\ 
&\geq 3^m - \sum_{n=0}^{m-1} 3^n \\
&= \frac{1}{2} \left( 3^m + 1 \right).
\end{align}
$$ As $m \to \infty$ , $\gamma_m \to 0$ . It follows that $f$ is not differentiable at $x$ . I have two questions regarding the proof. At $(38)$ it's stated that the sign is so chosen that no integer lies between $4^m x$ and $4^m \left( x + \delta_m \right)$ . Why is it necessary that no integer should be within that range and what would happen if they were? At the later part of the proof we have: $$
\begin{align}
\left\lvert \frac{ f \left( x + \delta_m \right) - f(x)  }{ \delta_m  }  \right\rvert &= \left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert \\ 
&\geq 3^m - \sum_{n=0}^{m-1} 3^n \\
&= \frac{1}{2} \left( 3^m + 1 \right).
\end{align}
$$ I want to know why is the following part true: $$
\begin{align}
\left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert 
&\geq 3^m - \sum_{n=0}^{m-1} 3^n \\
\end{align}
$$ Any help is appreciated!","['derivatives', 'sequences-and-series', 'analysis', 'real-analysis']"
4215045,A question on Cantor's proof of countability of algebraic numbers.,"So the exercise 2.2 in Baby Rudin led me to Cantor's original proof of the countability of algebraic numbers. See here for a translation in English of Cantor's paper. The question I have is regarding the computation of the height function as defined by Cantor, for the equation: $$\begin{equation}a_0\omega^n+a_1\omega^{n-1}+\dots+a_n=0\tag{1}\end{equation}$$ where all coefficients are integers. Here is the relevant bit from Cantor: If we go back to equation (1), which an algebraic number $\omega$ satisfies and
which, according to our restrictions, is completely determined, we can call the
sum of the absolute values of the coefficients and the number $n-1$ (where $n$ is the degree of $\omega$ ) the height of the number $\omega$ and denote it with $N$ ; using the now common notation, we therefore have $$N=n-1+|a_0|+|a_1|+\dots+|a_n|.\tag{3}$$ According to this, the height $N$ is for each real algebraic number a specified positive integer; conversely for each positive integer value of $N$ there are only a
finite number of algebraic real numbers with height $N$ ; let the number of these be $\varphi(N)$ ; for example, $\varphi(1)=1$ ; $\varphi(2)=2$ ; $\varphi(3)=4.$ Question: But when I try to compute $\varphi(N)$ , it doesn't match with Cantor's results! For example, consider $\varphi(2)$ - there are two possible cases, one for degree $n=1$ and other for degree $n=2$ . For $n=1$ we have from (3), $$2=1-1+|a_0|+|a_1|\implies 2=|a_0|+|a_1|\implies|a_0|=|a_1|=1\quad\text{or}\quad |a_0|=2$$ which corresponds respectively to the equations $\omega\pm 1=0$ and $2\omega = 0$ . Thus for $n=1$ alone we get $3$ such $\omega$ . Where is my mistake in computing $\varphi$ ?","['elementary-set-theory', 'real-analysis']"
4215066,"SDE for $Y_t = \frac{1}{Z_t}$ where $Z_t =\exp \left(\int_0^t X_u \, dW_u-\frac{1}{2}\int_0^t X^2_u \, du \right)$","The problem is the following (Problem 3.10 from Karatzas/Shreve): Let $Z_t = \exp \left(\int_0^t X_u \, dW_u-\frac{1}{2}\int_0^t X^2_u \, du \right)$ . Define $Y_t=1/Z_t$ . Show the following stochastic differential equation holds \begin{align*}
dY_t=Y_tX_t^2\,dt-Y_tX_t\,dW_t, \quad Y_0=1
\end{align*} where we have assumed $\int_0^t X_u^2 \, du<\infty$ almost surely for all $0<t<\infty$ , and $W_t$ is the standard Brownian motion. My questions are: How can we prove $Y_t$ is a semi-martingale? For the differential equation, this is my try (which I assume $Y_t$ is a semi-martingale \begin{align*}
Y_tZ_t=1 \ \Longrightarrow Z_t\,dY_t + Y_t\,dZ_t + d\langle Y_t, Z_t \rangle = 0
\end{align*} Since $Y_tZ_t=1$ , we have $\langle Y_t, Z_t\rangle = 0$ . Also, from the textbook, it is shown $dZ_t= Z_tX_t \,dW_t$ So from the above we have \begin{align*}
Y_tZ_t \,dY_t = -Y_t^2 \, dZ_t \ \Longleftrightarrow\ dY_t = -Y_t^2\,dZ_t = -Y_tX_t\,dW_t
\end{align*} since $dZ_t = Z_tX_t\,dW_t$ and $Z_tY_t=1$ . However, it seems the $dt$ term is missing.... I can't really see what is wrong in the calculation though. Does anyone have any comments?","['stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4215112,How to derive an orientation from a triangulation?,"Suppose some connected, closed 2-manifold $M$ , and suppose I have a triangulation $t:M\rightarrow S$ where $S$ is a homeomorphic simplicial complex such that for each individual 2-cell I can specify a cyclic order on the vertices so that any 2 triangles sharing an edge disagree on the order of their 2 shared vertices, illustrated. Does this necessarily imply that $M$ is topologically orientable, in the sense that for each point $x \in M$ there is a local orientation i.e. choice of generator $\mu_x \in H_2 (M |x)$ such that there is an open ball $B$ around $x$ where all points $y \in B$ have local orientations $\mu_y$ that are the images of one generator $\mu_B$ of $H_2(M|B)$ under the natural maps $H_2(M|B) \rightarrow H_2(M|y)$ ? I feel like I'm missing something very obvious. I understand the geometric intuition as to why a consistent choice of clockwise or counterclockwise would induce opposite orientations on each edge. I can also see how, for any given triangle, an order on its vertices could produce a choice of generator for any point on the interior of the triangle, but I don't understand how to prove consistency across the entire manifold. Specifically, how do I find the orientation of points lying on the edges and vertices of the triangulation? And why doesn't this work if there isn't an ordered triangulation where all the edges disagree?","['surfaces', 'triangulation', 'orientation', 'general-topology', 'algebraic-topology']"
4215182,Name/source for cardinal invariant in topology,"Given a topological space $\mathcal{X}=(X,\tau)$ and a set $A\subseteq X$ , say that $A$ is $\mathcal{X}$ - sufficient iff every continuous function $(A,\tau_A)\rightarrow\mathbb{R}$ extends to a unique continuous function $\mathcal{X}\rightarrow\mathbb{R}$ (where we equip $\mathbb{R}$ with the usual topology). Let $\mathfrak{suff}(\mathcal{X})$ be the smallest cardinality of an $\mathcal{X}$ -sufficient set; what is the general term for this (and what are some good sources)? Note that we can replace $\mathbb{R}$ with any topological space $\mathcal{C}$ here and get the analogous notions $(\mathcal{X},\mathcal{C})$ -sufficiency and and $\mathfrak{suff}_\mathcal{C}(\mathcal{X})$ . Changing the target space, even staying in the realm of ""rich"" target spaces, can drastically alter the value of the relevant sufficiency cardinal. For example, $\mathfrak{suff}(\mathbb{R})=2^{\aleph_0}$ , since every disconnected subspace admits a continuous function to $\mathbb{R}$ not extendible to all of $\mathbb{R}$ and every subspace missing an interval has too many continuous extensions of (say) the constant-zero function (so the only sufficient subspace is $\mathbb{R}$ itself), but $\mathfrak{suff}_\mathcal{\mathcal{D}}(\mathbb{R})=1$ for every totally disconnected $\mathcal{D}$ . I'm more broadly interested in the whole function $\mathfrak{suff}_{-}(-)$ , but the specific case $\mathcal{C}=\mathbb{R}$ seems more likely to have a lot of material easily available. (This is related to a variation of this recent question of mine - basically, given a first-order sentence $\varphi$ and a space $\mathcal{X}$ with $C(\mathcal{X},\mathbb{R})\models\varphi$ , how small a subspace $\mathcal{Y}\subseteq\mathcal{X}$ can I find such that $C(\mathcal{Y},\mathbb{R})\models\varphi$ ?)","['logic', 'reference-request', 'general-topology', 'set-theory', 'terminology']"
4215218,Question about partial derivative at a point.,"I'm currently studying Partial Derivatives and ran into this problem: $f(x,y) = \begin{cases} \frac{x^2}{x^2+y^4}, & (x,y) \neq (0,0) \\ 0, & (x,y) = (0,0) \end{cases}$ In this question, we are supposed to find $\frac{\partial f}{\partial x}$ at $(0,0)$ . (Sorry if this question seems very easy, but I'm new to the material). From what I know, we use the definition as follows: $\frac{\partial f}{\partial x}(x_0,y_0) = \lim_{\delta x \to 0} \frac{f(x_0+\delta x,y_0) - f(x_0,y_0)}{\delta x}$ . At $(0,0)$ we have $f(x,y) = 0.$ Therefore, $\lim_{\delta x \to 0} \frac{0 - 0}{\delta x} = 0$ . Is this correct? Or do we do the following: $\lim_{\delta x \to 0} \frac{\frac{\delta x^2}{\delta x^2 + 0^4}-\lim_{(x,y) \to (0,0)} \frac{x^2}{x^2 + y^4}}{\delta x}$ We can observe that $\lim_{(x,y) \to (0,0)} \frac{x^2}{x^2 + y^4}$ does not exist as it converges to different values at different paths (try $x = y$ and $x = y^2$ ). So, $\frac{\partial f}{\partial x}(0,0)$ does not exist. Now I believe the first solution is correct because the question says that at $(0,0)$ , the function is equal to zero and so we only consider the zero while finding the limit at that point. But I'm not 100% sure. I talked about this with other students and I found conflicting answers. Would it be possible for someone to tell me which answer is the correct one? A brief explanation would be very much appreciated. Thank you.","['differential', 'limits', 'calculus', 'derivatives']"
4215233,Classify the embedding of Lie groups: $U(1)$ in $SU(2)$ versus $U(1)$ in $SO(3)$,"I am interested in knowing the way to embed a Lie group to another Lie group. For example, we can
embed $$U(1) \subset SU(2) \tag{1}$$ also $$U(1) \subset SO(3). \tag{2}$$ Here they are all regarded as some Lie groups. But in terms of differentiable manifolds, $SU(2) \cong S^3$ as a 3-sphere and $SO(3)  \cong RP^3$ as a real protective space both in real 3-dimensions. Now, my question is that how do we classify the way of their embedding? Here I must specify a way to define what exactly is a classification. But I am not certain what is the correct math definition. What I can prescribe is that there should be some identification of deformations between different embedding. As long as the embedding map can be continuous deformed to each other (maybe in the sense of homeomorphic), then the embedding maps are identified. Question: Is there a concrete math definition of such classification of the Lie group embedding identified via continuous deformations or homeomorphic)? My take is that to classify the embedding of Lie groups: $U(1)  \cong S^1 \subset SU(2) \cong S^3$ , since they are all Lie groups so their identities must be the same identity group element $1$ . Different embedding of $U(1)$ must intersect at the identity point on $SU(2)$ . Next we can continuously deform such $U(1)$ on $SU(2)$ . For those maps which cannot be continuous deformed, I expect that they are classified by the homotopy class $$
[U(1), SU(2)]=[S^1,S^3]=\pi_1(S^3)=0.
$$ Thus there is only one way of embedding of $U(1) \subset SU(2)$ in terms of homotopy or homeomorphic. To classify the embedding of Lie groups: $U(1)  \cong S^1 \subset SO(3) \cong RP^3$ , since they are all Lie groups so their identities must be the same identity group element $1$ . Different embedding of $U(1)$ must intersect at the identity point on $SO(3)$ . Next we can continuously deform such $U(1)$ on $SO(3)$ . For those maps which cannot be continuous deformed, I expect that they are classified by the homotopy class $$
[U(1), SO(3)]=[S^1,RP^3]=\pi_1(RP^3)=\mathbf{Z}/2.
$$ Thus there are two ways of embedding of $U(1) \subset SO(3)$ in terms of homotopy or homeomorphic. In terms of the lift map, we can see that the nontrivial $\mathbf{Z}/2$ class of the embedding $[U(1), SO(3)]$ is the obstruction to lift the $U(1) \to SO(3)$ to $U(1) \to SU(2)$ .
And a lift diagram is like: $$
\begin{array}{ccc}
  &  & SU(2)\\
          &\nearrow &           \downarrow\\
  U(1) & \longrightarrow & SO(3)
\end{array}.
$$ Please correct me and point toward the good way to think about the classifications of the embedding of Lie groups.","['algebraic-topology', 'homotopy-theory', 'general-topology', 'lie-groups', 'differential-geometry']"
4215255,The set that $(f_n)$ converges to a real or a rational number is measurable,"Let $(f_n(x)):\Omega\rightarrow \mathbb{R}$ be a sequence of measurable functions, the sets $$A=\{x \in \Omega: (f_n(x)) \text{ converges to a real number}\}$$ $$B=\{x \in \Omega: (f_n(x)) \text{ converges to a rational number}\}$$ $$C=\{x \in \Omega: (f_n(x)) \text{ converges to an irrational number}\}$$ are measurable? This type of question is answered in many places, but there are a few confusions that I have after reading. For example: The set that $(f_n)$ converges to a real number is measurable , it says $\{x \in \Omega: (f_n(x)) \text{ converges to a real number}\}$ is equivalent to $\{x \in \Omega: (f_n(x)) \text{ is Cauchy}\}$ . Here are my confusions: I understand that $\mathbb{R}$ is complete, but why do we work with Cauchy instead of convergence itself? Here: $$A=\{x\in\Omega: \forall k \in \mathbb{N},\, \exists N \in \mathbb{N} \text{ s.t. } \forall n>N,\, |f_n(x)-r_x| < 1/k\}$$ where $r_x\in\mathbb{R}$ is the pointwise limit of $(f_n(x))$ , $x\in \Omega$ . Then $$A = \bigcap_{k\in\mathbb{N}}\bigcup_{N\in\mathbb{N}}\bigcap_{n > N} \{x\in\Omega:|f_n(x)-r_x|<1/k\}$$ Since $r_x$ are constants, therefore the set $\{x\in\Omega:|f_n(x)-r_x|<1/k\}$ is measurable, and therefore $A$ is measurable. (right???) I don't see why we can't work with convergence directly. Suppose what they did in the link is correct, why can we ignore what number that $(f_n(x))$ converges to? What if $f_n(x)$ converges to a rational (set $B$ ), or an irrational (set $C$ )? The arguments are the same? And does that mean $B=C$ if we only need $(f_n(x))$ to be Cauchy? It does not feel right. Thanks for any insights and help.","['measure-theory', 'sequence-of-function', 'real-analysis', 'calculus', 'sequences-and-series']"
4215329,Show that $\int_0^\infty \frac{\tan^{-1} ax-\tan^{-1} x}{x}~dx = \frac{1}{2}\pi ~ \ln(a)$,"How can I show that $\int_0^\infty \frac{\tan^{-1} ax-\tan^{-1} x}{x}~dx = \frac{1}{2}\pi \ln(a)$ ? I tried by using $\tan^{-1} ax-\tan^{-1} x = \int_1^a\frac{1}{1+(ux)^2}~du$ and incorporate this in the original expression: $$\int_0^\infty \frac{1}{x}\left[ \int_1^a \frac{1}{1+(ux)^2}~du\right] dx$$ and then switching the order of integration: $$\int_0^a \left[~ \int_1^\infty \frac{1}{x}~\frac{1}{1+(ux)^2}~dx\right] du$$ $$\\$$ which doesn't seem to lead anywhere. But I somehow want to use $\int_0^\infty \frac{\sin x}{x}~dx=\frac{1}{2}\pi$ , because the arctan function is involved in the process of showing this by  differentiating $\int_0^\infty e^{-ux}\frac{\sin x}{x}~dx$ with respect to u (aka Feynman's method). The lower bond here is $0$ though, and in the integral above the lower bond is $1$ , so I don't know if that's the right way to go either. Or is there possibly a different way to try here? $$\\$$ $$\\$$ (The choice of the Fourier-tags is because this is from a book on the subject)","['integration', 'fourier-analysis', 'fourier-transform']"
