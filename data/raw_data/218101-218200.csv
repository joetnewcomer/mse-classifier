question_id,title,body,tags
4452858,Hausdorff dimension of sets with positive Lebesgue measure,"I am reading Hausdorff Dimension, Its Properties, and Its Surprises by Dierk Schleicher. Among the elementary properties of the Hausdorff dimension, the last one is: If $X\subset \Bbb R^n$ has finite positive $d$ -dimensional Lebesgue measure, then $\dim_H X = d$ . My work. It will be enough to show that $\mathcal H^s(X) = 0$ for all $s > d$ , and $\mathcal H^s(X) = \infty$ for all $s < d$ . As usual, $$H^s(X) = \lim_{\delta\to 0} H^s_\delta(X)$$ where $$\mathcal H^s_\delta(X) = \inf\left\{\sum_{i=1}^\infty |U_i|^s: \{U_i\} \text{ is a }\delta\text{-cover of }X  \right\}$$ I'm unable to relate the Lebesgue measure with coverings of $X$ , which would help me find a connection with $\mathcal H^s_\delta(X)$ for given $\delta > 0$ . Thanks a lot!","['measure-theory', 'dimension-theory-analysis', 'analysis']"
4452869,Pseudo-periodicity of analytic self-maps of the upper half-plane,"I have a couple of questions, in increasing order of softness: Consider an analytic map of the upper-half plane into itself $f: \mathbb{H}\to\mathbb{H}$ . When this function is $1$ -periodic, i.e., $f(z+1) = f(z)$ , then one can expand $f$ into a Fourier series (sometimes called the $q$ -expansion in this context). I'm interested in functions that have a ""pseudo-periodicity"" property, say, $f(z+3)=f(z)+1$ . Is there anything that can be said about the form $f$ must take in this case? More generally, I am interested in analytic maps $f: \mathbb{H}\to\mathbb{H}$ which satisfy a ""pseudo-equivariance"" (is there a better name for this?) condition: given a finite index subgroup $H<{\rm PSL}(2,\mathbb{Z})$ , there is some virtual endomorphism* $\phi: H\to G$ such that $$f(g.z) = \phi(g).f(z)$$ for every $g\in H$ . Basically this will correspond to a set of functional equations that $f$ must satisfy. My general hope is that if one is given sufficiently many of these functional equations, then the map $f$ will be more or less determined. My reasoning is that if one can determine the fundamental domain of $f$ under the action of $H$ , and also determine the image of this domain, then it should be possible to determine the map everywhere else as well. The theory of modular forms seemed like a good starting place for results along these lines, but the transformation condition for modular forms is quite different and I don't think there's a neat trick to transfer my problem into the modular form setting. Does anyone know any theories/papers which might be relevant here? I feel like problems of this sort have probably been studied before, but my google skills are failing me. *A virtual endomorphism of a group $G$ is a homomorphism from a subgroup of finite index $H \leq G$ into $G$ .","['functional-equations', 'reference-request', 'complex-analysis', 'modular-forms', 'soft-question']"
4452873,Periodic sequences of integers generated by $a_{n+1}=\operatorname{rad}(a_{n})+\operatorname{rad}(a_{n-1})$,"Let's define the radical of the positive integer $n$ as $$\operatorname{rad}(n)=\prod_{\substack{p\mid n\\ p\text{ prime}}}p$$ and consider the following Fibonacci-like sequence $$a_{n+1}=\operatorname{rad}(a_{n})+\operatorname{rad}(a_{n-1})$$ If $a_1=1,\,a_2=1$ the sequence coincides with OEIS A121369 $$1, 1, 2, 3, 5, 8, 7, 9, 10, 13, 23, 36, 29, 35, 64, 37, 39, 76, 77, ...$$ If $a_1=2,\,a_2=2$ the sequence becomes $$2, 2, 4, 4, 4, ...$$ If $a_1=3,\,a_2=3$ the sequence becomes $$3, 3, 6, 9, 9, 6, 9, ...$$ If $a_1=5,\,a_2=5$ the sequence becomes $$5, 5, 10, 15, 25, 20, 15, 25, ...$$ If $a_1=7,\,a_2=7$ the sequence becomes $$7, 7, 14, 21, 35, 56, 49, 21, 28, 35, 49, 42, 49, 49, 14, 21, ...$$ The above sequences, except for the first, are all periodic. Continuing with the successive prime numbers , we obtain: for $\,p=11,\,$ a sequence with a period length of $\,9$ , for $\,p=13,\,$ a sequence with a period length of $\,81$ , but for $\,p=17\,$ and $\,p=19\,$ two apparently divergent sequences. Other primes that generate periodic sequences are (the respective period lengths in brackets): $$23 (9), 29 (12), 31 (207), 37 (27), 41 (36), 47 (39), 73 (198), 79 (60)$$ Some questions arise from the previous experimental observations: is the period length always a multiple of $3$ (not considering the case $p=2$ )? also in the doubtful cases mentioned above, does the sequence become periodic at some point? given the starting prime number, is it possible to predict the length of the period of the generated sequence or, at least, to identify some pattern? I have posted a more general question of the same nature here . Edit For the calculation of $\,\operatorname{rad}(n)\,$ I used the sympy.primefactors() method inside Python: from sympy import primefactors

def rad(num):
    primes = primefactors(num)
    value = 1
    for p in primes:
        value *= p
    return value

(a0, a1) = (17, 17)
for n in range(2, 10001):
    a2 = rad(a1) + rad(a0)
    print(n, a2)
    a0 = a1
    a1 = a2","['prime-factorization', 'number-theory', 'experimental-mathematics', 'sequences-and-series', 'prime-numbers']"
4452883,Tail bounds for sub-Gaussian and sub-exponential distributions,"Massart and Laurent (see [ 1 ], Lemma 1 on page 1325] give tail bounds for $\chi^2$ random variables. A corollary of their bound is the following: $$P\left[\frac{1}{k} X \leq 1- 2\sqrt{\frac{x}{k}} \right]\leq \exp (-x) $$ $$P\left[\frac{1}{k} X \geq 1+2\sqrt{\frac{x}{k}}+2 \frac{x}{k} \right]\leq \exp (-x) $$ I am looking for similar statements for sub-Gaussian and sub-exponential random variables, but could only find the following statement for independent $\sigma^2$ -sub-Gaussian RV: $$P\left[ \frac{1}{k} \sum_{i=1}^{n} X_i \geq t \right]\leq \exp \left(-\frac{n t^2}{\frac{2}{n}\sum_{i=1}^{n} \sigma_i^2}\right) $$ which is similar to the second concentration inequality (""upper tail"") by Massart and Laurent. I couldn't find any equivalent for the first one (""lower tail""). Furthermore, I couldn't find any inequalities for sub-exponential RV without any additional prerequisites (e.g., maximum known). Is there any similar statements to the first (""upper"") bound by Massart and Laurent for sub-Gaussian distributions and similar statements for both bounds for sub-exponential distributions? Thank you for your help!","['statistics', 'concentration-of-measure', 'probability-limit-theorems', 'probability-theory', 'probability']"
4452885,Can we always pick an integer power of $e^{i \theta}$ such that its difference with $1$ has rational argument?,"Let $z = e^{i\theta}, \theta \in \mathbb{R}$ . Then, does there exist $n \in \mathbb{N}$ such that: $$1 - z^n  = re^{2 \pi i \tau}$$ for some $\tau \in \mathbb{Q}$ ? Naturally, this exists if $\theta$ is a rational multiple of $\pi$ . However, does this hold for any $\theta$ ? Although this question appears quite simple, I have no idea how I would approach it, and I suspect that its proof or disproof would be very difficult. Link to motivation (it may appear entirely unrelated (it almost is); I wish to show that $\mathbb{C}$ has a certain property that I defined on fields with the addition of some analysis.)","['complex-numbers', 'analysis', 'rational-numbers']"
4452889,Rank 1 operator on an infinite dimensional vector space number of eigenvalues.,"Does a rank 1 bounded operator from $\mathscr{K}:L^2([0,1])\to L^2([0,1])$ have at most 1 non-zero eigenvalue? The reason this is not obvious to me is that $L^2([0,1])$ is infinite dimensional. In general, is rank a bound on the number of eigenvalues?","['linear-algebra', 'functional-analysis', 'analysis', 'real-analysis']"
4452892,Sequence of random variables converging to zero arbitrarily slowly,"The following question occurred to me: Suppose $X_n$ is a sequence of positive random variables satisfying for all $\delta>0$ , $P(X_n < \delta) \to 1$ . Is it true that there must exist a sequence $a_n \to 0$ so that $X_n = O_P(a_n)$ ? My inclination is that this is false, but I cannot come up with a counter example. Such a counter example sequence of random variables would effectively need to decrease to zero ""arbitrarily slowly"". Can any of you smart folks come up with a proof or counter example?","['probability-theory', 'probability', 'real-analysis']"
4452923,Convergence of measures on a compact metric space,"In the paper ""Ergodic optimization"" by Oliver Jenkinson, Proposition 2.1 says (among other things): Let $T:X\to X$ be a continuous map on a compact metric space. If $f:X\to\mathbb{R}$ is either continuous, or the characteristic function of a closed subset, then $$\sup_{\mu\in\mathcal{M}_{T}}\int fd\mu =
 \limsup_{n\to\infty}\frac{1}{n}\sup_{x\in X}S_{n}f(x).$$ Here $\mathcal{M}_{T}$ is the collection of Borel probability measures invariant under $T$ in $X$ , and $S_{n}f := \sum_{i=0}^{n-1} f\circ T^{i}$ . I want to prove that $$\sup_{\mu\in\mathcal{M}_{T}}\int fd\mu  \geq \limsup_{n\to\infty}\frac{1}{n}\sup_{x\in X}S_{n}f(x)$$ to understand the proof. My doubt is: Why do you have an accumulation point $\mu$ with respect to the weak- $\ast$ topology? Is it some usual property of compact metric spaces? If you could give me some reference to read about it I would appreciate it very much? The proof in the paper: Compactness of $X$ means that the set $\mathcal{M}$ of Borel probability measures on $X$ is compact with respect to the weak- $\ast$ topology. If $$\mu_{n} := \frac{1}{n}\sum_{i=0}^{n-1}\delta_{T^{i}x_{n}},$$ where $x_{n}$ is such that $$\max_{x\in X}\frac{1}{n}S_{n}f(x) = \frac{1}{n}S_{n}f(x_{n}) = \int fd\mu_{n},$$ then the sequence $(\mu_{n})$ has a weak- $\ast$ accumulation point $\mu$ . It is easy to see that in fact $\mu\in\mathcal{M}_{T}$ . Without loss of generality we shall suppose that $\mu_{n}\to\mu$ in the weak- $\ast$ topology. If $f$ is continuous, this means that $$\lim_{n\to\infty}\int fd\mu_{n} = \int fd\mu \leq \sup_{\mu\in\mathcal{M}}\int fd\mu.$$","['measure-theory', 'ergodic-theory', 'metric-spaces', 'probability']"
4452940,"Easier way to solve equation systems of $a+b+c+\cdots{}= 1$, $a^2 + b^2 + c^2+\cdots{}=2$ and so on without having to crunch massive expressions","I study at below college level. I have been trying to solve certain systems of equations involving $n$ equations of $n$ unknowns. For example, for $2$ unknowns, the problem is \begin{align} 
a^{\phantom{1}} + b^{\phantom{1}} &= 1 \\ 
a^2 + b^2 &= 2 \\ 
a^3 + b^3 &={} ?
\end{align} This can be solved with elementary algebra and/or WolframAlpha. You can generalize this to more unknowns: \begin{align} 
a^{\phantom{1}} + b^{\phantom{1}} + c^{\phantom{1}} &= 1 \\ 
a^2 + b^2 + c^2 &= 2 \\ 
a^3 + b^3 + c^3 &= 3 \\ 
a^4 + b^4 + c^4 &={} ? 
\end{align} with the same restraint: $n$ unknowns, $n$ equations, in each equation the powers of each variable is the same, and the pattern is clear. Now, I, off of only the first $3$ cases (including the trivial case $a = 1$ , find $a^2$ ) made a conjecture about the result (the missing value of the final expression). Since this is such a random guess at the value, and so many functions could meet just the first few data points, I want to solve the version with $4$ unknowns, just to see whether the conjecture's true. However, this is very difficult. The expansions quickly get out of hand and not even WolframAlpha can do it. I want a way to at least get the solving process under control. Usually, one'd generate equations and use those to solve for things like $a \cdot b^3$ , but here the issue is that just setting up the equations is a task too difficult. Is there a way to elegantly solve the system? I don't mind trading in time for maybe some more difficult math.","['algebra-precalculus', 'systems-of-equations', 'symmetric-polynomials', 'polynomials']"
4452957,Bounding diameter of the arc of a closed curve,"I was reading chapter 4 of Colding and Minicozzi's A Course in Minimal surfaces and I came across a statement in the proof of Lemma 4.14: Suppose $\Gamma\subset\mathbb{R}^3$ is a simple closed curve of finite arc length. Then for any sufficiently small $\epsilon>0$ there exists $d>0$ such that if $p,q\in\Gamma$ with $0<|p-q|<d$ , then $\Gamma\setminus\{p,q\}$ has exactly one component with diameter $<\epsilon$ . I think I know how to prove something like this assuming that $\Gamma$ is $C^2$ (or whatever condition that leads to the existence tubular neighbourhoods), but I'm not sure how to prove this when the curve is simply finite arc-length. Edit:
Changed ''for any $\epsilon>0$ '' to ''for any sufficiently small $\epsilon>0$ ''.","['curves', 'minimal-surfaces', 'differential-geometry', 'real-analysis']"
4452971,Is a normal domain whose prime ideals are totally ordeded a valuation ring?,"Recall one of the definition of a valuation ring is a domain whose ideals are totally ordered. (Then it will be a normal domain.) But if we restrict to all prime ideals the reverse is not true. The stalk at a non-normal closed point of a one-dimensional scheme is a counter-example. For example, $k[[x^2, x^3]]$ or $\mathbb{Z}_2[\sqrt{-3}]$ . I just found that this has been already discussed in Does totally ordered prime ideals in a domain imply valuation ring? All counter-examples so far are non-normal rings, so the question in the title naturally appears.","['algebraic-geometry', 'commutative-algebra']"
4452977,"$( \ X\to G \ , \ \star \ )$ is a group if $(\varphi \star \psi)(a) = \varphi(a) \star \psi(a)$","Let $X$ be a set and $G$ a group with the operation $\star$ . Show that the set $$
\mathcal{X} = \Big\{ \varphi : X\to G \mid \text{$\varphi$ is a function} \Big\}
$$ is a group with the operation \begin{equation}\label{star}
\big(\varphi \star \psi\big)(a) \; = \; \varphi(a) \star \psi(a)  \qquad \quad \forall\,a\in G.
\end{equation} So associative is pretty easy since $(G,\star)$ is a group:
Let $\varphi,\tau,\phi\in\mathcal{X}$ . Thus, \begin{align*}
        ((\varphi\star\tau)\star\phi)(g) &= (\varphi\star\tau)(g)\star\phi(g)\\
        &= (\varphi(g)\star\tau(g))\star\phi(g)\\
        &=\varphi(g)\star (\tau(g)\star\phi(g))&G \text{ group}\\
        &= \varphi(g)\star(\tau\star\phi)(g)\\
        &= (\varphi\star(\tau\star\phi))(g)
    \end{align*} And for identity, let $id:G\to G$ with the map $g\mapsto e$ , with $e$ the identity on G, is a function and acts as an identity for $\mathcal{X}$ : $$(\varphi\star id)(g) = \varphi(g)\star id(g) = \varphi(g)\star e = \varphi(g) = e\star \varphi(g) = id(g)\star\varphi(g) = (id\star\varphi)(g).$$ But I'm having trouble proving closure and inverses. Since we don't know if an element is bijective or not, then we can't construct an inverse. And for closure, how can I show that $\varphi(a) \star \psi(a)$ is still a function?","['functions', 'group-theory', 'abstract-algebra']"
4453011,Fisher-Neyman Factorisation Theorem and sufficient statistic misunderstanding,"Fisher Neyman Factorisation Theorem states that for a statistical model for $X$ with PDF / PMF $f_{\theta}$ , then $T(X)$ is a sufficient statistic for $\theta$ if and only if there exists nonnegative functions $g_{\theta}$ and $h(x)$ such that for all $x,\theta$ we have that $f_{\theta}(x)=g_{\theta}(T(x))(h(x))$ . Computationally, this makes sense to me. However, recently I have started to have some doubts about when and where I can apply this theorem. For example, if I have the PDF for a uniform distribution $f_{\theta}(x)=\frac{1}{\theta}$ , doesn't this allow me to make any sufficient statistic that I like by rewriting the PDF as $$f_{\theta}(x)=\Big(\frac{X^2+X^4}{\theta}\Big)\Big(\frac{1}{X^2+X^4}\Big)$$ which would make our sufficient statistic $T(X)=X^2+X^4$ . Or if we replace this particular choice of $T(X)$ for something else, doesn't this allow us to construct almost any choice of $T(X)$ as a valid sufficient statistic? Is this correct, or am I doing something wrong here by arbitrarily adding in functions of $X$ that cancel out in order to create sufficient statistics?","['statistical-inference', 'statistics', 'uniform-distribution', 'sufficient-statistics', 'probability']"
4453028,Why an LTI system with some zero eigenvalues still stable?,"The textbook says an LTI system $\dot x=Ax$ is stable if and only if the eigenvalues of $A$ have the strictly negative real part. However, I found a counterexample. If $$A= \begin{bmatrix}-3 &  -1 &  -1\\
     1 &  -0.5 &  -0.5\\
     1 &  -0.5  & -0.5\end{bmatrix}$$ The state response of this system is convergent, and $x_2 = x_3$ . The system is stable even if an eigenvalue of $A$ is $0$ . Am I wrong?","['stability-theory', 'linear-algebra', 'control-theory', 'dynamical-systems']"
4453048,Degeneration of a spectral sequence,"I am reading the book â€œFourier-Mukai transforms in algebraic geometryâ€ by Daniel Huybrechts. On page 140, he is written that due to some results the spectral sequence $$E^{p,q}_2=H^p(X\times X,\mathcal{E}xt^q(\iota_*\mathcal{O}_X,\iota_*\mathcal{O}_X))\Longrightarrow Ext^{p+q}(\iota_*\mathcal{O}_X,\iota_*\mathcal{O}_X)$$ degenerates and this implies that $$Ext^i(\iota_*\mathcal{O}_X,\iota_*\mathcal{O}_X)\cong \bigoplus_{p+q=i}H^i(X\times X, \mathcal{E}xt^q(\iota_*\mathcal{O}_X,\iota_*\mathcal{O}_X))$$ Now, my question is that what is the meaning of the degeneration of a spectral sequence and why does it imply this isomorphism here?","['homological-algebra', 'abelian-categories', 'algebraic-geometry', 'spectral-sequences']"
4453070,"""Multiply everything so far, plug into polynomial"" - can these always yield primes?","EDIT: I forgot how open number theory is! (I think that gets me put on mathematician probation or something.) For this question, I will accept any answer which assumes ""standard conjectures"" such as RH, Schanuel, Bunyakovsky, etc. I'm definitely more optimistic about conditional results. Say that a factonomial sequence is a (possibly infinite) sequence of natural numbers $x_i$ such that each $x_i$ is prime, and there is some (single variable, integer coefficients, nonconstant) polynomial $p$ such that for all $i>1$ we have $$x_i=p(\prod_{j<i}x_j).$$ Call the polynomial $p$ the shape of the factonomial sequence; each factonomial sequence is determined by its shape and its initial value. The basic idea is that factonomial sequences come out of some elementary proofs that infinitely many primes of a certain form exist. For instance, the usual ""multiply everything and add one"" argument gives rise to the shape $p_1(u)=u+1$ , and the usual ""multiply everything twice and add two"" proof that there are infinitely many primes $\equiv 3 (\mathsf{mod}$ $4)$ gives rise to the shape $p_2(u)=u^2+2$ . The maximal factonomial sequence with shape $p_2$ and starting value $3$ is $$3,11,1091, 1296216011, 2177870960662059587828905091.$$ The next term would be $$10329907495268194677701503661780370732730049826819138974714891651071966324541232011,$$ but that's not prime. (Amusingly, its smallest prime factor is $41$ , but its smallest prime factor which is $3$ mod $4$ is a bit bigger: $76870667$ .) Question : is there an infinite factonomial sequence? (This is a question which I'm 99% sure one of my students will ask me in the next couple days!) I'm aware of multiple results of the form ""no function of such-and-such type has output consisting entirely of primes,"" but I don't immediately see one which applies here. Unfortunately, terms in factonomial sequences grow so fast that I can't do much experimenting.","['number-theory', 'prime-numbers']"
4453111,How to evaluate $\int^{\infty}_0 \frac{x^{1010}}{(1 + x)^{2022}} dx$?,How to evaluate the following integral? $$\int^{\infty}_0 \frac{x^{1010}}{(1 + x)^{2022}} dx$$ Here's my work: $$\begin{align}I &= \int_0^\infty \dfrac{x^{1010}}{(1+x)^{2022}} dx  \\&=\int_0^\infty \dfrac{1}{x^{1012}(1 + \frac1x)^{2022}}dx\end{align}$$ Putting $( 1 + \frac1x) = t$ $$\begin{align}\implies I& =\int^1_\infty -\dfrac{1}{(\frac1{1-t})^{1010}(t)^{2022}}dx\\& =\int_1^\infty \dfrac{1}{(\frac1{1-t})^{1010}(t)^{2022}}dx \\&=\int_1^\infty \dfrac{1}{(\frac1{1-t})^{1010}\cdot t^{1010} \cdot (t)^{1012}}dx \\&=\int_1^\infty \dfrac{1}{(\frac t{1-t})^{1010} \cdot (t)^{1012}}dx\\& =\int_1^\infty \dfrac{1}{(\frac 1{1/t-1})^{1010} \cdot (t)^{1012}}dx \\&=\int_1^\infty \dfrac{(1/t-1)^{1010}}{ (t)^{1012}}dx \\&=\int_1^\infty \dfrac{(\frac{1-t}{t})^{1010}}{ t^2\cdot (t)^{1010}}dx \\& = \int_1^\infty\dfrac{1}{t^2} \cdot\left( \dfrac{1-t}{t^2}\right)^{1010} dx \end{align} $$ I don't know how to continue from here. I also thought that Integration by parts would work but not sure how to apply here.,"['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4453156,Why do we use exponentials while integrating trigonometric functions in complex analysis,"Let p(x) be some polynomial function. Now, we have an integral of the form : $$I=\int_{-\infty}^{\infty} \frac{\cos(x)}{p(x)}dx$$ What is usually done is that, we define this integral as : $$I'=\int_{-\infty}^{\infty} \frac{e^{iz}}{p(z)}dz$$ Then we use the residue theorem, to integrate over the upper half of the complex plane. This gets separated into two integrals, over the curve and over the real axis. We can show that as $R\rightarrow\infty$ , the integral over the curve vanishes, and we are left with just the integral over the real axis. Then we use the fact that $\cos(x)$ is just the real part of $e^{ix}$ . So, comparing the real parts of the solution, we find the answer to our initial problem. My question is, why do we need to take $e^{iz}$ in the first place ? Is there some reason why we can't work with $\cos(x)$ directly? Moreover, depending on how we proceed, we can get different values of the residue. Consider the function $$f(z)=\frac{\cos(z)}{z^2}$$ Expanding using a taylor series, we can see, the $z^-1$ term doesn't exist, and so the residue is $0$ . Similarly, if I use the exponential, then I can use the following formula : $$Re(z=0)=\frac{1}{1!}\frac{d}{dz}(z-0)^2\frac{e^{iz}}{z^2}|_{z=0} = i$$ Hence, depending on what function I use, I'd get different values of the residue. The same thing happens for the function : $$f(z)=\frac{\cos(z)}{z^2}$$ I get the residue $\frac{\cosh(1)}{2i}$ or $\frac{1}{2ei}$ depending on whether I take the exponential or keep my cosine function. So, why do we replace trigonometric functions with complex exponentials in these problems ? I think this has something to do with the integral over the curve as $R\rightarrow\infty$ . Any help in understanding this would be highly appreciated.","['integration', 'complex-analysis', 'cauchy-principal-value', 'residue-calculus', 'complex-integration']"
4453194,Differentiability of Wiener integral with respect to a parameter,"Let $f:[0,T]\times\Theta\to\mathbb R$ and let $\{B_t\}_{t\in [0,T]}$ be a Brownian motion. Consider the Wiener integral $$\int_0^T f(t,\theta)dB_t.$$ I am looking for conditions that ensure that $$\frac{d}{d\theta}\int_0^T f(t,\theta)dB_t=\int_0^T \partial_{\theta}f(t,\theta)dB_t$$ In this and this article conditions are set in the case in which $f$ is not deterministic. Thanks in advance.","['stochastic-integrals', 'derivatives', 'stochastic-calculus']"
4453199,Doubts on asymptotic criterion for $\sum_{n=1}^{\infty}a_n=\sum_{n=1}^{\infty}n^{a}\tan^{-1}\bigg(\frac{1}{n^a}\bigg)-e^{1/n}$ with $a>0$.,"I have to valuate the character of the following series: $$\sum_{n=1}^{\infty}a_n=\sum_{n=1}^{\infty}n^{a}\tan^{-1}\bigg(\frac{1}{n^a}\bigg)-e^{1/n}$$ with $a>0$ . I have thought that definitely the sequence $a_n$ is made by constant signed terms. So I can apply the asymptotic criterion to study the series. $=n^{a}\tan^{-1}\big(\frac{1}{n^a}\big)-e^{1/n}\\
=n^{a}\big(\frac{1}{n^a}-\frac{1}{3n^{3a}}+o(\frac{1}{n^{3a+1}})\big)-1-\frac{1}{n}+o(\frac{1}{n})\\
=1-\frac{1}{3n^{2a}}+o(\frac{1}{n^{2a+1}})-1-\frac{1}{n}+o(\frac{1}{n})\\
=\frac{1}{3n^{2a}}+o(\frac{1}{n^{2a+1}})-\frac{1}{n}+o(\frac{1}{n})\\
\color{red}{=\frac{1}{3n^{2a}}-\frac{1}{n}+o(\frac{1}{n})}$ The red passage is right? I have thought that since $2a+1>1$ then I can put the $o(\frac{1}{n^{2a+1}})$ into $o(\frac{1}{n})$ . Now: if $a\leq 1/2$ then I have $\frac{1}{3n^{2a}}-\frac{1}{n}+o(\frac{1}{n})=\frac{1}{3n^{2a}}+o(\frac{1}{n})$ , so the corresponding series diverges and also the original one. if $a>1/2$ then $\frac{1}{3n^{2a}}-\frac{1}{n}+o(\frac{1}{n})=-\frac{1}{n}+o(\frac{1}{n})$ and again the series diverges. My overall attempt is right? Edit : I understand there are some problems in the final part with the little $o$ . Can someone help me","['limits', 'sequences-and-series', 'asymptotics', 'real-analysis']"
4453238,Stopped version of UI martingale is UI,"Suppose $(X_n)_{n\geq0}$ is a uniformly integrable martingale. Let $T$ be any stopping time. Show that the stopped process $(X_n^T)_{n\geq0}=(X_{n\wedge T})_{n\geq0}$ is uniformly integrable. My only idea was to work on the two events $\{T<\infty\}$ and $\{T=\infty
\}$ . The result is obvious on the second event by assumption, but I'm not sure how to formally prove that the limit must go to zero (as in the definition of uniform integrability). Any advice would be greatly appreciated!","['measure-theory', 'uniform-integrability', 'martingales', 'stopping-times', 'probability-theory']"
4453341,how to show an inequality from P.D.E,"Let $\Omega$ a regular bounded open subset of $\mathbb{R}^N$ .  Let $T>0$ , $u_0 \in L^2$ , $b \in L^{\infty}(]0,T[\, \times\, \Omega)^N$ , and $c \in L^{\infty}(]0,T[\, \times\, \Omega)$ .
We consider the following equation on $]0,T[\, \times\, \Omega$ . $\partial_{t} u+b \cdot \nabla u+c u-\Delta u=0$ $u|_{\partial \Omega} =0$ and $u|_{t=0}=u_0$ . Question : Show that if $u \in C^1([0,T],C^2(\Omega)) $ is a   solution then $\frac{d}{d t}\|u(t)\|_{L^{2}(\Omega)}^{2}+\int_{\Omega}|\nabla u(t, x)|^{2} d x \leq\left(2\|c\|_{\infty}+\|b\|^2_{\infty}\right)\|u(t)\|_{L^{2}(\Omega)}^{2}$ . My attempt :  I can show that $$\frac 12 \frac{d}{d t}\|u(t)\|_{L^{2}(\Omega)}^{2}+\int_{\Omega}|\nabla u(t, x)|^{2} d x \leq\|c\|_{\infty}\|u(t)\|_{L^{2}(\Omega)}^{2}+|\int_{\Omega} (b \cdot \nabla u)u |$$ How to continue with this term $|\int_{\Omega} (b \cdot \nabla u)u |$ ?","['functional-analysis', 'partial-differential-equations']"
4453378,Understanding Rokhlin's theorem of cross-sections,"I am reading the book Conformal Fractals: Ergodic Theory Methods by Przytycki and UrbaÅ„ski  (the book is available legally on a site of the first author https://www.impan.pl/~feliksp/ksiazka1.pdf ) and I have trouble understanding Theorem 1.9.1. The statement is as follows: Suppose that $\mathcal{A}$ and $\mathcal{B}$ are two measurable partitions of a Lebesgue space $(X, \mathcal{F}, \mu)$ such that $\mathcal{A} \cap B$ is countable ( $\operatorname{mod}0$ with respect to $\mu_B$ ) for almost every $B \in \mathcal{B}$ . Then there exists a countable partition $\Gamma = \{\gamma_1, \gamma_2, ...\}$ of $X$ $(\operatorname{mod}0)$ such that each $\gamma_j \in \Gamma$ intersects almost every $B$ at not more than one point, which is then an atom of $\mu_B$ , in particular $$\mathcal{A} \vee \mathcal{B} = \Gamma \vee \mathcal{B} \,\,(\operatorname{mod}0).$$ My problem is I don't see how this theorem can be true, which leads me to believe I'm misunderstanding some other concepts. Can someone point out to me what is wrong with my ""counterexample"": Let $X$ be the unit interval. Take $\mathcal{A}$ to be some partition of $X$ into two sets and $\mathcal{B}$ to be the partition into $1$ element, the whole set $X$ . These are measurable partitions, of course $\mathcal{A} \cap B$ is countable (even finite), but how could there possibly be a partition $\Gamma$ from the statement? It would yield a partition of the unit interval into a countable set of points. Obviously I'm doing something wrong, help would be greatly appreciated, thanks.","['measure-theory', 'ergodic-theory', 'entropy', 'probability']"
4453422,Derivative of the inverse of a symmetric matrix w.r.t itself,"I'm trying take the derivative of a symmetrix matrix $\mathbf{C}$ with respect to itself. $$
\begin{equation}
\frac{\partial \mathbf{C}^{-1}}{\partial \mathbf{C}}
\end{equation}
$$ Using the indicial notation, above equation can be rewritten as follows $$
\begin{equation}
\frac{\partial C_{ij}^{-1}}{\partial C_{kl}}
\end{equation}
$$ At first I've used the following formula, $$
\begin{equation}
\frac{\partial C_{ij}^{-1}}{\partial C_{kl}} = -C^{-1}_{ik}C^{-1}_{lj}
\end{equation}
$$ But I quickly realized that we've lost the symmetry of the problem now. I read The Matrix Cookbook and the other posts about the same problem but unfortunately, I couldn't understand the things they've done. For example in this article , at Eq.(100) authors have used the property below when taking the derivative of Eq.(99) $$
\begin{equation}
\frac{\partial \mathbf{C}^{-1}}{\partial \mathbf{C}} = -\mathbf{C}^{-1} \boxtimes \mathbf{C}^{-T} \mathbf{I}_s
\end{equation}
$$ Where $\boxtimes$ is the square product, $\mathbf{I}_s$ is the symmetric fourth-order identity tensor and they are defined as follows $$
\begin{align}
(\mathbf{A} \boxtimes \mathbf{B})_{ijkl} &= \mathbf{A}_{ik}\mathbf{B}_{jl} \\
(\mathbf{I}_s)_{ijkl} &= \frac{1}{2}(\delta_{ik}\delta_{jl}+\delta_{il}\delta_{jk})
\end{align}
$$ I couldn't understand how did they achieve this result and how can I derive it myself.","['matrices', 'matrix-calculus', 'derivatives', 'tensors']"
4453428,"My teacher said that this is not necessary in the line integral, but why?","Question:
Calculate the Scalar line integral: $$\int_C \left(ð‘¥\,ð‘‘ð‘¥ âˆ’ ð‘¦\,ð‘‘ð‘¦\right)$$ C is the segment traveled in the direction: $$(1,1)\,to\,(2,3)$$ I started by solving this question by parameterizing the points. Parameterization formula: $$(x,y)=B\,t+(1-t)\,A$$ $$(x,y)=(2,3)\,t+(1-t)\,(1,1)$$ $$(x,y)=(2t,3t)+(1-t,1-t)$$ $$(x,y)=(1+t,1+2t)$$ Our parameterization determines that $$ 0 â‰¤ tâ‰¤1$$ So our function $$r(t)=(1+t)\hat{i}+(1+2t)\hat{j}$$ $$r'(t)=(1,2)$$ $$ |r'(t) |=\sqrt{1^{2}+2^{2}}=\sqrt{5}$$ So our scalar line integral will be $$\int_C ð‘¥\,ð‘‘ð‘¥ âˆ’ ð‘¦\,ð‘‘ð‘¦ =\int_0^1 [(1+t)\,1 - (1+2t)\,2 ]\,|r'(t) | dt$$ $$=\int_0^1 [1+t - 2-4t ]\,\sqrt{5}dt $$ $$=\sqrt{5} \int_0^1 -3t - 1 dt$$ $$\therefore\sqrt{5}\,(\frac{-5}{2})$$ The thing is, my professor said that it is not necessary to have put the $$|r'(t) |$$ in the scalar integral formula, because that was another case, but he didn't explain... was he right?","['integration', 'multivariable-calculus', 'calculus', 'line-integrals']"
4453432,"in the unit circle, names of the various points depending on the angle at the origin","I found this image on German wikipedia, naming points related to the unit circle with letters. Most makes sense, like S for Sine , C for Cosine , O for Origin , P for Point , T for Tangent , and K for Cotangent (in German it is Kotangens , and C is taken by Cosine anyways). Now what's still open is what D and E and b stand for. Does someone have an idea? Thanks a lot! Background: I find it useful to have consistent naming for such things. It allows for briefer and more comprehensible descriptions. I find it of particular use when I program shaders.","['trigonometry', 'mathematical-german', 'terminology']"
4453446,Examples of dynamical systems that have structural stability,"I am looking for simple examples of structural stability, I read the definition of structural stability but couldn't figure out a concrete example of a system, its perturbated version and its conjugacy. The definition that I am using is from Wikipedia. ""Structural stability means that the qualitative behavior of the trajectories is unaffected by small perturbations ( $C^1$ -small)."" Moreover, what does it mean a $C^1$ -small perturbation?","['ordinary-differential-equations', 'stability-in-odes', 'stability-theory', 'perturbation-theory', 'dynamical-systems']"
4453465,Prove $\sum \frac{n^{p+1}}{a_1+2^pa_2+\cdots+n^pa_n}$ is convergent.,"Assume $\sum\limits_{n=1}^{\infty} \dfrac{1}{a_n}$ is a convergent positive term series and $p>0$ . Prove $$ \sum_{n=1}^{\infty} \frac{n^{p+1}}{a_1+2^pa_2+\cdots+n^pa_n}$$ is
convergent. Since $$a_1+2^pa_2+\cdots+k^pa_k\ge \sqrt[k]{a_1\cdot2^pa_2\cdots k^pa_k}=\sqrt[k]{a_1a_2\cdots a_k}\cdot \sqrt[k]{(k!)^p},$$ then \begin{align*} \frac{k^{p+1}}{a_1+2^pa_2+\cdots+k^pa_k}&\le \frac{k^{p+1}}{\sqrt[k]{a_1a_2\cdots a_k}\cdot \sqrt[k]{(k!)^p}}\sim \frac{k^{p+1}}{\sqrt[k]{a_1a_2\cdots a_k}\cdot \frac{k^p}{e^p}}= e^p\cdot \frac{k}{\sqrt[k]{a_1a_2\cdots a_k}}. \end{align*} This perhaps can not work.","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
4453501,How to evaluate the sum of $\sum_{n=0}^{\infty}\frac{1}{3n^{2}+4n+1}$,I hava an infinite sum $$\sum_{n=0}^{\infty}\frac{1}{3n^{2}+4n+1}$$ I factored the denominator $$\sum_{n=0}^{\infty}\frac{1}{\left(3n+1\right)\left(n+1\right)}$$ Then I separated the fraction $$\frac{1}{2}\sum_{n=0}^{\infty}\frac{3}{\left(3n+1\right)}-\frac{1}{\left(n+1\right)}$$ Then I set 1 (the numerator) to be equal to x to some power which I don't know if I can do $$\frac{3}{2}\sum_{n=0}^{\infty}\frac{x^{3n+1}}{3n+1}-\frac{1}{2}\sum_{n=0}^{\infty}\frac{x^{n+1}}{n+1}$$ Then I set the integral which would satisfy the previous terms $$\frac{3}{2}\sum_{n=0}^{\infty}\int_{0}^{1}x^{3n}dx$$ and $$-\frac{1}{2}\sum_{n=0}^{\infty}\int_{0}^{1}x^{n}dx$$ Then I changed the order of summation and integration and I got $$\frac{3}{2}\int_{0}^{1}\frac{1}{1-x^{3}}dx$$ and $$-\frac{1}{2}\int_{0}^{1}\frac{1}{1-x}dx$$ The first integral can be factored to $$\frac{3}{2}\int_{0}^{1}\frac{1}{\left(1-x\right)\left(1+x+x^{2}\right)}dx$$ Then separated $$\frac{1}{2}\int_{0}^{1}\frac{1}{1-x}+\frac{x+2}{1+x+x^{2}}dx$$ The first one will cancel out with $$-\frac{1}{2}\int_{0}^{1}\frac{1}{1-x}dx$$ And I'm left with $$\frac{1}{2}\int_{0}^{1}\frac{x+2}{1+x+x^{2}}dx$$ which is $$\frac{\sqrt{3}\pi}{12}+\frac{\ln\left(3\right)}{4}$$ But the correct answer is $$\frac{\sqrt{3}\pi}{12}+\frac{3\ln\left(3\right)}{4}$$ So I would like to ask if this approach is invalid or if I'm just missing something.,"['summation', 'taylor-expansion', 'sequences-and-series']"
4453507,Showing integrability of random variable at stopping time,"Let $X_1,X_2,\dots$ be iid integrable random variables in $\mathbb{R}$ with $\mathbb{E}(X_i)=0$ . Let $\eta$ be a stopping time with $\mathbb{E}(\eta)<\infty$ . Show that $X_\eta$ is integrable. I'm not sure where to start with this one. Of course, integrability of $\eta$ implies that $\mathbb{P}(\eta<\infty)$ almost surely, and so $X_\eta$ is certainly well-defined. Unfortunately, my only ideas involve thinking about the variable $X_{n\wedge\eta}$ and breaking this down using indicator functions to show that it is integrable, but I can't find a way to show that the fact that this is integrable for any $n$ implies that $X_\eta$ is integrable. I imagine my approach is not the correct one. Any advice would be greatly appreciated!","['stopping-times', 'probability-theory']"
4453530,"How to see that ""two manifolds are diffeomorphic when you can give them each a coordinate atlas with the same transition maps""","This question is about the diffeomorphism of $\mathbb{C}P^1$ and $S^2$ . At the end of youler's answer, we read ""the general fact that two manifolds are diffeomorphic when you can give them each a coordinate atlas with the same transition maps."" I am not sure why this is true. I have not even a geometric intuition. In my lecture notes, we were given the same example, up to showing that the two manifolds have the same transition maps. I think we are expected to understand that the transition maps imply that the two manifolds must be diffeomorphic to each other, but I don't see why. If possible, I would like an intuitive reasoning, followed by a rigorous answer.","['manifolds', 'diffeomorphism', 'smooth-manifolds', 'differential-geometry']"
4453550,How do I solve this nonlinear ODE given the asymptotic series solutions as follows?,"Differential Equation: $$-{\frac { \left( {\frac {\rm d}{{\rm d}R}}f \left( R \right)  \right) 
^{2}}{2\,f \left( R \right) }}+{\frac {{\rm d}^{2}}{{\rm d}{R}^{2}}}f
 \left( R \right) +{\frac {{\frac {\rm d}{{\rm d}R}}f \left( R
 \right) }{R}}+2\,f \left( R \right) -2\, \left( f \left( R \right) 
 \right) ^{2}-2\,{\frac {{B}^{2}}{f \left( R \right) {R}^{2}}}=0. $$ Series Solution at $R \to 0$ : $${R}^{-2}+1+ \left( {\frac {{B}^{2}}{3}}+{\frac{1}{3}} \right) {R}^{2}
+ \left( {\frac{2}{33}}+{\frac {2\,{B}^{2}}{33}} \right) {R}^{4}+
 \left( {\frac{31}{2277}}+{\frac {53\,{B}^{2}}{2277}}+{\frac {2\,{B}^{
4}}{207}} \right) {R}^{6}+ \left( {\frac{70}{29601}}+{\frac {136\,{B}^
{2}}{29601}}+{\frac {2\,{B}^{4}}{897}} \right) {R}^{8}+O \left( {R}^{
10} \right).$$ Series Solution as $R \to \infty$ : $$1-{\frac {{B}^{2}}{{R}^{2}}}+{\frac {-2\,{B}^{4}-2\,{B}^{2}}{{R}^{4}}}
+{\frac {-7\,{B}^{6}-23\,{B}^{4}-16\,{B}^{2}}{{R}^{6}}}+{\frac {-30\,{
B}^{8}-216\,{B}^{6}-474\,{B}^{4}-288\,{B}^{2}}{{R}^{8}}}+O \left( {R}^
{-10} \right) .$$ B is a free parameter. I tried solving it like a boundary value problem using series solution at very small R and large R as boundary conditions. I tried Newton iteration and imaginary time propagation for that. But that worked only for B=1. If I try solving it as an initial value problem starting from some Rmax, even RKF45 doesn't give good result. Can someone please suggest either analytical or numerical way of solving this equation? Or perhaps a way of analyzing the properties of this differential equation other than frobenius series solution? (If the questions lacks details, please let me know before downvoting) Edit: 2 downvotes without any explanation. I mean if you donâ€™t wanna respond, then donâ€™t respond. Why do you have to ruin my chances of getting any help? This is the worst forum. Most of the times people just keep downvoting without any explanation.","['frobenius-method', 'ordinary-differential-equations', 'asymptotics', 'nonlinear-system', 'numerical-methods']"
4453555,Multiplying by $1$ adds a solution to an equation,"I have a question, which is motivated by my book's solution to finding the inverse function of $f(x)=\frac{x}{1-x^2}$ with the domain of $f(x)$ restricted the open interval $(-1,1)$ . The questions are stated at the bottom, but first is the book's solution: Solution $f(x)$ is injective and surjective, and hence it has an inverse, $f^{-1}(x)$ . $y=\frac{x}{1-x^2}$ $x=\frac{y}{1-y^2}$ $xy^2+y-x=0$ $y=\frac{-1\pm\sqrt{1+4x^2}}{2x}$ The negative solution is the right one, and then the book notices that $y=\frac{-1\pm\sqrt{1+4x^2}}{2x}$ excludes $x=0$ from the domain, but it should be in the domain since $0$ is in the image of $f(x)$ , namely $f(0)=0$ . It fixes this by rationalizing the numerator by multiplying by $1$ as follows: $y=\frac{-1+\sqrt{1+4x^2}}{2x}\cdot \frac{-1-\sqrt{1+4x^2}}{-1-\sqrt{1+4x^2}} = \frac{2x}{1+\sqrt{1+4x^2}}$ Hence $f^{-1}(x)=\frac{2x}{1+\sqrt{1+4x^2}}$ . $\blacksquare$ Question Why is it that multiplying a function by $1$ will add a value for the graph of $f^{-1}(x)$ ? I can see why multiplying by $1$ will sometimes remove points on the graph, namely where the denominator is zero (e.g. multiply an equation by $\frac{x}{x}$ and it won't have a value at $x=0$ ), but how does it work here that multiplying by $1$ adds a point to the graph? Did it just get shifted somewhere else?","['functions', 'quadratics', 'inverse-function']"
4453585,Remarquable identities $f(n) = \frac{a^n}{(a-b)(a-c)} + \frac{b^n}{(b-a)(b-c)} + \frac{c^n}{(c-a)(c-b)}$,"Let $n$ be an integer, and \begin{equation}
f(n) = \frac{a^n}{(a-b)(a-c)} + \frac{b^n}{(b-a)(b-c)} + \frac{c^n}{(c-a)(c-b)}
\end{equation} \begin{equation}
g(n) = \frac{(bc)^n}{(a-b)(a-c)} + \frac{(ac)^n}{(b-a)(b-c)} + \frac{(ab)^n}{(c-a)(c-b)}
\end{equation} We have the following impressive identities, for all $a,b,c$ , \begin{align}
f(0) &= 0 \\ 
f(1) &= 0 \\
f(2) &= 1 \\
f(3) &= a+b+c \\
f(4) &= a^2 + b^2 + c^2 + ab + ac + bc \\
f(5) &= a^3 + b^3 + c^3 + a^2b + a^2c + b^2c + ab^2 + ac^2 + bc^2 \\
f(6) &= a^4 + b^4 + c^4 + a^3b + a^3c + b^3c + ab^3 + ac^3 + bc^3 + a^2bc + ab^2c + abc^2 +a^2b^2 + a^2c^2 + b^2c^2 \\
 \\
g(0) &= 0 \\ 
g(1) &= 1 \\
g(2) &= ab + ac + bc \\
g(3) &= a^2b^2 + a^2c^2 + b^2c^2 + a^2bc + ab^2c + abc^2 \\
g(4) &= a^3b^3 + a^3c^3 + b^3c^3 + a^3b^2c + a^3bc^2 + a^2b^3c + ab^3c^2 + a^2bc^3 + ab^2c^3 + a^2b^2c^2     
\end{align} which I have verified by plugging the expressions into Wolfram Alpha. It seems that the general form should be, for $n > 2$ . \begin{align}
f(n) &= \sum_{i+j+k = n-2}a^ib^jc^k \\
g(n) &= \sum_{\substack{i+j+k = 2(n-1)\\1\leq i,j,k \leq n-1}}a^ib^jc^k
\end{align} The questions are : How to demonstrate the statements for $n$ general using induction. Intuitively, we should use induction, however I do not see the induction step. Could we demonstrate the general case without using induction ? There is a link, between these formulas and Vandermondt matrices (see below), would there be a nice demonstration using matrices ? ======================================================================= I arrived at such identities when working with partial fractions decomposition,
and after some related work I realized that the Vandermondt matrices where almost the inverse of the matrices which appear when we do partial fractions decomposition,
Then I realised that the Vandermondt matrices have very nice inverse : \begin{equation}
\begin{pmatrix}
1&1 \\
a&b 
\end{pmatrix}
\begin{pmatrix}
-\frac{b}{(a - b)} & \frac{1}{(a - b)} \\
-\frac{a}{(b - a)} & \frac{1}{(b - a)} \\
\end{pmatrix}
= I_{2}
\end{equation} \begin{equation}
\begin{pmatrix}
1&1&1 \\
a&b&c \\
a^2&b^2&c^2
\end{pmatrix}
\begin{pmatrix}
\frac{b c}{(a - b) (a - c)} & -\frac{b + c}{(a - b) (a - c)} & \frac{1}{(a - b) (a - c)} \\
\frac{a c}{(b - a) (b - c)} & -\frac{a + c}{(b - a) (b - c)} & \frac{1}{(b - a) (b - c)} \\
\frac{a b}{(c - a) (c - b)} & -\frac{a + b}{(c - a) (c - b)} & \frac{1}{(c - a) (c - b)}
\end{pmatrix}
= I_{3}
\end{equation} \begin{equation}
\begin{pmatrix}
1&1&1&1 \\
a&b&c&d \\
a^2&b^2&c^2&d^2 \\
a^3&b^3&c^3&d^3
\end{pmatrix}
\begin{pmatrix}
-\frac{bcd}{(a - b) (a - c)(a-d)} & \frac{bc + cd + bd}{(a - b) (a - c)(a-d)} &-\frac{b+c+d}{(a - b) (a - c)(a-d)} &  \frac{1}{(a - b) (a - c)(a-d)}\\
-\frac{a cd}{(b - a) (b - c)(b-d)} & \frac{ac + ad + cd}{(b - a) (b - c)(b-d)} & -\frac{a + c + d}{(b - a) (b - c)(b-d)}& \frac{1}{(b - a) (b - c)(b-d)}\\
-\frac{a bd}{(c - a) (c - b)(c-d)} & \frac{ab + ad + bd}{(c - a) (c - b)(c-d)} & -\frac{a + b + d}{(c - a) (c - b)(c-d)}&\frac{1}{(c - a) (c - b)(c-d)}\\
-\frac{a bc}{(d - a) (d - b)(d-c)} & \frac{ab + ac + bc}{(d - a) (d - b)(d-c)} & -\frac{a + b + c}{(d - a) (d - b)(d-c)}&\frac{1}{(d - a) (d - b)(d-c)}
\end{pmatrix}
= I_{4}
\end{equation} The identities $f(0),f(1), f(2)$ are the last column of the inverse equation for 3-dim matrices. However, it seems that such matrix argument is not sufficient to prove the case for $n$ general, and that many similar identities (the other places in the matrices) should exist.","['matrices', 'algebra-precalculus', 'inverse']"
4453599,"Prove that $\lim_{(x,y) \to (0,0)} \frac{xy(x-y)}{x^3 + y^3}$ does not exist","I am trying to prove that the following limit does not exist. $$\lim\limits_{(x,y) \to (0,0)} \frac{xy(x-y)}{x^3 + y^3}$$ I have tried several paths such as: $\operatorname{\gamma}(t) = (t,0)$ $\operatorname{\gamma}(t) = (0,t)$ $\operatorname{\gamma}(t) = (t,t)$ $\operatorname{\gamma}(t) = (t,t^2)$ $\operatorname{\gamma}(t) = (t^2,t)$ but all these paths equal $0$ . I have started think that the limit is in fact $0$ and I expended a quite long time trying to prove it by the squeeze theorem and them I gave up and looked over WolframAlpha and learned that the limit does not exist. I do not know how to prove it. Can someone, please: Show for this particular limit a path that is different than $0$ ? Explain the thought processes I should apply to this kind of problem? How does one get the feeling that this limit does not exist after trying so many paths?","['multivariable-calculus', 'limits', 'calculus']"
4453603,"Is $(1+c^2)^n-\lfloor(1+c^2)^{n/2}\rfloor^2<(1+c^2)^{(n+1)/2}$ true for all integers $c>1$, when $n$ is an odd integer?","Let $n$ be an odd integer. Is $$(1+c^2)^n-\lfloor(1+c^2)^{n/2}\rfloor^2<(1+c^2)^{(n+1)/2}$$ true for all integers $c>1$ ? Notes: $c=1$ has a counterexample $2^{31}-\lfloor2^{31/2}\rfloor^2>2^{16}$ ; there are probably infinitely many of these. No counterexamples have been found for $(c,(n-1)/2)\in[2,20]\times[1,50]$ . The bounds $x-1\le\lfloor x\rfloor\le x+1$ are too weak to prove anything non-trivial. This is because we are looking for a Diophantine approximation of half-integer powers of $1+c^2$ to an integer, and this doesn't seem straightforward at all. This inequality comes from an attempt to bound the difference $|b-a|$ when an integer can be represented as $a^2+b^2$ in more than one way. It turns out that this is a very difficult thing to do, as we need to know and there is only a paucity of literature on the irrationality measure involving inverse trigonometric functions.","['ceiling-and-floor-functions', 'diophantine-approximation', 'number-theory', 'square-numbers', 'inequality']"
4453637,Intuitive Explanation for Number of Dyck Paths Never Going Above Diagonal of a Rectangle,"Suppose we have a an $a\times{}b$ rectangle whose bottom-left corner is at $(0,0)$ and whose upper-right corner is at $(b,a)$ . Let $a$ and $b$ both be positive integers, and let $b\geq{}a$ . If $a$ and $b$ are mutually prime then the number of Dyck paths inside the rectangle going from $(0,0)$ to $(b,a)$ which never go above the diagonal line between those two points is $\frac{1}{a+b}\binom{a+b}{a}$ . Otherwise, if $b=ak$ for some positive integer $k$ the number of Dyck paths is $\frac{1}{b+1}\binom{a+b}{a}$ . These formulas are very simple and it would seem there would be some straightforward combinatorial derivation of them, but if that is the case I haven't found one yet. I have tried applying the reflection strategy used to solve Bertrand's ballot theorem to this problem, with no success. Indeed, Bertrand's ballot theorem (with ties allowed) seems to be special case of this problem if $a=b=n$ (the probability of one candidate always being tied or ahead in the vote count is $\frac{1}{n+1}$ in that case, and the fraction of Dyck paths which never go above the diagonal from $(0,0)$ to $(n,n)$ is also $\frac{1}{n+1}\binom{2n}{n}\big/\binom{2n}{n}=\frac{1}{n+1}$ ). Does anybody know of any intuitive combinatorial, geometric, or other kinds of proofs for those two formulas?","['catalan-numbers', 'combinatorics', 'intuition']"
4453659,Speed of convergence of continued radicals with constant term,"This is an old prelim problem in Analysis. Continued radicals of the form $$x_n=\sqrt{a+x_{n-1}},\qquad x_0=0$$ have been considered in MSE before. It is easy to check that $x_n$ defines a bounded monotone increasing sequence and that $x_n\xrightarrow{n\to \infty}\ell _a,$ where $\ell _a$ is the unique positive solution to the quadratic equation $$x^2-x-a=0,$$ namely, $$\ell _a=\frac{1+\sqrt{1+4a}}{2}>1.$$ This is typically written as $$\ell _a=\sqrt{a+\sqrt{a+\ldots +\sqrt{a+\ldots }}}.$$ Since $$\ell _a-x_n=\frac{\ell _a^2-x_n^2}{\ell _a+x_n}=\frac{\ell _a-x_{n-1}}{\ell _a+x_n},$$ we have that \begin{align*}
\ell _a-x_n & =(\ell _a-x_1)\prod \limits _{k=1}^n\frac{1}{\ell _a+x_k} \\
& \leq (\ell _a-x_1)\frac{1}{\ell^n _a}\tag{1}\label{main}.
\end{align*} The problem is to improve the rate of convergence $|\ell _a-x_n|=O( \ell _a^n)$ . In particular, to show that \begin{align*}\ell _a-x_n & \sim \frac{C_a}{(2\ell_a)^n}\tag{2}\label{better}
\end{align*} for some constant $C_a>0$ . From \eqref{main} we have that $$b_n:=(2\ell _a)^n(\ell _a-x_n)=(\ell _a-x_1)\prod \limits _{k=1}^n\frac{2\ell _a}{\ell _a+x_k}\leq (\ell _a-x_1)\prod \limits _{k=1}^n\frac{\ell _a}{x_k}.$$ Since $\dfrac{2\ell _a}{\ell _a+x_n}>1$ , $b_n$ is monotone increasing. The result would follow if for example $p_n:=\prod \limits _{k=1}^n\dfrac{\ell _a}{x_k}$ were bounded above (or better yet, if $p_n$ converged). Any hints will be appreciated.","['calculus', 'convergence-divergence', 'asymptotics', 'real-analysis']"
4453696,A confusion on $\beta\eta$-reduction,"Recently I've started exploring lambda calculus, and currently I'm tackling the next exercise: Prove that if $M =_{\beta\eta} N$ , then $FV(M) = FV(N)$ , Where $FV(P)$ stands for free variables in the term $P$ . Obviously, $\eta$ -conversion doesn't affect on the free variables, since $\lambda x.Mx =_\eta M$ exactly when $x \notin FV(M)$ , and $FV(\lambda x.Mx) = FV(M)$ when the same condition holds. But what about $\beta$ -reduction? It's obvious, that if $M \to_\beta N$ , then $FV(N) \subseteq FV(M)$ . It's also known that $\beta$ -reduction can eliminate free variables, as said here or here . How then it might be possible that $FV(M) = FV(N)$ for the stated condition? The simple example $(\lambda x.y)z \to_\beta y$ , and thus $(\lambda x.y)z =_\beta y$ breaks the statement. Maybe there's a typo in the exercise? Or maybe I don't quite understand the difference between $\beta$ -reduction and $\beta$ -equivalence: as far as I understand, if $M \to_\beta N$ , then $M =_\beta N$ , and the latter, the $\beta$ -equivalence, just means (by Church-Rosser) that $\exists L$ such that $M \twoheadrightarrow_\beta L$ and $N \twoheadrightarrow_\beta L$ . But this facts, as I suppose, can't help me to resolve the issue. Seems obvious that there exists such $M$ and $N$ , that $M \twoheadrightarrow_\beta N$ , but the converse doesn't hold. And still it's said that $M =_\beta N$ . Thus $\to_\beta \subset \twoheadrightarrow_\beta \subset =_\beta$ should hold. Then, again, I don't get how the statement in the exercise above holds. Can anyone give a hint? Thanks in advance!","['lambda-calculus', 'definition', 'discrete-mathematics', 'computer-science']"
4453713,Relation between uniform and operator norm,"Let $f:U\subset \mathbb{C}\to \mathbb{R}^n$ be a $\mathcal{C}^1$ function. I would like to know the relation between the uniform norm and the operator norm of the differential. For this question to make sense, I must make a few comments... Remembering that the differential of the function $f$ , is $f':U\to \mathcal{L}\left (\mathbb{C},\mathbb{R}^n\right )$ . But in this special case, we are talking about a differentiable curve, and we have a natural isomorphism between $\mathcal{L}\left (\mathbb{C},\mathbb{R}^n\right )$ and $\mathbb{R}^n$ so the derivative can be seen as a function $f':U\to \mathbb{R}^n$ . And so we have the uniform norm: $$\|f' \|_\infty =\sup \limits _{z\in U}\|f'(z) \|.$$ Furthermore, seeing the differential as $f'\in \mathcal{L}\left (\mathbb{C},\mathbb{R}^n\right )$ , we can apply the operator norm, $$\|f'(z)\|=\sup \limits _{|v|=1}\|f'(z)\cdot v\|.$$ So, even with this information, I don't know if my question makes much sense, but I would like to know if the two norms are related, more specifically, if $\|f'\|_\infty <\infty$ we will also have $\|f'(z)\|<\infty$ for all $z\in U$ . What did I do: \begin{align*}\|f'(z)\| & =\sup \limits _{|v|=1}\|f'(z)v\|_{\mathbb{R}^n}=\sup \limits _{|v|=1}\|v(f'(z)\cdot 1)\|_{\mathbb{R}^n} \\
& ""=""\sup \limits _{|v|=1}\|v f'(z)\|_{\mathbb{R}^n} \\
& =\sup \limits _{|v|=1}|v|\|f'(z))\|_{\mathbb{R}^n}<\infty.
\end{align*} Where in the second line I used the identification I mentioned earlier... I would like to know if my reasoning is correct...","['normed-spaces', 'linear-algebra', 'functional-analysis', 'analysis']"
4453719,Push-forward of a smooth function,"I'm confused about the relation between two concepts: the push-forward of a smooth map between two smooth manifolds and the differential of a smooth real-valued function. Let $M,N$ be smooth manifolds and $\varphi :M\to N$ smooth. Then the push-forward of $\varphi$ at $p\in M$ is defined as \begin{align*}\varphi _{*p}:T_pM & \to T_{\varphi (p)}N \\
\varphi _{*p}(X_p)(g) & =X_p(g\circ \varphi )
\end{align*} for $g$ any smooth function on $N$ . On the other hand, for a smooth function $f:M\to \mathbb{R}$ , i.e. for the special case $N=\mathbb{R}$ , the differential of $f$ at $p$ is defined as \begin{align*}d_pf:T_pM & \to \mathbb{R} \\
d_pf(X_p) & =X_p(f).
\end{align*} Starting from these definitions, can we show that $f_{*p}=d_pf$ , or are they different objects?","['pushforward', 'differential-geometry']"
4453723,Partial derivative of a recursive function,"There are two functions: $$f(x,y):\mathbb{R}^2\to \mathbb{R}$$ and $$g(z):\mathbb{R}\to \mathbb{R}.$$ Both are differentiable. I have a function $G(x)=g(f(x,G(x)))$ . I want to take the derivative with respect to $x$ . Is this possible? If so, how should I go about this? Thank you!","['partial-derivative', 'calculus', 'derivatives']"
4453728,Prove or disprove: $t \mapsto \mathbb{P}( X_t \in A)$ is measurable,"Let $(X_t)_{t \geq 0}$ be an $\mathbb{R}$ -valued stochastic process and let the law of $X_t$ be given by $\mathbb{P}_t$ , $t \geq 0$ . Let $A \in \mathcal{B}(\mathbb{R})$ be a Borel set. I am interested in the measurability of the following mapping $$
[0, \infty ) \ni t \mapsto \mathbb{P} (X_t \in A) = \mathbb{E}[1_A(X_t)] \in [0, 1]. \tag{1}
$$ under the conditions stated below. Here is an example showing that this mapping is not measurable in general. Let $N \subset [0, \infty)$ be a set which is not Borel measurable, and let the random variables $X_t$ , $t \geq 0$ , be such that $\mathbb{P}(X_t =1)=1_N(t)$ . Then clearly $t \mapsto \mathbb{P}(X_t = 1)$ is not measurable. Now assume that: If $(t_n) \subset [0, \infty)$ is such that $t_n \rightarrow t_0 \in [0, \infty)$ , then $\mathbb{P}_{t_n}$ converges weakly to $\mathbb{P}_{t_0}$ . This means that for every continuous and bounded function $f : \mathbb{R} \rightarrow \mathbb{R}$ we have $$
\int_{\mathbb{R}} f(x) \mathbb{P}_{t_n} (dx) = \mathbb{E} [f (X_{t_n})] \overset{n \rightarrow \infty}{\rightarrow} \int_{\mathbb{R}} f(x) \mathbb{P}_{t_0} (dx) = \mathbb{E} [f (X_{t_0})]
$$ The assertion is also true for bounded Lipschitz functions. Does this ensure the measurability? Is it maybe possible to approximate the indicator function $x \mapsto 1_A(x)$ by suitable continuous functions which will imply the measurability? Alternatively, is it possible to obtain a right-continuous modification of $(X_t)_{t \geq 0}$ under these conditions? If so, this will ensure the joint measurability of $(t, \omega ) \mapsto X_t (\omega)$ and Tonelli's theorem will yield the measurability of the mapping in $(1)$ . If the answer is affirmative, it would be interesting to see if it also applies to the case where $X_t$ , $t \geq 0$ , are $S$ -valued for a sufficiently nice metric space $S$ .","['probability-theory', 'functional-analysis', 'measure-theory', 'real-analysis']"
4453744,"Spivak's Calculus, Ch. 11, ""Significance of the Derivative"", prob. 58: Prove $f'$ increasing then every tangent line intersects graph of $f$ only once","The following is a problem from ch. 10, ""Significance of the Derivative"", from Spivak's Calculus Prove that if $f'$ is increasing, then every tangent line of $f$ intersects the graph of $f$ only once. My proof seems at first glance to be the same as the solution manual's proof, but I don't understand the last step in the terse solution manual proof. Here is my proof $f'$ increasing means $\forall x \forall y, x<y \implies f'(x)<f'(y)$ . Consider a point $(x_0, f(x_0))$ on the graph of $f$ . The tangent line at this point has slope $f'(x_0)$ . Assume this line intersects the graph of $f$ at $(x,f(x))$ . Then $f'(x_0)=\frac{f(x)-f(x_0)}{x-x_0}$ . By the Mean Value Theorem we know that $$\exists c, c \in (x_0, x) \land f'(c)=\frac{f(x)-f(x_0)}{x-x_0}$$ $$\implies f'(x_0)=f'(c)$$ $$\bot$$ The contradiction occurs because $f'$ is increasing by assumption. Therefore, by proof by contradiction, we conclude that the tangent line line at any point does not intersect the graph of $f$ at any other point. Here is the proof from the solution manual The tangent line through $(a,f(a))$ is the graph of $$g(x)=f'(a)(x-a)+f(a)$$ $$=f'(a)x+f(a)-af'(a)$$ If $g(x_0)=f(x_0)$ for some $x_0 \neq a$ , then $$0=g'(x)-f'(x)=f'(a)-f'(x), \text{ for some } x \text{ in } (a,x_0) \text{ or } (x_0,a)\tag{1}$$ This is impossible, since $f'$ is increasing. Where does $(1)$ come from?","['proof-explanation', 'calculus', 'derivatives']"
4453757,"Convexity in ""usual Partition of Unity arguments""","I stumbled upon Problem 13-2 on p.344 in John Lee's Introduction to Smooth Manifolds (2nd Edition) where Lee explains that the proof for the existence of a Riemannian Metric on a manifold is done by a ""Partition of Unity""-Argument and he emphasizes that a crucial part in the proof was that the set of inner products on a given tangent space is a convex subset of the vector space of all symmetric $2$ -tensors. Now it seems that the convexity property can not be omitted in ""usual partition of unity"" arguments but unfortunately, I don't understand why it's so important. Where exactly does the convexity of the (respective) subset comes into play if we want to do such partition of unity argument (in a much more general sense)? So, to make my question a bit more precise: My question: Say we want to patch together local objects to a global one (e.g. local sections of a vector bundle to a global section) by a usual partition of unity argument. Why and where do we need to use convexity of the subset containing the images of the objects we want to patch together? (in the case of local sections we require that there is an open set $V\subset E$ (total space) so that $V\cap E_p$ is a convex subset of the fiber $E_p$ . Why do we need convexity of $V\cap E_p$ for example? Thanks in advance for any help!","['fiber-bundles', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
4453764,Coin change problem with specific multiples,"Background There are coins with value of 1, 5, 10, 20, 50, 100. Asking for how many ways are there to make up 10000? The answer is 174716753951. As far as I know this is equivalent to finding the 10000th term of the following generating function: And the formula for general term will be very complicated and can only be solved by recursion. $$
G(x) = \frac{1}{(1-x) \left(1-x^5\right) \left(1-x^{10}\right) \left(1-x^{20}\right) \left(1-x^{50}\right) \left(1-x^{100}\right)}
$$ But the solution gives a formula, if n is a multiple of 100, then there is: $$g_{n|100}=\frac{50 n^5}{3}+\frac{475 n^4}{6}+\frac{265 n^3}{2}+\frac{551 n^2}{6}+\frac{137 n}{6}+1$$ The solution does not give more explanation. Question I want to know how this polynomial is obtained. Under what circumstances will I get such a polynomial, I tried $n|50$ , and there is no polynomial relationship under 100 degree.","['combinatorics', 'generating-functions']"
4453767,"If someone wins 3000 rounds of a game out of 100000 numbered rounds of a game, what is the expected no. of last-3-digits of a round that they solved?","I participate in r/picturegame on Reddit. Each round is numbered from Round 0 (1 actually, but let's say it was 0) up to Round 111400 at the moment. Suppose the top player of this game had won 3000 rounds out of all the rounds up to and including round 99999 and has been playing since the beginning (this is roughly true and keeps the numbers convenient I think.) Let's also suppose that he isn't especially trying to win rounds of a certain number, and the round numbers don't follow any particular pattern relative to his sleep schedule. (This is also realistic.) Now let's take the last 3 digits of each round that he won, so round 89345 would be ""345"", round 40080 would be ""080"" and (for the sake of argument) round 66 would be ""066"". Out of the 3000 rounds that he won, what is the expected number of round-endings, from ""000"" to ""999"", that are not represented amongst those rounds?","['random', 'statistics', 'probability']"
4453785,Quadratic With Periodic Coefficents,"I've come across a problem that results in the equation: $t^2 -2t\sin t -2\cos t -2 = 0$ I've tried to do this analytically but I can't figure it out. At this point, I just want to know if it's even possible for something like this to be solved exactly. So can an equation $ax^2+bx+c=0$ , where $a, b, \text{or } c$ are a trig function, be solved?","['trigonometry', 'quadratics', 'transcendental-equations']"
4453811,Let $A = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr)$. Prove for $n \geq 1$ using induction that $A^n =$ ...,"Can someone check to see if my proof is correct? Feel free to nitpick, trying to get better at writing proofs. Here's the problem: Let $A = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr)$ . Prove for $n \geq 1$ that $A^n = 4^n \bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^n \bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr)$ by induction. Note: $A^n$ means $A$ matrix multiplied by itself $n$ times. So for example, $A^2 = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) = \bigl( \begin{smallmatrix}2 & 14 \\ -7 & 23\end{smallmatrix}\bigr)$ My attempt at the proof: Base case, $n=1:$ $A^1 = \bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) = 4^1\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr)+ 3^1\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) \ \checkmark$ Assume the inductive hypothesis, $n=k: A^k = 4^k\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^k\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr)$ WTS that the proposition holds for $n=k+1: A^{k+1} = 4^{k+1}\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^{k+1}\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr)$ \begin{align*}
A^k A = \left(4^k\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^k\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr)
\right)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr)
&= 4^k\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) + 3^k\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr)\bigl( \begin{smallmatrix}2 & 2 \\ -1 & 5\end{smallmatrix}\bigr) \\
&= 4^k\bigl( \begin{smallmatrix}-4 & 8 \\ -4 & 8\end{smallmatrix}\bigr)
+ 3^k\bigl( \begin{smallmatrix}6 & -6 \\ 3 & -3\end{smallmatrix}\bigr) \\
&= (4^k)(4)\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr)  + 
(3^k)(3)\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) \\
A^k A &= 4^{k+1}\bigl( \begin{smallmatrix}-1 & 2 \\ -1 & 2\end{smallmatrix}\bigr) + 3^{k+1}\bigl( \begin{smallmatrix}2 & -2 \\ 1 & -1\end{smallmatrix}\bigr) \\
A^k A &= A^{k+1}
\end{align*} By induction the proposition holds for all $n \geq 1$","['matrices', 'induction', 'solution-verification']"
4453882,How to get an entire function $f(z)$,"$f$ is an entire function such that $|zf(z)-1+e^z|\leq 1+|z| \ \forall z\in \mathbb C$ then which of the folowing are true? (1) $f'(0)=1$ , (2) $f'(0)=-1/2$ , (3) $f'(0)=-1/3$ , (4) $f'(0)= -1/4 $ My approach: Since $f$ is an entire function so it has a power series expansion then $f(z)=\sum_{n=0}^{\infty}a_nz^n$ . So, $|z\sum_{n=0}^{\infty}a_nz^n-1+e^z|=|z\sum_{n=0}^{\infty}a_nz^n+\sum_{n=1}^{\infty}\frac{z^n}{n!}|=|a_0z+\sum_{n=1}^{\infty}(a_nz+\frac{1}{n!})z^n|$ So, $|a_0z+\sum_{n=1}^{\infty}(a_nz+\frac{1}{n!})z^n|\leq 1+|z|$ Now I can not understand how to proceed. Can anyone please help me? Thank you in advance.",['complex-analysis']
4453884,"Prove that a relation R on set A is antisymmetric if and only if $R \cap R^{-1} \subseteq \{(a,a):a \in A\}$.","Can someone check to see if my proof is correct? If it actually is correct, can someone tell me how to be less verbose and ""make it mathy and less wordy"" for my backwards implication $(\Leftarrow)$ portion of the proof? Here's the problem: Prove that a relation $R$ on set $A$ is antisymmetric if and only if $R \cap R^{-1} \subseteq \{(a,a) : a \in A\}$ . My proof attempt: $(\Rightarrow)$ Let $R$ be an antisymmetric relation on $A$ . Suppose $(a,b),(b,a) \in R$ where $a, b \in A$ . Then by definition of antisymmetric it must the case that $a=b$ , so $(a,a) \in R \Rightarrow (a,a) \in R^{-1}$ $\Rightarrow (a,a) \in R\cap R^{-1} \Rightarrow (a,a) \in \{(a,a) : a \in A\}$ . Since $a, b$ were arbitrary elements, $R \cap R^{-1} \subseteq \{(a,a) : a \in A\}$ $(\Leftarrow)$ Let $R\cap R^{-1} \subseteq \{(a,a) : a\in A\}$ . To be a subset of $\{(a,a) : a \in A\}$ , $R\cap R^{-1}$ must be a set of ordered pairs where the 1st and 2nd terms of the ordered pairs are equal to a single element from $A$ . Consider then, any arbitrary element of A, say $a \in A$ . Then $(a,a) \in A \times A$ and $(a,a) \in R \cap R^{-1}$ . By definition of intersection, $(a,a) \in R$ as well. These results hold for all elements of $A$ , so $R \subseteq A\times A$ . Thus, $R$ is a relation on $A$ by definition. To say $R$ is an antisymmetric relation on $A$ is vacuously true since there will never be an ordered pair of the form $(x, y)$ or $(y,x)$ in $R$ , unless, of course, $x = y$ . Since $R\cap R^{-1} \subseteq \{(a,a) : a\in A\} \Rightarrow$ the relation $R$ on set $A$ is antisymmetric And since the relation $R$ on set $A$ is antisymmetric $\Rightarrow R\cap R^{-1} \subseteq \{(a,a) : a\in A\}$ Therefore the relation $R$ on set $A$ is antisymmetric $\Leftrightarrow$ $R \cap R^{-1} \subseteq \{(a,a) : a \in A\}$ . SECOND ATTEMPT : Thanks for the comments. Is this an ""improvement"" for my forward implication? $(\Rightarrow)$ We are given the relation $R$ on $A$ is antisymmetric. WTS that $R \cap R^{-1} \subseteq \{(a,a):a\in A\}$ . Let $a,b \in A$ be arbitrary elements such that $(a,b) \in R\cap R^{-1}$ . Then $(a,b) \in R$ and $(a,b) \in R^{-1}$ . If $(a,b) \in R^{-1}$ then $(b,a) \in R$ . By def of antisymmetric, since we have $(a,b)$ and $(b,a)$ in $R$ , it must be the case that $a = b$ , that is, $(a,a) \in R$ . Then $(a,a) \in R^{-1}$ as well, which means $(a,a) \in R \cap R^{-1}$ . Thus, $(a,b) \in R\cap R^{-1} \Rightarrow (a,a) \in R\cap R^{-1}$ , so $R\cap R^{-1} \subseteq \{(a,a): a \in A\}$ Which as you said, is not true since we could have the case where $R = \emptyset$ which is antisymmetric, and any non-empty $A$ like $a \in A$ then $(a,a) \notin R \cap R^{-1}$ ... how do I get around to showing this result holds for all relations R on A?","['elementary-set-theory', 'solution-verification', 'relations']"
4453933,Why is the trace of a matrix important?,"Lower division linear algebra course at my university taught the simple computation steps to finding a trace of a matrix, but not the intuition nor the purpose for it. What information is gained from summing the diagonal entries? Why is the trace of a matrix important?","['matrices', 'trace', 'linear-algebra']"
4453935,"On the number of roots of $p(z,\bar z)$","Let $p$ be a polynomial with complex (or even real) coefficients in the variables $z$ , $\bar z$ , where $\bar z$ is the conjugate of $z$ . What can we say on the number of complex roots of $p$ ? Clearly $p$ can have both zero roots (e.g. $p(z,\bar z) = z + \bar z - i$ ) or infinite roots (e.g. $p(z,\bar z) = z + \bar z$ ). I'd like to know if we can say anything when the number of roots is known to be finite. Clearly this relates in some way to the number of real solutions to the system $$
\begin{cases}
P(x,y) = 0, \\
Q(x,y) = 0,
\end{cases}
$$ where $P,Q$ are polynomials with real coefficients. I know some vague things about Bezout's Theorem, but since this is not my field I'm not sure if this result can be applied to this specific case.
Besides, maybe there are some better results for this kind of polynomials which I have failed to find. Thanks in advance to everyone who will help!","['complex-analysis', 'roots', 'polynomials', 'complex-numbers']"
4453969,Expected value of $X/(X+Y)$,"I have a task. $X$ and $Y$ are independent random variables with exponential distribution and $\mathbb{E}X=1$ , $\mathbb{E}Y=\frac{1}{2}$ . Calculate $\mathbb{E}\big(\frac{X}{X+Y}\big)$ . I tried to calculate it as $\int_{0}^{\infty}\int_{0}^{\infty} \frac{x}{x+y}2e^{-2y-x}\mathrm{d}x\mathrm{d}y$ , but it is hard. Is the easier way to calculate this task?","['expected-value', 'probability-distributions', 'probability-theory', 'probability']"
4454001,How to understand the definition of the base locus and the base ideal of a complete linear system?,"Let $X$ be a projective variety and $D$ a Cartier divisor on $X$ . In the book Positivity in AG , the definition of the base ideal of $|D|$ is the image of the map $$eval_{|D|}: H^0(X,D) \otimes O_X(-D) \to O_X .$$ And the base locus of $|D|$ is defined to be the closed subset of $X$ cut out by the base ideal. My question is: How does it related to the usual definition of the base locus(i.e., the set of points at which all the sections in $H^0(X,D)$ vanish)?",['algebraic-geometry']
4454013,Generalizing Ramanujan cubic denesting formula to higher powers,"We have the following theorems for denesting radicals of degree 2 and 3 : Denesting theorem for degree 2 : If $\alpha, \beta$ are the roots of the equation, \begin{equation}
x^2-ax+b = 0
\end{equation} then \begin{equation}
\sqrt{\alpha} + \sqrt{\beta} = \sqrt{a + 2\sqrt{b}}
\end{equation} This theorem can easily be proved by using the Vieta formulas, and squaring both sides. It tells us that such nested radicals can be denested when the determinant is a perfect square ( $a^2-4b^2 = n^2$ ). Denesting theorem for degree 3 (from Ramanujan) : If $\alpha, \beta,\gamma$ are the roots of the equation, \begin{equation}
x^3-ax^2+bx - 1 = 0
\end{equation} then \begin{align}
\sqrt[3]{\alpha} + \sqrt[3]{\beta}+ \sqrt[3]{\gamma} &= \sqrt[3]{a + 6 + 3t} \\
\sqrt[3]{\alpha\beta}+\sqrt[3]{\beta\gamma}+\sqrt[3]{\gamma\alpha}&=\sqrt[3]{b+6+3t}
\end{align} where $t$ satisfy the equation \begin{equation}
t^3âˆ’3(a+b+3)tâˆ’(ab+6(a+b)+9)=0
\end{equation} This theorem is well proved here , and allows to produce many nice identities, such as, $$\sqrt[3]{\cos\tfrac {2\pi}7}+\sqrt[3]{\cos\tfrac {4\pi}7}+\sqrt[3]{\cos\tfrac {8\pi}7}=\sqrt[3]{\tfrac 12\left(5-3\sqrt[3]7\right)}$$ Questions : How to generalize such denesting theorems to higher powers ? What nice identities can we generate ? Let's take the case for degree 4.
Let $\alpha, \beta,\gamma,\delta$ are the roots of the equation, \begin{equation}
x^4-ax^3+bx^2 -cx + 1 = 0
\end{equation} then \begin{equation}
\sqrt[4]{\alpha} + \sqrt[4]{\beta}+ \sqrt[4]{\gamma} + \sqrt[4]{\delta} = \sqrt[4]{a + 4t + 6u + 12v}
\end{equation} where $t,u,v$ are \begin{align}
t &= \sum_{perm} \sqrt[4]{\alpha}^3\sqrt[4]{\beta} \\
u &= \sum_{perm} \sqrt[4]{\alpha}^2\sqrt[4]{\beta}^2 \\
v &= \sum_{perm} \sqrt[4]{\alpha}^2\sqrt[4]{\beta}\sqrt[4]{\gamma}
\end{align} which we can find by taking the LHS to the power 4, and using the multinomial theorem. It would be very nice if the $t,u,v$ where also to satisfy a polynomial equation of degree 4 ( does it ? ). If it so then we would have a theorem similar to the cubic and we could generate new identities. However to verify if $t,u,v$ satisfy a quartic by elevating them to the power 4 is algebraically heavy and I have not been able to do it. I tried to use symmetrical polynomials to simplify computations.","['nested-radicals', 'roots', 'symmetric-polynomials', 'polynomials', 'algebra-precalculus']"
4454047,Lie derivative in Kahler manifold,"I have the following question $X$ - a compact Kahler manifold and $v\in \Gamma(X,TX)$ - Killing vector field. I don't understand why Lie derivatives equal to zero $L_v \omega=0$ where $\omega$ is the Kahler form $L_vI=0$ where $I$ is the complex structure My attempt is the following. Killing field is satisfied the following equation $L_vg=0$ . We have known from the definition that A Kahler Manifold is an hermitian manifold, whose KÃ¤lher form is closed. It follows that $d\omega=0$ . Let $h$ -hermitian metric and from the famous theorem we know that there is a correspondence between $h \to \omega=-Im  h$ . And we have that $g(m,n)=\omega(m,In)$ , where $I$ is the complex structure. And we can obtain the result or am I a wrong about the first statement? I don't know what to do with the second statement. If you don't mind please explain it in more details. Thank you!","['kahler-manifolds', 'complex-geometry', 'riemannian-geometry', 'differential-geometry']"
4454112,How to get a tuple representation of a DirectProduct in GAP? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 2 years ago . Improve this question Question. Is there any way to get and use a tuple representation of a DirectProduct ? Explaination. Let's assume we have groups $G, H$ which are cyclic group and symmetric group respectively.  Let's take $L$ as a direct product of $G$ and $H$ .
I am aware of Projection and Embedding functions as the example below presents. gap> G := CyclicGroup(6);
<pc group of size 6 with 2 generators>
gap> H := SymmetricGroup(6);
Sym( [ 1 .. 6 ] )
gap> L := DirectProduct(G,H);
<group of size 4320 with 4 generators>
gap> Embedding(L,1);
MappingByFunction( <pc group of size 6 with 
2 generators>, <group of size 4320 with 4 generators>, function( elm ) ... end )
gap> Image(Embedding(L,1));
<group of size 6 with 2 generators>
gap> Image(Embedding(L,2));
<group of size 720 with 2 generators>
gap> My goal is to take an exact element of $g \in G$ and $h \in H$ and get an element $(g,h) \in L$ . I have noticed there is an object called DirectProductElement , but there is no documentation about it.","['gap', 'group-theory']"
4454117,Question about applying Dominated Convergence Theorem,"Question: $\phi_n(x)=\int_{x_0}^xf(t,\phi_n(t))dt$ , where $\phi_n(x)$ is continuous on $[a,b]$ and $f$ is continuous and bounded on $[a,b]\times(-\infty,+\infty)$ . If $\phi_n(x)$ converges uniformly to $\phi(x)$ , prove that $\phi(x)=\int_{x_0}^xf(t,\phi(t))dt$ . If $\phi_n(x)$ just converges to $\phi(x)$ , can we draw the same conclusion by applying Dominated Convergence Theorem like this: $|f|\leqslant M,\lim\limits_{n\to\infty}\int_{x_0}^xf(t,\phi_n(t))dt=\int_{x_0}^x\lim\limits_{n\to\infty}f(t,\phi_n(t))dt=\int_{x_0}^xf(t,\phi(t))dt$ , the first equation by DCT and the second by continuity of $f$ . Attempt: Since $\phi_n$ uniformly converges to $\phi$ ,then $\phi_n$ is bounded and $\phi$ is continuous on $[a,b]$ . So $f$ is uniformly continuous on $[a,b]\times[-M,M]$ ,where $|\phi_n|,|\phi|<M$ . Therefore $f(t,\phi_n(t))$ converges uniformly, so we can exchange the limit and integral. Is that correct? Thanks for checking! I'm wondering if DCT is correctly used here. Can we use $M(|f|<M)$ as the dominating function?","['lebesgue-integral', 'analysis', 'real-analysis', 'uniform-convergence', 'convergence-divergence']"
4454126,"Weak* separability of dual unit ball of D[0,1]","Let $D[0,1]$ be the space of all right-continuous left-limited functions $f\colon [0,1]\to \mathbb{R}$ equipped with the supremum norm $f\mapsto \|f\|_\infty = \sup_{t\in[0,1]} |f(t)|$ . This is a non-separable Banach space whose dual $D[0,1]^\ast$ is known to be separable in the weak* topology; see, e.g., Chapter 41, p. 1756 of Johnson, W. B. (ed.); Lindenstrauss, J. (ed.) , Handbook of the geometry of Banach spaces. Volume 2, Amsterdam: North-Holland. xii, 1007-1866 (2003). ZBL1013.46001 . Is the unit ball in $D[0,1]^\ast$ separable in the weak* topology?","['banach-spaces', 'functional-analysis', 'weak-topology']"
4454143,Is there a way to solve $\alpha_1\sin{(x})+\beta_1\cos{(x)}+\alpha_2\sin{(2x)}+\beta_2\cos{(2x)}=c$ for $x$?,Is there any method to solve the equation above for $x$ ? $$\alpha_1\sin{(x})+\beta_1\cos{(x)}+\alpha_2\sin{(2x)}+\beta_2\cos{(2x)}=c$$ This would be a trivial problem if it was only the first pair or second pair of functions. Perhaps it's also a trivial question as it stands and I'm simply unaware. All I know is that WolframAlpha was really chugging and couldn't give a clean answer before timing out.,['trigonometry']
4454255,Convergence of sum of MLE's over all possible $0/1$ sequences,"Fix $N\in\mathbb N$ and take $\Theta=\{\frac1{N+1},\ldots,\frac N{N+1}\}$ . For $y^T\in\{0,1\}^T$ , let $\hat\theta(y^T)$ be the number in $\Theta$ that is closest to $\frac1T\sum_{i=1}^Ty^T_i$ . Let $\sum_{i=1}^Ty^T_i=:k(y^T)$ , and let $p_\theta(y^T)=\theta^{k(y^T)}(1-\theta)^{T-k(y^T)}$ . In my lecture notes I read that $$\sum_{y^T\in\{0,1\}^T}p_{\hat{\theta}(y^T)}(y^T)\to N,$$ as $T\to\infty$ , by the law of large numbers. I was not able to figure out why this was the case, exactly. My first problem is that I don't even see how the LLN comes into play here.  Any help on this is much appreciated.","['statistics', 'maximum-likelihood', 'limits', 'law-of-large-numbers', 'probability']"
4454263,Eigenvalue and spectral condition,"Let $A=\begin{pmatrix} 1& 1 \\ a^2 &1 \end{pmatrix} \text{ with } a\in (0,\frac{1}{2}]$ . Show $$cond_2(A)=||A||_2 \cdot ||A^{-1}||_2\leq 4(1-a^2)^{-1}$$ by first showing $||A||^2_2\leq||A||_1||A||_{\infty}$ . $||A||^2_2$ is the maximal eigenvalue of $A^TA$ and $||A||_1||A||_{\infty}=2\cdot2 = 4$ . Prove $||A||^2_2\leq||A||_1||A||_{\infty}=4$ by contradiction: Suppose $\lambda_{max} >4$ , then $$4<\lambda_{\ast}=\frac{\sqrt{a^8+2a^4+4a^2+1}+a^4+3}{2} \iff 5<\sqrt{a^8+\underbrace{2a^4}_{\leq \frac{1}{2}}+\underbrace{4a^2}_{\leq 1}+1}+\underbrace{a^4}_{\leq 1} \\ \leq \sqrt{4}+1 = 3$$ contradiction! So I know $||A||^2_2\leq 4 \Rightarrow ||A||_2\leq 2$ but I need to show $cond_2(A) = \underbrace{||A||_2}_{\leq 2}||A^{-1}||_2\leq 4(1-a^2)^{-1}$ . I don't know why I needed to show $||A||^2_2\leq||A||_1||A||_{\infty}$ first? How does it help? I still need the maximal eigenvalue of $(A^{-1})^TA^{-1}$ . The eigenvalues are $$\lambda_1=\frac{\sqrt{a^4-2a^2+5}-a^2+1}{2\sqrt{a^4-2a^2+5}-4} \\ \lambda_2=\frac{\sqrt{a^4-2a^2+5}-a^2-1}{2\sqrt{a^4-2a^2+5}+4}$$ What am I supposed to do now? Thanks for any help!","['condition-number', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'inequality']"
4454279,A combined arithmetic and geometric sequence question,"Here is a question I am currently struggling with - The first, the tenth and the twentieth terms of an increasing arithmetic sequence are also consecutive terms in an increasing geometric sequence. Find the common ratio of the geometric sequence. Here's what I've done so far - $u_1=v_1$ $u_{10}=v_2$ $u_{20}=v_3$ We know that, $\displaystyle \frac{v_2}{v_1}=\frac{v_3}{v_2}$ and, $u_1=u_1$ $u_{10}=u_1+9d$ $u_{20}=u_1+19d$ Therefore, $\displaystyle \frac{u_1+9d}{u_1}=\frac{u_1+19d}{u_1+9d}$ Upon simplifying - $(u_1+9d)^{2}=u_1(u_1+19d)$ $\therefore \displaystyle u_1=81d$ Now, what do I do after this?","['arithmetic-progressions', 'sequences-and-series']"
4454296,How do you find this limit with a relationship to $e$ using Taylor series?,"The limit in question is $$
\lim_{x \to 0}\left(\frac{\sin(x)}{x}\right)^{1/x^2}
$$ When I replace $\sin(x)$ with its Taylor series about $0$ and cancel out the $x$ , I get $$
\lim_{x \to 0}\left(1-\frac{x^2}{6} + \frac{x^4}{5!} \mp \cdots\right)^{1/x^2}
$$ The answer in the book is $e^\frac{-1}{6}$ . If I only look at the $x^2$ term, I can see where $-1/6$ comes from. I'm just not sure how I can definitively say the answer is $e^\frac{-1}{6}$ . Why can I discount the other powers of $x$ ?","['limits-without-lhopital', 'calculus', 'taylor-expansion', 'power-series', 'limits']"
4454307,Attempting to restate the question of whether the collatz conjecture has a nontrivial cycle as a combinatorics problem,"It occurs to me that the question about whether non-trivial cycles exist for the collatz conjecture can be restated as these two questions (details on how this relates to the collatz conjecture can be found here ): Is there a general method for determining how many distinct values of $t_1, t_2, \dots, t_k$ exist for a given $k$ such that: $t_k > t_{k-1} > \dots > t_2 > t_1 > 0$ $2\left(2^{t_k} - 3^k\right) < 3^{k-1} + \sum\limits_{i=1}^{k-1}3^{k-1-i}2^{t_i}$ $2^{t_k} - 3^k > 1$ Would it follow that as $k$ increases, the number of distinct values approaches infinity? It seems to me that the conjecture is false if any nontrivial cycle occurs. A non-trivial cycle occurs if $2^{t_k}âˆ’3^k$ divides $3^{kâˆ’1}+\sum\limits_{i=1}^{ð‘˜âˆ’1}3^{hâˆ’1âˆ’i}2^{t_i}$ which would seem to me be a high probability if there are an infinite number of distinct values. Infinity does not mean this is necessary the case. More information is needed on the variability of the distinct values Do my assumptions sound reasonable?  Are there any well known papers that investigate the collatz conjecture from this viewpoint? Update: By ""non-trivial"" cycles, I mean cycles that involve $2^{t_k} - 3^k > 1$ and include all cycles listed here as ""trivial"". I have added a third bullet point above to clarify this point.  Thanks to Rosie F for noticing that it was missing.","['collatz-conjecture', 'number-theory', 'combinatorics', 'reference-request']"
4454329,Law of the square of a martingale divided by its bracket,"Let $(M_t)_{t\geq 0}$ be a continuous martingale such that $M_0=0$ almost surely. There exists an increasing process $(\langle M\rangle_t)_{t\geq 0}$ which is called the bracket of $M$ such that $M^2-\langle M\rangle$ is a martingale. If we look at the special case where $M$ is a Brownian motion, then for every $t\geq 0$ , $\langle M\rangle_t=t$ . Then, for every $t>0$ the quantity $\frac{M_t^2}{\langle M\rangle_t}$ is distributed as the square of a standard Gaussian random variable which is also $2\Gamma(1/2,1)$ . My question is the following: is this always true that $\frac{M_t^2}{\langle M\rangle_t}$ is distributed as $2\Gamma(1/2,1)$ ? I am under the impression that this is true for the geometric Brownian motion. However I do not see any general proof. If it is false, does someone know a counter example?","['stochastic-processes', 'probability-distributions', 'probability-theory', 'martingales']"
4454422,Positive integers satisfying $a^b = cd$ and $b^a = c+d$,"Yesterday, at 23:18, I thought it was a remarkable moment of the day. The digits on the watch were providing a quadruplet of positive integers that satisfy the following system of equations: $$\begin{align} a^b &= cd \\ b^a &= c+d \end{align}$$ I wondered what the set of all positive integer solutions of this system was. Writing $d=b^a-c,$ I obtained a quadratic of $c$ with coefficients in terms of $a$ and $b$ , which led me to the following: $$\left\{ c,d \right\} = \left\{\frac{b^a-\sqrt{b^{2a}-4a^b}}{2},\frac{b^a+\sqrt{b^{2a}-4a^b}}{2}\right\}$$ Therefore, given positive integers $a$ and $b,$ there is a solution if and only if $b^{2a}-4a^b$ is a perfect square. A brute force search using this result yields the following solutions: $(1,2,1,1),$ $(2,2,2,2),$ $(2,3,1,8),$ and $(2,3,8,1).$ I believe that there is no other solution than these. However, I'm unable to prove it. I would be glad if anyone could help me.","['exponential-diophantine-equations', 'number-theory', 'elementary-number-theory', 'diophantine-equations']"
4454430,Spider and fly problem on a Teserract.,"In the spider and fly puzzle , there is a spider on the inside of a cuboidal room wondering how it would get to a fly on another point on the inside of the cuboid. Here, we just ""flatten out"" the cuboid in different ways and take the shortest path across all of their shortest paths. Now, replace the cube with a Tesseract. If the spider is restricted to the cubical boundaries (it can now ""fly in 3-d space"") and the fly is similarly on another cubical boundary, the same algorithm can be extended. Just flatten it out in 3-d space every possible way and find the shortest straight-line path among them. But what do we do if the spider loses its new power and goes back to only crawling on the 2-d surfaces (and the fly is on one of those surfaces as well). Now the trick from before where we flatten out doesn't work anymore. It is impossible to flatten out a Tesseract onto a 2-d surface without having some faces overlap (see: Prove that it isn't possible to flatten a Teserract into 2-d space. ). So what do we do now? How to find the shortest path between two points on the 2-d surfaces of a Tesseract? Is there no algorithm? NOTE: The accepted answer here: Is there a general solution for the ""Spider and the Fly Problem""? in the 3-d case does require looping through unfoldings. The unfoldings are basically spanning trees. But for a given spanning tree, not sure how to get the shortest path without being able to ""lay it flat"".","['solid-geometry', 'optimization', 'geometry']"
4454435,Why does this cycle of 44 show up in the Collatz Conjecture?,"Consider this function: $$f\left(x\right)=\frac{x-b^{\left(\operatorname{floor}\left(\log_{b}x\right)\right)}}{b^{\left(\operatorname{floor}\left(\log_{b}x\right)\ +\ 1\right)}-b^{\left(\operatorname{floor}\left(\log_{b}x\right)\right)}}$$ Here is a StackExchange post with more information about the function. But basically, you could describe the function as: A relative measure of how far away a number is from the smallest number with the same number of digits. or A relative measure of how far a number 'x' is from the biggest power of 'b' that is still less than or equal to 'x' So anyways, if we set b=6, then apply f(x) to elements of Collatz orbits that have a length > 44, we see a cycle. It's easy to see this when you graph the points ( i , f(x) ) on a polar graph. i = the index of the element in the sequence x = the value at COLLATZ[ i ] where COLLATZ is an array created from the elements of the Collatz orbit starting at n and terminating at 1. Here is the graph for n = 27. The points are labeled ( i , COLLATZ[ i ]) It clearly cycles at 44. To test this hypothesis further, I tried graphing n = 670617279, which has 949 steps. Here is the result. I thought it may be simply because I was using polar coordinates, but I see similar symmetry when using cartesian coordinates. I think I'm on the verge of understanding why, but I just wanted to ask the community for any insight. I find it odd that 44 shows up. By the way, here is a medium article where I go over the use of this function in more detail as it applies to Collatz.","['modular-arithmetic', 'collatz-conjecture', 'number-theory', 'polar-coordinates', 'dynamical-systems']"
4454455,How do we calculate $\int\frac{\mathrm{d}x}{\cos(x) - 3\sin(x)}$?,"\begin{align*}
\int\frac{\mathrm{d}x}{\cos(x) - 3\sin(x)}
\end{align*} I can't pick this numbers for formula $\cos(x)\cos(y) - \sin(x)\sin(y)$ . May you help me? Maybe I can use different way, but this way is more simple","['indefinite-integrals', 'trigonometry']"
4454473,Dual of $L \log L(\mathbb{R})$,"Consider the space $$L\log L(\mathbb{R})=\left \{f\in L^1(\mathbb{R}):\int \limits _\mathbb{R}|f(x)|\log ^+|f(x)|\,dx<\infty \right \}.$$ Is it known what its dual and predual spaces are? Also any reference on its properties would be great! I learnt of this space and its relationship with the Hardy-Littlewood maximal operator in Stein's paper in Studia Math, and I would like to discover further properties.","['reference-request', 'harmonic-analysis', 'functional-analysis', 'real-analysis']"
4454474,"Does the filter, $F$, on $S$ exist such that $p,q\in S$ and $p,q\in \lim{F}$","Consider two points $p,q\in S$ with $p\ne q$ . Is it possible to find a filter, $F$ , on $S$ such that all neighborhoods of $p$ and $q$ are contained in $F$ ?
I would assume that when $p\ne q$ then, in the general case, there exist two neighborhoods $N_p$ and $N_q$ such that $N_p\cap N_q=\emptyset$ which would mean that $F$ is not a filter. However, my lecturer said that such a filter does exist.","['real-analysis', 'filters', 'abstract-algebra', 'discrete-mathematics', 'general-topology']"
4454479,Analytic extension of $(-1)^n$ satisfying growth condition,"Can the function $(-1)^n$ , $n=1,2,...$ be extended to an analytic function $f(z)$ defined on the right half complex plane satisfying the growth condition $$|f(x+iy)|\le C e^{Px+A|y|},$$ with $A<\pi$ and $C,P\in\mathbb{R}$ ? I know that such an extension would be unique by Carlson's theorem and I showed that the obvious ""power function"" extension from complex analysis does not work for any choice of branch. I'm not sure where to go from here.","['complex-analysis', 'interpolation', 'interpolation-theory']"
4454485,Good Algorithm to Compute all Subgroups of a Finite Group.,"Let's suppose we have a group $G$ of finite order $n$ . We want to algorithmically compute all subgroups, and there are some ways to do that. First one: Compute all subsets. Verify for each of them if they're closed. Then we're done. It's not precisely an efficient algorithm. For $n=24$ we already have 16 millions of subsets to check. A second one would be to compute all the generated subgroups: Get a prime decomposition $n=p_1^{e_1}\cdots p_{r}^{e_r}$ . Compute $$b=\sum_{i=1}^r (e_i+1).$$ Compute all subsets with at most $b$ elements Compute the generated subgroups of such subsets. We're done. In this case we need an upper bound on the number of generators a subgroup can have. I chose $b$ because it's a slightly worse bound than the one found here , but that doesn't depend on the structure of the group or its subgroups (since it's information we're not given). I'd like to know. Is there a better way? And... is there a better bound for the second method? I mean manually we can use a lot of the structure of the group so most of the time computing  all the subgroups is way easier, but if we want a computer to do it I think it may be hard to implement something like that. Edit: There are some ideas in the comments. Using Sylow (How?) Computing only the subsets which order divides the order of the group. The number of subsets you need to compute is $$\sum_{d\mid n} \binom{n}{d}$$ while the one in my second method is $$\sum_{i=1}^b \binom{n}{i}.$$ At least in the case of $n=24$ that method is not as good in the sense that you have to compute 3587174 subsets instead of 190050 with my method. Edit 2: Because the approach to the problem is different, I don't think this and this questions contain the answer to mine. My questions are 1. If there is a better method (this one is answered), and 2. How to improve the bound for my method (this one has not been answered). Edit 3: For a way to slightly improve the bound I used while still not depending on the structure of each group we just need to see that it always holds that $f_i\geq 1$ (Because of First Sylow theorem and that every $p$ -group has a cyclic subgroup of order $p$ ). So we can use $$b=\sum_{i=1}^r e_i$$ as a better bound. This is a huge improvement for the case of $n=24$ , since in this case $b=4$ and we only need to compute 12950 subsets.","['finite-groups', 'algorithms', 'group-theory', 'computational-complexity', 'computer-science']"
4454490,Continuity of polar decomposition,"This question is about a step in the proof of this answer , which is not directly clear to me. Consider the following scenario: $H$ is a Hilbert space, $A\in GL(H)$ a positive self-adjoint operator. We surround the spectrum $\sigma(A)$ by a contour $\gamma$ which does not intersect $]-\infty, 0]$ . This allows to use the holomorphic functional calculus to define $\sqrt{A}$ . I would like to see that $\sqrt{\cdot}$ is continuous at $A$ . My problem is the following: If $B$ is sufficiently close to $A$ wrt $\|\cdot\|_{op}$ , how do we see that $\sigma(B)$ is still enclosed by $\gamma$ ? If we know this, we get the desired result simply by continuity of parameter integrals. I can't quite put the argument together. Intuitively it should be right, as the spectrum is compact, $\gamma$ encloses an open set, $GL(H)\subseteq (\mathcal L(H), \|\cdot\|_{op})$ is open, so everything seems nice and fitting. However, I struggle with pinning down the exact argument. Any suggestions?","['continuity', 'spectral-theory', 'functional-analysis', 'functional-calculus']"
4454503,What is the probability of two people of the same race randomly sitting next to each other in a room?,"Imagine there are four people, two of them are white, and two of them are asian. There are 2 chairs on one side of a room and 2 on the other side. Everyone is randomly assigned a seat. What is the probability that two people of the same race are seated on the same side of the room? My intuition leads me to believe the probability is 1/3, since it can be thought of in terms of pulling colored balls from a bucket. There are two red balls and two blue balls. The color of the first ball doesn't matter, whichever color is chosen, there will be one left of the same color and two left of opposite color, hence 1/3 chance of choosing a ball of same color. Is my reasoning correct?","['discrete-mathematics', 'probability']"
4454528,"What is meant by ""the image of an interval is also an interval"" when this phrase is used to describe the intermediate value property?","Wikipedia says of the Intermediate Value Theorem that In mathematical analysis, the intermediate value theorem states that
if $f$ is a continuous function whose domain contains the interval $[a, b]$ , then it takes on any given value between $f(a)$ and $f(b)$ at some point within the interval. Of Darboux's Theorem it says It states that every function that results from the differentiation of
another function has the intermediate value property: the image of an
interval is also an interval. What is meant by ""the image of an interval is also an interval""?","['calculus', 'derivatives']"
4454565,Prove that it isn't possible to flatten a Teserract into 2-d space.,"Its certainly possible to flatten a Teserract into 3-d space (see: What does a flattened Teserract look like? ). But what about 2-d space? It doesn't seem possible without having some faces fall on top of each other. But can we prove this? Alternately, can someone draw out an unfolding with no overlapping faces and prove this conjecture wrong via counter example?","['solid-geometry', 'geometry']"
4454568,A curious limit: $\lim\limits_{n\to\infty}\sum\limits_{i=1}^{n}\left[\left(\frac{n}{n+1-i}\right)\right]^{a}f(i) = c\sum\limits_{i\geq 1}f(i)$,"I am trying to prove, for the general case whereby $\zeta(\cdot\,,\cdot)$ is the Hurwitz-Zeta function, and $a\in \mathbb{N}$ , that $$\mathcal{L} = \lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{a}\Big[\zeta(a,i)-\zeta(a,n+1)\Big] = 2\zeta(a-1),\quad (a>2)$$ Importantly, I was hoping to accomplish this without transitioning the limit of the sum to a Riemann integral. However, evaluating the limit is tricky because of the behavior of the factor $f_n=[n/(n+1-i)]$ since $i$ spans all the way to $n$ itself. I realized after a bit that $$\mathcal{L} = 2\zeta(a-1)=2\sum\limits_{i\geq 1}\sum\limits_{k\geq 0}\left(\frac{1}{k+i} \right)^a=2\sum\limits_{i\geq 1}\zeta(a,i)$$ where the summand corresponds to the main $i$ -dependent function in the original sum. Although I still don't know how to prove the limit rigorously, it made me wonder if it's true more generally. For example, let $g(a,i)=\exp(-ai)$ and $h(a,i)=(1/i!)^a$ then is it true that for some constant $c$ $$\lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{a}g(a,i)\stackrel{?}{=}c\sum\limits_{i\geq 1}g(a,i),\quad (a>0)$$ $$\lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{a}h(a,i)\stackrel{?}{=}c\sum\limits_{i\geq 1}h(a,i),\quad (a>0)$$ Numerical evaluations show that $c$ is most likely $1$ in both of these cases. Assuming $\lim\limits{n\to\infty} f_n \sim 1$ , then the limits with $g$ and $h$ make sense. However why doesn't the first limit I proposed follow the same convention and instead have $c=2$ ? It appears that for $0<k<a$ , factor $c$ is reduced back to $1$ (at least by numerical evaluations). $$\lim\limits_{n\to\infty^{+}}\sum\limits_{i=1}^{n}\left(\frac{n}{n+1-i}\right)^{k}\Big[\zeta(a,i)-\zeta(a,n+1)\Big] \stackrel{?}{=} \zeta(a-1),\quad (a>2)$$ I'm wondering how you guys would approach any of these limits, since I don't have a convincing proof for any of them. Potentially would also like to know how/why the exponent on $f_n$ is able to change things in the first limit example. Please share your thoughts!","['riemann-zeta', 'limits', 'summation', 'real-analysis']"
4454574,Examples of topological spaces satisfying certain properties,"I have learnt that if a regular, second countable space is normal(Theorem 32.1 of Munkres's Topology), and that a regular, second countable space is metrizable(Urysohn metrization theorem, Theorem 34.1). So, I want to analyize these four properties using a Venn diagram, and I would like to know if the diagram is correct, and examples for each part. Normality implies regularity, so there is no space that is normal but not regular. Also, metrizable space is always normal. I would like to know some examples each in 1, 2, 3, 4, 5 of the diagram. Here are some of my thoughts: $\mathbb{R}_l^2$ (Sorgenfrey plane) $\mathbb{R}_l$ (Sorgenfrey line) ?? A good space, such as $\mathbb{R}$ with Euclidean metric topology ??",['general-topology']
4454609,What is an instanton? (On a complex surface or a differentiable 4-manifold ),"The question is as in the title. I have browsed online (Wikipedia, etc) and while they do give me the definition, it gets a bit too much physics-y for me. Therefore I would appreciate it if someone would spoonfeed me the math a little. I am especially interested in the case of moduli of instantons on a compact complex surface. How does one construct such moduli spaces? How can these objects be used to differentiate between surfaces? I believe I have a decent enough background in algebraic/complex/differential geometry so a high-tech answer would be appreciated. Thank you!","['complex-geometry', 'algebraic-geometry', 'moduli-space', 'differential-geometry']"
4454615,How do I integrate $\int^{\infty}_0\frac{\sin(x)}{e^x+1}\text{ d}x$,"I need to find $$I=\int^{\infty}_0\frac{\sin(x)}{e^x+1}\text{ d}x$$ I have tried using Feynman's method by introducing $$I(t)=\int^{\infty}_0\frac{\sin(tx)}{e^x+1}\text{ d}x$$ but this has led me nowhere. I tried using parts to see if I can change this to anything useful but in both cases the result is a mess. I tried then substituting the exponential equivalent of $\sin(x)$ into the integral which at first seemed ot make progress but ultimately ended up in an integral involving the hypergeometric function (something I am not comfortable in using) so it would be a pain trying to find the definite integral: $$\int_0^{\infty}\frac{\sin(x)}{e^x+1}\text{ d}x=\frac{1}{2i}\int_0^{\infty}\frac{e^{ix}-e^{-ix}}{e^x+1}\text{ d}x$$ $$\implies u=e^x+1,\qquad\text{d}x=\frac{du}{u-1}$$ $$\implies\frac{1}{2i}\int^{\infty}_2\frac{(u-1)^{i-1}-(u-1)^{-i-1}}{u}\text{ d}u=\text{hypergeometric junk with bounds}$$ I also tried integrating about a rectangular contour with semi/quarter-circles cut out about the poles that appear on $\Im(z)$ every $2\pi$ as the rectangular contour goes to positive infinity on $\Re(z)$ but that did not really go well (I am also bad at complex analysis so rip) So I am completely out of ideas. Any help would be appreciated! Edit: I now see how you can use series to solve this but Iâ€™m wondering if you can use residue theorem or contours as well?","['integration', 'calculus', 'improper-integrals', 'trigonometric-integrals']"
4454630,Show that $ \binom{n}{i\;j\;k} \le \binom{n}{m\;m\;m} $,"Is it true that for integers $i+j+k= 3m = n$ where $i , j, k , m , n\ge 0$ the inequality holds ? $$
\binom{n}{i\;j\;k} \le \binom{n}{m\;m\;m}
$$ I tried to show $$
 \frac{n!}{m!m!m!} \Big/ \frac{n!}{i!j!k!} = \frac{i!j!k!}{m!m!m!} \ge 1 
$$ but I have no idea how to proceed , are there any related theorems  ?","['factorial', 'combinatorics', 'multinomial-coefficients', 'discrete-mathematics', 'inequality']"
4454700,Understanding random variables as functions,"First of all, I have read What is a function and I have understood it basically and it is clear to me that in order to caluclate statistics ""things"" have to be transformed or mapped to numbers. I have read that a random variable $X$ is (or can be thought) as a function. $X:\Omega\rightarrow\mathbb{R}$ and then $X(\omega) = ...$ Say we have a coin with $\Omega = \{H,T\}$ then we could do $X:\{H,T\} = \{0,1\}$ . My question is here about the meaning of $X$ or how to ""pronouce"" it. I would say $X$ is just a placeholder or short for ""map (or transform) the character ""H"" into 0 and ""T"" into 1.
Or if we wean to count the numbers of getting tails then X:{H,T} = if tails is facing upwards increase the counter by 1. And $X$ is just short for the if sentene. Is this right? Second, say I have a data set like this \begin{array}{|c|c|c|}
\hline
id& coin & value \\ \hline
 1& H & 0\\ \hline
 2& H & 0\\ \hline
 3& T & 1\\ \hline
\end{array} then ""coin"" is no random variable because it isn't a number and only ""value"" is. Is this true?","['notation', 'statistics', 'functions', 'random-variables']"
4454719,Understanding Legendre transform (and convex conjugate),"I am learning convex analysis on my own and I would appreciate some help. I know that convex conjugate is the generalization of the Legendre transform. I also know the formula known for Legendre transform (as stated on Wikipedia ). However, I am reading a paper on which I can not understand how Legendre transform is calculated. There is a section called Fact II.2 that claims the dual of the quantum relative entropy (it's Legendre transform) holds. Could you explain how these two expressions in Fact II.2 are equivalent? How can I calculate similar Legendre transforms? Edit: More specifically, I don't understand why the following statements are dual in the mentioned paper: $$ 
D(\rho\,\|\,\sigma) = \sup_{w \in P_{\geq}(A)}\Big\{\operatorname{tr} \rho \log w - \log\operatorname{tr}\exp(\log w + \log \sigma)\Big\}
$$ and $$
\log\operatorname{tr}\exp(H + \log \sigma) = \sup_{w \in S(A)}\Big\{\operatorname{tr} H w - D(w\,\|\,\sigma)\Big\}
$$ [55]: D. Sutter, M. Berta, and M. Tomamichel. Multivariate trace inequalities. Communications in
Mathematical Physics, 352(1):37â€“58, 2017. DOI: 10.1007/s00220-016-2778-5.","['legendre-transformation', 'convex-analysis', 'real-analysis']"
4454786,Can a manifold be reconstructed from its charts?,"I'm learning special relativity and I am having a confusion on this mathematical point. Whenever any sort of motion or non motion happens in the world, it can only be perceived by the scientist in a chart (inertial frame). But, we say there exists a common manifold which is being charted by different observers when doing observation. How can one from just seeing the charts with transition maps and metric (spacetime - metric) know the existence of a manifold? In a more simplistic sense, could I reconstruct how the earth sits in space just from seeing pages the world map (with marked distances)?","['special-relativity', 'mathematical-physics', 'differential-geometry']"
4454922,Why does $z$-axis have to point out of the page for a typical orientation of the $y$-axis and $x$-axis?,"In class, when I constructed my plot of a $3$ D solid, I thought that it was arbitrary which directions I chose as my positive $x$ -, $y$ -, and $z$ - axes. But when I got the paper back, I was told that the $z$ -axis must be the cross product of the $x$ - and $y$ axes (in that order of course, because cross product is not commutative). Why is this? What does it affect if this rule is not followed?","['multivariable-calculus', 'vector-analysis']"
4454939,Is there any hope to prove that $g(x)>-4$ if $f(x)<0$?,"I have these two functions for $x>0$ , $\beta>0$ and $\alpha$ (all reals) $$ f(x)= \frac{\alpha \; \sin (\beta \; x)}x+4    \cos (\beta\;  x) ,\qquad\qquad\qquad\qquad\qquad\qquad\\
g(x)=\frac{\alpha  \cos (\beta  \;x)}{\beta \; x^2}+\frac{\alpha  \cos (2 \beta \; x)}{\beta \; x^2}-\frac{\alpha\; \cot  (\frac{\beta \; x}{2})}x    -\frac{4\;\sin (2 \beta \; x)}{\beta\;x}   
$$ Numerically, I am sure that if $f(x)<0$ , then $g(x)>-4$ (an example is attached); any hints to prove this analytically?","['real-analysis', 'maxima-minima', 'multivariable-calculus', 'calculus', 'solution-verification']"
4454952,Simpler proof that $y^3[d^2y/dx^2]$ is a constant if $y^2=ax^2+bx+c$?,"here's my question
If $y^2=ax^2+bx+c$ then prove that $y^3[d^2y/dx^2]$ is a constant .
I have solved this using the conventional method, taking square root, differentiating w.r.t to x  and using chain and quotient rule
But can't think of some alternative smaller and more efficient method ??
Can you ? Also can someone suggest me any alternative to quotient rule
I need something more non conventional, time saving and vastly
Applicable method .ðŸ™‚","['alternative-proof', 'calculus', 'derivatives', 'chain-rule']"
4454968,When does a symmetric matrix admit a positive eigenvector (i.e. $v_i >0 \forall i$) for a positive eigenvalue?,"Let $S \in \mathbb{R}^{n \times n}$ be a symmetric matrix. I am trying to understand when such a matrix possesses a positive eigenvector for a positive eigenvalue, that is a $v \in \mathbb{R}^n, v_i >0 \forall i$ , $\lambda>0$ , such that $$Sv = \lambda v.$$ So, basically I am looking for a more general answer to this question . It is obvious that if we have one such eigenvector then by orthogonality the other eigenvectors have to have alternating signs. Also, one can see that in general $S$ should be similar to a matrix with constant row sum (see my question here ). But this holds in the general case and does not yet use anything about the symmetry of the matrix. Can we somehow make use of this special structure to find another necessary condition on $S$ , or can we maybe get some bounds on $\lambda$ ? Edit Of course one can get a very crude bound on the eigenvalues by the row sum of absolute values: Let $v$ be a positive eigenvector for eigenvalue $\lambda$ , $||v|| =1$ . Then $$\lambda v_i = \sum_j S_{ij} v_j \leq \sum_j |S_{ij}| v_j \leq \max_k v_k \sum_j |S_{ij}|$$ so $$ \lambda \leq \max_i \sum_j |S_{ij}|.$$ But this bound is rather useless, since for example $$S = \begin{pmatrix} n & -n+\epsilon \\ -n+\epsilon & n \end{pmatrix}$$ has eigenpair $\left( (1,1), \epsilon\right)$ , while the bound is $2n + \epsilon$ . Edit2 : After some more thought one can obtain a bound by the (signed) rowsums, which is better than the above:
Let $v$ be positive eigenvector for $\lambda$ such that $||v||_1 = \sum_i v_i = 1$ .
Then by summing up the equation above we get $$ \lambda \sum_i v_i = \sum_i (\sum_j S_{ij} v_j)  = \sum_i v_i (\sum_j S_{ij})$$ Now for a positive, unit $l_1$ -norm vector $v$ and another vector $w$ we have $\min_i w_i \leq v^Tw \leq \max_i w_i$ . Thus, with $w_i = (\sum_j S_{ij})$ we obtain $$ \min_i (\sum_j S_{ij}) \leq \lambda \leq \max_i (\sum_j S_{ij}),$$ which in our above example already determines $\epsilon$ .
(In general this shows that for constant row sum $c$ this is the only possible eigenvalue with a positive eigenvector. In fact, one can also say that in the other direction, there only is a positive  eigenvector  corresponding to the eigenvalue $\lambda =  \max_i (\sum_j S_{ij})$ if the row-sum is constant: Since we required $v_i>0$ , the greater/lesser equal above only allows for equality when the row sum is the same for all $i$ ) Another (obvious) condition that $S$ has to fulfill, which does not seem to follow from the above, is that in every row, there has to be at least one positive entry (else for that row $\sum_j S_{ij} v_j \leq 0$ .)","['matrices', 'linear-algebra', 'symmetric-matrices', 'eigenvalues-eigenvectors']"
4454990,"If $ax^2 + 2hxy + by^2 = 0$ (here $a, b, h$ are real constants), then find $\frac{dy}{dx}$.","Question: If $ax^2 + 2hxy + by^2 = 0$ (Where $a, b, h$ are real constants), then find $\dfrac{dy}{dx}$ .
Following choices are given:- $\dfrac yx$ $\dfrac xy$ $\dfrac {-y}x$ $\dfrac {-x}y$ My work: Differentiating the equation given, $$2ax + 2h \left[y + x \dfrac{dy}{dx} \right] + 2by \dfrac{dy}{dx} = 0 $$ $$\implies \dfrac{dy}{dx} = \dfrac{-(ax+ hy)}{(hx+ by)}$$ Although I obtained $\dfrac{dy}{dx}$ , but there is no such option given. I need to write the answer is terms of $x$ and $y$ only. I tried to find the value of $h$ from the given equation and substituted in the value of $\dfrac{dy}{dx}$ but that seems not working here. What would be the appropriate way to solve this question?","['calculus', 'derivatives']"
4455021,Is the function $x^2\sin(y/x^2)$ differentiable?,"Let $f:\mathbb{R}^2 \to \mathbb{R}$ defined by: $f(x,y) = \begin{cases} \left( x^2 \right) \sin \left(\frac{y}{x^2} \right) & \text { if } x \neq 0\\ 0 & \text{ if } x = 0 \end{cases}$ Is the function $f$ differentiable or not? I donâ€™t know whether my solution is right, so Iâ€™ll post it here. My solution: If $x \neq 0$ , than $f(x,y) = x^2  \sin(y/x^2)$ is differentiable as it is the product of differentiable functions. Now for $x=0$ , let's consider the limit: $$\begin{align*}
\lim_{h \to 0} \frac{f(h,y) - f(0,y)}{h} &= \lim_{h \to 0} h^2\sin(\frac{y}{h^2})\\
&= \lim_{h \to 0} h^2\sin(yh^{-2})\\
&= \lim_{h \to 0} h^{2} = 0 
\end{align*}$$ Therefore the function $f$ is differentiable. MY NEW SOLUTION :
I first calculated the partial derivatives with respect to x and y with limits at the point $(0,y)$ . $\frac{\partial f}{\partial x}(0,y)= \lim_{h \to 0} \frac{f(0+h,y)-f(0,y)}{h} = 0$ $\frac{\partial f}{\partial y}(0,y) = \lim_{k \to 0} \frac{f(0,y+k)-f(0,y)}{k} = \lim_{k \to 0} \frac{0-0}{k} = 0$ Consequently, since $f(x, 0) = 0$ and the only candidate for a derivative matrix is $f'(x, 0) = (0, 0)$ , we have: $E(h,k):= f(0+h,y+k) = f(h,y+k) = h^2 \sin(\frac{y+k}{h^2})$ Then: $0 \leq \frac{|E(h,k)|}{||(h,k)||} \leq \frac{h^2\sin(\frac{y+k}{h^2})}{\sqrt{h^2+k^2}} \leq |h| \cdot \frac{|h|}{\sqrt{h^2+k^2}} \leq |h|$ This converge to zero when $(h,k) \to 0$ , therefore $f$ is differentiable at the point $(0,y)$ .",['multivariable-calculus']
4455078,Is there a purely topological proof that a certain topological space derived from logical compactness is compact?,"Let $L$ be a first order language, and let $S_{L}=\{\sigma:\sigma\;\mbox{is an $L$-sentence}\}$ . Also, by logical compatness I mean the Compactness Theorem of first order logic. Compactness Theorem: Let $\Sigma$ be a set of $L$ -sentences. $\Sigma$ is satisfiable if and only if each finite subset of $\Sigma$ is satisfiable. Now, we may consider the set $X=\{M:M\;\mbox{is an $L$-structure}\}$ and define a topology on $X$ as follows: For each $L$ -sentence $\sigma$ define a basic open set $U_{\sigma}=\{M\in X:M\vDash\sigma\}$ . Open sets in $X$ are arbitrary unions and finite intersections of the basic open sets $U_{\sigma}$ . A few facts we observe are: $X$ does not seem to be Hausdorff (not even $T_{0}$ ), and the connected components of $X$ appear to be the equivalence classes of structures by the relation $\equiv$ of elementary equivalence of $L$ -structures . In fact, for each $L$ -structure $M$ we let $T_{M}=\{\sigma\in S_{L}:M\vDash\sigma\}$ , and consider the set $Y=\{T_{M}:M\;\mbox{is an $L$-structure}\}$ . We can define a topology on $Y$ by defining open sets in $Y$ to be arbitrary unions and finite intersections of basic open sets $V_{\sigma}$ defined for each $L$ -sentence $\sigma$ by $V_{\sigma}=\{T\in Y:\sigma\in T\}$ . With this topology $Y$ is totally separated, and hence totally disconnected and Hausdorff. In fact it seems to be homeomorphic to $X/\negthickspace\equiv$ in the quotient topology. I believe I can show that the Compactness Theorem is true if and only if $Y$ (resp. $X$ ) is a compact space. My question is: Is there a purely topological proof showing that $Y$ (resp $X$ ) is a compact topological space? Note: Where does this question come from? From working out the analogous situation for propositional logic (i.e. $L$ is a propositional language) and trying to work out the case for first order logic. For propositional logic the analogously obtained $X$ (space of $L$ -valuations) and $Y$ are really homeomorphic, and readily seen to be homeomorphic to $\{0,1\}^{L}$ with the product topology. Hence, the Tychonoff Theorem provides the answer to my question. I have not found such a simple path for the first order situation I have described. Perhaps I should define $X$ and $Y$ differently? Where I am going wrong? Thanks to the comments by Greg Nisbet and Chris Eagle that pointed out $X$ being a proper class. The $U_{\sigma}$ seem proper classes also. Hence, $X$ is given a topology in which the closed ""setsâ€ are proper classes (except for the empty set). Can we speak of $X$ being a compact class? It's quotient by $\equiv$ is indeed a set. Are there any conditions on $L$ or otherwise that would make $X$ a set? (Chris Eagle points to some in a comment below) Ok, $X$ is a proper class, it can be given a topology with the basic closed ""sets"" being the proper classes $U_{\sigma}$ . All is well since $U_{\sigma}\cup U_{\tau}=U_{\sigma\vee\tau}$ , and every closed subclass is of the form $$\displaystyle\bigcap_{\sigma\in S} U_{\sigma},$$ for some subset $S$ of $L$ -sentences. Now, is this topology compact? Well, in order for logical compactness to say that $X$ is compact we would need that $$\displaystyle\bigcap_{\sigma\in S} U_{\sigma}\neq\emptyset,$$ whenever $S\subseteq S_{L}$ , and that for every finite subset $S_0$ of $S$ $$\displaystyle\bigcap_{\sigma\in S_0} U_{\sigma}\neq\emptyset.$$ I think, one way to make sure $X$ is a set would be to define it to contain only the $L$ -structures $M$ satisfying the satisfiable sets of $L$ -sentences. That is, for every satisfiable set $S$ of $L$ -sentences let $M_{S}$ be an $L$ -structure such that $M_{S}\vDash S$ . Define $$X=\{M_{S}:S\;\mbox{is a satisf. set of $L$-sentences}\}$$ This question is related, and Noah Schweber provides a very useful answer there as well a here.","['general-topology', 'predicate-logic', 'logic', 'first-order-logic']"
4455093,Are invertibly cobordant manifolds diffeomorphic,"Let $M$ and $N$ be oriented, closed, $n-1$ manifolds and $F$ a cobordism from $M$ to $N$ and $G$ a cobordism from $N$ to $M$ such that the composite cobordism $G\circ F\cong M\times I$ and $F\circ G\cong N\times I$ . Does the existence of an ""inverse"" cobordism imply that $M$ is diffeomorphic to $N$ ? My first, naive strategy is to take the following composite: if $j: N\to F$ is the inclusion of $N$ into $F$ , $\psi: G\circ F\to M\times I$ the diffeomorphism, and $\pi: M\times I\to M$ the projection on the first factor, then the composite $\pi \circ \psi \circ j$ is a map from $N\to M$ . However, it is easy to foresee that this map may fail to be injective or surjective. Is there reason to believe this statement to be true?","['cobordism', 'differential-topology', 'differential-geometry']"
4455115,"""Spanning"" of solutions of ordinary differential equations","Suppose we have a switched ODE $$\dot{x} = A_{\sigma(t)}x,$$ where $A_{\sigma(t)}$ is a constant matrix given $\sigma(t)\in\mathcal{M}=\{1,2,\cdots,m\}$ .
If we fix the initial condition and can arbitrarily switch the signal $\sigma(t)$ , what would the reachable set of $x(t)$ look like? Intuitively, I think it is something like the spanning of the solutions of $\dot{x} = A_1x, \dot{x} = A_2x, \cdots,\dot{x} = A_mx$ . I wonder whether there are tools available for analyzing such a space.","['nonlinear-dynamics', 'control-theory', 'ordinary-differential-equations', 'dynamical-systems']"
4455146,Solution of $\theta$ when $\tan(\theta)-\sin(\theta)=\frac{\sqrt3}{2}$,"I came across this trigonometry problem. If, $$\tan(\theta)-\sin(\theta)=\frac{\sqrt3}{2}$$ What is the value of $\theta$ I got the solution that $\theta$ will be $\frac{\pi}{3}$ by expanding the equation and turning into, $$\sin^4(\theta)+\sqrt3\sin^3(\theta)+\frac{3}{4}\sin^2(\theta)-\sqrt3\sin(\theta)-\frac{3}{4}=0$$ And then solving $\sin(\theta)$ . But this method seems too complicated. Is there any easier and better solution?",['trigonometry']
4455147,"Prove $\sum_{i,j:i<j} Î¼(E_{i} \cap E_{j} )=\infty$.","The problem is: Assume that $(X,\mathcal{A} , Î¼)$ is a finite measure space (i.e., $Î¼(X) < \infty $ ) and the sequence of sets $E_{j}\in \mathcal{A}$ satisfies $\sum^\infty_{j=1} Î¼(E_{j} )=\infty$ . Show that then $\sum_{i,j:i<j} Î¼(E_{i} \cap E_{j} )=\infty$ . I tried to assume that $\sum_{i,j:i<j} Î¼(E_{i} \cap E_{j} )<\infty$ to get a contradiction, and I know that we can subtract since the measure is finite. I need help! I have no idea how to solve it. Thanks in advance.","['measure-theory', 'real-analysis']"
4455169,Collatz conjecture but with $n^2-1$ instead of $3n+1.$ Does the sequence starting with $13$ go to infinity?,"Let's consider the following variant of Collatz $(3n+1) : $ If $n$ is odd then $n \to n^2-1.$ $1\to 0.$ $3\to 8\to 1\to 0.$ $5\to 24\to 3\to 0.$ $7\to 48\to 3\to 0.$ $9\to 80\to 5\to 0.$ $11\to 120\to 15\to 224\to 7\to 0.$ $\color{red}{13\to 168\to 21\to 440\to 55\to 3024\to 189\to 35720\to 4465\to 19936224\to 623007\to\ldots\ ?}\ $ Does the sequence starting with $13$ go off to infinity? If yes, what
is a proof? If no, is there a  starting number whose sequence does go off to infinity, and
how do we prove either that such a number must exist, or even better that a specific starting number goes off to infinity? Here is some Python code I ran, which suggests that the numbers in the sequence starting at $13$ quickly become large: n=13
num_loops=0
print(n)
while n!=0:
    if n%2==0:
        n//=2
    else: n=n**2-1
    print('\n', n)
    num_loops+=1
    if num_loops==70:
        print(""too many loops"")
        break","['collatz-conjecture', 'number-theory', 'recreational-mathematics']"
4455187,Can we calculate the opponent's hidden values in this statistical battle?,"First-order statistical battle Imagine there is a game in which the user should guess what values the opponent is hiding from the user. In the first battle, the opponent has two hidden values a and b where a is the mean and b is the standard deviation (in a normal distribution). The rules of the game are: User knows how the generation system works on the opponent's side. User does not know what the a and b values are. User can ask the opponent to generate a new value using the values a and b and show it to the user. The user can ask for a new number infinitely. âœ… Solution In the first battle, the user wins. Because after generating so many numbers, the user eventually finds a and b or at least gets very close to them. Second-order statistical battle Imagine a new game where the opponent hides four static values a , b , c , and d . The same rules of the previous game apply to this game, but instead of only a and b , they apply to c and d as well. In this battle, every time the user asks for a new number, the opponent first generates a new tuple (X=mean, Y=stdDev) where X is generated using a and b ( a is mean and b is stddev) and Y is generated using c and d ( c is mean and d is the stddev). The opponent then generates a new number using (X, Y) and sends it to the user. Can the user find out what those four values are eventually? â“ Possible solution? A potential solution that one might propose is to generate K numbers (where K is large enough) and then calculate the mean and stddev of these K numbers, let it be (X1, Y1). Then repeat this step and generate another K number and calculate a new tuple (X2, Y2). We repeat this until we have so many tuples of Xn and Yn. Then we can calculate a and b using all the Xn, and c and d using all the Yn. Questions of post Does the proposed solution work? Can the user find a , b , c , and d using this method? If that method doesn't work, can the user figure out what those four values are using any other approach?",['statistics']
