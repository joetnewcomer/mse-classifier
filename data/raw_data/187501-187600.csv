question_id,title,body,tags
3486800,solve:$\int \frac{e^x+1}{e^x-1}dx$,solve: $$\int \frac{e^x+1}{e^x-1}dx$$ I tried $$\int \frac{e^x}{e^x+1}dx+\int   \frac{1}{e^x-1}dx$$ But I cant calculate second part,"['integration', 'calculus', 'closed-form']"
3486839,Text book Possible Error,"Paul's Online Notes vs. James Stewart Essential Calculus Early Transcendentals In the case of these two sources I am deeply lost whether the textbook is even right about this particular matter. Alternating Series Test The text book by James Stewart states on page 455, that: If the alternating series $$\sum_\limits{n=1}^\infty(-1)^{n-1} b_n = b_1-b_2+b_3-b_4+b_5-b_6+... b_n>0$$ satisfies (i) $b_{n+1} \le b_n$ for all n (ii) $\lim_\limits{n \to \infty}b_n=0$ Then the series is convergent. However, I started to do this problem: \begin{align}
\sum_\limits{n=1}^\infty (-1)^n \frac{n}{\sqrt{n^3+2}} &=-\frac{\sqrt{3}}3+\frac{\sqrt{10}}{5}-\frac{3\sqrt{29}}{29} && \mathbf{Given} \\
\mathbf{Condition \ 1 \ False \ (to \ my \ belief)}
\end{align} At right then, and there I thought game over the series is divergent; however, I ended up going to Symbolab, and Wolfram Alpha, and found out that by the Alternating Series Test this series is convergent. I did some digging, and found Paul's Online Notes and it looked at the series expanded more terms and found that the first condition was valid in the long run. Is my textbook wrong or is my understanding of it wrong? What is Wrong? I believe the first condition is wrong in this case since it is not valid for all n.","['reference-request', 'calculus', 'online-resources', 'convergence-divergence', 'sequences-and-series']"
3486847,Infinite sign switching 1/prime series,"Given the following series \begin{equation}
\sum _{p{\text{ prime}} \atop p{\text{ is the i'th prime}}}{ \frac {(-1)^i}{p}}
\end{equation} (sry maybe there is a better way to describe this series)  so the first n steps would look like this: \begin{equation}
\frac{1}{2} - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \frac{1}{11} - ...
\end{equation} My Questions: Does this series converge? It seems to me that it should but i would like to see a proof. Here is what the first 500 steps look like plotted: The number it seems to converges to (gained from a simulation) is round about 0.2696... Is there anything special about this number? Does it have some name? What is the exact number this series converges to (if it does), and if this is possible to say is this number irrational (I would guess yes)?","['number-theory', 'prime-numbers', 'sequences-and-series']"
3486854,Absolute convergence to a rational number,Let's recall the not so popular/familiar form of completeness of real numbers : Theorem : Absolute convergence of a series implies its convergence. Since $\mathbb{Q} $ is not complete there should exist a series $\sum_{n=1}^{\infty} u_n$ with rational terms such that $\sum_{n=1}^{\infty} |u_n|$ converges to a rational number and $\sum_{n=1}^{\infty}u_n$ converges to an irrational number. I could not think of an obvious example of such a series. Please provide one such example.,"['sequences-and-series', 'real-analysis']"
3486868,Operator commutes with translation is a convolution with a measure. Stein.,"Hi! following the thread of Bounded linear operators that commute with translation I can't prove that $\left\|m\right\|=\left\|T\right\|$ I have only one inequality: $\left\|T(f)\right\|_{L^1}\leq \left\|f\right\|_{L^1}\left\| \mu\right\|$ . Then, $\left\|T\right\|\leq \left\|\mu\right\|$ For the second, I thought of taking $f =\delta$ Dirac delta function which satisfies $\int_{\mathbb{R}^n}\delta(x)=1$ and $\delta\ast \mu=\mu$ to have to $\left\|T\right\|\geq \left\|T(\delta)\right\|_{L^1}=\left\|\delta\ast \mu\right\|_{L^1}=\left\|\mu\right\|_{L^1}$ then $\left\|T\right\|$ is a upper bound for $\left\|\mu\right\|_{L^1}$ then $\left\|\mu\right\|\leq \left\|T\right\|$ But I don't know if the above is legal. Actualization 1: Is the following valid?
In Bounded linear operators that commute with translation have this: $\lim_{k} \int f(y)T\phi_{\epsilon_k}(x-y)dy=\int f(y)\mu(x-y)dy$ with $x=2y$ $\lim_{k} \int f(y)T\phi_{\epsilon_k}(y)dy=\int f(y)\mu(y)dy$ then \begin{align*}
&\left\|\mu\right\|=\sup_{|f|_{L^1}\leq 1} \left|\int f(x)d\mu(x)\right|\\
&=\sup_{|f|_{L^1}\leq 1}\lim_{k} \left|\int f(x) T\phi_{\epsilon_k}(x)dx\right|\\
&\leq  \int|T\phi_{\epsilon_k}(x)|dx\\
&=|T\phi_{\epsilon_k}|_{L^1}\leq \left\|T\right\| |\phi_{\epsilon_k}|_{L^1}\\
&=\left\|T\right\|
\end{align*} Therefore $\left\|\mu\right\|\leq \left\|T\right\|$ . It is correct? This argument is based on the demonstration of the Grafakos Classical Fourier Analysis book.
Theorem 2.5.8. (characterization of multipliers for p = 1), equation 2.5.14 page 153 pd: $d\mu(x)=\mu(x)dx$ ? Actualization 2. I am trying to understand the demonstration of link Bounded linear operators that commute with translation . I have the following with some doubts: $L^1:=L^1(\mathbb{R}^n)$ Let $\phi\in L^1$ such that $\int_{R^n}\phi(x)dx=1$ and, for any $\epsilon>0$ ,\ $\phi_{\epsilon}:=\epsilon^{-n}\phi(x/\epsilon)$ . Is easy to see that $\|\phi_{\epsilon}\|_{1}=\|\phi\|_{1}$ lemma. Let $\phi\in L^1$ , $\|\phi\|_{1}=1$ . For any $f\in L^p,\ 1\leq p<\infty,\ 
\lim_{\epsilon\to 0}\|f\ast \phi_{\epsilon}-f\|_{p}=0$ . Observation: $\{\phi_{\epsilon}\}_{\epsilon>0}$ is an aproximation identity. By the lemma, $\lim_{\epsilon\to 0} \int_{R^n}f\ast \phi_{\epsilon}(x)dx=\lim_{R^n}f(x)dx$ . Now, as $\|\phi_{\epsilon}\|_{1}=1$ then $\|T\phi_{\epsilon}\|_{1}$ is bounded in $L^1$ because $T$ is bounded. Because $L^1$ is naturally embedded in the space of finite Borel measures,  wich is the dual of the space $C_{0}:=C_{0}(R^n)$ of continuous  functions that tend to zero at infinity, we obtain that the family $T\phi_{\epsilon}$ lies in a fixed multiple  of the unit ball of $(C_{0})^{\ast}$ . By the Banach-Alaoglu theorem, this is a $weak^{\ast}$ compact set. Therefore, some subsequence of $T\phi_{\epsilon}$ converges  in the $weak^{\ast}$ topology to a measure $\mu$ .
That is, for some $\epsilon_{k}\to 0$ , and all $g\in C_{0}$ , we have $$\lim_{k\to\infty}\int_{R^n}g(x)T\phi_{\epsilon_{k}}(x)dx=\int_{R^n}g(x)\mu(x)dx$$ (Here I used $d\mu(x)=\mu(x)dx$ that which I don't know if it's true ... Now, as $T$ is linear, commute with traslations, in addition  of the fact that $C_{0}$ is dense in $L^1$ and $T\phi_{\epsilon_{k}}\to \mu$ , $weak^{\ast}$ convergence, it has: \begin{align*}
f\ast \mu(x)&=\int_{\mathbb{R}^n}f(y)\mu(x-y)dy\\
&=\lim_{k\to\infty} \int_{\mathbb{R}^n}f(y)(T\phi_{\epsilon_{k}})(x-y)dy\\
&=\lim_{k\to\infty} \int_{\mathbb{R}^n}f(y)\tau_{y} (T\phi_{\epsilon_{k}}(x))dy\\
&=\lim_{k\to\infty} \int_{\mathbb{R}^n}f(y) T(\tau_{y}\phi_{\epsilon_{k}})(x)dy\\
&=\lim_{k\to\infty}\int_{\mathbb{R}^n}f(y)T(\phi_{\epsilon_{k}}(x-y))dy\\
&=\lim_{k\to\infty}T\left(\int_{\mathbb{R}^n}f(y)(\phi_{\epsilon_{k}}(x-y))dy\right)\\
&=Tf(x)
\end{align*} In the above, I do not know if the following is correct: $$\text{If } \lim_{k\to\infty}\int_{R^n}f(y)T\phi_{\epsilon_{k}}(y)dy=\int_{R^n}f(y)\mu(y)dy$$ for any $f\in L^1$ (because $C_0$ is dense in $L^1$ )
Implies $$\lim_{k\to\infty}\int_{R^n}f(y)T\phi_{\epsilon_{k}}(x-y)dy=\int_{R^n}f(y)\mu(x-y)dy ?$$ thanks","['measure-theory', 'operator-theory', 'fourier-analysis', 'functional-analysis']"
3486876,Tricky complex contour integration,"Show that $$\int_{0}^{2\pi} \frac{\cos^2{(\theta)}}{13-5\cos{(2\theta)}} \; d\theta = \frac{\pi}{10}$$ using complex contour integration. I let $z = e^{i\theta}$ which means $dz = ie^{i\theta} \; d\theta = iz \; d\theta$ (meaning we know integrate over the unit circle $C$ ). Hence, $$\int_{0}^{2\pi} \frac{\cos^2{(\theta)}}{13-5\cos{(2\theta)}} \; d\theta = \oint_{C} \frac{1/4\left(z+1/z\right)^2}{13-5/2\left(z^2+1/z^2\right)} \; \frac{dz}{iz} = -\frac{i}{2}\oint_{C}\frac{z(z^2+1)^2}{(z^3-5)(5z^3-1)} \; dz.$$ Not sure what to do next. Is it to find the poles that are in the unit circle? Thanks!!!",['complex-analysis']
3486888,"Is it true that if $\varepsilon > 0$ and $x \in int(A)$ then $\exists s > 0 \mid d(x,y) \ge \varepsilon + s,\;\forall y \not\in A^\varepsilon$?","Let $X=(X,d)$ be a metric space and $A$ be a nonempty subset of $X$ with interior $\overset{\circ}{A}$ , closure $\overline{A}$ , and boundary $\partial A$ . Given $\varepsilon > 0$ , define $\varepsilon$ -enlargement of $A$ by $A^\varepsilon := \{x \in X \mid d(x,A) \le \varepsilon\}$ , where $d(x,A) := \inf_{a \in A} d(x,a)$ . Note that $\overline{A} = \{x \in X \mid d(x,A) = 0\}$ . Observe that if $x \in \overline{A}$ and $y \in X\setminus A^\varepsilon$ , then $\varepsilon < d(y, A) \le d(x, y) + d(x,A) = d(x,y) + 0$ , i.e $$
d(x,y) > \varepsilon. \tag{1}
$$ One would hope that if $x$ is an interior point of $A$ , then the RHS of the bound (1) can be increased. Question. Is it true that if $x \in \overset{\circ}{A}$ there exists $\delta > 0$ such that $d(x,y) > \varepsilon + \delta$ for every $y \in X\setminus A^\varepsilon$ ? Affirmative answer for normed inner-product vector space Suppose $X=(X,d)$ is a normed inner-product vector space. Because $x \in \overset{\circ}{A}$ , there exists $\delta > 0$ such that $\{x' \in X \mid d(x',x) < \delta\} \subseteq A$ . It is clear that $d(x,\partial A) \ge \delta$ . Let $z$ be a point of intersection between $\overline{A}$ and the coord $[x,y]$ . Note that we must have $z \in \partial A$ and $d(z,y) > \varepsilon$ . By positive collinearity of $x-z$ and $z-y$ , we compute $$d(x,y) = d(y,z) + d(z,x) > \varepsilon + d(z,x) \ge \varepsilon + d(x,\partial A) \ge \varepsilon + \delta.
$$ Thus $d(x,y) > \varepsilon + \delta$ . This motivates the following relaxed question. Question 2. In case the above question can not be answered affirmatively in general, minimal assumptions can be made about $X$ to alleviate this ? Edit It has been observed in the comments that the question doesn't have an affirmative answer in general (example: $X = (-\infty,-1] \cup \{0\} \cup [1,+\infty)$ , $A=\{0\}$ , $x=0$ , $\varepsilon=1$ , and $y=-1$ ). I've noted that all counterexamples seem to be somewhat ""pathological"". So I'm wondering In case $(X,d)$ is a space with the ""mid-point property"", I've answered the question in the affirmative. See answer below. Such spaces include: normed vector spaces (already proved here), and general complete geodesic spaces . Question 3. Is there would some general condition (beyond the normed vector-space example) on the space $X$ which would ensure that the question is answered in the affirmative ?","['general-topology', 'metric-spaces']"
3486892,A function with a point of slope of exactly $2$,"Let $f:[0,1]\rightarrow\mathbb{R}$ be a function continuous on $[0,1]$ and differentiable on $(0,1)$ with $f(0)=f(1)$ and $f(\alpha)=f(\beta)+1$ for some $\alpha,\beta$ such that $0<\alpha<\beta<1$ . Prove that there exists some $\xi\in(0,1)$ such that $\lvert f^{\prime}(\xi)\rvert=2$ . I've come up with some approaches but none give me a complete solution: If $\beta-\alpha\leq\frac{1}{2}$ , then by the mean value theorem, there exists a point with a derivative less than or equal to $-2$ , and since $f(0)=f(1)$ by Rolle we also have a point with derivative $0$ so then by Darboux for any number $d$ in $[-2,0]$ there exists a point at which the derivative is exactly equal to $d$ . But I don't know how to deal with the case $\beta-\alpha>\frac{1}{2}$ . But also any other approach to finding a $c\in (0,1)$ for which $\lvert f^{\prime}(c)\rvert\geq 2$ would be sufficient for Darboux to finish the job... Let $g(x)=f(x)-2x$ be a function also defined on the segment $[0,1]$ then it is also continuous so by Weierstrass' extremum theorem it must reach its maximum value on its domain. And since $0=g(0)>g(1)=-2$ , the maximum isn't achieved at $1$ , so if I could also prove that it doesn't achive the max value at $0$ , then it would be that a maximum is achieved for some $x$ in $(0,1)$ , and from there by Fermat's theorem the derivative of $g$ should vanish, giving the desired claim. But again, I’m stuck, since I don’t know how to prove that the max isn't attained at $0$ , or equivalently that there exists $s\in(0,1)$ such that $g(s)>g(0)=0$ . I have been able to do the following: since $g(\alpha)-g(\beta)=1+2(\beta-\alpha)$ it must be that (a) $g(\alpha)\geq\frac{1}{2}+\beta-\alpha$ but then the right-hand side is strictly greater than zero which is sufficient for the claimed result; or (b) $-g(\beta)\geq\frac{1}{2}+\beta-\alpha$ with which I, once again, have no idea what to do with... I guess I could have taken $g(x)=f(x)+2x$ to get $f^{\prime}(\xi)=-2$ , since a point $\xi$ with slope $2$ will also give a point with slope $-2$ , because the function $f$ cannot be monotone because of $f(0)=f(1)$ . Right? I am sorry for such an elementary query, but I've tried this a couple of times and it just won't budge; I’m starting to develop serious self-esteem issues from this. I'm sure there's an elegant and easy solution and I just wish to see it so I can then deal with the frustration of not having come up with it myself. My deepest gratitudes to anyone willing to humour me with this problem.","['rolles-theorem', 'continuity', 'derivatives', 'real-analysis']"
3486894,Sum of squares of products of subsets without neighboring elements equals $(N+1)! -1$,"Question : Let $n$ be any natural number. Consider all nonempty subsets of the set $\{1,2,...,n\}$ , which do not contain any neighboring elements. Prove that the sum of the squares of the products of all numbers in these subsets is $$(n + 1)! - 1.$$ For example, if $n = 3$ , then such subsets of $\{1,2,3\}$ are $\{1\}$ , $\{2\}$ , $\{3\}$ , and $\{1,3\}$ , and $$1^2 + 2^2 + 3^2 + (1\cdot3)^2 = 23 = 4! -1.$$ This question can be proved by induction, as shown here: induction (sum of squares of products of elements of certain subsets of $\{1,\dots,n\}$) This seems to me like something that is really out of the blue. So my question is, is there another way to see why this is true ? Such as a combinatorial argument. In particular, does the quantity ""sum of squares of products of numbers in subsets without neighboring elements"" arise in some natural way?","['induction', 'combinatorics']"
3486896,"Besides jointly normal random variable, what other distribution satisfies uncorrelated if and only if independent?","It is well known that for a jointly normally distributed random variables $(X_1,...,X_n)^T,$ they are uncorrelated if and only if independent. It is also well-known that for any random variable, independent implies uncorrelated but not the converse. Here comes my question: Question: Besides jointly normal random variable, what other distribution satisfies uncorrelated if and only if independent?","['statistics', 'independence', 'normal-distribution', 'probability']"
3486914,Derivative of polynomial root function,"Find the derivative of $f(x)=-10\sqrt{x^{20}+9}$ with respect to $x$ I know to take the constant out and let $u=x^{20}+9$ \begin{align}
f'=&-10 \cfrac{df}{dx}(u)^{1/2} \hspace{2cm} (1)
\end{align} \begin{align}
f'=&-10 \cfrac{1}{2}(u)^{-1/2} \hspace{2cm} (2) \\ f'=& \cfrac{-10}{2\sqrt{u}} \hspace{3.75cm} (3) \\ f' =& \cfrac{-5}{\sqrt{x^{20}+9}} \hspace{2.8cm} (4)
\end{align} I know this is very wrong, but I don't understand why the correct answer is $-10 \cdot \cfrac{1}{2\sqrt{x^{20}+9}}\cdot 20x^{19}$ . I understand everything except the last term, $20x^{19}$ . I'm aware it is the derivative of $u$ , but I don't understand why we multiply by that to the numerator after having already taken the derivative of root $u$ as shown in line $2$ . can anyone explain why multiplying that term is necessary?","['calculus', 'derivatives']"
3486946,Subgroups of $GL_n$ containing upper triangular matrices,"EDIT: I rephrased the claim for clarity. Let $k$ be a field (that we may assume to be algebraically closed, but I don't think it is necessary). Let $n\geq 1$ and $T$ denote the subgroup of $GL_n$ consisting of invertible upper triangular matrices. Let $1\leq r \leq n$ and consider a sequence $a_1,\ldots,a_r$ of positive numbers such that $a_1+\ldots+a_r=n$ . Consider the subgroup $P_{(a_1,\ldots,a_r)}$ of $GL_n$ consisting exactly of all matrices $M$ of the form $$M = 
\begin{bmatrix}
    M_1 & * & \dots  & * \\
    0 & M_2 & \ddots  & \vdots \\
    \vdots & \ddots & \ddots & * \\
    0 & \dots & 0  & M_r
\end{bmatrix}$$ with $M_i\in GL_{a_i}(k)$ for all $i$ , and the $*$ 's being any elements of $k$ (or rather, any matrices with coefficients in $k$ and of appropriate dimensions). In other words, $P_{(a_1,\ldots,a_r)}$ consists of all invertible upper-triangular by blocks matrices with diagonal blocks being squares of dimensions $a_1,\ldots,a_r$ . The claim I am considering is the following: Any subgroup $P$ of $GL_n$ containing $T$ must have the form $P=P_{(a_1,\ldots,a_r)}$ for some $r$ and $a_1,\ldots,a_r$ . I suspect that this result may be true, however I can't find a way to prove it. In particular, given a group $P$ containing $T$ , I have trouble seeing how I could characterize $r$ and the (ordered !) sequence $a_1,\ldots,a_r$ solely in terms of $P$ . The motivation behind this lies in the theory of algebraic groups. We know that $T$ is a connected closed solvable subgroup of $GL_n$ . With the above result, I could deduce that $T$ is maximal with respect to such properties, because all subgroups described above with $r<n$ are unsolvable.","['algebraic-groups', 'linear-groups', 'matrices', 'group-theory', 'block-matrices']"
3486954,sample means with distributions that almost have a true mean,"The Cauchy distribution, $\frac 1 {\pi(1+x^2)}$ , has no mean. The distribution of the sample mean $\bar X$ is the original Cauchy distribution so you end up where you started, no matter how many samples you take. This is in contract to the ""usual"" behavior where standard deviation of the sample mean scales as $\frac 1 {\sqrt N}$ . But the Cauchy distribution is close . Changing the distribution to $\frac C {(1+|x|^{2+\epsilon})}$ for any small $\epsilon>0$ gives X a true mean of zero which $\bar X$ converges to. But what about a smaller change? $X $ ~ $ \frac C {1+x^{2}ln|x|}$ . For large x , the 1 on the denominator becomes irrelevant: $\int \frac {Cx} {1+x^{2}ln|x|} \approx \int \frac {Cx} {x^{2}ln|x|} = C ln(ln(x)) + (const),\ \ x >>0$ . This grows much more slowly than $ln(x)$ but it still diverges, and we have no mean. But will the sample mean converge for this modified $X$ ? Convergence in this case would mean that the sample mean gets within a given $\epsilon$ of zero with probability 1 as $N$ goes to infinity.","['statistics', 'convergence-divergence']"
3487122,Computation of the estimator of the ELBO in Variational Auto-encoder,"I am reading the paper below by Kingma et.al. https://arxiv.org/pdf/1906.02691.pdf Section 2.4.4 which is titled as ""Computation of $\log q_{\phi}(z|x)$ "": In Eqn. (2.33) the authors explain a relation between the densities of $\epsilon$ and $z$ as follows: \begin{equation}
\log q_{\phi}(z|x) = \log p(\epsilon) − \log d_{\phi}(x, \epsilon)
\end{equation} How is the second term of the RHS derived? Let me explain my question with more details. Applying Bayes rule and joint probability definition on $\log q_{\phi}(z|x)$ implies $\log \frac{q_{\phi}(z, x)}{q_{\phi}(x)} = \log \frac{q_{\phi}(x|z) q_{\phi}(z)}{q_{\phi}(x)}$ applying log yields $\log q_{\phi}(z|x) = \log q_{\phi}(x|z) + \log q_{\phi}(z) - \log q_{\phi}(x)$ Replacing $\log q_{\phi}(z)$ with $\log p(\epsilon)$ (because of reparametriztion) and rearranging leads to $\log q_{\phi}(z|x) = \log p(\epsilon) + \log q_{\phi}(x|z) - \log q_{\phi}(x)$ However, Eqn. 2.33 in the paper is: $\log q_{\phi}(z|x) = \log p(\epsilon) - \log d_{\phi}(x, \epsilon)$ . I know this is a variable change but I have two questions: 1)  Am I right with what I explained above? 2) I can't understand the sentence which is mentioned after it. That is, ""where the second term is the log of the absolute value of the determinant
of the Jacobian matrix $(∂z/∂\epsilon)$ ""","['machine-learning', 'statistical-inference', 'statistics']"
3487132,What is the minimal possible order of an $n$-universal group?,"Suppose $G$ is a finite group. We call $G$ $n$ -universal iff any group $H$ , such that $|H| \leq n$ is isomorphic to some subgroup of $G$ . Here are some examples of universal groups: $S_n$ is $n$ -universal. Proof of this fact (usually known as Cayley theorem) can be found in any group-theory textbook. Suppose $p$ is a prime and $n \in \mathbb{N}$ , that satisfies the conditions: $p^n+1$ is composite If $p = 2$ , then $n \geq 4$ Then $S_{p^n}$ is $p^n + 1$ universal That follows from Cayley theorem and the fact that all incompressible groups are $p$ -groups . If $G$ is $n$ -universal, then it is $(n-1)$ -universal. Trivially follows from the definition Let's define $UG(n)$ as the minimal possible order of an $n$ -universal group. Does there exist some explicit formula (or at least asymptotic) for $UG(n)$ ? I only managed to prove the three following facts: $UG(n)$ is monotonously non-decreasing Follows from the third example $UG(n) \leq n!$ Follows from Cayley theorem $UG(n) \geq e^{\psi(n)}$ , where $\psi$ stands for the Second Chebyshev function Follows from Lagrange theorem","['finite-groups', 'extremal-combinatorics', 'combinatorics', 'discrete-mathematics', 'group-theory']"
3487138,Distance formula for generalized knight movement on infinite chessboard from a corner,"Consider a chessboard infinite in positive x and y directions, all square has non-negative integer coordinates, and the only corner is at $(0,0)$ . A $(p,q)$ -knight is a piece that can move so that after each move one of the coordinate change by $p$ and the other change by $q$ (we will just call it a knight from now on). Set a knight at the corner $(0,0)$ , and assume that $(p,q)$ is such that every position on the board can be reached by the knight. For a position $(m,n)$ on the board, let $d(m,n)$ be the minimum number of moves needed for a knight from the corner to reach $(m,n)$ . Now the following claims are true: $\gcd(p,q)=1$ and $p,q$ are not both odd. This is necessary and sufficient conditions for every square to be reachable. Necessary is easily seen, for sufficient a sketch of the solution is in this question Can an $(a,b)$-knight reach every point on a chessboard? For every square on the board, every ways to reach it require the number to moves to have the same parity as $m+n$ , this is from black-white coloring. So $d(m,n)$ has the same parity as $m+n$ $d(m,n)\max(p,q)>=\max(m,n)$ , obviously. $d(m,n)(p+q)>=m+n$ So let's $B(m,n)$ be the smallest integer that satisfy all the constraints: $B(m,n)\max(p,q)>=\max(m,n)$ and $B(m,n)(p+q)>=m+n$ and $B(m,n)$ has the same parity as $m+n$ . Then we know that $d(m,n)>=B(m,n)$ for all $(m,n)$ . We make $B(m,n)$ the predicted value of $d(m,n)$ . DEFINITION: An ""awkward spot"" on the board is a position $(m,n)$ in which $d(m,n)$ is not equal to $B(m,n)$ . QUESTION: is it true that for all valid values of $(p,q)$ then the number of awkward spots are finite? Example: for the normal chess knight $(p,q)=(1,2)$ then you can check against this answer chess board knight distance (but need some small modification since we start from a corner) to see that the awkward spots are $(0,1),(1,0),(1,1),(2,2)$ so there are only a finite number of them. (I have heard suggestions to use Fourier transform but I have no clues what to do with it)","['elementary-number-theory', 'multivariable-calculus', 'combinatorics', 'chessboard', 'recreational-mathematics']"
3487143,"Show that $a^2+b^2+c^2$ is a square when $\frac{1}{a}+\frac{1}{b} = \frac{1}{c}$ and $a,b,c\in\mathbb{Q}$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Knowing that $$\dfrac1a+ \dfrac1b=\dfrac1c$$ Prove that $a^2+b^2+c^2$ is a square, where $a,b,c\not=0$ are rational numbers. It can probably be solved by a quick factoring trick, but I really can’t figure it out.","['contest-math', 'number-theory', 'elementary-number-theory', 'factoring', 'rational-numbers']"
3487144,Techniques on finding consecutive primes with large gaps,"I found two consecutive prime numbers $401!-3463$ and $401!+4021$ . These have a difference of $7664$ . Is there some kind of technique that is known in order to find consecutive prime numbers with sufficiently large gaps? I just used the fact that for all positive integers $n$ , it follows that all the integers on the interval $[n!+2, n!+n]$ are composite, which can make great prime gaps. Are there other more efficient techniques? Apologies if this question is too broad. I was just curious :)","['number-theory', 'prime-gaps', 'prime-numbers']"
3487170,Can any subset of $\mathbb{R}$ be generated from open intervals?,"Can any subset of $\mathbb{R}$ be generated by taking countable unions, countable intersections and complements of open intervals? Clearly, singletons can be generated from the complement of the union of half-rays, e.g.: $$a=((-\infty, a) \cup (a, \infty))^C .$$ Closed intervals can also be generated by a countable intersection of open sets of the form $(a- \frac1n, b+\frac1n)$ and similarly for half-open intervals. From this, it seems obvious that any countable union/intersection of intervals can be generated. How about for example uncountable unions/intersections of intervals?  It is unclear to me whether this is enough to generate all subsets of $\mathbb{R}$ . Can this be done?","['measure-theory', 'real-analysis']"
3487189,Visualizing Conditional Gaussian,I am looking at a graph that depicts a conditional Gaussian: I understand what the titled red spheres mean - that the variables are somewhat are correlated with each other. I don't understand the significance of the blue line. I know it's related to conditional gaussian but I can't quite make sense of it. Could someone explain what the blue line means?,"['statistics', 'bayesian', 'probability']"
3487202,prove that for every parallelogram,Prove that for every parallelogram $$d_1^2+d_2^2=2(a^2+b^2)$$ My attempt is: $d_1^2=a^2+b^2-2ab\cos\alpha$ $d_2^2=a^2+b^2-2ab\cos(180-\alpha)=a^2+b^2+2ab\cos\alpha$ It follows that: $$d_1^2+d_2^2=2(a^2+b^2)$$ but I don't know whether that is correct. Please help me.,['trigonometry']
3487210,"Given an urn with $40$ balls: $10$ white,$10$ black,$10$ red, $10$ green. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question We extract $2$ balls (simultaneously). What is the probability that at least one of them to be a white ball?","['probability-distributions', 'probability-theory', 'probability']"
3487217,Proof of irrationality of infinite continued fractions,We have the identity $$\tan x=\frac{x}{1+\underset{n=1}{\overset{\infty}{\mathrm K}} \frac{-x^2}{2n+1}}.$$ From the Wikipedia article on Proof that π is irrational : [...] Lambert proved that if x is non-zero and rational then this expression must be irrational. But how can we prove Lambert's assertion? I couldn't find any resource containing the proof.,"['analysis', 'calculus', 'pi', 'sequences-and-series', 'trigonometry']"
3487245,Find CDF of quotient of uniform RVs,"Let $U_1,U_2 = \operatorname{Uniform}[0,1]$ and independent. Find the CDF of $Z=\frac{U_1}{U_2+1}$ . My attempt: $P(Z\le z)=P(U_1\le (U_2+1)z, U_2+1> 0)+P(U_1\ge (U_2+1)z, U_2+1<0)$ . The last term will be equal to $0$ , since $U_2$ is never less than $-1$ . This results in: $$ F_Z(z)=\int_0^1du_2\int_0^{(u_2+1)z}du_1=z\int_0^1(u_2+1)du_2=\frac32z.$$ This holds only if $0\le (u_2+1)z\le 1$ and since $2\ge u_2+1>0$ , we get $0\le z\le \frac12.$ So, my answer would be $$ F_Z(z)=\begin{cases} 0 & \text{if }\quad z<0 \\ \frac32z&\text{if }\quad 0\le z\le\frac12\\1&\text{if }\quad z>\frac12\end{cases}.$$ However, in the solutions, there is an extra component: $2-\frac{1}{2z}-\frac{z}2$ if $\frac12\le z\le 1$ . I don't see where this one comes from. Thanks.","['statistics', 'solution-verification', 'probability']"
3487317,Hopf submersion,"Let's consider two maps: $H:\mathbb{S}^3\subset \mathbb{C}^2\to \mathbb{CP}^1$ with $H(z_0,z_1)=[z_0:z_1]$ and $h:\mathbb{S}^3\to \mathbb{S^2}$ with $h(x,y,z,t)=(x^2+y^2-z^2-t^2, 2(yz-xt), 2(xz+yt))$ . I have to prove that (1) $h$ and $H$ are smooth submersions. (2) $\mathbb{CP}^1$ is diffeomorphic to $\mathbb{S}^2$ . I have some problems solving both points because if I take some charts of $\mathbb{S}^2$ or $\mathbb{S}^3$ , then the calculations are extremely complicated. My attempt: (1) I can consider the natural extension of $h$ to $\mathbb{R}^4$ , $\widetilde{h}:\mathbb{R}^4\to \mathbb{R}^3$ , which has only one critical point 0. Then, since $h=\widetilde{h}\circ \iota\mid_{\mathbb{S}^3}$ , by chain rule we get, $(dh)_p=(d\widetilde{h})_p\circ (d\iota\mid_{\mathbb{S}^3})_p$ where $(d\widetilde{h})_p$ is surjective and $(d\iota\mid_{\mathbb{S}^3})_p$ . However, it's not trivial to conclude that $(df)_p$ is surjective. I don't know if it's the right way to do this. (2) Let's consider the map $\Phi: \mathbb{CP}^1\to \mathbb{S}^2$ such that $h=\Phi\circ H$ . $h$ is a surjective smooth submersion, then $\Phi$ is a surjective smooth submersion too. Moreover, since $\mathbb{CP}^1$ and $\mathbb{S}^2$ have the same dimension, i get that $\Phi$ is an immersion and therefore a local diffeomorphism. However, I still have to prove that $\Phi$ is injective. Any hint?","['hopf-fibration', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
3487376,When does multiplication have an inverse?,"Consider the function $m: \mathbb{N}^2 \to \mathbb{N}$ given by $m((a,b))=ab$ . I think the function has an inverse only when $a=b=ab=1$ . Am I right? Here's my reasoning. For the function to have an inverse somewhere, it must be injective there, i.e. only $1$ element in the preimage must target a given element in the image. i) For any $(a,b)$ s.t. $a \neq 1$ and $b \neq 1$ (i.e. $ab$ composite), we have $m((a,b))=m((1,ab))$ , so there are multiple elements in the preimage mapping to $ab$ . ii) For any $(a,b)$ s.t. $a \neq b$ , we have $m((a,b))=m((b,a))$ , so again there are multiple elements in the preimage mapping to $ab$ . The only $(a,b) \in \mathbb{N}^2$ satisfying both these requirements is $(1,1) \; \square$ . Merry Christmas.",['functions']
3487384,Solving Diophantine Equations Involving Primes,"Find all primes $p$ and $q$ such that $$p^3-q^7=p-q$$ The above equation can be written as $$p^3-p=q^7-q \implies p(p^2-1)=q(q^3-1)(q^3+1),$$ Now if $p$ divides $q$ then $p=q$ giving no solutions It is easy to see that $p>q$ On the other hand $p\mid q^3-1$ or $p\mid q^3+1$ , The former one gives $q^3=kp+1$ for some $k>0$ . So we get $$p(p^2-1)=q(kp+2)(kp) \implies p^2-1=k^2pq+2kq \implies p^2-k^2pq-1-2kq=0$$ which is a quadratic in $p$ so the discriminant $k^4q^2+8kq+4$ must be a perfect square but $$(k^2q)^2<k^4q^2+8kq+4<(k^2q+1)^2$$ for $k \geq 4$ so a contradiction. And if $p\mid q^3+1$ then $q^3=kp-1$ , so we get the equation $$p(p^2-1)=q(kp-2)(kp) \implies p^2-1=k^2qp-2kq \implies p^2-k^2qp+2kq-1=0.$$ This is a quadratic in $p$ , so the discriminant $k^4q^2-8kq+4$ must be a perfect square, but $$(k^2q-1)^2<k^4q^2-8kq+4<(k^2q)^2$$ for $k \geq 4$ , so a contradiction. So we cannot have $k \geq 4$ and putting these values of $k$ i.e. $ k \leq 3$ we can get the values of the primes $p$ and $q$ Is My Proof Correct???",['number-theory']
3487411,Question about Borel-Cantelli lemma,"$(X_n)$ is a sequence of $L^2$ random variables with $EX_n=0$ for all $n$ and suppose there is a constant $c$ s.t. $\operatorname{Var}(X_{n+k}−X_n)\leq ck$ , for all $n,k\geq0$ . Show that $X_n/n$ converges to $0$ a.s. (Hint: First prove along a suitable subsequence). I can see that we are trying to make the probabilities summable along a subsequence but may I know how to choose a subsequence so that the upper bound we use for variance makes the probabilities summable? $$P(|X_{n_k}|>nϵ)≤\operatorname{Var}(X_{n_k})/{n_k}^2ϵ^2\leq?$$","['probability-theory', 'real-analysis']"
3487435,Counting paths with reflection,"Fix two points $y=0$ and $x=(x_1,x_2).$ We assume without loss of generality that $x_1 ,x_2 \ge 0.$ Let $S=x_1+x_2.$ The number of paths on $\mathbb Z^2$ from $y=0$ to $x=(x_1,x_2)$ of length $n=2m+S$ is given as $$f_2(m) = n!\sum_{a_1 + a_2 = m}\frac{1}{a_1!a_2!(a_1 + x_1)!(a_2 + x_2)!} = \binom n m \sum_{a = 0}^m\binom m a \binom{m + S}{a + x_1},$$ see https://math.stackexchange.com/q/3476929 So to be precise, the paths are allowed to be self-intersecting (and you can go back and forth) and can move horizontically and vertically. Now, if along such a path we go in the $n$ -th step from one point $z_1$ to some point $z_2$ where $\Vert z_1-z_2 \Vert_{\infty}=1$ and then back to $z_1$ in the $n+1$ -st step, then I call this step a reflection . Let $f_2(m)$ be the total number of paths as above and let $f^{even}_2(m)$ be the number of paths with an even number of reflections and $f^{odd}_2(m)$ the number of paths with an odd number of reflections such that $$f_2(m)=f^{\text{even}}_2(m)+f^{\text{odd}}_2(m).$$ I would like to know whether we can explicitly bound $$\vert f^{\text{even}}_2(m)-f^{\text{odd}}_2(m) \vert? $$ My intuition is somehow that $\vert f^{\text{even}}_2(m)-f^{\text{odd}}_2(m) \vert$ should be significantly smaller than $f_2(m).$ Please let me know if you have any questions.","['analysis', 'real-analysis', 'calculus', 'combinatorics', 'discrete-mathematics']"
3487446,Circles and Tangents property,In the following figure $AB=5$ and $BC=4$ . I have to find radius of the sector. Somebody help me with this question,"['power-of-the-point', 'euclidean-geometry', 'geometry']"
3487451,Determining sample space and random variable for a construction of random maximal P-graph,"Let $P$ be a graph property that is preserved by the removal of edges. We give a construction of a Random maximal $P-$ graph, denoted $\textbf{M}_n(P)$ , as follows: Fix $V=\{1,2,...,n\}$ , $K_n=\text{complete graph on vertex set} \; V$ , $E^0=\text{initial edge set}=\emptyset.$ For each $i \ge 0$ , if $F^i=\{e \in K_n: \text{The graph} \;\langle V, E^i \cup \{e\} \rangle \; \text{satisfies} \; P\}\neq
 \emptyset$ , then $E^{i+1}=E^i \cup \{e\}$ where $e$ is chosen randomly
  and uniformly from $F^i$ . If $F^i=\emptyset$ , then $\textbf{M}_n(P)=\langle V,E^i\rangle,$ and the construction is complete. I am currently reading this paper (also cited below), in which the above construction appears. I want to understand the underlying sample space, probability measure, and the description of $\textbf{M}_n(P)$ , which I think, is a random variable. But if it is a random variable, then what is its sample space and what is its co-domain? Is the process of construction a Markov chain? I am confused here. I have never studied stochastic processes before, but when I came across the definition of the same, I felt that it might be the way here but I am not able to precisely write the things down. Erdős, Paul; Suen, Stephen; Winkler, Peter , On the size of a random maximal graph , Random Struct. Algorithms 6, No. 2-3, 309-318 (1995). ZBL0820.05054 .","['random-graphs', 'graph-theory', 'markov-chains', 'stochastic-processes', 'probability-theory']"
3487544,Can I solve this combinatorics problem using only high school math?,"My actual problem is way more complex than the example I'll give, but I'm not asking for a solution as much as I'm asking if it's plausible I can solve it on my own with only high school algebra. Say I blindly draw 5 slips of paper from a hat, each marked with a digit from 0 to 9.  I magically know that the first will be even, the second will also be even but smaller than 5 (so 0, 2, or 4), the third will be odd, the fourth will be any number, and the fifth will be any number smaller than the fourth. I need to assign probabilities per slip, for each number it can possibly be.  And I need to update these probabilities each time a number on a slip is revealed. That's the sort of problem I have, and I need to come up with a generalizable formula that I can use to just plug-in the known slips to get the probabilities. I am hoping the math just involves high school stuff, am I correct or do I need to get a mathematician to do this for me? I hope I'm correct that this problem is called combinatorics.","['conditional-probability', 'combinatorics']"
3487550,Existence of four dimensional subspace of $\mathbb{C}^3$,"Let $f:\mathbb{C}\rightarrow\mathbb{C}^3$ defined by $f=(e^{f_1},-e^{f_1},e^{f_3})$ where $f_1,f_3:\mathbb{C}\rightarrow\mathbb{C}$ are holomorphic. $\DeclareMathOperator{\Span}{Span}$ We identify $\mathbb{C}^3$ to $\mathbb{R}^6$ : ( $z_1,z_2,z_3)=(x_1,y_1,x_2,y_2,x_3,y_3$ ). Let $H_1$ , $H_2$ , $H_3$ and $H_4$ be four hyperplanes in $\mathbb{C}^3$ , defined by; $$\begin{array}{ccc}
&H_1=&\Span_\mathbb{R}\big[(1,0,0,0,0,0);(0,1,0,0,0,0)\big],\\
&H_2=&\Span_\mathbb{R}\big[(0,0,1,0,0,0);(0,0,0,1,0,0)\big],\\
&H_3=& \Span_\mathbb{R}\big[(0,0,0,0,1,0);(0,0,0,0,0,1)\big],\\
&H_4=&\Span_\mathbb{R}\big[(1,0,1,0,1,0);(0,1,0,1,0,1)\big].\\
\end{array}$$ $\textbf{Question}$ : Is there a real subspace, H, of real dimension four such that $\Span_{\mathbb{R}}(H_i,H_ j, H^\perp)= \mathbb{R}^6$ for all $i\neq j$ , $~~~~i,j \in \lbrace 1,2,3,4\rbrace,$ and such that $f$ avoid H? I tried the four dimensional subspace : $$\textbf{(H)} \  \left\{\begin{array}{ccllll}
X_1-X_2&=&0 & \\
 X_1-X_3&=&0&\\
\end{array}\right.
$$ $f$ avoid this subspace, but $H^\perp=\Span_\mathbb{R}\big[(-1,0,-1,0,0,0);(-1,0,0,0,-1,0)\big]$ does not satisfies the condition $\Span_\mathbb{R}(H_i,H_j,H^\perp)=\mathbb{R}^6$ for all $j\neq k,~~j,k\in\lbrace 1,...,4\rbrace$ \
I tried also the following four dimensional subspace: $$H=\left\{\begin{array}{cllll}
2y_1+x_2+y_2+y_3&=&0\\
~~\\
 x_1+2x_2+2x_3+y_2&=&0 \\
\end{array}\right.$$ Then $H^\perp=\Span_\mathbb{R}\big[(0,2,1,1,0,1);(1,0,2,1,2,0)\big]$ , which of course satisfies the condition $\Span_\mathbb{R}(H^\perp,H_j,H_k)=\mathbb{R}^6$ for all $j\neq k,~~j,k\in\lbrace 1,...,4\rbrace$ , but $f$ does not avoid This subspace. ask me please for more information.","['complex-analysis', 'complex-geometry', 'algebraic-geometry']"
3487571,"Let $f:[0,1] \rightarrow\mathbb R$ be a continuous map such that $f(0)=f(1)$","Let $f:[0,1] \rightarrow \mathbb R$ be a continuous map such that $f(0)=f(1) .$ Let $n \geq 2$ Show that there is some $x \in[0,1]$ such that $f(x)=f\left(x+\frac{1}{n}\right) .$ My attempt. Assume $f(x)\neq f(x+1/n)$ for all $x$ . Then either $f(x)<f(x+1/n)$ or $f(x)>f(x+1/n)$ . WLOG, assume $f(x)<f(x+1/n)$ . Then $f(0) So how can I get a contradiction? May you help?","['analysis', 'real-analysis']"
3487589,Numerical instability of an extended tetration,"For bases $a\in(1,e^{1/e})$ , ${}^na=a^{({}^{n-1}a)}=a^{a^{a^{.^{.^{.^a}}}}}$ converges to a value denoted as ${}^\infty a$ . By observing the convergence rate of this sequence, we can derive the limit: $$\lim_{n\to\infty}\frac{{}^\infty a-{}^{n+x}a}{{}^\infty a-{}^na}=[\ln({}^\infty a)]^x$$ By supposing we seek a continuous version of tetration that satisfies this, and rearranging so that ${}^xa$ is solved for, we derive: $${}^xa=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^x)\tag1$$ where $\log^{\circ n}$ is the logarithm applied $n$ times. As an example, with $n=10$ , I obtained the following plot: which looks really nice. Then looking at $n=15$ , I get this: which raises concern. For $a$ close to $1$ and $n=10$ , I get It would seem to work well for small $n$ and large $a$ , but then for larger $n$ or smaller $a$ , it becomes unstable. As far as I can tell, this issue is due to the amount of numerical precision required while evaluating $(1)$ , especially when the base is closer to $1$ . So the first question is whether this is due to numerical precision, or if it's simply because $(1)$ does not converge. If it's the former, then is there any way to circumvent this without brute forcing with more precision? And how should I pick the values of $n$ for a given base $a$ (and $x$ )? If it's the latter, then does it converge anywhere? Code for computing $(1)$ , showing the following for $a=\sqrt2$ and $x=1.5$ : n   1.4142135623730^^n
--------------------------
0   1.42291711861386
1   1.4657586018199498
2   1.4910645646490854
3   1.5069501895748705
4   1.5172760309843982
5   1.5241342747726574
6   1.528753204049527
7   1.5318927292918296
8   1.5340399138955585
9   1.5355145848360043
10  1.5365302824374432
...
45  1.538805432574356
46  1.5388054445894592
47  1.5388054519338499
48  1.5388054652284342
49  1.5388054823911386
50  1.538805506512146
...
90  1.7233534923554696
91  1.755592017472159
92  2.0000000000000004
93  2.000000000000001
94  2.000000000000001 Showing the apparent value of $^{1.5}\sqrt2\simeq1.5388$ followed by divergence.","['asymptotics', 'sequences-and-series', 'limits', 'convergence-divergence', 'tetration']"
3487605,Image by a translate/homeomorphism of a set E of Lebesgue measure 0 is disjoint with E,"This is a question on Walter Rudin's RCA (exercise 5.21): Suppose $E\subset\mathbb{R}$ is a measurable set with $m(E)=0$ . Must there be a translate $E+x$ of $E$ that does not intersect $E$ ? Must there be a homeomorphism $h:\mathbb{R}\rightarrow\mathbb{R}$ such that $h(E)$ does not intersect $E$ ? I am guessing the answer to the first one is no, because it being true would immediately answer the second one ( $t\mapsto t+x$ would be a valid homeomorphism). However I can't even start to prove neither of the questions. PS: this was asked before I know, but an example of $E$ for which a translate doesn't exist wasn't given, and also the existence of $h$ was proven using a little bit of category theory which I know nothing about.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3487616,"Solve $\int \frac{x^4+x^8}{(1-x^4)^{\frac{7}{2}}} \, dx$","Solve $\int \dfrac{x^4+x^8}{(1-x^4)^{\frac{7}{2}}} \, dx$ My attempt is as follows:- $$1-x^4=t$$ $$-4x^3=\dfrac{dt}{dx}$$ $$x^3dx=\dfrac{-dt}{4}$$ $$\frac{-1}{4}\cdot\int\dfrac{(1-t)^{\frac{1}{4}}(1-t)}{t^{\frac{7}{2}}}dt$$ $$\frac{-1}{4}\cdot\int\dfrac{(1-t)^\frac{5}{4}}{t^{\frac{7}{2}}}dt$$ $$\frac{-1}{4}\cdot\int\left(\dfrac{1}{t} -1\right)^{\frac{5}{4}}\cdot\left(\dfrac{1}{t} \right)^{\frac{9}{4}}dt$$ $$\frac{-1}{4}\cdot\int\left(\dfrac{1}{t} -1\right)^{\frac{5}{4}}\cdot\left(\dfrac{1}{t} \right)^2\cdot\left(\dfrac{1}{t}\right)^{\frac{1}{4}} \, dt$$ $$\frac{1}{t}-1=y$$ $$\frac{-1}{t^2}=\frac{dy}{dt}$$ $$\frac{dt}{t^2}=-dy$$ $$\frac{1}{4}\int y^\frac{5}{4}(y+1)^{\frac{1}{4}} \, dy$$ $$\frac{1}{4}\int y(y^2+y)^{\frac{1}{4}} \, dy$$ $$\frac{1}{8}\int (2y+1-1)(y^2+y)^{\frac{1}{4}} \, dy$$ $$\frac{1}{8}\left(\int (2y+1)(y^2+y)^{\frac{1}{4}} \, dy-\int (y^2+y)^{\frac{1}{4}} \, dy\right)$$ $$\frac{1}{8}\left(\int (2y+1)(y^2+y)^{\frac{1}{4}} \, dy - \int\left(\left(y+\dfrac{1}{2}\right)^2-\left(\frac{1}{2}\right)^2\right)^\frac{1}{4} \right)$$ How to proceed from here or feel free to suggest some shorter and clean way. Another way recommended $$I=I_1+I_2$$ $$I=\int \dfrac{x^4}{(1-x^4)^{\frac{7}{2}}}dx+\int \dfrac{x^8}{(1-x^4)^{\frac{7}{2}}}dx$$ First let's solve $I_2$ $$I_2=\int x^5\left(\dfrac{x^3}{(1-x^4)^{\frac{7}{2}}}\right)dx$$ Integrating by parts:- $$I_2=\dfrac{1}{10}\cdot\dfrac{x^5}{(1-x^4)^{\frac{5}{2}}}-\dfrac{1}{10}\cdot\int \dfrac{5x^4}{(1-x^4)^{\frac{5}{2}}}dx$$ $$I_2=\dfrac{1}{10}\cdot\dfrac{x^5}{(1-x^4)^{\frac{5}{2}}}-\dfrac{1}{10}\cdot\int \dfrac{5x^4(1-x^4)}{(1-x^4)^{\frac{7}{2}}}dx$$ $$I_2=\dfrac{1}{10}\cdot\dfrac{x^5}{(1-x^4)^{\frac{5}{2}}}-\dfrac{1}{10}\cdot\int \dfrac{5x^4(1-x^4)}{(1-x^4)^{\frac{7}{2}}}dx$$ $$I_2=\dfrac{1}{10}\cdot\dfrac{x^5}{(1-x^4)^{\frac{5}{2}}}-\dfrac{1}{2}\cdot\int \dfrac{x^4}{(1-x^4)^{\frac{7}{2}}}dx+\dfrac{I_2}{2}$$ $$\dfrac{I_2}{2}+\dfrac{I_1}{2}=\dfrac{1}{10}\cdot\dfrac{x^5}{(1-x^4)^{\frac{5}{2}}}$$ $$I_1+I_2=\dfrac{1}{5}\cdot\dfrac{x^5}{(1-x^4)^{\frac{5}{2}}}$$ $$I=\dfrac{1}{5}\cdot\dfrac{x^5}{(1-x^4)^{\frac{5}{2}}}$$","['integration', 'indefinite-integrals', 'calculus']"
3487678,"Finding angle $x$, a geometry question","Here is the question. Find the value of $x$ . I have solved this question by my own (with 3 different methods). However, all methods of mine are based on the construction of equilateral triangles.I am wondering whether there exist any other possible geometric construction because I don’t think equilateral triangle is the only way out but I couldn’t find any other possible construction apart from equilateral. I would like to understand more about the hidden geometric relation about all given informations so any helps are kindly appreciated. The answer for $x$ is $30^o$","['contest-math', 'euclidean-geometry', 'geometric-construction', 'geometry', 'geometric-transformation']"
3487697,Are there at least two people present who shook hands with exactly the same number of people? [duplicate],"This question already has an answer here : Prove that in every graph with at least two vertices, there are at least two vertices which have the same degree? [duplicate] (1 answer) Closed 4 years ago . The following is an interview question. You are invited to a welcome party with $25$ fellow team members. Each of the fellow members shakes hands with you to welcome you. Since a number of people in the room haven't met each other, there is a lot of random handshaking among others as well. If you don't know the total number of handshakes, can you say with certainty that there are at least two people present who shook hands with exactly the same number of people? I have a vague feeling that the answer is yes, which can be justified using the Pigeonhole principle. However, I do not know how to answer the question concre","['pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
3487706,Jointly Gaussian random vectors,"$\newcommand{cov}{\operatorname{cov}}$ Suppose two scalar valued random variables $X$ and $Y$ are jointly Gaussian. We then have the joint density $$f_{X, Y}(x, y) = \frac{1}{2 \pi \sqrt{|K|}} \exp \left\{ -\frac{1}{2} \begin{bmatrix}
x - \mu_X\\
y - \mu_Y
\end{bmatrix}^T K^{-1} \begin{bmatrix}
x - \mu_X\\
y - \mu_Y
\end{bmatrix}  \right\} $$ where $K = \begin{bmatrix}
\cov(X, X) & \cov(X, Y)\\
\cov(Y, X) & \cov(Y, Y)
\end{bmatrix}$ . Now how do we write the joint density when $X \in \mathbb{R}^m$ and $Y \in \mathbb{R}^n$ are random vectors. Does $K$ become a third order tensor? And what goes in the exponential power? I think $K \in \mathbb{R}^{m \times n}$ where $K_{ij} = \cov(X_i, Y_j)$ even though strict analogy with the scalar case would suggest something like $K \in \mathbb{R}^{2 \times m \times n}$","['random-matrices', 'probability-distributions', 'probability', 'random-variables']"
3487739,Is the regular conditional probability of $\mathbb P$ dominated by $\mathbb P$ almost surely?,"Let $(X, \mathcal X, \mathbb P)$ be a probability space, let $\mathcal G$ be a $\sigma$ -subfield, and let $\mathbb P^{\mathcal G}$ be a regular conditional probability for $\mathbb P$ given $\mathcal G$ . That is, for all $x \in X$ , $\mathbb P_x^{\mathcal E}$ is a probability measure, $x \mapsto \mathbb P_x(A)$ is $\mathcal G$ measurable for all $A \in \mathcal X$ , and $$\mathbb P(A \cap G) = \int_G \mathbb P^{\mathcal G}_x(A)\mathbb P(dx)$$ holds for all $A \in \mathcal X$ and $G \in \mathcal G$ . Is it true that $P_x^{\mathcal G} \ll \mathbb P$ for almost every $x$ ? I can confirm that this is true if $\mathcal X$ is separable, but I'm not sure it holds in general.","['conditional-expectation', 'measure-theory', 'absolute-continuity', 'probability-theory']"
3487740,Seeking solution for: $16^x+36^x=81^x$,Solve for x: $$16^x+36^x=81^x$$ This question was given by one of the student in the lesson. Can this question be solved? A possible approach is to take the log $$x\log(81)=\log(16^x+36^x)$$ It is impossible to isolate the $x$ on the RHS.,"['exponentiation', 'algebra-precalculus']"
3487745,Let $(x-1)^3$ divides $p(x)+1$ and $(x+1)^3$ divides $p(x)-1$. Find a polynomial $p(x)$ of degree 5. [duplicate],"This question already has answers here : Find a polynomial f(x) of degree 5 such that 2 properties hold. (3 answers) Closed 4 years ago . Let $(x-1)^3$ divides $p(x)+1$ and $(x+1)^3$ divides $p(x)-1$ . Find a polynomial $p(x)$ of degree 5. Here's what I have tried:— As $(x-1)^3$ divides $p(x)+1$ , $p(1)+1=0$ , $p(1)=-1$ $p(-1)=1$ Letting $p(x)=a_5x^{5} + a_4x^{4} + a_3x^{3} + a_2x^{2} + a_1x + a_0$ I don't know how to proceed any further. I think the divisibility rule is gonna be used somewhere.","['algebra-precalculus', 'polynomials']"
3487764,Relation between adding the areas of two circles and Pythagorean Theorem,"The other day, I was ordering pizza with someone, and I came across an interesting property. You see, we both wanted the same type of pizza, and each knew what size we wanted individually, so we decided to sum the areas of each of our desired pizzas and get one together that has an equal or greater area (so we got the amount of pizza each of us wanted). To find the radius of the pizza we want, R, given the radii of each pizza individually, $r_1$ and $r_2$ and the formula for the area of a circle, $A=\pi r^2$ : $$\pi (r_1^2) + \pi (r_2^2) = \pi (R^2)$$ Dividing by $\pi$ yields an all too familiar theorem from Algebra I. $$ r_1^2 + r_2^2 = R^2$$ My question is – how are these related? What does this relationship between adding the areas of two circles and the relationship between side lengths of a right triangle mean geometrically? It's entirely possible that this is a coincidence, but, I figured, maybe someone out there smarter than I can synthesize this correlation into something meaningful.","['algebra-precalculus', 'geometry']"
3487784,How to solve $xy'+x=y\ln(xy)$?,"i.e. $x\frac{\mathrm{d}y}{\mathrm{d}x}+x=y\ln(xy)$ .
The MATHEMATICA can't solve this problem. DSolve[x*y'[x]+x==y[x]*Log[y[x]*x],y[x],x] leads Solve::ifun: Inverse functions are being used by Solve, so some solutions may not be found; use Reduce for complete solution information.",['ordinary-differential-equations']
3487802,"Prove$\left(\bigcap_{i\in I}A_i\right)\cup\left(\bigcap_{j\in J}B_j\right)=\bigcap_{(i,j)\in I\times J}{(A_i\cup B_j)}$","If there is two family of sets $\left\{A_i:i\in I\right\}$ , $\left\{B_j:j\in J\right\}$ , Prove $$\left(\bigcap_{i\in I}A_i\right)\cup\left(\bigcap_{j\in J}B_j\right)=\bigcap_{(i,j)\in I\times J}{(A_i\cup B_j)}$$ My approach $$x\in\left(\bigcap_{i\in I}A_i\right)\cup\left(\bigcap_{j\in J}B_j\right)\\
\Leftrightarrow x\in \left(\bigcap_{i\in I}A_i\right)\lor x\in\left(\bigcap_{j\in J}B_j\right)\\\Leftrightarrow x\in\bigcap_{(i,j)\in I\times J}{(A_i\cup B_j)}$$ But, I am not sure I did it right between the second and the third expression. Please check if my approach is right, and if there is some ways to improve it.
And if there is other ways to prove this, show them too.",['elementary-set-theory']
3487814,Does $f{(x)} = a x^4 - x^3 + ax - a$ have exactly $1$ positive root given $a>0$?,"Let $$f{(x)} = a x^4 - x^3 + ax - a$$ where $a>0$ .
We know that it contains $4$ roots. Also, by the Descartes' rule of signs , we know that There are either $1$ or $3$ positive roots. There is exactly $1$ negative root. I observed from plotting the function that it may contain exactly one positive root. Therefore, can we prove/disprove that the function always contain a pair of conjugate complex roots? Equivalently, can we prove/disprove that it has exactly $1$ positive root?","['roots', 'complex-analysis', 'polynomials', 'algebra-precalculus', 'complex-numbers']"
3487826,Is there a cubic polynomial $f(x)$ with real coefficients such that $f$ is monotonic and $f(x)=f^{-1}(x)$ has more than $3$ real roots?,"Does there exist a cubic polynomial $f(x)$ with real coefficients such that $f$ is monotonic (when regarded as a function from $\mathbb{R}$ to $\mathbb{R}$ ), and such that the equation $f(x)=f^{-1}(x)$ has more than $3$ real roots? I couldn't find such an $f$ , but I couldn't prove that no such $f$ exists. It's clear that any such $f$ must be monotonically decreasing.","['algebra-precalculus', 'roots', 'polynomials']"
3487872,Prove that sinh(2x) = 2sinh(x)cosh(x),When trying to solve a textbook question I'm getting a wrong result but I'm pretty sure I'm not doing anything wrong. Can somebody spot my mistake? Question: Prove that $$sinh(2x) = 2sinh(x)cosh(x)$$ My (wrong) solution: $$sinh(x) = \frac{e^x - e^{-x}}{2}$$ $$sinh(2x) = \frac{e^{2x} - e^{-2x}}{2}$$ $$sinh(2x) = \frac{(e^{x} + e^{-x})(e^{x} - e^{-x})}{2}$$ $$sinh(2x) = cosh(x)sinh(x)$$ So I'm missing a multiplication by 2. What is my mistake?,"['hyperbolic-functions', 'proof-explanation', 'analysis', 'trigonometry', 'exponential-function']"
3487899,"Are ""locally continuous"" functions continuous?","Let $A$ and $B$ be topological spaces and $f : A \rightarrow B$ be a function. I call $f$ locally continuous if every $a \in A$ has an open neighborhood $U \subseteq A$ with the property that $f : U \rightarrow B$ is continuous. Obviously, if $f$ is continuous, then it is locally continuous: if $X \subseteq B$ is open, then $f^{-1}(X) \subseteq A$ is open, and so is $f^{-1}(X) \cap U$ for any open set $U \subseteq A$ . Is every locally continuous function also continuous?","['continuity', 'general-topology']"
3487953,What is this Lie group and does it have interesting properties?,"For a fixed positive diagonal matrix $D$ , the set of all real matrices $A$ satisfying $A^T = -D A D^{-1}$ form a Lie algebra with the matrix commutator as the Lie bracket. 
Since $$
\left( e^{A} \right)^T = e^{A^T} = e^{-D A D^{-1}} = D e^{-A} D^{-1},
$$ the Lie group consists of matrices $G=e^{A}$ satisfying $$
G^T = D G^{-1} D^{-1} \implies G = (D^{-1})^T (G^{-1})^T D^T
$$ Does this Lie group have a special name? Remarks:
1) The Lie algebra corresponds to ODE's whose trajectories are ellipsoids. Please see the post When do the solutions of a linear ODE system lie on ellipses? . 2) Note that in the special case of $D$ being the identity matrix, this Lie algebra is the skew-symmetric matrices and the corresponding Lie group is $SO(n)$ , the group of rotations.","['lie-algebras', 'lie-groups', 'ordinary-differential-equations']"
3487966,Finding the work of friction force using line integrals,"A man in standing on the edge of a flat surface, $H^2$ meters above the ground ( $H>0$ ), which is also the highest and starting point of a parabolic skiing course, given by the simple parametrization: $$\gamma(t)=(t,t^2)\\t\in[-H,h]$$ When $h>0$ . At some moment he begins to ski, reaching the height $h^2<H^2$ twice: once before reaching the minimum point of the course (which is at ground level), and once again afterwards. We will define the second point he reaches to be the endpoint of the course. It is given that the course is not smooth, and the man feels a friction force $\vec{F}$ along the course. At every point of the course, the force is parallel to the parabola, and its orientation is opposite to the direction of the man. The force at every point of the course is given by $|\vec{F}|=\mu|\vec{N}|$ , when $\mu\in\mathbb{R}$ is a given constant, and $\vec{N}$ is the normal force at that point. I am required to compute the work of the friction force along the course. I don't have a problem with the physics of the problem (I hope), but with the math. I'll explain: I know that at every point $(t,t^2)$ of the course, the normal force $\vec{N}$ is given by $|\vec{N}|=mg\cos\alpha$ , when $\alpha$ is the angle between the $\hat{x}$ axis and the tangent line to the parabola. Therefore, at every point of the course, $\tan\alpha$ is given by $\tan\alpha = \frac{d}{dt}t^2=2t$ . The direction of $\vec{F}$ would be, therefore, $-\cos\alpha\hat{x}-\sin\alpha\hat{y}$ . In conclusion, the friction force is given by: $$\vec{F}=-\mu mg\cos\alpha\left(\cos\alpha,\sin\alpha\right)=-\mu mg\left(\frac{1}{1+4t^2},\frac{2t}{1+4t^2}\right)$$ Now: $$W_F\equiv\int_\gamma\vec{F}\ d \vec{r}=-\mu mg\int_{-H}^{h}\left(\frac{1}{1+4t^2},\frac{2t}{1+4t^2}\right)\cdot(1,2t)\ dt=-\mu mg\int_{-H}^{h}dt=-\mu mg(h+H)$$ Something seems odd here - how come $W_F$ is given by the the product of the size of the force, with the horizontal length of the curve? This would seem legitimate to me if the course was horizontal, but it's not. What am I missing here? Is my math incorrect, or maybe is it my physics? Thank you!","['integration', 'physics', 'multivariable-calculus']"
3488005,Locally euclidean and first countability,"Suppose $X$ is a topological space that is locally euclidean of dimension some $n \in \Bbb{N}$ . Show that $X$ is first countable. My attempt: Let $p\in X$ and $U$ a neighborhood of $p$ . By assumption, there exists a neighborhood $U'$ of $p$ such that $U'$ is homeomorphic to a first countable space. Hence $U'$ is first countable.
Since $U\cap U' \subseteq U'$ . It follows that $U \cap U'$ is first countable. Hence each $p \in U\cap U'$ has a local basis, $\mathbb{B}_p$ . Since $U\cap U'$ is open in $U'$ and $U'$ is open in $X$ , each term in $\mathbb{B}_p$ is open in $X$ . Hence $\mathbb{B}_p$ the required local basis. Is my attempt correct? What would be a better proof?","['alternative-proof', 'solution-verification', 'manifolds', 'general-topology', 'first-countable']"
3488027,Solve $x^2(xdx+ydy)+2y(xdy-ydx)=0$,"Solve $x^2(xdx+ydy)+2y(xdy-ydx)=0$ My Attempts $$x^2(xdx+ydy)+2y(xdy-ydx)=0$$ $$\dfrac {xdx+ydy}{xdy-ydx}=-\dfrac {2y}{x}$$ Put $x=r\cos (\theta)$ and $y=r\sin (\theta)$ So, $r^2=x^2+y^2$ and $\tan (\theta)=\dfrac {y}{x}$ Now, $$x^2+y^2=r^2$$ Differentiating both sides, $$2xdx+2ydy=2rdr$$ $$xdx+ydy=rdr$$","['calculus', 'ordinary-differential-equations']"
3488059,Why is Fourier transform of sin(2t) not 2?,"I have been told that the Fourier transform outputs the frequencies of a given function. So I would have imagined to get $2$ from the function $\sin(2t)$ , for it is its frequency. But from calculation (online) i get a whole different answer. My professor told me that I will get 2 only for calculating $C_n$ , that are the Fourier coefficients for the Fourier series. So what DOES the Fourier transform output for a given function?","['trigonometry', 'fourier-analysis', 'fourier-transform']"
3488095,Question about the MISSISSIPPI problem,"Problem: Consider strings of digits and letters of length $7$ without repetition. Find the probability that a string contains $2$ digits, $4$ consonants, and $1$ vowel. Solution: For the favorable outcomes, pick $2$ positions in the string for
the digits, $4$ places for the consonants, leaving $1$ for the vowel. Then fill the
spots: $\displaystyle{\frac{\binom 72 \times \binom 54\times 10 \times 9 \times 21 \times 20 \times 19 \times 18 \times 5}{36 \times 35 \times 34 \times 33 \times 32 \times 31 \times 30}}$ My question: Counting the favorable outcomes part of the problem above looks like the MISSISSIPPI problem where the number of permutations is $\displaystyle{\binom{11}{4}\binom74\binom32\binom11}$ . But in the case of MISSISSIPPI problem, after choosing the places in a string for the letters, we do not additionally fill them in as in the problem above. What's different between these problems? Thanks.","['combinatorics', 'discrete-mathematics']"
3488096,How to evaluate $\mathbb{E}(|X_1-X_2|)?$,"I am trying to solve the question in this post using an alternative. For completeness, I will retype the question in my post. What is the average result of rolling two dice, and only taking the value of the higher dice roll? For example: I roll two dice and one comes up as a four and the other a six, the result would just be six. My attempt: Let $X_1,X_2$ be the score by dice $1$ and $2$ respectively. Since we have $$\max(X_1,X_2)=\frac{|X_1+X_2| + |X_1-X_2|}{2},$$ it follows that \begin{align*}
\mathbb{E}[\max(X_1,X_2)] & = \frac{1}{2}\left[\mathbb{E}(|X_1+X_2|) + \mathbb{E}(|X_1-X_2|)\right] \\
& = \frac{1}{2}\left[ \mathbb{E} (X_1) + \mathbb{E}(X_2) + \mathbb{E}(|X_1-X_2|) \right] \\
& = \frac{1}{2}\left[ 7 + \mathbb{E}(|X_1-X_2|) \right].
\end{align*} where I apply the fact that $X_1,X_2>0$ at second equality.
I got stuck at evaluating $$\mathbb{E}(|X_1-X_2|).$$ Any hint is appreciated. Just for record purpose, the answer is $$\mathbb{E}[\max(X_1,X_2)] = \frac{161}{36}.$$ This is an interview question. So, I expect that there is an easy way to calculate the expectation.","['expected-value', 'probability', 'random-variables']"
3488134,"Expected number of dice rolls before rolling ""1,2,3,4,5,6""","QUESTION: I roll a single six-sided die repeatedly, recording the outcomes in a string of digits. I stop as soon as the string contains "" $123456$ "". What is the expected length of the string? My answer so far: My initial approach is to try and find the probability mass function. If we let the random variable $X$ be the length of the string, then we can easily calculate for $x\in\{6,\ldots,11\}$ , $$\mathbb{P}(X=x) = \left(\frac{1}{6}\right)^6$$ and zero for $x<6$ . As soon as we reach $x\ge12$ , we need to consider the probability that the final six rolls are "" $123456$ "" but that sequence isn't contained in the string before that. I believe the result for $x\in\{12,\ldots,17\}$ becomes $$\mathbb{P}(X=x) = \left(\frac{1}{6}\right)^6 - \left(\frac{1}{6}\right)^{12}(x-11).$$ Now for $x\ge18$ , we will need an extra term to discount the cases when two instances of "" $123456$ "" are contained before the final six rolls. And indeed every time we reach another multiple of six, we need to consider the number of ways of having so many instances of the string before the final six rolls. I've messed around with this counting problem but I'm getting bogged down in the calculations. Any input is appreciated to help shed some light on this. Thanks!","['expected-value', 'dice', 'combinatorics', 'probability-theory', 'probability']"
3488138,"Evaluating $\int \sqrt{\frac{5-x}{x-2}}\,dx$ with two different methods and getting two different results [duplicate]","This question already has an answer here : Getting different answers when integrating using different techniques (1 answer) Closed 4 years ago . I tried Evaluating $\int \sqrt{\dfrac{5-x}{x-2}}dx$ using two different methods and got two different results. Getting two different answers when tried using two different methods:- M- $1$ : $$\int \dfrac{5-x}{\sqrt{\left(5-x\right)\left(x-2\right)}}dx$$ $$\dfrac{1}{2}\int\dfrac{-2x+7}{\sqrt{\left(5-x\right)\left(x-2\right)}}dx+\dfrac{3}{2}\int\dfrac{dx}{\sqrt{\left(5-x\right)\left(x-2\right)}}$$ $$\sqrt{\left(5-x\right)\left(x-2\right)}+\dfrac{3}{2}\int\dfrac{dx}{\sqrt{-x^2+7x-10}}$$ $$\sqrt{\left(5-x\right)\left(x-2\right)}+\dfrac{3}{2}\int\dfrac{dx}{\sqrt{\left(\dfrac{3}{2}\right)^2-\left(x-\dfrac{7}{2}\right)^2}}$$ $$\sqrt{\left(5-x\right)\left(x-2\right)}+\dfrac{3}{2}\sin^{-1}\dfrac{x-\dfrac{7}{2}}{\dfrac{3}{2}}$$ $$\sqrt{\left(5-x\right)\left(x-2\right)}+\dfrac{3}{2}\sin^{-1}\dfrac{2x-7}{3}$$ M- $2$ : $$x=5\sin^2\theta+2\cos^2\theta$$ $$dx=\left(10\sin\theta\cos\theta-4\cos\theta\sin\theta\right) \, d\theta$$ $$\int \sqrt{\dfrac{5\cos^2\theta-2\cos^2\theta}{5\sin^2\theta-2\sin^2\theta}}\cdot6\sin\theta\cos\theta \,d\theta$$ $$6\int \cos^2\theta \,d\theta$$ $$\int 3\left(1+\cos2\theta\right) \,d\theta$$ $$3\left(\theta+\dfrac{\sin2\theta}{2}\right)$$ $$3\theta+\dfrac{3}{2}\sin2\theta$$ $$x=5\sin^2\theta+2-2\sin^2\theta$$ $$\sin^{-1}\sqrt{\dfrac{x-2}{3}}=\theta$$ $$\cos2\theta=1-2\sin^2\theta$$ $$\cos2\theta=1-2\cdot\dfrac{x-2}{3}$$ $$\cos2\theta=\dfrac{7-2x}{3}$$ $$3\sin^{-1}\sqrt{\dfrac{x-2}{3}}+\dfrac{3}{2}\cdot\dfrac{\sqrt{9-(49+4x^2-28x)}}{3}$$ $$3\sin^{-1}\sqrt{\dfrac{x-2}{3}}+\sqrt{\left(5-x\right)\left(x-2\right)}$$ In first and second method I am getting the different results of $\dfrac{3}{2}\sin^{-1}\dfrac{2x-7}{3}$ and $3\sin^{-1}\sqrt{\dfrac{x-2}{3}}$ respectively. I checked that these are not inter-convertible. Why am I getting this difference?","['integration', 'calculus']"
3488175,Verifying tetration properties,"In my previous question I asked about the numerical instability and convergence of my tetration. It would seem to be the case that it converges, but suffers from catastrophic cancellation. The definition of my tetration is provided as: $${}^xa=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^x)$$ where ${}^na$ for natural $n$ is defined as the usual tetration, with ${}^\infty a$ as the limit of that, and $\log^{\circ n}$ being the $n$ times applied logarithm. We consider the above for $a\in(1,e^{1/e})$ and $x\in(-2,\infty)$ . I want to prove that it satisfies the basic tetration properties: ${}^0a=1$ ${}^{x+1}a=a^{({}^xa)}$ It is easy enough to verify the first one, as we have: \begin{align}{}^0a&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^0)\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na))\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^na)\\&=\lim_{n\to\infty}1\\&=1\end{align} I attempted to verify the second property: \begin{align}a^{({}^xa)}&=\lim_{n\to\infty}\log_a^{\circ(n-1)}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^x)\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^{n+1}a)[\ln({}^\infty a)]^x)\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1}+\mathcal O(({}^\infty a-{}^na)^2[\ln({}^\infty a)]^x))\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1})\tag{$\star$}\\&={}^{x+1}a\end{align} How can I justify $(\star)$ though? Update: Rough outline of possible proof? For $t\ge1,~a>1$ , and sufficiently small $\epsilon>0$ , we have \begin{align}\log_a(t+\epsilon)&=\log_a(t)+\log_a(1+\epsilon/t)\\&\le\log_a(t)+\frac\epsilon{t\ln(a)}\end{align} In this case, $t\ge{}^xa$ by monotonicity of the logarithm and the limit. We start with $\epsilon=q^n$ where $q=\ln^2({}^\infty a)$ and apply the above $n$ times to get: $$\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1}+\epsilon)\le\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1})+\left(\frac{\ln^2({}^\infty a)}{{}^xa\cdot\ln(a)}\right)^n$$ Seeing as we have $$\frac{\ln^2({}^\infty a)}{{}^xa\cdot\ln(a)}=\frac{{}^\infty a}{{}^xa}\cdot\ln({}^\infty a)$$ and $\ln({}^\infty a)<1$ , this should work for sufficiently large $x$ .","['limits', 'tetration', 'sequences-and-series']"
3488187,Expected number of collisions while distributing balls in boxes,"Suppose we have n balls $b_1, b_2,\cdots , b_n$ and n boxes. Each ball is placed into a box chosen independently and uniformly randomly. A colliding pair is defined as $(b_i,b_j)$ , where $i<j$ and $b_i$ and $b_j$ are placed in the same box. We are asked to evaluate the expected number of colliding pairs. What I did - Clearly, for any $k$ -set of balls in some box, there are $\binom{k}{2}$ colliding pairs in that box. Next if $C$ is the total number of colliding pairs after randomly placing all the balls in boxes, $E[C] = E[C_1+C_2+\cdots+C_n]=\displaystyle\sum_{i=1}^{n}E[C_i]=nE[C_k]$ where $C_k$ is the number of colliding pairs in box $k$ and $E[C_k] = E[C_1]=E[C_2]=\cdots$ . Now as each box will have $\binom{i}{2} = i(i-1)/2$ colliding pairs if the box contains $i$ balls, we can calculate the expected value as follows - $\begin{align}
nE[C_k] &= n\left(\displaystyle\sum_{i=2}^{n}\binom{i}{2}\text{Pr}\left[i(i-1)\text{ colliding pairs in box k}\right]\right) \\&= n\left(\displaystyle\sum_{i=2}^{n}\binom{n}{i}\dfrac{i(i-1)}{2}\left(\dfrac{1}{n}\right)^i\left(\dfrac{n-1}{n}\right)^{n-i}\right)\end{align}$ This can be calculated using various tricks like differentiating the $(1+x)^n$ binomial expansion and so on. But, the answer $E[C] = \dfrac{n-1}{2}$ . Given that the answer is so simple, is there a much simpler/slicker/quicker way to see it?","['expected-value', 'combinatorics', 'probability']"
3488195,The concept of nearness/farness in topology not induced by a metric.,"I'm trying to build an intuition/understanding of the notion of a topology. I've read the following thread on MO https://mathoverflow.net/questions/19152/why-is-a-topology-made-up-of-open-sets/19691#19691 and several others here, like: Intuition behind topological spaces and still could not convince myself that I've understood the notion properly. Things I think I've understood are: When we go 'top-down' and strip spaces off the angles, distances, norms, etc. What we left with is a Topological Space of a form (X, T) , which deals with a ground set X and some other explicitly chosen set T that we call Topology and which serves as a structure definition for X . It seems that, since there is no ""distance"" anymore it is left to explicitly define the relations between the elements of X, which is now the job of T. A case of topologies induced by a metric is rather a special case and not the general. Q: I am interested in the general case of topologies, not induced by a metric.
If say, I set T explicitly for my X like this: $$X = \{A,B,C,D,E\}$$ $$ T = \{X, \emptyset, \{A, B\},\{B, C\}, \{A,B,C\},\{B\}\}$$ Does T here somehow define, which elements of X are near to / far from every other element? Since there is no distance here defined, then my guess is that ""far/near"" will be a binary value, which will tell us which elements from X are connected to each other and which are not, making X look like a non-directed graph. Also, how exactly do we choose the rule by which we say that, e.g. A is near/connected to B or not? Is it fixed or freely defined? Furthermore, if above is the case, then isn't this ""rule"" also something like a metric? Could you please list these connections in the above X according to T? If I am not wrong, then they are all unconnected / far from each other.","['general-topology', 'intuition']"
3488267,Towards a Little proof of Fermat's last theorem,"A final version of this article was posted here , on 1/29/2020. Question: can you check if my reasoning below makes sense and has no major flaws? Update : I fixed an issue in my definition of $G$ : we must exclude $u=w$ and $v=w$ . This has impacts on the charts too, with the new definition of $G$ . I don't claim to have a proof here, just a potential path to a proof, and it is by no means elementary if one wants to make my arguments mathematically rigorous. It might look like what Fermat could have written when saying ""my proof is too long to fit in the margin of my letter"". Certainly, Fermat did not get a proof either. At best, I think you can (maybe) derive from my discussion below, that the number of solutions (if any) is bounded in certain ways -- a much weaker result than Andrew Wiles' final solution to this problem. But I don't think there are flaws in my reasoning, contrarily to most would-be ""simple proofs"" regularly published and based on high-school arithmetic, such as here . Hopefully, my perspective here brings some new light on this 300-old problem, and the methodology could be applied to other Diophantine equations. Anyway, here is how it goes. We are interested in solving $$u^n + v^n = w^n$$ where $u, v, w > 0$ are integers, and $n>2$ is an integer. We start with the following generating function: $$G_M(x) = \frac{1}{M^\alpha}\sum_{0<u,v,w\leq M, \\ u\neq w, v \neq w} x^{(u^n+v^n-w^n)^2}.$$ It is still unclear to me if $\alpha$ should be $0$ , I am still doing research on this.  This function has a Taylor series expansion $$G_M(x) = \sum_{k=0}^\infty h_k x^{k^2},$$ where $h_k$ is the number of ways (combinations of $u, v, w$ ) that $k$ can be written as $k=u^n + v^n - w^n$ . We all know that if $n>2$ , then $h_0 = 0$ regardless of $M$ (that's Fermat's Last Theorem.) If $n=3,\alpha=0$ and $M=100$ , then $h_1=4$ , as we have $(6^3 + 8^3 - 9^3)^2 = 1$ $(8^3 + 6^3 - 9^3)^2 = 1$ $(9^3 + 10^3 - 12^3)^2 = 1$ $(10^3 + 9^3 - 12^3)^2 = 1$ If $n=3,\alpha=0$ and $M=200$ , then $h_1=12$ : in addition to the four previous solutions, we also have $(64^3 + 94^3 - 103^3)^2 = 1$ $(94^3 + 64^3 - 103^3)^2 = 1$ $(71^3 + 138^3 - 144^3)^2 = 1$ $(138^3 + 71^3 - 144^3)^2 = 1$ $(73^3 + 144^3 - 150^3)^2 = 1$ $(144^3 + 73^3 - 150^3)^2 = 1$ $(138^3 + 175^3 - 172^3)^2 = 1$ $(175^3 + 138^3 - 172^3)^2 = 1$ If $h_1\rightarrow\infty$ as $M\rightarrow\infty$ and the growth follows a power law ( $h_1 \sim M^\alpha$ ), then we must have $\alpha\neq 0$ . Note that $h_2$ could follow a power low with a different $\alpha$ , this is a tricky problem. But at first glance, there seems to be enough smoothness in the way growth occurs among $h_0, h_1, h_2$ and so on, so that it is possible to find a suitable candidate for $\alpha$ . Indeed a simple rule consists in choosing $\alpha$ such that $G_M(\frac{1}{2}) = 1$ , always . Table for the coefficients $h_k$ Assuming $n=3, \alpha=0$ . The table reads as follows (example): $$G_{800}(x) = 24 x + 10x^4 + x^9 + 7 x^{36} + 4 x^{49}+30 x^{64}+\cdots$$ Main fact : There is no solution to $u^n+v^n=w^n$ (with $0<u,v,w\leq M$ ) if and only if $G_M(0) = 0$ . This result is trivial. Here $n$ is assumed to be fixed. Of course we are interested in $$G(x) = \lim_{M\rightarrow\infty} G_M(x), \mbox{ for } |x|<1.$$ First, note that the case $n=2$ leads to a singularity, and $G$ does not exist if $n=2$ , at least not with $\alpha=0$ (but maybe with $\alpha=1$ ). Also $n$ can be a real number, but it must be larger than $2$ . For instance, it seems that $n=2.5$ works, in the sense that it does not lead to a singularity for $G$ . Also, we are interested in $x$ close to zero, say $-0.5\leq x \leq 0.5$ . Finally, $G(x)$ is properly defined (to be proved, may not be easy!) if $|x|<1$ and $n>2$ . If $n$ is not an integer, there is no Taylor approximation for $G_M$ , as the successive powers in the Taylor expansion would be positive real numbers, but not integers (in that case it means $G_M(x)$ is defined only for $0\leq x <1$ .) Below is the plot for $G_M(x)$ with $-0.5<x<0.5, n = 3,\alpha=0$ and $M=200$ . Note that as $M\rightarrow\infty$ , the function $G_M$ tends to a straight line around $x=0$ , with $G(0)=0$ . This suggests that if there are solutions to $u^n + v^n = z^n$ , with $n=3$ , then the number of solutions must be $o(M)$ . The same is true if you plot the same chart for any $n>2$ . Of course, this assumes that $G$ does not have a singularity at $x=0$ . Also, if some $(u,v,w)$ is a solution, any multiple is also a solution: so the number of solutions should be at least $O(M)$ . This suggests that indeed, no solution exists. By contrast, the plot below corresponds to $n=2, \alpha=0, M = 200$ . Clearly, $G_M(0) > 0$ , proving that $u^2 + v^2 = w^2$ has many, many solutions, even for $0<u,v,w\leq 200$ . Below is the source code (Perl) used to compute $G_M$ . It is easy to implement it in a distributed environment. $M=200;
$ n=2;
$alpha=0;  

for ( $u=1; $ u<= $M; $ u++) {
  for ( $v=1; $ v<= $M; $ v++) {
    for ( $w=1; $ w<= $M; $ w++) {
      if (( $u != $ w) && ( $v != $ w)) { $z=($ u** $n+$ v** $n-$ w** $n)**2;
        $ hash{$z}++;
      }
    }
  }
}


open(OUT,"">fermat.txt"");
for ( $x=-0.5; $ x<=0.5; $x+=0.01) {
  $G=0;
  foreach $z (keys(%hash)) {
    if ($z<20) { $G+=$hash{$z}*($x**$z); }
  }
  $G=$G/($M**$alpha);
  print OUT ""$x\t$ G\n"";
}
close(OUT); This code is running very slowly because it generates a huge hash table. If we are only interested in the first few coefficients $h_k$ 's, then the following change in the triple loop significantly improves the speed of the calculations: for ( $u=1; $ u<= $M; $ u++) {
  for ( $v=1; $ v<= $M; $ v++) {
    for ( $w=1; $ w<= $M; $ w++) {
      if (( $u != $ w) && ( $v != $ w)) { $z=($ u** $n+$ v** $n-$ w** $n)**2;
        if ($ z < 2000) { $hash{$ z}++;
        }
      }
    }
  }
} Note: I did this work not because of my interest in Fermat's last theorem, but as I was exploring generating functions for sums of squares. The methodology is similar in both cases, though a little simpler for sums of squares.","['number-theory', 'generating-functions', 'diophantine-equations', 'real-analysis']"
3488273,"Compute $E\left[ \|U\|^2 \mid U+V \in S , V\in S \right]$ is $U,V$ are standard normal, $S=\{ x \in \mathbb{R}^k: x_1 \le x_2 \le ... \le x_k \}$","Let $U \in \mathbb{R}^k$ and $V\in \mathbb{R}^k$ be two independent standard normal vectors (i.e., $U \sim \mathcal{N}(0,I)$ and $U \sim \mathcal{N}(0,I)$ ).   Define a set $S$ as \begin{align} 
S=\{ x \in \mathbb{R}^k: x_1 \le x_2 \le x_3 \le ... \le x_k  \}
\end{align} We are interested in computing the following conditional expectation \begin{align}
E\left[ \|U\|^2 \mid   U+V \in  S , V\in S \right].
\end{align} My guess is that, most likely, there is no closed-form expression, so an upper bound would be also fine. One upper bound I that I tried is via Cauchy-Schwarz: \begin{align}
E\left[ \|U\|^2 \mid   U+V \in  S , V\in S \right]&= \frac{E\left[ \|U\|^2 1_{ \{  U+V \in  S , V\in S \}} \right] }{P [  U+V \in  S , V\in S ]}\\
&\le \frac{ \sqrt{E\left[ \|U\|^4 \right]} \sqrt{ P [  U+V \in  S , V\in S ]} }{P [  U+V \in  S , V\in S ]}\\
&= \frac{ \sqrt{E\left[ \|U\|^4 \right]}  }{\sqrt{ P [  U+V \in  S , V\in S ]}}.
\end{align} Now computing $E\left[ \|U\|^4 \right]$ is simple.  However, $P [  U+V \in  S , V\in S ]$ is not so much.   I tried using inclusion-exclusion principle \begin{align}
 P [  U+V \in  S , V\in S ]&=   P [  U+V \in  S  ]+  P [   V\in S ]- P [  U+V \in  S \text{ or } V\in S ]\\
&= \frac{2}{k!}-P [  U+V \in  S \text{ or } V\in S ]
\end{align} where we used that $P [  U+V \in  S  ]=  P [   V\in S ]=\frac{1}{k!}$","['conditional-expectation', 'probability']"
3488306,Circle geometry problem (find an angle in a figure),"I’m trying to do this problem, which is to find the shaded angle. I noticed that one of the triangles is isosceles, and so I could calculate the other two angles. And so using a result about alternate angles I think, I was able to calculate one of the angles of the triangle of interest. The solution claims to use the angles corresponding to the same segment are equal, but I’m not sure how to see this.","['circles', 'geometry']"
3488308,Can this set of subsets of $\mathbb{N}$ be uncountable? [duplicate],"This question already has an answer here : Finding an uncountable chain of subsets the integers (1 answer) Closed 4 years ago . Let $S$ be a collection of subsets of $\mathbb{N}$ such that for every $A, B ∈ S$ we have $A ⊂ B$ or $B ⊂ A$ . Can $S$ be uncountable? I think we don't have to be concerned about elements of $S$ which are finite subsets of $\mathbb{N}$ , as finite subsets can be listed by their size according to this property. But I am not sure how to treat infinite subsets. It would be much appreciated if comments and answers come in the form of hints rather than complete solutions as I think I'd benefit more that way.",['elementary-set-theory']
3488340,Prove this partition of the plane is not a foliation,"Let us define a partition of the plane as follows: for the points $(x_0,y_0)$ with $y_0\leq0$ we have leaves that are straight lines ( $y=y_0$ ) and for $y_0>0$ we have leaves $e^{x+\operatorname{ln}y_0-x_0}$ . This is indeed a partition. But I am being asked whether it is a foliation. A rank $k$ foliation is a collection $\{L_\alpha\}_{\alpha\in A}$ of leaves of a connected immersed submanifold such that it forms a partition of that manifold and for every point of the manifold there exists a chart $(U,\phi=(x_1,\ldots,x_n)$ such that $U\cap L_\alpha$ is a countable union of slices $\{x_{k+1}=\text{ constant},\ldots,x_n=\text{ constant}\}$ or empty. Now I do not believe the given partition is a foliation and that the problem lies on the $y=0$ line. When we take a chart around $(x_0,0)$ we have leaves which are lines and leaves which are exponential curves, which will contradict the continuity of $\phi$ the homeomorphism of the chart, I think. But I do not know how to rigorously prove this. edit: The Frobenius theorem tells us that there exists a bijection between foliations and involutive distributions which maps a foliation to a distribution $D$ , where $D_p := T_pL_\alpha$ for all $p\in \mathbb{R}^2$ , where $L_\alpha$ is the leaf through $p$ . 
A distribution on $\mathbb{R}^2$ is a subbundle of $T\mathbb{R}^2$ . Lemma 10.32 of Lee gives us a good criterium for subbundles. It says that in our case, we have a subbundle if and only if for every $p\in \mathbb{R}^2$ there exists a neighborhood $U$ on which there exists sections $\sigma_1,\ldots,\sigma_m: U\rightarrow T\mathbb{R}^2$ with the property that $\sigma_1(q),\ldots,\sigma_m(q)$ form a basis for $D_q$ at each $q\in U$ . The tangent space to a leaf at a point $(x,y)$ will be a straight line with slope $y$ if $y>0$ and slope $0$ if $y\leq 0$ . They are linear spaces of dimension $1$ . What we wanted to show is that for a neighborhood around $(0,0)$ (or some other point on $y=0$ ), there is no smooth section such that the above property holds. I believe a continuous section always exists, but I am not sure how to show whether it is smooth or not.","['foliations', 'differential-geometry']"
3488384,"Determine if $\frac{x^2y}{x^2+y^2}$ is differentiable at $(x,y)=(0,0)$","Let $k$ be a real number. Define $f\colon\Bbb{R}^2\to\Bbb{R}$ by $$f(x,y)=\begin{cases}\dfrac{x^ky}{x^2+y^2}&\text{if $(x,y)\neq(0,0)$},\\k-2&\text{if $(x,y)=(0,0)$}.\end{cases}$$ Find the value of $k$ such that $f$ is continuous at $(0,0)$ . For the value of $k$ found in part (1), determine whether $f$ is differentiable at $(0,0)$ . (Image that replaced text.) So, I have solved the part (1) and found that the $k=2$ .
Then both partial derivatives $f_x(0,0)$ and $f_y(0,0)$ of the function $f(x,y)$ equal $0$ . I know that if the partial derivatives exist and continuous at $(0,0)$ , then the function is differentiable at $(0,0)$ . However, I also know that we can check the differentiability using this formula: Definition. Let $f\colon X\to\Bbb{R}$ where $X\subset\Bbb{R}^2$ is open, and let $\mathbf{a}\in X$ . Suppose $f_x(\mathbf{a}),f_y(\mathbf{a})$ exist. We say that $f$ is differentiable at $\mathbf{a}$ if $$\lim_{\mathbf{x}\to\mathbf{a}}\frac{f(\mathbf{x})-[f(\mathbf{a})+f_x(\mathbf{a})(x_1-a_1)+f_y(\mathbf{a})(x_2-a_2)]}{\|\mathbf{x}-\mathbf{a}\|}=0.$$ (Image that replaced text.) And using, this formula I get that it is not continuous at $(0,0)$ (I have used $x=r\cos\alpha$ and $y = r\sin\alpha$ substitution to check it.) Can you please tell me which approach is right, and is the function differentiable at $(0,0)$ if $k=2$ ?","['partial-derivative', 'multivariable-calculus', 'derivatives']"
3488399,Is it true that $E[|X-E[X]|^j] \le E[|X|^j]$?,"Let $X$ be a random variable and let $j\in\mathbf{N}$ whith $j >2$ , is it true that $$E[|X-E[X]|^j] \leq E[|X|^j]\quad?$$","['expected-value', 'inequality', 'convex-analysis', 'probability']"
3488403,A quadrilateral inscribed in a rectangle,"Given a rectangle $ABCD$ in which there is an inscribed quadrilateral $XYZT$ , with exactly one vertex on each side of the rectangle, how could I prove that the perimeter of the inscribed quadrilateral is larger then $2|AC|$ (two diagonals)? I tried to use the triangle inequality, but I can't find the right way to do it.",['geometry']
3488411,What is the probability that you never lose this hypothetical dice game? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question You have a four sided die numbered 1-4 and are playing a game. On the first round, you roll the die once. If you get a 1 you lose the die. If you get a  2 you keep the die. If you get a 3 or 4 you get another, identical die. On the second round you roll each die that you have, and the same thing happens with each die. Once you finish rolling all your dice you move on to the third round and so on. If you have zero dice you lose the game. What is the probability that you never lose the game over infinite rounds?",['probability']
3488448,why do we assume this solution form in the method of variation of constants?,"In Variation of parameters for linear ODEs of second orders, we assume that the particular solution we are looking for is of the form $y_p=u_1y_1+u_2y_2$ , where $y_1$ and $y_2$ are two solutions for the homogeneous part of the ODE, and $u_1$ and $u_2$ are two functions to determine.  Why do we assume that $y_p$ must be of this form? I can guess that should be like this based on ODEs of first orders: If we are dealing with the equation $a_1(x)y'+a_2(x)y=f(x)$ and we know that $y_h$ is a solution of $a_1(x)y'+a_2(x)y=0$ , then $\frac{y_p}{y_h}$ cannot be a constant, otherwise, $y_p$ would be a solution of the homogeneous part, hence, $u(x)=\frac{y_p}{y_h}$ is a nonconstant function.   From this, we write $y_p=u(x)y_h$ and proceed as the method suggests (I am not finishing because I am assuming the reader is familiar with it). Based on this, I can guess that for the general case should be $y_p=u_1y_1+\cdots+u_ny_n$ , where $y_1,\dots , y_n$ are solutions for the homogeneous part and $u_1,\cdots, u_n$ to determine.  Whether or not this guess works is not convincing me. Any idea is welcome to understand this. Thanks",['ordinary-differential-equations']
3488464,How are restrictions introduced to a differential equation addressed when we have found the equation's solutions?,"This is how I was shown how to solve the following differential equation: $xy'=\sqrt{x^2-y^2}+y  \qquad \rightarrow \qquad y'=\sqrt{1-\frac{y^2}{x^2}}+\frac{y}{x} \qquad [1] $ $\\$ let $z=\frac{y}{x}\qquad \rightarrow y=zx \qquad \rightarrow \frac{dy}{dx}=z+x\frac{dz}{dx}=\sqrt{1-z^2}+z $ $\hspace{5.6cm}\rightarrow x\frac{dz}{dx}=\sqrt{1-z^2} $ $\hspace{5.6cm}\rightarrow \int \frac{1}{\sqrt{1-z^2}} dz=\int \frac{1}{x}dx \qquad$ provided $z^2\neq1 \hspace{0.7cm}and \hspace{0.6cm}x\neq0$ $\hspace{5.6cm}\rightarrow \arcsin(z)=\ln|x|+C$ $\hspace{5.6cm}\rightarrow \frac{y}{x}=\sin(\ln|Cx|)$ since $z=\pm 1 \hspace{0.5cm}$ was excluded from our workings we consider it as a possible solution and find out that it corresponds to a solution: $y=\pm x$ I was told that dividing by x at step [1] was fine as the equation would be ""rubbish"" otherwise. I understand all the workings that follows after [1]; however, my not understanding of [1] I feel like points to a misconception I have of differential equations. In the next few lines I'll write my understanding of step [1] and would appreciate it if you'd correct me anywhere I'm wrong: I think what my tutor meant when he said, the equation is useless when $x=0$ was when $x=0$ is being considered as a line and not as the coordinate of a point. It's reasonable to say that an expression containing $y'$ is useless when we're dealing with the line $x=0$ (or, as a matter of fact, any line of the form $x=a$ ). Granted, that way of thinking about $x=0$ would make the differential equation useless for $x=0$ . But, what about $x=0$ as the coordinate of a point? If it's just a point $y'$ no longer needs to be meaningless (as long as the point whose x coordinate is 0 is a point on a continuous curve). So, in other words, $x=0$ referring to a single point should be allowed and possibly correspond to points on the integral curves of the equation. The solution $y=x$ to the equation suggests that $x=0$ makes sense as a coordinate; but I don't understand why. Why are we including points with x-coordinates $x=0$ in a solution that is dependent on a step at which we divide by x? $x=0$ should completely be excluded from any solutions that include in their derivation the step [1]. So, the solutions should instead be rewritten as: $y=\pm x \quad$ where $x\neq 0 \quad$ and $ \quad \frac{y}{x}=\sin(\ln|Cx|) \quad$ where $\quad x \neq 0 \quad $ and $\quad C \neq 0$ This would then give rise to the issue that all points with an x-coordinate $x=0$ are being completely ignored; while, they have as much right to be a part of the solution as any other point does (the fact that they're being excluded is simply because the only method that has yielded solutions uses the step [1]). So, how do we show that the point (0,0) does, in fact, belong to the integral curves $y=\pm x$ ? I really hope that I've made at least some sense in the past few lines. I'd greatly appreciate it if you would correct my misconceptions about solving differential equations.",['ordinary-differential-equations']
3488480,"Are $(0,1) \times [0,1)$ and $[0,1) \times [0,1)$ homeomorphic?","I do not know how to approach. I tried removing points and see if the remaining spaces are connected or not, but i couldn't conclude anything without doubt.","['general-topology', 'product-space', 'connectedness']"
3488494,Does equidistribution imply convergence,"The following is an interesting problem presented to this site which has yet to bet solved : Does $$\sum_{n=1}^\infty \frac{\sin(n!)}{n}$$ converge. While attempting this problem, I thought that proving the equidistribution of $n!$ modulo $2\pi$ would be sufficient for the original conjecture. For those who do not know, an sequence $a_n$ is said to be equidistributed on a non-degenerate interval $[a,b]$ if $$\lim_{n\to \infty}\frac{|\{a_1,a_2,\cdots,a_n\}\cap [c,d]|}{n}=\frac{d-c}{b-a}$$ for all subintervals $[c,d]\subseteq [a,b]$ . My thoughts then turned to the more general question: If $a_n$ is any sequence of real numbers such that $\mod(a_n,2\pi)$ is equidistributed over $[0,2\pi]$ , does $$\sum_{n=1}^\infty \frac{\sin(a_n)}{n^\beta}$$ necessarily converge for $\beta>0$ . Obviously, if $\beta>1$ then the series converges absolutely, so the interesting cases are $0<\beta<1$ and $\beta=1$ (although they might be the same case overall). One possible way forward is using Weyl's criterion : we know that if $a_n$ is equidistributed over $[0,2\pi]$ , then $$\lim_{n\to\infty} \sum_{j=1}^n\frac{\sin(q a_j)}{n}=0$$ for all $q\in\mathbb{N}$ . I'm not sure how this could be useful but it seems pretty close to the original sum. One result in favor of this conjecture is discussed on this mathoverflow post. That is, if $p(n)$ is any polynomial with rational coefficients, then $$\sum_{n=1}^\infty \frac{\sin(p(n))}{n}$$ converges.","['ergodic-theory', 'equidistribution', 'sequences-and-series']"
3488512,"spectrum of $-\frac{d^2}{dx^2}$ with respect to $C[0,2\pi]$ with Dirichlet boundary conditions","Let $\mathcal{B}$ denote the Banach space of all continuous functions $f: [0,2\pi] \to \mathbb{C}$ such that $f(0) = f(2\pi) =0$ . Let $A$ denote the operator $$Af = -f'', \qquad \text{Dom}(A) = \{ f \in \mathcal{B} : f \in C^2[0,2\pi] \}.$$ I would like to find the spectrum of $A$ . Note that if $\sqrt{\lambda} \in \left\{\frac{n}{2} | n \in \mathbb{N} \right\}$ , then $$f(x) =  e^{i \sqrt{\lambda}x}- e^{-i \sqrt{\lambda}x} = i2\sin(\sqrt{\lambda}x) \in \text{Dom}(A)$$ is an eigenfunction of $A$ with eigenvalue $\lambda$ . Therefore the spectrum of $A$ contains $\left\{\frac{n^2}{4} | n \in \mathbb{N}\right\}.$ Showing that this is all the spectrum where I am stuck. If $\lambda \notin \left\{\frac{n^2}{4} | n \in \mathbb{N}\right\}$ , I need to conjecture a formula for the resolvent $u =(A - \lambda)^{-1}f$ , $f \in \mathcal{B}$ , and then show it is a bounded map, and that $u$ is smooth and obeys the boundary condition. My best guess is that the resolvent is described using Fourier series $$(A - \lambda)^{-1} f = \sum_{k = -\infty}^\infty \frac{e^{ikx}}{k^2 - \lambda}\hat{f}(k), \qquad \hat{f}(k) = \int^{2\pi}_0 e^{-ikt}f(t) dt,$$ or some variant thereof. Hints or solutions are greatly appreciated!","['spectral-theory', 'fourier-analysis', 'ordinary-differential-equations']"
3488533,Fundamental theorem of calculus with chain rule,"I'm trying to prove that if we define $$F(x) =\int_{g(x)}^{h(x)} f(t) dt$$ If $f$ is continous in $x_0$ , $g(x)$ , $h(x)$ are differentiable in $x_0$ , and the range of a neighbourhood of $h(x_0)$ and $g(x_0)$ is included in the domain of $f$ then $F'(x_0)=f(h(x_0))h'(x_0)-f(g(x_0))g'(x_0)$ I tried to divide the proof in $g(x_0)<h(x_0)$ , $h(x_0)<g(x_0)$ and $g(x_0)=h(x_0)$ When $g(x_0)<h(x_0)$ , what I did is define $$M(y) = \int _a^y f(t) dt$$ $$N(y) = \int_y^a f(t) dt$$ And I have proven that if $f$ is continous in $x_0$ : $$M'(x_0)=f(x_0)$$ $$N'(x_0)=-f(x_0)$$ As $F(x) = M(h(x)) + N(g(x)) $ when $g(x)<h(x)$ and this holds in a neighbourhood of $x_0$ , $(x_0-ε,x_0+ε)$ as $g$ and $h$ are continous in $x_0$ , it follows that $$F'(x_0)=f(h(x_0))h'(x_0)-f(g(x_0))g'(x_0)$$ Similarly we prove the case when $h(x_0)<g(x_0)$ , using that $$F(x) =\int_{g(x)}^{h(x)} f(t) dt =-\int_{h(x)}^{g(x)} f(t) dt$$ The problem comes when $g(x_0)=h(x_0)$ , using $$F(x_0)= \int_{g(x_0)}^{g(x_0)} f(t) dt = 0$$ as I don't know what happen in a neighbourhood of $x_0$ I tried to do $$F'(x_0)=\lim_{h\to 0}\frac{F(x_0+h)-F(x_0)}{h}=\lim_{h\to 0}\frac{F(x_0+h)}{h}$$ but I didn't get anything and I don't know what else should I do. Edit: I change the way to prove it and, with the same inicial conditions, I define $$M(y) = \int _a^y f(t) dt$$ We also have that if $f$ is continous in $x_0$ , then: $$M'(x_0)=f(x_0)$$ Now we have $F(x_0)=M(h(x_0))-M(g(x_0))$ and this holds for all 3 cases, so: $$F'(x_0)=f(h(x_0))h'(x_0)-f(g(x_0))g'(x_0)$$","['integration', 'calculus', 'derivatives', 'chain-rule']"
3488571,Birationally transforming general curve of genus 1 to Weierstrass form,"What are general rules to birationally transform general curve of any
  degree of genus 1 to Weierstrass form, provided we have one rational
  point? Example of curve of degree 12: $$x^9 y^3+9 x^9 y^2+27 x^9 y+27 x^9+9 x^8 y^3+81 x^8 y^2+243 x^8 y+243 x^8+35 x^7 y^3+318 x^7 y^2+963 x^7 y+972 x^7+74 x^6 y^3+687 x^6 y^2+2124 x^6 y+2187 x^6+90 x^5 y^3+871 x^5 y^2+2799 x^5 y+2988 x^5+67 x^4 y^3+692 x^4 y^2+2358 x^4 y+2655 x^4+39 x^3 y^3+415 x^3 y^2+1466 x^3 y+1717 x^3+21 x^2 y^3+211 x^2 y^2+723 x^2 y+840 x^2+4 x y^3+47 x y^2+180 x y+228 x-3 y^3-20 y^2-40 y-20=0$$ This curve has (geometric) genus 1. It has rational point $(-3, -\frac{17}{5})$ . I am familiar with transforming $y^2=a x^4+b x^3+c x^2+d x+e$ to Weierstrass form but have never seen similar process for curves of higher degree than $4$ .","['algebraic-geometry', 'elliptic-curves', 'birational-geometry']"
3488577,Diagonalizable random matrix,"Let $p_n$ the probability that a random matrix $M\in\mathcal{M}_n(\mathbb{R})$ such that its entries $(m_{i,j})_{1\leqslant i,j\leqslant n}$ are independant and following an uniform distribution over $[-1,1]$ , is diagonalizable. I was wondering  how to calculate $p_n$ and maybe how to find its limit or an equivalent. Diagonalization in $\mathbb{C}$ : I proved that $p_n=1$ for all $n\in\mathbb{N}$ if we talk about diagonalization in $\mathbb{C}$ : Let $$ \Phi_n : \left|\begin{aligned} &\ \ \ \ \ \ \ _ \ \ \ \ \mathbb{R}_{=n}[X] &\longrightarrow &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbb{C} \\ &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ P &\longmapsto &\prod_{1\leqslant i<j\leqslant n}\left(\lambda_i(P)-\lambda_j(P)\right)  \end{aligned}\right. $$ where $(\lambda_i(P))_{1\leqslant i\leqslant n}$ are the roots of $P$ ordered in the lexicographical order. For any $P\in\mathbb{R}_{=n}[X]$ , $\Phi_n(P)$ is, by a factor, the discriminant of $P$ and thus is a polynomial function of the coefficients of $P$ . Moreover, $\Phi_n(P)=0$ if and only if $P$ has a multiple root so that $$ p_n \geqslant \mathbb{P}(\Phi_n(\chi_M)\neq 0)=1-\mathbb{P}(\Phi_n(\chi_M)=0) $$ because $M$ is diagonalizable in $\mathbb{C}$ if $\chi_M$ has no multiple root. Moreover, if we denote $\lambda_n$ the Lebesgue measure on $\mathbb{R}^n$ , one can show that for any non-constant $P\in\mathbb{R}[X_1,\ldots,X_n]$ , if $$ \zeta(P) := \{ x\in\mathbb{R}^n\ |\ P(x)=0 \} $$ then $\lambda_n(\zeta(P))=0$ . We show it by induction on $n$ : if $n=1$ then $\zeta(P)$ is finite so that $\lambda_1(\zeta(P))=0$ . If $n\geqslant 2$ , we write $$ \zeta(P)=\bigcup_{t\in\mathbb{R}}\zeta(P(\cdot,t)) $$ where $P(\cdot,t):(x_1,\ldots,x_{n-1})\mapsto P(x_1,\ldots,x_{n-1},t)$ . By hypothesis $\lambda_{n-1}(\zeta(P(\cdot,t)))=0$ for all $t\in\mathbb{R}$ , thus using Fubini's theorem we have $$ \lambda_n(\zeta(P))=\int_{-\infty}^{+\infty}\lambda_{n-1}(\zeta(P(\cdot,t)))dt=0 $$ Finally, since $M\mapsto\Phi_n(\chi_M)$ is a polynomial function of the coefficients of $M$ (because $\Phi_n$ and $M\mapsto\chi_M$ are), the measure of the set $$ \{M\in\mathcal{M}_n(\mathbb{R})\ |\ \Phi_n(\chi_M)=0\} $$ is $0$ and $\mathbb{P}(\Phi_n(\chi_M)=0)=0$ and thus $p_n=1$ . Diagonalization in $\mathbb{R}$ : Because of what said above, for any $M\in\mathcal{M}_n(\mathbb{R})$ , $\chi_M$ has no multiple root almost surely so that $$ p_n=\mathbb{P}(\text{Sp}(M)\subset\mathbb{R}) $$ I believe that $\lim\limits_{n\rightarrow +\infty}p_n=0$ but I don't know how to prove it, and even less how to find an equivalent of $p_n$ . EDIT : While searching for papers I found out about the circular law. Notice that the entries of $M$ has zero mean and, since $A$ is diagonalizable if and only if $\lambda A$ is diagonalizable for all $\lambda\in\mathbb{R}$ , we can study the special case where the entries have a variance of $1$ (in our case we would study $\sqrt{\frac{3}{2}}A$ ). Let $\mu_n$ be the measure $$ \mu_n=\frac{1}{n}\sum_{k=1}^n \delta_{n^{-1/2}\lambda_k(M_n)} $$ with $M_n\in\mathcal{M}_n(\mathbb{R})$ a random matrix, $\lambda_k(M_n)$ its random eigenvalues and $\delta$ the Dirac measure. What is interesting is that $n\mu_n(\mathbb{R})$ is the number of real eigenvalues (counted with multiplicity) of $M_n$ so that $$ p_n=\mathbb{P}(\text{Sp}(M_n)\subset\mathbb{R})=\mathbb{P}(\mu_n(\mathbb{R})=1) $$ Since $\mu_n(\mathbb{R})\leqslant 1$ almost surely, we have using Markov's inequality $$ p_n=\mathbb{P}(\mu_n(\mathbb{R})\geqslant 1)\leqslant\mathbb{E}(\mu_n(\mathbb{R})) $$ If we prove that $\lim\limits_{n\rightarrow +\infty}\mu_n(\mathbb{R})=0$ almost surely, we can use the dominated convergence theorem with the domination $\mu_n(\mathbb{R})\leqslant 1$ almost surely to prove that $\lim\limits_{n\rightarrow +\infty}\mathbb{E}(\mu_n(\mathbb{R}))=0$ and this would finally show that $\lim\limits_{n\rightarrow +\infty}p_n=0$ . The circular law states that the sequence of measures $(\mu_n)_{n\in\mathbb{N}^*}$ converges in distribution to the uniform measure on the unit disk almost surely. This means that for all smooth function $f:\mathbb{C}\longrightarrow\mathbb{R}$ that has a compact support, we have $$ \lim\limits_{n\rightarrow +\infty}\int_{\mathbb{C}}f(z)d\mu_n(z)=\frac{1}{\pi}\int_{x^2+y^2\leqslant 1}f(x+iy)dxdy $$ Let $\varepsilon>0$ , $\beta>0$ and $f:\mathbb{C}\longrightarrow\mathbb{R}^+$ a smooth function such that $f(z)=1$ for all $z\in[-\beta,\beta]$ and $$\frac{1}{\pi}\int_{x^2+y^2\leqslant 1}f(x+iy)dxdy<\frac{\varepsilon}{2}$$ (such a function exists, $\varphi(x)=\mathbf{1}_{\{|x|\leqslant 1\}}+\left(1-e^{-\frac{x^2}{x^2-1}}\right)\mathbf{1}_{\{|x|>1\}}$ is a smooth function such that $\varphi(x)=1$ for all $x\in[-1,1]$ , we can use $f(x+iy)=\varphi(x/\beta)\varphi(y/\eta)$ with $\eta>0$ small enough). Thus $$ \limsup\limits_{n\rightarrow +\infty}\mu_n([-\beta,\beta])\leqslant\lim\limits_{n\rightarrow +\infty}\int_{\mathbb{C}}f(z)d\mu_n(z)=\frac{1}{\pi}\int_{x^2+y^2\leqslant 1}f(x+iy)dxdy<\frac{\varepsilon}{2} $$ Furthermore $$ \begin{aligned} \limsup\limits_{n\rightarrow +\infty}\mu_n(]-\infty,-\beta[\cup]\beta,+\infty[)&\leqslant\int_{-\infty}^{-\beta}\frac{t^2}{\beta^2}d\mu_n(t)+\int_{\beta}^{+\infty}\frac{t^2}{\beta^2}d\mu_n(t) \\
&\leqslant\frac{1}{\beta^2}\int_{\mathbb{C}}|z|^2 d\mu_n(z) \end{aligned}$$ However $$\sum_{k=1}^n{\lambda_k(M_n)^2}=\text{tr}({}^t M_n M_n)=\sum_{1\leqslant i,j\leqslant n}m_{i,j}^2$$ so that $$ \limsup\limits_{n\rightarrow+\infty}\int_{\mathbb{C}}|z|^2 d\mu_n(z)=\limsup\limits_{n\rightarrow +\infty}\frac{1}{n^2}\sum_{1\leqslant i,j\leqslant n}m_{i,j}^2\leqslant\mathbb{E}(m_{1,1}^2)=1 $$ almsot surely according to the law of large numbers. Thus there exists $C>0$ such that $$ \forall n\in\mathbb{N}^*,\int_{\mathbb{C}}|z|^2d\mu_n(z)\leqslant C $$ Finally $$ \limsup\limits_{n\rightarrow +\infty}\mu_n(]-\infty,-\beta[\cup]\beta,+\infty[)\leqslant\frac{C}{\beta^2} $$ and if we set $\beta=\sqrt{\frac{2C}{\varepsilon}}$ , we have $$ \limsup\limits_{n\rightarrow +\infty}\mu_n(\mathbb{R})=\limsup\limits_{n\rightarrow+\infty}\mu_n([-\beta,\beta])+\limsup\limits_{n\rightarrow +\infty}\mu_n(]-\infty,-\beta[\cup]\beta,+\infty[)<\varepsilon $$ Letting $\varepsilon\rightarrow 0$ gives $\limsup\limits_{n\rightarrow+\infty}\mu_n(\mathbb{R})=0$ almost surely and thus $\lim\limits_{n\rightarrow+\infty}p_n=0$ .","['diagonalization', 'linear-algebra', 'probability-theory']"
3488587,Alterative Sum of Squared Error formula proof,"The well-known formula of calculating Sum of Squared Error for a cluster is this: SSE formula where ""c"" is the mean and ""x"" is the value of an observation. But this formula also brings the same result: Alternative SSE formula where ""m"" is the number of the observations and ""y"" takes in every iteration, values of the observations. For example, if we have {3, 7, 8} , our mean ""c"" = 6 and: Using the usual formula: (6-3)² + (6-7)² + (6-8)² = 14 Using the alternative formula: [ 1∕(2*3) ] × [ (3-3)² + (3-7)² + (3-8)² + (7-3)² + (7-7)² + (7-8)² + (8-3)² + (8-7)² + (8-8)²] = 14 Starting from the first formula, I 'm trying to prove the alternative, but I 'm lost. Can someone help me with the proof?",['statistics']
3488590,"Finding the volume of the tetrahedron with vertices $(0,0,0)$, $(2,0,0)$, $(0,2,0)$, $(0,0,2)$. I get $8$; answer is $4/3$.","The following problem is from the 7th edition of the book ""Calculus and Analytic Geometry Part II"". It can be found in section 13.7. It is
problem number 5. Find the volume of the tetrahedron whose vertices are the given points: $$ ( 0, 0, 0 ), ( 2, 0, 0 ), ( 0, 2, 0 ), ( 0, 0, 2 ) $$ Answer: In this case, the tetrahedron is a parallelepiped object. If the bounds of such an object is given by the vectors $A$ , $B$ and $C$ then
the area of the object is $A \cdot (B \times C)$ . Let $V$ be the volume we are trying to find. \begin{align*}
x^2 &= 6 - y^2 - z^2 \\[4pt]
A &= ( 2, 0, 0) - (0,0,0) = ( 2, 0, 0) \\
B &= ( 0, 2, 0) - (0,0,0) = ( 0, 2, 0) \\
C &= ( 0, 0, 2) - (0,0,0) = ( 0, 0, 2) \\[4pt]
V &= \begin{vmatrix}
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3 \\
c_1 & c_2 & c_3 \\
\end{vmatrix} =
\begin{vmatrix}
2 & 0 &0 \\
0 & 2 & 0 \\
0 & 0 & 2\\
\end{vmatrix} \\
&= 2 \begin{vmatrix}
2 & 0 \\
0 & 2\\
\end{vmatrix} = 2(4 - 0) \\
&= 8
\end{align*} However, the book gets $\frac{4}{3}$ .","['solid-geometry', 'multivariable-calculus']"
3488593,Definition of signed measure,"I am currently reading Folland real analysis. For the definition of signed measure on p.85, I am confused about the third condition for signed measure. If { $E_j$ } is a sequence of disjoint sets in $M$ , then $v(\cup_1^\infty E_j)=\sum_1^\infty v(E_j)$ , where the latter sum converges absolutely if $v(\cup_1^\infty E_j)$ is finite. I understand the absolute convergence is for rearrangement, but why  we do not need to worry about the infinite case. For example, what if $v(\cup_1^\infty E_j)=\infty$ . I feel like rearrangement is still a problem in this case. Can anyone explain this? Thanks in advance.",['measure-theory']
3488615,integrating $\sin^2(x)$ and other even trig functions [duplicate],"This question already has answers here : Evaluating $\int P(\sin x, \cos x) \text{d}x$ (3 answers) Closed 4 years ago . How do you integrate even powers of sine and cosine? For example, how is $\int\sin^2(x)\ dx$ solved? What about $\int\cos^2(x)\ dx$ or $\int\sin^2(x)cos^2(x)\ dx$ or $\int\sin^2(x)cos^4(x)\ dx$ ? I know that when there is an odd function you can use the identity property - but here they are even.  A similar question would be solving tan/sec integrals when they are odd. I want to show my steps, but I am not even sure where to begin. The only thing I can say is that I saw mention of something called the power reduction formula when reading through other questions - but I never learned it.","['integration', 'calculus', 'trigonometry']"
3488632,$e$ showing up in expected value,"My question was inspired by this numberphile video on the maths of secret santa. So suppose you have a group of $n$ people who are all randomly choosing another person in the group at random. The probability the any given person chooses themselves is $p = 1/n$ and the expected value of $X$ (the number of people who choose themselves) is equal to $np = n \times 1/n = 1$ .
If someone(s) chooses themselves, then everyone has to choose another person at random again. Let's define the random variable $Y$ as the number of attempts the group will have to make until everyone chooses someone who is not themselves. My question is, find the expected value of $Y$ , $E(Y)$ . I didn't know how to compute this mathematically but when I ran a bunch of simulations I found that the answer rounded to $e$ ( $2.71828\ldots$ )! Can someone please explain why $e$ is showing up here.","['expected-value', 'probability', 'eulers-number-e']"
3488652,Optional stopping theorem question,"$X_n$ is an irreducible and aperiodic Markov chain with transition matrix $P$ . There is a function $f$ s.t. $Pf(x)\leq\alpha f(x)$ for $x\notin B$ , $B$ a finite set and $\alpha<1$ . $f(x)>M$ on $x\notin B$ . Show $$E_x(\tau_B)<\infty$$ I can show that $Y_n:= \alpha^{-n}f(X_n)$ is a super-martingale off of $B$ and let $\tau = \tau_B$ . So by optional stopping: $$\begin{align}
f(x)\geq E_x[Y_{n\wedge \tau}]&= E_x[f(X_{\tau\wedge n}\frac{1}{\alpha^{\tau\wedge n}})] \\
\implies f(x) &\geq \liminf_{n\to \infty} E_x[f(X_{\tau\wedge n}\frac{1}{\alpha^{\tau\wedge n}})] \\
&\geq E_x[\liminf_{n\to \infty} f(X_{\tau\wedge n}\frac{1}{\alpha^{\tau\wedge n}})] \text{ By Fatou's lemma} \\
&\geq E_x M(1/\alpha)^\infty.1_{\tau=\infty} + E_x (1/\alpha)^\tau f(X_\tau) .1_{\tau<\infty}
\end{align}
$$ Since $\alpha<1$ , I can see that $P(\tau = \infty)= 0$ but not sure how to go from $\infty>f(x)>E_x [(1/\alpha)^\tau f(X_\tau) .1_{\tau<\infty}]$ to saying $E_x(\tau)<\infty$ ?","['martingales', 'probability-theory']"
3488662,Prove there are $75$ ways to put five different fish in three buckets s.t. two buckets are identical and no bucket is empty,"Let $\{\}_1, \{\}_2, \{\}_3$ stand for three distinct buckets. The number of onto functions is given by $n!S(k, n)$ where $S(k, n)$ is a Stirling Number of the Second Kind. So, $3!S(5, 3)$ is a number of ways to distribute five distinct fish into three different buckets with none empty. But this number counts situations like $\{\text{bass}\}_1, \{\text{trout, salmon}\}_2, \{\text{sardine, herring}\}_3$ and $\{\text{trout, salmon}\}_1, \{\text{bass}\}_2, \{\text{sardine, herring}\}_3$ as distinct even though two out of three buckets are identical. We need to divide out the redundant permutation meaning the final answer looks like $\displaystyle{\frac{3!S(5, 3)}{2!} = 75}$ . Does it make sense to you? Thanks.","['solution-verification', 'combinatorics', 'discrete-mathematics']"
3488664,"A functional ""composition"" of homotopies is also a homotopy.","Let $h,h':X\to Y$ be homotopic and $k,k':Y\to Z$ be homotopic. The idea is to show that $k\circ h$ and $k'\circ h'$ are homotopic. Let $F$ be a homotopy between $h(x)$ and $h'(x)$ and $G$ a homotopy between $k(x)$ and $k'(x)$ . Define $H:X\times[0,1]\to Z$ by $G(F(x,t),t)$ . Then $H$ is continuous as a composition of continuous maps and  for $x\in X$ we have \begin{align}
H(x,0)&=G(F(x,0),0) = G(h(x),0) = k\circ h(x)\\ H(x,1) &= G(F(x,1),1) = G(h(x),1) = k'\circ h'(x),
\end{align} so $H$ is a homotopy between $k\circ h$ and $k'\circ h'$ . My question is this: We cannot exactly compose $F$ and $G$ , since both are functions of two inputs, and one output, but instead we compose $G\circ (F, t\mapsto t)$ . It turns out that this is correct, but how did we know a priori to choose $t\mapsto t$ ? Why not, e.g. $t\mapsto t^2$ ? This turns out to be a homotopy as well, since \begin{align}
G(F(x,0),0^2) = G(F(x,0),0) = k\circ h(x)\\
G(F(x,1),1^2) = G(F(x,1),1) = k'\circ h'(x).
\end{align} But clearly $t\mapsto 1-t$ would not work. Is it the case that we can choose any continuous $\alpha : [0,1]\to[0,1]$ with $\alpha(0)=0$ and $\alpha(1)=1$ and $G\circ(F,\alpha)$ would be a homotopy?","['general-topology', 'homotopy-theory', 'algebraic-topology']"
3488665,Why is $!n\pmod{n+k}$ a multiple of $k+1$ so often?,"Motivation: A permutation of a set with no fixed points is called a derangement . The number of derangements of $n$ elements is notated as $!n$ or "" $n$ subfactorial"". The relation $$!n=(n-1)(!(n-1)+!(n-2))$$ implies that $!n\equiv0\pmod{n-1}$ . Likewise, $$!n=n(!(n-1))+(-1)^n$$ implies that $!n\equiv(-1)^n\pmod{n}$ . I wanted to see if there was any similar relation involving other factors $n+k$ . I wasn't able to find one; instead, I stumbled upon something weirder. A weird pattern: When looking at the table of $!n\pmod{n+1}$ (where by $\text{mod}$ we mean the least non-negative residue), there's no apparent pattern. However, if you look close enough, you'll notice that a disproportionate amount of elements are multiples of $2$ . After calculating the first $5000$ values, this number seems to converge to around $76\%$ , greater than the $50\%$ you'd expect if this sequence was random. Likewise, a disproportionate amount of elements in the table of $!n\pmod{n+2}$ are multiples of $3$ : $56\%$ , contrasted to the expected $33\%$ . This trend continues: there's no single $k\leq1000$ for which there are less than the expected $\frac1{k+1}$ from the first $\require{cancel}\cancel{5000}$ $50000$ values of $!n\pmod{n+k}$ that are multiples of $k+1$ . Looking at the data, I'd even argue that the trend gets more pronounced the farther you go. Here's a Pastebin with my data [ edit: and with the predicted data from Haran's answer ]. My question: Why does this pattern hold? Why is $!n\pmod{n+k}$ a multiple of $k+1$ so much more often than it would be by chance? And where do these percentages come from?","['elementary-number-theory', 'combinatorics', 'derangements']"
3488697,What is the difference between a matrix and a tuple of vectors?,"Why do we need the term matrix? Why can't we just use vectors to define everything we need? I understand we need the terms object, set, group, field, vector, and vector space. I don't understand why we need the term ""matrix"". Is it just shorthand? Similar to the term ""Ket"" used in Dirac Notation for Quantum Mechanics? There a ""Bra"" is a co-vector and a ""Ket"" is a vector. BTW, a $n \times 1$ matrix is a vector.","['matrices', 'linear-algebra', 'vectors', 'terminology']"
3488712,Is there a prime every year if YYYYMMDD is treated as a base-$10$ number?,"As we approach the end of the year, I thought about the appearance of primes in calendar dates as a recreational problem. Consider the number formed by concatenating the digits of a date in the form YYYYMMDD. For example 31-Dec-2019 will be written as $20191231$ . I was investigating if the number YYYYMMDD is prime. I checked for the next hundred thousand years and found that each year has between a minimum of $1$ for the year $5771237$ and a maximum of $37$ primes for the year $450060$ . I could not yet find a single year which did not have a prime. Conjecture : There is at least one prime every year. Update : The year $27789755$ is the smallest year which has no prime. What is the smallest counter example? Also $37$ primes occurring in the year $450060$ because it implies that the interval $(4500600001, 4500601231)$ contains at least $37$ primes. Upon checking, it turns out this interval contains $77$ primes which is quite a dense for a short interval between two large numbers.","['divisibility', 'number-theory', 'elementary-number-theory', 'recreational-mathematics', 'prime-numbers']"
3488776,Why is $2^\omega$ not a larger cardinal then $\omega$?,"Cantor's diagonalization argument shows that the power set of natural numbers is larger than $\aleph_0$ , that is, it has a larger cardinality. Every natural number could either be in any given set of the superset, or it could be out of said set. So, that gives two possibilities for every natural number. Since there are infinite natural numbers, that is $2^\omega$ different sets. Therefore, the superset of the natural numbers has an order of $2^\omega$ . Therefore, $2^\omega$ has a larger cardinality than $\omega$ because it can not be counted, as it is equal to the superset of natural numbers, which is of a larger cardinality then the set of all natural numbers. If this were a valid proof, then $\omega_1$ would not be the first ordinal that is higher than $\omega$ , as is the current mathematical consensus, as far as I know. Why is this not a valid proof, or what have I misunderstood about ordinals and cardinals?","['elementary-set-theory', 'ordinals']"
3488791,"Define the triple factorial, $n!!!$, as a continuous function for $n \in \mathbb{C}$","For anyone unfamiliar with multifactorial notation, I will give a quick rundown of it (at least, to the best of my understanding) for non-negative integer values of $n$ : $$n!=n(n-1)(n-2)(n-3)...(n-a), (n-a) > 0$$ $$n!!=n(n-2)(n-4)(n-6)...(n-a), 2 \geq (n-a) > 0$$ $$n!!!=n(n-3)(n-6)(n-9)...(n-a), 3 \geq (n-a) > 0$$ A more generalised expression can be given like so, where $k$ represents the number of factorial symbols: $$n!^{k}=\left( \begin{cases} 
      1 & n=0\\
      n & 1\leq n\leq k \\
      n(n-k)(n-2k)(n-3k)...(n-a) & n>k 
   \end{cases} \right),
 k \geq (n-a) > 0$$ Now, this is great for when you're working with (primarily) positive integers, but I'm curious how you'd go about extending the definition such that it will be valid for all real and complex numbers. While a general definition for any multifactorial would be amazing, I am primarily just looking for for the definition regarding the triple factorial. I already know it's possible to do so for the double factorial; $z!!=2^{(1+2z-\cos(\pi z))/4}\pi^{(\cos(\pi z)-1)/4}\Gamma(z/2+1), z \in \mathbb{C}$ , so it has me hopeful that it's also possible to define $z!!!$ in a similar such manner.","['complex-analysis', 'factorial']"
3488830,Proof verification. Topology,"Let be $\left ( X,\tau \right )$ a path-connected space. Let be $\left ( X,{\tau}' \right )$ a space such that $\left ( X,{\tau}' \right ) \subset \left ( X, \tau \right )$ ( $\tau$ is finer than ${\tau}'$ ). Show that $\left ( X,{\tau}' \right )$ is path-connected. My attemp: Let be $a,b \in \left ( X,{\tau}' \right )$ . For all $x,y \in \left ( X,\tau \right )$ there is a path $f:\left [ 0,1 \right ] \rightarrow \left ( X,\tau \right )$ from $x$ to $y$ , i.e. $f$ is continuous and $f\left ( 0 \right )=x$ and $f\left ( 1 \right )=y$ . Particularly for $a,b \in \left ( X,\tau \right )$ there is a path $F:\left [ 0,1 \right ] \rightarrow \left ( X,\tau \right )$ . The identity function $i : \left ( X,\tau \right ) \rightarrow \left ( X,{\tau}' \right )$ is continuous because given $U\in{\tau}'$ , $i^{-1}\left ( U \right ) = \left \{ x \in \left ( X,\tau \right )\mid i\left ( x \right )=x\in U \right \}=U$ is open in $\tau$ because ${\tau}' \subset \tau$ . Then $i\circ F:\left [ 0,1 \right ] \rightarrow \left ( X,{\tau}' \right )$ is a path from $a$ to $b$ . Therefore $\left ( X,{\tau}' \right )$ is path-connected. If I'm wrong give me a hint to be right, please. Thanks in advance.","['general-topology', 'solution-verification']"
3488844,"Nonabelian finite $G$ such that $O(g) \subseteq Z(G)g, \forall g \in G$","Is there any nonabelian finite group $G$ such that: $$O(g) \subseteq Z(G)g, \forall g \in G \tag 1$$ where $O(g)$ is the orbit ""by $g$ "" of the natural action of $\operatorname{Aut}(G)$ on $G$ , namely $\sigma \cdot g := \sigma(g)$ ?","['group-actions', 'group-theory', 'abstract-algebra', 'finite-groups']"
3488857,Understanding the Proof of a Theorem 1.9 and Theorem 2.5 from Chartrand's Graphs and Digraphs,"There are two Theorems in Chartrand's Graphs and Digraphs that follow the same line of argument and arrive at an inequality that I can't seem to understand where it is deduced from. The inequality being the following: $(deg(u)-1) + (deg(v)-1) \leq n-2$ and $deg(u) + deg(v) \leq n-2$ in Theorem $1.9$ and Theorem $2.5$ respectively. The two Theorems are shown below: Both proofs follow the same line of logic. I understand the proofs, however, how can we be certain that the sum of the degrees of vertices is always less than $(n-2)$ ? For example, the deletion of $1$ from $deg(u)$ and $deg(v)$ , respectively, from the LHS of the inequality: $(deg(u)-1) + (deg(v)-1) \leq n-2$ comes from there being no vertex $w$ adjacent to both vertex $u$ and $v$ (since the new graph does not contain a triangle), is that correct? Furthermore, how is it bounded above by $(n-2)$ ? Also, in Theorem $2.5$ , is it (implicitly) following a proof by contracdition method? Since it isn't stated, however comes to a contradiction.","['graph-theory', 'proof-explanation', 'discrete-mathematics']"
3488876,Closed form for the series $\sum_{n=1}^{\infty}‎\frac{\pi^n}{n!n^p}z^n$.,"I know that the following series $$\sum_{n=1}^{\infty}‎\frac{\pi^n}{n!n^p}z^n ;\;z\in\mathbb{C},$$ is convergent by applying the ratio test. Now I wanted to know that Does the series have any closed form. Anyone can help me? Thanks.
Do attention that $p‎>‎1$ .","['real-analysis', 'complex-analysis', 'calculus', 'sequences-and-series', 'power-series']"
3488895,Rationalizing denominator containing a root of a polynomial - why is this possible / why does it work? [duplicate],"This question already has answers here : Intermediate ring between a field and an algebraic extension. (4 answers) Algebraic field extensions: Why $k(\alpha)=k[\alpha]$. (3 answers) Closed 4 years ago . The problem is to express the number $$1\over x^5 + 2x^4 + 3x^3 + 3x^2 + 2$$ using only rational numbers in the denominator, knowing that $x^5 + 2 = -2(x + 1)(x^3 + x^2 + x)$ . (This is an example from a final exam of an algebra course from the past year (which is freely available to students).) I know how to solve this problem, but I do not understand why this is possible. The trick comes from knowing that this number can be expressed using addition and multiplication of $x$ and rational numbers, ie. as $$r_0 + r_1x + r_2x^2 + r_3x^3 + r_4x^4 + r_5x^5 + \dots$$ My solution follows , skip to questions if you are not interested. From the polynomial equation it follows that $x^5$ equals $-2 - 2x - 4x^2 - 4x^3 - 2x^4$ , therefore an expression containing $x$ with exponent 5 or greater can be rewritten as an expression with only $x^0$ to $x^4$ . So my number can be expressed as $$ax^4 + bx^3 + cx^2 + dx + e$$ Putting an equals sign between these two expressions and multiplying by the denominator gives $$1 = (x^5 + 2x^4 + 3x^3 + 3x^2 + 2)(ax^4 + bx^3 + cx^2 + dx + e)$$ I can multiply the right hand side and ""normalize"" it (by replacing $x^{\ge5}$ ) to get $$1 = (e - f)x^4 + \dots + (2e - 2d)$$ From this formula I can create a system of linear equations, because the coefficient of $x^n$ has to be one if n equals zero, and zero otherwise. Solving this system will let me find the values of the coefficients. Question 0 : Why does this problem even have a solution of the form I used? Question 1 : Is there a solution like this for any expression $1 \over f(x)$ , where f is a polynomial of $x$ , knowing $x$ is a root of another polynomial? Will my method always work? Why? Apologies if this is a duplicate, but everything I could find about rationalizing the denominator was about simple fractions like $1 \over 2 + \sqrt 6$ . I'd appreciate even just comments directing me towards what I can study because I am out of keywords.","['algebra-precalculus', 'abstract-algebra']"
3488915,Arcwise connected vs. path connected,"I've been reading An Introduction to Algebraic Topology by Andrew Wallace. His definition of an arcwise connected space is: a space in which there is a path between every two points. In the Wikipedia page for connected space , however, this is taken as the definition of a path-connected space not an arcwise connected space. I am confused by this mix-up. Which one is more standard? Update I'm quoting Wallace's definition of path and arcwise connectedness. Definition. Let $E$ be a given topological space, and let $I$ denote the unit interval $0 \le t \le 1$ , regarded as a subspace of the space of real numbers in the usual topology. Then a path in $E$ joining two points $p$ and $q$ of $E$ is defined to be a continuous mapping $f$ of $I$ into $E$ such that $f(0) = p$ and $f(1) = q$ . The path will be said to lie in a subset $A$ of $E$ if $f(I) \subset A$ . Definition. A topological space $E$ is said to be arcwise connected if, for every pair of points $p$ and $q$ of $E$ there is a path in $E$ joining $p$ and $q$ . If $A$ is a set in a topological space $E$ , then $A$ is arcwise connected if every pair of points of $A$ can be joined by a path in $A$ . It seems that the situation is as Henno Brandsma pointed out: since most spaces are Hausdorff, path-connectedness is identified with arcwise connectedness and thus entirely omitted from the book. However, this strikes me as odd as I am actually interested in non-Hausdorff spaces such as those arising from computer science.","['general-topology', 'homotopy-theory', 'algebraic-topology']"
3488942,Is $\sqrt{x}$ continuous at $0$? [duplicate],"This question already has answers here : Is $\sqrt x$ continuous at $0$? Because it is not defined to the left of $0$ (3 answers) Closed 4 years ago . Is $f(x) = \sqrt{x}$ is continuous at $0$ ? I see that people say that $\sqrt{x}$ is continuous at the interval $[0,\infty)$ . Their proof is based on that: (continuous from the right): $$\lim_{x \to 0^+}f(x) = f(0) = 0$$ At least from what I have seen: Is function continuous at 0? But there is no left limit, namely $\lim_{x \to 0^-}$ doesnt exists. And from what I know the statement is that: $f$ is continuous at $x_0$ if and only if, it continuous from the left and from the right. But here, there is no limit from the left. So how $\sqrt{x}$ is continuous at $0$ so we say it continuous at $[0,\infty)?$","['limits', 'real-analysis']"
3488957,Expectation using Monte Carlo explaination,"Although Monte Carlo (MC) is an easy-implementating algorithm, but I'm still confused with the sampling theory behind it. Especially, the MC can approximate an expectation with $$
\int f(X(\omega))\mathbb{P}(d\omega) \approx \sum_i^N f(X(\omega_i)),
$$ where $(X, \Omega, \mathbb{P})$ the probability space, and $\omega_i$ some samples from sampling space. My first intuition is to use quadrature in a deterministic way, i.e., Gauss-Hermite. Although it would be great to compute it with evaluation of samples, but I don't quite understand why summation over samples work here. My question is, why can we calculate the above integral with that summation using samples ? What are the theory and mathematical formulations behind it? My attempt: 
Recall the definition of integral. Suppose that $X$ and $f$ are simple: $X=\sum_i^Ka_i \chi_{A_i}$ , where $A_i = X^{-1}\{a_i\}$ and $\chi$ the indicator function. Thus $\int f(X(\omega))\mathbb{P}(d\omega) = \sum_i^Ka_i\mathbb{P}(A_i)\approx1/N \sum_i^N f(X(\omega_i))$ , where $a_i\in A_i$","['integration', 'statistics', 'expected-value', 'probability-theory', 'probability']"
