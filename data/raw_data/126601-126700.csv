question_id,title,body,tags
1926944,"Is the vector $(2,2)$ perpendicular to the level curve 2 of $f(x,y)=4-x^{2}-y^{2}$?","Problem: 
If $f(x,y)=4-x^{2}-y^{2}$, then the vector $(2,2)$ is orthogonal to the level curve 2 of $f(x,y)$ in the point $(1,1)$. Solution : $z=4-x^{2}-y^{2}$ $=\left \langle z=2 \right \rangle$ $2=4-x^{2}-y^{2}$ $=\left \langle \text{Subtract}\ -4\right \rangle$ $-2=-x^{2}-y^{2}$ $=\left \langle \text{Multiply by}\ -1\ \right \rangle$ $2=x^{2}+y^{2}$ $=\left \langle \text{Divide by}\ 2\right \rangle$ $1=\frac{x^{2}}{2}+\frac{y^{2}}{2}$ So $a=b=\sqrt{2}\approx1.4$ Then I plotted the level curve as follows: Graphically we see that the vector $(2,2)$ is orthogonal to the level curve of 2 at point $(1,1)$. But... how do I make this analytically?",['multivariable-calculus']
1926964,A Neat Identity Involving Zeta Zeroes,"While playing around, I encountered the following very curious and cool identity. Consider the exponential integral $\text{Ei}(x)$ and the $n$th nontrivial zero of the Riemann Zeta function $p_n$. Now, look at the first few imaginary parts of the following function:
$$f(x)=\sum_{n=1}^x \text{Ei}(p_n)$$ $$\Im \ \ f(1)=3.13732$$
$$\Im \ \ f(10)=31.3169$$
$$\Im \ \ f(100)=314.097$$ 
$$\Im \ \ f(1000)=3141.54$$
$$\Im \ \ f(10000)=31415.9$$ As you can see, it is each time adding a digit of pi. Question: Is this a known result that can be proved easily? Does this pattern even continue?","['calculus', 'riemann-hypothesis', 'riemann-zeta', 'pi', 'summation']"
1927025,Help in evaluating $\displaystyle\int\ \cos^2\Big(\arctan\big(\sin\left(\text{arccot}(x)\right)\big)\Big)\ \text{d}x$,"Is there an easy way to prove this result? $$\int\ \cos^2\Big(\arctan\big(\sin\left( \text{arccot}(x)\right)\big)\Big)\ \text{d}x = x - \frac{1}{\sqrt{2}}\arctan\left(\frac{x}{\sqrt{2}}\right)$$ I tried some substitutions but I got nothing helpful, like: $x = \cot (z)$ I also tried the crazy one: $x = \cot(\arcsin(\tan(\arccos(z))))$ Any hint? Thank you!","['indefinite-integrals', 'integration', 'trigonometric-integrals', 'calculus']"
1927027,"$\|f\|^2 =\sum_j|\langle f, f_j \rangle|^2$ implies $\langle f, g \rangle=\sum_j \langle f, f_j\rangle\langle f_j, g\rangle$.","Let $\mathcal{H}$ be a complex Hilbert space, let $f, g \in \mathcal{H}$ and let $(f_j)_{j \in J} \subset \mathcal{H}$, which is indexed by an index set $J \subseteq \mathbb{Z}$. Suppose $\langle f, f \rangle = \sum_{j \in J} | \langle f, f_j \rangle|^2$. Then 
  $$ \langle f, g \rangle = \sum_{j \in J} \langle f, f_j \rangle \langle f_j, g \rangle. $$ I have seen the above result several times, and it seems to be a standard result.  In general, it is mentioned that the result follows by ""polarisation"". However, I am not entirely sure how to apply the polarisation identity here. Any help and/or comment is highly appreciated.","['functional-analysis', 'hilbert-spaces']"
1927046,"can ""sets"" be interpreted as ""predicates""?","Instead of writing ""$r\in\mathbb{R}$"", we can write ""$r$ is a real number"". In the latter statement we are asserting $P(r)$ where $P(x)$ is the predicate ""$x$ is a real number"". It seems like sets give rise to predicates. How far can this be taken? Can I replace every mention of a set with a predicate and have an equivalent theory (to set theory)? If not, why not? The reason for the question is this: the notion of ""is in a set"", to me, suggests something different than ""is a"". ""is a"" seems to just tell me that a certain object has a certain property or type (and we don't have to think about size). ""is in a set"" tells me that we have to think of all of those objects together in a certain place (and we do have to think about size).","['soft-question', 'elementary-set-theory']"
1927052,Does analysis do anything other than estimating things and discussing convergence? [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 years ago . Improve this question Let me first of all declare that till a few months ago, Analysis was the subject I liked most, as I was pretty good at it and the idea was simple: you essentially bound things. But then, I had these spurts of realizations: has Analysis nothing else to offer? Let's take a quick look at what analysis does. Fourier analysis: Hell lot of PDE's, heat equations, etc. and hell lot of estimating things. The concept of generalized function I admit, is really nice, but that's essentially all to it. From the beginning to the end, estimate integrals or show certain functions belong to a class. Complex analysis: This is a beautiful subject, although estimations crop up here as well, quite often. However, I do like things like Cauchy's Theorem, Morera's Theorem, etc. simply because they are NOT estimating things! Functional analysis: Convergence on arbitrary spaces and some fairly complicated existential theorems. Due to lack of concrete integration, there is lack of estimating things but then, there's always the concept of showing convergence. Analytic Number Theory: I got bored to death trying to study this. From the first page to the last (probably the book I chose wasn't friendly) I saw integrals being estimated. I have no grievance towards analysis in particular, and as I have mentioned, I am actually good in it. I can grasp analysis concepts really well and my background is quite strong. However, after a point, you really begin to wonder whether a subject has anything else to offer other than estimating integrals/series and checking convergence. I, unfortunately, haven't been exposed to prospective fields of Analysis which go beyond these. So, at times, the journey has been immensely boring. I would like to ask the community of mathematicians here: what is your opinion? I would love to know if there are topics in analysis beyond these estimations and computations, so if you know of them please do tell me. Yes, something I missed is: why did I like analysis? Because it reduced a lot of computations I used to do as a high school student. Look at the Riemann-Lebesgue Lemma. Look at the power of Stone-Weierstrass. These are theorems that really boost my interest. But then, what about the rest?","['soft-question', 'analysis']"
1927074,Converge in probability and Expectation.,"Let $(\Omega, F , P)$ be probability space. Let $X_i$ be sequence of random variable and $X$ be Random variable. Claim. $X_n \to X$ in probability if and only if $E(\frac{|X_n-X|}{1+|X_n-X|})$ $\to 0$ I can not catch any hint. Can I get some hints?","['expectation', 'probability', 'random-variables']"
1927093,Differential of a function between manifolds,"The book we are using in class is Frank Warner Foundation of Differential Manifold and Lie Group. Let $M,N$ be two smooth $d$-dim manifold, the differential of a $C^\infty$ function $\phi:M\rightarrow N$ is defined by 
$$d\phi: M_m \rightarrow N_{\phi(m)}$$
For $v\in M_n$, and $g:N \rightarrow \mathbb{R}$ a smooth function, we define
$$d\phi(v)(g) = v(g\circ \phi).$$ Now for a smooth function $f:M \rightarrow \mathbb{R}$, I don't quite see why the book states
$$df(v)(g) = v(f) \frac{\partial}{\partial r}\bigg|_{r_0} g$$
from the definition above. Edit: Reading the answer here seems that if we plug $g(r) = r$, we will get the identity $df(v) =  v(f)$, but how do we know it will hold for all $g$?","['differential-forms', 'real-analysis', 'differential-geometry', 'differential-topology']"
1927096,Is $(x+1)/(x^2-1)$ defined for $x=-1$? [duplicate],"This question already has answers here : Why does factoring eliminate a hole in the limit? (16 answers) Closed 7 years ago . It might sound like a silly question, but I can't come up with a clear answer.
By looking at the expression, the answer should be ""no"", since $(-1)^2=1$ and we're in trouble. However, if I factorize: $(x^2-1) =(x+1)(x-1)$, $x=-1$ is still illegal. But now the terms cancel out and I am left with $1/(x-1)$, which is clearly defined for $x=-1$? What happened? Was some information lost in the manipulation or was the original expression an ""illusion""? I hope my question makes sense.","['fractions', 'functions', 'arithmetic']"
1927140,Maximizing expected value of coin reveal game,"I was asked this question today in an interview and am still not sure of the answer. The question is as follows. Part one: 
Say I have flipped 100 coins already and they are hidden to you. Now I reveal these coins one by one and you are to guess if it is heads or tails before I reveal the coin. If you get it correct, you get $\$1$. If you get it incorrect, you get $\$0$. I will allow you to ask me one yes or no question about the sequence for free. What will it be to maximize your profit? My approach for this part of the problem was to ask if there were more heads than tails. If they say yes, I will just guess all heads otherwise I just guess all tails. I know the expected value for this should be greater than 50 but is it possible to calculate the exact value for this? If so, how would you do it? Part two:
Same scenario as before but now I will charge for a question. I will allow you to ask me any amount of yes or no questions as I go through this process for $\$1$. What is your strategy to maximize your profit? I was not sure about the answer to this part of the question. Would the best option be to guess randomly? I think the expected value of this should be 50. I am not sure about the expected value of part one but if it is greater than 51, I think I could also use that approach. Anyone have a good idea for this part?","['statistics', 'probability']"
1927237,composition of stereographic projections is inversion through the ball - a geometric way,"$S^{n}$ be the unit sphere in $\mathbb{R}^{n+1}$.  Let $\pi_N$ be stereographic projection from the sphere without the north pole on to $\mathbb{R}^{n}$ and let $\pi_S$ be defined similarly using the south pole. On $\pi_N(\pi_S^{-1}(\mathbb{R^n}))=\mathbb{R^n-0}$, one can take $\pi_S \circ \pi_N^{-1}$.  By messy algebra, I showed that this is the inversion through the unit sphere $S^{n-1} \subset \mathbb R^{n}$ sending $x \in \mathbb{R}^{n} \mapsto x/|x|^2$. In full detail, I showed that that under this correspondence $z \mapsto z \cdot 2/(1+|z|^2)+(|z|^2-1)/(|z|^2+1)(0,....,0,1) \mapsto z  \cdot (2/(1+|z|^2))/(2 |z|^2 /(1+|z|^2))=z/|z|^2$ Is there a geometric way of seeing this using just inner products and such. i.e. I don't want to have to use the quadratic forumula like I did (I am trying to do this along the lines of the symmetry lemma given in Axler's Harmonic function theory. You don't have to though.)","['harmonic-functions', 'euclidean-geometry', 'differential-geometry']"
1927306,"Real Analysis, Folland Proposition 2.23 Integration of Complex Functions","Background Information: 2.20 Proposition - If $f\in L^{+}$ and $\int f < \infty$ , then $\{x:f(x) = \infty\}$ is a null set and $\{x:f(x) > 0\}$ is $\sigma$ -finite. Question: Proposition 2.23: a.) If $f\in L^1$ , then $\{x:f(x)\neq 0\}$ is $\sigma$ -finite. b.) If $f,g\in L^1$ , then $\int_{E} f = \int_{E} g$ for all $E\in M$ if and only if $\int |f - g| = 0$ if and only if $f = g$ a.e. Attempted proof a.) Note Folland states this follows from Proposition 2.20 I will attempt to use this in part of my proof. Let $f\in L^1$ and $E = \{x:f(x)\neq 0\}$ . Since $f\in L^1$ we know that $\int f < \infty$ . We have from Proposition 2.20 that the set $\{x:f(x) > 0\}$ is $\sigma$ -finite. Now since $$\{x: f(x) < 0\} = \bigcup_{n\in\mathbb{N}} \{x:f(x) < 1/n\}$$ For each $n$ set $\phi_n = \frac{1}{n}\chi_{\{x:f(x)<1/n\}}$ such that $0\leq |\phi_n|\leq |f|$ Then we have $$\int |f| \geq \int |\phi_n| = \int |\frac{1}{n}\chi_{\{x:f(x)<1/n\}}| = \frac{1}{n}\int |\chi_{\{x:f(x)<1/n\}}|$$ Thus $$n\int |f| \geq \int |\chi_{\{x:f(x)<1/n\}}|$$ Before I continue I am wondering since we already have proposition 2.20 that $\{x:f(x) > 0\}$ is $\sigma$ -finite what I am trying to show above is that $\{x:f(x) < 0\}$ is $\sigma$ -finite and obviously if we have $f(x) = 0$ then the quantity would equal zero thus can we conclude that $\{x:f(x)\neq 0\}$ is $\sigma$ -finite. Attempted proof b.) I need a little bit of help with this part here. Any suggestions is greatly appreciated.","['real-analysis', 'measure-theory']"
1927334,How to compute the normal to the ellipsoid at the point on the surface of ellipsoid?,"Let the equation of an ellipsoid be: $$2x^2+y^2+2z^2=5$$ And the point on the surface of ellipsoid be: $$(1,1,1)$$ How to compute the normal to the ellipsoid at the point on the surface of ellipsoid? I read some article says that I can compute it by gradient, but I am not sure how to do it...","['multivariable-calculus', 'geometry']"
1927339,Existence of $\xi$ and $\eta$ such that $f'(\xi)+f'(\eta)=\xi+\eta$,"Let $f$ be continuous on $[0,1]$ , differentiable in $(0,1)$ . Assume futher that $f(0)=0$ , $f(1)=1/2$ . Show that there exist $\xi,\eta\in (0,1)$ such that $f(\xi)+f'(\eta)=\xi+\eta$ . I saw this problem in a draft. I do not know whether it is true. Up to now, I have not find a counterexample. My idea is as follows: $f(\xi)-\xi=\eta-f'(\eta)$ . Let $F(x)=f(x)-x$ , then $F(0)=0, F(1)=-1/2$ , so $f(\xi)-\xi$ can be chosen to be arbitrary $a\in (-1/2,0)$ . The strategy is then to find $\eta$ such $a=\eta-f'(\eta)$ . Roll's theorem may be applied. However, letting $G(x)=ax-x^2/2+f(x)$ , then $G(0)=0$ , $G(1)=a$ . This fact does not verify that of Roll. So how can I get across the difficulties? If we change $f(\xi)+f'(\eta)=\xi+\eta$ to be $f'(\xi)+f'(\eta)=\xi+\eta$ for some $\xi,\eta\in (0,1), \xi\neq \eta$ . Can we prove it?","['real-analysis', 'calculus']"
1927348,(Graph Theory) Prove that $H_n$ has a Hamiltonian cycle for $n$ ≥ 2.,"Where $H_n$ is the graph that has a vertex for each n-digit binary sequence such that 2 vertices are connected if their binary sequences are different in exactly 1 digit. Attempt: By induction on n we have, Base case: True for n=2 Induction hypothesis: $H_n$ has a Hamiltonian cycle for $n$ ≥ 2 Induction step: $H_{n+1}$ has $2^{n+1}$ = $2(2^n)$ vertices so by the induction hypothesis, the graph $K_n$ consisting of every other vertex has a Hamiltonian cycle. Since the other half of the vertices not in $K_n$ all have exactly one digit different from those in $K_n$, $H_{n+1}$ must have a Hamiltonian cycle. But I'm not sure if the hypothesis $K_n$ immediately follows since those vertices don't differ by only 1 digit.","['combinatorics', 'graph-theory', 'hamiltonian-path']"
1927394,"Number of all positive continuous function $f(x)$ in $\left[0,1\right]$","Number of all positive continuous function $f(x)$ in $\left[0,1\right]$ which satisfy $\displaystyle \int^{1}_{0}f(x)dx=1$ and $\displaystyle \int^{1}_{0}xf(x)dx=\alpha$ and $\displaystyle \int^{1}_{0}x^2f(x)dx=\alpha^2$ Where $\alpha$ is a given real numbers. $\bf{My\; Try::}$ :: Adding $(1)$ and $(3)$ and subtracting $2\times  (2),$ we. Get $$\displaystyle \int^{1}_{0}(x-1)^2f(x)dx=(\alpha-1)^2$$ now how can I solve it after that, Thanks",['calculus']
1927410,What does the p-value really mean?,"Suppose I fit a linear model m1 to the data produced below: #true parameters
    x = rnorm(100,5,1)
    b = 0.5
    e = rnorm(100,0,3)
    beta_0= 2.5
    beta_1= 0.5
    y = beta_0 + beta_1*x + e
    plot(x,y)
   #linear fit
    m1 = lm(y~x)
    abline(m1)
    summary(m1) The p-value I get is 0.66. When I sample x again (rerun the code), and fit the regression the p-value is 0.05. Why do I get different p-values for different samples, and how do you reconcile these differences?","['statistics', 'probability']"
1927414,Number of distinct solutions of $f(f(x))=0$,Let $f(x)=x^3-3x+1.$ Then what is the number of different real solutions of the equation $f(f(x))=0$? $f(x)$ has three roots and $f(f(x))$ will be 0 when value of $f(x)$ is equal to its root. But this approach is turning out to be tedious as finding exact values of roots of $f(x)$ is not possible Could someone suggest a better approach?,"['derivatives', 'calculus']"
1927467,Basic probability space problem,"I'm taking a signal analysis course with a strong emphasis on probability. Long story short, the professor is assigning homework from the text which he said was optional, and expects us to learn the material straight out of that text. Many of these problems have to do with probability spaces, something which I feel completely lost with, not to mention set theory in general. I have here an example problem but would love if someone could help explain a solution in very general terms applying to probability spaces as a whole. Please bear in mind I'm basically floundering in the water until my textbook comes in the mail, and would appreciate a very simple explanation. The Problem: A receiver listens to the data stream of zeros until the first one appears. Then $S = (0,01, 001, ...)$. Choose $A = 2^S$ and show that $(S, A, P)$ is a probability space. Use your own probability measure $P$. My approach: I'm dimly aware of the criteria for a probability space, but I'm probably wrong somewhere: $S$ must be a set: Check. $A$ must be a sigma-algebra of $S$: i) $S \in A$ : What does it mean if $S$ is in $2^S$? How is this expressed in terms of sets? ii)  $ x \in A \Rightarrow \bar{x} \in A $: What would be the complement of some element in $A$ here? My best guess would be the next element in the sequence; i.e. the complement of 001 would be 0001 because the sequence continues if a 0 is chosen in the third place instead of a one. However, that also works for the infinite elements following 001, and surely they can't all be complements? iii)  $ x, y \in A \Rightarrow x \cup y \in A$: I'm not sure how to express a union here. Isn't there only one set, the ""union"" of which would be itself? $P$ must be a (countably additive) probability measure of $S$: The biggest issue here is not with the criteria but the actual construction of this probability measure. It seems to me that each term in the series has half the probability of the one preceding it; i.e. the $nth$ term has a $1/(2^n)$ chance of defining the actual event. This is a geometric series adding to 1, and links to the definition of the sigma-algebra from before, but how do I express this as a probability measure? i) $P(S) = 1$ : Check. ii) $P($any  element  of $A) \geq 0 $: Check. iii) $P(A \cup B) = P(A) + P(B), A \cap B = \varnothing$: What is the significance of A and B here? An example would be very helpful. Thanks for any help you can offer, and sorry if the formatting is bad. I made an account and learned a bit of LaTex just to post this conundrum!","['statistics', 'probability']"
1927574,Lebesgue differentiation theorem and surface measure,"I came across the following calculation ( Evans p.26 ) $$
\dots =  \lim_{t \to 0} \frac{1}{n \alpha(n)t^{n-1}} \int_{\partial B(x,t)}u(y) dS(y) \overset{\ast}{=} u(x),
$$
for $u \in C^2(U)$, $U \subseteq R^n$ and $\alpha(n)$ the volume of the unit ball (i.e. $n\alpha(n)$ is the surface area of the unit sphere). I am trying to figure out the equality $\ast$.
It looks like the Lebesgue differentiation theorem but Evans only mentions this for Balls $B(x,t)$ but not for spheres.
When I try to write out the surface integral as an integral over an n-1-dimensional submanifold things get kind of confusing. Can you maybe share a reference or explain why the Lebesgue differentiation theorem holds for surface integrals? Based on the hints of  H. H. Rugh this could do the trick: $$
 \frac{1}{n \alpha(n)t^{n-1}} \int_{\partial B(x,t)}u(y) dS(y) 
= \left(\frac{1}{n \alpha(n)t^{n-1}} \int_{\partial B(x,t)} (u(y) - u(x))dS(y)\right)  + u(x)
$$ Left to show is that $$\left(\frac{1}{n \alpha(n)t^{n-1}} \int_{\partial B(x,t)} (u(y) - u(x))dS(y)\right) \to 0 \quad\text{for}\quad t \to 0$$ which seems to be basically the same problem. Any suggestions?","['real-analysis', 'measure-theory', 'partial-differential-equations']"
1927580,Prove $R/M$ is a division ring for a non-commutative ring $R$ with max ideal $M$,I am trying to prove that $R/M$ is a division ring if $M$ is a maximal ideal of a non-commutative ring $R$. I tried by using similar arguments as in $R$ commutative. But the proof that $R/M$ is a field if $M$ is maximal fails if $R$ is non-commutative.,"['abstract-algebra', 'ring-theory', 'maximal-and-prime-ideals', 'noncommutative-algebra']"
1927642,Which is the best book for studying geometric flows?,I have some knowledge about the basics in Riemannian Geometry (I used Do Carmo's and Petersen's books). Now I would like to focus my attention on geometric flows (mostly mean curvature flow and Ricci flow). Where should I start? Which is the best introductory book? Thank you!,"['riemannian-geometry', 'mean-curvature-flows', 'reference-request', 'ricci-flow', 'differential-geometry']"
1927662,Proof of a theorem on measurable functions,"Suppose $(\Omega, \mathscr{L})$ and $(S, \mathscr{B})$ are measure spaces and that a collection of sets $\mathscr{A}$ generates $\mathscr{B}$, i.e., $\sigma(\mathscr{A}) = \mathscr{B}$. Let $X: \Omega \rightarrow S$. If $X^{-1}(A) \in \mathscr{L} \quad \forall \space A \in \mathscr{A}$, then $X$ is measurable $\mathscr{B}$ (or $X$ is a random variable).","['probability-theory', 'measure-theory']"
1927669,Are there periodic functions satisfying a quadratic differential equation?,"Question: Are there periodic functions satisfying a quadratic differential equation, as opposed to just linear or cubic? Bonus question: Are there periodic functions satisfying differential equations which are polynomials of any degree $n$ ? Background: I know that on the real line, any periodic function can be decomposed into a (possibly infinite) sum of sines and cosines via Fourier series, and that this technique is somewhat extensible to the complex plane (I think). Likewise, cosine and sine can be defined as the solutions to a linear systems of ordinary differential equation (at least on the real line, I am not sure about the complex plane), see e.g here . A natural generalization/extension of the cosine and sine would be to either consider (1) functions defining the trigonometry of more general conic sections than the circle or (2) functions which have more ""advanced"" periodicity properties, like double periodicity. Functions which seem to satisfy both of these criteria would be the elliptic functions , because they are (1) the inverses of elliptic integrals, and (2) doubly periodic. However, what is surprising to me is that the (complex) differential equation which they satisfy is cubic in the function and its first derivative, rather than quadratic. This seems to suggest a substantial jump in complexity from the trigonometric functions. This question is motivated by Algebraic Geometry: A Problem Solving Approach , which mentions the elliptic functions in the context of cubic curves, but does not mention any special functions related to conic sections -- is this because there aren't any, or are they just somehow less important? Attempt: Maybe the hyperbolic tangent function? I don't think that it's periodic, but it is related to the exponential, which is periodic. Also it seems to satisfy a quadratic differential equation . Note: I tagged this (complex-analysis) because it seems to be about complex differential equations.","['complex-analysis', 'ordinary-differential-equations']"
1927726,How to integrate $ \int^a_0{\cfrac{dx}{x \ + \ \sqrt{a^2 \ - \ x^2}}} $?,"I am having a little problem with my maths homework. The problem is as follows: \begin{equation}
  \int^a_0{\cfrac{dx}{x \ + \ \sqrt{a^2 \ - \ x^2}}}
\end{equation} I tried to do the following but got stuck halfway: Let $\ \ x \ = asin\theta, \ hence, \  dx = acos\theta \ d\theta $ $
\int^a_0{\cfrac{dx}{x \ + \ \sqrt{a^2 \ - \ x^2}}}
$ $
= \int^\frac{\pi}{2}_0{\cfrac{acos\theta}{asin\theta \ + \ \sqrt{a^2 \ - \ a^2sin^2\theta}}}\ d\theta
$ $
= a \cdot \int^\frac{\pi}{2}_0{\cfrac{(cos\theta \ + \ sin\theta) \ + \ (cos\theta \ - \ sin\theta) - \ cos\theta}{asin\theta \ + \ \sqrt{a^2cos^2\theta}}}\ d\theta
$ $
= a \cdot \int^\frac{\pi}{2}_0{\cfrac{(cos\theta \ + \ sin\theta) \ + \ (cos\theta \ - \ sin\theta) - \ cos\theta}{asin\theta \ + \ acos\theta }}\ d\theta
$ $
= \int^\frac{\pi}{2}_0{\cfrac{(cos\theta \ + \ sin\theta) \ + \ (cos\theta \ - \ sin\theta) - \ cos\theta}{sin\theta \ + \ cos\theta }}\ d\theta \\ \\
$ $
= \int^\frac{\pi}{2}_0{\left(1 \ + \ \cfrac{(cos\theta \ - \ sin\theta)}{sin\theta \ + \ cos\theta } - \cfrac{cos\theta}{sin\theta \ + \ cos\theta}\right)}\ d\theta
$ Could someone please advise me how to solve this problem?","['definite-integrals', 'integration', 'trigonometry']"
1927758,Norms on a normed vector space $X$ are equivalent if and only if Cauchy sequences are preserved,"This is my attempt in proving the above statement. Suppose $\left \| . \right \|_{1}$ and $\left \| . \right \|_{2}$ are equivalent.$\\$ And suppose $\left \{ x_{n} \right \}$ is a Cauchy sequence under the first norm.$\\$ 
Let $\varepsilon$ be arbitrary. So $\exists N$ such that $\forall m\geq n> N$  $\left \| x_{m}-x_{n} \right \|_{1}< C\varepsilon $ where,
$C\left \| x_{m}-x_{n} \right \|_{2}\leq \left \| x_{m}-x_{n} \right \|_{1}$
 (since the norms are equivalent). so we can have  $\left \| x_{m}-x_{n} \right \|_{2}< \varepsilon $ Hence the sequence is Cauchy under the second norm. Now for the converse(that is where I got stuck) suppose that the Cauchy sequences are preserved under the two norms. For the equivalence in norms the alternating definition that I'm going to use is, If norms are equivalent then $C\overline{B_{1}}\subset \overline{B_{2}}\subset c\overline{B_{1}}$ Where $\overline{B_{1}}$ and $\overline{B_{2}} $ are the closed unit balls under the norm 1 and norm 2 Let $x \in  \overline{B_{1}}$ So we have a sequence $ \left \{ x_{n} \right \} $ in $B_{1} $ that converges to x. So that sequence is Cauchy in 
$\left \| . \right \|_{1}$ Hence by the supposition it is cauchy under $\left \| . \right \|_{2}$ After this could somebody please let me know how to proceed?","['functional-analysis', 'normed-spaces']"
1927845,Is U=V in the SVD of a symmetric positive semidefinite matrix?,"Consider the SVD of matrix $A$: $$A = U \Sigma V^\top$$ If $A$ is a symmetric, positive semidefinite real matrix, is there a guarantee that $U = V$? Second question (out of curiosity): what is the minimum necessary condition for  $U = V$?","['matrices', 'svd', 'symmetric-matrices', 'positive-semidefinite', 'linear-algebra']"
1927944,How do we know that the diagonal number produced in Cantor's argument for the uncountability of real numbers isn't periodic?,As for example stated in the answer to this question: Why does Cantor's diagonal argument not work for rational numbers?,['elementary-set-theory']
1927954,Why don't likelihood ratio tests all go to one?,"Suppose we have the following general setup where we wish to test the hypothesis,
$$
H_0 : \theta\in\Theta_0 ~~~~~~~~~~~~~~~~~~~~~~~~~H_1 : \theta\not\in\Theta_0.
$$
It seems to be pretty well established that under some regularity conditions that a likelihood ratio test,
$$
\Lambda\left(\mathbf{X}\right) = \frac{\sup_{\theta\in\Theta_0} \mathcal{L}\left(\theta;\mathbf{X}\right)}{\sup_{\theta\in\Theta} \mathcal{L}\left(\theta;\mathbf{X}\right)}
$$
has the property that $-2\log\Lambda\left(\mathbf{X}\right)\sim\chi^2_k$, where $k = \dim\left(\Theta\right) - \dim\left(\Theta_0\right)$ asymptotically. I don't understand why $\Lambda\left(\mathbf{X}\right) \to 1$ in the limit of infinite data when the null hypothesis is true. Allow me to explain. We know that maximum likelihood is asymptotically consistent such that
$$
\lim_{n\to\infty} \Pr\left[\theta = \hat{\theta}\right] = 1.
$$
Therefore, if the null is true (i.e. $\theta\in\Theta_0$) it should also be the case that with probability one $\hat{\theta} \in\Theta_0$. By definition, $\hat{\theta}$ maximizes the denominator of the likelihood ratio, but because $\hat{\theta}\in\Theta_0$, it is clear that $\hat{\theta}$ also maximizes the numerator. Additionally, because they are maximized by the same point, the numerator and denominator are equal so that,
$$
\Lambda\left(\mathbf{X}\right) = \frac{\mathcal{L}\left(\hat{\theta};\mathbf{X}\right)}{\mathcal{L}\left(\hat{\theta};\mathbf{X}\right)} = 1.
$$
Moreover, $-2\log\Lambda\left(\mathbf{X}\right)=0$. I know there is a problem with this reasoning, but I don't know what it is. It seems that the correction must have to do with the additional $k$ free parameters that were assumed to be present in the unrestricted parameter space $\Theta$.","['maximum-likelihood', 'statistics', 'log-likelihood']"
1927960,Is there a general rule for how to write high order polynomials in matrix form?,Is there a general rule for how to write high order polynomials in matrix form? For example a linear combination of parameters: $$a_1 x_1+a_2 x_2+a_3 x_3 + \cdots+ a_n x_n$$ Can be written as $$\sum^n_{i=1} a_i x_i = \vec{a}^T\vec{x} $$ Second order forms are given by $$ (a_1 x_1+a_2 x_2+a_3 x_3 + \cdots+ a_n x_n)^2 = \vec{x}^T {\mathbf A} \vec{x}$$ Which ensures all combinations of second order terms. What about the higher orders? i.e. $$(a_1 x_1+a_2 x_2+a_3 x_3 + \dots +a_n x_n)^k$$ What forms ensure all combinations of terms. Is there a general rule to this? Does it have a name?,['linear-algebra']
1928029,Does the Riemann map change smoothly with respect to smooth change of underlying domain?,"Let $\gamma:S^{1} \times [0,1] \to \mathbb{C}$ be in $C^1$ and suppose that $\gamma(.,t)$ is a Jordan curve for all $t\in [0,1]$. Call the domain bounded by $\gamma(.,t)$ as $\Omega(t)$. Assume that for all $t\in [0,1]$, $0 \in \Omega(t)$ and that there exists a $\delta>0$ such that $d(0,\partial \Omega(t) )\geq \delta$. For each fixed $t\in [0,1]$ consider the unique Riemann map $\Phi(.,t):\mathbb{D} \to \Omega(t)$ such that $\Phi (0,t) =0 $ and $\Phi_{z}(0,t) >0$. As $\partial \Omega(t)$ is a Jordan curve we know that $\Phi(,.t)$ extends continuously to $\mathbb{D}$ by Caratheodary's theorem. Question: Is $\Phi(z,t)$ differentiable in $t$? And if that is the case, is it true that for each fixed $t$, $\Phi_{t}(z,t)$ extends continuously to $\mathbb{D}$? I know that $\Phi(z,t)$ is continuous in $t$ by a theorem of Rado (Proof given in the book by Pommerenke: Boundary behaviour of Conformal maps). It seems that there is an unpublished paper (Lavrentiev curves and conformal mapping) by Coifman and Meyer on this problem where they show that the Riemann map depends analytically on the curve. However first of all I cannot find the paper anywhere and even with it I don't know how it solves the above question as the analytic dependence is with respect to the BMO norm associated with the chord arc curve (The problem is that all constants are killed by the BMO norm which corresponds to a rotation. So a lot of domains correspond to the same BMO function and hence you lose information. Also all possible Riemann maps from $\Omega$ to the upper half plane get assigned the same BMO function ) I would be very grateful if someone can answer this or give me a reference for it.","['complex-analysis', 'reference-request', 'analysis', 'harmonic-analysis']"
1928040,How to prove $f(n)=\sum_{k=0}^n\binom{n+k}{k}\left(\frac{1}{2}\right)^k=2^n$ without using the induction method? [duplicate],"This question already has answers here : How to show $\sum_{k=0}^{n}\binom{n+k}{k}\frac{1}{2^k}=2^{n}$ (16 answers) Closed 7 years ago . The problem is to prove the following statement for all natural numbers.
$$f(n)=\sum_{k=0}^n\binom{n+k}{k}\left(\frac{1}{2}\right)^k=2^n$$ I already proved this using mathematical induction.
But, my instinct is telling me that there is some kind of combinatorial/algebraic method that can be used to solve this easily. (Unless my instinct is wrong.)
Can anyone give me hint on how to do this?","['algebra-precalculus', 'combinatorics']"
1928058,Are there physical interpretations of fractional order integrals and derivatives?,"Integral and differential calculus restricted to integer order operations of integration and differentiation have solid, definite connections with models of the physical world such as figuring areas, volumes, rates of change, etc. But what of the fractional calculus? Are there any physical interpretations that come from fractional order derivatives and integrals?","['derivatives', 'fractional-calculus', 'integration', 'mathematical-modeling']"
1928063,Picking certain number of balls without replacement and finding its probability distribution,"Suppose I am to choose three balls without replacement from a bag containing $5$ white and $4$ red balls. What will be the probability distribution of the red balls drawn ?. According to my book, probability function will be
$$
{3\choose x}\left(\,{4 \over 9}\,\right)^{x}\left(\,{5 \over 9}\,\right)^{3 - x}
$$
What I didn't understand is why my book is taking probability of choosing red ball to be $4/9$ and the probability of choosing a white ball to be $5/9$. I think the above probabilities are of choosing the red and the white balls in the first trial. In other trials the probability of the above two events will change as we are drawing balls without replacement.","['probability', 'probability-distributions']"
1928106,How many passes does a monkey saddle have?,"John Stillwell writes in Poincaré's Papers on Topology : The study of ""pits, peaks, and passes"" on surfaces in $\mathbb R^3$ by Cayley (1859) and Maxwell (1870). A family of parallel planes in $\mathbb R^3$ intersects a surface $S$ in curves we may view as curves of ""constant height"" (contour lines) on $S$. If the planes are taken to be in general position, and the surface is smooth, then $S$ has only finitely many ""pits, peaks, and passes"" relative to the height function. It turns out that
  $$\text{number of peaks} - \text{number of passes} + \text{number of pits}$$
  is precisely the Euler characteristic of $S$. So a peak is a local maximum, a pit is a local minimum. But what is a pass? From the wiki on mountain pass I guess that a pass is a path from one hollow to another hollow. Say, we have a saddle, then we have one pass. But what if we have a monkey saddle and thus three hollows adjacent to each other: How many passes do we have here and what are they exactly? Edit : From ""Surface Topology"" by Firby and Gardiner: A pass moves from a hollow, through the critical point, into the adjacent hollow. Hence, the number of passes through the critical point is one less than the number of hollows adjacent to the critical point. I guess we have three hollows adjacent to the critical point. So we should have two passes. But what and where are they?","['general-topology', 'differential-geometry', 'differential-topology', 'definition']"
1928116,Nullspace of a Matrix and Linear Transformation Matrix,"I'm having some issues to answer the following question: Consider the subspace of S of $\mathbb{R}$ generated by columns of the matrix A $$
        \begin{pmatrix}
        1 & 1 & 1 \\
        1 & 2 & 3 \\
        2 & 3 & 4 \\
        \end{pmatrix}
$$ Find the real numbers a, b, c for wich 
S = {(x, y, z) ∈ $\mathbb{R^3}$ : ax + by + cz = 0. My tentative to solve the question was put the matrix in the echelon form and then solving the equation A.x = 0
Is this the right approach?","['matrices', 'linear-algebra', 'linear-transformations']"
1928251,"Change of measure to make things ""easier""?","I am familiar with the Radon-Nikodym Theorem and an R/N Derivative, but while reading a set of lecture notes on: Stochastic Calculus, Filtering, and Stochastic Control in section 1.6: ""Induced measures, independence, and absolute continuity"", (bottom of page 42, emphasis mine): Absolutely continuous measures and the Radon-Nikodym theorem Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a given probability space.
  It is often interesting to try to find other measures on $\mathcal{F}$
  with different properties. We may have gone through some trouble to
  construct a measure $\mathbb{P}$, but once we have such a measure, we
  can generate a large family of related measures using a rather simple
  technique. This idea will come in very handy in many situations; calculations which are difficult under one measure can often become
  very simple if we change to a suitably modiﬁed measure (for example,
  if $\{X_n\}$ is a collection of random variables with some complicated
  dependencies under $\mathbb{P}$, it may be advantageous to compute
  using a modiﬁed measure $\mathbb{Q}$ under which the $X_n$ are
  independent. Later on, the change of measure concept will form the basis form one of the most basic tools in our stochastic toolbox, the Girsanov theorem. I'm having trouble conceptualizing the idea that ""calculations which are difficult under one measure can often become very simple if we change to a suitably modiﬁed measure"". Can someone further explain, perhaps by a couple examples, how this is the case? How can $X_n$ have complicated dependencies under $\mathbb{P}$, but simple (independent) dependencies under a $\mathbb{Q}$?","['probability-theory', 'examples-counterexamples', 'probability', 'measure-theory']"
1928286,Convergence/divergence of $\sum_{k=1}^\infty\frac{2\times 4\times 6\times\cdots\times(2k)}{1\times 3\times 5\times\cdots\times(2k-1)}$,"A problem asks me to determine if the series $$\sum_{k=1}^\infty \frac{2 \times 4 \times 6 \times \cdots \times (2k)}{1 \times 3 \times 5 \times \cdots \times (2k-1)}$$ converges or diverges. (from the textbook Calculus by Laura Taalman and Peter Kohn (2014 edition); section 7.6, p. 639, problem 33) I am allowed to use the ratio test first and then any other convergence/divergence test if the former test does not work. In my original work, I attempted the ratio test and it was rendered inconclusive. $$ a_k = \frac{2 \times 4 \times 6 \times \cdots \times (2k)}{1 \times 3 \times 5 \times \cdots \times (2k-1)} $$ $$ a_{k + 1} = \frac{2 \times 4 \times 6 \times \cdots \times (2k) \times (2(k+1))}{1 \times 3 \times 5 \times \cdots \times (2k-1) \times (2(k+1)-1)} = \frac{2 \times 4 \times 6 \times \cdots \times (2k) \times (2k+2)}{1 \times 3 \times 5 \times \cdots \times (2k-1) \times (2k+1)} $$ $$ \frac{a_{k + 1}}{a_k} = \frac{\frac{2 \times 4 \times 6 \times \cdots \times (2k) \times (2k+2)}{1 \times 3 \times 5 \times \cdots \times (2k-1) \times (2k+1)}}{\frac{2 \times 4 \times 6 \times \cdots \times (2k)}{1 \times 3 \times 5 \times \cdots \times (2k-1)}} = \frac{2 \times 4 \times 6 \times \cdots \times (2k) \times (2k+2)}{1 \times 3 \times 5 \times \cdots \times (2k-1) \times (2k+1)} \times \frac{1 \times 3 \times 5 \times \cdots \times (2k-1)}{2 \times 4 \times 6 \times \cdots \times (2k)} = \frac{2k+2}{2k+1}$$ Evaluating $\rho = \lim_{x \to \infty} \frac{a_{k + 1}}{a_k}$ will determine if $\sum_{k=1}^\infty \frac{2 \times 4 \times 6 \times \cdots \times (2k)}{1 \times 3 \times 5 \times \cdots \times (2k-1)}$ converges or diverges. The conclusions for the ratio test are as follows: $\circ$ If $\rho < 1$, then $\sum_{k=1}^\infty a_k$ converges. $\circ$ If $\rho > 1$, then $\sum_{k=1}^\infty a_k$ diverges. $\circ$ If $\rho = 1$, then the test is inconclusive. $$ \rho = \lim_{x \to \infty} \frac{a_{k + 1}}{a_k} = \lim_{x \to \infty} \frac{2k+2}{2k+1} = 1$$ Since $\rho = 1$, the ratio test is rendered inconclusive, as I stated earlier. I will have to use other convergence/divergence tests to solve the problem. My issue is that I'm not sure which other convergence/divergence test to use. Any suggestions? Many thanks for the help.","['sequences-and-series', 'calculus']"
1928329,Prove $\int_0^{\pi/2}{\frac{1+2\cos 2x\cdot\ln\tan x}{1+\tan^{2\sqrt{2}} x}}\tan^{1/\sqrt{2}} x~dx=0$,"I'm curious, how one can prove the following integral
$$
\int_0^{\pi/2}{\frac{1+2\cos 2x\cdot\ln\tan x}{1+\tan^{2\sqrt{2}} x}}\tan^{1/\sqrt{2}} x~dx=0
$$
Here is the Wolfram Alpha computation which shows that it is correct to at least 45 digits. My attempt: I knew the integral
$$
\int_0^{\pi/2}\frac{1}{1+\tan^\alpha\phi}d\phi=\int_0^{\pi/4}\frac{1}{1+\tan^\alpha\phi}d\phi+\int_0^{\pi/4}\frac{\tan^\alpha\phi}{1+\tan^\alpha\phi}d\phi=\frac{\pi}{4}
$$
which can be calculated for all values of $\alpha$. I tried to find an analogous symmetry that will allow me to cancel all the terms also in this case, but so far no luck. I also suspect that this integral might be related to derivative of Herglotz integral . Herglotz showed that 
$$
\int_{0}^{1}
\frac{\ln\left(1 + t^{\,{\large\alpha}}\right)}{1 + t}\,{\rm d}t
$$
can be computed for some algebraic values of $\alpha$, e.g. $\alpha=4+\sqrt{5}$.
If we take derivative of this integral with respect to $\alpha$ then we get
$$
\int_{0}^{1}
\frac{t^\alpha\ln t}{(1 + t)(1+t^\alpha)}\,{\rm d}t
$$
Change of variables $t=\tan^2\phi$ gives
$$
4\int_{0}^{\pi/4}
\frac{\tan^{2\alpha+1}\phi\cdot\ln \tan\phi}{1+\tan^{2\alpha}\phi}\,{\rm d}\phi
$$
which looks quite similar to the integral under consideration.","['integration', 'definite-integrals', 'calculus', 'closed-form']"
1928345,Are topological manifolds with boundary metrizable?,It is standard that topological manifolds (without boundary) are metrizable. Is the same true for manifolds with boundary?. I'm using the following definition: Let $\mathbb{R}^n_{x_n\ge 0}=\{x\in \mathbb{R^n}:x_n\ge 0\}$. A topological manifold with boundary is a paracompact hausdorff topological space $M$ such that every point $p\in M$ is contained in some open set $U_p$ that is homeomorphic to an open subset of $\mathbb{R}^n_{x_n\ge 0}$. It'd be nice to have some reference. The only reference about this I've found is John Lee's Introduction to Smooth manifolds but this deals with smooth manifolds.,"['manifolds', 'differential-geometry', 'manifolds-with-boundary']"
1928394,Closed formula for the sums $\sum\limits_{1 \le i_1 < i_2 < \dots < i_k \le n} i_1 i_2 \cdots i_k $?,"I've worked out a few summation formulas, and I am hoping to find a pattern. Unless I have made a mistake somewhere, we have the following identities: $$\sum_{1 \le i \le n} i = \frac{(n+1)n}{2} $$ $$\sum_{1 \le i < j \le n} ij = \frac{(3n+2)(n+1) \, n \, ( n-1)}{24}$$ $$\sum_{1 \le i < j < k \le n} ijk = \frac{(n+1)^2 \, n^2 \, (n-1)(n-2) }{48}$$ It seems clear that
$$p_k(n)= \sum_{1 \le i_1 < i_2 < \dots < i_k \le n} i_1 i_2 \cdots i_k $$ is a polynomial in $n$ of degree $2k$. But is there a nice closed-form formula for it? Maybe in terms of its factorization? Comment: I realize that what I am looking for is the coefficient of $t^k$ in the expansion
$$(1+t)(1+2t) \dots (1+nt),$$
so maybe generating function techniques could be helpful. But the main way I know to extract the coefficient of $t^k$ is to take derivatives $k-1$ times and on the surface of things that looks like a huge mess.","['stirling-numbers', 'closed-form', 'generating-functions', 'combinatorics', 'summation']"
1928399,List all the subsets of a set containing an empty set,"I would like someone to confirm whether this is correct or not. Thanks! A = { 1, 2, ∅ }
list all the subsets of set A. since the number of subsets in one set = 2^(cardinality of the set), and
the cardinality of 3 in set A. We have 2^(3) = 8, 8 possible subsets. all 8 possible subsets are : {1}, {2}, {∅}, {1, 2}, {1, ∅}, {2, ∅}, {1, 2, ∅}, and ∅","['self-learning', 'elementary-set-theory']"
1928451,The largest field contained in $GL_2(\mathbb{R})$,"It seems pretty intuitive that the complex numbers can be represented as elements in $GL_2(\mathbb{R})$ by identifying $$ a+bi \equiv \begin{bmatrix} a & -b\\b & a \end{bmatrix}$$
And additionally including the matrix $\begin{bmatrix} 0&0\\0&0 \end{bmatrix} $ since it is not invertible. So it appears that this set in fact forms a field under matrix addition and matrix multiplication. My question is, can we say that this is the largest field we can obtain out of elements of $GL_2(\mathbb{R}) \cup \{0\} $? If yes, would there be some extension of such a procedure to higher dimensions for say $GL_n(\mathbb{R})$ in which we can get fields out of $\mathbb{R}$-vector spaces? Or can it be shown that it is not possible? Some initial thoughts on the case for $GL_2(\mathbb{R})$: I can see that $GL_2(\mathbb{R})$ itself does not form a field since it is not closed under addition and additional constraints would be needed to ensure $det(A + B)$ is non-zero given $det(A)$ and $det(B)$ are non-zero. 
The condition that: $$det (A_1 + A_2) = det(\begin{bmatrix} a_1&b_1\\c_1&d_1 \end{bmatrix} + \begin{bmatrix} a_2&b_2\\c_2&d_2 \end{bmatrix}) \neq 0$$ Seems to give me:
$$\det(A_1) + det(A_2) \neq -(a_1d_2 - b_1c_2 + a_2d_1 - b_2c_1)$$
Which seems like a pretty odd condition since it would have to be true regardless of choice of $A_1$ and $A_2$. Not sure if this is the right approach... Maybe we could obtain a lower bound on $det(A_1 + A_2)$ in terms of $det(A_1)$ and $det(A_2)$ by restricting ourselves to matrices of positive determinant along with some extra condition. Clearly skew symmetric matrices with identical diagonal entries works, so maybe positive entries on the diagonal plus skew-symmetric is a step forward?","['matrices', 'abstract-algebra', 'field-theory', 'linear-algebra']"
1928460,Kissing number for equal ellipses on the plane,"It's quite easy to determine maximum kissing number for circles (disks) on a plane. Since we have circumference $C=2 \pi r$, we just take the integer part of $2\pi$, which is $6$. However, for ellipses the problem seems to be much more complicated. To get maximum kissing number we need to arrange the ellipses on 'pointy ends' around the central ellipse. Obviously, for large $a/b$ ratio, the kissing number will be large as well, approaching infinity when the ellipse becomes a line segment. This is how (awkwardly) the packing is supposed to look: The only approximation I could think of is to us the length of an ellipse formula, imagining the centers of surrounding ellipses lying on another ellipse. Let $a,b$ be the major and minor axes (radii) of the ellipses we are packing. Then the kissing number will approximately be: $$N \left( \frac{a}{b} \right) \approx \left[\frac{4A}{2b} E \left(1-\frac{B^2}{A^2} \right) \right]=\left[\frac{4a}{b} E \left(1-\frac{(a+b)^2}{4a^2} \right) \right]$$ Here $A=2a$ and $B=a+b$ are the major and minor axes of the 'external' ellipse, on which the other ellipses are arranged. $E$ is complete elliptic integral of the second kind and Mathematica notation is used for the parameter, i.e.: $$E(m)=\int_0^{\pi/2} \sqrt{1-m \sin^2 x}dx$$ The approximation is very close to $5 a/b$, as we can see from the plot: But how good is this approximation? And how to obtain a better estimate (or even exact value) for the maximum kissing number for arbitrary $a/b$ ratio?","['conic-sections', 'packing-problem', 'geometry']"
1928473,Finding the smallest value of the sum,"The multiplication of three natural numbers $a$, $b$ and $c$ equals $2016$. What is the smallest value of $a + b + c$? I started by a prime factorization to find that $2016 = 2^5 \cdot 3^2 \cdot 7$.
What should I do next?","['number-theory', 'prime-factorization', 'optimization']"
1928500,Real analysis contradiction I cannot get rid of,"Let $G$ be the Cantor set. It is well known that: $G$ is perfect and hence closed. $G$ has the cardinality of the continuum. $G$ has measure zero. For any set $S \subset \mathbb{R}$ (I will not keep writing that we are in $\mathbb{R}$) we have $S \text{ is closed} \Leftrightarrow S^c \text{ is open}$. Any open set $O$ can be written as a --- in fact unique --- countable union of disjoint open intervals. $G^c$ can thus be written as a countable union of disjoint open intervals. We now imagine this union as being superposed on the real line graphically as follows: R: <<<----(....)---(..)--(.)---------(...)--->>> where the (...) represents the open disjoint intervals (of differing size) composing $G^c$, and the --- represents the remaining non-covered real numbers (that is those in $G$). Now we can cover $\mathbb{R}$ in its entirety by ""collecting"" the --- into disjoint closed intervals. Any one of these closed intervals might of course consist of only a single element. We now obtain: R: <<<[--](....)[-](..)[](.)[-------](...)[-]>>> Take the union of these disjoint closed intervals. This must be $(G^c)^c = G$. Now it is not hard to imagine a mapping from the (...) 's to the [...] 's. Just take the next (...) in line for each [...] (and do some trivial fixing at the ends). Therefore we have written $G$ as a countable union of disjoint closed sets. However, $G$ has measure zero and therefore cannot contain any closed sets other than the single element type. Hence $G$ is countable. Contradiction. Where do I go wrong?","['real-analysis', 'measure-theory', 'cantor-set']"
1928532,Number of roots of $x^4 + x^3 \sin(x) + x^2 \cos(x) = 0$,"How many roots do the following polynomial have? $$x^4 + x^3 \sin(x) + x^2 \cos(x)$$ Obviously $0$ is a root. Dividing by $x^2$, I am left with $$x^2 + x\sin(x) + \cos(x)$$ How do I know this polynomial has no root?","['algebra-precalculus', 'calculus', 'gre-exam']"
1928533,Is there an alternating group on any given infinite set?,"Given any infinite set $X$ does there exist a surjective group homomorphism $\varepsilon: \mathrm{Perm}(X) \twoheadrightarrow \{+1, -1\}$? Here, $\mathrm{Perm}(X)$ denotes the group under composition of all permutations of $X$. If such a homomorphism exists, then is it unique?","['combinatorics', 'group-theory']"
1928538,Why is $\int_{-\infty}^\infty \frac{e^{itx}}{(x+i)^2}dx$ different when $t<0$ and $t >0$?,"Question: Evaluate the integrals 
$$I(t) = \int_{-\infty}^\infty \frac{e^{itx}}{(x+i)^2}dx, \ \ \ -\infty < t < \infty.$$ My attempt: I have seen the answer to this question on this site and elsewhere, and I understand most of the algebra/calc steps given in those solutions, but I'm very confused about something. So, to evaluate the integral when $t > 0$, we use the basic semi-circle in the upper half plane contour, noting that the integral of 
$$f(z) = \frac{e^{itz}}{(z+i)^2}$$
is $0$ over that contour, and thus the integral over the real line must be $0$. That makes sense to be, but I am confused as to why we can't just do the same thing when $t < 0$. I can see that then we would have:
$$f(z) = \frac{e^{-i|t|z}}{(z+i)^2},$$
but isn't this function still holomorphic in the semi-circle contour? And it seems like the bound
$$\frac{|e^{-i|t|z}|}{|z+i|^2} \le \frac{1}{(R+1)^2}$$
still applies over the semi-circle contour, which should give the same result, that the integral over the real line is $0$. I know I'm just being dumb here, but this is clearly a huge conceptual gap in my knowledge and I am hoping that someone can help explain where I'm going wrong. Thanks! Note: It might possibly have to do with switching up sines and cosines when $t$ is negative in the Euler's formula? Not sure though.","['complex-analysis', 'residue-calculus', 'complex-integration']"
1928557,What's my mistake in this integration??? $\int \sqrt{3-2x-x^2} dx$,"This is how I'm trying to integrate this function: \begin{align*}
\int \sqrt{3-2x-x^2}\, dx &= \int \sqrt{4-(x+1)^2}\, dx \\
&= \int \sqrt{2^2-(x+1)^2}\, dx
\end{align*}
Here I make the substitution: $$ u=x+1 $$
$$du=dx$$ So the integral is now: $$ \int \sqrt{2^2-u^2}\, du $$ I make a trigonometric substitution thinking about a right triangle where the hypothenuse is $2$, the adjacent side is $u$, the opposite side is $\sqrt{2^2-u^2}$, and the angle is called $\theta$. $$ \sin(\theta)= \frac{\sqrt{2^2-u^2}}{ 2}$$
$$\bbox[2px,border:2px solid red]
{ 2\sin(\theta)= \sqrt{2^2-u^2}\qquad  
}$$ $$\frac{u}{2} =\cos(\theta)$$
$$ u=2 \cos(\theta)$$
$$ \bbox[2px,border:2px solid red]
{du=-2 \sin(\theta)\,d\theta\qquad  
}$$ So I write the integral as:
\begin{align*}
 \int 2\sin(\theta)(-2)\sin(\theta)\,d\theta &= \int (-4)\sin(\theta)\sin(\theta)\,d\theta \\
&= \int (-4){\sin}^2(\theta)\,d\theta \\
&= (-4)\int {\sin}^2(\theta)\,d\theta \\
&= (-4)\int \frac{1}{2}(1-\cos(2\theta))\,d\theta \\
&= (-4)\frac{1}{2}\int (1-\cos(2\theta))\,d\theta \\
&= (-2)\int (1-cos(2\theta))\,d\theta \\
&= (-2)\left[\int d\theta-\int \cos(2\theta)\,d\theta \right]\\
&= (-2)\left[\theta-\int \cos(2\theta)\,d\theta \right]\\
&= (-2)\left[\theta-\frac{1}{2} \sin(2\theta) \right]\\
&= \sin(2\theta) -2\theta
\end{align*}
And since $\cos(\theta) = u/2$, I know that $\theta =\arccos(u/2)$. Therefore I have: $$ \sin(2\arccos(u/2)) -2\arccos(u/2)$$ According to my first substitution $u=x+1$ so the final result is: $$\bbox[2px,border:2px solid red] {\sin\left(2\arccos\left(\frac{x+1}{2}\right)\right) -2\arccos\left(\frac{x+1}{2}\right) + constant }\qquad$$ Can anyone help me? I don't understand what I'm doing wrong. Thanks!","['integration', 'trigonometry', 'calculus']"
1928572,Old Putnam problem that I could use some help on,"I've been beginning to practice for the Putnam this year, and I came across a
problem from a few years back that I could use some help on. I want to find the
values of $\alpha$ for which the curves $y = \alpha x^2 + \alpha x +
\frac{1}{24}$ and $x = \alpha y^2 + \alpha y + \frac{1}{24}$ are tangent to
each other. Hence, I believe that what I am looking for is all of the $\alpha$s such that
the defined curves intersect at one point and only one point (based on the
tangential aspect of this problem). Say this point is $(x_1,y_1)$ we need to
show that this point uniquely satisfies
$y_1 = \alpha x_1^2 + \alpha x_1 + \frac{1}{24}$ and
$x_1 = \alpha y_1^2 + \alpha y_1 + \frac{1}{24}.$ What I had imagined that we
would need to do is plug one equation into the other, and find the $\alpha$s
such that the given equation has only one root (although, this is primarily
just a hypothesis of what may work). So far, I set
$$y_1 = \alpha (\alpha y_1^2 + \alpha y_1 + \frac{1}{24})^2
+ \alpha (\alpha y_1^2 + \alpha y_1 + \frac{1}{24}) + \frac{1}{24}$$
Although I am having some difficulties figuring out what may be the best way
to manner to simplify this equation, given that it will end up having one side
with a polynomial of degree $4$ (which seems somewhat unweildy). Any
recommendations?","['contest-math', 'functions', 'analytic-geometry']"
1928591,"$|\arctan(x)-\arctan(y)| $for all reals $x,y$ is it bounded?","for all $x,y\in \mathbb{R}$ is $$|\arctan(x)-\arctan(y)| $$
bounded? plotted $\arctan(x)$ in wolfram graphically $|\arctan(x)| < 3 $ so $$ |\arctan(x)-\arctan(y)| < |\arctan(x)|+|\arctan(y)|< 6$$ I do know the highest slope of unit circle is  at angle $\pi /2 $ which seems to be the highest. It is not elegeant using 6. Not sure how to say it elegangly that it is bounded Is it just from definion of arctan?","['real-analysis', 'trigonometry']"
1928631,Is $\mathbb{Q}$ isomorphic to $\mathbb{Z^2}$?,"Most of us are aware of the fact that $\mathbb{C}$ is isomorphic to $\mathbb{R^2}$, as we can define $\mathbb{C}$ as follows : $$\mathbb{C} := \left\{z : z=x+iy \ \ \ \text{where} \ \ \langle x,y \rangle \in \mathbb{R^2}\right\}$$ Now this implies $$\mathbb{C} \cong \mathbb{R^2} $$ But, by this same argument, can't we make the same argument for $\mathbb{Q}$, the set of rational numbers and  $\mathbb{Z^2}$? We can define $\mathbb{Q}$ as follows: $$\mathbb{Q} := \left\{\frac{m}{n} :  \langle m,n \rangle \in \mathbb{Z^2} \ \ \land \ n \neq 0 \right\}$$ Which would imply $$\mathbb{Q} \cong \mathbb{Z^2} $$ And we could go further and explain that $\{\mathbb{Q}\} = \mathbb{Z}r\mathbb{Z} - 0$, where $r$ is a relation on $\mathbb{Z}$. So is $\mathbb{Q}$ isomorphic to $\mathbb{Z^2}$?","['abstract-algebra', 'real-numbers', 'rational-numbers']"
1928651,How to show if this function satisfies the wave equation?,"If F and G have second partial derivatives, show that $U(x,t)=F(x+at)+G(x-at)$ satisfies the wave equation:
$$a^{2}\frac{\partial^2U}{\partial x^2}=\frac{\partial^2 U}{\partial t^2}$$.
Sol: $$\\$$
I first say that $s=x+at$ and $w=x-at$, then i try to use the chain rule $$\frac{\partial U}{\partial t}=\frac{\partial U}{\partial s}\frac{\partial s}{\partial t}+ \frac{\partial U}{\partial w}\frac{\partial w}{\partial t}$$
$$\frac{\partial U}{\partial t}=\frac{\partial (F+G)}{\partial s}\frac{\partial s}{\partial t}+ \frac{\partial (F+G)}{\partial w}\frac{\partial w}{\partial t}$$
For propierties: 
$$\frac{\partial U}{\partial t}=\left( \frac{\partial F}{\partial s}+\frac{\partial G}{\partial s} \right) \frac{\partial s}{\partial t}+ \left( \frac{\partial F}{\partial w}+\frac{\partial G}{\partial w} \right)\frac{\partial w}{\partial t}$$
$$\frac{\partial U}{\partial t}=a\left( \frac{\partial F}{\partial s}+\frac{\partial G}{\partial s} \right) - a\left( \frac{\partial F}{\partial w}+\frac{\partial G}{\partial w} \right)$$
$$\frac{\partial^2 U}{\partial t^2}=a\frac{\partial}{\partial t}\left( \frac{\partial F}{\partial s}+\frac{\partial G}{\partial s} \right) - a\frac{\partial}{\partial t}\left( \frac{\partial F}{\partial w}+\frac{\partial G}{\partial w} \right)$$
$$\frac{\partial^2 U}{\partial t^2}=a\left( \frac{\partial F}{\partial t\partial s}+\frac{\partial G}{\partial t\partial s} \right) - a\left( \frac{\partial F}{\partial t\partial w}+\frac{\partial G}{\partial t\partial w} \right)$$ I do not know what to do in this part. I appreciate help.","['multivariable-calculus', 'partial-derivative', 'chain-rule']"
1928668,Proving Sauer-Shelah's theorum,"I've been working on some problems in introductory combinatorics, and I would
like some help on going about proving Sauer-Shelah's theorem. A family $\mathcal{F}$ shatters a set $A$ if for every $B \subset A$, there is
an $F \in \mathcal{F}$ such that $F \cap A = B.$ I want to show that if
$\mathcal{F} \subset 2^{[n]}$ (i.e. the power set of $[n]$) and
$|\mathcal{F}| > \sum_{i=0}^k \binom{n}{i}$, then there is a set $A \subset
[n]$ of size $k+1$ such that $\mathcal{F}$ shatters $A$. I started this by going about an ""assume for the sake of contradiction"" proof in
which I stated that for all sets $A \subset [n]$ of size $k+1$, $\mathcal{F}$
does not shatter $A$. Thus, for every $A$ that satisfies the above, there
exists a $B \subset A$ such that there is no $F \in \mathcal{F}$ such that
$F \cap A = B.$ The problem is, I am having some difficulty figuring out how
the construction of $\mathcal{F}$ makes meaningful implications about the
contents of $B$ in this situation. Is it possible that I should go about this
proof directly? Any recommendations would be appreciated.","['statistics', 'discrete-mathematics']"
1928713,Closed Range Convolution Operator,Does there exists a nontrivial $f \in L^1(\mathbb{R})\cap L^\infty(\mathbb{R})$ such that $f\ast L^\infty(\mathbb{R})$ is a closed subspace of $L^\infty(\mathbb{R})$? I couldn't find any good reference on closed range convolution operators.,"['functional-analysis', 'convolution', 'operator-theory', 'open-map']"
1928726,"In Proposition 1.15 of Folland's Real Analysis, why can we assume I = (a,b]? (on page 34)","I understand why the function $\mu_0$, which we would like to show is a premeasure on the algebra $\mathbb{A}= \{$finite disjoint unions of h-intervals$\}$, is well-defined on this set. It's also clear that this function takes the empty set to $0$ by it's definition in Folland. To show that $\mu_0$ is premeasure, we then must let a sequence $\{I_j\}_{j=1}^\infty$ of disjoint elements of $\mathbb{A}$ be given with $I=\bigcup_{j=1}^\infty I_j\in\mathbb{A}$ and show that $\mu_0(\bigcup_{j=1}^\infty I_j) = \sum_{j=1}^\infty\mu_0(I_j)$. In proving the first inequality, I understand that since $I\in\mathbb{A}$, it can be written as the finite disjoint union of h-intervals, but Folland then writes that we can assume $I=(a,b]$. I don't quite understand his explanation of how we are allowed to assume $I=(a,b]$ and I am wondering if someone would be willing to explain this step of the proof to me. Thanks in advance!","['real-analysis', 'measure-theory']"
1928808,How can a function have multiple outputs?,Quoting Wikipedia (from this page)- Functions with multiple outputs are often written as vector-valued functions. How is it possible for a function to give multiple outputs?,"['vectors', 'functions']"
1928818,Difference between a convex hull and alpha shape,"What is the difference between a convex hull and alpha shape? What I get to understand from wiki , is that, alpha shape generates a set of boundary points such that together they form a smaller surface area when considering the selected points than a normal convex hull. I am unable to understand the difference mathematically. Can someone please explain it in a simple way if possible?","['optimization', 'convex-hulls', 'convex-optimization', 'euclidean-geometry', 'geometry']"
1928859,Is argument of a function and independent variable essesntially the same things?,"From the top of this page of Wikipedia- In mathematics, an argument of a function is a specific input in the function, also known as an independent variable. From the same page- A mathematical function has one or more arguments in the form of independent variables designated in the function's definition. Again from the same page- The independent variables are mentioned in the list of arguments that the function takes. So, I gets confused about the difference of these terms. It seems to me that they should be different but Wikipedia is not agreeing. Can you help me with this?","['terminology', 'functions']"
1928892,"How do I prove that for $ x,y,z > 0$ $\frac{x}{y+z} + \frac{y}{x+z} + \frac{z}{x+y} > 3/2$?","I'm solving a graduate entrance examination problem. 
We are required to establish the inequality using the following result: for $x,y > 0$, $\frac{x}{y} + \frac{y}{x} > 2$ (1), which is easy to prove as it is equivalent to $(x - y)^2 > 0$. But when it comes to an inequality combining $x, y, z$, I got stuck as I've tried to develop the expression into one single fraction and obtain something irreducible. Any hints ? My intuition tells me that for $x,y,z >0$, any fraction of the form $\frac{x}{y+z}$ is greater than 1/2. As there are three fractions of this kind with mute variables playing symmetrical roles, we get: $1/2 + 1/2 + 1/2 = 3/2$. I just don't figure out how to play with the result (1).","['algebra-precalculus', 'inequality']"
1928923,Gap between an even integer and the next smaller prime?,"I am desparately searching for a case that would skip the following conjecture (a variation of the Goldbach conjecture): ""Let $N$ an even integer, $P$ the very next prime smaller than $N$, and $D=N-P$. Then $D$ is always a prime. (Except $D=1$)"" Can anybody help me with a case to reject this conjecture? Thank you in advance.","['number-theory', 'goldbachs-conjecture', 'prime-numbers', 'elementary-number-theory']"
1928924,Hausdorff-dimensions and self-similarity,"While reading around online earlier today, I managed to stumble onto a strange claim, that I have not been able to find or produce a proof for. The claim is that if $X$ is a compact subset of $\mathbb R^n$, which can be written as a disjoint union of some $X_i$ where $X_i$ is isometric to a dilation of $X$ by some constant $c_i$, then the Hausdorff dimension has the property that $\sum c_i^d = 1$. I know the definition of Hausdorff dimension, and so I was able to verify that this is true for some examples that I know off the top of my head, i.e. Cantor sets in $\mathbb R$, Sierpinski fractals, etc. But I have not been able to come up with a proof. I expect it is not a hard theorem, because a google search tells me that more technical versions of this theorem can be stated, with all kinds of overlap conditions on the $X_i$ and whatnot, but none of this is necessary, so somehow I think it should not be hard.. but I cannot figure it out. Any help is appreciated.","['hausdorff-measure', 'general-topology', 'measure-theory', 'fractals']"
1928938,How do I show a random variable converges in probability?,"I've been given the following problem: For $n \in \mathbb{N}^{*}$ , let $X_{n}$ be a random variable such that $\mathbb{P} \left[ X_{n} = \frac{1}{n} \right] = 1 - \frac{1}{n^{2}}$ and $\mathbb{P} \left[ X_{n} = n \right] = \frac{1}{n^{2}}$ . Does $X_{n}$ converge in probability? And the definition I've been given for convergence in probability of a random variable is: Let $\left( T_{n} \right)_{n \geq 1}$ be a sequence of r.v. and $T$ a r.v. ( $T$ may be deterministic). Then: $T_{n}\overset{\mathbb{P}}{\underset{n \rightarrow \infty}\rightarrow} T$ if and only if $\mathbb{P} \left[ \left| T_{n} - T \right| \geq \epsilon \right] {\underset{n \rightarrow \infty}\rightarrow} 0$ , for all $\epsilon > 0$ . But I don't understand how to apply this definition to the problem. I'm not even sure what I'm supposed to be seeing if $X_{n}$ is converging to. We have no $X$ as far as I can see. Can anyone give me some insight on this?","['statistics', 'probability', 'convergence-divergence']"
1928954,Number of permutations of $n$ numbers,"What is the number of permutations of the first $N$ natural numbers such that for exactly $K \leq N$ of the numbers the following condition is true: Number $x$ is in position $x$ in the permutation Example : if $N = 3$, the numbers are $1,2,3$ Permutations $132$,$213$,$321$ have only $1$ element each in required position.
So, for $K=1$ and $N=3$, the required number of permutations is $3$. I tried using exclusion-inclusion principle, but I was able to find an answer only for $K=1$. For larger values of $K$, it is becoming too cumbersome. Please help !","['permutations', 'combinatorics', 'inclusion-exclusion', 'discrete-mathematics']"
1928971,Evaluating the limit of the sequence: $\frac{ 1^a + 2^a +..... n^a}{(n+1)^{a-1}[n^2a + n(n+1)/2]}$,"My friend gave me this question to solve a few days ago and after I got no way to solve this, I thought I should seek some help. I had to evaluate the limit of the following when $n$ tends to infinite. $$ \frac{ 1^a + 2^a +..... n^a}{(n+1)^{a-1}[n^2a + n(n+1)/2]}$$ I tried to convert this limit into a definite integral but I couldn't get the expression solely in the  terms of $(r/n)$ Any help on how to proceed will be appreciated. Note: I know to solve limit of certain  series by turning it into an integral from the  following form:
$$\sum\frac{f(r/n)}{n}$$
It turns into an integral of $f(r/n)$.","['integration', 'sequences-and-series', 'closed-form', 'limits']"
1928980,Awkward behavior of $x^2 \sin\frac{1}{x}$ at $x=0$?,"According to p. 107 of the book Advanced Calculus by Fitzpatrick, Assuming that the periodicity and differentiability properties of the sine function are familiar, the following is an example of a differentiable function having a positive derivative at $x = 0$ but such that there is no neighborhood of $0$ on which it is monotonically increasing:
  $$f(x) = \begin{cases}x^2\sin 1/x & \text{if}\ x\neq 0 \\
0 & \text{if}\ x = 0\end{cases}$$
  The source of this counterintuitive behavior is that the derivative $f'$ is not continuous at $x = 0$. There are two confusing things about it: a. By the definition of derivative (at $x=0$), $$\lim_{x\to 0} \dfrac{f(x)-f(0)}{x-0} = \lim_{x\to 0} \dfrac{x^2 \sin \bigl(\frac{1}{x}\bigr)}{x} = 0,$$ since $\bigl\lvert\sin\bigl(\frac{1}{x}\bigr)\bigr\rvert \le 1$. b. By use of rules for the derivative of products and quotients, $$f'(0) = \biggl[ 2x \sin \biggl(\frac{1}{x}\biggr) + x^2 \biggl(-\frac{1}{x^2} \cos \biggl(\frac{1}{x}\biggr)\biggr) \biggr]_{x=0} $$ which is not defined since $\cos\bigl(\frac{1}{0}\bigr)$ is not defined. So why the text says that its derivative exists and its value is $>0$? The function $f(x)$ is monotonically increasing because it is an odd function so for any neighborhood abound $x=0$, $f(x_1>0)>f(x_2<0)$. So why the text says otherwise?","['derivatives', 'real-analysis']"
1928997,Show that $\tan{(\pi/7)} \tan{(2\pi/7)}\tan{(3\pi/7)}=\sqrt{7}$,I tried in this way.$\tan(a+b)=\frac{(\tan a + \tan b)}{1 - \tan a \tan b }$value of $\tan \frac{\pi}{7}$ is coming in decimal.what to do,['trigonometry']
1929009,Find the gradient of unimodal function,"I am doing the following exercise: 
given the level lines of the unimodal function $f$ with minimum $x^*$, a point $x_0$, and vectors $v_1$,$v_2$, $v_3$, $v_4$, $v_5$, one of which is equal to $\nabla f(x_0)$, which vector $v_i$ is equal to $\nabla f(x_0)$? My answer is $v_5$, because I know that the gradient of a function in a point $x_0$ must be perpendicular to level line in that point and the only two perpendicular vectors are $v_1$ and $v_5$. Furthermore, the gradient points towards the direction of maximum increase and $v_1$ is going towards the minimum $x^*$, therefore the correct answer should be $v_5$. Am I correct or am I missing something?",['analysis']
1929014,Extracting vector containing the elements of the main diagonal of a matrix [duplicate],"This question already has answers here : Mathematical expression to form a vector from diagonal elements (2 answers) Closed 7 years ago . Is there any mathematical operation that would extract the elements of the main diagonal as a vector? i.e. multiply it by certain vectors or something like that. I'm using this in the context of linear systems. In the specific case I'm looking at I have a relationship between the elements of three vectors as follows: $ \bf{a} = \begin{bmatrix} a_{1} \\ a_{2} \\ a_{3} \\ a_{4} \end{bmatrix} $ , $ \bf{b} = \begin{bmatrix} b_{1} \\ b_{2} \\ b_{3} \\ b_{4} \end{bmatrix} $ , and $ \bf{c} = \begin{bmatrix} c_{1} \\ c_{2} \\ c_{3} \\ c_{4} \end{bmatrix} $ I also know that: $c_{i} = a_{i}b_{i} $ for $i \in [1, 4]$ Now I want to express this relationship as a vector equation. I understand that $\bf{a} \bf{b}^\top$ would give a square matrix with the elements of $\bf{c}$ on its main diagonal, but is there anyway to extract them as a vector? EDIT: Let me clarify a bit. If I multiply $\bf{a}$ by $\bf{b}^\top$ I get the following matrix: $\bf{a} \bf{b}^\top = \begin{bmatrix} \bf{a_1b_1} && a_1b_2 && a_1b_3 && a_1b_4 \\ a_2b_1 && \bf{a_2b_2} && a_2b_3 && a_2b_4 \\ a_3b_1 && a_3b_2 && \bf{a_3b_3} && a_3b_4 \\ a_4b_1 && a_4b_2 && a_4b_3 && \bf{a_4b_4}  \end{bmatrix} $ The elements which have been made bold are the ones I'm interested in extracting as a vector. This vector would be $\bf{c}$. If I multiply this by the all-ones vector, as some of the answers have suggested, I would get: $\bf{a} \bf{b}^\top \bf{1}= \begin{bmatrix} \bf{a_1b_1} && a_1b_2 && a_1b_3 && a_1b_4 \\ a_2b_1 && \bf{a_2b_2} && a_2b_3 && a_2b_4 \\ a_3b_1 && a_3b_2 && \bf{a_3b_3} && a_3b_4 \\ a_4b_1 && a_4b_2 && a_4b_3 && \bf{a_4b_4}  \end{bmatrix} \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} a_1b_1 + a_1b_2 + a_1b_3 + a_1b_4 \\ a_2b_1 + a_2b_2 + a_2b_3 + a_2b_4 \\ a_3b_1 + a_3b_2 + a_3b_3 + a_3b_4 \\ a_4b_1 + a_4b_2 + a_4b_3 + a_4b_4  \end{bmatrix}$ Which is not the vector I'm looking for (it isn't equal to $\bf{c}$). EDIT 2: Multiplying by the $\bf{1}$ vector would obviously work if all off diagonal elements become zero. So if anyone knows of a way to do that without modifying the elements of the main diagonal that would also answer my question. EDIT 3: The other question pointed out in the comments area is essentially the same and I have received similar answers but I was hoping for a simpler solution. I haven't marked it as duplicate to allow people to contribute in the future. I was hoping for a solution that would be linear in $\bf{b}$ which I would substitute in place of $\bf{c}$ into the equation I'm trying to solve. In that case $\bf{b}$ would be my only unknown and I would be able to get an algebraic solution.","['matrices', 'matrix-equations', 'linear-algebra', 'vectors']"
1929050,Proving a trigonometric expression is identical to $2(\operatorname{cosec}^{2}{B}-1).$,"I came across this trigonometric identity: $$\frac{\operatorname{cosec}{B} - \cot{B}}{\operatorname{cosec}{B} + \cot{B}} + \frac{\operatorname{cosec}{B} + \cot{B}}{\operatorname{cosec}{B} - \cot{B}} = 2(\operatorname{cosec}^{2}{B} - 1) = 2(\frac{1+\cos^{2}{B}}{1 - \cos^{2}{B}})$$ And as I solved it, the equation came down to: $2\operatorname{cosec}^{2}{B}+2\cot^{2}{B}.$ This can be written as $2(\operatorname{cosec}^{2}{B} + \cot^{2}{B}),$ which can further be written as $2\frac{1+\cos^{2}{B}}{1 - \cos^{2}{B}}.$ But I can't seem to get my mind around the middle part of the question, that is, $2(\operatorname{cosec}^{2}{B} - 1)$ Is it possible to write it like this, or is this an error in the question paper itself?","['algebra-precalculus', 'trigonometry']"
1929058,If $\det(AB)=4$ then find the value of $\det(BA)$ [duplicate],"This question already has answers here : When A and B are of different order given the $\det(AB)$,then calculate $\det(BA)$ (3 answers) Closed 7 years ago . Let $A$ be a $2 \times 3$ matrix with real entries and let $B$ be a $3 \times 2$ matrix with real entries. If $\det(AB)=4$ then find the value of $\det(BA)$ . My attempt: I am aware that $\det(AB)=\det(BA)$ when $A$ and $B$ are of same order. But how to do this?",['matrices']
1929087,"Chain rule for a least-square regression, where does the transpose come from?","I am reading a machine learning book, and I can't understand a couple of things from the following explanation: Suppose we want to ﬁnd the value of $x$ that minimizes $$f(x) = \frac{1}{2} \left \| Ax - b \right \|^2_2$$ There are specialized linear algebra algorithms that can solve this problem eﬃciently. However, we can also explore how to solve it using gradient-based optimization as a simple example of how these techniques work. First, we need to obtain the gradient: $$\triangledown_xf(x) = A^{T}(Ax-b)$$ ... Am I correct with my understanding that by "" specialized linear algebra algorithms "" they mean finding inverse (or pseudoinverse) and calculating $A^{-1}b$. But my main question is why do they have $A^{T}$ in the derivative? As far as I understood they apply chain rule, but I still can't understand where the transpose comes from.","['derivatives', 'linear-algebra']"
1929094,Linear algebra for computer scientists,I'm planning a linear algebra course for computer science freshmen. Do you have any good textbook to recommend?,"['matrices', 'linear-algebra']"
1929167,"Basic Statistics, confused. Events","There are two questions that I am having problems with. The first one is  this one: Event A: an even number turns up

             Event B: a 5 or 6 turns up A die is rolled once: The probability of getting a 1 is 1/21,
 a 2 is 2/21, a 3 is 3/21, a 4 is 4/21,a 5 is 5/21, and a 6 is 6/21. They ask you to find P(A), P(B), P(AUB), P(AnB), P(AlB), P(BlA), and if the events are independent. I have an idea of how to do it and my results were: P(A): 4/7 ---- P(b): 11/21 ----P(AnB): 6/21----- P(AUB) 17/21 -----P(AlB): 6/11 ------ P(BlA): 1/2 and the events to not be independent because (AnB) =/= P(A) x P(B) I am not exactly sure if I'm right. The second one is this one: An unfair coin (Probability of 0.6 for heads) is flipped twice. Given that on at least one flip a head occurred, what is the probability that a head occurred on both flips. So in my mind, this question is really simple, but the wording makes me believe that I am overlooking something and that it is a bit more complicated. am I supposed to do like an x chooses n / binomial formula thing. Thank you in advance to anyone that can help me.","['statistics', 'probability']"
1929186,Why are events $A$ and $B$ independent whilst events $A$ and $C$ are not?,Two ordinary fair dice (one red and one blue) are thrown. Event A: The red die will show a 5 or a 6. Event B: The sum of the two dice will be 7. Event C: The sum of the two dice will be 8. Using the test for independence: $$P(A\cap B) = P(A).P(B) $$ It can be seen that $A$ and $B$ are independent events whilst events $A$ and $C$ are not independent. I can't understand why this is the case. I understand the mathematics but I can't understand the logic behind it. I drew out the sample space but I am none the wiser. Is there some intrinsic difference between events $B$ and $C$ that results in one being independent and the other not?,['probability']
1929212,Both sides differentiation expression with integral,"I have a problem with derivation of some function. This is related to computing the Poincare Map of Logistic Population model with Periodic harvesting. This function is: $\phi(t,x_0) = x_0 + \int_{0}^{t} f(s,\phi(s,x_0)) ds $, where $\phi(t,x_0)$ is a function: $\mathbb{R}  \times \mathbb{R} \rightarrow \mathbb{R}$. I'm trying to differentiate  equation of this function both sides  with respect to $x_{0}$. In book from this equation comes from the result of this differentiation is: $\frac{\partial \phi}{\partial x_0} (t,x_0) = 1 + \int_0^{t} \frac{\partial f}{\partial x_0} (s, \phi(s,x_0)) \cdot \frac{\partial \phi}{\partial x_0} (s, x_0)ds$. I can't understand expression under the integral. In this book it is explained that it is clear from chain rule .Could someone explain to me this transformation in easy way? I will be grateful for your help
Best regards","['derivatives', 'integration', 'ordinary-differential-equations']"
1929217,Set Theory Computer Science A-Level Question Confusion,"I am studying for a Computer Science Degree and am using the book ""Essential Maths Skills for AS/A-Level Computer Science"" by Gavin Craddock and am on the Set Theory portion. There is a practice question that states: If: A = {1,2,3,4,5,6}
B = {2,4,6,8}
C = {6,7,8,9} Calculate: (C ∩ B) \ A I work this out as: 1. (C ∩ B) = {6,8}
2. {6,8} \ {1,2,3,4,5,6} = {8} However the answer paper gives this as an empty set {} or Ø What am I missing here?",['elementary-set-theory']
1929289,Why $4a^3+27b^2<0 \iff x^3+ax+b=0$ has exactly three solutions?,"According to Exc. 8 Sec. 4.3 of the book Advanced Calculus by Fitzpatrick, For numbers $a$ and $b$, prove that the following equation has exactly three solutions if and only $4a^3+27b^2<0$: $$ x^3+ax+b=0, \ \  \ \ \  x \ \text{in} \ \mathbb R.$$ Long time struggling, I can't prove neither directions. The book is pretty rudimentary and discussions like here are beyond the scope of the book. Let $f(x) = x^3 + ax + b$. So $f'(x) = 3x^2 + a$. Set $3x^2 + a = 0$, so that at the points $\sqrt{-a/3}$ and $-\sqrt{-a/3}$, $f'(x) = 0$. There exists exactly two maximum/minimum due to the existence of $3$ solutions. These $2$ max/min must be at $x = \sqrt{-a/3}$ and $x = -\sqrt{-a/3}$. Note that must $a<0$. Also must $f(-\sqrt{-a/3}) > 0$ and $f(\sqrt{-a/3})<0$. Putting $\pm \sqrt{-a/3}$ into $f(x)$, I fail to conclude $4a^3 + 27b^2 < 0$. As, $$-(-a/3)^{3/2}-a(-a/3)^{1/2}+b>0 \\ (-a/3)^{3/2}+a(-a/3)^{1/2}+b<0$$ results in $a<0$ and no more thing! Please help!","['algebra-precalculus', 'real-analysis']"
1929331,Approximation of square root,"I have a square root in a problem which needs to be approximated. I'm not entirely sure how to do this algebraically. $$ \sqrt{10^2-(6.9\times 10^{-2})^2}$$
The answer the problem is proposing as the approximation is $$ 10[1-\frac 1 2(6.9)^2\times10^{-6}]$$
This hasn't exactly been the most reputable textbook, however, so it could be wrong. My attempt: It should be able to approximated by the square-root of 100, because the other number is so small. $$ 10 -(6.9)^2\times 10^{-4} $$ I can see they factored a 10 out, so I go ahead and do that. $$ 10[1- \frac 1 {10}(6.9)^2\times 10^{-4}] $$
$$ 10[1-(6.9)^2\times 10^{-5}]  $$ I'm not sure where exactly I'm slipping up, or what I'm comprehending incorrectly, any help would be greatly appreciated.","['algebra-precalculus', 'approximation']"
1929365,Minimal polynomial of a matrix does not check Hamilton-Cayley theorem,"Hi there I have to calculate the minimal polynomial for the matrix 
$$ \begin{pmatrix}
2 & 0 & 0 &0 \\ 
1 &  3& 2 & 1\\ 
0 &  -1& 0 &-1 \\ 
 -1& 0 & 0 & 2
\end{pmatrix}$$ In oreder to do that I tried to put its characteristic matrix in the canonical diagonal form, and I got the matrix
$$\begin{pmatrix}
1 & 0 & 0 &0 \\ 
0 &  1& 0 & 0\\ 
0 &  0& -x^2+3x-2 &0\\ 
 0& 0 & 0 & (x-2)(2-x)
\end{pmatrix}$$. From here I see that the minimal polynomial is $ \mu_A(x) = (x-2)(2-x)$, but the problem is that this minimal polynomial dosen't check the Hamilton-Cayley theorem(i.e. $\mu_A(A) = 0$).","['matrices', 'cayley-hamilton', 'linear-algebra', 'minimal-polynomials']"
1929380,How do you prove $P(A) \cup P(B) = P(A \cup B) \Rightarrow (A \subseteq B) \lor (B \subseteq A)$,"I’m learning proofs on my own and I’m on the section about cases.  I have to prove the following but don’t how to do this.  Can someone tell me how you would prove this? (where $P()$ represents the power set):
$P(A) \cup P(B) = P(A \cup B) \Rightarrow (A \subseteq B) \lor (B \subseteq A)$ Your help is greatly appreciated! :)",['elementary-set-theory']
1929390,Polar to cartesian form of $r=\tan(2θ)$,"I have attempted converting $r=\tan(2θ)$ to cartesian coordinates: $$r=\frac{2\sin(θ)\cos(θ)}{\cos^2(θ)-\sin^2(θ)}$$
$$r=\frac{2r\sin(θ)r\cos(θ)}{r^2\cos^2(θ)-r^2\sin^2(θ)}$$ $r^2 = x^2 + y^2\\
x = r \cos \theta\\
y = r \sin \theta$ $$r=\frac{2xy}{x^2-y^2} $$
$$(x^2+y^2)^{1/2}=\frac{2xy}{x^2-y^2} $$
$$(x^2+y^2)= \left(\frac{2xy}{x^2-y^2}\right)^2 $$
This doesn't graph properly on Wolfram Alpha, so I must have made a mistake.",['trigonometry']
1929393,"Fourier method heat equation, partial time derivative","Let's consider the heat equation $u_t-u_{xx}=0$. A simple method to solve this PDE is to do a Fourier transform of the equation and get the ODE $\hat{u_t} + x^2 \hat u_{xx}=0$. We can solve this, do a back transformation and obtain a solution to our original PDE. Most people don't care why $\widehat{u_t} = \frac{d}{dt} \hat u$ is justified. When one writes it out, we get into the situation that we need to interchange limit with integration. This is not trivially justified. I would like to know in which situations we are justified in doing this. Someone gave me a hint to consider the question from another viewpoint, namely to consider the spatial Fourier transform as a Banach space values function and consider generalized derivatives, but this idea wasn't made more precise.","['functional-analysis', 'heat-equation', 'fourier-transform', 'partial-differential-equations']"
1929405,About holomorphic function which avoids $\mathbb R$,"I was asking myself whether or not the following statement is true, and I think it is. The conjecture Let $f$ be an entire function, i.e. $f$ holomorphic and $f:\mathbb C\to \mathbb C$. We assume that $$f(\mathbb C)\cap \mathbb R=\emptyset.$$ Then $f$ is constant. What I tried This statement came to me because of the Cauchy-Riemann equations. If $f$ is an holomorphic function ($f:\mathbb R^2\simeq \mathbb C\to \mathbb C$), $P(x,y)=\mathfrak R(f)$ and $Q=\mathfrak I(f)$ then $$\begin{cases} \displaystyle\frac{\partial P}{\partial x}=\frac{\partial Q}{\partial y} \\ \displaystyle\frac{\partial P}{\partial y}=-\frac{\partial Q}{\partial x}. \end{cases}$$ With those equations, we can say that if $f(\mathbb C)\subset \mathbb R$, then $f$ is constant (we then have both partial derivates of $Q$ which are null). May be we could find a solution here, because if $f(\mathbb C)\cap \mathbb R=\emptyset$, then $Q$ doesn't change sign. But we lack a lot of informations to conclude... May be we can use Liouville theorem to prove a partial result: if $$\vert f(x)\vert \xrightarrow[\vert z \vert \to \infty]{} +\infty$$ then $f$ is constant. But again, this is not really satisfying.",['holomorphic-functions']
1929445,Proving statement about the ODE $y'=y+y^4$,"Is there a solution to the problem $$\left\{\begin{matrix}
y'=y+y^4\\ 
y(x_0)=y_0
\end{matrix}\right.$$
  which is defined on $\mathbb{R}$? ($x_0,y_0$ might be any real numbers) It's easy to prove that for all $(x,y)\in\mathbb{R}^2$ there exists an open interval $I$ (with $x_0\in I$) where the problem has a unique solution. However is the maximal interval always $(-\infty,\infty)$? I know that the answer is no, but that's just because I found a solution for particular values of $x_0$ and $y_0$ and checked its domain. But is there a way to prove that $I$ need not be $I=(-\infty,\infty)$ without actually solving the problem for a certain initial condition? In other words, how can I prove that the unique solution need not be defined on $\mathbb{R}$?",['ordinary-differential-equations']
1929481,Check if polynomials are linearly independent.,"I would like to check if polynomials $1, 1+t^2, 1+t+t^2$ are linearly independent. My idea is: $1 \to [1,0,0]$ $1+t^2\to [1,1,0]$ $1+t^2+t^3 \to [1,1,1]$ And now $\left( \begin{array}{ccc}
1 & 0 & 1 \\
0 & 1 & 1 \\
0 & 1 & 1 \end{array} \right)$ I would like to find rank of this array. Rank of this array is $3$ so columns are linearly independent. Is it correct reasoning ?",['linear-algebra']
1929543,"Prove this function defined for a convex, bounded, open, symmetric set $V$ is a norm on $\mathbb{R}^n$","$V\subset \mathbb{R}^n $ be open, symmetric and contains 0 . Define $||\cdot||:\mathbb{R}^n \rightarrow \mathbb{R}$ as: \begin{equation}
||x||=\inf\{ \lambda > 0: \lambda^{-1}x\in V \} 
\end{equation}
  for all x $\in \mathbb{R}^n$. Prove this is a norm $\iff$ $V$ is convex and bounded with respect to the euclidean metric. Approach: $||x||\ge 0$ (Done) $||\alpha x||= |\alpha|||x||$ (not done) triangle in equality (not done)","['functional-analysis', 'normed-spaces', 'convex-analysis']"
1929552,"Let $p$ be a prime so $p\equiv3\pmod4$. If $p|a^2+b^2$, then $p|a,b$","Let $p$ be a prime so $p\equiv3\pmod4$. If $p| a^2+b^2$, then $p| a,b$ How do I prove this small theorem? I know that it's quite useful. Are there other small theorems like this one? I am mostly searching elementary proofs, so not involving to complicated stuff...","['number-theory', 'sums-of-squares', 'modular-arithmetic', 'elementary-number-theory']"
1929561,Evaluating an integral limit when L'Hospital's rule doesn't work.,"I want to evaluate the limit $$\lim_{n\to\infty}n\int_{1-\frac{1}{n}}^{1}f(x)dx.$$ I have seen that the solution to this is $f(1)$, but I don't see how. My idea was to use L'Hospistal's rule, but that didn't work.","['integration', 'limits-without-lhopital', 'analysis', 'limits']"
1929569,"Partial order on $\langle P(\mathbb{N}), \subseteq\rangle$ contains continuum-size chains and anti-chains","Consider the partially ordered set $\langle P(\mathbb{N}), \subseteq\rangle$. I am trying to show two things: (a) There exists a chain $C$ such that $|C|=\mathfrak{c}$. (b) There exists an anti-chain $A$ such that $|A|=\mathfrak{c}$. Can you help me with this ? It seems to be fairly hard.","['order-theory', 'elementary-set-theory']"
1929572,Proving the quotient rule for limits,"In real analysis, we have been asked to finish a proof of the quotient rule for limits (Given that $f(x)$ approaches $L$ and $g(x)$ approaches M as $x$ approaches a, prove that $\frac{f(x)}{g(x)}$ approaches $\frac{L}{M}$ . I know that I could rewrite the quotient as multiplication and prove it that way but that is not the way the proof we are completing starts off. Here is how the proof in the book starts: We showed in a previous exercise that $|g(x)| > \frac{|M|}{2}$ for all $x$ belonging to $D$ near $a$ . In particular, $g(x)$ does not equal $0$ for all $x$ belonging to $D$ near $a$ . Consequently, $\frac{f(x)}{g(x)}$ makes sense near $a$ . Let $\epsilon > 0$ be given. The proof is based on the triangle inequality: \begin{align*}
\left|\frac{f(x)}{g(x)} - \frac{L}{M}\right| &= \left|\frac{f(x)}{g(x)} - \frac{L}{g(x)} + \frac{L}{g(x)} - \frac{L}{M}\right|\\
& = \left|\frac{f(x)}{g(x)} - \frac{L}{g(x)} + L \frac{M - g(x)}{Mg(x)}\right| \\
&\leq \left|\frac{1}{g(x)}\right| |f(x) - L| + \left|\frac{L}{M}\right| \left|\frac{1}{g(x)}\right| |M - g(x)|.
\end{align*} We are expected to pick up the proof at this point. I was thinking that since I want everything above $< \frac{\epsilon}{2} + \frac{\epsilon}{2}=\epsilon$ , that I would construct $\epsilon_f$ and $\epsilon_g$ based on $\epsilon$ . So, I said we want $|\frac{1}{g(x)}| |f(x) - L| < \epsilon/2$ and because the limit approaches $L$ end up with $\epsilon_f$ equals $\frac{\epsilon|M|}{4}$ . I intended to do the same for the g part; however, when I go to define $\epsilon_g$ it involves $M$ . Am I allowed to use $M$ in that definition for $\epsilon_g$ ? And if not, any ideas on where to go from here? Thank you!!","['real-analysis', 'limits']"
1929575,Is Kruskal rank and rank of a matrix the same?,"The definition of Kruskal rank : maximum value of $k$ such that any $k$ columns of a matrix $\textbf{A}$ are linearly independent, then $k$ is the Kruskal rank of matrix $\textbf{A}$. How is it different from rank of the matrix $\textbf{A}$ ? Can you give an example where Kruskal rank and rank of a matrix are different.","['matrices', 'matrix-rank']"
1929601,A line intersecting segments,"There are n parallel segments on a plane, for any three of them, there exist a line crossing all three of them, how to prove there exist a line crossing all n segments? Thank you very much.","['linear-programming', 'discrete-geometry', 'geometry', 'linear-algebra', 'discrete-mathematics']"
1929611,Why does a polynomial's degree reduce when the leading coefficient approaches $0$ (but not equal to $0$)?,"Here is the definition of a polynomial taken from here A polynomial is a mathematical expression involving a sum of powers in one or more variables multiplied by coefficients. A polynomial in one variable (i.e., a univariate polynomial) with constant coefficients is given by
  $$a_0 + a_1x + a_2x^2 + \cdots + a_nx^n = \sum_{i = 0}^{n} a_ix^i$$ A polynomial is said to be an $nth$ degree polynomial if the highest power of the variable is $n$ and the leading coefficient is not equal to $0$. Now, take the following case: 
$$f(x) = \lim_{a_n \to 0} \sum_{i = 0}^{n} a_ix^i = \sum_{i = 0}^{n - 1} a_ix^i$$ Now, this shows that the degree of the polynomial $$\lim_{a_n \to 0} \sum_{i = 0}^{n} a_ix^i$$ is $n - 1$. Now, here is another thing from Wikipedia To say that 
  $$\lim_{x \to p} f(x) = L,$$
  means that $f(x)$ can be made as close as desired to $L$ by making $x$ close enough, but not equal , to $p$. It means in the expression 
$$\lim_{a_n \to 0} \sum_{i = 0}^{n} a_ix^i$$ the value of $a_n$ is infinitesimally close to $0$ but $a_n \ne 0$. This means that the degree of the polynomial is $n$. So, my question is that is it just because $a_n$ is extremely close to zero that we consider it to be equal to zero? If so, then is the degree of the expression $n - 1$?","['polynomials', 'limits']"
1929622,Lifting of group homomorphisms,"In my first course on algebraic topology I heard about the following: Suppose you have topological spaces $X,Y, Z$, and morphisms $\phi: X \to Y$ and $p : Z \to Y$. Then one raises the question if the map $\phi$ is liftable, i.e. the existence of a map $f : X \to Z$ such that $\phi = p \circ f$. (Probably I have forgotten some assumptions on the spaces but that's not the point here for me) One can prove that this lifting problem has a solution if and only if the ""corresponding"" lifting problem regarding the fundamental groups has a solution. (I mean, consider the corresponding fundamental groups and induced group homomorphism and ask the same question) Our teacher told us (informally) that typically the ""topological lifting problem"" is difficult where the ""algebraic lifting problem"" can be used to prove the former one. My question is the following:
Has there been some study (I personally guess so) to solve the ""algebraic lifting problem"" using a translation to a ""lifting problem"" living in another category. Hence, let $G,H,T$ be groups, morphisms $f : G \to H$ and $p : T \to H$. We want to lift $f$ to $T$. I thought of translating this to group algebras or $C^*$-group algebras.","['reference-request', 'category-theory', 'algebraic-topology', 'algebras', 'group-theory']"
1929638,How to find the transformation matrix given two vectors and their particular transformations?,"I know this is probably a pretty simple thing to do, but I can't really wrap my head around it: I have two vectors in $\mathbb{R}^2$ , $〈1,2〉$ and $〈1,0〉$ , whose transformations are $〈1,2,0〉$ and $〈3,0,1〉$ respectively. How do I find out the transformation matrix from this information? I know I could manually write down a new system of equations using the elements of the matrix as my unknowns, but I supposed that's too tedious to be the right solution. Also, do basis transformation matrices have anything to do with this? I thought I could use a new matrix consisting of my two column vectors in $\mathbb{R}^2$ (representing a change of basis from the standard basis vectors), but I'm not sure if doing the same thing to my transformed vectors in $\mathbb{R}^3$ makes any sense (can I group them up too? Would that be a change of basis in the range?). I'm sorry if I'm mixing up two unrelated things, but I had a hunch the problem might be related to that. Thanks for the help!","['linear-algebra', 'linear-transformations']"
1929660,Let $f(a) = \frac{13+a}{3a+7}$ where $a$ is restricted to positive integers. What is the maximum value of $f(a)$?,Let $f(a) = \frac{13+a}{3a+7}$ where $a$ is restricted to positive integers. What is the maximum value of $f(a)$? I tried graphing but it didn't help.  Could anyone answer?  Thanks!,"['optimization', 'functions', 'algebra-precalculus', 'maxima-minima', 'linear-algebra']"
1929676,orthogonal polynomial of the second kind,"Let $L:  \mathbb{R}[x] \rightarrow \mathbb{R}$ be a positive definite linear functional and let that $\{s_n\}$ be a positive semi-definite sequence such that $L(x^n)= s_n, n\ge 0.$ Given a positive definite sequence, I was able to use the Gram-Schimdt orthogonalization method to construct a sequence of orthogonal polynomials $\{p_n\}$ whose leading coefficient is positive due to the  positivity nature of the sequence given. It turns out that this sequence of orthogonal polynomials  $\{p_n\}$ satsifies a three term recurrence relation given below
\begin{equation}
xp_n(x) =b_np_{n+1}(x)+a_np_n(x)+b_{n-1}p_{n-1}(x) , \quad n\ge 0
\end{equation} We can see the sequence $p_n(x)$ as a solution to the three term recurrence relations stated above. Akhiezer http://www.maths.ed.ac.uk/~aar/papers/akhiezer.pdf as my reference introduced another solution to this three term recurrence relation by defining another solution by \begin{equation} q_n(x)= \displaystyle L\left(\frac{p_n(x)-p_n(y)}{x-y}\right)
\end{equation} where the  quotient $\frac{p_n(x)-p_n(y)}{x-y}$  is a polynomial in $x$ and $y$ and $q_n(x)$ is a polynomial in variable $x$ and its degree is $n-1$ for any $x,y \in \mathbb{R}$ so that we have $ \displaystyle xq_n(x) =b_nq_{n+1}(x)+a_nq_n(x)+b_{n-1}q_{n-1}(x), n\ge 1$ with $q_0(x)=0$ and $q_1(x)= \frac{1}{b_0}$ My question is how can the  linear functional $L$ be defined on rational functions since its domain is $\mathbb{R}[x]$. I have troubles understanding  this definition even though I actually confirmed the definition is true by computing $q_1,q_2$.You can please check Akhiezer page 8 for more clarification. Details and explanation will be much appreciated.","['recurrence-relations', 'real-analysis', 'measure-theory', 'linear-algebra', 'orthogonal-polynomials']"
1929682,Relationship between X and its Projection Matrix,"Suppose $Q_{1}$ is an $n\times p $ matrix (derived from the $QR$ Decomposition of $X$) whose columns provide an orthonormal basis for the subspace ${\chi}$ of $\mathbb R^{n}$ spanned by the columns of an $n\times p$ matrix $X = (x_1,...,x_p)$. The hat matrix $H = Q_{1}Q_{1}^{T}$ projects vectors orthogonally onto $X$. Suppose the first two rows of $X$ are the same. Explain why the first two rows of $H$ are the same.","['matrices', 'statistics', 'linear-algebra', 'least-squares']"
