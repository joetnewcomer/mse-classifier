question_id,title,body,tags
1515155,Proving uniqueness in Lebesgue decomposition,"I'm reading a book(Measure, Integral and Probability by Capinski), where in the proof of the Lebesgue decomposition theorem, it leaves to the reader to prove uniqueness. As a hint, the authors state that we should use the following proposition: «Let $\mu,\nu, \lambda_1, \lambda_2$ measures on sigma algebra $\mathcal{F}$. Then we have: i) If $\lambda_1 \perp \mu$ and $\lambda_2 \perp \mu$, then $\lambda_1+\lambda_2 \perp \lambda_2$. ii) If $ \lambda_1 \ll \mu$, and $\lambda_2 \perp \mu$, then $\lambda_1 \perp \lambda_2$. iii) If $ \nu \ll \mu$, and $\nu \perp \mu$, then $\nu=0$.» I've tried using point iii) to the subtraction of the absolutely continuous part of the two representations of the same measure, but subtraction of two measures may not necessarily be a measure... Any help would be appreciated.",['measure-theory']
1515201,$L^{1}$ Boundedness of Hilbert Transform on $\left\{f\in L^{1}(\mathbb{R}) : \int_{\mathbb{R}}f=0\right\}$,"It is well-known that the Hilbert transform $H(f)$ of a bounded, compactly supported function $f:\mathbb{R}\rightarrow\mathbb{C}$ belongs to $L^{1}(\mathbb{R})$ precisely when $\int f=0$. One can relax the boundedness assumption to $f\in L^{p}$, for $p>1$, or even $L\log L$. More generally, we only have a hope that the Hilbert transform $H(f)$ of an integrable function $f$, not necessarily compactly supported, is itself integrable if $\int f=0$. Problem. I am quite confident that it is true that we have the proper containment $$H^{1}(\mathbb{R}):=\left\{f\in L^{1}(\mathbb{R}) : H(f)\in L^{1}(\mathbb{R})\right\}\subsetneq\left\{f\in L^{1}(\mathbb{R}) : \int_{\mathbb{R}}f=0\right\}$$ however, I am struggling to produce an example proving the properness
  of the containment. Can someone help me with such an example? My motivation for this question is understanding how close the real Hardy space $\mathbb{R}^{n}$ is to integrable functions which have good cancellation (see also this question ). In one dimension, an equivalent characterization of the Hardy space $H^{1}(\mathbb{R})$ is $L^{1}$ functions with $L^{1}$ bounded Hilbert transforms and in fact, $\left\|f\right\|_{L^{1}}+\left\|Hf\right\|_{L^{1}}$ defines an equivalent norm. In higher dimensions, the analogue of this equivalence is $L^{1}$ boundedness of the Riesz transforms.","['hardy-spaces', 'fourier-analysis', 'real-analysis', 'functional-analysis', 'harmonic-analysis']"
1515232,Why is Schouten-Nijnhuis bracket trivial on Poisson cohomology?,"For a commutative algebra $A$, let a biderivation $P$ be called a Poisson structure if $[[P,P]]=0$ (the bracket is Schouten-Nijenhuis). Then one obtains a complex of multiderivations with $[[P,{}]]$ being the differential, and so one gets Poisson cohomology. Due to graded Jacobi identity for Schouten-Nijenhuis bracket, Poisson cohomology inherits this bracket. Let $P$ be called nondegenerate if $da \mapsto P(a,{})$ defines an isomorphism between 1-forms and 1-derivations. (Here one also needs to assume that the module of 1-forms is projective and finitely generated.) How can one see that for a nondegenerate Poisson structure Schouten-Nijenhuis bracket on cohomology is trivial? I highly suspect that for X and Y from cohomology we have $[[X,Y]]=[[P,i_\Omega(X\wedge Y)]]$ where $\Omega$ is the symplectic 2-form corresponding to $P$ and $i$ is insertion, but I only managed to show that for $|X|=1$ and $|Y|=0$ which is a little bit not enough.","['differential-forms', 'homology-cohomology', 'abstract-algebra', 'differential-geometry', 'ordinary-differential-equations']"
1515236,"""Last Fermat's Theorem"" modulo m","The last Fermat's Theorem is a claim about the non-existence of non-trivial integer solution for $X^n+Y^n=Z^n$ for $n\in \mathbb N$ , $n\ge 3$ . However, given $m\in \mathbb N$ , we can investigate the integer solutions for $X^n+Y^n\equiv Z^n \pmod m$ for all $n\in \mathbb N$ with the restriciton that $X,Y,Z\not\equiv0\pmod m$ . It seems to me that we always have solutions in this case, but I did no find any reference or exposition about this. Is it ""relevant"" to think on it? Have this question appeared elsewhere?",['number-theory']
1515247,How to interpret the plot of a function of a complex variable?,"I know what a complex number is: $a+bi$. But I have seen these functions that make no sense to me, something such as this:
$$f(z)=z^2+1$$ where $z$ is a complex number. Does this have to do with that plane where the ""y""-axis is Real numbers and the ""x""-axis is Imaginary numbers? I typed it into this complex grapher http://davidbau.com/conformal/#z%5E2%2B1 and I am utterly confused. What's with all the colors? What's with that weird 8 shape?","['graphing-functions', 'complex-analysis', 'complex-numbers']"
1515284,Prove by mathematical induction that $a^n$ is an irrational number.,Let $a$ be an irrational number where $a^2$ is a rational number. Prove by mathematical induction or generalized mathematical induction that $a^n$ is an irrational number for all odd integers $n ≥ 1$. So I know that a (rational) * (irrational) -> (irrational) but I'm not sure how to go about this with induction.,"['induction', 'discrete-mathematics']"
1515290,Tensor products and morphisms,"Let $C$ be  semisimple category with simple objects $X_1, \dots, X_r$. Suppose we have a fusion relation $X_i\otimes X_j =\bigoplus_{l=1}^r N_{ij}^l X_l$. Let $m\in \mathbb{N}$ and let $g:mX_j \to mX_j$ be a morphism given by $m$ by $m$ matrix over a field $k$. Using the relation above, the algebra $\mathrm{End}((X_i\otimes mX_j)\otimes X_l)=\mathrm{End}(X_i \otimes (mX_j \otimes X_l))$ is equal to $$\otimes_{s=1}^r \mathrm{Mat}_{mn_{s}}(k),$$ where $$n_s=\sum_{p=1}^rN_{ij}^p N_{pl}^s=\sum_{q=1}^r N_{iq}^s N_{jl}^q.$$ I want to show that in this algebra, we have $$(\mathrm{id}_{X_i} \otimes g ) \otimes \mathrm{id}_{X_l}=\bigoplus_{p=1}^r \mathrm{id}_{N_{ij}^p} \otimes g \otimes \mathrm{id}_{N_{pl}^s}.$$ [That formula needs to be fixed; for example, $s$ is undefined.] I don't understand how morphisms are changed by the fusion relation. EDIT It is really hard to see but actually $s$ is defined. The subscript in $\otimes_{s=1}^r \mathrm{Mat}_{mn_{s}}(k)$ is $mn_{s}$.","['abstract-algebra', 'monoidal-categories', 'abelian-categories', 'category-theory']"
1515299,"Is a ""diagonal-like"" set always a null set?","For this question, let $\mu$ be the Lebesgue measure on $[0,1]$ and $\mu^2$ the product measure on $[0,1]^2$. Suppose I have a measurable function $f\colon [0,1]^2\to \mathbb{R}$. For all $r\in \mathbb{R}$, let $$Z(r) = \{(x,y)\mid f(x,y) = r\} = f^{-1}[\{r\}],$$ and suppose that $\mu^2(Z(r)) = 0$. Now given $x\in [0,1]$, let $$Z(r)_x = \{y\in [0,1] \mid f(x,y) = r\}$$ be the fiber above $x$, and let $$N(r) = \{x\mid \mu(Z(r)_x) > 0\}.$$ This is the set of all $x$ such that $Z(r)$ has positive measure in the fiber above $x$. By Fubini, $N(r)$ is a null set: it is the preimage of $(0,\infty]$ under the measurable function $g(x) = \int_y 1_{Z(r)}(x,y)\, d\mu$, and since $\int_x g(x)\,d\mu = \mu^2(1_{Z(r)}) = 0$, $g$ is $0$ almost everywhere. For all $x\in [0,1]$, $\{r\in \mathbb{R}\mid x\in N(r)\}$ is countable, since the fiber above $x$ can contain at most countably many disjoint positive measure sets. Let $D = \bigcup_{r\in \mathbb{R}} N(r)^2$, where $N(r)^2 = \{(x,x')\mid x,x'\in N(r)\}$. I'd like to show that $\mu^2(D) = 0$. Intuitive gloss: I need two pieces of information, $x$ and $y$, to specify a real $f(x,y)$. If I pick $x$ and $y$ randomly, any particular real appears with probability $0$. But given partial information $x$, I might know that a real $r$ appears with positive probability when I pick $y$ at random (i.e. $x\in N(r)$). In this case we'll say $r$ is likely given $x$. We know that at most countably many reals are likely given $x$, and the probability that $x$ makes any particular real likely is $0$. Now if I pick two partial pieces of information $x$ and $x'$ independently, I want to show that almost surely there is no real $r$ that is likely given $x$ and likely given $x'$. To me, this seems intuitively correct. Two remarks: If $D$ is measurable, it's an easy consequence of Fubini's theorem that $\mu^2(D) = 0$. Indeed, the fiber above every $x$ consists of a union of countably many null sets. The problem is showing that $D$ is measurable... We can view this as a generalization of the fact that the diagonal has measure $0$ in $[0,1]^2$ (taking $f(x,y) = x$), and a similar proof (to the non-Fubini proof of this fact) might work. Hence the title.","['probability-theory', 'measure-theory']"
1515317,Unitriangular group $UT_n(\Bbb Z)$ is nilpotent with class $n$,"The unitriangular group $UT_n(\Bbb Z)$ is the group of all $n \times n$ invertible triangular matrices with the identity on each entry of the main diagonal, and integer entries everywhere else in the triangle. Show that this group is nilpotent, and that its nilpotence class is $n$ . Definition ( upper central series ): For any group $G$ define the following subgroups inductively: $$Z_0(G) = 1, \qquad Z_1(G) = Z(G)$$ and $Z_{i+1}(G)$ is the subgroup of $G$ containing $Z_i(G)$ such that $$Z_{i+1}(G)/Z_i(G) = Z(G/Z_i(G)).$$ The chain of subgroups $$Z_0(G) \leq Z_1(G) \leq Z_2(G) \leq \cdots$$ is called the upper central series of $G$ . Definition ( nilpotent ): A group $G$ is called nilpotent if $Z_c(G) = G$ for some $c \in \Bbb Z$ . The smallest such $c$ is called the nilpotence class of $G$ . To show that it is nilpotent, I think it is suffient to show that it is a $p-$ group; i.e. $|UT_n(\Bbb Z)| = p^{\alpha}$ where $p$ is a prime number and $\alpha$ is a positive integer. I feel like there must be some sort of algorithm to calculate how many possible matrices we can get, similar to the formula for finding the order of $GL_n(\Bbb F)$ , the general linear group. I tried Googling, but I can't find a formula for the unitriangular group $UT_n(\Bbb Z)$ . To show the nilpotence class is $n$ , I have to prove that $Z_n(G) = G$ , and that $n$ is the smallest such integer. So by the given definition above, I know that $Z_n(G)/Z_{n-1}(G) = Z(G/Z_n(G))$ . How can I manipulate this to arrive at $Z_n(G) = G$ , and also show that $n$ is the smallest such integer?","['matrices', 'p-groups', 'abstract-algebra', 'group-theory', 'linear-algebra']"
1515322,Solving a recursion efficiently,"I have a recursive formula $$v(n+1) = v(n)\dfrac{1+v(n-1)-n}{1+v(n-1)-v(n)}$$
and I also know $$v(1)=2v(0)$$ $$n+1 \le v(n+1) \le v(n)+1.$$ I wish to find, for example, $v(10)$ more efficiently than my current method does. That involves using the numerical instability of the recursion, by trying different values of $v(0)$, taking them forward until one of the inequalities is not met, then adjusting my estimate for $v(0)$ using a bisection method, and trying again. If I use sufficient decimal precision (about $45$ decimal places in this example) for my calculations and search then I can get a value for $v(0)$ of about $0.6245357205\ldots$ which then gives a value for $v(10)$ of about $10.0000000114\ldots$. But this seems excessive, especially if I want $v(n)$ for larger $n$; I ended up finding $v(0)$ to $2000$ decimal places in http://www.se16.info/hgb/lockedbox.pdf I was wondering whether there might be an alternative approach.","['sequences-and-series', 'numerical-methods', 'algorithms', 'recursion']"
1515344,Munkres exercise,"Problem Let $\{A_\alpha\}$ be a collection of subsets of $X$; let $X=\bigcup_{\alpha}A_\alpha$. Let $f:X\rightarrow Y$;suppose that $f\vert_{A_\alpha}$ is continuous for each $\alpha$. An index family of sets $\{A_\alpha\}$ is defined to be locally finite if each point $x$ has a neighborhood that intersects $A_\alpha$ for only finitely many values of $\alpha$. Show that if the family $\{A_\alpha\}$ is locally finite and each $A_\alpha$ closed, then $f$ is continuous. Attempted Solution It suffices to show that $f^{-1}\left(V\right)$ is closed in
$X$ for any closed set $V$ in $Y$. Pick an arbitrary $x\in\overline{f^{-1}\left(V\right)}$
, then we have $U\cap f^{-1}\left(V\right)\neq\emptyset$ for any
open set $U$ containing $x$. By local finiteness, $\exists$ an
open neighborhood $N$ such that $x\in N$ and $N\cap A_{\alpha}\neq\emptyset$
for finitely $\alpha,$ namely $\left\{ A_{i}\right\} _{i=1}^{k}$.
Since $x\in U\cap N,$ we have 
\begin{align}
U\cap f^{-1}\left(V\right)\cap\left(\cup_{i=1}^{k}A_{i}\right) & =U\cap\left[\cup_{i=1}^{k}\left(f^{-1}\left(V\right)\cap A_{i}\right)\right]\\
 & =U\cap\left(\cup_{i=1}^{k}f\vert_{A_{i}}^{-1}\left(V\right)\right)\\
 & \supset U\cap N\cap\left(\cup_{i=1}^{k}f\vert_{A_{i}}^{-1}\left(V\right)\right)\neq\emptyset
\end{align}
from which we have that $x\in\overline{\cup_{i=1}^{k}f\vert_{A_{i}}\left(V\right)}$.
Note that $f\vert_{A_{i}}$ is continuous, so $f\vert_{A_{i}}^{-1}\left(V\right)=f^{-1}\left(V\right)\cap A_{i}$
is closed in the subspace topology of $A_{i}.$ So $f\vert_{A_{i}}^{-1}\left(V\right)=F_{i}\cap A_{i}$
for some closed set $F_{i}\subset X$. Since $A_{i}$ is closed, $f\vert_{A_{i}}^{-1}\left(V\right)$
is closed in $X$ as well. Thus, $x\in\cup_{i=1}^{k}f\vert_{A_{i}}^{-1}\left(V\right)=f^{-1}\left(V\right)\cap\left(\cup_{i=1}^{k}A_{i}\right)\subset f^{-1}\left(V\right)$,
from which we conclude $\overline{f^{-1}\left(V\right)}\subset f^{-1}\left(V\right)$
and as a result, $f^{-1}\left(V\right)$ is closed because it contains
all the limit points. Question (1).This is is problem in Munkrs topology. I tried to solve it and I think I had it. I really appreciate is anyone can take a look at my solution.","['proof-verification', 'general-topology']"
1515353,Stability of periodic solution,"I am stuck on this question: $$x'(t) = x(x-p(t))(x-1),$$ where $p$ is $1$-periodic and $0< p(t) <1$ for all $t$. We want to show the existence of a STABLE periodic solution $\mu$ such that $0< \mu(t) <1$ for all $t$. If we require $0< x_0 <1$, where we impose the initial condition that $x(0) = x_0$, then it is clear that there exists a solution $\mu$ that satisfies the ODE with this initial condition. Standard result also shows the existence of such periodic solutions. My question is how to show the stability of such solutions by the definition of stable periodic solutions.","['dynamical-systems', 'ordinary-differential-equations']"
1515355,Forecast equivalence between two steady-state Kalman filters,"I have two related steady-state Kalman filter problems that I want to prove satisfy a condition associated with their respective Kalman gains. I am not really looking for a complete proof since this is likely require quite a bit of work. However, any ideas about where to start or useful results that I could use would be much appreciated. Problem 1: $${\bf P} ={\bf F}({\bf I}_{n}-{\bf K} {\bf H}){\bf P}{\bf F}^\top+{\bf Q}, \;\;\;\;\;\;\;\;\text{where}\;\;\;\;{\bf K}\equiv {\bf P} {\bf H^\top}\left({\bf H}{\bf P} {\bf H}^\top+ \frac{1}{\gamma}{\bf R} \right)^{-1}$$ where ${\bf P}$, ${\bf F}$, and ${\bf Q}$ are $n\times n$, ${\bf H}$ is $r\times n$, ${\bf K}$ is $n\times r$, ${\bf R}$ is  $r\times r$ and symmetric, ${\bf Q}$ is diagonal. Problem 2: $${\bf W} ={\bf A}({\bf I}_{2n}-{\bf L} {\bf B}){\bf W}{\bf A}^\top+{\bf C}, \;\;\;\;\;\;\;\;\text{where}\;\;\;\;{\bf L}\equiv {\bf W} {\bf B}^\top\left({\bf B}{\bf W} {\bf B}^\top+ {\bf R} \right)^{-1}$$ where ${\bf W}$, ${\bf A}$ and ${\bf C}$ are $2n\times 2n$, and ${\bf B}$ is $r\times 2n$, and ${\bf L}$ is $2n\times r$. Relationship between the two problems: $${\bf A} \equiv \left[ \begin{matrix} {\bf F} & {\bf0} \\ {\bf K} {\bf H} {\bf F} & ({\bf I}_{n}-{\bf K} {\bf H}){\bf F} \end{matrix} \right], \;\;\;\; {\bf B}^\top \equiv \left[ \begin{matrix} {\bf H}^\top & {\bf 0} \end{matrix} \right], \;\;\;\; {\bf C} \equiv \left[ \begin{matrix} {\bf Q} & {\bf Q}{\bf H}^\top{\bf K}^\top \\ {\bf K}{\bf H}{\bf Q} & {\bf K}{\bf H} {\bf Q}{\bf H}^\top{\bf K}^\top \end{matrix} \right]$$ also let
$${\bf L} \equiv \left[ \begin{matrix}  {\bf L}_{1} \\ {\bf L}_{2} \end{matrix} \right], \;\;\;\; {\bf W} \equiv \left[ \begin{matrix} {\bf W}_{11} & {\bf W}_{21}' \\ {\bf W}_{21} & {\bf W}_{22} \end{matrix} \right]$$ Want to prove: $${\bf K} = \gamma {\bf L}_{1} + (1-\gamma) {\bf L}_{2} $$ All matrices are real. Doing a bit of algebra, it is easy to show that \begin{align}
{\bf L}_{1} &={\bf W}_{11}{\bf H}^\top({\bf H}{\bf W}_{11}{\bf H}^\top+{\bf R})^{-1} \\[1.5ex]
{\bf L}_{2} &={\bf W}_{21}{\bf H}^\top({\bf H}{\bf W}_{11}{\bf H}^\top+{\bf R})^{-1}
\end{align} and that \begin{align}
{\bf W}_{11} &= {\bf F}({\bf I}_n-{\bf L}_{1}{\bf H}){\bf W}_{11}{\bf F}^\top+{\bf Q} \\
{\bf W}_{21} &={\bf K}{\bf H}{\bf W}_{11}+({\bf I}_n-{\bf K}{\bf H}){\bf F}{\bf W}_{21}({\bf I}_n-{\bf H}^\top {\bf L}_{1}^\top){\bf F}^\top
\end{align}","['bayesian-network', 'linear-algebra', 'kalman-filter']"
1515374,Frechet Derivatives of a nonlinear integral operator,"The nonlinear integral operator $P:C[0,1]\to C[0,1]$ is defined as follow: $$P(f)(x)=1+kxf(x)\int_0^1\frac{f(s)}{x+s}ds$$ In order to obtain the Frechet derivative of the operator, I start with: $$P(f+h)(x)-P(f)(x)=kxf(x)\int_0^1\frac{h(s)}{x+s}ds+kxh(x)\int_0^1\frac{f(s)}{x+s}ds+kxh(x)\int_0^1\frac{h(s)}{s+x}ds$$ It seems the last term is of order $\lVert h(x)\lVert _\infty ^2$ but how to prove it? Note that the function inside the integral is no longer continuous.","['integral-operators', 'normed-spaces', 'operator-theory', 'derivatives']"
1515400,Proving a summation inequality with induction,The exact question: Prove: $\displaystyle\sum_{k=1}^n \frac{1}{\sqrt{k}}\gt2(\sqrt{n+1}-1)$ I have looked at similar problems but still don't understand how to prove this inequality by induction. So far I have this: Induction basis: Let n=1 $\displaystyle\sum_{k=1}^n \frac{1}{\sqrt{k}} = \frac{1}{\sqrt{1}} = 1 > 2(\sqrt{1+1}-1) = ~.828$ $1>.828$ So it proves the inequality true when n=1. Now i really don't know how to continue even with all the examples i have browsed through. One of them i came across showed that the induction hypothesis should let P(n) equal the equation above and do something with P(n+1). I am not looking for the answer I just need help on how to continue with the problem. What other steps are necessary for me to complete this proof by induction.,"['induction', 'discrete-mathematics', 'summation', 'proof-verification', 'inequality']"
1515429,How to show that $x_n = \sum_{k=1}^{n} \frac{\cos(k+1)x - \cos kx}{k}$ converges?,"How to show that $x_n = \sum_{k=1}^{n} \frac{\cos(k+1)x - \cos kx}{k}$ converges? My solution: I tried to use Cauchy convergence theorem. For any $\epsilon>0$, I need to find $N$ such that for all $n \geq m \geq N$, the inequality 
$$ 
\left| \sum_{k=m+1}^{n} \frac{\cos(k+1)x - \cos kx}{k} \right|<\epsilon 
$$
holds. We have
$$ 
\left| \sum_{k=m+1}^{n} \frac{\cos(k+1)x - \cos kx}{k} \right|\\
 =\left| -2\sum_{k=m+1}^{n} \frac{\sin(\frac{k}{2}+1)x \sin \frac{1}{2}x}{k} \right| \\
\leq \left| -2\sum_{k=m+1}^{n} \frac{1}{k} \right|\\
= \sum_{k=m+1}^{n} \frac{1}{k}.
$$
But I am not it seems that only when $n$ is close to $m$, $\sum_{k=m+1}^{n} \frac{1}{k}$ is small. Thank you very much.","['sequences-and-series', 'calculus', 'limits', 'trigonometry']"
1515464,Possible Lack of Rigor in Common Multivariable Chain Rule Expression?,"I'm trying to get my head around an issue with the multivariable chain rule as it's commonly written. By way of illustration, let's use this simple example, taken from here (about halfway down the page). $z$ is a bivariate function of $x,y$, ie $z=f(x,y)$. $y$ is also a function of $x$, ie $y=g(x)$. Now, we have this expression for the total derivative, or chain rule: $$\frac{dz}{dx}= \frac{\partial z}{\partial x} + \frac{\partial z}{\partial y} \frac{dy}{dx}      (1)$$ The meaning of $\frac{\partial z}{\partial y}$ is the rate of change of $z$ when $y$ is varied but $x$ is held constant. But since $y=g(x)$ holds at the same time, this is impossible to do, because if $x$ is held constant, so must be $y$ - it's impossible to vary $y$ and for $y=g(x)$ to hold at the same time. Of course, we know how to evaluate $\frac{\partial z}{\partial y}$: my way is to ""pretend"" that we don't know there is a dependence $y=g(x)$, and compute $\frac{\partial z}{\partial y}$ as if $x$ and $y$ are independent. So: Is Equation $(1)$, in fact, not rigorous? Is $y$ and $x$ doing ""double duty""? If so, I'm trying to figure out how to make it rigorous - can ""dummy variables"" be introduced to do this?","['multivariable-calculus', 'real-analysis']"
1515484,The number of pendant vertices in a tree,"Let $T$ be a tree with vertices $\{v_1, v_2, . . . , v_n \}$ for $n \geq 2$. Prove that the number of pendant vertices in $T$ is equal to $$\large{2 + \sum_{v_i,deg(v_i) 
\geq 3}\big( deg(v_i) - 2 \big)}$$ A pendant vertex is a vertex of degree $1$. To make sense out of this equation we try $n=2$ then we only have one edge connecting these two vertices and the number of pendant vertices is equal to $2$ just as the formula suggests. Now if $n=3$ then we have two edges, one vertex has degree of $2$ and still two vertices with degree $2$ also as the formula suggests. But I can't understand why we have that summation and it only takes the vertices with degree greater than or equal to $3$. Can I prove this formula by induction ? I know that $$\large{\sum_{i=1}^n = deg(v_i) = 2(n-1)= 2n-2}$$ since the number of edges for a tree $T$ with $n$ vertices is $n-1$ and also from this we get that $$\large{\sum_{i=1}^n = deg(v_i)-2 = \sum_{i=1}^n deg(v_i) + \sum_{i=1}^n -2= (2n-2)  + (-2n) = -2}$$. So  I feel like I am close somehow but still couldn't get it. Also the formula that I am trying to derive is suggesting that every tree with at least two vertices must have at least $2$ pendant vertices, Why is that true ?","['discrete-mathematics', 'graph-theory', 'combinatorics', 'trees']"
1515492,Why does cancelling change this equation?,"The equation $$2t^2 + t^3 = t^4$$ is satisfied by $t = 0$ But if you cancel a $t^2$ on both sides, making it
$$2 + t = t^2$$
$t = 0$ is no longer a solution. What gives? I thought nothing really changed, so the same solutions should apply. Thanks",['algebra-precalculus']
1515567,How a blow up changes the Canonical bundle?,"lt $f:Y\longrightarrow X$ be a blowup a subvariety $V\subset$, where say both X and Y are smooth. Then what is the relation of $K_Y$ and $K_X$ ? The case of surfaces is clear What about higher dimension varieties? In particular, Y is the blowup of a curve in a smooth threefold X.","['algebraic-geometry', 'sheaf-theory']"
1515600,A trapezium's diagonals' point of intersection lies on the line passing through its parallel sides' mid-points,"Prove, using a vector method, that a trapezium's diagonals' point of intersection lies on the line passing through its parallel sides' mid-points. My Attempt: Let the trapezium be $OABC,$ $O$ be the origin, and the position vectors of $A,B,C$ be $\vec{a},\vec{b},\vec{c}.$ Then the equation of diagonal $OB$ is $\vec{r}=\vec{0}+\lambda \vec{b}.\tag1$ And the equation of diagonal $AC$ is $\vec{r}=\vec{a}+\mu(\vec{c}-\vec{a}).\tag2$ And the equation of the line joining the mid points of $OA$ and $BC$ is $\vec{r}=\frac{\vec{a}}{2}+t(\frac{\vec{b}+\vec{c}}{2}-\frac{\vec{a}}{2}).\tag3$ Here, $\lambda,\mu,t$ are scalars. I do not know how to solve $(1)$ and $(2)$ and $(3)$ to obtain the desired result.","['geometry', 'vectors']"
1515603,Proving the inverse of a bijection is bijective,"Let $f: A\to B$ and that $f$ is a bijection. Show that the inverse of $f$ is bijective. Surjectivity: Since $f^{-1} : B\to A$, I need to show that $\operatorname{range}(f^{-1})=A$. But since $f^{-1}$ is the inverse of $f$, and we know that $\operatorname{domain}(f)=\operatorname{range}(f^{-1})=A$, this proves that $f^{-1}$ is surjective. Injectivity: I need to show that for all $a\in A$ there is at most one $b\in B$ with $f^{-1}(b)=a$. But we know that $f$ is a function, i.e. for all $a\in A$ there is exactly one (at least one and never more than one) $b\in B$ with $f(a)=b$. 'Exactly one $b\in B$' obviously complies with the condition 'at most one $b\in B$'. Since $f^{-1}$ is the inverse of $f$, $f^{-1}(b)=a$. So combining the two, we get for all $a\in A$ there is exactly one (at least one and never more than one) $b\in B$ with $f^{-1}(b)=a$. I think my surjective proof looks ok; but my injective proof does look rather dodgy - especially how I combined '$f^{-1}(b)=a$' with 'exactly one $b\in B$' to satisfy the surjectivity condition. Could someone verify if my proof is ok or not please? Thank you so much!","['elementary-set-theory', 'proof-verification']"
1515619,Subgroup of $A_{n}$,Suppose $G \leq S_{n}$ and $G$ has an odd number of elements. Prove $G \leq A_{n}$. I'm trying to do this by contradiction by assuming $G$ has an odd permutation but I can't show how it would be in $A_{n}$.,"['abstract-algebra', 'group-theory', 'symmetric-groups']"
1515641,Convergence in L^p and convergence in norm,"I am trying to show these two statements: Let $(\mathbb R^n,\mathcal M,m)$ where $m$ is the Lebesgue measure and $\mathcal M$ are the Lebesgue measurable sets, and let $(f_n)_{n \in \mathbb N}$ and $f$ in $L^p(\mathbb R^n)$ for $1 \leq p < \infty$. (a) $||f_n-f||_{L^p} \to 0 \implies ||f_n||_{L^p(\mathbb R^n)} \to ||f||_{L^p(\mathbb R^n)}$ (b) If $f_n \to f$ a.e. and $||f_n||_{L^p(\mathbb R^n)} \to ||f||_{L^p(\mathbb R^n)}$, then $||f_n-f||_{L^p} \to 0 $ There is a suggestion for part (b) which is to apply Fatou's lemma to the sequence $g_n(X)=2^{p-1}(|f_n(x)|^p+|f(x)|^p)-|f_n(x)-f(x)|^p$. I'll write what I could do: For part (b), I could not show that the suggested sequence $(g_n)_{n \in \mathbb N}$ is non negative, so instead I've used the sequence $h_n(x)=2^p(|f_n(x)|^p+|f(x)|^p)-|f_n(x)-f(x)|^p$. To apply Fatou's lemma to this sequence let's show non-negativity of the terms: $$|f_n(x)-f(x)|^p \leq (|f_n(x)|+|f(x)|)^p$$$$\leq 2^p\max\{|f_n(x)|,|f(x)|\}^p$$ $$\leq 2^p(|f_n(x)|^p+|f(x)|^p)$$ It follows that $h_n(x) \geq 0$. Notice that $\lim_{n \to \infty} h_n(x)=2^{p+1}|f(x)|^p$ a.e., applying Fatou's lemma we have $$\int_{\mathbb R^d} 2^{p+1}|f(x)|dx \leq \lim inf \int_{\mathbb R^d}2^p(|f_n(x)|^p+|f(x)|^p)-|f_n(x)-f(x)|^pdx$$$$=\int_{\mathbb R^d}2^p|f|^pdx+\int_{\mathbb R^d}2^p|f|^pdx-\lim sup \int_{\mathbb R^d}|f_n-f|^pdx$$ From here it follows $\lim sup \int_{\mathbb R^d}|f_n-f|^pdx=0$, so $||f_n-f||_{L^p(\mathbb R^d)}=0$. I don't know what to do to prove (a), could someone help me with that part and tell me if my solution to (b) is correct? Thanks in advance.","['lp-spaces', 'real-analysis', 'lebesgue-integral', 'measure-theory']"
1515652,Showing that two definitions of $\limsup$ are equivalent,"In Rudin, $\limsup$ is defined as follows: Let $S$ be the set of numbers $x$ (in the extended real number system) such that $s_{n_k}\to x$ for some subsequence $\{s_{n_k}\}$ . Then $$\limsup s_n = \sup S. \tag{1}$$ However, our real analysis instructor defined $\limsup$ in a different manner: $$\limsup s_n = \lim_{n \to \infty} \sup_{m \ge n} s_m. \tag{2}$$ I am having trouble understanding how these two definitions are equivalent. It would be very helpful to me if somebody could provide a proof with some explanation. My thoughts on the problem: I have noticed that the usual trend with these sort of proofs is to prove the upper bound $(1) \le (2)$ and then the lower bound $(1) \ge (2)$ to get the desired conclusion. However, I am unsure how to even begin.","['real-analysis', 'definition', 'limsup-and-liminf']"
1515667,"Help me show that the limit $\lim_{(x,y)\to (0,0)}\frac{2e^x y^2}{x^2+y^2}$ does not exist","Show that the limit $\lim_{(x,y)\to (0,0)}\frac{2e^x y^2}{x^2+y^2}$
  does not exist $$\lim_{(x,y)\to (0,0)}\frac{2e^x y^2}{x^2+y^2}$$ Divide by $y^2$: $$\lim_{(x,y)\to (0,0)}\frac{2e^x}{\frac{x^2}{y^2}+1}$$ $$=\frac{2(1)}{\frac{0}{0}+1}$$ Since $\frac{0}{0}$ is undefined. This limit does not exist. I am not satisfied with my proof. Makes me think that what I just did was simple step, and not acceptable university level mathematics. Any comments on my proof?","['calculus', 'limits']"
1515711,"$B$ is a Borel set, implies $f(B)$ is a Borel set.","I have this exercise: Suppose $f$ is a real valued continuous function on $[a,b]$. Show that
  $f(B)$ is a Borel set for every Borel subset $B$ of $[a,b]$. Hint: Consider the collection $M$ of all subsets $A$ of $[a,b]$ for which
  $f(A)$ is a Borel set. Show that $M$ is a sigma-algebra. I am struggling a little. The hint tells me to look at: $M=\{A\mid f(A) \text{ is Borel}\}$, and show that this is a sigma algebra. I do get that $\emptyset \in M$, and that M is closed under countable unions. But how do I get closed under complements?, I mean $A \in M \rightarrow A^c \in M$?, the problem is that we only have $f(A)^c=f(A^c)$
 if f is bijective. And to finish the proof I also need that $f(O)$ is borel for every open set, but how do I get this?","['real-analysis', 'measure-theory']"
1515725,Probability of at least one event,"There are 2 independent events, probability that Exam A is a success is 0.4. Probability that Exam B is a success is 0.7. What is the probability that at least one of these is a success. So I thought the way 'at-least- one of these is a success is 'a is a success' or 'b is a success' or 'a and b is a success'. But the answer is  'a is a success' or 'b is a success' - 'a and b is a success'. I am confused as to why this is.","['probability', 'statistics']"
1515749,"Prove that if four numbers are chosen from the set $\{1,2,3,4,5,6\}$, at least one pair must add up to $7$.","Prove that if four numbers are chosen from the set $\{1,2,3,4,5,6\}$, at least one pair must add up to $7$ using the Pigeonhole principle. I am supposed to identify the pigeons and the pigeonholes. We know that $\{1,6\},\{2,5\},$ and $\{3,4\}$ all add up to $7$, so I am guessing these are perhaps the pigeonholes? We also know that any set of four numbers has six unique pairs in it. I am not really sure how to tie this to one of the pairs adding up to $7$, though.","['combinatorial-proofs', 'discrete-mathematics', 'pigeonhole-principle', 'combinatorics']"
1515798,"If $A^2 = I$, then $A$ is diagonalizable, and is $I$ if $1$ is its only eigenvalue","Let $A$ be a square matrix of order $n$ such that $A^2 = I$ . Prove that if $1$ is the only eigenvalue of $A$ , then $A = I$ . Prove that $A$ is diagonalizable. For  (1), I know that there are two eigenvalues which are $1$ and $-1$ , how do I go about proving what the question asks me?","['eigenvalues-eigenvectors', 'linear-algebra', 'diagonalization', 'matrices']"
1515812,How to evaluate $\int_0^{\pi /2}\frac{u^2\ln{(2\cos u)}}{(u^2+\ln^2{(2\cos u)})^2}du$?,"I want to find the value of  $$\int_0^{\pi /2}\dfrac{u^2\ln{(2\cos u)}}{(u^2+\ln^2{(2\cos u)})^2}du.$$ Let $v=\frac{\pi}{2}-u$, then
$$\int_0^{\pi /2}\dfrac{u^2\ln{(2\cos u)}}{(u^2+\ln^2{(2\cos u)})^2}du=\int_0^{\pi /2}\dfrac{(\pi /2-v)^2\ln{(2\sin v)}}{((\pi /2-v)^2+\ln^2{(2\sin v)})^2}dv.$$ I don't know how to go from here.
Thank you.","['calculus', 'closed-form', 'improper-integrals', 'definite-integrals', 'integration']"
1515817,What is your idea about this conjecture?,"I conjecture that in a consecutive sequence of $n$ natural numbers all greater than $n$, there exists at least one number which is not divisible by any prime number less than or equal to $n/2$. Can any one prove or disprove this?","['prime-numbers', 'number-theory', 'analytic-number-theory', 'elementary-number-theory', 'divisibility']"
1515860,Local diffeomorphisms as isomorphisms on stalks,"The stacks project says that a smooth map $f:M\rightarrow N$ naturally gives rise to an arrow between the sheaves of smooth functions. I don't understand how this fits in with the notion of a local diffeomorphism: a local diffeo is a smooth map whose germs are diffeomorphisms. If every such arrow induced (and was induced from) an arrow of sheaves, then every local diffeo would be a global diffeo. What am I missing?","['differential-geometry', 'smooth-manifolds', 'sheaf-theory']"
1515872,Degree of $f:\mathbb{P}^1_k\rightarrow \mathbb{P}^1_k$,"Let $k$ be an algebrically closed field and consider $\mathbb{P}^1_k$ the $1$-dimensional projective space over $k$. My question is the following: Let consider $f:\mathbb{P}^1_k\rightarrow \mathbb{P}^1_k$,
  what can be the possible degree of this map? I answered to myself that if $f:X \rightarrow Y$ is a morphism of curves, $\deg f =[K(X):K(Y)] $ so in our case $\deg f $ can be just equal to 1 and it has to be an isomorphism but it seems to me somehow controintuitive (I think to $S^1 \simeq \mathbb{P}^1_\mathbb{R} $ that admitt TOPOLOGICALLY coverings of arbitrary degree by itself). Consequence: The same proceedings ($[K(X):K(X)]=1$) can be generalize to generic maps $f:X \rightarrow X$, where $X$ is a curve, saying that this map can only be an isomorphism... Is this true?","['projective-geometry', 'algebraic-geometry', 'schemes', 'algebraic-curves']"
1515905,If $\int_{0}^{\frac{\pi}{4}}\tan^6(x) \sec(x) dx = I$ then express $\int_{0}^{\frac{\pi}{4}} \tan^8(x) \sec(x) dx$ in terms of $I$,how can I proceed with this exercise? If $$\int_{0}^{\frac{\pi}{4}} \tan^6(x) \sec(x) dx = I$$ then express $$\int_{0}^{\frac{\pi}{4}} \tan^8(x) \sec(x) dx$$ in terms of $I$. What I've got so far: $$\int_{0}^{\frac{\pi}{4}} \tan^8(x) \sec(x) dx = \int_{0}^{\frac{\pi}{4}} \tan^2(x) \tan^6(x) \sec(x) dx = \int_{0}^{\frac{\pi}{4}} \left( \sec^2(x) - 1 \right) \tan^6(x) \sec(x) dx = \\ = - \int_{0}^{\frac{\pi}{4}} \tan^6(x) \sec(x) dx + \int_{0}^{\frac{\pi}{4}} \tan^6(x) \sec^3(x) dx = -I + \cdots$$ Any help is highly appreciated. $\\$ (Exercise 50 from Stewart's Calculus book section chapter 7.2 7th edition),"['trigonometry', 'calculus', 'integration']"
1515946,Brownian motion martingale,"I have been wondering about the following equality in the textbook by Liggett. 
I put a red circle at the position where my question is. They use the theorem that $B_t^2-t$ is a martingale and the martingale stopping theorem to argue that $B_{t \wedge n}^2-(\tau \wedge n)$ is a martingale and from this they derive the equality of expectation values, but I don't see how this follows. Does anybody have an idea? Theorem 1.102. If $\tau$ is a stopping time with $E_\tau<\infty$, then (a) $EB(\tau)=0;$ (b) $EB^2(\tau)=E_\tau;$ (c) $E_\tau^2\le4EB^4(\tau).$ Proof. It is easier to prove the first two parts together. By Theorems 1.95(b) and 1.93, $B^2(\tau\wedge n)-\tau\wedge n$ is a martingale. Therefore $$EB^2(\tau\wedge n)\require{enclose}\color{red}{\enclose{circle}=}E(\tau\wedge n)\le E_\tau<\infty\tag{1.32}$$","['probability-theory', 'brownian-motion', 'probability', 'stochastic-processes']"
1515958,Has my understanding of a generating set been wrong this entire time?,"My understanding of the generating set (informally) was the following Pick any number of elements, say $g_1, g_2, \dotso, g_n$, in a finite group $G$. The set of all elements produced by a finite product of any combination of these elements is called the generating set of the elements $g_1, g_2, \dotso, g_n$. However, the course notes had the following definition of a generating set. If $S \subseteq$ G is a subset, then define $S^{-1} =  \left ( s^{-1} | s \in S  \right )$ and let $\langle S \rangle$ denote the set of all elements  of $G$ which can be written as finite products of elements of $S \bigcup S^{-1}.$ Am I missing something in my understanding of the generating set? Because in my understanding of it, there's absolutely nothing to do with any inverse elements whereas the course notes makes some mention of them for some reason.","['abstract-algebra', 'definition']"
1515979,"Are lines which pass respectively through vertices $A,B,C$ and incenter, circumcenter and orthocenter of $\Delta ABC$ concurrent?","Prove that the lines through $A$ and the incenter of $\Delta ABC $, through $B$ and the circumcenter of $\Delta ABC$, and through $C$ and the orthocenter of $\Delta ABC $ are concurrent if and only if $\cos^{2} A =\cos B \cdot \cos C $. My attempt: By Ceva's Theorem we have that $\cfrac {AP \cdot CQ \cdot BR}{PC \cdot BQ \cdot AR}=1$,which can be rearranged in the form $\cos^2 A=\left(\cfrac {AP \cdot CQ \cdot BR}{PC \cdot BQ \cdot AC}\right)^2$, from that I've tried to solve the problem working on these two directions: 1)Show that $\left(\cfrac {AP \cdot CQ \cdot BR}{PC \cdot BQ \cdot AC}\right)^2=\cos B \cdot \cos C$ 2) Since we have also that $\cos^2 A = \left( \cfrac {AR}{AC} \right)^2 $, we have to show that $\cos B \cdot \cos C = \left ( \cfrac {AR}{AC} \right)^2 $ I've worked most on the second line since it seems simpler and that's what I was able to do so far: Given that $ \cos B = \cfrac {BR}{RC} $ and $\cos C =\cfrac {BC^2 +AC^2-AB^2}{2AC \cdot BC} $, I have in the end (after some algebraic manipulations): $AC^2 +CB^2-AB^2 =\cfrac {2AR ^2 \cdot BC^2}{AC \cdot BR}$ and that's where my tombstone is. I don't know how to simplify this any further, I don't know if it is even worth to simplify it given that this might be the wrong path to take... I know that I am not coming to a solution since I am not  using the fact that $BP$ passes through the circumcenter $K$ of $\Delta ABC$ and that obviously is a key point in solving the problem but I don't know how to use this information. Edit: I've tried the following:
Let $J$ be the point of intersection of lines $BP$ (this line passes through the circumenter $K$) and line $CO$ (where $O$ is the orthocentre ) ,so now what i've to do is to prove that $A,J,Q$ are collinear,i.e. I have to prove that $$\cfrac{AP \cdot CQ \cdot BJ}{BQ \cdot AC \cdot PJ}=1 \tag 1$$ .
Applying the Angle Bisector Theorem to $A$ I find $\cfrac{CQ}{BQ}=\cfrac { AC}{AB}$ from  which i have got the following :  $$ \cos^2 A = \left ( \cfrac {PJ \cdot BQ \cdot CQ}{BJ \cdot AP} \right)^2 $$ Since we know that $\cos B = \cfrac{BR}{BC}$ so i have to prove 
$$ \cos C = \left (\cfrac { PJ \cdot BQ \cdot CQ}{BJ \cdot AP} \right)^2  \cdot \cfrac {BC}{BR}$$ but so far i was unable to do that. Any hint ,solution is appreciated. *Only geometrical methods,please.","['euclidean-geometry', 'geometry', 'triangles']"
1515988,Is there a geometrical meaning of $\limsup_{n\to\infty }A_n$?,"Let $(A_n)$ a sequence of sets. How can I see geometrically $\limsup\limits_{n\to\infty} A_n$ ? I tried to make drawings, but I don't really see how it works. I know that $x\in\limsup\limits_{n\to\infty }A_n$  if for all $n\in\mathbb N$ there is a $p\in\mathbb N$ such that $x\in A_k$ for all $k\geq p$, or in other words that $x\in A_k$ for an infinite number of $A_k$. But it doesn't say much. I would like a geometric vision if it's possible.","['elementary-set-theory', 'limsup-and-liminf', 'measure-theory']"
1515990,Points necessary to intersect all lines in finite projective geometry,"I'm reading about finite geometries, projective and affine. I wonder what the smallest set of points is, given a geometry $PG(d,q)$, that intersects all lines. (or hyperplanes.) For example in the Fano plane, it looks like three points are enough. I'm still not quite used to thinking about geometries like this, so I wonder if anybody has a hint for how I might approach the problem? Edit: It's clear that $q+1$ points are enough, since you can use that to cover an entire line, and all lines intersect in some point. However, it's not as clear to me that you can't do with less points. Update: For AG(d,q), this paper says that we need exactly $d(q-1)+1$ points to intersect all hyperplanes.","['geometry', 'combinatorial-designs', 'finite-geometry', 'combinatorics']"
1516021,Solving the cubic-exponential Diophantine equation $x^3+3=2^n$,"The Diophantine equation $x^3+3=2^n$ has the obvious solutions $(-1,1)$,$(1,2)$ and $(5,7)$. I have been wondering if there are any other, but my attempts have been fruitless (I tried factoring it over $\Bbb{Q}(\sqrt[3]{3})$, but I don't know the basic properties of this field, such as what the ring of integers are, the class number etc.). Any help in solving this problem would be greatly appreciated. Edit: We can actually split this into two, more general equations, namely the elliptic curves
$$
x^3+3=y^2
\quad \text{and} \quad
x^3+3=2y^2
$$
so this opens up another method for solving it.The first one is, in fact, a special case of the infamous Mordell equation. Edit 2: Looking at this paper by Tzanakis and De Weger : http://www.math.uoc.gr/~tzanakis/Papers/PracticalSolutionThueEq.pdf I was wondering if we could use the methods explained in Section 3 and extend them to these equations (the methods in the paper, however, require some computational machinery).This could lead to solving a Thue equation, which has only finitely many integral solutions and the general method for solving them is in the paper linked.","['field-theory', 'number-theory', 'diophantine-equations', 'algebraic-number-theory']"
1516027,Measuring $\pi$ by throwing darts,"I want to give an approximation of $\pi$ in this way: 
I inscribe a circle in a square then I throw darts at random on the square from far away. If the darts falling on the square are $n$ and the darts falling on the circle are $m<n$ I approximate $\pi$ with $4 \frac{m}{n}$. Suppose I want an approximation such that $|4\frac{m}{n}-\pi|<0,0001$. How can I quantify how many shots I have to do at least (before doing the experiment)? I know that the strong law of large numbers tells me that
$P(\lim_{n\to\infty} 4\frac{m}{n}=\pi)=1$, but I can't do an infinite number of shots so I try with the weak law: $\lim_{n\to\infty}P(|4\frac{m}{n}-\pi|<0,0001)=1$.
Again, this seems to be unhelpful to my cause. Maybe I could take the compromise that I want an approximation such that $|4\frac{m}{n}-\pi|<0,0001$ with a probability greater than $0,95$, but even if I succeed in this goal, nothing assures me that in $5\%$ of cases, the approximation obtained is such that $|4\frac{m}{n}-\pi|>3$. What your approach to this problem would be?","['probability-theory', 'probability', 'probability-limit-theorems', 'law-of-large-numbers']"
1516062,A good way to solve this trigonometric equation,"$$\sin x+\cos x=\frac{1}{2}$$ What is the value of $\tan x$? I tried using 
$$\sin2 x=\frac{2\tan x}{1+\tan^2x}$$ and $$\cos2 x=\frac{1-\tan^2x}{1+\tan^2x}$$ but we get a quadratic for $\tan\left(\frac{x}{2}\right)$ . So any better approach would be much appreciated. Thanks!","['geometry', 'trigonometry']"
1516115,How is integrability used in fundamental principle 'you can't beat the system'?,"From Williams' Probability w/ Martingales: Re (iii), why do we need square integrability? I mean, why is integrability not good enough? Based on an answer in my previous question, I think integrability is sufficient for 'taking out what is known' .","['probability-theory', 'expectation', 'stochastic-processes', 'martingales', 'integration']"
1516120,"Understanding proof that $\mathbb{Q}_{p}(\zeta)/\mathbb{Q}_{p}$ is unramified for $(n,p)=1$.","Problem Consider the extension $\mathbb{Q}_{p}(\zeta)/\mathbb{Q}_{p}$, where $\zeta$ is a $n$-th primitive root of unity and $(n,p)=1$. I want to show that $\mathbb{Q}_{p}(\zeta)/\mathbb{Q}_{p}$ is an unramified extension. My attempt of understanding proof Let $f\in\mathbb{Q}_{p}[x]$ be the minimal polynomial of $\zeta$ over $\mathbb{Q}_{p}$. Question $(1)$. Is $f$ a polynomial with coefficientes actually in $\mathbb{Z}_{p}$? Why? If the above is true, then the polynomial $\overline{f}$obtained by reducing coeficients modulo $p\mathbb{Z}_{p}$ is monic and vanishes at the image $\overline{\zeta}$ of the element $\zeta$ by the canonical homomorphism $\mathcal{O}\to\mathcal{O}/\beta$, where $\mathcal{O}$ (resp. $\beta$) is the valuation ring (resp. maximal ideal) of $\mathbb{Q}_{p}(\zeta)$. By the way, no doubt that $\zeta\in\mathcal{O}$. If we show that $\overline{f}$ is irreducible, then it is the minimal polynomial of $\overline{\zeta}$ over $\mathbb{F}_{p}$. We know that $f$ is a primitive polynomial (since it is monic) and hence we can use Hensel's Lemma as a tool for that. Namely, if $\overline{f}$ is a product of coprime polynomials $\overline{g}$, $\overline{h}\in\mathbb{F}_{p}[x]$, each of degree $>1$, then $f$ would be a product of polynomials $g$, $h\in\mathbb{Z}_{p}[x]$, each of degree >1, a contradiction. However, we still need to prove that $\overline{f}$ is not a product of noncoprime polynomials. Neukirch says that since $\overline{f}$ divides the coefficient-reduced modulo $p\mathbb{Z}_{p}$ separable polynomial $x^{n}-\overline{1}\in\mathbb{F}_{p}[x]$, this case cannot happen, but I don't understand this. Question $(2)$. Why is $x^{n}-\overline{1}\in\mathbb{F}_{p}[x]$ a separable polynomial? Question $(3)$. If $\overline{f}$ is separable, then can't it be a product of noncoprime polynomials? If the above is cleared, then we actually confirmed that $\overline{f}$ is the minimal polynomial of $\overline{\zeta}$. In particular, being irreducible gives us that $\operatorname{deg}(f)=\operatorname{deg}(\overline{f})$, which means \begin{equation}
[\mathbb{Q}_{p}(\zeta)\colon\mathbb{Q}_{p}]=\operatorname{deg}(f)=\operatorname{deg}(\overline{f})=[\mathbb{F}_{p}(\overline{\zeta})\colon\mathbb{F}_{p}].
\end{equation} On the other hand, we have that \begin{equation}
[\mathbb{Q}_{p}(\zeta)\colon\mathbb{Q}_{p}]\geq [\beta\colon\mathbb{F}_{p}]
\end{equation} Putting these two together gives $\mathbb{F}_{p}(\overline{\zeta})=\beta$ and hence the extension is unramified. Is this correct? Thank you advance for answering my above questions EDIT . Can someone point out where we used that (n,p)=1?","['p-adic-number-theory', 'number-theory', 'local-field', 'valuation-theory', 'algebraic-number-theory']"
1516137,The preimage of a Lebesgue measurable set under a measurable function need not be measurable [duplicate],"This question already has an answer here : Pre-image of a measurable set A is always measurable? (1 answer) Closed 3 years ago . I am reading measure theory from Royden, and I am stuck in some of them. I have this question: suppose  $E$ is  a measurable set and let $f: E \to \mathbb{R}$. Prove that : $f$ is measurable if and only if $f^{-1}(A)$ is measurable for any $A \subseteq \mathbb{R}$. I know this is not true if measurable means ""Lebesgue measurable"", can anyone give a counterexample in details ?","['examples-counterexamples', 'real-analysis', 'measure-theory']"
1516153,Classification of Polish topologies on a countably infinite set,"Let $X$ be a countably infinite set.
While investigasting the literature on Polish spaces, I met so far only examples for compact or locally compact Polish topologies on $X$ : the order topology on $[0, \Gamma)$ for a countable limit ordinal $\Gamma$ ( see here ) - this one is locally compact but not compact (for $\Gamma = \omega$ we have the discrete topology on $\mathbb{N} = [0, \omega)$ ) one-point-compactifications of $X \setminus \{ x \}$ for some $x \in X$ are compact Polish, e.g. $[0, \Gamma]$ (but see also the example of the countable Fort space . Wikipedia also mentions that the Fort space arises as a one-point-compactification of some discrete space). Question 1: If $X$ carries a Polish topology is $X$ necessarily locally compact? Probably we can do more: Question 2: Are the above examples some kind of ""prototypes"" for any (locally) compact Polish topology on $X$ ? By prototype I mean something like any Polish topology arises from these examples by those operations for which $X$ is Polish and $X$ remains countably infinite, e.g. finite disjoint unions, finite products, countably infinite $G_\delta$ -subsets. I somehow doubt that such a simple classification holds. There are for sure examples that I have overlooked.","['descriptive-set-theory', 'general-topology']"
1516215,Counting the number of ways to cover $4 \times n$ board with $1 \times 3$ or $3 \times 1$ dominoes,"I'm trying to solve the problem described in the title by writing down some recurrent relation for the number of ways $T(n)$. It is not very simple, one can not just say that it is $T(n) = 3 \times T(n - 3)$. What I do is trying to describe the ``complete set of final profiles''. Profile is four numbers $(a_1, a_2, a_3, a_4)$ that stand for length of corresponding row. Let us say, that there are 5 profiles that form this ``complete set'': $T(n) = (n, n, n, n)$, $A_{-1}(n) = (n, n, n, n - 1)$ or $(n - 1, n, n, n)$, $A_{-2}(n) = (n, n, n, n - 2)$ or $(n - 2, n, n, n)$, $A_{+1}(n) = (n, n, n, n + 1)$ or $(n + 1, n, n, n)$, or $A_{+2}(n) = (n, n, n, n  + 2)$ or $(n + 2, n, n, n)$. Then, I need a system of recurrent relations for these quantities. And this is where problems start. Say, I wish to write:
$$T(n) = 3 T(n - 3) + 2 \times (A_{+2}(n) + A_{+1}(n) + A_{-2}(n - 1) + A_{-1}(n - 2)).$$ In the last bracket I'm trying to count the number of transitions between profiles $A$ and $T$. But I really don't know if this set is not overfilled? How can I be sure that I don't calculate anything twice? What is the correct choice of these profiles and the correct way to write down relations between them?","['sequences-and-series', 'combinatorics']"
1516218,Subgroups of Index $2$ of $(\mathbb{Z}_{2})^{\aleph_{0}}$,"I am studying infinite Galois theory and I proved that if 
$$
L=\mathbb{Q}(\sqrt{p}:\,\text{$p$ is prime )}
$$ That is $L$ is the composition of all fields of the form $$\mathbb{Q}(\sqrt{p})$$ where $p$ is a prime then
$$
Gal(L/\mathbb{Q})\cong(\mathbb{Z}_{2})^{\aleph_{0}}
$$ The notes I read claim that there are only $\aleph_{0}$ subextensions
$E$ s.t 
$$
\mathbb{Q\subseteq E\subseteq L}
$$ and 
$$
[E:\mathbb{Q}]=2
$$ but $\aleph$ subgroups of index $2$ for $(\mathbb{Z}_{2})^{\aleph_{0}}$. The first part is clear since $L$ is countable and each such extension
is of the form $\mathbb{E=Q}(\sqrt{\alpha})$ for $\alpha\in L$. Can someone help me understand why $(\mathbb{Z}_{2})^{\aleph_{0}}$
have $\aleph$ subgroups of index $2$ ?","['abstract-algebra', 'group-theory', 'galois-theory']"
1516234,"Critical point of a function of two variables $f(x,y)=3x^4 -4x^2y+y^2$: minimum, maximum or saddle?","If $f(x,y)=3x^4 -4x^2y+y^2$ then : a) $(0,0)$ is local max point b) $(0,0)$ is local min point c) $(0,0)$ is saddle point d) none I think we find critical points so we must find all point of equation $\nabla f =0$ then $(0,0)$ is critical point but $f_{x,x} = f_{y,y} =f_{x,y}=0$ now  we can't use  this test.","['optimization', 'calculus', 'multivariable-calculus']"
1516260,If a property in $\mathbb{N}$ is true up to $10^{47}$ are there reasons to think it is probably true in all $\mathbb{N}$?,"You have probably heard at some point statements like that the twin prime conjecture (namely that $2$ is an infinitely ocurring prime gap) is ""probably"" or ""almost certainly"" true. Same goes for a number of other problems in number theory asserting properties that we believe are satisfied by all natural numbers. An argument that seems to give ground to such beliefs is the observation that said conjecture holds up to a certain large number. My question would be if there some solid argument to think that a given conjecture is ""probably"" true, given that we know it is true up to some large number (as big as you wish but fixed). The answer upfront seems to be no, and still the thought is so unavoidable I wonder if someone has thought of an argument to justify it. EDIT: Do not take the question rigurously. I was looking for a broad take on this. Think for example of the case of the twin prime conjecture. Of course I know there are properties satisfied by numbers only above a given number (for example the property of being bigger than said number). Be imaginative. Tell me results or arguments that you think might relate. Tell me about conjectures where, while not knowing if they hold, this line of thought is somehow justified and why. EDIT $2$: Case where this question would apply . Let's say we conjecture some property of the natural numbers. About the size of a possible minimum counterexample we are not able to find anything besides that it must be greater than $10^{47}$. At this point the thought might reach our minds that the conjecture is ""probably"" true. Now, is there any convincing argument that justifies this thought?","['philosophy', 'number-theory', 'elementary-number-theory']"
1516265,Map closed under addition but not multiplication,I have been helping undergrads in an introduction to linear algebra course. When solving some exercise consisting in showing that a map is linear some get lazy after proving that it is closed under addition and do not prove the closure under scalar multiplication. I wanted to confront them with an example of a map closed under addition but not under the multiplication but could not come up with an example. Do you have any?,"['education', 'linear-algebra']"
1516295,Show that $\sum_{n=1} ^{\infty} \frac {1}{(x+n)^2} \leq \frac{2}{x} $,"Show that $\displaystyle \sum_{n=1} ^{\infty} \frac {1}{(x+n)^2} \leq \frac{2}{x} $ For any real number x $\geq 1$, I want to show the above Very rusty on my analysis, I think I need to do a comparison test to show the series converges, but then not sure what after that to get the inequality.","['sequences-and-series', 'real-analysis', 'inequality']"
1516309,Total variation of a continuously differentiable function,"Let $f\in C^1[0,1]$. For a partition $\mathcal {P} :0=a_0<a_1<\cdots <a_n=1$ , define $$S(\cal P)=\sum_{k=1}^n|f(x_k)-f(x_{k-1})|.$$Compute the supremum of $S(\cal P)$ over all possible partitions of $[0,1]$. Here , $\displaystyle \sup_{\cal P}S(\cal P)$ is the total variation of he function $f$ in $[0,1]$. But if the function is unknown how we find the total variation ? I saw this but it is NOT clear .","['analysis', 'real-analysis']"
1516328,Is equality inherently defined?,"I recently pulled out my old Real Analysis textbook and noticed something that didn't stand out when I was taking the class all those years ago.  When the book is listing out the axioms it seems to assume we understand what equality is. Consider the first axiom $(A)$ , which defines commutivity over addition and multiplication.  It states $x+y=y+x, \forall x,y \in \mathbb{R}$ and similiarly for multiplication. I understand that we're in process of defining how $+$ operates on $\mathbb{R}$ here, but we never defined what $=$ means.  Equality seems like a crucial concept, but I never see it defined anywhere. Is equality inherently defined or an understood concept, or does it have a more formal definition? EDIT It was commented that $x+y$ is not the same as $y+x$ , but rather for the case above they could be considered equal if they evaluate to the same real number.  My question has to do with the general sense, though.  It seems this axiom is defining a property of $+$ to say that these two terms can be treated the same, an idea which could be applied in other situations outside of the example of the $+$ operation and $\mathbb{R}$ .","['logic', 'real-analysis']"
1516348,Image of set of measure zero has measure zero if the function is absolutely continuous,"I have been trying to solve this problem from Bass. Let $f$ be a real valued absolutely continuous function defined on $[0,1].$ Denote $f(A)=\{f(x): x \in A\}$ for $A \subset [0,1].$ Prove that if $A$ has Lebesgue-measure zero, then so does $f(A)$ . My attempt: By outer regularity, given an $\varepsilon>0$ , I have a collection of disjoint open intervals $\{(a_i,b_i)\}_{i \geq 1}$ such that $A \subset \bigcup_i (a_i,b_i)$ and $\sum_i (b_i-a_i) < \varepsilon$ . Can we link $f(A)$ and $\bigcup_i f((a_i,b_i))$ ? We know that $f(a,b)$ is an interval but what about it's length ? Is it related to $f(b)-f(a)?$","['real-analysis', 'absolute-continuity']"
1516438,Find the limit $\lim_\limits{x\to +\infty}{\left( \left( e+1\right) ^{\ln \left( e^x+1\right)} - \left( e+1\right) ^x\right)} $,"Find without using De L'Hospital's rule the following limit: $$\lim_\limits{x\to +\infty}{\left( \left( e+1\right) ^{\ln \left( e^x+1\right)} - \left( e+1\right) ^x\right)} $$ I have tried to factorize it but I always seem to end up with an indeterminate form... How can I do it with using DLH? Please don't use approximations because I haven't ""officially"" learnt them yet...","['limits-without-lhopital', 'limits']"
1516483,"What does it mean to ""Converge in Law""?","If $X_1, X_2, X_3, \cdots$ is a sequence of independent identically distributed random variables with $E[X_i] < \infty$ and $Var(X_i)< \infty$ such that the sequence $Y_n = 3 \frac{X_1 + X_2 + \cdots + X_n }{\sqrt{n}}$ converges in law to a standard Normal distribution, compute $E[X_i]$ and $var(X_i)$. As it is used in this question, what does it mean for this sequence to converge in law to a standard normal? I know law is synonymous with distribution but I still don't fully understand the implications of a convergence in law.","['probability-theory', 'convergence-divergence', 'probability-distributions', 'law-of-large-numbers']"
1516538,"Is it possible that $A\subseteq A\times B$ for some non empty sets $A,B$?","I was wondering if there exist two non empty sets $A,B$ such that
$$A\subseteq A\times B.$$
I know that always exists a subset of $A\times B$ with the same cardinality of $A$, but i'm requesting here that $A$ is a subset of $A\times B$ without using any identification map. 
At first i thought that this was not possible because $A$ and $B\times A$ are two sets containing different kind of elements: the second contains pairs like $(a,b)$ with $a\in A, b\in B$; the first just single elements $a\in A$. Moreover, suppose $A\subseteq A\times B$ holds and take $a \in A$. Then $a=(a_1,b_1)$ for some $a_1 \in A, b_1\in B$. For the same reason $a_1=(a_2,b_2)$ and so $a=((a_2,b_2),b_1)$. Following this argument I got some sort of recursive infinite definition for $a$ that made me suspect something is wrong. However if I take $$A=\mathbb{N}^{\mathbb{N}} ;B=\mathbb{N}$$
is it true that $A=A\times B$ or I'm missing something? Moreover, if $A\subseteq A\times B$ can be true, are there other examples? edit: I add another strange example: take $A=\bigcup_{i=1}^{\infty} \mathbb{N}^i $ and $B=\mathbb{N}$, then $A \times B \subset A$. This makes me think that maybe exists also an example for the other inclusion.",['elementary-set-theory']
1516553,Subsets of a Cartesian product are disjoint iff there exist projections that are disjoint,"The Theorem? Suppose $\{X_\alpha\}_{\alpha \in J}$ is a family of non-empty sets. Let $X = \prod_{\alpha \in J} X_\alpha$. For each $\alpha \in J$ define $p_\alpha : X \to X_\alpha$ to be the canonical projection from $X$ to $X_\alpha$. Suppose $U,V$ are subsets of $X$. I want to show that $U$ and $V$ are disjoint if and only if there exists $\beta \in J$ such that $p_\beta(U) \cap p_\beta(V) = \emptyset$. Background I wanted to use this ""theorem"" to show that if the product space $X$ is $T_4$ then so is each $X_\alpha$. My Thoughts The $\Longleftarrow$ direction seems straightforward from the definition of the Cartesian product, although I haven't worked it out precisely since it is really $\Longrightarrow$ I care about. Likewise, it seems intuitively true that $\Longrightarrow$ is true. I've done a bit of googling and searching through textbooks, but I haven't quite found what I am looking for. Moreover, I am unsure how to proceed with a proof. Here is my problem: I want to say that if $U$ and $V$ are disjoint subsets of $X$, then I can write $U$ is a product of subsets of $X_\alpha$ for each $\alpha \in J$, but this can't be correct. For example, $U$ could be the union of subsets of $X$ which cannot, in general, be written as a product of a union of subsets.","['elementary-set-theory', 'general-topology']"
1516573,Universal property of l^p-spaces,"The category $\mathsf{Ban_1}$ of Banach spaces together with short linear maps (i.e. those of norm $\leq 1$) seems to have a natural construction which interpolates between coproduct and product: Let $p \in [1,\infty]$ and let $(V_i)_{i \in I}$ be a family of Banach spaces over $\mathbb{K}=\mathbb{R},\mathbb{C}$. Then we may define a new Banach space $\bigoplus^p_{i \in I} V_i$ (how is this usually denoted?) as follows: The points are the sequences $(v_i)_{i \in I}$ with $v_i \in V_i$ and $\sum_{i \in I} \lVert v_i \rVert^p < \infty$. We let $\lVert (v_i)_{i \in I} \rVert := (\sum_{i \in I} \lVert v_i \rVert^p)^{1/p}$. For $p=\infty$ this has to be interpreted as $\lVert (v_i)_{i \in I} \rVert := \sup_{i \in I} \lVert v_i \rVert < \infty$. For $V_i=\mathbb{K}$ we get the usual space $l^p(I)$. If $p=1$, then $\bigoplus^1_{i \in I} V_i$ is the coproduct of $(V_i)_{i \in I}$. If $p=\infty$, then $\bigoplus^{\infty}_{i \in I} V_i$ is the product of $(V_i)_{i \in I}$. Question. Does $\bigoplus^p_{i \in I} V_i$ have a useful universal property if $1<p<\infty$? If necessary, you may work in a different category than $\mathsf{Ban_1}$. But would be nice if this category does not depend on $p$.","['functional-analysis', 'lp-spaces', 'banach-spaces', 'universal-property', 'category-theory']"
1516580,Show $f$ is integrable,"Let $f$ be such that $\int_0^{\infty} |f(s)|e^s ds< \infty.$ Now, I want to argue that for $x,y$ sufficiently large and $\lambda < 1$ fixed we have that $$\int_0^{\infty} \int_x^y e^{\lambda z} e^{s} |f(s+z)|dz ds$$ can be made arbitrarily small for $\lambda <1.$ In other words: $\forall \varepsilon >0 \exists N: \left(x,y >N \Rightarrow \int_0^{\infty} \int_x^y e^{\lambda z} e^{s} |f(s+z)|dz ds< \varepsilon \right)$ But how can I show this rigorously? Does anybody have an idea","['analysis', 'real-analysis', 'lebesgue-integral', 'integration']"
1516590,Calculating a simpler derivative for $\frac{1+\cot x}{2- \sec x}$,"So I'm fairly new to derivative exercises, and I am often concerned about the fact that many of my answers are larger than the original function. For example: $$\frac{1+\cot x}{2- \sec x}$$ Becomes $$\frac{-\csc^2x \cdot(2-\sec x) + \sec x \cdot \tan x \cdot (1 + \cot x)}{(2-\sec x)^2}$$ I am assuming that this is technically the right answer, but it is surprisingly long and would probably make things very messy if I try to calculate the second derivative. So my question is, is there a way to simplify this derivate in case I need to calculate yet another derivative? I'm afraid they just keep getting larger.","['calculus', 'derivatives']"
1516602,Does weak convergence in $L^2$ imply almost everywhere convergence of Cesaro averages?,"Consider a bounded sequence $f_n\in L^2(X,\mu)$, $\|f_n\|_{L^2}\leq C$. Is the following true: if $f_n \to f$ weakly (that is $\langle f_n,g \rangle \to \langle f,g \rangle$ for every $g\in L^2(X,\mu)$),
then Cesaro averages $\frac{1}{n} \sum_{k=1}^n f_k$ converge to $f$ almost everywhere? What about other relations between convergence of a sequence and convergence of its Cesaro averages? Added afterwards: The first statement is false.
Consider $f_n(x) = \sin kx$ for $10^{k-1}\leq n < 10^k$.
Obviously, $f_n\to^w 0$, while $| \frac{1}{10^n} \sum_{k=1}^{10^n} f_k - \sin nx|\leq \frac{1}{10}$, meaning that $\limsup_n |\frac{1}{n} \sum_{k=1}^n f_k(x)| > 0$ for a.e. $x\in X$. Question about other types of convergence remains open: Assuming that a sequence $f_n\in L^2$ converges weakly, can we say anything about the sequence of Cesaro averages $\frac{1}{n} \sum_{k=1}^n f_k$? Does it converge strongly, in measure?.. In fact , the question is motivated by the following problem:
Fix $\varepsilon >0$. Is it true that $$ \mu(\{ x\in X : \limsup_n |\frac{1}{n} \sum_{k=1}^n f_k(x)| > \varepsilon \}) < \varepsilon $$ whenever $\|f_n\|_{L^2}< \delta$ for small enough $\delta = \delta(\varepsilon)$? I will be grateful for any useful comments, suggestions or references!","['measure-theory', 'almost-everywhere', 'random-variables', 'functional-analysis', 'weak-convergence']"
1516615,Calculating the derivative of $\csc^2(4x)$,"I am having problems calculating derivatives of squared trigonometric functions. $$f(x) = \csc^2(4x)$$ Let's see... This is equivalent to $$f(x) = \csc(4x)\cdot \csc(4x)$$ Now, to get the derivative, we use the product rule: $$f'(x) = 2 \cdot (-\csc(4x)\cdot\cot(4x)\cdot\csc(4x))$$ But this appears to be wrong when I evaluate this in a calculator. What is the problem with this procedure?","['calculus', 'derivatives']"
1516624,$X_n\xrightarrow[]{p}c$ implies that $\vert X_n\vert\xrightarrow[]{p}\vert c\vert$?,"If $X_n\xrightarrow[]{p}c$, does this imply that $\vert X_n\vert\xrightarrow[]{p}\vert c\vert$? Since $X_n\xrightarrow[]{p}c$, we know that $\mathbb{P}(\vert X_n - c\vert>e)\rightarrow 0$. By the reverse triangle inequality, we know $\big \vert X_n - c\big\vert \geq\big\vert \vert X_n \vert- \vert c\vert \big\vert$, which implies that $\{w : \big\vert \vert X_n \vert- \vert c\vert \big\vert>\epsilon\}\subset\{w : \big\vert X_n - c \big\vert>\epsilon\}$. This shows that $\mathbb{P}(\big\vert \vert X_n \vert- \vert c\vert \big\vert>\epsilon)\leq\mathbb{P}(\big\vert X_n - c \big\vert>\epsilon)\rightarrow 0$, which proves the claim?","['probability-theory', 'probability', 'probability-limit-theorems']"
1516659,First course in linear algebra and matrices over arbitrary fields,"I'm looking for an elementary introduction to linear algebra (and matrices) over an arbitrary field. A lot of recommended books on the subject work only over the field of real or complex numbers (and this is inappropriate for me, I would like that there won't be a discussion whether I'm right from a pedagogical perspective). I know of the book by Hoffman and Kunze. Are there any more books like that (maybe more modern and a little less dry, maybe not)?","['book-recommendation', 'reference-request', 'linear-algebra', 'matrices']"
1516666,"The Radon-Nikodym derivative of a measure such that $|\int f'\,d\mu|\le \|f\|_{L^2}$ for $f\in C^1$","Suppose that $\mu$ is a measure on the Borel $\sigma$-algebra on $[0,1]$ and for every $f$ that is real-valued and continuously differentiable we have $$
\left|\int f'(x)~\mu(\text{d}x)\right|  \leqslant \left(\int_0^1 f^2(x)
~\text{d}x\right) ^{1/2}.
$$ (1) Show that $\mu$ is absolutely continuous with respect to Lebesgue measure on [0,1]. (2) If $g$ is the Radon-Nikodym derivative of $\mu$ with respect to Lebesgue measure, prove that there exists a constant $c>0$ such that
$$
\left|g(x)-g(y)\right|\le c\left|x-y\right|^{1/2},~~~~~~x,y\in[0,1].
$$ Attempt The inequality given reminds me of Jensen's inequality but I've no idea how to use it on (1).","['derivatives', 'real-analysis', 'functional-analysis', 'integration']"
1516670,What is a quicker method to differentiate $h(x)=2x-|3x-3|+|x^2-1|$?,"$$h(x)=2x-|3x-3|+|x^2-1|$$ My Approach: $$u=|3x-3|$$
$$u^2=(3x-3)^2$$
$$2 u'=2(3x-3)\cdot3$$ $$u'=3(3x-3)$$ $$m=|x^2-1|$$
$$m^2=(x^2-1)^2$$
$$2m'=2(x^2-1)\cdot 2x$$ $$m'=2x(x^2-1)$$ $$\therefore h'(x)=2-3(3x-3)+2x(x^2-1)$$ Quicker and shorter method?","['calculus', 'derivatives']"
1516725,"If I know all the distinct factorizations of a number, how do I use that to figure out the unique factorization of the ideal?","For example, in $\textbf{Z}[\sqrt{10}]$, we have $$6 = 2 \times 3 = (4 - \sqrt{10})(4 + \sqrt{10})$$ and $$10 = 2 \times 5 = (\sqrt{10})^2.$$ How do I use this knowledge to figure out the factorizations of $\langle 6 \rangle$ and $\langle 10 \rangle$? Or is there something else I would need to know before being able to factorize those ideals? I am aware of $(-1)(2 - \sqrt{10})(2 + \sqrt{10})$, but I'm also aware that it is not distinct from the other factorization I listed above (multiplication by unit). But is that other factorization somehow more ""fundamental""? Would it get me quicker to the factorization of $\langle 6 \rangle$?","['number-theory', 'ideals', 'algebraic-number-theory']"
1516739,"Limit of $\int_E f(nx) dx$ for a $1$-periodic function $f$ on $[0,2\pi]$","Let $E$ be a measurable subset of $[0, 2\pi]$. Assume that $f \in C(\mathbb R)$ is $1$-periodic, i.e. $f(x + 1) = f(x)$. Compute $$\lim_{n\to\infty} \int_{E} f(nx) dx$$. Since $f$ is continuous on $\mathbb R$ so it is continuous in each measurable subset of $[0, 2\pi]$ and so $f(nx)$ is Lebesgue integrable for each $n$. But how can I use $1$ periodicity of $f$?","['periodic-functions', 'real-analysis', 'lebesgue-integral', 'measure-theory']"
1516746,convergence of expectation sum of infinite random variables,"If $X_n$ are independent random variables such that $\sum \mathbb{E} X_n$ exists,
  and that $\sum X_n$ converges a.s. (almost surely), must it be that $\mathbb{E} \sum X_n = \sum
  \mathbb{E} X_n$? If $X_n \ge 0$, then this is obvious by using monotone convergence theorem since $\sum_{n=1}^{N} X_n$ is non-decreasing. Also, we know the following three things from $\sum X_n$ converges a.s. 1).$\sum\mathbb{P}(|X_n|>1)<\infty$ 2).$\sum\mathbb{E}Y_n<\infty$, where $Y_n=X_n\mathbb{1}_{|X_n|\le 1}$ 3).$\sum Var(Y_n)<\infty$ I am stuck on this problem and don't know how to proceed. Any help appreciated.","['probability-theory', 'expectation', 'measure-theory', 'independence', 'convergence-divergence']"
1516747,Prove $f^{-1}(U_1 \times \cdots \times U_n) = \bigcap_{i \in I} (f_i)^{-1}(U_i)$,"I was looking through some problems in one of my books which does not have solutions in the back, and I found a problem stating to construct a proof for the following problem. If someone would not mind verifying whether or not the argument is valid, I would appreciate it. Problem Let the set $I = \{1, \dotsc, n\}$ for some $n \in \mathbb{N}$, let $B$ be a set, let $A_1, \dotsc, A_n$ be sets, let $U_i \subseteq A_i$ be a subset for all $i \in I$, and let $f: B \rightarrow A_1 \times \cdots \times A_n$ be a function. Prove that $$f^{-1}(U_1 \times \cdots \times U_n) = \bigcap_{i \in I} (f_i)^{-1}(U_i)$$ where the $f_i$ are the coordinate functions of $f$ (Recall that the coordinate function $f_i : B \rightarrow A_i$ is defined by $f_i = \pi_i \circ f$ for each $i = \{1, \dotsc, n\}$, and the function $\pi_i: A_1 \times \cdots \times A_n \rightarrow A_i$ is a projection mapping). Proof First, let $b \in B$. If $b \in f^{-1}(U_1 \times \cdots \times U_n)$, then there exists an image $f(b) \in U_1 \times \cdots \times U_n$, so that $f(b) = (u_1, \dotsc, u_n)$ for some $u_i \in U_i$, where $i \in I$. Mapping $f(b)$ to an element in $U_i$ by $\pi_i$, we obtain the image $\pi_i(f(b)) = f_i(b) \in U_i$. Consequenlty, $b \in (f_i)^{-1}(U_i)$ for each $i \in I$, so we can write $$b \in \bigcap_{i\in I} (f_i)^{-1}(U_i)$$ from which we conclude that $$f^{-1}(U_1 \times \cdots \times U_n) \subseteq \, \bigcap_{i \in I} (f_i)^{-1}(U_i)$$ Now suppose that $b\in B$. Assuming $m \in \bigcap_{i \in I} (f_i)^{-1}(U_i)$, we deduce that, for every $i \in I$, $b\in (f_i)^{-1}(U_i)$. Hence, there exists $f_i(b) \in U_i)$ such that $f_i(b) = \pi_i(f(b))$. Define the preimage $(\pi_i \circ f)^{-1} = \{u \in U_1 \times \dotsc \times U_n \, : \, (\pi_i \circ f)(u) \in U_i\}$. Clearly, $\pi_i(f(b)) \in (\pi_i \circ f)^{-1}$, so $f(b) \in U_1 \times \cdots \times U_n$. Consequently, $b \in f^{-1}(U_1 \times \cdots \times U_n)$. Therefore, $$\bigcap_{i \in I} (f_i)^{-1}(U_i) \subseteq f^{-1}(U_1 \times \cdots \times U_n)$$ We conclude that $$f^{-1}(U_1 \times \cdots \times U_n) = \bigcap_{i \in I} (f_i)^{-1}(U_i)$$ $\blacksquare$","['elementary-set-theory', 'proof-verification', 'relations', 'functions']"
1516771,Prove that $g : B \rightarrow A_1 \times \dotsc \times A_n$ is unique,"I was wondering if someone would not mind proofreading my demonstration for the following problem. Any sentences in brackets [] will be omitted in the formal proof. Problem Let $B$ be a set, let $A_1, \dotsc, A_n$ be sets for some $n \in \mathbb{N}$ and let $h_i : B \rightarrow A_i$ be a function for each $i \in \{1,\dotsc,n\}$. Prove that there is a unique function $g:B\rightarrow A_1 \times \cdots \times A_n$ such that $\pi_i \circ g = h_i$ for all $i \in \{1,\dotsc, n\}$ where $\pi_i:A_1 \times \cdots A_n \rightarrow A_i$ is the projection map. Proof Existence Set $I$ to be an indexing set such that $I = \{1,\dotsc,n\}$ for some $n \in \mathbb{N}$. Let $h_i:B \rightarrow A_i$ for each $i \in I$. Let $b \in B$. Iterating through the values of $i$, we have a sequence of images $h_1(b), \dotsc, h_n(b)$. If the set $A_1 \times \dotsc \times A_n$ is a Cartesian product for each $A_i$, then the $n$-list $(h_1(b), \dotsc, h_n(b)) \in A_1 \times \dotsc \times A_n$. Thus, we have an ordered pair $(b,(h_1(b), \dotsc, h_n(b)))$. Define the relation $g:B \rightarrow A_1 \times \dotsc \times A_n$. Then $(b,(h_1(b), \dotsc, h_n(b))) \in g$. To prove that $g$ is a function, we first note that for every $b \in B$, the function $h_i$ gaurantees the (unique) existence of an image $(h_1(b),\dotsc,h_n(b)) \in A_1 \times \dotsc \times A_n$. To prove uniqueness, let the images $x = g(b)$ and $y = g(b)$ for each $b \in B$ such that $x \neq y$. Thus, the n-lists $x = (h_1(b),\dots, h_n(b))$ and $y = (h_1(b),\dotsc, h_n(b))$ have at least one differing component $j$ where we denote $h_j(b)$ to be the $j^{th}$-component of $x$, and $h^*_j(b)$ to be the $j^{th}$ componenet of $y$. Therefore, $h_j(b) \neq h^*_j(b)$. But this implies that $h_j$ is not a function for an arbitrary $j \in I$, which is a contradiction. Hence, $g$ is a function. Now construct the composition $\pi_i \circ g: B \rightarrow A_i$.
We will prove that $\pi_i \circ g = h_i$. We first note that the domain and codomain of $\pi_i \circ g$ and $h_i$ are equal by construction. We now prove that for all $b \in B$, we have $h_i(b) = \pi_i(g(b))$ for all $i \in I$ Choose any $m \in I$. First, let $b \in B$. Then $h_m \in A_m$. By definition of $\pi_i \circ g$, the fact that $h_m \in A_m$ for an arbitrary $m \in I$ implies that $(b,h_m(b)) \in \pi_i \circ g$, so $h_i \subseteq \pi_i \circ g$. Now, since we are assuming that $b \in B$, we have $g(b) \in A_1 \times \dotsc \times A_n$. Then applying $\pi_m$ to $g$, we deduce that $\pi(g(b)) \in A_m$. By definition of $h$, we must have $(b, \pi_m(g(b))) \in h_m$. Therefore, $\pi_i \circ g \subseteq h_i$. We conclude that $\pi_i \circ g = h_i$ for each $i \in I$. Uniqueness Suppose to the contrary that there are two functions $g:B\rightarrow A_1 \times \dotsc \times A_n$ and $g':B\rightarrow A_1 \times \dotsc \times A_n$ such that $h_i = \pi_i \circ g$, $h_i = \pi_i \circ g'$ for all $i \in I$, and $g(b) \neq g'(b)$ for some $b\in B$ [This implies that some of the components of $g(b)$ and $g'(b)$ are not equal; recall that $g(b) \in A_1 \times \dotsc \times A_n$, which means that the element $g(b) = (a_1,\dotsc,a_n)$ for some $a_i \in A_i$] Let $b \in B$. For each $m \in I$, we have $h_m(b) \in A_m$. From our premise, we know that $h_m = \pi_m(g(b))$ and $h_m = \pi_m(g'(b))$. We note that we cannot have $\pi_m(g(b)) \neq \pi_m(g(b))$, because then $h_m$ would not be a function. Hence, $\pi_m(g(b)) = \pi_m(g'(b))$. However, observe that $\pi_m(g(b))$ is an arbitrary component of the preimage $g(b)$. Thus the equality $\pi_m(g(b)) = \pi_m(g'(b))$ implies that the components of $g(b)$ and $g'(b)$ are equal, entailing that $g(b) = g'(b)$. Hence the function $g = g'$. Therefore, there exists a unique function $g: B \rightarrow A_1 \times \dotsc \times A_n$ such that $\pi_i \circ g = h_i$ for all $i \in I$. $\blacksquare$","['elementary-set-theory', 'proof-verification', 'relations']"
1516839,Integer coefficients polynomial. Find largest number of roots.,"The polynomial $p(x)$ has integer coefficients, and $p(100)=100$. Let $r_1, r_2, …, r_k$ be distinct integers that satisfy the equation $p(x)=x^3$. What is the largest possible value of $k$?","['contest-math', 'polynomials', 'algebra-precalculus']"
1516842,How many bit strings of length 10 either begin with three 0s or end with two 0s?,"The question : How many bit strings of length 10 either begin with three $0$s or end with two $0$'s? My solution : $0$ $0$ $0$ X X X X X $0$ $0$ = $2^5 = 256$ editing** I noticed the word""or"" so I changed the solution to $2^7$ (three $0$'s) +$2^8$(two $0$'s) - $2^5$(both) =416 is this the correct way to do it?","['discrete-mathematics', 'combinatorics']"
1516849,Upper bound for order of finite group given relations,"Say I have a group with the following presentation:
$$
G = \langle a,b \mid a^2 = b^3 = (ab)^3 = e \rangle
$$ During a conversation someone had mentioned that the order for $G$ must be less than or equal to $12$. I couldn't follow the conversation very well, but on trying to figure out where this bound came from I got confused. They seemed to make it sound like there was some certain property that allowed them to calculate this fairly rapidly. Is there some theorem that gives an upper bound to finite groups that are relatively nicely behaved? (Like those with two or maybe three generators).","['group-presentation', 'group-theory', 'finite-groups']"
1516865,"Number of ways to arrange $a,b,c,d$ such that $a$ is not followed immediately by $b$","Can someone explain this solution? The question is: How many ways are there to arrange the letters $a,b,c,d$ such that
$a$ is not followed immediately by $b$? The solution is: $4! − 3! = 18$ I know that $4!$ comes from $4$ letters but where does $3!$ come from?","['discrete-mathematics', 'combinatorics']"
1516871,Theorem of Diffeomorphism.,"I'm starting out in geometry, I dont particularly understand the notation of $df_x$. what exactly is this linear map? a matrix? Can someone please draw an analogy between this and basic ""highschool""  differentiation? ie polynomials?","['derivatives', 'geometry', 'differential-geometry']"
1516875,Show that a complex equation represents a circle,"I'm having troubling understanding the answer to a question. The question is: If $\ v=1+i$ and $\ z=x+iy$, for any real numbers x and y: Show that the equation   $\left|z-v\right|= \left|vz\right|$ represents a circle, and find its centre and radius. The answers states: $\ vz = (1+i)(x+iy) = (x-y) +i(x+y)$ so $\left|z-v\right|= \left|vz\right|$ becomes $\ (x-1)^2 + (y-1)^2 = (x-y)^2 + (x+y)^2$ $\ -2x - 2y +2 = x^2 + y^2 $ ... The part I don't understand is how they remove $\ i $.","['conic-sections', 'circles', 'complex-numbers', 'algebra-precalculus']"
1516892,What is an intuitive explanation to why elimination retains solution to the original system of equations?,"I've studied linear algebra before, however, I wanted to come back to the foundations and understand it again from the beginning. I was looking the following inoffensive linear equations: $$ x - 2y = 1 $$
$$ 3x + 2y = 11 $$ and after elimination one has: $$ x - 2y = 1 $$
$$ 8y = 8 $$ the claim is that both systems of equations have the same solution. Geometrically: if you've never seen this before it nearly seems like magic! We managed to combine different equations and still retain that the solutions is preserved. The new equations seems to be completely different and the lines that we care about don't even have same gradients. Basically, at first glance, the problem seems like it dramatically changed! So even though, the system seems to have changed a lot, in reality, it didn't change that much since they still intersect at the same point. My question is, what is the intuition to why manipulating system of equations in this way and combining them retains the original solution. The intuition/justification that I used to think of was that, if we combine equations, in principal, the ""total"" information that we had at the beginning of the system is preserved as long we are combining different equations and we don't discard one any of them. Basically, combining two equations implicitly retains the information that we had about the old equation. However, we can ""forget"" about the old form of the new equation because that information is preserved even though the equation changed. i.e. its ok to combine equation 1 and 2 to form 2' and discard 2, since 2' AND 1 contains all the information of the original system. This is sort of the intuition that I use but I wasn't sure if that was a good way to think about it or if people maybe had better intuitions or justification to why elimination worked.","['gaussian-elimination', 'linear-algebra', 'intuition']"
1516899,Confused by peculiar norm,"Let $X$ be an infinite subset of $ [0,1]$. In an exercise I am considering the norm on $P([0,1])$ (polynomials on unit interval) defined by: $$||p||_X=\sup_X |p|$$
My question is, how do I make sense of convergent sequences under this norm? For instance, say $X=[0,1/2]$ then a sequence $p_n$ converging to $f$ under $||\cdot||_X$ will only tell me about $f$ on $X$ and not the whole of $[0,1]$; so how can the limit $f$ be fully determined? Maybe I am missing a crucial point here...","['limits', 'real-analysis', 'polynomials', 'normed-spaces', 'convergence-divergence']"
1516961,loop invariant for simple algorithms,"The following is an algorithm which finds the maximum value in a list of integers, and I want to prove that it is correct by using a loop invariant. algorithm max(list A[0..n − 1])
x ← A[0]
i ← 1
while i < n do
if A[i] > x then x = A[i]
i ← i + 1
return x I really struggle when it comes to finding appropriate loop invariants, so any tips regarding this would also be appreciated. Anyway, for the above algorithm I have (tentatively) come up with a loop invariant as follows: $x \ge A[i-1]$ I am not sure how to actually use this to prove the correctness of the algorithm, or even if the invariant I have come up with is correct.","['algorithms', 'invariance', 'discrete-mathematics']"
1517003,How many functions satisfy the property $f(i)<f(j)$ for some $1 \leq i \leq j \leq n$?,"Let $F$ be the set of one-to-one functions from the set $\{1, 2, ….., n\}$ to the set $\{1, 2, ….., m\}$ where $m\geq n\geq 1$. How many functions are members of $F?$ How many functions $f$ in $F$ satisfy the property $f(i)=1$ for some $i,
        1 \leq i \leq n?$ How many functions $f$ in $F$ satisfy the property $f(i)<f(j)$ for some
        $1 \leq i \leq j \leq n$? Somewhere it explained as : A function from $A$ to $B$ must map every element in $A$. Being one-one, each element must map to a unique element in $B$. So, for $n$ elements in $A$, we have $m$ choices in $B$ and so we can have $^m\mathbb{P}_n$ functions. Continuing from $(1)$ part. Here, we are forced to fix $f(i) = 1$. So, one element from $A$ and $B$ gone with $n$ possibilities for the element in $A$ and $1$ possibility for that in $B$, and we get $n \times ^{m-1}\mathbb{P}_{n-1}$ such functions. $f(i) < f(j)$ is satisfied by all such functions as we are considering only one-one functions. So, this answer will be same as in $(1)$. I've understood well part $(1)$ and part $(2)$ . I'm not getting explanation of part $(3)$. How is it satisfied by one to one function , and how many such functions are there ?","['elementary-set-theory', 'functions']"
1517014,Is tensor of chain complex commutative?,"Let $B_*$ and $C_*$ be chain complexes (say of $R$-modules). Then is $B_*\otimes_R C_*$ isomorphic as a chain complex to $C_*\otimes B_*$? There are lots of signs involved, and I am not sure if it can be possible to arrange them such that the two chain complexes are isomorphic.","['homological-algebra', 'abstract-algebra', 'commutative-algebra']"
1517021,How to distinguish between combination and permutation questions?,"How do you distinguish combination and permutation question? An example of a combination question: Example:   How many different committees of 4 students can be chosen
from a group of 15? Answer:   There are  possible combinations of 4 students from a set of
15. There are 1365 different committees. An example of a permutation question: Example:   How many ways can 4 students from a group of 15 be lined up for a photograph? Answer:     There are 15P4 possible permutations of 4 students from a group of 15. These are different  lineups. How to know in which case to use $nCm$ and in which $nPm$ ?","['combinations', 'discrete-mathematics', 'combinatorics', 'permutations']"
1517029,Residues of poles,"Find $Res_{f}\left ( z_{0} \right )$, where, $f\left ( z \right )=\frac{1}{z^{4}+4}$, for $z_{0}=1+i$ The definition for 
$$Res_{f}\left ( 1+i \right ) =\lim_{z \to z_{0}} \left\{\left ( z-\left ( 1+i \right ) \right ) \cdot \frac{1}{z^{4}+4} \right\}$$
and the roots for
$$z^{4}+4$$ are $\sqrt{+2i}$, $\sqrt{-2i}$, $- \sqrt{+2i}$, $- \sqrt{-2i}$ I'm a bit stuck here. Could someone give me a push?","['complex-analysis', 'residue-calculus']"
1517033,Characterizing sets with a function,"Given a set $A=\{z_1,\ldots,z_n\},\epsilon>0$ where $z_i \in \mathbb{N}$, I wonder if there is an easy way to choose numbers $0<\lambda_1,\ldots,\lambda_n<\epsilon$ explicitly such that for all subsets $T\subseteq\{1,\ldots,n\}$ we have that the numbers $$f(T)=\sum_{t \in T}\sqrt{z_{t}+\lambda_{t}}$$ are pairwise different? That is $f(T)\neq f(T')$ if $T \neq T'$. The square root seems to make things difficult, otherwise one could choose something like $\lambda_i=2^{-i -k}$ for some constant $k>0$.","['analysis', 'number-theory']"
1517036,Decomposing geodesic tessellations over a sphere into parallelograms,"I'm working with some icosahedron-based tessellations of triangles over the surface of a sphere. Class I and Class II tessellations have a nice property where, cutting along the edges of the tessellated grid, I can split the surface into a series of parallelograms. This gives a neat way to associate points on the tessellated sphere with points in a simple rectangular array (which I hope to use for things like game logic and texturing) Here's what it looks like for the Class I case:
(starting from a corresponding Goldberg polyhedron) For Class II, we get one rhombus per edge of the icosahedron, three meeting in the middle of each icosahedral face - the same pattern as a rhombic triacontahedron But many geodesics I might want to decompose are Class III: So far I've only found similar decompositions for those of the form (n, n + 1) and (n, n + 2), sketched in blue on the right side above, but although these cover every vertex of the tessellation they do leave some faces uncovered (which is probably tolerable for my use case but not my first choice). So the question I'm chipping away at now is: Is there a method to decompose a Class III geodesic tessellation into a set of non-overlapping parallelograms , either in general or for special cases? Or conversely, is there no such method and I'm on a wild goose chase?","['3d', 'geometry', 'polyhedra', 'spheres', 'tessellations']"
1517057,Boundary conditions for Green's function?,"I have been told that when using green's function we need boundary conditions of that are homogenous and of the form: 
$$\alpha y(a)+\beta y'(a)+\gamma y(b)+\epsilon y'(b)=0$$
But I cannot see the reason for the need of boundary conditions like this, why won't any boundary conditions work?","['greens-function', 'functional-analysis']"
1517072,A Probabilistic Approach to Stirling's Formula,"I am working on the following problem: Suppose $X_1, X_2,\dots$ are i.i.d Poisson $(1)$ random variables, and let $S_n=X_1+\dots+X_n$ . a)Compute $E\left[\left( \frac{S_n-n}{\sqrt{n}} \right)^-\right]$ exactly, where $x^- = (-x)\lor 0.$ b) Explain why $\left( \frac{S_n-n}{\sqrt{n}} \right)^- \implies N^-,$ where $\implies$ denotes convergence in distribution, and $N\sim N(0,1).$ c) Show that $E\left[\left( \frac{S_n-n}{\sqrt{n}} \right)^-\right]\rightarrow E(N^-)=\frac{1}{\sqrt{2\pi}}$ . d) Conclude that $n! \sim n^n e^{-n}\sqrt{2\pi n}$ So far for part (a) I was able to compute that $E\left[\left( \frac{S_n-n}{\sqrt{n}} \right)^-\right]=\frac{e^{-n}n^{n+1/2}}{n!}$ . For part (b) since $x^-$ is a continuous function and $\frac{S_n-n}{\sqrt{n}}\implies N\sim N(0,1)$ we get that $\left( \frac{S_n-n}{\sqrt{n}} \right)^-\implies N^-$ . Once I prove (c), (d) follows through manipulation of the two expectations. I am stuck on proving (c) and could use some help. By (b) I know that $\left( \frac{S_n-n}{\sqrt{n}} \right)^-\implies N^-$ , if I can show that $\{\left( \frac{S_n-n}{\sqrt{n}} \right)^-\}_n$ is uniformly integrable then I get $E\left[\left( \frac{S_n-n}{\sqrt{n}} \right)^-\right]\rightarrow E(N^-)$ . I could use help showing that $\{\left( \frac{S_n-n}{\sqrt{n}} \right)^-\}_n$ is uniformly integrable. Thanks! UPDATE: So my professor told me the following to help with this problem. If $\{\left|\frac{S_n-n}{n}\right|\}_n$ is uniformly integrable then $\{\left( \frac{S_n-n}{\sqrt{n}} \right)^-\}_n$ is uniformly integrable. Which is true because $\left( \frac{S_n-n}{\sqrt{n}} \right)^-\leq \left|\frac{S_n-n}{n}\right|$ . Then I was told that I should recall that $S_n$ can be written as the sum of $n$ i.i.d random variables. So now I am stuck trying to show that $\{\left|\frac{S_n-n}{n}\right|\}$ is uniformly integrable.","['probability-theory', 'probability', 'uniform-integrability']"
1517087,Ammunition Depot: Monte Carlo Method,"I was given the following question from a friend of mine and I can't seem to understand it to well: A squadron of 10 bombers attempts to destroy an ammunition depot. The fighter jet flies in the horizontal direction. The aiming point is the center of the depot. The point of impact is assumed to be a normally distributed around the aiming point $(60,0)$ with a standard deviation $\sigma$ of $200$ yards in the horizontal direction and $100$ yards in the vertical direction. Simulate the operation and estimate the number of bombs on target. The middle two corners of the following ammunition depot are $(-150,0)$ and $(270,0)$. $X$~Normal (mean $= 60, \sigma = 200$) and $Y$~Normal (mean $=0,\sigma = 100$). The picture above is a diagram of the ammunition depot. I don't have a strong background in statistics to fully understand how the normal distribution is being applied here. I am somewhat familiar with Monte Carlo Method, but I have only used it as an alternative to solving integrals numerically. My idea was to just define the region above using the following equations: 
\begin{array}
` y_1 = 120 &  & -210\leq x \leq 210\\
y_2 = -120 &  & -210\leq x \leq 210\\
y_3 = 2x-540 &  & 210\leq x \leq 270\\
y_4 = -2x+540 &  & 210\leq x \leq 270\\
y_5 = 2x+300 &  & -210\leq x \leq -150\\
y_6 = -2x-300 &  & -210\leq x \leq -150\\
\end{array}
Then randomly generate points that represent where the projectiles land. If the projectiles (coordinates in this case) landed within the boundary I specified, then I would mark it as a hit; otherwise, a miss. However, I do not know if this is sufficient since it does not incorporate the distribution (at least not that I know of). Thank you for your time and have a wonderful day.","['normal-distribution', 'statistics', 'monte-carlo']"
1517101,When is the region $|P(z)|<r$ connected?,"$z$ is in the complex plane, r is real, and P(z) is an nth order polynomial in z Is there a way to determine the minimum value of $r$ for which the region $|P(z)|<r$ is a connected region?  By connected I mean that any point in the region can be reached from any other point in the region by a path entirely in the region. An equivalent question would be:
Is there a way to determine at what value of $r$ the surface $P(z)=r$ changes from being a single continuous surface to multiple disconnected surfaces? In the limit of large $r$, the region becomes one large circle.  In the limit of small $r$, it becomes a set of disconnected circles, one around each root of $P(z)$. If the threshold $r$ cannot be determined analytically for a general $P(z)$, are there non-trivial special cases of $P(z)$ for which it can? Example: $|(z+2)(z+i)(z-1)|<2.7$ is connected. $|(z+2)(z+i)(z-1)|<1.3$ is not connected. $|(z+2)(z+i)(z-1)|<1.1$ is further disconnected.",['complex-analysis']
1517103,How to calculate expected value of matrix norms of $A^TA$?,"Let $A$ be a random $m$ by $n$ rectangular sign matrix, chosen uniformly at random, with $m < n$. To be clear,  $A$ is a matrix whose entries are chosen from $\{1,-1\}$. Let $B = A^T A$.  We know, for example, that $B$ is a square and symmetric $n$ by $n$ matrix with all its  diagonal entries equal to $m$ exactly.   I am trying to learn how to calculate the expected Frobenius and spectral norm of $B$.  We can assume both $m$ and $n$ are large if that helps give a good bound or estimate. How can you calculate $\mathbb{E}(||B||_F)$ and
  $\mathbb{E}(||B||_2)$? The expected Frobenius norm of $B$ is defined to be $$
\mathbb{E}(||B||_F)=\mathbb{E}\left(\sqrt{\sum_{i=1}^n\sum_{j=1}^n |b_{ij}|^2}\right)
$$ where $b_{ij}$ are the elements of $B$. The expected spectral norm of $B$ is defined to be $$
\mathbb{E}(||B||_2)= \mathbb{E}\left(\max_{|x|_2 \ne 0}\frac{|Bx|_2}{|x|_2}\right).
$$ Cross-posted to https://mathoverflow.net/questions/222994/how-to-calculate-expected-value-of-matrix-norms-of-ata now.  I will update this question if anything substantive is added there.","['random-matrices', 'probability', 'linear-algebra', 'normed-spaces']"
1517108,Irreducible implies minimal polynomial?,"In the context of field theory, let's say we are finding the minimal polynomial of the $\sqrt[3] 2$ over $\mathbb{Q}$. Clearly a candidate will be $x^3-2$, which is irreducible by Eisenstein's criterion. Can we then immediately conclude that it is the minimal polynomial? What I am worried is, is there such thing as an irreducible polynomial that is somehow not the minimal polynomial? (I know minimal polynomial implies irreducibility, but not sure about the converse: does irreducible polynomial implies minimality?) Thanks for any help.","['abstract-algebra', 'field-theory', 'galois-theory']"
1517122,Does there exist a Banach space with no complemented closed subspaces?,"I know that every Hilbert space can be decomposed as the direct sum of two non-trivial closed subspaces, eg. taking the kernel and range of any non-trivial bounded projection. But I don't know what happens in Banach spaces. Does there exist a Banach space $B$ with no complemented closed subspaces? In other words, such that there do not exist closed subspaces $U,V\subset B$ with $B=U\oplus V$?","['banach-spaces', 'functional-analysis']"
1517134,Convergence of generalized inverses,"I copy-paste the post from Math Overflow - maybe someone can give me a tip. During the reading about Fisher–Tippett–Gnedenko theorem (it can be found easily on wiki - I don't have enough reputation to post more links), I've got stuck, trying to understand more deeply one of the lemmas which lead to the final result. Instead of using CDFs in explanation why the distribution of normalized maxima can converge to one of the three distributions (Gumbel, Weibull or Frechet), many authors (de Haan, Resnick) do the same thing with the help of quantile functions. For that reason, it seems useful to prove this kind of lemma (here scan from De Haan L., Ferreira A., Extreme Value Theory An Introduction, p. 5; simmilar proposition - without proof - can be found here as well: http://cims.nyu.edu/~nica/Extreme_Values_Resnick.pdf ) As usual, wording ""the proof of the left-hand inequality is similar"" makes me a bit suspicious. As I understand correctly,  remaining part should be started with choosing $ 0 < \epsilon_{1} < \epsilon $ such that $ g^{\leftarrow}(x) + \epsilon_{1} $ is a continuity point of $g$. Of course, it can be done because of monotonicity of $ g $. However, it should be also provided that $ g^{\leftarrow}(x) + \epsilon_{1} \in (a,b) $, knowing that $ x \in (g(a), g(b)) $, which is not so obvious. It's easy to show the problem in a more graphical way (image below quoting text in the link). $ u $ is of course a continuity point of $ g^{\leftarrow} $, and of course for all $ \epsilon_{1} $, $ g^{\leftarrow}(u) + \epsilon_{1} $ is a continuity point of $g$, but there is no chance that $ g^{\leftarrow}(x) + \epsilon_{1} \in (a,b) $. I am wondering, first of all, if this lemma in such a shape is valid (alternatively, how it can be refined) and, what is maybe more important/interesting, if convergence of inverses implies convergence: $$ f_{n} \rightarrow g $$. Because this lemma is useful in context of probability theory, I can assume that both $ f_{n} $ and $ g $ are CDFs (I don't know if this helps). Thanks for any feedback.","['probability-theory', 'generalized-inverse', 'weak-convergence']"
1517152,"Show,by vector method, that the angle between any edge and a face not containing the edge is $\arccos(\frac{1}{\sqrt3})$","Let $k$ be the length of any edge of a regular tetrahedron.Show that the angle between any edge and a face not containing the edge is $\arccos(\frac{1}{\sqrt3})$. Let the regular tetrahedron be $OABC$.Let $O$ be the origin and position vectors of $A,B,C$ are the $\vec{a},\vec{b},\vec{c}$. Let us find the angle between face $OAB$ and the edge $OC$.Angle between a plane and a line is found by finding the angle between the normal to the plane and the line. The plane $OAB$ is spanned by the vectors $\vec{a}$ and $\vec{b}$.So its normal is given by $\vec{a}\times\vec{b}$.And the edge $OC$ is $\vec{c}$. Let $\theta$ be the angle between the face $OAB$ and $OC$.So angle between the normal to the face $OAB$ and $OC$ is $\frac{\pi}{2}-\theta$ $\cos(\frac{\pi}{2}-\theta)=\frac{(\vec{a}\times\vec{b}).\vec{c}}{|\vec{a}\times\vec{b}||\vec{c}|}$ $\sin\theta=\frac{(\vec{a}\times\vec{b}).\vec{c}}{|\vec{a}||\vec{b}||\vec{c}|\sin\frac{\pi}{3}}$ I am stuck here and could not solve further.Please help me.Thanks.","['geometry', 'vectors']"
1517164,Definition of weakly continuous map from one Banach space to another,"What is the definition of a weakly continuous function from a Banach space to a Banach space? Suppose $X$ and $Y$ are Banach spaces. Define $f : X \rightarrow Y$ as a function. Am I right to say that $f$ is weakly continuous if the net $x_{\alpha} \rightarrow x$ in the weak topology of $X$, then $f(x_\alpha) \rightarrow f(x)$ in the weak topology of $Y$? I know that we cannot change the net into a sequence, as some Banach spaces are Schur spaces, which have the property that every weakly convergent is norm convergent. If my definition is not correct, then may I know what is the proper definition? Remark: Sorry for the confusion. I am talking about weak-to-weak continuous.","['definition', 'weak-convergence', 'functional-analysis']"
1517167,Finite State Space (Markov Chain): Show $\alpha_{m+n}\leq\alpha_{n}\alpha_{m}$,"It is a question in Durrett's book probability: theory and examples, Chapter 6, Exercise 6.6.3. Assume the state space is finite. For any transition matrix $p$, define 
$\alpha_n = \sup_{i,j} \frac{1}{2}\sum_{k}|p^n(i,k)-p^n(j,k)|.$ The $1/2$ is there because for any $i$ and $j$ we can defince random variables $X$ and $Y$ so that $P(X=k)=p^n(i,k), P(Y=k)=p^n(j,k)$, and $P(X \neq Y)=\frac{1}{2}\sum_{k}|p^n(i,k)-p^n(j,k)|. $ Show that $\alpha_{m+n}\leq\alpha_{n}\alpha_{m}$. Anyone has any idea? It seems that I can easily prove that $\alpha_{m+n}\leq 2\alpha_{n}\alpha_{m}$, but I just cannot improve more.","['probability-theory', 'markov-chains']"
