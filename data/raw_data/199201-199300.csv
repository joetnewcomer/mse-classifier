question_id,title,body,tags
3872613,Willard 17R; uncountably many compact subsets of real line,"I am self-studying topology and came across question 17R of Willard's General Topology . 17R. Compact subsets of $\mathbb{R}$ There are uncountably many nonhomeomorphic compact subsets of $\mathbb{R}$ . [Use ordinals.] The discussions I found which are similar (e.g. Uncountably many non-homeomorphic compact subsets of the circle ) use what seems to be more advanced stuff (""Cantor-Bendixson rank"", for example). I guess the hint suggest us to look at $\Omega=[0,\omega_1]$ , where $\omega_1$ is the first uncountable ordinal. I can do the following: Every countable ordinal embeds into $\mathbb{R}$ . This is more-or-less straightforward induction. So the problem boils down in proving that there are uncountably many non-homeomorphic countable ordinals. It is also clear that if $\alpha$ is an infinite ordinal and $\beta$ is the largest limit ordinal $\leq\alpha$ , then the compacts $[0,\alpha]$ and $[0,\beta]$ are homeomorphic. I can also prove that there are uncountably many countable limit ordinals, but some of these are homeomorphic to each other (e.g. $\omega^2+\omega$ and $\omega^2$ ). I would appreciate help, using not much more than basic facts about $\omega_1$ (as it is introduced in Willard's book).","['general-topology', 'ordinals', 'compactness']"
3872617,Does $\int1+\sin^2x+\sin^4x+\sin^6x+...\text{dx}=\tan{x}$?,"I want to find $$\int1+\sin^2x+\sin^4x+\sin^6x +...\text{dx}$$ My method was to interpret the integrand as a geometric series with first term $1$ and common ratio $\sin^2x$ . Assuming $\sin x\ne1$ , I reasoned the integrand should converge. So the integrand should be equal to $$\frac{1}{1-\sin^2x}=\frac{1}{\cos^2x}=\sec^2{x}$$ $$\implies \int1+\sin^2x+\sin^4x+\sin^6x +...\text{dx}=\int\sec^2{x}~\text{dx}=\tan x$$ I would just like to know, is this right?
If it is, it seems to me a rather beautiful result.","['integration', 'calculus', 'solution-verification', 'trigonometric-integrals', 'indefinite-integrals']"
3872647,If |a|> |a+b+c| prove that there is complex root such that |z|< 2,"Let $a x^2 + b x +c =0$ be a quadratic equation where $|a|> |a+b+c| $ , $a,b,c \in  \mathbb{R}$ . Prove that this equation has at least one solution $ z \in \mathbb{C}$ such that $|z| < 2$ . I don't know how to prove this, I noticed that $|a+b+c | = |f(1)|$ , so $|a|> |f(1)|$ , but I'm not sure if it helps. Also, $|z|= \frac{b^2}{4a^2} + \frac{ 4ac - b^2}{4 a^2}  <4\implies a c <4 a^2  $ . Can anyone help? I would be greatly thankful.","['complex-analysis', 'quadratics']"
3872657,Show $\{X_t=a\} \notin \mathcal{F}_{t-}$ for canonical process,"Let $\Omega$ be the set of applications in $\mathbb{R}_{+}$ which are right continuous and have left limit (except in 0). For all $t\in\mathbb{R}_{+}$ , we define the coordinated application $X_t$ as $X_t(\omega)=\omega(t)\quad (\omega \in \Omega)$ . Given $a \in \mathbb{R}$ , prove that the set $\{X_t=a\}$ is in $\mathcal{F}_t$ but not in $\mathcal{F}_{t-}$ . Here $\mathcal{F}_t$ is the natural filtration $\mathcal{F}_t=\{X_s: s\leq t\}$ and $\mathcal{F}_{t-}=\sigma\{\mathcal{F}_s :s<t\}$ . Intuitively is clear, but I don't know how to formalize it.","['stochastic-processes', 'measure-theory']"
3872867,Derivative of limited convolution,"If I take infinite convolution, I know that its derivative is the convolution between one term and other term's derivative. However, I am interested on derivative of $$\dfrac {d}{dt}\int_0^t f (t) g (t-a) da$$ i.e. in terms of limited convolution . Many thanks in advance! ====== EDIT: Please review comments on answer","['definite-integrals', 'derivatives', 'convolution']"
3873034,Proof that sinusoids are unique in retaining their shape when summed with waveforms of the same shape and frequency,"The Wikipedia article on Sine wave states that the sine wave ""retains its wave shape when added to another sine wave of the same frequency and arbitrary phase and magnitude.""  I am aware that this can be shown using the harmonic addition theorem . However, the article goes on to state that the sine wave ""is the only periodic waveform that has this property."" How can it be shown that the sinusoid is unique in this respect? (that is, that for sinusoidal waveforms only does summing together waveforms with the same frequency but arbitrarily different amplitudes and phase offsets result in a waveform of the same shape)? It is not difficult to test the property with single examples, such as with square waves, but it is not clear to me how to determine that the property only holds for sine waves. It seems to me that this property could be stated as a functional equation along the lines of: $$ f(t) + A_2 \, f(t + \delta_2) = A_3 \, f(t + \delta_3) $$ I found several answers by user21467 that reference the paper below, in which a set of functional equations is used to define sine and cosine; however, they are not quite the same functional equations as what is directly relevant to my question. Robison, G. (1968). A New Approach to Circular Functions, II and lim (sin x)/x. Mathematics Magazine, 41 (2), 66-70. [ doi:10.2307/2689051 ] [ jstor ]","['periodic-functions', 'trigonometry', 'signal-processing', 'fourier-analysis']"
3873049,"Proof: For all real numbers $x$, If $x^2−5x+4≥0$, then either $x≤1$ or $x≥4$. Is this a valid technique?","For all real numbers $x$ , If $x^2−5x+4≥0$ , then either $x≤1$ or $x≥4$ . Is this a valid technique? Instead of showing that either (x-4) and (x-1) are both positive or both negative, can I just do what I have written below instead? $x^2−5x+4=(x-4)(x-1)≥0$ There are two possible situations: $x≥4$ or $x<4$ . if $x<4$ then $(x-4)<0$ and so we get $(x-1)≤ 0$ which implies that $x≤1$ . so either $x≤1$ or $x≥4$ .","['inequality', 'proof-writing', 'fake-proofs', 'solution-verification', 'algebra-precalculus']"
3873136,Determining the Green's function and solution for $f''(x)=-g(x)$ with boundary conditions $f(0)=f(1)=0$.,"I am trying to solve the Poisson's equation in one dimension using Green's function: $$f''(x)=-g(x)$$ With the boundary conditions $f(0)=f(1)=0$ . I know that the Green's function is going to satisfy the same boundary conditions and de ODE: $$\frac{d^2G(x,t)}{dx^2}=\delta(x-t)$$ And I also know that $\frac{d^2G(x,t)}{dx^2}=0$ for $x\neq y$ . So I first solved the ODE on the region $0\leq x<t$ : $$G(x,t)=Ax+B$$ Apllying the boundary condition $G(0,t)=0$ , it implies $B=0$ and thus $G(x,t)=Ax$ For the region $t<x\leq 1$ : $$G(x,t)=Cx+D$$ Applying the boundary condition $G(1,t)=0$ , it implies $C=-D$ and thus $G(x,t)=C(x-1)$ . $$
  G(x,t) =
  \begin{cases}
                                   Ax & \text{if $x<t$} \\
                                   C(x-1) & \text{if $x>t$} \\
  \end{cases}
$$ To determine $A$ and $C$ , I integrated $\frac{d^2G(x,t)}{dx^2}=\delta(x-t)$ over a small iterval that included $x=t$ : $$\lim_{\epsilon\to0}\int_{t-\epsilon}^{t+\epsilon}\frac{d^2G(x,t)}{dx^2}=\frac{dG(x,t)}{dx}\Bigg\rvert_{x=t^+}-\frac{dG(x,t)}{dx}\Bigg\rvert_{x=t^-}=C-A=1$$ thus $C=1+A$ . Since both sides pieces of $G(x,t)$ must be equal at $x=t$ : $$At=C(t-1)=(1+A)(t-1)\Rightarrow A=t-1\ \text{and}\ C=t$$ Thus the Green's function is: $$
  G(x,t) =
  \begin{cases}
                                   x(t-1) & \text{if $x<t$} \\
                                   t(x-1) & \text{if $x>t$} \\
  \end{cases}
$$ After finding the Green's function, I have to find the solution for de ODE when $g(x)=\sin(\pi x)$ , thus: $$f(x)=\int_{0}^{1}G(x,t)g(t)dt=(x-1)\int_{0}^{x}t\sin(\pi t)dt+x\int_{x}^{1}(t-1)\sin(\pi t)dt$$ After computing the integral, I got: $$f(x)=-\frac{1}{\pi^2}\sin(\pi x)$$ But it isn't the correct answer, according to my book it should be $$f(x)=\frac{1}{\pi^2}\sin(\pi x)$$ I'm just missing the minus sign, what am I doing wrong? Thanks for helping me.","['boundary-value-problem', 'greens-function', 'ordinary-differential-equations']"
3873152,Derivative of $\left | x-\left \lfloor x+1 \right \rfloor \right |$ at $x = 1.5$?,"Q: If $f(x)=\left | x-\left \lfloor x+1 \right \rfloor \right |$ , where $\left \lfloor x \right \rfloor$ denotes the greatest integer less than or equal to x and $\left | x \right |$ denotes the absolute value of x, then $f'(1.5)$ = I am not quite sure how the derivative of floor function and absolute function. I did some research and found out that the derivative of an absolute function is $\frac{\left \lfloor x \right \rfloor}{x}$ . But I am stuck with the floor function. What concept should I be aware of to solve this question?","['calculus', 'derivatives', 'absolute-value', 'ceiling-and-floor-functions']"
3873283,A general but accessible version of the divergence theorem,"The purpose of this question is to gather one/multiple statements of the divergence theorem that can cover most of the cases that one might encounter, say, in a standard PDE course (see the bottom of the post for details). The statement should thoroughly clarify the terms it uses, and should include a reference too, if possible. The reasons for this question are two: I couldn't find neither online or on MSE a precise and general enough statement, but not so advanced it would take me hours to understand it (for instance, geometric measure theory versions with sets of finite perimeter and such) Even in the books I've read (Evans' ""Partial differential equations"" to mention one), only the classic case is presented ( $C^1$ boundary, $C^1$ functions), but then different versions are used. Summing up, let's remove as many regularity assumptions as possible, making the statement as general as possible, with the constraint of not ending up in something not accessible (see below). Thank you! By ""statement covering the cases one might encounter while in a standard PDE course"" I mean something powerful enough to hold on an emisphere for example(I guess we are talking about Lipschitz open bounded sets), and with weak derivatives (even distributional ones are welcome, if there exists a statement with it). Lastly, I'm comfortable with Sobolev spaces and Hausdorff measures, and I guess a measure theoretic approach is necessary. I will provide feedback to communicate when a reply is ""too advanced"".","['measure-theory', 'reference-request', 'real-analysis', 'partial-differential-equations', 'soft-question']"
3873289,Question about axiom schema of specification and notation,"I have a question related to axiom schema of specification and set notation. Now my specific question goes like this. Let $X = \{a,b\}$ be given ( $a \neq b$ ). Define $f: \mathcal{P}(X) \to X$ as $f(\emptyset) = a$ , $f(\{a\}) = b$ , $f(\{b\}) = a$ , $f(\{a,b\}) = a$ . Now consider the set (written informally) $$ S = \{f(A) \in X: f(A) \notin A\}. $$ Is $a \in S$ ? My reasoning is that using axiom schema of specification, $S = \{ x \in X : \exists A \big( x=f(A) \land A \in \mathcal{P}(X) \land f(A) \notin A \big) \}$ . Since $ f(\emptyset) = a \notin \emptyset $ , $a \in S$ . Although $f(\{a,b\}) = a \in \{a,b\}$ , I don't think this gives the contradictory result $a \notin S$ . To say $a \notin S$ (in other words, $a \in X \setminus S$ ), we have to check $$ \lnot \exists A(x=f(A) \land A \in \mathcal{P}(X) \land f(A) \notin A) \iff \forall A (x \neq f(A) \lor A \notin \mathcal{P}(X) \lor f(A) \in A) $$ Since $A = \emptyset$ gives $a = f(\emptyset)$ , $\emptyset \in \mathcal{P}(X)$ , $f(\emptyset) = a \notin \emptyset$ , I think one cannot conclude $a \notin S$ . I'm very confused. Is $S$ just an ill-defined set? Any comments are appreciated","['elementary-set-theory', 'axioms']"
3873299,How to find a real function with that has certain imaginary roots,"I recently started learning about imaginary numbers, and a question asked to find a quadratic with roots of $2i$ , $4i$ . Solved simply by expanding $(x-2i)(x-4i)$ into $x^2 - 6ix - 8 = 0$ . However, how would I find a real function with the same roots, that is a function without $i$ in it? I attempted by saying: $-b/2a = 3i$ (is this a correct assumption?) $\dfrac{\sqrt{b^2 - 4ac}}{2a} = i $ Then I made $-b = 6ai$ , and thus $b^2 = -36a^2$ .
Then I solved for $c$ . But I keep reaching a dead end, saying $b = 6ai$ and $c =8a$ . Whilst this agrees with the initial function $x^2 - 6ix - 8$ , it’s not what I’m looking for. Any help would be appreciated, thanks!","['functions', 'quadratics', 'complex-numbers']"
3873427,Definition of pointwise continuous orientation of smooth manifolds,"Let $M$ be a smooth $n$ -manifold. A pointwise orientation is to specify an orientation of each tangent space $T_pM$ . A local frame $(E_i)$ on $U\subset M$ for $TM$ is said to be oriented, if $(E_1|_p,\cdots,E_n|_p)$ is a positively oriented basis for $T_pM$ at any $p\in U$ . By John Lee's Introduction to Smooth Manifolds, a continuous pointwise orientation means that every point $p\in M$ is in the domain of a certain oriented local frame. My question: Is the oriented local frame above in the definition of continuous pointwise orientation merely continuous ? Here is why I doubt this: In John Lee's proof of Proposition 15.5 (The Orientation Determined by an $n$ -Form), one specifies a local frame $(E_i)$ on a connected neighbourhood $U$ of $p$ , and $(\mathcal{E}^i)$ be its dual coframe, $f$ a nonvanishing continuous function on $M$ . Then he claimed that $\omega:=f\mathcal{E}^1\wedge\cdots\wedge\mathcal{E}^n$ is a nonvanishing $n$ -form. But everything here is merely continuous, then why would $\omega$ be a smooth differential $n$ -form?","['smooth-manifolds', 'differential-geometry']"
3873430,"No simple group of order 720, again","In his Notes on Group Theory, 2019 edition ( http://pdvpmtasgaon.edu.in/uploads/dptmaths/AnotesofGroupTheoryByMarkReeder.pdf p. 83 and ff.)
Mark Reeder gives a proof of the non-existence of simple groups of order 720. Here : ( No simple group of order 720 ) I asked for a clarification about a statement at the beginning of the proof and I got it. (Thanks again to David A. Craven.) I have another problem, perhaps more serious, at the end of the proof. M. Reeder proves that if $G$ is a simple group of order 720, if $s$ is an involution (element of order 2) of $G$ , then $C_{G}(s)$ is a Sylow 2-subgroup of $G$ and that if $s$ and $t$ are two differents involutions of $G$ , then $C_{G}(s)$ and $C_{G}(t)$ are different. So far, so good. A little further, M. Reeder says : ""Since $s$ is contained in just one Sylow 2-subgroup, namely $C_{G}(s)$ (...)"". I don't understand how that can be deduced from what precedes. I must say that I am skeptical about the possibility of proving this in the frame of M. Reeder's proof, for the following reason. After proving that (for a simple group $G$ of order 720), $G$ has exactly ten Sylow 3-subgroups, that these Sylow 3-subgroups are non-cyclic and intersect pairwise trivially, M. Reeder seems to derive a contradiction from only the following properties : s1 : $G$ is a group (I don't say ""simple group"") of order 720 ; s2 : $G$ has exactly ten Sylow 3-subgroups ; s3 :  these Sylow 3-subgroups are non-cyclic ; s4 :  these Sylow 3-subgroups  intersect pairwise trivially ; s5 : $G$ has no element of order 6 ; s6 : every involution of $G$ normalizes at least two  Sylow 3-subgroups of $G$ ; s7 : $G$ has several Sylow 2-subgroups. (M. Reeder uses another property to prove that some given subgroups of $G$ are isomorphic to $Q_{8}$ , but he notes that this fact plays no real role.) Now, properties s1 to s7 are not contradictory, because the group $M_{10}$ possesses them. (By the way, properties s1 to s7 are not independent : s3 can be deduced from s1, s2 and s5, using the N/C lemma.) My question is : do you see how the statement "" $s$ is contained in just one Sylow 2-subgroup, namely $C_{G}(s)$ "" can be proved in the frame of Mark Reeder's proof ? Thanks in advance. Note : I wrote up M. Reeder's proof by breaking it down into simple elements and indicating, for each element, the minimum hypotheses. If anyone is interested, I can post this work here. I can also post a proof that $M_{10}$ possesses properties  s1 to s7.","['simple-groups', 'group-theory', 'sylow-theory']"
3873583,a question about decomposition of the cotangent space on a complex manifold M (from Griffiths and Harris),"Recently, I am reading Griffiths and Harris' book 'principles of algebraic geometry '. In chapter 0, section DeRham and Dolbeault Cohomology . they mention that By linear algebra, the decomposition $$T^*_{\mathbb{C},z}(M)=T^{*'}_z(M)\bigoplus T^{*''}_z(M)$$ of the cotangent space of a complex manifold M at each point $p\in M$ gives a decomposition $$\wedge^nT^*_{\mathbb{C},z}(M)=\bigoplus_{p+q=n} (\wedge^p T^{*'}_z(M)\bigotimes \wedge^q T^{*''}_z(M))$$ My question is what is $\bigotimes$ means here? Does it means tensor product? If so, why? In my mind, it should be $\wedge$ here i.e we have $$\wedge^nT^*_{\mathbb{C},z}(M)=\bigoplus_{p+q=n} (\wedge^p T^{*'}_z(M)\wedge\wedge^q T^{*''}_z(M))$$ For example, when n=2 (we assume $dim_{\mathbb{C}}M \ge 2$ ). The decomposition here should be $$\wedge^2T^*_{\mathbb{C},z}(M)=\wedge^2 T^{*'}_z(M)\bigoplus \wedge^{1,1} T^{*'}_z(M)\bigoplus \wedge^2 T^{*''}_z(M))$$ where $$\wedge^{1,1} T^{*'}_z(M)=T^{*'}_z(M)\wedge T^{*''}_z(M)=\{f(z)dz_i\wedge dz_{\bar{j}}\} $$ not $$\wedge^{1,1} T^{*'}_z(M)=T^{*'}_z(M)\bigotimes T^{*''}_z(M)=\{f(z)dz_i\bigotimes dz_{\bar{j}}\} $$ I see many other book use the same symbol as Griffiths and Harris' book (for example 'Complex geometry; an introduction' by Daniel Huybrechts). Can anyone tell what's happening here. Thank you so much.","['complex-geometry', 'differential-geometry']"
3873598,Infinitesimal rotation matrix close to the identity,"Suppose $R_{ij}$ is a matrix that corresponds to an ""infinitesimal"" rotation. Then my notes mention that such matrix can be rewritten as: $$R_{ij} = \delta _{ij} + \epsilon w_{ij}$$ where $w_{ij} = - w_{ji}$ is antisymmetric and $\epsilon$ is a small quantity. Now I know that a rotation matrix must be orthogonal,but I am unsure on why is $w_{ij}$ must be antisymmetric. Also why would such a matrix represent an infinitesimal rotation?","['linear-algebra', 'linear-transformations', 'rotations']"
3873614,Proving that cardinality of this set is at most $\frac{n(n-1)}{2}$,"This question is from Apostol introduction to analytic number theory 144 page. Question 12 Let $G$ be a group and let $S$ be a subset of $n$ distinct elements of $G$ with the property that $a \in S$ implies $a^{-1} \notin S$ . Consider the $n^2$ products of form $ab$ , where $a$ and $b$ both belong to $S$ . Then prove that at most $n(n-1) /2$ of these products belong to $S$ . Attempt : I assumed that let a, b, c belongs to S and a=bc then a $b^{-1}$ = c, but $b^{-1}$ doesn't belongs to S.There will also be a case when a doesn't belongs to S. Then how to use it to prove the result in question? Unfortunately, I am not able to move ahead. It is my humble request to help me","['group-theory', 'abstract-algebra']"
3873617,Adding infinite cardinalities,"Let $a,b,c,d,e,f$ all be infinite sets. They are all disjoint. Assume $|a \cup b| > |c \cup d|$ and $|c \cup e| > |b \cup f|$ . From this, can I conclude that $|a \cup b \cup c\cup e|>|b\cup c\cup d \cup f|$ and $|a \cup e|>|d\cup f|$ ? My gut feeling says yes. Since $|a \cup b| > |c \cup d|$ , there is an injective function and no bijective one from $c\cup d$ to $a\cup b$ . Similarly, there is an injective function and no bijective one from $b \cup f$ to $c \cup e$ . So, there must be an injective function and no bijective one from $b\cup c\cup d \cup f$ to $a \cup b \cup c\cup e$ . Hence, the first inequality. For the second inequality, I'm thinking that there must be a bijection between $b\cup c$ and itself. But since $|a \cup b \cup c\cup e|>|b\cup c\cup d \cup f|$ , there must be an injective function and not bijective one from $d \cup f$ to $a \cup e$ . Hence, the second inequality. Is this right?","['elementary-set-theory', 'cardinals', 'functions']"
3873667,Differential Equation involving Mixture Problem,"I have encountered many mixing problem like this but I just can't answer this one.
Here is the problem, A mass of inert material containing $15 \text{ lb}$ of salt in its pores is agitated with $10 \text{ gal}$ of water initially fresh. The salt dissolves at a rate which varies jointly as the number of pounds of undissolved salt and the difference between the concentration of the solution and that of a saturated solution ( $3 \text{ lb/gal}$ ). If $9 \text{ lb}$ are dissolved in $10 \text{ min}$ , when will $90\%$ be dissolved? Here's my solution: Let $P$ be the amount of dissolved salt, $15-P$ be the undissolved. The concentration of the solution $= \frac P {10} \text{ lb/gal}$ . Then, $$\frac{\mathrm dP}{\mathrm dt} = (15-P)\left(3-\frac P {10}\right)\text;$$ $$\frac{\mathrm dP}{\mathrm dt} = (15-P)(30-P)/(10)\text.$$ Since this is separable, I used partial fraction then integrate both side and got an answer of around $11 \text{ min}$ , which is wrong according to the textbook. The answer key provided an answer of $30.5 \text{ min}$ . I don't really know whats wrong in my equation above.","['word-problem', 'ordinary-differential-equations']"
3873762,A problem on the equation $\bar{\partial} g=f$ in complex analysis,"I'm reading Voisin's famous book Hodge theory and Complex algebraic geometry, page 30. And in this section Voisin proved the following fact: given a smooth function $f$ , we can solve the equation $\bar{\partial} g=f$ locally. To be more precise, we can suppose that $f$ is of compact support and write down the explicit formula for $u$ as: $$
u(z)=\frac{1}{2 i \pi} \int_{\mathbb{C}} \frac{f(\zeta)}{\zeta-z} d \zeta \wedge d \bar{\zeta}.
$$ Of course this looks very reasonable. But I also read Hormander's famous book Introduction to complex analysis in several variables. In page 30, theorem 2.3.1 (1990 edition), he wrote a remark, which said $\bar{\partial} g=f$ needn't have a solution even when $f$ is of compact support! He said [take an arbitrary $f$ with nonzero Lebesgue integral on $\mathbb{C}$ ]. I am very much confused for the conclusions on two masters' books look like quite different. Can anyone explain why Hormander said we can [take an arbitrary $f$ with nonzero Lebesgue integral on $\mathbb{C}$ ] as a counterexample? Or have I misunderstood anything? Thanks in advance!","['complex-analysis', 'several-complex-variables']"
3873779,Recurrence relation $a_n = 4a_{n-1} - 3a_{n-2} + 2^n + n + 3$ with $a_{0} = 1$ and $a_{1} = 4$,"This is a nonhomogeneous recurrence relation, so there is a homogeneous and a particular solution. Homogenous: $a_n - 4a_{n-1} + 3a_{n-2} = 0$ $r^2 - 4r + 3 = 0$ $(r - 3)(r - 1)$ $a_n^h = \alpha(3^n) + \beta(1^n)$ This is where my solution stops because I don't know how to solve the particular solution since it would be $a_n - 4a_{n-1} + 3a_{n-2} = 2^n + n + 3$ and I'm not sure what form it should be. Would it be $A_0(r^n) + A_1(n) + A_2$ where $A_n$ is a constant or not? I've tried solving it with that form and it ended like this: $A_0(2^n) + A_1(n) + A_2 - 4(A_0(2^{n-1}) + A_1(n-1) + A_2) + 3(A_0(2^{n-2}) + A_1(n-2) + A_2) = 2^n + n + 3$ After simplifying and dividing $2^{n-2}$ : $A_0(2^n) - 4A_0(2^{n-1}) + 3A_0(2^{n-2}) - 4 = n + 3 + 2A_1(n) + 2A_2 - 2A_1$ And that's where I stop since I don't know what to do next. Thanks for answering.","['recurrence-relations', 'discrete-mathematics']"
3873834,Limit of Sum related question,"For $a\in R$ & $a \ne 1$ $$\mathop {\lim }\limits_{n \to \infty } \frac{{{1^a} + {2^a} + ... + {n^a}}}{{{{\left( {n + 1} \right)}^{a - 1}}\left[ {\left( {na + 1} \right) + \left( {na + 2} \right) + .. + \left( {na + n} \right)} \right]}} = \frac{1}{{60}}$$ , then find the value of $a$ . My approach is as follows: $$\mathop {\lim }\limits_{n \to \infty } \frac{{{1^a} + {2^a} + ... + {n^a}}}{{{{\left( {n + 1} \right)}^{a - 1}}\left[ {\left( {{n^2}a + \frac{{n\left( {n + 1} \right)}}{2}} \right)} \right]}} = \frac{1}{{60}}$$ How to I convert it into limit of sum so that I can proceed with the integration?","['limits', 'calculus']"
3873854,Are there non-commuting matrices for which $\mathrm{tr}(ABC)=\mathrm{tr}(BAC)$?,"For linear operators $A,B$ acting on $\mathbb{C}^n$ the fact that $\text{tr}(AB)=\text{tr}(BA)$ follows from the cyclic property of the trace, which says more generally that if $A,B,C$ are linear operators acting on $\mathbb{C}^n$ we have that $$\text{tr}(ABC)=\text{tr}(CAB)=\text{tr}(BCA)=\text{tr}(ABC).$$ My question is whether there are specific conditions on $A$ , $B$ , and $C$ such that $$\text{tr}(ABC)=\text{tr}(BAC)?$$ Clearly, its true if $A$ and $B$ commute, but its also sufficient for either $A$ or $B$ to commute with $C$ , but are these necessary for this property to hold? I should mention that I am primarily interested in the case that $A,B,C$ are invertible, since clearly if $A,B,C$ have zero divisors the property can be satisfied in a trivial sort of way.","['matrices', 'trace', 'linear-algebra']"
3873857,I'm asked to differentiate this $\dfrac{C}{2} \sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2}$ but I barely understand the notation.,"I'm asked to differentiate $$\dfrac{C}{2} \sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2},$$ according to $w^{j'}_{k}$ which is the $k$ th weight of the vector of weight of $j'$ . It seems that $\| W^{j'} \|^{2}_{2}$ stands for the the $L2$ norm squared. This seems to indicate that we have: $$\| W^{j'} \|^{2}_{2} = w^{2}_1 + w^{2}_2 + ... + w^{2}_k$$ Then to my understanding $\sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2}$ seems to indicate that we have: $$\sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2} = (w^{2}_1 + w^{2}_2 + ... + w^{2}_k)_{j'=1} + (w^{2}_1 + w^{2}_2 + ... + w^{2}_k)_{j'=2} + ... + (w^{2}_1 + w^{2}_2 + ... + w^{2}_k)_{j'=m}$$ I have absolutely no clue how to derivate this formula.
Do I use the sum rule to get this? $$\dfrac{\partial }{\partial w^{j'}_{k}} =\dfrac{C}{2} (2w_k)_{j'=1} + (2w_k)_{j'=2}  + ... + (2w_k)_{j'=m}$$ Edit:
This is from my machine learning course. This formula represents the regularization term we add to the loss function of an SVM (Support vector machines) in order to minimize the objective function. Here $W$ is a vector containing scalars $w$ .","['normed-spaces', 'machine-learning', 'calculus', 'linear-algebra', 'derivatives']"
3873860,Starting point for learning etale cohomology,"I am currently interested in learning etale cohomology. My background is what you could find in Hartshorne (mostly in chapter II and III) and I also have strong intuition from algebraic topology. I heard that there are several good references on this topic Kiehl, Etale Cohomology and The Weil Conjectures . Lei fu, Etale Cohomology Theory . Milne, Lectures on Etale Cohomology . Tamme, Introduction to Etale Cohomology . I incline to use Leifu's book, but I wonder whether the author treats too many auxiliary results so I desire a roadmap for reading this book. Also, I appreciate any comment about the pros and cons of each of the aforementioned books. Thanks in advance.","['etale-cohomology', 'homological-algebra', 'category-theory', 'algebraic-geometry', 'soft-question']"
3873893,"Representations of $SU(1,1)$ and its isomorphism with $SL(2,\mathbb{R})$","I've been reading up on the Lorentz group in $(2+1)-$ dimensions, i.e. $SO(2,1)$ . As I understand it, $SO(2,1)$ is isomorphic to $SL(2,\mathbb{R})$ , $SU(1,1)$ and $Sp(2,\mathbb{R})$ (modulo $\mathbb{Z}_2$ ). I am most interested in the $SL(2,\mathbb{R})$ , $SU(1,1)$ isomorphisms. In particular, there are a couple of confusions I have about the matrix representations of these groups, which I shall now elaborate on. $$$$ 1. $~$ For $SL(2,\mathbb{R})$ , the anti-fundamental representation is trivially equivalent (in fact, identical), to the fundamental representation $N\in SL(2,\mathbb{R})$ , since by definition, the group elements are real. I assume this means that, unlike in the case of $SL(2,\mathbb{C})$ , we don't need dotted indices, and $SL(2,\mathbb{R})$ naturally acts on real two-dimensional spinors $\psi_a$ , which transform under $SL(2,\mathbb{R})$ as $$\psi_a \quad\rightarrow\quad \psi'_a = N_a^{~~b}\psi_b\;,$$ i.e. we do not need to consider "" barred "" spinors $\bar{\psi}_{\dot{a}} = (\psi_a)^\ast$ at all? If this is correct, then my main confusion comes when considering $SU(1,1)$ . In this case, there exists an invariant matrix $$K=\begin{pmatrix}1&0\\ 0&-1\end{pmatrix} \;,$$ such that for $\zeta,\zeta^\dagger\in SU(1,1)$ we have $$\zeta^\dagger K\zeta = K \;.$$ This implies an isomorphism between the fundamental and anti-fundamental representations $\zeta$ and $\zeta^\ast$ , respectively. However, does that mean that we can drop the dotted indices? In principle, we still have that $$ \chi_{\alpha} \quad\rightarrow\quad \chi'_{\alpha} = \zeta_{\alpha}^{~~\beta}\chi_{\beta} \;,\qquad \bar{\chi}_{\dot{\alpha}} \quad\rightarrow\quad \bar{\chi}'_{\dot{\alpha}} = \zeta_{\dot{\alpha}}^{~~\dot{\beta}}\bar{\chi}_{\dot{\beta}} \;.$$ What I find particularly confusing, is if we then consider a bi-spinor representation of a Lorentz vector $p^\mu$ , i.e. $p_\mu\sigma^\mu$ (where $\sigma^\mu$ are the Pauli matrices, and $\mu=0,1,2$ ), should it have one dotted and one un-dotted index, i.e. $p_{\alpha\dot{\alpha}}$ , such that it transforms under $SU(1,1)$ as $$p_{\alpha\dot{\alpha}}\quad\rightarrow\quad p'_{\alpha\dot{\alpha}} = \zeta_{\alpha}^{~~\beta}(\zeta^\ast)_{\dot{\alpha}}^{~~\dot{\beta}}p_{\beta\dot{\beta}}\;?$$ $$$$ 2. $~$ Following on from this, $SL(2,\mathbb{R})$ and $SU(1,1)$ are mapped to one another via a Cayley transformation, of the form: $$C = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & i\\ i & 1\end{pmatrix} \;,$$ such that $CNC^{-1}=CNC^{\dagger}\in SU(1,1)$ for $N\in SL(2,\mathbb{R})$ . What is the consistent way to map $SL(2,\mathbb{R})$ spinors $\psi_a$ to $SU(1,1)$ spinors $\chi_{\alpha}$ , particularly if the above is correct about the anti-fundamental representation? Naively, it seems like it should be $$\chi_{\alpha} = C_{\alpha}^{~~a}\psi_a\;,\qquad \bar{\chi}_{\dot{\alpha}} = (C^\ast)_{\dot{\alpha}}^{~~a}\psi_a \;,$$ but I'm concerned by the mixing of dotted and un-dotted indices, and also barred and un-barred spinors. $$$$ 3. $~$ Finally, both constructions are quite confusing in indicial form. In particular, given that the Levi-Civita tensor $\epsilon^{ab}$ ( $\epsilon^{12}=1=-\epsilon_{12}$ ) can be used as a metric in both $SL(2,\mathbb{R})$ and $SU(1,1)$ , to raise/lower spinor indices, e.g. $\psi^a = \epsilon^{ab}\psi_b$ in the $SL(2,\mathbb{R})$ case. Can $\epsilon^{ab}$ be used to raise/lower indices of the matrix representations of $SL(2,\mathbb{R})$ and $SU(1,1)$ also? For example, is $$ N^{a}_{~~b} = \epsilon^{ac}\epsilon_{db} \,N_c^{~~d}$$ correct? If so, it seems to imply odd consequences, e.g. for $N\in SL(2,\mathbb{R})$ , $$\epsilon^{ac}\epsilon_{db} \,N_c^{~~d} = (N^{-1})_b^{~~a} = N^{a}_{~~b} = (N^T)_{b}^{~~a} \;,$$ but this can't be right, because $SL(2,\mathbb{R})$ isn't an orthogonal group. $$$$ Apologies for the length of this post. Any help on clearing up this matter for me would be very much appreciated. It's been bugging me for a while, and I haven't been able to find any enlightening literature on the subject (Bargmann's seminal paper doesn't really go into detail on this it seems). $$$$ Disclaimer : Please bear with me here, as I am a physicist, with a physicist's understanding of group theory, and representation theory. Apologies for the lack of rigour, and index notation.","['group-theory', 'representation-theory', 'index-notation']"
3873899,Finite Galois representations are Geometric?,"A famous conjecture, the Fontaine-Mazur conjecture, predicts which $p$ -adic Galois representations of a number field ""come from geometry"" are a subquotient of the (Weil) cohomology of a scheme. Two questions on that: what is the current state of the conjecture (I think we know the rank one case by class field theory) and second are there analogues, where we consider representations of the Galois group of a function field or consider complex representations? To adress the question the titel: do we know that Galois representations with finite image come from geometry?","['algebraic-number-theory', 'algebraic-geometry', 'galois-representations', 'arithmetic-geometry']"
3873937,The real function $f$ such that $\log \cdots \log (f)$ is strictly convex on its domain for any number of $\log$'s,"Does there exist a function $f: (a,b) \to \mathbb R$ ( $a,b$ are allowed to be infinity) such that $\log \cdots \log (f)$ is strictly convex on its whole domain of definition for an arbitrary number (though finitely many) of $\log$ 's? If such function exists, it must be very very convex (""more convex"" than any $\exp \dots \exp (x) $ , which will become concave after a finitely many $\log$ 's applied on) The form of $f$ doesn't have to be concrete. It can be infinite series or one can even show its existence or nonexistence.","['logarithms', 'real-analysis', 'functions', 'iterated-function-system', 'convex-analysis']"
3873938,Is the degree homomorphism $\text{deg}: \text{Pic}(X)\to \mathbb{Z}$ surjective?,"Let $k$ be a field, $X$ a curve over $k$ , $\operatorname{Div}(X)$ the divisor group of $X$ , and $\operatorname{Pic}(X)$ the divisor class group (the Picard group) of $X$ . Consider the degree homomorphism $$
\begin{split}
\deg: \operatorname{Div}(X)&\to\mathbb{Z} \\
\sum_{P\in X}n_{P}P &\mapsto \sum_{P\in X}n_{P}\cdot [k(P):k],
\end{split}
$$ and the induced degree homomorphism $$
\deg: \operatorname{Pic}(X)\to\mathbb{Z}.
$$ Here $n_{p}\in\mathbb{Z}$ , and $k(P)$ is the residue field of $P$ . Are those homomorphisms surjective? Namely, is there necessarily exists a divisor $D\in\operatorname{Div}(X)$ , with $\deg(D)=1$ ?","['field-theory', 'divisors-algebraic-geometry', 'algebraic-geometry', 'algebraic-curves']"
3873941,"What is the smallest digraph whose reflexive, symmetric, transitive closures (in all combinations) are distinct?","For any given directed graph, we may consider the various closures of it with respect to reflexivity, symmetry, and transitivity, in any combination, like this: For the particular graph shown above, this process results in eight distinct graphs, including the original graph. This graph is not the smallest instance with this feature, however, since if we delete the source point at right, we will still have eight distinct graphs, like this: Question. What is the smallest directed graph such that these various closures are all distinct and distinct from the original? The second example gets it down to five vertices and four edges. The question arose in a reply of Bryan Bischof to my recent tweet https://twitter.com/JDHamkins/status/1318447368732397569 . The first image is drawn from the chapter on Functions and Relations in my book, Proof and the Art of the Mathematics, available from MIT Press: https://mitpress.mit.edu/books/proof-and-art-mathematics .","['graph-theory', 'combinatorics', 'relations']"
3873965,Is $Y - E[Y|X]$ independent from $X$?,"For any two random variables $X$ and $Y$ , is $Y - E[Y|X]$ independent from $X$ ? Intuitively I think it should be the case, since conditioning should include all the aspects of $X$ which are tangled with $Y$ .",['probability-theory']
3874019,Question about coefficients of generating functions,"Theorem: Let $n> 0 \in \mathbb Z.$ Let $p_n$ stand for the number of integer partitions of $n$ and let $k$ be the  number of consecutive integers in a partition. Then $p_n + \sum_{k \ge 1}(-1)^k(p_{n - k(3k - 1)/2} + p_{n - k(3k + 1)/2}) = 0$ . Proof: $1 = \left(\sum_{k \ge 0}p_kx^k\right)\left(1 + \sum_{k \ge 1}(-1)^k\left(x^{k(3k - 1)/2} + x^{k(3k + 1)/2}\right)\right)$ . The power series on the right hand side of the equality in the proof above equals $1$ and so  the coefficient of $x^n$ must equal $0$ except for that of $x^0$ . That means $p_n = p_{n - k(3k - 1)/2} = p_{n - k(3k + 1)/2} = 0.$ That much of the proof I think I understand. When we expand the product in the proof we get a term $\sum_{k \ge 1}(-1)^k\left(p_nx^{k(3k - 1)/2} + p_nx^{k(3k + 1)/2}\right)$ in the sum (I think). With that said, how do we get the coefficients of $x^{k(3k + 1)/2}, \ x^{k(3k - 1)/2}$ to become $p_{n - k(3k + 1)/2}, \ p_{n - k(3k- 1)/2}$ , respectively?","['integer-partitions', 'number-theory', 'combinatorics', 'discrete-mathematics', 'generating-functions']"
3874101,"If $\lim_{n\to\infty}P(A_n)=1$, there exists a subsequence $\{A_{n_k}\}_{k=1}^\infty$ with $P\left(\bigcap_{k=1}^\infty A_{n_k}\right)>0$","Problem: Let $(\Omega,\mathcal F,P)$ be a probability space. Given a sequence of events $\{A_n\}_{n=1}^\infty\subset\mathcal F$ with $\lim\limits_{n\to\infty}P(A_n)=1$ , there exists a subsequence $\{A_{n_k}\}_{k=1}^\infty$ such that $$P\left(\bigcap_{k=1}^\infty A_{n_k}\right)>0.$$ My Proof: Choose a sequence $\{\varepsilon_j\}_{j=1}^\infty$ with $\varepsilon_j=4^{-j-1}>0$ for all $j\in\mathbb N$ . Then $\sum_{j=1}^\infty\varepsilon_j=12^{-1}<1$ by the geometric series formula. Next, since $P(A_n)\to1$ as $n\to\infty$ , we can choose a subsequence $\{A_{n_j}\}_{j=1}^\infty$ such that $P(A_{n_j})>1-\varepsilon_j$ for all $j\in\mathbb N$ . Then $$P\left(\bigcap_{j=1}^\infty A_{n_j}\right)=1-P\left(\bigcup_{j=1}^\infty A_{n_j}^\complement\right)\geq1-\sum_{j=1}^\infty P\left(A_{n_j}^\complement\right)>1-\sum_{j=1}^\infty \varepsilon_j=\frac{11}{12}>0.$$ Do you agree with my proof above? Thank you for your time.","['solution-verification', 'probability-theory', 'probability']"
3874140,Justifying the change of variables formula $\int_{g(a)}^{g(b)} f(y)dy = \int_a^b f(g(x))g'(x)dx$ for Lebesgue Integration,"This is a problem from Royden & Fitzpatrick 4th ed, page 129 problem 59. I am struggling proving it and was wondering if someone can help prove it please? Thank you For a nonnegative integrable function $f$ over $[c,d],$ and a strictly increasing absolutely continuous function $g$ on $[a,b]$ such that $g([a,b]) \subseteq [c,d],$ is it possible to justify the change of variables formula $$\int_{g(a)}^{g(b)} f(y)dy = \int_a^b f(g(x))g'(x)dx,$$ by showing that $$\frac{d}{dx} \left[\int_{g(a)}^{g(x)} f(s)ds - \int_a^x f(g(t))g'(t)dt \right] = 0 \text{ for almost all } x\in (a,b)?$$","['integration', 'measure-theory', 'lebesgue-integral', 'real-analysis', 'absolute-continuity']"
3874164,"Relation between the convolution power $\mu^{\ast k}$ of a measure $\mu$ and the composition $κ^k(1,\;\cdot\;)$ of the kernel $κ(x,B):=\mu(x^{-1}B)$","Let $G$ be a group and $\mathcal G$ be a $\sigma$ -algebra on $G$ . Assume the inversion $G\ni x\mapsto x^{-1}$ is $(\mathcal G,\mathcal G)$ -measurable and the group law $G^2\ni(x,y)\mapsto xy$$ is $ (\mathcal G\otimes\mathcal G,\mathcal G)$-measurable. Now let $$\tau_k:G^k\to G\;,\;\;\;x\mapsto x_1\cdots x_k$$ denote the iterated group law for $k\in\mathbb N$ and $\mu_i$ be a $\sigma$ -finite measure on $(G,\mathcal G)$ . Then the *convolution of $\mu_1,\ldots,\mu_k$ is defined to be the pushforward $\mu_1\ast\cdots\ast\mu_k:=\tau_k(\mu_1\otimes\cdots\otimes\mu_k)$ of $\mu_1\otimes\cdots\otimes\mu_k$ under $\tau_k$ . If $\mu:=\mu_1=\cdots\mu_k$ , we write $\mu^{\ast k}:=\mu_1\ast\cdots\ast\mu_k$ . I was able to show that $$\mu^{\ast2}(B)=\int\mu({\rm d}x)\mu(x^{-1}B)\;\;\;\text{for all }B\in\mathcal G\tag1,$$ where $x^{-1}B:=\{x^{-1}y:y\in B\}$ . If we denote $$\tau_x:G\to G\;\;\;\;y\mapsto x^{-1}y$$ for $x\in G$ , $$\kappa(x,B):=\mu(x^{-1}(B))\;\;\;\text{for }(x,B)\in(G,\mathcal E)$$ and the inverse element of $G$ by $1$ , then $(1)$ is equivalent to $$\mu^{\ast2}(B)=\kappa^2(1,B)\;\;\;\text{for all }B\in\mathcal G\tag2,$$ where $\kappa^2$ denotes the composition of kernels . Are we able to generalize $(2)$ and show $$\mu^{\ast k}(B)=\kappa^k(1,B)\;\;\;\text{for all }B\in\mathcal G\tag3$$ for all $k\in\mathbb N$ ?","['convolution', 'stochastic-processes', 'markov-process', 'group-theory', 'probability-theory']"
3874199,Maximizing the amount of fish caught,"In one of my classes, our teacher introduced us to a software. Using this software, we had to model a lake with fish in it. The growth of the fish population would be governed according to two things: the natural logistic growth rate and fishing. The task was then to maximize the amount fished after a set period of time. This was to learn how to use the software, but just for fun, I wanted to find the absolute maximum. Let $r$ be the logistic growth rate, $k$ be the carrying capacity, $T$ be the total time, and $P_0$ be the starting fish population with $P_0 < k$ . All four of these values are fixed. Let $P(t)$ be the fish population at time $t$ and $f(t)$ be the fishing rate. Then I got that the differential equation for $P$ would be $$\frac{dP}{dt} = \underbrace{rP\left(1 - \frac{P}{k}\right)}_{\text{logistic growth}} - f \tag 1$$ while the amount fished (which is the value to be maximized) is $$\int_0^T f(t)dt \tag 2$$ The main condition is that if $P$ drops to $0$ , then $f$ would also have to drop to $0$ . You can't get fish if there's no fish left! Otherwise, you could make $f$ arbitrarily large. Also, $f$ must be nonnegative: no adding fish to the lake. Trying to solve $(1)$ for an explicit equation for $P$ seems impossible, but I'll show what I tried. Setting it up as $Mdt + NdP = 0$ makes it $$\left( rP \left( 1-\frac{P}{k} \right) - f \right) dt + (-1) dP = 0$$ Multiplying by an integrating factor $u$ , I get that $$r \left( 1- \frac{2P}{k} \right)u = -\frac{\partial u}{\partial t} - \left( 
rP\left(1-\frac{P}{k}\right)-f \right) \frac{\partial u}{\partial P}$$ I'm pretty new to differential equations, and I don't know how to solve this (or if it even is solvable). Rearranging $(1)$ , I get that $f = rP\left(1-\frac{P}{k}\right) - \frac{dP}{dt}$ , which means that $(2)$ is $$\int_0^T \left( rP\left(1-\frac{P}{k}\right) - \frac{dP}{dt} \right) dt = r\int_0^T P\left(1-\frac{P}{k}\right)dt - P(T)+P_0$$ Intuitively, it is true that $P(T) = 0$ . If it was greater than $0$ , then more fish could be taken.
I'm not sure what to do from here though. One of the challenges with this problem is that it's impossible  to work from the end. It might pay off to let the fish grow more at that instant so that you can fish more later. My questions: $1. $ What is the $f(t)$ that maximizes $(2)$ given that $P$ changes according to $(1)$ , $f(t) \ge 0$ , and if $P = 0$ , then $f = 0$ ? $2. $ Can the problem be modified in some way to get rid of $r, k, T,$ or $P_0$ (i.e. can it be transformed so that one of those can be fixed at $1$ [or any other value])? I think a linear transformation might be useful here, but I don't know. $3. $ I'm doubtful of an exact closed-form solution, so are there approximations of $f(t)$ ?","['ordinary-differential-equations', 'calculus-of-variations', 'multivariable-calculus', 'partial-differential-equations', 'optimization']"
3874213,Matrix Multiplication in Index Notation,"I am trying to express the $i$ and $j$ th component of the product $AB^{T}C$ in terms of the components of $A$ , $B$ and $C$ in index notation/Einstein summation convention, where $A\in\Bbb{R}^{n\times p}$ , $B\in\Bbb{R}^{q\times p}$ and $C\in\Bbb{R}^{q\times s}$ . I am aware of the standard notation for a supposed matrix product $C=AB$ , which is $c_{ik}=a_{ij}b_{jk}$ , but afterwards I am not sure how to proceed - appreciate any help I can get, thanks.","['multivariable-calculus', 'calculus']"
3874266,Why are the eigenfunctions of my Hermitian operator not orthogonal?,"I am finding that the eigenfunctions of my Hermitian differential operator are not orthogonal and I do not know why. Consider the differential operator $$ \mathcal{L} = x^2 \frac{d^2}{dx^2} + 2x \frac{d}{dx} + c $$ where $c$ is a constant. This is an Hermitian operator with respect to the inner product $$ \langle \psi , \phi \rangle =\int_{- \infty}^\infty dx\psi^*\phi$$ We have $$ \langle \psi , \mathcal{L} \phi \rangle =\int_{- \infty}^\infty dx\psi^*\left( x^2 \frac{d^2 \phi}{dx^2} + 2x \frac{d \phi}{dx} + c \phi\right) \\ 
= \int_{- \infty}^\infty dx \left(\frac{d^2}{dx^2} \left( x^2 \psi^* \right) \phi - \frac{d}{dx}\left( 2x \psi^* \right) \phi + c \psi^* \phi \right) \\
= \int_{- \infty}^\infty dx\left( 2\psi^* + 4x \frac{d \psi^*}{dx} + x^2 \frac{d^2 \psi^*}{dx^2} - 2\psi^* - 2x \frac{d \psi^*}{dx} + c\psi^* \phi\right) \\
 = \langle \mathcal{L} \psi , \phi \rangle $$ where I have assumed my solutions vanish at $\pm \infty$ so the boundary terms vanish when I integrate by parts. So my operator is Hermitian and I expect my eigenfunctions to be orthogonal. Consdider the eigenvalue equation $\mathcal{L} \psi = \lambda \psi$ , this yields the differential equation $$  \quad x^2 \psi''(x) + 2x \psi'(x) + (c - \lambda)\psi = 0$$ The eigenvalue equation is therefore an Euler differential equation. If we take a trial solution $\psi(x) =  x^n$ , then substituting this in yields the quadratic equation $$ n^2 + n + (c- \lambda) = 0 \quad \Rightarrow \quad n=-\frac{1}{2} \pm \frac{1}{2} \sqrt{1- 4(c-\lambda)}$$ Suppose we took the special case where the eigenvalues are negative and of the form $\lambda = -E^2$ , for some $E$ , and let $ c = \frac{1}{4}$ , then we have $ n = -\frac{1}{2} \pm i E$ and the solutions will be given by $$ \psi_\pm(x) = \frac{1}{\sqrt{x}} x^{\pm i E}$$ My problem is that these solutions do not appear to be orthogonal for different eigenvalues. If we take the solutions whose eigenvalues are $\lambda $ and $\lambda'$ , then the inner product would be $$ \langle \psi , \psi' \rangle = \int_{-\infty}^\infty dx \frac{1}{x} x^{\pm i (E'-E)} $$ which according to Wolfram is divergent. I am not sure why my solutions for different eigenvalues are not orthogonal. Any hints would be greatly appreciated.","['inner-products', 'adjoint-operators', 'functional-analysis', 'ordinary-differential-equations']"
3874289,Invariant for interesting set of functions generalizing $\sin$ and $\cos$ and other properties,"In an attempt to generalize $\sin(x)$ and $\cos(x)$ â€”but just a curiosityâ€”I found some functions with fairly interesting properties. The idea was to extent the definition of $\sin$ and $\cos$ as unique solutions to $y''+y=0$ (with $y(0)=0$ and $y'(0)=1$ in the former and $y(0)=1$ and $y'(0)=0$ in the latter case), as well as their relationship to each other through differentiation. Define the ""first"" $n$ 'th root of unity as $\xi _{n}=\exp(2 \pi i / n)$ . I define $X_{m}^{n}$ as $$X_{m}^{n}(x)=\frac{\xi_{2n}^{m}}{n}\sum_{i=0}^{n-1} \xi_{n}^{im} \exp\left ( \xi_{n}^{i}\xi_{2n}x \right )$$ I know the $n$ 'th roots can be simplified, but I find it easier to conceptualize in this way. For example, with $n=2$ we get: $$X_{0}^{2}(x)=\frac{1}{2}\left ( e^{ix} + e^{-ix} \right )=\cos(x)$$ $$X_{1}^{2}(x)=\frac{i}{2}\left ( e^{ix} - e^{-ix} \right )=-\sin(x)$$ Properties Note that $\frac{d}{dx}X_{m}^{n}(x)=X_{m+1}^{n}(x)$ for $m$ in $\left \{ 0, 1...n-1 \right \}$ , and $\frac{d}{dx}X_{n-1}^{n}(x)=-X_{0}^{n}(x)$ . If you continue differentiating you'll enter an ""endless cycle"". You'll also find that $X_m^{n}$ is the unique solution to $y^{ \left ( n \right )}+y=0$ with $y^{\left ( i \right )}(0)=\begin{cases}
1 & \text{ if } i= m \text{ mod } 2n\\ 
-1 & \text{ if } i= m+n \text{ mod } 2n\\ 
0 & \text{ otherwise }
\end{cases}$ Question The last thing I know about these functions is that they admit an invariant in the same way that $cos(x)^{2}+sin(x)^{2}=1$ , and remarkably in a simple way. I'm looking for help in finding a simple expression for this invariant. I have found a method for generating this invariant but I find the algebra much too overwhelming. Below I show how to find the invariant for $n=3$ . You should probably write it out or it'll just look like gibberish. We have: $$X_{0}^{3}(x)=\frac{1}{3} \left ( e^{\xi_6 x} + e^{\xi_6 \xi_3 x} + e^{\xi_6 \xi_3^{2} x} \right )$$ $$X_{1}^{3}(x)=\frac{\xi_6}{3} \left ( e^{\xi_6 x} + \xi_3 e^{\xi_6 \xi_3 x} + \xi_3^{2} e^{\xi_6 \xi_3^{2} x} \right )$$ $$X_{2}^{3}(x)=\frac{\xi_6^{2}}{3} \left ( e^{\xi_6 x} + \xi_3^{2} e^{\xi_6 \xi_3 x} + \xi_3 e^{\xi_6 \xi_3^{2} x} \right )$$ Next note $$e^{\xi_{6} x} = X_{0}^{3}(x) + \xi_{6}^{5} X_{1}^{3}(x) + \xi_{6}^{4} X_{2}^{3}(x)$$ Substituting $x\rightarrow \xi_{3}x$ and $x\rightarrow \xi_{3}^{2}x$ in the above and multiplying the resulting expressions, we find: $$e^{\xi_{6} \left ( 1 + \xi_{3} + \xi_{3}^{2} \right ) x} = e^{\xi_{6} 0} = 1 = \left ( X_{0}^{3}(x) + \xi_{6}^{5} X_{1}^{3}(x) + \xi_{6}^{4} X_{2}^{3}(x) \right )\left ( X_{0}^{3}(\xi_{3} x) + \xi_{6}^{5} X_{1}^{3}(\xi_{3} x) + \xi_{6}^{4} X_{2}^{3}(\xi_{3} x) \right )\left ( X_{0}^{3}(\xi_{3}^{2} x) + \xi_{6}^{5} X_{1}^{3}(\xi_{3}^{2} x) + \xi_{6}^{4} X_{2}^{3}(\xi_{3}^{2} x) \right )$$ This may not look helpful at all, but you can check that $X_{m}^{3}(\xi_{3}^{i} x)=X_{m}^{3} \xi_{3}^{-im} (x)$ . Again, try it with a few cases. We then get $$1=\left ( X_{0}^{3}(x) + \xi_{6}^{5} X_{1}^{3}(x) + \xi_{6}^{4} X_{2}^{3}(x) \right ) \left ( X_{0}^{3}(x) + \xi_{6}^{3} X_{1}^{3}(x) + X_{2}^{3}(x) \right ) \left ( X_{0}^{3}(x) + \xi_{6} X_{1}^{3}(x) + \xi_{6}^{2} X_{2}^{3}(x) \right )$$ You can also tell at this point that it is constant by complex conjugating on the RHS, seeing that we have a real holomorphic function which is therefore constant. This we can multiply out (by hand) to get $$(X_{0}^{3})^{3}-(X_{1}^{3})^{3}+(X_{2}^{3})^{3}+3X_{0}^{3}X_{1}^{3}X_{2}^{3}=1$$ I graphed this with Python and it seems to hold true very well. If you decide to plot these functions yourself, because these functions taken individually blow up in an exponential way, you'll find that beyond a certain point this expression seems to blow up due to floating point (im)precision. I find it absolutely amazing that such a simple expression exists when we know that multiplying (complicated expressions (three times!)) tends to complicate more than simplify. The question is, in the case of $n=4$ and beyond, what is the invariant? For every $n$ I expect there to be a real $n$ -th degree polynomial invariant expression in these functions. What are the coefficients? I would also like to know if there's more to these functions than I've written so far, or if there is another way to look at them? I have found the general formula for the product (the one I used with $n=3$ ), in which sense I am really asking how you would simplify $$1=\prod_{i=0}^{n} \sum_{j=0}^{n} \xi_{2n}^{-j\left ( 2i + 1 \right )}X_{j}^{n}$$ (For this I don't think you'll need any special properties of $X_{j}^{n}$ ) EDIT I found that if we name $(X_{0}^{4}(x),...,X_{3}^{4}(x)) = (a,b,c,d)$ then $$1 = a^{4}+b^{4}+c^{4}+d^{4}
+2a^{2}c^{2}+2b^{2}d^{2}+4a^{2}bd+4acd^{2}-4bc^{2}d-4ab^{2}C$$ For $n=5$ ... $$1 = a^{5} + 5 a^{3} b e + 5 a^{3} c d - 5 a^{2} b^{2} d - 5 a^{2} b c^{2} + 5 a^{2} c e^{2} + 5 a^{2} d^{2} e + 5 a b^{3} c + 5 a b^{2} e^{2} - 5 a b c d e - 5 a b d^{3} - 5 a c^{3} e + 5 a c^{2} d^{2} + 5 a d e^{3} - b^{5} - 5 b^{3} d e + 5 b^{2} c^{2} e + 5 b^{2} c d^{2} - 5 b c^{3} d + 5 b c e^{3} - 5 b d^{2} e^{2} + c^{5} - 5 c^{2} d e^{2} + 5 c d^{3} e - d^{5} + e^{5}$$ At this point I think it's fair to say that maybe I'm looking in the wrong place.","['invariance', 'trigonometry', 'functions', 'exponential-sum', 'algebra-precalculus']"
3874307,Good upper bound on $f(n)$,"This is an elementary question. I'm trying to understand the function (a discrete), $$f(n) = \left(\displaystyle \dfrac{\sum_{k=0}^n \frac{a^k}{k!}}{\sum_{k=0}^{n-1}\frac{a^k}{k!}}\right)^n\,$$ where $a>0$ is a constant. I used WolframAlpha to check the behavior of a graph of a function similar to this and it turns out it's a bell curve (like the Binomial Distribution), which means that this's neither monotonically increasing nor decreasing.So, there must be a local maximum but I don't think there's a proper way to figure out the exact maximum value. Anyways, I'm interested in a good upper bound on $f(n)$ . Also, for a given $a>0$ , I noticed that $f(n)\to 1$ as $n\to \infty.$ It can be proved that $f(n)^{1/n} \to 1$ as $n\to \infty$ but not sure how to show that for $f(n)$ . Any help would be apreciated.","['real-analysis', 'calculus', 'discrete-mathematics', 'sequences-and-series', 'exponential-function']"
3874371,Bound of sum of cos(kx),"How can I show that for each $x\in (0,2\pi)$ , there is a number $C(x)>0$ such that $|\sum^n_{k=1}\cos(kx)|\leq C(x)$ , independent of $n$ . I know that \begin{align}
2\sin(\frac{x}{2})\sum^n_{k=1}\cos(kx)=\sum^n_{k=1}\sin((k+\frac{1}{2})x)-\sin((k-\frac{1}{2})x)
\end{align} Any help is much appreciated!","['sequence-of-function', 'sequences-and-series', 'real-analysis']"
3874407,Prove Laplace's equation is rotation invariant,"Prove that Laplace's equation $\Delta u = 0$ is rotation invariant; that is, if $O$ is an orthogonal $n\times n$ matrix and we define $$       v(x) := u(Ox) \quad (x \in \mathbb{R}^n)$$ then $\Delta v = 0$ . I wanted to see if what I had below was correct and complete. Any feedback on rigor is greatly appreciated. In cartesian coordinates, we have $\Delta u = \sum_{i = 1}^n \frac{\partial^2 u}{\partial x_i^2}$ . Letting the $i$ th row and $j$ th column of the matrix $O$ be $a_{ij}$ we have \begin{equation}
 \frac{\partial v}{\partial x_j} = \sum_{i =1}^n\frac{\partial u}{\partial x_i}a_{ij}
\end{equation} Thus, \begin{equation}
 \frac{\partial^2 v}{\partial x_j^2} = \sum_{i =1}^n\sum_{k = 1}^n\frac{\partial^2 u}{\partial x_i\partial x_k}a_{ij}a_{kj}
\end{equation} Therefore: \begin{align}
\Delta v &= \sum_{j = 1}^n \frac{\partial^2 v}{\partial x_j^2}\\
    &= \sum_{j = 1}^n \left(\sum_{i =1}^n\sum_{k = 1}^n\frac{\partial^2 u}{\partial x_i\partial x_k}a_{ij}a_{kj}\right)\\
    &= \sum_{i =1}^n\sum_{k = 1}^n\left(\sum_{j = 1}^n \frac{\partial^2 u}{\partial x_i\partial x_k}a_{ij}a_{kj}\right)\\
    &= \sum_{i =1}^n\sum_{k = 1}^n \frac{\partial^2 u}{\partial x_i\partial x_k}\left(\sum_{j = 1}^na_{ij}a_{kj}\right)\\
    &=\sum_{i =1}^n\sum_{k = 1}^n \frac{\partial^2 u}{\partial x_i\partial x_k}\delta_{i, k}\\
    &= \sum_{i =1}^n\frac{\partial^2 u}{\partial x_i^2} = 0
\end{align}","['ordinary-differential-equations', 'partial-differential-equations']"
3874450,The limit of the integral of $\text{sinc}(x)^n$,"I saw this exercise, and I'm wondering what methods could be used to solve it. $$
\lim_{n\to\infty}\left[%
n^{y}\int_{0}^{\infty}
\operatorname{sinc}^n\left(x\right)\,
\mathrm{d}x\right]
$$ Any naïve approach I've tried has failed pretty fast, such as splitting $\operatorname{sinc}^{n}\left(x\right)$ to $\sin^{n}\left(x\right)\cdot\frac{1}{x^{n}}$ and integrating by parts, so what method would you use to tackle this problem $?$ .","['integration', 'limits']"
3874519,Proof: Brownian motion has no intervals of monotonicity,"I quote Morters-Peres (2010) . My observations/questions in $\color{red}{\text{red}}$ . Theorem Almost surely, for all $0<a<b<\infty$ , Brownian motion $\left(B_t\right)_t$ is not monotone on the interval $[a,b]$ . Proof Fix a nondegenerate interval $[a,b]$ . If it is an interval of monotonicity, then we pick numbers $a=a_1\le\ldots\le a_{n+1}=b$ and divide $[a,b]$ into $n$ sub-intervals $[a_i,a_{i+1}]$ . Each increment $B(a_{i+1})-B(a_i)$ has to have the same sign. As the increments are independent (by definition), this has probability $2\cdot2^{-n}$ , and taking $n\to\infty$ shows that probability that $[a,b]$ is an interval of monotonicity must be zero. $\color{red}{\text{(So far so good to me and I am believing that this suffices to prove the above theorem,}}$ $\color{red}{\text{doesn't it?)}}$ Taking a countable union gives that, almost surely, there is no nondegenerate interval of monotonicity with rational endpoints, but each nondegenerate interval would have a nondegenerate rational sub-interval. $\color{red}{\text{(I cannot really understand the immediately above statement. Is it crucial to conclude}}$ $\color{red}{\text{the proof of the above theorem? If so, why? And what does it mean?}}$ $\color{red}{\text{Why are 
""countable union"", ""rational endpoints"" and ""rational sub-intervals"" recalled?}}$ $\color{red}{\text{Could you please help me understand this part with a detailed answer?)}}$","['proof-explanation', 'monotone-functions', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
3874529,Proving $abc-1+\sqrt\frac 2{3}\ (a-c)\ge 0$,"The question is this: If $a\ge b\ge c\ge 0$ and $a^2+b^2+c^2=3$ , then prove that $$abc-1+\sqrt\frac 2{3}\ (a-c)\ge 0$$ For my work on this inequality, I have proved already under constraints that it is true. Proof for: $\sqrt{3}(bc - 1) + \sqrt{2}(1-c)\geqslant0.$ $$
\sqrt{3}abc + 
\sqrt{2}a  -
\sqrt{3} -
\sqrt{2}c 
\geqslant 0
$$ $$
a\left(
\sqrt{3}bc + 
\sqrt{2}
\right) 
+ (-1)\left(
\sqrt{3} + 
\sqrt{2}c 
\right) \geqslant 0
$$ $$
(1 + 1)(a\left(
\sqrt{3}bc + 
\sqrt{2}
\right) 
+ (-1)\left(
\sqrt{3} + 
\sqrt{2}c 
\right)) \geqslant 0
$$ By Chebyshev, $$
(a - 1)
(\sqrt{3}bc + 
\sqrt{2} + 
\sqrt{3} +  
\sqrt{2}c 
)\geqslant0
$$ $$
a \geqslant 1
$$ Chebyshev Inequality requires the sequences to be monotonous. As $a+1>0$ , we need to have the other sequence also in the same order, hence the condition: $\sqrt{3}bc + \sqrt{2} \geqslant\sqrt{3} + \sqrt{2}c$ . The sequences are $(a,-1)$ and $(\sqrt{3}bc + \sqrt{2} ,\sqrt{3} + \sqrt{2}c)$ . I have tried another way but that was untrue. I have reached this far. The constraint $\sqrt{3}(bc - 1) + \sqrt{2}(1-c)\geqslant0$ isn't true always. Try $(a,b,c) = (\sqrt{3},0,0)$ . Thanks for extensions or other solutions too are welcome!","['multivariable-calculus', 'lagrange-multiplier', 'inequality']"
3874546,Determine whether a function is surjective?,"So say you have the function ${f: \mathbb{Z} \times \mathbb{Z} \rightarrow \mathbb{Z}}$ , and want to determine whether $f(m, n)=2m-n$ is a surjective function. I found this video online, which he then proves it. The part I don't understand is why the proof works. In the video, he gets to $f(0, -y) = 2(0)-(-y)=y$ , but I don't understand how that proves ${f: \mathbb{Z} \times \mathbb{Z} \rightarrow \mathbb{Z}}$ . Same with the function $f(m, n)=m^2-n^2$ , I found this answer online, but it looks like $f(m, n)=m^2-n^2$ is not a surjective function. But doesn't the function just have to output an integer? Why can't something like $m=2$ and $n=4$ work?","['functions', 'discrete-mathematics']"
3874566,Why the process $M_t = \sup_{0\leq s\leq t} W_s$ is not a markov process?,"For the sake of completeness, I will define what is a Markov Process Definition: An adapted, $\mathcal{X}$ -valued stochastistic process $X:\mathbb{R}_{+}\times \Omega \to \mathcal{X}$ is a Markov process if, for all $f\in \mathcal{B}_b(\mathcal{X})$ and all $0\leq s\leq t$ , $$\mathbb{E}[f(X_t)\mid \mathcal F_s]= \mathbb{E}[f(X_t)\mid X_s].$$ I recently started studying Markov processes and I am stuck in the following question Problem : Let $W_t$ be the standard Brownian motion, show that the process $$M_t=\sup_{0\leq s\leq t}W_s$$ is not a Markov process. The question does not actually say what is the filtration considered but I strongly think that is $\mathcal{F}_t = \sigma(W_s,\ s\in[0,t])$ . Generally, I post some ideas about what I have tried so far, but this time I am genuinely lost. I tried to use the equalities $\mathbb{P}(M_t \geq a) = 2\mathbb{P}(W_t \geq a)$ and $\mathbb{P}(M_t \geq a) = \mathbb{P}(|W_t| \geq a)$ , but I failed miserably. Can anyone help me?","['measure-theory', 'stochastic-processes', 'markov-process', 'brownian-motion', 'probability-theory']"
3874590,Solutions of $32p^7-q^7 = \pm 1$,"I am solving a problem and the original problem is equivalent to proving that there are no positive integers $p,q$ such that $32p^7-q^7 = \pm 1$ .","['number-theory', 'elementary-number-theory']"
3874603,Counting Problem with Arrangement of Items,"I have $12$ cupcakes, $6$ of them are vanilla, $4$ are chocolate, and $2$ are birthday cake flavor. Each cupcake is decorated differently making them unique. How many different ways are there to arrange these cupcakes that all the cupcakes of the same flavor are right next to each other? My Solution: I thought that we could could use a permutation here so: $\frac{12!}{6!4!2!}$ Where the chocolate can be organized in varies ways, same with vanilla, and birthday cake. How many ways can the cupcakes be arranged if one birthday cake flavored cupcakes must be on each end and the vanilla  cupcake cannot be placed next to the birthday cake flavors? My Solution: Since we only have two birthday cake flavor then they would go on the ends. Then two chocolate flavors would go next to those leaving us $2$ chocolate flavors left and $6$ vanilla flavors. $$\frac{12!}{2!6!2!2!}$$ $2$ ways to arrange the birthday cakes, $2$ ways to arrange the chocolate flavors(buffer of the birthday cake flavors), $2$ more ways to arrange the chocolate flavors, and then $6$ ways to arrange the vanillas. I was wondering if my approach to this problems correct using permutation in this manner.","['solution-verification', 'combinatorics']"
3874624,Finding the conditional probabilities of a latent dirichlet allocation model,"Let's say I'm defining a LDA as the following: For each doc $m$ : Sample topic probabilities $\theta_m \sim Dirichlet(\alpha)$ For each word $n$ : Sample a topic $z_{mn} \sim Multinomial(\theta_m)$ Sample a word $w_{mn} \sim Multinomial(\beta)$ where $\alpha, \beta$ are fixed hyperparameters. Now, to do this, I'm going to use an EM algorithm. For the 'E' step, given $\alpha$ and $\beta$ , I infer $z_{mn}$ for all $n$ and $\theta_m$ for all $m$ given $w, \alpha, \beta$ using Gibbs sampling. Equivalently, I'm trying to find $p(z, \theta | w, \alpha, \beta) = \frac{p(z, \theta, w | \alpha, \beta)}{p(w|\alpha, \beta)}$ using Gibbs sampling. At a high level, I understand what Gibbs Sampling is and what this LDA model does. We assigns topics to each word and document (assuming all others are correct). We do repeat this process in a chain until we maximize the probability of each $m$ and $n$ belonging to a particular category. However, I'm having trouble representing the conditional probabilities of the Gibbs Sampler for this model. Where do I even start and what am I looking for when asked to find the conditional probabilities of this sampler?","['statistical-inference', 'statistics', 'expected-value', 'machine-learning', 'probability']"
3874642,Solving a (fun!) coequalizer problem for $\mathrm{SL}_n(\mathbb{R})\rightarrow\mathrm{SL}_n(\mathbb{C})$ in $\mathbf{Grp}$,"First off, the problem posed below is mostly arbitrary; it's just for my own education. (And maybe for yours, as well.) It's fairly clear to me what the (co)equalizers of abelian groups in $\mathbf{Grp}$ are, but it's less clear what those mean for non-abelian groups. So, I came up with a problem that seems non-trivial and interesting. I'm trying to coequalize $f,g:\mathrm{SL}_n(\mathbb{R})\rightrightarrows\mathrm{SL}_n(\mathbb{C})$ , where $f(A)=A$ $g(A)=(A^*)^{-1}$ (Both purposely not surjective.) To solve this, we need to find ""the best"" $l:\mathrm{SL}_n(\mathbb{C})\rightarrow L$ . For now, I'll settle for any $L$ that isn't $\{0\}$ . The images of both $f$ and $g$ are $\mathrm{SL}_n(\mathbb{R})\subset\mathrm{SL}_n(\mathbb{C})$ , so to start with I'll just look at that part of the domain of $l$ . $l(A^*)=l(A^{-1})$ , based on $f$ and $g$ . (Again, just on $\mathrm{SL}_n(\mathbb{R})$ for now.) $l(AA^*)=l(A^*A)=e_L$ , following from the statement above, and $l$ being a homomorphism. Since $AA^*$ and $A^*A$ are positive-definite Hermitian (PDH), and PDH have Cholesky decompositions resembling $AA^*$ , we can more generally say that $l(B)=e_L$ when $B$ is PDH. (Extending $l$ to $\mathrm{SL}_n(\mathbb{C})$ .) This also means that $l(D)=e_L$ when $D$ is diagonal with positive entries. For any $A\in\mathrm{SL}_n(\mathbb{C})$ , we can create an SVD $A=U\Sigma V^*$ , with unitary $U$ and $V$ , and $U,\Sigma,V\in\mathrm{SL}_n(\mathbb{C})$ . Since $l(\Sigma)=e_L$ , $l(A)=l(UV^*)$ . ( $UV^*$ should be unique, since $A$ is of full rank.) If $A$ is unitary, it can be diagonalized as $A=VDV^*$ for unitary $V$ and diagonal $D$ . Importantly , $D$ should only be in the kernel of $l$ if it only has positive (real) values, which is only true for $I$ . So it seems like $L$ is (at most) isomorphic to $\mathrm{SU}(n)$ , with $l(A)$ taking $A$ to an equivalence class based on its rotation action after removing any distortion it makes. Does that sound accurate and/or reasonable? (For example, maybe a matrix with a non-real determinant can sneak in when removing $\Sigma$ , thereby breaking $\mathrm{SL}_n(\mathbb{C})$ .) I spent several hours going through this, and I changed my conclusion about 5 times. The last few times were while proofreading. Whether or not my answer above is correct, I'd appreciate any pointers regarding shortcuts I could have taken, etc.","['limits-colimits', 'category-theory', 'linear-algebra', 'linear-transformations', 'group-theory']"
3874669,Hermite polynomial relations,"How one can prove the relation on Hermite polynomial given as $$\int_{-\infty}^\infty H_n\left(x+\frac{x_0}{2}\right)e^{^{-\frac{x^2}{2}}}dx=\sqrt{\pi}x_0^n$$ I also didn't understand the meaning of $\displaystyle H_n\left(x+\frac{x_0}{2}\right)$ , what does that signify, I know by the way that $H_n(x)$ represents the Hermite polynomial but what is the meaning of $\displaystyle H_n\left(x+\frac{x_0}{2}\right)$ ? Please explain this alongwith. Thanks .","['integration', 'hermite-polynomials', 'ordinary-differential-equations']"
3874778,Regarding various types of topology,"I will just start reading topology but I am confused regarding: Point topology-Stephen Gaal Algebraic topology- Hatcher Combinatorial and Differential topology-Prasalov Topology - Munkres I am quite confused are they different topics?
Note:I searched but but I cannot understand the difference. And which topic should I start first.","['general-topology', 'soft-question', 'book-recommendation']"
3874872,A question from Hormander's book on operators with smooth kernel,"I'm having a bit of problem filling in the gap for Theorem 5.2.6. in Hormander's first volume on linear PDE. It says that if $\kappa \in \mathcal{C}^{\infty}(X_1 \times X_2)$ is a smooth function then there exists a continuous operator $K: \mathcal{E}'(X_2) \rightarrow \mathcal{C}^{\infty}(X_1)$ with Schwartz kernel $\kappa$ . Now this operator is constructed by the formula $Ku(x_1) = \langle u, \kappa(x_1, \cdot) \rangle$ and it is clear for me that this is a map $ \mathcal{E}'(X_2) \rightarrow \mathcal{C}^{\infty}(X_1)$ . But I cannot get the continuity statement. From my understanding here I need to show that if $u_j \rightarrow u$ in $\mathcal{E}'(X_2)$ with the weak star topology then we must have $$\sup_{x_1 \in K} |\langle u - u_j, \partial^{\alpha}_{x_1} \kappa(x_1,\cdot) \rangle| \rightarrow 0$$ for all compact subsets $K \subset X_1$ and multiindex $\alpha$ . I know that perhaps one possible way to get this is to show that $\{ \partial^{\alpha}_{x_1}\kappa(x_1,\cdot) \ / \ x_1 \in K \} $ is compact, then a theorem from Reed and Simons says that the uniform convergence holds. However Hormander seems to hint that this is elementary. So I must be missing something quiet easy here. I would appreciate if somebody can point out how do I obtain this. Many thanks!","['linear-pde', 'functional-analysis', 'distribution-theory']"
3874947,Imaginary asymptotics for the digamma function,"I often see asymptotics and precise expansion for the gamma $\Gamma$ or the digamma $\psi$ function $\psi$ when the argument goes to $+\infty$ , in particular when it stays real (or in a given angle sector towards $+\infty$ ). I would like to know the precise asymptotics along the imaginary axis, i.e. asymptotics for $$\psi(x_0 + iy) = \frac{\Gamma'}{\Gamma}(x_0 + iy)$$ when $x_0$ is fixed, say positive, and $y$ goes to $\pm \infty$ . Do we know such an expansion, with explicit dependencies in $x_0$ ? Typically, the Stirling formula $$\Gamma(z) \sim \sqrt{\frac{2\pi}{z}} \left(\frac{z}{e} \right)^z$$ is valid for all complex number in the angle sector $|\mathrm{arg}(z) - \pi| \geq \delta$ for any $\delta > 0$ . This is unfortunately not enough to obtain information on the derivative $\Gamma'$ , and therefore on $\psi$ . Is there a similar formula for the digamma function?","['complex-analysis', 'taylor-expansion', 'asymptotics', 'gamma-function']"
3874993,Solve the differential equation $y'=\frac{y+1}{y-1}$ by separating variables,"$y'=\frac{y+1}{y-1}$ Solve the differential equation by separating variables and give the solution in an implicit form. $$\frac{dy}{dx} = \frac{y+1}{y-1}$$ $$\Leftrightarrow dy = \left (\frac{y+1}{y-1} \right )dx$$ $$\Leftrightarrow \left (\frac{y-1}{y+1}\right) dy = dx$$ $$\Leftrightarrow \int{\frac{y-1}{y+1} dy} = \int{dx}$$ $$\Leftrightarrow \int{\frac{y+1-2}{y+1} dy} = \int{dx}$$ $$\Leftrightarrow \int{\frac{y+1}{y+1}-\frac{2}{y+1} dy} = \int{dx}$$ $$\Leftrightarrow y - 2\ln|y+1| = x + C $$ This is what I got right now. I literally don't know if this is correct and especially whether this is also good in the implicit form.
Normally you can always check the solution by filling it in, but I wouldn't know how to do that in this case.","['integration', 'calculus', 'derivatives', 'ordinary-differential-equations']"
3875004,Generalize the binomial equation,"I am wondering if generalizing the case $p=2$ we have for $1<p<\infty$ that there is a constant $c(p,l)$ such that for all $(z_j)$ with $z_j \in \mathbb C$ that $$\left\lvert \left\lvert \sum_{j=1}^l z_j \right\rvert^p - \sum_{j=1}^l \left\lvert z_j \right\rvert^p \right\rvert \le c(p,l) \sum_{j \neq k} \vert z_j \vert \vert z_k \vert^{p-1}$$ in the case $p=2$ I can clearly see it holds, but here, I don't know.","['inequality', 'analysis', 'real-analysis']"
3875008,Difference between the properties of differentiation in $\mathbb{C}$ and $\mathbb{R}^2$,"I'm taking a course on Complex Calculus and I've been provided with the following definition for the derivative of a function: Definition : Let $f$ be a function whose domain contains a neighborhood of a point $z_0$ . The derivative of $f$ at $z_0$ is the limit $$f'(z_0) = \lim_{z\to z_0} \frac{f(z) - f(z_0)}{z - z_0}$$ But for a course on Multivariable Calculus, I was provided with the following definition (from Munkres' Analysis on Manifolds): Definition : Let $A \subset \mathbb{R}^m$ , let $f: A\to \mathbb{R}^n$ . Suppose $A$ contains a neighborhood of a . We say $f$ is differentiable at a if there is an n by m matrix B such that $$\frac{f(\mathbf{a + h})-f(\mathbf{a}) - B\cdot \mathbf{h}}{|\mathbf{h}|}\ \text{as}\ \mathbf{h}\to 0.$$ My problem here is that my instructor stated that any function $f:D\to \mathbb{C},\ D\subset \mathbb{C}$ that has continuous partial derivatives which satisfy the Cauchy-Riemann equations at some $z\in D$ is also differentiable at $z$ . But this was not the case for a function $g:D\to \mathbb{R}^2,\ D\in \mathbb{R}^2$ using the second definition for a derivative. To my knowledge, $\mathbb{C} \cong \mathbb{R}^2$ when viewed as $\mathbb{R}$ -modules, so I would expect the differential operator to behave the same as well. Why is it the case that these two definitions do not agree? Are these two definitions describing different things? Is any definition encapsulating the other? (If that makes any sense) Edit : Initially, I stated that my instructor specified that every complex function continuous at a point is also differentiable at that point. This was an error on my part, as I probably mixed up the statement. I rephrased my question, though the answers are still satisfactory and explanatory of the differences between these two definitions.","['complex-analysis', 'multivariable-calculus', 'derivatives']"
3875041,Probability of both members of a committee being girls,"A small committee of two is formed from a group of 4 boys and 8 girls. If at least one of the members of
the committee is a girl, what is the probability that both members are girls? So far I have tried:
Multiplying the original number of original girls ( $\frac{8}{12}$ ) by one minus hence it is given to us one is already part of the committee ( $\frac{7}{11}$ ) to no avail. Also did some other things similar to what i just said before that I don't remember since I was trying everything that I could conjure up.How do i tackle this problem?.","['conditional-probability', 'probability']"
3875109,Find limits of 2 sequences,"I have 2 sequences I have to find the limit of. But I'm not sure if I've proven this correct enough or if there's a better way to do so. (a) $\lim_{n\to \infty}$ $\frac{n^2+(-1)^n}{5n^2+3n+1}$ (b) $\lim_{n\to \infty}$ $\frac{a^n+2b^n}{3a^n+5b^n}$ , for $a,b \gt 0$ So for (a) I have that since $(-1)^n$ is bounded, it is irrelevant for large n. Also, (I don't know the correct term for this) since $n^2$ is dominant compared to $n$ , for very large $n$ , the limit is basically $\lim_{n\to \infty}$ $\frac{n^2}{5n^2}$ $= 1/5$ . I don't know how to write this correctly since this seems quite informal and not proven. for (b) I'm not quite sure. First I tried factoring something out but I did not get very far. Now I used $c$ $=$ $a^n+2b^n$ . This results in $\frac{c}{3c-b^n}$ . But that's where I'm stuck. How do i proceed from this?","['limits', 'sequences-and-series']"
3875136,"Proof of continuity and the limit $ f(x,y) = \frac{1}{y^2-x^2}\int_x^y\ln(e+e^t)dt$","I have to prove the continuity and find the limit $(x,y)\to(1,1)$ of $f:D\to R$ , where $$D=\{(x,y)\in R^2:|x|\neq|y|\}$$ and $$ f(x,y) = \frac{1}{y^2-x^2}\int_x^y\ln(e+e^t)dt.$$ I don't know whether my way of thinking is correct and I am a little stuck on the limit. Continuity I have had a few thoughts: The multiplication of continuous functions is continuous, thus if $\int g(x)$ is differentiable and the fraction is continuous, it should be continuous. To be integrable, the integral of an absolute value of the function (the derivative) should have a finite limit. My steps $$ f(x,y) = \frac{1}{y+x}\cdot\frac{1}{y-x}\int_x^y\ln(e+e^t)dt$$ Since we can bound the integral by: $$\int_x^y\ln(e+e^t)dt<\int_x^y\ln(e^t)dt=\int_x^ytdt$$ Which is convergent on a given interval, thus $g(x)$ is integrable and continuous Additionally, from the intermediate value theorem we have: $$\frac{1}{y-x}\int_x^y\ln(e+e^t)dt=\ln(e+e^c)~~~~\text{for some}~~c\in(x,y)$$ Therefore: $$ f(x,y)=\frac{1}{y+x}\cdot\ln(e+e^c) $$ Since $|x|\neq|y|$ the function does not take zero value in the denominator, thus this multiplication of functions is continuous. I am still learning and I would really appreciate if you would point out my mistakes or present a more accurate solution . Limit I have also tried spheric coordinates, but I can't see nothing helpful in this case: $$ \frac{1}{y^2-x^2} = \frac{1}{r^2(\sin^2\theta-\cos^2\theta )} $$ One of my thoughts was that I would have to find the limit of the integral and of the fraction or calculate the integral and combine with the fraction, however I decided to test the fraction first. $$ h(x,y)_{(x,y)\to(1,1)} = \frac{1}{y^2-x^2}$$ I check two sequences: $a_n=(1, \frac{1}{n})$ and $ b_n=(\frac{1}{n},1)$ with $n\to\infty$ and get: $$ \lim_{n\to -\infty} a_n=\lim_{n\to \infty}\frac{n^2}{n^2-1}= \infty\\ \lim_{n\to \infty} b_n=\lim_{n\to \infty}\frac{n^2}{1-n^2} = -\infty~~~~\text{for big}~~n$$ For this function the limit does not exist since we can show two sequences with different limits. Again, I am not sure whether my solution presents the correct way of solving. I would like to ask you for some guidance.","['integration', 'real-analysis', 'continuity', 'multivariable-calculus', 'limits']"
3875155,"Unitisation of an algebra: Is the norm $\text{max}\lbrace\|a\|_A,|\lambda|\rbrace$ submultiplicative?","I feel as if I may have missed something really obvious here. It says in my notes that if $A$ is an algebra (over $\mathbb{C}$ ), then its unitisation $A^1:=A\times\mathbb{C}$ is a unital algebra with respect to the multiplication $$(a,\lambda)(b,\mu):=(ab+\lambda b+\mu a,\lambda\mu)$$ and that if $A$ is a normed algebra (with submultiplicative norm $\|\cdot\|_A$ ), this becomes a normed algebra with respect to the norm $$\|(a,\lambda)\|_\infty:=\text{max}\lbrace\|a\|_A,|\lambda|\rbrace$$ To prove that $\|\cdot\|_\infty$ is submultiplicative, we have to prove that $$\text{max}\lbrace\|ab+\lambda b+\mu a\|_A,|\lambda|\,|\mu|\rbrace\leq\text{max}\lbrace\|a\|_A,|\lambda|\rbrace\text{max}\lbrace\|b\|_A,|\mu|\rbrace.$$ If $\|ab+\lambda b+\mu a\|_A\leq|\lambda|\,|\mu|$ , then this is obvious, but what about if $\|ab+\lambda b+\mu a\|_A>|\lambda|\,|\mu|$ ? EDIT: Just to clarify, I'm not 100% sure if this result is true or not, it just seems to implicitly suggest that it is true in notes I am reading.","['banach-algebras', 'normed-spaces', 'functional-analysis', 'algebras']"
3875161,"The proof of $\frac{f(a)+f(b)}{2}\le\frac{1}{b-a}\int^b_af(x)dx$ provided that $f''(x)<0,x \in [a,b]$ and $f\in \mathcal C^2[a,b]$","This question is geometrically obvious, but I got a little trouble when proving it. Let's consider the Taylor expansion of $F(x)=\int^x_af(x)\mathrm dx$ at $a$ and $b$ and subsitute $b,a$ into it respectively. $$\int_a^bf(x)\mathrm dx=F_a(b)=f(a)(b-a)+f'(a)(b-a)^2/2+f''(\xi_1)(b-a)^3/6\\0=F_b(a)=\int^b_af(x)\mathrm dx+f(b)(a-b)+f'(b)(a-b)^2/2+f''(\xi_2)(a-b)^3/6$$ And subtract the second from the first $$\int_a^bf(x)\mathrm dx=F_a(b)-F_b(a)=-\int_a^bf(x)\mathrm dx+(b-a)(f(a)-f(b))+(f'(a)-f'(b))(b-a)^2/2+(f''(\xi_1)+f''(\xi_2))(b-a)^3/6$$ And divide both side by 2(b-a) and subsitute $f'(a)-f'(b)=-f(\xi_3)(b-a)$ into it $$\frac{1}{b-a}\int_a^bf(x)\mathrm dx=\frac{f(a)-f(b)}2-f''(\xi_3)(b-a)^2/4+(f''(\xi_1)+f''(\xi_2))(b-a)^2/12$$ since $f''(\xi_3)<0$ we have $$\frac{1}{b-a}\int_a^bf(x)\mathrm dx\ge\frac{f(a)-f(b)}2+(f''(\xi_1)+f''(\xi_2))(b-a)^2/12$$ I guess my approximation is too rough so I get an additional term $(f''(\xi_1)+f''(\xi_2))(b-a)^2/12$ .
Maybe the glitch is that I use $f''(x)<0$ just at one point i.e. $\xi_3$ but actually it holds at every point in $[a,b]$ which is a decisive condition.","['integration', 'numerical-methods', 'derivatives']"
3875249,Formula for cross product,"The formula for the cross product of two vectors in $R^3$ , $\vec{a} = (a_1, a_2, a_3)$ and $\vec{b} = (b_1, b_2, b_3)$ is $$\det\begin{pmatrix}\mathbf{i} & \mathbf{j} & \mathbf{k}\\\ a_1 & a_2 & a_3\\\ b_1 & b_2 & b_3\end{pmatrix}$$ I know that in general for three 3D vectors the determinant represents the volume of the parallelepiped. But how is it valid to put (basis) vectors $\mathbf{i}, \mathbf{j}, \mathbf{k}$ into a vector, and what graphical/intuitive significance does it have? What would have been the initial motivation of this formula? Note: I already read through similar questions and corresponding answers but was not satisfied. So please do not downvote this question and if possible give me some insight.","['determinant', 'cross-product', 'vector-spaces', 'matrices', 'linear-algebra']"
3875293,The Variety of 3-dimensional Real Lie Algebra Structures,"Let $C^{2}(\mathbb{R}^{3};\mathbb{R}^{3})$ be denote the vector space of all skew-symmetric bilinear maps from $\mathbb{R}^{3}\times\mathbb{R}^{3}$ to $\mathbb{R}^{3}$ and let $\operatorname{Lie}(\mathbb{R}^{3})$ be the algebraic subset of $C^{2}(\mathbb{R}^{3};\mathbb{R}^{3})$ consisting of all skew-symmetric bilinear maps $\mu \in C^{2}(\mathbb{R}^{3};\mathbb{R}^{3})$ such that $(\mathbb{R}^3,\mu)$ is a real Lie algebra . The general linear group $\operatorname{GL}(3, \mathbb{R})$ acts by changing of basis on $\operatorname{Lie}(\mathbb{R}^{3})$ : if $g\in \operatorname{GL}(3, \mathbb{R})$ and $\mu \in\operatorname{Lie}(\mathbb{R}^{3})$ , $g\cdot \mu(x,y) = g\mu(g^{-1}x,g^{-1}y)$ , for any $x,y \in \mathbb{R}^{3}$ . I know that the $\operatorname{GL}(3, \mathbb{R})$ -orbit of the usual cross product on $\mathbb{R}^3$ is a open set in $\operatorname{Lie}(\mathbb{R}^{3})$ with respect to the subspace topology inherited from the Euclidean topology of $C^{2}(\mathbb{R}^{3};\mathbb{R}^{3})$ ; for instance, by using the Killing form of $\mathfrak{so}(3,\mathbb{R}) = (\mathbb{R}^3,\times)$ . Recall that the cross product $\times$ is determined by $e_1 \times e_2 = e_3$ , $e_2 \times e_3 = e_1$ , $e_3 \times e_1 = e_2$ and $e_1\times e_1= e_2\times e_2=e_3\times e_3=0$ . I would like to learn/know if the above $\operatorname{GL}(3, \mathbb{R})$ -orbit is also a Zariski open set of the algebraic set $\operatorname{Lie}(\mathbb{R}^{3})$ , and in such case, what are the polynomials that vanish on the Zariski closure of such orbit in $\operatorname{Lie}(\mathbb{R}^{3})$ ?","['group-actions', 'algebraic-geometry', 'lie-algebras', 'reference-request']"
3875313,"Groups with ""unique"" elements of high order","A group $G$ can have an element $g$ for which every automorphism of the group fixes $g$ . Obviously, the identity is one such element, and one can easily find order-2 examples: the unique order-2 element in $C_{2n}$ , or $-1$ in the quaternion group. My question is whether one can find a group $G$ with an element $g$ of order at least 3 which is fixed by every automorphism of $G$ . One might suspect that there is always an automorphism taking $g$ to $g^{-1}$ , presenting an obvious obstacle to the high-order case, but this is false . For example, take $G$ to be the unique nonabelian group of order $21$ , realized as a semidirect product of $C_7$ and $C_3$ . Then any of the $14$ elements of order $3$ cannot be sent to their inverses. However, none of these elements are fixed by every automorphism; the $14$ elements fall into two conjugacy classes of size $7$ . Restricting our attention to those automorphisms given by conjugation, we see that $g\in Z(G)$ . However, I haven't found a way to strengthen this restriction into a proof of impossibility. Edit: This post originally contained the sentence Intuitively, one can think of such an element as ""unique"" in the sense that it has group-theoretic properties not shared by any other element. but I have moved it to the bottom to avoid confusion. By this, I just mean that automorphisms exchange elements that in some sense ""serve the same role"" as each other in the group; an element which is fixed by every automorphism can be thought of as having no such counterparts. I do not mean to search for a group in which only one element has the property of being fixed by every automorphism.",['group-theory']
3875343,Solve the inequality $x^4-3x^2+5\ge0$,"Solve $$\sqrt{x^4-3x^2+5}+\sqrt{x^4-3x^2+12}=7.$$ $D_x:\begin{cases}x^4-3x^2+5\ge0 \\x^4-3x^2+12\ge0\end{cases}.$ We can see that $x^4-3x^2+12=(x^4-3x^2+5)+7,$ so if $x^4-3x^2+5$ is non-negative, $x^4-3x^2+12$ is also non-negative (even positive). So I am trying to solve $$x^4-3x^2+5\ge0.$$ Let $x^2=y,y\ge0.$ Now we have $$y^2-3y+5\ge0; D=9-4\times5<0,a=1>0$$ so the function $f(y)=y^2-3y+5>0$ for all $y$ . I don't know what to do next. The solution of the inequality is indeed $x\in(-\infty;+\infty),$ but I have the restriction $y\ge0?$",['algebra-precalculus']
3875373,Finding intervals for a two variable function,"I had a similar question about 1 year ago, and I find more properties about my question. Now I want to ask it again and so thankful for new  solutions.
Suppose $$ H(x,y)=x e^{\pi y}-\frac{x(\pi-x)}{\pi}e^{xy}+(\pi-x)(\frac{\pi^2}{12}-1)e^{xy}-\frac{\pi^2}{12}x \sinh(\pi y)\\ +(1-\frac{\pi^2}{12})(\pi-x)-x$$ . I guess  there exist $\alpha\geq 0$ and $\beta\geq 0$ such that $$ H(x,y)\geq 0, \quad for \quad (x,y) \in [\alpha,\pi)\times [\beta, \infty).$$ I am looking for the best choices for $\alpha$ and $\beta$ .
With several calculations and plot graphs, I guess that if $\alpha=0$ then $0.4<\beta<0.5$ . Also we have, $$1)\,H(0,y)=0,
\qquad 2)\,H(\pi,y)= 0,
\qquad 3)H(x,0)=\frac{1}{\pi}x^2-1.$$ Furthermore $H(x,0)\geq 0$ if $x\geq \sqrt{\pi}$ . I appreciate any solutions, comments and hints.","['multivariable-calculus', 'calculus', 'functions', 'inequality']"
3875416,Dimension of a Lie group associated to a finite group,"Let $G$ be a finite group.  Consider—I think I've got my terminology right—the full subcategory $\mathcal C$ of the coslice category $G \downarrow \operatorname{Lie}$ of Lie groups under $G$ whose objects are (morphisms into) connected Lie groups.  In other words, consider the category whose objects are morphisms $G \to H$ with $H$ a connected Lie group, and whose morphisms are triangles under $G$ in the obvious sense.  (I changed 'over' to 'under', per @QiaochuYuan's comment .)  There is nothing inherently category theoretic about my questions, but the language seems to be well suited to it. (1) Does $\mathcal C$ have an initial object? (2) Even if the answer to (1) is ‘no’, there is a well defined dimension $$d(G) = \min \{\dim(H) \mathrel: \text{$H$ a connected Lie group and $G$ embeds in $H$}\}.$$ (Note that the set is non-empty; $G$ embeds in an appropriate general linear group via its regular representation.)  What purely group theoretic information about $G$ is recorded by $d(G)$ ? (3) What changes in (1) or (2) if we replace topologically connected Lie groups by Zariski connected linear algebraic groups over a fixed (not necessarily characteristic 0, not necessarily algebraically closed) field $k$ (and so differentiable maps by algebraic maps)?","['group-theory', 'finite-groups', 'lie-groups']"
3875444,"Evaluate $ \int_0^{\infty} \int_0^{\infty} \frac{1}{(1+x)(1+y)(x+y)}\,dy dx$","Evaluate $ \int_0^{\infty} \int_0^{\infty} \frac{1}{(1+x)(1+y)(x+y)}\,dy dx$ We can calculate $\int \frac{1}{(1+x)(x+y)} \,dx= \frac{\ln|x+1| - \ln|x+y|}{y-1}$ but that doesn't make it quite easier. Is there a trick to compute this easier?","['integration', 'multivariable-calculus', 'calculus', 'improper-integrals']"
3875482,"A certain country has four regions: North, East, South, and West. ...","A certain country has four regions: North, East, South, and West. The population of these regions are 3 million, 4 million, 5 million, and 8 million, respectively. There are 4 cities in the North, 3 in the East, 2 in the South, and there is only 1 city in the West. Each person in the country lives in exactly one of these cities. a) What is the average size of a city in a region? (This is the arithmetic mean of the population of the cities, and is also the expected value of the population of a city chosen uniformly at random.) b) A region of the country is chosen uniformly at random, and then a city within that region is chosen uniformly at random. What is the expected population size of this randomly chosen city? For part A, letting X = the size of the city, I got E(X) = 1/10[3,000,000 + 4,000,000 + 5,000,000 + 8,000,000] = 2,000,000 For part B, 1/4[750,000 + 4/3(1,000,000) + 2,500,000 + 8,000,000] = about 3145833.33 My answers seem off but I am not sure where exactly in my work I am off. Any help would be greatly appreciated!",['probability']
3875559,"lebesgue measure in $[0,1]\setminus \mathbb{Q}$","I found this in an old exam and I am not sure if what I've done is correct. Let $E=[0,1]\setminus\mathbb{Q}$ find a closed set $F$ such that the Lebesgue measure is at least 3/4 Should I suppose that closed here means closed in the subspace topology in $E$ or it is in the usual topology of $\mathbb{R}$ ? here what I've done: Consider $\lbrace q_n\rbrace_{n\geq 1}$ the rational numbers in $[0,1]$ and take $\varepsilon=\frac{1}{4}$ . For each $q_n$ we can consider the open interval $I_n$ whose Lebesgue measure is given by $m(I_n)=\frac{\varepsilon}{2^n}$ .  Now, let $\tilde{I_n}=I_n \cap E$ , it is clear that $m(\tilde{I_n})\leq \frac{\varepsilon}{2^n}$ and consider $U = \bigcup_{n\geq 1}\tilde{I_n}\subseteq E$ .Now, $U$ is open in $E$ with the subspace topology, and since $m(U)\leq \varepsilon$ we are done if we take $F=U^c$ . What do you think about it?","['general-topology', 'lebesgue-measure', 'measure-theory']"
3875621,"In what interval must the percentage of black squares fall in order to capture 95%, 99%, and 99.9% of possible QR codes?","Generally speaking most QR codes look as though they have about 50% of their cells black. If a putative QR code had 90% or 10% of its cells black, we would reasonably conclude that it didn't look like a QR code. Centred on the mean, which may possibly not be 50%, in what interval must the percentage of black squares fall in order to capture 95%, 99%, and 99.9% of possible QR codes ? I intend ""possible"" to mean ""appearing for normal purposes"", which I realise is open-ended. If it is necessary to assume a certain size, please use version 1, which defines a 21 x 21 cell array.",['statistics']
3875643,How to verify that this implicit equation is a solution to a nonlinear ordinary differential equation.,"I am studying the nonlinear ordinary differential equation $$\frac{d^2y}{dx^2}=\frac{1}{y}-\frac{x}{y^2}\frac{dy}{dx}$$ I have entered this equation into two different math software packages, and they produce different answers. software 1: $$0=c_2-\ln(x)-\frac{1}{2}\ln\left(-\frac{c_1y}{x}-\frac{y^2}{x^2}+1\right)-\frac{c_1}{\sqrt{-c_1^2+4}}\tan^{-1}\left(\frac{c_1+\frac{2y}{x}}{\sqrt{-c_1^2+4}}\right)$$ software 2: $$0=-c_2-\ln(x)-\frac{c_1}{\sqrt{c_1^2+4}}\tanh^{-1}\left(\frac{c_1x+2y}{x\sqrt{c_1^2+4}}\right)-\frac{1}{2}\ln\left(\frac{c_1xy-x^2+y^2}{x^2}\right)$$ I have not attempted to verify the solution from software 1 yet, but have done some work on software 2. I first used software 2 to try to solve for y, to substitute the expression for y directly into the ordinary differential equation.  The result was the following: I believe that this output is ambiguous, since there are essentially two equations that are supposed to be equated to zero I am not sure if it is possible to solve for y, and hence to check the validity of this solution using this method. I then did some reading on the internet, and it was suggested to, in this case, take the second implicit derivative with respect to x, then simplify. I tried to do this with math software 2, and the result was, after simplifying: $$\frac{d^2y}{dx^2}=\frac{c_1xy-x^2+y^2}{y^3}$$ I did some hand calculations, and it seems that software 2 simplifies the result before calculating the next derivative, even without using the simplify command. Considering this, I used the software to take the first derivative implicitly, then wrote out the equation in full, put that equation into a different form than the software output, and calculated the second derivative implicitly by hand, treating derivatives as functions of x for operations such as the product rule. The equation I calculated did not match the original differential equation. Software 2 has a function called odetest, which is supposed to verify that a function is a solution to an ordinary differential equation.  If you use odetest on this solution, the returned result is zero, implying that the function is a solution. The problem is that odetest does not show steps.  I contacted the company and asked to see the steps for this calculation, but they would not provide the steps. Are there any other ways to verify implicit solutions to an ordinary differential equation?","['implicit-differentiation', 'derivatives', 'ordinary-differential-equations']"
3875674,Frenet-Serret formula: why is $T$'s magnitude unitary?,Why is $T$ 's tangent vector magnitude unitary? $$T=\frac{dr}{ds}$$,"['multivariable-calculus', 'calculus', 'frenet-frame', 'differential-geometry']"
3875794,Textbook advice- Dynamical Systems and Differential Equations,"I am currently an undergrad math major taking a gap year (because my university is entirely online this semester).  For this year, I have signed up for a ""Directed Reading Program"" with a graduate student whose specialty involves dynamical systems.  For this program, I am supposed to read through a textbook that we can discuss.  Two of her suggestions were Nonlinear Dynamics and Chaos by Steven H. Strogatz and Differential Equations, Dynamical Systems, and an Introduction to Chaos by Hirsch, Smale, and Devaney. As I took a look at those books, I realized an additional reason why reading a book such as those could be useful: although I took a differential equations course at my local community college when I was in high school, I don't remember them all that well.  My university's math department is very theory-oriented, so I may never have the opportunity to take a DiffEQ course as an undergrad, though, as a math major who may want to go into something more applied, I feel as though a high level of comfort with differential equations would be nice to have.  Looking through the two textbooks online, neither appears to cover Laplace transforms, which I remember to have been an entire unit in my community-college course.  Because of this, I am having doubts about the efficacy of the two books with respect to giving me said comfort.  However, the books seem to be fantastic with respect to gaining a deeper understanding of the material, so I am not trying to criticize. Two questions: Between Strogatz and Hirsch/Smale/Devaney, which would you recommend? In light of the above (the lack of coverage of topics such as Laplace transforms), do you think I ought to, in addition to one of those two books, spend time with Ordinary Differential Equations by Tenenbaum and Pollard (which I got for Christmas or something awhile back but haven't spent much time with)?","['dynamical-systems', 'ordinary-differential-equations', 'reference-request']"
3875797,Question about differential notation ($\partial$ and $d$),"My question is if, given $f(x)$ , it's correct to write: $$\frac{\partial f(x)}{\partial x}$$ instead of $$\frac{d f(x)}{d x}.$$ Is it incorrect to use $\partial$ in one-variable expressions? Or it just doesn't really matter.","['notation', 'derivatives', 'partial-derivative', 'real-analysis']"
3875906,Controversial probability calculation regarding Thai lotto incident,"At 1st September 2020, the number ""999997"" was picked for the first prize in Thailand's government lotto. The consecutive repeating of the number ""9"" caused extensive controversial discussion whether the lotto machine was working properly or not, some even claim that this incident proved that the government was cheating. Note for the lotto drawing method. A six-digit number will be randomly picked from the set of 000000, ..., 999999 for the 1st prize by using 6 staffs each draw a number 0 - 9 from their corresponding machines. To simplify the problem, I will consider the 1st prize number ""999999"" instead of ""999997"" in this question. Commonly, most people know that every number has equal probability of $1/1000000$ . Let me define the mathematical statement for this. Statement 1: Randomly drawing a number $n$ from the set of six-digit numbers $000000, ..., 999999$ , the probability of $n$ being any specific number in the set is $1/1000000$ Now, the problem arises when someone proposes the following statement. Statement 2: Let $A$ be a set {000000, 111111, 222222, ..., 999999}, The probability of $n$ being a member of $A$ is $10/1000000$ . On one side, people use Statement 1 to explain that the number ""999999"" being drawn is as usual as any familiar number such as ""326648"", ""863439"", ... On the other side, people use Statement 2 to claim that the number ""999999"" being drawn is ""unusual"" as it has only $10/1000000$ probability to draw this kind of number. I got some feeling that latter claim using Statement 2 has something wrong because if I let the set $A$ being a set of my any desired 10 numbers such as {123456, 443253, 857342, ...}, I could claim that any number is unusual. But I cannot explain it clearly enough to convince the people who believe this claim. Please help me see if there is some mathematical explanation behind this conflict, which can explain why the claim using Statement 2 is invalid and why people find it difficult to figure it out spontaneously.","['statistics', 'lotteries', 'probability']"
3875932,"Distribution of determinants of $n\times n$ matrices with entries in $\{0,1,\ldots,q-1\}$","Consider the set $M(n,q)$ of $n\times n$ matrices with entries in $\{0,1,\ldots,q-1\}$ , where $q$ is a prime power. What can be said about the distribution of the determinant of matrices in $M(n,q)$ ? (A 'heuristic' statement of the problem: taking $\{0,1,\ldots,q-1\}$ as a basis for $F=\mathbb{Z}_q$ , what do the determinants of matrices over $F$ look like if you don't mod out $q$ ?) Obviously $|M(n,q)| = q^{n^2}$ . Since $|GL_n(\mathbb{F}_q)| = \prod_{k=0}^{n-1} q^n-q^k$ , in $\mathbb{F}_q$ we get a clean answer for how many are divisible by $q$ : the values are equally distributed (modulo $q$ , there are $\frac{1}{q-1}\prod_{k=0}^{n-1} q^n-q^k$ matrices with determinant $j$ , $1\le j\le q-1$ ). But if we do not look mod $q$ , as it were, the question becomes substantially more difficult; to be frank, I'm not sure where to start or if there are any clear patterns. Information about the limiting behavior or any upper bounds on the magnitude of the determinant would be welcome as well. I computed the distributions for several values of $n=2,3$ and $2\le q\le 5$ ; the plot labels are of the form $\{n,q\}$ . As expected, determinant zero is the most common option and a determinant of $a$ is just as likely as a determinant of $-a$ . Past that, I admit I'm a little out of my league, but it seems like an interesting problem.","['determinant', 'probability-distributions', 'matrices', 'linear-algebra', 'probability']"
3875943,Which step in deriving the derivative of $sec(x)$ is wrong?,I can't see any errors on the steps. But Step 3 makes me doubt my answer.,"['calculus', 'derivatives']"
3875977,Finding best players in a tournament with a probabilistic comparison function,"I am currently facing the following problem in my research and I have no clue how to tackle this kind of question. The problem Imagine you have a tournament with $n$ players $P=\{p_1,...,p_n\}$ . My goal is to determine one of the best players in my tournament. I do have a comparison function $f: P x P\to \{0,1\}$ that can tell me which of two given players is better, i.e. $f(p_1,p_2)=1$ iff player two is better than player one and $f(p_1,p_2)=0$ iff player one is better than player two. You can think of $f$ as the $<$ relation. The kicker is that my comparison function $f$ has an error, meaning that it will give me the correct result of my comparison with a probability $p>0.5$ . Calculating $f$ will take some time and thus I want to find a good player for my tournament with the least amount of queries. My current approach is to compare all players with each other which gives me a total amount of $b \in O(n^2)$ comparison calls. I then chose the player $p_i$ , which ""won"" the most comparisons. Edit: Please be aware that my comparison function will give me the same result for a call $f(p_i,p_j)$ no matter how often I call it. So the probability that the result is correct is $p$ , but the function itself is deterministic. My example below is a bit misleading. However, each comparison call is only done once so this won't be a problem. Key questions What is the probability that the chosen player is the best player? What is the probability that the chosen player is in the top k percent? My thoughts I think that question one might be easier to calculate as my best player will win all comparisons if $p=1$ and I can deduce the probability that $k$ comparisons were correct. However, I am stuck at the point at which I have to calculate the probability that it in fact is the player that ""won"" the most comparisons as others might be evaluated incorrectly. My dream is to get a formula that allows me to calculate the desired probabilities for different $p,n$ , and budget $b$ . Simulation I wrote a small simulation in Python which revealed some interesting facts about the influence of $p$ . In my example, the tournament players are represented as numbers $0,...,63$ . The function $f$ is the standard $<$ relation with a given probability. In the plot below I have plotted the mean position (y-axis) that was selected as the best individual for different $p$ (x-axis). You can find the source code below. import random
import numpy as np
from itertools import combinations
from tqdm import tqdm
import matplotlib.pyplot as plt

x, y = [], []

n = 64 # How many players
nums = np.arange(n).tolist() # Player strengths
count = 1000 # The amount of tests (O(n^2)) combinations that should be made

for p in tqdm(np.arange(0, 1, 0.01)):
    x.append(p)

    def compare(a, b):
        r = random.random()
        if r <= p:
            return a < b
        else:
            return a >= b

    def tournament():
        scores = [0] * n
        for a, b in combinations(nums, 2):
            result = compare(a, b)
            if result:
                scores[b] += 1
            else:
                scores[a] += 1

        best = max(nums, key=lambda x: scores[x])
        return best

    vals = []

    for _ in range(count):
        vals.append(tournament())

    y.append(np.mean(vals))

plt.plot(x, y)

plt.show()","['discrete-mathematics', 'stochastic-processes', 'combinatorics', 'probability']"
3876098,Is there a nontrivial homomorphism $\mathbb{Q} \to SL_n(\mathbb{Z})$?,"I am curious about whether there is a nontrivial group homomorphism $\mathbb{Q} \to SL_n(\mathbb{Z})$ for some $n$ . It's not hard to find such a homomorphism $\mathbb{Q} \to SL_2(\mathbb{Q})$ ; we can take the map given by $x \mapsto \left(\begin{smallmatrix} 1 & x \\ 0 & 1 \end{smallmatrix}\right)$ , but I don't see any obvious map into $SL_n(\mathbb{Z})$ . Another, weaker, question of interest is whether some $SL_n(\mathbb{Z})$ has an element $A \neq I$ for which $A$ has a $k$ -th root in $SL_n(\mathbb{Z})$ for every $k$ . In general, for a group $G$ , the condition that $G$ has an element with $k$ -th roots for every $k$ is strictly weaker than the condition that there is a nontrivial homomorphism $\mathbb{Q} \to G$ , so these questions may have different answers.","['matrices', 'group-homomorphism', 'group-theory', 'linear-algebra']"
3876126,What does Hartshorne mean by an (open) affine subset?,"This is possibly a very silly question. I am reading Hartshorne's Algebraic Geometry, and in Chapter 1.4 (Varieties -- Rational Maps) one of the propositions is as follows: On any variety, there is a base for the topology consisting of open affine subsets. I'm simply confused about what an ""affine subset"" is. (Is it just any subset of $\mathbb A^n$ ? But then the open affine subsets, which are the open subsets, obviously form a base.. Does he mean ""algebraic subset""? Then no open set is affine, besides $\mathbb A^n$ itself..) In Chapter 1.1 Hartshorne defines affine varieties, quasi-affine varieties, affine curves, but not what an affine set is! I have a feeling that I have just badly misunderstood something. Some help in clearing up this misunderstanding would be greatly appreciated!","['definition', 'algebraic-geometry']"
3876237,Attempt to define limit of a sequence of surreal numbers,"For sake of well-definedness, here we consider only ordinals less than the first uncountable ordinal, $\Omega$ . Just like $\infty$ in the notation $\lim_{n→\infty}$ is essentially $\omega$ , $\Omega$ will be the new $\infty$ . Likewise, surreal numbers will be capped by $\pm\Omega$ . I attempted to define the notion of limit of a sequence to arbitrary ordinal . It's just an extension of the usual $\epsilon$ - $N$ definition: $$
\lim_{n→O} f(n) = x \overset{\text{def.}}{\iff} \forall(\epsilon>0) \quad \exists(N<O) \quad \forall(n \text{ s.t. } N<n<O) \quad |f(n)-x|<\epsilon
$$ Where $\epsilon$ is real, $O$ is a limit ordinal, and $N$ and $n$ are ordinals. Note that in this notion, the notion of uncountable summation follows. One might wonder whether the following limit converges: $$
\lim_{n→\Omega}\sum_{k=0}^n\frac1{k^2}
$$ This makes sense only if $f$ is supposed to be a sequence of surreal numbers. But generally and unfortunately, limit doesn't uniquely exist amongst surreal numbers. For example, $\lim_{n→\omega}\frac1n$ converges to zero and every infinitesimal. Furthermore, in this definition, $\lim_{n→\omega}n$ diverges. By the notion of a limit ordinal, $\lim_{n→\omega}n = \omega$ must satisfy. To summarize, the definition above is flawed, and this question asks for a topology on surreal numbers such that: Surreal numbers form a topological field $\mathbb{R}$ as a subspace preserves its order topology The countable ordinals as a subspace preserves its order topology","['general-topology', 'ordinals', 'nonstandard-analysis', 'surreal-numbers']"
3876247,Proof of Strong Whitney Immersion,"I'm learning about embeddings and applications of Sard's Theorem in Guillemin & Pollack, and I'm trying to work out exercise 8.10: Prove that every $k-$ dimensional manifold $X$ may be immersed in $\mathbb{R}^{2k}.$ G&P call this The Whitney Immersion Theorem . I tried modeling my proof after the dimension reducing inductive argument provided in the proof for the weak embedding theorem, but after searching for an answer to something that had stumped me I found that I had essentially written out the proof of Theorem 5.9 found here . For completeness, this is: First, we use the Whitney's embedding theorem for $\mathbb{R}^{2k+1}$ to find an embedding of $f:M^{k} \longrightarrow \mathbb{R}^{2k+1}$ (here $M^{k}$ is our $k-$ manifold). Now, we define $g:TM^{k}
> \longrightarrow \mathbb{R}^{m}$ where $m > 2k$ such that $g(x,v) =
> df_{x}(v)$ . Recall that $\dim TM^{k} = 2k$ . Then, since $m > 2k$ we
know that every point in $TM^{k}$ is a critical point of $g$ . By
applying Sard's theorem, we can pick up an $a \in \mathbb{R}^{m}$ such
that $a \not \in g(TM^{k})$ and $a \neq 0$ . Let $\pi$ be the
projection of $\mathbb{R}^{m}$ onto the orthogonal complement of $a$ , $H_{a}$ . The composition $\pi \circ f: M^{k} \longrightarrow H_{a}$ is
an immersion if $d(\pi \circ f)$ is injective. To prove that $d(\pi \circ f)$ [the pdf says $f$ but I assume this is a
typo] is injective, suppose for a contradiction that $v \neq 0$ and $v
> \in T_{x}M^{k}$ such that $d(\pi \circ f)_{x}(v) = 0.$ Note that since $\pi$ is linear, $d(\pi \circ f)_{x} = \pi \circ df_{x}$ by the chain
rule. So $\pi \circ df_{x}(v) = 0$ . The projection of a vector $df_{x}(v)$ onto an orthogonal complement of $a$ is only zero if that
vector is a scalar multiple of $a$ . So $df_{x}(v) = ta$ for some $t
> \in \mathbb{R}$ . If $t = 0$ , then $df_{x}(v) = 0$ . This however cannot
happen, because $0 \not \in g(TM^{k})$ . So $t$ is not equal to zero.
Therefore, $g(x,\frac{1}{t}) = a$ or $df_{x}(\frac{1}{t}) = a$ which
is a contradiction since our choice of $a$ prohibits it from being
part of the image of $g$ . So $d(\pi \circ f)$ is injective. Note that $\pi \circ f$ creates an immersion into the $m-1$ dimensional subspace
of $\mathbb{R}^{m}$ . This process can be continued by induction on $m$ until $m = 2k$ , so by repeating this process we will have found an
immersion of $M^{k}$ into $\mathbb{R}^{2k}$ . My problem, specifically, is with the part: If $t = 0$ , then $df_{x}(v) = 0$ . This however cannot
happen, because $0 \not \in g(TM^{k})$ . Why can't this happen? To my understanding, we've only chosen for certain $a \not \in g(TM^{k})$ , and even further so, shouldn't $0$ be in the image since the derivative is a linear map? I believe a patch to this is that we are taking $f$ to be an embedding, so its derivative is injective and thus its kernel is trivial, but wouldn't using this break the inductive argument at the end? As far as I understand, we are giving an immersion into the lower dimensional subspace, not an embedding necessarily.","['differential-topology', 'linear-algebra', 'differential-geometry']"
3876341,What is the inverse/opposite of a double integral?,"I am currently taking Calc 3 and we just finished our unit on double/triple integrals. I started thinking about a problem back from AP Physics (where my teacher did an impressive amount of hand waving to somehow avoid directly explaining vector calculus when discussing Maxwell's equations, leading to me spending an entire semester confused about what the heck flux was supposed to be or why it should matter), which basically boils down to $\iiint_D{F(x, y, z)}{dV}=Q(D)$ . In Calc 1 and 2, if you have an equation of the form $\int{f(x)}{dx}=g(x)$ , then you can take the ""inverse integral"" (i.e. derivative with respect to x) of both sides, which gives $f(x)=\frac{d}{dx}g(x)$ . However, I cannot seem to figure out what the analogue of this would be for a double or triple integral. What is the opposite/inverse operation for a double or triple integral over some domain $D$ ?",['multivariable-calculus']
3876385,The intersection of the conjugates of $H\leq G$ is *exactly* the union of the conjugacy classes of $G$ that are fully in $H$,"There are several posts discussing the intersection of the conjugates of a subgroup, but I think this provides at least something new in this forum. I'd like to know if the reasoning and conclusion are valid, and whether or not this follows trivially from some other conclusion made elsewhere. Assume the following: $H\leq G$ is a subgroup. $N:=\bigcap_{g\in G}gHg^{-1}$ is the group intersection of all conjugates in $G$ of $H$ . $C:=\bigcup_{i\in I}C_i$ is the set union of all conjugacy classes $C_i$ of $G$ where $C_i\subseteq H$ . The goal is to show that $N=_{\mathbf{Set}}C$ . Show that $C\subseteq N$ : Let $Q\subseteq H$ be any set invariant under conjugation in $G$ . This means that $gQg^{-1}=Q$ for all $g\in G$ . Therefore, $Q$ is in all conjugates of $H$ , and is thus also in $N$ . This can be applied to any $C_i$ in the union $C$ . Show that $N\subseteq C$ : Let $x\in N$ . If $g^{-1}xg\in H$ for all $g\in G$ , then the conjugacy class of $x$ is also in $H$ . Equivalently, we can show that $x\in gHg^{-1}$ for all $g\in G$ . By definition of $N$ , this is true for any choice of $g$ ; therefore, the conjugacy class of $x$ is contained in $H$ , thereby also including that class in $C$ . Since $C\subseteq N$ and $N\subseteq C$ , we can conclude that $N=_{\mathbf{Set}}C$ . $\square$ At the very least, this shows that if $H$ contains a nontrivial conjugacy class then $N$ is also nontrivial. I'm fine with that result because it puts a lower bound on what $N$ can be. On the other hand, it implies that in many cases $H$ might already be normal if it contains a nontrivial conjugacy class. (Perhaps this is common for permutation groups? Seems plausible.)","['normal-subgroups', 'group-theory', 'abstract-algebra', 'solution-verification']"
3876477,Variation of Hoeffding's Inequality for (weakly) dependent random variables,"Is there a version, or modification, of this theorem for weakly dependent random variables? Or perhaps at least one for the special case involving Bernoulli random variables (that are now weakly dependent)? I can't seem to find anything formal on the subject. I've stated Hoeffding's theorem below for when we have iid random variables. Theorem (Hoeffding's Inequality). For iid random variables $X_1, \dots, X_n$ satisfying $$a_i \leq X_i \leq b_i~\text{a.s.}, \\
\gamma_i = b_i - a_i, \\
\gamma_i \leq \Gamma_i,$$ Hoeffding's inequality says $$\mathbb{P}\left[|S_n - \mathbb{E}[S_n]| > t\right] < 2\exp\left\{-\frac{2t^2}{\sum_{i=1}^n \gamma_i^2}\right\} < 2\exp\left\{-\frac{2t^2}{n\Gamma^2}\right\},$$ where $S_n := X_1 + \dots + X_n$ . Edit: Weak Dependence. We can think of weakly dependent random variables in a time sense. That is, suppose we are given a time dependent sequence of random variables $\{X_t\}_{t=1}^{\infty}$ . If we fix a $t$ and let $s \in \mathbb{N}$ , then for any $X_t$ and $X_{t + s}$ as $s$ increases the $\text{Cov}(X_t, X_{t+s})$ decreases to $0$ asymptotically (e.g. exponential decay).","['inequality', 'probability-theory']"
3876546,Asymptotic behavior of a Fourier/Laplace transform,"I see many results concerning the asymptotics of Fourier transforms. These link in particular the regularity/continuation properties of the function to the polynomial/exponential decay of its Fourier transform. However, these results often hold only in the real variable. I am interested in the Fourier transform ""along the imaginary axis"" instead. Let us be more precise. I am interested by the digamma function $\psi = \frac{\Gamma'}{\Gamma}$ , and in the function $$h(\nu) = \exp\left(-\alpha \psi \left( \frac14 \pm \frac{i\nu}{2} \right)\right),$$ where $\alpha$ is a fixed parameter, say $\alpha > 1$ . I am interested in the asymptotic behavior of the Fourier transform of $h$ at $+\infty$ . More precisely, $$\widehat{h}(x) = \int_{\mathbb{R}} h(\nu) e^{ix\nu} d\nu.$$ How to get asymptoptics when $x \to +\infty$ in this situation? I have no feeling about what determines it: size? variations? only asymptotics of $h$ ? I had many trials, not convincing. Typically, just changing variables, I can get an expression of the shape $$e^{-\frac{x}{2}} \int_{i\mathbb{R}} e^{-\alpha \psi(u)} e^{2xu} du$$ which looks more like a Laplace (?) transform than a Fourier transform. I was motivated by the fact that I am expecting for other reasons an exponential decay as above, so that I am hoping for a polynomial behavior in $x$ for the remaining integral. However, is the growth/decay estimate of this last integral easier to understand than the original one? So my question could be synthzised into Do we have $\int_{i\mathbb{R}} e^{-\alpha \psi(u)} e^{xu} du \ll x^A$ for a certain $A$ ?","['definite-integrals', 'fourier-transform', 'laplace-transform', 'complex-analysis', 'gamma-function']"
3876555,Tutors correcting tests - Confidence intervals,"At the end of the semester, two tutors Albert and Ben are correcting an exam with $10$ tasks. They share the $100$ written exams and measure the time needed to correct a task in minutes. The difference $x_i$ of the correction times (Ben's time $-$ Albert's time) for task $i$ is given in the following table: The sample mean $\bar{x} = 4.4$ and the sample standard deviation $\bar{\sigma} = 6.82$ . We assume that the values $x_1, x_2, ..., x_{10}$ are realizations of $10$ independent and identically normally distributed random variables. For the significance level $\alpha = 0.05$ , find a confidence interval for the difference $x_i$ and determine the acceptance region for $\bar{x}.$ Since the population standard deviation $\sigma$ is not given, we will use the $t-$ distribution (or Student- $t$ -distribution) to find the confidence interval for the population mean $\mu$ . First we calculate our acceptance thresholds $t_c$ and $-t_c$ : Since we know that $\alpha = 0.05$ , the area of the region right to $t_c$ $= 0.025 = $ the area left to $-t_c$ . We also know that we have $n-1 = 10-1 = 9$ degrees of freedom. Using the $t-$ distribution values table, we find $t_c = 2.26$ and $-t_c = -2.26.$ Now we find our test statistic $T_s$ : $T_s = \dfrac{\bar{x} - \mu}{\dfrac{\bar{\sigma}}{\sqrt{n}}}$ $= \dfrac{4.4 - \mu}{\dfrac{6.82}{\sqrt{10}}}$ . We know that $P(-t_c \leq T_s \leq t_c) = 1- \alpha = 0.95.$ Substituting then gives us: $$\bar{x} - t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}} \leq \mu \leq \bar{x} + t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}}$$ $$4.4 -2.26 \cdot \dfrac{6.82}{\sqrt{10}} \leq \mu \leq 4.4 +2.26 \cdot \dfrac{6.82}{\sqrt{10}}$$ $$-0.474 \leq \mu \leq 9.274$$ So we know that $-0.474 \leq \mu \leq 9.274$ with $95\%$ confidence. The acceptance region for $\bar{x}$ would be $[-t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}}, t_c \cdot \dfrac{\bar{\sigma}}{\sqrt{n}}] = [-4.874, 4.874].$ Did I do this correctly? I'm very unsure about my work and don't know how to interpret the negative values in the confidence interval.","['statistical-inference', 'statistics', 'confidence-interval', 'probability']"
3876700,Sum of odd powers of even number of complex numbers,"I’ve come across a result and I’m having a bit of trouble seeing where it comes from, and it seems like it should be obvious why it is true. Say we are given n complex numbers $ \{z_1,...,z_n\}$ where n is even, such that the sum of their odd integer powers is zero for all powers less than $n$ . Ie; $$\sum_{i=1}^n z_i^k=0$$ for all k an odd integer less than n. Then if $m$ is an odd integer greater than n, we get that $$\sum_{i=1}^n z_i^m=0$$ Again I have a feeling that this is a well-known result, or at least a re-wording or special example of one, but I don’t know where to start looking.","['complex-analysis', 'abstract-algebra', 'polynomials']"
3876747,Prove: if $m \in n$ then $m^+ \subseteq n$.,"The following is exercise 5(d), section 6.2, from A book of set theory , by Charles Pinter (pg. 122). 5. Prove the following, where $m, n, p \in \omega$ . d) If $m \in n$ , then $m^+ \subseteq n$ . These are the relevant definition and results, quoted from the same book: 6.2 Theorem For each $n \in \omega$ , $n^+ \neq 0$ . 6.4 Lemma Let $m$ and $n$ be natural numbers; if $m \in n^+$ , then $m \in n$ or $m = n$ . 6.1 Definition By the set of the natural numbers we mean the intersection of all the successor sets. The set of the natural numbers is designated by the symbol $\omega$ ; every element of $\omega$ is called a natural number . Attempted proof: Suppose $m^{+} \nsubseteq n$ then $n \in m$ and so $n \subseteq m$ ,but $m \subseteq m^{+}$ and so $n \subseteq m^{+}$ . But by 6.4 $m \in n$ , then (because $n$ is transitive) $m \subseteq n$ ; and then $m \subseteq n^{+}$ . So we have two successors for $m$ by 6.4 (contradiction, since we must have $m \in n$ ). Hence $S(m) \subseteq n$ . 🥶","['elementary-set-theory', 'peano-axioms']"
3876777,ODE in variable $r$,I'm sure this is a simple question but I will need help. While trying to solve a PDE using separation of variables I came across an ODE. I want to solve the following  ODE: $$\frac{R''(r)+\frac{2}{r}R'(r)}{R(r)}=-\lambda^2$$ The solution is $A\cos(\lambda r)/r +B\sin(\lambda r)/2\lambda r$ It checks out when you plug it in but how do we actually get the solution?,['ordinary-differential-equations']
3876803,Integration by parts for definite integrals,"Question Evaluate $$\int_{2}^{4} \frac {\mathrm{d}x} {x \ln x}\ .$$ My working Let $u = \frac {1} {\ln x}$ and $v' = \frac {1} {x}$ $\implies u' = -\frac {1} {x (\ln x)^2}$ and $v = \ln x$ $\therefore \int_{2}^{4} \frac {dx} {x \ln x} = 1 + \int_{2}^{4} \frac {dx} {x \ln x}$ $\implies 0 = 1$ (say what?) Answer $\int_{2}^{4} \frac {dx} {x \ln x} = \ln 2$ When I use the substitution $u = \ln x$ and proceed, I do arrive at the answer, but that is trivial so I am not here to discuss that. What I am here to discuss, however, is my working when I use integration by parts. I seem to have gone wrong somewhere, which I find very intriguing. I assume I must have been careless, but I have been doing calculus all day, so perhaps my mind is fatigued. Even worse, is there some inherent misunderstanding in my concept of integration by parts? I will be very grateful if anyone can point out where I have gone wrong :) Edit Following the answers given, it seems I did have a conceptual misunderstanding about integration by parts and it turns out that integration by parts cannot be used to solve this particular integral! Today, I have also found out that integration by parts cannot solve all integrals!","['integration', 'calculus', 'definite-integrals', 'real-analysis']"
3876836,How to prove $\mathop {\lim }\limits_{x \to {\rm{ + }}\infty } f'(x){\rm{ = 0}}$,"Today I have came across a problem.It is harder than I think. Given that (1) $f(x)$ is differentiable on $\left[ {{\rm{0}},{\rm{ + }}\infty } \right)$ , (2) $f'(x)$ is uniformly continuous on $\left[ {{\rm{0}},{\rm{ + }}\infty } \right)$ , and (3) $\mathop {\lim }\limits_{x \to {\rm{ + }}\infty } f(x)$ is existent, prove that $$\mathop {\lim }\limits_{x \to {\rm{ + }}\infty } f'(x){\rm{ = 0}}.$$ If not, is there any counter example? (I've been working on this for a long time.)","['probability-limit-theorems', 'uniform-continuity', 'real-analysis', 'continuity', 'limits']"
3876946,"Let $(C, \prec)$ be the completion of $(P, <)$. Then, if $c, d \in C$, there is some $p \in P$ such that $c \prec p \prec d$.","I'm studying the book Introduction to Set Theory by Hrbacek and Jech and came across this theorem: 5.3 Theorem Let $(P, \lt )$ be a dense linearly ordered set without endpoints. Then there exists a complete linearly ordered set $(C, \prec )$ such that (a) $P \subseteq C$ . (b) If $p,q \in P$ , then $p \lt q$ if and only if $p \prec q$ ( $\prec$ coincides with $\lt$ On $P$ ). (c) $P$ is dense in $C$ , i.e., for any $p,q \in P$ such that $p\lt q$ , there is $c\in C$ with $p\prec c\prec q$ . (d) $C$ does not have endpoints. Moreover, this complete linearly ordered set $(C, \prec )$ is unique up to isomorphism over $P$ . In other words, if $(C^*, \prec ^* )$ is a complete linearly ordered set which satisfies (a)-(d), then there is an isomorphism $h$ between $(C, \prec )$ and $(C^*, \prec ^*)$ such that $h(x)=x$ for each $x \in P$ . The linearly ordered set $(C, \prec )$ is called the completion of $(P, \lt )$ . Specifically I'm interested in property (c); I would like to reverse the roles of $P$ and $C$ and prove the following: Proposition: For any $c, d \in C$ such that $c \prec d$ , there is $p \in P$ with $c \prec p \prec d$ . It seems reasonable, since in practice $P$ is a model for the rationals and $C$ is a model for the real numbers, so this proposition would simply say ""between any pair of real numbers there is always a rational number"". In fact, the authors use this result without ever proving it a couple of lines below. However, I am unable to prove the result. I tried the contrapositive method but got confused with the quantifiers; then I tried contradiction, but couldn't quite finish the argument. I would like to ask for any hint that could help me prove this result, since I'm pretty much stuck here. Thank you.","['elementary-set-theory', 'order-theory']"
3876975,"Non-strictly monotonic bijection $f:\Bbb R\to(0,+\infty)$ satisfying $f(a)=1, f(0)=1, f(x+y)=f(x)f(y)$","Is there any way of constructing a non-strictly monotonic bijective function $f:\Bbb R\to(0,+\infty)$ satisfying: $$f(x+y)=f(x)f(y), f(0)=1, f(1)=a>0\space$$ (without a Hammel basis for $\Bbb R$ over $\Bbb Q$ )? This question, without the condition that $f$ is not strictly monotonic, has already been asked many times, but I couldn't think of any discontinuous bijection from $\Bbb R$ to $(0,+\infty)$ with the properties above. I know that strict monotonicity implies $f(x)=a^x,\space\forall x\in\Bbb R$ . One idea was to take some dense additive subgroup $G\subset\Bbb R$ and define $f(x)=a^x,\space\forall x\in G$ , but then, as we require injectivity and $f>0$ , the problem arises with $f(\Bbb R\setminus G)$ . I found a related answer where it is proven that $f$ is either identically $0$ or $f>0\space\forall x\in\Bbb R$ , but I couldn't use that answer to construct a function I'm looking for because we haven't learned about a Hammel basis in real analysis lectures yet. I also eliminated $f(x)=\alpha x,\alpha\in\Bbb R$ after realizing I couldn't fix one $\alpha$ . Is there any more elementary method I'm failing to see? Thank you in advance!","['functions', 'real-analysis']"
3877006,Does group of permutations of natural numbers contain subgroup isomorphic to $SO(2)$?,"Consider following two infinite groups: group of all permutations of natural numbers (i.e. group of all bijections $f: \mathbb{N} \to \mathbb{N}$ ) and group of all rotations of a plane. Does group of permutations contain subgroup isomorphic to the group of rotations? Both groups have cardinality of the continuum, so simple cardinality considerations do not work.","['group-theory', 'abstract-algebra', 'infinite-groups']"
3877042,"two reduced row echelon matrices have the same nullspace, prove they are identical","I am trying to prove if R and R' are two reduced row echelon matrices, and have the same nullspaces, they are identical. I have observed this when testing them, but I have trouble finding a formal proof. I tried reading this question and extracting a general solution, but this question is focused on a particular 2 by 3 matrix whereas I am trying to find a formal proof for an m by n matrix. I believe I should start by writing two general reduced row echelon matrices, writing Rx=R'x=0 and finding a general answer for each row, then proving they are equal, but I have trouble proving it generally and without presumptions. Thank you in advance.","['matrices', 'matrix-rank', 'linear-algebra', 'matrix-decomposition']"
3877083,"Finding the domain and range of $f(x)=\frac{2e^x}{1+e^x}$, and of $f^{-1}(x)$",What is the domain and range of : $$ f(x) = \frac{2e^x}{1+e^x} $$ and the domain and range of : $$ f^{-1}(x)$$ I've already found $$ f^{-1}(x) = \ln\left(\frac {x}{2-x}\right) $$ I'm looking for the process necessary to find its domain and range. Many thanks.,"['algebra-precalculus', 'functions']"
