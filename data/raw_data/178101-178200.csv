question_id,title,body,tags
3219899,How to decompose a group representation which is a direct sum of copies of one irreducible representation?,"Let $G$ be a finite group. Let $F$ be a field of characteristic zero. Let $V$ and $W$ be finite dimensional representations of $G$ . Assume that $V$ is irreducible and $W$ is isomorphic to $nV$ for some $n$ (this is the external direct sum of $n$ copies of $V$ ). I'm looking for a procedure to find elements $w_1,\dotsc,w_n\in W$ such that $w_i$ generates a subrepresentation $V_i$ of $W$ which is isomorphic to $V$ , and such that the internal direct sum of the $V_i$ is $W$ . I'm not even sure how to just find $w_1\in W$ that generates a subrepresentation isomorphic to $V$ . EDIT: The accepted answer tells us what to do if $F=\mathbb{C}$ . What if $F=\mathbb{R}$ ?. Can we use extension of scalars?","['group-theory', 'representation-theory']"
3219910,Proving $ \sum_{n \in \mathbb{Z} } \left[\frac{\sin (n \alpha + \theta) }{ n \alpha + \theta} \right]^2 = \frac{\pi}{\alpha} $,"Proving $$
\sum_{n \in \mathbb{Z} } \left[\frac{\sin (n \alpha + \theta)  }{ n \alpha + \theta} \right]^2 = \frac{\pi}{\alpha} \,\, \forall \alpha , \theta \in \mathbb{R}
$$ My attempt It is known that $$\sum_{n\in\mathbb{Z}}\frac{1}{(n\pi+x)^2}=\csc^2(x)\tag{1}$$ which can be yielded from $$\sin(\pi x)=\pi x\prod_{n=1}^\infty\left(1-\frac{x^2}{n^2}\right)$$ Thus for the situation $\alpha = \pi$ we have $$
\sum_{n \in \mathbb{Z} } \left[\frac{\sin (n \pi + \theta)  }{ n \pi + \theta} \right]^2 
= 
\sum_{n \in \mathbb{Z} } \left(\frac{\sin\theta }{ n \pi + \theta} \right)^2 
= 1
$$ But how to deal with the situation that $\alpha \ne \pi$ ? Since $$
\displaystyle\sum_{n \in \mathbb{Z} } \frac{1}{ (n \alpha + \theta)^2} = \frac{\pi^2 \csc^2 \frac{\pi\theta}{\alpha} }{\alpha^2}
$$ is a corollary of $(1)$ ,
we only need to evaluate $
\displaystyle\sum_{n \in \mathbb{Z} } \frac{\cos (2n \alpha + 2\theta)  }{ (n \alpha + \theta)^2}  
$ , which means to evaluate $$
\displaystyle\sum_{n \in \mathbb{Z} } \frac{\cos (2n \alpha ) }{ (n \alpha + \theta)^2} \tag{2}
$$ and $$
\displaystyle\sum_{n \in \mathbb{Z} } \frac{\sin(2n \alpha ) }{ (n \alpha + \theta)^2} \tag{3}
$$ In the post linked , we get a nice equation $$
\frac{\pi \cos(xy)}{\sin(\pi y)}
=\sum_{n=-\infty}^\infty \frac{(-1)^n\cos(nx)}{y-n}
$$ If there doesn't exist $(-1)^n$ , we can evaluate $(2)$ and $(3)$ by differentiating and selecting appropriate $x$ . But I got stuck on that. Any hints? Thanks in advance!","['complex-analysis', 'sequences-and-series', 'fourier-analysis', 'real-analysis']"
3219922,99th Percentile: top 1% or top 2%? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question If one achieves a score in the 99th percentile on an exam, is that score considered in the top 1% or 2%? How is percentile defined in statistics? I read this somewhere: It’s top 2% - being in the x percentile doesn’t imply top (100-x) percent because the percentage getting exactly x is counted twice. Is this correct?",['statistics']
3219925,Stability of optimal transference mappings (Exercise 2.17 in Villani's Topics in Optimal Transportaiton),"I'm really stuck on the first part of this exercise. I've tried to type it up in a fashion that is as self-contained as possible, but some background in optimal transport is likely required to read this. Let $P_{ac,2}(\mathbb{R}^n)$ denote the space of absolutely continuous probability measures with finite second moments. Suppose $\sigma \in P_{ac,2}(\mathbb{R}^n)$ is given, as well as a family $\rho_k \in P_{ac,2}(\mathbb{R}^n)$ that weakly converges to some $\rho \in P_{ac,2}(\mathbb{R}^n)$ . Let $\nabla \varphi_k$ be the optimal mapping of the Monge problem with quadratic cost between $\sigma$ and $\rho_k$ , i.e. $$\int_{\mathbb{R}^n} |x-\nabla \varphi_k(x)|^2 d\sigma(x) = \inf_{T \# \sigma = \rho_k} \int_{\mathbb{R}^n} |x - T(x)|^2 d\sigma(x)$$ Here, the infimum is taken over functions $T : x \mapsto T(x)$ satisfying the pushforward constraint. Similarly, we can define $\nabla \varphi$ between $\sigma$ and $\rho$ . Let $d\pi_k(x,y) = d\sigma(x) \delta(y = \nabla \varphi_k(x))$ , and similarly define $\pi$ between $\sigma$ and $\rho$ . How can I show that $\pi_k$ weakly converges to $\pi$ ? (The hint given is to note that the optimal transference plan is unique.)","['measure-theory', 'optimal-transport', 'convex-analysis']"
3219935,Show that $ ax + b = 0$ has integer solution,"If $$\forall_m ax + b \equiv 0 \mod m$$ has solution then show that $ ax + b = 0$ has integer solution I know the fact that if $a,b \in \mathbb Z, a \neq 0, \ m \in \mathbb N, d = gcd(a,m)$ then congurency $$ax \equiv b \mod m$$ has $d$ solutions $x$ from set $\left\{0,1,2,...,m-1 \right\}$ when $d|b$ and without solutions in other case but I am not sure if that theorem is helpful here... 
 I have tried to start deduction in that way let $d_m = gcd(a,m)$ . From task it follows that if $$ \forall_m ax \equiv b \mod m $$ has solution then $$ d_m = gcd(a,m) | b $$ but I stucked here - I am not sure what does it exactly give me.","['modular-arithmetic', 'discrete-mathematics']"
3219968,"Show that the system $v_1 - u, \dots, v_n - u$ is linearly dependent precisely when $\lambda_1 + \dots + \lambda_n=1$","Let $v_1, \dots v_n$ be a linearly independent system of the $\mathbb{K}$ -vectorspace $V$ and $u = \lambda_1v_1 + \dots + \lambda_n v_n$ with $\lambda_1, . . . \lambda_n \in \mathbb{K}$ . Show that the system $v_1 - u, \dots, v_n - u$ is linearly dependent precisely when $\lambda_1 + \dots + \lambda_n=1$ In other words, I have $u=\sum\limits^n_{i=1}\lambda_iv_i$ and I should show, that $$v_1 - \left(\sum\limits^n_{i=1}\lambda_iv_i\right), \dots, v_n - \left(\sum\limits^n_{i=1}\lambda_iv_i\right)$$ is linear dependend only for $\lambda_1 + \dots + \lambda_n=1$ So, all $v_i$ 's would terminate one $v_i$ : $$ -\sum\limits^{n}_{i=2}\lambda_iv_i, \dots, -\sum\limits^{n-1}_{i=1}\lambda_iv_i$$ I'm not sure how to go on exactly. I could suppose, that $\lambda_1 + \dots + \lambda_n=1$ and I would get $$ -\sum\limits^{n}_{i=2}v_i, \dots, -\sum\limits^{n-1}_{i=1}v_i=\left(-v_2-v_3-\dots-v_n\right),\dots,\left(-v_1-v_2-\dots-v_{n-2}-v_{n-1}\right)$$ But that wouldn't help me either, or am I missing somthing? EDIT: Using N. S. hint, I finally understood it:
Because $\beta_i=\frac{\beta_i}{\beta_1+\dots+\beta_n} \quad i\in\{1,\dots,n\}$ one can say that $\beta_1+\beta_2+\dots+\beta_n=\frac{\beta_1}{\beta_1+\beta_2+\dots+\beta_n}+\dots+\frac{\beta_n}{\beta_1+\beta_2+\dots+\beta_n}=\frac{\beta_1+\beta_2+\dots+\beta_n}{\beta_1+\beta_2+\dots+\beta_n}=1$","['proof-verification', 'linear-algebra', 'vector-spaces']"
3220000,Number of Atoms on Earth's Surface (Question based on Vsauce video which involves fractal dimension),"In a Vsauce video titled ""How much of the Earth can you see at once"" they try to do a calculation to estimate the number of atoms on Earth's surface. The first part is easy: 1) Calculate surface area of the Earth (let's call it 'A') 2) Divide by cross sectional area of an atom (let's call it 'a') 3) The total number of atoms is: N = A/a The next part is what got me thinking and I just can't seem to understand how they got to it. They explain that Earth has surface roughness which is consistent with a fractal dimension of 2.3 . They then say that by considering that this applies from the scale of a human hair to that of a mountain, the total number of atoms that could fit on the Earth is 1000x greater than what is estimated from the previous calculation. Since they never explained their calculations, would anyone be willing to guess and perhaps share how they would do this? Cheers! P.S. If you can go step by step I would GREATLY appreciate it!","['area', 'geometry', 'fractals']"
3220014,Subset of a subgroup is not closed under group actions,"In the following image (from ""Field Arithmetic by Fried & Jarden"" Page 6, Lemma 1.2.2(b) ), red rectangle, I'm trying to figure out why it's right to claim $h^{-1} \in H$ . I thought the following solved it: $g=k_ih_i$ and $g=kh^{-1}$ $\Rightarrow k_ih_i=kh^{-1} \Rightarrow h_ih=k_i^{-1}k \in H$ because right hand side is in H. So $h^{-1}=k^{-1}g=k^{-1}k_ih_i$ , hence, because $k_i^{-1}k \in H$ we get $h^{-1} \in H \Rightarrow g=kh^{-1} \in KH$ . But then I realized there's no logic in assuming $h_ih \in H$ because we're talking subsets here, not subgroups. So is it ok to assume $h_ih \in H$ ? if so, why? If not, I'd appreciate an explanation for the red part..","['elementary-set-theory', 'general-topology', 'abstract-algebra', 'profinite-groups']"
3220049,Commutative unital Banach algebra not isomorphic to $C(X)$ for some compact Hausdorff space $X$,"According to some lecture notes I am reading, ""it is not so difficult to find an example of a commutative unital Banach algebra which is not isomorphic to $C(X)$ for some compact Hausdorff space $X$ ""... Well I was thinking about using the disk algebra $\mathcal{A}$ of continuous functions on the disk $D = \{z \mid |z| \leq 1\}$ which are holomorphic on the interior of $D$ , with the sup norm. This should be a commutative unital Banach algebra. However, since it is the beginning of the chapter about the Gelfand transformation, I would like to prove that $\mathcal{A}$ is not isomorphic to $C(X)$ without using Gelfand's result. What other tools could I use to prove this fact?","['banach-algebras', 'functional-analysis', 'operator-algebras']"
3220091,Simplifying product of two modified Bessel functions,"Is it possible to simplify the product of two modified Bessel functions of the first kind with order zero, \begin{align}
\Bigg( \sum_{n=0}^{\infty} \dfrac{ \Big( \frac{x^{2}}{4} \Big)^{n} }{n! \cdot n!} \Bigg) \cdot \Bigg( \sum_{k=0}^{\infty} \dfrac{ \Big( \frac{x^{2}}{4} \Big)^{k} }{k! \cdot k!} \Bigg) \text{?}
\end{align} I have gotten as far as \begin{align}
& \Bigg( \sum_{n=0}^{\infty} \dfrac{ \Big( \frac{x^{2}}{4} \Big)^{n} }{n! \cdot n!} \Bigg) \cdot \Bigg( \sum_{k=0}^{\infty} \dfrac{ \Big( \frac{x^{2}}{4} \Big)^{k} }{k! \cdot k!} \Bigg) \\
&= \sum_{n=0}^{\infty} \Big( \frac{x^{2}}{4} \Big)^{n} \sum_{k=0}^{n} \frac{1}{k! \cdot k!} \cdot \frac{1}{(n-k)! \cdot (n-k)!} \\
&= \sum_{n=0}^{\infty} \dfrac{ \Big( \frac{x^{2}}{4} \Big)^{n} }{n! \cdot n!} \sum_{k=0}^{n} \binom{n}{k}^{2} \\
&= \sum_{n=0}^{\infty} \binom{2n}{n}^{2} \cdot \dfrac{ \Big( \frac{x}{2} \Big)^{2n}}{(2n)!} \text{;}
\end{align} however, I cannot see a way forward. If possible, I'd like to represent the product in terms of another modified Bessel function, beyond the very obvious definition already given above. Perhaps the exponential generating function of the central binomial coefficients could be useful.","['binomial-coefficients', 'combinatorics', 'bessel-functions', 'sequences-and-series']"
3220109,"Show that $\int_0^{\pi/3} \frac{1}{(\cos\theta + \sqrt{3}\sin\theta)^2}\,d\theta = \frac{\sqrt3}{4}$","Show that $$\int_0^{\pi/3} \frac{1}{(\cos\theta + \sqrt{3}\sin\theta)^2}\,d\theta = \frac{\sqrt3}{4}$$ So I have attempted this, but have got a different answer so I have obviously done something wrong but i can't spot it. My workings are as shown. Any help will be great. I started by writing $\cos\theta + \sqrt{3}\sin\theta$ as $2\cos(\theta - \frac \pi 3)$ . This means that integral becoems, $$\int_0^{\pi/3} \frac{1}{2\cos^2(\theta - \frac\pi3)}d\theta = \int_0^{\pi/3} \frac12\sec^2(\theta - \frac\pi3)d\theta = \left[\frac12 \tan(\theta-\frac\pi3)\right]_0^{\pi/3} $$ $$ = \frac12\tan(0) - \frac12\tan(-\frac\pi3) = 0 - \frac12\sqrt3$$ So I get a final answer of $-\sqrt3/2$ any help will be great.","['integration', 'trigonometry', 'definite-integrals', 'trigonometric-integrals']"
3220172,Distance from origin of biased random walk conditioned to be positive at time n,"Let $S_n$ be the position of a simple random walk on the integers started from $0$ that moves right with probability $p<1/2$ . What is the asymptotic behavior of $$E[ S_n \mid S_n >0 ]$$ as $n \to \infty$ ? This is a large deviation event, so I don't have  good intuition for how far the walk should be. I would believe anything between $O(1)$ and $O(n)$ . However, my gut tells me that it is $O(\sqrt n)$ which is what would occur for $p=1/2$ . Note that $S_k$ is allowed to be nonpositive for $k<n$ , we are just conditioning on its location at time $n$ .","['large-deviation-theory', 'random-walk', 'probability']"
3220173,Problem with this $\frac{d}{dx}(y^3)$,"How do you differentiate this equation with respect to $x$ ? $$x^2=xy^3+2$$ $$\frac{d}{dx}(x^2)=\frac{d}{dx}(xy^3)+\frac{d}{dx}(2)$$ $$2x=x\frac{d}{dx}(y^3)+y^3\frac{d}{dx}(x)+0$$ $$2x=x\frac{d}{dx}(y^3)+y^3$$ Here is the problem I am facing with, $$\frac{d}{dx}(y^3)?$$ this $\frac{d}{dx}(x^3)=3x^2$ it is understandable",['derivatives']
3220176,Deciphering a problem - calculus; maps,"A map from an open set $\mathcal{D} \subset \mathbb{R}^n$ to $\mathbb{R}^m$ , denoted $f : \mathcal{D} \subset \mathbb{R}^n \longrightarrow \mathbb{R}^m$ , is a rule that assigns each $x \in \mathcal{D}$ a value $f(x) \in \mathbb{R}^m$ . Decomposing $f(x)$ into components, we can write such a map as $$f(x) = \left( f_1(x), f_2(x), \dots, f_m(x) \right), \qquad x = \left( x_1, x_2, \dots, x_n \right) \in \mathcal{D} \subset \mathbb{R}^n, $$ where each component $f_i(x), i=1, 2, \dots, m$ , defines a function on $\mathcal{D} \subset \mathbb{R}^n$ . We say that $f(x)$ is differentiable on $\mathcal{D}$ if each one of the component functions $f_i(x), i=1, 2, \dots, m$ , are differentiable on $\mathcal{D}$ . Suppose that $f : \mathcal{D} \subset \mathbb{R}^n \longrightarrow \mathbb{R}^m$ is differentiable map on $\mathcal{D}$ and $$r(t) = \left( r_1(t), r_2(t), \dots, r_n(t) \right), \qquad -1\lt t \lt 1,$$ is a differentiable curve on $\mathbb{R}^n$ that lies in $\mathcal{D}$ . We can use the map $f(x)$ to define a differentiable curve on $\mathbb{R}^m$ by setting $$\gamma(t) = f(r(t)),$$ or equivalently, $$ \gamma(t) = \left( \gamma_1(t), \gamma_2(t), \dots, \gamma_m(t) \right) $$ where $$\gamma_i(t) = f_i \left( r_1(t), r_2(t), \dots, r_n(t) \right), \qquad i = \left( 1, 2, \dots, m \right). $$ We then know the vectors $$v = r'(0) \quad \text{and} \quad w = \gamma'(0)$$ area tangent to the curves $r(t)$ and $\gamma(t)$ at the points $x_0 = r(0) \in \mathcal{D} \subset \mathbb{R}^n$ and $\gamma_0 \in \mathbb{R}^m$ , respectively. Using column notating to denote the tangent vectors $v$ and $w$ , that is, $$ v = \left(\begin{array}{c} v_1 \\ v_2 \\ \vdots \\ v_n \end{array}\right) \quad \text{and} \quad w = \left(\begin{array}{c} w_1 \\ w_2 \\ \vdots \\ w_n \end{array}\right), $$ show that vectors $v$ and $w$ are related by the linear relation $$ w = Df(x_0)v, $$ where $Df(x)$ , for $x \in \mathcal{D} \subset \mathbb{R}^n$ , is the matrix defined by $$ Df(x) = \left(\begin{array}{ccccc} \dfrac{\partial f_1}{\partial x_1}(x) & \dfrac{\partial f_1}{\partial x_2}(x) & \dfrac{\partial f_1}{\partial x_3}(x) &\cdots & \dfrac{\partial f_1}{\partial x_n}(x) \\ \dfrac{\partial f_2}{\partial x_1}(x) & \dfrac{\partial f_2}{\partial x_2}(x) & \dfrac{\partial f_2}{\partial x_3}(x) &\cdots & \dfrac{\partial f_2}{\partial x_n}(x) \\ \dfrac{\partial f_3}{\partial x_1}(x) & \dfrac{\partial f_3}{\partial x_2}(x) & \dfrac{\partial f_3}{\partial x_3}(x) &\cdots & \dfrac{\partial f_3}{\partial x_n}(x)  \\ \vdots & \vdots & \vdots & & \vdots \\ \dfrac{\partial f_m}{\partial x_1}(x) & \dfrac{\partial f_m}{\partial x_2}(x) & \dfrac{\partial f_m}{\partial x_3}(x) &\cdots & \dfrac{\partial f_m}{\partial x_n}(x) \end{array}\right).$$ [Hint: Use the chain rule.] I understand the individual concepts in this problem, but I can't get started on actually answering it. How would one go about putting the pieces together here? Sorry that I haven't done any working, but I need something to get started with.","['multivariable-calculus', 'linear-algebra', 'linear-transformations', 'chain-rule']"
3220197,Crossing number of a graph with an additional edge,"Suppose that I have a graph $G=(V,E)$ with a known crossing number, say $n=cr(G)$ . I would like to show that the graph $G'=(V,E\cup e')$ obtained by adding an edge $e'$ to $G$ that was not previously in it satisfies the relation: $$cr(G')\leq cr(G)+1.$$ Is this true in general? Or at least for the family of complete graphs $K_{m,n}$ ?","['graph-theory', 'combinatorics']"
3220219,Law of large numbers for dependent random variables with fixed covariance,"Given identically distributed random variables $X_1,...,X_n$ , where $ \mathbb E X_i = 0$ , $\mathbb E [X_i^2] = 1$ , and $\mathbb E [X_i X_j] = c<1$ , define $S_n = X_1 + ... + X_n$ . Will $$\frac{S_n}{n} \rightarrow 0$$ in probability as $n\rightarrow \infty$ ? The closest question I could find is this one, where an additional constraint is placed on the covariances, such that Chebyshev inequality can be used to bound $\left|\frac{S_n}{n} \right|$ by $\text{Var}\left(\frac{S_n}{n}\right)$ . However, in my case $\text{Var}\left(\frac{S_n}{n}\right)$ is constant, so this approach will not work. Indeed, when I simulate using MATLAB on an example where I generate Gaussian random variables which all have covariance $c=0.1$ , I do not see it approaching zero. rng(1)
NN =20000;
C = ones(NN,NN)*0.1 + 0.9*diag(ones(NN,1));
rndnums = mvnrnd(zeros(NN,1), C, 1);
NNs = 1:20:NN
means = -99*ones(1,length(NNs));
for ti=1:length(NNs)
    means(ti) = mean(rndnums(1,1:NNs(ti)));
end
scatter(NNs ,means, '.'); xlabel('n'); ylabel('mean') Intuitively, since all the variables are all ""locked"" to eachother in correlation, I can't imagine how their mean will ""eventually"" reach zero. Is there a law of large numbers in this context? If so, what is the rate of convergence? Or if not, is there any way to tell what it will converge to?","['law-of-large-numbers', 'probability-theory']"
3220229,Borel-measurable function,"The original question is:
 Let $(\Omega, \mathcal{F})$ a measurable space with $\mathcal{F}=({\emptyset , \Omega})$ . Show that a function $f:\Omega\to\mathbb{R}$ is borel-measurable, if and only if $f$ is constant.
I know that the inverse image of any real interval is $\emptyset$ or $\Omega$ , thence if is $\emptyset$ the image of all element of $\Omega$ is in the complement of the interval, and if it is $\Omega$ all element of $\Omega$ have image in the interval, but I don't know how to continue this proof. Can you help me?.","['measurable-functions', 'probability-theory', 'probability']"
3220253,Implicit Function Theorem with $x^2+y^2+z^2=\psi(ax+by+cz)$,"Given the equation $x^2+y^2+z^2=\psi(ax+by+cz)$ , with $a,b,c\in\mathbb{R},\ c\neq 0$ , and $\psi:\mathbb{R}\to\mathbb{R}$ that satisfies $\psi\in C^2,\ \psi(0)=0,\ \psi'(0)\neq0$ , prove that in a neighborhood of $(0,0,0)$ the solutions of the equation can be expressed as $(x,y,f(x,y))$ , with $f$ differentiable in that neighborhood, and find $\frac{\partial f}{\partial x}(0,0)$ and $\frac{\partial f}{\partial y}(0,0)$ . My idea is to define a function $F:\mathbb{R^3}\to\mathbb{R},\ F(x,y,z)=(x^2+y^2+z^2-\psi(ax+by+cz))$ . In the first part we have to prove that $F$ verifies the Implicit Function Theorem (IFT) in $p$ = $(0,0,0)$ : $F(0,0,0)=\psi(0)=0,\ F\in C^2(\mathbb R)$ (so it's $C^2$ 'near' $p$ ), and the third requirement is that $\frac{\partial F}{\partial z}=0$ , $\frac{\partial F}{\partial z}=2z-c\psi'\neq 0$ when evaluated in $p$ , because $c\neq0$ and $\psi'(0)\neq0$ , so applying the IFT we are done with the first part of the exercice. We can express $F$ as $F(x,y,f(x,y))=(x^2+y^2+(f(x,y))^2-\psi(ax+by+cf(x,y)))$ . Now we want to compute $\frac{\partial f}{\partial x}(0,0)$ and $\frac{\partial f}{\partial y}(0,0)$ : $\frac{\partial F}{\partial x}(x,y)=2x+2f(x,y)\frac{\partial f}{\partial x}(x,y)-(a+c\frac{\partial f}{\partial x}(x,y))\frac{\partial\psi}{\partial x}\to\frac{\partial F}{\partial x}(0,0)=-(a+c\frac{\partial f}{\partial x}(0,0))\frac{\partial\psi}{\partial x}$ But I don't know how to continue. Could you help me? Is the first part correctly done? Thanks!","['multivariable-calculus', 'functions', 'implicit-differentiation', 'implicit-function-theorem']"
3220269,How the curvature of a curve affects its behavior relative to a circle,"I'm trying to prove Osserman's third lemma in his proof of the Four Vertex Theorem: Lemma 3. Let a smooth oriented unit speed curve $\gamma$ have the same unit tangent vector $\vec{t}$ at a point $P$ as a positively oriented circle $C$ of radius $R$ . Let $\kappa$ be the curvature of $\gamma$ . Then if $\kappa(P) > 1/R$ , a neighborhood of $P$ on $\gamma$ lies inside $C$ , while if $\kappa(P) < 1/R$ , a neighborhood of $P$ on $\gamma$ lies outside $C$ . It seems intuitive that that if the curvature of $\gamma$ is less than the curvature of the circle, then a portion of the $\gamma$ near $P$ should lie outside of the circle, while if the curvature of $\gamma$ is greater than the curvature of the circle, then a portion of the $\gamma$ near $P$ should lie inside of the circle. I'm having trouble finding a rigorous solution, though, and I haven't been able to find anything relevant online. My attempt at a proof Consider the case where $\kappa(P) > 1/R$ at $P$ . Now the osculating circles of $\gamma$ must have radius $\frac{1}{\kappa}<R$ . By assumption, $\gamma$ and $C$ have the same unit normal vector at $P$ , so the osculating circles of $\gamma$ on lie inside $C$ . If $\kappa' \neq 0$ on $P$ , we can show algebraically that the osculating circle of $\gamma$ intersects with $\gamma$ on a neighborhood of $P$ , and we are done. If $\kappa$ is constant on a neighborhood of $P$ , then $\gamma$ must coincide with its osculating circle on that neighborhood, and we are also done. Otherwise, since $\gamma$ is smooth, there is a point infinitesmally close to $P$ with $\kappa' \neq 0$ and $\kappa(P) > 1/R$ , so we have the same result. The case where $\kappa(P) < 1/R$ is extremely similar. This feels very hand-wavy and probably incorrect -- especially in the case where $\kappa' = 0$ . (I found that requirement for the intersection with the osculating circle from an answer on here). Any advice would be appreciated!","['curves', 'curvature', 'differential-geometry']"
3220286,Tricky Trigonometry Conundrum?,"I need to develop $\cos\theta$ so that the expression doesn't contain $x$ (or any expressions containing $x$ ). The point $P$ is at $(x,y)$ . Given are: $|AB| = D = \text{constant}$ $|AP|+|BP|= L = \text{constant}$ Despite several attempts I just don't seem to be able to get rid of $x$ . Is it even possible or am I overlooking something?",['trigonometry']
3220297,Series that evaluates to different values,"I am trying to find a sequence $x_{pq}$ such that $\sum_{p=1}^\infty\sum_{q=1}^\infty x_{pq}$ and $\sum_{q=1}^\infty\sum_{p=1}^\infty x_{pq}$ both exist and evaluate to different values. I am thinking of this as a matrix, but I can't seem to see how adding all the elements of it column wise vs row-wise would change the total sum? Any hints/examples would be helpful.","['summation', 'sequences-and-series', 'real-analysis']"
3220362,How to prove $\det(I_m+AA^t)=\det(I_n+A^tA)$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $A \in M_{m \times n}({\Bbb F})$ , prove $$ \det\left(I_m + AA^t\right) = \det\left(I_n + A^tA\right) $$ I don't need the full answer, maybe a hint. I've tried using Sylvester's identity but I can't solve either way. I tried to replicate the proof from here , but I wondered if there were other proofs, that is why I asked for a hint, this is not homework.","['matrices', 'determinant', 'linear-algebra']"
3220423,A subset of $\mathbb{N}$ is finite if and only if it has a maximum element,"I have been stumped on both directions of the proof for quite some time now. For the forward direction, I initially naively thought I could use the definition of finite (bijection $f: S \rightarrow  [m] $ for some $m \in \mathbb{N}$ , and some subset of $S\subseteq\mathbb{N}$ ) and declare $f^{-1}(m)$ to be the greatest element; I can't because the bijection does not guarantee the mapping in order. I have thought about using a recursive definition but I feel like there is another easier way to go about doing this with maybe composition of functions? Any suggestions and advice?",['elementary-set-theory']
3220432,Show $\alpha^{-1}$ is a Dedekind Cut,"Problem statement: Given an arbitrary positive Dedekind Cut $\alpha = A|B$ , prove that the following $\begin{align*}\alpha^{-1} &= C|D \\
&= \{r\in\mathbb{Q}: r\leq 0 \text{ or } \exists b\in B, \text{ not the smallest element of } B, r = \frac{1}{b} \}|\text{ rest of } \mathbb{Q}\end{align*}$ is a Dedekind Cut. A Dedekind Cut $X|Y$ fulfills three  conditions: $X\cup Y = \mathbb{Q}, X\neq\emptyset, Y\neq\emptyset, X\cap Y\neq\emptyset$ If $x\in X$ and $y \in Y$ then $x<y$ . $X$ has no largest element. Proof attempt: $0\in C, \frac{1}{a}\in D$ since $\frac{1}{a}\not\in C$ for all $a\in A$ and trivially from the definition $C\cup D=\mathbb{Q}, C\cap D = \emptyset$ . $c<\frac{1}{\inf(B)}\leq d\hspace{1mm}\forall c\in C \hspace{1mm}\forall d\in D$ (WRONG) Suppose $C$ has a maximal element $\gamma$ . Then it is true that $\gamma < \frac{1}{b}$ from the definition. But then $\gamma = \frac{\gamma}{2} + \frac{\gamma}{2} < \frac{\gamma}{2} + \frac{1}{2b} < \frac{1}{2b} + \frac{1}{2b} = \frac{1}{b}$ . So there is no maximal element for $C$ . (WRONG) So $\alpha^{-1}$ is a cut. I suspect that showing $\alpha \cdot \alpha^{-1} = 1$ is non trivial but I'll leave that for another post.","['elementary-set-theory', 'real-numbers', 'proof-verification', 'real-analysis']"
3220444,"Does there exist a continuous function $f: \Bbb R\to \Bbb R$ that is rational at (Lebesgue) almost every irrational, and irrational at every rational?","Does there exist a continuous function $$f: \mathbb{R} \rightarrow \mathbb{R}$$ that is rational at (Lebesgue) almost every irrational, and irrational at every rational? Some thoughts: for some $q\in \mathbb{Q}$ , $f^{-1}(q)$ is a closed set of positive measure disjoint from the rationals. But of course that's possible.  Though it does at least mean that the null set of irrationals where $f$ is irrational is dense in $\mathbb{R}$ , by the Baire Category theorem, since the set is the complement of the countable union of nowhere dense sets: $$\big(\mathbb{Q}\cup(\cup_{q\in\mathbb{Q}} f^{-1}(q))\big)^c$$ Also if it exists, $f$ can't be injective on any open interval. So the Cantor function comes to mind, but that obviously doesn't work since it's locally constant on an open set. Maybe a ""nowhere injective"" function is guaranteed to be locally constant somewhere?  I have no idea. (I saw this asked on reddit, and after thinking for a while I'm too curious not to ask here.)","['general-topology', 'measure-theory', 'analysis']"
3220445,"How to properly represent a conditional expectation $E(Y\mid X,Z)$ if there are values of $X$ and $Z$ that cannot simultaneously exist?","Suppose $Y, X, Z$ are all discrete random variables. Then the conditional mean of $Y$ on levels of $X=x$ and $Z=Z$ is represented by $E(Y\mid X=x,Z=z)$ now suppose that there are values of $x_1 \in X$ and $z_1 \in Z$ such that both cannot simultaneously exist. An example might be if $Y, X, Z$ were vectors of observations corresponding to people, and so such person had $x_1, z_1$ simultaneously. In such a case, $E(Y\mid X=x_1,Z=z_1)$ is undefined. Is there a way define the conditional expectation in a notationally correct way? One thought is using the indicator function $\mathbb{1}(X=x_1,Z=z_1)E(Y\mid X=x_1,Z=z_1)$ but $E(Y\mid X=x_1,Z=z_1)$ is still undefined and multiplying by zero seems to be a hacky fix. Example: For an example, consider $Y$ to be mortality, where $Y=1$ is death, $Y=0$ is living. Then, suppose $X$ is gender, such that $X=1$ is female, $X=0$ male and that $Z$ is whether one had ovarian cancer before ( $Z=1$ ). Then, $$
E(Y\mid X=0, Z=1)
$$ can't be computed because to have ovarian cancer implies one is female, therefore we cannot compute it since $X=0$ is male (making the assumption only females can get ovarian cancer). Further Motivation: I am also motivated by how the law of total probability might work under such cases. For example, suppose we wanted to find $E(Y\mid X) but only had information at the level of $ X $ and $ Z$. We can therefore use the law of total probability to have: $$
E(Y\mid X) = \sum_z E(Y\mid X,Z=z)P(Z=z\mid X)
$$ However, in such cases, sometimes, like the example above, $E(Y\mid X,Z=z)$ is undefined at certain values. How might the law of total probability work in this case? I would appreciate any insights. Thanks!","['statistics', 'probability']"
3220493,Prove that three common chords are concurrent,"Three circles intersect each other as shown. Prove that the three common chords are concurrent. Now the book does this by proving that the chord out of E and through M is the same for circle (2) and circle (3) which is very easy to show by using the intersecting chord theorem. However, why does this imply that all three chords are concurrent at M?","['euclidean-geometry', 'circles', 'geometry']"
3220526,Definition of degree of invertible sheaf?,"In Hartshorne's, he states that # $C\cap D=deg_C(\mathcal{F}(D)\otimes\mathcal{O}_C)$ , where $deg_C$ denotes the degree of the invertible sheaf $\mathcal{F}(D)\otimes\mathcal{O}_C$ . I couldn't find any explicit definition about degree of sheaf in the book, but there are some implications. Since divisor $D$ and the corresponding invertible sheaf $\mathcal{F}(D)$ is one to one. So the degree of an invertible sheaf means the degree of the corresponding divisor. Is that right?","['divisors-algebraic-geometry', 'algebraic-geometry']"
3220564,Find the value of $\lim\limits_{n\rightarrow \infty}\left( \frac{2^{-n^2}}{\sum_{k=n+1}^{\infty}2^{-k^2}}\right)$,"Find the value of $$\lim_{n\rightarrow \infty}\left( \frac{2^{-n^2}}{\sum\limits_{k=n+1}^{\infty}2^{-k^2}}\right)$$ My answer: Take $r_{n+1} = \sum\limits_{k=n+1}^{\infty}2^{-k^2}$ . If $\lim\limits_{n\rightarrow \infty}\left( \dfrac{2^{-n^2}}{\sum\limits_{k=n+1}^{\infty}2^{-k^2}}\right)$ exists, then \begin{align*}
1+\lim\limits_{n\rightarrow \infty}\left( \dfrac{2^{-n^2}}{\sum\limits_{k=n+1}^{\infty}2^{-k^2}}\right)&=\lim\limits_{n\rightarrow \infty}\left( 1+\dfrac{2^{-n^2}}{\sum\limits_{k=n+1}^{\infty}2^{-k^2}}\right)\\
&=\lim\limits_{n\rightarrow \infty}\left( \dfrac{\sum\limits_{k=n}^{\infty}2^{-k^2}}{\sum\limits_{k=n+1}^{\infty}2^{-k^2}}\right)\\
&=\lim\limits_{n\rightarrow \infty}\dfrac{r_n}{r_{n+1}}=??
\end{align*} Moreover, I know $r_n$ is a remainder of the series $\sum\limits_{k=1}^{\infty}2^{-k^2}$ , which is convergent. $~~~~\left(\mbox{ By ratio test,} ~~\lim\limits_{n\rightarrow\infty} \dfrac{2^{-(n+1)^2}}{2^{-n^2}}=\lim\limits_{n\rightarrow\infty}\dfrac{1}{2.2^{2n}}=0<1 \right)$","['limits', 'sequences-and-series']"
3220597,Subrings $A$ of $\mathbb{F}_p[x]$ such that $\dim_{\mathbb{F}_p}\mathbb{F}_p[x]/A=1$.,"Suppose we have the ring $\mathbb{F}_p[x]$ of polynomials with coefficients in the Galois field with $p$ prime number elements. I want to find all subrings $A$ containing the identity such that when considered as vector spaces over $\mathbb{F}_p$ , the quotient $\mathbb{F}_p[x]/A$ is isomorphic to $\mathbb{F}_p$ . Also want to find the isomorphism classes of these subrings. So far I have only been able to find one subring: the ring generated by $1,x^2,x^3$ . I know that if $x\in A$ , then $A=\mathbb{F}_p[x]$ , so I don't want $n\cdot x$ to be in $A$ . After this, trying to find other subrings, it becomes really messy from my point of view. So I don't think this might be a good way to approach the problem. It also seems to me that we can view this as an extension problem: $$0\rightarrow A\rightarrow \mathbb{F}_p[x]\rightarrow\mathbb{F}_p\rightarrow 0$$ And for groups, I know that using the second homology group would help if I were given $Q$ and $N$ but not $G$ in the following sequence: $$0\rightarrow N\rightarrow G\rightarrow Q\rightarrow 0$$ My questions are if there is a standard method for computing $N$ given $G$ and $Q$ , and if these methods generalize to ring extensions? Any help would be appreciated, specially references to literature.","['finite-fields', 'reference-request', 'abstract-algebra', 'polynomial-rings', 'group-theory']"
3220645,How to find $\int_0^{{\pi}/{2}} (\pi x-4x^2)\log(1+\tan x)\mathrm dx$,"How do you find the integral of $$
\int_0^{\pi/2}\left(\pi x - 4x^{2}\right)
\log\left(1 + \tan\left(x\right)\right)\,\mathrm{d}x
$$ The integral can be simplified to $$
\int_0^{\pi/2}x\left(4x + {\pi}\right)
\ln\left(1 - \dfrac{\mathrm{i}\left[\mathrm{e}^{\mathrm{i}x}-\mathrm{e}^{-\mathrm{i}x}\right]}{\mathrm{e}^{\mathrm{i}x}+\mathrm{e}^{-\mathrm{i}x}}\right)
\,\mathrm{d}x$$","['integration', 'definite-integrals', 'closed-form']"
3220705,Computing volume inside a ball and outside a cylinder,"I try to calculate the volume inside the ball $\ x^2 + y^2 + z ^2 = 4 $ the outside the cylinder $\ x^2+y^2=2x $ using double integral. 
Since the shape is symmetric I chose $\ z = 0 $ as bottom limit and the ball is the upper limit. I try to convert to polar form and so I get $$\ x^2 + y^2 + z ^2 = 4 \Rightarrow z = \sqrt{4-r^2} \\x^2 + y^2 = 2x \Rightarrow r = 2 \cos \theta $$ therefore the integral should be $$\ 4 \cdot \int_0^{\pi/2} \int_0^{2\cos\theta} (\sqrt{4-r^2}) \ r \ dr \ d \theta$$ but this integral is way to messy for me to calculate. I mean first step of calculating integral for $\ r $ is okay but then the value I get and calculating integral of $\ \theta $ is beyond me and I believe it is beyond the scope of the course. I guess I'm missing something in the process here?","['analytic-geometry', 'multivariable-calculus', 'multiple-integral', 'volume']"
3220753,Does there exist polynomials $P$ s.t. $P(k)\mid k!$ holds for only finitely many $k\in\Bbb N$?,"Question: Does there exist polynomials $P$ with integer coefficients such that $$P(k)\mid k!\tag{*}$$ holds for only finitely many $k\in\Bbb N$ ? I think such polynomial doesn't exist (don't know how to prove it). For some specific polynomials, we can prove there exists infinitely many $k\in\Bbb N$ satisfying $(*)$ by constructing $k=\text{a polynomial}$ : Example: $P(k)=k^n-1$ , $n\in \Bbb N$ Choose distinct primes $p_1,p_2,\dots,p_j$ coprime to $n$ such that $$\ell=\prod_{i =1}^j \frac{p_i-1}{p_i}<\frac 1n.$$ Let $m=p_1p_2\cdots p_j$ and $k=r^{m}$ , $r\in\Bbb N$ , then $$P(k)=P(r^m)=r^ {mn}-1=\prod_{d\mid mn 
}\Phi_d(r),$$ where $\Phi_d(r)$ are cyclotomic polynomials. Note that $$\deg \Phi_d=\phi(d)\le \phi(mn)\le m\phi(n)\le mn \ell<m,$$ Therefore for integer $r>N$ , we have $r^m>\Phi_d(r)$ and thus $$\left(\prod \Phi_d(r)\right)\mid\left(r^m\right)!\quad \iff\quad k^n-1\mid k!.$$ With the aid of Mathematica, ""surprisingly"" I found that the growth rate of the number of integers smaller than $n$ satisfying $(*)$ approximately equals $C\cdot n$ for some constants $C$ : \begin{align*}
P(k)&=k^2-1,\quad \#(k\le n \text{ satisfying } (*))\sim 0.85 n,\\
P(k)&=k^2+1,\quad \#(k\le n \text{ satisfying }(*))\sim 0.23 n.
\end{align*} Here are some graphs for different $P$ : $x$ -axis: $n$ $y$ -axis: $\#(k\le n\text{ satisfying }(*))$ Graph 1: $P(k)=k^2-1$ Graph 2: $P(k)=k^6-6k^4+9k^2-1$ Conjecture: For any polynomial $P$ with integer coefficients, there are infinitely many $k\in \Bbb N$ satisfying $(*)$ , and the growth rate $$\#(k\le n\text{ satisfying }(*))\sim C\cdot n$$ for large $n$ and a constant $C$ .","['number-theory', 'divisibility']"
3220783,What is Summability Calculus?,"I came across a book called ""Summability Calculus, The first book in the literature, which is devoted to fractional finite sums as an object of study on its own right."" ( See: https://www.springer.com/gp/book/9783319746470 ) What is Summability Calculus? Is it an entirely new field? Or is it a part of Discrete Mathematics / Discrete Calculus? Fields which study finite sums. What justifies the new name?","['calculus', 'discrete-mathematics']"
3220792,Is argmin attained equivalent to local convexity?,"I have the following optimization problem: $$
\theta^* \in \arg \min_\theta f(\theta)
$$ $$
\widehat{\theta^*} \in \arg \min_\theta \widehat{f}(\theta)
$$ With $f(\theta) = \mathbb{E}(g(X; \theta))$ ( $X$ is a r.v.) and $\widehat{f}$ its empirical counterpart: $\widehat{f}(\theta) = \frac{1}{M}\sum_m^Mg(X^m;\theta)$ ( $X^m$ s are iid). I only assume $g$ is continuous (typically it is a neural network). I know that for $\theta$ fixed, $f(\theta)-\widehat{f}(\theta)=\mathcal{O}_\mathbb{P}(1/\sqrt{M})$ (CLT). I can also prove results like $\sup_\theta |f(\theta)-\widehat{f}(\theta)|\rightarrow_{M\rightarrow +\infty}0$ if I use properties of my neural networks $g(.;\theta)$ . Now my question is, if I assume that the $\arg \min$ is attained for both $\theta^*$ and $\widehat{\theta^*}$ , can I have a result that ""transfers"" the convergence property of $\widehat{f} \rightarrow f$ to $\widehat{\theta^*} \rightarrow \theta^*$ , like: $$
\theta^*-\widehat{\theta^*}=\mathcal{O}_\mathbb{P}(1/\sqrt{M}) \quad (1)
$$ Moreover, back to the title of the question - if I assume that the $\arg \min$ is attained (say, with a good initialization $\theta_0$ and a stochastic gradient descent), is it equivalent to saying that $f$ and $\widehat{f}$ are strictly convex in a neighbourhood of $\theta_0$ , in which case it would be easier to prove $(1)$ (how?) Thank you","['probability-limit-theorems', 'neural-networks', 'optimization', 'convergence-divergence', 'probability-theory']"
3220855,Are any values of this sum involving the Thue Morse sequence known?,"Let $t_n$ denote the $n^{\rm th}$ term in the Thue Morse sequence. Note that $t_n=1$ if the number of $1$ s in the binary expansion of $n$ is odd, $0$ otherwise. Now define a variant of the Riemann Zeta function as follows: $$
\zeta_{TM}(s) = \sum_{n\geq0} \frac{t_{n}}{(n+1)^s}
$$ for $ \rm{Re}(s)>1$ . Are any values of $\zeta_{TM}(s)$ known? Is there some kind of closed form formula for this (I highly doubt there is, but one never knows). A closely related sum for which a value is known is: $$
\sum_{n\geq1} \frac{s_{n}}{n(n+1)} = 2\ln2,
$$ where $s_n$ is the binary sum-of-digits function. Another related sum gives the Prouhet-Thue-Morse constant, which has been shown to be transcendental: $$
\tau =\sum _{{n\geq0}}{\frac  {t_{n}}{2^{{n+1}}}}=0.412454033640\ldots 
$$","['zeta-functions', 'sequences-and-series']"
3220878,"""A manifold with boundary has dimension at least 1"" if it has a dimension and if it has nonempty boundary?","My book is An Introduction to Manifolds by Loring W. Tu. As can be found in the following bullet points Can a topological manifold be non-connected and each component with different dimension? Is $[0,1) \cup \{2\}$ a manifold with boundary? My issue is the $2$. Understanding topological and manifold boundaries on the real line we have that Tu's manifolds with or without boundaries do not necessarily have (uniform) dimensions. Tu has considered manifolds to be manifolds with boundaries (with empty boundaries). Question: For Definition 22.6 (see here and here ), Tu says that ""A manifold with boundary has dimension at least 1"". Should this instead be ""A manifold with boundary has dimension at least 1 if it has a dimension and if it has nonempty boundary"" or ""An $n-$ manifold with boundary with non-empty boundary has $n \ge 1$ "" (Notice that the prefix "" $n-$ "" precisely gives the manifold with boundary a dimension)? Embedding photos:","['geometry', 'manifolds', 'general-topology', 'differential-topology', 'differential-geometry']"
3220906,Fibonacci numbers as weighted averages of powers of 5 -- a bijective/combinatorial proof?,"A straightforward use of the binomial formula shows that the nonrecursive formula for the Fibonacci numbers, $$
F_n=\frac{1}{\sqrt{5}}\left(\left(\frac{1+\sqrt{5}}{2}\right)^n-\left(\frac{1-\sqrt{5}}{2}\right)^n\right),
$$ is equivalent to $$
F_n=\frac{1}{2^{n-1}}\sum_k{\binom{n}{2k+1}5^k}=\frac{\sum_k{\binom{n}{2k+1}5^k}}{\sum_k{\binom{n}{2k+1}}}.
$$ However, I don't think I've seen a bijective proof of that identity, or perhaps, its division-free version, $$
2^{n-1}F_n=\sum_k{\binom{n}{2k+1}5^k}.
$$ Most likely, I am simply unaware of an existing proof (who knows, maybe even on MSE). Do you know of any references where this is proved? Alternatively, a combinatorial proof would be great. Thanks!","['fibonacci-numbers', 'combinatorics', 'combinatorial-proofs', 'reference-request']"
3220912,Is $x^3-6xy+y^2=-108$ a regular submanifold but not a regular $k$-submanifold?,"My book is An Introduction to Manifolds by Loring W. Tu. Let $S = \{x^3-6xy+y^2=-108\}$ , and let ""submanifold"" and "" $k$ -submanifold"" mean, respectively, ""regular"" and ""regular $k$ -submanifold"". As in here , we have that Tu's manifolds with or without boundaries do not necessarily have dimensions. Do Tu's (regular) submanifolds, however, necessarily have dimensions? Here is Definition 9.1 of (regular) submanifolds, which seems to have dimensions. But now consider Problem 9.1 : The answer given is all real numbers except $0$ and $-108$ . A solution given by Richard G. Ligo claims that the reason (or a reason) why $x^3-6xy+y^2=-108$ is not a (regular) submanifold of $\mathbb R^2$ is that connected components do not have the same dimension. I think we must have either that Ligo's solution is wrong. Tu's submanifolds have dimensions and so $S$ is not a submanifold (i.e. $k$ -submanifold, in this case) of $\mathbb R^2$ because of the connected components and no other reason . Tu's regular submanifolds have dimensions and so $S$ is not a submanifold of $\mathbb R^2$ because of the connected components, but there are other reasons why $S$ is not a $k$ -submanifold of $\mathbb R^2$ . Tu intended a definition that allows submanifolds to not have dimensions. However, $S$ is neither a submanifold nor a $k$ -submanifold of $\mathbb R^2$ for a different reason . Tu intended a definition that allows submanifolds to not have dimensions and should have allowed $S$ to be a submanifold of $\mathbb R^2$ even though $S$ is not a $k$ -submanifold of $\mathbb R^2$ . Thus each nonzero $c$ gives a submanifold with or without uniform dimension (same dimension for each connected component), while $c=-108$ is the only nonzero value that gives submanifold without uniform dimension. Update : I asked Hello Prof Tu, I replied on stackex, but anyway for your convenience: It's actually just that your answer excluded -108. I think you meant to exclude -108 for submanifold with uniform dimension (same dimension for each connected component) but to include -108 for submanifold with non-uniform dimension? ..., and Prof Tu replied ...The critical values are 0 and -108, but the inverse image of -108 is a regular submanifold.  Your interpretation is correct...","['geometry', 'manifolds', 'general-topology', 'differential-topology', 'differential-geometry']"
3220926,"Decidability of the membership problem for Baumslag Solitar group $BS(1,2)$","Is the subsemigroup or subgroup membership problem for $BS(1,2)$ decidable ? That is, given elements $g,g_1,g_2,\dots,g_n$ from $BS(1,2)$ , is there a decision procedure to check whether $g$ belongs to the semigroup/subgroup generated by $\{g_1,g_2,\dots,g_n\}$ ? Note that the decidability of the semigroup membership implies the decidability of subgroup memebership problem and the undecidability of the subgroup membership problem implies the undecidability of the semigroup membership problem.","['group-theory', 'abstract-algebra', 'decidability']"
3220971,Exercise about sub-$\sigma$-algebra of $\mathcal{B}(\mathbb{R})$,"Let $C=\{(-a, a): a \in \mathbb{R}\}$ and $F=\sigma(C)$ . Prove that $F=\mathcal{B}(\mathbb{R})\cap\{A\subseteq\mathbb{R}: A=-A\}$ . I don't have problems in proving $F\subseteq \mathcal{B}(\mathbb{R})\cap\{A\subseteq\mathbb{R}: A=-A\}$ , I'd like a confirm for the other inclusion. Let $F^+=\{A \cap [0,+\infty): A \in F\}$ , then $F^+=\mathcal{B}[0,+\infty)$ : $F\subseteq \mathcal{B}(\mathbb{R}) $ and $[0,+\infty) \in \mathcal{B}(\mathbb{R})$ imply $F^+\subseteq \mathcal{B}[0,+\infty)$ ; $\forall a,b\in[0,+\infty)$ we have $(a,b)=[0,b) \cap \cup_n[a+n^{-1},+\infty) \in F^+$ , but $\mathcal{B}[0,+\infty) $ is generated by those intervals so $F^+\supseteq \mathcal{B}[0,+\infty)$ ; hence the equality. Now take $B\in  \mathcal{B}(\mathbb{R})\cap\{A\subseteq\mathbb{R}: A=-A\}$ ; we have $B\cap[0,+\infty)\in \mathcal{B}[0,+\infty)=F^+$ . So there exists $A\in F$ such that $A\cap[0,+\infty)=B\cap[0,+\infty) $ which means that A and B have the same positive elements, and since they are equals to their opposites they also have the same negative elements. Therefore $B=A\in F$ , from which $$F\supseteq \mathcal{B}(\mathbb{R})\cap\{A\subseteq\mathbb{R}: A=-A\}$$","['real-numbers', 'measure-theory', 'proof-verification', 'proof-writing', 'borel-sets']"
3220990,Evaluate $\lim\limits_{x \to 0}\frac{e^{(1+x)^{\frac{1}{x}}}-(1+x)^{\frac{e}{x}}}{x^2}$ [duplicate],"This question already has answers here : Evaluate $\lim_{x\to 0}\frac{e^{(x+1)^{1/x}}-(x+1)^{e/x}}{x^2}$ (3 answers) Closed 5 years ago . Solution Expanding $(1+x)^{\frac{1}{x}}$ at $x=0$ by Taylor's Formula，we obtain \begin{align*}
(1+x)^{\frac{1}{x}}&=\exp\left[\frac{\ln(1+x)}{x}\right]=\exp \left(\frac{x-\frac{x^2}{2}+\frac{x^3}{3}+\cdots }{x}\right)\\
&=\exp \left(1-\frac{x}{2}+\frac{x^2}{3}+\cdots \right)=e\cdot\exp \left(-\frac{x}{2}+\frac{x^2}{3}+\cdots \right) \\
&=e\left[1+\left(-\dfrac{x}{2}+\dfrac{x^2}{3}+\cdots \right)+\frac{1}{2!}\left(-\dfrac{x}{2}+\dfrac{x^2}{3}+\cdots \right)^2+\cdots\right]\\
&=e\left(1-\frac{x}{2}+\frac{11}{24}x^2+\cdots\right)\\
&=e-\frac{ex}{2}+\frac{11}{24}ex^2+\cdots
\end{align*} Likewise, expanding $e^{(1+x)^{\frac{1}{x}}}$ at $x=0$ , we obtain \begin{align*}
e^{(1+x)^{\frac{1}{x}}}&=(e^e)^{1-\frac{x}{2}+\frac{11}{24}x^2-\cdots}=e^e\cdot (e^e)^{-\frac{x}{2}+\frac{11}{24}x^2+\cdots}\\
&=e^e\cdot\left[1+\left(-\frac{x}{2}+\frac{11}{24}x^2+\cdots\right)\ln e^e+\frac{1}{2!}\left(-\frac{x}{2}+\frac{11}{24}x^2+\cdots\right)^2\ln^2 e^e+\cdots\right]\\
&=e^e \cdot\left[1-\frac{ex}{2}+\frac{1}{24}(11e+3e^2)x^2+\cdots\right]
\end{align*} Expanding $(1+x)^{\frac{e}{x}}$ at $x=0$ , it follows that \begin{align*}
(1+x)^{\frac{e}{x}}&=\exp\left[\frac{e\ln(1+x)}{x}\right]=\exp \left(e\cdot\frac{x-\frac{x^2}{2}+\frac{x^3}{3}+\cdots }{x}\right)\\
&=\exp \left(e-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right)=e^e\cdot\exp \left(-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right) \\
&=e^e\left[1+\left(-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right)+\frac{1}{2!}\left(-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right)^2+\cdots\right]\\
&=e^e\left[1-\frac{ex}{2}+\frac{1}{24}e(8+3e)x^2+\cdots\right]
\end{align*} Therefore \begin{align*}
&\lim_{x \to 0}\frac{e^{(1+x)^{\frac{1}{x}}}-(1+x)^{\frac{e}{x}}}{x^2}\\
=&\lim_{x \to 0}\frac{e^e\cdot\left[1-\dfrac{ex}{2}+\dfrac{1}{24}e(11+3e)x^2+\cdots\right]-e^e\cdot\left[1-\dfrac{ex}{2}+\dfrac{1}{24}e(8+3e)x^2+\cdots\right]}{x^2}\\
=&e^e\cdot\frac{1}{8}e\\
=&\frac{1}{8}e^{e+1}
\end{align*} Please check. Is there any more simpler solution?","['limits', 'proof-verification']"
3220995,Some questions on the nature of statistical/probabilistic deduction,"I have been doing some reading and thinking on the nature of statistical inference and the way formal statistics models an event seems a bit strange to me. Here is how I like to think about it: In the deterministic world, the most basic way of trying to model something is to try to fit a function $y=f(x)$ that models input x to output y. The way this is usually is done is that either you have some theoretical calculations that suggests you the form of $f(x)$ or you look at many data and make a guess (or some computational fitting procedure). In the first case you simply test your guess function against your data by calculating mean absolute error between observed results $y_{obs}$ and the calculated ones $y_{calc}$ via the function $f$ given the inputs $x$ . In the second case when you are training your $f(x)$ based on data then you train on some training subset and test it on some validation subset, but again using mean absolute error. On the other hand when you want to model your data probabilistically, i,e it is too hard or impossible to build $f(x)$ , you try to come up with a probability distribution $p(x)$ that tells you how likely it is to get some set of outcomes each time you make an experiment. In this case you have to test your error on the level of the probability distribution. The way I would naturally do is as follows: I assume that you have some guess $p_0(x)$ about what the probability would be like. Then I take set of experimental outcomes $\{x_i\}_{i=1}^N$ and look at the measure $p^N(x)= \frac{1}{N}\sum_{i=1}^N \delta_{x_i}(x)$ . 
One can calculate the difference between $p^N$ and $p_0$ using a norm defined on the space of probability measures. If my guess $p_0$ is really good and $p^N$ converges to $p_0$ strongly then anyways the norm measured in any of these will tend to $0$ as $N$ goes to $\infty$ . You can probably only spot differences for low number of data but then again with low number of data modeling your data because more of a choice as there is too much flexibility in what you can fit. Below is an example where I model data points drawn from a normal distribution (their variance is denoted in the titles as data sigma and all has mean 0). Once I construct $p_N$ from data, I iterate over different values of variance and determine $p_0$ based on which variance gives the less distance to $p_N$ ( I fix mean=0 but that can be varied too). The distance I use is the mean absolute distance between cumulative distribution functions of $p_0$ and $p^N(x)= \frac{1}{N}\sum_{i=1}^N \delta_{x_i}(x)$ .  (a modified version of Kolmogorov distance). I also compare it to the normal distribution which has the variance and mean of the data which I call the unbiased and denote by $\hat{p}$ . In the first figure I compare $p_0$ and $\hat{p}$ with the data on the x-axis. In the second figure I plot cdfs for $p_0$ , $\hat{p}$ , $p^N$ . As can be seen for the cases when there is a lot of data they are almost identical and they are only noticably different for low number of data points. In the first figure the sigma calculated for the $p_0$ is denoted in the titles as Min Dist Gaussian sigma. The distances of $p_0$ and $\hat{p}$ to data are given in the legends. To me this seems like the only sane way of thinking about modelling probabilistic phenomenon. On the other hand in the statistics domain what people do is they first guess a $p_0$ and then an alternate not $p_0$ which I denote as $p_1$ . Looking at $p_0$ and $p_1$ you can somehow determine a region which if most of the data falls then $p_0$ is rejected. This should be somehow a region which is more towards the ""high concentration regions"" of $p_1$ and ""low concentration regions"" of $p_0$ so that you are more confident in your prediction. This just seems to me like a very round about way of measuring some sort of distance between the probability measures $p_0,p_1$ and the experimental measure $ \frac{1}{N}\sum_{i=1}^N \delta_{x_i}(x)$ . However I was not able to formalise this. So my questions are: 1- Is it possible to see this as some sort of norm comparison between these probability measures? 2- If so, what is the reason this particular comparison is used? 3- And finally if we are looking at distance, why do we not try to find some $p(x)$ which minimises the distance rather and rather just compare one $p_1$ and its negatition?","['statistical-inference', 'statistics', 'probability-distributions']"
3221006,"Proving the representability of a functor, which is covered by open subfunctors","I want to prove Theorem 8.9 from Algebraic Geometry I ( U.Görtz, T.Wedhorn), which reads as follows: Let $S$ be a scheme $F: Sch/S°\rightarrow Set $ a functor such that: F is a sheaf for the Zariski topology F has a cover by open subfunctors $\alpha_i:F_i\rightarrow F$ , such that every $F_i$ is representable by a scheme $X_i$ Then F is representable. A cover by open subfunctors means, that for every scheme $T$ and for every morphism $h_T\rightarrow F$ , the pullback $F_i\times_F h_T$ is representable, say by $Y_i$ and the morphism of schemes $Y_i\rightarrow T$ corresponding to the projection $F_i\times_F h_T\rightarrow h_T$ is an open immersion. In addition the images of $Y_i\rightarrow T$ form an open covering of $T$ Let me explain what I have done so far and where I am stuck: The $X_i$ can be glued to a scheme $X$ .
The morphisms $h_{X_i}\cong F_i\rightarrow F$ correspond via the yoneda lemma to elements $f_i\in F(X_i)$ .  Using the sheaf property of F, the $f_i$ glue together to an element $f\in F(X)$ which gives us a natural transformation $\alpha: h_X\rightarrow F$ . For a scheme $T$ and a morphism $g\in\mathrm{Hom}(T,X)$ this is given by $\alpha(T)(g)=F(g)(f)$ . The last step is to show that this assignment is bijective. I managed to show the surjectivity, but can't find a proof for the injectivity. It would suffice to show that the following diagram is a pullback $\require{AMScd}$ \begin{CD}
h_{X_i} @>>> h_X\\@VidVV @VV\alpha V\\ h_{X_i} @>>> F
\end{CD} where the morphism $h_{X_i}\rightarrow h_X$ is induced from the open immersion $X_i\rightarrow  X$ . I tried to prove this with ideas similiar to the answer of Representable open immersion of functors is a monomorphism .
I also looked at the proof in EGA I (Springer edition 1971), where this is Proposition 4.5.4 in chapter 0. From what I understand, it is also used, that this square is a pullback, but not commented on why this is indeed a pullback. I am thankful for any idea on how to proove the injectivity or the fact, that this square is a pullback.","['algebraic-geometry', 'representable-functor']"
3221016,Approximate a density function from sampled data,"Let $(E,\mathcal E,\mu)$ be a measure space $E_0\in\mathcal E$ with $\mu(E_0)\in(0,\infty)$ and $\mathcal E_0\subseteq\left.\mathcal E\right|_{E_0}$ be finite and disjoint with $$E_0=\biguplus\mathcal E_0$$ $f:E\to[0,\infty)$ be $\mathcal E$ -measurable with $$c:=\int f\:{\rm d}\mu\in(0,\infty)$$ and $$f_0:=\frac1{\mu(E_0)}\int_{E_0}f\:{\rm d}\mu$$ $\nu:=c^{-1}f\mu$ $\delta$ denote the Dirac kernel on $(E,\mathcal E)$ and $\delta_x:=\delta(x,\;\cdot\;)$ for $x\in E$ Let $n\in\mathbb N$ and $x_1,\ldots,x_n\in E$ be mutually independent samples drawn from $\nu$ . I would like to approximate $\left.f\right|_{E_0}$ using these samples in a similar way as described here: How can we approximate a function by sampling a distribution proportial to it and making a histogram of samples? . The assumption is that we're not able to compute $c$ , but the average $$f_0:=\frac1{\mu(E_0)}\int_{E_0}f\:{\rm d}\mu$$ of $f$ over $E_0$ with respect to $\mu$ . Moreover, I guess it is assumed that $\left.f\right|_B\approx f_B$ for some $f_B\ge0$ for all $B\in\mathcal E_0$ . Then $$\int_Bf\:{\rm d}\mu=\mu(B)f_B\Leftrightarrow f_B=c\frac{\nu(B)}{\mu(B)}\;\;\;\text{for all }B\in\mathcal E_0.\tag1$$ Letting $X_1,\ldots,X_n$ be identically distributed mutually independent random variables on a common probability space with $X_1\sim\nu$ and $\zeta_n:=\sum_{i=1}^n\delta_{X_i}$ we know that $$\frac1n\zeta_n(B)\xrightarrow{n\to\infty}\nu(B)\;\;\;\text{almost surely for all }B\in\mathcal E\tag3$$ by the strong law of large numbers. In the sense of $(3)$ , $$\nu(B)\approx\frac1n\zeta_n(B)\;\;\;\text{for all }B\in\mathcal E.\tag4.$$ Now we may write $$c=\frac{\mu(E_0)}{\nu(E_0)}f_0\tag5$$ and hence $$f_B=\frac{\mu(E_0)}{\nu(E_0)}f_0\frac{\nu(B)}{\mu(B)}\;\;\;\text{for all }B\in\mathcal E.\tag6$$ Noting that $$\frac1n\sum_{B\in\mathcal E_0}\zeta_n(B)=\frac1n\zeta_n(E_0)\approx\nu(E_0)\tag7,$$ we may approximate $f(x)$ by $$\tilde f(x):=\sum_{B\in\mathcal E_0}1_B(x)f_B=\frac{\mu(E_0)f_0}{\sum_{B\in\mathcal E_0}\zeta_n(B)}\sum_{B\in\mathcal E_0}1_B(x)\frac{\zeta_n(B)}{\mu(B)}\tag8$$ for all $x\in E_0$ . However, in the link above they suggest to compute the average $h$ of samples belonging to an element (a ""bin"" so to say) of $\mathcal E_0$ , i.e. $$h=\frac1{|\mathcal E_0|}\sum_{B\in\mathcal E_0}\zeta_n(B)\tag9$$ and approximate $f(x)$ by $$\hat f(x):=\frac{f_0}h\sum_{B\in\mathcal E_0}1_B(x)\zeta_n(B)\tag{10}.$$ Could anyone make sense of $(10)$ for me? I don't understand why this is a sensible approximation (why taking the average $h$ ?) We may note that $\zeta_n(B)$ is the (random) number of samples lying in $B\in\mathcal E$ and $\sum_{B\in\mathcal E_0}\zeta_n(B)$ is the number of samples lying in $E_0$ . EDIT : The crucial thing is surely the following: The histogram given by $$h(x):=\sum_{B\in\mathcal E_0}1_B(x)\zeta_n(B)\;\;\;\text{for }x\in E_0$$ is an approximation of the ""shape"" of $\left.f\right|_{E_0}$ . So, the only left is to ""scale"" it the right way ...","['statistics', 'approximation', 'probability-distributions', 'probability-theory', 'density-function']"
3221075,"The matrix $A^2+A+m.I_n$ is non singular if $\mbox{ gcd }(m,\det\,A)=1$.","Let $A\in M_{n}(\Bbb{Z})$ , $m>1$ such that $\mbox{ gcd }(m,\det\,A)=1$ . Show that the matrix $A^2+A+m.I_n$ is non singular I have tried like this: Suppose $\lambda$ be any eigenvalue of $A$ . Then it is enough if we can show $\lambda^2+\lambda+m\neq0$ . But how to connect this with $\mbox{ gcd }(m,\det\,A)=1$ .","['matrices', 'linear-algebra']"
3221092,"Suppose that $a=\binom70+\binom73+\binom76,b=\binom71+\binom74+\binom77,c=\binom72+\binom75$. How to algebraically compute $a^3+b^3+c^3-3abc$?","$$
\begin{align}
a = {7 \choose 0}+{7 \choose 3}+{7 \choose 6}\\
b = {7 \choose 1}+{7 \choose 4}+{7 \choose 7}\\
c = {7 \choose 2}+{7 \choose 5}
\end{align} 
$$ then $a^3+b^3+c^3-3abc$ is equal to _____. I tried to write $a^3+b^3+c^3-3abc$ in terms of $a+b+c$ and failed. $$
\begin{align}
a^3+b^3+c^3-3abc & = (a+b+c)(a^2+b^2+c^2-ab-bc-ca)\\
& = (2^7)((a+b+c)^2-3(ab+bc+ca))\\
& = (2^7)((2^7)^2-3(ab+bc+ca))
\end{align}
$$ I think the expression should be written in terms of another binomial series which I can not think of.","['algebra-precalculus', 'binomial-coefficients']"
3221100,Is there a simpler geometric solution?,"In the diagram below, $ABC$ is a triangle. Given that $\overline{AD}=\overline{BC}$ , $\angle ABC=120^{\circ}$ , $\angle BDA=3\phi$ , and $\angle BCA=2\phi$ , determine the measure of $\phi$ . Construct the equilateral triangle $BQC$ and the parallelogram $ABPD$ . Angle chasing gives $\angle DBC = \phi$ , $\angle DAB\cong PBQ \cong BPD=60^{\circ}-2\phi$ , $\angle BDP = 120^{\circ}-\phi$ . $\triangle BPQ$ is isosceles, thus $\angle BQP\cong\angle BPQ=60^{\circ}+\phi$ . It follows that $BQPD$ is cyclic. It follows that $\angle BPD=60^{\circ}-2\phi$ , thus $\angle BQD=60^{\circ}-2\phi$ . It follows that $\triangle BQD$ is isosceles, and so is $\triangle DQC$ . It follows that $\angle DBQ\cong\angle QDB = 60^{\circ}+\phi$ . It follows that the angles of the triangle $DQC$ is $60^{\circ}+2\phi+60^{\circ}+2\phi+2\phi$ . Thus $\phi=10^{\circ}$ . It feels as if there should be a simpler geometric solution. Can you come up with one?","['euclidean-geometry', 'geometry']"
3221102,Why $99x^2 \equiv 1 \mod 5 \implies (-1)x^2 \equiv 1 \mod 5$,Lastly I have read a example of some exercise. There was this statement: $$99x^2 \equiv 1 \pmod 5\quad  \implies\quad (-1)x^2 \equiv 1 \pmod 5$$ Can somebody explain that simple fact to me?,"['modular-arithmetic', 'discrete-mathematics']"
3221111,Exterior derivative on loop space,"Notations: Let $X$ be a manifold, and denote by $LX := C^\infty(S^1,X)$ its loop space. For a loop $\gamma \in LX$ we can think at the tangent space of $LX$ at the point $\gamma$ as the space of loops that are 'arbitrarily closed' to $\gamma$ . This motivates the following definition $$T_{\gamma}LX := \Gamma(S^1, \gamma^*TX),$$ where $\Gamma(S^1, \gamma^*TX)$ denote the space of section of the pullback bundle $\require{AMScd}$ \begin{CD}
    \gamma^*TX @>>> TX\\
    @V  V V @VV  V\\
    S^1 @>>\gamma> X.
\end{CD} This is the description of the tangent bundle $TLX \to LX$ of the loop space of $X$ . By definition a $k$ -differential form on $LX$ is a section of the $k^{th}$ exterior power of $TLX$ , $$\Omega^k(LX):= \Gamma(LX, \Lambda^kTLX).$$ Question: How is the exterior differential $d$ defined for differential forms on loop space? \begin{CD}
\Omega^k(LX) @>d>> \Omega^{k+1}(LX)
\end{CD}","['loop-spaces', 'differential-forms', 'differential-geometry']"
3221115,Lebesgue measure zero vs Jordan content zero,"Is a set of Lebesgue measure zero necessarily a countable union of sets of Jordan content zero? This was a question posed by a student in my undergraduate analysis course. I asked an analyst colleague about this and he did not have an answer off the top of his head. Here are a few thoughts about this question. Since the closure of a set of Jordan content 0 also has content 0 and a compact set has measure 0 iff it has content 0, this question can be rephrased as follows: is any Lebesgue measurable set contained in an $F_\sigma$ set of the same measure? Off hand this might seem to be rather implausible. However it is true for open sets. They are countable unions of open balls and are contained in the corresponding union of closed balls.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3221116,"If $x, y, z$ are three distinct positive integers, where $x + y + z = 13$ and $xy, xz, yz$ form an arithmetic...","If $x, y, z$ are three distinct positive integers, where $x + y + z = 13$ and $xy, xz, yz$ form an increasing arithmetic sequence, what is the value of $(x + y)^z$ ? I've been trying to solve this by forming equations based off of formulas ( $yz - xz = xz - xy$ ) and by trying to clean it up by systems of equations (I did $(13 - z)^2 = 13$ ), but I can't seem to find the solution.","['algebra-precalculus', 'arithmetic-progressions', 'diophantine-equations']"
3221118,Stability on the number of connected components on a moduli space of smooth manifolds.,"Let $g:\mathbb{R}^n \times \mathbb{R} \rightarrow \mathbb{R}^k$ be a function whose coordinates $g_i$ are homogeneous polynomials  and let $u \in \mathbb{R}^k$ and $c \in [0,1]$ and let $J_x(c)$ be the Jacobian matrix of $g(\cdot,c)$ at $x$ . Let $M(c)=\{x \in \mathbb{R}^n | g_i(x,c)=u_i, i=1\cdots,k\}$ be a space such that $J_x(c)$ is full rank at the interior of $M(c)$ for all $c \in [0,1]$ . Thus $M(c)$ is a collection of spaces whose interior are smooth manifolds. I try to investigate if the number of connected components of $M(c)$ is equal to $M(c')$ where $c' = c + \delta c$ and $\delta c$ is sufficient small. Some of you have an idea or which tools to use in order to investigate the above property? Thanks in advance.t","['differential-topology', 'algebraic-geometry', 'algebraic-topology', 'differential-geometry']"
3221120,The odds in rolling two dice together.,"Two dice are rolled. I want to see the odds of the following: $1.$ A sum of $5$ . $2.$ A sum of $8$ or $10$ . $3.$ A sum less than $6$ . $4.$ Not a sum of $7$ . Solution: $1.$ A sum of $5$ . $5 = 1+4,2+3,3+2,4+1$ . So the odds is $4/36 = 1/9.$ $2.$ A sum of $8$ or $10$ . We can express $8 = 2+6,3+5,4+4,5+3,6+2$ and $10 = 4+6,5+5,6+4$ . So the odds is $8/36 = 2/9$ . $3.$ A sum less than $6$ . We can express $2=1+1$ , $3=1+2,2+1$ , $4=1+3,2+2,3+1$ and $5 = 1+4,2+3,3+2,4+1$ . So the odds is $10/36 = 5/18$ . $4.$ Not a sum of $7$ . We can express $7 = 1+6,2+5,3+4,4+3,5+2,6+1$ . So the odds is $\frac{36-6}{36} = \frac{30}{36} = \frac56$ Is the solution correct?","['discrete-mathematics', 'combinatorics', 'probability']"
3221128,Why are |vertical lines| used to mark matrix determinants?,This notation is sometimes used to denote the determinant: $$ \begin{vmatrix}a & b \\ c & d\end{vmatrix} = ad-bc$$ Why? Where did this notation come from? Was there any relationship between this notation and the absolute value $|x|$ or the norm $\lVert\mathbf{x}\rVert$ ?,"['notation', 'soft-question', 'linear-algebra']"
3221131,A twisted hypergeometric series $\sum_{n=1}^\infty\frac{H_n}{n}\left(\frac{(2n)!}{4^n(n!)^2}\right)^2$,"Question. I was given that $$S=\sum_{n=1}^\infty\frac{H_n}{n}\left(\frac{(2n)!}{4^n(n!)^2}\right)^2=\frac{32}\pi G\ln2+\frac{64}\pi\Im\operatorname{Li}_3\left(\frac{1+i}2\right)-2\ln^22-\frac53\pi^2$$ where $H_n$ harmonic numbers, $G$ Catalan and $\operatorname{Li}_n$ polylogarithm. How can it be proved? My Approach. Using $\int_0^1x^{n-1}\ln(1-x)dx=-\frac{H_n}n$ one have $$S=\int_0^1-\ln(1-x)\sum_{n=1}^\infty\left(\frac{(2n)!}{4^n(n!)^2}\right)^2x^{n-1}dx
=\int_0^1-\ln(1-x)\left(\frac2\pi\frac{\mathbf{K}(x)}x-\frac1x\right)dx$$ where $\mathbf{K}$ denotes elliptic integral of the first kind. The question boils down to finding $$\int_0^1\frac{\mathbf{K}(x)\ln(1-x)}xdx$$ For this integral, I tried to use the integral representation of the elliptic integral and got: $$\int_{(0,1)^3}\frac{dxdydz}{\sqrt{1-y^2}\sqrt{1-xy^2}(zx-1)}$$ This is the furthermost step I can get.","['integration', 'definite-integrals', 'calculus', 'polylogarithm', 'hypergeometric-function']"
3221171,Solve $99x^2 \equiv 1 \mod 125$,"Solve $$x^{98} \equiv 99 \mod 125$$ Is there any easy way to solve equations like that? My observation is that from Euler's theorem we know that $$ x^{100} \equiv 1 \mod 125 $$ so $$x^{98} \equiv 99 \mod 125 \\ 
x^{100} \equiv 99x^2 \mod 125 \\
99x^2 \equiv 1 \mod 125$$ but what is general method how to deal with equations like that?","['elementary-number-theory', 'modular-arithmetic', 'discrete-mathematics']"
3221212,Curl of implicit vector field?,"I know I can check whether a given (continous-differentiable) vector field (with simple connected domain) is conservative by checking if its curl is zero. In the 2d case for example $$
\begin{bmatrix}
z_1 \\
z_2
\end{bmatrix} = \begin{bmatrix}
F_1(x, y) \\
F_2(x, y)
\end{bmatrix}
$$ so if the curl $$
\frac{\partial }{\partial x} F_2(x,y) - \frac{\partial }{\partial y} F_1(x,y) = 0
$$ then the vector field is conservative. But what can be done if the vector field is not available as a set of explicit functions but instead only implicitly (and it is not possible to isolate the expressions)? I.e. like this: $$
\begin{bmatrix}
G_1(x, y, z_1) \\
G_2(x, y, z_2)
\end{bmatrix} = \begin{bmatrix}
0 \\
0
\end{bmatrix} \tag{1}
$$ Or even like this: $$
\begin{bmatrix}
H_1(x, y, z_1, z_2) \\
H_2(x, y, z_1, z_2)
\end{bmatrix} = \begin{bmatrix}
0 \\
0
\end{bmatrix} \tag{2}
$$ Question : Can I still somehow check if the implicit vector field $(1)$ or even $(2)$ is conservative? Example : I will give an explicit example: $$
\begin{align}
z_1 &= G_1(x, y, z_1) = x^2 + y^2 + z_1^3 = 0 \\
z_2 &= G_2(x, y, z_2) = (x + y + z_2)^3 = 0 
\end{align}
$$ Implicit differentiation: $$
\begin{align}
0 &= 2 y + 3 z_1^2  \frac{\partial z_1}{ \partial y} \\
0 &= 3(x + y + z_2)^2 \Big(1 + \frac{\partial z_2}{\partial x} \Big)
\end{align}
$$ Solve for partial derivatives: $$
\begin{align}
\frac{\partial z_2}{\partial x} &= -1 \\
\frac{\partial z_1}{\partial y} &= -\frac{2 y}{3 z_1^2}
\end{align}
$$ Then the curl should be $$
\frac{\partial z_2}{\partial x} - \frac{\partial z_1}{\partial y} = \frac{2 y}{3 z_1^2} - 1 \neq 0
$$ So in this case, the vector field should be not a conservative vector field. Is this conclusion and the derivation correct like this?","['vectors', 'ordinary-differential-equations', 'multivariable-calculus', 'calculus', 'vector-analysis']"
3221239,The probability of ending up with red balls,"In a bag there are $a$ red balls and $b$ blue balls ( $a,b>0$ ). You conduct the following process. Randomly fetch a ball from the bag, record the colour as the current colour . And throw it away. Go to 2. If there's no ball left, break. Otherwise, randomly fetch a ball from the bag. If the colour coincides with current colour, throw it away, and repeat 2; otherwise, put back the ball, and go back to 1. Qs: What's the probability that the last ball fetched is red, in terms of $a,b$ ? (I seem to lack the necessary combinatorial tools to tackle it. Yeah I deem it to be combinatorial. Initially thought of constructing some kind of martingale based on the balls fetched and apply optional stopping, but didn't work out...) Does a closed form solution exist? (I have absolutely no idea other than imagination about a numerical implementation.) Is it generalisable to multicoloured balls (more than two) cases? (This one just for fun.)","['discrete-mathematics', 'probability-theory', 'probability']"
3221281,Are continuous functions measurable with respect to abstract Borel sigma algebras?,"I've always taken for granted that if $(X, \tau)$ and $(Y, \tau^{'})$ are topological spaces with Borel sigma algebras $\sigma$ and $\sigma^{'}$ , repectively, then if $f:X\rightarrow Y$ is continuous, it is Borel measurable. The proof seems straightforward: Let $Z=\{A \in Y \mid f^{-1}(A) \in \sigma \}. $ Then, by continuity it holds that $\tau^{'} \subseteq Z$ , and once can check that $Z$ is a sigma-algebra. Because $Z$ is itself a sigma-algebra, it holds that $\sigma^{'} \subseteq Z$ . Therefore, for any Borel measurable set $B \in \sigma^{'}$ , $f^{-1}(B) \in \sigma$ , so that $f$ is Borel measurable. However, there seems to be some posts concerned with whether the topology is Hausdorff or not (see https://mathoverflow.net/questions/181752/is-every-continuous-function-measurable and Is every continuous function measurable? ) For the life of me, given the proof above, I cannot see why this would be a concern. Can anybody give me any points of reference here? Thanks!","['general-topology', 'measure-theory', 'real-analysis']"
3221306,"$E[f(X, Y)|\mathscr{F}]$ where $X$ is independent of $\mathscr{F}$ and $Y$ is $\mathscr{F}$ measurable.","I'm struggling with trying to find a formula for $E[f(X, Y)|\mathscr{F}]$ where $X$ and $Y$ are like in the title, $f$ is borel, and $E|f(X, Y)|<\infty$ . I know that there exists a formula for $E[f(X, Y)|Y]$ and it's equal to $g(Y)$ where $g(y) = E[f(X, y)]$ . Does it hold that $E[f(X, Y)|\mathscr{F}] = g(Y)$ as well? My problem is that the proof of the formula relies on the fact that if $B\in\sigma(Y)$ then $B = A^{-1}(Y)$ where $A$ is a Borel set. I can't use that approach here.",['probability-theory']
3221309,Equality of the stochastic integral under two probability measures,"This questions is very short. Under the Girsanov Theorem assumptions we have two equivalent probability measures $\mathbb P$ and $\mathbb Q$ and a measurable space $(\Omega,\mathcal F)$ , right? We have that $$\mathcal E(Z)=\frac{d\mathbb Q}{d\mathbb P}$$ for some contiuous local martingale $Z$ in $(\Omega,\mathcal F,\mathbb P)$ and $\mathcal E (\cdot)$ is the  exponential local martingale operator. I want to proof that for a semi-martingale $X$ we have $H\bullet X$ is the same in both the spaces $(\Omega,\mathcal F,\mathbb P)$ and $(\Omega,\mathcal F,\mathbb Q)$ . I have read a couple of books (e.g. Revuz-Yor) but they are all very fast in this. It seems that they somehow just conclude it. I found a detailed proof in Jean-François Le Gall's book Brownian Motion etc. but even there it is too fast in my opinion. In page 136 the author says more or less the following. Let us denote $(H\bullet X)_\mathbb P$ to emphasize the space.  We can write $X=M+V$ with $M$ a continuous local martingale in $(\Omega,\mathcal F,\mathbb P)$ and $V$ a process of bounded variation. We know that by Girsanov we can write $X=M'+V'$ with $M'=M-\langle M,Z\rangle $ a $\mathbb Q$ local martingale. So $M'$ is a semi-martingale and I know how to integrate that in $(\Omega,\mathcal F ,\mathbb P)$ $$(H\bullet M')_\mathbb P=(H\bullet M)_\mathbb P-\langle (H\bullet M)_\mathbb P,Z\rangle $$ Girsanov says that $(H\bullet M')_\mathbb P$ is a $\mathbb Q$ -local-martingale. But then Jean-François Le Gall concludes that every $\mathbb Q$ -local-martingale $N'$ has bracket process with $(H\bullet M')_\mathbb P$ equal to $$\langle (H\bullet M')_\mathbb P,N'\rangle =H\bullet \langle M,N'\rangle =H\bullet \langle M',N'\rangle \tag{A} $$ I know that for $N$ a $\mathbb P$ -local-martingale we have $$H\bullet \langle M,N\rangle =\langle (H\bullet M)_\mathbb P,N\rangle $$ It seems that such formula is used but I don't know why it would hold for $N'$ . Question. Why does (A) hold?","['measure-theory', 'stochastic-analysis', 'stochastic-differential-equations', 'local-martingales', 'stochastic-calculus']"
3221318,Prove the following statement about measurable sets and functions,"Let $(f_n)_{n = 1}^{\infty}$ be a sequence of measurable functions defined on a measurable set $E$ , $m(E) < \infty.$ Suppose that $(\forall x \in E)(\exists M_x \in \mathbb{R})$ such that $\sup_n \mid f_n(x) \mid \le M_x.$ Prove that $\forall \varepsilon > 0$ there exists $F$ , a measurable subset of $E$ , $m(E \setminus F) < \varepsilon$ and $M = M(F, \varepsilon)$ such that $\mid f_n(x) \mid \le M$ for all $n \in \mathbb{N}$ and every $x \in F.$ I can't think what would the set $F$ look like. Something hints me that I should have used the fact that $f_n$ are measurable and $[0, M_x]$ are closed, but I can't figure it out. Any help is appreciated.","['measure-theory', 'measurable-functions']"
3221344,What is the number of ways of arranging two colours such that no more than $k$ pieces of same colour are together?,"Given that we have $x$ number of $A$ 's and $y$ number of $B$ 's. How can I find the number of ways of arranging them in a line, such that no more than $k$ pieces of the same colour are adjacent? Constraints: x,y<=1000 I first thought it could be ${x+y \choose x}-{x+y-k \choose x}$ . Based on total combinations - combinations having a group with greater than $k$ pieces of the same color together. Then for the latter part, we can combine $k+1$ pieces of the same color (say $A$ ) as a single piece, so that the total number of pieces reduces to $(x+y-(k+1)+1) $ and the number of pieces of color A reduces to $x-(k+1)+1$ . Now each combination of this scheme will be invalid. Total combinations like these will be ${x+y-k \choose x}$ But we can also inject some color $B$ pieces into the combined $k+1$ pieces of color $A$ and still get invalid combination.
How to add these?
Is there some simpler way of solving the problem? A recursive solution will also work.
The question appeared on a programming contest.","['combinations', 'combinatorics', 'dynamic-programming']"
3221360,Constructibility of an inscribed square using Galois Theory,"A problem in my Galois theory syllabus in the chapter on constructible numbers is as follows: Check if the small square if constructible from the big square. [Hint: Choose coordinates $0,1\in\mathbf{C}$ as in the picture, and find $z=a+bi$ .] I found two equations from which I can solve $a$ and $b$ . Adding up the area's of the four congruent triangles and the small square, we get $1=4\cdot \frac{1}{2}b+(1-a)^2$ . Using similar triangles, I can extract a second relation, which is $\frac{1}{\sqrt{b^2+(1-a)^2}}=\frac{\sqrt{b^2+(1-a)^2}}{1-a}$ or $b^2=a(1-a)$ . This gives the following system of equations to solve: $$\begin{cases} 2b+(1-a)^2=1 \\ b^2=a(1-a) \end{cases}$$ I tried finding the real solution of this system of equations, in vain, which after seeing the solution from Mathematica is not such a surprise. The minimal polynomial of $z=a+bi$ is, according to Mathematica, $X^3 - 4 X^2 + 6 X - 2$ . This polynomial is Eisenstein with $p=2$ , so irreducible. By Galois theory, we can then conclude that $z$ is not constructible. My question is: how can I find either a closed expression for $z$ or it's minimal polynomial without a ton of calculations or use of Mathematica? The author of this problem in my syllabus obviously intended an approach by hand.","['galois-theory', 'geometry']"
3221364,Find the maximum velocity of this particle,"Question: Let the position of the particle be $\mathbf r(t) = (x(t),y(t))$ . The equations of motion are $$\ddot x = \frac{ay}{x^2+y^2} \qquad \ddot y = -\frac{ax}{x^2+y^2}$$ or equivalently, $$\ddot {\mathbf r} = -a \nabla \tan^{-1}\bigg(\frac yx\bigg)$$ where $a>0$ is a constant. The initial conditions are $\mathbf r(0) = (x_0,y_0)$ and $\dot{\mathbf r}(t) = \mathbf 0$ , where $x_0,y_0>0$ . Find the maximum speed attained by the particle in its subsequent motion. Attempt: So we want to find the maximum value of $\dot x^2 + \dot y^2$ (which is the squared velocity). Call this quantity $Q^2$ (of course, the actual quantity we are after is $Q$ ). I started off by observing that $$y\ddot x - x \ddot y = a$$ The left hand side can be factorised: \begin{align}
\implies & \frac{d}{dt}\big(y\dot x - x\dot y \big) = a \\
\implies & y\dot x - x\dot y = at + B
\end{align} for some constant $B$ . Using initial conditions, we see that $B=0$ , so $$\implies y\dot x - x\dot y = at$$ Now differentiating $Q^2$ with respect to $t$ : $$\frac{dQ^2}{dt} = 2\dot x\ddot x + 2\dot y \ddot y = 2\dot x \frac{ay}{x^2+y^2} - 2\dot y \frac{ax}{x^2+y^2} = \frac{2a(y\dot x - x\dot y)}{x^2+y^2} = \frac{2a^2t}{x^2+y^2}$$ So it follows that $\frac{dQ^2}{dt}>0$ for all $t>0$ . This implies that the speed is forever increasing - what have I done wrong? What is the correct approach?","['classical-mechanics', 'ordinary-differential-equations']"
3221375,"Convex, absorbing sets and nonempty interior","Let $A$ be a convex, absorbing subset of a real Banach space $X$ with the additional property that the closure $\rm{cl}(A)$ contains an open ball around $0\in X$ . Does this imply that already $A$ itself contains an open ball around 0? (I hope that this additional property is already a consequence of Baire's lemma. But please correct me if this is wrong...)","['banach-spaces', 'general-topology', 'functional-analysis']"
3221379,Why does a C.D.F need to be right-continuous?,"As you may know, if $(\Omega,\mathcal{F},\mathbb{P})$ is a probability space and $X\colon\Omega\to\mathbb{R}^k$ is a random variable, then the cumulative distribution function of $X$ is defined as $$F_X(a):=\mathbb{P}(\{\omega\in\Omega\,\colon X(\omega)\leq a\})=\mathbb{P}\Big(X^{-1}\Big(\prod_{i\in [k]}(-\infty,a_i]\Big)\Big),\,\text{ for each } a\in\mathbb{R}^k.$$ This function is always right-continuous. That is, for each $x\in\mathbb{R}^k$ we have $\lim_{a\downarrow x}F_X(a)=F_X(x)$ . My question is: Why is this property important? Is there any capital result in probability theory that depends on it?","['soft-question', 'probability-theory']"
3221427,What is the best way to study graduate level mathematics?,"I am studying a 400/500 level measure theory math book on my own.
Right now, when I read it I try to read the proposition then the following proof. And then try to do the exercises on my own. 
I wonder if I should change it to:-
1. read the proposition
2. try to prove it on my own.
3. If I could not then proceed to read the proof from the book.
4. Do the exercises. Or should I stick with everything above minus step 2. It can save me some time but I wonder if that is what good mathematics students do? Any feedback from your experience would be appreciated. Best,","['self-learning', 'measure-theory', 'advice', 'soft-question']"
3221457,"$M=\{ (x_1-x_2, x_2-x_3,....)| (x_n)_{n \in \mathbb{N}} \in \ell^{\infty} \}$ is closed.","Consider $M=\{ (x_1-x_2, x_2-x_3,....)| (x_n)_{n \in \mathbb{N}} \in \ell^{\infty} \}$ . Show that $M$ is closed in $\ell^{\infty}$ . I'm trying to show that any convergent sequence in $M$ converges in $M$ . Take $(x^k)_{k \in \mathbb{N}} $ in $M$ such that $x^k \to x$ . $\forall \epsilon > 0 \exists k_0 \in \mathbb{N} $ such that $$||x^k-x||< \epsilon \text{ whenever } k>k_0$$ I can not conclude that $x \in M$ I'm trying to solve the problem of the image. I do not need M to be closed to apply the version of Hahn Banach that he uses in the tip?","['general-topology', 'functional-analysis', 'real-analysis']"
3221463,"If $f,g$ are any two functions with $f>g$, then there is a continuous function $h$ such that $f>h>g$","Is the following claim true? Let $f,g : [a,b] \to \mathbb{R} $ be any two functions such that $f>g$ . Then there exists a continuous function $h:[a,b] \to \mathbb{R}$ such that $f>h>g$ . I have no idea how to approach this, although the claim seems true to me. That being said, I am not interested in a proof, if the claim does happen to be correct ( I would like to find it for myself). However, I welcome counter examples, if the claim is false! I merely want to know whether or not the claim is true. Any help would be greatly appreciated!","['calculus', 'functions', 'analysis', 'real-analysis']"
3221487,"If $f,g: [a,b] \to \mathbb{R}$ are bounded with $g$ continuous and $f>g$, is there a continuous $h:[a,b] \to \mathbb{R}$ with $f>h>g$?","Is the following claim true: Let $f,g :[a,b]  \to  \mathbb{R}$ be bounded functions with $g$ continuous and $f>g$ . Then there exists a continuous function $h: [a,b]  \to  \mathbb{R}$ , such that $f>h>g$ . I am only interested in whether or not the claim is true or not, so please leave the proof to me if the claim is true. However, if the claim is false, I welcome counter examples!
Any help is greatly appreciated!","['calculus', 'functions', 'analysis', 'real-analysis']"
3221569,Conjecture: all complex roots of $\sum_{k=0}^\infty \frac{z^k}{\left(nk\right)!}$ are real,"Conjecture: $$\left[n\in\mathbb{Z}^+,z\in\mathbb{C},0=\sum_{k=0}^\infty \frac{z^k}{\left(nk\right)!}\right]\Rightarrow z\in\mathbb{R}$$ This conjecture has been verified for $n\in\{1,2,4\}$ . The motivation for this conjecture arose during the study of the exponential sum function which has applications to exponentiation in rings with abelian multiplication: $$\text{rues}_n\left(z\right)=\sum_{k=0}^\infty \frac{z^{nk}}{\left(nk\right)!}=\frac{1}{n}\sum _{k=1}^n \exp\left(ze^{2ki\pi/n}\right)$$","['complex-analysis', 'conjectures', 'roots', 'mittag-leffler-function']"
3221581,Faa di Bruno's formula and alternating functions,"Suppose you have a function $f(x)$ such that ${\rm sgn}\Big(\frac{d^k}{dx^k}\big(f(x)\Big) = (-1)^k$ and a function $g(x)$ such that ${\rm sgn} \Big(\frac{d^k}{dx^k}g(x)\Big) = (-1)^{(k+1)}$ , $\forall k \in \mathbb{Z}^+, x \in \mathbb{R}^+$ . ${\rm sgn(x)}$ is the sign (or signum) function. I want to prove that ${\rm sgn} \Big(\frac{d^k}{dx^k} f(g(x))\Big) = (-1)^k$ . After trying to find patterns in the expression that arose after differentiation, I realized that this problem could be approached using the Faa di Bruno's rule and Bell's polynomials: $$\frac{d^n}{dx^n}f(g(x)) = \sum_{k=1}^n f^{(k)}(g(x))\cdot B_{n,k}(g^{(1)},g^{(2)},...,g^{(n-k+1)})$$ where $g^{(m)}(x)$ represents the $m$ -th derivative of $g$ wrt $x$ . However, I am unable to generalize this and find an inductive or, a somewhat out of my depth, combinatorial proof for this. How should I go about it? Any advice would be appreciated. eg: $f(x) = e^{-x}$ and $g(x) = \sqrt{x}$ . Headway I have made: Rewriting Faa di Bruno's polynomial as $$\frac{d^n}{dx^n}f(g(x)) = \sum_{k=1}^n \frac{n!}{\prod_{i=1}^n m_i!(i!)^{m_i}} f^{(\sum_{i=1}^n m_i)}(g(x))\cdot \prod_{j=1}^n(g^{(j)}(x))^{m_j}$$ subject to $\sum_{i=1}^n im_i = n$ . What we want to prove now is the following: 
if $n$ is even, $(\sum m_i),(\sum m_{2j})$ are both even or are both odd. ... this problem seems to be harder than I thought it would be.","['integer-partitions', 'combinations', 'number-theory', 'combinatorics', 'functional-analysis']"
3221594,"A random series has infinitely many zeros in $[0,1)$ almost surely.","These days I've been learning the properties of Brownian sample paths(Chapter 2 in Le Gall's Brownian Motion, Martingales, and Stochastic Calculus ). As he mentioned in Proposition 2.14 : If $B=(B_t)_{t\geq0}$ is a Brownian motion, then we have a.s. for every $\epsilon>0$ , $$\sup_{0\leq s\leq\epsilon}B_s>0,\qquad\inf_{0\leq s\leq\epsilon}B_s<0,$$ which means $B$ attains zero infinitely many times on $s\in[0,\epsilon]$ almost surely. This property reminds me a problem I met several weeks ago, which goes as follows: Suppose $(\epsilon_n)_{n\geq1}$ is a sequence of i.i.d. random variables and the common law is Bernoulli: $$\mathbb{P}[\epsilon_1=1]=\mathbb{P}[\epsilon_1=-1]=1/2.$$ Consider the random series $f(x)=\sum_{n=1}^\infty \epsilon_nx^n$ . Show that the random series attains zero infinitely many times on $x\in[0,1)$ almost surely. I had some idea on this problem: the series $f(x)$ must vibrate a lot in the left of $x=1$ . All we need to prove is that for all $0<c<1$ , we can find a zero of $f(x)$ in the interval $[c, 1)$ . Any help would be appreciated.","['brownian-motion', 'stochastic-processes', 'roots', 'probability-theory']"
3221626,"prove or disprove: For a prime number p, there is a positive integer n such that $p+1 $= $n^2$ if and only if $ p = 3.$","Prove or disprove: For a prime number p, there is a positive integer n such that $p+1 $ = $n^2$ if and only if $ p = 3.$ first im trying to proof if p is prime number there is positive integer n such that $p+1=n^2$ implies $p=3$ suppose there is positive n integer such that $p+1=n^2$ and prove that $p=3$ . $p=n^2-1=(n+1)(n-1)$ since $p$ is prime $n-1=1$ , $ n = 2 $ $p+1=4$ $p=3$ hence it is proved that $p=3$ now if $p=3$ implies there is positive integer n such that $p+1=n^2 $ $p=3$ and p is prime $p+1=n^2$ , $4=n^2$ , $n2$ or $n=-2$ hence it is proved that there is positive integer $n$ such that $p+1=n^2 $ is my proof correct?
and also for prime number can i write $n+1=1$ instead $n-1=1$ ?","['proof-verification', 'discrete-mathematics']"
3221659,Averaging i.i.d. variables: Equal chance to be right and left of the mean?,"Let $\{X_i\}_{i=1}^{\infty}$ be i.i.d. random variables. Define $$ L_n = \frac{1}{n}\sum_{i=1}^n X_i \quad \forall n \in \{1, 2, 3, …\} $$ Using the central limit theorem, it can be shown that if $E[X_i]=0$ and $0<Var(X_i)<\infty$ then: $$ \lim_{n\rightarrow\infty} P[L_n\leq x] =  \left\{ \begin{array}{ll}
1 &\mbox{ if $x > 0$} \\
c & \mbox{ if $x=0$}\\
0 & \mbox{ if $x<0$} 
\end{array}
\right.$$ where $c=1/2$ .  If the variance is infinite then the law of large numbers implies a similar structure for the cases $x>0$ and $x<0$ , but the case $x=0$ is unclear to me. Questions: For infinite variance, can we get different behavior for the case $x=0$ , such as $c=1/3$ ?  Can we get related step-function structure when the mean does not exist, but with different behavior for the case $x=0$ ? Notes: We can get such a limiting function with $c=1/3$ for random sequences with different structure, such as $L_n= A/n$ with $P[A=1]=2/3, P[A=-1]=1/3$ . I came up with this question while reflecting on the question here: Why does a C.D.F need to be right-continuous?","['stochastic-processes', 'probability-theory']"
3221691,Using De Moivre's law to compute $(-\sqrt3+i)^{2/3}$,"Question: If $z=-\sqrt{3}+i$ , then $z^{2 / 3} = ?$ My work (which is wrong but I am not sure why): We can write $z = r(\cos\theta + i\sin\theta)$ $$r = \sqrt{(-\sqrt{3})^2 + 1^2} = \sqrt{4} = 2$$ $$\theta = \tan^{-1} \left(\frac{-1}{\sqrt{3}}\right) = \frac{5\pi}{6} \quad (*)$$ Therefore $$z = 2\left(\cos\left(\frac{5\pi}{6}\right) + i\sin\left(\frac{5\pi}{6}\right) \right) $$ so (applying De Moivre's) $$ z^{\frac{2}{3}} = 2^{\frac{2}{3}} \left(\cos\left(\frac{5\pi}{9}\right) + i\sin\left(\frac{5\pi}{9}\right) \right)$$ However, the answer is apparently: $$-2^{2 / 3} \sin \left(\frac{\pi}{18}\right)+i 2^{2 / 3} \cos \left(\frac{\pi}{18}\right)$$ How does one arrive at the given answer? Forgive my remembrance of high school trigonometry. In $(*)$ I understand that we can take the negative out of $\tan^{-1}$ resulting in $$\theta=-\tan ^{-1}\left(\frac{1}{\sqrt{3}}\right)=-\frac{\pi}{6} = \frac{11\pi}{6}$$ which results in a different $\theta$ than I had. My intuition for why $\theta =\frac{5 \pi}{6}$ is because $\frac{-1}{2} \div \frac{\sqrt{3}}{2}$ as $\sin \frac{5 \pi}{6} = -\frac{1}{2}$ and $\cos \frac{5 \pi}{6} = \frac{\sqrt{3}}{2}$ . But I am not sure if this is the correct reasoning","['complex-analysis', 'trigonometry', 'complex-numbers']"
3221733,Cosine related to Bitwise XOR through Repeated Roots,"I have been doing a lot of calculations involving cosine and have noticed a pattern. I spent some time and developed this algorithm for calculating cosines. To calculate the $\cos(x)$ where $x$ is in radians: Sanitize the input so that $x$ is $[0,2\pi)$ : $x_1 \gets (|x| \mod 2\pi )$ Divide by $\pi$ . This is usually fairly trivial as radians are often expressed as multiples of $\pi$ . $x_2 \gets x_1/\pi$ This is where things get interesting. Use the bitwise XOR operation: $x_3 \gets x_2 \veebar 2x_2$ Here we are going to express $x_3$ in binary and map its bits to signs in a repeated root of 2, which will then be assigned to $x_4$ . This is easier to illustrate with an example rather than with a formula: Finally, divide by 2: $x_5 \gets x_4/2$ This will result in the $\cos(x)$ , i.e. $\cos(x) = x_5$ . I have used this algorithm quite a lot now, and I am confident that it accurately calculates cosines, but I can't explain why there would be this relationship between cosine and bitwise XOR. Why does this relation exist? Can someone please explain why cosines would be related at all to the bitwise XOR operator like this. Thanks! Edit: For those of you who want a more technical description for step 4, I will need to define some things first: A function $D$ that returns the value of a given digit $d$ for a given number $n$ in a given base $b$ : $$D(n,d,b) = \left \lfloor{n \cdot b^{-d}} \right \rfloor \mod b$$ A recursive function $f$ that builds repeated roots of 2 with $N$ radicals from the bits of $x$ mapped to the signs: $$f(x,n,N) = 
   \begin{cases} 
  0 & n\geq N \\
  (-1)^{D(x,-n,2)}\sqrt{2+f(x,n+1,N)} & n \lt N
   \end{cases}
   $$ A function $F$ that takes the limits of processing all the bits of $x$ into a repeated root: $$F(x) = \lim_{N\to\infty} f(x,0,N)$$ With those defined, step 4 from above would be: $x_4 \gets F(x_3)$",['trigonometry']
3221741,Prove two numbers are coprime,"I encountered some other problem and I found a beautiful proof here Write $1/1 + 1/2 + ...1/ (p-1)=a/b$ with $(a,b)=1$. Show that $p^2 \mid a$ if $p\geq 5$. (see Thomas Andrew's post) But I thought he may miss the proof that $(a_1,(p-1)!)=1,$ which is not trivial for me. (As for the definitions of $a_1$ and $p$ please see the link above. The definitions are simple and clear there.) My problem is just that how to prove $(a_1,(p-1)!)=1$ . I tried to use the property that $(n,m)=(n.n+km)$ for any integers $n,m,k.$ But it turns out it makes the expression messy and dirty. And I can't go further. Any help will be thanked.","['number-theory', 'coprime', 'elementary-number-theory', 'prime-numbers']"
3221749,justification of $\operatorname {E} \left[2X\operatorname {E} [X]\right] = 2\operatorname {E} [X]\operatorname {E} [X]$,"I am learning Variance . $${\displaystyle {\begin{aligned}\operatorname {Var} (X)&=\operatorname {E} \left[(X-\operatorname {E} [X])^{2}\right]\\[4pt]&=\operatorname {E} \left[X^{2}-2X\operatorname {E} [X]+\operatorname {E} [X]^{2}\right]\\[4pt]&=\operatorname {E} \left[X^{2}\right]-2\operatorname {E} [X]\operatorname {E} [X]+\operatorname {E} [X]^{2}\\[4pt]&=\operatorname {E} \left[X^{2}\right]-\operatorname {E} [X]^{2}\end{aligned}}}
$$ where, the part $$\operatorname {E} \left[2X\operatorname {E} [X]\right] = 2\operatorname {E} [X]\operatorname {E} [X]
$$ is a little bit difficult to justify, can anyone give a hint? which rule can apply this.",['probability']
3221786,"unclamped smoothstep with.... not sure if this is the right term, but ""dynamic steepness""?","So, I need a smoothstep function for my game, and this seems to be the standard function that most people use: $y = (x^2) *(3-2*x)$ This is computationally friendly, which is important, but it doesn't provide a lot of control over the curve as far as I can tell.  The advice I've received is to use the function again on it's own output to get a steeper curve, but that doesn't really allow for very fine control, nor does it allow for a more shallow curve. Also, going beyond 1 actually inverts the direction with this function. Is there some way to control the steepness of this curve through a ""strength"", and continue past 0 & 1 at least a little bit.  Ideally I think I want a function that can produce graphs with a shape like these: Example graphs This shows the kind of function I am trying to get with three different ""strength"" values resulting in different steepness of the drop-off.  Unfortunately I can't figure out how to get a function like that(I actually had to draw the graph in an image editor...). I tried fiddling with tangents because I thought a sigmoid might at least give me easy control over the strength, but it needs to pass through the points {(0,0),(0.5,0.5),(1,1)} to be useful for interpolation and sigmoids never seem to actually reach the edge clamp values {(0,0),(1,1)} which isn't really usefull for interpolation from 0...1, or at least I haven't figured out how to do it... Also, full disclaimer, I'm a computer scientist, not a mathematician, so please explain it to me like I'm an idiot. Hopefully I've explained my problem adequately...  If you have questions I can try to answer them...","['interpolation', 'functions']"
3221884,Showing that $0\leq A\leq B$ and $B \in \mathcal{L}_c(H)$ implies that $A \in \mathcal{L}_c(H)$.,"Exercise : Let $H$ be a Hilbert space and $A,B \in \mathcal{L}(H)$ be self-adjoint operators with $0 \leq A \leq B$ and $B \in \mathcal{L}_c(H)$ . Show that $A \in \mathcal{L}_c(H)$ . Thoughts : Relying only on the definition of a compact operator, we essentialy need to conclude that $A$ transfers bounded sets to relatively compact sets (compact closure). Now, since $B$ is compact and self adjoint, I know that also $B^*B$ is compact. This may be of use since the property of $A$ and $B$ being self adjoint is noted in the exercise. I think that $A \leq B \implies \|A\| \leq \|B\|$ since they are both bounded  and we could take $\mathbf{1} \in H$ which yields that $$\|A(\mathbf{1})\| \leq \|A\|\|1\| \equiv \|A\| \quad \text{and} \quad \|B(\mathbf{1})\| \leq \|B\|\|1\| \equiv \|B\|$$ and since $0 \leq A \leq B$ implies that their values follow the inequality for any $x \in H$ thus the implied result. Request : Beyond these points, I sadly do not have an intuition for a head-start, so I would really appreciate any hints or elaborations.","['hilbert-spaces', 'operator-theory', 'compact-operators', 'functional-analysis']"
3221903,"Large modular n'th roots, e.g. solve $\,x^{118}\equiv 113\pmod{\!1001}$","Solve $$\exists_k \mbox{  } 1001\cdot k+113 = x^{118} $$ How to deal when I have something like in that exercise when my big number is $ 13\cdot 11 \cdot 7$ ? My current way:
It is equivalent to: $$   x^{118} \equiv 1 \mod 7 \wedge x^{118} \equiv 3 \mod 11 \wedge x^{118} \equiv 9 \mod 13  $$ then I consider first equation: $$x^{118} \equiv 1 \implies x \equiv \pm1 \mod 7 $$ so $$ x \equiv 1 \vee x \equiv 6 \mod 7 $$ In first case I have that $$ x= 1 + 7k $$ but when I put that to $x^{118}$ I am not able to continue computing...","['modular-arithmetic', 'discrete-mathematics']"
3221910,How to minimise the cost of guessing a number in a high/low guess game?,"In a high/low guess game, the ""true"" number is one of $\{1,\cdots,1000\}$ . You'll be told if your guess is $<,>$ or $=$ the true number for each guess you make, and the game terminates when you guess out the true number. Suppose the following three scenarios: If your guess is either lower or higher, you pay $\$1$ in both cases. If your guess is correct, you pay nothing and the game ends. If your guess is higher, you pay $\$1$ ; if your guess is lower you pay $\$2$ . If your guess is correct, you pay nothing and the game ends. If your guess is higher, you pay $\$1$ ; if your guess is lower you pay $\$1.5$ . If your guess is correct, you pay nothing and the game ends. In these three scenarios, what is, respectively, the minimum number of $\$$ you must have to make sure you can find the true number? Formally, define a space of all guess strategies $\Sigma$ . We can then identify the cost $C$ as a function $$C:\Sigma\times \{1,\cdots,1000\}\to\Bbb N_+,\quad v\mapsto C(S,v)$$ that maps the pair $(S,v)$ (where $v$ is the ""true"" number) to the cost it's going to take under this arrangement. Our problem is then to find $$\min_{S} \max_{v} C(S,v).$$ Can this min-max problem be analytically solved? What would be the corresponding optimal strategy then? For scenario 1, my intuition is to use bisection search, which, at worst, costs $\$10$ ( $2^{10}=1024$ ). But I cannot prove this is indeed optimal. For the second scenario, since making an under-guess means a higher cost, maybe it's more optimal to use a ""right-skewed"" bisection, i.e. you keep making guesses at the $2/3$ -quantile point. But this is just (rather wild ) intuition, not anything close to a proof.","['optimization', 'recreational-mathematics', 'discrete-mathematics', 'algorithms']"
3221934,Let B be any set and $g : B \rightarrow B$ be a function satisfying the condition $(g \circ g)(x) = x$,"Let $g : B \rightarrow B$ be a function satisfying the condition $(g \circ g)(x) = x$ for all $x \in B$ . Show that $g$ is a bijection. For $g$ is onto
we want to prove that there exists an $a \in B$ such that $g(a) = b$ for arbitrary element of $B$ . Let $b \in B$ , then  since $(g \circ g)(b)=b$ , then $ g(g(b))=b$ .
There exists $a \in B$ , $ g(b)=a \in B$ so that $g(g(b))=g(a)=b$ as required. Hence $g$ is onto. How we want to prove that there exists $g(x_1)=g(x_2)$ for $x_1,x_2 \in B$ ,  where $x_1=x_2$ . Let $x_1,x_2 \in B$ and  suppose $g(x_1)=g(x_2)$ , so $g(g(x_1))=g(g(x_2)) $ . Then since $g \circ g(x)=x$ and $x$ is in $B$ , $x_1=x_2$ , hence $g$ is one to one. Is this right? Or is there better way to prove this?","['functions', 'proof-verification', 'discrete-mathematics']"
3221949,Differential Equation for a Baseball seam,"Can we define differential equation and parametrization of a smooth continuous line on a unit ball with symmetry about two orthogonal axes ? And also have same geodesic curvature $k_g$ at four equidistant points on the curve on the two planes of symmetry? With a relation between curvature and torsion of the space curve seam? Can this space curve be unique? Imagine the seams of a common baseball or  tennis ball that divides surface area into two equal parts with two identical/congruent regions. However it has discontinuities at four locations... with curvature jumps at join of four small circles... these should translate to inflection points in the smooth curve sought. Looking for a relation between their curvature and torsion.. From an earlier question here we have a relation $$  \theta = (\pi/3)\sin 2\phi $$ that had been suggested (by @ minopret). It appears by visual inspection to me that not all $k_g$ are equal in magnitude and moreover there are six points of maximum geodesic curvature, not four. Thanks for all thoughts and hope would it be some fun too.","['multivariable-calculus', 'differential-geometry']"
3221955,Help understanding how a topological cone is constructed.,"This is the definition of the cone from wikipedia: I have trouble understanding why the cone has that particular shape. Based on what I understand about equivalence relations and quotients, shouldn't the cone consist of copies of X over the length $(0,1]$ , together with the copy $X \times \{0\}$ collapsed to a point? The mental image of a cone that I have is that of a cylinder, together with just one point on top. If we take the set $X$ to be $S^1$ , for example, in what sense are the ""middle"" circle of the cone and the circle $S^1 \times \{0.5\}$ in the original cylinder equivalent? Is the last remark in the pic supposed to explain this situation? If so please explain how that homeomorphism works.",['general-topology']
3221967,Notation Atiyah-Singer Index theorem,"I am trying to have a go at the Atiyah-Singer Index theorem and my question is very basic. Any help is appreciated! In Nakahara's book Geometry,Topology and Physics the theorem is stated as follows : $$ IND(E,D) = (-1)^{m(m+1)/2} \int_{M} ch  \left( \oplus (-1)^r E_r \right) \frac{Td(TM^\mathbb{C})}{E(M)}\Big|_{Vol}$$ The r.h.s contains objects like Chern,Todd and Euler classes each of which are sub-classes of the  DeRham Cohomology group of TM and are hence (Lie-algebra valued) differential forms of various degrees. So essentially the right hand side tells me to divide by differential forms??
I have no idea how one would perform such an operation. The splitting principle eases things somewhat but one is still dividing by differential forms. How do I make sense out of this? In addition my guess is that the notation $\Big|_{Vol}$ picks the volume forms out of the many terms of various degrees that appear on the r.h.s. Is that correct? If yes could there be more than one term? many thanks!","['algebraic-topology', 'elliptic-operators', 'differential-topology', 'mathematical-physics', 'differential-geometry']"
3221979,Proving that $S^1\times \mathbb{R}^2$ is not homeomorphic to $S^2\times \mathbb{R}$,"Prove that $S^1\times \mathbb{R}^2$ is not homeomorphic to $S^2\times \mathbb{R}$ . So far our only approach to proving spaces are not homeomorphic is removing points and checking the number of path components. Latest discussion was on pushouts and pullbacks, so another idea I though of was showing one is a pushout of some diagram that the other isn't, but while we've shown that two spaces satisfying the same pushout are homeomorphic, we have not shown two homeomorphic spaces satisfy the same pushout and I am not sure it is true. Another approach was to try and prove that $S^1\times\mathbb{R}\not\cong\mathbb{R}^2$ , but this has two problems - first is that I don't know how to do it either (without homotopies) and second is that I do not know if the canceling in the factorization is even legal. I know it is not legal in general (Taking any countable set $S$ with the discrete topology and $s\in S$ we have $S\times S \cong S\times\{s\}$ ), but am unable to prove or disprove the specific case of $\mathbb{R}$ . Are there any more elementary approaches then to prove this?",['general-topology']
3222089,Recursion-like sequences which are hard to relate recursively,"Consider the sequence \begin{align*}
   a_1&=1\\
   a_2&=2+\sqrt1\\
   a_3&=3+\sqrt{2+\sqrt1}\\
   &\kern5.5pt\vdots\\
   a_n&=n+\sqrt{n-1+\sqrt{\cdots+\sqrt{1}}}.
\end{align*} Something like this is easy to work with inductively, since we can simply relate $a_n = n+\sqrt{a_{n-1}}$ , and prove things that way. But now consider something which instead unfolds ""on the inside"", such as \begin{align*}
   b_1&=1\\
   b_2&=1+\sqrt2\\
   b_3&=1+\sqrt{2+\sqrt{3}}\\
   &\kern5.5pt\vdots\\
   b_n&=1+\sqrt{2+\sqrt{\cdots+\sqrt{n}}},
\end{align*} where it is not easy to relate $b_n$ to $b_{n-1}$ . How does one work with such sequences, where we are typically interested in the same sorts of questions? Or another example, consider the sequence \begin{align*}
   c_1&=1\\
   c_2&=1(1+2)\\
   c_3&=1(1+2(1+3))\\
   c_4&=1(1+2(1+3(1+4)))\\
   &\kern5.5pt\vdots\\
   c_n&=1(1+2(1+3(1+\cdots+(n-1)(1+n)\cdots))).
\end{align*} Is it possible to prove inductively that $c_n = 1!+2!+\cdots+n!$ , even though there is no obvious way to relate $c_n$ to $c_{n-1}$ ?","['induction', 'recurrence-relations', 'discrete-mathematics']"
3222156,What's a 'simply put' difference between Sx and S(x)?,The professor wouldn't really put it bluntly enough. This is a rookie question but I don't know what to search for (suggestions appreciated). What's the difference between: $C_n = \{X \in C:|X| = n\}$ vs $C(n) = \{X \in C:|X| = n\}$ Do they vary in terms of usage in other formulas or is it really just a syntactic sugar?,['elementary-set-theory']
3222223,Ergodic measures are extreme points - integration rather than average,"Let $T$ be a measureable map on the measureable space $(X,B)$ . It is fairly elementary that the ergodic measures are the extreme points of the space of the $T$ -invariant measures, which means that no ergodic measure can be presented as a convex combination of two (different) invariant measures. It is obvious that this can be generalized by induction to any finite-length convex combinations. My question is if it is true for general integration. It makes sense to me that the following will hold (or something similar), yet I couldn't find any references: ""If $\mu=\int \nu_x dm(x)$ where $\mu$ is ergodic, $\nu_x$ are $T$ -invariant, and $m$ is a probability measure on the space of invariant probability measures, then $\nu_x=\mu$ for $m$ -a.e. $x$ .""","['measure-theory', 'ergodic-theory', 'dynamical-systems']"
3222233,"Let $\lambda \in \mathbb{R}, \lambda > 0$ and let $X, Y, Z \sim P(\lambda)$ (they have Poissons distribution) independent random variables..","Let $\lambda \in \mathbb{R}, \lambda > 0$ and let $X, Y, Z \sim P(\lambda)$ (they have Poissons distribution) independent random variables. Calculate $Var (XYZ) $ . I tried by calculating $ \mathbb{E} (XYZ) ^2 ( = \lambda ^6)$ because $X,Y,Z$ are independent and $(\mathbb{E} (XYZ) )^2 ( = \lambda ^6)$ (let $g$ be function so $g(X) = X^2$ and then because $X,Y,Z$ are independent so are $g(X), g(Y), g(Z))$ which means $Var(XYZ) =0$ . Is that correct?",['probability']
3222248,Applying Intermediate Value Theorem to a function,"Question: $f(x$ ) = $x^5-15x^4-10x^2+20$ Prove that the equation $f(x) = 0$ has a real solution with $x \geq 1$ . Idea: Started off by coming up with a boundary $[1,x]$ for some real number $x$ . $f(1)=-4$ so is less than zero, then attempted to find an $x$ that gave me a positive result, very quickly came to the conclusion that any number above 1 when inserted into a function gave me a negative number. Pretty sure I could be missing something really simple.
Any help or hints would be appreciated.","['functions', 'real-analysis']"
3222251,Expected Minimum Distance between draws of rv,"If $X$ is a random variable, then Chebyshev's inequality states that $$\Pr(|X-E(X)|\ge t)\le\frac{Var(X)}{t^2}.$$ If the ""spread""/variance of the distribution $X$ is drawn from is very large, then for small $t$ this inequality gives no useful information. My question is if we get several draws from $X$ , can we say something a bit stronger, something like this: If $X_1,\ldots, X_N, X_{N+1}$ are iid, then can we bound $\Pr(\min_{1\le i\le N} |X_i - X_{N+1}| \ge t)?$ It seems like we should be able to say that this would be small with much higher confidence than in the vanilla Chebyshev, since the $N$ samples we draw before comparing the distance to $X_{N+1}$ intuitively seems to capture the spread of the distribution, and would potentially yield a better concentration than comparing to the mean. Any help is appreciated!","['concentration-of-measure', 'probability-theory', 'probability']"
3222259,"The inradius in a right angled triangle with integer side is r, if r = 4 the greatest perimeter is ??","The inradius in a right angled triangle with integer side is $r$ ,  if $r = 4$ the greatest perimeter is ?? My attempt- I know that $ r = (s-a)\tan \frac{A}{2}  $ Thus $ 2r = a+b-c $ I also know that $ a \le  b \lt c $ I also tried squaring both the sides and using Pythagorean theorem.","['trigonometry', 'triangles']"
3222292,Removable singularity in $\phi(\vec{x})=\int \left[\phi\frac{\partial (\ln r)}{\partial n} -\ln(r) \frac{\partial \phi}{\partial n}\right] ds$,"If I have the following integral equation $$\phi(\vec{x})=\frac{1}{\pi}\int \left[\phi\frac{\partial (\ln r)}{\partial n} -\ln(r) \frac{\partial \phi}{\partial n}\right] ds$$ An approximate solution of $\phi$ is obtained numerically by dividing the boundary into a finite number of segments ,N. So we can write $$\phi(\vec x_j)=\sum_{i=1}^{N}\left[\phi(\vec x_i)\frac{\partial \ln(r_{ij})}{\partial n} -\ln(r_{ij})\frac{\partial \phi}{\partial n}(\vec x_i)\right]\Delta s_i $$ Where $\Delta s_j$ represents the boundary segment length and $r_{ij}$ is the distance between the $i^{th}$ and the $j^{th}$ segment So it's easy to write $$\frac{\partial \ln(r_{ij})}{\partial n}=\frac{1}{r_{ij}}\left[-\frac{(x_i -x_j)}{r_{ij}}(\frac{\Delta y}{\Delta x})_j +\frac{(y_i -y_j)}{r_{ij}}(\frac{\Delta x}{\Delta y})_j\right]\Delta s_j$$ And $$Z_{ij}=\ln(r_{ij})\Delta s_j$$ Prove that $$\lim_{j \to i} \frac{\partial \ln(r_{ij})}{\partial n}=\left[\frac{(-x_{ss} y_s + x_s y_{ss})_i}{2}\right]\Delta s_i$$ My try $$\lim_{j \to i} \frac{\partial \ln(r_{ij})}{\partial n}=\lim_{h\to 0}\frac{-(x_i -x_{i+h})(y'_{i+h})+(y_i - y_{i+h})(x'_{i+h})}{(x_i -x_{i+h})^2 +(y_i -y_{i+h})^2}$$ Using $$x_{i+h}=x_i +h x'_i +(h^2/2)  x''_i$$ and $$x'_{i+h}=x'_i +hx''_i$$ Hence we get the required result My question How to prove that $$\lim_{j \to i} Z_{ij} = \left[\ln(\frac{\Delta s_i}{2})-1\right]\Delta s_i$$ Thanks in advance .","['approximation', 'real-analysis', 'complex-analysis', 'limits', 'numerical-methods']"
3222313,The collection of cylinder sets is a semiring,"Suppose that $ \{(\Omega_i, \mathcal {F}_i, P_i): i \in I\} $ is a non-empty finite or countable collection of probability spaces, and let be $ \Omega: = \prod_{i\in I}\Omega_i$ the product set. A set $A$ is called a cylinder set if $ A = \prod_ {i = 1} ^ {n} A_i $ with $ A_i \in \mathcal {F} _i $ . Indicate the collection of all cylinder sets with $ \mathcal {C} $ . I have to show that $ \mathcal {C} $ is a semiring.
So that
(1) $ \emptyset \in \mathcal {C} $ ;
(2) $ A, B \in \mathcal {C} \Rightarrow A \cap B \in \mathcal {C} $ ;
(3) $ A, B \in \mathcal {C} \Rightarrow exist \; C_1, ..., C_n \in \mathcal {C} $ pairwise disjoint, such that $ A \setminus B = \bigcup_ {i = 1} ^ n C_i $ .
I did the first two properties but I can't prove the third property. Could anyone help me?","['elementary-set-theory', 'measure-theory']"
3222317,Average area of the shadow of a convex shape [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question What is the average area of the shadow of a convex shape taken over all possible orientations? If we take a sphere, its surface area is exactly 4 times the area of its shadow. How can it be generalised for any convex shape? I know there are a lot of books like ""Introduction to Geometric Probability"", but I would appreciate an almost-intuitive explanation.","['polyhedra', 'geometric-probability', 'geometry', 'linear-algebra', 'linear-transformations']"
