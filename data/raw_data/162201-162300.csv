question_id,title,body,tags
2815455,Divisor class group of the quadric cone in $\mathbb{P}^3$,"I would like to compute the divisor class group of the projective quadric cone 
$$
Q=\mathrm{Proj}(\mathbb{C}[X_0,X_1,X_2,X_3]/(X_1X_2-X_3^2)).
$$
It has as an open subset the quadric cone $U$ in $\mathbb{C}^3$, which, according to Example II.6.5.2 of Hartshorne's, satisfies $\mathrm{Cl}(U)\simeq \mathbb{Z}/2\mathbb{Z}$. Therefore we have an exact sequence:
$$
\mathbb{Z}\rightarrow \mathrm{Cl}(Q)\rightarrow \mathbb{Z}/2\mathbb{Z}\rightarrow 0,
$$
where the first map sends $1$ to the class of the divisor $Q\setminus U$, and the second map is restriction. I am not sure if this helps, but it is all I have been able to do so far. By the way, what would be the canonical class of $Q$? Could we use the adjunction formula to compute it?","['divisors-algebraic-geometry', 'singularity', 'algebraic-geometry']"
2815461,Absolutely Continuous Spectrum of an operator $Af(x)=\sin x f(x)$,"Let us consider the following operator 
$$
Af(x)=\sin(x)f(x),
$$
in $L^2(\mathbb R)$. This operator is self-adjoint, so $\sigma(A)\subset \mathbb R.$ Solving equation $(A-\lambda I)f=g,$ we obtain $(A-\lambda I)^{-1}g=\frac{g}{\sin x-\lambda}$, and this formula defines resolvent if $\lambda\in\mathbb R\setminus[-1,1].$ Let $\lambda \in [-1,1].$ From the fact, that $\parallel \frac{1}{\sin x- \lambda}\parallel=\infty$, we get, that $[-1,1]$ is continuous spectrum. Now I am trying  to check, if this continuous spectrum is (or is not)a absolutely continuous spectrum. I would by very grateful if anyone can give me a hint, how to do that.","['functional-analysis', 'spectral-theory', 'operator-theory']"
2815469,Which mathematical subfield or formalism deals with local but non-pointwise function approximations?,"Main Question I have a class of high-dimensional functions which obey certain constraints (of value or partial derivative) on specific submanifolds of their domain.
I am quite confident that there is a more specific class of functions that matches these functions on these submanifolds and approximates them in the vicinity.
I want to prove this or find out what additional assumptions I need to make.
To this end, I am looking for a mathematical subfield or formalism that deals with such questions. Example A simple statement along the lines of what I wish to prove is: If $f(0,y) = f(x,0) = 0$ and $f$ is sufficiently benign, there exist functions $g$, $h$, and $r$ such that: $f(x,y) = g(x) h(y) + r(x,y)$, $g(0) = h(0) = 0$, $r(x,0) = r(0,y) = 0$, $∂_1 r(0,y) = ∂_2 r(x,0) = 0$. In this case, the answer is to choose
$g(x) = \frac{∂_2 f(x,0)}{∂_1 ∂_2 f(0,0)}$,
$h(x) = ∂_1 f(0,y)$, and
$r(x,y) = f(x,y) - g(x) h(y)$. What I considered so far Taylor series allow me to perform pointwise function approximations.
If done sufficiently general, this may allow me to find the shape of the desired approximation, but the process is rather tedious and suggests that there is a more general variant of it. Approximation theory seems to focus on numerically finding best approximations on the entire domain without paying special attention to certain regions – which is not what I want.","['reference-request', 'functions', 'approximation']"
2815477,On diophantine equations involving Stirling numbers of the second kind I: the equation ${m\brace k}-{n\brace k}=z^k$ for fixed integers $k\geq 3$,"I would like to know the solutions of next diophantine equation(s) involving the sequence of Stirling numbers of second kind ${r\brace s}$, for $1\leq s\leq r$. To create my variant I was inspired in last paragraph of the section D3 from [1]. Problem. Calculate the solutions $(m,n,z)$ of $${m\brace k}-{n\brace k}=z^k,\tag{1}$$
where $m\geq 1, n\geq 1$ and $z\geq 1$ are positive integers that satisfy the condition $$m>n,$$ and $k\geq 3$ is a fixed positive integer. Question. For each integer $k\geq 3$ calculate all solutions of previous Problem. Justify your words. Many thanks. I believe that previous problem isn't in the literature. Answer as a reference request if it is in the literature, adding the reference where is studied our  diophantine equations and I try to search and read those propositions. I suspect that the case $k=3$ of the Problem isn't similar to the case $k=2$ for which I know that has infinitelty many solutions. I'm not sure how to work the generalization that I've created as our Problem for the mentioned cases $k\geq 3$. I believe that it is possible to prove that the equation $(1)$ has no solutions (satisfying the required conditions) when $k$ is arbitrarly large, due an asymptotic approximation for Stirling numbers of the second kind. I know that for the case $k=3$ that $(m,n,z)=(3,1,1)$ and $(3,2,1)$ are solutions. And for the case $k=4$ I know three solutions $(m,n,z)=(4,1,1),(4,2,1)$ and $(4,3,1)$. I don't know if it is obious to prove that the solution that we can glimpse are the only solutions (respectively for each case). References: [1] Richard K. Guy, Unsolved problems in number theory , Second Edition, Springer (1994).","['diophantine-equations', 'stirling-numbers', 'reference-request', 'number-theory', 'elementary-number-theory']"
2815488,How to prove this is a triangulated subcategory?,"I'm studying homological algebra. And I'm working on this problem. The relation between thick subcategories in $\mathcal{D}$ and localizing classes of morphisms is as follows.
  A localizing class $S$ in $\operatorname{Mor}(\mathcal{D})$ is said to be saturated if
  \begin{align*}
      &\, s \in S \\
  \iff&\, \text{there exist morphisms $f,f'$ in $\mathcal{D}$ such that $f \circ s \in S$ and $s \circ f' \in S$}.
\end{align*} Prove the following result.
  Let $\mathcal{D}$ be a triangulated category.
  Then
  $$
          \mathcal{C}
  \mapsto \varphi(\mathcal{C})
  =       \left\{
            \begin{matrix}
              \text{$s \in \operatorname{Mor} \mathcal{C}$, $s$ is contained in a distinguished} \\
              \text{triangle $X \xrightarrow{s} Y \to Z \to X[1]$ with $Z \in \operatorname{Ob} \mathcal{C}$.}
            \end{matrix}
          \right\}
$$
  determines a one-to-one correspondence between the set of thick subcategories in $\mathcal{D}$ and the set of saturated localizing classes in $\mathcal{D}$ compatible with triangulation. The converse mapping associates with a class $S \subset \operatorname{Mor} \mathcal{D}$ the full subcategory $\psi(S)$ generated by such objects $Z \in \operatorname{Ob} \mathcal{D}$ that there exists a distinguished triangle $X \xrightarrow{s} Y \to Z \to X[1]$ with $s \in S$. (Original image here.) Let $\mathcal{D}$ be a triangulated category. Then there's a bijection between the set of thick subcategories in $\mathcal{D}$ and the set of saturated localizing classes (sometimes called saturated multiplicative system). This is also known for the Verdier correspondence . I've already proved that a thick triangulated subcategory can map to a saturated localizing classes by $\varphi$. But I'm stuck with the converse mapping. That is, if $S$ is a saturated localizing classes, then verify $\psi(S)$ being a thick triangulated subcategory. My Effort: Let $\mathcal{C} = \psi(S)$. TS1 : First verify that $\mathcal{C}$ is closed under $\Sigma$, i.e. $\Sigma \mathcal{C} = \mathcal{C}$. Let $Z \in \operatorname{Ob}(\mathcal{C})$, then there's a distinguished triangle 
$\require{AMScd}$
\begin{CD}
    X @>s>> Y @>>> Z @>>> \Sigma X
\end{CD}
with $s \in S$. So it suffices to show that $\Sigma^n s \in S$. But I don't know how to do this. TS2 : Let 
$\require{AMScd}$
\begin{CD}
    X @>>> Y @>>> Z @>>> \Sigma X
\end{CD}
be a distinguished triangle. Verify that if $X, Y \in \operatorname{Ob}(\mathcal{C})$, then so is $Z$. Actually I'm more confused with this part for I don't know how to use the properties of localizing and saturated. Can anyone enlighten me? Thanks in advance.","['triangulated-categories', 'algebraic-geometry', 'homological-algebra', 'category-theory', 'algebraic-topology']"
2815493,Shortest distance between parabola and point,"Find the shortest distance between the parabola defined by $y^2 = 2x$ and a point $ E:= (1.5, 0)$. I can't use the distance formula because I'm missing a set of points $(x, y)$ to plug into. So, instead, I have a normal that passes through the point $E$ from the parabola. Which is the definition of the shortest distance to a point. $$y - y_1 = m(x - x_1)$$ The slope of the normal is $\frac{1}{y_1}$ by using implicit differentiation and that's where I'm stuck, because I plug the point E into it and I get $$y_1^2=x_1-1.5$$ How do I prove the shortest distance is $\sqrt{2}$?","['maxima-minima', 'conic-sections', 'optimization', 'calculus']"
2815532,$n$ bugs moving in a line,"I have a line of $n$ bugs, where no $2$ bugs have the same size. They all move in the same direction. If a bigger bug is behind a smaller bug, it will eat the smaller bug. What is the expectation of the number of bugs left after a long enough time? I am not sure how to approach this. From $n=1, 2, 3, 4$ I guess the answer might be $$ 2- \frac{1}{n!} $$ but I am not sure if this is correct and how to derive this. For example, when $n=3$, if my bugs are $a,b,c$, where $a>b>c$, assuming they are moving from left to right, then I have the following $6$ situations. $a,b,c$ — 1 left $a,c,b$ — 1 left $b,c,a$ — 2 left $b,a,c$ — 2 left $c,a,b$ — 2 left $c,b,a$ — 3 left then the expectation is 1*2/6+ 2*3/6 + 3*1/6 = 11/6 = 2-1/3!","['puzzle', 'expectation', 'probability']"
2815548,Determining the statistical significance of the performance of a gambler,"Imagine someone claims they win significantly more than they lose when betting on roulette. Presuming that it were possible to have a winning system how could you calculate the statistical significance of their claim? The roulette table has no ""edge"" (i.e. no zero/double zero...). The issue comes with the fact that they are able to change their stake in addition to their odds. Even if they were hopeless at guessing and were wrong far more often than not, provided they only placed small losing bets and very large winning bets they would still come out on top. Their largest bets are 10, their smallest is 1, they are not simply making ever greater bets to recoup their losses. I do not believe there is such a thing as a winning strategy and have no intention of testing this, so please give your answer safe in the knowledge that it won't inspire me to gamble myself into the poor house.","['statistics', 'probability', 'gambling']"
2815551,Trigonometric functions differentiation with number inside the argument part.,"I had a question regarding the differentiation of trigonometric functions. To be a bit more specific, I'm currently studying a chapter about vector functions and vector calculus for an engineering mathematics course I'm taking at school. There is a rather simple detail that I don't understand though for an example question in the textbook. In the particular example question, the vector function is given as: $$\mathbf{r}(t) = \cos 2t\ \mathbf{i} + \sin t\ \mathbf{j}$$ And the differentiation of the function is apparently as follows: $$\mathbf{r^\prime}(t) = -2\sin 2t\ \mathbf{i} + \cos t\ \mathbf{j}$$ What I'm having trouble understanding is why the $-2$ has been added to the front of the $\sin$ for the differential equation? I thought that $$\frac{d}{dx}(\cos{x}) = -\sin{x}$$ Thank you.","['derivatives', 'trigonometry']"
2815556,Is there $n \geq 2$ such that $1^1 + 2^2 + \dots + n^n$ is a perfect square?,"Define $S_n = 1^1 + 2^2 + 3^3 + \dots + (n - 1)^{n - 1} + n^n$. It is clear that $S_1 = 1^1 = 1$ is a perfect square. However, I have found no others so far. Question : Does there exist $n \geq 2$ such that $S_n$ is a perfect square? I have checked via computer that there are no solutions for $2 \leq n \leq 3000$. Analysis modulo $3$ reveals that $$n \equiv 2,3,5,6,10,13 \pmod{18} \Rightarrow S_n \equiv 2 \pmod{3},$$ and hence $S_n$ is not a perfect square for these values of $n$. I imagine one could do similar analysis using other moduli, but it does not seem to eliminate many values of $n$. I currently do not have a great reason for believing that the answer to the question is no, but I also have no idea how, aside from burning my CPU, to show that the answer could be yes. Any other ideas would be greatly appreciated. ( I even tried searching for perfect powers in general, and have so far only found $$S_3 = 2^5$$ as an example. ) Edit 1 : The analysis done modulo 3 can be replicated for other moduli, and this can actually be useful if you are trying to search for solutions by computer. I make a useful conjecture: Conjecture: The residues of $S_n$ modulo an odd prime $p$ are periodic (over consecutive integers $n$) with period $p^2(p - 1)$. Furthermore, there are exactly $p(p - 1)^2/2$ residues of $n$ modulo $p^2(p - 1)$ which imply $S_n$ to not be congruent to a quadratic residue modulo $p$. Thus, we can identify, based on the residue of $n$ modulo $p^2(p - 1)$, whether $S_n$ is congruent to a quadratic residue modulo $p$. This is beneficial since I imagine computing $n$ mod $p^2(p - 1)$ is faster than checking whether $S_n$ is a perfect square. With this ""result"" applied to only the first six odd primes $3,5,7,11,13,17$ (I have verified the conjecture is true for these), it appears we already can say that for about $96.4 \%$ of the natural numbers, $S_n$ is not a perfect square. You could probably show this rigorously via the Chinese Remainder Theorem, but whatever. (And of course, I imagine the more primes you use, the better the results, but at some point too many modulus operations becomes a burden.)","['number-theory', 'square-numbers', 'diophantine-equations']"
2815561,"Find the volume bounded by the $xy$ plane, cylinder $x^2 + y^2 = 1$ and sphere $x^2 + y^2 +z^2 = 4$ [duplicate]","This question already has answers here : Find the volume of the region inside both the sphere $x^2+y^2+z^2=4$ and the cylinder $x^2+y^2=1$ (2 answers) Closed 6 years ago . Find the volume bounded by the $xy$ plane, cylinder $x^2 + y^2 = 1$ and sphere $x^2 + y^2 +z^2 = 4$. I am struggling with setting up the bounds of integration. First, I will calculate the 'first-quadrant' piece of the volume. $z$  will traverse from $0$ to $2$. $x$ should start from the cylinder and go to the edge of the current circle of the sphere:
$$\sqrt{1-y^2} \le x \le \sqrt{4-x^2-z^2}$$
However, the same applies to $y$: (I am only calculating half of the volume right now, where the smaller circle is the lower bound): $$\sqrt{1-x^2} \le y \le \sqrt{4-y^2-z^2}$$ 
However, this cannot work as both $x$ and $y$ are dependent. What is the error?","['multivariable-calculus', 'volume', 'calculus']"
2815627,Convergence of $\sum_{n=1}^{\infty} 3^n \sin(\frac{1}{4^nx})$,"I wish to prove the convergence of: $$\sum_{n=1}^\infty 3^n \sin\left(\frac 1 {4^nx}\right)$$ for $1\le x \lt \infty$, using Cauchy's criterion. Here is what I tried: \begin{align}
|S_{n+p}-S_n| & = \left| 3^{n+1} \sin\left(\frac 1 {4^{n+1}x}\right) 
 + \cdots+3^{n+p} \sin\left(\frac 1 {4^{n+p}x}\right)\right| \\[10pt]
& \le \left|4^{n+1} \sin\left(\frac 1 {4^{n+1}x}\right) \cdots 4^{n+p} \sin\left(\frac 1 {4^{n+p}x}\right) \sin\left(\frac 1 {4^{n+1}x}\right)(4^{n+1}+\cdots+4^{n+p}) \right|
\end{align} I tried using geometric series sum from here but came empty handed. how can I show that $|S_{n+p}-S_n|\lt \varepsilon$?","['cauchy-sequences', 'sequences-and-series', 'functions']"
2815687,Can this be computed analytically? [duplicate],"This question already has answers here : Prove: $\binom{n}{k}^{-1}=(n+1)\int_{0}^{1}x^{k}(1-x)^{n-k}dx$ for $0 \leq k \leq n$ (7 answers) Closed 6 years ago . For even values of $n$, can the following be proven analytically? $$(n+1)\binom{n}{\frac{n}2}\int_{\frac12}^1[x(1-x)]^{\frac{n}2}dx=\frac12$$ I cannot seem to compute this analytically. Is it possible? I can compute this numerically for various values of $n$: In [202]: from sympy import binomial as bnm; from numpy import power as pw
In [203]: wp=lambda n=10: (n+1)*bnm(n,int(n/2))*I(lambda x:pw(x*(1-x),int(n/2)),1/2,1)
In [204]: wp(n=18),wp(20),wp(26),wp(32),wp(42),wp(48),wp(52)
Out[204]: 
    (0.500000000000000,
     0.500000000000000,
     0.500000000000000,
     0.500000000000000,
     0.500000000000000,
     0.500000000000001,
     0.500000000000000) Please note that this question is not a duplicate of Prove: $\binom{n}{k}^{-1}=(n+1)\int_{0}^{1}x^{k}(1-x)^{n-k}dx$ for $0 \leq k \leq n$ The reason that this question is not a duplicate of that question is the different integration limits. The different integration limits completely alter the question as well as the solution.","['integration', 'definite-integrals']"
2815692,"System of differential equations $x'=f(y-x),\, y'=f(x-y)$","I have a problem regarding differential equations, $f$ is increasing. $x,y$ are dependent on $t$ 
$$ f(0)=0 $$
$$x'=f(y-x)$$
$$y'=f(x-y)$$
$$x(0)=1$$
$$y(0)=0$$
prove that when $x,y$ are the solutions of this equation then $$ \lim_{t \to \infty} x = \lim_{t \to\infty}y $$
So far I have managed to see that $x'(0)=f(-1)$ and $y'(0)=f(1)$ which implies that $x'(0)<0$ and $y'(0)>0$ and basically $x'(t)<0$, $y'(t)>0$ as $x>y$ so as $f$ is increasing and $x$ is declining with $t$ and $y$ is increasing with $t$, the derivatives of $x$ and $y$ will be declining and increasing respectivly and they will have the boundary. Can anyone help from this on?","['differential', 'ordinary-differential-equations']"
2815705,Packing a triangle full of circles,"This is a semi-popular doodleing thing people do when they have a pen, paper and some time I want to construct one of these mathematically, and I got to a point where I'm able to find the coordinates of the triangles I marked on the right using the method on the left But I have no idea how to construct the other ones. I know everything about the triangle.","['vectors', 'geometry']"
2815711,Prove that $g$ is tangent to the graph of $f$,"So I had these two functions and the following exercise: $f(x)=x^3-2x^2$ $g_p(x)=px$ Prove that $g_{-1}$ is a tangent line to the graph of $f$ . How many points do $g_{-1}$ and $f(x)$ have in common? So yeah, the points in common are fairly simple to figure out: Points in common: $x=1$ and $x=0$ So to see if it's actually tangent to the graph, I took the derivative of both functions at the points where they meet. The result: $f'(x)=3x^2-4x$ $g'_{-1}(x)=-1$ So $f'(1)=-1$ and $f'(0)=0$ So I'm not sure why I get this answer? By the question, we are supposed to prove that its tangent to the graph of $f$ but by my calculation, it seems like only $f(1)$ is tangent with $g_{-1}(1)$ . Help is appreciated! (Please avoid hint answers).","['derivatives', 'tangent-line', 'calculus']"
2815734,Definite integral $\int_0^{\pi/4}\log\left(\tan{x}\right)\ dx$,"$$\int_0^{\pi/4}\log\left(\tan{x}\right)\ dx$$
My turn :
$$I=\int_0^{\pi/4}\log\left(\sin{x}\right)-\log\left(\cos{x}\right)\ dx$$
$$I_1=\int_0^{\pi/4}\log\left(\sin{x}\right)\ dx$$
let
$$u=2x$$
$$I_1=\frac{1}{2}\int_0^{\pi/2}\log\left(\sin{\frac{u}{2}}\right)\ du$$
But i could not evaluate the last integral ?","['integration', 'definite-integrals', 'calculus']"
2815737,Analyze the symmetric property of positive definite matrices [duplicate],"This question already has answers here : Do positive semidefinite matrices have to be symmetric? (3 answers) Closed 6 years ago . In the definition, a positive definite matrix is usually referred to symmetric expressed in quadratic form. So I am confused about is it always symmetric? Why do they refer to the symmetric property in its definition? Please give me some examples and proof of this problem.","['matrices', 'symmetric-matrices', 'positive-definite', 'linear-algebra']"
2815751,Lipschitz constant of limit of functions,"Consider two metric spaces $(X,d_X)$ and $(Y,d_Y)$ and define the lipschitz constant of every continuous function $f:X\rightarrow Y$ as $$Lip(f):=\sup\limits_{x\neq y}\frac{d_Y(f(x),f(y))}{d_X(x,y)}$$ Consider a sequence of continuous functions $f_n:X\rightarrow Y$ such that there is a $k>1$ such that for every $n\in \mathbb{N}$ it is $Lip(f_n)\le k$ $\{f_n\}$ has limit $f:X\rightarrow Y$ for the uniform convergence on 
compact sets (this means that for every $K\subset X$ compact it results $\lim\limits_{n\rightarrow \infty}\sup\limits_{x\in K}d_Y(f(x),f_n(x))=0$) $Lip(f_n)\rightarrow 1$ Question: does it follow $Lip(f)=1$? Can you motivate your answer? I am sorry if I do not show my reasoning, but I can not even understand if the statement is true or not","['functional-analysis', 'real-analysis', 'lipschitz-functions', 'analysis']"
2815752,Gambling game: What is the probability of eventually going broke,"You find 2  dollars in your pocket and decide to go gambling. Fortunately, the game you're playing has very favourable odds: each time you play, you gain 1 dollar with probability 3/4 and lose $1 with probability 1/4. Suppose you continue playing so long as you have money in your pocket. If you lose your first two bets, you go broke and go home after only two rounds; but if you win forever, you'll play forever. What's the probability you'll eventually go broke? I think I am overthinking this: To go broke, you have to end on two Losses, which have p(L) = 1/4 Before the two losses there has to have been an even number of losses and win to have a balance of 2 dollars before the final two losses. $P(Broke)=\left( \dfrac {1}{4}\right) ^{2}\sum ^{\infty }_{i=0}\left( \dfrac {1}{4}\right) ^{i}\left( \dfrac {3}{4}\right) ^{i}N_{i}$ But then for each summand I need to multiply by the number of ways it is possible to get to an equal number of losses is without going broke beforehand. so for i = 1 LW and WL, are allowed, for i = 2 WLLW , WLWL, LWWL, LWLW, LLWW are allowed. So my final calculatin is $P(Broke) = \left( \dfrac {1}{4}\right) ^{2}\sum ^{\infty }_{i=0}\left( \dfrac {1}{4}\right) ^{i}\left( \dfrac {3}{4}\right) ^{i}\left( \begin{pmatrix} 2i \\ i \end{pmatrix}-\sum ^{i-1}_{k=1}\begin{pmatrix} 2k \\ n-1 \end{pmatrix}\right) $ But this doesn't equal anything and seems rather complicated.","['algebra-precalculus', 'probability']"
2815783,"If $fg_n$ converges in $L^p$ for all $f\in L^p$, then $g_n$ converges in $L^\infty$.","Let $(X,\mathcal{A},\mu)$ be a measure space with $\mu(X)<\infty$. I know that if $g:X\rightarrow\mathbb{R}$ satisfies that $fg\in L^p$ for all $f\in L^p$, then $g\in L^\infty$. My question is whether there is a ""sequential version"" of this result: if $fg_n$ converges in $L^p$ for all $f\in L^p$, then $g_n$ converges in $L^\infty$. I would like a reference of this result (if it is true).","['real-analysis', 'reference-request', 'lebesgue-measure', 'functional-analysis', 'lebesgue-integral']"
2815789,Limit of $S_N = \sum_{i=0}^N X_i$ where $X_i$ is Laplace distributed and $N$ is Poisson distributed.,"Let $S_N = \sum_{i=0}^N X_i$ where $X_i$ is Laplace distributed with parameter $a$ and independent. Let $N \sim \text{Poi}(m)$ independent of the $X_i$. I want to find the limit distribution of $S_N$, given that when $m \to\infty$ and $a \to 0$ then $m  a^2 \to 1$. My attempt is $\text{E}[S_n]=0$ since $X_i$ independent and $\text{E}[X_i]=0$. Furthermore, $\text{Var}[S_n]=2ma^2$ given $N=n$, since $X_i$'s and $N$ independent and $\text{Var}[X_i]=2a^2$. Then I believe Central limit theorem and Slutsky's theorem would fit nicely, however I'm not sure how to proceed due to how the $N,m$ tends to infinity. I also tried with using characteristic functions, but that didnt seem make any more sense. All help is greatly appreciated.","['probability-limit-theorems', 'probability-theory', 'probability', 'probability-distributions']"
2815801,Existence of at least one inert prime,"Let $L/K$ be an extension of number fields. Denote by $\mathcal{O}_L$ and $\mathcal{O}_K$ the rings of integers of $L$ and $K$. Given a prime ideal $\mathfrak{p} \subseteq \mathcal{O}_K$, the corresponding ideal $\mathfrak{p}\mathcal{O}_L := \{ \sum_i p_i x_i : p_i \in \mathfrak{p}, x_i \in \mathcal{O}_L \}$ in $\mathcal{O}_L$ decomposes uniquely into prime ideals as
$$
\mathfrak{p}\mathcal{O}_L = \mathfrak{q_1}^{a_1}\mathfrak{q_2}^{a_2} \cdots \mathfrak{q}_k^{a_k}.
$$
One says that $\mathfrak{p}$ is an inert prime if we have $k=1$ and $a_1 = 1$, i.e. if the ideal stays prime in $\mathcal{O}_L$. My question is: Does there exist at least one prime ideal in $\mathcal{O}_K$ which is inert?","['number-theory', 'maximal-and-prime-ideals', 'extension-field', 'algebraic-number-theory']"
2815823,A problem in trigonometry,"If  $\tan(x+y) =a+b$
and, $\tan(x-y)=a-b$
Then prove that:$a \tan(x) -b \tan(y)=a^{2}-b^{2}$ I  have used the formulae for $\tan(x+y)$ and $\tan(x-y)$ and then cross multiplied and then found $a$ and $b$ individually and then tried to put the values of $a$ and $b$ in ""$a \tan(x) -b \tan(y)$""(l.h.s) but it did not lead to the r.h.s.",['trigonometry']
2815826,Classifying evolution operators with always-positive outputs given positive initial conditions,"Consider the following evolution equation for $c$, given by a convection-diffusion equation over one spatial dimension with positive $D$ and constant $v$: $c_t = Dc_{xx} - vc_x$ Physical reasoning—and arguably common sense—indicate that, by selecting $c(x,0) \geq 0$ everywhere, you should expect to always find $c(x,t) \geq 0$ for $t > 0$. The same intuition should hold for similar systems (i.e. the diffusion equation) and in higher dimensions. Is there a way to generally identify evolution operators that possess this property, in the sense that they always output $c(x,t)\geq0$ for $t\geq0$ given $c(x,0) \geq 0$? The same question can be framed from a mathematical modeling perspective: How do I know whether or not my PDE model for transport of a positive quantity is plausible in the sense that it won't output negative values a priori ? A comment: This question is inspired by Pawula's theorem, which indicates that a Taylor series expansion of a master evolution equation for some probability density must either stop at the second-order term or before (giving a Fokker-Planck equation) or contain infinite terms. This is due precisely to the fact that expanding to a finite number of terms larger than second-order can generate equations that evolve the probability density into negative states: see Hannes Risken's book for a discussion of this. Another motivation comes from the Ostrogradsky instability , which seems to rule out equations of motion depending on derivatives larger than second-order because it would allow negative energies. Perhaps both of these examples are related?","['partial-differential-equations', 'probability-theory', 'operator-theory', 'functional-analysis', 'mathematical-modeling']"
2815859,Hartshorne Algebraic Geometry Exercise II.2.16.,For part a) of this exercise we need to show that for a scheme $X$ and an open affine subset $U=$ Spec $B$ that $U \cap X_f = D(\bar{f})$ where $\bar{f}$ is the restriction of $f \in \mathcal O_X(X)$ to $\mathcal O_X(U)$ and $X_f= \{x \in X :$ the stalk of $f$ at $x$ $(f_x)$ is not contained in the maximal ideal $(m_x)$  of $\mathcal O_x \}$. Now I believe the proof goes something along the lines of using the fact that this maximal ideal in the affine scheme is just $xB_x$ but I can't see how $f_x \not\in m_x$ implies $\bar{f}_x \not\in xB_x$. Like how can we go from the local ring $\mathcal O_x$ to $B_x$?,"['schemes', 'algebraic-geometry']"
2815866,How to use dimensional analysis to find these integrals?,"Use dimensional analysis to find $\displaystyle \int_0^∞ \mathrm{e}^{-ax} \,\mathrm{d}x$ and $\displaystyle \int\frac{\mathrm{d}x}{x^2 + a^2}$.  A useful result is$$
\int \frac{\mathrm{d}x}{x^2 + 1} = \arctan x + C.
$$ I am using the the book called street mathematics to learn more about dimensional analysis. I am trying to understand a problem in the book. The question is to use dimensional analysis to find the solutions to two integrals. both are in the attached photo. I tried to understand the question and how to best tackle it but I did not succeed.","['dimensional-analysis', 'calculus']"
2815886,Simultaneous eigenvectors of symmetric and antisymmetric parts,"This question will seem over-specific and obscure, but the motivation comes from a problem I am trying to solve in game theory. I hope someone can help, as it requires only linear algebra! Let $H$ be a real invertible matrix, decomposed into symmetric and antisymmetric parts as $H = S+A$. Assume that $S$ is positive semi-definite, and that there exists a simultaneous eigenvector $u$ of $S$ and $A$ such that $Su = 0$ and $Au = \lambda u$ with non-zero (pure imaginary) $\lambda$. Prove or disprove that $u$ is also an eigenvector of $S_d$, the sub-matrix of $S$ consisting of its diagonal part only. I can neither prove this nor find a counter-example. It is definitely true for $2 \times 2$ matrices, and I think also for $3 \times 3$. In the general case, notice that $u$ is also an eigenvalue of $H$ since $Hu = Au = \lambda u$. [The assumption that $\lambda \neq 0$ is superfluous since $H$ is assumed invertible.] I tried to use a criterion on the possibility of a matrix having pure imaginary eigenvalues, e.g. that there exists a rank-1 matrix $M$ such that $HM$ is antisymmetric ( ref ). I have also tried to use a relationship between eigenvectors and diagonal elements of a diagonalisable matrix ( ref ). I didn't get very far. EDIT: Thanks to fedja's counter-example below, the answer to this question is no. Any such $u$ is not necessarily an eigenvector of $S_d$. The question I am really interested in, however, is the following. If any two such eigenvectors $u_i$ and $u_j$ exist with distinct eigenvalues, they must be orthogonal since $A$ is anti-symmetric. Can we also prove that $u_i$ and $S_d u_j$ are orthogonal, namely
$$ u_i^* S_d u_j = 0 \ ? $$
As you can see, if we could have shown that $u_i$ is an eigenvector of $S_d$, we would be done. This is not true, but fedja's counter-example does not contradict this more restrictive claim.",['linear-algebra']
2815906,Upcrossing inequality: from discrete to continuous time,"I'm struggling with the proof of the upcrossing inequality for stochastic processes: Let $(X_t)_{t \geq 0}$ be a submartingale w.r.t. a filtered probability space $(\Omega, \mathcal{F}, \mathbb{F}, P)$ such that for every $\omega \in \Omega$ the map $t \mapsto X_t(\omega)$ is continuous. Let $[\sigma, \tau]$ be a subinterval of $[0, \infty)$ and let $\alpha < \beta, \lambda > 0$ be real numbers. Then we have
$$\mathbb{E}[U_{[\sigma, \tau]}(\alpha, \beta, X(\omega))] \leq \frac{\mathbb{E}[X_\tau^+] + |\alpha|}{\beta - \alpha}$$
where $U$ denotes the number of upcrossings of $X$ from $\alpha$ to $\beta$ on the interval $[\sigma, \tau]$. This version of the upcrossing inequality and the proof I am concerned with is taken from Theorem 6 of these lecture notes . What I don't understand is the proof of ""$\leq$"" in the proof of claim 2. Since $F$ may also contain non rational numbers, why must $U_F \leq U_{F_k}$? Or, to put it more generally: if we try to prove the upcrossing inequality by using the discrete time result and finding an increasing series of sets of rational indices that converges towards a set thats dense in the interval $[\sigma, \tau]$,  how can we ensure that we have upcrossings that happen within non rational numbers? Alternatively, I'd also be very happy to see an alternative rigorous proof of the continuous time upcrossing inequality. Thanks!","['stochastic-processes', 'probability-theory', 'martingales']"
2815928,Example to linear but not continuous,"We know that when $(X,\|\cdot\|_X)$ is finite dimensional normed space and $(Y,\|\cdot\|_Y)$ is arbitrary dimensional normed space if $T:X \to Y$ is linear then it is continuous (or bounded) But I cannot imagine example for when $(X,\|\cdot\|_X)$ and $(Y,\|\cdot\|_Y)$ are arbitrary dimensional normed spaces $T:X \to Y$ is linear and not bounded or continuous. Could someone give any simple example please? Thanks","['functional-analysis', 'normed-spaces', 'vector-spaces']"
2815944,Was Atiyah's proof of the odd order (Feit-Thompson) theorem false?,"I read last year that Atiyah thought he had found a proof of the odd order theorem of only 12 pages, using $K$-theory, and that people were trying to figure out if it was correct or not. But I never heard about it afterwards.
Did people actually go through it? Was it correct or not?","['finite-groups', 'abstract-algebra', 'algebraic-topology', 'k-theory', 'group-theory']"
2815985,"Prove that if ${x_1, x_2, x_3}$ are roots of ${x^3 + px + q = 0}$ then ${x_1^3+x_2^3 + x_3^3 = 3x_1x_2x_3}$","How to prove that ${x_1^3+x_2^3 + x_3^3 = 3x_1x_2x_3}$ holds in case ${x_1, x_2, x_3}$ are roots of the polynomial? I've tried the following approach: If $x_1$, $x_2$ and $x_3$ are roots then $$(x-x_1)(x-x_2)(x-x_3) = x^3+px+q = 0$$ Now find the coefficient near the powers of $x$: $$
x^3 - (x_1 + x_2 + x_3)x^2 + (x_1x_2 + x_1x_3 + x_2x_3)x - x_1x_2x_3 = x^3+px+q
$$ That means that I can write a system of equations: $$
\begin{cases}
-(x_1 + x_2 + x_3) = 0 \\
x_1x_2 + x_1x_3 + x_2x_3 =  p \\
- x_1x_2x_3 = q
\end{cases}
$$ At this point I got stuck. I've tried to raise $x_1 + x_2 + x_3$ to 3 power and expand the terms, but that didn't give me any insights. It feels like I have to play with the system of equations in some way but not sure what exact.","['polynomials', 'linear-algebra']"
2816013,ODE initial conditions and integrating backwards in time,"This is a very basic question on initial value problems and ordinary differential equations. When solving an IVP, with $y$ as the dependent variable and $t$ as the independent variable (given some set of initial conditions $y(0)$, $y'(0)$,...), why do we ""integrate forward"" in time? That is, in an engineering/physics context the author would typically say something along the lines of $y(t)$ can be found for $t > 0$ (or generally $t > t_0$ where $t_0$ is the point at which the initial condition is specified). Doesn't this assume that the output was zero or undefined before the initial condition? Mathematically, can't we just as well find $y$ before $t = 0$ if we are given a set of initial conditions?",['ordinary-differential-equations']
2816038,the connection between matrix and convex cone,I'm trying to understand the connection between convex cone and matarix. according to Boyd  as you can see in the pic: X is a p.s.d matrix but how this matrix represent a convex cone? and why the matrix is looking like this the the value 'y' appears twice? what am I missing from here? Another thins is what should I do when the matrix is 3x3 or 4x4? how do I need to build the matrix then? thnx in advanced,"['matrices', 'convex-optimization', 'optimization', 'convex-cone']"
2816055,How can I prove the following equality?,"I need to prove:
$$\sum_{k=0}^{n} {k\choose 2}\cdot{{n-k}\choose 2} = \sum_{k=0}^{n} {k\choose 3}\cdot(n-k).$$ I wasn't able to have any progress with algebra, I tried to think about a combinatorical proof: Let's imagine there are $n$ people in a row and we need to choose $4.$ 
One way will be to take the $k$ ""leftest"" people, choose $2$ of them, and choose $2$ from the $n-k$ left. That way we get the left sum. The second way will be similar, look at the ""leftest"" $k$ people and choose $3$ of them, and then choose $1$ from the $n-k$ left. That gives the right side of the equation. The only problem with this proof is that some of the options are counted twice and more, so it is not equal to ${n\choose 4.}$ Can someone help please? I would love to see both algebric and combinatorical proofs.","['combinatorics', 'binomial-theorem', 'summation', 'binomial-coefficients']"
2816087,A generalization of conditional expectation,"Consider the following generalization of conditional expectation: Start with measurable spaces $A,B$ equipped with measures $\mu_A, \mu_B$ respectively and a measurable map $T : A \to B$ s.t. $\mu_A \circ T^{-1} \leq C \mu_B$ for some $C > 0$ (i.e. this inequality holds event-wise). Then $T$ induces (in a functorial, contravariant way) a linear map $$\mathcal L_2(T) : \mathcal L_2(B,\mu_B) \to \mathcal L_2(A,\mu_A), g\mapsto g\circ T.$$ Indeed $$\|g\circ T\|_2 = \int g(Tx)^2\,d\mu_A(x) \leq \int g(y)^2 \,dC\mu_B(y) = C\|g\|_2,$$ which shows that $\mathcal L_2(T)$ is well-defined and bounded with operator norm $\leq C$ (in fact, it is an isometry if $\mu_A \circ T^{-1} = \mu_B$). Then $\mathcal L_2(T)$ has an adjoint operator $\mathbb E^T$. If we consider a probability space $(\Omega, \mathcal F, \mathbb P)$ and sub-$\sigma$-algebra $\mathcal G$ of $\mathcal F$. and set $A = (\Omega,\mathcal F)$, $B = (\Omega, \mathcal G)$, $\mu_A = \mathbb P$, $\mu_B = \mathbb P_{|\mathcal G}$ and $T(t) = t$, the defining property of adjoints implies $$\mathbb E(\mathbb E^T X \cdot \mathbb 1_B) = \mathbb E(X\cdot \mathbb 1_B)$$ for all $B\in \mathcal G$. So $\mathbb E^T$ is the conditional expectation of $X$ w.r.t. $\mathcal G$. Are there any important examples of $\mathbb E^T$ besides conditional expectation?","['functional-analysis', 'reference-request', 'probability-theory']"
2816109,Prove that any function can be written as the sum of an even function and an odd function. [duplicate],This question already has answers here : How do I divide a function into even and odd sections? (3 answers) Closed 6 years ago . I understand some of the basic concepts that surrounds even and odd functions but this question just stumped me and I'm not sure on how to tackle it. Any Starting points/methods would be helpful Prove that any function can be written as the sum of an even function and an odd function.,"['even-and-odd-functions', 'functions']"
2816127,"Find the operator norm: $T\in B(L^{2}(\mathbb{R},e^{-x^{2}}dx)) $, $Tf(x) = f(x+1)$. $||T|| = $?","Let $L_{e}(\mathbb{R})$ denote the Hilbert space with inner product $\langle f, g \rangle = \int_{-\infty}^{\infty}f(x)\overline{g(x)}e^{-x^2}dx$. Let $T: L_{e}(\mathbb{R}) \to L_{e}(\mathbb{R})$ be an operator defined by $Tf(x) = f(x+1)$. I need to find the adjoint operator of $T$, $T^{*}$, and find $||T||$ in the operator norm. I was able to find $T^{*}$, but unable to find the norm. I'll share my proof for $T^{*}$ and the progress I made on finding the norm (which might be useless towards finding an actual solution, but it's all I have). Finding $T^{*}$: $$ \langle Tf, g \rangle = \int_{-\infty}^{\infty} f(x+1)\overline{g(x)} e^{-x^2}dx =^{1} \int_{-\infty}^{\infty} f(t)\overline{g(t-1)}e^{-(t-1)^2}dt = \int_{-\infty}^{\infty} f(t)\overline{g(t-1)e^{t^2-(t-1)^2}} e^{-t^2}dt = \int_{-\infty}^{\infty} f(t) \overline{g(t-1)e^{2t-1}} e^{-t^2}dt = \langle f, T^{*}g \rangle.$$ In the equality marked by footnote $1$, I made a substitution $t = x+1$. Therefore: $$T^{*}g(t) = g(t-1)e^{2t-1}.$$ Now, for my progress on finding the norm (and some random ideas): $$||Tf||^2 = \int_{-\infty}^{\infty}|f(x+1)|^{2}e^{-x^2}dx = \int_{-\infty}^{\infty}|f(t)|^{2} e^{-(t-1)^2}dt = \int_{-\infty}^{\infty} |f(t)|^{2}e^{2t-1}e^{-t^2}dt = \frac{1}{e} \int_{-\infty}^{\infty}|f(t)e^{t}|^{2}e^{-t^2}dt = \frac{1}{e}||fe^{t}||^2.$$
Is there an some kind of submultiplicativity inequality that I could use here? Also, some other ideas: finding $||T^{*}||$ instead of $||T||$; also, if I could prove $T$ is compact (which I also am not able to; I don't even know if it is), I could then use the fact that $TT^{*}$ is compact and self-adjoint, and therefore $||TT^{*}|| = ||T||^2 = \max_{\lambda} |\lambda|$, where the $\lambda$'s are the eigenvalues of $TT^{*}$. However, I haven't made any progress on any of these.","['functional-analysis', 'operator-theory', 'adjoint-operators']"
2816138,Extension of map from the generic fiber of a Neron model,"Let $E \to \operatorname{Spec} K$ be an elliptic curve and $S$ be its Neron model. That is, a scheme $S$ over a Dedekind domain $R$ with $E/K$ as the generic fiber of the structure morphism $N \to \operatorname{Spec}R$. Given the addition morphism $n_E:E\to E$, how would one go about showing that this can be extended to a morphism on a dense open subset of $S$? This fact is used in Silverman's Advanced Topics in the Arithmetic of Elliptic Curves, Theorem IV.5.c, but it is stated without proof. I have (so far) been unable to prove it myself or find a reference, although it seems frustratingly simple.","['arithmetic-geometry', 'elliptic-curves', 'algebraic-geometry']"
2816144,LU decomposition of a diagonally dominant matrix,"Let $A \in M_{n\times n}(\mathbb{R})$ a matrix such that $a_{ij}< 0$ for $i \ne j$ and  $A$ is diagonally (row) dominant, that is $a_{ii}>\sum_{j\ne i} |a_{ij}|$. I know that all the leading minors of $A$ are $>0$, and so $A$ has an LU decomposition 
$$A= L \cdot D \cdot U$$
where $L$ is lower triangular with $1$ on the diagonal, $D$ is a diagonal matrix with positive diagonal elements, $U$ is upper triangular with $1$ on the diagonal.
I would like to get a confirmation whether the off-diagonal elements of $L$ and $U$ are negative.","['matrices', 'linear-algebra']"
2816150,Showing an $L^2$ convergence (with convergence rate),"We have $(X_i)_{i \in \mathbb Z}$ iid random variables with $1\le X_i \le2$ almost surely. We define $X(x,\omega) \equiv X_i (\omega)$ if $x\in [i,i+1[$ (so it is bounded almost surely) and, for $\epsilon >0$, $X_\epsilon (x, \omega) \equiv X(x/\epsilon, \omega)$. Define for $x\in [0,1]$ $$u_\epsilon(x,\omega)= \int_0^x \frac {c_\epsilon(\omega) - F(x)}{X_\epsilon(x,\omega)}\, dx$$ where $F$ is an $L^1(]0,1[)$ function (we can add continuity of $F$ on $[0,1]$ if it helps solving the problem) and $c_\epsilon(\omega)$ is defined by $$c_\epsilon(\omega)\equiv \frac{\int_0^1 \frac{F(y)}{X_\epsilon(y,\omega)} \, dy}{\int_0^1\frac 1 {X_\epsilon(y,\omega)}\, dy}$$ It can be shown that for each $x\in [0,1]$, we have almost surely (we use law of large numbers and the density of the staircase functions in the $L^1$ functions) $$u_\epsilon(x,\omega) \to u_0(x) \equiv -E[\frac 1 {X_1}](\int_0^x F(y) dy - x \int_0^1 F(y) dy) $$ when $\epsilon \to 0^+$ We would like to find a function $f$ such that $$\Vert u_\epsilon - u_0\Vert_{L^2(]0,1[ \times \Omega)} \le f(\epsilon) \to 0$$ when $\epsilon \to 0^+$ Related questions: Show that those random quantities converge in distribution to a normal variable (hard analysis problem) How to use the Lindeberg CLT in this scenario ? (analysis problem) Controlling this function in $L^2$ norm","['real-analysis', 'probability-theory', 'functional-analysis', 'probability', 'measure-theory']"
2816184,Proving $2+2^2/2+2^3/3+2^4/4+\cdots=0$ elementarily,"In the first chapter of Gouvea's intro to $p$-adics, there's a heuristic argument that $$ \frac{2}{1}+\frac{2^2}{2}+\frac{2^3}{3}+\frac{2^4}{4}+\cdots=0 \tag{$\ast$}$$ as $2$-adic numbers, since it's the Mercator series for $\ln(-1)$ and $2\ln(-1)=\ln(-1)^2=\ln1=0$. (Like I said, heuristic.) I assume $(\ast)$ can be proven by analyzing $\ln(1+x)$ as a function of $p$-adic numbers, but there's an exercise that says we can show $(\ast)$ by elementary means. But how? I feel like I've considered this question in the past before, but don't remember if I ever solved it. We should be able to prove it's congruent to $0$ mod $2^N$ for any $N$. This automatically truncates the series to a finite sum, and all of the denominators divisible by $2$ are underneath numerators even more divisible by $2$, so it's well-defined. Perhaps we can split the sum into subsums of even and odd indices and establish a recursion?","['number-theory', 'p-adic-number-theory', 'modular-arithmetic', 'elementary-number-theory']"
2816226,Is a perfect game of Set always possible?,"For anyone not familiar with the game of Set, I'll refer you to the description on this question . My question is this: The game ends when there are no more cards remaining in the deck and there are no sets visible on the table.  I will define a ""Perfect Game"" as one in which there are no cards at all on the table at the end, i.e. all 81 cards have been removed as part of a set.  Can any ordering of cards result in a Perfect Game? Now, in practice, this is a fairly rare occurrence.  Games typically end with 6, 9, or 12 cards remaining.  They can even end with as many as 15 (this seems to be even less common than a 0-card ending, but I have seen it happen). 18+ I believe to be impossible.  I think that 16 is the maximum number of cards that can be showing without making a set, and there must always be a multiple of 3, so that rules out anything > 15. (Edit: Apparently up to 18 is possible.)  A 3-card ending is also impossible, per the question I linked above.  (The last 3 must form a set, so this would result in a Perfect Game.) Now, in a normal game, there are often times where more than one set is visible, and which one you pick can affect the outcome of the game at the end, and you have no way of knowing what those outcomes would be.  But let's say that we are given full knowledge ahead of time about the exact order of cards in the deck.  Let's say we also have the ability to perfectly predict the outcomes of each choice, or alternatively to ""hit Undo"" and backtrack all the way to the beginning as often as needed.  Could we with this knowledge ensure that any ordering of cards can result in a Perfect Game? I suspect that this would be easier to disprove than to prove - Simply find a contrived deck ordering which at no time gives you multiple choices of visible sets or which we can trace the game through to all possible endings if there are choices, and prove that they all must leave some cards on the table.  Proving that any deck order can end perfectly would be quite the challenge... Edit: I just noticed this question , but it is not a duplicate since it does not consider the possibility of knowing the order of cards ahead of time.","['discrete-geometry', 'combinatorics', 'finite-geometry', 'card-games']"
2816227,What is $\lim_{n\to\infty} \left(\sum_{k=1}^n \frac1k\right) / \left(\sum_{k=0}^n \frac1{2k+1}\right)$?,"I have the following problem: Evaluate $$ \lim_{n\to\infty}{{1+\frac12+\frac13 +\frac14+\ldots+\frac1n}\over{1+\frac13 +\frac15+\frac17+\ldots+\frac1{2n+1}}} $$ I tried making it into two sums, and tried to make it somehow into an integral, but couldn't find an integral. The sums I came up with, $$ \lim_{n\to\infty} { \sum_{k=1}^n {\frac1k} \over {\sum_{k=0}^n {\frac{1}{2k+1}}}} $$","['real-analysis', 'summation', 'limits']"
2816246,"$ \varphi(x_1, \dots , x_n) = (c_1x_1, \dots, c_nx_n)$. Then, $\mu(\varphi(A)) = |c_1\dots c_n|\mu(A)$, where $\mu$ is the Lebesgue measure","Given $c_1, \dots, c_n \in \mathbb{R}$, consider the function $\varphi: \mathbb{R}^n \rightarrow \mathbb{R}^n, \varphi(x_1, \dots , x_n) = (c_1x_1, \dots, c_nx_n)$. If $\mu$ is the Lebesgue measure, show that for every $A \in \mathcal{M}(\mathbb{R})$ (set of Lebesgue measurable sets), we have
\begin{equation*}
\mu(\varphi(A)) = |c_1\dots c_n|\mu(A)
\end{equation*} First, notice that $\varphi(A) \in \mathcal{M}(A)$. \begin{align*}
\mu(\varphi(A)) &= \int_{\varphi(A)} 1 dm(x)\\
&= \int_{\mathbb{R}^n} \chi_{\varphi(A)} dm(x)
\end{align*} Now, notice that $x = (x_1, \dots, x_n) \in \varphi(A)$ iff $x_i = c_iy_i$, for some $y = (y_1, \dots, y_n)$. I don't know how to put this information inside the integral so that I can conclude the result. Could someone help me?","['lebesgue-measure', 'integration', 'lebesgue-integral', 'measure-theory']"
2816251,"Krull dimension of $k[x_{1},...,x_{n}]$ is $\leq n$.","Let $k$ be a field and $R$ be the polynomial ring $k[x_{1},...,x_{n}]$. $\mathbf{Problem}$: Show that any nonzero prime ideal of $R$ contains an element $f$ such that $R(f)$ is transcendental over $R$ with transcendence degree $1$.  Use induction to show that the Krull dimension of R is at most $n$. First of all, I'm confused if there is a typo and it should say ""$k(f)$ is transcendental over $k$"".  In any case, the most I've got towards a solution is observing that any nonzero element of a prime ideal of $R$ generates an extenstion of $k$ of transcendence degree $1$, but that doesn't seem very helpful for the induction. EDIT: It is a typo, it should be $k$ instead of $R$. The whole point of this exercise is to reduce to the case where the algebra is a polynomial ring.  But it seems like to do the induction you need to pass to a quotient which might not be a polynomial ring.","['abstract-algebra', 'algebraic-geometry']"
2816259,Geometric Interpretation of Eigendecomposition,"If we have an orthogonal matrix $U$ , then $U^Tx$ is essentially a rotation of the vector $x$ . If we have a diagonal matrix $\Lambda$ , then $\Lambda x$ is scaling the vector $x$ in each direction by the corresponding diagonal value. Since any symmetric matrix $S=U\Lambda U^T$ , $Sx=U\Lambda U^Tx$ which is rotation of $x$ by some angle $\vartheta$ , scaling it by $\Lambda$ and then re-rotating by angle $-\vartheta$ . Does this imply that multiplying by any symmetric matrix $S$ is just a scaling by its eigenvalues since the net rotation is $0$ ?","['eigenvalues-eigenvectors', 'topological-vector-spaces', 'linear-algebra', 'vector-spaces']"
2816267,Area between two spirals,"I need to find the area between these two spirals given in the polar coordinates: 
$$r = e^{5 \theta}$$
$$r = e^{10 \theta}$$
$$0 \le \theta \le 3\pi$$
This seems to be quite simple, yet this problem is marked as ""hard"" and so I am 
not sure of the solution. I think that it is enough to find the area between these two exp curves and then 'convert' this area into the polar coordinates. The Jacobian is $r$. The are will be $$\int_{\theta = 0}^{\theta = 3 \pi} \int_{r = e^{5 \theta}}^{r = e^{10 \theta} } r drd\theta$$ Is my method a correct way to solve this?
If not, please, tell me where the mistake is before posting your own, different solution.","['multivariable-calculus', 'integration', 'calculus']"
2816277,Helpful examples in learning Abhyankar's conjecture for the affine line?,"I'm reading Raynaud's proof for the Abhyankar conjecture on $\mathbb{A}^1$, namely, Let $k$ be an algebraically closed field of characteristic $p>0$. Then
  every quasi-$p$ group is the Galois group of some connected etale
  cover of $\mathbb A_k^1$. But I could't find illustrative examples in the literature. There are some explicit examples in Abhyankar's old papers, but they look quite concise. In this 1992 paper , Abhyankar listed some Galois groups coming from certain types of equations, but the calculation are not fully showed (some of the them even involve the classifying result of finite simple groups). So I wonder: is there some explicit examples in the literature, that can (partly) exhibit the spirit of Raynaud's proof? (Actually I would be glad to see a ""computable"" cover aside from the Artin-Schreier.) Any suggestion would be greatly appreciated.",['algebraic-geometry']
2816362,Meaning behind Filter in Set Theory,"In a course in logic and set theory, we studied the concept of a Filter. We defined a filter $F \in P(S)$ on $S$ an equivalent of the following definition from Jech's Introduction to Set Theory: (a) $S \in F$ and $\emptyset \notin F.$ (b) If $X\in F$ and $Y \in F$ then $X \cap Y \in F$. (c) If $X \in F$ and $X \subseteq Y \subseteq S$, then $Y \in F$. I am having trouble grasping this concept and it's meaning. My question is, what is the intuition behind this definition, and why are these kinds of sets called filters? Thanks","['intuition', 'terminology', 'filters', 'elementary-set-theory', 'definition']"
2816377,Can someone explain to me why hot hand phenomenon is considered a fallacy?,"Hot hands refers to the idea that a player who has scored a basket (therefore, has ""hot hands"") is more likely to score the next basket. It is suggested that this is a fallacy because apparently scoring is considered a random event (as far as I understand it).  It is the equivalent of flipping coins.  And just as when you flip coins, you  might get three heads in a row by chance, the same applies to scoring in basketball.  So players probably remember those sequences when they scores several baskets in a row and think it had something to do with them. Now I can not shake the feeling that this explanation is incomplete. Let me compare scoring with my efforts to learn probability on my own, which I have been doing for a while. When I get some question right, I become more energized and confident, and am more likely to work on the following question because I feel more hopeful that I will figure it out. But when I try several probability questions and get them all wrong, I am quite unlikely to try my best on the next one.  I have sometimes later returned to questions that I had failed at, noting that they were quite easy but that earlier I had simply lost the will to put in any effort. Anyhow, so to go back to the basketball example, why is each shot is assumed to be completely independent of the previous shots.  Why doesn't a player's effort or confidence level is irrelevant?  I can imagine hot hands applying to someone blindingly throwing the ball and once in a while getting lucky, but the same thing applies to professional players even?  Yes, the ball has no memory but the person throwing the ball does. No?","['probability', 'random']"
2816390,integral $\int \frac{1-a\cos x}{1+a^2-2a\cos x}dx$,"I'm trying to solve this integral: $$\int \frac{1-a\cos x}{1+a^2-2a\cos x}dx.$$ My trouble: Using software, i found that if $|a|>1$, the integral is equal to $$\arctan \left(\frac{x}{2}\right)+\frac{1-a^2}{|a-1||a+1|}\arctan \left(\frac{|a+1|}{|a-1|}\tan \left( \frac{x}{2} \right) \right )+C.$$ 
If $|a|<1$, then the antiderivate is $$x-\arctan \left(\frac{x}{2}\right)+\frac{1-a^2}{|a-1||a+1|}\arctan \left(\frac{|a+1|}{|a-1|}\tan \left( \frac{x}{2} \right) \right )+C.$$ Integrating over the interval $[0,2\pi]$, the integral is equal to $2\pi$ if $|a|<1$ and $0$ if $|a|>1$. Using some skills, like this: $$\int \frac{1-a\cos x}{1+a^2-2a\cos x}dx =\frac{1}{2}\int \frac{2-2a\cos x+a^2-a^2}{1+a^2-2a\cos x}dx $$ $$=\frac{1}{2}\int dx+\frac{1}{2}\int \frac{1-a^2}{1+a^2-2a\cos x}dx$$ I found the answer $$\frac{x}{2}+\frac{1-a^2}{|a-1||a+1|}\arctan \left(\frac{|a+1|}{|a-1|}\tan \left( \frac{x}{2} \right) \right )+C.$$ Integrating this over the same interval as before, the result is not correct. So, my question is: Using only real analysis, is it possible to calculate this integral? If the answer is yes, how? What makes $x-\arctan (x/2)$ show up instead of $x/2$ (and the same for only $\arctan (x/2)$)?","['indefinite-integrals', 'integration', 'trigonometry', 'calculus']"
2816396,Possible mistake in proof of limit?,"Please if someone with enough reputation could show my image it would be great. In this limit question, at the point where it states: Take the limit as $n \rightarrow \infty$ I really don't follow the logic at all. Since $a \in (0,1)$ then as $n \rightarrow \infty$, $|f(x)-f(a^nx)| \rightarrow |f(x)-f(0)|$, did they confuse $f(0)$ with $0$? Because otherwise how do you go from $$|f(x)-f(0)| \leq \frac{\epsilon}{1-a} |x|$$ to $$|f(x)| \leq \frac{\epsilon}{1-a} |x|$$ It is also very strange that the first property was not used.","['real-analysis', 'inequality', 'limits', 'epsilon-delta', 'proof-explanation']"
2816407,Derivative of exponential function wrt a vector,"Let $\mathbf{A}$ be a $k\times k$ invertible matrix, let $\mathbf{x}$ be a $k\times 1$ vector and let $\mathbf{1}$ be a $k\times 1$ vector of ones. For a generic $k\times 1$ vector $\mathbf{z}$, let the function $\exp\left(\cdot\right)$  be defined as follows: $\exp\left(\mathbf{z}\right)=\exp\left(\left[\begin{array}{c}
z_{1}\\
z_{2}\\
\vdots\\
z_{k}
\end{array}\right]\right)=\left[\begin{array}{c}
e^{z_{1}}\\
e^{z_{2}}\\
\vdots\\
e^{z_{k}}
\end{array}\right]$ Is the following equality true? If so, under what conditions? $\frac{d}{d\mathbf{x}}\;\mathbf{1}'\mathbf{A}^{-1}\exp\left(\mathbf{Ax}\right)=\exp\left(\mathbf{Ax}\right)$ More in general, I am looking for a scalar function whose derivative with respect to vector $\mathbf{x}$ is $\exp\left(\mathbf{Ax}\right)$ (or its transpose). Thanks a lot in advance!","['derivatives', 'matrix-equations', 'matrix-calculus', 'exponential-function', 'vectors']"
2816467,Probabilistic/combinatoric proof of $\sum_{k=0}^{n}\binom{tk+r}{k}\binom{t(n-k)+s}{n-k}\frac{r}{tk+r}=\binom{tn+r+s}{n}$,"In this posting , OP asks a proof of the following identity $$ \sum_{k=0}^{n} \binom{tk+r}{k}\binom{t(n-k)+s}{n-k} \frac{r}{tk+r} = \binom{tn+r+s}{n} \tag{1} $$ for non-negative integers $t, n, r, s$ with $t, n \geq 1$. I would like to reformulate this question in terms of probability. Here is my try: Setting. Let $(\Omega, 2^{\Omega}, \mathbb{P})$ be the probability space where $\Omega = \{ \omega \subseteq [tn+r+s] : |\omega| = n\}$ is the family of all subsets of $[tn+r+s]$ with size $n$, and $\mathbb{P}$ is the law of uniform distribution on $\Omega$, i.e., $\mathbb{P}(\{\omega\}) = \frac{1}{|\Omega|}$ for each $\omega \in \Omega$. Then define random variables $S_k$ and $T$ on $\Omega$ by $S_k(\omega) = \left|\omega\cap[tk+r]\right|$, for $k = 0, \cdots, n$, $T(\omega) := \min\{ k \geq 0 : S_k(\omega) = k \}$. $\hspace{9em}$ Since $k \mapsto S_k(\omega)$ is a non-decreasing function from $\{0,\cdots,n\}$ to itself, this map has a fixed point and the above definition makes sense. Then I am interested the following claim: Claim. We have $$ \mathbb{P}(T = k)
= \frac{r}{tk+r} \frac{\binom{tk+r}{k}\binom{t(n-k)+s}{n-k}}{\binom{tn+r+s}{n}}. \tag{2} $$ Given this claim, the identity $\text{(1)}$ is simply $1 = \mathbb{P}(T < \infty) = \sum_{k=0}^{n} \mathbb{P}(T = k) $. I checked this claim for various values of $t, n, r, s$ but was unable to establish a proof. So, here is a question: Although there is a proof using complex analysis in the original posting , I would be happy to know whether a probabilistic or combinatorial proof of $\text{(1)}$ or $\text{(2)}$ is available, not necessarily based on the setting above. It might be helpful to note that $\text{(2)}$ is equivalent to proving the following problem: For each $k$, the number of subsets $A$ of $[tk+r]$ satisfying $|A| = k$ and $|A\cap[tj+r]| > j$ for all $j<k$ is given by $\frac{r}{tk+r}\binom{tk+r}{k}$. This sounds like a generalization of Catalan numbers, although I have no good idea for this.","['combinatorics', 'probability', 'alternative-proof']"
2816468,What tiresome procedure can I follow to find the exact value of inverse trigonometric functions?,"Because of convenient trigonometric identities, we can find the exact value of things like $\tan70^{\circ}$ because $70^{\circ}=2\cdot35^{\circ}$ (tangent double-angle identity), $35^{\circ}=36^{\circ}-1^{\circ}$ (tangent angle difference identity), $3^{\circ}=3\cdot1^{\circ}$ (tangent triple-angle identity) and $3^{\circ}=18^{\circ}-15^{\circ}$ (tangent angle difference identity), $18^{\circ}=\frac{1}{2}\cdot36^{\circ}$ (tangent half-angle identity), and $15^{\circ}=\frac{1}{2}\cdot30^{\circ}$, leaving $\tan70^{\circ}$ in terms of the more well-known $\tan30^{\circ}$ and $\tan36^{\circ}$. I am fairly sure that we can find exact values by following similar procedures for all $\tan\theta$ ($\theta$ in degrees) as long as $\theta\in\mathbb{Q}$. So, can we do the same for finding the exact values of inverse trigonometric functions? Specifically, I am looking for a method of finding the exact value of $\tan^{-1}\frac{1}{2}$ in terms of things like integers, radicals, and complex numbers (which are required in the case of $\tan70^{\circ}$, for example), but not other trigonometric functions or infinite series. I am not asking for the exact value itself as I am sure no one would like to burden themselves finding it, but rather just the method so that I may.",['trigonometry']
2816488,Proving $\sum\limits_{k=0}^{\infty}\binom {m-r+s}k\binom {n+r-s}{n-k}\binom {r+k}{m+n}=\binom rm\binom sn$,"Question: How do you show the following equality holds using binomials$$\sum\limits_{k=0}^{\infty}\binom {m-r+s}k\binom {r+k}{m+n}\binom {n+r-s}{n-k}=\binom rm\binom sn$$ I would like to prove the identity using some sort of binomial identity. The right-hand side is the coefficient of $x^m$ and $y^n$ in$$\begin{align*}a_{m,n} & =\left[x^m\right]\left[y^n\right](1+x)^r(1+y)^s\\ & =\binom rm\binom sn\end{align*}$$ However, I don’t see how the left-hand side can be proven using the binomials. Using the generalized binomial theorem, we get the right-hand side as $$\begin{align*}(1+x)^r(1+y)^s & =\sum\limits_{k\geq0}\sum\limits_{l\geq0}\binom rk\binom slx^ky^l\end{align*}$$However, what do I do from here?","['binomial-theorem', 'binomial-coefficients', 'sequences-and-series']"
2816596,Integral $I_m=\int_0^1 \frac{x^m-x}{\sin \pi x} \:dx $ expressed as finite sum of $\frac{r_n \zeta(2n+1)}{\pi^{2n+1}}$,"The integral
\begin{equation}
I_m=\int_0^1 \frac{x^m-x}{\sin \pi x} \:dx
\end{equation}
is expressed as a finite sum of terms of the shape $r_n \zeta(2n+1) \pi^{-(2n+1)}$, where $r_n$ is a rational number that depends on $n$. In this question Evaluating $\int_0^1 \frac{x^2-x}{\sin \pi x} dx = - \frac{7 \zeta(3)}{\pi^3}.$ The user ""user90369"" provided a answer for $I_2$. So I followed his steps and generalized the integral. Define:
\begin{align}
&f(a) = \int_0^1 x e^{ax} dx= \frac{1+e^a(a-1)}{a^2} \\
&g(a) = \int_0^1 x^m e^{ax} dx = \frac{e^a}{a^{m+1}} \left(  (-1)^mm!  + \sum_{n=1}^{m}  (-1)^{m+n} \frac {a^n}{n!} \right) - \frac{(-1)^mm!}{a^{m+1}}	
\end{align} Rewrite $I_m$ as: \begin{align}
I_m &= \int_0^1 \frac{x^m-x}{\sin \pi x} = 2i \int_0^1 \frac{x^m-x}{ e^{i\pi x } - e^{-i \pi x} }\nonumber\\
&= 2i \sum_{k=0}^{\infty} \int_0^1 (x^m-x)e^{-i\pi x(2k+1) } dx \\
&= 2i \sum_{k=0}^{\infty} g(-i\pi(2k+1)) - f(-i\pi (2k+1)) \nonumber
\end{align} From that I arrived at the following formulas for $I_m$. When $m$ is even and $\geq 2$: \begin{align*}
I_m =  \frac{4m!i^m}{\pi^{m+1}} \left( 1-\frac{1}{2^{m+1}}\right)  \zeta(m+1) + 2 m! \sum_{n\:odd \geq 0}^{m-2} \frac{i\: \zeta (m-n)}{(n+1)!(i\pi)^{m-n}}   \left(  1-\frac{1}{2^{m-n}} \right)
\end{align*} When $m$ is odd and $\geq 3$: \begin{align*}
I_m = 2 m! \sum_{n\:even \geq 0}^{m-2} \frac{i\: \zeta (m-n)}{(n+1)!(i\pi)^{m-n}}   \left(  1-\frac{1}{2^{m-n}} \right).
\end{align*} A few values for $I_m$: \begin{align*}
& I_2 = - \frac{7\zeta(3)}{\pi^3} \\
& I_3 = -\frac{21 \zeta(3)}{2\pi^3}\\
& I_4 =  \frac{93\zeta(5)}{\pi^5}-\frac{21\zeta(3)}{\pi^3}	\\
& I_5 =  \frac{465\zeta(5)}{2\pi^5} - \frac{35\zeta(3)}{\pi^3}	\\
& I_6 = -\frac{1}{2} \left(  \frac{5715\zeta(7)}{\pi^7} - \frac{1395\zeta(5)}{\pi^5} + \frac{105\zeta(3)}{\pi^3}  \right) \\
& I_7 = -\frac{1}{2} \left(  \frac{40005\zeta(7)}{2\pi^7} - \frac{3255\zeta(5)}{\pi^5} + \frac{147\zeta(3)}{\pi^3} \right) \\
&I_8 = \frac{160965\zeta(9)}{\pi^9} - \frac{40005\zeta(7)}{\pi^7} + \frac{3255\zeta(5)}{\pi^5}-\frac{98\zeta(3)}{\pi^3}
\end{align*} It's also possible to integrate by parts $I_m$. Anyway, I found this integral interesting and wanted to share. Perphaps someone can do something cool with it, or not. Sorry for the weird/sloppy english.","['real-analysis', 'integration', 'riemann-zeta', 'definite-integrals', 'trigonometric-integrals']"
2816615,Is my argument correct?,"In the book of Analysis on Manifolds by Munkres, at page 71, it is asked that Let $A$ be open in $\mathbb{R}^n $; let $f : A \to \mathbb{R}^n $ be of
  class $C^r$; assume $Df(x)$ is non-singular for $x\in A$. Show that
  even if $f$ is not one-to-one on $A$, the set $B = f(A)$ is open in
  $\mathbb{R}^n $. I have argued that: By the inverse function theorem, for $x \in A$, $\exists U_x$ s.t $U_x \subseteq A$ and $U_x$ is open, and $f$ is one-to-one on $U_x$.Moreover, we have $\cup_{x\in A} U_x = B $. Since $B$ is the union of the open sets, $B$ is also open. Is there any problem in this argument ? Could someone provide me a feedback, please.","['real-analysis', 'analysis']"
2816629,Lipschitz constant of limit of functions part 2,"This question follows from my other question Lipschitz constant of limit of functions . Consider two metric spaces $(X,d_X)$ and $(Y,d_Y)$ and define the lipschitz constant of every continuous function $f:X\rightarrow Y$ as $$Lip(f):=\sup\limits_{x\neq y}\frac{d_Y(f(x),f(y))}{d_X(x,y)}$$ Consider a sequence of continuous functions $f_n:X\rightarrow Y$ such that there is a $k>1$ such that for every $n\in \mathbb{N}$ it is $Lip(f_n)\le k$ $\{f_n\}$ has limit $f:X\rightarrow Y$ for the uniform convergence on 
compact sets (this means that for every $K\subset X$ compact it results $\lim\limits_{n\rightarrow \infty}\sup\limits_{x\in K}d_Y(f(x),f_n(x))=0$) $Lip(f_n)\rightarrow 1$ User pcp showed that it is not true $Lip(f)=1$, but showed an example when it happens $Lip(f)=0$. It seems to me that his counter-example only works for proving $Lip(f)<1$, so my other question is the following. Question: Can it happen $Lip(f)>1$? Can you motivate your answer?","['functional-analysis', 'real-analysis', 'lipschitz-functions', 'analysis']"
2816644,A square modulo every prime is a square. Proof valid?,"As Eric Schneider asked, ""Am I mistaken, or does the following (actually) elementary proof work?"" Theorem. Any integer which is a square modulo every prime is a square. Lemma. For any odd prime $p$ , any integer which is a square modulo $p$ is a square modulo every power of $p$ . Proof. Let $a$ be any square modulo $p$ . Let $r$ be any integer $\geqslant 1$ . Use induction on $r$ . (I adopt this approach to avoid a bug, spotted by Ingix, in my earlier proof.) The result is true for $r=1$ . Suppose, by the inductive hypothesis, that $a$ is a square modulo $p^{r-1}$ . Then $a=x^2\mod p^{r-1}$ for some $x$ . Work modulo $p^r$ . If $x=0$ then $a=0=0^2$ modulo $p^r$ . Otherwise, for $0\leqslant k<p$ , $(kp^{r-1}+x)^2=jp^{r-1}+a\mod p^r$ for some $0\leqslant j<p$ . Suppose two distinct $kp^{r-1}+x$ had the same square. Then \begin{align*}
(kp^{r-1}+x)^2&=(lp^{r-1}+x)^2\mod p^r\tag{ with $k\ne l$}\\
(k^2-l^2)p^{2r-2}+2(k-l)p^{r-1}x&=0\mod p^r\\
2(k-l)p^{r-1}x&=0\mod p^r\tag{ as $r>1$}\\
2(k-l)x&=0\mod p
\end{align*} $p$ is an odd prime and $x\ne 0\mod p$ , so $p\nmid 2x$ , so the last line is false. Therefore, by the pigeonhole principle, the values of the $p$ distinct expressions $(kp^{r-1}+x)^2$ for $0\leqslant k<p$ are the values $jp^{r-1}+a$ for $0\leqslant j<p$ in some order. In particular, in one case $j=0$ , so $a$ is a square modulo $p^r$ . Proof of theorem. Let $a$ be a square modulo every prime. Then, by Lemma 1, $a$ is a square modulo every odd prime-power. Then, by the Chinese remainder theorem, $a$ is a square modulo every odd integer. Then, by Eric Schneider's argument , with $n=2$ , but applying it only to $p$ being an odd prime and not to $p=2$ , for every odd prime $p$ , $p^r\; ||\;a$ for an even number $r$ . Thus either $a=x^2$ or $a=2x^2$ for some integer $x$ . Let $p$ be a prime where $p=\pm 3\mod 8$ . Then, modulo $p$ , $a$ is a quadratic residue modulo $p$ by supposition, but 2 is not a quadratic residue, so $2a$ is not a quadratic residue, so $2a$ is not a square in $\mathbb{Z}$ . Thus for every integer $x$ , $(2x)^2=4x^2\ne 2a$ , so $2x^2\ne a$ , so $a$ is a square, as required. I see no elementary proof of this theorem, with no use of quadratic reciprocity (QR). I wonder if the above qualifies. Chan asked for a proof of this very result but that question was closed as a duplicate of a different question, viz the one where Eric Schneider's answer has been used above. There is Hagen von Eitzen's proof of a similar result, but that relies on QR. ArithmeticGeometer requested a proof without using QR, and the only answer is an advanced proof.","['number-theory', 'modular-arithmetic', 'quadratic-residues']"
2816657,How to understand the expression $\int_{t\in\mathcal{T}}Y(t)dI\{t\ge T\}$,"The random process $Y(t)$
indexed by $t\in \mathcal{T}$ denotes outcomes under treatment
levels in $\mathcal{T}$ . In practice, one cannot observe $Y(t)$ for
all $t\in \mathcal{T}$ . Rather, only a single $Y(t_0)$ can be observed, where
$t_0$ is the realization of a random variable $T$. Thus, the observed
outcome is the random variable
$$Y=Y(T)=\int_{t\in\mathcal{T}}Y(t)dI\{t\ge T\}$$ My question is how to understand the expression $\int_{t\in\mathcal{T}}Y(t)dI\{t\ge T\}$. Why is this equal to $Y(T)$.","['probability-theory', 'statistics', 'probability', 'measure-theory', 'random-variables']"
2816677,What is the necessary and sufficient condition of Markov chain sample average converging to the expectation wrt the stationary distribution?,"The ergodic theorem says that for an irreducible and positive-recurrent Markov chain $P$, any distribution $\lambda$, and $x_n (n\geq0) \sim Markov(\lambda, P)$ then it follows that for any bounded function $f:I\rightarrow R$
$$P(\frac{1}{n}\sum^{n-1}_{k=0}f(x_k)\rightarrow \bar{f} \text{as } n\rightarrow\infty)=1$$
where $\bar{f}=\sum_{i\in I}\pi_if_i $ and $\pi$ is the unique stationary distribution
(ref: ergodic theorem ). It seems being irreducible and positive-recurrent is a sufficient but not necessary condition here. As I can think of a reducible and transient Markov chain that has the same property (please correct if I'm wrong)
$$P=\left[\begin{matrix}1&0\\1&0\end{matrix}\right]$$
where the unique stationary distribution is $\pi=[1,0]$. So what is the necessary and sufficient condition for such property to hold? It seems the only case that this property will fail is when the limit of the average probability is not equal to the stationary distribution
$$\lim_{n\rightarrow\infty}\frac{1}{n}\sum^{n-1}_{k=0}p(x_k)\neq \pi.$$
The only case I can think of is when there're more than one closed classes, so is the necessary and sufficient condition having one closed and positive recurrent communicating class (basically go from irreducible to one closed class compared to the original sufficient condition)?","['stochastic-processes', 'markov-chains', 'probability']"
2816727,entire function that maps real line to itself is linear [duplicate],"This question already has an answer here : Holomorphic function satisfying $f^{-1}(\Bbb R)=\Bbb R$ is of the form $f(z)=az+b$ (1 answer) Closed 3 years ago . Let $f:\mathbb{C}\to\mathbb{C}$ be an entire function be such that $\mathbb{R}=f^{-1}(\mathbb{R})$. Show that $f$ is linear. i.e. $$\exists\ a,b\in\mathbb{R}:f(z)=az+b$$ Hint I think that $f$ must map the upper half plane and lower half plane to certain two disconnected open sets. Maybe, if we can show that $f$ is a mobius transformation, then we can then we can look at $$g(z)=\frac{f(z)-f(0)}{z}$$ and finish.","['complex-analysis', 'entire-functions']"
2816794,Fundamental Theorem of Poker,"I've been doing an investigation into the mathematics behind poker, and I have stumbled upon this theorem called 'The Fundamental Theorem of Poker', which is as follows: ""Every time you play a hand differently from the way you would have
  played it if you could see all your opponents' cards, they gain; and
  every time you play your hand the same way you would have played it if
  you could see all their cards, they lose."" There has been many different articles on this rule, and most state that there is 'a strong mathematical background' and 'a practical application' of the Law of Iterated Expectations/Law of Total Expectation: E(X) = E(E(X|Y)) However, I am yet to encounter an explanation HOW those two things are related, and I'm not sure why the two are related in the first place. To my understanding one is more or less common sense and the other is about expectations. Can someone please explain to me the Law of Iterated Expectations implicates or at least is related to the Fundamental Theorem of Poker? P.S Upon further search, I've also found another theorem called 'Morton's Theorem'. It states that: In multi-way pots, a player’s expectation may be maximized by an opponent making a correct decision."" It's a direct contrast to the fundamental theorem in the way that it's stating players win when the opponents make a correct decision. I'm not quite sure why the two theorems exist when they seem like they're literal polar opposites of each other. I understand that Morton's theorem is for several players while the Fundamental Theorem is only for two players, but I'm unclear on why more players would suddenly reverse what is described by the Fundamental Theorem. If you can, can you please explain why such is the case?","['expectation', 'conditional-expectation', 'probability', 'poker']"
2816795,"General solution of $\frac{dy}{dx}+y=1$, $(y\neq 1)$","Solve $\frac{dy}{dx}+y=1$, $(y\neq 1)$ The general solution for this differential equation is given in my reference as $y=1+Ae^{-x}$, but is it a complete solution ? My Attempt $$
\frac{dy}{dx}=1-y\implies \frac{dy}{1-y}=dx\\
\int\frac{dy}{1-y}=\int dx\implies\int\frac{dy}{y-1}=-\int dx\\
\log|y-1|=-x+C_1\implies |y-1|=e^{-x}.e^{C_1}\\
|y-1|=A.e^{-x}\\
y=\begin{cases}1+A.e^{-x};\quad y\geq1\\
1-A.e^{-x};\quad y<1
\end{cases}
$$
right ?","['exponential-function', 'integration', 'ordinary-differential-equations']"
2816799,Proof that Integral over the arc vanishes as $R\rightarrow\infty$ in Inverse Mellin Transform of $\Gamma(s)$,"It´s a very well know result that $$e^{-x}=\frac{1}{2\pi i}\int_{\gamma-i\infty}^{\gamma+i\infty} \Gamma(s)x^{-s}ds$$ In order to solve this integral we have to close the contour to the left and show that the integral over this path vanishes as $R\rightarrow\infty$ . Unfortunately it´s not obvious to me how to proof it.
 All textbooks that I came across assume that the integral goes to zero without any proof. What I have tried so far is the following:
I assume that we are closing this contour with a semi circle to the left, and I get the following integral: 
Let $s=Re^{i\theta}$ and $ds=iRe^{i\theta}d\theta$ , $$\int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} \Gamma(Re^{i\theta})x^{-Re^{i\theta}}iRe^{i\theta}d\theta$$ Using Stirling´s approximation for the Gamma function we have $$\Gamma(Re^{i\theta})\sim\sqrt{2\pi} e^{-Re^{i\theta}}(Re^{i\theta})^{Re^{i\theta}-\frac{1}{2}}$$ and $$\int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} \Gamma(Re^{i\theta})x^{-Re^{i\theta}}iRe^{i\theta}d\theta\sim\ \sqrt{2\pi} iR \int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} e^{-Re^{i\theta}}(Re^{i\theta})^{Re^{i\theta}-\frac{1}{2}}x^{-Re^{i\theta}}e^{i\theta}d\theta$$ I took the modulus of the last integral and arrived in the following expression: $$\sim \sqrt{2 \pi} \int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} e^{Rcos(\theta)(log(R)-log(x)-1)}e^{\frac{1}{2}log(R)}e^{-R \theta sin(\theta)}d\theta$$ But I don´t know how to evaluate the $\lim$ as $R \to \infty$ I really appreciate if someone could show this for me.
Thank you in advance.","['limits', 'mellin-transform', 'integration', 'contour-integration', 'gamma-function']"
2816829,Is the image of an equivariant map always a weakly embedded submanifold?,"Let $M,N$ be smooth manifolds, with a smooth $G$-action on them, by some Lie group $G$. Suppose also that $M$ has a finite number of orbits under $G$'s-action. Let $f:M \to N$ be a smooth, equivariant, injective immersion . Is $f(M)$ a weakly embedded submanifold of $N$? Weakly embedded here means that for every manifold $Q$ and for every smooth map $h:Q \to N$, with $h(Q)\subset f(M)$,the associated map $h:Q\to f(M)$ is also smooth. In other words, it's always valid to restrict the range. It is known that it suffices to prove that $h:Q\to f(M)$ is continuous. Note that in general $f(M)$ is only an immersed submanifold. In particular, it can have more open sets than those that come from the subspace topology. Weakly embedded is a notion which is between ""immersed"" and ""embedded"". It is also known that every Lie subgroup is weakly embedded. A famous example for a weakly embedded submanifold, which is not embedded is the dense curve on the torus . (In that case, there is also a Lie group action in the background, by $\mathbb{R}$).","['equivariant-maps', 'differential-topology', 'smooth-manifolds', 'submanifold', 'differential-geometry']"
2816831,Point of small maximum distance to $n$ arbitrary points in $m$-dimensional unit cube,"Informally speaking , if $n$ people living at the vertices of the $m$-dimensional unit cube $C_m$ want to have a party at the most convenient vertex of $C_m$ (possibly at one of the $n$ vertices themselves), what is the worst case maximum distance in terms of $m$ and $n$ that any one of them may have to travel in the ""Manhattan"" or $\ell_1$ metric (that is: along the edges of the cube). Note that this is referring to the best choice of party venue subject to the minimisation of the longest journey, with the worst-case distribution of people in that regard Formally speaking , I'm defining $D_{m,n} \in \mathbb{N}$ to be minimal such that the following is true:
$$\forall x_1, x_2, ..., x_n \in C_m ~ \exists ~ c\in C_m ~\forall~ 1\leq i \leq n: d_1(x_i,c) \leq D_{m,n}$$
where $(C_m,d_1)$ is the metric subspace $\lbrace0,1\rbrace^m$ of $(\mathbb{R}^m,||\cdot||_1)$. My limited progress so far : By the triangle inequality, I can see that $D_{m,2}=\lceil\frac{m}{2}\rceil$. I can see it can't be any smaller in the case $x_1=(0,0,...,0), x_2=(1,1,...,1)$. The simple example for $m=2$ and $n=4$ where the $x_i$ are all the different vertices of the square shows that $D_{2,4}=2 > D_{2,2}=1$. So $D_{m,n}$ definitely somehow increases with $n$ if $m$ is held constant. I just don't quite see how. For example, I haven't yet managed to construct a distribution showing that $D_{12,4}>6=D_{12,2}$. I'd be grateful for any suggestions of a way to think about this systematically without exhaustive simulation!","['graph-theory', 'discrete-mathematics']"
2816858,"Durrett's Probability problem 3.1.1, show $\prod_{j=1}^n (1+c_{j,n}) \to e^\lambda$","Given: $$\max_{1\leq j \leq n} |c_{j,n}| \to 0$$
$$\sum_{j=1}^n  c_{j,n} \to \lambda$$
$$\sup_n \sum_{j=1}^n |c_{j,n}| < \infty,$$
show that:
$$ \prod_{j=1}^n (1 + c_{j,n}) \to e^\lambda.$$ My partial solution: Taking the log of both sides, the conclusion is equivalent to
$$ \sum_{j=1}^n \ln (1 + c_{j,n}) \to \lambda.$$
Indeed,
$$ \sum_{j=1}^n \ln (1 + c_{j,n}) =  \sum_{j=1}^n  \frac{\ln (1 + c_{j,n})}{c_{j,n}} c_{j,n}.$$
Let
$$ \overline{c}_{j,n} := \max_{1\leq j \leq n} c_{j,n},$$
$$ \underline{c}_{j,n} := \min_{1\leq j \leq n} c_{j,n}.$$
Now define
$$k_n := \frac{\ln (1 + \underline{c}_{j,n})}{\underline{c}_{j,n}},$$ 
and 
$$l_n := \frac{\ln (1 + \overline{c}_{j,n})}{\overline{c}_{j,n}}.$$ 
Clearly, for all $n$ and all $1 \leq j \leq n$,
$$ l_n \leq \frac{\ln (1 + c_{j,n})}{c_{j,n}} \leq k_n.$$
Using the first assumption, $l_n,k_n \to 1$.
Assuming that $\lambda>0$, we write
$$ l_n \sum_{j=1}^n c_{j,n} \leq \sum_{j=1}^n  \frac{\ln (1 + c_{j,n})}{c_{j,n}} c_{j,n} \leq k_n \sum_{j=1}^n c_{j,n},$$
and finally, using the squeeze theorem, and the second assumption, our result follows. (If $\lambda<0$, the inequalities switch sides.) My question: I haven't used the last assumtion, so I'm guessing something is missing in my solution.","['probability', 'sequences-and-series']"
2816866,A combinatorial identity: $\sum_{k=0}^j (-1)^k \frac{n-2k}{n-j-k} \binom{n}{k} \binom{n-j-k}{j-k} = 0$,"I would like to prove the following identity: For $0< j\leq \left\lfloor{n \over 2}\right\rfloor$ , $$\sum_{k=0}^j (-1)^k \frac{n-2k}{n-j-k} \binom{n}{k} \binom{n-j-k}{j-k} = 0.$$ I have checked for several small $j$ , but no clue on how to prove it for all $0<j\leq \lfloor \frac{n}{2} \rfloor$ . I have thought about whether one can have a combinatorial proof, like using inclusion-exclusion, but it seems not possible becuase some terms would not be integers. Would be glad if there is any proof or hint. Thanks.","['combinatorics', 'binomial-coefficients']"
2816902,Japanese Temple Geometry,"Hello, I was trying to solve this problem using descarte circle theorem for my maths report. I looked through the solution but I don't understand the part in the answer, where it says the two solutions are $p_{n+1}, p_{n-1}.$ Can someone explain it for me. Thanks!","['circles', 'sangaku', 'euclidean-geometry', 'geometry', 'quadratics']"
2816919,Local ring of a surface: polynomial expression,"Let $(X,\mathscr O_X)$ be a smooth surface over a field $k$ (two dimensional scheme, regular, noetherian....) and fix a point $x\in X$. Then if we complete the local ring $\mathscr O_{X,x}$ at its maximal ideal we get the isomorphism:
$$\widehat{\mathscr O_{X,x}}\cong k(x)[[t,u]]$$
So my question is the following: We have the inclusion $\mathscr O_{X,x}\subset \widehat{\mathscr O_{X,x}}$; how can we express elements $\mathscr O_{X,x}$ as power series? Are they just those elements in $k(x)[[t,u]]$ with finite power series expansion? What is the relationship between  $\mathscr O_{X,x}$ and $k(x)[t,u]$?","['algebraic-geometry', 'abstract-algebra', 'schemes', 'ring-theory', 'local-rings']"
2816981,"If the composition of a (strictly positive with compact domain) function with the logarithm is Lipschitz, then the function is Lipschitz.","I'm reading a proof in which the authors want to show that a certain function $f:[0,T]\to\Bbb{R}_+\cup \{0\}$ is Lipschitz continuous. They prove that $$e^{−C|t−s|}f(t)≤f(s)≤e^{C|t−s|}f(t)$$
for all $t,s\in [0,T]$ and certain $C>0$ and then say that ""the result follows"". Why? I don't even know that $f$ is continuous , a priori . I've tried the following: Suppose $f$ is not Lipschitz. Then there are sequences $t_n,s_n\in [0,T]$ (which I can suppose $t_n\to t\in [0,T]$ and $s_n\to s\in [0,T]$, by compactness) such that the definition of Lipschitz fails for constants, say, $e^n$, $n\in \Bbb N$, i.e. 
$$|f(t_n)-f(s_n)|>e^n|t_n-s_n|.$$
If $t\neq s$, then the inequality above says that $|f(t_n)-f(s_n)|\to +\infty$ (the images of $t_n,s_n$ by $f$ get arbitrarily far). But then... what? And what if $t=s$? Then I can't say if $e^n|t_n-s_n|\to 0$ or $+\infty$. Furthermore, I have the difficult of working with $f$ without knowing anything about its continuity... Remark.: If I knew that $f>0$ ( strictly positive), then applying the logarithm the proven inequality would be equivalent to
$$|\ln (f(t))-\ln (f(s))|\leq C|t-s|.$$
This is not exactly the case (since $f$ can be zero) but I've used this as motivation to the title of this post. Edit: I've just realized that if $f(t)=0$ for some $t\in [0,T]$, then the proven inequality says that $0\leq f(s)\leq 0$, for any $s\in [0,T]$ and therefore $f\equiv 0$. So we can indeed suppose that $f>0$ as stated in the title.","['functions', 'lipschitz-functions', 'calculus', 'analysis']"
2817101,Boundary of the set of critical points,"Let $u\in C^\infty(\mathbb{R}^d)$ and consider
$$ E = \partial\{x:Du(x)=0\}. $$
I am interested in understanding whether $\mu(E)=0$ or not, $\mu$ being the $d$-dimensional Lebesgue measure. What I could prove so far is that there exists a point $x\in E$ such that, for all neighborhoods $U\in U(x)$ we have
$$ \mu(E\cap \overline{U})>0. $$
Any idea on how to proceed with proving that $\mu(E)>0$, assumed that it is possible?","['general-topology', 'real-analysis', 'differential-geometry']"
2817111,What does it mean that a probability distribution has full support?,"I am reading an article which introduces the following assumption at a certain point: The probability distribution of the (real- valued) random variable $X$ has full support and is absolutely continuous with respect to the Lebesgue measure. I am confused on what is commonly intended by ""full support"". Does the author want to say that the support is $\mathbb{R}$? (which, together with absolute continuity, would then imply that the cumulative distribution function is strictly positive - and, hence, strictly monotone increasing - on $\mathbb{R}$)","['probability', 'measure-theory', 'random-variables', 'probability-distributions']"
2817122,"Laplace transform of a ""heat kernel""","This question is closely releted to this question: How do we solve the laplace transform of the Heat Kernel? Let $A>0$ and
$$f(t) = \frac{A^2}{2\sqrt{\pi}t^\frac{3}{2}}e^{-\frac{A^2}{4t}}$$
Following the same computations as in the question I linked, one can prove that for $s \in \mathbb{R}$, $s\geq 0$ one has 
$$\mathcal{L[f](s) :=\int_0^{+\infty}e^{-st}f(t)\,dt}= e^{-A\sqrt{s}}$$
(see also https://projecteuclid.org/download/pdf_1/euclid.aoms/1177731708 at page 252 ) Is this formula true also for $s \in \mathbb{C}, \mathrm{Re(s)} \geq 0$, thinking the square root of $s$ as the principal branch of the root (i.e. the square root of $s$ with positive real part)?","['real-analysis', 'laplace-transform', 'integral-transforms', 'calculus', 'complex-analysis']"
2817129,Lecture to solve 2nd order differential equation in matrix form.,"I have an system of three differential equation coupled. I put this system to matrix form. I think it should be more easy to solve.
$$
\begin{bmatrix}
    \ddot x_1 \\ 
    \ddot x_2 \\
    \ddot x_3 \\
    \end{bmatrix}
=
    \begin{bmatrix}
    0 & a_{12} & -a_{31} \\ 
    -a_{12} & 0 & a_{23} \\
    a_{31} & -a_{23} & 0 \\
    \end{bmatrix}
\begin{bmatrix}
    \dot x_1 \\ 
    \dot x_2 \\
    \dot x_3 \\
    \end{bmatrix}+\begin{bmatrix}
    c_1 \\ 
    c_2 \\
    c_3 \\
    \end{bmatrix}
$$
All constants are positive or zero. Could someone tell me what I should read to get the knowledge to solve this differential equation ? Thanks for your help.
Tof Edit: $a_{23} -> -a_{23} $","['matrix-equations', 'ordinary-differential-equations']"
2817168,Can a non-invertible matrix be extended to an invertible one?,"For a square matrix $M$ call any square matrix M' of the form 
$$\left(\begin{array}{cc}
M & A\\
B & C
\end{array}\right)$$
an extension of $M$. Does it follow that if $M$ is not invertible that all extensions $M'$ are not invertible? I believe the answer is no. If not, is there an extension that is invertible? Can we prove that there always is?","['matrices', 'linear-algebra', 'linear-transformations']"
2817176,Is there an uncountable subspace $F \subseteq \omega^{\omega_1}$ which is closed and discrete?,"Does there exist an uncountable subspace $F \subseteq \omega^{\omega_1}$ which is closed and discrete? Edit : I made a couple attempts to solve it, but they didn't get anywhere (also I don't believe they're interesting, but just in case...). The classic argument that the square of the Sorgenfrey line, $X^2$, is not T4 uses the fact that it is separable and has a discrete closed subspace of size $2^{\aleph_0}$. Namely, if it were T4, then by the Tietze extension theorem there would be at least $2^{2^{\aleph_0}}$ continuous functions $X^2 \to \mathbb{R}$. On the other hand, from separability the number of such functions is at most $2^{\aleph_0}$, which is a contradiction, since $2^{\aleph_0} < 2^{2^{\aleph_0}}$. But the analogous argument doesn't seem to work here, firstly because I don't know if $\omega^{\omega_1}$ is normal* (I do know it is separable), secondly because we only get that $2^{\aleph_1} \leqslant |C(\omega^{\omega_1})| \leqslant 2^{\aleph_0}$ and that is consistent with ZFC. I don't know whether $\omega^{\omega_1}$ is a Lindelöf space*. If it is, it follows that the answer is negative: suppose $F \subseteq \omega^{\omega_1}$ is a closed discrete subspace. Then as a closed subspace of a Lindelöf space, it is also Lindelöf, but since it's discrete, it must be countable. Also since $\displaystyle \omega = \bigcup_{n < \omega} n$ and $n^{\omega_1}$ is compact, I feel like $\omega^{\omega_1}$ should be close to being compact in a sense I am not able to formalize, but if it is something similar to being Lindelöf, maybe it would be of use here. The question was conceived by myself, so I have no idea how difficult it is or whether the answer is decidable in ZFC. *I would also be interested in answers to these two questions (if $\omega^{\omega_1}$ is normal and if it is Lindelöf), but I don't know if I should ask them all at once.","['general-topology', 'infinite-product']"
2817241,Controlling this function in $L^2$ norm,"We have $(X_i)_{i \in \mathbb Z}$ iid random variables with $1\le X_i \le2$ almost surely. We define $X(x,\omega) \equiv X_i (\omega)$ if $x\in [i,i+1[$ (so it is bounded almost surely) and, for $\epsilon >0$, $X_\epsilon (x, \omega) \equiv X(x/\epsilon, \omega)$. Define for $x\in [0,1]$ $$err_\epsilon(x,\omega)=\frac {\int_0^1 \frac {F(y)}{X_\epsilon(y,\omega)} dy}{X_\epsilon(x,\omega)\int_0^1 \frac {1}{X_\epsilon(y,\omega)} dy}$$ where $F$ is an $L^1(]0,1[)$ function (we can add continuity if it helps). We would like to find functions $g$ and $f$ such that $$\Vert err_\epsilon - g\Vert_{L^2(]0,1[ \times \Omega)} \le f(\epsilon) \to 0$$ when $\epsilon \to 0^+$ Related question: How to use the Lindeberg CLT in this scenario ? (analysis problem)","['real-analysis', 'probability-theory', 'functional-analysis', 'probability', 'measure-theory']"
2817270,Finding $\sum_{r=1}^{n}{\cot{}^{ - 1}(3r^2-\frac{5}{12})}$,"Find the sum of
  $$\sum_{r=1}^{n}{\cot{}^{ - 1}(3r^2-\frac{5}{12})}$$ I tried to convert all into 
$\tan {}^{ - 1}(x) $
using 
$\cot {}^{ - 1} (x) = { \tan {}^{ - 1} ( \frac{1}{x} ) }$ 
and then tried to simplify them using property 
$ \tan {}^{ - 1} ( \frac{x - y}{1 - xy} )  = \tan {}^{ - 1} (x) - \tan {}^{- 1} (y)  $
so that some terms cancel each other. But I do not manage to simplify the problem. What to do?","['trigonometry', 'inverse-function']"
2817284,What is the area of the shaded region in terms of $n$?,"We can see that shaded region is area of FEH minus the sector HGE. 
To find the sector HGE i called the angle GHE as $\theta$ and used
$$
\pi\times\left(\frac{\theta}{360^{\circ}}\right)
$$
If FE is $x$, we see that $\cos(\theta)=\frac{1}{\sqrt{x^2+1}}$ therefore
$$
\theta=\arccos\left(\frac{1}{\sqrt{x^2+1}}\right)
$$
My attempt on this problem was by ""brute-forcing"" my way from the bottom of the figure to top, to find FE, using Law of Cosine on the way. The shaded region will now be 
$$
\frac{x}{2} - \pi\times\left(\frac{\arccos\left(\frac{1}{\sqrt{x^2+1}}\right)}{360^{\circ}}\right)
$$ To find $x$ i approached in two different ways: Connect CD to AI by lengthening CD and connect HD. Do Law of Cosine on these triangles to get to FE. Connect AD and HD, do law of cosines on these to get to FE. Both of these resulted in immense work and, on the first case i was able to write $x$ in terms of $n$ but the equation barely fit a word page with font size 1. (and because of the immense work, it is likely to be wrong).","['trigonometry', 'geometry']"
2817290,"Matroid Theory with Graph Theory, Need an Introduction Book","I am looking for a undergraduate introduction to matroid theory. We got them introduced today, to prove the Kruskal algorithm.... I can't say it was more elegant then the direct proof of the algorithm. I also do not see any benefits. Could maybe some explain why matroids are ""great"", my prof was very very fond of matroids. I am also looking for an introduction book, maybe with exercises and solutions. (Russian, English, and German are okay.)","['graph-theory', 'matroids', 'discrete-mathematics']"
2817297,variation of random variable greater or equal than sum sum variations of two independent conditional expectations,"I found the following problem, which seems very simple, but I'm stuck concerning the ideas I can use. The statement is as follows: Let $X$ be a random variable with finite expectation and let $\mathcal{G}_1$ and $\mathcal{G}_2$ be two $\sigma$-algebras which are independent of each other. Let $X_1=E[X|\mathcal{G}_1]$ and $X_2=E[X|\mathcal{G}_2]$. Show that $Var(X) \geq Var(X_1) + Var(X_2)$. Use an example to show that independence of $\mathcal{G}_1$ and $\mathcal{G}_2$ is crucial. It's obvious that if $\mathcal{G}_1=\mathcal{G}_2$ and $X$ is $\mathcal{G}_1$-measurable the inequality does not hold. If I use the law of total variation I can find one of the terms on the r.h.s., but I'm clueless regarding how to get from one sigma-algebra to the other and use their independence. I also thought about somehow using Jensen to get an inequality or conditioning w.r.t. $\sigma(\mathcal{G}_1, \mathcal{G}_2)$, but arrived nowhere. Any clues? Help is greatly appreciated.","['conditional-expectation', 'probability', 'variance']"
2817313,Proof for $\sum_{k=2}^\infty \frac{1}{k^4-1}= \frac{7}{8}-\frac{\pi}{4}\coth(\pi)$,"How can this identity be derived?
I have been searching the internet but I have no clue where to find a proof for this identity. Any help is highly appreciated.
$$\sum_{k=2}^\infty  \frac{1}{k^4-1}= \frac{7}{8}-\frac{\pi}{4}\coth(\pi)$$","['sequences-and-series', 'limits']"
2817340,What happens to roots of a function when deriving?,"While studying, I stumbled upon a statement that all the roots of $q(x) = x^{(p^n)} - x$, where p is a prime number are different, because otherwise $q$ would share some factors with its derivative. This lead me to  wonder, what happens to the roots when we derive, as my basic understanding would say that the multiplicity of all of the roots drops by one, but I cannot find some proof for this or counter-example. Can anyone please describe what happens (preferably also with a link to the proof).",['algebra-precalculus']
2817343,Every convex polyhedron is a spectrahedron,"I'm trying to show that convex polyhedra are special cases of spectrahedra. This was left as an exercise to the reader in a convex optimization text that I'm reading. I'm not sure how standard the definitions and notation in this book are, so I'll include them below. Notation: Letting $A, B \in \mathbb{R}^{m \times m}$ be symmetric matrices, write $B \succeq A$ if $B - A \succeq 0$, i.e., $B-A$ is positive semidefinite. For vectors $a, b \in \mathbb{R}^n$, write $a \geq b$ if $ a_i \geq b_i \forall i \in \{1, 2, \dots, n\}$. Definition: Given symmetric matrices $A_1, A_2, \dots, A_n, B \in \mathbb{R}^{m \times m}$, the following convex set $$\left\{ x \in \mathbb{R}^n \,\,\middle|\,\, \sum_{i = 1}^n x_i A_i \preceq B \right\}$$ is called a spectrahedron . Definition: Given $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$, $\left\{ x \in \mathbb{R}^n \mid Ax \leq b \right\}$
is called a convex polyhedron . My attempt so far: I've been trying to come up with a representation of a spectrahedron, probably with $B = bb^T$. I was thinking of $AA^T = A_i$ for $i = 1, \dots, n$. But I haven't made much progress with this approach. I figure making a solid guess on the form and then showing ""$\subseteq$"" and ""$\supseteq$"" is the best way to prove this.","['semialgebraic-geometry', 'spectrahedra', 'polyhedra', 'linear-matrix-inequality', 'geometry']"
2817381,Linear combination of matrix functions is invertible,"We are given two smooth $m \times m$ matrix functions $F_1, F_2: \mathbb{R}^n \to \mathbb{C}^{m \times m}$. We are also given bounded open sets $U_1$ and $U_2$, such that $F_i$ is non-singular (invertible) in a slightly larger open set containing $\overline{U_i}$ for $i = 1, 2$. Is there a linear combination $C = a_1F_1 + a_2F_2$ for some $a_1, a_2\in \mathbb{C}$ such that $C$ is invertible on $U_1 \cup U_2$?","['matrices', 'linear-algebra', 'functions']"
2817437,Meaningless partial derivative,"The function in question is $$f(x,y) =
\begin{cases}
0 & (x,y)=(0,0)\\
\frac{xy}{|x|+|y|} & (x,y) \neq (0,0)
\end{cases}
$$ So far I've calculated the continuity of the function at $(0,0)$ using the squeeze theorem with the functions $xy$ and $\frac{xy}{\sqrt{x^2+y^2}}$. The former is always bigger than the second branch of $f$, and the latter always smaller; they all have limit $0$, therefore the function is continuous at $(0,0)$. Now I have to calculate the first partial derivative in order to $x$ of $f$ at $(0,0)$. Using the definition, we have $$\frac{\partial f}{\partial x}(0,0)=\lim_{t\to 0}\frac{f(t,0)-f(0,0)}{t}$$ Now, $f(0,0)$ is just $0$, and $f(t,0)$ is $0$ as well. So we have $$\lim_{t\to 0}\frac{f(t,0)-f(0,0)}{t}=\lim_{t\to 0}\frac{0}{t}$$ which is not really meaningful, at least to me, and there doesn't seem to be an obvious way of solving this limit without finding the full expression. So I started hunting for an expression for the partial derivative using the limit definition. That should be $$\frac{\partial f}{\partial x}(x_0,y_0)=\lim_{t\to 0}\frac{f(x_0+t,y_0)-f(x_0,y_0)}{t}$$ but that seems impressingly difficult to calculate. If any of this is correct, we have $$\lim_{t\to 0}\frac{\frac{(x+t)y}{|x+t|+|y|}-\frac{xy}{|x|+|y|}}{t}=\lim_{t\to 0}\frac{(x+t)y}{t(|x+t|+|y|)}-\frac{xy}{t(|x|+|y|)}$$ and after a few ""simplifications"" (which don't simplify anything at all), we get $$\lim_{t\to 0} \frac{xy|x|+ty|x|+xy|y|+ty|y|-xy|x+t|-xy|y|}{t(|x+t|+|y|)(|x|+|y|)}$$ and there's where it becomes a huge mess. I think separating $|x+t|$ would help a lot , but I have no idea on how to justify doing that. This analysis class is killing me, even if you can't help me with the problem just recommending a good textbook would help a ton. Here's the function plotted on WA: https://www.wolframalpha.com/input/?i=f(x,y)+%3D+(xy)%2F(%7Cx%7C%2B%7Cy%7C)","['multivariable-calculus', 'real-analysis']"
2817445,Prove a statement about the GCD of $a$ and $b$,"Prove that if $\gcd(a, b) = 1$, then $\gcd(a - b, a + b)$ must be $1$ or $2$. I have it if $a$ and $b$ are odd, but what if they aren't?","['discrete-mathematics', 'elementary-number-theory']"
2817492,Conditional probability question involving balls w/o replacement,"A box contains 12 balls numbered 1 through 12.  If 5 balls are selected one at a time from the​ box, without​ replacement, what is the probability that the largest number selected will be 9​? I want to just say $$\frac{9\times8\times7\times6\times5}{\binom{12}{5}}$$ but that is wrong and I don't know why.",['probability']
2817536,A surjective map $S^{2} \longrightarrow S^{2}$,"Is there any continuous surjective map  $S^{2} \longrightarrow S^{2}$ such that it sends one of the meridians of the first sphere into the south pole of the second one? I can see that is true, but I'm having a hard time figuring out a proper ""analytical"" definition (with some kind of coordinates etc.). EDIT: For a meridian I mean the shortest arc from the north pole to the south pole (with poles included).","['multivariable-calculus', 'general-topology', 'differential-topology']"
2817548,Sampling from partitions of graph vertices into connected subsets,"Suppose I have a connected graph $G=(V,E)$, where $E\subseteq V\times V$ contains undirected edges.  For $V'\subseteq V$, denote $G_{V'}$ as the induced subgraph $G_{V'}:=(V',E\cap(V'\times V')).$ Denote $S$ as the set of partitions into two connected components:
$$
S:=\{V'\subseteq V : G_{V'}\textrm{ and }G_{V\backslash V'}\textrm{ are connected subgraphs}\}.
$$ Is it tractable to sample from $S$ with uniform probability for each element? Apologies for clunky notation.  Some ideas that sample non-uniformly include: Randomly draw a spanning tree and cut one edge Merge pairs of vertices together until two clusters are left","['graph-theory', 'set-partition', 'sampling', 'probability', 'discrete-mathematics']"
2817560,Linearization of differential system of equation,"I would like to ask if I understand correctly the process of linearization for analyzing critical points. I was given differential equation: $\dot x = xy+1$ $\dot y = x+xy$ And my task was to linearize the system around stationary points. What I've done is: calculated critical point (I think the result is point $T(1,-1)$) and then I calculated Jacobian matrix and it's eigenvalues and corresponding eigenvectors and after that I've got that the point $T$ is saddle and then I drew the picture. I am not quite sure where can I check if steps that I've done are correct. Is there some program available for drawing phase portraits or online tool. Or if someone could solve the task and tell whether are my steps correct and if I'm missing something.","['jacobian', 'ordinary-differential-equations', 'linear-algebra', 'linearization']"
2817588,Applying Inverse Function Theorem to open set $A$ to prove $f^{-1}:f(A) \to A$ is a differentiable map,"This is the first question in Spivak's Calculus on Manifolds from the chapter on the Inverse Function Theorem. Problem 2-36. * Let $A \subset \mathbb{R}^n$ be an open set and $f : A \to \mathbb{R}^n$ a continuously differentiable $1$-$1$ function such that $\det f'(x) \neq 0$ for all $x$. Show that $f(A)$ is an open set and $f^{-1} : f(A) \to A$ is differentiable. Show also that $f(B)$ is open for any open set $B \subset A$. To prove that $f^{-1}:f(A) \to A$ is differentiable, I began by applying the Inverse Function Theorem to some $x\in A$, which gives an inverse function from some subset of $f(A)$, lets call it $W_{f(x)}$ to a subset of $A$, called $V_x$, thus giving $f^{-1}:W_{f(x)} \to V_x$ as a differentiable map.  How then is $f^{-1}:f(A) \to A$ differentiable?  Do I generate inverse functions for each $y\in f(A)$ and define $f^{-1}:f(A) \to A$ as a piecewise function such that $f^{-1}:W_{f(x_1)} \to V_{x_1}$ is the function that applies when we are mapping an element that's contained in $W_{f(x_1)}$?  Or, do I just need one of the differentiable maps $f^{-1}:W_{f(x)} \to V_x$ to prove this? Thanks in advance.","['functions', 'calculus', 'multivariable-calculus', 'inverse', 'analysis']"
2817595,Restriction of the exceptional divisor to itself as a line bundle,"If we take a complex projective variety $X$ and blow it up at a point, we get an exceptional divisor $E\cong \mathbb{P}^{n-1}$, where $n=dim(X)$. My question basically regards $\mathcal{O}_{\tilde X}(E)$ or to be more precise $\mathcal{O}_{\tilde X}(E)|_E$. This will be a line bundle on $E$ and I think it should be just $\mathcal{O}_E(-(n-1))$. The question is, how can I see this claim? We can consider e.g. the simple case of the blow up of $\mathbb{P}^2$, which looks like this $\{[a:b:c],[x:y]|ay-bx=0\}\subset \mathbb{P}^2\times\mathbb{P}^1$. Then $E$ will be of the form $E=\{a=b=0\}\subset X$. My attempt:
I tried to represent the divisor $E$ by a line bundle through choosing a covering and then calculating coycles for that covering (which was just the product of the standard affine coverings). If needed I can elaborate on this, but the cocycles that I got gave me the trivial bundle as restriction on $E$, which is not what I should get, but rather the tautological bundle! How do I actually end up with the tautological bundle?","['line-bundles', 'complex-geometry', 'algebraic-geometry', 'blowup']"
2817622,How to use the Lindeberg CLT in this scenario ? (analysis problem),"We have $(X_i)_{i \in \mathbb Z}$ iid random variables with $1\le X_i \le2$ almost surely. We define $X(x,\omega) \equiv X_i (\omega)$ if $x\in [i,i+1[$ and $X_\epsilon (x, \omega) \equiv X(x/\epsilon, \omega)$. A post that solves a very similar problem: Show that those random quantities converge in distribution to a normal variable (hard analysis problem) Define $$u'_\epsilon(x,\omega)= \frac {c_\epsilon(\omega) - F(x)}{X_\epsilon(x,\omega)}$$ where $F$ is an $L^1([0,1])$ function and $c(\omega)$ is defined by $$c_\epsilon(\omega)\equiv \frac{\int_0^1 \frac{F(y)}{X_\epsilon(y,\omega)} \, dy}{\int_0^1\frac 1 {X_\epsilon(y,\omega)}\, dy}$$ Show that $$\epsilon^\alpha\int_0^1 u'_\epsilon(x,\omega) g(x)\, dx \to \mathcal N(?, ?)$$ for a certain $\alpha\in\mathbb R$, in distribution when $\epsilon \downarrow 0$ for any sufficiently smooth function $g$, where we need to characterize the expectation and the variance. A hint says for this problem that we should notice that $u'_\epsilon(x,\omega)$ can be written as a product of a random part and a deterministic part + an error that can be controlled in $L^2(]0,1[ \times \Omega)$. We can easily notice that $$u'_\epsilon(x,\omega)= \frac {F(x)}{X_\epsilon(x,\omega)} + err_\epsilon(x,\omega)$$ where $$err_\epsilon(x,\omega)=\frac {\int_0^1 \frac {F(y)}{X_\epsilon(y,\omega)} dy}{X_\epsilon(x,\omega)\int_0^1 \frac {1}{X_\epsilon(y,\omega)} dy}$$ I have no idea how to control this and how will this help (maybe this is not the intended form). The question related to this control can be found here : Controlling this function in $L^2$ norm Another post that brings more information about $u'_\epsilon$ is the following: Showing an $L^2$ convergence (with convergence rate) EDIT: we can suppose that $F$ is continuous on $[0,1]$ if it helps. EDIT2: Bonus rep will be awarded if, in addition to the above, the convergence is shown using quantitative argument (total variation distance, Wasserstein distance, Kolmogorov distance, Zolotarev distance...)","['stochastic-processes', 'real-analysis', 'probability-theory', 'probability', 'measure-theory']"
2817630,Why is the condition number of a matrix given by these eigenvalues?,"${\bf A}$ is $n \times n$ matrix. I want to know why the condition number of ${\bf A}$ is given by: $$\frac{\rm{max}(\lambda_i)}{\rm{min}(\lambda_j)},$$ where $\lambda$ are eigenvalues of ${\bf A}$ . Would you please give me a proof? This equation come from the page 80 in ""Deep Learning"" written by Ian Goodfellow.","['matrices', 'condition-number', 'numerical-linear-algebra', 'linear-algebra']"
2817635,Why is the derivative important? [duplicate],"This question already has answers here : Why do we differentiate? (6 answers) Closed 6 years ago . Derivatives, both ordinary and partial, appear often in my mathematics courses. However, my teachers have never really given a good example of why the derivative is useful. My questions: Other than the usual instantaneous rate of change, what are some common uses of the derivative? What does the partial derivative tell us? And what does the total derivative tell us? I find that often times, the derivative is simply explained as ""the instantaneous rate of change"". I am thinking about switching my major, because the applications of math at such an elementary level seem trivial when professors just push symbols and don't have any real world motivation included in their lectures. P.S. This question is not a duplicate of Why do we differentiate? I do not want to know why we differentiate. I want to know why it is important past our undergraduate learning. What are the applications beyond Calculus 3? Beyond academia, what makes the derivative important in complex situations?","['derivatives', 'applications', 'soft-question', 'motivation']"
