question_id,title,body,tags
2576779,Viewing schemes as varieties,"So this question is motivated from example 6.5.2 in Hartshorne Chapter 2. There he looks at $X=$ Spec $(A)$ where $A=k[x,y,z]/(xy-z^2)$ and he calls $X$ a cone in $k^3$ which to me is the variety of that polynomial as defined in Chapter 1. From section 4 there is a correspondence between varieties and integral separated schemes over a field and since we're working with divisors all schemes we are dealing with have these properties (plus others), so I can sort of see the relationship he is using here. However to what extent can we treat $X$ as a variety (ie. $Z(xy-z^2)$ )? As he goes through the example he uses other variety-esque constructions (eg. $Y=\{y=z=0\}$ he claims is a prime divisor of $X$ - how can you show that $Y$ is a closed integral subscheme of codim 1?) so how is this all justified?","['schemes', 'algebraic-geometry']"
2576799,How would a radical frequentist argue against the Conditionality Principle?,"I have the following definition: Def: (Conditionality Principle (CP)). If two experiments on the parameter $\theta$, $\mathcal{E}_1$ and $\mathcal{E}_2$, are available and if one of these two experiments is selected with probability $p$, the resulting inference on $\theta$ should only depend on the selected experiment. My book provides the following example: Suppose you work in a laboratory that contains three  thermometers, $T_1$,  $T_2$, and $T_3$. All three thermometers produce measurements that are normally distributed about the true temperature  being  measured.   The  variance  of  $T_1$'s  measurements is equal to that of $T_2$'s but much smaller than that of $T_3$'s.  $T_1$ belongs to your colleague John,  so he always gets to  use  it.   $T_2$  and  $T_3$  are  common  lab  property,  so  there  are frequent disputes over the use of $T_2$.  One day, you and another colleague  both  want  to  use  $T_2$,  so  you  toss  a  fair  coin  to  decide who gets it.  You win the toss and take $T_2$.  That day, you and  John  happen  to  be  performing  identical  experiments  that involve  testing  whether  the  temperature  of  your  respective  indistinguishable samples of some substance is greater than $0^\circ C$ or not.  John uses $T_1$ to measure his sample and  finds that his result is just statistically significantly different from $0^\circ C$. John celebrates and begins making plans to publish his result.  You use $T_2$ to measure your sample and happen to measure exactly the same value as John.  You celebrate as well and begin to think about how you can beat John to publication. ""Not so fast"", John says. ""Your experiment was different from mine. I was bound to use $T_1$ all along, whereas you had only a 50% chance of using $T_2$.  You need to include that fact in your calculations.  When you do, you'll  find that your result is no longer significant."" The book I'm using then states: ""According to radically behaviouristic forms of frequentism, John is correct."" Question: On what base would a frequentist say that John is correct? The book I'm using provides this example to illustrate the Conditionality Principle (according to the CP John would be incorrect), but it doesn't explain why frequentists would even say otherwise. Thanks in advance!","['statistical-inference', 'probability-theory', 'hypothesis-testing', 'statistics', 'probability']"
2576862,What is the implication of the likelihood principle when $X=2$?,"I have the following definition of the likelihood principle: Def: (Likelihood Principle). The information brought by an observation $x$ about $\theta$ is  entirely contained in the likelihood function $L(\theta|x)$. Moreover,  if $x$ and $x'$ are  two  observations  depending  on  the  same  parameter (possibly in different experiments),  such that there exists a constant $c$ satisfying $L(\theta|x) =cL'(\theta|x')$ for every $\theta$, they bring the same information about $\theta$ and must lead to identical inferences. In my book further explanation is given: The likelihood principle says this:  the likelihood function, long known  to  be  a  minimal  sufficient  statistic,  is  much  more  than merely a sufficient statistic, for given the likelihood function in which an experiment has resulted, everything else about the experiment -what  its  plan  was,  what  different  data  might  have resulted from it, the conditional distributions of statistics under given parameter values, and so on- is irrelevant Consider the following exercise: Exercise: Consider an experiment with outcomes $\{1,2,3\}$ and probability mass functions $f(\cdot|\theta)$, $\theta\in\{0,1\}$ given by
$$\begin{array}{|c|c|c|}
\hline
x & 1 & 2 & 3\\ \hline
f(\cdot|0) & 0.9 & 0.05 & 0.05 \\ \hline
f(\cdot|1) & 0.1 & 0.05 & 0.85 \\ \hline
\end{array}$$ Show that the procedure to reject the hypothesis $H_0:\theta = 0$ vs $H_1:\theta =1$ when $X\in\{2,3\}$ has a probability of $0.9$ to be correct (both under $H_0$ and $H_1$). What is the implication of the likelihood principle? What I've tried: I was able to show that the procedure has a probability of $0.9$ to be correct (both under $H_0$ and $H_1$). However, I'm not sure what the likelihood principle implies when $X = 2$. The likelihood principle says that for given the likelihood everything else is irrelevant. I don't really understand what irrelevant even means in this context. Shouldn't the likelihood principle imply something in general? That is, for whatever value of $X$? Question: What is the implication of the likelihood principle when $X = 2$? Thanks in advance!","['probability-theory', 'probability', 'statistics', 'statistical-inference']"
2576872,Maximize function on orthogonal matrices,"Consider the function $f$ from the set of $n \times n$ real matrices taking $A=(a_{ij})$ to $f(A):= \prod_{(i,j) \neq (k,l)}(a_{ij}-a_{kl}) $. Edit: Note that $f(A) \ge 0$ for all $A$, since grouping pairs we have
$$f(A):=(-1)^{\frac{n^2(n^2-1)}{2}}\prod_{(i,j) \lt_ {lex} (k,l)}(a_{ij}-a_{kl})^2$$
and $ n^2(n^2-1) \equiv 0 \, \text{mod} \, 4$ for all $n$, end edit. Is there a way to compute $M:=\text{max}\{  f(A)   : A A^{t}=I  \}$ ?. I know that, since $A$ orhogonal implies $\mid a_{ij} \mid \leq 1$ for all $ 1\leq i,j \leq n $, we have that $2^{2 {n^2 \choose 2}}=2^{n^2(n^2-1)}$ is a trivial upper bound on M, but for example if $n=2$ we have that actually $M=0$ because every $2 \times 2$ orthogonal matrix has at least two equal entries. So if we can't compute $M$ can we at least give a better upper bound?. Following your advice. I estimated $M$ using random orthogonal matrices for $n \le 6$. Here are the results I've got: \begin{array}{|c|c|c|c|}
\hline
n & \text{# of orth. Matrices} & \text{Estimated} \, M  \\ \hline
2 & \infty & 0 \\ \hline
3 & 5\times 10^4 & 1.13762 \times 10^{-17} \\ \hline
4 & 5\times 10^4 & 3.10228 \times 10^{-80} \\ \hline
5 & 1 \times 10^3 & 5.71162 \times 10^{-248} \\ \hline
6 & 1\times 10^3 & 2.80541 \times 10^{-588}\\ \hline.
\end{array} for example this is what the graphic for $n=3$ looks like: So it seems that $M$ is much more smaller than the trivial bound. Does anyone has an idea of how to construct a formal proof?. I'll be offering a bounty as soon as is allowed.","['optimization', 'calculus', 'maxima-minima', 'orthogonal-matrices', 'linear-algebra']"
2576899,"$\langle Tu\;|\;u\rangle=0,\;\forall u\in E \Longrightarrow T=0$?","Let $E$ be  a complex Hilbert space. Let $T\in \mathcal{L}(E)$. I have two questions: Why it is not true that for an arbitrary operator $T\in \mathcal{L}(E)$, we have $\langle Tu\;|\;u\rangle=0,\;\forall u\in E \Longrightarrow T=0$? And is  this property true for normal operators? I think it is true for self adjoint operators because the norm of a self adjoint operators is given by $$\left\|T\right\|= \sup\big\{\;\left|\langle Tu\;|\;u\rangle \right|,\;\;u \in E\;, \left\| u \right\| = 1\;\big\}$$ Thank you.","['functional-analysis', 'operator-theory', 'hilbert-spaces']"
2576931,Homeomorphism between subset of 3-Sphere and Cartesian Product of unit disc and unit circle,"I am stuck trying to solve the following exercise. Let f: $S^1 \times S^1 \to D^2 \times S^1$ be defined by $f(x,y)=(y,x)$. 
Let \begin{align*}
X= & \; \{(x_1,...,x_4) \in S^3 \mid x_1^2+x_2^2\geq x_3^2+x_4^2\}~\text{and}\\
Y= & \; \{(x_1,...,x_4) \in S^3 \mid x_1^2+x_2^2\leq x_3^2+x_4^2\}.
\end{align*} a) Show that $X\cap Y$ is homeomorphic to $S^1\times S^1$ and that X and Y are homeomorphic to $D^2 \times S^1$. b) Show that $(S^1 \times D^2)\cup_f(D^2 \times S^1)$ is homeomorphic to $S^3$. I was able to show the first part of a) but I'm stuck with the second part. Obviously I need a homeomorphism $h: X\to D^2\times S^1$. Since $1\geq x_1^2+x_2^2\geq x_3^2+x_4^2$ I can define $h(x_1,...,x_4)=((x_1,x_2),.)$ but I have problems defining the second component in such a manner that h is a homeomorphism. I probably have to use the fact that $x_1^2+x_2^2+x_3^2+x_4^2=1$, but I do not see how.",['general-topology']
2576941,Nasty logarithmic infinite sums arising from a tan integral,"While trying to work on an old post , I managed after some manipulations to show that the nasty improper tan integral
$$
I=\int_0^{\pi} \left( \frac{\pi}{2} - x \right) \frac{\tan x}{x} \, {\rm d}x\approx 2.138967
$$
could be represented as
$$
I=\sum_{k=1}^\infty\frac{\ln \left(\frac{2 k+1}{2 k-1}\right)}{2 k-1}-
\sum_{k=1}^\infty\frac{\ln \left(\frac{2 k-1}{2 k+1}\right)}{2 k+1}\ .\qquad(\star)
$$
The purpose of the original post was to find a closed-form solution for $I$. Clearly the problem would be solved if closed-form solutions for the (equally nasty) logarithmic sums in $(\star)$ could be found. I have worked a bit on those, but didn't achieve much. My question is: can the sums in $(\star)$ be evaluated in closed form? Many thanks, folks.","['logarithms', 'trigonometric-integrals', 'definite-integrals', 'sequences-and-series']"
2576947,Solve $x^3 +y^3 + z^3 =57$,How can we solve $x^3 + y^3 + z^3 =57$ efficiently in a shorter way. $x$ $y$ and $z$ are integers. Given that modulus of $x$ $y$ and $z$ is less than or equal to five. We can of course do by hit and trial but what is the method of solving such questions. I actually stumbled upon this equation while solving a determinant. How to proceed. Pls help,"['algebra-precalculus', 'number-theory', 'diophantine-equations']"
2576982,Why solvable finite groups can't have 6 sylow 5-subgroups?,"As mentioned in Question on the unsolvability a group , a solvable finite group can't have exact 6 Sylow 5-subgroups. And I'm trying to prove it. Suppose the opposite that $\Sigma=\{ P_1,...P_6\}$ is the set of Sylow 5-subgroups of $G$ and let $G$ acts on it by conjugacy. Then $G/H$ can be embedded into $S_6$ where $H= \bigcap_{i=1}^6 N_G(P_i)$. Since the number of Sylow 5-subgroups of $G/H$ is at most 6, and $S_6$ has 36 Sylow 5-subgroups, $G/H$ is a proper subgroup of $S_6$. Then I lost my may. Since $A_5$ has exact 6 Sylow 5-subgroups and it's simple, I guess there's something to do with $G/H$ and $A_5$. Then lead to a contradiction as $G$ is supposed to be solvable. Am I right? What to do next? Thanks in advance.","['finite-groups', 'abstract-algebra', 'group-theory']"
2576990,Is every variation field a Lie derivative of the metric?,"$\newcommand{\Vol}{\text{Vol}}$ Let $(M,g)$ be a smooth Riemannian manifold, and let $V \in \Gamma(T^*M \otimes T^*M)$ be symmetric. Is it true that $V=L_Xg$ for some vector field $X \in \Gamma(TM)$? I think the answer is negative, but I have no idea how to show this. (This is a question about whether every section is ""exact"" in a suitable sense). Edit: The global question: The answer is in general no. To understand the question better, let's separate into cases: $M$ is compact. Let $X \in \Gamma(TM)$, and denote by $\phi_t$ its flow. Define $g_t=\phi_t^*g$. Since $$\Vol(M,\phi_t^*g)=\Vol(M,g)=\text{const},$$ we get by differentiating that
$$0=\left.\frac{d}{dt}\Vol(M,\phi_t^*g) \right |_{t=0}= \frac{1}{2}\int_M \langle g,\left.\frac{\partial g}{\partial t}\right |_{t=0}\rangle \Vol_g=\frac{1}{2}\int_M \langle g ,L_x g \rangle \Vol_g.$$ So, a necessary condition for $V \in \{ L_xg \, | \, X \in \Gamma(TM) \}$
is $ \int_M \langle g ,V \rangle \Vol_g=0$. Is this condition sufficient? What happens when $M$ is non-compact? Are there any necessary conditions? The local question: In dimension $1$ every variation field is realized in this way. In higher dimensions I guess that is not so; my heuristic is that there are different numbers of degrees of freedom: If $\dim M=d$, then pointwise $\dim(T^*M \otimes T^*M)=d^2$ while $\dim(TM)=d$.","['riemannian-geometry', 'differential-topology', 'calculus-of-variations', 'vector-bundles', 'differential-geometry']"
2577024,Visualizing Multivariable Functions,The 2D Circle eliminates option B. The next two 2D graphs seem to work for both option A and C... How can I be 100% sure about which one it is?,['multivariable-calculus']
2577040,Which values of $f$ satisfying $f(x)=xf(x^2-3)-x$ we know?,"My question is inspired by Functional puzzle: find $f(2)$ A function $f\colon \mathbb{R}\to\mathbb{R}$ satisfies
$$f(x)=xf(x^2-3)-x$$
for all $x$. For which $x$  is the value of $f(x)$ known? I know values of $f(x)$ for $x\in\{0,\pm1,\pm2\}$. Edit: As @StevenStadnicki observed, a domain may be smaller than reals, so every suggestion how large it can be is also interesting.","['algebra-precalculus', 'functions']"
2577056,element not in subgoup,"Let $G$ be a finite group and $H$ a subgroup of $G$ of index $k$ i.e. $|G| = k* |H|$. Consider any $g \in G$\ $H$ (in the set $G$ but not in $H$). Now if $H$ is normal then the quotient group $G$ \ $H$ has order $k$ so by Lagrange we have that $g^k \in H$ for all $g \in G$. My question is, if $H$ is not normal, we should find a $g \in G$\ $H$ such that $g^k \notin H$, but looking at some particular examples I couldn't find a counterexample.",['group-theory']
2577065,"Using the Left-Inverse to ""Solve"" an Impossible System of Equations","I was working with the following system of equations: $$\begin{split}
    \begin{bmatrix}
    4 & 0\\
    0 & 5\\
    0 & 0\\
    \end{bmatrix}
    \begin{bmatrix}
    x_1\\
    x_2\\
    \end{bmatrix}
     & = 
    \begin{bmatrix}
    1\\
    1\\
    1\\
    \end{bmatrix}
    \end{split}
$$ Clearly, this has no solution on account of the last rows in the coefficient and solution matrices. However, multiplying by the left-inverse of the coefficient matrix seems to imply a solution: $$\begin{split}
    \begin{bmatrix}
    \frac{1}{4} & 0 & b_{13}\\
    0 & \frac{1}{5} & b_{23}\\
    \end{bmatrix}
    \begin{bmatrix}
    4 & 0\\
    0 & 5\\
    0 & 0\\
    \end{bmatrix}
    \begin{bmatrix}
    x_1\\
    x_2\\
    \end{bmatrix}
     & = 
    \begin{bmatrix}
    \frac{1}{4} & 0 & b_{13}\\
    0 & \frac{1}{5} & b_{23}\\
    \end{bmatrix}
    \begin{bmatrix}
    1\\
    1\\
    1\\
    \end{bmatrix} \\
    \begin{bmatrix}
    x_1\\
    x_2\\
    \end{bmatrix}
     & = 
    \begin{bmatrix}
    \frac{1}{4} + b_{13}\\
    \frac{1}{5} + b_{23}\\
    \end{bmatrix}
    \end{split}
$$ Where $b_{13}$ and $b_{23}$ can be any number. As stated above, there is no solution to this system, so any solution obtained by the above method is wrong. What I am not understanding is why, after applying the rules of matrix multiplication, does it seem possible that there is a solution? Did I make a mistake somewhere in the process, and/or is there some fundamental nuance of linear algebra that I'm missing here?",['linear-algebra']
2577146,Fubini's Theorem and Integral Bounds,"In Shao's mathematical statistics, a problem asks to show for r.v. $X$ with cdf $F$, the expectation (provided it exists) can be evaluated as: 
$$
EX=\int_0^\infty[1-F(x)]dx-\int_{-\infty}^0F(x)dx
$$
I can't convince myself why the integral bounds change the way they do, once Fubini's theorem is used to swap the order of integration. Here's the start of his solution:
$$
\begin{align*}
\int_0^\infty[1-F(x)]dx&=\int_0^\infty\int_x^\infty dF(y)dx\\
&=\int_0^\infty\int_0^ydxdF(y)
\end{align*}
$$
This might be a basic calc misunderstanding, but hopefully someone could clarify.","['improper-integrals', 'probability-theory', 'integration', 'expected-value']"
2577159,Axiom of Finite Additivity and Axiom of Continuity imply Axiom of Countable Additivity.,"This exercice in particular comes from Statistical Inference by Casella and Berger, I have been following its solution manual. The exercice is 1.12 and can be found here http://exampleproblems.com/Solutions-Casella-Berger.pdf Let $S$ be a sample space with associated $\sigma$-algebra $\mathcal{A}$. Finite Additivity: Let $A,B \in \mathcal{A}$, $P(A \sqcup B) = P(A) + P(B)$ Continuity: Let $\{A_i\}_i$ be a sequence of nested sets s.t. $A_k \downarrow \emptyset$, then $P(A_k) \rightarrow 0$. $$P\left(\bigsqcup_{i=1}^\infty A_i\right) = P\left(\bigsqcup_{i=1}^k A\right) + P\left(\bigsqcup_{i=k}^\infty\left) = \sum_{i=1}^k P(A_i) + P\right(\bigsqcup_{i=k}^\infty A_i\right)$$ Define $A^*_k = \bigsqcup_{i=k}^\infty A_i$, then $\{A^*_k\}_k$ is a sequence of nested sets. My problem is how to see that $A^*_k \downarrow \emptyset$. The argument is that $A_k \downarrow \emptyset$ as $k \rightarrow \infty$ otherwise the sum of probabilities would be infinite. But for example, say $S = [0,1]$, $\mathcal{A} = \mathcal{B}([0,1])$ and for every $A \in \mathcal{A}$ we define $P(A)$ as its lebesgue measure, then $(S, \mathcal{A},P)$ is a probability space. In particular we can construct a nested sequence of sets $\{A_i\}$ st $A_i = \{x_0\}$ for all $i>k$, $\sum_i P(A_i) \leq 1$ and $A_i \not\downarrow \emptyset$. What am I missing?",['probability-theory']
2577171,About interesting numbers,"We call a natural number interesting if it can be decomposed into natural factors, each of which is less than $30$. Prove that out of $10000$ interesting numbers, we can always choose two, whose product is a complete square. My attempt. Order a set of $10000$ thousand interesting numbers in ascending order
$$
a_1 < a_2 < \ldots < a_{10000}.
$$
Suppose, that $a_1 < 10000$. We note that the estimate $10000 = 100^2 < a_{10000}$. Then it is clear that under these restrictions there are a number $b < 100$, that $b^2 = a_n$, for some $n \in \{1,
\ldots,10000\}$. But how to prove if $a_1 \ge 10000$?","['number-theory', 'elementary-number-theory']"
2577235,Galois cohomology elements becoming trivial over $k(\mathrm{SB}(A))$,"Assume we work over a field $k$ of characteristic zero. Let $A$ be a central simple algebra over $k$ of index $\mathrm{ind}(A) = 2$ and let $[A]$ denote its class in Br$(k)$. Let $\alpha \in$ $H^3(k,\mu_2)$. Let us consider the restriction map in Galois cohomology to the function field of the Severi-Brauer variety of $A$, which we denote by $\mathrm{res}_{k(\mathrm{SB}(A))/k}$. Assume that $\alpha$ lies in the kernel of $\mathrm{res}_{k(\mathrm{SB}(A))/k}$. Question: Does $[A]$ divide $\alpha$ ? (over $k$) The degree of $H^3$ and the case $\mathrm{ind}(A) = 2$ shouldn't influence the outcome, but I am particularly interested in this situation. Also the opposite direction is trivial and I do not see why $[A]$ would not divide $\alpha$, since I think the kernel of the restriction is generated by $[A]$. But still I am not sure if this is all one needs.","['galois-cohomology', 'algebraic-geometry']"
2577236,Differential equations and exponential growth,"I was reading about differential equations and got stuck in a small detail that I can't make peace with. If a population doubles every unit of time, I would write $$ \frac{dP}{dt}=2P $$ which by separation of variables would yield $$ P=P(0)e^{2t} $$ But I also know intuitively that I should have $$ P = P(0)e^{(ln2)t} = P(0)2^{t}$$ which suggests the factor in the differential equation should be $\ln2$ instead of 2. What's wrong with my logic? Thanks! EDIT: This is the part of the textbook that confused me. 
Doesn't it confuse discrete and continuous cases as well?",['ordinary-differential-equations']
2577281,"""Good"" linear approximation criteria?","I've been told that linear approximation is considered as ""good"" if it meets the criteria below:
$$\lim_{x \to a} \frac{f(x)-f(a)-f'(a)(x-a)}{x-a} = 0$$ As far as I understand, the differentiation of $f(x)$ suppose to provide such a good approximation? So I wrote a simple script testing out $f = x^2$ at the $a = 2$. Thus following sequence expected to be infinitely small:
$$\lim_{x \to 2}\frac{x^2 - 4x - 12}{x - 2}$$ Now let me publish some bits of computation done by my machine: [""f(2.0001) = -160000""; ""f(2.0002) = -80000""; ""f(2.0003) = -53333.3"";
     ""f(2.0004) = -40000""; ""f(2.0005) = -32000""; ""f(2.0006) = -26666.7"";
     ""f(2.0007) = -22857.1""; ""f(2.0008) = -20000""; ""f(2.0009) = -17777.8"";
     ""f(2.001) = -16000""; ""f(2.0011) = -14545.5""; ""f(2.0012) = -13333.3"";
     ""f(2.0013) = -12307.7""; ""f(2.0014) = -11428.6""; ""f(2.0015) = -10666.7"";
     ""f(2.0016) = -10000""; ""f(2.0017) = -9411.76""; ""f(2.0018) = -8888.89"";
     ""f(2.0019) = -8421.05""; ""f(2.002) = -8000""; ""f(2.0021) = -7619.05"";
     ""f(2.0022) = -7272.73""; ""f(2.0023) = -6956.52""; ""f(2.0024) = -6666.66"";
     ""f(2.0025) = -6400""; ""f(2.0026) = -6153.84""; ""f(2.0027) = -5925.92"";
     ""f(2.0028) = -5714.28""; ""f(2.0029) = -5517.24""; ""f(2.003) = -5333.33"";
     ""f(2.0031) = -5161.29""; ""f(2.0032) = -5000""; ""f(2.0033) = -4848.48"";
     ""f(2.0034) = -4705.88""; ""f(2.0035) = -4571.43""; ""f(2.0036) = -4444.44"";
     ""f(2.0037) = -4324.32""; ""f(2.0038) = -4210.52""; ""f(2.0039) = -4102.56"";
     ""f(2.004) = -4000""; ""f(2.0041) = -3902.43""; ""f(2.0042) = -3809.52"";
     ""f(2.0043) = -3720.93""; ""f(2.0044) = -3636.36""; ""f(2.0045) = -3555.55"";
     ""f(2.0046) = -3478.26""; ""f(2.0047) = -3404.25""; ""f(2.0048) = -3333.33"";
     ""f(2.0049) = -3265.3""] From the result above clearly seen: the closer $x$ gets to the $a = 2$, the bigger output. How I suppose to interpret such a result? 
Did I do a mistake?
Did I misunderstand the ""good linear approximation"" concept?
Did I broke math?","['derivatives', 'linear-approximation', 'calculus']"
2577315,"Why does a surface F(x,y,z) that never fold back on itself have $\nabla F$ $\cdot \vec{p} \neq 0$?","Why does an implicit surface F(x,y,z) that never fold back on itself have $\nabla F$ $\cdot \vec{p} \neq 0$?","['analytic-geometry', 'multivariable-calculus', 'differential-geometry', 'surfaces', 'vector-analysis']"
2577322,Evaluate $ \frac{1}{(q)_\infty} \sum_{m \in \mathbb{Z}} q^{\frac{m^2}{2}} (-q^{-\frac{1}{2}}x)^m y^m(q^{1-m}y^{-1};q)_\infty $,"This identity is taken from a physics paper [ 1 ] stated without proof, on page 43. $$ \frac{1}{(q)_\infty} \sum_{m \in \mathbb{Z}} q^{\frac{m^2}{2}} (-q^{-\frac{1}{2}}x)^m y^m(q^{1-m}y^{-1};q)_\infty
= \frac{(xy;q)_\infty(qx^{-1}y^{-1};q)_\infty}{(x;q)_\infty}
\tag{$*$}$$ This formula is new and interesting and probably correct.  I don't know a good starting point.  While this is a $q$-series, there are two deformation paramters $x$ and $y$.  It may help to recall some definitions: $\displaystyle \prod_{n \geq 0} \big( 1 - q^n x \big) =: (x;q)_\infty $ this is the $q$-Pochhammer symbol $\displaystyle \prod_{n = 0}^{k-1} \big(1 - q^n x \big) =: (x;q)_k$ this is the $q$-Pochhammer symbol The symbol $(q;q)_k$ gets its own abbreviation $(q)_k$ but it's very similar. $\displaystyle (q)_\infty  := (q;q)_\infty = \prod_{n \geq 1} (1 - q^n) $ $\displaystyle (q)_k  := (q;q)_k = \prod_{n=1}^k (1 - q^n) $ These series are likely to ""converge"" as elements of $\mathbb{Z}[q^{\frac{1}{2}}][[x^{\pm 1}, y^{\pm 1}]]$, but do these also converge analytically?  This might not converge for any $q \in \mathbb{C}$. In order to check the equivalence, we might start with some partial fraction decomposition: $$ \frac{1}{(x;q)_\infty} = \sum_{n=0}^\infty \frac{x^n}{(q;q)_n} $$ I also wonder what the RHS of $(*)$ is counting.","['theta-functions', 'integer-partitions', 'number-theory', 'q-series', 'sequences-and-series']"
2577343,$M^2=I_n$ implies $M$ diagonalizable,"Let $M$ be an  $n \times n$ matrix  such that
$M^{2}=I_n$; does this imply that $M$ is similar to a diagonal matrix $D$? How can we prove this?","['matrices', 'diagonalization', 'linear-algebra']"
2577376,Factorising $x^2-x+1$,"I attempted to factorise $x^2-x+1$, which seemed fairly straightforward: $$ 
\begin{align}
x^2-x+1 &= (x+1)^2-3x \\
        &= (x+1-\sqrt{3x})(x+1+\sqrt{3x}) \\
        &= (x-\sqrt{3x}+1)(x+\sqrt{3x}+1)
\end{align}$$ Technically neither $(x-\sqrt{3x}+1)$ nor $(x+\sqrt{3x}+1)$ are linear functions, which is why I didn't get the ""correct"" answer: $$(x-\frac{1}{2}-i\frac{\sqrt{3}}{2})(x-\frac{1}{2}+i\frac{\sqrt{3}}{2})$$ But I am curious as to why Desmos only considers the positive cases of $x$? If $(x-\sqrt{3x}+1)(x+\sqrt{3x}+1)$ does expand perfectly into $x^2-x+1$, why isn't it graphed the same way?","['graphing-functions', 'factoring', 'complex-numbers', 'geometry']"
2577392,Cross-ratio of Conics on a pencil of conics,"Let $C_1, C_2, C_3$ be the three split conics in a simple pencil $L$ of conics on $\mathbb{P}_2$ , with $a,b,c,d$ as its four base points. For an arbitrary conic $C \in L$ , how can I compute the cross-ratio $[C_1, C_2, C_3, C]$ on $L$ . And how will that compare to the cross-ratio $[a,b,c,d]$ on $C$ ?","['plane-geometry', 'algebraic-geometry', 'projective-geometry', 'geometry', 'conic-sections']"
2577396,"If $A,B$ are positive definite, is $(A+B)^{-1} - B^{-1}$ negative definite?","In $\mathbb{R}$, given $a,b>0$, you have $a+b>b\iff \frac{1}{a+b} < \frac{1}{b} \iff (a+b)^{-1} - b^{-1}<0$. Is this true for positive definite $A,B\in\mathbb{R}^{n\times n}$ of arbitrary $n$?","['matrices', 'positive-definite']"
2577404,Get from one vertex to the opposite - dodecagon,"Let $x$ be a regular dodecagon. Starting from one vertex, an ant wants to reach the opposite vertex of the dodecagon, moving to adjacent vertices. If $p_n$ is the number of such paths with length $n$, compute $p_1+p_2+p_3...+p_{12}$. Obviously we can't have  a path of length $1$,$2$, $3$, or $4$. It's easy to find the amount of cases in which we have path lengths of $5$, $6$, or $7$. However, I need help finding the cases beyond this. Can anyone help me.","['combinatorics', 'geometry']"
2577433,Paths on a dodecahedron,"Looking at this question , I misread ""dodecagon"" as ""dodecahedron"".  I think the latter is a cool problem, so I'm posing it as a question of its own :) Starting from one vertex of a dodecahedron, an ant wants to reach the opposite vertex of the dodecahedron, moving to adjacent vertices. If $p_n$ is the number of such paths with length $n$, compute $p_1+p_2+\dots+p_{12}$.","['polyhedra', 'combinatorics']"
2577437,Rudin's RCA; Exercise 3.4 Part (b) Only,"I'm working from Rudin's $Real~And~Complex~Analysis$ (3rd Ed.) on my own -- more specifically on Exercise 4 from Chapter 3 which is partially restated below (slightly modified). I only reference part (b) of the exercise since this is the only part I'm struggling with. For the rest of the problem (namely part (a) and parts (c), (d) & (e) of the exercise), refer to this link pointing to a free download of Rudin's book referenced above (see PG. 71 in his book), or see this post , etc. Please note that I'm working out of this book exclusively, and all concepts, definitions, theorems, etc., therein is what I'm working with. Exercise 3.4 Suppose $(X,\mathfrak{M},\mu)$ is a measure space, where $\mu$ is a positive measure, and suppose $f\!:\!X\!\rightarrow\!\mathbb{C}$ is a measurable function. At any $p_{0}\in(0,+\infty)$ we define $\varphi(p_{0}):=\big(\|f\|_{p_{0}}\big)^{p_{0}}\!\!={\displaystyle{\!\!\int_{X}|f|^{p_{0}}~\!d\mu}}$ , as well as we define the set $E:=\big\{\widetilde{p}\in(0,+\infty):\varphi(\!~{\widetilde{p}}\!~)\!<\!+\infty\big\}$ . We also assume that $\|f\|_{+\infty}\!>0$ . (b) Prove that $\log(\varphi)$ is convex on the interior of $E$ as well as that $\varphi$ is continuous on $E$ . I'm having difficulty showing that $\varphi$ is continuous at all points in $E$ . As far as my dilemma in particular, I graciously need help with an elaboration of the very last, strict, inequality found in this proof for part (b) only. The estimate in that proof gives continuity on all of $E$ (note also that my comment is at the bottom of that webpage also requesting the same elaboration, to which I haven't received a response as of yet, and I figured I'd come here to see what I can find in order to move on in the book overall). In case the link for this proof is broken (or breaks at some point in the future), I will provide the relevant part of the proof posted there below (also slightly modified) in regards to part (b) in particular. //Proof (b): Begin by letting $r,s\in E\subseteq\mathbb{R}^{+}$ where $r<s$ , and then fix $\lambda\in(0,1)$ arbitrarily in order to set $p=p_{\lambda}=(1-\lambda)r+\lambda s\in(r,s)$ . By appealing to H $\ddot{\text{o}}$ lder's inequality, we can now deduce that $\varphi(p)\leq\big(\varphi(r)\big)^{1-\lambda}\big(\varphi(s)\big)^{\lambda}\!<\!+\infty$ . We can use this inequality to show the first part of (b) -- we need $\varphi>0$ on the interior of $E$ , which is indeed the case since $\|f\|_{+\infty}\!>0$ (I showed this with a quick proof by contradiction), and so $\log(\varphi)$ is convex on the interior of $E$ yielding $\varphi$ is convex on the interior of $E$ and we can conclude $\varphi$ is continuous on the interior of $E$ . In order to show that $\varphi$ is continuous at all points in $E$ , we first let $\varepsilon>0$ , and then we can find an $N\in\mathbb{N}$ such that the set $E_{N}:=\big\{\widetilde{x}\in X:|f(\widetilde{x})|>N\big\}\cup\big\{\widetilde{x}\in X:0<|f(\widetilde{x})|<\frac{1}{N}\big\}$ has measure less than $\varepsilon$ -- namely, $0\leq\mu(E_{N})<\varepsilon$ , and this is indeed possible since $\|f\|_{+\infty}\!>0$ . This being said, choose a $\delta>0$ such that $0\leq\text{min}\big\{|1-N^{\delta}|,\big|1-\frac{1}{N^{\delta}}\big|\big\}<\varepsilon$ . Then, for any $x,y\in E$ where $|x-y|<\delta$ we estimate that: ${\displaystyle{|\varphi(x)-\varphi(y)|=\bigg|\int_{X}|f|^{x}-|f|^{y}d\mu\bigg|=\bigg|\int_{E_{N}~\sqcup~X\backslash E_{N}}\!\!\!\!\!\!|f|^{x}-|f|^{y}d\mu\bigg|}}$ ${\displaystyle{~~~~~~~~~~~~~~~~~~~~~\leq\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|+\bigg|\int_{X\backslash E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|}}$ ${\displaystyle{~~~~~~~~~~~~~~~~~~~~~<2\varepsilon\big(\varphi(x)+\varphi(y)\big)}}$ , which shows that $\varphi$ is continuous on $E$ . I can't seem to simplify everything properly in order to show the very last, strict inequality above ${\displaystyle{\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|+\bigg|\int_{X\backslash E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|<2\varepsilon\big(\varphi(x)+\varphi(y)\big)}}$ holds. I was hoping, not only if this is correct, but to determine how to establish this, since this actually shows $\varphi$ is uniformly continuous on $E$ implying continuity on $E$ (right [?] -- it looks correct as $\varphi(x)+\varphi(y)<+\infty$ whenever $x,y\in E$ ). I figured we need to take the first integral ${\displaystyle{\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|}}$ and use our assumption that $\mu(E_{N})<\varepsilon$ in order to get ${\displaystyle{\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|<\varepsilon\big(\varphi(x)+\varphi(y)\big)}}$ ; additionally, I figured our choice of $\delta>0$ will enable us to show ${\displaystyle{\bigg|\int_{X\backslash E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|<\varepsilon\big(\varphi(x)+\varphi(y)\big)}}$ as well -- I think I can do this alone, but, if I'm correct, then my difficult arises with the first integral...I'm stumped. Any help is greatly appreciated!","['integral-inequality', 'continuity', 'lebesgue-integral', 'measure-theory', 'analysis']"
2577452,Property of Medians and Cirumcircle,"Let $ABC$ be a non-isosceles triangle. Medians of $\triangle ABC$ intersect the circumcircle in points $L,M,N$. If $L$ lies on the median of $BC$ and $LM=LN$, then prove that $2a^2=b^2+c^2$. My Attempt: Let $G$ be the centroid of $\triangle ABC$ and $D$ be the mid-point of $BC$. Since $LM=LN$, therefore $LM$ and $LN$ will subtend equal angles on the circumference of circumcircle $\Rightarrow \angle GBL=\angle LCG$ $GL=GL$ Altitude of $\triangle BGL$ from $B$ to $GL$=Altitude of $\triangle LGC$ from $C$ to $GL$ Area($\triangle BGL$)=Area($\triangle LGC$) Now, based on this , can it be said that $\square GBLC$ is a parallelogram. If yes then $$DL=GD=\frac{m_{a}}{3}$$ $\Rightarrow AD.DG=BD.DC$ $$m_{a}.\frac{m_{a}}{3}=\frac{a^2}{4}$$ $\Rightarrow 4m^2_{a}=3a^2$ $\Rightarrow 2b^2+2c^2-a^2=3a^2$ $\Rightarrow b^2+c^2=2a^2$ I am not sure whether this justification is sufficient(the one that I have written in bold). What more can be added to seal the issue","['circles', 'trigonometry', 'triangles', 'geometry']"
2577471,How to compute the volume of a ball in the space of matrices with norm distance as a metric? What about unitary or special unitary matrices?,"Consider the space of $n \times n$ complex matrices with metric equal to the distance in the Frobenius norm so that $d(A,B) \equiv \| A - B \| \equiv \sqrt{Tr[(A-B)^{\dagger}(A-B)]}$. I want to know the volume of a ball defined as $B_{\epsilon}(U_0) \equiv \{U: \|U - U_0\| \leq \epsilon\}$. What are some appropriate notions of volume and what would each mean? (Answers that don’t assume previous knowledge of measure theory would be helpful). What if I want to compute this based on some other norm distance? What is the general way to think about it? Also, what if I want to restrict $U$ to unitary matrices, so that for example I have a ball $S_{\epsilon}(U_0) \equiv \{U \in U(n): \|U - U_0\| \leq \epsilon\}$? What if I further restrict it to $V_{\epsilon}(U_0) \equiv \{U \in SU(n): \|U - U_0\| \leq \epsilon\}$? Note: please help with tags ...","['manifolds', 'lebesgue-measure', 'measure-theory', 'haar-measure']"
2577474,Exercise II-13 of Eisenbud-Harris Geometry of Schemes,"I'm a topologist who is learning a bit of algebraic geometry for fun (to pass the time during my Christmas break), and I'm stuck on Exercise II-13 of Eisenbud-Harris's ""The Geometry of Schemes"". Let $\mathcal{H}$ be the set of finite subschemes of degree $3$ of $\mathbb{A}_k^2$ which are supported at the origin.  In other words, $\mathcal{H}$ is the space of ideals in $k[x,y]/(x,y)^3$ whose underlying vector space is $3$-dimensional.  This vector space is a subspace of the $5$-dimensional vector space $V$ whose basis is $\{x,y,x^2,xy,y^2\}$, so $\mathcal{H}$ is identified with a subspace of the Grassmannian of $3$-dimensional subspaces of $V$. The exercise in question asserts that $\mathcal{H}$ is isomorphic to a $2$-dimensional cubic cone in $\mathbb{P}^3_k$ whose vertex is the ideal spanned by $\{x^2,xy,y^2\}$.  Can anyone help me prove this? EDIT: I've figured out more, but not enough.  Here's what I've got.  As above, let $V$ be the vector space with basis $\{x,y,x^2,xy,y^2\}$, so our ideals are $3$-dimensional subspaces of $V$.  Let $I_0$ be the subspace of $V$ spanned by $\{x^2,xy,y^2\}$, so $I_0 \in \mathcal{H}$.  Every $I \in \mathcal{H}$ such that $I \neq I_0$ can be written as $I_L$ via the following construction: Let $L$ be a line in $V$ that does not lie in $I_0$. Pick some nonzero $\vec{v} \in L$. Define $I_{L}$ to be the subspace spanned by $\{\vec{v},x \vec{v}, y \vec{v}\}$.  Here multiplication by $x$ and $y$ works in the obvious way where cubic terms are set to $0$. Now let $W \subset V$ be the span of $\{x,y\}$.  We get an injective map $f\colon \mathbb{P}(W) \rightarrow \mathcal{H}$ taking $L \in \mathbb{P}(W)$ to $I_L$.  What is more, for every $L \in \mathbb{P}(W)$ there is a subvariety $C_L \subset \mathcal{H}$ with the following properties: $C_L$ consists of $I_0$ together with all $I_{L'}$ such that $L'$ is a line that projects to $L$ under the natural projection $V \rightarrow W$. $C_L \cong \mathbb{P}^1$. Every two distinct $C_L$ intersect only at $I_0$. Every point of $\mathcal{H}$ is in some $C_L$. This all makes it look a lot line $\mathcal{H}$ is a cone on something resembling $\mathbb{P}^1$, but I can't quite get the description in the problem.  Can anyone help me?",['algebraic-geometry']
2577506,"Find, from the first principle, the derivative of $\sqrt {\sin (2x)}$","Find, from the first principle, the derivative of:
$$\sqrt {\sin (2x)}$$ My Attempt:
$$f(x)=\sqrt {\sin (2x)}$$
$$f(x+\Delta x)=\sqrt {\sin (2x+2\Delta x)}$$
Now,
$$f'(x)=\lim_{\Delta x\to 0} \dfrac {f(x+\Delta x)-f(x)}{\Delta x}$$
$$=\lim_{\Delta x\to 0} \dfrac {\sqrt {\sin (2x+2\Delta x)} - \sqrt {\sin (2x)}}{\Delta x}$$","['derivatives', 'calculus']"
2577516,Matrix representation of $R$ module homomorphism,We know that a linear transformation from a vector space to another is essentially a matrix. How about a $R$-module homomorphism from an $R$-module to another $R$-module? (let's assume the $R$-modules are finitely generated.) Can it be represented as a matrix with values in the ring $R$? (Suppose $R$ is commutative ring with 1.) Thanks.,"['abstract-algebra', 'modules']"
2577527,$\frac{d}{dX}[tr(-(CX(X^TCX)^{-1})(A+A^T)(X^TCX)^{-1})]=?$,"I want to obtain the derivative of the trace of the following statement with regard to $X$, where $A$, $C$, and $X$ are matrices and $C$ is symmetric. $$\frac{d}{dX}[tr(-(CX(X^TCX)^{-1})(A+A^T)(X^TCX)^{-1})]=?$$
where $\frac{d}{dX}(y)$is a matrix whose $(i,j)$ element is $\frac{dy}{dX}(i,j)$. I doubt that my calculation is correct or not, so I'm grateful for your help. Thanks a lot for any response.","['matrices', 'linear-algebra', 'derivatives']"
2577559,How to show an estimator is consistent and solve the asymptotic distribution?,"Question: Let $X_1,……,X_n$ be an i.i.d random sample from the distribution with PDF
$f(x;\theta)=\theta(1+\theta)x^{\theta-1}(1-x),0<x<1,\theta>0$ a)Find the method of moments estimator for $\theta$ based on the first moment of $X_i$ b)Show the estimator obtained in part(a) is consistent.Derive its asymptotic distribution."" Confusion: It's easy to calculate that the answer for part should be $\hat{\theta}=\frac{2\overline{X_n}}{1-\overline{X_n}}$,the question is part b. To prove consistent, we must prove$\frac{2\overline{X_n}}{1-\overline{X_n}}\xrightarrow[]{p}\theta$,since only ""$L_P$-Convergence"" and ""Almost Sure convergence"" can led to  ""Convergence in Probability"", but I've tried these two method, nothing. What's the correct method to solve this problem?","['statistics', 'statistical-inference']"
2577564,A special chess board,"Consider a special $n\times n$ chess board such that each $1\times 1$ square is ether black or white. We  know that every $1\times 1$ square has exactly one white adjacent square (two squares are adjacent, if they share exactly one side). For example the figure shows a possible coloring for $n=4$. Example: a) Prove that if $n$ is an odd number, then the coloring is not possible! b) Prove that if $n$ is an even number, then the coloring is possible! c) Find the number of white cells if $n$ is an even number! These problems are really beautiful, try them!","['coloring', 'discrete-mathematics']"
2577576,Two fair coins are tossed until both turn up heads,"A penny and a dime are tossed together until both turn up heads, after which no more tosses are made. Find the expected number of times the penny comes up heads. What I've tried: Let $X$ and $Y$ be the number of times the penny and dime come up heads, respectively. Then, for $x = 1,2,3,\ldots$ $$P(X = x) = \sum_{y=1}^\infty P(X = x, Y = y) \\ = \sum_{y=1}^x P(X=x,Y=y) + \sum_{y=x+1}^\infty P(X=x, Y=y) \\ = \sum_{y=1}^x \sum_{n=x}^\infty \left( \frac{1}{4} \right) \binom{n-1}{x-1} \left( \frac{1}{2} \right)^{n-1} \binom{n-1}{y-1} \left( \frac{1}{2} \right)^{n-1} + \\ \sum_{y=x+1}^\infty \sum_{n=y}^\infty \left( \frac{1}{4} \right) \binom{n-1}{x-1} \left( \frac{1}{2} \right)^{n-1} \binom{n-1}{y-1} \left( \frac{1}{2} \right)^{n-1} \\ = \sum_{y=1}^x \sum_{n=x}^\infty \left( \frac{1}{4} \right)^n \binom{n-1}{x-1} \binom{n-1}{y-1} + \sum_{y=x+1}^\infty \sum_{n=y}^\infty \left( \frac{1}{4} \right)^n \binom{n-1}{x-1} \binom{n-1}{y-1}$$ Is there a way to simplify the above expression? Or is there an easier approach that I'm missing?","['probability', 'probability-distributions']"
2577619,Does $f$ behave like $x^2$?,"Let $f \in C^1([0, \infty))$ be a nondecreasing function such that $\displaystyle \lim_{x \to \infty} \frac{f(x)}{x^{2+\epsilon}}=0 \ \forall \epsilon>0$ and $\displaystyle \lim_{x \to \infty} \frac{f(x)}{x^{2-\epsilon}}= \infty \ \forall \epsilon>0$. Then does the following hold? $\displaystyle \lim_{x \to \infty} \frac{f^{\prime}(x)}{x^{1+\epsilon}}=0 \ \forall \epsilon>0$ and $\displaystyle \lim_{x \to \infty} \frac{f^{\prime}(x)}{x^{1-\epsilon}}= \infty \ \forall \epsilon>0$. Any advise would be appreciated.","['functions', 'limits']"
2577634,Compute the Integral via Residue Theorem,"My goal is to compute $$I=\int_{0}^{+∞}\frac{\cos{ax}}{1+x^2}dx$$ where $a>0$. $$I=\frac{1}{2}\int_{-∞}^{+∞}\frac{\cos{ax}}{1+x^2}dx=\frac{1}{2}Re\bigg(\int_{-∞}^{+∞}\frac{e^{iax}}{1+x^2}dx\bigg)$$. Let $f(z)=\frac{e^{iaz}}{1+z^2}$. By Residue Theorem, $\int_{-R}^{R}\frac{e^{iax}}{1+x^2}dx+\int_{\gamma_R}\frac{e^{iaz}}{1+z^2}dz=2\pi Res(f,i)=\frac{e^{-a}}{2i}$, where $\gamma_R$ denotes the upper semi-circle centered at $O$ with radius $R$. As $R$—>$+∞$, $\int_{-R}^{R}\frac{e^{iax}}{1+x^2}dx$ —> $\int_{-∞}^{+∞}\frac{e^{iax}}{1+x^2}dx$ Now, I am stuck on how to prove $\int_{\gamma_R}\frac{e^{iaz}}{1+z^2}dz$ goes to $0$ as $R$ goes to infinity. Anyone know how to do it? Many thanks.","['complex-analysis', 'improper-integrals', 'residue-calculus', 'complex-integration']"
2577666,How to derive the sensitivity of the roots of a polynomial to its coefficients?,"I have a cubic polynomial $$f(x)=a_o x^3+a_1x^2+a_2x+a_0=(x-x_0)(x-x_1)(x-x_2)$$ but no closed form for its roots, i.e., I do not have closed form expressions for $x_1$, $x_2$ and $x_3$ in terms of the coefficients $a_0$ to $a_3$. However is there a means to derive the sensitivity of the roots with respect to the coefficients, i.e., $\frac{\partial x_1}{\partial a_1}$, etc?","['derivatives', 'polynomials']"
2577676,Almost surely finite stopping time for a random walk,"We consider a simple symmetric random walk $ (S_n)_{n \in \mathbb{N}} $ on $ \mathbb{Z} $ which starts at 1 : $ S_0 = 1 $ and there exists an iid sequence $ (X_n)_{n \geq 1} $ such that $ \mathbb{P}(X_1 = -1) = \mathbb{P}(X_1 = 1) = 1/2 $  and $ \forall n \in \mathbb{N} $ $  S_{n+1} = S_n + X_{n+1}  $ And we look at the stopping time $  T = inf  \{ n \geq 0, S_n = 0 \} $ Show that $ S_{ min(T,n) } $ converges almost surely towards a random variable X but that it does not converge in $ \mathbb{L}^1 $. What is the distribution of X? My first guess is that this convergence occurs iff   $ \: \: \mathbb{P} ( T < \infty ) = 1 $  but how do we compute this probability? We could try to find an upper bound for $ \: \: \mathbb{P} ( T = \infty )  $ ?  Borel-Cantelli doesn't seem to work here either. If that works then we could use Doob's convergence theorem. Thanks for any insight on this.","['random-walk', 'probability-theory', 'martingales', 'stopping-times']"
2577710,Abelian normal subgroup with a prescribed index. Problem 1.G.4 from M.Isaacs FGT,"I try to solve the problem with a number 1.G.4 from M.Isaacs book (one can see it here ). I will give this problem. Let $G$ be a nonabelian group and  $A\subset G$ be an abelian subgroup
  of $G$. Show that there exists a normal abelian subgroup $N$ of $G$
  such that $|G:N|<|G:A|^2$. It is an immediate consequence of the problem 1.G.1 (see here ) that  the Chermak-Delgado measure of $A$ is maximal and
$$
Z(G)\subset A,\  A=C_G(A),\ |G:A|^2 = |G:Z(G)|.
$$ Now I try to construct $N\vartriangleleft
G$ such that 
$$
Z(G)\subsetneq N\vartriangleleft
A.$$
I thought about $\mathrm{core}_G(A)$, but my attempts did not lead to success. Also there is a ""solution"" of this problem here . But there is incorrect argument there about the product of abelian groups: the product of (permutable but not necessary element-wise permutable) abelian groups  is abelian.","['finite-groups', 'normal-subgroups', 'abelian-groups', 'group-theory']"
2577711,Roots and divisibility for real polynomials of several variables,"For polynomials over the field $\mathbb K=\mathbb C$ the follwing theorem holds: Theorem. If $p(x_1,...,x_n), q(x_1,...,x_n)\in \mathbb K[x_1,...,x_n]$ are polynomials such that $p(x_1,...,x_n)$ is irreducible and for all $a_1,...,a_n \in K:$
  $$
p(a_1,...,a_n)=0 \Rightarrow q(a_1,...,a_n)=0, 
$$
  then $p(x_1,...,x_n) | q(x_1,...,x_n)$. This theorem is not true for polynomials over $\mathbb K=\mathbb R$. For example it is not true for $p(x_1,x_2)=x_1^2+x_2^2$, $q(x_1,x_2)=x_1$. Is a version of this theorem, with additional assumptions, for polynomials over $\mathbb R$? Is it  maybe true with additional assumption about $p$ that (*) ""there exists a $y=(y_1,...y_n)\in \mathbb R^n$ such that $p(y)=0,  grad f(y)\neq 0$"" sufficient? If not, is it true with additional assumptions (*) and (**) $p$ is of degree $2$ and $q$ of degree $\leq 2$ ? I'm mainly interested in the last case. Thanks","['abstract-algebra', 'polynomials', 'divisibility']"
2577734,"Prove that for any linear $f$, there is an $m$ such that $\text{Ker}(f^m)\cap\text{Range}(f^m)=\{0\}$","Let $V$ be a finite dimensional vector space. If $f$ is any linear transformation from $V$ to $V$. Prove that there is an integer $m$ such that the intersection of the image of $f^m$ and the kernel of $f^m$ is $\{0\}$. What I've tried: For any linear transformation $f$ we have $f(0)=0$. Thus $f\circ f\circ f\circ ....\circ f(0)=0$(m times) and hence $f^m(0)=0$. Thus $0 \in \ker(f^m) $. And since $f^m(0)=0$ , $0 \in Im(f^m) $. Then $0$ is in the intersection of $Im(f^m)$ and $\ker(f^m)$ Now for proving that their intersection is only $\{0\}$: I'm guessing that this implies that either $Im(f^m)=0$ or $\ker(f^m)=0$. For the first case, this would result in the zero map, but we can't get a zero map from multiple compositions of a non zero map (I think?). This would imply that $\ker(f^m)=0$ for some $m$, and the function would turn injective after $m$ compositions? Is my reasoning correct? Can anyone help me solve this or point me in the right direction? Thanks in advance!","['linear-algebra', 'linear-transformations', 'vector-spaces']"
2577758,"Find, from first principle, the derivative of:","Find, from first principle, the derivative of:
$$\log (ax+b)$$ My Attempt:
$$f(x)=\log (ax+b)$$
$$f(x+\Delta x)=\log (ax+a\Delta x+b)$$
Now,
$$f'(x)=\lim_{\Delta x\to 0} \dfrac {f(x+\Delta x)-f(x)}{\Delta x}$$
$$=\lim_{\Delta x\to 0} \dfrac {\log (ax+a\Delta x+b)-\log(ax+b)}{\Delta x}$$
$$=\lim_{\Delta x\to 0} \dfrac {\log (\dfrac {ax+a\Delta x+b}{ax+b})}{\Delta x}$$","['derivatives', 'calculus', 'limits']"
2577821,Probability of all 3 cards drawn being the same number,"Consider a scenario where Ace is considered as 1 (a number card) and the face cards (King, Queen, Jack) are not number cards. We have to find the probability of 3 cards drawn consecutively (without replacement) being the same number. My friend solved it using the following:
$$P = \frac{^4C_1\cdot ^{10}C_1\cdot ^3C_1\cdot ^2C_1 }{^{52}C_3} = \frac{12}{1105}$$
where we choose any one out of the 4 types, and then we choose a number card, and then we choose the same number card in the other types twice. I did the same problem like the following:
$$P = \frac{10}{13}\cdot \frac{3}{51}\cdot \frac{2}{50} = \frac{2}{1105}$$
where $\frac{10}{13}$ is the probability of choosing a number card first, $\frac{3}{51}$ is the probability of choosing the same number card out of the rest of 51 cards and $\frac{2}{50}$ is the probability of choosing the same number card out of the rest of 50 cards. Could you please explain as to why we are getting different answers and which one is correct (I guess it has to do something with $6$ multiplication error in either one of the answers arising from problems in permutations (as $6$ = $3!$) of the types of cards)?",['probability']
2577843,Prove that the series $\sum\limits_{n=-\infty}^{+\infty}f(x+n)$ converges absolutely for a.e. $x \in \mathbb{R}.$,"Problem: Let $f$ be a Lebesgue integrable function on $\mathbb{R}.$ Prove that
  the series
  $$\sum\limits_{n=-\infty}^{+\infty}f(x+n)$$
  converges absolutely for a.e. $x \in \mathbb{R}.$ What I have done: $\sum\limits_{n=-\infty}^{+\infty}f(x+n)$ converges absolutely for a.e. $x \in \mathbb{R}.$ This is true iff $\sum\limits_{n=-\infty}^{+\infty}\int\limits_{-\infty}^{+\infty}f(x+y)d\mu$ is finite where $\mu$ is lebesgue measure. This is also true iff, $\int\limits_{R}\int\limits_{-\infty}^{+\infty}f(x+y)d\mu d\nu$ is finite where $\nu$ is the counting measure. Iff by the Fubini, if $f$ is $-\mu\times \nu$ measurable. But I dont know here can I say that since $f$ is measurable and integrable then $F(x,y)=f(x+y)$ is $-\mu\times \nu$ meadurable. Comment: If this approach is not OK please let know. For the alternative way, please give me a hint.","['real-analysis', 'lebesgue-measure', 'absolute-convergence', 'lebesgue-integral', 'measure-theory']"
2577850,General formula for $\arctan$,"The question is to express 
$$
\arctan a_1+ \arctan a_2 + \dots + \arctan a_n
$$
as $\arctan(\cdot)$ I tried using the formula for the corresponding $\tan$ series but couldn't generalise to all cases.The principal branch is taken as $(-\pi/2,\pi/2)$. Any ideas?","['algebra-precalculus', 'trigonometry']"
2577869,Recurrence Relation involving Fractional Part,"I am considering the following sequence:
$$a_0=\sqrt 2$$
$$a_{n+1}=a_n+\{a_n\}$$
where $\{x\}:= x-\lfloor x\rfloor$ denotes the fractional part function.
Since I have observed that the sequence exhibits almost-linear growth, I am trying to find the value of the limit
$$\lim_{n\to\infty} \frac{a_n}{n}$$
This is by no means rigorous, but I believe that the value is $1/2$, because the $\{a_n\}$ seems to behave somewhat like a random variable, and if we instead considered the sequence
$$b_{n+1}=b_n+X_n$$
where each $X_n$ is a random variable uniformly distributed in $(0,1)$, the expected value of $\Delta b_n=X_n$ is $1/2$. Any ideas about how to prove this more rigorously? Is my reasoning even correct?","['recurrence-relations', 'fractional-part', 'sequences-and-series']"
2577898,Counting elements with prescribed trace and norm in finite field extension.,"Suppose $F$ is a finite field with $q$ elements, $l$ is a prime and $\tilde{F}$ is the degree $l$ extension of $F$. Given elements $a,b \in F$ with $b \neq 0$, how many elements in $\tilde{F}$ have $a$ as their trace and $b$ as their norm? If an explicit solution in terms of $a$ and $b$ cannot be given, can good lower and upper bounds depending on $l$ and $q$ be given? We know that $\tilde{F}$ has $q^{l-1}$ elements with any given trace (by the $F$-linearity of the trace map) and $\frac{q^l - 1}{q - 1}$ elements  with any given non-zero norm (the norm map is given by raising to the $\frac{q^l - 1}{q - 1}$-th power). In the case $l = 2$, an arbitrary element of norm b and trace a has characteristic polynomial $X^2 - aX + b$. If there exists an $x \in F$ with $b = x^2$ and $a = 2x$, then the polynomial is a square and $x$ is the only element with $b$ as norm and $a$ as trace. If the polynomial is irreducible over $F$, then it splits into two different linear factors over $\tilde{F}$, whereby there exist two elements with $b$ as norm and $a$ as trace. If the polynomial is reducible but not a square, then $a$ can not be the trace of an element with norm $b$.","['finite-fields', 'galois-theory', 'trace', 'extension-field', 'combinatorics']"
2577903,Definition of isomorphism from Herstein's book,"The above excerpt is from Herstein's book ""Topics in Algebra"". It confuses me by two reasons:
Firstly, in the first definition the mapping is into but in the second the mapping is onto . Why the author uses into and onto ? Could anyone clarify this distinction in definitions, please?","['abstract-algebra', 'group-theory', 'definition']"
2577915,Compute $e^{Ax}$ for the $2 \times 2$ matrix $A$,"Consider the real $2 \times 2$ matrix $A$  $$A=\begin{pmatrix} 1 & 0 \\ 0 & -1\end{pmatrix}$$ I want to compute $e^{Ax}$. I can see that $A^2=I$ which looks like it should be useful.
Using the definition of the matrix exponential $$e^{Ax}=I+Ax+\frac{(Ax)^2}{2!}+ \cdots$$ I find that $$e^{Ax}=I\left(1+\frac{x^2}{2!}+ \cdots\right)+A\left(x+\frac{x^3}{3!}+ \cdots\right)$$ Is there anything I can do from here?","['matrices', 'matrix-exponential']"
2577936,Showing that the integral with respect to an abstract measure is infinite [duplicate],"This question already has answers here : A set with a finite integral of measure zero? (2 answers) Closed 6 years ago . Suppose $\mu$ is a measure on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$. Then show that, for $\mu$-almost every $x$
$$
 \int_\mathbb{R} \frac{1}{(x-t)^2}\,\mathrm{d}\mu(t) = \infty
$$ I am not sure where to start. Any hints are welcome.","['real-analysis', 'integration', 'measure-theory', 'analysis']"
2577964,$X$ is basically disconnected if and only if every pair of disjoint open sets have disjoint closures?,"A space $X$ is said to be extremally disconnected if every open set has an open closure. A subset $S$ of a space $X$ is called a zero-set if there is a continuous function $f:X\to \mathbb{R}$ such that $S=f^{-1}(0)$ , and a subset $S'$ of $X$ is called a cozero-set if it is the complement of a zero-set. A space $X$ is basically disconnected  if every cozero-set has an open closure. Hence any extremally disconnected  space is basically disconnected. Are the following statements correct? The space $X$ is basically disconnected if and only if every pair of disjoint open sets have disjoint closures. The closure of every cozero-set is a zero-set.","['general-topology', 'topological-groups', 'connectedness']"
2577976,What is the cardinality of the union A∪B∪...∪Z?,"I have figured out that $|A \cup B| = |A| + |B| - |A \cap B| $ and that $|A \cup B \cup C| = |A| + |B| + |C| - |A \cap B| - |A \cap C| - |B \cap C| + |A \cap B \cap C|$. I have not managed to figure out what $|A \cup B \cup C \cup D|$ is equal to. What is the cardinality of $A \cup B \cup C \cup D$? Furthermore, how can one the generalized question: What is the cardinality of the union $A_1 \cup A_2 \cup ... \cup A_n$?","['inclusion-exclusion', 'combinatorics']"
2578005,Show that this polynomial has three different real roots,There is polynomial $f(x)=x^3 +ax^2+bx+c$ and $ab=9c$ and $b<0$. Like in the title. I've only come up so far with $9x^3 + 9ax^2 + 9bx+ab=0$ and factoring here does nothing.,"['algebra-precalculus', 'polynomials']"
2578076,"Let $f: \mathbb{R}^2\to \mathbb{R}^2$ given by $f(x, y) = (e^x \cos y, e^x \sin y)$.","Let $f: \mathbb{R}^2\to \mathbb{R}^2$ given by $f(x, y) = (e^x \cos y, e^x \sin y)$. Take $S$ to be the set $S = [0, 1]\times [0, \pi]$. (a) Calculate $Df$ and $\det Df$. (b) Sketch the image under $f$ of the set $S$. We remark that if one identifies $\mathbb{C}$ with $\mathbb{R}^2$ as usual, then $f$ is the function $f(z) = e^z$. For (a), $Df(x,y)=\begin{bmatrix}e^x \cos y & -e^x \sin y\\e^x \sin y  & e^x \cos y\end{bmatrix}$ and $\det \begin{bmatrix}e^x \cos y & -e^x \sin y\\e^x \sin y  & e^x \cos y\end{bmatrix}=e^{2x}\cos^2 y+e^{2x}\sin^2 y=e^{2x}$ I do not understand what I have to do in (b), could someone help me please? Thank you","['real-analysis', 'calculus', 'multivariable-calculus', 'analysis', 'vector-analysis']"
2578104,What is the standard approach to showing that two implication statements are equivalent?,"Suppose there are two statements: $P⇒Q$ and $A⇒B$ If I wanted to show that these two are equivalent, is it enough to assume $P$ and show that it implies $B$ or do I need to make use of $Q$ and $A%$ as well?","['propositional-calculus', 'proof-writing', 'logic', 'discrete-mathematics']"
2578134,Help me prove: directional derivative in terms of partial derivative,"Given a function $f(x, y)$ which has partial derivatives $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$, and whose partial derivatives are also differentiable, and    $\vec r = (x_r, y_r)$, I am trying to prove that the derivative of $f$ along $\vec r$ is: $Df_r (x,y) = \lim_{h\to 0} \dfrac{f(x+hx_r, y+hy_r)-f(x,y)}{h} = x_r \dfrac{\partial f}{\partial x} + y_r \dfrac{\partial f}{\partial y} = D$ I am trying to prove this by showing that for any given $\epsilon$, I can find a $\delta$ such that for all $h < \delta$, the actual rate-of-change is within $\epsilon$ of $D$. We can write $f(x+hx_r, y+hy_r)-f(x,y) = A + B$, where $A = f(x+hx_r, y)-f(x,y)$ and $B = f(x+hx_r, y+hy_r)-f(x+hx_r,y)$. I wish to make $|A + B| \le |A| + |B| \le (D + \epsilon)h$ ...[1]. Since $\frac{\partial f}{\partial x}$ exists, I can pick an $h$ to bring |A| within a factor of any $\epsilon_1$ to $x_r\frac{\partial f}{\partial x} h$. I can make the $h$ smaller, if necesssary, to bring $\frac{\partial f}{\partial y}(x+hx_r, y)$ within $\epsilon_2$ of $\frac{\partial f}{\partial y}(x, y)$. I then wish to bring $|B|$ sufficiently close to $y_r \frac{\partial f}{\partial y} h$, so that [1] is achieved. I am getting stuck at the last step. Because $\frac{\partial f}{\partial y}$ exists everywhere, for all $\epsilon_3$ there exists an $\delta_3$ such that for all $h_3 < \delta_3$, $|f(x+hx_r, y+h_3 y_r) - f(x+hx_r, y)| \le (y_r \frac{\partial f}{\partial y} h_r + \epsilon_3)h_3$. In my situation, however, $h_3$ is not a free variable, but fixed: $h_3 = h$. No matter how small I make my $h$, $\delta_3$ might be too small.","['multivariable-calculus', 'partial-derivative', 'calculus']"
2578144,"In linear regression, why is the hat matrix idempotent, symmetric, and p.s.d.?","In linear regression,
   $$y = X \beta + \epsilon$$
where $y$ is a $n \times 1$ vector of observations for the response variable, $X = (x_{1}^{T}, ..., x_{n}^{T}), x_{i} \in \mathbb{R}^p. i =1,...,n$ is a data matrix of $p$ explanatory variables, and $\epsilon$ is a vector of errors. Further, assume that $\mathbb{E}[\epsilon_i] = 0$ and $var(\epsilon_i) = \sigma^2, i=1,...n$ The least-squares estimate,
$$\hat{\beta} = (X^{T}X)^{-1}X^{T}y$$ The least-squares estimators are the fitted values,
$$\hat{y} = X \hat{\beta} = X(X^{T}X)^{-1}X^{T}y = X C^{-1}X^{T}y = Py$$ $P$ is a projection matrix. It is has the following properties: idempotent, meaning P*P = P symmetric positive semi-definite For property 1, what's the intuition behind this? How can you take some matrix do transformation, inverse and multiplication, then, you get idempotent. It's an important concept. But, it's hard to follow through the math to get an intuition. Why will we get property 2 and property 3, How am I supposed to think about this?","['regression', 'statistics', 'linear-regression', 'linear-algebra']"
2578152,Pullback and symmetric product,"Let $(X, \mathcal{O}_X), (Y, \mathcal{O}_Y)$ are ringed spaces. Let $\mathscr{F}$ is $\mathcal{O}_Y$-module and $f: X \to Y$ is morphism of ringed spaces. I want to prove that symmetric power commuting with pullbacks. I already prove that pullbacks commuting with tensor powers. So, my plan is the following, I consider exact triple
$$0 \to \mathscr{I} \to T^n \mathscr{F} \to S^n \mathscr{F} \to 0$$
and want to prove that triple
$$0 \to f^* \mathscr{I} \to f^* T^n \mathscr{F} \to f^*S^n \mathscr{F} \to 0$$
are exact too. But $f^*$ apriori only right-exact and I don't see any reason why it is left exact in this particular case. Hope for your help!","['sheaf-theory', 'algebraic-geometry']"
2578164,The centroid of the Morley triangle of an isosceles triangle is collinear with the foci of the Steiner inellipse,"I have a little question about geometry found by a friend: Let $\triangle ABC$ be isosceles. Let $M$ be the (equilateral) Morley triangle determined by the trisectors of the angles of $\triangle ABC$, and let $E$ be the Steiner inellipse tangent to the edges of $\triangle ABC$ at their midpoints. Prove that centroid of $M$ is collinear with the foci of $E$. I don't have any idea to prove this. Thanks a lot.","['euclidean-geometry', 'trigonometry', 'geometry']"
2578255,"Finding the derivative of $f(x)=\int_1^{\infty}\frac{e^{-xy}}{y^2}dy,\:\:\:x\in(0,\infty)$","Let 
  $$f(x)=\int_1^{\infty}\frac{e^{-xy}}{y^2}dy,\:\:\:x\in(0,\infty)$$
  Show that $f(x)$ is differentiable on $(0,\infty)$, find the formula for $f'(x)$? Is $f(x)$ twice differentiable? I'm thinking to define a sequence as follow
$$f_n(x)=\int_1^n\frac{e^{-xy}}{y^2}dy.$$ To show $f_n(x)$ is differentiable, I'm thinking to show the following limit exists, $$\lim_{h\to 0}\int_1^n\frac{e^{-(x+h)y}-e^{-xy}}{y^2h}.$$ To be able to pass the limit inside the integral, we can apply the Lebesgue dominated convergence theorem. So I want to see if  I can apply it. Since $\frac{e^{-(x+h)y}-e^{-xy}}{y^2h}$ is bounded by $\frac{e^{-uy}}{y}$, (where $x\leq u\leq x+h$) which is integrable on $[1,\infty).$ Hence $f'_n(x)=\int_1^n\frac{d}{dx}(\frac{e^{-xy}}{y^2})dy=\int_1^n-\frac{e^{-xy}}{y}dy. $ Now, $\lim_{n\to\infty}\int_1^n\frac{-e^{-xy}}{y}dy=\int_{1}^{\infty}\frac{-e^{-xy}}{y}dy.$ However I see a problem here, since in fact, we have 
$$f'(x)=\lim_{h\to 0}\lim_{n\to{\infty}}\int_1^n\frac{e^{-(x+h)y}-e^{-xy}}{y^2h}.$$ But I'm not sure, if I'm allowed to interchange these two limits. I appreciate any hint or alternative proof.","['derivatives', 'real-analysis', 'integration', 'lebesgue-integral', 'analysis']"
2578290,How would you prove this idea?,"Is it true that if the slope of the secant line between A and B is less than the slope of the secant line between B and C, then the slope of the secant line between A and B is less than the slope of A to C? Is this true and if so, how would you prove it? Edit: A, B, and C are all points on a function Edit2: A possible diagram for the question Edit3: Sorry, I realized I was missing information thanks to the comment and answer. The x values of A, B, and C are in increasing order i.e. A's x < B's x < C's x.",['calculus']
2578301,Limit $\lim_{x\to\infty}\left(\sum_{n=1}^{\infty}\left(\frac{x}{n}\right)^n \right)^{1/x}$,"Evaluate $$\lim_{x\to\infty}\left(\sum_{n=1}^{\infty}\left(\frac{x}{n}\right)^n \right)^{1/x}$$ My attempt: I tried to expand the internal series by substituting the consecutive values of n, but the series that appears is not known to me. Kindly help me to solve the problem.","['sequences-and-series', 'limits']"
2578326,Bases for symmetric polynomials,"I’ve been playing with symmetric polynomials, as one does, and I’ve run into something that must be familiar, but I can’t find anything about it. To present the idea, I’ll work with symmetric polynomials in $\Bbb Z[x,y]$, but the idea extends to any number of variables, with the details just getting a little more difficult to write down. We have the elementary symmetric polynomials in two variables: $\sigma_1=x+y$ and $\sigma_2=xy$. Now, we pick a degree, say $4$. If we want to express any degree $4$ symmetric polynomials in $x$ and $y$, there are two bases that seem natural to use: $\rho_1=\sigma_1^4\\
\rho_2=\sigma_1^2\sigma_2\\
\rho_3=\sigma_2^2$ and $\tau_1=x^4+y^4\\
\tau_2=x^3y+xy^3\\
\tau_3=x^2y^2$ These bases are related by the equation: $\left[\begin{matrix} 1&4&6\\0&1&2\\0&0&1\end{matrix}\right]\left[\begin{matrix} \tau_1\\ \tau_2\\ \tau_3\end{matrix}\right] = \left[\begin{matrix} \rho_1\\ \rho_2\\ \rho_3\end{matrix}\right]$ The entries in this matrix are binomial coefficients, if we were working in 3 variables, we’d be looking at trinomial coefficients instead, etc. My question is: what am I looking at? I assume this has been thoroughly studied. Are these bases, and the transformations between them, called something? Are the transformations special in some way? They all have characteristic polynomials of the form $(1-\lambda)^k$ for some $k$, being triangular with $1$s on the diagonal. Is there more that I’m not seeing? Is there a quicker way to write the transformations down than just expanding each $\rho_i$? Thanks in advance for any insight or information anyone can provide.","['combinatorics', 'linear-transformations', 'symmetric-polynomials']"
2578345,"Is ""open subfunctor"" transitive?","In topological spaces if $U \subseteq V \subseteq X$ with $U$ open in $V$ and $V$ open in $X$ then $U$ open in $X$.  My question is, is this true if we are instead talking about open subfunctors? Recall that in the functorial approach to algebraic geometry we look at functors from the category of commutative rings to the category of sets.  If $A$ is a ring and $\mathfrak a \leq A$ an ideal then we define
$$\mathrm{Spec} \ A = \hom(A, -)$$
$$D(\mathfrak a) = \{f \in \hom(A, -) \ | \ f(\mathfrak a) \ \text{generates the unit ideal}\}$$
and then a subfunctor $U \subseteq X$ is open if for all maps of the form $\phi\colon\mathrm{Spec} \ A \to X$ we have $\phi^{-1}(U) = D(\mathfrak a)$ for some ideal $\mathfrak a$. One can easily prove that the open subfunctors of $\mathrm{Spec} \ A$ are exactly the subfunctors of the form $D(\mathfrak a)$ for some ideal $\mathfrak a$.  I think the above statement is equivalent to the statement that the open subfunctors of $D(\mathfrak a)$ are all of the form $D(\mathfrak b)$ as well so it suffices to just prove the affine case, but even there I'm not sure how to proceed. A lot of what I've read about the functorial approach seems to take this fact for granted, by taking an affine open cover $\{V_i\}$ of a scheme $X$ and then working with open subsets $U \subseteq V_i$ as if they are open in $X$.  So it seems like this really should be true, cause I doubt all the literature in the field is broken, and I figured the proof should be easy but I'm stuck...","['category-theory', 'functors', 'algebraic-geometry']"
2578401,FOAG exercise 7.4.D.,"suppose $\pi:X\rightarrow \text{Spec}\ k$ is a quasifinite morphism. Show that $\pi$ is finite. (hint: deal first with the case where $X=\text{Spec}\ A,$ $A$ finitely generated over $k$ . If $A$ contains an element $x$ not algebraic over $k$ , we have a inclusion $k[x]\hookrightarrow A.$ ) In fact, I finished the exercise but used the Nullstellensatz, Chinese remainder theorem and the fact that $A$ is a Jacobson ring with finitely many prime ideals to show: 1. $\text{Spec}\ A\rightarrow \text{Spec}\ k$ is finite. This section is about Chevalley theorem, I want to know how to use this theorem directly to prove 1. Thanks a lot.",['algebraic-geometry']
2578422,Find the value of $\sum \frac{1}{\beta+\gamma}$.,"If $\alpha,\beta,\gamma$ are the roots of the cubic equation $px^3+3qx^2+3rx+s=0$ then find the value of $\sum \dfrac{1}{\beta+\gamma}$. TRY : We have $\sum \alpha=\dfrac{-3q}{p},\sum \alpha\beta =\dfrac{3r}{p},\alpha\beta\gamma=\dfrac{s}{p}.$ Now $\sum \dfrac{1}{\beta+\gamma}=\dfrac{1}{\alpha+\beta}+\dfrac{1}{\beta+\gamma}+\dfrac{1}{\gamma+\alpha}$. But how to proceed from here?
Please help.","['polynomials', 'roots', 'algebra-precalculus', 'fractions', 'discrete-mathematics']"
2578476,Trying to show that the orthogonal group is a manifold: why does this approach fail?,"I'm trying to get a better feeling about differential geometry and I was trying to prove that $G = \{ x \in M_2(\mathbb{R}) : xx^t = I\}$ has a manifold structure.  My approach was to set $X = M_2(\mathbb{R})$ and consider the smooth map $f: X \rightarrow X, f(x) = xx^t - I$ .  The regular value theorem says that if the tangent space map $T_x(f): T_x(X) \rightarrow T_0(X)$ is surjective for all $x \in f^{-1}\{0\} = G$ , then $G$ is a submanifold of $X$ .  This is what I wanted to show in order to prove that $G$ has a manifold structure. Identifying $X$ with $\mathbb{R}^4$ via $\begin{pmatrix} a & b \\ c & d \end{pmatrix} = (a,b,c,d)$ , and taking the basis of the tangent space at a point $p$ to be $\frac{d}{dx_1}|_p, ... , \frac{d}{dx_4}|_p$ , the matrix of the tangent map at any $p \in f^{-1}\{0\}$ is just the Jacobian evaluated at $p$ .  Checking that $$f(x_1,x_2,x_3,x_4) = (x_1^2+x_2^2 -1, x_1x_3 + x_2x_4, x_1x_3 + x_2x_4, x_3^2 + x_4^2 - 1)$$ we see that the Jacobian of $f$ is $$\begin{pmatrix} 2x_1 & 2x_2 & 0 & 0 \\ x_3 & x_4 & x_1 & x_2 \\ x_3 & x_4 & x_1 &x_2 \\ 0 & 0 & 2x_1 & 2x_4 \end{pmatrix}$$ which is never invertible.  Hence the tangent space map is never surjective.  So this approach fails. On the other hand, this blog uses a similar approach with the regular value theorem, except it actually works.  It notably uses the space of symmetric matrices as the codomain, rather than the space of all matrices. My question : If I were more experienced in differential geometry, how can I know that my approach above is destined to fail?  How would I recognize that I need to shrink my codomain to a manifold of smaller dimension?  Does this have something to do with the fact that the orthogonal group itself has smaller dimension?","['differential-geometry', 'lie-groups']"
2578482,Trigonometric equation: $\ln(\sin x + \cos x)^{1+\sin 2x}= 2$,$\ln(\sin x + \cos x)^{1+\sin 2x}= 2$ I am unable to solve it. I tried this way: $(\sin x + \cos x)^{1+\sin 2x}= e^2$ I know that: $\sin x + \cos x \le \sqrt2 $ $1+ \sin 2x \le 2 $ I don't know how to utilise this idea in my solution.,['trigonometry']
2578492,Under which conditions $\mathcal{L}^{S}(E)=\mathcal{L}(E)$?,"Let $E$ be a complex Hilbert space, $\mathcal{L}(E)$ be the algebra of all bounded linear operators on $E$. For $S\in \mathcal{L}(E)^+$, we set $$\mathcal{L}^{S}(E)=\left\{A\in \mathcal{L}(E);\;\exists \,M> 0;\;\|S^{1/2}Ay\| \leq M\|S^{1/2}y\|,\;\forall\,y\in E\right\}.$$
If $SA=AS$ and $S^{1/2}$ is surjective then $\mathcal{L}^{S}(E)=\mathcal{L}(E)$. I want to get a more weaker conditions under which $\mathcal{L}^{S}(E)=\mathcal{L}(E)$. Do you think if $S$ is injective operator with a closed range, the above equality holds? Thank you!","['functional-analysis', 'operator-algebras', 'operator-theory', 'hilbert-spaces']"
2578524,What kinds of groups are there where every (nontrivial) element has prime order?,"What kinds of finite groups satisfy the condition that the orders of all their (nontrivial) elements are prime?
Is there a way to classify such groups? Such groups definitely exist ($\mathbb{Z}^{n}_p$ (all its elements have prime order $p$ and this class of groups describes all abelian groups satisfying that condition (due to structure theorem for finite abelian groups)) and they can even be non-abelian (Discrete Heisenberg Group $H_{3}(\mathbb{Z}_p)$ (all all its elements have prime order $p$)). However, I do not know any other examples. Moreover, I failed even to make out, if the orders of each element of the group should be equal, or there is a group, satisfying both that condition and the condition, that there exist two its elements with distinct prime orders.
All I know, is that if such a group exists, it has to be non-abelian (as any finite abelian group with two elements of distinct prime orders $p$ and $q$ has an element of order $pq$ and $pq$ is not prime) Any help will be appreciated.","['finite-groups', 'abstract-algebra', 'prime-numbers', 'group-theory']"
2578558,Why isn't a continuous bijection from a locally compact space to a Hausdorff space an homeomorphism?,"I know that if $f : X \rightarrow Y$ is a continuous bijection from a compact space $X$ to a Hausdorff space $Y$, then $f$ is an homeomorphism. So I was thinking that if we relax the assumption $X$ compact to $X$ locally compact, it should be true as well. Using the above result, $f$ is an homeomorphism if we restrict it to a compact neighborhood of $X$. Since we can find a compact neighborhood around every point of $X$, $f$ should be a local homeomorphism. But a bijective local homeomorphism is a global homeomorphism, so that would be it. Yet, if I'm not mistaken, the map $f : [0, 2\pi[ \rightarrow S^1, f(\theta) = e^{i\theta}$ is a counterexample. What is wrong with my reasoning ?",['general-topology']
2578569,Prove that $\lim_{n \to \infty}\bigg(1+\frac{\alpha}{f(n)}\bigg)^{f(n)}=e^\alpha$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question how to prove that $$\lim_{n \to \infty}\bigg(1+\frac{\alpha}{f(n)}\bigg)^{f(n)}=e^\alpha$$ where $f(n)$ is any function, such  $$\lim_{n \to \infty}f(n)=\infty$$","['constants', 'limits']"
2578575,2nd solution of $\cos x \cos 2x\cos 3x= \frac 1 4 $,"$\cos x \cos 2x\cos 3x= \dfrac 1 4 $ Attempt explained: $(2\cos x \cos 3x)\cos 2x = \frac1 2 $ $(\cos 4x +\cos 2x )\cos 2x = \frac 1 2 \\\cos ^2y + \cos y (2\cos^2y- 1)= \frac1 2 \\ $ (Let, y = 2x) $\implies 4\cos^3 y+2\cos^2y- 2\cos y-1=0$ I solved this equation using Rational Root Theorem and got $y = \frac 1 2$ $\implies x= m\pi \pm \dfrac\pi3 \forall x\in \mathbb {Z}$ Using Remainder theorem, the other solution is $\cos^2 y = \dfrac 1 2 $ $\implies x = \dfrac n2\pi \pm\dfrac \pi 8 $ But answer key states: $ x= m\pi \pm \dfrac\pi3or x =(2n+1)\dfrac \pi 8$ Why don't I get the second solution correct?","['substitution', 'trigonometry', 'sequences-and-series']"
2578582,Analytic proof of $\sqrt{n}$ irrationality or integrality,"Basic arithmetics (uniqueness of prime factorization) tells us that, for $n\in \mathbb N$, either $\alpha:=\sqrt{n}$ is an integer or is an irrational. While designing undergraduate exams, I wondered if there was some elementary, calculus-based proof of that fact. First I tried an approach along the lines of Niven's proof of the irrationality of $\pi$. The proof studies the family of polynomials $P_k(x):=\frac{1}{k!}(qx(p-qx))^k$ for $p,q\in \mathbb N$. Assuming $\pi=\frac{p}{q}$ leads to a contradiction, since the sequence of integrals $\int_0^{p/q}P_k(t)\sin(t)~dt$ is both positive, integral and tending to $0$. But this works out nicely because of the differential relations satisfied by sin (that come in an essential way in integrating by parts). This argument works as well for any function with nice, linear differential relations with integers coefficients ( e.g. exp). Unfortunately, this is not so easy with, say, $x^2-2$. Or I missed something obvious. In the same spirit I tried to study various sequences of integrals related to $x^2 -2$, but to no avail. Or, again, I missed something obvious :) Another way I explored is to study $f(x):=x^\alpha$ from the point of view of functional equations. Assuming $\alpha=\frac{p}{q}$ is rational, we both have $$\cases{f\circ f(x)=x^n\\qf'(x)=pf(x)}$$ If an $f$ satisifies these constraints then, using Faà Di'Bruno formula, we can painstakingly prove the claim by observing that after differentiating $k$ times we have a relationship where $f^{(k)}$ appears alone only if $k$ is a square. One can arrange to prove that this condition is exactly the sought one: if $\alpha^2=n$ then this must be read at the $k=n$-th step. Although the latter idea works well, it is way from being elementary! (In particular, the exam talked about something else entirely ;) ) So I ask the question here: does anybody know a more elementary analytic proof?","['number-theory', 'calculus']"
2578592,Isomorphism between tensor products (base extension),"Let $A,B$ be two $\Bbb Q$-algebras. Assume that 
$A \otimes_{\Bbb Q} \Bbb C \cong B \otimes_{\Bbb Q} \Bbb C$ as $\Bbb C$-algebras. 
Does it follows that $A \otimes_{\Bbb Q} \overline{\Bbb Q} \cong B \otimes_{\Bbb Q} \overline{\Bbb Q}$ as $\overline{\Bbb Q}$-algebras ? Here $\overline{\Bbb Q}$ is an algebraic closure of $\Bbb Q$. (Clearly the converse holds, since $- \otimes_{\overline{\Bbb Q}} \Bbb C$ is a functor, so it preserves isomorphisms). My feeling is that this is true if $A,B$ are finitely generated as $\Bbb Q$-algebras. We could even replace $\Bbb Q$ by any (perfect) field $k$ and considering two algebraically closed fields $K \subset K'$ containing $k$. We would have $A = k[X_1,...,X_r]/I,B=k[X_1,...,X_s]/J$ with a $K'$-algebras isomorphism $\phi : K'[X_1,...,X_r]/I^e \to K'[X_1,...,X_s]/J^e$ ($(\cdot)^e$ denotes the extension of ideals). Then I would like that $\phi$ is actually defined over a finite extension $L \supset k$, so in particular $L \subset K$ and $\phi$ is defined over $K$ and provides an isomorphism $A \otimes_k K \cong B \otimes_k K$ of $K$-algebras. Looking at this question, I think that this is true if we replace algebras by modules . This is wrong in larger generality. For instance, let $A = \Bbb Q[x,y] / (y^2-x^2) \cong \Bbb Q \times \Bbb Q$ and $B=\Bbb Q[x,y] / (y^2-2x^2)$.
Then $A \otimes_{\Bbb Q} \Bbb Q(\sqrt 2) \cong  B \otimes_{\Bbb Q} \Bbb Q(\sqrt 2) \cong \Bbb Q(\sqrt 2) \times \Bbb Q(\sqrt 2)$ as $\Bbb Q(\sqrt 2)$-algebras, but $A \not\cong B$ as $\Bbb Q$-algebras (not even as rings, since $B$ is a domain but not $A$). So we need to make base extensions by algebraically closed fields.","['abstract-algebra', 'extension-field', 'tensor-products', 'commutative-algebra']"
2578593,"$\int f(x)\,\mathrm{d}x = \left(\int_0^x f(t) \, \mathrm{d}t\right) + C$","If $f(x)$ is a continuous function on $\mathbb{R}$ and I am asked to find 
$\int f(x) \, dx$, what is the problem with the following answer: $$\int f(x)\,\mathrm{d}x = \left(\int_0^x f(t) \, \mathrm{d}t\right) + C$$",['integration']
2578600,Number of solutions of $\sin x= \frac x {10}$,"The easiest way to do this is plot graphs of $10 \sin x$ and $y=x$ to find number of intersection points. However, is there a non graphical solution to this problem? Edit: My problem is different from the linked problem because the answers to that focus only on graphical solutions.","['algebra-precalculus', 'trigonometry']"
2578612,Relationship between $f(x)$ and $xf'(x)$,"Is the following statement true? Suppose we are given $f(x)$ defined and differentiable $\forall x \in \mathbb{R}$, $x\neq 0$. If $xf'(x)>0$, then $f$ has no minimum value. The statement seems reasonable, because if I choose $f(x) = -1/x^2$, then its derivative is $1/x^3$ so $xf'(x) = 1/x^2$ that is $>0$, for all $x\ne0$. And $-1/x^2$  clearly has no minimum value (its lower bound is $-\infty$). My problem is that I have no idea how I should continue going from this example to a general proof. Taken from a question of a calculus 1 exemption of 2011.","['derivatives', 'calculus']"
2578626,Evaluate $\int \frac{a^2\sin^2 x+b^2\cos^2 x}{a^4\sin^2 x+b^4\cos^2 x}dx$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Evaluate $$\int \frac{a^2\sin^2 x+b^2\cos^2 x}{a^4\sin^2 x+b^4\cos^2 x}\text dx$$ I would have given my attempt to this question but honestly, I think my attempts to solve this did nothing but only complicated it further. Any hints or suggestions are welcome.Please verify that your method gives answer:$$\frac 1{a^2+b^2}(x+\tan^{-1} ({\frac {a^2\tan x}{b^2}}))+C$$","['indefinite-integrals', 'integration']"
2578644,Why the factor $e^{g(z)}$ in the Weierstrass factorization theorem?,"The Weierstrass factorization theorem is usually stated as such (quote from Wikipedia): Let $f$ be an entire function, and let $\{a_n\}$ be the non-zero zeros of $f$ repeated according to multiplicity; suppose also that $f$ has a zero at $z = 0$ of order $m ≥ 0$ (...). Then there exists an entire function $g$ and a sequence of integers $\{p_{n}\}$ such that $$f(z)=z^me^{g(z)}\prod_{n=1}^\infty E_{p_n}\left(\frac{z}{a_n}\right).$$ It seems to me that this combines two different theorems: that $f$ can be written as
$$f(z)=z^mh(z)\prod_{n=1}^\infty E_{p_n}\left(\frac{z}{a_n}\right)$$
for some entire function $h$ with no zeros, and that any entire function $h$ with no zeros can be written as $h(z)=e^{g(z)}$ for some entire function $g$. The latter is an interesting nontrivial statement in its own right, and, as far as I can tell, its proof is not connected with the rest of the proof of the Weierstrass factorization theorem. So, what is the mathematical or historical reason for writing the theorem with $e^{g(z)}$ instead of $h(z)$?",['complex-analysis']
2578654,An identity for the product of two finite sum,"I found this formula in this Wikipedia page :
 $$\left(\sum_{k=0}^{n}a_{k}\right)\left(\sum_{k=0}^{n}b_{k}\right)=\sum_{k=0}^{2n}\sum_{i=0}^{k}a_{i}b_{k-i}-\sum_{k=0}^{n-1}\left(a_{k}\sum_{i=n+1}^{2n-k}b_{i}+b_{k}\sum_{i=n+1}^{2n-k}a_{i}\right)$$ but there is no reference. Is there someone who knows a reference in some book? And if not, how can we prove it? I tried to write the terms as a $2n\times2n$ matrix (since the ""convolution term"" are all in the diagonal) but I didn't see how to find it.","['algebra-precalculus', 'reference-request', 'summation', 'calculus']"
2578692,Why can we replace a variable with a constant in a limit?,"We say $\lim_{x \to c} f(x) = L$ that means $f(x)$ may be as close to $L$ as $x$ tends to $c$. "" Tends "" here means $x$ approaches $c$ but never actually becomes $c$. If it so, then why do we such easily replace $x$ with value $c$, whenever it is appropriate. E.g. $\lim_{x \to 5} 4 + x = 4 + 5 = 9$, or $\lim_{h \to 0} f(x+h) = f(x)$. Understand me right. I don't want to discuss cases when we can do the substitution and sometimes not, because we get a division by zero, and we need to do some simplification, etc. I understand all this. I just can't understand if $x$ actually is never $c$, what allows me to write $c$ as a value of $x$? Well, I used to think about it like ""ah, as $x \to 0$, x is very small number, let it be zero"". But it is not statistics, you know, to close eyes and make approximations. I met the notion of ""infinitesimal"" and as I understood it is opposed to ""$\delta-\epsilon$"" approach. I can't fully understand how they are related to each other and to my question. Maybe I lack of historical context. If it so, please clarify this for me. Thanks.","['calculus', 'limits']"
2578733,product of a module by an ideal,"I have a ring $R$, an ideal $I$ of $R$ and an $R$-module $M$. How does one define $IM$. Is it the set $\{im\ \vert\ i\in I, m\in M\}$ or is it the smallest module containing these elements?","['abstract-algebra', 'modules', 'definition']"
2578737,"Is there an intrinsic or natural topology on a field, motivating the definition of the Zariski topology","Let $k$ be a field. The usual motivation for the Zariski topology on affine space $\mathbb{A}^n(k)$ is that it is the coarsest topology for which the algebraic sets, the zero loci of polynomials, are closed. This can be phrased in more topological or categorical terms: we may characterize the Zariski topology on $\mathbb{A}^n$ as the initial topology with respect to all regular maps into $k$ , if we endow $k$ with the topology where the only nontrivial closed set is $0$ . Call this topology $\tau,$ so $\tau =\{\varnothing,k^\times,k\}.$ There are other topologies $k$ could carry for which the Zariski topology is the initial topology for regular maps in to $k$ , including for example the Zariski topology on $\mathbb{A}^1\cong k.$ But for the purposes of parsimony, and for non-circularity in motivating the definition of the Zariski topology, I prefer to use $\tau$ , which is the coarsest topology on $k$ with this property. Does this topological space $(k,\tau)$ occur in the literature, or have a name? Is there a natural or intrinsic algebraic justification for this topology (whatever that might mean)? It seems like an algebraic analogue of Sierpiński space , in that it classifies open sets in the regular category. The only intrinsic topology that I know for an arbitrary ring is the $I$ -adic topology. But the only ideal of a field is the zero ideal, and the $I$ -adic topology for the zero ideal gives the discrete topology. So $\tau$ is not the $I$ -adic topology. I do not see any way to view $\tau$ as the Zariski topology on $k$ , which, if it existed, should be the zero loci of the constant polynomials, hence the trivial topology. Actually that's not correct, that's not the Zariski topology on $k$ , the Zariski topology is properly assigned to $\mathbb{A}^0=\text{pt}$ , not to $k$ , which is instead its coordinate ring. Anyway, $\tau$ is not the trivial topology. We might identify $k$ with $\mathbb{A}^1,$ but the Zariski topology on $\mathbb{A}^1$ is usually the cofinite topology, which has $\lvert k\rvert$ many closed points, whereas $(k,\tau)$ has only one. So $\tau$ is not the Zariski topology either. I'm hoping to provide an intrinsic motivation for this topology $\tau$ on $k$ , to use in turn to motivate the definition of the Zariski topology from earlier principles, so even if we could view it as the Zariski (which, again, I don't see how we can), I'm hoping to hear a different justification.","['algebraic-geometry', 'zariski-topology', 'ring-theory', 'commutative-algebra', 'field-theory']"
2578760,If $u\in H^1(\Omega )$ then $|u|\in H^1(\Omega )$.,"Let $\Omega $ a smooth domain and $u\in H^1(\Omega )$. Prove that $$|u|\in H^1(\Omega ).$$ The proof goes as following :
We define $$v_\varepsilon=\sqrt{\varepsilon+u^2}.$$
Then by the chain rule $$\nabla v_\varepsilon=\frac{u\nabla u}{\sqrt{\varepsilon+u^2}},$$
 and $$v_\varepsilon\to |u|\quad \text{and}\quad \nabla v_\varepsilon=\frac{u\nabla u}{\sqrt{\varepsilon+v^2}}\to \frac{u}{|u|}\nabla u$$
in $L^2(\Omega )$. The claim follow. Questions 1) $\Omega $ is not supposed bounded. So why $\sqrt{\varepsilon+u^2}\in L^2(\Omega )$ ? Isn't it wrong ? To me $\int_\Omega v_\varepsilon^2=\int_\Omega (\varepsilon+u^2)$ and there is no reason for it to be finite. 2) If $v_\varepsilon$ is indeed in $L^2(\Omega )$ then why $$v_\varepsilon\to |u|\quad \text{and}\quad \nabla v_\varepsilon=\frac{u\nabla u}{\sqrt{\varepsilon+v^2}}\to \frac{u}{|u|}\nabla u$$
in $L^2(\Omega )$ give us $|u|\in H^1(\Omega )$ ?","['real-analysis', 'sobolev-spaces']"
2578771,What does the inner product of two complex general vectors have to do with complex conjugation?,"$
\newcommand{\qr}[1]{|#1 \rangle} 
\newcommand{\ql}[1]{\langle #1|} 
\newcommand{\q}[2]{\langle #1 | #2 \rangle}
\newcommand{\v}[2]{\langle #1,#2\rangle}
$ I'm reading ""Quantum computer science, an introduction,"" N. David Mermin, Cambridge University Press, 2007.  In Appendix A, he writes that ""in a vector space over the complex numbers the inner product of two general vectors is a complex number satisfying $\q{\psi}{\phi} = \q{\psi}{\phi}^*$, where $*$ denotes complex conjugation."" (Page 160.) I must not have understood this.  I tried the following example.  What's the inner product of $\v{2+3i}{7+3i} \cdot \v{1+2i}{2+2i}$?  It's $4 + 27i$.  If I commute $\v{1+2i}{2+2i} \cdot \v{2+3i}{7+3i}$, I get the same $4 + 27i$ because inner products are commutative.  Taking the complex conjugate $4 + 27i$, I get $4 - 27i$ which is not the same as its complex conjugate, $4 + 27i$. So I'm pretty lost here.  What does he mean by what he said?","['linear-algebra', 'complex-numbers', 'vector-spaces']"
2578783,Determine an analytic function over the disk,"I'm trying to solve this question: Let $f$ be analytic in a connected open neighbourhood of the closed  unit disk. Assume that $|f(z)|=|z+1|$ on the unit circle $|z|=1$, that $f(1)=2$, and that $f$ has simple zeros at $\pm i/2$ and no other zeros in the disk $|z|<1$. Show that these properties determine $f$ uniquely. Can someone give me any hint? My attempt was to use something like the Blaschke product decomposition bt $f$ doesn't necessarily map the disk to itself. I tried skimming through some books but I haven't found anything relevant to solve this question.",['complex-analysis']
2578797,How to compute the determinant of this Toeplitz matrix?,"Given a positive integer $n$, express$$
f_n(x) = \left|\begin{array}{c c c c c} 
1 & x & \cdots & x^{n - 1} & x^n\\
x & 1 & x & \cdots & x^{n - 1} \\
\vdots & x & \ddots & \ddots & \vdots\\
x^{n - 1} & \vdots & \ddots & 1 & x\\
x^n & x^{n - 1} & \cdots & x & 1
\end{array}\right|
$$
  as a polynomial of $x$. I tried to find a recurrence relation of $\{f_n\}_{n \geqslant 1}$ using Laplace expansion, but there seems to be no patterns in the minors in the expansion. Is there a somewhat simple recurrence relation of $\{f_n\}_{n \geqslant 1}$ or these determinants can be computed with other methods?","['matrices', 'toeplitz-matrices', 'determinant', 'linear-algebra', 'laplace-expansion']"
2578833,$\sin^2 x \cos^2 x + \sin x \cos x -1 = 0$,"$\sin^2 x \cos^2 x +  \sin x \cos x -1 = 0$ Attempt: $\sin^2 2x + 2\sin 2x -4 = 0 \\ \implies \sin 2x= \sqrt 5 - 1$ Now, using the formula $\sin 2 x = \dfrac {2\tan x}{1+\tan^2x}$, I couldn't express the answer in terms of $\tan()$ The answer given is in terms of tan. How do I express it in that way?",['trigonometry']
2578858,On the behaviour of $\left(1+\frac{1}{n+1}\right)^{n+1}-\left(1+\frac{1}{n}\right)^n$,I have to find the limit : (let $k\in \mathbb{R}$) $$\lim_{n\to \infty}n^k \left(\Big(1+\frac{1}{n+1}\Big)^{n+1}-\Big(1+\frac{1}{n}\Big)^n \right)=?$$ My Try : $$\lim_{n\to \infty}\frac{n^k}{\Big(1+\frac{1}{n}\Big)^n} \left(\frac{\Big(1+\frac{1}{n+1}\Big)^{n+1}}{\Big(1+\frac{1}{n}\Big)^n}-1\right)$$ we know that : $$\frac{\Big(1+\frac{1}{n+1}\Big)^{n+1}}{\Big(1+\frac{1}{n}\Big)^n}>1$$ now what do i do ?,"['sequences-and-series', 'limits']"
2578888,Xmas Maths 2017: The value of a $\color{red}m^{\color{green}{ince}\ \color{orange}{\pi}}$ and $\color{red}\pi^{\color{green}{ie}}$,"In the spirit of the festive season! (i) Evaluate $$\large \color{red}m^{\color{green}i\color{orange}n\color{red}c\color{green}e\;\color{red}\pi}$$ given that $m=12\;\;\text{(month when mince pies are consumed)}\\
n=5\;\;\text{(number of points of the star on a mince pie)}\\
c=2.997\times 10^{8}s^{-1}\; \text{(rate at which mince pies are consumed!)}\\
$ (ii) It is well known that $e^{i\pi}=-1$, but what is the value of $$\large\color{red}\pi^{\color{green}{ie}}$$ ? Merry Christmas!","['complex-analysis', 'recreational-mathematics']"
2578903,Lagrange multipliers tangency,"I'm at the start of a section on Lagrange multipliers and the graphical/intuitive explanation is that one can increase/decrease $c$ until the level curves of some function in 2 vars is tangent to the constraint curve. But what happens when we are already on the min/max level curve and it isn't tangent to the constraint curve? [EDIT] Thx everyone for helping out. My mistake was that I didn't notice that $\nabla f=\vec0$ when we are on the max level curve. So whether there is tangency or not the equation:
$$\nabla f=\lambda\nabla g$$
is still true.","['multivariable-calculus', 'lagrange-multiplier']"
2578910,$x^3 + 2x^2 + 5x + 2\cos x = 0$,"$x^3 + 2x^2 + 5x + 2\cos x = 0$ How do I find the number of solutions of  this equation (in $[0, 2\pi]$) without a graph ? Attempt: The equation simplifies to $x(x^2 + 2x + 5)=- 2\cos x $ Minima of the quadratic occurs at $x= -1$ and it's value is $4$ Minima of $-2\cos x$ is $-2$",['trigonometry']
2578923,Why is the integral of $\frac1{x^2}$ from $1$ to $\infty$ not the same as the infinite sum from $1$ to $\infty$?,"Studying series I am a bit confused on this point. The infinite sum of $1/x^2$ from $1$ to $\infty$ was proved by Euler to be $\pi^2$ divided by $6$: $$\sum_{x=1}^\infty\frac 1 {x^2}=\frac {\pi^2} 6$$ But if I integrate from $1$ to $\infty$ of the same entity namely $1/x^2$ it is $1$. Correct..? Unless I did it wrong. 
$$\int_1^\infty\frac 1 {x^2}dx=1$$
How can this be since by integrating it seems we are adding a lot more numbers to cover the same area so we should by all means get the same thing or something at least as large as $\pi^2/6$ ?","['improper-integrals', 'integration', 'sequences-and-series', 'calculus']"
2578976,Area of projected parallelogram onto a plane.,"Say you have a parallelogram which is defined by the to vectors: $\vec u$, $\vec v$. Prove that the area of its projection on a plane with a perpendicular vector $\vec n$  (where $|\vec n|=1$) is: $E=|(\vec u \times \vec v)\ \vec n|$. Now I know that the area of the original parallelogram is: $|\vec u \times \vec v|$, but i can't relate this with the other area, or find it from scratch.","['analytic-geometry', 'geometry', '3d', 'linear-algebra', 'vectors']"
2578987,Understanding when it's OK to use limits arithmetic of multiplying,"I'm having hard time understanding when I can do the following move:
$\lim_{n\to \infty}a_n\cdot b_n =  \lim_{n\to \infty}a_n\cdot\lim_{n\to \infty}b_n $ On one hand, my teacher said that I can do that only if prove that both limits of $a_n$ and $b_n$ are exist and finite. On the other hand, When I calculated the limit of the series $a_n=n*cos1/n$ using this site https://www.symbolab.com which is pretty reliable, I saw this steps: $\lim_{n\to \infty}n\cos(1/n)=\lim_{n\to \infty}n\cdot \lim_{n\to \infty}cos(1/n)=""\infty\cdot 1""=\infty$ So how this move is possible if the limit of $a_n=n$ is not finite?",['limits']
