question_id,title,body,tags
3714525,"Difference between the ""functions"" in calculus and the ""functions"" in Linear Transformations","The word function in calculus refers to something like $f(x) = x^2+2x^3$ or $f(x) =\sin(x) $ etc.... In linear algebra, the word function is used like- A linear transformation is a function from $V \rightarrow W$ . And the functions of calculus like $f(x) = x^2+2x^3$ or $f(x) =\sin(x) $ etc.  are actually vectors in  either a polynomial space ( $f(x) = x^2+2x^3$ ) or a Function space ( like $f(x) =\sin(x) $ ) . Now the word function in Linear Algebra is used twice as I showed above. So according to me the functions of calculus are just vectors in linear algebra. Is this correct or not. But then what are the functions being used in the definition of Linear Transformations. And how are they different from the functions of calculus and the functions which are vectors in linear algebra. Edit: Why is the graph of a linear transformation from any vector space to any other vector space not always a straight line. Can anyone give any counter examples.","['graphing-functions', 'calculus', 'functions', 'linear-algebra', 'linear-transformations']"
3714573,Counting certain paths,"Suppose we wish to count the number (say $P(n)$ ) of lattice paths from $(0,0)$ to $(n,n)$ where at each step  one can make a move $1$ unit to the right or up, with the condition that every point $(k_1,k_2)$ obeys $k_2\geq\left\lceil\dfrac{k_1^2}{n}\right\rceil$ , that is, lies inside the parabola passing through $(0,0)$ , $(n,n)$ and $(-n,n)$ . How does one enumerate these, other than of course bashing with brute force code? Also, is there a good way (other than simulation) to estimate $\dfrac{P(n)}{\binom{2n}{n}}$ , or maybe $\displaystyle\lim_{n\to\infty}\dfrac{P(n)}{\binom{2n}{n}}$ ? IE the probability that a lattice path from $(0,0)$ to $(n,n)$ lies in the parabola. Is there a nice way to estimate this? Any help is appreciated!","['combinatorics', 'probability']"
3714592,Convergence of Series of Independent Poisson Random Variables,"Let $\{X_r\}_{r\ge1}$ be independent Poisson random variables with respective parameters $\{\lambda_r\}_{r\ge1}$ . Show that $\sum_\limits{r\ge1} X_r$ converges or diverges almost surely according as $\sum_\limits{r\ge1} \lambda_r$ converges or diverges. I know this question has already been asked and has an answer outlined here but I have a question on the solution and given that the post is so old, I thought I would make a new post with a partial solution then pose my question. Since Poisson random variables take values on the set $\{0,1,2,3,..\}$ we have that: \begin{align}
\sum_\limits{r\ge1} X_r < \infty \ \text{a.s} 
&\iff \mathbb{P}(X_r=0\ \ \text{eventually})=1\\
&\iff \mathbb{P}(X_r>0\ \ \text{i.o.})=0\\
&\iff \sum_\limits{r\ge1} \mathbb{P}(X_r>0)<\infty \quad \text{by Borell-Cantelli Lemmas}
\end{align} Now note that: \begin{align}
\sum_\limits{r\ge1} \mathbb{P}(X_r>0)=\sum_\limits{r\ge1} (1- \mathbb{P}(X_r=0))=\sum_\limits{r\ge1} (1-e^{-\lambda_r})\le\sum_\limits{r\ge1} \lambda_r
\end{align} Thus, $\sum_\limits{r\ge1} \lambda_r < \infty \implies \sum_\limits{r\ge1}\mathbb{P}(X_r > 0)<\infty \implies \sum_\limits{r\ge1} X_r < \infty \ \text{a.s}$ , and half the result is proved. Now, again since Poisson random variables take values on the set $\{0,1,2,3,..\}$ we have that: \begin{align}
\sum_\limits{r\ge1} X_r = \infty \ \text{a.s} 
&\iff \mathbb{P}(X_r=0\ \ \text{eventually})=0\\
&\iff \mathbb{P}(X_r>0\ \ \text{i.o.})=1\\
&\iff \sum_\limits{r\ge1} \mathbb{P}(X_r>0)= \infty \quad \text{by Borell-Cantelli Lemmas}
\end{align} Now note that: \begin{align}
\sum_\limits{r\ge1} \mathbb{P}(X_r>0)=\sum_\limits{r\ge1} (1-e^{-\lambda_r}) \ge \sum_\limits{r\ge1} \Bigl(\frac{\lambda_r}{\lambda_r +1}\Bigr)
\end{align} Now we want to claim that: $\sum_\limits{r\ge1} \lambda_r = \infty \implies \sum_\limits{r\ge1}\mathbb{P}(X_r > 0) = \infty \implies \sum_\limits{r\ge1} X_r = \infty \ \text{a.s}$ , completing the proof. The only issue here is that this last claim is a little less obvious, so my question is does \begin{align}
\sum_\limits{r\ge1} \lambda_r = \infty \implies \sum_\limits{r\ge1} \Bigl(\frac{\lambda_r}{\lambda_r +1}\Bigr)= \infty?
\end{align} If so, is this an easy thing to see and I am just missing it or does it require a bit of unpacking? Or is there a better way to show the second half of the result?","['borel-cantelli-lemmas', 'convergence-divergence', 'probability-theory', 'probability']"
3714645,"Integrating $\sin(x)x^2$ by parts, why do we only add $C$ at the end?","Let $f(x)=\sin(x)x^2$ .
If you were to do integration by parts you would get: $$\int \sin(x)x^2dx=\int\sin(x)dx\times x^2-\iint\sin(x)dx\times\frac{d}{dx}x^2dx$$ $$\int \sin(x)x^2dx=-\cos(x)x^2-\int-2\cos(x)xdx$$ $$\int\sin(x)x^2dx=-\cos(x)x^2+2(\int\cos(x)dx\times x-\iint\cos(x)dx\times\frac{d}{dx}xdx)$$ $$\int\sin(x)x^2dx=-\cos(x)x^2+2(\sin(x)x+\cos(x))+C$$ The part that I don't understand is why do we add the constant only at the end? For example, in row # $1$ you need to find the $\int \sin(x)dx$ .  We take it to be $-\cos(x)$ but in reality, it is $-\cos(x)+C_1$ . If you were to compute this with $\int \sin(x)dx=-\cos(x)+C_1$ , you would get the wrong result because $\int \sin(x)dx$ needs to be integrated again, and $C_1$ would turn in $x$ , therefore you would get the wrong result. How do we know that $C_1=0$ ? I get that the solution will be correct that way, but why?","['integration', 'indefinite-integrals', 'calculus']"
3714682,limit related to the Lambert function,"I am trying to evaluate the following limit $$
L=\lim_{x \rightarrow 0^+}\frac{2 \operatorname{W}\left( -{{ e}^{-x-1}}\right)  \left( {{\operatorname{W}\left( -{{e}^{-x-1}}\right) }^{2}}+2 \operatorname{W}\left( -{{ e}^{-x-1}}\right) -2 x+1\right) }{{{\left( \operatorname{W}\left( -{{ e}^{-x-1}}\right) +1\right) }^{3}}}$$ where $W(z)$ is the principal branch of Lambert's function.
The numerical experiments show that it is $\sqrt{2}$ but the l'Hopital's rule does not produce anything useful. Here is the numerical experiment computed with Maxima: $L(x) - \sqrt{2}$","['lambert-w', 'limits']"
3714691,"If $X_i$ has supremum for any $i=1,...,n$ then $\sup\bigcup X_i=\max\{\sup X_i\}$.","Statement If $X$ is a totally ordered set and if $\mathfrak{X}=\{X_i\subseteq X:i=1,...,n\}$ is a finite subcollection of not empty subset of $X$ with supremum then $\bigcup\mathfrak{X}$ is limited above and $\sup\{\bigcup\mathfrak{X}\}=\max\{\sup X_i\}$ . Unfortunately I can't prove the statement so I ask to do it. So could someone help me, please?","['elementary-set-theory', 'order-theory']"
3714732,Brownian motion and stopping times (an exercise in LeGall),"I am a beginner in Brownian motion and not a pro of probability theory and I would like to check my solution of Exercise 2.26 of the book by LeGall, stating: Let $B$ be a Brownian motion, for each $a\ge0$ set $T_a=\inf\{t\ge0: B_t=a\}$ . Show that for $0\le a\le b$ the random variable $T_b - T_a$ is independent of $\sigma(T_c, 0\le c\le a)$ and it has the same distribution as $T_{b-a}$ . My idea is to use the strong Markov property of BM and to look at $B_t^{(T_a)} = 1_{T_a<\infty}(B_{T_a + t} - B_{T_a})$ , the BM ""rebooted"" at time $T_a$ . The strong Markov property states that this is again a BM and that it is independent of $$\mathscr{F}_{T_a} = \{A\in\mathscr{F}_\infty:\forall t\ge0,\ A\cap\{T_a\le t\}\in\mathscr{F}_t\}\ ,$$ where $\mathscr{F}_t$ is our filtration. Now the $\sigma$ -algebra $\sigma(T_c, 0\le c\le a)$ is generated by $\{T_c\le s\}\in\mathscr{F}_\infty$ for $0\le c\le a$ and $s\ge 0$ . We notice that since $c\le a$ and the sample paths of the BM are continuous, if $T_c>t$ then we need have $T_a>t$ . Therefore, it follows that $$\{T_c\le s\}\cap\{T_a\le t\} = \{T_c\le s\wedge t\}\cap\{T_a\le t\}$$ and since both sets are in $\mathscr{F}_t$ , so is their intersection. It follows that $\{T_c\le s\}\in\mathscr{F}_{T_a}$ , and thus that $\sigma(T_c, 0\le c\le a)\subseteq\mathscr{F}_{T_a}$ . In particular, we have obtained that $B_t^{(T_a)}$ is independent of $\sigma(T_c, 0\le c\le a)$ . Now we can conclude. Denote by $T_d^{(T_a)} = \inf\{t\ge0: B_t^{(T_a)}=a\}$ . For $A\in\sigma(T_c, 0\le c\le a)$ we have \begin{align}
P(T_b - T_a\mid A) ={}&P(T_{b-a}^{(T_a)} - T_0^{(T_a)}\mid A)\\
={}&P(T_{b-a}^{(T_a)} - T_0^{(T_a)})
\end{align} where the first line is quite obvious from the definitions, and in the second line we used the fact that the ""rebooted"" stopping times are defined only in terms of the ""rebooted"" BM, which is independent of $A$ . Now on the one hand the last line equals $P(T_b - T_a)$ by the same reasoning we already made in the first line of the above. On the other hand, we have that $T_0^{(T_a)} = 0$ a.s. and that $T_{b-a}^{(T_a)}$ has the same distribution as $T_{b-a}$ since the ""rebooted"" BM is again a BM. I think this fully proves the statement we wanted. Did I miss anything or was I imprecise at any step? Any help or comment is greatly appreciated. Thanks in advance!","['brownian-motion', 'probability-theory']"
3714777,Hodge Star Warped Product Metric,"Say M is an n-dimensional product manifold of the form $\mathbb{R}_{+}$ $\times$ N where N is an n-1 dimensional manifold and the metric on M is of the form g $_{M}$ =dr $^{2}$ +r $^{2}$ g $_{N}$ . I can write an arbitrary k-form $\omega$ on M as $\omega$ =dr $\wedge$ $\alpha$ + $\beta$ , where $\alpha$ and $\beta$ are forms on N potentially parameterized by r. My question is how can I relate the hodge star of $\omega$ with respect to the warped product metric to the hodge stars of the forms $\alpha$ and $\beta$ with respect to the metric g $_{N}$ ? I have been attempting to do this in coordinates but this has been a huge mess and I don't feel such an approach should be necessary since such a result would be independant of the coordinates on N anyway.","['differential-forms', 'differential-geometry']"
3714794,"If $|A|=pq$ and $A\lhd B$ is a Nontrivial Normal Subgroup, the Quotient Group $A/B$ is Cyclic","I want to prove that $A/B$ is cyclic when $B$ is non-trivial normal subgroup of $A$ , and $|A|$ = $pq$ such that $p$ and $q$ are primes. I had the idea of using Lagrange's Theorem which states that 'the order of any subgroup of group $A$ is a divisor of the order of $A$ ', along with the idea that $A$ must contain an element ' $a$ ' of order $q$ ; then prove that $A/B$ is cyclic since the ' $a$ ' of order $q$ could be the left side generator of the set of (left) cosets of $B$ in $A$ (thus using the trivial normal subgroup $B$ to form a cyclic quotient group $A/B$ )... but I am coming up empty when trying to prove this directly...","['number-theory', 'cyclic-groups', 'quotient-group', 'abstract-algebra', 'group-theory']"
3714831,inverse function of $f(x)=x^{x^x}$,"I have easily found the inverse of $f(x)=x^x$ using the following: $$y=x^x\Rightarrow \ln(y)=x\ln(x)\Rightarrow W(\ln(y))=\ln(x)\therefore x=e^{W(\ln(y))}$$ However I am struggling to do the same for $f(x)=x^{x^x}$ , this is what I have tried: $$x=y^{y^y}$$ $$\ln(x)=y^y\ln(y)$$ and if: $u=y^y$ then: $$\frac{ue^u}{e^{W(ln(u))}}=\ln(x)$$ now by letting $\ln(u)=ve^v$ we get: $$e^{ve^v}v=\ln(x)\Rightarrow \ln(v)+ve^v=\ln(\ln(x))$$ However I cannot see an easy way of getting from here or even if this is the correct way to approach it, as I have tried several methods and they have all failed The closest progress I have made is if i define a function $g(x)=\ln(x)+xe^x$ then $G(x)=g^{-1}(x)$ then I get: $$y=\exp\left(W\left(G(\ln(\ln(x)))+\ln(G(\ln(\ln(x)))\right)\right)$$ However this is a fairly ugly expression and I still have no way of defining $G(x)$","['lambert-w', 'algebra-precalculus', 'inverse-function']"
3714864,Is an infinitesimally small portion of a surface essentially a 2-D area? (Surface Integrals),"I'm trying to derive the equation for surface flux $$\iint_{S}\vec F\cdot \hat n \;dS $$ So far, I understand that if we consider the vector field going through smooth surface. Then the flux on a small piece of the surface, or surface element, $\Delta S$ , is  given by the contribution of $\vec F$ in the direction of the unit normal $\hat n$ , times the piece $\Delta S$ $$\vec F\cdot \hat n \Delta s$$ Stewart's Calculus then suggests that this portion $\Delta S$ is essentially $\Delta A$ , a 2-D area. My question is if an infinitesimally small portion of a surface is essentially a 2-D area somewhat like a plane? Thanks.","['integration', 'multivariable-calculus', 'calculus']"
3714942,Showing $\{X_n\}$ is uniformly integrable when $\sup _{n} \mathbb{E}\left[X_{n}^{2}\right]<\infty$,"I got a question that show that a family of rvs $\left\{X_{n}\right\}$ is uniformly integrable when $\sup _{n} \mathbb{E}\left[X_{n}^{2}\right]<\infty$ What I have tried: $$\sup_n\mathbb{E}[|X|] = \sup_n\{\mathbb{E}[|X|\cdot 1_E] + \mathbb{E}[|X| \cdot 1_{E^c}] \}\leq \sup_n\{\mathbb{E}[1 \cdot1_E] + \mathbb{E}[X^2 \cdot 1_{E^c}] \}\leq\sup_n\{1+\mathbb{E}[X^2]\} < \infty.$$ However, it seems like that it $\sup_n\mathbb{E}[|X|]< \infty$ cannot imply uniformly integrable. How can I prove $\lim _{\alpha \rightarrow \infty} \sup _{n} \mathbf{E}\left(\left|X_{n}\right| \mathbf{1}_{\left|X_{n}\right| \geq \alpha}\right)=0$ ? Thanks","['measure-theory', 'lebesgue-measure', 'uniform-integrability', 'lebesgue-integral', 'probability-theory']"
3714995,"volume of solid generated by the regin bounded by curve $y=\sqrt{x},y=\frac{x-3}{2},y=0$ about $x$ axis","Using sell method to find the volume of solid generated by revolving the region bounded by $$y=\sqrt{x},y=\frac{x-3}{2},y=0$$ about $x$ axis, is (using shell method) What I try: Solving two given curves $$\sqrt{x}=\frac{x-3}{2}\Longrightarrow x^2-10x+9=0$$ We have $x=1$ (Invalid) and $x=9$ (Valid). Put $x=9$ in $y=\sqrt{x}$ we have $y=3$ Now Volume of solid form by rotation about $x$ axis is $$=\int^{9}_{0}2\pi y\bigg(y^2-2y-3\bigg)dy$$ Is my Volume Integral is right? If not then how do I solve it? Help me please.","['multivariable-calculus', 'calculus', 'volume']"
3715025,"Is every predictable process a pointwise limit of left-continuous, adapted processes?","Define the predictable $\sigma$ -algebra as $$
\mathcal P := \sigma(X: \text{ $X$ is a left-continuous and adapted process }),
$$ and say that a stochastic process is predictable if  it is measurable w.r.t. $\mathcal P$ . [The stochastic processes are assumed to take values in some Polish space $E$ and be indexed by $[0,\infty)$ ] As measurablility is preserved under pointwise limits, a pointwise limit of left-continuous and adapted processes will be predictable. Is it true that any predictable process is a pointwise limit (or even a.s. pointwise limit if we assume our probability space is complete) of a sequence of left-continuous and adapted processes.? More generally, is it always true that if we define $\mathcal F := \sigma(f: \text{ $ f $ with some property })$ , then every $\mathcal F $ -measurable function is the limit of a sequence of functions with that same property?","['stochastic-processes', 'measure-theory']"
3715033,Determining that a certain diffeomorphism of $\Bbb R^n-\{0\}$ is orientation preserving or not,"Consider the diffeomorphism $f:\Bbb R^n-\{0\} \to \Bbb R^n-\{0\}$ (whose inverse is itself) given by $x\mapsto x/|x|^2$ . How can we determine that $f$ is orientation preserving? For $n=1$ it is clearly orientation reversing, and also for $n=2$ , but it seems not easy to compute its Jacobian determinant for large $n$ , so I think there should be another method. Can I get a hint?","['orientation', 'smooth-functions', 'smooth-manifolds', 'differential-geometry']"
3715035,How are joint probability distributions constructed from product measures?,"I often see a construction in measure theory in regards to product measures. This is outlined below (taken from Wikipedia because it's very generic) Let $ (X_{1},\Sigma _{1})$ and $(X_{2},\Sigma _{2})$ be two measurable
  spaces, that is, $\Sigma _{1} $ and $\Sigma _{2}$ are sigma algebras
  on $X_{1}$ and $X_{2}$ respectively, and let $\mu _{1} $ and $\mu _{2}$ be measures on these spaces. Denote by $\Sigma _{1}\otimes \Sigma_{2}$ the sigma algebra > on the Cartesian product $X_{1}\times X_{2} $ generated by subsets of the form $B_{1}\times B_{2}$ , where $B_{1}\in
> \Sigma _{1}$ and $B_{2}\in \Sigma _{2}.$ This sigma algebra is called
  the tensor-product σ-algebra on the product space. A product measure $\mu _{1}\times \mu _{2}$ is defined to be a measure on the measurable
  space $(X_{1}\times X_{2},\Sigma _{1}\otimes \Sigma _{2})$ satisfying
  the property $(\mu _{1}\times \mu _{2})(B_{1}\times B_{2})=\mu
> _{1}(B_{1})\mu _{2}(B_{2})$ for all $B_{1}\in \Sigma _{1},\ B_{2}\in \Sigma _{2}.$ Questions: From the perspective of probability theory, this product measure construction looks  a lot like the construction of a joint probability $P(X,Y)$ where the $X$ and $Y$ are independent. Is this assumption correct? If (1.) is correct, then how does the notion of correlation come into the structure of product measures? How is correlation built into measure theory so that it can pass onto probability theory?","['statistics', 'probability-theory', 'measure-theory', 'products']"
3715062,Dihedral group as a semidirect product?,"It is known that the dihedral group $D_{2n}$ is isomorphic to the semidirect product $Z_n\rtimes Z_2$ , where both $Z_n,Z_2$ are cyclic. My question is, for a semidirect prouct, the two subgroups should have trivial intersection. But, if $n$ be even, then how can we get trivial intersection? Namely, an order $2$ element  would be common to both $Z_n,Z_2$ right? And the order two elements in a dihedral group would be either a $180$ degree rotation about an axis or a reflection about an axis, which are both identical right? Any hints? Thanks beforehand.","['semidirect-product', 'group-theory', 'abstract-algebra', 'dihedral-groups']"
3715139,Hypothesis testing with an exponential distribution,"I have the following problem: Given the data $X_1, X_2, \ldots, X_{15}$ which we consider as a sample from a distribution with a probability density of $\exp(-(x-\theta))$ for $x\ge\theta$ . We test the $H_0: \theta=0$ against the $H_1: \theta>0$ . As test statistic $T$ we take $T = \min\{x_1, x_2, \ldots, x_{15}\}$ . Big values for $T$ indicate the $H_1$ . Assume the observed value of $T$ equals $t=0.1$ . What is the p-value of this test? Hint: If $X_1, X_2,\ldots,X_n$ is a sample from an $\operatorname{Exp}(\lambda)$ distribution, than $\min\{X_1,  X_2,\ldots,X_n\}$ has an $\operatorname{Exp}(n\lambda)$ distribution. The solution says 0.22. I know that the first question you have to ask youself regarding the p-value is: ""What is the probability that the H0 would generate a sample θ>0?"" So I assume H0 is true and take θ = 0. The probability-density function becomes: f(x) = Exp(-x). I take up the hint, so I make it f(x) = Exp(-nx) This is where I get stuck. I don't know how to proceed with the information given: Assume the observed value of T equals t=0.1. Can I have feedback on this problem? Thanks,
Ter","['statistics', 'probability-distributions', 'exponential-distribution', 'hypothesis-testing', 'probability']"
3715141,Quadratic Euler sums $\sum _{n=1}^{\infty } \frac{(-1)^{n-1} \widetilde H(n)^3}{2 n+1}$,"I'm computing a class of harmonic sums. Denote $H(n)=\sum_1^n \frac{1}{k}, \widetilde H(n)=\sum_1^n \frac{(-1)^{k-1}}{k}$ Harmonic numbers, then how to prove $\small\sum _{n=1}^{\infty } \frac{(-1)^{n-1} \widetilde H(n)^2 H(n)}{2 n+1}=\frac{\pi ^2 C}{12}-40 \Im(\text{Li}_4(1+i))+\frac{11}{12} \pi  \log ^3(2)+\frac{11}{16} \pi ^3 \log (2)+\frac{5 \psi ^{(3)}\left(\frac{1}{4}\right)}{256}-\frac{5 \psi ^{(3)}\left(\frac{3}{4}\right)}{256}$ $\small \sum _{n=1}^{\infty } \frac{(-1)^{n-1}\widetilde H(n) H(n)^2}{2 n+1}=\frac{\pi ^2 C}{6}+64 \Im(\text{Li}_4(1+i))-\frac{1}{3} 5 \pi  \log ^3(2)-\frac{9}{8} \pi ^3 \log (2)-\frac{\psi ^{(3)}\left(\frac{1}{4}\right)}{32}+\frac{\psi ^{(3)}\left(\frac{3}{4}\right)}{32}$ $\small \sum _{n=1}^{\infty } \frac{(-1)^{n-1} \widetilde H(n)^3}{2 n+1}=\frac{\pi ^2 C}{6}+24 \Im(\text{Li}_4(1+i))-\frac{1}{2} \pi  \log ^3(2)-\frac{3}{8} \pi ^3 \log (2)-\frac{5 \psi ^{(3)}\left(\frac{1}{4}\right)}{384}+\frac{5 \psi ^{(3)}\left(\frac{3}{4}\right)}{384}$ I'd like you to help me establish any of the 3 identities. Any help will be appreciated! Appendix: To 'guess' the closed-form, use S = {Pi^3 Log[2], Pi Log[2]^3, Pi^2*Catalan, Catalan*Log[2]^2, 
Pi*Zeta[3], Log[2] Im[PolyLog[3, 1 + I]], Im[PolyLog[4, 1 + I]], 
PolyGamma[3, 1/4] - PolyGamma[3, 3/4]};
S0 = {0, Pi^3 Log[2], Pi Log[2]^3, Pi^2*Catalan, Catalan*Log[2]^2, 
Pi*Zeta[3], Log[2] Im[PolyLog[3, 1 + I]], Im[PolyLog[4, 1 + I]], 
PolyGamma[3, 1/4] - PolyGamma[3, 3/4]};
TS = FindIntegerNullVector[
N[{'Numeric value of the sum', 
Pi^3 Log[2], Pi Log[2]^3, Pi^2*Catalan, Catalan*Log[2]^2, 
Pi*Zeta[3], Log[2] Im[PolyLog[3, 1 + I]], Im[PolyLog[4, 1 + I]], 
PolyGamma[3, 1/4] - PolyGamma[3, 3/4]}, 50]];
Expand[TS.S0/(-TS[[1]])] Update: See arXiv $2007.03957$ for @pisco's solution on these series, evaluating a large class of polylog integrals including the one @Dr. Wolfgang Hintze's offered.","['harmonic-numbers', 'euler-sums', 'closed-form', 'sequences-and-series']"
3715166,Regular Matrix Definition,"I found a definition of Regular Matrices that is, A regular matrix $A$ is a square matrix and there are some n ( $\geq$ 1) such that all the entries of $A^n$ are positive. I would like to know is this a correct definition? According to the definition, the following matrix is regular but how to prove it? Any hints $\\
$ \begin{pmatrix}
1 & 4& 3\\
2& 0& 1\\
4& 3& 2\\
\end{pmatrix}","['matrices', 'linear-algebra']"
3715194,"Determine all pairs of positive integers $(a,b)$ such that $a^2+b^2+ab$ is a perfect square.","Consideration via mod $4$ shows that $a,b=0$ mod $4$ or one of between $a,b$ is $=1$ mod $4$ while the other is $=0 $ mod $4$ .
Considering $(a+b)^2,a^2+an+b^2,(a+b-1)^2$ as we can deduce that $a,b>2$ .","['algebraic-number-theory', 'number-theory', 'elementary-number-theory', 'diophantine-equations', 'discrete-mathematics']"
3715205,Show that $\int_0^1 h(t)dt\geq\left(\int_0^1 f(t)dt \right)^a\left(\int_0^1 g(t)dt \right)^{1-a}$,"Let $ a \in (0,1)$ and $f,g,h :[0,1]\to (0,\infty)$ be continuous functions satisfying $h(ax+(1-a)y)\geq f(x)^ag(y)^{1-a}$ for all $x,y \in [0,1]$ . Prove that $$ \int_0^1 h(t)dt\geq\left(\int_0^1 f(t)dt  \right)^a\left(\int_0^1 g(t)dt  \right)^{1-a} $$ I tried integrating left side with respect to $x$ and $y$ separately, but it didn't work. Edit: Number ' $a$ ' is chosen only once, the condition does not have to hold for values of ' $a$ ' other then the one we chose.","['inequality', 'real-analysis']"
3715212,For which $a$ the solution is defined on the interval,"For which $a$ the solution of $$\begin{cases} \frac{z'}{z^2}= e^{-x^2} \\ z(0)=a \end{cases}$$ is defined on the interval $[0,\infty)$ My try: $$\frac{z'}{z^2}= e^{-x^2}$$ $$\int \frac{z'}{z^2} \,dx=\int e^{-x^2}\,dx$$ $$-\frac{1}{z(x)}+C=\int e^{-x^2}\,dx$$ $$z(x)=\frac{1}{\frac 1a -\int e^{-x^2}\,dx}$$ I think that this equation has the solution when exist $z'$ . $z'(x)=\frac{e^{-x^2}}{(\frac 1a - \int e^{-x^2}\,dx)^2}$ so exist when $(\frac 1a - \int e^{-x^2}\,dx)$ doesn't converge  to $0$ However, I do not know how it matters for the range given in the task. Can you help me finish this?",['ordinary-differential-equations']
3715271,"Prove there exist two distinct points $\eta,\xi \in (a,b)$ such that $f'(\eta)f'(\xi)=\left[\frac{f(b)-f(a)}{b-a}\right]^2$.","Suppose $f(x)$ is continuos over $[a,b]$ and differentiable over $(a,b)$ . Prove there exist two distinct points $\eta,\xi \in
 (a,b)$ such that $f'(\eta)f'(\xi)=\left[\dfrac{f(b)-f(a)}{b-a}\right]^2$ . For this purpose, if we can prove that, there exists $c\in(a,b)$ such that $$\frac{f(c)-f(a)}{c-a}\cdot\frac{f(b)-f(c)}{b-c}=\left[\frac{f(b)-f(a)}{b-a}\right]^2,\tag{*}$$ then applying Lagrange MVT over $(a,c)$ and $(c,b)$ respectively, the conclusion is followed. But $(*)$ seems not to hold necessarily.","['calculus', 'analysis']"
3715368,Proving $(A \cap B) \cup (A - B) = A$,"I think I have figured out this proof, but was hoping someone could verify it. \begin{align*}
x \in (A \cap B) \cup  (A - B) & \iff x \in A \cap B \land x \in A - B \\
& \iff (x \in A \land x \in B) \lor(x \in A \land x \not \in B) \\
& \iff x \in A \land (x \in B \lor x \not \in B) \\
& \iff x \in A.
\end{align*} The first   line is the definition of union. The second is the definition of interection and set difference. The third uses the rule from propositional logic that $p \wedge (q \lor r) \equiv (p \wedge q) \lor (p \wedge r)$ . Finally, $p \lor \neg p$ is a tautology that is always true, and $p \wedge T \equiv p$ .","['elementary-set-theory', 'solution-verification']"
3715384,"Prove: $(\forall m, n\in\Bbb N_{>0})(\exists x\in\Bbb R)$ s. t. $2\sin n x \cos m x \ge 1$","Problem 1 :
Prove that for any $m,n\in\Bbb N_{>0}$ , there exists $x \in\Bbb R$ such that $2\sin n x \cos m x \ge 1$ . Four months ago, someone asked the above question. However, when I wanted to post my answer,
the question was deleted. I searched by Approach0 without any result.
I think that it is a nice question. I don't know why it was deleted. I post it here. I don't remember who posted it before. Edit 2021/02/20: We may restrict $x$ to be the form $x = r\pi$ where $r$ is a rational number. We give the following problem: Problem 2 : Prove that for any $m, n \in \mathbb{N}_{>0}$ , there exists rational $r$ such that $2\sin (n r \pi) \cos (m r\pi) \ge 1$ . Any comments and solutions are welcome and appreciated. Partial results are as follows. If $n = m$ , let $x = \frac{\pi}{4n}$ and we have $2\sin n x \cos m x = \sin 2n x = 1$ . If $n > m$ , let $x = \frac{\pi}{2(2n-1)}$ . Since $0 < n x < \pi$ and $0 < m x \le (n-1)x < \pi$ , we have \begin{align}
2\sin n x \cos m x &\ge 2\sin n x \cos (n-1)x \\
&= \sin (2n-1)x + \sin x \\
&= 1 + \sin \frac{\pi}{2(2n-1)}\\
& \ge 1.
\end{align}","['trigonometry', 'inequality']"
3715391,Euler class of a manifold,"While computing the Euler class of the manifold, can we use connections other than the Levi-Civita connection ? What is the restriction on the connection that can be used to compute the Euler class ?","['manifolds', 'connections', 'characteristic-classes', 'differential-geometry']"
3715475,Banach-Alaoglu Theorem over spherically complete non-Archimedean fields,"About a year ago I asked here whether the Banach-Alaoglu Theorem works over the $p$ -adics. The satisfactory answer I got is that the ""usual"" proof only uses local compactness, and so the Banach-Alaoglu Theorem holds for any local field. Now I would like to look at other, more general non-Archimedean fields. I know that Hahn-Banach holds for all spherically complete such fields, and so I was wondering if it is possible to prove Banach-Alaoglu for such fields as well? Because Hahn-Banach works, a related question is whether in the complex setting there is a proof of Banach-Alaoglu that uses Hahn-Banach, but not local compactness of $\mathbb{R}$ or $\mathbb{C}$ .","['banach-spaces', 'p-adic-number-theory', 'functional-analysis', 'nonarchimedian-analysis', 'hahn-banach-theorem']"
3715479,Directional derivative for non differentiable function?,"How may I find the Directional derivative for this function if it's not differentiable at (0,0). Which means I can't use: $\frac{\partial f}{\partial v} = \nabla f \cdot v$ $$f(x,y)= \begin{cases}
\frac{x^2y}{x^4+y^2}, & (x,y) \ne (0,0) \\
0, & (x,y) = (0,0)
\end{cases}$$ Note: I tried to write the function here but failed since it has two cases.","['calculus', 'functions', 'derivatives']"
3715535,Probability of Exiting A Roundabout,"I am piggybacking on this question: probability of leaving The question was closed, but I found it interesting and I would like feedback on what I have done with it, and I have more questions about it. I'm not sure what proper etiquette is for piggybacking on closed questions. I will rephrase the question as I understand it: In the diagram below, you start at the node marked with lowercase $a$ . From $a$ , you move to one of the adjacent nodes, chosen uniformly at random. That is, you move to $b$ , $d$ , or $A$ . If you are at a lowercase node, you continue in the same fashion, moving to an adjacent node uniformly at random. Once you reach an uppercase node, you have left the roundabout, and the journey stops. The question is, what are the probabilities of the journey ending at $A$ , $B$ , $C$ , and $D$ ? In the original question, there was an answer that used Markov chains, but I am wondering if there is a way to do it without that technique. I modeled this experiment in Excel, and after $100,000$ trials, the probabilities appear to be roughly: $P(A)=46.5\%$ $P(B)=P(D)=20\%$ $P(C)=13.5\%$ I have no way of knowing if these are the exact answers, or even if the exact answers are rational numbers, but they make intuitive sense to me because of the structure and symmetry in the diagram. I wonder if there is a way to ""juggle"" conditional probabilities to find exact answers to this question, without having to use Markov chains. I would start by calculating $P(A|a)$ , the probability of leaving at $A$ given you started at $a$ . By symmetry, $P(B|b)$ , $P(C|c)$ , and $P(D|d)$ would be the same as $P(A|a)$ . To get $P(A|a)$ , notice that the total length of the walk must be odd. Either you leave immediately ( $1$ step), or you take an even number of steps in the roundabout to return to $a$ , and then leave at $A$ ( $2m+1$ , for some integer $m$ , steps). For a given walk of $k$ steps, the probability of taking that walk is simply $(\frac13)^k$ . Let $N_k$ be the number of walks of $k$ steps that leave the roundabout at $A$ . Then $P(A|a)=\frac13+N_3\cdot(\frac13)^3+N_5\cdot(\frac13)^5+...$ I'm unable to find a systematic way of calculating $N_k$ . It is easy enough to do for $k=3$ or $5$ , but I can't be sure of what pattern is emerging. Beyond that, assuming I did have an exact answer for $P(A|a)$ , I would still need to figure out $P(B|a)$ . Would that just be $\frac13\cdot P(A|a)$ since there is a $\frac13$ probability of going to $b$ on the first step? By symmetry, $P(D|a)=P(B|a)$ , so if I had $P(A|a)$ and $P(B|a)$ , I could easily figure out all the probabilites. I would appreciate any input on this!","['random-walk', 'probability']"
3715541,Is the supremum of a continuous function bounded?,"Let $f:\mathbb{R}\times[a,b]\to \mathbb{R}$ be a continuous function with $a<b$ , such that for every $y\in [a,b]$ we have that $$\sup_x f(x,y)\in \mathbb{R}$$ Does that mean that $$\sup_{y\in [a,b]}\sup_{x\in \mathbb{R}}f(x,y)\in \mathbb{R}$$ I believe it is not true, but I don't know how to prove it.","['real-analysis', 'continuity', 'multivariable-calculus', 'calculus', 'supremum-and-infimum']"
3715585,Is a group of prime-power order always abelian?,"Let $G$ be a group of order $p^n$ , with $p$ prime. By Sylow's first theorem, there exists at least one  subgroup of order $p^n$ (the number of subgroups of order $p^i$ is $1$ mod $p$ per $i$ ). The subgroups with order $p^n$ are all Sylow- $p$ groups. Now, by Sylow's third theorem, because the group is of order $p^n$ , the number $m_{p^{n}}$ of such subgroups must divide $\#G/p^n =1$ , and only $1$ divides $1$ , so there is only one subgroup of order $p^n$ . By Sylow's second theorem, all Sylow- $p$ groups are conjugated to each other by at least one element $g\in G$ , so, for any $S$ and $S'$ , we have $S=gS'g^{-1}$ . In this case, there is only one Sylow- $p$ group, so it is conjugated to itself. Of course, that one subgroup is the group itself. We now have $gG=Gg$ for some $g$ in $G$ . Can we get to the entire group being abelian, from here? I ask because my textbook on Abstract Algebra states that any group of order $p^2$ is abelian, and I'm curious whether it generalises. Edit: As has been pointed out, everything I've proved above is quite trivial. Below it is discussed that the essential question is actually "" How does one prove that groups of order $p^2$ are abelian using Sylow theory? "", since my textbook explicitly mentions this property as an application of Sylow's theorems. Edit 2: One of the authors has confirmed that they accidentally mixed some classic classification theorems into the list of applications of Sylow theory, and that this was one of them.","['group-theory', 'sylow-theory', 'abelian-groups', 'prime-numbers']"
3715596,Isn't the Lebesgue measure space complete?,"Let $X$ be a non-empty set. Let $\mathcal A$ be an algebra of subsets of $X$ and $\mathcal S (\mathcal A)$ be the $\sigma$ -algebra of subsets of $X$ generated by $\mathcal A.$ Let $\mu : \mathcal A \longrightarrow [0,+\infty]$ be a measure on $\mathcal A.$ Let $\mu^*$ be the induced outer measure. Let $\mathcal S^*$ be the $\sigma$ -algebra of $\mu^*$ -measurable subsets of $X.$ Then what I know is that $\mathcal A \subseteq \mathcal S^*$ and hence $\mathcal S (\mathcal A) \subseteq \mathcal S^*.$ Then the measure space $(X,\mathcal S^*, \mu^*)$ is complete since $\mathcal N \subseteq \mathcal S^*,$ where $$\mathcal N : = \{E \subseteq X\ |\ \mu^*(E) = 0 \}.$$ The measure space $(X,\mathcal S^*,\mu^*)$ is called the completion of the measure space $(X,\mathcal S (\mathcal A),\mu^*).$ For the Lebesgue measure space we have $X = \Bbb R,$ $\mathcal S^* = \mathcal L_ {\Bbb R},$ the $\sigma$ -algebra of Lebesgue measurable sets, $\mathcal S(\mathcal A) = \mathcal B_{\Bbb R},$ the $\sigma$ -algebra of Borel sets and $\mu^* = \lambda^*,$ the outer Lebesgue measure induced by the length function. Hence in this case we can say that the Lebesgue measure space $(\Bbb R, \mathcal L_{\Bbb R}, \lambda^*)$ is complete and it is the completion of $(\Bbb R, \mathcal B_{\Bbb R},\lambda^*).$ Let $$\mathcal N : = \{E \subseteq X\ |\ \lambda^*(E)=0 \}.$$ Now since the Lebesgue measure space is complete, $\mathcal N \subseteq \mathcal L_{\Bbb R}.$ That means all the subsets of $\Bbb R$ which have outer Lebesgue measure $0$ are Lebesgue measurable. But how can it be true in reality? I know the existence of non-Lebesgue measurable sets (i.e. Vitali set ) having outer Lebesgue measure $0.$ I don't understand where did I mess up! Can anybody please help me in clearing my confusion? Thank you so much for your valuable time for reading.","['measure-theory', 'lebesgue-measure', 'outer-measure']"
3715657,Why are exact differential equations called so?,"As the question says, why are exact differential equations called so? From Wikipedia, I got ""The nomenclature of ""exact differential equation"" refers to the exact differential of a function"". That leads me to ask why is an exact differential called so? Usually, the term 'exact' in the context of English refers to some quantity that is not approximated in any way. How does that definition fit here?","['math-history', 'ordinary-differential-equations', 'terminology']"
3715674,"basic notion about schemes.. what is the difference between $s(x)$ and $s_x$ for $s \in \Gamma(U, O_X)$","Let $X$ be a scheme and $U$ an open subset of $X$ . Let $s \in \Gamma(U, O_X)$ and $x \in U$ . I am getting confused with what the difference is between $s_x$ and $s(x)$ ... Are they the same thing?","['affine-schemes', 'algebraic-geometry', 'schemes']"
3715711,Explanation for devissage argument,"Let $K$ be a local field of characteristic $0$ with the ring of integers $\mathcal{O}_K$ and uniformizer $\pi$ . Let $k$ be the residue field of $K$ with $\text{card}(k)=q$ . Let $\mathcal{O}_\mathcal{E}$ be the $\pi$ -adic completion of $\mathcal{O}_K((u))$ , where $u$ is a fixed local co-ordinate. Then $\mathcal{O}_\mathcal{E}$ is complete local ring with uniformizer $\pi$ and residue field $E:=k((u))$ . Let $\mathcal{E}$ be the field of fractions of $\mathcal{O}_\mathcal{E}$ . Let $\widehat{\mathcal{E}^{ur}}$ be the completion of the maximal unramified extension of $\mathcal{E}$ . Let $\mathcal{O}_{\widehat{\mathcal{E}^{ur}}}$ denotes the ring of integers of $\widehat{\mathcal{E}^{ur}}$ . Then $\mathcal{O}_{\widehat{\mathcal{E}^{ur}}}$ is a complete local ring with uniformizer $\pi$ and residue field as $E^{sep}$ . Then we have the following exact sequence \begin{equation*}
	0\rightarrow k \rightarrow E^{sep}\xrightarrow{x\mapsto x^q-x}E^{sep}\rightarrow 0. 
	\end{equation*} In other words, the sequence \begin{equation*}
0\rightarrow \mathcal{O}_K/\pi\mathcal{O}_K \rightarrow \mathcal{O}_{\widehat{\mathcal{E}^{ur}}}/\pi \mathcal{O}_{\widehat{\mathcal{E}^{ur}}}\xrightarrow{x\mapsto x^q-x} \mathcal{O}_{\widehat{\mathcal{E}^{ur}}}/\pi \mathcal{O}_{\widehat{\mathcal{E}^{ur}}}\rightarrow0
\end{equation*} is exact as $k$ is the residue field of $\mathcal{O}_K$ and $E^{sep}$ is the residue field of $\mathcal{O}_{\widehat{\mathcal{E}^{ur}}}$ .
Then by devissage the sequence \begin{equation}
	0\rightarrow \mathcal{O}_K/\pi^n\mathcal{O}_K \rightarrow \mathcal{O}_{\widehat{\mathcal{E}^{ur}}}/\pi^n \mathcal{O}_{\widehat{\mathcal{E}^{ur}}}\xrightarrow{x\mapsto x^q-x} \mathcal{O}_{\widehat{\mathcal{E}^{ur}}}/\pi^n \mathcal{O}_{\widehat{\mathcal{E}^{ur}}}\rightarrow0,
	\end{equation} is exact for all $n\geq1$ .
I don't want to say the word ""by devissage"" and want to write the explicit proof. I am trying to induction on $n$ . but somehow I am not able to prove that the sequence is exact. Is there any other way to prove the exactness of this sequence.","['hodge-theory', 'homological-algebra', 'p-adic-number-theory', 'algebraic-geometry', 'commutative-algebra']"
3715727,Issue with Hartshorne definition of finite morphism?,"Hartshorne defines a morphism $f:X\rightarrow Y$ to be an finite morphism if ""there exists a covering of $Y$ by open affine subsets $V_i=\operatorname{Spec}(B_i)$ , such that for each $i$ , $f^{-1}(V_i)$ is affine, equal to $\operatorname{Spec}(A_i)$ , where $A_i$ is a $B_i$ -algebra which is a finitely generated $B_i$ -module"". In the example immediately following this definition, he talks about schemes of ""finite type over $k$ "", which I assume just means that the morphism $X\rightarrow \operatorname{Spec}(k)$ is finite. What disturbs me in this definition is the the part about being each $A_i$ being finitely generated $B_i$ - module , rather than, say, a finitely generated $B_i$ -algebra. For example, if $X=\operatorname{Spec}( k[x])=\mathbb{A}^1$ , then $X$ is obviously an affine scheme over $k$ but the morphism $X\rightarrow \operatorname{Spec}(k)$ is not of finite type over $k$ since $k[x]$ is not a finitely generated $k$ -vector space (though it finitely generated as a $k$ -algebra). Thus, according to this definition, $\mathbb{A}^1$ is not of finite type over $k$ , which seems wrong to me. Am I misunderstanding something here, or is this a typo?","['algebraic-geometry', 'schemes', 'modules']"
3715731,Multiplying $P(x) = (x-1)(x-2) \dots (x-50)$ and $Q(x)=(x+1)(x+2) \cdots(x+50)$,"Let $P(x) = (x-1)(x-2) \dots (x-50)$ and $Q(x)=(x+1)(x+2) \cdots(x+50).$ If $P(x)Q(x) = a_{100}x^{100} + a_{99}x^{99} + \dots + a_{1}x^{1} + a_0$ , compute $a_{100} - a_{99} - a_{98} - a_{97}.$ I've been quite stuck with this one. If I multiply and group the polynomials with the similar terms e.g $(x-1)$ and $(x+1) ...$ I would get $P(x)Q(x)= (x^2-1)(x^2-2^2) \dots(x^2-50^2)$ right? From here on if I would keep multiplying the terms wouldn't I end up with $50$ times the term $x^2$ as the initial term, hence $a_{100} = 1$ since it wouldn't have any coefficient? For the other terms I don't have a clue how to find them so any hints would be appreciated","['contest-math', 'algebra-precalculus', 'polynomials']"
3715757,Simplification of ${0 \binom{n}{0} + 2 \binom{n}{2} + 4 \binom{n}{4} + 6 \binom{n}{6} + \cdots}$ [duplicate],"This question already has answers here : How do I prove $2 \binom n2 + 4 \binom n4 + 6 \binom n6 + \cdots = n 2^{n-2}$ (4 answers) Closed 4 years ago . Simplify $$0 \binom{n}{0} + 2 \binom{n}{2} + 4 \binom{n}{4} + 6 \binom{n}{6} + \cdots,$$ where $n \ge 2.$ I think we can write this as the summation $\displaystyle\sum_{i=0}^{n} 2i\binom{n}{2i},$ which simplifies to $\boxed{n\cdot2^{n-2}}.$ Am I on the right track?","['summation', 'binomial-coefficients', 'combinatorics', 'binomial-theorem']"
3715765,Lower bound for upper $\pi/2$ angular density,"This is exercise 2.3 in Falconer's book 'The Geometry of Fractal Sets'. Let $E\subset \mathbb{R}^n$ be an $\textit{s}$ -set. That is, it is measurable for the s-dimensional Hausdorff measure $H^s$ and has $0 < H^s(E)< \infty$ . For $x\in \mathbb{R}^n$ and a unit vector $\theta \in \mathbb{R}^n$ , let $$
S_r(x,\theta,\pi/2) = x + \lbrace y\in \mathbb{R}^n: y\cdot \theta \geq 0, \|y\|<r\rbrace.
$$ In other words, we intersect the cone of vectors with angle $\leq \pi/2$ from $\theta$ with the unit ball of radius $r$ and translate to $x$ . This gives a $\textit{hemiball}$ . $$
\overline{D^s}(E,x,\theta,\pi/2):= \limsup\limits_{r\to0} \frac{H^s(E\cap S_r(x,\theta,\pi/2))}{(2r)^s}
$$ is the upper angular density with respect to the angle $\pi/2$ . Exercise: Show that for $H^s$ -almost every $x\in E$ , we have $2^{-s} \leq \overline{D^s}(E,x,\theta,\pi/2)$ . The book demonstrates that the similarly defined upper convex density is $1$ almost everywhere on $E$ so that the upper angular density when we take the full ball (angle = $2\pi$ ) instead of a cone is bounded below by $2^{-s}$ . I don't see how to imitate the spirit of that proof. Please help me.","['measure-theory', 'dimension-theory-analysis', 'hausdorff-measure', 'geometric-measure-theory']"
3715779,Intuition behind area of ellipse,"This is not meant to be a formal proof, but I just wanted to know if this is a valid way of thinking about the area of an ellipse. It does assume knowledge of the area of a circle, but this can be proven without knowledge of the area of an ellipse . I also don't know how to include pictures, so please excuse that. Draw an ellipse with semi-major axis $a$ and semi-minor axis $b$ . Because $a$ and $b$ are both linear quantities (i.e. they have units of distance), $k=\frac{b}{a}$ is dimensionless. Hence, we can draw a new ellipse with a semi-major axis of $ka$ and a semi-minor axis of $b$ . Because $b=ka$ , this new ellipse is a circle of radius $b$ , so it has an area of $$A_{circ}=\pi b^2.$$ Because $a$ and $b$ are both linear, the area of the first ellipse, $A$ , can be expressed as the product of these two quantities and some constant. Also, because $A$ was only scaled by a factor of $k$ in one dimension to get $A_{circ}$ , $$kA=A_{circ}.$$ Substituting, $$\left(\frac{b}{a}\right)A=\pi b^2$$ $$\therefore \boxed{A=\pi ab}$$ as desired. $\blacksquare$","['euclidean-geometry', 'geometry', 'intuition']"
3715824,Proof that $\lim_{n\to\infty}\left(1+\frac{x^2}{n^2}\right)^{\frac{n}{2}}=1$ without L'Hospital,"I proved that $$\lim_{n\to\infty}\left(1+\frac{x^2}{n^2}\right)^{\frac{n}{2}}=1$$ using L'Hospital's rule. But is there a way to prove it without L'Hospital's rule? I tried splitting it as $$\lim_{n\to\infty}n^{-n}(n^2+x^2)^{\frac{n}{2}},$$ but that didn't work because $\lim_{n\to\infty}(n^2+x^2)^{\frac{n}{2}}$ diverges.","['limits', 'calculus', 'limits-without-lhopital']"
3715845,Function that transforms a differential equation,"What function $u$ can transform the equation $$fy''-4f'y'+gy=0$$ to an equation of the form $$v''+kv=0$$ . Here, $f,g,y,k,u,v$ are all functions of $x$ and $y=uv$ . Is there any theory for such type of transformations? By simple calculation, it is seen that $v''=fy''-4f'y'$ . But how to proceed further? Any hints? Thanks beforehand.","['calculus', 'ordinary-differential-equations', 'real-analysis']"
3715846,Topologies and completeness,"Is it possible to have two norms on the same space, the topology of one being strictly finer than the other, yet the space is complete in both norms?","['general-topology', 'normed-spaces', 'functional-analysis']"
3715853,Integrate $\frac{e^{itx}}{1-it}$,"I need this integral to find the probability density function from the characteristic function but I don't know how to find it, every method that I try fails. I tried integration by parts but I didn't get the result that I need to. The integral is $$\frac{1}{2\pi}\int_{-\infty}^{\infty}{\frac{e^{itx}}{1-it}dt}.$$ The result I need to get by my computations is $e^{-x}$ , but I dont know how.","['integration', 'complex-analysis', 'definite-integrals', 'real-analysis']"
3715926,Confusion about what it means for a section to vanish on a subscheme,"Let $i: V \rightarrow \mathbb{A}^n_k$ be a subscheme of the affine space of dimension $n$ over a field $k$ . Let $f \in k[T_1, \dots, T_n]$ . What does it mean for $f$ to vanish on $V$ ? Does it mean that for all $P \in V$ , the image of $f$ in $\mathcal{O}_{\mathbb{A}^n_k, i(P)} / \mathfrak{m}_{\mathbb{A}^n_k,i(P)}$ is zero. Or does it mean that the $f$ lies in the kernel of the following map $$  \mathcal{O}_{\mathbb{A}^n_k, i(P)}  \rightarrow \mathcal{O}_{V, P}.$$",['algebraic-geometry']
3715939,about free group and fundamental group.,"Compute the fundamental group of the space obtained from two tori $S^{1} \times S^{1}$ by identifying a circle $S^{1} \times\left\{x_{0}\right\}$ in one torus with the corresponding circle $S^{1} \times\left\{x_{0}\right\}$ in the other torus. solution : Let $X$ be the surface, the identification of two tori $S^{1} \times S^{1}$ as described in the exercise. And we know the fundamental group of the torus is $\mathbb{Z} \times \mathbb{Z}$ . Let's assume the two tori $T_{1}, T_{2}$ are identified by ""stacking"" one on the other. i.e. If $a, b$ and $c, d$ are the generators of the fundamental groups respectively. And $a$ and $c$ are their longitudes. The way we stack the two tori will make $a$ and $c$ identified. To use the Van Kampen's Theorem, let $A$ be the top torus $T_{1}$ together with a strip of open neighborhood of $a$ on itself and on the bottom torus $T_{2} .$ Similarly, let $B$ the bottom torus $T_{2}$ together with a strip of open neighborhood of $c$ on itself and the top one $T_{1}$ Then $A$ and $B$ are open subset of $X$ and $A \cap B$ is open and path connected. since $A$ and $B$ deformation retracts to $T_{1}$ and $T_{2}$ respectively, so $\pi_{1}(A)=\pi_{1}(B)=\mathbb{Z} \times \mathbb{Z}$ . since $A \cap B$ deformation retracts to a circle, we have $\pi_{1}(A \cap B) \simeq \mathbb{Z}$ , the generator has its image $a, c$ in $A, B$ respectively. By Van Kampen, $\pi_{1}(X)$ is isomorphic to the quotient of $\pi_{1}(A) * \pi_{1}(B)$ by the normal subgroup generated by $\left\langle a c^{-1}\right\rangle$ , $
\pi_{1}(X) \cong \frac{(\mathbb{Z} \times \mathbb{Z}) *(\mathbb{Z} \times \mathbb{Z})}{\left\langle a c^{-1}\right\rangle} \cong(\mathbb{Z} * \mathbb{Z}) \times \mathbb{Z}
$ i can't understand why $ \frac{(\mathbb{Z} \times \mathbb{Z}) *(\mathbb{Z} \times \mathbb{Z})}{\left\langle a c^{-1}\right\rangle} \cong(\mathbb{Z} * \mathbb{Z}) \times \mathbb{Z}$ ? is solution true ?","['group-theory', 'free-groups', 'algebraic-topology']"
3715968,"Missing flaw in finding all integers satisfying $\varphi(n)=n/2$, where $\varphi$ is the Euler totient function","I was reading this book on analytic number theory by Tom M. Apostol, and I came across this problem that asks for all integers that satisfy the following equality: $$ \varphi(n) = n/2$$ where $\varphi$ is the the Euler totient function. This was my first attempt: $$\varphi(n) =( N * \mu )(n)$$ $$\implies u * (N * \mu) =  \varphi * u = N$$ But the numbers we are looking for should fulfill $\varphi(n) = n/2$ , therefore the following proposition should be true for these numbers. $$N = \frac{1}{2}N*u $$ $$\implies N(n) = (\frac{1}{2}N*u)(n)$$ $$ \implies n = \sum_{d|n}\frac{1}{2}d$$ $$ \implies n =  \sum_{d|n,d\neq n}d$$ where $*$ is the Dirichlet multiplication, $\mu$ is the mobius function  of order 1, $N(n) = n, u(n) = 1 ,\forall n \in \mathbb{Z}$ which means the solution is all perfect numbers, but obviously one can disprove it by verification. Somehow I managed to find the correct  solution using another way. But I couldn't  find out what I missed on my first attempt. Can some one help me  figure out what it is ? Thanks.","['analytic-number-theory', 'number-theory', 'totient-function']"
3715975,understanding the definition of measurable mappings,"In definition of Measurable mappings, we consider measurable spaces $(\Omega_1, \mathcal{F}_1)$ and $(\Omega_2, \mathcal{F}_2)$ and the mapping $T:\Omega_1 \rightarrow \Omega_2$ is measurable $\mathcal{F}_1/\mathcal{F}_2$ if $T^{-1}A \in \mathcal{F}_1$ for each $A\in \mathcal{F}_2$ . my question is why we cannot define like ' $TA\in\mathcal{F}_2$ for all $A\in\mathcal{F}_1$ '. What is the fallacy in this definition? Thanks in advance","['measurable-functions', 'measure-theory', 'probability-theory', 'probability']"
3715995,Why does $\frac{|\sin\theta|}{2}<\frac{|\theta|}{2}<\frac{|\tan\theta|}{2}$ not imply that $1>\lim_{\theta\to 0}\frac{\sin\theta}{\theta}>1$?,I was watching this proof of the equality $$\lim_{\theta\to 0} \frac{\sin \theta}{\theta} = 1$$ The author says about the following areas that red area <= yellow area <= blue area . Which leads to the following inequality: $$\frac{|\sin\theta|}{2} \le \frac{|\theta|}{2} \le \frac{|\tan\theta|}{2}$$ and in the end proofs the theorem. $$1 \ge \lim_{\theta\to 0} \frac{\sin \theta}{\theta} \ge 1 $$ I noticed that the statement red area < yellow area < blue area about the areas is also true and in fact more accurate. But this would lead to the following: $$\frac{|\sin\theta|}{2} \lt \frac{|\theta|}{2} \lt \frac{|\tan\theta|}{2}$$ ... $$1 \gt \lim_{\theta\to 0} \frac{\sin \theta}{\theta} \gt 1 $$ Obviously that cannot be true. Have I just broken the proof?,"['limits', 'trigonometry', 'solution-verification', 'proof-explanation']"
3716032,Can I replace $x$ with $t=2x+1$ to find the radius of convergence of $\sum_{n=1}^\infty\left(1+\frac{(-1)^n}n\right)^{n^2}\cdot\frac{(2x+1)^n}n$?,"I am requesting some help to spot where I did mistake since I spent hours reviewing my solution but wasn't successful spotting the mistake to correct it. I need to find the radius of convergence for the following summation: $$ f(x) = \sum_{n=1}^\infty \left(1+\frac{(-1)^n}{n}\right)^{n^2} \cdot \frac{(2x+1)^n}{n}$$ So, I defined a new function $f(t)$ that's exactly like $f(x)$ but replaced $2x+1$ with $t$ I calculated the radius of convergence for $f(t)$ and found that it's $1/e$ Since $t=2x+1$ thus $x=\frac{t-1}{2}$ I said that the radius of convergence for $f(x)-R_x$ is equal to $$\frac{R_{t}-1}{2}=\frac{\frac{1}{e}-1}{2}$$ But, I can confirm that my answer is wrong (through automatic checker).","['power-series', 'calculus', 'functions', 'convergence-divergence']"
3716073,Convergence/divergence of a function of the first $n$ terms of a sequence,"Let $(x_n)$ be an arbitrary sequence in $\mathbb R$ and let $y_n=y_n(x_1,\ldots,x_n)$ be a function which depends solely on the first $n$ terms of $x$ . My question concerns the convergence and divergence of $(y_n)$ . Firstly consider the case that $y_n=\sqrt{x_1+\sqrt {x_2+\sqrt{\ldots+\sqrt{x_n}}}}$ and $x_n\geq 0$ . Clearly, $y_n$ is an non-decreasing sequence. Also, if the sequences $(x_i^{(1)})_{i=1}^\infty, (x_i^{(2)})_{i=1}^\infty$ satisfies $\lim x_i^{(1)}/x_i^{(2)}<\infty$ , then $y_n(x^{(1)})$ converges whenever $y_n(x_i^{(2)})$ converges. The two sequences is said to have the same order of infinity (which is an equivalence relation) if $0<\lim x_i^{(1)}/x_i^{(2)}<\infty$ . Define the set of orders of infinity to be the set of such equivalence classes of sequences. By using zorn's lemma in some way, I proved that there exist an order of infinity, represented by the sequence $c_n$ , such that whenever $\lim x_n/c_n=0$ , $y_n(x)$ converges, and  whenever $\lim x_n/c_n=\infty$ , $y_n(x)$ diverges. Now, the question is: could I find the sequence $c_n$ explicitly? Of course, the sequence $x_n=Ae^{2^nk}$ will fit into the square roots nicely, but this is clearly not satisfying the requirement of $c_n$ .","['analysis', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
3716083,When does Darboux's theorem on symplectic manifolds work globally?,"I'm a Physics PhD student working on Hamiltonian systems in the context of General Relativity. Recently, I was working on adding a perturbation to a dynamical system with known solutions. Without going to much into detail, we are working on a 2n-dimensional manifold where the new symplectic 2-form after the perturbation takes the form \begin{equation}
\Omega_{AB}=\Omega^0_{AB}+\epsilon \Omega^1_{AB}
\end{equation} where $\epsilon$ is supposed to be a small number and $\Omega^0=dp_\alpha \wedge dz^\alpha$ is the symplectic 2-form of the unperturbed system (I'm using the canonical coordiantes $(z^\alpha,p_\alpha)$ with $\alpha=1,2\dots n$ . Since $\Omega^1$ has to be closed and nondegenerate like $\Omega$ then there's a set of local coordinates $(\bar{z}^\alpha,\bar{p})_\alpha$ where it takes the form \begin{equation}
\Omega^1 = d\bar{p}_\alpha \wedge d\bar{z}^\alpha
\end{equation} Which means that we can use the diffeomorphism connecting the two sets of variables to write the perturbation as the pullback of $\Omega^0$ like \begin{equation}
\Omega^1 = \mathcal{L}_X \Omega^0
\end{equation} where $X$ is the vector field that generates the diffeomorphism connecting the bared and unbared coordinates. Now, this whole thing works due to Darboux's theorem which guarantees that there are coordinates where $\Omega^1$ takes the canonical form, at least locally. The question is: Are there conditions for this to work globally? I think that there are probably some topological conditions on the manifold but I wouldn't know  how to approach the question. Any directions will be welcomed. Edit: Based on the comments I want to clarify what the goal is. The general question is what are the conditions for Darboux Theorem to hold globally. That is, what conditions need to be satisfied for the perturbation to be $\Omega^1 =d\bar{p}_\alpha\wedge d\bar{z}^\alpha$ globally. In particular, I want to know what conditions have to be satisfied for the expression $\Omega^1 = \mathcal{L}_X \Omega^0$ to work globally. But I think both concerns are the same question.","['differential-topology', 'symplectic-geometry', 'differential-geometry']"
3716140,Eigenvalues of offset multiplication tables,"Consider the $n$ x $n$ 'multiplication table' constructed as (using Mathematica language) $$
M_n^s = \text{M[n_,s_]:=Table[ k*m , {k,1+s, n+s}, {m, 1+s, n+s} ] }
$$ For example, $$M_4^0 = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 2 & 4 & 6 & 8 \\3 & 6 & 9 & 12 \\4 & 7 & 12 & 16 \end{pmatrix} $$ and $$M_4^1 = \begin{pmatrix} 4 & 6 & 8 & 10 \\ 6 & 9 & 12 & 15 \\8 & 12 & 16 & 20 \\10 & 15 & 20 & 25 \end{pmatrix} $$ Amazingly, these matrices have all zero eigenvalues, except for a single value.  The non-zero eigenvalue exhibits an interesting pattern as a function of $n$ and $s.$ Starting with $n=2,$ the sequences read as follows $$ \text{eigv }M_n^0=\{1,5,14,30,55,91,140...\} = (n-1)(2n^2-n)/6 $$ $$ \text{eigv }M_n^1=\{4,13,29,54,90,139,203...\} = (n-1)(2n^2 + 5n + 6)/6 $$ $$ \text{eigv }M_n^2=\{9,25, 50, 86, 135, 199,280...\} = (n-1)(2n^2 + 11n + 24)/6 $$ $$ \text{eigv }M_n^3=\{25,61,110,174,255,355...\} = (n-1)(2n^2 + 17n + 54)/6 $$ I've worked out many of these, and it appears that, in considering the quadratic polynomial, and using $[n^1]$ to mean the coefficient of $n,$ $$ [n^2] = 2, \, [n^1]=6s-1, \, [n^0] = 6s^2 , \ s=1,2,3... $$ The question is: can these observations be proved?","['matrices', 'number-theory', 'discrete-mathematics', 'eigenvalues-eigenvectors']"
3716201,Smooth boundary integration by parts formula proof,"I'm having trouble proving the following integration by parts formula for a bounded domain $\Omega \subset \mathbb{R}^n$ with $\partial \Omega$ being at least $C^2$ : $$ \int_{\partial \Omega} \psi \delta_j \phi = - \int_{\partial \Omega} \phi \delta_j \psi  + \int_{\partial \Omega} \psi \phi H \eta^j $$ where $\delta_j = \sum_{i=1}^n( \delta_{ij} - \eta^i \eta^j) D_i $ is the $j^{th}$ component of the tangential derivative, and $\eta = Dd$ is the inward pointing normal from the boundary (which exists since the boundary is at least $C^2$ ) and $H = \Delta d|_{\partial \Omega}$ for $d$ the distance function for $\partial \Omega$ in a sufficiently small strip near the boundary. The text claims: These integration by parts formulae are easily proved. We just the divergence theorem over $\Omega$ to evaluate each side of the identity $\int_{\Omega} (d_k \phi)_{ki} = \int_{\Omega} (d_k \phi)_{ik}$ , (where subscripts denote partial derivatives in the indicated variable), and this gives the required identity after replacing $\phi$ by $\phi \psi$ . I have no idea really how to interpret the hint and use it. The identity above seems to simply be a trivial consequence of the distance function being $C^2$ , and I don't know what useful information I'm supposed to gather from it. Moreover, I truly don't know how to use the divergence theorem to ""evaluate both sides"" of that identity - since I have no idea what that quantity is the divergence of, so I don't know how to apply the divergence theorem. If someone could tell me: How to ""apply the divergence theorem"" and why the identity above is useful for the problem, I would very much appreciate it. Thanks","['integration', 'partial-differential-equations', 'sobolev-spaces', 'differential-geometry']"
3716221,How many ways to write a commutative non-associative product of $n$ terms?,"The Catalan numbers give the number of ways to write a non-commutative non-associative product of $n$ terms, as $C_{n-1}\cdot n!=\frac{(2n-2)!}{(n-1)!}$ . For example, there are $C_{3-1}\cdot3!=12$ ways to write a product of $3$ terms: $$(ab)c,\;(ac)b,\;(ba)c,\;(bc)a,\;(ca)b,\;(cb)a,\\a(bc),\;a(cb),\;b(ac),\;b(ca),\;c(ab),\;c(ba).$$ What if the multiplication is commutative? Then we have $(ba)c=(ab)c=c(ab)$ and so on. How many distinct products can we make? Here are the first few numbers. $$a;$$ $N_1=1$ . $$ab;$$ $N_2=1$ . $$(ab)c,\;(ac)b,\;(bc)a;$$ $N_3=3$ . $$((ab)c)d,\;((ab)d)c,\;((ac)d)b,\;((bc)d)a,\;(ab)(cd),\\((ac)b)d,\;((ad)b)c,\;((ad)c)b,\;((bd)c)a,\;(ac)(bd),\\((bc)a)d,\;((bd)a)c,\;((cd)a)b,\;((cd)b)a,\;(ad)(bc);$$ $N_4=15$ .","['nonassociative-algebras', 'combinatorics']"
3716294,Solving the Rubik cube with given initial and target states (generalization of standard Rubik cube),"Consider the generalization of the Rubik cube problem, where we are given an initial state $A$ and a final state $B$ , and we search for a path between the two. We can easily show that given $A$ and $B$ , one can compute $X$ such that the path from $X$ to $S$ is the same as the path from $A$ to $B$ .  Here, $S$ denotes the standard solved state. Now, is there a simple and convenient way to represent this isomorphism between the group subsumed by $A$ and $B$ and the group subsumed by $X$ and $S$ ? Let $G_1$ be the group such that $A$ and $B$ are members of $G_1$ . Accordingly, let $G_2$ be the group such that $X$ and $S$ are members of $G_2$ . How to characterize the isomorphism between $G_1$ and $G_2$ , which maps $A$ to $X$ and $B$ to $S$ ? The isomorphism basically consists in recoloring the minicubes in $A$ to generate $X$ in  such a way that the solution (path) from $X$ to $S$ is the same as the solution from $A$ to $B$ .  Note that we are able to compute $X$ without solving the Rubik cube problem, which is nice, because actually $X$ and $S$ may be in a different connected component than $A$ and $B$ . Correspondingly, we are searching for an isomorphism that does not require that $A$ and $B$ be in the same connected component as $X$ and $S$ . Naturally, the isomorphism cannot rely on solving the Rubik cube, as the solution would forcefully require the four elements to be in the same connected component. As an example, consider the following initial state $A$ , depicted below, The goal is to find a path to state $B$ , below This is equivalent to finding a path from state $X$ below to the canonical target state $S$ How to describe, using the language of group theory, the isomorphism that maps $A$ into $X$ and that maps $B$ into the canonical target state $S$ ? In what follows, we focus on the simplest case wherein $A$ , $B$ , $X$ and $S$ are all in the same connected component.   The possible configurations that we can get by manipulating a cube in configuration $A$ is the ""orbit"" of $A$ .  So, we're saying that in what follows the orbit of $A$ (which includes $B$ ) must be the same as the orbit of $S$ (which includes $X$ ). We can think about each configuration of the cube as being an alias to a sequence of moves that brings that configuration to the canonical solved state $S$ . Say that we refer to $M1$ as the sequence of moves that brings $A$ to the solved state $S$ , and we refer to $M2$ as the sequence of moves that brings $B$ to the solved state $S$ . Then, the sequence of moves that brings $A$ to $B$ is $M1 M2^{-1}$ and $ A$ maps to $M1$ and $B$ maps to $M2$ in the same way that $X$ maps to $M1 M2^{-1}$ and $S$ maps to $M2 M2^{-1} $ which is the identity element. Now, to build this mapping between $A$ and $X$ and between $B$ and $S$ we had to take the original sequence of moves and multiply each of those by $M2^{-1}$ (assuming operations are applied from left to right, otherwise, we would get $M2^{-1}M1$ as the mapping of $X$ instead of $M1 M2^{-1}$ ) I'd be curious to know if there is an alternative way to represent that mapping, which does not rely on knowing the sequence of moves $M2^{-1}$ . We can write a computer program to build $X$ from $A$ and $B$ without knowing $M1$ and $M2$ , but I don't know how to represent the operator that this program is implementing using the language of group theory. Is there an operator that maps states without necessarily sticking to the cube rules, but which allows to map $A$ into $X$ and $B$ into $S$ without knowing $M2$ ? I believe this operator is a sort of  exogenous or ""out of band"" operator in the group elements, leveraging side information about alternative representations of the elements of the groups.","['group-theory', 'group-isomorphism', 'rubiks-cube']"
3716301,Verifying that $(A \times B) \cap (C \times D) = (A \cap C) \times (B \cap D)$,"I believe that I have proved that $(A \times B) \cap (C \times D)= (A \cap C) \times (B \cap D)$ , but I am not completely certain that every step I have written is reversible. Here is what I have: \begin{align*}
x \in (A \times B) \cap (C \times D) & \iff x \in A \times B \text{ and } x \in C \times D \\
& \iff x = (a,b) \text{ for } a \in A, \; b \in B, \; \text{ and } x = (c,d) \text{ for } c \in C, \; d \in  D\\
& \iff x = (a,b) = (c,d) \text{ for } a \in A, \; b \in B, \; c \in C, \; d \in D \\
& \iff x = (\alpha, \beta) \text{ where } \alpha \in A, \; \alpha \in C, \; \beta \in C, \; \beta \in D \\
& \iff x = (\alpha, \beta) \text { where } \alpha \in A \cap C, \; \beta \in C \times D \\
& \iff x \in (A \cap C) \times (C \times D)
\end{align*} The first line is from the definition of intersetion. The second line is from the definition of the Cartesian product. The third line is from transitivity of equality. The fourth line is just a rewrite since the notion of an ordered tuple is well-defined. (I am not sure exactly how to say this other than that we clearly cannot have $x = (a,b)$ and $x = (c,d)$ where $a \neq c$ , because then $x$ is a meaningless object.) The fifth line is from the definition of intersection. The sixth line is from the definition of the Cartesian product. How is this?","['elementary-set-theory', 'solution-verification']"
3716340,Solution to nonlinear ODE,"Find all solutions the to initial value problem $$\frac{dy}{dx}=y^3e^{\sin{(x+y^2)}}, \quad y(2020)=0$$ and explain why there are no other solutions. So there exists a unique solution due to the existence and uniqueness theorem centred at $x=2020$ . Do we use separation of variables here? 
Thanks for the help.","['calculus', 'ordinary-differential-equations']"
3716387,"If $a, b, c, d\in\mathbb R^+, $ then prove that $\displaystyle\frac{a-b}{b+c}+\frac{b-c}{c+d}+\frac{c-d}{d+a}+\frac{d-a}{a+b}\ge 0.$",My approach: We have: $\displaystyle\frac{a-b}{b+c}+\frac{b-c}{c+d}+\frac{c-d}{d+a}+\frac{d-a}{a+b}$ $\displaystyle=\frac{a+c}{b+c}-1+\frac{b+d}{c+d}-1+\frac{c+a}{d+a}-1+\frac{d+b}{a+b}-1$ $\displaystyle=(c+a)\left(\frac1{b+c}+\frac1{d+a}\right)+(b+d)\left(\frac1{c+d}+\frac1{a+b}\right)-4\ge (c+a) \left(\frac{2\cdot 2}{b+c+d+a}\right)+(b+d)\left(\frac{2\cdot 2}{c+d+a+b}\right)-4$ (on applying AM $\ge$ HM.) $\displaystyle=\left(\frac4{a+b+c+d}\right) (c+a+b+d)-4=0.$ Hence the inequality. But I would love to know is there any other elegant method to prove the same? Please mention. Thanks in advance.,"['summation', 'inequality', 'alternative-proof', 'cauchy-schwarz-inequality', 'algebra-precalculus']"
3716445,Finite Markov Chain: $ X_n = X_{n-1}+B_{n} $ with two recurrent states.,"I have a simple and rather basic question regarding Markov Chains: let $B_{n}$ be an i.i.d process that gets the values $\{-1,1\}$ with probability of $\frac{1}{2}$ . We define the following Markov Chain above $\{0,1,...,K-1,K\}$ states: $X_{n}=\begin{cases}
X_{n-1}+B_{n}, & 0<X_{n-1}<K\\
X_{n-1}, & X_{n-1}=\{0,K\}
\end{cases}$ The State Diagram for ( $ p=\frac{1}{2} $ ): I'm interested in finding the transition probability: $\rho_{ji}=P\{\exists n>0:X_{n}=i|X_{0}=j\}$ for $i,j\in\{1,2,...,K-1\}$ which is the probability that the process $X_{n}$ reaches state $i$ at some time $n$ , given that the initial state was $j$ . first, I was able to calculate: $\pi_{k}=\rho_{k0}=P\{\exists n>0:X_{n}=0|X_{0}=k\}=\begin{cases}
1-\frac{k}{K}, & 0<k<K\end{cases}$ which is the probability to reach state $0$ given that the initial
state was at state $0<k<K$ . this was done by assuming a general solution $\pi_{k}=a\lambda^{k}+b$ ,
and solving an equation that has a relation between $\pi_{k,}\pi_{k-1,}\pi_{k+1}$ , which gives $\pi_{k}=1-\frac{1-\lambda^{k}}{1-\lambda^{K}}$ where $\lambda={{\frac{p}{1-p}} }$ . taking the limit $\lambda\rightarrow1$ , which suits our case,
(in which $p=\frac{1}{2}$ ), gives the result for $\pi_{k}$ that I wrote above. I want to use $\pi_{k}$ to find an expression for $\rho_{ji}$ for $i,j\in\{1,2,...,K-1\}$ . I argue that, in case $j>i$ : $\rho_{ji}=P\{\exists n>0:X_{n}=i|X_{0}=j\}=P\{\exists n>0:X_{n}=0|X_{0}=|j-i|\}=\pi_{|j-i|0}$ from symmetry. which means moving (to the left) from state $j$ to $i$ (where $i,j\in\{1,2,...,K-1\}$ ),
is equivalent to moving from state $|j-i|$ to state $0$ . thus, by using the previous solution for $\pi_{k}$ , one can find $\rho_{ji}$ for $j>i$ case: $\rho_{ji}=P\{\exists n>0:X_{n}=i|X_{0}=j\}=P\{\exists n>0:X_{n}=0|X_{0}=|j-i|\}=\begin{cases}
1 & |j-i|=k=0\\
1-\frac{k}{K} & |j-i|=k\in\{1,..,K-1\}\\
0 & |j-i|=k=K
\end{cases}$ I somehow concluded also that $\rho_{ji}$ for $j<i$ should be equal
to to the expression I got fior $j>i$ case, since intuitively, it sounds right to me. but turns out, according to an official solution (which wasn't really detailed), that the final answer for case $j<i$ (moving to the right) should be: $\rho_{ji}={ {\frac{j}{i}} }$ I can't figure out why, and I would be glad for some enlightenment. Note: it's possible that there is a mistake in the official solution, and I hope you can help me verify that. Modification: Thanks to Misha's Answer, I figured out that the reason behind replacing $K$ with $K-i$ is the following: for example for $i<j:$ we are interested in defining a ""new"" Markov Chain depending on the distance we want to move from $j$ to $i$ . (from right to left) for example if $K=6$ and we want to move from $state$ $j=5$ to $state$ $i=2$ , its equivalent to moving from $state$ $j=$$3$ to $state$ $i=0$ in a ""smaller""
Markov Chain that has the state $K-i$ at it's far right instead of $K.$ (which is the state we are never interested to visit, since our goal is to meet the wanted state which happens to be on the left). in other words, the Markov chain $\{0,...,K-i\}$ includes all the
possible traveling distances to state $0$ starting from the state $\{K-i-1\}$ , and the largest distance is moving $\{K-i-1\}$ $states$ down to state $zero$ , which is equivalent to the largest traveling distance from $state$ $j=5$ down to $state$ $i=2$ , which is $\{j-i\}$ .","['random-walk', 'markov-chains', 'stochastic-processes', 'markov-process', 'probability']"
3716552,Distribution of sum of discrete random variables and central limit theorem,"Let $X_1 , X_2,\ldots$ be iid random variables such that $P(X_i=1)=P(X_i=-1)=\frac{1}{2}$ . If $S_n=\sum X_i$ prove that: $$\lim_{n\to \infty} P(S_n=k^2\text{ for some } k) =0$$ and find this limit: $$\lim_{n\to \infty}\frac {\ln P\left(\frac {S_n}{n}>t\right)}{n}$$ this is from the
""Theory of Probability and Random Processes"" book and i try to find  the solution and apply central limit theorem then we have $\frac {S_n}{\sqrt{n}}$ converges to normal distribution (weak convergence)","['central-limit-theorem', 'weak-convergence', 'probability-distributions', 'probability-theory', 'probability']"
3716576,Nested intervals theorem - a special case on open intervals,"I learned the nested intervals theorem in the class: If $I_n\ (n\in\Bbb N)$ is a sequence of bounded closed intervals, i.e. $[a_n,b_n]$ , then $\bigcap I_n\ (n\in\Bbb N)\neq\varnothing$ . In the proof of the theorem, we used another theorem: monotone non-decreasing sequence (in this case $a_n$ ) is convergent, if bounded above. And we discussed in the class that the theorem does not hold for open intervals, i.e. $(a_n,b_n)$ . We made a counterexample $I_n=(0,\frac{1}{n}),\ n\in\Bbb N$ , where $\bigcap I_n=\varnothing$ . It also makes sense to me. There is no real staying in the intersection of nested intervals, because the candidate $0$ is ruled out by the open intervals. Then, the professor gave us an extended question: What if we have nested $I_n$ as open intervals $(a_n,b_n)$ , but this time, let $a_n$ be a strictly increasing sequence ( $\forall n\in\Bbb N,\ a_n\lt a_{n+1}$ ) and $b_n$ a strictly decreasing sequence? It seems to me that there would be a real finally staying in the nested interval. However, I cannot prove it. Appreciated if anyone can provide me a hint. Thanks.","['analysis', 'real-analysis']"
3716595,"Line integral for vector field $F(x,y)=\left(-\frac{y}{x^2 + y^2} , \frac{x}{x^2+y^2}\right)$ with the curve $\vert x \vert + \vert y \vert = 5$","$Q)$ For a vector field $F(x,y)=\left(-\frac{y}{x^2 + y^2} , \frac{x}{x^2+y^2}\right)$ in $\mathbb{R}^2$ Find the $\int_{\vert x \vert + \vert y \vert = 5 } -\frac{y}{x^2 + y^2} dx + \frac{x}{x^2 + y^2}dy $ In my lecture's note he claim that $\int_{\vert x \vert + \vert y \vert = 5 } -\frac{y}{x^2 + y^2} dx + \frac{x}{x^2 + y^2}dy $ = $\int_{x^2 +  y^2 = 1 } -\frac{y}{x^2 + y^2} dx + \frac{x}{x^2 + y^2}dy $ And then, he solved by parameterizing curve $x^2+y^2=1$ to $(cost, sint), [0 \leq t \leq 2\pi]$ But my doubt is firstly the vector field, $F$ is not defined at $(0,0)$ , Hence it is not simply connected. So we can't say the not conservative of the $F$ though $curl F = (0,0)$ . Therefore we can't guarantee the path independence, we can't claim that $\int_{\vert x \vert + \vert y \vert = 5 } = \int_{x^2 + y^2 = 1 } $ . Is my opinion right? If the lecture is right what is his ground justifying $\int_{\vert x \vert + \vert y \vert = 5 } = \int_{x^2 + y^2 = 1 } $ ? If the lecture's solution is false, How to Solve it? Any help would be appreciated. thanks.",['multivariable-calculus']
3716684,In which sense modules generalize vector bundles?,"At https://ncatlab.org/nlab/show/module , we find the following statement: The theory of monoids or rings and their modules, its “meaning” and usage, is naturally understood via the duality between algebra and geometry: A ring R is to be thought of as the ring of functions on some space An R-module is to be thought of as the space of sections of a vector bundle on that space. I am very fascinated by the seeming depth of this fact and I would like to understand this better. I think I have the necessary ""vocabulary"" in the sense that I do know what a ring, a module, a vector bundle is, but sadly the entry is quite vague in the following. How do we identify the space in 1? How do we identify the vector bundle in 2? It seems to me, these realization problems depend on the situation, especially from the chosen geometric context. Anyhow, the entry cites the fact that for algebraic varieties this can be made precise, and this assures the generality of the statement. Any brief reference (such as a pamphlet or lecture notes) or help in understanding better would be grateful.","['category-theory', 'reference-request', 'algebraic-geometry', 'soft-question', 'commutative-algebra']"
3716741,"Finding MLE estimator for given density $f(x, \alpha, \beta)$","I'm having trouble with the following example problem of MLE: Let $X = (X_1, ..., X_n)$ be a trial from i.i.d r.v. with density: $$
g(x) = \frac{\alpha}{x^2}\mathbb{1}_{[\beta, \infty)}(x)
$$ where $\beta> 0$ . Write $\alpha$ in terms of $\beta$ to obtain $f(x, \beta)$ Find its likelihood function and draw its graph Using above result get MLE estimator of $\beta$ Could anyone give me a hint on the first task? I'm banging my head against the wall but can't see how $\alpha$ may be written only in terms of $\beta$ . I derived $L$ as $$
L(\textbf{x}, \alpha, \beta) = \frac{\alpha^n}{\prod_\limits{i=1}^n x_i^2}\mathbb{1}_{[\beta, \infty)(X(1))}
$$ to maybe find some clues there but without any meaningful effect.","['statistics', 'log-likelihood', 'maximum-likelihood']"
3716851,"Finding $\lim_{n\to\infty} \frac1{3^n}\left(a^{\frac{1}{n}}+b^{\frac{1}{n}}+c^{\frac{1}{n}} \right)^n$ where $a,b,c>0$","$$\lim_{n\to\infty} \bigg(\frac{a^{1/n}+b^{1/n}+c^{1/n}}{3} \bigg)^n, \quad \textrm{$a>0$, $b>0$ and $c>0$.}$$ I had an idea to present the terms and decompose them as $1+(a-1), 1+(b-1)$ and $1+(c-1)$ , but in this way I got to the answer $\exp(\frac{1}{3}(a+b+c-1))$ while the correct answer is $(abc)^{1/3}$ .","['limits', 'calculus']"
3716875,On the Example of Vector-Valued Integrals,"Let $p>1$ . I am looking for a sequence $(f_{n}(x))$ such that \begin{align*}
\left(\sum_{n}|f_{n}(x)|^{2}\right)^{1/2}&\notin L^{p}(\mathbb{R}^{N}),\\
\sum_{n}a_{n}f_{n}(x)&\in L^{p}(\mathbb{R}^{N}),~~~~\text{for any}~(a_{n})\in l^{2}~\text{with}~\|(a_{n})\|_{l^{2}}\leq 1
\end{align*} so that \begin{align*}
\sup_{\|(a_{n})\|_{l^{2}}\leq 1}\left\|\sum_{n}a_{n}f_{n}(x)\right\|_{L^{p}(\mathbb{R}^{N})}<\infty.
\end{align*} The tricky thing is that, the space $l^{2}$ is self-dual, but when we treat it as the inner one but taking $L^{p}$ integral as outer one, the situation is different. I also think about the Khintchine inequality , but this does not help. Any suggestion?","['measure-theory', 'functional-analysis', 'real-analysis']"
3716935,Number of ways to select the election dates,"A general election is to be scheduled on 5 days in May such that it is not scheduled on two consecutive days. In how many ways can the 5 days be chosen to hold the election? My approach There are 31 days in MAY month $5$ days are election days and 26 days are non election days. Set aside
5 days 26 non election days will create 27 places to insert 5 election days, like: first gap $\mathrm{d}_{1} \mathrm{d}_{2} \mathrm{d}_{3}$ $\mathrm{d}_4 \mathrm{d}_5 \mathrm{d}_6 \mathrm{d}_7 \mathrm{d}_8 \mathrm{d}_9 ........ 
\mathrm{d}_{26}$ last gap Total 27 gaps to insert 5 days : number of ways $=27$ c 5 Am I going correctly any another approach ??? Thanks in advance!","['solution-verification', 'combinatorics', 'discrete-mathematics']"
3716982,"what is a ""powerset"" with base larger than 2?","A powerset $P(S) $ of some set $S$ can be treated as all different possible ways of partition $S$ into 2 ordered pair of disjoint subsets. And I'm curious what is the equivalence of partition $S$ into more than 2 subsets? For example, partition the set S into 3 ordered pair of disjoint subsets. With $|S| \geq 3$ , The cardinality should be $3^{|S|}$ ? And moreover what is the name of arbitrary base powersets?","['elementary-set-theory', 'combinatorics']"
3717003,"If $U \leq \mathbb{R}^4$, $\dim(U) =3$ and $\langle(0,0,0,1)\rangle \cap U = \{0\}$ then $U = \langle(1,0,0,0), (0,1,0,0), (0,0,1,0)\rangle$","Suppose $U \leq \mathbb{R}^4$ , $\operatorname{dim}(U)=3$ and $\langle(0,0,0,1)\rangle \cap U =  \{0\}$ . Is it then true that $U = \langle(1,0,0,0), (0,1,0,0), (0,0,1,0)\rangle$ ? I say yes. Here's my reasoning:
Put the three basis vectors of $U$ into RRE form. That is consider the matrix with the three basis vectors as rows. The only RRE form that doesn't contain $(0,0,0,1)$ is the matrix $\begin{bmatrix}1&0 &0 & 0\\0&1&0 &0 \\ 0 &0 &1 &0\end{bmatrix}$ . This proves the claim. Is this correct?","['matrices', 'solution-verification', 'linear-algebra']"
3717033,How to evaluate $\iint_R \sin(\frac{y-x}{y+x})dydx$ with Jacobian substitution?,"I want to calculate this integral with substitution $x=u+v , \ y=u-v$ : $$\iint_R \sin\left(\frac{y-x}{y+x}\right)dydx$$ $$R:= \{(x,y):x+y≤\pi, y≥0,x≥0\}$$ but I don't know how to set new bounds for $u$ and $v$ .","['integration', 'bounds-of-integration', 'jacobian', 'multivariable-calculus', 'substitution']"
3717038,Fatou lemma for $\{f_n+g_n \}$,"Let $(E,\mathcal {A },\mu) $ be a finite measure space. Take $\{f_n\}$ and $\{g_n\} $ two integrables sequences such that $\{f_n\}$ is positive. Can we say that $$
\int_E \liminf_n\big ( f_n+g_n\big )d\mu\leq  \liminf_n\int_E \big (f_n+g_n\big )d\mu
$$","['integration', 'measure-theory', 'probability-theory', 'probability']"
3717046,Integrate probability of derivative with respect to time to find probability with respect to time,"I'm currently working on modelling systems with both continuous and discrete random variables that operate in continuous time, and I've run into a problem that I don't know how to solve or search for on a search engine.  I've also not encountered anything quite like it in any of my math classes in all my years in college, so I don't know what domain this sort of problem belongs to or what the proper terminology or notation is. I have a random variable $X$ that varies with time $t$ .  I have a probability density function of the form $P \left[ \frac{d X}{d t} = \frac{d x}{d t} | X = x \right] (t)$ .  This is meant to represent the probability that the derivative of $X$ is $\frac{d x}{d t}$ at time $t$ , given that the current value of $X$ is $x$ .  Let's assume that I can integrate and differentiate this function. I ultimately want to know how to obtain $P \left[ X = x \right] (t)$ (or the probability that $X$ has value $x$ at time $t$ ).  I have the initial conditions $P \left[ X = x \right] (0)$ .  I would like to take the approach of finding $\frac{d P \left [ X = x \right] (t)}{d t}$ and then integrating, if possible. Is it possible to obtain $\frac{d P \left[ X = x \right] (t)}{d t}$ from $P \left[ \frac{d X}{d t} = \frac{d x}{d t} | X = x \right] (t)$ ?  I suspect that it is not, but I'd love to be wrong. If so, how? If not, how else might I obtain $P \left[ X = x \right] (t)$ from $P \left[ \frac{d X}{d t} = \frac{d x}{d t} | X = x \right] (t)$ ? I'd like answers that generalize to dealing with higher order derivatives and systems of equations that relate multiple random variables.  In addition, some context on what domain this sort of problem belongs to would be appreciated. EDIT: I clarified a bit what I'm asking for.  I realized that I had some conditional probability in my problem, which might make things easier or harder. I can construct a recursive expression that seems like it would be a step toward obtaining $P \left[ X = x \right] (t)$ directly, but I am then confronted with an integral that I don't see how to solve. $$
P \left[ X = x \right] (t) = \int_{-\infty}^{x} P \left[ X = p \right] (t - d t) \cdot P \left[ \frac{d X}{d t} = \frac{x - p}{d t} | X = p \right] (t - d t) \cdot d p
$$","['derivatives', 'probability', 'random-variables']"
3717074,Does the product rule for differentiation has anything to do with $\sin( \alpha + \beta)$?,"When learning about differentation, I came along the product rule: $$D(f \cdot g) = f \cdot Dg + g \cdot Df$$ I immediately thought of this rule from trigonometry: $$ \sin( \alpha + \beta ) = \sin(\alpha) \cdot \cos(\beta) + \cos(\alpha) \cdot \sin(\beta)$$ Is there any relation between these two rules? How can this similarity be explained? Has it something to do with geometry? For those who don't see the relation that I see: if $\alpha$ is the 'normal' function and $\beta$ is the derivative, then you 'get': $$\sin \cdot D\cos + \cos \cdot D\sin$$ And thus if we name sine $f$ and cosine $g$ , we 'get': $$f \cdot Dg + g \cdot Df$$ Of course, I write 'get' but I know you can't do this like that, or can you? That's exactly my question: is there any relation between those two rules/equations stated the top? Or is this some weird thought in my brain? Thanks in advance!","['trigonometry', 'functions', 'derivatives']"
3717101,"If $\mathrm{M,N}$ are $3\times 2, 2 \times 3$ matrices such that $\mathrm{MN}=$ is given. Then $\mathrm{det(NM)}$ is?","If $\mathrm{M,N}$ are $3\times 2, 2 \times 3$ matrices such that $\mathrm{MN}=\pmatrix{8& 2 & -2\\2& 5& 4\\-2& 4&5}$ , then $\mathrm{det(NM)}$ is? ( $\mathrm{NM}$ is invertible.) $\mathrm{det(MN)}$ must be (and is) zero. But how to find $\mathrm{det(NM)}$ ? Any hint?","['matrices', 'linear-algebra']"
3717109,"What is the best way to partition the $4$-subsets of $\{1,2,3,\dots,n\}$?","Also asked on MO: What is the best way to partition the $4$ -subsets of $\{1,2,3,\dots,n\}$ ? . Consider the set $X = \{1,2,3,\dots,n\}$ . Define the collection of all $4$ -subsets of $X$ by $$\mathcal A=\{Y\subset X: Y\text{ contains exactly $4$ elements}.\}$$ I want to partition $\mathcal A$ into groups $A_1,A_2,\dots, A_m\subset \mathcal A$ (each of them is a collection of $4$ -subsets of $X$ ) such that $\bigcup_{i=1}^m A_i=\mathcal A$ and such that the intersection of any two distinct $4$ -subsets in each $A_k$ has cardinality at most $1$ , i.e. such that for all $i\in\{1,\dots,m\}$ and $Y_1, Y_2\in A_i$ , we have $$Y_1\neq Y_2 \implies \lvert Y_1\cap Y_2\rvert \le 1.$$ My question: What can be said about the smallest $m$ (depending on $n$ ) such that such a partition exists? My thoughts: I was expecting that each $A_i$ can contain ""roughly"" $\frac n4$ elements, so we would have $$m(n)=\Theta\left(\frac{\binom n4}{\frac n4}\right)=\Theta(n^3).$$ In particular, we would have $m(n)\le c n^3$ for some constant $c\in\mathbb R$ . However, I am neither sure if this is correct, nor how to formalize this.","['set-partition', 'combinatorics', 'extremal-combinatorics', 'coloring']"
3717146,Property of solution to initial value problem,"Let $f\colon [0,\infty) \to [0, \infty)$ be a continuous function such that $\int_0^\infty f(t) \, dt = \infty$ . Assume that $y \colon [0, \infty) \to \mathbb R$ is a solution to the initial value problem $$\begin{cases}y'' + f(t) y = 0 \\ y(0) = 1 \end{cases}. $$ I want to show that $y$ has infinitely many zeros that do not accumulate anywhere and at each zero the derivative of $y$ is non-vanishing. Furthermore, between the zeros $y$ is either positive and concave or negative and convex. Thoughts so far: I could not come up with something smart that uses the integral assumption on $f$ . Clearly the theorem holds for constant $f$ , e.g. $f = 1$ . Then the solutions are of the form $y(t) = A\sin(t) + B\cos(t)$ and satisfy the desired properties. Plotting the solution for other choices of $f$ give similar trigonometric patterns of the solution, but I could not make it further.","['derivatives', 'analysis', 'ordinary-differential-equations', 'real-analysis']"
3717161,Find fundamental matrix for the equation $\dot{y}=A(t)y$.,"So I want to find the fundamental matrix for the equation \begin{equation}\begin{split}\dot{y}=A(t)y,\quad \text{where }A(t)=\begin{pmatrix} t & 4\\ -1 & t\end{pmatrix}.\qquad (1)\end{split}\end{equation} So far I have computed the fundamental matrix for $\dot{y}=By$ where $B=\begin{pmatrix} 0&4 \\ -1 &0\end{pmatrix}$ and found it to be $$\begin{pmatrix} \cos(2t)&2\sin(2t)\\ -\frac{\sin(2t)}{2}&\cos(2t)\end{pmatrix}.$$ I have also found the fundamental matrix for $\dot{y}=ty$ which is $e^{\frac{t^2}{2}}$ (EDIT: I suppose the fundamental matrix for this equation is $\begin{pmatrix} e^{\frac{t^2}{2}}& 0\\ 0&e^{\frac{t^2}{2}}\end{pmatrix}$ ). As a hint I been told to use these two results to find the fundamental matrix of (1), but I'm not sure how to do it and could use some help/hints.","['ordinary-differential-equations', 'fundamental-solution']"
3717254,Dimension theorem (in infinite dimension) - all basis of any vector space have the same cardinality.,"I am reading a note from author Justin Tatch Moore, in which he proves the Dimension Theorem, which says that all the basis of a vector space have the same cardinality (in infinite dimension), using Zorn's Lemma. The text can be found HERE! The idea is basically as follows: suppose that $A$ and $B$ are basis of the vector space $V$ , and define $P$ to be the set consisting of all linear transformations $T$ from a subspace $W$ of $V$ onto $W$ such that: $A \cap W$ and $B \cap W$ are each a basis for $W$ , and the restriction of $T$ to $A \cap W$ is a bijection between $A \cap W$ and $B \cap W.$ then we show using Zorn's Lemma that $P$ has a maximal element, which is a transformation $ T: W \rightarrow W $ with the properties (1) and (2). Note that if we show that $ W = V $ , the theorem ends due to the propertie (2). Hence to do this the argument starts as in the figure below My question is how to build two sets of sequences with the above four properties? The first two properties seem very reasonable, but the last two I have no idea how to do it. Can anyone help me with this?","['proof-explanation', 'linear-algebra', 'vector-spaces']"
3717258,"Generic behavior amongst ""polynomialish"" models of $\mathsf{Q}$","Now asked at MO . For $\mathcal{R}=(R_i)_{i\in\mathbb{N}}$ a sequence of countable commutative ordered rings such that $R_i$ is an sub-ordered ring of $R_{i+1}$ and $R_0=\mathbb{Z}$ , let $$Poly_\mathcal{R}=\{\sum_{i<n}a_ix^i\in (\bigcup_{i\in\omega}R_i)[x]: a_i\in R_i\}$$ be the set of polynomials whose $i$ th term comes from the $i$ th ring in the sequence. $Poly_\mathcal{R}$ naturally inherits the structure of an ordered ring, namely by taking as the set of positive elements all those $p$ whose leading coefficient is positive in the sense of the appropriate $R_i$ . For example, we'll have $x>1$ since the leading coefficient of $x-1$ is $1$ which is positive. The set $Poly_\mathcal{R}^+$ of nonnegative elements of $Poly_\mathcal{R}$ is a nonstandard model of Robinson's $\mathsf{Q}$ . Unsurprisingly the theory of $Poly^+_\mathcal{R}$ depends on $\mathcal{R}$ . I'm interested in the ""generic"" behavior of such models. To make this precise, let $\mathfrak{X}$ be the space of all such ordered ring sequences topologized by taking as basic open sets of those of the form $$[(R_i)_{i<n}]:=\{\mathcal{S}=(S_i)_{i\in\omega}: \forall i<n(S_i=R_i)\}$$ for each finite sequence of ordered rings $(R_i)_{i<n}$ . (Fine, this $\mathfrak{X}$ is a proper-class-sized space; do any of the usual tricks to fix this.) Say that a sentence $\varphi$ in the language of Robinson arithmetic is generically true (resp. false ) iff $\{\mathcal{R}: Poly_\mathcal{R}^+\models\varphi\}$ is comeager in $\mathfrak{X}$ (resp. meager). For example, ""Every number is either even or odd"" is neither generically true nor generically false. On the other hand, the more complicated sentence $(*)\quad$ ""There is some $k$ such that for all $n>k$ there is some $m<k$ such that $n+m$ is even"" is generically true. Given a (code for a) basic open set $(R_i)_{i<n}$ , consider the extension $(S_i)_{i<n+1}$ with $S_i=R_i$ for $i<n$ and $S_n$ be some ring containing $R_{n-1}$ as a subring in which $2x=1$ has a solution. Then every $\mathcal{R}\in[(S_i)_{i<n+1}]$ has $Poly_\mathcal{R}^+\models(*)$ : just take $k=x^n$ . (Note that this is nontrivial since $Poly_{(\mathbb{Z},\mathbb{Z},\mathbb{Z}, ...)}^+\models\neg(*)$ .) In general, many basic facts about modular arithmetic have ""perturbed"" versions a la $(*)$ above which are generically true. I'm interested in unpacking this more. Suppose we have a sentence of the form $$\varphi\equiv\forall x_1,...,x_n\exists y_1,...,y_k\theta(x_1,...,x_n,y_1,...,y_k)$$ with $\theta$ quantifier-free. Define the perturbed version of $\varphi$ , which I'll denote by "" $\varphi^\circ$ ,"" as: $$\exists u\forall x_1,...,x_n\exists x_1',...,x_n'\mbox{ such that }$$ $$\bigwedge_{i\le n}\lfloor{x_i\over u}\rfloor\le x_i'\le x_iu\quad\mbox{ and }$$ $$\quad\exists y_1,...,y_k(\theta(x_1', ..., x_n', y_1,...,y_k)).$$ The idea is that $\varphi^\circ$ doesn't require $\exists\overline{y}\theta$ to hold on every tuple of $x$ s; we're allowed to perturb the inputs by up to a fixed factor $u$ . Note that by ""disjointifying"" the variables involved we have in an appropriate sense that the perturbation of a conjunction is the conjunction of the perturbations. In the original version of this question I didn't notice that variable-disjointification did this, and since commutation with conjunctions seems an obviously desirable property I used a much nastier notion of perturbation which I now realize was silly. My question is: Suppose $\varphi$ is an $\forall\exists$ -sentence in the language of Robinson arithmetic and $\mathbb{N}\models\varphi$ . Is $\varphi^\circ$ generically true? (We can also consider ""perturbed versions"" of higher-complexity sentences, but the $\forall\exists$ -case seems already interesting. However, the particular question above is boring for higher-complexity sentences: ""There is some $k$ such that for every $n$ there is some $m\in (\lfloor {n\over k}\rfloor, nk)$ such that $m$ is not even"" is generically false .)","['ring-theory', 'model-theory', 'abstract-algebra', 'logic']"
3717298,"What are such ""off-by-a-factor"" ring maps called?","Suppose $R,S$ are ( non -unital) rings. What is the term for a function $f:R\rightarrow S$ such that $f(a+b)=f(a)+f(b)$ for all $a,b\in R$ , and there is some $u\in S$ such that for all $a,b\in R$ we have $uf(ab)=f(a)f(b)$ ? I'm currently calling these ""weak homomorphisms"" (and ""weak embeddings"" in the injective case), but I suspect they have an actual name. For example, let $R=\mathbb{Q}$ and let $S$ be the ring of polynomials with rational coefficients whose constant terms are integers. The map $R\rightarrow S:q\mapsto qx$ is of course not a homomorphism, and indeed there is no homomorphism from $R$ to $S$ , but it does satisfy the weaker property above via $u=x$ . (I'm primarily running into this notion in the context of certain models of Robinson arithmetic , the idea being that ""sufficiently generic"" such models admit weak embeddings from lots of rings and this leads to some interesting structural properties, but I'm also interested in them in other contexts - including non-unital ones.)","['ring-theory', 'abstract-algebra', 'terminology']"
3717300,Proving $(A - B) \times (C - D) = (A \times C - B \times C) - A \times D$,"I am trying to prove $(A - B) \times (C - D) = (A \times C - B \times C) - A \times D$ using biconditionals, but cannot quite get there. For any ordered tuple $(\alpha, \beta)$ , we have: \begin{align*}
(\alpha, \beta) \in (A - B) \times (C - D) & \iff \alpha \in (A - B) \land \beta \in (C - D) \\
& \iff (\alpha \in A \land \alpha \not \in \beta) \land (\beta \in C \land \beta \not \in D) \\
& \iff ((\alpha \in A \land \alpha \not \in \beta) \land \beta \in C) \land (\beta \not \in D) \\
& \iff ((\alpha \in A \land \alpha \not \in \beta) \land \beta \in C) \land (\alpha \in A \land \beta \not \in D) \\
& \iff ((\alpha \in A \land \beta \in C) \land (\alpha \not \in B \land \beta \in C)) \land (\alpha \in A \land \beta \not \in D) \\ 
& \iff (((\alpha, \beta) \in A \times B) \land (\alpha \not \in B \land \beta \in C)) \land (\alpha, \beta) \not \in A - D.
\end{align*} At this point, I do not know how to finish. $\alpha \not \in B$ and $\beta \in C$ does imply that $\alpha \not \in B \times C$ , but the converse isn't true since we may have $\beta \not \in C$ .","['elementary-set-theory', 'proof-explanation']"
3717304,"verifying the result $\int_{B^4}e^{x^2+y^2-z^2-w^2}\,dx\,dy\,dz\,dw=\pi^2(\sinh(1)+1-\frac{1}{e})$","the problem is: $$I=\int_{B^4} e^{x^2+y^2-z^2-w^2} \,dx\,dy\,dz\,dw $$ where $B^4$ is the unit 4 ball, explicitly: $$ B^4=\{(x,y,z,w)\in\Bbb{R}^4 \mid x^2+y^2+z^2+w^2\leq 1\}$$ here is my solution: using fubini's theorem we obtain: $$I=\int_{B^2}e^{-z^2-w^2}\left(\int_{{B^2}_{\sqrt{1-z^2-w^2}}} e^{x^2+y^2} \, dx\, dy\right)\,dz\,dw$$ where $B^2$ is the unit 2 ball and ${B^2}_{\sqrt{1-z^2-w^2}}$ is the 2 ball centred at the origin with radius $\sqrt{1-z^2-w^2}$ . next we calculate $$\int_{{B^2}_{\sqrt{1-z^2-w^2}}}e^{x^2+y^2}dxdy$$ using polar cordinates, so: $$\int_{{B^2}_{\sqrt{1-z^2-w^2}}}e^{x^2+y^2}dxdy=\int_{0}^{2\pi}\int_{0}^{\sqrt{1-z^2-w^2}}e^{r^2}rdrd\theta=\pi(e^{1-z^2-w^2}-1)$$ plugging this result into the outer integral we get: $$I=\int_{B^2}e^{-z^2-w^2}(\pi(e^{1-z^2-w^2}-1))dzdw=\pi e \int_{B^2}e^{-2(z^2+w^2)}dzdw-\pi \int_{B^2}e^{-z^2-w^2}dzdw$$ again using polar cordinates we receive: $$\int_{B^2}e^{-2(z^2+w^2)}dzdw=\int_{0}^{2\pi}\int_{0}^{1}e^{-2r^2}rdrd\theta=\frac{\pi}{2}(1-e^{-2})$$ and: $$\int_{B^2}e^{-z^2-w^2}dzdw=\int_0^{2\pi}\int_0^1re^{-r^2}drd\theta=\pi(1-\frac{1}{e})$$ plugging this in we get: $$I=\pi^2(\sinh(1)+1-\frac{1}{e})$$ is this solution correct or have I made a mistake along the way?","['integration', 'multivariable-calculus', 'multiple-integral']"
3717365,Playing with the Möbius strip... mathematically,"On the internet you often see these animations, where a Möbius strip gets cut along the center line, or other fun stuff. But I have never seen a mathematical description of what is happening there. What is the technique used to ""cut"" a topological space in half, like a sphere. In my textbook there is the following task: A Möbius strip arises from gluing the ends of a paper strip after twisting them in a half turn. More generally is the $n/2$ -twisted Möbius strip obtained after twisting, and gluing the ends n/2 times ( $n\in\mathbb{Z}$ ).
How many parts do you get after cutting in the middle along the direction of the strip. Are the parts again twisted Möbius strips? I understand this task as ""get a pair of scissors and some glue, and start playing around"" what I did. But how can you mathematically describe what is happening, and prove the results?
What is the mathematics behind these constructions? Twisting is simple, with an equivalence relation. But besides that? Thanks in advance.","['general-topology', 'soft-question']"
3717399,"Random walk : probability of return in $\leq N$ steps, equivalent as $N\to\infty$","It is well known that in dimension 1, the probability of at least one return to the origin in $N$ steps or less is $$ q_{N}=1-{1 \over {2^{2N}}}{\binom {2N}{N}}\,.$$ Using Stirling's formula one has the equivalent $$ q_{N}\sim  1 - {1 \over {\sqrt {\pi N}}} $$ which explains that the convergence is slowish, and is consistent with the fact that the expected number of steps (which is actually $\sum 1-q_N$ ) needed to return to the origin is not finite. I was wondering if we could obtain a similar equivalent in dimension 2 ? I expect convergence to be much slower, maybe involving $\frac 1{\log(N)}$ or even $\frac 1{\log(\log(N))}$ ? If one writes $u_n$ for the probability to be at the origin at step $n$ and $f_n$ for the probability of a first return at step $n$ , then the sequences $u$ and $f$ are linked through a generating series, as already pointed out in Tom Boardman's answer here Proving that 1- and 2-d simple symmetric random walks return to the origin with probability 1 In the document Tom Boardman linked http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter12.pdf , pages 3-8 the $f_n$ are computed in dimension $1$ but not $2$ . Knowing the $f_n$ , we would just have to compute an equivalent of their sum from $1$ to $N$ as $N\to \infty$ (the series $w$ in the document), but I'm not sure these computations are ""easily"" doable. Does anybody know if this has already been done, or if there is an easier way to answer my question in bold ? Thanks a lot","['random-walk', 'probability-theory', 'probability']"
3717438,What type of differential equation is $f'(x) = f(x/2)$ and how do you solve it?,"I have the following different equation $$f'(x) = f(x/2)$$ with $f(0)=10$ . What type of DE is this, and how would you solve it? It seems $f(x)$ is likely to be some relative of $e^x$ , since $f'(x) = f(x)$ , which is close, but I don't even know what that type of DE is called with that $f(x/2)$ feature, so I'm not having any luck searching for a tutorial. The best candidate I've found was a ""delay differential equation"", but that seems more suited to $f(x-3)$ than $f(x/2)$ .",['ordinary-differential-equations']
3717482,"Problem in finding $\delta$ to prove that $\lim_{x\to a}[\lim_{y\to b}f(x,y)]=\lim_{y\to b}[\lim_{x\to a}f(x,y)]=L$","Theorem: Let $f$ be a vector field. If $\lim_{(x,y)\to(a,b)}f(x,y)=L$ and $1$ D limits $\lim_{x\to a}f(x,y)$ and $\lim_{y\to b}f(x,y)$ exist then, $\lim_{x\to a}[\lim_{y\to b}f(x,y)]=\lim_{y\to b}[\lim_{x\to a}f(x,y)]=L$ . I tried to prove it as follows: Geometrically, let $\lim_{x\to a}f(x,y)=g(y)$ , whence it follows that we fix some $y$ arbitrarily and then approach $a$ along $x$ -axis to get $\lim_{x\to a}f(x,y)$ . Similarly for the other $1$ D limit.  Then since the limit $\lim_{(x,y)\to(a,b)}f(x,y)=L$ exists, it doesn't matter how we approach $(a,b)$ and hence the result is verified. Is my interpretation correct? Now coming to the proof: Let $\lim_{y\to b}f(x,y)=h(x)$ . Since $\lim_{(x,y)\to(a,b)}f(x,y)=L$ , it follows that $\forall \epsilon_1 \gt 0, \exists \delta_1 \gt 0$ such that if $0\lt \sqrt{(x-a)^2+(y-b)^2}\lt \delta_1$ , then $||f(x,y)-L||\lt \epsilon_1 \tag{1}$ Since $\lim_{x\to a}f(x,y)=g(y)$ , it follows that $\forall \epsilon _2\gt 0, \exists \delta_2\gt 0$ such that if $0\lt |x-a|\lt \delta_2$ , then $||f(x,y)-g(y)||\lt \epsilon_2 \tag{2}$ Similarly, $\forall \epsilon _3\gt 0, \exists \delta_3\gt 0$ such that if $0\lt |y-b|\lt \delta_3$ , then $||f(x,y)-h(x)||\lt \epsilon_3\tag {3}$ From (2)& (3), it follows that $\require{enclose}
     \enclose{horizontalstrike}{||g(y)-h(x)||\le ||f(x,y)-g(y)||+||f(x,y)-h(x)||\lt\epsilon_2+\epsilon_3}$ , whenever $\require{enclose}
     \enclose{horizontalstrike}{0\lt |y-b|\lt \delta_3}$ and $\require{enclose}
     \enclose{horizontalstrike}{0\lt |x-a|\lt \delta_2}$ That is by taking $\require{enclose}
     \enclose{horizontalstrike}{\epsilon_2=\epsilon_3=\epsilon/2}$ , we have $\require{enclose}
     \enclose{horizontalstrike}{||g(y)-h(x)||\lt\epsilon}$ , whenever $\require{enclose}
     \enclose{horizontalstrike}{0\lt \sqrt{(x-a)^2+(y-b)^2} \lt  \sqrt{\delta_2^2+\delta_3^2}}$ Since $\lim_{x\to a}[\lim_{y\to b}f(x,y)]=\lim_{x\to a}h(x)$ , consider $||h(x)-L||\lt||h(x)-f(x,y)||+||f(x,y)-L|\lt \epsilon_3+\epsilon_1$ , whenever $0\lt |y-b|\lt \delta_3$ and $0\lt \sqrt{(x-a)^2+(y-b)^2}\lt \delta_1$ whence by taking $\epsilon_3=\epsilon_1=\epsilon/2$ , it would follow that $||h(x)-L||\lt \epsilon$ whenever $0\lt \sqrt{(x-a)^2+(y-b)^2}\lt \delta$ and similarly for $g(y)$ . But the problem is how to get this $\delta$ from $(1)$ and $(3)$ ? I tried the following here: since $0\lt |y-b|\lt \sqrt{(x-a)^2+(y-b)^2}$ , taking $\delta=\min\{\delta_1,\delta_3\}$ will cure the problem. Is this correct? Please help. Thanks for your time.","['real-analysis', 'multivariable-calculus', 'calculus', 'solution-verification', 'limits']"
3717486,Union of nontrivial intervals is a countable sub-union.,"Let $A=\bigcup\limits_{i\in \mathcal I} I_i$ where $\mathcal{I}$ is an indexing set of any size. Each $I_i$ is a nontrivial interval, i.e. an interval with at least two points. The intervals are not guaranteed to be open or closed. So any $I_i\subseteq\Bbb R$ and has the form of either $(a,b), (a,b], [a,b),$ or $[a,b]$ where $a < b, a,b\in\Bbb R$ . I want to prove that $A=\bigcup\limits_{k=1}^\infty I_{i_k}$ for some countable sub-indexing $\{i_k\}_{k=1}^\infty$ . source : exercise in Axler’s Measure, Integration & Real Analysis , section 2D, $\mathcal N\underline{o}\  4$ . My first attempt : We know that any union of disjoint open intervals is a countable union.  Although this isn’t proved and I don’t think it’s stated in Axler’s book, it may be well-known enough to be fair game.  Then, $\forall I_i$ with end-points $a_i, b_i$ we can form the set $E =\bigcup\limits_{i\in \mathcal I} \{a_i,b_i\}$ which is to say the set of all the endpoints. $\forall x_i\in E$ we take $y_i = \inf\{x\in E: x > x_i\}$ and then form the intervals $(x_i,y_i)$ which will be disjoint open intervals. (If necessary we can throw away any empty ones.) This kind of seems like it might be heading in the right direction, but what I’m starting to notice is that I am building a collection of open intervals based on a mixture of end-points of the original intervals.  I don’t see how I’m going to continue this path in order to not just get a countable union, but a countable union of the exact same intervals that were in the original set. Edit: The more I think about this attempt, the more I realize it's doomed in another way.  If $\{I_i\}_{i\in\mathcal I}$ is the set of all intervals with left and right end-points any ordered pair of irrational numbers, then every interval constructed in the procedure above will be empty. My next attempt is to think “Why is this problem in this section? Maybe it has a more measure theoretic solution.” If I take the measure of $A$ it might be infinite.  I could consider taking the measure of $A\cap [n,n+1]$ and argue that some countable collection of intervals covers this. However we don’t know if $A$ is closed so we can’t argue from compactness.  And for all we know, even in this bounded interval, this part of $A$ might be composed of an uncountable collection of intervals.","['measure-theory', 'proof-writing', 'real-analysis']"
3717549,Solve the following constrained maximization problem [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Question: let $T\geq$ 1 be some finite integer, solve the following maximization problem. Maximize $\sum_{t=1}^T$ ( $\frac{1}{2}$ ) $^t$$\sqrt{x_t}$ subject to $\sum_{t=1}^{T}$$x_t\leq1$ , $x_t\geq0$ , t=1,...,T I have never had to maximize summations before and I do not know how to do so. Can someone show me a step by step break down of the solution?","['optimization', 'cauchy-schwarz-inequality', 'derivatives', 'lagrange-multiplier']"
3717604,Solving Inverse trig problems using substitution?,"I have this problem $$\arccos\left(\frac{x+\sqrt{1-x^2}}{\sqrt{2}}\right)$$ The answer comes out be $\arcsin(x)-\frac{\pi}{4}$ I've realized that this problem can be solved by using something called substitution, but i really dont get the idea of how you can just substitute $x$ with $\cos(x),~\sin(x)$ . Or anything else for that matter. Also how do you know what to substitute? Is there a method for that? This has been confusing me a lot and i would appreciate if the answer is not just the solution but also an explanation to how substitution works in brief. Thanks in advance.","['trigonometry', 'functions', 'inverse-function', 'substitution']"
3717628,"Can anyone please help me to understand what does ""well-defined"" mean in the definition of Set?","We know that A set is a well-defined collection of distinct objects, considered as
an object in its own right. Can anyone please help me to understand  what does well-defined mean? Let's say $X = \{ 1 , 2 , 3 , \tan \frac{\pi}{2} \}$ . Is $X$ a set ? $\tan x$ tends to infinity when $x \in (0 , \frac{\pi}{2})$ and $x$ tends to $\frac{\pi}{2}$ . And $\tan x$ tends to minus infinity when $x \in ( \frac{\pi}{2} , \pi )$ and $x$ tends to $\frac{\pi}{2}$ . But we do not have any concrete idea about $\tan \frac{\pi}{2}$ . So it is undefined. So $X$ can not be called a set. Am I correct ?","['elementary-set-theory', 'abstract-algebra']"
3717654,Deriving hyperparameter updates in Online Interactive Collaborative Filtering,"I've been going through "" Online Interactive Collaborative Filtering Using Multi-Armed Bandit with Dependent Arms "" by Wang et al. and am unable to understand how the update equations for the hyperparameters (section 4.3, equation set (23)) were derived. I'd deeply appreciate it if anyone could provide a full or partial derivation of the updates. Any general suggestions regarding how to proceed with the derivation would also be appreciated. ICTR Graphical Model The variables are sampled as below $$\mathbb{p}_m|\lambda \sim \text{Dirichlet}(\lambda)$$ $$\sigma^2_n|\alpha,\beta \sim \text{Inverse-Gamma}(\alpha,\beta)$$ $$\mathbb{q}_n |\mu_{\mathbb{q}}, \Sigma_{\mathbb{q}}, \sigma_n^2 \sim \mathcal{N}(\mu_{\mathbb{q}}, \sigma_n^2\Sigma_{\mathbb{q}})$$ $$\mathbb{\Phi}_k |\eta \sim \text{Dirichlet}(\eta)$$ $$z_{m,t} | \mathbb{p}_m \sim \text{Multinomial}(\mathbb{p}_m)$$ $$x_{m,t} | \mathbb{\Phi}_k \sim \text{Multinomial}(\mathbb{\Phi}_k) $$ $$y_{m,t} \sim \mathcal{N}(\mathbb{p}_m^T\mathbb{q}_n, \sigma_n^2)$$ And the update equations are below","['machine-learning', 'optimization', 'statistics', 'bayesian-network']"
3717664,A formula for Ramanujan's tau function,"In his paper On certain Arithmetical Functions published in Transactions of the Cambridge Philosophical Society , XXII, No. 9, 1916, 159-184, Ramanujan makes some bold claims about the tau function defined as follows: $$\sum_{n=1}^{\infty} \tau(n) q^n=q\prod_{n=1}^{\infty} (1-q^n)^{24}\tag{1}$$ To quote him It appears that $$\sum_{n=1}^{\infty} \frac{\tau(n)} {n^s} =\prod_{p} \frac{1}{1-\tau(p)p^{-s} +p^{11-2s}}\tag{2}$$ This assertion is equivalent to the assertion that, if $$n=p_1^{a_1}p_2^{a_2}\dots p_r^{a_r} $$ where $p_1,p_2,\dots,p_r$ are the prime divisors of $n$ , then $$n^{-11/2}\tau(n)=\frac{\sin((1+a_1)\theta_{p_1})}{\sin\theta_{p_1}}\cdot\frac{\sin((1+a_2)\theta_{p_2})}{\sin\theta_{p_2}}\dots\frac{\sin((1+a_r)\theta_{p_r})}{\sin\theta_{p_r}}\tag{3}$$ where $\cos\theta_p=\frac{1}{2}p^{-11/2}\tau(p)$ . It would follow that, if $n$ and $n'$ are prime to each other, we must have $$\tau(nn') =\tau(n) \tau(n') \tag{4}$$ Let us suppose that $(3)$ is true, and also that (as appears to be highly probable) $$\{2\tau(p)\}^2\leq p^{11}\text{ (see note at the end)} \tag{5}$$ so that $\theta_p$ is real. It is rather very remarkable that Ramanujan starts with a proposed equation $(2)$ without any proof (only God knows how he guessed it) and then draws conclusions like $(3),(4)$ . IMHO Ramanujan uses empirical evidence and his hope that $\theta_p$ should be real to make the bold conjecture $(5)$ which was finally proved by Deligne using very sophisticated tools (of which I have no inkling). Identity $(4)$ was proved by Mordell and his proof is replicated here . Based on these ideas one can prove the identity $(2)$ . My question is regarding equation $(3)$ . It appears that Ramanujan uses some general theory of Dirichlet series and their expression into infinite products to derive $(3)$ and he has used that approach to derive many similar identities based on Dirichlet series in the same paper. Is there any general theory which which allows us to deduce $(3)$ from $(2)$ ? I am hoping that this is the easy part in whatever is presented above and expect some sort of a general proof here which can work of other Dirichlet series and its corresponding infinite product representation. Note : Equation $(5)$ has a typo in the original paper also (or perhaps in my copy of Collected Papers of Ramanujan). It should be fixed as $$\{\tau(p) \} ^{2}\leq 4p^{11}\tag{6}$$","['infinite-product', 'dirichlet-series', 'sequences-and-series']"
3717670,Help making sense of setbuilder notation found in Real Mathematical Analysis by Pugh,"2 Theorem The set $\Bbb R$ , constructed by means of Dedekind cuts, is complete in the sense that it satisfies the Least Upper Bound Property :If $S$ is a nonempty subset of $\Bbb R$ and is bounded above then in $\Bbb R$ there exists a least upper bound for $S$ . Proof : Easy! Let $\mathcal C\subset\Bbb R$ be any nonempty collection of cuts which is bounded above, say by the cut $X | Y$ .
Define $C:=\{a\in\Bbb Q:\text{ for some cut } A | B\in\mathcal C\text{ we have } a\in A\}$ and $D=$ the rest of $\Bbb Q$ . source : Real Mathematical Analysis by Pugh. I'm having trouble understanding what is meant by his definition of $C$ , maybe an example would help? What would $C$ look like if the nonempty collection of cuts was $\{M|N, O|P, Q|R\}$ ? Is it saying that $C$ will take on all rationals from one of the cuts (say $M$ ) from the collection? Or will it have all rationals belonging to $M,O,$ and $Q$ i.e. $C=\{a\in M,O,Q \}$ ?","['elementary-set-theory', 'notation', 'real-analysis']"
3717720,"If $(A-\lambda I)x_0=0,~y_0^{T}(A-\lambda I)=0$ and $y_0^{T}x_0=0$, prove that eigenvalue $\lambda$ is not simple.","Let $A\in M_{n\times n}(\Bbb R), \lambda\in\sigma(A)\cap\Bbb R\setminus\{0\}$ . If $x_0,\,y_0$ are real eigenvectors of $A$ such that $(A-\lambda I)x_0=0$ and $y_0^{T}(A-\lambda I)=0$ and $y_0^{T}x_0=0$ , prove that eigenvalue $\lambda$ is not simple, i.e. has algebraic multiplicity $>1$ . Attempt. We just need to work with the case that geometric multiplicity of lambda is equal to $=1$ . Thanks in advance.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3717744,Shuffling the digits of an integer so that the ratio between the resulting numbers is fixed.,"Suppose we have a 3-digit integer with all digits distinct. If we create its two shift numbers (resulting by rotation of the original digits), the ratio between the two subsequent numbers (if we order them in ascending or descending order) must be fixed.
Find this number. Suppose the number is xyz and the two resulting numbers, if we shift the digits to the right, are yzx and zxy. Assuming $xyz<yzx<zxy$ (there is no equal, since all digits are distinct), we want $\frac{zxy}{yzx} = \frac{yzx}{xyz} = k$ , where k: rational. Furthermore, it is easy to deduce that 0 is excluded, or one of the resulting numbers would be 2-digit. I have found 2 such sets of numbers, {243, 324, 432} and {486, 648, 864} and now I am trying to formulate an algebraic solution. If xyz is the original number, the first rotation is yzx and can be found as follows: $yzx = xyz*10+x-1000*x$ . Also $zxy= yzx*10+y-1000*y$ . Assuming wlg that $xyz<yzx<zxy$ , we must have $\frac{xyz*10+x-1000*x}{xyz} = \frac{yzx*10+y-1000*y}{xyz*10+x-1000*x} = k$ I also noticed that if the 3 numbers are in ascending order $xyz<yzx<zxy$ , then their differences $yzx-xyz$ and $zxy-yzx$ have the same 3 digits in rotation. but I don't think I can advance it any further...","['number-theory', 'combinatorics']"
3717828,"Is a continuous function bounded on a set $\mathbb{R}\times [a,b]$?","Let's consider $$f:\mathbb{R}\times [a,b]\to\mathbb{R}$$ a $C^1$ function such that, for every $y_0\in [a,b]$ we have that $f(x,y_0)\in H^1(\mathbb{R})$ , would that mean that $$\sup_{(x,y)\in \mathbb{R}\times [a,b]}|f(x,y)|$$ is bounded? I believe it's true, but I don't know how to prove it, any hint would be appreciated.","['real-analysis', 'calculus', 'sobolev-spaces', 'functional-analysis', 'supremum-and-infimum']"
3717891,Using residue theorem to calculate integral $\int\limits_0^{2\pi} \frac{dx}{10+6\sin x}$ - where is my mistake?,"I am to calculate: $$
\int\limits_0^{2\pi} \frac{dx}{10+6\sin x}
$$ We can set $\gamma(t)=e^{it}$ for $t \in [0, 2\pi]$ and then $z = e^{it}$ , $\dfrac{dz}{iz}=dt$ , $\sin t =\dfrac{1}{2i}(z-\frac{1}{z})$ so that: $$
\int\limits_0^{2\pi} \frac{dx}{10+6\sin x} = \int\limits_\gamma \frac{dz}{\left(10+\frac{3}{i}(z-\frac{1}{z})\right)iz} = \int\limits_\gamma \frac{dz}{\left(10iz+3z^2-3\right)}
$$ Roots of denominator are $-3i$ and $\frac{-i}{3}$ but since the winding number of $-3i$ is equal to 0 we have: $$
\int\limits_\gamma \frac{dz}{10iz+3z^2-3} = 2 \pi\, i\, Res\left(f,\frac{-i}{3}\right)\cdot1
$$ Calculating residue: $$
Res\left(f,\frac{-i}{3}\right) = \lim_{\large z \to \frac{-i}{3}} \frac{1}{(z+3i)}=\frac{3}{8i}
$$ summing up: $$
\int\limits_0^{2\pi} \frac{dx}{10+6\sin x} = 2\pi i \cdot \frac{3}{8i} = \frac{3\pi}{4}
$$ But wolfram says it is equal to $\dfrac{\pi}{4}$ . Could you help me spot my mistake?","['definite-integrals', 'complex-analysis', 'calculus', 'solution-verification', 'complex-numbers']"
3717917,Finite map $\mathbb{R}^n \rightarrow \mathbb{R}^m$,"Suppose $f :\mathbb{R}^n \rightarrow \mathbb{R}^m$ is continuous and has all of its fibers being finite. I believe it must follow that $n \leq m $ , but I can’t seem to prove it really. The case $m=1$ is kind of easy, since if $f$ is not constant we can choose infinitely many (up to endpoints) disjoint paths from $f(x)$ to $f(y)$ with $f(x) \neq f(y)$ and so by the IVT every value between $f(x)$ and $f(y)$ is now reached infinitely many times. I don’t think this argument can be generalized. Maybe some homology/cohomology/homotopy argument is possible?","['general-topology', 'differential-topology', 'algebraic-topology']"
