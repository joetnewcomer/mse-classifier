question_id,title,body,tags
3868182,Approximation to the monkey typewriter probability,"Say that we are interested in the problem of a monkey randomly typing out a specific string of length m on a keyboard with k keys, in n trials, at least once: $$
1 - \left(1 - \frac{1}{k^{m}}\right)^n
$$ I would like to get some intuition about how this expression varies for m and n (with k around 26). Is there an approximation we can use in this instance? I would like to understand how this probability varies as we add one more character - by how much must we increase $n$ to keep the probability the same $?$ . Related to $1.$ : How do I compute the probability for the case where $k = 26, m = 50, n = 1,000,000\ ?$ . I couldn't do it in Python. Thank you",['probability']
3868321,Condition number of a positive definite matrix,I want to prove that the condition number of a positive definite matrix $S$ is $$ k_2(S)= \frac{\max \lambda_i}{\min \lambda_i} $$ where $k_2$ is condition number for spectral norm. Can someone please help me with this?,"['condition-number', 'spectral-norm', 'matrices', 'linear-algebra', 'positive-definite']"
3868366,Geometry of typical sets,"A common construction in statistics is the distribution of $n$ iid random variables, for $n$ arbitrarily large. For large enough $n$ the distribution is  approximately uniform over a highly regular typical set in $n$ -space. The ""letter-typicality"" definition essentially resembles a quotient of a large permutation group, or more properly the disjoint union of relatively few highly similar quotients. Information theory often studies covering, packing and projection of these sets. These spatial intuitions are not studied directly in any presentation I have seen.
Does anyone know of any articles or books that make this connection precise?
It would be especially useful to see an approach that clarifies the essential structure across many trials of two or more jointly-distributed random variables.","['statistics', 'geometry', 'information-theory', 'reverse-math', 'combinatorics']"
3868378,Finding the determinant of cofactor matrix,"Let \begin{align}
\Delta &= 
\begin{vmatrix}
x_1 & x_2 & x_3 \\ 
x_4 & x_5 & x_6 \\ 
x_7 & x_8 & x_9
\end{vmatrix}
\end{align} and let $C_i$ represent the cofactor of $x_i$ . Find \begin{align} 
\begin{vmatrix}
C_1 & C_2 & C_3 \\ 
C_4 & C_5 & C_6 \\ 
C_7 & C_8 & C_9
\end{vmatrix}
\end{align} in terms of $\Delta$ . Generalize that if $\Delta$ represents the determinant of a $n\times n$ matrix, then the determinant of the cofactor matrix is $\Delta^{n-1}$ . Looking at the generalization, I was tempted to multiply both determinants. $$
\begin{vmatrix}
x_1 & x_2 & x_3 \\ 
x_4 & x_5 & x_6 \\ 
x_7 & x_8 & x_9
\end{vmatrix}
\times 
\begin{vmatrix}
C_1 & C_2 & C_3 \\ 
C_4 & C_5 & C_6 \\ 
C_7 & C_8 & C_9
\end{vmatrix}
=
\begin{vmatrix}
x_1C_1+x_2C_2+x_3C_3 & x_1C_4+x_2C_5+x_3C_6 & x_1C_7+x_2C_8+x_3C_9 \\ 
x_4C_1+x_5C_2+x_6C_3 & x_4C_4+x_5C_5+x_6C_6 & x_4C_7+x_5C_8+x_6C_9 \\ 
x_7C_1+x_8C_2+x_9C_3 & x_7C_4+x_8C_5+x_9C_6 & x_7C_7+x_8C_8+x_9C_9
\end{vmatrix}
$$ $$
\implies
\Delta
\times 
\begin{vmatrix}
C_1 & C_2 & C_3 \\ 
C_4 & C_5 & C_6 \\ 
C_7 & C_8 & C_9
\end{vmatrix}
=
\begin{vmatrix}
\Delta & x_1C_4+x_2C_5+x_3C_6 & x_1C_7+x_2C_8+x_3C_9 \\ 
x_4C_1+x_5C_2+x_6C_3 & \Delta & x_4C_7+x_5C_8+x_6C_9 \\ 
x_7C_1+x_8C_2+x_9C_3 & x_7C_4+x_8C_5+x_9C_6 & \Delta 
\end{vmatrix}
$$ Can someone help me figure out the next step? (This huge determinant must be equal to $\Delta^3$ .)","['matrices', 'determinant']"
3868412,"Show solution to BVP has a saddle-point at $(0,0)$","This problem was from a PDE class I took years ago: Let $\Omega=(-1,1)\times(-1,1)$ and consider the BVP $$
\left\{\begin{aligned}
\Delta u&=0,~~x\in\Omega\\
u(-1,y)&=u(1,y)=0\\
u(x,-1)&=u(x,1)=f(x)
\end{aligned}\right.
$$ where $f:[-1,1]\to\mathbb{R}$ is even, strictly decreasing on $[0,1]$ , and satisfies $f(\pm 1)=0$ .  Show that $u$ has a saddle point at $u(0,0)$ and that $u(0,0)>0$ . I gave a rather hand-wavy argument about symmetry and the partial derivatives that I don't think is entirely accurate: Note that $f$ is even and decreasing on $[0,1].$ Since $f:[-1,1]\to \mathbb{R},$ $f$ has a maximum at $x=0,$ i.e. $f$ has a critical point there (since $f$ has bounded variation, and is increasing on $[0,1]$ , $f$ is differentiable a.e.). Since $u\equiv 0$ if $x=\pm 1,$ the directional derivative of $u$ pointing into $\Omega$ from the boundary is positive. In particular, $u$ has a critical point at $(0,0)$ by symmetry; from $x=\pm1$ it is increasing, but from $x=0,y=\pm1$ it is decreasing. Therefore $u(0,0)>0$ and $u$ has a saddle point there. I also graphed the problem for $f(x)=\{|1-x|,1-x^2,\cos(\pi/2 x)\}$ and used Mathematica's NDSolve to make numerical plots of the solution; each appeared geometrically to have a saddle point at $(0,0)$ . For example, with $f(x) = 1-x^2$ the solution looks like: I'd be interested in a proper solution, if only to satisfy my curiosity.","['boundary-value-problem', 'multivariable-calculus', 'harmonic-functions', 'partial-differential-equations']"
3868465,What's this distribution?,"Suppose you have a row of $n$ switches with two states each, on ( $1$ ) or off ( $0$ ), as well as a coin that shows heads with probability $p$ and tails with prob. $q=1-p$ . Initially, all switches are off. You flip the coin, and if you see tails, you randomly toggle one of the switches (when it's on, you turn it off, and vice versa) and then flip the coin again. If you see heads, you stop and count the number of switches that are on. Q: What probability distribution does this number $K$ of activated switches follow? I found this is a very easy to simulate but tricky to formalize probability distribution.
My thoughts and insights so far: $K$ obviously has support in $\lbrace 0,\dots,n\rbrace$ . The number of total flips $M$ follows a geometric distribution with parameter $p$ . The probability of switch $i$ being toggled $a_i$ times is the probability of drawing $i$ exactly $a_i$ times out of a uniform distribution over $\lbrace 1,\dots,n\rbrace$ in $M$ tries, so $$P(a_i)=\sum_{m=0}^\infty P(M=m)\cdot{m\choose a_i}\left({1\over n}\right)^{a_i}\left({n-1\over n}\right)^{m-a_i}$$ with $P(M=m)=q^{m}\cdot p$ . Now the probability for $K=k$ is the probability that among all $(a_i)_{i\in [n]}$ exactly $k$ are odd. This is where I don't know how to continue, I haven't been able to come up with a closed form solution to this problem. Some insights: For $p\searrow 0$ , $P(K=k)$ approaches a uniform binomial distribution over $\lbrace 0,\dots,n\rbrace$ (EDIT: Thanks to Henry for the correction) For large $n\gg \mathbb{E}[M]={1\over q}$ , the probability $P(a_i)=1$ approaches $q$ , which is by design, i.e. the process simulates flipping all switches independently with probability $q$ . My questions: Do you, by any chance, recognize this as a known distribution? Do you have an idea for how to find a closed form expression for $P(K=k)$ , if it exists at all? Thanks in advance, best regards! - smuecke","['statistics', 'simulation', 'probability-distributions', 'closed-form', 'probability']"
3868514,Nash-Kuiper embedding of the hyperbolic plane into $\mathbb{R}^3$,"According to Hilbert , the $\mathbb{H}^2$ cannot be embedded into $\mathbb{R}^3$ . But then there's Nash-Kuiper theorem, which states that a $C^1$ embedding exists. How to reconcile these two results? My understanding so far is that measuring curvature requires at least $C^2$ embedding. Is that the difference here? Follow-up questions: How does the Nash embedding actually look like? Or is it just an existence result? Nash embedding is still isometric, meaning I can measure tangent vectors. Intuitively that seems to imply that I can do things such as measuring curve lengths, angles, areas. But wouldn't that mean that I'm effectively able to measure curvature? What am I missing here?","['hyperbolic-geometry', 'differential-geometry']"
3868536,"Milnor, Lectures on the h-cobordism theorem, proof of Theorem 3.4","I have a question while reading the proof of Theorem 3.4 in Milnor's book Lectures on the h-cobordism theorem . ( https://www.maths.ed.ac.uk/~v1ranick/surgery/hcobord.pdf ) Theorem 3.4. If the Morse number $\mu$ of the triad $(W;V_0,V_1)$ is zero, then $(W;V_0,V_1)$ is a product cobordism. Proof) $(\cdots)$ We obtain an integral which satisfies $f(\psi(s))=s$ . Each integral curve can be extended uniquely over a maximal interval, which, since $W$ is compact, must be $[0,1]$ . $(\cdots)$ My question is: How did the compactness of $W$ used to show that the maximal domain of an integral curve must be $[0,1]$ ?","['proof-explanation', 'vector-fields', 'smooth-manifolds', 'differential-topology', 'differential-geometry']"
3868541,Number of right isosceles triangles that can be formed with points lying on the curve $8x^3+y^3+6xy=1$,"Number of right isosceles triangles that can be formed with points lying on the curve $$8x^3+y^3+6xy=1$$ MY ATTEMPT :
We have, $$8x^3+y^3+6xy=1$$ adding both the sides $$6xy^2+12x^2$$ and simplifying we get , $$y^2+y(1-2x)+4x^2+2x+1=0$$ after this I got struck pls help me out with this question Answer given is 3","['euclidean-geometry', 'analytic-geometry', 'geometry', 'factoring', 'triangles']"
3868571,Proving a determinant of a particular type is always null,"Let $n\ge 1$ and $A,B\in\mathrm M_n(\mathbb R)$ . Let's assume that $$\forall Q\in\mathrm M_n(\mathbb R), \quad \det\begin{pmatrix} I_n & A \\ Q & B\end{pmatrix}=0$$ where $I_n$ is the identity matrix of $\mathrm M_n(\mathbb R)$ . Can we prove that $\mathrm{rank} \begin{pmatrix}A\\ B\end{pmatrix}<n$ ? This fact seems quite obvious, but I can't find any straightforward argument to prove it. Some ideas. With $Q=0$ , we deal with a block-triangular matrix, so we have $\det B=0$ . Moreover, with $Q=\lambda I_n$ , $\lambda\in\mathbb R$ , since it commutes with $B$ , we have $$\forall \lambda\in\mathbb R,\quad \det(B-\lambda A)=0,$$ so if $\det(A)\ne 0$ , we have $$\forall \lambda\in\mathbb R,\quad\det((BA-\lambda I_n)A^{-1})=\det(BA-\lambda I_n)\det(A)^{-1}=0,$$ which means that every $\lambda\in\mathbb R$ is an eigenvalue of $BA$ (since for all $\lambda\in\mathbb R$ , $\det(BA-\lambda I_n)=0$ ), which is absurd. So $\det(A)=0$ also.","['matrices', 'determinant', 'linear-algebra', 'eigenvalues-eigenvectors']"
3868572,What is the structure behind $ \partial \partial M = \varnothing $?,"In my lectures about manifolds, I learned about the statement $ \partial \partial M = \varnothing $ where $M$ notates a manifold and $\partial M$ its boundary. My professor said, it is similar to the statement in differential geometry, that $ d^2 = 0 $ where $d$ is the exterior derivative. What is the underlying cause for $ \partial \partial M $ being always empty?
What theory connects the statements $d^2 = 0$ and $\partial\partial M = \varnothing$ ?
Are there similar statements like the two given? Please note, that I am not looking for a proof or explanation of why $\partial \partial M = \varnothing$ is true. I am asking about a theory explaining the underlying structure of the given statements. I believe it could be something about comology theory but please enlighten me. Edit: The comment from @Aurelio is a very good reformulation of my questions: My understanding is that OP wants to know the general framework in which to say that $\partial^2=0$ is the same phenomenon as $d^2=0$ .","['exterior-derivative', 'homology-cohomology', 'smooth-manifolds', 'differential-geometry']"
3868610,Sheafification of a subpresheaf,"Let $\mathscr{F}$ be a subpresheaf of a sheaf $\mathscr{H}$ . If I understood correctly, in this case the sheafification has a particularly simple form. It is the sheaf defined by $$\widetilde{\mathscr{F}}(U)=\{s\in \mathscr{H}(U)\:|\:s\text{ locally lies in }\mathscr{F}\},$$ where we say that $s\in \mathscr{H}(U)$ locally lies in $\mathscr{F}$ is there's an open cover $\{U_i\}$ of $U$ such that $s|_{U_i}\in \mathscr{F}(U_i)$ for every $i$ . It is clear that this is indeed a sheaf but I can't see how it satisfies the universal property of the sheafification. I thought about doing it in the following way: I really think that this construction is functorial in $\mathscr{F}$ . Supposing that, given a morphism $\mathscr{F}\to\mathscr{G}$ we should have an induced morphism $\widetilde{\mathscr{F}}\to\widetilde{\mathscr{G}}$ which coincides with our original morphism when restricting to $\mathscr{F}$ . If $\mathscr{G}$ is a sheaf, $\widetilde{\mathscr{G}}=\mathscr{G}$ and so this yields our desired morphism. But I can't really see why this construction is functorial in $\mathscr{F}$ .","['algebraic-geometry', 'abstract-algebra', 'algebraic-topology', 'sheaf-theory']"
3868632,"Permutation of the swedish word ""matematik""","I have three questions and (I think) I have solved two of them so far. First one is Calculate the number of words from the word ""matematik"" with switching order of the letters this I got to be 45360 as its $\frac{9!}{2!2!2!}$ calculate how many of these words contain both a $t$ in the end AND the start this I got to $\frac{7!}{2!2!}$ or $1260$ How many of these words contain the sequence ""mat"" atleast one place in the word I do not really know how to approach this as it is 2 sequences of ""mat"" that can be made from the word ""matematik"" since m, a, t appear twice.","['permutations', 'combinatorics']"
3868639,Determine the smallest $k$ for which a graph is k-partite.,"A graph $G=(V,E)$ is said to be $k$ -partite if you can partition all of the vertices of $G$ into independent sets. However, is there a way to determine the minimum value of $k$ ? Suppose that $G$ is 4-partite - that also means that $G$ is $(5,6,7,..., |V|)$ partite. But how do I know that $G$ is not $3$ partite or bipartite? I know that doing it manually is an option, but only for graphs with a relatively small number of vertices and edges. But what about a graph with $100$ vertices and $200$ edges? How can I work it out then?","['graph-theory', 'discrete-mathematics', 'computer-science']"
3868665,Is there a unique function $f:\Bbb R\to\Bbb R$ satisfying $f(x)^3+3f(x)^2-x^3+2x+3f(x)=0$?,"Is there a unique function $f:\Bbb R\to\Bbb R$ satisfying $f(x)^3+3f(x)^2-x^3+2x+3f(x)=0$ ? Prove your claim. Note: $f(x)^3=f(x)\cdot f(x)\cdot f(x)$ . My attempt: I rewrote the expression like this: $$f(x)^3+3f(x)^2+3f(x)=x^3-2x$$ Function $g(x):=x^3-2x$ is surjective and differentiable, so I thought the same holds for the LHS and hence for $f(x)$ . I factorized the LHS as $f(x)(f(x)^2+3f(x)+3)$ . I argue $f(x)^2+3f(x)+3>0,\forall x\in\Bbb R$ because the parabola $t^2+3t+3$ doesn't have any real roots and is convex. Since $g(x)$ on the RHS is surjective and $f(x)^2+3f(x)+3>0$ , $f(x)$ has to be surjective, too. Let $h(x)=f(x)^3+3f(x)^2+3f(x)$ . $\begin{aligned}(h(x))'&=3f(x)^2f'(x)+6f(x)f'(x)+3f'(x)\\&=3f'(x)(f(x)^2+2f(x)+1)\\&=f'(x)\underbrace{(f(x)+1)^2}_{\ge 0}.\end{aligned}$ Next: $(g(x))'=3x^2-2$ $\begin{aligned}3x^2-2&=0\\\iff x&=\pm\sqrt{\frac23}\\\implies g'(x)&<0, x\in\left(-\sqrt{\frac23},\sqrt{\frac23}\right).\end{aligned}$ This means $h'(x)<0$ on the same interval and $h'(x)=f'(x)(f(x)+1)^2<0\iff f'(x)<0$ , so $f(x)$ should be decreasing on $\left(-\sqrt{\frac23},\sqrt{\frac23}\right)$ as well, and increasing on $\left(-\infty,-\sqrt{\frac23}\right]\cup\left[\sqrt{\frac23},+\infty\right)$ . Function $g(x)=x^3-2x$ , from the beginning, is odd, so $h(x)=f(x)^3+3f(x)^2+3f(x)$ should also be odd. I think $f(x)$ shouldn't be odd, because $3f(x)^2$ would be even (while $f(x)^3$ and $f(x)$ would be odd) preventing $h$ from being odd. I'm neither sure if the information I have are $100\%$ accurate nor relevant. May I ask for advice on solving this task? Thank you in advance!","['functional-equations', 'calculus', 'functions', 'real-analysis']"
3868726,"""Perfect"" solutions to the kissing number problem besides in dimensions 1,2,8, and 24.","The kissing number problem asks how many n dimensional unit spheres can fit around a central one with no overlapping; a natural question is in what dimensions can this be done so that there is no extra space to move any outer sphere around the central one, as in the following picture: There are such configurations in dimensions 1, 2, 8, and 24, and they tie into some exceptional mathematics. My question is, is it possible there are other dimensions that also exhibit this property? Which dimensions have been ruled out, and if little is known, are there natural conjectures or heuristics?","['spheres', 'combinatorial-geometry', 'geometry', 'packing-problem']"
3868739,Is the set of pushdown transductions closed under composition?,"Let’s define a pushdown transducer as a 9-tuple $V = (A, B, S, Q_A, Q_S, \phi, \psi, \chi, q_0)$ , where $A$ is the finite input alphabet , $B$ is the finite output alphabet , $S$ is the finite stack alphabet , $Q_A$ are the finite set of read-from-input states , $Q_S$ is the finite set of read-from-stack states , $\phi: (Q_A \times A) \cup (Q_S \times (S \cup \{ \epsilon \})) \to (Q_A \cup Q_S)$ (where $\epsilon \not\in S$ ) - is the state transition function , $\psi: (Q_A \times A) \cup (Q_S \times (S \cup \{ \epsilon \})) \to S^*$ (where $\epsilon \not\in S$ ) is stack transition function , $\chi: (Q_A \times A) \cup (Q_S \times (S \cup \{ \epsilon \})) \to B^*$ (where $\epsilon \not\in S$ ) is output function , $q_0 \in Q_A$ is the initial state . Now, let’s define the total transducer function of $V$ of $V$ as $f_V: A^* \to  (Q_A \cup Q_S) \cup S^* \to B^*$ defined by recurrence relation $$f_V(\Lambda, q, \sigma) = \Lambda$$ $$f_V(a\alpha, q, \Lambda) = \begin{cases} \chi(q, a) f_V(\alpha, \phi(q, a), \psi(q, a)) & \quad q \in Q_A  \\ \chi(q, \epsilon) f_V(\alpha, \phi(q, \epsilon), \psi(q, \epsilon)) & \quad q \in Q_S \end{cases}$$ $$f_V(a\alpha, q, \sigma s) = \begin{cases} \chi(q, a) f_V(\alpha, \phi(q, a),  \sigma s \psi(q, a)) & \quad q \in Q_A \\ \chi(q, s) f_V(\alpha, \phi(q, s), \sigma \psi(q, s)) & \quad q \in Q_S \end{cases}$$ and limited transduction function as $t_V(A^*) = f_V(A^*, q_0, \Lambda)$ . We call a deterministic function $A^* \to B^*$ a pushdown transduction iff it is a limited transduction function of some pushdown transducer. Pushdown transducers are a more powerful computation model than finite state transducers, but less powerful than Turing machines. Is the set of pushdown transductions closed under composition? I know, how to prove that composition of a pushdown transduction and a regular transduction is a pushdown transduction, and that composition of a regular transduction and a regular transduction is a regular transduction, by explicitly constructing corresponding automata. But attempting to do this for two irregular pushdown transductions in the same straightforward way we get a transducer with two stacks (which is equivalent to Turing machine), which gives us nothing we have not known before…","['automata', 'abstract-algebra', 'semigroups', 'discrete-mathematics', 'pushdown-automata']"
3868753,Proof of Toeplitz Transformation.,"Toeplitz Transformation: Let $\{c_{n,k}:1\leq k\leq n, n\geq 1\}$ be an array of real numbers such that: i) $c_{n,k}\longrightarrow 0$ as $n\rightarrow{\infty}$ for  each $k\in \mathbb{N}$ . ii) $\sum\limits_{k=1}^{n} c_{n,k} \longrightarrow 1$ iii) There exists $C>0$ such that for all positive integers n: $$\sum_{k=1}^{n}|c_{n,k}| \leq C.$$ Then for any convergent sequence $\{a_n\}$ the transformed sequence $\{b_n\}$ given by $$b_n= \sum_{k=1}^{n}c_{n,k}a_k,\ \ n\geq 1$$ is also convergent and $(b_n) \rightarrow a \leftarrow (a_n)$ Proof: Let $a_n=a$ then $$\underset{n\rightarrow \infty}{\lim} b_n= a\underset{n\rightarrow \infty}{\lim} \sum\limits_{k=1}^{n} c_{n,k} =a.\ \ \ [by (ii)]$$ Now assuming [ $(a_n) \rightarrow 0 \implies (b_n) \rightarrow 0$ ] we can show that [ $(a_n) \rightarrow a \implies (b_n) \rightarrow a$ ] where $a\neq 0$ . Let $d_n:=(a_n-a) \implies (d_n) \rightarrow 0$ . Then from the assumption: $$ e_n:= \sum_{k=1}^{n}c_{n,k}d_k \longrightarrow 0 \\
   \implies \sum_{k=1}^{n}c_{n,k}(a_k-a) \longrightarrow 0 \\
   \implies \sum_{k=1}^{n}c_{n,k}(a_k) - \sum_{k=1}^{n}c_{n,k}a \longrightarrow 0 \\
   \implies b_n - \sum_{k=1}^{n}c_{n,k}a \longrightarrow 0 \\
   \implies b_n - a \longrightarrow 0\ \ \text{as n} \rightarrow \infty 
                             \ \ \                \textbf{USING (ii)}\\
   \implies \underset{n\rightarrow \infty}{\lim} b_n=a.$$ All that remains to show is: If $\underset{n\rightarrow \infty}{\lim} a_n=0$ then $\underset{n\rightarrow \infty}{\lim} b_n=0$ . $|b_n - 0|=|\sum\limits_{k=1}^{n}c_{n,k}a_k|\leq \sum\limits_{k=1}^{n}|c_{n,k}||a_k|\ \ \ \ \ \ \ (1)$ Now USING (iii) we have $\sum\limits_{k=1}^{n}|c_{n,k}| \leq C$ , and for any $\epsilon>0$ there exists $n_1 \in \mathbb{N}$ such that for all $n \geq n_1$ , $|a_n| \leq \frac{\epsilon}{2C} \ \ \ \ \ (2)$ From (2) & (1) [ USING (iii) ] we obtain: For all $n\geq n_1$ we have $|b_n| \leq \sum\limits_{k=1}^{n_1-1}|c_{n,k}||a_k|+ \sum\limits_{k=n_1}^{n}|c_{n,k}||a_k| \leq \sum\limits_{k=1}^{n_1}|c_{n,k}||a_k|+C.\frac{\epsilon}{2C}(=\frac{\epsilon}{2}) \ \ \ \ \ (3)$ To get control over this $\sum\limits_{k=1}^{n_1-1}|c_{n,k}||a_k|$ one can USE (i) . As for each $k$ , $(c_{n,k}) \longrightarrow 0$ as $n \rightarrow \infty$ ( $1\leq k < n_1).\ \ \ \ \ \  (4)$ Also since $(a_n)$ is convergent, hence $(a_n)$ is bounded by D(say),  i.e $|a_k|<D \ \ \ \text{for all} \ \ k \in \mathbb{N}$ $\ \ \ \ \  (5)$ Let $\epsilon > 0$ then there exists $n_{2,k} \in \mathbb{N}, 1\leq k < n_1$ such that $$|c_{n,k}|< \frac{\epsilon}{2(n_1-1)D} \text{for all}\ n \geq n_2:=max\{n_{2,1},n_{2,2},...,n_{2,n_1-1}\}\\
  \implies \sum\limits_{k=1}^{n_1-1} |c_{n,k}| < \frac{\epsilon}{2D}\ \ \ \ \ \ \  \text{for all}\ \ n\geq n_2 \ \ \ \ \  (6)$$ Using (5) & (6) we have: $$ \sum\limits_{k=1}^{n_1-1} |c_{n,k}||a_k| < \frac{\epsilon}{2D}.D=\frac{\epsilon}{2}\ \ \text{for all}\ \  n \geq n_2\ \ \ \ \ (7)$$ from (3) & (7) we have: for any $\epsilon > 0$ there exist $N:=\text{max}\{n_1,n_2\} \in \mathbb{N}$ such that for all $n \geq N$ we have $$|b_n - 0| \leq \sum\limits_{k=1}^{n_1}|c_{n,k}||a_k|+ \sum\limits_{k=n_1}^{n}|c_{n,k}||a_k| \leq \frac{\epsilon}{2D}.D + C.\frac{\epsilon}{2C}=\epsilon \\
\implies \underset{n\rightarrow \infty}{\lim} b_n=a.$$ Please tell me if my argument is correct. Observation: If $a=0$ then condition (ii) $\sum\limits_{k=1}^{n} c_{n,k} \longrightarrow 1$ can be removed. Am I correct?","['limits', 'sequences-and-series', 'epsilon-delta', 'real-analysis']"
3869030,Showing $| \mathbb{N} | =|\mathbb{N} \times \mathbb{N}|$ using the diagonal argument,"Showing $| \mathbb{N} | =|\mathbb{N} \times \mathbb{N}|$ using the diagonal argument is seeming a little hard for me to prove by an explicit function. It's clear that we need a function $f: \mathbb{N} \to \mathbb{N} \times \mathbb{N}$ , and arranging them diagonally seems to work very well. In other words: $(0,0) (0,1) (0,2) (0,3) ...$ $(1,0) (1,1) (1,2) (1,3) ...$ $(2,0) (2,1) (2,2) (2,3) ...$ ... (Not sure how to put in a table). And we can say $f(0) = (0,0), f(1) = (0,1), f(2) = (1,0), f(3) = (0,2), f(4) = (1,1) ...$ , going diagonally all the way through. I'm trying to find an explicit function for $f$ though. The most I've come up with is that: $0$ maps to $(0,0)$ $1$ and $2$ get mapped to the points $(a,b)$ with $a,b \in \{ 0,1 \}$ , and $a + b = 1$ . $3,4,$ and $5$ get mapped to points $(a,b)$ with $a,b \in \{ 0,1,2 \}$ and $a+b = 2$ . $6, 7, 8$ , and $9$ get mapped to points $(a,b)$ with $a,b \in \{ 0,1,2,3 \}$ and $a + b = 3$ . How can we write an explicit formula for $f(n)?$ I'm aware there are many other ways to show these sets are the same, but I'd like to stick with this technique for now. Thanks.",['elementary-set-theory']
3869111,$X$ is locally connected and countably compact,"Let $(X,\tau)$ be a topological space $T_3$ . Show that the following statements are equivalent: Every open and finite coverage of X has a finite refinement consisting of connected sets. Space X is locally connected and countably compact. A topological space is called countably compact if every open and enumerable coverage admits a finite subcoverage. Any ideas: $1 \to 2$ To prove that $X$ is locally connected I thought of the following, by hypothesis $A=\{X\}$ has a finite refinement consisting of connected sets, say $B=\{B_i:B_i \text{ is connected and } i \in \{1,...,n\}\}$ . Let $x \in X$ and $U$ open of $X$ , such that $x \in U \subset X$ , since $X=\cup B_i$ , $x \in B_k$ where $k \in \{1,...,n\}$ , if $B_k \subset U$ and $B_k$ is open, $X$ is locally connected in $x$ , If the above does not happen, I don't know how to proceed. proving that it is countably compact does not occur to me how. $2 \to 1$ let $U=\{U_i:i \in \{1,2,...,n\}\}$ be a finite coverage of $X$ . Since $X$ is locally connected, for each $x \in U_i$ there is a connected open $V_{ix}$ such that $V_{ix} \subset U_i$ . Thus, $U_i=\cup _{x \in U_i} V_{ix}$ . My idea here was to use the hypothesis that $X$ is countably compact and apply it to each $U_i$ , but I'm not sure of this fact, because I don't know if there is a countable amount of $V_{ix}$ that covers $U_i$ . I don't know how to use the hypothesis that $X$ is $T_3$ . any help would be very useful. Thank you","['connectedness', 'general-topology', 'locally-connected', 'compactness']"
3869165,Representing a linear transformation as a matrix in terms of a given basis,"I am new to linear algebra, I need help in understanding how to represent a linear transformation into standard basis of a matrix Consider $M_{2}(\mathbb{R}),$ the vector space of all $2 \times 2$ real matrices.  Let $$
A=\left(\begin{array}{cc}
1 & -1 \\
-1 & 1
\end{array}\right)
$$ and if we  define $\mathcal{A}(B)=A B$ for any $B \in M_{2}(\mathbb{R})$ . Show that $\mathcal{A}$ is a linear transformation on $M_{2}(\mathbb{R})$ and find the matrix of $\mathcal{A}$ under the basis $E_{i j}, i, j=1,2$ I can show the linearity part by considering the action of this linear transformation on matrix $B+ \lambda C$ , in fact I know this will be true for any matrix $A$ . But how to represent this in terms of given basis. Note here I have taken basis $E_{i j}$ be the $2 \times 2$ matrix with $(i, j)^{\text {th }}$ entry 1 and other entries 0.","['matrices', 'change-of-basis', 'linear-algebra', 'linear-transformations']"
3869173,Combinatorial Proof for Composite/Nested Binomial Coefficient [duplicate],"This question already has an answer here : Prove the following identity combinatorially (1 answer) Closed 12 months ago . I'm working on a problem right now for where we're asked to give a combinatorial proof for the following where $n \geq 4$ : $${{n \choose 2} \choose 2} = 3{n \choose 4} + 3{n \choose 3}$$ LHS: Number of subsets of size 2 from $n$ , and then we count all the ways to make subsets of 2 from those subsets. RHS: Number of subsets we can make of size 4 from $n$ multiplied by 3 added to the number of subsets we can make of size 4 from $n$ multiplied by 3. I originally tried to relate it using three different groups with $n$ elements, but I suspect my logic was flawed in that I may have been double counting on the RHS. Any help would be greatly appreciated.","['solution-verification', 'combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
3869193,Explain the following mysterious property of the function $f_1$ where $f_1(k)$ is the square of sum of digits of $k$.,"We denote $f_n(k)=f_1(f_{n-1}(k))$ . I have to find out the value of $f_{1995}(2^{1995})$ My attempt : Consider $f_x(2^x)$ , with a little investigation, we would notice that value of $f_x(2^x)$ is $1$ whenever $x$ is a multiple of $3$ . Many of my friends concluded the same thing. I was not satisfied with the answer. Nobody understood why it was the case. My thoughts : I think the reason is when we calculate $f_x(2^x)$ when $x$ is a multiple of $3$ , we end up with $f_i(10^j)$ at some point, then all later calculations will only yield $1$ . But what's the guarantee that you will definitely end up with power of ten at some point? Maybe we can observe a pattern when we write numbers in both binary?( I thought it because we are concerned with power of 2). Unfortunately, I didn't see any pattern (maybe I have missed). Maybe be $8$ is involved in some way? I could not come to conclusion to any of the questions.Can anybody give hints or provide a proof? Any help would be great.","['contest-math', 'recreational-mathematics', 'functions']"
3869212,"Show that $e^{A}$ makes sense, $A$ be an $n \times n$ real matrix","I have to solve: Let $A$ be an $n \times n$ real matrix. The exponential of $A$ is defined as $$
e^{A}:=I+A+\frac{A^{2}}{2 !}+\frac{A^{3}}{3 !}+\cdots
$$ provided each entry of the matrix converges. Show that $e^{A}$ makes sense and $P$ is an invertible matrix show that $P e^{A} P^{-1}$ is also an exponential matrix. I have solved Part (2) as : This follows by noting that $$
\begin{aligned}
\left(T B T^{-1}\right)^{k} &=\left(T B T^{-1}\right)\left(T B T^{-1}\right) \cdots\left(T B T^{-1}\right) \\
&=T B\left(T^{-1} T\right) B\left(T^{-1} T\right) \cdots\left(T^{-1} T\right) B T^{-1} \\
&=T B^{k} T^{-1}
\end{aligned}
$$ then yields $$
\begin{aligned}
e^{P^{-1} A P} &=I+P^{-1} A P+\frac{\left(P^{-1} A P\right)^{2}}{2 !}+\cdots \\
&=I+P^{-1} A P+P^{-1} \frac{A^{2}}{2 !} P+\cdots \\
&=P^{-1}\left(I+A+\frac{A^{2}}{2 !}+\cdots\right) P=P^{-1} e^{A} P
\end{aligned}
$$ I don't know how to solve part (1) i.e Show that $e^{A}$ makes sense ?","['matrices', 'matrix-equations', 'linear-algebra', 'exponential-function']"
3869245,Surjective and pre-contractive mapping on compact space,"Suppose that $X$ is a compact metric space, $T:X \to X$ is a surjection. Besides, $\forall x,y\in X$ , $$d(Tx,Ty)\leq d(x,y).$$ Show that $T$ is an isometry. The conclusion seems to be correct. But I just can prove that $\inf\{d(x,Tx):x\in X\}$ can attain its infimum. It seems that if there exist $x,y\in X$ such that $d(Tx,Ty)<d(x,y)$ , then $ T$ would not be surjection. But I don’t know how to illustrate it in details. Or maybe this conclusion is false. However,I also cannot give a counter-example.","['general-topology', 'isometry', 'metric-spaces', 'compactness']"
3869288,How to prove $ L_X(\omega(Y)) = (L_X\omega)Y + \omega(L_XY)$ starting from the fundamental definition of Lie derivative?,"The Lie derivative of a smooth real valued function $f$ along a vector field $X$ , on a point $p$ in some smooth manifold is given as $$
L_X f(p) :=  \lim_{h\to 0} \frac{1}{h}\left[ f(\phi(p)) - f(p) \right]\label{Lief}\tag{1}
$$ The Lie derivative of a vector field $Y$ along another vector field $X$ , on a point $p$ in some smooth manifold is given as $$
L_X Y(p) =\frac{d}{dt}\left[\phi_{-t*}Y(p)\right] :=  \lim_{h\to 0} \frac{1}{h}\left[ (\phi_{-h*}Y)_p - Y_p \right]\label{Liev}\tag{2}
$$ $\phi_t$ is the integral curve of the vector field $X$ , with the push-forward map defined by $$(\phi_{-h*}Y)_p  = \phi_{-h*}Y_{\phi_h(p)}$$ Similarly, the Lie derivative of a one-form $\omega$ along a vector field $X$ , is given by $$
L_X \omega(p) =\frac{d}{dt}\left[\phi_{t}^*\omega\right](p) :=  \lim_{h\to 0} \frac{1}{h}\left[ (\phi_{h}^*\omega)_p - \omega_p \right]\label{Lieo}\tag{3}
$$ $$
(\phi_h^* \omega)(p)(X_p) = \omega(\phi_h(p)) (\phi_{h*}X_p)
$$ Now, I want to prove that the Lie derivative $$
L_X(\omega(Y)) = (L_X\omega)Y + \omega(L_XY)
$$ The function $\omega(Y)(p) = \omega_p (Y_p)$ , as $\omega(Y)(p)$ is a function it's transformation rule should be something like \begin{align}
L_X(\omega(Y)) &=  \lim_{h\to 0} \frac{1}{h}\left[ (\phi_{h}^*(\omega(Y)))_p - (\omega(Y))_p \right] \label{LiewY}\tag{4}
\end{align} Is the equation \eqref{LiewY} the right way to begin?, or the expression be more like \eqref{Lief} as $\omega(Y)$ is a real valued function over the manifold, I am not sure how $(\phi_{h}^*(\omega(Y)))_p$ in \eqref{LiewY} will look when
simplified. This question has an answer here but in terms of Cartan's formula.
I'd like to know how to start from the very basic definition of Lie derivative","['lie-derivative', 'smooth-manifolds', 'differential-geometry']"
3869301,Showing solution to ODE is defined up to infinite time,"Question Consider the system $$x'(t)=-x(t)^{3}+y(t)^{2}$$ $$y'(t)=\cos(x(t)y(t)^{2})$$ with initial conditions $x(0)=1, y(0)=2$ . Let $(a,b)$ be the maximal interval that the solution is defined and show $b=\infty$ . Partial Progress An attempt for a proof by contradiction. Let $b<\infty$ so for some finite $b$ we know $||x(t)||\to\infty$ as $t\to b$ . Since $y'(t)$ is a trigonometric function with the given initial condition, we know $|y'(t)|\leq 1$ and $|y(t)|\leq t+2$ . This doesn't seem to substitute nicely into the equation for $x'(t)$ or $x(t)$ to form a finite upper bound. Any help will be appreciated.","['proof-writing', 'ordinary-differential-equations']"
3869354,Integral of $\exp(-\|x\|_p)$,"$\newcommand{\RR}{\mathbb{R}}$ Let $1 \leq p < \infty$ and let $d$ be a positive integer. I want to show that $$I := \int_{\RR^d} \exp\left(-\|x\|_p\right) dx = \frac{(2\Gamma(1/p))^d}{p^{d-1}}\frac{\Gamma(d)}{\Gamma(d/p)}.$$ Due to the $L^p$ norm, the integrand is not radially symmetric. If I convert to spherical coordinates I end up with $$I = \Gamma(d) \int_{S^{d-1}} \frac{d\sigma(x)}{\|x\|_p^d} $$ where $\sigma$ is the spherical measure. How should I proceed from here? Or alternatively how should I compute this integral? I would appreciate detailed calculations!","['integration', 'gamma-function']"
3869519,Prove that $f'(c)=(f(c)-f(a))/(c-a)$ isn't always correct,"I must find a function $f:[a,b]->R$ derived such that there is no c in $(a,b)$ for which: $$f'(c)=(f(c)-f(a))/(c-a)$$ My professor said that $f(x)=x^3$ will work but I think that he is wrong, because $f'(-0.5)$ proves that the claim is correct and not the opposite. can someone tell me who is wrong here and what example may work?","['calculus', 'derivatives']"
3869530,Wirtinger derivative form of Cauchy–Riemann equations,"I'm trying to understand the Cauchy-Riemann equations using the traditional $u, v$ form and the Wirtinger derivative form. Taking $\ln|z|$ as an example function, for the normal $u, v$ form I have: $$\begin{align}u(x,y) &= \ln|x + iy|\\ v(x,y) &= 0\end{align}$$ so the Cauchy-Riemann equations are not satisfied: $$\frac{\partial}{\partial x} u(x,y) = \frac{1}{x + iy} \neq v \frac{\partial}{\partial y} = 0$$ $$\frac{\partial}{\partial y} u(x,y) = \frac{i}{x + iy} \neq -v \frac{\partial}{\partial x} = 0$$ So far so good, I didn't expect them to be.  But there's another form for the Cauchy-Riemann equations using Wirtinger derivatives: $$\frac{\partial}{\partial \overline{z}} f(z) = 0$$ Doing it this way I get $$\begin{align} \frac{\partial}{\partial \overline{z}} \ln|z| &= \\
 &= \frac{1}{2} (\frac{\partial}{\partial x} + i \frac{\partial}{\partial y}) \ln|x + iy| \\
 &= \frac{1}{2} (\frac{1}{x + iy} + i \frac{i}{x + iy}) \\
 &= \frac{1}{2} (\frac{1}{z} - \frac{1}{z}) \\
 &= 0\end{align}$$ So using the Wirtinger derivative form it would seem that $\ln|z|$ is holomorphic?  I don't think that's right; I thought real valued functions should only be holomorphic if they're constant.  What am I doing wrong?","['complex-analysis', 'cauchy-riemann-equations']"
3869638,Prove the limit of an IVP is zero.,"Let $U\in\mathbb{R}^d$ be open and let $F\in C^1(U, \mathbb{R}^d)$ . Fix $p\in U$ . Consider the IVP $$
\dot{x}=F(x),\ x(0)=p
$$ Let $(T_-, T_+)$ be the maximal interval be the maximal interval of existence. Assume $$
\lim_{t\uparrow T_+}x(t)=\zeta\in U
$$ Prove that $F(\zeta)=0$ . I just learned the extensibility of solution and have difficulty in working out the answer. However, by the extensibility theory, it seems $T_+$ has to be $+\infty$ . Do I mistaken something? Please give a concrete explaination. Any hint or answer of the original question could be very helpful!","['initial-value-problems', 'ordinary-differential-equations']"
3869679,"Cumulative Distribution Function given mean, std dev, skew and kurtosis?","Is there a function that calculates the cumulative distribution function (CDF) of a Gaussian distribution given the mean, std dev, skew, and kurtosis?  Does anyone know of one written in a c-like language (c, c++, java, c#)?
Thanks","['programming', 'statistics', 'cumulative-distribution-functions', 'probability-distributions', 'moment-generating-functions']"
3869703,Multiple Solutions to an ODE,"I want to find the solution to the IVP: $$y' = 2\cos(x)\sqrt{y-1},\;\; y\geq1
$$ with initial condition $y(0)=2$ . I used separation of variables to get the general solution $$y =\left(\sin(x)+C\right)^2 +1.$$ When I use the initial condition to try and find a specific solution, I get that $C = \pm 1$ . Both of these values of $C$ work when subbed back into the original differential equation. But since the ODE is continuous for all $y\geq 1$ and the $y$ partial derivative is continuous for all $y>1$ , Picard's theorem says that there should be a unique solution for the initial condition $y(0)=2$ . Is there any way that I can further test the two solutions to see which one is invalid?","['initial-value-problems', 'ordinary-differential-equations']"
3869740,Doubts on application of continuity definition and Dominated Convergence theorem,"I quote Øksendal (2003) . Let $\mathcal{V}=\mathcal{V}(S,T)$ be the class of functions $f(t,\omega):[0,\infty)\times\Omega\to\mathbb{R}$ such that $(t,\omega)\to f(t,\omega)$ is $\mathcal{B}\times\mathcal{F}$ -measurable (where $\mathcal{B}$ denotes the Borel $\sigma$ -algebra on $[0,\infty)$ ), $f(t,\omega)$ is $\mathcal{F}_t$ -adapted and $\mathbb{E}\bigg[\int_{S}^T f(t,\omega)^2 dt\bigg]<\infty$ . [...]
Recall that a function $\phi\in\mathcal{V}$ is called elementary if it has the form $$\phi(t,\omega)=\sum_j e_j(\omega)\cdot\chi_{[t_j, t_{j+1}]}(t)\tag{1}$$ [...] Statement Let $g\in\mathcal{V}$ be bounded and $g(\cdot,\omega)$ continuous for each $\omega$ . Then there exists elementary functions $\phi_n\in\mathcal{V}$ such that $$\mathbb{E}\left[\int_S^T\left(g-\phi_n\right)^2dt\right]\to 0\hspace{1.5cm}\text{as }n\to\infty\tag{2}$$ Proof Define $\phi_n(t,\omega)=\sum_j g(t_j,\omega)\cdot\chi_{[t_j,t_{j+1})}(t)$ . Then, $\phi_n$ is elementary since $g\in\mathcal{V}$ , and $$\int_S^T(g-\phi_n)^2dt\to0\hspace{1.5cm}\text{as }n\to\infty\text{ for each }\omega$$ since $g(\cdot,\omega)$ is continuous for each $\omega$ . Hence $\mathbb{E}\left[\int_S^T(g-\phi_n)^2dt\right]\to0$ as $n\to\infty$ by bounded convergence. My questions : Why does definition of continuity of $g(\cdot,\omega)$ imply that $$\displaystyle{\int_S^T(g-\phi_n)^2dt}\to0\hspace{1.5cm}\text{as }n\to\infty\text{ for each }\omega\hspace{3.5cm}\text{?}$$ My interpretation : I think I am allowed to conceive $\phi_n$ as a kind of step-function, whose value at time $t_n$ corresponds to the value of the continous and bounded function $g$ at time $t_n$ . Does that mean that if I shrink the differential of time $[t_j,t_{j+1})$ , continuity of $g$ implies that $|g-\phi_n|<\varepsilon\text{ for }|t_j-t_{j-1}|<\delta$ (which implies that $\displaystyle{\int_S^T(g-\phi_n)^2dt}\to0\hspace{0.5cm}\text{as }n\to\infty\text{ for each }\omega$ )? In the end, is Lebesgue's dominated convergence theorem applied? If so, why does it lead from $$\displaystyle{\int_S^T(g-\phi_n)^2dt}\to0\hspace{1cm}\text{as }n\to\infty\text{ for each }\omega$$ to $$\mathbb{E}\left[\displaystyle{\int_S^T(g-\phi_n)^2dt}\right]\to0\hspace{2.3cm}\text{as }n\to\infty\text{ for each }\omega\hspace{1.8cm}\text{ ?}$$ My interpretation : What I think is that one could set $X_n=(t_{j+1}-t_j)$ and $Y_n=\displaystyle{\int_S^T(g-\phi_n(t,\omega))^2}$ , which - as seen in my interpretation in point $1.$ - since $g$ is continuous, by definition of continuity, is such that for every $t$ , $|Y_n|<\epsilon$ whenever $|X_n|<\delta$ . In other words, $$g=\lim_{n\to\infty}\phi_n(t,\omega)\hspace{0.5cm}\text{ pointwise}\tag{3}$$ implies that $$0=\lim_{n\to\infty}\int_S^T(g-\phi_n(t,\omega))^2dt\hspace{0.5cm}\text{ pointwise}\tag{4}$$ Hence, given the immediately above explained conditions: $|Y_n|<\epsilon\text{ for every }t$ (namely, ""boundedness"" ), whenever $|X_n|<\delta$ ; $0=\lim\limits_{n\to\infty}\displaystyle{\int_S^T(g-\phi_n(t,\omega))^2dt}\hspace{0.5cm}\text{ pointwise}$ (namely, ""pointwise convergence"" ) one could apply Lebesgue's dominated convergence theorem : $$\lim_{n\to\infty}\mathbb{E}\left(Y_n\right)=\mathbb{E}\left(\lim_{n\to\infty}Y_n\right)=\mathbb{E}\left(0\right)=0$$ Are my interpretations of points $1.$ and $2.$ correct? If not, why?","['measure-theory', 'proof-explanation', 'continuity', 'solution-verification', 'convergence-divergence']"
3869757,Question about the proof of Schröder–Bernstein theorem,"I am learning a real analysis textbook in Chinese by myself and my question arising from seeing the proof of statement: $A, B$ are two sets. If $\exists A^*\subset A, B^*\subset B,$ s.t. $A\sim B^*, A^*\sim B\Rightarrow A\sim B$ . ( $A\sim B$ means $A$ and $B$ have the same cardinality or there exists 1-1 correspondence $\phi$ between A and B.) The proof is done as follow: If $A\sim B^* \subset B,B\sim A^*\subset A$ ,let $\phi$ be a 1-1 function between $A$ and $B^*$ , $\psi$ be a function between $A^*$ and $B$ . Let $A_0=A^*, B_0=B^*, A_1=A-A_0.$ Define: $$B_1=\phi(A_1)\equiv\{y|y=\phi(x),x\in A_1\}$$ $$A_2=\psi(B_1)\equiv\{x|x=\psi(y),y\in B_1\}$$ (1) $\color{red}{\text{Since}\ A_2\subset A_0}$ , we have $A_1\cap A_2=\emptyset$ . (2) Also, let $B_2=\phi(A_2)$ , since $\phi$ is 1-1, $B_1\cap B_2=\emptyset.$ (3) $\color{red}{\text{In general, if we have constructed } A_1, A_2,..., A_n\ \textbf{pairwise disjoint}, B_1,B_2,...,B_n \ \textbf{pairwise disjoint,}} $$A_{i+1}=\psi(B_i),B_i=\phi(A_i),i=1,2,...,n-1,$ let $$A_{n+1}=\psi(B_n),B_{n+1}=\phi(A_{n+1}).$$ (4) Becuase $\psi$ is 1-1, from $B_1,...B_n$ pairwise disjoint, we know that $\color{red}{A_{n+1}\text{ and } A_2,...,A_n\text{ are pairwise disjoint.}}$ (5) Also, since $A_{n+1}\subset A_o$ , $A_{n+1}$ and $A_1$ are pairwise disjoint. (6) Now, since $\phi$ is 1-1, $A_1,...,A_{n+1}$ are pairwise disjoint, $B_{n+1}$ and $B_1,...,B_n$ are pairwise disjoint. (7) We obtain two sequences of pairwise disjoint sets $\{A_n\}^\infty_{n=1}$ , $\{B_n\}^\infty_{n=1}$ ， $A_{i+1}=\psi(B_n),B_{n+1}=\phi(A_{n+1}),i=1,2,3...$ . Therefore $\bigcup^\infty_{n=1}A_n\sim^\phi\bigcup^\infty_{n=1}B_n$ . (8) Also, through $\psi$ , $B\sim A_0, B_k\sim A_{k+1}$ , therefore $$B-\bigcup^\infty_{k=1}B_k\sim^\psi A_0-\bigcup^\infty_{k=1}A_k=A_0-\bigcup^\infty_{n=2}A_n.$$ (9) $A_1=A-A_0$ , $A_0\subset A\Rightarrow A_0=A-A_1$ . Therefore $$A_0-\bigcup^\infty_{n=2} A_n= A-\bigcup^\infty_{n=1}A_n,$$ therefore \begin{align}
A & = (A-\bigcup^\infty_{n=1} A_n)\cup(\bigcup^\infty_{n=1}A_n)\\
 & = (A_0-\bigcup^\infty_{n=2} A_n)\cup (\bigcup^\infty_{n=1}A_n)\\ 
 & \sim (B-\bigcup^\infty_{n=1}B_n)\cup(\bigcup^\infty_{n=1}B_n) \\
 & = B 
\end{align} Starting from (1) and (2). When I first attempted to follow the proof myself, I wrote something: $A_1$ is a subset of $A$ where the one-to-one correspondence does not hold. Therefore, $B_1=\phi(A_1)$ may or may not be in $B_0$ . But then I don't see why $A_2\cap A_1=\emptyset$ . I think I am not quite sure if $\phi$ is the 1-1 function between $A_0$ and $B$ , what exactly is $\phi(A_1)$ . Where will $\phi$ map $A_1$ onto? In my understanding, $A_2\cap A_1=\emptyset$ only if $B_1\subset B_0$ , if so, why?
Did I misunderstand some very important concepts or I misunderstand the proof?. Proceed to line (3), if we iterate the process, we could obtain $A_1\cap A_2=\emptyset$ ， $A_2 \cap A_3=\emptyset$ , so on and so force. It is a weaker condition as compared to pairwise disjoint. Are we simply suppose $A_1,A_2,...,A_n$ are pairwise disjoint? (Similarly, to $B_n$ , 1,...,n.) If that's the case, what makes it supposition legitimate? It first occur to me line (3) to (7) is proof by induction, but I tried to follow the iterative procedure but cannot see why $A_1,A_2,...,A_n$ and $B_1,B_2,...B_n$ are pairwise disjoint. I am pretty lost at the end of the proof. It seems each step other than (1) and (3) are true but I don't know what exactly this proof is showing in each step. Could someone please, aside from answering my two questions stated above, also give me some idea what is the rough idea of the proof? In other words, are there are general concepts or ideas behind the proof as a whole?","['elementary-set-theory', 'functions']"
3869823,Covariant derivate of a ortogonal endomorphism,"I am working with different topics of Riemannian geometry and I have a little problem with a question. Assume that $(M,g)$ is a Riemannian manifold endowed with the Levi Civita connection. Let $P$ be a $(1,1)$ -tensor field, i.e., $P$ is a endomorphism of fields $$
P : \mathfrak{X}(M) \to \mathfrak{X}(M)
$$ Assume that $P$ is fibered with respect $TM$ , that's mean if $p \in M$ and $X_p \in T_pM$ then $P(X_p) \in T_pM$ , i.e., $P$ is a $TM$ -endomorphism (attending to its bundle nature). Assume that $P$ is orthogonal with respect each metric, that is $$
g_p(P(v_p),P(v_p)) = g_p(v_p,v_p).
$$ I have two a question that I can't solve: Can I say anything about $\nabla P$ ? I know that $$
P(\nabla_X Y)) = \nabla_X P(Y) - (\nabla_X P)(Y)
$$ I'm especially interested in the relation among $$
\langle E_i, \nabla_X E_i\rangle \stackrel{???}{=} \langle P(E_i), \nabla_X P(E_i)\rangle 
$$ (that is the connection 1-forms in different orthonormal frames). I know every closed differential form $\alpha$ in a simply connected is exact. Is there any theorem related to the covariant derivate? If $T$ is a tensor field $(r,s)$ , the contraction $i_X \nabla T = \nabla_X T$ is another tensor field of type $(r,s)$ . My question is if $T$ is a tensor field, there exists any tensor field $S$ such that $i_X \nabla S = T$ ?","['riemannian-geometry', 'differential-geometry']"
3870113,"Describe all the compact subsets of $\ell^{1}$ -- General Result Proved, Example Needed.","The space $\ell^{1}$ the space of all infinite sequence $\mathbf{x}:=(x_{1},x_{2},x_{3},\cdots)$ such that the infinite sum of the coordinate is absolutely convergent. That is, $\sum_{i=1}^{\infty}|x_{i}|<\infty$ . We give this space a metric defined as $$d(\mathbf{x},\mathbf{y}):=\sum_{i=1}^{\infty}|x_{i}-y_{i}|,$$ and I want to study on the compactness of this space and its subsets. I read several online notes and post in the stackexchange, but what I got was mostly the subsets that are not compact. For example, http://math.stanford.edu/~ksound/Math171S10/Hw7Sol_171.pdf this note shows that $\ell^{1}$ itself is not compact. This post Closed and bounded but not compact subset of $\ell^1$ shows that even closed and bounded subset of $\ell^{1}$ is not compact (so our adorable closed unit ball is not compact). The only thing I got is the page 24 of this note: https://www.math.kit.edu/iana3/~schnaubelt/media/fa14-skript.pdf , but it only gives a sufficient and necessary condition for a subset $K\subset\ell^{p}$ to be relatively compact in $\ell^{p}$ , not compact. Is there a way to describe the compact subsets of $\ell^{1}$ ? Or is there any sufficient (and/or necessary) condition of a subset of $\ell^{1}$ to be compact? Thank you! Edit 1: As commented by Alessandro, I have known the sufficient conditions of relative compact. A subset $K$ that is relatively compact in $\ell^{1}$ has the closure $\overline{K}$ compact in $\ell^{1}$ . Therefore, if I require additionally the set to be closed, then the closure is the set itself and thus the set is compact in $\ell^{1}$ . Therefore, combining the Proposition 1.45 in the page 24 of the note I linked above. We have the following proposition: Proposition. Let $p\in[1,\infty)$ . A set $K\subset\ell^{p}$ is compact if and only if it is closed and bounded, and $$\lim_{N\rightarrow\infty}\sup_{(x_{j})\in K}\sum_{j=N+1}^{\infty}|x_{j}|^{p}=0.$$ However, I don't know if such a set truly exists. Is it possible to construct a set $K\subset\ell^{p}$ such that it satisfies all these requirements? Edit 2: As mentioned in the above edition, we have found a general sufficient condition. However, I am not sure if such a set really exists. As commented by ""Kavi"", one such set can be $\{\mathbf{0}\}$ . Indeed, it is clearly bounded. Any singleton is closed with respect to any metric space, proved here: are singletons always closed? . This set contains only zero sequence, so clearly it satisfies $$\lim_{N\rightarrow\infty}\sup_{(x_{j})\in K}\sum_{j=N+1}^{\infty}|x_{j}|^{p}=0.$$ Therefore, $\{\mathbf{0}\}$ is a compact subset of $\ell^{p}$ . However, is this the only set? Are there any other examples? ""Kavi"" commented that $\{\mathbf{0}\}$ is the only linear subspace that is compact in $\ell^{p}$ , why is this true? Does this mean $\{\mathbf{0}\}$ is the only compact subset? why? Thank you!","['general-topology', 'lp-spaces', 'functional-analysis', 'compactness']"
3870205,Solving dimension of an affine algebraic variety.,"By dimension, I used this definition: algebraic set $V$ has dimension d if maximum length of chains $V_0\subset V_1 \subset\cdots\subset V_d$ is $d$ where $V_i's$ are irreducible subvariety of $V$ , and all of them are distinct. Let $k$ be an algebraicly closed field, and consider $\mathbb{A}^3$ .
Let $X=Z(y-x^2,z-x^2)$ . I proved that $X$ is an affine variety by showing that $(y-x^2, z-x^2)$ is a prime ideal in $k[x,y,z]$ . However, I am struggling to show $X$ has dimension $1.$ Intuitively, it makes sense because $X$ is just a curve, but I don't know how to prove it. More specifically, how can I show that there is no irreducible subvariety between a point and the curve itself? Thanks!","['affine-varieties', 'algebraic-geometry', 'commutative-algebra']"
3870206,Geometric Similarity of Functions,"I am a 16 year old high school student and recently I have written a paper on a numerical approximation of distinct functions. I have shown my teachers this and they do not understand it. My questions: Is this a valid theorem to use to estimate functions with differently based functions? Has something similar already been created? Is it all useful/publishable? Any tips on how to improve? I will give an outline but you can find it here: https://www.overleaf.com/read/xjqhfgvrcrbj Definitions Geometric similarity refers to the dilation of a particular shape in all its dimensions. Proofs of geometric similarity are included in congruence proofs of triangles with AAA (Angle-Angle-Angle) proofs. Knowing the sizes of all sides of both triangles: $\triangle{ABC}$ and $\triangle{A'B'C'}$ , to find the dilation factor and prove geometric similarity the following must be true: $\frac{\mid A' \mid}{\mid A \mid} =\frac{\mid B' \mid}{\mid B \mid}=\frac{\mid C' \mid}{\mid C \mid}$ . Interpreting functions as shapes on the Cartesian plane and using geometry, geometrically similar functions can be calculated. Analytically this would imply for a function $y=f(x)\; \{x_0\leq x \leq x_1\}$ a geometrically similar function would be of the form $ny=f(nx)\;\{\frac{x_0}{n}\leq x \leq \frac{x_1}{n}\}$ where $n\in {\rm I\!R}$ . This is because the function is scaled by the same factor in the $x$ and $y$ direction thus would be geometrically similar. $y_1=\sin(x)\;\{0\leq x \leq 2\pi\}$ and $y_2=\frac{1}{2}\sin(2x)\; \{0 \leq x\leq \pi\}$ "" /> However to compare two functions which are distinct, multiplying $x$ and $y$ by $n$ will not suffice for proving similarity. The formula to find the dilation factor can be used to prove similarity between two functions. By describing a function geometrically it has three superficial 'edges' which can be represented as sets. Two of the edges are the two axis $x$ and $y$ . The length of the side ' $y$ ' is the $\max \{ f(x) : x = 1 .. n \}-\min \{ f(x) : x = 1 .. n \}$ and the length of the side $x$ is $b_1$ - $a_1$ where $b_1$ is the upper bound and $a_1$ is the lower bound. Finally the third side of the function will be the arc length over the interval $\{a_1\leq x\leq b_1\}$ . Another characteristic for two shapes to be geometrically similar is the area is increased by the dilation factor squared.Thus from the formula for the dilation factor for two similar triangles the following theorem can be derived: Theorem Let $y_1\;\{a_1\leq x \leq b_1\}$ and $y_2\;\{a_2\leq x \leq b_2\}$ be functions whose derivative exists in every point. If both functions are geometrically similar then the following system holds: \begin{equation}
    \frac{1}{\big(b_1-a_1\big)}\int_{a_1}^{b_1}  \sqrt{1+\bigg( \frac{dy_1}{dx}  \bigg) ^{2} } dx= \frac{ 1 }{ \big(b_2-a_2\big) } \int_{a_2}^{b_2}  \sqrt{1+\bigg( \frac{dy_2}{dx}  \bigg) ^{2} } dx
\end{equation} \begin{equation}
    \frac{1}{\big(b_1-a_1\big)^2} \int_{a_1}^{b_1} y_1 dx= \frac{1}{\big(b_2-a_2\big)^2}\int_{a_2}^{b_2} y_2dx 
\end{equation} Similarity Between Distinct Functions When describing a function as distinct it denotes that the functions have different bases, i.e. sinusoidal and exponential. As mentioned above, for geometric similarity to exist of a function $y=f(x)$ the resultant function will become $ny=f(nx)$ . However if comparing functions of different bases, equations (1) and (2) are necessary to find the bounds of similarity. For example, the problem: Find the bounds $b$ and $a$ where $e^x\;\{0\leq x\leq 1\}$ is similar to $x^2 $ . To see examples go to the above link. Any help would be much appreciated and apologies if this is crude mathematics.","['calculus', 'numerical-methods']"
3870288,Evaluating the challenging sum $\sum _{k=1}^{\infty }\frac{H_{2k}}{k^3\:4^k}\binom{2k}{k}$.,"I managed to evaluate the sum , my approach can be found $\underline{\operatorname{below as an answer}}$ , I'd truly appreciate if any of you could share new methods to evaluate this series, thank you. The following are the short proofs of the other series I encountered during my approach. $$\sum _{k=1}^{\infty }\frac{x^k}{4^k}\binom{2k}{k}=\frac{1}{\sqrt{1-x}}-1\tag{$\ast$}$$ $$\sum _{k=1}^{\infty }\frac{x^k}{k\:4^k}\binom{2k}{k}=\:-2\ln \left(1+\sqrt{1-x}\right)+2\ln \left(2\right)$$ $$\sum _{k=1}^{\infty }\frac{1}{k\:4^k}\binom{2k}{k}=\:2\ln \left(2\right)$$ On $\left(\ast\right)$ perform the following manipulations. $$-\sum _{k=1}^{\infty }\frac{1}{4^k}\binom{2k}{k}\int _0^1x^{k-1}\ln \left(x\right)\:dx=-\int _0^1\frac{\ln \left(x\right)\left(1-\sqrt{1-x}\right)}{x\sqrt{1-x}}\:dx$$ $$\sum _{k=1}^{\infty }\frac{1}{k^2\:4^k}\binom{2k}{k}=-2\int _0^1\frac{\ln \left(1-x^2\right)}{1+x}\:dx$$ $$=-2\int _0^1\frac{\ln \left(1-x\right)}{1+x}\:dx-2\int _0^1\frac{\ln \left(1+x\right)}{1+x}\:dx$$ $$=2\operatorname{Li}_2\left(\frac{1}{2}\right)-\ln ^2\left(2\right)$$ $$\sum _{k=1}^{\infty }\frac{1}{k^2\:4^k}\binom{2k}{k}=\zeta \left(2\right)-2\ln ^2\left(2\right)$$","['integration', 'definite-integrals', 'real-analysis', 'harmonic-numbers', 'sequences-and-series']"
3870324,Find point of $n$-sphere after moving on geodesic by given angle,"Let $\mathbb{S}^n=\{\mathbf{x}\in\mathbb{R}^{n+1}\colon\lVert\mathbf{x}\rVert^2=1\}$ be the unit n-Sphere and $\mathbf{x}\in\mathbb{S}^n$ . Given a vector $\mathbf{a}\in\mathbb{R}^{n+1}$ and an angle $\theta\in[-\pi,\pi)$ , we need to find a point $\mathbf{y}\in\mathbb{S}^n$ such that it belongs to both the n-Sphere surface and on the geodesic $g_a$ , as shown in the figure below. Edit: I will update the question asap with an answer I came up with after getting help from another question.","['arc-length', 'geodesic', 'riemannian-geometry', 'differential-geometry']"
3870434,How to find $AL$ in a triangle with one side trisected?,"Here is the original diagram of the question: Let $D$ and $E$ be the trisection points of $BC$ . And let $K$ and $L$ be points on $AB$ and $AC$ such that $\angle BKE= \angle DLC = \alpha$ . If $KB=16$ , $AK=12$ and $LC=6$ . Then find the length of $AL$ . The only thing I could think of that could use the fact that $\angle BKE$ and $\angle DLC$ are equal is the sine law. Also for convenience let $BD=DE=EC=y$ . $$\dfrac{2y}{\sin \alpha}=\dfrac{16}{\sin (\alpha + \angle B)}=\dfrac{KE}{\sin\angle B}$$ $$\dfrac{2y}{\sin \alpha}=\dfrac{6}{\sin{(\alpha + \angle C)}}=\dfrac{LD}{\sin \angle C}$$ Since I got $\sin \angle C$ and $\sin \angle B$ I thought of using sine rule again in the original triangle. $$\dfrac{x+6}{\sin \angle B}=\dfrac{28}{\sin \angle C}$$ $$\begin{align*}
\dfrac{\sin (\alpha + \angle B)}{\sin ( \alpha + \angle C)} & =\dfrac{16}{6} \\
\implies \dfrac{\sin (\alpha + \angle B)+\sin ( \alpha + \angle C)}{\sin (\alpha + \angle B)-\sin ( \alpha + \angle C)} & = \dfrac{11}{5} \\
\implies 2 \cdot \tan \biggl(\alpha + \dfrac{B+C}{2} \biggr) \cot \biggl( \dfrac{B-C}{2} \biggr) & = \dfrac{11}{5}
\end{align*}$$ But I could not really get anywhere with these equations. Since the terms got very messy and I really could not simplify anything. Would anyone please provide an elementary solution? Or perhaps give a hint on how to complete my approach?","['triangles', 'trigonometry', 'geometry']"
3870491,Inverse of Hodge star operator,According to Wikipedia [https://en.wikipedia.org/wiki/Hodge_star_operator] the inverse of Hodge star operator $*:\Lambda^k\rightarrow \Lambda^{n-k}$ is $*^{-1}:\Lambda^k\rightarrow \Lambda^{n-k}$ defined by $$\eta\rightarrow (-1)^{k(n-k)}*\eta.$$ But why it is defined in that way? How is it the inverse of $*$ operator? The definition seems it has been deduced from the twice Hodge star operator but how does it work?,"['riemannian-geometry', 'differential-geometry']"
3870494,"Let $\phi$ be a bijective function. Can we conclude that two monoids $(M_1,.)$ and $(M_2,∗)$ are isomorphic?","Condiser two monoids $(M_1,.)$ and $(M_2,*)$ with identity elements $e_1$ and $e_2$ and a bijective function $\phi$ which has the property $\phi(a.b)=\phi(a) * \phi(b)$ for all $a,b \in M_1$ can we conclude that $(M_1,.)$ and $(M_2,*)$ are isomorphic? My thoughts: from this A homomorphism between two monoids $(M_1,.)$ and $(M_2,*)$ is a function f : $M_1$ → $M_2$ such that $f(a.b)=f(a) * f(b)$ for all $a,b \in M_1$ $f(e_1)=e_2$ and then a bijective monoid homomorphism is called a monoid isomorphism. I'm saying if $f$ is bijective we need to show for all $c \in M_2$ : $c*f(e_1)=f(e_1)*c=c$ and then we can conclude $f(e_1)=e_2$ and the second condition is not nesseccery for being isomorphic. Consider $c \in M_2$ . beacuse $f$ is bijective so there is $x \in M_1$ which $f(x)=c$ so $c*f(e_1)=f(x)*f(e_1)=f(x.e_1)=f(x)=c$ and by doing the same for $f(e_1)*c$ we get $c*f(e_1)=f(e_1)*c=c$ and as I said $f(e_1)=e_2$ .
so the second condition is not nesseccery for being isomorphic. and only $f$ being bijective is enough. Is the conclusion I made correct?","['monoid', 'group-theory', 'abstract-algebra', 'group-isomorphism']"
3870580,"Given that $a<0$, how to compute the limit of $\int_{0}^t e^{a(t-s)}f(s)ds$ when $t\rightarrow{+\infty}$?","Given that $a<0$ , how to prove $I(t):=\int_{0}^t e^{a(t-s)}f(s)ds$ converges to $0$ when $t$ goes to positive infinity? This problem emerged when I tried to determine how the solution(s) of the following ODE converges when $f(t)\rightarrow 0(t\rightarrow +\infty)$ : $$a_0\frac{d^2x}{dt^2}+a_1\frac{dx}{dt}+a_2x=f(t).$$ From the theorems I've known about ODE, the solutions are of the form $$
x=e^{at}\left( c_1\cos b t+c_2\sin b t \right) +\int_0^t{K\left( t-s \right) f\left( s \right) ds}
$$ when the solution of $a_0\lambda^2+a_1\lambda+a_2=0$ are conjugate imaginary, i.e. , $\lambda_1=a+bi,\lambda_2=a-bi$ ,
where $K(t)$ denotes the kernel function $$
K\left( t \right) =\frac{e^{\lambda _1t}-e^{\lambda _2t}}{\lambda _1-\lambda _2}=\frac{e^{at}\sin bt}{b}.
$$ The first part of solution $e^{at}\left( c_1\cos b t+c_2\sin b t \right)$ obviously converges to $0$ because $\left( c_1\cos b t+c_2\sin b t \right)$ is bounded and $e^{at}$ converges to $0$ when $a<0$ . For the second part $
\int_0^t{}\frac{e^{a\left( t-s \right)}\sin b\left( t-s \right)}{b}f(s)ds$ , $\frac{\sin b(t-s)}{b}$ is bounded, so the uncertain part is $$\int_{0}^t e^{a(t-s)}f(s)ds\xrightarrow{?}0.$$ This is tricky for me because it contains 2 variables and I need to regard $t$ as a parameter primarily, which is beyond my knowledge and capability.
Any help would be appreciated! Thank you in advance!","['integration', 'multivariable-calculus', 'ordinary-differential-equations']"
3870623,On derivatives of the inverse of a real function,"Take two open intervals $I,J\subseteq \mathbb{R}$ and a bijection $f:I\rightarrow J$ with inverse $g:J\rightarrow I$ . For every $k \in \mathbb{N}, k \geq 1$ , I know that  if $f \in C^k(I,J)$ and for all $t \in I$ , $f'(t) \neq 0 $ then $g \in C^k(J,I)$ , but I don't know how to prove it. The way I thought I would go about this is: I find the general form of the $k$ -th derivative of $g$ I realize that $g^{(k)}$ is the composition of continuous functions and is therefore continuous The trouble is, the general form of $g^{(k)}$ seems to get very complicated, so I was wondering if there is any other way to show this in a simpler way (most likely using induction). My final aim is to prove that $f$ is smooth if and only if $g$ is. On a related sidenote, I seem to recall that the result is still true if we take $I,J\subseteq \mathbb{R}^n$ open and simply connected (with the jacobian of $f$ being invertible for all $x \in I$ ). I would just like to know if this is true out of curiousity. Edit: I forgot an ipotesis on the first derivative being not $0$ on all of $I$ , added it.","['multivariable-calculus', 'real-analysis']"
3870670,"Decomposition of the variation of a signed measure as $|\mu|(A) = \int_A |\frac{d\mu_{1a}}{d\mu_2}-1|d\mu_2 + \mu_{1s}(A)$, where $\mu=\mu_1-\mu_2$","Let $\mu_1$ and $\mu_2$ be two finite measures on $(\Omega, \mathcal{F})$ . Let $\mu_1 = \mu_{1a}+\mu_{1s}$ be the Lebesgue decomposition of $\mu_1$ w.r.t. $\mu_2$ , that is, $\mu_{1a} \ll \mu_2$ and $\mu_{1s}\perp \mu_2$ . Let $\mu = \mu_1 - \mu_2$ . I'd like to show that for all $A\in \mathcal{F}$ , $$ |\mu|(A) = \int_A |h-1|d\mu_2 + \mu_{1s}(A) $$ where $h = \frac{d\mu_{1a}}{d\mu_2}$ is the Radon-Nikodym derivative of $\mu_{1a}$ w.r.t. $\mu_2$ . I know that we have the following decomposition: $$|\mu|=\mu_+ + \mu_- $$ Here, $\mu_+(A) = \mu(A \cap \Omega_+)$ and $\mu_-(A)=\mu_-(A \cap \Omega_-)$ , where $\Omega = \Omega_+ \cup \Omega_-$ is the Hahn decomposition of $\Omega$ w.r.t $\mu$ . Also, there exists a finite measure $\lambda$ such that $\mu_1 = \mu_+ + \lambda$ and $\mu_2 = \mu_-+\lambda$ with $\lambda = 0$ iff $\mu_1 \perp \mu_2$ . By the definition of Radon-Nikodym derivative, we have $\mu_{1a}(A) = \int_A h d\mu_2$ for all $A\in \mathcal{F}$ . I am unable to use these facts to prove the desired result. Any hint as to how I should proceed would be highly appreciated. Edit: This problem is exercise 4.13 from the book ""Measure theory and Probability Theory"" by Krishna B. Athreya and Soumendra N. Lahiri.","['measure-theory', 'radon-nikodym', 'signed-measures', 'real-analysis']"
3870683,Problem working out a second derivative,"I am studying maths as a hobby and feel I am having problems working out second derivatives. The problem is as follows: Find the maximum or minimum values of the function $y = (2x - 5)^4$ Here is my working: $$dy/dx = 4(2x - 5)^3\cdot 2 = 8(2x -5)^3 ,$$ Which is zero when $x = 2\frac {1}{2}$ . To find whether this is a maximum or a minimum I find the second derivative: $$d^2y/dx^2 = 24(2x - 5)^2.2 = 48(2x - 5)^2.$$ But this is where I feel I must have gone wrong because this is zero when $x = 2\frac {1}{2}$ .","['maxima-minima', 'calculus', 'derivatives']"
3870732,On a minimal nonsolvable group,"By ""minimal nonsolvable"" I mean a nonsolvable group whose all proper subgroups are solvable. Let $G$ be a finite minimal nonsolvable group, with the following properties: 1- $G$ has only one proper normal subgroup $N$ , 2- $N$ is an elementary abelian 2-group; 3- $C_{G}(N)=N$ ; 4- $\dfrac{G}{N}\cong A_{5}$ . 5- $\forall x\in G$ , $o(x)\in\lbrace 1,2,3,4,5\rbrace$ . Does there exist any group $G$ with the above conditions?","['group-theory', 'simple-groups', 'finite-groups', 'solvable-groups']"
3870749,Determining a proper sample size in binomial distribution,"An agency is holding a poll, where each participant can either support or not support some upcoming motion. We model the answers with i.i.d. Bernoulli variables for which $X = 1$ means to support the motion with probability $P(X = 1) = p$ . We'd like that results of a poll are within $a$ percentage points of the true fraction with $b$ probability and we are trying to determine what number of people we need to interview in order for this poll to be reliable. We know that we are dealing with a binomial distribution and thus $E(X) = pn$ . With Chernoff bounds we get $P(X \geq (1 - d)E(X)) \leq e^{-(E(X)d^2)/2}$ and $P(X \geq (1 + d)E(X)) \leq e^{-(E(X)d^2)/(2 + d)}$ So if we set $p = \frac{m}{n}$ , where $n$ denotes the total number of participants and $m$ denotes the number of participants who support the motion, we have that $E(X) = pn = m$ . Therefore to me this question sounds like finding $m$ for which $P((1-d)E(X) \leq X \leq (1+d)E(X)) = 0.99$ . This ends up being equivalent of $0.01 = P(X \geq (1+d)E(X)) + P(X \geq (1-d)E(X))$ . However if I use the Chernoff bounds and expand this equation I end up in a situation where $0.01 = e^{-(E(X)d^2)/2} + e^{-(E(X)d^2)/(2 + d)}$ and I have no idea how to factor out the $E(X)$ . So is my reasoning correct and/or is there a less painful way of solving for $E(X)$ ?",['statistics']
3870779,Independent over rational number field,"Is there some theorems which can make sure that $$1, \frac{\log 2}{\log 3}, \frac{\log 3}{\log 2}$$ are $\mathbb{Q}$ -independent?","['number-theory', 'independence', 'algebraic-number-theory', 'transcendental-numbers']"
3870799,"$\mathbb{E}[f \mathbb{1}_{[0,\theta]}]=0$ for all $\theta \geq 0$ implies that $f=0$ almost surely?","my question is essentially in the title.
Do we have for a function on the nonnegative real line, if for all $\theta\geq 0$ we have $$
\mathbb{E}[f \mathbb{1}_{[0,\theta]}]=0
$$ then $f$ must be $0$ almost surely? I would think so since the intervals $[0,\theta]$ generate the Borel sigma algebra for the nonnevative real numbers but I can't find the rigorous argument. Any tips on this?
Thanks in advance.","['expected-value', 'measure-theory', 'probability-theory', 'probability']"
3870813,Prove that if $A^{t}A$ is idempotent then $A^{t}=A^{+}$,"Let $A_{m\times{n}}$ prove that if $A^{t}A$ is idempotent, then $A^{t}=A^{+}$ . I already proved the reciprocal, but I am having some troubles with this one. I think that I have to prove that the 4 conditions of the Moore-Penrose inverse holds. I would like to know if my prove is correct. My attempt:
Because $A^{t}A$ is idempotent, then $A^{t}AA^{t}A=A^{t}A$ $\hspace{1cm}$ associating we have $(A^{t}AA^{t})A=A^{t}A$ Then $A^{t}AA^{t}=A^{t}$ Is this correct? To prove that $AA^{t}A=A$ is analogous. And to prove that $AA^{t}$ and $A^{t}A$ is symmetric is a property already proved and I can use it. My question is: what property guarentees that if $(A^{t}AA^{t})A=A^{t}A$ then $A^{t}AA^{t}=A^{t}$ ?","['matrices', 'pseudoinverse', 'matrix-decomposition']"
3870848,The ordinary generating function for words whose longest run has length $\le k$,"Consider words on the alphabet $X=\{a,b\}$ . a)  I have to show that the Ordinary Generating function (OGF) for words on $\{a,b\}$ whose longest run has length $\leqslant k$ (at most $k$ ) is: $$
 W_{\leqslant k}(z)= \frac{1-z^{k+1}}{1-2z+z^{k+1}}= \frac{1+z+\dots+z^k}{1-z-\dots-z^k } 
$$ I know that I have to use the definition of the set of words: $$ W(z)= \frac{1}{1-2z} 
$$ where $2$ is the cardinality of the alphabet, i.e. the number of letters. I need to know how to use this information to find the ordinary generating function. b) How likely is that a word of length $250$ contains a run of length $7$ or more?","['discrete-mathematics', 'generating-functions']"
3870856,Is this Integration From First Principles notation standard outside of A Level Maths?,"I teach A Level Maths in the UK. We are required to do some 'introduction' to integral from first principles as part of the specification ( link , page 25 is the interesting part). In a previous exam question (Paper 2, June 2018), this was essentially the question: Suppose we have the curve $y= \sqrt{x}$ The point $P(x,y)$ lies on the curve. Consider a rectangle with height $y$ and width $\delta x$ . Calculate $\displaystyle \lim_{\delta x \rightarrow 0}\sum_{x = 4}^9 \sqrt{x} \,\delta x$ The answer involves us recognising $$ \lim_{\delta x \rightarrow 0}\sum_{x = 4}^9 \sqrt{x} \, \delta x = \int_4^9 \sqrt{x} \, dx$$ and evaluating the integral. Is this notation standard? To me, it doesn't make sense. How can you have $x=4$ to $9$ as the limits on the sum, for example? A sum only works over integral values. In addition, one could easily give meaning to the limit as $\displaystyle \lim_{\delta x \rightarrow 0}\sum_{x = 4}^9 \sqrt{x} \, \delta x = \lim_{\delta x \rightarrow 0}( \sqrt{4}(\delta x) + \sqrt{5} (\delta x) + \cdots + \sqrt{9} (\delta x))$ which should be $0$ as $\delta x \rightarrow 0$ . The reason I am so confused is that this question and notation appears on a Pearson A Level Maths Exam paper. It is a regulated qualification. There are very qualified people out there that have deemed this to make sense and be used to assess the understanding of thousands of A Level students in the UK.","['notation', 'riemann-sum', 'riemann-integration', 'real-analysis']"
3870947,"To show a sequence is bounded, monotone and to find its limit","I am new to analysis and following is the question: Show that the sequence $\frac{n+1}{n}$ is monotone, bounded and find its limit. The way I approached it is the following: To show that it is monotone, We can write the sequence as $a_n = 1 + \frac{1}{n}$ . Since $n_{2} > n_{1}$ , we have that $\frac{1}{n_2}<\frac{1}{n_1} $ . And hence $1 + \frac{1}{n_{1}} > 1 + \frac{1}{n_{2}}$ . So this shows that the sequence is monotonically decreasing. Question 1: With analysis I never know if my argument is complete, so is it complete here? Am I missing something? To show that it is bounded, We know that since $n\in \mathbb{N}$ , we have that $0 < \frac{1}{n} \le 1$ , and so $1 < 1+\frac{1}{n} \le 2$ . Hence it is bounded. Question 2: Another analysis question, how do I even know that I am not using things that have not been defined yet? Like, have I taken things for granted in my proof above? Finally, to find the limit, Can we just say that since this is a monotonically decreasing sequence, that is also bounded we can say that: $\lim_{n\rightarrow \infty} x_{n} = inf$ ${x_{n} : n \in \mathbb{N}}$ , we can say that the limit in this case would be 1? Question 3: I feel like this is not enough, and we would still have to show officially that 1 is the infimum of this sequence, which I am not sure how I can prove without saying that it makes intuitive sense for me? So if someone could tell me what the official proof of this part would be that would be great. Final Question: Is what I have so far correct or have I made any assumptions that one should not make while solving analysis questions?!","['sequences-and-series', 'monotone-functions', 'analysis', 'real-analysis']"
3870962,"Show that $\int_0^{\frac{\pi}{4}}(\tan^{n+2}x + \tan^nx)\,dx = \frac{1}{n+1} $","Show that the integral of $$\tan^{n+2}x + \tan^nx = \frac{1}{n+1} dx$$ between the limits $\frac{\pi}{4} , 0$ I attempted to use u-substitution: $$u = \tan x$$ $$\frac{du}{dx} = \sec^2x$$ $$\frac{du}{\sec^2x} = dx$$ Well, here's what I got (I can't input the integral sign, sorry): $$\frac{1}{\sec^2x}(u^{n+2} + u^2) \, du$$ I don't understand how to get rid of the $\frac{1}{\sec^2x}$ . I am a bit new to integration, so can someone shed some light on what the next steps would be? How do I get rid of $\sec^2x$ ?","['integration', 'calculus', 'definite-integrals']"
3870972,How do I find a function $f(3n) = 3n$ such that it is different from the identity function?,"This is my first time posting. I'm sorry if I'm neglecting some good etiquette practices; I tried to read everything that's been sent my way, but I probably missed something anyway. Also, English is not my first language, so I'm relying on Google to translate math-specific terminology. If something isn't clear, please let me know! I'm a Computer Science student at University, and I've been requested to find a function $f: \Bbb{N}\to\Bbb{N}$ such that $\forall n \in \Bbb{N}$ , $f(3n) = 3n \land f\neq \mathrm{id}_\Bbb {N}$ . I absolutely cannot find a solution, as $f(x) = x$ (and, as such, $f(3n) = 3n$ too)  literally is the definition of identity function as far as I know... Am I missing something? Thanks in advance. EDIT: Thanks a lot everyone!",['functions']
3870989,Connection on pullback bundle.,"Let $E\to N$ be a vector bundle and $f:M\to N$ be a smooth map.
The pullback $f^*:\Omega^k(N,E)\to \Omega^k(M,f^*E)$ is then defined by $$(f^*\omega)_x(v_1,...,v_k)=\omega_{f(x)}((df)_x(v_1),...,(df)_x(v_k))$$ with $x\in M$ and $v_1,...,v_k\in T_xM$ . First question : how can I rewrite this pullback as a map $f^*\omega:\mathfrak{X}(M)\times...\times \mathfrak{X}(M)\to \Gamma(M,f^*E)$ ?
My problem is that $df:\mathfrak{X}(M)\to \mathfrak{X}(N)$ is well defined as long as $f$ is a diffeomorphism. However, $df:\mathfrak{X}(M)\to \Gamma(M,f^*TN)$ is actually well defined. Then, I know that the connection $f^*\nabla$ on $f^*E\to M$ is uniquely determined by $$(f^*\nabla)(f^*s):=f^*(\nabla(s))\in \Omega^1(M,f^*E)$$ Second question: In some books, I saw the notation $$(f^*\nabla)_X(f^*s)=f^*(\nabla_{df(X)}(s))=\nabla_{df(X)}(s)\circ f$$ but as long as $f$ is not a diffeomorphism $df(X)$ is not a vector field on $N$ so it doesn't make sense writing $\nabla_{df(X)}$ . How can I solve this problem?","['connections', 'vector-bundles', 'differential-geometry']"
3871018,How to evaluate a limit here?,"I have a certain limit to evaluate: $\lim_{x\to  0}\frac{4^x-1}{\ln{(7x+1)}}$ Here are the steps I've followed: suppose $4^x-1$ is t. This means that $t\to0$ . This also means that $x=\log_4(t+1)$ My limit is now: $\lim_{t\to  0}\frac{t}{\ln(7log_4(t+1) + 1)}$ Now $\ln(log_4(t+1) +1)$ is equal to $log_4(t+1)$ beacuse the $ln$ function is also $\to0$ This, i suppose, leaves us with the following limit: $\lim_{t\to  0}\frac{t}{7log_4(t+1)}$ At my last step I got really confused. Where else can we go from here? Every time I find myself with an indermediate form of [ $\frac{0}{0}$ ]. Thanks in advance for any help.","['limits', 'real-analysis']"
3871047,Nested radicals like Ramanujan's infinite radicals,"$$\sqrt{1+\sqrt{5+\sqrt{11+\sqrt{19+\sqrt{29+...}}}}}=?$$ This like the
My question is about ""how to start this problem ?"".I've been thinking for over two hours but get stuck at the end. I tried by a calculator to approximate the value, I find out if I define a sequence like below,It seems $a_n\to 2$ $$a_1=\sqrt1\\
a_2=\sqrt{1+\sqrt{5}}\sim 1.798907\\
a_3=\sqrt{1+\sqrt{5+\sqrt{11}}}\sim 1.97075\\
a_4=\sqrt{1+\sqrt{5+\sqrt{11+\sqrt{19}}}}\sim 1.99661\\
a_5=\sqrt{1+\sqrt{5+\sqrt{11+\sqrt{19+\sqrt{29}}}}}\sim 1.99967\\\vdots \\a_n  \text{tends to } 2$$ But I have no idea to solve it analytically. Can someone help me? or get me the clue?","['nested-radicals', 'calculus', 'sequences-and-series']"
3871052,Prove a series of a subsequence converges.,"Let $\sum_{n=1}^{\infty} a_n$ converge. Let $(n_k)_{k=1}^{\infty}$ be a subsequence of the sequence of positive integers. For each k, define: $$b_k = a_{n_{k-1}+1} + ...+ a_{n_k}$$ where $n_0 = 0$ . Prove $\sum_{n=1}^{\infty} b_k$ converges and that $\sum_{n=1}^{\infty} a_n = \sum_{k=1}^{\infty} b_k$ . This is the question I am looking at. I know that each subsequence of a convergent sequence also converges, and therefore because if a series converges, the sequence must also converge. I guess I'm just struggling notationally with this. I'm not really sure what $b_k$ is defining. I'm not looking for an answer (besides, this problem will not be graded, it is for practice), but a little help in the right direction would be greatly appreciated.","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
3871055,Prove $0 < x < \pi /2 \implies \sin x > x/\sqrt{x^2+1}$ using Mean Value Theorem,"I'm solving the following problem: Show that if $0 < x < \pi /2$ then $\sin x > \dfrac{x}{\sqrt{x^2+1}}$ . One of the hints given is to apply mean value theorem for $\sin (x)$ on the interval $[0,x]$ This is my attempt so far: Let $f(x) = \sin(x)$ Since all trigonometric functions are continuous and $\sin (x)$ is differentiable, mean value theorem can be applied. $$\frac{\sin x - \sin 0}{x - 0} = \cos c$$ $$\frac{\sin x}{x} = \cos c$$ We know that $0 < c < x$ So, $0 < c < x \leq \pi / 2$ So, $0 \leq \cos c < 1$ Also, $\cos c > \cos x$ $$\cos c > \sqrt{1 - \sin^2 x}$$ $$\frac{\sin x}{x} > \sqrt{1 - \sin^2 x}$$ $$\sin x > x\sqrt{1 - \sin^2 x}$$ Now after this I'm stuck. I'm not sure how to bring $\sqrt{x^2 + 1}$ into the proof! I did think over it and was able to find some relations involving it like: $$\sqrt{x^2 + 1} > 1$$ But I think I'm going the wrong path. How should I complete the proof ?","['calculus', 'derivatives', 'trigonometry']"
3871125,Is 0 and 1 invertible in modulo p?,Is 0 and 1 invertible in modulo p? I think 0 never is while 1 always is? E.g. 1 in modulo 13 is invertible to 13 and since 1*1=1 modulo 13.,"['modular-arithmetic', 'discrete-mathematics']"
3871144,L'Hospital rule to find limit on indeterminate form.,"I have the following limit to compute: $$\lim_{x\to 0}{\left(\cos x -1\over {5 x^2}\right)}$$ I need to find the limit as $x \to 0$ .  I tried using L'Hospital rule , so I found the derivative of the numerator, which is: $${-\sin x}$$ The derivative of the denominator is: $${10x}$$ Now I have the following: $${-\sin x \over {10x}}$$ What should I do now? , I'm not sure on how I could apply direct substitution to this problem?","['limits', 'substitution', 'derivatives']"
3871155,Ideals of ring of continuous functions on a compact Hausdorff space.,"I am curious to know if the following question can be answered by my naive approach. I understand that there is a huge body of work in rings of continuous function and some really deep ideas. Consider the ring of continuous functions on a compact Hausdorff space $X$ (taking $X=[0,1]$ is also fine as far as this question is concerned), denoted $C(X)$ . Let $I$ be a proper ideal. Is $I=I(S)$ for some $S$ , where $I(S):=\{f\in C(X)\mid  f(s)=0\, \forall s\in S\}$ ?? My naive approach: We know that $I$ is contained in some maximal ideals which are of the form $I(p)$ for some point $p\in X$ according to the above notation. We take all such maximal ideals containing $I$ and intersect them. So $I\subset \bigcap I(p)=I(\bigcup \{p\})$ . I was hoping to show an equality here, but I am not sure how to approach this or whether it is even true that they are equal?? I wonder if taking $f\in \bigcap I(p)$ and $f\notin I$ leads to a contradiction by breaking some maximality condition; like now we have $I\subset (I,f)\subset I(\bigcup \{p\})$ an so on... Any help will be appreciated. Thanks in advance.","['general-topology', 'ring-theory', 'ideals', 'real-analysis']"
3871178,Question regarding the proof of $f^{−1 }(Y \setminus C) = X \setminus f^{−1 }(C)$,"I have a question which says the following:
Let: $f: X \to Y$ be a function, and assume that $C \subset Y$ . Show that $f^{-1}(Y \setminus C) =  X \setminus f^{-1}(C)$ I'm not too sure where to go with this question and I have a few more examples after this one which have a similar pattern. I'd really like if someone could show me how to solve these or direct me to some resources which have examples I could take a look at in order to try and solve a few myself. Thank you.","['elementary-set-theory', 'functions']"
3871198,"A complex integral which shouldn't diverge, where is my mistake?","First of all I have a small request. I was banned for a very long time because I got lazy and formulated my questions without any effort. I accept the punishment and I'm happy I got a second chance. From now on I try to put as much effort in my questions as possible. This is my first question after the ban and it would make me happy if you don't down-vote if you think that my question is not good or not understandable since I'm probably getting banned again. Please write suggestions for improvement in the comments. Thank you and sorry for this way too long introduction. So here we go :) What do I want to solve? I want to calculate $$I = \cfrac{1}{2\pi i}\int \limits_{c-i\infty}^{c+i\infty}\cfrac{\log(s-1)}{s^2}\,x^s \mathrm{d}s,$$ where $c>1$ along the same contour as in ""Applications of integral theorems"", example 5: https://en.wikipedia.org/wiki/Contour_integration#Example_5_–_the_square_of_the_logarithm . To implement our integral $I$ we modify our desired closed contour integral to $$\oint_\mathcal{C}= \left(\int_{R_1}+\int_{R_2}+\int_M+\int_N+\int_r+\int\limits_{c-i\infty}^{c+i\infty}\right)\cfrac{\log(s-1)}{s^2}\,x^s\mathrm{d}s,$$ where $R_1$ ends at $c+i\infty$ and $R_2$ ends at $c-i\infty$ .
The whole integral is actually an Inverse laplace transform, if we define $t=\log(x)$ . If we put $\frac{\log(s-1)}{s^2}$ in this online calculator https://www.emathhelp.net/calculators/differential-equations/inverse-laplace-transform-calculator/?f=ln%28s-1%29%2Fs%5E2 we get $-t\operatorname{Ei}(t)+e^t-1 \Longleftrightarrow-\log x\operatorname{li}(x)+x-1$ . How I tried to solve it The integrals $R_1,R_2$ and $r$ tend to zero (I use the same notation as in the Wikipedia link). The closed contour is also equal to zero since $0$ is excluded from the contour. After parameterizing $s=-s_\mathcal{R}+i\varepsilon$ for $M$ and $s=-s_\mathcal{R}-i\varepsilon$ for $N$ we get: $$I + \int\limits_{0}^{\infty}\cfrac{\log\lvert-s_\mathcal{R}+i\varepsilon-1\rvert + i\pi}{(-s_\mathcal{R}+i\varepsilon)^2}\,x^{-s_\mathcal{R}+i\varepsilon}\mathrm{d}s_\mathcal{R}-
    \int\limits_{0}^{\infty}\cfrac{\log\lvert-s_\mathcal{R}+i\varepsilon-1\rvert - i\pi}{(-s_\mathcal{R}-i\varepsilon)^2}\,x^{-s_\mathcal{R}-i\varepsilon}\mathrm{d}s_\mathcal{R} = 0$$ If we now let $\varepsilon\rightarrow0$ we can also evaluate the absolut value. Our new expression is $$\begin{align}
    I + \int\limits_{0}^{\infty}\cfrac{\log(s_\mathcal{R}+1) + i\pi}{s_\mathcal{R}^2}\,x^{-s_\mathcal{R}}\mathrm{d}s_\mathcal{R}-
    \int\limits_{0}^{\infty}\cfrac{\log(s_\mathcal{R}+1) - i\pi}{s_\mathcal{R}^2}\,x^{-s_\mathcal{R}}\mathrm{d}s_\mathcal{R}
    =0,
\end{align}$$ which is the same as saying that $$I = -2\pi i\int\limits_{0}^{\infty}\cfrac{1}{x^{s_\mathcal{R}}s_\mathcal{R}^2}\mathrm{d}s_\mathcal{R}$$ And now the disappointment: If I put ""integral from 0 to infinity of 1/(a^x*x^2)"" into Wolfram Alpha it returns ""(integral does not converge)"". I can't find the mistake I did while evaluating. Please give me a small hint what I should correct to get the desired result. Thank you :)! Edit: I want to thank Maxim for helping me with my question. My mistake was the definition of the branch of $\log(s-1)$ . Now I (only) have to solve $$\lim_{\varepsilon\to0}\left(\int\limits_{0}^{\infty}\cfrac{\log\lvert-s_\mathcal{R}+i\varepsilon-1\rvert + i\pi}{(-s_\mathcal{R}+i\varepsilon)^2}\,x^{-s_\mathcal{R}+i\varepsilon}\mathrm{d}s_\mathcal{R}-
    \int\limits_{0}^{\infty}\cfrac{\log\lvert-s_\mathcal{R}+i\varepsilon-1\rvert - i\pi}{(-s_\mathcal{R}-i\varepsilon)^2}\,x^{-s_\mathcal{R}-i\varepsilon}\mathrm{d}s_\mathcal{R}\right),$$ what could get a little bit hard since I'm not allowed to set $\varepsilon$ to $0$ immediatly.",['complex-analysis']
3871210,Show that $S_{\mathbb{N^*}}$ isn't countable.,"Let $S_{\mathbb{N^*}}$ be the set of all permutations of $\mathbb{N^*}$ . Show that $S_{\mathbb{N^*}}$ isn't countable. Let $f : \mathbb{N^*} \mapsto S_{\mathbb{N^*}}$ be a bijection. I introduced $$
\sigma(k)
=
\begin{cases}
\min
\{
l \mid l\neq f(k)(k) \text{ et } l \neq \sigma(i) \, \forall i \in [1,k-1]
\} ,&\text{if }k>1\\
\min
\{
l \mid l\neq f(k)(k)
\} &\text{if }k=1\\
\end{cases}
$$ It's clear that $\sigma$ is injective, but I don't know how to show that it's surjective. Do you have any hint?","['elementary-set-theory', 'functions']"
3871217,Continuity of the Random Variable Defining the Occupation Measure of Gaussian Process,"Suppose $Z:\Omega \times [0,1] \to \mathbb{R}$ is a continuous Gaussian process with mean $\mu(t)$ and covariance kernel $C(t,s)$ . Consider the random variable $$
X_\alpha = \lambda( \{t \; : \; Z(t) > \alpha \})
$$ where $\lambda$ is Lebesgue measure. $X_\alpha$ is a random variable taking values in $[0,1]$ . Another thing to note is that for many Gaussian processes the distribution of $X_\alpha$ will have mass at 0 and 1.  My question is: under what conditions on $C$ is $$
P( X_\alpha = x ) = 0, \mbox{ for all } x \in (0,1)? 
$$ Clearly this holds for many well known Gaussian processes, e.g. Brownian motion, and does not hold for many others (for example, suppose $Z(t) = N\sin(2\pi t)$ , where $N\sim \mathcal{N}(0,1)$ ). I hoped there would be some result in the literature giving guidance on this, but I cannot find anything.","['stochastic-processes', 'absolute-continuity', 'probability-theory', 'stochastic-calculus']"
3871223,Limit of the sequence using intermediate value theorem $\lim_{x\to\infty}[\frac{1}{\sin(\frac{1}{x+\psi(x)})}-\frac{1}{\sin(\frac{1}{x+\phi(x)})}]$,"Find the limit of: $$\lim_{x\to\infty}[\frac{1}{\sin(\frac{1}{x+\psi(x)})}-\frac{1}{\sin(\frac{1}{x+\phi(x)})}]$$ where: $$\psi(x)=(1+\frac{1}{x})^x, ~~~~~~~\phi(x)=\sqrt[x]{x}$$ I used Lagrange theorem for the intermediate value for $f(x)=\frac{1}{\sin(x)}$ , which is a recommended way of solving the problem, but I am stuck now and I would ask you for some help. $$\frac{f(b)-f(a)}{b-a}=f'(c),~~~~c\in(a,b)\\ f(b)-f(a)=f'(c)\cdot(b-a) \\ \lim_{x\to\infty}\frac{\frac{1}{\sin(\frac{1}{x+\psi(x)})}-\frac{1}{\sin(\frac{1}{x+\phi(x)})}}{x+\psi(x)-x-\phi(x)}=\lim_{x\to\infty}\frac{\cos(\frac{1}{c})}{\sin^2(\frac{1}{c})\cdot x^2}\cdot (\psi(x)-\phi(x))$$ That's the end of my story. I have also tried $\frac{1-\cos(2\alpha)}{2}=\sin^2(\alpha)$ , but it leaded me to nowhere. I would really appreciate your hints.","['limits', 'trigonometry', 'real-analysis']"
3871267,Intuition for Conjugacy Classes in Groups,"On the Wikipedia page for Conjugacy Classes, it says that ""members of the same conjugacy class cannot be distinguished using the group structure alone"". In what sense is this true? I can see that this is not meant to taken literally: for example, it's not true that if two elements of a group are conjugate, they act the same way in the group multiplication table.  So in what sense can conjugate elements ""not be distinguished""? The best that I can come up with is that conjugate elements have the same order -- but the opposite direction is not even true in general! (i.e: $|x| = |y|$ does not imply $x$ and $y$ are conjugate) Are there other properties conjugate elements share -- properties that $x$ and $y$ share if and only if they are conjugate? To be clear, I understand why conjugacy is important in certain examples. For instance: In $GL(n, \mathbb{F})$ , for example, if two matrices are conjugate then they have the same rank, nullity, trace, determinant, and so on. In $S_n$ , two permutations are conjugate if and only if they have the same cycle type. But in a general group $G$ , I cannot see why saying "" $x$ and $y$ are conjugate"" is significant / what it tells us. Any clarification would be much appreciated. Thanks!","['group-theory', 'finite-groups', 'intuition']"
3871274,"Integral from Mathematica's documentation: $\int_0^1 \frac{\log (\frac{1}{2}(1+\sqrt{4 x+1}))}{x} \, dx = \frac{\pi^2}{15} $","I like to peruse Mathematica's documentation and look at the 'Neat Examples': this is one I managed to figure out. Apparently it's due to Ramanujan: $$
I=\int_0^1 \frac{\log \left(\frac{1}{2} \left(1+\sqrt{4 x+1}\right)\right)}{x}
   \, dx = \frac{\pi^2}{15}. 
$$ Here are the steps for my solution: Make the substitution $x=y^2-y$ , yielding $$
    I= \int _{1}^{\phi}\frac{\log(y)(2y-1)}{y(y-1)}\,dy,  
$$ where $\displaystyle{\phi = \frac{1+\sqrt{5}}{2}}$ is the golden ratio. Factor out the $\log(y)$ term and use partial fractions to write $$I = \underbrace{\int _{1}^{\phi}\frac{\log(y)}{y}\,dy}_{I_1}  + \underbrace{\int _{1}^{\phi}\frac{\log(y)}{y-1}\,dy}_{I_2}
$$ $I_1$ can be evaluated using a simple substitution, yielding $\displaystyle{I_1 = \frac{\log ^2(\phi )}{2}}$ . Use the Taylor series for $\log(y)$ centered at $y=1$ and interchange the sum and integral to show $$
    I_2 = -\sum_{k=1}^{\infty} \frac{(1-\phi)^{k}}{k^2}= -\sum_{k=1}^{\infty} \frac{(-\phi^{-1})^{k}}{k^2}= - \text{Li}_2(-\phi^{-1})
$$ $\text{Li}_2$ has the following properties: $\text{Li}_2(x) + \text{Li}_2(-x) = \frac{1}{2}\text{Li}_2(x^2)$ $\text{Li}_2(x) + \text{Li}_2(1-x) = \zeta(2) - \log(x)\log(1-x)$ $\text{Li}_2(1-x) + \text{Li}_2(1-x^{-1}) = -\frac{1}{2}\log^2(x)$ Put $x=\phi^{-1}$ and use $\phi^2=\phi+1$ ; this gives: $$
    \text{Li}_2(\phi^{-1}) + \text{Li}_2(-\phi^{-1}) = \frac{1}{2}\text{Li}_2(1-\phi^{-1})
$$ $$
    \text{Li}_2(\phi^{-1}) + \text{Li}_2(1-\phi^{-1}) = \zeta(2) -2 \log^2(\phi)
$$ $$
    \text{Li}_2(1-\phi^{-1}) + \text{Li}_2(-\phi^{-1}) =-\frac{1}{2}\log^2(\phi)
$$ 5. Relabel for clarity. Let $A=\text{Li}_2(\phi^{-1})$ , $B=\text{Li}_2(-\phi^{-1})$ , $C=\text{Li}_2(1-\phi^{-1})$ , and $L= \log^2(\phi)$ . This gives the system $$
    \begin{cases}
    A+ B & = \frac{1}{2}C\\
    A+ C&= \zeta(2)- 2L\\
    C+B &= -\frac{1}{2}L
    \end{cases}
$$ Solving gives $B=-I_2=\displaystyle{\frac{1}{2}L-\frac{2}{5}\zeta(2)}$ , whence $\displaystyle{I = \frac{\pi^2}{15}}.$ I'd be curious to see if there are any other methods of proof, perhaps involving simpler substitutions than the ones I used.","['integration', 'improper-integrals', 'definite-integrals', 'alternative-proof', 'polylogarithm']"
3871287,Find a number divisible by 7 by erasing digits from a seven digit number,"Let $x$ be a seven-digit number. Prove that only by erasing digits from the left side or the right side (or both) of the number, you can remain with a number that is divisible by $7$ . I assume the Pigeonhole Principal is useful here, however I'm having trouble finding its application here. I would like to get a hint (or the full proof with a hint beforehand). Thanks","['pigeonhole-principle', 'combinatorics']"
3871313,Looking for help understanding general mathematics with second order differential equation. Solution doesn't work.,"I've been in pursuit of general solution to a second order homogeneous ODE for a long time. My degree is in the sciences, so my mathematics is built mostly around that. Consider my an amateur mathematician I guess. Anyway, I was trying to figure out how one might solve $$y(z)'' + f(z)y(z)' + g(z)y(z) = 0$$ I've tried many different avenues and found many cool and interesting relationships, but nothing that solves this for y(z) in terms of f(z) and g(z). This is my most recent attempt. Start with $$y(z) = k(z)$$ Exponentiate in a logarithm $$y(z) = e^{ln(k(z))}$$ Integrate the derivative of ln(k(z)).  The constant on the integral must be 0 to retain the initial conditions. $$y(z) = e^{\int \frac{k(z)'}{k(z)} dz}$$ Multiply by $\frac{k(z)'}{k(z)}$ on  both sides $$y(z) \frac{k(z)'}{k(z)} = \frac{k(z)'}{k(z)} e^{\int \frac{k(z)'}{k(z)} dz}$$ Integrate across. $a$ is some constant. $$\int y(z) \frac{k(z)'}{k(z)} dz + a = \int \frac{k(z)'}{k(z)} e^{\int \frac{k(z)'}{k(z)} dz} dz$$ The right hand side evaluates to e^{\int \frac{k(z)'}{k(z)} dz}, so we have$$ $$\int y(z) \frac{k(z)'}{k(z)} dz + a = e^{\int \frac{k(z)'}{k(z)} dz}$$ But we know that $e^{\int \frac{k(z)'}{k(z)} dz} = y(z)$ so we have $$y(z) = \int y(z) \frac{k(z)'}{k(z)} dz + a$$ Now we'll recurse once $$y(z) = \int \Bigl( \int y(z) \frac{k(z)'}{k(z)} dz + a  \Bigr) \frac{k(z)'}{k(z)} dz + a$$ And now we take a derivative $$y(z)' = \Bigl( \int y(z) \frac{k(z)'}{k(z)} dz + a  \Bigr) \frac{k(z)'}{k(z)}$$ And move that fraction over. $$y(z)' \frac{k(z)}{k(z)'} = \int y(z) \frac{k(z)'}{k(z)} dz + a $$ And one more derivative. $$y(z)'' \frac{k(z)}{k(z)'} + y(z)' \Bigl( 1 - \frac{k(z)''}{k(z)'^2} \Bigr) = y(z) \frac{k(z)'}{k(z)}$$ Lets subtract the right hand side over now $$y(z)'' \frac{k(z)}{k(z)'} + y(z)' \Bigl( 1 - \frac{k(z)''}{k(z)'^2} \Bigr) - y(z) \frac{k(z)'}{k(z)} = 0$$ And multiply by $ \frac{k(z)'}{k(z)} $ across the whole equation. $$y(z)'' + y(z)' \Bigl( \frac{k(z)'}{k(z)} - \frac{k(z)''}{k(z)'} \Bigr) + y(z) \Bigl( -\frac{k(z)'^2}{k(z)^2} \Bigr) = 0$$ If you check this with $ y(z) = k(z),$ you will find that it yields a correct solution. $$y(z)'' + \frac{y(z)'^2}{y(z)} - y(z)'' - \frac{y(z)'^2}{y(z)} = 0$$ Just a heads up, when I say and function, I mean those functions which are differentiable and integrable. Okay, thats neat. This seems to me like it should be pretty fool proof for any function. If you know k(z), then you automatically know y(z), because they are the same. Just set $\Bigl( \frac{k(z)'}{k(z)} - \frac{k(z)''}{k(z)'} \Bigr)$ to f(z) and $\Bigl( -\frac{k(z)'^2}{k(z)^2} \Bigr)$ to g(z). Both of those are easy enough to solve. Here's the thing though, it doesn't work. Also, in setting to two above coefficients to f(z) and g(z) respectively, you have two equations and can solve for f(z) in terms of g(z). It comes out to be $\,2f(z) = g(z)'/g(z)\,$ if you're interested, and this doesn't hold for any of the examples I tried. I even tried going through and solving an example with constant coefficients and then reverse plugging into the equation I just explained. I wondered if maybe the coefficients would pop out and the rest would cancel, but nope. No dice. I was left with something of the form $$\frac{1}{c_1e^{az} + c_2e^{-az}}$$ So my question is, why doesn't this work? If I have $y(z) = k(z)$ and I make any other substitution for k(z), it could always be re-substituted back with k(z) at the end. And if I try to substitute in and other function for f(z) or g(z), it will always be able to be re-substituted back in at the end also. As far as I can tell, I haven't broken any rules. I feel very confused about this and strangely anxious. If anyone is able to shed some light on why this doesn't work, I'd be very grateful.",['ordinary-differential-equations']
3871319,How many directed acyclic graphs (DAG) s are possible using N vertices?,"If all the number of all the possible DAGs are K, then K can consist of disconnected vertices (not all connected vertices) from the directionality of a particular edge is counted as a new graph My attempt to analyze: Example for 2 nodes X,Y ; G1 : {X,Y} ( all disconnected) G2 : {X->Y} G3 : {X<-Y} ( directionality changes) But for 3 nodes X, Y, Z; there are a large number of possible graphs; set 1: with 1 edge set 2: with 2 edges set 3: with 3 edges set 4: with 0 edges Likewise, the possible space is growing. I tried, this resource but when I try to cont it for 2 edges it should be (2^(2^2)), but it violates my above analysis. Maybe this solution is not applicable to my situation. May I know any clue on how to count the number of all the possible DAGs that can be generated using N vertices incorporating the above analysis?","['graph-theory', 'combinatorics', 'directed-graphs']"
3871322,"Closed-form of $\mathbb E(\|G\|_\infty)$ where $G\sim\mathcal N(0,\mathbf{Id}_n)$.","Let $I_n = \mathbb E(\|G\|_\infty)$ , i.e. $$I_n = (2\pi)^{-\frac{n}{2}}\int_{x\in\mathbb R^n}\|x\|_\infty e^{-\frac{1}{2}\|x\|_2^2}\,dx.$$ I wonder if I can get its closed-form. By symmetry I got $$I_n = 2n\sqrt{\frac{2}{\pi}}\int_0^\infty xe^{-x^2}\operatorname{erf}(x)^{n-1}\,dx,$$ and then by integration by parts, for $n\ge2$ , $$I_n = \frac{2\sqrt2}{\pi}n(n-1)\int_0^\infty e^{-2x^2}\operatorname{erf}(x)^{n-2}\,dx,$$ where $\operatorname{erf}$ is the error function. These two formulas give me $$I_1 = \sqrt{\frac{2}{\pi}},\quad I_2 = 2\sqrt{\frac{1}{\pi}},\quad I_3 = \frac{12}{\pi\sqrt\pi}\arctan\frac{\sqrt2}{2}.$$ In this step, I think a general closed-form is almost impossible, so I post here to see if anyone has a better approach (at least for $I_4$ ). Update Series expansion of $I_4$ : $$I_4 = \frac{8\sqrt2}{\pi^2}\sum_{n=0}^{\infty}\left(\frac43\right)^n\frac{n!}{(2n+1)!}\,\Gamma(n+3/2)\,{}_2F_1(1/2,-n;3/2;1/4).$$ By the way $$I_n = \sqrt2n\int_0^1t^{n-1}\operatorname{erf}^{-1}(t)\,dt \,=\!\!\!?\; \sqrt2n\sum_{k=0}^\infty a_k \left(\frac{\sqrt\pi}{2}\right)^{2k+1}\frac1{2k+n+1},$$ where $a_k$ is the $k$ -th coefficient of the Maclaurin series of $\operatorname{erf}^{-1}(2x/\sqrt\pi)$ (see InverseErf ). Well, I don't really know the behavior of $(a_k)$ , but numerically the series does converge. I don't think this will lead to anything though. Let me explain a little about this problem. Imagine we have $n$ points to throw at 0 at the real axis, and the resulted position of one point is determined by $\mathcal N(0,1)$ . We want to study the behavior of the farthest distance from 0. This distance $D = \|G\|_\infty$ is determined by the density function defined below $$f:x \mapsto n\sqrt{\frac2\pi}\,\exp\left(-\frac{x^2}2\right) \operatorname{erf}^{n-1}\frac{x}{\sqrt2} \mathbb1_{x\ge0}.$$ (For fun one can check that $\int_0^\infty f(x)\,dx=1$ .) And now, what we want to know is, how to calculate $\mathbb E(D)$ (at least when $n=4$ )? @YuriNegometyanov has given a formula for $\mathbb E(\|G\|_2)$ . Even though it's not quite the topic, let's write it down as well: $$\mathbb E(\|G\|_2) =\sqrt2\,\frac{\Gamma\left(\dfrac{n+1}2\right)}{\Gamma\left(\dfrac n2\right)}.$$ A jupyter notebook to calculate numerical results. So from the series expansion of $I_4$ mentioned above (and tons of calculation), I got: $$I_4 = \frac{24}{\pi\sqrt\pi}\arctan\frac{1}{2\sqrt2}.$$ This is kind of interesting since the form is similar to $I_3$ . Maybe a general closed-form is in fact possible?","['integration', 'probability-distributions', 'probability', 'real-analysis']"
3871453,About the proof of Euler’s Pentagonal Number Theorem on Wiki,"Euler’s Pentagonal Number Theorem on Wikipedia For convenience, here below is the statement: Let $n$ be a nonnegative integer, let $q_e(n)$ be the number of partitions of $n$ into even number of distinct parts and $q_o(n)$ - number of partitions of $n$ into odd number of distinct parts. Then $q_e(n) - q_o(n) = \cases{(-1)^k \ \text{ if } n = \frac{k(3k \pm 1)}{2} \\ 0  \text{  otherwise}}$ . I understand the proof all the way up to the point where it says ""...in which case there is exactly one Ferrers diagram left over "". According to the proof, integer $12$ has precisely one single non-invertible partition which is $(5, 4, 3)$ and it contributes $(-1)^3$ to the coefficient of $x^{12}.$ Since $12$ is a relatively small integer, the claim could be checked manually (which I admit I hadn't done). But how do we know the claim in bold holds for horrendously large numbers or in general? Exactly what part of the proof shows the claim in bold? Thanks.","['integer-partitions', 'number-theory', 'combinatorics', 'discrete-mathematics']"
3871479,"If $\sum_{n=0}^\infty a_n f(x,n) = \sum_{n=0}^\infty b_n f(x,n)$, what conditions on $f$ allow us to conclude that $a_n=b_n$ for all $n$?","If we have an equality of the form: $$\sum_{n=0}^\infty a_n x^n = \sum_{n=0}^\infty b_n x^n $$ We can conclude that $a_n=b_n$ for all $n$ . This can be easily seen by say, differentiating term-wise and showing that $a_n=\frac{f^{(n)}(0)}{n!}$ and $b_n=\frac{g^{(n)}(0)}{n!}$ for LHS $f$ and RHS $g$ . However, it's clear that in general we do not have for any $f(x,n)$ that \begin{equation}
\sum_{n=0}^\infty a_n f(x,n) = \sum_{n=0}^\infty b_n f(x,n)
\implies \forall n \ a_n=b_n
\end{equation} My question is, what is the most general restrictions we can put on $f$ in order for the implication above to be true? I can go through some special cases, i.e. polynomials, but I don't believe I have the background to work with general $f$ . I'm in my first undergrad analysis class, so apologies if this is a well-known result.","['analysis', 'real-analysis', 'calculus', 'sequences-and-series', 'power-series']"
3871698,Least Singular Value of bidiagonal matrix,"Consider the $n \times n$ , bidiagonal matrix $$
\left(\begin{array}{ccccc}
x & \\
1 & x \\
& 1& x \\
& & & \ddots \\
& & & 1 & x
\end{array} \right)
$$ It is claimed that the least singular value for this matrix is less than $O(2^{-n})$ when $x$ is fixed and $|x-1| > 1/2$ .  Is there a simple way to see this?  I have thought about it as a diagonal shift of a nilpotent matrix, but this does not seem to help.",['linear-algebra']
3871728,"Prove if a function is holomorphic, then its complex conjugate is holomorphic, by Cauchy-Riemann Equations","Given a function $f(z)$ is holomorphic on a disc $D(0,R)$ , we want to prove that $g(z)= \overline {f(\bar z)}$ is holomorphic. Below is my proof. $\lim_{h\to 0} \frac{\overline {f(\overline{z+h})}-\overline{f(\bar z)}}{h} =  \lim_{h\to 0} \frac{\overline {f(\overline{z+h})-\overline{f(\bar z)}}}{h} = \overline{\lim_{h\to 0} \frac {f(\overline{z+h})-\overline{f(\bar z)}}{\bar h}} = \overline{f'(\bar z)}$ Since $ f'(\bar z)$ exists, $g(z)= \overline {f(\bar z)}$ is holomorphic. This question is actually an old one, but it is not duplicated, because I want to ask how to use Cauchy Riemann Equations to solve it. I mean, if I write $f(z)=u(x,y)+iv(x,y)$ and $g(z)=u(x,-y)-iv(x,-y)$ , and try to show $g(z)$ is holomorphic by CREs, what should I do? The relationship between partial derivatives of $u(x,y)$ and $u(x,-y)$ is not just addition inverse. So what should I do next? Also, I will be very grateful if you can help me to check my proof is correct. So, actually my question is, how do I write $\frac {\partial} {\partial x } u(x,-y),\frac {\partial} {\partial y } u(x,-y)$ ?","['complex-analysis', 'cauchy-riemann-equations']"
3871800,solving for $x$:$\cos^2 3x+\frac{\cos^2 x}{4}=\cos 3x\cos^4 x$,solve for x: $$\cos^2 3x+\frac{\cos^2 x}{4}=\cos 3x\cos^4 x$$ My attempt : completing square: $${(\cos 3x-\frac{\cos x}{2})}^2=\cos 3x\cos x (1-\cos^3 x)$$ or $$\cos x\cos 3x\ge 0$$ also  by some basic identities : $$1+\cos 6x+\frac{1+\cos 2x}{4}=(\cos 4x+\cos 2x)\left(\frac{\cos 3x+3\cos x}{4}\right)$$ I cant proceed now.....,['trigonometry']
3871809,Proof of Cantor Intersection Theorem by Bolzano-Weierstrass Theorem,"I was reading Mathematical Analysis by Tom M. Apostol . There Cantor Intersection Theorem was proven using Bolzano-Weierstrass Theorem in this way Theorem : Let $\left\{Q_{1}, Q_{2}, \ldots\right\}$ be a countable collection of nonempty sets in $\mathbb{R}^n$ such that: i) $Q_{k+1} \subseteq Q_{k} \quad(k=1,2,3, \ldots)$ ii) Each set $Q_{k}$ is closed and $Q_{1}$ is bounded. Then the intersection $\bigcap\limits_{k=1}^{\infty} Q_{k}$ is closed and nonempty. Proof : Let $S=\bigcap\limits_{k=1}^{\infty} Q_{k} .$ Then $S$ is closed [As intersection of an arbitrary collection of close sets is closed]. To show that $S$ is nonempty, we exhibit a point $x$ in $S .$ We can assume that each $Q_{k}$ contains infinitely many points; otherwise the proof is trivial. Now form a collection of distinct points $A=\left\{{x}_{1}, {x}_{2}, \ldots \right\},$ where ${x}_{k} \in Q_{k}, $ since $A$ is an infinite set contained in the bounded set $Q_{1},$ it has an accumulation point, say $x$ . We shall show that $x \in S$ by verifying that $x \in Q_{k}$ for each $k .$ It will suffice to show that $x$ is an accumulation point of each $Q_{k},$ since they are all closed sets. But every neighborhood of ${x}$ contains infinitely many points of $A,$ and since all except (possibly) a finite number of the points of $A$ belong to $Q_{k},$ this neighborhood also contains infinitely many points of $Q_{k}$ . Therefore $x$ is an accumulation point of $Q_{k}$ and the theorem is proved. Honestly I didn't understand the proof from the line It will suffice to show that $x$ is an accumulation point of each $Q_{k},$ since they are all closed sets. Can you help me to understand it.","['elementary-set-theory', 'general-topology', 'real-analysis']"
3871838,Algebraic Geometry as Generalized Calculus?,"I've heard people say that (part of) algebraic geometry deals with generalizing results from differential geometry to fields other than $\mathbb{C}$ . Some searching has brought me to ""GAGA Theorems"", but I don't have anywhere near the background that it looks like I might need to understand those. It seems like this program was started in order to work on the Weil Conjectures, since the computational evidence seemed to point to ""continuous"" theorems (such as the Lefschetz fixed point theorem) being relevant in a ""discrete"" setting. Again, though, I don't have the background to understand even the wikipedia page . I have taken a class in ""classical"" algebraic geometry (using Shaferevich, Vol 1), but it is still unclear to me which aspects of differential geometry were able to be transported, and how.
The only analogies that I can see for myself are the definition of an abstract variety (which we barely touched on at the end of my class) being clearly inspired by the definition of an abstract manifold the notion of ""singular"" and ""regular"" points on a curve, which are also clearly inspired by their analogues in differential geometry. I would appreciate it if someone can provide some insight into this program at a level I can understand. I am perfectly happy to see some hand-waving if that helps, but I would also like to see (relatively) concrete examples of ideas in differential geometry being moved to the world of arbitrary fields. Thanks!","['algebraic-geometry', 'soft-question', 'differential-geometry']"
3871876,On tangents of parabolas,"Question Let $a > 0$ and $b > 0$ be constants. Suppose that the parabolas $$C_1 : y^2 = 4a(a - x)\ \mathrm {and}\ C_2 : y^2 = 4b(b + x)$$ intersect at a point $P$ . Prove that the tangent line to $C_1$ at $P$ and the tangent line to $C_2$ at $P$ are perpendicular. My Working From $C_1$ , $$2y\frac {dy} {dx} = -4a$$ $$\implies \frac {dy} {dx} = -\frac {2a} {y}$$ From $C_2$ , $$2y\frac {dy} {dx} = 4b$$ $$\implies \frac {dy} {dx} = \frac {2b} {y}$$ Then, from elementary geometry, I know that for two lines to be perpendicular to each other, the product of their gradients must be $-1$ . Thus, working backwards, $$(-\frac {2a} {y})(\frac {2b} {y}) = -1$$ $$\implies y^2 = 4ab$$ so the problem reduces to showing that $$y^2 = 4ab$$ and we are done. However, this is where I am stuck. How do I show the above relation given only the equations of the two curves? Any suggestions to how I may proceed or even alternatives to solving this problem will be nice :)","['calculus', 'derivatives', 'real-analysis']"
3871950,Fastest way to obtain limit of a function,"I'm tasked to find the limit of such a function: $\lim_{x\to 0} \frac{\sqrt{1 + \tan(x)} - \sqrt{1 + \sin(x)}}{x^3}$ My immediate instinct is to use L'Hospital's Rule to differentiate the numerator and denominator, and rinse and repeat until the denominator no longer contained $x$ , which I did to arrive at an answer of $0$ , but boy was the process extremely tedious and painful. However, this process was way too painful. My other instinctive thought was to rationalise the function to obtain $\lim_{x\to 0} \frac{\tan(x) - \sin(x)}{x^3\bigl(\sqrt{1 + \tan(x)} + \sqrt{1 + \sin(x)}\bigr)}$ but this not only does not remove $x$ from the denominator, I have to apply the product rule which will no doubt complicate the process further. Can someone advise how else I can go about solving this more efficiently?","['fractions', 'limits', 'calculus']"
3872033,On the sum of absolute value of Gauss variables,"Currently I meet with the following interesting problem. Let $x_1,\cdots,x_n$ be i.i.d standard Gaussian variables. How to calculate the probability distribution of the sum of their absoulte value, i.e., how to calculate $$\mathbb{P}(|x_1|+\cdots+|x_n|\leq nt).$$ Here I use $nt$ instead of $t$ for sake of possible concise formula. I cannot find out the exact value. However, a practical lower bound is also good. Here practical means the ratio between the exact value and lower bound is independent of $n$ and $t$ , and is small. Thanks very much!","['integration', 'multivariable-calculus', 'probability-distributions', 'probability-theory']"
3872036,I'm looking for references for generalized confluent hypergeometric differential equation,"According to wolfram , A generalization of the confluent hypergeometric differential equation is given by; $$y''+\left(\frac{2R}{x}+2F'+p\frac{H'}{H}-H'-\frac{H''}{H'}\right) y'+\left[\left(p\frac{H'}{H}-H'-\frac{H''}{H'}\right)\left(\frac{R}{x}+F'\right)+\frac{R(R-1)}{x^2}+\frac{2R}{x}F'+F''+(F')^2-\frac{q}{H}(H')^2 \right]y=0$$ Which has the solutions $y_1=x^{-R} e^{-F} M(q,p,H)$ and $y_2=x^{-R} e^{-F} O(q,p,H)$ , where $M(q,p,H)$ is the confluent hypergeometric function of the first kind and $O(q,p,H)$ is the confluent hypergeometric function of the second kind. Meanwhile, $R,F$ and $H$ are fucntions of $x$ . I tried to look on google for more details about this equation but i didn't find anything, can anyone here please give me more references about this particular equation? Like how it was deriven, the relation between the parameters $p$ and $q$ ..etc.","['analysis', 'hypergeometric-function', 'ordinary-differential-equations', 'reference-request']"
3872041,Finding specific values of $n$ such that $n^2+3n+3$ is factorisable with certain constraints on the factors,"I received the following question during a Maths Olympiad: Let $n$ be a positive integer. When is it possible to express $n^2+3n+3$ into the form $ab$ with $a$ and $b$ being positive integers, and such that the difference between $a$ and $b$ is smaller than $2\sqrt{n+1}$ ? My work so far: WLOG $a<b$ . We can factor this quadratic into $(n+1)(n+2)+1$ . This means both $n+1$ and $n+2$ cannot divide $n^2+3n+3$ , and $a$ can at most be $n$ . If $a=n$ , then $b=\frac{n^2+3n+3}n=n+3+\frac3n>n+3$ and $b$ must be equal to or larger than $n+4$ . The difference between $a$ and $b$ is at least $4$ . We need $4<2\sqrt{n+1}\Rightarrow2<\sqrt{n+1}\Rightarrow4<n+1\Rightarrow n>3$ . Apart from this, $n^2+3n+3\equiv1,3\pmod 6$ . It also seems that apart from $3$ , the only primes that divide $n^2+3n+3$ are of the form $6k+1$ . However this is conjecture and based on the few test cases I checked. Quadratic residues might come in handy in this problem but I'm not sure. How could I do this problem?","['algebra-precalculus', 'factoring']"
3872069,How do I write this equation in einstein notation?,"I am a programmer who does quite a bit of maths. One of the operations in my code is the operation: C = np.einsum(""ij,kj->ki"",A,B) ( the documentation for his function ). The dimensions of the matricies (is it right to call these matricies?) are: A : m by n B : p by n C : m by p Ultimately my question is, how do I write the operation of C = np.einsum(""ij,kj->ki"",A,B) in a purely mathematical context? I have looked through a few examples, but I have had difficulty connecting these to what I am doing. (I apologise if the tags and/or the title are/is bad, please drop a comment and I'll correct it)","['matrices', 'linear-algebra']"
3872075,Where did I go wrong in applying the factor theorem?,"Given that $x + 1$ and $x - 3$ are two of the four factors of the expression $x^4 + px^3 + 5x^2 + 5x + q$ , find the values of $p$ and $q$ . I tried to answer this question using the factor theorem but got the answer wrong: $$ \text{Let } f(x) = x^4 + px^3 + 5x^2 + 5x + q $$ $$ \text{Since } x + 1 \text{ and } x - 3 \text{ are factors of } f(x), \text{ then } f(-1) = 0 \text{ and } f(3) = 0, \text{ i.e.} $$ \begin{align}
(-1)^4 + p(-1)^3 + 5(-1)^2 + 5(-1) + q &= 0 \color{red}{\leftarrow (1)} \\
(3)^4 + p(3)^3 + 5(3)^2 + 5(3) + q &= 0 \color{blue}{\leftarrow (2)}
\end{align} $$ \text{From } \color{red}{(1)}: $$ \begin{align}
(-1)^4 + p(-1)^3 + 5(-1)^2 + 5(-1) + q &= 0 \\
1 + p(-1) + 5(1) + (-5) + q &= 0 \\
1 - p + 5 - 5 + q &= 0 \\
1 - p + q &= 0 \\
q &= p - 1 \color{limegreen}{\leftarrow (3)}
\end{align} $$ \text{From } \color{blue}{(2)}: $$ \begin{align}
(3)^4 + p(3)^3 + 5(3)^2 + 5(3) + q &= 0 \\
81 + 27p + 45 + 15 + q &= 0 \\
27p + q + 60 + 81 &= 0 \\
27p + q + 141 &= 0 \\
q &= -27p - 144 \color{orange}{\leftarrow (4)}
\end{align} $$ \color{orange}{(4)} + \color{limegreen}{(3)}: $$ \begin{align}
-27p - 144 &= p - 1 \\
-27p - p &= 144 - 1 \\
-28p &= 143 \\
28p &= -143 \\
p &= -\frac{143}{28} \\
\therefore p &= -5\frac{3}{28} \color{mediumpurple}{\leftarrow (5)}
\end{align} $$ \text{Substitute } \color{mediumpurple}{(5)} \text{ into } \color{limegreen}{(3)}: $$ \begin{align}
q &= -5\frac{3}{28} - 1 \\
\therefore q &= -6\frac{3}{28}
\end{align} The answers were $ p = -5, q = -6 $ . Where did I go wrong?","['algebra-precalculus', 'solution-verification']"
3872156,"Uniformly at random, break a unit stick in two places. What is the probability that the smallest piece is $\leq 1/5$?","I was asked this in an interview and wasn't sure how to solve it: Consider a stick of length $1$ . Select two points independently and uniformly at random on the stick. Break the stick at these two points, resulting in $3$ smaller pieces. What is the probability that the smallest of these pieces is $\leq 1/5$ ? For starters, I noted that the three sections must have lengths: $$
\begin{aligned}
\ell_1 &= \max(X,Y)
\\
\ell_2 &= \max(X,Y) - \min(X,Y)
\\
\ell_3 &= 1 - \max(X,Y)
\end{aligned}
$$ Let $\ell_{\texttt{min}}$ , $\ell_{\texttt{mid}}$ , and $\ell_{\texttt{max}}$ represent the smallest, middle, and largest stick lengths. So clearly we want to compute $$
\mathbb{P}(\ell_{\texttt{min}} \leq 1/5)
$$ However, I wasn't sure how to move forward from here. I assume I need to frame the problem such that I can perform integration by computing an area on a unit square.",['probability']
3872243,Regular differentials on a singular curve.,"Please also refer to the same question over on mathoverflow.net , where I proposed a solution which is still incomplete (and not entirely correct...). Let $X'$ be an irreducible singular algebraic curve over an algebraically closed field $k$ , and let $X \to X'$ be its normalization, and consider a (singular) point $Q \in X'$ . Let $K = Q(X)$ be the function field of $X$ and $X'$ . Let $\mathcal{O}_Q' = \mathcal{O}_{X', Q}$ be the stalk of the structure sheaf of $X'$ at $Q$ , and let $\mathcal{O}_Q = \bigcap_{P \mapsto Q} \mathcal{O}_P$ be its normalization. Here $\mathcal{O}_P$ is the stalk of the structure sheaf of $X$ at $P \in X$ , and the intersection is over all points mapping to $Q$ . In his book Algebraic Groups and Class Fields , chapter IV §3, Serre introduces the module $\underline{\Omega}_Q'$ of regular differentials at $Q$ . A differential $\omega \in D_k(K)$ is called regular, iff \begin{equation}\sum_{P \mapsto Q} \operatorname{Res}_P(f \omega) = 0 \quad \text{for all} \  f\in \mathcal{O}_Q'.\end{equation} Similarly to $\mathcal{O}_Q$ , Serre defines $$ \underline{\Omega}_Q = \bigcap_{P \mapsto Q} \Omega_P.$$ Since every differential $\omega \in \underline{\Omega}_Q$ has no poles at any point $P \mapsto Q$ , clearly $\operatorname{Res}_P(f \omega) = 0$ for $f \in \mathcal{O}_Q'$ , so that $\underline{\Omega}_Q \subset \underline{\Omega}_Q'$ . Now to my question: The mapping \begin{align}
\mathcal{O}_Q / \mathcal{O}_Q' \times \underline{\Omega}_Q' / \underline{\Omega}_Q & \to k \\
(f, \omega) & \mapsto \sum_{P \mapsto Q} \operatorname{Res}_P(f \omega)
\end{align} is clearly bilinear and well-defined. Serre claims that it is a perfect pairing, but I don't know why. I think we have to show two things: If $f \in \mathcal{O}_Q$ , with the property that for each $\omega \in \underline{\Omega}_Q'$ , one has $\sum_P \operatorname{Res}_P(f \omega) = 0$ , then in fact $f \in \mathcal{O}_Q'$ . If $\omega \in \underline{\Omega}_Q'$ , such that for each $f \in \mathcal{O}_Q$ , one has $\sum \operatorname{Res}_P(f \omega) = 0$ , then $\omega \in \underline{\Omega}_Q$ , i.e. $\omega$ is regular at every $P \mapsto Q$ . Any help would be appreciated :)","['algebraic-curves', 'algebraic-geometry', 'singularity-theory']"
3872260,Solution to a differential equation in a specific format,"I am having trouble getting the solution that this problem is asking for. The problem is:
""Show that the general solution to the differential equation $$x \frac{dy}{dx} = y \ln(x)$$ is $$ y = Cx^{\ln(\sqrt{x})}$$ I used separation and got: $$ \int \frac 1y dy = \int \frac{\ln(x)}{x} dx $$ Integrating both sides I got: $$ \ln (y) = \frac{({\ln(x)})^2}{2} +c $$ Then I raised both sides as a power of e and considered e to the power of c as my constant C and got: $$ y = Ce^{\frac{({\ln(x)})^2}{2}}$$ Then I considered the portion containing e as: $$ (e^{({\ln(x)})^2})^\frac 12 $$ This finally gave me : $$ y= C\sqrt {(x^{\ln(x)})} $$ I do not know how the form in the problem was reached.","['calculus', 'algebra-precalculus', 'ordinary-differential-equations']"
3872314,Meaning of directional derivative of a vector field,"Suppose I have a vector field $ \vec{B} (x,y,z)$ then do $ \frac{ \partial B}{ \partial n}$ where n is the direction vector of a line denote the directional derivative of the vector in the direction of $n$ ? The reason I ask is that I recently encountered this in a physics textbook but all the gradients and directional derivatives that I've seen till now were defined for scalar fields. Edit: The real quantity that I started with was the one from this mse post : $$ (\nabla B_i) n_i $$ I thought this would be the directional derivative since it looked like one but then I later realized this is actually a vector field. A picture from the book: Page-158, I.e irodov basic laws of electromagnetism",['multivariable-calculus']
3872348,N needed for probability > .001 of two people being a match at six genetic markers?,"If for each marker there is a 1/9 chance that any two people are a match, how large would the sample need to be for the probability to exceed .001 that two people are a match at six markers?","['statistics', 'probability']"
3872443,Weak markov property question,"I have a doubt concerning the definition of Weak Markov Property: Weak Markov Property: For every initial distribution $\mu$ and every $(X_1, ... , X_n)$ we have $E_\mu (h(X_{n+1},X_{n+2},...) | x_0, x_1, ..., x_n ) = E_{x_n} (h(X_1,..., X_n) )$ provided the expectation exists. My textbook says that, when $h()$ is the indicator function, this property boils down to the very Markov chains definition property, i.e. $P((X_{n+1} | x_0, ... , x_n )=P(X_{n+1} | x_n )$ I see that if $h() = 1_A () $ then
the weak markov property gets $P_\mu ((X_{n+1},X_{n+2},...) \in A | x_0, x_1, ..., x_n ) = P_{x_n} ((X_1,..., X_n) \in A)$ , but I am not sure how to go from here to $P((X_{n+1} | x_0, ... , x_n )=P(X_{n+1} | x_n )$ . Can someone help? Thanks!","['probability-theory', 'markov-chains']"
3872447,solve ;$\sqrt{\frac{1-4\cos^2 4x}{8\cos (2x-2\pi/3)}}=\cos (2x-\pi/6)$,This is one more of my unsolved trigonometry  questions: solve ; $$\sqrt{\frac{1-4\cos^2 4x}{8\cos (2x-2\pi/3)}}=\cos (2x-\pi/6)$$ My Try provided that $\cos (2x-2\pi/3)\neq 0$ and squaring both sides $$1-4\cos^2 4x=8\cos (2x-2\pi/3)\cos^2(2x-\pi/6)....(1)$$ $$1-4\cos^2 4x=4\cos(2x-2\pi/3)(1+\cos(4x-\pi/3))...(2)$$ For convenience we take $2x=t$ : $$1-4\cos^2 2t=4\cos(t-2\pi/3)(1+\cos(2t-\pi/3))....(3)$$ $$=4\cos(t-2\pi/3)-2\cos(3t)+2\cos (t+\pi/3)....(4)$$ What do i do next? Source :A Panchishkin- Trigonometric functions,['trigonometry']
3872515,Reference Request: Witt Vectors,"I'm interested in a nice (and possibly gentle) introduction to Witt vectors . It doesn't need to go into too much detail, for now, I'd just like to roughly know what they are about. So essentially my wish is something along the following lines: a definition/construction along with some motivation and possibly some suggestive examples/applications. (Though I would fancy references that do not satisfy all of this points.) The Wikipedia article seems decent but I'm interested to know where members from MSE first learned about Witt vectors and what you would recommend. (I was surprised by the way that there was no reference request about Witt vectors on MSE or MO. Usually, those forums would be the first place for me to check when I am looking for good book suggestions.)","['number-theory', 'p-adic-number-theory', 'algebraic-number-theory', 'reference-request']"
3872577,"Is the relation $R:=\{(1,2),(1,3)\}$ transitive on $M=\{1,2,3\}$ with $R\subseteq M\times M$?","Is the relation $R:=\{(1,2),(1,3)\}$ transitive on $M=\{1,2,3\}$ with $R\subseteq M\times M$ ? I think it's transitive, because we don't have elements that satisfy $xRy \land yRz $ and therefore $\forall x,y,z \in M: xRy \land yRz \implies xRz$ is always true. Is this right?","['elementary-set-theory', 'order-theory', 'relations', 'discrete-mathematics']"
