question_id,title,body,tags
3742422,"Given the equation $\alpha \mathbf{v} + \mathbf{v}\times\mathbf{a} = \mathbf{b}$, solve for $\mathbf{v}$.","I'm reading a textbook at the moment that provides the following linear equation, $$
\alpha \mathbf{v} + \mathbf{v}\times\mathbf{a} = \mathbf{b},
$$ and asks to solve for $\mathbf{v}$ . The form of $\mathbf{v}$ is given as $$
\mathbf{v} = \frac{\alpha^2 \mathbf{b} - \alpha (\mathbf{b} \times \mathbf{a}) + (\mathbf{a}\cdot\mathbf{b})\mathbf{a}}{\alpha(\alpha^2+\lvert \mathbf{a} \rvert^2)}.
$$ It's easy enough to verify that this is the correct solution. However, I can't figure out how I'd solve for $\mathbf{v}$ if given just the original equation. Are there any general approaches to solving this kind of equation systematically? Edit: $\mathbf{a}, \mathbf{b}$ and $\mathbf{v}$ are all vectors, whereas $\alpha$ is a scalar such that $\alpha \neq 0$ .","['linear-algebra', 'vectors']"
3742490,Is the cap product the same on cochain complexes with isomorphic cohomology rings?,"Let $X$ be a topological space. Let $\mathcal{D}$ be a cochain algebra, with cohomology ring $H^* (\mathcal{D})$ . Suppose that we have that the cohomology ring $H^* (X)$ (endowed with the cup product) is isomorphic, as a ring, to $H^* (\mathcal{D})$ , equipped with its respective operation (and the homology groups of $X$ are isomorphic to the homology groups of $\mathcal{D}$ ). There is an action of $H^* (X)$ on $H_* (X)$ given by the cap product: $\frown:H_* (X) \otimes H^* (X) \rightarrow H_* (X)$ . There is also an action of $H^* (\mathcal{D})$ on $H_* (\mathcal{D})$ given by dualizing the algebra structure of $\mathcal{D}$ (described below). I am interested in whether this ""cap product"" on $\mathcal{D}$ induces the same action of $H^* (\mathcal{D})$ on $H_* (\mathcal{D})$ as the cap product does for the cohomology and homology of $X$ . Denote by $*$ the multiplication in $\mathcal{D}$ . Let $\widehat{\mathcal{D}}$ denote the chain complex obtained by dualizing the cochain complex $\mathcal{D}$ . We obtain an operation $$\frown:\mathcal{D} \times \widehat{\mathcal{D}} \rightarrow \widehat{\mathcal{D}}$$ by defining $\langle u * v, w \rangle = \langle u, v \frown w \rangle$ , the dual of $*$ . This is the exact way in which, if we were given the cup product on the level of cochains of $X$ , we would dualize to get the cap product on the cochains/chains of $X$ . There is an induced operation on homology: $$\frown:H_* (\mathcal{D}) \otimes H^* (\mathcal{D}) \rightarrow H_* (\mathcal{D}).$$ Is this operation the same (isomorphic to) the cap product on the isomorphic cohomology and homology groups of $X$ ? I think that the answer is yes, but I cannot find a proof. EDIT: Additionally, I would at least like to know if I am correct in thinking that this is always true in the case of zero torsion.","['ring-theory', 'abstract-algebra', 'homology-cohomology', 'algebraic-topology']"
3742505,Strong law of large numbers when the expectation is infinite,"Let $\{X_n\}$ be a sequence of i.i.d. positive random variables and $S_n = X_1 + \cdots + X_n$ .
If $\mathbb{E}(X_1)=\infty$ , then we have that $$ \limsup_{n \to \infty} \frac{S_n}{n} = \infty ~~\text{a.e.}~. $$ Can we prove the stronger result that $$ \lim_{n \to \infty} \frac{S_n}{n} = \infty ~~\text{a.e.}~ ?$$","['law-of-large-numbers', 'probability-theory']"
3742523,"A specific change of variable, similar to spherical coordinates","Is it possible to get an explicitly formula for the following change of variable (formula for the Jacobian or for the inverse. I would even accept results from mathematica or other software, which I never use and I'm not proficient in). Let me first introduce the following function $\omega : \left\lbrace \begin{aligned} \mathbb{R}^3 & \longrightarrow \ \mathbb{R}\\ \mathbf{k}\; & \longmapsto  \sqrt{\mathbf{k}^2 + m^2} \end{aligned} \right. $ . I'm now considering $$ \int_{\mathbb{R}^{3\times 3}} \frac{ \varphi\big(\omega(\mathbf{k}_1) + \omega(\mathbf{k}_2) +\omega(\mathbf{k}_3), - \mathbf{k}_1 - \mathbf{k}_2 - \mathbf{k}_3 \big)}{ \omega(\mathbf{k}_1)\hspace{1pt} \omega(\mathbf{k}_2) \hspace{1pt} \omega(\mathbf{k}_3)\, \big[ \big(\omega(\mathbf{k}_1) + \omega(\mathbf{k}_2) + \omega(\mathbf{k}_3) \big)^2 - \omega(\mathbf{k}_1+ \mathbf{k}_2 + \mathbf{k}_3)^2 \big]^2}\, d\mathbf{k}_1\,d\mathbf{k}_2\, d\mathbf{k}_3 $$ where $\varphi$ is a function of 4 variables. It seems natural to consider $$ \Phi: \left\lbrace \begin{aligned}
	U \subset \mathbb{R}^9 
		& \longrightarrow V \subset \mathbb{R}^3 \times [m,+\infty[^3 \times ]0,\pi[^3 \\
	\begin{pmatrix}
		\mathbf{k}_1 \\ \mathbf{k}_2 \\ \mathbf{k}_3
	\end{pmatrix}\ 
		& \longmapsto 
	\begin{pmatrix}
		\mathbf{k} \\ \omega_{\mathbf{k}_1} \\ \omega_{\mathbf{k}_2} \\ \omega_{\mathbf{k}_3}\\ \theta_{12} \\ \theta_{23} \\ \theta_{31}
	\end{pmatrix} 
	:= \begin{pmatrix}
		\mathbf{k}_1 + \mathbf{k}_2 + \mathbf{k}_3\\
		\omega(\mathbf{k}_1) \\ \omega(\mathbf{k}_2) \\ \omega(\mathbf{k}_3)\\
		\arccos \big( \mathbf{k}_1 \cdot \mathbf{k}_2 \big/ \left\lVert\mathbf{k}_1 \right\rVert \left\lVert\mathbf{k}_2\right\rVert \big)\\
		\arccos\big( \mathbf{k}_2 \cdot \mathbf{k}_3 \big/ \left\lVert\mathbf{k}_2\right\rVert\left\lVert\mathbf{k}_3\right\rVert \big)\\
		\arccos\big( \mathbf{k}_3 \cdot \mathbf{k}_1 \big/ \left\lVert\mathbf{k}_3\right\rVert \left\lVert\mathbf{k}_1\right\rVert \big)
	\end{pmatrix}
	\end{aligned} \right. $$ where $U:= \left\lbrace (\mathbf{k}_1 , \mathbf{k}_2 , \mathbf{k}_3)\in \mathbb{R}^9,\ \operatorname{det}(\mathbf{k}_1 , \mathbf{k}_2 , \mathbf{k}_3) >0 \right\rbrace$ is just half the space and the image $V$ a little complicated. I'm not entirely sure this a $\mathcal{C}^1$ -diffeomorphism. The Jacobian looks like $$ \begin{vmatrix}
	1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
	0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
	0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 \\
	k_1^1 / \omega_{\mathbf{k}_1} & k_1^2 / \omega_{\mathbf{k}_1} & k_1^3 / \omega_{\mathbf{k}_1} & 0 & 0 & 0 & 0 & 0 & 0\\	
	0 & 0 & 0 & k_2^1 / \omega_{\mathbf{k}_2} & k_2^2 / \omega_{\mathbf{k}_2} & k_2^3 / \omega_{\mathbf{k}_2} & 0 & 0 & 0\\
	0 & 0 & 0 & 0 & 0 & 0 & k_3^1 / \omega_{\mathbf{k}_3} & k_3^2 / \omega_{\mathbf{k}_3} & k_3^3 / \omega_{\mathbf{k}_3} \\
	\frac{\cos \theta_{12}\hspace{.8pt} \frac{k_1^1}{\lVert\mathbf{k}_1\rVert}- \frac{k_2^1}{\lVert\mathbf{k}_2\rVert}}{\lVert\mathbf{k}_1\rVert\, \sin \theta_{12}} & ''
	%\frac{\cos \theta_{12}\, \frac{k_1^2}{\norm{\mathbf{k}_1}}- \frac{k_2^2}{\norm{\mathbf{k}_2}}}{\norm{\mathbf{k}_1} \sin \theta_{12}}  
	& ''
	% \frac{\cos \theta_{12}\, \frac{k_1^3}{\norm{\mathbf{k}_1}}- \frac{k_2^3}{\norm{\mathbf{k}_2}}}{\norm{\mathbf{k}_1} \sin \theta_{12}} 
	& 0 & 0 & 0 & 0 & 0 & 0\\
	0 & 0 & 0 &	\frac{\cos \theta_{23}\hspace{.8pt} \frac{k_2^1}{\lVert\mathbf{k}_2\rVert}- \frac{k_3^1}{\lVert\mathbf{k}_3\rVert}}{\lVert\mathbf{k}_2\rVert\, \sin \theta_{23}} & '' & '' & 0 & 0 & 0\\
	0 & 0 & 0 & 0 & 0 & 0 & \frac{\cos \theta_{31}\hspace{.8pt} \frac{k_3^1}{\lVert\mathbf{k}_3\rVert}- \frac{k_1^1}{\lVert\mathbf{k}_1\rVert}}{\lVert\mathbf{k}_3\rVert\, \sin \theta_{31}} & '' & ''
	\end{vmatrix}$$ where $''$ stands for similar and not identical and where I used the convention from physics that $k^i_j$ is the $i^{\text{th}}$ -component of $\mathbf{k}_j$ and not something to the power $i$ .","['multivariable-calculus', 'calculus', 'change-of-variable', 'determinant']"
3742538,When is the projection of an algebraic variety another algebraic variety?,"Let $P_1,\dots,P_k \in \mathbb{C}[x_1,\dots,x_n,y_1,\dots,y_m]$ be a sequence of polynomials
on $n+m$ variables, and let $Z\subset\mathbb{C}^{n+m}$ be the intersection of the zero sets (so $Z = \cap_{j=1}^k \{(\vec{x},\vec{y}) | P_j(\vec{x},\vec{y}) = 0\}$ ). Let $\Pi:\mathbb{C}^{n+m}\to\mathbb{C}^n$ be the projection onto the first $n$ coordinates.  When is it the case that $\Pi(Z)$ is also defined by a system of polynomials?  I know that, for example, if $k=n=m=1$ and $P(x,y) = xy-1$ , the projection onto the $x$ -axis is everything except for the origin, so this does not always happen. My understanding from the little I've been able to read and understand is falls under elimination theory, but my algebraic geometry is far from good enough to be able to understand the statements of the main results.","['algebraic-geometry', 'polynomials']"
3742603,"Find every equation of the line that passes through the point $(5,13)$","""Find every equation of the line that passes through the point $(5,13)$ and  passes both axis at non-negative, whole values."" Here's my attempt: Finding first two equations, with $k=\pm1$ is fairly simple. After that, plugging in the $x=5$ and $y=13$ in the equation yields $b=13-5k$ . Since the line passes the $y$ axis at $(0,b)$ , $b$ has to be whole. That means $13-5k$ has to be whole, $\implies 5k \in Z$ . Only non-negative value of $k$ that passes through the $x$ axis at non-negative value is $k=1$ , so for every other line $k<0$ . For lines with $k<0$ , $b>13$ and value of $x >=6$ . $$kx+b=0$$ $$x=\frac{-b}{k}$$ $$\frac{5k-13}{k} \geq 6 $$ $$ k \leq -13 \implies b\leq 78$$ I'm not sure how to proceed from here on, I could just check for each value of $b \in (13,78]$ but that doesn't seem very efficient. What am I missing? Is my way of doing this correct? Or is there a better way? And if my attempt is correct, how do I proceed?","['algebra-precalculus', 'functions', 'graphing-functions']"
3742674,Number of arrangements in a rectangular table (II),"18 diplomats sit on a rectangular table. Three are from China, four are from Japan, six are from the United States and five are from France. In how many ways can we seat the diplomats at the table so that both the Chinese and the Japanese stay together, but separated from each other? I thought I had this sorted out, but no.
My proposed solution was the following: First I allocated the 11 diplomats (US + France) that can sit without restrictions: $$ 11! $$ Then I count the number of places between these diplomats in which the Chinese or the Japanese groups can sit: $$ 11 $$ , since that this is a closed arrangement, the 'last' diplomat is next to the 'first' one.
Then I started by allocating the Chinese group. The Chinese group can stay in 11 places between the $$ 11 $$ diplomats and they can be arranged in $$ 3! $$ ways within themselves: $$ 11 * 3! $$ After the Chinese are seated, we have $$ 10 $$ places left between the diplomats where the Japanese can sit; the Japanese diplomats can be arranged in $$ 4! $$ ways amongst themselves: $$ 10 * 4! $$ . Lastly, we have to consider the symmetry of the rectangle, meaning that we have been counting these arrangements twice, as the sides of the rectangle are equal two-by-two. So in my mind we should have in total $$ \frac{11! * 11* 3! * 10 * 4!}{2} = 316 141 056 000 $$ .
However, the number of ways is supposed to be $$ 379 369 267 200 $$ . Can you please help me find what is wrong in my thinking? Thank you.",['combinatorics']
3742728,Is the derivative of a periodic function always periodic?,"True or False :
The derivative of a periodic functions is always periodic. I thought it to be true , as everything about a periodic function repeats itself at regular intervals, and so should it's derivative . But , to my surprise it is given false , which suggests that it might be true most of the time but not always , I have given all my thoughts to finding a counter example but I just can't seem to find even one counter example . One possibility was $\{x\}$ , which is not differentiable at every integer , and I am confused about whether I should call it periodic or not , because it's graph will be a straight line with holes at every integer , so in a sense it is periodically not defined , just like $\tan x$ wich is not defined at every odd multiple of $\pi\over 2$ but still it is said to be periodic . Could someone please help me find a counter example and clarify about periodicity of derivative of $\{x\}$ . Thanks ! $\{x\}$ is fractional part of x .","['periodic-functions', 'derivatives']"
3742777,Minimum cost of a connected graph,"$G$ is a connected graph with cost $p:E(G)\to\mathbb{R}$ defined on its edges. Let $e' \in E(G)$ be such that $p(e')<p(e)$ for every $e\in E(G)-\{e'\} $ .
Is it possible to find two spanning trees of minimium weight, $T_1$ , $T_2$ ( $T_1 \neq T_2$ ), such that one of them has $e'$ and the other one doesn't?","['graph-theory', 'trees', 'discrete-mathematics', 'discrete-optimization']"
3742825,"Why is the Penrose triangle ""impossible""?","I remember seeing this shape as a kid in school and at that time it was pretty obvious to me that it was ""impossible"". Now I looked at it again and I can't see why it is impossible anymore.. Why can't an object like the one represented in the following picture be a subset of $\mathbb{R}^3$ ?",['geometry']
3742842,"In a row of $40$ kids, $22$ are sitting next to girls and $30$ are sitting next to boys. How many girls are there?","There are $40$ kids sitting in a row.  Number of kids sitting next to girls is 22, Number of kids sitting next to boys is 30. How many girls are sitting in a row? This is a problem from my brother's 6th grade homework. I've tried to solve it by considering easier cases and going from there, but couldn't really see the general pattern. Is there an easy solution that a 6th grader can understand?","['puzzle', 'combinatorics', 'problem-solving']"
3742848,"What does it mean that the ""$(X_1,\dots, X_n)$ are drawn from a product distribution""?","In the book High-Dimensional Statistics: A Non-Asymptotic Viewpoint , Wainwright writes: My question: What exactly is the measure $\mathsf P$ (he uses $\mathbb P$ ) here and what are the random variables $(X_1,\dots, X_n)$ ? As far as I can understand, the $(X_1,\dots, X_n)$ are a measurable function from some ""un-important"" event probability space $(\Omega, \mathcal A, \mathsf Q)$ to $\mathcal X^n$ and $\mathsf P$ is given as pushforward measure in the following way: $$\mathsf Q= (X_1,\dots,X_n)_* \mathsf P.$$ But then $\mathsf P$ is a measure on $\mathcal X^n$ , no? So how could something like $\mathsf P(Z\ge\mathsf E(Z)+\delta)$ be well-defined?","['measure-theory', 'probability-theory', 'statistics']"
3742872,Square root of 1 modulo N,"Given a positive integer N,  how do we compute $card(A)$ where $A = \{x\in\mathbb{Z}, 0 < x < N \mid x^{2}\equiv1\pmod N\}$ , when the prime factorization of N is known. In other words, how many square roots of 1 modulo N exist? We know that when N is prime, there are only two square roots -> 1 and -1 (except for N = 2, where 1 and -1 coincide). So what what the equation for generic N looks like?
A formal proof would be appreciated. I don't need to find these roots (this task can be accomplished by using EEA on every pair of factors of N), I need only to compute their amount.","['number-theory', 'square-numbers', 'modular-arithmetic', 'prime-numbers']"
3742885,Understanding a plot of a complex plane,"I am sorry if this sounds a bit convoluted but here goes. I have written a program that traces a symmetric approximation of a square, my function does not use sine, cosine,or any trigonometric functions, angles or pi... At least not explicitly. It takes two arguments - i,j which are indexes of the center point of the circle, and a variable r denoting the radius. What it does is use complex vector spaces to enable parallelization of the process of tracing the curve directly into the relevant cells that indicate the curve around the i,j center point. The program works very well, tracing a perfect circle(the circle is not centered properly because my matrix had an even number of rows and columns- but the circle itself is perfectly symmetric): But there was something that made me curious and I failed to figure it out, I inserted into the program a part which saves the real distance of every cell on the circumference from the radius(I am approximating a circle with squares here), just out of curiousity to see how the plot looks. when I plotted it, here is what I got ( this is a 1D plot): My questions: Why are there various elliptic curves inside this 1D plot of real valued distances? I calculated the mean of the distances from each point on the curve to the radius, It seemed oddly close to 0.676211.... which is very close to e/4. When I tried plotting with a larger radius, it never got over the value of e/4, and it seemed to be converging on it. why? The point with the maximum distance between it and the radius, was 1.55... which is converging on pi/2 but from above - meaning the value is usually above pi/2, but again - as r grows it also seems to converge on it - although not asymptotically.I guess that makes sense somehow because the radius marks the circumference, but still. why pi/2? Not a question but just a note, the program terminates after exactly 8 r points have been traced. the area of the circle seems to follow the following polynomial equation 2 (r - 1)^2 +2(r - 1) + 1. Just to finish - plots of distances from the radius when the length of the circle radius = 459, and length of the circle radius = 4799 (just random values) if anyone knows any method of understanding what the hell is going here I will be very intrested:","['analytic-geometry', 'elliptic-curves', 'complex-analysis', 'algorithms', 'complex-numbers']"
3742900,The Newton-Raphson Method for finding a correct monthly interest rate,"I am very new to this topic and just started to learn about this method. Trying to understand this method intuitively. In this document I found and interesting real life question: A loan of $A$ dollars is repaid by making $n$ equal monthly payments of $M$ dollars, starting a month after the loan is made. It can be shown
that if the monthly interest rate is $r$ , then $$Ar=M\left(1-\frac1{(1+r)^n}\right).$$ A car loan of $10000$ dollars was repaid in $60$ monthly payments of $250$ dollars.
Use the Newton Method to find the monthly interest rate correct to $4$ significant figures. Can somebody please explain intuitively why do we use this method in real life? If I understood correctly so far, we can make a guess and then find a very close number to the real answer, using this method. Appreciate your time and other interesting examples, ideally with a code example in R/Python. Thanks!","['newton-raphson', 'calculus', 'derivatives']"
3742940,How to differentiate $f(r\cos\theta) = r$ with respect to $r\cos\theta$?,"Let $r,\theta \in \mathbb{R}$ . As stated in the title, how do I differentiate $f(r\cos\theta) = r$ with respect to $r\cos\theta$ ? I have never encountered a question or concept like this, and am not sure where to start. My first thought is to start from the fundamentals: perhaps I should try differentiating $x$ with respect to $2x$ . Perhaps I can use change of variables with $u$ = $2x$ . Then the problem would be equivalent to differentiating $\frac{x}{2}$ with respect to $x$ , which is easy. However, evaluating either $\frac{d}{d(2x)} x$ or $\frac{d}{d(r\cos\theta)} r$ in various software produces error messages, so I'm not sure that the change of variables idea is even valid. How should I proceed? Any advice is deeply appreciated.","['trigonometry', 'derivatives']"
3742948,"How to naturally encounter the properties of identity, commutativity, associativity, and distributivity (to define abstract algebra)?","In elementary school, I remember learning about the basic algebraic properties of the integers like identities, commutativity, associativity, and distributivity, and not really thinking much about them (I mean, as a kid I thought they were obvious and not worth dedicating a month to, haha). Now that I'm starting abstract algebra, these four things pop up again, but this time around, these laws seem far more mysterious, perhaps because they are being used as some sort of ""basis"" for generating a ""valid"" algebraic structure, instead of just random facts about numbers. My question is this; I would expect there to be lots of formulas regarding elementary arithmetic, but somehow these four ideas generate everything. How could one trying to isolate algebraic properties of $\mathbb Z$ come up with this exact ""basis""? Is there some kind of logical/algorithmic method we could use to systematically discover these laws and be sure that they encompass everything we care about when it comes to elementary arithmetic? For example here: What is the role of associative and commutative properties in Mathematics and what if someone want to prove them?? , one answer proved commutativity of addition from the Peano axioms. But surely there could be tons of little identities proven from the Peano axioms, about the same level of difficulty, so why should commutativity be so important compared to all the other ""exercise problems"" ? Phrased another way; is there another list of properties that in a sense is equivalent to the four I mentioned above? If so, what reasons would one consider when choosing which ""basis"" do define abstract algebra with? The problem is that these laws don't seem obviously important a priori, so I am hoping that someone has some sort of motivating example to illustrate how these properties sort of ""bubbled up"" out of the stew containing all arbitrary identities . For example, one answer here: Jacobi identity - intuitive explanation , claims that the Jacobi identity arose out of examining the properties of an important commutator (though I do not at all know what that all means; it is just an example to illustrate what I would want a ""motivating example"" to look like). An idea I had was that if someone could tell a story about building arithmetic from the Peano axioms, like here: https://www.math.wustl.edu/~kumar/courses/310-2011/Peano.pdf , sort of like: ok we defined the operator $+$ that takes in two things from $\mathbb N$ and spits out one thing in $\mathbb N$ recursively by saying $n+1 = \sigma(n)$ and $n+\sigma(m)=\sigma(n+m)$ . Now an example: we already defined "" $1$ "", and let's define $2$ as $2 = \sigma(1)$ . Then $1+1=\sigma(1)=2$ . Nice! How about $2+1$ ? Well, $2+1 = \sigma(2)$ which we'll call $3$ . But what if I asked about $1+2$ ? Then the 1st rule won't help, but we can write $1+2=1+\sigma(1)=\sigma(1+1)=\sigma(2)=3$ . Yay! But this was annoying because we know intuitively that switching the things around on the $+$ operator doesn't change anything, so let's prove this property (which we'll call commutativity). However, I can't seem to shoehorn associativity or distributivity in a convincing manner, so perhaps this is the wrong approach. Another idea I had was like starting again from the Peano axioms and then saying like ""ok, we rigorously defined numbers and addition and multiplication and induction. Let's do the age old Gauss integer sum problem from the Peano axiom framework!"". This problem immediately forces us to define addition for $n$ numbers (associativity), and then the end result involves $n(n+1)$ so distributivity comes up naturally. However, this is kind of awkward (like it's awkward to shoehorn in Gauss's sum problem randomly in the middle discussing foundational arithmetic--at least it feels slightly unnatural in my eyes), so I don't know. Phrased another way, my complaints for this idea is that there arise two questions: ""why should we consider this Gauss problem"" and ""why should this problem be all that is needed to develop every property we care about in arithmetic""? Criticisms and ideas are welcome!","['elementary-number-theory', 'abstract-algebra', 'associativity', 'arithmetic', 'soft-question']"
3743007,"Evaluate $\int_0^1\left(\frac{x-1}{x+1}\right)^n\frac{1}{\ln x} \,dx$","Evaluate: $$ I(n) = \int_0^1\left(\frac{x-1}{x+1}\right)^n\frac{1}{\ln x} \,dx$$ For $n \in \mathbb N$ . I started by noticing that $\int_0^1 x^y \,dy = (x-1)/\ln x$ So I can convert this into a double integral as follows: $$I(n) = \int_0^1\int_0^1\frac{(x-1)^{n-1}x^y}{(x+1)^n}\,dy\,dx$$ Changing the order of integration and plugging in $x \rightarrow \cos 2\theta$ , I get: $$I(n) = 2(-1)^n\int_0^1 \int_0^{\pi/4}\tan^{2n-1}\theta \cos^y 2\theta \,d\theta \,dy$$ I have a feeling that I can somehow transform the inner integral into a Beta function, but the $\pi/4$ and $2\theta$ are a bit irritating and hard to get rid of. Any methods to tackle this?","['integration', 'calculus', 'definite-integrals']"
3743113,Intuitive Understanding of this probability drawings question,"I'm doing this problem from Carol Ash's The Probability Tutoring Book Draw 20 times from the integers {1,2,3...,100}. Find the probability that your draws come out in increasing order if the drawings are with replacement My attempt:
We are drawing with replacement, the probability of getting a greater number the 2nd, 3rd, 4th and 5th time depend on the previous draws, so I reasoned: $$\sum_i\frac{1}{100^4}{\frac{100 - i}{100}}$$ But this doesn't work because $i$ - the variable we sum over - isn't measuring what our greatest number at any time is. So I peeked at the answers: $$\frac{100 \choose 20}{100^{20}}$$ The denominator, as I understand it, is the total number of possible drawings with replacement. The numerator is the total number of groups of 20 that I can make from the 100 numbers. But these groupings don't have to necessarily be in increasing order. Also, I learned in high school that when we use the combination operator we draw without replacement... Can someone explain the numerator to me please(whether mathematically or intuitively)?","['discrete-mathematics', 'combinatorics', 'probability']"
3743116,Using sum-to-product formula to solve $\sin(2\theta)+\sin(4\theta)=0$,"Trying to use the sum-to-product formula to solve $\sin(2\theta)+\sin(4\theta)=0$ over the interval $[0,2\pi)$ , but I'm missing solutions. $$\sin(2\theta)+\sin(4\theta)=0$$ Apply sum-to-product formula: $$2\sin\left(\frac{2\theta+4\theta}{2}\right)\cos\left(\frac{2\theta-4\theta}{2}\right)=0$$ $$2\sin(3\theta)\cos(-\theta)=0$$ By odd-even identities: $\cos(-\theta)=\cos(\theta)$ $$2\sin(3\theta)\cos(\theta)=0$$ $$\sin(3\theta)\cos(\theta)=0$$ By the zero-product property $\sin(3\theta)=0$ or $\cos(\theta)=0$ Then solving for theta gives: $\theta=0, \frac{\pi}{2}, \frac{3\pi}{2}, \pi$ . However, there are missing solutions $\frac{\pi}{3}, \frac{2\pi}{3}, \frac{4\pi}{3}, \frac{5\pi}{3}$ . A solution online used double angle identities instead: $$\sin(2\theta)+\sin(4\theta)=0$$ $$\sin(2\theta)+\sin(2*2\theta)=0$$ Apply double angle identity for: $\sin(2*2\theta)$ $$\sin(2\theta)+2\sin(2\theta)\cos(2\theta)=0$$ Factor out $\sin(2\theta)$ $$\sin(2\theta)*[1+2\cos(2\theta)]=0$$ Apply double angle identities: $\cos(2\theta)= 1-2\sin^2(\theta)$ $\sin(2\theta)= 2\sin(\theta)\cos(\theta)$ $$2\sin(\theta)\cos(\theta)*[1+2(1-2\sin^2(\theta))]=0$$ $$2\sin(\theta)\cos(\theta)*[-4\sin^2(\theta)+3]=0$$ By the zero-product property $2\sin(\theta)\cos(\theta)=0$ or $-4\sin^2(\theta)+3=0$ Which further simplifies to $\sin(\theta)=0$ , $\cos(\theta)=0$ , or $-4\sin^2(\theta)+3=0$ Solving for theta now gives all possible solutions over $[0, 2\pi)$ . My questions are:
(1) Can the sum-to-product formula be used to solve this equation? (2) If so, why were solutions missing when using the sum-to-product formula but not the double angle identities? What was I doing incorrectly?","['algebra-precalculus', 'solution-verification', 'roots', 'trigonometry']"
3743123,About essential range and essential supremum,"$\newcommand{\esssup}{\mathrm{ess\,sup}}$$\newcommand{\essrng}{\mathrm{ess\,range}}$ I am trying to prove that $\esssup(f) = \sup(\essrng(f))$ , where we define $$
\esssup(f) = \inf \{b \in \mathbb{R}_+ : \mu(f^{-1}((b, \infty))) = 0\} 
$$ and similarly $$
\essrng(f) = \{w \in \mathbb{R}_+ : \mu(f^{-1}(B(w, \epsilon))) > 0\}.
$$ Actually, I already showed that $\esssup(f) \leq \sup(\essrng(f))$ , but I have not been able to prove the other direction. The answer on this related question hasn't been useful for me to prove the desired reverse inequality. If you could give me a hint to prove it, I'll be really grateful.","['measure-theory', 'lp-spaces', 'measurable-functions']"
3743131,Closed-form solution for the determinant of a Vandermonde-like matrix,"I'm trying to find a closed-form solution $\forall$ odd integer $n\ge 3$ for the determinant of a matrix with some structure on it. After some manipulation, I've reduced it to the following matrix: $\small\begin{bmatrix}\boldsymbol{t_{1}^{n}-t_{a}^{n}} & \boldsymbol{t_{2}^{n}-t_{a}^{n}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}^{n}-t_{a}^{n}} & nt_{1}^{n-1} & \cdots & nt_{a-1}^{n-1} & nt_{a}^{n-1}\\
\boldsymbol{t_{1}^{n-1}-t_{a}^{n-1}} & \boldsymbol{t_{2}^{n-1}-t_{a}^{n-1}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}^{n-1}-t_{a}^{n-1}} & (n-1)t_{1}^{n-2} & \cdots & (n-1)t_{a-1}^{n-2} & (n-1)t_{a}^{n-2}\\
\boldsymbol{\vdots} & \boldsymbol{\vdots} & \boldsymbol{\ddots} & \boldsymbol{\vdots} & \vdots & \ddots & \vdots & \vdots\\
\boldsymbol{t_{1}^{2}-t_{a}^{2}} & \boldsymbol{t_{2}^{2}-t_{a}^{2}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}^{2}-t_{a}^{2}} & 2t_{1} & \cdots & 2t_{a-1} & 2t_{a}\\
\boldsymbol{t_{1}-t_{a}} & \boldsymbol{t_{2}-t_{a}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}-t_{a}} & 1 & \cdots & 1 & 1
\end{bmatrix}_{n\times n}$ where $a:=\frac{n+1}{2}$ , the bold block is $n\times(\frac{n+1}{2}-1)$ , and the non-bold block is $n \times \frac{n+1}{2}$ . Although it has some similarities with the Vandermonde Matrix or some generalizations , it's not the same. Using some values of n, its determinant looks pretty simple, which leads me to think that there should be a closed-form solution: $n=3$ : $$
det\left( 
\left[\begin{array}{ccc} {t_{1}}^3-{t_{2}}^3 & 3\,{t_{1}}^2 & 3\,{t_{2}}^2\\ {t_{1}}^2-{t_{2}}^2 & 2\,t_{1} & 2\,t_{2}\\ t_{1}-t_{2} & 1 & 1 \end{array}\right]
\right)=
-{\left(t_{1}-t_{2}\right)}^4
$$ $n=5$ : $$
det\left( 
    \left[\begin{array}{ccccc} {t_{1}}^5-{t_{3}}^5 & {t_{2}}^5-{t_{3}}^5 & 5\,{t_{1}}^4 & 5\,{t_{2}}^4 & 5\,{t_{3}}^4\\ {t_{1}}^4-{t_{3}}^4 & {t_{2}}^4-{t_{3}}^4 & 4\,{t_{1}}^3 & 4\,{t_{2}}^3 & 4\,{t_{3}}^3\\ {t_{1}}^3-{t_{3}}^3 & {t_{2}}^3-{t_{3}}^3 & 3\,{t_{1}}^2 & 3\,{t_{2}}^2 & 3\,{t_{3}}^2\\ {t_{1}}^2-{t_{3}}^2 & {t_{2}}^2-{t_{3}}^2 & 2\,t_{1} & 2\,t_{2} & 2\,t_{3}\\ t_{1}-t_{3} & t_{2}-t_{3} & 1 & 1 & 1 \end{array}\right]
\right)=
-{\left(t_{1}-t_{2}\right)}^4\,{\left(t_{1}-t_{3}\right)}^4\,{\left(t_{2}-t_{3}\right)}^4
$$ I was wondering if there is a known closed-form solution for this determinant, or if it could be found using the determinant of a generalized Vandermonde matrix Thanks!","['matrices', 'determinant', 'closed-form', 'matrix-decomposition']"
3743153,Is the kernel of a differential form a subset of $M$ or of $TM$?,"What do we mean by the ""kernel of a differential 1-form""? In particular, if $\alpha$ is a 1-form on $M$ , then I understand that it takes points $p\in M$ to functionals $\alpha_p:T_pM\to\mathbb R$ . So is the kernel of $\alpha$ the set of points $p\in M$ such that $\alpha_p$ is the zero map, or is $\ker\alpha$ actually the set of vectors $v$ in $TM$ such that $\alpha_p(v)=0$ for the appropriate $p$ ? I ask this mostly because I would've expected $\ker\alpha$ to be the first one, but I also saw somewhere that the kernel of the 1-form is a 2-plane distribution (which doesn't make any sense if $\ker\alpha\subseteq M$ , of course). (Also, sorry if something like this has been posted before; I saw some similar-looking posts, but I'm still a bit shaky with differential forms, so I had a lot of trouble understanding them.)","['differential-forms', 'differential-geometry']"
3743160,"Prove that for every bijection $f : S \to S$, there exists a function $g : S \to S$ such that $f \circ g = i$ and $g \circ f = i$","I'd like to know if the following proof is correct. Prove that for every bijection $f : S \to S$ , there exists a function $g : S \to S$ such that $f \circ g = i$ and $g \circ f = i$ proof:
Let $f$ be a bijection from $S$ onto $S$ , and let $i:S \to S$ be defined as $i(x)=x$ for any $x \in S$ . By definition, each $y \in S$ is mapped to by exactly one element in $S$ . Then let $g: S \to S$ be defined as $g(x_{0}) = z_{0}$ and $g(y_{0})=x_{0}$ , where $f(x_{0})=y_{0}$ and $f(z_{0})=x_{0}$ . It follows that $i(x_{0})=x_{0}=g(y_{0})=g(f(x_{0}))=(g \circ f)(x_{0})$ and $i(x_{0})=x_{0}=f(z_{0})=f(g(x_{0})) = (f \circ g)(x_{0})$",['functions']
3743173,Spectrum of Koopman operator,"I've been trying to solve the following exercise from Foundations of Ergodic Theory - Krerley Oliveira and Marcelo Viana: Let $R_{\theta}\colon S^{1}\to S^{1}$ be an irrational rotation and $m$ be the Lebesgue measure on the circle. Calculate the eigenvalues and the eigenvectors of the Koopman operator $U_{\theta}\colon L^{2}\left(m\right)\to L^{2}\left(m\right).$ Show that the spectrum of $U_{\theta}$ coincides with the unit circle $\left\{z\in \mathbb{C}: |z|=1 \right\}.$ I can show that the set of functions on the form $\varphi_{k}\left(x\right)=e^{2\pi i k x}$ form a Hilbert basis for $L^{2}\left(m\right)$ and are all eigenfunctions (with eigenvalue $\lambda_{k}=e^{2\pi i k \theta}$ ).  As $\theta \in \mathbb{R}\setminus \mathbb{Q}$ this basis is a dense subset in $S^{1}.$ So, because the spectrum is a compact set and $U_{\theta}$ is an isometry, we have the following: $$S^{1}=\overline{\left\{\text{eigenvalues of }U_{\theta}\right\}} \subseteq  \text{spec}\left(U_{\theta}\right) \subseteq \mathbb{D},$$ where $\mathbb{D}=\left\{z\in \mathbb{C}: |z|\leq 1\right\}.$ But this doesn't answer neither questions stated above: are those the only eigenvalues and eigenvector? Is that true that $\text{spec}\left(U_{\theta}\right)\subseteq S^{1}?$ I can't see why this are/aren't true... If someone can help I would appreciate a lot! Thanks!","['operator-theory', 'ergodic-theory', 'functional-analysis', 'spectral-theory', 'dynamical-systems']"
3743191,Is there a rigorous way to describe $g(x)$ continuously deforming into $h(x)$ and could it be useful?,"One thing that bothers me about mappings is that they seem to instantaneously transport points from one space to another. I feel like there should be a space of unique, non-intersecting paths that each of the points travel on to get to their new destination. Does anyone agree? For example consider a mapping $F:\Bbb R^2 \to \Bbb R^2$ with $F(x,y)=(e^x,e^y).$ Consider the function $g(x)=\frac{1}{x}$ embedded in the standard $x-y$ cartesian system. We start with $g$ and magically get $h(x)=e^{\frac{1}{\log(x)}}$ with no information about how $g$ was deformed into $h!$ Maybe it's just perspective but I feel like at every point in time we should be able to track the deformations as $g$ morphs into $h.$ I drew a picture with the paths that I think each of the points should follow as they start with $g$ and move to become $h.$ The upper bound path is $y=e^x$ and the lower bound path is $y=\log(x).$ The central path is $y=x.$ Obviously there's not enough rigor here, but I tried my best with what I know. My question is: Is there a rigorous way to describe $g$ continuously deforming into $h$ and could it be useful?","['general-topology', 'functions', 'soft-question', 'transformation']"
3743212,"Optimizing expectation, unknown parameters, normal distribution. How many restaurants should I try before choosing one for the rest of my n-m meals?","Suppose you move to a new city with an infinite number of restaurants and you plan to stay there for a predetermined amount of time.  You plan to have n meals at restaurants over the course of your stay in the city.  Assume there are no reviews on any of the restaurants and the only way to determine how good a restaurant is is by eating there and giving it a rating.  Also assume the quality of restaurants follows a normal distribution but you don't know the parameters, µ and σ, of the distribution.  Your goal is to optimize the expectation of your combined restaurant experiences. You do not value variety and would happily eat every single meal at the best restaurant if you could find it. For example, if you simply ate at a different restaurant for every meal you would have an expected combined experience of n $\times$ µ. Also if you had a meal at a random restaurant then decided to have all your meals there without trying any others you would again have an expected combined experience of n $\times$ µ. But you could improve upon that by trying two restaurants then choosing the better of the two and having all your remaining meals there.  Then your expected combined experience would be (n-1) $\times$ a 1 +a 2 . where a 1 is the expected rating of the better of the two restaurants and a 2 is the expectation of the worse restaurant.  (What would a 1 and a 2 be in terms of µ and σ for this case? I know (a 1 +a 2 )/2=µ but don't know how far apart they would be). You could improve further by trying 3 restaurants and choosing the best of those and so on.  If you sampled m restaurants before settling on one for your remaining meals your combined expectation would be (n-m+1) $\times$ a 1 +a 2 +a 3 +...+a m where again a 1 is the highest expectation of these m drawings. Main question: How many restaurants, m, should you try before picking the best of those restaurants for your remaining n-m meals?","['statistics', 'normal-distribution', 'expected-value', 'optimization', 'probability']"
3743219,Solution to quadratic program,"Let $ (a_{ij}) $ and $ (b_{ij}) $ be two sequences of real numbers. I'm trying to solve the quadratic program $$
\min_{(a_{ij})} \sum_i \Big(\sum_j a_{ij} \Big)^2 \quad \text{s.t.} \quad \sum_i\sum_j a_{ij}b_{ij} = 1
$$ using Lagrange multipliers, but can't seem to get anywhere. Does anyone know any references for these types of programs? Edit: With $ a = (a_{ij}) $ , the Lagrangian is $$
\mathcal{L}(a, \lambda) = \sum_i \Big(\sum_j a_{ij} \Big)^2 - \lambda \sum_i\sum_j a_{ij}b_{ij}
$$ and its partial derivatives are $$
\frac{\partial}{\partial a_{ij}}\mathcal{L}(a, \lambda) = 2\sum_{j'} a_{ij'} - \lambda b_{ij}.
$$","['optimization', 'multivariable-calculus', 'quadratic-programming', 'lagrange-multiplier']"
3743275,A fair coin is tossed untill head appears for the first time. What is probability that the number of tosses required is odd? [duplicate],This question already has answers here : A fair coin is tossed until a head comes up for the first time. The probability of this happening on an odd number toss is? (3 answers) Closed 3 years ago . Q. A fair coin is tossed untill head appears for the first time. What is probability that the number of tosses required is odd? My work: suppose that head comes in first toss so probability of getting head in the first toss $=\dfrac{1}{2}$ suppose that first & second tosses show tails & third toss shows head so  probability of getting head in the third toss $=(1-\dfrac12)(1-\dfrac12)\dfrac{1}{2}$ $=\dfrac1{2^3}$ suppose that first 4 tosses show tails & fifth toss shows head so  probability of getting head in the fifth toss $=(1-\dfrac12)^4\dfrac{1}{2}$ $=\dfrac1{2^5}$ suppose that first 6 tosses show tails & seventh toss shows head so  probability of getting head in the fifth toss $=(1-\dfrac12)^6\dfrac{1}{2}$ $=\dfrac1{2^7}$ ……………. and so on But I am not able to find the final probability of getting head first time so that the number of tosses required is odd.  what should do I next to it? please help me.,"['conditional-probability', 'probability']"
3743282,Motivation behind definition of complex sympletic group [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question One definition of complex sympletic group I have encountered is (sourced from Wikipedia ): $$Sp(2n,F)=\{M\in M_{2n\times 2n}(F):M^{\mathrm {T} }\Omega M=\Omega \}$$ What is the motivation for imposing the condition $M^{\mathrm {T} }\Omega M=\Omega$ instead of others such as $M^{-1}\Omega M=\Omega$ ?","['group-theory', 'definition', 'lie-groups']"
3743293,Evaluating $\lim\limits_{n \to \infty}\frac{2^{2n}(n!)^2}{(2n+1)!}$,"Can you help me find this limit: $$\lim_{n \rightarrow \infty} \dfrac{2^{2n}(n!)^2}{(2n+1)!}$$ I tried to use $$ \lim_{n \rightarrow \infty} \dfrac{2^{2n}(n!)^2}{(2n+1)!} = \ln\left(\exp\left( \lim_{n \rightarrow \infty} \dfrac{2^{2n}(n!)^2}{(2n+1)!} \right)\right)$$ but it didn't work out so well. This problem came up when I was trying to find the radius of convergence of the sum $$\sum_{n=0}^{\infty}(-1)^n\left[\dfrac{2^{n}(n!)^2}{(2n+1)!}\right]^p.x^n$$ Thanks! (Anyway I omitted the exponential $p$ in the limit because it just a constant). (*Edited. Is there any others way instead of using Stirling approximation(1) or the Center Binomial Coefficient(2)? I just stated learning Analysis and this limit is only a part of the finding the radius of convergence problems,though. It take way too long and hard to prove (1) and (2)). Sorry for not  ask clearly the first time.","['limits', 'sequences-and-series', 'real-analysis']"
3743300,Proving that $\left|A \times A\right|$ is equal to $\left|A\right|$ for every infinite set,"How do you prove that $\left|A \times A\right|$ is equal to $\left|A\right|$ for every infinite set? I'm trying to prove this basic fact of cardinal arithmetic, but I'm getting stuck on the uncountable sets. I suspect there's an obvious technique that I'm missing for proving statements like this. I'm okay with taking the axiom of choice and am not trying to prove that this fact is equivalent to the axiom of choice as in this question . Let $\varepsilon$ refer to the empty set. Let $\mathbb{N}$ refer to the positive integers. I'm trying to prove that $$ \text{$\left|A\right| = \left|A \times A\right|$ if and only if $\left|A\right| \in \{0, 1\}$ or $A$ is infinite.} $$ Here's my attempt to prove it. Let's consider five different mutually exclusive cases for the size of $A$ . $A$ is empty $A$ is a singleton $\left|A\right| \in \mathbb{N}_{\ge2}$ $A$ is countably infinite. $A$ is uncountably infinite. case 1/5: $A$ is empty. $ \varepsilon \times \varepsilon $ is equal to $\varepsilon$ . case 2/5: $A$ is a singleton. $ \{\sigma_1\} \times \{\sigma_2\} $ is $\{(\sigma_1, \sigma_2)\}$ . case 3/5: $\left|A\right| \in \mathbb{N}_{\ge2}$ Let $x$ be a real variable. $$ \text{If $x = xx$, then $x \in \{0, 1\}$  } $$ Therefore, the left and right components of the theorem are both never true. case 4/5: $A$ is countably infinite. Since $A$ is countably infinite, there exists a bijective function $f : A \to \mathbb{N}$ . There's a bijective function $g$ from $\mathbb{N} \times \mathbb{N}$ to $\mathbb{N}$ , Arguments to $g$ are shown below with a leading dot, for example ·7 . * marks a value that has been omitted for expository purposes. x
         ·1  ·2  ·3  ·4
     ·1   1   3   6  10
  y  ·2   2   5   9   *
     ·3   4   8   *   *
     ·4   7   *   *   * We notice that the first row is the triangle numbers, so $g(x, 1) = \frac{x+xx}{2}$ . For the cells below the first row, we note that $g(x, y) = g(y+x-1, 1) + (1 - y)$ . $g$ is a bijection from $\mathbb{N} \times \mathbb{N}$ to $\mathbb{N}$ . Thus $h$ , as defined below, is a bijection from $A \times A$ to $A$ . $$ h(a, b) = f^{(-1)}(g(f(a), f(b))) $$ case 5/5: $A$ is uncountably infinite. This case, I can't prove, but I can prove it for some of the uncountable cardinalities. Let $\varphi$ be a bijection from $\mathbb{R}$ to $\mathbb{R} \cup \{ -\infty, \infty \}$ . Let $a$ be a bijection from $\mathbb{R}$ to $[0,1]$ . $$ a(t) = \frac{\arctan(t) + \frac{\pi}{2}}{\pi} $$ $$ a(-\infty) = 0 $$ $$ a(+\infty) = 1 $$ Let $j$ be a bijection from $[0,1] \times [0,1]$ to $[0,1]$ . Let $j(x, y)$ be given as follows. Write out $x$ and $y$ in base 2. Using the following table, convert each digit pair one at a time in $x$ and $y$ to base 4. x
       ·0  ·1
 y  ·0  0   1
    ·1  2   3 for example $$ j(.01\cdots, .11\cdots) = .23\cdots $$ We can use this to define a bijection from $\mathbb{R} \times \mathbb{R}$ to $\mathbb{R}$ . $$ \psi(u, v) = \varphi^{(-1)} \circ a^{(-1)} \circ j(a\circ \varphi(u), a \circ \varphi(v)) $$ Let $S_X$ be the set of functions from $X$ to $\mathbb{R}$ , then we can define a bijection $p$ from $S_X \times S_X$ to $S_X$ as follows. Let $\lambda \cdots \mathop. \cdots$ denote an anonymous function. $$ p(a, b) = \lambda x \mathop. \psi(a(x), b(x)) $$ The same arguments works for the set $D$ , where $x \in D$ if $x$ is positive or negative infinity or if $\arctan(x)$ is a dyadic rational. In the case of dyadic rationals, after a prefix of finite length all the digits are zero in both base 2 and base 4. Therefore $D$ is countable. Therefore, we have bijections between $A$ and $A \times A$ for all cardinalities of the form $\left|\mathbb{R}\right|$ , $\left|X \to \mathbb{R}\right|$ , and $\left|X \to D\right|$ . However, I don't have any reason to believe that all of the uncountable cardinalities are expressible in this way.","['elementary-set-theory', 'cardinals']"
3743302,I don't get why the answer is 5-diamond? It could also be Queen-heart and 4-heart.,"The aim is for A and B to guess the right card from the below deck of cards. A is told the number only of the card. B is told the shape only of the card. Based on the following conversation between A and B, A and B manage to guess correctly the right card. A: I don't know the card . B: Before you mentioned, I already know you don't know, but I still don't know . A: Now, I know the card. B: I also know the card now. Ok so my question is: Why is the answer 5 diamond? It could also be Queen-heart and 4-heart. From 1, the message from A provides me the information that the right card has at least 1 other card with the same number though different symbol. As such, I can already rule out those cards without at least 1 other card with the same number. From 2, the message from B provides me and A some information about the symbol of the right card. I can already rule out ALL spade and club because 8 spade and 6 club do not have at least one other card of the same number but different symbol. From 3, the message from A provides me and B information that Ace heart and Ace diamond can be ruled out because after A received B's message from Line 2, A can decisively know the answer. Now here lies the confusion for me -- hope some kind soul can enlighten me -- how could B reply ""I also know the card"" and choose 5 diamond because, at least to me, there is not enough information for B to conclude logically that it is 5 diamond. It could also be Queen-heart and 4-heart.","['combinations', 'puzzle', 'discrete-mathematics', 'combinatorial-game-theory', 'game-theory']"
3743341,Concrete Mathematics: Josephus Problem: Odd induction,"I am trying to work through the odd induction case of the closed form solution to the Josephus problem. To start with a quick review of the even case - I'm being quite verbose though to help frame the question and also to potentially highlight any mistakes in my understanding that just happen to work in the even case. Quick review of even case Recurrence: $J(2n) = 2J(n) - 1$ Closed form to prove: $J(2^m+l)=2l+1$ First we express it in terms of the recurrence $$J(2^m+l)=2J(2^{m-1}+\frac{l}{2})-1$$ Logically, then, these two are equivalent $$2J(2^{m-1}+\frac{l}{2})-1=2(\frac{2l}{2}+1)-1$$ Which finally gives us what we want $$2(\frac{2l}{2}+1)-1=2(l+1)-1=2l+2-1=2l+1$$ Odd case Odd recurrence: $J(2n+1)=2J(n)+1$ I am trying to apply the closed form in the same way. First in terms of the odd recurrence: $$J(2^m+l)=2J(2^{m-1}+\frac{l}{2})+1$$ Then plugging in the closed form: $$2(2\frac{l}{2}+1)+1$$ But then this does not induct: $$2(\frac{l}{2}+1)+1=2(l+1)+1=2l+3$$ I am not sure what I am misunderstanding.","['induction', 'discrete-mathematics']"
3743397,Number of functions from $\mathbb{Z}_m$ to $\mathbb{Z}_n$. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I'm just wondering if there's a generalised way of computing the number of functions from $\mathbb{Z}_m$ to $\mathbb{Z}_n$ , and additionally the number of one-to-one and onto functions between $\mathbb{Z}_m$ and $\mathbb{Z}_n$ .","['functions', 'discrete-mathematics']"
3743407,Gradient is perpendicular to level set and implicit function theorem,"My lecture notes on the gradient state the following: For $f : U \to \mathbb{R}$ differentiable consider the level set $N_w = \{v \in U : f(v)=w\}$ where $U \in \mathbb{R}^n$ . Suppose that $c : I \to N_w \in U$ is a differentiable curve. Then $f
\circ c = w$ and so $0=\frac{d}{dt}(f \circ c)=\langle grad f(c(t)),c'(t)\rangle \iff grad
(c(t)) \perp c'(t)$ . Since this holds for any differentiable curve running through $N_w$ ,
we can say that the gradient vector of $f$ is perpendicular to the
level sets $N_w$ . The proof is clear to me, but this brings up the question whether such a curve $c(t)$ exists. I've read a couple of other posts about this and it seems that the existence of such a function is guaranteed by the implicit function theorem. Implicit function theorem (IFT): Let $\Omega \subset \mathbb{R}^n \times \mathbb{R}^k =
\mathbb{R}^{n+k}$ be open and $f : \Omega \to \mathbb{R}^k, (x,y) \to
f(x,y)$ be continuously differentiable. Moreover, let $(a,b) \in
\Omega$ be a point with $f(a,b) = 0$ , such that \begin{equation*} J^Y_f =  \begin{pmatrix} \frac{\partial
f_1}{\partial y_1} & \cdots & \frac{\partial f_1}{\partial y_k} \\
\vdots  & \ddots  & \vdots \\ \frac{\partial f_k}{\partial y_1} &
\cdots & \frac{\partial f_k}{\partial y_k}  \end{pmatrix}
\end{equation*} satisfies the following equivalent conditions at $(a,b)$ : $(*) J^Y_f(a,b)$ is invertible $\iff$ rank $J^Y_f(a,b) = k \iff det
J^Y_f(a,b) \neq 0$ . Then there are neighbourhoods $X \subset \mathbb{R}^n$ of $a$ and $Y
\subset \mathbb{R}^k$ of $b$ with $X \times Y \subset \Omega$ as well
as a continuously differentiable mapping $g : X \to Y$ with $f(x,y)=0$ for $(x,y) \in X \times Y \iff y=g(x)$ for $x \in X$ . Now the theorem gives a way to express some of the variables as a function of the others, but for the statement about the gradient we would need a curve $c(t)$ parameterized by $t$ where $c(t)$ is a vector in $\mathbb{R}^n$ . How can we parameterize the graph $(x,g(x))$ by $t$ ? Thanks a lot!","['multivariable-calculus', 'implicit-function-theorem', 'parametrization']"
3743417,Differentiability of compositions of non differentiable functions at a point.,"If $f(x)$ and $g(x)$ are differentiable functions at all points in $[a,b]$ except a one point (c) in the interval and continuous everywhere on $[a,b]$ . Then it is guaranteed that $f(g(x))$ is differentiable wherever $f$ and $g$ are differentiable but is there an example for which $f(g(x))$ is also differentiable at c . Or can it be proven that something like this can’t happen? It is true that if they are nowhere differentiable then it is impossible to create an example but is it possible to create an example for such a case? What about 2 points, 3 points , infinitely many?","['calculus', 'derivatives', 'real-analysis']"
3743438,Recursion for Characteristic Polynomial - Proof?,"In the book ""Computational Complexity of Counting and Sampling"" I have found the following theorem: It gives a recursion formula for a division-free algorithm for the determinant in $O(n^4)$ . Now, we have the feeling that each trace $\mathrm{tr}(w(\cdot,\cdot,k))$ yields the $k$ -th homogeneous coefficient of the characteristic polynomial. Examples up to 5 confirm this conjecture. Though, a proof is missing still. I already tried various things. It would be good if one can show that $\mathrm{tr}(w(\cdot,\cdot,k))$ is invariant under change of basis of $M$ . Then one can consider w.l.o.g. diagonal matrices. For these matrices, the proof is easy. However, an induction seems not helpful here. Moreover, when one iteratively insert the recursion, the expressions quickly get complicated. I don't need the full proof here. A hint/direction or useful identity is already appreciated.","['computational-complexity', 'linear-algebra']"
3743451,4-Color Stacking Game Winning Strategy,"This game is played with rocks of 4 colors. Initially, there are N 1-rock stacks on the ground (N>5). Each stack has a size (number of its rocks) and a color (color of the rock on top of that stack) there is at least one rock of each color in the beginning. At each turn, the player whose turn it is can move a stack of rocks on top of another stack, given the two stacks have either the same size or the same color. so the two stacks $S_1$ and $S_2$ turn into one stack with size [size( $S_1$ )+size( $S_2$ )] and the color of whichever stack that was moved on top of the other.
The game ends when there is no other move possible and the person who played the last move wins. edit based on comments: So what's obvious here is that in the final state of the game we will have 1, 2, 3, or 4 stacks remaining (depending on how many colors are left) and creating each stack of size $K$ takes a total of $K-1$ moves throughout the game. So, assuming there are $t$ stacks left in the final state if $N-t$ is odd, the first player wins, and otherwise, the second player wins. I am looking for a way to find a winning strategy (if there is one)","['game-theory', 'recreational-mathematics', 'combinatorics', 'combinatorial-game-theory']"
3743475,$P = \pi_{1}(P) \times \pi_{2}(P)$?,"I’m starting my study of functions, I’m following the book “Proofs and Fundamentals” , by Ethan D. Bloch. This is one of the problems of the book and I’m not sure what would be the solution. Let $X$ and $Y$ be sets. Let $P \subseteq X \times Y$ . Let $\pi_{1}:X\times Y \rightarrow X$ and $\pi_{2}:X \times Y \rightarrow Y$ be the projection maps defined by $\pi_{1}((x,y))=x$ and $\pi_{2}((x,y))=y$ for all $(x,y) \in X \times Y$ . Is it true that $P = \pi_{1}(P) \times \pi_{2}(P)$ ? Give a proof or a counter-example. Intuitively, I believe this is true (correct me if I’m wrong please). Although I’m having trouble in formulating the proof for this result. Any ideas? Thank you for your time!","['elementary-set-theory', 'functions', 'problem-solving']"
3743545,Fibonacci and the spreading of viruses,"I tried to understand better the spreading of a virus on a case-by-case basis, not by differential or difference equations as in SIR and SEIR models with possibly non-integer rates and time constants. In the course of this, Fibonacci numbers showed up in a – for me – unexpected way. And a relation between Fibonacci numbers and powers of $2$ showed up that I'd like to understand better. This was my approach. Let $\lambda$ be the pre-infectious (or latent) period , and let $\lambda = 1$ (e.g. $1$ day). Let $\delta$ be the duration of infectiousness, e.g. $\delta = 4$ . After $\lambda + \delta$ days, an infected individual recovers. Finally let $\beta$ be the infection rate, i.e. the number of people an infectious individual infects per day. Let $\beta = 1$ . This choice of $\beta$ and $\delta$ corresponds to a basic reproduction number $R_0 = \beta\cdot\delta = 4$ . In a deterministic and idealized setup the number of infected indiviudals $I$ evolves like this starting with a single patient 0, neglecting the effect of the decreasing number of susceptible individuals: 1  1  1  1  1                        (patient 0)
       1  1  1  1  1                     (patient 1)
          2  2  2  2  2                  (patients 2 and 3)
             4  4  4  4   4              (patients 4 to  7)
                8  8  8   8   8          (patients 8 to 15)
                  15 15  15  15  15      ...
                     29  29  29  29  29  ...
                         56  56  56  56  ...
                            108 108 108  ...
                                208 208  ...
I = 1  2  4  8 16 30 58 112 216 416 ...
∆ =                2  6  16  40  96 ... Obviously, $I(t) = 2^t$ when $t < \lambda + \delta$ . For $t \ge \lambda + \delta$ , $I(t)$ deviates from $2^t$ , in this example by ∆ = 2, 6, 16, 40, 96, 222, 502, 1116, 2448, 5312, 11426, 24398, 51776, 109296, 229664, 480670 which is a sequence not to be found at OEIS , but I guess it's the number of binary strings of length $n$ having at least one run of length at least $5$ (see below). For the sake of comparison here for $\delta = 2$ 1  1  1                   
       1  1  1                    
          2  2  2                 
             3  3  3              
                5  5  5          
                   8  8  8
                     13 13  13   
                        21  21  21   
                            34  34 34
                                55 55 ...
I = 1  2  4  6 10 16 26 42  68 110 ...
∆ =          2  6 16 38 86 188 402 ... [I(n) is obviously twice the $n$ -th Fibonacci number. The Fibonacci numbers come also as the numbers of newly infected individuals per day – but only in this very special case $\delta = 2$ . According to OEIS , the sequence $\Delta$ gives the number of $n$ -tosses having a run of $3$ or more heads or a run of $3$ or more tails for a fair coin resp. the difference between the number of branches of a complete binary tree of $n$ levels, and the number of recursive calls needed to compute the $(n+1)$ -th Fibonacci number .] And for $\delta = 3$ : 1  1  1  1                         
       1  1  1  1                    
          2  2  2  2                   
             4  4  4  4               
                7  7  7  7            
                  13 13 13  13        
                     24 24  24  24    
                        44  44  44  44 
                            81  81  81 ...
                               149 149 ...
I = 1  2  4  8 14 26 48 88 162 298 ...
∆ =             2  6 16 40  94 214 ... [According to OEIS , the sequence $\Delta$ gives the number of binary strings of length n having at least one run of length at least 4 .] I am looking for a general formula $I_{\lambda\delta\beta}(t) = 2^t - \Delta_{\lambda\delta\beta}(t)$ relating in the special case of $\lambda = \beta = 1$ and $\delta =2$ the Fibonacci numbers with the powers of $2$ . A specific side question: What has the number of recursive calls needed to compute the $n$ -th
Fibonacci number to do with the number of binary strings of length $n$ having at least one run of length at least $3$ ? (see above) For $\lambda = \beta = 1$ and large $\delta$ , e.g. $\delta =20$ , the first non-zero terms of $\Delta_{\lambda\delta\beta}$ are ∆ = 2, 6, 16, 40, 96, 224, 512, 1152, 2560, 5632, 12288, 26624, 57344, 122880, 262144, 557056, 1179648, 2490368, 5242880, 11010048 which is the initial part of the sequence $a(n) = n\cdot 2^{n-2}$ (see OEIS ). Edit 1 : The link , user @heropup provided, yields this insight: Edit 2 : There is a follow-up question to this one: Fibonacci and tossing coins .","['biology', 'exponential-function', 'mathematical-modeling', 'sequences-and-series']"
3743552,How to prove that $\sum_{k=0}^n{(-1)^k{4n-2k\choose 2n}{2n\choose k}}=2^{2n}$?,"$$\sum_{k=0}^n{\left( -1 \right) ^k\left( \begin{array}{c}	4n-2k\\	2n\\\end{array} \right) \left( \begin{array}{c}	2n\\	k\\\end{array} \right)}=2^{2n}$$ I know the correctness of this formula, but how can I prove it? Thanks for your help.","['summation', 'binomial-coefficients', 'combinatorics']"
3743564,Another coins weighing puzzle,"In a bank safe deposit box 80 identical coins can be found, of which 2 or 3 are fake. Jason knows that there are 3 fake coins and has also identified them. He is challenged to prove it to his friends Christian and Mary, who both know that the fake coins are 2 or 3 and, in addition, know that each fake coins weigh 1 gram less than the genuine ones. Jason can use a balance scale to perform as many weighings as he likes, but without giving away the identity (fake/genuine) of any coin, at any stage in the process. Which are the optimum number of weighings that Jason must do so as to prove to his friends that the fake coins are exactly 3? No tricks are allowed :) To clarify, there is no limitation in the number of weighings; Jason can do as many as he wants (we are not necessarily looking for the minimum number). Below are my thoughts:
Jason randomly picks 64 coins and weighs 32 against the other 32. We have the following cases: The scale balances, so we have either 0+0 (all are genuine) or 1+1.
In this case, we again split them into two groups 16+16 and weight one against the other. If they balance, we are in the case of 0+0. Otherwise we have 1+1. So we know we have at least 2 fake coins. Then we need to prove that in the remaining 16 coins there is 1 more fake. The scale does not balance. We either have 0+1, or 0+2 or 0+3 or 1+2 (in any order). We take the lighter group and split them into 16+16. If the scale balances, we are in one of the first 3 cases. We then know that the second group contains from 1 to 3 fake. Then we take the 2nd group and split it into 16+16. Again we have the following cases: 1-0, 1+1, 2+0, 3+0, 1+2.
If the scale balances, we know we have 1+1.  Then we need to prove that in the remaining 16 coins there is 1 more fake. If it does not, we take the heavier and split it into 8+8. If the scale balances, we know we have 0+0 fake so we are in one of the cases 1+0, 2+0 or 3+0. We then take the lighter (for which we know it contains 1 or 2 or 3 fake) and split it into 8+8. We again have 5 cases: 1-0, 1+1, 2+0, 3+0, 1+2. If the scale does not balance, we have 1+2 (so we know for sure we have >2 fake). We continue with the remaining cases and then do the same with the 16 coins. Will this work? Can anyone provide a complete solution?",['combinatorics']
3743570,Using the Strong Markov property to show that a Brownian motion depending on a stopping time is identically distributed with a shifted BM,"This is a claim I found from Rene Schilling's Brownian Motion and Stochastic Calculus, but I don't know how to prove it. Let $\tau= \tau(B_\bullet)$ be a stopping time which can be expressed as a functional of a Brownian path, e.g. a first hitting time, and assume that $\sigma$ is a further stopping time such that $\sigma \le \tau $ a.s. Denote by $\tau' = \tau(B_{\bullet + \sigma})$ the stopping time $\tau$ for the shifted process $B_{\bullet+ \sigma}$ , which is the remaining time, counting from $\sigma$ , until the event described by $\tau $ happens. Set $W_\bullet:= B_{\bullet + \sigma} - B_\sigma$ ; then $\tau' = \tau(W_\bullet+ B_\sigma),$ and the functionals $u(B_\tau)$ and $u(W_{\tau'} + B_\sigma)$ have the same distribution, where $u : \mathbf{C}[0,\infty) \to \mathbb{R}$ be a bounded $\mathcal{B}(\mathbf{C})/\mathcal{B}(\mathbb{R})$ measurable functional. The only property I have are the strong Markov properties in the following forms: $\bullet \;W_t:= B_{\sigma +t} - B_\sigma$ , is again a Brownian motion which is independent of $\mathcal{F}_{\sigma +}$ for an a.s. finite stopping time $\sigma$ . $\bullet \;$ Let $\sigma$ be a stopping time. For all bounded Borel measurable $u \in \mathscr{B}_b(\mathbb{R}^d)$ and $P$ almost all $\omega \in \{\sigma < \infty\}$ $$E[u(B_{t+\sigma})|\mathscr{F}_{\sigma+}](\omega)=E[u(B_t+x)]|_{x = B_\sigma(\omega)}=E^{B_\sigma(\omega)}u(B_t).$$ $\bullet$ For all bounded $\mathscr{B}(C)/\mathscr{B}(\mathbb{R})$ measurable functionals $\Psi : C[0,\infty) \to \mathbb{R}$ which may depend on a whole Brownian path and $P$ almost all $\omega \in \{\sigma < \infty\}$ this becomes $$E[\Psi(B_{\bullet + \sigma})|\mathscr{F}_{\sigma +}]=E[\Psi(B_\bullet +x)]|_{x=B_\sigma}=E^{B_\sigma}[\Psi(B_\bullet)].$$ How can I use these properties to show that $u(B_\tau)$ and $u(W_{\tau'} + B_\sigma)$ have the same distribution (conditional on $\mathcal{F}_{\sigma +}$ is I think what the author means)? I have been stuck with this for some time and would greatly appreciate some help. Moreover, how do these facts imply the corollary that $$E[u(B_{\tau}) | \mathscr{F}_{\sigma +}](\omega)=E[u(W_{\tau'}+x)]|_{x=B_\sigma (\omega)}=E^{B_{\sigma}(\omega)} u(W_{\tau'})$$ holds for all $u\in \mathscr{B}_b(\mathbb{R}^d$ ) and P almost all $\omega \in \{\tau<\infty\}.$","['stochastic-processes', 'markov-process', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3743610,Transforming a Gaussian r.v. into an exponential/Laplace,"Let $X$ be a Gaussian distributed random variable and $f: \mathbb{R} \to \mathbb{R}^+$ a transformation function. Looking for $f$ that makes $f(x)$ exponentially (or Laplace) distributed. For instance if $f=x^2$ , $f(x)$ is Chi-square distributed, if $f=e^x$ , $f(x)$ becomes lognormally distributed, etc.","['probability-distributions', 'probability-theory']"
3743706,Upper bound on the $\|\cdot\|_2$ norm of a tridiagonal matrix,"Let $T\in M_{n}(\mathbb{R})$ be a tridiagonal matrix. What can we say about operator norm $\|T\|_2$ ? I'm asking this question because we know that if $T$ were only diagonal, then $\|T\|_2$ is the largest absolute value of any diagonal entry of $T$ , as shown here , and so there might also a similar result for tridiagonal or $k$ -diagonal matrices in general.","['spectral-norm', 'matrices', 'linear-algebra', 'matrix-norms', 'tridiagonal-matrices']"
3743707,Derangement related problem-bijective functions $f: A \to A$ such that $f(x) \neq x$ and $f(1) \neq 2$,"Let $R$ denote the set of all nonempty relations on the set $A$ , where $A = \{1, 2, 3, 4, 5\}$ . A function $f(x)$ is chosen at random from set $R$ . The probability that $f(x)$ is bijective, $f(x)\ne x$ such that $x\in A$ and $f(1)\ne 2$ is_____ My approach is as follow total number of relations is $5 \cdot 5=25$ . Hence, number of nonempty relations is $2^{25}-1$ . Given the formula of derangement where $f(x)\ne x$ we get $5! \cdot (1-\frac{1}{1!}+\frac{1}{2!}-\frac{1}{3!}+\frac{1}{4!}-\frac{1}{5!})=44$ , I am not able to insert condition $f(1)\ne 2$ because those case needs to be removed where $f(x)=2$ .","['derangements', 'functions', 'probability']"
3743722,How to prove this combinatorially $\binom{n}{k}+\binom{n+1}{k}+\binom{n+2}{k}+\cdots+\binom{n+m}{k} = \binom{n+m+1}{k+1}-\binom{n}{k+1}$?,"$n,m,k$ are natural numbers. $$\binom{n}{k}+\binom{n+1}{k}+\binom{n+2}{k}+\cdots+\binom{n+m}{k} = \binom{n+m+1}{k+1}-\binom{n}{k+1}$$ I need to prove this combinatorially but I can't think of a story, how can I approach this? I thought about starting from the left side","['binomial-coefficients', 'combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
3743777,Why do we need scalar-by-matrix derivative?,"We all know that there are such types of derivative: And scalar-by-matrix derivative $\frac{\partial y}{\partial \textbf{X}}  $ is defined as follow: \begin{pmatrix}
\partial y / \partial x_{11} & \partial y / \partial x_{21} & \ldots  &\partial y / \partial x_{p1}\\
\partial y / \partial x_{12} & \partial y / \partial x_{22} & \ldots &\partial y / \partial x_{p2}\\
\vdots & \vdots & \ddots & \vdots \\
\partial y / \partial x_{1q} & \partial y / \partial x_{2q} & \ldots &\partial y / \partial x_{pq}\\
\end{pmatrix} So, why do we need this if we can just put all variables of $y$ in vector and take scalar-by-vector derivative? Probably, I failed to get the main idea about scalar-by-matrix derivative, can you give me some numerical example? I can't find any concrete example in any textbooks.","['matrices', 'matrix-calculus', 'derivatives']"
3743804,Is there a way to determine the eigenvectors of a matrix without working out the eigenvalues?,"Normally, I would first work out the eigenvalues of a matrix and use them to determine the eigenvectors. However, is it possible to go the other way around? Is there any way to determine the eigenvectors of a matrix without working out the eigenvalues?","['eigenvalues-eigenvectors', 'vector-spaces', 'matrices', 'linear-algebra', 'numerical-linear-algebra']"
3743811,Evaluating $\int_{0}^{1}\frac{x-1}{(x+1)\ln x} dx $ [duplicate],"This question already has answers here : How to prove $\int_0^1\frac{1-x}{(\ln x)(1+x)}\ dx=\ln\left(\frac2{\pi}\right)$? (4 answers) Closed 3 years ago . Evaluate the following integral $$\displaystyle I=\int_{0}^{1}\frac{x-1}{(x+1)(\ln x)} \mathrm{d}x $$ My work: I tried it by letting $\displaystyle I(a)=\int_{0}^{1}\frac{(x-1)x^a}{(x+1)(\ln x)} \mathrm{d}x$ and then $\displaystyle I'(a)=\int_{0}^{1}\frac{(x-1)x^a}{x+1} \mathrm{d}x$ . Now $\displaystyle I'(a)=\int_{0}^{1}x^a \mathrm{d}x-\int_{0}^{1}\frac{2x^a}{x+1} \mathrm{d}x$ Now if $\displaystyle J(a)=\int_{0}^{1}\frac{x^a}{x+1}\mathrm{d}x$ , then by applying integration by parts, we get the reccurence relation $J(a)+J(a-1)=\dfrac{1}{a}$ and we can solve it then, but the thing is, we neet to find $I(0)$ ,so even if we compute $J(a)$ , it wouldn't be defined at $0$ and so would $I(a)$ , then how do I find $I'(a)$ by other method? I also tried the substitution $x \to \frac{1}{x}$ , which yields $\displaystyle I=\int_{1}^{\infty}\frac{(x-1)}{(x+1)(\ln x)} \mathrm{d}x$ and when I saw their graphs, it clearly doesn't seem that the area under the graph of this function from $0$ to $1$ and from $1$ to $\infty$ are equal. I would appreciate if someone could  continue from my method and other solutions are also welcomed...","['integration', 'calculus', 'definite-integrals']"
3743817,limit of the sequence $x_{n}:= \sqrt[n]{n \sqrt[n]{n \sqrt[n]{n\ldots}}}$,"I was thinking what happens with the sequence $\{x_n\}_{n\in \Bbb N}$ where: $$x_{n}:= \sqrt[n]{n \sqrt[n]{n \sqrt[n]{n\ldots}}}$$ When you look some terms, for example $x_{1}=1$ , $x_{2}=\sqrt[]{2 \sqrt[]{2 \sqrt[]{2 ...}}}$ , $x_{3}=\sqrt[3]{3 \sqrt[3]{3 \sqrt[3]{3 ...}}}$ , these terms and the others will be continued fractions, where each one converges. I'm asking what happens with $\lim\limits_{n \to \infty}\sqrt[n]{n \sqrt[n]{n \sqrt[n]{n ...}}}$ ?. I have an idea and it is $\lim\limits_{n \to \infty}\sqrt[n]{n \sqrt[n]{n \sqrt[n]{n ...}}}=1$ . My reasoning is in the fact: $$\sqrt[n]{n \sqrt[n]{n \sqrt[n]{n ...}}}= \displaystyle {n^{\frac{1}{n}}} n^{\frac{1}{n^2}} n^{\frac{1}{n^3}}...$$ And you know that: $${\displaystyle \frac{1}{n}> \frac{1}{n^k} \textrm{ for } n,k \in \Bbb N}$$ Then: $$n^{\frac{1}{n}}> n^{\frac{1}{n^k}} \geq 1$$ Like $\lim\limits_{n \to \infty}n^{\frac{1}{n}}=1$ and $\lim\limits_{n \to \infty}1=1$ , by the Squeeze Theorem $\lim\limits_{n \to \infty}\sqrt[n]{n \sqrt[n]{n \sqrt[n]{n ...}}}=1$ . Is this reasoning correct? What do you think about $x_{n}$ ? Do you think there is another way to prove it? I receive suggestions or comments. Thank you.","['number-theory', 'sequences-and-series', 'analysis', 'real-analysis']"
3743821,"Given matrix $A^2$, how to find matrix $A$?","Let $$A^2 = \begin{pmatrix} 3 & 1 \\ 2 & 2 \end{pmatrix}$$ Knowing that $A$ has positive eigenvalues, what is $A$ ? What I did was the following: $$A = \begin{pmatrix}
a & b \\
c & d 
\end{pmatrix}$$ so $$A^2 = \begin{pmatrix}
a^2 + bc & ab+bd \\
ac+cd & bc+d^2 
\end{pmatrix}$$ I got stuck here after trying to solve the 4 equations. Can someone help, please?","['matrices', 'matrix-equations', 'linear-algebra']"
3743837,Evaluating the $ \lim_{n \to \infty} \prod_{1\leq k \leq n} (1+\frac{k}{n})^{1/k}$,"I am really struggling to work out the limit of the following product: $$ \lim_{n \to \infty} \prod_{1\leq k \leq n} \left (1+\frac{k}{n} \right)^{1/k}.$$ So far, I have spent most of my time looking at the log of the above expression. If we set the desired limit equal to $L$ , I end up with: $$\log L = \lim_{n\to \infty}\log\left(\frac{n+1}{n} \right)+\frac{1}{2}\log\left(\frac{n+2}{n} \right) +\cdots +\frac{1}{n}\log\left(\frac{n+n}{n} \right),$$ which I can simplify to: $$ \log L = \lim_{n\to \infty} \log(n+1)+\frac{1}{2}\log(n+2)+\cdots \frac{1}{n}\log(2n)-\log(n)\left(1+\frac{1}{2}+\cdots\frac{1}{n}\right). $$ I tried to consider the above expression in a different form with an integral, but was unable to arrive at anything useful. I have been stuck on this for quite awhile now, and would appreciate any insight. Thanks","['infinite-product', 'limits', 'analysis']"
3743854,"What does ""measurable"" mean intuitively?","So if we have an outer measure $\mu$ on a set $\Omega$ , we defined: A subset A $\subseteq$ $\Omega$ is called $\mu$ -measurable, if for all B $\subseteq$ $\Omega$ : $\mu$ (B) = $\mu$ (B $\cap$ A) + $\mu$ (B \ A). And i understand the definition, but i always thought we can only measure measurable sets, i.e. $\mu$ is only defined for measurable sets, but it's defined for all subsets of $\Omega$ . So why do we define it this way or what's the intuition behind it? If we measure a non-measurable set, does that mean the value will be ""wrong"" in a way? Or do I take the word ""measurable"" too literally?",['measure-theory']
3743858,"The Fermat-Catalan conjecture with exponents $(2,n,4)$, $n\ge4$","The Fermat-Catalan conjecture is that for coprime $x,y,z$ and positive integers $a,b,c$ with $1/a+1/b+1/c<1$ , the generalized Fermat equation $x^a + y^b = z^c$ has only finitely many solutions. I'm considering exponent triples $(a,b,c)$ which are solved. Table 1 of [ BCDY ] surveys known results and states that $(2,n,4)$ , $n\ge4$ has been solved completely and that this is 'Immediate from Bennett–Skinner [ BS ], Bruin [ Br3 ]'. [Br3] covers the case $n=5$ . Fermat dealt with $n=4$ . This leaves $n=6, 9$ and prime $n\ge7$ , but I can't see how [BS] is relevant to that. Can someone explain and/or point me to the relevant part of [BS]. [BCDY] 'Generalized Fermat equations: A miscellany', Bennett, Chen, Dahmen, Yazdani, International Journal of Number Theory, Vol. 11, No. 1 (2015) [BS] 'Ternary Diophantine Equations via Galois Representations and Modular Forms', Bennett, Skinner, Canad. J. Math. Vol. 56(1), 2004 p23-54. [Br3] 'Chabauty methods using elliptic curves', Bruin, J.reine angew. Math. 562 (2003), 27-49.",['number-theory']
3743888,Prove that $|\mathbf{A}+\mathbf{B}|=\mathbf{0}$ As per following condition,"If $\mathbf{A}$ and $\mathbf{B}$ are real orthogonal matrices of the same order and $|\mathbf{B}|+|\mathbf{A}|=\mathbf{0}$ Prove that $|\mathbf{A}+\mathbf{B}|=\mathbf{0}$ My Approach:- $|\mathrm{A}|+|\mathrm{B}|=0$ $\Rightarrow|A|=-|B|$ $|\mathrm{A}| \cdot|\mathrm{B}|=-1 \quad\left[\because|\mathrm{B}|=\left|B^{-1}\right| \text {as they are orthogonal }\right]$ Let, $C=A\left(A^{T}+B^{T}\right) B$ $\Rightarrow|C|=\left|A A^{T} B+A B^{T} B\right|=|B+A| \ldots \ldots \ldots$ (i) And $|\mathrm{C}|=|\mathrm{A}|\left|A^{T}+B^{T}\right||B|=-\left|A^{T}+B^{T}\right|$ $\Rightarrow-\left|(A+B)^{T}\right|=-|A+B| \ldots \ldots \ldots \ldots$ (ii) $|A+B|=-|A+B|$ $\Rightarrow 2|A+B|=0$ $\boxed{\Rightarrow|A+B|=0}$ I am looking for another short approach.Any alternate solution would be greatly appreciated","['matrices', 'determinant', 'linear-algebra']"
3743929,Simplifying $\frac{b^2+c^2-a^2}{(a-b)(a-c)}+\frac{c^2+a^2-b^2}{(b-c)(b-a)}+\frac{a^2+b^2-c^2}{(c-a)(c-b)}$,"Simplify $$\frac{b^2+c^2-a^2}{(a-b)(a-c)}+\frac{c^2+a^2-b^2}{(b-c)(b-a)}+\frac{a^2+b^2-c^2}{(c-a)(c-b)}\,.$$ I tried very hard but I am not being able to solve it easily I opened up everything and multiplied all of it and got the answer -2. But it took me 1 hour and I also made many silly mistakes. Is there a quicker way than brute force?","['symmetric-functions', 'symbolic-computation', 'polynomials', 'algebra-precalculus', 'rational-functions']"
3743983,"For $n$ points on a plane, prove that there are at most $3n$ pairs of vertices with distance 1","Question: Given $n$ points in a plane, the distance between any $2$ vertices is at least $1$ . Prove there are at most $3n$ pairs of points with distance of exactly $1$ . I've seen this thread, which looks very very similar: Given n points in the plane, such that the minimal euclidian distance is 1, show that there are at most 3n pairs of points with distance exactly 1 However it is about planar graphs - and we did not study what is a planar graph yet (and we won't in the future) Is there a way to solve it without using properties of planer graphs? I did not understand how to start the proof without using planar graphs theory.. Thank you!","['euclidean-geometry', 'graph-theory', 'combinatorial-geometry', 'combinatorics', 'discrete-mathematics']"
3743997,$f^{-1}(D-C)=f^{-1}(D)-f^{-1}(C)$,"Please can you give me feedback on this proof? Result: Let $f:A \rightarrow B$ be a function. Let $C$ , $D \subseteq B$ . Then $f^{-1}(D-C)=f^{-1}(D)-f^{-1}(C)$ . Proof: To show that $f^{-1}(D-C)=f^{-1}(D)-f^{-1}(C)$ , it is sufficient to show that the set in each side is a subset of the other. Let $x \in f^{-1}(D-C)$ . By definition, we see that $f(x) \in D-C$ . Hence, $f(x) \in D$ and $f(x) \notin C$ . We deduce that $x \in f^{-1}(D)$ and $x \notin f^{-1}(C)$ . Then $x \in f^{-1}(D) - f^{-1}(C)$ . Therefore $f^{-1}(D-C) \subseteq f^{-1}(D) - f^{-1}(C)$ . Now, let $y \in f^{-1}(D) - f^{-1}(C)$ . Then $y \in f^{-1}(D)$ and $y \notin f^{-1}(C)$ . By definition, we see that $f(y) \in D$ and $f(y) \notin C$ . From here we see that $f(y) \in D-C$ . Then, by definition, $y \in f^{-1}(D-C)$ . Therefore $f^{-1}(D)-f^{-1}(C) \subseteq f^{-1}(D-C)$ . This ends the proof. Thank you for your attention!","['elementary-set-theory', 'proof-writing', 'functions', 'solution-verification']"
3743998,"Defining rigorously, the intuition of Limits","This is an attempt to define rigorously from scratch, the following intuitive definition of limits: $\displaystyle{\lim_{x \to c}f(x)}=L$ means that $f(x)$ can be made arbitrarily close to $L$ by making $x$ be sufficiently close to $c$ . Given the above definition of limits, we can say that $\displaystyle{\lim_{x \to c}f(x)}=L$ means that for every $x \ne c$ in the domain of $f$ such that $f(x)\ne L$ there exists some $x_0$ in the domain of $f$ that is closer to $c$ than $x$ is and for which $f(x_0)$ is closer to $L$ than $f(x)$ is. Making $f(x)$ be arbitrarily close to $L$ : If for some $x$ you give me, I can give you some $x_0$ such that $f(x_0)$ is closer to $L$ than $f(x)$ is then we will say that $f(x)$ can be made closer to $L$ at that particular $x$ value.
If I can do this for every $x\ne c$ in the domain of $f$ such that $f(x)\ne L$ then we will say that $f(x)$ can be made arbitrarily close to L . The relationship between $x$ and its corresponding $x_0$ can be stated as follows: $$|f(x)-L|>|f(x_0)-L|; f(x)\ne0$$ Making $x$ be sufficiently close to $c$ : Now that we have made the first condition true, we need $x_0$ to be closer to $c$ than $x$ is. We don't want an $x_0$ that is farther away from $c$ than $x$ is. If $f(x_0)$ does get $f$ closer to $L$ but $x_0$ is farther away from $c$ than $x$ is and we can't find any $x_0$ which: Gets $f(x)$ closer to $L$ and Is itself closer to $c$ than $x$ is. Then we would say that $\displaystyle{\lim_{x \to c}f(x)}$ does not exist. The relationship between $x$ and $x_0$ can be stated as follows: $$|x-c|>|x_0-c|;x\ne c$$ Reason for specifying that $x\ne c$ and $f(x)\ne L$ : If $x$ is exactly at $c$ then it is impossible to make $x$ be any closer to $c$ than it already is. The distance between $x$ and $c$ is $0$ and it can't be made any smaller. Similarly, $f(x)$ can't get any closer to $L$ if $f(x)=L$ . Putting it all together: $\displaystyle{\lim_{x \to c}f(x)}=L$ means that for every $x\ne c$ in the domain of $f$ such that $f(x)\ne L$ , there exists an $x_0$ in the domain of $f$ such that: $$|x-c|>|x_0-c|$$ and $$|f(x)-L|>|f(x_0)-L|$$ Is my reasoning correct? If not, where did I go wrong?","['limits', 'epsilon-delta']"
3744039,"Confusion about the behavior of $p$-simplices under ""inversions"" (reversing ordering of indices)","I have found a very nice paper which generalizes much of vector calculus to discrete lattices through the use of simplicial complexes, https://journals.aps.org/pre/abstract/10.1103/PhysRevE.59.1217 alternative link (should be free to access): https://www.semanticscholar.org/paper/Vector-difference-calculus-for-physical-lattice-Schwalm-Moritz/76580b168d066976b0fa1317b57d1c234b8414f9 Following the authors, we define a 0-, 1-, 2-, and 3-fields according to $$\phi = \sum_{[i]} \phi_i \,(i)$$ $$\alpha = \sum_{[i,j]} \alpha_{ij} \, (i,j)$$ $$\beta = \sum_{[i,j,k]} \beta_{ijk} \, (i,j,k)$$ $$\gamma = \sum_{[i,j,k,l]} \gamma_{ijkl} \, (i,j,k,l)$$ where $(i_0, \cdots, i_p)$ is a $p$ -simplex. We take the usual convention that a simplex changes sign under an odd permutation of its indices, which in the authors' notation is written as $$(\mathcal{P}(i_0), \cdots \mathcal{P}(i_p)) = \mathrm{sgn}(\mathcal{P}) (i_0, \cdots, i_p)$$ If I understand correctly, each field should be invariant under reordering of the indices, and thus we conclude that each of the field ""tensor"" components should be fully anti-symmetric, ""The summations include each simplex only once. Thus the square bracket $[\cdots]$ denotes one representative of an equivalence class related by exchange of indices."" Now this is the part I am confused about, the authors state: Renumbering the vertex set of $\mathcal{G}$ [(the underlying graph)]
is a formal symmetry which cannot have physical consequences.
Reversing the vertex ordering changes the sign of $p$ -simplices with
odd $p$ , so it changes the sign of the components for $p$ -fields with $p=1$ and 3. Thus by analogy with the parities of continuum fields
with respect to coordinate inversion, we classify the basic $p$ -field
quantities as 0-fields (scalars), 1-fields (polar vectors), 2-fields
(axial vectors), and 3-fields (pseudoscalars). I am not able to figure out precisely what this means since they do not give examples. My understanding is this: Assume we label every vertex with an index $\{1,2,\cdots\}$ , then we can take for each representative of the equivalence classes the one in which the indices are in ascending order, i.e. we choose the representative 2-simplex of the equivalence class $[i_0,i_1,i_2]$ to be $(i_{n_0}, i_{n_1}, i_{n_2})$ where $i_{n_0} < i_{n_1} < i_{n_2}$ and $\{n_0,n_1,n_2\}$ is a permutation of $\{0,1,2\}$ (apologies, I realize this notation is rather cumbersome but I hope I made my point). Now, when the authors say ""reversing the vertex ordering"", I am not sure what they mean. If the graph is finite, and the list of labels ends, $\{0,1,\cdots, N\}$ , then I would assume they mean relabel each vertex $i \rightarrow N-i$ , but I cannot figure out how to recover their statement that this changes the sign of $p$ -simplices with odd $p$ . Every simplex now has an ordering which is descending rather than ascending, but then I find $$(1) \rightarrow (1) = +(1)$$ $$(1,2) \rightarrow (2,1) = -(1,2)$$ $$(1,2,3) \rightarrow (3,2,1) = -(1,2,3)$$ $$(1,2,3,4) \rightarrow (4,3,2,1) = +(1,2,3,4)$$ which is not the statement the authors make, I am expecting to find that the signs alternate as $+,-,+,-$ . I seem to have thoroughly confused myself, and am doubting that I really understand what they mean by reversing the ordering, how this is akin to spatial inversions in vector calculus, and generally think I am failing to grasp something here. Any help appreciated parsing the quoted statement.","['simplex', 'discrete-mathematics', 'general-topology', 'homology-cohomology', 'simplicial-complex']"
3744041,Understanding the concept of neighborhood basis,"The definition of basis and neighborhood basis are: Let $(X,\tau)$ be a topological space, a base of $\tau$ is a subset $\mathfrak{B}$ of $\tau$ such that each open set $A \in \tau$ is union of elements of $\mathfrak{B}$ If $p \in X$ , a subset $\mathfrak{B}_p\subseteq U_p=\{U \in \tau | p \in U\}$ of neighborboods of $  p$ is called a neighborhood basis of $ p$ if for each $U \in U_p$ there exist an $V\in \mathfrak{B}_p$ such that $V\subseteq U$ I have the following example in my lecture notes: If X is a set such that $|X|>\aleph_0$ and $\tau$ is the discrete topology on $X$ , now $(X,\tau)$ does not have a countable basis. In fact, let $\mathfrak{B}$ be a basis of $\tau$ : Because $\{p\} \in \tau$ for each $p \in X$ , the set $\{p\}$ must be union of elements of $\mathfrak{B}$ . Therefore $\{p\}\in \mathfrak{B}$ and $|\mathfrak{B}|\geq|X|>\aleph_0$ . Furthermore, each neighborhood of $p \in X$ contains the open set $\{p\}$ , so it is a finite   neighborhood basis of $p$ I don't understand quite well the concept of neighborhood basis so to understand the last paragraph, I came up with a concrete example: Let $X=\{1,2,3\}$ and $(X,\tau)$ a topological space with discrete topology. Then $\tau$ equals the power set $\tau=\{  \{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},X,\emptyset   \}$ If I choose $p=1$ the set of neighborhoods of $p$ are: $U_p= \{  \{1\},\{1,2\},\{1,3\},X   \}$ so that $\mathfrak{B}_p \subseteq U_p$ , $\mathfrak{B}_p=\{\{1\}\}$ is a neighborhood basis because it is contained in each element of $U_p$ in agreement with the definition of neighborhood basis and $\mathfrak{B} \subseteq \tau$ , $\mathfrak{B}=\{  \{1\},\{2\},\{3\} \}$ is a basis Is this example correct? and does it correctly explain why $\{p\}$ is a finite   neighborhood basis of $p$ in the initial example? Assuming this example is correct,
just like when having a basis, each element of the topology can be expressed as union of elements of the basis, I was expecting that when having a neighborhood basis, each element of the set of neighborhoods of a point $p$ could be expressed as union of elements of the neighborhood basis, but it looks like it is not the case, since with the basis $\mathfrak{B}_p=\{\{1\}\}$ , I can't express the elements $ \{1,2\},\{1,3\}$ and $X  $ of $U_p$ as union of elements of $\mathfrak{B}_p$ . How do I make sense of this?",['general-topology']
3744077,What is the optimal strategy of guessing a number where closest without going over wins?,"When a group of people need to decide a winner or leader between them, one approach would be that a random hidden integer is chosen with uniform distribution on $\{0, 1, ..., n\}$ and all $p$ participants publicly choose a number. Then, the number is revealed and the participant who was the closest wins. A variant of this happens when we introduce what is informally known as the 'price is right' rule, where you only win when you aren't going over (so the one who is the closest from the bottom wins). Now I am having trouble formalizing the optimal strategy for games like this in my head, and even more so for how the rules would change when the 'price is right' variant is introduced.","['game-theory', 'random', 'probability']"
3744124,$\lim_{n \to \infty} x_n = 0$ and $\lim_{n \to \infty} f(x_n) = a$,"Let $f(x)$ be a continuous function defined on $(0,1]$ . Define a  subset $I \subseteq \Bbb R$ as follows: $a\in I$ if and only if there exists a sequence in the interval $(0,1]$ such that $\lim_{n \to \infty} x_n =0$ and $\lim_{n \to \infty} f(x_n) = a.$ I want to show that in general, if $I$ is nonempty, then it is a connected closed set. When $f(x)$ is $\sin1/x$ , I think $I=[-1,1]$ by seeing graph. (I was taught this by Elliot G.). More precisely,by taking $x_n＝1/2nπ ,y_n＝1/（２nπ＋1/2）$ ,we can check $1,-1∈I$ .In the same way, for arcitrary a∈[-1,1],we just need to let $x_n＝1/（２πn＋arcsina）$ .
But in general, I cannot show $I$ is connected closed set in real line.
According to Michael, we just need to show $ let a,b∈I,a＜b,c∈（a,b）,
then, c∈I$ . I understand that, but I don't know how to apply intermediate value theorem in this situation. Thank you.","['general-topology', 'sequences-and-series']"
3744148,expected number of turns of the game,"A board game is played on a hexagonal grid of 19 tiles. A 'traveler' token starts on the center tile. Each turn a die is rolled to determine what neighboring tile the traveler moves to (all six directions equally likely). The turn that the traveler leaves the board, the game ends. What is the expected number of turns of the game? I have tried making a diagram for the problem ..but not sure if this is correct!!","['markov-chains', 'discrete-mathematics', 'probability']"
3744165,Strong Markov property on shifted Brownian Motion,"This is a Corollary from Rene Schilling's Brownian Motion. The proof below simply states that this is a corollary from the below equation (6.8) and the fact that $u(B_\tau)$ and $u(W_{\tau'} + B_\sigma)$ have the same distribution. So we have $E[u(B_\tau) |\mathscr{F}_{\sigma+}](\omega) = E[u(W_{\tau'}+B_\sigma)|\mathscr{F}_{\sigma+}]$ . And we need to show that this is equal to $E[u(W_{\tau'}+x)]|_{x=B_\sigma(\omega)}$ using (6.8). However, $u$ here is a bounded measurable Borel function on $\mathbb{R}^d$ , and $\Psi$ in (6.8) is a bounded $\mathscr{B}(\mathcal{C})/\mathscr{B}(\mathbb{R})$ measurable function. So I can't see how to define the $\Psi$ in (6.8) to get the first identity in (6.9). How do we get the proof for this Corollary rigorously? I would greatly appreciate any help.","['stochastic-processes', 'markov-process', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3744206,Local Characterization of Hodge Star Operator on a Vector Bundle,"I am reading the proof of Nakano's Identity in Complex Geometry by Huybrechts. This is Lemma 5.2.3, for reference. If $\mathcal{A}_X^{p,q}$ denotes the sheaf of smooth $(p,q)-$ forms on a compact Kähler manifold $X$ , then the Hodge star operator $$ \star:\mathcal{A}^{p,q}_X\to\mathcal{A}_X^{n-p,n-q}$$ is defined using the pairing $\mathcal{A}^{p,q}_X\otimes \mathcal{A}^{n-p,n-q}_X\to \Bbb{C}$ coming from pairing to the volume form and integrating. If $\pi:E\to X$ is a Hermitian holomorphic vector bundle, we can define a new Hodge star $\star_E$ similarly by $$ \star_E:\mathcal{A}_X^{p,q}(E)\to \mathcal{A}_X^{n-p,n-q}(E^*)$$ by sending $\omega\otimes s\mapsto \star\omega\otimes s^*$ where $s^*$ is defined using the Hermitian metric. Anyway, in Huybrechts' book - it is claimed that with respect to an orthonormal local trivialization of $E$ on an open $U$ , $\star$ agrees with $\star_E$ . I don't really see why this is true. Indeed, locally $E|_U\cong U\times \Bbb{C}^r$ so that a form in $\mathcal{A}^{p,q}_X(E|_U)$ is a vector valued differential form, rather than an ""ordinary"" differential form. Does he mean that $\star_E$ acts componentwise by $\star$ ? I am inclined to believe this, but where is the orthonormality assumption used here?","['hodge-theory', 'kahler-manifolds', 'complex-geometry', 'complex-analysis', 'algebraic-geometry']"
3744227,"Find maximum point of $f(x,y,z) = 8x^2 +4yz -16z +600$ with one restriction","I need to find the critical points of $$f(x,y,z) = 8x^2 +4yz -16z +600$$ restricted by $4x^2+y^2+4z^2=16$ . I constructed the lagrangian function $$L(x, y, z, \lambda ) = 8x^2 +4yz -16z +600 - \lambda (4x^2+y^2+4z^2-16) $$ but I'm very confused about how to determine those points. I know I need to make a system with all the first derivatives of $L$ equaled to $0$ . I did it but every time I try to solve it I get different solutions. How can I get the points? Thanks.","['nonlinear-optimization', 'lagrange-multiplier', 'multivariable-calculus', 'calculus', 'optimization']"
3744231,"Is the map $\mathbb{Z}\times (\mathbb{Z}_{>0})\to \mathbb{Q}$, given by $(m,n)\mapsto \frac{m}{n}$ injective, surjective or bijective?","Is the following mapping $$\mathbb{Z}\times (\mathbb{Z}_{>0})\to \mathbb{Q},(m,n)\mapsto \frac{m}{n}$$ injective, surjective or bijective? I have been working on this problem for a couple of hours and I think it is surjective, and here is my proof: $\cfrac{m}{n}$ multiply the numerator and the denominator
by any positive integer is the same value
despite the fact that there are different numbers. for instance: $\cfrac{4}{5}$ is the same as $\cfrac{8}{10}$ and $\cfrac{12}{15}$ But I think this might be a little weak and might be wrong, and I would be much grateful if anyone is able to point out the correct answer and suitable explanation for me, thank you",['discrete-mathematics']
3744279,How to determine the domain and range of the following function?,"I have the following statement: Determine the domain and range of $\large{f(x) = \frac{x}{x^2 - 1}}$ The domain are allowed input values, in this case the function is undeterminated in reals when $x \in \{-1, 1\}$ hence the domain is $\mathbb{R} - \{-1, 1\}.$ But, get the range is harder to me. My attempt was: Let $f(x) = y$ , that is $y = \frac{x}{x^2 -1} \iff yx^2 -x-y=0$ In the case that $y = 0$ i have: $-x = 0 \iff x = 0$ and since $x \in Dom_f \to y \in Rec_f$ . In other case, $ y\neq 0$ i have: $\large{x = \frac{1 \pm \sqrt{1+4y^2} }{2y}}$ and from here i need to get $\frac{1 \pm \sqrt{1+4y^2} }{2y} \in \mathbb{R} - \{-1, 1\}$ . so there shouldn't be a $y$ related to $x = \pm 1$ . here i don't know how to continue. Any help is really appreciated.","['algebra-precalculus', 'functions', 'rational-functions']"
3744281,Q: Proving G is d-colorable,"I'm studying graph theory, coloring at the moment. I'm stuck with a proof of the following exercise: Let $G = (V, E)$ be a connected graph and let $v \in V$ be such that $deg(v)\lt d$ . If $deg(w)\le d$ for all $w \in V-{v}$ , then $G$ is $d$ -colorable. I have an idea that I could use Brooks' Theorem but I'm not really sure. I'd appreciate it if someone could give me a hand here","['graph-theory', 'coloring', 'discrete-mathematics']"
3744330,Evaluating $\int _0^1\frac{\ln \left(x^2+x+1\right)}{x\left(x+1\right)}\:dx$,"How can i evaluate this integral, maybe differentiation under the integral sign? i started expressing the integral as the following, $$\int _0^1\frac{\ln \left(x^2+x+1\right)}{x\left(x+1\right)}\:dx=\int _0^1\frac{\ln \left(x^2+x+1\right)}{x}\:dx-\int _0^1\frac{\ln \left(x^2+x+1\right)}{x+1}\:dx\:$$ But i dont know how to keep going, ill appreciate any solutions or hints.","['integration', 'improper-integrals', 'definite-integrals', 'real-analysis', 'calculus']"
3744355,How many times must a bounded random number generator bounded by its output run until it outputs 1?,"Say you have a function with input $x$ that generates a random integer between $1$ and $x$ . Every time you run the function you replace its input with its output until the function outputs $1$ . What is the expected number of times you will have to run the function? For example, if $x$ is $1000$ . You run $f(1000)$ and it gives you, say $500$ . Then you run $f(500)$ and it gives you $1$ . So it took you $2$ runs to roll a $1$ , I was wondering what the expected amounts of runs would be for $x$ . I initially thought it would just be $\log_2x$ because you are expected to halve the value on each run, but that clearly doesn't work for $x=2$ because you're expected to roll twice to get $1$ .
Maybe that's just because its a base case so I wrote this on python to check for 1000 - y = 0
for j in range(100000):
    i = 1000
    z = 0
    while i>1:
        i = randrange(1, i+1)
        z+=1
    y += z
y = y/100000
print(y) and it gives approximately ~8.5 rolls. Any ideas?","['expected-value', 'combinatorics']"
3744439,Why $\arctan x$ not equal to $\arcsin(x)/\arccos(x)$?,Why $\arctan x$ not equal to $\frac{\arcsin(x)}{\arccos(x)}$ ? Is there a counter example that I can use to show that they are not equal? Thank!,"['calculus', 'inverse-function', 'trigonometry']"
3744440,Coordinate Transformation and Derivatives,"I'm currently working on an assignment where I end up with the following differential equation in cylindrical coordinates: $$ 0 = \frac{\partial}{\partial r}\left(r \frac{\partial w_\varphi}{\partial r}\right), $$ where $r$ is the radius and $w_\varphi$ describes the angular velocity of a stirred liquid. If I assume $r$ to be a ""regular"" variable and I apply the chain rule I get the following equation: $$ 0 = \frac{\partial^2 w}{\partial r^2}r + \frac{\partial w}{\partial r} \tag{1} $$ The last task of the assignment is to compare the results of this equation with the results obtained from simplifying the Navier-Stokes equation (basically the holy grail of fluid mechanics) and to explain possible differences. The equation I obtain from the Navier-Stokes equation looks like this: $$ 0 = \frac{\partial^2w}{\partial r^2} r + \frac{\partial w}{\partial r} - \frac{w}{r} \tag{2} $$ so I get an extra $-\frac{w}{r}$ out of it. I'm fairly certain that both equations are correct, or at least the intended outcome of their task, because my professor said that everything was looking good up until now. The question is of course where does this extra $-\frac{w}{r}$ come from? My professor gave me the hint of looking at the $r$ variable more closely and to try and think of how  it would change in carthesian coordinates. Or to look at it in a vector form, so using $\vec{r} = \begin{pmatrix} r \> cos(\varphi)\\r \> sin(\varphi)\end{pmatrix}$ instead of just $r$ to account for the moving unit vectors in cylindrical coordinates. To be honest my courses in calculus happened quite a while ago and I forgot a good portion of them so I'm a bit confused by this problem and don't really know where to begin. If anyone has an idea on how to solve or approach this problem I would be very grateful. EDIT: Upon further research I found a chapter out of an ODE Textbook in which it seems the same or at least a similar problem is posed as an exercise. Unfortunately I couldn't find any solutions for the exercises of the book, but maybe it can shed some light into what my question is. https://faculty.missouri.edu/~chiconec/pdf/water1.pdf The exercise in question is 6.7","['cylindrical-coordinates', 'derivatives', 'fluid-dynamics']"
3744493,What are the equations that describe a Gömböc?,"By 'Gömböc', I am referring to the family of shapes pictured on the Wikipedia page S.V. 'Gömböc', https://en.wikipedia.org/wiki/Gömböc In their paper ""Static equilibria of rigid bodies: dice, pebbles and the Poincaré-Hopf Theorem"", Domokos and Várkonyi define a family of shapes. They prove that some shapes in the family are convex and mono-monostatic. However, the shapes need to be indistinguishably close to a sphere in order to be convex, and the Gömböc pictured on Wikipedia is not in this family of shapes. https://link.springer.com/content/pdf/10.1007/s00332-005-0691-8.pdf In another paper entitled ""Mono-monostatic Bodies: The Answer to Arnold’s Question"", the same authors repeat the same text and equations almost verbatum, excepting some obvious errors, e.g. in Eq 1 (parenthesis) and Eq 5 (negative phi in numerator). At the end of this paper, they present a picture of a Wikipedia-style Gömböc and declare that it is a mono-monostatic body of a different class than what is analyzed in the paper. However, they do not provide further references or elaboration about how the shape is defined, or how they proved it is mono-monostatic. http://www.gomboc.eu/docs/100.pdf I further searched Domokos's publication list on Google Scholar and did not find any further technical description of the Gömböc. https://scholar.google.com/citations?sortby=pubdate&hl=en&user=jvULYuYAAAAJ&view_op=list_works So my question is three-fold: What equations describe the shape of the Gömböc, Where is the mathematical proof that it is mono-monostatic, and Is there some funny-business going on here?","['solid-geometry', 'physics', 'geometry']"
3744499,When do quadratically integrable functions vanish at infinity?,"In quantum mechanics we use quadratically integrable functions ( $\psi \in L^2$ ).
This means $$ \int_{-\infty}^\infty |\psi(x)|^2 \mathrm{d}x < \infty. $$ I'm interested in the question when those function vanish at infinity, i.e. $$ \lim_{x \rightarrow \pm \infty} \psi(x) = 0. $$ I know that this is not the case for every function in $L^2$ , see for example this answer or this answer . I found in a similar question something interesting : Suppose $f : \mathbf R \to \mathbf R$ is uniformly continuous, and $f\in L^p$ for some $p\geq 1$ . Then $|f(x)|\to 0$ as $|x| \to \infty$ . Another interesting answer is this one . My questions are: How can one prove the given statement? What are other cases where quadratically integrable functions vanish at infinity? Which cases are relevant in physics (for quantum mechanics)? Edit: My first question was answered in the comments by @reuns. My remaining question is: What criteria (beside uniform continuity) do exist, so that quadratically integrable functions vanish (or not) at infinity?","['integration', 'functions', 'functional-analysis', 'quantum-mechanics']"
3744510,"Smallest $c$ such that $f'<cf$ holds for all $f$ such that $f,f',f'',f'''>0$ and $f''' \le f.$","Let $f: \mathbb{R} \to \mathbb{R}$ be a $C^3$ function such that $f,f',f'',f'''>0$ and $f''' \le f.$ What is the smallest $c$ such that we can guarantee $f'<cf$ ? Since $f(x)=e^x$ works, we must have $c>1.$ On the other hand, I managed to show $c = 1.5^{1/3}$ works. Proof: First, we note $\lim\limits_{x \to -\infty} f'(x) = \lim\limits_{x \to -\infty} f''(x) = 0.$ Now $f''' \le f \Rightarrow (f''^2)' = 2f''f''' \le 2f''f < 2f''f + 2f'^2 = (2ff')',$ so integrating yields $(f''^2)(x) - (f''^2)(x_0) < (2ff')(x)-2ff'(x_0)$ for any $x,x_0.$ Taking $x_0 \to -\infty$ gives us $f''^2 < 2ff'.$ We use $f''' \le f$ again, but multiply by $f'$ instead: $(f'f'')' = f''^2 + f'f''' \le f''^2 + f'f < 3ff' = (1.5f^2)'.$ Integrating, we get $(f'f'')(x) - (f'f'')(x_0) < (1.5f^2)(x) - (1.5f^2)(x_0) < (1.5f^2)(x).$ Take $x_0 \to -\infty$ again to get $f'f'' < 1.5f^2 \Rightarrow (\frac{1}{3}f'^3)' = f'^2f'' < 1.5f^2f' = (0.5f^3)'.$ Integrate and take $x_0 \to -\infty$ for the last time to get $\frac{1}{3}f'^3 < 0.5f^3 \Rightarrow f' < 1.5^{1/3} f.$","['inequality', 'functional-inequalities', 'derivatives']"
3744539,A question of convergence in Hilbert spaces,"I am stuck on a question of convergence, and I am not sure it is true. Suppose $(x_n)$ is a sequence of vectors in a separable Hilbert space and $T_n$ is a sequence of bounded operators such that $||T_n||<K$ for all $n$ . If $x_n\to x\neq 0$ , $T_n x_n\to y\neq 0$ (both convergence in norm), and $T_n\stackrel{WOT}{\to} T\neq 0$ (convergence in weak operator topology), does it follow that $Tx=y$ ? Does it make a difference if we require $T_n$ to converge in operator norm or SOT?","['hilbert-spaces', 'banach-spaces', 'operator-theory', 'functional-analysis']"
3744560,Deriving a function based on its properties,"Suppose I have a function $\Lambda(t)$ for any $t>0$ . This function has the following three properties: $\Lambda(t)$ is differentiable. $\Lambda(t)$ is strictly increasing. $\Lambda(T) = \Lambda(T+S) - \Lambda(S)$ for any $T,S>0$ . It is stated that the function has the form $\Lambda(t) = \lambda t$ , but how can I formally derive this from the above three properties. Thanks in advance.",['functions']
3744582,Two permutation matrices represent conjugate permutations iff they have same characteristic polynomial.,"I was told that Two permutation matrices represent conjugate permutations iff they have same characteristic polynomial (where the conjugacy is considered only in $S_{n}$ ). The first implication is clear to me i.e. permutation matrices representing conjugate permutations, being similar matrices, have same characteristic polynomial.
But, I do not understand why is it necessary that if two permutation matrices have same characteristic polynomial they should represent conjugate permutation? I was told to see newton's identities. But I do not see anything relating characteristic polynomial with permutation matrices in them. Anyhow, I did it and I have written answer. But, I would really like to know a whether there exists a proof that uses newtons identities or algaebraic manipulations.","['permutation-matrices', 'abstract-algebra', 'linear-algebra', 'characteristic-polynomial', 'group-theory']"
3744587,What is the concrete meaning of fixing an extension field through a subgroup of automorphisms in $x^3-2$?,"The automorphisms corresponding to the extension fields of the splitting polynomials of $x^3-2$ are enumerated in this answer : we know that the automorphism of $\mathbb{Q}(\sqrt[3]{2}, \omega_3)$ , which fix $\mathbb{Q}$ are determined by the action on $\sqrt[3]{2}$ and $\omega_3$ , where $\omega_3$ is a third root of unity. It's trivial to conclude that such an automorphism sends $\sqrt[3]{2}$ to a root of $x^3 - 2$ and $\omega_3$ to a root of $x^2 + x + 1$ . Making all possible combinations we get: $$ e :
     \begin{array}{lr}
       \sqrt[3]{2} \mapsto \sqrt[3]{2}\\
       \omega_3 \mapsto \omega_3
     \end{array}
\quad \quad  r:
     \begin{array}{lr}
       \sqrt[3]{2} \mapsto \omega_3\sqrt[3]{2}\\
       \omega_3 \mapsto \omega_3
     \end{array}
\quad \quad   r^2:
     \begin{array}{lr}
       \sqrt[3]{2} \mapsto \omega_3^2\sqrt[3]{2}\\
       \omega_3 \mapsto \omega_3
     \end{array}
$$ $$ f :
     \begin{array}{lr}
       \sqrt[3]{2} \mapsto \sqrt[3]{2}\\
       \omega_3 \mapsto \omega_3^2
     \end{array}
\quad \quad  fr:
     \begin{array}{lr}
       \sqrt[3]{2} \mapsto \omega_3^2\sqrt[3]{2}\\
       \omega_3 \mapsto \omega_3^2
     \end{array}
\quad \quad  fr^2:
     \begin{array}{lr}
       \sqrt[3]{2} \mapsto \omega_3\sqrt[3]{2}\\
       \omega_3 \mapsto \omega_3^2
     \end{array}
$$ These are shown as symmetries in the dihedral group $D3$ ( $\zeta,$ in place of $\omega,$ and $zeta^2$ on either side of the face of the triangle; $2^{1/3}$ elements at the vertices): Now by checking which elements of the basis remain fixed by a subgroup of $ G,$ you can determine the corresponding fixed fields. I understand that (composition from R to L): $r$ fixes $\omega$ and $\omega^2.$ $f$ fixes $2^{1/3}.$ $fr$ fixes $2^{1/3}(1+\omega).$ $fr^2$ fixes $2^{2/3}(1+\omega).$ But I am still not sure what ""fixing"" means - How can you check that these automorphisms ""fix"" these subfields? It would be possibly enlightening to see how $fr$ fixes $2^{1/3}(1+\omega),$ and $fr^2$ fixes $2^{2/3}(1+\omega).$ I don't quite see without a concrete example what is meant by ""positions"" for example, although I understand it has to do with permutations. Notes in relation to the accepted answer: The expression of the fixed elements above, which was extracted from here, was reformulated using the minimal polynomial $x^2 + x + 1,$ which for a root of it, $\omega,$ obeys $\omega^2 + \omega + 1 =0;$ and hence, $\omega + 1 = -\omega^2.$ Therefore, $2^{1/3}(1+\omega)=-2^{1/3}\omega^2.$ the automorphisms detailed above can be summarized into: $$\begin{align}
&r: 2^{1/3} \mapsto \omega 2^{1/3}\\
&r:  \omega \mapsto \omega
\end{align}
$$ $$\begin{align}
f: &2^{1/3} \mapsto 2^{1/3}\\
f: & \omega \mapsto \omega^2
\end{align}
$$ As shown in the answer, there is a mistake in the list of fixed points above. The correct correspondence is: $$\begin{align}
&\omega 2^{1/3}  \text{ is a fixed point of } fr.\\
&\omega^2 2^{1/3}  \text{ is a fixed point of } fr^2.
\end{align}$$ Finally a reminder of of the rules to apply automorphisms to the elements of the basis following the rules: $$\begin{align}
\phi(a+b)&=\phi(a)+\phi(b)\\
\phi(ab)&=\phi(a)\phi(b)\\
\phi(z)&=z, \forall z\in F\text{ base field}
\end{align}$$ So if we look into the rotations of $\omega_3,$ $$\begin{align}
r(\omega_3)&=\omega_3\\
r^2(\omega_3)&=\omega_3
\end{align}$$ by definition of the automorphism. But also, $$\begin{align}
r(\omega_3^2)&=r(\omega_3)r(\omega_3)=\omega_3\omega_3=\omega_3^2\\
r^2(\omega_3^2)&=r(r(\omega_3^2))=r(\omega_3^2)=\omega_3^2
\end{align}$$ Hence, $r$ fixes $\omega_3$ and $\omega_3^2.$ This corresponds to $\mathbb Q(\omega_3).$","['galois-theory', 'group-theory', 'abstract-algebra', 'extension-field']"
3744627,Trace of map & matrices,"Let $1\leq m,n\in \mathbb{N}$ and let $\mathbb{K}$ be a field.
For $a\in M_m(\mathbb{K})$ we consider the map $\mu_a$ that is defined by $$\mu_a: \mathbb{K}^{m\times n}\rightarrow \mathbb{K}^{m\times n}, \ c\mapsto ac$$ I want to show that $trace(\mu_a)=n\cdot trace(a)$ . I have done the following: Let $\lambda$ be the eigenvalues of $\mu_a$ then we have that $\mu_a(c)=\lambda c$ . From $\mu_a(c)=\lambda c$ we get $ac=\lambda c$ . So if $\lambda$ is an eigenvalue of $\mu_a$ , tthere is a non-zero $c\in\mathbb{K}^{m\times n}$ with $\mu_a(c)=\lambda c$ . The columns of $c$ are all eigenvectors of $a$ with eigenvalue $\lambda$ . The matrix $c$ has $n$ columns. So for each eigenvalue $\lambda$ of $a$ there are $n$ eigenvectors, so the multiplicity of $\lambda$ is $n$ . The trace of a matrix is the sum of teh eigenvalues considering the multiplicity. Since each eigenvalue of $\mu_a$ has a multiplicity of $n$ , it follows that $\text{trace}(\mu_a)=\sum_i n\cdot \lambda_i=n\cdot \sum_i\lambda $ . Since $\lambda_i$ is the eigenvalue of $a$ , it follows that $\text{trace}(a)=\sum_i\lambda_i$ . Therefore we get $\text{trace}(\mu_a)=n\cdot \text{trace}(a)$ . Is everything correct?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3744702,Scores of black and white marks,"We have a connected (undirected) graph with $n$ vertices, and $b$ black and $w$ white marks with $b+w=n$ and $\max(b,w)\geq 2$ . In a placement of the marks on the vertices, the ""score"" of a mark is the ratio of its neighbors of the same color to its total neighbors (this ratio is always well-defined due to connectivity). A placement is called ""nice"" if it is not possible to rearrange the marks so that every mark gets a better or same score as before, and at least one mark gets a better score. Is it true that in any nice placement, the sum of the scores is at least $1$ ? The score may be $1$ , if the graph is a triangle and $b=2,w=1$ .","['graph-theory', 'combinatorics']"
3744751,How do I evaluate t$\lim_{n\to\infty} \sum_{i=1}^n \left[\sqrt{1+ \frac{2i}{n}}\right]\frac{2}{n}$? (From MIT OCW 18.01 sc final Q7(a)),"This was one of the questions on the final for MIT's 18.01: $$\lim_{n\to\infty} \sum_{i=1}^n \left[\sqrt{1+ \frac{2i}{n}}\right]\frac{2}{n}$$ The answer converts it to an integral, but I'm not sure how they made that logical step. Is this using L'Hopital's rule somehow?","['limits', 'calculus']"
3744784,Why sine and cosine are orthogonal over continuum and over discrete set?,"Consider functions of $x$ : $$
1,\quad \sin \frac{\pi n}{N} x, \quad \cos \frac{\pi n}{N} x, \quad \cos \pi x
$$ where $n$ runs $1,\,2,\, \ldots,\,N - 1$ Those functions are orthogonal if the inner product is defined as $$
\int_0^{2 N} f(x) g(x) \, dx
$$ but also if it is $$
\sum_{x = 0}^{2 N - 1} f(x)g(x)
$$ What is special about the sine and the cosine? Do other sets of functions have a similar property. As @badjohn suggested I experimented with four triangular functions: The integrals and the sums are zero. The sums are zero also with a shift ( $0 \le \delta < 1$ , $N = 2$ ): $$
\sum_{x = 0}^{2 N - 1} f(x + \delta)g(x + \delta)
$$","['integration', 'fourier-analysis', 'orthogonality', 'fourier-series', 'trigonometry']"
3744801,Error analysis by differentiation,"I've been studying physics and I found this weird differentiation. $\ln x = \ln a  + \ln b$ Now differentiating both sides, $\dfrac{dx}x = \dfrac{da}a + \dfrac{db}b$ First of all this weird differentiation doesn't make sense to me. I understand that $(\ln x)'$ will be $\frac1x$ and in no way $\frac{dx}x$ . So I asked a person about it and they replied with this: Basically, they told me that we have differentiated both sides wrt $x$ . Now according to me, differentiating R.H.S. i.e. $\ln a$ wrt x should yield $0$ . But according to them, it is $\dfrac{d(\ln a)}{dx} = \dfrac{da}{adx}$ I don't get it!","['ordinary-differential-equations', 'partial-differential-equations']"
3744926,matrix inner product between positive semidefinite matrix and positive definite matrix,"Let $F_0, F_1, \ldots, F_m$ be a $n \times n$ symmetric matrices. We define $$F(x) := F_0 + x_1 F_1 + \cdots + x_m F_m$$ Show that if there does not exist $x \in \Bbb R^m$ such that $F(x)$ is positive definite, then there does exist a positive semidefinite matrix $H \neq 0$ such that $$\sup_x \mbox{tr} \left( F(x) H \right) \leq 0$$ Can you solve it? Contraposition may be useful, but I have no idea.","['linear-matrix-inequality', 'semidefinite-programming', 'trace', 'matrices', 'linear-algebra']"
3744940,General method of evaluating $\small\sum_{n\geq 0}\left(\frac{4^n}{(2n+1)\binom{2n}{n}}\right)^2\frac{1}{n+k}$,"Question: $
\mbox{How can we evaluate}\quad
\sum_{n \geq 0}\left[{4^{n} \over \left(\, 2n + 1\,\right)
\binom{2n}{n}}\right]^{2}{1 \over n + k}\quad
\mbox{for general $k$ ?.}
$ General methodology will be enough, but a closed-form is more preferable (if exists). Note that the previous problem, i.e. expressing binomial series in terms of MZVs, is solved via an alternative method (by user @pisco ), so I simplified the question. For his method see here .","['integration', 'legendre-polynomials', 'harmonic-numbers', 'sequences-and-series', 'hypergeometric-function']"
3744960,Evaluate $\lim\limits_{n \to \infty}\frac{(-1)^n}{n!}\int_0^n (x-1)(x-2)\cdots(x-n){\rm d}x$,"Evaluate $$\lim\limits_{n \to \infty}\frac{(-1)^n}{n!}\int_0^n (x-1)(x-2)\cdots(x-n){\rm d}x\,.$$ \begin{align*} I_n:&=\frac{(-1)^n}{n!}\int_0^n\prod_{k=1}^{n}(x-k){\rm d}x=\frac{(-1)^n}{n!}\int_0^n\prod_{k=1}^{n}(n-k-x){\rm d}x\\&=\frac{(-1)^n}{n!}\int_0^n\prod_{k=0}^{n-1}(k-x){\rm d}x=\frac{1}{n!}\int_0^n\prod_{k=0}^{n-1}(x-k){\rm d}x\\ &=\int_0^n\binom{x}{n}{\rm d}x, \end{align*} but it's hard to go forward.","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'limits']"
3744992,Find the properties of an ellipse from 5 points in 3D space,"Problem I'd like to write code that solves the following problem: Take 5 arbitrary points in Cartesian coordinates $(x,y,z)$ . Check whether there is an ellipse that goes through them all (with some tolerance for floating-point inaccuracies) . If so, find the ellipse's center $\mathbf{c}$ , major radius $a$ (length of the semi-major axis), minor radius $b$ (length of the semi-minor axis). Canonical approach Similar discussions tend to already start with five points in 2D space. Extending that to 3D points, I suppose the ""canonical"" approach would look like this: Check that all five points are coplanar, and determine that plane. Convert the five 3D points to 2D points on that plane. Use the five $(x,y)$ points to solve the conic equation $$ax^2+by^2+cxy+dx+ey+f=0$$ for the coefficients $a, b, c, d, e, f$ using some algorithm for solving linear system of equations (hopefully stable with floating-point numbers) . Check the coefficients to make sure they represent an ellipse, and not another type of conic. Calculate the ellipse properties from the coefficients ( formulae ) . Convert the ellipse center $\mathbf{c}$ back to 3D space. Is there a shortcut? The above approach seems cumbersome to implement, and possibly inefficient at run-time. So, I'm wondering if there's a better way to do this in my case — where input and output is in 3D space, and I'm not actually interested in the full ellipse equation, just the three ellipse properties mentioned above. I'm holding out hope, because for the simpler but conceptually similar problem of ""finding the circle through three 3D points"" , this Wikipedia section provides a closed-formula solution with just a few dot products and cross products. Any ideas?","['conic-sections', 'geometry']"
3745029,Proving consistency for an estimator. Limits and Convergence in Probability.,"I need to show that $U$ , as defined below, is a consistent estimator for $\mu^{2}$ . $U=\bar{Y}^{2}-\frac{1}{n}$ By the continuous mapping theorem, which states that, $X_{n} \stackrel{\mathrm{P}}{\rightarrow} X \Rightarrow g\left(X_{n}\right) \stackrel{\mathrm{P}}{\rightarrow} g(X)$ Then, $\bar{Y} \stackrel{P}{\longrightarrow} \mu $ gives me $\bar{Y}^{2} \stackrel{P}{\longrightarrow} \mu^{2} .$ And since $\frac{1}{n} \rightarrow 0$ as $n \rightarrow \infty$ the result for conistency seems intuitively obvious. But I have a confusion with how to show this formally, whether using only the mapping theorem, or if I need something else.  Showing how the $\frac{1}{n} \rightarrow 0$ part leads to consistency is the part that I'm missing, since this is a standard limit and not a convergence in probability. Any help in completing this is greatly appreciated.","['statistical-inference', 'statistics', 'real-analysis', 'convergence-divergence', 'probability-theory']"
3745034,When does a bounded continuous function extend continuously to its closure,Let $\Omega$ be a domain in $\mathbb{C}^n$ . Let $f:\Omega\longrightarrow\mathbb{C}$ be a bounded continuous function. I wanted know if there are any necessary and sufficient conditions for $f$ to be extended continuously to $\bar{\Omega}$ ?,"['complex-analysis', 'continuity', 'general-topology']"
3745060,Problematic solution related to finding the function range,"The Question: Find the range of the following function $$y=\dfrac{x}{2}+\dfrac{8}{x}$$ Solution $-1.$ (the solution given to me) By Cauchy inequality, $$\dfrac{x}{2}+\dfrac{8}{x}≥2\sqrt{ \dfrac{x}{2}× \dfrac{8}{x}}=4$$ where $x>0$ $$\dfrac{x}{2}+\dfrac{8}{x}≤-2\sqrt{ \dfrac{x}{2}× \dfrac{8}{x}}=-4$$ where $x<0$ which implies $y \in(-\infty, -4] ∪ [4, +\infty).$ But, as far as I know, we don't define inequality of arithmetic and geometric means for negative numbers.For this reason, I strange this mathematical way. $$\dfrac{x}{2}+\dfrac{8}{x}≤-2\sqrt{ \dfrac{x}{2}× \dfrac{8}{x}}=-4$$ where $x<0.$ Okay, If our equation were equal to a $$y=\dfrac{x-2}{2}+\dfrac{8}{2-x}$$ or $$ y=\dfrac{x}{2}+\dfrac{8}{|x|}$$ then we can not apply, $$y=\dfrac{x-2}{2}+\dfrac{8}{2-x} \leq -2\sqrt{ \dfrac{x-2}{2}× \dfrac{8}{2-x}} \in {\emptyset}.$$ $$ y=\dfrac{x}{2}+\dfrac{8}{|x|} ≤-2\sqrt{ \dfrac{x}{2} × \dfrac{8}{|x|}} \in {\emptyset}.$$ where $x<0$ . What I mean, For $x<0$ ,the arithmetic meaning of $\dfrac{\dfrac{x}{2}+\dfrac{8}{x}}2$ doesn't exist and the geometric meaning of $\sqrt{ \dfrac{x}{2}× \dfrac{8}{x}}$ doesn't exist. So, for $x<0$ to write the $\sqrt{ \dfrac{x}{2}× \dfrac{8}{x}}$ ,  I think, it doesn't make sense. I would go on like this. $$y=\dfrac{x}{2}+\dfrac{8}{x}≥2\sqrt{ \dfrac{x}{2}× \dfrac{8}{x}}=4$$ where $x>0$ Then, for $x<0$ we have both $\dfrac{x}{2}$ and $\dfrac{8}{x}$ are negative. In this sense we can write $$\dfrac{x}{2}+\dfrac{8}{x}≥2\sqrt{ \dfrac{x}{2}× \dfrac{8}{x}}=4$$ $$-\left(\dfrac{x}{2}+\dfrac{8}{x}\right)\leq-4$$ $$- \dfrac{x}{2}+\left(-\dfrac{8}{x}\right) \leq-4$$ where $x>0$ . It seems more sense to me. So we get, $y \in(-\infty, -4] ∪ [4, +\infty)$ I don't know how right I am. My solution: $$\begin{align} y=\dfrac{x}{2}+\dfrac{8}{x} \Longrightarrow 2yx=x^2+16 \Longrightarrow x^2-2yx+16=0 \Longrightarrow \Delta=y^2-16 \geq0 \Longrightarrow y \in(-\infty, -4] ∪ [4, +\infty). \end{align}$$ Question $-1$ :Do you find the solution $-1$ perfect? Question $-2$ :Is my own solution correct? Remark. If our function were as follows, we could easily apply the arithmetic-geometric inequality. $$y=\dfrac{x^2}{2}+\dfrac{8}{x^2}$$ $$y=\dfrac{x^2}{2}+\dfrac{8}{x^2}≥2\sqrt{ \dfrac{x^2}{2}× \dfrac{8}{x^2}}=4$$ $$y \in [4, +\infty)$$","['algebra-precalculus', 'functions', 'solution-verification']"
3745118,Rendering a Homomorphism Injective,"Over a year ago now I asked a bad question . Recently I was reminded of this question, though in the year of growth I've had as a mathematician and MSE user, I have gained the language to make precise what was once a glimmer of an idea. I apologize for reposting, but I think this is sufficiently changed to warrant an entirely new question. Given a group hom $\varphi : G \to H$ , we can look at its mono-epi factorization $\varphi = \iota \pi$ : Here $N = \text{Ker }\varphi$ is the obstruction to $\varphi$ 's injectivity, and it is natural to ask if we can extend $H$ by $N$ in a way that is compatible with $\varphi$ . However, I think this question is best phrased geometrically. We can view $G$ as a bundle over $G/N$ , where each fibre is isomorphic to $N$ . Then since $G/N$ includes into $H$ , our question becomes this: Can we extend this bundle structure from $G/N$ to all of $H$ in a way that is compatible with the group structure of $G$ ? Ideally, we should end with a commutative square of the following sort: This has to do with solving an extension problem and I suspect this will require other geometric tools, such as group cohomology. Is this problem solvable? Even special cases would be of interest. Have people considered this problem before? I would be happy for references to papers or books. Thanks in advance ^_^","['homological-algebra', 'group-theory', 'abstract-algebra', 'fiber-bundles']"
3745152,How to differentiate the trace of a matrix times its diagonal,"Let $\mathbf{\Theta}\in\mathbb{R}^{p\times p}$ be a matrix and denote $\mbox{diag}(\mathbf{\Theta})\in\mathbb{R}^{p\times p}$ the matrix that has the same diagonal as $\mathbf{\Theta}$ and every off-diagonal element zero. I am trying to calculate $$\frac{\partial \|\mathbf{X}\,[\mathbf{I}-\,(\mathbf{\Theta}-\mbox{diag}(\mathbf{\Theta}))]\,\|_{F}^{2} }{\partial \mathbf{\Theta}}$$ where $\|\cdot\|_{F}$ denotes the Frobenius norm, $\mathbf{I}$ the identity matrix and $\mathbf{X} \in \mathbb{R}^{n \times p}$ . The frobenius norm is equal to \begin{align*}
&tr(\mathbf{X}^{\intercal}\mathbf{X})+tr(\mathbf{\Theta}^{\intercal}\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})+tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta})\\
&-2tr(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})+2tr(\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta}))-2tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})
\end{align*} I have also worked out the derivatives to be \begin{align*}
&\frac{\partial tr(\mathbf{\Theta}^{\intercal}\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})}{\partial\mathbf{\Theta}}=2\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta}, \frac{\partial tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta})}{\partial\mathbf{\Theta}}=2diag(\mathbf{X}^{\intercal}\mathbf{X})diag(\mathbf{\Theta})\\
&\frac{\partial tr(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})}{\partial\mathbf{\Theta}}=\mathbf{X}^{\intercal}\mathbf{X},\frac{\partial tr(\mathbf{X}^{\intercal}\mathbf{X}diag(\mathbf{\Theta}))}{\partial \mathbf{\Theta}}=diag(\mathbf{X}^{\intercal}\mathbf{X}),\\
&\frac{\partial tr(diag(\mathbf{\Theta})\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})}{\partial\mathbf{\Theta}}=(\mathbf{X}^{\intercal}\mathbf{X})diag(\mathbf{\Theta})+diag(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta}).
\end{align*} But when I replace I get \begin{align*}
\frac{\partial ||\mathbf{X}\,[\mathbf{I}-\,(\mathbf{\Theta}-diag(\mathbf{\Theta}))]\,||_{F}^{2} }{\partial \mathbf{\Theta}}=2\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta}-2diag(\mathbf{X}^{\intercal}\mathbf{X}\mathbf{\Theta})+2diag(\mathbf{X}^{\intercal}\mathbf{X})-2\mathbf{X}^{\intercal}\mathbf{X},
\end{align*} which I think is wrong because the right hand side includes components from the diagonal of $\mathbf{\Theta}$ while the left hand side does not. As I am not very good with matrix calculus, I would appreciate any intuition. Thank you.","['matrices', 'solution-verification', 'derivatives', 'matrix-calculus']"
3745170,"Is $\frac{f'}{f}$ bounded for $f$ convex, $f>c$?","Let $c>0$ , $f\colon \mathbb{R} \to [c,\infty)$ be differentiable and convex. Do we have $$ \left\|\frac{f'}{f}\right\|_{\infty} < \infty ?$$ This seems to be true in simple examples, but I am not sure whether this is true in general, so I would appreciate some hint or a counterexample.","['convex-analysis', 'derivatives', 'real-analysis']"
3745172,Why can't we prove that $X= f^{-1}(f(X))$ in general?,"Let $f:A \rightarrow B$ be a map. Let $X \subseteq A$ , $Y \subseteq B$ . I saw this result that states $X \subseteq f^{-1}(f(X))$ . Well to prove that I said the following: Let $x \in X$ . By definition, $f(x) \in f(X)$ . Again, by definition, $x \in f^{-1}(f(X))$ . Hence $X \subseteq f^{-1}(f(X))$ . I already try a few examples of the other inclusion, i.e., $f^{-1}(f(X)) \subseteq X$ and indeed is false (we can see for example the quadratic function). But my doubt is why can't we say the following: Let $x \in f^{-1}(f(X))$ . By definition, $f(x) \in f(X)$ . Again by definition, $x \in X$ . It seems that there is something wrong with the definitions that I'm missing. What is my mistake here? Thank you!","['elementary-set-theory', 'proof-explanation', 'functions', 'solution-verification']"
3745180,Numbers with decimal expansions of only 4 and 7 is $E = \cap_{n=1}^{\infty}E_n $,"Fix that $E$ is the set of real numbers $x \in [0,1]$ whose decimal expansion contains only the digits $4$ and $7$ . Let $S_n$ be the set consisting of all natural numbers not exceeding $10^n$ whose digits consists only of $4$ or $7$ . For example, \begin{equation*}
    \begin{split}
        S_1 &= \{4, 7\} \\
        S_2 &= \{44, 77, 47, 74\} \\
        S_3 &= \{444, 744, 474, 447, 774, 747, 477, 777\} \\
        \vdots
    \end{split}
\end{equation*} I want to prove that $E$ can be defined as: \begin{equation*}
    E = \cap_{n=1}^{\infty}E_n, \textrm{ where } E_n = \cup_{a \in S_n} \left[\frac{a}{10^n}, \frac{a+1}{10^{n}}\right]  
\end{equation*} For instance, \begin{equation*}
    \begin{split}
        E_1 &= [0.4, 0.5] \cup [0.7, 0.8] \\
        E_2 &= [0.44, 0.45] \cup [0.77, 0.78] \cup [0.47, 0.48] \cup [0.74, 0.75] \\
        &\vdots
    \end{split}
    \end{equation*} The $E \subseteq \cap_{n=1}^{\infty}E_n$ part is immediate but I am having trouble proving $\cap_{n=1}^{\infty}E_n \subseteq E$ rigorously (I know this is immediate as well but I have to prove that rigorously). Here's my naive attempt so far: Ley $y \in \cap_{n=1}^{\infty}E_n$ . Then, $y \in E_n$ for each $n \in \mathbb{N}$ , that is, \begin{equation*}
    \begin{split}
        y \in E_1 &= [0.4, 0.5] \cup [0.7, 0.8] \\
        y \in E_2 &= [0.44, 0.45] \cup [0.77, 0.78] \cup [0.47, 0.48] \cup [0.74, 0.75] \\
        &\vdots
    \end{split}
    \end{equation*} Since $y \in E_1$ , either $y \in [0.4, 0.5]$ or $y \in [0.7, 0.8]$ . If $y \in [0.4, 0.5]$ , then since $y \in E_2$ as well, we have $y \in [0.44, 0.45]$ , or $y \in [0.47, 0.48]$ . Now, the issue is that I can keep typing up this proof indefinitely and never complete it. How can I complete the proof in a finite (and reasonable) amount of time? I thought of using induction but I am not sure what that would look like.","['general-topology', 'proof-writing', 'discrete-mathematics', 'real-analysis']"
