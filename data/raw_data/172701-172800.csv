question_id,title,body,tags
3068735,What is the surface area of the 3-dimensional elliptope?,"The $n$ -elliptope is defined as the set of $n$ -by- $n$ correlation matrices; that is, the set of $n$ -by- $n$ symmetric positive-definite matrices with ones on the diagonal. Such matrices are parametrized by their $n(n-1)/2$ upper off-diagonal elements. In the case of $n=3$ , this yields the 3-elliptope $$\Gamma = \{(x,y,z)\in[-1,1]^3 : x^2+y^2+z^2\leq 1+2xyz\}.$$ The volume of $\Gamma$ was considered in an earlier question ( What is the volume of the $3$ -dimensional elliptope? ) and shown to be $\pi^2/2$ . However, what I'm interested in presently is the subset of singular 3-by-3 correlation matrices. This corresponds precisely to the boundary of the above set: $$\partial \Gamma = \{(x,y,z)\in [-1,1]^3: x^2+y^2+z^2=1+2xyz\}.$$ With that in mind, what I want to know is the the surface area of $\partial \Gamma.$ Formally, this is not so hard: The surface can be expressed as the union of the surfaces $z=f_{\pm}(x,y)=x y\pm \sqrt{1-x^2}\sqrt{1-y^2}$ , and the bottom surface is the mirror of the top. Hence their areas are the same, so the total area is given the double integral $$S=2\int_{-1}^1\int_{-1}^1 \sqrt{1+\left(\frac{\partial f_+}{\partial x}\right)^2+\left(\frac{\partial f_+}{\partial y}\right)^2}\,dx\,dy,$$ where $$\frac{\partial f_+}{\partial x} = y-x\sqrt{\frac{1-y^2}{1-x^2}},\quad \frac{\partial f_+}{\partial y}=x-y\sqrt{\frac{1-x^2}{1-y^2}}.$$ If one substitutes $x=\cos\alpha,y=\cos\beta$ over the ranges $0\leq \alpha,\beta\leq \pi$ , then the result may be placed in the form $$S = 2\int_{0}^\pi\int_0^\pi \sqrt{\sin^2(\alpha)  \sin^2(\alpha -\beta )+\sin^2(\beta ) \sin^2(\alpha -\beta )+\sin^2(\alpha)  \sin^2(\beta) }\;d\alpha \,d\beta.$$ Alas, while this integral is intriguing it has defied my attempts at analytical solution (as well as those of Mathematica). Numerically, however, the integral seems to be exactly $5\pi$ . Can anyone show that this result is correct?","['analytic-geometry', 'definite-integrals', 'multivariable-calculus', 'closed-form', 'spectrahedra']"
3068787,Transformation Jacobian for integration by substitution in single and multiple variables?,"I wonder how to reconcile the transformation rules for integration by substitution in a single variable vs several variables . In the single-variable case the Jacobian factor $\varphi'(x)$ is just a derivative and may take on negative as well as positive values depending on the particular function and integration range. However, in the multi-variable case the Jacobian determinant factor Det $(D\varphi)(u)$ is taken in absolute value, which allows this transformation Jacobian to take on positive values only. If we take the multi-variable expression and consider the single-variable special case, then the partial derivative determinant again reduces to simply $\varphi'(x)$ . However, the additional absolute value of the determinant seems to introduce a discrepancy between the purely single-variable treatment and this special case. Moreover, it is easy to find examples showing that the absolute value indeed should not be there in the single-variable case. How to reconcile the two? Is the absolute value truly needed and correct in the multi-variable case? EDIT: Since some doubt was expressed in the comments on whether $\varphi'(x)$ may or may not be negative in the single-variable case, consider the following example: $$\int_0^1 du\,u^2=\frac{1}{3}$$ Now take $u\equiv\varphi(x)=-x$ . If we consider the absolute value of the Jacobian to be true, we get: $$\int_{0}^{-1}\left|\frac{d\varphi(x)}{dx}\right|dx\,(-x)^2=\int_0^{-1}dx\,x^2=-\frac{1}{3}$$ which gives a wrong result; while if we do not take the absolute value, we recover: $$\int_{0}^{-1} \frac{d\varphi(x)}{dx}dx\,(-x)^2=-\int_0^{-1}dx\,x^2=\frac{1}{3}$$ which is correct.","['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'substitution']"
3068821,A standard example of calculus of variation that I don't understand,"Let us consider the problem $$ 
 \min \left\{ \int_0^1 \psi\left(\dot{u}\right) \ dx: u \in \mathcal{C}^1([0,1]), u(0) =0,  u(1)=2 \right\}
$$ where $\psi \colon \mathbb{R} \longrightarrow \mathbb{R}$ is the function $$\psi (x)  = (x^2-1)^2.$$ Hence by my professor is it easy to conjecture that the minimum of the functional is $u_0 (x) = 2x$ . Anyway I do not find it ""easy to conjecture"". To convince myself of this conjecture I tried to work as it follows: Find with the first variation of the integral a condition that assures the second derivative to be zero Notice that there is only one linear function that satisfies the requirements of the ambient space of the problem. (By linear I improperly mean $f(x) = mx + q$ ). The function $u_0(x)$ is the only minimum of the functional Let us start with point 1. Let us consider $v \in V = \left \{ v \in \mathcal{C}^1([0,1]) : v(0)=v(1) = 0\right\}$ evaluating the first variation I find the condition: $$
\int_0^1 \left( 4 \dot{u}(x)^3\dot{v}(x) - 2 \dot{u}(x) \dot{v}(x)\right) \ dx
$$ which I find pretty strange since given the function $\psi$ I would have expected a condition that will allow me to deduce $\dot{u} = 1$ where the $\inf$ of the function $\psi$ is taken, even tough is not in the ambient space of the problem. Maybe I am doing something wrong, suggestions accepted. Let us focus on point 2 so that let us consider the convexified function of $\psi$ that coincides with $\psi$ for $|x| \geq 1$ and is zero otherwise. This is a convex function and, for my professor, $u_0(x) = 2x$ is the minimum of the function driven by this convexified function. Why? Anyway if it is true, if I consider the functional $G(u)$ the one driven by the convexified function we should have $F(u) \geq G(u) \geq G(u_0) = F(u_0)$ hence this shall prove that $u_0$ is the minimum of the functional of the problem. Finally the uniqueness of the minimum is given by the convexity of the functional $G$ hence it is a unique minimum also for $F$ . Can somebody please help me filling in the gaps what I am missing? Thanks in advance.","['functional-analysis', 'analysis', 'calculus-of-variations']"
3068890,Comparison of integrals by algebraic means,"$$ \begin{align}A&:=\int_0^1\frac1{\sqrt{x(1-x)}}\ \mathrm dx \\ B&:=\int_0^1\sqrt{x(1-x)}\ \mathrm dx \end{align} $$ My CAS tells me that $A = \pi$ and $B = \frac18\pi$ . How can one prove that $A=8B$ using just basic rules of integration such as the chain rule? Trigonometric functions are not allowed since they are not definable as integrals. Neither is the Gamma function allowed, since it is defined in terms of exp, which is like a trigonometric function. These restrictions are part of what I mean by ""algebraic means"". On the other hand, integration by parts is fine. Equivalently, the fundamental theorem of calculus is also fine.","['integration', 'calculus']"
3068898,Class Number Calculation of a Real Quadratic Number Field,I am looking at the example below. Can anyone explain how they end up with the contradiction. Why do they reduce $a^2-65b^2$ modulo $5$ to show that it has no integer solutions?,"['number-theory', 'algebraic-number-theory', 'maximal-and-prime-ideals', 'ideals']"
3068905,Help save this proof about the regularity of the lebesgue measure on $\mathbb{R}^d$,"In a previous homework our task was to prove the regularity of the Lebegue-measure on $\mathbb{R}^d$ .
More precisely: Let $(\mathbb{R}^d, \mathcal{M}^*, \lambda)$ be a measure space and $\mathcal{M}^* := \mathcal{M}^*(\mathbb{R}^d)$ be the $\sigma$ -algebra of the $\lambda^*$ -measurable sets, with $\lambda$ being the by the Lebesgue-outer-measure $\lambda^*$ induced measure. Show that \begin{align*}
\lambda(A)
& = \inf\{\lambda(O) \mid O \subset \mathbb{R}^d \text{ is open and } A \subset O \} \\
& = \sup\{\lambda(K) \mid K \subset \mathbb{R}^d \text{ is compact and } K \subset A \}.
\end{align*} Our attempt goes as follows: For all $A \in \mathcal{M}^*$ we want to show \begin{equation*}
\lambda(A)
\ge \inf\{ \lambda(O): O \subset \mathbb{R}^d \text{ open und } A \subset O \}
\ge \sup\{ \lambda(K): K \subset \mathbb{R}^d \text{ compact and } K \subset A \}
\ge \lambda(A)
\end{equation*} First inequality: If $\lambda(A) = \infty$ , because of the monotonicity of the measure, we have $\lambda(O) = \infty$ for all open sets $O$ with $A \subset O$ . If $\lambda(A) < \infty$ , we define $A_{\delta} := \bigcup_{a \in A} U_{\delta}(a)$ , which is a open and therefore measurable set with $A \subset A_{\delta}$ . Now we have $A_{\delta} \setminus A \xrightarrow{\delta \to 0} \emptyset$ and because of the continuity from below in the emptyset of $\lambda$ , and, because $A_{\delta} \setminus A$ is measurable, because it's the difference of measurable sets, \begin{equation*}
\forall \varepsilon > 0 \ 
\exists \delta > 0:
\lambda(A_{\delta} \setminus A) < \varepsilon.
\end{equation*} Because $A_{\delta}$ is open and $\sigma$ -additivity of $\lambda$ we have \begin{equation*}
\inf\{ \lambda(O): O \subset \mathbb{R}^d \text{ open und } A \subset O \}
\le \lambda(A_{\delta})
= \lambda(A_{\delta} \setminus A) + \lambda(A)
< \varepsilon + \lambda(A).
\end{equation*} Because $\varepsilon > 0$ was arbitrary, the inequality follows. The second inequality follows from the monotonicity of $\lambda^*$ :
        From $K \subset A \subset O$ we have $\lambda(K) \le \lambda(O)$ for $K$ und $O$ as defined above.
Because taking the supremum or infimum doesn't change weak inequalities, the inequality follows. The third inequality: One can show, that every open set is $\sigma$ -compact. Lemma Let $A_{\delta} \subset \mathbb{R}^d$ be an open subset.
Then there exists a countable family of closed bounded axially parallel cubes $(W_k)_{k \in \mathbb{N}}$ , so that $A_{\delta}  = \bigcup_{n \in \mathbb{N}} W_n$ . Proof Let $a \in A_{\delta} $ .
Because $A_{\delta} $ is open, there exists an $\varepsilon_{a} > 0$ , so that $U_{\varepsilon_a}(a) \subset A_{\delta} $ .
For every $a \in A_{\delta} $ we choose a bounded closed axially parallel cube $W_a$ with rational midpoint from $\mathbb{Q}^d$ and rational edge length $q \in \mathbb{Q}$ , so that \begin{equation*}
a
\in W_{a}
\subset U_{\varepsilon_a}(a)
\subset A_{\delta} .
\end{equation*} This is always possible, because $\mathbb{Q}$ is dense in $\mathbb{R}$ .
Therefore, we have $A_{\delta} = \bigcup_{n \in \mathbb{N}} W_n$ .
Since there are only countable many cubes of this form, the union is countable. $\square$ We let $K_n := \bigcup_{j = 1}^{n} W_j$ , which is compact as union of compact sets.
Then $K_n \xrightarrow{n \to \infty} A_{\delta}$ . Now we have $A_{\delta} \setminus K_n \xrightarrow{n \to \infty} \emptyset$ and with analogous argumentation as above \begin{equation*}
\forall \varepsilon > 0 \ 
\exists N \in \mathbb{N}:
\lambda(A_{\delta} \setminus K_n) < \varepsilon \ 
\forall n > N.
\end{equation*} Therefore follows for all $\varepsilon > 0$ \begin{align*}
\lambda(A)
\le \lambda(A_{\delta})
\le \lambda(A_{\delta}\setminus K_n) + \lambda(K_n)
< \varepsilon + \sup\{ \lambda(K): K \subset \mathbb{R}^d \text{ compact und } K \subset A_{\delta} \},
\end{align*} Because $\varepsilon > 0$ was arbitrary and $\delta$ can be arbitrarily small, the inequality follows. My problem I assume the proof of the second inequality is right. But: I not sure if the reasoning at the end of the proof of the last inequality is rigorous enough. Especially, if you take $A := \mathbb{Q}$ in the proof of the first inequality, then for every $\delta > 0$ , we have $A_{\delta} = \mathbb{R}$ and therefore, we don't have $A_{\delta} \setminus A \to \emptyset$ or even $\lambda(A_{\delta} \setminus A) \to 0$ . Is there anyway to ''save'' this proof by fixing it and not changing the approach? Correct Proof First inequality Case 1: $\lambda(A) = \infty$ .
As above. Case 2: $\lambda(A) < \infty$ . Utilising the Caratheodory construction of the Lebesgue measure, we know that $$
\forall \varepsilon > 0 \
\exists (a_n,b_n] := \prod_{i=1}^d \left(a_{n}^{(i)},b_{n}^{(i)}\right]: 
A \subset \bigcup_{n \in \mathbb{N}} (a_n,b_n]
\quad \text{and} \quad
\sum_{k=1}^\infty \lambda \left((a_k,b_k]\right)
< \lambda(A) + \frac{\varepsilon}{2},
$$ where the $d$ -dimensional cubes $(a_n,b_n]$ are pairwise disjoint. Now let $$
U := \bigcup_{n=1}^\infty (a_n, b_n+ t_n \varepsilon)
\qquad \text{with} \qquad t_n := 2^{-n-2d-1} \max\{1,b_{n}^{(i)} -a_{n}^{(i)}\}^{-(d-1)}.
$$ Now we have $A \subset \bigcup_{n=1}^\infty (a_n,b_n] \subset U$ and $\lambda(U) < \lambda(A) + \varepsilon$ . Therefore, we have \begin{equation*}
\inf\{ \lambda(O): O \subset \mathbb{R}^d \text{ open und } A \subset O \}
\le \lambda(U)
< \lambda(A) + \varepsilon.
\end{equation*} Because $\varepsilon > 0$ was arbitrary, the inequality follows. Second inequality Same as above Third inequality From a previous homework we know that $\mathcal{B}(\mathbb{R}^d) \subset \mathcal{M}^*$ and therefore, that \begin{equation*}
\forall M \in \mathcal{M}^* \
\exists B \in \mathcal{B}(\mathbb{R}^d), N \in \mathcal{N}:
M = B \cup N,
\end{equation*} where $\mathcal{N}$ is the set of borel-null-sets.
Now define \begin{equation*}
\mathcal{D}
:= \{ B \in \mathcal{B}(\mathbb{R}^d): \lambda(B) = \sup\{\lambda(K) \mid K \subset \mathbb{R}^d \text{ is compact and } K \subset B \} \}
\end{equation*} as the set of all inner regular sets in the borel $\sigma$ -algebra, which, by construction is a subset of $\mathcal{B}(\mathbb{R}^d)$ . Now we want to show, that $\mathcal{B}(\mathbb{R}^d) \subset \mathcal{D}$ to conclude $\mathcal{B}(\mathbb{R}^d) = \mathcal{D}$ . From the above lemma we know, that for all open sets $\mathcal{O} \subset \mathbb{R}^d$ we have $\mathcal{O} \in \mathcal{D}$ , since by the continuity of the Lebesgue-measure, for all $\varepsilon > 0$ we have $\lambda(A) \le \lambda(\bigcup_{n \in \mathbb{N}} W_n) + \varepsilon$ . Since the set of open sets is a $\cap$ -stable generator of $\mathcal{B}(\mathbb{R}^d) \subset \mathcal{P}(\mathbb{R})^d$ , we only need to show that $\mathcal{D}$ is a dynkin system. ( German Wikipedia article on this line of argument ) We have $\mathbb{R}^d \in \mathcal{D}$ , because $\lambda(\mathbb{R}^d) = \infty = \sup\{\lambda(K): K \subset \mathbb{R}^d \}$ . Let $D \in \mathcal{D}$ . Then ?? Let $\{ A_n \}_{n \in \mathbb{N}} \subset \mathcal{D}$ be a family of disjoint subsets. Then","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3068914,Notation for using a relation as a function from an element to set?,"Let $R \subseteq X \times Y$ Is there a commonly used term/notation for the functions $f:X\rightarrow \mathcal{P}(Y)$ and $g:Y\rightarrow \mathcal{P}(X)$ defined as follows?: $$f(x) = \{ y \mid (x, y) \in R\}$$ $$g(y) = \{ x \mid (x, y) \in R\}$$ Since these values are a very elementary part of the structure of a relation, I'd think that this shows up commonly and might have a name. If there is notation for $f$ then perhaps $g$ doesn't need different notation as $g$ is just $f$ on $R^{-1}$","['elementary-set-theory', 'notation', 'relations', 'terminology']"
3068930,Is there a name for the general class of triangles for dimensions other than two?,"Triangles differ from all other two-dimensional polygons in that their angles are rigidly fixed when the side lengths are known.  It occurs to me that a triangular pyramid has the same property in three dimensions that a triangle does in two-dimensions.  Is there a name for this phenomenon in general, or for the class of shapes that share this property?","['triangles', 'geometry']"
3068937,What is the relationship between simple prime-power counting function and $\log\zeta(s)$?,"This question assumes the following definitions of prime-counting functions where $p$ denotes a prime number, $k$ denotes a positive integer, and $\theta(y)$ is the Heaviside step function which takes a unit step at $y=0$ . (1) $\quad\pi(x)=\sum\limits_{p}\theta(x-p)\,\qquad\quad\text{(fundamental prime counting function)}$ (2) $\quad\Pi(x)=\sum\limits_{n=p^k}\frac{1}{k}\,\theta(x-n)\,\quad\text{(Riemann's prime-power counting function)}$ (3) $\quad k(x)=\sum\limits_{n=p^k}\theta(x-n)\,\qquad\text{(simple prime-power counting function)}$ The following plot illustrates $\pi(x)$ , $\Pi(x)$ , and $k(x)$ in blue, orange, and green respectively. Note that $\pi(x)$ takes a step of $1$ at each prime and $k(x)$ takes a step of $1$ at each prime-power. $\Pi(x)$ is more complicated in that it takes a step of $\frac{1}{k}$ at each prime-power $p^k$ . The $\pi(x)$ and $\Pi(x)$ functions defined above are related to $\log\zeta(s)$ as defined below. (4) $\quad\log\zeta(s)=s\int\limits_0^\infty\Pi(x)\,x^{-s-1}\,dx\,,\quad\Re(s)>1$ (5) $\quad\log\zeta(s)=s\int\limits_0^\infty\frac{\pi(x)}{x\,\left(x^s-1\right)}\,dx\,,\qquad\quad\Re(s)>1$ Question : What is the relationship between $k(x)$ and $\log\zeta(s)$ ? More specifically, what is the definition of the function $f(x)$ consistent with (6) below? (6) $\quad\log\zeta(s)=s\int\limits_0^\infty k(x)\,f(x)\,dx\,,\qquad\quad\Re(s)>1$ The following relationships between $\pi(x)$ , $\Pi(x)$ , and $k(x)$ may provide some insight where $rad(n)$ is the greatest square-free divisor of $n$ also referred to as the square-free kernel of $n$ . (7) $\quad\Pi(x)=\sum\limits_{n=1}^{\log_2(x)}\frac{1}{n}\,\pi(x^{1/n})$ (8) $\quad\pi(x)=\sum\limits_{n=1}^{\log_2(x)}\frac{\mu(n)}{n}\,\Pi(x^{1/n})$ (9) $\quad k(x)=\sum\limits_{n=1}^{\log_2(x)}\pi(x^{1/n})$ (10) $\quad\pi(x)=\sum\limits_{n=1}^{\log_2(x)}\mu(n)\,k(x^{1/n})$ (11) $\quad k(x)=\sum\limits_{n=1}^{\log_2(x)}\frac{\phi(n)}{n}\,\Pi(x^{1/n})$ (12) $\quad\Pi(x)=\sum\limits_{n=1}^{\log_2(x)}\frac{\mu(rad(n))\,\phi(rad(n))}{n}\,k(x^{1/n})$","['riemann-zeta', 'number-theory', 'mellin-transform', 'prime-numbers']"
3068939,Limit of solution of Cauchy problem,"I have the following Cauchy problem: \begin{align}
y'(t) = \arctan(t^3(y-1)) \\
y(0) = \alpha
\end{align} I want to study the limit of the solution on the boundary. This is what I have done so far: I know that the function is $C^\infty$ so it is Lipshitz and then I have uniqueness and existence of global solution for each $\alpha$ . The constant solution is y = 1. By uniqueness, other solutions cannot interest this line. If $\alpha < 1$ , then I have that the function increases monotonically up until $\alpha$ and then decreases monotonically to infinity. If $\alpha > 1$ , then I have that the function decreases monotonically down to $\alpha$ and then increases monotonically to infinity. This means that I always have the limit both at $-\infty$ and at $\infty$ for whatever alpha. However my problem now is actually finding the limit. Any suggestion?","['cauchy-problem', 'ordinary-differential-equations']"
3069000,Demonstrate that solution of differential equation is bounded,"Let $b(t) \in C^1([0,+\infty))$ . I have to find a formula for the solution of this Cauchy's problem: $$
\left\{\begin{aligned}
x''(t)+x(t)&=b(t) \\
x(0)&=x_0\\
x'(0)&=x_1
\end{aligned}\right.
$$ I have solved this part of the question and the formula is: $$x(t) = x_0\cos(t)+x_1\sin(t) -\cos(t)\int_0^tb(s)\sin(s)\,ds+\sin(t)\int_0^tb(s)\cos(s)\,ds.$$ Then the problem asks to demonstrate that if $b(t)$ is bounded and monotone also $x(t)$ is bounded. It also asks to find a counterexample if $b(t)$ is bounded but not monotone. I can solve the second part by picking for example $b(t) = \cos(t)$ but I'm not able to demonstrate the first fact. I have tried to estimate $|x(t)|$ but the only inequality I come up with is $|x(t)| \leq |x_0|+|x_1|+2Mt$ where $M:=\max_{[0,+\infty)}|b(t)|.$ Thank you in advance for your help.","['cauchy-problem', 'ordinary-differential-equations', 'estimation']"
3069016,Constant scalar curvature with positive Ricci curvature,"Let $M$ be a compact smooth manifold and $g$ a Riemannian metric on $M$ . By the solution of the Yamabe problem, there exists a metric $\tilde{g}$ of constant scalar curvature on $M$ which is conformal to $g$ . Suppose $g$ has positive Ricci curvature. Does there exist a metric $\tilde{g}$ conformal to $g$ such that $\tilde{g}$ has constant scalar curvature and positive Ricci curvature as well?","['geometry', 'riemannian-geometry', 'differential-geometry']"
3069049,Finding a homomorphism between groups with a given kernel,"What is a homomorphism defined on the group of invertible upper-triangular $3\times 3$ matrices whose kernel consists of matrices $\begin{bmatrix} 1 & 0 & a \\ 0 & 1 & 0 \\ 0 &0 & 1\end{bmatrix}$ ? I want to use this to study the quotient group, also is there always a way to find a group homomorphism, once the kernel is given and one the group is known in general ? What changes when the diagonal entries in the group of upper triangular matrices are all equal to one . I prefer a correct answer as much as easy to follow explanation, because my backgroud in algebra does not go beyond ""first abstract algebra course"" Thank you.","['group-theory', 'abstract-algebra']"
3069050,Explicit Euler method for Fokker-Planck equation,"I'm trying to obtain an approximation of the solution of the following equation: $$
\left\lbrace \begin{array}{l,l}
u_t = \alpha u_{xx} + (\beta u)_x,  & u,\alpha ,\beta \in [T_0,T_f]\times [X_0,X_f]\\
u(t=T_0,x) = u_0(x), & \forall x \in [X_0,X_f] \\
u(t,x=X_0) = g_0(t), u(t,x=X_f) = g_f(t), & \forall t \in [T_0,Tf]
\end{array}\right.
$$ With $u_t = \frac{\partial u}{\partial t}$ , $(\beta u)_x = \frac{\partial (\beta u)}{\partial x}$ and $u_{xx} = \frac{\partial^2 u}{(\partial x)^2}$ . The thing is, there must be something wrong since whenever $\beta$ is negative the approximations end up blowing up several times (so far it goes well for as long as this is avoided). I'll explain what I used for obtaining this approximations in case you can see if there's something I missed or simply is mistaken. I used a finite difference method, by first applying the method of lines leaving the $t$ variable continuous as $x$ is discretized with $N+2$ nodes as follows: $$
\begin{array}{l}
\Delta x = \frac{X_f - X_0}{N+1} \\
x_j = X_0 + j\Delta x,\hspace{.2cm} j \in \lbrace0,1,...,N+1\rbrace \\
u_j(t) \simeq u(t,x_j)
\end{array}
$$ Using central difference for $u_{xx}(x)$ ( $u_{xx}\simeq \frac{u(x+\Delta x) - 2u(x) + u(x-\Delta x)}{(\Delta x)^2}$ ) the equation results as follows: $$
\frac{\partial u_j}{\partial t}(t) = \alpha(t,x_j)\frac{u_{j+1} - 2u_j + u_{j-1}}{(\Delta x)^2}(t) + \beta(t,x_j)\frac{u_{j+1}-u_{j-1}}{2\Delta x}(t) + \beta_x(t,x_j) u_j(t) = \\
= u_{j-1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} - \frac{\beta(t,x_j)}{2\Delta x} \right) + u_j(t)\left( \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2} \right) + u_{j+1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x} \right)
$$ Naming $a_1(t,x_j) = \frac{\alpha(t,x_j)}{(\Delta x)^2} + \frac{\beta(t,x_j)}{2\Delta x}$ , $a_2(t,x_j) = \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2}$ and $a_3(t,x_j) = \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x}$ the following system of equations is obtained: $$
U_t(t) = A(t)U(t)
$$ With: $$
\begin{array}{l}
 U_t(t) \simeq (u_t(t,x_1),...,u_t(t,x_N))' \\
 U = (u_1(t),...,u_N(t))' \\
 g = (a_1(t,x_1)g_0(t),0,0,...,0,a_3(t,x_N)g_f(t))'
\end{array}
$$ All of them being $N\times 1$ . A is the following $N\times N$ matrix: $$
\left(\begin{array}{c,c,c,c,c,c,c,c}
a_2(t,x1) & a_3(t,x_1) & 0 & 0 & ... & ...& ... & 0 \\
a_1(t,x_2) & a_2(t,x_2) & a_3(t,x_2) & 0 & ... & ... & ... & 0 \\
0 & a_1(t,x_3) & a_2(t,x_3) & a_3(t,x_3) & 0 & ... & ... & 0 \\
0 & 0 & \ddots & \ddots & \ddots & 0 & ... & 0 \\
\vdots & 0 & 0 & \ddots & \ddots & \ddots & ... & \vdots \\
0 & ... & ... & 0 & 0 & a_1(t,x_{N-1}) & a_2(t,x_{N-1}) & a_3(t,x_{N-1}) \\
0 & 0 & ... & ... & ... & 0 & a_1(t,x_N) & a_2(t,x_N)
\end{array}\right)
$$ Then I discretized time with $M+1$ nodes as follows: $$
\begin{array}{l}
\Delta t = \frac{T_f - T_0}{M}\\
t_i = T_0 + i\Delta t, \hspace{.2cm} i \in \lbrace 0, 1, ..., M\rbrace \\
u_j^i \simeq u(t_i,x_j)
\end{array}
$$ Applying forward Euler ( $\frac{\partial u_j^i}{\partial t} \simeq \frac{u_j^{i+1} - u_j^i}{\Delta t}$ ) to the equation obtained after space discretization: $$
\frac{u_j^{i+1} - u_j^i}{\Delta t} = u_{j-1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} - \frac{\beta(t,x_j)}{2\Delta x} \right) + u_j(t)\left( \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2} \right) + u_{j+1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x} \right)
$$ Hence, we can state $u_j^{i+1}$ as: $$
u_j^{i+1} = u_j^i + \Delta t \left(u_{j-1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} - \frac{\beta(t,x_j)}{2\Delta x} \right) + \\ u_j(t)\left( \beta_x(t,xj) - 2\frac{\alpha(t,x_j)}{(\Delta x)^2} \right) + u_{j+1}(t)\left( \frac{\alpha(t,x_j)}{(\Delta x)^2} +\frac{\beta(t,x_j)}{2\Delta x} \right)\right)
$$ With the following notation: $$
\begin{array}{l}
U^i = (u_1^i,u_2^i,...,u_N^i)' \\
h^i = (a_1(t_i,x_ 1)g_0(t_i)\Delta t,0,...,0,a_3(t_i,x_N)g_f(t_i)\Delta t )' 
\end{array}
$$ $U^i$ and $h^i$ with dimensions $N \times 1$ . Considering $I_N$ the $N \times N$ identity matrix we have: $$
U^{i+1} = \left(I_N + \Delta t A(t_i)\right)U^i + h^i, \hspace{.2cm} 1 \leq i \leq N 
$$ I plotted the approximations at some time values and, as I said before, whenever $\beta$ has negative values several blow ups appear. I would like to know if there's something wrong with the method I used or mistakes during the process so I can determine whether the error lies in the method or the code. Thank you in advance! P.S: The discretizations are taken in a way the Courant-Friedrichs-Levy condition is satisfied, that is not the issue.","['numerical-methods', 'finite-differences', 'ordinary-differential-equations', 'partial-differential-equations']"
3069076,"Prove that if $f(x)=f(y)$ for all $f\in X^{*},$ then $x=y$","Can you check if my proof is correct? Let $X$ be a normed linear space. Prove that if $f(x)=f(y)$ for all $f\in X^{*},$ then $x=y$ Let $f\in X^{*}$ , then $f$ is a bounded linear functional. Assume that $x,\in X$ such that \begin{align}f(x)=f(y)&\iff f(x)-f(y)=0, \\& \iff f(x-y)=0, \;\text{since}\;f \;\text{is a linear functional}\;\\& \iff x-y\in \ker f =\{0\}\\& \iff x=y\end{align}","['linear-algebra', 'functional-analysis', 'analysis']"
3069171,Prove that $(((f_{*})^*)^*)_{*}: P(P(P(P(A))))\rightarrow P(P(P(P(B))))$ is injective,"Let $f^*$ be the image of $f$ and $f_{*}$ be the preimage of $f$ . How can I prove (step by step, using definitions of images and preimages) that if $f: A \rightarrow B$ is injective, then $$(((f_{*})^*)^*)_{*}: P(P(P(P(A))))\rightarrow P(P(P(P(B))))$$ is injective?","['functions', 'logic']"
3069178,"Prove that $ \|x\|=\sup\{|f(x)|:f\in X^*, \,\|f\|=1\},$ where $x\in X$ and $X^*$ denotes the dual space of $X$.","Let $x$ be an element of a normed linear space $X$ and let $X^*$ denote the dual space of $X$ . Prove that \begin{align} \|x\|=\sup\{|f(x)|:f\in X^*, \,\|f\|=1\} \end{align} MY TRIAL It suffices to show that \begin{align} \forall\;\epsilon>0,\;\exists\;|f(x_{\epsilon})|\in \{|f(x)|:f\in X^*, \,\|f\|=1\}\;\;\text{such that}\end{align} \begin{align} \|x\|-\epsilon< |f(x_{\epsilon})|\leq \|x\|.\end{align} Let $x\in X$ such that $x\neq 0.$ Otherwise, $\|f\|=0$ . Then, by Hanh-Banach Theorem, there exists a linear functional $f$ on $X$ such that \begin{align} \|f\|=1 \;\;\;\text{and}\;\;\;|f(x)|= \|x\|\leq \|x\|.\end{align} Please, I'm I right thus far? If yes, I am stuck here as I don't know how to arrive at \begin{align} \|x\|-\epsilon< |f(x)|.\end{align} If no, can you help fix my wrong(s)?","['functional-analysis', 'analysis']"
3069186,What can we say about a regular version of the conditional distribution given a random variable $X$ on the set $\left\{X=x\right\}$?,"Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $(E_i,\mathcal E_i)$ be a measurable space $X_i:\Omega\to E_i$ Assume $X_2$ is $(\mathcal A,\mathcal E_2)$ -measurable and that there is a regular version $\kappa$ of the conditional distribution of $X_2$ given $X_1$ on $(\Omega,\mathcal A,\operatorname P)$ , i.e. $\kappa$ is a Markov kernel with source $(\Omega,\sigma(X_1))$ and target $(E_2,\mathcal E_2)$ with $$\operatorname P\left[X_2\in B_2\mid X_1\right]=\kappa(\;\cdot\;,B_2)\;\;\;\text{almost surely for all }B_2\in\mathcal E_2\tag1.$$ I've often seen the claim (see, for example, Definition 8.28 ) that $$(E_1,\mathcal E_2)\ni(x_1,B_2)\mapsto\kappa\left(X_1^{-1}\left(\left\{x_1\right\}\right),B_2\right)=\operatorname P\left[Y\in B_2\mid X_1=x_1\right]\tag2$$ is a Markov kernel with source $(E_1,\mathcal E_1)$ and target $(E_2,\mathcal E_2)$ . (You might want to compare this with the (strange looking) claim on Wikipedia about the ""topological support of the distribution of $X_1$ under $\operatorname P$ "" ) Neither I see why $(2)$ is well-defined nor why the claimed equality holds. The author is assuming that $X_1^{-1}\left(\left\{x_1\right\}\right)$ is set to an arbitrary value if $x_1\not\in X_1(\Omega)$ . That's not the problem. The most obvious problem is that $\left\{x_1\right\}$ might not belong to $\mathcal E_1$ . However, let's assume that each singleton set is contained in $\mathcal E_1$ . We still got the problem that the right-hand side of $(2)$ is $0$ if $\operatorname P\left[X_1=x_1\right]=0$ ; and that might be the case for all $x_1$ (for example if the distribution of $X_1$ under $\operatorname P$ has a density with respect to the Lebesgue measure). Having said that, what's clear to me is that we can easily find (without any assumption on $(E_1,\mathcal E_1)$ ) a Markov kernel $\tilde\kappa$ with source $(E_1,\mathcal E_1)$ and target $(E_2,\mathcal E_2)$ with $$\operatorname P\left[X_2\in B_2\mid X_1\right]=\tilde\kappa(X_1,B_2)\;\;\;\text{almost surely for all }B_2\in\mathcal E_2\tag3$$ and $\tilde\kappa(\;\cdot\;,B_2)$ is uniquely determined on $X_1(\Omega)$ for all $B_2\in\mathcal E_2$ . By $(3)$ we immediately obtain $$\operatorname P\left[\left(X_1,X_2\right)\in\;\cdot\;\right]=\operatorname P\left[X_1\in\;\cdot\;\right]\otimes\tilde\kappa\tag4$$ (where the right-hand side denotes the product of transition kernels ). We trivially obtain $$\operatorname P\left[X_2\in B_2\mid X_1=x_1\right]=\tilde\kappa(x_1,B_2)\tag5$$ for all $x_1\in E_1$ with $\operatorname P\left[X_1=x_1\right]>0$ (hence, maybe none) and $B_2\in\mathcal E_2$ . Is there anything I'm missing? EDIT: I've just observed that $$\operatorname P\left[X_2\in B_2\mid X_1\right]=\tilde\kappa(x_1,B_2)\;\;\;\text{on }\left\{X_1=x_1\right\}\text{ for all }x_1\in X_1(\Omega)\text{ almost surely}\tag6$$ for all $B_2\in\mathcal E_2$ (note that the null set in $(6)$ does depend on $B_2$ only); maybe that's what's actually meant.","['measure-theory', 'conditional-probability', 'probability-distributions', 'conditional-expectation', 'probability-theory']"
3069192,Is a Hahn-Banach extension always continuous?,"We proved the following version of the Hahn-Banach extension theorem in a course I'm taking: Theorem (Hahn-Banach): Let $X$ be a real vector space and $q : X \to \mathbb{R}$ be sublinear. Let $U \subseteq X$ be a linear subspace
  and $l : U \to \mathbb{R}$ be a linear functional that satisﬁes $l(u)
 \leq q(u)$ for all $u \in U$ . 
  Then there exists a linear extension $L : X \to \mathbb{R}$ of $l$ that satisﬁes $L(x) \leq q(x)$ for all $x \in X$ . Is it true that this Hahn-Banach extension $L$ is also continuous , hence a $\mathbb{R}$ -linear function al ? The way we use the theorem later in this course seems to rely on getting a functional out of the extension. Additionally, the Wikipedia entry states that Hahn-Banach yields a linear functional. Some of my thoughts/work so far: I see that, if $q$ is continuous, then $L$ is also continuous: 
Let $(x_n) \subseteq X$ with $x_n \to 0$ . Then $$
L(x_n) \leq q(x_n) \to 0
$$ $$
-L(x_n) = L(-x_n) \leq q(-x_n) \to 0
$$ So $|L(x_n)| \leq \max\{q(x_n), q(-x_n)\} \to 0$ . Hence, $L$ is continuous at $0$ , and therefore continuous. But a sublinear function isn't necessarily continuous... so this doesn't help, right? From what I showed above, we see that $$
-q(-x) \leq L(x) \leq q(x) \qquad \forall x \in X.
$$ Not sure if this inequality might help.","['hahn-banach-theorem', 'functional-analysis', 'real-analysis']"
3069221,A function satisfying a given condition,"Is there a continuous  real valued function $f$ satisfying $f(x+1)(f(x)+1)=1$ for all $x$ in the domain of $f$ (possibly $\mathbb{R})$ ? Clearly the image of $f$ does not include $0$ and $-1$ .  By the continuity of $f$ and the intermediate value theorem, $f$ has to be always positive or always negative since otherwise there would be a zero of $f$ . Does such a continuous $f$ exist? How about discontinuous functions? Please give a hint to proceed. Thank you.","['continuity', 'functions', 'functional-equations', 'real-analysis']"
3069232,Constructing diffeomorphisms of moduli spaces of $J$-holomorphic curves,"Let $M^{2n}$ be a smooth manifold admitting two almost complex structures $J_0$ and $J_1$ . Suppose that $J_0$ and $J_1$ are both regular in the sense that the moduli space $$
\mathcal{M}_i:=\mathcal{M}(A; J_i)/PSL(2,\mathbb{C})
$$ of $J_i$ -holomorphic spheres representing the homology class $A\in H_2(M;\mathbb{Z})$ is a smooth , compact manifold for both $i=0,1$ . Suppose further that there exists a path $(J_t)_{t\in [0,1]}$ of regular almost complex structures on $M$ connecting $J_0$ and $J_1$ . In McDuff and Salamon's book they show that the manifolds $$
\mathcal{M}_t:=\mathcal{M}(A;J_t)/PSL(2,\mathbb{C})
$$ are oriented cobordant for all $t\in [0,1]$ . QUESTION: Are the moduli spaces $\mathcal{M}_t$ diffeomorphic for all $t\in [0,1]$ ? If not, how does the diffeomorphism fail?","['complex-geometry', 'symplectic-geometry', 'differential-geometry']"
3069242,A very strange and difficult hyperbolic integral,"This integral gave me serious problems, I tried to solve it by parts but it is madness! The calculations are too long and difficult, I do not think we should solve this. $$\int _{ -\frac { 1 }{ 3 }  }^{ \frac { 1 }{ 3 }  }{ \sqrt { 36{ x }^{ 4 }-40{ x }^{ 2 }+4 } \cosh { (3x+\tanh ^{ -1 }{ (3x)-\tanh ^{ -1 }{ (x)) }  }  }  }$$ the answer must be $$\frac { 12+4{ e }^{ 2 } }{ 9e }$$ Can some expert help me?","['integration', 'calculus', 'definite-integrals']"
3069258,"For a set of functions, can we create a function with a maximal average correlation?","Let $F = \{F_1, F_2, \dots, F_n\}$ be a set of functions of the form $F_i: S \to \mathbb R$ . Additionally, let $X$ be a random variable with state space $S$ . The average correlation of $f: S \to \mathbb R$ (not necessarily in $F$ ) is defined as $$\overline{\rho_{f(X), F(X)}} = \frac 1n \sum_{i=1}^n\rho_{f(X), F_i(X)}$$ if $f(X)$ is defined. My question is is there is function $f: S \to \mathbb R$ with maximal average correlation (by which I mean $f(X)$ is defined and there does not exist a $f': S \to \mathbb R$ such that $f'(X)$ is defined and $\overline{\rho_{f'(X), F(X)}} > \overline{\rho_{f(X), F(X)}}$ )? Since $\overline{\rho_{f(X), F(x)}}$ is bounded above by $1$ , a supremum exists. I do not know if a $f$ exists achieving this supermum, though (hence my question). Also, $f$ is not unique, since if $f$ has maximal average correlation so does $g$ if $g(x) = a + bf(x)$ for $a \in \mathbb R_+$ , $b \in \mathbb R_{\neq 0}$ , and $x$ ranging over $S$ . That said $f$ is likely unique up to this transformation on the support of $X$ . Note also that simply taking the average of $F$ likely will not work, since the pearson coefficient ignores constant factors, whereas the average does not.","['statistics', 'correlation', 'optimization', 'probability', 'random-variables']"
3069263,"If $P(\max_{n \leq i \leq j \leq k \leq m} \min \{|S_i - S_j|, |S_k - S_j|\} \geq \lambda)$ is bounded appropriately, does $\sum \xi_j$ converge a.s.?","Suppose that $$E\left[|S_j - S_i|^\gamma |S_k - S_j|^\gamma \right] \leq \left(\sum_{i < l \leq j} u_l\right)^\alpha \left(\sum_{j < l \leq k} u_l\right)^\alpha$$ for $0 \leq i \leq j \leq k \leq m$ with $S_k = \sum_{i \leq k} \xi_i$ ( $S_0 = 0$ ) and $\gamma, \alpha$ postitive, and $u_l \geq 0$ for every $l$ . It follows from a theorem in Billingsley's book Convergence of Probability Measures (first edition) that $$P\left(\max_{n \leq i \leq j \leq k \leq m} \min \{|S_i - S_j|, |S_k - S_j|\} \geq \lambda\right) \leq \frac{K}{\lambda^{2\gamma}} \left(u_{n+1} + \ldots + u_{m}\right)^{2\alpha} \times \min_{n + 1 \leq h \leq m}\left(1 - \frac{u_h}{u_{n + 1} + \ldots + u_m}\right)$$ where $K$ depends only on $\lambda$ and $\gamma$ . Assume additionally that $\sum u_l < \infty$ . Can we use the above bounded probability to show that $\sum \xi_i < \infty$ almost surely? I think we can simplify things by saying that since $\min_{n + 1 \leq h \leq m}\left(1 - \frac{u_h}{u_{n + 1} + \ldots + u_m}\right) \leq 1$ we actually only need to work with $$P\left(\max_{n \leq i \leq j \leq k \leq m} \min \{|S_i - S_j|, |S_k - S_j|\} \geq \lambda\right) \leq \frac{K}{\lambda^{2\gamma}} \left(u_{n+1} + \ldots + u_{m}\right)^{2\alpha}$$ This question is related to a question I asked before and I think a similar approach to the solution given there could be used here, but I don't know how to do it. I want to relate $\max_{n \leq i, j \leq m} |S_i - S_j|$ to $\max_{n \leq i \leq j \leq k \leq m} \min \{|S_i - S_j|, |S_k - S_j|\}$ but I don't know if I can do so in a useful way.","['summation', 'almost-everywhere', 'convergence-divergence', 'probability-theory', 'random-variables']"
3069272,$d(I\omega) + I(d\omega) = \omega$ for differential forms,"Let $U$ be a convex connected and open set in $\mathbb{R}^n$ , such that $0\in U$ . For every $k$ -differential form $\omega$ , $$\omega =\sum_{i_1\lt\dots\lt i_k} c_{i_1,\dots,i_k}(x)dx{_{i_1}}\wedge\dots\wedge dx{_{i_k}},$$ we define a new $k-1$ differential form, $I\omega$ as follows - $$
\begin{split}
(I\omega)
 = \sum_{i_1\lt\dots\lt i_k}
   \sum_{j=1}^kx_{i_j}
   &\left( \int_0^1s^{k-1}c_{i_1,\dots,i_k}(sx)ds\right)\\
&
dx_{i_1}\wedge\dots dx_{i_{j-1}}\wedge dx_{i_{j+1}}\wedge\dots\wedge dx_{i_k}
\end{split}
$$ Prove  that $d(I\omega) + I(d\omega) = \omega$ . So I know that - $d\omega = \sum_{i_1\lt\dots\lt i_k} \sum_{j=1}^n\frac{\partial c_{i_1,\dots,i_k}(x)}{\partial x_j}dx_j\wedge dx_{i_1}\wedge\dots\wedge dx_{i_k}$ and therefore - $I(d\omega) =  \sum_{i_1\lt\dots\lt i_k} \sum_{j=1}^k x_{i_j}(\int_0^1\frac{\partial c_{i_1,\dots,i_k}(sx)}{\partial x_j}ds)dx_{i_1}\wedge\dots dx_{i_{j-1}}\wedge dx_{i_{j+1}}\wedge\dots\wedge dx_{i_k}$ All I need now is to calculate $d(I\omega)$ , and put it into $d(I\omega) + I(d\omega)$ and show that it is equal to $\omega$ . However, I'm not sure - How can I calculate $d(I\omega)$ ? What is $d(I\omega)=\sum_{i_1\lt\dots\lt i_k}x_{i_j}\sum_{j=1}^n\frac{\partial(\int_0^1s^{k-1}c_{i_1,\dots,i_k}(sx)ds)}{\partial d_{x_j}}dx_{i_1}\wedge\dots dx_{i_{j-1}}\wedge dx_{i_{j+1}}\wedge\dots\wedge dx_{i_k}$ ? Or perhaps, is there any other way(some tricks) so I won't need to calculate it explicitly?","['integration', 'multivariable-calculus', 'calculus', 'differential-forms']"
3069314,"If $|f'(x)| \leq c|f(x)|$ for all $x \in (0,1)$ then $f(x)=0$","Question:   Let $f:[0,1] \to \mathbb{R}$ be a real valued continuous function which is differentiable on $(0,1)$ , and satisfies $f(0)=0$ . Suppose there exists a $c \in (0,1)$ such that $|f'(x)| \leq c|f(x)|$ for all $x \in (0,1)$ . Show that $f(x)=0$ . Solution attempt: $f$ being continuous at $x=0$ , for a given $\epsilon >0$ $\exists$ a $\delta>0$ , such that $|f(x)|< \epsilon$ whenever $x \in [0, \delta) \cap [0,1]$ . Now, consider $|f'(h)|=|\lim_{k \to 0}\frac{f(h+k)-f(h)}{k} |\leq |cf(h)| \implies \lim_{ \ k \to 0} |f(2h)| \leq |f(h)|[1+ c|k|] $ Being continuous at $x=0$ , $\lim_{h \to 0} f(h) = f(0) = 0 \implies f(2h) = 0$ [by applying $|f(2h)| \leq |f(h)|(c|k|+1) \ $ ]. In this manner, let us partition the interval $[0,1)$ into $n$ subintervals of length $h$ each. As the length $h \to 0$ , $n \to \infty$ , and recursively, we get $|f(rh)| \leq |f(h)|(c|k|+1)^r \ $ . Hence, finally for all $r$ , we get $f(rh)=0$ . By continuity, we can safely say, for all $x$ in those respective subintervals, $f(x)=0$ . Again, by continuity, we have $\lim_{ x \to 1} f(x) = 0$ . Hence, $f(x)=0$ for all $x$ in $[0,1]$ .","['proof-verification', 'real-analysis', 'continuity', 'calculus', 'derivatives']"
3069349,Proving $\binom{n}{k}=\binom{n}{n-k}$ using set theory?,"Let $N$ be a set of $n$ items. Define a grouping to be some subset of $N$ . Let $A$ be the set of all groupings of $k$ items from $N$ . Let $B$ be the set of all groupings of $n-k$ items from $N$ . Namely, if $|A|=|B|$ then we have $\binom{n}{k}=\binom{n}{n-k}$ . For $|A|=|B|$ , we must have there exists a bijection from $A$ to $B$ . Let us define a function $f:A \rightarrow B$ such that $f(a)=b$ if $a \cup b = N$ We first show $f$ is injective. Take $a_1,a_2 \in A$ and assume $f(a_1)=f(a_2)=b$ . This means $a_1 \cup b = N = a_2 \cup b$ , i.e. $a_1=a_2=N-b$ . So, $f$ is injective. We now show $f$ is surjective. Take $b \in B$ and consider $a=N-b$ . Since $|N-b|=n-(n-k)=k$ , we have $|a|=k$ so $a \in A$ . We note $f(a)=b$ because $a\cup b=(N-b)\cup b=b$ . Since $f$ is injective and surjective, it is bijective. So $|A|=|B|$ , and as such we have $\binom{n}{k}=\binom{n}{n-k}$ QED. Is this proof correct?","['elementary-set-theory', 'proof-verification', 'combinatorics']"
3069401,What is the maximum value of $(a+ b+c)$ if $(a^n + b^n + c^n)$ is divisible by $(a+ b+c)$ where the remainder is 0?,"The ‘energy’ of an ordered triple $(a, b, c)$ formed by three
positive integers $a$ , $b$ and $c$ is said to be n if the following $c$ $\ge b\geq a$ , gcd $(a, b, c) = 1$ , and $(a^n + b^n + c^n)$ is
divisible ( remainder is 0 ) by $(a +b+ c) $ . There are some
possible ordered triple whose ‘energy’ can be of all values of $n \ge$ $1$ . In this case, for which ordered triple, the value of $(a+b+c)$ is
maximum? Second part (of the original problem) Determine all triples $(a,b,c)$ that are simultaneously $2004$ -good and $2005$ -good, but not $2007$ -good. Source: Bangladesh Math Olympiad 2017 junior category (Originally from Canada, 2005). I can't understand the first line of this question. Any 3 consecutive numbers have a gcd of 1. Moreover if $n=1$ , then $(a^n + b^n + c^n) = (a +b+ c) $ .","['contest-math', 'number-theory', 'gcd-and-lcm', 'elementary-number-theory']"
3069405,Proof that $x^4 - qy^4 = az^2$ has no integral solution,"This is a question from Takashi Ono's book, Problem 1.45 to be exact.
The question is Let $q$ be a prime such that $q = 1 \mod 8$ and $a$ be an integer such that $p^2\not\mid a$ for any prime $p$ and that $x^4 = a \mod q$ has no solution in $Z$ . Prove that the equation $x^4 - qy^4 = az^2$ has no
integral solution other than x = y = z = 0. I'm really stuck, this question comes after the section on quadratic reciprocity but not sure what I can do with it","['number-theory', 'quadratic-reciprocity']"
3069482,Discrete Math - Proving Distributive Laws for Sets by induction,"I'm working on doing a proof by induction on this question: Use induction to prove that if $X_1, . . . , X_n$ and $X$ are sets, then $X∩(X_1∪X_2∪· · ·∪X_n) = (X∩X_1)∪(X∩X_2)∪· · ·∪(X∩X_n)$ . I've shown the basis case: $X∩X_1 = X∩X_1$ But I'm having difficulty proving the $n + 1$ case. This is my work so far: Assume $X∩(X_1∪X_2∪· · ·∪X_n) = (X∩X_1)∪(X∩X_2)∪· · ·∪(X∩X_n)$ Show $X∩(X_1∪X_2∪· · ·∪X_n∪X_{n+1}) = (X∩X_1)∪(X∩X_2)∪· · ·∪(X∩X_n) = (X∩X_1)∪(X∩X_2)∪· · ·∪(X∩X_n)∪(X∩X_{n+1})$ I'm not sure what I'm allowed to do from here. Where can I use the induction hypothesis?","['elementary-set-theory', 'proof-explanation', 'induction', 'discrete-mathematics']"
3069490,How to prove this property using convexity?,"Suppose that $f:[a,b]\to\mathbb{R}$ be a twice-differentiable function, and that there exists $c\in(a,b)$ such that $f'(c)=\frac{f(b)-f(a)}{b-a}$ . Show that if $f''(x)>0$ for all $x\in[a,b]$ and $f''$ is strictly increasing on $[a,b]$ , then $c>\frac{a+b}{2}$ . Using Taylor's theorem, i solved the problem. But, i'd like to prove that using convexity. Give some comments or hints. Thank you!","['convexity-inequality', 'derivatives', 'real-analysis']"
3069492,$G$ is a group of order $60$. Will $G$ always contain a subgroup of order $6$?,"$G$ is a group of order $60$ . Will there be a subgroup of order $ 6$ ? Alternating group $A_5$ has a subgroup of order $6$ . That is the group generated by this set $\{(123), (23) (45)\}$ . Will we be able to prove that there always exists a subgroup of order $6$ in a group of order $ 60$ ? Can anyone help me to understand by giving a hint?","['group-theory', 'abstract-algebra', 'finite-groups']"
3069497,How to find a recurrence relation for counting the number of solutions?,"Consider the diophantine equation $$x_1+3x_2+5x_3 = n$$ where $x_i\geq 0$ and $n\geq 1.$ Let $P_n(1,3,5)$ denote the number of solutions to this equation. I want to express $P_n(1,3,5)$ in terms of $P_{1}(1,3,5), P_{2}(1,3,5),\cdots, P_{n-1}(1,3,5).$ Here is what I observed: If $(x_1,x_2,x_3)$ is a solution to $$x_1+3x_2+5x_3 = k$$ then $(x_1+1,x_2,x_3)$ is a solution to $$x_1+3x_2+5x_3 = k+1$$ $(x_1,x_2+1,x_3)$ is a solution to $$x_1+3x_2+5x_3 = k+3$$ and $(x_1,x_2,x_3+1)$ is a solution to $$x_1+3x_2+5x_3 = k+5.$$ But I don't know know how to combine this to get the desired relation. Any ideas will be much appreciated. Edit : Based on the answer given below, we observe that a solution (x_1,x_2,x_3) to $$x_1+3x_2+5x_3 = n$$ must have $x_1>0$ or $x_2>0$ or $x_3>0.$ If $x_1>0$ then (x_1-1,x_2,x_3) is a solution $$x_1+3x_2+5x_3 = n-1$$ and there are $P_{n-1}(1,3,5).$ Proceeding in a similar manner for $x_2$ and $x_3$ and applying the inclusion-exclusion principle we get: $$P_{n}(1,3,5) = P_{n-1}(1,3,5)+P_{n-3}(1,3,5)+P_{n-5}(1,3,5)-P_{n-4}(1,3,5)-P_{n-8}(1,3,5)-P_{n-6}(1,3,5)+P_{n-9}(1,3,5).$$","['elementary-set-theory', 'number-theory', 'linear-diophantine-equations', 'diophantine-equations']"
3069505,Compute volume of parallelepiped using triple vector,"Before anyone claims that I am not studying, my university recently changed their coursework that does not require linear algebra as a pre-requisite to multi-variable calculus. My professor gave us these questions even though we have no experience with matrix, he did not even teach us about it and I had to study them online by myself. I only got as far as I could and I only want one question help so I can work on the other similar questions by myself. I would not be here if I can't find any other help.","['multivariable-calculus', 'linear-algebra']"
3069579,Solving differential equations of the form $f'(x)=f(x+1)$ [duplicate],"This question already has answers here : Differential equations that are also functional (3 answers) When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? (5 answers) What function satisfies $F'(x) = F(2x)$? (1 answer) Closed 5 years ago . How to solve differential equations of the following form: $\frac{df}{dx} = f(x+1)$",['ordinary-differential-equations']
3069610,Compute $S = \sum_{k=0}^{m} \left\lfloor \frac{k}{2}\right\rfloor$,"I want to compute the following sum $$S = \sum_{k=0}^{m} \left\lfloor \frac{k}{2}\right\rfloor.$$ Here is what I tried: $$ S = \sum_{k\geq 0, 2|k}^{m} \left\lfloor \frac{k}{2}\right\rfloor + \sum_{k\geq 0, 2\not |k}^{m} \left\lfloor \frac{k}{2}\right\rfloor.$$ If $m= 2t$ then $$S =\sum_{k\geq 0, 2|k}^{m} \left\lfloor \frac{k}{2}\right\rfloor + \sum_{k\geq 0, 2\not |k}^{m} \left\lfloor \frac{k}{2}\right\rfloor = \frac{t(t+1)}{2} + \frac{(t-1)t}{2} = t^2.$$ If $m= 2t+1$ then $$S = \sum_{k\geq 0, 2|k}^{m} \left\lfloor \frac{k}{2}\right\rfloor + \sum_{k\geq 0, 2\not |k}^{m} \left\lfloor \frac{k}{2}\right\rfloor = \frac{t(t+1)}{2} + \frac{t(t+1)}{2}= t(t+1).$$ But I am not sure if this is correct. Perhaps someone could give an indication.","['number-theory', 'sequences-and-series']"
3069624,We throw $5$ dice: What is the probability to have $4$ different numbers?,"We throw $5$ dice: What is the probability to have $4$ different numbers? I know it is $$\frac{6\cdot 5\cdot 4\cdot 3}{6^5}.$$ I wanted to use an other argument, but it look to not work : I take $\binom{6}{4}$ numbers. Then I have $$\frac{6\cdot 5\cdot 4\cdot 3}{4!}$$ possibilities. Then I have to multiply this result by $4!$ and I don't understand why. Indeed, I would like to multiply by $5!$ since we can distribute the $5$ colors in e.g. $1;2;3;4;4$ in $5!$ different ways. If I want all dice different, this argument works: I take $\binom{6}{5}$ number, then I can distribute the colors in $5!$ different way which give $\frac{5\cdot 5\cdot 4\cdot 3\cdot 2}{5!}5!=6\cdot 5\cdot 4\cdot 3\cdot 2$ possibilities, that is the correct answer. So why doesn't it work with the previous situation ?","['combinatorics', 'probability']"
3069676,Laplace transform for solving differential equations [closed],Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I am confused about Laplace transform. the only thing that is important in Laplace theorem is the value of $f(t)$ in $t \geq 0$ . laplace transform: $\mathcal{L}\{f(t)\}$ $=$ $\int_0^\infty \! e^{-st}f(t)$ so as we see in the above transform $f(t)$ if $t<0$ is not important at all. so do we find the answer for only $t > 0$ or the answer is correct in $\mathbb{R}$ ?,"['laplace-transform', 'ordinary-differential-equations']"
3069699,Where does this property involving quadrilaterals come from?,"$ABCD$ is a square. $|AF|=6$ , $|FK|=2$ , and $DE \parallel AB$ . What is $|EK|=?$ My geometry book has a property for this: $$|AF|^2=|FK|\cdot|FE|$$ Can you show me where does this property come from in simple terms?","['quadrilateral', 'geometry']"
3069704,Asymptotic behaviour of gradient flows for $t \to \infty$,"I have often heard about the asymptotics of gradient flows converging to some ""equilibrium point"" as $t \to \infty$ . This concept has come to my ear by word of mouth multiple times and is often verified by direct calculations e.g. as for the 1-dimensional heat equation. It also has come to my attention that minimizing movements as described in e.g. Braides Book on $\Gamma$ -convergence try to use this concept. I would like to learn more about it, but can not find a good point to start. As for the notation in this question, $\partial $ denotes the subdifferential and $'$ the derivative with respect to the time. Let's consider a a gradient flow in the euclidean space, for simplicity and let $F:\mathbb{R}^n \to \mathbb{R}$ be a $\lambda$ -convex function (for $\lambda > 0$ the function $F(x)-\frac{\lambda}{2} |x|^2$ is convex) and lets for simplicity assume it has a gradient $\nabla F$ . Then consider the smooth solution of the IVP: $$
u'(t)=-\nabla F(u(t)) \\
u(0)=u_0
$$ You can estimate the difference of two solutions by their initial conditions; i.e. $$
|u_1(t)-u_2(t)|\leq e^{-\lambda t }|u_1(0)-u_2(0)|
$$ So taking the limit $t\to \infty$ yields that both solutions seem to converge to the same point, that is: $\lim_{t \to \infty} u_1(t)=\lim_{t \to \infty} u_2(t)$ . Now let say $F$ has 1 critical point $c \in \mathbb{R}^n$ such that $\nabla F(c)=0$ . Now we can consider the problem with inital condition $u_3(0)=c$ . Now the function $u_3(t)=c$ solves the IVP and so we get $$
\lim_{t \to \infty} u(t)=c
$$ for all solutions of the problem. So we found some sort of equilibrium point and described the asymptotic behaviour of the function $u$ using $F$ . The question that I have now is if this concept still holds if I replace $\mathbb{R}^n$ with a Hilbert space (as for metric spaces, I might explore them later). For example, the heat equation in a suitable domain, suitable initial/boundary conditions with the Dirichlet energy $E(u)=\int \frac{1}{2}|\nabla u|^2$ statsifies $$
u'=-\nabla_{L^2}E(u) \\
u(0,x)=g(x) \\ 
u(t,x)|_{\partial \Omega}=f(x) 
$$ and we have that $u$ converges as $t \to \infty$ to the solution of the Laplace equation, a critical point of $E(u)$ : $$
\Delta u=0 \\
u|_{\partial \Omega}(t,x)=f(x) 
$$ Now consider the setting for gradient flows as in Evans PDE; if you have a similar or more general setting, feel free to use it. Let $H$ be a real Hilbert space, $I:H \to (-\infty,+\infty]$ be convex, proper and lower semicontinouos and the domain of the subdifferential statisifies $\overline{D(\partial I)}=H$ .Then for each $g \in {D(\partial I)}$ there exists a unique function $$
u \in C([0,\infty);H) \; u'\in L^\infty(0,\infty;H)
$$ such that $u(0)=g$ , $u(t) \in D(\partial I)$ for each $t>0$ as well as $u'(t)\in-\partial I(u(t))$ . So is there any way or estimate to describe the existence (and maybe uniqueness) of $$
\lim_{t \to \infty}u(t)=h \in H
$$ where the limit is taken with respect to the Hilbert space $H$ ? Is $h$ a critical value of $I$ ? Is there some sort of ""exponential decay"" estimate like $||u(t)-h||_H\leq Ce^{-t}||u(0)-h||_H$ ? I am thankful for every reference, hint or answer covering any of the aspects of my question. If you need to modify the assumptions, feel free to do so. Any textbook suggestions are appreciated, as I want to learn more about this topic in a more rigorous way!","['gradient-flows', 'calculus-of-variations', 'functional-analysis', 'partial-differential-equations', 'soft-question']"
3069723,Show that there are at most $n^2$ roads.,"There are $2n$ places. If place $a$ and place $b$ has a road between them and place $b$ and place $c$ has a road between them, then there is no road between place $a$ and place $c$ .
Now show that there are at most $n^2$ roads. I observed that if we also include the odd number of places during examination of the total number of roads, a pattern is found. When total number of places is 0, 1, 2, 3, 4, 5, 6, 7; the maximum number of paths are 0, 0, 1, 2, 4, 6, 9, 12 respectively. The differences between the maximum number of paths are like 0, 1, 1, 2, 2, 3, 3 respectively. Another approach can be: We can choose two places in $^nC_2$ ways and then subtract $^nC_3$ (every triangle formed). I am not sure of this.","['graph-theory', 'combinatorics']"
3069747,Compact operators and weak convergence,"Let $X$ and $Y$ be Banach spaces. (a) Let $T \in \mathcal{L}(X, Y )$ . For each sequence $(x_n)_{n \geq 1}$ in $X$ and each $x \in X$ , show that $x_n →x$ weakly, as $n \rightarrow  \infty$ ,implies that $Tx_n \rightarrow Tx$ weakly, as $n\rightarrow \infty$ . (b)  Let $T \in \mathcal{K}(X, Y )$ . For each sequence $(x_n)_{n \geq 1}$ in $X$ and each $x \in X$ , show that $x_n →x$ weakly, as $n \rightarrow  \infty$ ,implies that $||Tx_n -Tx|| \rightarrow 0$ , as $n\rightarrow \infty$ .. (c) Conversely, if $X$ is reflexive and separable, and $T \in \mathcal{L}(X,Y)$ satisfies that $∥Tx_n − Tx∥ \rightarrow 0$ , as $n \rightarrow \infty$ , whenever $(x_n)_{n\geq 1}$ is a sequence in $X$ converging weakly to $x \in X$ , then $T \in \mathcal{K}(X, Y )$ . (d) Show that each $T \in \mathcal{L}(X,l_1(\mathbb{N}))$ is compact, whenever $X$ is reflexive and separable. (e) Let $Y$ be infinite dimensional. Show that no $T \in \mathcal{K}(X, Y )$ is open. (f) Show that there is no reflexive separable Banach space $X$ such that $l_1(\mathbb{N}) = T(X)$ , for some $T \in \mathcal{L}(X,l_1(\mathbb{N}))$ . My attempt: (a) We have that $x_n \rightarrow x$ weakly if and only if $f(x_n) \rightarrow f(x)$ weakly for every $ f \in X^*$ . Now $Tx_n \rightarrow Tx$ weakly if and only if $g(Tx_n) \rightarrow g(Tx)$ weakly for every $g \in Y^*$ . But for every $g \in Y^*$ we have $gT \in X^*$ . Therefore $Tx_n \rightarrow Tx \Leftrightarrow g(Tx_n) \rightarrow g(Tx) \Leftrightarrow  (gT)x_n \rightarrow (gT)x \Leftrightarrow x_n \rightarrow x$ weakly. b) Since $T$ is compact I know that every sequence is sent to a sequence that has a convergent subsequence, but then I don't know how to proceed. c) I have a hint for this problem: Suppose that $T$ is not compact. Show that there exists $\delta > 0$ and a sequence $(x_n)_{n\geq 1}$ in the unit ball of $X$ such that $∥Tx_n −Tx_m∥ \geq \delta$ , for all $n \neq m$ .. Show next that $(x_n)_{n\geq 1}$ has a weakly convergent subsequence. (d) Now let $(x_n)_{n \geq 1}$ be a sequence in $ X$ and $x \in X$ such that $ x_n \rightarrow x$ weakly, as $n \rightarrow \infty$ . By part (a) since $T \in \mathcal{L}(X,l_1(\mathbb{N}))$ we get $Tx_n \rightarrow Tx$ weakly, as $n \rightarrow \infty$ . But weak convergence is the same as norm convergence in $l_1(\mathbb{N})$ . Therefore $||Tx_n-Tx|| \rightarrow 0$ , as $n \rightarrow \infty$ . So we can use part $c)$ to deduce that $T \in \mathcal{K}(X,l_1(\mathbb{N}))$ , as requested. (e) I know that if $Y$ is infinite dimensional the unit ball in $Y$ is not compact. (f) Since $X$ is reflexive and separable, by part (d) we get that $T$ is compact. Suppose now that $l_1(\mathbb{N})=T(X)$ , i.e. $T$ is surjective. By the Open Mapping Theorem $T$ is open and this contradicts part (e).","['banach-spaces', 'compact-operators', 'weak-convergence', 'functional-analysis', 'sequences-and-series']"
3069761,"Meaning of ""absolute zero"" in set theory","With great pleasure I am reading ""Believing the axioms"" by Penelope Maddy after someone linked to it here on MSE. ( https://www.jstor.org/stable/2274520 ). However on page 495 there is a sentence I don't understand: For example, a subset of the unit interval is called ""absolute zero"" if it can be covered by any countable collection of intervals. (The 'for example' refers to 'strong smallness properties', of which being absolute zero is an example) I don't understand this definition and due to the overwhelming number of articles on the more famous ""absolute zero"" from physics it is impossible to google. I think my problem is with the interpretation of the word 'any'. It seems to me that for every point $x$ in the unit interval it is easy to construct a countable collection of intervals not covering $x$ , hence showing that no non-empty set can be absolute zero. But clearly that is not what is meant. But then, what is? The next sentence from the article does not make it any clearer: If covering is only required when the intervals are of equal length, then the set would have Lebesgue measure zero, but would not necessarily be absolute zero. I was hoping I could think of some criterion that sounds vaguely like 'can be covered by any countable collection of intervals of the same length' that would imply being measure zero but I failed. The next sentence from the article elaborates on the distinction between measure zero and absolute zero: Thus Cantor's discontinuum has Lebesgue measure zero, but is not absolute zero, because it cannot be covered by countable manby intervals of length $(1/3)^n$ . 'Ok', I thought, 'this gives some hint of what might be meant'. Perhaps they meant 'A set $Z$ is absolute zero if for each sequence $a_1, a_2 ,\ldots$ of numbers in $(0, 1]$ there is a collection of intervals $I_1, I_2, \ldots$ such that $I_n$ has length $a_n$ and together the intervals cover $Z$ .' At least this sounds like a well defined property. EDITED IN: Due to Asaf Karagila' comment below I now know that this property actually has name and a Wikipedia page, increasing the likelihood that this indeed is what is meant. END OF EDIT But then I thought what the analogue with 'intervals of equal length' would be and everything collapses: obviously the property 'for each number $a \in (0, 1]$ there is a countable collection of intervals, each of length $a$ , that together cover $X$ ' is true for every subset of $[0,1]$ , not just those of measure zero. So I am back to square one. Can anyone tell me what the definition of 'absolute zero' is and what property implying 'ordinary' measure zero is intended in the second sentence? UPDATE: after reading the wikipedia page on 'strong measure zero' I actually believe that indeed (as Asaf wrote) strong measure zero and absolute zero are the same thing. The relations of both properties to the continuum hypothesis and conjectures by Borel discussed in both articles are just too similar for them not to be the same property. But I am still mystified what property of a given set involving collections of intervals of the same length and implying the set to have measure zero is alluded to in the second of the quoted sentences. Does anyone know?","['measure-theory', 'set-theory']"
3069828,"$\mathrm{ess\, sup}(f)=\text{sup}(f)$ for a continuous non negative function $f$","My intuition tells me that the following proposition is true: Let $f: \Omega \rightarrow \mathbb{R}$ be a measurable function on a measurable set $\Omega \subseteq \mathbb{R}^n$ . If $f$ is continuous and non negative then $\mathrm{ess\, sup}(f)=\text{sup}(f)$ . I'm using here the Borel sigma algebras on $\Omega$ and $\mathbb{R}$ respectively, and the Lebesgue measure $m$ on $\Omega$ . My measure theory background is very poor, that being said I tried to give a proof for the case where $\Omega$ is compact: Let $A=\{a: m(\{ f\geq a \})=0 \}$ . Therefore $\mathrm{ess\, sup}(f)= \inf A = E$ . Let also $M=\max f = \sup f$ (it exists because $f$ is continuous on a compact set). I will prove $E \geq M$ , and $E \leq M$ (otherwise there would be a contradiction on the defition of $\inf A$ ). $E \leq M$ : if $a>M$ then $\{ f\geq a  \}=\emptyset$ , so $m(\{ f\geq a  \})=0$ , so $a \in A$ . Therefore $E=\inf A \leq M$ $E \geq M$ : let $a \in A$ , and suppose there's $x$ such that $f(x)>a$ . By continuity of $f$ there also an open neighbourood $U(x)$ such that for every $y\in U(x)$ $f(y)>a$ . Therefore $m(\{ f>a\})>0$ and this is impossible, since $m(\{ f\geq a  \})=0$ (because $a \in A$ ), $\Omega$ has finite measure since it is (closed and) bounded and $\{ f<a  \} $ and $\{ f > a  \}$ are disjoint. Provided the above reasoning holds I would like to extend from here the conclusion to a generic measurable $\Omega$ : anybody could give me a hint in this direction? (or show me why the general proposition is wrong?) Thanks! Edit Thanks to everybody for the help. Maybe it could be more  interesting to require $\Omega$ non empty and connected.","['general-topology', 'lebesgue-measure', 'measure-theory']"
3069832,Interior point in convex set on n.v.s,"I'm reading Peter Lax's Functional analysis, and I have a question about a definition: Definition. $X$ is a linear space over the reals, $S$ a subset of $X$ .  A point $x_0$ is called an interior point of $S$ if for any $y$ in $X$ there is an $\epsilon$ , depending on $y$ , such that $$x_0 + ty \in S \qquad\text{for all real $t$, $|t|<\epsilon$.}$$ I'm wondering if this definition of interior point is equivalent to the classical one (topological interior) in the case of a convex set. I was looking for a counterexample but I couldn't find it.","['general-topology', 'functional-analysis']"
3069861,Intuition about Poisson bracket,"I've been reading about Hamiltonian mechanics which in its mathematical description uses Poisson manifolds From my limited understanding, on a Poisson manifold $M$ we can look at the Poisson bracket as a gadget that gives a smooth vector field $\{f,- \}$ for every smooth function on $M$ . This gives a nice way to write Hamilton's equations of motion . My questions are: how should I visualize this vector field $\{f,- \}$ ? What's its connection to the function $f \in C^\infty(M)$ ? What's the connection of the flow of $\{f,- \}$ to the function $f$ ? Am I correct in saying that $\{f,g \} = 0$ means that $g$ is constant along the flow of $\{f,- \}$ ? If that helps, my background is primarily in algebra, so I'm asking about a physicist's/geometer's way of thinking about this.","['symplectic-geometry', 'abstract-algebra', 'hamilton-equations', 'physics', 'poisson-geometry']"
3069864,Affine charts are dense in projective space,"Given a field $k$ , we define the scheme-theoretic $n$ -th affine space over $k$ by $\mathbb{A}^n_k=\text{Spec}(k[X_1,\dots,X_n])$ and the $n$ -th projective space over $k$ by $\mathbb{P}^n_k=\text{Proj}(k[X_0,\dots, X_n])$ . We know $\mathbb{P}^n_k$ is covered by $n+1$ affine charts given by $D_+(X_i)=\mathbb{P}^n_k\smallsetminus V_+(X_i)$ for $i=0,\dots, n$ , each isomorphic to $\mathbb{A}^n_k$ . I was asking myself if - as it happens in the naive case - each of those charts is dense in $\mathbb{P}^n_k$ . Here is my attempt. Take e.g. $D_+(X_0)$ . Then $D_+(X_0)$ being dense in $\mathbb{P}^n_k$ is equivalent to $V_+(X_0)$ having empty interior. Suppose there exists $f\in (X_0,\dots, X_n)$ s.t. $D_+(f)\subset V_+(X_0)$ . Then every prime $\mathfrak{p}\in \mathbb{P}^n_k$ s.t. $f\notin \mathfrak{p}$ is s.t. $X_0\in \mathfrak{p}$ , which means $D_+(fX_0)=\emptyset$ , i.e. $V_+(fX_0)=V_+(0)=\mathbb{P}^n_k$ , and thus $f=0$ since $k[X_0,\dots, X_n]$ is an integral domain. Is this proof correct? Does anyone know a shorter way to prove it?","['algebraic-geometry', 'projective-schemes', 'schemes', 'projective-space']"
3069892,Evaluating $\int_{1}^{\infty}\frac{1}{x^2}\prod_{k=1}^{n}\left[1-\frac{1}{2k+x}\right]\text{d}x$,"I'm trying to find a closed form for the following integral, for all $n\in\mathbb{N}^*$ : $$\int_{1}^{\infty}\frac{1}{x^2}\prod_{k=1}^{n}\left[1-\frac{1}{2k+x}\right]\text{d}x$$ As suggested by Zacky, a $x=\frac{1}{t}$ substitution gives : $$\int_{0}^{1}\prod_{k=1}^{n}\left[\frac{(2k-1)x+1}{2kx+1}\right]\text{d}x$$ The thing is that, for every specific $n\in\mathbb{N}^*$ , mathematica is able to compute the integral in terms of logarithms only. In fact, it can always find the antiderivative in terms of logarithms. What I'm looking for is a general expression for all $n\in\mathbb{N}^*$ , if given that it exists. Now the integrand is an already factorized rational function, so I assumed I could tackle this with some partial fraction decomposition, but I'm not good enough with this method to lead to anything. I'm also not familiar with complex analysis so I haven't tried contour integration, but if you think it can lead to anything, feel free to do so. Any suggestion ?","['integration', 'definite-integrals', 'real-analysis']"
3069939,"The set $H=\{(x,y)\in \Bbb{R^2}:\;3x+2y=5 \},$ is a closed subset of $\Bbb{R^2}$","Kindly check if my proof is correct. Thanks for your time and efforts. MY PROOF For all $n\in \Bbb{N},$ let $(x_n,y_n)\in H$ such that $(x_n,y_n)\to (x,y),$ as $n\to \infty.$ We show that $(x,y)\in H.$ However, $(x_n,y_n)\in H$ implies $3x_n+2y_n=5,$ for all $n\in \Bbb{N}.$ As $n\to \infty, $ $$ 3x+2y=\lim\limits_{n\to\infty}(3x_n+2y_n)=\lim\limits_{n\to\infty}5=5 .     $$ Hence, $(x,y)\in H$ which implies that the set, $H=\{(x,y)\in \Bbb{R^2}:\;3x+2y=5 \},$ is a closed subset of $\Bbb{R^2}$","['linear-algebra', 'analysis']"
3069953,If $\{X_\alpha\}$ is a collection of mutually disjoint meas. subsets of $\mathbb{R} \rightarrow$ then at most countable of them has positive measure.,"Let $\{X_\alpha\}_{\alpha \in I}$ be a collection of mutually disjoint measurable subsets of $\mathbb{R}. Show that at most countable of them has positive measure. I want to see if my proof is correct. Proof: Let $\{X_\alpha\}_{\alpha \in \Gamma \subset I}$ be the subcollection such that $m(X_\alpha) > 0, \forall \alpha \in \Gamma$ . Since each set is measurable and has positive measure, $$ \forall X_{\alpha}, \alpha \in \Gamma, \exists O_{\alpha} = (a_\alpha,b_\alpha) \mbox{ such that } O_\alpha \subset X_{\alpha}$$ Now we must show that $|\Gamma| \leq |\mathbb{N}|$ Let $f: \{O_{\alpha}\}_{\alpha \in \Gamma} \to \mathbb{Q}$ be the function that $$f(O_\alpha) = q_{\alpha}, \mbox{ where }q_\alpha \in (a_\alpha,b_\alpha)$$ This is possible because $\mathbb{Q}$ is dense in $\mathbb{R}$ This function is clearly injective since $\{O_\alpha\}$ is a disjoint collection of open sets. Hence, the same rational can't be in two different open intervals from this collection. Since $f$ is injective, $\{O_\alpha\}_{\alpha \in \Gamma}| = |\Gamma| \leq |\mathbb{Q}| = |\mathbb{N}| = \aleph_0$ Q.E.D","['measure-theory', 'proof-verification', 'real-analysis']"
3069963,Is the Lie bracket always invariant under coordinate transformations?,"This is my first question on StackExchange. I think it's probably quite easy, but it's been baffling me for a while. I'm  doing computations to determine invariant properties of the quantity $X\circ Y= \nabla_Y X$ where $X$ and $Y$ are vector fields,  when we change coordinate choices (when $\nabla$ is a flat torsion-free connection). So naturally, we start with the simplest case, $\mathbb{R}$ with its usual connection. However we encounter the following problem. Start with vector fields $X=f(x)dx$ and $Y=g(x)dx.$ We have $X\circ Y = g(x)\frac{df}{dx}dx.$ We then make a general change of coordinates $\phi(y)=x.$ So $X=f(\phi(y))\frac{d\phi}{dy}dy$ and $Y=g(\phi (y))\frac{d\phi}{dy}dy.$ Then $X \circ' Y = g(\phi(y))\frac{d\phi}{dy}(\frac{df(\phi(y))}{d \phi}\frac{d \phi}{dy}+f(\phi(y))\frac{d^2\phi}{d y^2})dy$ in the new coordinates. Attempting to return back to $x$ -coordinates we get $X \circ' Y =(\frac{d \phi}{dy})^2(g(x)\frac{df}{dx}dx)+ g(x)f(x)\frac{d^2 \phi}{dy^2}dx$ . The rightmost term is symmetric in $X$ and $Y$ and for the purposes of this question can be neglected. In this case $X\circ Y-Y\circ X = [Y,X]$ as $\nabla$ is flat torsion free. But in the expression above we pick up a factor of $(\frac{d\phi}{d y})^2$ when we compute the bracket. But the bracket is coordinate independent! What's gone wrong?","['manifolds', 'calculus', 'vector-fields', 'differential-geometry']"
3070104,How do I calculate the F value from the Null Hypothesis of equality of means,"This question comes from a practice exam, the following contains the question and the answer provided. However my teachers for this exam are non-respondent and I really need someone to please help explain this to me. I attach the answer below as well. In an agricultural trial, yields are recorded for samples of plants from four different varieties of a crop: Variety A: 15, 14, 12, 13 Variety B: 11, 18, 13 Variety C: 18, 25, 19, 20 Variety D: 19, 20, 24 It is revealed that varieties C and D were genetically modified (GM) plants. Varieties A and B were not. By fitting an appropriate model to the data, test the hypothesis: H0: The mean yields of the two GM varieties are equal and the mean yields of the two non-GM varieties are equal. My questions are:
- Where this equation for F comes from. I understand the need to test for it, but in my experience it is calculated from doing (MS Variable / MS Error).
What the values of p_f and p_r represent? My rationale for the latter is that they represent the upper limits of i from the ""appropriate model"". If anyone could explain this to me then that would be very much appreciated. Thank you.","['statistical-inference', 'statistics', 'hypothesis-testing']"
3070109,"Find the number of functions $h:\{1,2,3,\ldots ,2n\}\to \{-1,1\}$ such that $\sum_{j=1}^{2n}h(j)>0$ .","Fix $n\in \Bbb N$ .
  Find the number of functions $h:\{1,2,3,\ldots ,2n\}\to \{-1,1\}$ such that $$\sum_{j=1}^{2n}h(j)>0$$ My try : Let $A=\{x:h(x)=1\}$ and $B=\{x:h(x)=-1\}$ In order for $\sum_{j=1}^{2n}h(j)>0$ we should have $|A|>|B|$ where $|A|$ denotes cardinality of set $A$ . Now Cardinality of $B$ can have values from the set $\{0,1,2,3,\ldots ,n-1\}$ When we fix $B$ 's cardinality the cardinality of $A$ gets fixed too. Thus if $B$ has cardinality $j$ then the $j$ elements can be chosen from $\{1,2,3,\ldots ,2n\}$ in $\binom{2n}{j}$ ways. Thus the total number of functions become $$\sum_{j=0}^{n-1}\binom{2n}{j}$$ Is my solution correct?Kindly check","['proof-verification', 'combinatorics']"
3070122,Smooth map with null differential at each point are constant on the connected component of the domain,"Let $F:M\to N$ be a smooth map between smooth manifolds $M$ and $N$ (with or without boundary). I want to show that $dF_p:T_pM\to T_{F(p)}N$ is the zero map for each $p\in M$ if and only if $F$ is constant on each component of $M$ . Here is my argument: Suppose $F$ is constant on each component of $M$ , and let's show that $dF_p:T_pM\to T_{F(p)}N$ is the zero map for each $p\in M$ . Let $p\in M$ and let $U$ be the component of $M$ containing $p$ . Since $M$ is locally path connected, I know that $U$ is open in $M$ . By hypothesis we have $F_{|U}:U\to N$ is consant. Then $d(F_{|U})_p:T_pU\to T_{F(p)}N$ is the zero map: let be $v\in T_pU$ and $f\in C^{\infty}(N)$ , then $d(F_{|U})_p(v)(f)=v(f\circ F_{|U})=0 $ since $f\circ F_{|U}$ is costant from $U$ to $\mathbb{R}$ . Since $d(\iota)_p:T_pU \to T_pM$ is isomprhism, and since we have that $dF_p\circ d(\iota)_p=d(F_{|U})_p$ , we have that also $dF_p$ is the zero map. We have to prove the converse (But here my problems begin) The best I have thought is this: For simplicity suppose $M$ itself is connected. We know that $dF_p:T_pM\to T_{F(p)}N$ is the zero map for each $p∈M$ and we have to prove that $F:M→N $ is constant. I want to show that $F$ is locally constant, i.e. each point $p$ in $M$ has an open neighborhood in $M$ such that $F$ is constant on this neighborhood. Let $p\in M$ and let $(U,\phi=(x^1,\dots,x^m))$ be a chart on $M$ in $p$ . Then $\{{\frac{\partial}{\partial x^i}}|_p\}$ is a basis for $T_pM$ . By hypothesis we know that $dF_p(\frac{\partial}{\partial x^i}|_p)(f)=0$ for each $f∈C^∞(N)$ , i.e. $\partial_i|_{\phi(p)}(f\circ F \circ \phi^{-1})=0$ . We can suppose that $U$ and thus $\phi(U)$ are connceted, so by Ordinary Analysis we have thaht $f\circ F \circ \phi^{-1}$ is constant on $\phi(U)$ . But $\phi$ is a diffeomorphism, so we have thaht $f \circ F:U\to N$ is constant, for each $f∈C^∞(N)$ . Now suppose there are $p\ne q \in M$ such that $F(p) \ne F(q)$ . I want to construct a function $f∈C^∞(N)$ such that $f(F(p))\ne f(F(q))$ . The best I come out is: suppose there is a smooth chart $(V,\psi)$ on $N$ containing $F(p)$ and $F(q)$ and such that there is $K$ closed subset of $N$ such that $K\subseteq V$ . Since $\psi $ is injective, then $\psi (F(p))\ne \psi( F(q))$ , so they differ by at least a component, say the $j$ component. Let $\pi_j:\mathbb{R}^n\to \mathbb{R}$ the $j$ projection, and consider $\psi \circ \pi_j:\psi(V)\to \mathbb{R}$ . Extend this function to a function $f∈C^∞(N)$ such that $f$ and $\psi \circ \pi_j$ agree on $K$ . Then we have $f(F(p))\ne f(F(q))$ which is a contraddiction. I know that this is quite completely wrong (I did many not-necessarily-true assumptions). And maybe this argument does not work in the case of manifolds with boundary. So can anyone help me with observations/ sugestions/ hints, or even a full solution? Thank you. I mention that this is Problem 3.1 in John Lee's Book ""Introduction to smooth manifolds, 2 edition"" EDIT Thanks to the hint of @Ted Shifrin I come with another argument. Let's start from a fact that I konw: if $A$ is an open subset of $\mathbb{R}^n$ and $A$ is connected, then each smooth function $f:A\to \mathbb{R}$ whose partial derivatives are zero in $A$ , is constant. Now we can generalize this as: if $A$ is an open subset of $\mathbb{R}^n$ and $A$ is connected, then each smooth function $f:A\to \mathbb{R}^m$ such that all component functions have partial derivatives that are zero in $A$ , is constant. (We can deduce this from the former, simply noting that each component function is constant, right?) Now, let $p\in M$ and $(U,\phi)$ smooth chart on $M$ in $p$ and $(V,\psi)$ smooth chart on $N$ in $F(p)$ with $F(U)\subseteq V$ . Then $\psi \circ F \circ \phi^{-1}:\phi(U)\to \psi (V)$ is smooth, and we can suppose $U$ is connceted, and so is $\phi(U)$ . We have that $d(\psi \circ F \circ \phi^{-1})_{\phi(q)}=d\psi_{F(q)} \circ dF_q \circ d(\phi^{-1})_{\phi(q)}$ and since $dF_q$ is zero for all $q \in U$ , then also $d(\psi \circ F \circ \phi^{-1})_{x}$ is zero for all $x \in \phi(U)$ . But this is the jacobian matrix of $\psi \circ F \circ \phi^{-1}:\phi(U)\to \psi (V)$ . So by the above discussion we have that $\psi \circ F \circ \phi^{-1}:\phi(U)\to \psi (V)$ is constant, and then $F$ is constant on $U$ , right? Then, since $F$ is locally constant, we have that $F$ is constant on each component of $M$ , right? Is my new argument correct? Have I used well the Ted's hint?","['multivariable-calculus', 'smooth-functions', 'smooth-manifolds', 'differential-geometry']"
3070142,Group of order $p^{\alpha}q$ is not simple.,"$|G|=p^{\alpha}q$ , where $p,q$ are distinct primes, $\alpha \geq 1$ . Show $G$ is not simple. I am trying to follow a proof and I understand all of it except one part which is blocking me. The proof begins by assuming $G$ is simple. First it shows that the $p$ Sylow subgroups cannot intersect trivially (counting argument). Then take $P_1 \neq P_2$ to be two Sylow subgroups such that their intersection, $D=P_1\cap P_2$ is maximal. Then $D < P_1,P_2$ so $D<N_{P_1}(D)$ and $D<N_{P_2}(D)$ . Consider $N_G(D)$ . If $N_G(D)$ is a $p$ subgroup then by Sylow it is contained inside $P_3$ , which is a $p$ Sylow group. $\textbf{This next line I cannot understand, this is what I want explained.}$ $P_3 \cap P_1 \geq N_{P_1}(D) > D \implies P_3 = P_1$ (*). I don't understand why the intersection contains the normalizer.
I will include the rest of the proof for anyone who looks here in the future. And similarly $P_3 = P_2$ , so $P_1 = P_2$ , so $N_G(D)$ is not a $p$ group. Thus a $q$ Sylow group lies in $N_G(D)$ . Then $P_1N_G(D) = G$ and if we pick $g\in G$ , then $g=hx$ , $h\in P, x\in N_G(D)$ and then $P_1^g = P_1^{hx}=P_1^x \geq D^x = D $ , and thus $D$ lies in every $p$ Sylow subgroup of $G \implies 1 < D \leq \cap_{g\in G}P_1^g \triangleleft G$ , which is a contradiction.","['proof-explanation', 'group-theory', 'sylow-theory', 'finite-groups']"
3070213,Rolling $4$ dice and multiplying the results. What is the probability that the product is divisible by $5$ or has $5$ as the least significant digit?,"Four fair dice are rolled and the four numbers shown are multiplied together. What is the probability that this product (a) Is divisible by $5$ ? (b) Has last digit $5$ ? MY ATTEMPT a) A number is divisible by $5$ if and only if it has at least one factor equal to $5$ . Let us denote by $F$ the event ""it occurs at least one number five"". Hence the sought probability is given by \begin{align*}
\textbf{P}(F) = 1 - \textbf{P}(F^{c}) = 1 - \frac{5^{4}}{6^{4}} = 1 - \left(\frac{5}{6}\right)^{4}
\end{align*} b) Here is the problem. I am not able to describe properly the target results. Am I on the right track? Can someone please help me to solve it? Any help is appreciated.","['combinatorics', 'probability']"
3070224,Verify Gauss divergence theorem of $F=xi+yj+zk$ over the sphere $x^{2}+y^{2}+z^{2}=a^{2}$,"Verify Gauss divergence theorem of $F=xi+yj+zk$ over the sphere $x^{2}+y^{2}+z^{2}=a^{2}$ When I evaluate taking normal $N=k$ , I get the answer $2\pi a^{3}$ .
But when I take normal to the surface by taking out gradient  of $f(x,y,z)= x^{2}+y^{2}+z^{2}$ I am able to verify theorem. But my course instructor told we can take normal $N=k$ when whenever we are able to translate system in $xy$ plane . 
And if plane is given then we have to find gradient otherwise not. Please clarify!!!","['multivariable-calculus', 'calculus', 'vectors', 'vector-analysis']"
3070251,Is there a simple formula for $\binom{2n}{n} \pmod{n^3}$?,"Is there a simple formula for the following? $$f(n) = \binom{2n}{n} \pmod{n^3}$$ I know $f(n) = 2$ iff $n$ is prime and greater than $3$ , but I don't know anything about composite numbers.","['modular-arithmetic', 'prime-numbers', 'binomial-coefficients', 'combinatorics', 'probability']"
3070254,Why does the progression of values of $\sin(x)$ seem so arbitrary?,"I don't know how to ask this in a way that doesn't sound a little stupid, but what I mean is, the progression of values of say, sin(x) seems really arbitrary. Is there an intuitive way to understand why, for example: x            y
-------   -------------
sin(45) = 0.85090352453
sin(46) = 0.90178834764 What is determining the amounts that Y goes up or down by as X changes? And even better, is there something (fairly accessible to non-mathematicians) I can read that can explain this sort of thing in a very intuitive way? I don't want to prove that it works, just understand how come. Thanks!",['trigonometry']
3070256,How to prove A→B is equivalent to ¬B→¬A? [duplicate],This question already has answers here : $(A \implies B) \implies (\neg B \implies \neg A)$? (6 answers) Closed 5 years ago . A→B is equivalent to ¬B→¬A This comes from a member called 6005 as an answer to my question: Why $F \rightarrow T \implies \neg F \rightarrow \neg T \implies T \rightarrow F$ is wrong? Any proof or reference for this rule ?,"['logic', 'discrete-mathematics']"
3070276,"Proof of divergence of power series in Theorem 3.39, baby Rudin","This is theorem 3.39 from baby Rudin. Here { $c_n$ } is a complex sequence, $z$ is a complex number. Given the power series $\Sigma c_nz^n$ , put $$\alpha = \text{lim sup} \sqrt[n] {|c_n|}, R = \frac{1}{\alpha}.$$ (If $\alpha = 0, R = +\infty;$ if $\alpha  =+\infty, R = 0.$ ) Then $\Sigma c_nz^n$ converges if $|z| < R$ , and diverges if $|z| > R$ . Proof Put $a_n = c_nz^n$ , and apply the root test: $$\text{lim sup} \sqrt[n] {|a_n|} = |z|\text{lim sup} \sqrt[n] {|c_n|} = \frac{|z|}{R}.$$ Using notation above, the root test (Theorem 3.33) tests for absolute convergence of the series $\sum|a_n|$ . Therefore, if $\text{lim sup} \sqrt[n] {|a_n|} < 1$ , the series $\sum|a_n|$ converges and hence $\sum a_n$ (this is true for any complex series; see https://math.stackexchange.com/a/1655838/606584 ). But why is it true that if $\text{lim sup} \sqrt[n] {|a_n|} > 1$ (or equivalently $|z| > R$ ), $\sum a_n$ diverges? Since, for as far as I can tell, $\text{lim sup} \sqrt[n] {|a_n|} > 1$ only implies that $\sum|a_n|$ diverges, which doesn't in general imply that $\sum a_n$ (think alternating harmonic series). So my question is: how is it so that when $|z| > R$ , the power series diverges?","['complex-analysis', 'convergence-divergence', 'power-series', 'real-analysis']"
3070288,"Prove or give a counterexample: If $A$ is a subset of $B$ and $B$ belongs to $C$, then $A$ belongs to $C$.","Prove or give a counterexample: If $A$ is a subset of $B$ and $B$ belongs to $C$ , then $A$ belongs to $C$ . I think this is false because of the counterexample $$
\begin{align}
A &= \{1,2\}\\
B &= \{1,2,3\}\\
C &= \{\{1,2,3\}\}
\end{align}
$$ but I am not sure if I am right.","['elementary-set-theory', 'discrete-mathematics']"
3070297,What is the best algorithm to find the inverse of matrix $A$,"I'm going to build a C-library so I can do linear algebra at embedded systems. It's most for machine learning. https://github.com/DanielMartensson/EmbeddedAlgebra/ Anyway, I need to compute inverse of matrix $A$ . What is the absolute best algorithm to use for that? Remember it must be a numerical algorithm. Which one should I use? Analytic solution $\mathbf{A}^{-1}={1 \over \begin{vmatrix}\mathbf{A}\end{vmatrix}}\mathbf{C}^{\mathrm{T}}={1 \over \begin{vmatrix}\mathbf{A}\end{vmatrix}}
\begin{pmatrix}
\mathbf{C}_{11} & \mathbf{C}_{21} & \cdots & \mathbf{C}_{n1} \\
\mathbf{C}_{12} & \mathbf{C}_{22} & \cdots & \mathbf{C}_{n2} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{C}_{1n} & \mathbf{C}_{2n} & \cdots & \mathbf{C}_{nn} \\
\end{pmatrix}$ Cholesky decomposition $\mathbf{A}^{-1} = (\mathbf{L}^{*})^{-1} \mathbf{L}^{-1}$ Eigendecomposition $\mathbf{A}^{-1}=\mathbf{Q}\mathbf{\Lambda}^{-1}\mathbf{Q}^{-1}$ Cayley–Hamilton method $\mathbf{A}^{-1} = \frac{1}{\det (\mathbf{A})}\sum_{s=0}^{n-1}A^s$ Newton's method $X_{k+1} = 2X_k - X_k A X_k$ Neuman series $\lim_{n \to \infty} (\mathbf I - \mathbf A)^n = 0$ Gaussian elimination $A =
\begin{bmatrix}
2 & -1 & 0 \\
-1 & 2 & -1 \\
0 & -1 & 2
\end{bmatrix}$ To find the inverse of this matrix, one takes the following matrix augmented by the identity, and row reduces it as a 3 × 6 matrix: $ 
[ A | I ] = 
\left[ \begin{array}{rrr|rrr}
2 & -1 & 0 & 1 & 0 & 0\\
-1 & 2 & -1 & 0 & 1 & 0\\
0 & -1 & 2 & 0 & 0 & 1
\end{array} \right]$ By performing row operations, one can check that the reduced row echelon form of this augmented matrix is: $[ I | B ] = 
\left[ \begin{array}{rrr|rrr}
1 & 0 & 0 & \frac34 & \frac12 & \frac14\\[3pt]
0 & 1 & 0 & \frac12 & 1 & \frac12\\[3pt]
0 & 0 & 1 & \frac14 & \frac12 & \frac34
\end{array} \right]$","['matrices', 'numerical-linear-algebra', 'inverse']"
3070332,Can Erdős-Turán $\frac{5}{8}$ theorem be generalised that way?,"Suppose for an arbitrary group word $w$ ower the alphabet of $n$ symbols $\mathfrak{U_w}$ is a variety of all groups $G$ ,  that satisfy an identity $\forall a_1, … , a_n \in G$ $w(a_1, … , a_n) = e$ . Is it true, that for any group word $w$ there exists a positive real number $\epsilon (w) > 0$ , such that any finite group $G$ is in $\mathfrak{U_w}$ iff $$\frac{\lvert\{(a_1, … , a_n) \in G^n : w(a_1, … , a_n) = e\}\rvert}{{|G|}^n} > 1 - \epsilon(w)?$$ How did this question arise? There is a widely known theorem proved by P. Erdős and P. Turán that states: A finite group $G$ is abelian iff $$\frac{|\{(a, b) \in G^2 : [a, b] = e\}|}{{|G|}^2} > \frac{5}{8}.$$ This theorem can be rephrased using aforementioned terminology as $\epsilon([a, b]) = \frac{3}{8}$ . There also is a generalisation of this theorem, stating that a finite group $G$ is nilpotent of class $n$ iff $$\frac{|\{(a_0, a_1, … , a_n) \in G^{n + 1} : [ … [[a_0, a_1], a_2]… a_n] = e\}|}{{|G|}^{n + 1}} > 1 - \frac{3}{2^{n + 2}},$$ thus making $\epsilon([ … [[a_0, a_1], a_2]… a_n]) = \frac{3}{2^{n + 2}}$ . However, I have never seen similar statements about other one-word varieties being proved or disproved, despite such question seeming quite natural . . . Actually, I doubt that the conjecture in the main part of question is true. However, I failed to find any counterexamples myself.","['universal-algebra', 'conjectures', 'finite-groups', 'combinatorics', 'group-theory']"
3070418,Closed form for $\sum\limits_{n=2}^{\infty}\frac1{n^3-1}$,"I am investigating (just for fun) the sum $$S=\sum_{n=2}^{\infty}\frac1{n^3-1}$$ Wolfram Alpha gives me the 'value' $$S=-\frac13\sum_{\{\omega\,:\,\omega^3+6\omega^2+12\omega+7=0\}}\frac{\psi_{0}(-\omega)}{\omega^2+4\omega+4}$$ Where $\psi_{0}(s)$ is the di-gamma function. I would like to know how this is found. My attempts: recall that $$x^3-1=\prod_{k=1}^{3}(x-\alpha_k)$$ Where $\alpha_k=\exp\frac{2i\pi(k-1)}{3}$ . Hence $$S=\sum_{n\geq2}\prod_{k=1}^3\frac1{n-\alpha_k}$$ I have shown in other posts that given some non-zero sequence $\{a_k:k=1,2,..,m\}$ where $j\neq k\iff a_j\neq a_k$ then $$\prod_{k=1}^{m}\frac1{x-a_k}=\sum_{k=1}^{m}\frac1{x-a_k}\prod_{j=1\\j\neq k}^{m}\frac1{a_k-a_j}$$ So $$S=\sum_{n\geq2}\bigg[\sum_{k=1}^{3}\frac1{n-\alpha_k}\prod_{j=1\\j\neq k}^{3}\frac1{\alpha_k-\alpha_j}\bigg]$$ Setting $b(k)=\prod_{j=1\\j\neq k}^{3}\frac1{\alpha_k-\alpha_j}$ , $$S=\sum_{n\geq2}\sum_{k=1}^3\frac{b(k)}{n-\alpha_k}$$ $$S=b(1)\sum_{n\geq2}\frac1{n-\alpha_1}+b(2)\sum_{n\geq2}\frac1{n-\alpha_2}+b(3)\sum_{n\geq2}\frac1{n-\alpha_3}$$ Then focusing on $$\begin{align}
S_k=&\sum_{n\geq2}\frac1{n-\alpha_k}\\
=&\sum_{n\geq2}\int_0^1x^{n-1}\frac{\mathrm dx}{x^{\alpha_k}}\\
=&\int_0^1\frac{1}{x^{\alpha_k-1}}\sum_{n\geq0}x^{n}\mathrm dx\\
=&\int_0^1x^{1-\alpha_k}(1-x)^{-1}\mathrm dx\\
=&\text{???}
\end{align}$$ for this final integral I considered using the beta function but that wouldn't work because $\Gamma(0)$ is undefined. Plus I'm not sure if $S_k$ even converges. As you can see, I'm stuck. Could I have a little help? Thanks Edit: It can be seen here that there seems to be a pattern to evaluations of the function $$S(k)=\sum_{n\geq2}\frac1{n^{2k+1}-1},\qquad k\in\Bbb N$$ Which seem to be in the form $$S(k)=-\frac1{1+2k}\sum_{\{\omega\,:\,R_k(\omega)=0\}}\frac{\psi_0(-\omega)}{P_k(\omega)}$$ Where $R_k(x)$ is a polynomial of degree $k$ and $P_k(x)$ is a polynomial of degree $k-1$ .","['analytic-number-theory', 'closed-form', 'sequences-and-series']"
3070440,Ways to show that $\int_{0}^{1}((1-x^r)^{1/r}-x)^{2k}dx=\frac{1}{2k+1}$,"Through some calculation, I found that for all $r>0$ $$
\begin{array}{rcl}
{\displaystyle\int_{0}^{1}\left[\left(1 - x^{r}\right)^{1/r} - x\right]^{2}\,\mathrm{d}x} &  {\displaystyle =} &
{\displaystyle{1 \over 3}}
\\
{\displaystyle\int_{0}^{1}\left[\left(1 - x^{r}\right)^{1/r} - x\right]^{4}\,\mathrm{d}x} &  {\displaystyle =} &
{\displaystyle{1 \over 5}}
\\
{\displaystyle\int_{0}^{1}\left[\left(1 - x^{r}\right)^{1/r} - x\right]^{6}\,\mathrm{d}x} &  {\displaystyle =} &
{\displaystyle{1 \over 7}}
\end{array}
$$ It seems like for $k \in \mathbb{N}$ $$
\int_{0}^{1}
\left[\left(1 - x^{r}\right)^{1/r} - x\right]^{2k}\mathrm{d}x = {1 \over 2k + 1}
$$ I want to prove this general form. Someone suggested to make the substitution $$y=(1-x^r)^{1/r}$$ Let $n=2k$ , and I rewrote the integral into $$\int_{0}^{1}((1-x^r)^{1/r}-x)^ndx=\int_{0}^{1}(y-x)^ndx$$ and tried to use the binomial formula: $$(y-x)^{n}=\sum _{k=0}^{n}{\binom {n}{k}}(-1)^{n-k}x^{n-k}y^{k}$$ The integral then becomes $$\begin{align}
\int_{0}^{1}(y-x)^ndx&=\int_{0}^{1}\sum _{k=0}^{n}{\binom {n}{k}}(-1)^{n-k}x^{n-k}y^{k}dx\\
&=\sum _{k=0}^{n}{\binom {n}{k}}(-1)^{n-k}\int_{0}^{1}x^{n-k}y^{k}dx\\
&=\sum _{k=0}^{n}{\binom {n}{k}}(-1)^{n-k}\int_{0}^{1}x^{n-k}(1-x^r)^{k/r}dx\\
\end{align}$$ Now I think I need to use Beta function: $$B(x,y) = \frac{(x-1)!(y-1)!}{(x+y-1)!}= \int_{0}^{1}u^{x-1}(1-u)^{y-1}du=\sum_{n=0}^{\infty}\frac{{\binom{n-y}{n}}}{x+n}$$ Am I on the right track? Are here any easier ways to prove the general form?","['integration', 'calculus', 'definite-integrals']"
3070441,Role of Dirac operators in Index Theorems,"I'm trying to approach the Atiyah-Singer Index Theorem by getting an overview of the area. One thing that confuses me a lot is that some treatments give (and hence prove) the theorem for Dirac operators , while other sources not even mention Dirac operators and work just with general elliptic operators . From what I read seems that the Index theorem for Dirac operators implies the theorem for general elliptic operators. Indeed the Dirac operator is some sense ""the"" elliptic operator. I've heard this claim can be justified with some K-theory (which I know nothing about at the moment). Can someone explain in simple (i.e. vague) terms this idea? Why the Dirac operator is ""the"" elliptic operator? Apart from the K-theoretical justification, are there any methods of showing this? Can this be seen from the Analysis side? Thanks in advance","['elliptic-operators', 'topological-k-theory', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
3070450,Partial derivative of coordinates with respect to function,Let $f : \mathbb{R}^n \rightarrow \mathbb{R}^n$ . Then $$\frac{\partial f^i}{\partial x^j} = (\nabla f)^i_j$$ where $\nabla f$ is the Jacobian matrix of $f$ . When reading this paper I came across the expression $$\frac{\partial x^i}{\partial f^j}$$ Should I interpret this as $$\frac{\partial x^i}{\partial f^j} = ((\nabla f)^{-1})^i_j$$,"['matrices', 'jacobian', 'multivariable-calculus', 'vector-analysis', 'differential-geometry']"
3070465,Is it possible to put a topology on Turing-recognizable languages to express density among all the languages?,"In a Calculability and complexity course I had at univeristy, we proved that there exist languages that are not Turing-recognizable basiclly using Cantor's diagonal argument (the set of all languages is uncountable and the set of Turing-recognizable languages is countable). This immediately brought the analogy with $\mathbb Q$ which is countable inside $\mathbb R$ which is not. But we have a topology on $\mathbb R$ (and thus on $\mathbb Q$ ) which allows us to show that $\mathbb Q$ is a dense subset of $\mathbb R$ (also it's a metric space). A question then popped in my head: is there any way to put a topology onto the set of languages that would lead to a smilar result, i.e. density of the Turing-recognizable lanuages among all the languages? Note that I have no idea if this even makes sense: I'm not really into theoretical computing, and I have no idea if the notion of ""closeness"" between languages makes any sense. This is just a question I've hold for too long now and I had to ask if somebody already answered this or if this is just a pointless question.","['general-topology', 'computability', 'turing-machines', 'computer-science']"
3070467,Linear combination of two vectors in complex space,"Let $\mathbf{x},\mathbf{y} \in \mathbb{C}^2$ be two linearly independent vectors in two dimensional complex space. Assume that $\|\mathbf{x}\|\leq \|\mathbf{x} \pm \mathbf{y}\|$ . I want to show (or understand why the following does not hold) that $\|\mathbf{x}\| \leq \|\mathbf{x} + \alpha \mathbf{y}\|$ for all $\alpha \in \mathbb{C}$ such that $|\alpha|\geq 1$ . Any hints as to how to prove the inequality would be greatly appreciated, and if it doesn't necessarily hold, what other constraints need to be imposed on $\alpha$ ? So far I have managed to show the trivial case that this holds where we have either $\|\mathbf{x}\|>2\|\mathbf{y}\|$ or $\|\mathbf{y}\|>2\|\mathbf{x}\|$ . I am struggling to prove it for the other cases.","['vectors', 'linear-algebra', 'geometry', 'complex-numbers']"
3070490,What's the precise definition of coordinates in Euclidean space?,"I have a loose understanding of what coordinates are, but not something rigorous or concrete. For example take the statement of this result below When the author says let $x = (x^1, \dots x^n)$ denote the ""standard coordinates"" on $U$ and $y = (y^1, \dots, y^m)$ those on $\widetilde{U}$ what precisely does the author mean? For example there is a very clear and rigorous definition of what a basis for a vector space is, but I can't seem to find that for coordinates. The closest thing I know to a definition for coordinates is the following: If I have a smooth manifold say $M$ of dimension $n$ and a smooth chart $(U, \phi)$ , then for any $p \in U$ we have $\phi(p) = (x^1(p), \dots, x^n(p))$ where the $x^i : U \to \mathbb{R}$ are the component functions of the homeomorphism $\phi$ and the collection of component functions $(x^1, \dots, x^n)$ are called local coordinates on $M$ . However this notion doesn't seem to apply to the corollary above, because it seems ""coordinates"" take on a different meaning above, because for example $$\frac{\partial G^i}{\partial y^k}$$ (in the statement of the result above) doesn't make sense since $y^k$ interpreted this way would actually be a function and it's meaningless to take the partial derivative of a function with respect to another function. Furthermore the only definition of the partial derivative of a function that I'm familiar with is the following Definition: Let $U \subseteq \mathbb{R}^m$ be an open set and let $f : U \to \mathbb{R}$ . The $j$ -th partial derivative of $f$ at $a$ is defined to be the directional derivative of $f$ at $a$ with respect to the basis vector $e_j = (0, \dots, 1, \dots, 0)$ (where $1$ is in the $j$ -th position) provided the derivative exists $$\frac{\partial f}{\partial x^j} = \lim_{t \to 0} \frac{f(a +te_j) - f(a)}{t}$$ and in this definition above I just think of $x^j$ as a reminder that I'm taking the directional derivative with respect to the $e_j$ basis vector on $\mathbb{R}^m$ . How do I reconcile the definition above with what the authors mean by standard coordinates? Basically the question I'm asking is, what does ""let $x = (x^1, \dots, x^n)$ denote the standard coordinates on $U$ "" mean? Or more generally if $V$ is an open subset of $\mathbb{R}^k$ what would ""let $z = (z^1, \dots, z^k)$ denote the coordinates on $V$ "" mean?","['euclidean-geometry', 'multivariable-calculus', 'differential-geometry']"
3070497,What is the asymptotic form for the sum of the reciprocals of the first $n$ primes?,"It is well known that the sum of the reciprocals of the primes $p$ less than or equal to a maximum value $n$ is asymptotic to $\ln \ln n$ : $$\sum_{p \leq n} \frac{1}{p} \sim \ln \ln n.$$ (The next subleading term on the RHS is the Meissel–Mertens constant .) But what's the asymptotic form for the sum of the reciprocals of the first $k$ primes? That is, if $p_k = \{2, 3, 5, 7, 11, \dots\}$ is the sequence of primes, then what is the asymptotic form of $$\sum_{k=1}^n \frac{1}{p_k}?$$ The latter expression seems to me to be a closer variant of the famous asymptotic relation $$\sum_{k=1}^n \frac{1}{k} \sim \ln n$$ for the harmonic numbers.","['number-theory', 'asymptotics', 'harmonic-numbers', 'prime-numbers']"
3070526,What inspires people to define linear maps?,"I am currently self studying Linear Algebra Done Right . I am doing OK and currently on Chapter 3 Linear Maps. My understanding now is that a map is like a function that maps something to another thing. Why people like to particularly define linear maps? Just curious..maybe there are non-linear maps? Another stupid question is that is linear maps really look like a straight line in some specific examples, like, $u, v $ is 1 dimensional vector space. And then $T(u+v) = T(u) + T(v)$ Or put it in another way why it is called linear map. And finally, why linear maps have the additivity and homogeneity properties?",['linear-algebra']
3070531,Direct Proof that Continuous Functions Satisfy Epsilon-Delta Condition,"If we define a continuous real function $f: M \to R$ as one that preserves sequential convergence: $$
 (x_n)\to p \Rightarrow f(x_n) \to f(p), \tag{1}
$$ then to show that this definition is equivalent to the epsilon-delta condition, $$
\forall p \forall \epsilon \exists \delta: x \in V_{\delta}(p) \Rightarrow f(x) \in V_{\epsilon}(f(p)), \tag{2}
$$ my textbooks use use a proof by contradiction in which they derive a convergent sequence that does not converge under the mapping. Question: Is it possible to have a direct proof of $(1) \to (2)$ ? Thought: I think to directly show that $(1) \to (2)$ , we will have to show that every element of M is an element of at least a sequence in $M$ , i.e., the union of the elements of all the sequences in $M$ is $M$ .","['analysis', 'real-analysis']"
3070538,Set dot product/multiplication,"I've been reading How To Prove It Second edition by Daniel J. Velleman, and on page 224, there's an end of sub-chapter question that defines an operation as follows: Suppose $\mathcal{F}$ and $\mathcal{G}$ are partitions of a set $A$ . We define a new family of sets $\mathcal{F} \cdot \mathcal{G}$ as follows: $$ \mathcal{F} \cdot \mathcal{G} = \lbrace Z \in \mathscr{P}(A) : Z \neq \emptyset \wedge \exists X \in \mathcal{F} \, \exists Y \in \mathcal{G} (Z = X \cap Y) \rbrace$$ Whenever I search for set multiplication all I found is the Cartesian product. Is this ""set dot product"", so to speak, a one-off thing or is there other sources that uses this notation? EDIT: The question asks to prove that $\mathcal{F} \cdot \mathcal{G}$ is a partition of $A \times B$ , which I have proven. What I'm asking for is whether this notation is common within Set theory or if it's just a convenient notation the author is using within the textbook only.",['elementary-set-theory']
3070565,Evaluate $ \lim\limits_{x \to 0}\frac{\int_0^{x^2}f(t){\rm d}t}{x^2\int_0^x f(t){\rm d}t}.$,"Problem Let $f(x)$ be continuously differentiable. $f(0)=0$ and $f'(0) \neq 0$ . Evaluate $ \lim\limits_{x \to 0}\dfrac{\int_0^{x^2}f(t){\rm d}t}{x^2\int_0^x f(t){\rm d}t}.$ Solution Consider applying l'Hôpital's rule. \begin{align*}
\lim_{x \to 0}\frac{\int_0^{x^2}f(t){\rm d}t}{x^2\int_0^x f(t){\rm d}t}&=\lim_{x \to 0}\frac{2xf(x^2)}{2x\int_0^x f(t){\rm d}t+x^2f(x)}\\
&=2\lim_{x \to 0}\frac{f(x^2)}{2\int_0^x f(t){\rm d}t+xf(x)}\\
&=2\lim_{x \to 0}\frac{2xf'(x^2)}{2f(x)+f(x)+xf'(x)}\\
&=4\lim_{x \to 0}\frac{xf'(x^2)}{3f(x)+xf'(x)}\\
\end{align*} It seems to be impossible to go on from here. Is it a wrong question?","['limits', 'definite-integrals']"
3070721,Pushforward of a line bundle along a finite morphism of curves,"Let $f:X\rightarrow Y$ be a finite morphism (a branched covering) of degree $n$ of smooth complex algebraic curves. It is a known result that for any line bundle $L$ on $X$ , the pushforward $f_* L$ is a vector bundle of rank $n$ (i.e. the pushforward sheaf of the sheaf of sections of $L$ is locally free and of rank $n$ ). Could anyone give a reference for a simple proof of this result? I know it can be derived as a particular case of more general results, but I am interested in this simple case.","['algebraic-curves', 'vector-bundles', 'algebraic-geometry', 'algebraic-vector-bundles']"
3070728,Derive the volume of a torus using the divergence theorem and Fubini's theorem.,"Given is the vector field $f=(x,y,0)$ and the set $\Omega :=((\sqrt{x^2+y^2}-2)^2+z^2 <1)$ (which is obviously a torus) I'm asked to : a) Calculate the outward unit normal field $v: \delta \Omega \to \mathbb{R}^3$ b) Calculate $\int_{\delta\Omega}\langle f,v\rangle dS$ using the Divergence Theorem. Hint : It is expected that you again derive the volume of a torus. For that, calculate first the area enclosed of the set $A_z:=((\sqrt{x^2+y^2}-2)^2\leq1-z^2) $ for $ -1\leq z \leq 1$ and then use Fubini's theorem. a) So here I think that the outward unit field vector is just the gradient of $f$ normalized. So the gradient is $(1,1,0)$ and when normalized, it is $(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}},0)$ . b) Now, here I'm not really sure. For the area enclosed, I'm obviously supposed to use polar coordinates. So $(r-2)^2\leq1-z^2$ . So $\pm(r-2)=\pm\sqrt{(1-z^2)}$ . 
So $r=2+\sqrt{1-z^2}$ or $r=2-\sqrt{1-z^2}$ . 
Now, the area is $\int_{2-\sqrt{1-z^2}}^{2+\sqrt{1-z^2}}\int_{0}^{2\pi}1rdrd\theta=\int_{-\sqrt{1-z^2}}^{\sqrt{1-z^2}}\int_{0}^{2\pi}1rdrd\theta$ . 
And so the volume will be $\int_{-1}^{1}\int_{-\sqrt{1-z^2}}^{\sqrt{1-z^2}}\int_{0}^{2\pi}1rdrd\theta$ And our divergence is $2$ , so we will just multiply the result by $2$ . Now, my questions are: Is my procedure for a) correct ? Is my procedure for b) correct and if yes, am I now just supposed to evaluate that integral ? Thanks for you help !","['integration', 'real-analysis', 'multivariable-calculus', 'calculus', 'vector-analysis']"
3070745,Why does the real projective plane / Boy surface look like this?,"In geometry, Boy's surface is an immersion of the real projective plane in 3-dimensional space found by Werner Boy in 1901 My question is, you can see that the Boy surface is made up of three identical parts. But how does the number $3$ come up? I cannot see it in the definition of $\mathbb{R}P^2$ .","['general-topology', 'projective-geometry', 'soft-question']"
3070762,How can I construct a nilpotent matrix with the property $A^2 \not= 0$ but $A^3=0$,"An example of a matrix $A$ that has the property $A^2=0$ would be $$A= \begin{pmatrix} 0 &1 \\ 0&0\end{pmatrix}$$ However, I can't seem to figure out a ""formula"" to construct a matrix that has the property $A^3=0$ but $A^2 \not = 0$ . Or in general, a formula for a matrix that has the property $A^k=0$ with $A^{k-1} \not=0$ . Does such a formula even exist?","['matrices', 'linear-algebra']"
3070788,Understanding why variance of the standard normal distribution equals one intuitively,"Can anyone explain to me why the variance of the standard normal distribution is 1? I am trying to understand the mechanism behind standardising random variable. While I know minus the variable by the mean is like shifting the graph to make it centre at the origin, I don't know why dividing it by SD makes the variable having SD = 1 as well","['statistics', 'normal-distribution', 'calculus', 'linear-algebra', 'linear-transformations']"
3070838,How much of an $n$-dimensional manifold can we embed into $\mathbb{R}^n$?,"I observed some naive examples. Spheres, for example, when we cut out one point, can be embedded into $\mathbb{R}^n$. And if we cut out a measure zero set of a projective space, it can be embedded into the Euclidean space of the same dimension.
So I wonder if all manifolds can be embedded into a same dimensional Euclidean space when we cut out a measure zero set? Can anyone prove it or disprove it by giving me some counterexamples?","['manifolds', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
3070841,"What is the canonical process of a measure space $(\Omega, \mathscr{F}, \mathbb{Q})$?","I was reading  paper which referred to a canonical process within the context of a measure space $(\Omega, \mathscr{F}, \mathbb{Q})$ . The surrounding discussion was of functions $a:[t,T] \rightarrow \Omega$ , and the canonical process was denoted $(\mathbb{S}_t)^T_{t=0}$ . It may be useful to know that the term was used when defining a ""martingale measure"". A measure $\mathbb{Q}$ was said to be a martingale measure if the canonical process is a local martingale with respect to the measure and $\mathbb{S}(0) = 1$ a.s. What could this canonical process be? Any help is appreciated.","['stochastic-processes', 'measure-theory']"
3070857,Finding maxima of a function $f(x) = \sqrt{x} - 2x^2$ without calculus,"My question is how to prove that $f(x) = \sqrt x - 2x^2$ has its maximum at point $x_0 = \frac{1}{4}$ It is easy to do that by finding its derivative and setting it to be zero (this is how I got $x_0 = \frac{1}{4}$ ). But the task is to do that without using any calculus tools and I`m stuck at it. My idea was to introduce $t = \sqrt x$ to get $f(t) = t - 2t^4$ and then find its maximum because I know how to do that with parabola which can be transformed to $a(x-x_0)^2 + y_0$ . So I was trying to turn $f(t) = t - 2t^4$ into $f(t) = a(x-x_0)^4 + y_0$ but it seems impossible. My second thought was to use the definition of rising function on an interval so I supposed we have $a, b \in \left(0; \frac{1}{4}\right) \text{and } a < b$ . Then I prove that $$\sqrt a - 2a^2 < \sqrt b - 2b^2$$ $$2(b - a)(a + b) < \sqrt b - \sqrt a$$ $$ 2(\sqrt a + \sqrt b)(a + b) < 1$$ which is true since $0 < a < b < \frac{1}{4}$ Exactly the same way I prove that for every $a, b \in \left(\frac{1}{4}; +\infty\right) \text{and } a < b$ $$\sqrt a - 2a^2 > \sqrt b - 2b^2$$ So we have that $f(x)$ is rising on $\left(0; \frac{1}{4}\right)$ and declining on $\left(\frac{1}{4}; +\infty\right)$ , consequently at $\frac{1}{4}$ we have a maximum of $f$ But I guess it is not fair to use derivatives to find the maximum and then simply prove that this value is correct. Is there an even better solution? What are your thoughts about my proof? UPD: there are a lot of solutions which are formally OK but first we have to guess $x_0 = \frac{1}{4}$ or $y_0 = \frac{3}{8}$ . My goal is to find a solution which does not rely on guessing and I think the inequality above is the best way to do that","['a.m.-g.m.-inequality', 'maxima-minima', 'functions', 'optimization', 'algebra-precalculus']"
3070895,The nth term of the maclaurin sequence of $\frac 1{1+x+x^2}$,"$$\frac 1{1+x+x^2}$$ $$ = \sum^\infty_{n=0} {(-1)}^n{(x+x^2)}^n$$ $$ = \sum^\infty_{n=0}{(-x)}^n \sum^n_{k=0} {_nC_k}x^k$$ $$ = \sum^\infty_{n=0}\sum^{[\frac n2]}_{k=0}{(-1)}^{n-k}{_{n-k}C_k}x^n$$ I wanted to simplify the last expression to an expression with a single $\sum$ . So I tried substituting a few terms. $$\begin{align}
\tag{n=6} \sum^3_{k=0}{(-1)}^{6-k}{_{6-k}C_k} = 1 \\
\tag{n=7} \sum^3_{k=0}{(-1)}^{7-k}{_{7-k}C_k} = -1 \\
\tag{n=8} \sum^4_{k=0}{(-1)}^{8-k}{_{8-k}C_k} = 0 \\
\tag{n=9} \sum^4_{k=0}{(-1)}^{9-k}{_{9-k}C_k} = 1 \\
\end{align} $$ Then, is there any generic simplified form of $\sum^{[\frac n2]}_{k=0}{(-1)}^{n-k}{_{n-k}C_k}x^n$ ?","['euler-maclaurin', 'binomial-coefficients', 'combinatorics', 'sequences-and-series']"
3070919,Showing that $F$ is not representable [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question As I'm trying to find (counter)examples of representable functors, I tried looking up some instructive examples. One of the counterexamples I'm having trouble with, is the following: Show that the functor $$ F:CRings \rightarrow Sets: R \mapsto \left\{r^2 \rvert r \in R\right\}$$ is not representable. Any help is appreciated :)","['representable-functor', 'functors', 'category-theory', 'abstract-algebra', 'commutative-algebra']"
3070937,"Let $F ( x ) = \frac { x } { \| x \| ^ { 3 } }$, if $0 \in \Omega$ show $\iint _ { \partial \Omega } ( F \cdot \nu ) d s = 4 \pi$","Problem : Let $\Omega \subset \mathbb { R } ^ { 3 }$ a regular domain and $\nu$ the unit external vector of $\partial \Omega$ . Let $F ( x ) = \frac { x } { \| x \| ^ { 3 } }$ . Prove that if $0 \in \Omega$ , then $\iint _ { \partial \Omega } ( F \cdot \nu ) d s = 4 \pi$ . I tried to apply the divergence theorem, $$\iiint _ { \Omega } \operatorname { div } F \mathrm { d } x _ { 1 } \mathrm { d } x _ { 2 } \mathrm { d } x _ { 3 } = \iint _ { \partial \Omega } ( F \cdot \nu ) \mathrm { d } s$$ But without getting interesting results since I have so few informations about $\Omega$ .","['multivariable-calculus', 'calculus', 'vector-analysis']"
3070975,"If $X$ is an Ito process, is $\mathbb E(\int X \mathrm d X)$ convex?","Consider the functional $F$ , which is defined for each Ito process $$X(t) = \int_0^t \mu(s) \mathrm d s + \int_0^t \sigma(s) \mathrm d W(s)$$ as $$F(X) := \mathbb E\bigg(\int_0^T X(s) \mathrm dX(s)\bigg)$$ Now I would like to prove that $F$ is convex, which seems to be intuitive because it is true for absolutely continuous processes. Due to the Ito formula / product rule, $$
\int_0^T X(s) \mathrm dX(s)
= \frac 1 2 X(T)^2 - \int_0^T \sigma(s)^2 \mathrm ds
$$ If we restrict ourselves on Ito processes for which we have $\sigma = 0$ , the proof is thus very easy. However, the quadratic variation term for $\sigma \ne 0$ seems to mess everything up. On the other hand, the $\sigma$ is also included in the $X(T)^2$ term, so we don't immediately get a counterexample. Is there another proof for the claim? Or is the claim wrong and there is a counterexample?","['stochastic-analysis', 'stochastic-processes', 'functional-analysis', 'stochastic-calculus']"
3070990,What does it mean for a set to be contained in another?,"If $A$ & $B$ are two sets, what does ' $A$ is contained in $B$ ' mean ? Does it mean that $A$ is a subset of $B$ or $A$ is a proper subset of $B$ ?","['elementary-set-theory', 'definition']"
3070993,A proposition about bounded and weak-$L^2$ integrable fucntion,"I have this proposition that I have no idea how to start. any hints helps. If $f\in L^{2,\infty}\cap L^{\infty}$ then $f\in L^p$ for $p\geq2$ and the second part asks me to generalize this result. $L^{p,\infty}:=\left\{f:||f||_{p,\infty}:= \text{sup}_{\lambda>0}\lambda \mu(\{|f(x)|>\lambda\})^{1/p} <\infty\right\}$ .","['harmonic-analysis', 'measure-theory', 'lp-spaces', 'weak-lp-spaces']"
3071001,$1+ {1\over 11}+ {1\over 111}+ {1\over 1111}+....=?$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question What is the sum of the series $$1+ {1\over 11}+ {1\over 111}+ {1\over 1111}+....$$ .The partial sum is a monotonically increasing and bounded above sequence, so sum must exits in real.",['sequences-and-series']
3071005,How to show a certain group element must belong to the stabilizer of a set element,"I'm studying for personal fun and culture group theory, specifically the orbit stabilizer theorem. I've then found this interesting problem: Let a group $G$ acting on $\Omega$ and take $\alpha,\beta\in\Omega$ , $x\in G$ and $\alpha^x=\beta$ (meaning $x$ acts on $\alpha$ to get $\beta$ ).
  Show that if $y\in G$ and $\alpha^y=\beta$ $\implies \exists g\in \operatorname{Stab}(\alpha)$ s.t. $y=gx$ , i.e. $y\in \operatorname{Stab}(\alpha)x$ . In words, this is tantamount to claim all group elements moving $\alpha$ to $\beta$ belong to the group set given by the stabilizer elements of $\alpha$ followed by a guess element moving $\alpha$ to $\beta$ . To get a better visual mental grasp on it, I've used the rotational symmetries of a cube acting on the set of cube faces. In this case, given a rotation moving a face onto another one, we can get the full rotations which realise the same thing using the stabilizer subgroup of each face. This is the set of rotations around an axis perp to the face mid point, and is isomorphic to a cyclic group of order 4. So, taking each stabilizer rotation followed by the initial rotation from one face to the target one, gives you back the full list of moves. That said, I've tried to show that such $g$ element exists (problem 3.3 of Groups and Characters book) but I'm not fully sure this demo is complete.
It goes like this: Let's take the element $g\in G$ s.t. $y=gx$ , i.e. $g=yx^{-1}$ . We can also claim $\alpha^y=\alpha^x$ so, replacing the y: $$\alpha^{gx}=\alpha^x \implies (\alpha^g)^x = \alpha^x$$ But this can be true only iff: $$\alpha^g=\alpha \implies g\in \operatorname{Stab}(\alpha) \implies y\in \operatorname{Stab}(\alpha)x$$ I was wondering if this is enough, or if I'm missing something. I mean: is this enough to proof that the elements $\in \operatorname{Stab}(\alpha)x$ are the only ones bringing $\alpha$ to $\beta$ ? I was also wondering if there could be a reductio ad absurdum possible version of this, i.e. ""Let's suppose such $g \notin \operatorname{Stab}(\alpha)$ . . . "" and deriving from it a contradiction. Thanks for your precious support in advance.","['group-theory', 'group-actions']"
3071011,How many functions $f: A \to B$ such that for all $x \in A$ there exists exactly one $y \in A$ such that $y \ne x$ and $f(x) = f(y)$?,"How many functions $f$ are there from $A = \{1,2,3,4,5,6\}$ to $B =\{a,b,c,d,e\}$ such that for all $x$ in $A$ there exists exactly one $y$ belonging to $A$ such that $x$ isn’t equal to $y$ and $f(x) = f(y)$ ? My attempt:
We basically have to make 3 pairs in domain and give each pair a value from $B$ . So Number of ways of making 3 pairs = $${6}\choose{2,2,2}$$ Number of ways of picking three values from $B$ = $${5}\choose{3}$$ Number of ways of assigning these 3 values to the the pairs = $$3!$$ So total number of functions should be $$90*10*6 = 5400$$ Is this the correct answer?","['functions', 'combinatorics']"
3071027,"Integral $\int_0^\frac{\pi}{2} x^2\sqrt{\tan x}\,\mathrm dx$","Last year I wondered about this integral: $$\int_0^\frac{\pi}{2} x^2\sqrt{\tan x}\,\mathrm dx$$ That is because it looks very similar to this integral and this one . Surprisingly the result is quite nice and an approach can be  found here . $$\boxed{\int_0^\frac{\pi}{2} x^2\sqrt{\tan x}\,\mathrm dx=\frac{\sqrt{2}\pi(5\pi^2+12\pi\ln 2 - 12\ln^22)}{96}}$$ Although the approach there is quite skillful, I believed that an elementary approach can be found for this integral. Here is my idea. First we will consider the following two integrals: $$I=\int_0^\frac{\pi}{2} x^2\sqrt{\tan x}\,\mathrm dx \,;\quad J=\int_0^\frac{\pi}{2} x^2\sqrt{\cot x}\,\mathrm dx$$ $$\Rightarrow I=\frac12 \left((I-J)+(I+J)\right)$$ Thust we need to evaluate the sum and the difference of those two from above. I also saw from here that the ""sister"" integral differs only by a minus sign: $$\boxed{\int_0^\frac{\pi}{2} x^2\sqrt{\cot x}\,\mathrm dx=\frac{\sqrt{2}\pi(5\pi^2-12\pi\ln 2 - 12\ln^22)}{96}}$$ Thus using those two boxed answer we expect to find: $$I-J=\frac{\pi^2 \ln  2}{2\sqrt 2};\quad I+J=\frac{5\pi^3}{24\sqrt 2}-\frac{\pi \ln^2 2}{2\sqrt 2}\tag1$$ $$I-J=\int_0^\frac{\pi}{2} x^2\left(\sqrt{\tan x}-\sqrt{\cot x}\right)\,\mathrm dx=\sqrt 2\int_0^\frac{\pi}{2} x^2 \cdot \frac{\sin x-\cos x}{\sqrt{\sin (2x)}}dx$$ $$=-\sqrt 2\int_0^\frac{\pi}{2} x^2 \left(\operatorname{arccosh}(\sin x+\cos x) \right)'dx=2\sqrt 2 \int_0^\frac{\pi}{2} x\operatorname{arccosh} (\sin x+\cos x)dx$$ Let us also denote the last integral with $I_1$ and do a $\frac{\pi}{2}-x=x$ substitution: $$I_1=\int_0^\frac{\pi}{2} x\operatorname{arccosh} (\sin x+\cos x)dx=\int_0^\frac{\pi}{2} \left(\frac{\pi}{2}-x\right)\operatorname{arccosh} (\sin x+\cos x)dx$$ $$2I_1=\frac{\pi}{2} \int_0^\frac{\pi}{2} \operatorname{arccosh} (\sin x+\cos x)dx\Rightarrow I-J=\frac{\pi}{\sqrt 2}\int_0^\frac{\pi}{2} \operatorname{arccosh} (\sin x+\cos x)dx$$ By using $(1)$ we can easily deduce that: $$\bbox[10pt,#000, border:2px solid green ]{\color{orange}{\int_0^\frac{\pi}{2} \operatorname{arccosh} (\sin x+\cos x)dx=\frac{\pi}{2}\ln 2}}$$ Doing something similar for $I+J$ we get: $$I+J=\int_0^\frac{\pi}{2} x^2\left(\sqrt{\tan x}+\sqrt{\cot x}\right)\,\mathrm dx=\sqrt 2\int_0^\frac{\pi}{2} x^2 \cdot \frac{\sin x+\cos x}{\sqrt{\sin (2x)}}dx$$ $$=\sqrt 2 \int_0^\frac{\pi}{2} x^2 \left( \arcsin \left(\sin x-\cos x\right)\right)'dx=\frac{\pi^3 \sqrt 2}{8}-2\sqrt 2 \int_0^\frac{\pi}{2} x \arcsin \left(\sin x-\cos x\right)dx$$ Unfortunately, we're not lucky this time and the substitution used for $I-J$ doesn't help in this case. 
Of course using $(1)$ we can again  deduce that: $$\bbox[10pt,#000, border:2px solid green ]{\color{red}{\int_0^\frac{\pi}{2} x \arcsin \left(\sin x-\cos x\right)dx=\frac{\pi^3}{96}+\frac{\pi}{8}\ln^2 2}}$$ In the meantime I found  a way for the first one, mainly using: $$\frac{\arctan x}{x}=\int_0^1 \frac{dy}{1+x^2y^2}$$ Let us denote: $$I_1=\int_0^\frac{\pi}{2} \operatorname{arccosh} (\sin x+\cos x)dx\overset{IBP}= \int_0^\frac{\pi}{2} x \cdot \frac{\sin x-\cos x}{\sqrt{\sin(2x)}}dx$$ $$\overset{\tan x\rightarrow x}=\frac{1}{\sqrt 2}\int_0^\infty \frac{\arctan x}{1+x^2}\frac{x-1}{\sqrt x}dx=\frac1{\sqrt 2}\int_0^\infty \int_0^1 \frac{dy}{1+x^2y^2} \frac{\sqrt x(x-1)}{1+x^2}dx$$ $$=\frac1{\sqrt 2}\int_0^1 \int_0^\infty \frac{1}{1+y^2x^2} \frac{\sqrt x(x-1)}{1+x^2} dxdy$$ $$=\frac{1}{\sqrt 2}\int_0^1 \frac{{\pi}}{\sqrt 2}\left(\frac{2}{y^2-1}-\frac{1}{\sqrt y (y^2-1)}-\frac{\sqrt y}{y^2-1}\right)dy=\frac{\pi}{2}\ln 2$$ Although the integral in the third row looks quite unpleasant, it can be done quite elementary. Sadly a similar approach for the second one is madness, because we would have: $$I_2=\int_0^1 \int_0^1 \int_0^\infty \frac{\sqrt x (x+1)}{1+x^2}\frac{1}{1+y^2x^2}\frac{1}{1+z^2x^2} dxdydz$$ But atleast it gives hope that an elementary approach exists. For this question I would like to see an elementary approach (without relying on special functions) for the second integral (red one). If possible please avoid contour integration, although this might be
included in elementary .","['integration', 'alternative-proof', 'definite-integrals', 'closed-form']"
3071056,"Difference between $[a,b]\in \mathbb R$ and $[a,b]\subset \mathbb R$?","What is the difference between the following? Are they both mathematically correct? \begin{align}
[a,b]\in \mathbb R \tag 1 \\
[a,b]\subset \mathbb R \tag 2
\end{align} And also, which one should I use If I want to say ""the interval between $a$ and $b$ is real""? Feel free to correct me if this phrasing is inaccurate.","['elementary-set-theory', 'calculus', 'notation', 'real-analysis']"
3071118,Solve differential equation $xyy'=x^4+y^4$,How to find general solution to this differential equation (if it exists): $$ xyy'=x^4+y^4 ?$$ I do not know how to even approach it since I never dealt with nonlinear equations. Only thing that I notice is that $x$ and $y$ are symetric in equation: $$ y'=\dfrac{x^4+y^4}{xy}$$ so I expect (maybe) some kind of symetric function for y(x). Thanks for any help.,"['nonlinear-analysis', 'analysis', 'ordinary-differential-equations']"
3071145,Is my proof that $ \lim\limits_{x \to 2} \left(x^2 - 3x\right) = -2 $ correct?,"My proof: take $x$ such that $|{x - 2}| < 1$ . Since $|{x}| -|{2}| < |{x - 2}|$ , then $|{x}| < 3$ .
Now we want to find a number $n$ such that $|{x - 2}| <1 \leq \frac{\epsilon}{n} $ or $|{x - 2}| < \frac{\epsilon}{n} \leq 1 $ and also so that $|{x - 1}| < n$ , and since $|{x}| - |{1}| < |{x - 1}|$ we have that $|{x}| < n + 1$ so we can choose $n = 2$ and by definition of limit, for all $\epsilon$ we have at least one $\delta$ equal to the smaller between 1 and $\frac{\epsilon}{2}$ such that $|{x - 2}| < \delta$ implies $|{x - 1}||{x - 2}| = |{x^2 - 3x - (-2)}| < \frac{\delta}{2} = \epsilon$ .","['epsilon-delta', 'proof-verification', 'calculus', 'functions', 'limits']"
3071153,dimension of vector space of measurable functions over finite set X,"Suppose we have $X=\{1,2,3\}$ and $M$ a $\sigma$ -algebra over X. I've stumped across the question of finding the dimension of the vector space of all the $M$ -measurable real functions over $X$ (e.g. $\quad f:X \to \mathbb{R}$ ).
For the best of me i can't seem to wrap my head around this, even if i think it's rather intuitive. My difficulty is coming up with a possible basis of functions for the vector space of functions from $X$ to $\mathbb{R}$ . Once i get that checking for misurability is trivial in this case, but i don't know how to approach basis of functions. Of course functions from $X$ to $\mathbb{R}$ have at most 3 distinct values. I thought of using as basis 3 separate functions, one for each value of $X$ : $e_i(i)=1\quad i=1,2,3$ . But i don't think this cuts it since i can't come up with a way of writing every f via linear combination of these basis functions.
Any hint on how to tackle this problem? Or maybe a general idea of how a base function should look like? Thanks!","['measurable-functions', 'functions', 'vector-spaces']"
3071156,proof that weak axiom of pairing and axiom schema of specification imply axiom of pairing,"as the title says I am trying to give a (nearly, but not fully formal) proof that the weak axiom of pairing (i.e. $\forall x \forall y \exists p: x \in p \wedge y \in p$ ) together with a suitable instance of the axiom schema of specification does imply the axiom of pairing. 
I haven't found a suitable instance yet, so this would be the first step to take.","['elementary-set-theory', 'proof-writing', 'logic', 'axioms']"
3071213,Different notions of torsors in algebraic geometry,"In what follows $X$ will be a scheme and $G$ a group scheme. In the examples I will take $X=\mathbb{P}^1_k$ and $G=\mathbb{G}_{m}$ . When reading about ""the torsor..."" I found many definitions, not sure which are equivalent and why exactly: (Principle $G$ -bundle) A $G$ -torsor $P$ over a scheme $X$ is a scheme $P\to X$ with a $G$ action $G\times P\to P$ , which is locally trivial in the sense that there is a covering map $Y\to X$ (in the Zariski,etale,fppf,... topology), s.t. $Y\times _X P\to Y$ is isomorphic to $Y\times G\to Y$ . Example: $P=X\times G$ or $\mathbb{A}^2_k-\{0\} \to X$ the Hopf bundle (Sheaf version) A $G$ -torsor $P$ is a sheaf on $(Sch/X)$ (with the Zariski, etale,fppf,... topology), if there is a covering $\{U_i\to X\}$ , s.t. $P|_{U_i}$ is a trivial $G_{U_i}$ torsor (on sheafs a trivial torsor is a sheaf with a $G$ simply transitive action and non empty global sections). Example: $Hom(-,X)$ for a principal $G$ -bundle as in 1) ? (Small sheaf version) Let $\mathcal{G}$ be a sheaf of abelian groups on the topological space $X$ . A $\mathcal{G}$ -torsor is a sheaf $P$ on $X$ with a $\mathcal{G}$ action with non-empty stalks on which $G$ acts simply transitive. (Tannakian version) A $G$ -torsor is an exact tensor functor $Rep_k(G)\to Bun_X$ , where $X$ is a scheme over $k$ . (cf this paper 4.1.) If $G$ happens to be $GL_n$ then we even have two more notions: $P$ is a vector bundle, i.e. a locally free sheaf of rank $n$ $P$ is a geometric vector bundle, i.e. $P\to X$ is a morphism which is locally of the form $pr_2:\mathbb{A}_n\times U_i\to U_i$ in a compatible way (cf. Hartshore ex.5.19 for more details) Now here is what I think is equivalent: If we consider the etale topology, 1. and 4. are equivalent I think I read that 1. and 2. are equivalent in some good cases, if some affinity condition is fulfilled In the $GL_n$ case 3., 5. and 6. are all equivalent I am not sure if 1. 2. or 4. are connected to 3. (well $Hom(-,\mathbb{G}_m)=\mathcal{O}_X^*$ when restricted to X seems no coincidence?) Further questions: What do $\check{\mathrm{H}}^1(X,\mathcal{G})$ and $\check{\mathrm{H}}^1(X,G)$ really capture? Is the line bundle corresponding to the Hopf bundle in 1. $\mathcal{O}(-1)$ ?","['principal-bundles', 'group-schemes', 'algebraic-geometry', 'reference-request']"
