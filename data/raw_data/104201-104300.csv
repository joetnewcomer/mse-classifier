question_id,title,body,tags
1464162,Transforming matrix to remove an eigenvalue,"Suppose I have a square symmetric matrix $A$, whose largest eigenvalue is $\lambda_1$ and the corresponding eigenvector is $v_1$. Also, suppose the second largest eigenvalue is $\lambda_2$ and the corresponding eigenvector is $v_2$, but the second largest eigenvalue is not known (not yet calculated). Can I transform the matrix $A$ to get another matrix $A_2$ whose largest eigenvalue is now $\lambda_2$?","['eigenvalues-eigenvectors', 'symmetric-matrices', 'matrices']"
1464191,Prove $[L:\mathbb{F_p}]=n$,"Let $f(x) \in \mathbb{F_p}[x] $ be an irreducible polynomial of degree $n$. Let $L$ be the splitting field of $f$. Prove $[L:\mathbb{F_p}]=n$. If $a_1,...,a_n$ are the roots of $f(x)$, then $L=\mathbb{F_p}(a_1,...,a_n)$. It is obvious that $[L:\mathbb{F_p}] \le [\mathbb{F_p}(a_1,...,a_n):\mathbb{F_p}(a_1,...,a_{n-1})]...[\mathbb{F_p}(a_1):\mathbb{F_p}] \le n^n$. Hence $L/\mathbb{F_p}$ is a finite extension of a finite field - thus separable, and simple,So $L=\mathbb{F_p}(\alpha)$, and $L$ is a finite field. We can also see that it is the splitting field of $x^{p^m}-x$ ($m$ is such that $|L|=p^m$), so this is a Galois Extension and $[L:\mathbb{F_p}]=|Gal(L/\mathbb{F_p})$|. So if I could find the size of the galois group... I would be done... I don't know how to find it though :) Please help","['extension-field', 'field-theory', 'galois-theory', 'abstract-algebra', 'finite-fields']"
1464274,"Existence of a prime in between two integers, of which the larger integer is divisible by all prime divisors of the smaller integer.","Let $a$, $b \in \mathbb{N}$ and $3 < a < b$. Suppose all prime divisors of $a$ divide $b$ and all prime divisors of $b$ less than $a$ also divide $a$. Does there always exist a prime $p$ such that $a<p<b$ and $p\nmid b$. I was trying to find a counterexample but I am not good with programming.","['primality-test', 'prime-numbers', 'number-theory']"
1464279,Differentiation under the integral sign for an electrostatic field,"Let $\rho\in C(\bar{D})$ be a continuous function on the compact set $\bar{D}$ and let us define $$\mathbf{E}(\mathbf{x}_0):=k\int_D\frac{\rho(\mathbf{x})}{\|\mathbf{x}_0-\mathbf{x}\|^3}(\mathbf{x}_0-\mathbf{x})d\mathbf{x}$$where I have used the short notation $d\mathbf{x}$ for $dxdydz$, with $\mathbf{x}=(x,y,z)$, and where $k$ is a constant, Coulomb's constant if we intend $\rho$ to be an electric charge density and $\mathbf{E}$ the electrostatic field.
Clearly $$\int_D\frac{\rho(\mathbf{x})}{\|\mathbf{x}_0-\mathbf{x}\|^3}(\mathbf{x}_0-\mathbf{x})d\mathbf{x}=-\int_{D-\mathbf{x_0}}\frac{\rho(\mathbf{x}+\mathbf{x}_0)}{\|\mathbf{x}\|^3}\mathbf{x}d\mathbf{x}$$therefore I think that imposing conditions upon $\rho$ would allow us to have a finite $\mathbf{E}$ variously subject to desired conditions of regularity. Although this problem arises in a physics context, I would like to find a mathematical proof of how to guarantee the usual conditions of regularity assumed in physics for $\mathbf{E}$. For example if $\rho\in C^k(A)$ where $A$ is open and contains $\bar{D}$, can it be guaranteed that $\mathbf{E}$ is of class $C^k$? If it can, how is it proved? The fact that the domain of integration $D-\mathbf{x}_0$ depends upon the variable(s) $\mathbf{x}_0$ does not allow me to use standard results of differentiation under the integral sign like the fact that if $V\subset\mathbb{R}^3$ is compact and $f:V\times[a,b]\to\mathbb{R}$ has a continuous partial derivative $\frac{\partial f}{\partial t}\in C(V\times[a,b])$ then for all $t\in[a,b]$ $$\frac{d}{dt}\int_V f(x,y,z, t)dxdydz=\int_V\frac{\partial}{\partial t} f(x,y,z, t)dxdydz.$$ I heartily thank you for any answer!","['derivatives', 'physics', 'multivariable-calculus', 'integration']"
1464299,The limit of $\frac{1}{n}\sum_{k=1}^{n}n^{\frac1k}$,"While playing around with some sequences, I came across the following problem: Decide wether the limit of the sequence $\left(x_n\right)_{n=1}^{\infty}$ given by:
$$
x_n=\frac{1}{n}\sum_{k=1}^{n}n^{\frac1k}
$$
exits and if so, calculate it. Under the assumption that the limit indeed exists, I proved that it is $2$: Lower bound:
$$
x_n=\frac{1}{n}\sum_{k=1}^{n}n^{\frac1k}=1+\frac{1}{n}\sum_{k=2}^{n}n^{\frac1k}≥1+\frac{n-1}{n}\implies\\
x:=\lim_{n\to\infty}x_n≥2
$$
For the upper bound I proved that:
$$
x_{mn}≤\frac{1}{mn}\left[\left(\sum_{r=1}^{m-1}\exp\left(\frac{\log(mn)}{r}\right)\right)-(m-1)\exp\left(\frac{\log(mn)}{mn}\right)\right]+\left(mx_n\right)^\frac{1}{m}
$$
Under the assumption that the limit exists, we can consider $n\to\infty$ which yields:
$$
x≤1+\left(mx\right)^\frac{1}{m}
$$
By then considering $m\to\infty$ this implies $x≤2$ and thus $x=2$. But I cannot think of an approach to show the existence of $x$. The sequence seems to be monotone decreasing for $n≥13$ which suggests to use induction, but to relate $x_n$ and $x_{n+1}$ seems extremely hard. How can it be proven? Thanks in advance. Ps: If it helps, I can add the prove of the upper bound, just comment.","['analysis', 'sequences-and-series', 'calculus', 'limits']"
1464335,Prove that 'isomorphism' is an equivalence relation (on any set of sets),"Consider identity function $1_A: A \to A.$ The identity function is bijective, so isomorphism is reflexive. Inverse of a bijection is bijection, so isomorphism is symmetric. Composition of two bijections is bijection, so isomorphism is transitive. Then isomorphism on any set of sets is an equivalence relation. Please, see if this argument works.","['elementary-set-theory', 'proof-verification']"
1464411,On the exponential function.,I was rereading the prologue to Real and Complex Analysis by Rudin (in image) and I realized I never really understood why we have the first and second equality after$\sum_{k = 0}^{\infty}\frac{a^k}{k!}\sum_{m = 0}^{\infty}\frac{b^m}{m!} $ and where does the absolute convergence of (1) play a role. So could somebody provide some extra steps and maybe I will understand?,"['summation', 'limits', 'real-analysis', 'exponential-function']"
1464421,Finding the $1000$-th decimal of $\sqrt{1111...111}$,"As I was cleaning up my desk, I found my Calculus exam from almost a year ago. I remember there was only a bonus task that required either a tad more wit, either a bit more time. It goes like this : $$ \text{Find the 1000-th  decimal of }\underbrace{\sqrt{1111...111}}_{1998 \text{ times}} . $$ I remember noticing $11 = \frac{10^2-1}{9}$, building up a general case upon this observation and representing it via series using the  binomial theorem, but nothing actually led me to the actual answer. Any ideas ?","['radicals', 'calculus', 'decimal-expansion']"
1464423,Does this integral converge?,"WolframAlpha says that
$$\int_{-\infty}^\infty \int_{-\infty}^\infty \int_{-\infty}^\infty \frac 1{(1+x^2+y^2+z^2)^2} \, dx \, dy \, dz$$ converges , but it cannot compute integrals that are more than three variables . Does this integral $$\int_{-\infty}^\infty \int_{-\infty}^\infty \int_{-\infty}^\infty \int_{-\infty}^\infty \frac 1{(1+x^2+y^2+z^2+w^2)^2} \, dx \, dy \, dz \, dw$$
converge? In general, does this integral
$$\int_{\mathbb R^n} \frac 1{(1+|x|^2)^2} \, dx$$
converge?","['convergence-divergence', 'integration']"
1464448,Find Discrete Time Fourier coefficients of $(-1)^n x[n]$,"Given that $x[n]$ is an N-periodic sequence with Fourier coefficients $a_k$, I want to find the Fourier coefficients of $$(-1)^n x[n]$$ for the situation in which $N$ is odd.  I'm also interested in the case of when $N$ is even, but the odd scenario is more important at the moment. I believe the property $$x[n]y[n] \Rightarrow b_k = \sum_{m=0}^{N-1} a_m b_{k-m}$$ would be useful.  I know that this also matches the form of periodic convolution.  I have up to this point determined the Fourier coefficients of the $(-1)^n$ portion.  The Fourier coefficients are given by $$(a_k)_{k=0}^{N-1} = \frac{1}{N} \sum_{n=0}^{N-1} x[n] \cdot \mathrm{e}^{-\mathrm{j}2 \pi n k /N}$$ which for $(-1)^n$ are given by $$a_k = \frac{1}{N} \frac{1 - \mathrm{e}^{\mathrm{j} \pi (N-2k)}}{1-\mathrm{e}^{\mathrm{j} \pi (1-\frac{2k}{N})}}$$ where we used a geometric series to eliminate the sum.  I'm kind of lost as to where to proceed from here.  I feel like there's probably a really straight forward answer to this question that I'm missing and things start getting really complicated when I attempt to plug into the property.  Any suggestions?","['fourier-series', 'signal-processing', 'discrete-mathematics']"
1464450,"""standard co-deviation""","This is a terminology/notation question.  I swear I've seen covariance matrices written as \begin{bmatrix}
  \sigma_x^2 & \sigma_{x,y}^2 \\
  \sigma_{y,x}^2 & \sigma_y^2
\end{bmatrix} Given that $\sigma_x^2$ is a variance, then $\sigma_x$ is a standard deviation. Given that $\sigma_{x,y}^2$ is a covariance, what is $\sigma_{x,y}$ called? Does it have a name?","['notation', 'probability', 'statistics', 'terminology']"
1464458,Conjecture about the limit of $\left(\frac1n\sum_{k=r}^n n^{\frac1k}\right)^{n^{c}}$,"By thinking a little further about my previous question , I made the following conjecture:
$$
\lim_{n\to\infty}{\left(\frac1n\sum_{k=r}^n n^{\frac1k}\right)^{n^{c}}}=\begin{cases}
\infty  & \text{if $c>\frac{r-1}{r}$} \\
e & \text{if $c=\frac{r-1}{r}$} \\
1 & \text{if $c<\frac{r-1}{r}$}
\end{cases}
$$
Where $r\in\mathbb{N}\setminus\{1\}$ and $c\in\mathbb{R}$. I approached it like in the question linked, but it didn't work. Is this conjecture true? And if so, how to find the right bounds to prove it?","['sequences-and-series', 'calculus', 'limits']"
1464479,Group cohomology: $\mathbb{Z}G$-maps from $G^{n+1}$ are the same as set maps from $G^n$,"I'm learning about group cohomology from Knapp's book Advanced Algebra . Given a group $G$, and an abelian group $M$, he defines $F_n$ to be the $(n+1)$-fold product of $G$, endowed with a $G$-action by $g\cdot(g_0, g_1, \dotsc, g_n)= (gg_0, gg_1,  \dotsc, gg_n)$. Then he defines boundary maps $F_n \to F_{n-1}$ and applies $\hom_{\mathbb{Z}G}(-, M)$ to get a cochain complex. He points out that $F_n$ is free as a $G$-module over the $(n+1)$-tuples with $1$ in the first entry. Further, $\hom_{\mathbb{Z}G}(F_n, M)$ is isomorphic as an abelian group to $C^n(G, M)$, i.e. the abelian group of set maps from $G^n$ to $M$, via the isomorphism
$$\hom_{\mathbb{Z}G}(F_n, M) \overset{\Phi}{\to} C^n(G,M)$$
$$\Phi(\varphi)(g_1, \dotsc, g_n) = \varphi(1, g_1, g_1g_2, \dotsc, g_1\dotsb g_n).$$ My Question : Why does he choose this isomorphism? I'm not seeing the point of the successive multiplication in later entries. It seems like 
$$\Phi(\varphi)(g_1, \dotsc, g_n) = \varphi(1, g_1, \dotsc, g_n)$$
would work. What am I missing?","['group-theory', 'group-cohomology']"
1464496,"Geometric series of matrices, left and right multiplying","Let A and M be square matrices of the same size. Assuming that the following serries converges, what is its sum? $M+AMA^T + A^2M{A^T}^2+\ ... $ I suspect that the sum can be expressed by a closed formula similar to the classical formula for geometric series with scalars. Edit:
I am not sure if that helps but: $M$ is symetric positive definite, $A$ is invertible, $A^iM(A^T)^i$ is symetric positive definite and the sum is also symetric positive definite. I don't mind if you use some additional assumptions. But I would realy appretiate solutiuon using matrix algebra.","['sequences-and-series', 'matrices']"
1464522,Order of the group of integer orthogonal matrices,"Let $O_n(\mathbb Z)$ be the group of orthogonal matrices (matrices $B$ s.t. $BB^T=I$ ) with entries in $\mathbb Z$ . 1) How do I show that $O_n(\mathbb Z)$ is a finite group and find its order? 2) I need to show also that symmetric group $S_n$ is a subgroup of $O_n(\mathbb Z)$ . So it needs to satisfy associativity/identity/inverse. It is easy to see that every orthogonal matrix $A \in O(\mathbb Z)$ has an inverse, namely $A^T$ . Moreover, the product of two orthogonal matrices is orthogonal since $(AB)^T = B^T A^T$ .  If $A, B \in O_n(\mathbb Z)$ then $(AB)^T(AB) = B^T A^T AB = BIB^T = BB^T = I$ , hence $O_n(\mathbb Z)$ is closed under multiplication, since $I \in O_n(\mathbb Z)$ .","['matrices', 'group-theory', 'finite-groups', 'linear-algebra', 'orthogonal-matrices']"
1464523,Is the p-value the probability that your null hypothesis is true?,"I know that the p-value is the probability that the measurement takes a value that is at least as unlikely under the null hypothesis as the observed value. Furthermore, the null hypothesis is rejected if and only if the p-value of the observation is smaller than the signification level. My question is: Is the p-value then the probability that your null hypothesis is true? Why (not)? Edit: I do not think it is, but I can not properly explain why.",['statistics']
1464595,Trouble understanding why Disjunctive Normal Form is polynomial time solvable but not CNF.,"So from my understanding when looking at a Boolean formula in Disjunctive Normal Form, its satisfiability is decidable in polynomial time, yet this isn't the case for CNF. Is this because with DNF you only need to find one clause that is satisfiable to prove it? Where as in CNF you would need to compare all of them regardless?","['logic', 'np-complete', 'discrete-mathematics']"
1464603,Multivariable limit of a piecewise function,"Consider the following expression: $$\lim_{(x,y) \to (0,0)} g(x,y)= \begin{cases} \frac{\sin x}{x} y \text{ $\hspace{0.5in}$if $x \neq 0$}\\ y \text{ $\hspace{0.81in}$if $x=0$}\end{cases}$$ I am seeking guidance in regards to a general method for finding limits for piecewise functions such as the one above. Do I take each case individually and find the limit? Since the top function exists only for $x \neq 0$, can the limit exist as $(x,y) \to (0,0)$? Now say the limit exists for this piecewise function. How would I prove this using the epsilon-delta definition? I am familiar with the process for non-piecewise functions, but this type of question has always confused me. Any general advice (that may assist me in solving more questions like this one) would be appreciated. Thank you! EDIT: Here is my attempt. We can directly find that $\lim_{(x,y) \to (0,0)}y=\lim_{y \to 0}y=0$. Now, we will consider the other case. Let $\varepsilon >0$ be arbitrary and take $\delta=\varepsilon$. Then, for all $(x,y)$ with $0<\sqrt{x^2+y^2}<\delta$, we have that $$\left|\frac{\sin x}{x}y\right|\leq |y| \leq \sqrt{x^2+y^2}<\delta=\varepsilon$$ Thus, the limit exists and is $0$.","['calculus', 'limits', 'multivariable-calculus']"
1464605,Cantor's Theorem proof seems a bit too convenient,"I am taking my first analysis class, and I am really enjoying it. I have recently stumbled upon Cantor's Theorem which states that there exists no surjective map $f$ of a set $A$ to its powerset, $P(A)$. The proof is pretty straight forward, that is not the issue, but I was wondering if anyone could speak to the existance of the Cantor Diagonal Set , $B = \{x\in A:x\notin f(x)\}$. It seems a little too convenient to define a set where a mapping is restricted when one is proving that there is a set which cannot be mapped to. I am looking for a little intuition. Thanks. EDIT: I would like to thank those who provided answers/comments to this question. The Cantor Diagonal Set is now much more clear to me. I withdrawal my statement that the proof to Cantor's Theorem is ""convenient."" Thanks so much!",['elementary-set-theory']
1464622,Prove derivative is continuous - is this $\delta/\epsilon$ proof correct?,"I am aware of proofs of this fact, including those given at prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. . This question is not about how to prove it efficiently using MVT or any other clever method, but whether the proof I've sketched here is correct. This is not a duplicate, because I'm trying to get a review of my delta-epsilon reasoning. Suppose $f$ is differentiable on an open interval $I$, $c\in I$, and $\lim_{x\to c} f'(x) = L \neq\infty$. Prove that $f'$ is continuous at $c$. In other words, $f'$ cannot have a removable discontinuity. My intuition is this: if $x$ is close to $c$, then the quantity $\frac{f(x)-f(c)}{x-c}$ is close to $f'(x)$ and also to $f'(c)$, therefore they are close to each other. However, I'm having a hard time formalizing this. I figure that for any $\epsilon >0$, we can find a $\delta$ so that, whenever $x$ is $\delta$-close to $c$, that puts our difference quotient $\frac{\epsilon}{2}$-close to $f'(c)$. That's true because $f'(c)$ exists. Additionally, I can find another $\delta$ so that $f'(x)$ is $\frac{\epsilon}{2}$-close to $L$ when $x$ is $\delta$-close to $c$. That's true because the limit of $f'$ exists at $c$. Choosing the smaller of the two $\delta$'s, the triangle inequality gives us that $|f'(c)-L|<\epsilon$, and so they're equal? Does that work?","['proof-verification', 'real-analysis', 'derivatives']"
1464634,Mathematical induction and pigeon-hole principle,"I am trying to prove that if $n$ is even and if $n+1$ integers are
  chosen from the set $\{1,2,....,2n \}$ then there are always two integers
  that differ by 2. In my attempt. I try $n=2$, and so we have $\{1,2,3,4\}$ and we could partition $\{1,2,3,4\}$ into two boxes with $2$ numbers in each as follows $\{1,3 \}$ and $ \{2,4 \}$. Now these two boxes are my pigeon-holes and I want to take $n+1=3$ numbers from these two boxes and so by the pigeon-hole principle, I have to take one of the two boxes that have two integers that differ by 2 and hence done. Now I want to generalise this for any $n=2k$, How can I do it. I thought about proceeding by induction. Assume this works for $n=2k$ now I want to show that it works for $n=2(k+1)$. Assume that it works for $\{1,2,.....,4k \}$,   I want to show that it works for $\{1,2,.....,4k + 4 \}$ So because it works for $$\{1,2,.....,4k \}$$ This means that if $n+1 = 2k +1$ integers are chosen from $$\{1,2,.....,4k \}$$ then there are always two integers that differ by 2. Now considering $\{1,2,.....,4k + 4 \}$ The difference between $\{1,2,.....,4k + 4\}$ and $\{1,2,.....,4k \}$ Is just 4 numbers $$ \{4k+1,4k+2,4k+3,4k+4 \}$$ Now if we want to choose $n+1 = 2k +3$. Then I stop here, And don't know how to proceed . Any suggestions ? or is there another way to answer this question without mathematical induction !","['discrete-mathematics', 'pigeonhole-principle', 'induction', 'combinatorics']"
1464665,How to determine whether this function is differentiable at a point?,"We are given the following function: $$f(x) =     \left\{
\begin{array}{ll}
      \dfrac{x}{1+x} & x \geq 0 \\
      x^2 & x < 0 \\
\end{array} 
\right.$$ We wanted to determine whether or not $f(x)$ is differentiable at $0$. I already know that $f(x)$ is continuous at $0$ using the definition of continuity. If I am correct, to show differentiability we have to show that the following limit exists: $f'(x)=\lim_{~h \to 0} \dfrac{f(x+h)-f(x)}{h}$. Since $f(x) = \dfrac{x}{1+x}$ at $x=0$, would it then be enough to say that the derivative of $[\dfrac{x}{1+x}]' = \dfrac{1}{(x+1)^2}$ is defined at $x=0$, and since we know that $f(x)$ is also continuous at $0$, we can say $f(x)$ is differentiable at $0$?","['calculus', 'limits']"
1464707,Is the empty set in every language?,"I know that in set theory, $\forall A:\emptyset \subseteq A$ My question is, does this apply to formal languages? In my mind, formal languages are just a set of strings that are over some set of letters, which can be looked at as a set of strings with size 1. If I look at it strictly from the formal language side, all non-empty alphabets cannot have an ""empty"" letter and there would not? be an empty set in every language. Note I am not looking at the set containing only the empty string, which I understand is not in every language. However, would a predicate $P(w)=w\in AB \iff \exists a:\exists b:a \in A \land b \in B \land ab=w$ that tests the membership of string $w$ in the concatenation of two finite (non empty) formal languages ($A,B$) always be true if there is no string $w$ at all ergo $w$ is the empty set? I think but am not sure if it is just vacuously true that if there is no string at all, $P$ would be true by the fact that $\forall A:\emptyset \subseteq A$ is vacuously true, so for any set product $\forall A:\forall B:\emptyset \subseteq AB$ would also be vacuously true Perhaps my confusion is due to misunderstanding the empty set with regards to language theory.","['elementary-set-theory', 'formal-languages', 'discrete-mathematics']"
1464715,Solve the recurrence relation: $a_n = 6a_{n-1} - 9a_{n-2}$,"Recurrence relation:$$a_n = 6a_{n-1} - 9a_{n-2}$$ Initial conditions:$$a_1 = 1, a_2 = 9$$ I am having a bit of trouble finishing off this problem. So far I have: Assume:$$a_n = r^n$$ $$r^n = 6r^{n-1}-9r^{n-2}$$
$$r^2 = 6r - 9$$
$$r^2 - 6r + 9 = 0$$
$$(r-3)^2 = 0$$
This means:
$$a_n = A(3)^n+B(n3^n)$$
$$a_1 = A3^1 + B((1)3^1) = 3A + 3B = 1$$
$$a_2 = A3^2 + B((2)3^2) = 9A + 18B = 9$$ From there I can't seem to combine the equations to find out what A and B are equal to correctly. Any help would be appreciated!","['recurrence-relations', 'discrete-mathematics']"
1464752,Probability that inequality holds infinitely often implies it holds almost surely,"I have been working on some probability problems out of Durrett and in searching for answers I have come upon this sort of explanation a lot. Let $X_1, X_2, \dots$ be i.i.d. random variables. Suppose that $P(X_n>a \text{ i.o. })=1$ then we have that $\limsup_{n\rightarrow\infty} X_n > a$ almost surely. Note: here i.o. means infinitely often. I'm not exactly sure why this has to be true. Could anyone explain this to me or provide me with a source that does? Thanks!","['probability-theory', 'elementary-set-theory', 'probability']"
1464769,Underdamped free vibration proof,"I need to prove the solution form of: $$y''+2cwy'+wy=0$$ My book says, after assuming a solution of the form $Ce^pt$, you can show that: $$y=[A\sin(wt)+B\sin(sw)] \cdot e^pt$$ I tried using the characteristic equation on the original ODE but it didn't get me anywhere. Any ideas? Obviously, just plugging in the suggested solution isn't a sufficient proof...Any ideas?","['harmonic-functions', 'harmonic-analysis', 'ordinary-differential-equations']"
1464774,the ideal generated by general polynomials is radical,"Let $F_1,\dots,F_s$, $s<n$, be homogeneous polynomials in $n+1$ variables of degrees $d_1,\dots,d_s$. 
Let $X$ be the intersection of the $s$ hypersurfaces of $\mathbb{P}^n$ defined
by $F_1,\dots,F_s$. By the homogeneous Hilbert's nullstellensatz, we know that 
$I_X = \operatorname{rad}(F_1,\dots,F_s)$. However, when $F_1,\dots,F_s$ are general, we have the remarkable fact that $(F_1,\dots,F_s)$ is radical. Can anyone please provide a proof (or sketch of proof) of this fact? Edit/Definition: Recall that the space of polynomials of degree $d$ in $n+1$ variables is parametrized by $\mathbb{P}^{M_n}, M_n:={n+d \choose d}-1$. Then requiring $F_1,\dots,F_s$ to be general for the statement to hold, is interpreted as ""there exists a Zariski open set $U$ of $\mathbb{P}^{M_{n_1}} \times \cdots \times \mathbb{P}^{M_{n_s}}$, such that the statement is true for every $s$-tuple of polynomials inside $U$"". PS: Since the relation between an ideal and its radical is in general a complicated one, i find the statement of the question extremely interesting and useful and i don't know why finding a proof in the literature seems so hard.","['algebraic-geometry', 'commutative-algebra']"
1464779,Proving the total curvature of the trefoil curve is $4\pi $.,"I'm looking at the trefoil curve $\vec x (t) = (\cos (3t) \cos (t), \cos (3t)\sin (t)) $ defined along the interval $[0,\pi] $. First I tried calculating this by integrating curvature along the length of the curve, but it seemed to be getting extremely complicated. Then I thought of trying to count the number of times the tangent points parallel to one of the coordinate axes, but finding the zeros of the derivatives seemed to end up with really ugly expressions. I don't really know what to do now short of numerical integration but the problem is asking me to $prove$ the total curvature is $4\pi$.","['differential-geometry', 'curvature']"
1464799,Why b%2==1 implies that rightmost digit of b in binary form is 1,How one can deduce that if b is any number and if b%2==1 then rightmost digit of b in binary form will be 1 without checking it manualy,"['binary', 'discrete-mathematics']"
1464803,Finding the number of lattice paths,"Find the number of lattice path of length $2n$ that starts on $(0, 0)$ such that for all the points $(x, y)$ in the path, $x < y$. So pretty much all the points besides the origin are strictly above the line $y = x$. What I have done so far is that, given a point:
$$p =(k, 2n-k)$$
We know that $$k < 2n - k$$ The total number of path paths will be
$${2n \choose k}$$ so my approach is that in order for the paths to not meet the requirement, it must have some other point on the diagonal. I defined a set $A(j)$, which consists of paths from $(0, 0)$ to $(j, j)$ where no points other than the origin and endpoint. I found that $|A(j)| = \frac{2}{j} {2n-2 \choose n-1}$ because for every such path, if you take away the first and last step, it becomes dyck path from 
$(0, 0)$ to $(j-1, j-1)$. Then you multiply that by the number of paths from$(j, j)$ to $(k, 2n-k)$.
Which is $${2n-2j \choose k-j}$$ Thus, 
$$B(j) = |A(j)|{2n-2j \choose k-j} $$
will be the amount of paths from $(0, 0)$ to point $p$ such that $(j, j)$ is the first point in the diagonal. Then I do
$$F(p)=\sum_{j=1}^{k} B(j)$$
To get amount of all the paths that intersects with the diagonal at least once after the origin.
Subtracting that from the total amount of paths possible I get the number of paths that meets the requirement for that point. Then I do it for all the points to get a total number of paths that never intersects with diagonal. 
But I'm kinda stuck on the summation part. Is there another approach to this?","['integer-lattices', 'catalan-numbers', 'binomial-coefficients', 'combinatorics']"
1464809,Perfect square all 9's,"For $9, 99, 999, 9999, 99999,\dots$, except $9$, are the rest of the numbers $9$'s perfect squares? Are there other perfect squares with all $9$'s. This problem is given for K-12 students, which I have no idea how to do it. Thank you.",['number-theory']
1464842,"Prove that if 51 positive integers between 1 and 100 are chosen, then one of them must divide another.","Given $51$ integers $a_1, a_2, \ldots, a_{51}$ satisfying $1 \leq a_i \leq 100$ . Prove that there exist $i$ and $j$ such that $a_i \mid a_j$ . I'm a little unsure how to approach this problem. Thanks for any help.","['pigeonhole-principle', 'discrete-mathematics']"
1464844,Intuition behind left and right translations being bijective in a group?,"In my algebra class, we learn that the maps $l_g(x) = gx$ for $x \in G$ and $r_g(x) = xg$ for $x \in G$ are bijective. The proof given uses the fact that $l_g l_{g^{-1}} = l_{g^{-1}} l_g = 1_G$, so both functions are bijective since $1_G$ is and therefore $l_g$ is bijective. The proof for $r_g$ is analogous. Is there are more intuitive approach to achieving this result? The proof, while elegant, doesn't provide intuition (in my opinion).","['abstract-algebra', 'group-theory', 'functions']"
1464878,How many ways are there to add toppings to ice cream?,"At an ice cream shop, in addition to chocolate chips, there are 8
  different toppings. If a customer wants to add two more toppings in
  addition to the chocolate chips in how many ways can he do so? Assume
  that repetition is not allowed. Now we're supposed to imagine there is a bowl of ice cream and we must consider how many ways are there to add 2 more toppings in addition to the chocolate chips. I think that this means this is a combinations without repetitions question. As chocolate chips are already chosen, that means we have to choose 2 toppings from 8 available toppings. So this means C(n,r) = C(8,2) = 28. Am I correct?","['discrete-mathematics', 'combinatorics']"
1464897,proof the triangle similarity,"The figure below represents arbitrary triangle ABC. The points K,L,M are the midpoints of its sides. a) Show that triangle CLK ~ triangle CAB  (and similarly for the other two corner triangles) How do I go about writing a proof for this. The definition for similarity: two figures are similar if one is congruent to the dilation of the other. Definition of congruent: Two figures are congruent if one can be obtained from the other by a rigid motion. b) . Show that in fact all four smaller triangles (the three corner ones and the middle one) are congruent to each other by showing that they are all similar to each other with magnification factor equal to one. Any tips on how to go about proving this? Any help is appreciated. Thanks.","['geometry', 'triangles']"
1464898,Prove that if $f$ is increasing then so is $f^{-1}$,"Prove that if $f$ is increasing then so is $f^{-1}$, when $f$ is a one-to-one function. I'm having trouble figuring out how to get started with this question. I'm assuming it has something to do with the definition of infectivity, but that's as far as i've gotten.","['inverse', 'functions']"
1464900,How to find the inflection point of $f(x)=\frac{e^x}{1+e^{2x}}$,"With the second derivative, the solution gives logarithm of negative number, but I know the graph and the function changes the concavity. $$f(x)=\frac{e^x}{1+e^{2x}}$$ What are the inflection points?","['derivatives', 'calculus', 'functions']"
1464923,"Set theory: If $C \cup B = U$, $D \cup F = U$ and $B \cap D = \emptyset$, then $C \cup F = U$","So far I have: let $x$ exist in $C \cup F$, then $x$ is in $C$ or $x$ is in $F$. Then since $C \cup B = U$ and $D \cup F = U$, $x$ is in $C$ or $x$ is in $B$ AND $x$ is in $D$ or $x$ is in $F$. But because $B \cap D = \emptyset$, we know $x$ is not in $B$ or $D$.",['elementary-set-theory']
1464958,Is every Cauchy sequence in a non-complete metric space convergent?,"A metric space $X$ is called complete if every Cauchy sequence in $X$ has a limit in $X$. For a non-complete metric space $X$, can we say that every Cauchy sequence is convergent? (even though the limit is not in $X$) In other word, is every Cauchy sequence convergent?","['cauchy-sequences', 'limits', 'real-analysis', 'convergence-divergence']"
1465008,Why do we need compactness?,"There's a theorem that says if $E$ is an infinite subset of a compact set $K$, then $E$ has a limit point in $K$. Why can it not be done with $K$ only being closed? If $K$ is closed then $\overline{E} \subset K$, so if $x$ is a limit point of $E$, then $x \in \overline{E} \implies x \in K$.","['examples-counterexamples', 'real-analysis', 'general-topology', 'compactness']"
1465042,Center Manifold Exercise (small solution for small changes of the parameter),"Hi I'm stuck with this problem at first I didn't know how to begin so I copy an argument from [Carr, Application of Centre Manifold Theory]. But I don't know how can I find the coefficient from a, b and c? and also I don't understand the theory very well, it's very complicated to me. I'd appreciate if someone can help me with a detailed answer. Thank you For the following system give an explicit formula for the centre manifold, what happening in the origin? \begin{align*} \dot{x}&=-x+y^2-2x^2\\
\dot{y}&= \epsilon y-xy\end{align*} The linearized problem has eigenvalues $-1, \epsilon$. This means that the results in the centre manifold theory does not apply directly. Now if we write the above equation as 
\begin{align*} \dot{x}&=-x+y^2-2x^2\\
\dot{y}&= \epsilon y-xy\\
\dot{\epsilon} &=0\end{align*} We considered as an equation in $\mathbb{R}^3$ and the term $\epsilon x$ in the equation is nonlinear. Thus the linearized problem associated has eigenvalues $-1, 0,0$. Now the theory applies and by the centre manifold theorem for some $\delta_i>0$ for $i=1,2$ there is a map $h(y,\epsilon)=x$ for $|x|<\delta_1$ and $|\epsilon|<\delta_2$. So we have $\dot{x}=h_y(y,\epsilon) \dot{y}$, where $h_y$ is the partial derivative with respect to y. From this, it follows that $h$ satisfies the functional equation $$h_y(y,\epsilon)[\epsilon y-h(y,\epsilon)y]+h(y,\epsilon)-y^2+2h^2(y,\epsilon)=0$$ We have an expansion $h(y,\epsilon)=ay^2+by\epsilon+c\epsilon^2+\ldots+$ from here we have \begin{align*}
0&=h_{y}(y, \epsilon)\dot{y}-\dot{x}\\
&=h_{y}(y, \epsilon)(\epsilon y -xy)+h(y, \epsilon)-y^{2}+2h^{2}\\
&= h_{y}\epsilon y-h_{y}xy+h-y^{2}+2h^{2}\\
&= (2ay+b\epsilon)\epsilon y-(2ay+b\epsilon)hy+h-y^{2}+2h^{2}\\
&= 2a\epsilon y^{2}+b\epsilon ^{2}y-(2ay+b\epsilon)(ay^{2}+by\epsilon +c\epsilon ^{2})y+ay^{2}+by\epsilon +c\epsilon ^{2}-y^{2}+2h^{2}\\
&= 2a\epsilon y^{2}+b\epsilon ^{2}y-(2ay^{2}+b\epsilon y)(ay^{2}+by\epsilon +c\epsilon ^{2})+ay^{2}+by\epsilon +c\epsilon ^{2}-y^{2}+2(ay^{2}+by\epsilon +c\epsilon ^{2})(ay^{2}+by\epsilon +c\epsilon ^{2})\\
&= 2a\epsilon y^{2}+b\epsilon ^{2}y-(2a^{2}y^{4}+2ab\epsilon y^{3}+2ac\epsilon ^{2}y^{2}+ab\epsilon y^{3}+b^{2}\epsilon ^{2}y^{2}+bc\epsilon ^{3}y)+ay^{2}+by\epsilon +c\epsilon ^{2}-y^{2}+2(a^{2}y^{4}+aby^{3}\epsilon +ac\epsilon ^{2}y^{2}+aby^{3}\epsilon +b^{2}y^{2}\epsilon ^{2}+bc\epsilon ^{3}y+ac\epsilon ^{2}y^{2}+bcy\epsilon ^{3}+c^{2}\epsilon ^{4})
\end{align*} But from here we can find $b=c=0$ and $a=1$. Thus $h(y,\epsilon)=y^2 + \ldots$ and plugging this in the $y$ equation we have $\dot{y} =\epsilon y- y^3 + \ldots $ and when $\epsilon= 0$ the system is stable. Am I right? Thanks","['stability-in-odes', 'nonlinear-system', 'dynamical-systems', 'ordinary-differential-equations']"
1465057,Why are units called units?,"Why are units in abstract algebra called units? Is it just because they generalize the notion of $-1$ and $1$?, and the like? There's often a sense that units don't matter, when talking about things like irreducibility- is the name unit supposed to trivialize them somehow?","['abstract-algebra', 'terminology']"
1465082,"$f$, $g$ is measurable, then $fg$ is measurable.","I'm trying to show that, given $f$ and $g$ measurable, $fg$ is also measurable. Is it true that $$\{x : fg(x) > c\} = \left( \bigcup_{\substack{a,b \in \mathbb{Q}^+\\ ab \geq c}} \{x: f(x) > a \} \cap \{x : g(x) > b\} \right) \cup \left( \bigcup_{\substack{a \in \mathbb{Q}^-\\ b \in \mathbb{Q}^+ \\ ab \geq c}} \{x: f(x) < a \} \cap \{x : g(x) > b\} \right) \cup \left( \bigcup_{\substack{a \in \mathbb{Q}^+\\ b \in \mathbb{Q}^- \\ ab \geq c}} \{x: f(x) > a \} \cap \{x : g(x) < b\} \right) \cup \left( \bigcup_{\substack{a,b \in \mathbb{Q}^-\\ ab \geq c}} \{x: f(x) < a \} \cap \{x : g(x) < b\} \right)?$$ As of now I'm still trying to work through this problem, so hints, not answers would be appreciated, as well as if the above is correct. If the above is correct and there is a more compact way to write this, then I'd be happy to hear that as well.","['analysis', 'real-analysis', 'measure-theory']"
1465101,How to show that $\sum_{k=1}^n |a_{kk}| \le \sum_{k=1}^n |\lambda_k|$ for hermitian matrix $A$?,I see that this is not true in general. Is it true that $\sum_{k=1}^n |a_{kk}| \le \sum_{k=1}^n |\lambda_k|$ for any complex square matrix $A$? But it is true in hermitian matrix. How to prove that this is true?,"['linear-algebra', 'matrices']"
1465140,How can I prove that $\lim_{n \rightarrow \infty} \sqrt[n]x=1$ [duplicate],"This question already has answers here : Prove $\forall K > 0: \lim_{n\rightarrow\infty} \sqrt[n]{K} = 1$ [duplicate] (3 answers) Show that $\lim_{n \rightarrow \infty} \sqrt[n]{a} =1$ (3 answers) Closed 4 years ago . How can I prove that $$\lim_{n \rightarrow \infty} \sqrt[n]x=1$$
for all $x>0$ I know I've got to use the monotone convergence theorem somehow. It's easy to show that $\sqrt[n]x$ is bounded, but having trouble showing that it is strictly increasing for $0<a<1$ and strictly decreasing for $a>1$. Also how can prove the infimum and supremum is 1 for the two cases?","['radicals', 'sequences-and-series', 'limits', 'real-analysis']"
1465197,Mathematical induction and Stirling numbers,"I want to find a formula for the following series $$ \sum_{i=1}^m {m \choose i} i! S(n,i)$$ Where $S(n,m)$ is the Stirling numbers of the second kind. Now I evaluated this series at $m=1,2,3$ for $m=1$ we have $${1 \choose 1}S(n,1) = m$$ because $S(n,1) = 1 = 1^n$ for $m=2$ we have $${2 \choose 1}1! S(n,1) + {2 \choose 2}2! S(n,2) = 2 + 2(2^{n-1} -1)= 2 + 2^n - 2= 2^n$$ for $m=3$ we have $${3 \choose 1}1! S(n,1) + {3 \choose 2}2! S(n,2) + {3 \choose 3} 3! S(n,3) = 3 + 3.2!(2^{n-1}-1) + 3! \times \frac{1}{6} \times (3^n - 3\times2^n +3) = 3^n$$ And now I see a pattern which is $m^n$. Now I want to prove that I have the right patten, So I proceed by induction, Clearly It works for the base case as I did it already. Now assume it works for $m=k$, and so we assume that $\sum_{i=1}^k {k \choose i} i! S(n,i) = k ^n$, we now want to prove it works for $k+1$ that is, we want to show that $$\sum_{i=1}^{k+1} {k+1 \choose i} i! S(n,i) = (k+1) ^n$$ but I am wondering where to proceed from here. Any suggestions ? I feel like we should use pascal's triangle on ${k+1 \choose i}$ and so we have ${k+1 \choose i} = {k \choose i} + {k \choose i-1}$ and so we now need to prove that  $$\sum_{i=1}^{k} \big({k \choose i} + {k \choose i-1} \big) i! S(n,i) = (k+1) ^n$$ Am I right there ? And then we can break this into two separate sums, so now we need to prove that $$\sum_{i=1}^{k} {k \choose i}i! S(n,i) + \sum_{i=1}^{k} {k \choose i-1}  i! S(n,i) = (k+1) ^n$$ Now by our hypothesis we assumed that $\sum_{i=1}^{k} {k \choose i}i! S(n,i) = k^n$ and now we need to evaluate $\sum_{i=1}^{k} {k \choose i-1}  i! S(n,i)$, But the first term in this sum is $1$ because we have ${k \choose 0} 1! S(n,1) = 1$ and then we have $\sum_{i=2}^{k} {k \choose i-1}  i! S(n,i)$ which is basically same like $\sum_{i=2}^{k} {k \choose i-1}  i! S(n,i)$ and this is the same like $\sum_{i=1}^{k} {k \choose i}  i! S(n,i)$ minus the last term ${k \choose k}k! S(n,k)$ but then I get stuck here. Any suggestions ? and then we","['discrete-mathematics', 'induction', 'combinations', 'combinatorics', 'stirling-numbers']"
1465203,Evaluate limit of $(2\sin x\log \cos x + x^{3})/x^{7}$ as $x \to 0$,"While trying to solve this question , I came across the following limit $$\lim_{x \to 0}\frac{2\sin x\log \cos x + x^{3}}{x^{6}}\tag{1}$$ Using some algebraic manipulation (and L'Hospital's Rule) I was able to show that $$\lim_{x \to 0}\frac{2\sin x\log \cos x + x^{3}}{x^{5}} = 0\tag{2}$$ From the fact that the numerator in the above limit expression is an odd function, I guessed that the limit in $(1)$ would also be $0$ (it would be great if this guess can be supported by a proof). However I was not able to do this via simple algebraic manipulation. Also note that evaluating $(1)$ is equivalent to solving the linked question (without the assumption of existence of limit). I think it is better to go one step ahead and establish that $$\lim_{x \to 0}\frac{2\sin x \log \cos x + x^{3}}{x^{7}} = -\frac{1}{40}\tag{3}$$ It is possible to evaluate the above limit via Taylor's series very easily, but I would prefer to have a solution of either $(1)$ or $(3)$ without using Taylor's series. Update : I provide an evaluation of limit $(2)$ as an illustration of the kind of answer I would prefer. We have
\begin{align}
L &= \lim_{x \to 0}\frac{2\sin x\log \cos x + x^{3}}{x^{5}}\notag\\
&= \lim_{x \to 0}\frac{\sin x\log (1 - \sin^{2}x) + x^{3}}{x^{5}}\notag\\
&= \lim_{x \to 0}\frac{\sin x\log (1 - \sin^{2}x) + \sin^{3}x + x^{3} - \sin^{3}x}{x^{5}}\notag\\
&= \lim_{x \to 0}\frac{\sin x\log (1 - \sin^{2}x) + \sin^{3}x}{x^{5}} + \frac{x^{3} - \sin^{3}x}{x^{5}}\notag\\
&= \lim_{x \to 0}\frac{\sin x\log (1 - \sin^{2}x) + \sin^{3}x}{\sin^{5}x}\cdot\frac{\sin^{5}x}{x^{5}} + \lim_{x \to 0}\frac{x - \sin x}{x^{3}}\cdot\frac{x^{2} + x\sin x + \sin^{2}x}{x^{2}}\notag\\
&= \lim_{x \to 0}\frac{\log (1 - \sin^{2}x) + \sin^{2}x}{\sin^{4}x}\cdot 1 + \lim_{x \to 0}\frac{1 - \cos x}{3x^{2}}\cdot (1 + 1 + 1)\text{ (via LHR)}\notag\\
&= \lim_{t \to 0}\frac{\log (1 - t) + t}{t^{2}} + \frac{1}{2}\notag\\
&= \lim_{t \to 0}\dfrac{-\dfrac{1}{1 - t} + 1}{2t} + \frac{1}{2}\text{ (via LHR)}\notag\\
&= -\frac{1}{2} + \frac{1}{2} = 0\notag
\end{align} Further Update : I have finally found a solution which uses algebraic manipulation and L'Hospital's Rule. The rule has been applied 4 times in total and resulting expressions are simple. See the details in my answer.","['calculus', 'limits']"
1465268,What are the next primes in this sequence?,"Define $z_k$ to be the smallest number $z$ , such that $$z\equiv \phi(\phi(p))\pmod p$$ for every prime $p\le k$ . We can assume, that $k$ itself is prime. The first few numbers are $z_2=z_3=1$ , $z_5=7$ , $z_7=z_{11}=37$ , $z_{13}=11\ 587$ , $z_{17}=131\ 707$ $z_k$ is prime for $k=5,7,11,13,17,163,197,199,557,587,673,1753,2719,3449$ and no other prime $k\le 10007$ What are the next primes $z_k$ ? Is $z_k$ prime for infinite many prime numbers $k$ ?","['prime-numbers', 'number-theory', 'chinese-remainder-theorem']"
1465274,Is $2^n -1$ finitely many times the product of consecutive primes?,"Are there finitely many $(n,k) \in \mathbb{N}^2$ with 
  $$2^n-1=p_1p_2\cdots p_k$$ 
  where $p_1=3,p_2=5 ,\dots,p_k$ are consecutive odd primes in ascending order? An example is when $n=4, k=2$: $$2^4-1=3\cdot 5=p_1p_2.$$ 
Are there finitely many $n$? I tried to use Zsigmondy's theorem without success.  Thanks in advance!","['prime-numbers', 'number-theory', 'elementary-number-theory']"
1465284,Prove that $U(n)$ is a manifold,"I would like to prove that $U(n)$ is a manifold, where $$U(n) = \{A \in M_n(\mathbb C): A^*A = I\}.$$ In order to do so I thought of considering the function $M_n(\mathbb C)\to M_n(\mathbb C)$ such that $A\mapsto A^*A$. Then if I prove that the identity is a regular value of this function I'm done. My problem is that I am struggling in proving that. I tried to consider that $(A^*A)_{ij}=\langle v_i,v_j\rangle $ where $v_i$ are the column vectors of $A$ but from there I don't know how to proceed. Does anyone have any suggestion? Thanks","['lie-groups', 'differential-geometry', 'smooth-manifolds']"
1465320,$x^{2000} + \frac{1}{x^{2000}}$ in terms of $x + \frac 1x$.,"If $x + \frac{1}{x} = 1$, then what is $$ x^{2000} + \frac{1}{x^{2000}} = ?$$","['complex-numbers', 'algebra-precalculus']"
1465321,Proving that $\frac{\sin \alpha + \sin \beta}{\cos \alpha + \cos \beta} =\tan \left ( \frac{\alpha+\beta}{2} \right )$,"Using double angle identities a total of four times, one for each expression in the left hand side, I acquired this. $$\frac{\sin \alpha + \sin \beta}{\cos \alpha + \cos \beta} = \frac{\sin \left (  \frac{\alpha}{2}\right ) \cos \left (  \frac{\alpha}{2}\right ) + \sin \left (  \frac{\beta}{2}\right ) \cos \left (  \frac{\beta}{2}\right )}{\cos^2 \left ( \frac{\alpha}{2} \right) - \sin ^2 \left ( \frac{\beta}{2} \right )}$$ But I know that if $\alpha$ and $\beta$ are angles in a triangle, then this expression should simplify to $$\tan \left ( \frac{\alpha + \beta}{2} \right )$$ I can see that the denominator becomes $$\cos \left ( \frac{\alpha + \beta}{2} \right ) $$ But I cannot see how the numerator becomes $$\sin \left ( \frac{\alpha + \beta}{2} \right )$$ What have I done wrong here?","['proof-verification', 'trigonometry']"
1465329,Closed form of integral using contour integration,"Here is the integral I am interested in evaluating using contour integration: Prove that: $$\int_0^\infty \frac{{\rm d}x}{(1+x^2)(1+x^r)}=\frac{\pi}{4}$$ That is that the above integral is independant of $r$ which is assumed to be a positive real number. I have a couple of approaches using real analysis. For instance, $$\begin{align*}
\int_{0}^{\infty}\frac{{\rm d}x}{\left ( 1+x^2 \right )\left ( 1+x^r \right )} &=\int_{0}^{1}\frac{{\rm d}x}{\left ( 1+x^2 \right )\left ( 1+x^r \right )}+ \int_{1}^{\infty}\frac{{\rm d}x}{\left ( 1+x^2 \right )\left ( 1+x^r \right )} \\ 
 &\overset{u=1/x}{=\! =\! =\!}\int_{0}^{1}\frac{{\rm d}x}{\left ( 1+x^2 \right )\left ( 1+x^r \right )} +\int_{0}^{1}\frac{x^r}{\left ( x^2+1 \right )\left ( 1+x^r \right )}\, {\rm d}x \\ 
 &= \require{cancel} \int_{0}^{1}\frac{\cancel{x^r+1}}{\left ( x^2+1 \right )\cancel{\left ( x^r+1 \right )}}\, {\rm d}x \\
 &= \int_{0}^{1}\frac{{\rm d}x}{x^2+1}\\
 &= \arctan 1 = \frac{\pi}{4}
\end{align*}$$ or by applying the sub $x=\tan u$ we have that: $$\begin{align*}
\int_{0}^{\infty}\frac{{\rm d}x}{\left ( 1+x^2 \right )\left ( 1+x^r \right )} &\overset{x=\tan u}{=\! =\! =\! =\!}\int_{0}^{\pi/2}\frac{\sec^2 u}{\left ( 1+\tan^2 u \right )\left ( 1+\tan^r u \right )}\, {\rm d}u \\ 
 &=\int_{0}^{\pi/2}\frac{{\rm d}u}{1+\tan^r u} \\ 
 &= \int_{0}^{\pi/2}\frac{{\rm d}u}{1+\cot^r u}
\end{align*}$$ Since $\cot u = 1/\tan u$ the last integral is evaluated easily again at $\pi/4$. A third approach would be to kill it directly with the sub $u=1/x$ which leads to, if we name the initial integral $I$, $$I= \frac{\pi}{2}-I$$ and the result follows. Now, what I am interested in here is to evaluate this using contour integration. I have a feeling that the contour will be a wedge shaped contour with some angle, let me call that $\omega$, but that $r$ cause some problems. Therefore I don't know how to integrate it using complex analysis. Any help appreciated.","['contour-integration', 'calculus', 'improper-integrals']"
1465331,Understanding Complex Differentials (forms),"In the study of Riemann surfaces, many books bring in their discussions, the complex differentials or differential forms , and there my understanding gets stopped. I personally interacted with many people to understand the meaning of complex differentials or complex differentials form, but no one explained in intuitive or motivational way what that means . I tried to follow some books like - Calculus on manifolds- Spivak, Principles of Mathematical Analysis - Rudin, Analysis on Manifolds - Munkres, or even some specifically written books on differential forms like differential forms - Weintraub Everyone tried to explain it through algebraic meanings, but that is too abstract; I can not see its relations with analysis quickly. Question: Can one give an intuitive explanation of complex differential forms? What should we keep in mind when we came across it in some study? I would mostly welcome explanation through examples as well.","['complex-geometry', 'differential-geometry', 'complex-analysis', 'riemann-surfaces']"
1465347,"Probabilistic convergence of $\cos n\Theta$ with $\Theta$ uniform on $[0,2\pi]$","This is a homework question so I'd appreciate advice rather than a solution. I'm asked to determine in what modes the following sequence of random variables converges. In particular, I am interested in convergence in probability (measure) and also a bit in almost sure convergence. $$X_n=\cos n\Theta$$
where $\Theta$ is uniform on the interval $[0,2\pi]$. My attempt: First a few thoughts: It feels a bit like convergence in probability here is related to the rationals being dense in the reals. Whereas a.s. convergence does not hold since the rationals are only a countable subset and have measure $0$. Well, this is the gut feeling I had anyway. Focusing on convergence in probability (measure), rewriting
$$X_n= \cos (n \Theta \mod \pi)$$
it is enough to show that $n\Theta$ converges $\mod \pi$. I.e. we want to show that
$$P(|n \Theta - m\pi|>\epsilon)\to 0$$
for some $m \in \mathbb{N}$. I don't really see where to go from here or how to show that the above statement is true (maybe it isn't?). Am I missing something? Any advice is much appreciated.","['probability-theory', 'weak-convergence', 'probability-distributions']"
1465366,Interpreting partial derivatives $\frac{\partial f}{\partial u}$ in differential geometry,"I'm reading Do Carmo's ""Riemannian Geometry""  and at some point he introduces the following notation: Let $A \subset \mathbb{R}^2$ be an open region bounded by a piecewise differentiable curve and $f: A \to M$ be a differentiable mapping into a manifold $M$. Now define: $$\frac{\partial f}{\partial u} (u,v) := T_{(u,v)}f \cdot \frac{\partial}{\partial u}$$ And similarily for $\frac{\partial f}{\partial v}$. Next in the book there's a remark saying both of these can be viewed as vector fields along $f$. What does that mean? Does it mean that $(p \mapsto T_{f^{-1}(p)}f \cdot \frac{\partial}{\partial u})$ is a smooth vector field defined on $f(A)$? What if $f^{-1}$ isn't differentiable? Straight after he introduces the notation $\frac{D}{\partial u}$ for the covariant derivative along the curve $(u \mapsto f(u,v_0))$. Is it correct to translate that to $\nabla_{\frac{\partial f}{\partial u}}$ ? What's really bothering me is that I'm finding the partial derivative notation very confusing. After a couple of pages the following expression appears: $$\frac{\partial}{\partial u}<\frac{\partial f}{\partial v},\frac{\partial f}{\partial u}> = <\frac{D}{\partial u} \frac{\partial f}{\partial v},\frac{\partial f}{\partial u}> + <\frac{\partial f}{\partial v},\frac{D}{\partial u} \frac{\partial f}{\partial u}>$$
Which to me looks extremely confusingly simplistic. As i see it, $\frac{\partial}{\partial u}$ at the start of the line must really stand for $\frac{\partial f}{\partial u}$ and plugging it in it does make much more sense to me (and looks like the usual inner product rule for covariant derivatives). Now, for the general question: How should I think of an object like $\frac{\partial f}{\partial u}$ in the context of differential geometry? Should I always think of it $Tf \cdot \frac{\partial }{\partial u} : A \to TM$ ? If $f$ is a diffeomorphism i can interpert it as a pushforward of the section $\frac{\partial }{\partial u} |_{(u,v)}$ by $f$ i.e. $$\frac{\partial f}{\partial u} := f_* (\frac{\partial}{\partial u}) = Tf \circ \frac{\partial}{\partial u} \circ f^{-1} : f(A) \to TM$$ Is this the right interpretation? I'm really lost here, thanks for the help.","['differential-geometry', 'vector-analysis', 'riemannian-geometry']"
1465378,Unbounded sequence diverging to $\infty$?,"If $\{x_n\}$ is a sequence of positive real numbers which is not bounded, then it diverges to infinity. State whether the above statement is true or false.
  If true/false , give the reason. I just know that if the sequence is of positive real numbers then it must be either increasing or decreasing sequence or it would be constant sequence. My question is what can I conclude about the word ""not bounded""? Does it means not bounded above or not bounded below or neither bounded above nor bounded below? Anyone just help me to draw conclusion whether the statement is true or not?","['sequences-and-series', 'real-analysis']"
1465398,The Normal Bundle of a level set is trivial,"Let $M,N$ be manifolds and $f:N\to M$ an embedding.
For every $p\in N$, $df_p:T_pN\to T_pM$ shows that $T_pN\subset T_pM$.
For every $p\in N$, define the normal space of $N$ in $M$ at $p$ as 
$$
(\eta_{N|M})_p:=\frac{T_pM}{T_pN}.
$$
Gluing together all these spaces, we obtain
$$
\eta_{N|M}:=\frac{TM|_N}{TN}
$$
the normal bundle of $N$ in $M$. The Tubular Neighbourhood Theorem holds. Let $S$,$M$ be manifolds without boundary, $i:S\to M$ an embedding.
  Then there exists $T$, neighborhood of $i(S)$ in $M$ $v:\eta_{S|M}\to T$ such that $i=v\circ s_0$, where $s_0:S\to \eta_{S|M}$ is the zero section. Now, let $M$ be a manifold with our boundary, $f:M\to\mathbb{R}$ be a smooth function, $y\in Reg(f)$ and $N$ be a connected component of $f^{-1}(y)$. I have to prove that $\eta_{N|M}$ is trivial. I can't understand how to proceed. First of all because the definition of normal bundle is not clear to me at all.","['differential-topology', 'differential-geometry', 'smooth-manifolds', 'fiber-bundles']"
1465402,Real orthogonal matrices with fixed entry value has zero Haar-measure?,"Consider the set of real orthogonal matrices of size $n \times n$ such that one entry, say $a_{i,j}$ for fixed $i$ and $j$, satisfies $a_{i,j}=0$. Has this set zero Haar-measure? A simple proof, in affirmative (which I believe) or negative case, would be appreciated. Many Thanks in advance.","['measure-theory', 'matrices', 'lie-groups', 'real-analysis', 'topological-groups']"
1465414,Is a Markov process uniquely determined?,"Let $E$ be a Polish space and $\mathcal E$ be the Borel $\sigma$-algebra on $E$ $I\subseteq[0,\infty)$ be closed under addition and $0\in I$ Please consider the following result: Let $(\kappa_t:t\in I)$ be a Markovian semigroup on $(E,\mathcal E)$ $\Rightarrow$ There is a measurable space $(\Omega,\mathcal A)$ and a Markov process $X$ with distributions $(\operatorname P_x)_{x\in E}$ such that $$\operatorname P_x\left[X_t\in B\right]=\kappa_t(x,B)\;\;\;\text{for all }x\in E,B\in\mathcal E\text{ and }t\in I\;.\tag 1$$ Conversely, given a Markov process $X$ with distributions $(\operatorname P_x)_{x\in E}$ on a measurable space $(\Omega,\mathcal A)$, a Markovian semigroup $(\kappa_t:t\in I)$ is defined by $(1)$. It turns out that $X$ in the first part of the statement can be constructed as the family of coordinate maps on $(\Omega,\mathcal A)=(E^I,\mathcal E^{\otimes I})$. I've seen that many authors assume that Markov processes are such coordinate maps. Why can they do that? The statement above doesn't state, that given $(\Omega,\mathcal A)$ there is one unique Markov process, does it? However, the finite-dimensional distributions of $X$, i.e. $$\operatorname P_x\left[X\in\;\cdot\;\right]\circ\pi_J^{-1}\;\;\;\text{for }J\subseteq I\text{ with }|J|<\infty\;,\tag 2$$ where $\pi_J:E^I\to E^J$ are the canonical projections, are uniquely determined by $(1)$. Maybe $(\operatorname P_x)_{x\in E}$ (not only the finite-dimensional distributions) are uniquely determined by $(1)$, if $I\subseteq \mathbb N_0$ or $I$ is at least almost countable or when $E$ is almost countable. So, why does the stated result allows us to think about $X$ as being uniquely determined?","['probability-theory', 'markov-process', 'markov-chains', 'stochastic-processes', 'probability']"
1465416,Measure of how far a basis is from orthogonality,"here's a problem that is a bit vaguely defined. Let the $m\times m$ matrix $U$ have columns $U_i$ that form a basis in $R^m$. Assume these columns have unit length, $|U_i|=1$. The matrix $R = U^\top U$ then contains scalar products, $R_{ij} = U_i^\top U_j$. Obviously, $R$ is symmetric and positive semi-definite (we assume definite). The question is: is there a standard measure of ""how far"" this basis is from being orthogonal? The transformation $V = U R^{-1/2}$ constructs an orthonormal basis $V$ (since $V^\top V = I$; $R^{-1/2}$ is the inverse of the symmetric square root of $R$ -- could also be a Cholesky factor). Thus one may suspect that a ""standard measure of distance from orthogonality"" should use in $R$ in some way... The condition number of $R$ maybe? Any pointers appreciated! /Tobias","['orthogonality', 'linear-algebra']"
1465450,Expected mean squared error and MSR,"In a small-scale regression study, five observations on $Y$ were
  obtained corresponding to $X = 1,4,10, 11$, and $14$. Assume that
  $\sigma=0.6,B_0=5,B_1=3$ a. What are the expected values off MSR and MSE here? b. For derermining whether or not a regression relation exists, would
  it have been better or worse to have made the five observations at $X
 = 6,7, 8, 9$, and $10$? Why? Would the same answer apply if the principal purpose were to estimate the mean response for $X = 8$?
  Discuss. $$Y_i=B_0+B_1X_i+\epsilon_i$$
$$\hat{Y}_i=\hat{B}_0+\hat{B}_1X_i$$ $$MSR=\sum(\hat{Y}_i-\overline{Y})^2$$ $$MSE=\frac{\sum (Y_i-\hat{Y}_i)^2}{n-2}=\frac{\sum(B_0+B_1X_i+\epsilon_i-\hat{B}_0-\hat{B}_1 X_i)^2}{n-2}$$ I'm still doesn't understand what they want, they want
$$E(MSE);E(MSR) \text{ ?}$$ What they mean by expected values?","['self-learning', 'statistics', 'mean-square-error', 'regression']"
1465463,find the eigenvalues of $5×5$ matrix,"The product of the non-zero eigenvalues of the matrix is ____. using characteristic equation , it is too lengthy to find eigenvalues of $5×5$ matrix , I'm looking for any short trick to solve this question .","['eigenvalues-eigenvectors', 'linear-algebra', 'matrices']"
1465469,what are prerequisite to study Stochastic differential geometry?,"I am doing a project in stochastic differential geometry mainly(Brownian motion on manifolds) ,the proof of laplacian-beltrami operator. I am currently reading williams ""diffusions and markov process"" and i want to get book recommendation, so that i can go through this text easily .i am currently reading stochastic process by jazwinski .i also want to know from where i should read the geometry part of it.","['book-recommendation', 'stochastic-calculus', 'differential-geometry']"
1465471,What probability would you assign to India's win?,"Karan tells truth with probability $\frac 13$ and lies with probability $\frac 23$. Independently, Arjun tells truth with probability $\frac 34$ and lies with probability $\frac 14$. Both watch a cricket match. Arjun tells you that India won, Karan tells you that India lost. What probability will you assign to India's win? $(a) \frac  12$ $(b)\frac 23$ $(c)\frac  34$ $(d)\frac 56$ $(e)\frac 67$ According to me the answer should be $\frac 12$ i.e option $(a)$ Arjun told: India won, Karan told: India lost, Probability of India won = Probability that Arjun told truth$(=\frac 34)$ & Karan lied$(= \frac 23)$ So probability that India won = $\frac 34\times\frac 23 =\frac 12$.
Is it correct?",['probability']
1465528,"Contradiction from evaluating $\int _0^2 (x^2+1) \; d \lfloor x\rfloor$ by splitting into two parts, where is the error?","Evaluate: $$\int _0^2 (x^2+1)  \; d \lfloor x\rfloor$$ Here $[x]$ denotes the greatest integer function of $x$. I know this has to be done by parts as: $$\int _0^2 (x^2+1) \, d [x]= {|(1+x^2)[x]|}_0^2- \int_0^2 [x] \, d(1+x^2)$$ Note:- This integral can be quite easily evaluated.I don't need the method for this. But if we split the given integral into the sum of $2$ integrals as:- $$\int _0^2 (x^2+1) \, d [x]=\int _0^1 (x^2+1)  d [x]+\int _1^2 (x^2+1) \, d [x]$$ My question is as $[x]$ is constant in each of the intervals $[0,1)$  and $[1,2)$ in each of these $2$ integrals $d[x] = 0$ So, the value of the given integral should be $0$. This seems contradictory !! Kindly correct my reasoning for the part $d[x]=0$.","['definite-integrals', 'integration']"
1465556,Limit $\lim_{n \to \infty} \frac {a_n}{2^{n-1}}=\frac 4{\pi}$ for $a_{n+1}=a_n+\sqrt{1+a_n^2}$ and $a_0=0$,"I found the following question in a book:- $Q:$Let $a_1, a_2, ... , a_n$ be a sequence of real numbers with $a_{n+1}=a_n+\sqrt{1+a_n^2}$ and $a_0=0$. Prove that $$\lim_{n \to \infty} \frac {a_n}{2^{n-1}}=\frac 4{\pi}$$ I tried many things none of which seemed fruitful. First thing I did was to define $a_n=2^{n-1}b_n$. Substituting this into the condition, we get $$2^{n}b_n=2^{n-1}b_n+\sqrt{1+2^{2n-2}b_n^2}\implies b_{n+1}=\frac {b_n}2+\sqrt{\frac1{2^{2n}}+\left(\frac {b_n}2\right)^2}$$ Simplifying this gives $$b_{n+1}^2-b_{n+1}b_n=\frac1{2^{2n}}\implies b_{n+1}(b_{n+1}-b_n)=\frac1{2^{2n}}$$ This doesn't lead anywhere. One lead that I got was by substituting $a_n=\tan \theta_n$. This gives $$\begin{align}a_{n+1}&=\tan \theta_n+\sec \theta_n\\&=\frac{\sin\theta_n+1}{\cos\theta_n}\\&=\frac{1+\tan\frac{\theta_n}2}{1-\tan\frac{\theta_n}2}\\&=\tan\left(\frac {\theta_n}2+\frac{\pi}4\right)\end{align}$$ Hence $$\tan\theta_{n+1}=\tan\left(\frac {\theta_n}2+\frac{\pi}4\right)\implies \theta_{n+1}=\frac {\theta_n}2+\frac{\pi}4\implies \theta_n=\frac {\pi}2+c\left(\frac 12\right)^n$$From initial conditions we get $\theta_0=0\implies c=-\frac{\pi}2$. Therefore $$\lim_{n \to \infty} \frac{a_n}{2^{n-1}}=\lim_{n\to\infty}\frac{\tan\left(\frac {\pi}2-\frac{\pi}2 \left( \frac 12 \right)^n\right)}{2^{n-1}}=\lim_{n\to\infty}\frac{2\left(\frac12\right)^n}{\tan\left(\frac {\pi}2\left(\frac12\right)^n\right)}=\lim_{n\to\infty}\frac{2\left(\frac12\right)^n}{\frac{\pi}2\left(\frac12\right)^n}=\frac4{\pi}$$ Is this proof valid and are there any other ways of doing it?","['calculus', 'limits', 'substitution', 'proof-verification', 'trigonometry']"
1465567,Flatness and constancy of Hilbert polynomial,"Let $S$ be a scheme of finite type over $\mathbb C$, and $f:X\to S$ a projective morphism. Let $F$ be a sheaf on $X$, and for a point $s\in S$ let us denote by $F_s=F\otimes_{\mathscr O_S}k(s)$ the induced sheaf on $X_s=f^{-1}(s)$. Question . Assume $F_s$ is supported in dimension zero for all $s\in S$, and $h^0(F_s)$ is constant on $S$. Is $F$ flat over $S$? So, I think the answer would be yes if $S$ were reduced, as constancy of Hilbert polynomial and flatness are equivalent over a reduced base. But I was hoping that the zero-dimensional assumption could provide an example where one does not need reducedness to conclude that the constancy of the Hilbert polynomial implies flatness. Is this the case? Would any more assumptions on the morphism $f$ be of any help? For example, if the family was projection from a product $X=Y\times S\to S$ with $Y$ a smooth variety (so $X_s\cong Y$ for all $s\in S$), could we say the answer is yes in this case? Thank you!",['algebraic-geometry']
1465571,"What is the radius of the circle through $(-1,1)$ and touching the lines $x\pm y=2?$","What is the radius of the circle through $(-1,1)$ and touching the lines $x\pm y=2?$ The lines $x+y=2$ and $x-y=2$ are perpendicular to each other and the circle is touching both the lines,these lines are tangents to the circle.Let points of tangency be $P$ and $Q$,let the center of the circle be $O$ and let the point where the lines $x\pm y=2$ meet be $R$.$OP$ is perpendicular to $PR$ and $OQ$ is perpendicular to $QR$,therefore $OPRQ$ is a square and the point $(-1,1)$ does not lie on any of the lines $x\pm y=2$. But now i am stuck,how to solve further.Please help me.","['geometry', 'circles', 'analytic-geometry']"
1465578,"I want to prove $[0, \infty)$ with $d(x, y) = |\sqrt x - \sqrt y|$ is not complete.","prove that $[0, \infty)$ with $d(x, y) = |\sqrt x - \sqrt y|$ is not complete. I want to find Cauchy sequence but not convergent with $d$.
It seems too hard for me.
Please tell me the hint.","['analysis', 'metric-spaces', 'general-topology']"
1465583,If $|z_1|=|z_2|=|z_3|$ and $\arg z_1\leq \arg z_2 \leq \arg z_3$ prove that $\arg{\frac{z_3-z_2}{z_3-z_1}}=\frac{1}{2}\arg \frac{z_2}{z_1}$,"If $z_1,z_2,z_2 \in \mathbb{C}$ and $|z_1|=|z_2|=|z_3|$ and $\arg z_1\leq \arg z_2 \leq \arg z_3$ prove that $$\arg{\frac{z_3-z_2}{z_3-z_1}}=\frac{1}{2}\arg \frac{z_2}{z_1}$$ Answer: $\ \arg z_1= \varphi_1,\ \arg z_2= \ \varphi_2, \ \arg z_3=\varphi_3\ldots$ The answer comes to the point where: $$\arg\frac{z_3-z_2}{z_3-z_1}=\arg\left(e^{i \frac{\varphi_1-\varphi_2}{2}}\frac{\sin{\frac{\varphi_3-\varphi_2}{2}}}{\sin{\frac{\varphi_3-\varphi_1}{2}}}\right)= \text{ (this part is not clear)}\\$$ $$ = \arg e^{i \frac{\varphi_1-\varphi_2}{2}}+\arg
 \frac{\sin{\frac{\varphi_3-\varphi_2}{2}}}{\sin\frac{\varphi_3-\varphi_1}{2}}=\arg
 e^{i \frac{\varphi_1-\varphi_2}{2}}$$ $$=\frac{1}{2}\arg \frac{z_2}{z_1}$$","['analysis', 'complex-analysis', 'inequality']"
1465589,Does 'connected surface' in differential geometry actually mean 'path-connected surface'?,"While studying differential geometry I often come across propositions with $M$ being a connected surface as their hypothesis. They then often take paths between arbitrary points, which to me suggests that they actually mean that $M$ is path-connected. Is this true? Is it some sort of convention in differential geometry to not distinguish between normal- and path-connectedness?","['differential-geometry', 'definition', 'terminology']"
1465590,the first chern class of complex vector bundles,"Let $\xi^\mathbb{C}$ be an $n$-dimensional complex vector bundle over a manifold $M$, $n\geq 2$. Question 1: Are there any practical methods to detect whether the first Chern class (with integer coefficients, NOT with rational coefficients) 
$$c_1(\xi^\mathbb{C})$$ is zero or not? I have found two ideas: (I). The first idea is given in the mathoverflow question as in the following picture . Question 2: Where to find the references giving the formula
$$
c_1(\wedge ^n \xi^\mathbb{C})=c_1(\xi^\mathbb{C})?
$$ (II). The second idea is given in Vanishing of the first Chern class of a complex vector bundle . Question 3: Any references giving whether the following is true or not?
""The first Chern class $c_1(\xi^\mathbb{C})=0$ if and only if the structure group of $\xi^\mathbb{C}$ can be reduced to $SU(n)$."" But I think the answer is about rational coefficient Chern class, not integer coefficient as we want.","['homology-cohomology', 'fiber-bundles', 'characteristic-classes', 'differential-geometry', 'algebraic-topology']"
1465600,Is this proof correct?($\mathbb{Q}$ is not locally compact),"I am supposed to prove that $\mathbb{Q}$ is not locally compact. The definition of locally compactness is: A space $X$ is said to be locally compact at x if there is some
  compact subspace C of X that contains a neighborhood of x. If X is
  locally compact at each of its points, X is said simpy to be locally
  compact. Attempt at proof: Look at the number 0. Assume that there is some compact subset A in $\mathbb{Q}$, that contains a neighborhood around 0. Then it must conain an interval around zero. Look at the identity function from $\mathbb{Q}\rightarrow \mathbb{R}$, this is a continuous function, so $f(A)$ must also be compact in $\mathbb{R}$. But $f(A)$ contains an interval of rational numbers around zero, so we can find an irrational number close to zero, that is ""between"" rational numbers close to zero, every interval around this neighborhood will contain rational numbers, hence f(A) is not a closed subset of $\mathbb{R}$. But since $\mathbb{R}$ is Hausdorff, every compact set, must be closed, hence we have a contradiction, and $f(A)$ can not be closed. Can I prove it this way?, it is a little messy. I wasn't able to show it directly with the open covering property, that is: If we are in $\mathbb{Q}$: for every set around a rational number, containing a neighborhood of the rational number, this set can not have the open covering property, is it hard to prove it directly?","['proof-verification', 'general-topology', 'compactness']"
1465627,Maximising determinant problem,The problem is to maximize the determinant of a $3 \times 3$ matrix with elements from $1$ to $9$. Is there a method to do this without resorting to brute force?,"['discrete-optimization', 'optimization', 'linear-algebra', 'matrices']"
1465628,Law of Sines - Not Working?,"Sorry that I cannot post a picture (I don't have 10 rep), so this might be confusing. Basically, I had a bunch of lines, two parallel, and 2 transverse lines going through them, making a triangle. The triangle only has an angle and 2 sides. However, since two of the lines are parallel, I am able to figure out that one of the angles of the triangle is 40 degrees, and another is 70 degrees. Problem Now: Lets say I now have $\triangle {ABC}$ $\angle A = 40 ^\circ $ $AB = 10 $ $\angle B = 70 ^\circ $ $BC = 7$ $\angle C = 70 ^\circ $ ** Find CA ** Using my head, I know the answer for CA is 10, since this is an isosceles triangle. However, what I don't get is why Law of Sines doesn't work here. What I did was set $\frac{\sin 40}{7} = \frac {sin(70)}{x}$. Solving for $x$, I got 10.23. Is it supposed to do this, or did I just mess up somewhere?","['euclidean-geometry', 'triangles', 'trigonometry']"
1465629,Numerically solving a non-linear PDE by an ODE on the Fourier coefficients,"I need to solve numerically a PDE of the form
$$
u_t(x,t)=u_{xx}(x,t)+u_x(x,t)^2-a(x)u_x(x,t)-a_x(x)
$$
with initial condition $u(x,0)=u_0(x)$. I can assume that both $u(\cdot,t)$ and $a(\cdot)$ are periodic. I did an implementation of the method of lines and so far it works OK, but I was wondering whether I could improve on it. So I converted the PDE into a coupled system of ODEs by expanding $u(x,t)$ into a Fourier series to obtain the corresponding system on the coefficients. This is, by expressing $u(x,t)=\sum_{n\in\mathbb Z} c_n(t) e^{inx}$
and $a(x)=\sum_{n\in\mathbb Z} a_n e^{inx}$, I have:
\begin{eqnarray}
u_t(x,t)&=&\sum_{n\in\mathbb Z} c_n'(t) e^{inx},\\
u_x(x,t)&=&\sum_{n\in\mathbb Z} c_n(t)in e^{inx},\\
u_{xx}(x,t)&=&-\sum_{n\in\mathbb Z} c_n(t)n^2 e^{inx},\\
a_x(x)&=&\sum_{n\in\mathbb Z} a_nin e^{inx},\\
a(x)u_{x}(x,t)&=&\sum_{n\in\mathbb Z} (a*u_x)_n e^{inx}=\sum_{n\in\mathbb Z} \big(\sum_{k\in\mathbb Z} a_k c_{n-k}(t)i(n-k)\big) e^{inx},\\
u_{x}(x,t)^2&=&\sum_{n\in\mathbb Z} (u_x*u_x)_n e^{inx}=-\sum_{n\in\mathbb Z} \big(\sum_{k\in\mathbb Z} c_k(t) c_{n-k}(t)k(n-k)\big) e^{inx},
\end{eqnarray}
where $(a*b)_n=\sum_{k\in\mathbb Z} a_k b_{n-k}$. Then by matching the Fourier coefficients, the original PDE turns into the system of ODEs
$$
c_n'(t)=-c_n(t)n^2-\sum_{k\in\mathbb Z} c_k(t) c_{n-k}(t)k(n-k)-\sum_{k\in\mathbb Z} a_k c_{n-k}(t)i(n-k)-a_nin,\quad n\in\mathbb Z.
$$
What I do next is to solve the truncated system of ODEs for $n\in\{-N,\ldots,N\}$ with an ODE solver. This implies truncating the convolutions up to the coefficients with indexes in $\{-N,\ldots,N\}$. The solver receives the real coefficients $(a_0,a_1,\ldots,a_n,b_1,\ldots,b_n)$ corresponding to $c_n$ using equation (29) from here . I am not getting the results that I might expect, and in particular when I increase $N$ the solution seems to shrink towards the initial condition . Here are some pictures illustrating this phenomena for simple choices of $a(\cdot)$ and $u_0(\cdot)$. (The method of lines gives the right solution.) I wonder whether what I am experiencing is just a bug in my code or something is inherently wrong in the math. So my question is: is this a valid numerically scheme? I.e., is it guaranteed that when $N\to\infty$ the solution will converge to the solution of the PDE? Hints on what can be causing the shrinking or related alternative approaches are very welcomed.","['fourier-analysis', 'fourier-series', 'numerical-methods', 'ordinary-differential-equations', 'partial-differential-equations']"
1465630,Minkowski-like inequality for the trace of outer products of random vectors,"I am wondering if the following inequality is correct and can be shown? Let $A$ and $B$ be random vectors of dimension $n$.  Then for $ p \ge 1$
\begin{align}
E^{\frac{1}{2p}} \left[ \left| Tr \left\{(A-B)(A-B)^T \right\} \right|^p \right] \le E^{\frac{1}{2p}} \left[\left| Tr \left\{AA^T \right\} \right|^{p} \right]+ E^{\frac{1}{2p}} \left[\left| Tr \left\{BB^T \right\} \right|^{p}\right]
\end{align} For $n=1$ the inequality becomes \begin{align}
E^{\frac{1}{2p}} \left[  |A-B|^{2p} \right] \le E^{\frac{1}{2p}} \left[|A|^{2p}\right]+ E^{\frac{1}{2p}} \left[ |B|^{2p}\right]
\end{align}
which simply follows from Minkowski inequality. How would one even start proving an inequality like this?
If you can also point me to some similar result it will be great. 
The only related discussion on this cite that I found is here Any help would be greatly appreciated. Thank you","['probability-theory', 'expectation', 'random-matrices', 'functional-analysis', 'inequality']"
1465660,Largest family of subsets,"Given an $n$-element set $S$, the problem is to find the size of the largest possible family of subsets $\{S_1, \dots, S_m\}, S_i \subseteq S$ with constraints $|S_i| = 3, |S_i \cap S_j| \neq 1$. My thoughts on it are below - I think I'm on the right way to the solution, but have no idea how to proceed. For 3-element sets intersection $\neq 1$ means they don't intersect, or intersect by 2 elements. An intuitively obvious way to build the required subsets is to split all the $n$ elements into groups of four, and in each group make subsets like $\{1,2,3\}, \{2,3,4\}, \{1,2,4\}, \{1,3,4\}$ - this way there will be exactly $n$ subsets for $n$ divisible by 4. This family is maximal by inclusion, i.e. no other subset can be added to it, but how to prove it's the largest one (as is it, actually?)? In case 4 doesn't divide $n$, subsets can look like $\{1,2,3\}, \{2,3,4\}, \{2,3,5\}, \dots$ - but again, is it the largest family? Also it's interesting, if there is some general method to solve this, when $|S_i| = k, |S_i \cap S_j| \neq l$? I couldn't apply similar intuition here...","['elementary-set-theory', 'combinatorics']"
1465691,Convexity and mean curvature,"Let $N$ be a Riemannian 3-Manifold with and $M \subset N$ an embedded, oriented codimension $0$ submanifold-with-boundary, bounded by a non-empty smooth subsurface $S := \partial M$. Now, with $M$ and therefore $S$ being oriented, the latter has a tubular neighborhood $T \cong S\times(-1,1)$ via some diffeomorphism $\phi$, such that under the same $\phi$, $T \cap M \cong S\times(-1,0]$. 
We call this the inward normal section of $T$ and, for $s \in S$, its intersection with the fiber $F_s \subset T$ the inward normal at $s$. Further, we say that $M$ has almost convex boundary if for any $s \in S$ there exists an $\epsilon > 0$ such that for all $0 < \delta < \epsilon$, the cut-off ball $B(\delta,s) \cap M$ is a convex subset of $N$ (any two points in this set can be connected by a minimal geodesic which lies completely inside the set). 
In contrast, $M$ is said to have sufficiently convex boundary if $S$ has nonnegative mean curvature with respect to the inward normal section. Now I want to prove that \begin{equation} \mbox{M has almost convex boundary} \implies\mbox{M has sufficiently convex boundary}. \end{equation}
As it is natural (for me at least) with these kinds of problems, I first tried to solve this problem in eucledian space $\mathbb R^3$ and then look at what needs to be changed and generalized for arbitrary 3-manifolds. Here is what I came up with so far: If $M \subset \mathbb R^3$ and $s \in S$, there is a small neighborhood $U \ni s$ and a smooth function $f: U \to \mathbb R$ that describes $U$ in terms of its level sets, i.e we have $U \cap M = f^{-1}[0,\infty)$ and $U \cap S = f^{-1}(0)$. This implies that the inward unit normal field of $S$ restricted to $U$ can be expressed by \begin{equation} \frac{\nabla f}{|\nabla f|}, \end{equation}
with the corresponding mean curvature just being the negative divergence of the above vectorfield. Now assuming $M$ has almost convex boundary and $U$ has been chosen sufficiently small, the unique line segment $(1-t)x + ty, t \in [0,1]$ connecting any two given points $x,y \in S \cap U$ lies completley in $M \cap U$, which is equivalent to $g_y(t) := f((1-t)x + ty) \geq 0$. Since $g_y(0) = f(x) = 0$, we must have $0 \leq g'_y(0) = \nabla f|_x * (y-x)$ by the chain rule. This nicely illustrates an intuitive property for manifolds with almost convex boundary: The angle between the inward normal at $x$ and any vector emanating from $x$, pointing in the direction of $y$, should be at most $90$ degrees for any $y$ on $S$ sufficiently close to $x$. Moreover, it seems to me that this ""angle condition"" should also suffice to prove the nonnegativity of mean curvature. Does anyone have an idea how to proceed from here?","['differential-geometry', 'riemannian-geometry']"
1465692,"Why are split coequalizers ""contractible""?","In the book Toposes, Triples and Theories by Barr and Wells, the authors define a contractible coequalizer (elsewhere known as a split coequalizer) to be a commutative diagram:
$A \rightrightarrows_{d^0}^{d^1} B \to^d C$, together with maps $A \leftarrow^t B \leftarrow^sC$ satifying: $d^0 \circ t = id_B$ $d^1 \circ t = s \circ d$ $d \circ s = id_C$ Now, the definition of $s,t$  feel a lot like a ""contracting homotopy"", which probably is what motivates their terminology, but I would like to make this more precise. That is, I would like to interpret this definition  simplicially, as saying something about the nerve of the relevant diagrams. Can someone show me how this works? I'm not firm on necessarily using nerves, but I'd like an explicit connection with simplicial/topological things. A test of how good the connection is might be:  can you extend the definition of contractible coequalizer to other diagrams/colimits?   I'm open to  overly-sophisticated technology,  so if this has a particularly nice phrasing in some sort of $\infty$ categorical world, let me know. I'm also open to any philosophical musings about how the topological interpretation of a coequalizer fits into the Beck monadicity theorem.","['limits-colimits', 'higher-category-theory', 'general-topology', 'monads', 'category-theory']"
1465697,Quadratic Variation of Brownian motion indexed by cadlag increasing function,"Let $(B_t)_{t \geq 0}$ a Brownian motion on $\mathbb R^n$ and $\ell_t \in \mathbb S$, where $\mathbb S$ is the space of all increasing càdlàg function from $(0,\infty)$ to $(0,\infty)$ with $\lim_{s\downarrow 0} = 0$. Then the quadratic variation of $(B_{\ell_t})_{t\geq 0}$ is given by
  $$[B_{\ell}]_t = \ell_t - \sum_{0<s\leq t}\Delta \ell_s + \sum_{0<s\leq t} | \Delta B_{\ell_s} |^2.$$ Heuristically, this is very clear: the square bracket can be written as the sum of a continuous and discontinuous part. Since the cts. part $[B_\ell]_t^c = l_t$ we get the first summand + the jumps which is the third summand. Finally, since $\ell_t$ is càdlàg, we have to subtracte the jumps $\Delta \ell_s$. My thoughts so far: By Definition, the quadratic variation of a semimartingale $X$ is given by 
$$[X] = X^2 - \int X_- \ \mathrm dX.$$
We have to calculate the stochastic integral for $X_t := B_{\ell_t}$. Let $\Pi = \{0 \leq s_1^k \leq ... \leq s_{n(k)}^k \leq t\}$ be a partition of the interval $[0,T]$. Then
$$\sum_i B_{\ell(s_j)} (B_{\ell(s_{j+1})} - B_{\ell(s_j)}) \overset{ucp}\longrightarrow \int_0^t B_{\ell_{t-}} \ \mathrm d B_{\ell_t}.$$ If there are reasonable sets A und B, then using the elementary equality $b^2 - a^2 - (b-a)^2 = 2a (b-a)$, we find
    \begin{align*}
    	2 \sum_i B_{\ell(s_j)} (B_{\ell(s_{j+1})} - B_{\ell(s_j)}) &= \sum_i (B_{\ell(s_{j+1})}^2 - B_{\ell(s_j)}^2) - \sum_i (B_{\ell(s_{j+1})} - \sum B_{\ell(s_j)})^2\\
    	&= \underbrace{ \sum_{i,B} (B_{\ell(s_{j+1})}^2 - B_{\ell(s_j)}^2) }_{= \ B_{\ell_t}} - \sum_{i,A} (B_{\ell(s_{j+1})}^2 - B_{\ell(s_j))^2 }\\
    	&\qquad - \Bigg( \underbrace{ \sum_{i,B} (B_{\ell(s_{j+1})} - B_{\ell(s_j)})^2 }_{\overset{ucp}{\longrightarrow} \ \ell_t} - \sum_{i,A} (B_{\ell(s_{j+1})} - B_{\ell(s_j)} )^2 \Bigg)\\
    	&\overset{k\to \infty}\longrightarrow B_{\ell_t}^2 - \sum_{0<s\leq t} (B_{\ell_s} - B_{\ell_{s-}} )^2 - (\ell_t  - \sum_{0<s\leq t} (\ell_s - \ell_{s-})) \\
    	&= B_{\ell_t}^2 - \ell_t - \sum_{0<s\leq t} \Delta(B^2)_s + \sum_{0<s\leq t} \Delta \ell_s.
	\end{align*} Is this decomposition of the sums possible? My idea was something like $$J(\epsilon) := \{s \in [0,t] : |\Delta X_s| > \epsilon\}.$$ Since $s \mapsto X_s$ is càdlàg, $J$ is a.s. finite, each $\epsilon > 0$ and $\sum_{s \in J(\epsilon)} \overset{\epsilon \to 0}\longrightarrow \sum_{0<s\leq t}$ a.s. Is there an easier way? Thanks.","['probability-theory', 'stochastic-calculus', 'stochastic-processes', 'quadratic-variation', 'brownian-motion']"
1465698,Set of Possible Polynomials,$$x^6\pm x^5\pm x^4 \pm x^3 \pm x^2 \pm x \pm 1 = p(x)$$ What is the set $A$ of possible polynomials from the class of polynomials $p(x)$ such that the polynomial only has real roots. I am confused over how to approach the problem. Should I use Descarte's rule?,['algebra-precalculus']
1465702,"If $z_1,z_2,z_3 \in \mathbb{C}$ and $|z_1|=|z_2|=|z_3|$ and $z_1+z_2+z_3=0$. Prove that $z_1,z_2,z_3$ are points of a equilateral triangle [duplicate]","This question already has answers here : showing certain vertices form an equilateral triangle (2 answers) Closed 8 years ago . If $z_1,z_2,z_3 \in \mathbb{C}$ and $|z_1|=|z_2|=|z_3|$ and $z_1+z_2+z_3=0$ . Prove that $z_1,z_2,z_3$ are points of a isosceles triangle that is on a unit circle with the center in the coordinate beginning. The answer is given in the following manner: $$z_1=|z_1|e^{i\varphi_1}\\ z_2=|z_2|e^{i\varphi_2} \\ z_3=|z_3|e^{i\varphi_3}\\ |z_1|=|z_2|=|z_3|=|z|$$ It goes on to state that this needs to be true, which I understand why: $$|z_1-z_2|=|z_2-z_3|=|z_3-z_1|=\sqrt{3}|z|$$ and then finding $$|z_1-z_2|=....$$ $$2|z|\left|\sin\left(\frac{\varphi_1-\varphi_2}{2}\right)\right|=2|z|\left|\sin\left(\frac{ \pi }{3}\right)\right|????$$ How can we assume this??","['analysis', 'geometry', 'complex-analysis', 'complex-numbers']"
1465717,An open cover characterization of connected spaces?,"Is it true that a topological space $X$ is connected if and only if for every open cover $\{U_s\}_{s \in S}$ of $X$, and for every pair of points $x_1,x_2 \in X$, there is a finite sequence $s_1,...,s_k$ of elements of $S$ such that $x_1 \in U_{s_1} , x_2 \in U_{s_k}$ and $U_{s_i} \cap U_{s_j} \ne \emptyset$ iff $|i-j|\le1$ ?","['connectedness', 'general-topology']"
1465750,To show there exists a unique function $u \in C^{1}(\mathbb{C^n})$ that satisfies $(\bar{\partial u})=f$,"Assume $n \gt 1$. Let $f$ be a $(0,1)$ form in $\mathbb{C^n}$, with $C^1$-coefficients and compact support $K$, such that $\bar{\partial} f=0$. Let $\Omega_{0}$ be the unbounded component of $\mathbb{C^n}-K$. I need to show there exists a unique function $u \in C^{1}(\mathbb{C^n})$ that satisfies $(\bar{\partial u})=f$ as well as $u(z)=0$ for every $z \in \Omega_{0}$ Let $f=\sum f_j(z) d\bar{z_j} $. Let's define for $z \in \mathbb{C^n}$ $$u(z)=\frac{1}{2\pi i}\int_{\mathbb{C}}f_{1}(\lambda,z_2,..,z_n) \frac{d\lambda \wedge d\bar{\lambda}}{\lambda-z_1}$$
$$=\frac{1}{2\pi i}\int_{\mathbb{C}}f_{1}(\lambda+z_1,z_2,..,z_n) \frac{d\lambda \wedge d\bar{\lambda}}{\lambda}$$ I am trying to evoke the following proposition here: Let $\Omega \subset \mathbb{C}$ be a bounded open set. Suppose $f \in C^{1
}(\Omega)$, $f$ is bounded and $$u(z)=\frac{1}{2\pi i} \int_{\Omega} \frac{f(\lambda)}{\lambda-z} d\lambda \wedge d\bar{\lambda}, (z \in \Omega).$$ Then $u \in C^{1}(\Omega)$ and $\bar{D}u=f$. From the above mentioned Proposition, $\bar{D_1}u=f_1$. I also think that $u \in C^1(\mathbb{C})$ follows from here but I am not sure about it. Now since we have $(\bar{\partial f})=0$, for $2 \le j \le n$, $\bar{D_j}f_1=\bar{D_1}f_j$, so that $$(\bar{D_j})u(z)=\frac{1}{2\pi i}\int_{\mathbb{C}}(\bar{D_j}f_{1})(\lambda,z_2,..,z_n) \frac{d\lambda \wedge d\bar{\lambda}}{\lambda-z_1}$$(why can I take the differential inside: because the function is bounded ??)
$$=(\bar{D_j})u(z)=\frac{1}{2\pi i}\int_{\mathbb{C}}(\bar{D_1}f_{j})(\lambda,z_2,..,z_n) \frac{d\lambda \wedge d\bar{\lambda}}{\lambda-z_1}$$ It should follow from here that the last statement is equal to $f_j(z)$. I am thinking of this proposition (but not able to find out why) $$\text{Let $\Omega$ be a bounded region in $\mathbb{C}$, with smooth oriented boundary $\partial\Omega$. If $u \in C^1(\bar{\Omega})$, then we have $u(a)=\dfrac{1}{2\pi i}\int_{\partial \Omega}\dfrac{u(\lambda)d\lambda}{\lambda-a}-\dfrac{1}{2\pi i}\int_{\Omega}\dfrac{\bar{D}u(\lambda)d\bar{\lambda}\wedge d\lambda}{\lambda-a} $}$$ Suppose $\bar{D_j}u=f_j$ for $1 \le j \le n$, then we are done with the first part. This also shows that $u$ is holomorphic in $\Omega_0$(as the derivatives vanish there). I have no idea how to show that $u(z)=0$ in $\Omega_0$( take $|z_1|$ large in the first equation??) Also why this theorem fails when $n=1$?? Thanks for the help","['several-complex-variables', 'complex-analysis']"
1465768,Find the points on the graph of $f(x) = 12(x + 9) − (x + 9)^3$ where the tangent line is horizontal.,"I cannot figure out how to get started on this question. Would I First simplify, and then take the derivative? Please help!","['calculus', 'derivatives']"
1465812,Sequence of rationals with an irrational limit have denominators going to infinity,"Let $\alpha$ be an irrational real number and let $a_j$ be a sequence of rational numbers converging to $\alpha$. Suppose that each $a_j$ is a fraction expressed in lowest terms: $a_j = \alpha_j / \beta_j$. Prove that the $\beta_j$ tend to $\infty$ Attempt: AFSOC that $\beta_j$ does not tend to infinity, then it is bounded by some $M$. We can also find an interval $\alpha \in (k, k+1)$. Let $\epsilon > 0$, $\exists N_0$ such that for all $N > N_0$ we have $|a_N - \alpha|  < \epsilon$. So $|\frac{p_N}{q_N} - \alpha| < \epsilon$. We try to use the fact that $q$ is bounded but fail to derive a contradiction. I do not know how to use the fact that $\alpha$ is between two integers, although it might not be relevant at all. Ideas?","['real-analysis', 'epsilon-delta']"
1465859,About proving that $Aut(\mathbb{Z}_n)\simeq \mathbb{Z}_n^\times$.,"I need to prove that
$$
Aut(\mathbb{Z}_n) \simeq \mathbb{Z}_n^\times.
$$
My definition of $\mathbb{Z}_n$ is that
$$
\mathbb{Z}_n =\{\bar{m}: m\in \mathbb{Z}\}
$$
where $\bar{m}$ is the equivalence class containing $m$. I have defined the map
$$
\Psi: Aut(\mathbb{Z}_n) \to \mathbb{Z}_n^{\times}
$$
by
$$
\Psi(f) = f(\bar{1}).
$$
I have two questions about this (1) Does this map even make sense? I mean, $f(1)$ is an element of the group $\mathbb{Z}_n$ and not $\mathbb{Z}_n^\times$. My thinking is that this is not a problem because $\mathbb{Z}_n^\times\subseteq \mathbb{Z}_n$. Is that right? (2) Assuming that the map is well-defined, I am trying to show that this is a homomorphism. So I need $\Psi(fg) = \Psi(f)\Psi(g)$. I have
$$
\Psi(fg) = fg(\bar{1}) = f(g(\bar{1})).
$$
and
$$
\Psi(f)\Psi(g) = f(\bar{1})g(\bar{1}).
$$
I can see that on can thinking about $g(\bar{1})$ as just an integer and then it makes sense. But $g(\bar{1})$ is really an equivalence class. So how can I do this precisely?","['equivalence-relations', 'group-theory', 'integers']"
1465865,path-connected boundary implies path-connected closure,Let $X$ be a path-connected space and $Y\subset X$ a subspace. If the boundary $\partial Y$ is path-connected then $\overline{Y}$ is also path-connected. I tried to construct a path joining any two points in $\overline{Y}$ using the path-connectedness of $\partial Y$ but I don't see a way to do it. Also trying to show the contrapositive seems not very helpful. (Unlike statements about connectedness.) Could anyone come up with a hint?,"['connectedness', 'general-topology']"
1465874,"After how much time, in total is the tank full again given the following conditions?","A tank is full of water.A drain pipe,which can empty the full tank in 60 minutes,is opened.18 minutes later another pipe which can fill the empty tank in 30 minutes is opened.After how much time, in total is the tank full again? options: a) 18 b)20 c) 36 d)40 My approach: Tank (A) can empty in 60 minutes. (A) done work $=-\frac{1}{60}$ Lets another tank say (B), can empty in 30 minutes $=\frac{1}{30}$ but after 18 minutes I am not getting how to use these 2 forms to solve the problem. Can anyone guide me how to solve the problem?",['algebra-precalculus']
1465884,Composition of Lie Derivatives,I was wondering if anyone has any insight into what the composition of the lie derivative would look like. For example say you take the lie derivative of some p-form $ \omega $ with $X$ and then $Y$. Using Cartan's formula it looks something like this $$ \mathcal{L}_Y(\mathcal{L}_X\omega) = (i_Yd+di_Y)(i_Xd\omega+d(i_X\omega))=i_Yd(i_Xd\omega) +di_Yi_Xd\omega +  di_Yd(i_X\omega)$$ I was wondering if there are some simplifications that can be made that could get it into a familiar form.,"['differential-geometry', 'differential-forms', 'lie-derivative']"
1465914,Intuitive understanding of behaivor of 2nd order ODE,"I am dealing with the equation: $$my''+y'+y=0$$ and being asked, what do the different graphs look like for varying values of m between .04 and 1. How can I intuitively figure out what the different graphs are going to look like? Is this some kind of famous function??",['ordinary-differential-equations']
1465942,"How would I solve the following equation, which is similar to an algebraic Riccati equation or a nonlinear sylvester equation?","I have the following matrix equation that I would like to solve for $X$: $0 = AX + XB + XCX + D$ In general, $X$ will be rectangular, with $(m\times n)$ dimensions. So if I write the equation out with indices, it is: $0 = A_{mm}X_{mn} + X_{mn}B_{nn} + X_{mn}C_{nm}X_{mn} + D_{mn}$ I assume everything to be real, and $m,n$ are dimensions small enough that a diagonalization of an $m\times n$ matrix is computationally feasible. I see that if $C=0$, then it is just a linear Sylvester equation, and if $A=B$ then it seems to be an Algebraic Riccati equation, but neither of these assertions can be made. I appreciate any guidance towards a solution. Perhaps this is an equation with well known properties (I'm not a mathematician)? Thanks in advance!","['control-theory', 'linear-algebra', 'algorithms', 'matrices']"
1465950,"If $X$ is a right-continuous, discrete Markov process, then $\displaystyle\lim_{t\downarrow 0}\operatorname P_x\left[X_t=x\right]=1$","Let $E$ be an at most countable Polish space and $\mathcal E$ be the discrete topology on $E$ $X=(X_t)_{t\ge 0}$ be a discrete Markov process with values $(E,\mathcal E)$ and distributions $(\operatorname P_x)_{x\in E}$ Claim $\;\;\;$Let $$\operatorname p_t(x,y):=\operatorname P_x\left[X_t=y\right]\;\;\;\text{for }x,y\in E\text{ and }t\ge 0\;.$$ If $X$ is right-continuous, then $$\lim_{t\downarrow 0}p_t(x,x)=1\;.\tag 1$$ How can we prove the claim? I've tried the following: Let $$\tau:=\inf\left\{s>0:X_s\ne X_0\right\}$$ and $x\in E$. Clearly, since $\operatorname P_x$-almost surely $X_0=x$, $$\operatorname P_x\left[\tau>t\right]=\operatorname P_x\left[X_s=X_0\text{ for all }s\in (0,t]\right]\le p_t(x,x)\;\;\;\text{for all }t>0\;.\tag 2$$ Two questions: How exactly does the right-continuity of $X$ imply $$\tau>0\;\;\;\operatorname P_x\text{-almost surely}\tag 3$$ for all $x\in E$? Why can we conclude $(1)$ from $(2)$ and $(3)$?","['probability-theory', 'markov-process', 'markov-chains', 'stochastic-processes']"
1465955,"Proving that if $|z_1|=|z_2|=|z_3|=|z_4|$ and $z_1+z_2+z_3+z_4=0.$ then $z_1,z_2,z_3,z_4$ are points of some rectangle","...or two pairs of points are identical. The assignment is done as follows:
$$z_i=|z|e^{i \varphi_i}=re^{i \varphi_i}, i=1,\ldots,4$$ the assignment goes on to prove that $$\varphi_1-\varphi_2=\pm(\varphi_3-\varphi_4) \\ \varphi_2-\varphi_3=\pm(\varphi_1-\varphi_4) \\ \varphi_1-\varphi_3=\pm(\varphi_2-\varphi_4)$$ then this is unclear: $$|z_1-z_2|=r|e^{\varphi_1}-e^{i\varphi_2}|=2r\left|\sin \frac{\varphi_1-\varphi_2}{2}\right|$$ How??",['complex-analysis']
1465965,Continuity of $L_p$ norm in $p$ with $\varepsilon$-$\delta$ definition,"Assume that $\|f\|_p< \infty$ for $1\le p<\infty$.
In this question we showed that
$$
g(p)=\|f\|_p
$$
is continuous in $p \ge 1$. The technique was to use Dominant Convergence theorem. Using $\varepsilon$-$\delta$ language, what this means is that for any $\varepsilon>0$ there is a $\delta>0$ such that for all $|q-p| < \delta(\varepsilon)$ implies that
$$
\left | \|f\|_p-\|f\|_q \right| \le \varepsilon
$$ My question the following. Can we characterize $\delta(\varepsilon)$ more explicitly in term of $\varepsilon$ and have an expression for $\delta$? Observer, that $\delta$ should probably be a function of $p$ as well, otherwise I don't think it is possible.","['lp-spaces', 'functional-analysis', 'normed-spaces']"
1465979,Invariant tensors derived from an affine connection,"Given $\nabla$,an affine connection in the tangent bundle of a manifold $M,$ we have two very important and cannonical tensors asociated to $\nabla,$ namely the torsion tensor $T$ and the curvature tensor $R.$ Are there any other tensors derived from the connection $\nabla$ and independent from $T$ and $R?$ A tensor will be independent from $T$ and $R$ if it is not produced from $T$ and $R$ by differentiation(covariant) or by an algebraic equation like the tensor product.",['differential-geometry']
1465989,Is null vector always linearly dependent?,"I'm trying to find the column space of $\begin{bmatrix}a&0\\b&0\\c&0\end{bmatrix}$, which I think is $span\left(~\begin{bmatrix}a\\b\\c\end{bmatrix}~\begin{bmatrix}0\\0\\0\end{bmatrix}~\right)$.  Since by definition a span needs to include the null vector, this is redundant.  Would it also be correct to say that this is redundant because $\begin{bmatrix}0\\0\\0\end{bmatrix}$ is linearly dependent on $\begin{bmatrix}a\\b\\c\end{bmatrix}$ (can multiply it by a scalar of 0 to reach the null vector)?  Or is that a misapplication of the concept of linear dependence?","['vector-spaces', 'linear-algebra', 'matrices']"
1466017,Localization commutes with Hom for finitely-generated modules,"Let $M,N$ be $R$-modules with $M$ finitely generated and let $S\subseteq R$ be multiplicatively closed. Then there exists a module isomorphism
  $$S^{-1}\text{Hom}_R(M,N) \xrightarrow{\sim} \text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N).$$ As homework, I found a 'proof' of this statement. However, Does localisation commute with Hom for finitely-generated modules? claims to prove me wrong. I am having trouble in finding the error in my proof. Could someone help me out? I did prove there exists a map $\phi:S^{-1}\text{Hom}_R(M,N) \to\text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N)$ defined by $f/s\mapsto(m/t\mapsto f(m)/st)$, and I am fairly confident that it contains no mistake. Proof : Let $m_1,\dotsc,m_n$ be the generators for M. Let $f\in \text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N)$ be given.
  Choose $n_i\in N$ and $s_i\in S$ such that $f(m_i/1)=n_i/s_i$.
  Define $\pi_f = s_1\dotsm s_n$. Then
  $$\pi f(m/1) = \pi f\left( \sum_{i=1}^n r_i m_i \right) = \pi \sum_{i=1}^n r_i f(m_i) = \pi \sum_{i=1}^n r_i n_i/s_i = \left(\sum_{i=1}\left(\prod_{j=1, j\neq i}^n s_i\right)r_in_i\right)/1,$$
  so $\pi f(m/1) \in N$ using the canonical embedding in $S^{-1}N$, so $m\mapsto \pi f(m/1)$ is a map $M\to N$ and is clearly an homomorphism. If we define the map $\psi:\text{Hom}_{S^{-1}R}(S^{-1}M,S^{-1}N)\to S^{-1}\text{Hom}_{R}(M,N)$ as $$ f \mapsto (m\mapsto \pi_f f(m)) / \pi_f,$$
then we clearly have a homomorphism if it is well-defined. Proof : It remains to check that the element we map to does not depend on choice of $\pi_f$.
  But if we have both $\pi_f$ and $\pi_f'$ for which $\pi_f f$ and $\pi_f' f$ are homomorphisms to $N$, then
  $$(\pi_f f(m))/\pi_f = (\pi_f\pi_f' f(m))/(\pi_f\pi_f') = (\pi_f' f(m))/(\pi_f')$$
  for all $m\in M$, so $\psi$ is in fact well defined. It is now easy to check that $\psi$ and $\phi$ are inverses, which makes them homomorphisms.","['abstract-algebra', 'localization', 'modules', 'commutative-algebra']"
