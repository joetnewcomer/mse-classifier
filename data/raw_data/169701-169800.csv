question_id,title,body,tags
2987424,Lower bound for an integral of a polynomial over an interval of finite length,"Assume that $Q$ is a degree $k$ polynomial and $I$ an interval of finite length $c$ . Can you please give me some hints on how to show that $$ \left( \sum_{m=0}^k \frac{c^{m}}{m!} |Q^{(m)} (t_0) |  \right)^2  \leq \frac{C(k)^{2c}}{c} \int_{I} Q^{2}(t) dt, $$ for some positive constant $C(k)$ depending only on the degree $k$ and any $t_0 \in I$ . The statement is very puzzling and I have had a hard time establishing it. All hints are therefore greatly appreciated. EDIT: If this is wrong, as it looks like, can it be repaired under some additional assumptions? Thank you.","['inequality', 'polynomials', 'integral-inequality', 'real-analysis']"
2987449,How to show these three-regular graphs on 10 vertices are non isomorphic?,"The number of vertices and edges are same, with each vertex having the same degree and the degree sequence of the graph is also the same. I have even tried finding a bipartite graph in any one of them even that seems to fail. Question- How to show the three graphs with degree sequence [3,3,3,3,3,3,3,3,3,3] are non isomorphic (see figure)?","['graph-theory', 'discrete-mathematics']"
2987472,The product of a function that vanishes at infinity and a continuous bounded one,"Let $X$ be a Hausdorff completely regular space. Let $B(X)$ denote the algebra of real bounded functions, and let $$B_\infty (X) = \{ f \in B (X) \mid f^{-1} ([r , \infty)) \text{ is compact} \ \forall r>0 \}$$ be (by definition) the subalgebra of the functions that vanish at infinity. I don't think that $B_\infty (X)$ is an ideal, but if $f \in B_\infty (X)$ and $g$ is continuous and bounded, does $fg$ vanish at infinity? (Take them positive, for simplicity.) Empirical evidence suggests so (for instance, try $f$ the characteristic function of some compact subset), but I am looking for a proof. Notice that if $r>0$ is fixed and $K_r = \{ x \in X \mid f(x) g(x) \ge r \}$ , then $x \in K_r$ implies $g(x) \ne 0$ , therefore $$K_r = \bigcup _{t>0} \left\{x \in X \mid g(x) = t \text{ and } f(x) \ge \frac r t \right\} = \bigcup _{t > 0} g^{-1} (\{t\}) \cap f^{-1} \left( \left[ \frac r t, \infty \right) \right)$$ and this leads nowhere, since an arbitrary union of compact subsets is not necessarily compact, not even closed.","['continuity', 'general-topology', 'analysis', 'compactness']"
2987549,The function from the $\{\}$ to an any other set?,"In the $\mathcal{SET}$ - category of sets and maps between them - there is an initial object - the $\{\}$ . It means that there is unique map from the $\{\}$ to an any other set (object of $\mathcal{SET}$ ). Not sure I understand it completely. A map $\{\} \mapsto S \in Obj(\mathcal{SET})$ must take some $e \in \{\}$ to a some other $s \in S$ . However, there is no such $e$ by the definition of the empty set . Thus such function does exist, however why is it unique ? As long as particular $S$ contains more that one element, there are many maps of $\{\} \mapsto S$ kind. Thus the only way to prove that all those maps are essentially the same is to compare them to each other. In order to run comparsion, one has to kind of ""compute"" those functions - i.e. to assign some $e \in \{\}$ to each and ensure that outputs $s_0, s_1, ..., s_n \in S$ are the same. Otherwise, how can you determine that abovementioned functions are equal bypassing their evluation?","['definition', 'functions', 'category-theory']"
2987572,How to convert this complex number to exponential form?,"I have $Z = {\frac{2+i}{2-i}}$ . I need to write it in exponential form, i.e. $Z = r*e^{i{\phi}}$ . I simplified the given example to ${\frac{3 + 4i}{5}}$ , i.e. $a={\frac{3}{5}}, b={\frac{4}{5}}$ . Therefore, $r = 1$ . Then I calculate $cos({\phi}) = a / r = 3 / 5, sin({\phi})=b/r = 4/5$ . How do I calculate ${\phi}$ from here? The online Complex Number Exponential Form given ${\phi}=tan^{-1}({\frac{4}{3}})$ .","['complex-analysis', 'complex-numbers']"
2987574,"How to ""see"" discontinuity of second derivative from graph of function","Suppose we have a real function $ f: \mathbb{R} \to \mathbb{R}$ that is two times differentiable and we draw its graph $\{(x,f(x)), x \in \mathbb{R} \} $ . We know, for example, that when the first derivative is not continuous at a point we then have an ""corner"" in the graph. How about a discontinuity of $f''(x)$ at a point $x_0$ though? Can we spot that just by drawing the graph of $f(x)$ -not drawing the graph of the first derivative and noticing it has an ""corner"" at $x_0$ , that's cheating. What about discontinuities of higher derivatives, which will of course be way harder to ""see""?","['functions', 'derivatives', 'real-analysis']"
2987598,Gaussian multi-variate integral,"I would like to compute the following integral $$
I_n = \frac{1}{\sqrt{det(2\pi A)}} \int_{\mathbb{R}^n} ||x||^2_2 \exp\left(-\frac{1}{2} x^TAx\right) \mathrm{d} x
$$ where $A$ is symmetric and positive definite. Any suggestions or hints? Thanks in advance!","['multivariable-calculus', 'gaussian-integral']"
2987636,"We have $2n+1$ irrational numbers, then exists $n+1$ of them such that every subset of this set with $n+1$ elements has the sum an irrational number.","Show  that  if   we  are given a set $S$ containing $2n+1$ irrational  numbers,  there  exists a subset $T\subset S$ containing $n+1$ elements, such that every   non-empty subset   of $T$ sums  to an  irrational  number. I   tried   to  consider an equivalence relation on  real numbers: two  elements  are  equivalent iff   the   difference  of   them  is  irrational. Then $n+1$ equivalence classes each have a representative that is ""positive"" or ""negative"" ... but  I  don't  know  hot  to  continue. Any  idea? Thank  you!","['graph-theory', 'combinatorics', 'real-analysis']"
2987674,Difference between germs of holomorphic functions and the functions themselves?,"I'm learning about the space of germs of holomorphic functions. As far as I understand we define the space $\mathcal{O}_x$ to be the set of ""germs"" i.e. equivalence classes of functions that coincide on a certian neighbourhood of the point $x$ . I haven't seen this addressed anywhere so this may be a silly question, but doesn't the identity principle imply that each such germ contains only one function, in any meaningful way? I suppose you could of course define two functions $f$ , $g$ so that they are both the same but one has a smaller domain, or they have disconnected domains and differ on some different components. So i suppose my question is this: It seems like the identity principle implies that two holomorphic functions have the same germ only if they are ""essentially the same"", i.e. can be extended to the same function on some connected domain? Am I wrong in saying this? And if not, doesnt this make the distinction between holomorphic germs and functions a bit pointless, since each of these equivalence classes would only really contain one function?","['complex-analysis', 'germs']"
2987681,Upper semicontinuous functions to $\mathbb{N}$ are locally constant on a dense subset,"Let us take $f : X \rightarrow \mathbb{N}$ an upper semicontinuous function. In Wikipedia - Semi-continuity it is said that such a function must be locally constant on a dense open subset. I don't know how to prove it. I tried playing with $\mathbb{R}_{usc}$ , where the subscript means upper semicontinuity , i.e. the open subsets are of the form $(- \infty, x)$ , $x \in \mathbb{R}$ and considering $i \circ f$ , where $i : \mathbb{N} \hookrightarrow \mathbb{R}_{usc}$ is the inclusion with respect to the subspace topology, but it didn't work. Any help?","['general-topology', 'semicontinuous-functions']"
2987706,Area bounded by $y=\sqrt{\frac{1+\sin{x}}{\cos{x}}}$ and $y=\sqrt{\frac{1-\sin{x}}{\cos{x}}}$,"The area of the region between the curves $y=\sqrt{\frac{1+\sin{x}}{\cos{x}}}$ and $y=\sqrt{\frac{1-\sin{x}}{\cos{x}}}$ bounded by the lines $x=0$ and $x=\frac{\pi}{4}$ is: a) $\int_{0}^{\sqrt{2}-1}\frac{t}{(1+t^2)\sqrt{1-t^2}}dt$ b) $\int_{0}^{\sqrt{2}-1}\frac{4t}{(1+t^2)\sqrt{1-t^2}}dt$ c) $\int_{0}^{\sqrt{2}+1}\frac{4t}{(1+t^2)\sqrt{1-t^2}}dt$ d) $\int_{0}^{\sqrt{2}+1}\frac{t}{(1+t^2)\sqrt{1-t^2}}dt$ My Attempt: So I started with, $$\int_{0}^{\frac{\pi}{4}}\sqrt{\frac{1+\sin{x}}{\cos{x}}}-\sqrt{\frac{1-\sin{x}}{\cos{x}}}dx$$ $$\int_{0}^{\frac{\pi}{4}}\frac{2\sin{x}}{\sqrt{\cos{x}}(\sqrt{1+\sin{x}}+\sqrt{1-\sin{x}})}dx$$ Putting $\cos{x}=u$ $$\int_{1}^{\frac{1}{\sqrt{2}}}\frac{-2}{\sqrt{u}(\sqrt{1+\sqrt{1-u^2}}+\sqrt{1-\sqrt{1-u^2}})}du$$ Putting $u=\frac{1}{w}$ and further $w-1=t$ , I end up with, $$\int_{0}^{\sqrt{2}-1}\frac{2}{(1+t)(\sqrt{(1+t)+\sqrt{(1+t)^2-1}})+(\sqrt{(1+t)-\sqrt{(1+t)^2-1}})}dt$$ But I am not able to reduce this to any of the given options? Am I miscalculating something or this is reducible? Any hints would be helpful. Thank you.","['integration', 'trigonometry', 'definite-integrals']"
2987760,Uniqueness of the Haar measure.,"I am aware that uniqueness of a Haar measure on a locally compact topological group is a well known fact in the sense below: ""If $v,\mu$ are Radon measures that are left invariant then there exists a positive constant $a$ such that $v=a\mu$ .
(Where a Radon measure is a locally finite Borel measure that satisfies the weak inner regularity and outer regularity conditions)."" For the Lebesgue measure on $\mathbb{R}^n$ the above can be strengthened in the sense that the assumptions of regularity are redundant. So, the following holds: ""If $\mu$ is a locally finite Borel measure on $\mathbb{R}^n$ such that $\mu(A+x)=\mu(A)$ for any $A \in \mathcal{B}\left(\mathbb{R^n}\right)$ and $x \in \mathbb{R^n}$ then $\mu=a\lambda$ , where $a\geq0$ and $\lambda$ is the Lebesgue measure."" Is an analogous statement true for an arbitrary locally compact topological group $G$ and a Haar measure $v$ on $G$ . In other words, does the following hold? ""If $G$ is a locally compact topological group, $v$ a Haar measure on $G$ and $\mu$ is a locally finite measure on $G$ that is left invariant, then there exists a positive constant $a$ , such that $\mu=av$ .""","['harmonic-analysis', 'measure-theory']"
2987773,A simple conjecture (and a question) about three parabolas related to any triangle,"Given any triangle, we can build three parabolas, each with focus on one vertex and with directrix the opposing side, as illustrated here: My first conjecture, likely trivial, is that, given any triangle, The three parabolas never intersect, but they are tangent to one another in at most three points. For instance, in case of an equilateral triangle, it seems that the three parabolas ""touch"" each other in three points $E,F,G$ However, it is not obvious to me whether the equilateral triangle is the only case in which we can find three tangential points $E,F,G$ . The question is, then: In which conditions (on the initial triangle) can we find three, two, one or no tangential points? I apologize in case the question is trivial. But thank you very much for your hints, comments, suggestions!","['euclidean-geometry', 'geometric-construction', 'conic-sections', 'geometry', 'triangles']"
2987801,Using MGFs to determine independence of sum and difference of two variables $(X+Y$ and $X-Y)$,"I have a question for class that asks: Let $X$ and $Y$ be i.i.d. Unif $(0,1)$ . Are $X+Y$ and $X-Y$ independent? In an earlier part of the question I found that the covariance between (X+Y) and (X-Y) was 0, but I know this does not necessarily mean independence. My professor advisd that I use their MGFs, because I know that if they are independent then: $$ M_{(X+Y)+(X-Y)}(t) = M_{X+Y}(t) \cdot M_{X-Y}(t) $$ I think that if I let W = X+Y: $$ M_{W}(t) = M_{w_1}(t) + M_{W_2}(t) = M_{X}(t) + M_Y(t) $$ and if I let U = X-Y: $$ M_{U}(t) = M_{U_1}(t) + M_{U_2}(t) = M_{X}(t) + M_{-Y}(t) $$ (but I'm not really sure how to handle the negative). and finally, if I let Z = (X+Y)+(X-Y) = X + Y + X - Y = X + X: $$ M_Z(t) = M_{Z_1}(t) + M_{Z_2}(t) = M_{X}(t) + M_{X}(t) $$ I also know that because X and Y are i.i.d. their MGFs should be the same. So it looks like: $$ M_{(X+Y)+(X-Y)}(t) = 2 \cdot M_X(t) $$ and $$ M_{X+Y}(t) \cdot M_{X-Y}(t) = 3 \cdot M_{X}(t) + M_{-Y}(t) $$ BUT, if that negative signed in $M_{U}$ worked out differently, so that it was $M_X(t) - M_Y(t)$ when I know the MGFs are the same, then the whole thing would come out differently! Both MGFs would ultimately be $2 \cdot M_X(t)$ and they would be independent. (Based on a passage in my text, I think they're supposed to come out independent). So is this whole approach bad, or is it possible I mishandled the negative?","['moment-generating-functions', 'statistics', 'probability']"
2987858,Prove that $M_n/\log n\to 1$ a.s. where $X_i$ are a sequence of i.i.d $\text{exp}(1)$ random variables and $M_n=\max_1^n X_i$,"Question Let $(X_n)_{n\geq 1}$ be an i.i.d sequence of random variables with $P(X_1>x)=e^{-x}$ and put $M_n=\max_1^n X_i$ . Then $M_n/\log n\to 1$ a.s. My attempt I have shown that $\limsup X_n/\log n=1$ a.s and $\lim\inf M_n/\log n\geq 1$ a.s. For the first claim it suffices to note that $$
\sum _{n=1}^\infty P(X_n\geq c \log n)=\sum _n n^{-c}
$$ which is finite if $c>1$ and equal to $\infty$ if $c<1$ . In particular for any $\epsilon >0$ , $X_n/\log n\ge 1+\epsilon $ finitely many times with probability one which implies that $\lim \sup X_n/\log _n\leq 1$ . Similarly, for any $\epsilon >0$ , $X_n/\log n\geq 1-\epsilon$ infinitely often with probability one, so $\limsup X_n/\log n \geq 1$ , For the other claim it suffices to show that $P(M_n/\log n <1-\epsilon \quad \text{i.o})=0$ . This can be shown by an application of Borel Cantelli. Indeed, $$
P(M_n<(1-\epsilon)\log n)=P(X_1<(1-\epsilon)\log n)^n=(1-n^{-(1-\epsilon)})^n\leq \exp(-n\times n^{-(1-\epsilon)}) 
$$ But $\sum \exp(-n^{\epsilon})<\infty$ . My Problem I am unable to show that $\limsup M_n/\log n\leq 1$ a.s. I tried to use Borel cantelli by showing that $P(M_n>(1+\epsilon)\log n \quad \text{i.o})=0$ . To this end, put $c_n=(1+\epsilon)\log n)$ and $$
P(M_n>c_n)=1-P(M_n\le c_n)=1-(1-e^{-c_n})^n\leq \exp 
(-(1-e^{-c_n})^n)
$$ but I am not sure if this sequence is summable. Any help is appreciated and other methods are welcome.","['borel-cantelli-lemmas', 'probability-theory', 'probability', 'real-analysis']"
2987859,Artin's Algebra Exercise 1.1.16,"The question goes $A^k = 0$ , for some $k>0$ ( $A$ is square). Prove that $A+I$ is invertible. I did $A^k = 0 \implies A^k+I=I$ . So, $(A+I)(A^{k-1} +.... + I) = I$ . So it's invertible with the inverse given above. I feel like this is wrong. I'm not very confident that I'm allowed to factorise it like that since $\det{A^k}=(\det{A})^k=0.$ And doing $A^{k-1}$ during factorisation doesn't feel right since you are multiplying with $A^{-1}$ . Can you please direct me to the correct direction and tell me why the thing I did is incorrect? Thanks in advance. Edit: I see that my factorisation is incorrect (plus/minus signs) but the question remains the same.","['matrices', 'linear-algebra', 'inverse']"
2987911,"If $\Sigma$ is a homotopy sphere, then $\Sigma\#(-\Sigma)$ bounds a contractible manifold.","In Kosinski's Differential manifolds , he shows (in page 93) by imbedding $M\#(-M)$ (where $\#$ is the connected sum for differential manifolds and $M$ is an oriented manifold) in $M\times[-1,1]$ that $M\#(-M)$ bounds a manifold that has $M$ with the interior of a disc deleted as a deformation retract. I can get that. However, after showing that, he asserts that if $\Sigma$ is a homotopy sphere, then $\Sigma\#(-\Sigma)$ bounds a contractible manifold. If $\Sigma=S^m$ , it is clear, but I don't see why this is true for a homotopy sphere. Why is this true for a homotopy sphere? (I'm preparing an expository talk which includes this issue, but my knowledge on algebraic topology is not very good: I'm just starting to learn algebraic topology. Maybe this is a silly question).","['differential-topology', 'algebraic-topology', 'differential-geometry']"
2987970,"Proving an identity for $\sum_{m,n\in\mathbb{Z}_{>0}}\frac{\gcd(m,n)^r}{m^sn^t}$.","My task is to prove the well-known identity Here all variables are positive integers I only know  I should use  mobius inversion formula,  but how to proceed I am getting confusion, please any one can help me in this, and is there any article which shows the applications of mobius inversion formula.","['summation', 'number-theory', 'analytic-number-theory', 'transcendental-numbers', 'riemann-zeta']"
2987994,Definite integrals solvable using the Feynman Trick,I'm looking for definite integrals that are solvable using the method of differentiation under the integral sign (also called the Feynman Trick) in order to practice using this technique. Does anyone know of any good ones to tackle?,"['integration', 'definite-integrals', 'big-list', 'calculus', 'leibniz-integral-rule']"
2988014,"Real Analysis, Folland Excercise 2.40","Exercise 40 - Show in Egoroff's theorem, the hypothesis "" $\mu(X)<\infty$ "" can be replaced by "" $|f_n|\le g$ for all $n$ , where $g \in L^1(\mu)$ ."" Egoroff's Theorem - Suppose $\mu(X)<\infty$ , and $f_1, f_2, ...$ and $f$ are measurable complex-valued functions on $X$ such that $f_n \rightarrow f$ a.e. Then for every $\epsilon > 0$ there exists $E\subset X$ such that $\mu(E)<\epsilon$ and $f_n \rightarrow f$ uniformly on $E^c$ . The proof of the original Egoroff's theorem uses continuity from above, which requires the finiteness assumption. I tried to use the dominating $g$ to get some finiteness condition. When $\mu$ is $\sigma$ -finite, I showed the integral of $g$ is concentrated on some set of finite measure. But this does not seem to help proving the statement since the complement of this set still have infinite measure (if $X$ has). Can anyone give me some hint on the problem?
Thank you","['measure-theory', 'real-analysis']"
2988031,Simultaneous real solution of $x^3+y^3+1+6xy=0$ & $xy^2+y+x^2=0$,"I am trying to solve the following system of non-linear equations in real numbers: $x^3+y^3+1+6xy=0$ & $xy^2+y+x^2=0$ , with $x,y$ real. I can only see that $xy\ne 0$ . I have no clue whether a solution exists or not and how to find any solution. I cannot seem to be able to separate $x,y$ or write it in a good parametric form. Please help.","['real-analysis', 'resultant', 'polynomials', 'real-algebraic-geometry', 'algebra-precalculus']"
2988072,How to determine function is onto,"Consider the below function $f(x)=\frac{x}{2x+1}\{x \neq- \frac{1}{2}\}$ Onto functions are those $\forall y \exists x(f(x)=y)$ , means for all elements in co-domain we have a pre-image in the domain. I particularly get stuck how to determine when a function is onto especially when the function is given as a mathematical expression. Please guide.",['functions']
2988160,Intuition for $\lim\sup$ and $\lim\inf$,"After reading several alternative definitions of $\lim\sup$ and $\lim\inf$ , such as $\lim\sup$ being the supremum of the set of all subsequential limits, I'm still having trouble building the intuition for $\lim\sup$ . One thing that I feel is true, but not sure, is that $\lim\sup$ represents the greatest real number that infinitely many $a_n$ gets close to, and $\lim\inf$ represents the smallest value that infinitely many $a_n$ gets close to. Are these correct statements? If so, how would one go about showing it? 
Thanks",['real-analysis']
2988194,Why don't terms in the summation expression for an integral (almost) cancel out?,"If I think of an integral, $\int_a^b f(x) dx$ as roughly $\sum f(x)\delta x$ where $\delta x$ is very very small, then can't I write this sum as $$
f(x)x - f(x)(x-\delta x) + f(x-\delta x)(x-\delta x) +\dots -f(a)a
$$ Then, since $\delta x$ is so small, assuming $f(x)$ is ""smooth"" in some sense, shouldn't $f(x) \approx f(x-\delta x)$ so $$f(x)(x-\delta x) \approx f(x-\delta x)(x-\delta x)$$ and then all terms would cancel out except for $f(x)x$ and $f(a)a$ ? I.e. wouldn't this give $\int_a^b f(x)dx \approx f(x) x - f(a)a$ ? Why does this not give a decent approximation? For example, I think this is always exact for a constant function. I would then think it should be pretty good for any function that doesn't change too quickly at any point. but what is ""too quickly"" I thought it would also be pretty good for a linear function, but this is not the case...",['integration']
2988221,Convert $\mathrm{i}^\pi$ into trigonometric form,"How can I convert the complex number $\mathrm i^\pi$ to trigonometric form? I usually do these steps: take $ Z = a + b\mathrm i $ form, find $ r = \sqrt{a^2 + b^2} $ , $ \cos(\phi) = a / r, \sin(\phi) = b / r $ , find $ \phi $ from the above 2 equations. For $ \mathrm i^\pi $ I have $ a = 1, b = 1, r = 1 $ , $ \cos(\phi) = 1, \sin(\phi) = 1 $ .
There's no such $ \phi $ . The online Convert Complex Numbers to Polar Form gives the answer $ \phi = 77.2567 $ or just, $ \phi = \dfrac{180 \arg(\mathrm i^\pi)}{\pi}$",['trigonometry']
2988227,How to prove $\sum_{k=0}^{n-j}(-1)^k{n-j\choose k}{n+j+k\choose 2j+1+k}$ equal to zero?,How to prove $$\sum_{k=0}^{n-j}(-1)^k{n-j\choose k}{n+j+k\choose 2j+1+k}$$ equal to zero? Can you give me a direction to try?,"['summation', 'binomial-coefficients', 'combinatorics']"
2988321,A function vanishing at infinity which is not $L_1$,"I need an example of a continuous function $f:\mathbb{R}\rightarrow  \mathbb {R}$ vanishing at infinity, i.e. $$\lim_{x\rightarrow\infty}f (x) =\lim_{x\rightarrow-\infty}f (x)=0$$ but $f$ is not $L_1$ .","['integration', 'functional-analysis', 'real-analysis']"
2988339,"if 1, $\alpha_1$, $\alpha_2$, $\alpha_3$, $\ldots$, $\alpha_{n-1}$ are nth roots of unity then...","if 1, $\alpha_1$ , $\alpha_2$ , $\alpha_3$ , $\ldots$ , $\alpha_{n-1}$ are nth roots of unity then $$\frac{1}{1-\alpha_1} + \frac{1}{1-\alpha_2} + \frac{1}{1-\alpha_3}+\ldots+\frac{1}{1-\alpha_n} = ?$$ Now this has the solution too, but I do not understand the last step of the solution so here is the solution from the book. $1,\, \alpha_1, \, \alpha_2,\, \ldots, \, \alpha_n$ are the $n^\text{th}$ of unity. These are the roots of $x^n-1=0$ Let $y=\frac{1}{1-\alpha}$ where $\alpha = \alpha_1, \, \alpha_2, \, \alpha_3, \, \ldots,  \, \alpha_n$ $$1 - \alpha = \frac{1}{y} \Rightarrow \alpha = \frac{y-1}{y}.$$ But $\alpha$ is a root of $x^n-1=0 \therefore \alpha^n=1 \Rightarrow (y-1)^n = y^n$ $$\Rightarrow y^n - _nC_1y^{n-1} + _nC_2y^{n-2}-\ldots+(-1)^n = y^n \\
\Rightarrow  - _nC_1y^{n-1} + _nC_2y^{n-2}-\ldots+(-1)^n = 0.$$ Sum of roots $$\frac{1}{1-\alpha_1}+\frac{1}{1-\alpha_2}+\ldots+\frac{1}{1-\alpha_{n-1}} = \frac{_nC_2}{_nC_1} = \frac{n-1}{2}$$ So this last part from ""Sum of roots"" I do not understand. I cannot see how this last shape relates to this binomial theorem notation. Can anyone help?","['summation', 'roots', 'roots-of-unity', 'algebra-precalculus', 'complex-numbers']"
2988342,ّFind $x$ such that $ \frac{1}{x^2} + \frac{1}{(3-x)^2} = \frac{104}{25}$.,"ّFind $x$ such that $$ \frac{1}{x^2} + \frac{1}{(3-x)^2} = \frac{104}{25}\,.$$ My attempt: After clearing the denominators, I obtain this quartic equation $$104 x^{4} -624 x^{3} +886 x^{2} +150x-225=0.$$ I don't know how to proceed from here.","['algebra-precalculus', 'roots', 'polynomials', 'rational-functions']"
2988348,What am I doing wrong solving this system of equations?,"$$\begin{cases}
2x_1+5x_2-8x_3=8\\
4x_1+3x_2-9x_3=9\\
2x_1+3x_2-5x_3=7\\
x_1+8x_2-7x_3=12
\end{cases}$$ From my elementary row operations, I get that it has no solution. (Row operations are to be read from top to bottom.) $$\left[\begin{array}{ccc|c}
2 & 5 & -8 & 8  \\
4 & 3 & -9 & 9  \\
2 & 3 & -5 & 7  \\
1 & 8 & -7 & 12
\end{array}\right]
\overset{\overset{\large{R_1\to R_1-R_3}}{{R_2\to R_2-2R_3}}}{\overset{R_3\to R_3-2R_4}{\large\longrightarrow}}
\left[\begin{array}{ccc|c}
0 &  2  & -3 &  1  \\
0 & -3  &  1 & -5  \\
0 & -13 &  9 & -17 \\
1 &  8  & -7 &  12 
\end{array}\right]
\overset{\overset{\large{R_3\,\leftrightarrow\, R_4}}{R_2\,\leftrightarrow\, R_3}}{\overset{R_1\,\leftrightarrow\,R_2}{\large\longrightarrow}}
\left[\begin{array}{ccc|c}
1 &  8  & -7 &  12 \\
0 &  2  & -3 &  1  \\
0 & -3  &  1 & -5  \\
0 & -13 &  9 & -17
\end{array}\right]$$ $$\overset{R_4\to R_4-R_3}{\large\longrightarrow}
\left[\begin{array}{ccc|c}
1 &  8  & -7 &  12 \\
0 &  2  & -3 &  1  \\
0 & -3  &  1 & -5  \\
0 &  10 &  8 & -12
\end{array}\right]
\overset{\overset{\large{R_3\to R_3+R_2}}{R_4\to R_4-5R_2}}{\large\longrightarrow}
\left[\begin{array}{ccc|c}
1 &  8 & -7  &  12 \\
0 &  2 & -3  &  1  \\
0 & -1 & -2  & -4  \\
0 &  0 &  23 & -17
\end{array}\right]
\overset{\overset{\large{R_2\to R_2+2R_3}}{R_3\to-R_3}}{\large\longrightarrow}$$ $$\left[\begin{array}{ccc|c}
1 & 8 & -7  &  12 \\
0 & 0 & -7  & -7  \\
0 & 1 &  2  &  4  \\
0 & 0 &  23 & -17 \\
\end{array}\right]
\overset{R_2\,\leftrightarrow\,R_3}{\large\longrightarrow}
\left[\begin{array}{ccc|c}
1 & 8 & -7  &  12 \\
0 & 1 &  2  &  4  \\
0 & 0 & -7  & -7  \\
0 & 0 &  23 & -17 \\
\end{array}\right]$$ However, the answer in the book $(3, 2, 1)$ fits the system. Was there an arithmetical mistake, or do I misunderstand something fundamentally?","['systems-of-equations', 'linear-algebra']"
2988376,Solving a Markov Chain,"Let the distribution on variables $(X_t)$ for $t \in N$ satisfy a Markov chain. Each variable can take the values $\{1, 2\}$ . We are given the pmfs $$p(X_1=i) = 0.5$$ for $i=1,2$ and $$p(X_{t+1} = j\mid X_t = i) = p_{i,j}$$ where $p_{i,j}$ is the $(i, j)$ -th element of the matrix $$P=\begin{pmatrix}
0.3 & 0.7\\
0.6 & 0.4
\end{pmatrix}$$ Find: $P(X_3 = 2)$ and $p(X_2 = 1\mid X_3 = 2)$ . I'm stuck with how to start this problem. So any hints would be appreciated.","['markov-chains', 'probability-theory', 'probability']"
2988382,Gradient systems and equilibrium points,"I have been studying the following problem of gradient systems. Given a system: \begin{equation}
\overset{\cdot}{x}=f(x), \ \ x=x(t) \in \mathbb{R}^3,
\end{equation} where \begin{equation}
f=-\nabla g, \ \ g\in C^1(\mathbb{R}^3),
\end{equation} let's suppose that $x_0=0$ is an equilibrium point of this system. I have found references stating that when for example $x_0=0$ is also a local minimum of $g$ , then this means that $0$ is a stable point of equilibrium. This is shown by means of the Lyapunov function $V=g(x)-g(0)$ . I have found no reference whatsoever about maxima. If we had to study the same system, but $x_0=0$ is now a local (or global) maximum of $g$ , then is there a conclusion about the stability of $0$ ? I have tried defining the function $V=g(0)-g(x)$ , but for this function we have that $\overset{\cdot}{V} \geqslant 0$ and not $\overset{\cdot}{V} \leqslant  0$ . Is there even a way to define a Lyapunov function? Thanks in advance.","['stability-in-odes', 'ordinary-differential-equations', 'dynamical-systems']"
2988393,Can we always find a stochastic process whose autocorrelation function is a given positive semidefinite function?,"The auto-correlation function of a stochastic process is defined by $$R(t_1, t_2)=\mathbb{E}[X(t_1)X(t_2)]$$ We say that a function is positive semidefinite if and only if $$\forall x,y: f(x,y)^2 \leq f(x,x)f(y,y)$$ It is easy to see that if $X(t)$ is a stochastic process, then using the Cauchy-Schwartz inequality we have $$R(t_1, t_2)^2=\mathbb{E}[X(t_1)X(t_2)]^2 \leq \mathbb{E}[X(t_1)^2]\mathbb{E}[X(t_2)^2]=R(t_1, t_1)R(t_2, t_2)$$ So, I wonder if one is given a positive semidefinite function $f$ , is it always possible to find a stochastic process whose auto-correlation function is $f$ ? Note: I'm looking for answers that do not use measure theory.","['stochastic-processes', 'signal-processing', 'proof-verification', 'probability-theory']"
2988410,Algebraic manipulation with indices,"The question is: For a>0 and $\sqrt{a}+\frac{1}{\sqrt{a}}=3$ , find the value of $a\sqrt{a}+\frac{1}{a\sqrt{a}}$ So I first squared the given equation and got: $$a+\frac{1}{a}+2=9$$ $$a+\frac{1}{a}=7$$ Then to get the form of $a\sqrt{a}+\frac{1}{a\sqrt{a}}$ : $$(a+\frac{1}{a})(\sqrt{a}+\frac{1}{\sqrt{a}})=a\sqrt{a}+\frac{1}{a\sqrt{a}}+\frac{\sqrt{a}}{a}+\frac{a}{\sqrt{a}}$$ And I just got stuck right here because I didn't really know what to do with the $\frac{\sqrt{a}}{a}+\frac{a}{\sqrt{a}}$ . So I looked into the solutions and apparently it's $$(a+\frac{1}{a})(\sqrt{a}+\frac{1}{\sqrt{a}})=a\sqrt{a}+\frac{1}{a\sqrt{a}}+\sqrt{a}+\frac{1}{\sqrt{a}}$$ I'm not sure how those two are equal...",['algebra-precalculus']
2988430,Solution to differential equation system and solution to its conversion into 2nd order differential equation,"Following is the differential equation system with IVP $\vec{x}'=\small\begin{pmatrix}3&-9\\4&-3\end{pmatrix}\vec{x},   
\vec{x}(0)=\small\begin{pmatrix}2\\-4\end{pmatrix}$ The particular solution to this differential equation system is given below. $\vec{x}(t)=\frac23\small\begin{pmatrix}3\cos{(3\sqrt{3}t)}\\\cos{(3\sqrt{3}t)}+\sqrt{3}\sin{(3\sqrt{3}t)}\end{pmatrix}+\frac{14}{3\sqrt{3}}\small\begin{pmatrix}3\sin{(3\sqrt{3}t)}\\\sin{(3\sqrt{3}t)}-\sqrt{3}\cos{(3\sqrt{3}t)}\end{pmatrix}...(1)$ When i converted this differential equation system into 2nd order differential equation, I got $y""+13y'-7y=0, y(0)=2,y'(0)=-4$ Now, the particular solution to this 2nd order equation is $-\frac{2\sqrt{6}}{3}\sin{(\sqrt{6}t)}+2\cos{(\sqrt{6}t)}...(2)$ Now why there is a difference between these two solutions namely (1) and (2)","['self-learning', 'trigonometry', 'complex-numbers', 'ordinary-differential-equations']"
2988504,How do I prove that $\cos\left(2x\right)=1-2\sin^2\left(x\right)$?,"While trying to solve the equation $\sin\left(x\right)=\cos\left(2x\right)$ , a user on this forum suggested that I turn the equation into a quadratic form by converting $\cos(2x)$ using the identity $\cos\left(2x\right)=1-2\sin^2\left(x\right)$ . What is the logic behind this identity and how can I derive it?",['trigonometry']
2988510,$\text {dom}(R)$ and $\text {ran}(R)$ exists for any definition of order pair.,"Suppose that we define, for any sets $x,y$ , a set $(x,y)$ with the propertie that $$(x,y)=(x',y')\rightarrow x=x'\wedge y=y'(*)$$ Let $R$ be a set. I want to show that the classes $\{x:\exists y((x,y))\in R\}$ and $\{y:\exists x((x,y)\in R)\}$ exists. My attempt: Let $A:=\{z\in R: \exists v,w(z=(v,w))\}$ (this exists by separation axiom). Now take $\phi(z,u)\equiv\exists x,y(z=(x,y))\rightarrow u=x$ . It is easy to show that for each $z\in A$ exists an unique $u$ such that $\phi(z,u)$ . So, by replacement axiom exists a set $B$ such that $\forall z\in A\exists u\in B\phi(z,u)$ . So, by separation axiom exists the set $\{x\in B:\exists z\in A\phi(z,x)\}=\{x:\exists y((x,y))\in R\}$ (In the same way we proof the existence of the another set). My doubt is that I don't use $(*)$ or I don't see where I did. So, I don't know if my proof is right. Edit: The only axioms that I can use are: extensionality, separation, pairing, union and replacement.","['elementary-set-theory', 'proof-verification', 'axioms']"
2988516,Possible definitions of exponential function,"I was wondering how many definitions of exponential functions can we think of. The basic ones could be: $$e^x:=\sum_{k=0}^{\infty}\frac{x^k}{k!}$$ also $$e^x:=\lim_{n\to\infty}\bigg(1+\frac{x}{n}\bigg)^n$$ or this one:
Define $e^x:\mathbb{R}\rightarrow\mathbb{R}\\$ as unique function satisfying: \begin{align}
e^x\geq x+1\\
\forall x,y\in\mathbb{R}:e^{x+y}=e^xe^y
\end{align} Can anyone come up with something unusual? (Possibly with some explanation or references).","['definition', 'big-list', 'real-analysis']"
2988594,"Prove that for any sets $A$ and $B $ with $(A - B) ∪ (B - A) = A ∪ B$ , then $A ∩ B = ∅$","So this is an assignment question. I'm not sure how to do it so  I started off assuming that intersection of $A$ and $B$ is not empty so if $ p$ is an element in it, then $p$ is an element of $A \cup{ B } $ too. Is this right and if so where do I go from there? Thanks in advance!","['elementary-set-theory', 'proof-verification']"
2988624,What are the coefficients in the expansion of $(x+y)(x+2y) \cdots (x+ny)$?,"Are the numbers appearing as coefficients in the following sequence of polynomials known? Is there a known recurrence relation to compute them? \begin{align*}
  (x+y) &= x+y \\
  (x+y)(x+2y) &= x^2+3yx+2y^2 \\
  (x+y)(x+2y)(x+3y) &= x^3+6yx^2+11y^2x+6y^3 \\
  (x+y)(x+2y)(x+3y)(x+4y) &= x^4+10yx^3+35y^2x^2+50y^3x+24y^4 \\
 &\text{etc.}
\end{align*}","['algebra-precalculus', 'polynomials']"
2988636,How do I find a tangent plane without a specified point?,"I was having a problem finding the points on $z=3x^2 - 4y^2$ where vector $n=<3,2,2>$ is normal to the tangent plane. How do we calculate the tangent plane equation without a specific point to calculate it at? I also had an idea to take the cross product of $2$ vectors in the plane and somehow compare it to the $n$ vector but I don't know exactly how to do this. Thank you for any help!","['multivariable-calculus', 'orthonormal', 'vectors', 'tangent-spaces']"
2988706,"Inverting Fourier transform ""on circles""","Dear Math enthusiasts, I am struggeling with a problem for which a solution is already given to me, but I can just not see why it is true. Here is the setting: I am given a function $f(x,y,t)$ . It's well behaved let's say. Smooth and things like that. Now, this function should be reexpressed in the following form $$f(x,y,t) = \int g(k_x,k_y,w) {\rm e}^{-\jmath (k_x x + k_y y - w t)} d k_x dk_y dw. \tag{1}\label{eq1}$$ If it were only this it would be very simple, $g$ is some sort of 3-D Fourier transform of $f$ . However, the trouble is that the variables $k_x, k_y, w$ are not independent. They need to satisfy the relation $k_x^2 + k_y^2 = w^2$ . Therefore, I would claim that the correct form of the above expression should be $$f(x,y,t) = \int \oint_{S(w)} g(k_x,k_y,w) {\rm e}^{-\jmath (k_x x + k_y y - w t)} d \begin{bmatrix} k_x\\ k_y\end{bmatrix}  dw,\tag{2}\label{eq2}$$ where $S(w)$ is a circle of radius $w$ , so that the inner integral goes over the perimeter of the circle and the outer over circle radii. My question is essentially: given a target function $f(x,y,t)$ , how do I find $g(k_x, k_y, w)$ such that \eqref{eq2} is true for every point $x,y,t$ ? The reference I have for this simply redefines $g(k_y,k_y,w)$ into $h(k_x,w)$ since only two variables are independent (I'm assuming this means $h(k_x,w)=g(k_x,\pm \sqrt{w^2-k_x^2},w)$ though that's never written) and uses this in the first integral. This leads to $$f(x,y,t) = \int \int h(k_x,w) {\rm e}^{-\jmath (k_x x+k_y y - w t)} d  k_x  dw = \int \int \tilde{g}(k_x,y,w)  {\rm e}^{-\jmath (k_x x - w t)} d  k_x  dw,\tag{3}\label{eq3}$$ where $\tilde{g}(k_x,y,w) = h(k_x,w) {\rm e}^{-\jmath k_y y} $ (again, omitting the argument $k_y$ for me can only mean the implicit relation $k_y = \pm \sqrt{w^2-k_x^2}$ ). From \eqref{eq3}, they claim that $f$ is the 2-D Fourier transform of $\tilde{g}$ along the first and third dimension so that all we need to do to find $\tilde{g}$ is $$ \tilde{g}(k_x,y,w) = \frac{1}{4\pi^2} \int \int f(x,y,t) {\rm e}^{\jmath (k_x x-wt)} dx dt \tag{4}\label{eq4}$$ which gives $h$ as $ h(k_x,w) = \tilde{g}(k_x,y,w) {\rm e}^{\jmath k_y y} $ . However, I have the feeling this is oversimplifying things a bit. I'm lacking rigor. My feeling is that the original problem \eqref{eq2} may not have a unique solution (due to the variable dependence) and a particular one was chosen here. Integration limits are always skipped which may be a delicate issue (after all, $k_x$ should never leave the interval $[-w,w]$ , maybe this can be solved by defining $h$ zero outside this support). The fact that we cannot directly solve for $k_y$ (due to the $\pm$ ) troubles me. Overall I have a vague feeling that this may work but I cannot quite put my finger on it and really understand what's going on. Would anyone be able to enlighten me how to treat such problems rigorously? edit : A concrete example I am interested in is the function $f(x,y,t)={\rm e}^{-\jmath \left(\omega_0 t - \frac{\omega_0}{c}\sqrt{(x-x_0)^2+(y-y_0)^2}\right)}$ . I'm awarding a bounty to anyone who can systematically explain me how to find the (set of) function(s) $g(k_x,k_y,\omega)$ that satisfy \eqref{eq2} for a given $f(x,y,t)$ everywhere. A concrete example may be helpful for the understanding, it can be the one I provided in this paragraph, but I'm also happy with any other non-trivial example, as long as it aids the understanding.","['integration', 'fourier-analysis', 'functional-analysis', 'integral-geometry']"
2988719,"Proving continuity of a multivariable function by ""reducing"" it to a single variable function","My goal is to examine the continuity of a certain multivariable function. In that regard, (mainly because my math in the specific area is dusty) I find it somewhat tricky to either prove or disprove that property in the N -diensional space. However, I have come up with the following idea: let $f : \mathbb{R}^2 \rightarrow \mathbb{R} $ be the function that I want to examine at some point $(x_0, y_0)$ . Now I am using the following two functions $g: \mathbb{R}^2 \rightarrow \mathbb{R} $ and $h: \mathbb{R} \rightarrow \mathbb{R} $ so that I can write function $f$ as: $$ f(x,y) = h(g(x,y)),\quad \forall (x,y) \in \mathbb{R}^2
$$ I also know that function $g$ is indeed continuous in $\mathbb{R}^2$ . My assumption is that I can examine the continuity of $h$ at $x' = g(x_0,y_0)$ in order to decide the continuity of $f$ at $(x_0,y_0)$ - a much easier task. In mathimatical notation, I guess what I mean is: $$
\lim_{(x,y)->(x_0,y_0)} f(x,y) = f(x_0, y_0) \quad \iff \\ \lim_{x->g(x_0,y_0)}h(x) = h(g(x_0,y_0))
$$ Actual Questions: 1. Is my assumption correct? 2. If not, maybe do either of $\Rightarrow$ or $\Leftarrow$ hold? (Because if one of those holds then I can use the above to only prove or only disprove accoridnlgy.) 3. Does the same hold for proving/disproving differentiability as well? 4. Under what conditions do any of those hold for a (finite) sum of $h,g$ functions, i.e. $f(x,y) = h_1(g_1(x,y)) + h_2(g_2(x,y)) + ... + h_k(g_k(x,y))$ Note: I can actually post my original function and the transformation if the question is too broad or the answers depend greatly on the actual $f,g$ and $h$ definitions. However I believe a more general question may help more people find it and also make it easier to be understood.","['continuity', 'multivariable-calculus', 'derivatives']"
2988750,Show a coordinate ring is not Artinian,"Show that $\mathbb{R}[x,y]/(x^2+y^2-1)$ is not Artinian. I am told to use my geometric intuition to prove it doesn't satisfy the d.c.c., but my intuition isn't very strong when it comes to coordinate rings. I know this ring is what we get by restricting real-valued polynomial functions to the unit circle and that polynomials equal on the circle are equal in the ring. But how do I use this to construct a chain of ideals that is strictly decreasing?","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
2988766,"What kind of ""geometric"" regularity $f'^2$ gives on $f$","When solving real-analysis' problems I like to represent the functions involved and think geometrically what is going on. Today I got the following exercise : Let $f \in \mathcal{C}^1(\mathbb{R},\mathbb{R})$ , such that : $$\int_\mathbb{R} \mid f \mid \in \mathbb{R}$$ $$\int_\mathbb{R}  f'^2  \in \mathbb{R}$$ Proove that $f$ is Hölder continuous. When I see the assumption involved it's not hard to see that there must be some Cauchy-Schwartz inequality somewhere (because the assumption of absolutely integrable is on the square of the derivative and not only on the derivative itself). So, with this in mind we easily get the following proof : We have (using CS) : $$ \mid f(x) -f(y) \mid \leq \int_x^y 1 \times f' \leq \sqrt{\int_x^y f'^2}\sqrt{y-x} $$ Hence it follows that $f$ is $\frac{1}{2}-$ Hölder continous since that $\sqrt{\int_x^y f'^2}$ is bounded. As you noticed it's not hard to come up with the proof since the assumption of the problem on the absolute integrability of the square of the derivative of $f$ directly leads to thinking we must use the Cauchy-Schwartz inequality. The problem is that I don't like this way of thinking. That's why I am seeking a geometric intuition of the problem. For example, if we only have the assumption of absolute integrability on $f'$ instead of $f'^2$ does the result still holds? Why the fact that: $\int_\mathbb{R}  f'^2  \in \mathbb{R}$ imply so much regularity for $f$ ? $\ldots$","['integration', 'uniform-continuity', 'real-analysis', 'calculus', 'cauchy-schwarz-inequality']"
2988770,Interchanging summation involving divisors in index,"I was reading Apostle's Analytic Number Theory book and I saw this formula being used in many cases. Why is this true? $$ \sum_{n=1}^{\infty} \sum_{d|n} f(d,n) = \sum_{d=1}^{\infty} \sum_{n=1}^{\infty} f(d,nd) $$ I don't see the intuition behind it. Also, will this hold for finite sums, i.e, $$ \sum_{n=1}^{m} \sum_{d|n} f(d,n) =^{?} \sum_{d=1}^{m} \sum_{n=1}^{m} f(d,nd) $$","['analytic-number-theory', 'summation', 'divisor-counting-function', 'sequences-and-series']"
2988786,Norm inequality $\| x - y \| \cdot \| z \| \leq \| x - z \| \cdot \| y \| + \| z - y \| \cdot \| x \|$,"Let $x$ , $y$ , $z$ be $3$ vectors in Euclidean space $V$ . $\| x \|$ is norm of $x$ (length) How do you prove: $$\| x - y \| \cdot \|z\| \leq \| x - z\| \cdot \| y \| + \| z - y\| \cdot \| x \|?$$ I have tried Cauchy-Schwartz inequality, | < x,y > | <= ||x|| •||y||, where  is dot product between x and y, trying to move everything on one side and then proving that it is greater than zero. Also I used modules properties, |a + b| < = |a| + |b| and so on. Also triangle inequality, ||x + y || <= ||x|| + ||y|| , trying to get the other side of inequality. Considering that ||x|| is sqrt( < x,x > ) and < x , a+b > is < x ,a > + < x ,b > and that, norm is always greater than 0 and sqrt( a+b )<= sqrt(a) + sqrt(b) , if a and b are positive, I played with this properties without getting any result.. Also distance formula, d(a,b) <= d(a,c) + d(c,a), where d(a,b) = || a - b|| There is this formula: Ilz - xII^2 + Ilz - yII^2 - Ilx - yII^2 =2 Maybe it helps. I have tried to use it but without a result..","['euclidean-geometry', 'normed-spaces', 'linear-algebra', 'geometry']"
2988818,"Rank, dimension, basis","I think I am a little bit confused with the terms in the title, so I hope you can correct me if I got it wrong... $$
\left\lbrace \begin{bmatrix} x_{1}\\0\\0 \end{bmatrix} : x_{1} \in \mathbb{R} \right\rbrace
$$ is a vector space. So far so good. The dimension of the vector space is the number of vectors in the basis. In class, I wrote down that a basis is $$
B=(\begin{bmatrix}{1}\\{0}\\{0}\end{bmatrix}, \begin{bmatrix}{0}\\{0}\\{0}\end{bmatrix}, \begin{bmatrix}{0}\\{0}\\{0}\end{bmatrix})
$$ But this does not seem logical to me now, as it should be the smallest number of independent vectors that span the vector space. And shouldn’t that be just $
(\begin{bmatrix}{1}\\{0}\\{0}\end{bmatrix})
$ So this is the place where I'm not sure: what is the dimension of the vector space? It is a line in ${\mathbb R}^3$ , and it would seem logical that the dimension of a line is $1$ . And what would the rank be? I totally confused myself. Or is it like the dimension is $3$ and the rank is $1$ . so the above solution for the basis would be correct... Many thanks for your help",['linear-algebra']
2988824,How to use derivatives to prove that $f(x)=2\cos^2\left(\frac{\pi }{4}-\frac{x}{2}\right)-\sin \left(x\right)=1$?,"I'm supposed to use the derivative to prove that $$f(x)=2\cos ^2\left(\frac{\pi }{4}-\frac{x}{2}\right)-\sin \left(x\right)=1$$ What I have so far is: $$f'\left(x\right)=D\left(2\left(\cos\left(\frac{\pi}{4}-\frac{x}{2}\right)\right)^2\right)-D\left(\sin\left(x\right)\right)=1$$ $$f\:'\left(x\right)=4\cos\left(\frac{\pi}{4}-\frac{x}{2}\right)\left(-\frac{1}{2}\sin\left(\frac{\pi}{4}-\frac{x}{2}\right)\right)-\cos\left(x\right)=1$$ $$f\:'\left(x\right)=-2\sin\left(\frac{\pi}{4}-\frac{x}{2}\right)\cos\left(\frac{\pi}{4}-\frac{x}{2}\right)-\cos\left(x\right)=1$$ And by continuing this I can get all the way to zero, which would mean that $0=1$ and from there by taking the derivative of $1$ we'd get $0=0$ , which is obviously true. But what I'm confused about is that even if the constant was any number, let's say two, the equation would still come out as true. And obviously, the original function doesn't equal two. I'm assuming that I'm somehow supposed to get $1$ to the left side as well? Honestly, I've gone through the formulas so many times, and yet I can't figure out how to do it.","['calculus', 'derivatives']"
2988910,Graphical multiplication tables for $\mathbb{Z}/p\mathbb{Z}$ and $\mathbb{Z}$,"Inspired by Burkard Polster's beautiful video on Times Tables, Mandelbrot and the Heart of Mathematics I wondered how this graphical approach to visualize the multiplicative structure of finite rings could be extended to infinite rings, especially to $\mathbb{Z}$ . Here are a few examples of graphical times (= multiplication) tables for $p= 11,29,97$ : Enlarged for the first numbers for $p=97$ : Note that one immediately sees that $\mathbb{Z}/p\mathbb{Z}$ for $p$ prime has no zero divisors: no lines end at $0$ , the point at the bottom. My simple idea was: Just project $\mathbb{Z}$ on the unit circle and just draw the ""times tables"": Note that and how and why the diversity of forms for finite rings gets lost! Note further that the $\times +n$ and $\times -n$ tables tend to look the same for $n \rightarrow \infty$ : I wonder: Has this graphical approach been taken already? What might its didactical/pedagogical/educational values be? Mathematically: How do the graphs for finite rings – among each other and with the graph for the infinite ring $\mathbb{Z}$ – relate? (In which terms would one define such relations, how would one analyse them?)","['ring-theory', 'visualization', 'abstract-algebra', 'education']"
2988915,Calculating $\lim_{n\to \infty}{\left(\frac{n}{n+3}\right)^\sqrt{n(n+1)}}$,"I want to calculate $$\lim_{n\to \infty}{\left(\frac{n}{n+3}\right)^\sqrt{n(n+1)}}$$ Tried by using $$\lim_{n\to \infty} {a^b} = \left(\lim_{n\to \infty}a\right) ^{(\lim_{n\to \infty}b)} $$ but got the notorious $$\lim_{n\to \infty}{1^\infty}$$ Then I tried with the identity $x=e^{\ln{x}}$ : $$\lim_{n\to \infty}{\left(\frac{n}{n+3}\right)^\sqrt{n(n+1)}} = \lim_{n\to \infty}{e^{\sqrt{n(n+1)}\cdot\ln\left(\frac{n}{n+3}\right)}}$$ Now what? My guess is to continue with inequality: $$>\lim_{n\to \infty}{e^{n\cdot\ln\left(\frac{n}{n+3}\right)}} = 
\lim_{n\to \infty}{e^{n\cdot\ln{n}-n\cdot \ln(n+3)}}$$ And now I'm really stuck... I'm I on the correct direction? Any tip is welcomed...","['limits', 'calculus']"
2988940,Is the palindrome-prime factor of $p-1$ always larger than that of $p+1$?,"Suppose, $p\ge 7$ is a palindrome-prime , the largest prime factor of $p-1$ is a  palindrome-prime and the largest prime factor of $p+1$ is also a palindrome-prime. Must the prime factor of $p-1$ be larger than the prime factor of $p+1$ ? The first few solutions are : p   factor(p - 1)  factor(p + 1)
          7              3               2
         11              5               3
        383            191               3
      38783          19391             101
12211811221          30703             151
18345254381      917262719             101 Beginning with $383$ , the prime factor of $p-1$ is even vastly larger than that of $p+1$ , but this could be a case of the ""law of small numbers"".","['prime-factorization', 'number-theory', 'elementary-number-theory', 'palindrome', 'prime-numbers']"
2988954,A question on Hartshorne Chapter III Proposition 2.6 [duplicate],"This question already has an answer here : Hartshorne's weird definition of right derived functors and prop. III 2.6 (1 answer) Closed 5 years ago . When I read Hartshorne, I saw the Proposition 2.6 in Chapter III as follows: Let $(X,\mathcal{O}_X)$ be a ringed space. Then the derived functors of the functor $\Gamma(X,-)$ from the category of $\mathcal{O}_X$ -modules to the category of abelian groups coincide with the cohomology functors $H^{i}(X,-)$ . I am pretty confused here because I think it is just the definition of cohomology functor... Why should we bother to use the result about the flasque sheaf and acyclic resolution to prove this? So I must miss something important and obvious. Please point it out. Thanks!","['homological-algebra', 'algebraic-geometry', 'sheaf-cohomology']"
2988956,Spinor representation for $\operatorname{Spin}(V \oplus V^*)$,"I'm studding Hitchin's Generalized Calabi-Yau Manifolds https://arxiv.org/abs/math/0209099 and I've stuck here: Suppose that $V$ is a vector space and denote its dual by $V^*$ . Now we know that the $\bigwedge^\bullet(V^*)$ that is the exterior algebra over the dual space is a representation for Clifford algebra $CL(V \oplus V^*)$ by the action $$(v,\xi).\varphi=i_v\varphi+\xi\wedge\varphi ,  (v,\xi)\in CL(V \oplus V^*)$$ we are mainly interested in those representations of Spin group $\operatorname{Spin}(V \oplus V^*)$ that is not a representation of $SO(V \oplus V^*)$ and we call them Spinor representation. I need  to find out if the restriction of this representation to the subgroup $\operatorname{Spin}(V \oplus V^*)$ of $CL(V \oplus V^*)$ is one of these representations. 
I see that if we take $$\rho:CL(V \oplus V^*)\rightarrow \operatorname{End}(\bigwedge^\bullet(V^*))$$ by restricting the representation we have $$\rho:\operatorname{Spin}(V \oplus V^*)\rightarrow GL(\bigwedge^\bullet(V^*))$$ and we have $$\rho(-1)=-\operatorname{id}$$ so why it cant be a representation of $SO(V \oplus V^*)$ ? I know that $\operatorname{Spin}(V \oplus V^*)$ is a double cover of $SO(V \oplus V^*)$ but can't see how its relevant. 
That would be perfect if after figuring this out I get to understand how tensoring $\bigwedge^\bullet(V^*)$ in the space of top forms of $V$ $$\bigwedge^\bullet(V^*)\otimes (\bigwedge^n V)^\frac{1}{2}$$ will contruct another Spinor representation and in what aspects this will arise more useful constructions than $\bigwedge^\bullet(V^*)$ so Hitchin prefered this one. Any help would be a lot appreciated.","['spin-geometry', 'algebraic-geometry', 'representation-theory', 'clifford-algebras']"
2988975,"Integration/measure theory ""paradox""?","I have encountered the following ""paradox."" Consider a dense countable subset of $\mathbb{R}$ , e.g. $\mathbb{Q}$ . Because the set is countable we may parametrise it by $\mathbb{Q} = \{ a_n \}_{n=1}^\infty$ . Then consider the function (for some $\epsilon >0$ ) $$\sum_{n=1}^\infty \chi_{[a_n, a_n + \epsilon/2^n)} $$ where $\chi$ is the indicator function. Because the set $\mathbb{Q}$ is dense, this function converges to infinity everywhere. But its integral according to Lebesgue measure is $$\int_{\mathbb{R}} \sum_{n=1}^\infty \chi_{[a_n, a_n + \epsilon/2^n)} d\mu = \sum_{n=1}^\infty \int_{\mathbb{R}}  \chi_{[a_n, a_n + \epsilon/2^n)} d\mu = \sum_{n=1}^\infty \frac{\epsilon}{2^n} = \epsilon$$ where we have commuted summation and integral using B. Levi's theorem on monotone convergence. Where is my mistake? EDIT: Soon after posting this I realised that my intuition the function converges to infinity everywhere is wrong.","['measure-theory', 'lebesgue-integral', 'real-analysis']"
2988987,Exact notation of the domain of a function,"For example, we have $f(x)=\frac{1}{x^2-1}$ Would the domain be $$\mathcal D(f)=\{x\in\mathbb{R}\mid x\neq(1,-1)\}$$ or rather $$\mathcal D(f)=\{x\in\mathbb{R}\mid x\neq \{1,-1\}\}$$ or $$ D(f)=\{x\in\mathbb{R}\mid x \setminus \{1,-1\}\}$$ or are all notations correct?","['notation', 'functions', 'real-analysis']"
2989004,"Show that $f(x,y)$=$\sqrt[3]{\lvert x^2-(y+1)^2 \lvert}\sin(\lvert x+y+1 \lvert))$ is differentiable at $(0,-1)$, $(1,0)$ and $(-1,0)$","I want to show that this function is differentiable in these points. At $(0,-1)$ i use the definition to check the differentiability: At $(0,-1)$ i have that $f(0,-1)=0$ , $f_y(0,-1)=0$ and $f_x(0,-1)=0$ . To prove differentiability i need to check if the limit $$\lim_{(x,y)\to (0,-1)}\frac{f(x,y)-f(0,-1)-f_x(0,-1)(x-0)-f_y(0,-1)(y+1)}{\sqrt{(x-0)^2+(y+1)^2}}$$ equals to zero. This limit becomes $$\lim_{(x,y)\to (0,-1)}\frac{\sqrt[3]{\lvert x^2-(y+1)^2 \lvert}\sin(\lvert x+y+1 \lvert))}{\sqrt{x^2+(y+1)^2}}$$ I am not sure how to proceed. Thanks for the help in advance!!","['real-analysis', 'multivariable-calculus', 'calculus', 'limits', 'derivatives']"
2989040,partial derivative of multivariable function with respect to another function,"Say I have a function $f(x,y,z)$ . If I know $t= \sqrt{x+\sqrt{x^2+ y*z}}$ and I know the partials $\large\frac{\partial{f}}{\partial{x}}$ , $\large\frac{\partial{f}}{\partial{y}}$ , $\large\frac{\partial{f}}{\partial{z}}$ , how could I apply the chain rule in order to obtain $\large\frac{\partial{f}}{\partial{t}}$ ? I would have thought: $$\frac{\partial{f}}{\partial{t}} = \frac{\partial{f}}{\partial{x}}\frac{\partial{x}}{\partial{t}} + \frac{\partial{f}}{\partial{y}}\frac{\partial{y}}{\partial{t}} +\frac{\partial{f}}{\partial{z}}\frac{\partial{z}}{\partial{t}}$$ however, I already feel like i'm on the wrong track.  Can anyone give me a start on how to construct $\large\frac{\partial{f}}{\partial{t}}$ in terms of $\large\frac{\partial{f}}{\partial{x}}$ , $\large\frac{\partial{f}}{\partial{y}}$ and $\large\frac{\partial{f}}{\partial{z}}$ ?","['partial-derivative', 'multivariable-calculus', 'calculus']"
2989042,Clarification on a proof of Roth's theorem,"Roth's theorem is stated in the book by Einsiedler and Ward, theorem 7.14 page 191 as: Let $(X,\mathscr{B},\mu,T)$ be a measure-preserving probability system. Then, for any functions $f_1,f_2 \in L^{\infty}(X,\mathscr{B},\mu)$ , $$\frac{1}{N}\sum\limits_{n=1}^{N}U_T^{n}f_1U_T^{2n}f_2$$ converges in $L^2(X,\mathscr{B},\mu)$ (Here $U_T g:=g\circ T$ ).  Moreover, for any $A\in \mathscr{B}$ with $\mu(A)>0$ we have $$\lim_{N\to \infty}\frac{1}{N}\sum\limits_{n=1}^{N} \mu(A\cap T^{-n}A \cap T^{-2n}A)>0.$$ Question: Can I deduce this theorem from the special case where $(X,\mathscr{B},\mu,T)$ is taken to be an invertible, ergodic, Borel probability system? The reason I'm asking is that Einsiedler-Ward only seem to prove this for the special case. I'm not sure if i'm misreading their proof or if the reduction to the general case is easy. My issue is primarily with the $L^2$ convergence claim. A first attempt at the reduction could be to apply the ergodic decomposition theorem. However this isn't a valid approach since our space isn't assumed to be a Borel space.","['measure-theory', 'additive-combinatorics', 'ergodic-theory', 'functional-analysis', 'probability-theory']"
2989055,Is $\mathbb{R}^4$ minus a line simply connected?,"Is the set $\mathbb{R}^4\setminus \{(0,0,0,w)  | w\in \mathbb{R} \}$ simply connected? I started trying to grasp the notion of simple connectedness in higher dimensions and realized I could not even a figure out such a basic question. My intuition says it is not simply connected but maybe you can twist things around in 4 dimensions in ways I can not visualize.","['general-topology', 'connectedness']"
2989099,Showing that a function between $\Bbb R[x]/(x^2 - 1)$ and $ \Bbb R\times \Bbb R $ is surjective?,"I am fairly new to more abstract mathematical structures and frankly I'm quite new to detailed proofs in general. For context, this is not a homework problem, I am just trying to better understand how to convert conceptual understandings of things into parts of proofs. I would like to show that the function $$f: \Bbb R[x]/(x^2 - 1) \rightarrow \Bbb R \times \Bbb R $$ $$f(a + bx + (x^2 - 1)) := (a + b, a - b) $$ is surjective. I understand that this requires showing that for every $q \in \Bbb R \times \Bbb R$ there must exist some $p\in \Bbb R[x]/(x^2 - 1)  $ such that $f(p)=q$ . It seems clear to me that this is the case, because two elements of $\Bbb R$ are being mapped to two elements of $\Bbb R$ , but for one thing I don't know if this is always the case, and more importantly that's not even a remotely rigorous explanation. Assuming I could show that $f$ is a ring homomorphism, how exactly could I show surjectivity? And is there some direction to take for showing this in general or is the route to showing surjectivity dependent on the problem? For instance, I find showing injectivity very straightforward because it only requires one to consider what would cause $a=b$ in the case that $f(a) = f(b)$ , which is usually even more straightforward if $f$ is a homomorphism.","['functions', 'ring-theory', 'abstract-algebra']"
2989103,What is the limit of $ \lim ((x-1)/(x+1))^x$ when $x$ approaches infinity (without l'Hôpital's rule)?,"this is my first post.
I've been struggling whit this limit for too long (without using l'Hôpital's rule): $$\lim_{x\to {\infty}} \left(\frac{x-1}{x+1}\right)^x$$ My answer is $\frac1e$ , but the correct answer should be $\frac{1}{e^2}$ .
Could anyone help me understand why? Thanks in advance!","['limits', 'calculus', 'functions', 'limits-without-lhopital']"
2989108,If the area under graph of $f$ is measurable then $f$ is measurable,"Let $(X,\mathcal{M},\mu)$ be a $\sigma$ -finite measure space and $f \colon X \to [0,\infty]$ such that \begin{equation}
\{ (x,y) \in  X \times \mathbb{R} \colon 0<y<f(x) \}
\end{equation} is measurable respect to $\mu \otimes \lambda$ where $\lambda$ is the Lebesgue measure on $\mathbb{R}$ .  How can I prove that $f$ is measurable?",['measure-theory']
2989189,"What's an example of a finite, non-abelian, non-simple group that is *not* semidirectly reducible? [duplicate]","This question already has answers here : Can every non-simple group $G$ be written as a semidirect product? (2 answers) Closed 5 years ago . Say I want to classify all groups of a given order. The abelian case is completely understood by the structure theorem for finitely generated abelian groups. Assume our group is non-abelian, and we somehow managed to find a normal subgroup $N$ (e.g., by considering the core of some Sylow subgroup). It would be pretty nice if we could go on to construct all semidirect products of $N$ with groups of the remaining order – but $G$ need not be semidirectly reducible*. The only counterexample that comes to mind is $0 \to C_2 \to C_4 \to C_2 \to 0$ , which does not admit an appropriate section, i.e. a subgroup complementing our normal subgroup with trivial intersection. What are counterexamples in the non-abelian case?","['finite-groups', 'semidirect-product', 'normal-subgroups', 'abstract-algebra', 'group-theory']"
2989200,Automorphism group of $\mathbb Z_2^3$,"I am trying to find $\text{Aut}(\mathbb Z_2^3)$ and express it in terms of familiar groups and the direct and/or semi direct product. Here's what I have so far: I know that the set of generators $A:=\{(1,0,0),(0,1,0),(0,0,1)\}$ of $\mathbb Z_2^3$ must be mapped onto another set of $3$ generators by an arbitrary automorphism $\varphi$ . I have calculated combinatorially that $\mathbb Z_2^n$ has $2^n-1$ sets of $n$ generators, and since each automorphism maps $A$ onto one of these $n$ -element sets in one of $n!$ ways, there are a total of $n!(2^n-1)$ automorphisms of $\mathbb Z_2^n$ . In summary, $|\text{Aut}(\mathbb Z_2^n)|=n!(2^n-1)$ . So for $n=3$ , we can see that $|\text{Aut}(\mathbb Z_2^3)|=6\cdot 7=42$ . Is this reasoning all correct? If so, how do I proceed from here to actually find out what the group $\text{Aut}(\mathbb Z_2^3)$ is?","['automorphism-group', 'group-presentation', 'group-theory']"
2989211,Prove that $e^x \cos (\sqrt{x^2+1}) \leq 1$,"I would like to prove that : $$\forall x \in [0,1], e^x\cos(\sqrt{x^2+1}) \leq 1$$ When plotting the graph this inequality is not sharp at all and we even have : $$\forall x \in [0,1], e^x \cos(\sqrt{x^2+1}) \leq 0.8$$ I tried several things such has : Calculating the derivative and try to apply the mean value theorem to get an upper bound, but the derivative is hard to manipulate and it doesn’t seem I am getting something. Moreover trying something on convexity but once again this is difficult due to the horrible looking of the derivative. I am very interested in sharper upper bound, even if I can’t manage to prove the inequality for $1$ ...","['calculus', 'inequality', 'real-analysis']"
2989242,Verifying an increasing function on a closed interval is Riemann Integrable,"So I was having some difficulty coming up with a conceptual reason for why an increasing function on a closed interval would be Riemann Integrable, when without effort a proof seemingly fell out of some computations. Could anyone verify if it is correct? Thank you very much. Here's what I have. $\textbf{Proof:}$ Suppose that $f:[a,b]\to\mathbb{R}$ is increasing. WLOG we can assume $f(b)>f(a)$ , for else $f$ is constant, and trivially integrable. Let $\epsilon>0$ . Choose a partition $P=\{x_i\}_0^n$ of $[a,b]$ such that for all $1\leq i,j\leq n$ it follows that $x_i-x_{i-1}=x_j-x_{j-1}<\epsilon/(f(b)-f(a))$ . We compute $$U(f,P)-L(f,P)=\sum_{i=1}^n\bigg(\sup_{[x_{i-1},x_i]}f(x)-\inf_{[x_{i-1},x_i]}f(x)\bigg)(x_i-x_{i-1})$$ $$=\sum_{i=1}^n\bigg(\sup_{[x_{i-1},x_i]}f(x)-\inf_{[x_{i-1},x_i]}f(x)\bigg)(x_1-x_{0})$$ $$=\sum_{i=1}^n(f(x_i)-f(x_{i-1}))(x_1-x_{0})=(f(b)-f(a))(x_1-x_0)$$ $$<(f(b)-f(a))(\epsilon/(f(b)-f(a)))=\epsilon.$$ This completes the proof. $\square$","['integration', 'proof-verification', 'monotone-functions', 'real-analysis', 'riemann-integration']"
2989261,Proof one aspect of Nim game,"Prove that if the Nim sum is not zero, then one of the piles is bigger than the Nim sum of the all the other piles. I've already proven that if the Nim sum of the piles is zero, then any one move will leave a nonzero Nim sum, and if there is a pile with more stones than the Nim sum of all the other piles, then there is a move that makes the Nim sum equal to zero. However, I have problem coming up with the proof for this one and would appreciate some insight into it.","['discrete-mathematics', 'combinatorial-game-theory']"
2989305,Find general solution of the differential equation (linear algebra),How to find the general solution of a differential equation? $$8y''' + 8y' = 0$$ I literally know nothing about solving differential equations. Honestly I have never been taught about differential equations and I don't know why my professor threw this question into the homework assignmnent.,"['linear-algebra', 'ordinary-differential-equations']"
2989352,Solutions of $n^{n}=2n$?,"What is the best way to find solutions to $n^{n}$ and $2n$ ? Someone suggested the Lambert W function, but I cannot find a way to set it up in a way to use that. Any other suggestions? Meaning, what are the solutions for $n$ in $$n^n=2n$$","['alternative-proof', 'algebra-precalculus']"
2989359,"Which sets are ""clompact""?","This is an exercise taken verbatim from Abbott's Understanding Analysis : Let’s call a set clompact if it has the property that every
  closed cover (i.e., a cover consisting of closed sets) admits a finite subcover.
  Describe all of the clompact subsets of $\mathbf R$ . I am unable to fully resolve the problem. So far, I have been able to see that singleton sets are always clompact, because the single element must be in at least one set belonging to the closed cover, and that one set is a sufficient finite subcover. The null set is also clompact for obvious reasons. I know that every non-singleton interval (regardless of if they are open, closed, or half-open) is not clompact. As an example, the closed cover $$ \{0\}\cup\bigcup_1^\infty\left[\frac1{n+1},\frac1n\right] $$ for $[0,1]$ does not have a finite subcover. Similar constructions of closed covers show that $[a,b]$ , $(a,b]$ , $[b,a)$ and $(a,b)$ are not clompact as well. In addition, $\mathbf R$ itself and any unbounded interval is also not clompact. Is anyone able to help in solving this problem? Any assistance is appreciated.","['analysis', 'real-analysis']"
2989381,"Intuition: If the columns of a matrix are colinear, then its rows are also colinear.","For simplicity's sake, I'm working with a 3x3 square matrix in which none of the column or row vectors is the zero vector. I tried graphing the columns of the matrix {{1,4,-3},{2,7,-5},{3,6,-3}} (where each group of $3$ is a row) and those $3$ vectors are on the same plane. I understand why that's the case; what I don't understand is why it necessarily implies that the $3$ row vectors are also on a single plane. I have some algebraic intuition for the 2d case. For example with vectors v = <1,2> and w = <3,6>, the fact that w = 3v means that the respective components of the two vectors are in the same proportion, and therefore the vectors x = <1,3> and y = <2,6> are also colinear. I tried to extend that kind of logic to the 3d case but I didn't manage to because of the increased complexity. For ex in the 3x3 matrix that I used, if column vectors are u, v and w, then u = v + w. Since one vector isn't simply a scalar multiple of another as in the 2d case, I can't seem to apply the same logic of proportions kept. I saw this question: For a square matrix, row vectors are linearly independent if and only if columns are. , but it doesn't give the kind of intuition I'm looking for.","['matrices', 'linear-algebra', 'vectors']"
2989388,"Let ${x_n}=2^{n}a_n$, and $a_{n+1}=\sqrt{\frac{1-\sqrt{1-a_n^2}}{2}}, a_0=1$, how to prove ${x_n}$ converges?","How to show $x_n=2^na_n$ converges, where $a_{n+1}=\sqrt{\frac{1-\sqrt{1-a_n^2}}{2}}$ The question originated from Professor David McKinnon. Attempt:
I did prove it is increasing but failed to show it is bounded above. The sequence should converge to $\frac{\pi}{2}$ , if I'm not mistaking. In addition, ${a_n}$ is a decreasing sequence, and it is bounded below by 0, so I tried to use the fact that ${a_n}$ is a Cauchy to prove that ${x_n}$ is also a Cauchy.","['recurrence-relations', 'real-analysis', 'calculus', 'sequences-and-series', 'trigonometry']"
2989398,Second derivative of polar coordinates,"How do I express $\dfrac{\partial^2z}{\partial\theta^2}$ in terms of Cartesian coordinates given that $(x,y)$ are Cartesian coordinates and $(r,\theta)$ are polar coordinates. Attempt: $$
\frac{\partial^2z}{\partial\theta^2} = 
\frac{\partial }{\partial \theta}\left[\frac{\partial z }{\partial x}\frac{\partial x}{\partial \theta}+\frac{\partial z}{\partial y}\frac{\partial y}{\partial \theta}\right]
$$ I'm not entirely sure if I am on the right track because z isn't specified so simplifying that expression down isn't possible.","['partial-derivative', 'multivariable-calculus', 'calculus']"
2989414,Set theoretic issues in the definition of a site in Stacks Project,"I've been learning about sites from the Stacks Project, which is generally very precise in its terminology, but I've found some of their conventions very confusing in this part. Their definition of a site is given here . Below the definition, they immediately make a remark about set theoretic issues (Remark 6.3 in the link). In particular, they address the issue of big categories, and when such categories give rise to a proper class of covering families. They fix the convention that they will allow big categories, and then use the result here to choose a set of covering families that gives the same category of sheaves as the proper class of covering families. From the outset, this seems ok. But reading the statements of these theorems, the entire idea seems redundant. For example, in the second link, they define a site to be a small category. Indeed the second link above begins: ""Suppose that $\mathcal{C}$ is a category (as in Categories, Definition 4.2.1) and that Cov( $\mathcal{C}$ ) is a proper class of coverings satisfying properties (1), (2), and (3) of Sites, Definition 7.6.2."" The definition of category they refer to in Definition 4.2.1 requires that it only have a set of objects. But then surely it is impossible to choose a proper class of coverings. Am I misunderstanding the notion of a small category, and the notion of sites completely? Or is this just an inconsistency in the Stacks Project (this seems unlikely since it specifically references the Stacks Project definition) or is there something else going on I am not understanding?","['grothendieck-topologies', 'category-theory', 'algebraic-geometry', 'sheaf-theory', 'set-theory']"
2989445,Is the $\sigma$-algebra generated by closed rectangles Borel $\sigma$-algebra?,"$\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$ Suppose we have rectangles in the form $\{[a_1, b_1]\times[a_2,b_2]\times\cdots\times[a_n,b_n]\}\subset \mathbb{R}^n$ . Let $\mathcal{C}$ be the set of all such rectangles. Does $\mathcal{B}^n =\sigma(\mathcal{C})$ ? I'm also curious about balls. Let $\mathcal{D}$ be sets of balls in the form of $\{ x\in\mathbb{R}^n|\norm{x-a}<r^2 \}$ . Does $\mathcal{B}^n =\sigma(\mathcal{C})$ ? Let's say all $a,b,r$ are rationals. Also, is $\sigma(\mathcal{D})=\sigma(\mathcal{C})$ true? I think the proof could be done by saying balls can be represented by countably union/intersection of rectangles. But I don't know how to write it explicitly? An answer for the case in dimension $2$ would be appreciated.","['probability-theory', 'functional-analysis', 'measure-theory', 'real-analysis']"
2989531,"Every group homomorphism from $(\mathbb{Q}, +)$ to $(\mathbb{Q}, \times)$ is the trivial map.","How do you show that every group homomorphism from $(\mathbb{Q}, +)$ to $(\mathbb{Q}, \times)$ is the trivial map? I am trying to use proof by contradiction by assuming there is an element $\frac{a}{b}$ such that some homomorphism $\phi$ has $\phi(\frac{a}{b}) = \frac{c}{d} \ne 1$ . But I cannot seem to deduce any contradictions from here. Maybe using direct proof is a better approach? Any help is appreciated.","['group-theory', 'abstract-algebra']"
2989555,The automorphism group of $S_6$ is isomorphic to a semidirect prodct,On this document an outer automorphism of $S_6$ is constructed. I would like to use this construction to prove that $\mathrm{Aut}(S_6)\cong S_6\rtimes_\varphi\mathbb{Z}_2$ . The idea would be to find an outer automorphism of order 2. If $F$ is the outer automorphism constructed in the document above then I can prove that the order of $F$ is even. Any outer automorphism is of the form $F\gamma_\sigma$ for some inner automorphism $\gamma_\sigma$ . I can also prove that $F^2$ is inner so $F^2=\gamma_\tau$ . I think one needs to use $\tau$ to construct $\sigma$ so that $F\gamma_\sigma$ has order 2. This is basically the approach used in this paper . But it seems to me the the construction of the outer automorphism done in the paper is a bit different than the construction I am interested in. I feel like there should be a way to modify Rotman's argument to make it work for the first construction. Any ideas?,"['automorphism-group', 'finite-groups', 'abstract-algebra', 'symmetric-groups', 'group-theory']"
2989612,Differentiability in the complex plane and in $\Bbb R^2$.,"Are the differentiability in the complex plane and differentiability in $\Bbb R^2$ different concepts? Consider the linear operator $T$ on $\Bbb R^2$ defined by $T(x,y) = (x+y,x-y),\ x,y \in \Bbb R$ .Then clearly $T$ is differentiable in $\Bbb R^2$ but it is not differentiable in $\Bbb C$ since Cauchy-Riemann equation is not satisfied at any point of $\Bbb C$ . What is the basic difference between these two notions of differentiability? Any help will be highly appreciated.","['complex-analysis', 'cauchy-riemann-equations', 'multivariable-calculus', 'derivatives']"
2989639,What is this sum equal to? $\sigma(n)=\sum_{i\neq j} \frac{1}{i^n j^n}$,"I have recently come across the following sum, taken over all positive integers $i$ and $j$ such that $i \neq j$ : $$
\sigma(n)=\sum_{i\neq j} \frac{1}{i^n j^n},
$$ where $n$ is a positive integer greater than $1$ . Can this somehow be written in terms of the Riemann Zeta function? Is there already a zeta function of this kind? As a corollary question, what about the sum $$
\sigma(n)=\sum_{i\neq j} \frac{1}{i^n j^{n-1}}?
$$","['riemann-zeta', 'convergence-divergence', 'sequences-and-series']"
2989663,"Geometrical principle used in Fourier's paper ""Theory of Heat""","Below follow an extract from Fourier's paper  ""THEORY OF HEAT"" in which he says: Consider the variable state of a solid whose heat is dispersed
into air, maintained at the fixed temperature 0. Let $ω$ be an
infinitely small part of the external surface, and $μ$ a point of $ω$ ,
through which a normal to the surface is drawn ; different points
of this line have at the same instant different temperatures.
Let $v$ be the actual temperature of the point $μ$ , taken at a
definite instant, and $w$ the corresponding temperature of a point $ν$ of the solid taken on the normal, and distant from $μ$ , by an infinitely small quantity $α$ . Denote by $x, y, z$ the co-ordinates of
the point $μ$ , and those of the point $ν$ by $x + δx, y + δy, z + δz$ ;
let $f(x, y, z) =0$ be the known equation to the surface of the solid,
and $v = Φ(x,y,z,t)$ ; the general equation which ought to give the value of $v$ as a function of the four variables $x,y,z,t$ . Differentiating the equation $f(x, y, z) = 0$ , we shall have: $$mdx+ndy+pdz=0$$ $m,n,p$ being functions of $x,y,z$ . (...) Now, it follows from the principles of geometry, that the co-ordinates $δx,δy,δz$ which fix the position of the point $ν$ of the
normal relative to the point $μ$ satisfy the following conditions: $$pδx=mδz$$ and $$pδy=nδz$$ My question is about the geometrical principle he make use to derive the last expressions? You can find the paper here: page 115-116 https://www3.nd.edu/~powers/ame.20231/fourier1878.pdf","['fourier-analysis', 'geometry', 'ordinary-differential-equations']"
2989683,Analyzing Zeeman's heartbeat equations. Proving that there is one and only limit cycle,"From Poincare Bendixon Theorem we know that a non empty compact limit set of a continuously differentiable dynamical system in the plane which contains no equilibrium point is a closed orbit. I'm trying to analyse Zeeman's heartbeat equations, \begin{align}ϵ\,\frac{dx}{dt}&=−(x^3−Tx+b),\;T>0\\ \frac{db}{dt}&=(x−x_0),\end{align} and I have to prove that there exist a closed orbit (limit cycle) and that it's the only one. So I need to use the above mentioned theorem to prove that there exist a limit cycle. I've already found out that the origin of my system is totally unstable (which is (0,0) for $\epsilon = 0.2$ , $T=1$ and $x_0=0$ ). Also all the trajectories flow away from the origin. Generally the critical point of my system is $$(x,b)=(x_0,Tx_0-x_0^3).$$ Do I consider this point to be a unique equilibrium point? Theorem says that there must be no equilibrium point, so what if there exists totally unstable critical point? Is there any other method for proving that there exist one and only limit cycle?","['bifurcation', 'analysis', 'ordinary-differential-equations']"
2989693,An example of the Pseudo-inverse of an operator,"Let $E$ an infinite dimensional complex Hilbert space and $\mathcal{L}(E)$ be the algebra of all bounded linear operators on $E$ . Definition: Let $T \in \mathcal{L}(E)$ . The Moore-Penrose inverse of $T$ , denoted by $T^{+}$ , is defined as the unique linear extension of $(\overline{T})^{-1}$ in $$D(T^{+}) = \mathcal{R}(T)+\mathcal{R}(T)^{\perp},$$ with $\mathcal{N}(T^{+}) = \mathcal{R}(T)^{\perp}$ and $\overline{T}$ is the isomorphism $$\overline{T}:=T|_{{\mathcal{N}(T)}^{\perp}}: {\mathcal{N}(T)}^{\perp} \longrightarrow \mathcal{R}(T).$$ Moreover, $T^{+}$ is the unique solution of the four ''Moore-Penrose equations'': $$TXT = T,\quad XTX = X,\quad XT = P_{N{(T)^{\bot}}}\,\,\mbox{and}\,\,\quad TX = P_{\overline{\mathcal{R}(T)}}{{|}_{D(T^{+})}}.$$ Here $\mathcal{R}(T)$ and $\mathcal{N}(T)$ denote respectively the range and the nullspace of $T$ . Also $P_{F}$ denote the orthogonal projection onto $F$ . I want to see with an example how we compute $T^{+}$ when $T$ is a non invertible operator acting on an infinite dimensional complex Hilbert space $E$ .","['hilbert-spaces', 'operator-theory', 'pseudoinverse', 'functional-analysis']"
2989694,"Math probability that ""5-out-of-36"" lottery draw has at least one pair of numbers with difference = 1.","Given a $5$ out of $36$ lottery ( $5$ unique numbers out of pool of $36$ numbers ranging $[1,2,…,36]$ ). How to calculate probability that a draw has at least one pair of consecutive numbers (like $22, 23$ -a  pair of numbers whose difference $23-22 = 1$ )? Every draw ( $5$ numbers) has $10$ pairs. For dif 1 we have total of $35$ pairs ( $1$ and $2$ , $2$ and $3$ ... $34$ and $35$ ). There are total of $630$ pairs (binomial(35, 2)). Problem is I think cannot think like that: $1$ concrete pair out of $630$ appears with $1/630$ chances. Probability to have any of $35$ dif 1 pairs is $35/630$ (if I choose $2$ numbers randomly). But I choose $5$ numbers (which give $10$ pairs) - and it is not the same as just drawing $2$ pairs out of $630$ pairs. I cannot figure out how to reason in this case. The question is about not dif 1 but also dif 2 , dif 3 ... dif 35 (there is only single such pair!). How to mathematically calculate the probability? Can I think of a single 5-out-of-36 draw as an equivalent of 10 independent ""pick a pair out of all possible pairs""? It would give dif1 ( $35$ dif1 pairs out of $630$ all pairs) as $((35/630)+(34/629)+(33/628)+(32/627)+(31/626)+(30/625)+(29/624)+(28/623)+(27/622)+(26/621))$ . But it differs greatly from a real lottery, which makes me think that formula (and reasoning) above is not applicable! P.S. Stars and bars method is described here (wiki) , example how to use it is here","['lotteries', 'probability-theory', 'probability']"
2989697,"$f(x,y)=\frac{1}{2x^2y}$ , $1\le x<\infty $ , $\frac{1}{x}\le y<x$ marginal of X and Y is?","$f(x,y)=\dfrac{1}{2x^2y}$ , $1\le x<\infty $ , $\dfrac{1}{x}\le y<x$ Derive marginal probability density function of $X$ and $Y$ I have a problem in calculating marginal of Y. We have to calculate $\int_{x}f(x,y)dx$ Usually, I draw graphs to get my limits of integration first before calculating marginal distribution but this time I am unable to figure out limits. Graph is Function changes. I have $y<x$ and $y<1/x$ as well . So how do I calculate marginal in this case?","['statistics', 'probability-distributions', 'probability']"
2989742,Can $(AA^T)^{-1}A$ be simplified? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Given an $m\times n$ matrix $A$ , where $m<n$ , can the expression $(AA^T)^{-1}A$ be simplified?","['matrices', 'linear-algebra']"
2989799,"Finding $\int \frac{\sqrt{ 25x^2 - 4}}{x} \, \mathrm{d}x$: confusion on the ""correct"" substitution","Right off the bat I factored a $4$ from the radicand to get it into  a form such that I can leverage $${\tan^2(\theta)} = {\sec^2(\theta)} - 1$$ Then I set $$\begin{align*}
\frac{25}{4}  x^2  &=  \sec^2(\theta) \\
x &= \frac{2}{5}\sec(\theta) \\
\mathrm{d}x &= \frac{2}{5} \sec(\theta)\tan(\theta) \, \mathrm{d} \theta
\end{align*}$$ Now I went ahead and substituted into the integrand; after simplification and evaluation of the new integral we get $$2\tan\theta - 2\theta$$ Now to sub back in terms of $x$ by using a right triangle $$2\tan(\theta) = \sqrt{ 25x^2 - 4}$$ (Note: on the right triangle, Opposite = $\sqrt{ 25x^2 - 4}$ , Adjacent = $2$ , hypotenuse = $5x$ ) However for $2\theta$ I made the resubstitution that $$ x  =  \frac{2}{5} \sec\theta $$ so to solve for $\theta$ $$\operatorname {arcsec} \frac{5}{2} x = \theta$$ However that was the wrong resubstitution for $\theta$ ; the correct one was $$\theta =  \arccos \left(\frac{2}{5x}\right)$$ How did $\arccos$ even get into the picture? We already had $x = \frac{2}{5}\sec(\theta)$ so intuitively I just solved for theta. I don't understand how it's $\arccos$ and not $\operatorname {arcsec}$ ; can someone help me make sense of the last part?","['integration', 'trigonometry']"
2989835,What's $\frac{\partial}{\partial A}A?$,"What's $\frac{\partial}{\partial A}A?$ In this Python App it says that it's $I\otimes I$ , but how can it be? $\frac{\partial}{\partial A}A=\left[\frac{\partial}{\partial A_{ij}}A\right]=[\frac{\partial}{\partial A_{ij}}A_{lk}]$ that equals to 1 only when $l=i$ and $k=j$ ,i.e. $\frac{\partial}{\partial A_{ij}}A$ is a matrix of $A$ 's dimensions, but only one entry is 1. All the others are zero...","['multivariable-calculus', 'matrix-calculus']"
2989919,"Does $\mathbb P\{X\in A, Y\in B\}$ mean $\mathbb P(\{X\in A\}\cap \{Y\in B\})$?","Let $X,Y:\Omega \to \mathbb R$ two randoms variables on $(\Omega ,\mathcal F,\mathbb P)$ . Now, let consider $(X,Y)$ on $(\Omega ^2, \mathcal F^2, \mathbb P\times \mathbb P)$ . I am a bit confuse with $\mathbb P\times \mathbb P=:\mathbb P_2$ . Now, I always thought that $\mathbb P_2\{X\in A, Y\in B\}$ was to denote $\mathbb P(\{X\in A\}\cap \{Y\in B\})$ . But $\{X\in A, Y\in B\}\in \mathcal F^2$ , whereas $\{X\in A\}\cap \{Y\in B\}$ is in $\mathcal F$ , no ? So I'm a bit confuse with this.","['measure-theory', 'probability']"
2989943,Proof verification of if $\{x_n\}$ is monotone then $\{y_n\} = {1\over x_1 + x_2 + \dots + x_n}$ is monotone,"Let $n\in \mathbb N$ and $\{x_n\}$ is a monotone sequence.
  Prove that: $$
\{y_n\} = {1\over x_1 + x_2 + \dots + x_n}
$$ is also a monotone sequence. Given $\{x_n\}$ is monotone then by definition: $$
\forall n\in\mathbb N:x_n \le x_{n+1} \tag1
$$ or: $$
\forall n\in\mathbb N:x_n \ge x_{n+1} \tag2
$$ Let's prove for $(1)$ first. Using definition of a monotone sequence: $$
x_1 \le x_2 \le x_3 \le \dots \le x_n
$$ So from this: $$
x_1 + x_2 + x_3 + \dots + x_n \le x_1 + x_2 + x_3 + \dots + x_n + x_{n+1} \tag 3
$$ Thus taking the reciprocal of $(3)$ we get that: $$
{1\over x_1 + x_2 + x_3 + \dots + x_n} \ge {1\over x_1 + x_2 + x_3 + \dots + x_n + x_{n+1}}
$$ But LHS is $y_n$ and RHS of the inequality is $y_{n+1}$ , therefore by definition of a monotone sequence we conclude that $y_n$ is also monotone. Case $(2)$ is obtained similarly. What bugs me is that the following still holds (at least for $x_n \ge 0$ ): $$
x_1 + x_2 + x_3 + \dots + x_n \le x_1 + x_2 + x_3 + \dots + x_n + x_{n+1}
$$ And we get once again that: $$
{1\over x_1 + x_2 + x_3 + \dots + x_n} \ge {1\over x_1 + x_2 + x_3 + \dots + x_n + x_{n+1}}
$$ But how is this possible? From the above it looks like $y_n$ is always monotonically decreasing, which seems false. For example when $\sum_{k=1}^nx_k < 1$ . Where have i taken the wrong road? Update As shown in answers and comments monotonicity is only preserved assuming all $x_n$ have the same sign.","['algebra-precalculus', 'proof-verification', 'monotone-functions', 'inequality']"
2989947,An $m$-ary function that represents all $n$-ary functions,"Motivation It is well-known that any binary operator $*$ on the boolean ring $\{0,1\}$ can be represented using only one of the $\operatorname{NAND}$ and $\operatorname{NOR}$ operators.  For example, $$x\rightarrow y=\big((x\operatorname{NOR}x)\operatorname{NOR}y\big)\operatorname{NOR}\big((x\operatorname{NOR}x)\operatorname{NOR}y\big)$$ and \begin{align}x\operatorname{XOR}y&= \Big(\big((p\operatorname{NAND}p)\operatorname{NAND}(q\operatorname{NAND}q)\big)\operatorname{NAND}\big((p\operatorname{NAND}p)\operatorname{NAND}(q\operatorname{NAND}q)\big)\Big)
\\&\hphantom{123}\operatorname{NAND}\Big(\big((p\operatorname{NAND}p)\operatorname{NAND}(q\operatorname{NAND}q)\big)\operatorname{NAND}\big((p\operatorname{NAND}p)\operatorname{NAND}(q\operatorname{NAND}q)\big)\Big),\end{align} where $$p=\big(x\operatorname{NAND}(y\operatorname{NAND}y)\big)\operatorname{NAND}\big(x\operatorname{NAND}(y\operatorname{NAND}y)\big)$$ and $$q=\big((x\operatorname{NAND}x)\operatorname{NAND}y\big)\operatorname{NAND}\big((x\operatorname{NAND}x)\operatorname{NAND}y\big).$$ The notation $\rightarrow$ is the implication connective , and $\operatorname{XOR}$ is the exclusive-or operator . Definitions Let $S$ be a set and $f:S^m\to S$ is an $m$ -ary function (or $m$ -ary operator ).  A valid  expression in $f$ is an expression $E(x_1,x_2,\ldots,x_n)$ , where $x_1,x_2,\ldots,x_n\in S$ involving only finite iterations of $f$ and using only variables among $x_1,x_2,\ldots,x_n$ . For example, if $m=n=2$ and $0\in S$ , $f\big(x_1,f(0,x_2)\big)$ is not a valid expression because there is a constant $0$ that is not in the form of the variables $x_1,x_2$ .  If $m=2$ , $n=1$ , $S=\mathbb{R}_{>0}$ , and $f(x_1,x_2)=\sqrt{x_1\sqrt{x_2}}$ , then this is also not a valid expression because it involves infinite iterations of $f$ : $$f\Biggl(x,f\Big(x,f\big(x,f(\ldots)\big)\Big)\Biggr)=\sqrt{x\sqrt{x\sqrt{x\sqrt{\ldots}}}}\ (=x).$$ (However, if you want to represent the identity function $x\mapsto x$ on $S=\mathbb{R}_{>0}$ with the function $f(x_1,x_2)=\sqrt{x_1\sqrt{x_2}}$ , this can be done without involving infinite iterations of $f$ like the previous example, i.e., $f\big(f(x,x),x\big)=\sqrt{(x\sqrt{x})\sqrt{x}}=x$ .) For $m=n=3$ , this is a valid expression of $f$ : $$f\Biggl(f\big(x_1,x_2,f(x_1,x_3,x_2)\big),f\Big(f\big(x_1,f(x_2,x_3,x_1),x_3\big),x_2,x_1\Big)\Biggr).$$ Also, not all variables need to be used.  So, for $m=n=2$ , $$f\big(x_1,f(x_1,x_1)\big)$$ is still a valid expression of $f$ .  In the previous example, one can say that this is a valid expression of $f$ with $m=2$ and $n=1$ , as well.  There can also be more variables than the number of arguments of $f$ , that is, if $m=2$ and $n=3$ , $$f\big(f(x_1,x_2),f(x_2,x_3)\big)$$ is a valid expression of $f$ . Let $g:S^n\to S$ .  We say that $g$ is representable by $f$ if there exists a valid expression $E(x_1,x_2,\ldots,x_n)$ of $f$ such that $$g(x_1,x_2,\ldots,x_n)=E(x_1,x_2,\ldots,x_n)$$ for all $x_1,x_2,\ldots,x_n\in S$ .  For an $m$ -ary function $f:S^m\to S$ , we say that $f$ is $n$ -fulfilling if every $n$ -ary function $g:S^n\to S$ is representable by $f$ . Question For a given non-empty set $S$ and positive integers $m,n$ , when does there exist a $n$ -fulfilling $m$ -ary function $f:S^m\to S$ ?  Does there exist a set $S$ with $|S|\geq 3$ along with a positive integer $m$ such that for some an $m$ -ary function $f:S^m\to S$ exists, and for any positive integer $n$ , every $n$ -ary function $g:S^n\to S$ is representatble by $f$ . Has there been a study on this type of questions?  Any reference is greatly appreciated. Known Results If $S$ is infinite, then there are only countably many valid expressions of $f$ in $n$ variables, but there are uncountably many $n$ -ary functions $g:S^n\to S$ .  Therefore, such a function $f$ does not exist.  Hence, we can assume that $S$ is finite. If $m=1$ , then there exists an $n$ -fulfilling function $f:S\to S$ if and only if $|S|=1$ or $\big(n,|S|\big)=(1,2)$ .  Clearly, the only valid expression of $f$ in any number of variables is of the form $f^k(x)$ .  Therefore, when $|S|>1$ , there can only be one variable, so $n=1$ .  However, since the permutation group on $S$ is not abelian for $|S|>2$ , we must have $|S|=2$ (provided that $|S|>1$ ). Of course, if $|S|=1$ , then any $m$ -ary function $f:S^m\to S$ and any $n$ -ary function $g:S^m\to S$ have the same image.  So, this case is very trivial.   If $|S|=2$ , I think that we can identify $S$ as the boolean ring $\{0,1\}$ and use the $\operatorname{NAND}$ or $\operatorname{NOR}$ operators to represent any $n$ -ary function $g:S^n\to S$ . For the Bounty Prize I am willing to award the prize for An example of $(S,m,n)$ with $|S|\geq 3$ and $m\geq 2$ such that there does not not exist an $m$ -ary function $f:S^m\to S$ that is $n$ -fulfilling, or a proof without an explicit construction that such $(S,m,n)$ exists. An example of $(S,m,n)$ with $|S|\geq 3$ and $m\geq 2$ such that an $m$ -ary function $f:S^m\to S$ is $n$ -fulfilling, or a proof without an explicit construction that such $(S,m,n)$ exists. Any stronger result than 1. and 2.","['reference-request', 'functions', 'combinatorics', 'binary-operations', 'boolean-ring']"
2989979,"Given a function $f:\mathbb{R}^2 \to \mathbb{R}^2$, to find its inverse near a given point","Let $f:\mathbb{R}^2 \to \mathbb{R}^2$ be a function given by $$f\left(x,y\right)=\left(x-y,xy\right),\,\, \left(x,y\right) \in \mathbb{R}^2$$ Question : What is the inverse of $f$ near the point $\left(2,-3\right)$ ? Upon checking the conditions, inverse function theorem gives me the existence of $f^{-1}$ , that $f^{-1}$ is $C^1$ under appropriate condition and an explicit form of the derivative of $f^{-1}$ . However, I do not understand how to calculate an explicit form of $f^{-1}$ using inverse function theorem . By direct calculation, I find : $$f^{-1}\left(u,v\right)=\left(\frac{2v}{-u\pm\sqrt{u^2+4v}},\frac{-u\pm\sqrt{u^2+4v}}{2}\right)$$ Two issues : $1$ . It appears to be a one-to-two(!) map. $2$ . At the point $\left(2,-3\right)$ , both the arguments are complex numbers! Any help would be much appreciated.","['multivariable-calculus', 'inverse-function-theorem', 'real-analysis']"
2990005,Why this random walk can't go on forever?,"$A$ starts with $i$ coins, $B$ with $N-i$ . At each trial, $A$ gives one coin to $B$ with probability $p$ or $B$ gives one coin to $A$ with probability $q$ where $p+q=1$ . This can be modeled as a 2D random walk starting from $i$ where probability of moving right = $p$ , left = $q$ , and the walk ends at reaching either $0$ or $N$ Nothing in this statement seems to say that oscillating around i and never reaching either $0$ or $N$ is not a possibility. However, doing the following calculation, something seems off. Let $p_i$ be the probability that A will end up with all money, that is, the object will reach N, when starting position is $i$ . $$p_i = p*p_{i+1} + q*p_{i-1}$$ Solving this difference equation gives $$p_i = \frac{1-(\frac{q}{p})^i}{1-(\frac{q}{p})^N} $$ for $p\neq q$ Now,
P(reaching N starting from $i$ ) = $p_i$ By symmetry, P(reaching 0 starting from $i$ ) = $\frac{1-(\frac{p}{q})^{N-i}}{1-(\frac{p}{q})^N}$ Adding those together equals 1. Which means either A wins or B wins. That is no probability left for just oscillating around $i$ and never reaching either $0$ or $N$ . Why is that so? The probability for the event, going from i to i+1, then back to i, then i+1 and so on = $p*q*p*q*...  = (pq)^n$ where n can go up to infinity. However small, this is a positive number. And unless n goes to infinity, it is greater than zero. I understand that $\lim_{n \to \infty} (pq)^n = 0$ . But how is that applicable here. As $n \to \infty$ , probability $\to 0$ . But $n$ is always less than $\infty$ , so probability is always greater than $0$ . Please tell me if I am wrong with my interpretation of limits. Is it correct that either A or B has to win? Why?","['limits', 'infinity', 'random-walk', 'probability']"
2990030,For which values of $a$ is $f$ primitivable?,"Let $a \in \mathbb{R}$ and $p,q$ be natural numbers with $p \geq q+2.$ For which values of $a$ is the function $$f(x) = 
\begin{cases}
\frac{1}{x}\sin \frac{1}{x^p}\sin \frac{1}{x^q}, &x \neq 0 \\
a, &x=0
\end{cases}
$$ primitivable? I noticed that $\frac{1}{x}\sin \frac{1}{x^p}\sin \frac{1}{x^q}$ doesn't have an elementary antiderivative, so I tried to obtain it from the derivative of some function and work this from there. But those powers in the denominator are really getting in the way of any attempt. I also thought of using the formula $\sin a \sin b = \frac{1}{2}(\cos(a-b)-\cos(a+b))$ and so I split the function like  this: $$f(x)=
\frac{1}{2}\begin{cases}
\frac{1}{x}\cos(\frac{1}{x^p}-\frac{1}{x^q}), &x \neq 0 \\
a, &x=0
\end{cases} -
\frac{1}{2}\begin{cases}
\frac{1}{x}\cos(\frac{1}{x^p}+\frac{1}{x^q}), &x \neq 0 \\
a, &x=0
\end{cases}
$$ and this definitely looks more promising, but I don't know how to proceed.","['integration', 'indefinite-integrals', 'calculus', 'real-analysis']"
2990045,Why are Klein geometries $G/H$?,"The idea behind Klein geometries is simple, clear and beautiful. Simply we have a manifold M and a Lie group G who acts on it and we study the properties that remain invariants under this action. But then, when I open the books about Klein geometry they start with something like: choose a point $x \in M$ , take the stabilizer $H$ of x, then the Klein geometry is $G/H$ ; instead of $(M, x)$ we can study $(G, H)$ . Now, even if I ""understand"" what they say, I cannot really understand what this means and why one chooses this instead of the natural definition. In particular What does it really mean to study $G/H$ instead of $M$ ? For example if I want to study properties of curves, I will consider $\gamma: I \to M$ and not something like $\gamma: I \to G/H$ . For example if I consider $M = \mathbb{R}^2$ , $x = \vec O$ and $G = SO(2)$ (i.e I do not consider translations) I will have $H = SO(2)$ and $G/H = \{0\}$ : what does it mean that my geometry is ""empty""? What is $G/H$ in practice? Why do every one define them so? Which are the advantages? Thanks in advance","['homogeneous-spaces', 'lie-groups', 'differential-geometry']"
2990086,Contradiction between first derivative formal definition and derivative rules?,"When I try to find the derivative of $f(x) = \sqrt[3]{x} \sin(x)$ at $x=0$ , using the formal definition of first derivative, I get this: $$
f'(0) = \lim\limits_{x \to 0} \frac{\sqrt[3]{x} \sin(x)-0}{x-0},$$ which gives zero. However, when I use derivative rules I get that: $$
f'(x) = {\sin(x) \frac{1}{3\sqrt[3]{x^2}}+\cos(x)\sqrt[3]{x}}
$$ and thus $f'(0)$ doesn't exist. Why does this happen? what's the reason behind it?","['limits', 'calculus', 'derivatives']"
2990131,The role of constant of integration,"Let us consider the following integral $$\int \frac{\sin x}{\cos^3 x}dx\, .$$ If we note that $\tan x= \frac{\sin x}{\cos x}$ , we have $$\int \frac{\tan x}{\cos^2 x}dx = \frac{\tan^2 x}{2}+C \quad (1)$$ since $\int f(x) f^\prime (x) dx= \frac{f^2(x)}{2}+C$ and $f(x)=\tan x$ . Otherwise, if we substitute $t=\cos x$ : $$-\int\frac{dt}{t^3}=\frac{1}{2\cos^2 x}+C \quad (2) .$$ The result shall be the same, but $\frac{\tan^2 x}{2}\neq \frac{1}{2\cos^2 x}$ . The reason for this apparent difference is the constant $C$ in (1) and (2). They are not the same constant, and actually $\frac{1}{\cos^2 x}=\tan^2 x+1$ . If we denote by $C_1$ the constant of (1) and by $C_2$ the constant of (2), we have $C_2=\frac12+C_1$ . In my opinion, this example is useful to let students know how important is the constant of integration. Can someone suggest me other examples of this type, without trigonometric functions?","['integration', 'education']"
2990132,Find $y'$ where $y=\ln(x+\sqrt{a^2+x^2}).$,"My solution Of course, you may apply the derivative rule for the compound function. But I want to give another solution with little computation. Denote $$u=x+\sqrt{a^2+x^2},$$ $$v=x-\sqrt{a^2+x^2}.$$ Then $$u+v=2x,uv=-a^2.$$ Hence $$u'+v'=2,u'v+uv'=0.$$ We may obtain $$\frac{u'}{u}=\frac{2}{u-v}=\frac{1}{\sqrt{a^2+x^2}}.$$ As a result $$y'=\frac{u'}{u}=\frac{1}{\sqrt{a^2+x^2}}.$$ AM I RIGHT?","['calculus', 'proof-verification', 'derivatives']"
2990150,Growth of Digamma function,"For $1\le \sigma \le 2$ and $t\ge 2$ , $s=\sigma+it$ prove that $\displaystyle \frac{\Gamma'(s)}{\Gamma(s)}=O(\log t)$ . From Stirling's formula we have, $\displaystyle \Gamma(s)\approx \sqrt{2\pi}\exp\{s\log s-s-\frac 12 \log s\}$ . Then, $\displaystyle \frac{\Gamma'(s)}{\Gamma(s)}\approx\log s-\frac{1}{2s}$ . From here I'm unable to estimate !! Any hint. ? Where can I get rigorous proof ? Edit: Wikipedia links below the question are NOT clear enough to me.","['digamma-function', 'complex-analysis', 'analytic-number-theory', 'gamma-function', 'riemann-zeta']"
2990178,espace etale of the sheaf,"This is exercise I-8 from Geometry of schemes by Eisenbud and Harris. Let $\mathscr{F}$ be a sheaf, then topologize the union $\overline{\mathscr{F}}=\bigcup_{x \in X}\mathscr{F}_x$ by taking basic open set of this form $$\mathscr{V}(U,s)= \{(x,s_x): x \in U\}$$ Where $s \in \mathscr{F}(U)$ and $s_x$ is the corresponding equivalence class in the stalk. It is clear that there is a natural continuous ""projection"" $\pi:\overline{\mathscr{F}}\longrightarrow X$ I have trouble proving the following: Suppose $\sigma:U \longrightarrow \overline{\mathscr{F}}$ be a continuous map such that $\pi \circ \sigma$ is the identity on $U$ . Then there exists $s \in \mathscr{F}(U)$ such that $s_x = \sigma(x)$ for all $x \in X$ . Here is my idea, for every $x \in X$ , it is clear that $\sigma(x) \in \mathscr{F}_x$ . Then there exists open set $V_x$ , $t^x \in \mathscr{F}(V_x)$ such that $\mathscr{V}(V_x,t^x)$ is a basic open neighbourhood containing $\sigma(x)$ . It is not hard to see that for all $x' \in V$ , $t^x_{x'} = \sigma(x') \in \mathscr{F}_{x'}$ . Then $\{V_x\}_{x \in U}$ is an open cover for $U$ and apply the sheaf axiom on the collection $\{t^x\}_{x \in X}$ . By I'm stuck on one little detail. Let $x,y \in U$ and suppose $V = V_x \cap V_y \neq \emptyset$ . Why is it true that $t^x|_V = t^{y}|_V$ ?. What I know is that for all $w \in V$ , $t^x_{w} = t^{y}_{w} = \sigma(w) \in \mathscr{F}_w$ . But that only means there exists $W \subseteq V$ containing $w$ such that $t^x|_W = t^{y}|_W$ . This is a slightly weaker statement, so how should I proceed?","['algebraic-geometry', 'schemes', 'sheaf-theory']"
