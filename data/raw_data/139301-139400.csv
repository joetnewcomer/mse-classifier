question_id,title,body,tags
2237894,In how many ways can you arrange $4n$ tennis players into doubles matches?,"The Problem A tennis club has $4n$ members. To specify a doubles match, you choose two teams of two people. In how many ways can you arrange the members into doubles matches so that each player is in one doubles match?  In how many ways can you do it if you also specify who serves first on each team? Attempt at a Solution So this problem is actually a follow up to this question .  It comes from the same exact text as well.  From the other problem we know that there are exactly $$\frac{(2n)!}{(n!)2^n}$$ ways to pair up $2n$ members for singles matches.  I am tempted to simply double this number for $4n$ people, but that seems like it would be far to easy. Questions and Confusion While I am sure this problem abides by a similar logic to the one I linked, I am still struggling to understand a few key things. Do we consider the pairings $(p_1,p_2)$ and $(p_2, p_1)$ to be different?  I feel like we aren't, but I cannot explain why... Here we are specifying doubles matches ""so that each player is in one doubles match.""  Does this mean that each player can only be in one doubles match?  Or does this simply mean to ensure that no player is left out of a match? Thanks everyone!","['combinatorics', 'discrete-mathematics']"
2237909,Are the graphs of inverse functions always reflections of the function's graph in the line $y = x$?,"For an invertible function, is the graph of its inverse always the mirror image of the function's graph in the line $y=x$ ? And, if yes, then why is it so?",['functions']
2237924,"Show that $f(x) = x\ln(x)$ is continuous on $(0,1)$","The easiest thing I can think of is to compute the derivative which comes out to be $ f'(x) = \ln(x) + 1 $. Since this is defined for all values on the interval $(0,1)$, the original function must be continuous. However, I find $\epsilon - \delta$ proofs of continuity to be more... beautiful and elegant (and hard for me to get right). My attempt at that approach was this: $$ |f(x) - f(k)| = |x\ln(x) - k\ln(k)| = |x\ln(x) - k\ln(x) + k\ln(x) - k\ln(k)| \\ \leq |x\ln(x) - k\ln(x)||k\ln(x) - k\ln(k)| \leq |x - k||\ln(x)||k||\ln(x) - \ln(k)| \\ \leq  |x - k||\ln(x)||\ln(x) - \ln(k)| $$ But I get stuck here. Is the derivative method okay? And what could I do to make the other method work? Thank you for your input!","['derivatives', 'epsilon-delta', 'continuity']"
2237945,Derivative of expected log likelihood in a logistic regression model,"Consider the univariate logistic regression model:
$$
P(Y = 1\mid X = x) = \psi(x\beta_0)\equiv \frac 1 {1+\exp\{-x \beta_0\}},\quad\text{for all $x$, and some unknown $\beta_0\in\mathbb{R}$.}
$$
Assume that, $X$ has a finite positive variance and marginal distribution $Q(x)$. The score function based on one sample $(Y,X)$ is,
$$
Z(\beta :Y,X) = X\cdot\big\{Y-\psi(X\beta)\big\}.
$$
The expected log-likelihood based on one sample is
$$
M(\beta)\equiv \mathbf{E}\left[Y\log\psi(X\beta) + (1-Y)\log\big\{1-\psi(X\beta) \big\} \right],
$$
where, $\mathbf{E}$ denotes expectation under the true joint distribution of $(Y,X)$ under the parameter $\beta_0$. My questions are: (i) How to show that $M(\beta)$ is finite for all $\beta$. (ii) Is, $M^\prime(\beta)=\mathbf{E}\big(Z(\beta:Y,X)\big)$, for all $\beta$. If so, then what are the required conditions.","['maximum-likelihood', 'statistics', 'estimation', 'logistic-regression']"
2237952,Show this $|\sin{x}|+|\sin{(x+1)}|+|\sin{(x+2)}|>\frac85$,"Let $x\in R$ show that
$$f(x)=|\sin{x}|+|\sin{(x+1)}|+|\sin{(x+2)}|>\dfrac{8}{5}$$ since
$$f(x)=f(x+\pi),$$it sufficient to show $x\in (0,\pi]$","['inequality', 'trigonometry', 'convex-analysis', 'calculus', 'absolute-value']"
2237994,Back-projecting Pixel to 3D Rays in World Coordinates using PseudoInverse Method,"For perspective projection with given camera matrices and rotation and translation we can compute the 2D pixel coordinate of a 3D point. using the projection matrix, $$
P = K [R | t]
$$ where $K$ is intrinsic camera matrix, $R$ is rotation $t$ is translation. The projection is simple matrix multiplication $x = P X $. 
Zisserman's book , pg. 161 suggests using $3 \times 4$ projection matrix and taking pseudoinverse. Then one would compute $X$ which defined up to scale which can then be interpreted as the ray starting from camera center going to infinity. I quickly coded this up, I took $Z$ as depth, so I translated the camera in $Y$ direction (up 1 meter), and after retrieving $X$ flipped $Y,Z$ for plotting (most projective geom. math seems to be built to make $Z$ depth), K = [[ 282.363047,      0.,          166.21515189],
     [   0.,          280.10715905,  108.05494375],
     [   0.,            0.,            1.        ]]
K = np.array(K)
R = np.eye(3)
t = np.array([[0],[1],[0]])
P = K.dot(np.hstack((R,t)))

import scipy.linalg as lin

x = np.array([300,300,1])
X = np.dot(lin.pinv(P),x)
X = X / X[3] 
from mpl_toolkits.mplot3d import Axes3D
w = 20
f = plt.figure()
XX  = X[:]; XX[1] = X[2]; XX[2] = X[1]
ax = f.gca(projection='3d')
ax.quiver(0, 0, 1., XX[:3][0], XX[:3][1], XX[:3][2],color='red')
ax.set_xlim(0,10);ax.set_ylim(0,10);ax.set_zlim(0,10)
ax.quiver(0., 0., 1., 0, 5., 0.,color='blue')
ax.set_xlabel(""X"")
ax.set_ylabel(""Y"")
ax.set_zlabel(""Z"")
ax.set_title(str(x[0])+"",""+str(x[1]))
ax.set_xlim(-w,w);ax.set_ylim(-w,w);ax.set_zlim(-w,w)

ax.view_init(elev=29, azim=-30)
fout = 'test_%s_01.png' % (str(x[0])+str(x[1]))
plt.savefig(fout)
ax.view_init(elev=29, azim=-60)
fout = 'test_%s_02.png' % (str(x[0])+str(x[1]))
plt.savefig(fout) These images below are the result (blue arrow shows the normal vector perpendicular to the image plane, the images demonstrate all x=10,300 y=10,300 combinations): I give the camera/ray plot for each pixel from two different angles. Do these results look sensible? 10,10 and 200,200 looked odd, I played around with signs a little bit, if I translate up using negative -1, and using -Z after X calc., things improve somewhat? t = np.array([[0],[-1],[0]])
..
XX  = X[:]; XX[1] = X[2]; XX[2] = -X[1] I do not know why that is.","['computational-geometry', 'projective-geometry', 'image-processing', 'geometry', 'linear-algebra']"
2238009,How many different strings of length 12 containing exactly five a's can be chosen over the following alphabets?,"(a) 
The alphabet {a, b} (b) 
The alphabet {a, b, c} For a: 
I got $\binom{12}{5}$ For b: $2^7 \cdot \binom{12}{5}$ Normally, I would never waste the users time by asking a ""yes"" ""no"" question. But since, for some reason, authors these days have decided that including answers to exercises at the end of their books is a horrible idea, I have to go ahead and waste everyone's time if I want to learn a simple method. So, can someone please tell me if my answers are right? Thanks, and sorry for the waste of time.","['combinatorics', 'binomial-coefficients', 'discrete-mathematics']"
2238045,Squaring both sides of an inequality of opposite signs,Can someone explain to why the algebra does not work for this inequality $\sqrt{1-y}\gt-\frac{1}{2} $. $1-y\gt\frac{1}{4} $. $y\lt\frac{3}{4} $. I know that it should be $y \lt 1$ since the RHS is $\ge0$,"['algebra-precalculus', 'inequality']"
2238156,what is the surface area of a cap on a hypersphere?,"According to mathworld , let the sphere have radius $R$, then the surface area a spherical cap of height $h$ and base radius $a$ is given by
$$S=2\pi Rh=2\pi(a^2+h^2).$$ What is this value for an n-dimensional hypersphere? If it helps simplify the problem we can assume $R=1$ and $a=0.5$. Many thanks.","['spheres', 'integration', 'area', 'geometry']"
2238178,The concept/physical meaning/interpretations behind the Bessel's inequality,"If $\{\boldsymbol{\varphi}_1, \boldsymbol{\varphi}_2, ... \}$ is an orthonormal system in
  $\mathcal{H}$, then the Bessel's inequality is: $$\sum_{j=1}^{\infty} |\langle \mathbf{x},\boldsymbol{\varphi}_j \rangle|^2 \leq \lVert
 \mathbf{x} \rVert^2 \quad \text{for every } \mathbf{x}\in \mathcal{H}.$$ My question is: What is the meaning/concept of this inequality? Is there any geometric or physical interpretations about that? Any comments or answers are appreciated.","['inequality', 'abstract-algebra', 'geometric-inequalities', 'functional-analysis', 'linear-algebra']"
2238216,What are some research problems that fit as a good candidate for undergraduate research?,"I'm in my junior year in college and I hope thag I can do some original research in while being in college. Sadly, my departement is not large and most professors are elements of one of the following types: 1.They are available but they have not done  research for a long time. They are doing good research but either they don't come much to the uni or have a view against undergraduate research. So it seems that it would be difficult to get involved in research with some faculty member in my college. Nevertheless, I want to do some research. I'd be satisifed if the problems are in any topic: algebra, analysis, combinatorics, graph theory, logic etc ... More Importantly, I have the interest and passion to study and work through the required background if I don't fully have it as long as I find an interesting problem that I worlk work on. It would be even better if you think that the problem is suited to undergraduate research. I know that it is hard to indicate some problems without knowing the background of the student, but generally, as an indicatiin, you assume that the student has a good background in algebra (groups, rings, fielda, modules,universal algebra, category theory), in logic (FOL, completness, incompletness, set theory,forcing, boolean algebras) etc ... If needed I can provide more details regarding my background in a specific area if needed.","['abstract-algebra', 'problem-solving', 'soft-question', 'research', 'discrete-mathematics']"
2238243,Does this bundle related differential equation always have non-trivial solutions?,"Let $M$ be a smooth connected $d$-dimensional manifold, $E$ a vector bundle over $M$ of rank $d$. Let $\sigma \in \Omega^{d-1}(M,E)$ be a non-zero $E$-valued form on $M$ (of degree $d-1$). I am interested in the subspace $A_{\sigma}=\{ h \in C^{\infty}(M) \, | \, \sigma \wedge dh =0\}$. ($dh$ is a real valued one form, so $\sigma \wedge dh \in \Omega^{d}(M,E)$). $A_{\sigma}$ always contain the constant functions, so $\dim A_{\sigma} \ge 1$. Is it true that $\dim A_{\sigma} > 1$? Can $\dim A_{\sigma}$ be finite? infinite? Can we construct an ""explicit"" basis for $A_{\sigma}$ (or even one non-constant function in $A_{\sigma}$) in terms of $\sigma$? One possible strategy is to consider the relaxed equation $$\sigma \wedge \omega=0, \omega \in \Omega^1(M), \tag{1}$$ look for closed solutions, and hope they might be exact (The topology of $M$ might enter the game, of course). Actually, we should start by making sure there is no ""pointwise"" obstruction to equation $(1)$: Let $p \in M$. Does there exist $\omega_p \in T_p^*M$ such that $\sigma_p \wedge \omega_p=0$? (This is a fiberwise question, on the level of linear algebra).","['vector-bundles', 'riemannian-geometry', 'differential-geometry', 'partial-differential-equations']"
2238251,Let $A$ be a symmetric matrix of order $n$ and $A^2=0$ . Is it necessarily true that $A=0$,"Let $A$ be a symmetric matrix of order $n$ and $A^2=0$ . Is it necessarily true that $A=0$ . My approach : I tried to experiment with some $2\times 2$ matrices but never gotten any far . Now, Wikipedia says that there exists a diagonal matrix $D$ and orthogonal matrix $Q$ such that $D=Q^t A Q$ .
So $D^2=Q^t A^2 Q=0$ . As $D$ is diagonal matrix with real entry we get $D=0$ . So $A=QDQ^t =0 $ 
I think my proof is correct . I just want know if there is any way to prove this without citing any big theorems or in more elementary way . The problem is quoted from a part of the web-text that only used elementary definitions like what a symmetric matrix is . So i'm curious if there is an elementary solution to the problem .","['matrix-equations', 'matrices', 'abstract-algebra', 'linear-transformations', 'linear-algebra']"
2238253,Finding E[1/X] from E[X],"Is there a shortcut? 
I already have E[X] where X=Y+1, thus I can separate it into E[Y]+E[1], and for Y, it's number of tries is n, and the probability p.",['statistics']
2238270,How to solve polynomial of degree 4?,"I'm beginner in learning algebra and there is a question which came into my mind that how to find value of x in this type of equation -- > $x^4 + x^3 + x = 3$ I know that one of its answer will  be 1,but I was wondering how to solve it through equation form. Please help.Thank you in advance.","['algebra-precalculus', 'polynomials']"
2238274,Finding the exact value of a radical,How do I show that $\sqrt{97 +56\sqrt3}$ reduces to $7 +4\sqrt3?$. Without knowing intitially that it reduces to that value.,['algebra-precalculus']
2238297,Probability that product of means is larger than product of maximum and minimum,"Given $X_1,...,X_n \stackrel{iid}{\sim} \text{Unif}(a,b)$, what is $$\mathbb{P}\left[\frac{\sum x_i}{\sum \frac{1}{x_i}} \geq \max(x_i) \cdot \min(x_i)\right].$$ I don't need a closed-form formula, just a description of the asymptotic behavior would be nice. Numerical experiments seem to show that the probability tends to 1. Here $a,b$ are arbitrary. As wolfies pointed out in the comments, when $a=0$, the result is immediate since $\min(x_i)\to 0$. So we can assume $a>0$. The inequality is invariant by scaling, so we can assume w.l.o.g. that $a=1$ and $b$ is arbitrary. A little background :
I am studying an algorithm which has parameters $x_i \in \Bbb R_+, i =1,...,n$. The algorithm has provably good performances when 
$$\frac{\sum x_i}{\sum \frac{1}{x_i}} \geq \max(x_i) \cdot \min(x_i).$$
The left-hand side can be written as $AM(x_i) \cdot HM(x_i)$ where $AM$ and $HM$ are the arithmetic and harmonic means. Both quantities are between $\min(x_i)$ and $\max(x_i)$, so nothing can be said about their product in general. However, numerical evidences seem to indicate that the event occurs with probability one as $n\to \infty$ if the $x_i$'s are sampled uniformly. But I am unable to come up with a proof of this fact.","['inequality', 'probability']"
2238328,Cardinality & Schroder-Bernstein Theorem,"It is known that each $x,y \in (0,1)$ has a unique decimal expansion $ x = 0.x_1x_2x_3\ldots $ and $y = 0.y_1y_2y_3\ldots$ with the requirement that there are inﬁnitely many indices diﬀerent from $9$. Show that the following function $f : (0,1)×(0,1) → (0,1)$ is injective: $f(x,y) = 0.x_1y_1x_2y_2\ldots$ Is $f$ also surjective? Using the Schröder-Bernstein theorem show that $(0,1)×(0,1)$  and $(0,1)$ have the same cardinality. For the injectivity part of the question, this is what I have so far.
I have defined 2 sets $S=\{x_n : n \in \mathbb{N}\}$ and $T=\{y_n : n \in \mathbb{N}\}$. Then I have said $f(x,y)=\sum_{n\geq0}x_n10^{-2n+1}+y_n10^{-2n}$. Then I have concluded that since $x$ has a unique decimal expansion and so does $y$ then it follows that the sum of 2 unique expansions is itself unique, but I feel that this is missing a great deal of rigour and would like to know if it's even right or how to improve it. Also, I am unsure as to how to deal with the surjectivity part of the question. Any help would be greatly appreciated.","['decimal-expansion', 'real-analysis', 'cardinals', 'functions']"
2238329,Where is the axis of rotation sending given vector to another vector?,"Let $p,q $ be unit vectors in $\mathbb R^3$ with the Euclidean norm. Then it is known that there exists a rotation $f$ of $\mathbb R^3$ such that $f(p)=q$. 
It is easy to find two such rotation: one with rotation axis orthogonal to $p,q$ and the second with axis in the plane generated by $p,q$. How can one prove that if $p+q\neq 0$ then the rotation axis of $f$  has to lie in the plane generated by $p\times q$ and $p+q$?",['geometry']
2238374,Variance of squared norm of multivariate normal vector,"Suppose, that we have a secuence of independent random variables $\{w_i\}_{i=0}^n$ from $\mathcal{N}(0,\mathbb{I}_d)$. We consider the following weighted sum defined for some $\beta \in (0,1)$ $$
\theta = \sum_{i=0}^n (1 - \beta)^i w_i
$$ We are interested in the value of $\text{Var}(\|\theta\|_2^2)$. My approach. I have started with calculation of $\mathbb{E}(\|\theta\|_2^2)$. It appears to be $\frac{d}{\beta(2-\beta)}$. Next, I moved to estimation of $$
\mathbb{E}(\|\theta\|_2^4) 
= 
\mathbb{E}\left(\| \sum_{i=0}^n (1 - \beta)^i w_i \|_2^4\right)
= 
\sum_{i=0}^n (1 - \beta)^i \mathbb{E}(\|w_i\|_2^4)
+
\sum_{i,j=0, i\ne j}^n 3(1 - \beta)^{2i+2j} \mathbb{E}(\|w_i\|^2)\mathbb{E}(\|w_i\|^2)
$$ Then I have found that $\mathbb{E}(\|w_i\|_2^4) = d^2  + 2d$ and $\mathbb{E}(\|w_i\|^2) = d$. Then I used the following equality $$
\sum_{i,j=0}^n (1 - \beta)^{2i+2j} \approx \frac{1}{(1-(1-\beta))^2}.
$$ to obtain for small $\beta$
$$
\mathbb{E}(\|\theta\|_2^4) 
\approx 
\sum_{i,j=0, i\ne j}^n 3(1 - \beta)^{2i+2j} d^2
\approx
\sum_{i,j=0}^n \frac{3d^2}{(1-(1-\beta))^2}.
$$ Then $$
\text{Var}(\|\theta\|_2^2) = \mathbb{E}(\|\theta\|_2^4) - (\mathbb{E}(\|\theta\|_2^2))^2 = \frac{2d^2}{(1-(1-\beta))^2}.
$$ But computations show that the variance is 
$$
\text{Var}(\|\theta\|_2^2) = \frac{2d}{(1-(1-\beta))^2},
$$
which is $d$ times smaller than mine.","['multivariable-calculus', 'random', 'normal-distribution']"
2238417,Evaluate $\lim_{x\to 0} \frac{a^x -1}{x}$ without applying L'Hopital's Rule. [duplicate],"This question already has answers here : Show $\lim_{h\to 0} \frac{(a^h-1)}{h}$ exists without l'Hôpital or even referencing $e$ or natural log (5 answers) Closed 7 years ago . The questions is: Evaluate $$\lim_{x\to 0} \frac{a^x -1}{x}$$ without applying L'Hopital's Rule. Does this question fundamentally same as asking if the $\lim_{x\to 0} \frac{a^x -1}{x}$ exists? rather than straightway asking to find the limit. That means are questions (1) proving if the limit of a function exists and (2) asking what is the limit of that function, essentially same question?","['limits-without-lhopital', 'limits']"
2238427,Why is $\frac{\sin(z)}{z}$ analytic at $z=0$,I am attempting exercise $11.2.9 a$ from Mathematical Methods for Physicists. Having a look at a provided solution it states that: The derivative is: $f'(z) = \frac{\cos(z)}{z}-\frac{\sin(z)}{z^2}$. How can this be derived? I know that the quotient rule was used to determine the derivative but must one not prove that the function is analytic before using standard derivation tools? Or can one say that the function is analytic because it is only expressed in terms of $z$? The function is analytic everywhere except at infinity. I am having a hard to seeing this as I thought it is analytic everywhere except at $z=0$.,"['complex-analysis', 'complex-numbers']"
2238434,"How do we find closed form of $\int_{0}^{1}{\sqrt{x^n\over 1-x^m}}\mathrm dx=F(n,m)?$","Consider this integral Motivated by this question $$\int_{0}^{1}{\sqrt{x^n\over 1-x^m}}\mathrm dx=F(n,m)\tag1$$
  Where $n\ge -1$ and $m\ge 1$ We note the following values for $F(n,m):$ $$F(-1,1)=\pi$$ $$F(1,1)={\pi\over 2}$$ $$F(1,3)={\pi\over 3}$$ Where $k $ is an integer, what are other values of $ n$ and $m$ that will give ${\pi\over k}$? I guess to answer this question we have to find the closed form for $(1)$","['integration', 'definite-integrals', 'calculus']"
2238437,Set theory composite function proof,"The question is:
Let $g:B \to C$ and $h:B \to C$ be functions. Assume that $g \circ f=h \circ f$ for every function $f:A \to B$. Prove that $g=h$. Here is my attempt $g=h$ if and only if $g(b)=h(b)$   $\forall b \in B$ Consider some $b \in B$ such that $f(a)=b$ for some $a \in A$. $g(b)=(g \circ f)(a)=(h \circ f)(a)=h(b)$ However, I have not assumed for an arbitrary $b$ so, it does not prove it. How should I fix the problem?","['elementary-set-theory', 'functions']"
2238457,The Normal Distribution,"I'd like to correct some of my misconceptions about The Normal Distribution. Firstly, I don't know how to interpret the following formula: $ \displaystyle Z = \frac{X-\mu}{\sigma}$ I was first introduced to this formula in a earlier chapter and it was introduced to me as the Z-Score. Apparently this was useful since it could find the number of standard deviations a value in a data set $X$ is from the mean. However, upon starting The Normal Distriubution it seems $Z$ is interpreted differently. From what I can understand, it seems $Z$ is treated as another data set with $\mu = 0$ and $\sigma=1$ which can be transformed onto by taking all data values in $X$ and applying $ \displaystyle \frac{X-\mu}{\sigma}$ I'm struggling to accept this concept and don't understand how the formula $ \frac{X-\mu}{\sigma} $ maps any normal distribution X to the Standard Normal distribution $Z$. Secondly, I understand the Standard Normal Distribution has the equation $ \displaystyle f(x) = \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} $. Suppose we wanted to find the $P(Z=1)$, I understand this can be found by just calculating $f(1)$, since the vertical axis outputs probabilities for $x\in X$. However, when looking for probabilities such as $P(Z\leq a)$, I don't understand how this equals the area under the curve over the interval $(-\infty,a]$. Wouldn't it be some type of sum of all the f(x) values between $(-\infty,a]$, so $f(a) + f(a-1)+f(a-2)+f(a-3) + ... $",['statistics']
2238492,Does $\lim f(x) = \frac{1}{\lim\frac{1}{f(x)}}$?,"I was trying to find $\lim_{z\rightarrow{0}} \frac{z^2}{\cos(z)-1}$ and the solution guide just told me to do $\lim_{z\rightarrow{0}} \frac{\cos(z)-1}{z^2} = -\frac{1}{2}$  and then take the reciprocal of this, which is $-2$ as my answer. Can we actually do that? Does this actually obey the limit laws?","['calculus', 'limits']"
2238494,$p^{th}$ roots of a field with characteristic $p$,"This is problem 10.9 from the book ""Error-Correcting Codes and Finite Fields by Oliver Pretzel"". The Question: Show that in a field of characteristic $p$, any element $\alpha$ has at most one $p$-th root $\beta$ (i.e., an element $\beta\in F$ with $\beta^p = \alpha$). Show further that if $F$ is finite, then every element has exactly one $p$-th root This my attempt at the second part of the question.
From Fermat's little theorem $\beta^{{p^n}-1} = 1$, where $p^n$ is the size of the field. Now multiplying both sides by $\beta$ we get $\beta^{p^n} = \beta$.
If there is $p$ elements then $n=1$ and we can see this is true for any non-zero element. 
For the general case, take the $p$-th root of both sides $\beta^{p^{n-1}} = \beta^{1/p}$ and we know from the multiplicative properties of a field if $\beta$ is a non zero element of the field then any multiple will be. The first part of the question I'm not sure where to begin.
Any help would be appreciated.","['finite-fields', 'abstract-algebra', 'field-theory']"
2238519,How to prevent loss of roots in equations?,"I was studying the Theory of Equations when I came across a line which was something like this: Cancellation of common factors from both sides of equation leads to a loss of root.
         For example, consider an equation 
  \begin{align*}
   x^2-2x&=x-2\\
   x(x-2)&=x-2\\
   x&=1
\end{align*} They then proceed on to explain that if, instead of factoring out $x-2$, they had simply subtracted $x-2$ and made the RHS zero, they would have two solutions, namely, $x=1$ and $x=2$. I have not provided the entire process.
However, as I started solving some problems, I came across a problem like this: $\dfrac{x^2+3x+2}{x^2-6x-7}=0$ Solve the above equation I was stuck in this problem, and didn't cancel out the factors that came upon factorisation of the numerator and denominator, mainly because I was apprehensive about cancelling out the roots.  But in the solution of this problem given with the answers, the solution was like this: Since the domain of the solution set is $\Bbb R - \{7,-1\}$ $\dfrac{(x+1)(x+2)}{(x+1)(x-7)}=0$ $x=-2$ But isn't this process leading to a loss of the solution of the equation
$\dfrac1{x-7}=0$? Please help. If you think this question does not meet the standards of this marvellous site, please inform before giving any downvote.","['linear-algebra', 'quadratics']"
2238533,Finite-dimensional Banach algebras with non-dense invertible groups,"It is easy to see that for any finite $n$, the invertible elements in the algebras algebras $\mathbb{C}^n$ (edowed with the maximum norm, say) and $M_n$ (all square matrices) are dense. Thus, by the Artin-Wedderburn all finite-dimensional C*-algebras have dense invertible groups. Is there an example of a unital, finite-dimensional Banach algebra where the invertible elements are not dense?","['functional-analysis', 'abstract-algebra', 'operator-algebras', 'banach-algebras']"
2238534,Is $x^{101} + 101x^{100} + 102$ irreducible?,How can I show that $x^{101} + 101x^{100} + 102$  is irreducible over $\mathbb{Z}[x]$? I was not able to apply Eisenstein's criterion (Which was my first thought as 101 is prime) because of the $102$.,"['number-theory', 'real-analysis', 'polynomials', 'irreducible-polynomials']"
2238535,How does exp(x) keep showing up in mathematics,"When I first learnt about e, I just treated it as another number (as far as I know, it doesn't even have a natural definition), but how is it that exp(x) is so important and keeps showing up at various places in mathematics?","['exponential-function', 'functions']"
2238562,"Let $G$ be a group. Show that $\forall a, b, c \in G$, the elements $abc, bca, cab$ have the same order.","Let $G$ be a group. Show that $\forall a, b, c \in G$, the elements
  $abc, bca, cab$ have the same order. I thought that my solution ($?$) was enough to show that $abc, bca, cab$ have the same order, but my teacher told that it isn't, so I don't know what else to do here. Attempt: Let $o(abc) = n$. Then $(abc)^n = e$ Therefore \begin{align}
abc(abc)^{n-2}abc &= e\\
(bc)\left[abc(abc)^{n-2}abc\right](cb)^{-1} &= e\\
(bca)^n &=e\\
bca(bca)^{n-2}bca &= e\\
(ca)\left[abca(bca)^{n-2}bca\right](ac)^{-1} &=e\\
(cab)^n &= e
\end{align} Therefore $(abc)^n = (bca)^n = (cab)^n = e$ What else am I lacking after this?","['abstract-algebra', 'group-theory']"
2238572,Why do elements of the same conjugacy class tend to have similar properties?,"I am trying to develop an intuitive understanding of why elements of the same conjugacy class tend to have similar properties. For example, the rotations form a conjugacy class in the dihedral groups, and so do the flips. It is clear in some particular cases why they have similar properties, but a general understanding eludes me. Similar matrices, for example, have similar properties because they are really the same matrix but under a different basis. I considered that it may be because elements of the same conjugacy class must have the same order, but I'm not sure this is enough, since elements of the same order do not always have similar properties.",['group-theory']
2238588,Meaning of matrix 'diag' operator with matrix arguments,"I came across this definition in a paper and can't figure out what it is supposed to represent: I understand that there is a 'diag' operator which when given a vector argument creates a matrix with the vector values along the diagonal, but I can't understand how such an operator would work on a set of matrices.","['matrices', 'notation', 'block-matrices', 'linear-algebra']"
2238661,I'm trying to find out if this limit exists,"I'm trying to calculate the following limit but I can't wrap my head,  around it. Can you guys give me some hints: $$\lim_{x\to0^+}\frac{{\int_0^{x^2}\sin{\sqrt{t}}}~ dt}{x^3}$$","['integration', 'limits']"
2238662,Do Hermite polynomials exist for negative integers?,"I recently asked a question about a differential equation, and received this as an answer. It included a Hermite polynomial of negative degree, namely $H_{-3}$ . I searched online and it seems as though these $H_n$ 's are only defined for $n\ge0$ , and for $n<0$ something else might used - parabolic cylinder functions . I say 'might' because I am not sure if they are actually the same. I have never heard of these, and so was hoping someone would know about them here. Can they be expressed in terms of erf or erfc ? I found another link which suggested they could. If so, how? To clarify I want to know if Hermite polynomials (or their equivalent) can be expressed for negative integers $n$ in some closed form expression (including erf ).","['ordinary-differential-equations', 'hermite-polynomials', 'error-function']"
2238669,Why is the argument on the right?,"In most of Mathematics in English, the argument $x$ of a function (or partial map) $f$ with value $y$ is written $y=f(x)$ instead of, at least in Semigroup Theory and Formal Languages & Automata, $xf=y$. Why? Thoughts: A function $f$ is a subset of the Cartesian product of its domain $D$ and its codomain $C$ ( i.e., $f\subseteq D\times C$ ) such that for each $d$ in $D$, $\lvert\{(d, c)\mid c\in C\}\rvert=1$. We write either $y=f(x)$ or $xf=y$ for $(x, y)\in f$. The $xf=y$ notation has its strengths: composition of functions is usually taken left-to-right with this notation, as read in English, instead of right-to-left, with the argument $x$ first, as, I suppose, one usually has when evaluating $y$; one can read it as ""$x$ through $f$ is $y$"" instead of, say, ""$y$ is $f$ at $x$""; and the left-to-right order respects that of going from the domain to the codomain. Let me know if you can think of any strengths & weaknesses of the $xf=y$ notation. I think history might have gotten the better of convention, that's all.","['math-history', 'notation', 'convention', 'functions', 'soft-question']"
2238680,"Let $f$ be a differentiable function with $|f'(x)|\leq1$ and $f(-3)=-3, f(3)=3$. Then find $f(0)$. [duplicate]","This question already has answers here : Find a function value given 2 points (2 answers) Closed 7 years ago . Let $f$ be a differentiable function with $|f'(x)|\leq1$ and $f(-3)=-3, f(3)=3$. Then find $f(0)$. I think this is mean value theorem problem. But I can't solve... help me please.","['derivatives', 'calculus']"
2238702,"Is there a way to decompose a polynomial $xy+f(x,y)$ into the product of two convergent power series $x+g(x,y)$ and $y+h(x,y)$?","Suppose we have a polynomial $xy+f(x,y)$, where $f(x,y)$ is a polynomial in $\mathbb C[x,y]$ whose the lowest degree term has degree at least 3. My question is, are we always able to decompose $xy+f(x,y)$ into the product of two convergent power series $x+g(x,y)$ and $y+h(x,y)$ in a neighborhood of $(0,0)$, where terms in $g$ and $h$ have order higher than 1? I have no idea about the convergence of power series with two or more variables. Any solution or reference will be appreciated! Please also note that it's not enough to simply decompose $xy+f(x,y)$ formally into two $x+g(x,y)$ and $y+h(x,y)$. We also need the convergence of $g$ and $h$!","['real-analysis', 'algebraic-geometry', 'complex-analysis', 'convergence-divergence', 'power-series']"
2238766,Prove complex function is not holomorphic using definition (without Cauchy-Riemann equations),"I'm just starting to learn complex analysis and currently reading the book ""Complex analysis"" by Stein and Shakarchi. Since the fact of the function being holomorphic appears to be pretty important, I'm starting with it. Here's the definition:
The function $f$ is holomorphic at the point $z_0 \in \Omega$ if the quotient $$\frac{f(z_0+h)-f(z_0)}{h}$$ converges to a limit when
$h \to 0$. Then we are given two examples. First of the function $f(z)=z$ being holomorphic on any open set in $\Bbb C$, and $f'(z) = 1$. And second of the function $f(z)=\bar z$ being not holomorphic . While I think I understand first example, I cannot see how the second will be different, how the fact of the complex conjugate changes the set up? Basically, why $f(z)=\bar z$ is not holomorphic by definition? 
I am aware of the Cauchy-Riemann equations, but I want to see the proof based on the definition (especially that the fact is claimed at this point in the book). I tried to search for an answer but didn't find anything particularly related. I also think I might miss some very basic concept. With all that said I will really appreciate your input.","['complex-analysis', 'holomorphic-functions', 'definition']"
2238779,The order of a rational section is zero at all but finitely many points,"Let $X$ be a Noetherian, regular and integral scheme with function field $K$. Moreover let $L$ be an invertible sheaf on $X$. Take a nonzero rational section $s\in \Gamma(L\otimes_{\mathcal O_X} K)$, then at each point $x\in X$ we can write $s_x=f_xe_x$ for $f_x\in K$ and $e_x\in L$. In other words we have chosen a local basis $\{e_x\}_{x\in X}$ for $L$.- If $x$ is a point of codimension $1$ we have a discrete valuation on $K$ associated to $x$, and we denote it simply by $v_x$. Now we can put: $$\operatorname{ord}_x(s):=v_x(f_x)$$ It is easy to show that $\operatorname{ord}_x(s)$ doesn't depend on the local basis chosen,  but I don't understand why $\operatorname{ord}_x(s)=0$ for all but
  finitely many points of codimension $1$. Can you please explain  it? Edit: Equivalently a rational section $s$ can be seen as a nonzero element of $L_{\eta}$ where $\eta$ is the generic point of $X$. Clearly for any $x\neq \eta$ we have $L_\eta=L_x\otimes_{\mathcal O_{X,x}} K$. The problem is still there. I don't understand why the order is zero almost everywhere.","['divisors-algebraic-geometry', 'sheaf-theory', 'algebraic-geometry', 'schemes', 'valuation-theory']"
2238838,Fourier Transform of a Derivative [duplicate],"This question already has answers here : Fourier Transform of Derivative (2 answers) Closed 7 years ago . I'm trying to prove that: $$F\,\{f'(x)\} = -i\omega F(\omega) \qquad (1) $$ where $\, F(\omega) = F\,\{f(x)\}$ This is my procedure so far: $$F\,\{f'(x)\} = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} f'(t)e^{i\omega t} dt$$ Integrating by parts I obtained: $$=\frac{1}{\sqrt{2\pi}} \big[ \space f(t) e^{i\omega t} \space \big|_{-\infty}^{\infty} -  \int_{-\infty}^{\infty} i\omega f(t)e^{i\omega t} dt \space \big]$$ Now, in order to (1) to be true I need to get: $$ f(t)e^{i\omega t} \space \big|_{-\infty}^{\infty} =0 \qquad (2)$$ I developed it and got the following: $$ f(\infty)e^{i\omega \infty} - f(-\infty)e^{-i\omega \infty } = f(\infty)e^{i\omega \infty}  - 0= f(t)e^{i\omega \infty} $$ I undarstand that the second term is $0$ , since f(t) must have a value to accomplish Dirichlet conditions (and $e^{-\infty} = 0$ ),  but I don't see how $ f(\infty)e^{i\omega \infty}$ is $0$ .","['derivatives', 'fourier-transform']"
2238843,Symmetric alpha stable distributions with $X_1+X_2+\cdots+X_n \stackrel{d}{=} n^{1/\alpha}X$ as definition,"Suppose $X,X_1,X_2,\ldots$ are independent and identically distributed random variables with a symmetric distribution $F$. We say $F$ is symmetric $\alpha$ stable  where $0<\alpha\le 2$, if
  $$X_1+X_2+\cdots+X_n \stackrel{d}{=} n^{1/\alpha}X, \ \ \mbox{for all} \ n\in\mathbb{N}$$ I wish to show that the characteristic function of $F$ is given by $\psi(t)=e^{-c|t|^{\alpha}}$. Breiman's probability book solve the general version of this problem (consider stable only, not symmetric). However, it uses infinite divisible distributions and lot of calculations. So I was wondering if there is a short argument for the symmetric case. My try: Note that using the equal in distribution relation $$\psi(t)=\psi\left(\frac{t}{n^{1/\alpha}}\right)^n$$
Enough to take $t>0$ as $\psi(t)=\psi(-t)$. Now taking limit we have
$$\psi(t)=\lim_{n\to\infty}\psi\left(\frac{t}{n^{1/\alpha}}\right)^n=\exp\left(-\lim_{x\to 0^+}\frac{1-\psi(tx^{1/\alpha})}{x}\right)=\exp\left(-t^{\alpha}\lim_{z\to 0^+}\frac{1-\psi(z)}{z^\alpha}\right)$$ So it is enough to show that $\displaystyle \lim_{z\to 0^+}\frac{1-\psi(z)}{z^\alpha}$ exists and is non zero. However, I don't have any clue how to proceed from here. Any help/suggestions.","['characteristic-functions', 'probability-theory', 'probability', 'probability-distributions']"
2238882,"Let $A\subset \mathbb{R}^n$ be a convex set. Show that $f:\mathbb{R}^n\to\mathbb{R}$ defined by $f(x) = d(x,A)$ is convex","Let $A\subset \mathbb{R}^n$ be a convex set. Show that
  $f:\mathbb{R}^n\to\mathbb{R}$ defined by $f(x) = d(x,A)$ is convex All I need to prove is that:
$$f((1-t)x+ty) = d((1-t)x+ty, A) \le (1-t)f(x)+tf(y) = (1-t)d(x,A)+td(y,A)$$ *for all $t\in[0,1]$ and $x,y\in \mathbb{R}^n$ By using the fact that $A$ is convex. A convex set is defined as: A set $C$ in $S$(vector space) is said to be convex if, for all $x$
  and $y$ in $C$ and all $t$ in the interval $[0, 1]$, the point $(1 −
 t)x + ty$ also belongs to $C$ and the distance from $x$ to a set $A$ is: $$d(x,A) = \inf \{d(x,a), a\in A\}$$ To begin I'd choose a generic $a\in A$ and try to work with it to show that $$d((1-t)x+ty, a) \le (1-t)d(x,a)+td(y,a)\tag{1}$$ I'm tempted to think something related to the triangular inequality $d(x,y)\le d(x,a) + d(a,y)$ by doing something like: $$d((1-t)x+ty, a)\le d((1-t)x+ty, b) + d(b,a)\tag{2}$$ for some $b\in \mathbb{R}^n$, but the right side of $(2)$ is already bigger than the right side of $(1)$. Also, I'm not even using the fact that $A$ is convex yet. Could somebody help me?","['general-topology', 'real-analysis', 'metric-spaces', 'elementary-set-theory']"
2238894,Are the closest vertices of two non-convex polygons also part of their closest edge distance,"In calculating the minimum distance (in $\mathbb R ^\mathbb2$) between two disjoint non-convex polygons whose convex hulls cannot be computed without intersecting, I found Toussaint and Bhattacharya 's paper for ""Computing the Minimum Distance Between Finite Planar Sets"". Given two sets of points $S1$ and $S2$ Compute the Voronoi diagram of $S$ = $S1 \cup S2$. From the dual of the Voronoi diagram, obtain the Gabriel graph of $S$. From among the edges of the Gabriel graph, determine the set of pairs of points
such that one is from $S1$ and the other from $S2$. Compute the distance between all pairs of points in Step 3. Determine the minimum of the distances computed in Step 4. This works well, but only gives me the minimum point distance; it has no notion of a polygon with a closed ring, which may have a vertex-edge distance which is closer than vertex-vertex. What I'm wondering is whether the point pair (p1, p2) obtained using the algorithm is always involved in the minimum distance. E.g. either p1 is the vertex and p2 is one of the vertices in the closest edge, or vice versa. Example of two polygons: In the above polygons, the minimum distance is between the vertex at the point of the right polygon, and the upper diagonal edge of the left polygon. The algorithm returns the vertex of the right polygon, and the left vertex of the edge of the left polygon. However, I don't know whether this is always the case for disjoint non-convex polygons.","['polygons', 'computational-geometry', 'geometry']"
2238915,What are those groups generated by two cycles?,"Let G be generated by two cycles. One ihas order $n$ and moves the points $\{1,\ldots,n\}$ and the other has order $n-1$ and moves the points $\{1,\ldots,n-1\}$. I found following examples:
$$
\begin{array}{ccl}
n = 3 &:& S_3 \\
n = 4 &:& S_4 \\
n = 5 &:& S_5,\ C_5 \rtimes C_4\\
n = 6 &:& S_5,\ S_6 \\
n = 7 &:& S_7,\ C_7 \rtimes C_6\\
n = 8 &:& S_8,\ \operatorname{PSL}(3,2) \rtimes C_2\\
n = 9 &:& S_9 \\
n = 10 &:& S_{10} \\
n = 11 &:& S_{11},\ C_{11} \rtimes C_{10} \\
\end{array}
$$ Is there a way to predict the structure of these groups for arbitrary $n$? Edit Changed for $n=6 :S_5$ into $PGL(2,5)$ in accordance with the article mentioned in the answer from @Geoff Robinson.","['finite-groups', 'permutations', 'group-theory']"
2238936,Finding an integral using the Laplace transform,"I have to evaluate the following integral by using the Laplace transform: $$\int_0^\infty \frac{\sin^4 (tx)}{x^3}\,\mathrm{d}x.$$ How am I supposed to approach this question by using the Laplace transform?","['laplace-transform', 'integration', 'calculus']"
2238953,Linear Independence of Matrices with Powers,"I can't seem to figure this out, could someone explain the properties of linear independence when it comes to a matrix to the power of something. The question I'm having trouble with is: Let $A$ be an $n \times n$ matrix and $x$ is an element of $\mathbb{R}^n$ be a non zero vector. Assume that $x$ is an element of $\operatorname{null}(A^{3})$ but $x$ is not an element of $\operatorname{null}(A^{2})$. Show that the set $\{x,Ax, A^{2}x\}$ is linearly independent. Thanks for any help!","['matrices', 'independence', 'linear-algebra']"
2238963,Is dot product the only rotation invariant function?,"I am looking for rotation invariant scalar functions $f(x,y): x,y \in R^3$ that are not some scalar function over the dot product (or norm), i.e. $ f \neq g(x\cdot y, \Vert x \Vert, \Vert y \Vert ) $ Do they exist ? Edit: Edited to clarify that the norm is just another form of the dot product, and the norm being rotation invariant is really just the dot product being invariant.","['matrices', 'geometric-invariant', 'rotations', 'functions']"
2239108,Reference request for text containing proof of quadratic reciprocity law over function fields,"While I have found a number of sources online (e.g. this) that prove the law of quadratic reciprocity for polynomials over finite fields, I am looking for a book on the subject or at least has a section dedicated to the topic. Ideally, there would be some exposition and motivation for the theorem.","['number-theory', 'reference-request', 'quadratic-reciprocity']"
2239128,"Significance of ""virtual"" zeroes/poles introduced by the Pade expansion of $e^{sT}$","Let $s$ be a complex parameter and $T$ a real. The function $e^{sT}$ is entire. It can also be expanded in a Pade approximant via $$
e^{sT} = \frac{e^{sT/2}}{e^{-sT/2}} = \frac{1+ sT/2 + \frac{(sT/2)^2}{2!}+ \cdots}{1 - sT/2 + \frac{(sT/2)^2}{2!} - \cdots},
$$
which upon truncation to first order yields the bilinear mapping $$
\frac{2+sT}{2-sT}.
$$ Approximations of the exponential function by bilinear mappings are found frequently in digital signal processing, linearized analysis of delay systems, and other applications in systems theory, where they are known as ""Tustin's method"" or sometimes ""Tustin's approximation"". In all of these applications, poles and zeros introduced into a system transfer function are important factors which determine system stability, transient characteristics, and frequency response. The character of a system, say $e^{sT}H(s)$, where $H(s)$ is an arbitrary rational transfer function, may in fact be changed considerably if the exponential factor is approximated by Tustin's method, despite the fact that numerically the two are close when the latter is defined and nonzero, since the introduction of the pole-zero combination affects relative stability margins and the shape of the system's frequency response. Clearly the pole-zero combination is ""virtual"" in the sense that they do not arise from physical considerations, but it is unclear to me what their exact significance is.","['complex-analysis', 'control-theory', 'dynamical-systems', 'linear-control']"
2239140,"Solution verification: Let $f$ be a function from $(0,1)$ to $(0,1)$ such that $f(x)<x$ for every $0<x<1$. Must $f^n(x)\rightarrow 0$ for some $x$?","Let $f$ be a function (need not be continuous) from $(0,1)$ to $(0,1)$ such that
  $f(x)<x$ for every $0<x<1$. Must it be the case that for some $0<x<1$ we have $f^n(x)\rightarrow
 0$ as $n\rightarrow\infty$? My attempt: Let the interval $(2^{-k},2^{-k+1}]=I_k,$ $(I_1=(\frac12, 1))$ then take for $x\in I_k$ $$f(x)=\frac{x-2^{-k}}2+2^{-k}=\frac12(x+2^{-k}).$$ We can verify that $f(x)<x\space\forall x\in(0,1)$ and that if $x\in I_k$ then $f(x)\in I_k$ and so we conclude that the answer is 'no' as $\forall x\in(0,1)\space\exists\epsilon>0$ such that $f^n(x)>\epsilon\space \forall n\ge1$. In particular if $x\in I_k$ then $f^n(x)>2^{-k}\space\forall n$. Is this correct? I was really expecting the answer to be 'yes' and this sort of solution is not typical for the questions I am doing. I also have a knack of making lots of mistakes with these types of questions. (I am also looking to improve my solution writing so if this is correct feel free to criticise how I have written it up.) Thanks in advance","['real-analysis', 'functions', 'special-functions', 'sequences-and-series', 'analysis']"
2239150,Triangles of the form $a^n+b^n=c^n$,"Let $\Delta$ be a triangle with (real) sides length $a,b,c>0$. From this question , we know that $$a+b=c \qquad \iff \qquad \Delta \text{ is a straight line}$$ Now, we know from the Pythagorean theorem that 
  $$ a^2+b^2=c^2 \qquad \iff \qquad \Delta \text{ is a right triangle}$$
  I'm wondering if it is possible, for a fixed $n>2$, to have a geometric characterization of the triangles which satisfy $a^n+b^n=c^n$, that is:
  $$ a^n+b^n=c^n \qquad \iff \qquad \Delta \text{ is ?}.$$ Notes: For $n>1$, and any $a,b>0$ there exists always a non-degenerated triangle $\Delta$ with $c= (a^n+b^n)^{1/n}$. It seems that for the case $n=3$, things are already more complicated. All the following triangles satisfy 
$$a^3+b^3=c^3.$$","['triangles', 'geometry']"
2239160,"""Explosion in finite time"" in ODE and manifolds","We know that, if $F:\mathbb{R}^n\to\mathbb{R}^n$ is regular enough, then a curve $c:t\to\mathbb{R}^n$ solution of the ODE 
$$\left\{\begin{array}{l}c'(t)=F(c(t))\\ c(0)=x\in\mathbb{R}^n\end{array}\right.$$
check the following alternative: $c$ is defined on whole $\mathbb{R}$, or $c$ is defined on $]t_\star,t^\star[$, with $t^\star<+\infty$, and for all compact $K\subset\mathbb{R}^n$ it exists a time $t_K\in]t_\star,t^\star[$ such that for all $t\in[t_K,t^\star[$, $c(t)\in \mathbb{R}^n\setminus K$, or ""same as 2. with $t_\star$"". My first question is: does this result have a common name in english litterature? In France we call it ""explosion in finite time"" or ""getting out of all compact sets"", but these seem not to be actual translations. My second question is: does this alternative hold in differential manifold theory? More precisely, if I have a $\mathcal{C}^\infty$ vector field $X$ defined on a smooth manifold $M$, does a solution $c:t\to M$ of 
$$\left\{\begin{array}{l}c'(t)=X_{c(t)}\\ c(0)=p\in M\end{array}\right.$$
check exclusively one of the three conditions? If yes, I would be glad to have a sketch of proof/a reference. Thank you all for your attention!","['smooth-manifolds', 'ordinary-differential-equations', 'translation-request']"
2239167,Geodesics on Cylinders,"I have a question about Geodesics on Cylinders and think I have the right answer but am unsure. The question reads: Let $C_r:=[(x,y,z)\in\mathbb{R}^3: x^2+y^2=r]$ be the infinite cylinder of radius $r$. Show that $C_{r_1}$ is isometric to $C_{r_2}$ iff $r_1=r_2$. Now I understand the logic behind this question I think. An isometry preserves geodesics, and because if you intersect a plane parallel to the axis of the cylinder with this cylinder, you get a curve $C$, which is just a circle, that is a geodesic. Now, if the radius between the two cylinders are different, the smaller circle would lie inside of the bigger cylinder, thus not lying on the surface and definitely not a geodesic. Is this okay to write? Or do I have to explain it mathematically?","['isometry', 'differential-geometry', 'surfaces', 'geodesic']"
2239189,How to calculate inverses in the Nottingham Group,"The Nottingham group is defined as the group of formal power series over a unital ring $R$ under the operation of substitution, giving elements of the form $t(1+\sum_{i=1}^{\infty}a_{i}t^{i})$ with the coefficients $a_{i} \in R$. The fact that this indeed forms a group was proven by S Jennings in his paper ""Substitution groups of formal power series"". I'm particularly interested how one calculates the inverse of a given element in this group, as the proof in Jennings shows that the coefficients of the inverse are defined inductively in terms of the element you want the inverse of. The reason this confuses me is the fact that you must also subtract the sum of some polynomials, labelled $\phi_{s}$ in the paper, and there doesn't seem to be any indication of how to calculate what these are. For example, even taking some ""nice"" element such as $e_{1} := t(1+t)$ doesn't seem to have an easy solution for the inverse. We can define $a_{1} = -1$ for $e_{1}^{-1}$, but $t(1-t)$ on it's own doesn't work and requires further terms in its formal power series. Here is a link to the paper I'm talking about: https://cms.math.ca/10.4153/CJM-1954-031-9 The result is labelled (1.1.3).","['abstract-algebra', 'profinite-groups', 'group-theory', 'power-series']"
2239198,Metrizability of weak topology on separable Hilbert space,"The weak* topology on the dual of a separable space is metrizable.  On a Hilbert space, the weak topology and the weak* topology coincide, and the dual of the Hilbert space is itself.  Thus, on a separable Hilbert space, the weak topology is metrizable. What's the error in the reasoning here?  There must be an error because of the example $\{\sqrt{n} e_n \}$ being a set whose weak closure includes $0$ but no sequence in the set converges weakly to $0$.",['functional-analysis']
2239203,Why does $\lim\limits_{x\to\infty}(x!)^{1/x}\neq 1?$ [duplicate],"This question already has answers here : Evaluate $\lim_{x\to \infty}\ (x!)^{1/x}$ (4 answers) Closed 7 years ago . Why does $\lim\limits_{x\to\infty}(x!)^{1/x}\neq 1?$ As far as I know, anything to the power of $0$ is $1$. We have a factorial raised to $1/\infty=0$, but the limit is not $1$? I don't even know what the limit is. But it seems like infinity? Why is this?","['factorial', 'exponential-function', 'calculus', 'limits']"
2239238,Why does parenthesis before exponents not apply to squaring a binomial?,This must be a stupid question with an obvious answer that is hidden from me. No one I have found even mentions a conflict. lets say I want to find the square $(2 + 3)^2 = 2^2 + 2\times2\times3 + 3^2$ Why do I not simplify first? Parenthesis first? Edit To clarify for future views. The main confusion was why is the above problem always (in my limited experience) solved one way in situation A and another way in situation B...Z without ever being noted of why the method used is preferred for the given situation (again in my limited experience). Thanks for the clarification. I think I get it now. In some cases it is arbitrary because the order is divorced from the output. In some cases you have an variable that you cannot add or subtract to simplify so you must use the distributive property. I suppose also you may not simplify to show the trinomial pattern of the output produced by the distributive property (in a pedagogical situation).,['algebra-precalculus']
2239261,Solve $x^2+y^2+z^2+xy+yz+zx = 2w^2$,"Solve in integers the equation $$x^2+y^2+z^2+xy+yz+zx = 2w^2.$$ A trivial solution to the equation is $x = y = z = w = 0$. We can rewrite the given equation as $$(x+y)^2+(x+z)^2+(y+z)^2 = 4w^2.$$ I then thought about doing a substitution, but didn't see how to do it without becoming computational. How can we continue?","['number-theory', 'diophantine-equations']"
2239269,Does the series $\sum_{n=1}^{\infty}\frac{\sin(\cos(n))}{n}$ converge?,"I was doing some exercises and this one just stunned me.
I had to study the convergence of this serie: $$\sum_{n=1}^{\infty}\frac{\sin(\cos(n))}{n}$$ I tried alot of diffrent things, but I got no where.
Can anyone please help me with an idea or a clue just to start ? Thanks in advance !",['sequences-and-series']
2239312,Sets/Venn diagrams question,"In a group of 50 students at a summer school, 15 play tennis, 20 play cricket, 20 swim and 7 students do nothing. 3 students play tennis and cricket, 6 students play cricket and swim, 5 students play tennis and swim. How many do all three sports? Here's what I have: $n(\mathbb{U}) = 50$ $n(T) = 15$ $n(C) = 20$ $n(S) = 20$ $n(T\cap C) = 3$ $n(C\cap S) = 6$ $n(T\cap S) = 5$ $n(T\cup C\cup S)' = 7$ $n(T\cap C\cap S) = x$ And $n(\mathbb{U}) = n(T\cap C) + n(C\cap S) + n(T\cap S) + x$ $+ (n(T) - (T\cap C) - (T\cap S) - x)$ $+ (n(C) - (C\cap T) - (C\cap S) - x)$ $+ (n(S) - (S\cap T) - (S\cap C) - x)$ $+ (T\cup C\cup S)'$ Therefore $50=3+6+5+x$ $+(15-3-5-x)$ $+(20-3-6-x)$ $+(20-6-5-x)$ $+7$ Simplified: $-2x+41 = 43$ $-2x = 2$ $x=-1$ This answer isn't right because you can't have a negative amount of things. The actual solution to this problem is 2, however this doesn't make sense to me, as: $3+6+5+2$ $+(15-3-5-2)$ $+(20-3-6-2)$ $+(20-6-5-2)$ $+7$ $=44$ $\ne 50$","['combinatorics', 'elementary-set-theory']"
2239357,An increasing sequence of simple functions approximates a measurable function,"The following is from page 31 of Stein and Shakarchi's Real Analysis . My question is about an aspect of the proof of the following theorem. Theorem 4.1 Suppose $f$ is a non-negative measurable function on $\mathbb R^d$. Then there exists an increasing sequence of non-negative simple functions $\{\varphi_k\}_{k=1}^\infty$ that converges pointwise to $f$, namely,
  $$
\varphi_k(x) \le \varphi_{k+1}(x)\quad\text{and}\quad\lim_{k\to\infty}\varphi_k(x)=f(x),\ \text{for all $x$.}
$$ Proof. We begin first with a truncation. For $k\ge 1$, let $Q_k$ denote the cube centered at the origin and of side length $k$. Then we define
  $$
F_k(x) = \begin{cases}
f(x) & \text{if $x\in Q_k$ and $f(x)\le k$,} \\
k & \text{if $x\in Q_k$ and $f(x)> k$,}\\
0 & \text{otherwise.}
\end{cases}
$$
  Then $F_k(x)\to f(x)$ as $k$ tends to infinity for all $x$. Now, we partition the range of $F_k$, namely $[0,k]$ as follows. For fixed $k,j\ge 1$, we define
  $$
E_{\ell,j}=\left\{x\in Q_k:\frac{\ell}{j}<F_k(x)\le\frac{\ell+1}{j}\right\},\quad\text{for}\ 0\le\ell<kj.
$$
  Then we may form
  $$
F_{k,j}(x) = \sum_{\ell=0}^{kj-1}\frac{\ell}{j}{\large{\chi_{E_{\ell,j}}}}(x)
$$
  [where $\large{\chi_{E_{\ell,j}}}$ is the indicator function of $E_{\ell,j}$]. Each $F_{k,j}$ is a simple function that satisfies $0\le F_k(x)-F_{k,j}(x)\le 1/j$ for all $x$. If we now choose $j=k$, and let $\varphi_k = F_{k,k}$, then we see that $0\le F_k(x)-\varphi_k(x)\le 1/k$ for all $x$, $\color{red}{\underline{\color{black}{\text{and $\{\varphi_k\}$ satisfies all the desired properties.}}}}$ I do not see why $\varphi_k(x)\le\varphi_{k+1}(x)$ for all $x$. Can someone explain that?","['real-analysis', 'measure-theory']"
2239391,Why is an étale $k$-algebra a finite product of separable field extensions?,"Let $k$ be a field, and let $k\to A$ be an étale ring map, then I wish to prove that $A$ is a finite product of separable field extensions of $k$. I can prove the converse, by using the definition of étale-ness in which we write $A=k[x_1,...,x_n]/I$ and consider $k\to A$ to be étale precisely when the differential map $$I/I^2\to\bigoplus_{i=1}^n A\mathrm{d}x_i$$ is an isomorphism, but this doesn't seem to be a useful definition for this direction.","['algebraic-geometry', 'commutative-algebra']"
2239424,$6x^5+14x^3-21x+35$ is irreducible in $\mathbb{Q}[x]$,"I am trying to show that $p(x)=6x^5+14x^3-21x+35$ is irreducible in $\mathbb{Q}[x]$. I would like to be able to use Eisenstein's Criterion which states: Let $P$ be a prime ideal of the integral domain $R$ and let $f(x)=x^n+a_{n-1}x^{n-1}+\ldots+a_1x+a_0$ be a polynomial in $R[x]$, where $n \geq 1$. Suppose that $a_{n-1}, \ldots, a_1 \in P$ and suppose that $a_0$ is not an element of $P^2$. Then $f(x)$ is irreducible in $R[x]$. The problem is that the polynomial $p(x)$ that I am given does not have a leading coefficient of 1. Is there another approach I can take? I would like to be able to write a self-contained proof, if possible.","['irreducible-polynomials', 'abstract-algebra', 'ring-theory']"
2239450,Why $\frac {a}{b} = \frac {c}{d} \implies \frac {a+c}{b+d} = \frac {a}{b} = \frac {c}{d}$?,"In geometry class, this property was quickly introduced: $$\dfrac {a}{b} = \dfrac {c}{d} \implies \dfrac {a+c}{b+d} = \dfrac {a}{b} = \dfrac {c}{d} $$ It usually avoids some boring quadratic equations, so it's useful but I don't really get it. Seems trivial but I can't prove. How to demonstrate it?","['algebra-precalculus', 'fractions']"
2239510,"Sequence of measurable functions $\{g_n\}_{n=1}^{\infty}$ such that $\lim_{n\to\infty}\int_{0}^{1}f(t)g_n(t)dt=\int_0^1f(t)dt$ whenever $f\in C[0,1]$","I've been working on some old real analysis qualifier problems. One I'm having trouble with is Prove that there exists a sequence of measurable functions $g_n$ on $[0,1]$ such that $g_n(x)\geq 0$ for any $x\in [0,1]$; $\lim_{n\to\infty}g_n(x)=0$ a.e.; For any continuous function $f\in C[0,1]$, $$
\lim_{n\to\infty}\int_{0}^{1}f(x)g_n(x)dx=\int_{0}^{1}f(x)dx.
$$ I don't quite know how to approach this problem. My first thought was to see if I can come up with something assuming $f$ is a simple function -- but I couldn't think of anything. My next attempt is to just assume that $f$ is the characteristic function of interval, but even this proves to be a challenge; I'm thinking of something having to do with delta functions. Whatever the sequence is, it must have $\int_0^1\sup_n|g_n(x)|dx=\infty$; else Lebesgue's dominated convergence theorem would show that (3) would always be zero. Here's another thought I had in mind. The map defined by $T(f)=\int_0^1f(x)dx$ is an element of $C[0,1]^*$. Are there any results that say something to the effect of ""functionals of the form $f\mapsto \int_0^1fg dx$ are dense in $C[0,1]^*$""? ""Or integration against a measure which absolutely continuous to Lebesgue measure is dense in $C[0,1]^*$""? Thanks in advance.","['measurable-functions', 'real-analysis', 'lebesgue-integral', 'measure-theory']"
2239514,Prove $H^1(\Omega)$ is complete given that $L_2(\Omega)$ is complete,"Given that $L_2(\Omega)$ is complete, prove that $H^1(\Omega)$ is complete. Hint: Assume that $\|v_j-v_i\|\to 0$ as $i,j\to\infty$. Show that there are $v,w_k$ such that $\|v_j-v\|\to 0,\|\partial v_j/\partial x_k -w_k\|\to 0$ and that $w_k=\partial v/\partial x_k$ in the sense of weak derivative. So far I did, Since $L_2(\Omega)$ is complete we get $$\|v_j-v\|_{L_2}\to 0$$ where $v\in L_2(\Omega).$ Let $\{v_i\}$ be a Cauchy sequence in $H^1(\Omega)$, then 
\begin{align*}
&\|v_j-v_i\|_{H^1}\to 0\\
\implies\,&\|v_j-v_i\|_{L_2}+\|\nabla v_j-\nabla v_i\|_{L_2}\to 0.
\end{align*} which gives us $$\|v_j-v_i\|_{L_2}\to 0$$
and $$\|\nabla v_j-\nabla v_i\|_{L_2}\to 0.$$ Now I am stuck, any help would be greatly appreciated. Thanks in advance.","['partial-differential-equations', 'functional-analysis', 'weak-derivatives', 'lp-spaces', 'sobolev-spaces']"
2239525,Elementary proof of $f>0$ implies $\int f>0$?,"The question (Abbott, Understanding Analysis 2ed, 7.4.4) is: Show that if $f(x)>0$ for all $x\in[a,b]$ and $f$ is integrable, then $\int_a^b f>0$. I can show it using Baire's theorem (the sets $E_n=\{x: f(x)>1/n\}$ can't all be nowhere dense...), but that's optional in this book, and the Lebesgue characterization of integrable functions is two sections ahead.  Is there a way using not much more than the definition of Riemann integral?",['real-analysis']
2239530,Using Feymann's trick in Advanced Integration,"Introduction: $\def\d{\mathrm{d}}$A common integration technique is to employ Feymann's trick. Assume that we have the following function of two variables$$\int_a^bf(x,y)\, \d x$$Then we can differentiate with respect to $y$ provided that $f$ is continuous and has partial continuous derivative on a chosen interval$$F'(y)=\int_{a}^bf_y(x,y)\, \d x$$But using this approach may be difficult because you have to think a lot to get the required answer. Since most integrals are in one variable, you will have to introduce a second variable and assume it is a function with two variables. Example: A worked example of integrating $\dfrac{x^2-1}{\ln x}$ with respect to $x$.$$\int_0^1\frac {x^2-1}{\ln x}\, \d x=?\tag1$$Since we can get a natural log when we diffentiate an exponential function $F(a)=2^a\implies F'(a)=\ln a\cdot 2^a$. Applying this to our problem, we have$$F(a)=\int_0^1\frac {x^a-1}{\ln x}\, \d x$$And taking the partial derivative with respect to $a$ gives$$F'(a)=\int_0^1\frac {\partial}{\partial a}\left(\frac {x^a-1}{\ln x}\right)\, \d x=\int_0^1x^a\, \d x=\frac 1{a+1}\tag{2}$$Integrating with respect to $a$ gives us$$F(a)=\ln(a+1)+C$$Set $a=0$ to find the value of the constant and we get $C=0$. Therefore, it implies that$$\int_0^1\frac {x^a-1}{\ln x}\, \d x=\ln(a+1)\implies\int_0^1\frac {x^2-1}{\ln x}\, \d x=\ln 3\tag3$$ Questions: How do you know what to set $a$ as? In this case, they set the exponent of $x$ as $a$. Why? What was their reasoning? Isn't the derivative of $2^a=\ln 2\cdot 2^a$, not $\ln a\cdot 2^a$? Why did they set $a=0$? Why not $a=1$, or $2$? How do you integrate this using Feymann's trick?$$\int_0^\infty\frac {\sin x}x\, \d x\tag4$$",['integration']
2239563,Intuition for $E[E[X\vert \mathcal{H}]\vert \mathcal{G}]\not= E[X\vert \mathcal{G}]$ where $\mathcal{G}\supset\mathcal{H}$?,This is just the general Law of iterated Expectations but with the larger sigma-algebra in the outer (instead of the inner) expectation. I am wondering if anyone can provide some intuition for why this is not true? Mathematically I am thinking about it as the outer expectation must be equal to the inner expectation (which equals $E[X]$) for all subsets of $\mathcal{G}$. But this leaves some subsets of $\mathcal{H}$  where the inner and outer expectation can disagree (and thus the outer one is not necessarily $E[X]$ on those sets). Thanks.,"['probability-theory', 'conditional-expectation']"
2239604,Finding a matrix $Q \in \mathbb{R}^{d\times r}$ such that $Q^\top Q=I_r$ and $(QQ^\top)_{ii}=h_{ii}$,"Given $\{h_{ii}\}_{i=1}^d$, where $h_{ii}\in[0,1],$ and $\displaystyle\sum_{i=1}^d h_{ii}=r<d,$ does there exist a matrix $Q\in\mathbb{R}^{d\times r},$ s.t. $$Q^\top Q=I_r, \qquad (QQ^\top)_{ii}=h_{ii}?$$","['matrices', 'linear-algebra']"
2239654,Do continuous functions preserve Lebesgue Measure to any degree?,"So I'm trying to prove a homework problem, and as a lemma I'd like the following to be true, although I'm not sure it is: If $f$ continuous and $\mu(f(A)) = c >0$ then for all continuous and $\|g-f\|< \epsilon$ then $|\mu(g(A)) - c|<\epsilon$. If necessary, we can take $A$ compact (it is, in my problem). Moreover, the final restriction isn't necessary for my problem. If $\mu(f(A))>0$ then I'd like to be able to show that in some ball around $f$ (in the standard function norm), $\mu(g(A))>0$. This would suffice for my purposes. Here we take $\mu$ to be the Lebesgue measure and functions being continuous mappings of $[0,1]$ onto $[0,1]$. I tried something, but it ended up being a false proof. It used compactness to extract a finite number of intervals covering $f(A)$ and $g(A)$, but I couldn't send the differences between their measures to zero uniformly.","['continuity', 'lebesgue-measure', 'measure-theory']"
2239670,Sum of $X\sim\chi^2_ \nu$ and $Y\sim\chi^2_ k$ and gamma function,"Suppose $X\sim\chi^2_ \nu$ and $Y\sim\chi^2_ k$ are independent. (a) Show that $X+Y\sim\chi^2_{\nu+k}.$ (b) Additionally, find the value of
$$\int_0^1u^{\frac{\nu}{2}-1}(1-u)^{\frac{k}{2}-1}du$$
as a ratio of Gamma functions. This formula was discovered by Euler. I was able to solve part (a) and this is my result. Proof: If $X$ is $\chi^2_\nu$ distributed then we can express $X$ as $X=A_1^2+\cdots+A_\nu^2$ where $A_1,\ldots,A_\nu$ are $N(0,1)$.
Similarly, if $Y$ is $\chi^2_k$ distributed then we can express $Y$ as $Y=B_1^2+\cdots+B_k^2$ where $B_1,\ldots,B_k$ are $N(0,1)$. These two statements can be made due to the definition of $\chi_\nu^2$ with $\nu$ degrees of freedom.
Now, we know that the sum of two independent random variables preserves independence, therefore, adding $X+Y$ we obtain 
$$X+Y=A_1^2+B_1^2+\cdots+A^2_\nu+B_k^2$$
We can conclude that
$$X+Y \sim \chi^2_{\nu+k}$$ However, I am uncertain as to how to compute (b). I have seen and computed Gamma integrals but they have been in the conventional form with an exponential term and $1$ variable. This integrals contains $2$ variables and so I am uncertain how to go about it.","['statistics', 'probability', 'gamma-function', 'probability-distributions']"
2239734,Is it ever possible to fully avoid Cartesian coordinates?,"I think this question is more philosophical than mathematical, though I may be wrong, probably it is just a stupid lack of understanding. Anyway, if you don't mind, please read the question carefully to get the sense of what I am asking. It is a bit difficult to express. Tensors and differential geometry consider oblique (non-orthogonal) and even curvilinear ""axes"" or basis vectors.  But, the basis vectors themselves must be expressed somehow.  Does this require going back to some underlying ""master"" orthogonal (Cartesian) coordinate system? Here are two examples of my puzzle. Example 1. A vector $\mathbf{v}$ can be written in terms of Cartesian coordinates as
$$
   \mathbf{v} = v^k \mathbf{e}_k
$$
where $e_k$ are the regular basis vectors (in the 3D case, (1,0,0), (0,1,0), (0,0,1)).
A vector can also be written in terms of an arbitrary non-orthogonal system as for example
$$
   \mathbf{v} = w^k \mathbf{u}_k
$$
Suppose that $w^k$ are given. To know the vector, we have to know what the vectors $\mathbf{u}_k$ are, and these would either have to be specified in terms of an underlying Cartesian coordinate system, for example
$$
   \mathbf{u}_2 = (0.3, 0.799, -0.1)
$$
or in terms of some other coordinate system, which in turn(!) (possibility of infinite regress here) would need to be specified eventually in terms of the Cartesian coordinate system ? I suppose if the other coordinate system is a physical given thing (lines drawn by aliens in the desert)
then one can avoid the issue. Example 2. In Susskind's Einstein's General Theory of Relativity | Lecture 5 https://www.youtube.com/watch?v=WtPtxz3ef8U at around 18:30 he describes why the derivative of the components of a tensor are not themselves a tensor, giving the example of a field of constant-direction-and-constant-magnitude vectors. The derivatives of components of these vectors w.r.t. cartesian axes are zero, but the derivatives w.r.t spatially varying axes are non-zero. Here we have some vectors, which appear to be pointing in the same direction, and measure them using some underlying axes, which certainly appear to be curving.  But how do we know that the vectors are pointing in the same direction and the axes are curving and not the other way?  In everyday live, of course I can see that what was drawn on the white board was straight/curving, but that is not a scientific answer. Don't we need to measure with respect to some other coordinate system,  and wouldn't that one be...(eventually)...Cartesian? Note, it sounds that someone in the class was possibly asking this question around 18:30, but the professor closed the question. EDIT: Seeing the first 3 answers, it is clear that I failed to explain the question... or else the question is just nonsense. I fear that I am missing something very fundamental! Surprising however. The idea of a basis is something I have successfully used in various exercises and is second nature at this point.","['tensors', 'differential-geometry']"
2239773,A question about Frechet derivative,"Let $X,Y$ be real Banach spaces. Define $F: X \times Y \rightarrow \mathbb{R}$ as a functional mapping $\left(u,t\right) \mapsto F\left(u,t\right)$ and $F_u\left(u,t\right), F_t\left(u,t\right)$ be partial Frechet derivatives of $F.$ I want to know what are the relationships between $DF\left(u,t\right)$ and $ F_u\left(u,t\right), F_t\left(u,t\right)$. Can you show me? By the Riesz representation theorem, it can be represented in the form of an
inner product; denoting the representing element by
\begin{equation}
F_u\left(u,t\right)h=\langle \text{grad}F\left(u,t\right),h \rangle
\end{equation}
and 
$$F_t\left(u,t\right)l=\langle \text{grad}F\left(u,t\right),l \rangle$$ 
and 
$$DF\left(u,t\right)w=\langle \text{grad}F\left(u,t\right),w \rangle$$. I don't understand what $F\left(u,t\right)$ in above identities mean does?","['partial-derivative', 'calculus', 'multivariable-calculus', 'calculus-of-variations', 'frechet-derivative']"
2239790,$x^{p-1}+x^{p-2}+\cdots+x+1$ is irreducible over $\mathbb{F}_2$ if and only if $2$ generates $\mathbb{F}_p^{*}$,"I need to show the equivalence: $x^{p-1}+x^{p-2}+\cdots+x+1$ is irreducible over $\mathbb{F}_2$ if and only if $2$ generates $\mathbb{F}_p^{*}$. Maybe I should clarify the meaning of ""generates"": $\mathbb{F}_p^{*}$ is a cyclic group (as the mulplicitive group of a finite field) composed of the elements $1, 2, ...., p-1$, and $2$ generates $\mathbb{F}_p^{*}$ means that all elements are its powers. I tried what seems to be, intuitively the easy direction. Here's my attempt: Suppose $1+x+...+x^{p-1}$ is irreducible. Let $\alpha\in \overline{\mathbb{F}}_2$ be a root of the polynomial. Then $[\mathbb{F}_2[\alpha]: \mathbb{F}_2]=p-1$ thus $\mathbb{F}_2[\alpha]\cong \mathbb{F}_{2^{p-1}}$. Now I'm stuck. I think it might have something to do with the multiplcitive group of this field, since it's order is $2^{p-1}-1$ who's divisable be $p$, but maybe I'm wrong. Any ideas?","['finite-fields', 'abstract-algebra', 'irreducible-polynomials']"
2239795,"Limit of $\int _{\frac{1}{n}}^n \frac{\arctan x}{x^2 + 2ax + 1}\,\mathrm{d}x$ without Taylor expansion","$\def\d{\mathrm{d}}$Consider $$I_n(a) = \int _{\frac{1}{n}}^n \frac{\arctan x}{x^2 + 2ax + 1}\,\d x, \quad n\ge 1,\ a \in [0, 1)$$
and evaluate
$$\lim _{n \to \infty} I_n(a). \quad a \in (0, 1)$$ One thing I have found it that $$\int _{\frac{1}{n}}^n \frac{\arctan x}{x^2 + 2ax + 1}\,\d x = \int _{\frac{1}{n}}^n \frac{\arctan \frac{1}{x}}{x^2 + 2ax + 1}\,\d x$$ Also, I know $$\arctan x + \arctan \frac{1}{x} = \frac{\pi}{2}. \quad \forall x > 0$$ I didn't make it further with that method, so I tried this: $$\frac{1}{n} \le x \le n,\\
\arctan \frac{1}{n} \le \arctan x \le \arctan n.$$ So I deduced $$I_n(a) \le \frac{\pi}{2} \int _{\frac{1}{n}}^n \frac{\d x}{x^2 + 2at + 1}. \tag{1}$$ The discriminant of the denominator is $4 (a^2 - 1) \le 0\ (\forall a \in [0, 1))$, so $(1)$ becomes $$I_n(a) \le \int _{\frac{1}{n}}^n \frac{\d x}{(x+a)^2 + \left(\sqrt{\frac{1-a^2}{a}}\right)^2}.$$ I have to get to the answer $$\frac{\pi}{4 \sqrt{1-a^2}} \arctan \frac{\sqrt{1-a^2}}{a}.$$","['integration', 'limits']"
2239806,Volume by traversed triangle,"Given three smooth functions $f,g,h:\mathbb{R}\to\mathbb{R}^3$ how would one find the volume traversed by the triangle $\triangle(f(t),g(t),h(t))$ from $t=a$ to $t=b$? Assume that all triangles $\triangle(f,g,h)$ are distinct (i.e., there are no intersections between the areas of any two triangles $\triangle(f(s),g(s),h(s))$ and $\triangle(f(t),g(t),h(t))$ where $s\ne t$).","['derivatives', 'volume', 'calculus', 'integration', 'geometry']"
2239857,"How to write the set-builder notation for the ""best $n$ elements of the set S""?","I have a set $S$ with 100 elements, and a very simple function defined over my set $f : S → [0, 1]$. This function simply tells me how ""good"" an element is (this function is strictly monotonous). Now I want to define a subset $G \subset S$, which contains the top 10 best elements in $S$, i.e. those for which $f(x)$ gives the highest 10 values. How do I write $G$ in short set-builder notation ? I had an idea about repeatedly using $\underset{x \in S}{\operatorname{argmin}} ~f(x)$, where $S$ kept shrinking, namely $S_i = S_{i-1} - \{\underset{x \in S}{\operatorname{argmin}}~f(x)\}$, and after 90 iterations, $S_{90}$ would be my ""top 10"" set. But I have no idea how to write these iterations in set-builder notation.","['notation', 'elementary-set-theory']"
2239865,Set theory: product of families of sets proof,"$\{A_i\}_{i \in I}$ and $\{B_i\}_{i \in I}$ are families of nonempty sets with the same index set, $I$. Prove that if $\prod_{i \in I}A_i \subset \prod_{i \in I}B_i$, then $A_i \subset B_i$ for all $i \in I$. I am struggling with this proof question. I know the product is a set of functions, but how would I start the proof?",['elementary-set-theory']
2239911,Integrals and comparison,"Let $I_{1} = \int_0^\frac{\pi}{4}  (\tan x)^{\frac{1}{3}}dx$  and $I_{2} = \int_0^\frac{\pi}{4} (\tan x)^{\sqrt 2}dx $ then.... Prove that $I_{1}\lt \frac{3}{4}$ , $I_{2}\lt \frac{\pi}{4}$ ....
and $I_{2}\lt \log(\sqrt 2) \lt I_{1}$ So basically i assumed in $I_{1}$ that $\tan x= t^\frac{3}{2}\\$ ending up as $\frac{3}{2} \int_0^1 \frac{t}{t^3 +1}dt$ but could not think ahead..","['integration', 'functions']"
2239928,"How many elements can the set $S(f)=\{f(x+y)-f(x)-f(y)\ |\ x,y\in R\}$ have？","Question: For a  surjective function $f：\mathbb R\to \mathbb R$,where $\mathbb R$ denotes the set of real numbers, define the set
  $$S(f)=\{f(x+y)-f(x)-f(y)\ |\ x,y\in \mathbb R\}\ .$$
  Assume that $S(f)$ is finite and $|S(f)|\neq 1$. Find  the possible values of $|S(f)|$. I think $|S(f)|=2$ is possible because of the example
$$f(x)=\dfrac{1}{2}(\lfloor x\rfloor-\{x\})\ .$$ It is clear $f$ is surjective,because for any $t\in R$, if we let $x=2t+2\{-2t\}$, then we have
$$f(x)=\dfrac{1}{2}(2t+2\{-2t\}-2\{-2t\})=t\ ,$$
and we then have
$$S(f)=f(x+y)-f(x)-f(y)=\{x\}+\{y\}-\{x+y\}=\{0,1\}\ ,$$
so we have$$|S(f)|=2\ .$$ Using this method, Can we  find examples such that $|S(f)|=3,4,5,\ldots$? Maybe there are other methods to solve this problem.","['elementary-set-theory', 'combinatorics', 'functions']"
2239962,Show that this $\sum_{i=0}^{p-1}(-1)^i\binom{p^2-p}{pi}\equiv p^{p-1}\pmod {p^p}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $p$ is an odd prime,show that
$$\sum_{i=0}^{p-1}(-1)^i\binom{p^2-p}{pi}\equiv p^{p-1}\pmod {p^p}$$",['number-theory']
2239982,Winning strategy in $(2n+1) \times (2n+1)$ matrix game.,"Edit : A few minutes after posting this question (that I had been thinking about for about a day) I figured out the answer in the $3 \times 3$ case; see my answer below. However, the question might still be interesting in the more general $(2n+1) \times (2n+1)$ case, so I am still interested in the question more generally. Below is the original question. Fix some field $\mathbb K$. Two players, Eloise and Abelard, play a game. They start with an empty $3 \times 3$ matrix.
$$
\begin{pmatrix}
 * & * & * \\
 * & * & * \\
 * & * & *
\end{pmatrix}
$$
Their turns alternate: first Eloise, then Abelard, and so on. A turn consists of filling in one of the blanks in the matrix with an element of $\mathbb K$. After Eloise's first move, the matrix might look like
$$
\begin{pmatrix}
 * & 3 & * \\
 * & * & * \\
 * & * & *
\end{pmatrix};
$$
after Abelard's first move, it could look like
$$
\begin{pmatrix}
 * & 3 & * \\
 * & * & * \\
 -5 & * & *
\end{pmatrix}.
$$
The game ends when the matrix is filled, so a game always lasts 9 turns. Eloise wins the game if the matrix is invertible -- that is, if its determinant is non-zero. Abelard wins if the matrix is singular -- that is, its determinant vanishes. Clearly, since the number of turns in the game is bounded, one of the players must have a winning strategy. My question is: Does Eloise or Abelard have a winning strategy? Does this depend on the field $\mathbb K$, and if so, in what way? The question is motivated by this question and answer where it is shown that if we work in a $2n \times 2n$ matrix instead, Abelard has a winning strategy: whenever Eloise plays a move in an odd row, Abelard plays the same move in the row below; and when Eloise plays a move in an even row, Abelard plays the same move in the row above. This ensures that for any $i$, rows $2i-1$ and $2i$ agree, so that the matrix is singular. Note that this strategy also works if Abelard just copies Eloise's moves on rows 1 and 2, and plays random moves (outside row 1 and 2) otherwise. The same strategy does not work in the odd case: if Abelard tries to make rows 1 and 2 of the $3 \times 3$ matrix, Eloise can fill up row 3 first, forcing Abelard to make a move in row 1 and 2 before Eloise has. Since all even numbers are easy, and a $1 \times 1$ matrix is a trivial case, this makes the $3 \times 3$ matrix the first non-trivial case; but of course we can ask the question for a $(2n +1) \times (2n+1)$ matrix for any $n$. I wrote a short, inefficient Python script to brute force the solution for $\mathbb K = \mathbb F_2$, and $\mathbb K = \mathbb F_3$. (It is so inefficient that $\mathbb K = \mathbb F_5$ is already a problem.) It turns out that in both these cases, Abelard has a winning strategy (assuming my script is correct), although I haven't been able to see a pattern to the results of the computation that suggest a human-understandable strategy. For one final observation: it appears that Eloise has an advantage, getting both the first and the last turn, but it turns out that her first turn is (essentially) determined. Namely, if she plays a zero, then Abelard can beat her. Without loss of generality, suppose she played the zero in the top left. Then Abelard can add a zero next to it, and Eloise is forced to save the row from becoming all zeros:
$$
\begin{pmatrix}
 0 & 0 & e_1 \\
 * & * & * \\
 * & * & *
\end{pmatrix}.
$$
Then, Abelard can play a zero in the middle left, forcing Eloise to similarly save the left column from becoming all zeros:
$$
\begin{pmatrix}
 0 & 0 & e_1 \\
 0 & * & * \\
 e_2 & * & *
\end{pmatrix}.
$$
But then Abelard wins by playing a 0 in the center field:
$$
\det\begin{pmatrix}
 0 & 0 & e_1 \\
 0 & 0 & * \\
 e_2 & * & *
\end{pmatrix} = 0.
$$
Hence, Eloise must play a non-zero move, and by scaling the entire matrix we might as well assume she plays a 1. Thus, we can start the game as
$$
\begin{pmatrix}
 1 & * & * \\
 * & * & * \\
 * & * & *
\end{pmatrix}
$$
instead, letting Abelard take the first move.","['game-theory', 'combinatorial-game-theory', 'linear-algebra', 'determinant']"
2240018,In which C*-algebras one-sided-invertible elements are invertible?,"For matrices $AB=I$ is equivalent to $BA=I$, however this is no longer true for operators on infinite-dimensional spaces. I was wondering whether there was a characterisation of C*-algebras where similar phenomena takes place. What would be sufficient conditions for a Banach algebra to make it work too?","['c-star-algebras', 'banach-algebras', 'functional-analysis', 'von-neumann-algebras', 'operator-algebras']"
2240046,What is the least positive integer $x$ such that $x^2$ starts with 2017?,"Olympiad problem that I can't find solutions except for approximations. $n$ is said to start with the $m$ digits number $k \iff n = k10^{r+m} + s$ for some integers $r,s$ such that $r \geq 0$ and $0 \leq s < 10^r$.","['number-theory', 'contest-math']"
2240050,Requirements on fields for determinants to bust dependence.,"Being very much used to working on $\mathbb R^n$ $\mathbb C^n$ I just played around a bit with $$M = \left[\begin{array}{cc}
2&1\\
1&2
\end{array}\right]$$ If the elements are in ""integers mod $3$"" field, then $$\det(M) = 2\cdot 2-1\cdot 1=4-1=3\equiv 0 (\mod3)$$ Being so used to real matrices I would at first glance guess they were linearly independent. But if I double first column I actually get $[4,2]^T \equiv [1,2]^T (\text{mod } 3)$. So they actually are dependent anyway? My real question is : will determinants being $0$ tell us linear dependence for all finite fields in the same way they do if we work over $\mathbb R$ or $\mathbb C$ or is there some special requirement on the field for determinants to bust dependence?","['matrices', 'abstract-algebra', 'independence', 'determinant', 'linear-algebra']"
2240053,Integrating over removable singularity,"I have a question, for example if $f(z)$ is a function with a removable singularity at one point say $z = z_0$, then can i conclude that $\int_{C}f(z)dz = 0$? For example the function $\int_{C}\dfrac{e^{iz}-1}{z}dz$ Does it hold for any function with removable singularity.",['complex-analysis']
2240145,"How to integrate $\int \frac{\cos^m x}{\cos nx} \,\mathrm{d}x$","How to integrate $$\int \frac{\cos^m x}{\cos nx} \,\mathrm{d}x$$ Motivation: $\int \cos mx \sec nx \,\mathrm{d}x $ and $ \int \cos mx\sec^n x\,\mathrm{d}x $ and  $\int \cos^m x \cos nx \,\mathrm{d}x $ and $\int \cos mx \cos nx \,\mathrm{d}x$ are all integrals with closed forms (including summations) I've solved myself or read elsewhere. Plugging various $m,n$ into wolfram shows the antiderivative always has a combination of trig functions and $\tanh^{-1}$. Nothing I've tried has worked or even simplified it.","['indefinite-integrals', 'integration']"
2240152,Is the injectivity radius constant if the Riemannian manifold has constant curvature?,"Suppose $M$ is a 2-dimensional complete Riemannian manifold of constant curvature. Is it true that the injectivity radius $i(p)$ is constant for all $p\in M$? In all the examples I know, this is indeed the case. But I could not find the above result somewhere written. Does someone know the answer? Best regards","['riemannian-geometry', 'differential-geometry']"
2240195,Can we define the Cantor Set in this way?,"Let $$C_0 = \left\{ 0.0 \right\}$$ Define, for $$n∈ℕ : n> 0$$ subsequently: $$C_n = \left(C_{n-1} +  \frac{2}{3^n} \right) \bigcup  C_{n-1}$$ Let $C$ be the Cantor Set, then $$C = \bigcup_{n=1}^\infty C_n $$ My concern is that members like $0.1$ are never defined but $0.0222\dots$ might be, which is kind of the same thing.","['cantor-set', 'real-analysis', 'notation', 'elementary-set-theory']"
2240221,The Hilbert transform of analytic function is still analytic.,"If a function $f$ is analytic in the strip $\mathcal{D}_d = \left\{ z \in \mathbb{C} : |\Im(z)| < d \right\}$, how to show that the Hilbert transform of $f$, which is $\mathcal{H}f(x) = p.v. \int_{\mathbb{R}} \frac{f(t)}{x-t}dt$ ($p.v.$ means Cauchy principal value), is also analytic in this strip?",['complex-analysis']
2240238,Integral of $\sin(u)\mathrm du$?,"If I search for a general formula on google for $\displaystyle \int \sin(u) \ \mathrm du$, where $u$ represents a function, I am presented with $-\cos(u)+C$. But, shouldn't the answer be $\dfrac{-\cos(u)}{u'}+C$? This applies for integrals of basic trigonometric functions such as $\sec(u)$, $\csc(u)$, $\tan(u)$, etc. They all seem to be missing the division of u'. Link to general formulas: https://www.math.ksu.edu/courses/exam-archive/math221/221t1f07.pdf","['integration', 'trigonometry', 'calculus']"
2240245,to prove the Cohen theorem,"Now given a fact that I had proven: If $I$ is an ideal in a commutative ring $R$ such that $I$ is not finitely generated but every ideal properly containing $I$ is finitely generated, then $I$ is a prime ideal. My question is how to use this fact to prove the Cohen theorem: If every prime ideal of $R$ is finitely generated, then $R$ is noetherian. My attempt is: suppose not, then there is an ideal not finitely generated, but didn't know how to derive any contradictions. (I don't think the ideal properly containing it are all finitely generated?) Thanks for any helps in advance! Added: I asked this question for I just want to know if I can reach the conclusion without the axiom of choice, and after thinking these hours and with the comment and answer from you deer, I think that is unavoidable. Thank you all guys!","['abstract-algebra', 'commutative-algebra']"
2240247,Why are there multiple Jordan Blocks corresponding to the same eigenvalue?,"Though the title seems clear enough, I'd like to start with a discussion of how I personally came to derive the Jordan Normal Form, because my question is very specific to the details of my derivation. Notation To start, let $X$ be a finite dimensional vector space, $L(X)$ be the space of linear operators on $X$, and $A\in L(X)$. Let $\sigma(A) = \{\lambda_1,\ \cdots,\ \lambda_k\}$ be the spectrum of $A$. Now, we define $d(\lambda)$ to be the geometric multiplicity of $\lambda$ $m(\lambda)$ to be the algebraic multiplicity of $\lambda$ Next, we denote the $k$th generalized eigenspace of $\lambda$ by
$$
\text{N}_k(\lambda) = \text{Ker}(A-\lambda I)^k
$$
and finally, we let
$$
\text{N}(\lambda) = N_{n(\lambda)}(\lambda)\qquad n(\lambda)=\min\{k\in\mathbb{N}\ |\ \text{N}_k(\lambda)=N_{k+1}(\lambda)\}
$$
we note that it can be shown that $n(\lambda) = m(\lambda)$, and so the notation $n(\lambda)$ won't really be used. We will also let $\sum_\lambda$, $\prod_\lambda$, etc. represent the sum/product/etc. over distinct eigenvalues of $A$. Fundamentals First off, it is known that we can decompose $X$ as
$$
X = \text{N}(\lambda_1)\oplus\cdots\oplus\text{N}(\lambda_k)
$$
Hence $\sum_{\lambda} \dim\ \text{N}(\lambda) = \dim X$. Also, from the characteristic polynomial of $A$, the sum of the algebraic multiplicities of the eigenvalues must equal the degree of the polynomial, which is $\dim X$. Thus
$$
\sum_\lambda\dim\ \text{N}(\lambda) = \sum_\lambda m(\lambda) = \dim X
$$
Going in a different direction, we present the following theorem: Theorem: If $B\in L(X)$ is nilpotent of order $n$, and $S\subset X\backslash\text{Ker} B^{n-1}$ is linearly independent, then
  $$
\bigcup_{x\in S}\{x,\ Bx,\ B^2x,\ \cdots,\ B^{n-1}x\}
$$
  is linearly independent. Proof: We will show the case for $|S|=2$, and the general case follows the same format. Suppose $S = \{x,\ y\}$, and
$$
\sum_{k=0}^{n-1} a_k B^kx_1 + \sum_{k=0}^{n-1}b_k B^kx_2 = 0
$$
applying $B^{n-1}$ to both sides gives
$$
B^{n-1}\left(\sum_{k=0}^{n-1}a_kB^kx_1+b_kB^kx_2\right) = a_0B^{n-1}x_1+b_0B^{n-1}x_2 = B^{n-1}(a_0x_1+b_0x_2) = 0
$$
so $a_0x_1 + b_0x_2\in\text{Ker}B^{n-1}$. However, since $\text{Ker}B^{n-1}$ is a subspace of $X$, we can decompose $X$ as $X = \text{Ker}B^{n-1}\oplus Z$ for some vector space $Z$, for which $\{x_1,\ x_2\}\subset Z\backslash\{0\}$. Since $Z$ is a subspace, $a_0x_1+b_0x_2\in Z$. To say that $a_0x_1+b_0x_2\in \text{Ker}B^{n-1}\cap Z$ is equivalent to saying $a_0x_1+b_0x_2 = 0$. By linear independence of $S$, $a_0=b_0=0$. This process can be repeated to get $a_j=b_j=0$ for all $j$. $\blacksquare$ Now, take $x\in \text{N}(\lambda)\backslash \text{N}_{m(\lambda)-1}(\lambda)$. Note that $B_\lambda = (A - \lambda I)|_{\text{N}(\lambda)}$ (that is, $A - \lambda I$ restricted to $\text{N}(\lambda)$) is nilpotent of order $m(\lambda)$. Hence $\{x,\ B_\lambda x,\ \cdots,\ B_\lambda^{m(\lambda)-1}x\}$ is linearly independent, and it's span is a subspace of $\text{N}(\lambda)$. Hence $\dim \text{N}(\lambda) \ge m(\lambda)$. If we suppose that $\dim\text{N}(\lambda) > m(\lambda)$ for at least one $\lambda\in\sigma(A)$, then we contradict the fact that $\sum_\lambda\dim\text{N}(\lambda) = \dim X$, and so we conclude that $m(\lambda) = \dim\text{N}(\lambda)$. Alright, so far so good I hope... Jordan Normal Form By the above arguments, we conclude that $\text{Span}\{x,\ \cdots,\ B^{m(\lambda)-1}x\} = \text{N}(\lambda)$. Hence, if we let $e_0(\lambda)\in N(\lambda)\backslash N_{m(\lambda)-1}(\lambda)$, and $e_k(\lambda)=(A-\lambda I)^k e_0(\lambda)$, then
$$
\text{Span}\left(\bigcup_{\lambda}\bigcup_{k=0}^{m(\lambda)-1}\{e_k(\lambda)\}\right) = X
$$ Since $X = \text{N}(\lambda_1)\oplus\cdots\oplus\text{N}(\lambda_k)$, and each $\text{N}(\lambda_k)$ is $A$-invariant (that is $A(\text{N}(\lambda_k))\subseteq \text{N}(\lambda_k)$), it follows that if we have bases for each $N(\lambda_i)$, then we can get the following matrix representation of $A$ wrt the union of these bases:
$$
A = \left[\begin{matrix}
A|_{\text{N}(\lambda_1)} & O & \cdots & \vdots \\
O & A|_{\text{N}(\lambda_2)} & \cdots & \vdots \\
\vdots & \vdots & \ddots & \vdots \\
\cdots & \cdots & \cdots & A|_{\text{N}(\lambda_k)}
\end{matrix}\right]
$$
where $A|_{\text{N}(\lambda_i)}$ is the matrix representation of $A$ restricted to $\text{N}(\lambda_i)$ wrt the basis of $\text{N}(\lambda_i)$. Above, we demonstrated that $\{e_{m(\lambda)-1}(\lambda),\ \cdots,\ e_1(\lambda)\}$ is a basis for $\text{N}(\lambda)$. We can find a matrix representation for $A|_{\text{N}(\lambda_i)}$ by noting that
$$
Ae_k(\lambda) = A(A-\lambda I)^ke_1(\lambda) = (A-\lambda I)^{k+1}e_1(\lambda) + \lambda(A-\lambda I)^ke_1(\lambda) \\
Ae_k(\lambda) = e_{k+1}(\lambda)+\lambda e_k(\lambda) \\
Ae_{m(\lambda)-1}(\lambda) = \lambda e_{m(\lambda)-1}(\lambda)
$$
and so
$$
A|_{N(\lambda)} = \left[\begin{matrix}
\lambda & 1 & 0 & \cdots & 0 \\
0 & \lambda & 1 & \cdots & 0 \\
0 & 0 & \lambda & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & 1 \\
0 & 0 & 0 & \cdots & \lambda
\end{matrix}\right]
$$ These $A|_{N(\lambda)}$ are the Jordan Blocks , and the matrix representation of $A$ above is the Jordan Normal Form . Main Question I'm pretty content with this derivation, nothing seems confusing or out of place or contradictory or nonrigorous, at least at a surface level. I would not be asking this question if I didn't go to the Wikipedia page on the Jordan Normal Form and see this line: The number of Jordan blocks corresponding to $\lambda$ of size at least $j$ is $\dim \text{Ker}(A - \lambda I)^j - \dim \text{Ker}(A - \lambda I)^{j-1}$. My ""derivation"" doesn't account for the fact that there can be multiple Jordan Blocks corresponding to the same eigenvalue . So, in the broadest sense possible, why? What don't I account for? My idea was that I ""assumed"" that $\text{Span}\{x,\ \cdots,\ \text{B}_\lambda^{m(\lambda)-1}x\} = \text{N}(\lambda)$. If there are more elements in the basis for $\text{N}(\lambda)$ than this, then there are more Jordan blocks. But if $\text{N}(\lambda)>m(\lambda)$, then the decomposition of $X$ into the direct sum of generalized eigenspaces fails, since the dimensions don't add up. My only other guess is that $\{x,\ \cdots,\ \text{B}_\lambda^{m(\lambda)-1}x\}$ can be ""broken down"" in some sense into the union of smaller bases which then produce more Jordan blocks, but I can't quite see where to go with that. Any help would be appreciated. Thank you for your time!","['matrices', 'eigenvalues-eigenvectors', 'jordan-normal-form', 'linear-algebra']"
2240273,"Meaning of terminologies ""regular value"", ""point spectrum"", ""continous spectrum"" and ""residual spectrum""","On page 371, ""Introductory Functional Analysis with Applications"", written Erwin Kreyszig, the author give the following definition (Definition 7.2-1): Let $X \neq {0}$ be a complex normed space and $T: D(T) \rightarrow X$ a linear operator with domain $D(T) \subset X$. A regular value $\lambda$ of $T$ is a complex number such that (R1) $R_{\lambda}(T)$ exists, (R2) $R_{\lambda}(T)$ is bounded, (R3) $R_{\lambda}(T)$ is defined on a set which is dense in $X$. Question: Why do we need the condition that ""a set which is dense"" in this case? The resolvent set $\rho(T)$ of $T$ is the set of all regular value $\lambda$ of $T$. Its complement $\sigma(T) = \mathbb{C} - \rho(T)$ in the complex plane $\mathbb{C}$ is called the spectrum of $T$, and a $\lambda \in \sigma (T)$ is called a spectral value of $T$. Furthermore, the spectrum is partitioned into three disjoint sets as follows. The point spectrum or discrete spectrum $\sigma_p(T)$ is the set such that $R_{\lambda} (T)$ does not exist. Question: Why do we use the terminology ""point spectrum"" for this definition? The continous spectrum $\sigma _c (T)$ is the set such that $R_{\lambda}(T)$ exists and satisfies (R3) but not (R2), that is, $R_{\lambda}(T)$ is not bounded. Question: Why do we use the terminology ""continous spectrum"" for this definition? How does it make sense? The residual soectrum $\sigma _r (T)$ is the set such that $R_{\lambda}(T)$  exists (and may be bounded or not) but does not satisfy (R3), that is, the domain of $R_{\lambda}(T)$  is not dense in $X$ Question: Why do we use the terminology ""residual spectrum"" for this definition? How does it make sense?","['functional-analysis', 'spectral-theory']"
2240323,"Given $g: \Bbb R \to \Bbb R^+$ where $g(x) = 3^x$, define inverse function $g^{-1}$","Given $g: \Bbb R \to \Bbb R^+$ where $g(x) = 3^x$, define fully the inverse function $g^{-1}$ and state the value of $g^{-1}(1)$. This is what I have tried: $$\begin{align}
g(x) &= 3^x \\
3^x &= y \\
x \log_3 3 &= \log_3 y \\
x &= \log_3 y \\
g^{-1}(1) &= \log_3 1
\end{align}$$ Why is this wrong?","['real-analysis', 'inverse-function', 'functions']"
2240327,"If the integral of a function $f:\mathbb{R}\rightarrow\mathbb{R}$ along an interval symmetric to the origin is zero, is the function odd?","$\def\d{\mathrm{d}}$We know that if a function $f:\mathbb{R}\rightarrow\mathbb{R}$ is odd, then$$\int_{-a}^a f(x)\,\d x=0.$$ I'm wondering if the converse is true, and if not, if there are any counterexamples. Thanks! Edit: There was confusion about the quantifier for $a$, and I was also looking for continuity, even though I didn't say that in the question. So to clarify, I was wondering if the following statement was true or false: Given a function $f \in C(I)$, if for some $a$ the integral$$\int_{-a}^af(x)\,\d x=0,$$ then $f$ must be odd.","['integration', 'calculus']"
2240329,Show that there is no function $f$ that is analytic in punctured unit disc and $f'$ has a simple pole at $0$.,Show that there is no function $f$ that is analytic in punctured unit disc and $f'$ has a simple pole at $0$. Let such function exist.And I have $\int( f'(z)-a_{-1}\frac{1}{z})dz=0 $ over some closed circle in open unit disc. Then how to proceed further? Is this correct to say since $f(z)$ is primitive of $f'(z)$. then $\int f'(z) dz =0$ along that circle. Then it follow $\int (a_{-1}\frac{1}{z})dz=0 $ which is not true.,['complex-analysis']
